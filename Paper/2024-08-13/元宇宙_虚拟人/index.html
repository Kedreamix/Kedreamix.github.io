<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>元宇宙/虚拟人 | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="元宇宙&#x2F;虚拟人 方向最新论文已更新，请持续关注 Update in 2024-08-13  Digital Avatars Framework Development and Their Evaluation">
<meta property="og:type" content="article">
<meta property="og:title" content="元宇宙&#x2F;虚拟人">
<meta property="og:url" content="https://kedreamix.github.io/Paper/2024-08-13/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/index.html">
<meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World">
<meta property="og:description" content="元宇宙&#x2F;虚拟人 方向最新论文已更新，请持续关注 Update in 2024-08-13  Digital Avatars Framework Development and Their Evaluation">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://picx.zhimg.com/v2-91d7e492bd253608b8d214b16de3f38e.jpg">
<meta property="article:published_time" content="2024-08-13T15:52:05.000Z">
<meta property="article:modified_time" content="2024-08-13T15:52:05.041Z">
<meta property="article:author" content="Kedreamix">
<meta property="article:tag" content="元宇宙&#x2F;虚拟人">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://picx.zhimg.com/v2-91d7e492bd253608b8d214b16de3f38e.jpg"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/Paper/2024-08-13/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-74LZ5BEQQ1');
</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":true,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  noticeOutdate: {"limitDay":500,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: true,
  islazyload: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '元宇宙/虚拟人',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-08-13 23:52:05'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const now = new Date()
          const hour = now.getHours()
          const isNight = hour <= 6 || hour >= 24
          if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
          else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css"  media="defer" onload="this.media='all'"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload="this.media='all'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.0.0"><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml">

<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()
  window.addEventListener('load',() => { preloader.endLoading() })

  if (false) {
    document.addEventListener('pjax:send', () => { preloader.initLoading() })
    document.addEventListener('pjax:complete', () => { preloader.endLoading() })
  }
})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">304</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">17</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://picx.zhimg.com/v2-91d7e492bd253608b8d214b16de3f38e.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"/><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">元宇宙/虚拟人</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-08-13T15:52:05.000Z" title="发表于 2024-08-13 23:52:05">2024-08-13</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-08-13T15:52:05.041Z" title="更新于 2024-08-13 23:52:05">2024-08-13</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">4k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>13分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="元宇宙/虚拟人"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote>
<p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-08-13-更新"><a href="#2024-08-13-更新" class="headerlink" title="2024-08-13 更新"></a>2024-08-13 更新</h1><h2 id="Digital-Avatars-Framework-Development-and-Their-Evaluation"><a href="#Digital-Avatars-Framework-Development-and-Their-Evaluation" class="headerlink" title="Digital Avatars: Framework Development and Their Evaluation"></a>Digital Avatars: Framework Development and Their Evaluation</h2><p><strong>Authors:Timothy Rupprecht, Sung-En Chang, Yushu Wu, Lei Lu, Enfu Nan, Chih-hsiang Li, Caiyue Lai, Zhimin Li, Zhijun Hu, Yumei He, David Kaeli, Yanzhi Wang</strong></p>
<p>We present a novel prompting strategy for artificial intelligence driven digital avatars. To better quantify how our prompting strategy affects anthropomorphic features like humor, authenticity, and favorability we present Crowd Vote - an adaptation of Crowd Score that allows for judges to elect a large language model (LLM) candidate over competitors answering the same or similar prompts. To visualize the responses of our LLM, and the effectiveness of our prompting strategy we propose an end-to-end framework for creating high-fidelity artificial intelligence (AI) driven digital avatars. This pipeline effectively captures an individual’s essence for interaction and our streaming algorithm delivers a high-quality digital avatar with real-time audio-video streaming from server to mobile device. Both our visualization tool, and our Crowd Vote metrics demonstrate our AI driven digital avatars have state-of-the-art humor, authenticity, and favorability outperforming all competitors and baselines. In the case of our Donald Trump and Joe Biden avatars, their authenticity and favorability are rated higher than even their real-world equivalents. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.04068v1">PDF</a> This work was presented during the IJCAI 2024 conference proceedings   for demonstrations</p>
<p><strong>Summary</strong><br>提出了一种新的提示策略，用于驱动人工智能数字化身，通过创新的视觉工具和“众筹评分”机制展示其优越的幽默、真实感和好感度。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>提出了新的提示策略，用于驱动人工智能数字化身。</li>
<li>引入了“众筹评分”机制，评估大型语言模型在不同提示下的表现。</li>
<li>提出了端到端框架，用于创建高保真度的人工智能数字化身。</li>
<li>开发了实时音视频流算法，支持从服务器到移动设备的流畅传输。</li>
<li>数字化身在幽默、真实感和好感度等方面优于竞争对手和基准模型。</li>
<li>Donald Trump 和 Joe Biden 的数字化身比其现实世界等价物更具真实感和好感度。</li>
<li>研究成果显示，新方法显著提高了数字化身的人性化交互效果。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<p>好的，我会按照您的要求来整理这篇论文的信息。</p>
<ol>
<li><p><strong>标题</strong>： 数字化身：框架开发及其评估<br><strong>中文翻译</strong>： Digital Avatars: Framework Development and Their Evaluation</p>
</li>
<li><p><strong>作者</strong>： Timothy Rupprecht、Sung-En Chang、Yushu Wu等。<br><strong>英文作者名单</strong>： Timothy Rupprecht1, Sung-En Chang1, Yushu Wu1, Lei Lu1, Enfu Nan1, Chih-hsiang Li1, Caiyue Lai1, Zhimin Li1, Zhijun Hu1, Yumei He2, David Kaeli1 和 Yanzhi Wang1。</p>
</li>
<li><p><strong>作者隶属</strong>： 东北大学电子与计算机工程系和杜兰大学。<br><strong>中文隶属</strong>： 作者隶属东北大学电子与计算机工程系和杜兰大学。</p>
</li>
<li><p><strong>关键词</strong>： 大型语言模型（LLM）、数字化身、人工智能驱动、提示策略、度量指标。<br><strong>英文关键词</strong>： Large Language Model (LLM), Digital Avatar, Artificial Intelligence-driven, Prompting Strategy, Evaluation Metrics。</p>
</li>
<li><p><strong>链接</strong>： 请提供论文的链接和GitHub代码链接（如有）。论文链接：[论文链接]。GitHub代码链接：[GitHub链接]（如果可用，填写具体的GitHub链接，如不可用则填写”None”）。</p>
</li>
<li><p><strong>摘要</strong>：</p>
<ul>
<li>(1) 研究背景：本文研究了在人工智能驱动的数字化身领域，如何有效地使用提示策略来增强化身的幽默感、真实感和好感度。研究背景是随着虚拟角色角色扮演平台的兴起，对于真实、生动地呈现虚拟角色的需求日益增长。</li>
<li>(2) 相关方法及其问题：现有的平台主要通过文本交互进行虚拟角色角色扮演，但响应效果仍无法真实地呈现现实世界角色的个性。因此，需要一种新的提示策略来提升LLM的响应质量。过去的方法缺乏有效度量指标来评估数字化身的幽默感、真实感和好感度。</li>
<li>(3) 研究方法：本文提出了一种新的提示策略，称为“展示而非告诉”策略，用于提升LLM在角色扮演中的幽默感、真实感和好感度。同时，开发了一个端到端的数字化身框架，通过可视化工具展示LLM的响应效果。为了量化评估数字化身的质量，提出了一种新的度量指标——Crowd Vote。</li>
<li>(4) 任务与性能：本文在创建数字化身的任务上进行了实验，并比较了不同提示策略下LLM的性能。实验结果显示，使用“展示而非告诉”策略的数字化身在幽默感、真实感和好感度方面均优于其他方法，甚至超过了现实世界角色的等效物。性能结果支持了该方法的有效性。</li>
</ul>
</li>
</ol>
<p>以上是关于该论文的概括和解读，希望对您有帮助。<br>好的，我会按照您的要求对论文的<methods>部分进行解读和总结。以下是该论文的方法论概述：</methods></p>
<ol>
<li>方法论：</li>
</ol>
<p>(1) 提出新的提示策略——“展示而非告诉”策略，用于提升大型语言模型（LLM）在角色扮演中的幽默感、真实感和好感度。该策略通过提供示例让LLM直接学习，而非仅通过指令进行学习。初始提示中会提供大量现实角色的特性回应示例，并定义角色的基本特征，以期模拟真实世界的反应。为了增加回应的生动性和趣味性，还融入了幽默元素。</p>
<p>(2) 开发了一个端到端的数字化身框架，展示了在这项工作中开发的提示策略。该框架包括语音识别、文本转语音、说话面部合成和视频选择算法等模块。使用局部实现的最先进技术，无论基线架构如何，都能得到相同的定性趋势。该框架首次使用端到端的AI驱动数字化身管道，包括用于化身说话的的大型语言模型。</p>
<p>(3) 提出了名为“Crowd Vote”的新型评估指标。这是基于Crowd Score（一种利用大型语言模型作为评判来衡量幽默效果的方法）的改进方法。不同于原有的Crowd Score仅从大量笑话中排序出最有趣的笑话，Crowd Vote要求具有不同性格的评委从多个LLM候选回应中选择最能体现真实性、友好性或幽默感的回答。这种方法可以帮助研究人员更准确地评估数字化身的性能和质量。例如，在比较不同提示策略下LLM的性能时，可以通过观察评委对不同回应的投票结果来评估不同策略的优劣。这种评价方式更贴近现实世界的实际应用场景，有助于研究人员更好地了解数字化身的性能表现。例如，在某个对话场景中，不同LLM生成的回应可能略有差异，通过评委的投票结果可以直观地看出哪种回应更具说服力和自然性。因此，“Crowd Vote”评价指标的引入对于评估数字化身的性能和质量具有重要意义。                 </p>
<p>以上就是对该论文方法的解读和总结，希望对您有所帮助。</p>
<p>好的，我会按照您的要求来总结这篇论文的结论。以下是回答：</p>
<ol>
<li>Conclusion:</li>
</ol>
<p>（1）这篇论文的研究对于人工智能驱动的数字化身领域具有重要意义。它提出了一种新的提示策略来提升大型语言模型在角色扮演中的表现，并开发了一个端到端的数字化身框架来展示这种策略的效果。此外，论文还提出了一种新的度量指标来评估数字化身的性能。这些成果有助于提升虚拟角色角色扮演平台的真实感和生动性，满足日益增长的对真实、生动地呈现虚拟角色的需求。</p>
<p>（2）创新点：论文提出了一种新的提示策略——“展示而非告诉”，该策略通过提供示例让大型语言模型学习角色的特性回应，提高了角色扮演的幽默感、真实感和好感度。此外，论文还开发了一个端到端的数字化身框架，并首次使用了端到端的AI驱动数字化身管道。这些创新点都是对数字化身领域的贡献。</p>
<p>性能：论文通过实验验证了所提出的提示策略和数字化身框架的有效性。实验结果显示，使用“展示而非告诉”策略的数字化身在幽默感、真实感和好感度方面均优于其他方法，甚至超过了现实世界角色的等效物。此外，论文提出的度量指标也表现出了良好的评估效果。</p>
<p>工作量：论文的工作量较大，涉及到多个方面的研究和实验。包括提出新的提示策略、开发数字化身框架、设计实验验证等。不过，具体的工作量评估需要更深入的了解和研究，无法仅凭摘要给出准确的评价。</p>
<details>
  <summary>点此查看论文截图</summary>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-91d7e492bd253608b8d214b16de3f38e.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-56151ba859e203bc1c7d5707b0d535ab.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-4f434e083d4071ff2e1a61b64cd59eb6.jpg" align="middle">
</details>




<h2 id="AvatarPose-Avatar-guided-3D-Pose-Estimation-of-Close-Human-Interaction-from-Sparse-Multi-view-Videos"><a href="#AvatarPose-Avatar-guided-3D-Pose-Estimation-of-Close-Human-Interaction-from-Sparse-Multi-view-Videos" class="headerlink" title="AvatarPose: Avatar-guided 3D Pose Estimation of Close Human Interaction   from Sparse Multi-view Videos"></a>AvatarPose: Avatar-guided 3D Pose Estimation of Close Human Interaction   from Sparse Multi-view Videos</h2><p><strong>Authors:Feichi Lu, Zijian Dong, Jie Song, Otmar Hilliges</strong></p>
<p>Despite progress in human motion capture, existing multi-view methods often face challenges in estimating the 3D pose and shape of multiple closely interacting people. This difficulty arises from reliance on accurate 2D joint estimations, which are hard to obtain due to occlusions and body contact when people are in close interaction. To address this, we propose a novel method leveraging the personalized implicit neural avatar of each individual as a prior, which significantly improves the robustness and precision of this challenging pose estimation task. Concretely, the avatars are efficiently reconstructed via layered volume rendering from sparse multi-view videos. The reconstructed avatar prior allows for the direct optimization of 3D poses based on color and silhouette rendering loss, bypassing the issues associated with noisy 2D detections. To handle interpenetration, we propose a collision loss on the overlapping shape regions of avatars to add penetration constraints. Moreover, both 3D poses and avatars are optimized in an alternating manner. Our experimental results demonstrate state-of-the-art performance on several public datasets. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.02110v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://feichilu.github.io/AvatarPose/">https://feichilu.github.io/AvatarPose/</a></p>
<p><strong>Summary</strong><br>利用个性化的隐式神经化身作为先验，显著改进多人近距离交互情境下的3D姿势估计。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>提出利用个性化隐式神经化身作为先验来改进3D姿势估计的方法。</li>
<li>方法通过稀疏多视角视频的分层体积渲染有效重建化身。</li>
<li>新方法绕过了准确2D关节检测困难，直接优化基于颜色和轮廓渲染损失的3D姿势。</li>
<li>引入碰撞损失来处理化身的重叠区域，增加穿透约束。</li>
<li>通过交替优化3D姿势和化身，提高了方法的鲁棒性和精确度。</li>
<li>在多个公共数据集上展示出最先进的性能。</li>
<li>解决了多人近距离交互中的2D关节估计和姿势优化的挑战。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li><p>Title: 基于个性化隐式神经网络先验的多人紧密交互三维姿态估计方法（AvatarPose: Avatar-guided 3D Pose Estimation of Close Human Interaction）</p>
</li>
<li><p>Authors: Feichi Lu, Zijian Dong, Jie Song, Otmar Hilliges.</p>
</li>
<li><p>Affiliation: 作者来自苏黎世联邦理工学院计算机科学系和德国马普智能系统研究所。</p>
</li>
<li><p>Keywords: human pose estimation, human close interaction, multi-view pose estimation, avatar prior.</p>
</li>
<li><p>Urls: 论文链接：<a href="链接地址">论文链接</a>，GitHub代码链接：Github:None。</p>
</li>
<li><p>Summary:</p>
</li>
</ol>
<p>(1)研究背景：本文研究了从稀疏多视角视频估计多个紧密交互人的3D姿态和形状的问题。尽管已有一些多视角方法，但它们常常面临在估计紧密交互人的3D姿态和形状时的挑战。</p>
<p>(2)过去的方法及问题：现有方法主要依赖于准确的2D关节估计，但在人们紧密交互时，由于遮挡和身体接触，准确获取2D关节估计非常困难。</p>
<p>(3)研究方法：针对这些问题，本文提出了一种利用个性化隐式神经网络先验的新方法。首先，通过分层体积渲染从稀疏多视角视频中重建avatar。然后，利用重建的avatar先验，基于颜色和轮廓渲染损失直接优化3D姿态。此外，为了处理交互穿透问题，本文提出了在avatar重叠形状区域上的碰撞损失来添加穿透约束。最后，以交替的方式同时优化3D姿态和avatar。</p>
<p>(4)任务与性能：本文方法在多个公共数据集上实现了最先进的性能，证明了该方法的有效性。实验结果表明，该方法在估计多人紧密交互的3D姿态和形状任务上取得了显著的性能提升。</p>
<ol>
<li>Methods:</li>
</ol>
<ul>
<li>(1) 研究背景分析：文章针对从稀疏多视角视频估计多个紧密交互人的3D姿态和形状的问题展开研究。由于现有方法在处理紧密交互时面临诸多挑战，如遮挡和身体接触导致的准确获取2D关节估计困难等。</li>
<li>(2) 方法概述：文章提出了一种利用个性化隐式神经网络先验的新方法来解决这一问题。首先，通过分层体积渲染技术从稀疏多视角视频中重建avatar。然后，利用重建的avatar先验信息，基于颜色和轮廓渲染损失直接优化3D姿态。</li>
<li>(3) 交互处理策略：为了处理交互穿透问题，文章提出了在avatar重叠形状区域上的碰撞损失来添加穿透约束。这一策略能有效处理因紧密交互而产生的穿透问题。</li>
<li>(4) 优化过程：最后，以交替的方式同时优化3D姿态和avatar，以达到更准确的估计结果。实验结果表明，该方法在多个公共数据集上实现了最先进的性能，证明了其有效性。</li>
<li>(5) 实验验证：文章通过在实际数据集上进行实验验证，证明了该方法在估计多人紧密交互的3D姿态和形状任务上取得了显著的性能提升。</li>
</ul>
<ol>
<li>Conclusion:</li>
</ol>
<p>(1) 这项工作的意义在于提出了一种基于个性化隐式神经网络先验的多人紧密交互三维姿态估计方法，解决了从稀疏多视角视频估计多个紧密交互人的3D姿态和形状的问题，填补了相关领域的空白。</p>
<p>(2) 创优点：文章的创新点在于利用重建的avatar作为个性化先验信息，指导姿态优化，使得在多人紧密交互的场景下，能够更有效地估计3D姿态。同时，文章提出的碰撞损失有效地处理了交互穿透问题。<br>性能：文章的方法在多个公共数据集上实现了最先进的性能，证明了该方法的有效性。<br>工作量：文章进行了大量的实验验证，证明了方法的有效性，并在实际数据集上进行了广泛应用。此外，文章还对过去的方法进行了全面的回顾和分析，为后续研究提供了有力的支持。</p>
<p>总的来说，这篇文章在创新点、性能和工作量方面都表现出了一定的优势，为多人紧密交互三维姿态估计领域的研究提供了新的思路和方法。</p>
<details>
  <summary>点此查看论文截图</summary>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-3d50782e32cf65757b87c9d7ef8a007d.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-80099ec0387627a0de59f67f5f27de7a.jpg" align="middle">
</details>




</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://kedreamix.github.io/Paper/2024-08-13/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">https://kedreamix.github.io/Paper/2024-08-13/元宇宙_虚拟人/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">元宇宙/虚拟人</a></div><div class="post_share"><div class="social-share" data-image="https://picx.zhimg.com/v2-91d7e492bd253608b8d214b16de3f38e.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/Paper/2024-08-13/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-c51075d13244f3589cad4b953116ba5c.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Diffusion Models</div></div></a></div><div class="next-post pull-right"><a href="/Paper/2024-08-13/NeRF/" title="NeRF"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-2b0e0abdbc020b97d3e0fe97c1f53bf0.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">NeRF</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/Paper/2024-01-24/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-0f13a2b60bef4c886a3317754c99b456.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-24</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/Paper/2024-04-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-4f97970c093585e18e2db42fb96a6b75.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-06</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/Paper/2024-04-01/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-4f2a829065c463be027e4b423c4e43c8.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-01</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/Paper/2024-05-02/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-37516691b13dabbabb3b74ea46b402d8.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-02</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/Paper/2024-05-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-65e21e3a0a320adc36f81e6bfc7c5739.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-06</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/Paper/2024-05-28/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-dc27e0e81b6be96603dd90e8aa23e081.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-28</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-08-13-%E6%9B%B4%E6%96%B0"><span class="toc-text">2024-08-13 更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Digital-Avatars-Framework-Development-and-Their-Evaluation"><span class="toc-text">Digital Avatars: Framework Development and Their Evaluation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#AvatarPose-Avatar-guided-3D-Pose-Estimation-of-Close-Human-Interaction-from-Sparse-Multi-view-Videos"><span class="toc-text">AvatarPose: Avatar-guided 3D Pose Estimation of Close Human Interaction   from Sparse Multi-view Videos</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image: url('https://picx.zhimg.com/v2-91d7e492bd253608b8d214b16de3f38e.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><!-- hexo injector body_end start --><script data-pjax>
  function butterfly_swiper_injector_config(){
    var parent_div_git = document.getElementById('recent-posts');
    var item_html = '<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="Project/Linly-Talker - GPT-SoVITS/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="Project/Linly-Talker - GPT-SoVITS/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>';
    console.log('已挂载butterfly_swiper')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'undefined'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_swiper_injector_config();
  }
  else if (epage === cpage){
    butterfly_swiper_injector_config();
  }
  </script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script><!-- hexo injector body_end end --></body></html>