<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Talking Head Generation | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-11-05  Stereo-Talker Audio-driven 3D Human Synthesis with Prior-Guided   Mixture-of-Experts">
<meta property="og:type" content="article">
<meta property="og:title" content="Talking Head Generation">
<meta property="og:url" content="https://kedreamix.github.io/Paper/2024-11-05/Talking%20Head%20Generation/index.html">
<meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World">
<meta property="og:description" content="Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-11-05  Stereo-Talker Audio-driven 3D Human Synthesis with Prior-Guided   Mixture-of-Experts">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pica.zhimg.com/v2-5beacc9e107561f5a4f3fc35792c4159.jpg">
<meta property="article:published_time" content="2024-11-05T06:16:39.000Z">
<meta property="article:modified_time" content="2024-11-05T06:16:39.946Z">
<meta property="article:author" content="Kedreamix">
<meta property="article:tag" content="Talking Head Generation">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pica.zhimg.com/v2-5beacc9e107561f5a4f3fc35792c4159.jpg"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/Paper/2024-11-05/Talking%20Head%20Generation/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-74LZ5BEQQ1');
</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":true,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  noticeOutdate: {"limitDay":500,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: true,
  islazyload: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Talking Head Generation',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-11-05 14:16:39'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const now = new Date()
          const hour = now.getHours()
          const isNight = hour <= 6 || hour >= 24
          if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
          else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css"  media="defer" onload="this.media='all'"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload="this.media='all'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.0.0"><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml">

<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()
  window.addEventListener('load',() => { preloader.endLoading() })

  if (false) {
    document.addEventListener('pjax:send', () => { preloader.initLoading() })
    document.addEventListener('pjax:complete', () => { preloader.endLoading() })
  }
})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">304</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">17</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://pica.zhimg.com/v2-5beacc9e107561f5a4f3fc35792c4159.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"/><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Talking Head Generation</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-11-05T06:16:39.000Z" title="发表于 2024-11-05 14:16:39">2024-11-05</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-11-05T06:16:39.946Z" title="更新于 2024-11-05 14:16:39">2024-11-05</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">4.3k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>14分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Talking Head Generation"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote>
<p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-11-05-更新"><a href="#2024-11-05-更新" class="headerlink" title="2024-11-05 更新"></a>2024-11-05 更新</h1><h2 id="Stereo-Talker-Audio-driven-3D-Human-Synthesis-with-Prior-Guided-Mixture-of-Experts"><a href="#Stereo-Talker-Audio-driven-3D-Human-Synthesis-with-Prior-Guided-Mixture-of-Experts" class="headerlink" title="Stereo-Talker: Audio-driven 3D Human Synthesis with Prior-Guided   Mixture-of-Experts"></a>Stereo-Talker: Audio-driven 3D Human Synthesis with Prior-Guided   Mixture-of-Experts</h2><p><strong>Authors:Xiang Deng, Youxin Pang, Xiaochen Zhao, Chao Xu, Lizhen Wang, Hongjiang Xiao, Shi Yan, Hongwen Zhang, Yebin Liu</strong></p>
<p>This paper introduces Stereo-Talker, a novel one-shot audio-driven human video synthesis system that generates 3D talking videos with precise lip synchronization, expressive body gestures, temporally consistent photo-realistic quality, and continuous viewpoint control. The process follows a two-stage approach. In the first stage, the system maps audio input to high-fidelity motion sequences, encompassing upper-body gestures and facial expressions. To enrich motion diversity and authenticity, large language model (LLM) priors are integrated with text-aligned semantic audio features, leveraging LLMs’ cross-modal generalization power to enhance motion quality. In the second stage, we improve diffusion-based video generation models by incorporating a prior-guided Mixture-of-Experts (MoE) mechanism: a view-guided MoE focuses on view-specific attributes, while a mask-guided MoE enhances region-based rendering stability. Additionally, a mask prediction module is devised to derive human masks from motion data, enhancing the stability and accuracy of masks and enabling mask guiding during inference. We also introduce a comprehensive human video dataset with 2,203 identities, covering diverse body gestures and detailed annotations, facilitating broad generalization. The code, data, and pre-trained models will be released for research purposes. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.23836v1">PDF</a> </p>
<p><strong>Summary</strong><br>提出了一种名为Stereo-Talker的一步法音频驱动的3D人像视频生成系统，实现精确唇同步、表情和动作，并具有连续视角控制。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Stereo-Talker实现音频驱动的3D人像视频生成。</li>
<li>采用两阶段方法，第一阶段映射音频到高保真动作序列。</li>
<li>利用LLM和语义音频特征增强动作质量。</li>
<li>第二阶段改进了基于扩散的视频生成模型，加入MoE机制。</li>
<li>MoE机制包括视角引导和掩码引导。</li>
<li>引入掩码预测模块，提高掩码稳定性和准确性。</li>
<li>构建了包含2,203个身份的综合人像视频数据集。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li><p><strong>标题及翻译</strong>：Stereo-Talker: Audio-driven 3D Human Synthesis with Prior-Guided Mixture-of-Experts。翻译：Stereo-Talker：基于先验引导混合专家模型的音频驱动3D人像合成。</p>
</li>
<li><p><strong>作者</strong>：Xiang Deng, Youxin Pang, Xiaochen Zhao, Chao Xu等。具体作者名单及所属机构已在文中给出。</p>
</li>
<li><p><strong>所属机构翻译</strong>：清华大学自动化系等。具体根据文中给出的作者所属机构进行翻译。</p>
</li>
<li><p><strong>关键词</strong>：3D人类生成，多模态3D，运动合成。</p>
</li>
<li><p><strong>链接</strong>：论文链接待补充，GitHub代码链接（如果可用）：GitHub:None（若该论文的GitHub仓库尚未公开或没有可用的代码链接，可以如此填写）。</p>
</li>
<li><p><strong>摘要</strong>：</p>
<ul>
<li><p>(1)研究背景：本文的研究背景是音频驱动的三维人像合成技术。随着电影制作、人机交互和虚拟现实等领域的快速发展，该技术成为了一个热门研究方向。</p>
</li>
<li><p>(2)过去的方法及问题：早期的方法主要集中在基于音频信号的嘴巴区域合成，忽视了其他身体部分。随着面部重塑方法的进步，基于单幅肖像图像的全头谈话视频生成得到了广泛探索。但现有方法常常生成不真实或模糊的视觉伪影，特别是在面部和手部区域，且未探索视点控制。</p>
</li>
<li><p>(3)研究方法：本文提出了Stereo-Talker框架，一个用于音频驱动、可控制视点的三维人像视频合成的系统。通过结合扩散模型的生成能力和先验引导混合专家模型（MoE）机制，实现了高质量的三维人像合成。其中，引入了视点引导的MoE和掩膜引导的MoE，以提高不同视点和身体部位的生成质量。同时，使用了一个掩膜预测模块，从运动数据中推导出人体掩膜，增强了推理过程中的稳定性和准确性。</p>
</li>
<li><p>(4)任务与性能：本文的方法在音频驱动的三维人像合成任务上取得了显著成果。通过广泛的实验验证，证明了该方法在生成高质量、高保真的三维谈话视频方面的能力，且支持连续视点控制。性能结果支持了该方法的有效性。</p>
</li>
</ul>
</li>
</ol>
<p>希望上述回答能满足您的要求！</p>
<ol>
<li><p>方法论：</p>
<ul>
<li><p>(1) 研究背景与问题定义：文章针对音频驱动的三维人像合成技术进行研究，随着电影制作、人机交互和虚拟现实等领域的快速发展，该技术成为热门研究方向。早期方法主要基于音频信号的嘴巴区域合成，忽视了其他身体部分。现有方法常常生成不真实或模糊的视觉伪影，特别是在面部和手部区域，且未探索视点控制。</p>
</li>
<li><p>(2) 方法提出：文章提出了Stereo-Talker框架，一个用于音频驱动、可控制视点的三维人像视频合成的系统。结合扩散模型的生成能力和先验引导混合专家模型（MoE）机制，实现了高质量的三维人像合成。引入了视点引导的MoE和掩膜引导的MoE，以提高不同视点和身体部位的生成质量。同时，使用掩膜预测模块，从运动数据中推导出人体掩膜，增强了推理过程中的稳定性和准确性。</p>
</li>
<li><p>(3) 技术细节：首先，利用预训练的大型语言模型建立原始音频和人体运动序列之间的精细映射。然后，设计了一种观点引导混合专家（MoE）模块和掩膜引导混合专家（MoE）模块，旨在增强合成视频输出的真实性和稳定性，从而推动视频生成质量边界。通过利用投影网络将高级语音语义特征转化为与文本潜在空间兼容的表示形式，利用扩散模型生成对应的运动序列。此外，通过引入视点先验信息和图像分割技术，提高合成视频的连贯性和逼真度。同时，开发了一个辅助模块来预测人类面具从骨骼数据，增强了方法的实用性。整个框架的训练过程中使用大规模人脸视频数据集进行验证和优化。最终实现了高质量的音频驱动的三维人像合成效果。</p>
</li>
</ul>
</li>
<li>Conclusion:</li>
</ol>
<p>(1)这篇工作的意义在于其对于音频驱动的三维人像合成技术的重大贡献。随着电影制作、人机交互和虚拟现实等领域的快速发展，该技术成为了一个热门且具挑战性的研究方向。该文章的工作为这一领域提供了新颖的框架和方法，推动了技术的发展。</p>
<p>(2)创新点：该文章提出了Stereo-Talker框架，结合扩散模型的生成能力和先验引导混合专家模型（MoE）机制，实现了高质量的三维人像合成。引入了视点引导的MoE和掩膜引导的MoE，提高了不同视点和身体部位的生成质量。此外，使用掩膜预测模块，从运动数据中推导出人体掩膜，增强了推理过程中的稳定性和准确性。<br>性能：该文章的方法在音频驱动的三维人像合成任务上取得了显著成果，通过广泛的实验验证，证明了该方法在生成高质量、高保真的三维谈话视频方面的能力，且支持连续视点控制。<br>工作量：文章详细描述了方法的实现过程，包括技术细节、训练过程等。同时，使用大规模人脸视频数据集进行验证和优化，证明了方法的实用性和效果。</p>
<p>总体来看，该文章在音频驱动的三维人像合成领域做出了重要的贡献，提供了新颖的方法和框架，并取得了显著的成果。</p>
<details>
  <summary>点此查看论文截图</summary>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-9f5835b4096bdcfc08d0418006d96209.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-98fcfd0175aac556d0a2af50ecc82e78.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-f1a7980b13d1d4fa16fad4d64da60aa2.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-63160830171bfa3fcfbce1588b29996a.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-5beacc9e107561f5a4f3fc35792c4159.jpg" align="middle">
</details>




<h2 id="ProbTalk3D-Non-Deterministic-Emotion-Controllable-Speech-Driven-3D-Facial-Animation-Synthesis-Using-VQ-VAE"><a href="#ProbTalk3D-Non-Deterministic-Emotion-Controllable-Speech-Driven-3D-Facial-Animation-Synthesis-Using-VQ-VAE" class="headerlink" title="ProbTalk3D: Non-Deterministic Emotion Controllable Speech-Driven 3D   Facial Animation Synthesis Using VQ-VAE"></a>ProbTalk3D: Non-Deterministic Emotion Controllable Speech-Driven 3D   Facial Animation Synthesis Using VQ-VAE</h2><p><strong>Authors:Sichun Wu, Kazi Injamamul Haque, Zerrin Yumak</strong></p>
<p>Audio-driven 3D facial animation synthesis has been an active field of research with attention from both academia and industry. While there are promising results in this area, recent approaches largely focus on lip-sync and identity control, neglecting the role of emotions and emotion control in the generative process. That is mainly due to the lack of emotionally rich facial animation data and algorithms that can synthesize speech animations with emotional expressions at the same time. In addition, majority of the models are deterministic, meaning given the same audio input, they produce the same output motion. We argue that emotions and non-determinism are crucial to generate diverse and emotionally-rich facial animations. In this paper, we propose ProbTalk3D a non-deterministic neural network approach for emotion controllable speech-driven 3D facial animation synthesis using a two-stage VQ-VAE model and an emotionally rich facial animation dataset 3DMEAD. We provide an extensive comparative analysis of our model against the recent 3D facial animation synthesis approaches, by evaluating the results objectively, qualitatively, and with a perceptual user study. We highlight several objective metrics that are more suitable for evaluating stochastic outputs and use both in-the-wild and ground truth data for subjective evaluation. To our knowledge, that is the first non-deterministic 3D facial animation synthesis method incorporating a rich emotion dataset and emotion control with emotion labels and intensity levels. Our evaluation demonstrates that the proposed model achieves superior performance compared to state-of-the-art emotion-controlled, deterministic and non-deterministic models. We recommend watching the supplementary video for quality judgement. The entire codebase is publicly available (<a target="_blank" rel="noopener" href="https://github.com/uuembodiedsocialai/ProbTalk3D/">https://github.com/uuembodiedsocialai/ProbTalk3D/</a>). </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.07966v3">PDF</a> 14 pages, 9 figures, 3 tables. Includes code. Accepted at ACM   SIGGRAPH MIG 2024</p>
<p><strong>Summary</strong><br>提出基于情感控制的不确定性语音驱动3D面部动画合成方法，实现更丰富情感表达。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究关注音频驱动的3D面部动画合成。</li>
<li>忽略情感和情感控制，仅关注唇同步和身份控制。</li>
<li>缺乏情感丰富的面部动画数据和合成算法。</li>
<li>多数模型确定性高，输出运动相同。</li>
<li>强调情感和非确定性对多样性面部动画的重要性。</li>
<li>提出ProbTalk3D模型，使用VQ-VAE和3DMEAD数据集。</li>
<li>模型在客观、主观评估中优于现有方法。</li>
<li>代码库公开，可在线获取。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li><p>Title: 基于非确定性模型的面部表情控制语音驱动三维人脸动画合成研究</p>
</li>
<li><p>Authors: 吴思春, 哈克·因贾马穆勒, 于扎克瑞·尤玛克</p>
</li>
<li><p>Affiliation: 乌得勒支大学（荷兰）</p>
</li>
<li><p>Keywords: 三维人脸动画合成，深度学习，虚拟人物，非确定性模型，情感控制面部动画</p>
</li>
<li><p>Urls: 论文链接：暂未提供；Github代码链接：GitHub:None</p>
</li>
<li><p>Summary: </p>
</li>
</ol>
<p>(1) 研究背景：随着虚拟现实技术的不断发展，三维人脸动画不仅在影视制作和游戏生产中扮演着重要角色，还在各种扩展现实应用领域中发挥着关键作用。创建三维人物面部表情动画是一项需要手动工作并依赖于熟练技术人员的任务。近年来，音频驱动的三维人脸动画合成已成为一个热门研究领域，其中情感控制和表情丰富性是该领域的重要研究方向。然而，大多数现有模型在确定性的框架下进行工作，无法生成多样化的情感丰富的面部表情动画。本文旨在解决这一问题。</p>
<p>(2) 过去的方法及问题：当前的研究主要关注唇同步和身份控制，忽视了情感在生成过程中的作用。由于缺少情感丰富的面部动画数据和能够同时合成带有情感表达的语音动画的算法，这一领域的研究进展有限。此外，大多数模型是确定性的，即给定相同的音频输入，它们会产生相同的输出运动。这限制了面部表情动画的多样性和情感丰富性。</p>
<p>(3) 研究方法：本文提出了一种基于非确定性神经网络模型的语音驱动三维人脸动画合成方法——ProbTalk3D。该方法使用两阶段VQ-VAE模型和丰富的情感数据集3DMEAD。通过引入非确定性模型，ProbTalk3D能够生成多样化的情感丰富的面部表情动画。该模型使用同一音频输入生成多个不同的动画样本，确保动画的唇同步、情感表达力和视觉质量。此外，该模型还允许通过情感标签和强度级别控制面部表情动画。</p>
<p>(4) 任务与性能：本文对所提出的ProbTalk3D模型进行了广泛的比较分析，通过客观、主观和用户感知研究等多种评估方法，将其与最新的三维人脸动画合成方法进行了比较。实验结果表明，ProbTalk3D在随机和确定性的模型以及情感控制模型中实现了卓越的性能。该模型的性能支持了其目标，即生成多样化的情感丰富的面部表情动画。</p>
<p>总的来说，本文提出了一种创新的非确定性模型ProbTalk3D，用于情感控制的语音驱动三维人脸动画合成，实现了良好的性能。</p>
<ol>
<li>方法论：</li>
</ol>
<p>(1) 背景介绍：该研究针对虚拟现实技术中三维人脸动画合成的重要性，特别是在影视制作、游戏生产以及扩展现实应用领域的广泛应用进行了阐述。针对现有模型在确定性的框架下工作，无法生成多样化的情感丰富的面部表情动画的问题，提出了一种基于非确定性神经网络模型的语音驱动三维人脸动画合成方法。</p>
<p>(2) 数据集选择：研究使用了3DMEAD数据集，该数据集通过从二维视听数据集中重建得到，包含了多种情感和强度的面部动画数据。数据集的每个帧都用FLAME 3D模型参数表示。为了进行有效的训练和生成动画，研究对原始训练配置进行了修改，提出了一个新的数据集分割方法。</p>
<p>(3) 问题定义：任务是基于音频和风格输入生成面部动画序列。为了解决这个问题，研究提出了一种监督神经网络模型训练方法，通过学习数据中的模式来预测面部运动。训练过程中使用了音频-运动对的数据。</p>
<p>(4) 模型构建：研究提出了ProbTalk3D模型，该模型分为两个阶段进行训练。第一阶段是运动自编码器阶段，通过向量量化变分自编码器（VQ-VAE）学习运动先验知识。第二阶段是利用预训练的HuBERT音频编码器和第一阶段的运动先验知识，进行语音和风格条件网络的训练。与其他模型不同，ProbTalk3D能够产生多样化的非确定性输出，并且使用较少的训练数据，具有更简洁、更高效的架构。在训练过程中，使用了量化重建损失和表情重建损失等多种损失函数来优化模型性能。此外，为了允许通过情感标签和强度级别控制面部表情动画，研究将风格向量（包含主体ID、情感类别和情感强度等信息）融入模型中。</p>
<p>总的来说，该研究通过对神经网络模型的训练和构建，实现了语音驱动的、非确定性的、情感可控的三维人脸动画合成。</p>
<ol>
<li>Conclusion:</li>
</ol>
<ul>
<li>(1)这篇工作的意义在于提出了一种基于非确定性神经网络模型的语音驱动三维人脸动画合成方法，该方法在虚拟现实技术中具有重要的应用价值，能够生成多样化的情感丰富的面部表情动画，为影视制作、游戏生产以及扩展现实应用领域提供更加丰富、真实的虚拟人物表情表现。</li>
<li>(2)创新点：该文章提出了基于非确定性模型的ProbTalk3D语音驱动三维人脸动画合成方法，实现了良好的性能。其创新之处在于引入非确定性模型，能够生成多样化的情感丰富的面部表情动画，解决了现有模型在确定性的框架下工作，无法生成多样化的情感丰富的面部表情动画的问题。</li>
<li>性能：该文章提出的ProbTalk3D模型在广泛的比较分析和评估中表现出优异的性能，能够生成高质量的面部表情动画，且具有良好的唇同步、情感表达力和视觉质量。</li>
<li>工作量：该文章使用了大量的数据预处理和模型训练，工作量较大，但实验结果证明了其有效性。</li>
</ul>
<p>希望以上总结符合您的要求。</p>
<details>
  <summary>点此查看论文截图</summary>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-0f786a8506c73488471cf2bd67e6d4ff.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-4777d67595c1d84bae8d0ec3415d2564.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-5f56353d10aa89888fc2578337682d8f.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-39f033345d783b993c831788a64d7b28.jpg" align="middle">
</details>




</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://kedreamix.github.io/Paper/2024-11-05/Talking%20Head%20Generation/">https://kedreamix.github.io/Paper/2024-11-05/Talking Head Generation/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Talking-Head-Generation/">Talking Head Generation</a></div><div class="post_share"><div class="social-share" data-image="https://pica.zhimg.com/v2-5beacc9e107561f5a4f3fc35792c4159.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/Paper/2024-11-05/3DGS/" title="3DGS"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-a34358ee3cfbfe322b409cc489bb64ce.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">3DGS</div></div></a></div><div class="next-post pull-right"><a href="/Paper/2024-11-05/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙/虚拟人"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-afd271b7cd6a3967f3b9c44e1dc6f579.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">元宇宙/虚拟人</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/Paperscape/Real3D-Portrait/" title="REAL3D-PORTRAIT ONE-SHOT REALISTIC 3D TALKING PORTRAIT SYNTHESIS"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-68585b79de5f83b0dfa23304f41b9b98.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-15</div><div class="title">REAL3D-PORTRAIT ONE-SHOT REALISTIC 3D TALKING PORTRAIT SYNTHESIS</div></div></a></div><div><a href="/Paperscape/EMO/" title="EMO Emote Portrait Alive - 阿里HumanAIGC"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-6492e24fb03ffa98135dc584535ab7d9.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-03</div><div class="title">EMO Emote Portrait Alive - 阿里HumanAIGC</div></div></a></div><div><a href="/Paperscape/VividTalk/" title="VividTalk One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-8521b04f82075cc27b5e95148dba9792.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-05</div><div class="title">VividTalk One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior</div></div></a></div><div><a href="/Paperscape/SyncTalk/" title="SyncTalk The Devil is in the Synchronization for Talking Head Synthesis"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-a57e0937b2f452009023394a59529dfb.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-07</div><div class="title">SyncTalk The Devil is in the Synchronization for Talking Head Synthesis</div></div></a></div><div><a href="/Note/BlendShape/" title="Blendshape学习笔记"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://p6-sign.toutiaoimg.com/pgc-image/2c8cbd123e00470e95500a8ae62da605~noop.image?_iz=58558&from=article.pc_detail&lk3s=953192f4&x-expires=1710668214&x-signature=UHPhjWP4v96kbtfJzF97Z%2Bp3klc%3D" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-11</div><div class="title">Blendshape学习笔记</div></div></a></div><div><a href="/Paper/Awesome-Talking-Head-Synthesis/" title="超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-01</div><div class="title">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-11-05-%E6%9B%B4%E6%96%B0"><span class="toc-text">2024-11-05 更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Stereo-Talker-Audio-driven-3D-Human-Synthesis-with-Prior-Guided-Mixture-of-Experts"><span class="toc-text">Stereo-Talker: Audio-driven 3D Human Synthesis with Prior-Guided   Mixture-of-Experts</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ProbTalk3D-Non-Deterministic-Emotion-Controllable-Speech-Driven-3D-Facial-Animation-Synthesis-Using-VQ-VAE"><span class="toc-text">ProbTalk3D: Non-Deterministic Emotion Controllable Speech-Driven 3D   Facial Animation Synthesis Using VQ-VAE</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image: url('https://pica.zhimg.com/v2-5beacc9e107561f5a4f3fc35792c4159.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><!-- hexo injector body_end start --><script data-pjax>
  function butterfly_swiper_injector_config(){
    var parent_div_git = document.getElementById('recent-posts');
    var item_html = '<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="Project/Linly-Talker - GPT-SoVITS/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="Project/Linly-Talker - GPT-SoVITS/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>';
    console.log('已挂载butterfly_swiper')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'undefined'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_swiper_injector_config();
  }
  else if (epage === cpage){
    butterfly_swiper_injector_config();
  }
  </script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script><!-- hexo injector body_end end --></body></html>