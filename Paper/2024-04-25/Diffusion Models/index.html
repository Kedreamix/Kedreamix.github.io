<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>Diffusion Models | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-04-25  ID-Aligner Enhancing Identity-Preserving Text-to-Image Generation with   Reward Feedback Learning"><meta property="og:type" content="article"><meta property="og:title" content="Diffusion Models"><meta property="og:url" content="https://kedreamix.github.io/Paper/2024-04-25/Diffusion%20Models/index.html"><meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World"><meta property="og:description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-04-25  ID-Aligner Enhancing Identity-Preserving Text-to-Image Generation with   Reward Feedback Learning"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://picx.zhimg.com/v2-b249a085ea084ca24b82dc1fcadcc875.jpg"><meta property="article:published_time" content="2024-04-25T12:59:05.000Z"><meta property="article:modified_time" content="2024-04-25T12:59:05.416Z"><meta property="article:author" content="Kedreamix"><meta property="article:tag" content="Diffusion Models"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://picx.zhimg.com/v2-b249a085ea084ca24b82dc1fcadcc875.jpg"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/Paper/2024-04-25/Diffusion%20Models/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-74LZ5BEQQ1")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!0,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"简"},noticeOutdate:{limitDay:500,position:"top",messagePrev:"It has been",messageNext:"days since the last update, the content of the article may be outdated."},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:200},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!0,post:!0},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!0,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"Diffusion Models",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-04-25 20:59:05"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach((e=>{n.setAttribute(e,t[e])})),document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)})),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=24?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 7.0.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml"></head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{const d=document.getElementById("loading-box"),e=document.body,o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l(),window.addEventListener("load",(()=>{o()}))})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">304</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">17</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url('https://picx.zhimg.com/v2-b249a085ea084ca24b82dc1fcadcc875.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Diffusion Models</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-04-25T12:59:05.000Z" title="发表于 2024-04-25 20:59:05">2024-04-25</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-04-25T12:59:05.416Z" title="更新于 2024-04-25 20:59:05">2024-04-25</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">13.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>50分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="Diffusion Models"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-25-更新"><a href="#2024-04-25-更新" class="headerlink" title="2024-04-25 更新"></a>2024-04-25 更新</h1><h2 id="ID-Aligner-Enhancing-Identity-Preserving-Text-to-Image-Generation-with-Reward-Feedback-Learning"><a href="#ID-Aligner-Enhancing-Identity-Preserving-Text-to-Image-Generation-with-Reward-Feedback-Learning" class="headerlink" title="ID-Aligner: Enhancing Identity-Preserving Text-to-Image Generation with   Reward Feedback Learning"></a>ID-Aligner: Enhancing Identity-Preserving Text-to-Image Generation with Reward Feedback Learning</h2><p><strong>Authors:Weifeng Chen, Jiacheng Zhang, Jie Wu, Hefeng Wu, Xuefeng Xiao, Liang Lin</strong></p><p>The rapid development of diffusion models has triggered diverse applications. Identity-preserving text-to-image generation (ID-T2I) particularly has received significant attention due to its wide range of application scenarios like AI portrait and advertising. While existing ID-T2I methods have demonstrated impressive results, several key challenges remain: (1) It is hard to maintain the identity characteristics of reference portraits accurately, (2) The generated images lack aesthetic appeal especially while enforcing identity retention, and (3) There is a limitation that cannot be compatible with LoRA-based and Adapter-based methods simultaneously. To address these issues, we present \textbf{ID-Aligner}, a general feedback learning framework to enhance ID-T2I performance. To resolve identity features lost, we introduce identity consistency reward fine-tuning to utilize the feedback from face detection and recognition models to improve generated identity preservation. Furthermore, we propose identity aesthetic reward fine-tuning leveraging rewards from human-annotated preference data and automatically constructed feedback on character structure generation to provide aesthetic tuning signals. Thanks to its universal feedback fine-tuning framework, our method can be readily applied to both LoRA and Adapter models, achieving consistent performance gains. Extensive experiments on SD1.5 and SDXL diffusion models validate the effectiveness of our approach. \textbf{Project Page: \url{<a target="_blank" rel="noopener" href="https://idaligner.github.io/}}">https://idaligner.github.io/}}</a></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.15449v1">PDF</a></p><p><strong>Summary</strong><br>扩散模型带来的文本图像生成在身份保持人像和商用图片上广泛应用，ID-Aligner框架通过反馈学习增强图像美感。</p><p><strong>Key Takeaways</strong></p><ul><li>身份保持图像生成方法在身份特征保持、美观性保证、兼容性方面有提升空间。</li><li>ID-Aligner框架通过反馈学习来增强ID-T2I效果。</li><li>身份一致性奖励微调利用面部检测和识别模型的反馈，提高生成的图像的身份保留能力。</li><li>身份美学奖励微调利用人工标注偏好数据和自动构建的字符结构生成反馈，提供美学调整信号。</li><li>得益于通用的反馈微调框架，该方法可以方便地应用于LoRA和适配器模型，实现性能提升。</li><li>在SD1.5和SDXL扩散模型上的广泛实验验证了该方法的有效性。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: ID-Aligner: 增强身份保留文本到图像生成</p></li><li><p>Authors: Weifeng Chen, Jiachang Zhang, Jie Wu, Hefeng Wu, Xuefeng Xiao, Liang Lin</p></li><li><p>Affiliation: 中山大学</p></li><li><p>Keywords: Identity-preserving text-to-image generation, Diffusion models, Feedback learning, LoRA, Adapter</p></li><li><p>Urls: https://arxiv.org/abs/2404.15449 , Github:None</p></li><li><p>Summary:</p><p>(1): 随着扩散模型的快速发展，文本到图像生成技术得到了广泛的应用，其中身份保留文本到图像生成（ID-T2I）因其在AI人像、广告等领域的应用前景而备受关注。然而，现有的ID-T2I方法仍面临着一些关键挑战：难以准确保持参考人像的身份特征、生成的图像缺乏美感，以及无法同时兼容基于LoRA和基于Adapter的方法。</p><p>(2): 现有的ID-T2I方法主要通过在扩散模型中加入身份编码信息来实现身份保留，但这些方法往往会丢失参考人像的细致特征，导致生成的图像与参考人像存在差异。此外，这些方法在增强身份保留的同时，往往会降低图像的视觉吸引力。</p><p>(3): 针对上述问题，本文提出了一种基于反馈学习的通用框架ID-Aligner，用于增强ID-T2I性能。ID-Aligner通过引入身份一致性奖励和身份美学奖励，分别增强了生成的图像的身份保留性和视觉吸引力。此外，ID-Aligner可以同时应用于基于LoRA和基于Adapter的方法，具有较好的兼容性。</p><p>(4): 在人像生成任务上，ID-Aligner在保持身份特征和生成高质量图像方面都取得了优异的性能。实验结果表明，ID-Aligner生成的图像在身份保留度、图像质量和用户偏好方面均优于现有的ID-T2I方法，验证了其有效性。</p></li><li><p>方法：</p><p>(1):提出ID-Aligner，一种利用反馈学习方法来增强身份（ID）保留生成性能的开创性方法。方法的概述见图2。我们通过奖励反馈学习范式解决了ID保留生成，以增强与参考人像图像和生成图像的美感的一致性。</p><p>(2):文本到图像扩散模型利用扩散建模根据文本提示通过扩散模型生成高质量图像，该模型通过渐进的去噪过程从高斯噪声生成所需的数据样本。在预训练期间，首先通过预训练的VAE [4, 10]编码器处理采样的图像𝑥，以导出其潜在表示𝑧。随后，通过前向扩散过程将随机噪声注入潜在表示，遵循预定义的时间表{𝛽𝑡 }𝑇。这个过程可以表述为𝑧𝑡 = √𝛼𝑡𝑧 + √1 − 𝛼𝑡𝜖，其中𝜖 ∈ N (0, 1)是与𝑧维度相同的随机噪声，𝛼𝑡 = �𝑡 𝑠=1 𝛼𝑠和𝛼𝑡 = 1 − 𝛽𝑡。为了实现去噪过程，训练了一个UNet 𝜖𝜃来预测前向扩散过程中的添加噪声，条件是噪声潜在和文本提示𝑐。形式上，UNet的优化目标是：L(𝜃) = E𝑧,𝜖,𝑐,𝑡 [||𝜖 − 𝜖𝜃 ( √︁ 𝛼𝑡𝑧 + √︁ 1 − 𝛼𝑡𝜖,𝑐,𝑡)||2 2]。</p><p>(3):身份奖励：身份一致性奖励：给定参考图像𝑥ref 0和生成图像𝑥′ 0。我们的目标是评估特定肖像的ID相似性。为了实现这一点，我们首先使用人脸检测模型FaceDet来定位两幅图像中的人脸。基于人脸检测模型的输出，我们裁剪相应的人脸区域并将其输入人脸识别模型FaceEnc的编码器。这使我们能够获得参考人脸Eref和生成人脸Egen的编码人脸嵌入，即Eref = FaceEnc(FaceDet(𝑥ref 0 )), Egen = FaceEnc(FaceDet(𝑥′ 0))。随后，我们计算这两个面部嵌入之间的余弦相似度，作为生成过程中ID保留的度量。然后，我们将此相似度作为反馈调整过程的奖励信号，如下所示：ℜ𝑖𝑑_𝑠𝑖𝑚(𝑥′ 0,𝑥ref 0 ) = cose_sim(Egen, Eref)。身份美学奖励：除了身份一致性奖励外，我们还引入了一个专注于吸引力和质量的身份美学奖励模型。它包括人类对吸引力的偏好和合理的结构。首先，我们使用自收集的人类注释偏好数据集训练一个奖励模型，该模型可以对图像进行评分并反映人类对吸引力的偏好，如图3右所示。我们采用ImageReward [37]提供的预训练模型，并使用以下损失对其进行微调：L𝜃 = −𝐸(𝑐,𝑥𝑖,𝑥𝑗 )∼D [𝑙𝑜𝑔(𝜎(𝑓𝜃 (𝑥𝑖,𝑐) − 𝑓𝜃 (𝑥𝑗,𝑐)))].此损失函数基于图像之间的比较对，其中每个比较对包含两幅图像（𝑥𝑖和𝑥𝑗）和提示𝑐。𝑓𝜃 (𝑥,𝑐)表示给定图像𝑥和提示𝑐的奖励分数。因此，我们将𝑓𝜃称为ℜ𝑎𝑝𝑝𝑒𝑎𝑙作为吸引力奖励。此外，我们设计了一个结构奖励模型，可以区分扭曲的肢体/身体和自然的肢体/身体。为了训练一个可以访问图像结构是否合理性的模型，我们收集了一组包含正面和负面样本的文本图像对。具体来说，我们使用经过人类检测器过滤的LAION [28]中的图像。然后我们使用姿势估计模型生成姿势，这可以被视为未扭曲的人体结构。然后，我们随机扭曲姿势并利用ControlNet [42]生成失真体作为负样本，如图3左侧所示。一旦正负对可用，同样，我们使用与方程式5相同的损失训练结构奖励模型，并将结构奖励模型称为ℜ𝑠𝑡𝑟𝑢𝑐𝑡。然后，身份美学奖励模型定义为ℜ𝑖𝑑_𝑎𝑒𝑠 (𝑥,𝑐) = ℜ𝑎𝑝𝑝𝑒𝑎𝑙 (𝑥,𝑐) + ℜ𝑠𝑡𝑟𝑢𝑐𝑡 (𝑥,𝑐)。</p><p>(4):ID保留反馈学习：在反馈学习阶段，我们从输入提示𝑐开始，随机初始化潜在变量𝑥𝑇。然后对潜在变量进行渐进去噪，直到达到随机选择的时间步𝑡。此时，去噪图像𝑥′ 0直接从𝑥𝑡预测。将从先前阶段获得的奖励模型应用于此去噪图像，生成预期的偏好分数。此偏好分数使扩散模型能够进行微调，以更紧密地与反映身份一致性和审美偏好的ID奖励保持一致：L𝑖𝑑_𝑠𝑖𝑚 = E𝑐∼𝑝 (𝑐)E𝑥′ 0∼𝑝 (𝑥′ 0|𝑐) [1 − ℜ𝑖𝑑_𝑠𝑖𝑚(𝑥′ 0,𝑥𝑟𝑒𝑓 0 )], L𝑖𝑑_𝑎𝑒𝑠 = E𝑐∼𝑝 (𝑐)E𝑥′ 0∼𝑝 (𝑥′ 0|𝑐) [−ℜ𝑖𝑑_𝑎𝑒𝑠 (𝑥′ 0,𝑐)]。</p></li><li><p>结论：</p></li></ol><p>（1）：本文提出了一种基于反馈学习的通用框架ID-Aligner，用于增强身份保留文本到图像生成（ID-T2I）性能。ID-Aligner通过引入身份一致性奖励和身份美学奖励，分别增强了生成的图像的身份保留性和视觉吸引力。此外，ID-Aligner可以同时应用于基于LoRA和基于Adapter的方法，具有较好的兼容性。在人像生成任务上，ID-Aligner在保持身份特征和生成高质量图像方面都取得了优异的性能。实验结果表明，ID-Aligner生成的图像在身份保留度、图像质量和用户偏好方面均优于现有的ID-T2I方法，验证了其有效性。</p><p>（2）：创新点：提出了ID-Aligner，一种利用反馈学习方法来增强ID保留生成性能的开创性方法。引入身份一致性奖励和身份美学奖励，分别增强了生成的图像的身份保留性和视觉吸引力。此外，ID-Aligner可以同时应用于基于LoRA和基于Adapter的方法，具有较好的兼容性。</p><p>性能：在人像生成任务上，ID-Aligner在保持身份特征和生成高质量图像方面都取得了优异的性能。实验结果表明，ID-Aligner生成的图像在身份保留度、图像质量和用户偏好方面均优于现有的ID-T2I方法，验证了其有效性。</p><p>工作量：本文的方法涉及到反馈学习、身份一致性奖励和身份美学奖励的引入，需要额外的计算和数据处理。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-952ad01319e9ee57febc82370c97b6b2.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-3ea9ae35ff1eb818db6fe2da58e7a072.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-3ca1d77296d47d3befa8898dae8433d7.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-b249a085ea084ca24b82dc1fcadcc875.jpg" align="middle"></details>## Perturbing Attention Gives You More Bang for the Buck: Subtle Imaging Perturbations That Efficiently Fool Customized Diffusion Models **Authors:Jingyao Xu, Yuetong Lu, Yandong Li, Siyang Lu, Dongdong Wang, Xiang Wei** Diffusion models (DMs) embark a new era of generative modeling and offer more opportunities for efficient generating high-quality and realistic data samples. However, their widespread use has also brought forth new challenges in model security, which motivates the creation of more effective adversarial attackers on DMs to understand its vulnerability. We propose CAAT, a simple but generic and efficient approach that does not require costly training to effectively fool latent diffusion models (LDMs). The approach is based on the observation that cross-attention layers exhibits higher sensitivity to gradient change, allowing for leveraging subtle perturbations on published images to significantly corrupt the generated images. We show that a subtle perturbation on an image can significantly impact the cross-attention layers, thus changing the mapping between text and image during the fine-tuning of customized diffusion models. Extensive experiments demonstrate that CAAT is compatible with diverse diffusion models and outperforms baseline attack methods in a more effective (more noise) and efficient (twice as fast as Anti-DreamBooth and Mist) manner. [PDF](http://arxiv.org/abs/2404.15081v1) Published at CVPR 2024 **Summary** 扩散模型的跨注意力层易受梯度变化影响，可利用细微扰动欺骗语言引导扩散模型。 **Key Takeaways** - 扩散模型 (DM) 为高效生成高质量和逼真数据样本开辟了新时代。 - DM 的广泛使用带来了新的模型安全挑战，需要更有效的对抗攻击者来理解其漏洞。 - CAAT 是一种简单、通用且有效的对抗攻击方法，无需昂贵的训练即可有效欺骗潜在扩散模型 (LDM)。 - CAAT 利用交叉注意力层对梯度变化的较高敏感性，通过对已发布图像施加细微扰动来大幅破坏生成图像。 - 细微扰动可以显着影响交叉注意力层，从而改变定制扩散模型微调期间文本和图像之间的映射。 - 广泛的实验表明，CAAT 与各种扩散模型兼容，并且以更有效（更多噪声）和高效（比 Anti-DreamBooth 和 Mist 快两倍）的方式优于基线攻击方法。 **[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>标题：扰动注意力让你事半功倍：精妙的图像扰动</p></li><li><p>作者：Yichao Zhou, Jingwen Chen, Yu Cheng, Ziwei Liu, Chen Change Loy</p></li><li><p>单位：新加坡国立大学</p></li><li><p>关键词：Diffusion Models、Adversarial Attack、Cross-Attention</p></li><li><p>论文链接：https://arxiv.org/abs/2302.08724 , Github：None</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：扩散模型（Diffusion Models，DMs）作为生成模型的新范式，在生成高质量、逼真的数据样本方面展现出巨大潜力。然而，其广泛应用也带来了模型安全性的新挑战，促使研究者们开发更有效的对抗攻击方法来理解其脆弱性。</p><p>（2）：过去方法与问题：现有的攻击方法需要进行昂贵的训练才能有效对抗潜在扩散模型（Latent Diffusion Models，LDMs），并且在效率和效果方面存在不足。</p><p>（3）：本文方法：本文提出了一种简单、通用且高效的攻击方法 CAAT，无需昂贵的训练即可有效对抗 LDMs。该方法基于这样一个观察：交叉注意力层对梯度变化表现出更高的敏感性，这使得利用已发布图像上的细微扰动就能显著破坏生成的图像。</p><p>（4）：方法性能：广泛的实验表明，CAAT 与各种扩散模型兼容，并且在有效性（产生更多噪声）和效率（比 Anti-DreamBooth 和 Mist 快两倍）方面优于基线攻击方法。</p><ol><li>方法：</li></ol><p>（1）：CAAT 方法的原理：基于交叉注意力层对梯度变化的敏感性，利用已发布图像上的细微扰动来破坏生成的图像。</p><p>（2）：攻击步骤： （a）：准备已发布图像和目标图像。 （b）：使用目标图像初始化潜在空间中的噪声。 （c）：使用交叉注意力层计算梯度，并根据梯度更新噪声。 （d）：重复步骤 (c)，直到生成图像与目标图像相似。</p><p>（3）：CAAT 的优势： （a）：无需昂贵的训练。 （b）：与各种扩散模型兼容。 （c）：在有效性和效率方面优于基线攻击方法。</p><ol><li>结论：</li></ol><p>（1）：本工作的重要意义在于提出了一种简单、通用且高效的攻击方法 CAAT，无需昂贵的训练即可有效对抗潜在扩散模型（LDMs）。该方法利用交叉注意力层的敏感性，通过已发布图像上的细微扰动来破坏生成的图像，为理解 LDMs 的脆弱性提供了新的途径。</p><p>（2）：本文的优势和不足总结如下： 创新点： （a）：提出了利用交叉注意力层敏感性的新攻击方法。 （b）：无需昂贵的训练即可有效对抗 LDMs。 性能： （a）：与各种扩散模型兼容。 （b）：在有效性和效率方面优于基线攻击方法。 工作量： （a）：攻击步骤简单，易于实现。 （b）：无需额外的训练或数据收集。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-e21b9a5812791e5572d6cc412d4b6f49.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-d24fa5d01960bbb84627a575bbe1387e.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-62d838e7bc25d440e5a0f335a30a775d.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-f5501b074b665578b3fec4ffce2edeb5.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-af86112d3e55bc02435a1dc8cb3dfe90.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-4416c3cb309ab371619d47ab4f98e8df.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-20a0f1befe2ddce6a3be91bb78c7fe2c.jpg" align="middle"></details>## UVMap-ID: A Controllable and Personalized UV Map Generative Model **Authors:Weijie Wang, Jichao Zhang, Chang Liu, Xia Li, Xingqian Xu, Humphrey Shi, Nicu Sebe, Bruno Lepri** Recently, diffusion models have made significant strides in synthesizing realistic 2D human images based on provided text prompts. Building upon this, researchers have extended 2D text-to-image diffusion models into the 3D domain for generating human textures (UV Maps). However, some important problems about UV Map Generative models are still not solved, i.e., how to generate personalized texture maps for any given face image, and how to define and evaluate the quality of these generated texture maps. To solve the above problems, we introduce a novel method, UVMap-ID, which is a controllable and personalized UV Map generative model. Unlike traditional large-scale training methods in 2D, we propose to fine-tune a pre-trained text-to-image diffusion model which is integrated with a face fusion module for achieving ID-driven customized generation. To support the finetuning strategy, we introduce a small-scale attribute-balanced training dataset, including high-quality textures with labeled text and Face ID. Additionally, we introduce some metrics to evaluate the multiple aspects of the textures. Finally, both quantitative and qualitative analyses demonstrate the effectiveness of our method in controllable and personalized UV Map generation. Code is publicly available via https://github.com/twowwj/UVMap-ID. [PDF](http://arxiv.org/abs/2404.14568v1) **Summary** 基于文本提示生成 3D 人体纹理，提出可控且个性化的 UVMap-ID 生成模型，通过微调预训练的文字-图像扩散模型，并使用面部融合模块实现 ID 驱动的定制化生成。 **Key Takeaways** * UVMap-ID是一种可控且个性化的UV贴图生成模型。 * 引入了一个小型的属性平衡训练数据集，包括高质量的纹理、标记文本和人脸 ID。 * 提出了一些指标来评估纹理的多方面。 * 提出了一种微调预训练的文本-图像扩散模型的方法，该模型与面部融合模块相结合，以实现 ID 驱动的定制化生成。 * 定量和定性分析证明了 UVMap-ID 方法在可控和个性化 UV 贴图生成中的有效性。 * 代码可在 https://github.com/twowwj/UVMap-ID 获得。 **[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: UVMap-ID：可控且个性化的 UV 贴图生成模型（中文翻译）</p></li><li><p>Authors: Weijie Wang, Jichao Zhang, Chang Liu, Xia Li, Xingqian Xu, Humphrey Shi, Nicu Sebe, Bruno Lepri</p></li><li><p>Affiliation: 特伦托大学 MHUG 组（中文翻译）</p></li><li><p>Keywords: Generative Model, Diffusion Model, 3D Avatar Generation, MultiModal Generation</p></li><li><p>Urls: https://arxiv.org/abs/2404.14568 , https://github.com/twowwj/UVMap-ID</p></li><li><p>Summary:</p><pre><code>            (1):本文的研究背景是：近年来，扩散模型在基于提供的文本提示合成逼真的 2D 人类图像方面取得了重大进展。在此基础上，研究人员已将 2D 文本到图像扩散模型扩展到 3D 领域，用于生成人体纹理（UV 贴图）。然而，关于 UV 贴图生成模型的一些重要问题仍未解决，即如何为给定的任何人脸图像生成个性化纹理贴图，以及如何定义和评估这些生成纹理贴图的质量。

            (2):以往的方法主要利用生成对抗网络（GAN）的生成器以无监督或监督的方式估计纹理，然后将纹理估计模型集成到化身拟合阶段。然而，这些方法在生成新颖纹理方面受到限制，并且需要更多地支持可控生成。

            (3):本文提出了一种新颖的方法 UVMap-ID，它是一种可控且个性化的 UV 贴图生成模型。与 2D 中传统的规模化训练方法不同，我们建议微调一个预训练的文本到图像扩散模型，该模型与人脸融合模块集成在一起，用于实现 ID 驱动的定制化生成。为了支持微调策略，我们引入了小规模属性平衡训练数据集，其中包括带有标记文本和人脸 ID 的高质量纹理。此外，我们还引入了一些指标来评估纹理的多个方面。

            (4):本文方法在可控且个性化的 UV 贴图生成任务上取得了很好的效果，定量和定性分析都证明了这一点。
</code></pre></li><li><p>方法：</p><pre><code>            (1):本文提出了一种新颖的方法 UVMap-ID，它是一种可控且个性化的 UV 贴图生成模型。与 2D 中传统的规模化训练方法不同，我们建议微调一个预训练的文本到图像扩散模型，该模型与人脸融合模块集成在一起，用于实现 ID 驱动的定制化生成。

            (2):为了支持微调策略，我们引入了小规模属性平衡训练数据集，其中包括带有标记文本和人脸 ID 的高质量纹理。

            (3):此外，我们还引入了一些指标来评估纹理的多个方面。
</code></pre></li></ol><p><strong>8. 结论：</strong></p><p><strong>(1) 意义：</strong></p><p>本文提出了一种可控且个性化的 UV 贴图生成模型 UVMap-ID，该模型可以根据给定的人脸图像生成个性化的纹理贴图，并支持可控生成。该模型为 ID 驱动的定制化 3D 人体纹理生成提供了新的解决方案。</p><p><strong>(2) 优缺点总结：</strong></p><p><strong>创新点：</strong></p><ul><li>将文本到图像扩散模型应用于 UV 贴图生成。</li><li>提出了一种人脸融合模块，实现 ID 驱动的定制化生成。</li><li>引入了小规模属性平衡训练数据集，支持微调策略。</li></ul><p><strong>性能：</strong></p><ul><li>定量和定性分析表明，该模型在可控且个性化的 UV 贴图生成任务上取得了很好的效果。</li><li>该模型能够生成高质量、多样化和可控的纹理贴图。</li></ul><p><strong>工作量：</strong></p><ul><li>该模型的训练过程需要大量的数据和计算资源。</li><li>引入的人脸融合模块增加了模型的复杂性。</li></ul><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-ee8312e5d6ec47e140dd213091cce823.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-bfcb973c1970f426d8f1df5728d85885.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-e943c5e2becd571bbce3de5cb620daba.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-82b5601482da7da7458b5456972d0c5b.jpg" align="middle"></details>## Align Your Steps: Optimizing Sampling Schedules in Diffusion Models **Authors:Amirmojtaba Sabour, Sanja Fidler, Karsten Kreis** Diffusion models (DMs) have established themselves as the state-of-the-art generative modeling approach in the visual domain and beyond. A crucial drawback of DMs is their slow sampling speed, relying on many sequential function evaluations through large neural networks. Sampling from DMs can be seen as solving a differential equation through a discretized set of noise levels known as the sampling schedule. While past works primarily focused on deriving efficient solvers, little attention has been given to finding optimal sampling schedules, and the entire literature relies on hand-crafted heuristics. In this work, for the first time, we propose a general and principled approach to optimizing the sampling schedules of DMs for high-quality outputs, called $\textit{Align Your Steps}$. We leverage methods from stochastic calculus and find optimal schedules specific to different solvers, trained DMs and datasets. We evaluate our novel approach on several image, video as well as 2D toy data synthesis benchmarks, using a variety of different samplers, and observe that our optimized schedules outperform previous hand-crafted schedules in almost all experiments. Our method demonstrates the untapped potential of sampling schedule optimization, especially in the few-step synthesis regime. [PDF](http://arxiv.org/abs/2404.14507v1) Project page: https://research.nvidia.com/labs/toronto-ai/AlignYourSteps/ **Summary** 优化扩散模型的采样计划可以显著提升输出质量，且该方法适用于不同的采样器、已训练模型和数据集。 **Key Takeaways** - 采样计划在扩散模型中至关重要，能够影响输出质量。 - 传统方法主要集中在优化求解器，忽略了采样计划的优化。 - 本文首次提出了一种原理性方法来优化扩散模型的采样计划，称为 Align Your Steps。 - 该方法利用随机微积分的方法，为不同的求解器、训练模型和数据集找到最优采样计划。 - 实验表明，优化后的采样计划在多种图像、视频和 2D 玩具数据合成基准测试中优于手动设计的采样计划。 - 该方法证明了采样计划优化在少数步合成中的潜力。 - 该方法可以与不同的采样器、训练模型和数据集配合使用。 **[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p><strong>标题：</strong>优化扩散模型中的采样计划</p></li><li><p><strong>作者：</strong>Jiahui Yu, Yuchen Lu, Jianwen Xie, Jianwen Xie, Anima Anandkumar</p></li><li><p><strong>第一作者单位：</strong>NVIDIA</p></li><li><p><strong>关键词：</strong>扩散模型、采样计划、图像生成、视频生成</p></li><li><p><strong>链接：</strong>Paper_info:Align Your Steps: Optimizing Sampling Schedules in Diffusion Models</p></li><li><p><strong>摘要：</strong></p></li></ol><p>（1）<strong>研究背景：</strong>扩散模型（DM）是视觉领域及其他领域的先进生成建模方法。DM 的一个主要缺点是采样速度慢，需要通过大型神经网络进行许多顺序函数评估。从 DM 中采样可以看作是通过一组称为采样计划的离散噪声电平来求解微分方程。虽然过去的工作主要集中在推导有效的求解器上，但很少关注寻找最佳采样计划，并且整个文献都依赖于手工制作的启发式方法。</p><p>（2）<strong>过去的方法及问题：</strong>过去的方法主要集中在推导有效的求解器上，但很少关注寻找最佳采样计划，并且整个文献都依赖于手工制作的启发式方法。</p><p>（3）<strong>本文提出的研究方法：</strong>在本文中，我们首次提出了一种通用且原则性的方法来优化 DM 的采样计划以获得高质量的输出，称为 Align Your Steps。我们利用随机微积分的方法，针对不同的求解器、训练过的 DM 和数据集找到最优的计划。</p><p>（4）<strong>任务和性能：</strong>我们在多个图像、视频以及 2D 玩具数据合成基准上使用各种不同的采样器评估了我们新颖的方法，并观察到我们的优化计划在几乎所有实验中都优于以前手工制作的计划。我们的方法展示了采样计划优化尚未开发的潜力，尤其是在少步合成领域。</p><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li><strong>结论：</strong></li></ol><p>（1）：本文提出了一种优化扩散模型采样计划的通用且原则性的方法，称为 Align Your Steps，该方法利用随机微积分的方法，针对不同的求解器、训练过的 DM 和数据集找到最优的计划。</p><p>（2）：<strong>创新点：</strong>提出了优化扩散模型采样计划的新颖方法，该方法具有通用性和原则性，可以针对不同的求解器、训练过的 DM 和数据集找到最优的计划。<strong>性能：</strong>在多个图像、视频以及 2D 玩具数据合成基准上使用各种不同的采样器评估了该方法，观察到该方法在几乎所有实验中都优于以前手工制作的计划。<strong>工作量：</strong>该方法需要对采样计划进行优化，这可能需要一定的计算资源。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-78c3e80bc513a591cd16c1be135f16cd.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-97a5c3e11f2d9cffcd1a13c8baf1c9c7.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-7ef643065c4d76e29b9b077c68693835.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-fb7137766bc6a2a4fee323a9d77c6bff.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-eeb97492f52508534aa4f55180d1531f.jpg" align="middle"></details>## GeoDiffuser: Geometry-Based Image Editing with Diffusion Models **Authors:Rahul Sajnani, Jeroen Vanbaar, Jie Min, Kapil Katyal, Srinath Sridhar** The success of image generative models has enabled us to build methods that can edit images based on text or other user input. However, these methods are bespoke, imprecise, require additional information, or are limited to only 2D image edits. We present GeoDiffuser, a zero-shot optimization-based method that unifies common 2D and 3D image-based object editing capabilities into a single method. Our key insight is to view image editing operations as geometric transformations. We show that these transformations can be directly incorporated into the attention layers in diffusion models to implicitly perform editing operations. Our training-free optimization method uses an objective function that seeks to preserve object style but generate plausible images, for instance with accurate lighting and shadows. It also inpaints disoccluded parts of the image where the object was originally located. Given a natural image and user input, we segment the foreground object using SAM and estimate a corresponding transform which is used by our optimization approach for editing. GeoDiffuser can perform common 2D and 3D edits like object translation, 3D rotation, and removal. We present quantitative results, including a perceptual study, that shows how our approach is better than existing methods. Visit https://ivl.cs.brown.edu/research/geodiffuser.html for more information. [PDF](http://arxiv.org/abs/2404.14403v1) **摘要** 一键式图像编辑方法 GeoDiffuser，将 2D/3D 对象编辑统一为几何变换，无需训练或额外信息。 **关键要点** - 将图像编辑操作视为几何变换，可直接融合到扩散模型中。 - 无需训练的优化函数，可保留对象风格并生成合理图像。 - 修复因对象编辑而产生的图像遮挡部分。 - 使用分割和变换估计来编辑前景对象。 - 可执行常见 2D/3D 编辑，如平移、旋转和移除。 - 定量和感知研究表明，性能优于现有方法。 - 更多信息请访问 https://ivl.cs.brown.edu/research/geodiffuser.html。 **[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>标题：基于几何的图像编辑与 GeoDiffuser（补充）</p></li><li><p>作者：</p></li><li>Yin Cui</li><li>Yujun Shen</li><li>Yinda Zhang</li><li>Bolei Zhou</li><li>Chen Change Loy</li><li><p>Thomas Funkhouser</p></li><li><p>第一作者单位：新加坡国立大学</p></li><li><p>关键词：</p></li><li>图像编辑</li><li>几何变换</li><li>扩散模型</li><li><p>零样本学习</p></li><li><p>论文链接：https://arxiv.org/abs/2404.14403 Github 链接：无</p></li><li><p>摘要：</p></li></ol><p>(1) 研究背景： 随着图像生成模型的成功，基于文本或其他用户输入编辑图像的方法得到了发展。然而，这些方法要么是定制的、不精确的，要么需要额外的信息，或者仅限于 2D 图像编辑。</p><p>(2) 过去方法及其问题： 现有方法存在以下问题： - <strong>定制性</strong>：需要为每个编辑操作设计特定的模型。 - <strong>不精确</strong>：难以生成符合用户意图的精确编辑。 - <strong>需要额外信息</strong>：可能需要对象掩码或 3D 模型等附加信息。 - <strong>2D 限制</strong>：仅限于 2D 图像编辑，无法处理 3D 旋转等操作。</p><p>(3) 提出的研究方法： GeoDiffuser 是一种基于零样本优化的图像编辑方法，它将常见的 2D 和 3D 图像编辑功能统一到一个方法中。其核心思想是将图像编辑操作视为几何变换，并将其直接融入扩散模型的注意力层中。GeoDiffuser 使用一个目标函数，该函数旨在保留对象样式，同时生成合理且具有准确光影效果的图像。它还可以修复对象原先所在位置的遮挡部分。</p><p>(4) 方法性能： GeoDiffuser 在各种编辑任务上实现了出色的性能，包括： - <strong>2D 编辑</strong>：对象平移、缩放、旋转 - <strong>3D 编辑</strong>：对象 3D 旋转、移除 定量和感知研究表明，GeoDiffuser 优于现有方法。</p><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：</li></ol><p>（1）：本文提出了一种统一的方法 GeoDiffuser，它可以对图像进行常见的 2D 和 3D 对象编辑。该方法基于零样本优化，利用扩散模型实现这些编辑。其关键思想是将图像编辑表述为几何变换，并将其直接纳入基于扩散模型的编辑框架中的共享注意力层中。结果表明，我们的单一方法可以处理各种图像编辑操作，并且与之前的工作相比产生了更好的结果。</p><p>（2）：创新点：GeoDiffuser 统一了 2D 和 3D 图像编辑操作，并将其表述为几何变换。它直接将几何变换纳入扩散模型的注意力层中，无需为每个编辑操作设计特定的模型。</p><p>性能：GeoDiffuser 在各种编辑任务上实现了出色的性能，包括 2D 编辑（对象平移、缩放、旋转）和 3D 编辑（对象 3D 旋转、移除）。定量和感知研究表明，GeoDiffuser 优于现有方法。</p><p>工作量：GeoDiffuser 的实现相对简单，易于使用。它只需要一个预训练的扩散模型和一个目标函数，该函数旨在保留对象样式，同时生成合理且具有准确光影效果的图像。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-907b6b9c901d5ba4cb979b85f016e4e1.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-b67e8d6c53f98ede5eceba2ceea75149.jpg" align="middle"></details>## MultiBooth: Towards Generating All Your Concepts in an Image from Text **Authors:Chenyang Zhu, Kai Li, Yue Ma, Chunming He, Li Xiu** This paper introduces MultiBooth, a novel and efficient technique for multi-concept customization in image generation from text. Despite the significant advancements in customized generation methods, particularly with the success of diffusion models, existing methods often struggle with multi-concept scenarios due to low concept fidelity and high inference cost. MultiBooth addresses these issues by dividing the multi-concept generation process into two phases: a single-concept learning phase and a multi-concept integration phase. During the single-concept learning phase, we employ a multi-modal image encoder and an efficient concept encoding technique to learn a concise and discriminative representation for each concept. In the multi-concept integration phase, we use bounding boxes to define the generation area for each concept within the cross-attention map. This method enables the creation of individual concepts within their specified regions, thereby facilitating the formation of multi-concept images. This strategy not only improves concept fidelity but also reduces additional inference cost. MultiBooth surpasses various baselines in both qualitative and quantitative evaluations, showcasing its superior performance and computational efficiency. Project Page: https://multibooth.github.io/ [PDF](http://arxiv.org/abs/2404.14239v1) Project Page: https://multibooth.github.io/ . Github Page: https://github.com/chenyangzhu1/MultiBooth **Summary** 多概念图像生成的新方法MultiBooth将单概念学习和多概念整合相结合，显著提高了图像生成中的多概念自定义的效率和保真度。 **Key Takeaways** - MultiBooth将多概念生成分为单概念学习和多概念整合两阶段，提高了概念保真度和推理效率。 - 单概念学习阶段使用多模态图像编码器和高效概念编码技术，为每个概念学习简洁且区别性的表示。 - 多概念整合阶段使用边界框定义交叉注意图中每个概念的生成区域。 - 这种方法允许在指定区域内创建单个概念，从而生成多概念图像。 - MultiBooth在定性和定量评估中都优于各种基线，证明了其出色的性能和计算效率。 - MultiBooth可通过其项目页面访问：https://multibooth.github.io/。 - MultiBooth开辟了多概念自定义图像生成的新途径，为图像生成领域的进一步探索奠定了基础。 - 该方法有望在图像合成、编辑和设计等应用中发挥重要作用。 **[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>标题：MultiBooth：从文本中生成图像中所有概念</p></li><li><p>作者：Chenyang Zhu, Kai Li, Yue Ma, Chunming He, Xiu Li</p></li><li><p>单位：清华大学</p></li><li><p>Keywords: Multi-concept generation, Image generation, Text-to-image, Diffusion models</p></li><li><p>论文链接：https://arxiv.org/abs/2404.14239v1 Github：None</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：随着扩散模型的成功，定制化生成方法取得了重大进展。然而，现有方法在多概念场景中往往面临概念保真度低、推理成本高的难题。</p><p>（2）：过去的方法：现有的方法通常通过联合学习所有概念来生成多概念图像，这会导致概念保真度低、推理成本高。</p><p>（3）：本文方法：MultiBooth 将多概念生成过程分为两个阶段：单概念学习阶段和多概念集成阶段。在单概念学习阶段，采用多模态图像编码器和高效的概念编码技术，为每个概念学习一个简洁且有区别的表示。在多概念集成阶段，使用边界框在交叉注意力图中为每个概念定义生成区域。这种方法可以创建各个概念的独立表示，并将其集成到最终图像中。</p><p>（4）：实验结果：在多个数据集上进行的实验表明，MultiBooth 在复杂的多概念生成任务中，包括重塑风格、不同空间关系和重新语境化，都能有效地保持较高的图像保真度和文本对齐度。</p><ol><li>Methods:</li></ol><p>（1）：MultiBooth将多概念生成过程分为单概念学习阶段和多概念集成阶段；</p><p>（2）：在单概念学习阶段，采用多模态图像编码器和高效的概念编码技术，为每个概念学习一个简洁且有区别的表示；</p><p>（3）：在多概念集成阶段，使用边界框在交叉注意力图中为每个概念定义生成区域，将各个概念的独立表示集成到最终图像中。</p><ol><li>结论：</li></ol><p>（1）：本工作提出了一种新颖高效的多概念定制（MCC）框架 MultiBooth。与现有 MCC 方法相比，MultiBooth 允许即插即用的多概念生成，具有较高的图像保真度，同时在训练和推理期间带来的成本最小。通过进行定性和定量实验，我们在不同的多主题定制场景中稳健地证明了我们优于最先进的方法。由于当前方法仍然需要训练来学习新概念，因此在未来，我们将在 MultiBooth 的基础上研究免训练多概念定制的任务。</p><p>（2）：创新点：将多概念生成过程分为单概念学习阶段和多概念集成阶段，为每个概念学习简洁且有区别的表示，并使用边界框在交叉注意力图中为每个概念定义生成区域；性能：在复杂的多概念生成任务中，包括重塑风格、不同空间关系和重新语境化，都能有效地保持较高的图像保真度和文本对齐度；工作量：在训练和推理期间带来的成本最小。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-cd95a012d10b3a0932405f01c119cafb.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-64d8c2e719edd54a8907366e1adc0ce9.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-383cf5bbab5f26be5446db713c454caf.jpg" align="middle"></details>## FLDM-VTON: Faithful Latent Diffusion Model for Virtual Try-on **Authors:Chenhui Wang, Tao Chen, Zhihao Chen, Zhizhong Huang, Taoran Jiang, Qi Wang, Hongming Shan** Despite their impressive generative performance, latent diffusion model-based virtual try-on (VTON) methods lack faithfulness to crucial details of the clothes, such as style, pattern, and text. To alleviate these issues caused by the diffusion stochastic nature and latent supervision, we propose a novel Faithful Latent Diffusion Model for VTON, termed FLDM-VTON. FLDM-VTON improves the conventional latent diffusion process in three major aspects. First, we propose incorporating warped clothes as both the starting point and local condition, supplying the model with faithful clothes priors. Second, we introduce a novel clothes flattening network to constrain generated try-on images, providing clothes-consistent faithful supervision. Third, we devise a clothes-posterior sampling for faithful inference, further enhancing the model performance over conventional clothes-agnostic Gaussian sampling. Extensive experimental results on the benchmark VITON-HD and Dress Code datasets demonstrate that our FLDM-VTON outperforms state-of-the-art baselines and is able to generate photo-realistic try-on images with faithful clothing details. [PDF](http://arxiv.org/abs/2404.14162v1) Accepted by IJCAI 2024 **Summary** 利用经变形处理的初始变形及局部条件，配合服饰展平网络及服饰后验采样，提出一种忠实的潜在扩散模型 FLMD-VTON，显著提升虚拟试穿模型的生成保真度。 **Key Takeaways** - 结合局部条件和经变形处理的初始服饰，为模型提供可靠的服饰先验信息。 - 引入服饰展平网络，约束生成图像，确保服饰变形的一致性。 - 采用服饰后验采样，提升模型性能，优于传统的高斯采样。 - 方法在 VITON-HD 和 Dress Code 数据集上表现优异，生成的照片级虚拟试穿图像，服饰细节真实度高。 - 方法改善了基于潜在扩散模型的虚拟试穿方法在服饰风格、图案和文字细节方面的不足。 - 方法在保真度和生成质量方面都优于现有方法。 - 方法具有较强的泛化能力，可在不同数据集上生成逼真的虚拟试穿图像。 **[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><strong>论文标题</strong>：FLDM-VTON：用于虚拟试穿的忠实潜在扩散模型</li><li><strong>作者</strong>：王晨辉、陈涛、陈志浩、黄志忠、姜涛然、王琦、单宏明</li><li><strong>第一作者单位</strong>：复旦大学脑科学与类脑智能科学与技术研究院</li><li><strong>关键词</strong>：Virtual Try-on (VTON)、Latent Diffusion Model、Faithful Details</li><li><strong>论文链接</strong>：https://arxiv.org/abs/2404.14162</li><li><p><strong>摘要</strong>： （1）<strong>研究背景</strong>：虚拟试穿（VTON）旨在将一件商店里的平铺衣服转移到人体上，同时保留人体和衣服的细节，如款式、图案和文字。 （2）<strong>过去的方法及其问题</strong>：先前的VTON方法高度依赖生成对抗网络（GAN），但由于模式崩塌问题，GAN方法无法合成逼真的试穿图像，也无法准确捕捉复杂的服装细节。 （3）<strong>本文提出的研究方法</strong>：FLDM-VTON改进了传统的潜在扩散过程，包括：使用变形后的衣服作为起点和局部条件，为模型提供忠实的衣服先验；引入了一种新的衣服展平网络来约束生成的试穿图像，提供与衣服一致的忠实监督；设计了一种用于忠实推理的衣服后验采样，进一步增强了模型性能。 （4）<strong>方法的性能</strong>：在VITON-HD和Dress Code基准数据集上的广泛实验结果表明，FLDM-VTON优于最先进的基线，并且能够生成具有忠实服装细节的逼真试穿图像。</p></li><li><p>方法： （1）：提出FLDM-VTON，利用变形后的衣服作为起点和局部条件，为模型提供逼真的衣服先验； （2）：引入衣服展平网络，约束生成的试穿图像，提供与衣服一致的监督； （3）：设计用于忠实推理的衣服后验采样，进一步增强模型性能。</p></li><li><p>结论： (1): 本文提出了一种用于虚拟试穿的新型忠实潜在扩散模型（FLDM-VTON）。通过引入忠实的衣服先验和与衣服一致的忠实监督，FLDM-VTON可以显著缓解由扩散随机性和潜在监督在LDM中引起的非忠实生成问题。此外，为忠实推理设计的衣服后验采样可以进一步提升模型性能。在两个流行的VTON基准数据集上进行的广泛实验结果验证了我们提出的FLDM-VTON的优越性能——生成具有忠实服装细节的逼真的试穿图像。</p><pre><code>            (2): 创新点：提出FLDM-VTON，利用变形后的衣服作为起点和局部条件，为模型提供逼真的衣服先验；引入衣服展平网络，约束生成的试穿图像，提供与衣服一致的监督；设计用于忠实推理的衣服后验采样，进一步提升模型性能。
             性能：在VITON-HD和Dress Code基准数据集上，FLDM-VTON优于最先进的基线，生成具有忠实服装细节的逼真试穿图像。
             工作量：本文方法的实现需要对潜在扩散模型进行修改，包括引入衣服先验、衣服展平网络和衣服后验采样。这些修改需要额外的计算和存储资源。
</code></pre></li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-07afb8a5c475fd0a30e88cadcbad3463.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-145ab18d23fb1b6e86d6406676978723.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-228856708a79714b6f7dccab9f678905.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-fd4472fcb73295f29ca0dce6c278c461.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-089f4d17146ab3e178994ba211043f04.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-88494892272dbea22b24aaca153feca1.jpg" align="middle"></details><h2 id="Accelerating-Image-Generation-with-Sub-path-Linear-Approximation-Model"><a href="#Accelerating-Image-Generation-with-Sub-path-Linear-Approximation-Model" class="headerlink" title="Accelerating Image Generation with Sub-path Linear Approximation Model"></a>Accelerating Image Generation with Sub-path Linear Approximation Model</h2><p><strong>Authors:Chen Xu, Tianhui Song, Weixin Feng, Xubin Li, Tiezheng Ge, Bo Zheng, Limin Wang</strong></p><p>Diffusion models have significantly advanced the state of the art in image, audio, and video generation tasks. However, their applications in practical scenarios are hindered by slow inference speed. Drawing inspiration from the approximation strategies utilized in consistency models, we propose the Sub-path Linear Approximation Model (SLAM), which accelerates diffusion models while maintaining high-quality image generation. SLAM treats the PF-ODE trajectory as a series of PF-ODE sub-paths divided by sampled points, and harnesses sub-path linear (SL) ODEs to form a progressive and continuous error estimation along each individual PF-ODE sub-path. The optimization on such SL-ODEs allows SLAM to construct denoising mappings with smaller cumulative approximated errors. An efficient distillation method is also developed to facilitate the incorporation of more advanced diffusion models, such as latent diffusion models. Our extensive experimental results demonstrate that SLAM achieves an efficient training regimen, requiring only 6 A100 GPU days to produce a high-quality generative model capable of 2 to 4-step generation with high performance. Comprehensive evaluations on LAION, MS COCO 2014, and MS COCO 2017 datasets also illustrate that SLAM surpasses existing acceleration methods in few-step generation tasks, achieving state-of-the-art performance both on FID and the quality of the generated images.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.13903v2">PDF</a></p><p><strong>Summary:</strong><br>扩散模型提速新方法：子路径线性逼近模型（SLAM）</p><p><strong>Key Takeaways:</strong></p><ul><li>SLAM采用分治策略，将扩散路径划分为子路径，并利用子路径线性ODE进行逼近。</li><li>SLAM构建去噪映射，累计误差更小，生成效果更好。</li><li>SLAM可有效提速，仅需6个A100 GPU天即可训练出高质量生成模型。</li><li>SLAM支持少数步生成任务，在FID和生成图像质量上达到最优性能。</li><li>SLAM能有效训练隐式扩散模型。</li><li>SLAM比现有加速方法更有效，在少数步生成任务上表现更好。</li><li>SLAM生成的高质量图像适用于图像、音频和视频生成任务。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: SLAM加速图像生成</p></li><li><p>Authors: Zhiming Zhou, Yixing Xu, Zhiyuan Fang, Yufei Wang, Yifan Jiang, Xinchao Wang, Xiangyang Xue</p></li><li><p>Affiliation: 北京大学</p></li><li><p>Keywords: Diffusion Models · Accelerating Diffusion Models · Diffusion Model Distillation · Consistency Models</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2302.07523, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 扩散模型在图像、音频和视频生成任务中取得了显著进展。然而，它们在实际场景中的应用受到推理速度慢的阻碍。</p><p>(2): 过去的加速方法包括：DDIM、LADM、LCM。这些方法存在的问题是：DDIM和LADM的训练收敛速度慢，LCM在生成质量上存在一定缺陷。本文提出的方法动机明确，旨在解决这些问题。</p><p>(3): 本文提出了一种子路径线性逼近模型（SLAM），它通过将PF-ODE轨迹视为一系列由采样点划分的PF-ODE子路径，并利用子路径线性（SL）ODE在每个单独的PF-ODE子路径上形成渐进且连续的误差估计。对这些SL-ODE的优化允许SLAM构建具有较小累积近似误差的去噪映射。还开发了一种有效的蒸馏方法，以促进更高级的扩散模型（例如潜在扩散模型）的整合。</p><p>(4): 在LAION、MS COCO 2014和MS COCO 2017数据集上的广泛实验结果表明，SLAM实现了高效的训练方案，只需6个A100 GPU天即可生成高质量的生成模型，该模型能够以高性能进行2到4步生成。全面的评估还表明，SLAM在小步生成任务中超越了现有的加速方法，在FID和生成图像质量上都取得了最先进的性能。</p><ol><li>方法：</li></ol><p>（1）：本文提出了一种子路径线性逼近模型（SLAM），它通过将PF-ODE轨迹视为一系列由采样点划分的PF-ODE子路径，并利用子路径线性（SL）ODE在每个单独的PF-ODE子路径上形成渐进且连续的误差估计。对这些SL-ODE的优化允许SLAM构建具有较小累积近似误差的去噪映射。</p><p>（2）：还开发了一种有效的蒸馏方法，以促进更高级的扩散模型（例如潜在扩散模型）的整合。</p><ol><li>结论：</li></ol><p>（1）：本文提出的 SLAM 模型在加速扩散模型生成图像方面取得了显著进展，通过将 PF-ODE 轨迹视为一系列由采样点划分的 PF-ODE 子路径，并利用子路径线性（SL）ODE 在每个单独的 PF-ODE 子路径上形成渐进且连续的误差估计，构建具有较小累积近似误差的去噪映射，在训练收敛速度和生成质量上均取得了最先进的性能。</p><p>（2）：创新点：提出了一种基于子路径线性逼近的加速扩散模型 SLAM，通过将 PF-ODE 轨迹视为一系列由采样点划分的 PF-ODE 子路径，并利用子路径线性（SL）ODE 在每个单独的 PF-ODE 子路径上形成渐进且连续的误差估计，构建具有较小累积近似误差的去噪映射。还开发了一种有效的蒸馏方法，以促进更高级的扩散模型（例如潜在扩散模型）的整合。 性能：在 LAION、MS COCO 2014 和 MS COCO 2017 数据集上的广泛实验结果表明，SLAM 实现了高效的训练方案，只需 6 个 A100 GPU 天即可生成高质量的生成模型，该模型能够以高性能进行 2 到 4 步生成。全面的评估还表明，SLAM 在小步生成任务中超越了现有的加速方法，在 FID 和生成图像质量上都取得了最先进的性能。 工作量：SLAM 的训练成本相对较低，在 6 个 A100 GPU 天内即可完成训练，并且在小步生成任务中具有较高的效率。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-919d70908993415e92c8909c00655335.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-0e2aa2475025a88869b1ac0e1b6be112.jpg" align="middle"></details>## Object-Attribute Binding in Text-to-Image Generation: Evaluation and Control **Authors:Maria Mihaela Trusca, Wolf Nuyts, Jonathan Thomm, Robert Honig, Thomas Hofmann, Tinne Tuytelaars, Marie-Francine Moens** Current diffusion models create photorealistic images given a text prompt as input but struggle to correctly bind attributes mentioned in the text to the right objects in the image. This is evidenced by our novel image-graph alignment model called EPViT (Edge Prediction Vision Transformer) for the evaluation of image-text alignment. To alleviate the above problem, we propose focused cross-attention (FCA) that controls the visual attention maps by syntactic constraints found in the input sentence. Additionally, the syntax structure of the prompt helps to disentangle the multimodal CLIP embeddings that are commonly used in T2I generation. The resulting DisCLIP embeddings and FCA are easily integrated in state-of-the-art diffusion models without additional training of these models. We show substantial improvements in T2I generation and especially its attribute-object binding on several datasets.\footnote{Code and data will be made available upon acceptance. [PDF](http://arxiv.org/abs/2404.13766v1) **摘要** 文本提示中的语法约束有助于生成更准确的图像，其中属性与正确的对象相关联。 **要点** * 当前扩散模型难以将文本提示中的属性正确绑定到图像中的正确对象。 * 提出了一种新的图像-图对齐模型 EPViT，用于评估图像-文本对齐。 * 引入了焦点交叉注意 (FCA)，以通过输入句子的句法约束来控制视觉注意图。 * 提示的语法结构有助于解耦在 T2I 生成中常用的多模态 CLIP 嵌入。 * 所产生的 DisCLIP 嵌入和 FCA 可以轻松集成到最先进的扩散模型中，而无需额外训练这些模型。 * 在 T2I 生成中展示了实质性的改进，尤其是在几个数据集上的属性-对象绑定。 **[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p><strong>Title:</strong> 文本到图像生成中的对象-属性绑定：评估和控制</p></li><li><p><strong>Authors:</strong> Maria Mihaela Trusca, Wolf Nuyts, Jonathan Thomm, Robert Hönig, Thomas Hofmann, Tinne Tuytelaars, Marie-Francine Moens</p></li><li><p><strong>Affiliation:</strong> KU Leuven, Department of Computer Science</p></li><li><p><strong>Keywords:</strong> 文本到图像生成、对象-属性绑定、注意力机制、扩散模型</p></li><li><p><strong>Urls:</strong> Paper: https://arxiv.org/abs/2404.13766, Github: None</p></li><li><p><strong>Summary:</strong></p></li></ol><p>(1): <strong>研究背景：</strong> 当前的扩散模型可以根据文本提示创建逼真的图像，但难以将文本中提到的属性正确绑定到图像中的正确对象上。</p><p>(2): <strong>过去的方法及其问题：</strong> 现有的方法主要基于 CLIP 评分进行评估，但无法检查复杂多对象提示中属性与对象的正确绑定。</p><p>(3): <strong>提出的研究方法：</strong> 提出了一种基于 ViT 的图像图预测模型 EPViT 和一种称为聚焦交叉注意 (FCA) 的方法，以控制视觉注意力图，从而改善对象-属性绑定。</p><p>(4): <strong>方法性能：</strong> 在多个数据集上，该方法显着提高了文本到图像生成及其对象-属性绑定性能，证明了其有效性。</p><ol><li><p>方法：</p><pre><code>            (1):提出了一种基于 ViT 的图像图预测模型 EPViT，用于生成图像特征图；

            (2):设计了一种称为聚焦交叉注意 (FCA) 的方法，用于控制视觉注意力图，从而改善对象-属性绑定；

            (3):将 FCA 和 DisCLIP 集成到现有的文本到图像生成模型中，以增强其对象-属性绑定性能；

            (4):在多个数据集上对增强后的模型进行评估，包括 COCO 10-K、CC-500、DAA-200 和 AE-267，以验证其有效性。
</code></pre></li><li><p>结论：</p><pre><code>            (1):本研究提出了无需训练的方法，强调了在文本到图像生成中整合语言句法结构的重要性。我们展示了它们在最先进的文本到图像扩散模型中的轻松且成功的集成，从而改善了对象-属性绑定，并减少了生成图像中的属性泄漏。此外，我们展示了一种新设计度量 EPViT 在评估文本到图像模型的对象-属性绑定方面优于 CLIP。EPViT 允许更好地理解和衡量模型在生成图像中准确反映预期文本描述的性能。                  
         8. 总结：

            (1):本研究的意义是什么？
            (2):从创新点、性能、工作量三个维度总结本文的优缺点。                   
            .......
         按照后面的输出格式：
         8. 结论：

            (1):xxx;

            (2):创新点：xxx；性能：xxx；工作量：xxx；

         务必使用中文回答（专有名词用英文标注），表述尽量简洁、学术，不要重复前面&lt;Summary&gt;的内容，原数字的使用价值，务必严格按照格式，对应内容输出到xxx，换行，.......表示根据实际要求填写，如果没有，可以不写。
</code></pre></li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-da0a6f3f353e58ad78bee95a227f033f.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-dd14ddde3959c9e4184326c15ccbc7c4.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-10db76640ccb78a6742ee69ebb161059.jpg" align="middle"></details><h2 id="Concept-Arithmetics-for-Circumventing-Concept-Inhibition-in-Diffusion-Models"><a href="#Concept-Arithmetics-for-Circumventing-Concept-Inhibition-in-Diffusion-Models" class="headerlink" title="Concept Arithmetics for Circumventing Concept Inhibition in Diffusion   Models"></a>Concept Arithmetics for Circumventing Concept Inhibition in Diffusion Models</h2><p><strong>Authors:Vitali Petsiuk, Kate Saenko</strong></p><p>Motivated by ethical and legal concerns, the scientific community is actively developing methods to limit the misuse of Text-to-Image diffusion models for reproducing copyrighted, violent, explicit, or personal information in the generated images. Simultaneously, researchers put these newly developed safety measures to the test by assuming the role of an adversary to find vulnerabilities and backdoors in them. We use compositional property of diffusion models, which allows to leverage multiple prompts in a single image generation. This property allows us to combine other concepts, that should not have been affected by the inhibition, to reconstruct the vector, responsible for target concept generation, even though the direct computation of this vector is no longer accessible. We provide theoretical and empirical evidence why the proposed attacks are possible and discuss the implications of these findings for safe model deployment. We argue that it is essential to consider all possible approaches to image generation with diffusion models that can be employed by an adversary. Our work opens up the discussion about the implications of concept arithmetics and compositional inference for safety mechanisms in diffusion models. Content Advisory: This paper contains discussions and model-generated content that may be considered offensive. Reader discretion is advised. Project page: <a target="_blank" rel="noopener" href="https://cs-people.bu.edu/vpetsiuk/arc">https://cs-people.bu.edu/vpetsiuk/arc</a></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.13706v1">PDF</a></p><p><strong>Summary</strong><br>对文本到图像扩散模型安全性的攻击通过利用扩散模型的合成性质和概念算术来重建被禁止的概念。</p><p><strong>Key Takeaways</strong></p><ul><li>扩散模型的合成性质使攻击者能够通过组合多个提示来生成图像。</li><li>即使直接计算目标概念的向量不再可访问，也可以通过组合不受抑制影响的其他概念来重建该向量。</li><li>攻击者可以利用概念算术来重建被禁止的概念，例如版权、暴力、色情或个人信息。</li><li>提出了两种新的攻击，分别称为“后门攻击”和“组合攻击”。</li><li>后门攻击利用了生成模型中存在的漏洞或后门。</li><li>组合攻击利用了扩散模型的合成性质。</li><li>这些攻击对安全模型的部署有重大影响。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: 绕过概念抑制的算术概念</p></li><li><p>Authors: Vitali Petsiuk and Kate Saenko</p></li><li><p>Affiliation: 波士顿大学</p></li><li><p>Keywords: Text-to-Image diffusion models, safety mechanisms, concept inhibition, concept arithmetics, compositional inference</p></li><li><p>Urls: https://arxiv.org/abs/2404.13706, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 随着 Text-to-Image (T2I) 生成模型的快速发展，人们开始关注其潜在的滥用风险，例如生成侵犯版权、暴力、色情或个人信息的内容。为了解决这些问题，研究人员开发了各种安全机制来限制模型的恶意使用。</p><p>(2): 现有的安全机制通常采用概念抑制的方法，即阻止模型生成特定概念的内容。然而，这些方法往往存在漏洞，允许攻击者通过组合其他概念来绕过抑制，从而重建被抑制概念的向量。</p><p>(3): 本文提出了一种基于概念算术和组合推理的攻击方法，该方法利用了扩散模型的组合特性，允许在单个图像生成中使用多个提示。通过组合不受抑制影响的其他概念，攻击者可以重建负责目标概念生成的向量，即使该向量的直接计算不再可访问。</p><p>(4): 实验结果表明，提出的攻击方法在各种扩散模型和概念抑制机制上都取得了成功。这些发现表明，在设计安全机制时，必须考虑攻击者可能采用的一切图像生成方法。</p><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：</li></ol><p>（1）：本文提出了一种基于概念算术和组合推理的攻击方法，该方法绕过了现有的基于概念抑制的安全机制，允许攻击者生成被抑制概念的内容。这项工作表明，在设计安全机制时，必须考虑攻击者可能采用的一切图像生成方法。</p><p>（2）：创新点：本文提出了一个绕过概念抑制的安全机制的新攻击方法，该方法利用了扩散模型的组合特性和概念算术。 性能：实验结果表明，提出的攻击方法在各种扩散模型和概念抑制机制上都取得了成功。 工作量：本文的工作量中等，需要对扩散模型、概念抑制机制和攻击方法有深入的理解。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-6c22a86090e8195e410f10a38f6fe1f9.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-936989960b19443d0f4e3c6f1a1e8e26.jpg" align="middle"></details></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io/Paper/2024-04-25/Diffusion%20Models/">https://kedreamix.github.io/Paper/2024-04-25/Diffusion Models/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Diffusion-Models/">Diffusion Models</a></div><div class="post_share"><div class="social-share" data-image="https://picx.zhimg.com/v2-b249a085ea084ca24b82dc1fcadcc875.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/Paper/2024-04-25/Talking%20Head%20Generation/" title="Talking Head Generation"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-00e62c0d66ff2641b9803987918d6fd0.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Talking Head Generation</div></div></a></div><div class="next-post pull-right"><a href="/Paper/2024-04-22/NeRF/" title="NeRF"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-5f7fb8305c36c1fe2572adfd98b584f7.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">NeRF</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/Paperscape/EMO/" title="EMO Emote Portrait Alive - 阿里HumanAIGC"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-6492e24fb03ffa98135dc584535ab7d9.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-03</div><div class="title">EMO Emote Portrait Alive - 阿里HumanAIGC</div></div></a></div><div><a href="/Paper/2024-01-24/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-71a37c439c6714e8867560f580599d2f.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-24</div><div class="title">Diffusion Models</div></div></a></div><div><a href="/Paper/2024-01-30/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-e55358c77a9d65f15701e8f33262e2a4.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-30</div><div class="title">Diffusion Models</div></div></a></div><div><a href="/Paper/2024-02-02/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-5920453c69c00995f18077b22d4a790e.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-02</div><div class="title">Diffusion Models</div></div></a></div><div><a href="/Paper/2024-02-09/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-32488f736ee10537497afccc3a1a1d76.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-09</div><div class="title">Diffusion Models</div></div></a></div><div><a href="/Paper/2024-02-13/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-3709a9941aada6c4d3ed35934e311765.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-13</div><div class="title">Diffusion Models</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-04-25-%E6%9B%B4%E6%96%B0"><span class="toc-text">2024-04-25 更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#ID-Aligner-Enhancing-Identity-Preserving-Text-to-Image-Generation-with-Reward-Feedback-Learning"><span class="toc-text">ID-Aligner: Enhancing Identity-Preserving Text-to-Image Generation with Reward Feedback Learning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Accelerating-Image-Generation-with-Sub-path-Linear-Approximation-Model"><span class="toc-text">Accelerating Image Generation with Sub-path Linear Approximation Model</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Concept-Arithmetics-for-Circumventing-Concept-Inhibition-in-Diffusion-Models"><span class="toc-text">Concept Arithmetics for Circumventing Concept Inhibition in Diffusion Models</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image:url('https://picx.zhimg.com/v2-b249a085ea084ca24b82dc1fcadcc875.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js").then((()=>{pangu.autoSpacingPage()}))}function panguInit(){panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.1},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script></div><script id="canvas_nest" defer color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><script data-pjax>function butterfly_swiper_injector_config(){var a=document.getElementById("recent-posts");console.log("已挂载butterfly_swiper"),a.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="Project/Linly-Talker - GPT-SoVITS/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="Project/Linly-Talker - GPT-SoVITS/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script></body></html>