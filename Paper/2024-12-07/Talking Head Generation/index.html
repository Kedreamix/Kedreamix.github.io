<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Talking Head Generation | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-12-07  MEMO Memory-Guided Diffusion for Expressive Talking Video Generation">
<meta property="og:type" content="article">
<meta property="og:title" content="Talking Head Generation">
<meta property="og:url" content="https://kedreamix.github.io/Paper/2024-12-07/Talking%20Head%20Generation/index.html">
<meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World">
<meta property="og:description" content="Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-12-07  MEMO Memory-Guided Diffusion for Expressive Talking Video Generation">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://picx.zhimg.com/v2-1495909db6c3934be6b148d04c1c0a90.jpg">
<meta property="article:published_time" content="2024-12-07T06:01:50.000Z">
<meta property="article:modified_time" content="2024-12-07T06:01:50.492Z">
<meta property="article:author" content="Kedreamix">
<meta property="article:tag" content="Talking Head Generation">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://picx.zhimg.com/v2-1495909db6c3934be6b148d04c1c0a90.jpg"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/Paper/2024-12-07/Talking%20Head%20Generation/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-74LZ5BEQQ1');
</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":true,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  noticeOutdate: {"limitDay":500,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: true,
  islazyload: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Talking Head Generation',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-12-07 14:01:50'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const now = new Date()
          const hour = now.getHours()
          const isNight = hour <= 6 || hour >= 24
          if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
          else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css"  media="defer" onload="this.media='all'"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload="this.media='all'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.0.0"><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml">

<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()
  window.addEventListener('load',() => { preloader.endLoading() })

  if (false) {
    document.addEventListener('pjax:send', () => { preloader.initLoading() })
    document.addEventListener('pjax:complete', () => { preloader.endLoading() })
  }
})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">304</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">17</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://picx.zhimg.com/v2-1495909db6c3934be6b148d04c1c0a90.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"/><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Talking Head Generation</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-12-07T06:01:50.000Z" title="发表于 2024-12-07 14:01:50">2024-12-07</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-12-07T06:01:50.492Z" title="更新于 2024-12-07 14:01:50">2024-12-07</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">6.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>20分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Talking Head Generation"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote>
<p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-12-07-更新"><a href="#2024-12-07-更新" class="headerlink" title="2024-12-07 更新"></a>2024-12-07 更新</h1><h2 id="MEMO-Memory-Guided-Diffusion-for-Expressive-Talking-Video-Generation"><a href="#MEMO-Memory-Guided-Diffusion-for-Expressive-Talking-Video-Generation" class="headerlink" title="MEMO: Memory-Guided Diffusion for Expressive Talking Video Generation"></a>MEMO: Memory-Guided Diffusion for Expressive Talking Video Generation</h2><p><strong>Authors:Longtao Zheng, Yifan Zhang, Hanzhong Guo, Jiachun Pan, Zhenxiong Tan, Jiahao Lu, Chuanxin Tang, Bo An, Shuicheng Yan</strong></p>
<p>Recent advances in video diffusion models have unlocked new potential for realistic audio-driven talking video generation. However, achieving seamless audio-lip synchronization, maintaining long-term identity consistency, and producing natural, audio-aligned expressions in generated talking videos remain significant challenges. To address these challenges, we propose Memory-guided EMOtion-aware diffusion (MEMO), an end-to-end audio-driven portrait animation approach to generate identity-consistent and expressive talking videos. Our approach is built around two key modules: (1) a memory-guided temporal module, which enhances long-term identity consistency and motion smoothness by developing memory states to store information from a longer past context to guide temporal modeling via linear attention; and (2) an emotion-aware audio module, which replaces traditional cross attention with multi-modal attention to enhance audio-video interaction, while detecting emotions from audio to refine facial expressions via emotion adaptive layer norm. Extensive quantitative and qualitative results demonstrate that MEMO generates more realistic talking videos across diverse image and audio types, outperforming state-of-the-art methods in overall quality, audio-lip synchronization, identity consistency, and expression-emotion alignment. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04448v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://memoavatar.github.io">https://memoavatar.github.io</a></p>
<p><strong>Summary</strong><br>提出基于记忆引导的EMO情感感知扩散模型（MEMO），实现身份一致性、表情自然且与音频同步的说话视频生成。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>研究针对视频扩散模型在说话视频生成中的新潜力。</li>
<li>面临音频唇同步、身份一致性和表情自然性的挑战。</li>
<li>提出MEMO模型，包含记忆引导时序模块和情感感知音频模块。</li>
<li>记忆引导模块通过线性注意力指导时序建模，增强长期身份一致性和运动平滑性。</li>
<li>情感感知模块使用多模态注意力增强音频-视频交互，并通过情感自适应层规范调整面部表情。</li>
<li>MEMO模型在多种图像和音频类型上生成更逼真的说话视频，全面超越现有方法。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li><p>Title: 基于记忆引导扩散模型的表达性对话视频生成研究（MEMO: Memory-Guided Diffusion for Expressive Talking Video Generation）</p>
</li>
<li><p>Authors: Longtao Zheng, Yifan Zhang, Hanzhong Guo, Jiachun Pan, Zhenxiong Tan, Jiahao Lu, Chuanxin Tang, Bo An, Shuicheng Yan</p>
</li>
<li><p>Affiliation:<br>部分作者来自于天空AI公司（Skywork AI），南洋理工大学（Nanyang Technological University），新加坡国立大学（National University of Singapore）。</p>
</li>
<li><p>Keywords: 音频驱动的视频生成，记忆引导扩散模型，身份一致性，表情与情感对齐，视频扩散模型。</p>
</li>
<li><p>Urls: Paper Url: 暂时无法提供直接链接。Github代码链接：Github: None（若存在，请提供链接）</p>
</li>
<li><p>Summary:</p>
</li>
</ol>
<p>(1) 研究背景：随着虚拟形象、数字内容创作和实时通信等领域的快速发展，音频驱动对话视频生成技术受到广泛关注。然而，实现无缝的音频与口型同步、长期身份一致性以及自然、与音频对齐的表情生成仍是该技术的挑战。</p>
<p>(2) 过去的方法及问题：现有的视频扩散模型虽然在音频驱动的对话视频生成方面取得进展，但在保持长期身份一致性、口型与音频同步以及自然表情生成方面仍存在不足。大部分方法使用交叉注意力来结合音频指导视频生成，并通常基于过去2-4帧进行生成以提高运动平滑度，但这种方法存在长期身份不一致的问题。此外，一些方法使用单一的情感标签来指导整个视频的情感表达，忽略了音频中情感的细微变化。</p>
<p>(3) 研究方法：针对上述问题，本文提出了基于记忆引导扩散模型的表达性对话视频生成方法（MEMO）。该方法包含两个关键模块：a. 记忆引导时序模块，通过开发记忆状态来存储更长时间的上下文信息，并通过线性注意力来指导时序建模，从而提高长期身份一致性和运动平滑度；b. 情感感知音频模块，通过多模态注意力增强音频与视频的交互，同时通过从音频中检测到的情感来微调面部表情。使用情感自适应层归一化来细化表情表达。本文提出了MEMO模型生成更具真实感的对话视频。通过广泛的定量和定性评估证明MEMO在整体质量、音频口型同步、身份一致性和表情情感对齐方面优于现有方法。</p>
<p>(4) 任务与性能：本文的方法在音频驱动的对话视频生成任务上取得了显著的性能提升。生成的视频展示出了更高的身份一致性、音频口型同步精度以及更自然的表情表达。实验结果表明该方法能够有效支持其设定的目标。</p>
<ol>
<li>方法论：</li>
</ol>
<p>(1) 研究背景：该研究针对音频驱动对话视频生成技术展开，针对无缝音频与口型同步、长期身份一致性以及自然情感表达生成技术的挑战展开研究。</p>
<p>(2) 现有方法的问题：现有视频扩散模型在音频驱动对话视频生成方面虽有所进展，但在长期身份一致性、口型与音频同步以及自然表情生成方面仍存在不足。大部分方法使用交叉注意力来结合音频指导视频生成，但这种方法存在长期身份不一致的问题。此外，一些方法忽略了音频中情感的细微变化。</p>
<p>(3) 研究方法：针对上述问题，提出了基于记忆引导扩散模型的表达性对话视频生成方法（MEMO）。首先，设计了一个记忆引导时序模块，通过开发记忆状态来存储更长时间的上下文信息，并通过线性注意力来指导时序建模，以提高长期身份一致性和运动平滑度。其次，设计了一个情感感知音频模块，通过多模态注意力增强音频与视频的交互，同时根据音频中检测到的情感微调面部表情。使用情感自适应层归一化来细化表情表达。通过广泛的定量和定性评估，证明MEMO在整体质量、音频口型同步、身份一致性和表情情感对齐方面优于现有方法。</p>
<p>(4) 训练策略：MEMO的训练分为两个阶段，每个阶段都有特定的目标。第一阶段是面部域适应，初始化Reference Net和Diffusion Net的空间模块使用SD 1.5的权重。在此阶段，适应Reference Net、Diffusion Net的空间注意力模块和原始文本交叉注意力模块到面部域。第二阶段是情感解耦稳健训练，将情感感知音频模块和记忆引导时序模块集成到Diffusion Net中。首先对新添加的模块进行热身训练，保持第一阶段模块固定。热身完成后，所有模块联合训练。在此阶段使用情感条件流损失并扩大数据集进行更全面的训练。当训练视频片段来源于MEAD时采用情感解耦训练策略。此外，研究还发现即使应用了数据处理管道后仍然存在一些噪声数据导致扩散训练不稳定和模型优化偏差。为了缓解这一问题，进一步开发了一种稳健的训练策略，通过过滤掉损失值突然超过特定大值（本例中为0.1）的数据点来提高模型的鲁棒性。该方法的情感条件流损失通常会收敛并波动在约0.03左右。</p>
<ol>
<li>Conclusion:</li>
</ol>
<ul>
<li>(1) 该研究工作的意义在于针对音频驱动对话视频生成技术的挑战展开研究，特别是在无缝音频与口型同步、长期身份一致性以及自然情感表达生成技术方面。这项工作为相关领域提供了一种有效的方法，有望推动虚拟形象、数字内容创作和实时通信等领域的发展。</li>
<li>(2) 创新点：该文章提出了基于记忆引导扩散模型的表达性对话视频生成方法（MEMO），通过记忆引导时序模块和情感感知音频模块的设计，有效提高了长期身份一致性、音频口型同步精度以及表情表达的自然度。</li>
<li>性能：通过实验验证，MEMO在音频驱动的对话视频生成任务上取得了显著的性能提升，生成的视频展示出了更高的身份一致性、音频口型同步精度以及更自然的表情表达。</li>
<li>工作量：文章进行了广泛的实验验证，包括方法论和训练策略的研究，证明了MEMO的有效性。然而，文章未提供Github代码链接，无法评估其代码复现的难度和工作量。</li>
</ul>
<p>希望以上内容能够满足您的要求。</p>
<details>
  <summary>点此查看论文截图</summary>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-941476c2fc5c6159d9632247e8c47468.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-bf7673e12ff785c7eba3e37be48bdc1c.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-73104f1334c96e8289a517f970d92d87.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-92f13d51fe48e01fec21c2b9ef7e6a43.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-98b5c4a01c41fe9eaf0f7d5662ccd784.jpg" align="middle">
</details>




<h2 id="INFP-Audio-Driven-Interactive-Head-Generation-in-Dyadic-Conversations"><a href="#INFP-Audio-Driven-Interactive-Head-Generation-in-Dyadic-Conversations" class="headerlink" title="INFP: Audio-Driven Interactive Head Generation in Dyadic Conversations"></a>INFP: Audio-Driven Interactive Head Generation in Dyadic Conversations</h2><p><strong>Authors:Yongming Zhu, Longhao Zhang, Zhengkun Rong, Tianshu Hu, Shuang Liang, Zhipeng Ge</strong></p>
<p>Imagine having a conversation with a socially intelligent agent. It can attentively listen to your words and offer visual and linguistic feedback promptly. This seamless interaction allows for multiple rounds of conversation to flow smoothly and naturally. In pursuit of actualizing it, we propose INFP, a novel audio-driven head generation framework for dyadic interaction. Unlike previous head generation works that only focus on single-sided communication, or require manual role assignment and explicit role switching, our model drives the agent portrait dynamically alternates between speaking and listening state, guided by the input dyadic audio. Specifically, INFP comprises a Motion-Based Head Imitation stage and an Audio-Guided Motion Generation stage. The first stage learns to project facial communicative behaviors from real-life conversation videos into a low-dimensional motion latent space, and use the motion latent codes to animate a static image. The second stage learns the mapping from the input dyadic audio to motion latent codes through denoising, leading to the audio-driven head generation in interactive scenarios. To facilitate this line of research, we introduce DyConv, a large scale dataset of rich dyadic conversations collected from the Internet. Extensive experiments and visualizations demonstrate superior performance and effectiveness of our method. Project Page: <a target="_blank" rel="noopener" href="https://grisoon.github.io/INFP/">https://grisoon.github.io/INFP/</a>. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04037v1">PDF</a> </p>
<p><strong>Summary</strong><br>提出INFP，一种基于音频的双向头部生成框架，实现社交智能对话。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>INFP旨在实现社交智能对话，注重双向互动。</li>
<li>模型自动切换发言与聆听状态，无需手动分配角色。</li>
<li>包含运动头部模仿和音频引导运动生成两个阶段。</li>
<li>运动头部模仿从视频学习面部交流行为。</li>
<li>音频引导运动生成通过去噪将音频映射到运动代码。</li>
<li>使用DyConv数据集，包含丰富的双向对话。</li>
<li>实验证明方法具有优越性能和有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li><p>Title: INFP：双人对话中的音频驱动交互式头部生成</p>
</li>
<li><p>Authors: Yongming Zhu，Longhao Zhang，Zhengkun Rong，Tianshu Hu，Shuang Liang，Zhipeng Ge（由字节跳动公司支持）</p>
</li>
<li><p>Affiliation: 作者们均来自字节跳动公司。</p>
</li>
<li><p>Keywords: 音频驱动头部生成，双人对话，交互性，面部表达，动作生成，深度学习。</p>
</li>
<li><p>Urls: <a target="_blank" rel="noopener" href="https://arisecvpr24.github.io/INFP_ProjectPage/">https://arisecvpr24.github.io/INFP_ProjectPage/</a> 论文链接，Github代码链接（如有可用）。当前暂无代码链接提供。</p>
</li>
<li><p>Summary: </p>
<ul>
<li><p>(1) 研究背景：随着人工智能技术的发展，构建具有交互性的对话代理成为了一个热门的研究领域。本文旨在解决在双人对话场景中音频驱动的交互式头部生成问题，使代理能够根据不同的音频信号进行动态的面部表情和头部动作。</p>
</li>
<li><p>(2) 过去的方法及其问题：现有研究主要集中在单方面的音频驱动头部生成，如说话或聆听，忽视了双人对话中的交互性。此外，大多数方法需要手动分配角色和明确的角色切换，无法适应动态的对话场景。</p>
</li>
<li><p>(3) 研究方法：本文提出了一种新颖的音频驱动头部生成框架INFP，适用于双人对话场景。该框架包含两个阶段：基于运动的头部模仿阶段和音频引导的运动生成阶段。第一阶段学习从真实对话视频中将面部沟通行为投影到一个低维运动潜在空间，并使用这些运动潜在代码来驱动静态图像。第二阶段学习从输入的双人对话音频到运动潜在代码的映射，从而实现音频驱动的头部生成。</p>
</li>
<li><p>(4) 任务与性能：本文在DyConv数据集上进行了实验和可视化展示，该数据集包含丰富的双人对话场景。实验结果表明，INFP方法在交互式场景中实现了优越的头部生成性能，能够根据不同的音频信号进行动态的面部表情和头部动作切换。性能支持达到了研究目标。</p>
</li>
</ul>
</li>
</ol>
<p>希望以上回答能满足您的要求！</p>
<ol>
<li>Methods:</li>
</ol>
<ul>
<li>(1) 研究首先明确了问题的定义和背景，包括音频驱动的交互式头部生成在双人对话场景中的重要性。此外，对当前的研究现状进行了回顾，指出了现有方法的不足和局限性。</li>
<li>(2) 针对现有方法的不足，研究提出了一种新颖的音频驱动头部生成框架INFP。该框架包含两个阶段：基于运动的头部模仿阶段和音频引导的运动生成阶段。在第一个阶段中，研究使用深度学习模型从真实对话视频中提取面部沟通行为，并将这些行为投影到一个低维运动潜在空间。此外，该研究还利用这些运动潜在代码来驱动静态图像。在第二个阶段中，研究通过训练深度学习模型来学习从双人对话音频到运动潜在代码的映射，从而实现音频驱动的头部生成。</li>
<li>(3) 为了验证所提出方法的有效性，研究在DyConv数据集上进行了实验和可视化展示。实验结果表明，INFP方法在交互式场景中实现了优越的头部生成性能，能够根据不同的音频信号进行动态的面部表情和头部动作切换。此外，该研究还对所提出方法的关键参数进行了详细的分析和讨论，以验证其有效性和可靠性。</li>
</ul>
<p>希望以上内容符合您的要求！</p>
<ol>
<li>Conclusion:</li>
</ol>
<ul>
<li>(1) 这项研究工作的意义在于提出了一种新颖的音频驱动交互式头部生成框架INFP，旨在解决双人对话场景中的音频驱动交互式头部生成问题。该研究对于提升人工智能技术在对话代理领域的应用具有重要意义，能够更好地模拟人类对话时的面部表情和头部动作，提高对话代理的真实感和交互性。</li>
<li>(2) 创新点：本文提出了一个适用于双人对话场景的音频驱动头部生成框架INFP，该框架能够根据不同的音频信号进行动态的面部表情和头部动作生成，且能够适应不同的对话角色，无需手动分配角色和明确的角色切换。<br>性能：在DyConv数据集上的实验结果表明，INFP方法实现了优越的头部生成性能。<br>工作量：文章对方法的原理、实验设计、实验过程以及结果分析等方面进行了详细的阐述，但文章未提及该研究的代码开源情况，且数据量和工作复杂度方面未进行具体阐述。</li>
</ul>
<p>希望以上答复能够满足您的要求！</p>
<details>
  <summary>点此查看论文截图</summary>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-16c76e149541f70b8cde77669efb7290.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-c3be762eba6154196ed65c70710399ee.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-c084dff357cd500a71ead5639334cda0.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-1495909db6c3934be6b148d04c1c0a90.jpg" align="middle">
</details>




<h2 id="IF-MDM-Implicit-Face-Motion-Diffusion-Model-for-High-Fidelity-Realtime-Talking-Head-Generation"><a href="#IF-MDM-Implicit-Face-Motion-Diffusion-Model-for-High-Fidelity-Realtime-Talking-Head-Generation" class="headerlink" title="IF-MDM: Implicit Face Motion Diffusion Model for High-Fidelity Realtime   Talking Head Generation"></a>IF-MDM: Implicit Face Motion Diffusion Model for High-Fidelity Realtime   Talking Head Generation</h2><p><strong>Authors:Sejong Yang, Seoung Wug Oh, Yang Zhou, Seon Joo Kim</strong></p>
<p>We introduce a novel approach for high-resolution talking head generation from a single image and audio input. Prior methods using explicit face models, like 3D morphable models (3DMM) and facial landmarks, often fall short in generating high-fidelity videos due to their lack of appearance-aware motion representation. While generative approaches such as video diffusion models achieve high video quality, their slow processing speeds limit practical application. Our proposed model, Implicit Face Motion Diffusion Model (IF-MDM), employs implicit motion to encode human faces into appearance-aware compressed facial latents, enhancing video generation. Although implicit motion lacks the spatial disentanglement of explicit models, which complicates alignment with subtle lip movements, we introduce motion statistics to help capture fine-grained motion information. Additionally, our model provides motion controllability to optimize the trade-off between motion intensity and visual quality during inference. IF-MDM supports real-time generation of 512x512 resolution videos at up to 45 frames per second (fps). Extensive evaluations demonstrate its superior performance over existing diffusion and explicit face models. The code will be released publicly, available alongside supplementary materials. The video results can be found on <a target="_blank" rel="noopener" href="https://bit.ly/ifmdm_supplementary">https://bit.ly/ifmdm_supplementary</a>. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04000v1">PDF</a> underreview in CVPR 2025</p>
<p><strong>Summary</strong><br>提出了一种基于单图和音频输入的高分辨率说话头生成新方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>新方法可从单图和音频生成高分辨率说话头视频。</li>
<li>避免了显式人脸模型（如3DMM和面部特征点）的局限性。</li>
<li>使用隐式运动编码面部，提高视频生成质量。</li>
<li>解决了隐式运动与细微唇部动作对齐的问题。</li>
<li>提供运动可控性，优化运动强度与视觉质量之间的权衡。</li>
<li>支持实时生成512x512分辨率的视频，最高45fps。</li>
<li>性能优于现有扩散模型和显式人脸模型。</li>
<li>公开代码和补充材料。</li>
<li>视频结果可在指定链接查看。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li><p>Title: 隐式面部运动扩散模型用于高质量实时对话头部生成研究</p>
</li>
<li><p>Authors: Sejong Yang, Seoung Wug Oh, Yang Zhou, Seon Joo Kim (Adobe Research &amp; Yonsei University)</p>
</li>
<li><p>Affiliation: 第一作者来自Yonsei University。其他几位作者来自Adobe Research。</p>
</li>
<li><p>Keywords: Talking Head Generation, Video Diffusion Model, Implicit Face Motion, High-Fidelity Realtime Generation</p>
</li>
<li><p>Urls: 论文链接：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/cs.CV/papers/2412.04000v1">论文链接</a>（暂时无法提供GitHub代码链接）</p>
</li>
<li><p>Summary: </p>
</li>
</ol>
<p>(1) 研究背景：本文主要关注从单张图像和音频输入生成高质量对话视频的任务。尽管已有使用显式面部模型（如3D形态模型，面部特征点）的方法在该任务上取得一定成果，但它们生成的高保真视频质量仍有待提高。同时，基于生成模型的视频扩散模型虽然可以实现高视频质量，但其处理速度慢，难以应用于实际场景。因此，本文提出了一种新的隐式面部运动扩散模型（IF-MDM）。</p>
<p>(2) 过去的方法及其问题：过去的方法主要分为两类，一类是使用显式面部模型的方法，这类方法虽然计算效率较高，但难以生成高质量的视频，因为它们缺乏对面部运动的精确捕捉以及细节帧的生成能力。另一类是视频扩散模型，虽然它们可以生成高质量的视频，但计算量大，处理速度慢。因此，现有的方法难以在保持高视频质量的同时实现实时生成。</p>
<p>(3) 研究方法：针对上述问题，本文提出了隐式面部运动扩散模型（IF-MDM）。该模型采用隐式运动编码人类面部到面向感知的压缩面部潜在空间，从而增强视频生成能力。尽管隐式运动缺乏显式模型的空间分离特性，我们引入了运动统计信息来帮助捕获精细的运动信息。此外，我们的模型还提供了运动可控性，以在推理过程中优化运动强度与视觉质量的权衡。IF-MDM支持以高达每秒45帧的速度实时生成512x512分辨率的视频。</p>
<p>(4) 任务与性能：本文的方法在谈话头部生成任务上取得了显著成果。与现有的扩散和显式面部模型相比，IF-MDM表现出卓越的性能。实验结果表明，IF-MDM能够生成高质量的视频，同时保持实时性能，证明了其有效性。论文中提供的视频结果可以在相关链接中找到。</p>
<ol>
<li>方法：</li>
</ol>
<p>(1) 研究背景：文章主要关注如何从单张图像和音频输入生成高质量对话视频的任务。针对现有方法存在的问题，如显式面部模型生成视频质量不高和基于生成模型的视频扩散模型处理速度慢等，提出了一种新的隐式面部运动扩散模型（IF-MDM）。</p>
<p>(2) 方法概述：首先，文章介绍了隐式运动编码人类面部的理论基础知识，将其编码到面向感知的压缩面部潜在空间，以增强视频生成能力。为了捕获精细的运动信息，引入了运动统计信息。此外，该模型还提供了运动可控性，以在推理过程中优化运动强度与视觉质量的权衡。</p>
<p>(3) 扩散模型初步介绍：介绍了扩散模型的理论基础，这是一种通过正向过程将数据分布转化为已知噪声分布，并通过反向过程生成新数据样本的生成模型。在本文中，隐式运动生成器被集成到扩散管道中用于推理。</p>
<p>(4) 训练过程：训练分为两个阶段。第一阶段的目标是学习外观和运动的表示分离，通过利用身份图像和对应的运动图像进行训练，学习压缩运动向量。第二阶段则训练隐式运动生成器学习自然运动序列的分布，使用冻结的视觉编码器和语音编码器，提取运动向量序列和语音向量序列进行训练。</p>
<p>(5) 隐式运动生成器设计：介绍了隐式运动生成器的详细架构，包括其如何接受语音指导以合成表达和同步的头部视频。为了提高生成的运动质量，引入了运动均值和标准差作为附加条件指导，帮助模型学习整体运动动力学。同时，利用时序调制技术将语音向量和扩散时间融入生成过程，确保生成的视频与音频节奏和动力学对齐。</p>
<p>总的来说，本文提出的隐式面部运动扩散模型（IF-MDM）在谈话头部生成任务上取得了显著成果，实现了高质量实时视频生成。</p>
<ol>
<li>Conclusion:</li>
</ol>
<p>(1) 这项工作的意义在于提出了一种新的隐式面部运动扩散模型（IF-MDM），用于从单张图像和音频输入生成高质量对话视频。该模型在谈话头部生成任务上具有显著成果，具有重要的实际应用价值，可应用于虚拟助手、数字化身、视频会议和内容创作等领域，能够提升用户体验、可访问性和个性化，为数字媒体、通信和娱乐等领域带来革新。</p>
<p>(2) 创新点：该文章提出了隐式面部运动扩散模型（IF-MDM），结合隐式运动编码和扩散模型理论，实现了高质量实时视频生成。其引入运动统计信息和可控性优化，提高了运动信息的捕获和生成质量。此外，该模型在谈话头部生成任务上取得了显著成果，证明了其有效性和创新性。</p>
<p>性能：该模型实现了高质量的视频生成，同时保持了实时性能，支持高达每秒45帧的速度生成512x512分辨率的视频。与现有的扩散和显式面部模型相比，IF-MDM表现出卓越的性能。</p>
<p>工作量：文章详细阐述了模型的设计和实现过程，包括训练过程、隐式运动生成器的设计和实现等。工作量较大，但文章结构清晰，逻辑严谨，易于理解。</p>
<p>然而，该文章也存在一定的局限性，未来工作将聚焦于扩展IF-MDM的能力，处理更复杂的场景、多样化的环境条件和进一步提高生成视频的可控性和表现力。同时，也需要关注潜在伦理问题，如虚假信息等问题。</p>
<details>
  <summary>点此查看论文截图</summary>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-d0e2109339e6dadf6720d378c36b617e.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-00138b9c881d5f5772c1ecfefc967c46.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-fa55c6f6b4e5341598b00eea17364722.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-d7687d5dd74e676e999fbf3aeac19020.jpg" align="middle">
</details>




</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://kedreamix.github.io/Paper/2024-12-07/Talking%20Head%20Generation/">https://kedreamix.github.io/Paper/2024-12-07/Talking Head Generation/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Talking-Head-Generation/">Talking Head Generation</a></div><div class="post_share"><div class="social-share" data-image="https://picx.zhimg.com/v2-1495909db6c3934be6b148d04c1c0a90.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/Paper/2024-12-07/3DGS/" title="3DGS"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-f99336e9eaadb0ddb4c3dfffa1d84b60.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">3DGS</div></div></a></div><div class="next-post pull-right"><a href="/Paper/2024-12-07/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙/虚拟人"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-325bb7409947b2356cc510d3fabf325b.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">元宇宙/虚拟人</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/Paperscape/Real3D-Portrait/" title="REAL3D-PORTRAIT ONE-SHOT REALISTIC 3D TALKING PORTRAIT SYNTHESIS"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-68585b79de5f83b0dfa23304f41b9b98.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-15</div><div class="title">REAL3D-PORTRAIT ONE-SHOT REALISTIC 3D TALKING PORTRAIT SYNTHESIS</div></div></a></div><div><a href="/Paperscape/EMO/" title="EMO Emote Portrait Alive - 阿里HumanAIGC"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-6492e24fb03ffa98135dc584535ab7d9.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-03</div><div class="title">EMO Emote Portrait Alive - 阿里HumanAIGC</div></div></a></div><div><a href="/Paperscape/VividTalk/" title="VividTalk One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-8521b04f82075cc27b5e95148dba9792.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-05</div><div class="title">VividTalk One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior</div></div></a></div><div><a href="/Paperscape/SyncTalk/" title="SyncTalk The Devil is in the Synchronization for Talking Head Synthesis"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-a57e0937b2f452009023394a59529dfb.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-07</div><div class="title">SyncTalk The Devil is in the Synchronization for Talking Head Synthesis</div></div></a></div><div><a href="/Note/BlendShape/" title="Blendshape学习笔记"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://p6-sign.toutiaoimg.com/pgc-image/2c8cbd123e00470e95500a8ae62da605~noop.image?_iz=58558&from=article.pc_detail&lk3s=953192f4&x-expires=1710668214&x-signature=UHPhjWP4v96kbtfJzF97Z%2Bp3klc%3D" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-11</div><div class="title">Blendshape学习笔记</div></div></a></div><div><a href="/Paper/Awesome-Talking-Head-Synthesis/" title="超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-01</div><div class="title">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-12-07-%E6%9B%B4%E6%96%B0"><span class="toc-text">2024-12-07 更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#MEMO-Memory-Guided-Diffusion-for-Expressive-Talking-Video-Generation"><span class="toc-text">MEMO: Memory-Guided Diffusion for Expressive Talking Video Generation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#INFP-Audio-Driven-Interactive-Head-Generation-in-Dyadic-Conversations"><span class="toc-text">INFP: Audio-Driven Interactive Head Generation in Dyadic Conversations</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#IF-MDM-Implicit-Face-Motion-Diffusion-Model-for-High-Fidelity-Realtime-Talking-Head-Generation"><span class="toc-text">IF-MDM: Implicit Face Motion Diffusion Model for High-Fidelity Realtime   Talking Head Generation</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image: url('https://picx.zhimg.com/v2-1495909db6c3934be6b148d04c1c0a90.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><!-- hexo injector body_end start --><script data-pjax>
  function butterfly_swiper_injector_config(){
    var parent_div_git = document.getElementById('recent-posts');
    var item_html = '<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="Project/Linly-Talker - GPT-SoVITS/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="Project/Linly-Talker - GPT-SoVITS/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>';
    console.log('已挂载butterfly_swiper')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'undefined'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_swiper_injector_config();
  }
  else if (epage === cpage){
    butterfly_swiper_injector_config();
  }
  </script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script><!-- hexo injector body_end end --></body></html>