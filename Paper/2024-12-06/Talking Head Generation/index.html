<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>Talking Head Generation | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-12-06  SINGER Vivid Audio-driven Singing Video Generation with Multi-scale   Spectral Diffusion Model"><meta property="og:type" content="article"><meta property="og:title" content="Talking Head Generation"><meta property="og:url" content="https://kedreamix.github.io/Paper/2024-12-06/Talking%20Head%20Generation/index.html"><meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World"><meta property="og:description" content="Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-12-06  SINGER Vivid Audio-driven Singing Video Generation with Multi-scale   Spectral Diffusion Model"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://picx.zhimg.com/v2-214c0edb7fcf5bee1f7a257d234aa375.jpg"><meta property="article:published_time" content="2024-12-06T14:40:19.000Z"><meta property="article:modified_time" content="2024-12-06T14:55:45.656Z"><meta property="article:author" content="Kedreamix"><meta property="article:tag" content="Talking Head Generation"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://picx.zhimg.com/v2-214c0edb7fcf5bee1f7a257d234aa375.jpg"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/Paper/2024-12-06/Talking%20Head%20Generation/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-74LZ5BEQQ1")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!0,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"简"},noticeOutdate:{limitDay:500,position:"top",messagePrev:"It has been",messageNext:"days since the last update, the content of the article may be outdated."},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:200},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!0,post:!0},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!0,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"Talking Head Generation",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-12-06 22:55:45"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach((e=>{n.setAttribute(e,t[e])})),document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)})),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=24?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 7.0.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml"></head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{const d=document.getElementById("loading-box"),e=document.body,o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l(),window.addEventListener("load",(()=>{o()}))})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">304</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">17</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url('https://picx.zhimg.com/v2-214c0edb7fcf5bee1f7a257d234aa375.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Talking Head Generation</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-12-06T14:40:19.000Z" title="发表于 2024-12-06 22:40:19">2024-12-06</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-12-06T14:55:45.656Z" title="更新于 2024-12-06 22:55:45">2024-12-06</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">10.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>36分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="Talking Head Generation"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-12-06-更新"><a href="#2024-12-06-更新" class="headerlink" title="2024-12-06 更新"></a>2024-12-06 更新</h1><h2 id="SINGER-Vivid-Audio-driven-Singing-Video-Generation-with-Multi-scale-Spectral-Diffusion-Model"><a href="#SINGER-Vivid-Audio-driven-Singing-Video-Generation-with-Multi-scale-Spectral-Diffusion-Model" class="headerlink" title="SINGER: Vivid Audio-driven Singing Video Generation with Multi-scale   Spectral Diffusion Model"></a>SINGER: Vivid Audio-driven Singing Video Generation with Multi-scale Spectral Diffusion Model</h2><p><strong>Authors:Yan Li, Ziya Zhou, Zhiqiang Wang, Wei Xue, Wenhan Luo, Yike Guo</strong></p><p>Recent advancements in generative models have significantly enhanced talking face video generation, yet singing video generation remains underexplored. The differences between human talking and singing limit the performance of existing talking face video generation models when applied to singing. The fundamental differences between talking and singing-specifically in audio characteristics and behavioral expressions-limit the effectiveness of existing models. We observe that the differences between singing and talking audios manifest in terms of frequency and amplitude. To address this, we have designed a multi-scale spectral module to help the model learn singing patterns in the spectral domain. Additionally, we develop a spectral-filtering module that aids the model in learning the human behaviors associated with singing audio. These two modules are integrated into the diffusion model to enhance singing video generation performance, resulting in our proposed model, SINGER. Furthermore, the lack of high-quality real-world singing face videos has hindered the development of the singing video generation community. To address this gap, we have collected an in-the-wild audio-visual singing dataset to facilitate research in this area. Our experiments demonstrate that SINGER is capable of generating vivid singing videos and outperforms state-of-the-art methods in both objective and subjective evaluations.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03430v1">PDF</a></p><p><strong>Summary</strong><br>针对唱歌视频生成，提出SINGER模型，通过多尺度频谱模块和频谱过滤模块提升生成效果。</p><p><strong>Key Takeaways</strong></p><ol><li>生成模型进步显著，唱歌视频生成仍待探索。</li><li>说话与唱歌差异限制现有模型性能。</li><li>唱歌与说话音频差异表现在频率和幅度。</li><li>设计多尺度频谱模块学习唱歌频谱模式。</li><li>开发频谱过滤模块学习唱歌相关行为。</li><li>集成模块于扩散模型，提升唱歌视频生成。</li><li>收集真实唱歌视频数据集促进研究。</li><li>SINGER模型生成效果优于现有方法。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: 基于多尺度谱的音频驱动歌唱视频生成方法（SINGER: Vivid Audio-driven Singing Video Generation with Multi-scale）</p></li><li><p>Authors: （请提供作者名单）</p></li><li><p>Affiliation: 第一作者，（输出中文翻译后的）归属机构为（例如：XX大学计算机视觉研究中心）。</p></li><li><p>Keywords: 音频驱动、歌唱视频生成、多尺度谱模型、扩散模型、人脸渲染。</p></li><li><p>Urls: 由于无法直接提供链接，关于论文的具体链接和GitHub代码链接，您可以填写如下：</p><ul><li>论文链接：待论文发布后，将提供链接。</li><li>GitHub链接（如果可用）: [GitHub链接地址]（如果论文没有提供GitHub代码，可以填写None）。</li></ul></li><li><p>Summary:</p><ul><li><p>(1) 研究背景：随着生成模型的发展，说话人脸视频生成已经取得了显著的进步，但歌唱视频生成仍然是一个被忽视的领域。由于唱歌与说话的音频特性和行为表达存在根本差异，现有的说话人脸视频生成模型在应用于歌唱时表现有限。本文旨在解决这一问题。</p></li><li><p>(2) 过去的方法及问题：之前的方法主要关注说话人脸视频的生成，如Audio2Head、SadTalker和MuseTalk等，但它们在处理歌唱视频生成时效果不佳。此外，尽管有一些扩散模型如AniPortrait、Echomimic等被应用于动画和肖像生成，但它们并不专注于歌唱视频。这些问题的核心是现有模型无法有效处理歌唱音频中特有的频率和振幅差异以及与之相关的行为表达。</p></li><li><p>(3) 研究方法：针对上述问题，本文提出了一个基于多尺度谱的音频驱动歌唱视频生成方法（SINGER）。该方法包括两个主要模块：多尺度谱模块和谱滤波模块。多尺度谱模块帮助模型学习谱域中的歌唱模式，而谱滤波模块则帮助模型学习与歌唱音频相关的人类行为。这些模块被集成到扩散模型中，以改进歌唱视频生成的性能。此外，为了推动研究，作者还收集了一个野外歌唱视频数据集。</p></li><li><p>(4) 任务与性能：本文的方法在歌唱视频生成任务上取得了显著成果，生成的视频在客观和主观评估中都优于现有方法。通过收集真实世界的歌唱视频数据集，本文为评估模型性能提供了可靠的基准。实验结果表明，SINGER能够生成逼真的歌唱视频，验证了该方法的有效性和优越性。</p></li></ul></li><li>Methods:</li></ol><p>(1) 研究背景和方法论提出：随着生成模型的发展，说话人脸视频生成已经取得显著进步，但歌唱视频生成仍然是一个被忽视的领域。针对现有模型在处理歌唱音频时的局限性，本文提出了一个基于多尺度谱的音频驱动歌唱视频生成方法（SINGER）。</p><p>(2) 多尺度谱模块介绍：多尺度谱模块是本文的核心创新之一。该模块帮助模型学习谱域中的歌唱模式，通过对音频信号进行多尺度谱分析，提取与歌唱相关的特征信息。</p><p>(3) 谱滤波模块介绍：除了多尺度谱模块，文章还引入了谱滤波模块。这个模块旨在帮助模型学习与歌唱音频相关的人类行为，通过滤波操作来增强与歌唱相关的频率成分，进一步改善模型对歌唱行为的表达。</p><p>(4) 扩散模型的集成：为了改进歌唱视频生成的性能，这些模块被集成到扩散模型中。扩散模型在这里起到了生成高质量视频的重要作用，而多尺度谱模块和谱滤波模块的引入，使得模型能够更好地处理歌唱音频特有的频率和振幅差异以及与之相关的行为表达。</p><p>(5) 数据集收集：为了推动研究，作者还收集了一个野外歌唱视频数据集。这个数据集为评估模型性能提供了可靠的基准，使得实验结果的对比和验证更加客观。</p><p>(6) 实验与结果：本文的方法在歌唱视频生成任务上取得了显著成果，生成的视频在客观和主观评估中都优于现有方法。实验结果表明，SINGER能够生成逼真的歌唱视频，验证了该方法的有效性和优越性。</p><p>希望这个回答能够满足您的要求！</p><ol><li>Conclusion:</li></ol><ul><li><p>(1)该工作的重要性和意义在于，它提出了一种基于多尺度谱的音频驱动歌唱视频生成方法（SINGER），填补了歌唱视频生成领域的空白。此方法能够为音乐、电影和游戏等多媒体应用领域提供真实的歌唱视频生成，从而增强用户体验和沉浸感。此外，该工作收集了一个野外歌唱视频数据集，为相关研究和模型评估提供了可靠的基准。</p></li><li><p>(2)创新点：本文提出了基于多尺度谱的音频驱动歌唱视频生成方法，引入了多尺度谱模块和谱滤波模块，有效处理了歌唱音频中特有的频率和振幅差异以及与之相关的行为表达。性能：在歌唱视频生成任务上，本文方法显著优于现有方法，生成的视频在客观和主观评估中都表现出优异性能。工作量：本文不仅提出了创新的方法，还收集了一个野外歌唱视频数据集，推动了相关研究的发展。然而，文章未详细阐述数据集的具体规模和组成，以及方法的计算效率和实际应用场景等，这是其不足之处。</p></li></ul><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-df8af6d8e36645b8a8a6fa09a945029c.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-7f59bef051a51555005786d63905af75.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-1b66b1ee51c9af96a6182c18b555850c.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-0ae23b443c3fb183517c9f206a1f4c12.jpg" align="middle"></details><h2 id="It-Takes-Two-Real-time-Co-Speech-Two-person’s-Interaction-Generation-via-Reactive-Auto-regressive-Diffusion-Model"><a href="#It-Takes-Two-Real-time-Co-Speech-Two-person’s-Interaction-Generation-via-Reactive-Auto-regressive-Diffusion-Model" class="headerlink" title="It Takes Two: Real-time Co-Speech Two-person’s Interaction Generation   via Reactive Auto-regressive Diffusion Model"></a>It Takes Two: Real-time Co-Speech Two-person’s Interaction Generation via Reactive Auto-regressive Diffusion Model</h2><p><strong>Authors:Mingyi Shi, Dafei Qin, Leo Ho, Zhouyingcheng Liao, Yinghao Huang, Junichi Yamagishi, Taku Komura</strong></p><p>Conversational scenarios are very common in real-world settings, yet existing co-speech motion synthesis approaches often fall short in these contexts, where one person’s audio and gestures will influence the other’s responses. Additionally, most existing methods rely on offline sequence-to-sequence frameworks, which are unsuitable for online applications. In this work, we introduce an audio-driven, auto-regressive system designed to synthesize dynamic movements for two characters during a conversation. At the core of our approach is a diffusion-based full-body motion synthesis model, which is conditioned on the past states of both characters, speech audio, and a task-oriented motion trajectory input, allowing for flexible spatial control. To enhance the model’s ability to learn diverse interactions, we have enriched existing two-person conversational motion datasets with more dynamic and interactive motions. We evaluate our system through multiple experiments to show it outperforms across a variety of tasks, including single and two-person co-speech motion generation, as well as interactive motion generation. To the best of our knowledge, this is the first system capable of generating interactive full-body motions for two characters from speech in an online manner.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02419v1">PDF</a> 15 pages, 10 figures</p><p><strong>Summary</strong><br>该研究提出了一种基于音频的自动回归系统，旨在合成对话中两个角色的动态动作。</p><p><strong>Key Takeaways</strong></p><ol><li>现有协同语音动作合成方法在对话场景中表现不佳。</li><li>大多数方法依赖离线序列到序列框架，不适合在线应用。</li><li>引入基于音频驱动的自动回归系统，合成对话中角色的动态动作。</li><li>核心为基于扩散的全身动作合成模型，条件化于角色的过去状态、语音音频和任务导向的动作轨迹。</li><li>通过丰富现有双人对话动作数据集，增强模型学习多样性互动的能力。</li><li>通过多种实验证明，系统在各种任务中表现优于现有方法。</li><li>该系统是首个能够从语音中在线生成两个角色交互全身动作的系统。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: 双人实时协同语音交互动作生成研究<br>Abstract Conversational scenarios are common in the real-world。基于实时协同语音的双人交互动作生成研究。</p></li><li><p>Authors: （根据提供的文章摘要无法找到所有作者名字）具体作者名称需要您进一步提供信息。</p></li><li><p>Affiliation: 作者所属机构未提及。</p></li><li><p>Keywords: 双人协同语音动作生成；实时生成；交互动作；扩散模型；自动回归系统。</p></li><li><p>Urls: 由于无法确定文章是否已发布在可链接的平台上或是否有GitHub代码库，因此无法提供链接。如有链接，请直接填入相关链接地址。</p></li><li><p>Summary:</p><ul><li><p>(1)研究背景：现有的双人协同语音动作生成方法在真实场景中表现不足，特别是在对话场景中，一个人的音频和手势会影响另一个人的回应。此外，大多数现有方法依赖于离线序列到序列的框架，不适合在线应用。因此，本文旨在解决这些问题，提出一种音频驱动的自回归系统，用于合成对话过程中的两个角色的动态动作。</p></li><li><p>(2)过去的方法与问题：现有的方法主要依赖于文本嵌入或简单的动作捕捉数据来生成动作，无法处理复杂的双人交互场景，并且在动态环境中缺乏灵活性。他们的问题在于无法充分利用音频信息来驱动动作生成，并且在多人交互场景中表现不佳。</p></li><li><p>(3)研究方法：本文提出了一种基于扩散模型的自回归系统，该系统以音频为驱动，并根据过去的状态、语音音频和任务导向的运动轨迹输入进行动作合成。为了提高模型的交互能力，作者丰富了双人对话动作数据集，包含更多动态和交互性的动作。</p></li><li><p>(4)任务与性能：本文的方法在单人及双人协同语音动作生成、交互动作生成等多个任务上表现出优异的性能。据作者所知，这是第一个能够在语音驱动下在线生成两个角色的互动全身动作的在线系统。性能结果表明，该方法在生成真实感、互动性方面取得了显著的进步。</p></li></ul></li></ol><p>希望以上回答能满足您的要求！</p><ol><li><p>方法论：</p><pre><code> - (1) 数据处理：
     本文首先对语音信号进行处理，转换为适合模型输入的格式。具体地，使用librosa库将语音信号转换为Mel频谱图，并增强节奏对齐。此外，本文还处理了语义信息，使用预训练的语音语言模型对语音数据进行标记化，提取语义特征。

 - (2) 轨迹合成：
     不同于之前的方法只生成静态的、站立时的动作，本文旨在生成更动态和交互性的动作。因此，本文首先合成轨迹，包括人物在地面上的位置和方向。利用扩散模型自动预测轨迹，并结合多种条件如语音、活动值和终点位置作为输入。

 - (3) 双流运动生成：
     本文提出了一种反应性的双流运动生成器，该生成器以自回归的方式生成运动。它采用扩散模型作为概率生成器，根据多种条件预测未来可能的动作。这些条件包括过去的动作、未来的轨迹、语音特征和伙伴的过去动作等。通过分离的条件令牌，模型可以专注于处理不同方面的输入数据，增强了模型的解释性和泛化能力。

 - (4) 交互式运动生成：
     为了产生更真实的交互动作，本文采用了一些策略来改善生成的运动。其中包括使用随机掩码来避免模型对伙伴动作的依赖，并应用分类器无关的引导（CFG）来平衡条件和无条件生成。此外，还采用了交替根位置归一化的方法，以在两人交互的场景中更好地处理相对位置关系。

 以上就是本文的方法论概述。
</code></pre></li><li>Conclusion:</li></ol><ul><li><strong>(1)</strong> 这项工作的意义是什么？<br>这项研究的意义在于它提供了一种从语音生成双人交互动作的新方法。这是第一个支持从语音实时生成两个角色的全身动作的自回归系统，展示了其在响应应用中的潜力。该研究有助于增强人机交互的自然性和实时性，对于改善对话系统、虚拟现实、电影制作等领域有着重要价值。</li><li><strong>(2)</strong> 从创新点、性能、工作量三个方面总结本文的优缺点是什么？<br>创新点：该研究提出了一种基于扩散模型的自回归系统，用于从语音驱动合成对话过程中的两个角色的动态动作。该系统在双人协同语音动作生成和交互动作生成方面表现出显著的进步，尤其是其能够在线实时生成两个角色的互动全身动作，这是前所未有的。</li></ul><p>性能：在单人及双人协同语音动作生成、交互动作生成等多个任务上，该方法表现出优异的性能。特别是在生成真实感和互动性方面，取得了显著的进步。</p><p>工作量：文章中对方法的实现和实验进行了详细的描述，展示了作者们在该领域深入的研究和丰富的实践经验。然而，关于作者所属机构和具体作者的信息未提及，这可能对于评估工作量带来一定的影响。此外，文章未提供源代码和数据集的链接，难以独立验证其方法和性能。</p><p>总体来说，这篇文章在创新点和性能方面都表现出色，但在工作量和可重复性方面存在一些不足。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-d0f6255b72b8539d8aab13e42acd7a48.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-54ef72cfbbecac1eb389d95bcf9efdce.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-023425356e8d0b3df9cd25fa3d3bf131.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-8cf361129e15b5b04e85bcc554daf426.jpg" align="middle"></details><h2 id="Think-to-Talk-or-Talk-to-Think-When-LLMs-Come-Up-with-an-Answer-in-Multi-Step-Reasoning"><a href="#Think-to-Talk-or-Talk-to-Think-When-LLMs-Come-Up-with-an-Answer-in-Multi-Step-Reasoning" class="headerlink" title="Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in   Multi-Step Reasoning"></a>Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in Multi-Step Reasoning</h2><p><strong>Authors:Keito Kudo, Yoichi Aoki, Tatsuki Kuribayashi, Shusaku Sone, Masaya Taniguchi, Ana Brassard, Keisuke Sakaguchi, Kentaro Inui</strong></p><p>This study investigates the internal reasoning mechanism of language models during symbolic multi-step reasoning, motivated by the question of whether chain-of-thought (CoT) outputs are faithful to the model’s internals. Specifically, we inspect when they internally determine their answers, particularly before or after CoT begins, to determine whether models follow a post-hoc “think-to-talk” mode or a step-by-step “talk-to-think” mode of explanation. Through causal probing experiments in controlled arithmetic reasoning tasks, we found systematic internal reasoning patterns across models; for example, simple subproblems are solved before CoT begins, and more complicated multi-hop calculations are performed during CoT.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01113v1">PDF</a></p><p><strong>Summary</strong><br>研究揭示语言模型在符号多步推理中的内部推理机制，并探讨思维链（CoT）输出是否忠实反映模型内部状态。</p><p><strong>Key Takeaways</strong></p><ol><li>探讨语言模型内部推理机制在符号多步推理中的运作。</li><li>分析思维链（CoT）输出与模型内部状态的一致性。</li><li>调查模型在何时内部确定答案：CoT开始前或后。</li><li>区分“事后思考-说话”模式和“说话-思考”模式。</li><li>通过因果探查实验，在控制算术推理任务中进行分析。</li><li>发现模型存在系统性的内部推理模式。</li><li>简单子问题在CoT开始前解决，复杂多跳计算在CoT中进行。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: 思考与对话：语言模型的多步推理内部机制探究<br>Authors: Keito Kudo, Yoichi Aoki, Tatsuki Kuribayashi, Shusaku Sone, Masaya Taniguchi, Ana Brassard, Keisuke Sakaguchi, Kentaro Inui</p></li><li><p>Affiliation: 第一作者Keito Kudo的隶属机构为东北大学（Tohoku University）。</p></li><li><p>Keywords: Language Model, Internal Reasoning Mechanism, Chain-of-Thought, Multi-Step Reasoning, Post-hoc Explanation, Talk-to-Think</p></li><li><p>Urls: <a href="链接地址">论文链接</a> ，GitHub代码链接（如果有的话，填入具体链接，如果没有则填写”GitHub:None”）</p></li><li><p>Summary:</p><ul><li><p>(1)研究背景：随着大型语言模型（LLMs）的广泛应用，对其内部推理机制的探究变得至关重要。特别是在多步推理任务中，语言模型的内部如何逐步得出结论是一个核心问题。本文旨在探究语言模型在符号多步推理中的内部推理机制，以及它们的输出是否是忠实于其内部操作的。</p></li><li><p>(2)过去的方法及问题：之前的研究主要关注简单的算术推理，未能全面探究语言模型在复杂多步推理任务中的内部机制。此外，对于语言模型的输出是否是事后解释还是逐步推理的解释也存在争议。</p></li><li><p>(3)研究方法：本研究通过因果探测实验，在控制算术推理任务中观察语言模型的内部状态。实验设计了十个语言模型，通过训练分类器预测实例中的变量值来观察模型的内部推理流程。同时，通过因果干预分析模型内部的推理模式。</p></li><li><p>(4)任务与性能：实验结果表明，语言模型在解决简单单步推理问题方面，会在思维链（CoT）开始之前完成计算；而在解决更复杂的多步推理问题时，会在CoT过程中进行计算。通过因果干预分析发现，预先确定的答案对最终答案有影响，但它们的因果关系相对间接。总体而言，本研究揭示了语言模型在解决多步推理任务时采用的混合推理模式，即在“思考后说话”（think-to-talk）和“说话后思考”（talk-to-think）模式之间的系统性内部推理模式。实验性能表明该研究方法的有效性和实用性。</p></li></ul></li><li>Conclusion:</li></ol><p>(1) 研究意义：该研究对于理解大型语言模型在多步推理任务中的内部机制具有重要意义。通过探究语言模型在符号多步推理中的内部推理机制，有助于提升语言模型的性能并推动其在复杂任务中的应用。同时，该研究对于理解人工智能的推理过程也具有一定的启示作用。</p><p>(2) 创新点、性能、工作量总结：</p><p>创新点：文章通过实验探究了语言模型在多步推理任务中的内部推理机制，特别是通过因果探测实验和因果干预分析揭示了语言模型的混合推理模式。此外，文章还通过训练分类器预测实例中的变量值来观察模型的内部推理流程，这是一种新的研究方法。</p><p>性能：实验结果表明，语言模型在解决多步推理任务时表现出较高的性能，通过揭示其内部推理机制，有助于优化语言模型的性能。此外，该研究还通过实验验证了其研究方法的有效性和实用性。</p><p>工作量：文章进行了大量的实验和数据分析，包括设计实验、收集数据、分析模型内部状态等。同时，文章还进行了深入的文献调研和理论分析，对语言模型的多步推理内部机制进行了全面的探究。</p><p>总体而言，该文章在探究语言模型多步推理内部机制方面取得了一定的成果，对于理解语言模型的推理过程和优化其性能具有一定的启示作用。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-1aac80b235c41063d4bc2be457b14f10.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-27e10e9a1a4d4497f5830202a805616e.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-214c0edb7fcf5bee1f7a257d234aa375.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-b5edd430917bcd00410b5908e0283621.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-70aed0c3e4b806717869edd77641d0cd.jpg" align="middle"></details><h2 id="One-Shot-One-Talk-Whole-body-Talking-Avatar-from-a-Single-Image"><a href="#One-Shot-One-Talk-Whole-body-Talking-Avatar-from-a-Single-Image" class="headerlink" title="One Shot, One Talk: Whole-body Talking Avatar from a Single Image"></a>One Shot, One Talk: Whole-body Talking Avatar from a Single Image</h2><p><strong>Authors:Jun Xiang, Yudong Guo, Leipeng Hu, Boyang Guo, Yancheng Yuan, Juyong Zhang</strong></p><p>Building realistic and animatable avatars still requires minutes of multi-view or monocular self-rotating videos, and most methods lack precise control over gestures and expressions. To push this boundary, we address the challenge of constructing a whole-body talking avatar from a single image. We propose a novel pipeline that tackles two critical issues: 1) complex dynamic modeling and 2) generalization to novel gestures and expressions. To achieve seamless generalization, we leverage recent pose-guided image-to-video diffusion models to generate imperfect video frames as pseudo-labels. To overcome the dynamic modeling challenge posed by inconsistent and noisy pseudo-videos, we introduce a tightly coupled 3DGS-mesh hybrid avatar representation and apply several key regularizations to mitigate inconsistencies caused by imperfect labels. Extensive experiments on diverse subjects demonstrate that our method enables the creation of a photorealistic, precisely animatable, and expressive whole-body talking avatar from just a single image.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01106v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://ustc3dv.github.io/OneShotOneTalk/">https://ustc3dv.github.io/OneShotOneTalk/</a></p><p><strong>Summary</strong><br>从单张图片构建全身体验式对话头像，实现动态建模与新型手势表达。</p><p><strong>Key Takeaways</strong></p><ul><li>单图像构建全身体验式对话头像</li><li>复杂动态建模与新型手势表达</li><li>利用姿态引导图像到视频扩散模型生成伪标签</li><li>3DGS-mesh混合头像表示及关键正则化</li><li>生成逼真、精确且表情丰富的全身体验式头像</li><li>多样化实验证明方法有效性</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><strong>标题</strong>：基于单幅图像的全身动态说话人模型构建技术。</li></ol><p><strong>中文翻译</strong>：基于单张图像的全动态说话人模型构建研究。</p><ol><li><p><strong>作者名单及英文名称</strong>：Yunxiang (Jun) Xiang、Yudong Guo、Leipeng Hu、Boyang Guo、Yancheng Yuan等。</p></li><li><p><strong>作者所属单位中文翻译</strong>：主要作者来自中国科学技术大学和香港理工大学。</p></li><li><p><strong>关键词</strong>：Single-image Avatar Creation, Dynamic Modeling, Expression Control, Pose-guided Image-to-Video Diffusion Models等。</p></li><li><p><strong>链接</strong>：论文链接：<a href="链接地址">点击这里进入论文链接</a>，GitHub代码链接：GitHub:None（如果可用的话，请填写具体的GitHub链接）。</p></li><li><p><strong>摘要</strong>：</p><ul><li><p>(1) 研究背景：随着增强现实和虚拟现实技术的发展，创建真实感和可控制性的全身动态说话人模型变得越来越重要。然而，现有的方法通常需要多视角或单视角旋转视频，且缺乏对姿态和表情的精确控制。本文旨在从单幅图像构建全身动态说话人模型，解决上述问题。</p></li><li><p>(2) 过去的方法及其问题：现有创建全身动态说话人模型的方法大多依赖于多视角视频输入，过程复杂且对设备要求较高。它们缺乏从单幅图像中精确提取个性化细节和动态信息的能力。</p></li><li><p>(3) 研究方法：本文提出了一种新的方法，通过利用姿态引导的图像到视频的扩散模型生成伪标签视频帧，解决了复杂动态建模和泛化到新姿态和表情的问题。为了解决由不一致和噪声伪视频引起的不一致性，引入了紧密耦合的3DGS网格混合模型表示和关键正则化技术。</p></li><li><p>(4) 任务与性能：本文的方法在多样化的主体上进行了广泛实验，证明了从单幅图像创建逼真、可精确控制的全身动态说话人模型的可行性。该方法的性能达到了创建高质量全身说话人模型的目标，具有广泛的应用前景，特别是在AR/VR领域。</p></li></ul></li></ol><p>希望以上总结符合您的要求！如有需要调整的地方，请告知。</p><ol><li>方法论：</li></ol><p>这篇论文提出了一种基于单幅图像构建全身动态说话人模型的技术。以下是具体的方法论步骤：</p><p>（1）首先，提出了一个紧密耦合的3DGS网格混合模型表示法（Sec. 3.1）。该模型旨在解决从单幅图像中重建复杂动态模型的问题，从而继承了目标人的身份并使其能够进行自然动画。这一步骤是整个方法的核心，为后续的任务提供了基础。</p><p>（2）为了泛化到多种姿态和面部动作，通过驱动目标人的生成模型生成不完美的视频序列（Sec. 3.2）。这里的关键是利用扩散模型生成伪标签视频帧，并通过重新跟踪过程获得更准确的姿态参数。这一步骤确保了模型能够处理各种动态场景。</p><p>（3）最后，通过精心设计约束条件和损失函数来训练模型（Sec. 3.3）。这些约束和损失函数用于稳定头像重建过程并从输入的单张图像和不完美的伪标签中提取正确的信息。这包括应用于网格的约束损失、网格与高斯之间的一致性损失以及高斯平滑损失等。这些损失函数帮助模型更好地学习和泛化。</p><p>整个方法的流程概述如图2所示，通过一系列步骤将单幅图像转化为逼真的全身动态说话人模型。该方法的性能在多样化的主体上进行了广泛实验，证明了其创建逼真、可精确控制的全身动态说话人模型的可行性。</p><ol><li><p>Conclusion:</p><ul><li><p>(1) 这项工作的意义在于为基于单张图像的全身动态说话人模型构建提供了新技术。该研究对于增强现实和虚拟现实领域具有重要的应用价值，有助于创建逼真的动态角色模型，推动相关领域的技术发展。</p></li><li><p>(2) Innovation point（创新点）：该论文提出了一种新的基于单幅图像的全身动态说话人模型构建方法，通过姿态引导的图像到视频的扩散模型生成伪标签视频帧，解决了复杂动态建模的问题。此外，论文还引入了紧密耦合的3DGS网格混合模型表示和关键正则化技术，以处理由不一致和噪声伪视频引起的问题。<br>Performance（性能）：该论文的方法在多样化的主体上进行了广泛实验，证明了从单幅图像创建逼真、可精确控制的全身动态说话人模型的可行性。其性能达到了创建高质量全身说话人模型的目标，具有广泛的应用前景。<br>Workload（工作量）：从摘要中并未明确提及具体的工作量，因此无法对工作量进行评估。</p></li></ul></li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-e8a41f81918253ee098bb169823e20c1.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-34d39343b51c7d59c5b102666c05390e.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-c32c46c43b89ffb1045c65076ff3f13c.jpg" align="middle"></details><h2 id="FLOAT-Generative-Motion-Latent-Flow-Matching-for-Audio-driven-Talking-Portrait"><a href="#FLOAT-Generative-Motion-Latent-Flow-Matching-for-Audio-driven-Talking-Portrait" class="headerlink" title="FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking   Portrait"></a>FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking Portrait</h2><p><strong>Authors:Taekyung Ki, Dongchan Min, Gyeongsu Chae</strong></p><p>With the rapid advancement of diffusion-based generative models, portrait image animation has achieved remarkable results. However, it still faces challenges in temporally consistent video generation and fast sampling due to its iterative sampling nature. This paper presents FLOAT, an audio-driven talking portrait video generation method based on flow matching generative model. We shift the generative modeling from the pixel-based latent space to a learned motion latent space, enabling efficient design of temporally consistent motion. To achieve this, we introduce a transformer-based vector field predictor with a simple yet effective frame-wise conditioning mechanism. Additionally, our method supports speech-driven emotion enhancement, enabling a natural incorporation of expressive motions. Extensive experiments demonstrate that our method outperforms state-of-the-art audio-driven talking portrait methods in terms of visual quality, motion fidelity, and efficiency.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01064v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://deepbrainai-research.github.io/float/">https://deepbrainai-research.github.io/float/</a></p><p><strong>Summary</strong><br>基于流动匹配生成模型，FLOAT通过音频驱动实现面部表情视频生成，提高时序一致性和效率。</p><p><strong>Key Takeaways</strong></p><ol><li>扩散模型在肖像图像动画领域取得显著进展。</li><li>面临时序一致视频生成和快速采样挑战。</li><li>FLOAT利用音频驱动，基于流动匹配生成模型。</li><li>将生成模型从像素潜在空间转移到学习到的运动潜在空间。</li><li>引入基于Transformer的向量场预测器。</li><li>简单有效的帧级条件机制。</li><li>支持语音驱动的情感增强。</li><li>在视觉质量、运动保真度和效率方面优于现有方法。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: 基于流匹配的音频驱动动态肖像视频生成研究（FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking）</p></li><li><p>Authors: Taekyung Ki, Dongchan Min, Gyeongsu Chae.</p></li><li><p>Affiliation: Taekyung Ki, Dongchan Min, and Gyeongsu Chae are affiliated with DeepBrain AI Inc.</p></li><li><p>Keywords: audio-driven talking portrait video generation, generative motion latent flow matching, motion capture, deep learning, dynamic portrait generation.</p></li><li><p>Urls: <a target="_blank" rel="noopener" href="https://deepbrainai-research.github.io/float/">https://deepbrainai-research.github.io/float/</a>; Github Code Link: None</p></li><li><p>Summary:</p></li></ol><p>(1) 研究背景：随着扩散基生成模型的快速发展，肖像图像动画已经取得了显著的结果。然而，它仍然面临着在时间上一致的视频生成和快速采样等挑战。本文旨在解决这些问题，提出了一种基于流匹配的音频驱动动态肖像视频生成方法。</p><p>(2) 过去的方法及其问题：早期的研究主要集中在通过音频信号生成准确唇部运动的方法上。虽然这些方法能够在一定程度上生成动态肖像视频，但它们缺乏全面的运动范围，并且生成的运动的表达性有限。随后的一些研究尝试引入概率生成模型来解决这个问题，但仍然存在生成的运 动缺乏表现力的问题。</p><p>(3) 研究方法：本文提出了一种基于流匹配的音频驱动动态肖像视频生成方法。该方法将生成建模从像素级的潜在空间转移到学习到的运动潜在空间，从而实现高效的时序一致运动设计。为实现这一目标，引入了基于变压器的向量场预测器，并采用了简单有效的帧级条件机制。此外，该方法还支持语音驱动的情绪增强，能够自然地融入表达性运动。</p><p>(4) 任务与性能：本文的方法在音频驱动的谈话肖像任务上进行了实验，并表现出优异的性能，包括视觉质量、运动保真度和效率等方面。与现有的音频驱动谈话肖像方法相比，该方法取得了显著的性能提升。实验结果支持了该方法的有效性。</p><ol><li>Methods:</li></ol><p>(1) 研究背景分析：该研究首先分析了当前音频驱动动态肖像视频生成的背景，包括扩散基生成模型的快速发展以及所面临的挑战，如时间上一致的视频生成和快速采样等。</p><p>(2) 对过去方法的评估与问题识别：研究团队对早期通过音频信号生成唇部运动的方法进行了评估，发现这些方法虽然能够生成动态肖像视频，但存在运动范围不全面、表达性有限的问题。随后的一些研究尝试引入概率生成模型来解决这个问题，但仍然存在运动表现力不足的问题。</p><p>(3) 方法论创新点：针对以上问题，研究团队提出了一种基于流匹配的音频驱动动态肖像视频生成方法。该方法将生成建模从像素级的潜在空间转移到学习到的运动潜在空间，从而实现高效的时序一致运动设计。方法主要包括以下步骤：</p><pre><code>- (1) 引入基于变压器的向量场预测器，用于预测运动矢量场。

- (2) 采用简单有效的帧级条件机制，确保生成的视频在时间上的一致性。

- (3) 支持语音驱动的情绪增强，通过融入表达性运动，使生成的视频更具真实感和情感表达。
</code></pre><p>(4) 实验设计与性能评估：为验证该方法的有效性，研究团队在音频驱动的谈话肖像任务上进行了实验，并从视觉质量、运动保真度和效率等方面对方法性能进行了评估。实验结果表明，该方法在音频驱动的谈话肖像任务上取得了显著的性能提升。</p><ol><li>Conclusion:</li></ol><p>(1)研究意义：该研究针对当前音频驱动动态肖像视频生成领域面临的挑战，提出了一种基于流匹配的音频驱动动态肖像视频生成方法，具有重要的学术价值和应用前景。</p><p>(2)创新点、性能、工作量总结：</p><pre><code>- 创新点：该研究将生成建模从像素级的潜在空间转移到学习到的运动潜在空间，实现了高效的时序一致运动设计。引入基于变压器的向量场预测器，并采用简单有效的帧级条件机制，支持语音驱动的情绪增强。
- 性能：该研究在音频驱动的谈话肖像任务上进行了实验，表现出优异的性能，包括视觉质量、运动保真度和效率等方面。与现有的音频驱动谈话肖像方法相比，取得了显著的性能提升。
- 工作量：该研究进行了详尽的实验设计和性能评估，并详细阐述了方法的实现细节和理论分析。然而，关于该研究方法的实际应用效果和更广泛的场景应用，可能还需要更多的实验和验证。
</code></pre><p>总体而言，该研究在音频驱动动态肖像视频生成领域取得了重要的进展，具有潜在的应用价值。但也需要进一步的研究和实验来验证其在实际场景中的效果。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-4c48114dadf2693ebec847e1f4161b7e.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-932be8d0b3a67e4d27e3b20089557ffb.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-8c0ae55682bf86c0002a9cdf127c986e.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-c66bfe10dfd878860a466abb92c19030.jpg" align="middle"></details><h2 id="Synergizing-Motion-and-Appearance-Multi-Scale-Compensatory-Codebooks-for-Talking-Head-Video-Generation"><a href="#Synergizing-Motion-and-Appearance-Multi-Scale-Compensatory-Codebooks-for-Talking-Head-Video-Generation" class="headerlink" title="Synergizing Motion and Appearance: Multi-Scale Compensatory Codebooks   for Talking Head Video Generation"></a>Synergizing Motion and Appearance: Multi-Scale Compensatory Codebooks for Talking Head Video Generation</h2><p><strong>Authors:Shuling Zhao, Fa-Ting Hong, Xiaoshui Huang, Dan Xu</strong></p><p>Talking head video generation aims to generate a realistic talking head video that preserves the person’s identity from a source image and the motion from a driving video. Despite the promising progress made in the field, it remains a challenging and critical problem to generate videos with accurate poses and fine-grained facial details simultaneously. Essentially, facial motion is often highly complex to model precisely, and the one-shot source face image cannot provide sufficient appearance guidance during generation due to dynamic pose changes. To tackle the problem, we propose to jointly learn motion and appearance codebooks and perform multi-scale codebook compensation to effectively refine both the facial motion conditions and appearance features for talking face image decoding. Specifically, the designed multi-scale motion and appearance codebooks are learned simultaneously in a unified framework to store representative global facial motion flow and appearance patterns. Then, we present a novel multi-scale motion and appearance compensation module, which utilizes a transformer-based codebook retrieval strategy to query complementary information from the two codebooks for joint motion and appearance compensation. The entire process produces motion flows of greater flexibility and appearance features with fewer distortions across different scales, resulting in a high-quality talking head video generation framework. Extensive experiments on various benchmarks validate the effectiveness of our approach and demonstrate superior generation results from both qualitative and quantitative perspectives when compared to state-of-the-art competitors.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00719v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://shaelynz.github.io/synergize-motion-appearance/">https://shaelynz.github.io/synergize-motion-appearance/</a></p><p><strong>Summary</strong><br>提出了一种基于多尺度代码簿补偿的说话人头视频生成方法，以实现更精确的姿势和精细的面部细节。</p><p><strong>Key Takeaways</strong></p><ol><li>目标是生成与现实人物身份相符的说话人头视频。</li><li>生成过程中需同时保证准确的姿势和精细的面部细节。</li><li>面部运动复杂，单次源人脸图像无法提供足够的姿态变化指导。</li><li>联合学习运动和外观代码簿，进行多尺度代码簿补偿。</li><li>代码簿学习在统一的框架中同时进行，存储面部运动流和外观模式。</li><li>引入基于Transformer的代码簿检索策略，实现联合运动和外观补偿。</li><li>方法在多个基准测试中验证有效，生成结果优于现有技术。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p><strong>标题</strong>：<br>协同运动与外观：多尺度补偿代码库用于谈话头部视频生成</p></li><li><p><strong>作者</strong>：<br>赵舒琳^1^, 洪法廷^1^, 黄晓水^2^, 徐丹^1*</p></li><li><p><strong>作者隶属机构</strong>：<br>^1香港科技大学；^2上海交通大学</p></li><li><p><strong>关键词</strong>：<br>Talking Head Video Generation（谈话头视频生成）, Motion Estimation（运动估计）, Appearance Representation（外观表示）, Codebook（代码库）, Multi-scale Compensation（多尺度补偿）</p></li><li><p><strong>链接</strong>：<br>论文链接：<a target="_blank" rel="noopener" href="https://shaelynz.github.io/synergize-motion-appearance/">点击此处访问论文</a><br>GitHub代码链接：GitHub:None（若存在代码仓库，请在此处填入）</p></li><li><p><strong>摘要</strong>：</p></li></ol><ul><li><strong>(1) 研究背景</strong>：<br>谈话头视频生成是当前计算机视觉领域的一个热门研究方向。其目标是从源图像生成一个真实的谈话视频，同时保留人的身份和来自驱动视频的运动信息。尽管已有许多方法在该领域取得了进展，但仍然存在挑战，特别是在生成具有准确姿势和精细面部细节的视频时。</li><li><strong>(2) 过去的方法及其问题</strong>：<br>现有方法主要关注运动估计和外观表示的学习。然而，由于面部运动的复杂性和源图像提供的信息有限，这些方法在生成高质量视频时面临挑战。文章指出了现有方法在处理局部细微运动、动态和复杂运动时的问题，以及在处理遮挡区域或细微表情变化时的外观信息不足的问题。</li><li><strong>(3) 研究方法</strong>：<br>针对上述问题，本文提出了一个联合学习运动与外观代码库的方法，并进行了多尺度代码库补偿。具体而言，设计了一个统一框架，同时学习多尺度运动与外观代码库，以存储代表性的全局面部运动流和外观模式。然后，引入了一个新颖的多尺度运动和外观补偿模块，利用基于变压器的代码库检索策略，从两个代码库中查询互补信息进行联合运动和外观补偿。整个过程产生了更大灵活性的运动流和较少失真的外观特征，从而建立一个高质量的谈话头视频生成框架。</li><li><strong>(4) 任务与性能</strong>：<br>本文的方法在多种基准测试上进行了验证，并与最先进的方法进行了比较。实验结果表明，本文方法在定性和定量评估方面都取得了优越的结果。所提出的方法在谈话头视频生成任务中实现了高性能，有效支持了其目标。</li></ul><p>以上就是对该论文的概括，希望能够帮助您理解该论文的主要内容。</p><ol><li>Methods:</li></ol><p><em>(1) 研究背景和目标确定</em>：<br>针对谈话头视频生成领域中的挑战，特别是生成具有准确姿势和精细面部细节的视频时的问题，本文旨在开发一个能够从源图像生成真实谈话视频的方法，同时保留人的身份和来自驱动视频的运动信息。</p><p><em>(2) 分析现有方法的问题</em>：<br>现有方法主要关注运动估计和外观表示的学习，但在处理局部细微运动、动态和复杂运动时存在问题，同时在处理遮挡区域或细微表情变化时的外观信息不足。文章深入剖析了这些问题并指出其局限性。</p><p><em>(3) 构建统一框架以学习多尺度代码库</em>：<br>设计了一个统一框架，用以同时学习多尺度运动与外观代码库。这些代码库存储了代表性的全面部运动流和外观模式。通过这一框架，可以有效地捕捉并表达复杂的面部运动和外观变化。</p><p><em>(4) 引入多尺度补偿模块</em>：<br>为解决现有方法的不足，文章创新性地引入了一个多尺度运动和外观补偿模块。该模块利用基于变压器的代码库检索策略，从两个代码库中查询互补信息进行联合运动和外观补偿。这一策略增强了运动流的灵活性和外观特征的真实性，从而提高了生成视频的质量。</p><p><em>(5) 实验验证与性能评估</em>：<br>本文的方法在多种基准测试上进行了验证，并与最先进的方法进行了比较。实验结果表明，本文方法在定性和定量评估方面都取得了优越的结果，证明了所提出方法在谈话头视频生成任务中的高性能。</p><ol><li>Conclusion:</li></ol><ul><li>(1)该工作的意义在于解决谈话头视频生成领域的挑战，特别是生成具有准确姿势和精细面部细节的视频时的难题。该研究提出了一种新的方法，通过联合学习运动与外观代码库，进行多尺度补偿，从而提高了谈话头视频生成的质量。</li><li>(2)创新点：本文设计了一个统一框架，同时学习多尺度运动与外观代码库，并引入了一个多尺度运动和外观补偿模块，利用基于变压器的代码库检索策略，从两个代码库中查询互补信息进行联合运动和外观补偿。</li><li>性能：在多种基准测试上进行了验证，并与最先进的方法进行了比较，实验结果表明，本文方法在定性和定量评估方面都取得了优越的结果，证明了所提出方法在谈话头视频生成任务中的高性能。</li><li>工作量：文章进行了大量的实验和验证，证明了所提出方法的有效性。同时，文章还提供了详细的框架和算法描述，为相关领域的研究者提供了有益的参考。</li></ul><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-ee45a95e505c933e8f3275221c0fb6d4.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-f2c6d35af8762aadf1dce9683dbfde47.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-049c43bb4b28916640f409cebae20104.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-1fd161258dafe207e2c8c8a3661e05bf.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-eadaf7a550705d187fbdf5d2ecc313c9.jpg" align="middle"></details></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io/Paper/2024-12-06/Talking%20Head%20Generation/">https://kedreamix.github.io/Paper/2024-12-06/Talking Head Generation/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Talking-Head-Generation/">Talking Head Generation</a></div><div class="post_share"><div class="social-share" data-image="https://picx.zhimg.com/v2-214c0edb7fcf5bee1f7a257d234aa375.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/Paper/2024-12-06/NeRF/" title="NeRF"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-795f278885cf99cdb1d0990deba9567b.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">NeRF</div></div></a></div><div class="next-post pull-right"><a href="/Paper/2024-12-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙/虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-dc9887f1f37b328dfd11d4a1513a778b.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">元宇宙/虚拟人</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/Note/BlendShape/" title="Blendshape学习笔记"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://p6-sign.toutiaoimg.com/pgc-image/2c8cbd123e00470e95500a8ae62da605~noop.image?_iz=58558&from=article.pc_detail&lk3s=953192f4&x-expires=1710668214&x-signature=UHPhjWP4v96kbtfJzF97Z%2Bp3klc%3D" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-11</div><div class="title">Blendshape学习笔记</div></div></a></div><div><a href="/Paper/Awesome-Talking-Head-Synthesis/" title="超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-01</div><div class="title">超赞的数字人生成知识库 Awesome-Talking-Head-Synthesis</div></div></a></div><div><a href="/Project/Linly-Talker%20-%20GPT-SoVITS/" title="数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-20</div><div class="title">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</div></div></a></div><div><a href="/Project/SyncTalk/" title="SyncTalk实验笔记"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-3866dff2d07194c235eefab923f694c5.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-18</div><div class="title">SyncTalk实验笔记</div></div></a></div><div><a href="/Project/Linly-Talker/" title="数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-20</div><div class="title">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</div></div></a></div><div><a href="/Paperscape/EMO/" title="EMO Emote Portrait Alive - 阿里HumanAIGC"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-6492e24fb03ffa98135dc584535ab7d9.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-03</div><div class="title">EMO Emote Portrait Alive - 阿里HumanAIGC</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-12-06-%E6%9B%B4%E6%96%B0"><span class="toc-text">2024-12-06 更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#SINGER-Vivid-Audio-driven-Singing-Video-Generation-with-Multi-scale-Spectral-Diffusion-Model"><span class="toc-text">SINGER: Vivid Audio-driven Singing Video Generation with Multi-scale Spectral Diffusion Model</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#It-Takes-Two-Real-time-Co-Speech-Two-person%E2%80%99s-Interaction-Generation-via-Reactive-Auto-regressive-Diffusion-Model"><span class="toc-text">It Takes Two: Real-time Co-Speech Two-person’s Interaction Generation via Reactive Auto-regressive Diffusion Model</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Think-to-Talk-or-Talk-to-Think-When-LLMs-Come-Up-with-an-Answer-in-Multi-Step-Reasoning"><span class="toc-text">Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in Multi-Step Reasoning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#One-Shot-One-Talk-Whole-body-Talking-Avatar-from-a-Single-Image"><span class="toc-text">One Shot, One Talk: Whole-body Talking Avatar from a Single Image</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#FLOAT-Generative-Motion-Latent-Flow-Matching-for-Audio-driven-Talking-Portrait"><span class="toc-text">FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking Portrait</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Synergizing-Motion-and-Appearance-Multi-Scale-Compensatory-Codebooks-for-Talking-Head-Video-Generation"><span class="toc-text">Synergizing Motion and Appearance: Multi-Scale Compensatory Codebooks for Talking Head Video Generation</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image:url('https://picx.zhimg.com/v2-214c0edb7fcf5bee1f7a257d234aa375.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js").then((()=>{pangu.autoSpacingPage()}))}function panguInit(){panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.1},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script></div><script id="canvas_nest" defer color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><script data-pjax>function butterfly_swiper_injector_config(){var a=document.getElementById("recent-posts");console.log("已挂载butterfly_swiper"),a.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="Project/Linly-Talker - GPT-SoVITS/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="Project/Linly-Talker - GPT-SoVITS/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script></body></html>