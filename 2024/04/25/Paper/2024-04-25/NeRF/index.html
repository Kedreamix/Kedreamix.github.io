<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>NeRF | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="NeRF 方向最新论文已更新，请持续关注 Update in 2024-04-25  GaussianTalker Speaker-specific Talking Head Synthesis via 3D Gaussian   Splatting"><meta property="og:type" content="article"><meta property="og:title" content="NeRF"><meta property="og:url" content="https://kedreamix.github.io/2024/04/25/Paper/2024-04-25/NeRF/index.html"><meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World"><meta property="og:description" content="NeRF 方向最新论文已更新，请持续关注 Update in 2024-04-25  GaussianTalker Speaker-specific Talking Head Synthesis via 3D Gaussian   Splatting"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://picx.zhimg.com/v2-726da3cc31a9a838b18bd0268191d0f9.jpg"><meta property="article:published_time" content="2024-04-25T13:35:10.000Z"><meta property="article:modified_time" content="2024-04-25T13:35:10.572Z"><meta property="article:author" content="Kedreamix"><meta property="article:tag" content="NeRF"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://picx.zhimg.com/v2-726da3cc31a9a838b18bd0268191d0f9.jpg"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/2024/04/25/Paper/2024-04-25/NeRF/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-74LZ5BEQQ1")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!0,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"简"},noticeOutdate:{limitDay:500,position:"top",messagePrev:"It has been",messageNext:"days since the last update, the content of the article may be outdated."},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:200},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!0,post:!0},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!0,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"NeRF",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-04-25 21:35:10"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach((e=>{n.setAttribute(e,t[e])})),document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)})),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=24?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 7.0.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml"></head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{const d=document.getElementById("loading-box"),e=document.body,o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l(),window.addEventListener("load",(()=>{o()}))})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">285</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">17</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url('https://picx.zhimg.com/v2-726da3cc31a9a838b18bd0268191d0f9.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">NeRF</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-04-25T13:35:10.000Z" title="发表于 2024-04-25 21:35:10">2024-04-25</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-04-25T13:35:10.572Z" title="更新于 2024-04-25 21:35:10">2024-04-25</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">6.4k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>22分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="NeRF"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-25-更新"><a href="#2024-04-25-更新" class="headerlink" title="2024-04-25 更新"></a>2024-04-25 更新</h1><h2 id="GaussianTalker-Speaker-specific-Talking-Head-Synthesis-via-3D-Gaussian-Splatting"><a href="#GaussianTalker-Speaker-specific-Talking-Head-Synthesis-via-3D-Gaussian-Splatting" class="headerlink" title="GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian   Splatting"></a>GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian Splatting</h2><p><strong>Authors:Hongyun Yu, Zhan Qu, Qihang Yu, Jianchuan Chen, Zhonghua Jiang, Zhiwen Chen, Shengyu Zhang, Jimin Xu, Fei Wu, Chengfei Lv, Gang Yu</strong></p><p>Recent works on audio-driven talking head synthesis using Neural Radiance Fields (NeRF) have achieved impressive results. However, due to inadequate pose and expression control caused by NeRF implicit representation, these methods still have some limitations, such as unsynchronized or unnatural lip movements, and visual jitter and artifacts. In this paper, we propose GaussianTalker, a novel method for audio-driven talking head synthesis based on 3D Gaussian Splatting. With the explicit representation property of 3D Gaussians, intuitive control of the facial motion is achieved by binding Gaussians to 3D facial models. GaussianTalker consists of two modules, Speaker-specific Motion Translator and Dynamic Gaussian Renderer. Speaker-specific Motion Translator achieves accurate lip movements specific to the target speaker through universalized audio feature extraction and customized lip motion generation. Dynamic Gaussian Renderer introduces Speaker-specific BlendShapes to enhance facial detail representation via a latent pose, delivering stable and realistic rendered videos. Extensive experimental results suggest that GaussianTalker outperforms existing state-of-the-art methods in talking head synthesis, delivering precise lip synchronization and exceptional visual quality. Our method achieves rendering speeds of 130 FPS on NVIDIA RTX4090 GPU, significantly exceeding the threshold for real-time rendering performance, and can potentially be deployed on other hardware platforms.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.14037v1">PDF</a></p><p><strong>摘要</strong><br>高斯体态合成方法结合了神经辐射场和 3D 高斯体积表征，实现了精确的唇部运动和逼真的渲染视频。</p><p><strong>关键要点</strong></p><ul><li>使用3D高斯体积表征实现面部运动的直观控制。</li><li>扬声器特定的运动转换器通过定制唇部运动生成，实现准确的唇部运动。</li><li>动态高斯渲染器通过潜在姿势引入扬声器特定的混合形状，以增强面部细节表示。</li><li>广泛的实验结果表明，该方法在说话头部合成中优于现有技术，提供了精确的唇部同步和出色的视觉质量。</li><li>该方法在 NVIDIA RTX4090 GPU 上实现了 130 FPS 的渲染速度，显着超过了实时渲染性能的门槛。</li><li>该方法有可能部署在其他硬件平台上。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>题目：高斯说话者：基于 3D 高斯点云的说话人特定会说话的头合成</p></li><li><p>作者：洪运雨、湛泉、于启航、陈建川、蒋中华、陈志文、张胜雨、许吉民、吴飞、吕成飞、于刚</p></li><li><p>第一作者单位：阿里巴巴集团</p></li><li><p>关键词：音频驱动、说话头部合成、3D 高斯点云、隐式神经表示、神经辐射场</p></li><li><p>论文链接：xxx Github 链接：None</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：近年来，基于神经辐射场（NeRF）的音频驱动说话头部合成方法取得了显著进展。然而，由于 NeRF 隐式表示导致的姿势和表情控制不足，这些方法仍然存在唇部动作不同步或不自然、视觉抖动和伪影等问题。</p><p>（2）：过去方法：以往方法采用 NeRF 隐式表示进行说话头部合成，但存在姿势和表情控制不足的问题。</p><p>（3）：本文方法：本文提出了一种基于 3D 高斯点云的音频驱动说话头部合成方法 GaussianTalker。GaussianTalker 由说话人特定动作转换器和动态高斯渲染器两个模块组成。其中，说话人特定动作转换器通过通用音频特征提取和定制唇部动作生成，实现针对目标说话人的准确唇部动作。动态高斯渲染器引入了说话人特定混合形状和动态高斯点云，通过将高斯点云绑定到 3D 面部模型，实现了对面部动作的直观控制。</p><p>（4）：实验结果：在 TalkingHead2017 数据集上，GaussianTalker 在唇部动作准确性、视觉保真度和生成视频流畅性方面均取得了最优性能。这些结果表明，GaussianTalker 能够有效地合成具有精确唇部动作和高视觉质量的说话头部视频。</p><ol><li><p>方法：</p><p>（1）：采用通用音频特征提取和定制唇部动作生成，实现针对目标说话人的准确唇部动作。 （2）：引入了说话人特定混合形状和动态高斯点云，通过将高斯点云绑定到 3D 面部模型，实现了对面部动作的直观控制。 （3）：训练对象包括重建、唇部动作平滑度和潜在一致性三个部分。</p></li><li><p>结论：</p></li></ol><p>（1）：GaussianTalker 提出了一种基于 3D 高斯点云的音频驱动说话头部合成方法，该方法将多模态数据与特定说话人关联，减少了音频、3D 网格和视频之间的潜在身份偏差。说话人特定 FLAME 转换器采用身份解耦和个性化嵌入来实现同步和自然的唇部动作，而动态高斯渲染器通过潜在姿势细化高斯属性，以实现稳定和逼真的渲染。大量的实验表明，GaussianTalker 在说话头部合成中优于最先进的性能，同时实现了超高的渲染速度，远远超过其他方法。我们相信这种创新方法将鼓励未来的研究开发更流畅、更逼真的角色表情和动作。通过利用先进的高斯模型和生成技术，角色动画将远远超出简单的唇形同步，捕捉更广泛的角色动态。</p><p>（2）：创新点：提出了基于 3D 高斯点云的说话头部合成方法，引入了说话人特定混合形状和动态高斯点云，实现了对面部动作的直观控制。；性能：在 TalkingHead2017 数据集上，GaussianTalker 在唇部动作准确性、视觉保真度和生成视频流畅性方面均取得了最优性能。；工作量：GaussianTalker 的训练和推理速度都非常快，能够实时生成说话头部视频。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-f53af9ef57ed25d0699b508f7b856061.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-00e62c0d66ff2641b9803987918d6fd0.jpg" align="middle"></details><h2 id="NeRF-DetS-Enhancing-Multi-View-3D-Object-Detection-with-Sampling-adaptive-Network-of-Continuous-NeRF-based-Representation"><a href="#NeRF-DetS-Enhancing-Multi-View-3D-Object-Detection-with-Sampling-adaptive-Network-of-Continuous-NeRF-based-Representation" class="headerlink" title="NeRF-DetS: Enhancing Multi-View 3D Object Detection with   Sampling-adaptive Network of Continuous NeRF-based Representation"></a>NeRF-DetS: Enhancing Multi-View 3D Object Detection with Sampling-adaptive Network of Continuous NeRF-based Representation</h2><p><strong>Authors:Chi Huang, Xinyang Li, Shengchuan Zhang, Liujuan Cao, Rongrong Ji</strong></p><p>As a preliminary work, NeRF-Det unifies the tasks of novel view synthesis and 3D perception, demonstrating that perceptual tasks can benefit from novel view synthesis methods like NeRF, significantly improving the performance of indoor multi-view 3D object detection. Using the geometry MLP of NeRF to direct the attention of detection head to crucial parts and incorporating self-supervised loss from novel view rendering contribute to the achieved improvement. To better leverage the notable advantages of the continuous representation through neural rendering in space, we introduce a novel 3D perception network structure, NeRF-DetS. The key component of NeRF-DetS is the Multi-level Sampling-Adaptive Network, making the sampling process adaptively from coarse to fine. Also, we propose a superior multi-view information fusion method, known as Multi-head Weighted Fusion. This fusion approach efficiently addresses the challenge of losing multi-view information when using arithmetic mean, while keeping low computational costs. NeRF-DetS outperforms competitive NeRF-Det on the ScanNetV2 dataset, by achieving +5.02% and +5.92% improvement in mAP@.25 and mAP@.50, respectively.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.13921v1">PDF</a></p><p><strong>Summary</strong><br>神经辐射场（NeRF）统一新颖视图合成与三维感知，通过神经渲染在空间中的连续表示，提出多级采样自适应网络，改进多视图信息融合方法，提升了室内多视图三维物体检测性能。</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF-Det 统一新视图合成和 3D 感知任务，新视图合成方法显著提高了室内多视图 3D 目标检测性能。</li><li>NeRF 的几何 MLP 用于指导检测头的注意力，并结合新视图渲染的自监督损失，促进了性能改进。</li><li>NeRF-DetS 引入多级采样自适应网络，自适应地从粗到细进行采样。</li><li>多头加权融合方法解决了使用算术平均值丢失多视图信息的问题。</li><li>NeRF-DetS 在 ScanNetV2 数据集上优于 NeRF-Det，分别在 mAP@.25 和 mAP@.50 上提高了 +5.02% 和 +5.92%。</li><li>多级采样自适应网络和多头加权融合方法是 NeRF-DetS 的主要创新。</li><li>NeRF-DetS 证明了神经渲染在三维感知中的优势，特别是对于多视图物体检测。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>标题：NeRF-DetS：基于连续 NeRF 表示的采样自适应网络增强多视图 3D 目标检测</p></li><li><p>作者：Chi Huang、Xinyang Li、Shengchuan Zhang、Liujuan Cao、Rongrong Ji</p></li><li><p>单位：厦门大学多媒体可信感知与高效计算教育部重点实验室</p></li><li><p>关键词：3D 目标检测、NeRF、多视图</p></li><li><p>链接：https://arxiv.org/abs/2404.13921</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：NeRF-Det 统一了新视图合成和 3D 感知任务，表明感知任务可以受益于 NeRF 等新视图合成方法，显著提高了室内多视图 3D 目标检测的性能。使用 NeRF 的几何 MLP 指导检测头的注意力到关键部分，并结合新视图渲染的自监督损失，促成了实现的改进。</p><p>（2）：过去的方法和问题：为了更好地利用神经渲染在空间中通过连续表示的显着优势，引入了新颖的 3D 感知网络结构 NeRF-DetS。NeRF-DetS 的关键组件是多级采样自适应网络，使采样过程自适应地从粗到精。此外，提出了一个优越的多视图信息融合方法，称为多头加权融合。这种融合方法有效地解决了使用算术平均值时丢失多视图信息的问题，同时保持较低的计算成本。</p><p>（3）：提出的研究方法：NeRF-DetS 在 ScanNetV2 数据集上优于竞争对手 NeRF-Det，在 mAP@.25 和 mAP@.50 上分别实现了 +5.02% 和 +5.92% 的提升。</p><p>（4）：方法的性能和对目标的支持：NeRF-DetS 的性能支持其目标，即通过连续表示和自适应采样增强多视图 3D 目标检测。</p><ol><li>方法：</li></ol><p>（1）：多级采样自适应网络：通过对原始采样点进行偏移预测，实现自适应采样，弥补原始采样点信息的缺失，获取更丰富的空间特征信息；</p><p>（2）：多头加权融合：对不同视角的特征进行加权融合，通过多头权重分配机制，突出重要视角的信息，抑制无关视角的影响，提高融合特征的有效性；</p><p>（3）：训练目标：采用与 NeRF-Det 相同的损失结构，包括 Bounding Box 回归损失、分类损失、中心点损失和新视图渲染损失，以优化检测模型的性能。</p><ol><li>结论：</li></ol><p>（1）：本文提出了 NeRF-DetS，以增强基于连续 NeRF 表示的多视图图像目标检测性能。为了充分利用 NeRF 分支为感知过程带来的优势，我们采用了多级采样自适应网络，该网络充分利用了基于 NeRF 表示的连续性的显著特征。此外，认识到空间中多视图信息融合的不足，我们提出了多头加权融合。这种方法利用权重来解决在存在多个视角的情况下空间中的特定视角可能被遮挡的情况。在 ScanNetV2 数据集上的大量实验验证了我们的方法在提高检测任务性能方面的有效性。</p><p>（2）：创新点：多级采样自适应网络、多头加权融合；性能：在 ScanNetV2 数据集上优于竞争对手 NeRF-Det，在 mAP@.25 和 mAP@.50 上分别实现了 +5.02% 和 +5.92% 的提升；工作量：与 NeRF-Det 相同的损失结构，包括 Bounding Box 回归损失、分类损失、中心点损失和新视图渲染损失。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-239cc2f7c7a9838e9e872c8f4334e2d9.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-c1b28030a36aae7836362d0f5da6d44d.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-801202c40b51eebd7384f940b19468e9.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-83f3650828e2486fc3a4b3751e57b1e2.jpg" align="middle"></details><h2 id="CT-NeRF-Incremental-Optimizing-Neural-Radiance-Field-and-Poses-with-Complex-Trajectory"><a href="#CT-NeRF-Incremental-Optimizing-Neural-Radiance-Field-and-Poses-with-Complex-Trajectory" class="headerlink" title="CT-NeRF: Incremental Optimizing Neural Radiance Field and Poses with   Complex Trajectory"></a>CT-NeRF: Incremental Optimizing Neural Radiance Field and Poses with Complex Trajectory</h2><p><strong>Authors:Yunlong Ran, Yanxu Li, Qi Ye, Yuchi Huo, Zechun Bai, Jiahao Sun, Jiming Chen</strong></p><p>Neural radiance field (NeRF) has achieved impressive results in high-quality 3D scene reconstruction. However, NeRF heavily relies on precise camera poses. While recent works like BARF have introduced camera pose optimization within NeRF, their applicability is limited to simple trajectory scenes. Existing methods struggle while tackling complex trajectories involving large rotations. To address this limitation, we propose CT-NeRF, an incremental reconstruction optimization pipeline using only RGB images without pose and depth input. In this pipeline, we first propose a local-global bundle adjustment under a pose graph connecting neighboring frames to enforce the consistency between poses to escape the local minima caused by only pose consistency with the scene structure. Further, we instantiate the consistency between poses as a reprojected geometric image distance constraint resulting from pixel-level correspondences between input image pairs. Through the incremental reconstruction, CT-NeRF enables the recovery of both camera poses and scene structure and is capable of handling scenes with complex trajectories. We evaluate the performance of CT-NeRF on two real-world datasets, NeRFBuster and Free-Dataset, which feature complex trajectories. Results show CT-NeRF outperforms existing methods in novel view synthesis and pose estimation accuracy.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.13896v2">PDF</a></p><p><strong>Summary</strong><br>CT-NeRF是一种增量式重建优化管道，仅使用RGB图像即可恢复相机姿态和场景结构，适用于具有复杂轨迹的场景。</p><p><strong>Key Takeaways</strong></p><ul><li>CT-NeRF 提出了一种局部-全局束调整方法，以连接相邻帧之间的位姿图，通过位姿一致性约束场景结构来避免陷入局部最小值。</li><li>CT-NeRF 将位姿一致性实例化为基于输入图像对之间的像素级对应关系的重投影几何图像距离约束。</li><li>通过增量重建，CT-NeRF 能够恢复相机姿态和场景结构，并能够处理具有复杂轨迹的场景。</li><li>CT-NeRF 在 NeRFBuster 和 Free-Dataset 这两个具有复杂轨迹的真实世界数据集上优于现有方法。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: CT-NeRF：增量优化神经辐射场和位姿，复杂轨迹下的应用</p></li><li><p>Authors: Yunlong Ran, Yanxu Li, Qi Ye, Yuchi Huo, Zechun Bai, Jiahao sun, Jiming chen</p></li><li><p>Affiliation: 浙江大学</p></li><li><p>Keywords: Pose estimation, Implicit representation, Structure from motion, SLAM</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2404.13896, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 背景：神经辐射场（NeRF）在高质量3D场景重建中取得了显著成果。然而，NeRF严重依赖于精确的相机位姿。虽然BARF等近期工作已经引入了NeRF中的相机位姿优化，但其适用性仅限于简单的轨迹场景。现有方法在处理涉及大旋转的复杂轨迹时会遇到困难。</p><p>(2): 过去的方法及问题：BARF等方法将相机位姿优化引入NeRF，但仅限于简单轨迹场景，无法处理复杂轨迹。现有方法在处理涉及大旋转的复杂轨迹时会遇到困难。</p><p>(3): 本文方法：为了解决这一限制，本文提出了CT-NeRF，这是一种仅使用RGB图像而无需位姿和深度输入的增量重建优化管道。在该管道中，我们首先提出了一种局部-全局捆绑调整，在连接相邻帧的位姿图下，以强制位姿之间的一致性，从而逃离仅与场景结构的位姿一致性造成的局部最小值。此外，我们将位姿之间的一致性实例化为从输入图像对之间的像素级对应关系产生的重投影几何图像距离约束。通过增量重建，CT-NeRF能够恢复相机位姿和场景结构，并且能够处理具有复杂轨迹的场景。</p><p>(4): 性能：我们在两个具有复杂轨迹的真实世界数据集NeRFBuster和Free-Dataset上评估了CT-NeRF的性能。结果表明，CT-NeRF在新的视图合成和位姿估计精度方面优于现有方法。</p><ol><li>方法：</li></ol><p>（1）：提出局部-全局捆绑调整，在连接相邻帧的位姿图下，以强制位姿之间的一致性，从而逃离仅与场景结构的位姿一致性造成的局部最小值；</p><p>（2）：将位姿之间的一致性实例化为从输入图像对之间的像素级对应关系产生的重投影几何图像距离约束；</p><p>（3）：通过增量重建，CT-NeRF能够恢复相机位姿和场景结构，并且能够处理具有复杂轨迹的场景。</p><ol><li>结论：</li></ol><p>（1）：本文提出了 CT-NeRF，一种能够从沿复杂轨迹捕获的图像序列中恢复位姿和重建场景的方法。我们首先引入对应关系和重投影几何图像距离，对优化图施加额外的约束，实现鲁棒且准确的位姿估计和场景结构重建。随后，我们详细介绍了我们用于位姿恢复的增量学习过程，包括初始化、跟踪、窗口优化和全局优化。通过比较和消融实验，我们证明了我们方法的优越性和其各个组成部分的必要性。虽然我们的方法能够在复杂的相机轨迹下进行联合位姿估计和重建，但我们只探索了简单的位姿图。对于非常长的轨迹，需要更复杂的图优化。此外，正如论文中所讨论的，复杂相机轨迹需要评估数据集、协议和指标，当前的视觉指标无法充分反映重建质量。</p><p>（2）：创新点：提出了局部-全局捆绑调整，将位姿之间的一致性实例化为重投影几何图像距离约束，实现了鲁棒且准确的位姿估计和场景结构重建；性能：在具有复杂轨迹的真实世界数据集 NeRFBuster 和 Free-Dataset 上评估，结果表明 CT-NeRF 在新的视图合成和位姿估计精度方面优于现有方法；工作量：方法实现较为复杂，需要较高的计算资源。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-4e31cd388846d5e79eb8c6f1f5370705.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-51c39516accf12f9bec3760a243d8ec4.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-2f23b255b94f32edb903410a01a371e3.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-360fd8ee973080efc6f3769036860e2b.jpg" align="middle"></details><h2 id="Neural-Radiance-Field-in-Autonomous-Driving-A-Survey"><a href="#Neural-Radiance-Field-in-Autonomous-Driving-A-Survey" class="headerlink" title="Neural Radiance Field in Autonomous Driving: A Survey"></a>Neural Radiance Field in Autonomous Driving: A Survey</h2><p><strong>Authors:Lei He, Leheng Li, Wenchao Sun, Zeyu Han, Yichen Liu, Sifa Zheng, Jianqiang Wang, Keqiang Li</strong></p><p>Neural Radiance Field (NeRF) has garnered significant attention from both academia and industry due to its intrinsic advantages, particularly its implicit representation and novel view synthesis capabilities. With the rapid advancements in deep learning, a multitude of methods have emerged to explore the potential applications of NeRF in the domain of Autonomous Driving (AD). However, a conspicuous void is apparent within the current literature. To bridge this gap, this paper conducts a comprehensive survey of NeRF’s applications in the context of AD. Our survey is structured to categorize NeRF’s applications in Autonomous Driving (AD), specifically encompassing perception, 3D reconstruction, simultaneous localization and mapping (SLAM), and simulation. We delve into in-depth analysis and summarize the findings for each application category, and conclude by providing insights and discussions on future directions in this field. We hope this paper serves as a comprehensive reference for researchers in this domain. To the best of our knowledge, this is the first survey specifically focused on the applications of NeRF in the Autonomous Driving domain.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.13816v1">PDF</a></p><p><strong>Summary</strong></p><p>NeRF在自动驾驶领域具有感知、三维重建、SLAM和仿真等应用，本文对其应用进行了全面的综述。</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF能用于自动驾驶中的感知任务，如物体检测、语义分割和实例分割。</li><li>NeRF能用于自动驾驶中的3D重建任务，如场景重建和车辆建模。</li><li>NeRF能用于自动驾驶中的SLAM任务，如定位和建图。</li><li>NeRF能用于自动驾驶中的仿真任务，如场景生成和传感器模拟。</li><li>NeRF的应用在自动驾驶领域具有广阔的前景。</li><li>NeRF在自动驾驶中的应用仍面临一些挑战，如效率、鲁棒性和真实感。</li><li>本次调查为研究人员提供了NeRF在自动驾驶领域应用的全面参考。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: 神经辐射场在自动驾驶中的应用：综述</p></li><li><p>Authors: Lei He, Leheng Li, Wenchao Sun, Zeyu Han, Yichen Liu, Sifa Zheng, Jianqiang Wang, Keqiang Li</p></li><li><p>Affiliation: 中国科学院大学</p></li><li><p>Keywords: Neural Radiance Field, Autonomous driving, Perception, 3D Reconstruction, SLAM, Simulation</p></li><li><p>Urls: https://arxiv.org/abs/2404.13816v1</p></li><li><p>Summary:</p><p>(1): 神经辐射场（NeRF）是一种新颖的视图合成技术，它利用体渲染和隐式神经场景表示的能力来揭示 3D 场景几何的复杂性。它在 ECCV 2020 上首次亮相，迅速达到领先的视觉质量水平，并成为众多后续研究工作的灵感来源。近年来，自动驾驶领域取得了重大进展，在高速公路场景中得到广泛部署，但城市环境中的部署仍在进行严格测试。这种技术演变已经从最初依赖高精度地图提供静态场景理解转变为现在通过鸟瞰视觉实时感知局部环境。同时，它在功能上已经从 2 级（L2）发展起来，并正在努力实现 4 级（L4）自动驾驶。自动驾驶系统要求对周围环境有深入的了解，包括静态场景和交通参与者之间的动态交互，这是有效规划和控制的关键前提。通过自监督学习，NeRF 已证明其有效理解局部场景的能力，使其成为增强自动驾驶能力的有力候选者。在过去的两年中，NeRF 模型已在自动驾驶的各个方面得到应用，包括感知、3D 重建、同时定位和地图构建 (SLAM) 和仿真，如图 1 所示。</p><p>(2): 在感知领域，神经辐射场（NeRF）已成为一个有前途的竞争者，涵盖了对象检测、语义分割和占用预测等一系列关键任务。其受欢迎程度的激增主要归功于其获取精确且一致的几何信息的能力。该领域的研究可以分为两种主要范例，区别在于 NeRF 的利用：“NeRF for data”和“NeRF for model”。前者涉及 NeRF 的初始训练，然后将其用于增强感知任务的训练数据。相比之下，后者采用 NeRF 和感知网络的协作训练策略，使感知网络能够学习 NeRF 捕获的几何信息。</p><p>(3): 在 3D 重建应用领域，NeRF 可以根据场景理解的级别分为三种主要方法：动态场景重建、表面重建和逆向渲染。在第一类中，动态场景重建专注于重建具有可移动代理的动态场景，主要使用顺序 3D 边界框注释和相机参数。在第二类中，表面重建旨在重建场景的显式 3D 表面，例如网格。在第三类中，逆向渲染旨在从驾驶场景的图像中分解形状、反照率和可见性，以实现诸如重新照明之类的应用。</p><p>(4): 至于 SLAM 应用，NeRF 的利用可以分为三种主要方法，每种方法都针对映射、定位或两者兼而有之。至于定位，NeRF 用于在当前时间戳执行实时图像渲染，并通过最小化重投影误差来估计 SLAM 系统的精确姿态。虽然 NeRF for mapping 主要专注于增强 SLAM 系统的映射能力，这通过合并使用 NeRF 生成的深度图来实现，从而提高了地图精度。此外，NeRF 在其他一些研究中用于同时提高 3D 地图的质量和提高 SLAM 系统在姿态估计中的精度。这些分类展示了如何将 NeRF 策略性地集成到 SLAM 系统中以满足特定需求，无论这些需求涉及映射、定位还是两者兼而有之的功能。值得一提的是，一些现有的基于 NeRF 的 SLAM 方法是为室内场景设计的，但由于该技术类似于自动驾驶的大型室外环境，因此本文也对室内方法进行了综述。</p></li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：</li></ol><p>（1）：本综述工作对神经辐射场在自动驾驶领域的应用进行了全面的回顾。具体而言，我们首先介绍了神经辐射场的基本原理和背景，然后深入分析了神经辐射场在自动驾驶各个领域的应用，分为感知、三维重建、SLAM和仿真。在总结了神经辐射场在自动驾驶领域应用的最新进展的基础上，我们还讨论了该领域未来研究的潜在方向和挑战。</p><p>（2）：创新点：本文对神经辐射场在自动驾驶领域的应用进行了全面的综述，涵盖了感知、三维重建、SLAM和仿真等多个方面。本文总结了神经辐射场在自动驾驶领域应用的最新进展，并指出了该领域未来研究的潜在方向和挑战。此外，本文还对神经辐射场在自动驾驶领域应用的优势和不足进行了分析，为该领域的研究人员和从业者提供了有价值的参考。</p><p>性能：本文对神经辐射场在自动驾驶领域的应用进行了全面的综述，涵盖了感知、三维重建、SLAM和仿真等多个方面。本文总结了神经辐射场在自动驾驶领域应用的最新进展，并指出了该领域未来研究的潜在方向和挑战。此外，本文还对神经辐射场在自动驾驶领域应用的优势和不足进行了分析，为该领域的研究人员和从业者提供了有价值的参考。</p><p>工作量：本文对神经辐射场在自动驾驶领域的应用进行了全面的综述，涵盖了感知、三维重建、SLAM和仿真等多个方面。本文总结了神经辐射场在自动驾驶领域应用的最新进展，并指出了该领域未来研究的潜在方向和挑战。此外，本文还对神经辐射场在自动驾驶领域应用的优势和不足进行了分析，为该领域的研究人员和从业者提供了有价值的参考。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-f00a4edaa4deada8fbf20792a3bdb4f2.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-726da3cc31a9a838b18bd0268191d0f9.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-936b55512111274340010e2934e3af78.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-0376cf43fef8cbf7ce42618963f10673.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-db311dfa75c7afbf16e9c52d4642623e.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-18c975d626ca07af436db0c065d6d034.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-025492e7bc2802a1fe24dea9c19a7bbf.jpg" align="middle"></details></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io/2024/04/25/Paper/2024-04-25/NeRF/">https://kedreamix.github.io/2024/04/25/Paper/2024-04-25/NeRF/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/NeRF/">NeRF</a></div><div class="post_share"><div class="social-share" data-image="https://picx.zhimg.com/v2-726da3cc31a9a838b18bd0268191d0f9.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/05/02/Paper/2024-05-02/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙/虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-37516691b13dabbabb3b74ea46b402d8.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">元宇宙/虚拟人</div></div></a></div><div class="next-post pull-right"><a href="/2024/04/25/Paper/2024-04-25/3DGS/" title="3DGS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-ed80501d2ace1f8ad37b4cb8f3411d6f.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">3DGS</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/01/25/Paper/3DGS%20Survey/" title="3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-25</div><div class="title">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</div></div></a></div><div><a href="/2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" title="超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-01</div><div class="title">超赞的数字人生成知识库 Awesome-Talking-Head-Synthesis</div></div></a></div><div><a href="/2024/03/15/Paperscape/Real3D-Portrait/" title="REAL3D-PORTRAIT ONE-SHOT REALISTIC 3D TALKING PORTRAIT SYNTHESIS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-68585b79de5f83b0dfa23304f41b9b98.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-15</div><div class="title">REAL3D-PORTRAIT ONE-SHOT REALISTIC 3D TALKING PORTRAIT SYNTHESIS</div></div></a></div><div><a href="/2024/03/07/Paperscape/SyncTalk/" title="SyncTalk The Devil is in the Synchronization for Talking Head Synthesis"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-a57e0937b2f452009023394a59529dfb.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-07</div><div class="title">SyncTalk The Devil is in the Synchronization for Talking Head Synthesis</div></div></a></div><div><a href="/2024/03/05/Paperscape/VividTalk/" title="VividTalk One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-8521b04f82075cc27b5e95148dba9792.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-05</div><div class="title">VividTalk One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior</div></div></a></div><div><a href="/2024/01/24/Paper/2024-01-24/NeRF/" title="NeRF"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-eaae707aaf894e22e54246edd91e6dce.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-24</div><div class="title">NeRF</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-04-25-%E6%9B%B4%E6%96%B0"><span class="toc-text">2024-04-25 更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#GaussianTalker-Speaker-specific-Talking-Head-Synthesis-via-3D-Gaussian-Splatting"><span class="toc-text">GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian Splatting</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#NeRF-DetS-Enhancing-Multi-View-3D-Object-Detection-with-Sampling-adaptive-Network-of-Continuous-NeRF-based-Representation"><span class="toc-text">NeRF-DetS: Enhancing Multi-View 3D Object Detection with Sampling-adaptive Network of Continuous NeRF-based Representation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CT-NeRF-Incremental-Optimizing-Neural-Radiance-Field-and-Poses-with-Complex-Trajectory"><span class="toc-text">CT-NeRF: Incremental Optimizing Neural Radiance Field and Poses with Complex Trajectory</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Neural-Radiance-Field-in-Autonomous-Driving-A-Survey"><span class="toc-text">Neural Radiance Field in Autonomous Driving: A Survey</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image:url('https://picx.zhimg.com/v2-726da3cc31a9a838b18bd0268191d0f9.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js").then((()=>{pangu.autoSpacingPage()}))}function panguInit(){panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.1},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script></div><script id="canvas_nest" defer color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><script data-pjax>function butterfly_swiper_injector_config(){var a=document.getElementById("recent-posts");console.log("已挂载butterfly_swiper"),a.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/25/Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="2024/01/25/Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="2024/01/25/Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/17/Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023/12/17/Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="2023/12/17/Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script></body></html>