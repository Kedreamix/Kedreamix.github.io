<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>Diffusion Models | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-04-17  RefFusion Reference Adapted Diffusion Models for 3D Scene Inpainting"><meta property="og:type" content="article"><meta property="og:title" content="Diffusion Models"><meta property="og:url" content="https://kedreamix.github.io/2024/04/17/Paper/2024-04-17/Diffusion%20Models/index.html"><meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World"><meta property="og:description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-04-17  RefFusion Reference Adapted Diffusion Models for 3D Scene Inpainting"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://picx.zhimg.com/80/v2-bba220bfbb93f64f729fd79248d1ba37.png"><meta property="article:published_time" content="2024-04-17T06:02:48.000Z"><meta property="article:modified_time" content="2024-04-17T06:02:48.569Z"><meta property="article:author" content="Kedreamix"><meta property="article:tag" content="Diffusion Models"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://picx.zhimg.com/80/v2-bba220bfbb93f64f729fd79248d1ba37.png"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/2024/04/17/Paper/2024-04-17/Diffusion%20Models/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-74LZ5BEQQ1")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!0,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"简"},noticeOutdate:{limitDay:500,position:"top",messagePrev:"It has been",messageNext:"days since the last update, the content of the article may be outdated."},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:200},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!0,post:!0},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!0,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"Diffusion Models",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-04-17 14:02:48"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach((e=>{n.setAttribute(e,t[e])})),document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)})),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=24?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 7.0.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml"></head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{const d=document.getElementById("loading-box"),e=document.body,o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l(),window.addEventListener("load",(()=>{o()}))})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">278</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url('https://picx.zhimg.com/80/v2-bba220bfbb93f64f729fd79248d1ba37.png')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Diffusion Models</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-04-17T06:02:48.000Z" title="发表于 2024-04-17 14:02:48">2024-04-17</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-04-17T06:02:48.569Z" title="更新于 2024-04-17 14:02:48">2024-04-17</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">21.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>74分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="Diffusion Models"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-17-更新"><a href="#2024-04-17-更新" class="headerlink" title="2024-04-17 更新"></a>2024-04-17 更新</h1><h2 id="RefFusion-Reference-Adapted-Diffusion-Models-for-3D-Scene-Inpainting"><a href="#RefFusion-Reference-Adapted-Diffusion-Models-for-3D-Scene-Inpainting" class="headerlink" title="RefFusion: Reference Adapted Diffusion Models for 3D Scene Inpainting"></a>RefFusion: Reference Adapted Diffusion Models for 3D Scene Inpainting</h2><p><strong>Authors:Ashkan Mirzaei, Riccardo De Lutio, Seung Wook Kim, David Acuna, Jonathan Kelly, Sanja Fidler, Igor Gilitschenski, Zan Gojcic</strong></p><p>Neural reconstruction approaches are rapidly emerging as the preferred representation for 3D scenes, but their limited editability is still posing a challenge. In this work, we propose an approach for 3D scene inpainting — the task of coherently replacing parts of the reconstructed scene with desired content. Scene inpainting is an inherently ill-posed task as there exist many solutions that plausibly replace the missing content. A good inpainting method should therefore not only enable high-quality synthesis but also a high degree of control. Based on this observation, we focus on enabling explicit control over the inpainted content and leverage a reference image as an efficient means to achieve this goal. Specifically, we introduce RefFusion, a novel 3D inpainting method based on a multi-scale personalization of an image inpainting diffusion model to the given reference view. The personalization effectively adapts the prior distribution to the target scene, resulting in a lower variance of score distillation objective and hence significantly sharper details. Our framework achieves state-of-the-art results for object removal while maintaining high controllability. We further demonstrate the generality of our formulation on other downstream tasks such as object insertion, scene outpainting, and sparse view reconstruction.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.10765v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://reffusion.github.io">https://reffusion.github.io</a></p><p><strong>Summary</strong><br>基于多尺度个性化的图像修复扩散模型，提出了RefFusion，一种3D场景修复方法，该方法可通过参考图像对内容进行明确控制，从而实现高质量合成。</p><p><strong>Key Takeaways</strong></p><ul><li>3D场景修复仍然面临可编辑性有限的挑战。</li><li>场景修复是一个固有的病态任务，即存在许多可信地替代缺失内容的解决方案。</li><li>好的修复方法不仅要实现高质量的合成，还要具有高度的可控性。</li><li>RefFusion是一种基于多尺度个性化的图像修复扩散模型，可以对给定的参考视图进行3D修复。</li><li>个性化有效地将先验分布适应到目标场景，从而降低了评分蒸馏目标的方差，因此产生了明显更清晰的细节。</li><li>RefFusion框架在对象移除方面实现了最先进的结果，同时保持了高度的可控性。</li><li>RefFusion的公式在对象插入、场景外画和稀疏视图重建等其他下游任务上也具有普遍性。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>1.标题：RefFusion：用于 3D 场景修复的参考自适应扩散模型 2.作者：Ashkan Mirzaei, Riccardo de Lutio, SeungWook Kim, David Acuna, Jonathan Kelly, Sanja Fidler, Igor Gilitschenski, Zan Gojcic 3.第一作者单位：NVIDIA，多伦多大学 4.关键词：3D 场景修复、扩散模型、参考自适应 5.论文链接：https://arxiv.org/abs/2404.10765 Github 链接：无 6.摘要： （1）研究背景：神经重建方法能够从一组姿态图像中无缝重建 3D 场景。然而，它们的可编辑性仍然有限，而 3D 场景修复是一种关键的可编辑操作，涉及合成合理的内容，以便从任何角度观看时都能与场景的其余部分融为一体。 （2）过去的方法：过去的方法通常使用神经网络来生成修复内容。然而，这些方法通常缺乏对修复内容的显式控制，并且可能产生模糊或不连贯的结果。 （3）研究方法：本文提出了一种名为 RefFusion 的 3D 场景修复方法。RefFusion 基于图像修复扩散模型的多尺度个性化，可以将给定的参考视图中的先验分布有效地适应目标场景。这种个性化降低了分数蒸馏目标函数的方差，从而显著提高了细节的清晰度。 （4）方法性能：RefFusion 在对象移除任务上实现了最先进的结果，同时保持了较高的可控性。此外，本文还展示了该方法在其他下游任务（如对象插入、场景外延和稀疏视图重建）上的通用性。这些结果表明，RefFusion 能够有效地合成高质量且可控的修复内容，从而满足各种 3D 场景编辑需求。</p><ol><li><p>方法： (1) <strong>RefFusion</strong>：本文提出的3D场景修复方法，基于图像修复扩散模型的多尺度个性化，可以将给定的参考视图中的先验分布有效地适应目标场景。 (2) <strong>多尺度个性化</strong>：RefFusion在扩散模型的不同尺度上对参考视图进行个性化，从而使模型能够捕获不同尺度上的细节和结构。 (3) <strong>分数蒸馏目标函数</strong>：RefFusion使用分数蒸馏目标函数来匹配修复内容和参考视图的先验分布。通过个性化降低目标函数的方差，提高了修复内容的细节清晰度。</p></li><li><p>结论： (1): RefFusion 提出了一种基于图像修复扩散模型的多尺度个性化方法，用于 3D 场景修复，显著提高了修复内容的细节清晰度和可控性，在对象移除等任务上取得了最先进的结果。 (2): 创新点：</p><ul><li>多尺度个性化：在扩散模型的不同尺度上对参考视图进行个性化，捕获不同尺度上的细节和结构。</li><li>分数蒸馏目标函数：使用分数蒸馏目标函数匹配修复内容和参考视图的先验分布，降低目标函数的方差，提高修复内容的细节清晰度。 性能：</li><li>在对象移除任务上实现了最先进的结果。</li><li>在对象插入、场景外延和稀疏视图重建等下游任务上表现出通用性。 工作量：</li><li>训练 RefFusion 模型需要大量的计算资源。</li><li>RefFusion 的推理速度可能会受到模型复杂度的限制。</li></ul></li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/02817604a88632e7e3ea4560f26f9bac241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/c106f428df739a7142772c42a95151c1241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/15ab37f03412f6ff0a7f34d5503212ec241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/542185dc6584d7e97ea791d916e54a04241286257.jpg" align="middle"></details>## LaDiC: Are Diffusion Models Really Inferior to Autoregressive Counterparts for Image-to-Text Generation? **Authors:Yuchi Wang, Shuhuai Ren, Rundong Gao, Linli Yao, Qingyan Guo, Kaikai An, Jianhong Bai, Xu Sun** Diffusion models have exhibited remarkable capabilities in text-to-image generation. However, their performance in image-to-text generation, specifically image captioning, has lagged behind Auto-Regressive (AR) models, casting doubt on their applicability for such tasks. In this work, we revisit diffusion models, highlighting their capacity for holistic context modeling and parallel decoding. With these benefits, diffusion models can alleviate the inherent limitations of AR methods, including their slow inference speed, error propagation, and unidirectional constraints. Furthermore, we identify the prior underperformance of diffusion models stemming from the absence of an effective latent space for image-text alignment, and the discrepancy between continuous diffusion processes and discrete textual data. In response, we introduce a novel architecture, LaDiC, which utilizes a split BERT to create a dedicated latent space for captions and integrates a regularization module to manage varying text lengths. Our framework also includes a diffuser for semantic image-to-text conversion and a Back&amp;Refine technique to enhance token interactivity during inference. LaDiC achieves state-of-the-art performance for diffusion-based methods on the MS COCO dataset with 38.2 BLEU@4 and 126.2 CIDEr, demonstrating exceptional performance without pre-training or ancillary modules. This indicates strong competitiveness with AR models, revealing the previously untapped potential of diffusion models in image-to-text generation. [PDF](http://arxiv.org/abs/2404.10763v1) **Summary** 扩散模型在文本到图像生成方面表现出色，现在通过LaDiC架构在图像到文本生成方面也取得了突破性的进展。 **Key Takeaways** - 扩散模型具有整体上下文建模和并行解码能力。 - AR 方法存在推理速度慢、误差传播和单向约束等固有缺陷。 - 缺乏有效的图像文本对齐潜在空间和连续扩散过程与离散文本数据之间的差异是扩散模型先前表现不佳的原因。 - LaDiC 架构使用 split BERT 创建了专门的标题潜在空间，并集成了一个正则化模块来管理不同的文本长度。 - LaDiC 包括一个用于语义图像到文本转换的扩散器，以及一种在推理过程中增强标记交互性的 Back&amp;Refine 技术。 - 在 MS COCO 数据集上，LaDiC 实现了基于扩散方法的最新性能，BLEU@4 为 38.2，CIDEr 为 126.2，在没有预训练或辅助模块的情况下表现出色。 - 这表明扩散模型在图像到文本生成方面具有与 AR 模型相当的竞争力，揭示了其在该领域尚未开发的潜力。 **[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：图像段落描述：生成连贯且信息丰富的图像描述</li><li>作者：Jonathan Krause, Justin Johnson, Ranjay Krishna, Li Fei-Fei</li><li>隶属关系：斯坦福大学</li><li>关键词：图像描述，段落生成，多模态学习，自然语言处理，计算机视觉</li><li>论文链接：https://arxiv.org/pdf/1611.06607.pdf，Github 链接：无</li><li>摘要： (1)：研究背景：图像描述任务通常生成单个句子来描述图像，但这种描述过于粗略，无法捕捉图像的丰富细节。密集描述任务可以描述图像中的多个区域，但生成的描述缺乏连贯性。 (2)：过去的方法：传统图像描述方法生成单个句子，而密集描述方法生成多个短语，但都存在信息不足或缺乏连贯性的问题。 (3)：研究方法：本文提出了一种图像段落描述模型，该模型将图像分解为语义区域，并使用分层循环神经网络对语言进行推理。通过这种方法，模型可以生成连贯且信息丰富的段落，详细描述图像中的内容。 (4)：方法性能：在图像段落描述数据集上，该模型在 BLEU-4、METEOR 和 ROUGE-L 等指标上取得了最先进的性能，证明了其生成连贯且信息丰富的图像描述的能力，满足了研究目标。</li></ol><p>7.方法：(1) 利用文本编码器将离散文本空间 C 转换为连续文本潜在空间 X；(2) 训练扩散器以在图像空间 V 和文本空间 X 之间架起桥梁；(3) 文本解码器将文本潜在码映射回离散文本。</p><ol><li>结论： （1）： 本文研究了基于扩散的图像到文本范式，并引入了一种新颖的架构，称为 LaDiC。我们的模型在基于扩散的方法中取得了最先进的性能，并展示了与一些预训练的 AR 模型相当的能力。此外，我们广泛的实验揭示了扩散模型在考虑更多整体上下文和并行发出所有标记方面优于 AR 模型的令人兴奋的优势。因此，我们认为扩散模型在图像到文本生成方面具有巨大的潜力，我们希望我们的工作将为该领域的这一领域开辟新的可能性。 （2）： 创新点：提出了一种新颖的基于扩散的图像到文本生成架构 LaDiC，该架构在图像空间和文本空间之间建立了桥梁，并利用分层循环神经网络进行语言推理，从而生成连贯且信息丰富的图像描述。 性能：在图像段落描述数据集上，我们的模型在 BLEU-4、METEOR 和 ROUGE-L 等指标上取得了最先进的性能，证明了其生成连贯且信息丰富的图像描述的能力，满足了研究目标。 工作量：本文主要集中在图像到文本生成的主要研究课题上，以保持简洁性和重点。然而，我们观察到我们的模型可以很容易地适应其他模态甚至纯文本生成，而只需进行最小的修改。我们将这些潜在的扩展留给未来的工作，同时，我们希望本文将激发研究人员使用扩散模型从事以文本为中心的模态生成任务的信心，并期待在这个领域未来的精彩作品。此外，由于资源限制，我们研究中使用的模型参数和数据集并不广泛。考虑到像 GPT 这样的自回归模型在放大时表现出的显着的紧急能力，探索我们的模型或一般的扩散模型是否可以表现出类似的可扩展性，成为一项有趣且有价值的探索。 风险考虑：作为一个生成模型，我们的模型可能会无意中产生难以与人类书面内容区分开来的结果，引发对潜在误用的担忧。采用文本水印技术可能有助于减轻这个问题。此外，扩散模型通常需要大量的计算资源进行训练，从而导致二氧化碳排放和环境影响增加。</li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/ad873da96dc2ed96671aaa4ec1d1b20f241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/35dd717311044737324107cdc54b6822241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/9f7a7a48ee0399a430575adacda8ed66241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/093e2d146940e0222b3021bdfb674cf9241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/9d1dfb06c22a75df200fdac77b6e7498241286257.jpg" align="middle"></details>## GazeHTA: End-to-end Gaze Target Detection with Head-Target Association **Authors:Zhi-Yi Lin, Jouh Yeong Chew, Jan van Gemert, Xucong Zhang** We propose an end-to-end approach for gaze target detection: predicting a head-target connection between individuals and the target image regions they are looking at. Most of the existing methods use independent components such as off-the-shelf head detectors or have problems in establishing associations between heads and gaze targets. In contrast, we investigate an end-to-end multi-person Gaze target detection framework with Heads and Targets Association (GazeHTA), which predicts multiple head-target instances based solely on input scene image. GazeHTA addresses challenges in gaze target detection by (1) leveraging a pre-trained diffusion model to extract scene features for rich semantic understanding, (2) re-injecting a head feature to enhance the head priors for improved head understanding, and (3) learning a connection map as the explicit visual associations between heads and gaze targets. Our extensive experimental results demonstrate that GazeHTA outperforms state-of-the-art gaze target detection methods and two adapted diffusion-based baselines on two standard datasets. [PDF](http://arxiv.org/abs/2404.10718v1) **Summary** 基于视觉图像预测注视目标，提出端到端多人物GazeHTA框架，通过预训练扩散模型、重新注入头特征、学习连接图实现高效的目标检测。 **Key Takeaways** - 提出端到端多人物注视目标检测框架 - GazeHTA。 - 利用预训练扩散模型提取场景特征，提升语义理解。 - 重新注入头特征，增强头部先验，提升头部理解。 - 学习连接图，明确头部与注视目标之间的视觉关联。 - 采用扩散模型作为基准，实现更优的性能。 - 在标准数据集上，GazeHTA 优于现有方法。 - GazeHTA 提供了端到端的解决方案，可直接从图像预测注视目标。 **[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：GazeHTA：端到端的注视目标检测</li><li>作者：Zhi-Yi Lin、Jouh Yeong Chew、Jan van Gemert、Xucong Zhang</li><li>第一作者单位：代尔夫特理工大学计算机视觉实验室</li><li>关键词：注视目标检测、头部目标关联、扩散模型</li><li>论文链接：arXiv:2404.10718v1[cs.CV]16Apr2024</li><li>摘要： （1）研究背景： 人类注意力估计在人机交互、社会活动识别、心理健康诊断和客户行为分析等领域具有重要意义。注视目标检测旨在直接将个体与其注视目标关联起来，提供了一种端到端的注意力估计解决方案。</li></ol><p>（2）过去方法及问题： 大多数注视目标检测方法采用双流架构，存在以下问题： - 缺乏头部和注视目标之间的直接关联。 - 依赖于现成的头部检测器。 - 仅限于一次处理一个头部，在多人物场景中需要重复处理。</p><p>（3）本文方法： GazeHTA 提出了一种端到端的多人注视目标检测框架，通过头部目标关联来预测头部目标实例。其特点包括： - 利用预训练的扩散模型提取场景特征。 - 重新注入头部特征以增强头部先验知识。 - 学习连接图来表示头部和注视目标之间的视觉关联。</p><p>（4）性能： GazeHTA 在两个标准数据集上优于最先进的注视目标检测方法和两个改编的基于扩散的基准。实验结果表明，该方法可以很好地支持其目标。</p><p>7.方法： （1）利用预训练的扩散模型提取场景特征（Stable Diffusion）； （2）重新注入头部特征以增强头部先验知识（Head Feature Re-Injection）； （3）学习连接图来表示头部和注视目标之间的视觉关联（Connection Map）。</p><ol><li>结论： (1): GazeHTA 提出了一种端到端的注视目标检测框架，通过头部目标关联来预测头部目标实例，在两个标准数据集上优于最先进的注视目标检测方法和两个改编的基于扩散的基准。 (2): 创新点：</li><li>利用预训练的扩散模型提取场景特征，增强了头部先验知识，学习了头部和注视目标之间的视觉关联。</li><li>采用多任务学习策略，同时预测头部目标和注视目标，提高了检测精度。</li><li>引入了连接图，表示头部和注视目标之间的视觉关联，增强了头部目标关联的鲁棒性。 性能：</li><li>在两个标准数据集上，GazeHTA 在注视目标检测任务上取得了最先进的性能。</li><li>该方法在多人物场景中表现出色，能够准确地将头部与注视目标关联起来。</li><li>该方法对头部姿态和照明条件的变化具有鲁棒性。 工作量：</li><li>该方法的实现相对复杂，需要预训练扩散模型和训练连接图。</li><li>该方法的计算成本较高，在实际应用中可能需要优化。</li><li>该方法需要大量标注数据进行训练，这可能会限制其在某些场景中的应用。</li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/85b5892f7794783ad79c67d67689cac6241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/c2dd1f0a5f6c974a1e1cc5b4db6180e3241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/831d352002cad6012c105349850e0b6b241286257.jpg" align="middle"></details>## Efficient Conditional Diffusion Model with Probability Flow Sampling for Image Super-resolution **Authors:Yutao Yuan, Chun Yuan** Image super-resolution is a fundamentally ill-posed problem because multiple valid high-resolution images exist for one low-resolution image. Super-resolution methods based on diffusion probabilistic models can deal with the ill-posed nature by learning the distribution of high-resolution images conditioned on low-resolution images, avoiding the problem of blurry images in PSNR-oriented methods. However, existing diffusion-based super-resolution methods have high time consumption with the use of iterative sampling, while the quality and consistency of generated images are less than ideal due to problems like color shifting. In this paper, we propose Efficient Conditional Diffusion Model with Probability Flow Sampling (ECDP) for image super-resolution. To reduce the time consumption, we design a continuous-time conditional diffusion model for image super-resolution, which enables the use of probability flow sampling for efficient generation. Additionally, to improve the consistency of generated images, we propose a hybrid parametrization for the denoiser network, which interpolates between the data-predicting parametrization and the noise-predicting parametrization for different noise scales. Moreover, we design an image quality loss as a complement to the score matching loss of diffusion models, further improving the consistency and quality of super-resolution. Extensive experiments on DIV2K, ImageNet, and CelebA demonstrate that our method achieves higher super-resolution quality than existing diffusion-based image super-resolution methods while having lower time consumption. Our code is available at https://github.com/Yuan-Yutao/ECDP. [PDF](http://arxiv.org/abs/2404.10688v1) AAAI 2024 **Summary** 通过概率流采样的高效条件扩散模型（ECDP）在图像超分辨率方面实现了高超分辨率图像质量和低时间消耗。 **Key Takeaways** - 扩散概率模型能够处理图像超分辨率中固有的病态性问题。 - 迭代采样导致现有基于扩散的超分辨率方法时间消耗高。 - ECDP 引入了连续时间条件扩散模型以提高采样效率。 - 混合参数化为去噪网络提高了生成图像的一致性。 - 图像质量损失作为扩散模型得分匹配损失的补充，进一步提高了一致性和质量。 - ECDP 在 DIV2K、ImageNet 和 CelebA 上优于现有基于扩散的超分辨率方法，同时时间消耗更低。 **[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：基于概率流采样的高效条件扩散模型用于图像超分辨率</li><li>作者：袁宇涛，袁淳</li><li>单位：清华大学</li><li>关键词：图像超分辨率，扩散概率模型，概率流采样</li><li>论文链接：https://arxiv.org/abs/2404.10688 Github代码链接：None</li><li><p>摘要： （1）研究背景：图像超分辨率是一项本质上不适定的问题，因为对于一个低分辨率图像存在多个有效的高分辨率图像。基于扩散概率模型的超分辨率方法可以通过学习条件在低分辨率图像上的高分辨率图像分布来处理不适定性，避免了以 PSNR 为导向的方法中图像模糊的问题。 （2）过去的方法及问题：现有的基于扩散的超分辨率方法使用迭代采样，时间消耗大；并且由于颜色偏移等问题，生成图像的质量和一致性不理想。 （3）提出的研究方法：本文提出了用于图像超分辨率的基于概率流采样的高效条件扩散模型（ECDP）。为了减少时间消耗，设计了用于图像超分辨率的连续时间条件扩散模型，这使得可以使用概率流采样进行高效生成。此外，为了提高生成图像的一致性，提出了去噪器网络的混合参数化，它在不同的噪声尺度上对数据预测参数化和噪声预测参数化进行插值。此外，设计了一个图像质量损失作为扩散模型分数匹配损失的补充，进一步提高了超分辨率的一致性和质量。 （4）方法在任务和性能上的表现：在 DIV2K、ImageNet 和 CelebA 上进行的广泛实验表明，本文方法比现有的基于扩散的图像超分辨率方法实现了更高的超分辨率质量，同时具有更低的时间消耗。这些性能支持了本文的目标。</p></li><li><p>方法： (1): 设计了用于图像超分辨率的连续时间条件扩散模型，使得可以使用概率流采样进行高效生成； (2): 提出了去噪器网络的混合参数化，它在不同的噪声尺度上对数据预测参数化和噪声预测参数化进行插值，提高生成图像的一致性； (3): 设计了一个图像质量损失作为扩散模型分数匹配损失的补充，进一步提高了超分辨率的一致性和质量。</p></li><li><p>结论： （1）本文提出的 ECDP 框架在图像超分辨率任务上取得了较好的效果，在保证超分辨率质量的同时，降低了时间消耗。 （2）创新点：</p></li><li>提出了一种基于连续时间条件扩散模型的图像超分辨率方法，该方法可以有效地利用概率流采样进行生成。</li><li>提出了一种混合参数化的去噪器网络，该网络可以在不同的噪声尺度上对数据预测参数化和噪声预测参数化进行插值，从而提高生成图像的一致性。</li><li>设计了一个图像质量损失作为扩散模型分数匹配损失的补充，该损失可以有效地提高超分辨率结果的质量。 性能：</li><li>在 DIV2K、ImageNet 和 CelebA 数据集上的实验表明，本文方法比现有的基于扩散的图像超分辨率方法具有更高的超分辨率质量和更低的时间消耗。 工作量：</li><li>本文方法的实现相对简单，易于复现。</li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/a82d966193c93e548a48f6956d503a03241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/3243ba61ae0dc29581f4a5b0b8bc24ac241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/9ec5bc713a1dd0f65714d070ec06c103241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/8b672147537b2050d8dcfda1a25fba99241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/84fee720619599629de5e02d5ca3c96a241286257.jpg" align="middle"></details>## StyleCity: Large-Scale 3D Urban Scenes Stylization with Vision-and-Text Reference via Progressive Optimization **Authors:Yingshu Chen, Huajian Huang, Tuan-Anh Vu, Ka Chun Shum, Sai-Kit Yeung** Creating large-scale virtual urban scenes with variant styles is inherently challenging. To facilitate prototypes of virtual production and bypass the need for complex materials and lighting setups, we introduce the first vision-and-text-driven texture stylization system for large-scale urban scenes, StyleCity. Taking an image and text as references, StyleCity stylizes a 3D textured mesh of a large-scale urban scene in a semantics-aware fashion and generates a harmonic omnidirectional sky background. To achieve that, we propose to stylize a neural texture field by transferring 2D vision-and-text priors to 3D globally and locally. During 3D stylization, we progressively scale the planned training views of the input 3D scene at different levels in order to preserve high-quality scene content. We then optimize the scene style globally by adapting the scale of the style image with the scale of the training views. Moreover, we enhance local semantics consistency by the semantics-aware style loss which is crucial for photo-realistic stylization. Besides texture stylization, we further adopt a generative diffusion model to synthesize a style-consistent omnidirectional sky image, which offers a more immersive atmosphere and assists the semantic stylization process. The stylized neural texture field can be baked into an arbitrary-resolution texture, enabling seamless integration into conventional rendering pipelines and significantly easing the virtual production prototyping process. Extensive experiments demonstrate our stylized scenes' superiority in qualitative and quantitative performance and user preferences. [PDF](http://arxiv.org/abs/2404.10681v1) project page: https://chenyingshu.github.io/stylecity3d/ **Summary** 城市场景大规模纹理风格化在文图驱动下，融合神经纹理场，能生成逼真风格化的纹理和全景天空。 **Key Takeaways** - 提出了一个城市场景大规模纹理风格化系统。 - 提出了一种神经纹理场的风格化方法，将二维视觉与文本先验全局和局部地迁移到三维中。 - 渐进缩放输入三维场景的训练视图以保留场景内容的高质量。 - 通过调整风格图像和训练视图的尺度来全局优化场景风格。 - 采用语义感知的风格损失增强局部语义一致性，这对于真实感风格化至关重要。 - 采用生成式扩散模型合成风格一致的全景天空图像，提供更沉浸的氛围并辅助语义风格化。 - 风格化的神经纹理场可以烘焙成任意分辨率的纹理，无缝集成到传统渲染管道中，极大地简化了虚拟制作原型制作过程。 **[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：StyleCity：大规模 3D 城市场景</li><li>作者：Yingshu Chen, Huajian Huang, Tuan-Anh Vu, Ka Chun Shum, Sai-Kit Yeung</li><li>第一作者单位：香港科技大学</li><li>关键词：城市风格化、大规模场景、视觉和文本参考、渐进式优化</li><li>论文链接：https://arxiv.org/abs/2404.10681</li><li><p>摘要： (1) 研究背景：创建具有不同风格的大规模虚拟城市场景具有挑战性，需要复杂的材质和灯光设置。 (2) 过去方法：现有的方法主要依赖于手工制作的材质和灯光，难以实现大规模场景的自动风格化。 (3) 研究方法：本文提出 StyleCity 框架，通过将 2D 视觉和文本先验全局和局部地转移到 3D，对神经纹理场进行风格化。通过渐进式缩放训练视图和自适应风格图像比例以及语义感知风格损失，实现场景风格化和全景天空背景生成。 (4) 实验结果：在城市场景风格化任务上，StyleCity 在视觉保真度和语义一致性方面取得了最先进的性能，支持创建个性化且引人入胜的城市探索体验。</p></li><li><p>方法： （1）：枢轴视图规划：提出了一种基于枢轴的视图规划方法，通过在网格边界框的上、侧面均匀采样相机位置，并使用网格区域的质心作为相机视点，获得枢轴视图，覆盖可见表面的大部分区域，为新视图（图 2 中的绿色相机）提供初始化，以增强训练，见第 3.3 节多尺度渐进优化。 （2）：城市场景分割：为语义感知的城市场景风格化，考虑了建筑场景中感兴趣的类别，包括“天空”、“建筑物”、“窗户”、“道路”、“人”、“植物”、“汽车”、“水”和“灯光”。 （3）：神经纹理场定义：使用神经纹理场表示，将巨大的纹理贴图重新参数化为二维连续函数，通过一个 MLP 将归一化的 UV 纹理坐标映射为颜色 RGB 值，理论上支持任意纹理分辨率，平均纹理尺寸压缩 90%。 （4）：神经渲染：给定相机位姿，光栅化网格并检索 UV 以查询纹理模型 TΘ(·)，获得相应的纹理 RGB 值，然后将值重新组合到渲染的图像中。 （5）：内容和风格联合渐进优化：通过多个视图与每个迭代中的源内容和目标风格约束共同优化神经纹理场，随机采样视点，渲染内容视图及其分割，并从神经纹理中获得风格化视图。 （6）：多尺度渐进优化：在优化期间，沿贝塞尔曲线随机采样新视图，以附近的计划枢轴相机作为控制点，以扩大覆盖角度，并在训练期间以“放大”效果逐渐缩小采样视图的视场 (FoV)，确保每个表面都被全面风格化和全局谐波。 （7）：内容和真实感保留优化：利用渲染视图的内容特征和拉普拉斯值进行监督，以保持纹理内容和场景标识，屏蔽背景区域以保持内容完整性和真实感。 （8）：全局规模自适应风格优化：全局风格优化负责将风格特征全局转移到神经纹理场，以进行整体氛围对齐，基于训练视图和风格贴块之间的结构相似性匹配多尺度风格。 （9）：全局视觉和文本驱动的风格损失：通过惩罚全局风格特征分布的差异，快速获取和生成有意义的新风格化纹理。 （10）：局部语义感知风格优化：对于具有复杂上下文的城市场景，全局风格转移容易导致风格语义不匹配，因此引入局部风格优化策略，进行按类别特征正则化，以实现更逼真的风格化。</p></li><li><p>结论： （1）：StyleCity是一个基于视觉和文本驱动的城市级纹理网格大规模风格化管道。我们利用神经纹理场建模场景外观，并提出了一种新的多尺度渐进优化方法，以实现高保真风格化。对于谐波风格化，我们引入了尺度自适应风格优化和新的损失函数，以全局和局部正则化风格特征。此外，我们改进了漫反射全景合成方法，以支持风格对齐的高分辨率全方位天空合成，这作为沉浸式氛围和更好语义的背景。 （2）：创新点：StyleCity框架、神经纹理场表示、多尺度渐进优化、尺度自适应风格优化、全局视觉和文本驱动的风格损失、基于漫反射的全景天空合成； 性能：在城市场景风格化任务上，StyleCity在视觉保真度和语义一致性方面取得了最先进的性能，支持创建个性化且引人入胜的城市探索体验； 工作量：该方法需要大量的数据集和计算资源，并且训练过程可能需要很长时间。</p></li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/7cb106e145913b85abac0f0c3f097cc8241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/086cffd59fa8e20d53cc5c5be733a969241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/68c301be7084d603d72cc10756a2ae2c241286257.jpg" align="middle"></details>## Generating Counterfactual Trajectories with Latent Diffusion Models for Concept Discovery **Authors:Payal Varshney, Adriano Lucieri, Christoph Balada, Andreas Dengel, Sheraz Ahmed** Trustworthiness is a major prerequisite for the safe application of opaque deep learning models in high-stakes domains like medicine. Understanding the decision-making process not only contributes to fostering trust but might also reveal previously unknown decision criteria of complex models that could advance the state of medical research. The discovery of decision-relevant concepts from black box models is a particularly challenging task. This study proposes Concept Discovery through Latent Diffusion-based Counterfactual Trajectories (CDCT), a novel three-step framework for concept discovery leveraging the superior image synthesis capabilities of diffusion models. In the first step, CDCT uses a Latent Diffusion Model (LDM) to generate a counterfactual trajectory dataset. This dataset is used to derive a disentangled representation of classification-relevant concepts using a Variational Autoencoder (VAE). Finally, a search algorithm is applied to identify relevant concepts in the disentangled latent space. The application of CDCT to a classifier trained on the largest public skin lesion dataset revealed not only the presence of several biases but also meaningful biomarkers. Moreover, the counterfactuals generated within CDCT show better FID scores than those produced by a previously established state-of-the-art method, while being 12 times more resource-efficient. Unsupervised concept discovery holds great potential for the application of trustworthy AI and the further development of human knowledge in various domains. CDCT represents a further step in this direction. [PDF](http://arxiv.org/abs/2404.10356v1) Submitted to International Conference on Pattern Recognition (ICPR) 2024 **Summary** 利用扩散模型的卓越图像合成能力，提出一种新的概念发现框架，以发现黑盒模型中与决策相关的重要概念。 **Key Takeaways** - 黑盒模型中的概念发现对于建立信任和推进医学研究至关重要。 - CDCT是一种用于概念发现的新型三步框架，利用扩散模型的图像合成能力。 - CDCT通过生成反事实轨迹数据集、提取解耦表征和应用搜索算法来识别相关的概念。 - CDCT发现了皮肤病变分类器中的偏差和有意义的生物标志物。 - CDCT生成的对抗样本的FID得分比现有方法更好，同时资源效率提高了12倍。 - 无监督概念发现对于可信赖AI的应用和人类知识在各个领域的进一步发展具有巨大潜力。 - CDCT代表了朝着这个方向迈出的进一步一步。 **[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：基于潜在扩散模型的反事实轨迹生成，用于概念发现</li><li>作者：</li><li>Payal Varshney</li><li>Adriano Lucieri</li><li>Christoph Balada</li><li>Andreas Dengel</li><li>Sheraz Ahmed</li><li>第一作者单位：德国凯撒斯劳滕-兰道莱茵兰-普法尔茨技术大学</li><li>关键词：可解释性、反事实、基于概念的解释、潜在扩散模型、皮肤镜检查、概念发现</li><li>论文链接：https://arxiv.org/abs/2404.10356 Github 代码链接：无</li><li><p>摘要： (1) 研究背景： 深度学习模型在医疗等高风险领域的安全应用需要可信度。理解决策过程不仅有助于建立信任，而且还能揭示复杂模型以前未知的决策标准，从而推进医学研究的发展。从黑盒模型中发现与决策相关的概念是一项特别具有挑战性的任务。 (2) 过去的方法及其问题： 以往的方法主要依赖于梯度或扰动技术，但这些方法生成的反事实图像质量较差，难以从中提取有意义的概念。 本文提出的方法动机充分，它利用扩散模型出色的图像合成能力，通过生成反事实轨迹数据集来发现与分类相关的概念。 (3) 本文提出的研究方法： 本文提出了一种称为 CDCT 的三步框架，用于通过基于潜在扩散的反事实轨迹进行概念发现。该框架包括：</p><ul><li>使用潜在扩散模型生成反事实轨迹数据集。</li><li>利用变分自动编码器从反事实轨迹数据集中提取分类相关概念的解耦表示。</li><li>应用搜索算法在解耦的潜在空间中识别相关概念。 (4) 方法在任务上的表现及性能： 将 CDCT 应用于在最大的公开皮肤病变数据集上训练的分类器，发现了几个偏差和有意义的生物标记。 此外，在 CDCT 中生成的反事实图像显示出比先前建立的最新方法更好的 FID 分数，同时资源效率提高了 12 倍。 这些性能支持了本文提出的方法可以有效发现与分类相关的概念，并为可信赖的人工智能和各个领域的知识发展做出贡献。</li></ul></li><li><p>方法： （1）：潜在扩散模型生成反事实轨迹数据集； （2）：变分自动编码器从反事实轨迹数据集中提取分类相关概念的解耦表示； （3）：搜索算法在解耦的潜在空间中识别相关概念。</p></li><li><p>结论 (1): 本工作通过基于潜在扩散模型的反事实轨迹生成，为概念发现提供了一种可解释且高效的方法，在医疗等高风险领域的安全应用中具有重要意义。 (2): 创新点：</p></li><li>提出了一种称为 CDCT 的三步框架，用于通过基于潜在扩散的反事实轨迹进行概念发现。</li><li>利用扩散模型出色的图像合成能力，生成反事实轨迹数据集，从中提取分类相关概念的解耦表示。</li><li>在解耦的潜在空间中应用搜索算法识别相关概念。 性能：</li><li>在最大的公开皮肤病变数据集上训练的分类器中发现了几个偏差和有意义的生物标记。</li><li>与先前建立的最新方法相比，生成的反事实图像显示出更好的 FID 分数，同时资源效率提高了 12 倍。 工作量：</li><li>该方法易于实现，并且可以应用于各种分类任务。</li><li>训练和部署 CDCT 模型的计算成本相对较低。</li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/576c1e270406787f968cf066ad94df12241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/921adc3c228efd65b737862c3ffcf199241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/4d2bd80c6bf872188ed28b597b22867a241286257.jpg" align="middle"></details>## Efficiently Adversarial Examples Generation for Visual-Language Models under Targeted Transfer Scenarios using Diffusion Models **Authors:Qi Guo, Shanmin Pang, Xiaojun Jia, Qing Guo** Targeted transfer-based attacks involving adversarial examples pose a significant threat to large visual-language models (VLMs). However, the state-of-the-art (SOTA) transfer-based attacks incur high costs due to excessive iteration counts. Furthermore, the generated adversarial examples exhibit pronounced adversarial noise and demonstrate limited efficacy in evading defense methods such as DiffPure. To address these issues, inspired by score matching, we introduce AdvDiffVLM, which utilizes diffusion models to generate natural, unrestricted adversarial examples. Specifically, AdvDiffVLM employs Adaptive Ensemble Gradient Estimation to modify the score during the diffusion model's reverse generation process, ensuring the adversarial examples produced contain natural adversarial semantics and thus possess enhanced transferability. Simultaneously, to enhance the quality of adversarial examples further, we employ the GradCAM-guided Mask method to disperse adversarial semantics throughout the image, rather than concentrating them in a specific area. Experimental results demonstrate that our method achieves a speedup ranging from 10X to 30X compared to existing transfer-based attack methods, while maintaining superior quality of adversarial examples. Additionally, the generated adversarial examples possess strong transferability and exhibit increased robustness against adversarial defense methods. Notably, AdvDiffVLM can successfully attack commercial VLMs, including GPT-4V, in a black-box manner. [PDF](http://arxiv.org/abs/2404.10335v1) **Summary** 利用扩散模型生成逼真的对抗样本，显著提升图像和语言模型对抗攻击效率和安全性。 **Key Takeaways** - 提出 AdvDiffVLM，利用扩散模型生成自然、无约束的对抗样本。 - 采用自适应集成梯度估计，修改扩散模型反向生成过程中的评分，确保对抗样本包含自然的对抗语义。 - 使用 GradCAM 引导掩码方法，将对抗语义分散到整个图像，避免集中在特定区域。 - 与现有转移攻击方法相比，速度提高 10-30 倍，同时保持对抗样本的质量。 - 生成的对抗样本具有很强的迁移性，对对抗防御方法的鲁棒性更高。 - AdvDiffVLM 可以以黑盒方式成功攻击商用 VLM，包括 GPT-4V。 **[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：基于扩散模型的视觉语言模型目标迁移场景下高效对抗样本生成</li><li>作者：郭奇、庞山民、贾晓军、郭庆</li><li>隶属单位：西安交通大学</li><li>关键词：对抗样本、视觉语言模型、扩散模型</li><li>论文链接：https://arxiv.org/abs/2404.10335</li><li><p>摘要： (1) 研究背景： 针对视觉语言模型（VLM）的基于目标迁移的对抗攻击对 VLM 构成重大威胁。然而，最先进的基于迁移的攻击由于过多的迭代次数而产生高昂的成本。此外，生成的对抗样本表现出明显的对抗噪声，并且在规避 DiffPure 等防御方法方面表现出有限的效力。 (2) 过去的方法及问题： AttackVLM 使用基于查询的攻击方法并结合基于迁移的先验，提示黑盒 VLM 产生目标响应。但是，由于需要大量的查询，这个过程非常耗时，通常需要几个小时才能生成一个对抗样本。因此，考虑了另一种黑盒攻击方法，即基于迁移的攻击。然而，如图 1 所示，当前最先进的基于迁移的攻击在生成对抗样本方面也较慢，并且在规避对抗防御方法方面效果较差。此外，这些方法生成的对抗样本表现出明显的噪声。 (3) 论文提出的方法： 为了解决这些问题，受评分匹配和不受限制的对抗样本的启发，提出了 AdvDiffVLM，它使用扩散模型生成自然、不受限制的对抗样本。具体来说，利用并修改了预训练扩散模型的反向生成过程，其中利用自适应集成梯度估计来改变评分并将目标语义嵌入对抗样本中。为了增强输出的自然性，引入了 GradCAM 引导掩码，它将对抗目标语义分散在对抗样本中，而不是将它们集中在特定区域，从而提高图像质量。 (4) 方法在任务和性能上的表现： 该方法只需要几个后向去噪步骤即可生成对抗样本，使其明显快于以前公布的基于迁移的方法。此外，AdvDiffVLM 通过去噪生成对抗样本，对防御方法表现出更高的鲁棒性。实验结果表明，AdvDiffVLM 在目标和迁移场景中针对最先进的基于迁移的攻击实现了数量级或更大的加速，同时提供了具有更高图像质量的对抗样本。此外，这些对抗样本在跨不同 VLM（包括商业 VLM，如 GPT-4V）的迁移中表现出强大的可迁移性。</p></li><li><p>方法： (1): 从评分匹配的角度出发，将对抗攻击建模为生成过程，利用扩散模型的数据分布生成自然、无约束的对抗样本。 (2): 提出自适应集成梯度估计方法，利用 CLIP 模型作为代理模型，估计黑盒 VLM 的梯度信息。 (3): 引入 GradCAM 引导掩码，将对抗目标语义分散在对抗样本中，提高图像质量。</p></li><li><p>结论： （1）本工作提出了一种基于扩散模型的无约束对抗样本生成方法 AdvDiffVLM，它在目标迁移场景下高效地生成对抗样本。 （2）创新点：</p></li><li>提出自适应集成梯度估计方法，利用 CLIP 模型作为代理模型，估计黑盒 VLM 的梯度信息。</li><li>引入 GradCAM 引导掩码，将对抗目标语义分散在对抗样本中，提高图像质量。 性能：</li><li>与现有的系统相比，AdvDiffVLM 的速度提高了 10 倍到 30 倍。</li><li>生成的对抗样本具有较高的图像质量和鲁棒性。 工作量：</li><li>AdvDiffVLM 在不同的 VLM（包括商业 VLM，如 GPT-4V）中表现出强大的可迁移性。</li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/58f36ca28ba6f05864f978063dc05641241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/2e4d299418d7d5553271b20ba81e4ebf241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/aeab2739defd1322978a3eff9a615297241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/70b668e1887faf1bb1428acdafb7a9fc241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/888909a0a5fe2c07ff40b91188d0a47d241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/cb46778d4b426e082ad902ea978cc0fa241286257.jpg" align="middle"></details>## OneActor: Consistent Character Generation via Cluster-Conditioned Guidance **Authors:Jiahao Wang, Caixia Yan, Haonan Lin, Weizhan Zhang** Text-to-image diffusion models benefit artists with high-quality image generation. Yet its stochastic nature prevent artists from creating consistent images of the same character. Existing methods try to tackle this challenge and generate consistent content in various ways. However, they either depend on external data or require expensive tuning of the diffusion model. For this issue, we argue that a lightweight but intricate guidance is enough to function. Aiming at this, we lead the way to formalize the objective of consistent generation, derive a clustering-based score function and propose a novel paradigm, OneActor. We design a cluster-conditioned model which incorporates posterior samples to guide the denoising trajectories towards the target cluster. To overcome the overfitting challenge shared by one-shot tuning pipelines, we devise auxiliary components to simultaneously augment the tuning and regulate the inference. This technique is later verified to significantly enhance the content diversity of generated images. Comprehensive experiments show that our method outperforms a variety of baselines with satisfactory character consistency, superior prompt conformity as well as high image quality. And our method is at least 4 times faster than tuning-based baselines. Furthermore, to our best knowledge, we first prove that the semantic space has the same interpolation property as the latent space dose. This property can serve as another promising tool for fine generation control. [PDF](http://arxiv.org/abs/2404.10267v1) **Summary** 通过引入聚类条件模型和一个新的范例OneActor，该研究实现了无监督一致图像生成，同时保持图像质量和多样性。 **Key Takeaways** * 无需外部数据或昂贵的微调即可实现一致图像生成。 * 提出了一种基于聚类的评分函数，将后验样本纳入去噪轨迹。 * 为微调和推理设计了辅助组件以增强多样性和避免过拟合。 * 实验表明该方法在字符一致性、提示符合性和图像质量方面优于基线。 * 微调速度至少是基于微调的基线的 4 倍。 * 证明了语义空间具有与潜在空间相同的插值属性。 **[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：OneActor：通过集群条件引导实现一致的角色生成</li><li>作者：Jiahao Wang、Caixia Yan、Haonan Lin、Weizhan Zhang</li><li>单位：西安交通大学计算机科学与技术学院</li><li>关键词：文本到图像、扩散模型、一致性生成、集群引导</li><li>链接：https://arxiv.org/abs/2404.10267</li><li>摘要： (1) 研究背景：文本到图像扩散模型为艺术家提供了高质量的图像生成能力，但其随机性使得艺术家无法创建同一角色的一致图像。 (2) 过去方法：现有方法通过各种方式尝试解决这一挑战，但它们要么依赖于外部数据，要么需要对扩散模型进行昂贵的微调。 (3) 研究方法：本文提出了一种轻量级但复杂的指导方法，通过形式化一致性生成的目标、推导基于聚类的评分函数，并提出了一种新范式 OneActor。设计了一个集群条件模型，将后验样本纳入其中，引导去噪轨迹朝向目标集群。为了克服单次微调管道中常见的过度拟合挑战，设计了辅助组件来同时增强微调和调节推理。 (4) 实验结果：该方法在各种基准测试中优于其他方法，具有令人满意的角色一致性、出色的提示符合性以及较高的图像质量。并且该方法至少比基于微调的基线快 4 倍。此外，本文首次证明语义空间与潜在空间具有相同的插值特性，该特性可以作为细粒度生成控制的另一有前途的工具。</li></ol><p>7.方法：(1)定义问题：给定用户定义的描述提示 pt（例如，一个穿着长袍的霍比特人），标准扩散模型 ϵθ 生成用户首选图像 xt 作为目标角色。我们的目标是为原始 ϵθ 配备一个支持网络 ϕ，制定 ϵθ,ϕ。在对 ϕ 进行快速微调后，我们的模型 ϵθ,ϕ 可以生成具有任何其他以角色为中心的提示（例如，一个穿着长袍的霍比特人 + 在街上行走）的同一角色的一致图像。为了完成这项任务，我们在第 4.2 节中首先进行数学分析。然后我们在第 4.3 节中构建一个集群条件模型，这是一个自引导扩散模型。我们在第 4.4 节中以对比方式对其进行微调。最后在推理期间，我们在第 4.5 节中使用语义插值生成各种一致的图像，并在第 4.6 节中使用潜在引导。(2)集群引导评分函数的推导：给定用户定义的提示 pt（例如，一个穿着白色连衣裙的美丽女孩）和相应的 base 词 wb（例如，女孩），我们将 pt 输入标准扩散模型 ϵθ 以获得 N 个 base 图像 Xbase={xbasei}Ni=1。我们随机选择一张图片作为目标角色 xtar，并将其他图片标记为辅助样本 Xaux={xauxi}N−1i=1。我们对目标图像应用人脸裁剪和图像翻转，得到增强集 Xtar={xtari}Mi=1。在标准潜在扩散模型 ϵθ(zt, t, c) 中，用于文本控制的条件 c 由文本编码器 Et 生成，该编码器将文本提示 p 投影到语义向量：c=Et(p)。然后这些语义向量引导去噪网络从初始潜在噪声 zT 采样去噪 z0。在我们的任务中，我们假设在 z0 的潜在空间中，有不同的集群对应于角色的不同显着身份。给定不同的初始 zT 和相同的字符条件 pt，ϵθ 无法到达一个特定的集群，而是扩散到不同集群的区域。也就是说，Xbase 的生成形成一个 base 区域 Sbase，其中包括一个目标集群和几个辅助集群：Sbase⊃Star1∪Saux1∪Saux2∪... 一致性生成的关键是如何引导 ϵθ 到预期的目标集群 Star。首先，我们形式化一致性生成问题。从面向结果的角度来看，我们希望增加生成目标集群 Star 图像的概率，并降低辅助集群 Sauxi 的概率。因此，如果我们将原始扩散过程视为先验分布 p(x)，则我们的期望分布可以表示为：p(x)·p(Star|x)η1 / N−1i=1p(Sauxi|x)η2，其中 η1、η2 是比例因子。我们应用贝叶斯规则推导出：p(x)·(p(x|Star)p(Star)p(x))η1 / N−1i=1(p(x|Sauxi)p(Sauxi)p(x))η2，我们进一步取其对数概率的梯度并忽略无关项，得到：∇logp(x)+η1·[∇logp(x|Star)−∇logp(x)]−η2·N−1i=1[∇logp(x|Sauxi)−∇logp(x)]。如果我们引入评分函数的概念（Song 等人，[2021]），Eq.6 中的每个术语都表示一个评分并指导推理过程。利用 Ho 等人 [2020] 的重新参数化技巧，我们可以将评分表示为潜在空间中去噪网络 θ 的预测：ϵθ(zt, t)+η1·[ϵθ(zt, t, Star)−ϵθ(zt, t)]−η2·N−1i=1[ϵθ(zt, t, Sauxi)−ϵθ(zt, t)]。这个公式称为集群引导评分函数，是我们工作的核心。我们将在后续部分中实现它。(3)使用语义表示的集群条件模型：根据 Eq.7，我们需要将集群表示引入我们的管道并建立一个集群条件模型 ϵθ(zt, t, S)。为此，我们将样本的潜在代码视为集群表示，并将 ϕ 构建为编码器。如图 2 所示，它将潜在代码 z 作为输入，并输出一个特定于角色的向量 ∆c。此向量表示角色集群的语义方向。给定文本提示 p，我们将提示嵌入拆分为逐词嵌入：c={cwi}，其中 wi 是提示的第 i 个单词。我们将 base 词嵌入 cw 和偏移输出向量连接起来：c′wb=cwb+∆c。直观地说，ϕ 就像文本编码器一样，将潜在代码投影到语义嵌入中。因此，我们将 ϕ 称为潜在编码器。潜在编码器由一个提取器和一个投影仪组成。由于原始 U-Net 编码器 Eu 已经经过良好训练，可以从潜在样本中提取特征，因此我们直接将其用作提取器。投影仪是一个多层 ResNet（He 等人，[2016]）和线性网络，带有层归一化。在微调期间，我们只激发投影仪并冻结所有其他组件。然而，U-Net 提取器可能会造成额外的计算负担。为了简化，我们用 z1 的特征近似 z0 的特征：h=Eu(z1, c)≈Eu(z0)。因此，在生成 base 图像时，我们在最后一个采样步骤中保存 U-Net 编码器的输出 H={hi}Ni=1。h 可以近似为提取器的输出。在微调和推理过程中，h 直接馈送到投影仪：∆c=ϕ(h)。通过这种近似，我们避免了 U-Net 提取器的额外计算，并将计算成本降低了 30%。到目前为止，所有可以确定集群的因素（即 p、h）都可以由文本编码器和潜在编码器处理。因此，我们可以将 Eq.7 中的集群条件项表示为：ϵθ(zt, t, S)=ϵθ,ϕ(zt, t, Et(p), ϕ(h))。(4)使用辅助样本的广义简化微调：在我们的集群条件模型的微调过程中，扩散模型的内在属性是关键方面。我们的目标是充分利用目标和辅助样本：平衡和稳定投影仪。一次性微调中的主要挑战是过拟合。数据的不足可能会导致严重的偏差和生成图像的多样性有限。为了克服这一挑战，我们不仅使用目标样本，还使用辅助样本对模型进行微调。具体来说，我们随机选择 1 个目标和 K 个辅助样本来形成一批数据：B={xtar, htar}∪{xauxi, hauxi}Ki=1。因此，更多的数据有助于优化，并且批处理归一化可以应用于投影仪，从而产生更通用的投影。对于微调管道，我们首先通过：z=Ea(x), x∈B 为它们添加噪声 ϵt 来获得潜在代码。然后如图所示，我们将噪声潜在 zt、提示 p 和特征 h 输入集群条件模型。提示是一个随机模板，填充有 base 词（例如，一个女孩的肖像）。我们对目标和辅助样本应用标准去噪损失：Ltar(ϕ)=Et∈[1,T],ztar0,ϵt∥ϵt−ϵθ,ϕ(ztart, t, Et(p), ϕ(htar))∥2，Laux(ϕ)=Et∈[1,T],zaux0,ϵt∥ϵt−ϵθ,ϕ(zauxt, t, Et(p), ϕ(haux))∥2。对于辅助项 Eq.7，计算每个步骤 N-1 个辅助条件的去噪预测非常费力。为了简化，我们提出了一个平均条件 ∆¯c。它是通过对辅助条件的条件向量求平均获得的：∆¯c=1KKi=1∆cauxi=1KKi=1ϕ(hauxi)。直观地说，这个平均条件表示所有辅助集群的中心。因此，可以用这个平均预测来近似辅助去噪预测。去噪损失也用于此条件：Laver(ϕ)=Et∈[1,T],ztar0,ϵt∥ϵt−ϵθ,ϕ(ztart, t, Et(p), ∆¯c)∥2。请注意，这里的数据来自目标集。此平均条件将充当无分类器引导（Ho 和 Salimans [2022]）的空条件，如第 4.6 节中所述。(5)语义插值的证明和实现：在推理过程中，传统的基于微调的管道在一致性和多样性之间存在相同的失衡。为了解决这个问题，我们提出了一个温和的策略，语义插值，它利用了扩散模型的内部容量。无分类器引导模型（Ho 和 Salimans [2022]）证明了潜在空间中的条件插值和外推，这表明了我们的论点。</p><p>8.结论： （1）：本文提出了一种轻量级且高效的指导方法，通过形式化一致性生成的目标、推导基于聚类的评分函数，并提出了一种新范式 OneActor，解决了文本到图像扩散模型中角色一致性生成的问题。该方法在各种基准测试中优于其他方法，具有令人满意的角色一致性、出色的提示符合性以及较高的图像质量。并且该方法至少比基于微调的基线快 4 倍。此外，本文首次证明语义空间与潜在空间具有相同的插值特性，该特性可以作为细粒度生成控制的另一有前途的工具。 （2）：创新点：提出了一种基于集群条件引导的轻量级且高效的文本到图像扩散模型，实现了角色一致性生成。 性能：在各种基准测试中优于其他方法，具有令人满意的角色一致性、出色的提示符合性以及较高的图像质量。并且该方法至少比基于微调的基线快 4 倍。 工作量：该方法的实现相对简单，并且可以在各种文本到图像扩散模型上轻松部署。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/89b6f16a0a761f249e333f44a5168204241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/ac080ab3aeb6fbbdbf4314b392f0a4e6241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/a05f66aebcfc1504a123423e38d9d751241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/81b50b6c224994210d30702ea8e901e3241286257.jpg" align="middle"></details><h2 id="Taming-Latent-Diffusion-Model-for-Neural-Radiance-Field-Inpainting"><a href="#Taming-Latent-Diffusion-Model-for-Neural-Radiance-Field-Inpainting" class="headerlink" title="Taming Latent Diffusion Model for Neural Radiance Field Inpainting"></a>Taming Latent Diffusion Model for Neural Radiance Field Inpainting</h2><p><strong>Authors:Chieh Hubert Lin, Changil Kim, Jia-Bin Huang, Qinbo Li, Chih-Yao Ma, Johannes Kopf, Ming-Hsuan Yang, Hung-Yu Tseng</strong></p><p>Neural Radiance Field (NeRF) is a representation for 3D reconstruction from multi-view images. Despite some recent work showing preliminary success in editing a reconstructed NeRF with diffusion prior, they remain struggling to synthesize reasonable geometry in completely uncovered regions. One major reason is the high diversity of synthetic contents from the diffusion model, which hinders the radiance field from converging to a crisp and deterministic geometry. Moreover, applying latent diffusion models on real data often yields a textural shift incoherent to the image condition due to auto-encoding errors. These two problems are further reinforced with the use of pixel-distance losses. To address these issues, we propose tempering the diffusion model’s stochasticity with per-scene customization and mitigating the textural shift with masked adversarial training. During the analyses, we also found the commonly used pixel and perceptual losses are harmful in the NeRF inpainting task. Through rigorous experiments, our framework yields state-of-the-art NeRF inpainting results on various real-world scenes. Project page: <a target="_blank" rel="noopener" href="https://hubert0527.github.io/MALD-NeRF">https://hubert0527.github.io/MALD-NeRF</a></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.09995v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://hubert0527.github.io/MALD-NeRF">https://hubert0527.github.io/MALD-NeRF</a></p><p><strong>Summary</strong><br>使用场景定制和对抗性训练来改进神经辐射场（NeRF）图像修复中的扩散模型。</p><p><strong>Key Takeaways</strong></p><ul><li>扩散模型在 NeRF 图像修复中面临合成几何和纹理变化挑战。</li><li>像素差异损失在 NeRF 图像修复任务中是有害的。</li><li>使用场景定制可以减少扩散模型的随机性。</li><li>对抗性训练可以减轻纹理变化。</li><li>感知损失在 NeRF 图像修复中也不理想。</li><li>该框架在各种真实场景上实现了最先进的 NeRF 图像修复结果。</li><li>该方法解决了 NeRF 图像修复中的两个主要问题：合成几何和纹理变化。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：驯服隐式扩散模型用于补充材料</li><li>作者：C.H. Lin 等</li><li>隶属：未提及</li><li>关键词：NeRF、隐式扩散模型、图像编辑、图像生成、深度学习</li><li>论文链接：None</li><li><p>摘要： （1）研究背景：NeRF 是从多视图图像进行三维重建的一种表示方法。尽管最近一些工作展示了使用扩散先验编辑重建的 NeRF 的初步成功，但它们仍然难以在完全未覆盖的区域中合成合理的几何形状。 （2）过去方法及其问题：一个主要原因是扩散模型合成内容的高度多样性，这阻碍了辐射场收敛到清晰且确定性的几何形状。此外，由于自动编码错误，在真实数据上应用隐式扩散模型通常会导致与图像条件不一致的纹理偏移。这两个问题在使用像素距离损失时进一步加剧。 （3）本文提出的研究方法：为了解决这些问题，我们提出用特定于场景的定制来缓和扩散模型的随机性，并用掩码对抗性训练来减轻纹理偏移。在分析过程中，我们还发现常用的像素和感知损失在 NeRF inpainting 任务中有害。 （4）方法在什么任务上取得了什么性能：通过严格的实验，我们的框架在各种真实世界场景上产生了最先进的 NeRF inpainting 结果。这些性能支持了他们的目标。</p></li><li><p>方法： (1): 提出了一种用特定于场景的定制来缓和扩散模型的随机性，并用掩码对抗性训练来减轻纹理偏移的方法； (2): 使用预先在内部图像修复数据集上训练的潜在扩散模型来修复二维图像，然后替换用于 NeRF 训练的输入图像； (3): 使用像素级损失函数来重建输入图像中的已知区域，同时使用掩码对抗性训练来修复修复区域； (4): 设计了一种掩码对抗性训练方案，从判别器中隐藏图像补丁上的重建/修复边界； (5): 利用现成的单目深度先验来规范学习到的 NeRF 的几何形状； (6): 使用迭代数据设置更新和噪声调度，以减轻扩散模型的多样性和随机性带来的不一致问题。</p></li></ol><p>8.结论： （1）本工作的重要意义：本文提出了一种用于 NeRFinpainting 的新框架，该框架通过缓解扩散模型的随机性和减轻纹理偏移来显着提高 NeRF 的编辑质量。 （2）本文的创新点、性能和工作量： 创新点： - 提出了一种用特定于场景的定制来缓和扩散模型的随机性，并用掩码对抗性训练来减轻纹理偏移的方法。 - 设计了一种掩码对抗性训练方案，从判别器中隐藏图像补丁上的重建/修复边界。 性能： - 在各种真实世界场景上产生了最先进的 NeRFinpainting 结果。 工作量： - 实验设置复杂，需要大量的计算资源。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/308cb7c2143fa740c4192a671925dee1241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/68fb1ff3b56d69424096a269f5033b6e241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/8d6fc19a0079ac001db1a67326957da6241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/bad14fb5377a3790949b10e7d83b5ddb241286257.jpg" align="middle"></details>## Ctrl-Adapter: An Efficient and Versatile Framework for Adapting Diverse Controls to Any Diffusion Model **Authors:Han Lin, Jaemin Cho, Abhay Zala, Mohit Bansal** ControlNets are widely used for adding spatial control in image generation with different conditions, such as depth maps, canny edges, and human poses. However, there are several challenges when leveraging the pretrained image ControlNets for controlled video generation. First, pretrained ControlNet cannot be directly plugged into new backbone models due to the mismatch of feature spaces, and the cost of training ControlNets for new backbones is a big burden. Second, ControlNet features for different frames might not effectively handle the temporal consistency. To address these challenges, we introduce Ctrl-Adapter, an efficient and versatile framework that adds diverse controls to any image/video diffusion models, by adapting pretrained ControlNets (and improving temporal alignment for videos). Ctrl-Adapter provides diverse capabilities including image control, video control, video control with sparse frames, multi-condition control, compatibility with different backbones, adaptation to unseen control conditions, and video editing. In Ctrl-Adapter, we train adapter layers that fuse pretrained ControlNet features to different image/video diffusion models, while keeping the parameters of the ControlNets and the diffusion models frozen. Ctrl-Adapter consists of temporal and spatial modules so that it can effectively handle the temporal consistency of videos. We also propose latent skipping and inverse timestep sampling for robust adaptation and sparse control. Moreover, Ctrl-Adapter enables control from multiple conditions by simply taking the (weighted) average of ControlNet outputs. With diverse image/video diffusion backbones (SDXL, Hotshot-XL, I2VGen-XL, and SVD), Ctrl-Adapter matches ControlNet for image control and outperforms all baselines for video control (achieving the SOTA accuracy on the DAVIS 2017 dataset) with significantly lower computational costs (less than 10 GPU hours). [PDF](http://arxiv.org/abs/2404.09967v1) First two authors contributed equally; Project page: https://ctrl-adapter.github.io/ **Summary** 预训练图像控制网络通过一个高效通用框架 Ctrl-Adapter 拓展至视频生成，适配各类图像/视频扩散模型，为视频控制带来多样功能。 **Key Takeaways** - Ctrl-Adapter 将预训练控制网络适配到扩散模型，实现图像/视频控制。 - 适配层融合控制网络特征，保持控制网络和扩散模型参数不变。 - 时空模块处理视频的时间一致性。 - 潜在跳过和逆时步采样提升适应性和稀疏控制。 - 加权平均控制网络输出实现多条件控制。 - 适配各类扩散模型，图像控制效果与控制网络匹配，视频控制效果远超基线。 - 计算成本极低（低于 10 个 GPU 小时）。 **[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：CTRL-Adapter：一种高效且通用的框架，可将各种控制适配到任何扩散模型</li><li>作者：Han Lin、Jaemin Cho、Abhay Zala、Mohit Bansal</li><li>隶属机构：北卡罗来纳大学教堂山分校</li><li>关键词：控制网络、视频生成、扩散模型、条件控制、视频编辑</li><li>论文链接：https://arxiv.org/abs/2404.09967</li><li><p>摘要： (1) 研究背景：控制网络广泛用于图像生成中添加空间控制，但直接将预训练的图像控制网络用于视频生成面临挑战。 (2) 过去的方法：预训练的控制网络无法直接插入新的主干模型，且为新主干模型训练控制网络的成本很高；不同帧的控制网络特征可能无法有效处理对象的时序一致性。 (3) 研究方法：提出 CTRL-Adapter 框架，通过适配预训练的控制网络（并改进视频的时间对齐）将各种控制添加到图像/视频扩散模型中。 (4) 性能：CTRL-Adapter 具有图像控制、视频控制、稀疏帧视频控制、多条件控制、与不同主干模型的兼容性、对未见控制条件的适应以及视频编辑等强大且多样的功能。</p></li><li><p>Methods: (1) CTRL-Adapter框架：提出一种适配器，将预训练的控制网络与扩散模型连接，并通过时间对齐模块处理视频的时间一致性。 (2) 预训练控制网络适配：设计一种适配器，将预训练的控制网络特征映射到扩散模型的潜在空间，实现不同主干模型的兼容性。 (3) 视频时间对齐：引入时间对齐模块，通过循环一致性损失和时间平滑损失，确保不同帧的控制网络特征在时间上保持一致。 (4) 多条件控制：提出一种多条件控制机制，允许同时使用多个控制条件，并通过条件混合器将不同条件的特征融合。 (5) 未见控制条件适应：利用对抗性训练，使CTRL-Adapter能够适应未在训练集中出现的控制条件。 (6) 视频编辑：通过控制网络的掩码机制，实现对视频特定区域或帧的编辑。</p></li><li><p>结论： （1）：本文提出 CTRL-Adapter 框架，该框架通过适配和时间对齐预训练的 ControlNet，可以将各种控制添加到任何图像/视频扩散模型中，同时保持 ControlNet 和骨干扩散模型的参数不变。训练 CTRL-Adapter 明显比为新骨干模型训练 ControlNet 更有效率。CTRL-Adapter 还提供了许多有用的功能，包括图像控制、视频控制、具有稀疏输入的视频控制和多源控制。我们通过综合分析实证展示了 CTRL-Adapter 的有用性。使用不同的图像和视频扩散骨干，训练 CTRL-Adapter 可以匹配或优于训练新的 ControlNet，同时降低计算成本。CTRL-Adapter 对未见条件执行零样本适应，并帮助生成具有稀疏帧条件或多个条件（例如，深度图、Canny 边缘、人体姿势和曲面法线）的视频。我们还为 CTRL-Adapter 的设计选择和定性示例提供了全面的消融研究。我们希望我们的工作可以促进未来视频和图像高效受控生成的研究。 致谢：这项工作得到了 DARPA ECole 计划号 HR00112390060、NSF-AI Engage 研究所 DRL-2112635、DARPA 机器常识 (MCS) 补助金 N66001-19-2-4031、ARO 奖金 W911NF2110220、ONR 补助金 N00014-23-1-2356 和 Bloomberg 数据科学博士奖学金的支持。本文中包含的观点是作者的观点，不代表资助机构的观点。 （2）：创新点：提出 CTRL-Adapter 框架，该框架通过适配和时间对齐预训练的 ControlNet，可以将各种控制添加到任何图像/视频扩散模型中，同时保持 ControlNet 和骨干扩散模型的参数不变。 性能：CTRL-Adapter 在图像控制、视频控制、具有稀疏输入的视频控制和多源控制方面表现出强大且多样的功能。它可以与不同的图像和视频扩散骨干兼容，并对未见控制条件具有适应性。 工作量：训练 CTRL-Adapter 明显比为新骨干模型训练 ControlNet 更有效率。它还可以对未见控制条件执行零样本适应，并帮助生成具有稀疏帧条件或多个条件的视频。</p></li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/c25f069f15093064473dcdf4dfaa56ee241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/f677d02b9dd20e46686e3c8c08280d9d241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/ce2e3a267b9806afbb6606fbc59d0755241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/33b547f986f3df3860ea96d4a3aa9f63241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/48ab1365ae4e53ae2b1e46fafa4c5b7c241286257.jpg" align="middle"></details><h2 id="A-Diffusion-based-Data-Generator-for-Training-Object-Recognition-Models-in-Ultra-Range-Distance"><a href="#A-Diffusion-based-Data-Generator-for-Training-Object-Recognition-Models-in-Ultra-Range-Distance" class="headerlink" title="A Diffusion-based Data Generator for Training Object Recognition Models   in Ultra-Range Distance"></a>A Diffusion-based Data Generator for Training Object Recognition Models in Ultra-Range Distance</h2><p><strong>Authors:Eran Bamani, Eden Nissinman, Lisa Koenigsberg, Inbar Meir, Avishai Sintov</strong></p><p>Object recognition, commonly performed by a camera, is a fundamental requirement for robots to complete complex tasks. Some tasks require recognizing objects far from the robot’s camera. A challenging example is Ultra-Range Gesture Recognition (URGR) in human-robot interaction where the user exhibits directive gestures at a distance of up to 25~m from the robot. However, training a model to recognize hardly visible objects located in ultra-range requires an exhaustive collection of a significant amount of labeled samples. The generation of synthetic training datasets is a recent solution to the lack of real-world data, while unable to properly replicate the realistic visual characteristics of distant objects in images. In this letter, we propose the Diffusion in Ultra-Range (DUR) framework based on a Diffusion model to generate labeled images of distant objects in various scenes. The DUR generator receives a desired distance and class (e.g., gesture) and outputs a corresponding synthetic image. We apply DUR to train a URGR model with directive gestures in which fine details of the gesturing hand are challenging to distinguish. DUR is compared to other types of generative models showcasing superiority both in fidelity and in recognition success rate when training a URGR model. More importantly, training a DUR model on a limited amount of real data and then using it to generate synthetic data for training a URGR model outperforms directly training the URGR model on real data. The synthetic-based URGR model is also demonstrated in gesture-based direction of a ground robot.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.09846v1">PDF</a></p><p><strong>Summary</strong><br>通过扩散模型生成远距离物体图像，提升机器人手势识别能力。</p><p><strong>Key Takeaways</strong></p><ul><li>针对远距离手势识别，提出了基于扩散模型的DUR框架。</li><li>DUR接收距离和类别，生成相应远距离物体合成图像。</li><li>DUR生成的图像在保真度和识别成功率上优于其他生成模型。</li><li>URGR模型在真实数据和DUR合成数据上训练都优于直接在真实数据上训练。</li><li>基于DUR合成数据的URGR模型可用于地面机器人的手势引导。</li><li>DUR框架可推广到其他远距离物体识别任务。</li><li>合成数据集缓解了真实世界数据缺乏的问题，增强了模型的泛化能力。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：基于扩散的超远距离物体识别模型训练数据生成器</li><li>作者：Eran Bamani, Eden Nissinman, Lisa Koenigsberg, Inbar Meir, Avishai Sintov</li><li>隶属单位：特拉维夫大学机械工程学院</li><li>关键词：超远距离手势识别、数据生成、扩散模型</li><li>论文链接：https://arxiv.org/abs/2404.09846 Github 代码链接：无</li><li><p>摘要： (1)：研究背景：物体识别是机器人有效与环境交互的基础，但随着机器人与物体距离的增加，图像分辨率下降，识别性能会因失去判别性视觉特征而显着下降。 (2)：过去方法及其问题：生成对抗网络（GAN）等传统生成模型无法很好地复制远距离物体在图像中的真实视觉特征。 (3)：本文提出的研究方法：提出了一种基于扩散模型的超远距离扩散（DUR）框架，可以生成具有不同场景和距离条件的远距离物体标记图像。 (4)：方法在任务和性能上的表现：DUR 在超远距离手势识别（URGR）任务上训练了一个模型，该模型在距离高达 25 米的室内外环境中表现出良好的识别成功率。此外，基于合成数据的 URGR 模型还被证明可以用于基于手势的地面机器人导航。</p></li><li><p>方法： (1): 提出超远距离扩散（DUR）框架，基于扩散模型生成不同场景和距离条件下的远距离物体标记图像； (2): 定义超远距离物体识别问题，目标是根据距离小于 25 米的 RGB 图像，将图像基于展示的对象分类为 m 个可能的类别之一； (3): 收集标记数据集，包括图像、手势类别索引和距离信息，并使用 YOLOv8 检测用户并裁剪背景，使用 HQ-Net 增强图像质量； (4): 使用去噪扩散概率模型（DDPM）作为生成模型，通过逐渐添加和反转噪声来生成合成图像； (5): 学习模型 pθ 近似条件概率，以重建原始分布的图像。</p></li><li><p>结论： (1): 本工作提出了一种基于扩散模型的超远距离扩散（DUR）框架，该框架可以生成具有不同场景和距离条件的远距离物体标记图像，为超远距离物体识别任务提供了丰富的训练数据。 (2): 创新点：</p><ul><li>提出基于扩散模型的超远距离扩散（DUR）框架，为超远距离物体识别任务生成逼真的合成训练数据。</li><li>定义了超远距离物体识别问题，并收集了包含图像、手势类别索引和距离信息的标记数据集。</li><li>使用去噪扩散概率模型（DDPM）作为生成模型，通过逐渐添加和反转噪声来生成合成图像。</li></ul></li><li>性能：<ul><li>在超远距离手势识别（URGR）任务上训练了一个模型，该模型在距离高达25米的室内外环境中表现出良好的识别成功率。</li><li>基于合成数据的URGR模型还被证明可以用于基于手势的地面机器人导航。</li></ul></li><li>工作量：<ul><li>收集了包含50,000张图像的标记数据集，并使用YOLOv8和HQ-Net对图像进行了预处理。</li><li>训练了去噪扩散概率模型（DDPM），以生成具有不同场景和距离条件的远距离物体标记图像。</li><li>在超远距离手势识别（URGR）任务上训练了一个模型，并评估了其性能。</li></ul></li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/6b8ecb308186704259aa27fb23638660241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/e63d9562d6323864e562317f7c12ea55241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/c14de792d860790e380b7de225a0fa74241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/958ce44fd42dfbbff812a95c9714e514241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/14eb7f29fa6a016695804dd8fc230360241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/31c2f09164cbb10143e3860a7c7e3599241286257.jpg" align="middle"></details><h2 id="Digging-into-contrastive-learning-for-robust-depth-estimation-with-diffusion-models"><a href="#Digging-into-contrastive-learning-for-robust-depth-estimation-with-diffusion-models" class="headerlink" title="Digging into contrastive learning for robust depth estimation with   diffusion models"></a>Digging into contrastive learning for robust depth estimation with diffusion models</h2><p><strong>Authors:Jiyuan Wang, Chunyu Lin, Lang Nie, Kang Liao, Shuwei Shao, Yao Zhao</strong></p><p>Recently, diffusion-based depth estimation methods have drawn widespread attention due to their elegant denoising patterns and promising performance. However, they are typically unreliable under adverse conditions prevalent in real-world scenarios, such as rainy, snowy, etc. In this paper, we propose a novel robust depth estimation method called D4RD, featuring a custom contrastive learning mode tailored for diffusion models to mitigate performance degradation in complex environments. Concretely, we integrate the strength of knowledge distillation into contrastive learning, building the `trinity’ contrastive scheme. This scheme utilizes the sampled noise of the forward diffusion process as a natural reference, guiding the predicted noise in diverse scenes toward a more stable and precise optimum. Moreover, we extend noise-level trinity to encompass more generic feature and image levels, establishing a multi-level contrast to distribute the burden of robust perception across the overall network. Before addressing complex scenarios, we enhance the stability of the baseline diffusion model with three straightforward yet effective improvements, which facilitate convergence and remove depth outliers. Extensive experiments demonstrate that D4RD surpasses existing state-of-the-art solutions on synthetic corruption datasets and real-world weather conditions. The code for D4RD will be made available for further exploration and adoption.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.09831v1">PDF</a> 8 pages,6 figures</p><p><strong>Summary</strong><br>扩散深度估计中的三元对比学习模式，增强了模型对复杂环境的鲁棒性。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种新型的基于扩散的鲁棒深度估计方法 D4RD。</li><li>引入了定制的对比学习模式，以减轻复杂环境中的性能下降。</li><li>提出了一种“三元”对比方案，利用前向扩散过程中的采样噪声作为自然参考。</li><li>将噪声水平三元扩展到更通用的特征和图像级别，建立多层次对比。</li><li>通过三个简单的改进增强了基线扩散模型的稳定性。</li><li>D4RD 在合成损坏数据集和真实世界天气条件下优于现有最先进的解决方案。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：基于对比学习的鲁棒深度估计</li><li>作者：王继源、林春宇、聂朗、廖康、邵书伟、赵尧</li><li>第一作者单位：北京交通大学</li><li>关键词：深度估计、鲁棒感知、自监督学习、扩散方法</li><li>论文链接：None</li><li><p>摘要： （1）研究背景：扩散模型在深度估计中取得了优异的性能，但它们在雨雪等恶劣条件下表现不可靠。 （2）过去方法和问题：对比学习和知识蒸馏分别用于增强鲁棒性，但存在一些不足，例如对比学习容易受噪声影响，知识蒸馏需要高质量的伪标签。 （3）研究方法：本文提出了 D4RD，它结合了对比学习和知识蒸馏，构建了一个“三元”对比方案。该方案利用正向扩散过程中的采样噪声作为自然参考，引导不同场景中的预测噪声朝向更稳定和精确的极值。此外，还扩展了噪声级三元对比到更通用的特征和图像级别，建立了多级对比以跨整个网络分配鲁棒感知的负担。 （4）任务和性能：在合成损坏数据集和真实世界天气条件下，D4RD 超越了现有的最先进解决方案。这些结果表明，该方法可以有效地增强鲁棒性并减轻恶劣条件下的性能下降。</p></li><li><p><strong>方法</strong>： （1）提出了一种新的多级“三元”对比方案，以增强鲁棒性和减轻恶劣条件下的性能下降； （2）利用正向扩散过程中的采样噪声作为自然参考，引导不同场景中的预测噪声朝向更稳定和精确的极值； （3）将噪声级三元对比扩展到更通用的特征和图像级别，建立多级对比以跨整个网络分配鲁棒感知的负担。</p></li><li><p>结论： （1）本文提出了 D4RD，一种基于对比学习的鲁棒深度估计框架，增强了基于扩散的深度估计的稳定性和收敛性。 （2）创新点：</p></li><li>提出了一种新的多级“三元”对比方案，以增强鲁棒性和减轻恶劣条件下的性能下降。</li><li>利用正向扩散过程中的采样噪声作为自然参考，引导不同场景中的预测噪声朝向更稳定和精确的极值。</li><li>将噪声级三元对比扩展到更通用的特征和图像级别，建立多级对比以跨整个网络分配鲁棒感知的负担。 性能：</li><li>在合成损坏数据集和真实世界天气条件下，D4RD 超越了现有的最先进解决方案。</li><li>这些结果表明，该方法可以有效地增强鲁棒性并减轻恶劣条件下的性能下降。 工作量：</li><li>该方法需要对正向扩散过程中的采样噪声进行建模，这可能需要额外的计算开销。</li><li>多级对比方案的实现也可能增加模型的复杂性。</li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/944e85b707a7fb7eff2e43b4ba0298bc241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/bab219f0ca75b9f2908a62071e68eeda241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/c66b109acaddbd0ab6aa42c7965a9683241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/d9984015fa79bdccf3644f3446f0ee43241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/5ab47e273e1c34b2c8a6d3ee8c66b6ef241286257.jpg" align="middle"></details><h2 id="Equipping-Diffusion-Models-with-Differentiable-Spatial-Entropy-for-Low-Light-Image-Enhancement"><a href="#Equipping-Diffusion-Models-with-Differentiable-Spatial-Entropy-for-Low-Light-Image-Enhancement" class="headerlink" title="Equipping Diffusion Models with Differentiable Spatial Entropy for   Low-Light Image Enhancement"></a>Equipping Diffusion Models with Differentiable Spatial Entropy for Low-Light Image Enhancement</h2><p><strong>Authors:Wenyi Lian, Wenjing Lian, Ziwei Luo</strong></p><p>Image restoration, which aims to recover high-quality images from their corrupted counterparts, often faces the challenge of being an ill-posed problem that allows multiple solutions for a single input. However, most deep learning based works simply employ l1 loss to train their network in a deterministic way, resulting in over-smoothed predictions with inferior perceptual quality. In this work, we propose a novel method that shifts the focus from a deterministic pixel-by-pixel comparison to a statistical perspective, emphasizing the learning of distributions rather than individual pixel values. The core idea is to introduce spatial entropy into the loss function to measure the distribution difference between predictions and targets. To make this spatial entropy differentiable, we employ kernel density estimation (KDE) to approximate the probabilities for specific intensity values of each pixel with their neighbor areas. Specifically, we equip the entropy with diffusion models and aim for superior accuracy and enhanced perceptual quality over l1 based noise matching loss. In the experiments, we evaluate the proposed method for low light enhancement on two datasets and the NTIRE challenge 2024. All these results illustrate the effectiveness of our statistic-based entropy loss. Code is available at <a target="_blank" rel="noopener" href="https://github.com/shermanlian/spatial-entropy-loss">https://github.com/shermanlian/spatial-entropy-loss</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.09735v1">PDF</a> CVPRW 2024, best LPIPS in the NTIRE low light enhancement challenge 2024</p><p><strong>Summary</strong></p><p>图像修复从损坏的图像中恢复高质量图像，通常面临病态问题，即单一输入有多个解。然而，多数基于深度学习的工作使用 l1 损失，以确定性方式训练网络，导致过度平滑的预测，感知质量较差。本文提出了一种新方法，将重点从确定性逐像素比较转移到统计角度，强调学习分布而非个别像素值。核心思想是在损失函数中引入空间熵，测量预测与目标之间的分布差异。为了使空间熵可微，我们使用核密度估计 (KDE) 来近似每个像素与其邻域的特定强度值的概率。具体来说，我们用扩散模型装备熵，并针对基于 l1 的噪声匹配损失，追求更高的准确性和增强的感知质量。在实验中，我们评估了所提方法在两个数据集和 NTIRE 挑战 2024 中的低光增强。所有这些结果说明了我们基于统计的熵损失的有效性。</p><p><strong>Key Takeaways</strong></p><ul><li>图像修复面临病态问题，有多个解。</li><li>传统方法使用 l1 损失，导致过度平滑的预测。</li><li>本文提出了一种基于统计的空间熵损失。</li><li>空间熵损失使用核密度估计 (KDE) 来近似概率分布。</li><li>本文使用扩散模型装备了空间熵损失。</li><li>实验表明，所提方法在低光增强方面优于基于 l1 的损失。</li><li>代码可在 <a target="_blank" rel="noopener" href="https://github.com/shermanlian/spatial-entropy-loss">https://github.com/shermanlian/spatial-entropy-loss</a> 获得。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：可微空间熵扩散模型在低光图像增强中的应用</li><li>作者：文义连、文静连、紫薇罗</li><li>隶属：乌普萨拉大学</li><li>关键词：图像复原、低光增强、扩散模型、空间熵</li><li>链接：https://arxiv.org/abs/2404.09735 Github：https://github.com/shermanlian/spatial-entropy-loss</li><li>摘要： （1）研究背景：图像复原是一项具有挑战性的任务，传统的基于像素级的损失函数（如 L1 和 L2）在提高图像保真度方面表现良好，但往往会导致过度平滑的输出，无法捕捉图像的感知质量。 （2）过去方法及其问题：为了解决过度平滑的问题，一些研究利用对抗生成网络（GAN）和感知度量（如 VGG 损失和 LPIPS 损失）来提高图像的视觉质量。然而，这些方法仍然依赖于 L1 损失来维持复原精度，并且需要额外的（预训练的）网络，增加了模型的复杂性和不可预测性。 （3）本文提出的方法：本文提出了一种基于空间熵的损失函数，将重点从确定性的像素级比较转移到统计学角度，强调学习分布而不是单个像素值。具体来说，本文将空间熵引入损失函数中，以衡量预测值和目标值之间的分布差异。为了使空间熵可微，本文采用核密度估计（KDE）来近似每个像素特定强度值与其邻域的概率。 （4）方法在任务中的表现：本文将提出的方法应用于低光增强任务，并在两个数据集和 NTIRE 挑战 2024 上进行了评估。实验结果表明，基于统计的空间熵损失函数在准确性和感知质量方面均优于基于 L1 的噪声匹配损失函数。</li></ol><p><strong>Methods：</strong></p><p>(1) <strong>基于空间熵的损失函数：</strong> - 引入空间熵衡量预测值和目标值之间的分布差异。 - 采用核密度估计（KDE）近似每个像素的概率分布。</p><p>(2) <strong>低光增强中的应用：</strong> - 将提出的方法应用于低光图像增强任务。 - 在两个数据集和 NTIRE 挑战 2024 上进行评估。</p><p>(3) <strong>实验结果：</strong> - 基于统计的空间熵损失函数在准确性和感知质量方面优于基于 L1 的噪声匹配损失函数。</p><ol><li>结论： （1）：本文提出了一种基于统计匹配的空间熵损失函数，用于图像复原。具体来说，我们引入了核密度估计（KDE）来使空间熵可微分。然后，空间熵可以用于图像重建的不同基于学习的框架。通过将其装备到扩散模型中（以替代ℓ1或ℓ2），我们获得了用于真实图像复原的新颖统计噪声匹配损失。然后，我们将此模型应用于低光增强任务以说明其有效性。我们的模型在 NTIRE 低光增强挑战中取得了最佳 LPIPS 性能。所有这些结果表明，空间熵损失对于高感知扩散学习过程是有效的。 （2）：创新点：提出了一种基于空间熵的统计匹配损失函数，用于图像复原。 性能：在低光增强任务上，基于统计的空间熵损失函数在准确性和感知质量方面优于基于 L1 的噪声匹配损失函数。 工作量：该方法需要额外的计算开销来近似空间熵，这可能会增加训练时间和内存消耗。</li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/a3c133d398fb53ead47a00645c5327d0241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/cf4bf5cc17f4334e352f588ed2ccefcb241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/b7a0ec6067c60992e1a2e395bdf11c91241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/1f0299e2b835d711d284c66cc863a956241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/1ffcc1ff6c9b31250304fc557e4bfd89241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/26cfb6c25aaf0c1236c9980dae831e68241286257.jpg" align="middle"></details></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io/2024/04/17/Paper/2024-04-17/Diffusion%20Models/">https://kedreamix.github.io/2024/04/17/Paper/2024-04-17/Diffusion Models/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Diffusion-Models/">Diffusion Models</a></div><div class="post_share"><div class="social-share" data-image="https://picx.zhimg.com/80/v2-bba220bfbb93f64f729fd79248d1ba37.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/04/17/Paper/2024-04-17/Talking%20Head%20Generation/" title="Talking Head Generation"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-57afb9746460c539242f5be2406abcd8.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Talking Head Generation</div></div></a></div><div class="next-post pull-right"><a href="/2024/04/14/Paper/2024-04-14/NeRF/" title="NeRF"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-066b26c50380cb863d74934c40a0317f.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">NeRF</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/03/03/Paperscape/EMO/" title="EMO Emote Portrait Alive - 阿里HumanAIGC"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-6492e24fb03ffa98135dc584535ab7d9.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-03</div><div class="title">EMO Emote Portrait Alive - 阿里HumanAIGC</div></div></a></div><div><a href="/2024/01/30/Paper/2024-01-30/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-e55358c77a9d65f15701e8f33262e2a4.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-30</div><div class="title">Diffusion Models</div></div></a></div><div><a href="/2024/02/02/Paper/2024-02-02/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-5920453c69c00995f18077b22d4a790e.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-02</div><div class="title">Diffusion Models</div></div></a></div><div><a href="/2024/02/13/Paper/2024-02-13/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-3709a9941aada6c4d3ed35934e311765.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-13</div><div class="title">Diffusion Models</div></div></a></div><div><a href="/2024/02/23/Paper/2024-02-23/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-ff425802a32a4519e30b9044a3eed1e8.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-23</div><div class="title">Diffusion Models</div></div></a></div><div><a href="/2024/02/09/Paper/2024-02-09/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-32488f736ee10537497afccc3a1a1d76.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-09</div><div class="title">Diffusion Models</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-04-17-%E6%9B%B4%E6%96%B0"><span class="toc-text">2024-04-17 更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#RefFusion-Reference-Adapted-Diffusion-Models-for-3D-Scene-Inpainting"><span class="toc-text">RefFusion: Reference Adapted Diffusion Models for 3D Scene Inpainting</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Taming-Latent-Diffusion-Model-for-Neural-Radiance-Field-Inpainting"><span class="toc-text">Taming Latent Diffusion Model for Neural Radiance Field Inpainting</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#A-Diffusion-based-Data-Generator-for-Training-Object-Recognition-Models-in-Ultra-Range-Distance"><span class="toc-text">A Diffusion-based Data Generator for Training Object Recognition Models in Ultra-Range Distance</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Digging-into-contrastive-learning-for-robust-depth-estimation-with-diffusion-models"><span class="toc-text">Digging into contrastive learning for robust depth estimation with diffusion models</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Equipping-Diffusion-Models-with-Differentiable-Spatial-Entropy-for-Low-Light-Image-Enhancement"><span class="toc-text">Equipping Diffusion Models with Differentiable Spatial Entropy for Low-Light Image Enhancement</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image:url('https://picx.zhimg.com/80/v2-bba220bfbb93f64f729fd79248d1ba37.png')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js").then((()=>{pangu.autoSpacingPage()}))}function panguInit(){panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.1},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script></div><script id="canvas_nest" defer color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><script data-pjax>function butterfly_swiper_injector_config(){var a=document.getElementById("recent-posts");console.log("已挂载butterfly_swiper"),a.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/25/Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="2024/01/25/Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="2024/01/25/Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/17/Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023/12/17/Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="2023/12/17/Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script></body></html>