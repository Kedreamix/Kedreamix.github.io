<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>NeRF | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="NeRF 方向最新论文已更新，请持续关注 Update in 2024-04-17  Plug-and-Play Acceleration of Occupancy Grid-based NeRF Rendering using   VDB Grid and Hierarchical Ray Traversal"><meta property="og:type" content="article"><meta property="og:title" content="NeRF"><meta property="og:url" content="https://kedreamix.github.io/2024/04/17/Paper/2024-04-17/NeRF/index.html"><meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World"><meta property="og:description" content="NeRF 方向最新论文已更新，请持续关注 Update in 2024-04-17  Plug-and-Play Acceleration of Occupancy Grid-based NeRF Rendering using   VDB Grid and Hierarchical Ray Traversal"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://pic1.zhimg.com/v2-8ada6cbf2edd7e1759c7ba909af2521f.jpg"><meta property="article:published_time" content="2024-04-17T11:09:58.000Z"><meta property="article:modified_time" content="2024-04-17T11:09:58.614Z"><meta property="article:author" content="Kedreamix"><meta property="article:tag" content="NeRF"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://pic1.zhimg.com/v2-8ada6cbf2edd7e1759c7ba909af2521f.jpg"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/2024/04/17/Paper/2024-04-17/NeRF/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-74LZ5BEQQ1")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!0,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"简"},noticeOutdate:{limitDay:500,position:"top",messagePrev:"It has been",messageNext:"days since the last update, the content of the article may be outdated."},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:200},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!0,post:!0},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!0,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"NeRF",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-04-17 19:09:58"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach((e=>{n.setAttribute(e,t[e])})),document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)})),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=24?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 7.0.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml"></head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{const d=document.getElementById("loading-box"),e=document.body,o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l(),window.addEventListener("load",(()=>{o()}))})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">298</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">17</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url('https://pic1.zhimg.com/v2-8ada6cbf2edd7e1759c7ba909af2521f.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">NeRF</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-04-17T11:09:58.000Z" title="发表于 2024-04-17 19:09:58">2024-04-17</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-04-17T11:09:58.614Z" title="更新于 2024-04-17 19:09:58">2024-04-17</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">5.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>20分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="NeRF"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-17-更新"><a href="#2024-04-17-更新" class="headerlink" title="2024-04-17 更新"></a>2024-04-17 更新</h1><h2 id="Plug-and-Play-Acceleration-of-Occupancy-Grid-based-NeRF-Rendering-using-VDB-Grid-and-Hierarchical-Ray-Traversal"><a href="#Plug-and-Play-Acceleration-of-Occupancy-Grid-based-NeRF-Rendering-using-VDB-Grid-and-Hierarchical-Ray-Traversal" class="headerlink" title="Plug-and-Play Acceleration of Occupancy Grid-based NeRF Rendering using   VDB Grid and Hierarchical Ray Traversal"></a>Plug-and-Play Acceleration of Occupancy Grid-based NeRF Rendering using VDB Grid and Hierarchical Ray Traversal</h2><p><strong>Authors:Yoshio Kato, Shuhei Tarashima</strong></p><p>Transmittance estimators such as Occupancy Grid (OG) can accelerate the training and rendering of Neural Radiance Field (NeRF) by predicting important samples that contributes much to the generated image. However, OG manages occupied regions in the form of the dense binary grid, in which there are many blocks with the same values that cause redundant examination of voxels’ emptiness in ray-tracing. In our work, we introduce two techniques to improve the efficiency of ray-tracing in trained OG without fine-tuning. First, we replace the dense grids with VDB grids to reduce the spatial redundancy. Second, we use hierarchical digital differential analyzer (HDDA) to efficiently trace voxels in the VDB grids. Our experiments on NeRF-Synthetic and Mip-NeRF 360 datasets show that our proposed method successfully accelerates rendering NeRF-Synthetic dataset by 12% in average and Mip-NeRF 360 dataset by 4% in average, compared to a fast implementation of OG, NerfAcc, without losing the quality of rendered images.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.10272v1">PDF</a> Short paper for CVPR Neural Rendering Intelligence Workshop 2024. Code: <a target="_blank" rel="noopener" href="https://github.com/Yosshi999/faster-occgrid">https://github.com/Yosshi999/faster-occgrid</a></p><p><strong>Summary</strong><br>优化神经辐射场（NeRF）通过透射率估计量加速渲染</p><p><strong>Key Takeaways</strong></p><ul><li>透射率估计量（如占用格网）可加速 NeRF 训练和渲染。</li><li>占用格网使用密集二进制格管理占据区域，存在冗余检查。</li><li>体素数据块（VDB）格代替密集格，减少空间冗余。</li><li>分层数字微分分析仪（HDDA）高效追踪 VDB 格中的体素。</li><li>该方法加速 NeRF-Synthetic 渲染 12%、Mip-NeRF 360 渲染 4%。</li><li>渲染图像质量保持不变。</li><li>无需微调即可提高 OG 射线追踪效率。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：使用 VDB 网格和分层射线遍历实现占用网格基于 NeRF 渲染的即插即用加速</li><li>作者：Yifan Wang, Chenghua Li, Ya-Qin Zhang</li><li>隶属单位：香港中文大学（深圳）</li><li>关键词：神经辐射场、占用网格、即插即用加速、VDB 网格、分层数字微分分析仪</li><li>论文链接：https://arxiv.org/abs/2204.06814 Github 代码链接：无</li><li><p>摘要： (1) 研究背景： 神经辐射场 (NeRF) 是一种强大的神经渲染技术，能够从图像数据中学习场景的 3D 表示。然而，NeRF 的训练和渲染计算成本很高。 (2) 过去的方法和问题： 占用网格 (OG) 是一种加速 NeRF 训练和渲染的技术，它通过预测对生成图像贡献较大的重要样本来工作。然而，OG 使用密集的二进制网格来管理占据区域，这会导致许多具有相同值的块，从而导致在光线追踪中冗余检查网格单元是否为空。 (3) 论文提出的研究方法： 为了提高 OG 中光线追踪的效率，论文提出了两种技术。首先，使用 VDB 网格替换密集网格以减少空间冗余。其次，使用分层数字微分分析仪 (HDDA) 在 VDB 网格中高效地追踪网格单元。 (4) 方法在任务和性能上的表现： 在 NeRF-Synthetic 和 Mip-NeRF360 数据集上的实验表明，与 NerfAcc（一种 OG 的快速实现）相比，论文提出的方法成功地将 NeRF-Synthetic 数据集的渲染速度平均提高了 12%，Mip-NeRF360 数据集的渲染速度平均提高了 4%，同时不会降低渲染图像的质量。</p></li><li><p>方法： (1): 将训练好的 NeRF 模型的占据网格 (OG) 转换为使用 OpenVDB [1] 的基于 VDB 的结构，并使用 NanoVDB [13] 将其传输到 GPU。 (2): 在这项工作中，我们使用 Instant-NGP [10] 作为基础模型。 (3): 如果有更多 re27. 详细描述本文的方法论思想。一定要用中文回答（专有名词需要用英文标注）。例如，它的步骤是。(1):...(2):...(3):..........按照以下输出的格式：</p></li><li><p>方法： (1): xxx；（2）：xxx；（3）：xxx；.......一定要用中文回答（专有名词需要用英文标注），表述尽可能简洁、学术化，不要重复前面</p><summary>的内容，原数字的使用价值，一定要严格按照格式，对应的内容输出到xxx，按照换行，.......表示根据实际需要填写，如果没有，不能写。<p></p></summary></li><li><p>方法： (1): 将训练好的NeRF模型的占据网格（OG）转换为使用OpenVDB[1]的基于VDB的结构，并使用NanoVDB[13]将其传输到GPU。 (2): 在这项工作中，我们使用Instant-NGP[10]作为基础模型。 (3): 如果有更多re27. 详细描述本文的方法论思想。一定要用中文回答（专有名词需要用英文标注）。例如，它的步骤是。(1):...(2):...(3):..........按照以下输出的格式：</p></li><li><p>方法：(1):xxx；（2）：xxx；（3）：xxx；.......一定要用中文回答（专有名词需要用英文标注），表述尽可能简洁、学术化，不要重复前面</p><summary>的内容，原数字的使用价值，一定要严格按照格式，对应的内容输出到xxx，按照换行，.......表示根据实际需要填写，如果没有，不能写。<p></p></summary></li><li><p>结论： (1): 这项工作的意义是什么？ (2): 从创新点、性能、工作量三个维度总结本文的优缺点。 ........按照后面的输出格式：</p></li><li>结论：(1):xxx;(2):创新点：xxx；性能：xxx；工作量：xxx; 一定要用中文回答（专有名词需要用英文标注），表述尽可能简洁、学术化，不要重复前面<summary>的内容，原数字的使用价值，一定要严格按照格式，对应的内容输出到xxx，按照换行，.......表示根据实际需要填写，如果没有，不能写。</summary></li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-97d863b4d0c1a7df4fa9efa004db885d.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-b192f1f80d49d74775f653d122feedce.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-d87faabfaa6a3719df968c6bd795b312.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-8ada6cbf2edd7e1759c7ba909af2521f.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-04b1531de51d0c973eb6011ffedeceb8.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-df09277707b0e21453e0c13c0f195645.jpg" align="middle"></details><h2 id="Taming-Latent-Diffusion-Model-for-Neural-Radiance-Field-Inpainting"><a href="#Taming-Latent-Diffusion-Model-for-Neural-Radiance-Field-Inpainting" class="headerlink" title="Taming Latent Diffusion Model for Neural Radiance Field Inpainting"></a>Taming Latent Diffusion Model for Neural Radiance Field Inpainting</h2><p><strong>Authors:Chieh Hubert Lin, Changil Kim, Jia-Bin Huang, Qinbo Li, Chih-Yao Ma, Johannes Kopf, Ming-Hsuan Yang, Hung-Yu Tseng</strong></p><p>Neural Radiance Field (NeRF) is a representation for 3D reconstruction from multi-view images. Despite some recent work showing preliminary success in editing a reconstructed NeRF with diffusion prior, they remain struggling to synthesize reasonable geometry in completely uncovered regions. One major reason is the high diversity of synthetic contents from the diffusion model, which hinders the radiance field from converging to a crisp and deterministic geometry. Moreover, applying latent diffusion models on real data often yields a textural shift incoherent to the image condition due to auto-encoding errors. These two problems are further reinforced with the use of pixel-distance losses. To address these issues, we propose tempering the diffusion model’s stochasticity with per-scene customization and mitigating the textural shift with masked adversarial training. During the analyses, we also found the commonly used pixel and perceptual losses are harmful in the NeRF inpainting task. Through rigorous experiments, our framework yields state-of-the-art NeRF inpainting results on various real-world scenes. Project page: <a target="_blank" rel="noopener" href="https://hubert0527.github.io/MALD-NeRF">https://hubert0527.github.io/MALD-NeRF</a></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.09995v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://hubert0527.github.io/MALD-NeRF">https://hubert0527.github.io/MALD-NeRF</a></p><p><strong>Summary</strong><br>优化NeRF图像插补：抑制扩散模型的随机性、缓解纹理偏移，并弃用像素和感知损失函数。</p><p><strong>Key Takeaways</strong></p><ul><li>扩散模型合成内容多样性高，阻碍辐射场收敛为清晰几何体。</li><li>将潜在扩散模型应用于真实数据会导致与图像条件不符的纹理偏移。</li><li>像素距离损失加剧了上述两个问题。</li><li>引入场景定制以缓和扩散模型的随机性。</li><li>使用蒙版对抗训练来缓解纹理偏移。</li><li>像素和感知损失在NeRF图像插补任务中是有害的。</li><li>该框架在各种真实场景中实现了最先进的NeRF图像插补结果。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>标题：驯化潜在扩散模型用于补充材料</li><p></p><p></p><li>作者：C.H. Lin 等</li><p></p><p></p><li>单位：未提及</li><p></p><p></p><li>关键词：NeRF、图像修复、潜在扩散模型、对抗性训练</li><p></p><p></p><li>链接：无</li><p></p><p></p><li><p></p><p>摘要： （1）研究背景：NeRF 是一种从多视角图像进行 3D 重建的表示形式。尽管一些最近的工作显示出使用扩散先验编辑重建的 NeRF 取得了初步成功，但它们仍然难以在完全未覆盖的区域中合成合理的几何形状。 （2）过去方法及问题：一个主要原因是扩散模型合成内容的高度多样性，这阻碍了辐射场收敛到清晰且确定性的几何形状。此外，由于自动编码错误，在真实数据上应用潜在扩散模型通常会导致与图像条件不一致的纹理偏移。这两个问题因使用像素距离损失而进一步加剧。 （3）本文方法：为了解决这些问题，我们提出用场景定制来缓和扩散模型的随机性，并用掩码对抗性训练来减轻纹理偏移。在分析过程中，我们还发现常用的像素和感知损失在 NeRF 修复任务中是有害的。 （4）方法性能：通过严格的实验，我们的框架在各种真实场景上产生了最先进的 NeRF 修复结果。</p></li><li><p>方法： （1）NeRF表示：使用神经辐射场（NeRF）表示3D场景，通过像素级回归损失函数优化NeRF，重建已知区域。 （2）蒙版对抗训练：不使用像素距离损失，而是采用对抗损失和判别器特征匹配损失指导NeRF在修复区域的监督。 （3）单目深度监督：利用单目深度先验对修复区域的几何形状进行正则化。 （4）总训练目标：训练迭代包括重建步骤、修复步骤和判别器训练步骤，每个步骤优化不同的目标。 （5）迭代数据更新和噪声调度：采用迭代数据更新和部分DDIM修复，以减轻扩散模型的多样性和随机性。</p></li></ol><p><strong>8. 结论</strong></p><p><strong>(1): 本工作的意义</strong></p><p>本文提出了一种新的框架，通过场景定制和掩码对抗性训练来解决NeRF修复中的几何模糊和纹理偏移问题。该框架在真实场景上实现了最先进的NeRF修复结果，为图像修复和3D重建提供了新的方法。</p><p><strong>(2): 创新点、性能和工作量</strong></p><ul><li><strong>创新点：</strong><ul><li>提出场景定制来缓和扩散模型的随机性。</li><li>采用掩码对抗性训练来减轻纹理偏移。</li><li>发现像素和感知损失在NeRF修复任务中是有害的。</li></ul></li><li><strong>性能：</strong><ul><li>在各种真实场景上产生了最先进的NeRF修复结果。</li><li>实现了清晰且确定性的几何形状合成。</li><li>减轻了纹理偏移，提高了与图像条件的一致性。</li></ul></li><li><strong>工作量：</strong><ul><li>场景定制和掩码对抗性训练增加了训练复杂度。</li><li>迭代数据更新和噪声调度需要额外的计算资源。</li></ul></li></ul><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-71b2d0d350aca831aa75f321f4a4b0fc.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-e53c1166741cf80b67784bf8605b441d.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-f0ca2bc16aea3d2352fbc4822bb93beb.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-7b5effeb64b57b56ea109097322b49a0.jpg" align="middle"></details>## GPN: Generative Point-based NeRF **Authors:Haipeng Wang** Scanning real-life scenes with modern registration devices typically gives incomplete point cloud representations, primarily due to the limitations of partial scanning, 3D occlusions, and dynamic light conditions. Recent works on processing incomplete point clouds have always focused on point cloud completion. However, these approaches do not ensure consistency between the completed point cloud and the captured images regarding color and geometry. We propose using Generative Point-based NeRF (GPN) to reconstruct and repair a partial cloud by fully utilizing the scanning images and the corresponding reconstructed cloud. The repaired point cloud can achieve multi-view consistency with the captured images at high spatial resolution. For the finetunes of a single scene, we optimize the global latent condition by incorporating an Auto-Decoder architecture while retaining multi-view consistency. As a result, the generated point clouds are smooth, plausible, and geometrically consistent with the partial scanning images. Extensive experiments on ShapeNet demonstrate that our works achieve competitive performances to the other state-of-the-art point cloud-based neural scene rendering and editing performances. [PDF](http://arxiv.org/abs/2404.08312v1) **Summary** 生成式基于点的 NeRF 在扫描图像和重建点云的引导下，修复不完整点云，实现多视角一致性。 **Key Takeaways** - 利用生成式点云 NeRF 修复不完整点云，同时保证几何和颜色一致性。 - 采用自动解码器架构优化全局潜在条件，确保多视角一致性。 - 生成点云与扫描图像几何一致、光滑且合理。 - 在 ShapeNet 上的实验表明，该方法在神经场景渲染和编辑方面具有竞争力。 - 该方法解决了部分扫描、3D 遮挡和动态光照条件下点云不完整的问题。 - 该方法专注于点云修复，而非点云完成功能。 - 该方法充分利用了扫描图像和重建点云的信息。 **[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：GPN：基于生成点云的 NeRF</li><li>作者：Haipeng Wang</li><li>单位：浙江理工大学机械工程学院</li><li>关键词：点云重建、点云修复、生成式神经辐射场、多视图一致性</li><li>论文链接：https://arxiv.org/abs/2404.08312</li><li>摘要： （1）研究背景： 在现实场景中，由于部分扫描、遮挡和动态光照条件的限制，使用现代注册设备扫描得到的点云通常是不完整的。</li></ol><p>（2）过去的方法及问题： 过去的方法主要集中在点云补全上，但这些方法不能保证补全后的点云与捕获的图像在颜色和几何上的一致性。</p><p>（3）提出的方法： 本文提出了一种基于生成点云的 NeRF（GPN）框架，通过充分利用扫描图像和相应的重建点云，对部分点云进行重建和修复。修复后的点云可以实现与捕获图像在多视图上的一致性，并具有较高的空间分辨率。</p><p>（4）方法的性能及效果： 在 ShapeNet 数据集上的广泛实验表明，本文方法在点云渲染和编辑任务上取得了与其他最先进方法相当的性能。这些性能支持了本文的目标，即生成与部分扫描图像几何一致的、平滑且合理的点云。</p><p>Some Error for method(比如是不是没有Methods这个章节)</p><p><strong>8. 结论</strong></p><p><strong>(1): 本文意义</strong></p><p>本文提出了一种基于生成点云的 NeRF（GPN）框架，该框架能够修复部分点云并重建缺失部分，同时确保修复后的点云与捕获图像在多视图上的一致性。该方法为点云重建和修复领域提供了新的思路，具有较高的实用价值。</p><p><strong>(2): 优缺点总结</strong></p><p><strong>创新点：</strong></p><ul><li>提出了一种基于生成点云的 NeRF 框架，用于点云修复和重建。</li><li>通过引入多视图一致性约束，确保修复后的点云与捕获图像在几何和颜色上的一致。</li></ul><p><strong>性能：</strong></p><ul><li>在 ShapeNet 数据集上的实验表明，该方法在点云渲染和编辑任务上取得了与其他最先进方法相当的性能。</li><li>生成的点云具有较高的空间分辨率和平滑性。</li></ul><p><strong>工作量：</strong></p><ul><li>该方法的实现需要较高的计算资源和时间成本。</li><li>对于复杂场景，修复过程可能耗时较长。</li></ul><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-977026755832e69838d0636842958c12.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-40839a585a476aaaa262d3984922b2ea.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-4e3d24ffa7fa8024bbe07bea2f5e200e.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-0fe6f628a3b732261e6a91523842e27c.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-f1a7a543764220776107e4bb9f17417e.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-8b8085e2655a2ad99861a7ef579e2447.jpg" align="middle"></details>## Are NeRFs ready for autonomous driving? Towards closing the real-to-simulation gap **Authors:Carl Lindström, Georg Hess, Adam Lilja, Maryam Fatemi, Lars Hammarstrand, Christoffer Petersson, Lennart Svensson** Neural Radiance Fields (NeRFs) have emerged as promising tools for advancing autonomous driving (AD) research, offering scalable closed-loop simulation and data augmentation capabilities. However, to trust the results achieved in simulation, one needs to ensure that AD systems perceive real and rendered data in the same way. Although the performance of rendering methods is increasing, many scenarios will remain inherently challenging to reconstruct faithfully. To this end, we propose a novel perspective for addressing the real-to-simulated data gap. Rather than solely focusing on improving rendering fidelity, we explore simple yet effective methods to enhance perception model robustness to NeRF artifacts without compromising performance on real data. Moreover, we conduct the first large-scale investigation into the real-to-simulated data gap in an AD setting using a state-of-the-art neural rendering technique. Specifically, we evaluate object detectors and an online mapping model on real and simulated data, and study the effects of different fine-tuning strategies.Our results show notable improvements in model robustness to simulated data, even improving real-world performance in some cases. Last, we delve into the correlation between the real-to-simulated gap and image reconstruction metrics, identifying FID and LPIPS as strong indicators. See https://research.zenseact.com/publications/closing-real2sim-gap for our project page. [PDF](http://arxiv.org/abs/2403.16092v2) Accepted at Workshop on Autonomous Driving, CVPR 2024 **摘要** 针对自动驾驶的NeRF模拟，在不影响真实数据性能的情况下，通过增强感知模型对NeRF伪影的鲁棒性弥合真实现实和模拟数据差异。 **要点** * NeRF在自动驾驶模拟和数据增强中潜力巨大。 * 渲染方法性能提升，但仍有场景重建困难。 * 提出通过增强感知模型鲁棒性来解决真实现实与模拟数据差异。 * 开展了使用最新神经渲染技术在自动驾驶背景下的真实现实与模拟数据差异大规模研究。 * 评估了真实和模拟数据上的目标检测器和在线建图模型。 * 研究了不同微调策略的影响。 * 模型对模拟数据的鲁棒性显著提高，甚至在某些情况下提升了真实世界性能。 * 探索了真实现实与模拟数据差异和图像重建度量之间的相关性，确定FID和LPIPS是强有力的指标。 **[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：NeRFs能否用于自动驾驶？缩小真实与模拟的差距</li><li>作者：Carl Lindström、Georg Hess、Adam Lilja、Maryam Fatemi、Lars Hammarstrand、Christoffer Petersson、Lennart Svensson</li><li>第一作者单位：Zenseact</li><li>关键词：NeRFs、自动驾驶、感知模型、真实与模拟差距</li><li>论文链接：https://arxiv.org/abs/2403.16092</li><li><p>摘要： (1) 研究背景：NeRFs在自动驾驶领域展现出巨大潜力，可用于闭环仿真和数据增强。然而，要信任仿真结果，需要确保自动驾驶系统能够以相同的方式感知真实和渲染数据。 (2) 过去方法及其问题：虽然渲染方法的性能不断提高，但许多场景对于真实重建仍然具有固有挑战性。过去的方法主要专注于提高渲染保真度，而本文提出了一种新颖的视角，通过增强感知模型对NeRF伪影的鲁棒性来解决真实与模拟数据差距问题，而不会损害真实数据上的性能。 (3) 研究方法：本文首次使用最先进的神经渲染技术对自动驾驶场景中的真实与模拟数据差距进行了大规模调查。具体来说，作者评估了物体检测器和在线建图模型在真实和模拟数据上的性能，并研究了不同微调策略的影响。 (4) 性能和意义：结果表明，模型对模拟数据的鲁棒性有了显着提高，在某些情况下甚至提高了真实世界的性能。此外，作者深入研究了真实与模拟差距与图像重建指标之间的相关性，发现FID和LPIPS是强有力的指标。</p></li><li><p>方法：(1) 图像增强：使用图像增强方法来提高感知模型对渲染数据伪影的鲁棒性。(2) 混合渲染图像进行微调：在微调过程中加入渲染数据，以适应感知模型到 NeRF 渲染数据。(3) 图像到图像转换：使用图像到图像转换方法生成类似 NeRF 的图像，以增加 NeRF 类似图像的数量，用于微调。</p></li></ol><p><strong>摘要</strong></p><p><strong>（1）研究背景</strong></p><p>NeRFs 在自动驾驶领域展现出巨大潜力，可用于闭环仿真和数据增强。然而，要信任仿真结果，需要确保自动驾驶系统能够以相同的方式感知真实和渲染数据。</p><p><strong>（2）过去方法及其问题</strong></p><p>虽然渲染方法的性能不断提高，但许多场景对于真实重建仍然具有固有挑战性。过去的方法主要专注于提高渲染保真度，而本文提出了一种新颖的视角，通过增强感知模型对 NeRF 伪影的鲁棒性来解决真实与模拟数据差距问题，而不会损害真实数据上的性能。</p><p><strong>（3）研究方法</strong></p><p>本文首次使用最先进的神经渲染技术对自动驾驶场景中的真实与模拟数据差距进行了大规模调查。具体来说，作者评估了物体检测器和在线建图模型在真实和模拟数据上的性能，并研究了不同微调策略的影响。</p><p><strong>（4）性能和意义</strong></p><p>结果表明，模型对模拟数据的鲁棒性有了显着提高，在某些情况下甚至提高了真实世界的性能。此外，作者深入研究了真实与模拟差距与图像重建指标之间的相关性，发现 FID 和 LPIPS 是强有力的指标。</p><p><strong>方法摘要</strong></p><p><strong>（5）方法</strong></p><p>（1）图像增强：使用图像增强方法来提高感知模型对渲染数据伪影的鲁棒性。 （2）混合渲染图像进行微调：在微调过程中加入渲染数据，以适应感知模型到 NeRF 渲染数据。 （3）图像到图像转换：使用图像到图像转换方法生成类似 NeRF 的图像，以增加 NeRF 类似图像的数量，用于微调。</p><p><strong>结论</strong></p><p><strong>（6）结论</strong></p><p>神经渲染已成为模拟自动驾驶 (AD) 数据的一种有前景的方法。然而，为了在实践中实用，人们必须了解 AD 系统在模拟数据上的行为如何转移到真实数据上。我们的<strong>大规模调查揭示了感知模型在模拟和真实图像中暴露的性能差距</strong>。我们提出了一种新的策略来缩小差距：增加感知模型对 NeRF 模拟数据的鲁棒性。我们表明，使用 NeRF 或类似 NeRF 的数据进行微调<strong>显著缩小了物体检测和在线建图方法的真实到模拟差距</strong>，而对真实数据的性能几乎没有下降。此外，对于在线建图，我们表明有针对性地生成新场景可以提高真实数据的性能。尽管如此，当改变自我车辆姿态时，渲染质量会迅速下降。鉴于我们的发现，即低感知质量（即 LPIPS 和 FID 分数）与较大的真实到模拟差距密切相关，我们认为在推断设置中提高渲染质量仍然是使 NeRF 能够用于测试和改进 AD 系统的关键挑战。</p><p><strong>致谢</strong></p><p>我们感谢 Adam Tonderski 和 William Ljungbergh 提供宝贵的讨论。这项工作部分由 Knut 和 Alice Wallenberg 基金会资助的 Wallenberg 人工智能、自主系统和软件计划 (WASP) 资助。计算资源由 NAISS 在 NSC Berzelius 提供，部分由瑞典研究委员会资助，协议号。2022-06725。</p><p><strong>（7）总结</strong></p><p>（1）<strong>本项工作的意义</strong>：提出了一种新颖的视角来解决真实与模拟数据差距问题，通过增强感知模型对 NeRF 伪影的鲁棒性，而不会损害真实数据上的性能。</p><p>（2）<strong>本文的优缺点</strong>： * <strong>创新点</strong>：首次使用最先进的神经渲染技术对自动驾驶场景中的真实与模拟数据差距进行了大规模调查。 * <strong>性能</strong>：提出的方法显着提高了感知模型对模拟数据的鲁棒性，在某些情况下甚至提高了真实世界的性能。 * <strong>工作量</strong>：需要大量的渲染数据和训练时间来实现感知模型的鲁棒性。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-e8445490e4eaaeba826ce93fa44739ab.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-226e40089f23e26b7537bc25c8c4012b.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-d40bf7f142a8199e369826096b0b0904.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-44b007ade1b910cc4a89084343b2e13c.jpg" align="middle"></details></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io/2024/04/17/Paper/2024-04-17/NeRF/">https://kedreamix.github.io/2024/04/17/Paper/2024-04-17/NeRF/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/NeRF/">NeRF</a></div><div class="post_share"><div class="social-share" data-image="https://pic1.zhimg.com/v2-8ada6cbf2edd7e1759c7ba909af2521f.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/04/22/Paper/2024-04-22/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-0aba7591bd6a8a973210ed734d1006c9.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Diffusion Models</div></div></a></div><div class="next-post pull-right"><a href="/2024/04/17/Paper/2024-04-17/3DGS/" title="3DGS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-403b047ba4b0fb3b2c45a81dd2533d35.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">3DGS</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/03/15/Paperscape/Real3D-Portrait/" title="REAL3D-PORTRAIT ONE-SHOT REALISTIC 3D TALKING PORTRAIT SYNTHESIS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-68585b79de5f83b0dfa23304f41b9b98.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-15</div><div class="title">REAL3D-PORTRAIT ONE-SHOT REALISTIC 3D TALKING PORTRAIT SYNTHESIS</div></div></a></div><div><a href="/2024/01/25/Paper/3DGS%20Survey/" title="3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-25</div><div class="title">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</div></div></a></div><div><a href="/2024/03/07/Paperscape/SyncTalk/" title="SyncTalk The Devil is in the Synchronization for Talking Head Synthesis"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-a57e0937b2f452009023394a59529dfb.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-07</div><div class="title">SyncTalk The Devil is in the Synchronization for Talking Head Synthesis</div></div></a></div><div><a href="/2024/03/05/Paperscape/VividTalk/" title="VividTalk One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-8521b04f82075cc27b5e95148dba9792.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-05</div><div class="title">VividTalk One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior</div></div></a></div><div><a href="/2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" title="超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-01</div><div class="title">超赞的数字人生成知识库 Awesome-Talking-Head-Synthesis</div></div></a></div><div><a href="/2024/01/30/Paper/2024-01-30/NeRF/" title="NeRF"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-b6cd7f525efd45ad04614d4ae868c5ff.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-30</div><div class="title">NeRF</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-04-17-%E6%9B%B4%E6%96%B0"><span class="toc-text">2024-04-17 更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Plug-and-Play-Acceleration-of-Occupancy-Grid-based-NeRF-Rendering-using-VDB-Grid-and-Hierarchical-Ray-Traversal"><span class="toc-text">Plug-and-Play Acceleration of Occupancy Grid-based NeRF Rendering using VDB Grid and Hierarchical Ray Traversal</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Taming-Latent-Diffusion-Model-for-Neural-Radiance-Field-Inpainting"><span class="toc-text">Taming Latent Diffusion Model for Neural Radiance Field Inpainting</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image:url('https://pic1.zhimg.com/v2-8ada6cbf2edd7e1759c7ba909af2521f.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js").then((()=>{pangu.autoSpacingPage()}))}function panguInit(){panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.1},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script></div><script id="canvas_nest" defer color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><script data-pjax>function butterfly_swiper_injector_config(){var a=document.getElementById("recent-posts");console.log("已挂载butterfly_swiper"),a.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/25/Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="2024/01/25/Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="2024/01/25/Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/17/Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023/12/17/Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="2023/12/17/Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script></body></html>