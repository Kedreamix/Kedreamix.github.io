<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>NeRF | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="NeRF 方向最新论文已更新，请持续关注 Update in 2024-04-22  AG-NeRF Attention-guided Neural Radiance Fields for Multi-height   Large-scale Outdoor Scene Rendering"><meta property="og:type" content="article"><meta property="og:title" content="NeRF"><meta property="og:url" content="https://kedreamix.github.io/2024/04/22/Paper/2024-04-22/NeRF/index.html"><meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World"><meta property="og:description" content="NeRF 方向最新论文已更新，请持续关注 Update in 2024-04-22  AG-NeRF Attention-guided Neural Radiance Fields for Multi-height   Large-scale Outdoor Scene Rendering"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://picx.zhimg.com/v2-5f7fb8305c36c1fe2572adfd98b584f7.jpg"><meta property="article:published_time" content="2024-04-22T09:43:13.000Z"><meta property="article:modified_time" content="2024-04-22T09:43:13.959Z"><meta property="article:author" content="Kedreamix"><meta property="article:tag" content="NeRF"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://picx.zhimg.com/v2-5f7fb8305c36c1fe2572adfd98b584f7.jpg"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/2024/04/22/Paper/2024-04-22/NeRF/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-74LZ5BEQQ1")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!0,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"简"},noticeOutdate:{limitDay:500,position:"top",messagePrev:"It has been",messageNext:"days since the last update, the content of the article may be outdated."},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:200},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!0,post:!0},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!0,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"NeRF",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-04-22 17:43:13"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach((e=>{n.setAttribute(e,t[e])})),document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)})),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=24?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 7.0.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml"></head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{const d=document.getElementById("loading-box"),e=document.body,o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l(),window.addEventListener("load",(()=>{o()}))})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">221</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url('https://picx.zhimg.com/v2-5f7fb8305c36c1fe2572adfd98b584f7.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">NeRF</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-04-22T09:43:13.000Z" title="发表于 2024-04-22 17:43:13">2024-04-22</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-04-22T09:43:13.959Z" title="更新于 2024-04-22 17:43:13">2024-04-22</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">4.8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>17分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="NeRF"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-22-更新"><a href="#2024-04-22-更新" class="headerlink" title="2024-04-22 更新"></a>2024-04-22 更新</h1><h2 id="AG-NeRF-Attention-guided-Neural-Radiance-Fields-for-Multi-height-Large-scale-Outdoor-Scene-Rendering"><a href="#AG-NeRF-Attention-guided-Neural-Radiance-Fields-for-Multi-height-Large-scale-Outdoor-Scene-Rendering" class="headerlink" title="AG-NeRF: Attention-guided Neural Radiance Fields for Multi-height   Large-scale Outdoor Scene Rendering"></a>AG-NeRF: Attention-guided Neural Radiance Fields for Multi-height Large-scale Outdoor Scene Rendering</h2><p><strong>Authors:Jingfeng Guo, Xiaohan Zhang, Baozhu Zhao, Qi Liu</strong></p><p>Existing neural radiance fields (NeRF)-based novel view synthesis methods for large-scale outdoor scenes are mainly built on a single altitude. Moreover, they often require a priori camera shooting height and scene scope, leading to inefficient and impractical applications when camera altitude changes. In this work, we propose an end-to-end framework, termed AG-NeRF, and seek to reduce the training cost of building good reconstructions by synthesizing free-viewpoint images based on varying altitudes of scenes. Specifically, to tackle the detail variation problem from low altitude (drone-level) to high altitude (satellite-level), a source image selection method and an attention-based feature fusion approach are developed to extract and fuse the most relevant features of target view from multi-height images for high-fidelity rendering. Extensive experiments demonstrate that AG-NeRF achieves SOTA performance on 56 Leonard and Transamerica benchmarks and only requires a half hour of training time to reach the competitive PSNR as compared to the latest BungeeNeRF.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.11897v1">PDF</a></p><p><strong>Summary</strong><br>降低训练成本，实现多高度自由视角图像合成。</p><p><strong>Key Takeaways</strong></p><ul><li>多高度神经辐射场（NeRF）能够合成自由视角图像。</li><li>提出图像选择方法和注意力特征融合，解决不同高度细节差异问题。</li><li>AG-NeRF 在 56 Leonard 和 Transamerica 基准上达到最先进性能。</li><li>AG-NeRF 训练时间仅需半小时，即可达到 BungeeNeRF 的竞争水平。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: AG-NeRF: 多高度大尺度户外场景渲染的注意力引导神经辐射场</p></li><li><p>Authors: Jingfeng Guo, Xiaohan Zhang, Baozhu Zhao, Qi Liu</p></li><li><p>Affiliation: 华南理工大学</p></li><li><p>Keywords: Novel View Synthesis, NeRF, Large-scale Outdoor Scene Rendering</p></li><li><p>Urls: https://arxiv.org/abs/2404.11897v1 , Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 现有的基于神经辐射场 (NeRF) 的大规模户外场景新视角合成方法主要建立在单一高度上。此外，它们通常需要先验的相机拍摄高度和场景范围，当相机高度发生变化时，会导致低效且不实用的应用。</p><p>(2): 过去的方法： - 地理上将场景分解为几个单元格，并为每个单元格训练一个子 NeRF，然后将它们合并。 - 在位置编码中并行应用平面和网格特征以实现高效建模。 - 问题：它们在基础高度上重建大规模场景，当导航到更近的地方以检查大规模户外场景的微观细节时，表现出过度模糊的伪影和不完整的重建。</p><p>(3): 本文提出的研究方法： - 提出了一种端到端框架 AG-NeRF，通过合成基于场景不同高度的自由视角图像来降低构建良好重建的训练成本。 - 具体来说，为了解决从低高度（无人机级别）到高高度（卫星级别）的细节变化问题，开发了一种源图像选择方法和一种基于注意力的特征融合方法，从多高度图像中提取和融合目标视图最相关的特征，以实现高保真渲染。</p><p>(4): 本文方法在任务和性能上的表现： - 在 56 Leonard 和 Transamerica 基准测试中取得了 SOTA 性能。 - 只需要半小时的训练时间即可达到与最新 BungeeNeRF 相当的竞争性 PSNR。 - 性能支持了他们的目标：降低构建良好重建的训练成本。</p><ol><li>方法：</li></ol><p>（1）：提出了一种端到端框架 AG-NeRF，通过合成基于场景不同高度的自由视角图像来降低构建良好重建的训练成本。</p><p>（2）：开发了一种源图像选择方法和一种基于注意力的特征融合方法，从多高度图像中提取和融合目标视图最相关的特征，以实现高保真渲染。</p><p>（3）：利用可训练的 U-Net 网络从源图像中提取特征图，并使用 Transformer 对提取的特征向量进行融合，以最大化融合特征与目标像素之间的相关性。</p><p>（4）：采用分层采样方法，使用粗略网络和精细网络同时优化，并使用基于注意力的特征融合方法将多高度图像中的特征融合起来。</p><ol><li>结论：</li></ol><p>（1）：本文针对不同高度拍摄的大场景渲染提出了端到端的 AG-NeRF 框架，降低了构建良好重建模型的训练成本。</p><p>（2）：创新点：提出了一种源图像选择方法和基于注意力的特征融合方法，从多高度图像中提取和融合目标视图最相关的特征，以实现高保真渲染。 性能：在 56 Leonard 和 Transamerica 基准测试中取得了 SOTA 性能，只需要半小时的训练时间即可达到与最新 BungeeNeRF 相当的竞争性 PSNR。 工作量：采用分层采样方法，使用粗略网络和精细网络同时优化，并使用基于注意力的特征融合方法将多高度图像中的特征融合起来。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-82fe2876dffe132719e410910e28492d.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-fbedf0965ea4b6e30b80160a9ce71484.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-b5a30ff8e4f41c8671a8c9f7dbcb45d2.jpg" align="middle"></details><h2 id="SLAIM-Robust-Dense-Neural-SLAM-for-Online-Tracking-and-Mapping"><a href="#SLAIM-Robust-Dense-Neural-SLAM-for-Online-Tracking-and-Mapping" class="headerlink" title="SLAIM: Robust Dense Neural SLAM for Online Tracking and Mapping"></a>SLAIM: Robust Dense Neural SLAM for Online Tracking and Mapping</h2><p><strong>Authors:Vincent Cartillier, Grant Schindler, Irfan Essa</strong></p><p>We present SLAIM - Simultaneous Localization and Implicit Mapping. We propose a novel coarse-to-fine tracking model tailored for Neural Radiance Field SLAM (NeRF-SLAM) to achieve state-of-the-art tracking performance. Notably, existing NeRF-SLAM systems consistently exhibit inferior tracking performance compared to traditional SLAM algorithms. NeRF-SLAM methods solve camera tracking via image alignment and photometric bundle-adjustment. Such optimization processes are difficult to optimize due to the narrow basin of attraction of the optimization loss in image space (local minima) and the lack of initial correspondences. We mitigate these limitations by implementing a Gaussian pyramid filter on top of NeRF, facilitating a coarse-to-fine tracking optimization strategy. Furthermore, NeRF systems encounter challenges in converging to the right geometry with limited input views. While prior approaches use a Signed-Distance Function (SDF)-based NeRF and directly supervise SDF values by approximating ground truth SDF through depth measurements, this often results in suboptimal geometry. In contrast, our method employs a volume density representation and introduces a novel KL regularizer on the ray termination distribution, constraining scene geometry to consist of empty space and opaque surfaces. Our solution implements both local and global bundle-adjustment to produce a robust (coarse-to-fine) and accurate (KL regularizer) SLAM solution. We conduct experiments on multiple datasets (ScanNet, TUM, Replica) showing state-of-the-art results in tracking and in reconstruction accuracy.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.11419v1">PDF</a></p><p><strong>Summary</strong><br>Nerf-SLAM 通过采用从粗到细的跟踪模型和 KL 正则化器，在跟踪性能和重建精度上实现了最先进的成绩。</p><p><strong>Key Takeaways</strong></p><ul><li>SLAIM 提出了一种从粗到细的跟踪模型以提高 NeRF-SLAM 的跟踪性能。</li><li>SLAIM 通过高斯金字塔滤波器实现从粗到细的跟踪优化策略。</li><li>NeRF 系统难以使用有限的输入视图收敛到正确的几何形状。</li><li>SLAIM 使用体积密度表示和一个新的 KL 正则化器来约束场景几何形状。</li><li>SLAIM 实现局部和全局捆绑调整以提高鲁棒性和准确性。</li><li>SLAIM 在多个数据集上进行了实验，在跟踪和重建精度上均显示出最先进的结果。</li><li>SLAIM 解决了 NeRF-SLAM 在传统 SLAM 算法下表现出较差的跟踪性能这一难题。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>标题：SLAIM：用于在线跟踪和建图的鲁棒稠密神经SLAM</p></li><li><p>作者：Vincent Cartillier、Grant Schindler、Irfan Essa</p></li><li><p>隶属关系：佐治亚理工学院</p></li><li><p>关键词：神经辐射场、SLAM、稠密建图、跟踪</p></li><li><p>论文链接：https://arxiv.org/abs/2404.11419，Github 代码链接：None</p></li><li><p>摘要：</p></li></ol><p>（1）研究背景：稠密视觉SLAM是3D计算机视觉中的一个长期问题，在自动驾驶、室内外机器人导航、虚拟现实和增强现实等领域有着广泛的应用。</p><p>（2）过去的方法及问题：传统的SLAM系统通过估计图像对应关系来开始，这些对应关系可能是稀疏的，例如匹配的特征点。神经辐射场SLAM（NeRF-SLAM）方法通过图像对齐和光度捆绑调整来解决相机跟踪问题。由于图像空间中优化损失的吸引域窄（局部极小值）以及缺乏初始对应关系，此类优化过程难以优化。</p><p>（3）提出的研究方法：本文提出了一种新的粗到细跟踪模型，专门针对NeRF-SLAM，以实现最先进的跟踪性能。此外，本文还引入了一种新的目标射线终止分布，并将其用于KL正则化器中，以约束场景几何由空空间和不透明表面组成。</p><p>（4）任务和性能：本文方法在ScanNet、TUM、Replica等多个数据集上进行了实验，在跟踪和重建精度方面均取得了最先进的成果。这些性能支持了本文的目标。</p><ol><li><p>方法：</p><pre><code>            (1):SLAIM 是一种用于稠密映射和跟踪的 RGB-D 输入流的 novel 方法；

            (2):SLAIM 采用了一种从粗到精的跟踪模型，以实现最先进的跟踪性能；

            (3):SLAIM 引入了一种新的目标射线终止分布，并将其用于 KL 正则化器中，以约束场景几何由空空间和不透明表面组成；

            (4):SLAIM 在 ScanNet、TUM、Replica 等多个数据集上进行了实验，在跟踪和重建精度方面均取得了最先进的成果。
</code></pre></li><li><p>结论：</p></li></ol><p>（1）本文的工作意义：本文提出了一种最先进的稠密实时 RGB-D NeRF-SLAM 系统 SLAIM，该系统具有最先进的相机跟踪和建图能力。</p><p>（2）本文的优缺点总结： - 创新点： - 采用从粗到精的跟踪模型，实现最先进的跟踪性能。 - 引入新的目标射线终止分布，并将其用于 KL 正则化器中，以约束场景几何由空空间和不透明表面组成。 - 性能： - 在 ScanNet、TUM、Replica 等多个数据集上取得了最先进的跟踪和重建精度。 - 工作量： - 内存效率高，在 Replica 和 ScanNet 数据集上与基准相比，跟踪和建图时间均有明显降低。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-486ca0b76c4db89899a0670269d00796.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-f729a5308a9aa1435c3a0e2db312184f.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-ddcd1f27f832c7cfc1c274567204de22.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-c7d35d3daa3f9540491cf1d974f07bc9.jpg" align="middle"></details><h2 id="RainyScape-Unsupervised-Rainy-Scene-Reconstruction-using-Decoupled-Neural-Rendering"><a href="#RainyScape-Unsupervised-Rainy-Scene-Reconstruction-using-Decoupled-Neural-Rendering" class="headerlink" title="RainyScape: Unsupervised Rainy Scene Reconstruction using Decoupled   Neural Rendering"></a>RainyScape: Unsupervised Rainy Scene Reconstruction using Decoupled Neural Rendering</h2><p><strong>Authors:Xianqiang Lyu, Hui Liu, Junhui Hou</strong></p><p>We propose RainyScape, an unsupervised framework for reconstructing clean scenes from a collection of multi-view rainy images. RainyScape consists of two main modules: a neural rendering module and a rain-prediction module that incorporates a predictor network and a learnable latent embedding that captures the rain characteristics of the scene. Specifically, based on the spectral bias property of neural networks, we first optimize the neural rendering pipeline to obtain a low-frequency scene representation. Subsequently, we jointly optimize the two modules, driven by the proposed adaptive direction-sensitive gradient-based reconstruction loss, which encourages the network to distinguish between scene details and rain streaks, facilitating the propagation of gradients to the relevant components. Extensive experiments on both the classic neural radiance field and the recently proposed 3D Gaussian splatting demonstrate the superiority of our method in effectively eliminating rain streaks and rendering clean images, achieving state-of-the-art performance. The constructed high-quality dataset and source code will be publicly available.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.11401v1">PDF</a></p><p><strong>Summary</strong><br>基于神经网络的光谱偏差特性，RainyScape利用无监督框架重建干净场景，包含神经渲染模块和雨滴预测模块。</p><p><strong>Key Takeaways</strong></p><ul><li>利用神经网络的光谱偏差特性获得低频场景表示。</li><li>联合优化神经渲染模块和雨滴预测模块，以区分场景细节和雨滴条纹。</li><li>提出自适应方向敏感梯度重建损失，引导网络区分场景细节和雨滴条纹。</li><li>在经典神经辐射场和 3D 高斯斑点 splatting 数据集上均达到最先进的去雨性能。</li><li>提供高质量数据集和源代码，促进研究工作。</li><li>引入可学习潜在嵌入，捕捉场景的雨滴特征。</li><li>通过雨滴预测网络有效消除雨滴条纹，渲染干净图像。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: RainyScape: 无监督雨景重建使用解耦神经渲染</p></li><li><p>Authors: Xianqiang Lyu, Hui Liu, Junhui Hou</p></li><li><p>Affiliation: 香港城市大学计算机科学系</p></li><li><p>Keywords: Rainy scene reconstruction, Neural rendering, Unsupervised loss</p></li><li><p>Urls: https://arxiv.org/abs/2404.11401 , Github:None</p></li><li><p>Summary:</p></li></ol><p>(1):随着神经辐射场（NeRF）在图像合成中的广泛应用，当输入图像受到模糊、噪声或雨水等因素影响时，渲染结果不可避免地会产生明显的伪影。</p><p>(2):现有的方法针对特定任务提出了各种解决方案，但对于雨景重建任务，它们无法有效表示三维空间中稀疏且间歇性的降雨。</p><p>(3):本文提出RainyScape，一个解耦的神经渲染框架，它能够以无监督的方式从雨景图像中重建无雨场景。该框架通过神经渲染管道获得场景的低频表示，并使用可学习的雨水嵌入和预测器来表征雨水。此外，本文还提出了一个自适应角度估计策略和梯度旋转损失，以解耦场景高频细节和雨水条纹。</p><p>(4):在神经辐射场和三维高斯散射两种渲染技术上的广泛实验表明，该方法在有效消除雨水条纹和渲染清晰图像方面优于现有方法，达到了最先进的性能。</p><ol><li><p>方法：</p><p>（1）：提出RainyScape，一个解耦的神经渲染框架，可以无监督地从雨景图像中重建无雨场景；</p><p>（2）：通过神经渲染管道获得场景的低频表示，并使用可学习的雨水嵌入和预测器来表征雨水；</p><p>（3）：提出一个自适应角度估计策略和梯度旋转损失，以解耦场景高频细节和雨水条纹；</p><p>（4）：在神经辐射场和三维高斯散射两种渲染技术上的广泛实验表明，该方法在有效消除雨水条纹和渲染清晰图像方面优于现有方法，达到了最先进的性能。</p></li><li><p>结论：</p></li></ol><p>（1）：RainyScape的意义在于，它提出了一种无监督的解耦神经渲染框架，可以从雨景图像中重建无雨场景，有效解决了雨景重建中的雨水条纹去除问题，为雨景图像处理提供了新的思路和方法。</p><p>（2）：创新点：</p><ul><li><p>提出了一种解耦的神经渲染框架，通过低频场景表示、可学习的雨水嵌入和预测器以及自适应角度估计策略和梯度旋转损失，有效解耦了场景高频细节和雨水条纹。</p></li><li><p>性能：</p></li><li><p>在神经辐射场和三维高斯散射两种渲染技术上的广泛实验表明，该方法在有效消除雨水条纹和渲染清晰图像方面优于现有方法，达到了最先进的性能。</p></li><li><p>工作量：</p></li><li><p>该方法需要对雨景图像进行预处理，包括图像分割、雨水条纹检测和雨水嵌入提取等步骤，增加了计算量和时间开销。</p></li></ul><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-789763f7ebb6ec7a923539611ab1fe24.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-89f176b1378008d1c0b63c9241adfdb2.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-5f7fb8305c36c1fe2572adfd98b584f7.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-76be36036e15658d754b57c4864b0abf.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-3765b699865b1d89cc9f5f13f9843a0e.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-34d10a80ece07ba92081dfc066d00427.jpg" align="middle"></details>## REACTO: Reconstructing Articulated Objects from a Single Video **Authors:Chaoyue Song, Jiacheng Wei, Chuan-Sheng Foo, Guosheng Lin, Fayao Liu** In this paper, we address the challenge of reconstructing general articulated 3D objects from a single video. Existing works employing dynamic neural radiance fields have advanced the modeling of articulated objects like humans and animals from videos, but face challenges with piece-wise rigid general articulated objects due to limitations in their deformation models. To tackle this, we propose Quasi-Rigid Blend Skinning, a novel deformation model that enhances the rigidity of each part while maintaining flexible deformation of the joints. Our primary insight combines three distinct approaches: 1) an enhanced bone rigging system for improved component modeling, 2) the use of quasi-sparse skinning weights to boost part rigidity and reconstruction fidelity, and 3) the application of geodesic point assignment for precise motion and seamless deformation. Our method outperforms previous works in producing higher-fidelity 3D reconstructions of general articulated objects, as demonstrated on both real and synthetic datasets. Project page: https://chaoyuesong.github.io/REACTO. [PDF](http://arxiv.org/abs/2404.11151v1) **Summary** 对于一般性关节动作的3D物体，本文提出了一种新的变形模型，即准刚性混合蒙皮，以便从单个视频中进行全面重建。 **Key Takeaways** - 提出一种新的变形模型，准刚性混合蒙皮，增强了零件刚性，同时保持关节柔性变形。 - 采用增强骨骼绑定系统改善组件建模。 - 使用准稀疏蒙皮权重提高零件刚性和重建保真度。 - 应用测地线点赋值实现精确运动和无缝变形。 - 在真实和合成数据集上，该方法在生成高保真一般性关节动作的3D重建方面优于先前的工作。 - 该研究为一般性关节动作的3D物体重建提供了新的方法。 - 该研究在计算机视觉和图形学领域具有潜在应用价值。 - 该研究有助于推动相关领域的发展。 **[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>标题：REACTO：从单一视频中重建铰接物体</p></li><li><p>作者：Chaoyue Song、Jiacheng Wei、Chuan Sheng Foo、Guosheng Lin、Fayao Liu</p></li><li><p>隶属：南洋理工大学</p></li><li><p>关键词：铰接物体重建、动态神经辐射场、准刚性混合蒙皮</p></li><li><p>论文链接：https://arxiv.org/abs/2404.11151, Github：无</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：重建铰接物体是计算机视觉中的一项重要任务，但现有方法在处理具有分段刚性的通用铰接物体时面临挑战。</p><p>（2）：过去方法：NASAM和PARIS等方法需要多视角图像或多视图图像，在实际应用中受限。</p><p>（3）：研究方法：本文提出了一种准刚性混合蒙皮变形模型，该模型通过增强骨骼装配系统、使用准稀疏蒙皮权重和应用测地线点分配来提高刚性并保持关节的灵活变形。</p><p>（4）：任务与性能：REACTO在真实和合成数据集上对通用铰接物体的3D重建任务中取得了较高的保真度，证明了其性能可以支持其目标。</p><p><strong>7. Methods：</strong></p><p>(1)：提出准刚性混合蒙皮变形模型，增强骨骼装配系统，使用准稀疏蒙皮权重，并应用测地线点分配；</p><p>(2)：构建REACTO框架，包括骨骼装配、蒙皮变形、体绘制和渲染模块；</p><p>(3)：使用基于神经辐射场的渲染器，从单一视频中重建铰接物体；</p><p>(4)：通过优化骨骼参数、蒙皮权重和神经辐射场参数，实现铰接物体的高保真重建；</p><p>(5)：在真实和合成数据集上进行实验，验证REACTO的有效性。</p><ol><li>结论：</li></ol><p>（1）：本工作提出REACTO，一种从单一视频中重建通用铰接3D物体的开创性方法，通过重新定义装配结构并采用准刚性混合蒙皮，实现了建模和精度的提升。准刚性混合蒙皮通过利用准稀疏蒙皮权重和测地线点分配，确保了每个部件的刚性，同时在关节处保持平滑变形。广泛的实验表明，REACTO在真实和合成数据集上都优于现有方法，保真度和细节方面都有所提升。</p><p>（2）：创新点：提出准刚性混合蒙皮变形模型，增强骨骼装配系统，使用准稀疏蒙皮权重，并应用测地线点分配；</p><p>性能：在真实和合成数据集上，REACTO在保真度和细节方面都优于现有方法；</p><p>工作量：与需要多视角或多视图图像的现有方法相比，REACTO只需单一视频即可重建铰接物体，工作量更小。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-b24d1992bf52c35d5d68092f3855e178.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-cc1782e8c3f880dfa4512201f4175379.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-46959553add30d1e8d2dff8cb9e56563.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-f4000a7f506812312f58f8dd21486b3b.jpg" align="middle"></details></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io/2024/04/22/Paper/2024-04-22/NeRF/">https://kedreamix.github.io/2024/04/22/Paper/2024-04-22/NeRF/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/NeRF/">NeRF</a></div><div class="post_share"><div class="social-share" data-image="https://picx.zhimg.com/v2-5f7fb8305c36c1fe2572adfd98b584f7.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/04/25/Paper/2024-04-25/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-b249a085ea084ca24b82dc1fcadcc875.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Diffusion Models</div></div></a></div><div class="next-post pull-right"><a href="/2024/04/22/Paper/2024-04-22/3DGS/" title="3DGS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-c36e0e4bd338738e2a5420e68cd6ab5d.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">3DGS</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/03/15/Paperscape/Real3D-Portrait/" title="REAL3D-PORTRAIT ONE-SHOT REALISTIC 3D TALKING PORTRAIT SYNTHESIS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-68585b79de5f83b0dfa23304f41b9b98.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-15</div><div class="title">REAL3D-PORTRAIT ONE-SHOT REALISTIC 3D TALKING PORTRAIT SYNTHESIS</div></div></a></div><div><a href="/2024/03/07/Paperscape/SyncTalk/" title="SyncTalk The Devil is in the Synchronization for Talking Head Synthesis"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-a57e0937b2f452009023394a59529dfb.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-07</div><div class="title">SyncTalk The Devil is in the Synchronization for Talking Head Synthesis</div></div></a></div><div><a href="/2024/03/05/Paperscape/VividTalk/" title="VividTalk One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-8521b04f82075cc27b5e95148dba9792.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-05</div><div class="title">VividTalk One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior</div></div></a></div><div><a href="/2024/01/25/Paper/3DGS%20Survey/" title="3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-25</div><div class="title">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</div></div></a></div><div><a href="/2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" title="超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-01</div><div class="title">超赞的数字人生成知识库 Awesome-Talking-Head-Synthesis</div></div></a></div><div><a href="/2024/01/24/Paper/2024-01-24/NeRF/" title="NeRF"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-eaae707aaf894e22e54246edd91e6dce.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-24</div><div class="title">NeRF</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-04-22-%E6%9B%B4%E6%96%B0"><span class="toc-text">2024-04-22 更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#AG-NeRF-Attention-guided-Neural-Radiance-Fields-for-Multi-height-Large-scale-Outdoor-Scene-Rendering"><span class="toc-text">AG-NeRF: Attention-guided Neural Radiance Fields for Multi-height Large-scale Outdoor Scene Rendering</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SLAIM-Robust-Dense-Neural-SLAM-for-Online-Tracking-and-Mapping"><span class="toc-text">SLAIM: Robust Dense Neural SLAM for Online Tracking and Mapping</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RainyScape-Unsupervised-Rainy-Scene-Reconstruction-using-Decoupled-Neural-Rendering"><span class="toc-text">RainyScape: Unsupervised Rainy Scene Reconstruction using Decoupled Neural Rendering</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image:url('https://picx.zhimg.com/v2-5f7fb8305c36c1fe2572adfd98b584f7.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js").then((()=>{pangu.autoSpacingPage()}))}function panguInit(){panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.1},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script></div><script id="canvas_nest" defer color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><script data-pjax>function butterfly_swiper_injector_config(){var a=document.getElementById("recent-posts");console.log("已挂载butterfly_swiper"),a.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/25/Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="2024/01/25/Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="2024/01/25/Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/17/Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023/12/17/Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="2023/12/17/Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script></body></html>