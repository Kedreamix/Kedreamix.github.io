<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>3DGS | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="3DGS 方向最新论文已更新，请持续关注 Update in 2024-04-22  Learn2Talk 3D Talking Face Learns from 2D Talking Face"><meta property="og:type" content="article"><meta property="og:title" content="3DGS"><meta property="og:url" content="https://kedreamix.github.io/2024/04/22/Paper/2024-04-22/3DGS/index.html"><meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World"><meta property="og:description" content="3DGS 方向最新论文已更新，请持续关注 Update in 2024-04-22  Learn2Talk 3D Talking Face Learns from 2D Talking Face"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://pic1.zhimg.com/v2-c36e0e4bd338738e2a5420e68cd6ab5d.jpg"><meta property="article:published_time" content="2024-04-22T09:32:29.000Z"><meta property="article:modified_time" content="2024-04-22T09:32:29.438Z"><meta property="article:author" content="Kedreamix"><meta property="article:tag" content="3DGS"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://pic1.zhimg.com/v2-c36e0e4bd338738e2a5420e68cd6ab5d.jpg"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/2024/04/22/Paper/2024-04-22/3DGS/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-74LZ5BEQQ1")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!0,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"简"},noticeOutdate:{limitDay:500,position:"top",messagePrev:"It has been",messageNext:"days since the last update, the content of the article may be outdated."},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:200},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!0,post:!0},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!0,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"3DGS",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-04-22 17:32:29"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach((e=>{n.setAttribute(e,t[e])})),document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)})),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=24?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 7.0.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml"></head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{const d=document.getElementById("loading-box"),e=document.body,o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l(),window.addEventListener("load",(()=>{o()}))})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">205</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url('https://pic1.zhimg.com/v2-c36e0e4bd338738e2a5420e68cd6ab5d.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">3DGS</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-04-22T09:32:29.000Z" title="发表于 2024-04-22 17:32:29">2024-04-22</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-04-22T09:32:29.438Z" title="更新于 2024-04-22 17:32:29">2024-04-22</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">5k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>18分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="3DGS"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-22-更新"><a href="#2024-04-22-更新" class="headerlink" title="2024-04-22 更新"></a>2024-04-22 更新</h1><h2 id="Learn2Talk-3D-Talking-Face-Learns-from-2D-Talking-Face"><a href="#Learn2Talk-3D-Talking-Face-Learns-from-2D-Talking-Face" class="headerlink" title="Learn2Talk: 3D Talking Face Learns from 2D Talking Face"></a>Learn2Talk: 3D Talking Face Learns from 2D Talking Face</h2><p><strong>Authors:Yixiang Zhuang, Baoping Cheng, Yao Cheng, Yuntao Jin, Renshuai Liu, Chengyang Li, Xuan Cheng, Jing Liao, Juncong Lin</strong></p><p>Speech-driven facial animation methods usually contain two main classes, 3D and 2D talking face, both of which attract considerable research attention in recent years. However, to the best of our knowledge, the research on 3D talking face does not go deeper as 2D talking face, in the aspect of lip-synchronization (lip-sync) and speech perception. To mind the gap between the two sub-fields, we propose a learning framework named Learn2Talk, which can construct a better 3D talking face network by exploiting two expertise points from the field of 2D talking face. Firstly, inspired by the audio-video sync network, a 3D sync-lip expert model is devised for the pursuit of lip-sync between audio and 3D facial motion. Secondly, a teacher model selected from 2D talking face methods is used to guide the training of the audio-to-3D motions regression network to yield more 3D vertex accuracy. Extensive experiments show the advantages of the proposed framework in terms of lip-sync, vertex accuracy and speech perception, compared with state-of-the-arts. Finally, we show two applications of the proposed framework: audio-visual speech recognition and speech-driven 3D Gaussian Splatting based avatar animation.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.12888v1">PDF</a></p><p><strong>Summary</strong><br>通过借鉴2D说话人面部的唇形同步和言语感知领域的专业知识，提出了一种学习框架，可以构建更好的3D说话人面部网络。</p><p><strong>Key Takeaways</strong></p><ul><li>3D说话人面部研究在唇形同步和言语感知方面不如2D说话人面部研究深入。</li><li>Learn2Talk框架利用2D说话人面部领域的两个专业知识点来构建更好的3D说话人面部网络。</li><li>3D同步唇专家模型旨在实现音频和3D面部运动之间的唇形同步。</li><li>2D说话人面部方法中选择的教师模型用于指导音频到3D运动回归网络的训练，以提高3D顶点精度。</li><li>广泛的实验表明，该框架在唇形同步、顶点精度和言语感知方面优于现有技术。</li><li>该框架有语音-视觉语音识别和语音驱动3D高斯飞溅基于头像动画两个应用。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>标题：Learn2Talk：3D 说话人脸从 2D 说话人脸学习</p></li><li><p>作者：Yixiang Zhuang, Baoping Cheng, Yao Cheng, Yuntao Jin, Renshuai Liu, Chengyang Li, Xuan Cheng, Jing Liao, Juncong Lin</p></li><li><p>单位：暂缺</p></li><li><p>关键词：Speech-driven, 3D Facial Animation, 2D Talking face, Transformer, 3D Gaussian Splatting</p></li><li><p>论文链接：https://arxiv.org/abs/2404.12888v1 Github：None</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：说话人脸动画方法通常包含 3D 和 2D 说话人脸两大类，近年来两者都备受研究关注。然而，据我们所知，3D 说话人脸的研究在唇形同步（lip-sync）和语音感知方面并未像 2D 说话人脸那样深入。</p><p>（2）：过去的方法及其问题：本文方法动机充分。</p><p>（3）：本文提出的研究方法：提出一个名为 Learn2Talk 的学习框架，该框架通过利用 2D 说话人脸领域的两个专业知识点来构建一个更好的 3D 说话人脸网络。首先，受音频视频同步网络的启发，设计了一个 3D 同步唇形专家模型，以追求音频和 3D 面部动作之间的唇形同步。其次，选择一个来自 2D 说话人脸方法的教师模型来指导音频到 3D 运动回归网络的训练，以产生更高的 3D 顶点精度。</p><p>（4）：方法性能：本文方法在唇形同步、顶点精度和语音感知方面均优于现有技术。这些性能可以支持其目标。</p><ol><li>方法：</li></ol><p>（1）：提出一个名为 Learn2Talk 的学习框架，该框架通过利用 2D 说话人脸领域的两个专业知识点来构建一个更好的 3D 说话人脸网络。</p><p>（2）：设计了一个 3D 同步唇形专家模型，以追求音频和 3D 面部动作之间的唇形同步。</p><p>（3）：选择一个来自 2D 说话人脸方法的教师模型来指导音频到 3D 运动回归网络的训练，以产生更高的 3D 顶点精度。</p><ol><li>结论：</li></ol><p>（1）：本文提出了一种名为 Learn2Talk 的学习框架，该框架通过利用 2D 说话人脸领域的两个专业知识点来构建一个更好的 3D 说话人脸网络，在唇形同步、顶点精度和语音感知方面均优于现有技术。</p><p>（2）：创新点：提出了一种新的 3D 说话人脸动画方法，该方法利用了 2D 说话人脸领域的专业知识；性能：在唇形同步、顶点精度和语音感知方面均优于现有技术；工作量：需要收集和标注大量的数据。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-c2e8566372db83537dc565617387f4cf.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-c36e0e4bd338738e2a5420e68cd6ab5d.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-3fe7530e7260eff001a6736622671663.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-3f8c50de092534c8ec8b833626c35e42.jpg" align="middle"></details>## Contrastive Gaussian Clustering: Weakly Supervised 3D Scene Segmentation **Authors:Myrna C. Silva, Mahtab Dahaghin, Matteo Toso, Alessio Del Bue** We introduce Contrastive Gaussian Clustering, a novel approach capable of provide segmentation masks from any viewpoint and of enabling 3D segmentation of the scene. Recent works in novel-view synthesis have shown how to model the appearance of a scene via a cloud of 3D Gaussians, and how to generate accurate images from a given viewpoint by projecting on it the Gaussians before $\alpha$ blending their color. Following this example, we train a model to include also a segmentation feature vector for each Gaussian. These can then be used for 3D scene segmentation, by clustering Gaussians according to their feature vectors; and to generate 2D segmentation masks, by projecting the Gaussians on a plane and $\alpha$ blending over their segmentation features. Using a combination of contrastive learning and spatial regularization, our method can be trained on inconsistent 2D segmentation masks, and still learn to generate segmentation masks consistent across all views. Moreover, the resulting model is extremely accurate, improving the IoU accuracy of the predicted masks by $+8\%$ over the state of the art. Code and trained models will be released soon. [PDF](http://arxiv.org/abs/2404.12784v1) **Summary** 使用来自不同视角的对比高斯聚类实现 3D 场景分割。 **Key Takeaways** - 提出一种新的对比高斯聚类方法，能够从任何视角提供分割掩模，并实现场景的 3D 分割。 - 受新视角合成领域研究的启发，使用 3D 高斯云建模场景的外观。 - 通过将高斯投影到给定视点并对其颜色进行α混合，从给定视点生成准确的图像。 - 训练模型，使每个高斯都包含一个分割特征向量。 - 通过根据其特征向量对高斯进行聚类，可用于 3D 场景分割；通过将高斯投影到平面上并对其分割特征进行 α 混合，可生成 2D 分割掩模。 - 使用对比学习和空间正则化的组合，可以在不一致的 2D 分割掩模上训练我们的方法，并学习生成在所有视图中都一致的分割掩模。 - 所提出的方法非常准确，与现有技术相比，预测掩模的 IoU 准确度提高了 8%。 - 代码和训练好的模型即将发布。 **[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>标题：对比高斯聚类：弱监督 3D 场景分割</p></li><li><p>作者：Myrna C. Silva、Mahtab Dahaghin、Matteo Toso、Alessio Del Bue</p></li><li><p>单位：意大利理工学院模式分析与计算机视觉（PAVIS）</p></li><li><p>关键词：3D 高斯散射、3D 分割、对比学习</p></li><li><p>论文链接：arXiv:2404.12784v1 [cs.CV] Github 链接：无</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：近年来，新视角合成领域的研究表明，可以通过 3D 高斯云对场景的外观进行建模，并通过在给定视角上投影高斯并 α 混合其颜色来生成准确的图像。</p><p>（2）：过去方法与问题：高斯分组和 LangSplat 等方法存在以下问题： - 训练和评估需要大量 GPU 内存，导致某些场景无法处理。 - 无法从任意视角提供分割掩码，也无法实现场景的 3D 分割。</p><p>（3）：研究方法：本文提出对比高斯聚类方法，该方法通过以下步骤实现 3D 场景分割和 2D 分割掩码预测： - 训练模型为每个高斯体添加分割特征向量。 - 根据特征向量对高斯体进行聚类，实现 3D 场景分割。 - 将高斯体投影到平面上并 α 混合其分割特征，生成 2D 分割掩码。 - 使用对比学习和空间正则化，在不一致的 2D 分割掩码上训练模型，生成跨所有视角一致的分割掩码。</p><p>（4）：性能与目标： - 任务：3D 场景分割和 2D 分割掩码预测。 - 性能：IoU 准确率比现有技术提高 +8%，表明该方法能够有效实现其目标。</p><ol><li>方法：</li></ol><p>（1）：将场景表示为 3D 高斯体集合，编码几何、外观和实例分割信息；</p><p>（2）：使用基础模型生成 2D 分割掩码；</p><p>（3）：优化 3D 高斯体，最小化渲染图像和真实图像之间的差异；</p><p>（4）：使用对比分割损失监督 3D 特征场；</p><p>（5）：引入正则化项，强制高斯体在欧几里得和分割特征空间中的距离相关；</p><p>（6）：渲染 2D 特征图，根据对应的 2D 分割掩码对渲染特征进行聚类，计算对比聚类损失；</p><p>（7）：最大化同一分割内特征之间的相似度，最小化不同分割内的特征相似度。</p><p><strong>8. 结论</strong></p><p><strong>(1)</strong> 本工作的主要意义在于：</p><p>提出了对比高斯聚类方法，实现了 3D 场景分割和 2D 分割掩码预测，有效提高了分割精度。</p><p><strong>(2)</strong> 本文优缺点总结（创新点、性能、工作量）：</p><p><strong>创新点：</strong></p><ul><li>引入对比学习和空间正则化，提高了分割掩码的一致性。</li><li>使用 3D 高斯体表示场景，编码几何、外观和实例分割信息。</li></ul><p><strong>性能：</strong></p><ul><li>IoU 准确率比现有技术提高 +8%，分割精度高。</li></ul><p><strong>工作量：</strong></p><ul><li>训练和评估需要大量 GPU 内存，大场景处理困难。</li><li>无法从任意视角提供分割掩码，无法实现场景的完整 3D 分割。</li></ul><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-252e679c7e0a5cfc8056b41c43d99b59.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-668e640c91611b7b91220b00abd05f4e.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-03dada656b628530891ef19dcbebedba.jpg" align="middle"></details>## RainyScape: Unsupervised Rainy Scene Reconstruction using Decoupled Neural Rendering **Authors:Xianqiang Lyu, Hui Liu, Junhui Hou** We propose RainyScape, an unsupervised framework for reconstructing clean scenes from a collection of multi-view rainy images. RainyScape consists of two main modules: a neural rendering module and a rain-prediction module that incorporates a predictor network and a learnable latent embedding that captures the rain characteristics of the scene. Specifically, based on the spectral bias property of neural networks, we first optimize the neural rendering pipeline to obtain a low-frequency scene representation. Subsequently, we jointly optimize the two modules, driven by the proposed adaptive direction-sensitive gradient-based reconstruction loss, which encourages the network to distinguish between scene details and rain streaks, facilitating the propagation of gradients to the relevant components. Extensive experiments on both the classic neural radiance field and the recently proposed 3D Gaussian splatting demonstrate the superiority of our method in effectively eliminating rain streaks and rendering clean images, achieving state-of-the-art performance. The constructed high-quality dataset and source code will be publicly available. [PDF](http://arxiv.org/abs/2404.11401v1) **Summary** 雨景重建：无监督地从多视角雨景图重建干净场景。 **Key Takeaways** - 提出无监督框架 RainyScape，重建干净场景。 - RainyScape 由神经渲染和降雨预测模块组成。 - 降雨预测模块包含预测网络和可学习潜嵌入，捕捉场景的降雨特征。 - 基于神经网络的光谱偏差属性，优化神经渲染管道，获得低频场景表示。 - 利用自适应方向敏感梯度重建损失，优化两个模块，区分场景细节和雨痕。 - 在神经辐射场和 3D 高斯喷溅中进行的实验表明，该方法能有效消除雨痕、渲染干净图像，达到最先进性能。 - 将公开构建高质量数据集和源代码。 **[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>标题：RainyScape：基于解耦神经渲染的无监督雨景重建</p></li><li><p>作者：Xianqiang Lyu, Hui Liu, Junhui Hou</p></li><li><p>单位：香港城市大学计算机科学系</p></li><li><p>关键词：雨景重建、神经渲染、无监督损失</p></li><li><p>论文链接：xxx，Github 链接：None</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：神经辐射场（NeRF）在学习场景的连续体积表示方面取得了突破性进展，但当输入图像因模糊、噪声或雨水等因素而退化时，渲染结果不可避免地会出现明显伪影。</p><p>（2）：过去方法：现有方法针对不同的退化因素提出了特定任务的解决方案，但针对雨景重建任务的方法较少，且难以通过附加神经渲染场来表示雨水。</p><p>（3）：研究方法：本文提出 RainyScape，一个解耦的神经渲染框架，能够从雨景图像中无监督地重建无雨场景。该框架包括一个神经渲染模块和一个雨滴预测模块，通过学习雨滴嵌入和使用预测器来预测雨滴条纹，并提出自适应角度估计策略和梯度旋转损失来解耦场景高频细节和雨滴条纹。</p><p>（4）：方法性能：在经典神经辐射场和最近提出的 3D 高斯 splatting 上的广泛实验表明，该方法在有效消除雨滴条纹和渲染清晰图像方面优于现有方法，达到最先进的性能。</p><ol><li>方法：</li></ol><p>（1）：提出 RainyScape，一个解耦的神经渲染框架，能够从雨景图像中无监督地重建无雨场景。</p><p>（2）：该框架包括一个神经渲染模块和一个雨滴预测模块，通过学习雨滴嵌入和使用预测器来预测雨滴条纹。</p><p>（3）：提出自适应角度估计策略和梯度旋转损失来解耦场景高频细节和雨滴条纹。</p><ol><li>结论：<pre><code>            （1）：RainyScape 在雨景重建领域具有重要意义，它首次提出了一个解耦神经渲染框架，能够从雨景图像中无监督地重建无雨场景。 该框架通过将场景高频细节和雨滴条纹解耦，有效地消除了雨滴条纹，并渲染出清晰的图像。

            （2）：创新点：RainyScape 创新性地提出了一个解耦神经渲染框架，将场景高频细节和雨滴条纹解耦，有效地消除了雨滴条纹，并渲染出清晰的图像。

            性能：RainyScape 在经典神经辐射场和最近提出的 3D 高斯 splatting 上的广泛实验表明，该方法在有效消除雨滴条纹和渲染清晰图像方面优于现有方法，达到最先进的性能。

            工作量：RainyScape 的工作量中等，需要训练神经渲染模块和雨滴预测模块，并提出自适应角度估计策略和梯度旋转损失来解耦场景高频细节和雨滴条纹。
</code></pre></li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-789763f7ebb6ec7a923539611ab1fe24.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-89f176b1378008d1c0b63c9241adfdb2.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-5f7fb8305c36c1fe2572adfd98b584f7.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-76be36036e15658d754b57c4864b0abf.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-3765b699865b1d89cc9f5f13f9843a0e.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-34d10a80ece07ba92081dfc066d00427.jpg" align="middle"></details><h2 id="DeblurGS-Gaussian-Splatting-for-Camera-Motion-Blur"><a href="#DeblurGS-Gaussian-Splatting-for-Camera-Motion-Blur" class="headerlink" title="DeblurGS: Gaussian Splatting for Camera Motion Blur"></a>DeblurGS: Gaussian Splatting for Camera Motion Blur</h2><p><strong>Authors:Jeongtaek Oh, Jaeyoung Chung, Dongwoo Lee, Kyoung Mu Lee</strong></p><p>Although significant progress has been made in reconstructing sharp 3D scenes from motion-blurred images, a transition to real-world applications remains challenging. The primary obstacle stems from the severe blur which leads to inaccuracies in the acquisition of initial camera poses through Structure-from-Motion, a critical aspect often overlooked by previous approaches. To address this challenge, we propose DeblurGS, a method to optimize sharp 3D Gaussian Splatting from motion-blurred images, even with the noisy camera pose initialization. We restore a fine-grained sharp scene by leveraging the remarkable reconstruction capability of 3D Gaussian Splatting. Our approach estimates the 6-Degree-of-Freedom camera motion for each blurry observation and synthesizes corresponding blurry renderings for the optimization process. Furthermore, we propose Gaussian Densification Annealing strategy to prevent the generation of inaccurate Gaussians at erroneous locations during the early training stages when camera motion is still imprecise. Comprehensive experiments demonstrate that our DeblurGS achieves state-of-the-art performance in deblurring and novel view synthesis for real-world and synthetic benchmark datasets, as well as field-captured blurry smartphone videos.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.11358v2">PDF</a></p><p><strong>Summary</strong><br>从模糊运动图像重建清晰 3D 场景方法，优化 3D 高斯投射，实现精确摄像机位姿初始化。</p><p><strong>Key Takeaways</strong></p><ul><li>DeblurGS 优化高斯投射，提高运动模糊图像 3D 重建精度。</li><li>利用高斯投射的重建能力，还原精细锐利场景。</li><li>估计每幅模糊图像的 6 自由度摄像机运动，生成模糊渲染用于优化。</li><li>高斯密度退火策略防止错误位置生成不准确的高斯。</li><li>DeblurGS 在去模糊和合成新视角方面取得了最先进的性能。</li><li>适用于真实世界和合成基准数据集，以及现场拍摄的模糊智能手机视频。</li><li>DeblurGS 极大地扩展了运动模糊图像的 3D 重建的实际应用。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: DeblurGS: 高斯溅射相机运动模糊 (DeblurGS: Gaussian Splatting for Camera Motion Blur)</p></li><li><p>Authors: Jeongtaek Oh, Jaeyoung Chung, Dongwoo Lee, and Kyoung Mu Lee</p></li><li><p>Affiliation: 首尔国立大学人工智能与信息处理研究所 (IPAI, Seoul National University)</p></li><li><p>Keywords: 3D Gaussian Splatting · Camera Motion Deblurring</p></li><li><p>Urls: None, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 尽管从运动模糊图像重建清晰的 3D 场景方面取得了重大进展，但向实际应用的过渡仍然具有挑战性。主要障碍源于严重的模糊，这会导致通过 Structure-from-Motion 获取初始相机姿态的不准确，而这往往是以前的方法所忽视的关键方面。</p><p>(2): 过去的方法主要集中于模糊图像的去模糊处理，但对于初始相机姿态的噪声初始化不鲁棒。</p><p>(3): 本文提出 DeblurGS，这是一种从运动模糊图像优化清晰的 3D 高斯溅射的方法，即使在噪声相机姿态初始化的情况下也是如此。我们利用 3D 高斯溅射的出色重建能力来恢复细粒度的清晰场景。我们的方法估计每个模糊观测的 6 自由度相机运动，并为优化过程合成相应的模糊渲染。此外，我们提出了高斯致密化退火策略，以防止在相机运动仍然不精确的早期训练阶段在错误的位置生成不准确的高斯。</p><p>(4): 综合实验表明，我们的 DeblurGS 在真实世界和合成基准数据集以及现场捕获的模糊智能手机视频的去模糊和新视图合成方面实现了最先进的性能。</p><ol><li>方法：</li></ol><p>（1）：提出 DeblurGS，一种从运动模糊图像优化清晰的 3D 高斯溅射的方法；</p><p>（2）：利用 3D 高斯溅射的重建能力恢复细粒度的清晰场景；</p><p>（3）：估计每个模糊观测的 6 自由度相机运动，并合成相应的模糊渲染；</p><p>（4）：提出高斯致密化退火策略，防止在相机运动不精确的早期训练阶段生成不准确的高斯。</p><ol><li>结论：</li></ol><p>（1）：本文提出了一种从运动模糊图像优化清晰的 3D 高斯溅射的方法，即使在噪声相机姿态初始化的情况下也是如此。该方法利用 3D 高斯溅射的出色重建能力来恢复细粒度的清晰场景，估计每个模糊观测的 6 自由度相机运动，并为优化过程合成相应的模糊渲染。此外，该方法提出了高斯致密化退火策略，以防止在相机运动仍然不精确的早期训练阶段在错误的位置生成不准确的高斯。综合实验表明，该方法在真实世界和合成基准数据集以及现场捕获的模糊智能手机视频的去模糊和新视图合成方面实现了最先进的性能。</p><p>（2）：创新点：利用 3D 高斯溅射的重建能力恢复细粒度的清晰场景，即使在噪声相机姿态初始化的情况下也是如此；</p><p>性能：在真实世界和合成基准数据集以及现场捕获的模糊智能手机视频的去模糊和新视图合成方面实现了最先进的性能；</p><p>工作量：该方法需要估计每个模糊观测的 6 自由度相机运动，并为优化过程合成相应的模糊渲染，这可能会增加计算成本。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-d1b62fa212aabdf515b9baf8fdc306be.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-32c4f56eaf456fe86ff5f42abfbd6ffb.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-50b9e9cff40ee36449b6b3559539186a.jpg" align="middle"></details></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io/2024/04/22/Paper/2024-04-22/3DGS/">https://kedreamix.github.io/2024/04/22/Paper/2024-04-22/3DGS/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/3DGS/">3DGS</a></div><div class="post_share"><div class="social-share" data-image="https://pic1.zhimg.com/v2-c36e0e4bd338738e2a5420e68cd6ab5d.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/04/22/Paper/2024-04-22/NeRF/" title="NeRF"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-5f7fb8305c36c1fe2572adfd98b584f7.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">NeRF</div></div></a></div><div class="next-post pull-right"><a href="/2024/04/22/Paper/2024-04-22/Talking%20Head%20Generation/" title="Talking Head Generation"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-3fe7530e7260eff001a6736622671663.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Talking Head Generation</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/01/25/Paper/3DGS%20Survey/" title="3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-25</div><div class="title">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</div></div></a></div><div><a href="/2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" title="超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-01</div><div class="title">超赞的数字人生成知识库 Awesome-Talking-Head-Synthesis</div></div></a></div><div><a href="/2024/01/24/Paper/2024-01-24/3DGS/" title="3DGS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-3d3dcd00c27bc3d320b23d4247ae79f3.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-24</div><div class="title">3DGS</div></div></a></div><div><a href="/2024/02/09/Paper/2024-02-09/3DGS/" title="3DGS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-28074a5f13fdf5a52c0d4de04dfb9406.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-09</div><div class="title">3DGS</div></div></a></div><div><a href="/2024/01/30/Paper/2024-01-30/3DGS/" title="3DGS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-2032721a60695f2d41ac96f75dec65a2.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-30</div><div class="title">3DGS</div></div></a></div><div><a href="/2024/02/13/Paper/2024-02-13/3DGS/" title="3DGS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-785f0dd46228bdf108d1677b776eeb58.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-13</div><div class="title">3DGS</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-04-22-%E6%9B%B4%E6%96%B0"><span class="toc-text">2024-04-22 更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Learn2Talk-3D-Talking-Face-Learns-from-2D-Talking-Face"><span class="toc-text">Learn2Talk: 3D Talking Face Learns from 2D Talking Face</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DeblurGS-Gaussian-Splatting-for-Camera-Motion-Blur"><span class="toc-text">DeblurGS: Gaussian Splatting for Camera Motion Blur</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image:url('https://pic1.zhimg.com/v2-c36e0e4bd338738e2a5420e68cd6ab5d.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js").then((()=>{pangu.autoSpacingPage()}))}function panguInit(){panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.1},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script></div><script id="canvas_nest" defer color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><script data-pjax>function butterfly_swiper_injector_config(){var a=document.getElementById("recent-posts");console.log("已挂载butterfly_swiper"),a.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/25/Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="2024/01/25/Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="2024/01/25/Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/17/Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023/12/17/Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="2023/12/17/Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script></body></html>