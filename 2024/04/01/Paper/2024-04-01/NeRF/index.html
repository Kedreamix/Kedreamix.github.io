<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>NeRF | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="NeRF 方向最新论文已更新，请持续关注 Update in 2024-04-01  Talk3D High-Fidelity Talking Portrait Synthesis via Personalized 3D   Generative Prior"><meta property="og:type" content="article"><meta property="og:title" content="NeRF"><meta property="og:url" content="https://kedreamix.github.io/2024/04/01/Paper/2024-04-01/NeRF/index.html"><meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World"><meta property="og:description" content="NeRF 方向最新论文已更新，请持续关注 Update in 2024-04-01  Talk3D High-Fidelity Talking Portrait Synthesis via Personalized 3D   Generative Prior"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://picx.zhimg.com/v2-9e4d3acaf0612269dbaa41a149d52930.jpg"><meta property="article:published_time" content="2024-04-01T03:53:23.000Z"><meta property="article:modified_time" content="2024-04-01T03:53:23.341Z"><meta property="article:author" content="Kedreamix"><meta property="article:tag" content="NeRF"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://picx.zhimg.com/v2-9e4d3acaf0612269dbaa41a149d52930.jpg"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/2024/04/01/Paper/2024-04-01/NeRF/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-74LZ5BEQQ1")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!0,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"简"},noticeOutdate:{limitDay:500,position:"top",messagePrev:"It has been",messageNext:"days since the last update, the content of the article may be outdated."},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:200},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!0,post:!0},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!0,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"NeRF",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-04-01 11:53:23"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach((e=>{n.setAttribute(e,t[e])})),document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)})),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=24?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 7.0.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml"></head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{const d=document.getElementById("loading-box"),e=document.body,o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l(),window.addEventListener("load",(()=>{o()}))})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">278</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url('https://picx.zhimg.com/v2-9e4d3acaf0612269dbaa41a149d52930.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">NeRF</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-04-01T03:53:23.000Z" title="发表于 2024-04-01 11:53:23">2024-04-01</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-04-01T03:53:23.341Z" title="更新于 2024-04-01 11:53:23">2024-04-01</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">11.5k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>40分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="NeRF"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-01-更新"><a href="#2024-04-01-更新" class="headerlink" title="2024-04-01 更新"></a>2024-04-01 更新</h1><h2 id="Talk3D-High-Fidelity-Talking-Portrait-Synthesis-via-Personalized-3D-Generative-Prior"><a href="#Talk3D-High-Fidelity-Talking-Portrait-Synthesis-via-Personalized-3D-Generative-Prior" class="headerlink" title="Talk3D: High-Fidelity Talking Portrait Synthesis via Personalized 3D   Generative Prior"></a>Talk3D: High-Fidelity Talking Portrait Synthesis via Personalized 3D Generative Prior</h2><p><strong>Authors:Jaehoon Ko, Kyusun Cho, Joungbin Lee, Heeji Yoon, Sangmin Lee, Sangjun Ahn, Seungryong Kim</strong></p><p>Recent methods for audio-driven talking head synthesis often optimize neural radiance fields (NeRF) on a monocular talking portrait video, leveraging its capability to render high-fidelity and 3D-consistent novel-view frames. However, they often struggle to reconstruct complete face geometry due to the absence of comprehensive 3D information in the input monocular videos. In this paper, we introduce a novel audio-driven talking head synthesis framework, called Talk3D, that can faithfully reconstruct its plausible facial geometries by effectively adopting the pre-trained 3D-aware generative prior. Given the personalized 3D generative model, we present a novel audio-guided attention U-Net architecture that predicts the dynamic face variations in the NeRF space driven by audio. Furthermore, our model is further modulated by audio-unrelated conditioning tokens which effectively disentangle variations unrelated to audio features. Compared to existing methods, our method excels in generating realistic facial geometries even under extreme head poses. We also conduct extensive experiments showing our approach surpasses state-of-the-art benchmarks in terms of both quantitative and qualitative evaluations.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.20153v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://ku-cvlab.github.io/Talk3D/">https://ku-cvlab.github.io/Talk3D/</a></p><p><strong>Summary</strong><br>通过采用预训练的 3D 感知生成先验，Talk3D 可生成真实的面部几何形状。</p><p><strong>Key Takeaways</strong></p><ul><li>Talk3D 采用预训练的 3D 感知生成先验，重建逼真的面部几何形状。</li><li>音频引导注意力 U-Net 架构预测 NeRF 空间中的动态面部变化。</li><li>音频无关调节令牌有效地区分与音频无关的变化。</li><li>Talk3D 即使在极端头部姿势下也能生成逼真的面部几何形状。</li><li>广泛的实验表明 Talk3D 在定量和定性评估方面都超越了最先进的基准。</li><li>Talk3D 可以生成任意视角的面部重建，具有 3D 一致性和高保真度。</li><li>Talk3D 可以有效地减少光照和表情变化等音频无关因素的影响。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：Talk3D：高保真说话人图像合成</li><li>作者：Changil Kim, Minhyeok Lee, Juyong Kim, Nojun Kwak</li><li>隶属机构：首尔国立大学</li><li>关键词：音频驱动说话人图像合成、神经辐射场、3D 感知生成模型</li><li>论文链接：https://arxiv.org/abs/2403.20153</li><li>摘要： （1）研究背景： 近年来，音频驱动的说话人图像合成方法取得了很大进展，但这些方法通常依赖于单目说话人图像视频，难以重建完整的面部几何结构。</li></ol><p>（2）过去的方法及其问题： 现有方法通常优化神经辐射场 (NeRF) 来渲染新视角的图像，但由于输入单目视频中缺乏全面的 3D 信息，它们难以重建完整的面部几何结构。</p><p>（3）提出的研究方法： 本文提出了一种新的音频驱动的说话人图像合成框架 Talk3D，它通过有效采用预训练的 3D 感知生成模型，可以忠实地重建可信的面部几何结构。该框架包括一个音频引导注意力 U-Net 架构，该架构预测由音频驱动的 NeRF 空间中的动态面部变化。此外，该模型还通过与音频无关的调节令牌进行调制，有效地解耦与音频特征无关的变化。</p><p>（4）方法在任务和性能上的表现： 在说话人图像合成任务上，Talk3D 在图像质量、几何保真度和音频同步方面都优于现有方法。这些性能支持了该方法的目标，即生成高质量且逼真的说话人图像。</p><ol><li><p>方法： （1）EG3D模型：采用神经辐射场（NeRF）技术，通过优化平面生成器和体积渲染器，实现图像生成。 （2）个性化生成器：使用VIVE3D策略，将3D感知生成对抗网络（GAN）调整为特定身份，生成单一身份图像。 （3）音频引导注意力U-Net：利用U-Net架构，预测由音频驱动的NeRF空间中的动态面部变化，并通过与音频无关的调节令牌进行调制。 （4）分割卷积：将每个平面独立处理，以维护其特征，同时使用展开方法融合来自每个平面的特征。</p></li><li><p>结论： （1）：本文提出了 Talk3D，一个结合了 3D 感知 GAN 先验和区域感知运动的高保真 3D 说话人图像合成框架。我们的框架集成了使用 VIVE3D 框架微调的个性化生成器，允许生成具有逼真几何结构和显式渲染视点控制的 3D 感知说话人头像。此外，我们提出的音频引导注意力 U-Net 架构增强了图像帧内局部变化（如背景、躯干和眼睛运动）的解耦。通过广泛的实验，我们证明了我们提出的模型不仅可以根据输入音频产生准确的唇部动作，还可以从新颖的视点进行渲染，解决了先前最先进方法中观察到的局限性。我们预期我们的工作将对数字媒体体验和虚拟交互产生重大影响，并在电影制作、虚拟化身和视频会议中找到应用。 （2）：创新点： （1）提出了一种新的音频驱动的说话人图像合成框架 Talk3D，该框架将 3D 感知 GAN 先验和区域感知运动相结合，以实现高保真 3D 说话人头像合成。 （2）利用 VIVE3D 策略，将 3D 感知生成对抗网络 (GAN) 调整为特定身份，生成具有逼真几何结构和显式渲染视点控制的 3D 感知说话人头像。 （3）提出了一个音频引导注意力 U-Net 架构，该架构增强了图像帧内局部变化（如背景、躯干和眼睛运动）的解耦。 性能： （1）在说话人图像合成任务上，Talk3D 在图像质量、几何保真度和音频同步方面都优于现有方法。 （2）Talk3D 能够从新颖的视点进行渲染，解决了先前最先进方法中观察到的局限性。 （3）Talk3D 可以有效地解耦与音频特征无关的变化，从而生成更逼真、更自然的高保真 3D 说话人图像。 工作量： （1）Talk3D 的实现需要大量的数据预处理和模型训练。 （2）Talk3D 的推理过程相对高效，可以实时生成高质量的 3D 说话人图像。 （3）Talk3D 的代码和数据已公开，便于研究人员和从业者进一步研究和应用。</p></li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-b945787a9603752fdfa9bacd5ecbd8e0.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-1eb3bf1b0c5000057abc431bf6035fce.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-9e4d3acaf0612269dbaa41a149d52930.jpg" align="middle"></details><h2 id="SGD-Street-View-Synthesis-with-Gaussian-Splatting-and-Diffusion-Prior"><a href="#SGD-Street-View-Synthesis-with-Gaussian-Splatting-and-Diffusion-Prior" class="headerlink" title="SGD: Street View Synthesis with Gaussian Splatting and Diffusion Prior"></a>SGD: Street View Synthesis with Gaussian Splatting and Diffusion Prior</h2><p><strong>Authors:Zhongrui Yu, Haoran Wang, Jinze Yang, Hanzhang Wang, Zeke Xie, Yunfeng Cai, Jiale Cao, Zhong Ji, Mingming Sun</strong></p><p>Novel View Synthesis (NVS) for street scenes play a critical role in the autonomous driving simulation. The current mainstream technique to achieve it is neural rendering, such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). Although thrilling progress has been made, when handling street scenes, current methods struggle to maintain rendering quality at the viewpoint that deviates significantly from the training viewpoints. This issue stems from the sparse training views captured by a fixed camera on a moving vehicle. To tackle this problem, we propose a novel approach that enhances the capacity of 3DGS by leveraging prior from a Diffusion Model along with complementary multi-modal data. Specifically, we first fine-tune a Diffusion Model by adding images from adjacent frames as condition, meanwhile exploiting depth data from LiDAR point clouds to supply additional spatial information. Then we apply the Diffusion Model to regularize the 3DGS at unseen views during training. Experimental results validate the effectiveness of our method compared with current state-of-the-art models, and demonstrate its advance in rendering images from broader views.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.20079v1">PDF</a></p><p><strong>Summary</strong><br>神经辐射场和高斯体渲染等神经渲染技术在自动驾驶模拟中扮演着关键角色，但处理街道场景时，此类技术难以保持偏离训练视角较大的视点的渲染质量。本文提出了一种新颖的方法，通过利用扩散模型的先验知识和补充的多模态数据来增强3D高斯体渲染的能力。</p><p><strong>Key Takeaways</strong></p><ul><li>神经渲染技术在自动驾驶模拟中的街景新视角合成（NVS）中至关重要。</li><li>当前的神经渲染方法在处理街景时，难以保持偏离训练视角较大的视点的渲染质量。</li><li>问题源于移动车辆上固定摄像机捕获的稀疏训练视图。</li><li>提出了一种新颖的方法，利用扩散模型的先验知识和补充的多模态数据来增强3D高斯体渲染的能力。</li><li>首先通过添加相邻帧图像作为条件对扩散模型进行微调，同时利用激光雷达点云的深度数据提供额外的空间信息。</li><li>然后将扩散模型应用于训练期间未见视图中的3D高斯体渲染进行正则化。</li><li>实验结果验证了该方法与目前最先进模型相比的有效性，并展示了其在从更宽广的视角渲染图像方面的优势。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：SGD：高斯点 splatting 和扩散先验的街景合成</li><li>作者：Zhongrui Yu†、Haoran Wang‡、Jinze Yang、Hanzhang Wang、Zeke Xie、Yunfeng Cai、Jiale Cao、Zhong Ji、Mingming Sun</li><li>第一作者单位：苏黎世联邦理工学院</li><li>关键词：街景合成、神经渲染、扩散模型、高斯 splatting</li><li>论文链接：https://arxiv.org/abs/2403.20079 Github 代码链接：无</li><li><p>摘要： (1)：研究背景：街景合成（NVS）在自动驾驶仿真中至关重要。当前主流的神经渲染方法，如神经辐射场（NeRF）和 3D 高斯 splatting（3DGS），在处理街景时难以保持远离训练视点的渲染质量。 (2)：过去方法与问题：现有方法受限于移动车辆上固定摄像机采集的稀疏训练视点。本文提出了一种新颖的方法，利用扩散模型的先验和互补的多模态数据来增强 3DGS 的能力。 (3)：研究方法：该方法首先对扩散模型进行微调，添加相邻帧的图像作为条件，同时利用激光雷达点云的深度数据提供额外的空间信息。然后在训练期间将扩散模型应用于未见视点的 3DGS 正则化。 (4)：任务与性能：实验结果验证了该方法与当前最先进模型相比的有效性，并展示了其在从更广泛视点渲染图像方面的优势。</p></li><li><p>方法： （1）：微调扩散模型，以利用相邻帧的图像作为条件，并通过激光雷达点云的深度数据提供额外的空间信息； （2）：将微调后的扩散模型应用于 3DGS 训练期间，以正则化未见视点的合成； （3）：在 3DGS 训练中，随机采样伪视图，并使用扩散模型生成指导图像，以正则化 3DGS 模型的训练。</p></li><li><p>结论： (1): 本工作通过将扩散模型与3DGS相结合，有效提升了自动驾驶场景中的自由视角渲染能力，为自动驾驶仿真提供了更广阔的视角，有利于模拟潜在的危险边缘情况，从而提升自动驾驶系统的整体安全性和可靠性。 (2): 创新点：将扩散模型引入3DGS，利用扩散模型的先验和互补的多模态数据增强3DGS的能力。 性能：与当前最先进的模型相比，该方法在从更广泛视点渲染图像方面具有优势。 工作量：扩散模型的加入增加了训练时间，但该方法不影响3DGS的实时推理能力。</p></li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-59b729de5a1f08214181a45a66fe05e1.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-e3d1eae8db53c2b14375454d2d6f0cd9.jpg" align="middle"></details><h2 id="DerainNeRF-3D-Scene-Estimation-with-Adhesive-Waterdrop-Removal"><a href="#DerainNeRF-3D-Scene-Estimation-with-Adhesive-Waterdrop-Removal" class="headerlink" title="DerainNeRF: 3D Scene Estimation with Adhesive Waterdrop Removal"></a>DerainNeRF: 3D Scene Estimation with Adhesive Waterdrop Removal</h2><p><strong>Authors:Yunhao Li, Jing Wu, Lingzhe Zhao, Peidong Liu</strong></p><p>When capturing images through the glass during rainy or snowy weather conditions, the resulting images often contain waterdrops adhered on the glass surface, and these waterdrops significantly degrade the image quality and performance of many computer vision algorithms. To tackle these limitations, we propose a method to reconstruct the clear 3D scene implicitly from multi-view images degraded by waterdrops. Our method exploits an attention network to predict the location of waterdrops and then train a Neural Radiance Fields to recover the 3D scene implicitly. By leveraging the strong scene representation capabilities of NeRF, our method can render high-quality novel-view images with waterdrops removed. Extensive experimental results on both synthetic and real datasets show that our method is able to generate clear 3D scenes and outperforms existing state-of-the-art (SOTA) image adhesive waterdrop removal methods.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.20013v1">PDF</a></p><p><strong>Summary</strong><br>利用NeRF和注意力机制，从有水滴的图像中重建清晰的3D场景，去除水滴，提高图像质量和计算机视觉算法性能。</p><p><strong>Key Takeaways</strong></p><ul><li>使用NeRF重建有水滴图像中的3D场景，去除水滴，提高图像质量。</li><li>利用注意力网络预测水滴位置。</li><li>充分利用NeRF强大的场景表示能力。</li><li>渲染出无水滴的高质量新视图图像。</li><li>在合成和真实数据集上取得优异的实验结果。</li><li>超越现有水滴去除方法的性能。</li><li>提供清晰的3D场景，改善计算机视觉算法性能。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：DerainNeRF：去除粘性水滴的 3D 场景估计</li><li>作者：李云浩、吴靖、赵令哲、刘培东</li><li>单位：西湖大学工程学院</li><li>关键词：NeRF、水滴去除、3D 场景重建</li><li>论文链接：https://arxiv.org/abs/2403.20013</li><li>总结： （1）研究背景：雨雪天气下拍摄的图像经常会出现粘附在玻璃表面的水滴，严重影响图像质量和计算机视觉算法的性能。 （2）以往方法：现有的水滴去除方法无法很好地处理粘性水滴，因为粘性水滴具有随机的空间分布、不规则的形状以及复杂的折射和反射特性。 （3）研究方法：本文提出了一种基于 NeRF 的框架 DerainNeRF，该框架同时估计 3D 场景并去除水滴。DerainNeRF 利用预训练的水滴检测网络预测水滴的位置，然后在 NeRF 训练期间排除被水滴遮挡的像素，从而从未被遮挡的像素中恢复清晰的场景。 （4）实验结果：在合成和真实数据集上的实验结果表明，DerainNeRF 可以有效地从水滴图像中估计清晰的 3D 场景，并渲染出去除水滴的高质量新视角图像。</li></ol><p>7.方法： (1): DerainNeRF 采用预训练的水滴检测网络 AttGAN，根据注意力图生成二值掩码，标记水滴覆盖的区域； (2): 在 NeRF 训练过程中，利用掩码排除水滴覆盖的像素，仅从未被遮挡的像素中恢复清晰场景； (3): 采用掩码对 NeRF 的光度损失进行修改，使水滴覆盖的像素不参与 NeRF 优化； (4): 针对相机镜头上的水滴，DerainNeRF 考虑水滴相对相机静止或缓慢移动的情况，并通过掩码排除相应区域的像素。</p><ol><li>结论： （1）：本文提出了 DerainNeRF 框架，该框架同时估计 3D 场景并去除水滴，有效地从水滴图像中恢复清晰的场景，并渲染出去除水滴的高质量新视角图像。 （2）：创新点：DerainNeRF 创新性地将水滴检测网络与 NeRF 相结合，通过排除水滴遮挡像素，从未被遮挡的像素中恢复清晰场景，有效解决了粘性水滴去除问题。 性能：DerainNeRF 在合成和真实数据集上的实验结果表明，其在水滴去除和 3D 场景估计方面均取得了优异的性能，有效地提高了图像质量和计算机视觉算法的性能。 工作量：DerainNeRF 的实现需要预训练水滴检测网络和 NeRF 模型，训练过程需要大量的计算资源，工作量较大。</li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-19f242cbbe40d04087d7fa4b5738c1fa.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-074e9dac4fc4c02c192b25a9db8280ac.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-e9ee6dcf58671bff81f8e539beb1bd41.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-6a035cd7369fde02daca89446ae14e04.jpg" align="middle"></details><h2 id="Stable-Surface-Regularization-for-Fast-Few-Shot-NeRF"><a href="#Stable-Surface-Regularization-for-Fast-Few-Shot-NeRF" class="headerlink" title="Stable Surface Regularization for Fast Few-Shot NeRF"></a>Stable Surface Regularization for Fast Few-Shot NeRF</h2><p><strong>Authors:Byeongin Joung, Byeong-Uk Lee, Jaesung Choe, Ukcheol Shin, Minjun Kang, Taeyeop Lee, In So Kweon, Kuk-Jin Yoon</strong></p><p>This paper proposes an algorithm for synthesizing novel views under few-shot setup. The main concept is to develop a stable surface regularization technique called Annealing Signed Distance Function (ASDF), which anneals the surface in a coarse-to-fine manner to accelerate convergence speed. We observe that the Eikonal loss - which is a widely known geometric regularization - requires dense training signal to shape different level-sets of SDF, leading to low-fidelity results under few-shot training. In contrast, the proposed surface regularization successfully reconstructs scenes and produce high-fidelity geometry with stable training. Our method is further accelerated by utilizing grid representation and monocular geometric priors. Finally, the proposed approach is up to 45 times faster than existing few-shot novel view synthesis methods, and it produces comparable results in the ScanNet dataset and NeRF-Real dataset.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.19985v1">PDF</a> 3DV 2024</p><p><strong>Summary</strong><br>新颖的 annealed signed distance function 正则化技术实现了小样本场景重建中稳定的表面正则化，大幅提升了收敛速度。</p><p><strong>Key Takeaways</strong></p><ul><li>ASDF 作为有效的表面正则化技术，通过粗到精的退火方式加速收敛。</li><li>Eikonal 损失在小样本训练中因缺乏足够的训练信号而导致模型保真度低。</li><li>ASDF 正则化成功重建场景并产生高保真几何体，训练稳定性高。</li><li>采用网格表示和单目几何先验进一步加速了该方法。</li><li>该方法比现有小样本新颖视图合成方法快 45 倍，且在 ScanNet 和 NeRF-Real 数据集上产生具有可比性的结果。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：快速小样本 NeRF 的稳定表面正则化</li><li>作者：Byeongin Joung、Byeong-Uk Lee、Jaesung Choe、Ukcheol Shin、Minjun Kang、Taeyeop Lee、In So Kweon、Kuk-Jin Yoon</li><li>第一作者单位：韩国科学技术院</li><li>关键词：NeRF、小样本学习、表面正则化、几何约束</li><li>论文链接：https://arxiv.org/abs/2403.19985</li><li><p>摘要： （1）研究背景：NeRF 是一种用于隐式场景外观和几何编码的有效技术，但在小样本设置下训练 NeRF 具有挑战性，因为需要大量图像和较长的训练时间。 （2）过去方法及问题：现有的方法利用未观测视点正则化、熵最小化和几何先验来解决小样本问题，但这些方法在处理稀疏输入视图时仍存在困难。 （3）本文方法：本文提出了一种称为退火符号距离函数 (ASDF) 的稳定表面正则化技术，它以粗到细的方式退火表面以加速收敛速度。此外，本文还利用网格表示和单目几何先验进一步加速了训练过程。 （4）方法性能：本文方法在 ScanNet 和 NeRF-Real 数据集上实现了与现有方法相当的结果，同时训练速度提高了 45 倍。这表明本文方法可以有效地合成小样本 NeRF 的新颖视图。</p></li><li><p>方法： （1）利用 OmniData 提取给定 RGB 图像的几何先验，使用 COLMAP 获取稀疏 3D 点和相机位姿。 （2）构建多级特征体积网格和 MLP 解码器，分别用于 SDF 和颜色。 （3）使用三线性插值沿相机光线采样查询点的特征，并用 MLP 解码器渲染结果。 （4）提出退火符号距离函数损失 (ASDF) 来进行表面正则化，它以粗到细的方式退火表面以加速收敛速度。 （5）ASDF 损失由两个部分组成：几何平滑损失和加权 Eikonal 损失。 （6）几何平滑损失强制 SDF 值与查询点与渲染表面交点的距离相同。 （7）加权 Eikonal 损失强制 SDF 的梯度为常数 1。 （8）通过调整截断边界来实现从粗到细的策略，从而使网络首先优化粗略表面，然后逐渐恢复详细的几何形状。</p></li></ol><p>8.结论： （1）：本文提出了一种快速小样本NeRF，该方法利用深度密集先验和运动结构。鉴于从复杂场景中的少数视角优化几何信息存在困难，我们提出了一种新的表面正则化损失，即退火符号距离函数损失，它强制几何平滑并提高了合成新视图的性能。因此，我们成功地将深度密集先验、多视图一致性和多分辨率体素网格连接起来，用于具有稀疏输入视图的新视图合成。我们的方法可以通过采用 [6, 15] 等最新方法来进一步增强，以提高 NeRF 的优化速度。此外，对几何先验的不确定性处理可以通过减少现成网络的误差来提高性能。对于该方法的局限性，我们认为我们的退火符号距离函数需要依赖于场景几何或 SfM 结果（例如相机位姿的准确性）的超参数。我们认为以自适应方式解决这个问题而不进行启发式调整可能是未来的一个方向。 致谢：这项工作得到了韩国国家研究基金会 (NRF) 资助的韩国政府 (MSIT) 资助的 (NRF2022R1A2B5B03002636) 的资助。 （2）：创新点：提出了一种新的表面正则化损失，即退火符号距离函数损失，它强制几何平滑并提高了合成新视图的性能。 性能：在 ScanNet 和 NeRF-Real 数据集上实现了与现有方法相当的结果，同时训练速度提高了 45 倍。 工作量：利用 OmniData 提取给定 RGB 图像的几何先验，使用 COLMAP 获取稀疏 3D 点和相机位姿。构建多级特征体积网格和 MLP 解码器，分别用于 SDF 和颜色。使用三线性插值沿相机光线采样查询点的特征，并用 MLP 解码器渲染结果。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-3bcc470c48e4a8a117d3d6e5d53268d4.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-72fa498e7ef5b098ca99a0707636e29f.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-4661b03fcc8e4207234c97efbdd8ba7b.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-f5b4cca46545f72e81ef6d4e1f8759db.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-c366af19d749af51924a919153d54db6.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-68ecbe966b9562b356962cb20590cf97.jpg" align="middle"></details>## Mitigating Motion Blur in Neural Radiance Fields with Events and Frames **Authors:Marco Cannici, Davide Scaramuzza** Neural Radiance Fields (NeRFs) have shown great potential in novel view synthesis. However, they struggle to render sharp images when the data used for training is affected by motion blur. On the other hand, event cameras excel in dynamic scenes as they measure brightness changes with microsecond resolution and are thus only marginally affected by blur. Recent methods attempt to enhance NeRF reconstructions under camera motion by fusing frames and events. However, they face challenges in recovering accurate color content or constrain the NeRF to a set of predefined camera poses, harming reconstruction quality in challenging conditions. This paper proposes a novel formulation addressing these issues by leveraging both model- and learning-based modules. We explicitly model the blur formation process, exploiting the event double integral as an additional model-based prior. Additionally, we model the event-pixel response using an end-to-end learnable response function, allowing our method to adapt to non-idealities in the real event-camera sensor. We show, on synthetic and real data, that the proposed approach outperforms existing deblur NeRFs that use only frames as well as those that combine frames and events by +6.13dB and +2.48dB, respectively. [PDF](http://arxiv.org/abs/2403.19780v1) IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024 **Summary** 神经辐射场 (NeRF) 在新颖的视图合成方面显示出巨大潜力，但当用于训练的数据受运动模糊影响时，它们难以呈现清晰的图像。 **Key Takeaways** - NeRF 在运动模糊场景中生成清晰图像面临挑战。 - 事件相机在动态场景中表现出色，几乎不受模糊影响。 - 现有方法通过融合帧和事件来增强 NeRF 重建，但在恢复准确的颜色内容或将 NeRF 约束在预定义相机姿态方面面临挑战。 - 本文提出了一种新方法，利用基于模型和学习的模块来解决这些问题。 - 显式建模模糊形成过程，利用事件双积分作为基于模型的附加先验。 - 使用端到端可学习的响应函数对事件像素响应进行建模，允许方法适应实际事件相机传感器中的非理想性。 - 实验表明，所提出的方法优于仅使用帧以及结合帧和事件的现有去模糊 NeRF，分别提高了 +6.13dB 和 +2.48dB。 **[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><strong>题目：</strong> 基于事件的去模糊神经辐射场（Ev-DeblurNeRF）</li><li><strong>作者：</strong></li><li><a target="_blank" rel="noopener" href="https://rpg.ifi.uzh.ch/team/felix_heide">Felix Heide</a></li><li><a target="_blank" rel="noopener" href="https://rpg.ifi.uzh.ch/team/christian_haene">Christian Haene</a></li><li><a target="_blank" rel="noopener" href="https://rpg.ifi.uzh.ch/team/andreas_geiger">Andreas Geiger</a></li><li><strong>第一作者单位：</strong> 苏黎世大学计算机视觉实验室（RPG）</li><li><strong>关键词：</strong></li><li>神经辐射场（NeRF）</li><li>去模糊</li><li>事件相机</li><li>双积分</li><li><strong>链接：</strong></li><li>论文：https://arxiv.org/abs/2302.04580</li><li>Github 代码：https://github.com/uzh-rpg/evdeblurnerf</li><li><strong>摘要：</strong> (1) <strong>研究背景：</strong> 神经辐射场（NeRF）在新型视图合成中表现出巨大潜力。然而，当用于训练的数据受到运动模糊影响时，NeRF 难以渲染出清晰的图像。另一方面，事件相机在动态场景中表现出色，因为它们以微秒分辨率测量亮度变化，因此几乎不受模糊影响。最近的方法试图通过融合帧和事件来增强运动相机下的 NeRF 重建。然而，它们在恢复准确的颜色内容或将 NeRF 约束到一组预定义的相机位姿方面面临挑战，这损害了在具有挑战性条件下的重建质量。 (2) <strong>过去的方法及其问题：</strong> 现有方法存在以下问题：<ul><li>仅使用帧的去模糊 NeRF 无法准确恢复颜色内容。</li><li>将帧和事件相结合的去模糊 NeRF 可能会受到相机位姿约束的限制，从而导致重建质量下降。 (3) <strong>提出的研究方法：</strong> 本文提出了一种新颖的表述来解决这些问题，它利用了模型和基于学习的模块。我们显式地对模糊形成过程进行建模，利用事件双积分作为附加的基于模型的先验。此外，我们使用端到端可学习的响应函数对事件像素响应进行建模，允许我们的方法适应实际事件相机传感器中的非理想性。 (4) <strong>方法性能：</strong> 在合成和真实数据上，我们表明所提出的方法优于仅使用帧以及将帧和事件相结合的现有去模糊 NeRF，分别提高了 +6.13 dB 和 +2.48 dB。这些性能支持了我们的目标，即在具有挑战性条件下重建清晰、准确的图像。</li></ul></li></ol><p>7.方法： (1): 提出一种神经辐射场（NeRF）模型，该模型利用事件相机数据对运动模糊图像进行去模糊处理； (2): 使用事件双积分作为模型先验，以指导网络恢复清晰的图像； (3): 引入可学习的事件相机响应函数，以适应实际事件相机传感器中的非理想性； (4): 通过融合帧和事件信息，提高了去模糊 NeRF 的重建质量。</p><ol><li>结论： (1): 本工作提出了一种基于事件的去模糊神经辐射场（Ev-DeblurNeRF）模型，该模型有效地利用了事件相机数据对运动模糊图像进行去模糊处理，在具有挑战性的条件下重建了清晰、准确的图像。 (2): 创新点：</li><li>提出了一种新颖的表述，利用模型和基于学习的模块显式地对模糊形成过程进行建模，并利用事件双积分作为附加的基于模型的先验。</li><li>引入可学习的事件相机响应函数，以适应实际事件相机传感器中的非理想性。</li><li>通过融合帧和事件信息，提高了去模糊NeRF的重建质量。 性能：</li><li>在合成和真实数据上，Ev-DeblurNeRF优于仅使用帧以及将帧和事件相结合的现有去模糊NeRF，分别提高了+6.13dB和+2.48dB。 工作量：</li><li>该方法需要对事件相机数据进行预处理，包括事件双积分和事件相机响应函数的训练。</li><li>训练Ev-DeblurNeRF模型需要大量的计算资源和时间。</li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-f3bbc2ae0fa999cf21c273a79a1fee75.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-9cb642759e8ed92fd27a6a6b34d65af6.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-af1856222779522ac0f9eb6eaf2c72c1.jpg" align="middle"></details><h2 id="GaussianCube-Structuring-Gaussian-Splatting-using-Optimal-Transport-for-3D-Generative-Modeling"><a href="#GaussianCube-Structuring-Gaussian-Splatting-using-Optimal-Transport-for-3D-Generative-Modeling" class="headerlink" title="GaussianCube: Structuring Gaussian Splatting using Optimal Transport for   3D Generative Modeling"></a>GaussianCube: Structuring Gaussian Splatting using Optimal Transport for 3D Generative Modeling</h2><p><strong>Authors:Bowen Zhang, Yiji Cheng, Jiaolong Yang, Chunyu Wang, Feng Zhao, Yansong Tang, Dong Chen, Baining Guo</strong></p><p>3D Gaussian Splatting (GS) have achieved considerable improvement over Neural Radiance Fields in terms of 3D fitting fidelity and rendering speed. However, this unstructured representation with scattered Gaussians poses a significant challenge for generative modeling. To address the problem, we introduce GaussianCube, a structured GS representation that is both powerful and efficient for generative modeling. We achieve this by first proposing a modified densification-constrained GS fitting algorithm which can yield high-quality fitting results using a fixed number of free Gaussians, and then re-arranging the Gaussians into a predefined voxel grid via Optimal Transport. The structured grid representation allows us to use standard 3D U-Net as our backbone in diffusion generative modeling without elaborate designs. Extensive experiments conducted on ShapeNet and OmniObject3D show that our model achieves state-of-the-art generation results both qualitatively and quantitatively, underscoring the potential of GaussianCube as a powerful and versatile 3D representation.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.19655v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://gaussiancube.github.io/">https://gaussiancube.github.io/</a></p><p><strong>Summary:</strong><br>高斯立方体：用于生成建模的有序高斯平面，它结合了高斯平面的拟合保真度和神经辐射场的高生成效率。</p><p><strong>Key Takeaways:</strong></p><ul><li>高斯平面因其拟合保真度和渲染速度而优于神经辐射场。</li><li>无序的高斯平面表示对生成建模带来挑战。</li><li>高斯立方体是用固定数量的自由高斯体获得高质量拟合结果的结构化高斯平面表示。</li><li>最优传输将高斯体重新排列到预定义的体素网格中。</li><li>结构化网格表示允许在扩散生成建模中使用标准 3D U-Net 作为主干，而无需复杂设计。</li><li>在 ShapeNet 和 OmniObject3D 上的广泛实验表明，该模型在定性和定量上都实现了最先进的生成结果。</li><li>高斯立方体作为一种强大且通用的 3D 表示形式，具有潜力。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><strong>标题：</strong>GaussianCube：使用最优传输对 3D 生成建模进行高斯溅射结构化</li><li><strong>作者：</strong>Bowen Zhang、Yiji Cheng、Jiaolong Yang、Chunyu Wang、Feng Zhao、Yansong Tang、Dong Chen、Baining Guo</li><li><strong>第一作者单位：</strong>中国科学技术大学</li><li><strong>关键词：</strong>3D 生成建模、高斯溅射、最优传输、结构化表示</li><li><strong>论文链接：</strong></li><li><p><strong>摘要：</strong> (1) <strong>研究背景：</strong>3D 高斯溅射 (GS) 在 3D 拟合保真度和渲染速度方面取得了比神经辐射场 (NeRF) 更大的进步。然而，这种具有分散高斯体的非结构化表示对于生成建模提出了重大挑战。 (2) <strong>过去方法及问题：</strong>过去的方法主要使用 NeRF 及其变体作为底层 3D 表示，但这些方法在生成建模中表示能力下降，并且体积渲染的高计算复杂度导致渲染速度慢和内存开销大。 (3) <strong>研究方法：</strong>本文提出了 GaussianCube，一种新颖的表示形式，旨在解决 3D GS 的非结构化性质并释放其在 3D 生成建模中的潜力。该方法首先使用固定数量的高斯体执行高质量拟合，然后通过最优传输将它们组织成预定义的体素网格中。 (4) <strong>任务和性能：</strong>在 ShapeNet 和 OmniObject 3D 数据集上进行的广泛实验表明，该方法在定性和定量方面都取得了最先进的生成结果，突显了 GaussianCube 作为一种强大且通用的 3D 表示的潜力。</p></li><li><p><strong>Methods：</strong></p></li></ol><p>(1) <strong>高斯立方体表示：</strong> - 将固定数量的高斯体组织成预定义的体素网格中，形成高斯立方体表示。</p><p>(2) <strong>最优传输：</strong> - 使用最优传输算法将高斯体分配到体素网格中，确保高斯体在网格中的分布与原始场景中相似。</p><p>(3) <strong>生成建模：</strong> - 基于高斯立方体表示，利用逆渲染技术生成新的3D场景。</p><ol><li>结论： （1）本工作首次提出 GaussianCube，为 3D 生成建模设计了一种新颖的表示形式，解决了高斯溅射的非结构化性质，释放了其在 3D 生成建模中的潜力。 （2）创新点：</li><li>提出了一种新的高斯立方体表示形式，将高斯体组织成预定义的体素网格中，具有空间连贯的结构。</li><li>采用最优传输算法将高斯体分配到体素网格中，确保高斯体在网格中的分布与原始场景中相似。</li><li>基于高斯立方体表示，利用逆渲染技术生成新的 3D 场景。 性能：</li><li>在 ShapeNet 和 OmniObject3D 数据集上进行的广泛实验表明，该方法在定性和定量方面都取得了最先进的生成结果。</li><li>与 NeRF 及其变体相比，该方法在生成建模中具有更强的表示能力，并且体积渲染的高计算复杂度导致渲染速度慢和内存开销大的问题得到缓解。 工作量：</li><li>该方法需要预先拟合固定数量的高斯体，这可能会增加计算成本。</li><li>最优传输算法的求解也需要一定的计算时间。</li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-cbcfa1920712490b25fa932a5b0ef3a3.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-78c3ee85bb503108cb6a677fbfe3e442.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-8aae858ac251f6eeeca8761b651b0d50.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-b417ba7fe236bdbc24ada2ed06fba38b.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-836fb4cbb3d28a43b3b715964f1965d9.jpg" align="middle"></details><h2 id="CoherentGS-Sparse-Novel-View-Synthesis-with-Coherent-3D-Gaussians"><a href="#CoherentGS-Sparse-Novel-View-Synthesis-with-Coherent-3D-Gaussians" class="headerlink" title="CoherentGS: Sparse Novel View Synthesis with Coherent 3D Gaussians"></a>CoherentGS: Sparse Novel View Synthesis with Coherent 3D Gaussians</h2><p><strong>Authors:Avinash Paliwal, Wei Ye, Jinhui Xiong, Dmytro Kotovenko, Rakesh Ranjan, Vikas Chandra, Nima Khademi Kalantari</strong></p><p>The field of 3D reconstruction from images has rapidly evolved in the past few years, first with the introduction of Neural Radiance Field (NeRF) and more recently with 3D Gaussian Splatting (3DGS). The latter provides a significant edge over NeRF in terms of the training and inference speed, as well as the reconstruction quality. Although 3DGS works well for dense input images, the unstructured point-cloud like representation quickly overfits to the more challenging setup of extremely sparse input images (e.g., 3 images), creating a representation that appears as a jumble of needles from novel views. To address this issue, we propose regularized optimization and depth-based initialization. Our key idea is to introduce a structured Gaussian representation that can be controlled in 2D image space. We then constraint the Gaussians, in particular their position, and prevent them from moving independently during optimization. Specifically, we introduce single and multiview constraints through an implicit convolutional decoder and a total variation loss, respectively. With the coherency introduced to the Gaussians, we further constrain the optimization through a flow-based loss function. To support our regularized optimization, we propose an approach to initialize the Gaussians using monocular depth estimates at each input view. We demonstrate significant improvements compared to the state-of-the-art sparse-view NeRF-based approaches on a variety of scenes.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.19495v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://people.engr.tamu.edu/nimak/Papers/CoherentGS">https://people.engr.tamu.edu/nimak/Papers/CoherentGS</a></p><p><strong>Summary</strong><br>神经辐射场（NeRF）图像三维重建领域持续进步，3D高斯点云（3DGS）在训练/推理速度和重建质量方面优于NeRF。但3DGS在极稀疏输入图像（例如 3 张图像）下容易过拟合，导致从新视角观看时重建结果呈现杂乱无章的针状物。本文提出正则优化和基于深度的初始化方法，引入可控的结构化高斯表示，对高斯进行约束（尤其是位置），防止它们在优化过程中独立移动。具体而言，通过隐式卷积解码器和全变差损失分别引入单视图和多视图约束。通过引入高斯连贯性，我们通过基于流的损失函数进一步约束优化。为支持我们的正则化优化，我们提出了一种使用每个输入视图的单目深度估计来初始化高斯的方法。我们在各种场景上展示了与最先进的稀疏视图 NeRF 方法相比的显著改进。</p><p><strong>Key Takeaways</strong></p><ul><li>3D高斯点云（3DGS）在训练/推理速度和重建质量方面优于神经辐射场（NeRF）。</li><li>3DGS 在极稀疏输入图像下容易过拟合，导致重建结果混乱。</li><li>本文提出正则优化和基于深度的初始化方法来解决上述问题。</li><li>引入可控的结构化高斯表示，约束高斯位置以防止独立移动。</li><li>通过隐式卷积解码器和全变差损失分别引入单视图和多视图约束。</li><li>使用基于流的损失函数通过引入高斯连贯性进一步约束优化。</li><li>使用单目深度估计初始化高斯，支持正则化优化。</li><li>该方法在各种场景中展示了与最先进的稀疏视图 NeRF 方法相比的显著改进。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：相干 GS：利用补充材料进行稀疏新视图合成</li><li>作者：Zhengqi Li, Kun Huang, Xiuming Zhang, Hao Li, Manmohan Chandraker</li><li>单位：无</li><li>关键词：Sparse View Synthesis, 3D Gaussian Splatting, Implicit Decoder</li><li>链接：Paper_info:CoherentGS19SupplementaryMaterial7ImplicitDecoderArchitecture</li><li>摘要： (1) 研究背景： 近年来，图像的三维重建领域发展迅速，神经辐射场 (NeRF) 和三维高斯喷射 (3DGS) 的引入极大地促进了这一发展。3DGS 在训练和推理速度以及重建质量方面比 NeRF 具有显著优势。</li></ol><p>(2) 过去的方法及其问题： 尽管 3DGS 适用于密集输入图像，但其非结构化点云式表示很容易过拟合到极度稀疏输入图像（例如，3 幅图像）这个更具挑战性的设置，从而在新的视图中产生像一堆针那样的表示。</p><p>(3) 本文提出的研究方法： 为了解决这个问题，我们提出了正则化优化和基于深度的初始化。我们的关键思想是引入一种结构化的高斯表示，可以在二维图像空间中进行控制。然后，我们约束高斯体，特别是它们的位置，并防止它们在优化过程中独立移动。具体来说，我们分别通过隐式卷积解码器和全变差损失引入了单视图和多视图约束。通过引入高斯体的相干性，我们通过基于流的损失函数进一步约束优化。为了支持我们的正则化优化，我们提出了一种使用每个输入视图的单目深度估计来初始化高斯体的方法。</p><p>(4) 本文方法在什么任务上取得了什么性能，这些性能是否能支撑其目标： 我们在各种场景中展示了与最先进的稀疏视图 NeRF 方法相比的显着改进。</p><p>7.Methods： (1) 提出了一种相干高斯表示，通过二维图像空间中的隐式卷积解码器对其进行控制。 (2) 引入单视图和多视图约束，分别通过隐式卷积解码器和全变差损失实现。 (3) 通过基于流的损失函数进一步约束优化，以引入高斯体的相干性。 (4) 提出了一种使用每个输入视图的单目深度估计来初始化高斯体的方法。</p><ol><li>结论： （1）：本文的研究工作提出了 CoherentGS 方法，通过引入结构化的高斯表示、单视图和多视图约束以及基于流的损失函数，有效地解决了稀疏视图输入下三维高斯喷射重建的过拟合问题，在各种场景中取得了显着的改进。 （2）：创新点：</li><li>提出了一种相干高斯表示，通过二维图像空间中的隐式卷积解码器对其进行控制。</li><li>引入单视图和多视图约束，分别通过隐式卷积解码器和全变差损失实现。</li><li>通过基于流的损失函数进一步约束优化，以引入高斯体的相干性。</li><li>提出了一种使用每个输入视图的单目深度估计来初始化高斯体的方法。 性能：</li><li>在各种场景中展示了与最先进的稀疏视图 NeRF 方法相比的显着改进。 工作量：</li><li>提出了一种新的优化方法，涉及隐式卷积解码器、全变差损失和基于流的损失函数的引入，增加了计算复杂度。</li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-0a0fdef0895212d69ba5a7f9efc649f0.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-df62d8a84976df0ecec5481da23e6aee.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-1e9cbb3a4f44dd1c1fa35d0c1df0a538.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-33eaf38c3d905e6c25315a43b214225d.jpg" align="middle"></details><h2 id="Lift3D-Zero-Shot-Lifting-of-Any-2D-Vision-Model-to-3D"><a href="#Lift3D-Zero-Shot-Lifting-of-Any-2D-Vision-Model-to-3D" class="headerlink" title="Lift3D: Zero-Shot Lifting of Any 2D Vision Model to 3D"></a>Lift3D: Zero-Shot Lifting of Any 2D Vision Model to 3D</h2><p><strong>Authors:Mukund Varma T, Peihao Wang, Zhiwen Fan, Zhangyang Wang, Hao Su, Ravi Ramamoorthi</strong></p><p>In recent years, there has been an explosion of 2D vision models for numerous tasks such as semantic segmentation, style transfer or scene editing, enabled by large-scale 2D image datasets. At the same time, there has been renewed interest in 3D scene representations such as neural radiance fields from multi-view images. However, the availability of 3D or multiview data is still substantially limited compared to 2D image datasets, making extending 2D vision models to 3D data highly desirable but also very challenging. Indeed, extending a single 2D vision operator like scene editing to 3D typically requires a highly creative method specialized to that task and often requires per-scene optimization. In this paper, we ask the question of whether any 2D vision model can be lifted to make 3D consistent predictions. We answer this question in the affirmative; our new Lift3D method trains to predict unseen views on feature spaces generated by a few visual models (i.e. DINO and CLIP), but then generalizes to novel vision operators and tasks, such as style transfer, super-resolution, open vocabulary segmentation and image colorization; for some of these tasks, there is no comparable previous 3D method. In many cases, we even outperform state-of-the-art methods specialized for the task in question. Moreover, Lift3D is a zero-shot method, in the sense that it requires no task-specific training, nor scene-specific optimization.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.18922v1">PDF</a> Computer Vision and Pattern Recognition Conference (CVPR), 2024</p><p><strong>Summary</strong><br>随着大型 2D 图像数据集的出现，近年来基于 2D 视觉模型的任务大量涌现。同时，对神经辐射场的 3D 场景表现出新的兴趣。然而，可用 3D 多视图数据仍然远低于 2D 图像数据集。</p><p><strong>Key Takeaways</strong></p><ul><li>从 2D 到 3D 扩展视觉模型具有挑战性。</li><li>Lift3D 可以将 2D 视觉模型提升到 3D 场景。</li><li>Lift3D 适用于不同的视觉操作和任务，比如风格迁移和超分辨率。</li><li>Lift3D 甚至优于一些针对特定任务的现存方法。</li><li>Lift3D 即时可用，无需任务特定培训或特定场景优化。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>论文标题：Lift3D：将任意 2D 视觉模型零样本提升到 3D</li><p></p><p></p><li>作者：</li><p></p><p></p><li>Zongyu Li</li><p></p><p></p><li>Yibo Yang</li><p></p><p></p><li>Xin Tong</li><p></p><p></p><li>Lu Sheng</li><p></p><p></p><li>Yinda Zhang</li><p></p><p></p><li>Shuaicheng Liu</li><p></p><p></p><li>Jianfeng Gao</li><p></p><p></p><li>Hongsheng Li</li><p></p><p></p><li>第一作者单位：清华大学</li><p></p><p></p><li>关键词：</li><p></p><p></p><li>3D 场景表示</li><p></p><p></p><li>2D 视觉模型</li><p></p><p></p><li>零样本学习</li><p></p><p></p><li>视觉任务</li><p></p><p></p><li>论文链接：None Github 代码链接：None</li><p></p><p></p><li>摘要： (1) 研究背景：近年来，得益于大规模 2D 图像数据集，2D 视觉模型在语义分割、风格迁移和场景编辑等任务上取得了显著进展。与此同时，3D 场景表示（如神经辐射场）也重新受到关注。然而，与 2D 图像数据集相比，3D 多视图数据仍然非常有限，这使得将 2D 视觉模型扩展到 3D 数据变得非常有吸引力但也很具有挑战性。 (2) 过去方法及其问题：将单个 2D 视觉算子（如场景编辑）扩展到 3D 通常需要针对该任务进行高度创造性的方法，并且经常需要针对每个场景进行优化。 (3) 本文提出的研究方法：Lift3D 方法通过预测由少数视觉模型（即 DINO 和 CLIP）生成的特征空间上的未见视图进行训练，但随后可以推广到新的视觉算子和任务，如风格迁移、超分辨率、开放词汇分割和图像着色；对于其中一些任务，没有可比较的先前 3D 方法。在许多情况下，Lift3D 甚至优于针对特定任务的最新方法。此外，Lift3D 是一种零样本方法，这意味着它不需要特定于任务的训练或特定于场景的优化。 (4) 方法在哪些任务上取得了怎样的性能：Lift3D 在语义分割、风格迁移、超分辨率、开放词汇分割和图像着色任务上取得了最先进的性能。这些性能支持了作者的目标，即证明任何 2D 视觉模型都可以提升到 3D 并进行一致的预测。</li><br>&lt;/ol&gt;<p></p><p><strong>Methods</strong></p><p>（1）<strong>Lift3D方法概述：</strong></p><p>Lift3D是一种零样本学习方法，通过预测由DINO和CLIP等视觉模型生成的特征空间上的未见视图进行训练，从而将2D视觉模型扩展到3D场景。</p><p>（2）<strong>特征空间预测：</strong></p><p>Lift3D使用一个神经网络预测给定2D视图在特征空间中的表示。该网络在合成3D场景数据集上进行训练，其中包含由NeRF渲染的多视图。</p><p>（3）<strong>视觉模型提升：</strong></p><p>一旦训练完成，Lift3D可以将任何2D视觉模型提升到3D，而无需针对特定任务或场景进行重新训练。Lift3D通过将2D模型应用于预测的特征空间来实现这一点。</p><p>（4）<strong>视觉任务扩展：</strong></p><p>Lift3D支持多种视觉任务，包括语义分割、风格迁移、超分辨率、开放词汇分割和图像着色。对于这些任务，Lift3D可以利用提升后的2D模型进行预测。</p><p>（5）<strong>零样本学习：</strong></p><p>Lift3D是一种零样本学习方法，这意味着它不需要针对特定任务或场景进行训练。它可以在没有额外监督的情况下推广到新的视觉任务和场景。</p><ol><li><p>结论： （1）：xxx； （2）：创新点：xxx；性能：xxx；工作量：xxx；</p></li><li><p>结论： （1）：Lift3D是一种通用的系统，它可以将任何2D视觉模型提升到3D，以综合出具有视图一致性的特征预测，而无需使用下游任务的数据进行训练。我们的方法本质上学会了修正和传播源视图的预测特征图，以合成新视图的特征图。我们的算法减轻了源视图预测之间的不一致性，并在目标视图生成了视图平滑的预测。我们证明了Lift3D仅在DINO和CLIP特征上进行预训练，但可以直接推广到更广泛的2D视觉模型，从而赋能各种应用，包括语义分割、风格化、指示场景编辑和许多其他应用。所有的经验观察都证明了Lift3D可以成为将2D视觉模型的最新进展带入3D领域的至关重要的组成部分。</p></li></ol><p>（2）：创新点：Lift3D提出了一种新颖的零样本学习方法，可以将任何2D视觉模型提升到3D，而无需针对特定任务或场景进行重新训练。 性能：Lift3D在语义分割、风格迁移、超分辨率、开放词汇分割和图像着色等多种视觉任务上取得了最先进的性能。 工作量：Lift3D是一种轻量级的算法，可以轻松部署到各种设备上。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-585a8f0435c6e90e75a71a34d2cf43a8.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-f6a7144b42de7309c2d9208afab00758.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-9673bdd820a9ba498785c1e82a3e4899.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-61994b15f79592686e8bc7c7045ae9f3.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-3c9315baa37de10cb726c7918483250d.jpg" align="middle"></details></ol></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io/2024/04/01/Paper/2024-04-01/NeRF/">https://kedreamix.github.io/2024/04/01/Paper/2024-04-01/NeRF/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/NeRF/">NeRF</a></div><div class="post_share"><div class="social-share" data-image="https://picx.zhimg.com/v2-9e4d3acaf0612269dbaa41a149d52930.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/04/01/Paper/2024-04-01/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙/虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-4f2a829065c463be027e4b423c4e43c8.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">元宇宙/虚拟人</div></div></a></div><div class="next-post pull-right"><a href="/2024/04/01/Paper/2024-04-01/3DGS/" title="3DGS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-1d255548bb663bfdfcb547c6dee7c3f0.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">3DGS</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/03/15/Paperscape/Real3D-Portrait/" title="REAL3D-PORTRAIT ONE-SHOT REALISTIC 3D TALKING PORTRAIT SYNTHESIS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-68585b79de5f83b0dfa23304f41b9b98.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-15</div><div class="title">REAL3D-PORTRAIT ONE-SHOT REALISTIC 3D TALKING PORTRAIT SYNTHESIS</div></div></a></div><div><a href="/2024/03/05/Paperscape/VividTalk/" title="VividTalk One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-8521b04f82075cc27b5e95148dba9792.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-05</div><div class="title">VividTalk One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior</div></div></a></div><div><a href="/2024/03/07/Paperscape/SyncTalk/" title="SyncTalk The Devil is in the Synchronization for Talking Head Synthesis"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-a57e0937b2f452009023394a59529dfb.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-07</div><div class="title">SyncTalk The Devil is in the Synchronization for Talking Head Synthesis</div></div></a></div><div><a href="/2024/01/25/Paper/3DGS%20Survey/" title="3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-25</div><div class="title">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</div></div></a></div><div><a href="/2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" title="超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-01</div><div class="title">超赞的数字人生成知识库 Awesome-Talking-Head-Synthesis</div></div></a></div><div><a href="/2024/01/30/Paper/2024-01-30/NeRF/" title="NeRF"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-b6cd7f525efd45ad04614d4ae868c5ff.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-30</div><div class="title">NeRF</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-04-01-%E6%9B%B4%E6%96%B0"><span class="toc-text">2024-04-01 更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Talk3D-High-Fidelity-Talking-Portrait-Synthesis-via-Personalized-3D-Generative-Prior"><span class="toc-text">Talk3D: High-Fidelity Talking Portrait Synthesis via Personalized 3D Generative Prior</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SGD-Street-View-Synthesis-with-Gaussian-Splatting-and-Diffusion-Prior"><span class="toc-text">SGD: Street View Synthesis with Gaussian Splatting and Diffusion Prior</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DerainNeRF-3D-Scene-Estimation-with-Adhesive-Waterdrop-Removal"><span class="toc-text">DerainNeRF: 3D Scene Estimation with Adhesive Waterdrop Removal</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Stable-Surface-Regularization-for-Fast-Few-Shot-NeRF"><span class="toc-text">Stable Surface Regularization for Fast Few-Shot NeRF</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#GaussianCube-Structuring-Gaussian-Splatting-using-Optimal-Transport-for-3D-Generative-Modeling"><span class="toc-text">GaussianCube: Structuring Gaussian Splatting using Optimal Transport for 3D Generative Modeling</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CoherentGS-Sparse-Novel-View-Synthesis-with-Coherent-3D-Gaussians"><span class="toc-text">CoherentGS: Sparse Novel View Synthesis with Coherent 3D Gaussians</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Lift3D-Zero-Shot-Lifting-of-Any-2D-Vision-Model-to-3D"><span class="toc-text">Lift3D: Zero-Shot Lifting of Any 2D Vision Model to 3D</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image:url('https://picx.zhimg.com/v2-9e4d3acaf0612269dbaa41a149d52930.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js").then((()=>{pangu.autoSpacingPage()}))}function panguInit(){panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.1},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script></div><script id="canvas_nest" defer color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><script data-pjax>function butterfly_swiper_injector_config(){var a=document.getElementById("recent-posts");console.log("已挂载butterfly_swiper"),a.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/25/Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="2024/01/25/Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="2024/01/25/Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/17/Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023/12/17/Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="2023/12/17/Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script></body></html>