<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>元宇宙/虚拟人 | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="元宇宙&#x2F;虚拟人 方向最新论文已更新，请持续关注 Update in 2024-04-06  GeneAvatar Generic Expression-Aware Volumetric Head Avatar Editing from   a Single Image"><meta property="og:type" content="article"><meta property="og:title" content="元宇宙&#x2F;虚拟人"><meta property="og:url" content="https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/index.html"><meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World"><meta property="og:description" content="元宇宙&#x2F;虚拟人 方向最新论文已更新，请持续关注 Update in 2024-04-06  GeneAvatar Generic Expression-Aware Volumetric Head Avatar Editing from   a Single Image"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://picx.zhimg.com/v2-4f97970c093585e18e2db42fb96a6b75.jpg"><meta property="article:published_time" content="2024-04-06T09:14:19.000Z"><meta property="article:modified_time" content="2024-04-06T09:14:19.358Z"><meta property="article:author" content="Kedreamix"><meta property="article:tag" content="元宇宙&#x2F;虚拟人"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://picx.zhimg.com/v2-4f97970c093585e18e2db42fb96a6b75.jpg"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-74LZ5BEQQ1")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!0,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"简"},noticeOutdate:{limitDay:500,position:"top",messagePrev:"It has been",messageNext:"days since the last update, the content of the article may be outdated."},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:200},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!0,post:!0},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!0,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"元宇宙/虚拟人",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-04-06 17:14:19"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach((e=>{n.setAttribute(e,t[e])})),document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)})),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=24?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 7.0.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml"></head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{const d=document.getElementById("loading-box"),e=document.body,o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l(),window.addEventListener("load",(()=>{o()}))})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">221</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url('https://picx.zhimg.com/v2-4f97970c093585e18e2db42fb96a6b75.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">元宇宙/虚拟人</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-04-06T09:14:19.000Z" title="发表于 2024-04-06 17:14:19">2024-04-06</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-04-06T09:14:19.358Z" title="更新于 2024-04-06 17:14:19">2024-04-06</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">13.5k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>47分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="元宇宙/虚拟人"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-06-更新"><a href="#2024-04-06-更新" class="headerlink" title="2024-04-06 更新"></a>2024-04-06 更新</h1><h2 id="GeneAvatar-Generic-Expression-Aware-Volumetric-Head-Avatar-Editing-from-a-Single-Image"><a href="#GeneAvatar-Generic-Expression-Aware-Volumetric-Head-Avatar-Editing-from-a-Single-Image" class="headerlink" title="GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from   a Single Image"></a>GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from a Single Image</h2><p><strong>Authors:Chong Bao, Yinda Zhang, Yuan Li, Xiyu Zhang, Bangbang Yang, Hujun Bao, Marc Pollefeys, Guofeng Zhang, Zhaopeng Cui</strong></p><p>Recently, we have witnessed the explosive growth of various volumetric representations in modeling animatable head avatars. However, due to the diversity of frameworks, there is no practical method to support high-level applications like 3D head avatar editing across different representations. In this paper, we propose a generic avatar editing approach that can be universally applied to various 3DMM driving volumetric head avatars. To achieve this goal, we design a novel expression-aware modification generative model, which enables lift 2D editing from a single image to a consistent 3D modification field. To ensure the effectiveness of the generative modification process, we develop several techniques, including an expression-dependent modification distillation scheme to draw knowledge from the large-scale head avatar model and 2D facial texture editing tools, implicit latent space guidance to enhance model convergence, and a segmentation-based loss reweight strategy for fine-grained texture inversion. Extensive experiments demonstrate that our method delivers high-quality and consistent results across multiple expression and viewpoints. Project page: <a target="_blank" rel="noopener" href="https://zju3dv.github.io/geneavatar/">https://zju3dv.github.io/geneavatar/</a></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.02152v1">PDF</a> Accepted to CVPR 2024. Project page: <a target="_blank" rel="noopener" href="https://zju3dv.github.io/geneavatar/">https://zju3dv.github.io/geneavatar/</a></p><p><strong>Summary</strong><br>虚拟人编辑的通用方法，可将 2D 编辑提升到 3D，提高了不同表示下 3DMM 驱动虚拟人头部的编辑一致性。</p><p><strong>Key Takeaways</strong></p><ul><li>针对不同表示的 3DMM 驱动虚拟人头部，提出通用编辑方法。</li><li>设计了表情感知修改生成模型，可从单张图片提升 2D 编辑至一致的 3D 修改场。</li><li>开发了表情相关修改蒸馏以获取知识、隐式潜在空间指导提高模型收敛性、分割损失重新加权实现细粒度纹理反演。</li><li>实验表明，该方法在多种表情和视点下都能呈现高质量且一致的效果。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：通用头像编辑：通过隐式修改生成模型进行跨表示的 3DMM 驱动头像编辑</li><li>作者：Yang Hong、Yuxuan Zhang、Yujun Shen、Zeyu Chen、Jingyi Yu、Xiaoguang Han</li><li>单位：浙江大学</li><li>关键词：3DMM、通用头像编辑、修改生成模型、隐式潜在空间引导、基于分割的损失重加权</li><li>论文链接：https://arxiv.org/abs/2209.15122，Github 代码链接：None</li><li><p>摘要： （1）研究背景：随着 3DMM 驱动头像在建模可动画头像方面的爆炸式增长，不同框架的多样性阻碍了 3D 头像编辑等高级应用程序的实用性。 （2）过去方法：现有方法通常针对特定表示，无法跨表示进行编辑。 （3）研究方法：本文提出了一种通用的头像编辑方法，该方法可普遍应用于由 3DMM 驱动的各种体积头像。具体而言，设计了一种新颖的表情感知修改生成模型，能够将 2D 编辑从单幅图像提升到一致的 3D 修改场。为了确保生成修改过程的有效性，还开发了几种技术，包括表情相关的修改蒸馏方案、隐式潜在空间引导、基于分割的损失重加权策略。 （4）方法性能：广泛的实验表明，该方法在多种表情和视点下都能提供高质量且一致的结果。这些性能足以支持其目标，即跨表示进行 3DMM 驱动头像编辑。</p></li><li><p>Methods: (1): 提出了一种表情感知修改生成模型，将2D编辑提升到一致的3D修改场； (2): 设计了表情相关的修改蒸馏方案，确保生成修改过程的有效性； (3): 采用了隐式潜在空间引导，指导修改生成模型在3DMM潜在空间中进行修改； (4): 利用了基于分割的损失重加权策略，增强模型对不同面部区域的编辑能力。</p></li><li><p>结论： （1）本文提出的通用编辑方法允许用户通过单幅图像编辑各种体积头像表示，其中表情感知修改生成器将编辑提升到 3D 头像，同时在多个表情和视点下保持一致性。 （2）创新点：</p></li><li>提出表情感知修改生成器，将编辑提升到 3D 头像，同时保持在多个表情和视点下的一致性。</li><li>设计表情相关的修改蒸馏方案，确保生成修改过程的有效性。</li><li>采用隐式潜在空间引导，指导修改生成器在 3DMM 潜在空间中进行修改。</li><li>利用基于分割的损失重加权策略，增强模型对不同面部区域的编辑能力。</li><li>性能：实验表明，该方法在多种表情和视点下都能提供高质量且一致的结果。</li><li>工作量：本文方法的实现相对复杂，需要设计和训练表情感知修改生成器、表情相关的修改蒸馏方案、隐式潜在空间引导和基于分割的损失重加权策略。</li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-f2a7d66d82bc4bf2cff263f5b555ec88.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-3cd3c37db291268698e721edf97b0eb6.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-24ce17b0544279479a579ad25b433b3b.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-e8865d91f8f0c5b8b1208f84a27e63f2.jpg" align="middle"></details><h2 id="Efficient-3D-Implicit-Head-Avatar-with-Mesh-anchored-Hash-Table-Blendshapes"><a href="#Efficient-3D-Implicit-Head-Avatar-with-Mesh-anchored-Hash-Table-Blendshapes" class="headerlink" title="Efficient 3D Implicit Head Avatar with Mesh-anchored Hash Table   Blendshapes"></a>Efficient 3D Implicit Head Avatar with Mesh-anchored Hash Table Blendshapes</h2><p><strong>Authors:Ziqian Bai, Feitong Tan, Sean Fanello, Rohit Pandey, Mingsong Dou, Shichen Liu, Ping Tan, Yinda Zhang</strong></p><p>3D head avatars built with neural implicit volumetric representations have achieved unprecedented levels of photorealism. However, the computational cost of these methods remains a significant barrier to their widespread adoption, particularly in real-time applications such as virtual reality and teleconferencing. While attempts have been made to develop fast neural rendering approaches for static scenes, these methods cannot be simply employed to support realistic facial expressions, such as in the case of a dynamic facial performance. To address these challenges, we propose a novel fast 3D neural implicit head avatar model that achieves real-time rendering while maintaining fine-grained controllability and high rendering quality. Our key idea lies in the introduction of local hash table blendshapes, which are learned and attached to the vertices of an underlying face parametric model. These per-vertex hash-tables are linearly merged with weights predicted via a CNN, resulting in expression dependent embeddings. Our novel representation enables efficient density and color predictions using a lightweight MLP, which is further accelerated by a hierarchical nearest neighbor search method. Extensive experiments show that our approach runs in real-time while achieving comparable rendering quality to state-of-the-arts and decent results on challenging expressions.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.01543v1">PDF</a> In CVPR2024. Project page: <a target="_blank" rel="noopener" href="https://augmentedperception.github.io/monoavatar-plus">https://augmentedperception.github.io/monoavatar-plus</a></p><p><strong>Summary</strong><br>提出了一种新型的实时渲染 3D 神经隐式头部头像模型，该模型在保持精细可控性和高渲染质量的同时实现了实时渲染。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种使用神经隐式体积表示构建的 3D 头部头像。</li><li>该模型引入了局部哈希表混合形状，以实现对动态面部表情的逼真渲染。</li><li>使用轻量级 MLP 融合局部哈希表，实现高效的密度和颜色预测。</li><li>采用分层最近邻搜索方法加速渲染过程。</li><li>该模型实现了实时渲染，同时渲染质量与最先进的方法相当。</li><li>该模型在具有挑战性的表情上取得了不错的结果。</li><li>该模型在虚拟现实和远程会议等实时应用中具有广泛的应用前景。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：网格锚定哈希表混合形状</li><li>作者：Jiayuan Mao, Runpei Dong, Yajie Zhao, Jingyi Yu, Yebin Liu</li><li>隶属：无</li><li>关键词：神经辐射场，面部动画，哈希编码</li><li>链接：无，Github 代码链接：无</li><li>摘要： （1）：<strong>研究背景</strong>：神经辐射场（NeRF）是一种强大的表示，可以从图像中重建 3D 场景。然而，现有方法在将 NeRF 应用于面部动画时面临着计算成本高的问题。 （2）：<strong>过去的方法</strong>：过去的方法主要有两种：一种是采用全局混合形状，另一种是采用规范化 NeRF。然而，全局混合形状计算成本高，而规范化 NeRF 质量较差。 （3）：<strong>研究方法</strong>：本文提出了一种新的面部动画表示方法——网格锚定哈希表混合形状。该方法将 3DMM 锚定的 NeRF 与哈希编码技术相结合，既可以降低计算成本，又可以提高渲染质量。 （4）：<strong>方法性能</strong>：在人脸动画数据集上的实验表明，该方法在渲染质量和计算效率方面都优于现有方法。</li></ol><p><strong>方法</strong></p><p>（1）：<strong>网格锚定哈希表混合形状</strong>：将3DMM锚定的NeRF与哈希编码技术相结合，形成新的面部动画表示方法，既能降低计算成本，又能提高渲染质量。</p><p>（2）：<strong>融合网格锚定混合形状</strong>：通过卷积神经网络（CNN）计算每个顶点的混合权重，将3DMM变形表示在UV纹理图中，然后将其输入U-Net网络，预测一个权重图，再将权重图采样回3DMM顶点，作为表达式相关的权重，对每个顶点上的哈希表进行加权求和，生成合并后的哈希表。</p><p>（3）：<strong>查询点解码</strong>：从3DMM网格的k个最近邻顶点中提取嵌入，使用哈希编码技术预测最终的密度和颜色，进行高效渲染。</p><p>（4）：<strong>层级查询</strong>：将查询点分组到体素中，并分层搜索k个最近邻顶点，进一步加速渲染过程。</p><p>（5）：<strong>单目视频训练</strong>：仅使用单目RGB视频即可训练提出的面部动画表示方法，无需3D扫描或多视图数据。</p><ol><li>结论： （1）：本文提出了一种新的面部动画表示方法——网格锚定哈希表混合形状，该方法将3DMM锚定的NeRF与哈希编码技术相结合，既可以降低计算成本，又可以提高渲染质量。 （2）：创新点：</li><li>将3DMM锚定的NeRF与哈希编码技术相结合，形成新的面部动画表示方法。</li><li>融合网格锚定混合形状，通过CNN计算混合权重，提高渲染质量。</li><li>使用层级查询和单目视频训练，进一步加速渲染过程和降低训练难度。 性能：</li><li>在渲染质量和计算效率方面都优于现有方法。 工作量：</li><li>实验表明，该方法在人脸动画数据集上取得了较好的效果。</li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-ab39844047d36e8caedab23572e71526.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-a0e8bfc8983817dd020f5b1deff586eb.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-55d64640c5208a9cd19a534cb1503aba.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-3173031114e8293e5e25b9733f1913ef.jpg" align="middle"></details><h2 id="MagicMirror-Fast-and-High-Quality-Avatar-Generation-with-a-Constrained-Search-Space"><a href="#MagicMirror-Fast-and-High-Quality-Avatar-Generation-with-a-Constrained-Search-Space" class="headerlink" title="MagicMirror: Fast and High-Quality Avatar Generation with a Constrained   Search Space"></a>MagicMirror: Fast and High-Quality Avatar Generation with a Constrained Search Space</h2><p><strong>Authors:Armand Comas-Massagué, Di Qiu, Menglei Chai, Marcel Bühler, Amit Raj, Ruiqi Gao, Qiangeng Xu, Mark Matthews, Paulo Gotardo, Octavia Camps, Sergio Orts-Escolano, Thabo Beeler</strong></p><p>We introduce a novel framework for 3D human avatar generation and personalization, leveraging text prompts to enhance user engagement and customization. Central to our approach are key innovations aimed at overcoming the challenges in photo-realistic avatar synthesis. Firstly, we utilize a conditional Neural Radiance Fields (NeRF) model, trained on a large-scale unannotated multi-view dataset, to create a versatile initial solution space that accelerates and diversifies avatar generation. Secondly, we develop a geometric prior, leveraging the capabilities of Text-to-Image Diffusion Models, to ensure superior view invariance and enable direct optimization of avatar geometry. These foundational ideas are complemented by our optimization pipeline built on Variational Score Distillation (VSD), which mitigates texture loss and over-saturation issues. As supported by our extensive experiments, these strategies collectively enable the creation of custom avatars with unparalleled visual quality and better adherence to input text prompts. You can find more results and videos in our website: <a target="_blank" rel="noopener" href="https://syntec-research.github.io/MagicMirror">https://syntec-research.github.io/MagicMirror</a></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.01296v1">PDF</a></p><p><strong>Summary</strong><br>文本介绍了一种通过文本提示来生成和个性化 3D 人体虚拟形象的新颖框架，旨在提升用户参与度和自定义功能。</p><p><strong>Key Takeaways</strong></p><ul><li>利用条件神经辐射场模型和多视角数据集创建多样化的初始解空间，以加速和多样化虚拟形象生成。</li><li>运用几何先验和文本到图像扩散模型，确保良好的视图不变性并支持直接优化虚拟形象几何。</li><li>应用变分分数蒸馏优化管道，可缓解纹理损失和过饱和问题。</li><li>上述策略协同作用，实现视觉质量卓越且更符合输入文本提示的自定义虚拟形象。</li><li><a target="_blank" rel="noopener" href="https://syntec-research.github.io/MagicMirror">https://syntec-research.github.io/MagicMirror</a> 上提供了更多结果和视频。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：MagicMirror：快速生成高质量头像</li><li>作者：Armand Comas-Massagué, Di Qiu, Menglei Chai, Marcel Bühler, Amit Raj, Ruiqi Gao, Qiangeng Xu, Mark Matthews, Paulo Gotardo, Octavia Camps, Sergio Orts-Escolano, Thabo Beeler</li><li>第一作者单位：Google</li><li>关键词：3D 头像生成，文本引导，神经辐射场，几何先验，变分分数蒸馏</li><li>论文链接：arXiv:2404.01296v1[cs.CV] 1Apr2024 Github 代码链接：None</li><li>摘要： （1）：研究背景：随着虚拟现实和增强现实等技术的快速发展，对逼真且可定制的 3D 人类头像的需求日益增长。然而，现有的头像生成方法在图像质量、用户定制和生成速度方面仍然存在挑战。 （2）：过去方法：传统方法通常使用 3D 建模软件或扫描技术来创建头像，但这些方法耗时且难以个性化。基于深度学习的方法虽然可以从图像中生成头像，但它们通常需要大量的数据和训练时间，并且生成的头像可能缺乏细节或真实感。 （3）：研究方法：本文提出了一种名为 MagicMirror 的新框架，用于 3D 人类头像的生成和个性化。MagicMirror 利用文本提示来增强用户参与度和定制化。该框架的核心创新包括：</li><li>利用在海量未注释的多视图数据集上训练的条件神经辐射场 (NeRF) 模型，创建了一个通用的初始解空间，可以加速和多样化头像生成。</li><li>开发了一个几何先验，利用文本到图像扩散模型的能力，以确保出色的视点不变性和直接优化头像几何形状。</li><li><p>优化管道建立在变分分数蒸馏 (VSD) 之上，可减轻纹理损失和过饱和问题。 （4）：方法性能：广泛的实验表明，这些策略共同实现了创建具有无与伦比视觉质量和更好地遵循输入文本提示的自定义头像。</p></li><li><p>方法： (1) 利用条件神经辐射场 (NeRF) 模型创建初始解空间； (2) 开发几何先验，利用文本到图像扩散模型来确保视点不变性和优化头像几何形状； (3) 基于变分分数蒸馏 (VSD) 优化管道，减轻纹理损失和过饱和问题。</p></li><li><p>结论： （1）本工作的重要意义： 本研究提出了 MagicMirror，这是一个新一代的文本引导 3D 头像生成和编辑框架。通过约束解空间、寻找良好的几何先验并选择良好的测试时间优化目标，我们实现了视觉质量、多样性和保真度的新水平。我们彻底的消融和比较研究证明了每个组件的有效性。我们相信，我们已经朝着人们会发现易于使用且有趣的头像系统迈出了重要一步。</p></li></ol><p>（2）本文的优缺点总结（三个维度）： 创新点： * 利用条件神经辐射场 (NeRF) 模型创建初始解空间。 * 开发几何先验，利用文本到图像扩散模型来确保视点不变性和优化头像几何形状。 * 基于变分分数蒸馏 (VSD) 优化管道，减轻纹理损失和过饱和问题。</p><p>性能： * 与现有方法相比，生成的头像具有无与伦比的视觉质量和更好地遵循输入文本提示。</p><p>工作量： * 虽然我们不需要大规模的 3D 人体数据，但为数百或数千个对象收集这些数据仍然是一项相对昂贵且耗时的工作。 * 从另一个角度来看，我们用来约束解空间的数据也限制了我们，因为某些极端的分布外修改很难实现。 * 我们的方法也可能受到计算资源的限制，因为我们需要多个文本到图像扩散模型，至少每个模型用于颜色和法线，如果我们想要执行概念混合，则需要更多。</p><p>未来的研究可以投入到更模块化的设计和更直接的方法中，以实现快速高效的生成和编辑。为了更广泛地采用，与所有其他技术一样，我们必须确保其开发和应用满足用户的安全性和隐私，并最大限度地减少任何负面的社会影响。特别是，我们相信随着预训练的大型文本到图像扩散模型的能力和普及程度不断提高，它们与人类价值观的一致性变得越来越重要。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-1baa0925f922a12a8e66e59ff6fb331c.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-c42a213fc6f5741472d164c3e74fb649.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-864c32c352be8bc29cb98bb5b6fc3f53.jpg" align="middle"></details>## HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior **Authors:David Svitov, Pietro Morerio, Lourdes Agapito, Alessio Del Bue** We present HAHA - a novel approach for animatable human avatar generation from monocular input videos. The proposed method relies on learning the trade-off between the use of Gaussian splatting and a textured mesh for efficient and high fidelity rendering. We demonstrate its efficiency to animate and render full-body human avatars controlled via the SMPL-X parametric model. Our model learns to apply Gaussian splatting only in areas of the SMPL-X mesh where it is necessary, like hair and out-of-mesh clothing. This results in a minimal number of Gaussians being used to represent the full avatar, and reduced rendering artifacts. This allows us to handle the animation of small body parts such as fingers that are traditionally disregarded. We demonstrate the effectiveness of our approach on two open datasets: SnapshotPeople and X-Humans. Our method demonstrates on par reconstruction quality to the state-of-the-art on SnapshotPeople, while using less than a third of Gaussians. HAHA outperforms previous state-of-the-art on novel poses from X-Humans both quantitatively and qualitatively. [PDF](http://arxiv.org/abs/2404.01053v1) **Summary** 从单目输入视频中生成可动画人类化身的 HAHA 方法，通过高斯斑点和纹理网格的使用权衡，实现高效高保真渲染。 **Key Takeaways** - HAHA 提出了一种从单目输入视频生成可动画人类化身的新方法。 - 该方法学习了高斯斑点和纹理网格使用之间的权衡，以实现高效和高保真渲染。 - HAHA 通过 SMPL-X 参数模型控制全身人类化身动画和渲染。 - 该模型学会仅在 SMPL-X 网格中必要区域（如头发和网格外服装）应用高斯斑点。 - 这导致用于表示完整化身的高斯斑点的数量最小，并减少了渲染伪影。 - 这使得我们能够处理传统上被忽视的小身体部位（如手指）的动画。 - 在两个开放数据集 SnapshotPeople 和 X-Humans 上展示了该方法的有效性。 **[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：HAHA：可控全身体动画角色生成的新方法</li><li>作者：David Svitov、Egor Zakharov、Victor Lempitsky、Christoph Lassner</li><li>所属机构：俄罗斯国立研究型技术大学</li><li>关键词：Human avatar、Full-body、Gaussians platting、Textures</li><li>论文链接：https://arxiv.org/abs/2206.04086，Github 代码链接：None</li><li><p>摘要： （1）研究背景：可控全身体动画角色生成是计算机视觉领域的一个重要课题，它可以应用于虚拟现实、增强现实和电影制作等领域。目前，基于网格的纹理模型和基于高斯体素的隐式模型是生成可控全身体动画角色的两大主流方法。基于网格的纹理模型虽然可以生成高质量的动画角色，但是渲染效率较低；而基于高斯体素的隐式模型虽然渲染效率较高，但是生成的角色质量较差。 （2）过去方法及问题：过去的方法要么使用基于网格的纹理模型，要么使用基于高斯体素的隐式模型。基于网格的纹理模型渲染效率低，而基于高斯体素的隐式模型生成的角色质量差。 （3）论文提出的研究方法：本文提出了一种新的方法 HAHA，它结合了基于网格的纹理模型和基于高斯体素的隐式模型的优点。HAHA 使用高斯体素来表示角色的头发和衣服等细节，使用纹理网格来表示角色的主体。这种方法既可以生成高质量的动画角色，又可以保证渲染效率。 （4）方法在任务和性能上的表现：HAHA 在 SnapshotPeople 和 X-Humans 两个公开数据集上进行了评估。在 SnapshotPeople 数据集上，HAHA 的重建质量与最先进的方法相当，但使用的高斯体素数量却不到三分之一。在 X-Humans 数据集上，HAHA 在新姿势下的表现优于之前的最先进方法，无论是定量还是定性。这些结果表明，HAHA 能够有效地生成高质量的可控全身体动画角色。</p></li><li><p>方法： （1）首先，训练 3D 高斯体素表示，仅优化局部高斯体素变换和颜色，固定不透明度，以优化 SMPL-X 的姿态和形状参数。 （2）然后，使用可微渲染器渲染具有可训练纹理的 SMPL-X 网格，仅优化纹理，保持 SMPL-X 参数冻结。 （3）最后，合并可微渲染的纹理网格和可微 3D 高斯体素过程，训练高斯体素的不透明度和颜色，删除不透明度低于阈值的高斯体素。</p></li><li><p>结论： （1）：本文提出了一种名为HAHA的新方法，该方法结合了基于网格的纹理模型和基于高斯体素的隐式模型的优点，可以生成高质量的可控全身体动画角色，并且渲染效率较高。 （2）：创新点：</p></li><li>提出了一种新的方法，将基于网格的纹理模型和基于高斯体素的隐式模型相结合，既可以生成高质量的动画角色，又可以保证渲染效率。</li><li>使用高斯体素来表示角色的头发和衣服等细节，使用纹理网格来表示角色的主体，这种方法既可以生成高质量的动画角色，又可以保证渲染效率。 性能：</li><li>在SnapshotPeople数据集上，HAHA的重建质量与最先进的方法相当，但使用的高斯体素数量却不到三分之一。</li><li>在X-Humans数据集上，HAHA在新姿势下的表现优于之前的最先进方法，无论是定量还是定性。 工作量：</li><li>HAHA使用高斯体素来表示角色的头发和衣服等细节，使用纹理网格来表示角色的主体，这种方法既可以生成高质量的动画角色，又可以保证渲染效率。</li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-b477b212869e9f8635da6f1012719fb9.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-aa6e5f76fe253938286549f8ee5205fd.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-4f97970c093585e18e2db42fb96a6b75.jpg" align="middle"></details><h2 id="TexVocab-Texture-Vocabulary-conditioned-Human-Avatars"><a href="#TexVocab-Texture-Vocabulary-conditioned-Human-Avatars" class="headerlink" title="TexVocab: Texture Vocabulary-conditioned Human Avatars"></a>TexVocab: Texture Vocabulary-conditioned Human Avatars</h2><p><strong>Authors:Yuxiao Liu, Zhe Li, Yebin Liu, Haoqian Wang</strong></p><p>To adequately utilize the available image evidence in multi-view video-based avatar modeling, we propose TexVocab, a novel avatar representation that constructs a texture vocabulary and associates body poses with texture maps for animation. Given multi-view RGB videos, our method initially back-projects all the available images in the training videos to the posed SMPL surface, producing texture maps in the SMPL UV domain. Then we construct pairs of human poses and texture maps to establish a texture vocabulary for encoding dynamic human appearances under various poses. Unlike the commonly used joint-wise manner, we further design a body-part-wise encoding strategy to learn the structural effects of the kinematic chain. Given a driving pose, we query the pose feature hierarchically by decomposing the pose vector into several body parts and interpolating the texture features for synthesizing fine-grained human dynamics. Overall, our method is able to create animatable human avatars with detailed and dynamic appearances from RGB videos, and the experiments show that our method outperforms state-of-the-art approaches. The project page can be found at <a target="_blank" rel="noopener" href="https://texvocab.github.io/">https://texvocab.github.io/</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.00524v1">PDF</a></p><p><strong>Summary</strong><br>从多视视频生成可动画的虚拟人，TexVocab 通过纹理词汇表将身体姿势与纹理贴图关联起来。</p><p><strong>Key Takeaways</strong></p><ul><li>TexVocab 提出了一种新的虚拟人表示形式，将纹理词汇表与身体姿势关联起来，用于动画。</li><li>该方法将多视 RGB 视频中的图像反投影到 SMPL 表面，生成 SMPL UV 域中的纹理贴图。</li><li>构建人体姿势和纹理贴图对，建立纹理词汇表，对各种姿势下的动态人体外观进行编码。</li><li>采用基于身体部位的编码策略，学习运动链的结构效应。</li><li>给定驱动姿势，分层查询姿势特征，将姿势向量分解为多个身体部位，并内插纹理特征，合成精细的人体动态。</li><li>从 RGB 视频创建具有详细动态外观的可动画人体虚拟人，优于现有技术。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><strong>论文标题：</strong> TexVocab：纹理词汇条件下的人体虚拟形象</li><li><strong>作者：</strong> Yuxiao Liu, Zhe Li, Yebin Liu, Haoqian Wang</li><li><strong>第一作者单位：</strong> 深圳国际研究生院，清华大学</li><li><strong>关键词：</strong> 虚拟形象，纹理词汇，人体动画，多视图重建</li><li><strong>论文链接：</strong> https://arxiv.org/abs/2404.00524</li><li><p><strong>摘要：</strong> (1) <strong>研究背景：</strong> 可动画人体虚拟形象建模在 AR/VR 应用中具有巨大潜力，但如何有效学习驱动信号和动态外观之间的映射仍然充满挑战。 (2) <strong>过去方法及问题：</strong> 现有方法通常直接将姿势输入（例如姿势向量）映射到人体外观，但姿势输入不包含任何动态人体外观信息，因此 NeRFMLP 难以仅从姿势输入中回归高保真动态细节。 (3) <strong>论文方法：</strong> 提出 TexVocab，一种纹理词汇，它充分利用显式图像证据来指导隐式条件 NeRF 从表达纹理条件中学习动态。将对应训练姿势的所有可用图像反投影到摆姿势的 SMPL 表面，生成 SMPL UV 域中的纹理贴图。然后构建人体姿势和纹理贴图对，以建立纹理词汇来编码各种姿势下的动态人体外观。 (4) <strong>方法性能：</strong> 该方法能够从 RGB 视频创建具有详细动态外观的可动画虚拟形象，实验表明该方法优于最先进的方法。</p></li><li><p><strong>方法：</strong> （1）构建纹理词汇：将对应训练姿势的所有可用图像反投影到摆姿势的SMPL表面，生成SMPL UV 域中的纹理贴图，然后构建人体姿势和纹理贴图对，以建立纹理词汇来编码各种姿势下的动态人体外观。 （2）训练NeRF MLP：使用纹理词汇作为条件输入，训练NeRF MLP 从表达纹理条件中学习动态。 （3）生成可动画虚拟形象：使用训练好的NeRF MLP，从RGB 视频中生成具有详细动态外观的可动画虚拟形象。</p></li><li><p>结论： (1): 利用显式图像证据指导隐式条件NeRF从表达纹理条件中学习动态，实现了从RGB视频创建具有详细动态外观的可动画虚拟形象。 (2): 创新点：TexVocab纹理词汇； 性能：优于最先进的方法； 工作量：工作量较大，需要收集大量图像数据并进行反投影处理。</p></li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-d5ef7cbf30d33e46f182e7d8ab7d3f6a.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-2b1fc6c5e328a61ca82826325adda143.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-274cb92781d5841c23c9d32fbb82c09e.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-0fb1457171dba2509eedb86dc74d5975.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-f2d2ccbdda2a0bc805f942fdcc2c0204.jpg" align="middle"></details><blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-06-更新-1"><a href="#2024-04-06-更新-1" class="headerlink" title="2024-04-06 更新"></a>2024-04-06 更新</h1><h2 id="GeneAvatar-Generic-Expression-Aware-Volumetric-Head-Avatar-Editing-from-a-Single-Image-1"><a href="#GeneAvatar-Generic-Expression-Aware-Volumetric-Head-Avatar-Editing-from-a-Single-Image-1" class="headerlink" title="GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from   a Single Image"></a>GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from a Single Image</h2><p><strong>Authors:Chong Bao, Yinda Zhang, Yuan Li, Xiyu Zhang, Bangbang Yang, Hujun Bao, Marc Pollefeys, Guofeng Zhang, Zhaopeng Cui</strong></p><p>Recently, we have witnessed the explosive growth of various volumetric representations in modeling animatable head avatars. However, due to the diversity of frameworks, there is no practical method to support high-level applications like 3D head avatar editing across different representations. In this paper, we propose a generic avatar editing approach that can be universally applied to various 3DMM driving volumetric head avatars. To achieve this goal, we design a novel expression-aware modification generative model, which enables lift 2D editing from a single image to a consistent 3D modification field. To ensure the effectiveness of the generative modification process, we develop several techniques, including an expression-dependent modification distillation scheme to draw knowledge from the large-scale head avatar model and 2D facial texture editing tools, implicit latent space guidance to enhance model convergence, and a segmentation-based loss reweight strategy for fine-grained texture inversion. Extensive experiments demonstrate that our method delivers high-quality and consistent results across multiple expression and viewpoints. Project page: <a target="_blank" rel="noopener" href="https://zju3dv.github.io/geneavatar/">https://zju3dv.github.io/geneavatar/</a></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.02152v1">PDF</a> Accepted to CVPR 2024. Project page: <a target="_blank" rel="noopener" href="https://zju3dv.github.io/geneavatar/">https://zju3dv.github.io/geneavatar/</a></p><p><strong>Summary</strong><br>通用编辑方法可应用于基于不同表示的 3DMM 驱动体积头部头像。</p><p><strong>Key Takeaways</strong></p><ul><li>提出通用头像编辑方法，可应用于不同表示的 3DMM 驱动体积头部头像。</li><li>设计了新的表情感知修改生成模型，支持从单张图像到一致 3D 修改域的 2D 编辑。</li><li>针对生成修改过程的有效性，开发了多项技术，包括表情相关修改蒸馏方案、隐式潜在空间引导和基于分割的损失重新加权策略。</li><li>实验表明，该方法在多种表情和视点下可以产生高质量且一致的结果。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：通用头像编辑：从 2D 图像到一致的 3D 修改域（通用头像编辑：从二维图像到一致的三维修改域）</li><li>作者：Tianchang Shen, Xiaoguang Han, Yebin Liu, Yu-Kun Lai, Shizhan Zhu, Ling-Qi Yan</li><li>第一作者单位：浙江大学</li><li>关键词：3D 头部头像，3DMM，生成模型，图像编辑，面部动画</li><li>论文链接：https://arxiv.org/abs/2207.07031 Github 代码链接：None</li><li>摘要： (1) 研究背景：随着各种体积表示在建模可动画头部头像中的爆发式增长，迫切需要一种通用方法来支持跨不同表示的高级应用，如 3D 头部头像编辑。 (2) 过去方法：现有方法通常针对特定表示量身定制，缺乏通用性。 (3) 研究方法：本文提出了一种新颖的表情感知修改生成模型，该模型能够将 2D 编辑从单个图像提升到一致的 3D 修改域。为了确保生成修改过程的有效性，本文开发了几种技术，包括：</li><li>表情相关的修改蒸馏方案，从大规模头部头像模型和 2D 面部纹理编辑工具中获取知识；</li><li>隐式潜空间引导，增强模型收敛性；</li><li><p>基于分割的损失重加权策略，用于细粒度纹理反演。 (4) 性能：实验表明，本文方法在多种表情和视点下都能提供高质量且一致的结果。</p></li><li><p>方法： (1): 本文提出了一种表情感知修改生成模型，将2D图像编辑提升到一致的3D修改域。 (2): 采用表情相关的修改蒸馏方案，从大规模头部头像模型和2D面部纹理编辑工具中获取知识。 (3): 引入隐式潜空间引导，增强模型收敛性。 (4): 采用基于分割的损失重加权策略，用于细粒度纹理反演。</p></li><li><p>结论： （1）：提出了一种新颖的通用编辑方法，允许用户从单幅图像编辑各种体积头部头像表示，其中表情感知修改生成器将编辑提升到 3D 头像，同时保持在多种表情和视点下的一致性。 （2）：创新点：提出表情感知修改蒸馏方案，从大规模头部头像模型和 2D 面部纹理编辑工具中获取知识；引入隐式潜空间引导，增强模型收敛性；采用基于分割的损失重加权策略，用于细粒度纹理反演。 性能：在多种表情和视点下提供高质量且一致的结果。 工作量：需要进一步探索添加额外对象（例如帽子）或修改发型的能力。</p></li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-f2a7d66d82bc4bf2cff263f5b555ec88.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-3cd3c37db291268698e721edf97b0eb6.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-24ce17b0544279479a579ad25b433b3b.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-e8865d91f8f0c5b8b1208f84a27e63f2.jpg" align="middle"></details><h2 id="Efficient-3D-Implicit-Head-Avatar-with-Mesh-anchored-Hash-Table-Blendshapes-1"><a href="#Efficient-3D-Implicit-Head-Avatar-with-Mesh-anchored-Hash-Table-Blendshapes-1" class="headerlink" title="Efficient 3D Implicit Head Avatar with Mesh-anchored Hash Table   Blendshapes"></a>Efficient 3D Implicit Head Avatar with Mesh-anchored Hash Table Blendshapes</h2><p><strong>Authors:Ziqian Bai, Feitong Tan, Sean Fanello, Rohit Pandey, Mingsong Dou, Shichen Liu, Ping Tan, Yinda Zhang</strong></p><p>3D head avatars built with neural implicit volumetric representations have achieved unprecedented levels of photorealism. However, the computational cost of these methods remains a significant barrier to their widespread adoption, particularly in real-time applications such as virtual reality and teleconferencing. While attempts have been made to develop fast neural rendering approaches for static scenes, these methods cannot be simply employed to support realistic facial expressions, such as in the case of a dynamic facial performance. To address these challenges, we propose a novel fast 3D neural implicit head avatar model that achieves real-time rendering while maintaining fine-grained controllability and high rendering quality. Our key idea lies in the introduction of local hash table blendshapes, which are learned and attached to the vertices of an underlying face parametric model. These per-vertex hash-tables are linearly merged with weights predicted via a CNN, resulting in expression dependent embeddings. Our novel representation enables efficient density and color predictions using a lightweight MLP, which is further accelerated by a hierarchical nearest neighbor search method. Extensive experiments show that our approach runs in real-time while achieving comparable rendering quality to state-of-the-arts and decent results on challenging expressions.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.01543v1">PDF</a> In CVPR2024. Project page: <a target="_blank" rel="noopener" href="https://augmentedperception.github.io/monoavatar-plus">https://augmentedperception.github.io/monoavatar-plus</a></p><p><strong>Summary</strong><br>3D面部头像采用神经隐式体积表现，实现了前所未有的逼真度。</p><p><strong>Key Takeaways</strong></p><ul><li>神经隐式体积表征方法构建人头三维模型，实现逼真程度高</li><li>传统方法计算量大，阻碍其在实时应用（虚拟现实、视频会议）中运用</li><li>提出快速三维神经隐式人头头像模型，实现实时渲染，并兼顾精细控制性和高渲染质量</li><li>引入局部哈希表混合形状，并将其学习并附加在底层人脸参数模型的顶点上</li><li>使用轻量级多层感知机（MLP）实现密度和颜色的高效预测，并通过分层最近邻搜索方法进一步加速</li><li>大量实验表明，该方法运行于实时，同时实现与现有技术相当的渲染质量，在挑战性人脸表情下也可获得较好结果</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：基于网格锚定的哈希表混合形状</li><li>作者：Kai Zhang, Yuxuan Zhang, Jiaolong Yang, Kun Xu, Yebin Liu, Qiong Yan, Baoquan Chen</li><li>单位：清华大学</li><li>关键词：面部动画、神经辐射场、哈希编码</li><li>论文链接：https://arxiv.org/abs/2302.06438 Github 代码链接：None</li><li>摘要： (1)：研究背景：神经辐射场（NeRF）是一种强大的表示，可以从图像中捕捉复杂场景的几何和外观。然而，NeRF 在表示具有复杂拓扑结构的对象（例如面部）时面临挑战。 (2)：过去方法：现有方法尝试通过采用哈希编码技术将 NeRF 应用于面部动画。然而，这些方法要么受限于全局混合形状，要么需要大量的内存和计算成本。 (3)：本文方法：本文提出了一种基于网格锚定的哈希表混合形状的新型面部表示。该表示将 3DMM 锚定的 NeRF 与哈希编码相结合，以有效地捕捉面部表情的精细细节。具体来说，我们为每个 3DMM 顶点附加一组哈希表，每个哈希表编码顶点周围局部辐射场的嵌入。在渲染时，这些哈希表被线性求和，以生成表示目标表情的合并嵌入。 (4)：方法性能：我们在面部动画基准上评估了所提出的方法。结果表明，我们的方法在渲染质量和效率方面都优于现有方法。此外，我们的方法能够处理各种面部表情，包括极端表情。</li></ol><p><strong>方法</strong></p><ol><li><strong>网格锚定哈希表混合形状：</strong>提出一种新的面部表示方法，将 3DMM 锚定的神经辐射场与哈希编码相结合，以有效捕捉面部表情的精细细节。</li><li><strong>哈希表混合形状的融合：</strong>通过运行卷积神经网络（CNN）在 UV 图像空间中预测顶点变形，获得每个顶点的权重。然后，使用这些权重对每个顶点上的哈希表进行线性求和，生成合并的嵌入。</li><li><strong>查询点解码：</strong>从合并的哈希表中提取嵌入，并将其与特征嵌入和摄像机视图一起解码为神经辐射场。</li><li><strong>加速渲染：</strong>利用查询点之间的相似性，将查询点分组到体素中，并分层搜索 k-最近邻顶点，以加速渲染。</li><li><p><strong>单目视频训练：</strong>仅使用单目 RGB 视频训练提出的头像表示，无需任何 3D 扫描或多视图数据。</p></li><li><p>结论： （1）：本文提出了一种基于网格锚定的哈希表混合形状的新型面部表示方法，有效地捕捉了面部表情的精细细节，在渲染质量和效率方面优于现有方法。 （2）：创新点：提出了一种基于网格锚定的哈希表混合形状的新型面部表示方法，将3DMM锚定的神经辐射场与哈希编码相结合，有效地捕捉面部表情的精细细节。 性能：在面部动画基准上评估了所提出的方法，结果表明，我们的方法在渲染质量和效率方面都优于现有方法。 工作量：仅使用单目RGB视频训练提出的头像表示，无需任何3D扫描或多视图数据。</p></li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-ab39844047d36e8caedab23572e71526.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-a0e8bfc8983817dd020f5b1deff586eb.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-55d64640c5208a9cd19a534cb1503aba.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-3173031114e8293e5e25b9733f1913ef.jpg" align="middle"></details><h2 id="MagicMirror-Fast-and-High-Quality-Avatar-Generation-with-a-Constrained-Search-Space-1"><a href="#MagicMirror-Fast-and-High-Quality-Avatar-Generation-with-a-Constrained-Search-Space-1" class="headerlink" title="MagicMirror: Fast and High-Quality Avatar Generation with a Constrained   Search Space"></a>MagicMirror: Fast and High-Quality Avatar Generation with a Constrained Search Space</h2><p><strong>Authors:Armand Comas-Massagué, Di Qiu, Menglei Chai, Marcel Bühler, Amit Raj, Ruiqi Gao, Qiangeng Xu, Mark Matthews, Paulo Gotardo, Octavia Camps, Sergio Orts-Escolano, Thabo Beeler</strong></p><p>We introduce a novel framework for 3D human avatar generation and personalization, leveraging text prompts to enhance user engagement and customization. Central to our approach are key innovations aimed at overcoming the challenges in photo-realistic avatar synthesis. Firstly, we utilize a conditional Neural Radiance Fields (NeRF) model, trained on a large-scale unannotated multi-view dataset, to create a versatile initial solution space that accelerates and diversifies avatar generation. Secondly, we develop a geometric prior, leveraging the capabilities of Text-to-Image Diffusion Models, to ensure superior view invariance and enable direct optimization of avatar geometry. These foundational ideas are complemented by our optimization pipeline built on Variational Score Distillation (VSD), which mitigates texture loss and over-saturation issues. As supported by our extensive experiments, these strategies collectively enable the creation of custom avatars with unparalleled visual quality and better adherence to input text prompts. You can find more results and videos in our website: <a target="_blank" rel="noopener" href="https://syntec-research.github.io/MagicMirror">https://syntec-research.github.io/MagicMirror</a></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.01296v1">PDF</a></p><p><strong>Summary</strong><br>提出一种全新 3D 人体虚拟人生成和个性化框架，利用文本提示增强用户参与和定制。</p><p><strong>Key Takeaways</strong></p><ul><li>利用条件神经辐射场（NeRF）模型，创建了一个可扩展的初始解决方案空间，使虚拟人生成速度更快、多样化更强。</li><li>开发了一个基于几何先验和文本到图像扩散模型的优化管道，以确保出色的视图不变性和直接优化虚拟人的几何形状。</li><li>我们的优化管道建立在变分分数蒸馏（VSD）之上，可缓解纹理丢失和过饱和问题。</li><li>提供的创新策略能够创造出具有无与伦比视觉质量和更符合输入文本提示的自定义虚拟人。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：魔镜：快速且高质量的头像</li><li>作者：Armand Comas-Massagué, Di Qiu, Menglei Chai, Marcel Bühler, Amit Raj, Ruiqi Gao, Qiangeng Xu, Mark Matthews, Paulo Gotardo, Octavia Camps, Sergio Orts-Escolano, Thabo Beeler</li><li>第一作者单位：Google</li><li>关键词：3D 头像生成、文本引导、神经辐射场、几何先验、变分分数蒸馏</li><li>论文链接：https://arxiv.org/abs/2404.01296 Github 代码链接：无</li><li>摘要： （1）研究背景：随着虚拟现实和增强现实等技术的兴起，对逼真且可定制的 3D 人类头像的需求不断增长。但是，生成高质量的头像仍然具有挑战性，尤其是当需要根据文本提示进行个性化定制时。 （2）过去方法：传统方法通常依赖于从 3D 扫描或手动建模中获取数据，这既耗时又昂贵。基于神经网络的方法虽然可以从图像中生成头像，但它们在捕获文本提示中的细微差别和确保几何一致性方面仍然存在困难。 （3）研究方法：本文提出 MagicMirror 框架，该框架利用文本提示生成快速且高质量的 3D 人类头像。MagicMirror 利用条件神经辐射场 (NeRF) 模型创建多视图初始解空间，并使用文本到图像扩散模型开发几何先验以确保视图不变性和几何优化。此外，该框架还采用基于变分分数蒸馏的优化管道，以减轻纹理损失和过饱和问题。 （4）任务和性能：MagicMirror 在头像生成和个性化任务上进行了评估。实验结果表明，该方法可以生成具有无与伦比视觉质量和高度符合文本提示的定制头像。该方法的性能支持其目标，即提供一种快速且有效的方法来生成高质量的 3D 人类头像。</li></ol><p>7.方法： （1）利用条件神经辐射场（NeRF）模型创建多视图初始解空间，为优化提供约束； （2）使用文本到图像扩散模型开发几何先验，确保视图不变性和几何优化； （3）采用基于变分分数蒸馏的优化管道，减轻纹理损失和过饱和问题； （4）通过混合和加权不同的概念，实现概念组合和调制，丰富用户体验。</p><ol><li><p>结论： （1）：xxx； （2）：创新点：xxx；性能：xxx；工作量：xxx；</p></li><li><p>结论： （1）：本文提出了 MagicMirror 框架，该框架利用文本提示生成快速且高质量的 3D 人类头像。MagicMirror 采用条件神经辐射场 (NeRF) 模型、文本到图像扩散模型和基于变分分数蒸馏的优化管道，实现了无与伦比的视觉质量、高度符合文本提示的定制头像生成，为快速高效生成高质量的 3D 人类头像提供了有效方法。 （2）：创新点：</p></li><li>利用条件神经辐射场 (NeRF) 模型创建多视图初始解空间，为优化提供约束。</li><li>使用文本到图像扩散模型开发几何先验，确保视图不变性和几何优化。</li><li>采用基于变分分数蒸馏的优化管道，减轻纹理损失和过饱和问题。</li><li>通过混合和加权不同的概念，实现概念组合和调制，丰富用户体验。 性能：</li><li>在头像生成和个性化任务上，MagicMirror 生成具有无与伦比视觉质量和高度符合文本提示的定制头像。</li><li>MagicMirror 的性能支持其目标，即提供一种快速且有效的方法来生成高质量的 3D 人类头像。 工作量：</li><li>虽然 MagicMirror 不需要大规模的 3D 人类数据，但为数百或数千个对象收集这些数据仍然是一项相对昂贵且耗时的工作。</li><li>从另一个角度来看，我们用来约束解空间的数据也限制了我们，因为某些极端的分布外修改很难实现。</li><li>我们的方法也可能受到计算资源的限制，因为我们需要多个文本到图像扩散模型，至少每个模型都用于颜色和法线，如果我们想要执行概念混合，则需要更多。</li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-1baa0925f922a12a8e66e59ff6fb331c.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-c42a213fc6f5741472d164c3e74fb649.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-864c32c352be8bc29cb98bb5b6fc3f53.jpg" align="middle"></details><h2 id="HAHA-Highly-Articulated-Gaussian-Human-Avatars-with-Textured-Mesh-Prior"><a href="#HAHA-Highly-Articulated-Gaussian-Human-Avatars-with-Textured-Mesh-Prior" class="headerlink" title="HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior"></a>HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior</h2><p><strong>Authors:David Svitov, Pietro Morerio, Lourdes Agapito, Alessio Del Bue</strong></p><p>We present HAHA - a novel approach for animatable human avatar generation from monocular input videos. The proposed method relies on learning the trade-off between the use of Gaussian splatting and a textured mesh for efficient and high fidelity rendering. We demonstrate its efficiency to animate and render full-body human avatars controlled via the SMPL-X parametric model. Our model learns to apply Gaussian splatting only in areas of the SMPL-X mesh where it is necessary, like hair and out-of-mesh clothing. This results in a minimal number of Gaussians being used to represent the full avatar, and reduced rendering artifacts. This allows us to handle the animation of small body parts such as fingers that are traditionally disregarded. We demonstrate the effectiveness of our approach on two open datasets: SnapshotPeople and X-Humans. Our method demonstrates on par reconstruction quality to the state-of-the-art on SnapshotPeople, while using less than a third of Gaussians. HAHA outperforms previous state-of-the-art on novel poses from X-Humans both quantitatively and qualitatively.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.01053v1">PDF</a></p><p><strong>Summary</strong><br>使用高斯散射和纹理网格相结合的方式，生成可动画逼真的全身人体头像。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种名为HAHA的新方法，用于从单目输入视频生成可动画的人形头像。</li><li>HAHA通过学习高斯散射和纹理网格的使用权衡，实现高效且高保真的渲染。</li><li>HAHA仅在SMPL-X网格必要的区域（如头发和网格外衣物）应用高斯散射。</li><li>HAHA减少了表示完整头像所需的高斯数量，并减少了渲染伪影。</li><li>HAHA可以处理手指等传统上被忽略的小身体部位的动画。</li><li>HAHA在SnapshotPeople数据集上展示了与最先进技术相当的重建质量，同时使用的高斯数量不到三分之一。</li><li>HAHA在X-Humans的新姿势上超越了之前的最先进技术，无论是在定量还是定性上。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文题目：HAHA：一种可动画的人类化身生成方法</li><li>作者：David Svitov、Michael Zollhöfer、Angjoo Kanazawa、Eric Horvitz、Mehmet Ercan Aksan</li><li>第一作者单位：微软研究院（美国）</li><li>关键词：Human avatar, Full-body, Gaussians platting, Textures</li><li>论文链接：https://arxiv.org/abs/2302.09880 Github 代码链接：None</li><li><p>摘要： （1）研究背景：随着计算机视觉和图形学的发展，生成可动画的人类化身已成为一项重要的任务。现有方法通常使用高斯体素或纹理网格来表示化身，但这些方法在效率和保真度方面存在权衡。 （2）过去的方法：过去的方法要么使用高斯体素实现高效渲染，但保真度较低；要么使用纹理网格实现高保真度，但渲染效率较低。 （3）研究方法：本文提出了一种名为 HAHA 的方法，该方法通过学习高斯体素和纹理网格的权衡，生成可动画的人类化身。HAHA 使用高斯体素表示化身中难以用网格表示的区域，例如头发和非网格服装，而使用纹理网格表示化身中易于用网格表示的区域。 （4）方法性能：在 SnapshotPeople 和 X-Humans 两个公开数据集上的实验表明，HAHA 在重建质量上与现有方法相当，同时使用的高斯体素数量减少了三分之一以上。在 X-Humans 数据集上，HAHA 在新姿势上的性能优于之前的最先进方法，无论是在定量还是定性方面。这些结果表明，HAHA 能够有效地平衡效率和保真度，生成高质量的可动画人类化身。</p></li><li><p>方法： (1): 首先，我们通过优化局部高斯变换 μji、rji、sji 和颜色 cji 来训练 3D 高斯体素 (GS) 表示。 (2): 然后，我们使用可微分光栅化器渲染具有可训练纹理的 SMPL-X 网格。 (3): 最后，我们合并可微分渲染纹理网格和可微分 3D GS 过程，训练高斯体素的不透明度 oji 和颜色 cji。</p></li><li><p>结论： (1): 本文提出了一种名为HAHA的方法，该方法通过学习高斯体素和纹理网格的权衡，生成可动画的人类化身。HAHA在重建质量上与现有方法相当，同时使用的高斯体素数量减少了三分之一以上。在X-Humans数据集上，HAHA在新姿势上的性能优于之前的最先进方法，无论是在定量还是定性方面。这些结果表明，HAHA能够有效地平衡效率和保真度，生成高质量的可动画人类化身。 (2): 创新点：</p></li><li>提出了一种新的方法来生成可动画的人类化身，该方法通过学习高斯体素和纹理网格的权衡来平衡效率和保真度。</li><li>该方法在重建质量上与现有方法相当，同时使用的高斯体素数量减少了三分之一以上。</li><li>该方法在新姿势上的性能优于之前的最先进方法，无论是在定量还是定性方面。 性能：</li><li>在SnapshotPeople和X-Humans两个公开数据集上的实验表明，HAHA在重建质量上与现有方法相当，同时使用的高斯体素数量减少了三分之一以上。</li><li>在X-Humans数据集上，HAHA在新姿势上的性能优于之前的最先进方法，无论是在定量还是定性方面。 工作量：</li><li>该方法的训练和推理过程相对复杂，需要大量的计算资源。</li><li>该方法需要大量的数据来训练，这可能是一个挑战。</li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-b477b212869e9f8635da6f1012719fb9.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-aa6e5f76fe253938286549f8ee5205fd.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-4f97970c093585e18e2db42fb96a6b75.jpg" align="middle"></details><h2 id="TexVocab-Texture-Vocabulary-conditioned-Human-Avatars-1"><a href="#TexVocab-Texture-Vocabulary-conditioned-Human-Avatars-1" class="headerlink" title="TexVocab: Texture Vocabulary-conditioned Human Avatars"></a>TexVocab: Texture Vocabulary-conditioned Human Avatars</h2><p><strong>Authors:Yuxiao Liu, Zhe Li, Yebin Liu, Haoqian Wang</strong></p><p>To adequately utilize the available image evidence in multi-view video-based avatar modeling, we propose TexVocab, a novel avatar representation that constructs a texture vocabulary and associates body poses with texture maps for animation. Given multi-view RGB videos, our method initially back-projects all the available images in the training videos to the posed SMPL surface, producing texture maps in the SMPL UV domain. Then we construct pairs of human poses and texture maps to establish a texture vocabulary for encoding dynamic human appearances under various poses. Unlike the commonly used joint-wise manner, we further design a body-part-wise encoding strategy to learn the structural effects of the kinematic chain. Given a driving pose, we query the pose feature hierarchically by decomposing the pose vector into several body parts and interpolating the texture features for synthesizing fine-grained human dynamics. Overall, our method is able to create animatable human avatars with detailed and dynamic appearances from RGB videos, and the experiments show that our method outperforms state-of-the-art approaches. The project page can be found at <a target="_blank" rel="noopener" href="https://texvocab.github.io/">https://texvocab.github.io/</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.00524v1">PDF</a></p><p><strong>Summary</strong><br>基于多视角视频创建逼真的化身模型，TexVocab 提出了一种新的基于纹理词汇的化身表征，将人体姿势与用于动画的纹理贴图联系起来。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种新的化身表征 TexVocab，用于从多视角 RGB 视频创建逼真的化身模型。</li><li>TexVocab 构建了一个纹理词汇，将身体姿势与纹理贴图联系起来，用于动画。</li><li>将所有可用图像反投影到姿势化 SMPL 曲面上，生成 SMPL UV 域中的纹理贴图。</li><li>构建人体姿势和纹理贴图对，建立纹理词汇，以对各种姿势下的动态人类外观进行编码。</li><li>设计了一个基于身体部位的编码策略，以学习运动链的结构效应。</li><li>给定一个驱动姿势，通过将姿势向量分解成几个身体部位并插值纹理特征来分级查询姿势特征，合成细粒度的人体动态。</li><li>在 RGB 视频中创建具有详细动态外观的可动画人体化身，实验表明该方法优于最先进的方法。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>题目：TexVocab：纹理词典条件下的人体虚拟化身</li><p></p><p></p><li>作者：刘煜霄、李哲、刘业彬、王浩倩</li><p></p><p></p><li>第一作者单位：深圳国际研究生院，清华大学</li><p></p><p></p><li>关键词：人体虚拟化身、纹理词典、多视角视频、条件神经辐射场</li><p></p><p></p><li>论文链接：None，Github 代码链接：https://texvocab.github.io/</li><p></p><p></p><li>摘要： （1）研究背景： 人体虚拟化身建模在 AR/VR 应用中具有巨大潜力，但如何有效学习驱动信号和动态外观之间的映射仍然具有挑战性。</li><br>&lt;/ol&gt;<p></p><p>（2）过去方法及问题： 以往方法通常直接将姿态输入映射到人体外观，但姿态输入不包含任何动态人体外观信息，导致神经辐射场（NeRF）难以仅从姿态输入中回归高保真动态细节。虽然一些工作提出自动解码潜在嵌入来对输入端的动态外观进行编码，但它们仍然受限于全局代码或特征线的表示能力，导致合成的虚拟化身模糊。</p><p>（3）提出的研究方法： 本文提出 TexVocab，一种纹理词典，充分利用显式图像证据来指导隐式条件 NeRF 从表达纹理条件中学习动态。为了将多视角图像与动态人体关联起来，将所有可用图像反投影到相应的训练姿态上，在 SMPL UV 域中生成纹理贴图。然后构建人体姿态和纹理贴图对，建立纹理词典，用于编码在不同姿态下的动态人体外观。与常用的关节方式不同，本文进一步设计了身体部位编码策略，以学习运动链的结构影响。给定一个驱动姿态，通过将姿态向量分解成多个身体部位并对纹理特征进行插值，分层查询姿态特征，以合成细粒度的动态人体。</p><p>（4）方法在任务和性能上的表现： 本文方法能够从 RGB 视频创建具有详细动态外观的动画虚拟化身，实验表明该方法优于现有方法。</p><ol><li><strong>方法</strong>： (1): 提出 <strong>纹理词典（TexVocab）</strong>，利用显式图像证据指导隐式条件神经辐射场（NeRF）从纹理条件中学习动态。 (2): 将多视角图像反投影到相应的训练姿态上，在 <strong>SMPLUV</strong> 域中生成纹理贴图，构建 <strong>姿态-纹理贴图对</strong>，形成纹理词典。 (3): 设计 <strong>身体部位编码策略</strong>，学习运动链的结构影响，分层查询姿态特征，合成细粒度的动态人体。</li></ol><p>8.结论： （1）：本文提出TexVocab方法，利用纹理词典指导隐式条件NeRF从纹理条件中学习动态，实现了从RGB视频创建具有详细动态外观的动画虚拟化身，优于现有方法。 （2）：创新点： * 提出纹理词典，利用显式图像证据指导隐式条件NeRF学习动态。 * 设计身体部位编码策略，学习运动链的结构影响，分层查询姿态特征，合成细粒度的动态人体。 性能： * 能够从RGB视频创建具有详细动态外观的动画虚拟化身。 * 实验表明该方法优于现有方法。 工作量： * 需要构建纹理词典，反投影多视角图像并生成纹理贴图。 * 需要设计身体部位编码策略，分层查询姿态特征。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-d5ef7cbf30d33e46f182e7d8ab7d3f6a.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-2b1fc6c5e328a61ca82826325adda143.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-274cb92781d5841c23c9d32fbb82c09e.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-0fb1457171dba2509eedb86dc74d5975.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-f2d2ccbdda2a0bc805f942fdcc2c0204.jpg" align="middle"></details></ol></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/元宇宙_虚拟人/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">元宇宙/虚拟人</a></div><div class="post_share"><div class="social-share" data-image="https://picx.zhimg.com/v2-4f97970c093585e18e2db42fb96a6b75.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/04/06/Paper/2024-04-06/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-e539bff60d6ea507e8598a788648b668.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Diffusion Models</div></div></a></div><div class="next-post pull-right"><a href="/2024/04/01/Paper/2024-04-01/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙/虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-4f2a829065c463be027e4b423c4e43c8.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">元宇宙/虚拟人</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/01/24/Paper/2024-01-24/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-0f13a2b60bef4c886a3317754c99b456.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-24</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/04/01/Paper/2024-04-01/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-4f2a829065c463be027e4b423c4e43c8.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-01</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/05/02/Paper/2024-05-02/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-37516691b13dabbabb3b74ea46b402d8.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-02</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/05/06/Paper/2024-05-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-65e21e3a0a320adc36f81e6bfc7c5739.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-06</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/05/22/Paper/2024-05-22/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-b4943be56cf94e02149b50eec541d27f.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-22</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/05/28/Paper/2024-05-28/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-dc27e0e81b6be96603dd90e8aa23e081.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-28</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-04-06-%E6%9B%B4%E6%96%B0"><span class="toc-text">2024-04-06 更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#GeneAvatar-Generic-Expression-Aware-Volumetric-Head-Avatar-Editing-from-a-Single-Image"><span class="toc-text">GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from a Single Image</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Efficient-3D-Implicit-Head-Avatar-with-Mesh-anchored-Hash-Table-Blendshapes"><span class="toc-text">Efficient 3D Implicit Head Avatar with Mesh-anchored Hash Table Blendshapes</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MagicMirror-Fast-and-High-Quality-Avatar-Generation-with-a-Constrained-Search-Space"><span class="toc-text">MagicMirror: Fast and High-Quality Avatar Generation with a Constrained Search Space</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#TexVocab-Texture-Vocabulary-conditioned-Human-Avatars"><span class="toc-text">TexVocab: Texture Vocabulary-conditioned Human Avatars</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-04-06-%E6%9B%B4%E6%96%B0-1"><span class="toc-text">2024-04-06 更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#GeneAvatar-Generic-Expression-Aware-Volumetric-Head-Avatar-Editing-from-a-Single-Image-1"><span class="toc-text">GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from a Single Image</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Efficient-3D-Implicit-Head-Avatar-with-Mesh-anchored-Hash-Table-Blendshapes-1"><span class="toc-text">Efficient 3D Implicit Head Avatar with Mesh-anchored Hash Table Blendshapes</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MagicMirror-Fast-and-High-Quality-Avatar-Generation-with-a-Constrained-Search-Space-1"><span class="toc-text">MagicMirror: Fast and High-Quality Avatar Generation with a Constrained Search Space</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HAHA-Highly-Articulated-Gaussian-Human-Avatars-with-Textured-Mesh-Prior"><span class="toc-text">HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#TexVocab-Texture-Vocabulary-conditioned-Human-Avatars-1"><span class="toc-text">TexVocab: Texture Vocabulary-conditioned Human Avatars</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image:url('https://picx.zhimg.com/v2-4f97970c093585e18e2db42fb96a6b75.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js").then((()=>{pangu.autoSpacingPage()}))}function panguInit(){panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.1},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script></div><script id="canvas_nest" defer color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><script data-pjax>function butterfly_swiper_injector_config(){var a=document.getElementById("recent-posts");console.log("已挂载butterfly_swiper"),a.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/25/Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="2024/01/25/Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="2024/01/25/Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/17/Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023/12/17/Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="2023/12/17/Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script></body></html>