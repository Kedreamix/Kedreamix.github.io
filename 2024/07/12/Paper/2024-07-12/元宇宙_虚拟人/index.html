<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>元宇宙/虚拟人 | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="元宇宙&#x2F;虚拟人 方向最新论文已更新，请持续关注 Update in 2024-07-12  MobilePortrait Real-Time One-Shot Neural Head Avatars on Mobile Devices"><meta property="og:type" content="article"><meta property="og:title" content="元宇宙&#x2F;虚拟人"><meta property="og:url" content="https://kedreamix.github.io/2024/07/12/Paper/2024-07-12/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/index.html"><meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World"><meta property="og:description" content="元宇宙&#x2F;虚拟人 方向最新论文已更新，请持续关注 Update in 2024-07-12  MobilePortrait Real-Time One-Shot Neural Head Avatars on Mobile Devices"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://pic1.zhimg.com/v2-9e48453d77506272fa48d5700ae725b1.jpg"><meta property="article:published_time" content="2024-07-12T05:13:39.000Z"><meta property="article:modified_time" content="2024-07-12T05:13:39.432Z"><meta property="article:author" content="Kedreamix"><meta property="article:tag" content="元宇宙&#x2F;虚拟人"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://pic1.zhimg.com/v2-9e48453d77506272fa48d5700ae725b1.jpg"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/2024/07/12/Paper/2024-07-12/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-74LZ5BEQQ1")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!0,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"简"},noticeOutdate:{limitDay:500,position:"top",messagePrev:"It has been",messageNext:"days since the last update, the content of the article may be outdated."},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:200},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!0,post:!0},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!0,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"元宇宙/虚拟人",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-07-12 13:13:39"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach((e=>{n.setAttribute(e,t[e])})),document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)})),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=24?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 7.0.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml"></head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{const d=document.getElementById("loading-box"),e=document.body,o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l(),window.addEventListener("load",(()=>{o()}))})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">298</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">17</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url('https://pic1.zhimg.com/v2-9e48453d77506272fa48d5700ae725b1.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">元宇宙/虚拟人</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-07-12T05:13:39.000Z" title="发表于 2024-07-12 13:13:39">2024-07-12</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-07-12T05:13:39.432Z" title="更新于 2024-07-12 13:13:39">2024-07-12</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">6.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>21分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="元宇宙/虚拟人"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-07-12-更新"><a href="#2024-07-12-更新" class="headerlink" title="2024-07-12 更新"></a>2024-07-12 更新</h1><h2 id="MobilePortrait-Real-Time-One-Shot-Neural-Head-Avatars-on-Mobile-Devices"><a href="#MobilePortrait-Real-Time-One-Shot-Neural-Head-Avatars-on-Mobile-Devices" class="headerlink" title="MobilePortrait: Real-Time One-Shot Neural Head Avatars on Mobile Devices"></a>MobilePortrait: Real-Time One-Shot Neural Head Avatars on Mobile Devices</h2><p><strong>Authors:Jianwen Jiang, Gaojie Lin, Zhengkun Rong, Chao Liang, Yongming Zhu, Jiaqi Yang, Tianyun Zhong</strong></p><p>Existing neural head avatars methods have achieved significant progress in the image quality and motion range of portrait animation. However, these methods neglect the computational overhead, and to the best of our knowledge, none is designed to run on mobile devices. This paper presents MobilePortrait, a lightweight one-shot neural head avatars method that reduces learning complexity by integrating external knowledge into both the motion modeling and image synthesis, enabling real-time inference on mobile devices. Specifically, we introduce a mixed representation of explicit and implicit keypoints for precise motion modeling and precomputed visual features for enhanced foreground and background synthesis. With these two key designs and using simple U-Nets as backbones, our method achieves state-of-the-art performance with less than one-tenth the computational demand. It has been validated to reach speeds of over 100 FPS on mobile devices and support both video and audio-driven inputs.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.05712v1">PDF</a></p><p><strong>Summary</strong><br>MobilePortrait是一种轻量级的一次性神经头像方法，通过整合外部知识到动作建模和图像合成中，实现了在移动设备上的实时推断。</p><p><strong>Key Takeaways</strong></p><ul><li>MobilePortrait是一种轻量级的神经头像方法，能在移动设备上进行实时推断。</li><li>方法通过混合明确和隐式关键点表示来进行精确的动作建模。</li><li>图像合成利用预计算的视觉特征增强前景和背景合成效果。</li><li>使用简单的U-Net作为骨干网络，方法在计算需求上不到传统方法的十分之一。</li><li>MobilePortrait能够以超过100 FPS的速度运行，支持视频和音频驱动输入。</li><li>传统神经头像方法忽视了计算开销问题，且没有专为移动设备设计。</li><li>MobilePortrait在图像质量和动作范围方面实现了最新的性能水平。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>好的，我将会按照您的要求进行回答。</p><ol><li><p>Title: MobilePortrait: 实时单帧神经网络在移动设备上的头像动画技术</p></li><li><p>Authors: Jianwen Jiang, Gaojie Lin (共同贡献者), Zhengkun Rong, Chao Liang, Yongming Zhu, Jiaqi Yang, Tianyun Zhong</p></li><li><p>Affiliation: 所有作者均属于ByteDance Inc.公司。</p></li><li><p>Keywords: Mobile Device, Neural Head Avatars, Real-Time, One-Shot, Motion Modeling, Image Synthesis, Lightweight.</p></li><li><p>Urls: 由于这里没有提供论文链接和GitHub代码链接，所以无法填写。</p></li><li><p>Summary:</p><ul><li><p>(1)研究背景：随着移动设备性能的提升和用户需求的变化，移动设备上头像动画的需求日益增加。现有的神经网络头像方法虽然图像质量和动作范围取得了显著进展，但忽视了计算开销，且无法在移动设备上运行。本文的研究背景是提出一种能在移动设备上实时运行的轻量级单帧神经网络头像动画技术。</p><p>-(2)过去的方法及问题：现有神经网络头像方法主要关注图像质量和动作范围的改进，但忽视了计算开销，无法在移动设备上运行。因此，需要一种新的方法来解决这个问题。</p><p>-(3)研究方法：本文提出了MobilePortrait，一种轻量级的单帧神经网络头像方法。它通过整合外部知识到运动建模和图像合成中，降低了学习复杂性，实现了在移动设备上的实时推理。具体地，它引入了显式和隐式关键点的混合表示进行精确运动建模，并使用预计算视觉特征增强前景和背景合成。</p><p>-(4)任务与性能：本文的方法在头像动画任务上取得了显著的效果，实现了高质量的结果和显著的计算效率优势。通过与现有高计算成本的方法比较，本文的方法在计算效率上有了显著的提升，同时保持了图像质量和动作范围的先进性。性能结果支持了本文方法的目标，即在移动设备上实现实时头像动画。<br>好的，我会按照您的要求对论文的方法进行详细总结。以下是按照您提供的格式给出的摘要和方法的介绍：</p></li></ul></li></ol><p>摘要部分：随着移动设备性能的提升和用户需求的变化，移动设备上头像动画的需求日益增加。现有的神经网络头像动画技术虽然在图像质量和动作范围方面取得了显著进展，但忽略了计算开销的问题，无法在移动设备上运行。本文提出了MobilePortrait技术，一种轻量级的单帧神经网络头像动画方法。它结合了外部知识来进行运动建模和图像合成，以降低学习复杂性，实现在移动设备上的实时推理。该方法引入显式和隐式关键点的混合表示进行精确运动建模，并利用预计算视觉特征增强前景和背景合成。在头像动画任务上取得了显著的效果，实现了高质量的结果和显著的计算效率优势。性能结果支持了本文的目标，即在移动设备上实现实时头像动画。</p><p>方法部分：</p><p>（1）整合外部知识到运动建模和图像合成中：MobilePortrait技术利用外部知识来提高运动建模和图像合成的效率。这种方法有助于降低学习复杂性并加快推理速度。</p><p>（2）采用显式和隐式关键点的混合表示进行精确运动建模：该技术通过引入关键点的概念来捕捉头部运动的细微变化，并实现对精确头部运动的建模。这种混合表示方式可以提高运动建模的准确性。</p><p>（3）利用预计算视觉特征增强前景和背景合成：MobilePortrait技术使用预计算的视觉特征来增强头像动画的前景和背景合成效果。这种方法可以提高图像合成的质量和效率。同时应对动态环境进行有效渲染和调整保持稳定性以保证对姿态和表情变化等实时反应，同时实现更高的渲染效率以应对移动设备的性能限制。最终使得在移动设备上的头像动画具有实时性、流畅性和高画质。 综上是一种低成本的高效的方法完成目标，并拥有较大的应用价值和发展前景值得期待进一步的挖掘与提升效率实现广泛应用目标以进一步推进我国相关领域技术的蓬勃发展。</p><p>好的，根据您的要求，我将对这篇文章进行总体评价并概括出其创新点、性能和工作量方面的优缺点。以下是回答：</p><ol><li>Conclusion:</li></ol><p>(1) xxx的重要性体现在其解决了移动设备上的头像动画技术的难题，提出了一种轻量级的单帧神经网络方法，实现了在移动设备上的实时头像动画，满足了日益增长的用户需求，推动了移动设备上神经网络头像动画技术的发展。</p><p>(2) 创新点：该文章的创新性体现在其将外部知识整合到运动建模和图像合成中，采用显式和隐式关键点的混合表示进行精确运动建模，实现了高质量的结果和显著的计算效率优势。其提出的MobilePortrait技术为移动设备上实现实时头像动画提供了新的解决方案。</p><p>性能：该文章所提出的方法在头像动画任务上取得了显著的效果，实现了高质量的结果，与现有方法相比，具有显著的计算效率优势。实验结果表明，该方法具有较高的性能，支持视频和音频驱动输入。</p><p>工作量：文章的工作量大，涉及到运动建模、图像合成、关键点表示等多个方面的技术研究与实现。同时，文章对实验进行了充分的验证和性能评估，证明了所提出方法的有效性和优越性。然而，文章没有提供代码链接，无法评估其代码复用的便利性。</p><p>总体来说，该文章具有重要的实际意义和创新性，在性能上取得了显著的效果，但工作量较大，未来还有进一步优化的空间。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-145b164ed674bded6c5f14f1e5ae39a3.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-e76bf61a2edc074441e8ac3eaa911d9d.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-f91c77ec3d5c4828683cc17007e6a195.jpg" align="middle"></details><h2 id="CanonicalFusion-Generating-Drivable-3D-Human-Avatars-from-Multiple-Images"><a href="#CanonicalFusion-Generating-Drivable-3D-Human-Avatars-from-Multiple-Images" class="headerlink" title="CanonicalFusion: Generating Drivable 3D Human Avatars from Multiple   Images"></a>CanonicalFusion: Generating Drivable 3D Human Avatars from Multiple Images</h2><p><strong>Authors:Jisu Shin, Junmyeong Lee, Seongmin Lee, Min-Gyu Park, Ju-Mi Kang, Ju Hong Yoon, Hae-Gon Jeon</strong></p><p>We present a novel framework for reconstructing animatable human avatars from multiple images, termed CanonicalFusion. Our central concept involves integrating individual reconstruction results into the canonical space. To be specific, we first predict Linear Blend Skinning (LBS) weight maps and depth maps using a shared-encoder-dual-decoder network, enabling direct canonicalization of the 3D mesh from the predicted depth maps. Here, instead of predicting high-dimensional skinning weights, we infer compressed skinning weights, i.e., 3-dimensional vector, with the aid of pre-trained MLP networks. We also introduce a forward skinning-based differentiable rendering scheme to merge the reconstructed results from multiple images. This scheme refines the initial mesh by reposing the canonical mesh via the forward skinning and by minimizing photometric and geometric errors between the rendered and the predicted results. Our optimization scheme considers the position and color of vertices as well as the joint angles for each image, thereby mitigating the negative effects of pose errors. We conduct extensive experiments to demonstrate the effectiveness of our method and compare our CanonicalFusion with state-of-the-art methods. Our source codes are available at <a target="_blank" rel="noopener" href="https://github.com/jsshin98/CanonicalFusion">https://github.com/jsshin98/CanonicalFusion</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.04345v1">PDF</a> ECCV 2024 Accepted (18 pages, 9 figures)</p><p><strong>Summary</strong><br>提出了一种新颖的框架，用于从多个图像重建可动人物化身，称为CanonicalFusion。</p><p><strong>Key Takeaways</strong></p><ul><li>CanonicalFusion框架用于从多个图像中重建可动人物化身。</li><li>使用共享编码器-双解码器网络预测线性混合蒙皮权重图和深度图。</li><li>提出了压缩蒙皮权重的概念，通过预训练的MLP网络实现。</li><li>引入基于前向蒙皮的可微渲染方案，优化重建结果。</li><li>优化方案考虑顶点位置、颜色和关节角度，减少姿势误差影响。</li><li>进行了大量实验证明方法的有效性，并与最先进方法进行比较。</li><li>提供了开源代码，位于 <a target="_blank" rel="noopener" href="https://github.com/jsshin98/CanonicalFusion。">https://github.com/jsshin98/CanonicalFusion。</a></li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>好的，我会按照您的要求来总结这篇论文。以下是回答：</p><ol><li><p>标题：基于CanonicalFusion的多图像生成可驱动三维人体化身研究。</p></li><li><p>作者：Jisu Shin，Junmyeong Lee，Seongmin Lee，Min-Gyu Park等。</p></li><li><p>隶属机构：大部分作者来自韩国电子科技学院（KETI）和GIST人工智能研究生院。</p></li><li><p>关键词：可驱动的三维化身、CanonicalFusion、基于前向蒙皮的可微分渲染。</p></li><li><p>Urls：论文链接未提供，代码仓库链接为：<a target="_blank" rel="noopener" href="https://github.com/jsshin98/CanonicalFusion">GitHub代码链接</a>。</p></li><li><p>总结：</p></li></ol><p>(1) 研究背景：生成人体化身对于虚拟现实、增强现实和元宇宙等应用具有重要意义。传统方法需要大量手动工作和昂贵的设备，而神经网络的发展为此过程提供了简化方案。本文旨在提出一种基于多图像生成可驱动三维人体化身的新方法。</p><p>(2) 过去的方法及问题：目前的方法在生成三维人体化身时面临挑战，如姿势误差、几何和光度不一致性等问题。许多方法难以从多个图像中有效地整合信息以生成高质量的可驱动化身。</p><p>(3) 研究方法：本文提出了一个名为CanonicalFusion的框架，其核心技术是整合个体重建结果到规范空间。首先预测线性混合蒙皮（LBS）权重图和深度图，使用共享编码器双解码器网络。引入前向蒙皮可微分渲染方案来合并从多个图像重建的结果，通过优化初始网格并最小化渲染与预测结果之间的光度误差和几何误差来细化网格。优化过程考虑每个图像的顶点位置、颜色和关节角度，以减轻姿势错误的影响。</p><p>(4) 任务与性能：本文的方法在生成可驱动的三维人体化身任务上取得了良好效果。通过与现有方法的比较实验，证明了其性能优于其他方法。生成的三维化身具有良好的可驱动性和真实性，支持通过不同图像生成不同的个性化化身。性能结果表明该方法可以有效地生成高质量的可驱动三维人体化身。</p><ol><li><p>方法概述：</p><ul><li><p>(1) 研究人员首先利用神经网络预测几何形状和蒙皮权重，通过共享编码器双解码器网络预测初始网格，然后对初始网格进行规范化处理，生成规范网格。这一步骤利用线性混合蒙皮（LBS）权重图和深度图预测结果，以生成可驱动的三维人体化身。</p></li><li><p>(2) 在生成初始网格后，研究团队引入了前向蒙皮可微分渲染方案，对从多个图像重建的结果进行合并。通过优化初始网格并最小化渲染与预测结果之间的光度误差和几何误差来细化网格。该步骤旨在解决过去方法在生成三维人体化身时面临的姿势误差、几何和光度不一致等问题。</p></li><li><p>(3) 研究团队利用纹理预测网络对颜色和关节角度进行优化，以减轻姿势错误的影响。该网络采用UNet架构，接受输入图像和预测深度图得到的法线图作为输入，输出阴影移除的图像。</p></li><li><p>(4) 最后，研究团队利用规范网格进行逆向蒙皮操作，将其转换回原始空间并填充未见的几何区域。该研究团队的框架不限制图像数量、视角和姿势变化，能够生成高质量的可驱动三维人体化身。整个流程涉及深度学习、计算机视觉和图形学技术。</p></li></ul></li></ol><p>好的，以下是对上述内容的中文总结和评价：</p><ol><li>总结与观点：</li></ol><p>（1）研究意义：该研究对于虚拟现实、增强现实和元宇宙等应用中的三维人体化身生成具有重要意义。生成高质量的可驱动三维人体化身一直是计算机视觉和图形学领域的研究热点和难点。该研究提供了一个基于多图像生成的可驱动三维人体化身的新方法，对于相关应用的用户体验具有重要的推动作用。</p><p>（2）创新与优势：从创新点、性能和工作量三个维度对文章进行总结与评价如下：</p><p>创新点：该研究提出了一个名为CanonicalFusion的框架，通过整合个体重建结果到规范空间，解决了传统方法在生成三维人体化身时面临的挑战，如姿势误差、几何和光度不一致等问题。引入前向蒙皮可微分渲染方案，合并从多个图像重建的结果，提高了生成的三维化身的真实感和可驱动性。此外，该研究还采用了共享编码器双解码器网络预测初始网格，并引入了纹理预测网络对颜色和关节角度进行优化，进一步提高了生成质量。这些创新点使得该研究在生成可驱动的三维人体化身任务上取得了良好效果。</p><p>性能：该研究通过实验验证了所提出方法的有效性，与其他现有方法相比，该方法在生成高质量的可驱动三维人体化身方面表现出优越性。生成的化身具有良好的可驱动性和真实性，支持通过不同图像生成不同的个性化化身。此外，该方法对图像数量、视角和姿势变化具有鲁棒性。</p><p>工作量：该研究涉及深度学习、计算机视觉和图形学技术等多个领域的知识和技术，工作量较大。从论文的结构和内容来看，作者进行了充分的实验验证和理论分析，展现了较高的研究水平和专业素养。同时，代码仓库链接的提供也为后续研究提供了方便。</p><p>以上就是对该研究的总结和简要评价。如有更深入的研究或评价需求，可以进一步了解论文的细节和技术实现。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-3f284e5069ffaf122f32c8421ef1b5d9.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-1bc009bc7319b2def0c6d917f792ec02.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-f2480bf54c1b9f646b1b12e993eb77c6.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-9e48453d77506272fa48d5700ae725b1.jpg" align="middle"></details><h2 id="WildAvatar-Web-scale-In-the-wild-Video-Dataset-for-3D-Avatar-Creation"><a href="#WildAvatar-Web-scale-In-the-wild-Video-Dataset-for-3D-Avatar-Creation" class="headerlink" title="WildAvatar: Web-scale In-the-wild Video Dataset for 3D Avatar Creation"></a>WildAvatar: Web-scale In-the-wild Video Dataset for 3D Avatar Creation</h2><p><strong>Authors:Zihao Huang, Shoukang Hu, Guangcong Wang, Tianqi Liu, Yuhang Zang, Zhiguo Cao, Wei Li, Ziwei Liu</strong></p><p>Existing human datasets for avatar creation are typically limited to laboratory environments, wherein high-quality annotations (e.g., SMPL estimation from 3D scans or multi-view images) can be ideally provided. However, their annotating requirements are impractical for real-world images or videos, posing challenges toward real-world applications on current avatar creation methods. To this end, we propose the WildAvatar dataset, a web-scale in-the-wild human avatar creation dataset extracted from YouTube, with $10,000+$ different human subjects and scenes. WildAvatar is at least $10\times$ richer than previous datasets for 3D human avatar creation. We evaluate several state-of-the-art avatar creation methods on our dataset, highlighting the unexplored challenges in real-world applications on avatar creation. We also demonstrate the potential for generalizability of avatar creation methods, when provided with data at scale. We publicly release our data source links and annotations, to push forward 3D human avatar creation and other related fields for real-world applications.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.02165v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://wildavatar.github.io/">https://wildavatar.github.io/</a></p><p><strong>Summary</strong><br>提出了WildAvatar数据集，这是一个从YouTube中提取的规模庞大、真实场景中的人类头像创建数据集，比现有的3D人类头像创建数据集丰富至少10倍。</p><p><strong>Key Takeaways</strong></p><ul><li>现有的人类头像创建数据集通常仅限于实验室环境，难以适用于真实世界中的图像或视频。</li><li>WildAvatar数据集包含超过10,000个不同的人类主体和场景，远超过以往的数据集。</li><li>对该数据集进行评估揭示了当前头像创建方法在真实世界应用中面临的挑战。</li><li>研究展示了大规模数据对头像创建方法泛化能力的潜力。</li><li>研究团队公开了数据源链接和注释，推动3D人类头像创建及相关领域在真实世界应用中的发展。</li><li>提出了头像创建方法在实际应用中未开发的挑战。</li><li>强调了通过大规模数据的提供，头像创建方法的普适性可能性。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>好的，我会按照您的要求来总结这篇论文。以下是关于该论文的总结：</p><ol><li><p>标题：Web-scale In-the-wild Video Dataset for 3D Avatar Creation（用于创建三维个性化角色的网络规模野外视频数据集）。</p></li><li><p>作者：Zihao Huang（黄子豪），Shoukang Hu（胡寿康），Guangcong Wang（王光聪），Tianqi Liu（刘天琦），Yuhang Zang（藏玉杭），Zhiguo Cao（曹治国），Wei Li（李伟），Ziwei Liu（刘子炜）。</p></li><li><p>所属机构：黄子豪和王天琦来自华中科技大学，胡寿康、李伟和刘子炜来自南洋理工大学，王光聪来自海湾大学，藏玉杭的工作地点未提及。</p></li><li><p>关键词：WildAvatar数据集、三维个性化角色创建、在野视频数据集、大规模数据集、个性化角色创建方法评估。</p></li><li><p>Urls：论文链接为<a target="_blank" rel="noopener" href="https://wildavatar.github.io/，GitHub代码链接暂未提供。">https://wildavatar.github.io/，GitHub代码链接暂未提供。</a></p></li><li><p>总结：</p><ul><li><p>(1)研究背景：现有的个性化角色数据集大多局限于实验室环境，难以满足真实世界应用的需求。文章提出一种名为WildAvatar的大规模野外视频数据集，旨在解决这一难题。</p><p>-(2)过去的方法及问题：以往个性化角色创建数据集主要依赖于实验室环境的高质量标注数据，对于真实世界图像或视频的标注要求难以实现。因此，它们在实际应用中的表现受到限制。</p><p>-(3)研究方法：文章提出了WildAvatar数据集，该数据集从YouTube中提取大规模野外视频数据，包含超过10,000个不同的人类主体和场景。数据集至少比以前的3D个性化角色创建数据集丰富10倍，并提供了高质量标注。文章还评估了几种最先进的个性化角色创建方法在该数据集上的表现，强调了在实际应用中未被探索的挑战，并展示了大规模数据对个性化角色创建方法的泛化能力的影响。</p><p>-(4)任务与性能：文章主要在创建个性化角色的任务上评估了新方法，并通过大规模数据集验证了方法的有效性。与以往方法相比，文章提出的方法在实际应用中的性能表现更好，并展示了数据规模对提升模型泛化能力的重要性。文章还公开了数据源代码和标注，以推动个性化角色创建及其他相关领域在真实世界应用的发展。<br>好的，我会按照您的要求来总结这篇论文的方法部分。以下是关于该论文方法的总结：</p></li></ul></li><li><p>方法：</p></li></ol><p>(1) 数据收集：文章从YouTube上提取大规模野外视频数据，这些数据包含了超过10,000个不同的人类主体和场景。这种数据收集方式使得数据集更加接近真实世界的应用场景。</p><p>(2) 数据标注：文章提供了高质量的数据标注，这对于个性化角色创建是非常重要的。标注包括了人体姿态、面部表情、衣物纹理等信息，这些信息对于创建真实的个性化角色是必要的。</p><p>(3) 方法评估：文章评估了几种最先进的个性化角色创建方法在该数据集上的表现。通过对比实验，文章展示了大规模数据对个性化角色创建方法的泛化能力的影响，并强调了在实际应用中未被探索的挑战。</p><p>(4) 公开数据源代码和标注：文章公开了数据源代码和标注，以便其他研究者可以使用这些数据来推动个性化角色创建及其他相关领域的研究。这也是文章的一个重要贡献，可以推动该领域的进一步发展。</p><p>以上就是这篇论文的方法部分总结。</p><p>好的，根据您的要求，以下是对该文章总结得出的结论：</p><p>（第一部分回答）这篇工作的意义在于它提出了一种名为WildAvatar的大规模野外视频数据集，旨在解决个性化角色创建在真实世界应用中的难题。该数据集打破了以往个性化角色数据集局限于实验室环境的局限，使得研究者可以在更接近真实场景的数据上训练和测试他们的模型。这对于推动个性化角色创建技术的发展具有重要意义。此外，文章还评估了几种最先进的个性化角色创建方法在该数据集上的表现，强调了在实际应用中未被探索的挑战，展示了大规模数据对个性化角色创建方法的泛化能力的影响。因此，这篇工作的意义在于推动了个性化角色创建技术的发展和应用。同时公开的数据源代码和标注也使得其他研究者能够更容易地在此基础上进行研究和创新。总体而言，这篇工作的创新性、实用性、重要性等方面都具有重要意义。同时提供了一个广泛的评估平台以及进一步的挑战和探索空间。尽管工作具有一定的复杂性并且需要大量的资源去完成这项工作但是这些都是确保其实用性和广泛性的必要步骤。此外该研究也有助于推动相关领域如计算机视觉和人工智能的进步和发展。</p><p>（第二部分回答）创新点：该文章创新性地构建了一个大规模的野外视频数据集，涵盖大量真实场景的标注数据；采用这种新型数据集进行模型训练和测试提升了性能并更适用于真实应用情境。性能方面：文章中创建的新数据集能有效推动模型在实际应用中性能的提升，尤其是在个性化角色创建方面。同时文章通过对比实验揭示了大规模数据对模型泛化能力的影响，为后续研究提供了有价值的参考。工作量方面：文章构建的大规模数据集包含海量的视频数据且需要进行高质量的标注工作量巨大；同时实验设计以及评估过程也需要投入大量的时间和精力来完成；另外数据的收集和处理也是一项非常繁重的工作需要考虑数据的多样性和复杂性等问题。总体来说文章的工作量大而且非常具有挑战性需要在各个环节上付出极大的努力来确保整个项目的顺利进行并取得有价值的成果为后续的个性化和现实交互技术发展打下基础铺垫更多的可能。不过也有潜在的缺点比如在收集大规模数据过程中可能会存在质量参差不齐或者噪声数据的问题；并且在数据处理和分析方面还需要更多的优化和细节工作以提高模型的准确性和效率。尽管如此这项工作仍具有很高的价值推动了相关领域的发展和进步是值得关注和进一步研究的课题之一。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-4a5f997023a3e966f5af1eebd6b5d67b.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-2da06117942433682d578ba71609e8ce.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-12c8b95ea4bbed318d5561b2c5ce0a8e.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-c3473ef9fd65d6bc72ef8f47f3c9e7c0.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-8906d72e76ed7345bd2d64ad728c59ed.jpg" align="middle"></details></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io/2024/07/12/Paper/2024-07-12/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">https://kedreamix.github.io/2024/07/12/Paper/2024-07-12/元宇宙_虚拟人/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">元宇宙/虚拟人</a></div><div class="post_share"><div class="social-share" data-image="https://pic1.zhimg.com/v2-9e48453d77506272fa48d5700ae725b1.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/07/12/Paper/2024-07-12/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-a8ad4b666375cb5c86ca35bf2be7efdf.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Diffusion Models</div></div></a></div><div class="next-post pull-right"><a href="/2024/07/05/Paper/2024-07-05/NeRF/" title="NeRF"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-c19aea52c8a62496bf000ef5a8e942f3.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">NeRF</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/01/24/Paper/2024-01-24/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-0f13a2b60bef4c886a3317754c99b456.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-24</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/04/01/Paper/2024-04-01/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-4f2a829065c463be027e4b423c4e43c8.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-01</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/04/06/Paper/2024-04-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-4f97970c093585e18e2db42fb96a6b75.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-06</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/05/06/Paper/2024-05-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-65e21e3a0a320adc36f81e6bfc7c5739.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-06</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/05/02/Paper/2024-05-02/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-37516691b13dabbabb3b74ea46b402d8.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-02</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/05/22/Paper/2024-05-22/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-b4943be56cf94e02149b50eec541d27f.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-22</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-07-12-%E6%9B%B4%E6%96%B0"><span class="toc-text">2024-07-12 更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#MobilePortrait-Real-Time-One-Shot-Neural-Head-Avatars-on-Mobile-Devices"><span class="toc-text">MobilePortrait: Real-Time One-Shot Neural Head Avatars on Mobile Devices</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CanonicalFusion-Generating-Drivable-3D-Human-Avatars-from-Multiple-Images"><span class="toc-text">CanonicalFusion: Generating Drivable 3D Human Avatars from Multiple Images</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#WildAvatar-Web-scale-In-the-wild-Video-Dataset-for-3D-Avatar-Creation"><span class="toc-text">WildAvatar: Web-scale In-the-wild Video Dataset for 3D Avatar Creation</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image:url('https://pic1.zhimg.com/v2-9e48453d77506272fa48d5700ae725b1.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js").then((()=>{pangu.autoSpacingPage()}))}function panguInit(){panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.1},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script></div><script id="canvas_nest" defer color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><script data-pjax>function butterfly_swiper_injector_config(){var a=document.getElementById("recent-posts");console.log("已挂载butterfly_swiper"),a.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/25/Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="2024/01/25/Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="2024/01/25/Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/17/Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023/12/17/Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="2023/12/17/Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script></body></html>