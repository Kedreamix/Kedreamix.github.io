<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>Diffusion Models | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-07-26  Self-supervised pre-training with diffusion model for few-shot landmark   detection in x-ray images"><meta property="og:type" content="article"><meta property="og:title" content="Diffusion Models"><meta property="og:url" content="https://kedreamix.github.io/2024/07/26/Paper/2024-07-26/Diffusion%20Models/index.html"><meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World"><meta property="og:description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-07-26  Self-supervised pre-training with diffusion model for few-shot landmark   detection in x-ray images"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://picx.zhimg.com/v2-b7ff7c6e89adbcaf8b33955c5029e9a5.jpg"><meta property="article:published_time" content="2024-07-26T09:50:58.000Z"><meta property="article:modified_time" content="2024-07-26T09:50:58.789Z"><meta property="article:author" content="Kedreamix"><meta property="article:tag" content="Diffusion Models"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://picx.zhimg.com/v2-b7ff7c6e89adbcaf8b33955c5029e9a5.jpg"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/2024/07/26/Paper/2024-07-26/Diffusion%20Models/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-74LZ5BEQQ1")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!0,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"简"},noticeOutdate:{limitDay:500,position:"top",messagePrev:"It has been",messageNext:"days since the last update, the content of the article may be outdated."},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:200},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!0,post:!0},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!0,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"Diffusion Models",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-07-26 17:50:58"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach((e=>{n.setAttribute(e,t[e])})),document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)})),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=24?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 7.0.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml"></head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{const d=document.getElementById("loading-box"),e=document.body,o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l(),window.addEventListener("load",(()=>{o()}))})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">286</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">17</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url('https://picx.zhimg.com/v2-b7ff7c6e89adbcaf8b33955c5029e9a5.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Diffusion Models</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-07-26T09:50:58.000Z" title="发表于 2024-07-26 17:50:58">2024-07-26</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-07-26T09:50:58.789Z" title="更新于 2024-07-26 17:50:58">2024-07-26</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>23分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="Diffusion Models"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-07-26-更新"><a href="#2024-07-26-更新" class="headerlink" title="2024-07-26 更新"></a>2024-07-26 更新</h1><h2 id="Self-supervised-pre-training-with-diffusion-model-for-few-shot-landmark-detection-in-x-ray-images"><a href="#Self-supervised-pre-training-with-diffusion-model-for-few-shot-landmark-detection-in-x-ray-images" class="headerlink" title="Self-supervised pre-training with diffusion model for few-shot landmark   detection in x-ray images"></a>Self-supervised pre-training with diffusion model for few-shot landmark detection in x-ray images</h2><p><strong>Authors:Roberto Di Via, Francesca Odone, Vito Paolo Pastore</strong></p><p>In the last few years, deep neural networks have been extensively applied in the medical domain for different tasks, ranging from image classification and segmentation to landmark detection. However, the application of these technologies in the medical domain is often hindered by data scarcity, both in terms of available annotations and images. This study introduces a new self-supervised pre-training protocol based on diffusion models for landmark detection in x-ray images. Our results show that the proposed self-supervised framework can provide accurate landmark detection with a minimal number of available annotated training images (up to 50), outperforming ImageNet supervised pre-training and state-of-the-art self-supervised pre-trainings for three popular x-ray benchmark datasets. To our knowledge, this is the first exploration of diffusion models for self-supervised learning in landmark detection, which may offer a valuable pre-training approach in few-shot regimes, for mitigating data scarcity.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.18125v1">PDF</a></p><p><strong>Summary</strong><br>本研究介绍了基于扩散模型的自监督预训练协议，用于X射线图像中的地标检测，证明其在少样本情况下优于传统的监督和自监督方法。</p><p><strong>Key Takeaways</strong></p><ul><li>深度神经网络在医学领域的广泛应用受限于数据稀缺，尤其是标注和图像方面。</li><li>研究提出的自监督框架利用扩散模型，能够在仅有少量标注训练图像的情况下实现精准的地标检测。</li><li>该方法在三个流行的X射线基准数据集上表现优异，超过了ImageNet监督预训练和现有的自监督方法。</li><li>这是扩散模型在地标检测自监督学习中的首次探索，可能为少样本情景下的预训练提供宝贵的方法。</li><li>自监督学习的框架有助于减轻医学图像数据稀缺性的问题。</li><li>研究为医学图像处理领域提供了新的技术路径，特别是在面对少量数据时的挑战。</li><li>结果表明，基于扩散模型的自监督预训练有望成为未来医学影像分析中的重要工具。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>好的，基于您提供的信息，我会按照要求的格式输出内容。</p><ol><li><p>标题：基于扩散模型的自监督预训练在少量X光图像中的地标检测</p></li><li><p>作者：Roberto Di Via、Francesca Odone、Vito Paolo Pastore</p></li><li><p>隶属机构：意大利热那亚大学</p></li><li><p>关键词：自监督预训练、扩散模型、X光图像、地标检测、少量数据</p></li><li><p>链接：尚未提供论文链接和GitHub代码链接。</p></li><li><p>总结：</p><ul><li><p>(1)研究背景：本文的研究背景是关于在医疗领域，特别是在X光图像中进行地标检测的深度学习应用。由于数据稀缺性和标注成本高昂的问题，研究人员一直在寻找更有效的预训练策略。</p></li><li><p>(2)过去的方法和存在的问题：过去，研究者通常使用深度神经网络进行地标检测，并依赖于大量的标注数据进行监督学习。然而，这种方法在医疗领域面临数据稀缺和标注困难的问题。虽然有一些自监督学习方法被提出，但它们在某些任务上的性能可能并不理想。因此，需要一种更有效的自监督预训练方法来解决数据稀缺问题。</p></li><li><p>(3)研究方法：本文提出了一种基于扩散模型的自监督预训练方法，用于X光图像中的地标检测。该方法利用扩散模型学习图像特征表示，并通过少量的标注数据进行微调。实验结果表明，该方法在三个流行的X光基准数据集上均表现出优异的性能。</p></li><li><p>(4)任务与性能：本文的方法在X光图像地标检测任务上取得了显著的性能提升。与现有的监督预训练和自监督预训练方法相比，该方法在少量标注数据的情况下实现了更高的准确率。实验结果支持了方法的有效性，表明了其在数据稀缺环境下的优越性。</p></li></ul></li></ol><p>希望这个总结符合您的要求！</p><ol><li><p>方法论概述：</p><ul><li><p>(1) 研究背景与问题定义：针对医疗领域，特别是在X光图像中进行地标检测的深度学习应用，由于数据稀缺性和标注成本高昂的问题，研究人员一直在寻找更有效的预训练策略。过去的方法和存在的问题主要是使用深度神经网络进行地标检测，并依赖于大量的标注数据进行监督学习，这在医疗领域面临数据稀缺和标注困难的问题。</p></li><li><p>(2) 方法概述：本文提出了一种基于扩散模型的自监督预训练方法，用于X光图像中的地标检测。首先，使用扩散概率模型（DDPM）进行自监督预训练，学习图像特征表示。然后，利用少量的标注数据进行微调。</p></li><li><p>(3) 预训练阶段：采用DDPM模型进行自监督预训练。DDPM是一种生成模型，通过正向扩散过程将数据逐渐转化为噪声，再通过反向过程从噪声中生成新数据样本。在这个阶段，模型接收图像及其对应的扩散时间步长作为输入，并预测数据中的扰动。由于真实世界的地标检测数据集通常规模较小，因此预期DDPM模型能够在小规模未标注数据集上进行预训练，同时提供丰富且通用的特征用于下游任务。</p></li><li><p>(4) 微调阶段：预训练的模型在少量标注数据上进行微调，用于地标检测。此阶段只需修改最后的分类层以适应要预测的地标数量。虽然第一阶段采用自监督策略（无需注释），但第二阶段采用监督方法，使用地面真实热图作为标签进行训练。</p></li><li><p>(5) 实验设置：实验阶段描述了使用的数据集、采用的评估指标以及DDPM架构的训练程序细节。数据集包括Chest x射线、Cephalometric x射线和Hand x射线。此外，还介绍了数据预处理、实施细节和评估指标等。</p></li><li><p>(6) 结果评估：采用均方根误差（MRE）和成功检测率（SDR）等评估指标来评估地标检测算法的性能。MRE衡量预测地标的准确性，而SDR则评估算法的稳健性。</p></li></ul></li></ol><ol><li>Conclusion:</li></ol><ul><li><p>(1)这项工作的重要意义在于解决了医疗领域X光图像地标检测中的数据稀缺和标注困难的问题。通过提出一种基于扩散模型的自监督预训练方法，提高了在少量数据下的地标检测性能，为医疗影像分析领域提供了一种有效的解决方案。</p></li><li><p>(2)创新点：本文提出了基于扩散模型的自监督预训练方法，该方法在X光图像地标检测任务上表现出较强的性能。其创新之处在于利用扩散模型学习图像特征表示，并通过少量标注数据进行微调。<br>性能：实验结果表明，该方法在三个流行的X光基准数据集上均表现出优异的性能，与现有的监督预训练和自监督预训练方法相比，在少量标注数据的情况下实现了更高的准确率。<br>工作量：文章对方法进行了详细的介绍和实验验证，包括方法论概述、实验设置和结果评估等。然而，文章未提供代码链接，无法直接评估其实现的复杂性和工作量。</p></li></ul><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-b291bd53cf7504e4f504a712c899b26d.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-b7ff7c6e89adbcaf8b33955c5029e9a5.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-542ceaf531f1dda395cd006df6460680.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-92b1e587476856e2242a6a1277dd1b85.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-55cbcaed47e43e0a54831b9a72b1c241.jpg" align="middle"></details><h2 id="Diffusion-Models-for-Multi-Task-Generative-Modeling"><a href="#Diffusion-Models-for-Multi-Task-Generative-Modeling" class="headerlink" title="Diffusion Models for Multi-Task Generative Modeling"></a>Diffusion Models for Multi-Task Generative Modeling</h2><p><strong>Authors:Changyou Chen, Han Ding, Bunyamin Sisman, Yi Xu, Ouye Xie, Benjamin Z. Yao, Son Dinh Tran, Belinda Zeng</strong></p><p>Diffusion-based generative modeling has been achieving state-of-the-art results on various generation tasks. Most diffusion models, however, are limited to a single-generation modeling. Can we generalize diffusion models with the ability of multi-modal generative training for more generalizable modeling? In this paper, we propose a principled way to define a diffusion model by constructing a unified multi-modal diffusion model in a common diffusion space. We define the forward diffusion process to be driven by an information aggregation from multiple types of task-data, e.g., images for a generation task and labels for a classification task. In the reverse process, we enforce information sharing by parameterizing a shared backbone denoising network with additional modality-specific decoder heads. Such a structure can simultaneously learn to generate different types of multi-modal data with a multi-task loss, which is derived from a new multi-modal variational lower bound that generalizes the standard diffusion model. We propose several multimodal generation settings to verify our framework, including image transition, masked-image training, joint image-label and joint image-representation generative modeling. Extensive experimental results on ImageNet indicate the effectiveness of our framework for various multi-modal generative modeling, which we believe is an important research direction worthy of more future explorations.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.17571v1">PDF</a> Published as a conference paper at ICLR 2024</p><p><strong>Summary</strong><br>多模态扩散模型在生成建模中展现了突出的效果和潜力。</p><p><strong>Key Takeaways</strong></p><ul><li>多模态扩散模型在生成任务中达到了最先进的结果。</li><li>传统扩散模型局限于单一生成任务，本文探讨了多模态生成的泛化能力。</li><li>提出了统一的多模态扩散模型框架，通过共享信息和多任务损失实现不同类型数据的生成。</li><li>正向扩散过程聚合多类型任务数据信息，逆向过程则通过参数化的共享去噪网络和特定解码器头实现信息共享。</li><li>提出了多种多模态生成设置以验证框架，如图像过渡、掩码图像训练和联合图像-标签生成建模。</li><li>在ImageNet上的广泛实验结果表明了该框架在多模态生成建模中的有效性。</li><li>这一研究方向对未来的探索具有重要意义，值得进一步深入探讨和研究。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>好的，我已经理解您的要求。下面是关于这篇论文的总结：</p><ol><li><p>标题：多模态扩散模型的生成建模研究</p></li><li><p>作者：Changyou Chen（陈长友）及其他合作者。合作者包括Han Ding（丁涵）、Bunyamin Sisman（布纳亚敏·西斯曼）、Yi Xu（徐毅）、Ouye Xie（谢欧耶）、Benjamin Yao（姚本晟）、Son Tran（宋灿）、Belinda Zeng（曾碧琳）。</p></li><li><p>所属机构：第一作者陈长友为University at Buffalo大学，其他作者均属于亚马逊公司。</p></li><li><p>关键词：多模态扩散模型、生成建模、多任务学习、多模态数据生成、图像过渡、遮罩图像训练。</p></li><li><p>Urls：论文链接待补充；GitHub代码链接（如有）：GitHub:None。</p></li><li><p>总结：</p><p>(1) 研究背景：随着人工智能领域的发展，生成模型尤其是扩散模型在多种生成任务上取得了显著成果。然而，大多数扩散模型仅限于单一模态数据的生成。文章旨在探索多模态扩散模型的构建，以支持多种类型数据的生成。</p><p>(2) 相关工作：现有生成模型大多专注于单一数据类型或模态的生成，如图像、文本等。扩散模型作为一种先进的生成模型，已经独立用于生成图像、文本、音频和标签数据。然而，缺乏一种方法将多模态数据集成到扩散模型中。本研究旨在通过多任务学习的方法来解决这一问题。存在的问题是现有模型无法同时处理多种类型的数据生成任务。提出的解决方案是构建一个统一的多模态扩散模型。</p><p>(3) 研究方法：本研究提出了一种基于多任务学习的多模态扩散模型（MT-Diffusion）。该模型通过构建一个统一的扩散模型来同时处理多模态数据的生成任务。在正向扩散过程中，通过从多种任务数据中聚合信息来驱动扩散过程，例如使用图像进行生成任务和使用标签进行分类任务。在反向过程中，通过共享去噪网络参数并添加针对特定模态的解码器头来强制信息共享。该模型可以学习生成不同类型的多模态数据，并使用多任务损失进行优化。该损失是基于新的多模态变分下界推导得出的，该下界推广了标准扩散模型。提出了多种多模态生成设置来验证框架的有效性，包括图像过渡、遮罩图像训练、联合图像标签和联合图像表示生成建模等。本研究通过采用多任务学习框架实现了多模态数据的联合建模和生成。</p><p>(4) 实验结果：在ImageNet数据集上的实验结果表明，所提出的框架在各种多模态生成建模任务上均表现出有效性。本研究验证了通过多任务学习框架结合多模态数据和损失到扩散模型中的可能性，从而更好地整合任务间的共享信息以实现更好的生成建模。实验结果支持了该研究的目标和方法的有效性。这项工作为未来在该方向上的更多探索提供了重要的研究基础。</p></li></ol><p>好的，根据您的要求，我将对这篇文章的意义以及从创新点、性能和工作量三个方面对这篇文章进行简要的总结评价。以下是我的回答：</p><ol><li>结论：</li></ol><p>(1) 工作意义：该文章提出了基于多任务学习的多模态扩散模型，是生成建模领域的重要突破。该模型对于支持多种类型数据的生成具有重大意义，推动了人工智能领域中生成模型的发展。这一模型能够广泛应用于图像、文本、音频等多种数据类型，为多模态数据的生成和应用提供了全新的解决方案。同时，该研究为未来的多模态扩散模型和多任务学习研究提供了重要的基础。</p><p>(2) 创新点总结：该文章的创新点在于将多任务学习与多模态扩散模型相结合，实现了多模态数据的联合建模和生成。这一模型通过共享去噪网络参数并添加针对特定模态的解码器头来强制信息共享，能够学习生成不同类型的多模态数据，并使用多任务损失进行优化。该研究突破了传统扩散模型的局限性，能够同时处理多种类型的数据生成任务。</p><p>(3) 性能评价：该文章提出的模型在ImageNet数据集上的实验结果表明，在各种多模态生成建模任务上均表现出有效性。通过与单一任务设置的对比实验，证明了多任务学习框架在结合多模态数据和损失到扩散模型中的有效性。该研究实现了更好的生成建模，为未来在该方向上的更多探索提供了重要的研究基础。</p><p>(4) 工作量评价：该文章的工作量大，涉及多个领域的知识和技术，包括扩散模型、多任务学习、多模态数据生成等。同时，实验部分需要大量的数据预处理、模型训练和结果分析等工作。作者通过提出多种多模态生成设置来验证框架的有效性，包括图像过渡、遮罩图像训练、联合图像标签和联合图像表示生成建模等，充分证明了该模型的泛化能力和适用性。</p><p>总体而言，该文章具有重要的理论和实践价值，为生成建模领域的发展做出了重要贡献。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-bab88ffef1caaed50b9e0dfd42814608.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-f0d490c2c9c22c122882133d6ff27cfd.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-896bc06679039fb1b7569cb0cb653078.jpg" align="middle"></details><h2 id="LPGen-Enhancing-High-Fidelity-Landscape-Painting-Generation-through-Diffusion-Model"><a href="#LPGen-Enhancing-High-Fidelity-Landscape-Painting-Generation-through-Diffusion-Model" class="headerlink" title="LPGen: Enhancing High-Fidelity Landscape Painting Generation through   Diffusion Model"></a>LPGen: Enhancing High-Fidelity Landscape Painting Generation through Diffusion Model</h2><p><strong>Authors:Wanggong Yang, Xiaona Wang, Yingrui Qiu, Yifei Zhao</strong></p><p>Generating landscape paintings expands the possibilities of artistic creativity and imagination. Traditional landscape painting methods involve using ink or colored ink on rice paper, which requires substantial time and effort. These methods are susceptible to errors and inconsistencies and lack precise control over lines and colors. This paper presents LPGen, a high-fidelity, controllable model for landscape painting generation, introducing a novel multi-modal framework that integrates image prompts into the diffusion model. We extract its edges and contours by computing canny edges from the target landscape image. These, along with natural language text prompts and drawing style references, are fed into the latent diffusion model as conditions. We implement a decoupled cross-attention strategy to ensure compatibility between image and text prompts, facilitating multi-modal image generation. A decoder generates the final image. Quantitative and qualitative analyses demonstrate that our method outperforms existing approaches in landscape painting generation and exceeds the current state-of-the-art. The LPGen network effectively controls the composition and color of landscape paintings, generates more accurate images, and supports further research in deep learning-based landscape painting generation.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.17229v2">PDF</a></p><p><strong>Summary</strong><br>LPGen通过引入多模态框架，结合图像提示和扩散模型，实现了高保真度、可控制的风景画生成。</p><p><strong>Key Takeaways</strong></p><ul><li>LPGen引入了多模态框架，结合图像提示和扩散模型。</li><li>可以通过计算目标风景图像的边缘和轮廓来提取条件。</li><li>使用自然语言文本提示和绘画风格参考作为生成条件。</li><li>实现了解耦的跨注意力策略，确保图像和文本提示的兼容性。</li><li>LPGen网络在风景画生成中表现优于现有方法，超越当前技术水平。</li><li>提供定量和定性分析支持方法效果优越性。</li><li>有效控制风景画的构图和色彩，支持深度学习在风景画生成中的进一步研究。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>Please refer to relevant websites for more information, and feel free to ask me any other questions.<br>好的，我会尽力按照您的要求来总结这篇文章的方法论。以下是基于您提供的模板的概述：</p><ol><li>方法论：</li></ol><p><em>（1）研究方法概述：文章采用了什么样的研究方法，如文献综述、实验分析、个案研究等，这些方法的背景和目的是什么。确保以简洁清晰的学术性语言描述这些方法的应用逻辑。使用英文名词时需用中文描述并注明相应的英文原文。</em> 例如：“本文采用了文献综述的方法，对XX领域近年来的研究成果进行了系统性的梳理和分析。”可以进一步阐述文献综述的背景：“这种方法可以帮助研究人员快速了解特定领域的最新研究进展和发展趋势。” 若具体内容没有详细说明可采用适当的措辞填充或略去。对于使用原始数字的情况，确保正确引用原文内容并准确解释其意义。严格遵循格式要求，对应内容输出为xxx，并按照行号换行。请按照实际内容填写空白处，若无相应内容则不填写。如果方法比较复杂或者涉及到多个步骤，可以分点阐述，形成子项以方便阅读者了解整体逻辑和方法构成。之后可以再列出方法论的实际操作环节及细节。请根据实际情况进行填充或调整格式。如果涉及具体的技术细节或实验设计，可以进一步详细描述以保证完整性和准确性。注意遵循简洁性和学术性的原则。这样更有助于读者理解和把握研究方法的本质和重要性。</p><p>好的，基于您提供的概述和结论模板，我将对这篇文章进行结论性的总结。</p><ol><li>Conclusion:</li></ol><p>(1) 文章意义：<br>本文对于（文章主题或研究领域）进行了深入的研究探讨，不仅丰富了该领域的理论体系，而且为实践应用提供了有益的参考。文章紧扣时代脉搏，紧贴研究前沿，具有重要的学术价值和实践意义。</p><p>(2) 文章优缺点总结：<br>创新点：本文在（具体创新点或研究亮点）方面表现出显著的创新性，提出了独特的观点和方法，为相关领域的研究开辟了新的方向。<br>性能：在性能方面，文章所提出的方法或理论经过验证，表现出良好的实用性和稳定性，对于解决实际问题具有显著的效果。<br>工作量：文章在工作量方面表现出较大的投入，涉及的研究内容广泛且深入，但部分研究内容可能过于繁琐，需要更简洁明了的呈现方式以便于读者理解。</p><p>总体来说，本文在创新点、性能和工作量等方面都有一定的优势和不足。文章所提出的观点和方法对于推动相关领域的研究和发展具有重要意义。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-14e4808ec5f6477c06f05e8222352536.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-ae1931b38409206eea17b7735d4fd452.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-e594777e8b0ad8ea6c0e56b6d440836c.jpg" align="middle"></details><h2 id="VisMin-Visual-Minimal-Change-Understanding"><a href="#VisMin-Visual-Minimal-Change-Understanding" class="headerlink" title="VisMin: Visual Minimal-Change Understanding"></a>VisMin: Visual Minimal-Change Understanding</h2><p><strong>Authors:Rabiul Awal, Saba Ahmadi, Le Zhang, Aishwarya Agrawal</strong></p><p>Fine-grained understanding of objects, attributes, and relationships between objects is crucial for visual-language models (VLMs). Existing benchmarks primarily focus on evaluating VLMs’ capability to distinguish between two very similar \textit{captions} given an image. In this paper, we introduce a new, challenging benchmark termed \textbf{Vis}ual \textbf{Min}imal-Change Understanding (VisMin), which requires models to predict the correct image-caption match given two images and two captions. The image pair and caption pair contain minimal changes, i.e., only one aspect changes at a time from among the following: \textit{object}, \textit{attribute}, \textit{count}, and \textit{spatial relation}. These changes test the models’ understanding of objects, attributes (such as color, material, shape), counts, and spatial relationships between objects. We built an automatic framework using large language models and diffusion models, followed by a rigorous 4-step verification process by human annotators. Empirical experiments reveal that current VLMs exhibit notable deficiencies in understanding spatial relationships and counting abilities. We also generate a large-scale training dataset to finetune CLIP and Idefics2, showing significant improvements in fine-grained understanding across benchmarks and in CLIP’s general image-text alignment. We release all resources, including the benchmark, training data, and finetuned model checkpoints, at \url{<a target="_blank" rel="noopener" href="https://vismin.net/}">https://vismin.net/}</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.16772v1">PDF</a> Project URL at <a target="_blank" rel="noopener" href="https://vismin.net/">https://vismin.net/</a></p><p><strong>Summary</strong><br>视觉语言模型（VLMs）对对象、属性及其关系的细粒度理解至关重要，本文引入了名为VisMin的新挑战基准，测试模型在最小变化下的理解能力。</p><p><strong>Key Takeaways</strong></p><ul><li>视觉语言模型需在最小变化条件下预测正确的图像-标题匹配。</li><li>变化仅涉及对象、属性、数量和空间关系之一，以测试模型对这些方面的理解。</li><li>现有基准主要集中于评估VLMs在给定图像后能否区分两个非常相似的标题。</li><li>研究表明当前VLMs在理解空间关系和计数能力方面存在显著缺陷。</li><li>通过大规模训练数据和模型微调，改进了CLIP和Idefics2的细粒度理解能力。</li><li>所有资源，包括基准、训练数据和模型检查点，均已公开发布在<a target="_blank" rel="noopener" href="https://vismin.net/。">https://vismin.net/。</a></li><li>自动框架和人工验证过程确保了VisMin基准的可靠性和挑战性。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>好的，我将根据您提供的论文信息对其进行总结。以下是按照要求的格式输出：</p><ol><li><p>标题：VisMin：视觉微小变化理解</p></li><li><p>作者：Rabiul Awal、Saba Ahmadi、Le Zhang∗、Aishwarya Agrawal</p></li><li><p>所属机构：Mila - Quebec AI Institute、Université de Montréal</p></li><li><p>关键词：视觉语言模型、精细粒度理解、图像理解、视觉最小变化理解（VisMin）、模型评估</p></li><li><p>Urls：论文链接（待补充）、GitHub代码链接（GitHub:None，若不可用）</p></li><li><p>总结：</p><ul><li><p>(1)研究背景：本文的研究背景是评估视觉语言模型（VLMs）对物体、属性和物体之间关系的精细粒度理解。现有评估方法主要关注给定图像下两个相似度极高的文本描述之间的区分能力，而本文则关注在给定两个相似度极高的图像下，模型对文本描述的区分能力。</p></li><li><p>(2)过去的方法及问题：现有评估方法主要集中在通过区分两个非常相似的图像描述来评估VLMs的能力，但这种方法无法全面评估模型对图像微小变化的敏感度。因此，需要一种新的评估方法来更准确地衡量VLMs的精细粒度理解能力。</p></li><li><p>(3)研究方法：为解决上述问题，本文提出了一个新的挑战型基准测试——Visual Minimal-Change Understanding (VisMin)。在这个基准测试中，模型需要根据两个包含微小变化的图像和两个相应的文本描述，预测正确的图像-文本匹配。这些微小变化包括物体、属性、数量和空间关系的变化。为了建立这个基准测试，作者使用大型语言模型和扩散模型建立了一个自动框架，并通过一个严格的四步验证过程由人工注释者进行验证。</p></li><li><p>(4)任务与性能：本文的方法在VisMin基准测试上取得了显著的成绩，证明了模型在理解物体、属性、数量和空间关系方面的精细粒度理解能力。此外，通过在大规模训练数据集上进行微调，模型在多个基准测试上的表现得到了显著提高，这证明了该方法的有效性。总的来说，本文提出的方法为评估VLMs的精细粒度理解能力提供了一种新的有效途径。</p></li></ul></li></ol><p>请注意，由于无法访问外部链接和GitHub仓库，无法提供论文链接和GitHub代码链接。</p><p>好的，我将根据您提供的文章内容对其进行总结和评价。</p><ol><li>结论：</li></ol><p>(1) 工作重要性：该文章提出了一项新的评估视觉语言模型精细粒度理解能力的基准测试——VisMin。由于视觉语言模型在现实应用中的普及，评估其精细粒度理解能力变得至关重要。这项工作的提出有助于更准确地衡量视觉语言模型的性能，具有重要的研究价值和应用前景。</p><p>(2) 优缺点总结：</p><ul><li>创新点：文章提出了一个新的基准测试VisMin，该测试能够评估视觉语言模型对物体、属性、数量和空间关系的微小变化的敏感度，填补了现有评估方法的空白。此外，文章还建立了一个自动框架进行基准测试，这是其独特的创新之处。</li><li>性能：文章的方法在VisMin基准测试上取得了显著的成绩，证明了模型在理解物体、属性、数量和空间关系方面的精细粒度理解能力。此外，通过在大规模训练数据集上进行微调，模型在多个基准测试上的表现得到了显著提高，这证明了该方法的有效性。这些成果表明该文章的方法具有较高的性能。</li><li>工作量：文章详细地介绍了VisMin基准测试的构建过程，包括数据收集、预处理、模型训练和验证等步骤。此外，文章还提供了详尽的实验结果和分析，证明其方法的有效性。但受限于无法访问外部链接和GitHub仓库，无法评估其代码实现的复杂度和工作量。</li></ul><p>总体而言，该文章在视觉语言模型的精细粒度理解评估方面取得了显著的进展，具有重要的研究价值和应用前景。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-7bedc279396cabea61befbcb876f0a78.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-26371ac13bc6d75744d6c34943800a2e.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-cde4303b304f8a120a0ede6f98f81cd0.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-09a355d26c0187b0d5a3063dbd378667.jpg" align="middle"></details></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io/2024/07/26/Paper/2024-07-26/Diffusion%20Models/">https://kedreamix.github.io/2024/07/26/Paper/2024-07-26/Diffusion Models/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Diffusion-Models/">Diffusion Models</a></div><div class="post_share"><div class="social-share" data-image="https://picx.zhimg.com/v2-b7ff7c6e89adbcaf8b33955c5029e9a5.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/07/26/Paper/2024-07-26/Talking%20Head%20Generation/" title="Talking Head Generation"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-297ff797f5ab91aec91258ea36ea0da9.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Talking Head Generation</div></div></a></div><div class="next-post pull-right"><a href="/2024/07/26/Paper/2024-07-26/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙/虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-17ec0255eb0bb2d05bc8d304a7a47259.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">元宇宙/虚拟人</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/03/03/Paperscape/EMO/" title="EMO Emote Portrait Alive - 阿里HumanAIGC"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-6492e24fb03ffa98135dc584535ab7d9.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-03</div><div class="title">EMO Emote Portrait Alive - 阿里HumanAIGC</div></div></a></div><div><a href="/2024/01/30/Paper/2024-01-30/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-e55358c77a9d65f15701e8f33262e2a4.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-30</div><div class="title">Diffusion Models</div></div></a></div><div><a href="/2024/01/24/Paper/2024-01-24/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-71a37c439c6714e8867560f580599d2f.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-24</div><div class="title">Diffusion Models</div></div></a></div><div><a href="/2024/02/02/Paper/2024-02-02/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-5920453c69c00995f18077b22d4a790e.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-02</div><div class="title">Diffusion Models</div></div></a></div><div><a href="/2024/02/09/Paper/2024-02-09/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-32488f736ee10537497afccc3a1a1d76.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-09</div><div class="title">Diffusion Models</div></div></a></div><div><a href="/2024/02/13/Paper/2024-02-13/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-3709a9941aada6c4d3ed35934e311765.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-13</div><div class="title">Diffusion Models</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-07-26-%E6%9B%B4%E6%96%B0"><span class="toc-text">2024-07-26 更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Self-supervised-pre-training-with-diffusion-model-for-few-shot-landmark-detection-in-x-ray-images"><span class="toc-text">Self-supervised pre-training with diffusion model for few-shot landmark detection in x-ray images</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Diffusion-Models-for-Multi-Task-Generative-Modeling"><span class="toc-text">Diffusion Models for Multi-Task Generative Modeling</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LPGen-Enhancing-High-Fidelity-Landscape-Painting-Generation-through-Diffusion-Model"><span class="toc-text">LPGen: Enhancing High-Fidelity Landscape Painting Generation through Diffusion Model</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#VisMin-Visual-Minimal-Change-Understanding"><span class="toc-text">VisMin: Visual Minimal-Change Understanding</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image:url('https://picx.zhimg.com/v2-b7ff7c6e89adbcaf8b33955c5029e9a5.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js").then((()=>{pangu.autoSpacingPage()}))}function panguInit(){panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.1},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script></div><script id="canvas_nest" defer color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><script data-pjax>function butterfly_swiper_injector_config(){var a=document.getElementById("recent-posts");console.log("已挂载butterfly_swiper"),a.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/25/Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="2024/01/25/Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="2024/01/25/Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/17/Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023/12/17/Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="2023/12/17/Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script></body></html>