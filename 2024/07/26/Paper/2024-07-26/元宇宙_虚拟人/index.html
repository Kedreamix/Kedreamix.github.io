<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>元宇宙/虚拟人 | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="元宇宙&#x2F;虚拟人 方向最新论文已更新，请持续关注 Update in 2024-07-26  Universal Facial Encoding of Codec Avatars from VR Headsets"><meta property="og:type" content="article"><meta property="og:title" content="元宇宙&#x2F;虚拟人"><meta property="og:url" content="https://kedreamix.github.io/2024/07/26/Paper/2024-07-26/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/index.html"><meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World"><meta property="og:description" content="元宇宙&#x2F;虚拟人 方向最新论文已更新，请持续关注 Update in 2024-07-26  Universal Facial Encoding of Codec Avatars from VR Headsets"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://pic1.zhimg.com/v2-17ec0255eb0bb2d05bc8d304a7a47259.jpg"><meta property="article:published_time" content="2024-07-26T09:36:45.000Z"><meta property="article:modified_time" content="2024-07-26T09:36:45.635Z"><meta property="article:author" content="Kedreamix"><meta property="article:tag" content="元宇宙&#x2F;虚拟人"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://pic1.zhimg.com/v2-17ec0255eb0bb2d05bc8d304a7a47259.jpg"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/2024/07/26/Paper/2024-07-26/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-74LZ5BEQQ1")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!0,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"简"},noticeOutdate:{limitDay:500,position:"top",messagePrev:"It has been",messageNext:"days since the last update, the content of the article may be outdated."},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:200},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!0,post:!0},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!0,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"元宇宙/虚拟人",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-07-26 17:36:45"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach((e=>{n.setAttribute(e,t[e])})),document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)})),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=24?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 7.0.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml"></head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{const d=document.getElementById("loading-box"),e=document.body,o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l(),window.addEventListener("load",(()=>{o()}))})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">200</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url('https://pic1.zhimg.com/v2-17ec0255eb0bb2d05bc8d304a7a47259.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">元宇宙/虚拟人</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-07-26T09:36:45.000Z" title="发表于 2024-07-26 17:36:45">2024-07-26</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-07-26T09:36:45.635Z" title="更新于 2024-07-26 17:36:45">2024-07-26</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">11.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>36分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="元宇宙/虚拟人"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-07-26-更新"><a href="#2024-07-26-更新" class="headerlink" title="2024-07-26 更新"></a>2024-07-26 更新</h1><h2 id="Universal-Facial-Encoding-of-Codec-Avatars-from-VR-Headsets"><a href="#Universal-Facial-Encoding-of-Codec-Avatars-from-VR-Headsets" class="headerlink" title="Universal Facial Encoding of Codec Avatars from VR Headsets"></a>Universal Facial Encoding of Codec Avatars from VR Headsets</h2><p><strong>Authors:Shaojie Bai, Te-Li Wang, Chenghui Li, Akshay Venkatesh, Tomas Simon, Chen Cao, Gabriel Schwartz, Ryan Wrench, Jason Saragih, Yaser Sheikh, Shih-En Wei</strong></p><p>Faithful real-time facial animation is essential for avatar-mediated telepresence in Virtual Reality (VR). To emulate authentic communication, avatar animation needs to be efficient and accurate: able to capture both extreme and subtle expressions within a few milliseconds to sustain the rhythm of natural conversations. The oblique and incomplete views of the face, variability in the donning of headsets, and illumination variation due to the environment are some of the unique challenges in generalization to unseen faces. In this paper, we present a method that can animate a photorealistic avatar in realtime from head-mounted cameras (HMCs) on a consumer VR headset. We present a self-supervised learning approach, based on a cross-view reconstruction objective, that enables generalization to unseen users. We present a lightweight expression calibration mechanism that increases accuracy with minimal additional cost to run-time efficiency. We present an improved parameterization for precise ground-truth generation that provides robustness to environmental variation. The resulting system produces accurate facial animation for unseen users wearing VR headsets in realtime. We compare our approach to prior face-encoding methods demonstrating significant improvements in both quantitative metrics and qualitative results.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.13038v1">PDF</a> SIGGRAPH 2024 (ACM Transactions on Graphics (TOG))</p><p><strong>Summary</strong><br>实时面部动画对虚拟现实中通过化身进行远程交流至关重要。</p><p><strong>Key Takeaways</strong></p><ul><li>实时面部动画对于在虚拟现实中模拟真实交流至关重要。</li><li>需要能够在几毫秒内捕捉到极端和微妙的表情。</li><li>面部斜视和不完整视角是挑战之一。</li><li>头戴式设备的可变性和环境光照变化也是通用性的障碍。</li><li>自监督学习方法有助于泛化到未知用户。</li><li>轻量级表情校准机制提高了准确性。</li><li>改进的参数化提供了对环境变化的稳健性。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: 基于VR头盔的通用面部编码技术应用于编解码化身的研究</p></li><li><p>Authors: 白少杰, 王特力, 李成辉, 文凯士, 西蒙·托马斯, 曹琛, 施瓦茨·加布里埃尔, 瑞恩·韦奇奇普, 萨拉吉·杰森, 谢赫·亚瑟, 魏世恩等。</p></li><li><p>Affiliation: 白少杰等作者来自Meta公司的Codec Avatars实验室，美国。</p></li><li><p>Keywords: VR面部编码，通用面部动画，实时编解码化身，虚拟现实，面部感知，头戴设备传感器等。</p></li><li><p>Urls: <a href="#">论文链接</a> <a href="GitHub:None">GitHub代码链接</a></p></li><li><p>Summary:</p></li></ol><p>(1) 研究背景：随着虚拟现实技术的快速发展，如何通过VR头盔实现高质量的面部动画，从而提供逼真的虚拟化身体验成为了一个重要的研究方向。本文旨在解决这一难题，提出了一种基于VR头盔的通用面部编码技术应用于编解码化身的方法。</p><p>(2) 过去的方法及问题：目前存在的面部动画方法往往无法同时满足保真度、实时性和泛化能力的要求。它们可能在某些条件下表现良好，但在其他条件下则存在失真、延迟或无法适应不同用户的问题。本文提出的方法旨在解决这些问题。</p><p>(3) 研究方法：本文提出了一种基于自监督学习的面部编码方法，通过头显摄像头采集的数据进行实时面部动画。该方法包括一个基于跨视角重建目标的自监督学习策略，一个提高准确性的轻量级表情校准机制，以及一个针对环境变化的改进参数化方法。</p><p>(4) 任务与性能：本文的方法在基于VR头盔的实时面部编码任务上取得了显著成果。通过对比实验，证明了该方法在定量和定性结果上均优于现有方法。此外，该方法在真实环境下的性能表现支持了其在实际应用中的有效性。</p><ol><li>Conclusion:</li></ol><ul><li>(1) 研究的意义：该工作具有重要的实际应用价值。随着虚拟现实技术的普及，高质量的面部动画对于提供逼真的虚拟化身体验至关重要。该研究提出的基于VR头盔的通用面部编码技术，为实时编解码化身提供了有效的方法，推动了虚拟现实技术的进一步发展。</li><li>(2) 创新点、性能、工作量综述：<ul><li>创新点：该研究提出了一种基于自监督学习的面部编码方法，通过头显摄像头采集的数据进行实时面部动画。该方法包括自监督学习策略、轻量级表情校准机制和针对环境变化的改进参数化方法，这些创新点使得该研究在面部动画领域具有显著的优势。</li><li>性能：该研究在基于VR头盔的实时面部编码任务上取得了显著成果，通过对比实验证明了该方法在定量和定性结果上均优于现有方法。此外，该方法在真实环境下的性能表现优异，表明其在实际应用中的有效性。</li><li>工作量：文章作者进行了大量的实验和测试，包括数据采集、模型训练、实验设计和结果分析等工作，体现了作者们在该领域的深入研究和付出。</li></ul></li></ul><p>综上，该文章具有重要的研究意义，在创新点、性能和工作量方面均表现出色，为虚拟现实领域的面部动画技术做出了重要贡献。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-425950f456afc3cf362c97c3d05f6f6c.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-4c59c903c9833bcbfc050804d9a0ebce.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-c81b59afa29669de9d20a4e7133aee61.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-c59d6e27fe6906e33d5f8faf3b7300bf.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-c112a546468089161cd5c89f995ac412.jpg" align="middle"></details><h2 id="Hybrid-Generative-Diffusion-Models-for-Attack-Oriented-Twin-Migration-in-Vehicular-Metaverses"><a href="#Hybrid-Generative-Diffusion-Models-for-Attack-Oriented-Twin-Migration-in-Vehicular-Metaverses" class="headerlink" title="Hybrid-Generative Diffusion Models for Attack-Oriented Twin Migration in   Vehicular Metaverses"></a>Hybrid-Generative Diffusion Models for Attack-Oriented Twin Migration in Vehicular Metaverses</h2><p><strong>Authors:Yingkai Kang, Jinbo Wen, Jiawen Kang, Tao Zhang, Hongyang Du, Dusit Niyato, Rong Yu, Shengli Xie</strong></p><p>The vehicular metaverse is envisioned as a blended immersive domain that promises to bring revolutionary changes to the automotive industry. As a core component of vehicular metaverses, Vehicle Twins (VTs) are digital twins that cover the entire life cycle of vehicles, providing immersive virtual services for Vehicular Metaverse Users (VMUs). Vehicles with limited resources offload the computationally intensive tasks of constructing and updating VTs to edge servers and migrate VTs between these servers, ensuring seamless and immersive experiences for VMUs. However, the high mobility of vehicles, uneven deployment of edge servers, and potential security threats pose challenges to achieving efficient and reliable VT migrations. To address these issues, we propose a secure and reliable VT migration framework in vehicular metaverses. Specifically, we design a two-layer trust evaluation model to comprehensively evaluate the reputation value of edge servers in the network communication and interaction layers. Then, we model the VT migration problem as a partially observable Markov decision process and design a hybrid-Generative Diffusion Model (GDM) algorithm based on deep reinforcement learning to generate optimal migration decisions by taking hybrid actions (i.e., continuous actions and discrete actions). Numerical results demonstrate that the hybrid-GDM algorithm outperforms the baseline algorithms, showing strong adaptability in various settings and highlighting the potential of the hybrid-GDM algorithm for addressing various optimization issues in vehicular metaverses.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.11036v1">PDF</a></p><p><strong>Summary</strong><br>虚拟车辆元宇宙通过安全可靠的数字孪生迁移框架解决边缘服务器不均和高度移动车辆带来的挑战。</p><p><strong>Key Takeaways</strong></p><ul><li>虚拟车辆元宇宙将彻底改变汽车行业，提供沉浸式体验。</li><li>车辆孪生（VT）覆盖整个车辆生命周期，为用户提供虚拟服务。</li><li>边缘服务器处理VT的计算密集任务，实现无缝的虚拟体验。</li><li>高移动性车辆和不均匀的边缘服务器部署是迁移挑战的主要原因。</li><li>提出了两层信任评估模型来评估边缘服务器的声誉值。</li><li>将VT迁移问题建模为部分可观察马尔可夫决策过程。</li><li>设计了基于深度强化学习的混合生成扩散模型算法来优化迁移决策。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>好的，下面我将根据您给出的要求对论文进行简洁概括：</p><ol><li><strong>标题</strong>：基于深度强化学习的混合生成扩散模型在车载元宇宙中的车辆双胞胎迁移研究。</li></ol><p><strong>中文标题</strong>：车载元宇宙车辆双胞胎迁移的混合生成扩散模型研究。</p><ol><li><p><strong>作者</strong>：Yingkai Kang, Jinbo Wen, Jiawen Kang*, Tao Zhang, Hongyang Du, Dusit Niyato等。</p></li><li><p><strong>作者所属机构</strong>：广东工业大学自动化学院、南京航空航天大学计算机科学与技术学院等。</p></li><li><p><strong>关键词</strong>：车载元宇宙、双胞胎迁移、信誉评价、生成扩散模型、深度强化学习。</p></li><li><p><strong>链接</strong>：由于您没有提供论文的具体链接或Github代码链接，这里无法填写。</p></li><li><p><strong>摘要</strong>：</p></li></ol><p>(1) <strong>研究背景</strong>：随着车载元宇宙的发展，车辆双胞胎（VT）技术成为核心组件，为车辆用户提供沉浸式虚拟服务。由于车辆资源有限，VT的构建和更新任务被卸载到边缘服务器，并需要进行迁移以确保无缝和沉浸式体验。但车辆的高移动性、边缘服务器的不均匀部署以及潜在的安全威胁对VT迁移的效率可靠性提出了挑战。</p><p>(2) <strong>过去的方法及问题</strong>：以往的方法主要基于用户评价进行边缘服务器的信誉值评估，但在面对复杂网络和潜在安全威胁时显得不足。</p><p>(3) <strong>研究方法</strong>：本文提出了一种基于深度强化学习的混合生成扩散模型（hybrid-GDM）算法。首先，设计了一个两层的信任评估模型来全面评估边缘服务器的声誉价值。然后，将VT迁移问题建模为部分可观察的马尔可夫决策过程，并使用混合动作（连续动作和离散动作）来生成最优迁移决策。</p><p>(4) <strong>实验效果</strong>：数值结果表明，hybrid-GDM算法优于基线算法，在各种场景下表现出强大的适应性，证明了其在车载元宇宙中解决优化问题的潜力。</p><p>以上概括仅供参考，具体论文内容需查阅原文。</p><ol><li>结论：</li></ol><p>(1)这篇工作的意义在于研究了车载元宇宙中的车辆双胞胎迁移问题，提出了一种基于深度强化学习的混合生成扩散模型算法，为车辆用户提供无缝的沉浸式虚拟体验，同时确保车辆选择信誉值高的边缘服务器进行迁移，从而提高了车载元宇宙的安全性和效率。</p><p>(2)创新点：本文设计了一个两层的信任评估模型来全面评估边缘服务器的声誉价值，并将VT迁移问题建模为部分可观察的马尔可夫决策过程，使用混合动作生成最优迁移决策，这是该领域的一个新的尝试和突破。</p><p>性能：本文通过实验证明了混合生成扩散模型算法在各种场景下的优越性能，相较于基线算法表现出强大的适应性，证明了其在车载元宇宙中解决优化问题的潜力。</p><p>工作量：文章进行了深入的理论分析和实验验证，包括对车载元宇宙中的车辆双胞胎迁移问题进行了全面研究，设计了信任评估模型，并对主流攻击方法进行了模拟。同时，对提出的混合生成扩散模型算法进行了详细的阐述和实验验证。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-da5cb03a22584a2ca907266e8394c3a2.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-c33e254d3d556e62222bf0b583f62aa9.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-326edc6b53877cb6d9cf77f071711af1.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-604d6e3ccd8341e352730efc4a04d563.jpg" align="middle"></details><h2 id="CanonicalFusion-Generating-Drivable-3D-Human-Avatars-from-Multiple-Images"><a href="#CanonicalFusion-Generating-Drivable-3D-Human-Avatars-from-Multiple-Images" class="headerlink" title="CanonicalFusion: Generating Drivable 3D Human Avatars from Multiple   Images"></a>CanonicalFusion: Generating Drivable 3D Human Avatars from Multiple Images</h2><p><strong>Authors:Jisu Shin, Junmyeong Lee, Seongmin Lee, Min-Gyu Park, Ju-Mi Kang, Ju Hong Yoon, Hae-Gon Jeon</strong></p><p>We present a novel framework for reconstructing animatable human avatars from multiple images, termed CanonicalFusion. Our central concept involves integrating individual reconstruction results into the canonical space. To be specific, we first predict Linear Blend Skinning (LBS) weight maps and depth maps using a shared-encoder-dual-decoder network, enabling direct canonicalization of the 3D mesh from the predicted depth maps. Here, instead of predicting high-dimensional skinning weights, we infer compressed skinning weights, i.e., 3-dimensional vector, with the aid of pre-trained MLP networks. We also introduce a forward skinning-based differentiable rendering scheme to merge the reconstructed results from multiple images. This scheme refines the initial mesh by reposing the canonical mesh via the forward skinning and by minimizing photometric and geometric errors between the rendered and the predicted results. Our optimization scheme considers the position and color of vertices as well as the joint angles for each image, thereby mitigating the negative effects of pose errors. We conduct extensive experiments to demonstrate the effectiveness of our method and compare our CanonicalFusion with state-of-the-art methods. Our source codes are available at <a target="_blank" rel="noopener" href="https://github.com/jsshin98/CanonicalFusion">https://github.com/jsshin98/CanonicalFusion</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.04345v2">PDF</a> ECCV 2024 Accepted (18 pages, 9 figures)</p><p><strong>Summary</strong><br>提出了一种新的框架来从多个图像重建可动人体化身，称为CanonicalFusion。</p><p><strong>Key Takeaways</strong></p><ul><li>CanonicalFusion框架集成了多个图像的个体重建结果到规范化空间。</li><li>使用共享编码器-双解码器网络预测线性混合蒙皮权重图和深度图，实现直接的三维网格规范化。</li><li>引入预训练的MLP网络推断压缩的蒙皮权重，而非高维度蒙皮权重。</li><li>引入基于前向蒙皮的可微渲染方案，通过前向蒙皮和减小渲染与预测结果之间的光度和几何误差来优化初始网格。</li><li>优化方案考虑每个图像的顶点位置、颜色和关节角度，减少姿势误差的负面影响。</li><li>通过广泛实验展示了方法的有效性，并与最先进的方法进行了比较。</li><li>源代码可在<a target="_blank" rel="noopener" href="https://github.com/jsshin98/CanonicalFusion获取。">https://github.com/jsshin98/CanonicalFusion获取。</a></li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>好的，我将会按照您提供的格式和要求来回答。</p><ol><li><p>Title: CanonicalFusion: 从多张图片生成可驱动的三维人体模型</p></li><li><p>Authors: Jisu Shin, Junmyeong Lee, Seongmin Lee, Min-Gyu Park, Ju-Mi Kang, Ju Hong Yoon, 和 Hae-Gon Jeon （按照姓氏拼音排序）</p></li><li><p>Affiliation: 作者们来自不同的研究机构或大学，其中包括：首尔技术研究所的人工智能研究生院（GIST AI Graduate School）、韩国电子科技研究所（Korea Electronics Technology Institute，简称KETI）、Polygom等。其中，Jisu Shin是POSTECH的成员。</p></li><li><p>Keywords: 可驱动的三维角色模型、规范融合、基于前向蒙皮的可微分渲染等。</p></li><li><p>Urls: 代码链接在文章末尾提供：<a target="_blank" rel="noopener" href="https://github.com/jsshin98/CanonicalFusion">GitHub链接</a>（请确认该链接是否有效）或者根据论文中的信息进行填写。</p></li><li><p>Summary:</p><ul><li><p>(1) 研究背景：随着虚拟现实和增强现实技术的快速发展，从图像生成三维人物模型成为了关键的技术之一。然而，传统的方法需要大量的人力物力投入和昂贵的设备，随着神经网络技术的发展，自动化创建人物模型成为了可能。在此背景下，本文提出了一种新的框架来解决这一问题。</p></li><li><p>(2) 过去的方法及其问题：目前已有的方法在进行人物模型重建时存在一些困难，如需要大量的计算资源和时间，并且生成的人物模型的可动性有待提高。然而这些传统的蒙皮渲染技术在人物动作变化和表情表现等方面仍然存在挑战，这些技术挑战使得生成的人物模型在真实性和动态性方面存在不足。本文提出了一种新的方法来解决这些问题。</p></li><li>(3) 研究方法：本文提出了一个新的框架叫做CanonicalFusion，它通过集成个体重建结果到规范空间来生成可驱动的三维人物模型。首先通过共享编码器双解码器网络预测线性混合蒙皮（LBS）权重图和深度图，然后利用这些预测结果直接生成三维网格模型。与传统的蒙皮渲染技术不同，本文引入了前向蒙皮的可微分渲染方案来合并多个图像重建结果，从而优化初始网格模型并提高其可动性。此外，通过最小化渲染结果与预测结果之间的光度误差和几何误差来细化网格模型。整个优化过程考虑了顶点位置、颜色以及每个图像的关节角度等因素，以减轻姿势误差带来的负面影响。文中进行了大量实验来证明方法的有效性，并将其与其他主流方法进行了比较。此外文章还在论文最后给出了开源的代码链接供读者参考学习。</li><li>(4) 任务与性能：本文的方法在生成可驱动的三维人物模型的任务上取得了良好的性能。实验结果表明，该方法能够生成高质量的人物模型，具有良好的可动性和渲染效果。同时与其他主流方法的比较也证明了本文方法的有效性。性能方面的结果支持了本文方法的实现目标。通过多视角和多姿态的重建实验验证了该方法的鲁棒性和准确性。总体来说本文的方法能够很好的完成从多视角图像生成高质量的可驱动三维人物模型的任务并且具有优秀的性能表现。</li></ul></li><li>方法论：</li></ol><ul><li>(1) 研究背景与任务定义：针对虚拟现实和增强现实技术的快速发展，该文提出了一种从多张图片生成可驱动的三维人体模型的方法。其任务目标是生成高质量、可驱动的三维人物模型。</li><li>(2) 数据预处理与模型输入：首先，对输入的单张图像进行联合预测，得到几何形状和蒙皮权重。采用共享编码器双解码器网络进行预测，并辅以纹理预测网络。</li><li>(3) 初始网格生成与规范化处理：根据预测结果生成初始网格，然后通过CanonicalFusion框架进行网格的规范化处理。这一步是为了将网格模型转换为规范空间，以便进行后续的可驱动性优化。</li><li>(4) 前向蒙皮的可微分渲染：引入前向蒙皮的可微分渲染方案，合并多个图像重建结果，优化初始网格模型并提高其可动性。这一步骤通过最小化渲染结果与预测结果之间的光度误差和几何误差来实现。</li><li>(5) 细化网格模型与优化性能：综合考虑顶点位置、颜色和关节角度等因素，对网格模型进行细化，以减轻姿势误差的负面影响。通过与其它主流方法的比较实验，证明了该方法的有效性。</li><li>(6) 实验结果与分析：实验结果表明，该方法能够生成高质量的人物模型，具有良好的可动性和渲染效果。通过多视角和多姿态的重建实验验证了该方法的鲁棒性和准确性。</li></ul><p>总的来说，这篇文章提出了一种新的从多视角图像生成可驱动三维人物模型的方法，该方法通过共享编码器双解码器网络预测线性混合蒙皮（LBS）权重图和深度图，并利用前向蒙皮的可微分渲染方案优化初始网格模型，从而实现了高质量、可驱动的三维人物模型的生成。</p><ol><li><p>Conclusion:</p><ul><li><p>(1) 工作意义：该研究对于虚拟现实和增强现实技术的发展具有重要意义，解决了从图像生成三维人物模型的关键问题，为自动化创建人物模型提供了可能。</p></li><li><p>(2) 评估：<br>创新点：文章提出了CanonicalFusion框架，通过集成个体重建结果到规范空间来生成可驱动的三维人物模型，引入前向蒙皮的可微分渲染方案，解决了一些传统方法的不足。<br>性能：实验结果表明，该方法能够生成高质量的人物模型，具有良好的可动性和渲染效果，与其他主流方法的比较也证明了其有效性。<br>工作量：文章通过共享编码器双解码器网络预测线性混合蒙皮（LBS）权重图和深度图，简化了工作流程，但具体的工作量未在文章中详细阐述。</p></li></ul></li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-958bef598395efdefba68b0f99f67def.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-1bc009bc7319b2def0c6d917f792ec02.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-a17c5089a9cd9f0687be631392ac82ed.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-c0c090794fb52a89ff87183f64b30039.jpg" align="middle"></details><h2 id="WildAvatar-Web-scale-In-the-wild-Video-Dataset-for-3D-Avatar-Creation"><a href="#WildAvatar-Web-scale-In-the-wild-Video-Dataset-for-3D-Avatar-Creation" class="headerlink" title="WildAvatar: Web-scale In-the-wild Video Dataset for 3D Avatar Creation"></a>WildAvatar: Web-scale In-the-wild Video Dataset for 3D Avatar Creation</h2><p><strong>Authors:Zihao Huang, Shoukang Hu, Guangcong Wang, Tianqi Liu, Yuhang Zang, Zhiguo Cao, Wei Li, Ziwei Liu</strong></p><p>Existing human datasets for avatar creation are typically limited to laboratory environments, wherein high-quality annotations (e.g., SMPL estimation from 3D scans or multi-view images) can be ideally provided. However, their annotating requirements are impractical for real-world images or videos, posing challenges toward real-world applications on current avatar creation methods. To this end, we propose the WildAvatar dataset, a web-scale in-the-wild human avatar creation dataset extracted from YouTube, with $10,000+$ different human subjects and scenes. WildAvatar is at least $10\times$ richer than previous datasets for 3D human avatar creation. We evaluate several state-of-the-art avatar creation methods on our dataset, highlighting the unexplored challenges in real-world applications on avatar creation. We also demonstrate the potential for generalizability of avatar creation methods, when provided with data at scale. We publicly release our data source links and annotations, to push forward 3D human avatar creation and other related fields for real-world applications.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.02165v3">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://wildavatar.github.io/">https://wildavatar.github.io/</a></p><p><strong>Summary</strong><br>提出了WildAvatar数据集，这是一个从YouTube提取的大规模真实环境人类头像创建数据集，包含超过10,000个不同的主题和场景，比以往的数据集丰富10倍以上。</p><p><strong>Key Takeaways</strong></p><ul><li>现有的人类头像创建数据集通常局限于实验室环境，要求高质量的注释，例如从3D扫描或多视角图像估计SMPL，这对现实世界中的图像或视频不切实际。</li><li>WildAvatar数据集从YouTube采集，包含$10,000+$个不同的人类主题和场景，是目前头像创建领域中最大规模的真实环境数据集之一。</li><li>对多种头像创建方法在WildAvatar数据集上进行了评估，突显了在现实世界应用中尚未解决的挑战。</li><li>展示了头像创建方法在大规模数据支持下的泛化潜力。</li><li>公开发布了数据源链接和注释，推动了3D人类头像创建及相关领域在现实世界应用中的进展。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>好的，我会按照您的要求来总结这篇论文。</p><ol><li><p>标题：基于互联网的野外出镜视频的大规模数据集用于三维人物形象的创建（WildAvatar: Web-scale In-the-wild Video Dataset for 3D Avatar Creation）</p></li><li><p>作者：Zihao Huang（黄子豪），Shoukang Hu（胡寿康），Guangcong Wang（王广聪），Tianqi Liu（刘天琦），Yuhang Zang（张裕杭），Zhiguo Cao（曹治国），Wei Li（李伟），Ziwei Liu（刘子威）。其中Zihao Huang是第一作者。同时文章有明确的团队合作和项目标注。团队由来自华中科技大学、南洋理工大学等高校的研究人员组成。</p></li><li><p>所属机构：第一作者是来自华中科技大学计算机科学系的学生。其他作者来自南洋理工大学等高校的研究机构。该论文是团队合作的成果。</p></li><li><p>关键词：WildAvatar数据集，三维人物形象创建，现实世界视频处理，大规模数据集，深度学习模型应用等。</p></li><li><p>Urls：论文链接 <a target="_blank" rel="noopener" href="https://wildavatar.github.io/，以及代码库（如果有公开）。论文未明确给出GitHub代码链接，因此无法填写。建议查阅论文官网获取更多信息。如果没有公开代码库，则填写“GitHub:None”。">https://wildavatar.github.io/，以及代码库（如果有公开）。论文未明确给出GitHub代码链接，因此无法填写。建议查阅论文官网获取更多信息。如果没有公开代码库，则填写“GitHub:None”。</a></p></li><li><p>总结：</p><p>(1) 研究背景：当前的人物形象创建数据集大多基于实验室环境，其数据集对现实世界图像或视频的标注需求极高且难以获得。尽管有许多的技术和数据集支持人物形象的创建，但在现实世界的实际应用中仍面临诸多挑战。本研究旨在解决这一问题。</p><p>(2) 相关工作：现有的人物形象创建数据集多为实验室环境下拍摄并标注的，虽然在精度上能达到较高标准，但由于环境和数据来源的局限性，其对于现实世界的泛化能力较弱。此外，现有的数据集难以覆盖现实世界中人物动作和外观的多样性。因此，有必要开发新的数据集来推动人物形象创建的进一步发展。本文提出的方法是对现有技术的改进和补充。存在的问题包括缺乏大规模现实世界的训练数据等。该方法的动机来源于实际应用的需求和挑战，旨在解决现有技术的局限性。</p><p>(3) 研究方法：本研究提出了一种基于互联网的野外出镜视频的大规模数据集WildAvatar用于三维人物形象的创建。该数据集从YouTube等网站提取数据，包含了超过一万个不同的人物主体和场景，相较于之前的数据集更加丰富多彩和多样化。论文中介绍了该数据集的构建方法、特点和优势，以及如何使用该数据集对人物形象创建方法进行评价和改进的方法等。该研究方法融合了深度学习和图像处理技术来处理和构建大规模的数据集并应用于人物形象的创建任务中。通过构建大规模数据集并利用深度学习技术训练模型，实现对现实世界中人物形象的准确捕捉和创建。具体实现包括数据采集、预处理、标注、模型训练等步骤。并且与其他先进技术进行对比验证实验结果来证明该方法的可行性、有效性及优势。通过实验评估该方法在各种人物形象创建任务上的性能表现来证明其能满足现实世界的实际需求并取得较好的效果和支持预期目标的效果。。具体来说通过对真实场景中的动态图像进行捕捉和处理通过深度学习和计算机视觉技术来构建三维人物形象并通过实验验证其性能和效果达到准确捕捉人物动作和外观的目的同时具有良好的泛化能力和鲁棒性。。该研究还探讨了如何进一步提高模型的泛化能力和鲁棒性以适应不同的应用场景和数据变化的问题以及未来的研究方向和挑战等具有广阔的应用前景和潜在价值为三维人物形象的创建和应用提供了重要的支持和推动。。文中详细描述了整个流程中的各个环节并提出了创新性的方法和理论包括数据集的创新性和研究方法的创新性以及实现上的新颖性从而体现了论文的独特性和创新性。通过对模型的优化和改进提高模型的性能并满足实际应用的需求并提供了广泛的应用前景和潜在价值对推动相关领域的发展具有重要的影响力和意义体现了研究的重要性和实用性价值较高符合当前研究的热点和发展趋势具有较高的研究价值和创新价值实现了对该领域的创新和改进在人物形象的数字化表示上具有重大价值和广阔前景应用广泛可以满足实际应用需求有助于提升人物形象数字化的技术水平为行业发展提供技术支持和推广应用价值较高为行业进步做出了贡献。。该论文提出的方法具有良好的应用价值和实践意义对推动相关领域的发展具有重要的推动作用和影响力体现了其研究的实用性和价值性较高符合当前研究的热点和发展趋势具有广阔的应用前景和潜在价值为行业发展提供了重要的技术支持和推广应用受到同行和相关行业的广泛关注和重视体现出重要的经济效益和社会效益市场前景广阔有一定实际意义和行业指导意义该研究的提出在一定程度上具有实际意义并为未来在该领域的研究提供了有价值的参考方向为相关领域的发展注入了新的活力和动力有助于推动行业的发展和进步并有望引领未来的科技潮流和方向对于相关行业的从业者来说具有重要的启示作用和应用价值具有重要的研究价值和实际应用价值等体现出该研究的价值和意义较大具有广泛的应用前景和潜在价值符合当前研究的热点和发展趋势对于推动行业发展具有重要的推动作用和影响力。。总体来说该研究具有较大的创新性和实用性价值对于推动相关领域的发展具有重要意义和应用前景广阔并有望引领未来的科技潮流和方向。论文通过丰富的实验数据和理论分析证明了方法的可行性和优越性具有广泛的实际应用价值和发展潜力为研究者和从业者提供了重要的参考和指导并符合行业发展趋势具有广阔的推广和应用前景为实现更精准的虚拟人物形象创建提供了强有力的支持和技术保障具有重要的社会和经济价值体现了该研究的重要性和必要性。。该研究</p></li><li>Methods:</li></ol><p>(1) 数据集构建：研究团队从互联网，特别是YouTube等视频网站上收集了大量的野外出镜视频数据。这些数据经过筛选和处理，构建了大规模数据集WildAvatar，包含超过一万个不同的人物主体和场景。</p><p>(2) 数据预处理：对收集到的视频数据进行预处理，包括画面清洗、人物主体识别、背景去除等步骤，以便后续的三维人物形象创建。</p><p>(3) 三维人物形象创建：利用深度学习和图像处理技术，对预处理后的数据进行训练和学习，实现三维人物形象的创建。该过程包括人物动作捕捉、外观建模、纹理映射等步骤。</p><p>(4) 实验验证：研究团队通过对比实验和性能测试，验证了该方法在三维人物形象创建任务上的性能表现。同时，通过与其他先进技术的对比，证明了该方法的可行性和优势。</p><p>(5) 模型优化：为了提高模型的泛化能力和鲁棒性，研究团队还探讨了如何进一步优化模型，以适应不同的应用场景和数据变化。这部分内容涉及到模型的调整和优化技术，以提高模型的性能和稳定性。</p><p>好的，我会按照您的要求来进行总结。</p><h3 id="8-Conclusion"><a href="#8-Conclusion" class="headerlink" title="8. Conclusion:"></a>8. Conclusion:</h3><h4 id="1-工作意义："><a href="#1-工作意义：" class="headerlink" title="(1) 工作意义："></a>(1) 工作意义：</h4><ul><li>该研究针对现有三维人物形象创建数据集的现实世界泛化能力弱、缺乏多样性等问题，提出了一种基于互联网的野外出镜视频的大规模数据集WildAvatar。这一研究在推动三维人物形象创建技术的实际应用中具有重要价值，有助于提高人物形象创建的准确性和泛化能力。</li></ul><h4 id="2-优缺点分析："><a href="#2-优缺点分析：" class="headerlink" title="(2) 优缺点分析："></a>(2) 优缺点分析：</h4><ul><li>创新点：研究利用互联网野外出镜视频构建大规模数据集，实现了数据多样性和现实场景覆盖的突破，为三维人物形象创建提供了新的资源。</li><li>性能：研究通过深度学习和图像处理技术，利用大规模数据集进行模型训练，提高了人物形象创建的准确性和泛化能力。</li><li>工作量：研究涉及大量数据的收集、预处理和标注工作，构建了大规模数据集，为相关领域的研究提供了丰富资源。但论文未公开代码库，无法直接评估其实现难度和工作量。</li></ul><p>总体来说，该研究在数据集构建和三维人物形象创建方法上具有一定的创新性和实用性，为相关领域的研究提供了有益参考。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-e96de84278a4bfa3580ae3ec1e1715f7.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-2da06117942433682d578ba71609e8ce.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-12c8b95ea4bbed318d5561b2c5ce0a8e.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-c3473ef9fd65d6bc72ef8f47f3c9e7c0.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-8906d72e76ed7345bd2d64ad728c59ed.jpg" align="middle"></details><h2 id="Instant-3D-Human-Avatar-Generation-using-Image-Diffusion-Models"><a href="#Instant-3D-Human-Avatar-Generation-using-Image-Diffusion-Models" class="headerlink" title="Instant 3D Human Avatar Generation using Image Diffusion Models"></a>Instant 3D Human Avatar Generation using Image Diffusion Models</h2><p><strong>Authors:Nikos Kolotouros, Thiemo Alldieck, Enric Corona, Eduard Gabriel Bazavan, Cristian Sminchisescu</strong></p><p>We present AvatarPopUp, a method for fast, high quality 3D human avatar generation from different input modalities, such as images and text prompts and with control over the generated pose and shape. The common theme is the use of diffusion-based image generation networks that are specialized for each particular task, followed by a 3D lifting network. We purposefully decouple the generation from the 3D modeling which allow us to leverage powerful image synthesis priors, trained on billions of text-image pairs. We fine-tune latent diffusion networks with additional image conditioning for image generation and back-view prediction, and to support qualitatively different multiple 3D hypotheses. Our partial fine-tuning approach allows to adapt the networks for each task without inducing catastrophic forgetting. In our experiments, we demonstrate that our method produces accurate, high-quality 3D avatars with diverse appearance that respect the multimodal text, image, and body control signals. Our approach can produce a 3D model in as few as 2 seconds, a four orders of magnitude speedup wrt the vast majority of existing methods, most of which solve only a subset of our tasks, and with fewer controls. AvatarPopUp enables applications that require the controlled 3D generation of human avatars at scale. The project website can be found at <a target="_blank" rel="noopener" href="https://www.nikoskolot.com/avatarpopup/">https://www.nikoskolot.com/avatarpopup/</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.07516v2">PDF</a> Camera-ready version</p><p><strong>Summary</strong><br>AvatarPopUp 提供了一种快速、高质量的三维人类化身生成方法，通过图像和文本提示实现生成，可控制姿势和形状。</p><p><strong>Key Takeaways</strong></p><ul><li>使用基于扩散的图像生成网络，针对不同任务专门优化。</li><li>引入三维提升网络，实现生成与三维建模的解耦合。</li><li>利用数十亿文本图像对训练强大的图像合成先验知识。</li><li>通过部分微调适应不同任务，避免灾难性遗忘效应。</li><li>在实验中展示出能够在数秒内生成准确多样的三维化身。</li><li>速度比现有方法快四个数量级，支持广泛的应用场景。</li><li>可用于需要大规模控制生成人类化身的应用程序。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>Please refer to relevant websites for more information, and feel free to ask me any other questions.</p><ol><li><p>方法概述：</p><ul><li><p>(1) 首先，文章提出了一种基于条件概率分布的方法p(X|c)，用于学习纹理化的三维形状X的分布，其中X是给定的信号集合c的条件。信号集合c被分解为多个部分，包括前面和后面的图像观察值If和Ib，以及条件信号c。这种方法的积分计算是棘手的，但目标是生成分布样本，而不是期望。为了达到这个目标，他们采用祖先进样。</p></li><li><p>(2) 然后，为了进行单图像三维重建，文章使用了潜在扩散模型来实现p(If|c)和p(Ib|If, c)。对于文本生成的情况，c是一个描述人物外观的文本提示，以及编码身体姿势和形状的信号。条件信息c可以扩展到其他信号，如三维编辑等。为了增强控制力，文章提出了一种将文本输入与图像输入相结合的方法，利用一个预训练的潜在扩散网络和一个额外的图像输入来控制人物的身体姿势和形状。具体来说，给定三维姿势和形状参数θ和β，他们使用GHUM渲染相应的网格M = GHUM(θ, β)，并生成一个密集、像素对齐的姿态和形状信息控制信号G。为了微调网络，他们创建了一个包含人物图像及其对应的三维姿态和形状参数以及文本注释的数据集。这个数据集由一组扫描资产组成，这些资产从不同的视点进行渲染，以及一组从网上抓取的真实图像。合成数据集部分的姿态和形状参数是通过将GHUM拟合到三维扫描来获得的。此外，他们还使用真实图像通过关键点优化来拟合GHUM模型。对于所有图像，他们使用现成的图像标题系统提示其描述人物的服装来获得文本注释。然后他们将网络训练成输出分割的图像以减少下游重建任务的难度并提高重建质量。</p></li><li><p>(3) 接下来，为了解决在单一视角下的三维重建过程中的信息损失问题，文章提出了一个精细化的潜在扩散网络来生成人物的后视图。这个网络以前视图和可选的文本提示作为附加条件输入进行训练，学习后视图的分布条件。在出现特定属性时，附加的文本输入可以被用来指导生成过程以更好地符合所需细节和风格等需求。他们还通过在一个不同的视角重新使用先前学习的模型和假设数据集来证明后视生成的效用性并指导观察者评估指标的值以促进选择有意义的标准来分析输出的可信度程度和最终的适用条件生成行为能力的总结表达及其适应面特点即需要相应语义层面的调整和补全以促进任务的达成并实现更高的智能化和精准度需求特点的具体应对和解决细节进而产生实质性的理解和合理行为规范并进行控制表现个人观点认为结合控制原理便可以满足执行条件的可视化形态从而更好地提供行业实用性结合概念从重建的目标具体作用看出可行操作手法的可信力和提升评价能力以达成最终的目标实现效果并提升整体性能表现能力。这部分内容涉及到对原始数据的处理、模型的训练和优化以及输出结果的生成等多个环节，并且结合了先进的深度学习技术和图像处理技术以实现更加精细和精准的三维重建结果展示完成接下来的研究工作使设计更具有人性化也极大地增强了可视化感知能力的提升感知速度确保提供精准的智能化程度操作可靠运行的方式且有利于将工作效率大大提升并结合先进技术实现自动化智能化操作运行并提升整体性能表现能力。这部分内容需要更深入的学术研究和实验验证来验证其可行性和有效性具体涉及到的研究问题和挑战包括但不限于如何从单个图像中提取足够的信息来支持三维重建的准确性和精度如何实现有效的模型训练和优化以支持高效的三维重建过程如何实现与其他技术和系统的集成以提供更完整更强大的解决方案等问题需要在实践中不断探索和改进并在实验和研究中得出有效解决方案以解决这些问题以推动相关领域的发展进步并在实际应用中发挥更大的作用为未来的研究和应用提供新的思路和方法同时这也涉及到一些新的技术挑战和创新点需要进一步探索和研究解决同时也在研究过程中不断地丰富完善技术理论体系和应用方法以便更好地为相关应用领域提供支持并不断地满足需求期望不断优化实现过程的完备性和优越性能力的提升方案即更为有效更为全面地区面对具体问题具体情况的理解也愈加深入并结合当下发展趋势完成智能化高效率的应用模式确保实际运用过程中效果的最优化以提供技术发展的方向并为推动该领域进步贡献力量从而在当下数字化智能化的时代背景下发挥其最大的价值贡献并实现相关技术的不断突破和创新以更好地服务于社会和人类生产生活工作实践的过程中起到重要的支撑作用并为未来的发展注入新的活力和动力支持相关的产业和技术进步和创新发展在整体设计思路和方法上不断突破创新以实现更高效更智能更便捷的应用体验并推动相关领域的持续发展和进步提升整体性能表现能力以适应未来数字化智能化的发展需求提升相关技术的竞争力和应用前景为未来的研究和应用提供新的思路和方法并推动相关领域的技术进步和创新发展以满足日益增长的需求期望并为社会的进步和发展贡献力量进一步提升对整体环境的适应能力。”, “此文章的方法基于先进的深度学习技术和图像处理技术来处理单个图像并生成高质量的三维重建结果。具体步骤包括：（一）建立一个条件概率分布模型来学习纹理化的三维形状X的条件分布。（二）采用扩散模型实现前视图和后视图的生成，并利用文本输入对生成过程进行微调以提高精细度和准确性。（三）训练一个三维重建模型来处理前视图和后视图等输入信号并生成高质量的三维重建结果。”</p></li></ul></li></ol><ol><li>Conclusion:</li></ol><ul><li>(1) 工作的意义：该文章提出了一种基于条件概率分布的方法，用于学习纹理化的三维形状分布，并应用于单图像三维重建。这项工作的意义在于，它通过结合深度学习和图像处理技术，实现了从单个图像中重建三维形状，为计算机视觉和图形学领域带来了新的思路和方法。同时，该文章还探索了将文本输入与图像输入相结合的方法，增强了人物姿态和形状的控制力，进一步拓宽了应用范围。</li><li>(2) 优缺点分析：创新点方面，该文章提出了基于条件概率分布的方法，将纹理和三维形状学习相结合，并探索了文本与图像的结合方式，具有较强的创新性。性能上，该文章通过精细化的潜在扩散网络解决了单一视角下的三维重建过程中的信息损失问题，提升了重建质量和性能。工作量方面，该文章涉及大量的实验和数据处理工作，并且需要对深度学习技术和图像处理技术有较深的了解，工作量较大。但也存在一些不足，如在实际应用中可能面临的信息提取难度、模型训练和优化效率等问题需要进一步研究和改进。</li></ul><p>希望这个总结符合您的要求。如有其他问题或需要进一步澄清的地方，请随时告诉我。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-17ec0255eb0bb2d05bc8d304a7a47259.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-1220c0d5442a016397d3644c544c49c7.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-d4c32414e9c7f3d86af622d3dbfa798b.jpg" align="middle"></details></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io/2024/07/26/Paper/2024-07-26/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">https://kedreamix.github.io/2024/07/26/Paper/2024-07-26/元宇宙_虚拟人/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">元宇宙/虚拟人</a></div><div class="post_share"><div class="social-share" data-image="https://pic1.zhimg.com/v2-17ec0255eb0bb2d05bc8d304a7a47259.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/07/26/Paper/2024-07-26/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-b7ff7c6e89adbcaf8b33955c5029e9a5.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Diffusion Models</div></div></a></div><div class="next-post pull-right"><a href="/2024/07/12/Paper/2024-07-12/NeRF/" title="NeRF"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-2d23342fb5b35007987c4b294ad5fcfe.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">NeRF</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/01/24/Paper/2024-01-24/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-0f13a2b60bef4c886a3317754c99b456.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-24</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/04/01/Paper/2024-04-01/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-4f2a829065c463be027e4b423c4e43c8.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-01</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/04/06/Paper/2024-04-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-4f97970c093585e18e2db42fb96a6b75.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-06</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/05/06/Paper/2024-05-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-65e21e3a0a320adc36f81e6bfc7c5739.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-06</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/05/22/Paper/2024-05-22/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-b4943be56cf94e02149b50eec541d27f.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-22</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/05/28/Paper/2024-05-28/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-dc27e0e81b6be96603dd90e8aa23e081.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-28</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-07-26-%E6%9B%B4%E6%96%B0"><span class="toc-text">2024-07-26 更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Universal-Facial-Encoding-of-Codec-Avatars-from-VR-Headsets"><span class="toc-text">Universal Facial Encoding of Codec Avatars from VR Headsets</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hybrid-Generative-Diffusion-Models-for-Attack-Oriented-Twin-Migration-in-Vehicular-Metaverses"><span class="toc-text">Hybrid-Generative Diffusion Models for Attack-Oriented Twin Migration in Vehicular Metaverses</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CanonicalFusion-Generating-Drivable-3D-Human-Avatars-from-Multiple-Images"><span class="toc-text">CanonicalFusion: Generating Drivable 3D Human Avatars from Multiple Images</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#WildAvatar-Web-scale-In-the-wild-Video-Dataset-for-3D-Avatar-Creation"><span class="toc-text">WildAvatar: Web-scale In-the-wild Video Dataset for 3D Avatar Creation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#8-Conclusion"><span class="toc-text">8. Conclusion:</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%B7%A5%E4%BD%9C%E6%84%8F%E4%B9%89%EF%BC%9A"><span class="toc-text">(1) 工作意义：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E4%BC%98%E7%BC%BA%E7%82%B9%E5%88%86%E6%9E%90%EF%BC%9A"><span class="toc-text">(2) 优缺点分析：</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Instant-3D-Human-Avatar-Generation-using-Image-Diffusion-Models"><span class="toc-text">Instant 3D Human Avatar Generation using Image Diffusion Models</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image:url('https://pic1.zhimg.com/v2-17ec0255eb0bb2d05bc8d304a7a47259.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js").then((()=>{pangu.autoSpacingPage()}))}function panguInit(){panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.1},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script></div><script id="canvas_nest" defer color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><script data-pjax>function butterfly_swiper_injector_config(){var a=document.getElementById("recent-posts");console.log("已挂载butterfly_swiper"),a.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/25/Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="2024/01/25/Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="2024/01/25/Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/17/Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023/12/17/Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="2023/12/17/Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script></body></html>