<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>Diffusion Models | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-07-05  DisCo-Diff Enhancing Continuous Diffusion Models with Discrete Latents"><meta property="og:type" content="article"><meta property="og:title" content="Diffusion Models"><meta property="og:url" content="https://kedreamix.github.io/2024/07/05/Paper/2024-07-05/Diffusion%20Models/index.html"><meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World"><meta property="og:description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-07-05  DisCo-Diff Enhancing Continuous Diffusion Models with Discrete Latents"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://pic1.zhimg.com/v2-d5b65b7fa9624f0213b9bf74e8da7e0e.jpg"><meta property="article:published_time" content="2024-07-05T03:08:49.000Z"><meta property="article:modified_time" content="2024-07-05T03:08:49.690Z"><meta property="article:author" content="Kedreamix"><meta property="article:tag" content="Diffusion Models"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://pic1.zhimg.com/v2-d5b65b7fa9624f0213b9bf74e8da7e0e.jpg"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/2024/07/05/Paper/2024-07-05/Diffusion%20Models/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-74LZ5BEQQ1")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!0,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"简"},noticeOutdate:{limitDay:500,position:"top",messagePrev:"It has been",messageNext:"days since the last update, the content of the article may be outdated."},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:200},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!0,post:!0},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!0,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"Diffusion Models",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-07-05 11:08:49"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach((e=>{n.setAttribute(e,t[e])})),document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)})),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=24?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 7.0.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml"></head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{const d=document.getElementById("loading-box"),e=document.body,o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l(),window.addEventListener("load",(()=>{o()}))})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">166</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url('https://pic1.zhimg.com/v2-d5b65b7fa9624f0213b9bf74e8da7e0e.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Diffusion Models</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-07-05T03:08:49.000Z" title="发表于 2024-07-05 11:08:49">2024-07-05</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-07-05T03:08:49.690Z" title="更新于 2024-07-05 11:08:49">2024-07-05</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">9.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>30分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="Diffusion Models"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-07-05-更新"><a href="#2024-07-05-更新" class="headerlink" title="2024-07-05 更新"></a>2024-07-05 更新</h1><h2 id="DisCo-Diff-Enhancing-Continuous-Diffusion-Models-with-Discrete-Latents"><a href="#DisCo-Diff-Enhancing-Continuous-Diffusion-Models-with-Discrete-Latents" class="headerlink" title="DisCo-Diff: Enhancing Continuous Diffusion Models with Discrete Latents"></a>DisCo-Diff: Enhancing Continuous Diffusion Models with Discrete Latents</h2><p><strong>Authors:Yilun Xu, Gabriele Corso, Tommi Jaakkola, Arash Vahdat, Karsten Kreis</strong></p><p>Diffusion models (DMs) have revolutionized generative learning. They utilize a diffusion process to encode data into a simple Gaussian distribution. However, encoding a complex, potentially multimodal data distribution into a single continuous Gaussian distribution arguably represents an unnecessarily challenging learning problem. We propose Discrete-Continuous Latent Variable Diffusion Models (DisCo-Diff) to simplify this task by introducing complementary discrete latent variables. We augment DMs with learnable discrete latents, inferred with an encoder, and train DM and encoder end-to-end. DisCo-Diff does not rely on pre-trained networks, making the framework universally applicable. The discrete latents significantly simplify learning the DM’s complex noise-to-data mapping by reducing the curvature of the DM’s generative ODE. An additional autoregressive transformer models the distribution of the discrete latents, a simple step because DisCo-Diff requires only few discrete variables with small codebooks. We validate DisCo-Diff on toy data, several image synthesis tasks as well as molecular docking, and find that introducing discrete latents consistently improves model performance. For example, DisCo-Diff achieves state-of-the-art FID scores on class-conditioned ImageNet-64/128 datasets with ODE sampler.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.03300v1">PDF</a> project page: <a target="_blank" rel="noopener" href="https://research.nvidia.com/labs/lpr/disco-diff">https://research.nvidia.com/labs/lpr/disco-diff</a></p><p><strong>Summary</strong><br>Diffusion models (DMs) utilize a diffusion process to encode data into a Gaussian distribution. DisCo-Diff simplifies this by integrating discrete latent variables, enhancing model performance.</p><p><strong>Key Takeaways</strong></p><ul><li>扩散模型（DMs）利用扩散过程将数据编码为高斯分布。</li><li>DisCo-Diff通过整合离散潜变量简化了模型，提升了性能。</li><li>离散潜变量通过降低生成ODE曲率显著简化了学习复杂噪声到数据映射。</li><li>模型在多个任务中验证，如图像合成和分子对接，显示引入离散潜变量一致提升了性能。</li><li>DisCo-Diff不依赖预训练网络，具有普遍适用性。</li><li>模型引入了一个自回归变压器来建模离散潜变量的分布，仅需少量离散变量和小型码本。</li><li>在ODE采样器上，DisCo-Diff在类条件的ImageNet-64/128数据集上达到了最先进的FID分数。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>好的，我将按照您的要求阅读论文并总结其要点。以下是论文的简要介绍和概括：</p><p><strong>标题</strong>：离散连续潜变量扩散模型（DisCo-Diff）在生成学习中的应用增强</p><p><strong>作者</strong>：由一系列作者名字组成，具体名称需查阅论文原文。</p><p><strong>作者机构（中文翻译）</strong>：该论文的作者在多个研究机构或大学工作，包括人工智能、机器学习等相关领域的研究机构。</p><p><strong>关键词</strong>：离散连续潜变量、扩散模型（DMs）、生成学习、噪声到数据映射、离散潜变量推理、分子对接、图像合成。</p><p><strong>论文链接和代码链接</strong>：论文链接请查阅学术数据库，目前GitHub代码链接暂未提供，后续如有公开，请访问GitHub官方仓库获取。</p><p><strong>摘要</strong>：</p><ul><li><strong>(1)研究背景</strong>：本文研究背景是扩散模型（DMs）在生成学习中的革命性进展。DMs通过扩散过程将数据编码为简单的高斯分布，但将复杂的多模态数据分布编码为单一连续高斯分布是一个具有挑战性的学习任务。</li><li><strong>(2)过去的方法及问题</strong>：虽然已有许多使用扩散模型的方法，但它们大多依赖于连续潜变量，这可能导致学习困难。文章指出当前方法的问题，并提出需要一种更简单的方法来处理这种学习任务。</li><li><strong>(3)研究方法</strong>：本文提出离散连续潜变量扩散模型（DisCo-Diff）。该模型引入辅助离散潜变量来简化任务。通过引入可学习的离散潜变量，并与扩散模型一起进行端到端训练，简化了从噪声到数据的复杂映射。此外，还使用自回归变压器对离散潜变量进行建模。</li><li><strong>(4)任务与性能</strong>：文章在玩具数据、图像合成任务以及分子对接等任务上验证了DisCo-Diff的有效性。引入离散潜变量始终提高了模型性能。特别是在类条件ImageNet-64/128数据集上，DisCo-Diff实现了最先进的FID分数，并展示了强大的生成能力。性能结果支持该方法的目标。</li></ul><p>总结完毕，该论文提出了一种新的扩散模型，通过引入离散潜变量简化了生成学习任务，并在多个任务上取得了优异的性能。</p><p>好的，我将根据您给出的结构来总结这篇论文的关键内容。以下是对于这篇论文的简要总结性评述：</p><p><strong>一、重要性概述（Innovation point）</strong>:<br>本研究论文的工作显著提升了扩散模型在生成学习中的表现和应用能力。针对当前扩散模型面临的主要挑战——多模态数据分布的编码问题，该论文提出了离散连续潜变量扩散模型（DisCo-Diff）。该模型引入离散潜变量作为辅助工具，极大地简化了从噪声到数据的复杂映射过程，展现了其在解决生成学习任务中的显著优势和重要性。这一创新点不仅丰富了扩散模型的理论体系，也为相关领域的研究提供了新思路和方法。</p><p><strong>二、论文的优缺点分析（Performance &amp; Workload）</strong>：<br>创新点突出是该论文最大的优点，所提出的离散连续潜变量扩散模型是对当前扩散模型的一个重大改进。此外，该论文在多个任务上取得了优异的性能表现，特别是在图像合成任务中，实现了先进的FID分数，证明了模型的实用性。工作量方面，论文详细介绍了模型的构建过程、实验设计以及结果分析，显示出作者团队的严谨态度和扎实工作。然而，论文也存在一定的局限性，例如对于某些复杂任务，模型的训练时间和计算成本可能较高。此外，尽管论文展示了模型在不同任务上的性能表现，但关于模型的进一步优化和潜在应用领域的拓展尚未进行详尽探讨。这为该领域后续研究提供了更多可能性。</p><p><strong>三、结论（Conclusion）</strong>:<br>综上所述，这篇论文所提出的离散连续潜变量扩散模型在生成学习任务中展现出了显著的优势和重要性。该模型通过引入离散潜变量简化了学习任务，实现了多项任务的优秀表现。虽然存在一定局限性，但其突破性的研究成果仍具有极大的实际意义和研究价值。建议未来研究进一步深化模型理论，优化模型性能，并探索更多潜在应用领域。同时，也需要对该模型在实际应用中的效果进行更为深入的研究和验证。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-03c6d150bb9a3af64b5de4dd16b58354.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-6e3d87eea1aea3055c9079cd21ef8fcf.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-45205ec70193e7a3749a4a67957783a3.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-aa99ad472af5fed2368173de94f1a2a3.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-ca6c09a1257a1626895ea40b1ff3834d.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-54a72845b8424b3dd3f444f32ba71d82.jpg" align="middle"></details><h2 id="Frequency-Controlled-Diffusion-Model-for-Versatile-Text-Guided-Image-to-Image-Translation"><a href="#Frequency-Controlled-Diffusion-Model-for-Versatile-Text-Guided-Image-to-Image-Translation" class="headerlink" title="Frequency-Controlled Diffusion Model for Versatile Text-Guided   Image-to-Image Translation"></a>Frequency-Controlled Diffusion Model for Versatile Text-Guided Image-to-Image Translation</h2><p><strong>Authors:Xiang Gao, Zhengbo Xu, Junhan Zhao, Jiaying Liu</strong></p><p>Recently, large-scale text-to-image (T2I) diffusion models have emerged as a powerful tool for image-to-image translation (I2I), allowing open-domain image translation via user-provided text prompts. This paper proposes frequency-controlled diffusion model (FCDiffusion), an end-to-end diffusion-based framework that contributes a novel solution to text-guided I2I from a frequency-domain perspective. At the heart of our framework is a feature-space frequency-domain filtering module based on Discrete Cosine Transform, which filters the latent features of the source image in the DCT domain, yielding filtered image features bearing different DCT spectral bands as different control signals to the pre-trained Latent Diffusion Model. We reveal that control signals of different DCT spectral bands bridge the source image and the T2I generated image in different correlations (e.g., style, structure, layout, contour, etc.), and thus enable versatile I2I applications emphasizing different I2I correlations, including style-guided content creation, image semantic manipulation, image scene translation, and image style translation. Different from related approaches, FCDiffusion establishes a unified text-guided I2I framework suitable for diverse image translation tasks simply by switching among different frequency control branches at inference time. The effectiveness and superiority of our method for text-guided I2I are demonstrated with extensive experiments both qualitatively and quantitatively. The code is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/XiangGao1102/FCDiffusion">https://github.com/XiangGao1102/FCDiffusion</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.03006v1">PDF</a> Proceedings of the 38th AAAI Conference on Artificial Intelligence (AAAI 2024)</p><p><strong>Summary</strong><br>频率控制扩散模型（FCDiffusion）通过频域滤波模块，提供了一种新颖的文本引导图像翻译解决方案。</p><p><strong>Key Takeaways</strong></p><ul><li>FCDiffusion是基于扩散的框架，利用离散余弦变换在频域中过滤潜在特征。</li><li>控制信号来自不同的DCT频谱带，桥接源图像与生成图像的不同相关性。</li><li>支持风格引导内容创建、图像语义操作、图像场景翻译和图像风格转换。</li><li>在推断时通过切换不同的频率控制分支实现多样的图像翻译任务。</li><li>通过广泛实验证明了FCDiffusion在质量和数量上的优越性。</li><li>代码公开可用于进一步研究和实现：<a target="_blank" rel="noopener" href="https://github.com/XiangGao1102/FCDiffusion。">https://github.com/XiangGao1102/FCDiffusion。</a></li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>好的，我将会按照您的要求总结论文。</p><ol><li><p>Title: 频率控制扩散模型在文本引导图像转换中的应用研究（Frequency-Controlled Diffusion Model for Versatile Text-Guided Image-to-Image）</p></li><li><p>Authors: 向刚（Xiang Gao）, 郑博（Zhengbo Xu）, 刘俊翰（Junhan Zhao）, 刘佳颖（Jiaying Liu）*（Corresponding author）</p></li><li><p>Affiliation: 王选计算机技术研究所，北京大学（Wangxuan Institute of Computer Technology, Peking University）</p></li><li><p>Keywords: 频率控制扩散模型，文本引导图像转换，图像转换（Frequency-Controlled Diffusion Model, Text-Guided Image-to-Image, Image Translation）</p></li><li><p>Urls: <a target="_blank" rel="noopener" href="https://github.com/XiangGao1102/FCDiffusion">https://github.com/XiangGao1102/FCDiffusion</a> （Github代码链接）或论文链接：待补充。</p></li><li><p>Summary:</p><ul><li><p>(1)研究背景：近年来，大规模文本到图像（T2I）的扩散模型在图像到图像转换（I2I）中表现出强大的能力，允许通过用户提供的文本提示进行开放域图像翻译。本文提出了一个频率控制的扩散模型（FCDiffusion），这是一个端到端的扩散模型框架，从频率域的角度为文本引导的I2I提供了新的解决方案。</p><p>-(2)过去的方法及问题：早期I2I方法通过GANs学习跨域映射。随着研究的进展，UI2I方法由于无需配对训练数据而受到欢迎。然而，这些方法仍然限于有限的域间翻译。最近，利用CLIP引导I2I的方法允许使用自由形式的文本进行指令，从而将I2I从有限域扩展到开放域能力。然而，这些方法相对较慢且效率较低，因为它们涉及为每个时间点的图像进行单独的CLIP优化过程。因此，存在对更有效的方法的需求。</p><p>-(3)研究方法：本文提出的FCDiffusion模型是一个基于特征空间的频率域过滤模块，该模块使用离散余弦变换（DCT）作为核心。该模块过滤源图像的潜在特征并在DCT域中处理它们，产生不同的控制信号以控制预训练的潜在扩散模型。研究表明，不同DCT频谱带的控制信号在源图像和T2I生成图像之间建立了不同的关联（例如风格、结构、布局、轮廓等）。因此，该方法通过简单地切换不同的频率控制分支即可实现多样化的I2I应用。这是通过对潜在扩散模型的全新利用以及DCT的独特性质实现的。该框架统一了文本引导的I2I任务并适用于多种图像翻译任务。实验证明了其有效性及优越性。此外，框架公开可用并可供进一步研究使用。</p><p>-(4)任务与性能：本文方法在文本引导的I2I任务上取得了显著成果。通过广泛的实验证明其有效性并定量评估其性能。实验结果表明，该方法在多种图像翻译任务上表现出色，包括风格引导的内容创建、图像语义操作、图像场景翻译和图像风格翻译等。性能结果支持了方法的目标并证明了其在实际应用中的有效性。通过简单的频率控制分支切换即可适应不同的翻译任务需求。<br>好的，我将会按照您的要求详细阐述这篇论文的方法论。</p></li></ul></li><li><p>Methods:</p></li></ol><p>（1）研究背景与方法概述：近年来，大规模文本到图像的扩散模型在图像转换中展现出强大的能力。本文在此背景下，提出了频率控制的扩散模型（FCDiffusion），这是一个端到端的扩散模型框架，从频率域的角度为文本引导的图像到图像转换提供了新的解决方案。</p><p>（2）模型构建：模型的核心部分是特征空间的频率域过滤模块，该模块使用离散余弦变换（DCT）作为核心。该模块通过处理源图像的潜在特征并在DCT域中对其进行操作，生成控制信号来控制预训练的潜在扩散模型。</p><p>（3）频率控制信号的利用：研究指出，不同DCT频谱带的控制信号与源图像和文本引导生成的图像之间建立了不同的关联，如风格、结构、布局、轮廓等。通过简单地切换不同的频率控制分支，可以实现多样化的图像翻译应用。</p><p>（4）模型应用：该框架适用于多种图像翻译任务，包括风格引导的内容创建、图像语义操作、图像场景翻译和图像风格翻译等。实验结果表明，该方法在多种图像翻译任务上表现出色。</p><p>（5）模型评估：本文对所提出的方法进行了广泛的实验验证和定量评估，证明了其在实际应用中的有效性。此外，该框架公开可用，为进一步的研究提供了基础。</p><p>以上就是这篇论文的方法论概述。</p><p>好的，以下是对这篇文章的结论性总结：</p><ol><li>Conclusion:</li></ol><p>（1）研究意义：该工作提出一个创新的频率控制扩散模型（FCDiffusion），这是一个端到端的扩散模型框架，用于解决文本引导的图像到图像转换问题。该模型从频率域的角度提供了新的解决方案，具有重要的研究意义和实践价值。</p><p>（2）创新点、性能和工作量总结：</p><p>创新点：该文章提出了一个全新的频率控制扩散模型，该模型通过利用离散余弦变换（DCT）在特征空间进行频率域过滤，生成控制信号来控制预训练的潜在扩散模型。这一方法实现了多样化的图像翻译应用，只需简单切换不同的频率控制分支即可。</p><p>性能：实验结果表明，该模型在多种图像翻译任务上表现出色，包括风格引导的内容创建、图像语义操作、图像场景翻译和图像风格翻译等。这证明了模型在实际应用中的有效性。</p><p>工作量：文章进行了广泛的实验验证和定量评估，证明了模型的有效性。此外，该框架公开可用，为进一步研究提供了基础。但是，文章未详细阐述实验的具体实施过程和数据集，这可能对读者理解模型的性能造成一定影响。同时，文章未详细讨论模型的计算复杂度和运行时间等实际应用中的关键因素。</p><p>希望以上总结对您有所帮助。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-f1047ddd0be7b2efa29c4f070e995e94.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-65c14433465e59f184bf7fbfd3d7f023.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-149af2b24c3ff7f7e8e6c651c07b3699.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-51f3484d597316b34dc642c5e7714334.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-15696cf55299e41305b5108f89410785.jpg" align="middle"></details><h2 id="VEGS-View-Extrapolation-of-Urban-Scenes-in-3D-Gaussian-Splatting-using-Learned-Priors"><a href="#VEGS-View-Extrapolation-of-Urban-Scenes-in-3D-Gaussian-Splatting-using-Learned-Priors" class="headerlink" title="VEGS: View Extrapolation of Urban Scenes in 3D Gaussian Splatting using   Learned Priors"></a>VEGS: View Extrapolation of Urban Scenes in 3D Gaussian Splatting using Learned Priors</h2><p><strong>Authors:Sungwon Hwang, Min-Jung Kim, Taewoong Kang, Jayeon Kang, Jaegul Choo</strong></p><p>Neural rendering-based urban scene reconstruction methods commonly rely on images collected from driving vehicles with cameras facing and moving forward. Although these methods can successfully synthesize from views similar to training camera trajectory, directing the novel view outside the training camera distribution does not guarantee on-par performance. In this paper, we tackle the Extrapolated View Synthesis (EVS) problem by evaluating the reconstructions on views such as looking left, right or downwards with respect to training camera distributions. To improve rendering quality for EVS, we initialize our model by constructing dense LiDAR map, and propose to leverage prior scene knowledge such as surface normal estimator and large-scale diffusion model. Qualitative and quantitative comparisons demonstrate the effectiveness of our methods on EVS. To the best of our knowledge, we are the first to address the EVS problem in urban scene reconstruction. Link to our project page: <a target="_blank" rel="noopener" href="https://vegs3d.github.io/">https://vegs3d.github.io/</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.02945v1">PDF</a></p><p><strong>Summary</strong><br>神经渲染基于城市场景重建方法通常依赖于从前进行驶车辆收集的图像。然而，对于训练相机分布之外的新视角，性能表现不一定保证与训练时相当。</p><p><strong>Key Takeaways</strong></p><ul><li>城市场景重建中的神经渲染方法通常依赖于车辆前进收集的图像。</li><li>在训练相机轨迹类似的视角下，这些方法可以成功合成景象。</li><li>在训练相机分布之外的新视角下，性能可能不如预期。</li><li>该文研究了超出训练相机分布的视角合成（EVS）问题。</li><li>为了改善EVS的渲染质量，提出了使用密集LiDAR地图初始化模型，并利用场景先验知识。</li><li>提出了表面法线估计器和大规模扩散模型等先验场景知识。</li><li>实证和定量比较证明了方法在EVS上的有效性。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>好的，我会按照您的要求进行回答。</p><ol><li><p>标题：基于LiDAR地图的神经网络渲染在城市场景重建中的外延视角合成研究（VEGS: View Extrapolation of Urban Scenes in Supplementary Material）</p></li><li><p>作者：XXX。具体的作者名字需要您提供详细信息。</p></li><li><p>所属机构：XXX大学计算机视觉与智能系统实验室。具体的机构名称需要根据实际情况填写。</p></li><li><p>关键词：神经网络渲染、城市场景重建、外延视角合成（Extrapolated View Synthesis）。</p></li><li><p>链接：论文链接无法确定，Github代码链接（如有）：Github: None。</p></li><li><p>摘要：</p><ul><li>(1)研究背景：本文主要研究在神经网络渲染城市场景重建中，如何合成外延视角（即偏离训练相机轨迹的视角下的视图）。现有的基于图像的城市场景重建方法主要依赖于从驾驶车辆收集的面向前方并移动的图像，当视角偏离训练分布时，合成质量往往下降。因此，本文旨在解决外延视角合成（EVS）问题。</li><li>(2)过去的方法及问题：过去的方法主要关注于与训练相机轨迹相似的视图合成，当面对外延视角时性能不佳。缺乏有效的方法利用场景先验知识提高渲染质量。</li><li>(3)研究方法：本文首先通过构建密集的LiDAR地图初始化模型，然后提出利用场景先验知识，如表面法线估计器和大规模扩散模型，来提高外延视角的合成质量。文章进行了详尽的消融研究，以验证各组件的有效性。</li><li>(4)任务与性能：本文的方法在合成外延视角的任务上取得了显著的性能提升。通过在KITTI-360数据集上的实验，证明了所提出方法的有效性。本文的方法在合成外延视角方面的性能明显优于以往的方法，支持了其目标的实现。</li></ul></li></ol><p>以上内容仅供参考，具体的作者名字、所属机构名称以及论文链接需要根据实际情况填写。</p><ol><li>方法论：</li></ol><p>(1) 概述研究背景与问题：文章主要研究了神经网络渲染城市场景重建中，如何合成外延视角（即偏离训练相机轨迹的视角下的视图）的问题。现有方法主要关注与训练相机轨迹相似的视图合成，面对外延视角时性能不佳。文章旨在解决外延视角合成（EVS）问题。</p><p>(2) 数据准备与模型初始化：文章首先构建密集的LiDAR地图来初始化模型。LiDAR地图提供了场景的高精度几何信息，有助于提升渲染质量。</p><p>(3) 方法介绍：文章提出了利用场景先验知识提高外延视角的合成质量。这些先验知识包括表面法线估计器和大规模扩散模型。通过结合这些先验知识，文章的方法在合成外延视角的任务上取得了显著的性能提升。</p><p>(4) 动态场景建模与渲染：文章建立了动态场景模型，该模型由静态模型和多个动态对象模型组成。每个模型都用高斯均值、协方差矩阵、密度和颜色来表示。协方差矩阵的参数化表示有助于更好地描述场景的几何结构。</p><p>(5) 外延视角的合成：为了合成外延视角，文章使用了大型扩散模型的知识蒸馏方法。通过微调模型参数，实现了在场景特定知识和泛化到未见视图之间的平衡。</p><p>(6) 协方差优化的指导：文章识别了3D高斯模型在优化过程中存在的问题，即协方差形状和方向的过度拟合。为此，文章提出了利用表面法线先验来指导协方差的优化。通过最小化协方差与表面法线之间的对齐损失，有效地解决了协方差的懒惰优化问题。</p><p>总的来说，文章通过结合LiDAR地图、动态场景建模、大型扩散模型的知识蒸馏以及协方差的优化指导等方法，实现了在城市场景重建中合成外延视角的显著性能提升。</p><ol><li>结论：</li></ol><p>（1）本工作的重要性体现在其为城市场景重建领域带来了显著的进展，特别是在合成外延视角（Extrapolated View Synthesis）方面取得了重要突破。该工作提出的基于LiDAR地图的神经网络渲染方法有效提高了城市场景重建的精度和效果，对于自动驾驶、虚拟现实等领域具有潜在的应用价值。</p><p>（2）创新点：本研究提出了一种结合LiDAR地图和神经网络渲染的城市场景重建方法，有效解决了外延视角合成（EVS）的问题。在创新点上，本文利用LiDAR地图提供的高精度几何信息，结合动态场景建模和大型扩散模型的知识蒸馏方法，实现了显著的性能提升。</p><p>性能：本研究在合成外延视角的任务上取得了显著的性能提升，通过在KITTI-360数据集上的实验验证了所提出方法的有效性。与以往的方法相比，本文的方法在合成外延视角方面的性能优势明显。</p><p>工作量：本研究进行了大量的实验和验证工作，包括数据准备、模型初始化、方法介绍、动态场景建模与渲染、外延视角的合成、协方差优化的指导等。同时，本研究还进行了详尽的消融研究，以验证各组件的有效性。</p><p>总体而言，本研究在城市场景重建领域取得了重要的进展，为未来的研究提供了有益的参考和启示。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-ee7e6f3d3d2d8076006569c421fb79cd.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-edf942d1a0c2d9f44c702048d210f70d.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-270e6edeca4c54b0730791728c147383.jpg" align="middle"></details><h2 id="Magic-Insert-Style-Aware-Drag-and-Drop"><a href="#Magic-Insert-Style-Aware-Drag-and-Drop" class="headerlink" title="Magic Insert: Style-Aware Drag-and-Drop"></a>Magic Insert: Style-Aware Drag-and-Drop</h2><p><strong>Authors:Nataniel Ruiz, Yuanzhen Li, Neal Wadhwa, Yael Pritch, Michael Rubinstein, David E. Jacobs, Shlomi Fruchter</strong></p><p>We present Magic Insert, a method for dragging-and-dropping subjects from a user-provided image into a target image of a different style in a physically plausible manner while matching the style of the target image. This work formalizes the problem of style-aware drag-and-drop and presents a method for tackling it by addressing two sub-problems: style-aware personalization and realistic object insertion in stylized images. For style-aware personalization, our method first fine-tunes a pretrained text-to-image diffusion model using LoRA and learned text tokens on the subject image, and then infuses it with a CLIP representation of the target style. For object insertion, we use Bootstrapped Domain Adaption to adapt a domain-specific photorealistic object insertion model to the domain of diverse artistic styles. Overall, the method significantly outperforms traditional approaches such as inpainting. Finally, we present a dataset, SubjectPlop, to facilitate evaluation and future progress in this area. Project page: <a target="_blank" rel="noopener" href="https://magicinsert.github.io/">https://magicinsert.github.io/</a></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.02489v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://magicinsert.github.io/">https://magicinsert.github.io/</a></p><p><strong>Summary</strong><br>提出了 Magic Insert 方法，可从用户提供的图像中将主体拖放到具有不同风格的目标图像中，以物理合理的方式并匹配目标图像的风格。</p><p><strong>Key Takeaways</strong></p><ul><li>Magic Insert 方法允许从一个图像中将主体拖放到另一个风格不同的目标图像中。</li><li>方法首先通过 LoRA 和学习的文本标记对预训练的文本到图像扩散模型进行微调，并结合目标风格的 CLIP 表示。</li><li>为了实现物体插入，采用了 Bootstrapped Domain Adaption 将特定域的逼真物体插入模型适应到多样艺术风格的域中。</li><li>Magic Insert 显著优于传统的修补方法，如修补。</li><li>提供了 SubjectPlop 数据集，以促进该领域的评估和未来进展。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>Please refer to relevant websites for more information, and feel free to ask me any other questions.<br>好的，请您提供具体文章的方法论部分，我将按照要求的格式为您进行详细的中文总结。</p><p>例如，方法论的内容可能是这样的：</p><ol><li>方法论：</li></ol><ul><li>(1) 文章首先通过文献综述的方法梳理了当前领域的研究现状。</li><li>(2) 接着采用了实证研究的方法，通过收集数据进行分析。</li><li>(3) 在数据分析过程中，使用了定量分析与定性分析相结合的方法。</li><li>…（根据实际内容填写）</li></ul><p>请提供具体文章的方法论部分，我会为您进行更详细的中文总结。</p><p>好的，我来帮您总结文章中的结论部分。以下是按照您要求的格式进行的中文总结：</p><ol><li>结论：</li></ol><p>（1）本文的工作意义是什么？<br>本文引入了一种风格感知的拖放问题，这是图像生成领域的一个新挑战。该研究旨在实现在目标图像中直观地插入主体，同时保持风格的一致性。该研究对于图像生成领域的发展具有重要意义，能够推动该领域的进一步探索与进步。</p><p>（2）从创新点、性能和工作量三个方面总结本文的优缺点。<br>创新点：本文提出了Magic Insert方法，通过结合风格感知个性化以及使用引导域适应进行风格插入来解决拖放问题，这是一种新的尝试和探索。<br>性能：Magic Insert方法在风格一致性和插入现实性方面都取得了出色的结果，相对于基准方法有所超越。<br>工作量：文章不仅提出了风格感知的拖放问题，还介绍了用于研究这一问题的Magic Insert方法和SubjectPlop数据集，工作量较大。但同时需要考虑到数据集的广泛性和方法的普及性，以便更多研究者能够参与其中并推动该领域的发展。</p><p>以上总结仅供参考，具体的总结内容还需要根据您提供的文章内容进行调整和补充。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-91e34d3184f2218d1d74c65c2d683b93.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-abc2a451e93c3ab05aa8cc211cf41ec2.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-09fdf88c1998566cabc47fc55ffcceff.jpg" align="middle"></details><h2 id="Boosting-Consistency-in-Story-Visualization-with-Rich-Contextual-Conditional-Diffusion-Models"><a href="#Boosting-Consistency-in-Story-Visualization-with-Rich-Contextual-Conditional-Diffusion-Models" class="headerlink" title="Boosting Consistency in Story Visualization with Rich-Contextual   Conditional Diffusion Models"></a>Boosting Consistency in Story Visualization with Rich-Contextual Conditional Diffusion Models</h2><p><strong>Authors:Fei Shen, Hu Ye, Sibo Liu, Jun Zhang, Cong Wang, Xiao Han, Wei Yang</strong></p><p>Recent research showcases the considerable potential of conditional diffusion models for generating consistent stories. However, current methods, which predominantly generate stories in an autoregressive and excessively caption-dependent manner, often underrate the contextual consistency and relevance of frames during sequential generation. To address this, we propose a novel Rich-contextual Conditional Diffusion Models (RCDMs), a two-stage approach designed to enhance story generation’s semantic consistency and temporal consistency. Specifically, in the first stage, the frame-prior transformer diffusion model is presented to predict the frame semantic embedding of the unknown clip by aligning the semantic correlations between the captions and frames of the known clip. The second stage establishes a robust model with rich contextual conditions, including reference images of the known clip, the predicted frame semantic embedding of the unknown clip, and text embeddings of all captions. By jointly injecting these rich contextual conditions at the image and feature levels, RCDMs can generate semantic and temporal consistency stories. Moreover, RCDMs can generate consistent stories with a single forward inference compared to autoregressive models. Our qualitative and quantitative results demonstrate that our proposed RCDMs outperform in challenging scenarios. The code and model will be available at <a target="_blank" rel="noopener" href="https://github.com/muzishen/RCDMs">https://github.com/muzishen/RCDMs</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.02482v1">PDF</a></p><p><strong>Summary</strong><br>条件扩散模型展示了在生成连贯故事方面的潜力，提出了丰富语境条件条件扩散模型(RCDMs)来增强语义和时间连贯性。</p><p><strong>Key Takeaways</strong></p><ul><li>条件扩散模型有助于生成连贯故事，避免传统方法中的语境一致性问题。</li><li>提出了丰富语境条件扩散模型(RCDMs)，采用两阶段方法增强故事生成的语义一致性和时间一致性。</li><li>第一阶段引入了框架先验转换扩散模型，预测未知片段的语义嵌入。</li><li>第二阶段结合了参考图像、预测的框架语义嵌入和所有标题的文本嵌入，实现了丰富的语境条件。</li><li>RCDMs能够在单次推理中生成连贯故事，相较自回归模型表现更优。</li><li>定性和定量结果显示，RCDMs在挑战性场景中表现出色。</li><li>提供了代码和模型的开放资源链接：<a target="_blank" rel="noopener" href="https://github.com/muzishen/RCDMs。">https://github.com/muzishen/RCDMs。</a></li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>好的，我会按照您的要求进行回答。</p><ol><li>Title: 基于富文本上下文的条件扩散模型提升故事可视化一致性研究</li><li>Authors: 费飞, 叶虎, 刘思博, 张俊, 王聪, 韩啸, 杨威</li><li>Affiliation: 腾讯AI实验室</li><li>Keywords: 故事可视化、扩散模型、富文本上下文</li><li>Urls: <a target="_blank" rel="noopener" href="https://github.com/muzishen/RCDMs">https://github.com/muzishen/RCDMs</a> （Github代码链接）</li><li>Summary:</li></ol><p>(1) 研究背景：<br>随着自然语言处理和计算机视觉技术的发展，故事可视化已成为一个热门研究领域。该文章关注于如何利用条件扩散模型生成一致性的故事，以提升故事可视化效果。</p><p>(2) 过去的方法及问题：<br>当前的方法主要基于自回归和过度依赖标题的方式生成故事，但这种方法忽视了帧的上下文一致性和相关性。文章提出存在的问题是在序列生成过程中缺乏帧间的一致性和语义连续性。</p><p>(3) 研究方法：<br>文章提出了一种名为Rich-contextual Conditional Diffusion Models (RCDMs)的两阶段方法，旨在增强故事生成的语义一致性和时间一致性。在第一阶段，使用帧优先变压器扩散模型预测未知剪辑的帧语义嵌入，通过对齐已知剪辑的标题和帧之间的语义关联来实现。在第二阶段，建立一个具有丰富上下文条件的稳健模型，包括已知剪辑的参考图像、未知剪辑的预测帧语义嵌入和所有标题的文本嵌入。通过联合注入这些丰富的上下文条件在图像和特征级别，RCDMs能够生成语义和时间上一致的故事。此外，RCDMs可以通过单次前向推理生成一致的故事，与自回归模型相比具有优势。</p><p>(4) 任务与性能：<br>文章在故事可视化任务上进行了实验，证明了RCDMs方法的性能优于其他方法，特别是在具有挑战性的场景下。此外，文章提供了定量和定性的结果来支持其方法的性能。</p><p>性能评估方面，该文章提出的RCDMs方法能够在故事可视化任务中生成具有语义和时间一致性的故事，并且在挑战性场景下表现出优越的性能。实验结果表明，该方法可以支持其目标并生成高质量的故事。<br>好的，我会按照您的要求对文章的方法部分进行详细描述。</p><ol><li>方法：</li></ol><p>（1）背景介绍：本研究针对故事可视化领域中的序列生成问题展开。现有方法主要依赖自回归和标题的方式生成故事，但忽视了帧的上下文一致性和相关性，导致生成的故事缺乏语义连续性和时间一致性。</p><p>（2）研究方法介绍：本研究提出了一种名为Rich-contextual Conditional Diffusion Models (RCDMs)的两阶段方法。第一阶段是利用帧优先变压器扩散模型预测未知剪辑的帧语义嵌入，通过语义关联对齐已知剪辑的标题和帧。第二阶段是建立一个丰富上下文条件的模型，其中包括已知剪辑的参考图像、未知剪辑的预测帧语义嵌入和所有标题的文本嵌入。这些丰富的上下文条件在图像和特征级别联合注入，使得模型能够生成语义和时间上一致的故事。此外，RCDMs方法可以通过单次前向推理生成一致的故事，相较于自回归模型具有优势。</p><p>（3）实验设计：本研究在故事可视化任务上进行实验，通过与其他方法的对比实验来证明RCDMs方法的性能优势。实验包括对不同场景下的故事可视化任务进行实验，并对实验结果进行定量和定性的评估。实验结果表明，RCDMs方法能够在故事可视化任务中生成具有语义和时间一致性的故事，并且在挑战性场景下表现出优越的性能。</p><p>好的，我会按照您的要求来总结这篇文章。</p><p>关于文章的重要性的结论：<br>该文章对故事可视化领域进行了深入研究，提出了一种基于富文本上下文的条件扩散模型（RCDMs），旨在解决故事可视化中的一致性问题。该研究具有重要的理论和实践意义，对于提升故事可视化效果、拓展自然语言处理和计算机视觉技术的融合应用具有重要意义。同时，该研究也有助于推动故事可视化领域的进一步发展。<br>关于创新点、性能和工作量的结论：<br>创新点：该文章提出了RCDMs方法，通过引入丰富的上下文条件，在图像和特征级别上增强故事生成的语义一致性和时间一致性。该方法相较于传统的自回归方法具有优势，可以通过单次前向推理生成一致的故事。此外，该文章使用的扩散模型在自然语言处理和计算机视觉领域的结合上是一个新的尝试，具有创新性。<br>性能：该文章在故事可视化任务上进行了实验，证明了RCDMs方法的性能优于其他方法，特别是在挑战性场景下。实验结果表明，该方法能够生成具有语义和时间一致性的故事，并且具有良好的鲁棒性和可扩展性。此外，该文章提供了定量和定性的结果来支持其方法的性能。<br>工作量：该文章涉及的研究工作包括提出新的模型架构、设计实验方案、进行实验验证、分析实验结果等。工作量较大，具有一定的复杂性。同时，该文章对相关工作进行了全面的调研和分析，为研究工作提供了坚实的基础。但文章未提及跨数据集角色多样性的限制，这也是其潜在的一个局限性。<br>总体来说，该文章具有重要的理论和实践意义，具有创新性，在性能和工作量方面表现良好。未来研究方向可以包括如何克服跨数据集角色多样性的限制以及如何进一步提高生成故事的质量和多样性等方面。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-c62c9a8e74774ea1d684deb0c7217b9f.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-d5b65b7fa9624f0213b9bf74e8da7e0e.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-1da2d6bc1c0057da5739898d3b9b6e2c.jpg" align="middle"></details></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io/2024/07/05/Paper/2024-07-05/Diffusion%20Models/">https://kedreamix.github.io/2024/07/05/Paper/2024-07-05/Diffusion Models/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Diffusion-Models/">Diffusion Models</a></div><div class="post_share"><div class="social-share" data-image="https://pic1.zhimg.com/v2-d5b65b7fa9624f0213b9bf74e8da7e0e.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/07/05/Paper/2024-07-05/Talking%20Head%20Generation/" title="Talking Head Generation"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-fa427c8639955ed81a2ca89929b31915.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Talking Head Generation</div></div></a></div><div class="next-post pull-right"><a href="/2024/07/05/Paper/2024-07-05/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙/虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-67214cc96cc366e005e4d95251d82685.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">元宇宙/虚拟人</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/03/03/Paperscape/EMO/" title="EMO Emote Portrait Alive - 阿里HumanAIGC"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-6492e24fb03ffa98135dc584535ab7d9.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-03</div><div class="title">EMO Emote Portrait Alive - 阿里HumanAIGC</div></div></a></div><div><a href="/2024/01/24/Paper/2024-01-24/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-71a37c439c6714e8867560f580599d2f.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-24</div><div class="title">Diffusion Models</div></div></a></div><div><a href="/2024/01/30/Paper/2024-01-30/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-e55358c77a9d65f15701e8f33262e2a4.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-30</div><div class="title">Diffusion Models</div></div></a></div><div><a href="/2024/02/02/Paper/2024-02-02/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-5920453c69c00995f18077b22d4a790e.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-02</div><div class="title">Diffusion Models</div></div></a></div><div><a href="/2024/02/13/Paper/2024-02-13/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-3709a9941aada6c4d3ed35934e311765.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-13</div><div class="title">Diffusion Models</div></div></a></div><div><a href="/2024/02/09/Paper/2024-02-09/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-32488f736ee10537497afccc3a1a1d76.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-09</div><div class="title">Diffusion Models</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-07-05-%E6%9B%B4%E6%96%B0"><span class="toc-text">2024-07-05 更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#DisCo-Diff-Enhancing-Continuous-Diffusion-Models-with-Discrete-Latents"><span class="toc-text">DisCo-Diff: Enhancing Continuous Diffusion Models with Discrete Latents</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Frequency-Controlled-Diffusion-Model-for-Versatile-Text-Guided-Image-to-Image-Translation"><span class="toc-text">Frequency-Controlled Diffusion Model for Versatile Text-Guided Image-to-Image Translation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#VEGS-View-Extrapolation-of-Urban-Scenes-in-3D-Gaussian-Splatting-using-Learned-Priors"><span class="toc-text">VEGS: View Extrapolation of Urban Scenes in 3D Gaussian Splatting using Learned Priors</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Magic-Insert-Style-Aware-Drag-and-Drop"><span class="toc-text">Magic Insert: Style-Aware Drag-and-Drop</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Boosting-Consistency-in-Story-Visualization-with-Rich-Contextual-Conditional-Diffusion-Models"><span class="toc-text">Boosting Consistency in Story Visualization with Rich-Contextual Conditional Diffusion Models</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image:url('https://pic1.zhimg.com/v2-d5b65b7fa9624f0213b9bf74e8da7e0e.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js").then((()=>{pangu.autoSpacingPage()}))}function panguInit(){panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.1},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script></div><script id="canvas_nest" defer color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><script data-pjax>function butterfly_swiper_injector_config(){var a=document.getElementById("recent-posts");console.log("已挂载butterfly_swiper"),a.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/25/Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="2024/01/25/Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="2024/01/25/Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/17/Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023/12/17/Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="2023/12/17/Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script></body></html>