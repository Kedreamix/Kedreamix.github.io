<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>Talking Head Generation | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-10-12  MMHead Towards Fine-grained Multi-modal 3D Facial Animation"><meta property="og:type" content="article"><meta property="og:title" content="Talking Head Generation"><meta property="og:url" content="https://kedreamix.github.io/2024/10/12/Paper/2024-10-12/Talking%20Head%20Generation/index.html"><meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World"><meta property="og:description" content="Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-10-12  MMHead Towards Fine-grained Multi-modal 3D Facial Animation"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://pic1.zhimg.com/v2-bf351a38d373ad29c81b373fe10d2463.jpg"><meta property="article:published_time" content="2024-10-11T22:04:04.000Z"><meta property="article:modified_time" content="2024-10-11T22:04:04.196Z"><meta property="article:author" content="Kedreamix"><meta property="article:tag" content="Talking Head Generation"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://pic1.zhimg.com/v2-bf351a38d373ad29c81b373fe10d2463.jpg"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/2024/10/12/Paper/2024-10-12/Talking%20Head%20Generation/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-74LZ5BEQQ1")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!0,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"简"},noticeOutdate:{limitDay:500,position:"top",messagePrev:"It has been",messageNext:"days since the last update, the content of the article may be outdated."},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:200},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!0,post:!0},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!0,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"Talking Head Generation",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-10-12 06:04:04"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach((e=>{n.setAttribute(e,t[e])})),document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)})),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=24?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 7.0.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml"></head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{const d=document.getElementById("loading-box"),e=document.body,o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l(),window.addEventListener("load",(()=>{o()}))})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">245</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url('https://pic1.zhimg.com/v2-bf351a38d373ad29c81b373fe10d2463.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Talking Head Generation</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-10-11T22:04:04.000Z" title="发表于 2024-10-12 06:04:04">2024-10-12</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-10-11T22:04:04.196Z" title="更新于 2024-10-12 06:04:04">2024-10-12</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">9.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>32分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="Talking Head Generation"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-10-12-更新"><a href="#2024-10-12-更新" class="headerlink" title="2024-10-12 更新"></a>2024-10-12 更新</h1><h2 id="MMHead-Towards-Fine-grained-Multi-modal-3D-Facial-Animation"><a href="#MMHead-Towards-Fine-grained-Multi-modal-3D-Facial-Animation" class="headerlink" title="MMHead: Towards Fine-grained Multi-modal 3D Facial Animation"></a>MMHead: Towards Fine-grained Multi-modal 3D Facial Animation</h2><p><strong>Authors:Sijing Wu, Yunhao Li, Yichao Yan, Huiyu Duan, Ziwei Liu, Guangtao Zhai</strong></p><p>3D facial animation has attracted considerable attention due to its extensive applications in the multimedia field. Audio-driven 3D facial animation has been widely explored with promising results. However, multi-modal 3D facial animation, especially text-guided 3D facial animation is rarely explored due to the lack of multi-modal 3D facial animation dataset. To fill this gap, we first construct a large-scale multi-modal 3D facial animation dataset, MMHead, which consists of 49 hours of 3D facial motion sequences, speech audios, and rich hierarchical text annotations. Each text annotation contains abstract action and emotion descriptions, fine-grained facial and head movements (i.e., expression and head pose) descriptions, and three possible scenarios that may cause such emotion. Concretely, we integrate five public 2D portrait video datasets, and propose an automatic pipeline to 1) reconstruct 3D facial motion sequences from monocular videos; and 2) obtain hierarchical text annotations with the help of AU detection and ChatGPT. Based on the MMHead dataset, we establish benchmarks for two new tasks: text-induced 3D talking head animation and text-to-3D facial motion generation. Moreover, a simple but efficient VQ-VAE-based method named MM2Face is proposed to unify the multi-modal information and generate diverse and plausible 3D facial motions, which achieves competitive results on both benchmarks. Extensive experiments and comprehensive analysis demonstrate the significant potential of our dataset and benchmarks in promoting the development of multi-modal 3D facial animation.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.07757v1">PDF</a> Accepted by ACMMM 2024. Project page: <a target="_blank" rel="noopener" href="https://wsj-sjtu.github.io/MMHead/">https://wsj-sjtu.github.io/MMHead/</a></p><p><strong>Summary</strong><br>构建大规模多模态3D面部动画数据集MMHead，推动文本引导3D面部动画发展。</p><p><strong>Key Takeaways</strong></p><ol><li>3D面部动画在多媒体领域应用广泛，但多模态动画研究较少。</li><li>MMHead数据集包含3D面部运动、语音音频和文本注释，涵盖动作、情绪和场景描述。</li><li>集成5个公共2D肖像视频数据集，自动重建3D面部运动序列。</li><li>利用AU检测和ChatGPT获取文本注释，实现语义到动作的映射。</li><li>建立文本诱导3D talking head动画和文本到3D面部运动生成两个新任务基准。</li><li>提出MM2Face方法，融合多模态信息生成3D面部运动。</li><li>MMHead数据集和基准在多模态3D面部动画发展中具有重大潜力。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: MMHead：面向精细多模态的三维面部动画研究</p></li><li><p>Authors: Sijing Wu（吴思静）, Yunhao Li（李云豪）, Yichao Yan（颜一超）, Huiyu Duan（段慧宇）, Ziwei Liu（刘子玮）, Guangtao Zhai（翟光涛）</p></li><li><p>Affiliation: 上海交通大学</p></li><li><p>Keywords: 多模态三维面部动画；MMHead数据集；文本引导的面部动画；自动管道重建；层次文本注释；VQ-VAE方法；3D面部运动生成</p></li><li><p>Urls: 论文链接待确认，GitHub代码链接（如有）: None</p></li><li><p>Summary:</p><ul><li>(1) 研究背景：本文关注多模态三维面部动画领域的研究背景，鉴于三维面部动画在多媒体领域中的广泛应用，相关研究引起了广泛的关注。特别是音频驱动的三维面部动画已得到了广泛的探索与显著的结果，但文本引导的三维面部动画由于缺少多模态数据集而研究较少。因此，本文旨在解决这一问题。</li><li>(2) 过去的方法及其问题：当前研究虽然已经取得了进展，但由于缺乏大规模的多模态三维面部动画数据集，现有的方法在实践中难以达到预期效果。由于缺乏丰富的面部表情、头部姿态和可能的场景信息，现有数据集限制了该领域的发展。因此，本文提出建立一个新的多模态三维面部动画数据集MMHead。</li><li>(3) 研究方法：本文首先整合了五个公共二维肖像视频数据集，并开发了一个自动管道来处理这些数据集，包括从单目视频中重建三维面部运动序列和借助AU检测和ChatGPT获取层次化文本注释。基于MMHead数据集，本文建立了两个新任务的标准：文本引导的三维说话人动画和文本到三维面部运动的生成。同时，提出了一种简单的但有效的VQ-VAE方法——MM2Face，能够统一多模态信息并生成多样且合理的三维面部运动。</li><li>(4) 任务与性能：本文在建立的新数据集MMHead上进行了实验，验证了MMHead数据集的有效性和适用性。同时，基于该数据集建立的基准测试证明了MM2Face方法的竞争力。本文的研究成果对于推动多模态三维面部动画领域的发展具有重要意义。数据集将在指定的网站上公开发布。</li></ul></li></ol><p>请注意，由于论文尚未公开发表，某些链接和详细信息可能暂时不可用。以上内容是基于提供的论文摘要和相关信息进行的整理。</p><ol><li>方法：</li></ol><ul><li>(1) 研究背景分析：研究团队发现多模态三维面部动画在多媒体领域有广泛的应用前景，但目前相关研究因缺乏大规模多模态数据集而受到限制。特别是文本引导的三维面部动画领域缺乏相关的数据集，这影响了相关研究的发展。因此，研究团队决定解决这一问题。</li><li>(2) 数据集建立：为了克服现有研究的局限性，研究团队整合了五个公共二维肖像视频数据集，并开发了一个自动管道来处理这些数据集。该管道包括从单目视频中重建三维面部运动序列和借助AU检测和ChatGPT获取层次化文本注释。在此基础上，研究团队建立了新的多模态三维面部动画数据集MMHead。</li><li>(3) 新任务定义：基于MMHead数据集，研究团队建立了两个新任务的标准，即文本引导的三维说话人动画和文本到三维面部运动的生成。这两个任务的建立为后续的研究提供了基准测试。</li><li>(4) 方法提出：研究团队提出了一种简单的但有效的VQ-VAE方法——MM2Face。该方法能够统一多模态信息并生成多样且合理的三维面部运动。该方法基于MMHead数据集进行训练和测试，实验结果证明了其有效性和竞争力。数据集将在指定的网站上公开发布。该方法的创新性在于结合了多模态信息，使得三维面部动画更加生动自然。具体的操作流程和技术细节在论文中详细阐述。</li></ul><p>以上是对该论文方法部分的详细概括和总结，希望能够对您有所帮助。</p><ol><li>Conclusion:</li></ol><p>(1) 这篇文章的工作意义在于推动了多模态三维面部动画领域的发展。文章建立了一个新的多模态三维面部动画数据集MMHead，并基于该数据集建立了两个新任务的标准，即文本引导的三维说话人动画和文本到三维面部运动的生成。此外，文章提出了一种有效的VQ-VAE方法——MM2Face，能够统一多模态信息并生成多样且合理的三维面部运动。这些数据集和任务标准的建立以及方法的提出将有助于推动多模态三维面部动画领域的研究和应用。</p><p>(2) 创新点：文章建立了新的多模态三维面部动画数据集MMHead，并基于该数据集提出了两个新任务的标准，体现了较强的创新性。同时，文章提出的MM2Face方法能够统一多模态信息，并生成合理且多样的三维面部运动，具有较高的性能。但文章在性能方面也存在一定局限性，如对于大规模数据集的处理和复杂场景的应用仍需进一步研究和优化。在工作量方面，文章整合了多个公共二维肖像视频数据集并开发了自动管道处理这些数据集，工作量较大；但在具体方法的提出和实验验证方面，文章内容相对简洁，未涉及大量复杂的计算和推导。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-42f46b66e7d42d2ba71796a57ce9de1a.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-6a5913c90431177376e725a374854ba1.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-1ff25a2e0173b8c4c67bb51d49a7322e.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-50463d983b849736c22e81e3e312927d.jpg" align="middle"></details><h2 id="Hallo2-Long-Duration-and-High-Resolution-Audio-Driven-Portrait-Image-Animation"><a href="#Hallo2-Long-Duration-and-High-Resolution-Audio-Driven-Portrait-Image-Animation" class="headerlink" title="Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image   Animation"></a>Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation</h2><p><strong>Authors:Jiahao Cui, Hui Li, Yao Yao, Hao Zhu, Hanlin Shang, Kaihui Cheng, Hang Zhou, Siyu Zhu, Jingdong Wang</strong></p><p>Recent advances in latent diffusion-based generative models for portrait image animation, such as Hallo, have achieved impressive results in short-duration video synthesis. In this paper, we present updates to Hallo, introducing several design enhancements to extend its capabilities. First, we extend the method to produce long-duration videos. To address substantial challenges such as appearance drift and temporal artifacts, we investigate augmentation strategies within the image space of conditional motion frames. Specifically, we introduce a patch-drop technique augmented with Gaussian noise to enhance visual consistency and temporal coherence over long duration. Second, we achieve 4K resolution portrait video generation. To accomplish this, we implement vector quantization of latent codes and apply temporal alignment techniques to maintain coherence across the temporal dimension. By integrating a high-quality decoder, we realize visual synthesis at 4K resolution. Third, we incorporate adjustable semantic textual labels for portrait expressions as conditional inputs. This extends beyond traditional audio cues to improve controllability and increase the diversity of the generated content. To the best of our knowledge, Hallo2, proposed in this paper, is the first method to achieve 4K resolution and generate hour-long, audio-driven portrait image animations enhanced with textual prompts. We have conducted extensive experiments to evaluate our method on publicly available datasets, including HDTF, CelebV, and our introduced “Wild” dataset. The experimental results demonstrate that our approach achieves state-of-the-art performance in long-duration portrait video animation, successfully generating rich and controllable content at 4K resolution for duration extending up to tens of minutes. Project page <a target="_blank" rel="noopener" href="https://fudan-generative-vision.github.io/hallo2">https://fudan-generative-vision.github.io/hallo2</a></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.07718v1">PDF</a></p><p><strong>Summary</strong><br>研究提出改进后的Hallo模型，可生成长时、4K分辨率、文本驱动的肖像动画视频。</p><p><strong>Key Takeaways</strong></p><ol><li>Hallo模型升级支持长时视频合成。</li><li>引入图像空间中的增强策略，解决外观漂移和时序伪影。</li><li>实现了4K分辨率肖像视频生成。</li><li>采用矢量量化潜代码和时序对齐技术。</li><li>引入可调节的语义文本标签，提高可控性和内容多样性。</li><li>Hallo2为首个实现4K分辨率、时长可达数小时的文本驱动肖像动画方法。</li><li>在多个数据集上实验证明，该方法在长时肖像视频动画方面达到最先进水平。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>方法：</li></ol><p><em>(1) SEINE Chen等人（2023b）引入生成过渡的概念：</em></p><ul><li>该方法关注于平滑场景变化，通过生成过渡来增强视觉连贯性。</li><li>使用语义运动预测器来预测场景中的动作和事件，以实现自然流畅的过渡效果。</li></ul><p><em>(2) StoryDiffusion Zhou等人（2024）的方法：</em></p><ul><li>该方法专注于视觉故事叙述，通过引入生成模型来创建连贯的故事情节。</li><li>利用语义运动预测器来分析场景中的对象动作和它们之间的潜在关系。</li><li>通过这种方法，能够生成具有逻辑连贯性的场景，实现流畅的故事叙述。</li></ul><p>以上内容仅为根据您提供的描述进行的概括，具体的细节可能需要查阅原文以获取更准确的信息。</p><ol><li>结论：</li></ol><p>(1) 这项工作的意义在于：<br>该文章展示了通过增强Hallo框架的功能在肖像图像动画方面的进展。通过扩展动画持续时间至数十分钟并保持高分辨率的4K输出，该研究解决了现有方法的显著局限性。该工作对长时间、高分辨率肖像图像动画领域做出了重要贡献。</p><p>(2) 创新性、性能和工作量方面的总结：</p><ul><li>创新性：文章提出了创新的肖像图像动画方法，通过扩展动画持续时间、实施数据增强技术、实施向量量化潜在代码以及采用时间对齐技术等方法，为肖像图像动画带来了新的突破。此外，文章还结合了音频驱动信号和可调整的语义文本提示，实现了对面部表情和运动动态的精确控制。</li><li>性能：文章的方法在公共数据集上进行了广泛的实验验证，证明了其有效性。所生成的动画具有逼真的效果，且长时间保持高质量输出。然而，关于性能的具体数据（如处理速度、内存占用等）未在摘要中提及。</li><li>工作量：文章涉及大量的实验验证和算法开发，工作量较大。此外，文章的方法需要较高的计算资源和存储资源来实现高分辨率的长时间动画。但由于摘要中没有具体提及工作量的大小和计算资源的具体需求，无法准确评估该方面。</li></ul><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-bf351a38d373ad29c81b373fe10d2463.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-77d1fa55cf81360393f5957b78ed13bf.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-f81bbe1cc73d4a426701300e3abb6f04.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-27d927b8dac8bd9f3b3b9b030bc7fc2b.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-63f166e791c3b6969cb0c682cb2ee1ed.jpg" align="middle"></details><h2 id="FaceVid-1K-A-Large-Scale-High-Quality-Multiracial-Human-Face-Video-Dataset"><a href="#FaceVid-1K-A-Large-Scale-High-Quality-Multiracial-Human-Face-Video-Dataset" class="headerlink" title="FaceVid-1K: A Large-Scale High-Quality Multiracial Human Face Video   Dataset"></a>FaceVid-1K: A Large-Scale High-Quality Multiracial Human Face Video Dataset</h2><p><strong>Authors:Donglin Di, He Feng, Wenzhang Sun, Yongjia Ma, Hao Li, Wei Chen, Xiaofei Gou, Tonghua Su, Xun Yang</strong></p><p>Generating talking face videos from various conditions has recently become a highly popular research area within generative tasks. However, building a high-quality face video generation model requires a well-performing pre-trained backbone, a key obstacle that universal models fail to adequately address. Most existing works rely on universal video or image generation models and optimize control mechanisms, but they neglect the evident upper bound in video quality due to the limited capabilities of the backbones, which is a result of the lack of high-quality human face video datasets. In this work, we investigate the unsatisfactory results from related studies, gather and trim existing public talking face video datasets, and additionally collect and annotate a large-scale dataset, resulting in a comprehensive, high-quality multiracial face collection named \textbf{FaceVid-1K}. Using this dataset, we craft several effective pre-trained backbone models for face video generation. Specifically, we conduct experiments with several well-established video generation models, including text-to-video, image-to-video, and unconditional video generation, under various settings. We obtain the corresponding performance benchmarks and compared them with those trained on public datasets to demonstrate the superiority of our dataset. These experiments also allow us to investigate empirical strategies for crafting domain-specific video generation tasks with cost-effective settings. We will make our curated dataset, along with the pre-trained talking face video generation models, publicly available as a resource contribution to hopefully advance the research field.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.07151v1">PDF</a></p><p><strong>Summary</strong><br>我们构建了FaceVid-1K数据集，并设计预训练模型提升人脸视频生成质量。</p><p><strong>Key Takeaways</strong></p><ol><li>人脸视频生成研究热门，但高质模型需高性能预训练骨干。</li><li>现有模型忽视骨干限制，导致视频质量上限。</li><li>我们收集和整理了FaceVid-1K高质量人脸视频数据集。</li><li>设计预训练骨干模型，优化人脸视频生成。</li><li>在多种生成模型下进行实验，包括文本到视频、图像到视频和无条件视频生成。</li><li>实验表明，我们的数据集在性能上优于公共数据集。</li><li>数据集和模型将公开发布，以促进研究进展。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>Conclusion:</li></ol><ul><li>(1) 这项工作的意义在于引入了一个大规模、高质量、多种族的人脸视频数据集FaceVid-1K，并强调该资源能够满足人脸视频生成相关研究任务的需求。此外，通过在该数据集上进行的广泛实验，获得了一些有价值的实证见解。</li><li>(2) 创新点：引入了新的大规模、多种族的人脸视频数据集FaceVid-1K，并进行了广泛的实验验证。性能：文章未具体提及该数据集或实验的具体性能指标。工作量：数据集收集与实验的工作量较大，但文章未具体阐述其工作量的大小或投入的资源。</li></ul><p>请注意，由于无法获取整篇文章的完整内容，我的回答可能有所偏差。如果有需要，请提供更多文章信息以便更准确地回答。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-515afa1627a07ec6cd0b302c38c1577e.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-8dd77590dd8da3e9af10f3ce40ba386d.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-0462eb9f319928ced3326832fa043790.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-e5b52bc6d6018d95babb95c73e966833.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-cebc81cdf124fcc840075376cb634fbd.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-5b9b6908fdc18eab28a4f84f59456fef.jpg" align="middle"></details><h2 id="MimicTalk-Mimicking-a-personalized-and-expressive-3D-talking-face-in-minutes"><a href="#MimicTalk-Mimicking-a-personalized-and-expressive-3D-talking-face-in-minutes" class="headerlink" title="MimicTalk: Mimicking a personalized and expressive 3D talking face in   minutes"></a>MimicTalk: Mimicking a personalized and expressive 3D talking face in minutes</h2><p><strong>Authors:Zhenhui Ye, Tianyun Zhong, Yi Ren, Ziyue Jiang, Jiawei Huang, Rongjie Huang, Jinglin Liu, Jinzheng He, Chen Zhang, Zehan Wang, Xize Chen, Xiang Yin, Zhou Zhao</strong></p><p>Talking face generation (TFG) aims to animate a target identity’s face to create realistic talking videos. Personalized TFG is a variant that emphasizes the perceptual identity similarity of the synthesized result (from the perspective of appearance and talking style). While previous works typically solve this problem by learning an individual neural radiance field (NeRF) for each identity to implicitly store its static and dynamic information, we find it inefficient and non-generalized due to the per-identity-per-training framework and the limited training data. To this end, we propose MimicTalk, the first attempt that exploits the rich knowledge from a NeRF-based person-agnostic generic model for improving the efficiency and robustness of personalized TFG. To be specific, (1) we first come up with a person-agnostic 3D TFG model as the base model and propose to adapt it into a specific identity; (2) we propose a static-dynamic-hybrid adaptation pipeline to help the model learn the personalized static appearance and facial dynamic features; (3) To generate the facial motion of the personalized talking style, we propose an in-context stylized audio-to-motion model that mimics the implicit talking style provided in the reference video without information loss by an explicit style representation. The adaptation process to an unseen identity can be performed in 15 minutes, which is 47 times faster than previous person-dependent methods. Experiments show that our MimicTalk surpasses previous baselines regarding video quality, efficiency, and expressiveness. Source code and video samples are available at <a target="_blank" rel="noopener" href="https://mimictalk.github.io">https://mimictalk.github.io</a> .</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.06734v1">PDF</a> Accepted by NeurIPS 2024</p><p><strong>Summary</strong><br>提出MimicTalk模型，利用NeRF构建的通用模型，实现高效、个性化的说话人脸生成。</p><p><strong>Key Takeaways</strong></p><ul><li>提出MimicTalk，优化个性化说话人脸生成。</li><li>利用NeRF构建通用模型，提高效率和泛化能力。</li><li>设计静态-动态混合适配流程，学习个性化特征。</li><li>首次实现风格化的音频到动作模型，模仿说话风格。</li><li>适配过程快速，15分钟完成，远超传统方法。</li><li>实验证明，MimicTalk在视频质量、效率和表现力方面优于前人。</li><li>开放源代码和视频示例。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><strong>标题</strong>：MimicTalk: 模仿个性化与表达力的三维人脸快速生成技术（中文翻译）。</li><li><strong>作者</strong>：Zhenhui Ye, Tianyun Zhong, Yi Ren, 等（作者名单及所属单位）。</li><li><strong>作者所属单位（中文翻译）</strong>：浙江大学（Zhejiang University）与字节跳动（ByteDance）。</li><li><strong>关键词</strong>：Talking Face Generation (TFG), Personalized TFG, Neural Radiance Fields (NeRF), Efficient Adaptation, Style Mimicking。</li><li><strong>链接</strong>：论文链接（待补充），GitHub代码链接：[GitHub链接尚未提供]（如果可用）。</li><li><p><strong>摘要</strong>：</p><ul><li>(1) 研究背景：本文研究音频驱动的三维人脸生成技术，特别是针对个性化与表达力的快速生成方法。随着多媒体技术的发展，音频与视觉结合的应用越来越广泛，此技术广泛应用于视频会议、音频可视化聊天机器人等领域。</li><li>(2) 现有方法与问题：现有的个性化三维人脸生成方法通常采用学习个体神经辐射场（NeRF）的方式为每个身份隐式存储静态和动态信息。但这种方式效率低且缺乏通用性，因为需要为每个身份进行单独的训练且受限于训练数据。</li><li>(3) 研究方法：提出MimicTalk方法，首次利用通用的非个性化NeRF模型提高个性化TFG的效率与稳健性。包括构建一个非个性化的三维TFG模型作为基准模型，并对其进行个性化适应；采用静态与动态结合的适应流程来学习个性化的静态外观和面部动态特征；提出上下文风格化的音频到动作模型，模仿参考视频中的隐性说话风格。</li><li>(4) 任务与性能：在个性化说话风格的三维人脸生成任务上取得显著成果，与之前的基线相比，在视频质量、效率和表现力方面都有提升。实验证明MimicTalk方法的优越性。</li></ul></li></ol><h3 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h3><ul><li><strong>(1)</strong> 研究背景：本文研究了音频驱动的三维人脸生成技术，特别是如何快速生成具有个性化与表达力的三维人脸，在多媒体领域有广泛的应用前景。</li><li><strong>(2)</strong> 过去的方法与问题：现有方法通过为每个身份学习个体神经辐射场（NeRF）来隐式存储信息，但效率较低且缺乏通用性。</li><li><strong>(3)</strong> 研究方法：本文提出了MimicTalk方法，利用通用的非个性化NeRF模型来提高个性化TFG的效率与稳健性，包括构建基准模型、个性化适应、静态与动态特征学习以及音频到动作的风格模仿模型。</li><li><strong>(4)</strong> 任务与性能：在个性化说话风格的三维人脸生成任务上取得显著成果，实验证明该方法在视频质量、效率和表现力方面均超越先前方法。性能结果支持了该方法的有效性。</li></ul><ol><li>方法：</li></ol><p>(1) 研究背景与问题定义：文章聚焦于音频驱动的三维人脸生成技术，特别是如何实现快速生成具有个性化与表达力的三维人脸。针对现有方法效率低和缺乏通用性的问题，提出了改进的需求。</p><p>(2) 方法概述：提出MimicTalk方法，该方法利用通用的非个性化NeRF模型来提高个性化TFG的效率与稳健性。首先，构建一个非个性化的三维TFG模型作为基准模型。然后，对此基准模型进行个性化适应，以适应不同个体的特征。</p><p>(3) 静态与动态特征学习：采用静态与动态结合的适应流程，通过学习个性化的静态外观和面部动态特征，实现更为真实和自然的人脸生成。</p><p>(4) 音频到动作的风格模仿：提出上下文风格化的音频到动作模型，该模型能够模仿参考视频中的隐性说话风格，使得生成的三维人脸能够体现原说话人的表达特点。</p><p>(5) 实验验证：通过大量的实验验证，在个性化说话风格的三维人脸生成任务上，MimicTalk方法取得了显著成果，并在视频质量、效率和表现力方面均有提升。与现有方法相比，表现出了优越性。</p><ol><li>Conclusion:</li></ol><p>(1)工作意义：这篇文章研究了音频驱动的三维人脸快速生成技术，特别是模仿个性化与表达力的技术。随着多媒体技术的发展，这种技术在视频会议、音频可视化聊天机器人等领域有广泛的应用前景。该研究对于推动人脸生成技术的个性化和表达力提升具有重要意义。</p><p>(2)创新点、性能、工作量三维评价：</p><p>创新点：文章提出了MimicTalk方法，利用通用的非个性化NeRF模型来提高个性化TFG的效率与稳健性，这是该领域的一个新的尝试和探索。</p><p>性能：在个性化说话风格的三维人脸生成任务上，MimicTalk方法取得了显著成果，实验证明该方法在视频质量、效率和表现力方面均超越先前方法。</p><p>工作量：文章进行了大量的实验和验证，证明了所提方法的有效性。此外，文章还进行了深入的理论分析和讨论，为未来的研究提供了有价值的参考。但是，文章没有提供充分的实现细节和代码实现，这可能会限制其他研究者对该方法的深入理解和应用。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-1d9c9ab3a27964701eea89009297aa5e.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-a38af84c9b86216fd7d6091bfab25aa8.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-fde6139c2cf1945a51e91fbc6e38eda5.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-10b8e84a4e8953fda082597a1647d0a8.jpg" align="middle"></details><h2 id="Towards-a-GENEA-Leaderboard-—-an-Extended-Living-Benchmark-for-Evaluating-and-Advancing-Conversational-Motion-Synthesis"><a href="#Towards-a-GENEA-Leaderboard-—-an-Extended-Living-Benchmark-for-Evaluating-and-Advancing-Conversational-Motion-Synthesis" class="headerlink" title="Towards a GENEA Leaderboard — an Extended, Living Benchmark for   Evaluating and Advancing Conversational Motion Synthesis"></a>Towards a GENEA Leaderboard — an Extended, Living Benchmark for Evaluating and Advancing Conversational Motion Synthesis</h2><p><strong>Authors:Rajmund Nagy, Hendric Voss, Youngwoo Yoon, Taras Kucherenko, Teodor Nikolov, Thanh Hoang-Minh, Rachel McDonnell, Stefan Kopp, Michael Neff, Gustav Eje Henter</strong></p><p>Current evaluation practices in speech-driven gesture generation lack standardisation and focus on aspects that are easy to measure over aspects that actually matter. This leads to a situation where it is impossible to know what is the state of the art, or to know which method works better for which purpose when comparing two publications. In this position paper, we review and give details on issues with existing gesture-generation evaluation, and present a novel proposal for remedying them. Specifically, we announce an upcoming living leaderboard to benchmark progress in conversational motion synthesis. Unlike earlier gesture-generation challenges, the leaderboard will be updated with large-scale user studies of new gesture-generation systems multiple times per year, and systems on the leaderboard can be submitted to any publication venue that their authors prefer. By evolving the leaderboard evaluation data and tasks over time, the effort can keep driving progress towards the most important end goals identified by the community. We actively seek community involvement across the entire evaluation pipeline: from data and tasks for the evaluation, via tooling, to the systems evaluated. In other words, our proposal will not only make it easier for researchers to perform good evaluations, but their collective input and contributions will also help drive the future of gesture-generation research.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.06327v1">PDF</a> 15 pages, 2 figures, project page: <a target="_blank" rel="noopener" href="https://genea-workshop.github.io/leaderboard/">https://genea-workshop.github.io/leaderboard/</a></p><p><strong>Summary</strong><br>语音驱动手势生成评估缺乏标准化，本文提出新方法构建动态排行榜，推动研究进展。</p><p><strong>Key Takeaways</strong></p><ol><li>现有评估缺乏标准化，影响成果比较。</li><li>提出建立动态排行榜，定期更新。</li><li>排行榜结合大规模用户研究，提升评价质量。</li><li>排行榜开放，支持任意出版物提交。</li><li>逐步演进评价数据与任务，追求重要目标。</li><li>鼓励社区参与整个评估流程。</li><li>旨在提升评估质量，推动手势生成研究发展。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: 迈向GENEA排行榜——扩展型动态基准测试用于评估与推动对话动作合成的发展</p></li><li><p>Authors: Rajmund Nagy, Hendric Voss, Youngwoo Yoon, Taras Kucharenko, Teodor Nikolov, Thanh Hoang-Minh, Rachel McDonnell, Stefan Kopp, Michael Neff, Gustav Eje Henter等人</p></li><li><p>Affiliation:</p><ul><li>Rajmund Nagy, Gustav Eje Henter：KTH皇家理工学院</li><li>Hendric Voss, Stefan Kopp：比勒费尔德大学</li><li>Youngwoo Yoon：ETRI（电子和电信研究协会）</li><li>Taras Kucharenko：SEED - 电子艺术</li><li>Motorica AB公司的其他作者等。</li></ul></li><li><p>Keywords: 姿态生成评估、基准测试、对话动作合成、社区驱动解决方案等。</p></li><li><p>Urls: <a href="链接地址">论文链接</a>，Github代码链接（如果有的话，填写相应链接；如果没有，填写”None”）</p></li><li><p>Summary:</p><ul><li>(1)研究背景：当前姿态生成评估缺乏标准化，现有的评估方法过于关注易于度量的方面而忽略了实际重要的方面。这使得比较不同论文的方法变得困难，无法确定哪些方法对哪些特定任务效果最好。文章提出了一种解决这些问题的方法。</li><li>(2)过去的方法及问题：现有的姿态生成评估存在诸多不足，如缺乏统一的基准测试集、评估任务不连贯、用户研究标准化程度低等。这些问题限制了领域的发展，阻碍了新技术的推广和应用。本文提出的动机是解决这些问题，提供一个更完善、更标准的评估方法。</li><li>(3)研究方法：文章提出了一种新的社区驱动的整体解决方案来应对姿态生成模型评估中的主要挑战。该方案包括建立一个动态的、不断更新的GENEA排行榜，以标准的方式进行姿态生成的评估。此外，还强调了社区参与的重要性，从数据、任务、工具到评估系统的参与都被鼓励。文章还介绍了新的可视化工具的使用，以便更好地展示和分析姿态生成模型的结果。最后通过不断调整和更新评价数据和任务以适应社区识别的最重要目标来推动领域发展。</li><li>(4)任务与性能：本文提出的方案旨在通过建立一个动态的、不断更新的基准测试排行榜来推动对话动作合成领域的发展。该方案将采用大规模用户研究来评估新的姿态生成系统，并将系统提交到任何作者喜欢的出版渠道。通过不断进化的评价数据和任务，该方案将能够推动社区识别的重要目标的实现。本文的性能评价将通过未来发布在GENEA排行榜上的系统和相关研究来验证和支持其目标实现。</li></ul></li><li>Conclusion:</li></ol><p>(1) 这项工作的意义在于提出了一种解决姿态生成评估中存在问题的方法，通过建立动态的、不断更新的基准测试排行榜来推动对话动作合成领域的发展。这项工作的重要性在于为姿态生成评估提供了一个更完善、更标准的评估方法，有助于比较不同论文的方法，确定哪些方法对哪些特定任务效果最好，促进了新技术的推广和应用。</p><p>(2) 创新点：本文提出了一种新的社区驱动的整体解决方案来应对姿态生成模型评估中的主要挑战，建立了动态的、不断更新的GENEA排行榜，以标准的方式进行姿态生成的评估，强调了社区参与的重要性。<br>性能：文章所提方案旨在通过大规模用户研究来评估新的姿态生成系统，并通过不断进化的评价数据和任务来推动社区识别的重要目标的实现。<br>工作量：文章介绍了新的可视化工具的使用，以便更好地展示和分析姿态生成模型的结果，未来将通过发布在GENEA排行榜上的系统和相关研究来验证和支持其目标实现。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-242295a68599bb2566e8f606631fd0de.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-fc4b4785bdfe4b7afeb4b2b909af503a.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-1ad4de547fac69da6c8219d6807b3742.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-0d3eef1ef1c08767c7cd525acec5faf4.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-f8156047660ccdd1b9e929e2661294eb.jpg" align="middle"></details><h2 id="Incorporating-Talker-Identity-Aids-With-Improving-Speech-Recognition-in-Adversarial-Environments"><a href="#Incorporating-Talker-Identity-Aids-With-Improving-Speech-Recognition-in-Adversarial-Environments" class="headerlink" title="Incorporating Talker Identity Aids With Improving Speech Recognition in   Adversarial Environments"></a>Incorporating Talker Identity Aids With Improving Speech Recognition in Adversarial Environments</h2><p><strong>Authors:Sagarika Alavilli, Annesya Banerjee, Gasser Elbanna, Annika Magaro</strong></p><p>Current state-of-the-art speech recognition models are trained to map acoustic signals into sub-lexical units. While these models demonstrate superior performance, they remain vulnerable to out-of-distribution conditions such as background noise and speech augmentations. In this work, we hypothesize that incorporating speaker representations during speech recognition can enhance model robustness to noise. We developed a transformer-based model that jointly performs speech recognition and speaker identification. Our model utilizes speech embeddings from Whisper and speaker embeddings from ECAPA-TDNN, which are processed jointly to perform both tasks. We show that the joint model performs comparably to Whisper under clean conditions. Notably, the joint model outperforms Whisper in high-noise environments, such as with 8-speaker babble background noise. Furthermore, our joint model excels in handling highly augmented speech, including sine-wave and noise-vocoded speech. Overall, these results suggest that integrating voice representations with speech recognition can lead to more robust models under adversarial conditions.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.05423v1">PDF</a> Submitted to ICASSP 2025</p><p><strong>Summary</strong><br>本研究提出一种结合说话人表示的语音识别模型，显著提高了在噪声和语音增强条件下的鲁棒性。</p><p><strong>Key Takeaways</strong></p><ol><li>语音识别模型对分布外条件（如背景噪音）敏感。</li><li>提出结合说话人表示来增强模型鲁棒性的假设。</li><li>开发了一种基于transformer的模型，联合进行语音识别和说话人识别。</li><li>使用Whisper的语音嵌入和ECAPA-TDNN的说话人嵌入。</li><li>联合模型在干净条件下与Whisper表现相当。</li><li>联合模型在噪声环境中（如8个说话人的嘈杂背景噪声）优于Whisper。</li><li>联合模型在处理高度增强的语音（如正弦波和噪声编码语音）方面表现出色。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: 基于联合训练的说话人身份识别与语音识别模型改进研究</p></li><li><p>Authors: Sagarika Alavilli, Annesya Banerjee, Gasser Elbanna, Annika Magaro</p></li><li><p>Affiliation: Harvard University Speech and Hearing Bioscience and Technology</p></li><li><p>Keywords: voice identification, automatic speech recognition, joint training</p></li><li><p>Urls: xxx (论文链接未提供)</p></li><li><p>Summary:</p><ul><li><p>(1)研究背景：本文主要研究在恶劣环境下，如何提高自动语音识别模型的鲁棒性。当前最先进的语音识别模型在背景噪声和语音增强等条件下性能下降。</p></li><li><p>(2)过去的方法及问题：传统的语音识别模型主要关注语言内容，忽略说话人特定的声学线索。尽管已有研究表明语音和说话人特性影响语音感知，但自动语音识别系统很少利用这些特性。因此，现有的模型在面对噪声或其他干扰时，性能可能会显著下降。</p></li><li><p>(3)研究方法：本文提出了一种基于联合训练的语音识别和说话人识别模型。该模型结合了Whisper的语音嵌入和ECAPA-TDNN的说话人嵌入，通过多任务学习的方式，共同处理语音和说话人识别任务。模型利用变压器结构处理嵌入信息，生成同时包含语音和说话人特征的表示。</p></li><li><p>(4)任务与性能：该模型在噪声环境和语音增强任务上进行了测试，如8人同时说话的嘈杂背景噪声。实验结果表明，联合模型在恶劣环境下的性能优于Whisper模型。此外，该模型在处理高度增强的语音，如正弦波和噪声编码的语音方面表现优异。总的来说，这些结果支持了通过整合语音和说话人特性来提高语音识别模型鲁棒性的观点。</p></li></ul></li></ol><p>以上内容仅供参考，具体信息建议查阅论文原文获取。</p><ol><li><p>方法论：</p><ul><li><p>(1) 研究背景：该研究针对恶劣环境下自动语音识别模型的鲁棒性问题展开。现有的最先进的语音识别模型在背景噪声和语音增强等条件下性能下降。</p></li><li><p>(2) 过去的方法及问题：传统的语音识别模型主要关注语言内容，忽略了说话人的特定声学线索。尽管已有研究表明语音和说话人特性影响语音感知，但自动语音识别系统很少利用这些特性，导致在面对噪声或其他干扰时性能下降。</p></li><li><p>(3) 方法论创新：本研究提出了一种基于联合训练的语音识别和说话人识别模型。该模型结合了Whisper的语音嵌入和ECAPA-TDNN的说话人嵌入，通过多任务学习的方式，共同处理语音和说话人识别任务。模型利用变压器结构处理嵌入信息，生成同时包含语音和说话人特征的表示。具体来说，研究使用了Whisper模型作为语音识别的基本架构，并在此基础上结合了ECAPA-TDNN模型的优点，用于提取说话人的特征。联合模型通过多层变压器结构对两种嵌入进行联合处理，以提高模型在恶劣环境下的鲁棒性。</p></li><li><p>(4) 实验验证：该模型在噪声环境和语音增强任务上进行了测试，实验结果表明，联合模型在恶劣环境下的性能优于Whisper模型。此外，该模型在处理高度增强的语音，如正弦波和噪声编码的语音方面表现优异。这些结果支持了通过整合语音和说话人特性来提高语音识别模型鲁棒性的观点。</p></li></ul></li><li>Conclusion:</li></ol><ul><li><p>(1)研究意义：该研究提高了在恶劣环境下自动语音识别模型的鲁棒性，对于提高语音识别系统的实际应用效果具有重要意义。通过整合语音和说话人特性，模型能够在噪声和其他干扰条件下更加准确地识别语音内容。此外，该研究也为开发更健壮的语音识别系统提供了新的可能性。</p></li><li><p>(2)创新点、性能和工作量评价：</p><ul><li>创新点：该研究结合说话人识别和语音识别模型进行联合训练，这是一种新的尝试。通过结合Whisper的语音嵌入和ECAPA-TDNN的说话人嵌入，模型能够同时处理语音和说话人识别任务，生成包含语音和说话人特征的表示。这种结合方式有助于提高模型在恶劣环境下的鲁棒性。</li><li>性能：实验结果表明，联合模型在恶劣环境下的性能优于Whisper模型，尤其是在处理高度增强的语音时表现优异。这些结果支持了通过整合语音和说话人特性来提高语音识别模型鲁棒性的观点。</li><li>工作量：研究工作量较大，需要进行模型设计、实验设计、实验实施和结果分析等多个环节的工作。此外，还需要进行文献调研和理论分析，以支撑研究工作的进行。</li></ul></li></ul><p>综上所述，该研究具有一定的实际意义和创新性，但仍需要在实践中进一步验证和完善模型的性能。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-fb6606ff4cad4d279a82ddc9895cfc84.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-c1f38e27dcc86a1162335b4f7330fa6c.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-dee646e7476834d2d7d2fab444f54180.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-58d53d35023f8a5cf4c8b871f5a36ef0.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-7437da15257e2e4e956b63c6789636aa.jpg" align="middle"></details></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io/2024/10/12/Paper/2024-10-12/Talking%20Head%20Generation/">https://kedreamix.github.io/2024/10/12/Paper/2024-10-12/Talking Head Generation/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Talking-Head-Generation/">Talking Head Generation</a></div><div class="post_share"><div class="social-share" data-image="https://pic1.zhimg.com/v2-bf351a38d373ad29c81b373fe10d2463.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/10/12/Paper/2024-10-12/3DGS/" title="3DGS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-138787abc2188d0e954c7516ebaebfd7.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">3DGS</div></div></a></div><div class="next-post pull-right"><a href="/2024/10/12/Paper/2024-10-12/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙/虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-cdfd5481a219d4091af6266d68d7674b.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">元宇宙/虚拟人</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/03/11/Note/BlendShape/" title="Blendshape学习笔记"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://p6-sign.toutiaoimg.com/pgc-image/2c8cbd123e00470e95500a8ae62da605~noop.image?_iz=58558&from=article.pc_detail&lk3s=953192f4&x-expires=1710668214&x-signature=UHPhjWP4v96kbtfJzF97Z%2Bp3klc%3D" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-11</div><div class="title">Blendshape学习笔记</div></div></a></div><div><a href="/2024/03/03/Paperscape/EMO/" title="EMO Emote Portrait Alive - 阿里HumanAIGC"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-6492e24fb03ffa98135dc584535ab7d9.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-03</div><div class="title">EMO Emote Portrait Alive - 阿里HumanAIGC</div></div></a></div><div><a href="/2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" title="超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-01</div><div class="title">超赞的数字人生成知识库 Awesome-Talking-Head-Synthesis</div></div></a></div><div><a href="/2024/03/15/Paperscape/Real3D-Portrait/" title="REAL3D-PORTRAIT ONE-SHOT REALISTIC 3D TALKING PORTRAIT SYNTHESIS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-68585b79de5f83b0dfa23304f41b9b98.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-15</div><div class="title">REAL3D-PORTRAIT ONE-SHOT REALISTIC 3D TALKING PORTRAIT SYNTHESIS</div></div></a></div><div><a href="/2024/03/05/Paperscape/VividTalk/" title="VividTalk One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-8521b04f82075cc27b5e95148dba9792.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-05</div><div class="title">VividTalk One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior</div></div></a></div><div><a href="/2024/03/07/Paperscape/SyncTalk/" title="SyncTalk The Devil is in the Synchronization for Talking Head Synthesis"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-a57e0937b2f452009023394a59529dfb.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-07</div><div class="title">SyncTalk The Devil is in the Synchronization for Talking Head Synthesis</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-10-12-%E6%9B%B4%E6%96%B0"><span class="toc-text">2024-10-12 更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#MMHead-Towards-Fine-grained-Multi-modal-3D-Facial-Animation"><span class="toc-text">MMHead: Towards Fine-grained Multi-modal 3D Facial Animation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hallo2-Long-Duration-and-High-Resolution-Audio-Driven-Portrait-Image-Animation"><span class="toc-text">Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#FaceVid-1K-A-Large-Scale-High-Quality-Multiracial-Human-Face-Video-Dataset"><span class="toc-text">FaceVid-1K: A Large-Scale High-Quality Multiracial Human Face Video Dataset</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MimicTalk-Mimicking-a-personalized-and-expressive-3D-talking-face-in-minutes"><span class="toc-text">MimicTalk: Mimicking a personalized and expressive 3D talking face in minutes</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93%EF%BC%9A"><span class="toc-text">总结：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Towards-a-GENEA-Leaderboard-%E2%80%94-an-Extended-Living-Benchmark-for-Evaluating-and-Advancing-Conversational-Motion-Synthesis"><span class="toc-text">Towards a GENEA Leaderboard — an Extended, Living Benchmark for Evaluating and Advancing Conversational Motion Synthesis</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Incorporating-Talker-Identity-Aids-With-Improving-Speech-Recognition-in-Adversarial-Environments"><span class="toc-text">Incorporating Talker Identity Aids With Improving Speech Recognition in Adversarial Environments</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image:url('https://pic1.zhimg.com/v2-bf351a38d373ad29c81b373fe10d2463.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js").then((()=>{pangu.autoSpacingPage()}))}function panguInit(){panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.1},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script></div><script id="canvas_nest" defer color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><script data-pjax>function butterfly_swiper_injector_config(){var a=document.getElementById("recent-posts");console.log("已挂载butterfly_swiper"),a.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/25/Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="2024/01/25/Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="2024/01/25/Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/17/Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023/12/17/Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="2023/12/17/Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script></body></html>