<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>元宇宙/虚拟人 | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="元宇宙&#x2F;虚拟人 方向最新论文已更新，请持续关注 Update in 2024-08-05  PAV Personalized Head Avatar from Unstructured Video Collection"><meta property="og:type" content="article"><meta property="og:title" content="元宇宙&#x2F;虚拟人"><meta property="og:url" content="https://kedreamix.github.io/2024/08/05/Paper/2024-08-05/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/index.html"><meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World"><meta property="og:description" content="元宇宙&#x2F;虚拟人 方向最新论文已更新，请持续关注 Update in 2024-08-05  PAV Personalized Head Avatar from Unstructured Video Collection"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://picx.zhimg.com/v2-0878e816c3e8a4ed941dd1289728929e.jpg"><meta property="article:published_time" content="2024-08-04T16:09:44.000Z"><meta property="article:modified_time" content="2024-08-13T16:38:26.321Z"><meta property="article:author" content="Kedreamix"><meta property="article:tag" content="元宇宙&#x2F;虚拟人"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://picx.zhimg.com/v2-0878e816c3e8a4ed941dd1289728929e.jpg"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/2024/08/05/Paper/2024-08-05/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-74LZ5BEQQ1")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!0,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"简"},noticeOutdate:{limitDay:500,position:"top",messagePrev:"It has been",messageNext:"days since the last update, the content of the article may be outdated."},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:200},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!0,post:!0},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!0,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"元宇宙/虚拟人",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-08-14 00:38:26"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach((e=>{n.setAttribute(e,t[e])})),document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)})),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=24?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 7.0.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml"></head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{const d=document.getElementById("loading-box"),e=document.body,o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l(),window.addEventListener("load",(()=>{o()}))})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">303</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">17</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url('https://picx.zhimg.com/v2-0878e816c3e8a4ed941dd1289728929e.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">元宇宙/虚拟人</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-08-04T16:09:44.000Z" title="发表于 2024-08-05 00:09:44">2024-08-05</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-08-13T16:38:26.321Z" title="更新于 2024-08-14 00:38:26">2024-08-14</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">5.3k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>18分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="元宇宙/虚拟人"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-08-05-更新"><a href="#2024-08-05-更新" class="headerlink" title="2024-08-05 更新"></a>2024-08-05 更新</h1><h2 id="PAV-Personalized-Head-Avatar-from-Unstructured-Video-Collection"><a href="#PAV-Personalized-Head-Avatar-from-Unstructured-Video-Collection" class="headerlink" title="PAV: Personalized Head Avatar from Unstructured Video Collection"></a>PAV: Personalized Head Avatar from Unstructured Video Collection</h2><p><strong>Authors:Akin Caliskan, Berkay Kicanaoglu, Hyeongwoo Kim</strong></p><p>We propose PAV, Personalized Head Avatar for the synthesis of human faces under arbitrary viewpoints and facial expressions. PAV introduces a method that learns a dynamic deformable neural radiance field (NeRF), in particular from a collection of monocular talking face videos of the same character under various appearance and shape changes. Unlike existing head NeRF methods that are limited to modeling such input videos on a per-appearance basis, our method allows for learning multi-appearance NeRFs, introducing appearance embedding for each input video via learnable latent neural features attached to the underlying geometry. Furthermore, the proposed appearance-conditioned density formulation facilitates the shape variation of the character, such as facial hair and soft tissues, in the radiance field prediction. To the best of our knowledge, our approach is the first dynamic deformable NeRF framework to model appearance and shape variations in a single unified network for multi-appearances of the same subject. We demonstrate experimentally that PAV outperforms the baseline method in terms of visual rendering quality in our quantitative and qualitative studies on various subjects.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.21047v1">PDF</a> Accepted to ECCV24. Project page: <a target="_blank" rel="noopener" href="https://akincaliskan3d.github.io/PAV">https://akincaliskan3d.github.io/PAV</a></p><p><strong>Summary</strong><br>提出了PAV，用于在任意视角和面部表情下合成人脸的个性化头像化方法。</p><p><strong>Key Takeaways</strong></p><ul><li>PAV引入了学习动态可变形神经辐射场（NeRF）的方法。</li><li>支持学习同一角色不同外观和形状变化下的多外观NeRF。</li><li>引入了外观嵌入以及可学习的潜在神经特征。</li><li>提出了外观条件密度表达式，促进了人物形状的变化。</li><li>首个动态可变形NeRF框架，将外观和形状变化统一建模。</li><li>在定量和定性研究中，PAV在视觉渲染质量上优于基线方法。</li><li>实验证明PAV适用于多个主题的实验。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>好的，我会按照您的要求来回答。以下是关于该论文的总结：</p><ol><li><p><strong>标题</strong>： PAV：个性化头部化身研究（Personalized Head Avatar）。<strong>中文标题：个性化头部化身研究</strong>。</p></li><li><p><strong>作者</strong>： 阿金·卡里斯坦（Akin Caliskan）、伯克·基卡诺格鲁（Berkay Kicanaoglu）、亨永·金姆（Hyeongwoo Kim）。其中前两位作者来自Flawless AI公司，第三位作者来自帝国理工学院。</p></li><li><p><strong>作者所属单位</strong>： 无具体中文翻译，直接为作者的所属单位或实验室名称。</p></li><li><p><strong>关键词</strong>： 动态可变形神经辐射场（NeRF）、个性化头部化身、任意视角面部合成、表情合成等。<strong>英文关键词：dynamic deformable neural radiance field (NeRF), personalized head avatar, arbitrary viewpoint facial synthesis, expression synthesis等</strong>。</p></li><li><p><strong>链接</strong>： GitHub代码链接未知。<strong>链接说明：链接到该论文的相关文档或者论文下载链接</strong>。或者直接填：”GitHub:暂无”。</p></li><li><p><strong>摘要</strong>：</p><ul><li><p>(1)<strong>研究背景</strong>： 随着数字内容创建和电影工业的发展，对个性化头部化身的需求增加。文章研究背景是创建易于生成和动画化的个性化头部化身，能在新的姿态和表情下呈现真实的面部模型。基于神经辐射场（NeRF）的方法已成为面部建模的新趋势。本文旨在改进现有方法，实现更真实的面部合成效果。</p></li><li><p>(2)<strong>过去的方法及其问题</strong>： 现有方法主要依赖3D可变形模型进行面部合成，但无法充分捕捉面部的细微变化和细节。基于NeRF的方法提供了三维面部建模的机会，但在处理多外观和形状变化时仍有局限性。缺乏一个统一的框架来处理同一主体的多种外观和形状变化。本文提出的方法旨在解决这些问题。</p></li><li><p>(3)<strong>研究方法</strong>： 本文提出PAV（Personalized Head Avatar）方法，采用动态可变形神经辐射场（NeRF）。从一系列单目对话视频中学习，处理各种外观和形状变化。引入外观嵌入和可学习的潜在神经特征，以处理多外观的NeRF学习。此外，采用基于外观的条件密度公式，便于预测辐射场中角色形状的变化，如面部毛发和软组织。实验证明，PAV在视觉渲染质量上优于基准方法。</p></li><li><p>(4)<strong>任务与性能</strong>： 本文方法在合成头部化身任务上表现优异，能够在任意视角和表情下合成真实感强的面部模型。通过定量和定性研究验证PAV的有效性，实验结果显示其在多种主体上的性能优于基准方法。性能支持其达成目标，即创建一个易于生成和动画化的个性化头部化身方法。</p></li></ul></li></ol><p>希望这个回答对您有所帮助！<br>好的，我将详细概述该文章的实验方法。下面是简要的方法描述：</p><p>Methods:</p><ul><li>(1) <strong>数据准备和采集</strong>：收集了多个主题的一系列单目对话视频，以捕捉他们的面部动作和表情变化。这些视频作为训练数据，用于学习动态可变形神经辐射场（NeRF）。</li><li>(2) <strong>基于NeRF的个性化头部化身构建</strong>：利用动态可变形NeRF模型，从收集的视频中学习面部的细微变化和细节。通过引入外观嵌入和可学习的潜在神经特征，处理同一主体的多种外观和形状变化。</li><li>(3) <strong>面部建模与渲染</strong>：基于学习的NeRF模型，进行面部建模并预测辐射场中角色形状的变化。这些变化包括面部毛发和软组织等。这种方法能够在任意视角和表情下合成真实感强的面部模型。</li><li>(4) <strong>性能评估与实验验证</strong>：通过定量和定性研究验证所提出方法的有效性。在多种主体上进行实验，并与基准方法进行比较，结果显示PAV方法在合成头部化身任务上表现优异。</li></ul><p>以上内容遵循了学术性的简洁风格，且没有重复之前的内容。希望这能满足您的需求！</p><ol><li>Conclusion:</li></ol><p>(1) 该工作的重要性在于它解决了个性化头部化身创建中的关键问题，如面部细微变化和细节的捕捉，以及同一主体多种外观和形状变化的处理。它为数字内容创建和电影工业提供了更真实、更易于生成和动画化的个性化头部化身方法。</p><p>(2) 创新点：该文章提出了基于动态可变形神经辐射场（NeRF）的个性化头部化身创建方法，通过引入外观嵌入和可学习的潜在神经特征，有效处理了同一主体的多种外观和形状变化。性能：实验证明，该文章方法在合成头部化身任务上表现优异，优于基准方法。工作量：文章进行了大量的实验和性能评估，证明了方法的有效性，并展示了广泛的应用前景。但是，该文章可能受限于特定数据集和实验设置，需要更多的实际场景测试来验证其泛化性能。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-e9585f148771bcf3e526634c4f3a4cc6.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-0a577abb0b02bbbb56a8bb0818dd62fc.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-1ad38d899e1cd47742c1e6fb0b4f2690.jpg" align="middle"></details><h2 id="XHand-Real-time-Expressive-Hand-Avatar"><a href="#XHand-Real-time-Expressive-Hand-Avatar" class="headerlink" title="XHand: Real-time Expressive Hand Avatar"></a>XHand: Real-time Expressive Hand Avatar</h2><p><strong>Authors:Qijun Gan, Zijie Zhou, Jianke Zhu</strong></p><p>Hand avatars play a pivotal role in a wide array of digital interfaces, enhancing user immersion and facilitating natural interaction within virtual environments. While previous studies have focused on photo-realistic hand rendering, little attention has been paid to reconstruct the hand geometry with fine details, which is essential to rendering quality. In the realms of extended reality and gaming, on-the-fly rendering becomes imperative. To this end, we introduce an expressive hand avatar, named XHand, that is designed to comprehensively generate hand shape, appearance, and deformations in real-time. To obtain fine-grained hand meshes, we make use of three feature embedding modules to predict hand deformation displacements, albedo, and linear blending skinning weights, respectively. To achieve photo-realistic hand rendering on fine-grained meshes, our method employs a mesh-based neural renderer by leveraging mesh topological consistency and latent codes from embedding modules. During training, a part-aware Laplace smoothing strategy is proposed by incorporating the distinct levels of regularization to effectively maintain the necessary details and eliminate the undesired artifacts. The experimental evaluations on InterHand2.6M and DeepHandMesh datasets demonstrate the efficacy of XHand, which is able to recover high-fidelity geometry and texture for hand animations across diverse poses in real-time. To reproduce our results, we will make the full implementation publicly available at <a target="_blank" rel="noopener" href="https://github.com/agnJason/XHand">https://github.com/agnJason/XHand</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.21002v1">PDF</a></p><p><strong>Summary</strong><br>手部头像在数字界面中起着关键作用，增强用户沉浸感并促进虚拟环境中的自然交互。</p><p><strong>Key Takeaways</strong></p><ul><li>手部头像在数字界面中的重要性，提升用户沉浸感和自然交互。</li><li>对手部几何重建细节的重要性，对渲染质量至关重要。</li><li>引入实时生成手部头像XHand，包括形状、外观和变形。</li><li>通过三个特征嵌入模块预测手部变形、颜色和蒙皮权重。</li><li>利用基于网格的神经渲染器实现细粒度网格上的逼真渲染。</li><li>提出部分感知拉普拉斯平滑策略以维持细节并消除不必要的伪影。</li><li>在InterHand2.6M和DeepHandMesh数据集上验证了XHand的高效性，实时生成高保真度手部动画。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>好的，我会按照您的要求进行总结。以下是摘要和总结：</p><p>标题：XHand：实时表情手势化身技术研究</p><p>作者：甘启军、周子杰、朱建科</p><p>所属机构：浙江大学计算机科学与技术学院</p><p>关键词：三维手重建；动画人物；MANO模型；实时渲染；神经网络渲染器</p><p>URLs：论文链接：暂未提供；Github代码链接：<a target="_blank" rel="noopener" href="https://github.com/agnJason/XHand（如不可用，请留空）">https://github.com/agnJason/XHand（如不可用，请留空）</a></p><p>摘要：</p><p>一、研究背景<br>本文研究了在虚拟现实、数字娱乐和人机交互等环境中，手势化身技术的关键问题和挑战。由于手的肌肉复杂性和个性化特点，获得精细的手部表示对于虚拟空间中的用户体验至关重要。现有的方法难以准确表示手部的精细几何结构，特别是在实时环境中。因此，本文旨在设计一种能够全面生成手部形状、外观和变形的实时表达手势化身。</p><p>二、过去的方法及其问题<br>先前的研究主要集中在基于模型的方法和基于模型自由的方法。基于模型的方法虽然能够高效地分析和操作人体和手的形状和姿势，但由于主要依赖网格表示，它们受限于固定的拓扑结构和有限的3D网格分辨率，难以准确表示手的精细细节。模型自由的方法通过各种技术解决了手部网格重建的问题，但它们在保持几何细节方面仍存在困难。此外，现有的方法在手部动画的实时渲染方面存在挑战，特别是在保持高质量渲染的同时实现实时性能。</p><p>三、研究方法<br>针对这些问题，本文提出了XHand，一种实时表情手势化身。XHand通过利用特征嵌入模块来预测手部变形位移、顶点反射率和线性混合皮肤（LBS）权重，从而获得精细的手部网格。为了实现照片级的手部渲染，本文采用了一种基于网格的神经网络渲染器，利用网格拓扑一致性和嵌入模块的潜在代码。在训练过程中，提出了一种部分感知的Laplace平滑策略，通过结合不同级别的正则化来有效保持必要的细节并消除不必要的伪影。</p><p>四、任务与性能<br>本文在InterHand2.6M和DeepHandMesh数据集上评估了XHand的性能。实验结果表明，XHand能够恢复高保真度的几何和纹理，为各种姿势下的手部动画提供实时渲染。与现有方法相比，XHand在保持实时性能的同时实现了更高的渲染质量。此外，XHand将公开完整的实现，以便其他研究人员能够建立在此基础上进一步研究和改进。</p><p>综上所述，本文提出的XHand方法在手部动画的实时渲染方面取得了显著的进展，为虚拟现实和人机交互等领域的进一步应用提供了有力的支持。</p><ol><li>方法论：</li></ol><ul><li><strong>(1)</strong> 研究背景分析：针对虚拟现实、数字娱乐和人机交互等领域中手势化身技术的关键问题和挑战进行研究。指出获得精细的手部表示对于虚拟空间中的用户体验的重要性。</li><li><strong>(2)</strong> 对先前方法的评估与问题分析：主要分析了基于模型的方法和模型自由的方法的优缺点。基于模型的方法虽然能够高效分析和操作人体和手的形状和姿势，但难以准确表示手的精细细节。模型自由的方法虽然解决了手部网格重建的问题，但在保持几何细节方面仍有困难。此外，现有方法在手部动画的实时渲染方面存在挑战。</li><li><strong>(3)</strong> 本文方法介绍：提出了XHand实时表情手势化身技术。通过特征嵌入模块预测手部变形位移、顶点反射率和线性混合皮肤（LBS）权重，获得精细的手部网格。采用基于网格的神经网络渲染器实现照片级的手部渲染。在训练过程中，采用部分感知的Laplace平滑策略，有效保持必要的细节并消除不必要的伪影。</li><li><strong>(4)</strong> 实验与性能评估：在InterHand2.6M和DeepHandMesh数据集上评估XHand的性能。实验结果表明，XHand能够恢复高保真度的几何和纹理，为各种姿势下的手部动画提供实时渲染。与现有方法相比，XHand在保持实时性能的同时实现了更高的渲染质量。</li></ul><p>结论：</p><p>（1）这项工作的重要性在于它提出了一种实时表情手势化身技术，该技术对于提升虚拟环境、数字娱乐和人机交互中的用户体验具有重要意义。通过精细的手部表示和高质量渲染，该技术能够提供更真实、更生动的手部动画，从而增强用户的沉浸感和交互体验。</p><p>（2）创新点、性能和工作量三个方面对本文章进行了总结：</p><ul><li>创新点：本文提出了XHand实时表情手势化身技术，通过特征嵌入模块预测手部变形位移、顶点反射率和线性混合皮肤（LBS）权重，获得精细的手部网格。采用基于网格的神经网络渲染器实现照片级的手部渲染。此外，本文还提出了一种部分感知的Laplace平滑策略，以在保持必要细节的同时消除不必要的伪影。</li><li>性能：本文在InterHand2.6M和DeepHandMesh数据集上评估了XHand的性能，实验结果表明XHand能够恢复高保真度的几何和纹理，为各种姿势下的手部动画提供实时渲染。与现有方法相比，XHand在保持实时性能的同时实现了更高的渲染质量。</li><li>工作量：文章详细地介绍了XHand的设计和实现过程，包括方法论的各个方面和实验评估。但是，文章未详细阐述所有具体的工作步骤和实施细节，如模型训练的具体参数、数据集的具体处理过程等，可能使读者对工作量的大小有一定程度的模糊感知。不过总体而言，文章的工作量大且具有一定的挑战性。</li></ul><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-f07ea6ef44995519f8475cb72916ab48.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-0878e816c3e8a4ed941dd1289728929e.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-142bf6511ed22960f02f1f9d3960775a.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-e17253b78d15d266add20083515f2c9e.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-aa9478aeb66af3ae6f23095111a604d4.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-bd1c1d350009adbeabf6b5384a94c7a7.jpg" align="middle"></details><h2 id="Bridging-the-Gap-Studio-like-Avatar-Creation-from-a-Monocular-Phone-Capture"><a href="#Bridging-the-Gap-Studio-like-Avatar-Creation-from-a-Monocular-Phone-Capture" class="headerlink" title="Bridging the Gap: Studio-like Avatar Creation from a Monocular Phone   Capture"></a>Bridging the Gap: Studio-like Avatar Creation from a Monocular Phone Capture</h2><p><strong>Authors:ShahRukh Athar, Shunsuke Saito, Zhengyu Yang, Stanislav Pidhorsky, Chen Cao</strong></p><p>Creating photorealistic avatars for individuals traditionally involves extensive capture sessions with complex and expensive studio devices like the LightStage system. While recent strides in neural representations have enabled the generation of photorealistic and animatable 3D avatars from quick phone scans, they have the capture-time lighting baked-in, lack facial details and have missing regions in areas such as the back of the ears. Thus, they lag in quality compared to studio-captured avatars. In this paper, we propose a method that bridges this gap by generating studio-like illuminated texture maps from short, monocular phone captures. We do this by parameterizing the phone texture maps using the $W^+$ space of a StyleGAN2, enabling near-perfect reconstruction. Then, we finetune a StyleGAN2 by sampling in the $W^+$ parameterized space using a very small set of studio-captured textures as an adversarial training signal. To further enhance the realism and accuracy of facial details, we super-resolve the output of the StyleGAN2 using carefully designed diffusion model that is guided by image gradients of the phone-captured texture map. Once trained, our method excels at producing studio-like facial texture maps from casual monocular smartphone videos. Demonstrating its capabilities, we showcase the generation of photorealistic, uniformly lit, complete avatars from monocular phone captures. The project page can be found at <a target="_blank" rel="noopener" href="http://shahrukhathar.github.io/2024/07/22/Bridging.html">http://shahrukhathar.github.io/2024/07/22/Bridging.html</a></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.19593v2">PDF</a> ECCV 2024</p><p><strong>Summary</strong><br>通过简短的手机扫描生成接近完美的面部纹理贴图，弥补了传统复杂捕捉设备所产生的质量差距。</p><p><strong>Key Takeaways</strong></p><ul><li>利用手机快速扫描生成的3D头像贴图存在光照捕捉和面部细节缺失问题。</li><li>提出一种通过StyleGAN2的参数化处理方法，从手机捕捉的贴图生成接近完美的面部纹理。</li><li>使用少量样本对StyleGAN2进行微调，进一步优化生成的面部贴图。</li><li>引入扩散模型对生成结果进行超分辨率处理，提高面部细节的真实性和准确性。</li><li>新方法能够从普通手机视频生成光照均匀、完整的逼真头像。</li><li>技术展示了从单眼手机捕捉到生成的照片级别面部纹理贴图的能力。</li><li>详细信息可查看项目页面：<a target="_blank" rel="noopener" href="http://shahrukhathar.github.io/2024/07/22/Bridging.html">http://shahrukhathar.github.io/2024/07/22/Bridging.html</a></li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>Please refer to relevant websites for more information, and feel free to ask me any other questions.</p><ol><li>方法论概述：</li></ol><p>本文主要提出了一个基于手机捕获的图像生成类似工作室质量的肖像纹理映射的方法。具体步骤包括：</p><pre><code>- (1) 收集并预处理手机捕获的中性面部图像，提取中性纹理Iphone。

- (2) 使用StyleGAN2模型进行纹理翻译，训练一个针对手机捕获纹理的StyleGAN2模型（Gphone）。此模型能将手机捕获的纹理转换为具有工作室照明和可能的缺失区域填充的纹理。

- (3) 对Gphone进行微调以生成具有工作室照明的低分辨率纹理映射I∗。通过优化W +空间中的向量来获得I∗，这个向量由手机捕获的纹理映射通过StyleGAN2模型参数化表示。同时利用感知损失、身份损失等保证身份和语义的保留。

- (4) 利用扩散模型fϕ在I∗的基础上生成具有真实面部细节的高分辨率中性纹理。此步骤采用扩散模型的逆向过程，通过最小化扩散模型与实际结果的差异进行训练，最终通过该模型在生成的低分辨率纹理上添加真实的面部细节。

- (5) 在面部细节生成过程中，利用手机捕获的纹理映射的图像梯度信息来优化生成结果，使得最终生成的面部细节更加准确和真实。
</code></pre><p>以上是本篇文章的主要方法论概述。</p><ol><li><p>Conclusion:</p><ul><li><p>(1): 这项工作的意义在于，它提出了一种基于手机捕获的图像生成类似工作室质量的肖像纹理映射的方法。这种方法极大地降低了专业肖像摄影的成本和时间，使得普通用户也能够获得高质量的肖像纹理映射。它为数字肖像艺术、虚拟角色创建、游戏角色设计等领域提供了一种新的解决方案。</p></li><li><p>(2): 创新点：本文的创新之处在于提出了一种针对手机捕获纹理的StyleGAN2模型（Gphone），能够将手机捕获的纹理转换为具有工作室照明的纹理，并且利用扩散模型在面部细节生成过程中进行优化，使得最终生成的面部细节更加准确和真实。性能：该方法的性能表现在实验数据上表现出色，能够生成高质量的肖像纹理映射。然而，对于复杂面部表情和光照条件，该方法可能存在一定的局限性。工作量：文章详细介绍了方法的步骤和实验过程，展示了作者们的大量工作和努力。但是，文章未对方法的计算复杂度和实际应用中的耗时进行详细分析。</p></li></ul></li></ol><p>以上是对该文章的总结性评论，希望对您有所帮助。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-5ea70f9c57edf0075e3fcb3477588bdf.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-e09f144ae1e8ba2068707121897e810f.jpg" align="middle"></details></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io/2024/08/05/Paper/2024-08-05/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">https://kedreamix.github.io/2024/08/05/Paper/2024-08-05/元宇宙_虚拟人/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">元宇宙/虚拟人</a></div><div class="post_share"><div class="social-share" data-image="https://picx.zhimg.com/v2-0878e816c3e8a4ed941dd1289728929e.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/08/05/Paper/2024-08-05/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-be6cee2659a9cbdfe445b49595ea42d3.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Diffusion Models</div></div></a></div><div class="next-post pull-right"><a href="/2024/07/26/Paper/2024-07-26/NeRF/" title="NeRF"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-604b7794ae588e63ad59270528dc7af9.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">NeRF</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/01/24/Paper/2024-01-24/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-0f13a2b60bef4c886a3317754c99b456.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-24</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/04/01/Paper/2024-04-01/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-4f2a829065c463be027e4b423c4e43c8.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-01</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/04/06/Paper/2024-04-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-4f97970c093585e18e2db42fb96a6b75.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-06</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/05/06/Paper/2024-05-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-65e21e3a0a320adc36f81e6bfc7c5739.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-06</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/05/02/Paper/2024-05-02/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-37516691b13dabbabb3b74ea46b402d8.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-02</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/05/22/Paper/2024-05-22/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-b4943be56cf94e02149b50eec541d27f.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-22</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-08-05-%E6%9B%B4%E6%96%B0"><span class="toc-text">2024-08-05 更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#PAV-Personalized-Head-Avatar-from-Unstructured-Video-Collection"><span class="toc-text">PAV: Personalized Head Avatar from Unstructured Video Collection</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#XHand-Real-time-Expressive-Hand-Avatar"><span class="toc-text">XHand: Real-time Expressive Hand Avatar</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Bridging-the-Gap-Studio-like-Avatar-Creation-from-a-Monocular-Phone-Capture"><span class="toc-text">Bridging the Gap: Studio-like Avatar Creation from a Monocular Phone Capture</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image:url('https://picx.zhimg.com/v2-0878e816c3e8a4ed941dd1289728929e.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js").then((()=>{pangu.autoSpacingPage()}))}function panguInit(){panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.1},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script></div><script id="canvas_nest" defer color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><script data-pjax>function butterfly_swiper_injector_config(){var a=document.getElementById("recent-posts");console.log("已挂载butterfly_swiper"),a.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/25/Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="2024/01/25/Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="2024/01/25/Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/17/Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023/12/17/Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="2023/12/17/Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script></body></html>