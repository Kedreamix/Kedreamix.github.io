<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>NeRF | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="NeRF 方向最新论文已更新，请持续关注 Update in 2024-08-05  PAV Personalized Head Avatar from Unstructured Video Collection"><meta property="og:type" content="article"><meta property="og:title" content="NeRF"><meta property="og:url" content="https://kedreamix.github.io/2024/08/05/Paper/2024-08-05/NeRF/index.html"><meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World"><meta property="og:description" content="NeRF 方向最新论文已更新，请持续关注 Update in 2024-08-05  PAV Personalized Head Avatar from Unstructured Video Collection"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://picx.zhimg.com/v2-ae714318b916d0ec3524e5a68c4b2daf.jpg"><meta property="article:published_time" content="2024-08-04T16:23:58.000Z"><meta property="article:modified_time" content="2024-08-13T16:38:46.267Z"><meta property="article:author" content="Kedreamix"><meta property="article:tag" content="NeRF"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://picx.zhimg.com/v2-ae714318b916d0ec3524e5a68c4b2daf.jpg"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/2024/08/05/Paper/2024-08-05/NeRF/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-74LZ5BEQQ1")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!0,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"简"},noticeOutdate:{limitDay:500,position:"top",messagePrev:"It has been",messageNext:"days since the last update, the content of the article may be outdated."},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:200},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!0,post:!0},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!0,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"NeRF",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-08-14 00:38:46"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach((e=>{n.setAttribute(e,t[e])})),document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)})),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=24?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 7.0.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml"></head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{const d=document.getElementById("loading-box"),e=document.body,o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l(),window.addEventListener("load",(()=>{o()}))})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">254</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url('https://picx.zhimg.com/v2-ae714318b916d0ec3524e5a68c4b2daf.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">NeRF</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-08-04T16:23:58.000Z" title="发表于 2024-08-05 00:23:58">2024-08-05</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-08-13T16:38:46.267Z" title="更新于 2024-08-14 00:38:46">2024-08-14</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">8.6k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>29分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="NeRF"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-08-05-更新"><a href="#2024-08-05-更新" class="headerlink" title="2024-08-05 更新"></a>2024-08-05 更新</h1><h2 id="PAV-Personalized-Head-Avatar-from-Unstructured-Video-Collection"><a href="#PAV-Personalized-Head-Avatar-from-Unstructured-Video-Collection" class="headerlink" title="PAV: Personalized Head Avatar from Unstructured Video Collection"></a>PAV: Personalized Head Avatar from Unstructured Video Collection</h2><p><strong>Authors:Akin Caliskan, Berkay Kicanaoglu, Hyeongwoo Kim</strong></p><p>We propose PAV, Personalized Head Avatar for the synthesis of human faces under arbitrary viewpoints and facial expressions. PAV introduces a method that learns a dynamic deformable neural radiance field (NeRF), in particular from a collection of monocular talking face videos of the same character under various appearance and shape changes. Unlike existing head NeRF methods that are limited to modeling such input videos on a per-appearance basis, our method allows for learning multi-appearance NeRFs, introducing appearance embedding for each input video via learnable latent neural features attached to the underlying geometry. Furthermore, the proposed appearance-conditioned density formulation facilitates the shape variation of the character, such as facial hair and soft tissues, in the radiance field prediction. To the best of our knowledge, our approach is the first dynamic deformable NeRF framework to model appearance and shape variations in a single unified network for multi-appearances of the same subject. We demonstrate experimentally that PAV outperforms the baseline method in terms of visual rendering quality in our quantitative and qualitative studies on various subjects.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.21047v1">PDF</a> Accepted to ECCV24. Project page: <a target="_blank" rel="noopener" href="https://akincaliskan3d.github.io/PAV">https://akincaliskan3d.github.io/PAV</a></p><p><strong>Summary</strong><br>提出了PAV，一种用于合成人脸的个性化头像的方法，能够在任意视角和表情下合成人类面部。</p><p><strong>Key Takeaways</strong></p><ul><li>PAV引入了一种学习动态可变形神经辐射场（NeRF）的方法，特别是从同一角色的多种外观和形状变化的单目对话面部视频集合中学习。</li><li>与现有的头部NeRF方法不同，后者仅限于对每个外观基础上的输入视频进行建模，我们的方法通过引入每个输入视频的外观嵌入，通过可学习的潜在神经特征附加到底层几何形状来学习多外观NeRF。</li><li>提出的外观条件密度公式促进了人物形状的变化，如面部毛发和软组织，在辐射场预测中。</li><li>我们的方法是首个动态可变形NeRF框架，能够在单个统一网络中建模同一主题的多外观的外观和形状变化。</li><li>在各种主题的定量和定性研究中，实验证明PAV在视觉渲染质量方面优于基准方法。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>好的，我会按照您的要求来总结这篇论文。以下是按照您的格式要求的回答：</p><ol><li><p>标题：个性化头部阿凡达（PAV：Personalized Head Avatar）</p></li><li><p>作者：Akin Caliskan、Berkay Kicanaoglu和Hyeongwoo Kim。</p></li><li><p>作者隶属机构：第一作者Akin Caliskan隶属Flawless AI，第二作者Berkay Kicanaoglu和第三作者Hyeongwoo Kim分别隶属Imperial College London。</p></li><li><p>关键词：个性化头部阿凡达、动态变形神经辐射场（NeRF）、人脸合成、任意视角、面部表情、数字人类阿凡达。</p></li><li><p>链接：论文链接（待补充），GitHub代码链接（如有可用，填入Github；若无，填入None）。</p></li><li><p>摘要：</p><p>(1) 研究背景：本文的研究背景是关于个性化头部阿凡达（Personalized Head Avatar）的合成，特别是在任意视角和面部表情下的人脸合成。随着数字内容创建和电影工业的发展，对个性化人类阿凡达的需求也在增加。为了满足这一需求，研究者们提出了一系列方法来创建和动画化个性化头部阿凡达。</p><p>(2) 过去的方法与问题：现有的头部NeRF方法主要局限于对同一外观的输入视频进行建模。然而，它们无法处理多外观的NeRFs，也无法学习每个输入视频的外观嵌入。此外，它们没有考虑到面部毛发和软组织等形状变化在辐射场预测中的重要性。因此，开发一种能够处理多外观、学习每个视频的外观嵌入并考虑形状变化的动态可变形NeRF框架是非常必要的。</p><p>(3) 研究方法：本文提出了一种名为PAV（Personalized Head Avatar）的方法，用于合成任意视角和面部表情下的人脸。PAV引入了一种学习动态可变形神经辐射场（NeRF）的方法，特别是从同一角色的单目谈话视频集中学习。PAV允许学习多外观NeRF，通过引入与底层几何相关联的可学习潜在神经特征来为每个输入视频提供外观嵌入。此外，提出的外观条件密度公式有助于预测辐射场中的形状变化，如面部毛发和软组织。据我们所知，PAV是第一个在单一统一网络中建模多外观相同主体的外观和形状变化的动态可变形NeRF框架。</p><p>(4) 任务与性能：本文的实验表明，PAV在视觉渲染质量方面优于基准方法。在多种主题上的定量和定性研究表明，PAV可以有效地合成任意视角和面部表情下的人脸。PAV的性能支持其目标，为个性化头部阿凡达的开发提供了一种有效且实用的方法。</p></li></ol><p>希望这个总结符合您的要求！<br>好的，我会按照您的要求来详细阐述这篇论文的方法论。以下是按照您的格式要求的回答：</p><ol><li>方法论：</li></ol><p>（1）研究问题定义：本研究旨在解决个性化头部阿凡达（Personalized Head Avatar）的合成问题，特别是在任意视角和面部表情下的人脸合成。针对现有方法的局限性，提出了一种名为PAV（Personalized Head Avatar）的方法。</p><p>（2）数据集构建：使用同一角色的单目谈话视频集进行学习。这些数据集包含了不同视角和面部表情的丰富变化，为动态可变形神经辐射场（NeRF）的学习提供了充足的素材。</p><p>（3）方法框架：PAV方法引入了动态可变形神经辐射场（NeRF）的学习。该方法结合了深度学习技术和计算机图形学的知识，通过建立动态可变形NeRF模型来捕捉人脸的形状和纹理变化。具体来说，它允许学习多外观NeRF，为每个输入视频提供外观嵌入，并引入与底层几何相关联的可学习潜在神经特征。此外，提出的外观条件密度公式有助于预测辐射场中的形状变化。</p><p>（4）实验设计：为了验证PAV方法的性能，本研究进行了大量的实验。实验结果表明，PAV在视觉渲染质量方面优于基准方法。在多种主题上的定量和定性研究表明，PAV可以有效地合成任意视角和面部表情下的人脸。此外，本研究还通过用户研究验证了PAV方法的实用性和用户满意度。</p><p>（5）结果评估：本研究通过定量和定性评估方法，对PAV方法的性能进行了全面的评估。定量评估使用了常用的评估指标，如PSNR（峰值信噪比）、SSIM（结构相似性度量）等。定性评估则通过视觉检查结果和对比实验来验证PAV方法的优势。同时，本研究还通过用户研究来评估用户对PAV方法生成的头部阿凡达的满意度和实用性评价。</p><p>好的，以下是按照您的要求进行的回答：</p><ol><li>Conclusion:</li></ol><p>（1）这篇论文的重要性在于它解决了个性化头部阿凡达合成的问题，特别是在任意视角和面部表情下的人脸合成。随着数字内容创建和电影工业的发展，对个性化头部阿凡达的需求日益增加，这篇论文提出了一种有效的方法来满足这一需求。</p><p>（2）创新点：该论文提出了一种名为PAV（Personalized Head Avatar）的方法，用于合成个性化头部阿凡达。PAV方法引入了动态可变形神经辐射场（NeRF）的学习，能够处理多外观、学习每个输入视频的外观嵌入，并考虑形状变化。据作者所知，PAV是第一个在单一统一网络中建模多外观相同主体的外观和形状变化的动态可变形NeRF框架。</p><p>性能：实验结果表明，PAV方法在视觉渲染质量方面优于基准方法，能够有效地合成任意视角和面部表情下的人脸。此外，通过用户研究验证了PAV方法的实用性和用户满意度。</p><p>工作量：论文作者进行了大量的实验来验证PAV方法的性能，包括数据集构建、方法框架设计、实验设计和结果评估等。论文还详细阐述了方法论和实验过程，展示了作者在该领域深入的研究和实验工作。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-e9585f148771bcf3e526634c4f3a4cc6.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-0a577abb0b02bbbb56a8bb0818dd62fc.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-1ad38d899e1cd47742c1e6fb0b4f2690.jpg" align="middle"></details><h2 id="Garment-Animation-NeRF-with-Color-Editing"><a href="#Garment-Animation-NeRF-with-Color-Editing" class="headerlink" title="Garment Animation NeRF with Color Editing"></a>Garment Animation NeRF with Color Editing</h2><p><strong>Authors:Renke Wang, Meng Zhang, Jun Li, Jian Yan</strong></p><p>Generating high-fidelity garment animations through traditional workflows, from modeling to rendering, is both tedious and expensive. These workflows often require repetitive steps in response to updates in character motion, rendering viewpoint changes, or appearance edits. Although recent neural rendering offers an efficient solution for computationally intensive processes, it struggles with rendering complex garment animations containing fine wrinkle details and realistic garment-and-body occlusions, while maintaining structural consistency across frames and dense view rendering. In this paper, we propose a novel approach to directly synthesize garment animations from body motion sequences without the need for an explicit garment proxy. Our approach infers garment dynamic features from body motion, providing a preliminary overview of garment structure. Simultaneously, we capture detailed features from synthesized reference images of the garment’s front and back, generated by a pre-trained image model. These features are then used to construct a neural radiance field that renders the garment animation video. Additionally, our technique enables garment recoloring by decomposing its visual elements. We demonstrate the generalizability of our method across unseen body motions and camera views, ensuring detailed structural consistency. Furthermore, we showcase its applicability to color editing on both real and synthetic garment data. Compared to existing neural rendering techniques, our method exhibits qualitative and quantitative improvements in garment dynamics and wrinkle detail modeling. Code is available at \url{<a target="_blank" rel="noopener" href="https://github.com/wrk226/GarmentAnimationNeRF}">https://github.com/wrk226/GarmentAnimationNeRF}</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.19774v1">PDF</a></p><p><strong>Summary</strong><br>通过直接从身体运动序列中合成服装动画，本文提出了一种新方法，无需显式服装代理，能够生成高保真度的服装动画。</p><p><strong>Key Takeaways</strong></p><ul><li>传统的建模到渲染的工作流程在生成高保真度服装动画时既繁琐又昂贵。</li><li>最近的神经渲染技术虽然在计算密集型处理上有效，但在处理复杂的服装动画（如细微皱褶和服装与身体的真实遮挡）时存在挑战。</li><li>文章提出的方法通过身体运动推断服装动态特征，并结合预训练图像模型生成的服装图像特征，构建神经辐射场以渲染服装动画视频。</li><li>技术支持服装的重新着色，通过分解其视觉元素实现。</li><li>方法展示了在未见过的身体运动和摄像机视角下的普适性，保证了结构一致性。</li><li>在真实和合成服装数据上展示了颜色编辑的适用性。</li><li>与现有的神经渲染技术相比，该方法在模拟服装动态和皱褶细节建模方面展现出定性和定量上的改进。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>Please refer to relevant websites for more information, and feel free to ask me any other questions.</p><p>好的，我会根据您给出的格式进行文章总结。由于无法看到实际的文章内容和具体的</p><summary>部分，我只能根据您给出的题目和问题需求来进行概括和解答。以下内容是根据您的要求进行创作的：<p></p><p><strong>正文略</strong>（这里应该是该篇文章或论文的内容正文部分）</p><h3 id="8-Conclusion"><a href="#8-Conclusion" class="headerlink" title="8. Conclusion:"></a>8. Conclusion:</h3><h4 id="（对于第一小问的答案）："><a href="#（对于第一小问的答案）：" class="headerlink" title="（对于第一小问的答案）："></a>（对于第一小问的答案）：</h4><p>这一作品的意义在于……（此处应详细阐述作品的意义，如其在文学领域的重要性、对读者的启示等）。它……（简要总结作品的核心价值或影响）。</p><h4 id="（对于第二小问的答案，分别从创新点、性能和工作负载三个方面对文章进行强弱概括）："><a href="#（对于第二小问的答案，分别从创新点、性能和工作负载三个方面对文章进行强弱概括）：" class="headerlink" title="（对于第二小问的答案，分别从创新点、性能和工作负载三个方面对文章进行强弱概括）："></a>（对于第二小问的答案，分别从创新点、性能和工作负载三个方面对文章进行强弱概括）：</h4><ul><li><strong>创新点</strong>: 本文的创新之处在于……（简要描述文章在某一领域的独特视角、研究方法或观点）。然而，也存在一些创新点不够突出或缺乏深度的问题，需要进一步深入探讨。</li><li><strong>性能</strong>: 文章在性能方面的优点包括……（列举文章在论证、分析、论述等方面的优点）。但也可能存在一些不足之处，如对某些细节的分析不够深入等。</li><li><strong>工作量</strong>: 文章的工作量体现在……（描述文章在研究准备、数据收集、实验设计等方面的工作量投入）。但也可能存在工作量分配不均或者在某些环节工作深度不足的情况。总体来说，该文章具有一定的价值但也存在可提升的空间。<br>（注：具体内容需要根据文章的实际内容来填充。）</li></ul><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-814b1601a96b212bfda73ac99c9e9921.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-050a6e6d8bb7316a013745be6c5d5145.jpg" align="middle"></details><h2 id="FINER-Building-a-Family-of-Variable-periodic-Functions-for-Activating-Implicit-Neural-Representation"><a href="#FINER-Building-a-Family-of-Variable-periodic-Functions-for-Activating-Implicit-Neural-Representation" class="headerlink" title="FINER++: Building a Family of Variable-periodic Functions for Activating   Implicit Neural Representation"></a>FINER++: Building a Family of Variable-periodic Functions for Activating Implicit Neural Representation</h2><p><strong>Authors:Hao Zhu, Zhen Liu, Qi Zhang, Jingde Fu, Weibing Deng, Zhan Ma, Yanwen Guo, Xun Cao</strong></p><p>Implicit Neural Representation (INR), which utilizes a neural network to map coordinate inputs to corresponding attributes, is causing a revolution in the field of signal processing. However, current INR techniques suffer from the “frequency”-specified spectral bias and capacity-convergence gap, resulting in imperfect performance when representing complex signals with multiple “frequencies”. We have identified that both of these two characteristics could be handled by increasing the utilization of definition domain in current activation functions, for which we propose the FINER++ framework by extending existing periodic/non-periodic activation functions to variable-periodic ones. By initializing the bias of the neural network with different ranges, sub-functions with various frequencies in the variable-periodic function are selected for activation. Consequently, the supported frequency set can be flexibly tuned, leading to improved performance in signal representation. We demonstrate the generalization and capabilities of FINER++ with different activation function backbones (Sine, Gauss. and Wavelet) and various tasks (2D image fitting, 3D signed distance field representation, 5D neural radiance fields optimization and streamable INR transmission), and we show that it improves existing INRs. Project page: {<a target="_blank" rel="noopener" href="https://liuzhen0212.github.io/finerpp/}">https://liuzhen0212.github.io/finerpp/}</a></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.19434v1">PDF</a> Extension of previous CVPR paper “FINER: Flexible spectral-bias tuning in implicit neural representation by variable-periodic activation functions”. arXiv admin note: substantial text overlap with arXiv:2312.02434</p><p><strong>Summary</strong><br>隐式神经表示（INR）利用神经网络将坐标输入映射到对应属性，正在信号处理领域引发革命，通过扩展周期/非周期激活函数提出FINER++框架以处理频谱偏差和容量收敛差异，灵活调节支持的频率集合，显著提高信号表示性能。</p><p><strong>Key Takeaways</strong></p><ul><li>隐式神经表示（INR）通过神经网络将坐标映射到属性，革新了信号处理领域。</li><li>当前INR技术存在”频率”特定的频谱偏差和容量收敛差异，导致在表示多频率复杂信号时性能不佳。</li><li>FINER++框架通过扩展现有的周期/非周期激活函数，解决了上述问题。</li><li>利用不同激活函数骨干（正弦、高斯、小波）和各种任务（2D图像拟合、3D有符号距离场表示、5D神经辐射场优化和可流式INR传输）展示了FINER++的泛化能力和性能优势。</li><li>通过调整神经网络的偏置范围，选择变周期函数中具有不同频率的子函数进行激活。</li><li>FINER++使得支持的频率集合可以灵活调节，从而改善信号表示的性能。</li><li>项目页面：{<a target="_blank" rel="noopener" href="https://liuzhen0212.github.io/finerpp/}">https://liuzhen0212.github.io/finerpp/}</a></li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>好的，我会按照您的要求进行回答。</p><ol><li><p>标题：FINER++：构建可变周期函数家族以激活隐式神经表示</p></li><li><p>作者：Hao Zhu, Zhen Liu, Qi Zhang, Jingde Fu, Weibing Deng, Zhan Ma, Yanwen Guo, Xun Cao</p></li><li><p>隶属机构：第一作者Hao Zhu隶属南京大学电子科学与工程学院。</p></li><li><p>关键词：隐式神经表示，可变周期激活函数，谱偏置</p></li><li><p>网址：<a target="_blank" rel="noopener" href="https://liuzhen0212.github.io/finerpp/">https://liuzhen0212.github.io/finerpp/</a><br>Github代码链接：None</p></li><li><p>概述：</p><ul><li>(1)研究背景：本文的研究背景是关于隐式神经表示（INR）在信号处理领域的应用。隐式神经表示通过神经网络将坐标映射到相应的属性，正在引起革命。然而，当前的INR技术在处理具有多个“频率”的复杂信号时存在不完美的性能。</li><li>(2)过去的方法及问题：过去的方法主要集中在优化权重矩阵以更好地匹配频率候选，但存在谱偏置和容量收敛间隙等问题。这些问题与激活函数定义域利用不足有关。</li><li>(3)研究方法：本文提出了一个名为FINER++的通用框架，它通过扩展现有的周期性和非周期性激活函数到可变周期版本，解决了上述问题。通过控制输入值的范围来初始化神经网络的偏差，以选择可变周期函数中的子函数来激活。这样可以灵活调整支持的频率集，从而提高信号表示的性能。该框架使用不同的激活函数（如正弦、高斯和小波）和各种任务（如2D图像拟合、3D有符号距离场表示等）来验证其效果。</li><li>(4)任务与成果：本文提出的FINER++在多种任务上取得了良好的性能，包括图像拟合、距离场表示、神经辐射场优化和可流式传输的INR等。实验结果表明，FINER++提高了现有INRs的性能，证明了其有效性和灵活性。该方法的性能支持其目标，为解决现代信号处理中的逆问题提供了新的思路和方法。</li></ul></li></ol><ol><li>结论：</li></ol><p>(1)工作意义：本文提出了FINER++方法，通过构建可变周期激活函数家族，对隐式神经表示（INR）进行了扩展。这种方法解决了现有INR在处理具有多个“频率”的复杂信号时存在的问题，为提高信号表示的性能提供了新的思路和方法。这对于信号处理领域，尤其是需要处理复杂信号的领域，具有重要的理论和实践意义。</p><p>(2)创新点、性能、工作量评价：</p><p>创新点：本文提出的FINER++方法，通过扩展现有的周期性和非周期性激活函数到可变周期版本，解决了现有INR存在的问题。该方法在激活函数定义域利用不足、频谱偏置和容量收敛间隙等方面进行了改进，具有显著的创新性。</p><p>性能：在多种任务上，FINER++取得了良好的性能，包括图像拟合、距离场表示、神经辐射场优化和可流式传输的INR等。实验结果表明，FINER++提高了现有INRs的性能，证明了其有效性和灵活性。</p><p>工作量：本文不仅提出了FINER++方法，还进行了大量的实验验证，包括不同任务上的性能评估和对比分析。此外，作者对相关工作进行了全面的调研和分析，工作量较大。</p><p>总的来说，本文提出的FINER++方法在隐式神经表示领域具有重要的创新性和实用价值，为信号处理领域的发展做出了贡献。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-7b0f890bee55ee6805bb5ae1d934f58e.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-c34f21c0c89010a3f8416fcf29c3cd2a.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-ac61040a2a67d6c0550bbdeee90fca59.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-e6ba5023875d3a38d7992f3586913ab9.jpg" align="middle"></details><h2 id="IOVS4NeRF-Incremental-Optimal-View-Selection-for-Large-Scale-NeRFs"><a href="#IOVS4NeRF-Incremental-Optimal-View-Selection-for-Large-Scale-NeRFs" class="headerlink" title="IOVS4NeRF:Incremental Optimal View Selection for Large-Scale NeRFs"></a>IOVS4NeRF:Incremental Optimal View Selection for Large-Scale NeRFs</h2><p><strong>Authors:Jingpeng Xie, Shiyu Tan, Yuanlei Wang, Yizhen Lao</strong></p><p>Urban-level three-dimensional reconstruction for modern applications demands high rendering fidelity while minimizing computational costs. The advent of Neural Radiance Fields (NeRF) has enhanced 3D reconstruction, yet it exhibits artifacts under multiple viewpoints. In this paper, we propose a new NeRF framework method to address these issues. Our method uses image content and pose data to iteratively plan the next best view. A crucial aspect of this method involves uncertainty estimation, guiding the selection of views with maximum information gain from a candidate set. This iterative process enhances rendering quality over time. Simultaneously, we introduce the Vonoroi diagram and threshold sampling together with flight classifier to boost the efficiency, while keep the original NeRF network intact. It can serve as a plug-in tool to assist in better rendering, outperforming baselines and similar prior works.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.18611v1">PDF</a></p><p><strong>Summary</strong><br>NeRF提出了一种新的框架方法来解决多视角下的众多问题，通过图像内容和姿态数据迭代规划最佳视角，结合不确定性估计优化渲染质量。</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF方法通过使用图像内容和姿态数据来改进三维重建的质量。</li><li>新方法引入了不确定性估计，指导视角选择，提高信息获取效率。</li><li>引入Vonoroi图和阈值抽样，以及飞行分类器，增强了渲染效率。</li><li>该方法可作为插件工具，改善渲染效果，超越基线和类似先前工作。</li><li>保持原始NeRF网络的完整性。</li><li>迭代过程随时间增强了渲染质量。</li><li>提出的方法适用于现代应用中对高渲染保真度和低计算成本的需求。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>好的，我会按照您的要求进行回答。</p><ol><li><p>标题：基于增量最优视角选择的神经网络辐射场三维重建研究</p></li><li><p>作者：Jingpeng Xie（谢景鹏）, Shiyu Tan（谭世宇）, Yuanlei Wang（王远雷）, Yizhen Lao（劳亦真）</p></li><li><p>隶属机构：未知（请查看论文原文获取）</p></li><li><p>关键词：不确定性估计、无人机、神经网络辐射场、场景重建、视角选择</p></li><li><p>链接：论文链接未知，GitHub代码链接未知（如果可用，请填写GitHub链接）</p></li><li><p>摘要：</p><ul><li><p>(1) 研究背景：随着现代应用中对城市级三维重建需求的增长，对高质量渲染和高效计算的要求也越来越高。本文研究如何运用神经网络辐射场（NeRF）技术解决三维重建中的问题。</p></li><li><p>(2) 过去的方法及问题：过去的方法主要包括点云方法和NeRF方法。点云方法虽然可以构建三维模型，但表面连接性缺失，表面不够平滑。NeRF方法通过神经网络表示场景中的辐射场，可以实现高质量的图像重建和视角合成，但在多视角情况下会出现伪影，计算资源消耗大，对大规模场景的效率不高。</p></li><li><p>(3) 研究方法：针对上述问题，本文提出了一种新的NeRF框架方法，通过图像内容和姿态数据迭代规划下一个最佳视角。该方法涉及不确定性估计，通过选择信息增益最大的视角来提高渲染质量。同时，引入了Voronoi图、阈值采样和飞行分类器来提高效率，保持原NeRF网络不变。</p></li><li><p>(4) 任务与性能：本文的方法适用于大规模场景的三维重建任务，通过对比实验，本文方法在处理时间和重建效果上均优于其他方法。在ArtSci数据集上的实验结果表明，本文方法在保证较高性能的同时，也提高了计算效率。</p></li></ul></li></ol><p>希望这个回答符合您的要求。</p><ol><li>方法论概述：</li></ol><p>这篇文章提出了一个名为IOVS4NeRF的新方法来解决城市级三维重建中的问题。其方法论思想主要包含了以下几个步骤：</p><ul><li><p>(1) 研究背景和问题定义：随着现代应用中对城市级三维重建需求的增长，对高质量渲染和高效计算的要求也越来越高。传统的点云方法和NeRF方法存在一些问题，如表面连接性缺失、表面不够平滑、多视角下的伪影、计算资源消耗大以及对大规模场景的效率不高。针对这些问题，文章提出了一个新的NeRF框架方法。</p></li><li><p>(2) 方法设计：该方法通过图像内容和姿态数据的迭代规划来选择下一个最佳视角。它涉及不确定性估计，通过选择信息增益最大的视角来提高渲染质量。同时，引入了Voronoi图、阈值采样和飞行分类器来提高效率，保持原NeRF网络不变。</p></li><li><p>(3) 实验设计和评估：为了评估该方法的性能，文章进行了大量的实验，并将其与其他先进的NeRF基视角选择解决方案进行了比较。实验结果表明，IOVS4NeRF在不确定性预测和新型视图合成方面均优于其他方法。此外，还通过比较基线最优视角选择策略来进一步验证IOVS4NeRF的有效性。实验结果表明，IOVS4NeRF能够在保证较高性能的同时，提高计算效率。</p></li></ul><p>总的来说，该文章提出的IOVS4NeRF方法旨在通过更高效的视角选择和不确定性估计来解决城市级三维重建中的难题，从而提高渲染质量和计算效率。</p><p>好的，以下是对该文章的总结和评价：</p><ol><li>Conclusion:</li></ol><p>（1）工作意义：该文章提出了一种基于增量最优视角选择的神经网络辐射场三维重建方法，对于城市级三维重建任务具有重要意义，能够提高渲染质量和计算效率，满足现代应用中对高质量渲染和高效计算的需求。</p><p>（2）创新点、性能和工作量评价：</p><p>创新点：该文章引入了一种新的NeRF框架方法，通过图像内容和姿态数据的迭代规划选择下一个最佳视角，涉及不确定性估计，提高了渲染质量。同时，引入了Voronoi图、阈值采样和飞行分类器等技术，提高了计算效率。</p><p>性能：该文章的方法适用于大规模场景的三维重建任务，通过对比实验，证明该方法在处理时间和重建效果上均优于其他方法。在ArtSci数据集上的实验结果表明，该方法在保证较高性能的同时，也提高了计算效率。</p><p>工作量：该文章进行了大量的实验和比较，验证了所提出方法的有效性和性能。此外，文章还详细阐述了方法论的各个步骤和细节，说明作者进行了充分的工作。</p><p>总之，该文章所提出的基于增量最优视角选择的神经网络辐射场三维重建方法具有创新性和实用性，对于城市级三维重建任务具有重要的应用价值。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-5275e3f84e52c8ff4796ba9461971fce.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-f0ca7c22e785e136954cffa39dfeaa88.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-e2115c1e4656486864c645507a724445.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-8de57403ff71b26a84b1e96d069eac6a.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-de64750574b3c41fdac32fdd95e112fe.jpg" align="middle"></details><h2 id="Evaluating-geometric-accuracy-of-NeRF-reconstructions-compared-to-SLAM-method"><a href="#Evaluating-geometric-accuracy-of-NeRF-reconstructions-compared-to-SLAM-method" class="headerlink" title="Evaluating geometric accuracy of NeRF reconstructions compared to SLAM   method"></a>Evaluating geometric accuracy of NeRF reconstructions compared to SLAM method</h2><p><strong>Authors:Adam Korycki, Colleen Josephson, Steve McGuire</strong></p><p>As Neural Radiance Field (NeRF) implementations become faster, more efficient and accurate, their applicability to real world mapping tasks becomes more accessible. Traditionally, 3D mapping, or scene reconstruction, has relied on expensive LiDAR sensing. Photogrammetry can perform image-based 3D reconstruction but is computationally expensive and requires extremely dense image representation to recover complex geometry and photorealism. NeRFs perform 3D scene reconstruction by training a neural network on sparse image and pose data, achieving superior results to photogrammetry with less input data. This paper presents an evaluation of two NeRF scene reconstructions for the purpose of estimating the diameter of a vertical PVC cylinder. One of these are trained on commodity iPhone data and the other is trained on robot-sourced imagery and poses. This neural-geometry is compared to state-of-the-art lidar-inertial SLAM in terms of scene noise and metric-accuracy.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.11238v2">PDF</a></p><p><strong>Summary</strong><br>NeRF模型在现实世界的映射任务中表现出更高效、更精确的潜力。</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF实现变得更快、更高效、更精确，适用于实际的地图制作任务。</li><li>传统的3D地图制作依赖于昂贵的LiDAR传感器。</li><li>摄影测量可以进行基于图像的3D重建，但计算成本高，需要密集的图像表示来恢复复杂的几何和真实感。</li><li>NeRF通过在稀疏图像和姿态数据上训练神经网络来进行3D场景重建，比摄影测量表现更好，输入数据量更少。</li><li>论文评估了两种NeRF场景重建方法，用于估算垂直PVC圆柱体的直径。</li><li>其中一种使用普通iPhone数据进行训练，另一种使用机器人采集的图像和姿态数据。</li><li>将这种神经几何与最先进的激光惯性SLAM进行了比较，考虑了场景噪声和度量精度。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>好的，我会按照您的要求来总结这篇论文。以下是回答：</p><ol><li><p>标题：基于神经辐射场（NeRF）与SLAM方法的几何精度评估<br>中文翻译：NeRF与SLAM方法的几何精度评估</p></li><li><p>作者：Adam Korycki、Colleen Josephson、Steve McGuire</p></li><li><p>隶属机构：加州大学圣克鲁兹分校电气与计算机工程系<br>中文翻译：加州大学圣克鲁兹分校电气与计算机工程（系）</p></li><li><p>关键词：NeRF、SLAM、3D映射、场景重建、几何精度、神经辐射场实施、LiDAR感知</p></li><li><p>链接：具体论文链接需要您提供，GitHub代码链接（如可用）：GitHub:None</p></li><li><p>总结：</p><ul><li><p>(1)研究背景：<br>随着NeRF实施技术的快速发展，其在实际映射任务中的应用越来越广泛。传统的三维映射主要依赖于昂贵的LiDAR传感技术，而NeRF通过训练神经网络实现稀疏图像和姿态数据的3D场景重建，以较少的输入数据获得优于摄影测量的结果。本文旨在评估NeRF场景重建的几何精度。</p></li><li><p>(2)过去的方法及存在的问题：<br>传统的三维映射主要依赖于LiDAR传感技术，虽然其生成的结果精确，但成本高昂，且需要复杂的拼接过程。摄影测量虽可实现基于图像的三维重建，但计算量大，需要密集的图像表示来恢复复杂的几何和真实感。</p></li><li><p>(3)本文提出的研究方法：<br>本文评估了两种NeRF场景重建方法，一种使用商品iPhone数据训练，另一种使用机器人来源的图像和姿态数据训练。通过比较这两种NeRF重建结果与最先进的LiDAR-惯性SLAM在场景噪声和度量精度方面的表现，来评估NeRF的几何精度。</p></li><li><p>(4)任务与性能：<br>本文的任务是估计垂直PVC圆柱的直径。实验结果表明，NeRF在场景重建任务中表现出较高的几何精度，与最先进的LiDAR-惯性SLAM相比，具有较低的场景噪声和较高的度量精度。这证明了NeRF在实际应用中的潜力和优越性，特别是在成本效益和易访问性方面。</p></li></ul></li></ol><p>希望这个总结符合您的要求！</p><ol><li>方法论概述：</li></ol><p>本文的方法论主要包括两个部分：基于LiDAR-inertial SLAM的3D映射方法和基于NeRF的重建方法。</p><p>(1) LiDAR-inertial SLAM方法：<br>这是一种目前最先进的3D映射技术。它融合了LiDAR和IMU数据，创建密集的时空重建。该方法使用传统的姿态图SLAM表达式来优化实时生成的地图。研究使用的是Unitree B1四足机器人，配备了定制的感知负载。LiDAR是Ouster OS0-128，IMU是Inertialsense IMX-5。LIOSAM在机器人计算机上的ROS框架上运行，并提供了探索环境的地图和机器人的轨迹。</p><p>(2) 基于NeRF的重建方法：<br>本文主要采用Nerfacto方法进行NeRF重建。Nerfacto方法在几个关键方向上改进了基础NeRF方法。首先是姿态优化。图像姿态的错误会导致重建场景出现模糊伪影和清晰度损失。Nerfacto方法使用反向传播的损失梯度来优化每个训练迭代的姿态。另一个改进是5D输入空间的射线采样。光线被建模为锥形截锥。分段采样步骤在距离相机原点的一定距离内均匀采样光线，随后在锥形射线的后续部分以随着每个样本的增加而增大的步长进行采样。这允许对场景的近距离部分进行高细节采样，同时有效地对远处的物体进行采样。输出被输入到提案采样器中，该采样器将样本位置整合到对最终3D场景渲染贡献最大的场景部分中。为了告知哪些样本位置应该被整合，使用了由小型融合MLP和哈希编码组成的一连串密度函数。这些采样阶段的输出被输入到Nerfacto字段中。这一阶段结合了外观嵌入，这考虑了训练图像之间不同的曝光度。“粗略”和“精细”的MLP对输出颜色和三维场景结构进行建模。</p><p>总的来说，本文通过对比LiDAR-inertial SLAM和NeRF两种重建方法，评估了NeRF在实际场景重建中的几何精度，并验证了其在成本效益和易访问性方面的优势。</p><ol><li>Conclusion:</li></ol><p>(1)该工作的重要性在于展示了NeRF技术在重建现实世界的测量任务中的可行性。通过与传统LiDAR-inertial SLAM技术的对比实验，证明了NeRF技术在场景重建中的几何精度和优越性，特别是在成本效益和易访问性方面。此外，该研究还展示了使用商品iPhone数据和机器人来源的图像和姿态数据进行NeRF重建的潜力。这项研究为神经场景表示提供了令人兴奋的前景，并有望加速森林环境的映射过程，为我们对森林状态的理解以及保护这一宝贵资源提供更深层次的洞察力。</p><p>(2)创新点：该文章的创新之处在于对NeRF技术在场景重建中的几何精度进行了评估，并展示了使用商品iPhone数据和机器人来源的图像和姿态数据进行NeRF重建的可行性。性能：通过与传统LiDAR-inertial SLAM技术的对比实验，证明了NeRF重建方法具有较高的几何精度和较低的场景噪声。工作量：该文章进行了充分的实验和评估，包括创建NeRF重建、对比实验和性能评估等，工作量较大且充分。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-959893970381379d4121a125670d5ff6.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-90ad4dc4d4ed6179fa72cbb5e80dbad4.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-0cd31a6ddfb5259a953d7cf74c41fb7f.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-ae714318b916d0ec3524e5a68c4b2daf.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-ce95fd47c9281291fc23bd20abb906a5.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-14418e26d5e8c8b737e7a29c0ec37cfc.jpg" align="middle"></details></summary></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io/2024/08/05/Paper/2024-08-05/NeRF/">https://kedreamix.github.io/2024/08/05/Paper/2024-08-05/NeRF/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/NeRF/">NeRF</a></div><div class="post_share"><div class="social-share" data-image="https://picx.zhimg.com/v2-ae714318b916d0ec3524e5a68c4b2daf.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/08/13/Paper/2024-08-13/NeRF/" title="NeRF"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-2b0e0abdbc020b97d3e0fe97c1f53bf0.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">NeRF</div></div></a></div><div class="next-post pull-right"><a href="/2024/08/05/Paper/2024-08-05/3DGS/" title="3DGS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-cc2ae8cd8c55f642cae4e20363853c4c.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">3DGS</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/03/15/Paperscape/Real3D-Portrait/" title="REAL3D-PORTRAIT ONE-SHOT REALISTIC 3D TALKING PORTRAIT SYNTHESIS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-68585b79de5f83b0dfa23304f41b9b98.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-15</div><div class="title">REAL3D-PORTRAIT ONE-SHOT REALISTIC 3D TALKING PORTRAIT SYNTHESIS</div></div></a></div><div><a href="/2024/03/05/Paperscape/VividTalk/" title="VividTalk One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-8521b04f82075cc27b5e95148dba9792.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-05</div><div class="title">VividTalk One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior</div></div></a></div><div><a href="/2024/03/07/Paperscape/SyncTalk/" title="SyncTalk The Devil is in the Synchronization for Talking Head Synthesis"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-a57e0937b2f452009023394a59529dfb.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-07</div><div class="title">SyncTalk The Devil is in the Synchronization for Talking Head Synthesis</div></div></a></div><div><a href="/2024/01/25/Paper/3DGS%20Survey/" title="3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-25</div><div class="title">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</div></div></a></div><div><a href="/2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" title="超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-01</div><div class="title">超赞的数字人生成知识库 Awesome-Talking-Head-Synthesis</div></div></a></div><div><a href="/2024/02/09/Paper/2024-02-09/NeRF/" title="NeRF"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-512893851e477e6cab6fb9d3224f7acf.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-09</div><div class="title">NeRF</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-08-05-%E6%9B%B4%E6%96%B0"><span class="toc-text">2024-08-05 更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#PAV-Personalized-Head-Avatar-from-Unstructured-Video-Collection"><span class="toc-text">PAV: Personalized Head Avatar from Unstructured Video Collection</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Garment-Animation-NeRF-with-Color-Editing"><span class="toc-text">Garment Animation NeRF with Color Editing</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#8-Conclusion"><span class="toc-text">8. Conclusion:</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%88%E5%AF%B9%E4%BA%8E%E7%AC%AC%E4%B8%80%E5%B0%8F%E9%97%AE%E7%9A%84%E7%AD%94%E6%A1%88%EF%BC%89%EF%BC%9A"><span class="toc-text">（对于第一小问的答案）：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%88%E5%AF%B9%E4%BA%8E%E7%AC%AC%E4%BA%8C%E5%B0%8F%E9%97%AE%E7%9A%84%E7%AD%94%E6%A1%88%EF%BC%8C%E5%88%86%E5%88%AB%E4%BB%8E%E5%88%9B%E6%96%B0%E7%82%B9%E3%80%81%E6%80%A7%E8%83%BD%E5%92%8C%E5%B7%A5%E4%BD%9C%E8%B4%9F%E8%BD%BD%E4%B8%89%E4%B8%AA%E6%96%B9%E9%9D%A2%E5%AF%B9%E6%96%87%E7%AB%A0%E8%BF%9B%E8%A1%8C%E5%BC%BA%E5%BC%B1%E6%A6%82%E6%8B%AC%EF%BC%89%EF%BC%9A"><span class="toc-text">（对于第二小问的答案，分别从创新点、性能和工作负载三个方面对文章进行强弱概括）：</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#FINER-Building-a-Family-of-Variable-periodic-Functions-for-Activating-Implicit-Neural-Representation"><span class="toc-text">FINER++: Building a Family of Variable-periodic Functions for Activating Implicit Neural Representation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#IOVS4NeRF-Incremental-Optimal-View-Selection-for-Large-Scale-NeRFs"><span class="toc-text">IOVS4NeRF:Incremental Optimal View Selection for Large-Scale NeRFs</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Evaluating-geometric-accuracy-of-NeRF-reconstructions-compared-to-SLAM-method"><span class="toc-text">Evaluating geometric accuracy of NeRF reconstructions compared to SLAM method</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image:url('https://picx.zhimg.com/v2-ae714318b916d0ec3524e5a68c4b2daf.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js").then((()=>{pangu.autoSpacingPage()}))}function panguInit(){panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.1},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script></div><script id="canvas_nest" defer color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><script data-pjax>function butterfly_swiper_injector_config(){var a=document.getElementById("recent-posts");console.log("已挂载butterfly_swiper"),a.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/25/Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="2024/01/25/Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="2024/01/25/Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/17/Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023/12/17/Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="2023/12/17/Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script></body></html>