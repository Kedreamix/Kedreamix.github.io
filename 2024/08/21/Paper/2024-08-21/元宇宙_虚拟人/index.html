<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>元宇宙/虚拟人 | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="元宇宙&#x2F;虚拟人 方向最新论文已更新，请持续关注 Update in 2024-08-21  SG-GS Photo-realistic Animatable Human Avatars with Semantically-Guided   Gaussian Splatting"><meta property="og:type" content="article"><meta property="og:title" content="元宇宙&#x2F;虚拟人"><meta property="og:url" content="https://kedreamix.github.io/2024/08/21/Paper/2024-08-21/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/index.html"><meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World"><meta property="og:description" content="元宇宙&#x2F;虚拟人 方向最新论文已更新，请持续关注 Update in 2024-08-21  SG-GS Photo-realistic Animatable Human Avatars with Semantically-Guided   Gaussian Splatting"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://picx.zhimg.com/v2-d05a0aab7c3ee1cb21c6111b8ce45bf2.jpg"><meta property="article:published_time" content="2024-08-20T23:42:07.000Z"><meta property="article:modified_time" content="2024-08-20T23:42:07.741Z"><meta property="article:author" content="Kedreamix"><meta property="article:tag" content="元宇宙&#x2F;虚拟人"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://picx.zhimg.com/v2-d05a0aab7c3ee1cb21c6111b8ce45bf2.jpg"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/2024/08/21/Paper/2024-08-21/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-74LZ5BEQQ1")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!0,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"简"},noticeOutdate:{limitDay:500,position:"top",messagePrev:"It has been",messageNext:"days since the last update, the content of the article may be outdated."},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:200},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!0,post:!0},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!0,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"元宇宙/虚拟人",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-08-21 07:42:07"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach((e=>{n.setAttribute(e,t[e])})),document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)})),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=24?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 7.0.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml"></head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{const d=document.getElementById("loading-box"),e=document.body,o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l(),window.addEventListener("load",(()=>{o()}))})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">278</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url('https://picx.zhimg.com/v2-d05a0aab7c3ee1cb21c6111b8ce45bf2.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">元宇宙/虚拟人</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-08-20T23:42:07.000Z" title="发表于 2024-08-21 07:42:07">2024-08-21</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-08-20T23:42:07.741Z" title="更新于 2024-08-21 07:42:07">2024-08-21</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">7.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>24分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="元宇宙/虚拟人"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-08-21-更新"><a href="#2024-08-21-更新" class="headerlink" title="2024-08-21 更新"></a>2024-08-21 更新</h1><h2 id="SG-GS-Photo-realistic-Animatable-Human-Avatars-with-Semantically-Guided-Gaussian-Splatting"><a href="#SG-GS-Photo-realistic-Animatable-Human-Avatars-with-Semantically-Guided-Gaussian-Splatting" class="headerlink" title="SG-GS: Photo-realistic Animatable Human Avatars with Semantically-Guided   Gaussian Splatting"></a>SG-GS: Photo-realistic Animatable Human Avatars with Semantically-Guided Gaussian Splatting</h2><p><strong>Authors:Haoyu Zhao, Chen Yang, Hao Wang, Xingyue Zhao, Wei Shen</strong></p><p>Reconstructing photo-realistic animatable human avatars from monocular videos remains challenging in computer vision and graphics. Recently, methods using 3D Gaussians to represent the human body have emerged, offering faster optimization and real-time rendering. However, due to ignoring the crucial role of human body semantic information which represents the intrinsic structure and connections within the human body, they fail to achieve fine-detail reconstruction of dynamic human avatars. To address this issue, we propose SG-GS, which uses semantics-embedded 3D Gaussians, skeleton-driven rigid deformation, and non-rigid cloth dynamics deformation to create photo-realistic animatable human avatars from monocular videos. We then design a Semantic Human-Body Annotator (SHA) which utilizes SMPL’s semantic prior for efficient body part semantic labeling. The generated labels are used to guide the optimization of Gaussian semantic attributes. To address the limited receptive field of point-level MLPs for local features, we also propose a 3D network that integrates geometric and semantic associations for human avatar deformation. We further implement three key strategies to enhance the semantic accuracy of 3D Gaussians and rendering quality: semantic projection with 2D regularization, semantic-guided density regularization and semantic-aware regularization with neighborhood consistency. Extensive experiments demonstrate that SG-GS achieves state-of-the-art geometry and appearance reconstruction performance.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.09665v1">PDF</a> 12 pages, 5 figures</p><p><strong>Summary</strong><br>通过使用语义嵌入的3D高斯模型和骨架驱动的刚性变形，SG-GS方法能够从单眼视频中创建逼真的可动人体化身。</p><p><strong>Key Takeaways</strong></p><ul><li>使用语义嵌入的3D高斯模型和骨架驱动的刚性变形，能够提高动态人体化身的细节重建能力。</li><li>设计了语义人体部分注释器（SHA），利用SMPL的语义先验进行高效的语义标签生成。</li><li>提出了三维网络，整合几何和语义关联，用于人体化身的变形，以解决点级MLP局部特征接受域有限的问题。</li><li>引入了三种策略以提升3D高斯模型的语义精度和渲染质量：语义投影与2D正则化、语义引导的密度正则化以及语义感知的邻域一致性正则化。</li><li>SG-GS方法在几何和外观重建性能上实现了最先进水平，通过广泛实验验证了其有效性。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>标题：基于语义引导的逼真动画人类半身像重建研究</p></li><li><p>作者：Zhao Haoyu（赵浩宇）、Yang Chen（杨晨）、Wang Hao（王浩）、Zhao Xingyue（赵星越）、Shen Wei（沈炜）等。</p></li><li><p>所属机构：上海交通大学人工智能研究院等。</p></li><li><p>关键词：语义引导、高斯模型、人体动画重建等。</p></li><li><p>Urls：论文链接待补充，GitHub代码链接待补充（如果可用）。</p></li><li><p>摘要：</p><ul><li><p>(1)研究背景：本文研究基于单目视频生成逼真动画人类半身像的技术。此技术在计算机视觉和图形学领域具有挑战性，广泛应用于游戏、扩展现实故事叙述、远程呈现等领域。</p></li><li><p>(2)过去的方法及问题：虽然使用三维高斯模型表示人体进行重建的方法能更快优化和实时渲染，但它们忽略了人体语义信息（如内在结构和连接），导致动态人类半身像精细细节重建失败。</p></li><li><p>(3)研究方法：本文提出SG-GS方法，使用嵌入语义的三维高斯模型、骨架驱动刚性变形和非刚性布料动态变形来创建动画。设计语义人体标注器（SHA）利用SMPL的语义先验进行高效身体部分语义标注。为解决点级MLP的局部特征受限问题，提出一个三维网络，集成几何和语义关联进行半身像变形。同时实施三种关键策略提高三维高斯语义准确性和渲染质量。</p></li><li><p>(4)任务与性能：本文方法在静态场景的新视图合成任务上实现最先进的性能，通过大量实验验证SG-GS在几何和外观重建方面的优越性。性能支持其目标，即创建高质量、逼真的动画人类半身像。</p></li></ul></li><li>方法论概述：</li></ol><p>本文提出一种基于语义引导的逼真动画人类半身像重建方法（SG-GS方法）。其方法论主要包括以下几个步骤：</p><p>（1）提出使用嵌入语义的三维高斯模型、骨架驱动刚性变形和非刚性布料动态变形来创建动画。</p><p>（2）设计语义人体标注器（SHA），利用SMPL的语义先验进行高效身体部分语义标注。</p><p>（3）为解决点级MLP的局部特征受限问题，提出一个三维网络，集成几何和语义关联进行半身像变形。</p><p>（4）实施三种关键策略提高三维高斯语义准确性和渲染质量，包括语义投影与二维正则化、语义引导密度正则化和语义感知邻域一致性控制。</p><p>其中，具体实现方式如下：</p><ul><li>语义人体标注器（SHA）：通过使用SMPL模型的姿态感知形状先验，结合可微骨骼变换，对标准人体模型进行变形。然后，通过自定义的点渲染函数，将变形的SMPL模型渲染成图像，并通过k近邻算法对前景掩膜进行语义级别的标注，实现身体部分的精确语义标注。</li><li>三维几何和语义感知网络：为了有效地利用三维几何和语义信息，提出了一个三维几何和语义感知网络。该网络通过稀疏卷积操作，提取点云的局部几何和语义特征，然后结合语义属性进行优化，以实现更精细的变形和更真实的渲染效果。</li><li>变形和优化：通过刚性变形和非刚性变形相结合的方式，将高斯模型从规范空间变形到观察空间。在变形过程中，利用语义信息进行指导，提高变形的准确性和自然度。同时，通过实施一系列优化策略，如语义投影与二维正则化、语义引导密度正则化和语义感知邻域一致性控制等，进一步提高语义准确性和渲染质量。</li></ul><p>本文的方法在静态场景的新视图合成任务上实现了最先进的性能，通过大量实验验证了其在几何和外观重建方面的优越性。</p><ol><li><p>Conclusion:</p><ul><li><p>(1)意义：该研究对于计算机视觉和图形学领域具有重要的价值，特别是在游戏、扩展现实故事叙述、远程呈现等方面，逼真动画人类半身像重建技术具有重要的应用前景。该研究能够推动相关领域的技术进步，增强虚拟世界的真实感和交互性。</p></li><li><p>(2)创新点、性能和工作量综述：</p><ul><li>创新点：文章提出了基于语义引导的逼真动画人类半身像重建方法（SG-GS方法），集成了三维高斯模型、骨架驱动刚性变形和非刚性布料动态变形等技术，并利用语义人体标注器和三维网络进行高效的语义标注和半身像变形。此外，文章还实施了多种关键策略提高三维高斯语义准确性和渲染质量。</li><li>性能：文章在静态场景的新视图合成任务上实现了最先进的性能，并通过大量实验验证了其在几何和外观重建方面的优越性。与现有技术相比，该方法能够创建高质量、逼真的动画人类半身像。</li><li>工作量：文章进行了详尽的实验和验证，包括多种数据集上的实验、对比实验和消融实验等，证明了方法的有效性和优越性。同时，文章还进行了系统的理论分析和阐述，包括方法的设计原理、实现细节和优缺点等。</li></ul></li></ul></li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-ba5b1c9670f62cb1607c75082ab8bbc6.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-af72231531308d30699380f35f626a85.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-e9ae3ccb56e9ed00e228b564abf1d0db.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-a3fcde02ab958d6428de5a05025481ae.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-c35f094e651fb1b0c92fa9b15c01554e.jpg" align="middle"></details><h2 id="CHASE-3D-Consistent-Human-Avatars-with-Sparse-Inputs-via-Gaussian-Splatting-and-Contrastive-Learning"><a href="#CHASE-3D-Consistent-Human-Avatars-with-Sparse-Inputs-via-Gaussian-Splatting-and-Contrastive-Learning" class="headerlink" title="CHASE: 3D-Consistent Human Avatars with Sparse Inputs via Gaussian   Splatting and Contrastive Learning"></a>CHASE: 3D-Consistent Human Avatars with Sparse Inputs via Gaussian Splatting and Contrastive Learning</h2><p><strong>Authors:Haoyu Zhao, Hao Wang, Chen Yang, Wei Shen</strong></p><p>Recent advancements in human avatar synthesis have utilized radiance fields to reconstruct photo-realistic animatable human avatars. However, both NeRFs-based and 3DGS-based methods struggle with maintaining 3D consistency and exhibit suboptimal detail reconstruction, especially with sparse inputs. To address this challenge, we propose CHASE, which introduces supervision from intrinsic 3D consistency across poses and 3D geometry contrastive learning, achieving performance comparable with sparse inputs to that with full inputs. Following previous work, we first integrate a skeleton-driven rigid deformation and a non-rigid cloth dynamics deformation to coordinate the movements of individual Gaussians during animation, reconstructing basic avatar with coarse 3D consistency. To improve 3D consistency under sparse inputs, we design Dynamic Avatar Adjustment(DAA) to adjust deformed Gaussians based on a selected similar pose/image from the dataset. Minimizing the difference between the image rendered by adjusted Gaussians and the image with the similar pose serves as an additional form of supervision for avatar. Furthermore, we propose a 3D geometry contrastive learning strategy to maintain the 3D global consistency of generated avatars. Though CHASE is designed for sparse inputs, it surprisingly outperforms current SOTA methods \textbf{in both full and sparse settings} on the ZJU-MoCap and H36M datasets, demonstrating that our CHASE successfully maintains avatar’s 3D consistency, hence improving rendering quality.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.09663v1">PDF</a> 13 pages, 6 figures</p><p><strong>Summary</strong><br>提出了一种新方法CHASE，结合了3D一致性监督和几何对比学习，显著提高了稀疏输入下人物头像合成的效果。</p><p><strong>Key Takeaways</strong></p><ul><li>使用CHASE方法，结合了骨架驱动的刚性变形和非刚性布料动力学变形，实现了基本的头像合成，并保持了粗略的3D一致性。</li><li>引入Dynamic Avatar Adjustment（DAA）机制，根据数据集中类似姿态/图像调整变形高斯分布，进一步提高了稀疏输入情况下的3D一致性。</li><li>设计了3D几何对比学习策略，有助于维持生成头像的全局3D一致性。</li><li>CHASE方法在ZJU-MoCap和H36M数据集上展示出色，无论是在全输入还是稀疏输入条件下，均优于当前技术水平。</li><li>成功提升了头像合成的渲染质量，并展示了在头像的3D一致性方面的显著改进。</li><li>NeRFs和3DGS方法在3D一致性和细节重建方面存在挑战，尤其是在稀疏输入情况下表现不佳。</li><li>CHASE方法结合了多种技术，有效克服了现有方法的局限性，为虚拟人头像合成领域带来了新的发展方向。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>好的，我会按照您的要求来总结这篇论文。</p><ol><li><p>标题：基于高斯分裂和对比学习的稀疏输入下三维一致人形化身研究（CHASE: 3D-Consistent Human Avatars with Sparse Inputs via Gaussian Splatting and Contrastive Learning）</p></li><li><p>作者：Haoyu Zhao, Hao Wang, Chen Yang, Wei Shen等。</p></li><li><p>隶属机构：上海交通大学人工智能实验室等。</p></li><li><p>关键词：Human Avatar合成、稀疏输入、高斯分裂、对比学习、三维一致性。</p></li><li><p>Urls：论文链接待定，代码GitHub链接待定（如果可用）。</p></li><li><p>摘要：</p><ul><li><p>(1)研究背景：本文研究了在稀疏输入条件下，如何合成具有三维一致性的人形化身。近年来，虽然人形化身合成技术已经取得显著进展，但在稀疏输入条件下保持三维一致性和细节重建仍存在挑战。</p></li><li><p>(2)过去的方法及问题：现有的方法大多依赖于丰富的输入数据，如多视角图像或深度传感器数据。然而，在稀疏输入条件下，这些方法往往难以保持三维一致性并重建出高质量的细节。因此，需要一种新的方法来解决这个问题。</p></li><li><p>(3)研究方法：本文提出了一种基于高斯分裂和对比学习的方法来解决稀疏输入下的三维一致人形化身合成问题。首先，通过结合骨架驱动的刚性变形和非刚性布料动力学变形，创建具有粗略三维一致性的基本化身。然后，通过动态化身调整（DAA）策略，基于数据集中的相似姿势/图像对变形的高斯进行微调。此外，还提出了一种3D几何对比学习策略，以维持生成化身的全球三维一致性。</p></li><li><p>(4)任务与性能：本文的方法在ZJU-MoCap和H36M数据集上进行了测试，并在全数据和稀疏输入两种设置下均取得了出色的性能。结果表明，该方法在稀疏输入条件下成功地保持了化身的三维一致性，提高了渲染质量。性能结果支持了该方法的目标实现。</p></li></ul></li></ol><p>以上就是对该论文的概括，希望对你有所帮助。<br>好的，我将根据您提供的论文内容详细阐述这篇论文的方法论思想。</p><ol><li>方法论：</li></ol><p>（1）研究背景与问题概述：文章首先概述了在稀疏输入条件下合成具有三维一致性的人形化身的技术挑战，并指出现有方法的不足。</p><p>（2）研究方法概述：为了解决上述问题，文章提出了一种基于高斯分裂和对比学习的方法来解决稀疏输入下的三维一致人形化身合成问题。该方法的流程如图1所示。</p><p>（3）输入数据处理与模型构建：文章的输入包括从单目视频中获得的图像、拟合的SMPL参数以及图像的前景色掩码。模型通过对3D高斯模型在规范空间进行优化，然后通过变形匹配观测空间并进行渲染。这一过程中结合了骨架驱动的刚性变形和非刚性布料动力学变形技术。</p><p>（4）动态化身调整策略：为了解决极端稀疏输入的问题，文章提出了一种动态化身调整（DAA）策略。该策略基于数据集中的相似姿势/图像对变形的高斯进行微调，通过引入额外的二维图像监督，提高了人形化身的三维一致性。</p><p>（5）非刚性变形网络设计：为了实现对规范空间中的高斯模型的非刚性变形，文章设计了一个非刚性变形网络。该网络以高斯模型的规范位置和SMPL姿势编码作为输入，输出各种参数的偏移量，从而实现高斯模型的变形。</p><p>（6）刚性变换与皮肤网格技术：变形后的高斯模型进一步通过基于LBS的刚性变换映射到观测空间，通过与目标姿势对齐的变换矩阵实现。为了精确控制三维高斯模型，文章从SMPL模型中采样了稀疏控制点，并利用LBS权重获得密集运动场。</p><p>（7）动态调整过程与结果优化：利用稀疏控制点的LBS权重对变形后的高斯模型进行微调，以实现对选定相似姿势的精确匹配。调整过程通过最小化调整后的化身渲染图像与选定相似姿势图像之间的差异来实现额外的监督，从而增强动画化身的创建。</p><p>（8）三维几何对比学习策略：为了保持生成的化身在全球范围内的三维一致性，文章提出了一个三维几何对比学习策略。该策略将三维高斯模型视为一个三维点云，并采用DGCNN作为特征提取器来处理不同姿势下的点云特征，确保在动画过程中的三维一致性。</p><p>以上就是这篇论文的方法论思想概述。希望对你有所帮助。</p><p>好的，以下是这篇论文的总结：</p><ol><li>Conclusion:</li></ol><p>（1）研究意义：本文研究了在稀疏输入条件下如何合成具有三维一致性的人形化身，解决了现有方法在稀疏输入条件下难以保持三维一致性和细节重建的问题。该研究对于人工智能领域的人形化身合成技术具有重要的推动作用，有助于实现更加真实、生动的人形动画。此外，该研究在虚拟现实、增强现实、游戏制作等领域也有广泛的应用前景。</p><p>（2）论文的优缺点：</p><p>创新点：本文提出了基于高斯分裂和对比学习的方法来解决稀疏输入下的三维一致人形化身合成问题，该方法结合了骨架驱动的刚性变形和非刚性布料动力学变形技术，通过动态化身调整策略和三维几何对比学习策略来保持化身的三维一致性。该方法在稀疏输入条件下取得了显著的性能提升，具有较高的创新性。</p><p>性能：本文的方法在ZJU-MoCap和H36M数据集上进行了测试，并在全数据和稀疏输入两种设置下均取得了出色的性能。实验结果表明，该方法在稀疏输入条件下成功地保持了化身的三维一致性，提高了渲染质量。</p><p>工作量：文章涉及了较多的技术细节和实验验证，包括输入数据处理、模型构建、动态化身调整策略、非刚性变形网络设计、刚性变换与皮肤网格技术、动态调整过程与结果优化以及三维几何对比学习策略等。工作量较大，但实验结果证明了方法的有效性。</p><p>综上所述，本文提出了一种创新的基于高斯分裂和对比学习的人形化身合成方法，在稀疏输入条件下取得了显著的性能提升，具有较高的学术价值和应用前景。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-749622bdeb5ca1e6731520c549fdd0e3.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-060b38ccf8897d85a83f1eef91ce6c1b.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-3d3dca898a7edd9f20d2ba3cda712423.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-af62178f5fdd22828fd6edb951afcb8c.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-5456bb2bf3dabbd73a53ce6f04593b9a.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-8c68f49b04c0a784781a9f795f4373ae.jpg" align="middle"></details><h2 id="Barbie-Text-to-Barbie-Style-3D-Avatars"><a href="#Barbie-Text-to-Barbie-Style-3D-Avatars" class="headerlink" title="Barbie: Text to Barbie-Style 3D Avatars"></a>Barbie: Text to Barbie-Style 3D Avatars</h2><p><strong>Authors:Xiaokun Sun, Zhenyu Zhang, Ying Tai, Qian Wang, Hao Tang, Zili Yi, Jian Yang</strong></p><p>Recent advances in text-guided 3D avatar generation have made substantial progress by distilling knowledge from diffusion models. Despite the plausible generated appearance, existing methods cannot achieve fine-grained disentanglement or high-fidelity modeling between inner body and outfit. In this paper, we propose Barbie, a novel framework for generating 3D avatars that can be dressed in diverse and high-quality Barbie-like garments and accessories. Instead of relying on a holistic model, Barbie achieves fine-grained disentanglement on avatars by semantic-aligned separated models for human body and outfits. These disentangled 3D representations are then optimized by different expert models to guarantee the domain-specific fidelity. To balance geometry diversity and reasonableness, we propose a series of losses for template-preserving and human-prior evolving. The final avatar is enhanced by unified texture refinement for superior texture consistency. Extensive experiments demonstrate that Barbie outperforms existing methods in both dressed human and outfit generation, supporting flexible apparel combination and animation. The code will be released for research purposes. Our project page is: <a target="_blank" rel="noopener" href="https://2017211801.github.io/barbie.github.io/">https://2017211801.github.io/barbie.github.io/</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.09126v1">PDF</a> 9 pages, 7 figures</p><p><strong>Summary</strong><br>Barbie提出了一种新的框架，通过语义对齐分离人体和服装模型，实现了3D化身的精细解缠和高保真建模。</p><p><strong>Key Takeaways</strong></p><ul><li>Barbie框架采用语义对齐分离模型，实现了人体和服装的精细解缠。</li><li>通过专家模型优化，保证了领域特定的保真度。</li><li>提出了一系列损失函数，用于保持模板和进化人体先验，平衡几何多样性和合理性。</li><li>引入统一的纹理细化技术，提升了纹理一致性。</li><li>在服装生成和人体动画方面，Barbie优于现有方法。</li><li>研究支持灵活的服装组合和动画展示。</li><li>该研究将发布代码以供研究使用，项目页面链接为：<a target="_blank" rel="noopener" href="https://2017211801.github.io/barbie.github.io/。">https://2017211801.github.io/barbie.github.io/。</a></li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>好的，我会按照您的要求总结这篇论文。</p><ol><li><p>标题：Barbie：基于文本到Barbie风格3D角色的生成</p></li><li><p>作者：Xiaokun Sun, Zhenyu Zhang, Ying Tai, Qian Wang, Hao Tang, Zili Yi, Jian Yang</p></li><li><p>隶属机构：南京大学、中国移动研究学院、北京大学、南京理工大学</p></li><li><p>关键词：文本指导3D角色生成，Barbie风格角色，语义对齐模型，精细分解，领域特定保真度优化</p></li><li><p>链接：论文链接：待确定；GitHub代码链接：GitHub可用时填写（当前为None）</p></li><li><p>摘要：</p><ul><li><p>(1)研究背景：随着虚拟世界和增强现实技术的发展，创建逼真的三维数字人物成为研究的热点。近年来，文本指导的三维角色生成取得了进展，但仍面临精细分解和高保真度建模等挑战。本文提出一种生成Barbie风格三维角色的新方法。</p></li><li><p>(2)过去的方法及问题：现有的文本指导三维角色生成方法主要分为两类：整体角色生成和身体和服装的精细分解生成。整体角色生成方法无法灵活控制服装和配件，而精细分解方法则面临领域特定保真度损失的问题。本文提出的方法旨在解决这些问题。</p></li><li><p>(3)研究方法：本研究提出了一种名为Barbie的新框架，用于生成逼真的Barbie风格三维角色。该方法通过语义对齐的分离模型实现身体和服装的精细分解。然后，通过不同的专家模型对这些解耦的三维表示进行优化，以保证领域特定的保真度。同时，通过一系列损失函数平衡几何多样性和合理性，并对最终的角色进行纹理优化，以提高纹理一致性。</p></li><li><p>(4)任务与性能：本研究在服装人生成和服装生成任务上进行了实验，结果表明Barbie在生成具有Barbie风格的三维角色方面表现出色，支持灵活的服装组合和动画。性能结果表明，Barbie在几何多样性、纹理质量和与文本描述的一致性方面达到了先进水平。</p></li></ul></li></ol><p>希望这个摘要符合您的要求！</p><ol><li>方法论：</li></ol><ul><li>(1) 首先，研究团队提出了名为Barbie的新框架，用于生成逼真的Barbie风格三维角色。该框架基于语义对齐的分离模型，实现了身体和服装的精细分解。</li><li>(2) 在生成角色时，研究团队采用了分阶段的方法。首先进行人体生成初始化，使用SMPL-X网格建立精确初始输入。然后对人体几何建模进行优化，使用人类特定的扩散模型（如HumanNorm中的模型）进行详细的身体建模。为了平衡生成的几何形状的多样性和合理性，研究团队引入了一种自我进化的先验损失函数。这种损失函数会周期性地适应人体几何形状的变化，同时保留拓扑结构，为后续服装的初始化和组合提供了可靠但多样化的先验知识。</li><li>(3) 在服装和配饰的生成阶段，研究团队利用对象特定的扩散模型对每件衣物和配饰进行高质量创建。最后进行统一纹理优化，以增强整个角色的纹理和谐性和一致性。整个过程中涉及多种损失函数，包括SDS损失和先验损失等，用于优化生成的角色的几何形状和纹理质量。总体来说，该文章通过精细分解、领域特定保真度优化和自进化先验损失等方法，实现了基于文本指导生成Barbie风格三维角色的目标。</li></ul><ol><li>Conclusion:</li></ol><p>(1) 这项工作的意义在于提出了一种基于文本指导生成Barbie风格三维角色的新方法，为虚拟世界和增强现实技术中的三维角色生成提供了新的思路和技术手段。</p><p>(2) 创新点：该文章通过精细分解、领域特定保真度优化和自进化先验损失等方法，实现了基于文本指导生成Barbie风格三维角色的目标，具有一定的创新性。性能：在服装人生成和服装生成任务上的实验结果表明，Barbie在生成具有Barbie风格的三维角色方面表现出色，支持灵活的服装组合和动画，具有较高的性能。工作量：该文章实现了一种完整的框架和方法，包括人体和服装的精细分解、领域特定保真度优化、自进化先验损失等，工作量较大。</p><p>总体来说，该文章为基于文本指导的三维角色生成提供了新的思路和方法，具有一定的创新性和应用价值。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-b3666e914f9727885202edefed0f6b41.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-82aef8d8f1aed2ceef69e20d1f2aeaca.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-d05a0aab7c3ee1cb21c6111b8ce45bf2.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-10380f66381cdb3f0d26a35da5d2c482.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-a66b9f1c3e5e087c1b363bb26b124d4e.jpg" align="middle"></details><h2 id="HeadGAP-Few-shot-3D-Head-Avatar-via-Generalizable-Gaussian-Priors"><a href="#HeadGAP-Few-shot-3D-Head-Avatar-via-Generalizable-Gaussian-Priors" class="headerlink" title="HeadGAP: Few-shot 3D Head Avatar via Generalizable Gaussian Priors"></a>HeadGAP: Few-shot 3D Head Avatar via Generalizable Gaussian Priors</h2><p><strong>Authors:Xiaozheng Zheng, Chao Wen, Zhaohu Li, Weiyi Zhang, Zhuo Su, Xu Chang, Yang Zhao, Zheng Lv, Xiaoyuan Zhang, Yongjie Zhang, Guidong Wang, Lan Xu</strong></p><p>In this paper, we present a novel 3D head avatar creation approach capable of generalizing from few-shot in-the-wild data with high-fidelity and animatable robustness. Given the underconstrained nature of this problem, incorporating prior knowledge is essential. Therefore, we propose a framework comprising prior learning and avatar creation phases. The prior learning phase leverages 3D head priors derived from a large-scale multi-view dynamic dataset, and the avatar creation phase applies these priors for few-shot personalization. Our approach effectively captures these priors by utilizing a Gaussian Splatting-based auto-decoder network with part-based dynamic modeling. Our method employs identity-shared encoding with personalized latent codes for individual identities to learn the attributes of Gaussian primitives. During the avatar creation phase, we achieve fast head avatar personalization by leveraging inversion and fine-tuning strategies. Extensive experiments demonstrate that our model effectively exploits head priors and successfully generalizes them to few-shot personalization, achieving photo-realistic rendering quality, multi-view consistency, and stable animation.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.06019v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://headgap.github.io/">https://headgap.github.io/</a></p><p><strong>Summary</strong><br>提出了一种新颖的三维头像创建方法，能够从少样本野外数据中进行高保真和可动性强的泛化。</p><p><strong>Key Takeaways</strong></p><ul><li>创新的3D头像创建方法，能够从少样本数据中进行个性化建模。</li><li>引入先验知识至关重要，特别是在这种不受约束的问题情境中。</li><li>方法包括先验学习和头像创建两个阶段。</li><li>使用基于高斯飞溅的自动解码器网络和基于部件的动态建模来捕捉先验信息。</li><li>采用身份共享编码和个性化潜变量码以学习高斯原语的属性。</li><li>利用反演和微调策略实现快速头像个性化。</li><li>实验证明，模型能够有效地利用头部先验知识，并成功泛化到少样本个性化，达到了照片级渲染质量、多视角一致性和稳定动画效果。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>Please refer to relevant websites for more information, and feel free to ask me any other questions.<br>好的，请您提供需要总结的论文方法部分的内容，我会按照您的要求进行详细且简洁的中文总结。请确保您提供的内容清晰、连贯，我会尽力理解并按照要求的格式进行整理。如果没有具体的文章内容，我无法进行准确的总结。请提供具体的方法描述或相关段落，以便我能够帮助您完成这个任务。</p><ol><li>结论：</li></ol><p>（1）xxx。这项工作提出了一种创建高度真实感的3D头像的新方法，通过利用少量图像生成个性化头像，具有重要的应用价值和实践意义。它对于虚拟现实、增强现实、游戏设计等领域具有重要的推动作用。</p><p>（2）创新点：该文章的创新性主要体现在提出了一种基于高斯先验模型的个性化头像生成方法，通过引入GAPNet网络，能够利用大规模3D头像数据学习得到的3D高斯先验模型，辅助生成新型身份的头像。同时，文章展示了该方法在创建高度真实感的头像和稳健的动画方面的优越性。<br>性能：该文章所述方法在创建个性化头像方面具有优异的性能表现，能够在少量图像的情况下生成高质量的头像。此外，该方法还具有较好的泛化性能，能够在不同主体之间实现较为稳健的动画效果。<br>工作量：该文章涉及大量的实验和细节实现，工作量较大。文章详细介绍了数据集的处理、模型的构建、训练过程的细节等，体现了作者在研究中的严谨性和深入性。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-9c1a43c7674f0bee49951366b68f6c14.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-4768d6c4ded301cca943516e0c82a477.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-d311be7ef2485c6d182f6edcab5978b4.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-a1e0cdef0dcc76901e207d436b1ec963.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-2ba7795064ff602ff61c8717c10338cc.jpg" align="middle"></details></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io/2024/08/21/Paper/2024-08-21/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">https://kedreamix.github.io/2024/08/21/Paper/2024-08-21/元宇宙_虚拟人/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">元宇宙/虚拟人</a></div><div class="post_share"><div class="social-share" data-image="https://picx.zhimg.com/v2-d05a0aab7c3ee1cb21c6111b8ce45bf2.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/08/21/Paper/2024-08-21/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-60f199ba5d167ece24ebb3f059852456.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Diffusion Models</div></div></a></div><div class="next-post pull-right"><a href="/2024/08/14/Paper/2024-08-13/3DGS/" title="3DGS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-f052fe149e0b5be9567f15dabae0c0d4.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">3DGS</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/04/01/Paper/2024-04-01/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-4f2a829065c463be027e4b423c4e43c8.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-01</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/04/06/Paper/2024-04-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-4f97970c093585e18e2db42fb96a6b75.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-06</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/05/02/Paper/2024-05-02/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-37516691b13dabbabb3b74ea46b402d8.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-02</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/05/06/Paper/2024-05-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-65e21e3a0a320adc36f81e6bfc7c5739.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-06</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/05/22/Paper/2024-05-22/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-b4943be56cf94e02149b50eec541d27f.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-22</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/06/14/Paper/2024-06-14/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-3afa9e67f614d591989be2744ada9ff8.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-06-14</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-08-21-%E6%9B%B4%E6%96%B0"><span class="toc-text">2024-08-21 更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#SG-GS-Photo-realistic-Animatable-Human-Avatars-with-Semantically-Guided-Gaussian-Splatting"><span class="toc-text">SG-GS: Photo-realistic Animatable Human Avatars with Semantically-Guided Gaussian Splatting</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CHASE-3D-Consistent-Human-Avatars-with-Sparse-Inputs-via-Gaussian-Splatting-and-Contrastive-Learning"><span class="toc-text">CHASE: 3D-Consistent Human Avatars with Sparse Inputs via Gaussian Splatting and Contrastive Learning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Barbie-Text-to-Barbie-Style-3D-Avatars"><span class="toc-text">Barbie: Text to Barbie-Style 3D Avatars</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HeadGAP-Few-shot-3D-Head-Avatar-via-Generalizable-Gaussian-Priors"><span class="toc-text">HeadGAP: Few-shot 3D Head Avatar via Generalizable Gaussian Priors</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image:url('https://picx.zhimg.com/v2-d05a0aab7c3ee1cb21c6111b8ce45bf2.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js").then((()=>{pangu.autoSpacingPage()}))}function panguInit(){panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.1},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script></div><script id="canvas_nest" defer color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><script data-pjax>function butterfly_swiper_injector_config(){var a=document.getElementById("recent-posts");console.log("已挂载butterfly_swiper"),a.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/25/Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="2024/01/25/Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="2024/01/25/Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/17/Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023/12/17/Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="2023/12/17/Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script></body></html>