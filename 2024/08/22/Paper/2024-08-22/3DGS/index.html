<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>3DGS | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="3DGS 方向最新论文已更新，请持续关注 Update in 2024-08-22  Large Point-to-Gaussian Model for Image-to-3D Generation"><meta property="og:type" content="article"><meta property="og:title" content="3DGS"><meta property="og:url" content="https://kedreamix.github.io/2024/08/22/Paper/2024-08-22/3DGS/index.html"><meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World"><meta property="og:description" content="3DGS 方向最新论文已更新，请持续关注 Update in 2024-08-22  Large Point-to-Gaussian Model for Image-to-3D Generation"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://picx.zhimg.com/v2-8849bacfa31cad2cde282450aa71e051.jpg"><meta property="article:published_time" content="2024-08-21T23:53:56.000Z"><meta property="article:modified_time" content="2024-08-21T23:53:56.099Z"><meta property="article:author" content="Kedreamix"><meta property="article:tag" content="3DGS"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://picx.zhimg.com/v2-8849bacfa31cad2cde282450aa71e051.jpg"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/2024/08/22/Paper/2024-08-22/3DGS/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-74LZ5BEQQ1")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!0,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"简"},noticeOutdate:{limitDay:500,position:"top",messagePrev:"It has been",messageNext:"days since the last update, the content of the article may be outdated."},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:200},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!0,post:!0},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!0,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"3DGS",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-08-22 07:53:56"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach((e=>{n.setAttribute(e,t[e])})),document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)})),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=24?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 7.0.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml"></head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{const d=document.getElementById("loading-box"),e=document.body,o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l(),window.addEventListener("load",(()=>{o()}))})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">191</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url('https://picx.zhimg.com/v2-8849bacfa31cad2cde282450aa71e051.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">3DGS</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-08-21T23:53:56.000Z" title="发表于 2024-08-22 07:53:56">2024-08-22</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-08-21T23:53:56.099Z" title="更新于 2024-08-22 07:53:56">2024-08-22</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">16.5k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>55分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="3DGS"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-08-22-更新"><a href="#2024-08-22-更新" class="headerlink" title="2024-08-22 更新"></a>2024-08-22 更新</h1><h2 id="Large-Point-to-Gaussian-Model-for-Image-to-3D-Generation"><a href="#Large-Point-to-Gaussian-Model-for-Image-to-3D-Generation" class="headerlink" title="Large Point-to-Gaussian Model for Image-to-3D Generation"></a>Large Point-to-Gaussian Model for Image-to-3D Generation</h2><p><strong>Authors:Longfei Lu, Huachen Gao, Tao Dai, Yaohua Zha, Zhi Hou, Junta Wu, Shu-Tao Xia</strong></p><p>Recently, image-to-3D approaches have significantly advanced the generation quality and speed of 3D assets based on large reconstruction models, particularly 3D Gaussian reconstruction models. Existing large 3D Gaussian models directly map 2D image to 3D Gaussian parameters, while regressing 2D image to 3D Gaussian representations is challenging without 3D priors. In this paper, we propose a large Point-to-Gaussian model, that inputs the initial point cloud produced from large 3D diffusion model conditional on 2D image to generate the Gaussian parameters, for image-to-3D generation. The point cloud provides initial 3D geometry prior for Gaussian generation, thus significantly facilitating image-to-3D Generation. Moreover, we present the \textbf{A}ttention mechanism, \textbf{P}rojection mechanism, and \textbf{P}oint feature extractor, dubbed as \textbf{APP} block, for fusing the image features with point cloud features. The qualitative and quantitative experiments extensively demonstrate the effectiveness of the proposed approach on GSO and Objaverse datasets, and show the proposed method achieves state-of-the-art performance.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.10935v1">PDF</a> 10 pages, 9 figures, ACM MM 2024</p><p><strong>Summary</strong><br>基于大规模重建模型的图像到3D方法显著提升了生成质量和速度，特别是基于3D高斯重建模型，本文提出了一种新的点到高斯模型，通过初始点云生成高斯参数，显著促进了图像到3D生成。</p><p><strong>Key Takeaways</strong></p><ul><li>图像到3D方法的进步基于大规模重建模型，特别是3D高斯重建模型。</li><li>现有的大规模3D高斯模型直接将2D图像映射到3D高斯参数。</li><li>在没有3D先验的情况下，将2D图像回归到3D高斯表示具有挑战性。</li><li>文中提出了基于点云的大点到高斯模型，利用初始点云提供的3D几何先验生成高斯参数，从而促进图像到3D生成。</li><li>文中介绍了注意力机制、投影机制和点特征提取器（APP块），用于融合图像特征与点云特征。</li><li>实验结果在GSO和Objaverse数据集上广泛展示了所提方法的有效性。</li><li>所提方法在定量和定性实验中均达到了最先进的性能水平。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>好的，我会按照您的要求来总结这篇论文。</p><ol><li><p>标题：基于大视点至高斯模型的图像到3D生成研究</p></li><li><p>作者：Longfei Lu（龙飞）、Huachen Gao（高华琛）、Tao Dai（戴涛）、Yaohua Zha（赵耀华）、Zhi Hou（侯志）、Junta Wu（吴俊达）、ShuTao Xia（夏书涛）</p></li><li><p>隶属机构：龙飞、高华琛、赵耀华来自清华大学深圳国际研究生院，戴涛来自深圳大学计算机科学和软件工程学院，侯志和吴俊达来自腾讯公司，夏书涛同时担任清华大学和鹏城实验室的研究工作。</p></li><li><p>关键词：3D生成、3D高斯拼贴、单视图重建、点云</p></li><li><p>Urls：论文链接（待补充），GitHub代码链接（待补充，若无则填写“GitHub：无”）</p></li><li><p>总结：</p><p>(1) 研究背景：近年来，基于大重建模型的图像到3D转换技术已显著提高3D资产的质量和生成速度，特别是3D高斯重建模型。然而，直接映射2D图像到3D高斯参数仍存在挑战，缺乏3D先验信息。本文研究如何在缺乏3D先验信息的情况下，通过引入点云信息来提高图像到3D的生成质量。</p><p>(2) 过去的方法及问题：现有方法多直接通过2D图像回归得到3D高斯表示，但这样做在缺乏3D先验的情况下具有挑战性。因此，需要一种新的方法来解决这个问题。</p><p>(3) 研究方法：本文提出了一种基于大视点至高斯模型的图像到3D生成方法。该方法首先通过大3D扩散模型生成初始点云，然后结合图像特征，通过注意力机制、投影机制和点特征提取（APP块）融合点云特征，最后生成高斯参数。这种方法的引入点云信息作为初始的3D几何先验，显著促进了图像到3D的生成。</p><p>(4) 任务与性能：本文方法在GSO和Objaverse数据集上进行实验，展示所提出方法的有效性，并达到领先水平。实验结果表明，该方法在图像到3D转换任务上取得了优异的性能，验证了方法的有效性和先进性。</p></li></ol><p>请注意，由于缺少具体的GitHub链接和论文详细内容，部分信息可能无法准确填写。如有需要，请进一步提供详细信息。</p><ol><li>方法论概述：</li></ol><p>本文提出了一种基于大视点至高斯模型的图像到3D生成方法，具体步骤包括：</p><p>(1) 背景介绍：首先概述了基于大重建模型的图像到3D转换技术的研究背景，尤其是3D高斯重建模型的研究现状和挑战。</p><p>(2) 研究方法：针对缺乏3D先验信息的情况下，通过引入点云信息提高图像到3D的生成质量的问题，提出了一种新的图像到3D生成方法。</p><p>(3) 数据准备：使用预训练的3D扩散模型生成初始点云，并结合图像特征，通过注意力机制、投影机制和点特征提取（APP块）融合点云特征。这一步的目的是利用图像条件进一步增强几何和纹理特征。</p><p>(4) 实验验证：在GSO和Objaverse数据集上进行实验，展示所提出方法的有效性，并达到领先水平。实验结果表明，该方法在图像到3D转换任务上取得了优异的性能，验证了方法的有效性和先进性。在这个过程中，还介绍了损失函数和数据增强方法的优化。</p><p>具体的操作细节和技术实现方式如下：</p><p>(1) 对现有的图像到3D转换方法进行改进，引入点云信息作为初始的3D几何先验，显著促进了图像到3D的生成过程。这种方法的引入是通过一种叫做Point to Gaussian Generator的模型实现的。该模型以稀疏点云和配对图像为输入，输出3D高斯表示。该模型采用编码器-解码器结构，将点云转换为3D高斯。为了提高性能，使用预训练的扩散模型生成的点云进行初始化，并通过上采样操作增加点的数量。此外，还结合了条件图像来丰富高斯特征。解码器采用多头高斯解码器，将特征解码为高斯参数用于拼贴。为了简化学习过程，还引入了位置偏移学习机制。另外为了有效地提取多尺度点云特征并丰富高斯属性引入了投影和注意力机制用于跨模态增强。这一部分是模型的核心部分之一。除此之外还包括了点云上采样器以及多尺度高斯解码器。至于涉及到的技术细节如点云特征提取等则参考了现有的研究如PVCNN等模型的技术实现方式。总的来说这个模型融合了图像处理和点云处理的最新技术来达成图像到三维模型的转换目标并提供了高效可行的技术实现路径以及相对以往方法具有显著优越性的性能表现证明了其有效性及先进性。</p><ol><li>Conclusion:</li></ol><ul><li><p>(1)这项工作的意义在于推动了图像到3D转换技术的发展，特别是在缺乏3D先验信息的情况下，通过引入点云信息提高了图像到3D的生成质量。它为计算机视觉和图形学领域提供了一种新的图像到三维模型转换的方法，具有广泛的应用前景。</p></li><li><p>(2)创新点：本文提出了一种基于大视点至高斯模型的图像到3D生成方法，通过引入点云信息作为初始的3D几何先验，显著提高了图像到3D的生成质量。同时，文章还设计了新的模型结构和技术实现方式，如Point to Gaussian Generator模型、APP块等，提高了模型的性能和效率。<br>性能：本文方法在GSO和Objaverse数据集上进行实验，展示了所提出方法的有效性，并达到了领先水平。实验结果表明，该方法在图像到3D转换任务上取得了优异的性能，验证了方法的有效性和先进性。<br>工作量：本文不仅提出了创新性的算法和方法，还进行了大量的实验验证和性能评估，同时涉及到模型的实现和优化，工作量较大。</p></li></ul><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-4438af3ff62960055fd7154f2bf90075.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-417ffbd7a90005044d9e9a51b2e45c85.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-1cbf3a5101ca3b2f06d6a7d7e9dd16c7.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-29f5627c96df52cb771a6f2d0e3b473e.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-0e205528a76e4e76ef7882cca4b62991.jpg" align="middle"></details><h2 id="Learning-Part-aware-3D-Representations-by-Fusing-2D-Gaussians-and-Superquadrics"><a href="#Learning-Part-aware-3D-Representations-by-Fusing-2D-Gaussians-and-Superquadrics" class="headerlink" title="Learning Part-aware 3D Representations by Fusing 2D Gaussians and   Superquadrics"></a>Learning Part-aware 3D Representations by Fusing 2D Gaussians and Superquadrics</h2><p><strong>Authors:Zhirui Gao, Renjiao Yi, Yuhang Huang, Wei Chen, Chenyang Zhu, Kai Xu</strong></p><p>Low-level 3D representations, such as point clouds, meshes, NeRFs, and 3D Gaussians, are commonly used to represent 3D objects or scenes. However, humans usually perceive 3D objects or scenes at a higher level as a composition of parts or structures rather than points or voxels. Representing 3D as semantic parts can benefit further understanding and applications. We aim to solve part-aware 3D reconstruction, which parses objects or scenes into semantic parts. In this paper, we introduce a hybrid representation of superquadrics and 2D Gaussians, trying to dig 3D structural clues from multi-view image inputs. Accurate structured geometry reconstruction and high-quality rendering are achieved at the same time. We incorporate parametric superquadrics in mesh forms into 2D Gaussians by attaching Gaussian centers to faces in meshes. During the training, superquadrics parameters are iteratively optimized, and Gaussians are deformed accordingly, resulting in an efficient hybrid representation. On the one hand, this hybrid representation inherits the advantage of superquadrics to represent different shape primitives, supporting flexible part decomposition of scenes. On the other hand, 2D Gaussians are incorporated to model the complex texture and geometry details, ensuring high-quality rendering and geometry reconstruction. The reconstruction is fully unsupervised. We conduct extensive experiments on data from DTU and ShapeNet datasets, in which the method decomposes scenes into reasonable parts, outperforming existing state-of-the-art approaches.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.10789v1">PDF</a></p><p><strong>Summary</strong><br>提出了一种混合表示方法，结合超椭球体和二维高斯模型，以提升三维结构重建和渲染质量。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了超椭球体和二维高斯模型的混合表示方法。</li><li>方法支持将场景分解为语义部分，有利于进一步理解和应用。</li><li>通过将高斯中心附加到网格面，将超椭球体参数化到网格形式中。</li><li>实现了准确的结构几何重建和高质量渲染。</li><li>二维高斯模型用于复杂纹理和几何细节的建模。</li><li>提出的重建方法完全无监督，适用于不同数据集。</li><li>在DTU和ShapeNet数据集上进行了大量实验，显示出优于现有方法的效果。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>好的，我会按照您的要求来总结这篇论文。</p><ol><li><p>标题：学习部分感知的3D表示</p></li><li><p>作者：高志瑞、易仁娇、黄玉航、陈炜、朱晨曦、徐凯。*（注：这里按照您的格式保留了英文名字）</p></li><li><p>隶属机构：国防科技大学计算机学院。*（注：这里是对作者隶属机构的中文翻译）</p></li><li><p>关键词：部分感知重建、混合表示、二维高斯、超级二次曲面。*（注：关键词按照英文原样保留）</p></li><li><p>Urls：文章链接（具体链接需要提供），代码链接（如有）：Github:None（如没有提供代码链接）</p></li><li><p>总结：</p><ul><li><p>(1)研究背景：本文的研究背景是关于从多视角图像中学习部分感知的3D重建。大多数现有的3D重建方法产生的表示与人类感知场景的方式不同，人类更倾向于将场景理解为不同的语义部分。因此，本文旨在学习一种能够分解场景为不同语义部分或形状的方法。</p></li><li><p>(2)过去的方法及问题：先前的方法主要依赖于低级别的3D表示，如点云、网格、NeRF等，但它们在处理复杂的形状和纹理时存在问题。此外，尽管一些方法尝试通过NeRF进行部分感知对象的重建，但它们难以准确保留几何形状，这限制了它们在现实场景中的应用。</p></li><li><p>(3)研究方法：本文提出了一种混合表示方法，融合了超级二次曲面和二维高斯，以从多视角图像中提取3D结构线索。该方法结合了超级二次曲面的灵活形状表示能力和二维高斯对复杂纹理和几何细节的建模能力。通过优化超级二次曲面的参数和高斯变形，实现了高效的部分感知3D重建。</p></li><li><p>(4)任务与性能：本文在DTU和ShapeNet数据集上进行了实验，验证了该方法能够合理地将场景分解为部分，并表现出优于现有先进方法的效果。实验结果表明，该方法能够实现高质量的部分感知重建，为后续的任务如场景操作/编辑、场景图生成等提供了更准确的表示。性能结果支持了该方法的目标。<br>好的，根据您给出的信息，我将对文章的方法部分进行详细总结。以下是以中文形式给出的回答：</p></li></ul></li><li><p>方法：</p></li></ol><p>（1）研究背景与动机：文章针对多视角图像中的部分感知3D重建问题展开研究。由于大多数现有3D重建方法产生的表示与人类感知场景的方式不同，本文旨在学习一种能够分解场景为不同语义部分或形状的方法。</p><p>（2）过去的方法及问题：先前的方法主要依赖低级别的3D表示，如点云、网格、NeRF等，但在处理复杂的形状和纹理时存在问题。尽管一些方法尝试通过NeRF进行部分感知对象的重建，但它们难以准确保留几何形状，限制了其在现实场景中的应用。</p><p>（3）研究方法介绍：</p><ul><li>融合超级二次曲面和二维高斯进行混合表示：文章提出了一种混合表示方法，融合了超级二次曲面和二维高斯。超级二次曲面具有灵活的形状表示能力，而二维高斯能建模复杂纹理和几何细节。</li><li>从多视角图像中提取3D结构线索：通过优化超级二次曲面的参数和高斯变形，结合多视角图像信息，实现高效的部分感知3D重建。</li></ul><p>（4）实验验证：文章在DTU和ShapeNet数据集上进行了实验验证。实验结果表明，该方法能够实现高质量的部分感知重建，为后续任务如场景操作/编辑、场景图生成等提供更准确的表示。性能结果支持了该方法的目标。此外，该方法在实验中展示出了优于现有先进方法的效果。</p><p>注意：以上总结基于您提供的信息进行概括，具体内容可能需要根据实际论文内容进行补充和调整。</p><ol><li>结论：</li></ol><p>(1)重要性：该工作针对多视角图像中的部分感知3D重建问题进行了深入研究，提出了一种混合表示方法，融合了超级二次曲面和二维高斯，以从多视角图像中提取3D结构线索。该研究对于计算机视觉和图形学领域具有重要的理论和实践意义，有助于提高3D重建的准确性和质量。</p><p>(2)创新点、性能和工作量：<br>创新点：文章提出了一种新的混合表示方法，结合了超级二次曲面和二维高斯，能够更有效地进行部分感知的3D重建。<br>性能：文章在DTU和ShapeNet数据集上进行了实验验证，并与其他先进方法进行了比较，实验结果表明该方法在部分感知重建任务上取得了优异性能。<br>工作量：文章对部分感知的3D重建问题进行了全面的研究，包括方法设计、实验验证和性能评估等。然而，文章未提及背景场景的建模，未来工作可以探索如何对背景进行建模，并扩展到处理整个复杂场景。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-50980715c8e40f641a46157d2bc4c30d.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-db005d6f7c82d0989b5ba25dcd32b5a9.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-8849bacfa31cad2cde282450aa71e051.jpg" align="middle"></details><h2 id="CHASE-3D-Consistent-Human-Avatars-with-Sparse-Inputs-via-Gaussian-Splatting-and-Contrastive-Learning"><a href="#CHASE-3D-Consistent-Human-Avatars-with-Sparse-Inputs-via-Gaussian-Splatting-and-Contrastive-Learning" class="headerlink" title="CHASE: 3D-Consistent Human Avatars with Sparse Inputs via Gaussian   Splatting and Contrastive Learning"></a>CHASE: 3D-Consistent Human Avatars with Sparse Inputs via Gaussian Splatting and Contrastive Learning</h2><p><strong>Authors:Haoyu Zhao, Hao Wang, Chen Yang, Wei Shen</strong></p><p>Recent advancements in human avatar synthesis have utilized radiance fields to reconstruct photo-realistic animatable human avatars. However, both NeRFs-based and 3DGS-based methods struggle with maintaining 3D consistency and exhibit suboptimal detail reconstruction, especially with sparse inputs. To address this challenge, we propose CHASE, which introduces supervision from intrinsic 3D consistency across poses and 3D geometry contrastive learning, achieving performance comparable with sparse inputs to that with full inputs. Following previous work, we first integrate a skeleton-driven rigid deformation and a non-rigid cloth dynamics deformation to coordinate the movements of individual Gaussians during animation, reconstructing basic avatar with coarse 3D consistency. To improve 3D consistency under sparse inputs, we design Dynamic Avatar Adjustment(DAA) to adjust deformed Gaussians based on a selected similar pose/image from the dataset. Minimizing the difference between the image rendered by adjusted Gaussians and the image with the similar pose serves as an additional form of supervision for avatar. Furthermore, we propose a 3D geometry contrastive learning strategy to maintain the 3D global consistency of generated avatars. Though CHASE is designed for sparse inputs, it surprisingly outperforms current SOTA methods \textbf{in both full and sparse settings} on the ZJU-MoCap and H36M datasets, demonstrating that our CHASE successfully maintains avatar’s 3D consistency, hence improving rendering quality.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.09663v2">PDF</a> 13 pages, 6 figures</p><p><strong>Summary</strong><br>利用辐射场重建逼真且可动的人类头像，尤其在稀疏输入条件下，新方法CHASE通过3D一致性监督和几何对比学习显著提升重建质量。</p><p><strong>Key Takeaways</strong></p><ul><li>辐射场技术用于重建逼真可动的人类头像。</li><li>NeRFs和3DGS方法在保持3D一致性和重建细节方面存在挑战。</li><li>CHASE方法结合骨架驱动刚性变形和非刚性布料动力学，实现基础头像的重建。</li><li>动态头像调整（DAA）用于稀疏输入条件下的3D一致性改进。</li><li>引入3D几何对比学习以维持生成头像的全局一致性。</li><li>CHASE在ZJU-MoCap和H36M数据集上表现优越，即使在稀疏输入下也超越了当前的技术水平。</li><li>该研究展示了CHASE如何提升头像的3D一致性和渲染质量。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>好的，我会按照您的要求进行回答。</p><ol><li>Title: CHASE：基于稀疏输入的3D一致人形化身合成</li><li><p>Authors: 赵浩宇<em>, 王浩</em>, 杨晨*, 沈炜†</p></li><li><p>Affiliation:<br>赵浩宇, 王浩, 杨晨分别来自武汉大学和华中科技大学，沈炜来自上海交通大学。</p></li><li><p>Keywords: 人形化身合成、稀疏输入、高斯平铺、对比学习、3D一致性</p></li><li><p>Urls:<br>论文链接: <a href="论文链接地址">论文链接地址</a><br>GitHub代码链接: <a href="GitHub链接地址">GitHub链接地址</a>（如果可用，请填写；如果不可用，填写“None”）</p></li><li><p>Summary:</p></li></ol><p>(1)研究背景：<br>随着计算机图形学的发展，创建高质量、具有动态表现的人形化身在多个领域（如AR/VR、视觉特效、电影制作等）具有广泛应用。近期，尽管基于神经辐射场（NeRF）的方法在创建3D人形化身方面取得了显著进展，但在稀疏输入条件下，它们面临3D一致性维护和细节重建的挑战。本文的研究旨在解决这一问题。</p><p>(2)过去的方法及问题：<br>早期的方法主要依赖于多相机设置来捕捉高质量的数据，这需要大量的计算和手动工作。虽然这些方法在创建单一场景/对象时效果很好，但在面对新的场景/对象仅有少量样本时则面临挑战。近期的方法尝试使用NeRF或3DGS等方法建模3D人形化身，但它们在维持3D一致性和重建细节方面仍有不足，尤其在稀疏输入条件下。</p><p>(3)研究方法：<br>本研究提出了一种名为CHASE的方法，通过结合骨架驱动的刚性变形和非刚性布料动态变形，初步建立具有粗略3D一致性的人形化身。为提高稀疏输入下的3D一致性，研究设计了动态化身调整（DAA）策略，通过选择相似的姿势/图像来调整变形的高斯单元。此外，研究还提出了一种3D几何对比学习策略，以维持生成化身的3D全局一致性。</p><p>(4)任务与性能：<br>本研究在ZJU-MoCap和H36M数据集上进行了实验，结果显示CHASE方法在稀疏输入条件下仍能取得良好的性能，甚至在某些情况下超过了现有先进方法的全输入性能。这表明CHASE方法成功维持了化身的3D一致性，从而提高了渲染质量。性能支持了研究目标的实现。<br>好的，接下来我会详细阐述这篇论文的方法论思想。以下是我理解的具体步骤：</p><ol><li>方法论：</li></ol><p>（1）研究背景与问题定义：论文主要解决在稀疏输入条件下创建高质量、具有动态表现的3D人形化身的问题。现有的方法在面对新的场景/对象仅有少量样本时面临挑战，尤其是在维持3D一致性和重建细节方面。</p><p>（2）研究方法概述：本研究提出了一种名为CHASE的方法，通过结合骨架驱动的刚性变形和非刚性布料动态变形，初步建立具有粗略3D一致性的人形化身。为提高稀疏输入下的3D一致性，研究设计了动态化身调整（DAA）策略，通过选择相似的姿势/图像来调整变形的高斯单元。此外，研究还提出了一种3D几何对比学习策略，以维持生成化身的3D全局一致性。</p><p>（3）具体技术流程：</p><ul><li>数据准备：包括从单目视频获取的图片、SMPL参数以及图像的前景掩膜。</li><li>3D高斯优化与变形：在规范空间中对3D高斯进行优化，然后通过变形匹配观察空间并进行渲染。变形过程结合了刚性的骨架运动和非刚性的布料变形。</li><li>动态化身调整（DAA）：针对极端稀疏输入，利用人形化身在不同姿势/图像间的内在3D一致性，通过选择相似的姿势/图像来调整变形的高斯单元，进一步提高3D一致性。</li><li>3D几何对比学习：采用3D几何对比学习确保动画过程中的3D一致性。将3D高斯看作3D点云，使用DGCNN提取特征，并通过对比学习维持3D一致性。</li></ul><p>（4）实验验证：在ZJU-MoCap和H36M数据集上的实验结果表明，CHASE方法在稀疏输入条件下仍能取得良好的性能，甚至在某些情况下超过了现有先进方法的全输入性能，证明了CHASE方法成功维持了化身的3D一致性，提高了渲染质量。</p><p>以上就是对该论文方法论思想的详细阐述。</p><ol><li>Conclusion:</li></ol><p>(1)工作意义：<br>该研究对于计算机图形学领域具有重要的推动作用，特别是在创建高质量、具有动态表现的3D人形化身方面。其研究成果有望应用于多个领域，如AR/VR、视觉特效、电影制作等，对于提升这些领域的视觉效果和技术进步具有重要意义。</p><p>(2)创新点、性能、工作量评价：<br>创新点：论文提出了一种名为CHASE的方法，通过结合骨架驱动的刚性变形和非刚性布料动态变形，建立具有粗略3D一致性的人形化身。同时，论文还设计了动态化身调整（DAA）策略和3D几何对比学习策略，以提高稀疏输入下的3D一致性和维持生成的化身的3D全局一致性。<br>性能：论文在ZJU-MoCap和H36M数据集上进行了实验验证，结果表明CHASE方法在稀疏输入条件下仍能取得良好的性能，甚至在某些情况下超过了现有先进方法的全输入性能。这证明了CHASE方法在维持化身的3D一致性方面的成功。<br>工作量：论文进行了大量的实验和验证工作，包括数据集准备、实验设计、结果分析和对比等。此外，论文还详细介绍了方法论的详细步骤和技术流程，显示出研究团队对于该工作的投入和精细的工作。</p><p>总体而言，该论文在创新点、性能和工作量方面都表现出了一定的优势，对于推动计算机图形学领域的发展具有一定的意义。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-20792050bb488ed224cbedbc40247c7d.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-060b38ccf8897d85a83f1eef91ce6c1b.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-3d3dca898a7edd9f20d2ba3cda712423.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-af62178f5fdd22828fd6edb951afcb8c.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-5456bb2bf3dabbd73a53ce6f04593b9a.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-8c68f49b04c0a784781a9f795f4373ae.jpg" align="middle"></details><h2 id="Gaussian-in-the-Dark-Real-Time-View-Synthesis-From-Inconsistent-Dark-Images-Using-Gaussian-Splatting"><a href="#Gaussian-in-the-Dark-Real-Time-View-Synthesis-From-Inconsistent-Dark-Images-Using-Gaussian-Splatting" class="headerlink" title="Gaussian in the Dark: Real-Time View Synthesis From Inconsistent Dark   Images Using Gaussian Splatting"></a>Gaussian in the Dark: Real-Time View Synthesis From Inconsistent Dark Images Using Gaussian Splatting</h2><p><strong>Authors:Sheng Ye, Zhen-Hui Dong, Yubin Hu, Yu-Hui Wen, Yong-Jin Liu</strong></p><p>3D Gaussian Splatting has recently emerged as a powerful representation that can synthesize remarkable novel views using consistent multi-view images as input. However, we notice that images captured in dark environments where the scenes are not fully illuminated can exhibit considerable brightness variations and multi-view inconsistency, which poses great challenges to 3D Gaussian Splatting and severely degrades its performance. To tackle this problem, we propose Gaussian-DK. Observing that inconsistencies are mainly caused by camera imaging, we represent a consistent radiance field of the physical world using a set of anisotropic 3D Gaussians, and design a camera response module to compensate for multi-view inconsistencies. We also introduce a step-based gradient scaling strategy to constrain Gaussians near the camera, which turn out to be floaters, from splitting and cloning. Experiments on our proposed benchmark dataset demonstrate that Gaussian-DK produces high-quality renderings without ghosting and floater artifacts and significantly outperforms existing methods. Furthermore, we can also synthesize light-up images by controlling exposure levels that clearly show details in shadow areas.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.09130v2">PDF</a> accepted by PG 2024</p><p><strong>Summary</strong><br>高斯混合喷涂（3D Gaussian Splatting）作为一种强大的表示形式，能够使用一致的多视图图像合成出色的新视角，但在光线不足的环境下，图像亮度变化和多视图不一致性严重影响其性能。本文提出了Gaussian-DK方法，通过设计相机响应模块和梯度缩放策略来解决这些挑战，并在实验中展示其优越性能。</p><p><strong>Key Takeaways</strong></p><ul><li>高斯混合喷涂能够使用多视图图像合成新视角。</li><li>光线不足的环境中，图像可能会出现明显的亮度变化和多视图不一致性。</li><li>Gaussian-DK方法通过引入相机响应模块和梯度缩放策略来解决多视图不一致性问题。</li><li>该方法采用各向异性的3D高斯函数表示物理世界的一致辐射场。</li><li>引入的梯度缩放策略有助于约束近相机的高斯函数，防止出现漂浮和克隆问题。</li><li>Gaussian-DK在实验中表现出高质量的渲染效果，避免了幽灵和漂浮物等伪影现象。</li><li>方法还能够通过控制曝光水平合成明亮区域的细节图像。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>好的，我将会按照您提供的格式来总结这篇论文。</p><ol><li><p><strong>标题</strong>：基于不一致暗图像的实时视图合成研究（Gaussian in the Dark: Real-Time View Synthesis From Inconsistent Dark Images Using Gaussian Splatting）。</p></li><li><p><strong>作者</strong>：R. Chen, T. Ritschel, E. Whiting（主编）以及Sheng Ye, Zhen-Hui Dong, Yubin Hu等。</p></li><li><p><strong>作者隶属</strong>：MOE-Pervasive Computing重点实验室，清华大学计算机科学与技术系。</p></li><li><p><strong>关键词</strong>：视图合成、暗图像、高斯插值、不一致性处理、计算机图形学。</p></li><li><p><strong>链接</strong>：论文链接（待补充），GitHub代码链接（待补充）。</p></li><li><p><strong>摘要</strong>：</p><p><em>(1) 研究背景</em>：随着计算机视觉和计算机图形学的发展，视图合成已成为一项重要任务。现有方法在光照良好的环境下表现良好，但在暗环境或低光照条件下，由于亮度变化和视图不一致性，性能会受到严重影响。本文研究如何在暗环境下进行高质量视图合成。</p><p><em>(2) 相关工作与问题</em>：现有的视图合成方法主要基于一致的多视图图像作为输入。但在暗环境下，由于场景照明不足和相机成像差异，图像往往存在不一致性。这使得基于高斯插值的方法面临挑战，性能严重下降。因此，需要一种能够处理暗环境和不一致性的新方法。</p><p><em>(3) 研究方法</em>：针对上述问题，本文提出了基于高斯插值的Dark合成方法（Gaussian-DK）。该方法通过观察不一致性主要由相机成像引起，使用一组各向异性的三维高斯来代表物理世界的连续辐射场。设计了一个相机响应模块来补偿多视图的不一致性，并引入了一种基于梯度的缩放策略来约束接近相机的浮点数高斯。</p><p><em>(4) 任务与性能</em>：实验结果表明，Gaussian-DK在提出的基准数据集上能生成高质量渲染，无鬼影和浮点数伪影，并显著优于现有方法。此外，通过控制曝光水平，还能合成展现阴影细节的光照图像。总体而言，该方法的性能支持了其解决暗环境下视图合成的目标。</p></li></ol><p>希望这个摘要符合您的要求！如果有任何需要修改或改进的地方，请告诉我。</p><ol><li>方法论概述：</li></ol><p>本文提出了基于高斯插值的Dark合成方法（Gaussian-DK），旨在解决在暗环境下视图合成的问题。具体的方法论如下：</p><ul><li>(1) 研究背景分析：针对现有视图合成方法在暗环境下性能下降的问题，提出一种能够处理暗环境和不一致性的新方法。</li><li>(2) 方法设计：使用一组各向异性的三维高斯来代表物理世界的连续辐射场。设计了一个相机响应模块来补偿多视图的不一致性，并引入了一种基于梯度的缩放策略来约束接近相机的浮点数高斯。</li><li>(3) 相机响应建模：通过引入曝光水平作为主要的亮度条件，确定了光栅化二维辐射映射的整体亮度。此外，还设计了可学习的光特征精炼和色调映射函数，以精细控制亮度并保留丰富的纹理。</li><li>(4) 高斯光特征引入：为了模拟不同位置对同一曝光水平的不同响应，为每个高斯附加了一个可学习的光特征向量。结合光特征和曝光水平，实现了细粒度的亮度控制。</li><li>(5) 色调映射过程优化：为了优化数值稳定性和提高渲染质量，将全部辐射值转换为对数域进行处理，并采用轻量级的通道级卷积神经网络进行色调映射。</li><li>(6) 浮点数去除策略：尽管相机响应模块可以解决大多数由不一致性引起的问题，但仍然存在一些浮点数伪影。为此，文章提出了一种浮点数去除策略，以提高渲染质量。</li></ul><p>该方法综合了计算机视觉和计算机图形学的技术，旨在解决在暗环境下进行高质量视图合成的难题。</p><ol><li>Conclusion:</li></ol><ul><li>(1)这篇工作的意义在于解决暗环境下视图合成的问题，提出了一种基于高斯插值的Dark合成方法（Gaussian-DK），能够在暗环境下进行高质量视图合成，对于计算机视觉和计算机图形学领域具有十分重要的意义。</li><li>(2)创新点：文章提出了基于高斯插值的Dark合成方法，通过引入相机响应模块和基于梯度的缩放策略，解决了暗环境下视图合成的不一致性问题和性能下降的问题。</li><li>性能：实验结果表明，Gaussian-DK在提出的基准数据集上能生成高质量渲染，无鬼影和浮点数伪影，显著优于现有方法。此外，通过控制曝光水平，还能合成展现阴影细节的光照图像。</li><li>工作量：文章进行了大量的实验和数据分析，证明了所提出方法的有效性和优越性。此外，文章还收集了一个新的具有挑战性的数据集，包含了12个真实场景，为相关研究提供了有价值的资源。</li></ul><p>希望这个总结符合您的要求！如有任何需要修改或改进的地方，请告诉我。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-6ee18dbf49e26ebda40420ea6e0f3b17.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-67927cd71eaa8acc2d1a34d80afe62e1.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-d21fe236c4dad10202a55b404d85041f.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-7540304f0b025f494a11202be37b575d.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-5d500ebdda88ccf9b9fc8fdd3ed55fa3.jpg" align="middle"></details><blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-08-22-更新-1"><a href="#2024-08-22-更新-1" class="headerlink" title="2024-08-22 更新"></a>2024-08-22 更新</h1><h2 id="Large-Point-to-Gaussian-Model-for-Image-to-3D-Generation-1"><a href="#Large-Point-to-Gaussian-Model-for-Image-to-3D-Generation-1" class="headerlink" title="Large Point-to-Gaussian Model for Image-to-3D Generation"></a>Large Point-to-Gaussian Model for Image-to-3D Generation</h2><p><strong>Authors:Longfei Lu, Huachen Gao, Tao Dai, Yaohua Zha, Zhi Hou, Junta Wu, Shu-Tao Xia</strong></p><p>Recently, image-to-3D approaches have significantly advanced the generation quality and speed of 3D assets based on large reconstruction models, particularly 3D Gaussian reconstruction models. Existing large 3D Gaussian models directly map 2D image to 3D Gaussian parameters, while regressing 2D image to 3D Gaussian representations is challenging without 3D priors. In this paper, we propose a large Point-to-Gaussian model, that inputs the initial point cloud produced from large 3D diffusion model conditional on 2D image to generate the Gaussian parameters, for image-to-3D generation. The point cloud provides initial 3D geometry prior for Gaussian generation, thus significantly facilitating image-to-3D Generation. Moreover, we present the \textbf{A}ttention mechanism, \textbf{P}rojection mechanism, and \textbf{P}oint feature extractor, dubbed as \textbf{APP} block, for fusing the image features with point cloud features. The qualitative and quantitative experiments extensively demonstrate the effectiveness of the proposed approach on GSO and Objaverse datasets, and show the proposed method achieves state-of-the-art performance.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.10935v1">PDF</a> 10 pages, 9 figures, ACM MM 2024</p><p><strong>Summary</strong><br>最近，基于大型重建模型的图像到3D方法显著提升了生成质量和速度，特别是基于3D高斯重建模型。</p><p><strong>Key Takeaways</strong></p><ul><li>图像到3D方法的进步主要集中在使用大型重建模型生成高质量和快速的3D资产上。</li><li>大型3D高斯模型直接将2D图像映射到3D高斯参数。</li><li>在没有3D先验的情况下，将2D图像回归到3D高斯表示是具有挑战性的。</li><li>提出了大型点云到高斯模型，用于在图像到3D生成中输入由大型3D扩散模型生成的初始点云，以生成高斯参数。</li><li>点云为高斯生成提供了初始3D几何先验，显著促进了图像到3D生成。</li><li>提出了APP（Attention、Projection、Point feature extractor）块，用于融合图像特征和点云特征。</li><li>实验证明了该方法在GSO和Objaverse数据集上的有效性，并展示其达到了当前最先进的性能水平。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>好的，我已经理解您的要求，我将按照您提供的格式来总结这篇论文。</p><ol><li><p>标题：基于点云到高斯模型的大规模图像到三维生成研究</p></li><li><p>作者：Longfei Lu, Huachen Gao, Tao Dai, Yaohua Zha, Zhi Hou, Junta Wu, ShuTao Xia。</p></li><li><p>所属机构：该研究团队的成员来自于清华大学深圳国际研究生院、腾讯等公司以及深圳大学等。</p></li><li><p>关键词：三维生成、三维高斯拼贴、单视图重建、点云。</p></li><li><p>Urls：文章相关信息以及代码等可通过以下链接查看，论文链接：<a target="_blank" rel="noopener" href="https://www.example.com">论文链接</a>，GitHub代码仓库（如果有的话）：Github:None（如果没有代码仓库，则不填写）。</p></li><li><p>总结：</p><ul><li><p>(1)研究背景：随着三维重建技术的不断发展，基于图像的快速高质量三维重建已成为当前研究的热点。该文主要探讨了在图像到三维生成领域中的最新进展和挑战。</p></li><li><p>(2)过去的方法及存在的问题：现有的大型三维高斯模型直接将二维图像映射到三维高斯参数，但在没有三维先验信息的情况下，从二维图像回归到三维高斯表示仍具有挑战性。因此，需要一种新的方法来解决这一问题。</p></li><li><p>(3)研究方法：针对上述问题，本文提出了一种基于点云到高斯模型的大规模图像到三维生成方法。该方法首先通过大型三维扩散模型生成初始点云，然后将其作为输入，生成高斯参数，从而实现图像到三维的生成。此外，还引入了注意力机制、投影机制和点特征提取器（即APP块），以融合图像特征和点云特征，进一步提高生成质量。</p></li><li><p>(4)任务与性能：本文在GSO和Objaverse数据集上进行了广泛的实验，证明了所提出方法的有效性，并达到了当前最佳性能。实验结果表明，该方法在图像到三维生成任务中具有良好的表现，可以有效地支持其目标。</p></li></ul></li></ol><p>希望这个总结符合您的要求！</p><ol><li><p>方法论：</p><ul><li><p>(1) 研究者首先介绍了背景知识，指出随着三维重建技术的不断发展，基于图像的快速高质量三维重建已成为当前研究的热点。现有的大型三维高斯模型直接将二维图像映射到三维高斯参数，但在没有三维先验信息的情况下，从二维图像回归到三维高斯表示仍具有挑战性。</p></li><li><p>(2) 针对上述问题，文章提出了一种基于点云到高斯模型的大规模图像到三维生成方法。该方法的核心思想是利用大型三维扩散模型生成初始点云，然后将其作为输入，生成高斯参数，从而实现图像到三维的生成。</p></li><li><p>(3) 为了进一步提高生成质量，文章引入了注意力机制、投影机制和点特征提取器（即APP块），以融合图像特征和点云特征。首先，通过点云上采样器增加点云的数量，然后通过编码器提取多尺度点云特征。在每个块中，都包含了点特征提取器、投影和注意力，以增强跨模态特征。</p></li><li><p>(4) 文章的模型还采用了多线性头解码器，将特征解码为高斯参数，用于渲染新型视图。为了提高学习效率，模型还采用了姿态感知投影、交叉注意力和采样分组等技术。</p></li><li><p>(5) 在实验部分，文章在GSO和Objaverse数据集上进行了广泛的实验，证明了所提出方法的有效性，并达到了当前最佳性能。实验结果表明，该方法在图像到三维生成任务中具有良好的表现，可以有效地支持其目标。</p></li></ul></li></ol><ol><li>Conclusion:</li></ol><p>(1)这篇工作的意义在于提出了一种基于点云到高斯模型的大规模图像到三维生成方法，这对于图像到三维重建领域具有重要的推动作用，有助于推动三维重建技术的快速发展。</p><p>(2)创新点：文章提出了一个全新的基于点云到高斯模型的方法来解决图像到三维生成的问题，创新性较强。引入了注意力机制、投影机制和点特征提取器（APP块）来融合图像特征和点云特征，提高了生成质量。性能：文章在GSO和Objaverse数据集上进行了广泛的实验，证明了所提出方法的有效性，并达到了当前最佳性能。显示了该方法在图像到三维生成任务中的良好表现。工作量：文章对图像到三维生成的问题进行了深入的研究，不仅提出了全新的方法，还进行了大量的实验验证，工作量较大。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-4438af3ff62960055fd7154f2bf90075.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-417ffbd7a90005044d9e9a51b2e45c85.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-1cbf3a5101ca3b2f06d6a7d7e9dd16c7.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-29f5627c96df52cb771a6f2d0e3b473e.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-0e205528a76e4e76ef7882cca4b62991.jpg" align="middle"></details><h2 id="Learning-Part-aware-3D-Representations-by-Fusing-2D-Gaussians-and-Superquadrics-1"><a href="#Learning-Part-aware-3D-Representations-by-Fusing-2D-Gaussians-and-Superquadrics-1" class="headerlink" title="Learning Part-aware 3D Representations by Fusing 2D Gaussians and   Superquadrics"></a>Learning Part-aware 3D Representations by Fusing 2D Gaussians and Superquadrics</h2><p><strong>Authors:Zhirui Gao, Renjiao Yi, Yuhang Huang, Wei Chen, Chenyang Zhu, Kai Xu</strong></p><p>Low-level 3D representations, such as point clouds, meshes, NeRFs, and 3D Gaussians, are commonly used to represent 3D objects or scenes. However, humans usually perceive 3D objects or scenes at a higher level as a composition of parts or structures rather than points or voxels. Representing 3D as semantic parts can benefit further understanding and applications. We aim to solve part-aware 3D reconstruction, which parses objects or scenes into semantic parts. In this paper, we introduce a hybrid representation of superquadrics and 2D Gaussians, trying to dig 3D structural clues from multi-view image inputs. Accurate structured geometry reconstruction and high-quality rendering are achieved at the same time. We incorporate parametric superquadrics in mesh forms into 2D Gaussians by attaching Gaussian centers to faces in meshes. During the training, superquadrics parameters are iteratively optimized, and Gaussians are deformed accordingly, resulting in an efficient hybrid representation. On the one hand, this hybrid representation inherits the advantage of superquadrics to represent different shape primitives, supporting flexible part decomposition of scenes. On the other hand, 2D Gaussians are incorporated to model the complex texture and geometry details, ensuring high-quality rendering and geometry reconstruction. The reconstruction is fully unsupervised. We conduct extensive experiments on data from DTU and ShapeNet datasets, in which the method decomposes scenes into reasonable parts, outperforming existing state-of-the-art approaches.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.10789v1">PDF</a></p><p><strong>Summary</strong><br>通过结合超二次曲面和二维高斯模型，本文提出了一种新的混合表示方法，以实现对多视图输入的结构化几何重建和高质量渲染。</p><p><strong>Key Takeaways</strong></p><ul><li>使用超二次曲面和二维高斯模型的混合表示，实现了对复杂纹理和几何细节的高质量重建和渲染。</li><li>方法在无监督条件下进行重建，使用DTU和ShapeNet数据集验证了其有效性。</li><li>超二次曲面和二维高斯模型的结合能够灵活地分解场景，优于现有的先进方法。</li><li>通过参数化超二次曲面，并将高斯中心附加到网格面上，实现了高效的混合表示。</li><li>研究旨在解决基于部件的三维重建问题，将对象或场景解析为语义部件。</li><li>新方法能够在保持准确结构几何重建的同时，实现高质量的渲染效果。</li><li>通过优化超二次曲面的参数和相应地变形高斯模型，提高了重建的效率和质量。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>好的，我会按照您的要求来整理这篇文章。以下是该论文的简要介绍和概括：</p><p>标题：学习感知三维表示（Learning Part-aware 3D Representations）</p><p>作者：高志瑞（Zhirui Gao）、易仁娇（Renjiao Yi）、黄宇航（Yuhang Huang）、陈威（Wei Chen）、朱晨阳（Chenyang Zhu）、徐凯（Kai Xu）等。</p><p>所属机构：国防科技大学计算机学院，中国长沙。</p><p>关键词：感知重建、混合表示、二维高斯、超级曲面。</p><p>论文链接和GitHub代码链接：论文链接（请提供具体链接），GitHub代码链接（如果可用，请提供具体链接；如果不可用，填写GitHub:None）。</p><p>摘要：</p><p>一、研究背景</p><p>三维重建是计算机视觉和计算机图形学中长期研究的问题。大多数重建的场景是低层次的表示，如点云、体素或网格，与人类感知不同。人类理解三维场景或物体为不同的语义部分，因此，学习感知场景重建对于任务如场景操作/编辑、场景图生成等非常有帮助。本文旨在解决感知三维重建问题，将物体或场景解析为语义部分。</p><p>二、过去的方法及其问题</p><p>目前存在一些解决感知重建的先验方法，但它们主要依赖于三维监督学习，无法保留精确几何结构，这在现实场景中的应用造成不便。虽然神经辐射场（NeRF）在从多视角图像重建纹理化三维场景方面具有潜力，但现有方法在学习感知对象方面仍存在挑战。例如，Part-NeRF将物体表示为多个神经辐射场，但无法处理复杂场景的精确几何细节和纹理。本文提出了一种新的方法来解决这些问题。</p><p>三、研究方法</p><p>本文介绍了一种融合超级曲面和二维高斯混合表示的方法，尝试从多视角图像中提取三维结构线索。该方法实现了精确的结构化几何重建和高质量渲染。通过将网格形式的参数化超级曲面融入到二维高斯中，通过将高斯中心附着在网格面上，形成有效的混合表示。超级曲面的参数在训练过程中被迭代优化，高斯相应变形，这种混合表示既继承了超级曲面表示不同形状原始部位的优势，又通过二维高斯模拟复杂的纹理和几何细节确保了高质量渲染和几何重建。</p><p>四、实验任务和性能</p><p>本文在DTU和ShapeNet数据集上进行了广泛实验，该方法将场景分解为合理的部分，超越了现有先进方法的表现。实验结果表明，该方法在感知三维重建任务上的有效性，可以实现对场景的精细理解和应用。性能结果支持了其达到研究目标。</p><p>以上是关于该论文的简要介绍和概括，希望对您有所帮助。<br>好的，我会按照您的要求来详细阐述这篇论文的方法论。以下是概括的文章方法论部分：</p><ol><li>方法论：</li></ol><p>（1）研究背景分析：论文首先分析了三维重建的研究背景，指出了感知三维重建的重要性和挑战性。大多数现有的三维重建方法主要集中在低层次的表示，如点云、体素或网格，忽略了人类感知的感知语义部分。因此，本文旨在通过解决感知三维重建问题来提高场景操作/编辑和场景图生成等任务的效果。</p><p>（2）现存方法分析及其问题：对目前感知重建的先验方法进行了分析和评价，指出了现有方法存在的问题，如过于依赖三维监督学习，无法保留精确几何结构等。特别是在处理复杂场景的精确几何细节和纹理方面存在挑战。论文提到Part-NeRF虽然将物体表示为多个神经辐射场，但在处理复杂场景的精确几何细节方面存在不足。</p><p>（3）研究方法介绍：针对现有方法的问题，论文提出了一种融合超级曲面和二维高斯混合表示的方法。首先通过网格形式的参数化超级曲面融入到二维高斯中，通过将高斯中心附着在网格面上形成有效的混合表示。该方法充分利用超级曲面表示不同形状原始部位的优势，并结合二维高斯模拟复杂的纹理和几何细节，确保了高质量渲染和几何重建。在训练过程中，超级曲面的参数被迭代优化，高斯相应变形以适应不同的场景。这种方法融合了神经辐射场NeRF技术和其他计算机视觉技术来实现精确的感知重建。具体来说，它结合了多视角图像中的三维结构线索来实现精确的结构化几何重建和高质量渲染。此外，论文还介绍了如何利用这种方法进行大规模实验和性能评估，验证其在实际应用中的有效性。通过对多个数据集进行实验验证和对比分析，论文证明了该方法的性能优越性。总结起来就是结合了先进的计算机视觉技术来解决感知三维重建问题的方法论。</p><ol><li>Conclusion:</li></ol><p>（1）该工作的意义在于提出了一种新的感知三维重建方法，解决了计算机视觉领域中长期存在的难题。这项工作为三维重建提供了更高效、更精确的解决方案，有助于推动计算机视觉和计算机图形学领域的发展。此外，该方法的实际应用价值也非常高，可以应用于场景操作/编辑、场景图生成等领域，提高这些任务的性能和效果。</p><p>（2）创新点：本文提出了一种融合超级曲面和二维高斯混合表示的方法，实现了精确的结构化几何重建和高质量渲染。与现有的感知重建方法相比，该方法可以更好地保留几何结构细节，并在感知重建方面实现更好的性能。此外，本文还将先进的计算机视觉技术结合起来，为感知三维重建问题提供了新的解决方案。</p><p>性能：通过广泛的实验验证和对比分析，本文提出的方法在感知三维重建任务上表现出良好的性能。该方法可以有效地将场景分解为合理的部分，并实现对场景的精细理解和应用。与其他先进方法相比，本文提出的方法具有更好的性能和更高的准确性。</p><p>工作量：该论文工作量较大，包括对相关文献的调研、数据集的收集和处理、算法的设计和实现、实验验证和性能评估等。此外，该论文还提供了详细的实验过程和结果分析，方便其他研究者进行验证和进一步的研究。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-50980715c8e40f641a46157d2bc4c30d.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-db005d6f7c82d0989b5ba25dcd32b5a9.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-8849bacfa31cad2cde282450aa71e051.jpg" align="middle"></details><h2 id="CHASE-3D-Consistent-Human-Avatars-with-Sparse-Inputs-via-Gaussian-Splatting-and-Contrastive-Learning-1"><a href="#CHASE-3D-Consistent-Human-Avatars-with-Sparse-Inputs-via-Gaussian-Splatting-and-Contrastive-Learning-1" class="headerlink" title="CHASE: 3D-Consistent Human Avatars with Sparse Inputs via Gaussian   Splatting and Contrastive Learning"></a>CHASE: 3D-Consistent Human Avatars with Sparse Inputs via Gaussian Splatting and Contrastive Learning</h2><p><strong>Authors:Haoyu Zhao, Hao Wang, Chen Yang, Wei Shen</strong></p><p>Recent advancements in human avatar synthesis have utilized radiance fields to reconstruct photo-realistic animatable human avatars. However, both NeRFs-based and 3DGS-based methods struggle with maintaining 3D consistency and exhibit suboptimal detail reconstruction, especially with sparse inputs. To address this challenge, we propose CHASE, which introduces supervision from intrinsic 3D consistency across poses and 3D geometry contrastive learning, achieving performance comparable with sparse inputs to that with full inputs. Following previous work, we first integrate a skeleton-driven rigid deformation and a non-rigid cloth dynamics deformation to coordinate the movements of individual Gaussians during animation, reconstructing basic avatar with coarse 3D consistency. To improve 3D consistency under sparse inputs, we design Dynamic Avatar Adjustment(DAA) to adjust deformed Gaussians based on a selected similar pose/image from the dataset. Minimizing the difference between the image rendered by adjusted Gaussians and the image with the similar pose serves as an additional form of supervision for avatar. Furthermore, we propose a 3D geometry contrastive learning strategy to maintain the 3D global consistency of generated avatars. Though CHASE is designed for sparse inputs, it surprisingly outperforms current SOTA methods \textbf{in both full and sparse settings} on the ZJU-MoCap and H36M datasets, demonstrating that our CHASE successfully maintains avatar’s 3D consistency, hence improving rendering quality.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.09663v2">PDF</a> 13 pages, 6 figures</p><p><strong>Summary</strong><br>利用辐射场重建真实且可动的人体化身，CHASE方法在稀疏输入下表现优异，提高了渲染质量。</p><p><strong>Key Takeaways</strong></p><ul><li>CHASE方法利用骨骼驱动和非刚性布料动态变形协调动画，重建基础化身。</li><li>动态化身调整（DAA）通过数据集中相似姿势/图像调整高斯函数，改善稀疏输入的3D一致性。</li><li>提出的3D几何对比学习策略有助于保持生成化身的全局3D一致性。</li><li>在ZJU-MoCap和H36M数据集上，CHASE方法在全输入和稀疏输入情境下均优于现有技术，显示出卓越的3D一致性维护与渲染质量改进能力。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>好的，以下是对该论文的总结：</p><ol><li><p>标题：基于稀疏输入的3D一致人形化身合成研究（CHASE: 3D-Consistent Human Avatars with Sparse Inputs via Gaussian）</p></li><li><p>作者：Haoyu Zhao，Hao Wang，Chen Yang，Wei Shen</p></li><li><p>隶属机构：第一作者Haoyu Zhao为武汉大学计算机学院；Hao Wang为武汉光电国家实验室华中科技大学；Chen Yang和Wei Shen为上海交通大学人工智能研究院。</p></li><li><p>关键词：Human Avatar Synthesis，Sparse Inputs，Gaussian Splatting，Contrastive Learning，3D Consistency</p></li><li><p>链接：论文链接（待补充）；GitHub代码链接（待补充）</p></li><li><p>总结：</p><ul><li><p>(1)研究背景：随着虚拟现实、增强现实等技术的快速发展，对高质量、可动画的人形化身合成需求日益增长。然而，现有方法在稀疏输入条件下难以保持3D一致性和高质量重建。</p></li><li><p>(2)过去的方法及其问题：早期的人形化身合成方法主要依赖于多相机设置和高质输入数据，不适用于稀疏输入。最近的方法虽然可以处理稀疏输入，但在保持3D一致性和细节重建方面存在挑战。</p></li><li><p>(3)研究方法：本研究提出了一种基于稀疏输入的3D一致人形化身合成方法CHASE。首先，通过骨架驱动刚性变形和非刚性布料动力学变形融合动画基本化身，实现粗3D一致性。然后，通过动态化身调整（DAA）和基于相似姿势的图像选择，增强稀疏输入下的3D一致性。此外，引入了一种3D几何对比学习策略来保持生成的化身全局一致性。</p></li><li><p>(4)任务与性能：在ZJU-MoCap和H36M数据集上的实验表明，CHASE方法在稀疏输入条件下实现了与全输入相当的性能，成功保持了化身的3D一致性并提高了渲染质量。性能结果支持其达到研究目标。</p></li></ul></li></ol><p>请注意，由于无法直接提供论文链接和GitHub代码链接，请在相应位置填写相关链接或标注“待补充”。<br>好的，下面我将用中文详细描述该论文的方法论思想。按照格式要求，特定专有名词会用英文标注。请注意不使用重复内容，并且遵循原有的数字编号格式。</p><ol><li><p>方法论思想：</p><ul><li><p>(1) 背景与研究动机：针对虚拟现实、增强现实等技术对高质量人形化身合成的需求，考虑到稀疏输入条件下保持3D一致性和高质量重建的挑战，本文提出了一种基于稀疏输入的3D一致人形化身合成方法CHASE。</p></li><li><p>(2) 数据准备与输入处理：论文首先收集了包括图像、拟合的SMPL参数和前景掩模等输入数据。其中，图像是从单目视频中获取的，SMPL参数是用于描述人体姿态的，前景掩模用于提取图像中的人物区域。这些输入数据是后续处理的基础。</p></li><li><p>(3) 人形化身合成方法：通过骨架驱动刚性变形和非刚性布料动力学变形融合动画基本化身，实现粗3D一致性。这是核心的处理步骤之一。此外，引入动态化身调整（DAA）和基于相似姿势的图像选择，以增强稀疏输入下的3D一致性。这些策略有助于在有限的输入信息下创建更逼真的化身。</p></li><li><p>(4) 3D几何对比学习策略：为了保持生成的化身的全局一致性，论文提出了一种3D几何对比学习策略。该策略通过对3D高斯点云进行特征提取和对比学习来实现。具体来说，将变形后的高斯点云视为一个静态点云进行处理，并利用DGCNN进行特征提取。通过对比不同姿态下的点云特征，确保动画过程中的3D一致性。这一策略为创建连贯的动画提供了保障。</p></li><li><p>(5) 实验验证与优化：在ZJU-MoCap和H36M数据集上的实验验证了CHASE方法的有效性。结果表明，在稀疏输入条件下，该方法实现了与全输入相当的性能，成功保持了化身的3D一致性并提高了渲染质量。性能结果支持该研究达到其目标。在整个流程中，通过不断的实验验证和优化，确保了方法的性能和稳定性。</p></li></ul></li></ol><p>希望这个回答能够帮助你理解该论文的方法论思想！</p><p>好的，以下是按照您要求的回答：</p><ol><li>Conclusion:</li></ol><p>（1）研究意义：该研究在虚拟现实和增强现实领域中具有重要的应用价值。通过基于稀疏输入的3D一致人形化身合成研究，满足了高质量人形化身合成的需求，对于提升虚拟体验、增强人机交互等方面具有积极意义。</p><p>（2）创新点、性能、工作量总结：</p><ul><li>创新点：该研究提出了一种基于稀疏输入的3D一致人形化身合成方法CHASE，通过骨架驱动刚性变形和非刚性布料动力学变形的融合，实现了在稀疏输入条件下的高质量人形化身合成。此外，引入了动态化身调整和基于相似姿势的图像选择策略，以及3D几何对比学习策略，提高了化身的3D一致性和全局连贯性。</li><li>性能：在ZJU-MoCap和H36M数据集上的实验表明，CHASE方法在稀疏输入条件下实现了与全输入相当的性能，成功保持了化身的3D一致性，并提高了渲染质量。性能结果支持该研究达到其目标。</li><li>工作量：文章详细描述了方法的实现过程，包括数据准备、输入处理、人形化身合成方法、3D几何对比学习策略和实验验证等。然而，文章未提及对于硬件资源的需求和消耗，如计算资源、存储空间等方面的情况。</li></ul><p>希望以上总结能够帮到您！</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-20792050bb488ed224cbedbc40247c7d.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-060b38ccf8897d85a83f1eef91ce6c1b.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-3d3dca898a7edd9f20d2ba3cda712423.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-af62178f5fdd22828fd6edb951afcb8c.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-5456bb2bf3dabbd73a53ce6f04593b9a.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-8c68f49b04c0a784781a9f795f4373ae.jpg" align="middle"></details><h2 id="Gaussian-in-the-Dark-Real-Time-View-Synthesis-From-Inconsistent-Dark-Images-Using-Gaussian-Splatting-1"><a href="#Gaussian-in-the-Dark-Real-Time-View-Synthesis-From-Inconsistent-Dark-Images-Using-Gaussian-Splatting-1" class="headerlink" title="Gaussian in the Dark: Real-Time View Synthesis From Inconsistent Dark   Images Using Gaussian Splatting"></a>Gaussian in the Dark: Real-Time View Synthesis From Inconsistent Dark Images Using Gaussian Splatting</h2><p><strong>Authors:Sheng Ye, Zhen-Hui Dong, Yubin Hu, Yu-Hui Wen, Yong-Jin Liu</strong></p><p>3D Gaussian Splatting has recently emerged as a powerful representation that can synthesize remarkable novel views using consistent multi-view images as input. However, we notice that images captured in dark environments where the scenes are not fully illuminated can exhibit considerable brightness variations and multi-view inconsistency, which poses great challenges to 3D Gaussian Splatting and severely degrades its performance. To tackle this problem, we propose Gaussian-DK. Observing that inconsistencies are mainly caused by camera imaging, we represent a consistent radiance field of the physical world using a set of anisotropic 3D Gaussians, and design a camera response module to compensate for multi-view inconsistencies. We also introduce a step-based gradient scaling strategy to constrain Gaussians near the camera, which turn out to be floaters, from splitting and cloning. Experiments on our proposed benchmark dataset demonstrate that Gaussian-DK produces high-quality renderings without ghosting and floater artifacts and significantly outperforms existing methods. Furthermore, we can also synthesize light-up images by controlling exposure levels that clearly show details in shadow areas.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.09130v2">PDF</a> accepted by PG 2024</p><p><strong>Summary</strong><br>3D高斯飞溅作为一种强大的表示形式，能够利用一致的多视图图像合成出色的新视图，但在暗环境中捕捉的图像存在明显的亮度变化和多视图不一致性，严重挑战了其性能。</p><p><strong>Key Takeaways</strong></p><ul><li>3D高斯飞溅能够利用一致的多视图图像合成新视图。</li><li>暗环境中捕捉的图像可能会有亮度变化和多视图不一致性。</li><li>提出了Gaussian-DK方法来解决这些问题。</li><li>使用各向异性的3D高斯来表示物理世界的一致辐射场。</li><li>设计了相机响应模块来补偿多视图不一致性。</li><li>引入了基于步骤的梯度缩放策略，约束接近相机的高斯分布。</li><li>实验表明，Gaussian-DK在渲染时能够避免幽灵和漂浮物效应，显著优于现有方法。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: 在暗环境中利用高斯算法进行实时视图合成</p></li><li><p>Authors: R. Chen, T. Ritschel, E. Whiting （主编） 以及 Sheng Ye, Zhen-Hui Dong, Yubin Hu 等</p></li><li><p>Affiliation: 第一作者所在的单位（MOE-Key Laboratory of Pervasive Computing, Department of Computer Science and Technology, Tsinghua University）</p></li><li><p>Keywords: 高斯算法，暗环境，视图合成，图像一致性，计算机图形学</p></li><li><p>Urls: [论文链接]：[链接地址]，GitHub代码链接（如果可用）:GitHub:None（如未公开则填写）</p></li><li><p>Summary:</p><ul><li><p>(1) 研究背景：该文主要探讨了如何在暗环境下进行实时视图合成的问题。由于暗环境拍摄的照片往往存在亮度差异和视图不一致的问题，现有的视图合成方法在这些环境下的性能会大幅下降。因此，本文提出了一种新的解决方案来解决这一问题。</p></li><li><p>(2) 过去的方法及其问题：现有的视图合成方法大多依赖于一致的多视角图像作为输入，但在暗环境下拍摄的图像往往存在不一致性，导致现有方法的性能下降。因此，需要一种新的方法来处理这种不一致性。</p></li><li><p>(3) 研究方法：本文提出了一种基于高斯算法的解决方案，称为Gaussian-DK。该方法通过引入一组各向异性的三维高斯来表示物理世界的辐射场，并设计了一个相机响应模块来补偿多视角的不一致性。此外，还引入了一种基于梯度的缩放策略来约束相机附近的浮点数，防止其分裂和克隆。</p></li><li><p>(4) 任务与性能：本文的方法在提出的基准数据集上进行了实验验证，结果显示Gaussian-DK能够生成高质量的渲染图像，不会出现幽灵和浮标伪影，并且显著优于现有方法。此外，该方法还可以通过控制曝光级别来合成清晰显示阴影区域的光照图像。总体而言，该方法的性能支持其目标，为暗环境下的实时视图合成提供了一种有效的解决方案。</p></li></ul></li><li>方法论概述：</li></ol><p>本文提出了一种基于高斯算法的暗环境中实时视图合成的方法，主要步骤包括：</p><pre><code>- (1) 研究背景分析：针对暗环境下视图合成的问题，分析现有方法的不足，提出新的解决方案。

- (2) 数据集准备：提出基准数据集用于实验验证。

- (3) 方法设计：引入一组各向异性的三维高斯来表示物理世界的辐射场，设计相机响应模块来补偿多视角的不一致性。此外，还引入了一种基于梯度的缩放策略来约束相机附近的浮点数，防止其分裂和克隆。

- (4) 曝光级别控制：通过控制曝光级别来合成清晰显示阴影区域的光照图像。提出一种计算曝光水平的方法，并结合高斯光特征进行亮度补偿。

- (5) 光线特征细化：利用CNN网络学习响应函数，生成像素级亮度图，以精确捕捉相机对应的视角中的亮度水平。

- (6) 颜色映射：通过乘以亮度图来补偿像素级亮度变化，然后使用CNN来学习颜色映射函数，将调整后的辐射值转换为图像像素值。为了避免数值不稳定，所有辐射值都在对数域中表示。

- (7) 浮点数去除：针对仍存在的浮点数问题，通过优化训练过程中的高斯位置，以及引入额外的优化策略来减少浮点数的影响。
</code></pre><p>本文的方法在提出的基准数据集上进行了实验验证，结果显示该方法能够生成高质量的渲染图像，显著优于现有方法。总体来说，本文为暗环境下的实时视图合成提供了一种有效的解决方案。</p><ol><li>Conclusion:</li></ol><ul><li>(1)该文章提出了一种针对暗环境中实时视图合成的问题的有效解决方案，对于改善暗环境下图像的视图合成具有重大意义。</li><li>(2)Innovation point：该文章的创新点在于引入了基于高斯算法的实时视图合成方法，通过引入三维高斯表示物理世界的辐射场，并设计了相机响应模块来补偿多视角的不一致性，显著提高了暗环境下视图合成的性能。同时，还引入了基于梯度的缩放策略来约束相机附近的浮点数，防止其分裂和克隆。<br>Performance：该文章的方法在提出的基准数据集上进行了实验验证，结果显示该方法能够生成高质量的渲染图像，显著优于现有方法，性能表现良好。<br>Workload：文章的工作量大，涉及的方法论包括背景分析、数据集准备、方法设计、曝光级别控制、光线特征细化、颜色映射和浮点数去除等多个环节，工作较为复杂。</li></ul><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-6ee18dbf49e26ebda40420ea6e0f3b17.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-67927cd71eaa8acc2d1a34d80afe62e1.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-d21fe236c4dad10202a55b404d85041f.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-7540304f0b025f494a11202be37b575d.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-5d500ebdda88ccf9b9fc8fdd3ed55fa3.jpg" align="middle"></details></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io/2024/08/22/Paper/2024-08-22/3DGS/">https://kedreamix.github.io/2024/08/22/Paper/2024-08-22/3DGS/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/3DGS/">3DGS</a></div><div class="post_share"><div class="social-share" data-image="https://picx.zhimg.com/v2-8849bacfa31cad2cde282450aa71e051.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/08/22/Paper/2024-08-22/NeRF/" title="NeRF"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-3753f94921a69903dd19c26b35387b0c.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">NeRF</div></div></a></div><div class="next-post pull-right"><a href="/2024/08/22/Paper/2024-08-22/Talking%20Head%20Generation/" title="Talking Head Generation"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-5511b9415fa92a35eaac4b9c4fcf789a.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Talking Head Generation</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/01/25/Paper/3DGS%20Survey/" title="3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-25</div><div class="title">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</div></div></a></div><div><a href="/2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" title="超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-01</div><div class="title">超赞的数字人生成知识库 Awesome-Talking-Head-Synthesis</div></div></a></div><div><a href="/2024/01/24/Paper/2024-01-24/3DGS/" title="3DGS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-3d3dcd00c27bc3d320b23d4247ae79f3.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-24</div><div class="title">3DGS</div></div></a></div><div><a href="/2024/01/30/Paper/2024-01-30/3DGS/" title="3DGS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-2032721a60695f2d41ac96f75dec65a2.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-30</div><div class="title">3DGS</div></div></a></div><div><a href="/2024/02/09/Paper/2024-02-09/3DGS/" title="3DGS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-28074a5f13fdf5a52c0d4de04dfb9406.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-09</div><div class="title">3DGS</div></div></a></div><div><a href="/2024/02/02/Paper/2024-02-02/3DGS/" title="3DGS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-e4e5570dfa99dfac9b297f7650c717c3.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-02</div><div class="title">3DGS</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-08-22-%E6%9B%B4%E6%96%B0"><span class="toc-text">2024-08-22 更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Large-Point-to-Gaussian-Model-for-Image-to-3D-Generation"><span class="toc-text">Large Point-to-Gaussian Model for Image-to-3D Generation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Learning-Part-aware-3D-Representations-by-Fusing-2D-Gaussians-and-Superquadrics"><span class="toc-text">Learning Part-aware 3D Representations by Fusing 2D Gaussians and Superquadrics</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CHASE-3D-Consistent-Human-Avatars-with-Sparse-Inputs-via-Gaussian-Splatting-and-Contrastive-Learning"><span class="toc-text">CHASE: 3D-Consistent Human Avatars with Sparse Inputs via Gaussian Splatting and Contrastive Learning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Gaussian-in-the-Dark-Real-Time-View-Synthesis-From-Inconsistent-Dark-Images-Using-Gaussian-Splatting"><span class="toc-text">Gaussian in the Dark: Real-Time View Synthesis From Inconsistent Dark Images Using Gaussian Splatting</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-08-22-%E6%9B%B4%E6%96%B0-1"><span class="toc-text">2024-08-22 更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Large-Point-to-Gaussian-Model-for-Image-to-3D-Generation-1"><span class="toc-text">Large Point-to-Gaussian Model for Image-to-3D Generation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Learning-Part-aware-3D-Representations-by-Fusing-2D-Gaussians-and-Superquadrics-1"><span class="toc-text">Learning Part-aware 3D Representations by Fusing 2D Gaussians and Superquadrics</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CHASE-3D-Consistent-Human-Avatars-with-Sparse-Inputs-via-Gaussian-Splatting-and-Contrastive-Learning-1"><span class="toc-text">CHASE: 3D-Consistent Human Avatars with Sparse Inputs via Gaussian Splatting and Contrastive Learning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Gaussian-in-the-Dark-Real-Time-View-Synthesis-From-Inconsistent-Dark-Images-Using-Gaussian-Splatting-1"><span class="toc-text">Gaussian in the Dark: Real-Time View Synthesis From Inconsistent Dark Images Using Gaussian Splatting</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image:url('https://picx.zhimg.com/v2-8849bacfa31cad2cde282450aa71e051.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js").then((()=>{pangu.autoSpacingPage()}))}function panguInit(){panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.1},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script></div><script id="canvas_nest" defer color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><script data-pjax>function butterfly_swiper_injector_config(){var a=document.getElementById("recent-posts");console.log("已挂载butterfly_swiper"),a.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/25/Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="2024/01/25/Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="2024/01/25/Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/17/Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023/12/17/Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="2023/12/17/Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script></body></html>