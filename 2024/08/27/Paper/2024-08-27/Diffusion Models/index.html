<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Diffusion Models | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-08-27  How Diffusion Models Learn to Factorize and Compose">
<meta property="og:type" content="article">
<meta property="og:title" content="Diffusion Models">
<meta property="og:url" content="https://kedreamix.github.io/2024/08/27/Paper/2024-08-27/Diffusion%20Models/index.html">
<meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World">
<meta property="og:description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-08-27  How Diffusion Models Learn to Factorize and Compose">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://picx.zhimg.com/v2-806fc1c8232da95cacb188f0dc1e33f3.jpg">
<meta property="article:published_time" content="2024-08-26T17:27:25.000Z">
<meta property="article:modified_time" content="2024-08-26T17:27:25.235Z">
<meta property="article:author" content="Kedreamix">
<meta property="article:tag" content="Diffusion Models">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://picx.zhimg.com/v2-806fc1c8232da95cacb188f0dc1e33f3.jpg"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/2024/08/27/Paper/2024-08-27/Diffusion%20Models/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-74LZ5BEQQ1');
</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":true,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  noticeOutdate: {"limitDay":500,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: true,
  islazyload: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Diffusion Models',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-08-27 01:27:25'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const now = new Date()
          const hour = now.getHours()
          const isNight = hour <= 6 || hour >= 24
          if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
          else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css"  media="defer" onload="this.media='all'"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload="this.media='all'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.0.0"><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml">

<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()
  window.addEventListener('load',() => { preloader.endLoading() })

  if (false) {
    document.addEventListener('pjax:send', () => { preloader.initLoading() })
    document.addEventListener('pjax:complete', () => { preloader.endLoading() })
  }
})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">283</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://picx.zhimg.com/v2-806fc1c8232da95cacb188f0dc1e33f3.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"/><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Diffusion Models</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-08-26T17:27:25.000Z" title="发表于 2024-08-27 01:27:25">2024-08-27</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-08-26T17:27:25.235Z" title="更新于 2024-08-27 01:27:25">2024-08-27</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">13.4k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>46分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Diffusion Models"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote>
<p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-08-27-更新"><a href="#2024-08-27-更新" class="headerlink" title="2024-08-27 更新"></a>2024-08-27 更新</h1><h2 id="How-Diffusion-Models-Learn-to-Factorize-and-Compose"><a href="#How-Diffusion-Models-Learn-to-Factorize-and-Compose" class="headerlink" title="How Diffusion Models Learn to Factorize and Compose"></a>How Diffusion Models Learn to Factorize and Compose</h2><p><strong>Authors:Qiyao Liang, Ziming Liu, Mitchell Ostrow, Ila Fiete</strong></p>
<p>Diffusion models are capable of generating photo-realistic images that combine elements which likely do not appear together in the training set, demonstrating the ability to compositionally generalize. Nonetheless, the precise mechanism of compositionality and how it is acquired through training remains elusive. Inspired by cognitive neuroscientific approaches, we consider a highly reduced setting to examine whether and when diffusion models learn semantically meaningful and factorized representations of composable features. We performed extensive controlled experiments on conditional Denoising Diffusion Probabilistic Models (DDPMs) trained to generate various forms of 2D Gaussian data. We found that the models learn factorized but not fully continuous manifold representations for encoding continuous features of variation underlying the data. With such representations, models demonstrate superior feature compositionality but limited ability to interpolate over unseen values of a given feature. Our experimental results further demonstrate that diffusion models can attain compositionality with few compositional examples, suggesting a more efficient way to train DDPMs. Finally, we connect manifold formation in diffusion models to percolation theory in physics, offering insight into the sudden onset of factorized representation learning. Our thorough toy experiments thus contribute a deeper understanding of how diffusion models capture compositional structure in data. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.13256v1">PDF</a> 11 pages, 6 figures, plus appendix, some content overlap with   arXiv:2402.03305</p>
<p><strong>Summary</strong><br>扩散模型能够生成逼真的图像，结合训练集中不常见的元素，展示了组合泛化的能力，但其具体的组合机制及训练过程中如何获得仍不清晰。</p>
<p><strong>Key Takeaways</strong>  </p>
<ul>
<li>扩散模型能够生成包含不同训练集中元素的逼真图像，展示了组合泛化能力。</li>
<li>模型学习到了因子化表示，但在处理未见过的特征值时插值能力有限。</li>
<li>使用条件去噪扩散概率模型（DDPMs）进行广泛的实验，验证了模型对2D高斯数据的学习能力。</li>
<li>扩散模型通过少量组合示例就能实现组合性，表明了一种更高效的训练方式。</li>
<li>模型的流形形成与物理学中的渗透理论相关联，解释了因子化表示学习的突然发生。</li>
<li>研究的玩具实验深入理解了扩散模型如何捕捉数据的组合结构。</li>
<li>尽管学习到了因子化表示，模型对连续特征变化的编码仍不完全连续。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li><p>标题：基于扩散模型学习因子分解的研究<br>（中文翻译：Research on Learning Factorization Based on Diffusion Models）</p>
</li>
<li><p>作者：梁启耀、刘明正、奥斯特罗夫·米切尔、菲耶·伊拉（Qiyao Liang、Ziming Liu、Mitchell Ostrow、Ila Fiete）等人。</p>
</li>
<li><p>所属机构：麻省理工学院（中文翻译：Massachusetts Institute of Technology）。</p>
</li>
<li><p>关键词：扩散模型（Diffusion Models）、因子分解（Factorization）、合成泛化（Compositional Generalization）、图像生成（Image Generation）。</p>
</li>
<li><p>Urls：论文链接（论文网址）或GitHub代码链接（如果可用，填写GitHub：如果不可用则填写None）。</p>
</li>
<li><p>摘要：</p>
<p> (1) 研究背景：本文主要研究扩散模型如何通过训练学习因子分解和合成泛化的能力。尽管扩散模型能够生成逼真的图像，但模型的精确机制和如何通过学习获得组合泛化能力仍然不明确。本研究旨在通过简化模型和实验探究这一过程。</p>
<p> (2) 过去的方法及问题：虽然已有许多关于深度生成模型中因子分解和合成泛化的理论和实证研究，但这些研究在复杂数据集和大型模型上并未达成一致的结论。特别是在中间层学习到的因子化表示是否促进模型性能的组合泛化问题上存在争议。此外，由于数据的复杂性和混合特征，难以分析模型的表示学习能力。</p>
<p> (3) 研究方法：本研究通过高度简化的条件去噪扩散概率模型（DDPMs）进行实验，以考察扩散模型是否以及何时学习语义上有意义的可分解特征表示。实验包括对生成各种形式的二维高斯数据的DDPMs进行严格控制。研究发现，这些模型学习到的表示并非完全连续的流形表示，而是具有编码数据变异连续特征的分解表示。这种表示使得模型具有优越的特征组合能力，但在给定特征的未见值上的插值能力有限。此外，本研究还将扩散模型的流形形成与物理学中的渗流理论联系起来，为突然出现的因子表示学习提供了洞察。</p>
<p> (4) 任务与性能：实验结果表明，扩散模型可以使用较少的组合示例达到组合性，表明更高效地训练DDPMs的方法。此外，通过简化实验任务，本研究对扩散模型如何捕获数据中的组合结构有了更深入的理解。这些结果对于理解扩散模型在更复杂任务上的性能以及改进模型训练和设计具有重要意义。</p>
</li>
</ol>
<p>希望以上总结符合您的要求！</p>
<ol>
<li>方法论：</li>
</ol>
<p>(1) 研究背景：本文旨在探究扩散模型如何通过训练学习因子分解和合成泛化的能力。为此，研究使用了简化的条件去噪扩散概率模型（DDPMs）进行实验。</p>
<p>(2) 数据集制备：为了研究扩散模型对图像数据的处理能力，作者生成了二维高斯数据集，包括高斯凸起（Gaussian bump）和高斯SOS（Gaussian sum of stripes）图像。这些图像具有不同的特征，如位置、标准差等，为实验提供了丰富的数据样本。</p>
<p>(3) 模型设计：实验采用了条件DDPM模型，该模型具有标准UNet架构。通过输入图像的显式标签（如µx和µy），模型学习将图像数据与标签关联起来。为了更好地理解模型内部的学习过程，研究者对模型的第4层输出进行了深入研究。</p>
<p>(4) 数据表示学习：为了探究模型如何学习数据的因子分解表示，作者使用了主成分分析（PCA）或UMAP（Uniform Manifold Approximation and Projection）等工具来降低模型的内部维度。通过这些工具，研究者能够观察模型如何编码二维高斯数据集中的x和y维度，并理解其内部的因子分解机制。</p>
<p>(5) 实验方法：实验过程中，通过控制变量法，研究者探究了不同参数（如增量dx和dy、标准差σx和σy）对模型学习的影响。通过调整这些参数，生成了不同稀疏程度和重叠程度的数据集，这为模型的训练和研究提供了丰富的实验场景。</p>
<p>(6) 结果分析：通过对实验结果的分析，发现扩散模型能够在较少的组合示例下达到组合性，表明存在更高效的训练方法。此外，通过简化实验任务，研究对扩散模型如何捕获数据中的组合结构有了更深入的理解。这些结果对于理解扩散模型在更复杂任务上的性能以及改进模型训练和设计具有重要意义。</p>
<ol>
<li>Conclusion:</li>
</ol>
<p>(1)这篇工作的意义在于探究扩散模型在因子分解和合成泛化方面的学习能力。该研究有助于深入理解扩散模型的内部机制，为相关领域的研究提供新的思路和方法。同时，该研究对于提高扩散模型在图像生成等任务上的性能，以及推动计算机视觉和自然语言处理等领域的发展具有重要意义。</p>
<p>(2)创新点：本文创新性地使用简化的条件去噪扩散概率模型（DDPMs）进行实验，探究扩散模型学习因子分解和合成泛化的能力。此外，研究还将扩散模型的流形形成与物理学中的渗流理论联系起来，为理解模型学习机制提供了新的视角。<br>性能：实验结果表明，扩散模型在图像生成等任务上具有良好的性能，能够使用较少的组合示例达到组合性，表明存在更高效的训练方法。同时，研究对扩散模型如何捕获数据中的组合结构有了更深入的理解，有助于改进模型训练和设计。<br>工作量：文章进行了大量的实验和理论分析，探究了扩散模型在不同任务上的性能表现。同时，文章对实验结果进行了详细的阐述和讨论，为读者提供了丰富的信息和启示。但是，文章未涉及更复杂任务上的性能表现，如视频生成等，需要进一步的研究和探索。</p>
<details>
  <summary>点此查看论文截图</summary>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-e4a9f417c8df766dbd14f92c333cb623.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-cd477791f7ae05559da9e3dbd1021431.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-f895bf50b7d016d1bd7a62d8286679e0.jpg" align="middle">
</details>




<h2 id="CustomCrafter-Customized-Video-Generation-with-Preserving-Motion-and-Concept-Composition-Abilities"><a href="#CustomCrafter-Customized-Video-Generation-with-Preserving-Motion-and-Concept-Composition-Abilities" class="headerlink" title="CustomCrafter: Customized Video Generation with Preserving Motion and   Concept Composition Abilities"></a>CustomCrafter: Customized Video Generation with Preserving Motion and   Concept Composition Abilities</h2><p><strong>Authors:Tao Wu, Yong Zhang, Xintao Wang, Xianpan Zhou, Guangcong Zheng, Zhongang Qi, Ying Shan, Xi Li</strong></p>
<p>Customized video generation aims to generate high-quality videos guided by text prompts and subject’s reference images. However, since it is only trained on static images, the fine-tuning process of subject learning disrupts abilities of video diffusion models (VDMs) to combine concepts and generate motions. To restore these abilities, some methods use additional video similar to the prompt to fine-tune or guide the model. This requires frequent changes of guiding videos and even re-tuning of the model when generating different motions, which is very inconvenient for users. In this paper, we propose CustomCrafter, a novel framework that preserves the model’s motion generation and conceptual combination abilities without additional video and fine-tuning to recovery. For preserving conceptual combination ability, we design a plug-and-play module to update few parameters in VDMs, enhancing the model’s ability to capture the appearance details and the ability of concept combinations for new subjects. For motion generation, we observed that VDMs tend to restore the motion of video in the early stage of denoising, while focusing on the recovery of subject details in the later stage. Therefore, we propose Dynamic Weighted Video Sampling Strategy. Using the pluggability of our subject learning modules, we reduce the impact of this module on motion generation in the early stage of denoising, preserving the ability to generate motion of VDMs. In the later stage of denoising, we restore this module to repair the appearance details of the specified subject, thereby ensuring the fidelity of the subject’s appearance. Experimental results show that our method has a significant improvement compared to previous methods. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.13239v1">PDF</a> project page: <a target="_blank" rel="noopener" href="https://customcrafter.github.io/">https://customcrafter.github.io/</a></p>
<p><strong>Summary</strong><br>自定义视频生成旨在通过文本提示和主题参考图生成高质量视频。本文提出了CustomCrafter框架，保持视频扩散模型的运动生成和概念组合能力，无需额外视频或微调模型。</p>
<p><strong>Key Takeaways</strong>  </p>
<ul>
<li>自定义视频生成通过文本和参考图生成高质量视频。</li>
<li>传统方法中使用额外视频进行微调会影响模型的概念组合和运动生成能力。</li>
<li>CustomCrafter框架采用插拔式模块更新VDMs中的少量参数，增强了模型捕捉外观细节和概念组合能力。</li>
<li>VDMs在去噪过程中早期更注重视频运动的恢复，后期专注于主题细节的恢复。</li>
<li>提出动态加权视频采样策略，减少主题学习模块对运动生成的影响。</li>
<li>实验结果表明，CustomCrafter方法相较于传统方法有显著改进。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>方法论：</li>
</ol>
<p>(1) 分析视频扩散模型（VDMs）的特性及在定制化生成领域的问题：研究视频扩散模型的运行机制及其特点，如模型对于概念组合和运动生成的特性等。识别在现有定制化生成领域中遇到的挑战和问题，例如概念组合能力、运动能力受到破坏等问题。在视频扩散模型中，针对特定主题的学习往往会影响模型的原始运动生成能力和概念组合能力，因此需要提出新的解决方案来解决这一问题。</p>
<p>(2) 提出动态加权视频采样策略（Dynamic Weighted Video Sampling Strategy）：为了解决视频扩散模型在定制化生成过程中运动与外观之间的耦合问题，采用动态加权视频采样策略来分离运动和外观的修复过程。通过在早期去噪阶段减少空间主体学习模块的影响，并在后期恢复细节的过程中增加其影响，从而保留模型的原始运动生成能力并提升对新主题的学习效果。通过此策略，能够在没有额外的视频指导或训练的条件下生成高质量的视频。同时引入了动态权重调整机制，以适应不同阶段的去噪过程。</p>
<p>(3) 设计空间主体学习模块（Spatial Subject Learning Module）：为了学习新主体的外观细节，设计了一种空间主体学习模块。该模块通过更新注意力层的查询、键和值参数来增强模型对主体外观细节的学习能力。通过采用这种模块化的设计，可以在不破坏模型原有运动生成和概念组合能力的前提下，实现对新主体外观的学习。同时，该模块采用了一种插拨式的结构，便于在模型中进行灵活应用和调整。通过引入LoRA（Linearized Response Activation）等技术手段提升学习效率和效果。具体来说就是在更新注意力参数的同时还利用了预训练的文本编码器来表示新概念的嵌入向量来对新主题的外观进行学习从而将其融入视频生成的过程中提升其表达丰富性和生动性在完成训练之后我们的模型可以根据用户的输入文本提示来生成包含特定主题的高质量视频而不会破坏原有的运动生成能力和概念组合能力通过这种方式我们的方法可以在不需要额外视频指导或模型重复训练的情况下实现对新主题视频的定制化生成进而提升用户体验和提升模型的实用价值通过大量的实验验证了我们的方法相较于现有工作在定制化视频生成领域的优越性并提供了详尽的对比结果分析</p>
<details>
  <summary>点此查看论文截图</summary>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-0fac690e3fd7f8cc116790af2e2a577c.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-b1e1129b91edb8ee4b096d9111f762aa.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-b9f4b432e5c0a4a598a7dc12f6043a97.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-57b86f5676bffe8498af679e20e43194.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-c886db1851e9d936da1015a3d9eda7c3.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-bad2356e852636cf0b876c6601b96633.jpg" align="middle">
</details>




<h2 id="General-Intelligent-Imaging-and-Uncertainty-Quantification-by-Deterministic-Diffusion-Model"><a href="#General-Intelligent-Imaging-and-Uncertainty-Quantification-by-Deterministic-Diffusion-Model" class="headerlink" title="General Intelligent Imaging and Uncertainty Quantification by   Deterministic Diffusion Model"></a>General Intelligent Imaging and Uncertainty Quantification by   Deterministic Diffusion Model</h2><p><strong>Authors:Weiru Fan, Xiaobin Tang, Yiyi Liao, Da-Wei Wang</strong></p>
<p>Computational imaging is crucial in many disciplines from autonomous driving to life sciences. However, traditional model-driven and iterative methods consume large computational power and lack scalability for imaging. Deep learning (DL) is effective in processing local-to-local patterns, but it struggles with handling universal global-to-local (nonlocal) patterns under current frameworks. To bridge this gap, we propose a novel DL framework that employs a progressive denoising strategy, named the deterministic diffusion model (DDM), to facilitate general computational imaging at a low cost. We experimentally demonstrate the efficient and faithful image reconstruction capabilities of DDM from nonlocal patterns, such as speckles from multimode fiber and intensity patterns of second harmonic generation, surpassing the capability of previous state-of-the-art DL algorithms. By embedding Bayesian inference into DDM, we establish a theoretical framework and provide experimental proof of its uncertainty quantification. This advancement ensures the predictive reliability of DDM, avoiding misjudgment in high-stakes scenarios. This versatile and integrable DDM framework can readily extend and improve the efficacy of existing DL-based imaging applications. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.13061v1">PDF</a> </p>
<p><strong>Summary</strong><br>提出了一种新的深度学习框架——确定性扩散模型（DDM），通过渐进去噪策略实现通用计算成像，有效处理全局到局部的非局部模式，显著提升成像能力。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>提出了确定性扩散模型（DDM），用于通用计算成像。</li>
<li>DDM 采用渐进去噪策略，能有效处理全局到局部的非局部模式。</li>
<li>在多模光纤的斑点和二次谐波生成的强度模式等非局部模式下，DDM 显示出比先前最先进的深度学习算法更优异的图像重建能力。</li>
<li>将贝叶斯推断嵌入到DDM中，建立了理论框架并实验证明了其不确定性量化的可靠性。</li>
<li>DDM 的预测可靠性确保在高风险场景中避免误判。</li>
<li>DDM 框架的通用性和可集成性有助于扩展和提升现有基于深度学习的成像应用的效能。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>方法论：</li>
</ol>
<p>(1) 实验设计：本文首先设计了实验方案，旨在通过非线性SHG配置生成非局部模式。实验过程中使用了红外脉冲激光作为基本场，通过一系列光学元件对激光进行调制、扩展、聚焦等操作，生成了具有特定相位的激光束。实验中还对细胞图像数据集进行了处理和准备，为后续的模式生成和模式识别提供了数据基础。</p>
<p>(2) 方法介绍：文章介绍了一种结合了卷积神经网络（CNN）和可逆自编码网络（IRAE）的方法，用于处理非局部模式。该方法通过使用BiFormer模块改进CNN的编码和解码过程，提高模型对非局部特征的处理能力。同时，IRAE网络利用可逆流基生成算法构建深度网络架构，实现了信息的完全保留和模式的可逆性。此外，文章还介绍了控制网络（ControlNet）的实现方式，将IRAE的输出作为条件输入到网络中，实现了对噪声预测的控制。</p>
<p>(3) 扩散模型的应用：文章进一步将一般的扩散模型应用于非局部模式的生成和处理。通过设计特定的退化函数和恢复函数，实现了模式的确定性扩散和恢复。在此基础上，文章提出了一种确定性扩散模型（DDM），通过预定义的退化函数和线性扩散路径，实现了模式的快速生成和稳定训练。</p>
<p>(4) 实验验证：最后，文章通过实验验证了所提出方法的有效性。实验结果表明，该方法能够生成具有复杂结构的非局部模式，并在模式识别任务中取得较好的性能。此外，文章还展示了所提出方法在图像恢复和噪声去除等任务中的应用潜力。</p>
<ol>
<li>Conclusion:</li>
</ol>
<ul>
<li><p>(1) 这项工作的意义在于其对于非局部模式生成和处理的贡献。文章提出了一种结合卷积神经网络（CNN）和可逆自编码网络（IRAE）的方法，并应用于扩散模型，有效生成具有复杂结构的非局部模式，同时在模式识别、图像恢复和噪声去除等任务中展现出潜力。这项工作对于推动相关领域的发展具有重要意义。</p>
</li>
<li><p>(2) 创新点：文章结合了CNN和IRAE网络，通过BiFormer模块改进编码和解码过程，提高模型对非局部特征的处理能力。同时，利用可逆流基生成算法构建深度网络架构，实现了信息的完全保留和模式的可逆性。此外，文章还引入了控制网络（ControlNet）和确定性扩散模型（DDM）。</p>
<p>性能：文章通过实验验证了所提出方法的有效性，在模式生成和模式识别任务中取得较好性能。</p>
<p>工作量：文章详细介绍了实验设计、方法介绍、扩散模型的应用以及实验验证等方面，工作量较大，但内容表述清晰，逻辑连贯。</p>
</li>
</ul>
<details>
  <summary>点此查看论文截图</summary>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-fe04d7fdf3266f3ffd6fa3ac7aa38f8a.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-cb6a5af850064a0f58932641168ef19b.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-915db96f9d951e9b3f626c64ae0adeda.jpg" align="middle">
</details>




<h2 id="EasyControl-Transfer-ControlNet-to-Video-Diffusion-for-Controllable-Generation-and-Interpolation"><a href="#EasyControl-Transfer-ControlNet-to-Video-Diffusion-for-Controllable-Generation-and-Interpolation" class="headerlink" title="EasyControl: Transfer ControlNet to Video Diffusion for Controllable   Generation and Interpolation"></a>EasyControl: Transfer ControlNet to Video Diffusion for Controllable   Generation and Interpolation</h2><p><strong>Authors:Cong Wang, Jiaxi Gu, Panwen Hu, Haoyu Zhao, Yuanfan Guo, Jianhua Han, Hang Xu, Xiaodan Liang</strong></p>
<p>Following the advancements in text-guided image generation technology exemplified by Stable Diffusion, video generation is gaining increased attention in the academic community. However, relying solely on text guidance for video generation has serious limitations, as videos contain much richer content than images, especially in terms of motion. This information can hardly be adequately described with plain text. Fortunately, in computer vision, various visual representations can serve as additional control signals to guide generation. With the help of these signals, video generation can be controlled in finer detail, allowing for greater flexibility for different applications. Integrating various controls, however, is nontrivial. In this paper, we propose a universal framework called EasyControl. By propagating and injecting condition features through condition adapters, our method enables users to control video generation with a single condition map. With our framework, various conditions including raw pixels, depth, HED, etc., can be integrated into different Unet-based pre-trained video diffusion models at a low practical cost. We conduct comprehensive experiments on public datasets, and both quantitative and qualitative results indicate that our method outperforms state-of-the-art methods. EasyControl significantly improves various evaluation metrics across multiple validation datasets compared to previous works. Specifically, for the sketch-to-video generation task, EasyControl achieves an improvement of 152.0 on FVD and 19.9 on IS, respectively, in UCF101 compared with VideoComposer. For fidelity, our model demonstrates powerful image retention ability, resulting in high FVD and IS in UCF101 and MSR-VTT compared to other image-to-video models. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.13005v1">PDF</a> </p>
<p><strong>Summary</strong><br>在文本引导图像生成技术（如稳定扩散）的进展后，视频生成在学术界日益受到关注。然而，仅依赖文本指导视频生成存在严重局限性，视频的丰富内容难以用简单文本充分描述。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>视频生成比图像生成更复杂，尤其在动态内容方面。</li>
<li>计算机视觉中的各种视觉表示可以作为额外的控制信号来指导视频生成。</li>
<li>EasyControl 框架通过条件适配器传播和注入条件特征，使用户可以用单一条件图控制视频生成。</li>
<li>该框架能够集成不同的条件，如原始像素、深度、HED 等，以低成本应用于不同的基于Unet的预训练视频扩散模型。</li>
<li>EasyControl 在多个公共数据集上进行了全面实验，定量和定性结果表明其优于现有方法。</li>
<li>在sketch-to-video生成任务中，EasyControl在UCF101数据集上显著提升了FVD和IS指标。</li>
<li>在保真度方面，EasyControl在UCF101和MSR-VTT上表现出高的FVD和IS，相比其他图像到视频模型具有更强的图像保留能力。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>Title: EasyControl: Transfer ControlNet to Video Diffusion for Controllable Generation and Interpolation</li>
</ol>
<p>中文标题：EasyControl：ControlNet转移至视频扩散用于可控生成和插值</p>
<ol>
<li>Authors: Cong Wang, Jiaxi Gu, Panwen Hu, Haoyu Zhao, Yuanfan Guo, Jianhua Han, Hang Xu, and Xiaodan Liang</li>
</ol>
<p>作者：王聪、顾佳希、胡攀文、赵浩宇、郭远帆、韩建华、徐航、梁晓丹</p>
<ol>
<li>Affiliation: (Please note that the following is a placeholder, you should fill in the actual affiliations of the authors.)</li>
</ol>
<p>作者隶属：（请在此处填写作者的实际隶属）</p>
<ol>
<li>Keywords: Video Generation, Video Interpolation, Controllable Video Generation, Diffusion Model</li>
</ol>
<p>关键词：视频生成，视频插值，可控视频生成，扩散模型</p>
<ol>
<li>Urls: (Github code link if available, otherwise fill in “Github:None”)</li>
</ol>
<p>链接：（如有Github代码链接，请填写；否则填写“Github:None”）</p>
<ol>
<li><p>Summary:</p>
<ul>
<li><p>(1) 研究背景：随着文本引导的图像生成技术的发展，视频生成技术也受到了越来越多的关注。然而，仅依靠文本指导的视频生成存在严重局限性，因为视频内容比图像更丰富，特别是运动信息很难用纯文本描述。因此，研究一种能够更精细控制视频生成的方法具有重要意义。</p>
</li>
<li><p>(2) 过去的方法及问题：现有的视频生成方法主要依赖于文本指导或预定义的视觉表示作为控制信号。然而，这些方法在集成不同的控制信号时面临挑战，且难以处理复杂的文本描述和丰富的视频内容。因此，需要一种能够整合多种控制信号、适用于不同应用的更精细控制方法。</p>
</li>
<li><p>(3) 研究方法：本文提出了一种通用框架EasyControl，通过传播和注入条件适配器，使用户能够使用单个条件图控制视频生成。该框架可以整合各种条件，包括原始像素、深度、HED等，并轻松集成到不同的基于Unet的预训练视频扩散模型中。实验表明，该方法在多个公共数据集上的性能优于现有方法。</p>
</li>
<li><p>(4) 任务与性能：本文在公共数据集上进行了实验，结果表明EasyControl在视频生成和插值任务上取得了显著的性能提升。与现有工作相比，EasyControl在UCF101数据集上的FVD和IS评价指标上分别提高了152.0和19.9。此外，EasyControl还具有强大的图像保留能力，在UCF101和MSR-VTT数据集上的FVD和IS评价较高。这些结果表明，EasyControl在可控视频生成方面具有良好的性能和应用潜力。</p>
</li>
</ul>
</li>
</ol>
<ol>
<li><p>Conclusion:</p>
<ul>
<li>(1) 这项研究的意义在于解决当前视频生成技术的问题，通过引入EasyControl这一通用框架，实现了可控视频生成，提高了视频生成的质量和可控性，为视频生成领域带来了新的突破。</li>
<li>(2) 创新点：该文章的创新点在于提出了一种新的可控视频生成框架EasyControl，该框架能够整合多种控制信号，包括原始像素、深度、HED等，通过传播和注入条件适配器，使用户能够使用单个条件图控制视频生成。</li>
<li>性能：该文章在公共数据集上进行了实验，结果表明EasyControl在视频生成和插值任务上取得了显著的性能提升，与现有工作相比，在UCF101数据集上的FVD和IS评价指标上表现更优秀。</li>
<li>工作量：文章提出了一个新的框架和相应的技术方法，并进行了大量的实验验证，证明了其有效性和优越性。但是，文章未提供充分的细节，例如作者隶属、具体的实验数据和参数等，这些可能会影响到读者对该工作的深入理解和评估。</li>
</ul>
</li>
</ol>
<p>希望以上总结符合您的要求。</p>
<details>
  <summary>点此查看论文截图</summary>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-8e9b66c5aa3bf1d203170677f149fccd.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-ae3760ac538662583b77e45742d928a8.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-9507a4c314338044475eacee29c5e18b.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-c4cd1d82657cab6529eed1987f8b184a.jpg" align="middle">
</details>




<h2 id="When-Diffusion-MRI-Meets-Diffusion-Model-A-Novel-Deep-Generative-Model-for-Diffusion-MRI-Generation"><a href="#When-Diffusion-MRI-Meets-Diffusion-Model-A-Novel-Deep-Generative-Model-for-Diffusion-MRI-Generation" class="headerlink" title="When Diffusion MRI Meets Diffusion Model: A Novel Deep Generative Model   for Diffusion MRI Generation"></a>When Diffusion MRI Meets Diffusion Model: A Novel Deep Generative Model   for Diffusion MRI Generation</h2><p><strong>Authors:Xi Zhu, Wei Zhang, Yijie Li, Lauren J. O’Donnell, Fan Zhang</strong></p>
<p>Diffusion MRI (dMRI) is an advanced imaging technique characterizing tissue microstructure and white matter structural connectivity of the human brain. The demand for high-quality dMRI data is growing, driven by the need for better resolution and improved tissue contrast. However, acquiring high-quality dMRI data is expensive and time-consuming. In this context, deep generative modeling emerges as a promising solution to enhance image quality while minimizing acquisition costs and scanning time. In this study, we propose a novel generative approach to perform dMRI generation using deep diffusion models. It can generate high dimension (4D) and high resolution data preserving the gradients information and brain structure. We demonstrated our method through an image mapping task aimed at enhancing the quality of dMRI images from 3T to 7T. Our approach demonstrates highly enhanced performance in generating dMRI images when compared to the current state-of-the-art (SOTA) methods. This achievement underscores a substantial progression in enhancing dMRI quality, highlighting the potential of our novel generative approach to revolutionize dMRI imaging standards. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.12897v1">PDF</a> 11 pages, 3 figures</p>
<p><strong>Summary</strong><br>深度扩散模型为提升高质量dMRI图像提供了创新解决方案。</p>
<p><strong>Key Takeaways</strong>  </p>
<ul>
<li>dMRI是高级成像技术，用于表征人类大脑的组织微结构和白质结构连接。</li>
<li>高质量dMRI数据的需求增长，主要驱动因素包括更好的分辨率和改善组织对比度。</li>
<li>获得高质量dMRI数据昂贵且耗时。</li>
<li>深度生成建模作为解决方案，旨在提高图像质量同时降低成本和扫描时间。</li>
<li>提出了一种新的深度扩散模型生成方法，能够生成高维度（4D）和高分辨率数据，保留梯度信息和脑结构。</li>
<li>实验中使用3T到7T的图像映射任务验证方法，在生成dMRI图像质量方面表现出显著优越性。</li>
<li>该研究突破了当前技术水平，在提升dMRI质量方面具有重大潜力和意义。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li><p>Title: 当扩散模型与扩散MRI相遇：一种新型深度生成模型的应用研究</p>
</li>
<li><p>Authors: Xi Zhu（朱玺）, Wei Zhang（张伟）, Yijie Li（李艺洁）, Lauren J. O’Donnell（劳伦·J·奥唐奈）, Fan Zhang（张帆）等。</p>
</li>
<li><p>Affiliation: 第一作者Xi Zhu的所属单位为电子科技大学，位于中国的成都。其他作者所属单位未提及。</p>
</li>
<li><p>Keywords: Diffusion model（扩散模型）, Diffusion MRI（扩散MRI）, RISH feature（可能是指一种研究特性或特征）。</p>
</li>
<li><p>Urls: 论文链接未提供，GitHub代码链接未提供。</p>
</li>
<li><p>Summary:</p>
<ul>
<li><p>(1)研究背景：随着扩散成像技术的不断进步，对高质量扩散MRI数据的需求不断增长。但由于采集高质量数据需要先进的MRI扫描仪和采集协议，这使得在实际应用中难以获得高质量数据。因此，研究者提出使用机器学习来生成高质量的扩散MRI图像，以降低采集成本和缩短扫描时间。本研究在此背景下展开。</p>
</li>
<li><p>(2)过去的方法及问题：以往的方法在处理扩散MRI数据时可能存在分辨率低、信息丢失等问题。这些方法虽然能在一定程度上提高图像质量，但在生成高维、高分辨率数据时性能受限。因此，存在对更先进方法的迫切需求。</p>
</li>
<li><p>(3)研究方法：本研究提出了一种基于深度扩散模型的生成方法来进行扩散MRI生成。该方法能够生成高维（4D）和高分辨率数据，同时保留梯度信息和脑结构。该研究通过图像映射任务来增强从3T到7T的扩散MRI图像质量。通过与现有方法对比，该方法在生成扩散MRI图像方面表现出卓越性能。</p>
</li>
<li><p>(4)任务与性能：该研究旨在通过生成高质量扩散MRI图像来改进现有成像标准。通过对比实验，该方法在生成高维、高分辨率的扩散MRI图像时表现出良好性能，验证了该方法在提升扩散MRI质量方面的潜力。该方法的性能支持其目标的实现，为革命性改变扩散MRI成像标准提供了可能性。</p>
</li>
</ul>
</li>
<li><p>方法论概述：</p>
<ul>
<li><p>(1) 研究背景与问题定义：随着扩散成像技术的进步，高质量扩散MRI数据的需求不断增长。但由于采集高质量数据需要先进的MRI扫描仪和采集协议，使得实际应用中难以获得。因此，研究者提出使用机器学习来生成高质量的扩散MRI图像，以降低采集成本和缩短扫描时间。本研究旨在通过深度扩散模型生成方法改进现有成像标准，特别是解决生成高维、高分辨率的扩散MRI图像时的难题。</p>
</li>
<li><p>(2) 数据准备与预处理：研究使用了Human Connectome Project（HCP）提供的dMRI数据，共有1065名受试者的数据，其中171名有3T和7T的dMRI数据，894名只有3T数据。为了简化研究，仅使用了单壳b=1000的数据。此外，还对dMRI信号进行了表示和重建，使用了RISH特征，该特征可以适当地缩放dMRI信号而不改变纤维的主方向，并提供了一种紧凑且统一的dMRI数据表示方法。</p>
</li>
<li><p>(3) 方法概述：研究提出了一种基于深度生成模型的扩散MRI生成方法。首先，使用向量量化变分自编码器（VQ-VAE）将整脑图像压缩到潜在空间，并对其进行量化表示。为了解决3T和7T MRI数据集之间的差异，研究训练了两个单独的VQ-VAE模型。为了解决7T数据有限的问题，采用了迁移学习策略。然后，在潜在空间中进行扩散过程的数据生成，该过程可分为前向噪声过程和后向去噪过程。在前向过程中，通过添加噪声获得带噪声的潜在特征，在后向过程中，使用U-Net预测前一时间点的特征。在这个过程中，使用了类标签（3T和7T）来控制扩散模型的方向。通过控制生成过程，实现从3T到7T的扩散MRI图像生成。</p>
</li>
<li><p>(4) 技术细节：在模型的训练过程中，使用了均方误差损失（MSE loss）来优化U-Net的参数。在采样过程中，根据模型在步骤t的输出添加噪声来编码潜在特征。此外，还引入了类嵌入（class embeddings）和交叉注意力机制（cross-attention mechanism）来实现类标签对生成过程的控制。</p>
</li>
</ul>
</li>
</ol>
<ol>
<li><p>Conclusion:</p>
<ul>
<li><p>(1) 这项工作的意义在于提出了一种基于深度扩散模型的生成方法，旨在改进现有成像标准，特别是在生成高质量扩散MRI图像方面。该方法能够生成高维（4D）和高分辨率数据，同时保留梯度信息和脑结构，对于提高扩散MRI成像质量和降低采集成本具有重要意义。</p>
</li>
<li><p>(2) 创新点：该文章的创新之处在于将深度扩散模型应用于扩散MRI图像的生成，提出了一种基于VQ-VAE的扩散MRI生成方法，通过深度学习和图像处理技术的结合，实现了高维、高分辨率的扩散MRI图像生成。性能：该方法在生成扩散MRI图像方面表现出卓越性能，通过对比实验验证了其在提升扩散MRI质量方面的潜力。工作量：文章进行了充分的数据准备、预处理、方法设计和实验验证，工作量较大，但文章中未提及该方法的计算复杂度和运行时间，这可能对实际应用产生影响。</p>
</li>
</ul>
</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-64cc2af878677c8af1ac90f6d9ddb445.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-a62ab5274136cb2de2af28838073397a.jpg" align="middle">
</details>




<h2 id="GarmentAligner-Text-to-Garment-Generation-via-Retrieval-augmented-Multi-level-Corrections"><a href="#GarmentAligner-Text-to-Garment-Generation-via-Retrieval-augmented-Multi-level-Corrections" class="headerlink" title="GarmentAligner: Text-to-Garment Generation via Retrieval-augmented   Multi-level Corrections"></a>GarmentAligner: Text-to-Garment Generation via Retrieval-augmented   Multi-level Corrections</h2><p><strong>Authors:Shiyue Zhang, Zheng Chong, Xujie Zhang, Hanhui Li, Yuhao Cheng, Yiqiang Yan, Xiaodan Liang</strong></p>
<p>General text-to-image models bring revolutionary innovation to the fields of arts, design, and media. However, when applied to garment generation, even the state-of-the-art text-to-image models suffer from fine-grained semantic misalignment, particularly concerning the quantity, position, and interrelations of garment components. Addressing this, we propose GarmentAligner, a text-to-garment diffusion model trained with retrieval-augmented multi-level corrections. To achieve semantic alignment at the component level, we introduce an automatic component extraction pipeline to obtain spatial and quantitative information of garment components from corresponding images and captions. Subsequently, to exploit component relationships within the garment images, we construct retrieval subsets for each garment by retrieval augmentation based on component-level similarity ranking and conduct contrastive learning to enhance the model perception of components from positive and negative samples. To further enhance the alignment of components across semantic, spatial, and quantitative granularities, we propose the utilization of multi-level correction losses that leverage detailed component information. The experimental findings demonstrate that GarmentAligner achieves superior fidelity and fine-grained semantic alignment when compared to existing competitors. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.12352v2">PDF</a> Accepted by ECCV 2024</p>
<p><strong>Summary</strong><br>文本生成图像模型在艺术、设计和媒体领域带来了革命性创新，但在服装生成中存在细粒度语义错位问题。GarmentAligner通过多级修正实现了服装组件的语义对齐，提升了模型的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>文本生成图像模型在艺术、设计和媒体领域有革命性创新。</li>
<li>在服装生成领域，现有的文本生成图像模型在服装组件的数量、位置和关系上存在语义错位问题。</li>
<li>GarmentAligner 是一种针对服装生成的文本到服装的扩散模型。</li>
<li>GarmentAligner 引入了自动组件提取管道，从图像和标题中获取服装组件的空间和数量信息。</li>
<li>为了利用服装图像中的组件关系，GarmentAligner 构建了检索子集，并进行了对比学习以增强模型对组件的感知能力。</li>
<li>提出了多级修正损失，利用详细的组件信息增强了组件在语义、空间和数量上的对齐性。</li>
<li>实验结果显示，GarmentAligner 在细节和语义上都优于现有竞争对手。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li><p>Title: 基于检索增强的多层次修正的服装文本生成研究（GarmentAligner: Text-to-Garment Generation via Retrieval-augmented Multi-level Corrections）</p>
</li>
<li><p>Authors: 张志远（Shiyue Zhang）、钟正（Zheng Chong）、张旭杰（Xujie Zhang）、李慧楠（Hanhui Li）、程宇航（Yuhao Cheng）、颜毅强（Yiqiang Yan）和梁晓丹（Xiaodan Liang）。</p>
</li>
<li><p>Affiliation: 第一作者张志远（Shiyue Zhang）等人的隶属单位为中山大学深圳校区（Shenzhen Campus of Sun Yat-sen University，深圳，中国）。</p>
</li>
<li><p>Keywords: 文本生成、服装设计、语义对齐、多层次修正、检索增强（Text generation, Clothing design, Semantic alignment, Multi-level correction, Retrieval augmentation）。</p>
</li>
<li><p>Urls: 由于我无法直接提供论文的链接，您可以在学术搜索引擎中搜索论文标题或作者姓名以找到相关链接。至于代码链接，您可以在GitHub上搜索相关论文名称或团队名称以获取可能的代码仓库链接（GitHub link: 请在GitHub上搜索相关资源）。</p>
</li>
<li><p>Summary: </p>
<ul>
<li><p>(1)研究背景：随着文本到图像生成模型的快速发展，其在服装领域的应用变得日益重要。然而，现有模型在生成服装图像时存在精细语义对齐的问题，特别是在服装部件的数量、位置和相互关系上。因此，本文旨在解决这一问题。</p>
</li>
<li><p>(2)过去的方法及问题：以往的研究主要关注文本到图像的通用模型，但在应用于服装生成时，它们面临着精细语义对齐的挑战。特别是在服装部件的数量、位置和相互关系方面，现有的模型常常无法准确表达文本描述中的细节。</p>
</li>
<li><p>(3)研究方法：针对上述问题，本文提出了GarmentAligner模型。该模型通过引入自动部件提取管道，从对应的图像和文本描述中获取部件的空间和数量信息。此外，为了利用服装图像中的部件关系，研究提出了基于部件级别相似度排名的检索增强方法，并通过对比学习增强模型对正负面样本的感知能力。最后，为了增强语义、空间和数量粒度上的部件对齐，研究还提出了相应的策略。</p>
</li>
<li><p>(4)任务与性能：本文的方法在服装生成任务上取得了显著成果，能够生成高质量、准确描绘文本描述中部件数量和空间位置的服装图像。通过对比实验和性能指标评估，证明该方法在解决精细语义对齐问题上的有效性。性能结果支持了该方法的目标，即生成与文本描述高度匹配的服装图像。</p>
</li>
</ul>
</li>
<li><p>方法：</p>
<ul>
<li><p>(1) 研究首先引入了自动部件提取管道，从对应的图像和文本描述中获取部件的空间和数量信息。这一步骤是为了解决现有模型在服装部件细节上的语义对齐问题。</p>
</li>
<li><p>(2) 针对服装图像中的部件关系，研究提出了基于部件级别相似度排名的检索增强方法。该方法利用对比学习技术增强模型对正负面样本的感知能力，以提高模型的生成性能。</p>
</li>
<li><p>(3) 为了增强语义、空间和数量粒度上的部件对齐，研究还采取了相应的策略。这可能包括使用多层次的修正方法，结合检索结果对生成的服装图像进行精细化调整，确保生成的图像与文本描述高度匹配。</p>
</li>
<li><p>(4) 最后，该研究通过对比实验和性能指标评估，验证了所提出方法在服装生成任务上的有效性。具体来说，就是通过与其他方法对比，证明该方法在生成与文本描述高度匹配的服装图像方面的优越性。</p>
</li>
</ul>
</li>
</ol>
<p>以上就是对该研究方法的简要概述。研究通过引入自动部件提取、检索增强技术和多层次修正策略，旨在解决现有模型在生成服装图像时面临的精细语义对齐问题。</p>
<ol>
<li>Conclusion: </li>
</ol>
<ul>
<li>(1)工作意义：该研究针对文本到服装图像生成过程中的精细语义对齐问题，提出了一种基于检索增强的多层次修正的服装文本生成方法，具有重要的学术价值和应用前景。该研究能够提升服装设计的自动化水平，为服装产业带来创新。</li>
<li>(2)创新点、性能、工作量评价：<ul>
<li>创新点：该研究引入了自动部件提取管道，解决了现有模型在服装部件细节上的语义对齐问题；提出了基于部件级别相似度排名的检索增强方法，提高了模型对正负面样本的感知能力；采取了相应的策略增强语义、空间和数量粒度上的部件对齐。</li>
<li>性能：通过对比实验和性能指标评估，验证了所提出方法在服装生成任务上的有效性，生成了高质量、准确描绘文本描述中部件数量和空间位置的服装图像。</li>
<li>工作量：文章工作量较大，涉及的方法较为复杂，实现了从理论到实践的转化。然而，文章未详细阐述实验数据的规模以及实验的具体细节，这是其略微不足之处。</li>
</ul>
</li>
</ul>
<details>
  <summary>点此查看论文截图</summary>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-05251b45e6018ee8a79e676bcb68426d.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-f7360ff1eb5ed5c20e07d9432a1ef815.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-1277a5e5ff3e6b47a055dde4dc80c40f.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-684f45ad5f36fdb085c8ecff69be7d4e.jpg" align="middle">
</details>




<h2 id="Classifier-Free-Guidance-is-a-Predictor-Corrector"><a href="#Classifier-Free-Guidance-is-a-Predictor-Corrector" class="headerlink" title="Classifier-Free Guidance is a Predictor-Corrector"></a>Classifier-Free Guidance is a Predictor-Corrector</h2><p><strong>Authors:Arwen Bradley, Preetum Nakkiran</strong></p>
<p>We investigate the theoretical foundations of classifier-free guidance (CFG). CFG is the dominant method of conditional sampling for text-to-image diffusion models, yet unlike other aspects of diffusion, it remains on shaky theoretical footing. In this paper, we disprove common misconceptions, by showing that CFG interacts differently with DDPM (Ho et al., 2020) and DDIM (Song et al., 2021), and neither sampler with CFG generates the gamma-powered distribution $p(x|c)^\gamma p(x)^{1-\gamma}$. Then, we clarify the behavior of CFG by showing that it is a kind of predictor-corrector method (Song et al., 2020) that alternates between denoising and sharpening, which we call predictor-corrector guidance (PCG). We prove that in the SDE limit, CFG is actually equivalent to combining a DDIM predictor for the conditional distribution together with a Langevin dynamics corrector for a gamma-powered distribution (with a carefully chosen gamma). Our work thus provides a lens to theoretically understand CFG by embedding it in a broader design space of principled sampling methods. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.09000v2">PDF</a> AB and PN contributed equally. v2: Fixed typos</p>
<p><strong>Summary</strong><br>本文探讨了无分类器引导（CFG）在文本到图像扩散模型中的理论基础及其与其他扩散方法的关系。</p>
<p><strong>Key Takeaways</strong>  </p>
<ul>
<li>CFG在文本到图像扩散模型中占主导地位，但其理论基础不稳固。</li>
<li>CFG与DDPM和DDIM交互作用不同，且与这些采样器生成的分布不同。</li>
<li>CFG被阐明为一种预测-校正方法，交替进行去噪和锐化。</li>
<li>在SDE极限下，CFG等效于结合DDIM预测器和伽马功率分布的Langevin动力学校正器。</li>
<li>本文通过嵌入更广泛的采样方法设计空间，为理论理解CFG提供了视角。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li><p><strong>标题</strong>： 分类器无关引导是预测校正器（Classifier-Free Guidance as a Predictor-Corrector）</p>
</li>
<li><p><strong>作者</strong>： Arwen Bradley 和 Preetum Nakkiran（均为Apple公司）。</p>
</li>
<li><p><strong>作者所属机构</strong>： 苹果公司的机器学习团队。</p>
</li>
<li><p><strong>关键词</strong>： 分类器无关引导（Classifier-Free Guidance）、预测校正器（Predictor-Corrector）、文本到图像扩散模型（Text-to-Image Diffusion Models）、采样方法（Sampling Methods）。</p>
</li>
<li><p><strong>链接</strong>： 文章抽象和介绍：Url链接（请替换为实际的论文链接）和GitHub代码链接（如果可用，否则填写“GitHub：无”）。</p>
</li>
<li><p><strong>摘要</strong>： </p>
<ul>
<li><p>(1)研究背景：本文主要研究文本到图像扩散模型的条件采样中的分类器无关引导（CFG）的理论基础。虽然CFG是现代扩散模型尤其是文本到图像应用中的关键部分，但其理论基础并不稳固。</p>
</li>
<li><p>(2)过去的方法及问题：传统的条件采样方法通常不针对扩散模型进行优化，导致生成的样本在视觉上不连贯，不符合提示。分类器引导作为一种改进方法被引入，但它在某些情况下并不理想。</p>
</li>
<li><p>(3)研究方法：本文首先纠正了关于CFG与DDPM和DDIM交互的常见误解。然后，本文展示了CFG是一种预测校正器方法，它在去噪和锐化之间交替进行。在SDE（随机微分方程）的极限下，本文证明了CFG实际上等于结合DDIM预测器的条件分布以及一个针对gamma幂分布（具有精心选择的gamma值）的Langevin动力学校正器。</p>
</li>
<li><p>(4)任务与性能：本文的理论分析和证明为理解CFG在文本到图像扩散模型中的工作原理提供了视角，并通过实验证明了其有效性。文章提出的方法提高了条件采样的质量，生成了更加连贯的样本，支持了其研究目标。性能结果证明了方法的有效性。</p>
</li>
</ul>
</li>
</ol>
<p>希望这个摘要符合您的要求！</p>
<ol>
<li><p>Conclusion: </p>
<ul>
<li><p>(1)这篇文章的研究对于理解分类器无关引导（Classifier-Free Guidance）在文本到图像扩散模型中的工作原理具有重要意义。</p>
</li>
<li><p>(2)创新点：文章纠正了关于分类器无关引导与DDPM和DDIM交互的常见误解，并将其定位为预测校正器方法。文章的理论分析和证明为理解CFG的工作原理提供了新视角。<br>性能：文章通过实验证明了分类器无关引导能够提高条件采样的质量，生成更加连贯的样本。<br>工作量：文章对分类器无关引导进行了深入的理论分析，并通过实验验证了其有效性，工作量较大。</p>
</li>
</ul>
</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-51e3540f034d6363b9a9404326bef0da.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-806fc1c8232da95cacb188f0dc1e33f3.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-4bdf28cff7cd2827cf3974216f1ca5d0.jpg" align="middle">
</details>




<h2 id="LaWa-Using-Latent-Space-for-In-Generation-Image-Watermarking"><a href="#LaWa-Using-Latent-Space-for-In-Generation-Image-Watermarking" class="headerlink" title="LaWa: Using Latent Space for In-Generation Image Watermarking"></a>LaWa: Using Latent Space for In-Generation Image Watermarking</h2><p><strong>Authors:Ahmad Rezaei, Mohammad Akbari, Saeed Ranjbar Alvar, Arezou Fatemi, Yong Zhang</strong></p>
<p>With generative models producing high quality images that are indistinguishable from real ones, there is growing concern regarding the malicious usage of AI-generated images. Imperceptible image watermarking is one viable solution towards such concerns. Prior watermarking methods map the image to a latent space for adding the watermark. Moreover, Latent Diffusion Models (LDM) generate the image in the latent space of a pre-trained autoencoder. We argue that this latent space can be used to integrate watermarking into the generation process. To this end, we present LaWa, an in-generation image watermarking method designed for LDMs. By using coarse-to-fine watermark embedding modules, LaWa modifies the latent space of pre-trained autoencoders and achieves high robustness against a wide range of image transformations while preserving perceptual quality of the image. We show that LaWa can also be used as a general image watermarking method. Through extensive experiments, we demonstrate that LaWa outperforms previous works in perceptual quality, robustness against attacks, and computational complexity, while having very low false positive rate. Code is available here. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.05868v2">PDF</a> Accepted to ECCV 2024</p>
<p><strong>Summary</strong><br>生成模型产生的高质量图像与真实图像难以区分，引发了对恶意使用AI生成图像的担忧。潜隐性图像水印技术成为解决方案之一。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>生成模型能够生成与真实图像难以区分的高质量图像，引发了对恶意使用的担忧。</li>
<li>潜隐性图像水印技术可以通过在潜变空间中嵌入水印来保护图像免受恶意使用。</li>
<li>先前的水印方法将图像映射到潜变空间以添加水印。</li>
<li>潜隐性扩散模型（LDM）在预训练自动编码器的潜变空间中生成图像。</li>
<li>潜变空间可用于整合水印到生成过程中。</li>
<li>LaWa是一种适用于LDM的生成图像水印方法，通过粗到细的嵌入模块修改自动编码器的潜变空间，实现了高鲁棒性和感知质量的保留。</li>
<li>LaWa在感知质量、抗攻击鲁棒性和计算复杂性等方面优于先前的方法，并具有极低的误检率。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li><strong>标题</strong>：LaWa: 利用潜在空间进行生成中的图像水印技术。中文翻译：拉瓦：利用潜在空间进行图像生成中的水印技术。</li>
<li><strong>作者</strong>：作者包括Ahmad Rezaei, Mohammad Akbari, Saeed Ranjbar Alvar, Arezou Fatemi以及Yong Zhang。所有作者名字使用英文表示。</li>
<li><strong>第一作者所属机构</strong>：第一作者Affiliation为University of British Columbia。中文翻译：不列颠哥伦比亚大学。</li>
<li><strong>关键词</strong>：Image Watermarking（图像水印）、Responsible AI（负责任的人工智能）、Image Generation（图像生成）。</li>
<li><strong>链接</strong>：文章链接地址和GitHub代码链接请参照您提供的Abstract部分中的相关链接。如果GitHub代码链接不可用，可以填写“Github:None”。</li>
<li><p><strong>摘要</strong>：</p>
<ul>
<li><strong>(1)</strong>研究背景**：随着生成模型产生的高质量图像与真实图像难以区分，人们越来越担忧AI生成图像的恶意使用。针对这一问题，隐形图像水印是一种可行的解决方案。本文探讨在生成过程中集成水印的方法，特别是在潜在空间中进行水印嵌入的新方法。</li>
<li><strong>(2)</strong>过去的方法及问题**：以往的水印方法通常将图像映射到潜在空间以添加水印。然而，这些方法在计算复杂性和鲁棒性方面存在不足，同时可能降低图像的感知质量。本文提出的方法动机在于解决这些问题，特别是在潜在扩散模型（LDM）的生成过程中集成水印。</li>
<li><strong>(3)</strong>研究方法**：本研究提出了一种名为LaWa的生成中图像水印方法，专为LDM设计。通过粗细结合的水印嵌入模块，LaWa修改预训练自编码器的潜在空间，实现了对一系列图像变换的高鲁棒性，同时保持了图像的感知质量。</li>
<li><strong>(4)</strong>任务与性能**：本研究在图像水印任务上进行了实验，并证明LaWa在感知质量、对抗攻击的鲁棒性和计算复杂性方面均优于以前的方法，同时拥有极低的误报率。这些性能表明LaWa方法能有效支持其目标应用。</li>
</ul>
</li>
</ol>
<p>请注意，以上回答是基于您提供的论文摘要和引言信息进行的概括，具体的实验细节和性能分析需要参考完整的论文内容。</p>
<ol>
<li>方法论：</li>
</ol>
<p>这篇论文提出了一种名为LaWa的图像水印方法，其主要思想是利用潜在空间在图像生成过程中嵌入水印。具体步骤如下：</p>
<ul>
<li>(1) 研究背景：针对生成模型产生的图像与真实图像难以区分的问题，隐形图像水印是一种可行的解决方案。本文探讨在生成过程中集成水印的方法，特别是在潜在空间中进行水印嵌入的新方法。</li>
<li>(2) 方法介绍：本研究提出了一种名为LaWa的生成中图像水印方法，专为LDM（潜在扩散模型）设计。该方法通过粗细结合的水印嵌入模块，修改预训练自编码器的潜在空间，实现对一系列图像变换的高鲁棒性，同时保持图像的感知质量。LaWa嵌入水印的过程是通过潜在空间实现的，即利用潜在空间特征对图像进行多尺度粗到细的嵌入过程。这一过程还包括修改原始解码器、设计新的解码器和提取器等关键步骤。在此基础上引入了损失函数和优化策略以提高模型的性能。具体而言包括结合像素级的失真和感知损失函数，使用对抗性训练来提高水印图像的质量等。最后利用训练好的模型进行水印提取和匹配。在这个过程中采用了对抗性损失等策略来提高提取的准确性。通过调整不同的损失权重来实现对模型性能的优化。最后通过实验验证了该方法的有效性，相较于过去的方法，LaWa方法在感知质量、鲁棒性和计算复杂性方面均有显著提升。这表明该模型能更有效地支持实际应用中的目标应用。总的来说，该论文提出了一种新的图像水印方法，通过修改图像的潜在空间实现水印的嵌入和提取，并通过实验验证了其有效性。</li>
</ul>
<ol>
<li>Conclusion: </li>
</ol>
<ul>
<li><strong>(1)</strong> 工作意义：该论文针对生成模型产生的图像与真实图像难以区分的问题，提出了一种在图像生成过程中嵌入水印的新方法。这对于保护版权、防止AI生成图像被恶意使用具有重要意义。</li>
<li><strong>(2)</strong> 优缺点概述：</li>
</ul>
<pre><code>+ 创新点：论文提出了名为LaWa的图像水印方法，利用潜在空间进行水印嵌入，实现了在图像生成过程中的水印集成。该方法具有新颖性和创新性，克服了传统水印方法在计算复杂性和鲁棒性方面的不足。
+ 性能：实验表明，LaWa方法在感知质量、对抗攻击的鲁棒性和计算复杂性方面均优于过去的方法。这意味着LaWa方法在实际应用中具有更好的性能表现。
+ 工作量：论文详细介绍了LaWa方法的理论基础、实验设计和实验结果，工作量较大。同时，论文对相关工作进行了全面的调研和对比分析，为后续研究提供了有益的参考。
</code></pre><details>
  <summary>点此查看论文截图</summary>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-2a0a8a9316ebae19f79594289619c202.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-8c7d899f31e99be6582f02c42ecd22d9.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-6fbea1d802bc4934f95f063d2242a526.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-5f01d92700a1f2c299a8452272c3ac9b.jpg" align="middle">
</details>




</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://kedreamix.github.io/2024/08/27/Paper/2024-08-27/Diffusion%20Models/">https://kedreamix.github.io/2024/08/27/Paper/2024-08-27/Diffusion Models/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Diffusion-Models/">Diffusion Models</a></div><div class="post_share"><div class="social-share" data-image="https://picx.zhimg.com/v2-806fc1c8232da95cacb188f0dc1e33f3.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/08/28/Paper/2024-08-28/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙/虚拟人"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-fb38b82805491f3b8b63bc866361c519.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">元宇宙/虚拟人</div></div></a></div><div class="next-post pull-right"><a href="/2024/08/27/Paper/2024-08-27/NeRF/" title="NeRF"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-49e114eb9054bd8c4253d32bdf3bcc60.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">NeRF</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/03/03/Paperscape/EMO/" title="EMO Emote Portrait Alive - 阿里HumanAIGC"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-6492e24fb03ffa98135dc584535ab7d9.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-03</div><div class="title">EMO Emote Portrait Alive - 阿里HumanAIGC</div></div></a></div><div><a href="/2024/01/24/Paper/2024-01-24/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-71a37c439c6714e8867560f580599d2f.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-24</div><div class="title">Diffusion Models</div></div></a></div><div><a href="/2024/01/30/Paper/2024-01-30/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-e55358c77a9d65f15701e8f33262e2a4.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-30</div><div class="title">Diffusion Models</div></div></a></div><div><a href="/2024/02/09/Paper/2024-02-09/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-32488f736ee10537497afccc3a1a1d76.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-09</div><div class="title">Diffusion Models</div></div></a></div><div><a href="/2024/02/02/Paper/2024-02-02/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-5920453c69c00995f18077b22d4a790e.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-02</div><div class="title">Diffusion Models</div></div></a></div><div><a href="/2024/02/23/Paper/2024-02-23/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-ff425802a32a4519e30b9044a3eed1e8.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-23</div><div class="title">Diffusion Models</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-08-27-%E6%9B%B4%E6%96%B0"><span class="toc-text">2024-08-27 更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#How-Diffusion-Models-Learn-to-Factorize-and-Compose"><span class="toc-text">How Diffusion Models Learn to Factorize and Compose</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CustomCrafter-Customized-Video-Generation-with-Preserving-Motion-and-Concept-Composition-Abilities"><span class="toc-text">CustomCrafter: Customized Video Generation with Preserving Motion and   Concept Composition Abilities</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#General-Intelligent-Imaging-and-Uncertainty-Quantification-by-Deterministic-Diffusion-Model"><span class="toc-text">General Intelligent Imaging and Uncertainty Quantification by   Deterministic Diffusion Model</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#EasyControl-Transfer-ControlNet-to-Video-Diffusion-for-Controllable-Generation-and-Interpolation"><span class="toc-text">EasyControl: Transfer ControlNet to Video Diffusion for Controllable   Generation and Interpolation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#When-Diffusion-MRI-Meets-Diffusion-Model-A-Novel-Deep-Generative-Model-for-Diffusion-MRI-Generation"><span class="toc-text">When Diffusion MRI Meets Diffusion Model: A Novel Deep Generative Model   for Diffusion MRI Generation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#GarmentAligner-Text-to-Garment-Generation-via-Retrieval-augmented-Multi-level-Corrections"><span class="toc-text">GarmentAligner: Text-to-Garment Generation via Retrieval-augmented   Multi-level Corrections</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Classifier-Free-Guidance-is-a-Predictor-Corrector"><span class="toc-text">Classifier-Free Guidance is a Predictor-Corrector</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LaWa-Using-Latent-Space-for-In-Generation-Image-Watermarking"><span class="toc-text">LaWa: Using Latent Space for In-Generation Image Watermarking</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image: url('https://picx.zhimg.com/v2-806fc1c8232da95cacb188f0dc1e33f3.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><!-- hexo injector body_end start --><script data-pjax>
  function butterfly_swiper_injector_config(){
    var parent_div_git = document.getElementById('recent-posts');
    var item_html = '<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/25/Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="2024/01/25/Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="2024/01/25/Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/17/Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023/12/17/Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="2023/12/17/Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>';
    console.log('已挂载butterfly_swiper')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'undefined'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_swiper_injector_config();
  }
  else if (epage === cpage){
    butterfly_swiper_injector_config();
  }
  </script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script><!-- hexo injector body_end end --></body></html>