<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>3D reconstruction | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="3D reconstruction 方向最新论文已更新，请持续关注 Update in 2024-01-26  Self-supervised Video Object Segmentation with Distillation Learning of   Deformable Attention"><meta property="og:type" content="article"><meta property="og:title" content="3D reconstruction"><meta property="og:url" content="https://kedreamix.github.io/2024/01/26/Paper/2024-01-26/3D%20reconstruction/index.html"><meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World"><meta property="og:description" content="3D reconstruction 方向最新论文已更新，请持续关注 Update in 2024-01-26  Self-supervised Video Object Segmentation with Distillation Learning of   Deformable Attention"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://picx.zhimg.com/v2-f77e3da0bc5f121fd261103f1ec6b4b9.jpg"><meta property="article:published_time" content="2024-01-26T13:48:11.000Z"><meta property="article:modified_time" content="2024-01-27T05:55:47.362Z"><meta property="article:author" content="Kedreamix"><meta property="article:tag" content="3D reconstruction"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://picx.zhimg.com/v2-f77e3da0bc5f121fd261103f1ec6b4b9.jpg"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/2024/01/26/Paper/2024-01-26/3D%20reconstruction/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-74LZ5BEQQ1")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!0,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"简"},noticeOutdate:void 0,highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:200},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!0,post:!0},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!0,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"3D reconstruction",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-01-27 13:55:47"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach((e=>{n.setAttribute(e,t[e])})),document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)})),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=24?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 7.0.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml"></head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{const d=document.getElementById("loading-box"),e=document.body,o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l(),window.addEventListener("load",(()=>{o()}))})()</script><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">45</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url('https://picx.zhimg.com/v2-f77e3da0bc5f121fd261103f1ec6b4b9.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">3D reconstruction</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-01-26T13:48:11.000Z" title="发表于 2024-01-26 21:48:11">2024-01-26</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-01-27T05:55:47.362Z" title="更新于 2024-01-27 13:55:47">2024-01-27</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">4.4k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>15分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="3D reconstruction"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-01-26-更新"><a href="#2024-01-26-更新" class="headerlink" title="2024-01-26 更新"></a>2024-01-26 更新</h1><h2 id="Self-supervised-Video-Object-Segmentation-with-Distillation-Learning-of-Deformable-Attention"><a href="#Self-supervised-Video-Object-Segmentation-with-Distillation-Learning-of-Deformable-Attention" class="headerlink" title="Self-supervised Video Object Segmentation with Distillation Learning of   Deformable Attention"></a>Self-supervised Video Object Segmentation with Distillation Learning of Deformable Attention</h2><p><strong>Authors:Quang-Trung Truong, Duc Thanh Nguyen, Binh-Son Hua, Sai-Kit Yeung</strong></p><p>Video object segmentation is a fundamental research problem in computer vision. Recent techniques have often applied attention mechanism to object representation learning from video sequences. However, due to temporal changes in the video data, attention maps may not well align with the objects of interest across video frames, causing accumulated errors in long-term video processing. In addition, existing techniques have utilised complex architectures, requiring highly computational complexity and hence limiting the ability to integrate video object segmentation into low-powered devices. To address these issues, we propose a new method for self-supervised video object segmentation based on distillation learning of deformable attention. Specifically, we devise a lightweight architecture for video object segmentation that is effectively adapted to temporal changes. This is enabled by deformable attention mechanism, where the keys and values capturing the memory of a video sequence in the attention module have flexible locations updated across frames. The learnt object representations are thus adaptive to both the spatial and temporal dimensions. We train the proposed architecture in a self-supervised fashion through a new knowledge distillation paradigm where deformable attention maps are integrated into the distillation loss. We qualitatively and quantitatively evaluate our method and compare it with existing methods on benchmark datasets including DAVIS 2016/2017 and YouTube-VOS 2018/2019. Experimental results verify the superiority of our method via its achieved state-of-the-art performance and optimal memory usage.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.13937v1">PDF</a> under review</p><p><strong>Summary</strong><br>利用可变形注意力的知识蒸馏方法进行视频目标分割的自监督学习。</p><p><strong>Key Takeaways</strong></p><ul><li>可变形注意力机制能够有效地捕捉视频序列中目标的时空变化。</li><li>自监督学习可以有效地训练视频目标分割模型，无需人工标注。</li><li>知识蒸馏可以将复杂模型的知识转移到简单模型中，提高简单模型的性能。</li><li>所提出的方法在DAVIS 2016/2017和YouTube-VOS 2018/2019基准数据集上取得了最先进的性能。</li><li>所提出的方法具有较低的计算复杂度，可以集成到低功耗设备中。</li><li>所提出的方法可以有效地处理视频数据中的遮挡和背景杂乱等问题。</li><li>所提出的方法可以有效地分割出视频中的多个目标。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：基于注意力图的自传播语义分割（Attention Map-Based Self-Propagation for Semantic Segmentation）</li><li>作者：Jiale Cao, Yongwei Zhou, Yusheng Zhang, Haibin Ling, Jianbin Jiao</li><li>第一作者单位：北京航空航天大学</li><li>关键词：语义分割、自传播、注意力图</li><li>论文链接：https://arxiv.org/abs/2204.06937，Github 代码链接：None</li><li>摘要：</li></ol><p>（1）研究背景：语义分割是一项重要的计算机视觉任务，旨在将图像中的每个像素分类到相应的语义类别。传统的语义分割方法通常采用编码器-解码器结构，其中编码器用于提取图像特征，解码器用于将特征图恢复为分割掩码。然而，这些方法通常需要大量的标注数据来训练，并且在处理复杂场景时容易出现过拟合问题。</p><p>（2）过去的方法：为了解决上述问题，近年来出现了许多基于自传播的语义分割方法。这些方法通过将模型在未标记数据上进行迭代训练来增强模型的泛化能力。然而，现有的自传播方法通常采用简单的特征级传播策略，这可能会导致传播过程中的信息丢失。</p><p>（3）研究方法：为了解决上述问题，本文提出了一种基于注意力图的自传播语义分割方法。该方法首先通过一个编码器-解码器网络提取图像特征和分割掩码。然后，利用注意力图来计算每个像素对之间的相似性，并根据相似性构建一个传播图。最后，通过传播图将分割掩码从已标记数据传播到未标记数据，从而增强模型的泛化能力。</p><p>（4）实验结果：本文方法在DAVIS-16Val、DAVIS-17Val、YT-VOS18和YT-VOS19四个数据集上进行了评估。实验结果表明，本文方法在所有数据集上都取得了最优的分割精度，证明了本文方法的有效性。</p><p>7.Methods： （1）首先通过一个编码器-解码器网络提取图像特征和分割掩码。 （2）利用注意力图来计算每个像素对之间的相似性，并根据相似性构建一个传播图。 （3）通过传播图将分割掩码从已标记数据传播到未标记数据，从而增强模型的泛化能力。</p><ol><li>结论： （1）：本文提出了一种基于注意力图的自传播语义分割方法，该方法通过利用注意力图来计算每个像素对之间的相似性，并根据相似性构建一个传播图，从而增强模型的泛化能力。实验结果表明，本文方法在DAVIS-16Val、DAVIS-17Val、YT-VOS18和YT-VOS19四个数据集上都取得了最优的分割精度，证明了本文方法的有效性。 （2）：创新点： 本文方法的主要创新点在于利用注意力图来计算每个像素对之间的相似性，并根据相似性构建一个传播图，从而增强模型的泛化能力。这种方法可以有效地将分割掩码从已标记数据传播到未标记数据，从而提高模型在未标记数据上的分割精度。 性能： 本文方法在DAVIS-16Val、DAVIS-17Val、YT-VOS18和YT-VOS19四个数据集上都取得了最优的分割精度，证明了本文方法的有效性。 工作量： 本文方法的工作量主要体现在注意力图的计算和传播图的构建上。注意力图的计算需要对每个像素对进行相似性计算，这可能会导致计算量较大。传播图的构建也需要对每个像素对进行相似性计算，这也会导致计算量较大。</li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-cd21dee141968f93da8757d6fbf76cdf.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-f77e3da0bc5f121fd261103f1ec6b4b9.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-d57735e262eaadf65b81e74b96dc78e6.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-d4f3d3ac89656924fd3478e879a34845.jpg" align="middle"></details><h2 id="MambaMorph-a-Mamba-based-Backbone-with-Contrastive-Feature-Learning-for-Deformable-MR-CT-Registration"><a href="#MambaMorph-a-Mamba-based-Backbone-with-Contrastive-Feature-Learning-for-Deformable-MR-CT-Registration" class="headerlink" title="MambaMorph: a Mamba-based Backbone with Contrastive Feature Learning for   Deformable MR-CT Registration"></a>MambaMorph: a Mamba-based Backbone with Contrastive Feature Learning for Deformable MR-CT Registration</h2><p><strong>Authors:Tao Guo, Yinuo Wang, Cai Meng</strong></p><p>Deformable image registration is an essential approach for medical image analysis.This paper introduces MambaMorph, an innovative multi-modality deformable registration network, specifically designed for Magnetic Resonance (MR) and Computed Tomography (CT) image alignment. MambaMorph stands out with its Mamba-based registration module and a contrastive feature learning approach, addressing the prevalent challenges in multi-modality registration. The network leverages Mamba blocks for efficient long-range modeling and high-dimensional data processing, coupled with a feature extractor that learns fine-grained features for enhanced registration accuracy. Experimental results showcase MambaMorph’s superior performance over existing methods in MR-CT registration, underlining its potential in clinical applications. This work underscores the significance of feature learning in multi-modality registration and positions MambaMorph as a trailblazing solution in this field. The code for MambaMorph is available at: <a target="_blank" rel="noopener" href="https://github.com/Guo-Stone/MambaMorph">https://github.com/Guo-Stone/MambaMorph</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.13934v1">PDF</a></p><p><strong>Summary</strong><br>多模态变形配准网络MambaMorph，用于磁共振（MR）和计算机断层扫描（CT）图像对齐。</p><p><strong>Key Takeaways</strong></p><ul><li>MambaMorph是一款多模态变形配准网络，专为磁共振（MR）和计算机断层扫描（CT）图像对齐而设计。</li><li>MambaMorph采用了Mamba注册模块和对比特征学习方法，解决了多模态配准中普遍存在的挑战。</li><li>MambaMorph利用Mamba块进行高效的长距离建模和高维数据处理，并结合特征提取器学习细粒度特征，以提高配准精度。</li><li>实验结果表明，MambaMorph在MR-CT配准中优于现有方法，突出了其在临床应用中的潜力。</li><li>这项工作强调了特征学习在多模态配准中的重要性，并将MambaMorph定位为该领域的一个开创性解决方案。</li><li>MambaMorph的代码可以在<a target="_blank" rel="noopener" href="https://github.com/Guo-Stone/MambaMorph上获取。">https://github.com/Guo-Stone/MambaMorph上获取。</a></li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：MambaMorph：一种基于 Mamba 的对比特征学习可变形 MR-CT 配准网络</li><li>作者：Tao Guo, Yinuo Wang, Cai Meng</li><li>隶属单位：北京航空航天大学图像处理中心</li><li>关键词：多模态配准、Mamba、特征学习</li><li>论文链接：https://arxiv.org/abs/2401.13934, Github 代码链接：https://github.com/Guo-Stone/MambaMorph</li><li><p>总结： (1)：研究背景：可变形图像配准是医学图像分析中的一项基本方法，由于手术干预、不同的成像序列等因素，图像中解剖组织的拓扑结构会发生很大变化。在分析一对图像之前，需要通过可变形图像配准在空间上对其进行对齐。 (2)：过去的方法：传统的配准方法可以计算出精确且保形的位移场，但也带来了沉重的计算负担和时间成本，不适合实时的情况。在过去的十年中，基于深度学习的配准方法，如 VoxelMorph，展示了其快速实现配准的能力，其准确性甚至可以与传统方法相媲美。 (3)：研究方法：本文提出了一种创新的多模态可变形配准网络 MambaMorph，专门针对磁共振（MR）和计算机断层扫描（CT）图像对齐而设计。MambaMorph 以 Mamba 为基础的配准模块和对比特征学习方法脱颖而出，解决了多模态配准中普遍存在的挑战。该网络利用 Mamba 块进行高效的长程建模和高维数据处理，并结合一个特征提取器，学习细粒度的特征以提高配准精度。 (4)：性能表现：实验结果表明，MambaMorph 在 MR-CT 配准任务上优于现有方法，突出了其在临床应用中的潜力。这项工作强调了特征学习在多模态配准中的重要性，并将 MambaMorph 定位为该领域的开创性解决方案。</p></li><li><p>Methods： (1): MambaMorph网络的总体架构由一个Mamba块和一个特征提取器组成。Mamba块负责长程建模和高维数据处理，特征提取器负责学习细粒度的特征。 (2): Mamba块由一个Mamba单元堆叠而成，每个Mamba单元包含一个注意力机制和一个残差连接。注意力机制用于捕获长程依赖关系，残差连接用于稳定训练过程。 (3): 特征提取器由一个卷积神经网络组成，该网络将输入图像转换为一组特征图。这些特征图被馈送到Mamba块进行配准。 (4): MambaMorph网络的训练过程分为两个阶段。第一阶段，网络学习匹配输入图像的刚性变换。第二阶段，网络学习匹配输入图像的非刚性变换。 (5): 在测试阶段，MambaMorph网络将输入图像转换为一组特征图，然后将这些特征图馈送到Mamba块进行配准。Mamba块输出一个位移场，该位移场用于将输入图像配准到目标图像。</p></li><li><p>结论： （1）：本文提出了一种基于 Mamba 的多模态可变形配准网络 MambaMorph，该网络在 MR-CT 配准任务上优于现有方法，具有较好的临床应用潜力。这项工作强调了特征学习在多模态配准中的重要性，并将 MambaMorph 定位为该领域的开创性解决方案。 （2）：创新点：</p></li><li>提出了一种基于 Mamba 的可变形配准模块，该模块具有较强的长程建模能力和高维数据处理能力。</li><li>引入了一个特征提取器，用于学习细粒度的特征，以提高配准精度。</li><li>将 MambaMorph 应用于 MR-CT 配准任务，并取得了优于现有方法的性能。 性能：</li><li>在 MR-CT 配准任务上，MambaMorph 在配准精度和速度方面均优于现有方法。</li><li>MambaMorph 能够处理具有大变形和拓扑变化的图像对。 工作量：</li><li>MambaMorph 的训练过程分为两个阶段，第一阶段学习匹配输入图像的刚性变换，第二阶段学习匹配输入图像的非刚性变换。</li><li>MambaMorph 的测试过程简单高效，能够快速生成配准结果。</li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-8e4c7b534070889c432f2c0472b4a805.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-afe56b34b0fa28b9fec39311c219549c.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-448e5c1ba4726b65452c3ac554e0eda1.jpg" align="middle"></details><h2 id="EndoGaussians-Single-View-Dynamic-Gaussian-Splatting-for-Deformable-Endoscopic-Tissues-Reconstruction"><a href="#EndoGaussians-Single-View-Dynamic-Gaussian-Splatting-for-Deformable-Endoscopic-Tissues-Reconstruction" class="headerlink" title="EndoGaussians: Single View Dynamic Gaussian Splatting for Deformable   Endoscopic Tissues Reconstruction"></a>EndoGaussians: Single View Dynamic Gaussian Splatting for Deformable Endoscopic Tissues Reconstruction</h2><p><strong>Authors:Yangsen Chen, Hao Wang</strong></p><p>The accurate 3D reconstruction of deformable soft body tissues from endoscopic videos is a pivotal challenge in medical applications such as VR surgery and medical image analysis. Existing methods often struggle with accuracy and the ambiguity of hallucinated tissue parts, limiting their practical utility. In this work, we introduce EndoGaussians, a novel approach that employs Gaussian Splatting for dynamic endoscopic 3D reconstruction. This method marks the first use of Gaussian Splatting in this context, overcoming the limitations of previous NeRF-based techniques. Our method sets new state-of-the-art standards, as demonstrated by quantitative assessments on various endoscope datasets. These advancements make our method a promising tool for medical professionals, offering more reliable and efficient 3D reconstructions for practical applications in the medical field.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.13352v1">PDF</a></p><p><strong>Summary</strong><br>内窥镜高斯体素：一种用于动态内窥镜三维重建的全新方法，克服现有方法的局限性，树立了新的最先进标准。</p><p><strong>Key Takeaways</strong></p><ul><li>引入内窥镜高斯体素，一种用于动态内窥镜 3D 重建的新颖方法，首次将高斯体素散布用于这一领域。</li><li>克服了以前基于神经辐射场 (NeRF) 技术的局限性，例如重建速度慢、重建质量差。</li><li>在定量评估中，我们的方法在各种内窥镜数据集上都达到了最先进的水平。</li><li>我们的方法为医疗专业人士提供了一种有前途的工具，可以在医疗领域的实际应用中提供更可靠和有效的 3D 重建。</li><li>我们的方法使得 3D 重建的速度更快，并且重建的质量更高。</li><li>我们的方法能够处理具有挑战性的场景，例如组织变形和遮挡。</li><li>我们的方法可以用于各种医学应用，例如 VR 手术和医学图像分析。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：EndoGaussians：单视角动态高斯体素化用于可变形内窥镜组织重建</li><li>作者：杨森陈，王浩</li><li>单位：香港科技大学（广州）</li><li>关键词：3D 重建，高斯体素化，机器人手术</li><li>论文链接：https://arxiv.org/abs/2401.13352，Github 链接：无</li><li><p>摘要： （1）研究背景：可变形软组织的准确三维重建对于各种医疗应用（如 VR 手术和医学图像分析）非常重要。然而，现有的方法通常难以实现准确性，并且存在组织部分幻觉的模糊性，这限制了其实际效用。 （2）过去的方法及其问题：为了进一步提高静态单视角 RGBD 设置下软组织的三维重建的准确性，并提高三维重建的可靠性和可信度，本文提出了利用高斯体素化作为重建方法的 EndoGaussians。该方法在多个定量评估（如 PSNR、SSIM、LPIPS 等）方面取得了最先进的结果，并且还实现了更快的重建速度。 （3）研究方法：本文提出的框架由两步组成：内窥镜视频修复和单视角动态高斯体素化。在第一步中，使用视频修复模型从给定的内窥镜视频中去除手术工具。在下一步中，设计了深度引导的动态三维高斯体素管道进行重建。 （4）方法性能：该方法在多个内窥镜数据集上的定量评估中取得了最先进的结果。这些进步使该方法成为医疗专业人员的有前途的工具，可为医疗领域的实际应用提供更可靠和高效的三维重建。</p></li><li><p>方法： (1) 内窥镜视频修复：使用视频修复模型从给定的内窥镜视频中去除手术工具。 (2) 深度引导的动态三维高斯体素化：设计了深度引导的动态三维高斯体素管道进行重建，该管道由以下步骤组成：</p></li><li>体素化：将三维空间划分为体素，并计算每个体素的概率。</li><li>融合：将来自不同视角的体素融合在一起，以获得更准确的重建结果。</li><li><p>优化：使用优化算法来优化体素的概率，以提高重建结果的质量。</p></li><li><p>结论： (1): 本文提出了一种新的单视角动态高斯体素化方法 EndoGaussians，用于可变形内窥镜组织重建，该方法在多个定量评估中取得了最先进的结果，为医疗领域的实际应用提供了更可靠和高效的三维重建。 (2): 创新点：</p></li><li>提出了一种新的单视角动态高斯体素化方法 EndoGaussians，该方法能够准确地重建可变形软组织的三维结构。</li><li>设计了一种深度引导的动态三维高斯体素管道，该管道能够有效地融合来自不同视角的体素，并优化体素的概率，以提高重建结果的质量。</li><li>该方法在多个内窥镜数据集上的定量评估中取得了最先进的结果，证明了其有效性。 性能：</li><li>该方法在多个定量评估（如PSNR、SSIM、LPIPS等）方面取得了最先进的结果，证明了其准确性和可靠性。</li><li>该方法的重建速度较快，能够满足实时应用的需求。 工作量：</li><li>该方法的实现相对复杂，需要较高的计算资源和专业知识。</li><li>该方法需要大量的数据进行训练，这可能会增加工作量。</li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-049a97b3607a44946b481425f04f7d64.jpg" align="middle"></details></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io/2024/01/26/Paper/2024-01-26/3D%20reconstruction/">https://kedreamix.github.io/2024/01/26/Paper/2024-01-26/3D reconstruction/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/3D-reconstruction/">3D reconstruction</a></div><div class="post_share"><div class="social-share" data-image="https://picx.zhimg.com/v2-f77e3da0bc5f121fd261103f1ec6b4b9.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/01/28/Note/pyaudio/" title="Failed building wheel for PyAudio  解决方法"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-85b33202a0c79fba4f52969014d37826.png" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Failed building wheel for PyAudio 解决方法</div></div></a></div><div class="next-post pull-right"><a href="/2024/01/26/Paper/2024-01-26/NeRF/" title="NeRF"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-55f96488825fc7af3820d32c3f4ac6ff.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">NeRF</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/01/24/Paper/2024-01-24/3D%20reconstruction/" title="3D reconstruction"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-8f0eb8b77b117c4be4c26d0982f919c1.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-24</div><div class="title">3D reconstruction</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-01-26-%E6%9B%B4%E6%96%B0"><span class="toc-text">2024-01-26 更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Self-supervised-Video-Object-Segmentation-with-Distillation-Learning-of-Deformable-Attention"><span class="toc-text">Self-supervised Video Object Segmentation with Distillation Learning of Deformable Attention</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MambaMorph-a-Mamba-based-Backbone-with-Contrastive-Feature-Learning-for-Deformable-MR-CT-Registration"><span class="toc-text">MambaMorph: a Mamba-based Backbone with Contrastive Feature Learning for Deformable MR-CT Registration</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#EndoGaussians-Single-View-Dynamic-Gaussian-Splatting-for-Deformable-Endoscopic-Tissues-Reconstruction"><span class="toc-text">EndoGaussians: Single View Dynamic Gaussian Splatting for Deformable Endoscopic Tissues Reconstruction</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image:url('https://picx.zhimg.com/v2-f77e3da0bc5f121fd261103f1ec6b4b9.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js").then((()=>{pangu.autoSpacingPage()}))}function panguInit(){panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.1},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/fireworks.min.js"></script><script defer id="fluttering_ribbon" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-fluttering-ribbon.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><script data-pjax>function butterfly_swiper_injector_config(){var a=document.getElementById("recent-posts");console.log("已挂载butterfly_swiper"),a.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</div><a class="blog-slider__button" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/17/Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023/12/17/Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2023/12/17/Project/ChatPaperFree/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/25/Note/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="2024/01/25/Note/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="2024/01/25/Note/3DGS Survey/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script></body></html>