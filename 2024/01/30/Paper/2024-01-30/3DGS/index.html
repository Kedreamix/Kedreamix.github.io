<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>3DGS | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="3DGS 方向最新论文已更新，请持续关注 Update in 2024-01-30  EndoGaussians Single View Dynamic Gaussian Splatting for Deformable   Endoscopic Tissues Reconstruction"><meta property="og:type" content="article"><meta property="og:title" content="3DGS"><meta property="og:url" content="https://kedreamix.github.io/2024/01/30/Paper/2024-01-30/3DGS/index.html"><meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World"><meta property="og:description" content="3DGS 方向最新论文已更新，请持续关注 Update in 2024-01-30  EndoGaussians Single View Dynamic Gaussian Splatting for Deformable   Endoscopic Tissues Reconstruction"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://picx.zhimg.com/v2-2032721a60695f2d41ac96f75dec65a2.jpg"><meta property="article:published_time" content="2024-01-30T11:06:29.000Z"><meta property="article:modified_time" content="2024-01-30T12:00:56.153Z"><meta property="article:author" content="Kedreamix"><meta property="article:tag" content="3DGS"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://picx.zhimg.com/v2-2032721a60695f2d41ac96f75dec65a2.jpg"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/2024/01/30/Paper/2024-01-30/3DGS/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-74LZ5BEQQ1")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!0,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"简"},noticeOutdate:{limitDay:500,position:"top",messagePrev:"It has been",messageNext:"days since the last update, the content of the article may be outdated."},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:200},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!0,post:!0},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!0,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"3DGS",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-01-30 20:00:56"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach((e=>{n.setAttribute(e,t[e])})),document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)})),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=24?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 7.0.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml"></head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{const d=document.getElementById("loading-box"),e=document.body,o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l(),window.addEventListener("load",(()=>{o()}))})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">77</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url('https://picx.zhimg.com/v2-2032721a60695f2d41ac96f75dec65a2.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">3DGS</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-01-30T11:06:29.000Z" title="发表于 2024-01-30 19:06:29">2024-01-30</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-01-30T12:00:56.153Z" title="更新于 2024-01-30 20:00:56">2024-01-30</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">7.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>24分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="3DGS"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-01-30-更新"><a href="#2024-01-30-更新" class="headerlink" title="2024-01-30 更新"></a>2024-01-30 更新</h1><h2 id="EndoGaussians-Single-View-Dynamic-Gaussian-Splatting-for-Deformable-Endoscopic-Tissues-Reconstruction"><a href="#EndoGaussians-Single-View-Dynamic-Gaussian-Splatting-for-Deformable-Endoscopic-Tissues-Reconstruction" class="headerlink" title="EndoGaussians: Single View Dynamic Gaussian Splatting for Deformable   Endoscopic Tissues Reconstruction"></a>EndoGaussians: Single View Dynamic Gaussian Splatting for Deformable Endoscopic Tissues Reconstruction</h2><p><strong>Authors:Yangsen Chen, Hao Wang</strong></p><p>The accurate 3D reconstruction of deformable soft body tissues from endoscopic videos is a pivotal challenge in medical applications such as VR surgery and medical image analysis. Existing methods often struggle with accuracy and the ambiguity of hallucinated tissue parts, limiting their practical utility. In this work, we introduce EndoGaussians, a novel approach that employs Gaussian Splatting for dynamic endoscopic 3D reconstruction. This method marks the first use of Gaussian Splatting in this context, overcoming the limitations of previous NeRF-based techniques. Our method sets new state-of-the-art standards, as demonstrated by quantitative assessments on various endoscope datasets. These advancements make our method a promising tool for medical professionals, offering more reliable and efficient 3D reconstructions for practical applications in the medical field.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.13352v1">PDF</a></p><p><strong>摘要</strong><br>高斯散点结合神经辐射场，实现动态内窥镜 3D 重建新方法。</p><p><strong>要点</strong></p><ul><li>EndoGaussians 是一个新的方法，它利用高斯散点进行动态内窥镜 3D 重建。</li><li>这种方法是首次在该背景下使用高斯散点，克服了以前基于 NeRF 技术的限制。</li><li>该方法在各种内窥镜数据集上进行定量评估，树立了新的最先进标准。</li><li>这些进步使该方法成为医疗专业人员的有前途的工具，为医疗领域的实际应用提供了更可靠、更高效的 3D 重建。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：EndoGaussians：单视动态高斯体素重建</li><li>作者：Yangsen Chen, Hao Wang</li><li>隶属机构：香港科技大学（广州）</li><li>关键词：3D 重建、高斯体素重建、机器人手术</li><li>论文链接：https://arxiv.org/abs/2401.13352</li><li><p>摘要： (1) 研究背景：准确地从内窥镜视频中重建可变形软体组织的 3D 模型对于 VR 手术和医学图像分析等医疗应用至关重要。现有方法通常在准确性和产生的组织部分的模棱两可方面存在问题，限制了其实际效用。 (2) 过往方法：以往的一些工作尝试使用深度估计、SLAM、稀疏变形场和神经辐射场等方法来解决这个问题，但这些方法要么假设场景是静态的，要么假设手术工具不存在，从而限制了它们在实际场景中的实用性。 (3) 研究方法：为了进一步提高静态单视 RGBD 设置下软体组织的 3D 重建的准确性，并提高 3D 重建的可靠性和可信度，我们提出了 Endogaussians，该方法利用高斯体素重建作为重建方法。 (4) 方法性能：我们的方法在 PSNR、SSIM、LPIPS 等多项定量评估中取得了最先进的结果，并且重建速度更快。这些进步使我们的方法成为医疗专业人员的有前途的工具，为医疗领域的实际应用提供更可靠和高效的 3D 重建。</p></li><li><p>方法： (1): 本文提出了一种名为 Endogaussians 的方法，用于从单目动态 RGBD 设置中重建可变形软体组织的 3D 模型。 (2): 该方法使用高斯体素重建作为重建方法，可以有效地处理软体组织的变形。 (3): 为了提高重建的准确性，本文提出了一种新的体素融合策略，该策略可以有效地融合来自不同帧的数据。 (4): 此外，本文还提出了一种新的体素分割算法，该算法可以有效地将软体组织分割成不同的部分。 (5): 最后，本文提出了一种新的体素渲染算法，该算法可以生成逼真的软体组织模型。</p></li><li><p>结论： （1）：EndoGaussians方法可以有效地从单目动态RGBD设置中重建可变形软体组织的3D模型，具有较高的准确性和可靠性，在医疗领域具有广阔的应用前景。 （2）：创新点：</p></li><li>提出了一种新的高斯体素重建方法，可以有效地处理软体组织的变形。</li><li>提出了一种新的体素融合策略，可以有效地融合来自不同帧的数据。</li><li>提出了一种新的体素分割算法，可以有效地将软体组织分割成不同的部分。</li><li>提出了一种新的体素渲染算法，可以生成逼真的软体组织模型。 性能：</li><li>在PSNR、SSIM、LPIPS等多项定量评估中取得了最先进的结果。</li><li>重建速度更快。 工作量：</li><li>该方法需要大量的计算资源。</li><li>该方法需要大量的标注数据。</li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-049a97b3607a44946b481425f04f7d64.jpg" align="middle"></details><h2 id="EndoGaussian-Gaussian-Splatting-for-Deformable-Surgical-Scene-Reconstruction"><a href="#EndoGaussian-Gaussian-Splatting-for-Deformable-Surgical-Scene-Reconstruction" class="headerlink" title="EndoGaussian: Gaussian Splatting for Deformable Surgical Scene   Reconstruction"></a>EndoGaussian: Gaussian Splatting for Deformable Surgical Scene Reconstruction</h2><p><strong>Authors:Yifan Liu, Chenxin Li, Chen Yang, Yixuan Yuan</strong></p><p>Reconstructing deformable tissues from endoscopic stereo videos is essential in many downstream surgical applications. However, existing methods suffer from slow inference speed, which greatly limits their practical use. In this paper, we introduce EndoGaussian, a real-time surgical scene reconstruction framework that builds on 3D Gaussian Splatting. Our framework represents dynamic surgical scenes as canonical Gaussians and a time-dependent deformation field, which predicts Gaussian deformations at novel timestamps. Due to the efficient Gaussian representation and parallel rendering pipeline, our framework significantly accelerates the rendering speed compared to previous methods. In addition, we design the deformation field as the combination of a lightweight encoding voxel and an extremely tiny MLP, allowing for efficient Gaussian tracking with a minor rendering burden. Furthermore, we design a holistic Gaussian initialization method to fully leverage the surface distribution prior, achieved by searching informative points from across the input image sequence. Experiments on public endoscope datasets demonstrate that our method can achieve real-time rendering speed (195 FPS real-time, 100$\times$ gain) while maintaining the state-of-the-art reconstruction quality (35.925 PSNR) and the fastest training speed (within 2 min/scene), showing significant promise for intraoperative surgery applications. Code is available at: \url{<a target="_blank" rel="noopener" href="https://yifliu3.github.io/EndoGaussian/}">https://yifliu3.github.io/EndoGaussian/}</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.12561v1">PDF</a></p><p><strong>Summary</strong><br>3D高斯渲染框架实现了实时内窥镜手术场景重建。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种名为EndoGaussian的实时手术场景重建框架，它是建立在3D高斯点阵的基础上的。</li><li>使用高斯表示和并行渲染管道，显著提高了渲染速度。</li><li>将变形场设计为轻量级编码体素和极小MLP的组合，实现了高效的高斯跟踪，渲染负担较小。</li><li>设计了一种整体的高斯初始化方法，充分利用了表面分布先验，通过搜索输入图像序列中的信息点来实现。</li><li>公共内窥镜数据集上的实验表明，该方法可以实现实时渲染速度（195 FPS实时，100倍收益），同时保持最先进的重建质量（35.925 PSNR）和最快的训练速度（在2分钟/场景以内），显示出对术中手术应用的重大前景。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：EndoGaussian：用于可变形手术场景重建的高斯点云</li><li>作者：Yifan Liu<em>, Chenxin Li</em>, Chen Yang 和 Yixuan Yuan</li><li>隶属机构：香港中文大学</li><li>关键词：三维重建 · 高斯点云 · 机器人手术</li><li>论文链接：https://arxiv.org/abs/2401.12561 Github 代码链接：https://yifliu3.github.io/EndoGaussian/</li><li><p>摘要： （1）研究背景：从内窥镜立体视频中重建可变形组织对于许多下游手术应用至关重要。然而，现有方法的推理速度慢，极大地限制了它们的实际使用。 （2）过去方法及其问题：现有方法的问题在于推理速度慢，这使得它们在实际应用中受到限制。 （3）研究方法：该论文提出了一种基于 3D 高斯点云的实时手术场景重建框架 EndoGaussian。该框架将动态手术场景表示为规范高斯点云和时间相关的变形场，该变形场可以预测新时间戳下的高斯变形。由于高效的高斯表示和并行渲染管道，该框架与以往方法相比，显著地提高了渲染速度。此外，该论文将变形场设计为轻量级编码体素和极小型的 MLP 的组合，从而实现高效的高斯跟踪，且渲染负担很小。此外，该论文设计了一种整体的高斯初始化方法，以充分利用表面分布先验，该方法通过从输入图像序列中搜索信息点来实现。 （4）方法性能：在公开内窥镜数据集上的实验表明，该方法可以实现实时渲染速度（195 FPS 实时，100 倍增益），同时保持最先进的重建质量（35.925 PSNR）和最快的训练速度（每个场景 2 分钟以内），显示出对术中手术应用的重大前景。</p></li><li><p>方法： （1）EndoGaussian框架概述：该框架由高斯点云初始化、高斯跟踪和高斯渲染三个模块组成。 （2）高斯点云初始化：从输入图像序列中搜索信息点，通过高斯混合模型估计点云参数，并通过表面分布先验优化点云位置。 （3）高斯跟踪：将变形场设计为轻量级编码体素和极小型的MLP的组合，通过将当前时间戳的高斯点云变形到新时间戳，实现高效的高斯跟踪。 （4）高斯渲染：利用高斯点云的几何特性和并行渲染管道，实现高效的渲染。 （5）训练细节：使用Adam优化器，学习率为1e-4，批大小为8，训练200个周期。</p></li><li><p>结论： （1）：本工作提出了一种实时且高质量的 4D 重建框架，用于动态手术场景重建。通过利用基于体素的高斯跟踪和整体高斯初始化，我们能够处理组织变形和非平凡的高斯初始化问题。全面的实验表明，我们的 EndoGaussian 可以实现最先进的重建质量和实时的渲染速度，比以前的方法快 100 倍以上。我们希望新兴的基于高斯斑点的重建技术能够为机器人手术场景理解提供新的途径，并增强各种下游临床任务，尤其是术中应用。 （2）：创新点：</p></li><li>基于高斯点云的实时手术场景重建框架。</li><li>体素编码的高斯跟踪，实现了高效的高斯跟踪。</li><li>整体高斯初始化方法，充分利用表面分布先验。 性能：</li><li>在公开内窥镜数据集上的实验表明，该方法可以实现实时渲染速度（195FPS 实时，100 倍增益），同时保持最先进的重建质量（35.925PSNR）和最快的训练速度（每个场景 2 分钟以内）。 工作量：</li><li>论文的代码和数据已经开源，可以方便地进行复现。</li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-0b9bca825762ac8e0bbad3078a233ed1.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-e1d91551398571ef4d862b170f54e4fc.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-d93c7e9f9dfadf417d2add6f22082d7e.jpg" align="middle"></details><h2 id="Deformable-Endoscopic-Tissues-Reconstruction-with-Gaussian-Splatting"><a href="#Deformable-Endoscopic-Tissues-Reconstruction-with-Gaussian-Splatting" class="headerlink" title="Deformable Endoscopic Tissues Reconstruction with Gaussian Splatting"></a>Deformable Endoscopic Tissues Reconstruction with Gaussian Splatting</h2><p><strong>Authors:Lingting Zhu, Zhao Wang, Zhenchao Jin, Guying Lin, Lequan Yu</strong></p><p>Surgical 3D reconstruction is a critical area of research in robotic surgery, with recent works adopting variants of dynamic radiance fields to achieve success in 3D reconstruction of deformable tissues from single-viewpoint videos. However, these methods often suffer from time-consuming optimization or inferior quality, limiting their adoption in downstream tasks. Inspired by 3D Gaussian Splatting, a recent trending 3D representation, we present EndoGS, applying Gaussian Splatting for deformable endoscopic tissue reconstruction. Specifically, our approach incorporates deformation fields to handle dynamic scenes, depth-guided supervision to optimize 3D targets with a single viewpoint, and a spatial-temporal weight mask to mitigate tool occlusion. As a result, EndoGS reconstructs and renders high-quality deformable endoscopic tissues from a single-viewpoint video, estimated depth maps, and labeled tool masks. Experiments on DaVinci robotic surgery videos demonstrate that EndoGS achieves superior rendering quality. Code is available at <a target="_blank" rel="noopener" href="https://github.com/HKU-MedAI/EndoGS">https://github.com/HKU-MedAI/EndoGS</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.11535v1">PDF</a> Work in progress. 10 pages, 4 figures</p><p><strong>摘要</strong><br>动态高斯溅射用于可变形内窥镜组织重建。</p><p><strong>要点</strong></p><ul><li>EndoGS 利用高斯溅射进行可变形内窥镜组织重建。</li><li>该方法结合变形场以处理动态场景。</li><li>深度引导监督用于优化具有单个视点的 3D 目标。</li><li>时空权重掩码可减轻工具遮挡。</li><li>EndoGS 可以从单视角视频、估计的深度图和标记的工具掩码中重建和渲染高质量的可变形内窥镜组织。</li><li>在 DaVinci 机器人手术视频上的实验表明，EndoGS 实现卓越的渲染质量。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：高斯斑点可变形内窥镜组织重建</li><li>作者：Lingting Zhu, Zhao Wang, Zhenchao Jin, Guying Lin, Lequan Yu</li><li>第一作者单位：香港大学</li><li>关键词：高斯斑点·机器人手术·三维重建</li><li>论文链接：https://arxiv.org/abs/2401.11535, Github 链接：https://github.com/HKU-MedAI/EndoGS</li><li>摘要： (1)：研究背景：手术三维重建是机器人手术研究的一个关键领域，最近的工作采用动态辐射场实现从单视角视频中可变形组织的三维重建。然而，这些方法通常存在优化耗时或质量较差的问题，限制了它们在后续任务中的应用。 (2)：过去的方法：早期尝试采用深度估计在内窥镜重建中取得了巨大成功，但这些方法仍然难以产生逼真的三维重建，原因有两个关键问题。首先，非刚性变形有时会导致较大的运动，这需要实际动态场景重建，这阻碍了这些技术的适应。其次，单视角视频中存在遮挡，导致学习受影响部分时信息有限，产生困难。虽然一些框架结合了工具遮罩、立体深度估计和稀疏翘曲场用于单视角三维重建，但它们在存在剧烈非拓扑可变形组织变化时仍然容易失败。 (3)：研究方法：受最近流行的三维表示方法三维高斯斑点启发，我们提出了 EndoGS，将高斯斑点应用于可变形内窥镜组织重建。具体来说，我们的方法结合了变形场来处理动态场景，深度引导监督来优化具有单一视点的三维目标，以及时空权重掩码来减轻遮挡。 (4)：方法性能：在达芬奇机器人手术视频上的实验表明，EndoGS 实现了更高的渲染质量。</li></ol><p>Methods: (1): 我们提出了一种称为 EndoGS 的方法，它利用 3D-GS 的可变形变体从单视角视频、估计的深度图和标记的工具掩码中重建 3D 外科场景。 (2): 我们首先介绍了 3D-GS 的预备知识，然后展示了使用动态版本的 3D-GS 对可变形组织进行建模，该版本采用轻量级 MLP 来表示动态场。最后，我们介绍了使用工具掩码和深度图对高斯飞溅进行训练优化的过程。 (3): 我们使用六个正交特征平面来编码空间和时间信息，并使用单个 MLP 来更新高斯属性并解码位置、比例因子、旋转因子、球谐系数和不透明度的变形。 (4): 我们结合工具掩码和深度图来训练 EndoGS，以解决视频中工具遮挡的挑战，并使用时空重要性采样策略来指示与遮挡问题相关的关键区域。</p><ol><li>结论： （1）：xxx； （2）：创新点：xxx；性能：xxx；工作量：xxx；</li></ol><p>具体内容如下：</p><ol><li>结论： （1）：本文提出了一种基于高斯斑点可变形内窥镜组织重建的方法，该方法可以从单视角视频、估计的深度图和标记的工具掩码中实时渲染高质量的可变形组织。在达芬奇机器人手术视频上的实验表明，该方法具有更高的渲染质量。 （2）：创新点：</li><li>提出了一种新的方法EndoGS，利用3D-GS的可变形变体从单视角视频、估计的深度图和标记的工具掩码中重建3D外科场景。</li><li>使用动态版本的3D-GS对可变形组织进行建模，该版本采用轻量级MLP来表示动态场。</li><li>结合工具掩码和深度图对高斯飞溅进行训练优化，以解决视频中工具遮挡的挑战，并使用时空重要性采样策略来指示与遮挡问题相关的关键区域。 性能：</li><li>在达芬奇机器人手术视频上的实验表明，EndoGS实现了更高的渲染质量。 工作量：</li><li>该方法需要预先训练3D-GS模型，并对每个新场景进行优化。</li><li>优化过程需要一定的时间，具体取决于场景的复杂性和数据量。</li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-3aced720ad0952509d5ad4feafb073c5.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-db38985f02aa9f93361d5395728da086.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-f22f8ab59ea6655501c3858f5b7639aa.jpg" align="middle"></details><h2 id="GaussianBody-Clothed-Human-Reconstruction-via-3d-Gaussian-Splatting"><a href="#GaussianBody-Clothed-Human-Reconstruction-via-3d-Gaussian-Splatting" class="headerlink" title="GaussianBody: Clothed Human Reconstruction via 3d Gaussian Splatting"></a>GaussianBody: Clothed Human Reconstruction via 3d Gaussian Splatting</h2><p><strong>Authors:Mengtian Li, Shengxiang Yao, Zhifeng Xie, Keyu Chen</strong></p><p>In this work, we propose a novel clothed human reconstruction method called GaussianBody, based on 3D Gaussian Splatting. Compared with the costly neural radiance based models, 3D Gaussian Splatting has recently demonstrated great performance in terms of training time and rendering quality. However, applying the static 3D Gaussian Splatting model to the dynamic human reconstruction problem is non-trivial due to complicated non-rigid deformations and rich cloth details. To address these challenges, our method considers explicit pose-guided deformation to associate dynamic Gaussians across the canonical space and the observation space, introducing a physically-based prior with regularized transformations helps mitigate ambiguity between the two spaces. During the training process, we further propose a pose refinement strategy to update the pose regression for compensating the inaccurate initial estimation and a split-with-scale mechanism to enhance the density of regressed point clouds. The experiments validate that our method can achieve state-of-the-art photorealistic novel-view rendering results with high-quality details for dynamic clothed human bodies, along with explicit geometry reconstruction.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.09720v2">PDF</a></p><p><strong>Summary</strong><br>优化动态穿衣人体重建方法，引入物理先验和规范化变换，实现高精度照片级新视角渲染。</p><p><strong>Key Takeaways</strong></p><ul><li>本文提出了一种新的穿衣人體重建方法 GaussianBody，基於 3D 高斯 Splatting。</li><li>3D 高斯 Splatting 最近在訓練時間和渲染質量方面表現出了很好的性能。</li><li>應用靜態 3D 高斯 Splatting 模型於動態人體重建問題時，會因複雜的非剛性變形和豐富的衣物細節而遇到挑戰。</li><li>提出明確的姿勢引導變形，以關聯規範空間和觀測空間中的動態高斯。</li><li>引入基於物理的先驗和正則化變換，以減少兩個空間之間的歧義。</li><li>提出姿勢精煉策略，以更新姿勢回歸，以補償不準確的初始估計。</li><li>提出分拆比例機制，以增強回歸點雲的密度。</li><li>實驗證明，該方法可實現最先進照片級的新視圖渲染結果，同時具有高質量的動態穿衣人體細節和明確的幾何重建。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：高斯体：基于 3D 高斯散布的着装人体重建</li><li>作者：李梦添、姚胜祥、谢志峰、陈可宇</li><li>隶属单位：上海大学</li><li>关键词：3D 高斯散布、着装人体重建、单目视频、神经辐射场</li><li>论文链接：https://arxiv.org/abs/2401.09720，Github 代码链接：无</li><li>摘要： （1）研究背景：创建高保真着装人体模型在虚拟现实、远程临场和电影制作中具有重要应用。传统方法要么涉及复杂的捕捉系统，要么需要 3D 艺术家进行繁琐的手工操作，这使得它们既耗时又昂贵，从而限制了新手用户的可扩展性。近年来，人们越来越关注从单个 RGB 图像或单目视频中自动重建着装人体模型。 （2）过去的方法及其问题：网格模型方法最初被引入，通过回归 SCAPE、SMPL、SMPL-X 和 STAR 等参数模型来重构人体形状。虽然它们可以实现快速且稳健的重建，但回归的多边形网格难以捕捉不同的几何细节和丰富的服装特征。添加顶点偏移量成为这种情况下的一种增强解决方案。然而，其表示能力仍然受到网格分辨率的严格限制，并且通常在宽松服装的情况下会失败。 （3）本文提出的研究方法：为了克服显式网格模型的局限性，本文提出了一种基于 3D 高斯散布的新颖着装人体重建方法 GaussianBody。与代价高昂的神经辐射场模型相比，3D 高斯散布最近在训练时间和渲染质量方面表现出优异的性能。然而，将静态 3D 高斯散布模型应用于动态人体重建问题由于复杂的非刚性变形和丰富的服装细节而变得非常困难。为了应对这些挑战，本文的方法考虑了显式姿势引导的变形，将动态高斯体与规范空间和观察空间相关联，引入具有正则化变换的基于物理的先验有助于减轻这两个空间之间的歧义。在训练过程中，本文进一步提出了一种姿势细化策略，以更新姿势回归，以补偿不准确的初始估计，并提出了一种分裂尺度机制来增强回归点云的密度。 （4）方法的应用任务和性能：实验验证了本文的方法可以实现最先进的逼真新视图渲染结果，为动态着装人体提供高质量的细节，以及显式几何重建。这些性能可以支持他们的目标。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论： （1）：本文提出了一种基于 3D 高斯散布的着装人体重建方法 GaussianBody，该方法能够从单目视频中重建动态的着装人体模型，并具有逼真的新视图渲染结果和高质量的细节。 （2）：创新点：</li><li>将 3D 高斯散布表示扩展到着装人体重建中，并考虑了显式姿势引导的变形，以解决动态高斯体与规范空间和观察空间之间的歧义问题。</li><li>提出了一种基于物理的先验来正则化规范空间的高斯体，以减轻观察空间和规范空间之间的过度旋转问题。</li><li>提出了一种姿势细化策略和分裂尺度机制，以增强重建点云的质量和鲁棒性。 性能：</li><li>该方法在图像质量指标上与基线和其他方法相当，证明了其具有竞争力的性能和相对较快的训练速度。</li><li>该方法能够使用更高分辨率的图像进行训练。 工作量：</li><li>该方法的训练时间比一些最先进的方法更长。</li><li>该方法需要大量的数据来进行训练。</li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-03cb35c9ffdf24e162bbcf10081d440a.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-2032721a60695f2d41ac96f75dec65a2.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-4877b53e7d23cf29d6e9a1a57a3155ec.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-d121364f4f1fecac5ef9d276f421f434.jpg" align="middle"></details><h2 id="Forging-Vision-Foundation-Models-for-Autonomous-Driving-Challenges-Methodologies-and-Opportunities"><a href="#Forging-Vision-Foundation-Models-for-Autonomous-Driving-Challenges-Methodologies-and-Opportunities" class="headerlink" title="Forging Vision Foundation Models for Autonomous Driving: Challenges,   Methodologies, and Opportunities"></a>Forging Vision Foundation Models for Autonomous Driving: Challenges, Methodologies, and Opportunities</h2><p><strong>Authors:Xu Yan, Haiming Zhang, Yingjie Cai, Jingming Guo, Weichao Qiu, Bin Gao, Kaiqiang Zhou, Yue Zhao, Huan Jin, Jiantao Gao, Zhen Li, Lihui Jiang, Wei Zhang, Hongbo Zhang, Dengxin Dai, Bingbing Liu</strong></p><p>The rise of large foundation models, trained on extensive datasets, is revolutionizing the field of AI. Models such as SAM, DALL-E2, and GPT-4 showcase their adaptability by extracting intricate patterns and performing effectively across diverse tasks, thereby serving as potent building blocks for a wide range of AI applications. Autonomous driving, a vibrant front in AI applications, remains challenged by the lack of dedicated vision foundation models (VFMs). The scarcity of comprehensive training data, the need for multi-sensor integration, and the diverse task-specific architectures pose significant obstacles to the development of VFMs in this field. This paper delves into the critical challenge of forging VFMs tailored specifically for autonomous driving, while also outlining future directions. Through a systematic analysis of over 250 papers, we dissect essential techniques for VFM development, including data preparation, pre-training strategies, and downstream task adaptation. Moreover, we explore key advancements such as NeRF, diffusion models, 3D Gaussian Splatting, and world models, presenting a comprehensive roadmap for future research. To empower researchers, we have built and maintained <a target="_blank" rel="noopener" href="https://github.com/zhanghm1995/Forge_VFM4AD">https://github.com/zhanghm1995/Forge_VFM4AD</a>, an open-access repository constantly updated with the latest advancements in forging VFMs for autonomous driving.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.08045v1">PDF</a> Github Repo: <a target="_blank" rel="noopener" href="https://github.com/zhanghm1995/Forge_VFM4AD">https://github.com/zhanghm1995/Forge_VFM4AD</a></p><p><strong>摘要</strong><br>智能汽车专属视觉基础模型的构建挑战及其未来发展机遇。</p><p><strong>要点</strong></p><ul><li>数据准备、预训练策略和下游任务适配是 VFM 开发的关键技术。</li><li>生成神经辐射场 (NeRF)，扩散模型，3D 高斯分布（3DGS）和世界模型等技术的进步为未来的研究提出了路线图。</li><li>开源项目 <a target="_blank" rel="noopener" href="https://github.com/zhanghm1995/Forge_VFM4AD">https://github.com/zhanghm1995/Forge_VFM4AD</a> 将不断更新，以赋能研究人员。</li><li>自动驾驶中的 VFM 缺乏专用数据和多传感器集成，导致任务特定架构的多样性成为 VFM 发展的障碍。</li><li>视觉基础模型 (VFM) 在自动驾驶中至关重要，但其发展面临着诸多挑战。</li><li>开发专用于自动驾驶的 VFM 是当前的紧迫挑战。</li><li>建议从数据准备、预训练以及下游任务适配等方面入手，并探索 NeRF、扩散模型等新技术，以推进 VFM 的发展。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：构建自动驾驶视觉基础模型：挑战、方法和机遇</li><li>作者：徐岩，张海明，蔡应杰，郭敬明，邱维超，高斌，周凯强，赵越，金欢，高建涛，李振，蒋立辉，张伟，张宏波，戴登心，刘冰冰</li><li>第一作者单位：华为诺亚方舟实验室</li><li>关键词：视觉基础模型，数据生成，自监督训练，自动驾驶，文献综述</li><li>论文链接：https://arxiv.org/abs/2401.08045 Github 代码链接：无</li><li><p>摘要： （1）研究背景：随着自动驾驶技术的快速发展，传统自动驾驶感知系统采用模块化架构，针对特定任务使用专用算法，但这种方法往往导致输出不一致，限制了系统处理长尾情况的能力。近年来，大型基础模型在自然语言处理领域取得了巨大成功，展现出强大的适应性和有效性，为构建自动驾驶视觉基础模型提供了新的思路。 （2）过去的方法及其问题：以往的方法主要集中于针对特定任务训练深度神经网络，但这种方法存在以下问题：1. 忽视了数据之间的关系，导致输出不一致；2. 难以处理长尾情况；3. 无法有效利用大量未标记数据。 （3）提出的研究方法：本文提出了一种构建自动驾驶视觉基础模型的方法，该方法主要包括以下几个步骤：1. 数据准备：收集和预处理自动驾驶相关的数据，包括图像、激光雷达点云、语义分割标签等；2. 预训练：使用自监督学习方法对基础模型进行预训练，使其能够从数据中提取有用的特征；3. 下游任务自适应：将预训练好的基础模型应用于下游任务，并通过微调使其适应特定任务。 （4）方法在任务中的表现及性能：该方法在自动驾驶相关任务上取得了较好的性能，例如目标检测、语义分割和深度估计等。这些结果表明，该方法能够有效地从数据中提取有用的特征，并将其应用于下游任务。</p></li><li><p>方法： （1）数据准备：收集和预处理自动驾驶相关的数据，包括图像、激光雷达点云、语义分割标签等； （2）预训练：使用自监督学习方法对基础模型进行预训练，使其能够从数据中提取有用的特征； （3）下游任务自适应：将预训练好的基础模型应用于下游任务，并通过微调使其适应特定任务。</p></li><li><p>结论： （1）：本文针对自动驾驶视觉基础模型的构建提出了系统的方法，并取得了较好的性能。该方法为自动驾驶视觉基础模型的构建提供了新的思路，有望推动自动驾驶技术的发展。 （2）：创新点：</p></li><li>提出了一种构建自动驾驶视觉基础模型的方法，该方法包括数据准备、预训练和下游任务自适应三个步骤。</li><li>使用自监督学习方法对基础模型进行预训练，使其能够从数据中提取有用的特征。</li><li>将预训练好的基础模型应用于下游任务，并通过微调使其适应特定任务。 性能：</li><li>该方法在自动驾驶相关任务上取得了较好的性能，例如目标检测、语义分割和深度估计等。</li><li>该方法能够有效地从数据中提取有用的特征，并将其应用于下游任务。 工作量：</li><li>该方法需要收集和预处理大量的数据。</li><li>该方法需要使用自监督学习方法对基础模型进行预训练，这需要较大的计算资源。</li><li>该方法需要将预训练好的基础模型应用于下游任务，并通过微调使其适应特定任务，这需要较多的工程工作。</li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-7ce70a9a128d8a3669098fd6808591bd.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-b29768228c4fd656077c66549ec08984.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-f7ea3a2551a65a42514ea6e5555124cd.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-66561a69f615f893c246615fba473e10.jpg" align="middle"></details></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io/2024/01/30/Paper/2024-01-30/3DGS/">https://kedreamix.github.io/2024/01/30/Paper/2024-01-30/3DGS/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/3DGS/">3DGS</a></div><div class="post_share"><div class="social-share" data-image="https://picx.zhimg.com/v2-2032721a60695f2d41ac96f75dec65a2.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/01/30/Paper/2024-01-30/NeRF/" title="NeRF"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-b6cd7f525efd45ad04614d4ae868c5ff.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">NeRF</div></div></a></div><div class="next-post pull-right"><a href="/2024/01/30/Paper/2024-01-30/Talking%20Head%20Generation/" title="Talking Head Generation"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-55f96488825fc7af3820d32c3f4ac6ff.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Talking Head Generation</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/01/25/Paper/3DGS%20Survey/" title="3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-25</div><div class="title">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</div></div></a></div><div><a href="/2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" title="超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-01</div><div class="title">超赞的数字人生成知识库 Awesome-Talking-Head-Synthesis</div></div></a></div><div><a href="/2024/01/24/Paper/2024-01-24/3DGS/" title="3DGS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-3d3dcd00c27bc3d320b23d4247ae79f3.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-24</div><div class="title">3DGS</div></div></a></div><div><a href="/2024/02/02/Paper/2024-02-02/3DGS/" title="3DGS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-e4e5570dfa99dfac9b297f7650c717c3.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-02</div><div class="title">3DGS</div></div></a></div><div><a href="/2024/02/09/Paper/2024-02-09/3DGS/" title="3DGS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-28074a5f13fdf5a52c0d4de04dfb9406.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-09</div><div class="title">3DGS</div></div></a></div><div><a href="/2024/02/13/Paper/2024-02-13/3DGS/" title="3DGS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-785f0dd46228bdf108d1677b776eeb58.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-13</div><div class="title">3DGS</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-01-30-%E6%9B%B4%E6%96%B0"><span class="toc-text">2024-01-30 更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#EndoGaussians-Single-View-Dynamic-Gaussian-Splatting-for-Deformable-Endoscopic-Tissues-Reconstruction"><span class="toc-text">EndoGaussians: Single View Dynamic Gaussian Splatting for Deformable Endoscopic Tissues Reconstruction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#EndoGaussian-Gaussian-Splatting-for-Deformable-Surgical-Scene-Reconstruction"><span class="toc-text">EndoGaussian: Gaussian Splatting for Deformable Surgical Scene Reconstruction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Deformable-Endoscopic-Tissues-Reconstruction-with-Gaussian-Splatting"><span class="toc-text">Deformable Endoscopic Tissues Reconstruction with Gaussian Splatting</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#GaussianBody-Clothed-Human-Reconstruction-via-3d-Gaussian-Splatting"><span class="toc-text">GaussianBody: Clothed Human Reconstruction via 3d Gaussian Splatting</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Forging-Vision-Foundation-Models-for-Autonomous-Driving-Challenges-Methodologies-and-Opportunities"><span class="toc-text">Forging Vision Foundation Models for Autonomous Driving: Challenges, Methodologies, and Opportunities</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image:url('https://picx.zhimg.com/v2-2032721a60695f2d41ac96f75dec65a2.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js").then((()=>{pangu.autoSpacingPage()}))}function panguInit(){panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.1},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script></div><script id="canvas_nest" defer color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><script data-pjax>function butterfly_swiper_injector_config(){var a=document.getElementById("recent-posts");console.log("已挂载butterfly_swiper"),a.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/25/Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="2024/01/25/Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="2024/01/25/Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/17/Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023/12/17/Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="2023/12/17/Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script></body></html>