<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>NeRF | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="NeRF 方向最新论文已更新，请持续关注 Update in 2024-01-30  Divide and Conquer Rethinking the Training Paradigm of Neural Radiance   Fields"><meta property="og:type" content="article"><meta property="og:title" content="NeRF"><meta property="og:url" content="https://kedreamix.github.io/2024/01/30/Paper/2024-01-30/NeRF/index.html"><meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World"><meta property="og:description" content="NeRF 方向最新论文已更新，请持续关注 Update in 2024-01-30  Divide and Conquer Rethinking the Training Paradigm of Neural Radiance   Fields"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://pica.zhimg.com/v2-b6cd7f525efd45ad04614d4ae868c5ff.jpg"><meta property="article:published_time" content="2024-01-30T11:23:52.000Z"><meta property="article:modified_time" content="2024-01-30T12:01:08.574Z"><meta property="article:author" content="Kedreamix"><meta property="article:tag" content="NeRF"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://pica.zhimg.com/v2-b6cd7f525efd45ad04614d4ae868c5ff.jpg"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/2024/01/30/Paper/2024-01-30/NeRF/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-74LZ5BEQQ1")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!0,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"简"},noticeOutdate:{limitDay:500,position:"top",messagePrev:"It has been",messageNext:"days since the last update, the content of the article may be outdated."},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:200},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!0,post:!0},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!0,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"NeRF",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-01-30 20:01:08"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach((e=>{n.setAttribute(e,t[e])})),document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)})),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=24?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 7.0.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml"></head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{const d=document.getElementById("loading-box"),e=document.body,o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l(),window.addEventListener("load",(()=>{o()}))})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">47</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url('https://pica.zhimg.com/v2-b6cd7f525efd45ad04614d4ae868c5ff.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">NeRF</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-01-30T11:23:52.000Z" title="发表于 2024-01-30 19:23:52">2024-01-30</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-01-30T12:01:08.574Z" title="更新于 2024-01-30 20:01:08">2024-01-30</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">10.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>38分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="NeRF"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-01-30-更新"><a href="#2024-01-30-更新" class="headerlink" title="2024-01-30 更新"></a>2024-01-30 更新</h1><h2 id="Divide-and-Conquer-Rethinking-the-Training-Paradigm-of-Neural-Radiance-Fields"><a href="#Divide-and-Conquer-Rethinking-the-Training-Paradigm-of-Neural-Radiance-Fields" class="headerlink" title="Divide and Conquer: Rethinking the Training Paradigm of Neural Radiance   Fields"></a>Divide and Conquer: Rethinking the Training Paradigm of Neural Radiance Fields</h2><p><strong>Authors:Rongkai Ma, Leo Lebrat, Rodrigo Santa Cruz, Gil Avraham, Yan Zuo, Clinton Fookes, Olivier Salvado</strong></p><p>Neural radiance fields (NeRFs) have exhibited potential in synthesizing high-fidelity views of 3D scenes but the standard training paradigm of NeRF presupposes an equal importance for each image in the training set. This assumption poses a significant challenge for rendering specific views presenting intricate geometries, thereby resulting in suboptimal performance. In this paper, we take a closer look at the implications of the current training paradigm and redesign this for more superior rendering quality by NeRFs. Dividing input views into multiple groups based on their visual similarities and training individual models on each of these groups enables each model to specialize on specific regions without sacrificing speed or efficiency. Subsequently, the knowledge of these specialized models is aggregated into a single entity via a teacher-student distillation paradigm, enabling spatial efficiency for online render-ing. Empirically, we evaluate our novel training framework on two publicly available datasets, namely NeRF synthetic and Tanks&amp;Temples. Our evaluation demonstrates that our DaC training pipeline enhances the rendering quality of a state-of-the-art baseline model while exhibiting convergence to a superior minimum.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.16144v1">PDF</a></p><p><strong>Summary</strong><br>利用教师-学生知识蒸馏范式，提升 NeRF 模型的渲染质量。</p><p><strong>Key Takeaways</strong></p><ul><li>传统 NeRF 模型的训练范式对训练集中每个图像赋予同等重要性，这导致在渲染具有复杂几何结构的特定视图时表现不佳。</li><li>将输入视图根据其视觉相似性划分为多个组，并在每个组上训练单独的模型，使每个模型专注于特定区域，从而提高渲染质量。</li><li>通过教师-学生知识蒸馏范式将这些专门模型的知识聚合到一个实体中，实现在线渲染的空间效率。</li><li>在 NeRF 合成和 Tanks&amp;Temples 两个公开数据集上对提出的训练框架进行评估，结果表明该框架优于最先进的基线模型，并且收敛到更好的最小值。</li><li>提出了一种名为 DaC 的分而治之训练框架。</li><li>DaC 将训练集划分为多个子集，并在每个子集上训练一个单独的神经辐射场 (NeRF) 模型。</li><li>然后将这些子模型通过知识蒸馏聚合成一个最终模型。</li><li>DaC 在 NeRF 合成和 Tanks&amp;Temples 数据集上的实验表明，它优于最先进的 NeRF 模型。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：分而治之：重新思考神经辐射场的训练范式</li><li>作者：Rongkai Ma、Leo Lebrat、Rodrigo SantaCruz、Gil Avraham、Yan Zuo、Clinton Fookes、Olivier Salvado</li><li>第一作者单位：英伟达</li><li>关键词：神经辐射场、分而治之、教师-学生蒸馏、空间效率</li><li>链接：https://arxiv.org/abs/2401.16144 Github：无</li><li><p>摘要： （1）研究背景：神经辐射场（NeRF）在合成 3D 场景的高保真视图方面表现出潜力，但 NeRF 的标准训练范式预设了训练集中每个图像具有同等重要性。这种假设对渲染呈现复杂几何体的特定视图提出了重大挑战，从而导致性能不佳。 （2）过去的方法及其问题：以往的方法通常将所有场景视角的几何和光度信息统一压缩到神经网络权重中。这种方法往往忽略了复杂场景不同视角中存在的细节的自然不对称性，导致渲染质量下降。 （3）本文提出的研究方法：本文重新审视了当前训练范式的含义，并重新设计了该范式，以提高 NeRF 的渲染质量。将输入视图根据它们的视觉相似性划分为多个组，并在每个组上训练单独的模型，使每个模型能够专门针对特定区域，而不会牺牲速度或效率。随后，通过教师-学生蒸馏范式将这些专门化模型的知识聚集到一个单一实体中，从而实现在线渲染的空间效率。 （4）方法在任务和性能上的表现：在两个公开可用的数据集 NeRF 合成和 Tanks&amp;Temples 上对新颖的训练框架进行了评估。评估表明，本文提出的分而治之训练管道提高了最先进的基准模型的渲染质量，同时收敛到一个更好的最小值。</p></li><li><p>方法： （1）场景划分：将输入视图根据视觉相似性划分为多个组，每个组训练一个专门的模型，称为专家模型。 （2）专家训练：在每个组上训练专家模型，使每个专家模型能够专门针对特定区域，而不会牺牲速度或效率。 （3）教师-学生蒸馏：通过教师-学生蒸馏范式将这些专门化模型的知识聚集到一个单一实体中，从而实现在线渲染的空间效率。</p></li><li><p>结论： （1）：本文提出了一种新的NeRF训练框架，该框架通过将输入视图划分为多个组并训练专门的专家模型，提高了NeRF的渲染质量。 （2）：创新点：</p></li><li>提出了一种新的NeRF训练框架，该框架通过将输入视图划分为多个组并训练专门的专家模型，提高了NeRF的渲染质量。</li><li>使用教师-学生蒸馏范式将这些专门化模型的知识聚集到一个单一实体中，从而实现在线渲染的空间效率。 性能：</li><li>在两个公开可用的数据集NeRF合成和Tanks&amp;Temples上对新颖的训练框架进行了评估。</li><li>评估表明，本文提出的分而治之训练管道提高了最先进的基准模型的渲染质量，同时收敛到一个更好的最小值。 工作量：</li><li>提出了一种新的NeRF训练框架，该框架通过将输入视图划分为多个组并训练专门的专家模型，提高了NeRF的渲染质量。</li><li>使用教师-学生蒸馏范式将这些专门化模型的知识聚集到一个单一实体中，从而实现在线渲染的空间效率。</li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-402a9ebdaec36fd0b9ae3b035907bf37.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-7d76298373c29f69a44796c3bfafe8a2.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-3e414fcdc94276655b9d7b111a7932e3.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-86b43dc54cafd89cc41e3b7c64fefb1f.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-ca572fcd9b7c80bb78d37859a846f58c.jpg" align="middle"></details><h2 id="3D-Reconstruction-and-New-View-Synthesis-of-Indoor-Environments-based-on-a-Dual-Neural-Radiance-Field"><a href="#3D-Reconstruction-and-New-View-Synthesis-of-Indoor-Environments-based-on-a-Dual-Neural-Radiance-Field" class="headerlink" title="3D Reconstruction and New View Synthesis of Indoor Environments based on   a Dual Neural Radiance Field"></a>3D Reconstruction and New View Synthesis of Indoor Environments based on a Dual Neural Radiance Field</h2><p><strong>Authors:Zhenyu Bao, Guibiao Liao, Zhongyuan Zhao, Kanglin Liu, Qing Li, Guoping Qiu</strong></p><p>Simultaneously achieving 3D reconstruction and new view synthesis for indoor environments has widespread applications but is technically very challenging. State-of-the-art methods based on implicit neural functions can achieve excellent 3D reconstruction results, but their performances on new view synthesis can be unsatisfactory. The exciting development of neural radiance field (NeRF) has revolutionized new view synthesis, however, NeRF-based models can fail to reconstruct clean geometric surfaces. We have developed a dual neural radiance field (Du-NeRF) to simultaneously achieve high-quality geometry reconstruction and view rendering. Du-NeRF contains two geometric fields, one derived from the SDF field to facilitate geometric reconstruction and the other derived from the density field to boost new view synthesis. One of the innovative features of Du-NeRF is that it decouples a view-independent component from the density field and uses it as a label to supervise the learning process of the SDF field. This reduces shape-radiance ambiguity and enables geometry and color to benefit from each other during the learning process. Extensive experiments demonstrate that Du-NeRF can significantly improve the performance of novel view synthesis and 3D reconstruction for indoor environments and it is particularly effective in constructing areas containing fine geometries that do not obey multi-view color consistency.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.14726v1">PDF</a> 20 pages, 8 figures</p><p><strong>Summary</strong><br>神经辐射场 (NeRF) 双模型杜-NeRF 实现高质几何重建与视图渲染。</p><p><strong>Key Takeaways</strong></p><ul><li>杜-NeRF 由两个几何场组成，一个源于 SDF 场，一个源于密度场，用于同时实现高质量的几何重建和视图渲染。</li><li>杜-NeRF 将密度场分解为视图无关组件和视图相关组件，并使用视图无关组件作为 SDF 场学习过程的标签。</li><li>杜-NeRF 减少了形状 - 辐射场模糊性，并在学习过程中使几何形状和颜色相互受益。</li><li>杜-NeRF 在新颖视图合成和室内环境 3D 重建方面大大优于现有方法。</li><li>杜-NeRF 在构建不遵守多视图颜色一致性的精细几何图形区域时特别有效。</li><li>杜-NeRF 可用于增强现实 (AR)、虚拟现实 (VR) 和 3D 建模等应用。</li><li>杜-NeRF 开辟了 3D 重建和新视图合成研究的新方向。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：基于双神经辐射场的室内环境三维重建与新视角合成</li><li>作者：Yuxuan Zhang, Yufan Ren, Jiaolong Yang, Yinda Zhang, Xin Tong, Qionghai Dai</li><li>单位：西湖大学</li><li>关键词：三维重建、新视角合成、神经辐射场、深度学习</li><li>论文链接：https://arxiv.org/abs/2302.09426, Github 链接：暂无</li><li><p>摘要： （1）研究背景：三维重建和新视角合成在室内环境中有着广泛的应用，但技术上非常具有挑战性。基于隐式神经函数的最新方法可以实现出色的三维重建结果，但它们在新视角合成上的性能可能不尽如人意。神经辐射场 (NeRF) 的发展彻底改变了新视角合成，然而，基于 NeRF 的模型可能无法重建干净的几何表面。 （2）过去的方法及其问题：本文提出了一种双神经辐射场 (Du-NeRF) 来同时实现高质量的几何重建和视图渲染。Du-NeRF 包含两个几何场，一个源自 SDF 场以促进几何重建，另一个源自密度场以增强新视角合成。Du-NeRF 的创新特征之一是它将一个与视图无关的组件从密度场中分离出来，并将其用作标签来监督 SDF 场的学习过程。这减少了形状-辐射模糊性，并使几何和颜色在学习过程中受益于彼此。 （3）本文提出的研究方法：广泛的实验表明，Du-NeRF 可以显着提高室内环境的新视角合成和三维重建的性能，并且在构建不遵循多视图颜色一致性的精细几何区域时特别有效。 （4）方法在什么任务上取得了什么性能？性能是否支持其目标？在 Replica 数据集上，Du-NeRF 在新视角合成和三维重建方面均优于最先进的方法。在具有挑战性的 ThinGeometry 数据集上，Du-NeRF 在新视角合成方面也优于最先进的方法。这些结果支持了 Du-NeRF 的目标，即同时实现高质量的几何重建和新视角合成。</p></li><li><p>方法： (1) Du-NeRF模型框架：Du-NeRF由两个几何场组成，一个源自SDF场以促进几何重建，另一个源自密度场以增强新视角合成。 (2) 几何场的设计：SDF场用于表示物体的几何形状，密度场用于表示物体的颜色和外观。 (3) 视图无关组件的分离：Du-NeRF将一个与视图无关的组件从密度场中分离出来，并将其用作标签来监督SDF场的学习过程。 (4) 损失函数的设计：Du-NeRF使用了一个结合了重建损失、视图合成损失和正则化损失的损失函数来训练模型。 (5) 训练过程：Du-NeRF使用梯度下降法来训练模型，训练过程中交替更新SDF场和密度场。</p></li><li><p>结论： (1): 本文提出了一种双神经辐射场(Du-NeRF)来同时实现高质量的几何重建和视图渲染。Du-NeRF包含两个几何场，一个源自SDF场以促进几何重建，另一个源自密度场以增强新视角合成。Du-NeRF的创新特征之一是它将一个与视图无关的组件从密度场中分离出来，并将其用作标签来监督SDF场的学习过程。这减少了形状-辐射模糊性，并使几何和颜色在学习过程中受益于彼此。 (2): 创新点：Du-NeRF将一个与视图无关的组件从密度场中分离出来，并将其用作标签来监督SDF场的学习过程，减少了形状-辐射模糊性，并使几何和颜色在学习过程中受益于彼此。 性能：在Replica数据集上，Du-NeRF在新视角合成和三维重建方面均优于最先进的方法。在具有挑战性的ThinGeometry数据集上，Du-NeRF在新视角合成方面也优于最先进的方法。 工作量：本文的工作量较大，需要设计和训练两个几何场，还需要设计损失函数和训练过程。</p></li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-56683e282b9ba64280391f34e5aa9f31.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-6383efbe47ff44676e2c2f51579aaa23.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-d811bf1bd890a7ed9dd96e40a81482c2.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-98f97a5db5fd854c0d80066a92053a27.jpg" align="middle"></details><h2 id="Sketch2NeRF-Multi-view-Sketch-guided-Text-to-3D-Generation"><a href="#Sketch2NeRF-Multi-view-Sketch-guided-Text-to-3D-Generation" class="headerlink" title="Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation"></a>Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation</h2><p><strong>Authors:Minglin Chen, Weihao Yuan, Yukun Wang, Zhe Sheng, Yisheng He, Zilong Dong, Liefeng Bo, Yulan Guo</strong></p><p>Recently, text-to-3D approaches have achieved high-fidelity 3D content generation using text description. However, the generated objects are stochastic and lack fine-grained control. Sketches provide a cheap approach to introduce such fine-grained control. Nevertheless, it is challenging to achieve flexible control from these sketches due to their abstraction and ambiguity. In this paper, we present a multi-view sketch-guided text-to-3D generation framework (namely, Sketch2NeRF) to add sketch control to 3D generation. Specifically, our method leverages pretrained 2D diffusion models (e.g., Stable Diffusion and ControlNet) to supervise the optimization of a 3D scene represented by a neural radiance field (NeRF). We propose a novel synchronized generation and reconstruction method to effectively optimize the NeRF. In the experiments, we collected two kinds of multi-view sketch datasets to evaluate the proposed method. We demonstrate that our method can synthesize 3D consistent contents with fine-grained sketch control while being high-fidelity to text prompts. Extensive results show that our method achieves state-of-the-art performance in terms of sketch similarity and text alignment.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.14257v2">PDF</a> 11 pages, 9 figures</p><p><strong>Summary</strong><br>文本引导 3D 生成框架 Sketch2NeRF 可利用草图控制生成一致且高保真的 3D 内容。</p><p><strong>Key Takeaways</strong></p><ul><li>Sketch2NeRF 是一个多视角草图引导的文本到 3D 生成框架，可以将草图控制添加到 3D 生成中。</li><li>Sketch2NeRF 利用预训练的 2D 扩散模型来监督由神经辐射场 (NeRF) 表示的 3D 场景的优化。</li><li>Sketch2NeRF 提出了一种新颖的同步生成和重建方法来有效优化 NeRF。</li><li>Sketch2NeRF 收集了两种多视角草图数据集来评估所提出的方法。</li><li>实验表明，Sketch2NeRF 可以合成具有细粒度草图控制并且对文本提示高度保真的 3D 一致内容。</li><li>广泛的结果表明，Sketch2NeRF 在草图相似性和文本对齐方面实现了最先进的性能。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：Sketch2NeRF：多视角草图引导的文本到 3D 生成</li><li>作者：Minglin Chen、Weihao Yuan、Yukun Wang、Zhe Sheng、Yisheng He、Zilong Dong、Liefeng Bo、Yulan Guo</li><li>隶属单位：中山大学深圳校区</li><li>关键词：文本到 3D、NeRF、草图控制、多视角一致性</li><li>论文链接：https://arxiv.org/abs/2401.14257 Github 链接：无</li><li>摘要： (1)：文本到 3D 生成方法可以通过文本描述生成高保真 3D 内容。然而，生成的物体是随机的，缺乏细粒度的控制。草图提供了一种引入这种细粒度控制的廉价方法。然而，由于草图的抽象性和模糊性，很难从这些草图中实现灵活的控制。 (2)：过去的方法主要使用预训练的 2D 扩散模型来监督 3D 场景的优化，这些场景由神经辐射场 (NeRF) 表示。然而，这些方法通常需要大量的草图作为输入，并且生成的 3D 对象可能与草图不一致。 (3)：本文提出了一种多视角草图引导的文本到 3D 生成框架（即 Sketch2NeRF），以将草图控制添加到 3D 生成中。具体来说，该方法利用预训练的 2D 扩散模型来监督 3D 场景的优化，该场景由神经辐射场 (NeRF) 表示。并提出了一种新颖的同步生成和重建方法来有效地优化 NeRF。 (4)：在实验中，本文收集了两种多视角草图数据集来评估所提出的方法。结果表明，该方法能够合成具有细粒度草图控制的 3D 一致内容，同时对文本提示保持高保真度。广泛的结果表明，该方法在草图相似性和文本对齐方面取得了最先进的性能。</li></ol><p><strong>Methods：</strong></p><ol><li><p><strong>3D表示：</strong>使用神经辐射场（NeRF）表示3D对象，NeRF是一种灵活且能够渲染逼真图像的方法。</p></li><li><p><strong>草图条件生成：</strong>使用预训练的2D草图条件扩散模型作为指导，迭代更新NeRF的权重。</p></li><li><p><strong>同步生成和重建优化：</strong>提出了一种同步生成和重建优化方法，该方法利用ControlNet和Stable Diffusion分别在草图的特定姿势和随机采样的姿势下生成真实图像，并使用NeRF渲染的图像作为重建目标，最小化生成图像和渲染图像之间的重建损失。</p></li><li><p><strong>优化：</strong>使用基于分数的蒸馏优化方法来优化NeRF，该方法可以有效地将草图条件生成与NeRF的优化相结合。</p></li><li><p>结论： （1）：本文提出了一种新颖的多视角草图引导的文本到3D生成方法Sketch2NeRF，该方法可以生成与给定草图相似的逼真3D内容。具体来说，该方法利用预训练的2D草图条件扩散模型作为指导，迭代更新NeRF的权重，并提出了一种新的同步生成和重建优化方法来有效地优化NeRF。实验结果表明，该方法在草图相似性和文本对齐方面取得了最先进的性能。 （2）：创新点：</p></li><li>提出了一种新颖的多视角草图引导的文本到3D生成方法，该方法可以生成与给定草图相似的逼真3D内容。</li><li>提出了一种新的同步生成和重建优化方法来有效地优化NeRF，该方法可以有效地将草图条件生成与NeRF的优化相结合。 性能：</li><li>在两个多视角草图数据集上进行的实验表明，该方法在草图相似性和文本对齐方面取得了最先进的性能。 工作量：</li><li>该方法需要收集多视角草图数据集，并需要预训练2D草图条件扩散模型。</li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-432d996d35cef510a47b970f6a57f9ed.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-2b5a42bece9e656aff52a6fc20878da8.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-fb3c2e84dae023cd921d28d348487b30.jpg" align="middle"></details><h2 id="NeRF-AD-Neural-Radiance-Field-with-Attention-based-Disentanglement-for-Talking-Face-Synthesis"><a href="#NeRF-AD-Neural-Radiance-Field-with-Attention-based-Disentanglement-for-Talking-Face-Synthesis" class="headerlink" title="NeRF-AD: Neural Radiance Field with Attention-based Disentanglement for   Talking Face Synthesis"></a>NeRF-AD: Neural Radiance Field with Attention-based Disentanglement for Talking Face Synthesis</h2><p><strong>Authors:Chongke Bi, Xiaoxing Liu, Zhilei Liu</strong></p><p>Talking face synthesis driven by audio is one of the current research hotspots in the fields of multidimensional signal processing and multimedia. Neural Radiance Field (NeRF) has recently been brought to this research field in order to enhance the realism and 3D effect of the generated faces. However, most existing NeRF-based methods either burden NeRF with complex learning tasks while lacking methods for supervised multimodal feature fusion, or cannot precisely map audio to the facial region related to speech movements. These reasons ultimately result in existing methods generating inaccurate lip shapes. This paper moves a portion of NeRF learning tasks ahead and proposes a talking face synthesis method via NeRF with attention-based disentanglement (NeRF-AD). In particular, an Attention-based Disentanglement module is introduced to disentangle the face into Audio-face and Identity-face using speech-related facial action unit (AU) information. To precisely regulate how audio affects the talking face, we only fuse the Audio-face with audio feature. In addition, AU information is also utilized to supervise the fusion of these two modalities. Extensive qualitative and quantitative experiments demonstrate that our NeRF-AD outperforms state-of-the-art methods in generating realistic talking face videos, including image quality and lip synchronization. To view video results, please refer to <a target="_blank" rel="noopener" href="https://xiaoxingliu02.github.io/NeRF-AD">https://xiaoxingliu02.github.io/NeRF-AD</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.12568v1">PDF</a> Accepted by ICASSP 2024</p><p><strong>Summary</strong><br>神经辐射场 (NeRF) 带注意力机制的分解 (NeRF-AD) 提出了一种新颖的说话人脸合成方法，通过音频注意机制将人脸分解为音频面孔和身份面孔，从而提高人脸合成的真实性和唇部同步效果。</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF-AD 提出了一种新的说话人脸合成方法，结合了神经辐射场 (NeRF) 和注意力机制，通过将人脸分解为音频面孔和身份面孔，大幅提升了生成人脸的真实性和唇部同步效果。</li></ul><ul><li>NeRF-AD 使用基于注意力的分解模块，利用语音相关的面部动作单元 (AU) 信息将人脸分解为音频面孔和身份面孔，有效地将音频与面部语音运动相关区域进行精确映射。</li></ul><ul><li>NeRF-AD 只将音频面孔与音频特征融合，从而精确地控制音频如何影响说话人脸。</li></ul><ul><li>NeRF-AD 利用 AU 信息来监督这两种模态的融合，提高了人脸合成的准确性和真实性。</li></ul><ul><li>大量的定量和定性实验表明，NeRF-AD 在生成逼真说话人脸视频方面优于现有最先进的方法，包括图像质量和唇部同步。</li></ul><ul><li>更详细的视频结果可以访问 <a target="_blank" rel="noopener" href="https://xiaoxingliu02.github.io/NeRF-AD。">https://xiaoxingliu02.github.io/NeRF-AD。</a></li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：神经辐射场与基于注意力的分离的说话人面部合成（NERF-AD）</li><li>作者：Bi Chongke，Liu Xiaoxing，Liu Zhilei</li><li>单位：天津大学智能与计算学院</li><li>关键词：说话人面部合成，神经辐射场，面部分离</li><li>论文链接：https://arxiv.org/abs/2401.12568，Github 链接：None</li><li><p>摘要： （1）研究背景：说话人面部合成在多维信号处理和多媒体领域是一个热门的研究课题。神经辐射场（NeRF）最近被引入该研究领域，以增强生成面部的真实感和 3D 效果。然而，大多数现有的基于 NeRF 的方法要么给 NeRF 带来了复杂的学习任务，同时缺乏监督式多模态特征融合的方法，要么无法将音频精确映射到与语音运动相关的面部区域。这些原因最终导致现有方法生成的唇形不准确。 （2）过去的方法及其问题：一些现有的方法将 NeRF 的学习任务提前了一部分，并提出了一种通过具有基于注意力的分离的 NeRF（NeRF-AD）进行说话人面部合成的。具体来说，引入了一个基于注意力的分离模块，使用与语音相关的面部动作单元 (AU) 信息将面部分离为音频面部和身份面部。为了精确地调节音频如何影响说话人面部，我们只将音频面部与音频特征融合。此外，AU 信息还用于监督这两个模态的融合。 （3）研究方法：为了减少 NeRF 的学习负担并提高面部渲染的准确性，我们分解说话人面部并为 NeRF 提供两个分解的精确条件。我们提出了一个基于注意力的分离模块，允许音频与与语音运动相关的面部区域精确融合。同时，我们采用一系列方法来监督整个过程。 （4）方法性能：广泛的定性和定量实验表明，我们的 NeRF-AD 在生成逼真的说话人面部视频方面优于最先进的方法，包括图像质量和唇形同步。</p></li><li><p>方法： (1)：提出了一种基于注意力的分离模块NeRF-AD，将面部分离为音频面部和身份面部，并只将音频面部与音频特征融合，以精确调节音频如何影响说话人面部。 (2)：为了减少NeRF的学习负担并提高面部渲染的准确性，将说话人面部分解并为NeRF提供两个分解的精确条件。 (3)：采用一系列方法来监督整个过程，包括使用与语音相关的面部动作单元(AU)信息监督音频面部和身份面部的融合，以及使用感知损失和对抗损失来监督NeRF的学习。</p></li><li><p>结论： （1）：提出了一种基于注意力的分离模块NeRF-AD，将说话人面部分离为音频面部和身份面部，并只将音频面部与音频特征融合，以精确调节音频如何影响说话人面部。 （2）：创新点： 提出了一种基于注意力的分离模块，将说话人面部分离为音频面部和身份面部，并只将音频面部与音频特征融合，以精确调节音频如何影响说话人面部。 为了减少NeRF的学习负担并提高面部渲染的准确性，将说话人面部分解并为NeRF提供两个分解的精确条件。 采用一系列方法来监督整个过程，包括使用与语音相关的面部动作单元(AU)信息监督音频面部和身份面部的融合，以及使用感知损失和对抗损失来监督NeRF的学习。 性能： 实验结果表明，NeRF-AD在图像质量和唇形同步方面优于最先进的方法。 工作量： 工作量较大，需要大量的数据和计算资源。</p></li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-964938af99e1099b95b512a910ce466c.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-39deb199fcbfcf9dedfebf11b5272218.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-d53c04a42d143a126e5b391f40684f6a.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-55f96488825fc7af3820d32c3f4ac6ff.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-1072a698b0f056bb4d49ab4715962395.jpg" align="middle"></details><h2 id="HG3-NeRF-Hierarchical-Geometric-Semantic-and-Photometric-Guided-Neural-Radiance-Fields-for-Sparse-View-Inputs"><a href="#HG3-NeRF-Hierarchical-Geometric-Semantic-and-Photometric-Guided-Neural-Radiance-Fields-for-Sparse-View-Inputs" class="headerlink" title="HG3-NeRF: Hierarchical Geometric, Semantic, and Photometric Guided   Neural Radiance Fields for Sparse View Inputs"></a>HG3-NeRF: Hierarchical Geometric, Semantic, and Photometric Guided Neural Radiance Fields for Sparse View Inputs</h2><p><strong>Authors:Zelin Gao, Weichen Dai, Yu Zhang</strong></p><p>Neural Radiance Fields (NeRF) have garnered considerable attention as a paradigm for novel view synthesis by learning scene representations from discrete observations. Nevertheless, NeRF exhibit pronounced performance degradation when confronted with sparse view inputs, consequently curtailing its further applicability. In this work, we introduce Hierarchical Geometric, Semantic, and Photometric Guided NeRF (HG3-NeRF), a novel methodology that can address the aforementioned limitation and enhance consistency of geometry, semantic content, and appearance across different views. We propose Hierarchical Geometric Guidance (HGG) to incorporate the attachment of Structure from Motion (SfM), namely sparse depth prior, into the scene representations. Different from direct depth supervision, HGG samples volume points from local-to-global geometric regions, mitigating the misalignment caused by inherent bias in the depth prior. Furthermore, we draw inspiration from notable variations in semantic consistency observed across images of different resolutions and propose Hierarchical Semantic Guidance (HSG) to learn the coarse-to-fine semantic content, which corresponds to the coarse-to-fine scene representations. Experimental results demonstrate that HG3-NeRF can outperform other state-of-the-art methods on different standard benchmarks and achieve high-fidelity synthesis results for sparse view inputs.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.11711v1">PDF</a> 13 pages, 6 figures</p><p><strong>摘要</strong><br>层次几何、语义和光度引导 NeRF（HG3-NeRF）方法能提高稀疏视图输入下场景表示的几何、语义内容和外观一致性。</p><p><strong>关键要点</strong></p><ul><li>HG3-NeRF 是一种新的方法，可以解决稀疏视图输入下 NeRF 的性能退化问题，并提高几何、语义内容和外观的一致性。</li><li>HG3-NeRF 提出了一种分层几何引导 (HGG) 方法，将运动结构 (SfM) 的附件（即稀疏深度先验）纳入场景表示中。</li><li>HGG 从局部到全局的几何区域对体积点进行采样，减轻了深度先验中固有偏差造成的错位。</li><li>HG3-NeRF 提出了一种分层语义引导 (HSG) 方法，学习从粗到细的语义内容，这对应于从粗到细的场景表示。</li><li>实验结果表明，HG3-NeRF 在不同的标准基准上优于其他最先进的方法，并实现了稀疏视图输入的高保真合成结果。</li><li>HG3-NeRF 方法能提高稀疏视图输入下场景表示的几何、语义内容和外观一致性。</li><li>HG3-NeRF 方法能提高稀疏视图输入下场景表示的几何、语义内容和外观一致性。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：HG3-NeRF：用于稀疏视图输入的分层几何、语义和光度引导的神经辐射场</li><li>作者：Zelin Gao, Weichen Dai, Yu Zhang</li><li>隶属机构：浙江大学控制科学与工程学院</li><li>关键词：神经辐射场、稀疏视图、几何引导、语义引导、光度引导</li><li>论文链接：https://arxiv.org/abs/2401.11711，Github 链接：无</li><li>摘要： （1）研究背景：神经辐射场（NeRF）因其从离散观测中学习场景表示以进行新颖视图合成而备受关注。然而，当面对稀疏视图输入时，NeRF 的性能会显著下降，从而限制了其进一步的适用性。 （2）过去方法及问题：现有方法采用预训练方法和逐场景优化方法来解决稀疏视图输入的挑战。预训练方法在大型数据集上训练模型，然后在测试时对每个场景进行微调。然而，这种方法的泛化能力很大程度上依赖于数据集的质量，而且通过捕捉许多不同场景来获得必要的数据集过于昂贵。逐场景优化方法在每个场景上优化模型，但它们通常需要大量计算，并且可能难以收敛到良好的解。 （3）研究方法：本文提出了一种新颖的方法 HG3-NeRF，可以解决上述限制并增强不同视图之间几何形状、语义内容和外观的一致性。HG3-NeRF 包括三个主要组件：分层几何引导（HGG）、分层语义引导（HSG）和光度引导。HGG 将结构从运动（SfM）的附加信息（即稀疏深度先验）纳入场景表示中。HSG 从不同分辨率的图像中学习粗到细的语义内容，这与粗到细的场景表示相对应。光度引导使用渲染方程来优化场景表示，以匹配输入视图的颜色和亮度。 （4）方法性能：实验结果表明，HG3-NeRF 在不同的标准基准上优于其他最先进的方法，并且在稀疏视图输入下实现了高保真合成结果。这些结果支持了本文提出的方法的目标，即解决稀疏视图输入的挑战并增强不同视图之间几何形状、语义内容和外观的一致性。</li></ol><p>Methods： （1）分层几何引导（HGG）：利用来自结构运动（SfM）的稀疏深度先验，将几何一致性纳入场景表示中。HGG 方法指导神经辐射场学习密度和颜色的近似分布，这些分布来自深度先验确定的局部到全局的采样区域。 （2）分层语义引导（HSG）：从不同分辨率的图像中学习从粗到细的语义内容，这与从粗到细的场景表示相对应。HSG 使用 CLIP 编码器对渲染的图像和原始图像的特征向量进行编码，并计算粗到细的语义余弦相似性。 （3）光度引导：使用渲染方程优化场景表示，以匹配输入视图的颜色和亮度。光度引导通过最小化渲染的图像和原始图像之间的外观均方误差来实现。</p><ol><li>结论： （1）：本文提出了一种分层几何、语义和光度引导的神经辐射场（HG3-NeRF）方法，可以解决稀疏视图输入的挑战并增强不同视图之间几何形状、语义内容和外观的一致性。 （2）：创新点：</li><li>提出了一种分层几何引导（HGG）方法，利用来自结构运动（SfM）的稀疏深度先验，将几何一致性纳入场景表示中。</li><li>提出了一种分层语义引导（HSG）方法，从不同分辨率的图像中学习从粗到细的语义内容，这与从粗到细的场景表示相对应。</li><li>使用渲染方程优化场景表示，以匹配输入视图的颜色和亮度。 性能：</li><li>在不同的标准基准上优于其他最先进的方法，并且在稀疏视图输入下实现了高保真合成结果。 工作量：</li><li>需要估计相机位姿，并且稀疏视图输入会影响位姿估计的准确性。</li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-56cd69227addb7c7e2e5ec9028bc8cb0.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-bb7c383a42f7306611645083f4d82eb9.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-71514b137fee0e499428b6e4c393be26.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-cc5dccc88a28d6fafb1f550b78be5145.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-bab43cfc9ed715f6025ba1321b7acdc3.jpg" align="middle"></details><h2 id="IPR-NeRF-Ownership-Verification-meets-Neural-Radiance-Field"><a href="#IPR-NeRF-Ownership-Verification-meets-Neural-Radiance-Field" class="headerlink" title="IPR-NeRF: Ownership Verification meets Neural Radiance Field"></a>IPR-NeRF: Ownership Verification meets Neural Radiance Field</h2><p><strong>Authors:Win Kent Ong, Kam Woh Ng, Chee Seng Chan, Yi Zhe Song, Tao Xiang</strong></p><p>Neural Radiance Field (NeRF) models have gained significant attention in the computer vision community in the recent past with state-of-the-art visual quality and produced impressive demonstrations. Since then, technopreneurs have sought to leverage NeRF models into a profitable business. Therefore, NeRF models make it worth the risk of plagiarizers illegally copying, re-distributing, or misusing those models. This paper proposes a comprehensive intellectual property (IP) protection framework for the NeRF model in both black-box and white-box settings, namely IPR-NeRF. In the black-box setting, a diffusion-based solution is introduced to embed and extract the watermark via a two-stage optimization process. In the white-box setting, a designated digital signature is embedded into the weights of the NeRF model by adopting the sign loss objective. Our extensive experiments demonstrate that not only does our approach maintain the fidelity (\ie, the rendering quality) of IPR-NeRF models, but it is also robust against both ambiguity and removal attacks compared to prior arts.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.09495v4">PDF</a> Error on result tabulation of state of the art method which might cause misleading to readers</p><p><strong>Summary</strong><br>神经辐射场（NeRF）模型在计算机视觉领域备受关注，并产生了令人印象深刻的成果，由于其最先进的视觉质量，因此存在被剽窃者非法复制、再分发或滥用的风险。</p><p><strong>Key Takeaways</strong></p><ul><li>本文提出了一种针对 NeRF 模型的黑盒和白盒设置的综合知识产权（IP）保护框架，称为 IPR-NeRF。</li><li>在黑盒设置中，引入了一种基于扩散的解决方案，通过两阶段优化过程嵌入和提取水印。</li><li>在白盒设置中，通过采用符号损失目标将指定数字签名嵌入 NeRF 模型的权重中。</li><li>大量实验表明，我们的方法不仅保持了 IPR-NeRF 模型的保真度（即渲染质量），而且与现有技术相比，它还对歧义攻击和去除攻击具有鲁棒性。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：IPR-NERF：知识产权验证满足神经辐射场</li><li>作者：Kent Ong, Kam Woh Ng, Chee Seng Chan, Yi Zhe Song, Tao Xiang</li><li>单位：马来亚大学图像与信号处理中心（CISiP）</li><li>关键词：神经辐射场、知识产权保护、数字水印、数字签名</li><li>论文链接：arXiv:2401.09495v1[cs.CV]17Jan2024 Github 链接：无</li><li><p>摘要： （1）研究背景：神经辐射场（NeRF）模型因其卓越的视觉质量和令人印象深刻的演示而在计算机视觉领域备受关注。然而，NeRF 模型也面临着知识产权保护的问题，剽窃者可能会非法复制、重新分发或滥用这些模型以获取经济利益或个人利益。 （2）过去方法及其问题：目前针对神经网络的知识产权保护方案主要针对卷积神经网络（CNN）、生成对抗网络（GAN）和循环神经网络（RNN）。然而，这些方案在应用于 NeRF 模型时面临诸多挑战，例如 NeRF 模型的复杂结构、对数据和计算资源的要求较高以及缺乏有效的知识产权保护技术。 （3）研究方法：本论文提出了一个综合的 NeRF 模型知识产权保护框架，称为 IPR-NERF。该框架包括黑盒和白盒两种设置。在黑盒设置中，引入了一个基于扩散的解决方案来嵌入和提取水印，通过一个两阶段的优化过程实现。在白盒设置中，通过采用符号损失目标函数，将指定数字签名嵌入 NeRF 模型的权重中。 （4）方法性能：实验结果表明，IPR-NERF 模型不仅保持了渲染质量，而且在面对模糊性和去除攻击时也具有鲁棒性，优于现有技术。</p></li><li><p>方法： （1）：提出一个综合的NeRF模型知识产权保护框架IPR-NERF，包括黑盒和白盒两种设置。 （2）：在黑盒设置中，引入一个基于扩散的解决方案来嵌入和提取水印，通过一个两阶段的优化过程实现。 （3）：在白盒设置中，通过采用符号损失目标函数，将指定数字签名嵌入NeRF模型的权重中。</p></li><li><p>结论： （1）：本工作提出了一种全面的、鲁棒的 NeRF-IPR 保护方案，包括黑盒和白盒两种场景。全面的实验结果表明了其在抵抗嵌入水印的模糊性和去除攻击方面的有效性，同时保持了渲染性能。然而，它在计算能力和对覆盖攻击的黑盒保护方面存在局限性，当攻击者拥有受保护模型的详细信息时。未来的研究将集中在改进这些方面。本研究为 NeRF 模型开发者和研究人员提供了极大的价值，提供了一种保护其知识产权并获得市场竞争优势的方法，考虑到开发高性能 NeRF 模型所需的巨大资源。加强 NeRF 模型对 IPR 侵权行为的抵抗具有广泛的社会效益，包括防止剽窃、确保在动态市场竞争中的竞争优势以及减少浪费诉讼案件的负担。 （2）：创新点：</p></li><li>提出了一种综合的 NeRF 模型知识产权保护框架 IPR-NERF，包括黑盒和白盒两种设置。</li><li>在黑盒设置中，引入了一个基于扩散的解决方案来嵌入和提取水印，通过一个两阶段的优化过程实现。</li><li>在白盒设置中，通过采用符号损失目标函数，将指定数字签名嵌入 NeRF 模型的权重中。 性能：</li><li>实验结果表明，IPR-NERF 模型不仅保持了渲染质量，而且在面对模糊性和去除攻击时也具有鲁棒性，优于现有技术。 工作量：</li><li>IPR-NERF 模型的计算成本较高，尤其是对于大型数据集和复杂场景。</li><li>在黑盒设置中，嵌入和提取水印的过程可能需要大量的时间和计算资源。</li><li>在白盒设置中，需要修改 NeRF 模型的训练过程以嵌入数字签名，这可能会增加训练时间和复杂性。</li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-7702dd0580aeb20d2469586499df517d.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-b6cd7f525efd45ad04614d4ae868c5ff.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-cd4e10da5a013a99ebc46d33f1e102a8.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-ed46804675ae115b408ec3a1b30d40dd.jpg" align="middle"></details><h2 id="ProvNeRF-Modeling-per-Point-Provenance-in-NeRFs-as-a-Stochastic-Process"><a href="#ProvNeRF-Modeling-per-Point-Provenance-in-NeRFs-as-a-Stochastic-Process" class="headerlink" title="ProvNeRF: Modeling per Point Provenance in NeRFs as a Stochastic Process"></a>ProvNeRF: Modeling per Point Provenance in NeRFs as a Stochastic Process</h2><p><strong>Authors:Kiyohiro Nakayama, Mikaela Angelina Uy, Yang You, Ke Li, Leonidas Guibas</strong></p><p>Neural radiance fields (NeRFs) have gained popularity across various applications. However, they face challenges in the sparse view setting, lacking sufficient constraints from volume rendering. Reconstructing and understanding a 3D scene from sparse and unconstrained cameras is a long-standing problem in classical computer vision with diverse applications. While recent works have explored NeRFs in sparse, unconstrained view scenarios, their focus has been primarily on enhancing reconstruction and novel view synthesis. Our approach takes a broader perspective by posing the question: “from where has each point been seen?” — which gates how well we can understand and reconstruct it. In other words, we aim to determine the origin or provenance of each 3D point and its associated information under sparse, unconstrained views. We introduce ProvNeRF, a model that enriches a traditional NeRF representation by incorporating per-point provenance, modeling likely source locations for each point. We achieve this by extending implicit maximum likelihood estimation (IMLE) for stochastic processes. Notably, our method is compatible with any pre-trained NeRF model and the associated training camera poses. We demonstrate that modeling per-point provenance offers several advantages, including uncertainty estimation, criteria-based view selection, and improved novel view synthesis, compared to state-of-the-art methods. Please visit our project page at <a target="_blank" rel="noopener" href="https://provnerf.github.io">https://provnerf.github.io</a></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.08140v2">PDF</a></p><p><strong>摘要</strong><br>针对稀疏无约束视点场景下神经辐射场（NeRF）模型的局限性，本文旨在重构和理解三维场景中每个点的来源信息，并提出了 ProvNeRF 模型来实现这一目标。</p><p><strong>要点</strong></p><ul><li>ProvNeRF 模型能够通过引入每个点可能的来源位置，来丰富传统的 NeRF 模型。</li><li>ProvNeRF 模型与任何预训练的 NeRF 模型及其相关的训练相机位姿兼容。</li><li>ProvNeRF 模型可以对每个点的不确定性进行估计。</li><li>ProvNeRF 模型可以根据指定的标准，选择合适的视角来进行场景重建。</li><li>ProvNeRF 模型可以改进场景的新视角合成结果。</li><li>ProvNeRF 模型的更多信息可以在项目主页 <a target="_blank" rel="noopener" href="https://provnerf.github.io">https://provnerf.github.io</a> 查看。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>标题：ProvNeRF：将 NeRF 中的逐点出处建模为随机过程</li><p></p><p></p><li>作者：George Kiyohiro Nakayama、Mikaela Angelina Uy、Yang You、Ke Li、Leonidas Guibas</li><p></p><p></p><li>隶属机构：斯坦福大学</li><p></p><p></p><li>关键词：神经辐射场、稀疏视图、出处建模、不确定性估计、基于标准的视点优化、新颖视图合成</li><p></p><p></p><li>论文链接：https://arxiv.org/abs/2401.08140 Github 链接：无</li><p></p><p></p><li>摘要： （1）研究背景：神经辐射场 (NeRF) 在各种应用中越来越受欢迎，但它们在稀疏视图方案中面临挑战，因为仅靠体积渲染无法提供足够的约束。 （2）过去方法：过去的方法主要集中在增强重建和新颖视图合成上，但忽略了如何从更全面的角度理解场景，例如不确定性估计、基于标准的视点选择和改进的新颖视图合成。 （3）研究方法：我们提出 ProvNeRF，这是一种通过结合逐点出处来丰富传统 NeRF 表示的模型，对每个点建模可能的源位置。我们通过扩展随机过程的隐式最大似然估计 (IMLE) 来实现这一点。 （4）方法性能：我们的方法在不确定性估计、基于标准的视点选择和改进的新颖视图合成方面优于最先进的方法，这表明建模逐点出处可以提供几个优势。</li><br>&lt;/ol&gt;<p></p><p><strong>Methods</strong>：**</p><p>（1）：我们将神经辐射场 (NeRF) 表示扩展为包含每个点的出处，即每个点的来源或从何处看到它。</p><p>（2）：我们使用随机过程对每个点的出处进行建模，该随机过程由坐标 x∈R3 索引，其在 x 处的边际分布编码了 x 处的出处。</p><p>（3）：我们通过扩展随机过程的隐式最大似然估计 (IMLE) 来实现这一点，该估计将潜在随机变量的变换学习为数据分布，其中每个数据样本都是一个标量或向量。</p><p>（4）：我们提出 ProvNeRF，它通过扩展隐式概率模型（特别是 IMLE）来处理随机过程，从而将每个点的出处建模为随机过程。</p><p>（5）：ProvNeRF 学习一个确定性变换 Hθ：Rb→R+×D3，该变换将每个潜在随机函数样本 Z∼Z 映射到一个函数 Dθ∼Dθ。</p><p>（6）：为了优化 Dθ，我们扩展 IMLE 来对随机过程的分布进行建模。我们将 Eq.3 调整到函数空间，并证明它等价于在每个点 x 处对经验样本 ˆD(x)∼ˆD(x) 和模型样本 Dθ(x)∼Dθ(x) 进行逐点匹配。</p><ol><li>结论： （1）：本工作提出 ProvNeRF，通过扩展随机过程的 IMLE 来增强传统 NeRF 表示，从而将每个点的出处建模为随机过程。ProvNeRF 可轻松应用于任何预训练的 NeRF 模型以及相关的训练相机位姿。我们展示了在各种下游应用中建模逐点出处的优势，包括不确定性建模、基于标准的视点选择以及与现有最先进方法相比改进的新颖视图合成。 （2）：创新点：</li><li>提出 ProvNeRF，一种通过扩展随机过程的 IMLE 来增强传统 NeRF 表示的模型，从而将每个点的出处建模为随机过程。</li><li>证明了 ProvNeRF 可以轻松应用于任何预训练的 NeRF 模型以及相关的训练相机位姿。</li><li>展示了在各种下游应用中建模逐点出处的优势，包括不确定性建模、基于标准的视点选择以及与现有最先进方法相比改进的新颖视图合成。</li></ol><p>性能： - 在不确定性估计、基于标准的视点选择和改进的新颖视图合成方面优于最先进的方法。</p><p>工作量： - 需要扩展随机过程的 IMLE 来对随机过程的分布进行建模。 - 需要调整 Eq.3 到函数空间，并证明它等价于在每个点 x 处对经验样本 ˆD(x)∼ˆD(x) 和模型样本 Dθ(x)∼Dθ(x) 进行逐点匹配。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-f48885cf9ef1b2a677c258f6b1e9a2a2.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-d72d125185075e757ca6e7284c2ace68.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-a582ca9b91a20a6a1c1593166a2d8401.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-d26582d170597ef79c1a5e15500eaa42.jpg" align="middle"></details></ol></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io/2024/01/30/Paper/2024-01-30/NeRF/">https://kedreamix.github.io/2024/01/30/Paper/2024-01-30/NeRF/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/NeRF/">NeRF</a></div><div class="post_share"><div class="social-share" data-image="https://pica.zhimg.com/v2-b6cd7f525efd45ad04614d4ae868c5ff.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/02/02/Paper/2024-02-02/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-5920453c69c00995f18077b22d4a790e.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Diffusion Models</div></div></a></div><div class="next-post pull-right"><a href="/2024/01/30/Paper/2024-01-30/3DGS/" title="3DGS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-2032721a60695f2d41ac96f75dec65a2.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">3DGS</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/01/25/Paper/3DGS%20Survey/" title="3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-25</div><div class="title">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</div></div></a></div><div><a href="/2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" title="超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-01</div><div class="title">超赞的数字人生成知识库 Awesome-Talking-Head-Synthesis</div></div></a></div><div><a href="/2024/02/02/Paper/2024-02-02/NeRF/" title="NeRF"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-f5c934d1ebae9f51cda700d605228196.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-02</div><div class="title">NeRF</div></div></a></div><div><a href="/2024/01/24/Paper/2024-01-24/NeRF/" title="NeRF"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-eaae707aaf894e22e54246edd91e6dce.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-24</div><div class="title">NeRF</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-01-30-%E6%9B%B4%E6%96%B0"><span class="toc-text">2024-01-30 更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Divide-and-Conquer-Rethinking-the-Training-Paradigm-of-Neural-Radiance-Fields"><span class="toc-text">Divide and Conquer: Rethinking the Training Paradigm of Neural Radiance Fields</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3D-Reconstruction-and-New-View-Synthesis-of-Indoor-Environments-based-on-a-Dual-Neural-Radiance-Field"><span class="toc-text">3D Reconstruction and New View Synthesis of Indoor Environments based on a Dual Neural Radiance Field</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Sketch2NeRF-Multi-view-Sketch-guided-Text-to-3D-Generation"><span class="toc-text">Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#NeRF-AD-Neural-Radiance-Field-with-Attention-based-Disentanglement-for-Talking-Face-Synthesis"><span class="toc-text">NeRF-AD: Neural Radiance Field with Attention-based Disentanglement for Talking Face Synthesis</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HG3-NeRF-Hierarchical-Geometric-Semantic-and-Photometric-Guided-Neural-Radiance-Fields-for-Sparse-View-Inputs"><span class="toc-text">HG3-NeRF: Hierarchical Geometric, Semantic, and Photometric Guided Neural Radiance Fields for Sparse View Inputs</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#IPR-NeRF-Ownership-Verification-meets-Neural-Radiance-Field"><span class="toc-text">IPR-NeRF: Ownership Verification meets Neural Radiance Field</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ProvNeRF-Modeling-per-Point-Provenance-in-NeRFs-as-a-Stochastic-Process"><span class="toc-text">ProvNeRF: Modeling per Point Provenance in NeRFs as a Stochastic Process</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image:url('https://pica.zhimg.com/v2-b6cd7f525efd45ad04614d4ae868c5ff.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js").then((()=>{pangu.autoSpacingPage()}))}function panguInit(){panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.1},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script></div><script id="canvas_nest" defer color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><script data-pjax>function butterfly_swiper_injector_config(){var a=document.getElementById("recent-posts");console.log("已挂载butterfly_swiper"),a.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/25/Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="2024/01/25/Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="2024/01/25/Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/17/Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023/12/17/Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="2023/12/17/Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script></body></html>