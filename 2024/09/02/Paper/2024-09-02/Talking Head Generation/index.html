<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>Talking Head Generation | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-09-02  Mini-Omni Language Models Can Hear, Talk While Thinking in Streaming"><meta property="og:type" content="article"><meta property="og:title" content="Talking Head Generation"><meta property="og:url" content="https://kedreamix.github.io/2024/09/02/Paper/2024-09-02/Talking%20Head%20Generation/index.html"><meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World"><meta property="og:description" content="Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-09-02  Mini-Omni Language Models Can Hear, Talk While Thinking in Streaming"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://pic1.zhimg.com/v2-f110c7a74d2aae8799ee5d832e200c66.jpg"><meta property="article:published_time" content="2024-09-01T16:50:38.000Z"><meta property="article:modified_time" content="2024-09-01T16:50:38.124Z"><meta property="article:author" content="Kedreamix"><meta property="article:tag" content="Talking Head Generation"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://pic1.zhimg.com/v2-f110c7a74d2aae8799ee5d832e200c66.jpg"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/2024/09/02/Paper/2024-09-02/Talking%20Head%20Generation/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-74LZ5BEQQ1")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!0,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"简"},noticeOutdate:{limitDay:500,position:"top",messagePrev:"It has been",messageNext:"days since the last update, the content of the article may be outdated."},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:200},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!0,post:!0},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!0,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"Talking Head Generation",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-09-02 00:50:38"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach((e=>{n.setAttribute(e,t[e])})),document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)})),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=24?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 7.0.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml"></head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{const d=document.getElementById("loading-box"),e=document.body,o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l(),window.addEventListener("load",(()=>{o()}))})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">298</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">17</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url('https://pic1.zhimg.com/v2-f110c7a74d2aae8799ee5d832e200c66.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Talking Head Generation</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-09-01T16:50:38.000Z" title="发表于 2024-09-02 00:50:38">2024-09-02</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-09-01T16:50:38.124Z" title="更新于 2024-09-02 00:50:38">2024-09-02</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">6k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>20分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="Talking Head Generation"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-09-02-更新"><a href="#2024-09-02-更新" class="headerlink" title="2024-09-02 更新"></a>2024-09-02 更新</h1><h2 id="Mini-Omni-Language-Models-Can-Hear-Talk-While-Thinking-in-Streaming"><a href="#Mini-Omni-Language-Models-Can-Hear-Talk-While-Thinking-in-Streaming" class="headerlink" title="Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming"></a>Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming</h2><p><strong>Authors:Zhifei Xie, Changqiao Wu</strong></p><p>Recent advances in language models have achieved significant progress. GPT-4o, as a new milestone, has enabled real-time conversations with humans, demonstrating near-human natural fluency. Such human-computer interaction necessitates models with the capability to perform reasoning directly with the audio modality and generate output in streaming. However, this remains beyond the reach of current academic models, as they typically depend on extra TTS systems for speech synthesis, resulting in undesirable latency. This paper introduces the Mini-Omni, an audio-based end-to-end conversational model, capable of real-time speech interaction. To achieve this capability, we propose a text-instructed speech generation method, along with batch-parallel strategies during inference to further boost the performance. Our method also helps to retain the original model’s language capabilities with minimal degradation, enabling other works to establish real-time interaction capabilities. We call this training method “Any Model Can Talk”. We also introduce the VoiceAssistant-400K dataset to fine-tune models optimized for speech output. To our best knowledge, Mini-Omni is the first fully end-to-end, open-source model for real-time speech interaction, offering valuable potential for future research.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.16725v1">PDF</a> 10 pages</p><p><strong>Summary</strong><br>提出“Any Model Can Talk”训练方法，实现实时语音交互的Mini-Omni模型。</p><p><strong>Key Takeaways</strong></p><ol><li>GPT-4o模型达到近人类自然流畅度，实现实时人机对话。</li><li>实时对话需要模型具备音频推理和流式生成输出能力。</li><li>现有学术模型依赖TTS系统，存在延迟问题。</li><li>Mini-Omni是音频基于的端到端对话模型，实现实时语音交互。</li><li>提出文本指令语音生成方法，并采用批并行策略提升性能。</li><li>保留模型语言能力，最小化性能退化。</li><li>引入VoiceAssistant-400K数据集，优化语音输出模型。</li><li>Mini-Omni是首个完全端到端、开源的实时语音交互模型。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p><strong>标题</strong>： Mini-Omni: 语言模型能听会说（基于流式处理的思考）</p></li><li><p><strong>作者</strong>： Zhifei Xie（谢智飞）和Changqiao Wu（吴昌桥）。</p></li><li><p><strong>所属机构</strong>： 第一作者Zhifei Xie来自清华大学。</p></li><li><p><strong>关键词</strong>： Mini-Omni模型，实时语音交互，流式处理，语言模型，语音合成，端对端模型。</p></li><li><p><strong>链接</strong>： 论文链接：暂未提供；GitHub代码链接：<a target="_blank" rel="noopener" href="https://github.com/gpt-omni/mini-omni">GitHub</a>（注意：如果无法访问或链接失效，请尝试其他可靠的学术资源平台）。</p></li><li><p><strong>摘要</strong>：</p><ul><li><strong>(1)</strong> 研究背景：随着语言模型技术的不断进步，与人类进行实时对话已成为可能。然而，当前的模型在直接处理音频模态进行推理和生成输出方面存在困难，这限制了它们在实时交互中的应用。文章在此背景下探讨了Mini-Omni模型的应用。</li><li><strong>(2)</strong> 过去的方法及问题：过去的语言模型在语音合成上通常依赖于文本到语音（TTS）系统，这导致了不理想的延迟。然而，这些方法并不适合实时的、端到端的语音交互。文章指出了现有方法的不足并提出了新的方法。</li><li><strong>(3)</strong> 研究方法：本研究提出了Mini-Omni模型，一个基于音频的端到端对话模型，能够实现实时语音交互。通过引入文本指导的语音生成方法和批并行推理策略来增强性能。同时，通过保留原始模型的语言能力，使其他工作能够建立实时交互能力。该研究还引入了VoiceAssistant-400K数据集来优化模型的语音输出。</li><li><strong>(4)</strong> 任务与性能：Mini-Omni模型在实时语音交互任务上表现出优异的性能。它是第一个完全端到端、开源的实时语音交互模型，为未来的研究提供了有价值的潜力。通过实验结果证明了模型的有效性和实用性。</li></ul></li></ol><p>希望这个总结符合您的要求！如有任何需要调整或补充的地方，请告诉我。</p><ol><li><p>方法：</p><ul><li>(1) 研究背景分析：文章首先指出了当前语言模型技术在直接处理音频模态进行推理和生成输出方面的困难，限制了它们在实时交互中的应用。因此，文章基于这一背景，探讨了Mini-Omni模型的应用潜力。</li><li>(2) 过去方法的回顾与问题：传统语言模型在语音合成上依赖于文本到语音（TTS）系统，这导致了不理想的延迟。然而，这些方法并不适合实时的、端到端的语音交互需求。文章指出了这一不足并寻求新的解决方案。</li><li>(3) Mini-Omni模型的提出：为了克服现有方法的不足，研究团队提出了Mini-Omni模型，一个基于音频的端到端对话模型，能够实现实时语音交互。模型通过引入文本指导的语音生成方法和批并行推理策略来增强性能。此外，保留了原始模型的语言能力，使得其他功能能够在此基础上建立实时交互能力。为了更好地优化模型的语音输出，文章还引入了VoiceAssistant-400K数据集。</li><li>(4) 实验设计与结果：文章通过一系列实验验证了Mini-Omni模型在实时语音交互任务上的性能。实验结果表明，该模型是第一个完全端到端、开源的实时语音交互模型，具有显著的有效性和实用性。此外，实验结果还为未来的研究提供了有价值的参考。</li></ul></li></ol><p>希望这个总结符合您的要求！如果您还有其他问题或需要进一步的解释，请随时告诉我。</p><ol><li>Conclusion:</li></ol><ul><li>(1) 工作的意义：该工作引入了一种名为Mini-Omni的多模态模型，具有直接的语音识别能力，推动了实时语音交互领域的技术发展。该研究在人机交互领域中具有重要的实用价值，并为其他相关研究提供了有价值的参考。</li><li>(2) 创新点、性能和工作量总结：<ul><li>创新点：文章提出了Mini-Omni模型，该模型基于音频的端到端对话模型实现实时语音交互，引入了文本指导的语音生成方法和批并行推理策略，保留了原始模型的语言能力，为其他功能建立实时交互能力提供了基础。此外，文章还引入了VoiceAssistant-400K数据集以优化模型的语音输出。</li><li>性能：Mini-Omni模型在实时语音交互任务上表现出优异的性能，是第一个完全端到端、开源的实时语音交互模型，具有显著的有效性和实用性。</li><li>工作量：文章的工作量大，涉及到模型的构建、实验设计、数据集的制作等多个方面的工作。同时，文章还提供了详细的实验过程和结果分析，为其他研究者提供了有价值的参考。</li></ul></li></ul><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-71026de8fa830b36c55cac0303cdf935.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-0978d5710476765aa733dff4cc3c0839.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-6ff6b6ea275704133ab69e2bf4053833.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-f02a8ee9af043077b3169912bad47db0.jpg" align="middle"></details><h2 id="SpeechCaps-Advancing-Instruction-Based-Universal-Speech-Models-with-Multi-Talker-Speaking-Style-Captioning"><a href="#SpeechCaps-Advancing-Instruction-Based-Universal-Speech-Models-with-Multi-Talker-Speaking-Style-Captioning" class="headerlink" title="SpeechCaps: Advancing Instruction-Based Universal Speech Models with   Multi-Talker Speaking Style Captioning"></a>SpeechCaps: Advancing Instruction-Based Universal Speech Models with Multi-Talker Speaking Style Captioning</h2><p><strong>Authors:Chien-yu Huang, Min-Han Shih, Ke-Han Lu, Chi-Yuan Hsiao, Hung-yi Lee</strong></p><p>Instruction-based speech processing is becoming popular. Studies show that training with multiple tasks boosts performance, but collecting diverse, large-scale tasks and datasets is expensive. Thus, it is highly desirable to design a fundamental task that benefits other downstream tasks. This paper introduces a multi-talker speaking style captioning task to enhance the understanding of speaker and prosodic information. We used large language models to generate descriptions for multi-talker speech. Then, we trained our model with pre-training on this captioning task followed by instruction tuning. Evaluation on Dynamic-SUPERB shows our model outperforming the baseline pre-trained only on single-talker tasks, particularly in speaker and emotion recognition. Additionally, tests on a multi-talker QA task reveal that current models struggle with attributes such as gender, pitch, and speaking rate. The code and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/cyhuang-tw/speechcaps">https://github.com/cyhuang-tw/speechcaps</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.13891v1">PDF</a> SynData4GenAI 2024</p><p><strong>Summary</strong><br>该文提出基于指令的多说话者语音处理任务，提升情感识别与风格理解。</p><p><strong>Key Takeaways</strong></p><ol><li>指令式语音处理研究兴起。</li><li>多任务训练提升模型性能。</li><li>设计基础任务以惠及下游任务。</li><li>提出多说话者风格字幕任务。</li><li>使用大型语言模型生成描述。</li><li>模型在动态-SUPERB测试中优于单说话者任务模型。</li><li>多说话者问答任务中模型在性别、音高和语速识别上表现不佳。</li><li>代码和数据集公开。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>结论：</li></ol><p>（1）xxx作品的意义在于xxx（此处需要根据文章内容填写具体的意义，例如：该作品展示了当代社会的矛盾与冲突，或是揭示了人性的复杂性与多样性等）。</p><p>（2）创新点：xxx（例如：文章在理论框架、研究方法或研究视角上的创新之处）。文章在性能方面的优势在于xxx（例如：研究结果显著提高了某一领域的性能或效率），但也存在一些局限性，如xxx（例如：研究未充分考虑其他影响因素或存在实验样本量较小等）。在工作量方面，文章呈现了xxx的工作量（例如：文章进行了大量的实证研究或数据分析，展现了作者深入和全面的研究态度），但也存在某些重复或冗余的工作内容。总体来说，该文章在某些方面表现出色，但也存在一些改进的空间。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-8021415f823c5ce0acd5bb92d61e09b7.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-8e1c7406db684343030a6fdc9a395106.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-f5d9ab1e6a16acb6ef52191ed789cd35.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-5efff236d713d07c1290261d93c716a3.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-f110c7a74d2aae8799ee5d832e200c66.jpg" align="middle"></details><h2 id="TalkLoRA-Low-Rank-Adaptation-for-Speech-Driven-Animation"><a href="#TalkLoRA-Low-Rank-Adaptation-for-Speech-Driven-Animation" class="headerlink" title="TalkLoRA: Low-Rank Adaptation for Speech-Driven Animation"></a>TalkLoRA: Low-Rank Adaptation for Speech-Driven Animation</h2><p><strong>Authors:Jack Saunders, Vinay Namboodiri</strong></p><p>Speech-driven facial animation is important for many applications including TV, film, video games, telecommunication and AR/VR. Recently, transformers have been shown to be extremely effective for this task. However, we identify two issues with the existing transformer-based models. Firstly, they are difficult to adapt to new personalised speaking styles and secondly, they are slow to run for long sentences due to the quadratic complexity of the transformer. We propose TalkLoRA to address both of these issues. TalkLoRA uses Low-Rank Adaptation to effectively and efficiently adapt to new speaking styles, even with limited data. It does this by training an adaptor with a small number of parameters for each subject. We also utilise a chunking strategy to reduce the complexity of the underlying transformer, allowing for long sentences at inference time. TalkLoRA can be applied to any transformer-based speech-driven animation method. We perform extensive experiments to show that TalkLoRA archives state-of-the-art style adaptation and that it allows for an order-of-complexity reduction in inference times without sacrificing quality. We also investigate and provide insights into the hyperparameter selection for LoRA fine-tuning of speech-driven facial animation models.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.13714v1">PDF</a></p><p><strong>Summary</strong><br>谈头生成中，TalkLoRA通过低秩适应和分块策略有效解决现有模型风格适应性和运行速度问题。</p><p><strong>Key Takeaways</strong></p><ol><li>语音驱动面部动画在多个领域应用广泛。</li><li>现有基于Transformer的模型难以适应新说话风格。</li><li>Transformer模型因二次方复杂度导致长句运行缓慢。</li><li>TalkLoRA利用低秩适应有效适应新说话风格。</li><li>TalkLoRA通过训练少量参数的适配器实现。</li><li>分块策略降低Transformer复杂度，实现长句推理。</li><li>TalkLoRA在风格适应性和推理时间上均有显著提升。</li><li>探讨了LoRA微调的超参数选择。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: TalkLoRA：基于低秩适应的语音驱动动画方法</p></li><li><p>Authors: Jack Saunders, Vinay P Namboodiri</p></li><li><p>Affiliation:</p><ul><li>Jack Saunders: 英国巴斯大学（University of Bath）；Deepreel Ltd公司（位于伦敦）。</li><li>Vinay P Namboodiri: 英国巴斯大学（University of Bath）。</li></ul></li><li><p>Keywords: 语音驱动动画、低秩适应、身份适应、效率优化、Transformer模型。</p></li><li><p>Urls: 论文链接：[论文链接]；代码链接：Github：[待补充（如果可用）]，否则填写“Github:None”。</p></li><li><p>Summary:</p><ul><li>(1) 研究背景：语音驱动的动画技术在电视、电影、视频游戏、电信和AR/VR等领域有广泛应用。虽然基于Transformer的模型在此任务上表现卓越，但它们存在一些问题，如难以适应新的个性化说话风格和对于长句子的处理速度慢。本研究旨在解决这些问题。</li><li>(2) 过去的方法及问题：现有的基于Transformer的模型在适应新说话风格和处理速度方面存在挑战。它们难以有效地适应新的个性化说话风格，并且在处理长句子时速度较慢，这是由于Transformer的二次复杂度导致的。</li><li>(3) 研究方法：本研究提出了TalkLoRA方法，通过低秩适应技术有效地适应新说话风格，即使数据有限。该方法通过为每个主体训练一个小的适配器参数集来实现。同时，研究还采用了一种分块策略，以降低处理的复杂性。</li><li>(4) 任务与性能：TalkLoRA在语音驱动的动画任务上进行了测试，实现了对新的个性化说话风格的有效适应，并显著提高了处理长句子的速度。这些性能改进支持了该方法的目标。实验结果表明，TalkLoRA能够显著提高模型对新说话风格的适应能力，并显著减少处理时间，证明了其有效性。</li></ul></li></ol><p>希望以上总结符合您的要求。</p><ol><li>方法论：</li></ol><p>该研究提出了一种基于低秩适应（Low-Rank Adaptation，LoRA）技术的语音驱动动画方法，TalkLoRA。其主要思想是针对现有的基于Transformer的语音驱动动画系统进行改进，提出一系列改进组件以适应新的个性化说话风格和提速推理过程。具体步骤包括：</p><pre><code>- (1) 研究背景与问题阐述：
    该研究首先指出语音驱动动画技术在电视、电影、视频游戏、电信和AR/VR等领域的广泛应用，以及现有基于Transformer的模型在适应新说话风格和处理速度方面的挑战。研究旨在解决这些问题。

- (2) 研究方法介绍：
    研究提出了TalkLoRA方法，通过低秩适应技术有效地适应新说话风格，即使数据有限。该方法通过为每个主体训练一个小的适配器参数集来实现。同时，研究还采用了一种分块策略，以降低处理的复杂性，提高推理速度。

- (3) 模型架构介绍：
    该方法的模型架构基于所使用的基线模型进行适应。对于实验中使用的情况，可以选择FaceFormer或Imitator作为基线模型。每个模型都由音频编码器、Transformer解码器和每帧解码器三个组件构成。音频编码器负责将音频特征提取出来，Transformer解码器考虑时间信息，运动解码器则负责从Transformer输出中生成顶点。

- (4) 低秩适配器（LoRA）的应用：
    为了将基线模型适应到新的主体，研究使用了低秩适配器（LoRA）。LoRA是一种参数高效的微调方法，通过向权重矩阵添加偏移量来适应模型。研究确定了哪些网络组件适合应用LoRA技术，并探讨了LoRA引入的参数如何平衡模型的表示能力与正则化之间的权衡。

- (5) 分块策略的应用：
    为了提高推理速度，研究采用了分块策略。通过将输入音频分成固定大小的块，并并行处理这些块，从而限制Transformer的上下文窗口。这种方法降低了模型的计算复杂性，提高了处理长句子的速度。

- (6) 实验实现细节：
    研究提供了实施TalkLoRA方法的详细实现细节，包括训练过程、损失函数权重、优化器选择、学习率设置、LoRA的秩和alpha值的选择等。实验结果表明，TalkLoRA能够显著提高模型对新说话风格的适应能力，并显著减少处理时间，证明了其有效性。
</code></pre><ol><li>Conclusion:</li></ol><ul><li>(1) 这项工作的意义在于提出了一种基于低秩适应（LoRA）技术的语音驱动动画方法，TalkLoRA。该方法能够解决现有基于Transformer的模型在适应新说话风格和处理速度方面的挑战，具有重要的实际应用价值。</li><li>(2) 创新点：该研究通过引入低秩适配器（LoRA）技术，有效地提高了模型对新说话风格的适应能力，并采用了分块策略以提高推理速度。同时，该研究将LoRA技术应用于语音驱动动画任务，实现了对个性化说话风格的快速适应。</li><li>性能：实验结果表明，TalkLoRA在语音驱动的动画任务上实现了显著的性能改进，提高了模型对新说话风格的适应能力，并显著减少了处理时间。</li><li>工作量：该文章对TalkLoRA方法进行了详细的介绍和实验验证，包括方法论、模型架构、低秩适配器的应用、分块策略的应用以及实验实现细节等方面。工作量较大，但实验结果证明了方法的有效性。</li></ul><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-3313994c278d325c8ef3fb44a5ba2d76.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-7c2db76f55115f8dd725a17800048f2f.jpg" align="middle"></details><h2 id="Empowering-Whisper-as-a-Joint-Multi-Talker-and-Target-Talker-Speech-Recognition-System"><a href="#Empowering-Whisper-as-a-Joint-Multi-Talker-and-Target-Talker-Speech-Recognition-System" class="headerlink" title="Empowering Whisper as a Joint Multi-Talker and Target-Talker Speech   Recognition System"></a>Empowering Whisper as a Joint Multi-Talker and Target-Talker Speech Recognition System</h2><p><strong>Authors:Lingwei Meng, Jiawen Kang, Yuejiao Wang, Zengrui Jin, Xixin Wu, Xunying Liu, Helen Meng</strong></p><p>Multi-talker speech recognition and target-talker speech recognition, both involve transcription in multi-talker contexts, remain significant challenges. However, existing methods rarely attempt to simultaneously address both tasks. In this study, we propose a pioneering approach to empower Whisper, which is a speech foundation model, to tackle joint multi-talker and target-talker speech recognition tasks. Specifically, (i) we freeze Whisper and plug a Sidecar separator into its encoder to separate mixed embedding for multiple talkers; (ii) a Target Talker Identifier is introduced to identify the embedding flow of the target talker on the fly, requiring only three-second enrollment speech as a cue; (iii) soft prompt tuning for decoder is explored for better task adaptation. Our method outperforms previous methods on two- and three-talker LibriMix and LibriSpeechMix datasets for both tasks, and delivers acceptable zero-shot performance on multi-talker ASR on AishellMix Mandarin dataset.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.09817v2">PDF</a> Accepted to INTERSPEECH 2024</p><p><strong>Summary</strong><br>提出了一种创新方法，使 Whisper 模型同时应对多说话者和目标说话者的语音识别任务。</p><p><strong>Key Takeaways</strong></p><ul><li>联合处理多说话者和目标说话者的语音识别挑战。</li><li>利用 Whisper 模型，结合 Sidecar 分隔器进行混合嵌入分离。</li><li>引入目标说话者识别器，仅需3秒语音即可识别。</li><li>探索解码器软提示调优以适应任务。</li><li>在 LibriMix 和 LibriSpeechMix 数据集上优于先前方法。</li><li>在 AishellMix 数据集上实现可接受的零样本性能。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: 赋能whisper以联合处理多任务说话者和目标说话者的语音识别挑战</p></li><li><p>Authors: Lingwei Meng, Jiawen Kang, Yuejiao Wang, Zengrui Jin, Xixin Wu, Xunying Liu, Helen Meng</p></li><li><p>Affiliation: 中国香港大学 (The Chinese University of Hong Kong)</p></li><li><p>Keywords: 多说话人语音识别，目标说话人语音识别，提示微调，领域自适应</p></li><li><p>Urls: Paper链接：<a href="具体的链接地址需要您提供">xxx</a>；Github代码链接：<a target="_blank" rel="noopener" href="https://github.com/LingweiMeng/Whisper-Sidecar">Github</a>（若不可用则填”None”）</p></li><li><p>Summary:</p><ul><li><p>(1)研究背景：多说话人和目标说话人的语音识别在多种场景下均具有重要意义，特别是在语音交互和信息检索等领域。然而，现有方法往往针对单一任务进行设计，缺乏同时处理两个任务的能力。</p><p>-(2)过去的方法及其问题：早期的方法通常使用级联系统，通过语音分离模块将混合语音信号分离，然后输入到单说话人语音识别系统进行转录。然而，这些方法由于优化目标不匹配，通常需要联合训练来提高性能。最近，端到端的模型因其出色的性能而受到关注，但在训练多说话人端到端语音识别系统时，如何将预测与对应的目标标签关联起来以计算损失是一个主要挑战。尽管有一些方法如Permutation Invariant Training (PIT)、Heuristic Error Assignment Training (HEAT)和Serialized Output Training (SOT)等已经取得了一些成果，但它们通常需要从头开始训练或重新微调预训练模型，无法充分利用现有的单说话人语音识别模型的进步。</p><p>-(3)研究方法：针对上述问题，本文提出了一种基于Whisper模型的联合多说话人和目标说话人语音识别系统。首先，通过冻结Whisper模型的权重并引入Sidecar分离器到其编码器中以分离混合嵌入向量。其次，引入目标说话人识别器来实时识别目标说话人的嵌入流，仅需要三秒的注册语音作为提示。最后，采用软提示调整为解码器进行更好的任务适应。</p><p>-(4)任务与性能：本文方法在英文的LibriMix和LibriSpeechMix数据集以及Mandarin的AishellMix数据集上进行了实验验证。相较于之前的方法，本文方法在两项任务上都取得了领先性能。特别是在多说话人语音识别任务上，本文方法实现了令人满意的零样本性能。实验结果支持了该方法的有效性。</p></li></ul></li><li><p>方法论：</p><ul><li><p>(1) 研究背景与问题：针对多说话人和目标说话人的语音识别问题，现有的方法往往针对单一任务设计，缺乏同时处理两个任务的能力。因此，本文提出一种基于Whisper模型的联合多说话人和目标说话人语音识别系统。</p></li><li><p>(2) 方法框架：首先，通过冻结Whisper模型的权重并引入Sidecar分离器到其编码器中分离混合嵌入向量。其次，引入目标说话人识别器实时识别目标说话人的嵌入流，仅需要三秒的注册语音作为提示。再次，采用软提示调整为解码器进行更好的任务适应。</p></li><li><p>(3) 关键技术：采用Whisper作为语音识别的基础模型，Sidecar分离器用于将混合嵌入向量分离，目标说话人识别器用于识别目标说话人的嵌入流，软提示调整用于适应多任务语音识别。</p></li><li><p>(4) 数据集与实验设置：在英文的LibriMix、LibriSpeechMix数据集和Mandarin的AishellMix数据集上进行实验验证。对数据集进行预处理，以适应模型输入的要求。</p></li><li><p>(5) 训练目标：采用Permutation Invariant Training（PIT）方法确定说话人顺序，解决标签模糊问题。最终的目标函数是PIT-ASR损失和对应TTI损失的加权和。</p></li><li><p>(6) 模型评估：通过对比实验，验证所提出方法在多项任务上的性能表现，并与其他先进方法进行比较。实验结果支持该方法的有效性。</p></li></ul></li><li>Conclusion:</li></ol><p>(1) 该工作的重要性：</p><p>该文章研究了多说话人和目标说话人的语音识别问题，提出了一种基于Whisper模型的联合多任务语音识别系统。这一研究对于提高语音交互和信息检索等领域的性能和用户体验具有重要意义。</p><p>(2) 创新性、性能和工作量评价：</p><ul><li>创新性：文章提出了一种新的基于Whisper模型的联合多说话人和目标说话人语音识别系统，通过引入Sidecar分离器和目标说话人识别器，实现了对混合语音信号的有效分离和识别。此外，采用软提示调整解码器，提高了系统的性能。</li><li>性能：文章在多个数据集上进行了实验验证，相较于之前的方法，所提出的方法在多说话人和目标说话人语音识别任务上都取得了领先性能。实验结果支持了该方法的有效性。</li><li>工作量：文章对问题的研究深入，方法新颖，实验设计合理，数据量大且处理得当，工作量较大。</li></ul><p>总体来说，该文章在解决多说话人和目标说话人的语音识别问题上取得了一定的进展，具有一定的创新性和实用性。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-ad0809bf1f2a0e13bfb58fed883c328f.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-4760bb1b1f83c77ff470a2676d9247aa.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-ba94c08ea3020d878a6417b75885d8b6.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-bbbcd66af9e5a0c566946800bba17655.jpg" align="middle"></details></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io/2024/09/02/Paper/2024-09-02/Talking%20Head%20Generation/">https://kedreamix.github.io/2024/09/02/Paper/2024-09-02/Talking Head Generation/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Talking-Head-Generation/">Talking Head Generation</a></div><div class="post_share"><div class="social-share" data-image="https://pic1.zhimg.com/v2-f110c7a74d2aae8799ee5d832e200c66.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/09/02/Paper/2024-09-02/3DGS/" title="3DGS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-3bb8211b03b171a8f4a7ce70802b43cd.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">3DGS</div></div></a></div><div class="next-post pull-right"><a href="/2024/09/02/Paper/2024-09-02/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙/虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-16bf2abe47a9322d8a354326839ca5bd.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">元宇宙/虚拟人</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/03/11/Note/BlendShape/" title="Blendshape学习笔记"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://p6-sign.toutiaoimg.com/pgc-image/2c8cbd123e00470e95500a8ae62da605~noop.image?_iz=58558&from=article.pc_detail&lk3s=953192f4&x-expires=1710668214&x-signature=UHPhjWP4v96kbtfJzF97Z%2Bp3klc%3D" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-11</div><div class="title">Blendshape学习笔记</div></div></a></div><div><a href="/2024/03/15/Paperscape/Real3D-Portrait/" title="REAL3D-PORTRAIT ONE-SHOT REALISTIC 3D TALKING PORTRAIT SYNTHESIS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-68585b79de5f83b0dfa23304f41b9b98.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-15</div><div class="title">REAL3D-PORTRAIT ONE-SHOT REALISTIC 3D TALKING PORTRAIT SYNTHESIS</div></div></a></div><div><a href="/2024/03/03/Paperscape/EMO/" title="EMO Emote Portrait Alive - 阿里HumanAIGC"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-6492e24fb03ffa98135dc584535ab7d9.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-03</div><div class="title">EMO Emote Portrait Alive - 阿里HumanAIGC</div></div></a></div><div><a href="/2024/03/07/Paperscape/SyncTalk/" title="SyncTalk The Devil is in the Synchronization for Talking Head Synthesis"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-a57e0937b2f452009023394a59529dfb.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-07</div><div class="title">SyncTalk The Devil is in the Synchronization for Talking Head Synthesis</div></div></a></div><div><a href="/2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" title="超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-01</div><div class="title">超赞的数字人生成知识库 Awesome-Talking-Head-Synthesis</div></div></a></div><div><a href="/2024/03/05/Paperscape/VividTalk/" title="VividTalk One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-8521b04f82075cc27b5e95148dba9792.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-05</div><div class="title">VividTalk One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-09-02-%E6%9B%B4%E6%96%B0"><span class="toc-text">2024-09-02 更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Mini-Omni-Language-Models-Can-Hear-Talk-While-Thinking-in-Streaming"><span class="toc-text">Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SpeechCaps-Advancing-Instruction-Based-Universal-Speech-Models-with-Multi-Talker-Speaking-Style-Captioning"><span class="toc-text">SpeechCaps: Advancing Instruction-Based Universal Speech Models with Multi-Talker Speaking Style Captioning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#TalkLoRA-Low-Rank-Adaptation-for-Speech-Driven-Animation"><span class="toc-text">TalkLoRA: Low-Rank Adaptation for Speech-Driven Animation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Empowering-Whisper-as-a-Joint-Multi-Talker-and-Target-Talker-Speech-Recognition-System"><span class="toc-text">Empowering Whisper as a Joint Multi-Talker and Target-Talker Speech Recognition System</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image:url('https://pic1.zhimg.com/v2-f110c7a74d2aae8799ee5d832e200c66.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js").then((()=>{pangu.autoSpacingPage()}))}function panguInit(){panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.1},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script></div><script id="canvas_nest" defer color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><script data-pjax>function butterfly_swiper_injector_config(){var a=document.getElementById("recent-posts");console.log("已挂载butterfly_swiper"),a.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/25/Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="2024/01/25/Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="2024/01/25/Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/17/Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023/12/17/Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="2023/12/17/Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script></body></html>