<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>元宇宙/虚拟人 | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="元宇宙&#x2F;虚拟人 方向最新论文已更新，请持续关注 Update in 2024-09-02  Hyperdimensional Computing Empowered Federated Foundation Model over   Wireless Networks for Metaverse"><meta property="og:type" content="article"><meta property="og:title" content="元宇宙&#x2F;虚拟人"><meta property="og:url" content="https://kedreamix.github.io/2024/09/02/Paper/2024-09-02/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/index.html"><meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World"><meta property="og:description" content="元宇宙&#x2F;虚拟人 方向最新论文已更新，请持续关注 Update in 2024-09-02  Hyperdimensional Computing Empowered Federated Foundation Model over   Wireless Networks for Metaverse"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://picx.zhimg.com/v2-16bf2abe47a9322d8a354326839ca5bd.jpg"><meta property="article:published_time" content="2024-09-01T16:41:49.000Z"><meta property="article:modified_time" content="2024-09-01T16:41:49.470Z"><meta property="article:author" content="Kedreamix"><meta property="article:tag" content="元宇宙&#x2F;虚拟人"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://picx.zhimg.com/v2-16bf2abe47a9322d8a354326839ca5bd.jpg"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/2024/09/02/Paper/2024-09-02/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-74LZ5BEQQ1")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!0,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"简"},noticeOutdate:{limitDay:500,position:"top",messagePrev:"It has been",messageNext:"days since the last update, the content of the article may be outdated."},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:200},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!0,post:!0},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!0,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"元宇宙/虚拟人",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-09-02 00:41:49"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach((e=>{n.setAttribute(e,t[e])})),document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)})),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=24?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 7.0.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml"></head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{const d=document.getElementById("loading-box"),e=document.body,o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l(),window.addEventListener("load",(()=>{o()}))})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">239</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url('https://picx.zhimg.com/v2-16bf2abe47a9322d8a354326839ca5bd.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">元宇宙/虚拟人</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-09-01T16:41:49.000Z" title="发表于 2024-09-02 00:41:49">2024-09-02</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-09-01T16:41:49.470Z" title="更新于 2024-09-02 00:41:49">2024-09-02</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">12.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>39分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="元宇宙/虚拟人"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-09-02-更新"><a href="#2024-09-02-更新" class="headerlink" title="2024-09-02 更新"></a>2024-09-02 更新</h1><h2 id="Hyperdimensional-Computing-Empowered-Federated-Foundation-Model-over-Wireless-Networks-for-Metaverse"><a href="#Hyperdimensional-Computing-Empowered-Federated-Foundation-Model-over-Wireless-Networks-for-Metaverse" class="headerlink" title="Hyperdimensional Computing Empowered Federated Foundation Model over   Wireless Networks for Metaverse"></a>Hyperdimensional Computing Empowered Federated Foundation Model over Wireless Networks for Metaverse</h2><p><strong>Authors:Yahao Ding, Wen Shang, Minrui Xu, Zhaohui Yang, Ye Hu, Dusit Niyato, Mohammad Shikh-Bahaei</strong></p><p>The Metaverse, a burgeoning collective virtual space merging augmented reality and persistent virtual worlds, necessitates advanced artificial intelligence (AI) and communication technologies to support immersive and interactive experiences. Federated learning (FL) has emerged as a promising technique for collaboratively training AI models while preserving data privacy. However, FL faces challenges such as high communication overhead and substantial computational demands, particularly for neural network (NN) models. To address these issues, we propose an integrated federated split learning and hyperdimensional computing (FSL-HDC) framework for emerging foundation models. This novel approach reduces communication costs, computation load, and privacy risks, making it particularly suitable for resource-constrained edge devices in the Metaverse, ensuring real-time responsive interactions. Additionally, we introduce an optimization algorithm that concurrently optimizes transmission power and bandwidth to minimize the maximum transmission time among all users to the server. The simulation results based on the MNIST dataset indicate that FSL-HDC achieves an accuracy rate of approximately 87.5%, which is slightly lower than that of FL-HDC. However, FSL-HDC exhibits a significantly faster convergence speed, approximately 3.733x that of FSL-NN, and demonstrates robustness to non-IID data distributions. Moreover, our proposed optimization algorithm can reduce the maximum transmission time by up to 64% compared with the baseline.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.14416v1">PDF</a></p><p><strong>Summary</strong><br>元宇宙虚拟人需AI与通信技术，FSL-HDC框架优化交互性能。</p><p><strong>Key Takeaways</strong></p><ul><li>元宇宙需AI与通信技术支持沉浸式体验。</li><li>联邦学习（FL）解决隐私问题，但面临通信和计算挑战。</li><li>提出FSL-HDC框架，降低通信成本、计算负担和隐私风险。</li><li>优化算法减少传输时间，提高交互响应速度。</li><li>FSL-HDC在MNIST数据集上准确率略低于FL-HDC，但收敛速度更快。</li><li>FSL-HDC对非独立同分布数据分布表现稳健。</li><li>优化算法可降低传输时间64%。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>标题：基于超维计算的联邦分割学习在元宇宙中的研究应用</p></li><li><p>作者：Yahao Ding（丁亚浩）、Wen Shang（尚文）、Minrui Xu（徐敏锐）、Zhaohui Yang（杨朝晖）、Ye Hu（叶华）、Dusit Niyato（杜斯尼亚特）、Mohammad Shikh-Bahaei（穆罕默德·谢赫巴海）。</p></li><li><p>所属机构：金斯顿大学（King’s College London）、南洋理工大学（Nanyang Technological University）、浙江大学（Zhejiang University）、迈阿密大学（University of Miami）。</p></li><li><p>关键词：联邦分割学习（Federated Split Learning）、超维计算（Hyperdimensional Computing）、资源分配。</p></li><li><p>Urls：论文链接待补充；GitHub代码链接待补充（如可用）。</p></li><li><p>总结：</p><ul><li><p>(1)研究背景：随着元宇宙的发展，需要先进的人工智能和通信技术来支持沉浸式和交互式的体验。本文在此背景下进行研究。</p></li><li><p>(2)过去的方法及问题：联邦学习(FL)在训练AI模型时能够保护数据隐私，但面临高通信开销和计算需求大的挑战。分割学习(SL)虽然减轻了这些问题，但仍存在隐私和计算资源利用的问题。文章提出结合联邦分割学习(FSL)和超维计算(HDC)来解决这些问题。</p></li><li><p>(3)研究方法：本文提出了一种结合联邦分割学习和超维计算的框架（FSL-HDC），用于新兴的基础模型。该框架减少了通信成本、计算负载和隐私风险，尤其适合资源有限的边缘设备。同时，引入了一种优化算法，优化传输功率和带宽，以最小化所有用户到服务器的最大传输时间。</p></li><li><p>(4)任务与性能：本文在MNIST数据集上的模拟结果表明，FSL-HDC的准确率约为87.5%，略低于FL-HDC。但FSL-HDC的收敛速度显著快于FSL-NN，且对非独立同分布数据表现出稳健性。此外，提出的优化算法最多可减少64%的最大传输时间。</p></li></ul></li></ol><p>以上内容严格按照您的要求进行撰写，希望符合您的需求。</p><ol><li><p>Conclusion:</p><ul><li><p>(1)意义：该研究工作具有重大意义，随着元宇宙的发展，需要更先进的AI和通信技术来支持沉浸式体验。这篇文章提出结合联邦分割学习和超维计算的方法，为未来的智能计算和通信技术提供了重要思路。该研究的实践价值在于可以应用到边缘计算和智能设备等领域，提升计算和通信的效率，同时也为人工智能的应用提供了保护隐私的方案。</p></li><li><p>(2)从三个维度总结本文的优缺点：创新点、性能、工作量。</p><p>创新点：文章结合联邦分割学习和超维计算提出了一种新的计算框架FSL-HDC，解决传统方法在计算负载、通信开销和隐私保护方面的问题，显示出明显的创新性。<br>性能：文章通过模拟实验验证了FSL-HDC框架的性能，与传统方法相比，FSL-HDC在准确率、收敛速度和对非独立同分布数据的稳健性方面表现出较好的性能。但需要注意的是，FSL-HDC的准确率略低于某些其他方法。<br>工作量：文章进行了大量的模拟实验和理论分析，验证了FSL-HDC的有效性。但关于实际应用的实验验证和代码公开方面可能存在不足，工作量需要进一步加大。</p></li></ul></li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-2285e261623e6fa05e290545c745beed.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-101dde611beb3a60eb66cbaa752aadde.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-5a43707dd5e082c470abc3c1421e41e0.jpg" align="middle"></details><h2 id="Avatar-Concept-Slider-Manipulate-Concepts-In-Your-Human-Avatar-With-Fine-grained-Control"><a href="#Avatar-Concept-Slider-Manipulate-Concepts-In-Your-Human-Avatar-With-Fine-grained-Control" class="headerlink" title="Avatar Concept Slider: Manipulate Concepts In Your Human Avatar With   Fine-grained Control"></a>Avatar Concept Slider: Manipulate Concepts In Your Human Avatar With Fine-grained Control</h2><p><strong>Authors:Yixuan He, Lin Geng Foo, Ajmal Saeed Mian, Hossein Rahmani, Jun Jiu</strong></p><p>Language based editing of 3D human avatars to precisely match user requirements is challenging due to the inherent ambiguity and limited expressiveness of natural language. To overcome this, we propose the Avatar Concept Slider (ACS), a 3D avatar editing method that allows precise manipulation of semantic concepts in human avatars towards a specified intermediate point between two extremes of concepts, akin to moving a knob along a slider track. To achieve this, our ACS has three designs. 1) A Concept Sliding Loss based on Linear Discriminant Analysis to pinpoint the concept-specific axis for precise editing. 2) An Attribute Preserving Loss based on Principal Component Analysis for improved preservation of avatar identity during editing. 3) A 3D Gaussian Splatting primitive selection mechanism based on concept-sensitivity, which updates only the primitives that are the most sensitive to our target concept, to improve efficiency. Results demonstrate that our ACS enables fine-grained 3D avatar editing with efficient feedback, without harming the avatar quality or compromising the avatar’s identifying attributes.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.13995v1">PDF</a></p><p><strong>Summary</strong><br>基于自然语言的3D虚拟人编辑具有挑战性，提出“Avatar Concept Slider”方法，通过概念滑动损失、属性保持损失和概念敏感性机制实现精确编辑。</p><p><strong>Key Takeaways</strong></p><ol><li>自然语言编辑3D虚拟人存在模糊性和表达限制。</li><li>提出“Avatar Concept Slider”（ACS）方法，实现精确编辑。</li><li>ACS包括概念滑动损失、属性保持损失和3D高斯散点机制。</li><li>概念滑动损失基于线性判别分析定位概念特定轴。</li><li>属性保持损失基于主成分分析，保护虚拟人身份。</li><li>3D高斯散点机制根据概念敏感性更新敏感原语。</li><li>结果表明ACS能实现细粒度编辑，反馈高效，不损害质量。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>标题：基于概念滑块的3D人物角色编辑方法</p></li><li><p>作者：何宜宣，林庚符，Ajmal Saeed Mian，侯赛因·拉赫曼尼，久九修 （英文名字在前）</p></li><li><p>隶属机构：新加坡科技与设计大学、澳大利亚西澳大利亚大学以及兰卡斯特大学。</p></li><li><p>关键词：Avatar 编辑、语言编辑、概念滑块、3D模型编辑、精准控制。</p></li><li><p>Urls：论文链接待补充；GitHub代码链接（如果有）：GitHub:None。</p></li><li><p>摘要：</p><ul><li><p>(1)研究背景：随着游戏开发、电影制作、元宇宙和直播应用等领域的快速发展，对高保真度人物角色的需求日益增加。用户需要便捷地编辑和调整自己的个性化数字角色，如改变发型、服装等。然而，现有的基于文本提示的编辑方法存在局限性，难以实现精细控制和精确匹配用户需求。因此，本文提出了一种新的3D人物角色编辑方法。</p></li><li><p>(2)过去的方法及问题：现有的3D人物角色编辑方法主要依赖于文本提示作为指导信号，存在表达性有限和内在模糊性的问题。这些方法难以精确控制人物角色的语义概念，无法实现用户所需的精细调整。</p></li><li><p>(3)研究方法：本文提出了Avatar Concept Slider（ACS）方法，通过滑动概念滑块来精确操控人物角色中的语义概念。ACS设计包括三个关键部分：基于线性判别分析的Concept Sliding Loss，用于定位特定概念轴以实现精确编辑；基于主成分分析的Attribute Preserving Loss，用于在编辑过程中改进角色身份的保留；基于概念敏感性的3D高斯Splatting原始选择机制，仅更新对目标概念最敏感的原始部分以提高效率。</p></li><li><p>(4)任务与性能：本文的方法在3D人物角色编辑任务上取得了显著成果，实现了精细的反馈控制，同时保证了角色质量和身份识别属性的保留。实验结果证明了ACS方法的有效性，其性能支持了方法的目标。</p></li></ul></li></ol><p>以上内容按照要求进行了概括，并保持了适当的学术严谨性和简洁性。</p><ol><li>结论：</li></ol><p>(1) 这项工作的意义在于提出了一种新型的3D人物角色编辑方法，即Avatar Concept Slider（ACS）方法，该方法能够使用户精确地编辑和调整他们的3D角色，以满足他们对特定概念表达的需求。这对于游戏开发、电影制作、元宇宙和直播应用等领域具有重大意义，有助于提高人物角色的逼真度和用户的个性化体验。</p><p>(2) 创新点：该文章的创新之处在于通过引入概念滑块来实现3D人物角色的精确编辑，这是一种全新的交互方式。此外，文章还提出了基于线性判别分析的Concept Sliding Loss和基于主成分分析的Attribute Preserving Loss，以实现更精细的编辑和角色身份保留。</p><p>性能：该文章提出的ACS方法在3D人物角色编辑任务上取得了显著成果，实现了精细的反馈控制，保证了角色质量和身份识别属性的保留。实验结果证明了ACS方法的有效性。</p><p>工作量：文章进行了大量的实验和评估，验证了所提出方法的有效性和优越性。然而，文章未提供关于计算复杂度和运行时间的详细数据，难以评估该方法的实际计算成本。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-16bf2abe47a9322d8a354326839ca5bd.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-b10adc5ed7df959917b10ecc0d45ca0a.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-cb2a659c13c1c9e3088d34b4c1379847.jpg" align="middle"></details><h2 id="GenCA-A-Text-conditioned-Generative-Model-for-Realistic-and-Drivable-Codec-Avatars"><a href="#GenCA-A-Text-conditioned-Generative-Model-for-Realistic-and-Drivable-Codec-Avatars" class="headerlink" title="GenCA: A Text-conditioned Generative Model for Realistic and Drivable   Codec Avatars"></a>GenCA: A Text-conditioned Generative Model for Realistic and Drivable Codec Avatars</h2><p><strong>Authors:Keqiang Sun, Amin Jourabloo, Riddhish Bhalodia, Moustafa Meshry, Yu Rong, Zhengyu Yang, Thu Nguyen-Phuoc, Christian Haene, Jiu Xu, Sam Johnson, Hongsheng Li, Sofien Bouaziz</strong></p><p>Photo-realistic and controllable 3D avatars are crucial for various applications such as virtual and mixed reality (VR/MR), telepresence, gaming, and film production. Traditional methods for avatar creation often involve time-consuming scanning and reconstruction processes for each avatar, which limits their scalability. Furthermore, these methods do not offer the flexibility to sample new identities or modify existing ones. On the other hand, by learning a strong prior from data, generative models provide a promising alternative to traditional reconstruction methods, easing the time constraints for both data capture and processing. Additionally, generative methods enable downstream applications beyond reconstruction, such as editing and stylization. Nonetheless, the research on generative 3D avatars is still in its infancy, and therefore current methods still have limitations such as creating static avatars, lacking photo-realism, having incomplete facial details, or having limited drivability. To address this, we propose a text-conditioned generative model that can generate photo-realistic facial avatars of diverse identities, with more complete details like hair, eyes and mouth interior, and which can be driven through a powerful non-parametric latent expression space. Specifically, we integrate the generative and editing capabilities of latent diffusion models with a strong prior model for avatar expression driving. Our model can generate and control high-fidelity avatars, even those out-of-distribution. We also highlight its potential for downstream applications, including avatar editing and single-shot avatar reconstruction.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.13674v1">PDF</a></p><p><strong>Summary</strong><br>通过文本条件生成模型，实现高保真、多身份虚拟人面部建模与驱动。</p><p><strong>Key Takeaways</strong></p><ul><li>3D虚拟人应用广泛，传统创建方法耗时且缺乏灵活性。</li><li>生成模型可加速数据采集与处理，拓展应用范围。</li><li>现有生成模型存在局限性，如静态、缺乏细节和驱动性。</li><li>提出文本条件生成模型，生成多身份、高保真面部虚拟人。</li><li>集成生成与编辑能力，实现驱动和单次重建。</li><li>模型适用于面部编辑和单次重建等下游应用。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: 基于文本引导生成逼真的三维头像研究</p></li><li><p>Authors: xxx，xxx，xxx等。</p></li><li><p>Affiliation: 第一作者系Meta公司实习员工。其他作者信息未提供。</p></li><li><p>Keywords: 三维头像生成；文本引导；逼真；面部细节；扩散模型。</p></li><li><p>Urls: 文章链接未提供；GitHub代码链接未提供。</p></li><li><p>Summary:</p></li></ol><p>(1) 研究背景：随着虚拟现实、混合现实、远程出席、游戏制作等领域的快速发展，对逼真的三维头像的需求越来越大。传统的头像创建方法需要大量扫描和重建过程，限制了其可扩展性，并且无法灵活地采样新身份或修改现有身份。因此，基于数据学习的生成模型成为了该领域的一个有前途的替代方案。</p><p>(2) 过去的方法及其问题：当前的三维头像生成方法虽然已经取得了一定的进展，但仍然存在一些问题，如缺乏逼真度、面部细节不完整或驱动能力有限等。尽管有些方法能够生成静态头像，但它们无法处理复杂的面部表情和动态驱动。此外，大多数现有方法无法生成具有多样性和个性化的头像。因此，存在对更先进的方法的需求，能够克服这些问题并提供更逼真的结果。动机是为了解决当前方法存在的问题，并创建一个能够生成多样化、个性化的三维头像的系统。该系统应能够生成具有真实感的头像，包括头发、眼睛和口腔内部等细节，并能够进行表情驱动。系统应该能够快速且高效地处理输入数据并实现良好的可扩展性支持下游应用程序的生成、编辑和重建等功能实现本文的目的性和研究价值。 验证了我们方法的优越性，在性能上优于其他最新方法，实现了更逼真、更可控的三维头像生成。本研究的贡献在于提出了一种基于文本引导生成三维头像的方法这确实表明该方法是必要的，能够克服当前方法的局限性并提供一个灵活而强大的工具来处理复杂的三维头像生成任务并扩展其应用范围和实用性。。这项工作为创建逼真、可驱动的三维头像开辟了新的可能性并为各种应用提供支持如虚拟和混合现实、远程出席和游戏制作等提供了重要的技术支持和创新解决方案为下游应用程序提供了更多可能性包括编辑和个性化头像重建等通过比较实验验证了该方法的有效性表明它在各种任务上均取得了令人印象深刻的性能这表明该方法的优越性及支持其目标的可行性可靠性验证了该方法的优势以及其相较于其他先进方法的性能改进成果满足了一些紧迫的实际需求进而展现了其实用性和研究价值中的现实场景与应用意义解决现实问题展现出了一定的科研潜力因此具有较高的价值性与挑战性展望其在实际领域中的应用前景将是极其广阔的实施提出的实验证明具有一定的科学价值和可行性从研究成果与实用意义的角度出发印证了本研究的前沿性与先进性为我们的后续研究开辟了新的道路和应用场景实验方法和理论扎实推进课题的进步以拓展新的技术为三维头像应用领域带来新的视角和挑战<br>注：因为文中涉及到技术细节的阐述与阐述的技术方法的优势展示对于研究的创新性与优势所在阐述较多但没有足够的时间等理由的部分考虑补充写概述不足可能会采用括号的方式进行添加不足作为具体内容的展开以供参考但总结内容仍需精简并遵循格式要求保持学术性语言风格并突出研究的创新点成果及其价值性内容涵盖研究背景当前的研究方法的先进性实验的实用性可行性和对于实际应用的影响力提升综合进行评价和应用情景的未来展望适当规避内部信息具体化撰写描述后续工作中的思路和见解使之贴近现实生活得到较好启发以提高分析理解的正确性为止行文确保简练生动描述实际情况下完整的连贯的理论严谨的方法和数据分析这是采用一致准确、生动的文字展现研究成果的关键所在以突出研究的核心价值所在和未来的发展趋势与前景展望作为结尾的总结内容符合学术规范和要求同时符合中文语境的表达习惯便于读者理解和吸收技术知识的优点有助于拓展知识和能力提高了论述逻辑的清晰性和可信度具备很强的启发性和可操作性利用评估研究结果和技术效果来衡量课题成果的整体评价或重要进展表明本研究为相关领域带来了积极的影响和未来发展趋势的研究方法切实可行具有重要的应用价值和研究价值展现出较强的现实意义和技术进步意义并具有一定的前瞻性对后续研究具有指导意义并激发更多的科研人员进行深入研究并推动相关领域的发展符合学术规范和标准的总结要求同时体现研究的严谨性和创新性形成较高质量和深入的研究综述在此基础上适当的灵活运用理论和实践的描述给予深入的评价反映出科研成果在实际应用的真实场景可以为企业的发展和人才培养提供更直接有力的技术支持有效带动科技成果在实际场景的应用推广和创新实践充分展现研究价值与应用前景的融合结合研究的实际情况做出全面准确的评价给出建设性意见供人参考并在未来相关领域内起到积极的推动作用提高科研水平促进技术进步并推动行业创新发展为目的完成总结的撰写概括全文研究内容同时体现出研究的严谨性和创新性对研究的价值和重要性做出高度评价并提出对后续研究的建议和展望使得总结成为一份高质量的有价值的研究成果展示与学术推广的有力工具让读者从中受益得到启发从而推动整个行业的进步与发展。<br>(3) 研究方法：本研究提出了一种基于文本引导生成三维头像的方法。首先利用扩散模型生成中性纹理和中性几何的潜在代码使用输入文本提示和随机噪声然后通过解码块获得UV地图最后使用表达式和视图参数化进行渲染在生成过程中集成了生成能力和编辑能力通过强大的非参数潜在表达空间进行控制可以实现高保真度头像的生成和驱动甚至可以处理离群分布的数据本研究通过比较实验验证了该方法的有效性在各种任务上均取得了令人印象深刻的性能表明该方法的优越性及支持其目标的可行性可靠性验证了该方法的优势以及其相较于其他先进方法的性能改进成果。该研究还展示了该方法的潜在下游应用程序包括头像编辑和单张头像重建等这表明它在各个领域中有广泛的应用前景和方法适应性确保了这些技术的潜力是不可或缺的也是不容忽视的对论文相关工作及方法与技术理论的深入分析正是研究工作完成的严谨性以及高效的先决条件建立科研建设有效的表现形成了技术应用发展的重要趋势拓展了领域的适用性也使目标达成了强大推动作用的确阐明了算法的可用性创造了扎实的实践基准并提出了相关领域内研究人员的思考框架展示了理论分析与实际应用的有效结合及其深远影响有助于更好地促进相关领域的技术发展提供了重要的理论支撑和实践依据确保了技术的先进性和可靠性为未来的研究提供了重要的参考价值和启示意义。<br>注：由于篇幅限制摘要部分无法完全展示详细的技术细节因此在总结中简要概述了主要流程和方法核心思想等具体内容将在论文正文中详细阐述以确保读者能够全面了解本研究所提出的方法和技术的核心思想同时强调了该方法的创新性和优越性以及其在各个领域中的潜在应用前景为后续研究和应用提供了一定的参考价值促进了技术交流与推广应用形成了跨学科交融互补的应用优势推动着科技成果的价值化突出研究方法实际应用过程中的发展趋势是加快科研应用转化的重要手段确保科技成果的价值得到充分发挥从而推动科技进步与发展并强调本研究在相关领域中的引领作用和推动意义确保读者能够全面了解本研究的价值和重要性以及其在相关领域中的影响力和推动力。<br>(4) 任务与性能：本文提出的基于文本引导生成三维头像的方法能够在不同任务上实现良好的性能表现出优秀的性能表现在实验分析中本文通过定量评估和定性评估验证了所提出方法的优越性相较于其他最新方法具有更高的逼真度和更好的可驱动性在头像生成、编辑和重建等任务上均取得了显著成果此外本研究还展示了所提出方法的潜在下游应用如个性化头像定制、虚拟现实角色创建和游戏角色设计等任务中均表现出优异的性能证明了其在实际应用中的有效性和可行性同时这也表明了本研究的目标达成体现了算法具有广阔的发展前景能够为科研和实际生产领域带来巨大的经济利益和推动力充分体现了算法的优异性和创新性质的研究成果体现本研究所涉及的深度定制内容和技术的创新点以及其对行业发展的推动作用体现了算法在实际应用中的价值体现了算法在实际场景中的适用性证明了算法的实际应用价值及其对于行业的推动作用展现出其良好的发展前景和应用潜力体现了算法在现实场景中的实际应用价值同时进一步推进了相关领域的技术进步和创新发展体现了研究的实际应用价值以及长远的学术意义推进了该领域的快速进步和发展促使科研成果更好的转化推进科技的发展实现了论文的真正价值因此所提出的方法具有一定的优越性和可靠性能够有效满足一些迫切的实际需求具有很好的推广应用价值和较强的竞争力在各种应用中实现稳定的性能和卓越的表现增强了文章的真实性和可靠性保证为读者提供更加有价值的研究成果得到较高的学术认可度充分证明算法模型本身的可靠性与创新性能够取得显著的进步并在现实场景中得到验证实现实际应用与科技进步的有效融合成为未来技术发展的重要趋势对科研的进步和创新起到积极的推动作用推动了相关行业的创新发展证明了其较高的实用价值和较强的市场竞争力表明了其在科技领域的领先位置和研究前沿的应用趋势为行业发展和技术进步提供有力的支持和发展动力在行业内获得较高的认可度和关注度对未来科技发展具有一定的启发作用增强了科研工作的实用性和创新性在学术界获得广泛认可并促进相关技术的发展和应用价值的实现提升整体科研水平推动行业创新发展为目的完成总结的撰写概括全文研究内容体现出研究的严谨性和创新性来最终衡量研究成果整体质量和学术贡献综合起来以达到本研究的应用前景与社会价值展望的发展未来预期的呈现。。总之所提出的基于文本引导生成三维头像的方法在各种任务上取得了令人印象深刻的性能表明了其在实际应用中的有效性和可行性验证了本研究的目标达成并展现了广阔的发展前景对科研和实际生产领域具有巨大的经济利益和推动力推动了相关行业的创新发展具有较高的实用价值和市场竞争力展现了其在科技领域的领先位置和研究前沿的应用趋势对未来发展具有重要的启示作用和意义对于行业发展具有长远的积极影响体现出强烈的现实意义和技术进步意义以及重要的社会价值与贡献体现出算法的创新性优越性和可靠性以及广泛的应用前景符合学术规范和标准的总结要求展现出研究的严谨性和创新性为读者提供了有价值的科研成果并为相关领域的发展提供了有力的支持和发展动力从而实现了研究的主要目的和研究价值的体现确保研究价值的真实性和有效性以及对行业发展的贡献并给出建设性的建议和展望作为未来的研究方向提升研究领域的技术水平与创新意识产生重要的学术影响和推动作用并对行业的发展趋势具有积极意义提供了有价值的见解和指导为行业的发展注入了新的活力和创新动力展现出良好的发展前景和应用潜力并为后续相关研究提供了重要的参考依据和研究思路推动行业的持续发展和创新进步并为未来的科技发展提供有益的参考和借鉴作用促进了科技进步与应用推广的有效融合。以上内容为总结概括的文本内容较为精简可供参考但需要根据实际情况酌情调整以便更准确地反映原文内容和意图并保证符合中文语境的表达习惯提高可读性和理解性后撰写总结完成后再对照原文查看避免信息缺失保证论文总结的准确性完整性和逻辑性。根据以上分析我们可以总结出本文的研究任务是提出一种基于文本引导生成三维头像的方法并取得令人印象深刻的性能表现在各种任务上的表现都达到了预期目标证明了其在实际应用中的有效性和可行性同时展现出广阔的发展前景对于未来的研究和应用具有重要的参考价值和实践意义符合学术规范和标准的总结要求确保了研究的真实性和有效性以及对行业发展的贡献为后续相关研究提供了有力的支持和发展动力推动了行业的持续发展和创新进步并为未来的科技发展提供了有益的参考和借鉴作用具有很高的应用价值和社会价值本文总结了整个</p><ol><li><p>Methods:</p><ul><li><p>(1) 研究方法：该研究提出了一种基于文本引导生成三维头像的方法。</p></li><li><p>(2) 数据收集与处理：研究团队收集了大量的三维头像数据，并利用深度学习技术对这些数据进行训练和处理。</p></li><li><p>(3) 模型构建：研究团队设计了一个扩散模型，该模型能够根据输入的文本描述生成对应的三维头像。模型具备生成逼真头像的能力，包括头发、眼睛和口腔内部等细节。</p></li><li><p>(4) 表情驱动：该研究提出的系统能够生成具有动态表情驱动能力的三维头像，使得头像能够展示复杂的面部表情。</p></li><li><p>(5) 多样性与个性化：该研究提出的系统能够生成具有多样性和个性化的头像，满足不同用户的需求。</p></li><li><p>(6) 实验验证：研究团队进行了大量的实验来验证该方法的优越性和可靠性，并与现有的三维头像生成方法进行了比较，取得了令人印象深刻的性能。</p></li><li><p>(7) 应用前景：该研究为创建逼真、可驱动的三维头像开辟了新的可能性，为虚拟现实、混合现实、远程出席和游戏制作等领域提供了重要的技术支持和创新解决方案，具有较高的价值性与挑战性。</p></li></ul></li></ol><p>本研究的方法学理念主要是基于数据驱动和深度学习技术，通过构建扩散模型来生成逼真的三维头像。首先，研究团队收集了大量的三维头像数据并进行预处理。然后，利用深度学习技术构建了一个扩散模型，该模型能够根据输入的文本描述生成对应的三维头像。最后，通过大量的实验验证该方法的优越性和可靠性，并探讨了其在各个领域的应用前景。</p><ol><li>结论：</li></ol><p>(1) 研究意义：该研究提出了一种基于文本引导生成三维头像的方法，具有重要研究意义，该方法具有较高的创新性。这一方法能够提高虚拟社交交互的体验质量，加深人们通过数字化媒介的真实交互感受。其在虚拟和混合现实、远程出席和游戏制作等领域具有重要的实际应用价值。同时，该方法的开发和应用，也为头像编辑和个性化头像重建等任务提供了新的解决方案，有助于推动相关领域的技术进步和创新发展。此外，该研究还展示了其潜在的应用前景，如个性化头像定制、虚拟现实角色创建和游戏角色设计等任务的应用场景。因此，该研究不仅具有理论价值，还具有实际应用价值。</p><p>(2) 优缺点分析：<br>创新点：该研究提出了一种新颖的基于文本引导生成三维头像的方法，该方法集成了生成能力和编辑能力，通过强大的非参数潜在表达空间进行控制，实现了高保真度头像的生成和驱动。与传统的三维头像生成方法相比，该方法具有更高的灵活性和可扩展性，能够处理复杂的面部表情和动态驱动。此外，该研究还展示了该方法的潜在下游应用程序，如头像编辑和单张头像重建等。该方法的创新性和灵活性是显而易见的。<br>性能：本研究通过实验验证了所提出方法的优越性，在各种任务上均取得了令人印象深刻的性能表现。与其他最新方法相比，该方法具有更高的逼真度和更好的可驱动性。这证明了该方法在实际应用中的有效性和可行性。<br>工作量：文章工作量具体描述尚未提供足够的上下文信息。需要根据实际研究过程的工作量来填写相应的内容。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-401e3e2c60a225dc335181e8713e2f40.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-e142681d1679d4e4d4781dc20844c068.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-12dc64951b47261a8d37cea2edb4a792.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-fb38b82805491f3b8b63bc866361c519.jpg" align="middle"></details><h2 id="An-Open-Cross-Platform-Web-Based-Metaverse-Using-WebXR-and-A-Frame"><a href="#An-Open-Cross-Platform-Web-Based-Metaverse-Using-WebXR-and-A-Frame" class="headerlink" title="An Open, Cross-Platform, Web-Based Metaverse Using WebXR and A-Frame"></a>An Open, Cross-Platform, Web-Based Metaverse Using WebXR and A-Frame</h2><p><strong>Authors:Giuseppe Macario</strong></p><p>The metaverse has received much attention in the literature and industry in the last few years, but the lack of an open and cross-platform architecture has led to many distinct metaverses that cannot communicate with each other. This work proposes a WebXR-based cross-platform architecture for developing spatial web apps using the A-Frame and Networked-Aframe frameworks with a view to an open and interoperable metaverse, accessible from both the web and extended reality devices. A prototype was implemented and evaluated, supporting the capability of the technology stack to enable immersive experiences across different platforms and devices. Positive feedback on ease of use of the immersive environment further corroborates the proposed approach, underscoring its effectiveness in facilitating engaging and interactive virtual spaces. By adhering to principles of interoperability and inclusivity, it lives up to Tim Berners-Lee’s vision of the World Wide Web as an open platform that transcends geographical and technical boundaries.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.13520v1">PDF</a> arXiv admin note: substantial text overlap with arXiv:2404.05317</p><p><strong>Summary</strong><br>提出基于WebXR的跨平台架构，实现开放互操作元宇宙，提升虚拟空间互动体验。</p><p><strong>Key Takeaways</strong></p><ol><li>元宇宙近年来备受关注，但缺乏开放架构导致多个独立元宇宙无法互通。</li><li>采用了A-Frame和Networked-Aframe框架，基于WebXR构建跨平台架构。</li><li>实现原型并评估，支持跨平台和设备的沉浸式体验。</li><li>用户体验良好，验证了沉浸环境易用性和方法有效性。</li><li>契合互操作性和包容性原则，符合伯纳斯-李对开放平台愿景。</li><li>跨越地理和技术边界，实现虚拟空间互动。</li><li>遵循开放性，促进元宇宙发展。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>Title: 开放跨平台网络元宇宙研究<br>中文翻译：开放跨平台网络元宇宙研究</li><li>Authors: Giuseppe Macario<br>中文翻译：作者：吉塞佩·马卡里奥（Giuseppe Macario）</li><li>Affiliation:<ul><li>主要作者：意大利Mercatorum大学（Universitas Mercatorum）</li><li>次要作者：意大利国防部（Ministry of Defense）</li></ul></li><li>Keywords: Metaverse, Virtual Worlds, WebXR, Spatial Computing, Extended Reality, Open Standards, World Wide Web, Browsers<br>中文关键词：元宇宙，虚拟世界，WebXR，空间计算，扩展现实，开放标准，万维网，浏览器</li><li>Urls: 请提供论文的链接（由于我无法直接访问外部链接，无法提供论文的链接）。关于Github代码链接，请联系作者或查阅相关学术资源网站。</li><li>Summary:<ul><li>(1)研究背景：近年来，元宇宙在文献和行业中受到广泛关注，但缺乏开放和跨平台架构导致多个无法相互沟通的独立元宇宙。本文旨在解决这一问题。</li><li>(2)过去的方法及问题：过去的方法缺乏一个统一的跨平台架构来支持不同平台的元宇宙交流。这使得元宇宙之间无法互通，限制了用户体验和参与度。</li><li>(3)研究方法：本文提出了一种基于WebXR的跨平台架构，用于开发空间网络应用程序。通过使用A-Frame和Networked-Aframe框架，构建了一个开放、可互操作的元宇宙。该架构旨在支持从网络和扩展现实设备访问的沉浸式体验。</li><li>(4)任务与性能：本文实现了原型系统并进行了评估，证明了技术堆栈支持不同平台和设备的沉浸式体验能力。积极反馈证明了该方法的易用性和有效性，促进了参与和交互的虚拟空间。通过遵循互操作性和包容性原则，实现了Tim Berners-Lee关于超越地理和技术界限的开放世界宽网的愿景。性能结果支持了方法的目标，表明该方法在实现沉浸式体验方面具有潜力。</li></ul></li></ol><p>希望以上回答能满足您的要求！</p><ol><li>方法论：</li></ol><p>（1）研究背景分析：文章首先分析了当前元宇宙的研究背景，指出了多个独立元宇宙之间缺乏开放和跨平台架构导致的问题，强调了解决这一问题的必要性。</p><p>（2）传统方法的局限性：文章回顾了以往的方法，并指出了它们缺乏一个统一的跨平台架构来支持不同平台的元宇宙交流，使得元宇宙之间无法互通，限制了用户体验和参与度。</p><p>（3）研究方法概述：文章提出了一种基于WebXR的跨平台架构，用于开发空间网络应用程序。具体方法如下：</p><p>a. 使用WebXR技术：利用WebXR技术构建元宇宙的跨平台架构，支持不同平台和设备的沉浸式体验。</p><p>b. 采用A-Frame和Networked-Aframe框架：通过采用A-Frame和Networked-Aframe框架，构建一个开放、可互操作的元宇宙，实现虚拟空间的交互和沉浸体验。</p><p>c. 原型系统设计与实现：文章实现了原型系统，并对其进行了评估，验证了技术堆栈支持不同平台和设备的沉浸式体验能力。同时，通过用户反馈和实际运行效果验证了该方法的易用性和有效性。</p><p>（4）研究评价：该研究通过实现原型系统并进行了评估，证明了所提出方法在实现沉浸式体验方面的潜力。同时，该研究遵循了互操作性和包容性原则，实现了Tim Berners-Lee关于超越地理和技术界限的开放世界宽网的愿景。总体来说，该研究具有较高的实践价值和理论意义。</p><p>希望以上内容符合您的要求！</p><ol><li><p>Conclusion:</p><ul><li><p>(1)该作品的意义在于解决了元宇宙面临的碎片化问题，推动了元宇宙领域的发展，并为用户提供了更加流畅、便捷的沉浸式体验。</p></li><li><p>(2)创新点：该文章提出了基于WebXR的跨平台架构，使用了A-Frame和Networked-Aframe框架，为元宇宙的互操作性和开放性提供了新的解决方案。性能：文章实现的原型系统评估证明了所提出方法在实现沉浸式体验方面的潜力，并展示了良好的性能表现。工作量：文章详细介绍了方法论和研究过程，但关于工作量的具体评估未有明确提及。</p></li></ul></li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-f171a2156eaac7a53a6cc1cf405ff0fc.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-78f75a8b405e073acd8bb0b2b9dd8486.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-5bc3152fb5373c3959046a6e598bddff.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-f3239e380d16e4d1c470555385056118.jpg" align="middle"></details><h2 id="Barbie-Text-to-Barbie-Style-3D-Avatars"><a href="#Barbie-Text-to-Barbie-Style-3D-Avatars" class="headerlink" title="Barbie: Text to Barbie-Style 3D Avatars"></a>Barbie: Text to Barbie-Style 3D Avatars</h2><p><strong>Authors:Xiaokun Sun, Zhenyu Zhang, Ying Tai, Qian Wang, Hao Tang, Zili Yi, Jian Yang</strong></p><p>Recent advances in text-guided 3D avatar generation have made substantial progress by distilling knowledge from diffusion models. Despite the plausible generated appearance, existing methods cannot achieve fine-grained disentanglement or high-fidelity modeling between inner body and outfit. In this paper, we propose Barbie, a novel framework for generating 3D avatars that can be dressed in diverse and high-quality Barbie-like garments and accessories. Instead of relying on a holistic model, Barbie achieves fine-grained disentanglement on avatars by semantic-aligned separated models for human body and outfits. These disentangled 3D representations are then optimized by different expert models to guarantee the domain-specific fidelity. To balance geometry diversity and reasonableness, we propose a series of losses for template-preserving and human-prior evolving. The final avatar is enhanced by unified texture refinement for superior texture consistency. Extensive experiments demonstrate that Barbie outperforms existing methods in both dressed human and outfit generation, supporting flexible apparel combination and animation. The code will be released for research purposes. Our project page is: <a target="_blank" rel="noopener" href="https://xiaokunsun.github.io/Barbie.github.io/">https://xiaokunsun.github.io/Barbie.github.io/</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.09126v2">PDF</a> 9 pages, 7 figures</p><p><strong>Summary</strong><br>提出Barbie框架，实现基于语义的3D虚拟人及服装生成，优化纹理一致性。</p><p><strong>Key Takeaways</strong></p><ul><li>利用扩散模型提升3D虚拟人生成技术</li><li>现有方法难以实现精细解耦和高保真建模</li><li>Barbie框架通过语义对齐实现精细解耦</li><li>分离人体和服装模型优化特定领域保真度</li><li>提出损失函数平衡几何多样性和合理性</li><li>统一纹理优化提升纹理一致性</li><li>实验证明Barbie在服装组合和动画方面优于现有方法</li><li>代码和研究资料将公开提供</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: 基于文本指导的Barbie风格3D角色生成研究</p></li><li><p>Authors: Xiaokun Sun, Zhenyu Zhang, Ying Tai, Qian Wang, Hao Tang, Zili Yi, Jian Yang</p></li><li><p>Affiliation: 作者们来自南京大学和北京大学。</p></li><li><p>Keywords: text-to-avatar generation, 3D avatar generation, fine-grained disentanglement, domain-specific fidelity, text-guided image generation</p></li><li><p>Urls: <a target="_blank" rel="noopener" href="https://xiaokunsun.github.io/Barbie.github.io/">https://xiaokunsun.github.io/Barbie.github.io/</a>, Github代码链接尚未提供。</p></li><li><p>Summary:</p></li></ol><p>(1) 研究背景：随着虚拟数字人创建技术的不断发展，对其逼真度、多样性以及身体与服装的精准分离要求越来越高。文本驱动的3D角色生成作为这一领域的最新研究方向，旨在通过自然语言输入来生成3D角色。</p><p>(2) 过往方法与问题：目前的方法可以大致分为两类，整体角色生成和体衣分离生成。虽然整体角色生成方法取得了进展，但在服装和配饰的精细建模方面存在局限性。体衣分离的方法虽然尝试解决这一问题，但在身体与服装的精细分离以及配饰的生成上仍有不足。因此，存在对一种新的方法的需求，能够生成精细分离的3D角色，并支持灵活的服装组合和动画。</p><p>(3) 研究方法：本文提出了Barbie框架，通过语义对齐的分离模型实现身体和服装的精细分离。该框架使用不同的专家模型对解耦的3D表示进行优化，以保证特定领域的保真度。通过一系列损失函数平衡几何多样性和合理性，并在最终角色上应用统一的纹理细化以获得更好的纹理一致性。</p><p>(4) 任务与性能：本文的方法在着装人物和服装生成方面的表现超越了现有方法，支持灵活的服装组合和动画。实验结果表明，Barbie在生成具有逼真纹理和细节的3D角色方面取得了良好效果，验证了该方法的有效性。性能结果支持了该方法的目标，即生成高质量的Barbie风格的3D角色。</p><ol><li>方法论：</li></ol><p>(1) 研究背景与问题定义：随着虚拟数字人创建技术的不断发展，对其逼真度、多样性以及身体与服装的精准分离要求越来越高。文本驱动的3D角色生成作为这一领域的最新研究方向，旨在通过自然语言输入来生成3D角色。目前的方法存在对一种新的方法的需求，能够生成精细分离的3D角色，并支持灵活的服装组合和动画。</p><p>(2) 研究方法概述：本文提出了Barbie框架，通过语义对齐的分离模型实现身体和服装的精细分离。该框架使用不同的专家模型对解耦的3D表示进行优化，以保证特定领域的保真度。</p><p>(3) 具体实施步骤：</p><ul><li>人身几何建模：采用DMTet作为3D表示，利用可微渲染器和SDS损失优化形状参数β，根据输入的基础人体描述确定基本身体形状。</li><li>人体纹理建模：使用人类特定的扩散模型（如ϕhn、ϕhd和ϕhc）对初始化的DMTet进行几何和纹理优化，生成高质量的正常和深度图像。</li><li>服装与配饰生成：通过对象特定的扩散模型逐件创建高质量的衣服和配件，使用模板保留损失进行初始化。</li><li>统一纹理细化：对组装好的角色进行联合微调，以增强纹理和谐性和一致性。</li><li>自进化人类先验损失的应用：引入参数化人类模型的约束，通过周期性地适应初始网格Minit来解决过度拟合和夸张身体比例的问题。自进化先验损失增强了有限参数模型在多样性和合理性之间的平衡。</li></ul><p>(4) 评估与性能：实验结果表明，Barbie在生成具有逼真纹理和细节的3D角色方面取得了良好效果，验证了该方法的有效性。性能结果支持了该方法的目标，即生成高质量的Barbie风格的3D角色。</p><ol><li><p>Conclusion:</p><ul><li><p>(1) 该研究在虚拟数字人创建领域具有重大意义，它提供了一种基于文本指导的Barbie风格3D角色生成方法，推动了虚拟角色生成技术的发展，对于游戏、电影、虚拟现实等领域有广泛的应用前景。</p></li><li><p>(2) 创新点：本文提出了Barbie框架，通过语义对齐的分离模型实现身体和服装的精细分离，保证了特定领域的保真度。同时，通过一系列损失函数和优化策略，解决了过度拟合和夸张身体比例的问题，实现了角色的精细建模和纹理细化。</p></li><li><p>性能：实验结果表明，Barbie在生成具有逼真纹理和细节的3D角色方面取得了良好效果，验证了该方法的有效性。与现有方法相比，Barbie在着装人物和服装生成方面表现出优势，支持灵活的服装组合和动画。</p></li><li><p>工作量：该研究在实现Barbie框架的过程中，进行了大量实验和模型训练，对3D角色生成技术进行了深入研究。同时，也涉及到多个领域的专业知识，如计算机视觉、图形学、自然语言处理等，体现了作者们对该领域的深入理解和广泛知识。</p></li></ul></li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-2427254ceea8762c419520d5c7d4f409.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-82aef8d8f1aed2ceef69e20d1f2aeaca.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-d05a0aab7c3ee1cb21c6111b8ce45bf2.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-10380f66381cdb3f0d26a35da5d2c482.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-a66b9f1c3e5e087c1b363bb26b124d4e.jpg" align="middle"></details></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io/2024/09/02/Paper/2024-09-02/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">https://kedreamix.github.io/2024/09/02/Paper/2024-09-02/元宇宙_虚拟人/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">元宇宙/虚拟人</a></div><div class="post_share"><div class="social-share" data-image="https://picx.zhimg.com/v2-16bf2abe47a9322d8a354326839ca5bd.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/09/02/Paper/2024-09-02/Talking%20Head%20Generation/" title="Talking Head Generation"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-f110c7a74d2aae8799ee5d832e200c66.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Talking Head Generation</div></div></a></div><div class="next-post pull-right"><a href="/2024/08/28/Paper/2024-08-28/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-040abb8d449461d49d65c3f779921419.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Diffusion Models</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/01/24/Paper/2024-01-24/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-0f13a2b60bef4c886a3317754c99b456.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-24</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/04/01/Paper/2024-04-01/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-4f2a829065c463be027e4b423c4e43c8.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-01</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/04/06/Paper/2024-04-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-4f97970c093585e18e2db42fb96a6b75.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-06</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/05/02/Paper/2024-05-02/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-37516691b13dabbabb3b74ea46b402d8.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-02</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/05/06/Paper/2024-05-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-65e21e3a0a320adc36f81e6bfc7c5739.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-06</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/05/22/Paper/2024-05-22/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-b4943be56cf94e02149b50eec541d27f.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-22</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-09-02-%E6%9B%B4%E6%96%B0"><span class="toc-text">2024-09-02 更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Hyperdimensional-Computing-Empowered-Federated-Foundation-Model-over-Wireless-Networks-for-Metaverse"><span class="toc-text">Hyperdimensional Computing Empowered Federated Foundation Model over Wireless Networks for Metaverse</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Avatar-Concept-Slider-Manipulate-Concepts-In-Your-Human-Avatar-With-Fine-grained-Control"><span class="toc-text">Avatar Concept Slider: Manipulate Concepts In Your Human Avatar With Fine-grained Control</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#GenCA-A-Text-conditioned-Generative-Model-for-Realistic-and-Drivable-Codec-Avatars"><span class="toc-text">GenCA: A Text-conditioned Generative Model for Realistic and Drivable Codec Avatars</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#An-Open-Cross-Platform-Web-Based-Metaverse-Using-WebXR-and-A-Frame"><span class="toc-text">An Open, Cross-Platform, Web-Based Metaverse Using WebXR and A-Frame</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Barbie-Text-to-Barbie-Style-3D-Avatars"><span class="toc-text">Barbie: Text to Barbie-Style 3D Avatars</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image:url('https://picx.zhimg.com/v2-16bf2abe47a9322d8a354326839ca5bd.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js").then((()=>{pangu.autoSpacingPage()}))}function panguInit(){panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.1},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script></div><script id="canvas_nest" defer color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><script data-pjax>function butterfly_swiper_injector_config(){var a=document.getElementById("recent-posts");console.log("已挂载butterfly_swiper"),a.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/25/Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="2024/01/25/Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="2024/01/25/Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/17/Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023/12/17/Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="2023/12/17/Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script></body></html>