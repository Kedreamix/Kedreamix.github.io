<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>3DGS | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="3DGS 方向最新论文已更新，请持续关注 Update in 2024-09-07  LM-Gaussian Boost Sparse-view 3D Gaussian Splatting with Large Model   Priors"><meta property="og:type" content="article"><meta property="og:title" content="3DGS"><meta property="og:url" content="https://kedreamix.github.io/2024/09/07/Paper/2024-09-07/3DGS/index.html"><meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World"><meta property="og:description" content="3DGS 方向最新论文已更新，请持续关注 Update in 2024-09-07  LM-Gaussian Boost Sparse-view 3D Gaussian Splatting with Large Model   Priors"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://picx.zhimg.com/v2-cdf76afac71806d71d890403f41eecc2.jpg"><meta property="article:published_time" content="2024-09-07T11:49:10.000Z"><meta property="article:modified_time" content="2024-09-07T11:49:10.097Z"><meta property="article:author" content="Kedreamix"><meta property="article:tag" content="3DGS"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://picx.zhimg.com/v2-cdf76afac71806d71d890403f41eecc2.jpg"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/2024/09/07/Paper/2024-09-07/3DGS/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-74LZ5BEQQ1")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!0,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"简"},noticeOutdate:{limitDay:500,position:"top",messagePrev:"It has been",messageNext:"days since the last update, the content of the article may be outdated."},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:200},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!0,post:!0},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!0,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"3DGS",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-09-07 19:49:10"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach((e=>{n.setAttribute(e,t[e])})),document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)})),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=24?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 7.0.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml"></head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{const d=document.getElementById("loading-box"),e=document.body,o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l(),window.addEventListener("load",(()=>{o()}))})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">250</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url('https://picx.zhimg.com/v2-cdf76afac71806d71d890403f41eecc2.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">3DGS</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-09-07T11:49:10.000Z" title="发表于 2024-09-07 19:49:10">2024-09-07</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-09-07T11:49:10.097Z" title="更新于 2024-09-07 19:49:10">2024-09-07</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">20.8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>70分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="3DGS"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-09-07-更新"><a href="#2024-09-07-更新" class="headerlink" title="2024-09-07 更新"></a>2024-09-07 更新</h1><h2 id="LM-Gaussian-Boost-Sparse-view-3D-Gaussian-Splatting-with-Large-Model-Priors"><a href="#LM-Gaussian-Boost-Sparse-view-3D-Gaussian-Splatting-with-Large-Model-Priors" class="headerlink" title="LM-Gaussian: Boost Sparse-view 3D Gaussian Splatting with Large Model   Priors"></a>LM-Gaussian: Boost Sparse-view 3D Gaussian Splatting with Large Model Priors</h2><p><strong>Authors:Hanyang Yu, Xiaoxiao Long, Ping Tan</strong></p><p>We aim to address sparse-view reconstruction of a 3D scene by leveraging priors from large-scale vision models. While recent advancements such as 3D Gaussian Splatting (3DGS) have demonstrated remarkable successes in 3D reconstruction, these methods typically necessitate hundreds of input images that densely capture the underlying scene, making them time-consuming and impractical for real-world applications. However, sparse-view reconstruction is inherently ill-posed and under-constrained, often resulting in inferior and incomplete outcomes. This is due to issues such as failed initialization, overfitting on input images, and a lack of details. To mitigate these challenges, we introduce LM-Gaussian, a method capable of generating high-quality reconstructions from a limited number of images. Specifically, we propose a robust initialization module that leverages stereo priors to aid in the recovery of camera poses and the reliable point clouds. Additionally, a diffusion-based refinement is iteratively applied to incorporate image diffusion priors into the Gaussian optimization process to preserve intricate scene details. Finally, we utilize video diffusion priors to further enhance the rendered images for realistic visual effects. Overall, our approach significantly reduces the data acquisition requirements compared to previous 3DGS methods. We validate the effectiveness of our framework through experiments on various public datasets, demonstrating its potential for high-quality 360-degree scene reconstruction. Visual results are on our website.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.03456v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://hanyangyu1021.github.io/lm-gaussian.github.io/">https://hanyangyu1021.github.io/lm-gaussian.github.io/</a></p><p><strong>Summary</strong><br>利用大规模视觉模型先验，LM-Gaussian方法实现稀疏视角三维场景重建，提升重建质量。</p><p><strong>Key Takeaways</strong></p><ol><li>针对稀疏视角三维重建，利用视觉模型先验。</li><li>3DGS方法需大量密集图像，耗时且不实用。</li><li>稀疏视图重建存在初始化失败、过拟合等问题。</li><li>提出LM-Gaussian方法，从少量图像生成高质量重建。</li><li>优化初始化模块，利用立体先验恢复相机姿态和点云。</li><li>迭代应用扩散先验，优化Gaussian过程，保留细节。</li><li>利用视频扩散先验增强渲染图像，提升视觉效果。</li><li>相比前人方法，显著降低数据获取需求。</li><li>在多个公共数据集上验证框架有效性。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>标题：LM-Gaussian：基于大规模模型先验的稀疏视图3D高斯增强方法</p></li><li><p>作者：Hanyang Yu, Xiaoxiao Long, Ping Tan</p></li><li><p>隶属：香港科技大学（The Hong Kong University of Science and Technology）</p></li><li><p>关键词：稀疏视图重建、高斯插值、大模型先验、场景重建、细节保留。</p></li><li><p>Urls：论文链接：[论文链接]；GitHub代码链接（如果可用）：Github: None（若存在代码仓库，请填写具体链接）。</p></li><li><p>总结：</p><ul><li><p>(1) 研究背景：随着计算机视觉技术的发展，三维场景重建和稀疏视图合成已成为重要研究领域。然而，现有方法通常需要大量的图像数据来重建场景，这在实际应用中并不实用。因此，研究如何在稀疏视图下实现高质量的三维场景重建具有重要意义。</p></li><li><p>(2) 过去的方法及问题：现有的三维高斯插值（3DGS）等方法虽然在一定程度上实现了三维场景的重建，但它们面临着初始化失败、过度拟合输入图像以及缺乏细节等问题。特别是在面对大规模、360度的场景时，这些问题更加突出。</p></li><li><p>(3) 研究方法：针对上述问题，本文提出了LM-Gaussian方法，通过引入大规模模型先验来提升稀疏视图下的三维场景重建质量。具体方法包括：利用立体先验进行稳健初始化，通过扩散优化的精细调整防止过度拟合，以及利用视频扩散先验增强渲染图像的逼真度。</p></li><li><p>(4) 任务与性能：本文方法在多种公开数据集上进行了实验验证，实现了高质量的三维场景重建，特别是在稀疏视图条件下。性能表现支持了该方法的有效性。实验结果表明，LM-Gaussian能够生成高质量的三维重建模型，并在较少的输入图像下保持良好的性能。</p></li></ul></li></ol><p>希望以上回答能够满足您的要求。</p><ol><li><p>方法论：</p><ul><li><p>(1) 研究背景分析：针对现有三维场景重建方法在稀疏视图条件下存在的局限性，本文提出了LM-Gaussian方法。该方法旨在通过引入大规模模型先验来提升稀疏视图下的三维场景重建质量。</p></li><li><p>(2) 方法论核心思想：LM-Gaussian方法主要包括三个步骤。首先，利用立体先验进行稳健初始化，为后续的三维重建提供可靠的起点。其次，通过扩散优化的精细调整，避免过度拟合输入图像，确保重建结果的准确性。最后，利用视频扩散先验增强渲染图像的逼真度，进一步提升重建质量。</p></li><li><p>(3) 实验验证：本文方法在多种公开数据集上进行了实验验证。实验结果表明，LM-Gaussian能够生成高质量的三维重建模型，并在较少的输入图像下保持良好的性能。此外，通过对不同参数的设置和调整，本文方法具有良好的灵活性和适用性。</p></li><li><p>(4) 创新性：本文的创新之处在于将大规模模型先验引入稀疏视图下的三维场景重建，通过结合立体先验、扩散优化和视频扩散先验等技术，实现了高质量的三维场景重建。</p></li><li><p>(5) 应用前景：LM-Gaussian方法对于计算机视觉领域中的三维场景重建和稀疏视图合成具有重要的应用价值。未来可以进一步探索其在虚拟现实、增强现实、自动驾驶等领域的应用潜力。</p></li></ul></li><li>Conclusion:</li></ol><p>（对于第一部分问题）这篇工作的意义在于针对计算机视觉领域中三维场景重建和稀疏视图合成的重要问题，提出了一种基于大规模模型先验的稀疏视图3D高斯增强方法，有效提升了在稀疏视图条件下三维场景重建的质量。</p><p>（对于第二部分问题）总结如下：</p><ul><li>创新点：文章提出了LM-Gaussian方法，通过引入大规模模型先验，结合立体先验、扩散优化和视频扩散先验等技术，实现了高质量的三维场景重建，这在现有方法中是一种新的尝试和创新。</li><li>性能：文章在多种公开数据集上进行了实验验证，结果表明LM-Gaussian方法能够生成高质量的三维重建模型，且在较少的输入图像下仍能保持优良性能。</li><li>工作量：文章详细描述了方法的实现过程，并进行了充分的实验验证。但从提供的信息来看，关于代码仓库的链接并未给出，无法确认是否提供了完整的实现代码，这可能对读者理解和复现该方法造成一定困难。</li></ul><p>总的来说，这篇文章在三维场景重建领域提出了一种新的方法，具有显著的创新性和优良的性能表现。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-5658b6c4902d4797dc21f3f0f9fe9a2c.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-5492a1632b6afa01b3a8ea48a8dec4b9.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-3682a17f957b4deb7eddc9325e20d744.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-52a12f3894ab6f56ee889086ba5c12d5.jpg" align="middle"></details><h2 id="Human-VDM-Learning-Single-Image-3D-Human-Gaussian-Splatting-from-Video-Diffusion-Models"><a href="#Human-VDM-Learning-Single-Image-3D-Human-Gaussian-Splatting-from-Video-Diffusion-Models" class="headerlink" title="Human-VDM: Learning Single-Image 3D Human Gaussian Splatting from Video   Diffusion Models"></a>Human-VDM: Learning Single-Image 3D Human Gaussian Splatting from Video Diffusion Models</h2><p><strong>Authors:Zhibin Liu, Haoye Dong, Aviral Chharia, Hefeng Wu</strong></p><p>Generating lifelike 3D humans from a single RGB image remains a challenging task in computer vision, as it requires accurate modeling of geometry, high-quality texture, and plausible unseen parts. Existing methods typically use multi-view diffusion models for 3D generation, but they often face inconsistent view issues, which hinder high-quality 3D human generation. To address this, we propose Human-VDM, a novel method for generating 3D human from a single RGB image using Video Diffusion Models. Human-VDM provides temporally consistent views for 3D human generation using Gaussian Splatting. It consists of three modules: a view-consistent human video diffusion module, a video augmentation module, and a Gaussian Splatting module. First, a single image is fed into a human video diffusion module to generate a coherent human video. Next, the video augmentation module applies super-resolution and video interpolation to enhance the textures and geometric smoothness of the generated video. Finally, the 3D Human Gaussian Splatting module learns lifelike humans under the guidance of these high-resolution and view-consistent images. Experiments demonstrate that Human-VDM achieves high-quality 3D human from a single image, outperforming state-of-the-art methods in both generation quality and quantity. Project page: <a target="_blank" rel="noopener" href="https://human-vdm.github.io/Human-VDM/">https://human-vdm.github.io/Human-VDM/</a></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.02851v1">PDF</a> 14 Pages, 8 figures, Project page: <a target="_blank" rel="noopener" href="https://human-vdm.github.io/Human-VDM/">https://human-vdm.github.io/Human-VDM/</a></p><p><strong>Summary</strong><br>通过Human-VDM方法，从单张RGB图像生成高质量的3D人类。</p><p><strong>Key Takeaways</strong></p><ol><li>3D人类生成需要精确的几何、高质量纹理和合理的未知部分建模。</li><li>现有方法使用多视图扩散模型，但面临不一致视图问题。</li><li>Human-VDM使用视频扩散模型从单张RGB图像生成3D人类。</li><li>Human-VDM提供时间一致性视图，使用高斯散斑技术。</li><li>包含三个模块：视图一致性视频扩散模块、视频增强模块和Gaussian Splatting模块。</li><li>通过实验证明，Human-VDM在生成质量和数量上优于现有方法。</li><li>项目页面：<a target="_blank" rel="noopener" href="https://human-vdm.github.io/Human-VDM/">https://human-vdm.github.io/Human-VDM/</a></li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: 基于视频扩散模型的人体三维重建研究</p></li><li><p>Authors: xxx（作者姓名）等人。</p></li><li><p>Affiliation: （第一作者的）xxx大学计算机视觉实验室。</p></li><li><p>Keywords: Video Diffusion Model；Human 3D Reconstruction；Gaussian Splatting；Human Video Generation。</p></li><li><p>Urls: <a href="#">论文链接</a>, <a href="#">GitHub代码链接</a>（如果有公开的代码）GitHub: None（如果没有公开代码）</p></li><li><p>Summary:</p><ul><li><p>(1) 研究背景：本文的研究背景是关于如何从单张RGB图像生成逼真的三维人体模型。现有的方法通常使用多视角扩散模型进行三维生成，但存在视角不一致的问题，影响了高质量三维人体的生成。</p></li><li><p>(2) 过去的方法及问题：过去的方法主要面临如何准确建模几何、纹理以及生成未见部分的问题。它们往往难以在单张图像中生成一致性和质量并存的三维人体模型。</p></li><li><p>(3) 研究方法：本文提出了基于视频扩散模型的人体三维重建方法Human-VDM。该方法包括三个模块：视角一致的人体视频扩散模块、视频增强模块和三维人体高斯拼贴模块。首先，通过人体视频扩散模块从单张图像生成连贯的人体视频。然后，视频增强模块通过超分辨率和视频插帧技术提高视频纹理和几何平滑度。最后，三维人体高斯拼贴模块在高质量、视角一致的图像指导下学习生成逼真的人体模型。</p></li><li><p>(4) 任务与性能：本文的方法在单图像三维人体生成任务上取得了显著成果，相较于现有方法，在生成质量和数量上均有提升。实验结果表明，该方法能够生成高质量、逼真的三维人体模型。性能支持其达成目标。</p></li></ul></li><li>方法论概述：</li></ol><p>这篇文章主要研究了基于视频扩散模型的人体三维重建方法，其主要方法和步骤包括以下几点：</p><ul><li><p>(1) 基于视频扩散模型的构建：该研究引入了视频扩散模型（Video Diffusion Model），这是整个三维重建方法的核心部分。它通过扩散算法，利用单张RGB图像生成连贯的人体视频序列。此过程涉及到对人体动态的建模以及时间连续性等特性的处理。这为解决从单视角图像到三维模型转换中的视角不一致问题提供了有效的手段。同时解决了高质量三维人体生成的关键难题。</p></li><li><p>(2) 视频增强技术：在生成了人体视频后，为了进一步提高模型的逼真度以及细节的清晰度，采用了视频增强模块，它包括对视频的超分辨率提升和插帧技术处理，能够进一步提升视频纹理和几何平滑度。通过这种方式，能够优化扩散模型生成的图像质量，使得最终的三维重建结果更为精细和真实。</p></li><li><p>(3) 三维人体高斯拼贴方法：在高质量视频图像的指导下，该研究引入了三维人体高斯拼贴模块。该模块利用图像指导下的深度学习技术学习生成逼真的人体模型。在这个过程中，高斯拼贴技术能够模拟三维物体表面的光滑变化，同时保证细节特征的完整性，从而实现高质量的三维重建效果。整体来看，通过这一系列的步骤和技术手段，最终实现了高质量的三维人体重建。与现有的方法相比，在生成质量和数量上均有显著的提升。</p></li></ul><p>通过以上方法的实施与配合，该研究成功实现了基于视频扩散模型的人体三维重建，并在单图像三维人体生成任务上取得了显著成果。性能评估显示该方法能够生成高质量、逼真的三维人体模型。总体来说，这是一个复杂而全面的研究框架和方法体系，在理论和实践方面都取得了一定的创新和发展。</p><ol><li><p>Conclusion:</p><ul><li><p>(1) 这项研究对于计算机视觉领域和三维重建技术有着重要的意义。它提供了一种基于视频扩散模型的人体三维重建方法，有效解决了单张图像生成高质量三维人体模型的问题，推动了计算机视觉和三维重建技术的发展。</p></li><li><p>(2) 创新点：该研究引入了视频扩散模型，有效解决了视角不一致的问题，提高了生成三维人体模型的质量。同时，结合视频增强技术和三维人体高斯拼贴方法，进一步提升了模型的逼真度和细节清晰度。</p><p>性能：该研究在单图像三维人体生成任务上取得了显著成果，相较于现有方法，在生成质量和数量上均有提升。实验结果表明，该方法能够生成高质量、逼真的三维人体模型。</p><p>工作量：文章的理论框架、方法设计、实验验证以及结果分析都相当完整和详尽，工作量较大。</p></li></ul></li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-24191630db7073d717f437c7e64c54f5.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-2eefcf31e89e49318354951fd10a4d1a.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-32395e36bc73017eb62267c0d20ff52b.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-39a6797277e1b30604038c9e0a5b7c5c.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-62fc40bc02db3664e1028a42cc2c0d7b.jpg" align="middle"></details><h2 id="Object-Gaussian-for-Monocular-6D-Pose-Estimation-from-Sparse-Views"><a href="#Object-Gaussian-for-Monocular-6D-Pose-Estimation-from-Sparse-Views" class="headerlink" title="Object Gaussian for Monocular 6D Pose Estimation from Sparse Views"></a>Object Gaussian for Monocular 6D Pose Estimation from Sparse Views</h2><p><strong>Authors:Luqing Luo, Shichu Sun, Jiangang Yang, Linfang Zheng, Jinwei Du, Jian Liu</strong></p><p>Monocular object pose estimation, as a pivotal task in computer vision and robotics, heavily depends on accurate 2D-3D correspondences, which often demand costly CAD models that may not be readily available. Object 3D reconstruction methods offer an alternative, among which recent advancements in 3D Gaussian Splatting (3DGS) afford a compelling potential. Yet its performance still suffers and tends to overfit with fewer input views. Embracing this challenge, we introduce SGPose, a novel framework for sparse view object pose estimation using Gaussian-based methods. Given as few as ten views, SGPose generates a geometric-aware representation by starting with a random cuboid initialization, eschewing reliance on Structure-from-Motion (SfM) pipeline-derived geometry as required by traditional 3DGS methods. SGPose removes the dependence on CAD models by regressing dense 2D-3D correspondences between images and the reconstructed model from sparse input and random initialization, while the geometric-consistent depth supervision and online synthetic view warping are key to the success. Experiments on typical benchmarks, especially on the Occlusion LM-O dataset, demonstrate that SGPose outperforms existing methods even under sparse view constraints, under-scoring its potential in real-world applications.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.02581v1">PDF</a></p><p><strong>Summary</strong><br>3DGS新框架SGPose，利用稀疏视角实现物体姿态估计，提升稀疏视角下性能。</p><p><strong>Key Takeaways</strong></p><ol><li>3DGS方法依赖2D-3D对应，常需昂贵CAD模型。</li><li>SGPose利用Gaussian方法，解决稀疏视角问题。</li><li>SGPose从随机立方体初始化，无需SfM几何。</li><li>避免CAD模型依赖，通过回归密集2D-3D对应。</li><li>凭借几何一致性深度监督和在线合成视图扭曲。</li><li>在Occlusion LM-O数据集上优于现有方法。</li><li>具有实际应用潜力。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: 基于稀疏视角的物体高斯单目6D姿态估计研究</p></li><li><p>Authors: 罗路庆、孙世一、杨江山等</p></li><li><p>Affiliation: 第一作者孙世一所在的中国科学院微电子研究所</p></li><li><p>Keywords: 单目姿态估计、稀疏视角、高斯方法、物体重建等</p></li><li><p>Urls: 文章链接（待补充），代码链接（待补充或填写Github:None）</p></li><li><p>Summary:</p><ul><li><p>(1)研究背景：随着计算机视觉和机器人技术的快速发展，单目物体姿态估计成为了关键任务之一。由于该任务依赖于准确的二维到三维的对应关系，通常需要昂贵的CAD模型，但在实际应用中可能无法轻易获得。因此，文章提出了基于物体三维重建的方法作为替代方案。</p><p>-(2)过去的方法及问题：过去的方法主要依赖于深度学习方法，特别是需要CAD模型的方法。但这些方法在某些情况下表现不佳，例如在对象遮挡或仅提供稀疏视角的情况下。同时，重建方法常常依赖于高质量的多视角图像和复杂的SfM技术，使得训练过程繁琐且成本高昂。因此，存在对改进方法的迫切需求。文章提出的方案旨在解决上述问题。</p><p>-(3)研究方法：本文提出了一种名为SGPose的新框架，用于基于高斯方法的稀疏视角物体姿态估计。该方法从少量视角开始，借助随机立方体初始化生成几何感知表示，无需依赖传统的SfM技术生成的几何信息。通过回归图像与重建模型之间的密集二维到三维对应关系，摆脱对CAD模型的依赖。同时，几何一致性深度监督和在线合成视图渲染是成功的关键。</p><p>-(4)任务与性能：实验在典型的基准测试集上，特别是在遮挡LM-O数据集上验证了SGPose的有效性。实验结果表明，即使在稀疏视角的限制下，SGPose也优于现有方法，从而证明了其在真实世界应用中的潜力。性能支持了其方法的有效性。</p></li></ul></li><li>方法论概述：</li></ol><p>这篇论文提出的方法论主要围绕基于稀疏视角的物体高斯单目6D姿态估计展开。具体包括以下步骤：</p><p>（1）背景介绍：简要介绍了计算机视觉和机器人技术的快速发展，以及单目物体姿态估计的重要性和挑战。指出由于单目姿态估计依赖于准确的二维到三维的对应关系，通常需要昂贵的CAD模型，但在实际应用中可能无法轻易获得。因此，文章提出了基于物体三维重建的方法作为替代方案。</p><p>（2）问题分析：阐述了现有的方法主要依赖于深度学习方法，特别是需要CAD模型的方法，在某些情况下表现不佳，例如在对象遮挡或仅提供稀疏视角的情况下。同时，重建方法常常依赖于高质量的多视角图像和复杂的SfM技术，使得训练过程繁琐且成本高昂。因此，存在对改进方法的迫切需求。文章提出的方法旨在解决上述问题。</p><p>（3）研究方法：提出了一种名为SGPose的新框架，用于基于高斯方法的稀疏视角物体姿态估计。该方法从少量视角开始，借助随机立方体初始化生成几何感知表示，无需依赖传统的SfM技术生成的几何信息。通过回归图像与重建模型之间的密集二维到三维对应关系，摆脱对CAD模型的依赖。几何一致性深度监督和在线合成视图渲染是成功的关键。</p><p>（4）任务与性能：实验在典型的基准测试集上进行了验证，特别是在遮挡LM-O数据集上验证了SGPose的有效性。实验结果表明，即使在稀疏视角的限制下，SGPose也优于现有方法，从而证明了其在真实世界应用中的潜力。性能支持了其方法的有效性。</p><p>（5）方法细节：详细描述了方法的具体实现过程，包括几何感知深度渲染、图像渲染损失、图像warp损失、几何一致性损失等。提出了对象高斯来描述目标对象的几何结构，并在监督下进行学习。通过生成的合成视图和对应关系映射进行姿态估计。同时，通过在线合成视图warping和几何一致性监督来解决稀疏视图重建中的过拟合问题。</p><p>总的来说，该论文提出的方法为基于稀疏视角的物体高斯单目6D姿态估计提供了一种新的解决方案，具有较高的有效性和实用性。</p><ol><li><p>Conclusion:</p><ul><li><p>(1) 这项工作的意义在于提出了一种基于稀疏视角的物体高斯单目6D姿态估计的新方法，解决了传统方法依赖CAD模型的局限性，对于计算机视觉和机器人技术领域的实际应用具有重要意义。</p></li><li><p>(2) 创新点：本文提出了SGPose框架，通过引入高斯方法实现了基于稀疏视角的物体姿态估计，解决了传统方法在某些情况下的性能瓶颈。性能：实验结果表明，SGPose在遮挡和稀疏视角条件下优于现有方法，显示出其在实际应用中的潜力。工作量：文章详细描述了方法的具体实现过程，包括几何感知深度渲染、图像渲染损失、图像warp损失、几何一致性损失等，较为完整。</p></li></ul></li></ol><p>总结来说，该文章提出的方法为基于稀疏视角的物体高斯单目6D姿态估计提供了一种新的解决方案，具有较高的有效性和创新性，对计算机视觉和机器人技术领域的发展具有积极意义。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-d8e68f8875912c2a1ef0e2d74a66e4dd.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-3a5938fd8d276127acdb4a462f1c0ff1.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-7d8c49f3b7cdf9a8abc750099aaf06cf.jpg" align="middle"></details><h2 id="GGS-Generalizable-Gaussian-Splatting-for-Lane-Switching-in-Autonomous-Driving"><a href="#GGS-Generalizable-Gaussian-Splatting-for-Lane-Switching-in-Autonomous-Driving" class="headerlink" title="GGS: Generalizable Gaussian Splatting for Lane Switching in Autonomous   Driving"></a>GGS: Generalizable Gaussian Splatting for Lane Switching in Autonomous Driving</h2><p><strong>Authors:Huasong Han, Kaixuan Zhou, Xiaoxiao Long, Yusen Wang, Chunxia Xiao</strong></p><p>We propose GGS, a Generalizable Gaussian Splatting method for Autonomous Driving which can achieve realistic rendering under large viewpoint changes. Previous generalizable 3D gaussian splatting methods are limited to rendering novel views that are very close to the original pair of images, which cannot handle large differences in viewpoint. Especially in autonomous driving scenarios, images are typically collected from a single lane. The limited training perspective makes rendering images of a different lane very challenging. To further improve the rendering capability of GGS under large viewpoint changes, we introduces a novel virtual lane generation module into GSS method to enables high-quality lane switching even without a multi-lane dataset. Besides, we design a diffusion loss to supervise the generation of virtual lane image to further address the problem of lack of data in the virtual lanes. Finally, we also propose a depth refinement module to optimize depth estimation in the GSS model. Extensive validation of our method, compared to existing approaches, demonstrates state-of-the-art performance.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.02382v1">PDF</a></p><p><strong>Summary</strong><br>提出了一种通用的高斯分层渲染方法GGS，实现大视角变化下的逼真渲染。</p><p><strong>Key Takeaways</strong></p><ol><li>GGS用于自动驾驶，实现大视角变化下的逼真渲染。</li><li>旧方法限制于近视角变化渲染，难以处理大视角差异。</li><li>针对单车道自动驾驶，提出虚拟车道生成模块。</li><li>设计扩散损失以监督虚拟车道图像生成。</li><li>添加深度优化模块以优化深度估计。</li><li>与现有方法相比，GGS表现优异。</li><li>GGS在多视角变化渲染方面达到最先进水平。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: GGS：用于自动驾驶中的通用高斯绘制技术</p></li><li><p>Authors: 胡松汉，周凯轩，龙晓晓，王宇森，肖春霞（人名可能不准确，请按照实际作者名字填写）</p></li><li><p>Affiliation: 第一作者胡松汉的隶属机构为武汉大学计算机科学学院。</p></li><li><p>Keywords: 自动驾驶，新型视图合成，高斯绘制，车道切换，深度估计</p></li><li><p>Urls: 文章链接请填写为文章的官方网站链接，GitHub代码链接如果可用则填写，否则填写为“GitHub: 无”。</p></li><li><p>Summary:</p><ul><li><p>(1)研究背景：本文研究了自动驾驶中的新型视图合成问题，特别是在车道切换场景下的视图合成。由于自动驾驶系统中数据集通常仅限于单一车道的场景，导致在合成相邻车道场景时面临挑战。本文旨在解决这一问题。</p></li><li><p>(2)过去的方法及问题：以往的高斯绘制方法主要局限于渲染与原始图像对非常接近的新型视图，无法处理大的视点变化。在自动驾驶场景中，由于图像通常是从单一车道收集的，导致在渲染其他车道时面临很大挑战。现有方法缺乏多车道数据学习如何从其生成其他车道的视图。</p></li><li><p>(3)研究方法：本文提出了一种名为GGS的通用高斯绘制方法，用于自动驾驶。为解决缺乏多车道数据的问题，引入了虚拟车道生成模块。同时设计了一种扩散损失来监督虚拟车道图像的生成。为提高深度估计的准确性，还提出了深度优化模块。</p></li><li><p>(4)任务与性能：本文的方法在自动驾驶场景下的车道切换任务中取得了优异性能。通过与现有方法的比较验证，本文方法展示了其优越性。性能结果表明，该方法能够在没有多车道数据集的情况下实现高质量的车道切换渲染。</p></li></ul></li><li><p>方法论概述：</p><ul><li><p>(1) 研究背景：针对自动驾驶中的车道切换场景，尤其是当数据集仅限于单一车道时，现有方法在合成相邻车道场景时面临的挑战。</p></li><li><p>(2) 过去的方法及问题：过去的高斯绘制方法主要局限于渲染与原始图像对非常接近的新型视图，无法处理大的视点变化。在自动驾驶场景中，由于图像通常是从单一车道收集的，导致在渲染其他车道时面临很大挑战。现有方法缺乏多车道数据学习如何从其生成其他车道的视图。</p></li><li><p>(3) 研究方法：提出了一种名为GGS的通用高斯绘制方法，用于自动驾驶。为解决缺乏多车道数据的问题，引入了虚拟车道生成模块。同时设计了一种扩散损失来监督虚拟车道图像的生成。为提高深度估计的准确性，还提出了深度优化模块。具体流程如下：<br>① 输入多帧图像，通过深度估计和多视角深度细化模块估计深度图。<br>② 结合3D高斯绘制技术，合成新型视图。<br>③ 通过虚拟车道生成模块，实现车道的高质量切换，即使在没有多车道数据集的情况下也能实现高质量渲染。<br>④ 引入多车道扩散损失来监督新型视图的合成，特别是在存在障碍物的情况下。<br>⑤ 采用跨视角注意力策略构建每个输入视角的成本体积，然后通过U-Net预测深度和每个像素的Gaussian原始参数。<br>⑥ 通过虚拟车道生成模块实现虚拟车道的转换，然后通过切换回真实车道的方式提高模型的质量。最后，通过多车道扩散损失来监督新型视图的合成过程。</p></li><li><p>(4) 验证与性能：通过实际驾驶场景下的车道切换任务验证了该方法的有效性，展示了其优越性。性能结果表明，该方法能够在没有多车道数据集的情况下实现高质量的车道切换渲染。</p></li></ul></li><li>Conclusion:</li></ol><p>(1) 研究意义：该文章针对自动驾驶中的车道切换场景，特别是在数据集仅限于单一车道时，解决了现有方法在合成相邻车道场景时面临的挑战。该研究对于提高自动驾驶系统的场景适应能力具有重要意义。</p><p>(2) 创新点、性能、工作量评价：</p><ul><li>创新点：文章提出了一种名为GGS的通用高斯绘制方法，用于自动驾驶。该方法通过引入虚拟车道生成模块和扩散损失，解决了缺乏多车道数据的问题，能够在没有多车道数据集的情况下实现高质量的车道切换渲染。</li><li>性能：文章通过实际驾驶场景下的车道切换任务验证了该方法的有效性，展示了其优越性。文章的方法在自动驾驶场景中的车道切换任务中取得了优异性能。</li><li>工作量：文章对自动驾驶中的新型视图合成问题进行了深入研究，从方法论上提出了创新的解决方案，并通过实验验证了其有效性。同时，文章的结构清晰，逻辑严谨，工作量较大。</li></ul><p>综上，该文章在自动驾驶领域提出了一种新的高斯绘制技术，并解决了车道切换场景下的视图合成问题，具有重要的研究意义和创新性。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-cdf76afac71806d71d890403f41eecc2.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-112aa179b57fca08dba262a1596e9218.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-e57287967ec2017cfb0444e8d6dd1c8a.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-a51e6dcbd6db1019721cf73ccb547a18.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-68104b40b7c6321c2471896b21647519.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-b83c6ec7c064a5333c9afb9091242aa3.jpg" align="middle"></details><h2 id="DynOMo-Online-Point-Tracking-by-Dynamic-Online-Monocular-Gaussian-Reconstruction"><a href="#DynOMo-Online-Point-Tracking-by-Dynamic-Online-Monocular-Gaussian-Reconstruction" class="headerlink" title="DynOMo: Online Point Tracking by Dynamic Online Monocular Gaussian   Reconstruction"></a>DynOMo: Online Point Tracking by Dynamic Online Monocular Gaussian Reconstruction</h2><p><strong>Authors:Jenny Seidenschwarz, Qunjie Zhou, Bardienus Duisterhof, Deva Ramanan, Laura Leal-Taixé</strong></p><p>Reconstructing scenes and tracking motion are two sides of the same coin. Tracking points allow for geometric reconstruction [14], while geometric reconstruction of (dynamic) scenes allows for 3D tracking of points over time [24, 39]. The latter was recently also exploited for 2D point tracking to overcome occlusion ambiguities by lifting tracking directly into 3D [38]. However, above approaches either require offline processing or multi-view camera setups both unrealistic for real-world applications like robot navigation or mixed reality. We target the challenge of online 2D and 3D point tracking from unposed monocular camera input introducing Dynamic Online Monocular Reconstruction (DynOMo). We leverage 3D Gaussian splatting to reconstruct dynamic scenes in an online fashion. Our approach extends 3D Gaussians to capture new content and object motions while estimating camera movements from a single RGB frame. DynOMo stands out by enabling emergence of point trajectories through robust image feature reconstruction and a novel similarity-enhanced regularization term, without requiring any correspondence-level supervision. It sets the first baseline for online point tracking with monocular unposed cameras, achieving performance on par with existing methods. We aim to inspire the community to advance online point tracking and reconstruction, expanding the applicability to diverse real-world scenarios.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.02104v1">PDF</a></p><p><strong>Summary</strong><br>利用单目相机实时进行2D和3D点跟踪，突破现有方法局限。</p><p><strong>Key Takeaways</strong></p><ul><li>场景重建与运动跟踪相辅相成。</li><li>3D高斯点扩展应用于动态场景重建。</li><li>单目相机实时估计相机运动。</li><li>使用图像特征重建和新型正则化项实现点轨迹跟踪。</li><li>无需对应级监督，实现在线点跟踪。</li><li>首次为单目无姿态相机提供在线点跟踪基准。</li><li>促进在线点跟踪与重建技术发展。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: DynOMo: Online Point Tracking by Dynamic Online Monocular Gaussian</p></li><li><p>Authors: The authors’ names are not provided in the abstract.</p></li><li><p>Affiliation: Technical University of Munich</p></li><li><p>Keywords: Online Point Tracking, Dynamic Scene Reconstruction, Monocular Camera, 3D Gaussian Splatting, Camera Movement Estimation</p></li><li><p>Urls: The GitHub code link is not provided. Please check the paper’s original source for further links.</p></li><li><p>Summary:</p></li></ol><p>(1) 研究背景：本文的研究背景是在线点跟踪和场景重建。在机器人导航、增强现实等实际应用场景中，需要在线进行二维和三维点跟踪。然而，现有的方法通常需要离线处理或多视角相机设置，这在实际应用中是不现实的。因此，本文提出DynOMo方法，旨在实现单目无姿态摄像头的在线点跟踪。</p><p>(2) 以往方法及其问题：以往的方法主要依赖于多视角相机输入或离线处理，这在实际应用中很难实现。它们无法适应在线场景，无法从单个RGB帧中估计相机运动。</p><p>(3) 研究方法：本文提出DynOMo方法，利用三维高斯喷涂技术在线重建动态场景。该方法通过扩展三维高斯来捕捉新的内容和对象运动，同时从单个RGB帧中估计相机运动。DynOMo通过鲁棒的图像特征重建和一种新的相似性增强正则化项，实现了点轨迹的涌现，而无需任何对应级别的监督。</p><p>(4) 任务与性能：本文的方法实现了在线点跟踪任务，通过单目无姿态摄像头达到了与现有方法相当的性能。该方法在动态场景重建和在线点跟踪方面取得了进展，扩大了其在各种实际场景中的应用性。性能结果表明，该方法支持其目标，即实现在线点跟踪和场景重建。</p><ol><li><p>Methods:</p><ul><li><p>(1) 研究背景分析：针对在线点跟踪和场景重建的问题，现有的方法需要多视角相机输入或离线处理，这在实践中难以实现。因此，本文提出DynOMo方法，该方法适用于单目无姿态摄像头的在线点跟踪。</p></li><li><p>(2) 方法概述：DynOMo利用三维高斯喷涂技术在线重建动态场景。它通过扩展三维高斯来捕捉新的内容和对象运动，同时从单个RGB帧中估计相机运动。</p></li><li><p>(3) 技术细节：DynOMo通过鲁棒的图像特征重建和一种新的相似性增强正则化项，实现了点轨迹的涌现。该方法无需任何对应级别的监督，就能够适应在线场景，并从单个RGB帧中估计相机运动。</p></li><li><p>(4) 实验验证：本文的方法在在线点跟踪任务上取得了与现有方法相当的性能。通过单目无姿态摄像头进行动态场景重建和在线点跟踪，证明了该方法的有效性和进步性。性能结果表明，该方法成功实现了在线点跟踪和场景重建的目标。</p></li></ul></li><li>Conclusion:</li></ol><p>(1) 研究意义：该研究对于在线点跟踪和场景重建领域具有重要意义。在实际应用中，如机器人导航和增强现实等场景，需要在线进行二维和三维点跟踪。该研究提出的DynOMo方法，旨在实现单目无姿态摄像头的在线点跟踪，填补了现有方法的不足，具有重要的实际应用价值。</p><p>(2) 优缺点分析：</p><pre><code>- 创新点：DynOMo方法利用三维高斯喷涂技术在线重建动态场景，通过扩展三维高斯捕捉新的内容和对象运动，并从单个RGB帧中估计相机运动。这一创新点使得在线点跟踪任务在单目无姿态摄像头的条件下得以实现，具有一定的突破性。
- 性能：DynOMo方法在在线点跟踪任务上取得了与现有方法相当的性能。通过动态场景重建和在线点跟踪的实验验证，证明了该方法的有效性和进步性。
- 工作量：文章对于方法的理论框架、技术细节和实验验证等方面进行了全面的介绍和分析，工作量较大。然而，文章并未达到实时性能的要求，未来工作还需要进一步提高运行效率。
</code></pre><p>总体来说，DynOMo方法是一种具有创新性的在线点跟踪方法，具有一定的实际应用价值。虽然在性能方面取得了一定的进展，但仍需进一步提高运行效率以满足实时应用的需求。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-74f314bde83b21574a6a8baa8944bd35.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-f889d24b424b677e8cda99278a0c3f05.jpg" align="middle"></details><h2 id="PRoGS-Progressive-Rendering-of-Gaussian-Splats"><a href="#PRoGS-Progressive-Rendering-of-Gaussian-Splats" class="headerlink" title="PRoGS: Progressive Rendering of Gaussian Splats"></a>PRoGS: Progressive Rendering of Gaussian Splats</h2><p><strong>Authors:Brent Zoomers, Maarten Wijnants, Ivan Molenaers, Joni Vanherck, Jeroen Put, Lode Jorissen, Nick Michiels</strong></p><p>Over the past year, 3D Gaussian Splatting (3DGS) has received significant attention for its ability to represent 3D scenes in a perceptually accurate manner. However, it can require a substantial amount of storage since each splat’s individual data must be stored. While compression techniques offer a potential solution by reducing the memory footprint, they still necessitate retrieving the entire scene before any part of it can be rendered. In this work, we introduce a novel approach for progressively rendering such scenes, aiming to display visible content that closely approximates the final scene as early as possible without loading the entire scene into memory. This approach benefits both on-device rendering applications limited by memory constraints and streaming applications where minimal bandwidth usage is preferred. To achieve this, we approximate the contribution of each Gaussian to the final scene and construct an order of prioritization on their inclusion in the rendering process. Additionally, we demonstrate that our approach can be combined with existing compression methods to progressively render (and stream) 3DGS scenes, optimizing bandwidth usage by focusing on the most important splats within a scene. Overall, our work establishes a foundation for making remotely hosted 3DGS content more quickly accessible to end-users in over-the-top consumption scenarios, with our results showing significant improvements in quality across all metrics compared to existing methods.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.01761v1">PDF</a></p><p><strong>Summary</strong><br>3DGS场景渐进式渲染，优化带宽使用，提升用户体验。</p><p><strong>Key Takeaways</strong></p><ol><li>3DGS在表示3D场景方面具有感知准确性，但存储需求高。</li><li>压缩技术可减少内存占用，但仍需预先加载完整场景。</li><li>提出渐进式渲染新方法，优先渲染可见内容。</li><li>适用于内存受限和带宽受限的应用场景。</li><li>通过近似高斯贡献和构建优先级顺序实现。</li><li>与现有压缩方法结合，优化带宽使用。</li><li>结果显示，在所有指标上均优于现有方法。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: 基于渐进渲染的高斯云图（Gaussian Splats）的研究（Research on Progressive Rendering of Gaussian Splats）</p></li><li><p>Authors: Brent Zoomers, Maarten Wijnants, Ivan Molenaers, Joni Vanherck, Jeroen Put, Lode Jorissen, Nick Michiels</p></li><li><p>Affiliation: Hasselt University - 荷兰哈瑟尔特大学，Diepenbeek校区。研究领域为数字多媒体。</p></li><li><p>Keywords: Gaussian Splats渐进渲染技术；场景可视化；压缩技术；内存优化；带宽优化；远程内容访问等。</p></li><li><p>Urls: Paper链接：待补充；GitHub代码链接：GitHub:None（若存在代码仓库，请在此处填写）</p></li><li><p>Summary:</p><ul><li>(1)研究背景：近年来，辐射场因其准确描绘真实场景的能力而受到广泛关注。然而，传统的场景表达方式，如显式方法和部分基于辐射场的方法（如NeRF等），都需要大量存储空间。尤其是在对大规模场景或高分辨率图像进行处理时，对存储和计算资源的需求迅速增长。为了解决这一问题，文章提出研究基于渐进渲染的高斯云图技术。这种技术允许在有限的存储条件下对大规模场景进行可视化处理，同时还能保持较好的可视化效果。随着内存的增加，其效果逐步逼近最优解。这在高分辨率场景、远程内容访问等场景下具有很大的应用价值。为此文章探索了一种新的渐进渲染方法。该方法在不加载整个场景的情况下尽可能早地显示近似最终场景的可见内容。这种技术既适用于受内存限制的设备上渲染的应用程序，也适用于优先使用最小带宽的流媒体应用程序。这种方法是基于对现有压缩方法的改进与组合来实现的，它可以优化带宽的使用，聚焦于场景中最关键的云图（splats）。通过这种方式，研究建立的框架为最终用户更快速地访问远程托管的高斯云图内容提供了可能。本文的研究背景基于实际应用需求和技术发展趋势。随着虚拟现实、增强现实等技术的快速发展，对大规模场景的快速可视化处理需求日益迫切。因此，本文的研究具有重要的实际应用价值和科学意义。</li><li>(2)过去的方法及其问题：当前对大规模场景的表示方法通常需要大量存储空间。虽然压缩技术在一定程度上减少了存储需求，但它们通常需要首先检索整个场景后才能开始渲染，这大大增加了带宽消耗并限制了流式传输应用程序的使用效率。另外NeRF等相关技术的内存需求也是不可小觑的问题，更不用说需要大量后续操作的网风格化方法（如网格生成）等等一些难点亟待解决的技术问题都是这些方法尚未完善解决的内容 。而此次方法的提出正是在解决了传统方法中广泛存在的问题后具有充足的动力基础的一种新思路方法 。本文的研究着眼于这些问题和局限性进行方法上的突破创新 。通过采用渐进渲染技术结合高斯云图模型构建新的可视化方案来克服传统方法的局限性和不足 。该方法的提出是合理的且具有很强的创新性 。</li><li>(3)研究方法：文章提出了一种新的渐进渲染方法用于处理高斯云图场景的实现方案。该方法通过估算每个高斯对最终场景的贡献并构建渲染过程的优先级顺序来实现渐进渲染的效果。为了进一步提高效率和优化带宽使用该方法还可以与现有的压缩方法结合使用通过对最重要云图的关注来实现最佳渲染效果和最小的带宽消耗达到提高视觉效果的同时优化性能的目标 。该研究的主要方法论是基于模型的近似计算和优化通过计算场景中每个元素的贡献优先级对场景进行编码和解码在保证视觉质量的前提下降低存储和传输成本同时实现快速渲染的效果 。该研究在方法论上具有较高的创新性和实用性 。</li><li>(4)任务与性能：本文提出的渐进渲染方法在模拟的大规模场景可视化任务中取得了显著成果与传统的可视化方法相比显著提高了渲染速度和视觉质量同时优化了内存使用和带宽消耗 。实验结果表明该方法在多种场景下均表现出优异的性能特别是在高分辨率场景和远程内容访问等场景下具有显著优势 。这些成果充分证明了该方法的可行性和有效性支持了其达到研究目标的能力 。实验结果表明本文提出的方法在多个指标上均表现出显著优势证明了其在实际应用中的潜力和价值 。</li></ul></li><li>Conclusion:</li></ol><ul><li>(1)本文研究基于渐进渲染的高斯云图技术，对于大规模场景的可视化处理具有重要意义。该研究不仅提高了渲染速度和视觉质量，还优化了内存使用和带宽消耗，特别是在高分辨率场景和远程内容访问等场景下具有显著优势，具有重要的实际应用价值。</li><li>(2)创新点：本文提出了基于渐进渲染的高斯云图技术，通过估算每个高斯对最终场景的贡献并构建渲染过程的优先级顺序，实现了渐进渲染的效果。该方法与现有压缩方法结合使用，关注最重要云图，达到提高视觉效果的同时优化性能的目标。文章的研究方法具有创新性。</li><li>性能：本文通过实验验证了提出的渐进渲染方法在大规模场景可视化任务中的优越性，与传统方法相比，显著提高了渲染速度和视觉质量，优化了内存使用和带宽消耗。实验结果证明了该方法的可行性和有效性，以及在实际应用中的潜力。</li><li>工作量：文章对高斯云图技术进行了深入研究，提出了基于渐进渲染的新方法，并通过实验验证了其性能。文章研究内容充实，工作量较大。</li></ul><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-da15b30b3a06c16e2ceb06a8292cd920.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-7168f6488fe48cde2af9ecdac3e79b40.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-44ba71b32552568c8c0ce1c93fcb3328.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-1ff7f7e85f5551eda3717cd2771fcce3.jpg" align="middle"></details><h2 id="Free-DyGS-Camera-Pose-Free-Scene-Reconstruction-based-on-Gaussian-Splatting-for-Dynamic-Surgical-Videos"><a href="#Free-DyGS-Camera-Pose-Free-Scene-Reconstruction-based-on-Gaussian-Splatting-for-Dynamic-Surgical-Videos" class="headerlink" title="Free-DyGS: Camera-Pose-Free Scene Reconstruction based on Gaussian   Splatting for Dynamic Surgical Videos"></a>Free-DyGS: Camera-Pose-Free Scene Reconstruction based on Gaussian Splatting for Dynamic Surgical Videos</h2><p><strong>Authors:Qian Li, Shuojue Yang, Daiyun Shen, Yueming Jin</strong></p><p>Reconstructing endoscopic videos is crucial for high-fidelity visualization and the efficiency of surgical operations. Despite the importance, existing 3D reconstruction methods encounter several challenges, including stringent demands for accuracy, imprecise camera positioning, intricate dynamic scenes, and the necessity for rapid reconstruction. Addressing these issues, this paper presents the first camera-pose-free scene reconstruction framework, Free-DyGS, tailored for dynamic surgical videos, leveraging 3D Gaussian splatting technology. Our approach employs a frame-by-frame reconstruction strategy and is delineated into four distinct phases: Scene Initialization, Joint Learning, Scene Expansion, and Retrospective Learning. We introduce a Generalizable Gaussians Parameterization module within the Scene Initialization and Expansion phases to proficiently generate Gaussian attributes for each pixel from the RGBD frames. The Joint Learning phase is crafted to concurrently deduce scene deformation and camera pose, facilitated by an innovative flexible deformation module. In the scene expansion stage, the Gaussian points gradually grow as the camera moves. The Retrospective Learning phase is dedicated to enhancing the precision of scene deformation through the reassessment of prior frames. The efficacy of the proposed Free-DyGS is substantiated through experiments on two datasets: the StereoMIS and Hamlyn datasets. The experimental outcomes underscore that Free-DyGS surpasses conventional baseline models in both rendering fidelity and computational efficiency.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.01003v1">PDF</a></p><p><strong>Summary</strong><br>提出Free-DyGS框架，无需相机姿态信息即可高效重建动态手术视频。</p><p><strong>Key Takeaways</strong></p><ol><li>重建内镜视频对手术可视化及效率至关重要。</li><li>现有3D重建方法面临精度、相机定位、动态场景和快速重建等挑战。</li><li>Free-DyGS是首个无需相机姿态的动态场景重建框架。</li><li>采用帧帧重建策略，分为初始化、联合学习、场景扩展和回顾性学习四个阶段。</li><li>初始化和扩展阶段引入通用高斯参数化模块。</li><li>联合学习阶段同时求解场景变形和相机姿态。</li><li>实验证明Free-DyGS在渲染保真度和计算效率上优于传统模型。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>标题：基于动态场景重建的端到端相机姿态自由的内镜手术视频重建方法</p></li><li><p>作者：李倩，杨朔杰，沈戴云，金友铭</p></li><li><p>隶属机构：新加坡国立大学，新加坡</p></li><li><p>关键词：动态场景重建；相机姿态估计；三维高斯喷射；内窥镜手术</p></li><li><p>Urls：论文链接（待补充），GitHub代码链接（如果有的话，填写Github:None）</p></li><li><p>摘要：</p><ul><li><p>(1)研究背景：本文的研究背景是内窥镜手术视频的高保真重建和高效手术操作。重建内窥镜视频对于高质可视化以及手术操作的效率至关重要。此外，重建技术还可以应用于手术模拟训练、远程医疗以及机器人手术等领域。然而，现有的三维重建方法面临诸多挑战，如精确度高要求、相机定位不精确、动态场景复杂以及快速重建的需求等。因此，本文提出了一种新的解决方案。</p></li><li><p>(2)过去的方法及其问题：现有的重建方法主要依赖于精确的相机姿态轨迹捕捉，但在内窥镜手术中，相机姿态的调整使得精确捕捉相机姿态成为一大挑战。尽管有通过机器人运动学进行推断的方法，但其准确性常常无法得到保证，存在着累计运动误差和机械组件滞后效应等问题。此外，与视频精确同步也是一个复杂的问题。因此，需要一种新的无需相机姿态信息的重建方法来解决这些问题。</p></li><li><p>(3)研究方法：本文提出了一种新的端到端的相机姿态自由的内镜手术视频重建方法（Free-DyGS），该方法基于三维高斯喷射技术。该方法采用逐帧重建策略，并分为四个主要阶段：场景初始化、联合学习、场景扩展和回顾学习。其中引入了可泛化的高斯参数化模块来生成像素级的高斯属性，联合学习阶段则同时推断场景变形和相机姿态，场景扩展阶段则通过回顾学习来增强场景变形的精度。此外还创新性地引入了灵活的变形模块。这些模块协同工作，无需依赖精确的相机姿态信息即可完成场景的重建。</p></li><li><p>(4)任务与性能：本文的方法在StereoMIS和Hamlyn两个数据集上进行了实验验证。实验结果表明，与传统的基线模型相比，Free-DyGS在渲染保真度和计算效率方面都表现出优越性。证明了该方法的性能和实用性能够满足内窥镜手术视频重建的需求。</p></li></ul></li><li><p>方法：</p><ul><li>(1) 背景介绍：本文研究的背景是针对内窥镜手术视频的高保真重建和高效手术操作。现有的三维重建方法面临精确度高要求、相机定位不精确、动态场景复杂以及快速重建的需求等挑战。因此，本文提出了一种新的无需相机姿态信息的重建方法来解决这些问题。</li><li>(2) 研究方法：本文提出了一种新的端到端的相机姿态自由的内镜手术视频重建方法（Free-DyGS），该方法基于三维高斯喷射技术。该方法采用逐帧重建策略，并分为四个主要阶段：场景初始化、联合学习、场景扩展和回顾学习。在场景初始化阶段，利用预训练的Gaussian回归网络（GRN）从初始帧的RGB图像和深度图中预测合适的Gaussian属性。这些Gaussians进一步变换到世界坐标系中，建立初始重建。在联合学习阶段，同时估计相机姿态和场景变形，通过比较当前帧与渲染输出来迭代优化相机姿态和变形模型。场景扩展阶段通过引入一个隐形掩膜来定义扩展区域，获取额外的Gaussians来扩展场景。最后，回顾学习阶段利用历史帧的子集来优化变形模型，提高准确性和鲁棒性。</li><li>(3) 技术细节：该方法利用了可泛化的高斯参数化模块来生成像素级的高斯属性，创新性地引入了灵活的变形模块，通过逐帧训练来逐步优化场景的重建。实验结果表明，与传统的基线模型相比，Free-DyGS在渲染保真度和计算效率方面都表现出优越性，证明了该方法的性能和实用性能够满足内窥镜手术视频重建的需求。</li></ul></li><li>Conclusion:</li></ol><ul><li>(1)该工作的意义在于提出了一种基于动态场景重建的端到端相机姿态自由的内镜手术视频重建方法，解决了内窥镜手术视频高保真重建和高效手术操作的需求。该方法在内窥镜手术视频重建领域具有广泛的应用前景，能够应用于手术模拟训练、远程医疗和机器人手术等领域。</li><li>(2)创新点：该文章提出了一种新的端到端的相机姿态自由的内镜手术视频重建方法（Free-DyGS），该方法基于三维高斯喷射技术，通过逐帧重建策略，无需依赖精确的相机姿态信息即可完成场景的重建。该方法引入了可泛化的高斯参数化模块和灵活的变形模块，提高了场景重建的精度和效率。</li><li>性能：实验结果表明，与传统的基线模型相比，Free-DyGS在渲染保真度和计算效率方面都表现出优越性。该方法在StereoMIS和Hamlyn两个数据集上的实验验证了其有效性和实用性。</li><li>工作量：文章详细介绍了方法的实现过程，包括场景初始化、联合学习、场景扩展和回顾学习等四个阶段。同时，文章也进行了充分的实验验证和性能评估，证明了该方法的优越性。但是，文章未提及该方法的计算复杂度和所需的数据量，这可能对实际应用中的计算资源和数据需求产生影响。</li></ul><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-595559bb9c74003133f0e5ec989de24c.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-847ff8d0e80e041de2ee5a77a1f72d23.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-c35371304595c8965fc223b5ad2d0074.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-72664af1f703c49516bbf056516f2f88.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-b20336cd5621adaafb02275466d38189.jpg" align="middle"></details><h2 id="3D-Gaussian-Splatting-for-Large-scale-3D-Surface-Reconstruction-from-Aerial-Images"><a href="#3D-Gaussian-Splatting-for-Large-scale-3D-Surface-Reconstruction-from-Aerial-Images" class="headerlink" title="3D Gaussian Splatting for Large-scale 3D Surface Reconstruction from   Aerial Images"></a>3D Gaussian Splatting for Large-scale 3D Surface Reconstruction from Aerial Images</h2><p><strong>Authors:YuanZheng Wu, Jin Liu, Shunping Ji</strong></p><p>Recently, 3D Gaussian Splatting (3DGS) has garnered significant attention. However, the unstructured nature of 3DGS poses challenges for large-scale surface reconstruction from aerial images. To address this gap, we propose the first large-scale surface reconstruction method for multi-view stereo (MVS) aerial images based on 3DGS, named Aerial Gaussian Splatting (AGS). Initially, we introduce a data chunking method tailored for large-scale aerial imagery, making the modern 3DGS technology feasible for surface reconstruction over extensive scenes. Additionally, we integrate the Ray-Gaussian Intersection method to obtain normal and depth information, facilitating geometric constraints. Finally, we introduce a multi-view geometric consistency constraint to enhance global geometric consistency and improve reconstruction accuracy. Our experiments on multiple datasets demonstrate for the first time that the GS-based technique can match traditional aerial MVS methods on geometric accuracy, and beat state-of-the-art GS-based methods on geometry and rendering quality.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.00381v1">PDF</a> 11 pages</p><p><strong>Summary</strong><br>提出基于3DGS的大规模表面重建方法AGS，解决大规模航空图像表面重建问题。</p><p><strong>Key Takeaways</strong></p><ol><li>首次提出适用于多视角立体航空图像的3DGS表面重建方法AGS。</li><li>针对大规模航空图像提出数据分块方法，提高3DGS应用可行性。</li><li>引入Ray-Gaussian Intersection方法获取法向和深度信息。</li><li>集成多视角几何一致性约束，增强全局几何一致性。</li><li>实验证明GS技术在几何精度上可与传统方法媲美。</li><li>在几何和渲染质量上优于现有GS方法。</li><li>首次展示GS技术在航空MVS中的潜力。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>标题：基于3D高斯展开的大规模三维表面重建技术研究</p></li><li><p>作者：YuanZheng Wu, Jin Liu, Shunping Ji</p></li><li><p>所属机构：无信息提供</p></li><li><p>关键词：3D Gaussian Splatting；大规模三维表面重建；空中图像；多视角立体成像</p></li><li><p>Urls：[论文链接]；GitHub代码链接（如果有的话）：GitHub: None（待补充）</p></li><li><p>总结：</p><ul><li><p>(1) 研究背景：随着空中成像技术的发展，大规模三维场景的重建成为计算机视觉领域的研究热点。本文的研究背景是探索一种基于三维高斯展开（3DGS）的大规模三维表面重建技术，以解决传统方法在大型场景中的计算效率低下和精度不足的问题。</p></li><li><p>(2) 过去的方法及其问题：现有的大规模三维表面重建方法主要依赖于多视角立体成像技术（MVS），虽然取得了一定的成果，但在处理大规模场景时仍面临计算量大、训练时间长等问题。而基于神经辐射场表示（NeRF）的方法虽然可以实现高质量的场景渲染，但在处理大规模场景时同样面临计算资源需求巨大的问题。此外，传统的3DGS方法在处理大规模场景时也存在内存占用大、深度信息提取困难等问题。因此，现有的方法难以在保证计算效率的同时实现高精度的大规模三维表面重建。</p></li><li><p>(3) 研究方法：针对上述问题，本文提出了一种基于3DGS的大规模三维表面重建方法，名为Aerial Gaussian Splatting（AGS）。首先，通过数据分块方法处理大规模场景，降低内存占用和提高计算效率；其次，采用射线与高斯球相交的方法获取深度和法线信息，以便更好地约束几何形状；最后，通过引入多视角几何一致性约束，提高全局几何一致性和重建精度。本文还将这种方法应用于多个数据集进行实验验证。</p></li><li><p>(4) 任务与性能：本文的实验结果表明，该方法能够在保证计算效率的同时实现高质量的大规模三维表面重建。通过与传统的MVS方法和现有的GS方法进行对比实验，本文提出的方法在几何精度和渲染质量上均取得了显著的提升。这证明了该方法的有效性和优越性。此外，该方法还能够在不同的数据集上实现稳定的应用性能，进一步验证了其通用性和实用性。总体来说，本文提出的方法为大规模三维表面重建提供了一种新的解决方案，具有重要的实际应用价值。</p></li></ul></li><li>方法论概述：</li></ol><p>本文提出的方法论是基于三维高斯展开的大规模三维表面重建技术，名为Aerial Gaussian Splatting（AGS）。其主要步骤包括：</p><pre><code>- (1) 数据分块处理：针对大规模场景，通过自适应图像场景分块方法，将场景分割成多个数据块，降低内存占用并提高计算效率。

- (2) 射线与高斯球相交技术：采用射线与高斯球相交的方法获取深度和法线信息，以更好地约束几何形状。

- (3) 多视角几何一致性约束：引入多视角几何一致性约束，提高全局几何一致性和重建精度。通过比较不同视角下的深度图和法线图，建立几何一致性损失函数，以优化全局几何一致性。

- (4) 实验验证：将该方法应用于多个数据集进行实验验证，证明该方法在保证计算效率的同时，实现了高质量的大规模三维表面重建。与传统的多视角立体成像技术和基于神经辐射场的方法相比，该方法在几何精度和渲染质量上取得了显著的提升。
</code></pre><p>该方法的创新之处在于通过数据分块处理、射线与高斯球相交技术、以及多视角几何一致性约束等技术手段，实现了大规模三维场景的快速、准确重建，具有重要的实际应用价值。</p><ol><li><p>Conclusion:</p><ul><li><p>(1) 工作的意义：该文章研究的基于三维高斯展开的大规模三维表面重建技术具有重要的实际应用价值。随着空中成像技术的发展，大规模三维场景的重建成为计算机视觉领域的研究热点，该研究为大规模三维表面重建提供了一种新的解决方案。</p></li><li><p>(2) 优缺点：创新点方面，文章提出的基于三维高斯展开的大规模三维表面重建方法，结合了数据分块处理、射线与高斯球相交技术、多视角几何一致性约束等技术手段，实现了大规模三维场景的快速、准确重建，这是该文章的一大亮点。性能方面，文章的实验结果表明，该方法能够在保证计算效率的同时实现高质量的大规模三维表面重建，与传统的多视角立体成像技术和基于神经辐射场的方法相比，该方法在几何精度和渲染质量上取得了显著的提升。工作量方面，文章进行了多个实验验证，涉及多个数据集的应用，证明了方法的通用性和实用性。然而，文章未提及该方法可能面临的计算资源需求随着场景规模的增大而增大的挑战。</p></li></ul></li></ol><p>希望以上内容能够满足您的需求。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-1aa8e7cc95424684ab6f6809e6a95fbb.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-eb7f2d63a708e56cb3f315f891df5a81.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-38431324e793db5199b03288a0264191.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-47b487caefe29f17b9dfcae04f5cdd62.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-865671d2f4c2cbb7a401d50558eacd87.jpg" align="middle"></details><h2 id="UDGS-SLAM-UniDepth-Assisted-Gaussian-Splatting-for-Monocular-SLAM"><a href="#UDGS-SLAM-UniDepth-Assisted-Gaussian-Splatting-for-Monocular-SLAM" class="headerlink" title="UDGS-SLAM : UniDepth Assisted Gaussian Splatting for Monocular SLAM"></a>UDGS-SLAM : UniDepth Assisted Gaussian Splatting for Monocular SLAM</h2><p><strong>Authors:Mostafa Mansour, Ahmed Abdelsalam, Ari Happonen, Jari Porras, Esa Rahtu</strong></p><p>Recent advancements in monocular neural depth estimation, particularly those achieved by the UniDepth network, have prompted the investigation of integrating UniDepth within a Gaussian splatting framework for monocular SLAM.This study presents UDGS-SLAM, a novel approach that eliminates the necessity of RGB-D sensors for depth estimation within Gaussian splatting framework. UDGS-SLAM employs statistical filtering to ensure local consistency of the estimated depth and jointly optimizes camera trajectory and Gaussian scene representation parameters. The proposed method achieves high-fidelity rendered images and low ATERMSE of the camera trajectory. The performance of UDGS-SLAM is rigorously evaluated using the TUM RGB-D dataset and benchmarked against several baseline methods, demonstrating superior performance across various scenarios. Additionally, an ablation study is conducted to validate design choices and investigate the impact of different network backbone encoders on system performance.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.00362v1">PDF</a></p><p><strong>Summary</strong><br>UDGS-SLAM：基于UniDepth的Gaussian splatting单目SLAM，无需RGB-D传感器，实现深度估计与场景表示。</p><p><strong>Key Takeaways</strong></p><ul><li>首次将UniDepth与Gaussian splatting结合实现单目SLAM。</li><li>无需RGB-D传感器，降低成本和复杂性。</li><li>使用统计滤波确保深度估计局部一致性。</li><li>联合优化相机轨迹和场景表示参数。</li><li>实现高保真渲染图像和低ATERMSE。</li><li>在TUM RGB-D数据集上优于基线方法。</li><li>通过消融研究验证设计选择。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: UniDepth辅助的高斯分割方法在单目视觉SLAM中的应用</p></li><li><p>Authors: Mostafa Mansour, Ahmed Abdelsalam, Ari Happonen, Jari Porras, Esa Rahtu</p></li><li><p>Affiliation:</p><ul><li>Mostafa Mansour：芬兰坦佩雷大学工程与自然科学学院</li><li>Ahmed Abdelsalam：芬兰LUT大学工程科学学院</li><li>Ari Happonen, Jari Porras：芬兰Aalto大学电气工程学院</li><li>Esa Rahtu：芬兰大学信息技术与传播科学学院</li></ul></li><li><p>Keywords: UniDepth网络，高斯分割，单目SLAM，密集SLAM，映射，场景表示</p></li><li><p>Urls: 论文链接未知，GitHub代码链接未知（GitHub: None）</p></li><li><p>Summary:</p><ul><li>(1) 研究背景：本文的研究背景是近期在单目神经深度估计领域的进展，特别是UniDepth网络的出现，促使研究人员探索在单目SLAM中集成高斯分割方法的可能性。</li><li>(2) 过去的方法及问题：现有的SLAM技术中，地图（或场景）的表示是一个关键组件，影响着SLAM系统中其他子系统的性能，并影响着依赖于SLAM输出的外部系统的效果。尽管存在各种显式的手动设计稀疏和密集表示方法，但它们仍然面临一些局限性，如依赖3D几何特征、仅表示观察到的环境部分以及缺乏生成或合成不同相机视角下的逼真的高保真场景的能力。因此，需要一种新的方法来解决这些问题。</li><li>(3) 研究方法：本文提出了一种新的方法UDGS-SLAM，它通过集成UniDepth网络和高斯分割框架来消除对RGB-D传感器的深度估计需求。UDGS-SLAM使用统计滤波来确保估计深度的局部一致性，并联合优化相机轨迹和场景（高斯）表示参数。它通过使用神经渲染技术实现高保真图像渲染和低ATE-RMSE相机轨迹估计。此外，文章还进行了一项消融研究，以验证设计选择并研究不同网络骨干编码器对系统性能的影响。</li><li>(4) 任务与性能：本文的方法在TUM RGB-D数据集上进行了严格评估，并与几种基线方法进行了比较，在各种场景下均表现出优越的性能。其性能支持了方法的目标，即实现高精度的相机轨迹估计和场景重建。</li></ul></li><li>方法论：</li></ol><p>这篇论文提出了一个新的方法UDGS-SLAM来解决单目视觉SLAM中的问题。方法主要涉及到以下几个步骤：</p><ul><li>(1) 背景研究：首先，论文研究了单目神经深度估计领域的最新进展，特别是UniDepth网络的出现，这促使研究人员探索在单目SLAM中集成高斯分割方法的可行性。这是对背景的一个深入理解和评估，为后续的研究奠定了基础。</li><li>(2) 问题分析：现有的SLAM技术中，地图（或场景）的表示是一个关键组件，影响着SLAM系统中其他子系统的性能，并影响着依赖于SLAM输出的外部系统的效果。论文指出了现有方法面临的挑战，如依赖3D几何特征、仅表示观察到的环境部分以及缺乏生成或合成不同相机视角下的逼真的高保真场景的能力。因此，需要一种新的方法来解决这些问题。</li><li>(3) 方法提出：针对现有问题，论文提出了一种新的方法UDGS-SLAM。该方法通过集成UniDepth网络和高斯分割框架，消除了对RGB-D传感器的深度估计需求。UDGS-SLAM使用统计滤波来确保估计深度的局部一致性，并联合优化相机轨迹和场景（高斯）表示参数。它通过使用神经渲染技术实现高保真图像渲染和低ATE-RMSE相机轨迹估计。此外，文章还进行了一项消融研究，以验证设计选择并研究不同网络骨干编码器对系统性能的影响。</li><li>(4) 任务与性能评估：论文在TUM RGB-D数据集上严格评估了所提出的方法，并与几种基线方法进行了比较。实验结果表明，在各种场景下，论文提出的方法均表现出优越的性能。这验证了方法的目标，即实现高精度的相机轨迹估计和场景重建。</li><li>(5) 具体实现细节：论文详细阐述了UDGS-SLAM的实现细节，包括神经深度估计、渲染和损失计算、跟踪和映射等步骤。其中涉及到的关键技术包括3D高斯场景表示、彩色和深度可微分渲染、可微分相机姿态估计等。这些技术共同构成了UDGS-SLAM的核心框架，使得系统在单目视觉SLAM任务中取得了良好的性能。</li></ul><ol><li>Conclusion:</li></ol><ul><li><p>(1)这篇工作的意义在于提出了一种新的方法UDGS-SLAM，该方法在单目视觉SLAM中集成了UniDepth网络和高斯分割方法，从而实现了高精度的相机轨迹估计和场景重建。这项工作对于增强现实、自动驾驶、机器人导航等领域具有重要的应用价值。</p></li><li><p>(2)创新点：该文章的创新点主要体现在将神经深度估计与单目视觉SLAM相结合，利用UniDepth网络生成深度图，并采用高斯分割方法表示场景，实现了对场景的密集表示和相机轨迹的优化。此外，文章还进行了一项消融研究，验证了设计选择并研究了不同网络骨干编码器对系统性能的影响。<br>性能：在TUM RGB-D数据集上的实验结果表明，该方法在各种场景下均表现出优越的性能，实现了高精度的相机轨迹估计和场景重建。<br>工作量：该文章对方法进行了详细的阐述和实验验证，包括背景研究、问题分析、方法提出、任务与性能评估以及具体实现细节等方面。实验数据充分，论证严谨，工作量较大。</p></li></ul><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-602ddadeeaca45c983d51ebc3538e566.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-9c4f4d6baedebc9d9e195d1ccaa37278.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-e8f537a4ba013b2f2f03f9bd5bfebcb5.jpg" align="middle"></details><h2 id="Generic-Objects-as-Pose-Probes-for-Few-Shot-View-Synthesis"><a href="#Generic-Objects-as-Pose-Probes-for-Few-Shot-View-Synthesis" class="headerlink" title="Generic Objects as Pose Probes for Few-Shot View Synthesis"></a>Generic Objects as Pose Probes for Few-Shot View Synthesis</h2><p><strong>Authors:Zhirui Gao, Renjiao Yi, Chenyang Zhu, Ke Zhuang, Wei Chen, Kai Xu</strong></p><p>Radiance fields including NeRFs and 3D Gaussians demonstrate great potential in high-fidelity rendering and scene reconstruction, while they require a substantial number of posed images as inputs. COLMAP is frequently employed for preprocessing to estimate poses, while it necessitates a large number of feature matches to operate effectively, and it struggles with scenes characterized by sparse features, large baselines between images, or a limited number of input images. We aim to tackle few-view NeRF reconstruction using only 3 to 6 unposed scene images. Traditional methods often use calibration boards but they are not common in images. We propose a novel idea of utilizing everyday objects, commonly found in both images and real life, as “pose probes”. The probe object is automatically segmented by SAM, whose shape is initialized from a cube. We apply a dual-branch volume rendering optimization (object NeRF and scene NeRF) to constrain the pose optimization and jointly refine the geometry. Specifically, object poses of two views are first estimated by PnP matching in an SDF representation, which serves as initial poses. PnP matching, requiring only a few features, is suitable for feature-sparse scenes. Additional views are incrementally incorporated to refine poses from preceding views. In experiments, PoseProbe achieves state-of-the-art performance in both pose estimation and novel view synthesis across multiple datasets. We demonstrate its effectiveness, particularly in few-view and large-baseline scenes where COLMAP struggles. In ablations, using different objects in a scene yields comparable performance. Our project page is available at: \href{<a target="_blank" rel="noopener" href="https://zhirui-gao.github.io/PoseProbe.github.io/}{this">https://zhirui-gao.github.io/PoseProbe.github.io/}{this</a> https URL}</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.16690v2">PDF</a></p><p><strong>Summary</strong><br>利用日常物体作为“姿态探测”进行少视图NeRF重建，提高场景重建精度。</p><p><strong>Key Takeaways</strong></p><ul><li>NeRFs和3D Gaussians在渲染和重建方面潜力巨大，但需大量姿态图像。</li><li>COLMAP预处理需要大量特征匹配，在稀疏特征场景中表现不佳。</li><li>提出利用日常物体作为“姿态探测”的新方法。</li><li>SAM自动分割探测物体，形状初始化为立方体。</li><li>采用双分支体积渲染优化，结合对象NeRF和场景NeRF进行几何优化。</li><li>PnP匹配用于估计两个视图的对象姿态，适用于稀疏特征场景。</li><li>逐步增加视图以优化先前的姿态。</li><li>PoseProbe在多个数据集上实现姿态估计和新型视图合成的最先进性能。</li><li>在COLMAP难以处理的场景中，PoseProbe表现出色。</li><li>使用场景中不同物体获得相似性能。</li><li>项目页面：[this https URL]</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题**： 通用物体作为姿态探针用于少视点视图合成的研究</li></ol><p><strong>2. 作者</strong>： 智瑞高、任娇义、陈威丞等（注：原文中未提供完整的作者名单，只列出了部分作者）</p><p><strong>3. 所属机构</strong>： 国防科技大学（National University of Defense Technology）</p><p><strong>4. 关键词</strong>： NeRF重建、姿态估计、少视点视图合成、通用物体姿态探针等。</p><p><strong>5. Urls</strong>: Paper链接：<a href="#论文链接">点击这里</a>；GitHub代码链接：<a href="#GitHub">GitHub链接</a>，如无GitHub代码链接，则填写“GitHub:None”。</p><p><strong>6. 摘要</strong>：</p><p><em>(1) 研究背景：</em> 随着计算机视觉和图形学领域的发展，神经辐射场（NeRF）为场景的高保真渲染提供了新的能力。然而，它依赖于精确输入的相机姿态和足够数量的图像，这在现实场景中限制了其应用。特别是在有限的视角和稀疏的图像输入情况下，姿态估计成为了一个挑战。本文旨在解决在仅有少量未定位场景图像的情况下进行NeRF重建的问题。</p><p><em>(2) 过去的方法与问题：</em> 现有的方法如COLMAP在特征稀疏、大基线或少量输入图像的场景中表现不佳。其他方法依赖假设如完美姿态初始值或特定场景特性。尤其是传统的校准板方法并不适用于日常场景，因为校准板并不常见在普通图像中。因此，需要一种实用的替代方案来解决少视点NeRF重建的问题。</p><p><em>(3) 研究方法：</em> 本文提出了一种利用日常通用物体作为“姿态探针”的新方法。这些物体如可乐罐或盒子等在图像和现实生活中都很常见。通过SAM自动分割探针物体，并以立方体形状进行初始化。为了约束姿态优化并联合改进几何结构，应用了双分支体积渲染优化（对象NeRF和场景NeRF）。具体来说，通过PnP匹配在SDF表示中估计两个视图的物体姿态作为初始姿态。这种方法仅需要少量特征，因此适用于特征稀疏的场景。额外的视图会逐步纳入以从前一个视图中细化姿态。</p><p><em>(4) 任务与性能：</em> 在多个数据集上的实验表明，PoseProbe方法在姿态估计和新颖视图合成方面都达到了最先进的性能。特别是在少视点和大型基线场景中，PoseProbe取得了显著的效果，这些场景是COLMAP方法所困扰的。使用不同物体进行实验的结果表明，该方法的性能相当稳定。本研究验证了利用通用物体作为姿态探针的有效性和实用性。其性能达到了研究目标，为解决少视点NeRF重建问题提供了一个有效的解决方案。</p><ol><li><p>方法概述：</p><ul><li><p>(1) 利用通用物体作为姿态探针：选择场景中的通用物体（如可乐罐或盒子）作为姿态探针，这些物体在图像和现实中都很常见。通过SAM自动分割这些物体，并以立方体形状进行初始化。</p></li><li><p>(2) 设计了一种基于双分支体积渲染的优化方法：针对姿态优化和几何结构联合改进的问题，研究提出了一种双分支体积渲染优化方法。包括对象NeRF和场景NeRF两部分，用于约束姿态优化并改进几何结构。具体来说，通过PnP匹配在SDF表示中估计两个视图的物体姿态作为初始姿态。该方法仅需要少量特征，适用于特征稀疏的场景。额外的视图会被逐步纳入，以从前一个视图中细化姿态。</p></li><li><p>(3) 混合显式与隐式SDF表示：设计了一种混合显式与隐式SDF生成网络，用于高效优化相机姿态和物体表示。显式模板场T用于初始化，隐式变形场D用于预测变形场和校正场。这种表示法结合了显式与隐式表示的优点，实现了快速收敛与精细建模的平衡。</p></li><li><p>(4) 增量姿态优化：采用增量姿态优化的方法，通过逐步引入新图像进行训练。利用SuperPoint和SuperGlue计算图像之间的2D-3D对应关系，通过PnP算法结合RANSAC计算新图像的初始姿态。最后，联合优化新添加的图像视图和辐射场。</p></li><li><p>(5) 多视图几何一致性：采用多视图投影距离来约束相机姿态。通过最小化对应点之间的投影距离，提高几何一致性。还引入了一项正则化项，用于细化相机姿态，该正则化项基于射线与物体表面的距离最小化。</p></li></ul></li></ol><p>这些方法共同构成了该文章的姿态估计和新颖视图合成方案，针对少视点NeRF重建问题提供了有效的解决方案。</p><ol><li>Conclusion:</li></ol><p>(1) 这项研究工作的意义在于解决了少视点NeRF重建的问题。针对计算机视觉和图形学领域中的NeRF技术在实际应用中遇到的挑战，特别是当场景图像数量有限、视角狭窄时，该研究提出了一种实用的解决方案。通过利用通用物体作为姿态探针，实现了在少量未定位场景图像下的NeRF重建，为高质量渲染提供了可能。</p><p>(2) 创新点、性能和工作量三个方面对这篇文章进行简要总结如下：</p><p>创新点：文章提出了一种利用通用物体作为姿态探针的新方法，解决了少视点NeRF重建的问题。该方法结合了计算机视觉和图形学的技术，通过双分支体积渲染优化、混合显式与隐式SDF表示、增量姿态优化以及多视图几何一致性等技术手段，实现了在有限图像和视角下的NeRF重建。</p><p>性能：实验结果表明，该方法在多个数据集上的姿态估计和新颖视图合成方面都达到了最先进的性能。特别是在少视点和大型基线场景中，相较于传统方法如COLMAP，PoseProbe方法取得了显著的效果。</p><p>工作量：文章进行了大量的实验验证，使用了多个数据集进行性能测试。同时，文章详细介绍了方法的实现细节，包括姿态探针的选择、双分支体积渲染优化、混合显式与隐式SDF表示等。此外，文章还讨论了方法的局限性和未来研究方向。</p><p>总体而言，这篇文章针对少视点NeRF重建问题提出了一种有效的解决方案，具有重要的理论和实践价值。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-5ba1ef9a8809bb09066cef76ba85e436.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-111f9a405b1cbd89c50123286e9163cb.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-e56e79f4faacda08035fe179832f2bd5.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-0d7a0312eb0f82084bd210c10d98ba65.jpg" align="middle"></details><h2 id="EaDeblur-GS-Event-assisted-3D-Deblur-Reconstruction-with-Gaussian-Splatting"><a href="#EaDeblur-GS-Event-assisted-3D-Deblur-Reconstruction-with-Gaussian-Splatting" class="headerlink" title="EaDeblur-GS: Event assisted 3D Deblur Reconstruction with Gaussian   Splatting"></a>EaDeblur-GS: Event assisted 3D Deblur Reconstruction with Gaussian Splatting</h2><p><strong>Authors:Yuchen Weng, Zhengwen Shen, Ruofan Chen, Qi Wang, Jun Wang</strong></p><p>3D deblurring reconstruction techniques have recently seen significant advancements with the development of Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). Although these techniques can recover relatively clear 3D reconstructions from blurry image inputs, they still face limitations in handling severe blurring and complex camera motion. To address these issues, we propose Event-assisted 3D Deblur Reconstruction with Gaussian Splatting (EaDeblur-GS), which integrates event camera data to enhance the robustness of 3DGS against motion blur. By employing an Adaptive Deviation Estimator (ADE) network to estimate Gaussian center deviations and using novel loss functions, EaDeblur-GS achieves sharp 3D reconstructions in real-time, demonstrating performance comparable to state-of-the-art methods.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.13520v3">PDF</a></p><p><strong>Summary</strong><br>提出基于事件相机数据增强的3DGS去模糊重建方法，实现实时高清晰3D重建。</p><p><strong>Key Takeaways</strong></p><ul><li>3DGS和NeRF在3D去模糊重建技术中取得进展。</li><li>面对严重模糊和复杂运动，3DGS存在局限性。</li><li>提出EaDeblur-GS，结合事件相机数据增强3DGS。</li><li>使用ADE网络估计高斯中心偏差。</li><li>采用新型损失函数提高重建效果。</li><li>实现实时高清晰3D重建。</li><li>性能接近现有最先进方法。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>标题：基于事件辅助的三维去模糊重建技术研究——EaDeblur-GS方法</p></li><li><p>作者：Weng Yucheng（翁煜宸）, Shen Zhengwen（沈正文）, Chen Ruofan（陈若凡）, Wang Qi（王琦）, You Shaoze（尤少泽）, Wang Jun（王军）等。</p></li><li><p>隶属机构：中国矿业大学信息与控制工程学院。</p></li><li><p>关键词：三维高斯模糊技术、事件相机、神经辐射场。</p></li><li><p>Urls：论文链接（尚未提供），GitHub代码链接（尚未提供）。</p></li><li><p>总结：</p><ul><li><p>(1) 研究背景：随着计算机视觉和计算机图形学的发展，从图像重建三维场景和物体已成为研究热点。尽管存在诸如神经辐射场（NeRF）和三维高斯模糊技术（3DGS）等技术能够从模糊图像中恢复出相对清晰的3D重建结果，但它们仍面临处理严重模糊和复杂相机运动的挑战。本文的研究背景是探讨如何解决这些问题，提出一种新的基于事件相机数据辅助的三维去模糊重建技术。</p></li><li><p>(2) 过往方法与问题：现有的NeRF和3DGS技术在处理模糊图像时存在局限性。例如，Deblur-NeRF是首个解决这个问题的框架，但它可能无法完全恢复出高质量的图像。MP-NeRF进一步引入了多分支融合网络和基于先验的权重来处理极端模糊或不均匀模糊图像，但仍存在挑战。因此，需要一种新的方法来解决这些问题。</p></li><li><p>(3) 研究方法：本文提出了基于事件辅助的三维去模糊重建技术——EaDeblur-GS。该方法集成了事件相机数据，以提高3DGS对运动模糊的鲁棒性。EaDeblur-GS利用新型自适应偏差估计器（ADE）网络和两种新型损失函数，实现实时、清晰的3D重建。</p></li><li><p>(4) 任务与性能：本文的方法在3D去模糊重建任务上取得了先进的性能表现，相较于原始高斯模糊技术和其他去模糊高斯模糊技术，其性能得到了显著提升。实验结果表明，该方法能够处理严重模糊和复杂相机运动带来的挑战，实现高质量的3D重建。总体而言，该论文提出的EaDeblur-GS方法支持其解决运动模糊问题的目标。</p></li></ul></li></ol><p>以上就是对该论文的总结，希望符合您的要求。</p><ol><li><p>方法介绍：</p><ul><li><p>(1) 基于输入模糊RGB图像及其对应的事件流，使用事件双积分（EDI）技术生成一系列潜在的清晰图像。</p></li><li><p>(2) 利用COLMAP工具增强初始重建，提供相对精确的相机姿态估计，以优化模糊图像的重建质量。</p></li><li><p>(3) 构建一组三维高斯模型，并利用自适应偏差估计器（ADE）网络调整高斯中心位置，计算位置偏差。这些调整后的三维高斯模型被投影到各个视点（包括相应的潜在视点），生成清晰的图像。为了模拟真实模糊并改善高斯模型中的对象形状准确性，引入了模糊损失和事件集成损失。这一系列步骤使得模型能够学习精确的三维体积表示，实现高质量的三维重建。总的来说，该论文通过结合模糊RGB图像与事件数据来增强图像清晰度，并使用了复杂的技术手段来改善运动模糊带来的问题。</p></li></ul></li><li>Conclusion:</li></ol><p>(1) 工作意义：该研究对于解决计算机视觉和计算机图形学领域中的三维去模糊重建问题具有重要意义。它提出了一种新的基于事件辅助的三维去模糊重建技术，能够处理严重模糊和复杂相机运动带来的挑战，实现高质量的3D重建。</p><p>(2) 优缺点：</p><p>创新点：该研究结合了事件相机数据和RGB图像，提出了一种新型的基于事件辅助的三维去模糊重建方法EaDeblur-GS，具有显著的创新性。</p><p>性能：相较于其他去模糊高斯模糊技术，EaDeblur-GS在3D去模糊重建任务上取得了先进的性能表现。实验结果表明，该方法能够处理复杂场景下的运动模糊问题，实现高质量的3D重建。</p><p>工作量：研究涉及了事件相机数据与RGB图像的结合、自适应偏差估计器的构建、模糊损失和事件集成损失的设计等多个方面的工作，工作量较大。此外，该研究还涉及到了多个数据集的实验验证和性能评估，工作量较为繁重。</p><p>总之，该研究为三维去模糊重建领域提供了一种新的思路和方法，具有重要的学术价值和应用前景。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-9e06303bc08821a95ca9caeba9e4800a.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-1f7c5ca64a98273686668c65bfba6772.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-e8356b61d2f0ac6f36d5dc7923722354.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-aaa6009356e9f2bbf15bede06fe2ce90.jpg" align="middle"></details></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io/2024/09/07/Paper/2024-09-07/3DGS/">https://kedreamix.github.io/2024/09/07/Paper/2024-09-07/3DGS/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/3DGS/">3DGS</a></div><div class="post_share"><div class="social-share" data-image="https://picx.zhimg.com/v2-cdf76afac71806d71d890403f41eecc2.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/09/07/Paper/2024-09-07/NeRF/" title="NeRF"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-4bee50cb798179036a928d1f71e17522.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">NeRF</div></div></a></div><div class="next-post pull-right"><a href="/2024/09/07/Paper/2024-09-07/Talking%20Head%20Generation/" title="Talking Head Generation"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-a1b462b823ae781c464329b2e4e1d7a4.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Talking Head Generation</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/01/25/Paper/3DGS%20Survey/" title="3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-25</div><div class="title">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</div></div></a></div><div><a href="/2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" title="超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-01</div><div class="title">超赞的数字人生成知识库 Awesome-Talking-Head-Synthesis</div></div></a></div><div><a href="/2024/01/24/Paper/2024-01-24/3DGS/" title="3DGS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-3d3dcd00c27bc3d320b23d4247ae79f3.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-24</div><div class="title">3DGS</div></div></a></div><div><a href="/2024/02/02/Paper/2024-02-02/3DGS/" title="3DGS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-e4e5570dfa99dfac9b297f7650c717c3.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-02</div><div class="title">3DGS</div></div></a></div><div><a href="/2024/01/30/Paper/2024-01-30/3DGS/" title="3DGS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-2032721a60695f2d41ac96f75dec65a2.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-30</div><div class="title">3DGS</div></div></a></div><div><a href="/2024/02/09/Paper/2024-02-09/3DGS/" title="3DGS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-28074a5f13fdf5a52c0d4de04dfb9406.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-09</div><div class="title">3DGS</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-09-07-%E6%9B%B4%E6%96%B0"><span class="toc-text">2024-09-07 更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#LM-Gaussian-Boost-Sparse-view-3D-Gaussian-Splatting-with-Large-Model-Priors"><span class="toc-text">LM-Gaussian: Boost Sparse-view 3D Gaussian Splatting with Large Model Priors</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Human-VDM-Learning-Single-Image-3D-Human-Gaussian-Splatting-from-Video-Diffusion-Models"><span class="toc-text">Human-VDM: Learning Single-Image 3D Human Gaussian Splatting from Video Diffusion Models</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Object-Gaussian-for-Monocular-6D-Pose-Estimation-from-Sparse-Views"><span class="toc-text">Object Gaussian for Monocular 6D Pose Estimation from Sparse Views</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#GGS-Generalizable-Gaussian-Splatting-for-Lane-Switching-in-Autonomous-Driving"><span class="toc-text">GGS: Generalizable Gaussian Splatting for Lane Switching in Autonomous Driving</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DynOMo-Online-Point-Tracking-by-Dynamic-Online-Monocular-Gaussian-Reconstruction"><span class="toc-text">DynOMo: Online Point Tracking by Dynamic Online Monocular Gaussian Reconstruction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PRoGS-Progressive-Rendering-of-Gaussian-Splats"><span class="toc-text">PRoGS: Progressive Rendering of Gaussian Splats</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Free-DyGS-Camera-Pose-Free-Scene-Reconstruction-based-on-Gaussian-Splatting-for-Dynamic-Surgical-Videos"><span class="toc-text">Free-DyGS: Camera-Pose-Free Scene Reconstruction based on Gaussian Splatting for Dynamic Surgical Videos</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3D-Gaussian-Splatting-for-Large-scale-3D-Surface-Reconstruction-from-Aerial-Images"><span class="toc-text">3D Gaussian Splatting for Large-scale 3D Surface Reconstruction from Aerial Images</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#UDGS-SLAM-UniDepth-Assisted-Gaussian-Splatting-for-Monocular-SLAM"><span class="toc-text">UDGS-SLAM : UniDepth Assisted Gaussian Splatting for Monocular SLAM</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Generic-Objects-as-Pose-Probes-for-Few-Shot-View-Synthesis"><span class="toc-text">Generic Objects as Pose Probes for Few-Shot View Synthesis</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#EaDeblur-GS-Event-assisted-3D-Deblur-Reconstruction-with-Gaussian-Splatting"><span class="toc-text">EaDeblur-GS: Event assisted 3D Deblur Reconstruction with Gaussian Splatting</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image:url('https://picx.zhimg.com/v2-cdf76afac71806d71d890403f41eecc2.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js").then((()=>{pangu.autoSpacingPage()}))}function panguInit(){panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.1},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script></div><script id="canvas_nest" defer color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><script data-pjax>function butterfly_swiper_injector_config(){var a=document.getElementById("recent-posts");console.log("已挂载butterfly_swiper"),a.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/25/Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="2024/01/25/Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="2024/01/25/Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/17/Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023/12/17/Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="2023/12/17/Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script></body></html>