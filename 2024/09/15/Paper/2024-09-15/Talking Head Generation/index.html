<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Talking Head Generation | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-09-15  ProbTalk3D Non-Deterministic Emotion Controllable Speech-Driven 3D   Facial Animation Synthesis Using VQ-VAE">
<meta property="og:type" content="article">
<meta property="og:title" content="Talking Head Generation">
<meta property="og:url" content="https://kedreamix.github.io/2024/09/15/Paper/2024-09-15/Talking%20Head%20Generation/index.html">
<meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World">
<meta property="og:description" content="Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-09-15  ProbTalk3D Non-Deterministic Emotion Controllable Speech-Driven 3D   Facial Animation Synthesis Using VQ-VAE">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pic1.zhimg.com/v2-b7c6faba3d39fe9b3dabca7e4a16a8a8.jpg">
<meta property="article:published_time" content="2024-09-14T17:57:44.000Z">
<meta property="article:modified_time" content="2024-09-14T17:57:44.053Z">
<meta property="article:author" content="Kedreamix">
<meta property="article:tag" content="Talking Head Generation">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pic1.zhimg.com/v2-b7c6faba3d39fe9b3dabca7e4a16a8a8.jpg"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/2024/09/15/Paper/2024-09-15/Talking%20Head%20Generation/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-74LZ5BEQQ1');
</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":true,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  noticeOutdate: {"limitDay":500,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: true,
  islazyload: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Talking Head Generation',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-09-15 01:57:44'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const now = new Date()
          const hour = now.getHours()
          const isNight = hour <= 6 || hour >= 24
          if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
          else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css"  media="defer" onload="this.media='all'"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload="this.media='all'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.0.0"><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml">

<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()
  window.addEventListener('load',() => { preloader.endLoading() })

  if (false) {
    document.addEventListener('pjax:send', () => { preloader.initLoading() })
    document.addEventListener('pjax:complete', () => { preloader.endLoading() })
  }
})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">283</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://pic1.zhimg.com/v2-b7c6faba3d39fe9b3dabca7e4a16a8a8.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"/><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Talking Head Generation</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-09-14T17:57:44.000Z" title="发表于 2024-09-15 01:57:44">2024-09-15</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-09-14T17:57:44.053Z" title="更新于 2024-09-15 01:57:44">2024-09-15</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">8.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>30分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Talking Head Generation"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote>
<p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-09-15-更新"><a href="#2024-09-15-更新" class="headerlink" title="2024-09-15 更新"></a>2024-09-15 更新</h1><h2 id="ProbTalk3D-Non-Deterministic-Emotion-Controllable-Speech-Driven-3D-Facial-Animation-Synthesis-Using-VQ-VAE"><a href="#ProbTalk3D-Non-Deterministic-Emotion-Controllable-Speech-Driven-3D-Facial-Animation-Synthesis-Using-VQ-VAE" class="headerlink" title="ProbTalk3D: Non-Deterministic Emotion Controllable Speech-Driven 3D   Facial Animation Synthesis Using VQ-VAE"></a>ProbTalk3D: Non-Deterministic Emotion Controllable Speech-Driven 3D   Facial Animation Synthesis Using VQ-VAE</h2><p><strong>Authors:Sichun Wu, Kazi Injamamul Haque, Zerrin Yumak</strong></p>
<p>Audio-driven 3D facial animation synthesis has been an active field of research with attention from both academia and industry. While there are promising results in this area, recent approaches largely focus on lip-sync and identity control, neglecting the role of emotions and emotion control in the generative process. That is mainly due to the lack of emotionally rich facial animation data and algorithms that can synthesize speech animations with emotional expressions at the same time. In addition, majority of the models are deterministic, meaning given the same audio input, they produce the same output motion. We argue that emotions and non-determinism are crucial to generate diverse and emotionally-rich facial animations. In this paper, we propose ProbTalk3D a non-deterministic neural network approach for emotion controllable speech-driven 3D facial animation synthesis using a two-stage VQ-VAE model and an emotionally rich facial animation dataset 3DMEAD. We provide an extensive comparative analysis of our model against the recent 3D facial animation synthesis approaches, by evaluating the results objectively, qualitatively, and with a perceptual user study. We highlight several objective metrics that are more suitable for evaluating stochastic outputs and use both in-the-wild and ground truth data for subjective evaluation. To our knowledge, that is the first non-deterministic 3D facial animation synthesis method incorporating a rich emotion dataset and emotion control with emotion labels and intensity levels. Our evaluation demonstrates that the proposed model achieves superior performance compared to state-of-the-art emotion-controlled, deterministic and non-deterministic models. We recommend watching the supplementary video for quality judgement. The entire codebase is publicly available (<a target="_blank" rel="noopener" href="https://github.com/uuembodiedsocialai/ProbTalk3D/">https://github.com/uuembodiedsocialai/ProbTalk3D/</a>). </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.07966v1">PDF</a> 14 pages, 9 figures, 3 tables. Includes code. Accepted at ACM   SIGGRAPH MIG 2024</p>
<p><strong>Summary</strong><br>音频驱动3D面部动画合成，重视情感和非确定性，提出ProbTalk3D模型。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>3D面部动画合成研究活跃，但多忽视情感。</li>
<li>情感数据缺乏，算法难以同时合成语音和表情。</li>
<li>现有模型多确定性，输出运动单一。</li>
<li>提出ProbTalk3D，利用VQ-VAE模型和3DMEAD数据集。</li>
<li>比较分析，提出更合适的评价指标。</li>
<li>首次结合情感数据集和非确定性。</li>
<li>模型性能优于现有情感控制模型。</li>
<li>公开代码库，供进一步研究。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li><p>标题：ProbTalk3D：非确定性情绪控制</p>
</li>
<li><p>作者：吴思春、卡兹·因贾玛姆乌尔·哈克、泽林·尤马克（英文名字分别为Sichun Wu、Kazi Injamamul Haque、Zerrin Yumak）</p>
</li>
<li><p>所属机构：乌特勒支大学（荷兰）（Utrecht University, Netherlands）</p>
</li>
<li><p>关键词：面部动画合成、深度学习、虚拟人类、非确定性模型、情绪控制面部动画</p>
</li>
<li><p>Urls：论文链接未知，GitHub代码库链接为<a target="_blank" rel="noopener" href="https://github.com/uuembodiedsocialai/ProbTalk3D/">GitHub链接</a>（如有变动，请根据实际情况填写）</p>
</li>
<li><p>总结：</p>
<p> (1) 研究背景：随着音频驱动的3D面部动画合成的活跃研究，尽管该领域取得了显著的进展，但近期的方法主要关注唇同步和身份控制，忽视了情绪在生成过程中的作用。本文旨在解决这一问题。</p>
<p> (2) 前期方法与问题：多数现有模型是确定性的，即给定相同的音频输入，它们会产生相同的输出运动。这使得面部动画缺乏多样性和情感丰富性。文章指出缺乏情感丰富的面部动画数据和算法是主要原因。因此，需要一种能够合成带有情感表达的语音动画的非确定性模型。因此提出研究问题和动机。</p>
<p> (3) 研究方法：本文提出了一种非确定性的神经网络方法ProbTalk3D，用于情感控制的语音驱动3D面部动画合成。它采用两阶段VQ-VAE模型和丰富的情感动画数据集3DMEAD。文中进行了与最新3D面部动画合成方法的比较性分析，通过客观、主观和用户感知研究进行评价。据称这是首次将非确定性模型应用于情感控制的3D面部动画合成中。同时使用了多种适合评估随机输出的客观指标和真实与模拟数据的主观评估。文章提出的模型通过实验结果展示其优越性。</p>
<p> (4) 任务与性能：本文提出的ProbTalk3D模型在非确定性环境下进行情感控制的面部动画合成任务上取得了显著的成效。实验结果证明了模型相对于现有的情感控制和非确定性模型的优越性。提供的模拟视频和实际应用的代码库可供公众使用以验证其性能。通过广泛的评估和模拟结果支持了方法的有效性。</p>
</li>
<li>方法论：</li>
</ol>
<p>(1) 数据集选择：采用3DMEAD数据集，该数据集从二维音视频数据集MEAD重建而来，包含了多种情绪和强度的三维面部动画数据。</p>
<p>(2) 问题描述：文章旨在解决情感在面部动画合成过程中的缺失问题，现有模型大多确定性地生成输出，缺乏多样性和情感丰富性。因此，提出使用非确定性神经网络方法ProbTalk3D，用于情感控制的语音驱动3D面部动画合成。</p>
<p>(3) 方法概述：采用两阶段VQ-VAE模型和丰富的情感动画数据集3DMEAD。第一阶段为运动自编码器阶段，通过VQ-VAE学习运动先验；第二阶段为语音和情感条件阶段，通过冻结运动自编码器，训练音频编码器以产生与运动潜空间相似的音频潜空间，并融合风格嵌入与编码的音频信息。通过这种方式，给定音频和风格输入，可以生成带有情感表达的语音动画。</p>
<p>(4) 具体实现：在第一阶段，通过运动编码器将输入运动数据编码为潜在空间，并利用向量量化技术学习离散潜空间嵌入代码本。在第二阶段，冻结运动自编码器，训练音频编码器以产生音频潜空间，并融合风格嵌入。最终，利用冻结的运动解码器将量化后的潜空间解码为面部动画数据。在训练过程中，使用多种损失函数来优化模型性能，包括量化损失、重建损失等。</p>
<p>(5) 评估方法：通过客观指标和主观评估方法对模型性能进行评估。客观指标包括量化损失和重建损失等，主观评估则通过用户感知研究进行。实验结果证明了ProbTalk3D模型在非确定性环境下进行情感控制的面部动画合成任务上的优越性。</p>
<ol>
<li>Conclusion:</li>
</ol>
<p>（1）这项工作的重要性在于它解决了现有面部动画合成模型在情感表达方面的缺失问题。通过引入非确定性神经网络方法ProbTalk3D，该研究实现了情感控制的语音驱动3D面部动画合成，使得面部动画更加多样、富有情感。</p>
<p>（2）创新点：本文首次将非确定性模型应用于情感控制的3D面部动画合成中，提出了一种两阶段VQ-VAE模型，并采用丰富的情感动画数据集3DMEAD进行训练。<br>性能：通过客观指标和主观评估方法的评价，ProbTalk3D模型在非确定性环境下进行情感控制的面部动画合成任务上取得了显著的成效，相对于现有的情感控制和非确定性模型表现出优越性。<br>工作量：文章采用了先进的神经网络架构和多种评估方法，实验设计合理，工作量较大。</p>
<details>
  <summary>点此查看论文截图</summary>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-750e8f021377c93dae9805234c2fa996.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-4777d67595c1d84bae8d0ec3415d2564.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-b0cf5c7e6a853321218751ea3fc0a113.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-39f033345d783b993c831788a64d7b28.jpg" align="middle">
</details>




<h2 id="DiffTED-One-shot-Audio-driven-TED-Talk-Video-Generation-with-Diffusion-based-Co-speech-Gestures"><a href="#DiffTED-One-shot-Audio-driven-TED-Talk-Video-Generation-with-Diffusion-based-Co-speech-Gestures" class="headerlink" title="DiffTED: One-shot Audio-driven TED Talk Video Generation with   Diffusion-based Co-speech Gestures"></a>DiffTED: One-shot Audio-driven TED Talk Video Generation with   Diffusion-based Co-speech Gestures</h2><p><strong>Authors:Steven Hogue, Chenxu Zhang, Hamza Daruger, Yapeng Tian, Xiaohu Guo</strong></p>
<p>Audio-driven talking video generation has advanced significantly, but existing methods often depend on video-to-video translation techniques and traditional generative networks like GANs and they typically generate taking heads and co-speech gestures separately, leading to less coherent outputs. Furthermore, the gestures produced by these methods often appear overly smooth or subdued, lacking in diversity, and many gesture-centric approaches do not integrate talking head generation. To address these limitations, we introduce DiffTED, a new approach for one-shot audio-driven TED-style talking video generation from a single image. Specifically, we leverage a diffusion model to generate sequences of keypoints for a Thin-Plate Spline motion model, precisely controlling the avatar’s animation while ensuring temporally coherent and diverse gestures. This innovative approach utilizes classifier-free guidance, empowering the gestures to flow naturally with the audio input without relying on pre-trained classifiers. Experiments demonstrate that DiffTED generates temporally coherent talking videos with diverse co-speech gestures. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.07649v1">PDF</a> </p>
<p><strong>Summary</strong><br>针对现有方法生成动作不连贯、缺乏多样性的问题，DiffTED通过扩散模型实现从单一图像到TED风格视频的一步生成，并确保动作连贯性和多样性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有方法依赖视频转视频技术，生成动作不连贯。</li>
<li>现有方法生成的动作通常平滑或单调，缺乏多样性。</li>
<li>许多以动作为核心的方法不整合头像生成。</li>
<li>DiffTED提出一种从单一图像生成TED风格视频的新方法。</li>
<li>使用扩散模型生成关键点序列，控制动作动画。</li>
<li>确保动作时间上的连贯性和多样性。</li>
<li>利用无分类器的引导，使动作自然与音频输入同步。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li><p>标题：基于扩散模型的音频驱动TED演讲视频单帧生成技术</p>
</li>
<li><p>作者：Steven Hogue, 陈旭章, Hamza Daruger, Yapeng Tian, Xiaohu Guo</p>
</li>
<li><p>隶属机构：德克萨斯大学达拉斯分校</p>
</li>
<li><p>关键词：音频驱动、演讲视频生成、扩散模型、共语手势、TED演讲风格</p>
</li>
<li><p>链接：由于未提供论文链接和GitHub代码链接，无法填写。</p>
</li>
<li><p>摘要：</p>
<ul>
<li>(1)研究背景：随着音频驱动的谈话视频生成技术的不断发展，现有方法在很大程度上依赖于视频到视频的转换技术和传统的生成网络（如GANs），它们通常将头部和共语手势分开生成，导致输出不够连贯。此外，现有方法产生的手势往往过于平滑或过于平淡，缺乏多样性，且许多以手势为中心的方法并不整合讲话头部生成。本文旨在解决这些问题。</li>
<li>(2)过去的方法及问题：现有的音频驱动谈话视频生成方法主要依赖于视频到视频的转换技术和传统生成网络，产生的结果常常缺乏连贯性和多样性。同时，许多方法没有很好地将共语手势与头部动作相结合，使得生成的视频不够自然。</li>
<li>(3)研究方法：本文提出一种名为DiffTED的新方法，用于从单张图片生成TED风格的音频驱动谈话视频。该方法利用扩散模型生成Thin-Plate Spline运动模型的关键点序列，精确控制角色的动画，同时确保手势在时间上连贯且多样。此外，该方法采用无分类器引导，使手势能够自然地随音频输入流动，而无需依赖预训练分类器。</li>
<li>(4)任务与性能：本文的方法用于生成TED风格的音频驱动谈话视频，并在实验中证明了其生成的视频在时间上具有连贯性且共语手势多样。由于采用了扩散模型和Thin-Plate Spline运动模型，该方法能够生成高质量的视频，支持其设定的目标。</li>
</ul>
</li>
</ol>
<p>希望以上回答符合您的要求。</p>
<ol>
<li><p>Methods:</p>
<ul>
<li><p>(1) 研究背景分析：文章首先分析了现有的音频驱动谈话视频生成技术的不足，如依赖视频到视频的转换技术和传统生成网络，产生的结果缺乏连贯性和多样性，以及未能很好地结合共语手势和头部动作等问题。</p>
</li>
<li><p>(2) 方法引入：针对现有问题，文章提出了一种基于扩散模型的音频驱动TED演讲视频单帧生成技术（DiffTED）。该方法利用扩散模型生成Thin-Plate Spline运动模型的关键点序列，实现对角色动画的精确控制。</p>
</li>
<li><p>(3) 技术特点：DiffTED方法确保生成的手势在时间上连贯且多样，同时采用无分类器引导，使手势能够自然地随音频输入流动，无需依赖预训练分类器。此外，该方法能够从单张图片生成TED风格的音频驱动谈话视频。</p>
</li>
<li><p>(4) 实验验证：文章通过实验验证了该方法的有效性，证明其生成的视频在时间上具有连贯性且共语手势多样，能够生成高质量的视频。</p>
</li>
</ul>
</li>
</ol>
<p>希望以上内容符合您的要求。</p>
<ol>
<li>Conclusion:</li>
</ol>
<ul>
<li><p>(1) 这项工作的意义在于提出了一种基于扩散模型的音频驱动TED演讲视频单帧生成技术（DiffTED），解决了现有音频驱动谈话视频生成技术在连贯性和多样性方面存在的问题，并能够根据单张图片生成TED风格的音频驱动谈话视频，为教育性演讲视频的生成和虚假视频的识别提供了可能。</p>
</li>
<li><p>(2) 创新点：该文章的创新之处在于利用扩散模型生成Thin-Plate Spline运动模型的关键点序列，实现对角色动画的精确控制，确保生成的手势在时间上连贯且多样，同时采用无分类器引导，使手势能够自然地随音频输入流动。<br>性能：实验验证了该方法的有效性，证明其生成的视频在时间上具有连贯性且共语手势多样，能够生成高质量的视频。<br>工作量：文章进行了详尽的方法介绍和实验验证，但并未提供论文链接和GitHub代码链接，无法评估其实际工作量。</p>
</li>
</ul>
<details>
  <summary>点此查看论文截图</summary>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-14a61a5334506789f815b9b297fe0da3.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-fdd90264c03f9b4028f5243dc076c886.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-fd5e28acf4753d730cd0ceb5dafd4b98.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-e9f673ed505fddc1c27496b7aca092ec.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-be500b804187e266c242ca9958910168.jpg" align="middle">
</details>




<h2 id="EMOdiffhead-Continuously-Emotional-Control-in-Talking-Head-Generation-via-Diffusion"><a href="#EMOdiffhead-Continuously-Emotional-Control-in-Talking-Head-Generation-via-Diffusion" class="headerlink" title="EMOdiffhead: Continuously Emotional Control in Talking Head Generation   via Diffusion"></a>EMOdiffhead: Continuously Emotional Control in Talking Head Generation   via Diffusion</h2><p><strong>Authors:Jian Zhang, Weijian Mai, Zhijun Zhang</strong></p>
<p>The task of audio-driven portrait animation involves generating a talking head video using an identity image and an audio track of speech. While many existing approaches focus on lip synchronization and video quality, few tackle the challenge of generating emotion-driven talking head videos. The ability to control and edit emotions is essential for producing expressive and realistic animations. In response to this challenge, we propose EMOdiffhead, a novel method for emotional talking head video generation that not only enables fine-grained control of emotion categories and intensities but also enables one-shot generation. Given the FLAME 3D model’s linearity in expression modeling, we utilize the DECA method to extract expression vectors, that are combined with audio to guide a diffusion model in generating videos with precise lip synchronization and rich emotional expressiveness. This approach not only enables the learning of rich facial information from emotion-irrelevant data but also facilitates the generation of emotional videos. It effectively overcomes the limitations of emotional data, such as the lack of diversity in facial and background information, and addresses the absence of emotional details in emotion-irrelevant data. Extensive experiments and user studies demonstrate that our approach achieves state-of-the-art performance compared to other emotion portrait animation methods. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.07255v1">PDF</a> 12 pages, 7 figures</p>
<p><strong>Summary</strong><br>提出EMOdiffhead，实现情感驱动的说话头视频生成，突破传统限制，性能卓越。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EMOdiffhead可生成情感驱动的说话头视频。</li>
<li>方法支持细粒度情感控制与编辑。</li>
<li>利用FLAME模型和DECA方法提取表情向量。</li>
<li>结合音频引导扩散模型，实现精确唇同步。</li>
<li>从情感无关数据学习丰富面部信息。</li>
<li>克服情感数据多样性不足问题。</li>
<li>实验与用户研究证明性能领先。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li><p>标题：基于扩散模型的音频驱动情感肖像动画生成</p>
</li>
<li><p>作者：作者名称（具体名称需要根据原文提供）</p>
</li>
<li><p>隶属机构：某大学计算机视觉与人工智能实验室</p>
</li>
<li><p>关键词：音频驱动肖像动画、情感生成、扩散模型、表情建模、UNet、3D人脸重建</p>
</li>
<li><p>Urls：论文链接（如果可用），Github代码链接（如果可用，填写为“Github:None”如果不可用）</p>
</li>
<li><p>总结：</p>
<ul>
<li><p>(1)研究背景：本文的研究背景是音频驱动的肖像动画生成任务，特别是在生成具有精确情感表达能力的谈话头视频方面的挑战。现有方法往往难以同时实现精细的情感控制和视频质量，因此本文提出了一种新的解决方案。</p>
</li>
<li><p>(2)过去的方法及其问题：过去的方法主要关注唇同步和视频质量，但很少解决生成情感驱动谈话头视频的挑战。由于缺乏情感控制和数据多样性问题，现有方法难以生成具有丰富情感表达的视频。</p>
</li>
<li><p>(3)研究方法：本文提出了一种名为EMOdiffhead的新方法，用于情感谈话头视频生成。该方法结合了扩散模型和条件机制，以生成具有精确唇同步和丰富情感表达的视频。首先，使用FLAME 3D模型提取表情向量，并结合音频引导扩散模型。通过利用时间条件UNet和交叉注意力块，实现情感类别和强度的精细控制。此外，还引入了一种新的条件残差块以实现更好的结果。</p>
</li>
<li><p>(4)任务与性能：本文的方法在生成具有精确情感表达能力的谈话头视频方面取得了显著成果。通过在各种实验和用户研究中的表现，证明了该方法相较于其他情感肖像动画方法的优越性。性能结果支持了该方法的有效性。</p>
</li>
</ul>
</li>
</ol>
<p>请注意，以上内容是根据您提供的信息进行的概括，具体细节可能需要参考原始论文。</p>
<ol>
<li>方法：</li>
</ol>
<p>(1) 研究背景与问题定义：本文研究音频驱动的肖像动画生成任务，特别是在生成具有精确情感表达能力的谈话头视频方面的挑战。过去的方法主要关注唇同步和视频质量，但难以生成具有丰富情感表达的视频。</p>
<p>(2) 方法概述：本文提出了一种名为EMOdiffhead的新方法，用于情感谈话头视频生成。该方法结合了扩散模型和条件机制。</p>
<p>(3) 表情向量提取与扩散模型结合：使用FLAME 3D模型提取表情向量，结合音频引导扩散模型。通过时间条件UNet和交叉注意力块，实现情感类别和强度的精细控制。</p>
<p>(4) 引入新的条件残差块：为提高生成视频的质量，引入了一种新的条件残差块。</p>
<p>(5) 实验与评估：通过大量实验和用户研究，对提出的EMOdiffhead方法进行了评估。实验结果表明，该方法在生成具有精确情感表达能力的谈话头视频方面取得了显著成果，相较于其他情感肖像动画方法具有优越性。</p>
<p>注：以上内容根据论文摘要进行概括，具体细节和实现方式需参考原始论文。</p>
<ol>
<li>Conclusion:</li>
</ol>
<p>（1）该工作的意义在于提出了一种名为EMOdiffhead的新方法，解决了音频驱动肖像动画生成中情感表达的精确性问题。通过对情感类型和强度的精细控制，该方法提高了谈话头视频的情感表达能力，进一步推动了情感肖像动画领域的进展。同时，该研究也为计算机视觉和人工智能领域提供了一种新的思路和方法。</p>
<p>（2）创新点：该研究提出了一种结合扩散模型和条件机制的EMOdiffhead方法，实现了情感谈话头视频生成中的精确唇同步和丰富情感表达。该方法通过引入时间条件UNet和交叉注意力块等新技术手段，实现了对情感类别和强度的精细控制，提高了生成视频的质量和自然度。此外，该研究还引入了一种新的条件残差块，进一步优化了生成结果。</p>
<p>性能：该研究通过大量实验和用户研究对提出的EMOdiffhead方法进行了评估，实验结果表明该方法在生成具有精确情感表达能力的谈话头视频方面取得了显著成果，相较于其他情感肖像动画方法具有优越性。这表明该研究提出的方法具有较高的性能和稳定性。</p>
<p>工作量：该研究涉及了大量的理论分析和实验设计，需要进行大量的数据处理和模型训练等工作。同时，该研究还需要对现有的计算机视觉和人工智能技术进行深入研究和分析，以确定研究中的技术路线和方法选择。因此，该研究的工作量较大。</p>
<details>
  <summary>点此查看论文截图</summary>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-5b5e155308b98e6844d73c362a3aaac9.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-d793aaccb3f4f19ec734af9db1f30630.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-9f61281c36661a707a8e3d0e627fe0bd.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-e340404c2b992c0f1b7728b0bf9c293c.jpg" align="middle">
</details>




<h2 id="PersonaTalk-Bring-Attention-to-Your-Persona-in-Visual-Dubbing"><a href="#PersonaTalk-Bring-Attention-to-Your-Persona-in-Visual-Dubbing" class="headerlink" title="PersonaTalk: Bring Attention to Your Persona in Visual Dubbing"></a>PersonaTalk: Bring Attention to Your Persona in Visual Dubbing</h2><p><strong>Authors:Longhao Zhang, Shuang Liang, Zhipeng Ge, Tianshu Hu</strong></p>
<p>For audio-driven visual dubbing, it remains a considerable challenge to uphold and highlight speaker’s persona while synthesizing accurate lip synchronization. Existing methods fall short of capturing speaker’s unique speaking style or preserving facial details. In this paper, we present PersonaTalk, an attention-based two-stage framework, including geometry construction and face rendering, for high-fidelity and personalized visual dubbing. In the first stage, we propose a style-aware audio encoding module that injects speaking style into audio features through a cross-attention layer. The stylized audio features are then used to drive speaker’s template geometry to obtain lip-synced geometries. In the second stage, a dual-attention face renderer is introduced to render textures for the target geometries. It consists of two parallel cross-attention layers, namely Lip-Attention and Face-Attention, which respectively sample textures from different reference frames to render the entire face. With our innovative design, intricate facial details can be well preserved. Comprehensive experiments and user studies demonstrate our advantages over other state-of-the-art methods in terms of visual quality, lip-sync accuracy and persona preservation. Furthermore, as a person-generic framework, PersonaTalk can achieve competitive performance as state-of-the-art person-specific methods. Project Page: <a target="_blank" rel="noopener" href="https://grisoon.github.io/PersonaTalk/">https://grisoon.github.io/PersonaTalk/</a>. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.05379v1">PDF</a> Accepted at SIGGRAPH Asia 2024 (Conference Track)</p>
<p><strong>Summary</strong><br>提出基于注意力的PersonaTalk框架，实现个性化视觉配音的高保真度和真实性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语音驱动的视觉配音面临保持说话者个性和精确唇形同步的挑战。</li>
<li>PersonaTalk框架包含几何构建和面部渲染两个阶段。</li>
<li>第一阶段采用风格感知音频编码模块，通过交叉注意力层将说话风格注入音频特征。</li>
<li>第二阶段引入双重注意力面部渲染器，分别从不同参考帧采样纹理渲染整个面部。</li>
<li>实验和用户研究表明，PersonaTalk在视觉质量、唇形同步准确性和个性保持方面优于现有方法。</li>
<li>PersonaTalk作为通用人脸框架，在性能上可与其他针对特定人脸的方法相媲美。</li>
<li>可访问项目页面获取更多详情：<a target="_blank" rel="noopener" href="https://grisoon.github.io/PersonaTalk/。">https://grisoon.github.io/PersonaTalk/。</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li><p><strong>标题</strong>： PersonaTalk：视觉配音中的个性化关注</p>
</li>
<li><p><strong>作者</strong>： Longhao Zhang（张龙豪）、Shuang Liang（梁爽）、Zhipeng Ge（葛志鹏）、Tianshu Hu（胡天舒）</p>
</li>
<li><p><strong>作者隶属机构</strong>： 中国字节跳动</p>
</li>
<li><p><strong>关键词</strong>： 视觉配音、唇同步、注意力机制</p>
</li>
<li><p><strong>链接</strong>： 请根据论文实际链接填写，例如：<a target="_blank" rel="noopener" href="https://xxx">论文链接</a>，Github代码链接（如可用）：Github: None</p>
</li>
<li><p><strong>摘要</strong>：</p>
<p> (1) <strong>研究背景</strong>： </p>
<pre><code>随着音频驱动视觉配音技术的广泛应用，如何在合成高质量唇同步视频的同时，突出演讲者的个性，如说话风格和面部细节，成为了一个重要的挑战。本文旨在解决这一挑战。
</code></pre><p> (2) <strong>过去的方法及问题</strong>： </p>
<pre><code>现有的方法在音频驱动的视觉配音中往往难以捕捉演讲者的独特说话风格或保留面部细节。文章作者观察到这一痛点，并认为需要通过一种新的方法来解决。
</code></pre><p> (3) <strong>研究方法</strong>： </p>
<pre><code>本文提出了一个名为PersonaTalk的注意力机制两阶段框架，用于实现高质量和个性化的视觉配音。第一阶段是风格感知音频编码模块，通过交叉注意力层将说话风格注入音频特征。然后，使用这些特征驱动演讲者的模板几何来获得唇同步的几何形状。在第二阶段，引入了一个双注意力面部渲染器，通过两个平行的交叉注意力层（即唇注意力和面部注意力）从参考帧中渲染目标几何的纹理。整个设计能够很好地保留面部细节。
</code></pre><p> (4) <strong>任务与性能</strong>： </p>
<pre><code>本文的方法在视觉配音任务上进行了实验和用户研究，展示了在视觉质量、唇同步准确性和个性保留方面的优势。与其他最先进的方法相比，该方法具有竞争力。此外，作为一个人通用的框架，它还能在个性化任务上实现与个性化方法相当的性能。
</code></pre></li>
<li>方法论：</li>
</ol>
<p>(1) 研究背景分析：随着音频驱动视觉配音技术的广泛应用，如何同时合成高质量唇同步视频并突出演讲者的个性成为了一个重要的挑战。文章针对这一挑战展开研究。</p>
<p>(2) 过去方法及问题分析：现有方法在音频驱动的视觉配音中往往难以捕捉演讲者的独特说话风格或保留面部细节。文章作者针对这一痛点提出了新方法。</p>
<p>(3) 方法论概述：本文提出了一个名为PersonaTalk的注意力机制两阶段框架，用于实现高质量和个性化的视觉配音。第一阶段是风格感知音频编码模块，通过交叉注意力层将说话风格注入音频特征，驱动演讲者的模板几何获得唇同步的几何形状。第二阶段引入了双注意力面部渲染器，通过两个平行的交叉注意力层（即唇注意力和面部注意力）从参考帧中渲染目标几何的纹理，保留面部细节。整体设计旨在确保在合成视觉配音时能够同时实现高质量和个性化。</p>
<p>(4) 实验与评估：本文的方法在视觉配音任务上进行了实验和用户研究，通过对比实验和用户反馈验证了该方法在视觉质量、唇同步准确性和个性保留方面的优势。此外，该框架作为一个人通用的框架，在个性化任务上的性能也相当出色。</p>
<ol>
<li>结论：</li>
</ol>
<p>(1)工作意义：该论文针对音频驱动的视觉配音技术中的个性化问题进行了深入研究，提出了一种基于注意力机制的两阶段框架，旨在实现高质量且个性化的视觉配音，具有重要的实际应用价值。</p>
<p>(2)创新点、性能和工作量总结：</p>
<pre><code>- 创新点：该论文提出了一个名为PersonaTalk的注意力机制框架，通过风格感知音频编码模块和双注意力面部渲染器，实现了高质量和个性化的视觉配音。该框架能够捕捉演讲者的独特说话风格并保留面部细节，与现有方法相比具有竞争力。
- 性能：实验和用户研究结果表明，该方法在视觉质量、唇同步准确性和个性保留方面均表现出优势。与其他最先进的方法相比，该框架具有竞争力，并且在个性化任务上的性能也相当出色。
- 工作量：论文对方法的实现进行了详细的描述，并通过实验和用户研究对方法进行了验证。然而，关于工作量方面，论文未提供具体的工作量评估，如代码复杂度、计算资源消耗等。
</code></pre><p>总体来说，该论文在音频驱动的视觉配音技术方面取得了显著的进展，提出了一个具有竞争力的个性化视觉配音框架，并在实验和用户研究中验证了其有效性。</p>
<details>
  <summary>点此查看论文截图</summary>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-8af1624f90997286ea579f7e46df5075.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-b7c6faba3d39fe9b3dabca7e4a16a8a8.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-5e1e26a1220037676340587ada7ac37d.jpg" align="middle">
</details>




<h2 id="KAN-Based-Fusion-of-Dual-Domain-for-Audio-Driven-Facial-Landmarks-Generation"><a href="#KAN-Based-Fusion-of-Dual-Domain-for-Audio-Driven-Facial-Landmarks-Generation" class="headerlink" title="KAN-Based Fusion of Dual-Domain for Audio-Driven Facial Landmarks   Generation"></a>KAN-Based Fusion of Dual-Domain for Audio-Driven Facial Landmarks   Generation</h2><p><strong>Authors:Hoang-Son Vo-Thanh, Quang-Vinh Nguyen, Soo-Hyung Kim</strong></p>
<p>Audio-driven talking face generation is a widely researched topic due to its high applicability. Reconstructing a talking face using audio significantly contributes to fields such as education, healthcare, online conversations, virtual assistants, and virtual reality. Early studies often focused solely on changing the mouth movements, which resulted in outcomes with limited practical applications. Recently, researchers have proposed a new approach of constructing the entire face, including face pose, neck, and shoulders. To achieve this, they need to generate through landmarks. However, creating stable landmarks that align well with the audio is a challenge. In this paper, we propose the KFusion of Dual-Domain model, a robust model that generates landmarks from audio. We separate the audio into two distinct domains to learn emotional information and facial context, then use a fusion mechanism based on the KAN model. Our model demonstrates high efficiency compared to recent models. This will lay the groundwork for the development of the audio-driven talking face generation problem in the future. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.05330v1">PDF</a> </p>
<p><strong>Summary</strong><br>音频驱动的人脸生成研究广泛，提出基于双域模型的KFusion，高效生成与音频对齐的稳定关键点。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>音频驱动人脸生成应用广泛。</li>
<li>早期研究仅关注口型变化。</li>
<li>新方法构建完整面部，包括姿态、颈部和肩部。</li>
<li>标记点生成需与音频对齐。</li>
<li>提出KFusion双域模型生成标记点。</li>
<li>模型分离音频至两个域学习情感和面部信息。</li>
<li>模型效率高，为未来研究奠定基础。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li><p>标题：基于双域的音频驱动面部特征点生成研究（KAN-Based Fusion of Dual-Domain for Audio-Driven Facial Landmarks Generation）</p>
</li>
<li><p>作者：Vo-Thanh Hoang-Son、Nguyen Quang-Vinh、Kim Soo-Hyung（其中第一个名字是姓氏）</p>
</li>
<li><p>所属机构：韩国庆北国立大学人工智能融合学院。</p>
</li>
<li><p>关键词：音频驱动谈话面部生成、情感谈话面部、音频到特征点。</p>
</li>
<li><p>Urls：文章链接未提供，GitHub代码链接未提供。</p>
</li>
<li><p>总结：</p>
</li>
</ol>
<p>(1) 研究背景：音频驱动的谈话面部生成是一个热门的研究领域，具有广泛的应用前景，如教育、医疗、在线对话等。随着研究的深入，该领域逐渐从单纯的改变口型动作转向构建整个面部的动作，包括头部姿态、颈部和肩部动作等。因此，需要生成面部特征点来实现这一目的。然而，创建一个与音频良好匹配的稳定特征点是一个挑战。本研究旨在解决这一问题。</p>
<p>(2) 过去的方法及问题：早期的研究主要关注于改变口型动作，结果具有有限的实用性。近期的研究提出了一种构建整个面部的新方法，但需要生成特征点来实现。然而，现有的方法难以创建稳定的特征点，这些特征点需要与音频良好匹配。因此，有必要提出一种新的方法来改善这一问题。本论文提出的方法被充分激发并针对性地解决现有方法的问题。</p>
<p>(3) 研究方法：本研究提出了基于双域的KFusion模型来生成音频驱动的特征点。首先，将音频分为两个独立领域来学习情感信息和面部上下文。然后，使用基于KAN模型的融合机制来结合这两个领域的信息。这种方法能够更有效地生成与音频匹配的特征点。</p>
<p>(4) 任务与性能：本论文的方法在音频驱动的面部特征点生成任务上取得了良好的性能。通过创建稳定的特征点，该方法能够生成更真实的面部动作和表情。实验结果表明，该方法在生成整个面部的动作方面优于近期的方法，为音频驱动的谈话面部生成问题的发展奠定了基础。性能结果支持了该方法的有效性。</p>
<ol>
<li>方法论：</li>
</ol>
<ul>
<li>(1) 音频双域划分：首先，将音频信号分为两个独立领域。这一划分旨在提取音频中的情感信息和面部上下文信息。这两个领域将分别处理不同方面的信息，为后续的特征点生成提供数据基础。</li>
<li>(2) 基于KAN模型的融合机制：对于划分后的两个领域的信息，研究提出了基于KAN模型的融合机制。该机制通过特定的算法将两个领域的信息结合，实现信息的有效整合和综合利用。这一步骤是方法的核心部分，它直接影响到后续特征点的生成质量。</li>
<li>(3) 音频驱动的面部特征点生成：结合前两个步骤得到的信息，该方法进一步用于生成音频驱动的面部特征点。这些特征点能够反映音频中的情感信息和面部上下文信息，使得生成的面部动作更加真实、自然。这一步需要依赖于前面的数据处理和融合过程。</li>
<li>(4) 性能评估：为了验证方法的有效性，研究通过一系列实验对该方法进行了性能评估。实验结果表明，该方法在音频驱动的面部特征点生成任务上取得了良好的性能，相较于近期的方法具有优势。这也证明了方法的有效性和实用性。</li>
</ul>
<p>总体来说，这篇文章提出了一种基于双域的KFusion模型来生成音频驱动的面部特征点的方法。该方法通过划分音频信号、融合情感信息和面部上下文信息，实现了真实、自然的面部动作生成。实验结果证明了该方法的有效性。</p>
<ol>
<li>Conclusion:</li>
</ol>
<p>(1) 研究意义：这项工作对于音频驱动的面部特征点生成领域具有重要意义。它提出了一种基于双域的KFusion模型，能够有效生成与音频匹配的面部特征点，为音频驱动的谈话面部生成问题的发展奠定了基础。该研究的成果在教育、医疗、在线对话等领域具有广泛的应用前景。</p>
<p>(2) 创新点、性能、工作量评估：</p>
<pre><code>* 创新点：该研究提出了一种新的基于双域的音频驱动面部特征点生成方法，通过划分音频信号并融合情感信息和面部上下文信息，实现了真实、自然的面部动作生成。
* 性能：实验结果表明，该方法在音频驱动的面部特征点生成任务上取得了良好的性能，相较于近期的方法具有优势。
* 工作量：文章对音频信号的处理、模型的构建、实验的设计等方面进行了详细阐述，工作量较大。然而，文章未提供代码链接，无法对实现的复杂度和代码量进行准确评估。
</code></pre><details>
  <summary>点此查看论文截图</summary>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-7600f18367165445b7e6a98ad8f70fa2.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-04281b5c13292d79edb91089a734cb86.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-7114b73b4c6ad2bd4829e18507eaadb1.jpg" align="middle">
</details>




</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://kedreamix.github.io/2024/09/15/Paper/2024-09-15/Talking%20Head%20Generation/">https://kedreamix.github.io/2024/09/15/Paper/2024-09-15/Talking Head Generation/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Talking-Head-Generation/">Talking Head Generation</a></div><div class="post_share"><div class="social-share" data-image="https://pic1.zhimg.com/v2-b7c6faba3d39fe9b3dabca7e4a16a8a8.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/09/15/Paper/2024-09-15/3DGS/" title="3DGS"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-11840950c99eb7f2c5b34db295dcdf89.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">3DGS</div></div></a></div><div class="next-post pull-right"><a href="/2024/09/15/Paper/2024-09-15/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙/虚拟人"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-d6e379e1c6279a89b0ffe0aa8e95b394.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">元宇宙/虚拟人</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" title="超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-01</div><div class="title">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</div></div></a></div><div><a href="/2024/03/11/Note/BlendShape/" title="Blendshape学习笔记"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://p6-sign.toutiaoimg.com/pgc-image/2c8cbd123e00470e95500a8ae62da605~noop.image?_iz=58558&from=article.pc_detail&lk3s=953192f4&x-expires=1710668214&x-signature=UHPhjWP4v96kbtfJzF97Z%2Bp3klc%3D" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-11</div><div class="title">Blendshape学习笔记</div></div></a></div><div><a href="/2024/03/03/Paperscape/EMO/" title="EMO Emote Portrait Alive - 阿里HumanAIGC"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-6492e24fb03ffa98135dc584535ab7d9.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-03</div><div class="title">EMO Emote Portrait Alive - 阿里HumanAIGC</div></div></a></div><div><a href="/2024/03/15/Paperscape/Real3D-Portrait/" title="REAL3D-PORTRAIT ONE-SHOT REALISTIC 3D TALKING PORTRAIT SYNTHESIS"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-68585b79de5f83b0dfa23304f41b9b98.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-15</div><div class="title">REAL3D-PORTRAIT ONE-SHOT REALISTIC 3D TALKING PORTRAIT SYNTHESIS</div></div></a></div><div><a href="/2024/03/05/Paperscape/VividTalk/" title="VividTalk One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-8521b04f82075cc27b5e95148dba9792.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-05</div><div class="title">VividTalk One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior</div></div></a></div><div><a href="/2024/03/07/Paperscape/SyncTalk/" title="SyncTalk The Devil is in the Synchronization for Talking Head Synthesis"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-a57e0937b2f452009023394a59529dfb.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-07</div><div class="title">SyncTalk The Devil is in the Synchronization for Talking Head Synthesis</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-09-15-%E6%9B%B4%E6%96%B0"><span class="toc-text">2024-09-15 更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#ProbTalk3D-Non-Deterministic-Emotion-Controllable-Speech-Driven-3D-Facial-Animation-Synthesis-Using-VQ-VAE"><span class="toc-text">ProbTalk3D: Non-Deterministic Emotion Controllable Speech-Driven 3D   Facial Animation Synthesis Using VQ-VAE</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DiffTED-One-shot-Audio-driven-TED-Talk-Video-Generation-with-Diffusion-based-Co-speech-Gestures"><span class="toc-text">DiffTED: One-shot Audio-driven TED Talk Video Generation with   Diffusion-based Co-speech Gestures</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#EMOdiffhead-Continuously-Emotional-Control-in-Talking-Head-Generation-via-Diffusion"><span class="toc-text">EMOdiffhead: Continuously Emotional Control in Talking Head Generation   via Diffusion</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PersonaTalk-Bring-Attention-to-Your-Persona-in-Visual-Dubbing"><span class="toc-text">PersonaTalk: Bring Attention to Your Persona in Visual Dubbing</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#KAN-Based-Fusion-of-Dual-Domain-for-Audio-Driven-Facial-Landmarks-Generation"><span class="toc-text">KAN-Based Fusion of Dual-Domain for Audio-Driven Facial Landmarks   Generation</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image: url('https://pic1.zhimg.com/v2-b7c6faba3d39fe9b3dabca7e4a16a8a8.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><!-- hexo injector body_end start --><script data-pjax>
  function butterfly_swiper_injector_config(){
    var parent_div_git = document.getElementById('recent-posts');
    var item_html = '<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/25/Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="2024/01/25/Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="2024/01/25/Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/17/Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023/12/17/Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="2023/12/17/Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>';
    console.log('已挂载butterfly_swiper')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'undefined'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_swiper_injector_config();
  }
  else if (epage === cpage){
    butterfly_swiper_injector_config();
  }
  </script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script><!-- hexo injector body_end end --></body></html>