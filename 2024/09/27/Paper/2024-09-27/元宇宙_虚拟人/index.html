<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>元宇宙/虚拟人 | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="元宇宙&#x2F;虚拟人 方向最新论文已更新，请持续关注 Update in 2024-09-27  DreamWaltz-G Expressive 3D Gaussian Avatars from Skeleton-Guided 2D   Diffusion"><meta property="og:type" content="article"><meta property="og:title" content="元宇宙&#x2F;虚拟人"><meta property="og:url" content="https://kedreamix.github.io/2024/09/27/Paper/2024-09-27/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/index.html"><meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World"><meta property="og:description" content="元宇宙&#x2F;虚拟人 方向最新论文已更新，请持续关注 Update in 2024-09-27  DreamWaltz-G Expressive 3D Gaussian Avatars from Skeleton-Guided 2D   Diffusion"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://pic1.zhimg.com/v2-41ce0c960b001c3433e8f53f14598019.jpg"><meta property="article:published_time" content="2024-09-26T18:26:09.000Z"><meta property="article:modified_time" content="2024-09-26T18:26:09.275Z"><meta property="article:author" content="Kedreamix"><meta property="article:tag" content="元宇宙&#x2F;虚拟人"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://pic1.zhimg.com/v2-41ce0c960b001c3433e8f53f14598019.jpg"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/2024/09/27/Paper/2024-09-27/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-74LZ5BEQQ1")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!0,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"简"},noticeOutdate:{limitDay:500,position:"top",messagePrev:"It has been",messageNext:"days since the last update, the content of the article may be outdated."},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:200},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!0,post:!0},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!0,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"元宇宙/虚拟人",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-09-27 02:26:09"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach((e=>{n.setAttribute(e,t[e])})),document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)})),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=24?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 7.0.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml"></head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{const d=document.getElementById("loading-box"),e=document.body,o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l(),window.addEventListener("load",(()=>{o()}))})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">250</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url('https://pic1.zhimg.com/v2-41ce0c960b001c3433e8f53f14598019.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">元宇宙/虚拟人</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-09-26T18:26:09.000Z" title="发表于 2024-09-27 02:26:09">2024-09-27</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-09-26T18:26:09.275Z" title="更新于 2024-09-27 02:26:09">2024-09-27</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">6k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>20分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="元宇宙/虚拟人"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-09-27-更新"><a href="#2024-09-27-更新" class="headerlink" title="2024-09-27 更新"></a>2024-09-27 更新</h1><h2 id="DreamWaltz-G-Expressive-3D-Gaussian-Avatars-from-Skeleton-Guided-2D-Diffusion"><a href="#DreamWaltz-G-Expressive-3D-Gaussian-Avatars-from-Skeleton-Guided-2D-Diffusion" class="headerlink" title="DreamWaltz-G: Expressive 3D Gaussian Avatars from Skeleton-Guided 2D   Diffusion"></a>DreamWaltz-G: Expressive 3D Gaussian Avatars from Skeleton-Guided 2D Diffusion</h2><p><strong>Authors:Yukun Huang, Jianan Wang, Ailing Zeng, Zheng-Jun Zha, Lei Zhang, Xihui Liu</strong></p><p>Leveraging pretrained 2D diffusion models and score distillation sampling (SDS), recent methods have shown promising results for text-to-3D avatar generation. However, generating high-quality 3D avatars capable of expressive animation remains challenging. In this work, we present DreamWaltz-G, a novel learning framework for animatable 3D avatar generation from text. The core of this framework lies in Skeleton-guided Score Distillation and Hybrid 3D Gaussian Avatar representation. Specifically, the proposed skeleton-guided score distillation integrates skeleton controls from 3D human templates into 2D diffusion models, enhancing the consistency of SDS supervision in terms of view and human pose. This facilitates the generation of high-quality avatars, mitigating issues such as multiple faces, extra limbs, and blurring. The proposed hybrid 3D Gaussian avatar representation builds on the efficient 3D Gaussians, combining neural implicit fields and parameterized 3D meshes to enable real-time rendering, stable SDS optimization, and expressive animation. Extensive experiments demonstrate that DreamWaltz-G is highly effective in generating and animating 3D avatars, outperforming existing methods in both visual quality and animation expressiveness. Our framework further supports diverse applications, including human video reenactment and multi-subject scene composition.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.17145v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://yukun-huang.github.io/DreamWaltz-G/">https://yukun-huang.github.io/DreamWaltz-G/</a></p><p><strong>Summary</strong><br>利用预训练的2D扩散模型和分数蒸馏采样，提出DreamWaltz-G框架，实现从文本到可动3D虚拟人生成。</p><p><strong>Key Takeaways</strong></p><ol><li>结合2D扩散模型和SDS，实现文本到3D虚拟人生成。</li><li>DreamWaltz-G框架基于骨骼引导的分数蒸馏和混合3D高斯虚拟人表示。</li><li>骨骼引导的分数蒸馏增强SDS监督的视角和姿态一致性。</li><li>混合3D高斯虚拟人表示结合神经隐式场和参数化3D网格，实现实时渲染。</li><li>实验证明DreamWaltz-G在生成和动画3D虚拟人方面优于现有方法。</li><li>框架支持人视频重演和多主题场景合成等应用。</li><li>提升了动画表达性和视觉质量。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>标题：DreamWaltz-G：基于文本驱动的动画3D角色生成学习框架</p></li><li><p>作者：黄玉坤、王建安、曾爱玲、IEEE会员、郑俊杰、IEEE会员、张磊、IEEE资深会员、刘希辉、IEEE会员</p></li><li><p>所属机构：（按顺序）香港大学（HKU）、Astribot公司、腾讯公司、中国科学技术大学（USTC）、国际数字经济学院（IDEA）。</p></li><li><p>关键词：3D角色生成、3D人类模型、动态动画、扩散模型、分数蒸馏、3D高斯。</p></li><li><p>链接：论文链接（待补充），GitHub代码链接（如有）。</p></li><li><p>摘要：</p><p>(1) 研究背景：随着电影制作、游戏设计、虚拟现实等技术的快速发展，对高质量的3D角色生成的需求日益增长。传统的3D角色创建方法耗时耗力，而基于文本的3D角色生成成为了一种新的趋势。本文提出了一种基于文本驱动的零样本学习框架DreamWaltz-G，用于高质量的动画3D角色生成。</p><p>(2) 过往方法与问题：虽然现有方法利用预训练的二维扩散模型和分数蒸馏采样（SDS）在文本到三维角色的生成上取得了显著成果，但在生成高质量且能进行动态动画的三维角色方面仍面临挑战。问题包括几何结构的不准确，纹理细节的缺失，以及动态姿态下的变形问题等。</p><p>(3) 研究方法：本文提出的DreamWaltz-G框架通过Skeleton-guided Score Distillation（SkelSD）和Hybrid 3D Gaussian Avatars（H3GA）解决了上述问题。SkelSD通过将三维人体模板的骨架控制引入二维扩散模型，增强了SDS的稳定性并保持了三维一致性。H3GA则是一种混合的三维表示方法，旨在适应SDS优化并实现动态动画。具体来说，H3GA结合了神经隐式场和参数化三维网格，以实现实时渲染、稳定的SDS优化和动态动画。</p><p>(4) 任务与性能：本文在文本驱动的3D角色生成任务上进行了实验验证，DreamWaltz-G框架在视觉质量和动画表现力方面均表现出卓越的性能，超过了现有方法。此外，该框架还支持多种应用，如人类视频重演和多主体场景组合等。实验结果证明了其有效性和实用性。</p></li></ol><p>希望这个概括符合您的要求！如有任何修改或进一步的需求，请告知。</p><ol><li>方法论：</li></ol><p>（1）研究背景：随着电影制作、游戏设计、虚拟现实等技术的快速发展，对高质量的3D角色生成的需求日益增长。传统的3D角色创建方法耗时耗力，因此，研究出一种基于文本驱动的零样本学习框架DreamWaltz-G用于高质量的动画3D角色生成显得尤为重要。</p><p>（2）现有问题与挑战：现有方法虽然已经在文本到三维角色的生成上取得显著成果，但仍存在几何结构不准确、纹理细节缺失以及在动态姿态下的变形问题等挑战。</p><p>（3）研究方法介绍：针对上述问题与挑战，本文提出了基于文本驱动的零样本学习框架DreamWaltz-G。该框架主要包括两个部分：Skeleton-guided Score Distillation（SkelSD）和Hybrid 3D Gaussian Avatars（H3GA）。SkelSD通过将三维人体模板的骨架控制引入二维扩散模型，增强了SDS的稳定性并保持了三维一致性。H3GA则是一种混合的三维表示方法，旨在适应SDS优化并实现动态动画。具体来说，H3GA结合了神经隐式场和参数化三维网格，以实现实时渲染、稳定的SDS优化和动态动画。</p><p>（4）实验验证与性能表现：本文在文本驱动的3D角色生成任务上进行了实验验证，结果显示DreamWaltz-G框架在视觉质量和动画表现力方面均表现出卓越的性能，超过了现有方法。此外，该框架还支持多种应用，如人类视频重演和多主体场景组合等。实验结果证明了其有效性和实用性。</p><p>以上就是这篇论文的方法论部分的详细介绍。希望符合您的要求。</p><ol><li>Conclusion:</li></ol><p>(1) 这项工作的意义在于提出了一种基于文本驱动的零样本学习框架DreamWaltz-G，用于高质量的动画3D角色生成。该框架的应用能够简化3D角色创建流程，满足电影制作、游戏设计、虚拟现实等领域对高质量3D角色的需求。</p><p>(2) 创新点：文章提出了Skeleton-guided Score Distillation（SkelSD）和Hybrid 3D Gaussian Avatars（H3GA）方法，解决了现有方法在3D角色生成中的几何结构不准确、纹理细节缺失以及在动态姿态下的变形问题。<br>性能：实验验证显示，DreamWaltz-G框架在视觉质量和动画表现力方面表现出卓越的性能，超过了现有方法。<br>工作量：文章涉及的实验和验证工作量大，证明了该框架的有效性和实用性。</p><p>总体而言，这篇文章在3D角色生成领域具有一定的创新性和实用性，对于相关领域的研究者和从业人员具有一定的参考和借鉴意义。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-46a505fa4b2507a447461e4be7fc391d.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-2973cbb3e36d49ef1f3e15f1a0f4b9f1.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-2db9d9f5f928ad1d410198eae8af56b9.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-eae97248119c175e5de4631c7bd39e08.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-9a55f7407de4159e931c08bc20ba1e01.jpg" align="middle"></details><h2 id="Gaussian-Deja-vu-Creating-Controllable-3D-Gaussian-Head-Avatars-with-Enhanced-Generalization-and-Personalization-Abilities"><a href="#Gaussian-Deja-vu-Creating-Controllable-3D-Gaussian-Head-Avatars-with-Enhanced-Generalization-and-Personalization-Abilities" class="headerlink" title="Gaussian Déjà-vu: Creating Controllable 3D Gaussian Head-Avatars   with Enhanced Generalization and Personalization Abilities"></a>Gaussian Déjà-vu: Creating Controllable 3D Gaussian Head-Avatars with Enhanced Generalization and Personalization Abilities</h2><p><strong>Authors:Peizhi Yan, Rabab Ward, Qiang Tang, Shan Du</strong></p><p>Recent advancements in 3D Gaussian Splatting (3DGS) have unlocked significant potential for modeling 3D head avatars, providing greater flexibility than mesh-based methods and more efficient rendering compared to NeRF-based approaches. Despite these advancements, the creation of controllable 3DGS-based head avatars remains time-intensive, often requiring tens of minutes to hours. To expedite this process, we here introduce the <code>`Gaussian D\'ej\</code>a-vu” framework, which first obtains a generalized model of the head avatar and then personalizes the result. The generalized model is trained on large 2D (synthetic and real) image datasets. This model provides a well-initialized 3D Gaussian head that is further refined using a monocular video to achieve the personalized head avatar. For personalizing, we propose learnable expression-aware rectification blendmaps to correct the initial 3D Gaussians, ensuring rapid convergence without the reliance on neural networks. Experiments demonstrate that the proposed method meets its objectives. It outperforms state-of-the-art 3D Gaussian head avatars in terms of photorealistic quality as well as reduces training time consumption to at least a quarter of the existing methods, producing the avatar in minutes.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.16147v1">PDF</a> 11 pages, Accepted by WACV 2025 in Round 1</p><p><strong>Summary</strong><br>3DGS头像建模技术升级，提出“Gaussian D\’ej`a-vu”框架，缩短个性化建模时间。</p><p><strong>Key Takeaways</strong></p><ol><li>3DGS技术提升3D头像建模灵活性，渲染效率高。</li><li>创建3DGS头像需耗时，新框架旨在加速此过程。</li><li>框架包括通用模型训练和个性化定制。</li><li>通用模型基于大型2D图像数据集训练。</li><li>个性化定制通过单目视频实现，优化3D Gaussians。</li><li>使用可学习的表达感知混合图校正，提高收敛速度。</li><li>新方法在真实感质量和训练时间上优于现有技术。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: 高斯戴贾维：创建可控的3D高斯头部化身的方法<br>Abstract: 该论文提出了一种创建可控的3D高斯头部化身的方法，通过训练一个重建模型在大型人脸图像数据集上获得通用模型，并将其用于初始化个性化的头部化身。该方法使用合成和真实图像数据集进行训练，并通过单目视频进一步细化得到个性化的头部化身。实验表明，该方法在达到目标的同时，在逼真度和训练时间消耗方面优于现有的最先进的3D高斯头化身技术。</p></li><li><p>Authors: PeiZhi Yan（皮志燕）, Rabab Ward（拉巴卜·沃德）, Qiang Tang（唐强）, Shan Du（单杜）等。</p></li><li><p>Affiliation: 隶属于英国哥伦比亚大学（Yan和Ward）以及华为加拿大研究中心（Tang）。Du来自英国哥伦比亚大学奥肯根校区。</p></li><li><p>Keywords: 3D Gaussian Head Avatar, Gaussian D´ej`a-vu, Controllable Avatars, 3D Face Reconstruction, Personalized Avatars。</p></li><li><p>Urls: 请查看原文提供的链接。关于GitHub代码链接，由于我无法直接访问GitHub或其他在线数据库来查找信息，所以无法提供具体的链接。如果论文中有提及具体的GitHub链接，请直接在论文中查找。</p></li><li><p>Summary:</p><ul><li>(1)研究背景：随着视频游戏、虚拟现实和增强现实、电影制作、远程出席等行业的快速发展，创建逼真的三维头部化身变得越来越重要。现有的方法虽然取得了一定的成果，但在效率、质量和可控性方面仍存在挑战。因此，本文提出了一种创建可控的3D高斯头部化身的新方法。</li><li>(2)过去的方法及其问题：现有的方法主要基于网格或NeRF技术创建三维头部化身。这些方法虽然可以实现一定程度的逼真度，但在效率、渲染速度和控制方面存在问题。此外，个性化头部化身创建通常需要大量的时间和计算资源。因此，需要一种新的方法来克服这些问题。</li><li>(3)研究方法：本文提出了高斯戴贾维（Gaussian Deja-vu）框架来创建可控的3D高斯头部化身。首先，通过训练一个重建模型在大型人脸图像数据集上获得通用模型。然后，使用合成和真实图像数据集进行训练，并通过单目视频进一步细化得到个性化的头部化身。为了个性化，本文提出了可学习的表情感知校正映射图（learnable expression-aware rectification blendmaps），用于纠正初始的3D高斯模型，确保快速收敛且不依赖神经网络。</li><li>(4)任务与性能：实验表明，该方法在创建逼真的三维头部化身方面表现出优异的性能，不仅提高了质量，而且大大减少了训练时间消耗。与传统方法相比，该方法的训练时间至少减少了四分之一，能够在几分钟内生成头部化身。这些成果支持了该方法的有效性。</li></ul></li></ol><p>希望以上内容符合您的要求。</p><ol><li><p>Methods:</p><ul><li><p>(1) 研究背景分析：随着视频游戏、虚拟现实和增强现实等行业的快速发展，创建逼真的三维头部化身变得越来越重要。现有的方法在效率、质量和可控性方面存在挑战。</p></li><li><p>(2) 数据准备：首先，研究团队使用大型人脸图像数据集训练了一个重建模型，获得了通用模型。接着，使用合成和真实图像数据集进行训练。</p></li><li><p>(3) 个性化头部化身创建：通过单目视频进一步细化，得到个性化的头部化身。为了实现个性化，研究团队提出了可学习的表情感知校正映射图（learnable expression-aware rectification blendmaps），用于纠正初始的3D高斯模型。</p></li><li><p>(4) 方法优化：该研究采用的高斯戴贾维（Gaussian Deja-vu）框架确保了快速收敛，并且不依赖神经网络，从而大大提高了创建个性化头部化身的效率。</p></li><li><p>(5) 实验验证：实验结果表明，该方法在创建逼真的三维头部化身方面表现出优异的性能，不仅提高了质量，而且大大减少了训练时间消耗。与传统方法相比，该方法的训练时间至少减少了四分之一，能够在几分钟内生成头部化身。</p></li></ul></li><li><p>Conclusion:</p><ul><li><p>(1) 这项工作的意义在于提出了一种创建可控的3D高斯头部化身的新方法，具有重要的应用价值。随着视频游戏、虚拟现实和增强现实等行业的快速发展，创建逼真的三维头部化身的需求越来越迫切。该研究提出的D´ej`a-vu框架能够基于单张图像进行三维重建，并通过学习和调整实现个性化表达，具有广泛的应用前景。</p></li><li><p>(2) 创新点：该研究提出了一种新的创建可控的3D高斯头部化身的方法，具有显著的创新性。与传统的三维头部化身创建方法相比，该研究采用了先进的深度学习技术，并结合图像合成和真实图像数据集进行训练，实现了较高的逼真度和训练效率。同时，该研究还提出了可学习的表情感知校正映射图（learnable expression-aware rectification blendmaps），用于纠正初始的3D高斯模型，确保了快速收敛且不依赖神经网络。性能：实验结果表明，该方法在创建逼真的三维头部化身方面表现出优异的性能，不仅提高了质量，而且大大减少了训练时间消耗。与传统方法相比，该方法的训练时间至少减少了四分之一。工作量：该研究的工作量较大，涉及到大量的数据准备、模型训练和实验验证等工作。但研究结果具有显著的成效，为后续的相关研究提供了有益的参考和启示。</p></li></ul></li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-802802d534cf5037688351f162caf1cf.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-41ce0c960b001c3433e8f53f14598019.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-6fcd3ef7a1064ac1787a3a9488d68df8.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-35ca8870fea42c6b9c3feb32de431d47.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-14cc411449649510fb55a247aa080e88.jpg" align="middle"></details><h2 id="Barbie-Text-to-Barbie-Style-3D-Avatars"><a href="#Barbie-Text-to-Barbie-Style-3D-Avatars" class="headerlink" title="Barbie: Text to Barbie-Style 3D Avatars"></a>Barbie: Text to Barbie-Style 3D Avatars</h2><p><strong>Authors:Xiaokun Sun, Zhenyu Zhang, Ying Tai, Qian Wang, Hao Tang, Zili Yi, Jian Yang</strong></p><p>Recent advances in text-guided 3D avatar generation have made substantial progress by distilling knowledge from diffusion models. Despite the plausible generated appearance, existing methods cannot achieve fine-grained disentanglement or high-fidelity modeling between inner body and outfit. In this paper, we propose Barbie, a novel framework for generating 3D avatars that can be dressed in diverse and high-quality Barbie-like garments and accessories. Instead of relying on a holistic model, Barbie achieves fine-grained disentanglement on avatars by semantic-aligned separated models for human body and outfits. These disentangled 3D representations are then optimized by different expert models to guarantee the domain-specific fidelity. To balance geometry diversity and reasonableness, we propose a series of losses for template-preserving and human-prior evolving. The final avatar is enhanced by unified texture refinement for superior texture consistency. Extensive experiments demonstrate that Barbie outperforms existing methods in both dressed human and outfit generation, supporting flexible apparel combination and animation. The code will be released for research purposes. Our project page is: <a target="_blank" rel="noopener" href="https://xiaokunsun.github.io/Barbie.github.io/">https://xiaokunsun.github.io/Barbie.github.io/</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.09126v4">PDF</a> 9 pages, 7 figures, Project page: <a target="_blank" rel="noopener" href="https://xiaokunsun.github.io/Barbie.github.io/">https://xiaokunsun.github.io/Barbie.github.io/</a></p><p><strong>Summary</strong><br>提出Barbie框架，实现精细解耦的3D虚拟人生成。</p><p><strong>Key Takeaways</strong></p><ol><li>文章提出Barbie框架，用于生成可穿戴多样化服装的3D虚拟人。</li><li>通过语义对齐的分离模型实现人体和服装的精细解耦。</li><li>采用不同专家模型优化解耦的3D表示，确保特定领域的高保真度。</li><li>设计一系列损失函数，平衡几何多样性和合理性。</li><li>统一纹理细化提升纹理一致性。</li><li>实验证明Barbie在服装组合和动画方面优于现有方法。</li><li>代码将公开发布，方便研究。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: 基于文本指导的Barbie风格3D虚拟人生成研究</p></li><li><p>Authors: Xiaokun Sun, Zhenyu Zhang, Ying Tai, Qian Wang, Hao Tang, Zili Yi, Jian Yang</p></li><li><p>Affiliation: 南京大学教授Sun Xiaokun等</p></li><li><p>Keywords: Text-to-Avatar Generation; 3D Avatar; Text-Guided; Fine-grained Disentanglement; Domain-Specific Fidelity</p></li><li><p>Urls: <a target="_blank" rel="noopener" href="https://xiaokunsun.github.io/Barbie.github.io/；Github">https://xiaokunsun.github.io/Barbie.github.io/；Github</a>: None</p></li><li><p>Summary:</p></li></ol><p>(1) 研究背景：近年来，随着AR/VR技术的普及，创建3D数字人类引起了广泛关注。自动生成3D虚拟人的方法需要大规模的三维人类数据用于训练，这极大地限制了其应用范围。得益于文本到图像和文本到3D领域的快速发展，利用自然语言输入进行虚拟人生成已成为越来越受欢迎的研究方向。</p><p>(2) 过去的方法及问题：现有的文本到虚拟人的工作大致分为两类：生成整体虚拟人和生成身体与服装的解耦模型。整体虚拟人的生成方法虽然可以实现较高的逼真度，但缺乏灵活性，无法自由更换服装或进行服装转移等应用。解耦方法的目的是将身体和衣物分别建模，但存在精细度不足、服装和配饰生成不真实等问题。</p><p>(3) 研究方法：本研究提出了一种新型的基于文本指导的Barbie风格3D虚拟人生成框架。该框架通过语义对齐的分离模型实现身体和服装的精细解耦。使用不同的专家模型对解耦后的3D表示进行优化，以保证领域特定的保真度。通过一系列损失函数平衡几何多样性和合理性，同时采用统一的纹理优化算法提高纹理一致性。</p><p>(4) 任务与性能：本研究的方法在着装人类生成和服装生成方面表现出优异的性能，支持灵活的服装组合和动画。实验结果表明，Barbie在几何多样性、纹理逼真度和细节精细度等方面均优于现有方法。性能结果支持该研究的目标，即生成具有高度逼真度、多样性和解耦度的Barbie风格3D虚拟人。</p><ol><li><p>方法论：</p><ul><li><p>(1) 研究背景：近年来，随着AR/VR技术的普及，创建3D数字人类引起了广泛关注。自动生成3D虚拟人的方法需要大规模的三维人类数据用于训练，这极大地限制了其应用范围。该研究提出了一种基于文本指导的Barbie风格3D虚拟人生成框架，旨在解决现有方法的问题。</p></li><li><p>(2) 研究方法：该研究采用了一种新型的基于文本指导的Barbie风格3D虚拟人生成框架。首先，利用SMPL-X参数化人体模型表示全身的形状、姿态和表情。然后，采用Score Distillation Sampling方法，借助预训练的T2I模型指导3D表示与输入文本对齐。此外，研究采用了DMTet混合表示法，能够高效表示隐式签名距离函数（SDF）和可微分的四面体层。</p></li><li><p>(3) 流程设计：研究流程分为三个关键阶段。第一阶段是生成高质量的人体，采用有针对性的专家扩散模型进行特定正则化，产生高质量和合理的人体（Sec. 3.3）。第二阶段是生成服装，采用对象特定的扩散模型进行纹理建模（Sec. 3.4）。最后是组成虚拟人的微调阶段，采用统一的纹理优化算法提高纹理的一致性（Sec. 3.5）。</p></li><li><p>(4) 技术细节：在人体生成方面，研究采用SMPL-X网格进行准确的初始输入，并采用可微分渲染器和SDS损失来优化形状参数β，根据输入的基本人体描述确定基本人体形状。在几何建模方面，研究利用人类特定的扩散模型进行几何优化，包括正常适应扩散模型、深度适应扩散模型和纹理创建模型。此外，研究还引入了一种自我进化的人类先验损失，通过周期性地适应网格Minit来平衡生成的多样性和合理性。在纹理建模方面，利用正常对齐的扩散模型创建真实和高质量的纹理。</p></li><li><p>(5) 创新点：该研究的主要创新在于实现了身体和服装的精细解耦，通过领域特定的保真度优化和统一的纹理优化算法，生成具有高度逼真度、多样性和解耦度的Barbie风格3D虚拟人。同时，该研究的方法在着装人类生成和服装生成方面表现出优异的性能，支持灵活的服装组合和动画。</p><p>总的来说，该研究的方法为创建高度逼真、多样且解耦的3D虚拟人提供了一种有效的解决方案。</p></li></ul></li><li>Conclusion:</li></ol><ul><li><p>(1) 这项工作的意义在于提出了一种基于文本指导的Barbie风格3D虚拟人生成方法，具有广泛的应用前景。它能够根据自然语言输入生成具有高度逼真度、多样性和解耦度的虚拟人，为创建个性化的虚拟角色提供了新的可能性。同时，该方法还展示了在服装生成和组合方面的优异性能，为虚拟时尚、虚拟世界等领域的开发提供了有力支持。</p></li><li><p>(2) 创新点：该文章的创新之处在于实现了身体和服装的精细解耦，通过领域特定的保真度优化和统一的纹理优化算法，生成了具有高度逼真度、多样性和解耦度的Barbie风格3D虚拟人。其技术细节中的SMPL-X参数化人体模型表示、Score Distillation Sampling方法、DMTet混合表示法等均体现了作者的技术水平与创新思维。但现有的工作可能仍然存在对于复杂纹理和细节的处理不够完善的问题，未来的研究可以进一步探索如何进一步提高生成虚拟人的逼真度和细节质量。性能上，该文章的方法在几何多样性、纹理逼真度和细节精细度等方面均优于现有方法，体现了其优越的性能表现。工作量上，该文章对方法论进行了详细的阐述和实验验证，展示了作者们丰富的工作量和技术积累。</p></li></ul><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-9fe2afd4718a4a603a9059c758303dbc.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-82aef8d8f1aed2ceef69e20d1f2aeaca.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-d05a0aab7c3ee1cb21c6111b8ce45bf2.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-10380f66381cdb3f0d26a35da5d2c482.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-a66b9f1c3e5e087c1b363bb26b124d4e.jpg" align="middle"></details></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io/2024/09/27/Paper/2024-09-27/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">https://kedreamix.github.io/2024/09/27/Paper/2024-09-27/元宇宙_虚拟人/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">元宇宙/虚拟人</a></div><div class="post_share"><div class="social-share" data-image="https://pic1.zhimg.com/v2-41ce0c960b001c3433e8f53f14598019.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/09/27/Paper/2024-09-27/Talking%20Head%20Generation/" title="Talking Head Generation"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-b0cf5c7e6a853321218751ea3fc0a113.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Talking Head Generation</div></div></a></div><div class="next-post pull-right"><a href="/2024/09/24/Paper/2024-09-24/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-7611346411aa2d885ee691080836d8c3.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Diffusion Models</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/01/24/Paper/2024-01-24/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-0f13a2b60bef4c886a3317754c99b456.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-24</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/04/01/Paper/2024-04-01/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-4f2a829065c463be027e4b423c4e43c8.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-01</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/04/06/Paper/2024-04-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-4f97970c093585e18e2db42fb96a6b75.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-06</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/05/02/Paper/2024-05-02/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-37516691b13dabbabb3b74ea46b402d8.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-02</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/05/06/Paper/2024-05-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-65e21e3a0a320adc36f81e6bfc7c5739.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-06</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/05/22/Paper/2024-05-22/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-b4943be56cf94e02149b50eec541d27f.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-22</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-09-27-%E6%9B%B4%E6%96%B0"><span class="toc-text">2024-09-27 更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#DreamWaltz-G-Expressive-3D-Gaussian-Avatars-from-Skeleton-Guided-2D-Diffusion"><span class="toc-text">DreamWaltz-G: Expressive 3D Gaussian Avatars from Skeleton-Guided 2D Diffusion</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Gaussian-Deja-vu-Creating-Controllable-3D-Gaussian-Head-Avatars-with-Enhanced-Generalization-and-Personalization-Abilities"><span class="toc-text">Gaussian Déjà-vu: Creating Controllable 3D Gaussian Head-Avatars with Enhanced Generalization and Personalization Abilities</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Barbie-Text-to-Barbie-Style-3D-Avatars"><span class="toc-text">Barbie: Text to Barbie-Style 3D Avatars</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image:url('https://pic1.zhimg.com/v2-41ce0c960b001c3433e8f53f14598019.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js").then((()=>{pangu.autoSpacingPage()}))}function panguInit(){panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.1},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script></div><script id="canvas_nest" defer color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><script data-pjax>function butterfly_swiper_injector_config(){var a=document.getElementById("recent-posts");console.log("已挂载butterfly_swiper"),a.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/25/Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="2024/01/25/Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="2024/01/25/Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/17/Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023/12/17/Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="2023/12/17/Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script></body></html>