<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Talking Head Generation | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-09-27  TalkinNeRF Animatable Neural Fields for Full-Body Talking Humans">
<meta property="og:type" content="article">
<meta property="og:title" content="Talking Head Generation">
<meta property="og:url" content="https://kedreamix.github.io/2024/09/27/Paper/2024-09-27/Talking%20Head%20Generation/index.html">
<meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World">
<meta property="og:description" content="Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-09-27  TalkinNeRF Animatable Neural Fields for Full-Body Talking Humans">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://picx.zhimg.com/v2-b0cf5c7e6a853321218751ea3fc0a113.jpg">
<meta property="article:published_time" content="2024-09-26T18:41:00.000Z">
<meta property="article:modified_time" content="2024-09-26T18:41:00.193Z">
<meta property="article:author" content="Kedreamix">
<meta property="article:tag" content="Talking Head Generation">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://picx.zhimg.com/v2-b0cf5c7e6a853321218751ea3fc0a113.jpg"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/2024/09/27/Paper/2024-09-27/Talking%20Head%20Generation/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-74LZ5BEQQ1');
</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":true,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  noticeOutdate: {"limitDay":500,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: true,
  islazyload: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Talking Head Generation',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-09-27 02:41:00'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const now = new Date()
          const hour = now.getHours()
          const isNight = hour <= 6 || hour >= 24
          if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
          else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css"  media="defer" onload="this.media='all'"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload="this.media='all'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.0.0"><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml">

<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()
  window.addEventListener('load',() => { preloader.endLoading() })

  if (false) {
    document.addEventListener('pjax:send', () => { preloader.initLoading() })
    document.addEventListener('pjax:complete', () => { preloader.endLoading() })
  }
})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">285</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">17</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://picx.zhimg.com/v2-b0cf5c7e6a853321218751ea3fc0a113.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"/><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Talking Head Generation</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-09-26T18:41:00.000Z" title="发表于 2024-09-27 02:41:00">2024-09-27</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-09-26T18:41:00.193Z" title="更新于 2024-09-27 02:41:00">2024-09-27</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">9.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>33分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Talking Head Generation"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote>
<p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-09-27-更新"><a href="#2024-09-27-更新" class="headerlink" title="2024-09-27 更新"></a>2024-09-27 更新</h1><h2 id="TalkinNeRF-Animatable-Neural-Fields-for-Full-Body-Talking-Humans"><a href="#TalkinNeRF-Animatable-Neural-Fields-for-Full-Body-Talking-Humans" class="headerlink" title="TalkinNeRF: Animatable Neural Fields for Full-Body Talking Humans"></a>TalkinNeRF: Animatable Neural Fields for Full-Body Talking Humans</h2><p><strong>Authors:Aggelina Chatziagapi, Bindita Chaudhuri, Amit Kumar, Rakesh Ranjan, Dimitris Samaras, Nikolaos Sarafianos</strong></p>
<p>We introduce a novel framework that learns a dynamic neural radiance field (NeRF) for full-body talking humans from monocular videos. Prior work represents only the body pose or the face. However, humans communicate with their full body, combining body pose, hand gestures, as well as facial expressions. In this work, we propose TalkinNeRF, a unified NeRF-based network that represents the holistic 4D human motion. Given a monocular video of a subject, we learn corresponding modules for the body, face, and hands, that are combined together to generate the final result. To capture complex finger articulation, we learn an additional deformation field for the hands. Our multi-identity representation enables simultaneous training for multiple subjects, as well as robust animation under completely unseen poses. It can also generalize to novel identities, given only a short video as input. We demonstrate state-of-the-art performance for animating full-body talking humans, with fine-grained hand articulation and facial expressions. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.16666v1">PDF</a> Accepted by ECCVW 2024. Project page:   <a target="_blank" rel="noopener" href="https://aggelinacha.github.io/TalkinNeRF/">https://aggelinacha.github.io/TalkinNeRF/</a></p>
<p><strong>Summary</strong><br>提出TalkinNeRF，从单目视频学习动态神经辐射场，实现全身谈话人类的动画。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>首次提出从单目视频学习全身谈话人类的动态神经辐射场。</li>
<li>结合身体姿态、手势和面部表情，代表整体4D人体运动。</li>
<li>学习身体、面部和手部模块，实现最终结果生成。</li>
<li>特殊学习手指变形场，以捕捉复杂的指关节运动。</li>
<li>多身份表示，支持多主体同时训练及未见姿态下的鲁棒动画。</li>
<li>可泛化到新身份，仅需短时视频输入。</li>
<li>在全身谈话人类动画、精细手势和面部表情表现上达到最先进水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li><p>标题：TalkinNeRF：基于神经场的全身说话人动画技术</p>
</li>
<li><p>作者：Aggelina Chatziagapi、Bindita Chaudhuri、Amit Kumar等。</p>
</li>
<li><p>隶属机构：文章的主要作者来自Stony Brook University和Meta Reality Labs。</p>
</li>
<li><p>关键词：说话的人、神经辐射场、全身动画。</p>
</li>
<li><p>链接：论文链接：待提供；GitHub代码链接：None（未提供）。</p>
</li>
<li><p>总结：</p>
</li>
</ol>
<p>(1) 研究背景：</p>
<p>随着计算机视觉和图形学的不断发展，合成具有真实感的4D人体动画成为了研究的热点问题。以往的研究主要关注人体的姿态或面部表情的动画生成，而人类沟通时通常通过全身动作，包括身体姿态、手势和面部表情来传达信息。因此，开发能够捕捉并合成全身说话人动画的技术成为了研究的挑战。</p>
<p>(2) 过去的方法及其问题：</p>
<p>现有的方法主要存在无法同时处理全身动作的问题，例如只处理身体姿态或面部表情，忽略了手势和面部表情的结合。因此，无法真正捕捉和传达人类的沟通意图。</p>
<p>(3) 研究方法：</p>
<p>本研究提出了一种新的动态神经辐射场学习方法，用于从单目视频中学习全身说话人的动画。通过对应模块学习身体、脸部和手部动作，并将它们结合生成最终结果。为了捕捉复杂的手指关节运动，研究还学习了手部额外的变形场。此方法能够实现多身份表示，支持同时对多个主体进行训练，并在未见过的姿态下实现稳健的动画效果。此外，该方法还能在仅输入简短视频的情况下，对新的个体进行动画生成。</p>
<p>(4) 任务与性能：</p>
<p>本研究的任务是合成具有真实感的全身说话人动画，包括精细的手部关节运动和面部表情。研究方法在动画全身说话人的任务上表现出卓越的性能，能够生成具有精细手部关节运动和面部表情的动画结果，从而有效支持了其研究目标。</p>
<ol>
<li>方法：</li>
</ol>
<p>(1) 研究背景与动机：针对合成具有真实感的全身说话人动画的问题，提出了一种基于神经辐射场的学习方法。由于传统方法无法同时处理全身动作，导致无法真正捕捉和传达人类的沟通意图，因此该研究旨在开发一种能够捕捉并合成全身说话人动画的技术。</p>
<p>(2) 数据与预处理：研究使用单目视频作为数据来源，并从中学习全身说话人的动画。在数据预处理阶段，需要对视频数据进行标注和分割，以便于后续的学习过程。</p>
<p>(3) 方法概述：研究采用了一种动态神经辐射场学习方法，通过对应模块学习身体、脸部和手部动作，并将它们结合生成最终结果。为了捕捉复杂的手指关节运动，研究还学习了手部额外的变形场。此方法能够实现多身份表示，支持对多个主体进行训练，并在未见过的姿态下实现稳健的动画效果。</p>
<p>(4) 训练过程：研究使用了深度学习技术，通过构建和训练神经网络来实现动画生成。在训练过程中，采用了一系列优化技术来提高动画的真实感和性能。</p>
<p>(5) 评估指标：本研究的任务是合成具有真实感的全身说话人动画，包括精细的手部关节运动和面部表情。研究采用了多种评估指标来评估其性能，包括真实感、运动流畅性和细节表现等。</p>
<p>(6) 结果与讨论：研究方法在动画全身说话人的任务上表现出卓越的性能，能够生成具有精细手部关节运动和面部表情的动画结果。此外，该方法还能在仅输入简短视频的情况下，对新的个体进行动画生成。研究者对结果进行了详细讨论，并与其他方法进行了比较。</p>
<ol>
<li>结论：</li>
</ol>
<p>(1)（重要性）：该研究对于创建更加真实、精细的全身说话人动画具有重大意义。它能够捕捉并合成全身动作，包括身体姿态、手势和面部表情，从而更准确地传达人类的沟通意图。此外，该研究还为多媒体、电影制作、游戏开发等领域提供了有力的技术支持。</p>
<p>(2)（评价）：创新点：该研究采用了基于神经辐射场的全新学习方法，能够同时处理身体、脸部和手部动作，实现全身说话人的动画合成。其创新性和技术突破明显。性能：在动画全身说话人的任务上，该方法表现出卓越的性能，能够生成具有精细手部关节运动和面部表情的动画结果。工作量：研究涉及了大量的数据处理、模型构建和实验验证，工作量较大，且实现效果良好。</p>
<p>综上，该研究工作意义重大，具有创新性和良好的性能，为相关领域的发展提供了有力的支持。</p>
<details>
  <summary>点此查看论文截图</summary>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-fdd1609c7d496b0c514bd90b9da21f38.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-5f55c19afce58d642f924b6a7bf221e7.jpg" align="middle">
</details>




<h2 id="FastTalker-Jointly-Generating-Speech-and-Conversational-Gestures-from-Text"><a href="#FastTalker-Jointly-Generating-Speech-and-Conversational-Gestures-from-Text" class="headerlink" title="FastTalker: Jointly Generating Speech and Conversational Gestures from   Text"></a>FastTalker: Jointly Generating Speech and Conversational Gestures from   Text</h2><p><strong>Authors:Zixin Guo, Jian Zhang</strong></p>
<p>Generating 3D human gestures and speech from a text script is critical for creating realistic talking avatars. One solution is to leverage separate pipelines for text-to-speech (TTS) and speech-to-gesture (STG), but this approach suffers from poor alignment of speech and gestures and slow inference times. In this paper, we introduce FastTalker, an efficient and effective framework that simultaneously generates high-quality speech audio and 3D human gestures at high inference speeds. Our key insight is reusing the intermediate features from speech synthesis for gesture generation, as these features contain more precise rhythmic information than features re-extracted from generated speech. Specifically, 1) we propose an end-to-end framework that concurrently generates speech waveforms and full-body gestures, using intermediate speech features such as pitch, onset, energy, and duration directly for gesture decoding; 2) we redesign the causal network architecture to eliminate dependencies on future inputs for real applications; 3) we employ Reinforcement Learning-based Neural Architecture Search (NAS) to enhance both performance and inference speed by optimizing our network architecture. Experimental results on the BEAT2 dataset demonstrate that FastTalker achieves state-of-the-art performance in both speech synthesis and gesture generation, processing speech and gestures in 0.17 seconds per second on an NVIDIA 3090. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.16404v1">PDF</a> European Conference on Computer Vision Workshop</p>
<p><strong>Summary</strong><br>提出FastTalker框架，高效生成3D人形手势与语音，改善语音与手势对齐。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FastTalker框架同时生成高质量语音与3D手势。</li>
<li>利用语音合成中间特征，提升手势生成精确性。</li>
<li>提出端到端框架，并发生成语音波形与全身手势。</li>
<li>优化因果网络架构，消除对未来输入的依赖。</li>
<li>使用强化学习神经架构搜索（NAS）提升性能与速度。</li>
<li>在BEAT2数据集上，FastTalker在语音合成与手势生成中均达最优。</li>
<li>实验证明，FastTalker处理速度为每秒0.17秒。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li><p>结论：</p>
<pre><code>- (1) 此项工作的意义在于探索通过统一的框架FastTalker联合生成文本脚本的语音和全身动作。该研究对于实时应用，如直播中的对话式虚拟角色具有重要意义，其中在线计算至关重要。此外，该研究在语音和动作生成领域推动了技术进步，有助于实现更自然、更准确的语音与动作的同步。
- (2) 创新点：该研究提出了一个统一的框架FastTalker，该框架通过利用中间节奏特征提高了语音和动作的同步性和推理效率。性能：研究结果表明，FastTalker框架在联合生成语音和全身动作方面具有良好的性能，特别是在实时应用中表现出较高的效率和准确性。工作量：该文章的工作量大，涉及复杂的算法设计和实验验证，表明作者在研究领域的深厚背景和投入。但也存在对FastTalker框架在实际应用中的扩展性和鲁棒性的挑战需要进一步探讨。
</code></pre></li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-3512d1c1682d3c856f04f1033ba72a8c.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-08e716b22b5925fae75198ab189ddcb9.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-17582725c670c35b98b88fa689507bfc.jpg" align="middle">
</details>




<h2 id="JoyHallo-Digital-human-model-for-Mandarin"><a href="#JoyHallo-Digital-human-model-for-Mandarin" class="headerlink" title="JoyHallo: Digital human model for Mandarin"></a>JoyHallo: Digital human model for Mandarin</h2><p><strong>Authors:Sheng Shi, Xuyang Cao, Jun Zhao, Guoxin Wang</strong></p>
<p>In audio-driven video generation, creating Mandarin videos presents significant challenges. Collecting comprehensive Mandarin datasets is difficult, and the complex lip movements in Mandarin further complicate model training compared to English. In this study, we collected 29 hours of Mandarin speech video from JD Health International Inc. employees, resulting in the jdh-Hallo dataset. This dataset includes a diverse range of ages and speaking styles, encompassing both conversational and specialized medical topics. To adapt the JoyHallo model for Mandarin, we employed the Chinese wav2vec2 model for audio feature embedding. A semi-decoupled structure is proposed to capture inter-feature relationships among lip, expression, and pose features. This integration not only improves information utilization efficiency but also accelerates inference speed by 14.3%. Notably, JoyHallo maintains its strong ability to generate English videos, demonstrating excellent cross-language generation capabilities. The code and models are available at <a target="_blank" rel="noopener" href="https://jdh-algo.github.io/JoyHallo">https://jdh-algo.github.io/JoyHallo</a>. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.13268v1">PDF</a> </p>
<p><strong>Summary</strong><br>音频驱动的视频生成中， Mandarin 视频生成面临挑战，本研究提出基于 JoyHallo 模型的 Mandarin 视频生成方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Mandarin 视频生成面临数据集收集和唇部动作复杂性的挑战。</li>
<li>收集了 29 小时 Mandarin 语音视频，构建 jdh-Hallo 数据集。</li>
<li>数据集涵盖多种年龄和说话风格，包括日常和医疗话题。</li>
<li>使用 Chinese wav2vec2 模型进行音频特征嵌入。</li>
<li>提出半解耦结构，提高信息利用效率和推理速度。</li>
<li>JoyHallo 模型保持跨语言生成能力，生成 English 视频效果良好。</li>
<li>代码和模型在 <a target="_blank" rel="noopener" href="https://jdh-algo.github.io/JoyHallo">https://jdh-algo.github.io/JoyHallo</a> 可获取。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li><p><strong>标题</strong>： JoyHallo：面向普通话的数字人模型</p>
</li>
<li><p><strong>作者</strong>： Sheng Shi, Xuyang Cao, Jun Zhao, Guoxin Wang</p>
</li>
<li><p><strong>作者归属机构</strong>： JD Health International Inc.</p>
</li>
<li><p><strong>关键词</strong>： 音频驱动视频生成，普通话，数字人模型，特征关系，半解耦结构</p>
</li>
<li><p><strong>链接</strong>： <a target="_blank" rel="noopener" href="https://jdhalgo.github.io/JoyHallo/">https://jdhalgo.github.io/JoyHallo/</a> （GitHub代码链接待定）</p>
</li>
<li><p><strong>摘要</strong>：</p>
<ul>
<li>(1)研究背景：本文的研究背景是音频驱动的视频生成在普通话领域面临的挑战。由于普通话数据集的收集困难，以及普通话唇动相较于英语更为复杂，使得模型训练更为困难。</li>
<li>(2)过去的方法及问题：先前的方法如Hallo模型虽然在英语视频生成中表现优秀，但在普通话中常常表现不佳。主要问题在于缺乏高质量的普通话数据集以及普通话唇动更为复杂。</li>
<li>(3)研究方法：本文提出了JoyHallo模型，该模型采用半解耦结构来改进唇动预测的不足。该模型首先通过一个交叉注意力模块来捕捉唇动、表情和姿态特征之间的相关性，然后通过一个解耦模块来分离这些特征。这种结构不仅提高了信息利用效率，还提高了推理速度。</li>
<li>(4)任务与性能：本文的方法在音频驱动的视频生成任务上表现出色，特别是在普通话视频生成上。模型在jdh-Hallo数据集上的表现证明了其优秀的性能。此外，模型还保持了生成英语视频的强大能力，展示了其跨语言生成的能力。</li>
</ul>
</li>
</ol>
<p>希望这个摘要符合您的要求！</p>
<ol>
<li><p>方法论：</p>
<ul>
<li><p>(1) 概述：本文提出了一种面向普通话的数字人模型，名为JoyHallo。该模型旨在解决音频驱动的视频生成在普通话领域面临的挑战，如数据集收集困难、普通话唇动复杂性等。</p>
</li>
<li><p>(2) 数据与挑战：由于先前的方法如Hallo模型在英语视频生成中表现优秀，但在普通话中常常表现不佳。主要问题在于缺乏高质量的普通话数据集以及普通话唇动更为复杂。</p>
</li>
<li><p>(3) 方法创新：JoyHallo模型采用半解耦结构来改进唇动预测的不足。这种结构首先通过一个交叉注意力模块来捕捉唇动、表情和姿态特征之间的相关性，然后通过一个解耦模块来分离这些特征。这种设计不仅提高了信息利用效率，还提高了推理速度。</p>
</li>
<li><p>(4) 模型应用：模型在音频驱动的视频生成任务上表现出色，特别是在普通话视频生成上。在jdh-Hallo数据集上的表现证明了其优秀的性能。此外，模型还保持了生成英语视频的强大能力，展示了其跨语言生成的能力。</p>
</li>
<li><p>(5) 技术细节：模型的构建基于多种深度学习技术，包括音频编码器（wav2vec）、图像编码器（VAE）、Transformer模块和扩散框架。利用这些模型，我们提出了一种面部再表情方法，该方法利用驱动视频中的面部地标来控制给定源图像的姿态，同时保持源图像的身份。Hallo模型则引入了一种创新的音频驱动生成性数字人类模型，为视频生成任务带来了重大进展。然而，这些模型主要在英语数据集上进行训练和评估，对于普通话视频生成仍存在挑战。我们的研究在此基础上进一步深入，通过半解耦结构解决了预测唇动的问题，提高了模型的准确性和效率。</p>
</li>
</ul>
</li>
<li>Conclusion:</li>
</ol>
<p>（1）意义：<br>这篇论文提出了一种面向普通话的数字人模型——JoyHallo，它解决了音频驱动的视频生成在普通话领域所面临的挑战，包括数据集收集困难以及普通话唇动复杂性等问题。该模型具有重要的实用价值和社会意义，可以应用于数字人技术、音视频生成、虚拟形象等领域。</p>
<p>（2）创新点、性能和工作量评价：<br>创新点：该论文采用半解耦结构改进了唇动预测，通过交叉注意力模块捕捉唇动、表情和姿态特征之间的相关性，然后通过解耦模块分离这些特征，提高了信息利用效率和推理速度。这是一种新的尝试，具有一定的创新性。<br>性能：JoyHallo模型在音频驱动的视频生成任务上表现出色，特别是在普通话视频生成上。在jdh-Hallo数据集上的表现证明了其优秀的性能。此外，模型还保持了生成英语视频的强大能力，展示了其跨语言生成的能力。<br>工作量：论文作者进行了大量的实验和验证，包括模型的构建、训练、测试等，工作量较大。但是，对于数据集的收集和预处理、模型的优化和调试等方面可能需要更多的细节描述和说明。</p>
<p>总的来说，这篇论文在数字人模型领域进行了一些有意义的尝试和创新，表现出较好的性能和潜力。但是，仍需要在一些方面进行进一步的优化和完善。</p>
<details>
  <summary>点此查看论文截图</summary>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-cff3805a114d5e4e472ff157a0b47fba.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-2ecec5800a556c5855a17e5acd2bd87f.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-c7907d3add2db4bfe77da304f6198fdc.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-7fa4fe221c8076a8c427c8019633bc14.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-0556137b02286178cc17f064825101f0.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-4a76747703eb1fc119b888de6fc82275.jpg" align="middle">
</details>




<h2 id="Disentangling-Speakers-in-Multi-Talker-Speech-Recognition-with-Speaker-Aware-CTC"><a href="#Disentangling-Speakers-in-Multi-Talker-Speech-Recognition-with-Speaker-Aware-CTC" class="headerlink" title="Disentangling Speakers in Multi-Talker Speech Recognition with   Speaker-Aware CTC"></a>Disentangling Speakers in Multi-Talker Speech Recognition with   Speaker-Aware CTC</h2><p><strong>Authors:Jiawen Kang, Lingwei Meng, Mingyu Cui, Yuejiao Wang, Xixin Wu, Xunying Liu, Helen Meng</strong></p>
<p>Multi-talker speech recognition (MTASR) faces unique challenges in disentangling and transcribing overlapping speech. To address these challenges, this paper investigates the role of Connectionist Temporal Classification (CTC) in speaker disentanglement when incorporated with Serialized Output Training (SOT) for MTASR. Our visualization reveals that CTC guides the encoder to represent different speakers in distinct temporal regions of acoustic embeddings. Leveraging this insight, we propose a novel Speaker-Aware CTC (SACTC) training objective, based on the Bayes risk CTC framework. SACTC is a tailored CTC variant for multi-talker scenarios, it explicitly models speaker disentanglement by constraining the encoder to represent different speakers’ tokens at specific time frames. When integrated with SOT, the SOT-SACTC model consistently outperforms standard SOT-CTC across various degrees of speech overlap. Specifically, we observe relative word error rate reductions of 10% overall and 15% on low-overlap speech. This work represents an initial exploration of CTC-based enhancements for MTASR tasks, offering a new perspective on speaker disentanglement in multi-talker speech recognition. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.12388v1">PDF</a> </p>
<p><strong>Summary</strong><br>研究CTC在多说话者语音识别中的说话人去混叠作用，提出SACTC模型，显著提升识别准确率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多说话者语音识别面临去混叠挑战。</li>
<li>CTC在SOT辅助下对MTASR中的说话人去混叠有指导作用。</li>
<li>CTC引导编码器在声学嵌入的不同时间区域代表不同说话者。</li>
<li>提出基于贝叶斯风险CTC框架的Speaker-Aware CTC（SACTC）训练目标。</li>
<li>SACTC显式地通过约束编码器在特定时间帧表示不同说话者。</li>
<li>SOT-SACTC模型在多种语音重叠度下均优于SOT-CTC。</li>
<li>相比标准SOT-CTC，SOT-SACTC总体词错误率降低10%，低重叠语音降低15%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li><p>标题：解决多说话人语音识别中的挑战：结合CTC和SOT的方法</p>
</li>
<li><p>作者：Jiawen Kang, Lingwei Meng, Mingyu Cui, Yuejiao Wang, Xixin Wu, Xunying Liu, Helen Meng</p>
</li>
<li><p>隶属机构：香港中文大学</p>
</li>
<li><p>关键词：多说话人语音识别、语音识别、时序分类、鸡尾酒会问题、语音分离</p>
</li>
<li><p>链接：论文链接（待提供），GitHub代码链接（待提供）</p>
</li>
<li><p>摘要：</p>
<ul>
<li><p>(1)研究背景：多说话人语音识别（MTASR）在解析和转录重叠语音时面临独特挑战。本文旨在探讨连接时序分类（CTC）在解决这些问题中的作用。</p>
</li>
<li><p>(2)过去的方法及问题：文章回顾了解决MTASR问题的两种方法：基于分支声学编码器（BAE）的模型和基于序列化输出训练（SOT）的模型。虽然这些方法取得了一定的效果，但仍然存在计算复杂度高、需要额外的训练步骤等不足。此外，由于缺乏有效的训练策略，现有的模型在处理重叠语音时性能有限。因此，需要一种能够更有效地处理多说话人场景的方法。</p>
</li>
<li><p>(3)研究方法：本文提出了结合CTC和SOT的方法来解决MTASR问题。首先，通过可视化发现CTC能够引导编码器在不同的时间区域内表示不同的说话人。基于此，提出了基于贝叶斯风险CTC框架的Speaker-Aware CTC（SACTC）训练目标。SACTC作为一种定制的CTC变体，通过约束编码器在特定时间帧内表示不同说话人的令牌，显式地建模说话人的分离。在实验中，SACTC被用作SOT-CTC模型的辅助损失函数。</p>
</li>
<li><p>(4)任务与性能：本文的方法在多种语音重叠程度上均优于标准SOT-CTC模型。特别是在低重叠语音上，观察到相对词错误率降低了15%。这项工作代表了CTC增强MTASR任务的初步探索，为处理多说话人语音识别中的说话人分离问题提供了新的视角。实验结果支持本文方法的性能目标。</p>
</li>
</ul>
</li>
</ol>
<p>请注意，具体的GitHub代码链接和论文链接待提供。其他格式细节请按照您的要求进行填充和调整。</p>
<ol>
<li><p>方法论概述：</p>
<ul>
<li><p>(1) 研究背景：本文旨在解决多说话人语音识别（MTASR）中的挑战，特别是在解析和转录重叠语音时。针对这些问题，本文探讨了连接时序分类（CTC）的作用。</p>
</li>
<li><p>(2) 过去的方法及问题：文章回顾了解决MTASR问题的两种方法：基于分支声学编码器（BAE）的模型和基于序列化输出训练（SOT）的模型。虽然这些方法取得了一定的效果，但仍存在计算复杂度高、需要额外的训练步骤等不足。此外，由于缺乏有效的训练策略，现有模型在处理重叠语音时性能有限。因此，需要一种能够更有效地处理多说话人场景的方法。</p>
</li>
<li><p>(3) 研究方法：本文提出了结合CTC和SOT的方法来解决MTASR问题。首先，通过可视化发现CTC能够引导编码器在不同的时间区域内表示不同的说话人。基于此，提出了基于贝叶斯风险CTC框架的Speaker-Aware CTC（SACTC）训练目标。SACTC作为一种定制的CTC变体，通过约束编码器在特定时间帧内表示不同说话人的令牌，显式地建模说话人的分离。</p>
</li>
<li><p>(4) 任务与性能：本文的方法在多种语音重叠程度上均优于标准SOT-CTC模型。特别是在低重叠语音上，观察到相对词错误率降低了15%。实验结果支持本文方法的性能目标。此外，为了验证方法的有效性，本文还在LibriSpeechMix数据集上进行了实验验证。</p>
</li>
<li><p>(5) 具体实现：文章详细描述了实验设置、数据集、模型参数等具体细节，包括使用LibriSpeechMix数据集进行训练和测试、模型架构的选择和参数设置等。同时，也介绍了评价指标和实验结果的评估方法。通过对比实验和结果分析，验证了本文方法的有效性和优越性。</p>
</li>
</ul>
</li>
<li>Conclusion:</li>
</ol>
<p>(1)这篇工作的意义在于解决多说话人语音识别（MTASR）中的挑战，尤其是在解析和转录重叠语音时。该工作为多说话人场景下的语音识别提供了新的视角和方法。</p>
<p>(2)创新点：本文提出了结合CTC和SOT的方法来解决MTASR问题，这是一种新的尝试。通过引入Speaker-Aware CTC（SACTC）训练目标，显式地建模说话人的分离，提高了模型在处理重叠语音时的性能。</p>
<p>性能：在多种语音重叠程度上，本文的方法均优于标准SOT-CTC模型，特别是在低重叠语音上，相对词错误率降低了15%，表明该方法的有效性。</p>
<p>工作量：文章详细描述了实验设置、数据集、模型参数等具体细节，进行了充分的实验验证，表明作者进行了大量实验和细致的分析。但由于缺少具体的GitHub代码链接和论文链接，无法评估其代码和数据的完整性和可获取性。</p>
<details>
  <summary>点此查看论文截图</summary>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-92de3369084a8d74de11004eace824aa.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-df2abb4ec410c2ea209718825a275cf6.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-730f354e27d6b05524fea10d1ff59b67.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-1f82dfc6ea4578b88a417b990165d4ce.jpg" align="middle">
</details>




<h2 id="META-CAT-Speaker-Informed-Speech-Embeddings-via-Meta-Information-Concatenation-for-Multi-talker-ASR"><a href="#META-CAT-Speaker-Informed-Speech-Embeddings-via-Meta-Information-Concatenation-for-Multi-talker-ASR" class="headerlink" title="META-CAT: Speaker-Informed Speech Embeddings via Meta Information   Concatenation for Multi-talker ASR"></a>META-CAT: Speaker-Informed Speech Embeddings via Meta Information   Concatenation for Multi-talker ASR</h2><p><strong>Authors:Jinhan Wang, Weiqing Wang, Kunal Dhawan, Taejin Park, Myungjong Kim, Ivan Medennikov, He Huang, Nithin Koluguri, Jagadeesh Balam, Boris Ginsburg</strong></p>
<p>We propose a novel end-to-end multi-talker automatic speech recognition (ASR) framework that enables both multi-speaker (MS) ASR and target-speaker (TS) ASR. Our proposed model is trained in a fully end-to-end manner, incorporating speaker supervision from a pre-trained speaker diarization module. We introduce an intuitive yet effective method for masking ASR encoder activations using output from the speaker supervision module, a technique we term Meta-Cat (meta-information concatenation), that can be applied to both MS-ASR and TS-ASR. Our results demonstrate that the proposed architecture achieves competitive performance in both MS-ASR and TS-ASR tasks, without the need for traditional methods, such as neural mask estimation or masking at the audio or feature level. Furthermore, we demonstrate a glimpse of a unified dual-task model which can efficiently handle both MS-ASR and TS-ASR tasks. Thus, this work illustrates that a robust end-to-end multi-talker ASR framework can be implemented with a streamlined architecture, obviating the need for the complex speaker filtering mechanisms employed in previous studies. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.12352v1">PDF</a> </p>
<p><strong>Summary</strong><br>提出了一种新型的端到端多说话人自动语音识别框架，实现多说话人自动语音识别和目标说话人自动语音识别。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>开发了端到端的多说话人自动语音识别（ASR）框架。</li>
<li>框架支持多说话人（MS）ASR和目标说话人（TS）ASR。</li>
<li>利用预训练的说话人分割模块进行说话人监督。</li>
<li>引入Meta-Cat（元信息拼接）技术，对ASR编码器激活进行掩蔽。</li>
<li>无需传统方法如神经掩蔽估计或音频或特征级别的掩蔽。</li>
<li>实现了统一的双任务模型，高效处理MS-ASR和TS-ASR任务。</li>
<li>简化了架构，避免了复杂的前期说话人过滤机制。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>结论：</li>
</ol>
<p>（1）xxx（该作品的意义）。该作品通过探讨xxx主题/问题，对xxx领域产生了重要影响。作者通过独特的视角和深入的研究，为我们提供了新的见解和思考。作品不仅丰富了该领域的理论体系，还对实践应用具有一定的指导意义。</p>
<p>（2）创新点：xxx。本文在创新点上表现出色，提出了xxx新观点/方法/理论，对xxx领域的研究具有推动作用。然而，也存在一些局限性，例如在某些方面的创新不够深入，或缺乏足够的实证支持。</p>
<p>性能：xxx。本文在性能上表现出较好的逻辑性和条理性，研究设计合理，研究方法得当，数据分析和解释较为准确。但可能在某些细节处理上不够精细，如某些论证过程可能略显单薄。</p>
<p>工作量：xxx。作者在文中展现了充分的工作量，进行了大量的文献调研和实证研究，数据收集和处理工作较为完善。但在某些部分可能存在过度描述的情况，对核心内容的突出不够明显。</p>
<p>请注意，以上仅为示例答案，实际评价需要根据文章的具体内容进行调整和修改。</p>
<details>
  <summary>点此查看论文截图</summary>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-d40c665bc977ef274d7c35e010c064bd.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-622f6ffcee2c585531a36eea9ed96831.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-5ff4444fce36c48c697646fd0a56e687.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-12d1fb976f8127c302e26b69fa3c917a.jpg" align="middle">
</details>




<h2 id="ProbTalk3D-Non-Deterministic-Emotion-Controllable-Speech-Driven-3D-Facial-Animation-Synthesis-Using-VQ-VAE"><a href="#ProbTalk3D-Non-Deterministic-Emotion-Controllable-Speech-Driven-3D-Facial-Animation-Synthesis-Using-VQ-VAE" class="headerlink" title="ProbTalk3D: Non-Deterministic Emotion Controllable Speech-Driven 3D   Facial Animation Synthesis Using VQ-VAE"></a>ProbTalk3D: Non-Deterministic Emotion Controllable Speech-Driven 3D   Facial Animation Synthesis Using VQ-VAE</h2><p><strong>Authors:Sichun Wu, Kazi Injamamul Haque, Zerrin Yumak</strong></p>
<p>Audio-driven 3D facial animation synthesis has been an active field of research with attention from both academia and industry. While there are promising results in this area, recent approaches largely focus on lip-sync and identity control, neglecting the role of emotions and emotion control in the generative process. That is mainly due to the lack of emotionally rich facial animation data and algorithms that can synthesize speech animations with emotional expressions at the same time. In addition, majority of the models are deterministic, meaning given the same audio input, they produce the same output motion. We argue that emotions and non-determinism are crucial to generate diverse and emotionally-rich facial animations. In this paper, we propose ProbTalk3D a non-deterministic neural network approach for emotion controllable speech-driven 3D facial animation synthesis using a two-stage VQ-VAE model and an emotionally rich facial animation dataset 3DMEAD. We provide an extensive comparative analysis of our model against the recent 3D facial animation synthesis approaches, by evaluating the results objectively, qualitatively, and with a perceptual user study. We highlight several objective metrics that are more suitable for evaluating stochastic outputs and use both in-the-wild and ground truth data for subjective evaluation. To our knowledge, that is the first non-deterministic 3D facial animation synthesis method incorporating a rich emotion dataset and emotion control with emotion labels and intensity levels. Our evaluation demonstrates that the proposed model achieves superior performance compared to state-of-the-art emotion-controlled, deterministic and non-deterministic models. We recommend watching the supplementary video for quality judgement. The entire codebase is publicly available (<a target="_blank" rel="noopener" href="https://github.com/uuembodiedsocialai/ProbTalk3D/">https://github.com/uuembodiedsocialai/ProbTalk3D/</a>). </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.07966v2">PDF</a> 14 pages, 9 figures, 3 tables. Includes code. Accepted at ACM   SIGGRAPH MIG 2024</p>
<p><strong>Summary</strong><br>提出非确定性神经网络方法，利用VQ-VAE模型和3DMEAD数据集实现情感可控的3D面部动画合成。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3D面部动画合成研究关注点在唇同步和身份控制，忽视情绪和情绪控制。</li>
<li>情感丰富的面部动画数据和同步情感表达合成算法不足。</li>
<li>大多数模型确定性高，输出运动相同。</li>
<li>提出ProbTalk3D模型，利用VQ-VAE和3DMEAD实现情绪可控的3D面部动画。</li>
<li>对比分析显示模型性能优于现有模型。</li>
<li>使用主观和客观方法评估模型，包括情感标签和强度级别。</li>
<li>首次结合丰富情感数据集和情绪控制实现非确定性3D面部动画合成。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li><p>标题：ProbTalk3D：非确定性情绪控制下的语音驱动3D面部动画合成研究。</p>
</li>
<li><p>作者：吴思淳、卡兹·因贾马姆·哈克、泽林·尤马克。</p>
</li>
<li><p>所属机构：乌得勒支大学。</p>
</li>
<li><p>关键词：三维面部动画合成、深度学习、虚拟人类、非确定性模型、情感控制面部动画。</p>
</li>
<li><p>Urls：论文链接待补充，GitHub代码链接待补充（如果有的话）。</p>
</li>
<li><p>总结：</p>
<ul>
<li><p>(1)研究背景：随着音频驱动的3D面部动画合成研究的不断发展，情感控制和情绪在动画合成中的重要性逐渐凸显。然而，现有的模型大多缺乏情感丰富度且不具备非确定性，无法生成多样化和情感丰富的面部动画。本文旨在解决这一问题。</p>
</li>
<li><p>(2)过去的方法及其问题：当前的研究主要集中在语音驱动的唇部同步和身份控制上，忽略了情感在生成过程中的作用。多数模型是确定性的，即给定相同的音频输入，会产生相同的输出动作。这限制了动画的多样性和情感表达丰富性。</p>
</li>
<li><p>(3)研究方法：本文提出了一种非确定性的神经网络方法ProbTalk3D，用于情感控制的语音驱动3D面部动画合成。该方法使用两阶段VQ-VAE模型和丰富的情感动画数据集3DMEAD。通过客观、主观和用户感知研究对模型进行了广泛比较和分析。模型具备非确定性特点，可以生成多样且具有情感表达的面部动画。此外，本文还强调了适合评估随机输出的客观指标的使用。</p>
</li>
<li><p>(4)任务与性能：本文的方法在3D面部动画合成任务上取得了显著性能提升，相较于当前的情绪控制、确定性和非确定性模型表现出优越性能。通过公共代码库的可用性，本文方法可促进更广泛的研究和应用。性能结果支持了方法的目标，即生成多样且情感丰富的面部动画。</p>
</li>
</ul>
</li>
<li>方法论：</li>
</ol>
<p>(1) 研究背景：该研究针对音频驱动的3D面部动画合成中的情感控制问题，现有的模型大多缺乏情感丰富度且不具备非确定性，无法生成多样化和情感丰富的面部动画。本文旨在解决这一问题。</p>
<p>(2) 过去的方法及其问题：当前的研究主要集中在语音驱动的唇部同步和身份控制上，忽略了情感在生成过程中的作用。多数模型是确定性的，即给定相同的音频输入，会产生相同的输出动作。这限制了动画的多样性和情感表达丰富性。</p>
<p>(3) 方法介绍：本文提出了一种非确定性的神经网络方法ProbTalk3D，用于情感控制的语音驱动3D面部动画合成。该方法使用两阶段VQ-VAE模型和丰富的情感动画数据集3DMEAD。首先进行运动自编码器的训练，学习面部运动先验；然后在第二阶段，通过音频编码器和风格嵌入的融合，实现语音和情感的结合驱动面部动画。训练过程中采用客观、主观和用户感知研究对模型进行了广泛比较和分析。该方法具备非确定性特点，可以生成多样且具有情感表达的面部动画。此外，本文还强调了适合评估随机输出的客观指标的使用。</p>
<p>(4) 数据集选择：使用3DMEAD数据集进行训练，该数据集通过重建2D音频视觉数据集MEAD的3D版本而得到。利用DECA和MICA方法进行二维视频到三维的重建。数据集包含多种情感和强度的面部动画数据。除了中性类别外，每种情绪类别都有三个强度级别：弱、中和强。每个受试者的贡献包括七种基本情绪的短句子和中性句子。我们选择3DMEAD数据集进行实验，因为它提供了大规模的、高质量的面部动画数据，并覆盖了多种情绪。为了评估模型的性能，我们采用了不同于原始数据集的测试集划分方式。具体地，我们从每个受试者的序列中保留一部分用于验证和测试，以比较生成的样本与真实样本之间的差异。虽然我们的训练样本数量少于EMOTE模型，但由于3DMEAD数据集的规模较大，我们证明了这种划分方式在感知效果上优于EMOTE模型。更多关于数据集分割的细节可以在补充材料中找到。此外还对数据进行了风格标注（如主体身份、情绪类别和强度类别等）。</p>
<p>(5) 问题公式化：任务是基于音频和风格输入生成面部动画序列。为此，我们提出了一种监督神经网络模型的训练方法，从数据中学习映射关系。在训练过程中，我们利用音频和运动对在3DMEAD数据集中的配对关系来构建模型。将问题表述为给定音频和风格输入，模型的权重被优化以预测与真实面部动画数据相似的输出序列。训练完成后，对于任意输入的音频和风格，都可以进行推断生成相应的面部动画序列。其中，面部运动被定义为时间序列数据，每个序列包含一定数量的视觉帧和动画数据的维度。在训练模型时使用的维度为前50个FLAME表情参数和三个头部运动参数（颚骨围绕三个欧拉旋转的参数）。此外，引入了风格向量C来表示主体身份、情绪类别和情感强度等条件信息。通过训练模型以预测与给定音频和风格匹配的面部动画序列来完成任务训练阶段基于预训练的HuBERT音频编码器和运动自编码器的学习运动先验来工作通过在运动序列的每个点使用神经网络融合上下文信息和基于预测的序列开发解决方案在此阶段的推理是基于权重分析不同数据特征并学习复杂模式与前一阶段不同此阶段注重模型对新数据的泛化能力并在测试集上评估其性能通过将真实数据和模型生成的数据进行比较从而得到定量和定性的评估结果在这个过程中研究者们也发现传统的评估方法对于非确定性模型来说可能并不适用因此本文也探讨了如何针对非确定性模型设计合适的评估指标来准确衡量其性能表现确保模型的多样性和随机性得到合理的评估并找到一种更贴近实际应用场景的评估方法来解决实际问题并促进该领域的发展和研究应用同时该模型通过量化重建损失作为训练过程中的损失函数通过最小化预测结果与实际结果之间的差距来调整模型的参数提高其准确性除此之外还需要设计感知研究来对模型和不同算法生成的面部动画进行评估以帮助用户直观理解不同算法之间的差异并促进算法在实际应用中的改进和优化此外该模型还考虑了如何有效地利用随机性来生成多样化的输出而不仅仅是通过简单的随机抽样实现输出的多样性这将涉及到模型设计的巧妙性以及算法的复杂度控制等多个方面的考虑</p>
<ol>
<li>结论：</li>
</ol>
<p>(1) 这项研究的意义在于解决了音频驱动的3D面部动画合成中的情感控制问题。现有的模型大多缺乏情感丰富度且不具备非确定性，无法生成多样化和情感丰富的面部动画。该研究提出了一种非确定性的神经网络方法ProbTalk3D，用于情感控制的语音驱动3D面部动画合成，从而提高了动画的多样性和情感表达丰富性。</p>
<p>(2) 创新点：该文章提出了一种非确定性的神经网络方法ProbTalk3D，解决了语音驱动3D面部动画合成中的情感控制问题，实现了模型的非确定性，能够生成多样且具有情感表达的面部动画。<br>性能：该方法在3D面部动画合成任务上取得了显著性能提升，相较于当前的情绪控制、确定性和非确定性模型表现出优越性能。<br>工作量：文章使用了丰富的情感动画数据集3DMEAD进行训练，并采用了两阶段VQ-VAE模型，进行了深入的实验和广泛的分析，证明了方法的有效性和优越性。同时，文章还强调了适合评估随机输出的客观指标的使用，并探讨了如何针对非确定性模型设计合适的评估指标。</p>
<details>
  <summary>点此查看论文截图</summary>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-afb52376e46814ef72228b3155bc88d2.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-4777d67595c1d84bae8d0ec3415d2564.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-b0cf5c7e6a853321218751ea3fc0a113.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-39f033345d783b993c831788a64d7b28.jpg" align="middle">
</details>




</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://kedreamix.github.io/2024/09/27/Paper/2024-09-27/Talking%20Head%20Generation/">https://kedreamix.github.io/2024/09/27/Paper/2024-09-27/Talking Head Generation/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Talking-Head-Generation/">Talking Head Generation</a></div><div class="post_share"><div class="social-share" data-image="https://picx.zhimg.com/v2-b0cf5c7e6a853321218751ea3fc0a113.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/09/27/Paper/2024-09-27/3DGS/" title="3DGS"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-dabc6661c0bd0330bff01c9e5ac85fef.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">3DGS</div></div></a></div><div class="next-post pull-right"><a href="/2024/09/27/Paper/2024-09-27/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙/虚拟人"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-41ce0c960b001c3433e8f53f14598019.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">元宇宙/虚拟人</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/03/11/Note/BlendShape/" title="Blendshape学习笔记"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://p6-sign.toutiaoimg.com/pgc-image/2c8cbd123e00470e95500a8ae62da605~noop.image?_iz=58558&from=article.pc_detail&lk3s=953192f4&x-expires=1710668214&x-signature=UHPhjWP4v96kbtfJzF97Z%2Bp3klc%3D" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-11</div><div class="title">Blendshape学习笔记</div></div></a></div><div><a href="/2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" title="超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-01</div><div class="title">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</div></div></a></div><div><a href="/2024/01/20/Project/Linly-Talker%20-%20GPT-SoVITS/" title="数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-20</div><div class="title">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</div></div></a></div><div><a href="/2024/03/03/Paperscape/EMO/" title="EMO Emote Portrait Alive - 阿里HumanAIGC"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-6492e24fb03ffa98135dc584535ab7d9.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-03</div><div class="title">EMO Emote Portrait Alive - 阿里HumanAIGC</div></div></a></div><div><a href="/2024/03/18/Project/SyncTalk/" title="SyncTalk实验笔记"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-3866dff2d07194c235eefab923f694c5.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-18</div><div class="title">SyncTalk实验笔记</div></div></a></div><div><a href="/2024/01/20/Project/Linly-Talker/" title="数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-20</div><div class="title">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-09-27-%E6%9B%B4%E6%96%B0"><span class="toc-text">2024-09-27 更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#TalkinNeRF-Animatable-Neural-Fields-for-Full-Body-Talking-Humans"><span class="toc-text">TalkinNeRF: Animatable Neural Fields for Full-Body Talking Humans</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#FastTalker-Jointly-Generating-Speech-and-Conversational-Gestures-from-Text"><span class="toc-text">FastTalker: Jointly Generating Speech and Conversational Gestures from   Text</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#JoyHallo-Digital-human-model-for-Mandarin"><span class="toc-text">JoyHallo: Digital human model for Mandarin</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Disentangling-Speakers-in-Multi-Talker-Speech-Recognition-with-Speaker-Aware-CTC"><span class="toc-text">Disentangling Speakers in Multi-Talker Speech Recognition with   Speaker-Aware CTC</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#META-CAT-Speaker-Informed-Speech-Embeddings-via-Meta-Information-Concatenation-for-Multi-talker-ASR"><span class="toc-text">META-CAT: Speaker-Informed Speech Embeddings via Meta Information   Concatenation for Multi-talker ASR</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ProbTalk3D-Non-Deterministic-Emotion-Controllable-Speech-Driven-3D-Facial-Animation-Synthesis-Using-VQ-VAE"><span class="toc-text">ProbTalk3D: Non-Deterministic Emotion Controllable Speech-Driven 3D   Facial Animation Synthesis Using VQ-VAE</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image: url('https://picx.zhimg.com/v2-b0cf5c7e6a853321218751ea3fc0a113.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><!-- hexo injector body_end start --><script data-pjax>
  function butterfly_swiper_injector_config(){
    var parent_div_git = document.getElementById('recent-posts');
    var item_html = '<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/25/Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="2024/01/25/Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="2024/01/25/Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/17/Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023/12/17/Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="2023/12/17/Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>';
    console.log('已挂载butterfly_swiper')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'undefined'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_swiper_injector_config();
  }
  else if (epage === cpage){
    butterfly_swiper_injector_config();
  }
  </script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script><!-- hexo injector body_end end --></body></html>