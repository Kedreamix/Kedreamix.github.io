<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>VividTalk One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="VividTalk: One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid PriorPaper   : https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2312.01841.pdf Project : https:&#x2F;&#x2F;humanaigc.github.io&#x2F;vivid-talk&#x2F; Video   : https:&#x2F;&#x2F;www."><meta property="og:type" content="article"><meta property="og:title" content="VividTalk One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior"><meta property="og:url" content="https://kedreamix.github.io/2024/03/05/Paperscape/VividTalk/index.html"><meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World"><meta property="og:description" content="VividTalk: One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid PriorPaper   : https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2312.01841.pdf Project : https:&#x2F;&#x2F;humanaigc.github.io&#x2F;vivid-talk&#x2F; Video   : https:&#x2F;&#x2F;www."><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://picx.zhimg.com/80/v2-8521b04f82075cc27b5e95148dba9792.png"><meta property="article:published_time" content="2024-03-05T07:31:00.000Z"><meta property="article:modified_time" content="2024-03-07T08:03:21.030Z"><meta property="article:author" content="Kedreamix"><meta property="article:tag" content="Talking Head Generation"><meta property="article:tag" content="NeRF"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://picx.zhimg.com/80/v2-8521b04f82075cc27b5e95148dba9792.png"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/2024/03/05/Paperscape/VividTalk/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-74LZ5BEQQ1")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!0,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"简"},noticeOutdate:{limitDay:500,position:"top",messagePrev:"It has been",messageNext:"days since the last update, the content of the article may be outdated."},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:200},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!0,post:!0},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!0,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"VividTalk One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-03-07 16:03:21"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach((e=>{n.setAttribute(e,t[e])})),document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)})),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=24?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 7.0.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml"></head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{const d=document.getElementById("loading-box"),e=document.body,o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l(),window.addEventListener("load",(()=>{o()}))})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">72</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url('https://picx.zhimg.com/80/v2-8521b04f82075cc27b5e95148dba9792.png')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">VividTalk One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-03-05T07:31:00.000Z" title="发表于 2024-03-05 15:31:00">2024-03-05</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-03-07T08:03:21.030Z" title="更新于 2024-03-07 16:03:21">2024-03-07</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paperscape/">Paperscape</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">3.4k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>11分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="VividTalk One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="VividTalk-One-Shot-Audio-Driven-Talking-Head-Generation-Based-on-3D-Hybrid-Prior"><a href="#VividTalk-One-Shot-Audio-Driven-Talking-Head-Generation-Based-on-3D-Hybrid-Prior" class="headerlink" title="VividTalk: One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior"></a>VividTalk: One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior</h1><p>Paper : <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2312.01841.pdf">https://arxiv.org/pdf/2312.01841.pdf</a></p><p>Project : <a target="_blank" rel="noopener" href="https://humanaigc.github.io/vivid-talk/">https://humanaigc.github.io/vivid-talk/</a></p><p>Video : <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=lJVzt7JCe_4">https://www.youtube.com/watch?v=lJVzt7JCe_4</a></p><p>Code : <a target="_blank" rel="noopener" href="https://github.com/HumanAIGC/VividTalk">https://github.com/HumanAIGC/VividTalk</a> (Maybe Comming Soon)</p><p><strong>摘要</strong></p><p>创新的两阶段框架 VividTalk 可生成高质量视觉效果的说话人头部视频，包括唇形同步、丰富的面部表情、自然的头部姿势等。</p><p>（1）音频驱动的说话头生成已经引起广泛关注，在唇形同步、面部表情、头部姿势生成和视频质量方面取得了进展。然而，由于音频和动作之间的一对多映射，还没有模型能够在所有这些指标上达到最优SOTA。<br>（2）以往的方法通常使用混合形状Blendshape或顶点偏移vertex来表示面部表情，但这些方法在捕捉精细的表情细节方面存在局限性。此外，头部姿势的生成通常是通过直接从音频中学习来实现的，这可能会导致不合理和不连续的结果。<br>（3）本文提出了一种名为 VividTalk 的两阶段通用框架，支持生成具有所有上述属性的高视觉质量说话头视频。在第一阶段，通过学习非刚性表情运动和刚性头部运动将音频映射到网格。对于表情运动，采用混合形状和顶点作为中间表示，以最大限度地提高模型的表示能力。对于自然头部运动，提出了一种新颖的可学习头部姿势codebook，并采用两阶段训练机制。在第二阶段，提出了一种双分支运动-VAE 和生成器，将网格转换为密集运动并逐帧合成高质量视频。<br>（4）广泛的实验表明，所提出的 VividTalk 可以生成具有唇形同步和逼真头部姿势的高视觉质量说话头视频，并且在客观和主观比较中优于以往的最新作品。</p><p><strong>要点</strong></p><ul><li>VividTalk 采用双阶段通用框架，可以生成高质量视觉效果的说话人头部视频。</li><li>VividTalk 在第一阶段通过学习非刚性表情运动和刚性头部运动，将音频映射到网格。</li><li>VividTalk 在第二阶段使用双分支运动-VAE 和生成器将网格转换为密集运动并逐帧合成高质量视频。</li><li>广泛的实验表明，与目前最先进的作品相比，VividTalk 可以生成高质量视觉效果的说话人头部视频，并将唇形同步和逼真的增强效果提高很大幅度。</li></ul><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-8521b04f82075cc27b5e95148dba9792.png" alt="VividTalk can generate realistic and lip-sync talking head videos with expressive facial expression, natural head poses."></p><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p><strong>音频驱动人脸生成</strong></p><p>主要是利用音频驱动人脸，生成相匹配的图像，最近的一些工作如SadTalker，是用3DMM作为中间表示，再使用3DMM渲染得到对应的视频；也有利用人脸面部关键点的，这都是比较类似的。同时加入生成mask的嘴唇部份，但是由于中间的表示限制，所有这些方法都不足以生成口型同步和逼真的头部说话视频。</p><p>这个VIvidTalker是使用blendshape和vertex来作为中间表示，分别对粗粒度和细粒度进行建模。</p><p><strong>视频驱动人脸生成</strong></p><p>视频驱动可以认为是表情迁移，也就是将参考视频的动作迁移到目标人脸上，比如FOMM这样的方式，用无监督的关键点作为中间的表示，以及有利用depth深度作为信息的。</p><h2 id="主要方法"><a href="#主要方法" class="headerlink" title="主要方法"></a>主要方法</h2><p>VividTalk主要的框架由两个级联阶段组成，分别是</p><ul><li><strong>Audio-To-Mesh</strong> 音频到网格生成</li><li><strong>Mesh-To-VIdeo</strong> 网格到视频生成</li></ul><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-35ebd6e4eb48d485c2f77af937e3a762.png" alt="主要方法"></p><h3 id="前馈知识"><a href="#前馈知识" class="headerlink" title="前馈知识"></a>前馈知识</h3><h4 id="3DMM"><a href="#3DMM" class="headerlink" title="3DMM"></a>3DMM</h4><p>3D Morphable Model（3DMM）是一种用于建模和分析人脸形状和外观的计算机图形技术。它是基于数学模型的方法，用于描述和生成人脸的<strong>三维几何形状和表面纹理</strong>。3DMM的基本原理是利用统计学方法从大量的三维人脸数据中学习人脸形状和纹理的变化规律，并将这些信息编码到一个数学模型中。</p><p>这个模型包括两个主要的部分：形状模型和纹理模型。</p><ol><li><strong>形状模型</strong>：形状模型描述了人脸的几何形状的变化。通常采用的方法是使用主成分分析（PCA）对人脸的形状数据进行降维和建模。通过分析大量的人脸形状数据，可以得到一组主成分，它们描述了人脸形状变化的主要模式。形状模型可以用来生成新的人脸形状，或者对现有的人脸形状进行编辑和变形。</li><li><strong>纹理模型</strong>：纹理模型描述了人脸表面的颜色和纹理的变化。与形状模型类似，纹理模型也可以利用主成分分析等方法来建模人脸的表面纹理。通过分析大量的人脸纹理数据，可以得到一组主成分，它们描述了人脸表面颜色和纹理的变化模式。纹理模型可以用来生成新的人脸纹理，或者对现有的人脸纹理进行编辑和变换。</li></ol><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-efd80426cbb18b4f2ee91789c07277eb.png" alt="3DMM"></p><h4 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h4><p>除此之外，首先还需要对数据集进行预处理，使用FOMM对方式对视听数据集进行预处理，并且裁剪面部区域为256x256。同时也是用FaceVerse来提取表情系数和网格顶点序列。</p><h3 id="Audio-To-Mesh"><a href="#Audio-To-Mesh" class="headerlink" title="Audio-To-Mesh"></a>Audio-To-Mesh</h3><p>在数据预处理的时候，使用Faceverse重建我们的参考图像，从音频中学习非刚性面部表情运动和刚性头部运动来驱动重建的网格。为此，提出了一个多分支的Blendshape和Vertex偏移生成器以及学习头部姿势的codebook，具体如下图所示。</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-1648982e559021c0b5f5eaa6b201ef93.png" alt="Audio-To-Mesh"></p><p><strong>BlendShape and Vertex Offset Generator</strong></p><p>对于BlendShape and Vertex Offset Generator来说，首先会使用一个预训练的音频模型来提取音频特征，然后从参考图像中提取身份信息$\alpha$，并且编码为风格信息$z_{style}$，然后在音频特征中嵌入个人风格信息，再结合送到基于多分支的Transformer架构中，一共有三个分支，两个分支生成粗粒度的blendshape，第三个分支生成细粒度的与嘴唇相关的vertex偏移对嘴唇运动进行补充。</p><script type="math/tex;mode=display">\hat{\beta}_i^f=\Phi_i^{bs}(\hat{\beta}_i^{1...f-1},A,z^{style}),\quad i\in\{lip,other\}, \\
\hat{O}_{lip}^f=\Phi_{lip}^{\upsilon o}(\hat{O}_{lip}^{1...f-1},A,z^{style}),</script><p>训练完成后，就可以通过以下方式来进行驱动</p><script type="math/tex;mode=display">\hat{M}_{nr}=(\overline{S}+\alpha U_{id}+(\hat{\beta}_{lip},\hat{\beta}_{other})U_{exp}+\hat{O}_{lip})\otimes P_{ref}.</script><p>这里面的$P_{ref}$为参考图像的<strong>head pose</strong>，$\otimes$是对应的仿射变化。</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-15f9efd01582593cfaf9a3a5bd765dac.png" alt="BlendShape and Vertex Offset Generator"></p><p><strong>Learnable Head Pose Codebook</strong></p><p>头部姿势是非常重要的一环，直接从音频中学习还是比较困难的，因为这里面的关系是比较微弱的，因此，使用离散的codebook的，将生成的问题转化为在离散和且有限的姿势空间中查询codebook的任务，设计了两阶段的训练机制。</p><p>第一阶段是重建阶段，利用VQ-VAE来构建丰富的头部姿势codebook，是一个编码解码结构。</p><script type="math/tex;mode=display">Z_q=\mathbf{q}(\hat{z})=\underset{z_k\in\mathcal{Z}}{\operatorname*{\arg\min}}\left\|\hat{z}-z_k\right\|. \\
\hat{P}_r^{1:f}=\mathcal{D}(Z_q)=\mathcal{D}(\mathbf{q}(\mathcal{E}(P_r^{1:f}))).</script><p>第二阶段是映射阶段，将输入音频映射到codebook生成最终结果，具体来说，$\Phi_{map}$以音频序列A、特定于人的风格嵌入$z^{style}$和初始头部姿势$P^0$ 作为输入，输出中间特征$\hat Z$，该中间特征将从codebook$Z$量化为$Z_q$，然后由预训练的解码器$D$解码</p><script type="math/tex;mode=display">\hat{P}_r^{1:f}=\mathcal{D}(Z_q)=\mathcal{D}(\mathbf{q}(\Phi_{map}(A,s,P^0))).</script><p>从目前为止，非刚性面部表情运动和刚性头部姿势都已学习。现在我们就可以运用学习到的刚性头部姿势应用于Mesh $\hat{M}<em>{nr}$来获得最最终的驱动网格Mesh $\hat{M}</em>{d}$。</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-d9f01fd2be86dc73e859cc5df7c2f7d9.png" alt="Learnable Head Pose Codebook"></p><h3 id="Mesh-To-Video"><a href="#Mesh-To-Video" class="headerlink" title="Mesh-To-Video"></a>Mesh-To-Video</h3><p>这一部份是为了将驱动的Mesh转成视频，提出了一个双分支的Motion-VAE对这些2D密集运动进行建模，最后合成最终的视频。</p><p>如果要建模2D与3D之间的关系比较难，为了更好的学习，使用投影纹理表示来实现2D的转换。</p><p>并且为了更好的学习3D Mesh的纹理，首先在x,y,z三个轴的进行归一化的处理，归一化到0，得到纹理的新表示NCC：</p><script type="math/tex;mode=display">NCC_i=\frac{\overline{S}_i-min(\overline{S}_i)}{max(\overline{S}_i)-min(\overline{S}_i)},\quad i\in\{x,y,z\}.</script><p>然后，使用了Z-Buffer方式和NCC的颜色去渲染3D面度的纹理$PT<em>{in}$，由于3DMM的限制，外表的区域是无法被建模的，所以使用Deep Learning Face Attributes in the Wild 方法解析图像并获得外部面部区域纹理$PT</em>{out}$，例如躯干和背景，将其与$PT_{in}$ 组合如下：</p><script type="math/tex;mode=display">PT=PT_{in}\cdot M+PT_{out}\cdot(1-M)</script><p>其中$M$是内部人脸的Mask，为了进一步增强嘴唇运动并更准确地建模，我们还选择与嘴唇相关的标志并将其转换为高斯图，这是一种更紧凑、更有效的表示。然后，Hourglass网络将减去的高斯图作为输入并输出 2D 嘴唇运动，该运动将与面部运动连接并解码为密集运动和遮挡图。</p><p>最后，根据之前预测的密集运动图对参考图像进行变形，获得变形图像，该变形图像将与遮挡图一起作为生成器的输入，逐帧合成最终视频。</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-bd37230a1f7ac7c875a8b5555d5b43dd.png" alt="Mesh-To-Video"></p><h3 id="训练策略"><a href="#训练策略" class="headerlink" title="训练策略"></a>训练策略</h3><p>这几部分实际上都是分开训练的，不过训练后可以通过端到端的方式生成结果。</p><p><strong>BlendShape and Vertex Offset Generator</strong>由Blendshape和Mesh重建损失来进行监督</p><script type="math/tex;mode=display">L_{bsvo}=\left\|\beta-\hat{\beta}\right\|+\left\|M-\hat{M}_{nr}\right\|.</script><p><strong>Learnable Head Pose Codebook</strong>部分中，由于量化函数是不可微分的，所以使用straight-through gradient estimator将梯度从解码器复制到编码器，然后对两阶段训练进行如下监督：</p><script type="math/tex;mode=display">\begin{aligned}
L_{rec}= =\left\|P_r^{1:f}-\hat{P}_r^{1:f}\right\|^2+\left\|sg(\mathcal{E}(P_r^{1:f}))-z_q\right\|_2^2  \\
+\left\|sg(z_q)-\mathcal{E}(P_r^{1:f})\right\|_2^2, \\
L_{map} =\left\|P_r^{1:f}-\hat{P}_r^{1:f}\right\|^2+\left\|\hat{Z}-sg(Z_q)\right\|_2^2, 
\end{aligned}</script><p>sg表示停止梯度操作，也就是 <strong>stop gradient</strong></p><p><strong>Mesh-To-Video</strong>阶段中，基于预训练的VGG-19 网络的感知损失$L<em>{perc}$被用作主要驱动损失。特征匹配损失 $L</em>{fm}$还用于稳定训练产生更真实的结果。</p><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>接下来总结一下实验的结果方法的对比，该模型使用了HDTF和VoxCeleb数据集，使用Adam优化器，在两个阶段中学习率分别为1e-4和1e-5，最后用8个V100训练了2天得到最终的结果。</p><div class="table-container"><table><thead><tr><th>方法</th><th>优点</th><th>缺点</th></tr></thead><tbody><tr><td>SadTalker</td><td>无法生成精确的细节唇部动作</td><td>视频质量不佳</td></tr><tr><td>TalkLip</td><td>生成模糊结果，皮肤色调稍微偏黄，失去了一定程度的身份信息</td><td>质量较差</td></tr><tr><td>MakeItTalk</td><td>在交叉身份配音设置中不能生成准确的嘴部形状</td><td>嘴部形状不准确</td></tr><tr><td>Wav2Lip</td><td>容易合成模糊的口部区域，单一参考图像时输出视频头部姿势和眼部运动静止</td><td>视频输出质量较低</td></tr><tr><td>PC-AVS</td><td>需要一个驱动视频作为输入，身份保存困难</td><td>身份保存困难</td></tr><tr><td>VividTalker</td><td>可以生成高质量的说话头像视频，具有准确的唇同步和丰富的面部运动</td><td>视频质量高，唇同步准确，面部运动丰富</td></tr></tbody></table></div><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-66838829a274884142dde5ee251e190c.png" alt="实验结果"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>VividTalk框架的优点包括：</p><ol><li><p><strong>高质量的生成视频</strong>：VividTalk能够生成高质量的说话头像视频，具有清晰的面部表情和自然的头部姿势，为用户提供更具沉浸感和真实感的体验。</p></li><li><p><strong>丰富的表达能力</strong>：通过将混合形状和顶点映射为中间表示，VividTalk能够最大化模型的表达能力，从而呈现出丰富的面部表情，包括细微的细节运动。</p></li><li><p><strong>灵活的模型设计</strong>：采用多分支生成器，VividTalk能够灵活地对全局和局部面部运动进行建模，使得生成的视频更加生动和自然。</p></li><li><p><strong>自然的头部姿势合成</strong>：通过引入新颖的可学习的头部姿势码本和两阶段训练机制，VividTalk能够合成更加自然的头部姿势，使得生成的视频更加逼真。</p></li><li><p><strong>创新的双分支机制</strong>：利用双分支运动-VAE和生成器，VividTalk能够有效地转化驱动网格为密集运动，并用于合成最终视频，提高了生成视频的质量和真实感。</p></li><li><p><strong>超越性能</strong>：实验证明，VividTalk优于以往的最先进方法，为数字人类创建、视频会议等应用开辟了新的可能性。</p></li></ol></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io/2024/03/05/Paperscape/VividTalk/">https://kedreamix.github.io/2024/03/05/Paperscape/VividTalk/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Talking-Head-Generation/">Talking Head Generation</a><a class="post-meta__tags" href="/tags/NeRF/">NeRF</a></div><div class="post_share"><div class="social-share" data-image="https://picx.zhimg.com/80/v2-8521b04f82075cc27b5e95148dba9792.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/03/07/Paperscape/SyncTalk/" title="SyncTalk The Devil is in the Synchronization for Talking Head Synthesis"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-a57e0937b2f452009023394a59529dfb.png" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">SyncTalk The Devil is in the Synchronization for Talking Head Synthesis</div></div></a></div><div class="next-post pull-right"><a href="/2024/03/04/Paper/2024-03-04/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-1e4adba77bea5b8766028ddf128d14f8.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Diffusion Models</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" title="超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-01</div><div class="title">超赞的数字人生成知识库 Awesome-Talking-Head-Synthesis</div></div></a></div><div><a href="/2024/03/07/Paperscape/SyncTalk/" title="SyncTalk The Devil is in the Synchronization for Talking Head Synthesis"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-a57e0937b2f452009023394a59529dfb.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-07</div><div class="title">SyncTalk The Devil is in the Synchronization for Talking Head Synthesis</div></div></a></div><div><a href="/2024/03/11/Note/BlendShape/" title="Talking Head Generation"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://p6-sign.toutiaoimg.com/pgc-image/2c8cbd123e00470e95500a8ae62da605~noop.image?_iz=58558&from=article.pc_detail&lk3s=953192f4&x-expires=1710668214&x-signature=UHPhjWP4v96kbtfJzF97Z%2Bp3klc%3D" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-11</div><div class="title">Talking Head Generation</div></div></a></div><div><a href="/2024/03/03/Paperscape/EMO/" title="EMO Emote Portrait Alive - 阿里HumanAIGC"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-6492e24fb03ffa98135dc584535ab7d9.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-03</div><div class="title">EMO Emote Portrait Alive - 阿里HumanAIGC</div></div></a></div><div><a href="/2024/01/20/Project/Linly-Talker%20-%20GPT-SoVITS/" title="数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-20</div><div class="title">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</div></div></a></div><div><a href="/2024/01/20/Project/Linly-Talker/" title="数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-20</div><div class="title">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#VividTalk-One-Shot-Audio-Driven-Talking-Head-Generation-Based-on-3D-Hybrid-Prior"><span class="toc-number">1.</span> <span class="toc-text">VividTalk: One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C"><span class="toc-number">1.1.</span> <span class="toc-text">相关工作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E6%96%B9%E6%B3%95"><span class="toc-number">1.2.</span> <span class="toc-text">主要方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%89%8D%E9%A6%88%E7%9F%A5%E8%AF%86"><span class="toc-number">1.2.1.</span> <span class="toc-text">前馈知识</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3DMM"><span class="toc-number">1.2.1.1.</span> <span class="toc-text">3DMM</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86"><span class="toc-number">1.2.1.2.</span> <span class="toc-text">数据处理</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Audio-To-Mesh"><span class="toc-number">1.2.2.</span> <span class="toc-text">Audio-To-Mesh</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Mesh-To-Video"><span class="toc-number">1.2.3.</span> <span class="toc-text">Mesh-To-Video</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E7%AD%96%E7%95%A5"><span class="toc-number">1.2.4.</span> <span class="toc-text">训练策略</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="toc-number">1.3.</span> <span class="toc-text">实验结果</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">1.4.</span> <span class="toc-text">总结</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image:url('https://picx.zhimg.com/80/v2-8521b04f82075cc27b5e95148dba9792.png')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js").then((()=>{pangu.autoSpacingPage()}))}function panguInit(){panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"></div><script id="canvas_nest" defer color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><script data-pjax>function butterfly_swiper_injector_config(){var a=document.getElementById("recent-posts");console.log("已挂载butterfly_swiper"),a.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/25/Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="2024/01/25/Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="2024/01/25/Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/17/Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023/12/17/Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="2023/12/17/Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script></body></html>