<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>EMO Emote Portrait Alive - 阿里HumanAIGC | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="EMO: Emote Portrait Alive - 阿里HumanAIGC最近这一个星期，也就是2月28日的时候，阿里巴巴的HumanAIGC团队发布了一款全新的生成式AI模型EMO（Emote Portrait Alive）。EMO仅需一张人物肖像照片和音频，就可以让照片中的人物按照音频内容“张嘴”唱歌、说话，且口型基本一致，面部表情和头部姿态非常自然，发布的视频效果非常好，好的几乎难以置信"><meta property="og:type" content="article"><meta property="og:title" content="EMO Emote Portrait Alive - 阿里HumanAIGC"><meta property="og:url" content="https://kedreamix.github.io/2024/03/03/Paperscape/EMO/index.html"><meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World"><meta property="og:description" content="EMO: Emote Portrait Alive - 阿里HumanAIGC最近这一个星期，也就是2月28日的时候，阿里巴巴的HumanAIGC团队发布了一款全新的生成式AI模型EMO（Emote Portrait Alive）。EMO仅需一张人物肖像照片和音频，就可以让照片中的人物按照音频内容“张嘴”唱歌、说话，且口型基本一致，面部表情和头部姿态非常自然，发布的视频效果非常好，好的几乎难以置信"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://picx.zhimg.com/v2-6492e24fb03ffa98135dc584535ab7d9.png"><meta property="article:published_time" content="2024-03-03T13:20:00.000Z"><meta property="article:modified_time" content="2024-03-07T08:03:21.028Z"><meta property="article:author" content="Kedreamix"><meta property="article:tag" content="Talking Head Generation"><meta property="article:tag" content="Diffusion Models"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://picx.zhimg.com/v2-6492e24fb03ffa98135dc584535ab7d9.png"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/2024/03/03/Paperscape/EMO/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-74LZ5BEQQ1")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!0,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"简"},noticeOutdate:{limitDay:500,position:"top",messagePrev:"It has been",messageNext:"days since the last update, the content of the article may be outdated."},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:200},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!0,post:!0},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!0,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"EMO Emote Portrait Alive - 阿里HumanAIGC",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-03-07 16:03:21"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach((e=>{n.setAttribute(e,t[e])})),document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)})),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=24?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 7.0.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml"></head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{const d=document.getElementById("loading-box"),e=document.body,o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l(),window.addEventListener("load",(()=>{o()}))})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">123</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url('https://pica.zhimg.com/v2-24facf74c8152c3d19d0e57fce19c9b2.png')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">EMO Emote Portrait Alive - 阿里HumanAIGC</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-03-03T13:20:00.000Z" title="发表于 2024-03-03 21:20:00">2024-03-03</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-03-07T08:03:21.028Z" title="更新于 2024-03-07 16:03:21">2024-03-07</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paperscape/">Paperscape</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">2.8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>8分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="EMO Emote Portrait Alive - 阿里HumanAIGC"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="EMO-Emote-Portrait-Alive-阿里HumanAIGC"><a href="#EMO-Emote-Portrait-Alive-阿里HumanAIGC" class="headerlink" title="EMO: Emote Portrait Alive - 阿里HumanAIGC"></a>EMO: Emote Portrait Alive - 阿里HumanAIGC</h1><p>最近这一个星期，也就是2月28日的时候，阿里巴巴的HumanAIGC团队发布了一款全新的生成式AI模型EMO（Emote Portrait Alive）。EMO仅需一张人物肖像照片和音频，就可以让照片中的人物按照音频内容“张嘴”唱歌、说话，且口型基本一致，面部表情和头部姿态非常自然，发布的视频效果非常好，好的几乎难以置信，特别是蔡徐坤唱rap的第一段，效果非常好。</p><p><strong>EMO不仅能够生成唱歌和说话的视频，还能在保持角色身份稳定性的同时，根据输入音频的长度生成不同时长的视频。</strong></p><p>所以我就想借此机会，学习一下EMO的大概框架，剖析一下里面的一些技术要点，首先给出论文的链接和代码链接，不过HumanAIGC已经很久没有开源代码了，不过技术方向还是值得一看的。</p><p>论文：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2402.17485v1">EMO: Emote Portrait Alive - Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions</a></p><p>项目：<a target="_blank" rel="noopener" href="https://humanaigc.github.io/emote-portrait-alive/">https://humanaigc.github.io/emote-portrait-alive/</a></p><p>我也一直有关注这一部分的技术，大家也可以关注我的数字人知识库<a target="_blank" rel="noopener" href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis">https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis</a></p><h2 id="Diffusion相关"><a href="#Diffusion相关" class="headerlink" title="Diffusion相关"></a>Diffusion相关</h2><p>在之前的一些研究中，有过用Diffusion做Talking head generation的，比如Diffusion head和CVPR2023的DiffTalk等论文，这些论文都是用Diffusion得强大生成能力来完成音频驱动的人脸生成。</p><p>这里逐帧生成与音频对应的人脸的图像，mask人脸中嘴唇的部分，然后逐步生成视频，<strong>这个过程相当于，AI先看一下照片，然后打开声音，再随着声音一张一张地画出视频中每一帧变化的图像。</strong></p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-24c8ad5651ce25627b3e8bfff24d85b1.png" alt="DiffTalk"></p><p>如果我们看Diffusion Head论文，也是类似的做法，都是通过Diffusion的强大能力完成视频的生成。</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-3e6497aae4c003eb72bb3f24224c89ee.png" alt="Overview"></p><h2 id="EMO整体框架"><a href="#EMO整体框架" class="headerlink" title="EMO整体框架"></a>EMO整体框架</h2><p>接下来开始剖析一下EMO的框架，与DiffTalk和Diffusion Heads类似，都是利用Diffusion来生成，也是根据一个参考图像来逐帧生成图片最后得到视频。</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-24facf74c8152c3d19d0e57fce19c9b2.png" alt="EMO"></p><p>不同的是，EMO的工作过程分为两个主要阶段：</p><ol><li>首先，利用参考网络（ReferenceNet）从参考图像和动作帧中提取特征；</li><li>然后，利用预训练的音频编码器处理声音并嵌入，再结合多帧噪声和面部区域掩码来生成视频。</li></ol><p>该框架还融合了两种注意机制和时间模块，以确保视频中角色身份的一致性和动作的自然流畅。我觉得实际上这里是最重要的一部分，这一部分也是和之前Diffusion方法不同的点，其实这一部份又和HumanAIGC之前做的科目三驱动的方式很像，也就是那篇AnimateAnyone论文，这一部分也是火🔥了很久，现在也有人复现了该方法，不过还没有开源。</p><p>根据EMO的论文与项目的展现的结果，EMO不仅仅能产生非常Amazing的对口型视频，还能生成各种风格的歌唱视频，无论是在表现力还是真实感方面都显著优于现有的先进方法，如DreamTalk、Wav2Lip和SadTalker。</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-6492e24fb03ffa98135dc584535ab7d9.png" alt="EMO整体框架"></p><h2 id="EMO工作原理"><a href="#EMO工作原理" class="headerlink" title="EMO工作原理"></a>EMO工作原理</h2><p>从EMO的框架可以看到，利用骨干网络获取多帧噪声潜在输入，并尝试在每个时间步将它们去噪到连续的视频帧，这个骨干网络是类似于SD 1.5的UNet的结构配置。与之前的SD1.5不同的是，本身的SD是使用文本嵌入的，而现在是使用参考特征。</p><ol><li>与之前的工作类似，为了确保生成的帧之间的连续性，骨干网络嵌入了时间模块。</li><li>为了保持生成帧中肖像的ID一致性，使用了一个与Backbone并行的称为ReferenceNet的UNet结构，它输入参考图像以获得参考特征。</li><li>为了驱动角色说话动作，利用音频层对语音特征进行编码。</li><li>为了使说话角色的运动可控且稳定，我们使用面部定位器和速度层来提供弱条件。</li></ol><p><strong>预训练音频编码器：</strong>EMO使用预训练的音频编码器（如wav2vec）来处理输入音频。这些编码器提取音频特征，这些特征随后用于驱动视频中的角色动作，包括口型和面部表情。这里面还是使用附加特征m来解决动作可能会受到未来/过去音频片段的影响，例如说话前张嘴和吸气。</p><p><strong>参考网络（ReferenceNet）：</strong>该网络从单个参考图像中提取特征，这些特征在视频生成过程中用于保持角色的身份一致性。ReferenceNet与生成网络（Backbone Network）并行工作，输入参考图像以获取参考特征。</p><p><strong>骨干网络（Backbone Network）：</strong>Backbone Network接收多帧噪声（来自参考图像和音频特征的结合）并尝试将其去噪为连续的视频帧。这个网络采用了类似于Stable Diffusion的UNet结构，其中包含了用于维持生成帧之间连续性的时间模块。</p><p><strong>注意力机制：</strong>EMO利用两种形式的注意力机制——<strong>参考注意力（Reference-Attention）和音频注意力（Audio-Attention）</strong>。参考注意力用于保持角色身份的一致性，而音频注意力则用于调整角色的动作，使之与音频信号相匹配。</p><p><strong>时间模块：</strong>这些模块用于操纵时间维度并调整动作速度，以生成流畅且连贯的视频序列。时间模块通过自注意力层跨帧捕获动态内容，有效地在不同的视频片段之间维持一致性。</p><p><strong>训练策略：</strong>EMO的训练分为三个阶段：图像预训练、视频训练和速度层训练。在图像预训练阶段，Backbone Network和ReferenceNet在单帧上进行训练，而在视频训练阶段，引入时间模块和音频层，处理连续帧。速度层的训练在最后阶段进行，以细化角色头部的移动速度和频率。</p><p><strong>去噪过程：</strong>在生成过程中，Backbone Network尝试去除多帧噪声，生成连续的视频帧。去噪过程中，参考特征和音频特征被结合使用，以生成高度真实和表情丰富的视频内容。</p><p>EMO模型通过这种结合使用参考图像、音频信号、和时间信息的方法，能够生成与输入音频同步且在表情和头部姿势上富有表现力的肖像视频，超越了传统技术的限制，创造出更加自然和逼真的动画效果。</p><h2 id="EMO训练阶段"><a href="#EMO训练阶段" class="headerlink" title="EMO训练阶段"></a>EMO训练阶段</h2><p>训练分为三个阶段，<strong>图像预训练、视频训练和速度层训练。</strong></p><ul><li><p>在图像预训练阶段，网络以单帧图像为输入进行训练。此阶段，Backbone 将单个帧作为输入，而 ReferenceNet 处理来自同一帧的不同的、随机选择的帧，从原始 SD 初始化权重</p></li><li><p>在视频训练阶段，引入时间模块和音频层，处理连续帧，从视频剪辑中采样n+f个连续帧，开始的n帧是运动帧。时间模块从AnimateDiff初始化权重。</p></li><li><p>速度层训练专注于调整角色头部的移动速度和频率。</p></li></ul><p>这些详细信息提供了对EMO模型训练和其参数配置的深入了解，突显了其在处理广泛和多样化数据集方面的能力，以及其在生成富有表现力和逼真肖像视频方面的先进性能。</p><h2 id="EMO实验设置"><a href="#EMO实验设置" class="headerlink" title="EMO实验设置"></a>EMO实验设置</h2><p>EMO的数据集有两部份，首先HumanAIGC团队从互联网中收集了 <strong>超过250小时的视频和超过1.5亿张图像</strong>，同时加入了来自互联网和HDTF以及VFHQ数据集作为补充。这里面的数据集多种多样，包括演讲、电影和电视剪辑以及歌唱表演，涵盖了多种语言，如中文和英文，这也是为什么最后能表现出如此好效果的原因。</p><p>在第一阶段的时候，使用VFHQ数据集，因为它不包含音频。然后再对视频进行预处理，所有的视频可通过MediaPipe来获取人脸检测框区域，并且裁剪到512×512的分辨率。</p><p>在第一训练阶段，批处理大小BatchSize设置为48。在第二和第三训练阶段，生成视频长度设置为f=12，运动帧数设置为n=4，训练的批处理大小为4，学习率在所有阶段均设置为1e-5。</p><p>在推理时，使用DDIM的采样算法生成视频。时间步大约是40步，为每一帧生成指定一个恒定的速度值，最后方法的结果生成一批（f=12帧）的时间大约为15秒。</p><p>一般视频的长度为25～30帧左右，如果我们认为是1mins的视频，也就是60s的视频，那就是60*25=1500，1500/15 = 100s，也就是大概需要1mins40s能生成一分钟的视频，速度也得到了不错的改进，虽然没有实时，但是结果已经很好了。</p><h2 id="EMO特点"><a href="#EMO特点" class="headerlink" title="EMO特点"></a>EMO特点</h2><p>EMO模型有如下特点：</p><p><strong>直接音频到视频合成：</strong>EMO采用直接从音频合成视频的方法，无需中间的3D模型或面部标志，简化了生成过程，同时保持了高度的表现力和自然性。</p><p><strong>无缝帧过渡与身份保持：</strong>该方法确保视频帧之间的无缝过渡和视频中身份的一致性，生成的动画既生动又逼真。</p><p><strong>表达力与真实性：</strong>实验结果显示，EMO不仅能生成令人信服的说话视频，而且还能生成各种风格的歌唱视频，其表现力和真实性显著超过现有的先进方法。</p><p><strong>灵活的视频时长生成：</strong>EMO可以根据输入音频的长度生成任意时长的视频，提供了极大的灵活性。</p><p><strong>面向表情的视频生成：</strong>EMO专注于通过音频提示生成表情丰富的肖像视频，特别是在处理说话和唱歌场景时，可以捕捉到复杂的面部表情和头部姿态变化。</p><p>这些特点共同构成了EMO模型的核心竞争力，使其在动态肖像视频生成领域表现出色。</p><h2 id="EMO缺陷"><a href="#EMO缺陷" class="headerlink" title="EMO缺陷"></a>EMO缺陷</h2><p>对于EMO来说，也会有一些限制。</p><ul><li><p>首先，与不依赖扩散模型的方法相比，它更耗时。</p></li><li><p>其次，由于不使用任何明确的控制信号来控制角色的运动，因此可能会导致无意中生成其他身体部位（例如手），从而导致视频中出现伪影。</p></li></ul><p>所以这样的一个问题，如果要解决的话，可以考虑用专门控制身体部位的控制信号，这样就会较好的解决这个方法，每一个信号控制一部分，就不会生成错误。</p><p>参考</p><ul><li><a target="_blank" rel="noopener" href="https://m.huxiu.com/article/2728417.html">https://m.huxiu.com/article/2728417.html</a></li></ul></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io/2024/03/03/Paperscape/EMO/">https://kedreamix.github.io/2024/03/03/Paperscape/EMO/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Talking-Head-Generation/">Talking Head Generation</a><a class="post-meta__tags" href="/tags/Diffusion-Models/">Diffusion Models</a></div><div class="post_share"><div class="social-share" data-image="https://picx.zhimg.com/v2-6492e24fb03ffa98135dc584535ab7d9.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/03/04/Paper/2024-03-04/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-1e4adba77bea5b8766028ddf128d14f8.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Diffusion Models</div></div></a></div><div class="next-post pull-right"><a href="/2024/02/29/Paper/2024-02-29/NeRF/" title="NeRF"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-fe3f4f6d4cf8758d74cb0be86547e9f6.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">NeRF</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/03/11/Note/BlendShape/" title="Blendshape学习笔记"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://p6-sign.toutiaoimg.com/pgc-image/2c8cbd123e00470e95500a8ae62da605~noop.image?_iz=58558&from=article.pc_detail&lk3s=953192f4&x-expires=1710668214&x-signature=UHPhjWP4v96kbtfJzF97Z%2Bp3klc%3D" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-11</div><div class="title">Blendshape学习笔记</div></div></a></div><div><a href="/2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" title="超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-01</div><div class="title">超赞的数字人生成知识库 Awesome-Talking-Head-Synthesis</div></div></a></div><div><a href="/2024/01/20/Project/Linly-Talker/" title="数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-20</div><div class="title">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</div></div></a></div><div><a href="/2024/01/20/Project/Linly-Talker%20-%20GPT-SoVITS/" title="数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-20</div><div class="title">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</div></div></a></div><div><a href="/2024/03/18/Project/SyncTalk/" title="SyncTalk实验笔记"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-3866dff2d07194c235eefab923f694c5.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-18</div><div class="title">SyncTalk实验笔记</div></div></a></div><div><a href="/2024/03/15/Paperscape/Real3D-Portrait/" title="REAL3D-PORTRAIT ONE-SHOT REALISTIC 3D TALKING PORTRAIT SYNTHESIS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-68585b79de5f83b0dfa23304f41b9b98.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-15</div><div class="title">REAL3D-PORTRAIT ONE-SHOT REALISTIC 3D TALKING PORTRAIT SYNTHESIS</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#EMO-Emote-Portrait-Alive-%E9%98%BF%E9%87%8CHumanAIGC"><span class="toc-number">1.</span> <span class="toc-text">EMO: Emote Portrait Alive - 阿里HumanAIGC</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Diffusion%E7%9B%B8%E5%85%B3"><span class="toc-number">1.1.</span> <span class="toc-text">Diffusion相关</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#EMO%E6%95%B4%E4%BD%93%E6%A1%86%E6%9E%B6"><span class="toc-number">1.2.</span> <span class="toc-text">EMO整体框架</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#EMO%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86"><span class="toc-number">1.3.</span> <span class="toc-text">EMO工作原理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#EMO%E8%AE%AD%E7%BB%83%E9%98%B6%E6%AE%B5"><span class="toc-number">1.4.</span> <span class="toc-text">EMO训练阶段</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#EMO%E5%AE%9E%E9%AA%8C%E8%AE%BE%E7%BD%AE"><span class="toc-number">1.5.</span> <span class="toc-text">EMO实验设置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#EMO%E7%89%B9%E7%82%B9"><span class="toc-number">1.6.</span> <span class="toc-text">EMO特点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#EMO%E7%BC%BA%E9%99%B7"><span class="toc-number">1.7.</span> <span class="toc-text">EMO缺陷</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image:url('https://pica.zhimg.com/v2-24facf74c8152c3d19d0e57fce19c9b2.png')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js").then((()=>{pangu.autoSpacingPage()}))}function panguInit(){panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"></div><script id="canvas_nest" defer color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><script data-pjax>function butterfly_swiper_injector_config(){var a=document.getElementById("recent-posts");console.log("已挂载butterfly_swiper"),a.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/25/Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="2024/01/25/Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="2024/01/25/Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/17/Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023/12/17/Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="2023/12/17/Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script></body></html>