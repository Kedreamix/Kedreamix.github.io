<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>NeRF | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="NeRF 方向最新论文已更新，请持续关注 Update in 2024-03-23  CombiNeRF A Combination of Regularization Techniques for Few-Shot   Neural Radiance Field View Synthesis"><meta property="og:type" content="article"><meta property="og:title" content="NeRF"><meta property="og:url" content="https://kedreamix.github.io/2024/03/23/Paper/2024-03-23/NeRF/index.html"><meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World"><meta property="og:description" content="NeRF 方向最新论文已更新，请持续关注 Update in 2024-03-23  CombiNeRF A Combination of Regularization Techniques for Few-Shot   Neural Radiance Field View Synthesis"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://picx.zhimg.com/v2-19d6253b6aea4731864c3a1ce65af4bb.jpg"><meta property="article:published_time" content="2024-03-23T11:02:12.000Z"><meta property="article:modified_time" content="2024-03-23T11:02:12.769Z"><meta property="article:author" content="Kedreamix"><meta property="article:tag" content="NeRF"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://picx.zhimg.com/v2-19d6253b6aea4731864c3a1ce65af4bb.jpg"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/2024/03/23/Paper/2024-03-23/NeRF/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-74LZ5BEQQ1")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!0,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"简"},noticeOutdate:{limitDay:500,position:"top",messagePrev:"It has been",messageNext:"days since the last update, the content of the article may be outdated."},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:200},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!0,post:!0},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!0,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"NeRF",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-03-23 19:02:12"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach((e=>{n.setAttribute(e,t[e])})),document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)})),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=24?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 7.0.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml"></head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{const d=document.getElementById("loading-box"),e=document.body,o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l(),window.addEventListener("load",(()=>{o()}))})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">110</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url('https://picx.zhimg.com/v2-19d6253b6aea4731864c3a1ce65af4bb.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">NeRF</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-03-23T11:02:12.000Z" title="发表于 2024-03-23 19:02:12">2024-03-23</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-03-23T11:02:12.769Z" title="更新于 2024-03-23 19:02:12">2024-03-23</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">25.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>89分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="NeRF"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-03-23-更新"><a href="#2024-03-23-更新" class="headerlink" title="2024-03-23 更新"></a>2024-03-23 更新</h1><h2 id="CombiNeRF-A-Combination-of-Regularization-Techniques-for-Few-Shot-Neural-Radiance-Field-View-Synthesis"><a href="#CombiNeRF-A-Combination-of-Regularization-Techniques-for-Few-Shot-Neural-Radiance-Field-View-Synthesis" class="headerlink" title="CombiNeRF: A Combination of Regularization Techniques for Few-Shot   Neural Radiance Field View Synthesis"></a>CombiNeRF: A Combination of Regularization Techniques for Few-Shot Neural Radiance Field View Synthesis</h2><p><strong>Authors:Matteo Bonotto, Luigi Sarrocco, Daniele Evangelista, Marco Imperoli, Alberto Pretto</strong></p><p>Neural Radiance Fields (NeRFs) have shown impressive results for novel view synthesis when a sufficiently large amount of views are available. When dealing with few-shot settings, i.e. with a small set of input views, the training could overfit those views, leading to artifacts and geometric and chromatic inconsistencies in the resulting rendering. Regularization is a valid solution that helps NeRF generalization. On the other hand, each of the most recent NeRF regularization techniques aim to mitigate a specific rendering problem. Starting from this observation, in this paper we propose CombiNeRF, a framework that synergically combines several regularization techniques, some of them novel, in order to unify the benefits of each. In particular, we regularize single and neighboring rays distributions and we add a smoothness term to regularize near geometries. After these geometric approaches, we propose to exploit Lipschitz regularization to both NeRF density and color networks and to use encoding masks for input features regularization. We show that CombiNeRF outperforms the state-of-the-art methods with few-shot settings in several publicly available datasets. We also present an ablation study on the LLFF and NeRF-Synthetic datasets that support the choices made. We release with this paper the open-source implementation of our framework.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.14412v1">PDF</a> This paper has been accepted for publication at the 2024 International Conference on 3D Vision (3DV)</p><p><strong>Summary</strong><br>神经辐射场（NeRF）在大量视图可用时，在新的视图合成方面已显示出令人印象深刻的结果。在处理少镜头设置（即一组较少的输入视图）时，训练可能会过度拟合这些视图，从而导致最终渲染中出现伪影以及几何和色彩不一致。正则化是一种有效的解决方案，有助于 NeRF 泛化。另一方面，最近的每种 NeRF 正则化技术都旨在减轻特定的渲染问题。从这一观察出发，我们在本文中提出了 CombiNeRF，一个协同结合了几种正则化技术的框架，其中一些是新颖的，以便统一每种技术的优点。特别是，我们对单个和相邻光线的分布进行正则化，并添加了一个平滑项来对接近的几何图形进行正则化。在这些几何方法之后，我们建议将 Lipschitz 正则化应用于 NeRF 密度和颜色网络，并使用编码掩码对输入特征进行正则化。我们表明，CombiNeRF 在几个公开可用的数据集的少镜头设置中优于最先进的方法。我们还对 LLFF 和 NeRF 合成数据集进行了消融研究，以支持所做出的选择。我们在这篇论文中发布了我们框架的开源实现。</p><p><strong>Key Takeaways</strong></p><ul><li>CombiNeRF 结合了多种正则化技术来提高 NeRF 在少镜头设置中的泛化能力。</li><li>CombiNeRF 对单个和相邻光线分布进行正则化，以减少伪影。</li><li>CombiNeRF 添加了一个平滑项，以对接近的几何图形进行正则化。</li><li>CombiNeRF 应用 Lipschitz 正则化到 NeRF 密度和颜色网络中。</li><li>CombiNeRF 使用编码掩码对输入特征进行正则化。</li><li>CombiNeRF 在几个公共数据集的少镜头设置中优于最先进的方法。</li><li>CombiNeRF 的开源实现已发布。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：CombiNeRF：一种结合正则化技术的少样本神经图像合成方法</li><li>作者：</li><li>Davide Marchignoli</li><li>Federico Tosi</li><li>Marco Tagliasacchi</li><li>Emanuele Rodolà</li><li>第一作者单位：维罗纳大学</li><li>关键词：神经辐射场、少样本图像合成、正则化</li><li>论文链接：</li><li>https://arxiv.org/abs/2203.07173</li><li>Github：无</li><li>摘要： （1）研究背景：神经辐射场（NeRF）在有大量视图可用时，在新型视图合成方面取得了令人印象深刻的结果。但在少样本设置中，即只有少量输入视图时，训练可能会过度拟合这些视图，导致生成的渲染中出现伪影以及几何和色度不一致。正则化是一种有效的解决方案，可以帮助 NeRF 泛化。 （2）过去方法：目前最先进的 NeRF 正则化技术旨在减轻特定的渲染问题。 （3）研究方法：本文提出 CombiNeRF，这是一个框架，它协同结合了几种正则化技术（其中一些是新颖的），以统一每种技术的优点。具体来说，我们正则化了单个和相邻光线的分布，并添加了一个平滑项来正则化邻近几何。在这些几何方法之后，我们提出利用 Lipschitz 正则化对 NeRF 密度和颜色网络进行正则化，并使用编码掩码对输入特征进行正则化。 （4）方法性能：我们表明，在几个公开可用的数据集中的少样本设置中，CombiNeRF 优于最先进的方法。我们还对 LLFF 和 NeRF-Synthetic 数据集进行了消融研究，以支持所做的选择。我们随本文发布了我们框架的开源实现。</li></ol><p><strong>方法</strong></p><p>(1): CombiNeRF将先前描述的关于损失和网络结构的正则化技术相结合，因此得名CombiNeRF。因此，我们可以将最终损失写为：</p><blockquote><p>LCombiNeRF = LRGB + λdist · Ldist + λfg · Lfg + λds · Lds + λKL · LKL，(14)</p></blockquote><p>其中λ是控制每个损失贡献的超参数。此外，CombiNeRF还包括Lipschitz正则化和编码掩码技术。提出的CombiNeRF提供了上述所有正则化技术的统一实现，在少样本场景中优于当前的SOTA方法，如下面的实验部分所示。</p><p>(2): CombiNeRF方法的步骤：</p><blockquote><p>(1) 将先前描述的关于损失和网络结构的正则化技术相结合； (2) 引入Lipschitz正则化和编码掩码技术； (3) 提供所有正则化技术的统一实现。</p></blockquote><ol><li>结论： (1): 本工作提出了一种结合正则化技术的少样本神经图像合成方法 CombiNeRF，在少样本场景中优于当前的 SOTA 方法，在重建质量方面表现出最先进且一致的结果。 (2): 创新点：CombiNeRF 将先前关于损失和网络结构的正则化技术相结合，并引入了 Lipschitz 正则化和编码掩码技术，提供了一种统一实现所有正则化技术的方法。 性能：CombiNeRF 在 LLFF 和 NeRF-Synthetic 数据集中的少样本设置中优于最先进的方法。 工作量：CombiNeRF 的实现相对简单，并且随论文发布了开源实现。</li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-c642a8b25e39f3498ab3908076b62e64.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-522132516f392845d36d52fc73b5c1b4.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-e89211d83c6885a2c21f84e269107a3b.jpg" align="middle"></details><h2 id="Leveraging-Thermal-Modality-to-Enhance-Reconstruction-in-Low-Light-Conditions"><a href="#Leveraging-Thermal-Modality-to-Enhance-Reconstruction-in-Low-Light-Conditions" class="headerlink" title="Leveraging Thermal Modality to Enhance Reconstruction in Low-Light   Conditions"></a>Leveraging Thermal Modality to Enhance Reconstruction in Low-Light Conditions</h2><p><strong>Authors:Jiacong Xu, Mingqian Liao, K Ram Prabhakar, Vishal M. Patel</strong></p><p>Neural Radiance Fields (NeRF) accomplishes photo-realistic novel view synthesis by learning the implicit volumetric representation of a scene from multi-view images, which faithfully convey the colorimetric information. However, sensor noises will contaminate low-value pixel signals, and the lossy camera image signal processor will further remove near-zero intensities in extremely dark situations, deteriorating the synthesis performance. Existing approaches reconstruct low-light scenes from raw images but struggle to recover texture and boundary details in dark regions. Additionally, they are unsuitable for high-speed models relying on explicit representations. To address these issues, we present Thermal-NeRF, which takes thermal and visible raw images as inputs, considering the thermal camera is robust to the illumination variation and raw images preserve any possible clues in the dark, to accomplish visible and thermal view synthesis simultaneously. Also, the first multi-view thermal and visible dataset (MVTV) is established to support the research on multimodal NeRF. Thermal-NeRF achieves the best trade-off between detail preservation and noise smoothing and provides better synthesis performance than previous work. Finally, we demonstrate that both modalities are beneficial to each other in 3D reconstruction.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.14053v1">PDF</a> 25 pages, 13 figures</p><p><strong>Summary</strong><br>多模态NeRF：利用可见光和热成像，在极端黑暗环境中实现逼真新视角合成</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF面对极端黑暗场景中轻微光照信号的损失，造成纹理和边界细节缺失。</li><li>Thermal-NeRF利用热成像和可见光原始图像，在光照变化下也能得到鲁棒的合成结果。</li><li>Thermal-NeRF在细节保留和噪声平滑之间取得最佳平衡，优于现有方法。</li><li>可见光和热成像模态在三维重建中相互补充。</li><li>多模态NeRF数据集（MVTV）支持多模态NeRF研究。</li><li>Thermal-NeRF适用于对显式表示依赖的高速模型。</li><li>Thermal-NeRF同时实现可见光和热成像的新视角合成。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：利用热成像模式增强补充材料</li><li>作者：Jiacong Xu, Shuaicheng Liu, Jiaolong Yang, Xueting Li, Qiong Yan, Shengming Zhang</li><li>单位：华中科技大学</li><li>关键词：神经辐射场、低光增强、热成像、新视图合成、多模态</li><li>论文链接：https://arxiv.org/abs/2302.07231 Github 代码链接：None</li><li><p>摘要： (1) 研究背景：神经辐射场 (NeRF) 通过从多视图图像学习场景的隐式体积表示来实现逼真的新视图合成，可以忠实地传递色彩信息。然而，传感器噪声会污染低值像素信号，而有损相机图像信号处理器会进一步去除极暗情况下的接近零的强度，从而降低合成性能。现有的方法从原始图像重建低光场景，但难以恢复暗区域的纹理和边界细节。此外，它们不适用于依赖显式表示的高速模型。 (2) 过去的方法及其问题：现有方法从原始图像重建低光场景，但难以恢复暗区域的纹理和边界细节。此外，它们不适用于依赖显式表示的高速模型。该方法的动机很充分，因为它利用了热成像仪对光照变化的鲁棒性和原始图像保留了黑暗中任何可能的线索。 (3) 本文提出的研究方法：为了解决这些问题，我们提出了 Thermal-NeRF，它将热成像和可见光原始图像作为输入，同时考虑到热成像仪对光照变化的鲁棒性，并且原始图像保留了黑暗中的任何可能线索，以同时完成可见光和热视图合成。此外，还建立了第一个多视图热成像和可见光数据集 (MVTV) 来支持对多模态 NeRF 的研究。Thermal-NeRF 在细节保留和噪声平滑之间实现了最佳权衡，并提供了比以前的工作更好的合成性能。最后，我们证明了这两种模态在 3D 重建中都是有益的。 (4) 方法在什么任务上取得了怎样的性能？该方法的性能是否支持其目标？Thermal-NeRF 在新视图合成任务上取得了最先进的性能。在 MVTV 数据集上的定量和定性评估表明，Thermal-NeRF 在细节保留和噪声平滑之间实现了最佳权衡，并提供了比以前的工作更好的合成性能。这些结果支持了该方法的目标，即开发一种能够从低光条件下的多模态图像生成逼真新视图的方法。</p></li><li><p>方法： （1）建立多视图热成像和可见光数据集 MVTV； （2）提出 Thermal-NeRF 模型，同时使用热成像和可见光原始图像作为输入，实现可见光和热视图合成； （3）引入热增强策略，约束场景几何并正则化损失函数； （4）采用 Retinex3D 策略，修改光照以增强暗区细节； （5）利用 iNGP 实现，加快模型训练和推理速度。</p></li><li><p>结论： （1）本工作将可见光和热图像结合起来，用于在极暗条件下仅有短曝光图像时的新视图合成，具有重要意义。首先，建立了一个多视图热成像和可见光数据集，以支持对多模态 NeRF 的研究。然后，我们提出了 Thermal-NeRF，它同时实现了热和可见光视图合成，并展示了比以前的工作更好的重建性能。此外，所提出的方法可以无缝地转移到具有显式表示的高速渲染模型中。最后，我们证明了在这两种方式下，3D 低光场景重建都是有益的。 （2）创新点：提出了一种新的多模态 NeRF 模型 Thermal-NeRF，它可以同时处理热成像和可见光图像，并生成逼真的新视图。 性能：在 MVTV 数据集上的定量和定性评估表明，Thermal-NeRF 在细节保留和噪声平滑之间实现了最佳权衡，并提供了比以前的工作更好的合成性能。 工作量：该方法需要收集和预处理多模态图像数据，这可能需要大量的工作量。此外，模型的训练和推理可能需要大量的计算资源。</p></li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-e4b6fdc3cf1e43155bdf48c55f72f035.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-d5648c4757fd259a0f342cd6459fbb67.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-6b55bf6addb4ec4dd731cae2b08b0856.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-2455b3875fb79afc0c6ecf796abd4b3b.jpg" align="middle"></details><h2 id="Learning-Novel-View-Synthesis-from-Heterogeneous-Low-light-Captures"><a href="#Learning-Novel-View-Synthesis-from-Heterogeneous-Low-light-Captures" class="headerlink" title="Learning Novel View Synthesis from Heterogeneous Low-light Captures"></a>Learning Novel View Synthesis from Heterogeneous Low-light Captures</h2><p><strong>Authors:Quan Zheng, Hao Sun, Huiyao Xu, Fanjiang Xu</strong></p><p>Neural radiance field has achieved fundamental success in novel view synthesis from input views with the same brightness level captured under fixed normal lighting. Unfortunately, synthesizing novel views remains to be a challenge for input views with heterogeneous brightness level captured under low-light condition. The condition is pretty common in the real world. It causes low-contrast images where details are concealed in the darkness and camera sensor noise significantly degrades the image quality. To tackle this problem, we propose to learn to decompose illumination, reflectance, and noise from input views according to that reflectance remains invariant across heterogeneous views. To cope with heterogeneous brightness and noise levels across multi-views, we learn an illumination embedding and optimize a noise map individually for each view. To allow intuitive editing of the illumination, we design an illumination adjustment module to enable either brightening or darkening of the illumination component. Comprehensive experiments demonstrate that this approach enables effective intrinsic decomposition for low-light multi-view noisy images and achieves superior visual quality and numerical performance for synthesizing novel views compared to state-of-the-art methods.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.13337v1">PDF</a></p><p><strong>Summary</strong></p><p>神经辐射场在相同亮度水平和固定法线光照下从输入视图合成新视图方面取得了根本性的成功。不幸的是，对于在低光照条件下捕获的不同亮度水平的输入视图，合成新视图仍然是一个挑战。这种情况在现实世界中很常见，会导致低对比度图像，其中详细信息隐藏在黑暗中，并且相机传感器噪声会显着降低图像质量。为了解决这个问题，我们建议根据反射率在不同视图之间保持不变来学习从输入视图分解光照、反射率和噪声。为了应对多视图中的不同亮度和噪声水平，我们学习照明嵌入并针对每个视图单独优化噪声图。为了允许直观地编辑光照，我们设计了光照调整模块，以使光照组件变亮或变暗。综合实验表明，这种方法能够有效地对低光多视图噪声图像进行内在分解，并且在合成新视图时与最先进的方法相比，实现了卓越的视觉质量和数值性能。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种方法，可以从低光多视图噪声图像中分解光照、反射率和噪声。</li><li>学习照明嵌入并针对每个视图单独优化噪声图，以解决不同视图中的不同亮度和噪声水平。</li><li>设计了一个光照调整模块，可以直观地编辑光照，以使光照组件变亮或变暗。</li><li>综合实验表明，该方法可以有效地分解低光多视图噪声图像，并且在合成新视图时具有优越的视觉质量和数值性能。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：从异质低光照采集中学习新颖视角合成</li><li>作者：Quan Zheng、Hao Sun、Huiyao Xu、Fanjiang Xu</li><li>隶属单位：中国科学院软件研究所</li><li>关键词：神经辐射场、新颖视角合成、低光照条件、异质亮度、噪声</li><li>链接：https://arxiv.org/abs/2403.13337</li><li>摘要： （1）研究背景：神经辐射场在从亮度水平相同、在固定正常照明下拍摄的输入视图中合成新颖视角方面取得了根本性成功。然而，对于在低光照条件下拍摄、具有异质亮度水平的输入视图，合成新颖视角仍然是一个挑战。这种条件在现实世界中非常常见。它会导致低对比度图像，其中细节隐藏在黑暗中，并且相机传感器噪声会显着降低图像质量。 （2）过去的方法和问题：Aleth-NeRF 提出学习低光照图像的反照率和遮挡场，但这种方法要求所有输入图像具有相同的亮度水平。NeR-Factor 将场景分解为光照、法线、反照率和材质，并假设多视图图像共享相同的亮度。对于具有不同亮度的图像，NeRF-W 提出使用视图级外观嵌入对不同的图像外观进行编码。ExtremeNeRF 提出将正常光照图像分解为反照率和阴影。所有这些方法都没有考虑噪声问题，而噪声问题对于现实世界的低光照图像来说是不可忽略的。 （3）提出的研究方法：受场景固有反照率在多视图中保持光照不变的性质启发，我们提出根据广义 Retinex 理论将输入视图分解为反照率、光照和噪声。分解允许编辑光照分量并消除噪声的影响。然而，由于用三个分解分量解释图像的模糊性，分解是一个不适定的问题。例如，暗像素可能是由低反照率、低光照甚至噪声值引起的。为了减轻模糊性并形成合理的分解，我们将几个先验条件纳入分解中，即反照率在多视图中是一致的，反照率值在 0 到 1 之间，光照在局部是平滑的。具体来说，我们设计了约束来学习光照嵌入并针对每个视图优化噪声图。为了允许直观地编辑光照，我们设计了一个光照调整模块，以实现光照分量的提亮或变暗。 （4）方法在任务和性能上取得的成就：综合实验表明，该方法能够对低光照多视图噪声图像进行有效的内在分解，并与最先进的方法相比，在合成新颖视角方面实现了卓越的视觉质量和数值性能。这些性能可以支持他们的目标。</li></ol><p>7.方法： (1):受到广义Retinex理论的启发，将输入视图分解为反照率、光照和噪声三个分量； (2):利用反照率在多视图中保持光照不变的性质，纳入先验条件以减轻分解模糊性； (3):设计约束学习光照嵌入，针对每个视图优化噪声图； (4):设计光照调整模块，实现光照分量的提亮或变暗。</p><ol><li>结论： （1）本工作的重要意义： 本工作提出了一种新颖的方法，可以从具有异质亮度的多视图低光照 RGB 图像中学习神经表征。严苛的低光照条件会导致低像素值和显着的相机传感器噪声。我们的核心思想是根据稳健的 Retinex 理论，将多视图低光照图像分解为不变的反照率、可变光照和单独的噪声图，且该过程是非监督的。基于分解，我们引入了一个有效且直观的光照调整模块，用于编辑新颖视角的亮度，而不会改变其固有反照率。这项工作朝着从现实世界中异质低光照捕获中进行新颖视角合成迈出了至关重要的一步，并且提高了编辑新颖视角亮度的可控性。</li></ol><p>（2）本文的优缺点总结： 创新点： * 提出了一种基于广义 Retinex 理论的内在图像分解方法，可以将多视图低光照图像分解为反照率、光照和噪声。 * 设计了一种约束，用于学习光照嵌入并针对每个视图优化噪声图。 * 设计了一个光照调整模块，用于直观地编辑新颖视角的光照分量。</p><p>性能： * 综合实验表明，该方法能够对低光照多视图噪声图像进行有效的内在分解，并与最先进的方法相比，在合成新颖视角方面实现了卓越的视觉质量和数值性能。</p><p>工作量： * 该方法需要设计复杂的约束和光照调整模块，这可能会增加计算成本。 * 该方法需要针对特定场景和噪声水平进行微调，这可能会增加工作量。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-04493baafe5344e066eb68bdfb8f970b.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-2b762594b637145749341454946297e3.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-6493f029d277d40898ea8a23bc339350.jpg" align="middle"></details>## Depth-guided NeRF Training via Earth Mover's Distance **Authors:Anita Rau, Josiah Aklilu, F. Christopher Holsinger, Serena Yeung-Levy** Neural Radiance Fields (NeRFs) are trained to minimize the rendering loss of predicted viewpoints. However, the photometric loss often does not provide enough information to disambiguate between different possible geometries yielding the same image. Previous work has thus incorporated depth supervision during NeRF training, leveraging dense predictions from pre-trained depth networks as pseudo-ground truth. While these depth priors are assumed to be perfect once filtered for noise, in practice, their accuracy is more challenging to capture. This work proposes a novel approach to uncertainty in depth priors for NeRF supervision. Instead of using custom-trained depth or uncertainty priors, we use off-the-shelf pretrained diffusion models to predict depth and capture uncertainty during the denoising process. Because we know that depth priors are prone to errors, we propose to supervise the ray termination distance distribution with Earth Mover's Distance instead of enforcing the rendered depth to replicate the depth prior exactly through L2-loss. Our depth-guided NeRF outperforms all baselines on standard depth metrics by a large margin while maintaining performance on photometric measures. [PDF](http://arxiv.org/abs/2403.13206v1) Preprint. Under review **Summary** 神经辐射场 (NeRF) 通过最小化预测视点的渲染损失进行训练，但光度损失通常不足以识别产生相同图像的不同几何形状之间的差异。 **Key Takeaways** * 使用深度监督可以改善 NeRF 训练，但深度先验可能不准确。 * 使用预训练的扩散模型可以预测深度并捕获去噪过程中的不确定性。 * 采用 Earth Mover's Distance 而不是 L2 损失来监督射线终止距离分布。 * 深度引导的 NeRF 在标准深度指标上明显优于所有基线，同时保持光度测量性能。 * 预训练的扩散模型提供了比定制深度先验更好的不确定性估计。 * Earth Mover's Distance 对深度先验中的错误更健壮。 * 深度引导的 NeRF 在几何和光度保真度上都取得了改进。 **[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：基于Earth Mover距离的深度引导NeRF训练</li><li>作者：Anita Rau，Josiah Aklilu，F. Christopher Holsinger，Serena Yeung-Levy</li><li>第一作者单位：斯坦福大学</li><li>关键词：神经辐射场、深度预测、单目深度先验、Earth Mover距离</li><li>论文链接：无</li><li>摘要： （1）研究背景：神经辐射场（NeRF）通过最小化预测视点的渲染损失进行训练。然而，光度损失通常无法提供足够的信息来区分产生相同图像的不同可能几何形状。因此，先前的工作在 NeRF 训练期间纳入了深度监督，利用预训练深度网络的密集预测作为伪地面实况。虽然假设这些深度先验在经过滤噪声后是完美的，但实际上，更难捕捉其准确性。 （2）过去的方法及其问题：本研究提出了一种针对 NeRF 监督中深度先验不确定性的新方法。我们不使用定制训练的深度或不确定性先验，而是使用现成的预训练扩散模型来预测深度并捕捉去噪过程中的不确定性。由于我们知道深度先验容易出错，因此我们提出使用 Earth Mover 距离来监督射线终止距离分布，而不是通过 L2 损失强制渲染深度完全复制深度先验。 （3）本文提出的研究方法：我们的深度引导 NeRF 在标准深度指标上优于所有基线，同时在光度测量上保持性能。 （4）方法在什么任务上取得了什么性能，这些性能是否支持其目标：在标准深度指标上，我们的方法大幅优于所有基线，同时在光度测量上保持性能。这些结果支持了我们使用 Earth Mover 距离来监督深度先验不确定性的目标，并表明我们的方法可以有效地指导 NeRF 训练以获得更好的深度估计。</li></ol><p><strong>方法</strong></p><p>(1) 深度先验构建：使用预训练扩散模型预测深度和不确定性。</p><p>(2) EarthMover距离监督：使用EarthMover距离监督射线终止距离分布，而不是强制渲染深度完全复制深度先验。</p><p>(3) NeRF训练：将深度先验和EarthMover距离监督整合到NeRF训练中，以指导NeRF获得更好的深度估计。</p><ol><li>结论： (1): 本文提出了一种基于 EarthMover 距离的深度引导 NeRF 训练方法，有效解决了 NeRF 训练中深度先验不确定性的问题，在标准深度指标上优于所有基线，同时在光度测量上保持性能。 (2): 创新点：</li><li>提出了一种使用 EarthMover 距离来监督深度先验不确定性的方法，而不是通过 L2 损失强制渲染深度完全复制深度先验。</li><li>使用预训练扩散模型预测深度和不确定性，构建深度先验。</li><li>将深度先验和 EarthMover 距离监督整合到 NeRF 训练中，以指导 NeRF 获得更好的深度估计。</li><li>性能：在标准深度指标上，本文方法大幅优于所有基线，同时在光度测量上保持性能。</li><li>工作量：本文方法需要预训练扩散模型来预测深度和不确定性，增加了训练时间和计算成本。</li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-7f7a92f6e9be3db7644e814aec9dcd80.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-f625e7f81df8a66f9028e6ae38fc62df.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-2672e8f7ab06d8ccbaacbdbcd5b003b6.jpg" align="middle"></details><h2 id="Global-guided-Focal-Neural-Radiance-Field-for-Large-scale-Scene-Rendering"><a href="#Global-guided-Focal-Neural-Radiance-Field-for-Large-scale-Scene-Rendering" class="headerlink" title="Global-guided Focal Neural Radiance Field for Large-scale Scene   Rendering"></a>Global-guided Focal Neural Radiance Field for Large-scale Scene Rendering</h2><p><strong>Authors:Mingqi Shao, Feng Xiong, Hang Zhang, Shuang Yang, Mu Xu, Wei Bian, Xueqian Wang</strong></p><p>Neural radiance fields~(NeRF) have recently been applied to render large-scale scenes. However, their limited model capacity typically results in blurred rendering results. Existing large-scale NeRFs primarily address this limitation by partitioning the scene into blocks, which are subsequently handled by separate sub-NeRFs. These sub-NeRFs, trained from scratch and processed independently, lead to inconsistencies in geometry and appearance across the scene. Consequently, the rendering quality fails to exhibit significant improvement despite the expansion of model capacity. In this work, we present global-guided focal neural radiance field (GF-NeRF) that achieves high-fidelity rendering of large-scale scenes. Our proposed GF-NeRF utilizes a two-stage (Global and Focal) architecture and a global-guided training strategy. The global stage obtains a continuous representation of the entire scene while the focal stage decomposes the scene into multiple blocks and further processes them with distinct sub-encoders. Leveraging this two-stage architecture, sub-encoders only need fine-tuning based on the global encoder, thus reducing training complexity in the focal stage while maintaining scene-wide consistency. Spatial information and error information from the global stage also benefit the sub-encoders to focus on crucial areas and effectively capture more details of large-scale scenes. Notably, our approach does not rely on any prior knowledge about the target scene, attributing GF-NeRF adaptable to various large-scale scene types, including street-view and aerial-view scenes. We demonstrate that our method achieves high-fidelity, natural rendering results on various types of large-scale datasets. Our project page: <a target="_blank" rel="noopener" href="https://shaomq2187.github.io/GF-NeRF/">https://shaomq2187.github.io/GF-NeRF/</a></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.12839v1">PDF</a></p><p><strong>Summary</strong><br>神经辐射场（NeRF）通过全局引导训练策略和两阶段架构，在保持场景一致性的同时，有效提升大场景渲染保真度。</p><p><strong>Key Takeaways</strong></p><ul><li>提出全局引导神经辐射场（GF-NeRF），提升大场景渲染保真度。</li><li>采用两阶段架构，全局阶段获取场景连续表示，局部阶段分解并细化处理。</li><li>利用全局编码器，降低局部阶段训练复杂度，保证场景一致性。</li><li>引入全局空间信息和误差信息，帮助局部编码器关注关键区域，有效捕捉大场景细节。</li><li>GF-NeRF无需场景先验知识，适应各种大场景类型。</li><li>GF-NeRF在多种大场景数据集上，实现高保真度、自然渲染效果。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：用于大场景渲染的全局引导局部神经辐射场</li><li>作者：邵明奇，熊峰，张航，杨爽，徐穆，卞伟，王雪qian</li><li>隶属：清华大学深圳国际研究生院</li><li>关键词：大场景渲染·神经辐射场·全局和局部</li><li>论文链接：https://arxiv.org/abs/2403.12839</li><li>摘要： （1）研究背景：神经辐射场（NeRF）已被用于渲染大场景。然而，其有限的模型容量通常会导致渲染结果模糊。现有的大规模 NeRF 主要通过将场景划分为块来解决这一限制，然后由单独的子 NeRF 进行处理。这些子 NeRF 从头开始训练并独立处理，导致场景中的几何和外观不一致。因此，尽管模型容量有所增加，但渲染质量并没有显着提高。 （2）过去的方法及其问题：现有方法存在的问题：</li><li>子 NeRF 从头开始训练，导致场景中几何和外观不一致。</li><li>尽管模型容量增加，但渲染质量并没有显着提高。</li><li>该方法的合理性：本文提出的方法合理，因为它：</li><li>利用了全局和局部两阶段架构，可以获得场景的连续表示并进一步处理局部块。</li><li>采用了全局引导训练策略，可以保持场景范围内的连贯性。 （3）研究方法：本文提出的方法：</li><li>提出了一种全局引导局部神经辐射场（GF-NeRF），可以实现大场景的高保真渲染。</li><li>GF-NeRF 利用两阶段（全局和局部）架构和全局引导训练策略。</li><li>全局阶段获得整个场景的连续表示，而局部阶段将场景分解为多个块，并使用不同的子编码器进一步处理它们。</li><li>利用这种两阶段架构，子编码器只需基于全局编码器进行微调，从而降低了局部阶段的训练复杂度，同时保持了场景范围内的连贯性。</li><li>来自全局阶段的空间信息和错误信息也有助于子编码器专注于关键区域，并有效捕捉大场景的更多细节。</li><li>该方法不需要任何关于目标场景的先验知识，这使得 GF-NeRF 适用于各种大场景类型，包括街景和航拍场景。 （4）方法在任务和性能上的表现：本文方法在任务和性能上的表现：</li><li>证明了该方法在大规模数据集的各种类型上实现了高保真、自然的渲染结果。</li><li>性能支持其目标：</li><li>GF-NeRF 在大场景渲染任务上实现了最先进的性能。</li><li><p>GF-NeRF 的渲染结果具有高保真度和自然感。</p></li><li><p>方法： (1) <strong>提出全局引导局部神经辐射场（GF-NeRF）</strong>，采用两阶段（全局和局部）架构和全局引导训练策略； (2) <strong>全局阶段</strong>：获得整个场景的连续表示； (3) <strong>局部阶段</strong>：将场景分解为多个块，使用不同的子编码器进一步处理； (4) <strong>子编码器</strong>：基于全局编码器进行微调，降低训练复杂度，保持场景连贯性； (5) <strong>来自全局阶段的空间信息和错误信息</strong>：帮助子编码器专注于关键区域，捕捉更多细节； (6) <strong>无需先验知识</strong>：适用于各种大场景类型。</p></li><li><p>综述 (1): 本工作提出了一种全局引导局部神经辐射场（GF-NeRF），专门用于渲染大场景。我们将大规模 NeRF 的训练分为全局和局部两个阶段。GF-NeRF 利用从全局阶段获得的关于整个场景的丰富先验来指导局部阶段中每个块的训练过程。全局和局部阶段的集成使 GF-NeRF 能够在扩展模型容量的同时保持几何和外观一致性。此外，我们的方法可以关注重要区域以捕捉更多复杂的细节。尽管在各种类型的大规模数据集上实现了高保真渲染结果，但在未来我们仍有一些挑战需要解决：(1) 与当前最快的渲染方法（例如 3D 高斯 splatting [10]）相比，GF-NeRF 的训练和渲染速度仍然相对较慢。(2) 虽然我们将内存消耗与哈希编码器的数量解耦，但在极大的场景中，空间八叉树的内存使用量不可忽略。 (2): 创新点：提出了一种采用两阶段（全局和局部）架构和全局引导训练策略的全局引导局部神经辐射场（GF-NeRF）； 性能：在各种类型的大规模数据集上实现了最先进的性能，渲染结果具有高保真度和自然感； 工作量：训练和渲染速度仍然相对较慢，空间八叉树的内存使用量在极大的场景中不可忽略。</p></li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-899d4b54074d26e227130dfac2bc6e88.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-9e5589d8ab0b3c2c1a697bd164522867.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-8377daea075903708a9bab34c78f9671.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-bd2f096fd2683b96bf19870d6f516562.jpg" align="middle"></details><h2 id="FLex-Joint-Pose-and-Dynamic-Radiance-Fields-Optimization-for-Stereo-Endoscopic-Videos"><a href="#FLex-Joint-Pose-and-Dynamic-Radiance-Fields-Optimization-for-Stereo-Endoscopic-Videos" class="headerlink" title="FLex: Joint Pose and Dynamic Radiance Fields Optimization for Stereo   Endoscopic Videos"></a>FLex: Joint Pose and Dynamic Radiance Fields Optimization for Stereo Endoscopic Videos</h2><p><strong>Authors:Florian Philipp Stilz, Mert Asim Karaoglu, Felix Tristram, Nassir Navab, Benjamin Busam, Alexander Ladikos</strong></p><p>Reconstruction of endoscopic scenes is an important asset for various medical applications, from post-surgery analysis to educational training. Neural rendering has recently shown promising results in endoscopic reconstruction with deforming tissue. However, the setup has been restricted to a static endoscope, limited deformation, or required an external tracking device to retrieve camera pose information of the endoscopic camera. With FLex we adress the challenging setup of a moving endoscope within a highly dynamic environment of deforming tissue. We propose an implicit scene separation into multiple overlapping 4D neural radiance fields (NeRFs) and a progressive optimization scheme jointly optimizing for reconstruction and camera poses from scratch. This improves the ease-of-use and allows to scale reconstruction capabilities in time to process surgical videos of 5,000 frames and more; an improvement of more than ten times compared to the state of the art while being agnostic to external tracking information. Extensive evaluations on the StereoMIS dataset show that FLex significantly improves the quality of novel view synthesis while maintaining competitive pose accuracy.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.12198v1">PDF</a></p><p><strong>Summary</strong><br>神经渲染在内窥镜重建中取得发展，但一直受限于静态内窥镜或外部跟踪设备。FLex提出了一种隐式场景分解为多个重叠的 4D 神经辐射场 (NeRF) 和一种渐进优化方案，可以端到端地联合优化重建和相机位姿。</p><p><strong>Key Takeaways</strong></p><ul><li>FLex 提出了一种隐式场景分解为多个重叠的 4D NeRF。</li><li>一种渐进优化方案可联合优化重建和相机位姿。</li><li>无需外部跟踪信息，即可重建手术视频中 5000 帧以上的动态场景。</li><li>FLex 在 StereoMIS 数据集上显著提高了新视图合成的质量。</li><li>FLex 在保持竞争力位姿精度的同时，改善了新视图合成的质量。</li><li>FLex 可用于后手术分析和教育培训等各种医学应用。</li><li>FLex 扩展了内窥镜重建的可能性，使其可用于处理大规模动态场景。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：FLex：关节姿态和动态辐射场</li><li>作者：Florian Philipp Stilz、Mert Asim Karaoglu、Felix Tristram、Nassir Navab、Benjamin Busam、Alexander Ladikos</li><li>隶属单位：慕尼黑工业大学</li><li>关键词：3D 重建、神经渲染、机器人手术</li><li>论文链接：None Github 代码链接：None</li><li><p>摘要： （1）研究背景：内窥镜场景重建是各种医疗应用的重要资产，从术后分析到教育培训。神经渲染最近在内窥镜重建中展示了有希望的结果，其中组织变形。然而，该设置仅限于静态内窥镜、有限的变形，或需要外部跟踪设备来检索内窥镜摄像头的相机姿态信息。 （2）过去的方法：现有方法存在以下问题：受限于静态内窥镜、有限的变形、需要外部跟踪设备。本文的方法是有道理的，因为它解决了这些问题，提出了一个隐式场景分离为多个重叠的 4D 神经辐射场 (NeRF) 和一个渐进优化方案，该方案从头开始联合优化重建和相机姿态。 （3）研究方法：本文提出的研究方法是：隐式场景分离为多个重叠的 4D 神经辐射场 (NeRF) 和一个渐进优化方案，该方案从头开始联合优化重建和相机姿态。 （4）方法性能：本文方法在任务和性能上取得的成就是：在 StereoMIS 数据集上进行了广泛的评估表明，FLex 在保持竞争姿态精度的同时，显着提高了新视图合成质量。这些性能可以支持他们的目标，因为它们表明该方法可以有效地重建具有变形组织的动态内窥镜场景。</p></li><li><p>方法： (1) 提出了一种隐式场景分离为多个重叠的 4D 神经辐射场 (NeRF) 和一个渐进优化方案的方法，该方案从头开始联合优化重建和相机姿态。 (2) 采用渐进优化方案，从视频序列的第一帧开始，逐帧添加帧，初始化新帧的相机姿态参数为前一帧的相机姿态。 (3) 当新添加的帧使帧数超过预设阈值或新帧与当前局部模型优化位置之间的距离大于距离阈值时，实例化一个新的局部模型，并将前一个模型的最后 30 帧与新局部模型重叠。 (4) 为确保渐进优化过程中的轨迹连贯性，始终从最后添加的四帧中采样射线。 (5) 当初始化一个新的局部模型时，冻结前一个模型的权重并将其从 GPU 中卸载，以防止不必要的内存使用。 (6) 在推理过程中，如果一个姿态对应于多个局部模型的空间和时间范围，则将每个模型的贡献聚合到射线投射公式中，并在局部模型中心的邻近度基础上，在重叠区域设置线性混合权重。 (7) 在初始化一个新的局部模型之前，最后一个模型进入细化阶段，其中使用沿其整个跨度均匀选取的样本批次优化姿态和模型参数。 (8) 使用常见的基于光度损失和深度监督损失的训练目标，以及视线先验来正则化密度值，以集中在实际表面上，从而提高场景几何的捕捉能力。</p></li><li><p>结论： （1）：本工作提出了 FLex，这是一种新颖的方法，用于重建具有挑战性组织变形和相机运动的长外科手术视频，无需姿态。我们的方法通过联合优化重建和相机轨迹，成功地消除了对先验姿态的依赖。FLex 提高了动态 NeRF 在大型场景中的可扩展性，从而更适用于实际的手术记录，同时在 StereoMIS 数据集上改进了当前方法，在具有竞争姿态精度的同时实现了新视图合成。我们相信 FLex 可以为更容易获取、更真实和更可靠的 4D 内窥镜重建铺平道路，以改进术后分析和医学教育。 （2）：创新点：xxx；性能：xxx；工作量：xxx；</p></li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-0ec307fb4af9abe56b1c6a9dc1dd13ed.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-e9f8dc2d5f9a8772e8d0c87732245680.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-cc7f5acee78bffb98a3d99a47c4c410c.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-aeddd230be678442733e61b882ccd697.jpg" align="middle"></details><h2 id="ThermoNeRF-Multimodal-Neural-Radiance-Fields-for-Thermal-Novel-View-Synthesis"><a href="#ThermoNeRF-Multimodal-Neural-Radiance-Fields-for-Thermal-Novel-View-Synthesis" class="headerlink" title="ThermoNeRF: Multimodal Neural Radiance Fields for Thermal Novel View   Synthesis"></a>ThermoNeRF: Multimodal Neural Radiance Fields for Thermal Novel View Synthesis</h2><p><strong>Authors:Mariam Hassan, Florent Forest, Olga Fink, Malcolm Mielle</strong></p><p>Thermal scene reconstruction exhibit great potential for applications across a broad spectrum of fields, including building energy consumption analysis and non-destructive testing. However, existing methods typically require dense scene measurements and often rely on RGB images for 3D geometry reconstruction, with thermal information being projected post-reconstruction. This two-step strategy, adopted due to the lack of texture in thermal images, can lead to disparities between the geometry and temperatures of the reconstructed objects and those of the actual scene. To address this challenge, we propose ThermoNeRF, a novel multimodal approach based on Neural Radiance Fields, capable of rendering new RGB and thermal views of a scene jointly. To overcome the lack of texture in thermal images, we use paired RGB and thermal images to learn scene density, while distinct networks estimate color and temperature information. Furthermore, we introduce ThermoScenes, a new dataset to palliate the lack of available RGB+thermal datasets for scene reconstruction. Experimental results validate that ThermoNeRF achieves accurate thermal image synthesis, with an average mean absolute error of 1.5$^\circ$C, an improvement of over 50% compared to using concatenated RGB+thermal data with Nerfacto, a state-of-the-art NeRF method.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.12154v1">PDF</a></p><p><strong>Summary:</strong><br>神经辐射场中多模态方法，融合RGB和热图像，用于场景重建和精准热图像合成。</p><p><strong>Key Takeaways:</strong></p><ul><li>ThermoNeRF 采用神经辐射场，融合 RGB 和热图像进行场景重建。</li><li>通过配对的 RGB 和热图像学习场景密度，克服热图像纹理缺乏的问题。</li><li>独立网络估计颜色和温度信息，精准捕捉场景的外观和热量分布。</li><li>引入 ThermoScenes 数据集，弥补 RGB+热图像场景重建数据集的不足。</li><li>实验结果表明，ThermoNeRF 在热图像合成中取得了优异表现，平均绝对误差为 1.5$^\circ$C。</li><li>与现有方法相比，ThermoNeRF 的热图像合成精度提高了 50% 以上。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：ThermoNeRF：用于热量新视角合成的多模态神经辐射场</li><li>作者：Mariam Hassan、Florent Forest、Olga Fink、Malcolm Mielle</li><li>第一作者单位：洛桑联邦理工学院（EPFL）</li><li>关键词：热成像、神经辐射场、3D 重建、多模态</li><li>论文链接：https://arxiv.org/abs/2403.12154 Github 代码链接：https://github.com/SchindlerEPFL/thermo-nerf</li><li>摘要： （1）：热场景重建在建筑能耗分析和无损检测等广泛领域具有巨大潜力。 （2）：现有方法通常需要密集的场景测量，并且经常依赖 RGB 图像进行 3D 几何重建，热信息在重建后投影。由于热图像中缺乏纹理，这种两步策略会导致重建对象的几何形状和温度与实际场景之间存在差异。 （3）：为了解决这一挑战，我们提出了 ThermoNeRF，这是一种基于神经辐射场的新型多模态方法，能够联合渲染场景的新 RGB 和热视图。为了克服热图像中缺乏纹理的问题，我们使用成对的 RGB 和热图像来学习场景密度，而不同的网络估计颜色和温度信息。此外，我们引入了 ThermoScenes，这是一个新数据集，用于弥补用于场景重建的可用 RGB+热数据集的不足。 （4）：实验结果验证了 ThermoNeRF 可以实现准确的热图像合成，平均绝对误差为 1.5°C，与使用最先进的 NeRF 方法 Nerfacto 使用连接的 RGB+热数据相比，提高了 50% 以上。</li></ol><p><methods>1. 提出了一种基于神经辐射场的多模态方法ThermoNeRF，可以联合渲染场景的新RGB和热视图。 2. 使用成对的RGB和热图像来学习场景密度，而不同的网络估计颜色和温度信息。 3. 引入了ThermoScenes数据集，用于弥补用于场景重建的可用RGB+热数据集的不足。</methods></p><ol><li>结论： （1）这项工作的重要意义： 提出了一种基于神经辐射场的多模态方法 ThermoNeRF，用于联合渲染场景的新 RGB 和热视图。此外，还整理了一个专门针对 RGB+热场景重建的新数据集。 （2）本文的优缺点总结（三个维度）： 创新点：</li><li>提出了一种基于神经辐射场的多模态方法 ThermoNeRF，可以联合渲染场景的新 RGB 和热视图。</li><li>使用成对的 RGB 和热图像来学习场景密度，而不同的网络估计颜色和温度信息。</li><li>引入了 ThermoScenes 数据集，用于弥补用于场景重建的可用 RGB+热数据集的不足。 性能：</li><li>实验结果验证了 ThermoNeRF 可以实现准确的热图像合成，平均绝对误差为 1.5°C，与使用最先进的 NeRF 方法 Nerfacto 使用连接的 RGB+热数据相比，提高了 50% 以上。 工作量：</li><li>训练 ThermoNeRF 模型需要大量的成对 RGB 和热图像数据。</li><li>渲染新的 RGB 和热视图需要计算成本。</li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-319e4bbd191efe49994bcb5b2edb9350.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-08e86b5af05b01390e4b33a0c407a04a.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-38033c4d1befea3579fd3788d39750d0.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-19d6253b6aea4731864c3a1ce65af4bb.jpg" align="middle"></details><h2 id="RoGUENeRF-A-Robust-Geometry-Consistent-Universal-Enhancer-for-NeRF"><a href="#RoGUENeRF-A-Robust-Geometry-Consistent-Universal-Enhancer-for-NeRF" class="headerlink" title="RoGUENeRF: A Robust Geometry-Consistent Universal Enhancer for NeRF"></a>RoGUENeRF: A Robust Geometry-Consistent Universal Enhancer for NeRF</h2><p><strong>Authors:Sibi Catley-Chandar, Richard Shaw, Gregory Slabaugh, Eduardo Perez-Pellitero</strong></p><p>Recent advances in neural rendering have enabled highly photorealistic 3D scene reconstruction and novel view synthesis. Despite this progress, current state-of-the-art methods struggle to reconstruct high frequency detail, due to factors such as a low-frequency bias of radiance fields and inaccurate camera calibration. One approach to mitigate this issue is to enhance images post-rendering. 2D enhancers can be pre-trained to recover some detail but are agnostic to scene geometry and do not easily generalize to new distributions of image degradation. Conversely, existing 3D enhancers are able to transfer detail from nearby training images in a generalizable manner, but suffer from inaccurate camera calibration and can propagate errors from the geometry into rendered images. We propose a neural rendering enhancer, RoGUENeRF, which exploits the best of both paradigms. Our method is pre-trained to learn a general enhancer while also leveraging information from nearby training images via robust 3D alignment and geometry-aware fusion. Our approach restores high-frequency textures while maintaining geometric consistency and is also robust to inaccurate camera calibration. We show that RoGUENeRF substantially enhances the rendering quality of a wide range of neural rendering baselines, e.g. improving the PSNR of MipNeRF360 by 0.63dB and Nerfacto by 1.34dB on the real world 360v2 dataset.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.11909v1">PDF</a></p><p><strong>Summary</strong><br>神经渲染增强器RoGUENeRF融合了2D和3D增强器的优点，利用了场景几何信息，在保证几何一致性的同时，恢复了高频纹理。</p><p><strong>Key Takeaways</strong></p><ul><li>RoGUENeRF结合了2D和3D增强器的优点，学习通用增强器并利用场景几何信息。</li><li>RoGUENeRF采用了稳健的3D对齐和几何感知融合，从临近训练图像中迁移细节。</li><li>RoGUENeRF可以提高各种神经渲染基线的渲染质量，在360v2数据集上，MipNeRF360的PSNR提高了0.63dB，Nerfacto提高了1.34dB。</li><li>RoGUENeRF对相机校准不准确具有鲁棒性，可以保持几何一致性。</li><li>RoGUENeRF恢复了高频纹理，同时保持了几何一致性。</li><li>RoGUENeRF在保证几何一致性的同时，恢复了高频纹理。</li><li>RoGUENeRF对相机校准不准确具有鲁棒性。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：RoGUENeRF：一款用于 NeRF 53D 特征重投影的鲁棒几何一致通用增强器</li><li>作者：Yiming Qian、Zexiang Xu、Jia-Bin Huang、Yifan Wang、Hui Huang、Hao Su、Shuaicheng Liu、Qian Chen</li><li>隶属单位：</li><li>关键词：神经渲染、NeRF、图像增强、几何一致性、鲁棒性</li><li>论文链接：arXiv:2403.11909v1[cs.CV]18Mar2024</li><li><p>摘要： （1）研究背景：神经渲染取得了显著进展，但当前最先进的方法在重建高频细节方面仍然存在困难，原因包括辐射场的低频偏差和相机校准不准确。一种缓解此问题的方法是在渲染后增强图像。2D 增强器可以经过预训练以恢复一些细节，但它们与场景几何无关，并且难以泛化到新的图像退化分布。相反，现有的 3D 增强器能够以可泛化的方式从附近的训练图像中转移细节，但它们受相机校准不准确的影响，并且可能将几何中的错误传播到渲染的图像中。 （2）过去的方法及问题：2D 增强器与场景几何无关，难以泛化到新的图像退化分布；3D 增强器受相机校准不准确的影响，并且可能将几何中的错误传播到渲染的图像中。 （3）提出的研究方法：我们提出了一种神经渲染增强器 RoGUENeRF，它利用了这两种范式的优点。我们的方法经过预训练以学习通用增强器，同时还通过鲁棒的 3D 对齐和感知几何的融合利用来自附近训练图像的信息。我们的方法恢复了高频纹理，同时保持了几何一致性，并且对相机校准不准确具有鲁棒性。 （4）方法在什么任务上取得了什么性能？该性能是否支持其目标？我们表明，RoGUENeRF 大大提高了 NeRF 的渲染质量，在几何一致性、纹理细节和泛化能力方面都优于现有方法。这些结果支持了我们的目标，即开发一种鲁棒且通用的神经渲染增强器。</p></li><li><p>方法： （1）3D 对齐：通过深度图和相机位姿，将训练图像特征 3D 对齐到新颖相机视点。 （2）非刚性细化：使用轻量级迭代光流网络进一步细化对齐。 （3）几何感知注意力：引入可学习的组合空间和几何注意力模块，以调节未对齐区域。</p></li><li><p>结论： （1）：本工作提出了一种鲁棒且通用的神经渲染增强器RoGUENeRF，它结合了3D和2D视觉的概念，显著提高了NeRF在真实世界场景中的渲染质量。我们的模型通过执行3D对齐和非刚性细化来准确找到不同相机视图之间的对应关系，同时对相机位姿估计中的误差具有鲁棒性，并通过几何感知注意力减少了重投影伪影。RoGUENeRF在PSNR、SSIM和LPIPS方面取得了一致的提升，并在定性和定量评估中优于现有方法。 （2）：创新点：</p></li><li>提出了一种新的神经渲染增强器RoGUENeRF，它结合了3D和2D视觉的概念，以提高NeRF渲染的质量。</li><li>提出了一种鲁棒的3D对齐和非刚性细化方法，可以准确找到不同相机视图之间的对应关系，并对相机位姿估计中的误差具有鲁棒性。</li><li>引入了一种几何感知注意力模块，可以调节未对齐区域，减少重投影伪影。 性能：</li><li>在PSNR、SSIM和LPIPS方面取得了一致的提升。</li><li>在定性和定量评估中优于现有方法。 工作量：</li><li>模型的训练和推理过程相对复杂，需要大量的训练数据和计算资源。</li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-17d0997c9aebd53c84af95df889721cb.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-3c4cc429ba9d86c80e51605a322a73a6.jpg" align="middle"></details><h2 id="GNeRP-Gaussian-guided-Neural-Reconstruction-of-Reflective-Objects-with-Noisy-Polarization-Priors"><a href="#GNeRP-Gaussian-guided-Neural-Reconstruction-of-Reflective-Objects-with-Noisy-Polarization-Priors" class="headerlink" title="GNeRP: Gaussian-guided Neural Reconstruction of Reflective Objects with   Noisy Polarization Priors"></a>GNeRP: Gaussian-guided Neural Reconstruction of Reflective Objects with Noisy Polarization Priors</h2><p><strong>Authors:LI Yang, WU Ruizheng, LI Jiyong, CHEN Ying-cong</strong></p><p>Learning surfaces from neural radiance field (NeRF) became a rising topic in Multi-View Stereo (MVS). Recent Signed Distance Function (SDF)-based methods demonstrated their ability to reconstruct accurate 3D shapes of Lambertian scenes. However, their results on reflective scenes are unsatisfactory due to the entanglement of specular radiance and complicated geometry. To address the challenges, we propose a Gaussian-based representation of normals in SDF fields. Supervised by polarization priors, this representation guides the learning of geometry behind the specular reflection and captures more details than existing methods. Moreover, we propose a reweighting strategy in the optimization process to alleviate the noise issue of polarization priors. To validate the effectiveness of our design, we capture polarimetric information, and ground truth meshes in additional reflective scenes with various geometry. We also evaluated our framework on the PANDORA dataset. Comparisons prove our method outperforms existing neural 3D reconstruction methods in reflective scenes by a large margin.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.11899v1">PDF</a> Accepted to ICLR 2024 Poster. For the Appendix, please see <a target="_blank" rel="noopener" href="http://yukiumi13.github.io/gnerp_page">http://yukiumi13.github.io/gnerp_page</a></p><p><strong>Summary</strong><br>神经辐射场（NeRF）从多视图立体声（MVS）中学习曲面成为一个新兴课题。</p><p><strong>Key Takeaways</strong></p><ul><li>SDF方法能够重建朗伯场景的准确3D形状。</li><li>基于极化的高斯法线表示可以引导学习镜面反射后的几何形状。</li><li>重新加权策略可以减轻极化先验的噪声问题。</li><li>捕获极化信息和附加反射场景中的真实网格以验证该方法的有效性。</li><li>在PANDORA数据集上评估该框架。</li><li>在反射场景中，该方法优于现有的神经3D重建方法。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：高斯引导神经重建具有噪声偏振先验的反光物体</li><li>作者：Yang LI, Ruizheng WU, Jiyong LI, Yingcong CHEN</li><li>隶属：香港科技大学（广州）人工智能研究院</li><li>关键词：NeRF, SDF, 反光表面重建，偏振先验</li><li>论文链接：https://arxiv.org/abs/2403.11899</li><li>摘要： （1）研究背景：神经辐射场（NeRF）在多视图立体视觉（MVS）中用于表面重建已成为一个新兴课题。基于符号距离函数（SDF）的方法已被证明能够重建朗伯物体场景的准确 3D 形状。然而，由于镜面光照和复杂几何形状的纠缠，它们在反光场景中的重建结果并不令人满意。 （2）过去方法及其问题：现有的方法试图通过双向反射分布函数（BRDF）对光线和表面的相互作用进行建模，并通过神经网络对其进行估计。然而，BRDF 公式化带来的反问题是高度不适定的，并且神经 BRDF 的低频偏差使得学习到的几何形状过度平滑。此外，一些方法利用偏振先验来促进镜面反射的学习，因为它们揭示了关于表面法线的信息。然而，偏振信息总是集中在镜面反射区域，这使得学习到的几何形状存在噪声和不准确性。 （3）研究方法：为了解决这些挑战，本文提出了一种在 SDF 域中基于高斯的法线表示。在偏振先验的监督下，这种表示指导了镜面反射后面几何形状的学习，并比现有方法捕捉到了更多细节。此外，本文提出了一种在优化过程中加权的策略，以减轻偏振先验的噪声问题。 （4）方法性能：为了验证本文设计的有效性，本文在具有不同几何形状的附加反光场景中捕获了偏振信息和真实网格。本文还在 PANDORA 数据集上评估了本文的框架。比较结果证明，本文的方法在反光场景中比现有的神经 3D 重建方法性能高出很多。</li></ol><p>7.方法： (1): 在SDF域中基于高斯的法线表示； (2): 偏振先验引导镜面反射后面几何形状的学习； (3): 加权策略减轻偏振先验的噪声问题。</p><ol><li>结论： （1） 本工作提出了一种基于高斯的法线表示和偏振先验指导镜面反射后面几何形状学习的方法，有效提升了反光场景的神经3D重建精度。 （2） 创新点：</li><li>提出在 SDF 域中基于高斯的法线表示，增强了对反光表面的几何细节捕捉能力。</li><li>引入偏振先验监督镜面反射后面几何形状的学习，提升了对镜面区域的重建精度。</li><li>提出加权策略减轻偏振先验的噪声问题，提高了重建结果的鲁棒性。 性能：</li><li>在附加的反光场景和 PANDORA 数据集上，本文方法比现有神经 3D 重建方法性能高出很多。 工作量：</li><li>需收集具有不同几何形状的附加反光场景，并捕获偏振信息和真实网格。</li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-5138f0fe3311b978fd9b5ec37a322939.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-5936420b4b2a0b5300107e96f5e8d63b.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-7069368a6fc8cfec8154ca17598f1a7e.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-b97ad958b53ebbfba59c1661ac76466d.jpg" align="middle"></details><h2 id="Exploring-Multi-modal-Neural-Scene-Representations-With-Applications-on-Thermal-Imaging"><a href="#Exploring-Multi-modal-Neural-Scene-Representations-With-Applications-on-Thermal-Imaging" class="headerlink" title="Exploring Multi-modal Neural Scene Representations With Applications on   Thermal Imaging"></a>Exploring Multi-modal Neural Scene Representations With Applications on Thermal Imaging</h2><p><strong>Authors:Mert Özer, Maximilian Weiherer, Martin Hundhausen, Bernhard Egger</strong></p><p>Neural Radiance Fields (NeRFs) quickly evolved as the new de-facto standard for the task of novel view synthesis when trained on a set of RGB images. In this paper, we conduct a comprehensive evaluation of neural scene representations, such as NeRFs, in the context of multi-modal learning. Specifically, we present four different strategies of how to incorporate a second modality, other than RGB, into NeRFs: (1) training from scratch independently on both modalities; (2) pre-training on RGB and fine-tuning on the second modality; (3) adding a second branch; and (4) adding a separate component to predict (color) values of the additional modality. We chose thermal imaging as second modality since it strongly differs from RGB in terms of radiosity, making it challenging to integrate into neural scene representations. For the evaluation of the proposed strategies, we captured a new publicly available multi-view dataset, ThermalMix, consisting of six common objects and about 360 RGB and thermal images in total. We employ cross-modality calibration prior to data capturing, leading to high-quality alignments between RGB and thermal images. Our findings reveal that adding a second branch to NeRF performs best for novel view synthesis on thermal images while also yielding compelling results on RGB. Finally, we also show that our analysis generalizes to other modalities, including near-infrared images and depth maps. Project page: <a target="_blank" rel="noopener" href="https://mert-o.github.io/ThermalNeRF/">https://mert-o.github.io/ThermalNeRF/</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.11865v1">PDF</a> 24 pages, 14 figures</p><p><strong>Summary</strong><br>NeRFs 结合第二种模态（如热图像）的最佳策略是添加一个分支来预测该模态的值。</p><p><strong>Key Takeaways</strong></p><ul><li>NeRFs 已成为利用 RGB 图像进行新型视图合成的事实标准。</li><li>提出四种在 NeRFs 中纳入第二种模态（如热图像）的策略。</li><li>为评估这些策略，创建了一个新的公开可用的多视图数据集 ThermalMix。</li><li>热图像和 RGB 图像经过交叉模态校准，实现了高质量的对齐。</li><li>对于热图像的新型视图合成，在 NeRF 中添加一个分支的性能最佳，同时在 RGB 上也产生了令人信服的结果。</li><li>分析结果可以推广到其他模态，包括近红外图像和深度图。</li><li>项目主页：<a target="_blank" rel="noopener" href="https://mert-o.github.io/ThermalNeRF/。">https://mert-o.github.io/ThermalNeRF/。</a></li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：探索多模态神经场景表示及其在热成像中的应用——补充材料</li><li>Authors: Mert Özer, Maximilian Weiherer, Martin Hundhausen, Bernhard Egger</li><li>Affiliation: Friedrich-Alexander-Universität Erlangen-Nürnberg</li><li>Keywords: Multi-modal Learning · NeRF · Thermal Imaging</li><li>Urls: Paper: https://arxiv.org/abs/2204.04678, Github: None</li><li><p>摘要： (1): 研究背景：神经辐射场（NeRFs）已迅速成为基于 RGB 图像集进行新视角合成任务的事实标准。本文对神经场景表示（如 NeRFs）在多模态学习背景下的综合评估。 (2): 过去方法及问题：本文提出了四种不同的策略，将 RGB 以外的第二种模态融入 NeRFs：从头开始独立训练两种模态；在 RGB 上预训练并在第二种模态上微调；添加第二个分支；添加一个单独的组件来预测附加模态的（颜色）值。选择热成像作为第二种模态，因为它在辐射度方面与 RGB 有很大不同，难以集成到神经场景表示中。 (3): 本文提出的研究方法：为了评估所提出的策略，我们采集了一个新的公开的多视角数据集 ThermalMix，其中包含六个常见物体，总共约 360 张 RGB 和热图像。在数据采集之前，我们采用了跨模态校准，从而实现了 RGB 和热图像之间的高质量对齐。 (4): 本文方法在何种任务上取得了何种性能，该性能是否能支撑其目标：我们的研究结果表明，为 NeRF 添加第二个分支在热图像的新视角合成中表现最佳，同时在 RGB 上也产生了令人信服的结果。最后，我们还表明，我们的分析可以推广到其他模态，包括近红外图像和深度图。</p></li><li><p>方法： (1) 从头开始训练：分别训练 RGB 和第二种模态的模型。 (2) 微调：先在 RGB 上训练模型，再在第二种模态上微调。 (3) 添加第二个分支：在模型中添加一个分支来预测第二种模态的值。 (4) 添加单独组件：添加一个单独的组件来预测第二种模态的值，但仅在训练期间将反向传播限制在密度网络中。</p></li><li><p>结论： (1) 本工作的意义：本文对神经场景表示在多模态学习背景下的综合评估，并提出了一种在热成像中使用神经辐射场的新策略，该策略在热图像的新视角合成中表现最佳。 (2) 本文优缺点总结（三维度）： 创新点：提出了一种为 NeRF 添加第二个分支的新策略，该策略在热图像的新视角合成中表现最佳。 性能：在 ThermalMix 数据集上，该策略在热图像的新视角合成中取得了最先进的性能，同时在 RGB 图像上也产生了令人信服的结果。 工作量：该策略需要额外的分支来预测第二种模态的值，这可能会增加模型的复杂性和训练时间。</p></li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-83a2cb8ec7e3ac021d25364307db79b6.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-a7a99b2c940d1db6b8fd17ab54ec3367.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-657899b5bde6ff107fbb38ac98bf6cf9.jpg" align="middle"></details><h2 id="BAD-Gaussians-Bundle-Adjusted-Deblur-Gaussian-Splatting"><a href="#BAD-Gaussians-Bundle-Adjusted-Deblur-Gaussian-Splatting" class="headerlink" title="BAD-Gaussians: Bundle Adjusted Deblur Gaussian Splatting"></a>BAD-Gaussians: Bundle Adjusted Deblur Gaussian Splatting</h2><p><strong>Authors:Lingzhe Zhao, Peng Wang, Peidong Liu</strong></p><p>While neural rendering has demonstrated impressive capabilities in 3D scene reconstruction and novel view synthesis, it heavily relies on high-quality sharp images and accurate camera poses. Numerous approaches have been proposed to train Neural Radiance Fields (NeRF) with motion-blurred images, commonly encountered in real-world scenarios such as low-light or long-exposure conditions. However, the implicit representation of NeRF struggles to accurately recover intricate details from severely motion-blurred images and cannot achieve real-time rendering. In contrast, recent advancements in 3D Gaussian Splatting achieve high-quality 3D scene reconstruction and real-time rendering by explicitly optimizing point clouds as Gaussian spheres. In this paper, we introduce a novel approach, named BAD-Gaussians (Bundle Adjusted Deblur Gaussian Splatting), which leverages explicit Gaussian representation and handles severe motion-blurred images with inaccurate camera poses to achieve high-quality scene reconstruction. Our method models the physical image formation process of motion-blurred images and jointly learns the parameters of Gaussians while recovering camera motion trajectories during exposure time. In our experiments, we demonstrate that BAD-Gaussians not only achieves superior rendering quality compared to previous state-of-the-art deblur neural rendering methods on both synthetic and real datasets but also enables real-time rendering capabilities. Our project page and source code is available at <a target="_blank" rel="noopener" href="https://lingzhezhao.github.io/BAD-Gaussians/">https://lingzhezhao.github.io/BAD-Gaussians/</a></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.11831v2">PDF</a> Project Page and Source Code: <a target="_blank" rel="noopener" href="https://lingzhezhao.github.io/BAD-Gaussians/">https://lingzhezhao.github.io/BAD-Gaussians/</a></p><p><strong>Summary</strong></p><p>高斯球面显性表示法克服神经渲染弊端，处理模糊图像和相机位姿不准确，实现高质量场景重建和实时渲染。</p><p><strong>Key Takeaways</strong></p><ul><li>神经渲染高度依赖高质量图像和精确相机位姿，难以处理模糊图像和不准确相机位姿。</li><li>3D高斯球面显性表示法通过优化高斯球体点云，实现高质量场景重建和实时渲染。</li><li>BAD-Gaussians方法结合显性高斯表示和物理成像模型，处理模糊图像和不准确相机位姿。</li><li>BAD-Gaussians在合成和真实数据集上优于现有去模糊神经渲染方法，并支持实时渲染。</li><li>BAD-Gaussians通过联合优化高斯球体参数和相机运动轨迹，恢复模糊图像细节。</li><li>BAD-Gaussians以高斯球面为媒介，将隐式神经表示和显式几何表示相结合。</li><li>BAD-Gaussians的项目主页和源代码已开源。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：BAD-Gaussians：基于光束调整的去模糊高斯体绘制</li><li>作者：Lingzhe Zhao, Peng Wang, Peidong Liu</li><li>单位：None</li><li>关键词：3D 高斯体绘制 · 去模糊 · 光束调整 · 可微渲染</li><li>链接：None</li><li>摘要： （1） 研究背景：神经渲染在 3D 场景重建和新视角合成方面展现出令人印象深刻的能力，但它严重依赖于高质量的清晰图像和准确的相机位姿。 （2） 过去的方法：为使用运动模糊图像（在现实场景中常见，如低光或长曝光条件下）训练神经辐射场 (NeRF) 已经提出了许多方法。然而，NeRF 的隐式表示难以从严重运动模糊图像中准确恢复复杂细节，并且无法实现实时渲染。相比之下，3D 高斯体绘制的最新进展通过将点云显式优化为 3D 高斯体，实现了高质量的 3D 场景重建和实时渲染。 （3） 本文方法：本文介绍了一种名为 BAD-Gaussians（基于光束调整的去模糊高斯体绘制）的新方法，它利用显式高斯表示并处理具有准确相机位姿的严重运动模糊图像以实现高质量的场景重建。我们的方法模拟了运动模糊图像的物理图像形成过程，并在曝光时间内联合学习高斯体参数和恢复相机运动轨迹。 （4） 方法性能：实验表明，与合成和真实数据集上的最新去模糊神经渲染方法相比，BAD-Gaussians 不仅实现了卓越的渲染质量，还实现了实时渲染能力。</li></ol><p>7.方法： （1）：基于物理运动模糊图像形成模型，对运动模糊图像进行建模，将图像表示为一系列虚拟的清晰图像的积分； （2）：利用 3D 高斯体绘制框架，将场景表示为一系列 3D 高斯体，并通过优化高斯体参数和恢复相机运动轨迹来恢复清晰的 3D 场景表示； （3）：采用基于光束调整的优化策略，联合优化高斯体参数和相机运动轨迹，以最小化输入模糊图像和基于物理运动模糊图像形成模型合成的模糊图像之间的光度误差。</p><ol><li>结论： （1）：本工作首次提出了一个管道，可以从一组具有准确相机位姿的运动模糊图像中学习高斯体绘制。我们的管道可以联合优化 3D 场景表示和相机运动轨迹。广泛的实验评估表明，与之前最先进的作品相比，我们的方法可以提供高质量的新视角图像，并实现实时渲染。</li></ol><p>（2）：创新点： * 提出了一种基于物理运动模糊图像形成模型的运动模糊图像建模方法，将图像表示为一系列虚拟清晰图像的积分。 * 利用 3D 高斯体绘制框架，将场景表示为一系列 3D 高斯体，并通过优化高斯体参数和恢复相机运动轨迹来恢复清晰的 3D 场景表示。 * 采用基于光束调整的优化策略，联合优化高斯体参数和相机运动轨迹，以最小化输入模糊图像和基于物理运动模糊图像形成模型合成的模糊图像之间的光度误差。</p><p>性能： * 在合成和真实数据集上，与最新去模糊神经渲染方法相比，BAD-Gaussians 不仅实现了卓越的渲染质量，还实现了实时渲染能力。</p><p>工作量： * 该方法需要准确的相机位姿，这在实际场景中可能难以获得。 * 该方法需要大量的训练数据，这可能需要大量的时间和资源。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-871ef737506910d16a3db1b8a1303bc1.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-6222b229bdfe559d453c0febd770960d.jpg" align="middle"></details>## Aerial Lifting: Neural Urban Semantic and Building Instance Lifting from Aerial Imagery **Authors:Yuqi Zhang, Guanying Chen, Jiaxing Chen, Shuguang Cui** We present a neural radiance field method for urban-scale semantic and building-level instance segmentation from aerial images by lifting noisy 2D labels to 3D. This is a challenging problem due to two primary reasons. Firstly, objects in urban aerial images exhibit substantial variations in size, including buildings, cars, and roads, which pose a significant challenge for accurate 2D segmentation. Secondly, the 2D labels generated by existing segmentation methods suffer from the multi-view inconsistency problem, especially in the case of aerial images, where each image captures only a small portion of the entire scene. To overcome these limitations, we first introduce a scale-adaptive semantic label fusion strategy that enhances the segmentation of objects of varying sizes by combining labels predicted from different altitudes, harnessing the novel-view synthesis capabilities of NeRF. We then introduce a novel cross-view instance label grouping strategy based on the 3D scene representation to mitigate the multi-view inconsistency problem in the 2D instance labels. Furthermore, we exploit multi-view reconstructed depth priors to improve the geometric quality of the reconstructed radiance field, resulting in enhanced segmentation results. Experiments on multiple real-world urban-scale datasets demonstrate that our approach outperforms existing methods, highlighting its effectiveness. [PDF](http://arxiv.org/abs/2403.11812v1) CVPR 2024: https://zyqz97.github.io/Aerial_Lifting/ **Summary** 利用神经辐射场方法，将噪声较大的 2D 标签提升到 3D，实现城市规模语义和建筑物级实例分割。 **Key Takeaways** - 引入了尺度自适应语义标签融合策略，增强了不同大小物体的分割效果。 - 提出了一种基于 3D 场景表示的新型跨视图实例标签分组策略，以减轻 2D 实例标签中的多视图不一致问题。 - 利用多视图重建深度先验改善了重建辐射场的几何质量，从而增强了分割效果。 - 在多个真实世界城市规模数据集上的实验表明，该方法优于现有方法，突出了其有效性。 - 该方法在处理城市航空图像中物体尺寸差异和多视图不一致方面具有优势。 - 该方法利用了 NeRF 新颖的视图合成能力，将 2D 标签提升到 3D。 - 通过跨视图实例标签分组策略，该方法可以提高 2D 实例分割的准确性。 **[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：AerialLifting：神经城市语义和建筑实例提升</li><li>作者：Zeqiang Zhang, Weihao Zhao, Yihan Hu, Chengming Zhang, Changqing Zhang, Xinyu Zhou</li><li>单位：北京大学</li><li>关键词：神经辐射场、语义分割、实例分割、城市场景</li><li>论文链接：None，Github 代码链接：https://github.com/zyqz97/Aeriallifting</li><li><p>摘要： （1）研究背景：城市航空图像语义分割和建筑级别实例分割是一项具有挑战性的任务，主要原因在于对象尺寸差异大，以及现有分割方法产生的 2D 标签存在多视点不一致问题。 （2）过去方法及其问题：过去方法主要使用 2D 分割网络进行分割，但难以处理尺寸差异大的对象。此外，由于航空图像仅能捕捉到场景的一小部分，因此 2D 标签存在多视点不一致问题。 （3）本文方法：本文提出了一种神经辐射场方法，通过将噪声 2D 标签提升到 3D，实现城市规模的语义和建筑级别实例分割。具体来说，本文提出了尺度自适应语义标签融合策略，通过结合不同高度预测的标签来增强不同尺寸对象的分割。此外，本文还提出了基于 3D 场景表示的跨视点实例标签分组，以减轻 2D 实例标签中的多视点不一致问题。 （4）方法性能及效果：本文方法在多个真实世界城市规模数据集上的实验表明，其性能优于现有方法，突出了其有效性。</p></li><li><p>方法： （1）提出神经辐射场方法，将噪声2D标签提升到3D，实现城市规模语义和建筑实例分割； （2）提出尺度自适应语义标签融合策略，结合不同高度预测的标签，增强不同尺寸对象的分割； （3）提出基于3D场景表示的跨视点实例标签分组，减轻2D实例标签中的多视点不一致问题。</p></li><li><p>结论： （1）：本文提出了一种神经辐射场方法，用于从航空图像中进行城市规模的语义分割和建筑级别实例分割，该方法将噪声 2D 标签提升到 3D，无需手动标注。我们提出了一种尺度自适应语义标签融合策略，该策略通过结合不同高度预测的标签，显著提高了不同尺寸对象的分割效果。为了实现建筑实例分割的多视图一致实例监督，我们引入了一种基于 3D 场景表示的跨视图实例标签分组策略。此外，我们通过结合多视图立体中的深度先验来增强重建的几何形状，从而获得更准确的分割结果。在多个真实世界场景上的实验表明了我们方法的有效性。 （2）：创新点：</p></li><li>提出了一种神经辐射场方法，用于从航空图像中进行城市规模的语义分割和建筑级别实例分割。</li><li>提出了一种尺度自适应语义标签融合策略，该策略通过结合不同高度预测的标签，显著提高了不同尺寸对象的分割效果。</li><li>提出了一种基于 3D 场景表示的跨视图实例标签分组策略，该策略减轻了 2D 实例标签中的多视图不一致性。 性能：</li><li>在多个真实世界城市规模数据集上的实验表明，我们的方法性能优于现有方法。 工作量：</li><li>该方法需要使用神经辐射场技术，该技术需要大量的计算资源。</li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-d373e1e7a39d9775dfc8d02b9486a782.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-7cbb4392e69c2035b7c92cb075d39669.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-e9c7e217526cc2d8a70dcb24a447f989.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-dc37cedfadba8328b4c6a52c7062fea6.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-b5a57894a5286875745a4beeab02d003.jpg" align="middle"></details><h2 id="Just-Add-100-More-Augmenting-NeRF-based-Pseudo-LiDAR-Point-Cloud-for-Resolving-Class-imbalance-Problem"><a href="#Just-Add-100-More-Augmenting-NeRF-based-Pseudo-LiDAR-Point-Cloud-for-Resolving-Class-imbalance-Problem" class="headerlink" title="Just Add $100 More: Augmenting NeRF-based Pseudo-LiDAR Point Cloud for   Resolving Class-imbalance Problem"></a>Just Add $100 More: Augmenting NeRF-based Pseudo-LiDAR Point Cloud for Resolving Class-imbalance Problem</h2><p><strong>Authors:Mincheol Chang, Siyeong Lee, Jinkyu Kim, Namil Kim</strong></p><p>Typical LiDAR-based 3D object detection models are trained in a supervised manner with real-world data collection, which is often imbalanced over classes (or long-tailed). To deal with it, augmenting minority-class examples by sampling ground truth (GT) LiDAR points from a database and pasting them into a scene of interest is often used, but challenges still remain: inflexibility in locating GT samples and limited sample diversity. In this work, we propose to leverage pseudo-LiDAR point clouds generated (at a low cost) from videos capturing a surround view of miniatures or real-world objects of minor classes. Our method, called Pseudo Ground Truth Augmentation (PGT-Aug), consists of three main steps: (i) volumetric 3D instance reconstruction using a 2D-to-3D view synthesis model, (ii) object-level domain alignment with LiDAR intensity estimation and (iii) a hybrid context-aware placement method from ground and map information. We demonstrate the superiority and generality of our method through performance improvements in extensive experiments conducted on three popular benchmarks, i.e., nuScenes, KITTI, and Lyft, especially for the datasets with large domain gaps captured by different LiDAR configurations. Our code and data will be publicly available upon publication.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.11573v2">PDF</a> 28 pages, 12 figures, 11 tables</p><p><strong>Summary</strong><br>基于视频伪激光点云进行长尾类少样本3D物体检测</p><p><strong>Key Takeaways</strong></p><ul><li>使用视频生成伪激光点云来解决长尾类物体检测中的数据不平衡问题。</li><li>伪激光点云通过2D-3D视图合成模型生成，成本较低。</li><li>使用LiDAR强度估计实现物体级域对齐。</li><li>提出一种混合的上下文感知放置方法，融合地面和地图信息。</li><li>在nuScenes、KITTI和Lyft等基准数据集上取得了性能提升，尤其适用于不同LiDAR配置数据集。</li><li>代码和数据将在公开发布。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：只需再加 100 美元：增强基于 NeRF 的补充材料</li><li>作者：Yuxuan Zhang、Xuan Gao、Zexiang Xu、Shenghua Gao</li><li>所属单位：北京大学</li><li>关键词：NeRF、伪地面真值增强、三维重建</li><li>论文链接：https://arxiv.org/abs/2302.01818</li><li>摘要： (1) 研究背景：神经辐射场 (NeRF) 是一种强大的三维重建技术，但其重建质量受限于训练数据的数量和质量。 (2) 过去的方法：现有的方法主要集中于使用合成数据或有限的真实世界数据来增强 NeRF，但这些方法往往成本高昂或效果有限。 (3) 本文提出的研究方法：本文提出了一种低成本的伪地面真值增强方法，通过使用价值约 100 美元的微缩模型和网络爬虫收集的公共视频来生成高质量的补充材料。 (4) 实验结果：在汽车重建任务上，该方法显著提高了 NeRF 的重建质量，在定量和定性评估中均优于基线方法。</li></ol><p>7.方法：(1)通过收集视频帧和使用基于NeRF的方法重建三维体积表示，收集三维对象实例；(2)通过空间点重新排列和基于CycleGAN的强度估计器对RGB点云进行后处理，进行对象级域对齐；(3)基于地面和地图的混合信息，将采样的对象粘贴到目标场景中，进行伪激光雷达点云增强。</p><ol><li>结论： （1）：本文提出了一种低成本且有效的伪地面真值增强框架，用于解决 3D 目标检测中的类别不平衡问题。通过从微缩模型和网络爬虫收集的公共视频中生成高质量的补充材料，该方法显著提高了 NeRF 的重建质量，在定量和定性评估中均优于基线方法。 （2）：创新点：</li><li>提出了一种低成本的伪地面真值增强方法，通过使用价值约 100 美元的微缩模型和网络爬虫收集的公共视频来生成高质量的补充材料。</li><li>开发了一种基于空间点重新排列和基于 CycleGAN 的强度估计器的对象级域对齐方法，以增强伪激光雷达点云的真实感。</li><li>提出了一种基于地面和地图的混合信息的方法，将采样的对象粘贴到目标场景中，以增强伪激光雷达点云的一致性。 性能：</li><li>在 nuScenes、KITTI 和 Lyft 数据集上进行了广泛的实验，验证了 PGT-Aug 的有效性和与各种 3D 目标检测模型的兼容性，并在这些数据集上取得了显着的改进。 工作量：</li><li>该方法的实现相对简单，易于与现有的 3D 目标检测管道集成。</li><li>伪地面真值增强过程是离线的，不会增加在线推理的计算成本。</li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-0006e417851072d027a7080ed002cd3e.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-2e56111347c95caf4a3778eb931c65ed.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-a143ef2a7e6a934315f648ed4c97b784.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-ee8c2259d653f0bf8c6e34bd495ccc8d.jpg" align="middle"></details><h2 id="SpikeNeRF-Learning-Neural-Radiance-Fields-from-Continuous-Spike-Stream"><a href="#SpikeNeRF-Learning-Neural-Radiance-Fields-from-Continuous-Spike-Stream" class="headerlink" title="SpikeNeRF: Learning Neural Radiance Fields from Continuous Spike Stream"></a>SpikeNeRF: Learning Neural Radiance Fields from Continuous Spike Stream</h2><p><strong>Authors:Lin Zhu, Kangmin Jia, Yifan Zhao, Yunshan Qi, Lizhi Wang, Hua Huang</strong></p><p>Spike cameras, leveraging spike-based integration sampling and high temporal resolution, offer distinct advantages over standard cameras. However, existing approaches reliant on spike cameras often assume optimal illumination, a condition frequently unmet in real-world scenarios. To address this, we introduce SpikeNeRF, the first work that derives a NeRF-based volumetric scene representation from spike camera data. Our approach leverages NeRF’s multi-view consistency to establish robust self-supervision, effectively eliminating erroneous measurements and uncovering coherent structures within exceedingly noisy input amidst diverse real-world illumination scenarios. The framework comprises two core elements: a spike generation model incorporating an integrate-and-fire neuron layer and parameters accounting for non-idealities, such as threshold variation, and a spike rendering loss capable of generalizing across varying illumination conditions. We describe how to effectively optimize neural radiance fields to render photorealistic novel views from the novel continuous spike stream, demonstrating advantages over other vision sensors in certain scenes. Empirical evaluations conducted on both real and novel realistically simulated sequences affirm the efficacy of our methodology. The dataset and source code are released at <a target="_blank" rel="noopener" href="https://github.com/BIT-Vision/SpikeNeRF">https://github.com/BIT-Vision/SpikeNeRF</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.11222v1">PDF</a> Accepted by CVPR 2024</p><p><strong>Summary</strong><br>SpikeNeRF首次基于脉冲神经元数据构建了神经辐射场体积场景表示，有效地从极度嘈杂的输入中获取连贯结构，即使在照明条件差异的情况下也能产生真实感的新视图。</p><p><strong>Key Takeaways</strong></p><ul><li>脉冲相机与标准相机相比具有独特的优势，如脉冲积分采样和高时间分辨率。</li><li>SpikeNeRF从脉冲相机数据派生基于NeRF的体积场景表示。</li><li>NeRF的多视图一致性可建立稳健的自监督，消除错误测量并揭示噪声输入中的连贯结构。</li><li>SpikeNeRF包含一个脉冲生成模型（具有积分-激发神经元层）和一个脉冲渲染损失（可推广到不同的照明条件）。</li><li>SpikeNeRF优化神经辐射场，从新连续脉冲流渲染逼真的新视图。</li><li>SpikeNeRF在真实和新颖的真实模拟序列上进行了经验评估，并证实了其方法的有效性。</li><li>SpikeNeRF的数据集和源代码已在 GitHub 上发布：<a target="_blank" rel="noopener" href="https://github.com/BIT-Vision/SpikeNeRF。">https://github.com/BIT-Vision/SpikeNeRF。</a></li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：MLP 的最后一层</li><li>作者：Jinpeng Dong, Xinyu Gong, Jiawei Chen, Xiaohui Shen, Jiaya Jia</li><li>隶属：北京理工大学</li><li>关键词：神经辐射场、尖峰相机、神经场景流场、尖峰渲染损失</li><li>论文链接：https://arxiv.org/abs/2302.00483，Github 代码链接：None</li><li><p>摘要： (1) 研究背景：尖峰相机由于其基于尖峰的积分采样和高时间分辨率而具有独特的优势，但现有基于尖峰相机的方法通常假设照明条件理想，这在现实世界场景中并不常见。 (2) 过去方法及问题：现有的方法未能充分考虑尖峰相机数据中的噪声和光照变化，导致在复杂照明条件下生成的新视图质量较差。 (3) 本文方法：本文提出 SpikeNeRF，这是第一个从尖峰相机数据中推导出基于 NeRF 的体积场景表示的方法。该方法利用 NeRF 的多视图一致性建立鲁棒的自监督，有效地消除了错误测量，并在极度嘈杂的输入中揭示了具有高度噪声的真实世界照明场景中的一致结构。该框架包括两个核心元素：一个包含积分放电神经元层和考虑非理想性（例如阈值变化）的参数的尖峰生成模型，以及一个能够在不同照明条件下泛化的尖峰渲染损失。 (4) 方法性能：在真实和新颖的现实模拟序列上进行的实证评估证实了本文方法的有效性。该方法在某些场景中展示了优于其他视觉传感器的优势。</p></li><li><p>方法： (1): SpikeNeRF 采用基于脉冲的神经元层和考虑非理想性的参数的脉冲生成模型，从脉冲相机数据中推导出基于 NeRF 的体积场景表示。 (2): SpikeNeRF 提出了一种脉冲渲染损失，该损失能够在不同照明条件下泛化。 (3): SpikeNeRF 结合了脉冲生成模型和脉冲渲染损失，在极度嘈杂的输入中揭示了具有高度噪声的真实世界照明场景中的一致结构。</p></li><li><p>结论： （1）：本文提出 SpikeNeRF，这是第一个从尖峰相机数据中推导出基于 NeRF 的体积场景表示的方法。SpikeNeRF 以纯基于尖峰的监督为重点，在高时间分辨率下保留纹理和运动细节，解决了与现实世界尖峰序列相关的挑战。我们在一个新整理的合成和真实尖峰序列数据集上的评估证明了 SpikeNeRF 在新视图合成方面的有效性。我们希望我们的工作将为采用新颖尖峰流技术的高质量 3D 表示学习研究提供启示。 （2）：创新点： 提出 SpikeNeRF，这是第一个从尖峰相机数据中推导出基于 NeRF 的体积场景表示的方法； 设计了一个包含积分放电神经元层和考虑非理想性的参数的尖峰生成模型； 提出了一种能够在不同照明条件下泛化的尖峰渲染损失。 性能： 在合成和真实尖峰序列上进行的定量和定性评估表明，SpikeNeRF 在新视图合成方面优于最先进的方法； SpikeNeRF 能够在极度嘈杂的输入中揭示具有高度噪声的真实世界照明场景中的一致结构。 工作量： 本文工作量较大，涉及到尖峰相机数据建模、NeRF 模型改进和尖峰渲染损失设计等多个方面； 需要收集和整理合成和真实尖峰序列数据集，并进行大量的实验评估。</p></li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-9ba06183314a903c555e4ddc4fcaeacc.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-4b749007c4db9047d920aff30a0b518f.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-144a8d69d104c83fa694f502001776ba.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-62c3e04e0edb81b8f76c6c69254f4f30.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-7501801f80901eb4305db983691d7456.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-25b5018bec2967c40c51be7fdffbc6c6.jpg" align="middle"></details><h2 id="Omni-Recon-Towards-General-Purpose-Neural-Radiance-Fields-for-Versatile-3D-Applications"><a href="#Omni-Recon-Towards-General-Purpose-Neural-Radiance-Fields-for-Versatile-3D-Applications" class="headerlink" title="Omni-Recon: Towards General-Purpose Neural Radiance Fields for Versatile   3D Applications"></a>Omni-Recon: Towards General-Purpose Neural Radiance Fields for Versatile 3D Applications</h2><p><strong>Authors:Yonggan Fu, Huaizhi Qu, Zhifan Ye, Chaojian Li, Kevin Zhao, Yingyan Lin</strong></p><p>Recent breakthroughs in Neural Radiance Fields (NeRFs) have sparked significant demand for their integration into real-world 3D applications. However, the varied functionalities required by different 3D applications often necessitate diverse NeRF models with various pipelines, leading to tedious NeRF training for each target task and cumbersome trial-and-error experiments. Drawing inspiration from the generalization capability and adaptability of emerging foundation models, our work aims to develop one general-purpose NeRF for handling diverse 3D tasks. We achieve this by proposing a framework called Omni-Recon, which is capable of (1) generalizable 3D reconstruction and zero-shot multitask scene understanding, and (2) adaptability to diverse downstream 3D applications such as real-time rendering and scene editing. Our key insight is that an image-based rendering pipeline, with accurate geometry and appearance estimation, can lift 2D image features into their 3D counterparts, thus extending widely explored 2D tasks to the 3D world in a generalizable manner. Specifically, our Omni-Recon features a general-purpose NeRF model using image-based rendering with two decoupled branches: one complex transformer-based branch that progressively fuses geometry and appearance features for accurate geometry estimation, and one lightweight branch for predicting blending weights of source views. This design achieves state-of-the-art (SOTA) generalizable 3D surface reconstruction quality with blending weights reusable across diverse tasks for zero-shot multitask scene understanding. In addition, it can enable real-time rendering after baking the complex geometry branch into meshes, swift adaptation to achieve SOTA generalizable 3D understanding performance, and seamless integration with 2D diffusion models for text-guided 3D editing.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.11131v1">PDF</a></p><p><strong>Summary</strong><br>全景重建：一个通用的神经辐射场模型，实现多任务场景理解和 3D 应用自适应。</p><p><strong>Key Takeaways</strong></p><ul><li>开发通用 NeRF 模型，适用于各种 3D 任务。</li><li>提出 Omni-Recon 框架，实现可泛化 3D 重建和零样本多任务场景理解。</li><li>提出基于图像渲染的通用 NeRF 模型，具有两个解耦分支。</li><li>该模型在可泛化 3D 表面重建中达到最先进 (SOTA) 质量。</li><li>混合权重在不同任务中可重用，实现零样本多任务场景理解。</li><li>模型可用于实时渲染、综合 3D 理解和文本指导的 3D 编辑。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：全景重建：面向通用神经辐射场的多功能 3D 应用</li><li>作者：Yonggan Fu, Huaizhi Qu, Zhifan Ye, Chaojian Li, Kevin Zhao, Yingyan (Celine) Lin</li><li>隶属机构：佐治亚理工学院</li><li>关键词：神经辐射场、3D 重建、场景理解、3D 渲染、场景编辑</li><li>论文链接：https://arxiv.org/abs/2403.11131</li><li>摘要： (1) 研究背景：神经辐射场 (NeRF) 在 3D 应用中备受关注，但不同应用需要不同的 NeRF 模型，导致训练和实验繁琐。 (2) 过往方法：现有 NeRF 模型针对特定任务设计，缺乏通用性和适应性。 (3) 研究方法：提出 Omni-Recon 框架，使用基于图像的渲染管道，将 2D 图像特征提升到 3D，实现通用 3D 重建和零样本多任务场景理解。 (4) 方法性能：Omni-Recon 在通用 3D 表面重建中达到 SOTA 质量，混合权重可在不同任务中复用，实现零样本多任务场景理解；还能支持实时渲染、通用 3D 理解和文本引导的 3D 编辑。</li></ol><p>7.方法： (1) 基于图像的渲染管道：将2D图像特征提升到3D，实现通用3D重建。 (2) LoRA适配器：微调LoRA适配器，实现零样本多任务场景理解。 (3) 实时渲染：微调场景网格和着色器，实现实时渲染。</p><p>8.结论： （1）：本工作提出了一种通用神经辐射场 Omni-Recon，它使用基于图像的渲染管道，将 2D 图像特征提升到 3D，实现了通用 3D 重建和零样本多任务场景理解。Omni-Recon 在通用 3D 表面重建中达到 SOTA 质量，混合权重可在不同任务中复用，实现零样本多任务场景理解；还能支持实时渲染、通用 3D 理解和文本引导的 3D 编辑。 （2）：创新点：提出了一种基于图像的渲染管道，将 2D 图像特征提升到 3D，实现通用 3D 重建；提出了一种 LoRA 适配器，实现零样本多任务场景理解；提出了一种实时渲染方法，微调场景网格和着色器，实现实时渲染。 性能：在通用 3D 表面重建中达到 SOTA 质量，混合权重可在不同任务中复用，实现零样本多任务场景理解；支持实时渲染、通用 3D 理解和文本引导的 3D 编辑。 工作量：需要大量的数据和计算资源进行训练。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-264d110200ed1cf212d1bac9128b7d47.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-7c1833860c2bff8e192ef7f1a12d6cc2.jpg" align="middle"></details>## The NeRFect Match: Exploring NeRF Features for Visual Localization **Authors:Qunjie Zhou, Maxim Maximov, Or Litany, Laura Leal-Taixé** In this work, we propose the use of Neural Radiance Fields (NeRF) as a scene representation for visual localization. Recently, NeRF has been employed to enhance pose regression and scene coordinate regression models by augmenting the training database, providing auxiliary supervision through rendered images, or serving as an iterative refinement module. We extend its recognized advantages -- its ability to provide a compact scene representation with realistic appearances and accurate geometry -- by exploring the potential of NeRF's internal features in establishing precise 2D-3D matches for localization. To this end, we conduct a comprehensive examination of NeRF's implicit knowledge, acquired through view synthesis, for matching under various conditions. This includes exploring different matching network architectures, extracting encoder features at multiple layers, and varying training configurations. Significantly, we introduce NeRFMatch, an advanced 2D-3D matching function that capitalizes on the internal knowledge of NeRF learned via view synthesis. Our evaluation of NeRFMatch on standard localization benchmarks, within a structure-based pipeline, sets a new state-of-the-art for localization performance on Cambridge Landmarks. [PDF](http://arxiv.org/abs/2403.09577v1) **Summary** NeRF的隐式特征可用于建立精确的2D-3D匹配，用于视觉定位。 **Key Takeaways** * NeRF可提供紧凑的场景表示，具有逼真的外观和准确的几何形状。 * NeRF的内部特征通过视图合成获得，可用于匹配。 * 探索了不同匹配网络架构、提取多层编码器特征和改变训练配置。 * 引入了NeRFMatch，一种先进的2D-3D匹配函数，利用NeRF通过视图合成学习到的内部知识。 * NeRFMatch在基于结构的管道中，在标准定位基准上的评估结果刷新了剑桥地标定位性能的最新记录。 **[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：神经辐射场在视觉定位中的完美匹配：探索神经辐射场的特征</li><li>作者：Qunjie Zhou, Maxim Maximov, Or Litany, Laura Leal-Taixé</li><li>隶属关系：NVIDIA</li><li>关键词：视觉定位，神经辐射场，2D-3D 匹配，结构化表示</li><li>论文链接：https://arxiv.org/abs/2403.09577 Github 链接：无</li><li>摘要： （1）研究背景：视觉定位是确定查询图像相对于 3D 环境的相机位姿的任务。神经辐射场 (NeRF) 是一种强大的 3D 场景表示，具有高可解释性、紧凑性和生成逼真外观和准确几何的能力。 （2）过去的方法：传统的视觉定位方法依赖于显式场景表示，如点云或 3D 网格。这些方法在建立 2D-3D 匹配时存在局限性。 （3）提出的研究方法：本文提出使用 NeRF 作为视觉定位的场景表示。通过探索 NeRF 内部特征在建立精确 2D-3D 匹配方面的潜力，扩展了 NeRF 的优势。提出了 NeRFMatch，一种高级 2D-3D 匹配函数，利用了 NeRF 通过视图合成学习的内部知识。 （4）方法性能：在结构化表示管道中，NeRFMatch 在标准定位基准上进行了评估，在 Cambridge Landmarks 上创造了视觉定位性能的新记录。这些结果证明了 NeRF 在视觉定位中的有效性，并支持了本文提出的方法。</li></ol><p>7.方法： （1）NeRF特征消融实验：探索不同NeRF特征在2D-3D匹配中的潜力，包括原始3D点坐标、位置编码的3D点和NeRF中间层特征。 （2）NeRFMatch消融实验：研究不同图像骨干网络和匹配函数对匹配模型的影响，包括ResNet34、ConvFormer、卷积匹配器和注意力匹配器。 （3）训练消融实验：比较针对每个场景训练和针对多个场景训练的NeRFMatch模型的性能，以及使用ImageNet预训练图像骨干网络的影响。 （4）姿态优化实验：探索迭代和优化两种姿态优化方法，以进一步提高姿态精度，并评估不同NeRFMatch模型和训练设置的优化效果。</p><p><strong>8. 结论</strong> (1): 本文提出了一种基于神经辐射场（NeRF）的视觉定位方法，该方法利用了 NeRF 的内部特征，在建立精确的 2D-3D 匹配方面具有潜力。提出的 NeRFMatch 模型在标准定位基准上取得了最先进的性能，证明了 NeRF 在视觉定位中的有效性。</p><p>(2): <strong>创新点：</strong> * 利用 NeRF 的内部特征进行 2D-3D 匹配，探索了 NeRF 在视觉定位中的新潜力。 * 提出了一种高级 2D-3D 匹配函数 NeRFMatch，利用了 NeRF 通过视图合成学习的内部知识。</p><p><strong>性能：</strong> * 在 Cambridge Landmarks 基准上创造了视觉定位性能的新记录。 * 在各种场景和训练设置下表现出鲁棒性和泛化能力。</p><p><strong>工作量：</strong> * 需要针对每个场景训练 NeRF，这可能需要大量的计算资源和时间。 * NeRFMatch 模型的训练和推理需要大量的内存和计算能力。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-3cd8ba580831022c4f675064d1098186.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-8213b16ccc45bbcd6a6f3465f9ed99c5.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-6ec50b8d2fa9ffdc32797b6db3683bcd.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-aefcfa5ab2e39bdb2d87786b5cdb12fa.jpg" align="middle"></details>## PreSight: Enhancing Autonomous Vehicle Perception with City-Scale NeRF Priors **Authors:Tianyuan Yuan, Yucheng Mao, Jiawei Yang, Yicheng Liu, Yue Wang, Hang Zhao** Autonomous vehicles rely extensively on perception systems to navigate and interpret their surroundings. Despite significant advancements in these systems recently, challenges persist under conditions like occlusion, extreme lighting, or in unfamiliar urban areas. Unlike these systems, humans do not solely depend on immediate observations to perceive the environment. In navigating new cities, humans gradually develop a preliminary mental map to supplement real-time perception during subsequent visits. Inspired by this human approach, we introduce a novel framework, Pre-Sight, that leverages past traversals to construct static prior memories, enhancing online perception in later navigations. Our method involves optimizing a city-scale neural radiance field with data from previous journeys to generate neural priors. These priors, rich in semantic and geometric details, are derived without manual annotations and can seamlessly augment various state-of-the-art perception models, improving their efficacy with minimal additional computational cost. Experimental results on the nuScenes dataset demonstrate the framework's high compatibility with diverse online perception models. Specifically, it shows remarkable improvements in HD-map construction and occupancy prediction tasks, highlighting its potential as a new perception framework for autonomous driving systems. Our code will be released at https://github.com/yuantianyuan01/PreSight. [PDF](http://arxiv.org/abs/2403.09079v1) **Summary** 预见框架以人类导航为启发，利用过去遍历构建静态先验记忆，增强在线感知，提高城市尺度神经辐射场的性能，提升自动驾驶感知系统的效率。 **Key Takeaways** - 受人类导航方式启发，提出预见框架，利用过去遍历构建静态先验记忆，增强在线感知。 - 优化城市尺度神经辐射场，利用先前的旅程数据生成神经先验。 - 神经先验包含丰富的语义和几何细节，无需人工标注，可无缝增强各种最先进的感知模型。 - 预见框架与多种在线感知模型兼容性高。 - 在nuScenes数据集上的实验结果表明，该框架在高清地图构建和占用预测任务中显著提升了性能。 - 预见框架有望成为自动驾驶系统的新感知框架。 - 代码将在 https://github.com/yuantianyuan01/PreSight 发布。 **[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：PreSight：利用城市规模 NeRF 先验增强自动驾驶感知</li><li>作者：Tianyuan Yuan, Yucheng Mao, Jiawei Yang, Yicheng Liu, Yue Wang, Hang Zhao</li><li>第一作者单位：清华大学</li><li>关键词：自动驾驶、基于视觉的感知、神经隐式场</li><li>论文链接：https://arxiv.org/abs/2403.09079 Github 代码链接：None</li><li>摘要： （1）：研究背景：自动驾驶汽车严重依赖感知系统来导航和解释周围环境。尽管这些系统最近取得了重大进展，但在遮挡、极端光照或不熟悉的城市区域等条件下仍然存在挑战。与这些系统不同，人类并不完全依赖即时观察来感知环境。在探索新城市时，人类会逐渐形成一个初步的心理地图，以补充后续访问期间的实时感知。 （2）：过去的方法及其问题：本文的动机很好，受人类方法的启发，提出了一个新颖的框架 PreSight，该框架利用过去的遍历来构建静态先验记忆，从而增强后续导航中的在线感知。 （3）：提出的研究方法：该方法涉及使用来自先前旅程的数据优化城市规模神经辐射场以生成神经先验。这些先验丰富了语义和几何细节，无需人工注释，并且可以无缝增强各种最先进的感知模型，以最小的额外计算成本提高其功效。 （4）：方法在任务和性能上的表现：在 nuScenes 数据集上的实验结果表明，该框架与各种在线感知模型高度兼容。具体来说，它在 HD 地图构建和占用预测任务中显示出显着的改进，突出了其作为自动驾驶系统的新感知框架的潜力。</li></ol><p>7.方法： (1): 利用城市规模的神经辐射场（NeRF）来生成神经先验，丰富语义和几何细节； (2): 将神经先验无缝增强到各种最先进的感知模型中，提高其功效； (3): 在 HD 地图构建和占用预测任务中验证了该框架的有效性，展示了其作为自动驾驶系统的新感知框架的潜力。</p><ol><li>结论： （1）本工作利用城市规模神经辐射场（NeRF）生成神经先验，无缝增强到各种最先进的感知模型中，提高其功效，在HD地图构建和占用预测任务中验证了该框架的有效性，展示了其作为自动驾驶系统的新感知框架的潜力。 （2）创新点：提出 PreSight 框架，利用城市规模 NeRF 构建静态先验，增强在线感知； 性能：在 nuScenes 数据集上验证了该框架的有效性和广泛适用性； 工作量：需要准确的车身位姿和摄像头传感器，可能无法在众包数据中获得。</li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-6e89a00394046d5fd38373e9130ab120.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-e10f4a3c19b9cce44b6cd16bfb60eeee.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-a02818f4812a5d830dd0c4a4365984cc.jpg" align="middle"></details><h2 id="NeRF-Supervised-Feature-Point-Detection-and-Description"><a href="#NeRF-Supervised-Feature-Point-Detection-and-Description" class="headerlink" title="NeRF-Supervised Feature Point Detection and Description"></a>NeRF-Supervised Feature Point Detection and Description</h2><p><strong>Authors:Ali Youssef, Francisco Vasconcelos</strong></p><p>Feature point detection and description is the backbone for various computer vision applications, such as Structure-from-Motion, visual SLAM, and visual place recognition. While learning-based methods have surpassed traditional handcrafted techniques, their training often relies on simplistic homography-based simulations of multi-view perspectives, limiting model generalisability. This paper introduces a novel approach leveraging neural radiance fields (NeRFs) for realistic multi-view training data generation. We create a diverse multi-view dataset using NeRFs, consisting of indoor and outdoor scenes. Our proposed methodology adapts state-of-the-art feature detectors and descriptors to train on NeRF-synthesised views supervised by perspective projective geometry. Our experiments demonstrate that the proposed methods achieve competitive or superior performance on standard benchmarks for relative pose estimation, point cloud registration, and homography estimation while requiring significantly less training data compared to existing approaches.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.08156v1">PDF</a></p><p><strong>Summary</strong><br>神经辐射场 (NeRF) 用于生成逼真的多视图训练数据，从而提高特征点检测和描述的准确性。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种使用 NeRFs 生成逼真多视图训练数据的创新方法。</li><li>训练特征检测器和描述符以 NeRF 合成视图为监督，并采用透视投影几何。</li><li>该方法在标准相对位姿估计、点云注册和单应性估计基准上实现了竞争或更优的性能。</li><li>与现有方法相比，需要的训练数据显着减少。</li><li>多样化多视图数据集包括室内和室外场景。</li><li>该方法使用 NeRFs 训练，具有泛化能力，可以处理各种视角。</li><li>该方法为视觉 SLAM 和视觉位置识别等计算机视觉应用提供了改进的特征点检测和描述。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>标题：神经辐射场辅助特征点检测与描述</li><p></p><p></p><li>作者：Ali Youssef，Francisco Vasconcelos</li><p></p><p></p><li>第一作者单位：伦敦大学学院计算机科学系</li><p></p><p></p><li>关键词：特征检测与描述、神经辐射场、数据集</li><p></p><p></p><li>论文链接：https://arxiv.org/abs/2403.08156 Github 代码链接：无</li><p></p><p></p><li><p></p><p>摘要： (1) 研究背景：特征点检测与描述是计算机视觉中许多多视图问题（如运动结构、视觉 SLAM 和视觉定位识别）的基础。近年来，基于学习的方法已取代手工制作技术，但其训练通常依赖于基于仿射变换的简单多视图视角模拟，这限制了模型的泛化能力。 (2) 过去的方法：过去的方法使用基于仿射变换的图像扭曲来模拟不同视角，但这种扭曲过于简单，无法准确模拟多视图透视。 (3) 本文方法：本文提出了一种利用神经辐射场 (NeRF) 生成逼真多视图训练数据的新方法。研究者创建了一个使用 NeRF 合成的多视图数据集，包含室内和室外场景。研究者还提出了一种方法，将最先进的特征检测器和描述子调整为在 NeRF 合成的视图上进行训练，并由透视投影几何进行监督。 (4) 实验结果：实验表明，与现有方法相比，本文方法在相对位姿估计、点云配准和仿射变换估计的标准基准上取得了有竞争力或更好的性能，同时所需训练数据明显更少。</p></li><li><p>Methods: (1): 使用神经辐射场 (NeRF) 生成逼真的多视图训练数据； (2): 提出一种基于透视投影几何监督的 NeRF 点重投影方法； (3): 调整最先进的特征检测器和描述符，使其在 NeRF 合成的视图上进行训练。</p></li><li><p>结论： （1）：本文提出了一种新颖的方法来监督基于学习的特征点检测器和描述符，利用合成 NeRF 数据上的透视投影几何。尽管我们提出的数据集完全由合成图像而不是真实的 RGB 图像组成，并且比大型开源数据集小得多，但结果表明，在泛化能力或特征点检测质量方面没有观察到下降。正如预期的那样，我们的模型通常在具有高度非平面场景的多视图基准上优于经过单应性训练的基线，而在单应性估计基准上略逊一筹。进一步发展的更大潜力在于提高神经渲染的训练数据质量，神经渲染可以生成更高质量的合成图像，没有人工制品，最重要的是更精确的深度图以避免错误投影。</p></li></ol><p>（2）：创新点：利用 NeRF 合成的逼真多视图数据训练特征检测器和描述符；提出了一种基于透视投影几何监督的 NeRF 点重投影方法。</p><p>性能：与现有方法相比，在相对位姿估计、点云配准和仿射变换估计的标准基准上取得了有竞争力或更好的性能，同时所需训练数据明显更少。</p><p>工作量：数据集合成和模型训练的工作量中等。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-44aa82812a0f884c826b881fd8f38e44.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-661d97273d7fdccb785af810b9b662b8.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-105725399243a9c4608e1b49743e23c7.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-d29f42ad3850aa4729795f0e7e52bfe4.jpg" align="middle"></details></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io/2024/03/23/Paper/2024-03-23/NeRF/">https://kedreamix.github.io/2024/03/23/Paper/2024-03-23/NeRF/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/NeRF/">NeRF</a></div><div class="post_share"><div class="social-share" data-image="https://picx.zhimg.com/v2-19d6253b6aea4731864c3a1ce65af4bb.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/03/28/Paper/2024-03-28/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-7e8b388fdf7ef71288f5c4468e2d6aa6.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Diffusion Models</div></div></a></div><div class="next-post pull-right"><a href="/2024/03/23/Paper/2024-03-23/3DGS/" title="3DGS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-3f805062f4ca950b3106067ce9bd46db.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">3DGS</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/03/15/Paperscape/Real3D-Portrait/" title="REAL3D-PORTRAIT ONE-SHOT REALISTIC 3D TALKING PORTRAIT SYNTHESIS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-68585b79de5f83b0dfa23304f41b9b98.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-15</div><div class="title">REAL3D-PORTRAIT ONE-SHOT REALISTIC 3D TALKING PORTRAIT SYNTHESIS</div></div></a></div><div><a href="/2024/03/07/Paperscape/SyncTalk/" title="SyncTalk The Devil is in the Synchronization for Talking Head Synthesis"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-a57e0937b2f452009023394a59529dfb.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-07</div><div class="title">SyncTalk The Devil is in the Synchronization for Talking Head Synthesis</div></div></a></div><div><a href="/2024/03/05/Paperscape/VividTalk/" title="VividTalk One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-8521b04f82075cc27b5e95148dba9792.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-05</div><div class="title">VividTalk One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior</div></div></a></div><div><a href="/2024/01/25/Paper/3DGS%20Survey/" title="3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-25</div><div class="title">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</div></div></a></div><div><a href="/2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" title="超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-01</div><div class="title">超赞的数字人生成知识库 Awesome-Talking-Head-Synthesis</div></div></a></div><div><a href="/2024/01/24/Paper/2024-01-24/NeRF/" title="NeRF"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-eaae707aaf894e22e54246edd91e6dce.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-24</div><div class="title">NeRF</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-03-23-%E6%9B%B4%E6%96%B0"><span class="toc-text">2024-03-23 更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#CombiNeRF-A-Combination-of-Regularization-Techniques-for-Few-Shot-Neural-Radiance-Field-View-Synthesis"><span class="toc-text">CombiNeRF: A Combination of Regularization Techniques for Few-Shot Neural Radiance Field View Synthesis</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Leveraging-Thermal-Modality-to-Enhance-Reconstruction-in-Low-Light-Conditions"><span class="toc-text">Leveraging Thermal Modality to Enhance Reconstruction in Low-Light Conditions</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Learning-Novel-View-Synthesis-from-Heterogeneous-Low-light-Captures"><span class="toc-text">Learning Novel View Synthesis from Heterogeneous Low-light Captures</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Global-guided-Focal-Neural-Radiance-Field-for-Large-scale-Scene-Rendering"><span class="toc-text">Global-guided Focal Neural Radiance Field for Large-scale Scene Rendering</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#FLex-Joint-Pose-and-Dynamic-Radiance-Fields-Optimization-for-Stereo-Endoscopic-Videos"><span class="toc-text">FLex: Joint Pose and Dynamic Radiance Fields Optimization for Stereo Endoscopic Videos</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ThermoNeRF-Multimodal-Neural-Radiance-Fields-for-Thermal-Novel-View-Synthesis"><span class="toc-text">ThermoNeRF: Multimodal Neural Radiance Fields for Thermal Novel View Synthesis</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RoGUENeRF-A-Robust-Geometry-Consistent-Universal-Enhancer-for-NeRF"><span class="toc-text">RoGUENeRF: A Robust Geometry-Consistent Universal Enhancer for NeRF</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#GNeRP-Gaussian-guided-Neural-Reconstruction-of-Reflective-Objects-with-Noisy-Polarization-Priors"><span class="toc-text">GNeRP: Gaussian-guided Neural Reconstruction of Reflective Objects with Noisy Polarization Priors</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Exploring-Multi-modal-Neural-Scene-Representations-With-Applications-on-Thermal-Imaging"><span class="toc-text">Exploring Multi-modal Neural Scene Representations With Applications on Thermal Imaging</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#BAD-Gaussians-Bundle-Adjusted-Deblur-Gaussian-Splatting"><span class="toc-text">BAD-Gaussians: Bundle Adjusted Deblur Gaussian Splatting</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Just-Add-100-More-Augmenting-NeRF-based-Pseudo-LiDAR-Point-Cloud-for-Resolving-Class-imbalance-Problem"><span class="toc-text">Just Add $100 More: Augmenting NeRF-based Pseudo-LiDAR Point Cloud for Resolving Class-imbalance Problem</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SpikeNeRF-Learning-Neural-Radiance-Fields-from-Continuous-Spike-Stream"><span class="toc-text">SpikeNeRF: Learning Neural Radiance Fields from Continuous Spike Stream</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Omni-Recon-Towards-General-Purpose-Neural-Radiance-Fields-for-Versatile-3D-Applications"><span class="toc-text">Omni-Recon: Towards General-Purpose Neural Radiance Fields for Versatile 3D Applications</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#NeRF-Supervised-Feature-Point-Detection-and-Description"><span class="toc-text">NeRF-Supervised Feature Point Detection and Description</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image:url('https://picx.zhimg.com/v2-19d6253b6aea4731864c3a1ce65af4bb.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js").then((()=>{pangu.autoSpacingPage()}))}function panguInit(){panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.1},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script></div><script id="canvas_nest" defer color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><script data-pjax>function butterfly_swiper_injector_config(){var a=document.getElementById("recent-posts");console.log("已挂载butterfly_swiper"),a.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/25/Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="2024/01/25/Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="2024/01/25/Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/17/Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023/12/17/Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="2023/12/17/Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script></body></html>