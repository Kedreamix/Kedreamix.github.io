<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>SyncTalk The Devil is in the Synchronization for Talking Head Synthesis | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="SyncTalk: The Devil is in the Synchronization for Talking Head SynthesisPaper   : https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.17590 Project : https:&#x2F;&#x2F;ziqiaopeng.github.io&#x2F;synctalk&#x2F; Video    : https:&#x2F;&#x2F;ziqiaopeng.github"><meta property="og:type" content="article"><meta property="og:title" content="SyncTalk The Devil is in the Synchronization for Talking Head Synthesis"><meta property="og:url" content="https://kedreamix.github.io/2024/03/07/Paperscape/SyncTalk/index.html"><meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World"><meta property="og:description" content="SyncTalk: The Devil is in the Synchronization for Talking Head SynthesisPaper   : https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.17590 Project : https:&#x2F;&#x2F;ziqiaopeng.github.io&#x2F;synctalk&#x2F; Video    : https:&#x2F;&#x2F;ziqiaopeng.github"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://picx.zhimg.com/v2-a57e0937b2f452009023394a59529dfb.png"><meta property="article:published_time" content="2024-03-07T07:57:00.000Z"><meta property="article:modified_time" content="2024-03-09T09:37:44.711Z"><meta property="article:author" content="Kedreamix"><meta property="article:tag" content="Talking Head Generation"><meta property="article:tag" content="NeRF"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://picx.zhimg.com/v2-a57e0937b2f452009023394a59529dfb.png"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/2024/03/07/Paperscape/SyncTalk/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-74LZ5BEQQ1")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!0,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"简"},noticeOutdate:{limitDay:500,position:"top",messagePrev:"It has been",messageNext:"days since the last update, the content of the article may be outdated."},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:200},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!0,post:!0},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!0,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"SyncTalk The Devil is in the Synchronization for Talking Head Synthesis",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-03-09 17:37:44"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach((e=>{n.setAttribute(e,t[e])})),document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)})),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=24?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 7.0.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml"></head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{const d=document.getElementById("loading-box"),e=document.body,o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l(),window.addEventListener("load",(()=>{o()}))})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">132</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url('https://pica.zhimg.com/v2-03605cd4fbd659c9d341840c64fd3b41.png')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">SyncTalk The Devil is in the Synchronization for Talking Head Synthesis</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-03-07T07:57:00.000Z" title="发表于 2024-03-07 15:57:00">2024-03-07</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-03-09T09:37:44.711Z" title="更新于 2024-03-09 17:37:44">2024-03-09</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paperscape/">Paperscape</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">6k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>19分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="SyncTalk The Devil is in the Synchronization for Talking Head Synthesis"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="SyncTalk-The-Devil-is-in-the-Synchronization-for-Talking-Head-Synthesis"><a href="#SyncTalk-The-Devil-is-in-the-Synchronization-for-Talking-Head-Synthesis" class="headerlink" title="SyncTalk: The Devil is in the Synchronization for Talking Head Synthesis"></a>SyncTalk: The Devil is in the Synchronization for Talking Head Synthesis</h1><p>Paper : <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2311.17590">https://arxiv.org/abs/2311.17590</a></p><p>Project : <a target="_blank" rel="noopener" href="https://ziqiaopeng.github.io/synctalk/">https://ziqiaopeng.github.io/synctalk/</a></p><p>Video : <a target="_blank" rel="noopener" href="https://ziqiaopeng.github.io/synctalk/#teaser">https://ziqiaopeng.github.io/synctalk/#teaser</a></p><p>Code : <a target="_blank" rel="noopener" href="https://github.com/ziqiaopeng/SyncTalk">https://github.com/ziqiaopeng/SyncTalk</a></p><p><strong>摘要</strong></p><p>神经辐射场 - 生成对抗网络框架用于实现说话人头部视频的同步合成。</p><p>（1）研究背景： 生成逼真的、由语音驱动的谈话头部视频是一项具有挑战性的任务。传统生成对抗网络（GAN）难以保持一致的面部身份，而神经辐射场（NeRF）方法虽然可以解决这个问题，但通常会产生不匹配的唇部动作、不充分的面部表情和不稳定的头部姿势。一个逼真的谈话头部需要同步协调主体身份、唇部动作、面部表情和头部姿势。缺乏这些同步是导致不真实和人工结果的根本缺陷。</p><p>（2）过去的方法及其问题： GAN 方法难以保持一致的面部身份。NeRF 方法虽然可以解决这个问题，但通常会产生不匹配的唇部动作、不充分的面部表情和不稳定的头部姿势。</p><p>（3）提出的研究方法： SyncTalk 是一种基于 NeRF 的方法，它有效地保持了主体身份，增强了谈话头部合成的同步性和真实性。SyncTalk 使用面部同步控制器将唇部动作与语音对齐，并创新地使用 3D 面部混合形状模型来捕捉准确的面部表情。头部同步稳定器优化头部姿势，实现更自然的头部运动。肖像同步生成器恢复头发细节，并将生成的头部与躯干融合，以获得无缝的视觉体验。</p><p>（4）方法在什么任务上取得了什么性能，这些性能是否支持了它们的目标： SyncTalk 在谈话头部合成同步性和真实性方面优于最先进的方法。广泛的实验和用户研究表明，SyncTalk 在同步性和真实性方面优于最先进的方法。</p><p><strong>关键要点</strong></p><ul><li>传统生成对抗网络难以维持一致的面部身份。</li><li>神经辐射场方法可以解决面部身份一致性问题，但经常出现嘴唇运动不匹配、面部表情不足和头部姿势不稳定的问题。</li><li>逼真的说话人头部视频需要同步协调主体身份、嘴唇运动、面部表情和头部姿势。</li><li>缺少同步性是导致不真实和人为结果的根本缺陷。</li><li>SyncTalk 是一种基于神经辐射场的方法，有效地保持了主体身份，提高了说话人头部合成中的同步性和真实感。</li><li>SyncTalk 使用面部同步控制器将嘴唇运动与语音对齐，并创新地使用 3D 面部混合形状模型来捕捉准确的面部表情。</li><li>SyncTalk 的头部同步稳定器优化了头部姿势，实现了更自然的头部运动。</li><li>人像同步生成器恢复头发细节，将生成的头部与躯干融合，以获得无缝的视觉体验。</li></ul><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-a57e0937b2f452009023394a59529dfb.png" alt="SyncTalk"></p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>这篇论文中，解决最好的就是同步的问题，所以也称为同步的Devil 魔鬼😈。现有方法在四个关键领域需要更多的同步：<strong>主体身份</strong>、<strong>唇部运动</strong>、<strong>面部表情</strong>和<strong>头部姿势</strong>。</p><ul><li><p>首先，在基于GAN的方法中，由于连续帧中特征的不稳定性以及仅使用少量帧作为面部重建参考，保持视频中主体的身份是具有挑战性的。</p></li><li><p>其次，唇部运动与语音不同步。在基于NeRF的方法中，仅基于5分钟语音数据集训练的音频特征难以泛化到不同的语音输入。</p></li><li><p>第三，缺乏面部表情控制，大多数方法只能产生唇部运动或控制眨眼，导致面部动作不自然。</p></li><li><p>第四，头部姿势不同步。</p></li></ul><p>先前的方法依赖于稀疏的landmarks来计算投影误差，但这些landmarks的抖动和不准确性导致头部姿势不稳定。这些同步问题会引入伪影，并显著降低真实感。</p><p>为了解决这些同步挑战，引入了SyncTalk，这是一种基于NeRF的方法，专注于高度同步、逼真的、语音驱动的说话头部合成，采用三平面哈希表示来维护主体身份。通过面部同步控制器和头部同步稳定器，SyncTalk显著提高了合成视频的同步性和视觉质量。PortraitSync Generator进一步改善了视觉质量，精心细化了视觉细节。整个渲染过程可以实现50 FPS，并输出高分辨率视频。</p><div class="table-container"><table><thead><tr><th>模块</th><th>描述</th></tr></thead><tbody><tr><td>Face-Sync Controller</td><td>在Face-Sync控制器中，预先在2D音频视听数据集上对音频视觉编码器进行预训练，得到了一种通用表示，确保了不同语音样本之间的唇部同步运动。对于控制面部表情，采用了一个语义丰富的3D面部混合形状模型，该模型通过52个参数控制特定的面部表情区域。</td></tr><tr><td>Head-Sync Stabilizer</td><td>在Head-Sync稳定器中，使用AD-NeRF中的头部运动跟踪器来推断头部的粗略旋转和平移参数。由于粗略参数的不稳定性，借鉴了同步定位与地图(SLAM)的思想，结合头部关键点跟踪器跟踪稠密关键点，并采用bundle adjustment method 束调整方法来优化头部姿势，从而实现稳定连续的头部运动。</td></tr><tr><td>Portrait-Sync Generator</td><td>为了进一步提高SyncTalk的视觉保真度，设计了一个Portrait-Sync生成器。这个模块修复了NeRF建模中的伪影，特别是头发和背景等细节，输出高分辨率视频。</td></tr></tbody></table></div><p><strong>主要贡献</strong></p><ul><li>提出了一个Face-Sync控制器，结合音频视觉编码器和面部动画捕捉器，确保准确的唇部同步和动态面部表情渲染。</li><li>引入了一个Head-Sync稳定器，跟踪头部旋转和面部运动关键点。利用束调整方法，该稳定器保证了平滑同步的头部运动。</li><li>设计了一个Portrait-Sync生成器，通过修复NeRF建模中的伪影和细化头发和背景等细节，提高了视觉保真度。</li></ul><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p><strong>GAN-based Method</strong></p><p>近来，基于GAN的说话头合成成为了计算机视觉中的一个重要研究领域。然而，它们在保持视频中主体的身份一致性方面存在挑战。</p><p>例如，Wav2Lip引入了一个唇部同步专家来监督唇部运动。然而，由于使用了来自参考帧的五帧来重建唇部，它难以保持主体的身份。另一些方法尝试进行全脸合成，但往往难以确保面部表情和头部姿势之间的同步。除了视频流技术外，还有一些方法试图通过语音使单张图像“说话”，如SadTalker可以从单张图像生成一个人说话的视频。然而，这些方法无法生成自然的头部姿势和面部表情，难以保持主体的身份，影响了同步效果，导致视觉感知不真实。</p><p>与这些方法相比，SyncTalk使用NeRF<strong>对人脸进行三维建模</strong>。其能够在规范空间中表示<strong>连续的3D场景的能力</strong>，使其在保持主体身份一致性和保留细节方面表现出色。</p><p><strong>NeRF-based Method</strong></p><p>近来，随着NeRF的崛起，许多领域已开始利用它来解决相关挑战。先前的工作已将NeRF整合到合成说话头像的任务中，并将音频作为驱动信号，但这些方法都是基于普通的NeRF模型。</p><p>例如，AD-NeRF需要大约10秒来渲染单个图像。RADNeRF旨在实现实时视频生成，并使用了基于Instant-NGP的NeRF。ER-NeRF通过引入三平面哈希编码器来修剪空白空间区域，提倡紧凑且加速的渲染方法。GeneFace试图通过将语音特征转换为面部标志来减少NeRF的伪影，但这往往导致唇部运动不准确。尝试使用基于NeRF的方法创建角色头像，例如，不能直接由语音驱动。这些方法仅将音频作为条件，没有清晰的同步概念，并且通常导致唇部运动平均。</p><p>此外，先前的方法<strong>缺乏对面部表情的控制</strong>，仅限于控制眨眼，并且无法对抬眉毛或皱眉等动作进行建模。此外，这些方法在头部姿势不稳定方面存在显着问题，<strong>导致头部和躯干分离</strong>。相比之下，使用Face-Sync控制器来建模音频和唇部运动之间的关系，从而增强唇部运动和表情的同步性，使用Head-Sync稳定器来稳定头部姿势，通过解决这些同步问题，提高了视觉质量。</p><h2 id="主要方法"><a href="#主要方法" class="headerlink" title="主要方法"></a>主要方法</h2><p>SyncTalk主要由三部分组成，接下来会一一介绍</p><ul><li><strong>Face-Sync Controller</strong> 控制嘴唇运动和面部表情</li><li><strong>Head-Sync Stabilizer</strong> 稳定头部姿势</li><li><strong>Portrait-Sync Generator</strong> 渲染的高同步面部帧</li></ul><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-03605cd4fbd659c9d341840c64fd3b41.png" alt="Overview of SyncTalk"></p><h3 id="Face-Sync-Controller"><a href="#Face-Sync-Controller" class="headerlink" title="Face-Sync Controller"></a>Face-Sync Controller</h3><p><strong>Audio-Visual Encoder</strong></p><p>在现有的方法中，大部分的音频特征提取器是用类似于 <strong>DeepSpeech，Wav2Vec 2.0 和 HuBERT</strong> 等ASR模型，但是这些事专门为Automatic Speech Recognition ASR任务设计的，这种设计的音频编码器并不能真正反映嘴唇运动。这是因为预训练的模型是<strong>基于从音频到文本的特征分布，而需要从音频到嘴唇运动的特征分布</strong>。</p><p>针对这种情况，使用在LRS2上训练的<a target="_blank" rel="noopener" href="https://github.com/smeetrs/deep_avsr">deep avsr</a>来做音频特征提取器，使用预训练的唇形同步鉴别器 <a target="_blank" rel="noopener" href="https://github.com/joonson/syncnet_python">SyncNet</a>来监督视频的同步效果，这是使用连续的面部窗口F和相对应的音频帧A输入，同时分为正负样本进行训练，利用<strong>余弦相似度和交叉熵损失</strong>来最小化同步样本的距离并最大化非同步样本的距离。</p><script type="math/tex;mode=display">\begin{aligned}\sin(F,A)&=\frac{F\cdot A}{\|F\|_2\|A\|_2})\end{aligned},</script><script type="math/tex;mode=display">L_{\mathrm{sync}}=-\left(y\log(\sin(F,A))+(1-y)\log(1-\sin(F,A))\right),</script><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-6b250a8119b776d55493f82cfda54bc5.png" alt="正负样本"></p><p>同时在同步鉴别器的监督下，预训练对应的视听特征提取器，这里面堆叠卷积网络进行编码解码，最后用<strong>重建损失</strong>来进行监督。训练后，我们使用 Conv(A) 作为从音频中提取的唇部空间。</p><script type="math/tex;mode=display">L_{\mathrm{recon}}=\|F-\mathrm{Dec}(\mathrm{Conv}(A)\oplus\mathrm{Conv}(F))\|_1.</script><p><strong>Facial Animation Capturer</strong></p><p>在之前的研究中发现，基于NeRF的方法只能改变眨眼，无法准确地建模面部表情。这导致训练出的角色表情僵硬，面部细节不准确，特别是对于有明显面部动作的角色，如眨眼、抬眉毛或皱眉等。<strong>考虑到需要更加同步和逼真的面部表情，添加了一个表情同步控制模块。</strong></p><p>具体而言，引入了一个<strong>基于52个语义面部混合形状系数 B 的3D面部先验模型来建模面部</strong>，也就是3D blendshape 系数来控制面部，这一部分类似于 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2303.11089">EmoTalk</a>。因为3D面部模型能够保留面部运动的结构信息，所以它能够很好地反映面部动作的内容，同时又不会引起面部结构的失真。</p><p><strong>在训练过程中，首先使用一个复杂的面部混合形状捕捉模块将面部表情捕捉为E(B)，然后选择七个核心面部表情控制系数来控制眉毛、额头和眼睛区域。</strong>这些系数与表情高度相关，且独立于嘴唇的运动。因为面部系数具有语义信息，所以我们可以在推理过程中同步演讲者的面部表情。</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-9cfb1cfb7f4ae95b64a868f8e8abad0e.png" alt="Facial Animation Capturer"></p><p><strong>Facial-Aware Masked-Attention</strong></p><p>为了减少训练过程中嘴唇特征和表情特征之间的相互干扰，引入了Facial-Aware Disentangle Attention模块。基于区域注意力向量 V，这类似于<a target="_blank" rel="noopener" href="https://fictionarry.github.io/ER-NeRF/">ER-NeRF</a>，我们分别将Mask $M<em>{lip}$ 和 $M</em>{exp}$ 添加到嘴唇和表情的注意力区域。</p><script type="math/tex;mode=display">\begin{aligned}V_{\mathrm{lip}}&=V\odot M_{\mathrm{lip}},\\V_{\mathrm{exp}}&=V\odot M_{\mathrm{exp}}.\end{aligned}</script><p>通过这样设计的注意力机制，能够有效解耦嘴唇运动和眨眼运动等，从而减少耦合带来的伪影，最后利用解耦的嘴唇特征 $f<em>l = F</em>{lip} ⊙ V<em>{lip}$ 和表情特征$f_e = f</em>{exp} ⊙ V_{exp}$。</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-ba601309ab5cc09573f4291d7ae27f13.png" alt="ER-NeRF Mask"></p><h3 id="Head-Sync-Stabilizer"><a href="#Head-Sync-Stabilizer" class="headerlink" title="Head-Sync Stabilizer"></a>Head-Sync Stabilizer</h3><p><strong>Head Motion Tracker</strong></p><p>头部姿势，表示为 p，是指人的头部在 3D 空间中的旋转角度，由旋转 R 和平移 T 定义。</p><p>不稳定的头部姿势会导致头部抖动。为了获得头部姿势的粗略估计，首先，通过在预定范围内迭代 i 次来确定最佳焦距。对于每个焦距候选 fi，重新初始化旋转和平移值。目标是最小化 3D 可变形模型 (3DMM) 的投影地标与视频帧中的实际地标之间的误差。</p><script type="math/tex;mode=display">f_{\mathrm{opt}}=\arg\min_{f_i}E_i(L_{2D},L_{3D}(f_i,R_i,T_i)),</script><p>其中 $E_i$表示的就是MSE，这样能够以更好地将模型的投影lmk与实际视频lmk对齐，然后得到最优的旋转和平移矩阵，也是用MSE来最小化，这是对每一帧进行操作的，在对应视频帧的最优值。</p><script type="math/tex;mode=display">(R_{\mathrm{opt}},T_{\mathrm{opt}})=\arg\min_{R,T}E(L_{2D},L_{3D}(f_{\mathrm{opt}},R,T)).</script><p><strong>Head Points Tracker</strong></p><p>对于之前基于NeRF的方法来说，先前的方法利用基于 3DMM 的技术来提取头部姿势并生成不准确的结果。为了提高R和T的精度，我们使用像Co- tracker这样的光流估计模型来跟踪面部关键点K。</p><p>接下来，使用预训练的光流估计模型，在获取面部运动光流后，我们使用<strong>拉普拉斯滤波器</strong>选择位于最显著流变化位置的关键点，并在流序列中跟踪这些关键点的运动轨迹。通过这个模块确保了所有帧上的面部关键点对齐更加精确和一致，从而增强了头部姿势参数的准确性。</p><p><strong>Bundle Adjustment</strong></p><p>根据关键点和粗略的头部姿势，引入了一个两阶段优化框架来提高关键点和头部姿势估计的准确性。</p><ul><li><p>第一阶段，随机初始化 j 个关键点的 3D 坐标并优化它们的位置，以便与图像平面上跟踪的关键点对齐。这一部分最小化损失函数 $L_{init}$，捕获<strong>投影关键点 P 和跟踪关键点 K</strong> 之间的差异：</p><script type="math/tex;mode=display">L_{\mathrm{init}}=\sum_j\lVert P_j-K_j\rVert_2.</script></li><li><p>第二阶段，开始进行更全面的优化，以细化 3D 关键点和相关的头部联合姿势参数，通过Adam优化器优化算法，<strong>调整空间坐标、旋转角度R和平移T</strong>以最小化对齐误差$L_{sec}$，表示为：</p><script type="math/tex;mode=display">L_{\sec}=\sum_j\lVert P_j(R,T)-K_j\rVert_2.</script><p>经过这些优化后，观察到所得的头部姿势和平移参数平滑且稳定。</p></li></ul><h3 id="Dynamic-Portrait-Renderer"><a href="#Dynamic-Portrait-Renderer" class="headerlink" title="Dynamic Portrait Renderer"></a>Dynamic Portrait Renderer</h3><p><strong>Tri-Plane Hash Representation</strong></p><p>这一部分实际上就是NeRF的体渲染的方式，都是一些定义的部分。</p><script type="math/tex;mode=display">\hat{C}(\mathrm{r})=\int_{t_n}^{t_f}\sigma(\mathrm{r}(t))\cdot\mathrm{c}(\mathrm{r}(t),\mathrm{d})\cdot T(t)dt,</script><p>类似于ER-NeRF的方式，解决哈希冲突和优化音频特征处理的问题，结合了三个独特定向xyz的 2D 哈希网格，也就是 <strong>Tri-Plane Hash</strong>，作为hash的编码器。</p><script type="math/tex;mode=display">\mathcal{H}^{\mathrm{AB}}:(a,b)\to\mathrm{f}_{ab}^{\mathrm{AB}},\\
\mathrm{f_x}=\mathcal{H}^\mathrm{XY}(x,y)\oplus\mathcal{H}^\mathrm{YZ}(y,z)\oplus\mathcal{H}^\mathrm{XZ}(x,z),</script><p>其中输出 $f^{AB}<em>{ab} ∈ R</em>{LD}$，具有层数 $L$ 和每个方向的特征维度 $D$，表示与投影坐标$ (a, b)$ 相对应的平面几何特征，$H^{AB}$ 表示平面 $R^{AB}$ 的多分辨率哈希编码器。得到每个方向的向量以后，产生 $3 × LD$ 通道向量。采用$fx$、视角方向$d$、嘴唇特征$f_l$和表情特征$f_e$，三平面哈希的隐式函数定义为：</p><script type="math/tex;mode=display">\mathcal{F}^{\mathcal{H}}:(\mathrm{x},\mathrm{d},f_l,f_e;\mathcal{H}^3)\to(\mathrm{c},\sigma),</script><p>类似于ER-NeRF，训练采用了一个两步粗到细的策略。首先，使用MSE损失评估预测的 $\hat{C(r)}$与实际图像颜色$C(r)$之间的差异。鉴于MSE在细节捕捉方面的局限性。接下来进入一个细化阶段，引入LPIPS损失以增强细节，类似于ER-NeRF。我们从图像中提取随机补丁Patch $P$，并将LPIPS（由λ加权）与MSE结合起来以改善细节表示。</p><script type="math/tex;mode=display">\mathcal{L}_\mathrm{total}=\sum_\mathrm{r}\|C(\mathrm{r})-\hat{C}(\mathrm{r})\|_2+\lambda\times\mathcal{L}_\mathrm{LPIPS}(\hat{\mathcal{P}},\mathcal{P}).</script><p><strong>Portrait-Sync Generator</strong></p><p>在训练过程中，为了解决 NeRF 在<strong>捕捉发丝和动态背景</strong>等精细细节方面的局限性，引入了一个包含两个关键部分的 PortraitSync 生成器。</p><p>首先，NeRF 渲染面部区域 ($Fr$)，通过高斯模糊创建 $G(Fr)$，然后使用我们同步的头部姿势能够与原始图像 ($F_o$) 合并，以增强头发细节保真度。</p><p>其次，当头部和躯干结合在一起时，如果源视频中的角色说话而生成的面部保持沉默，则可能会出现暗间隙区域，如下图（b）所示。 所以用平均颈部颜色 ($Cn$) 填充这些区域。</p><p>这种方法通过肖像同步生成器产生更真实的细节并提高视觉质量。</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-421af4b4cfa489148de7fc8f4067427b.png" alt="比较"></p><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p><strong>数据集</strong></p><p>为了进行公平比较，我们使用了来自AD-NeRF，GeneFace和ER-NeRF中相同的视频序列，其中包括英语和法语。这些视频的平均长度约为8,843帧，每个视频以25 FPS录制。除了来自AD-NeRF的视频分辨率为450 × 450外，所有其他视频的分辨率均为512 × 512，并以角色为中心。</p><p><strong>比较基线</strong></p><ul><li>GAN-based 方法 ：Wav2Lip，VideoReTalking，DINet，TalkLip and IP-LAP。</li><li>NeRF-based 方法 ： AD-NeRF，RADNeRF，GeneFace and ER-NeRF。</li></ul><p><strong>实验细节</strong></p><ul><li>在粗略阶段，肖像头部经过100,000次迭代训练，在精细阶段训练25,000次迭代。</li><li>每次迭代使用2D哈希编码器（L=14，F=1）采样$256^2$条光线。</li><li>采用AdamW优化器[24]，哈希编码器的学习率为0.01，其他模块的学习率为0.001。</li><li>在NVIDIA RTX 3090 GPU上，总训练时间约为2小时。</li></ul><p><strong>定量评价</strong></p><div class="table-container"><table><thead><tr><th>评估指标</th><th>描述</th></tr></thead><tbody><tr><td>全参考质量评估</td><td>使用峰值信噪比（PSNR）、学习感知图像补丁相似性（LPIPS）、多尺度结构相似性（MS-SSIM）和Frechet Inception Distance（FID）作为评估指标。</td></tr><tr><td>无参考质量评估</td><td>在高PSNR图像中，纹理细节可能与人类视觉感知不一致。为了更精确地定义和比较输出，使用两种无参考方法：自然图像质量评估器（NIQE）和无参考图像空间质量评估器（BRISQUE）。</td></tr><tr><td>同步评估</td><td>对于同步性，使用地标距离（LMD）来衡量面部运动的同步性，动作单位误差（AUE）来评估面部运动的准确性，并引入唇同步误差置信度（LSE-C），与Wav2Lip一致，以评估唇部运动与音频之间的同步性。</td></tr></tbody></table></div><p><strong>定量评估结果</strong></p><ul><li>头部重建方法在图像质量和同步性方面均优于基于GAN和NeRF的最新方法。</li><li>经过<code>Portrait-Sync Generato</code>r处理后，图像质量得到了显著改善，头发细节得到了恢复。</li><li>方法在维持主体身份、唇部、表情和姿势的同步性方面表现出色。</li><li>使用分布外音频的最新SOTA方法的驱动器结果表明，方法在唇音同步评估方面领先。</li><li>渲染速度远远超过视频输入速度，可以实现实时生成视频流。</li></ul><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-3093f3d799bb12490a7f79dba96bde99.png" alt="The quantitative results of the head reconstruction."></p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-73c53cd37a7c9e87af9b918778a84d3e.png" alt="The quantitative results of the lip synchronization."></p><p><strong>定性评价</strong></p><div class="table-container"><table><thead><tr><th>评估结果</th><th>描述</th></tr></thead><tbody><tr><td>图像质量比较</td><td>在图中，我们展示了我们的方法与其他方法的比较。可以观察到，SyncTalk展示了更精确、更准确的面部细节。</td></tr><tr><td>与Wav2Lip的比较</td><td>与Wav2Lip相比，我们的方法在保持主体身份的同时提供了更高的保真度和分辨率。</td></tr><tr><td>与IP-LAP的比较</td><td>与IP-LAP相比，我们的方法在唇形同步方面表现出色，主要归功于音频-视觉编码器带来的音频-视觉一致性。</td></tr><tr><td>与GeneFace的比较</td><td>与GeneFace相比，我们的方法可以通过表情同步精确地重现眨眼和抬眉等动作。</td></tr><tr><td>与ER-NeRF的比较</td><td>与ER-NeRF相比，我们的方法通过姿势同步稳定器避免了头部和身体的分离，并生成了更准确的唇形。</td></tr><tr><td></td></tr></tbody></table></div><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-b076e645737b2297bee21027ac8e27ad.png" alt="Qualitative comparison of facial synthesis by different methods."></p><p><strong>User Study</strong></p><p>我们设计了一个详尽的用户研究问卷，35名参与者进行评分。问卷设计了五个方面的评分：唇同步准确性、表情同步准确性、姿势同步准确性、图像质量和视频真实性。</p><p>参与者平均完成问卷时间为19分钟，标准化的Cronbach α系数为0.96。用户研究结果显示，SyncTalk在所有评估中均超过以前的方法，特别是在视频真实性方面。</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-2666052562f51f053affc9fb748eec54.png" alt="User Study"></p><p><strong>Ablation Study</strong></p><p>接下来进行了消融研究，以检验我们模型中不同部分的贡献，选择了三个核心指标进行评估：PSNR、LPIPS和LMD。</p><p>我们选择了一个名为“May”的主体进行测试，结果如表所示。</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-b204e48268633b55ad93cf70dbc8f9bd.png" alt="Ablation study for our components"></p><p>音频-视觉编码器提供了主要的唇部同步信息，当替换此模块时，所有三个指标都变差，其中特别是LMD错误增加了21.15%，表明唇部动作同步减少，如图5（a）所示，显示出我们的音频-视觉编码器可以提取准确的唇部特征。</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-2fc44a31570aeacd6badcf909f669fdc.png" alt="Ablation Study"></p><p>用ER-NeRF 的<strong>眨眼模块</strong>替换<strong>Facial Animation Capture</strong>模块，这一部分会影响眉毛的运动和图像质量。</p><p><strong>Facial-Aware Masked-Attention</strong>主要解耦了唇部和面部其他部位之间的运动，在移除后略微影响图像质量。</p><p>若没有<strong>头部同步稳定器</strong>，所有指标都显著下降，特别是LPIPS，导致头部姿势抖动和头部与躯干分离，如图5（b）所示。</p><p><strong>Portrait-Sync Generator</strong>恢复了像头发这样的细节，移除此模块会影响头发等细节的恢复，导致明显的分割边界。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>本文介绍了SyncTalk，这是一种基于高度同步的NeRF方法，用于实现逼真的语音驱动的说话头部合成。</li><li>框架包括面部同步控制器、头部同步稳定器和肖像同步生成器，能够保持主体身份，并生成同步的唇部动作、面部表情和稳定的头部姿势。</li><li>通过广泛的评估，SyncTalk在创建逼真和同步的说话头部视频方面表现出优异的性能，相较于现有方法。</li><li>期望SyncTalk不仅能增强各种应用程序的功能，还能在说话头部合成领域激发进一步的创新。</li></ul></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io/2024/03/07/Paperscape/SyncTalk/">https://kedreamix.github.io/2024/03/07/Paperscape/SyncTalk/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Talking-Head-Generation/">Talking Head Generation</a><a class="post-meta__tags" href="/tags/NeRF/">NeRF</a></div><div class="post_share"><div class="social-share" data-image="https://picx.zhimg.com/v2-a57e0937b2f452009023394a59529dfb.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/03/09/Paper/2024-03-09/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-579c5c472fca8ba1022f880a544c4526.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Diffusion Models</div></div></a></div><div class="next-post pull-right"><a href="/2024/03/05/Paperscape/VividTalk/" title="VividTalk One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-8521b04f82075cc27b5e95148dba9792.png" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">VividTalk One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" title="超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-01</div><div class="title">超赞的数字人生成知识库 Awesome-Talking-Head-Synthesis</div></div></a></div><div><a href="/2024/03/15/Paperscape/Real3D-Portrait/" title="REAL3D-PORTRAIT ONE-SHOT REALISTIC 3D TALKING PORTRAIT SYNTHESIS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-68585b79de5f83b0dfa23304f41b9b98.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-15</div><div class="title">REAL3D-PORTRAIT ONE-SHOT REALISTIC 3D TALKING PORTRAIT SYNTHESIS</div></div></a></div><div><a href="/2024/03/05/Paperscape/VividTalk/" title="VividTalk One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-8521b04f82075cc27b5e95148dba9792.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-05</div><div class="title">VividTalk One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior</div></div></a></div><div><a href="/2024/03/11/Note/BlendShape/" title="Blendshape学习笔记"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://p6-sign.toutiaoimg.com/pgc-image/2c8cbd123e00470e95500a8ae62da605~noop.image?_iz=58558&from=article.pc_detail&lk3s=953192f4&x-expires=1710668214&x-signature=UHPhjWP4v96kbtfJzF97Z%2Bp3klc%3D" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-11</div><div class="title">Blendshape学习笔记</div></div></a></div><div><a href="/2024/03/03/Paperscape/EMO/" title="EMO Emote Portrait Alive - 阿里HumanAIGC"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-6492e24fb03ffa98135dc584535ab7d9.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-03</div><div class="title">EMO Emote Portrait Alive - 阿里HumanAIGC</div></div></a></div><div><a href="/2024/03/18/Project/SyncTalk/" title="SyncTalk实验笔记"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-3866dff2d07194c235eefab923f694c5.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-18</div><div class="title">SyncTalk实验笔记</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#SyncTalk-The-Devil-is-in-the-Synchronization-for-Talking-Head-Synthesis"><span class="toc-number">1.</span> <span class="toc-text">SyncTalk: The Devil is in the Synchronization for Talking Head Synthesis</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.1.</span> <span class="toc-text">介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C"><span class="toc-number">1.2.</span> <span class="toc-text">相关工作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E6%96%B9%E6%B3%95"><span class="toc-number">1.3.</span> <span class="toc-text">主要方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Face-Sync-Controller"><span class="toc-number">1.3.1.</span> <span class="toc-text">Face-Sync Controller</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Head-Sync-Stabilizer"><span class="toc-number">1.3.2.</span> <span class="toc-text">Head-Sync Stabilizer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Dynamic-Portrait-Renderer"><span class="toc-number">1.3.3.</span> <span class="toc-text">Dynamic Portrait Renderer</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="toc-number">1.4.</span> <span class="toc-text">实验结果</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">1.5.</span> <span class="toc-text">总结</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image:url('https://pica.zhimg.com/v2-03605cd4fbd659c9d341840c64fd3b41.png')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js").then((()=>{pangu.autoSpacingPage()}))}function panguInit(){panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"></div><script id="canvas_nest" defer color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><script data-pjax>function butterfly_swiper_injector_config(){var a=document.getElementById("recent-posts");console.log("已挂载butterfly_swiper"),a.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/25/Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="2024/01/25/Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="2024/01/25/Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/17/Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023/12/17/Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="2023/12/17/Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script></body></html>