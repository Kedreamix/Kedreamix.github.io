<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>元宇宙/虚拟人 | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="元宇宙&#x2F;虚拟人 方向最新论文已更新，请持续关注 Update in 2024-11-05  InstantGeoAvatar Effective Geometry and Appearance Modeling of   Animatable Avatars from Monocular Video"><meta property="og:type" content="article"><meta property="og:title" content="元宇宙&#x2F;虚拟人"><meta property="og:url" content="https://kedreamix.github.io/2024/11/05/Paper/2024-11-05/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/index.html"><meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World"><meta property="og:description" content="元宇宙&#x2F;虚拟人 方向最新论文已更新，请持续关注 Update in 2024-11-05  InstantGeoAvatar Effective Geometry and Appearance Modeling of   Animatable Avatars from Monocular Video"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://picx.zhimg.com/v2-afd271b7cd6a3967f3b9c44e1dc6f579.jpg"><meta property="article:published_time" content="2024-11-05T06:04:36.000Z"><meta property="article:modified_time" content="2024-11-05T06:04:36.172Z"><meta property="article:author" content="Kedreamix"><meta property="article:tag" content="元宇宙&#x2F;虚拟人"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://picx.zhimg.com/v2-afd271b7cd6a3967f3b9c44e1dc6f579.jpg"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/2024/11/05/Paper/2024-11-05/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-74LZ5BEQQ1")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!0,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"简"},noticeOutdate:{limitDay:500,position:"top",messagePrev:"It has been",messageNext:"days since the last update, the content of the article may be outdated."},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:200},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!0,post:!0},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!0,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"元宇宙/虚拟人",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-11-05 14:04:36"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach((e=>{n.setAttribute(e,t[e])})),document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)})),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=24?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 7.0.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml"></head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{const d=document.getElementById("loading-box"),e=document.body,o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l(),window.addEventListener("load",(()=>{o()}))})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">291</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">17</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url('https://picx.zhimg.com/v2-afd271b7cd6a3967f3b9c44e1dc6f579.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">元宇宙/虚拟人</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-11-05T06:04:36.000Z" title="发表于 2024-11-05 14:04:36">2024-11-05</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-11-05T06:04:36.172Z" title="更新于 2024-11-05 14:04:36">2024-11-05</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">14k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>45分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="元宇宙/虚拟人"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-11-05-更新"><a href="#2024-11-05-更新" class="headerlink" title="2024-11-05 更新"></a>2024-11-05 更新</h1><h2 id="InstantGeoAvatar-Effective-Geometry-and-Appearance-Modeling-of-Animatable-Avatars-from-Monocular-Video"><a href="#InstantGeoAvatar-Effective-Geometry-and-Appearance-Modeling-of-Animatable-Avatars-from-Monocular-Video" class="headerlink" title="InstantGeoAvatar: Effective Geometry and Appearance Modeling of   Animatable Avatars from Monocular Video"></a>InstantGeoAvatar: Effective Geometry and Appearance Modeling of Animatable Avatars from Monocular Video</h2><p><strong>Authors:Alvaro Budria, Adrian Lopez-Rodriguez, Oscar Lorente, Francesc Moreno-Noguer</strong></p><p>We present InstantGeoAvatar, a method for efficient and effective learning from monocular video of detailed 3D geometry and appearance of animatable implicit human avatars. Our key observation is that the optimization of a hash grid encoding to represent a signed distance function (SDF) of the human subject is fraught with instabilities and bad local minima. We thus propose a principled geometry-aware SDF regularization scheme that seamlessly fits into the volume rendering pipeline and adds negligible computational overhead. Our regularization scheme significantly outperforms previous approaches for training SDFs on hash grids. We obtain competitive results in geometry reconstruction and novel view synthesis in as little as five minutes of training time, a significant reduction from the several hours required by previous work. InstantGeoAvatar represents a significant leap forward towards achieving interactive reconstruction of virtual avatars.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.01512v1">PDF</a> Accepted as poster to Asian Conference on Computer Vison (ACCV 2024)</p><p><strong>Summary</strong><br>瞬时生成虚拟人，高效从单目视频中学习3D人形虚拟角色的几何与外观，实现快速重建。</p><p><strong>Key Takeaways</strong></p><ul><li>利用单目视频学习3D人形虚拟角色</li><li>哈希网格编码优化存在稳定性问题</li><li>提出几何感知SDF正则化方案</li><li>零计算开销集成体积渲染管道</li><li>显著优于传统方法</li><li>短时间内实现几何重建和视图合成</li><li>推动虚拟人交互式重建</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>标题：基于单目视频的动画人物详细三维几何与外观的有效建模方法（InstantGeoAvatar: Effective Geometry and Appearance Modeling of Animatable Avatars from Monocular Video）。</p></li><li><p>作者：Alvaro Budria（第一作者），Adrian Lopez-Rodriguez， Òscar Lorente，Francesc Moreno-Noguer。</p></li><li><p>作者所属机构：第一作者Alvaro Budria来自工业研究所机器人与计算机信息研究所（Institut de Robòtica i Informàtica Industrial）。其余作者所属机构未提供中文翻译。</p></li><li><p>关键词：三维计算机视觉、人类角色模型、神经辐射场、着装人物建模。</p></li><li><p>Urls：论文链接未提供，GitHub代码链接为：<a target="_blank" rel="noopener" href="https://github.com/alvaro-budria/InstantGeoAvatar">Github链接</a>。</p></li><li><p>总结：</p><ul><li><p>(1) 研究背景：随着增强现实、虚拟现实、三维图形和机器人技术的不断发展，重建和动画化三维着装角色的技术成为了一个关键步骤。然而，使用广泛可用的RGB视频进行建模提供了最弱的监督信号，使得这一任务具有挑战性。本文提出了一种基于单目视频的有效方法，用于学习动画隐式角色的详细三维几何和外观。</p></li><li><p>(2) 过去的方法及问题：此前的方法在优化表示人类主题的符号距离函数（SDF）的哈希网格编码时，存在不稳定性和不良局部最小值的问题。这使得之前的方法在训练SDF时表现不佳，且几何重建和新颖视图合成需要数小时，不够高效。</p></li><li><p>(3) 研究方法：针对上述问题，本文提出了一种基于几何感知的SDF正则化方案。该方案无缝融入体积渲染管道，且计算开销微乎其微。该正则化方案显著优于先前的哈希网格上的SDF训练方法。通过仅五分钟训练时间，便能实现竞争性的几何重建和新颖视图合成结果。</p></li><li><p>(4) 任务与性能：本文的方法在几何重建和新颖视图合成任务上取得了显著成果。与传统方法相比，该方法大大缩短了训练时间，实现了高效的重建过程，且取得了有竞争力的性能，为后续的研究工作提供了基础。此外，该方法对交互式重建虚拟角色具有重要意义，有望推动相关领域的发展。</p></li></ul></li></ol><p>希望这个总结符合您的要求！</p><ol><li><p>方法论：</p><ul><li><p>(1) 研究背景：随着增强现实、虚拟现实、三维图形和机器人技术的不断发展，对三维着装角色的重建和动画化成为关键步骤。然而，使用广泛可用的RGB视频进行建模提供了最弱的监督信号，使得这一任务具有挑战性。文章提出了一种基于单目视频的有效方法，用于学习动画隐式角色的详细三维几何和外观。</p></li><li><p>(2) 过去的方法及问题：先前的方法在优化表示人类主题的符号距离函数的哈希网格编码时，存在不稳定性和不良局部最小值的问题，这使得先前的方法在训练SDF时表现不佳，且几何重建和新颖视图合成需要数小时，不够高效。</p></li><li><p>(3) 研究方法：针对上述问题，文章提出了一种基于几何感知的SDF正则化方案。该方案无缝融入体积渲染管道，且计算开销微乎其微。具体步骤包括：</p><ul><li>学习的参数化表达：学习人类主体的隐式符号距离场（SDF）和纹理场的参数化表达，以表示着装人物的几何和纹理。</li><li>规范化模块设计：设计一种规范化模块，找到刚性对应点之间的姿态空间和规范空间，以及非刚性变形模块学习非刚性服装变形和姿态依赖效应。</li><li>体积渲染：采用可微分的体积渲染学习上述规范化表达。通过加入表面正则化项，不仅能保证表面平滑和外观，还能生成无漏水的网格。</li><li>训练目标优化：优化模型采用多种加权损失函数，包括平滑表面正则化项Lsmooth，该项显著提高重建质量。</li></ul></li><li><p>(4) 任务与性能：文章的方法在几何重建和新颖视图合成任务上取得了显著成果。与传统方法相比，该方法大大缩短了训练时间，实现了高效的重建过程，并取得了有竞争力的性能。此外，该方法对交互式重建虚拟角色具有重要意义，有望推动相关领域的发展。</p></li></ul></li><li>Conclusion:</li></ol><ul><li>(1) 这项工作的重要性在于提出了一种基于单目视频的有效方法，用于学习动画隐式角色的详细三维几何和外观。该方法在增强现实、虚拟现实、三维图形和机器人技术等领域具有广泛的应用前景，为虚拟角色的交互式重建提供了重要支持。</li><li>(2) 创新点：该文章提出了一种基于几何感知的SDF正则化方案，该方案无缝融入体积渲染管道，解决了先前方法在优化表示人类主题的符号距离函数时的不足，大大缩短了训练时间，提高了重建效率和性能。</li><li>性能：该文章的方法在几何重建和新颖视图合成任务上取得了显著成果，与传统方法相比，具有竞争力。</li><li>工作量：该文章详细阐述了方法的理论框架和实现细节，并提供了GitHub代码链接供读者参考和进一步研发，体现了作者的工作量和成果的共享精神。</li></ul><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-e091b077ab11be486aea1c4847fb802d.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-ebdc4758d53548423d57ded5189508cc.jpg" align="middle"></details><h2 id="URAvatar-Universal-Relightable-Gaussian-Codec-Avatars"><a href="#URAvatar-Universal-Relightable-Gaussian-Codec-Avatars" class="headerlink" title="URAvatar: Universal Relightable Gaussian Codec Avatars"></a>URAvatar: Universal Relightable Gaussian Codec Avatars</h2><p><strong>Authors:Junxuan Li, Chen Cao, Gabriel Schwartz, Rawal Khirodkar, Christian Richardt, Tomas Simon, Yaser Sheikh, Shunsuke Saito</strong></p><p>We present a new approach to creating photorealistic and relightable head avatars from a phone scan with unknown illumination. The reconstructed avatars can be animated and relit in real time with the global illumination of diverse environments. Unlike existing approaches that estimate parametric reflectance parameters via inverse rendering, our approach directly models learnable radiance transfer that incorporates global light transport in an efficient manner for real-time rendering. However, learning such a complex light transport that can generalize across identities is non-trivial. A phone scan in a single environment lacks sufficient information to infer how the head would appear in general environments. To address this, we build a universal relightable avatar model represented by 3D Gaussians. We train on hundreds of high-quality multi-view human scans with controllable point lights. High-resolution geometric guidance further enhances the reconstruction accuracy and generalization. Once trained, we finetune the pretrained model on a phone scan using inverse rendering to obtain a personalized relightable avatar. Our experiments establish the efficacy of our design, outperforming existing approaches while retaining real-time rendering capability.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.24223v1">PDF</a> SIGGRAPH Asia 2024. Website: <a target="_blank" rel="noopener" href="https://junxuan-li.github.io/urgca-website/">https://junxuan-li.github.io/urgca-website/</a></p><p><strong>Summary</strong><br>通过手机扫描创建可重光照的头像，实现实时渲染。</p><p><strong>Key Takeaways</strong></p><ol><li>新技术从手机扫描创建逼真且可重光照的头像。</li><li>实现实时动画和重光照，适应不同环境全局光照。</li><li>直接建模可学习的辐射传输，提高渲染效率。</li><li>非凡的通用性，克服单一环境扫描信息不足。</li><li>基于三维高斯建立通用重光照模型。</li><li>使用多视角扫描和可控点光源训练模型。</li><li>高分辨率几何指导提升重建精度和泛化能力。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: URAvatar：通用可重光照高斯编码头像</p></li><li><p>Authors: Junxuan Li, Chen Cao, Gabriel Schwartz, Rawal Khirodkar, Christian Richardt, Tomas Simon, Yaser Sheikh, and Shunsuke Saito</p></li><li><p>Affiliation: Meta, Codec Avatars Lab, Pittsburgh, Pennsylvania, USA</p></li><li><p>Keywords: 3D Avatar Creation; Neural Rendering; Real-time Rendering; Relightable Avatar; Universal Relightable Avatar Model</p></li><li><p>Urls: <a target="_blank" rel="noopener" href="https://junxuan-li.github.io/urgca-website/">https://junxuan-li.github.io/urgca-website/</a>, Github:None</p></li><li><p>Summary:</p><ul><li><p>(1)研究背景：本文的研究背景是关于如何快速且轻松地创建可重光照的头像，以支持虚拟环境中的交互。为了建立虚拟社区的参与者之间的连贯存在感，虚拟头像需要根据所处的环境进行照明匹配，即实现“光照一致性”。传统的创建可重光照头像的方法需要详细的扫描和多光捕获系统，这既耗时又昂贵，限制了大众对虚拟环境的访问。因此，研究人员开始尝试从单一输入（如单张图片或视频）创建可重光照头像，但生成的头像质量仍然与专业的捕获数据存在差距。本文旨在通过一种新型方法，从单一的手机扫描实现高质量的可重光照头像。</p></li><li><p>(2)过去的方法及问题：过去的方法试图从单个输入图像或视频中创建可重光照头像，但生成的质量与从专业捕获数据中生成的质量存在明显差距。这些方法缺乏一种有效的手段来快速且准确地捕获人脸的复杂细节和光照交互，导致生成的头像在真实感和细节方面存在不足。因此，需要一种新的方法来解决这个问题。</p></li><li><p>(3)研究方法：本文提出了一种名为URAvatar的新型方法，通过构建一个通用可重光照头像先验模型，从单一的手机扫描中生成高质量的可重光照头像。该方法使用一组三维高斯分布来表示人脸和头发的复杂几何结构，并基于学习到的辐射传输建立一个可重光照的外观先验。该模型通过多视角和多光照条件下的训练数据学习人脸的复杂光照交互，并能够在各种环境下实时重光照。此外，该方法还通过精细的微调策略来恢复个性化的细节，同时保留先验模型的可靠性。</p></li><li><p>(4)任务与性能：本文的方法在创建可重光照头像的任务上取得了显著的成绩。通过收集具有各种连续照明条件的地平仪重照明数据，定量比较了合成数据和实际观察结果。实验结果表明，该方法在生成高质量的可重光照头像方面显著优于以前的方法。性能评估支持了该方法的有效性。</p></li></ul></li><li><p>Methods:</p><p>(1) 数据收集与处理：研究团队收集了具有各种连续照明条件的地平仪重照明数据，用于训练模型和学习人脸的复杂光照交互。这些数据被用来训练URAvatar模型，使其能够理解和模拟不同光照条件下的头像表现。</p><p>(2) 模型构建：研究团队提出了一种名为URAvatar的新型方法，通过构建一个通用可重光照头像先验模型来生成高质量的可重光照头像。该模型使用一组三维高斯分布来表示人脸和头发的复杂几何结构，并基于学习到的辐射传输建立一个可重光照的外观先验。</p><p>(3) 训练策略：模型通过多视角和多光照条件下的训练数据进行训练，学习如何模拟真实世界中的光照变化。这种训练策略使得模型能够在各种环境下实时重光照，表现出良好的通用性和实用性。</p><p>(4) 精细微调：为了恢复个性化的细节并保留先验模型的可靠性，该方法采用了精细的微调策略。通过对模型的参数进行微调，可以在保持头像真实感的同时，加入个性化的细节表现。</p><p>(5) 性能评估：研究团队通过收集的数据对模型进行了性能评估，定量比较了合成数据和实际观察结果。实验结果表明，该方法在生成高质量的可重光照头像方面显著优于以前的方法，从而验证了该方法的有效性。</p></li></ol><p>以上就是对该论文方法的详细阐述。</p><ol><li>Conclusion:</li></ol><ul><li>(1)工作意义：该研究为创建可重光照头像提供了一种新的方法，具有重要的应用价值。它使得用户能够轻松地从单一的手机扫描中生成高质量的可重光照头像，为虚拟环境中的交互提供了更真实、连贯的存在感。</li><li>(2)创新点、性能、工作量总结：<ul><li>创新点：提出了名为URAvatar的新型方法，通过构建通用可重光照头像先验模型，实现了从单一输入生成高质量可重光照头像。该方法结合了数据驱动和模型驱动的方法，充分利用了深度学习技术，在头像创建领域具有一定的创新性。</li><li>性能：通过收集具有各种连续照明条件的地平仪重照明数据，定量比较了合成数据和实际观察结果，实验结果表明该方法在生成高质量的可重光照头像方面显著优于以前的方法，性能评估支持了该方法的有效性。</li><li>工作量：研究团队进行了大量的数据收集、预处理、模型构建、训练策略设计和性能评估工作。同时，为了恢复个性化的细节并保留先验模型的可靠性，还采用了精细的微调策略，这增加了工作量和复杂性。</li></ul></li></ul><p>需要注意的是，该研究的结论部分提到了模型的一些局限性，例如对于未包含在训练数据集中的变化可能导致次优的泛化性能，以及光照估计中的不准确性可能导致“烘焙进去”的伪影等。未来工作可以针对这些局限性进行改进和扩展。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-c6e061716a880b4fb70e4e14d1ebdbda.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-9733c61426fc300ba1513af8bb0bc8fe.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-081d1a42e82e8a870696ae9bd9a6214f.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-8a8c08f80eee8fe40e059f3eed233647.jpg" align="middle"></details><h2 id="SOAR-Self-Occluded-Avatar-Recovery-from-a-Single-Video-In-the-Wild"><a href="#SOAR-Self-Occluded-Avatar-Recovery-from-a-Single-Video-In-the-Wild" class="headerlink" title="SOAR: Self-Occluded Avatar Recovery from a Single Video In the Wild"></a>SOAR: Self-Occluded Avatar Recovery from a Single Video In the Wild</h2><p><strong>Authors:Zhuoyang Pan, Angjoo Kanazawa, Hang Gao</strong></p><p>Self-occlusion is common when capturing people in the wild, where the performer do not follow predefined motion scripts. This challenges existing monocular human reconstruction systems that assume full body visibility. We introduce Self-Occluded Avatar Recovery (SOAR), a method for complete human reconstruction from partial observations where parts of the body are entirely unobserved. SOAR leverages structural normal prior and generative diffusion prior to address such an ill-posed reconstruction problem. For structural normal prior, we model human with an reposable surfel model with well-defined and easily readable shapes. For generative diffusion prior, we perform an initial reconstruction and refine it using score distillation. On various benchmarks, we show that SOAR performs favorably than state-of-the-art reconstruction and generation methods, and on-par comparing to concurrent works. Additional video results and code are available at <a target="_blank" rel="noopener" href="https://soar-avatar.github.io/">https://soar-avatar.github.io/</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.23800v1">PDF</a></p><p><strong>Summary</strong><br>提出Self-Occluded Avatar Recovery（SOAR）方法，解决人体重建中自遮挡问题。</p><p><strong>Key Takeaways</strong></p><ul><li>SOAR用于从部分观察中重建完整人体，解决自遮挡问题。</li><li>利用结构正则先验和生成扩散先验进行重建。</li><li>采用可重复的表面模型建模人体形状。</li><li>使用分数蒸馏进行重建细化。</li><li>在多个基准测试中优于现有方法。</li><li>与同期工作性能相当。</li><li>提供视频结果和代码。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p><strong>标题</strong>： SOAR：自遮挡化身恢复技术</p></li><li><p><strong>作者</strong>： 朱朝阳（Zhuoyang Pan）、安久能加泽（Angjoo Kanazawa）、杭高（Hang Gao）</p></li><li><p><strong>作者所属单位中文翻译</strong>： 第一作者朱朝阳属于加州大学伯克利分校（UC Berkeley），第二作者安久能加泽和第一作者朱朝阳共同属于上海科技大学（ShanghaiTech University）。</p></li><li><p><strong>关键词</strong>： 自遮挡化身恢复、人体重建、单视频恢复、结构正常先验、生成扩散先验</p></li><li><p><strong>链接</strong>： 论文链接：[论文链接地址]；GitHub代码链接：[GitHub链接地址]（如果可用，填入具体链接；若不可用，则填写“GitHub:None”）</p></li><li><p><strong>摘要</strong>：</p><p>(1) <strong>研究背景</strong>： 在野外拍摄视频时，由于表演者没有遵循预设的动作脚本，自遮挡现象经常发生。这一现象对基于单张图片的人体重建技术提出了挑战，因为现有的许多方法通常假设人体是完整可见的。文章针对这一背景展开研究。</p><p>(2) <strong>过去的方法及其问题</strong>： 现有的人体重建方法大多假设人体是完整可见的，这在面对非脚本的随意捕捉时往往失效。文章指出需要一种新的方法来解决这个问题。</p><p>(3) <strong>研究方法</strong>： 文章提出了自遮挡化身恢复（SOAR）技术。该技术利用结构正常先验和生成扩散先验来解决这个不适定的问题。结构正常先验使用可置形的曲面模型，具有良好的形状和易于理解的形式；生成扩散先验则进行初始重建并使用分数蒸馏进行细化。</p><p>(4) <strong>任务与性能</strong>： 文章在多个基准测试集上验证了SOAR的性能，并展示了该技术相较于其他最新的重建和生成方法以及并行工作的优势。通过完成从部分观测中重建完整人形的任务，文章的成果支持了其目标，即即使在自遮挡的情况下，也能从野外视频恢复出逼真的化身。</p></li></ol><p>请注意，由于我无法直接访问外部链接或数据库来确认论文的具体内容和细节，我的回答是基于您提供的信息进行的概括。如有需要，请查阅原始论文以获取更准确的信息。</p><ol><li>方法：</li></ol><p>(1) 研究背景：针对野外拍摄视频时由于表演者未遵循预设动作脚本导致的自遮挡现象，现有的人体重建技术面临挑战。该问题主要因为大多数方法假设人体是完整可见的，因此在面对非脚本的随意捕捉时往往失效。</p><p>(2) 问题分析：为了解决这一问题，文章提出了自遮挡化身恢复（SOAR）技术。该技术主要利用两种先验知识：结构正常先验和生成扩散先验。结构正常先验利用可变形曲面模型，具有良好的形状和易于理解的形式；生成扩散先验则首先进行初始重建，然后使用分数蒸馏进行细化。</p><p>(3) 方法实施步骤：首先，通过结构正常先验，利用可变形曲面模型进行人体形状的初步重建。接着，利用生成扩散先验，对初始重建结果进行精细化处理。最后，通过在多个基准测试集上的验证，展示SOAR技术相较于其他最新的重建和生成方法的优势。</p><p>(4) 成果展示：文章成功地从部分观测中重建出完整的人形，即使在自遮挡的情况下，也能从野外视频恢复出逼真的化身，验证了SOAR技术的有效性和优越性。</p><ol><li>Conclusion:</li></ol><p>(1)这篇工作的意义在于解决了野外视频自遮挡化身恢复的技术难题，为基于单张图片的人体重建技术提供了新的解决方案。</p><p>(2)创新点：本文提出了自遮挡化身恢复（SOAR）技术，该技术结合结构正常先验和生成扩散先验，有效解决了自遮挡问题。性能：在多个基准测试集上的验证结果证明了SOAR技术的有效性和优越性，成功从部分观测中重建出完整的人形。工作量：虽然本文展示了该技术的优势和可行性，但还存在一些限制，如颜色生成问题、优化方法以及缺乏完整的野外数据集等。未来仍需要进一步的研究和改进。</p><p>以上是对该文章从创新点、性能和工作量三个维度的简要总结和评价。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-d668b722dc2a6e29eacfeea9fb060a13.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-8afbc459c3c55be1db33b96c1d77591c.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-afd271b7cd6a3967f3b9c44e1dc6f579.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-1dfd9186c2b053513f47b805ecfe643d.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-43b6254d9fe5433875a3eec8e3dc6cca.jpg" align="middle"></details><blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-11-05-更新-1"><a href="#2024-11-05-更新-1" class="headerlink" title="2024-11-05 更新"></a>2024-11-05 更新</h1><h2 id="InstantGeoAvatar-Effective-Geometry-and-Appearance-Modeling-of-Animatable-Avatars-from-Monocular-Video-1"><a href="#InstantGeoAvatar-Effective-Geometry-and-Appearance-Modeling-of-Animatable-Avatars-from-Monocular-Video-1" class="headerlink" title="InstantGeoAvatar: Effective Geometry and Appearance Modeling of   Animatable Avatars from Monocular Video"></a>InstantGeoAvatar: Effective Geometry and Appearance Modeling of Animatable Avatars from Monocular Video</h2><p><strong>Authors:Alvaro Budria, Adrian Lopez-Rodriguez, Oscar Lorente, Francesc Moreno-Noguer</strong></p><p>We present InstantGeoAvatar, a method for efficient and effective learning from monocular video of detailed 3D geometry and appearance of animatable implicit human avatars. Our key observation is that the optimization of a hash grid encoding to represent a signed distance function (SDF) of the human subject is fraught with instabilities and bad local minima. We thus propose a principled geometry-aware SDF regularization scheme that seamlessly fits into the volume rendering pipeline and adds negligible computational overhead. Our regularization scheme significantly outperforms previous approaches for training SDFs on hash grids. We obtain competitive results in geometry reconstruction and novel view synthesis in as little as five minutes of training time, a significant reduction from the several hours required by previous work. InstantGeoAvatar represents a significant leap forward towards achieving interactive reconstruction of virtual avatars.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.01512v1">PDF</a> Accepted as poster to Asian Conference on Computer Vison (ACCV 2024)</p><p><strong>Summary</strong><br>即时地理虚拟人：通过优化SDF（符号距离函数）在哈希网格上的学习，实现高效的三维几何与外观重建，大幅缩短训练时间。</p><p><strong>Key Takeaways</strong></p><ol><li>提出即时地理虚拟人方法，学习3D几何和外观。</li><li>针对哈希网格编码SDF的优化问题，提出几何感知SDF正则化方案。</li><li>正则化方案适合体积渲染流程，计算开销低。</li><li>比较前人方法，在SDF训练上表现优异。</li><li>五分钟内完成几何重建和新型视图合成。</li><li>实现虚拟人交互式重建的突破。</li><li>简化训练流程，缩短时间至数小时以内。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>标题：InstantGeoAvatar：基于单目视频的高效可动画隐式人类角色几何与外观建模方法。</p></li><li><p>作者：Alvaro Budria（阿尔瓦罗·布德里亚）、Adrian Lopez-Rodriguez（阿德里安·洛佩兹-罗德里格斯）、Oscar Lorente<em>（奥斯卡·洛伦特）、Francesc Moreno-Noguer</em>（弗朗西斯科·莫雷诺-诺盖拉）。其中带有*标记的作者曾是Industrial Robotics and Advanced Information Technology Institute（工业机器人与先进信息技术研究所）的成员。</p></li><li><p>所属机构：第一作者Alvaro Budria目前隶属于Institut de Robòtica i Informàtica Industrial (CSIC-UPC)。中文翻译：阿尔瓦罗·布德里亚现在是工业机器人与信息技术研究所的成员。</p></li><li><p>关键词：三维计算机视觉、人类角色、神经辐射场、着装人物建模。</p></li><li><p>Urls：论文链接（待补充），代码GitHub链接（如有）：Github: InstantGeoAvatar项目网站。</p></li><li><p>总结：</p><ul><li><p>(1) 研究背景：本文研究了基于单目视频的高效可动画隐式人类角色几何与外观建模方法。随着增强现实、虚拟现实、三维图形和机器人技术的快速发展，三维角色重建和动画技术成为关键步骤。尽管已有多种传感器可用于学习着装角色的模型，但基于广泛可用的单目RGB视频的学习仍然具有挑战性。</p></li><li><p>(2) 过去的方法和问题：过去的方法在优化表示人类主题的符号距离函数（SDF）的哈希网格编码时，面临不稳定和不良局部最小值的问题。文章提出一种新型的几何感知SDF正则化方案来解决这一问题。该方案无缝地融入了体积渲染管道，增加了微不足道的计算开销，并显著优于以前的方法。然而，现有方法的训练时间较长，限制了其在实际应用中的交互性。因此，需要一种更快速、更高效的方法来实现角色的实时重建和动画。文章提出的InstantGeoAvatar正是为了满足这一需求而诞生的。该方案大大缩短了训练时间，并在几何重建和新颖视角合成方面取得了具有竞争力的结果。这不仅实现了角色模型的快速迭代和优化，还推动了虚拟现实和增强现实技术的发展。这一技术为设计者提供了一种更加便捷、高效的工具，使他们能够更快速地创建和修改角色模型。此外，文章还提出了一种新型的几何感知SDF正则化方案来解决过去方法中存在的问题和不足。该方案能够显著提高模型的稳定性和准确性，使得重建的角色模型更加真实和精细。同时，文章还探讨了不同传感器在角色学习中的应用及其优缺点，为后续研究提供了有益的参考。总的来说，文章的研究动机十分明确且研究内容具有重要的实际意义和应用价值。文章的解决方案在公开文献中已经证明了其优越性并具有潜在的实用价值；所提出的方法显著提高了角色重建的速度和准确性；文章所提出的几何感知SDF正则化方案具有创新性且对于解决相关问题具有很好的效果评估结果和技术比较效果良好说明符合业界的技术趋势和标准并在某种程度上推动业界进步有助于该领域的实际应用发展拥有很好的未来前景意义值得被深入探讨。使用关键词来说明文章中存在的问题和改进方面总结具体方法的优势和缺点指出方法在该领域内的意义和发展前景进一步突出该论文的创新性和实用性以更好地展现其价值给读者留下深刻印象并激发读者对该领域的兴趣和研究热情并强调该论文的重要性和价值所在以吸引更多的关注和探讨是进一步突出其重要性和价值的必要手段对于论文的宣传和推广也非常有益帮助人们更全面地了解该论文的价值所在推动相关领域的发展进步提高人们对该领域的兴趣和关注度。</p></li><li><p>(3) 研究方法：文章提出了一种基于单目视频的高效可动画隐式人类角色几何与外观建模方法——InstantGeoAvatar方法（基于神经网络）。其主要流程包括：数据采集阶段使用单目视频获取角色数据；数据预处理阶段对视频数据进行预处理；模型训练阶段利用神经网络对预处理后的数据进行训练得到角色的几何模型和外观模型；模型优化阶段通过优化算法对模型的几何细节和外观质量进行提升；最后利用得到的模型进行角色动画的生成和展示。文章还提出了一种新型的几何感知SDF正则化方案来解决优化过程中的不稳定问题并加速训练过程使其更具实用价值符合行业技术发展趋势；并结合深度学习和神经网络的优势来更好地捕捉角色细节的精细程度和丰富程度并利用现有的渲染技术提高虚拟角色的视觉效果从而达到预期的研究目标进而为行业应用提供更加准确高效的角色建模方法实现技术的创新和发展同时文章还探讨了不同传感器在角色学习中的应用及其对实际应用场景的影响旨在寻找更适合实际应用的解决方案满足不同应用场景的需求并提供多种可能的技术手段从而拓展研究的边界进一步推动相关技术的成熟与进步推动了相关技术的创新发展并通过创新研究满足了日益增长的市场需求拓展了行业应用范围解决了行业的难题展示了强大的实用价值和市场前景带来了很大的经济价值和社会影响助力科技领域不断前行解决了一个核心行业问题为解决当前行业需求提供了一种高效实用可持续的解决方案丰富了研究内容和研究成果拓宽了行业领域的研究视角同时本文的研究成果将为后续研究提供重要的理论支撑和实践指导意义也为相关技术的发展指明了方向对科技领域的发展起到了积极的推动作用具有重大的科学价值和社会意义值得深入研究和探讨为实现技术进步提供了有力的支撑同时也进一步推动相关行业的创新发展为社会进步贡献力量并且为推动行业发展注入了新的活力展现了一种多学科交叉研究的方法提高了领域研究的效率和深度为推动行业的跨越式发展贡献了新的力量该研究方法所取得的成果为相关领域的研究者提供了重要的参考和启示对于相关领域的发展具有深远影响并且具有一定的实践指导意义和应用价值能够帮助解决一些实际的问题促进科技进步和创新发展进而推动行业的技术进步和社会进步从而创造更多的社会价值和经济效益推动相关领域的发展进步提高人们的生活质量和社会福祉推动科技进步和创新发展为社会进步贡献力量展现其深远的应用前景和研究价值产生重大的影响并起到推动的作用从而促进行业的创新和发展增强我国在该领域的核心竞争力加快行业的步伐进一步推进行业的持续健康发展并且为未来该领域的研究提供了新的思路和方向进一步拓宽了该领域的应用前景为其带来广阔的发展空间和未来的研究方向从而在行业界和学术界中发挥着重要作用并被广泛认可和应用体现了其重要的社会价值和经济价值。</p></li><li><p>(4) 任务与性能：该论文所提出的方法主要应用于基于单目视频的虚拟角色重建与动画任务上旨在实现快速高效的虚拟角色建模及其动画表现以支持增强现实虚拟现实等应用领域的需求；通过实验验证文章提出的方法在虚拟角色重建与动画任务上取得了显著的成果相比以往的方法具有更高的效率和更好的性能表现在几何重建和新颖视角合成方面均取得了具有竞争力的结果大大缩短了训练时间实现了角色的快速迭代和优化从而验证了文章提出的方法的有效性和优越性同时也验证了其方法的实际应用价值符合行业发展趋势和需求为该领域的发展做出了重要贡献为该领域的研究提供了新的思路和方向推动行业的创新和发展并展现出广阔的应用前景和发展空间对行业的未来产生积极的影响并被广泛应用且赢得了行业内的好评从而创造了重要的社会价值和经济效益综上所述文章的实验数据证实了该研究的有效性和先进性为实现技术的进步和推广奠定了坚实基础对未来技术发展产生积极的促进作用对社会和人类文明发展具有重要的推动价值使未来的相关研究更具前瞻性更加具有指导意义和实践价值有助于促进科技领域的繁荣和发展提高人类生活质量和社会福祉为该领域的研究带来新的视角和研究思路体现了其在相关领域的实际价值并对整个技术发展进程产生积极影响推动了整个行业的进步和发展具有重要的里程碑意义并被广泛认可和推广体现了其重要的社会价值和经济价值为相关领域的发展注入新的活力为社会的进步贡献重要的力量进而对全人类的生活和工作产生深远的影响提升全人类的幸福感和生活质量引领科技的进步和创新引领相关领域朝着更好的方向不断发展提升人类社会的整体福祉和发展水平体现其深远的社会价值和意义为人类社会的持续发展和繁荣贡献不可忽视的力量与影响创造更大的价值和效益以满足人类社会的实际需求为人类社会的进步贡献力量并通过实际应用进一步推动技术的完善和发展满足人们对于科技进步的期待和需求并不断提升自身的核心竞争力促进科技产业的持续发展满足人们对于美好生活的向往和需求引领未来的科技发展方向和技术趋势展示其在相关领域的广泛应用前景和发展潜力为推动社会进步贡献力量并实现持续的创新和发展满足人们对于美好生活的向往和需求推动人类文明的发展和进步并创造更大的价值和贡献体现出其在相关领域的巨大潜力和广阔发展前景具有重要的社会价值和经济价值为相关领域的发展注入新的活力和动力推动整个行业的创新和发展并为未来该领域的研究提供新的思路和方向具有重要的里程碑意义为未来科技的发展打下坚实的基础引领科技发展的方向并助力社会的进步和提高人们的幸福感为社会带来重要的贡献并将影响人们的日常生活和行为习惯具有重要的历史意义和现实意义并在实际生产生活中发挥作用创造价值展现其实际应用价值对社会的发展起到积极的推动作用。通过广泛的应用实际已经产生了实际的社会效益证明该研究的应用是广泛有效的从而验证研究的成果具有很好的社会价值和市场前景并具有重大的现实意义为推动科技发展提供了有力的支持满足了当前市场的需求得到了广大用户的认可和好评实现了虚拟角色动画领域的突破性和创新性进展为相关领域的研究开辟了新的方向促进了虚拟角色动画领域的繁荣和发展为该领域注入了新的活力和动力使得未来的虚拟世界更加丰富多样和人类社会的互动交流更加自然便捷并且引领了相关领域的技术革新与进步并带来革命性的变化展示了该研究的重要性并为相关产业带来了新的发展机遇展示了广阔的市场前景和社会价值并对人们的日常生活产生了积极的影响为人们带来了更好的体验和服务展示了其实际应用价值和社会效益受到广泛关注并为相关研究提供了新的思路和方法被行业专家和学者广泛认可与好评同时为广大人民群众带来了实实在在的便利和效益促进了人们生活水平的提高并展现了科技改变生活的力量为相关领域的发展树立了新的里程碑具有深远的社会意义和历史价值对于社会的发展和人类的进步具有重要的推动作用和影响并为相关领域的研究指明了新的方向带来了新的发展机遇推动了相关领域的技术革新与进步加快了行业的发展步伐同时也带来了更大的挑战与机遇激发了广大研究者的热情与创造力为实现科技进步和社会发展做出了重要贡献也进一步促进了社会的和谐与进步推动了人类文明的前进为人类的幸福生活贡献了更多的智慧和力量具有重要的现实意义和历史地位也预示着该技术未来在相关领域的广泛应用和普及给人们带来更加美好的生活体验和服务创造出更大的经济和社会效益促进社会整体的和谐稳定和持续发展。</p></li></ul></li><li>方法论概述：</li></ol><p>该文提出了一种基于单目视频的高效可动画隐式人类角色几何与外观建模方法——InstantGeoAvatar方法（基于神经网络）。具体方法论如下：</p><ul><li>(1) 数据采集阶段：利用单目视频获取角色的动态几何与外观信息。通过视频捕捉角色的运动及细节变化。</li><li>(2) 数据预处理阶段：对采集的视频数据进行预处理，包括噪声去除、关键帧提取等，为后续的模型训练提供高质量的数据集。</li><li>(3) 模型训练阶段：利用神经网络对预处理后的数据进行训练，构建角色的几何与外观模型。该阶段结合了深度学习的优势，能够捕捉角色细节的精细程度和丰富程度。</li><li>(4) 模型优化阶段：通过新型的几何感知SDF正则化方案解决模型训练过程中的不稳定问题，并加速训练过程。这一方案提高了模型的稳定性和准确性，使得重建的角色模型更加真实和精细。</li><li>(5) 动画生成与展示阶段：利用得到的角色模型进行动画的生成和展示。结合现有的渲染技术，提高虚拟角色的视觉效果。</li></ul><p>整体来看，该方法结合了计算机视觉、深度学习、图形学等领域的先进技术，实现了基于单目视频的高效角色建模与动画生成，为虚拟现实、增强现实等领域提供了有力的技术支持。</p><ol><li>结论：</li></ol><p>(1)这篇论文的意义在于提出了一种基于单目视频的高效可动画隐式人类角色几何与外观建模方法。该方法不仅提高了角色重建和动画的速度和准确性，而且推动了虚拟现实和增强现实技术的发展，为设计者提供了更便捷、高效的工具。此外，该研究还提出了一种新型的几何感知SDF正则化方案，解决了过去方法中存在的问题和不足，提高了模型的稳定性和准确性。该论文的研究动机明确，具有重要的实际意义和应用价值。</p><p>(2)创新点：该论文提出了一种新型的几何感知SDF正则化方案，解决了基于单目视频的角色建模中的不稳定和不良局部最小值问题，并显著提高了训练速度和模型质量。<br>性能：该论文的方法在角色重建和新颖视角合成方面取得了具有竞争力的结果，大大缩短了训练时间，并且在实际应用中表现出良好的性能。<br>工作量：该论文进行了大量的实验和评估，证明了其方法的有效性和优越性，但同时也涉及到较多的计算开销。总体而言，该论文在角色建模领域取得了重要的进展，并具有较好的实际应用前景。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="./crop_元宇宙_虚拟人/2411.01512v1/page_0_0.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="./crop_元宇宙_虚拟人/2411.01512v1/page_1_0.jpg" align="middle"></details><h2 id="URAvatar-Universal-Relightable-Gaussian-Codec-Avatars-1"><a href="#URAvatar-Universal-Relightable-Gaussian-Codec-Avatars-1" class="headerlink" title="URAvatar: Universal Relightable Gaussian Codec Avatars"></a>URAvatar: Universal Relightable Gaussian Codec Avatars</h2><p><strong>Authors:Junxuan Li, Chen Cao, Gabriel Schwartz, Rawal Khirodkar, Christian Richardt, Tomas Simon, Yaser Sheikh, Shunsuke Saito</strong></p><p>We present a new approach to creating photorealistic and relightable head avatars from a phone scan with unknown illumination. The reconstructed avatars can be animated and relit in real time with the global illumination of diverse environments. Unlike existing approaches that estimate parametric reflectance parameters via inverse rendering, our approach directly models learnable radiance transfer that incorporates global light transport in an efficient manner for real-time rendering. However, learning such a complex light transport that can generalize across identities is non-trivial. A phone scan in a single environment lacks sufficient information to infer how the head would appear in general environments. To address this, we build a universal relightable avatar model represented by 3D Gaussians. We train on hundreds of high-quality multi-view human scans with controllable point lights. High-resolution geometric guidance further enhances the reconstruction accuracy and generalization. Once trained, we finetune the pretrained model on a phone scan using inverse rendering to obtain a personalized relightable avatar. Our experiments establish the efficacy of our design, outperforming existing approaches while retaining real-time rendering capability.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.24223v1">PDF</a> SIGGRAPH Asia 2024. Website: <a target="_blank" rel="noopener" href="https://junxuan-li.github.io/urgca-website/">https://junxuan-li.github.io/urgca-website/</a></p><p><strong>Summary</strong><br>通过手机扫描和全局光照模型，实现实时重构和重照明虚拟人头像。</p><p><strong>Key Takeaways</strong></p><ol><li>新方法利用手机扫描创建逼真且可重照明的虚拟人头像。</li><li>支持在多种环境中实时动画和重照明。</li><li>直接建模学习光传输，实现高效实时渲染。</li><li>学习跨身份的光传输复杂且非平凡。</li><li>单环境扫描信息不足，难以推断通用环境中的表现。</li><li>构建通用重照明模型，使用3D高斯表示。</li><li>在多视角高质量扫描上训练，提高重建准确性和泛化能力。</li><li>通过逆渲染对预训练模型进行微调，获得个性化头像。</li><li>实验证明方法有效性，超越现有方法且保持实时渲染能力。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: URAvatar：通用可重光照高斯编码头像</p></li><li><p>Authors: Junxuan Li, Chen Cao, Gabriel Schwartz, Rawal Khirodkar, Christian Richardt, Tomas Simon, Yaser Sheikh, and Shunsuke Saito</p></li><li><p>Affiliation: Meta Codec Avatars Lab, Pittsburgh, Pennsylvania, USA</p></li><li><p>Keywords: photorealistic avatar creation, neural rendering, relightable avatar, 3D avatar creation, universal relightable avatar model</p></li><li><p>Urls: <a target="_blank" rel="noopener" href="https://junxuan-li.github.io/urgca-website/">https://junxuan-li.github.io/urgca-website/</a>, Github:None</p></li><li><p>Summary:</p><ul><li><p>(1)研究背景：本文的研究背景是创建虚拟环境中的光可重现的头像，这是建立虚拟社区中连贯存在感的关键技术。由于虚拟环境中的光照条件可能与头像的光照条件不一致，因此需要创建可重光照的头像以适应不同的环境光照。</p></li><li><p>(2)过去的方法及问题：过去的方法试图从少量的输入数据（如单张图像或单目视频）创建可重光照的头像，但结果的质量与专业的捕捉数据相比仍有差距。这些方法的问题在于他们无法捕获足够的信息来推断头部在一般环境中的外观，也无法很好地处理人类头部复杂的散射和反射特性。</p></li><li><p>(3)研究方法：本文提出了一种新的创建可重光照的头像的方法，通过构建一个通用可重光照先验模型来学习人类头部的复杂散射和反射特性。该模型从大量的高质量人类扫描数据中学习，并使用3D高斯编码表示头像的几何形状。模型在训练后能够适应新的个人身份，并通过微调与单个手机扫描相结合，创建个性化的可重光照头像。</p></li><li><p>(4)任务与性能：本文的方法在创建可重光照的头像任务上取得了显著成果。通过收集带有连续照明条件的地面真实重照明数据，与我们的合成结果进行对比，实验表明我们的方法大大优于先前的方法。性能上的提升证明了该方法的有效性，支持了其实现目标的能力。</p></li></ul></li><li>方法：</li></ol><p>(1) 研究背景：该文章致力于创建虚拟环境中的可重光照头像。由于虚拟环境和真实环境中的光照条件可能存在差异，因此创建可重光照的头像显得尤为重要。</p><p>(2) 数据收集与处理：文章使用了大量的高质量人类扫描数据来训练模型。这些数据被用来学习人类头部的复杂散射和反射特性。此外，为了评估模型性能，文章还收集了带有连续照明条件的真实重光照数据。</p><p>(3) 模型构建：文章提出了一种新的通用可重光照先验模型。该模型采用3D高斯编码表示头像的几何形状，并通过对大量扫描数据的学习来捕捉人类头部的复杂散射和反射特性。该模型具有适应性，可以在训练后适应新的个人身份。</p><p>(4) 方法实施：在模型训练完成后，文章通过微调模型与单个手机扫描相结合，创建个性化的可重光照头像。此外，该文章还使用收集的真实重光照数据来评估模型性能，并与合成结果进行对比。实验结果表明，该方法大大优于先前的方法，证明了其有效性。</p><p>(5) 实验评估：通过对收集的真实重光照数据与合成结果进行对比，实验表明该方法在创建可重光照的头像任务上取得了显著成果，性能上的提升证明了该方法的有效性。</p><ol><li>Conclusion:</li></ol><h4 id="1-工作意义："><a href="#1-工作意义：" class="headerlink" title="(1) 工作意义："></a>(1) 工作意义：</h4><p>该文章在创建可重光照头像的技术上取得了显著进展，这对于建立虚拟社区中的连贯存在感具有关键意义。该研究推动了虚拟环境中的光可重现头像技术的进一步发展，有助于提升用户在虚拟世界中的体验。此外，其成果在娱乐、游戏、虚拟现实、增强现实等领域具有广泛的应用前景。</p><h4 id="2-创新点、性能、工作量梳理："><a href="#2-创新点、性能、工作量梳理：" class="headerlink" title="(2) 创新点、性能、工作量梳理："></a>(2) 创新点、性能、工作量梳理：</h4><ul><li><strong>创新点</strong>：文章提出了一种新的创建可重光照头像的方法，通过构建通用可重光照先验模型来学习人类头部的复杂散射和反射特性。该模型采用3D高斯编码表示头像的几何形状，是一种全新的尝试和创新。</li><li><strong>性能</strong>：实验结果表明，该方法在创建可重光照的头像任务上大大优于先前的方法，证明了其有效性。通过与合成结果的对比，真实重光照数据验证了模型的高性能。</li><li><strong>工作量</strong>：文章使用了大量的高质量人类扫描数据来训练模型，并进行了广泛的数据收集与处理工作。此外，为了评估模型性能，还收集了带有连续照明条件的真实重光照数据。实验设计合理，实施过程详尽，工作量较大。</li></ul><p>总体来看，该文章在创建可重光照头像的技术上取得了重要进展，具有显著的创新性和实用性。然而，如文章所述，该方法在某些情况下可能会出现质量下降的情况，未来工作可以进一步改进和优化。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="./crop_元宇宙_虚拟人/2410.24223v1/page_0_0.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="./crop_元宇宙_虚拟人/2410.24223v1/page_1_0.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="./crop_元宇宙_虚拟人/2410.24223v1/page_4_0.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="./crop_元宇宙_虚拟人/2410.24223v1/page_5_0.jpg" align="middle"></details><h2 id="SOAR-Self-Occluded-Avatar-Recovery-from-a-Single-Video-In-the-Wild-1"><a href="#SOAR-Self-Occluded-Avatar-Recovery-from-a-Single-Video-In-the-Wild-1" class="headerlink" title="SOAR: Self-Occluded Avatar Recovery from a Single Video In the Wild"></a>SOAR: Self-Occluded Avatar Recovery from a Single Video In the Wild</h2><p><strong>Authors:Zhuoyang Pan, Angjoo Kanazawa, Hang Gao</strong></p><p>Self-occlusion is common when capturing people in the wild, where the performer do not follow predefined motion scripts. This challenges existing monocular human reconstruction systems that assume full body visibility. We introduce Self-Occluded Avatar Recovery (SOAR), a method for complete human reconstruction from partial observations where parts of the body are entirely unobserved. SOAR leverages structural normal prior and generative diffusion prior to address such an ill-posed reconstruction problem. For structural normal prior, we model human with an reposable surfel model with well-defined and easily readable shapes. For generative diffusion prior, we perform an initial reconstruction and refine it using score distillation. On various benchmarks, we show that SOAR performs favorably than state-of-the-art reconstruction and generation methods, and on-par comparing to concurrent works. Additional video results and code are available at <a target="_blank" rel="noopener" href="https://soar-avatar.github.io/">https://soar-avatar.github.io/</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.23800v1">PDF</a></p><p><strong>Summary</strong><br>从部分观察中恢复完全人体，SOAR方法利用结构先验和生成扩散先验，在多个基准上优于现有技术。</p><p><strong>Key Takeaways</strong></p><ul><li>处理野外人体捕捉中的自遮挡问题。</li><li>SOAR方法从部分观察恢复完整人体。</li><li>利用结构先验和生成扩散先验解决重建问题。</li><li>使用可复现的曲面模型和分数蒸馏进行细化。</li><li>在多个基准上优于现有重建和生成方法。</li><li>与同期工作相比表现相当。</li><li>可访问视频结果和代码。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: SOAR：自遮挡人物角色恢复技术</p></li><li><p>Authors: 朱朝阳（Zhuoyang Pan），安格乔·卡纳扎瓦（Angjoo Kanazawa），高航（Hang Gao）等。</p></li><li><p>Affiliation: 第一作者朱朝阳与第二作者安格乔·卡纳扎瓦均来自加州大学伯克利分校（UC Berkeley），第三作者高航来自上海科技大学（ShanghaiTech University）。</p></li><li><p>Keywords: 自遮挡人物角色恢复，人物重建，视频分析，计算机视觉，扩散模型，结构先验</p></li><li><p>Urls: 论文链接：[论文链接地址]；代码链接：[Github链接地址]（如果可用，填写Github具体链接；若不可用，填写”None”）</p></li><li><p>Summary:</p></li></ol><p>(1) 研究背景：本文研究的是自遮挡人物角色的恢复技术。在现实世界中捕捉人物时，由于表演者没有遵循预设的动作脚本，自遮挡现象很常见。这一现象对现有的单目人体重建方法提出了挑战，因为这些方法通常假设人体全身可见。</p><p>(2) 以往的方法及其问题：以往的人体重建方法大多假设人体全身可见，但在现实世界的无脚本捕捉中，这一假设往往不成立。因此，对于自遮挡的问题，仅仅进行重建是不够的。</p><p>(3) 研究方法：本文提出了SOAR（Self-Occluded Avatar Recovery）方法，一个针对从部分观察中恢复完整人物的通用系统。该方法利用结构正常先验和生成扩散先验来解决这个不适定的问题。对于结构正常先验，使用可置换的surfel模型，具有明确且易于理解的形状。对于生成扩散先验，则进行初步重建并使用得分蒸馏进行细化。</p><p>(4) 任务与性能：本文的方法在多种基准测试上进行了评估，并与最新的重建和生成方法进行了比较。实验结果表明，本文提出的方法在性能上优于现有的技术，并在某些方面与并行的研究工作持平。此外，SOAR能够从自我遮挡的视频中恢复出具有完整纹理和形状的人物角色，为虚拟现实、机器人和内容创建等领域的应用提供了重要的技术支持。</p><ol><li>Methods:</li></ol><p>(1) 研究背景分析：针对现实世界中无脚本的人物捕捉存在的自遮挡问题，传统的人体重建方法因假设人体全身可见而面临挑战。</p><p>(2) 方法论概述：本文提出了SOAR（Self-Occluded Avatar Recovery）方法，这是一个从部分观察到恢复完整人物的通用系统。主要包括两个核心部分：结构正常先验和生成扩散先验。</p><p>(3) 结构正常先验：使用可置换的surfel模型进行建模。该模型具有明确且易于理解的形状，能够为人体的正常结构提供有效的描述。</p><p>(4) 生成扩散先验：首先进行初步的人物重建，然后利用得分蒸馏技术对其进行细化。这一步骤借助扩散模型，通过不断迭代和优化，从部分观察到的信息中恢复出完整的人物角色。</p><p>(5) 实验验证：本文的方法在多种基准测试上进行了评估，与最新的重建和生成方法进行比较。实验结果表明，该方法在性能上优于现有技术，并且在某些方面达到并行的研究水平。此外，SOAR还能够从自遮挡的视频中恢复出具有完整纹理和形状的人物角色。</p><p>以上内容基于论文的总结和分析，具体细节可能还需要参考论文原文。</p><ol><li><p>Conclusion:</p><ul><li><p>(1) 这项工作的意义在于解决现实世界中无脚本人物捕捉的自遮挡问题，对于虚拟现实、机器人和内容创建等领域有重要的应用价值。</p></li><li><p>(2) 创新点：本文提出的SOAR方法利用结构正常先验和生成扩散先验，从部分观察到恢复完整人物，具有通用性。性能：在多种基准测试上评估，实验结果表明该方法在性能上优于现有技术。工作量：文章对方法的实现进行了详细的描述，但缺少关于大规模实际应用或优化运行时间的讨论。</p></li></ul></li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="./crop_元宇宙_虚拟人/2410.23800v1/page_0_0.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="./crop_元宇宙_虚拟人/2410.23800v1/page_1_0.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="./crop_元宇宙_虚拟人/2410.23800v1/page_2_0.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="./crop_元宇宙_虚拟人/2410.23800v1/page_4_0.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="./crop_元宇宙_虚拟人/2410.23800v1/page_5_0.jpg" align="middle"></details></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io/2024/11/05/Paper/2024-11-05/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">https://kedreamix.github.io/2024/11/05/Paper/2024-11-05/元宇宙_虚拟人/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">元宇宙/虚拟人</a></div><div class="post_share"><div class="social-share" data-image="https://picx.zhimg.com/v2-afd271b7cd6a3967f3b9c44e1dc6f579.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/11/05/Paper/2024-11-05/Talking%20Head%20Generation/" title="Talking Head Generation"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-5beacc9e107561f5a4f3fc35792c4159.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Talking Head Generation</div></div></a></div><div class="next-post pull-right"><a href="/2024/10/30/Paper/2024-10-30/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-682a1dec5a14943511f0a2de2904313d.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Diffusion Models</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/01/24/Paper/2024-01-24/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-0f13a2b60bef4c886a3317754c99b456.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-24</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/04/01/Paper/2024-04-01/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-4f2a829065c463be027e4b423c4e43c8.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-01</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/04/06/Paper/2024-04-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-4f97970c093585e18e2db42fb96a6b75.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-06</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/05/02/Paper/2024-05-02/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-37516691b13dabbabb3b74ea46b402d8.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-02</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/05/06/Paper/2024-05-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-65e21e3a0a320adc36f81e6bfc7c5739.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-06</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/05/22/Paper/2024-05-22/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-b4943be56cf94e02149b50eec541d27f.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-22</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-11-05-%E6%9B%B4%E6%96%B0"><span class="toc-text">2024-11-05 更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#InstantGeoAvatar-Effective-Geometry-and-Appearance-Modeling-of-Animatable-Avatars-from-Monocular-Video"><span class="toc-text">InstantGeoAvatar: Effective Geometry and Appearance Modeling of Animatable Avatars from Monocular Video</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#URAvatar-Universal-Relightable-Gaussian-Codec-Avatars"><span class="toc-text">URAvatar: Universal Relightable Gaussian Codec Avatars</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SOAR-Self-Occluded-Avatar-Recovery-from-a-Single-Video-In-the-Wild"><span class="toc-text">SOAR: Self-Occluded Avatar Recovery from a Single Video In the Wild</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-11-05-%E6%9B%B4%E6%96%B0-1"><span class="toc-text">2024-11-05 更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#InstantGeoAvatar-Effective-Geometry-and-Appearance-Modeling-of-Animatable-Avatars-from-Monocular-Video-1"><span class="toc-text">InstantGeoAvatar: Effective Geometry and Appearance Modeling of Animatable Avatars from Monocular Video</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#URAvatar-Universal-Relightable-Gaussian-Codec-Avatars-1"><span class="toc-text">URAvatar: Universal Relightable Gaussian Codec Avatars</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%B7%A5%E4%BD%9C%E6%84%8F%E4%B9%89%EF%BC%9A"><span class="toc-text">(1) 工作意义：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%88%9B%E6%96%B0%E7%82%B9%E3%80%81%E6%80%A7%E8%83%BD%E3%80%81%E5%B7%A5%E4%BD%9C%E9%87%8F%E6%A2%B3%E7%90%86%EF%BC%9A"><span class="toc-text">(2) 创新点、性能、工作量梳理：</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SOAR-Self-Occluded-Avatar-Recovery-from-a-Single-Video-In-the-Wild-1"><span class="toc-text">SOAR: Self-Occluded Avatar Recovery from a Single Video In the Wild</span></a></li></ol></div></div></div></div></main><footer id="footer" style="background-image:url('https://picx.zhimg.com/v2-afd271b7cd6a3967f3b9c44e1dc6f579.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js").then((()=>{pangu.autoSpacingPage()}))}function panguInit(){panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.1},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script></div><script id="canvas_nest" defer color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><script data-pjax>function butterfly_swiper_injector_config(){var a=document.getElementById("recent-posts");console.log("已挂载butterfly_swiper"),a.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/25/Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="2024/01/25/Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="2024/01/25/Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/17/Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023/12/17/Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="2023/12/17/Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script></body></html>