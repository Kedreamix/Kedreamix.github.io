<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>元宇宙/虚拟人 | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="元宇宙&#x2F;虚拟人 方向最新论文已更新，请持续关注 Update in 2024-11-27  DynamicAvatars Accurate Dynamic Facial Avatars Reconstruction and   Precise Editing with Diffusion Models">
<meta property="og:type" content="article">
<meta property="og:title" content="元宇宙&#x2F;虚拟人">
<meta property="og:url" content="https://kedreamix.github.io/2024/11/27/Paper/2024-11-27/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/index.html">
<meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World">
<meta property="og:description" content="元宇宙&#x2F;虚拟人 方向最新论文已更新，请持续关注 Update in 2024-11-27  DynamicAvatars Accurate Dynamic Facial Avatars Reconstruction and   Precise Editing with Diffusion Models">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://picx.zhimg.com/v2-4d3fe89cd1b4f0d62aff8e384da212b6.jpg">
<meta property="article:published_time" content="2024-11-26T16:57:51.000Z">
<meta property="article:modified_time" content="2024-11-26T16:57:51.506Z">
<meta property="article:author" content="Kedreamix">
<meta property="article:tag" content="元宇宙&#x2F;虚拟人">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://picx.zhimg.com/v2-4d3fe89cd1b4f0d62aff8e384da212b6.jpg"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/2024/11/27/Paper/2024-11-27/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-74LZ5BEQQ1');
</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":true,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  noticeOutdate: {"limitDay":500,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: true,
  islazyload: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '元宇宙/虚拟人',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-11-27 00:57:51'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const now = new Date()
          const hour = now.getHours()
          const isNight = hour <= 6 || hour >= 24
          if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
          else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css"  media="defer" onload="this.media='all'"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload="this.media='all'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.0.0"><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml">

<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()
  window.addEventListener('load',() => { preloader.endLoading() })

  if (false) {
    document.addEventListener('pjax:send', () => { preloader.initLoading() })
    document.addEventListener('pjax:complete', () => { preloader.endLoading() })
  }
})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">283</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://picx.zhimg.com/v2-4d3fe89cd1b4f0d62aff8e384da212b6.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"/><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">元宇宙/虚拟人</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-11-26T16:57:51.000Z" title="发表于 2024-11-27 00:57:51">2024-11-27</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-11-26T16:57:51.506Z" title="更新于 2024-11-27 00:57:51">2024-11-27</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">7.6k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>25分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="元宇宙/虚拟人"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote>
<p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-11-27-更新"><a href="#2024-11-27-更新" class="headerlink" title="2024-11-27 更新"></a>2024-11-27 更新</h1><h2 id="DynamicAvatars-Accurate-Dynamic-Facial-Avatars-Reconstruction-and-Precise-Editing-with-Diffusion-Models"><a href="#DynamicAvatars-Accurate-Dynamic-Facial-Avatars-Reconstruction-and-Precise-Editing-with-Diffusion-Models" class="headerlink" title="DynamicAvatars: Accurate Dynamic Facial Avatars Reconstruction and   Precise Editing with Diffusion Models"></a>DynamicAvatars: Accurate Dynamic Facial Avatars Reconstruction and   Precise Editing with Diffusion Models</h2><p><strong>Authors:Yangyang Qian, Yuan Sun, Yu Guo</strong></p>
<p>Generating and editing dynamic 3D head avatars are crucial tasks in virtual reality and film production. However, existing methods often suffer from facial distortions, inaccurate head movements, and limited fine-grained editing capabilities. To address these challenges, we present DynamicAvatars, a dynamic model that generates photorealistic, moving 3D head avatars from video clips and parameters associated with facial positions and expressions. Our approach enables precise editing through a novel prompt-based editing model, which integrates user-provided prompts with guiding parameters derived from large language models (LLMs). To achieve this, we propose a dual-tracking framework based on Gaussian Splatting and introduce a prompt preprocessing module to enhance editing stability. By incorporating a specialized GAN algorithm and connecting it to our control module, which generates precise guiding parameters from LLMs, we successfully address the limitations of existing methods. Additionally, we develop a dynamic editing strategy that selectively utilizes specific training datasets to improve the efficiency and adaptability of the model for dynamic editing tasks. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.15732v1">PDF</a> </p>
<p><strong>Summary</strong><br>动态3D头像生成与编辑技术，通过新型模型与GAN算法，实现高精度、适应性强的高仿真动态头像制作。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>动态3D头像在虚拟现实与电影制作中至关重要。</li>
<li>现有方法存在面部扭曲、不准确头部动作和编辑能力有限等问题。</li>
<li>DynamicAvatars模型从视频片段和面部位置与表情参数生成逼真3D头像。</li>
<li>引入基于提示的编辑模型，结合用户提示和LLMs参数进行精确编辑。</li>
<li>采用Gaussian Splatting双跟踪框架和提示预处理模块增强编辑稳定性。</li>
<li>专用GAN算法与控制模块连接，生成精确指导参数。</li>
<li>动态编辑策略利用特定训练数据集提高效率和适应性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li><p>Title: 动态头像重建与精确编辑：DynamicAvatars研究</p>
</li>
<li><p>Authors: 杨阳钱，袁森，郭宇（等）</p>
</li>
<li><p>Affiliation: 西安电子科技大学软件工程学院（杨阳钱）；西安电子科技大学人机混合智能国家重点实验室、西安电子科技大学视觉信息与应用的国家工程研究中心以及西安电子科技大学人工智能与机器人研究所（袁森、郭宇等）。</p>
</li>
<li><p>Keywords: 动态头像重建，精确编辑，动态模型，语言模型指导参数，高斯投影法，GAN算法。</p>
</li>
<li><p>Urls: 文章抽象链接（待补充）；代码GitHub链接（待补充，如果没有则为Github:None）。</p>
</li>
<li><p>Summary: </p>
<ul>
<li><p>(1) 研究背景：本文研究了虚拟现实和电影制作中的关键任务——动态3D头像的生成与编辑。由于现有方法在面部细节捕捉、头部运动准确性以及精细编辑能力方面的局限性，本文提出了DynamicAvatars模型。</p>
</li>
<li><p>(2) 过去的方法及其问题：传统方法使用明确的3D表示（如点云和网格），难以准确捕捉精细的几何细节。隐式3D表示方法虽然解决了这些问题，但在动态性和精细编辑方面仍有不足。本文方法受到挑战驱动，旨在解决现有方法的不足。</p>
</li>
<li><p>(3) 研究方法：本文提出了DynamicAvatars模型，通过视频剪辑和与面部位置和表情相关的参数生成逼真的动态3D头像。通过结合高斯投影法和大型语言模型（LLM）的指导参数，提出了一种基于提示的编辑模型。还开发了一种动态编辑策略，选择性利用特定训练数据集以提高模型的效率和适应性。</p>
</li>
<li><p>(4) 任务与性能：本文方法在动态头像重建和编辑任务上取得了显著成果。通过生成逼真的动态头像和精细编辑能力，证明了方法的有效性。此外，实验结果表明，该方法在效率和适应性方面也有所提高。性能结果支持了方法的目标。</p>
</li>
</ul>
</li>
</ol>
<p>希望以上回答能满足您的要求！</p>
<ol>
<li>方法论概述：</li>
</ol>
<p>本文提出了一种基于动态头像重建与精确编辑的研究方法，旨在解决虚拟现实和电影制作中的关键任务——动态3D头像的生成与编辑。方法论的主要步骤包括：</p>
<pre><code>- (1) 背景与问题定义：针对现有方法在面部细节捕捉、头部运动准确性以及精细编辑能力方面的局限性，提出DynamicAvatars模型的研究背景及挑战。

- (2) 研究方法：提出DynamicAvatars模型，通过视频剪辑和与面部位置和表情相关的参数生成逼真的动态3D头像。结合高斯投影法和大型语言模型（LLM）的指导参数，提出了一种基于提示的编辑模型。还开发了一种动态编辑策略，选择性利用特定训练数据集以提高模型的效率和适应性。

- (3) 语义基于网格高斯跟踪：为实现头部头像的灵活编辑，包括表情、纹理和附加配件的编辑，采用一种能够精确重建头部模型并易于编辑的技术是关键。引入了一种新颖的网格高斯绑定方法，与现有的Gaussian Avatars方法有所不同。该方法包括两个高斯跟踪模式，用于处理过程中的不同阶段。首先，通过光度学头部跟踪器拟合FLAME参数来处理输入视频。接下来，应用面部组成标识符生成语义蒙版，以确保在渲染图像时具有相同语义标签的高斯点始终一致，维持动态场景中的时间一致性。同时，将渲染结果与真实图像进行比较以训练头像。

- (4) 动态高斯编辑：传统3D编辑方法依赖于静态2D或3D蒙版来限制特定区域的变化。然而，这种方法在训练过程中的动态更新会导致静态蒙版不准确，从而限制其有效性。为了解决这个问题，本文利用双重跟踪方法来维持高斯点的相对位置，便于后续的编辑过程。提出了一种方法，能够考虑不同时间和姿势下对结果做出贡献的所有高斯点。通过利用映射网络来生成不同时间和姿态下的目标区域蒙版，我们能够追踪动态场景中目标区域的贡献高斯点。接下来，对选定集中的每个图像进行编辑以生成编辑后的图像集。最后，应用带有条件对抗损失的学习过程，以调节高斯点并保持时间一致性。

- (5) LLM精细编辑：针对之前工作在面对详细提示时的困境，例如方向、相对位置等信息的理解难题，我们利用LLM进行精细编辑。为了在面对这些困难条件时提高生成结果的质量，我们专注于解决与编辑相关的错位和误解问题，并基于精确详细的提示添加配件。提出了一个类似于SLD的框架，为精细编辑提供了实用方法。我们根据LLM重新调整提示结构，然后仔细修改由先前阶段生成的图像。这种图像校正基于潜在空间的操纵，并包含我们方法中的多视图一致性对齐。

- (6) 损失函数与正则化：主要损失应集中在渲染的图像上。因此采用了如下颜色损失函数：Lrgb = λL2(I, ˆI) + (1 − λ)Llpips(I, ˆI)。此外，还需要关注跟踪损失，该损失集中于处理网格和高斯点之间的相对位置以及特定语义区域与高斯点之间的关联。为了维持模型的基本结构并在编辑阶段监督位置和分布的损失以及优化每个高斯点的物理参数，需要采用一种能够惩罚点错位的损失函数。
</code></pre><ol>
<li>Conclusion: </li>
</ol>
<ul>
<li>(1) 这项工作的意义在于提出了一种基于动态头像重建与精确编辑的研究方法，解决了虚拟现实和电影制作中动态3D头像生成与编辑的关键问题，提高了头部模型的精确性和编辑能力。</li>
<li><p>(2) 创新点：本文提出了DynamicAvatars模型，通过结合视频剪辑、面部位置和表情参数，生成逼真的动态3D头像，并提出了一种基于提示的编辑模型和高斯投影法，提高了模型的效率和适应性。同时，引入大型语言模型（LLM）进行精细编辑，解决了传统方法在面对详细提示时的困境。</p>
<p>Performance: 该方法在动态头像重建和编辑任务上取得了显著成果，生成了逼真的动态头像并具备精细编辑能力。实验结果表明，该方法在效率和适应性方面也有所提高。</p>
<p>Workload: 文章详细阐述了方法论的主要步骤，包括背景与问题定义、研究方法、语义基于网格高斯跟踪、动态高斯编辑、LLM精细编辑以及损失函数与正则化等方面，工作量较大，但内容条理清晰，易于理解。</p>
</li>
</ul>
<details>
  <summary>点此查看论文截图</summary>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-4d3fe89cd1b4f0d62aff8e384da212b6.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-f672277cd8a596cfcf43c6b67a43d85d.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-a2e9482ae6db6a001920d6d473b196f5.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-54924fed58a1038fef38fc1d922193d5.jpg" align="middle">
</details>




<h2 id="FATE-Full-head-Gaussian-Avatar-with-Textural-Editing-from-Monocular-Video"><a href="#FATE-Full-head-Gaussian-Avatar-with-Textural-Editing-from-Monocular-Video" class="headerlink" title="FATE: Full-head Gaussian Avatar with Textural Editing from Monocular   Video"></a>FATE: Full-head Gaussian Avatar with Textural Editing from Monocular   Video</h2><p><strong>Authors:Jiawei Zhang, Zijian Wu, Zhiyang Liang, Yicheng Gong, Dongfang Hu, Yao Yao, Xun Cao, Hao Zhu</strong></p>
<p>Reconstructing high-fidelity, animatable 3D head avatars from effortlessly captured monocular videos is a pivotal yet formidable challenge. Although significant progress has been made in rendering performance and manipulation capabilities, notable challenges remain, including incomplete reconstruction and inefficient Gaussian representation. To address these challenges, we introduce FATE, a novel method for reconstructing an editable full-head avatar from a single monocular video. FATE integrates a sampling-based densification strategy to ensure optimal positional distribution of points, improving rendering efficiency. A neural baking technique is introduced to convert discrete Gaussian representations into continuous attribute maps, facilitating intuitive appearance editing. Furthermore, we propose a universal completion framework to recover non-frontal appearance, culminating in a 360$^\circ$-renderable 3D head avatar. FATE outperforms previous approaches in both qualitative and quantitative evaluations, achieving state-of-the-art performance. To the best of our knowledge, FATE is the first animatable and 360$^\circ$ full-head monocular reconstruction method for a 3D head avatar. The code will be publicly released upon publication. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.15604v1">PDF</a> project page: <a target="_blank" rel="noopener" href="https://zjwfufu.github.io/FATE-page/">https://zjwfufu.github.io/FATE-page/</a></p>
<p><strong>Summary</strong><br>单目视频重建可编辑全头3D头像，FATE方法实现高效性能。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>单目视频重建3D头像具挑战性。</li>
<li>FATE方法解决重建和表示效率问题。</li>
<li>样本密集化策略优化点分布。</li>
<li>神经烘焙技术实现属性图编辑。</li>
<li>完成框架恢复非正面外观。</li>
<li>FATE在性能评估中优于先前方法。</li>
<li>FATE为首个可动画和360°单目全头重建方法。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li><p>Title: FATE：基于纹理编辑的全头高斯化身技术（Full-head Gaussian Avatar with Textural Editing）</p>
</li>
<li><p>Authors: xxx</p>
</li>
<li><p>Affiliation: 暂无作者所属机构信息。</p>
</li>
<li><p>Keywords: 3D头像重建，单目视频，纹理编辑，高斯渲染</p>
</li>
<li><p>Urls: 由于没有提供论文的网址和Github代码链接，所以无法填写。</p>
</li>
<li><p>Summary:</p>
</li>
</ol>
<p>(1)研究背景：本文的研究背景是关于如何从单目视频中重建高保真、可动画的3D头像。尽管已有许多方法在此领域取得了进展，但仍面临诸如重建不完整和效率低下的挑战。本文旨在解决这些问题，提出一种基于单目视频的全头高斯化身技术（FATE）。</p>
<p>(2)过去的方法和存在的问题：现有的方法在处理单目视频时，往往存在重建不完整和纹理表示效率低下的问题。这些方法缺乏优化策略，无法有效地从单目视频中提取足够的信息来重建完整的头部模型。此外，传统的纹理表示方法在处理复杂的头部纹理时效率低下，不利于进行直观的外貌编辑。</p>
<p>(3)研究方法：本文提出了一种基于单目视频的全头高斯化身技术（FATE）。首先，采用基于采样的密集化策略，确保点的最优位置分布，提高渲染效率。其次，引入神经烘焙技术，将离散的高斯表示转换为连续的属性图，便于直观的外貌编辑。最后，提出了一种通用的完成框架，用于恢复非正面外貌，生成可渲染的3D头像。整个流程实现了从单目视频到完整头部模型的重建，并具有良好的可编辑性和渲染性能。</p>
<p>(4)任务与性能：本文的方法在单目视频重建任务上取得了显著的成果。实验结果表明，本文的方法在定量和定性评价方面都优于以往的方法，达到了业界最佳性能。此外，本文的方法生成了首个可动画和360度可渲染的全头单目重建方法。性能上能够满足真实场景下的需求，支持从单目视频中重建出高质量、可编辑的3D头像。</p>
<ol>
<li><p>方法：</p>
<ul>
<li><p>(1) 研究背景与现状：文章介绍了当前单目视频3D头像重建领域的背景和研究现状，指出了现有方法在处理单目视频时面临的一些挑战，如重建不完整和效率低下的问题。</p>
</li>
<li><p>(2) 研究方法：针对现有方法的不足，文章提出了一种基于单目视频的全头高斯化身技术（FATE）。首先，采用基于采样的密集化策略，确保点的最优位置分布，提高渲染效率。其次，引入神经烘焙技术，将离散的高斯表示转换为连续的属性图，便于直观的外貌编辑。最后，提出了一个通用的完成框架，用于恢复非正面外貌，生成可渲染的3D头像。</p>
</li>
<li><p>(3) 实验流程：在实验中，文章首先介绍了整体的单目重建方法，然后详细解释了采样密集化、神经烘焙、通用完成框架等模块的具体实现细节。并通过实验验证了方法的有效性。</p>
</li>
<li><p>(4) 结果分析：文章通过对比实验和性能评估，证明了所提出的方法在单目视频重建任务上取得了显著成果，优于以往的方法，并满足了真实场景下的需求，能够生成高质量、可编辑的3D头像。</p>
</li>
<li><p>(5) 局限性及未来工作：文章还讨论了一些局限性以及未来的研究方向，例如如何提高渲染质量、进一步优化模型性能等。同时，也提出了一些改进建议，如采用更先进的采样策略、优化神经烘焙技术等。</p>
</li>
</ul>
</li>
<li>Conclusion:</li>
</ol>
<ul>
<li>(1)工作意义：该研究提出了一种基于单目视频的全头高斯化身技术（FATE），实现了从单目视频中重建高保真、可动画的3D头像，为3D头像重建领域带来了新的突破和进展。</li>
<li>(2)创新点、性能、工作量方面总结：<ul>
<li>创新点：文章引入了基于采样的密集化策略，提高了渲染效率；采用神经烘焙技术，将离散的高斯表示转换为连续的属性图，便于直观的外貌编辑。</li>
<li>性能：文章的方法在单目视频重建任务上取得了显著成果，优于以往的方法，达到了业界最佳性能，满足真实场景下的需求。</li>
<li>工作量：文章进行了详细的实验和性能评估，证明了所提出方法的有效性。然而，文章也存在一定的局限性，如复杂和个性化的发型完成具有挑战性，神经烘焙技术的纹理映射在编辑细节时可能存在失败的情况。</li>
</ul>
</li>
</ul>
<p>总体来说，该文章在3D头像重建领域具有一定的创新性和实用性，为从单目视频中重建高质量、可编辑的3D头像提供了一种新的方法。</p>
<details>
  <summary>点此查看论文截图</summary>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-1414a6f48e6acfc31c3aef7df45abe55.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-a25cd0a9be1e99eab8cee7ec90dd306a.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-be3bf3e742614166844c32d32eebc961.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-4a7b59d1fd70b5cc59441dca6930a5da.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-83a22a649f114ac5c1216342056be71e.jpg" align="middle">
</details>




<h2 id="ConsistentAvatar-Learning-to-Diffuse-Fully-Consistent-Talking-Head-Avatar-with-Temporal-Guidance"><a href="#ConsistentAvatar-Learning-to-Diffuse-Fully-Consistent-Talking-Head-Avatar-with-Temporal-Guidance" class="headerlink" title="ConsistentAvatar: Learning to Diffuse Fully Consistent Talking Head   Avatar with Temporal Guidance"></a>ConsistentAvatar: Learning to Diffuse Fully Consistent Talking Head   Avatar with Temporal Guidance</h2><p><strong>Authors:Haijie Yang, Zhenyu Zhang, Hao Tang, Jianjun Qian, Jian Yang</strong></p>
<p>Diffusion models have shown impressive potential on talking head generation. While plausible appearance and talking effect are achieved, these methods still suffer from temporal, 3D or expression inconsistency due to the error accumulation and inherent limitation of single-image generation ability. In this paper, we propose ConsistentAvatar, a novel framework for fully consistent and high-fidelity talking avatar generation. Instead of directly employing multi-modal conditions to the diffusion process, our method learns to first model the temporal representation for stability between adjacent frames. Specifically, we propose a Temporally-Sensitive Detail (TSD) map containing high-frequency feature and contours that vary significantly along the time axis. Using a temporal consistent diffusion module, we learn to align TSD of the initial result to that of the video frame ground truth. The final avatar is generated by a fully consistent diffusion module, conditioned on the aligned TSD, rough head normal, and emotion prompt embedding. We find that the aligned TSD, which represents the temporal patterns, constrains the diffusion process to generate temporally stable talking head. Further, its reliable guidance complements the inaccuracy of other conditions, suppressing the accumulated error while improving the consistency on various aspects. Extensive experiments demonstrate that ConsistentAvatar outperforms the state-of-the-art methods on the generated appearance, 3D, expression and temporal consistency. Project page: <a target="_blank" rel="noopener" href="https://njust-yang.github.io/ConsistentAvatar.github.io/">https://njust-yang.github.io/ConsistentAvatar.github.io/</a> </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.15436v1">PDF</a> </p>
<p><strong>Summary</strong><br>本文提出ConsistentAvatar，通过时间敏感细节（TSD）映射实现时间一致性，显著提高虚拟人头像生成的一致性和真实性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在头像生成中存在时间、3D或表情不一致问题。</li>
<li>ConsistentAvatar框架解决时间一致性及高保真问题。</li>
<li>TSD映射包含时间轴上变化显著的高频特征和轮廓。</li>
<li>使用时间一致性扩散模块学习对齐TSD。</li>
<li>初始结果与视频帧真值对齐，生成最终头像。</li>
<li>TSD映射确保时间稳定性，抑制累积误差。</li>
<li>ConsistentAvatar在多方面优于现有技术。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li><p>标题：ConsistentAvatar: Learning to Diffuse Fully Consistent Talking<br>中文翻译：一致头像：学习扩散完全一致的说话</p>
</li>
<li><p>作者：Haijie Yang, Zhenyu Zhang, Hao Tang, Jianjun Qian, Jian Yang<em>（带有</em>号为通讯作者）</p>
</li>
<li><p>隶属机构：南京科技大学、南京大学、北京大学</p>
</li>
<li><p>关键词：ConsistentAvatar, 扩散模型, 说话头生成, 暂时性一致性, 3D一致性, 表情一致性</p>
</li>
<li><p>Urls：论文链接（待提供），GitHub代码链接（待提供，如果没有则为None）</p>
</li>
<li><p>总结：</p>
</li>
</ol>
<p>(1) 研究背景：<br>随着技术的发展，生成说话头像的需求逐渐增加。扩散模型在生成任务中展现出巨大的潜力，但在生成说话头像时面临一些问题，如暂时性、3D和表情的不一致性。本文旨在解决这些问题，提出一种全新的框架。</p>
<p>(2) 过去的方法及问题：<br>过去的方法主要基于单图像生成能力，但由于误差累积和固有局限性，导致生成的头像存在不一致性问题。</p>
<p>(3) 研究方法：<br>本文提出ConsistentAvatar框架，一个用于生成完全一致和高保真度说话头像的新型框架。该方法不直接将多模式条件应用于扩散过程，而是学习首先建立时间表示以提高稳定性。通过建模时间表示，框架能够在生成过程中保持一致性。</p>
<p>(4) 任务与性能：<br>本文的方法在生成说话头像的任务上取得了显著成果，解决了暂时性、3D和表情的不一致性问题。通过对比实验和真实数据，验证了该方法在缓解时间不一致性方面的显著效果。性能结果支持了该方法的有效性。</p>
<p>请注意，具体的GitHub链接和论文链接待提供，关键词和某些细节可能根据原始论文有所不同，建议查阅原始论文以获取更详细和准确的信息。</p>
<ol>
<li>方法论概述：</li>
</ol>
<p>(1) 研究问题定义：本文旨在解决生成说话头像时面临的暂时性、3D和表情的不一致性问题。</p>
<p>(2) 数据集准备：研究使用了相关的说话头像数据集，为了训练和验证所提出的方法。</p>
<p>(3) 方法框架介绍：提出了ConsistentAvatar框架，该框架旨在通过建模时间表示来解决多模式条件下的扩散模型的不一致性问题。框架不直接将多模式条件应用于扩散过程，而是通过建立时间表示来提高生成的稳定性。这种方法确保了生成过程中的一致性。</p>
<p>(4) 实验设计与实施：在生成说话头像的任务上进行了大量实验，通过对比实验和真实数据验证了框架的有效性。实验结果表明，该方法在解决不一致性问题方面取得了显著成果。</p>
<p>(5) 性能评估：使用特定的评估指标和方法对ConsistentAvatar框架进行了性能评估，证明了其有效性和优越性。</p>
<p>注：具体细节，如数据集、实验设置和性能评估方法，需参考原始论文以获取更详细和准确的信息。</p>
<ol>
<li>Conclusion:</li>
</ol>
<p>(1) 这项工作的意义在于解决生成说话头像时面临的不一致性问题，包括暂时性、3D和表情的不一致性。它提出了一种新的框架ConsistentAvatar，能够生成完全一致的、高保真度的说话头像，这对于虚拟角色、动画制作、游戏开发等领域具有重要的应用价值。</p>
<p>(2) 创新点：本文提出了ConsistentAvatar框架，通过建模时间表示来解决多模式条件下的扩散模型的不一致性问题，这是一种全新的尝试和探索。</p>
<p>性能：在生成说话头像的任务上，该方法取得了显著成果，解决了暂时性、3D和表情的不一致性问题，实验结果表明其有效性。</p>
<p>工作量：文章对问题的背景、过去的方法及问题、研究方法、任务与性能等方面进行了详细的阐述和总结，表明作者们进行了充分的研究和实验。但是，关于代码和数据的具体细节，需要参考原始论文以获取更详细和准确的信息。</p>
<details>
  <summary>点此查看论文截图</summary>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-8eb20a025344d901e59ae5318e834480.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-f134fe5b2ba0ae6810c4305b0eaa577c.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-0213273488eef4bb852af1dc84450fef.jpg" align="middle">
</details>




<h2 id="DAGSM-Disentangled-Avatar-Generation-with-GS-enhanced-Mesh"><a href="#DAGSM-Disentangled-Avatar-Generation-with-GS-enhanced-Mesh" class="headerlink" title="DAGSM: Disentangled Avatar Generation with GS-enhanced Mesh"></a>DAGSM: Disentangled Avatar Generation with GS-enhanced Mesh</h2><p><strong>Authors:Jingyu Zhuang, Di Kang, Linchao Bao, Liang Lin, Guanbin Li</strong></p>
<p>Text-driven avatar generation has gained significant attention owing to its convenience. However, existing methods typically model the human body with all garments as a single 3D model, limiting its usability, such as clothing replacement, and reducing user control over the generation process. To overcome the limitations above, we propose DAGSM, a novel pipeline that generates disentangled human bodies and garments from the given text prompts. Specifically, we model each part (e.g., body, upper/lower clothes) of the clothed human as one GS-enhanced mesh (GSM), which is a traditional mesh attached with 2D Gaussians to better handle complicated textures (e.g., woolen, translucent clothes) and produce realistic cloth animations. During the generation, we first create the unclothed body, followed by a sequence of individual cloth generation based on the body, where we introduce a semantic-based algorithm to achieve better human-cloth and garment-garment separation. To improve texture quality, we propose a view-consistent texture refinement module, including a cross-view attention mechanism for texture style consistency and an incident-angle-weighted denoising (IAW-DE) strategy to update the appearance. Extensive experiments have demonstrated that DAGSM generates high-quality disentangled avatars, supports clothing replacement and realistic animation, and outperforms the baselines in visual quality. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.15205v1">PDF</a> </p>
<p><strong>Summary</strong><br>利用文本生成解耦人体与服装，实现高质量、可替换服装的虚拟人模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本驱动虚拟人生成技术备受关注。</li>
<li>现有方法将服装与人体系统能为单一3D模型，限制使用。</li>
<li>提出DAGSM模型，生成解耦的人体与服装。</li>
<li>将身体各部分（如身体、上衣/下衣）建模为GS增强网格（GSM）。</li>
<li>引入语义算法实现人体与服装、服装与服装分离。</li>
<li>提出视图一致纹理优化模块，包括跨视图注意机制和入射角加权去噪策略。</li>
<li>实验表明，DAGSM生成高质量的解耦虚拟人，支持服装替换和真实动画。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li><p><strong>标题</strong>：DAGSM：基于GS增强的网格解纠缠式角色生成技术（中文版）</p>
</li>
<li><p><strong>作者</strong>：Jingyu Zhuang, Di Kang, Linchao Bao, Liang Lin, Guanbin Li</p>
</li>
<li><p><strong>作者所属机构</strong>：中山大学的Sun Yat-sen University 以及腾讯的Tencent（其中Jingyu Zhuang等为第一作者）</p>
</li>
<li><p><strong>关键词</strong>：文本驱动的角色生成、解纠缠式角色生成、GS增强网格、动态纹理处理、动画渲染等。</p>
</li>
<li><p><strong>链接</strong>：论文链接尚未提供，GitHub代码链接（如有）未知。请查阅相关数据库或官方渠道获取最新信息。</p>
</li>
<li><p><strong>摘要</strong>：</p>
<ul>
<li><p>(1) 研究背景：随着虚拟世界和互动娱乐技术的快速发展，高质量的数字角色生成需求日益增长。文本驱动的角色生成因其便利性而受到广泛关注，但现有方法存在局限性，如角色与服装的建模过于简化，难以实现服装替换和用户自定义控制等。本研究旨在解决这些问题。</p>
</li>
<li><p>(2) 过去的方法及问题：现有的角色生成方法大多将人体与所有服装建模为一个单一的3D模型，这限制了如服装替换等功能的实现，并降低了用户对生成过程的控制。因此，存在对一种更先进方法的迫切需求。</p>
</li>
<li><p>(3) 研究方法：本研究提出了一种名为DAGSM的新方法，用于从给定的文本提示生成解纠缠式的人体角色和服装。该方法将角色的每个部分（如身体、上衣、下装等）建模为一个增强网格（GSM），这是一个结合了二维高斯分布的传统网格，可以更好地处理复杂纹理并产生逼真的动画效果。研究中还引入了语义算法以实现更好的衣物分离和纹理优化技术，包括跨视角注意力机制和基于入射角的降噪策略等。</p>
</li>
<li><p>(4) 任务与性能：本研究的方法在生成高质量解纠缠式角色、支持服装替换和逼真动画方面取得了显著成果。实验证明，DAGSM在视觉质量上优于基线方法。这些性能的提升证明了该方法的实用性和先进性。</p>
</li>
</ul>
</li>
<li>方法概述：</li>
</ol>
<p>本方法提出了一种基于文本提示生成解纠缠式人体角色和服装的技术，命名为DAGSM。给定文本描述的人体及穿着的衣物，目标是生成高质量纹理的解纠缠式角色，其中衣物和身体被解耦并以GS增强网格（GSM）单独建模（第4.1节）。为了获得解纠缠的角色，DAGSM在不同的阶段生成不穿衣服的身体和衣物，然后进行细化步骤（图2）。其主要分为以下步骤：</p>
<pre><code>- (1) 生成身体基础模型：首先生成只穿内衣的人体模型（第4.2节）。利用文本提示和图像生成模型SD引导，结合人类先验知识SMPL-X模型进行身体生成。
- (2) 衣物生成：在后续的衣服生成阶段（第4.3节），首先创建衣物的网格代理，然后将二维高斯分布（2DGS）绑定到网格上，以获取衣物的纹理。基于网格的表示方法使得物理驱动的布料模拟更加真实，并且衣物编辑更为简单。
- (3) 纹理优化与细化：最后，提出视角一致的细化阶段（第4.4节），改进身体和衣物的纹理质量。包括跨视角注意力机制和基于入射角的降噪策略等。
</code></pre><p>在方法实现上，DAGSM利用GSM表示模型中的每个部分（身体、上衣、下装等），GSM结合了二维高斯分布的传统网格，可以更好地处理复杂纹理并产生逼真的动画效果。为了获得高质量的解纠缠角色，研究中还引入了语义算法以实现更好的衣物分离和纹理优化技术。</p>
<p>通过上述步骤，DAGSM在生成高质量解纠缠式角色、支持服装替换和逼真动画方面取得了显著成果，实验证明其在视觉质量上优于基线方法。</p>
<ol>
<li>Conclusion:</li>
</ol>
<p>(1)这篇工作的意义在于提出了一种基于文本提示生成解纠缠式人体角色和服装的技术，名为DAGSM。该技术能够解决现有角色生成方法中存在的问题，如角色与服装的建模过于简化、难以实现服装替换和用户自定义控制等。它为虚拟世界和互动娱乐领域提供了高质量的数字角色生成方案，有望为相关行业带来技术进步和创新。</p>
<p>(2)创新点：DAGSM将角色生成技术与文本驱动方法相结合，通过引入GS增强网格（GSM）来实现角色的解纠缠式生成。这种方法将角色各部分（如身体、上衣、下装等）单独建模，支持服装替换和纹理优化，提高了角色的生成质量和用户的自定义控制。<br>性能：实验证明，DAGSM在生成高质量解纠缠式角色、支持服装替换和逼真动画方面取得了显著成果，视觉质量优于基线方法。<br>工作量：文章对方法的实现进行了详细的描述，包括生成身体基础模型、衣物生成和纹理优化与细化等步骤。然而，关于方法的计算复杂度、所需的数据量和处理时间等方面未给出具体的信息。</p>
<p>总体来说，这篇文章提出了一种创新的角色生成技术，并在性能上取得了显著的成果。然而，关于方法的计算复杂度和工作量方面还需要进一步的研究和评估。</p>
<details>
  <summary>点此查看论文截图</summary>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-a70d07d258df61573718e79c308d03b1.jpg" align="middle">
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-4d8bd35a3715a7eb4fd6525625ae6978.jpg" align="middle">
</details>




</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://kedreamix.github.io/2024/11/27/Paper/2024-11-27/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">https://kedreamix.github.io/2024/11/27/Paper/2024-11-27/元宇宙_虚拟人/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">元宇宙/虚拟人</a></div><div class="post_share"><div class="social-share" data-image="https://picx.zhimg.com/v2-4d3fe89cd1b4f0d62aff8e384da212b6.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/11/27/Paper/2024-11-27/Talking%20Head%20Generation/" title="Talking Head Generation"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-e1aea1d45ab0f61c08a2347d2a6e0e21.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Talking Head Generation</div></div></a></div><div class="next-post pull-right"><a href="/2024/11/21/Paper/2024-11-21/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-b09251ad26a8d73945069c6f970ad813.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Diffusion Models</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/01/24/Paper/2024-01-24/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-0f13a2b60bef4c886a3317754c99b456.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-24</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/04/06/Paper/2024-04-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-4f97970c093585e18e2db42fb96a6b75.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-06</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/04/01/Paper/2024-04-01/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-4f2a829065c463be027e4b423c4e43c8.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-01</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/05/02/Paper/2024-05-02/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-37516691b13dabbabb3b74ea46b402d8.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-02</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/05/22/Paper/2024-05-22/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-b4943be56cf94e02149b50eec541d27f.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-22</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/05/28/Paper/2024-05-28/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-dc27e0e81b6be96603dd90e8aa23e081.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-28</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-11-27-%E6%9B%B4%E6%96%B0"><span class="toc-text">2024-11-27 更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#DynamicAvatars-Accurate-Dynamic-Facial-Avatars-Reconstruction-and-Precise-Editing-with-Diffusion-Models"><span class="toc-text">DynamicAvatars: Accurate Dynamic Facial Avatars Reconstruction and   Precise Editing with Diffusion Models</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#FATE-Full-head-Gaussian-Avatar-with-Textural-Editing-from-Monocular-Video"><span class="toc-text">FATE: Full-head Gaussian Avatar with Textural Editing from Monocular   Video</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ConsistentAvatar-Learning-to-Diffuse-Fully-Consistent-Talking-Head-Avatar-with-Temporal-Guidance"><span class="toc-text">ConsistentAvatar: Learning to Diffuse Fully Consistent Talking Head   Avatar with Temporal Guidance</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DAGSM-Disentangled-Avatar-Generation-with-GS-enhanced-Mesh"><span class="toc-text">DAGSM: Disentangled Avatar Generation with GS-enhanced Mesh</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image: url('https://picx.zhimg.com/v2-4d3fe89cd1b4f0d62aff8e384da212b6.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><!-- hexo injector body_end start --><script data-pjax>
  function butterfly_swiper_injector_config(){
    var parent_div_git = document.getElementById('recent-posts');
    var item_html = '<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/25/Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="2024/01/25/Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="2024/01/25/Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/17/Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023/12/17/Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="2023/12/17/Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>';
    console.log('已挂载butterfly_swiper')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'undefined'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_swiper_injector_config();
  }
  else if (epage === cpage){
    butterfly_swiper_injector_config();
  }
  </script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script><!-- hexo injector body_end end --></body></html>