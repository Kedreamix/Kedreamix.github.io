<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>元宇宙/虚拟人 | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="元宇宙&#x2F;虚拟人 方向最新论文已更新，请持续关注 Update in 2024-11-12  Discern-XR An Online Classifier for Metaverse Network Traffic"><meta property="og:type" content="article"><meta property="og:title" content="元宇宙&#x2F;虚拟人"><meta property="og:url" content="https://kedreamix.github.io/2024/11/12/Paper/2024-11-12/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/index.html"><meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World"><meta property="og:description" content="元宇宙&#x2F;虚拟人 方向最新论文已更新，请持续关注 Update in 2024-11-12  Discern-XR An Online Classifier for Metaverse Network Traffic"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://picx.zhimg.com/v2-461a6827c598f04a56dd76318e36ef09.jpg"><meta property="article:published_time" content="2024-11-12T02:05:27.000Z"><meta property="article:modified_time" content="2024-11-12T02:05:27.382Z"><meta property="article:author" content="Kedreamix"><meta property="article:tag" content="元宇宙&#x2F;虚拟人"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://picx.zhimg.com/v2-461a6827c598f04a56dd76318e36ef09.jpg"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/2024/11/12/Paper/2024-11-12/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-74LZ5BEQQ1")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!0,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"简"},noticeOutdate:{limitDay:500,position:"top",messagePrev:"It has been",messageNext:"days since the last update, the content of the article may be outdated."},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:200},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!0,post:!0},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!0,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"元宇宙/虚拟人",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-11-12 10:05:27"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach((e=>{n.setAttribute(e,t[e])})),document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)})),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=24?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 7.0.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml"></head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{const d=document.getElementById("loading-box"),e=document.body,o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l(),window.addEventListener("load",(()=>{o()}))})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">304</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">17</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url('https://picx.zhimg.com/v2-461a6827c598f04a56dd76318e36ef09.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">元宇宙/虚拟人</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-11-12T02:05:27.000Z" title="发表于 2024-11-12 10:05:27">2024-11-12</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-11-12T02:05:27.382Z" title="更新于 2024-11-12 10:05:27">2024-11-12</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">7.3k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>25分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="元宇宙/虚拟人"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-11-12-更新"><a href="#2024-11-12-更新" class="headerlink" title="2024-11-12 更新"></a>2024-11-12 更新</h1><h2 id="Discern-XR-An-Online-Classifier-for-Metaverse-Network-Traffic"><a href="#Discern-XR-An-Online-Classifier-for-Metaverse-Network-Traffic" class="headerlink" title="Discern-XR: An Online Classifier for Metaverse Network Traffic"></a>Discern-XR: An Online Classifier for Metaverse Network Traffic</h2><p><strong>Authors:Yoga Suhas Kuruba Manjunath, Austin Wissborn, Mathew Szymanowski, Mushu Li, Lian Zhao, Xiao-Ping Zhang</strong></p><p>In this paper, we design an exclusive Metaverse network traffic classifier, named Discern-XR, to help Internet service providers (ISP) and router manufacturers enhance the quality of Metaverse services. Leveraging segmented learning, the Frame Vector Representation (FVR) algorithm and Frame Identification Algorithm (FIA) are proposed to extract critical frame-related statistics from raw network data having only four application-level features. A novel Augmentation, Aggregation, and Retention Online Training (A2R-OT) algorithm is proposed to find an accurate classification model through online training methodology. In addition, we contribute to the real-world Metaverse dataset comprising virtual reality (VR) games, VR video, VR chat, augmented reality (AR), and mixed reality (MR) traffic, providing a comprehensive benchmark. Discern-XR outperforms state-of-the-art classifiers by 7% while improving training efficiency and reducing false-negative rates. Our work advances Metaverse network traffic classification by standing as the state-of-the-art solution.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.05184v1">PDF</a></p><p><strong>Summary</strong><br>设计Discern-XR元宇宙网络流量分类器，提高服务质量，实现高效训练与准确分类。</p><p><strong>Key Takeaways</strong></p><ol><li>提出Discern-XR元宇宙网络流量分类器。</li><li>利用分段学习与FVR算法提取关键统计数据。</li><li>首创A2R-OT算法实现在线训练。</li><li>构建综合真实元宇宙数据集。</li><li>Discern-XR性能优于现有分类器7%。</li><li>提升训练效率，降低误报率。</li><li>成为元宇宙网络流量分类的领先解决方案。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: 虚拟元宇宙网络流量分类器Discern-XR研究</p></li><li><p>Authors: Yoga Suhas Kuruba Manjunath, Austin Wissborn, Mathew Szymanowski, Mushu Li, Lian Zhao, and Xiao-Ping Zhang</p></li><li><p>Affiliation: 多伦多大学电气、计算机与生物医学工程系（针对瑜伽·库鲁巴·曼朱纳特、奥斯汀·维松博恩、马修·齐曼诺夫斯基于多伦多市；利休大学计算机科学工程系（针对李木续）；清华大学深圳国际研究生院普及无处不在数据工程实验室（针对张小平）。</p></li><li><p>Keywords: Metaverse, Extended Reality (XR), Augmented Reality (AR), Virtual Reality (VR), Mixed Reality (MR), Multi-Class Network Traffic Classification.</p></li><li><p>Urls: 论文链接（抽象中提供的链接）GitHub代码链接（如有可用，填入GitHub:None如果不可用）</p></li><li><p>Summary:</p><ul><li><p>(1)研究背景：随着元宇宙概念的兴起，包括虚拟现实（VR）、增强现实（AR）和混合现实（MR）在内的扩展现实（XR）技术变得越来越流行。用户需要头戴式显示器、软件平台、服务和网络连接来体验元宇宙，因此对网络流量管理提出了更高要求。准确的元宇宙网络流量分类对于互联网服务提供商（ISP）和路由器制造商提高服务质量（QoS）和用户体验质量（ QoE）至关重要。</p></li><li><p>(2)过去的方法及问题：已有一些网络流量分类方法，如基于决策树的方法，对于AR和云游戏的流量分类有一定的效果，但对于Metaverse其他服务的流量分类不够准确。另外，一些方法难以推广到其他Metaverse服务，如AR、MR和其他VR相关服务。因此，存在对非纯Metaverse网络流量数据的分类方法和准确性的挑战。</p></li><li><p>(3)研究方法：针对上述问题，本文提出了一种基于分段学习的元宇宙网络流量分类器Discern-XR。首先，通过Frame Vector Representation（FVR）算法和Frame Identification Algorithm（FIA）从原始网络数据中提取四个应用级别的特征。然后，使用一种新型在线训练算法Augmentation, Aggregation, and Retention Online Training（A2R-OT）来建立准确的分类模型。此外，还贡献了一个包含虚拟现实游戏、虚拟现实视频、虚拟现实聊天、增强现实和混合现实流量的现实世界Metaverse数据集，为分类提供了全面的基准测试。</p></li><li><p>(4)任务与性能：本文的方法在包含虚拟游戏、视频、聊天、AR和MR流量的现实世界Metaverse数据集上进行测试，并实现了优于现有方法的性能。Discern-XR相对于最新方法提高了约7%的分类准确性，同时提高了训练效率并降低了误报率。本研究推动了元宇宙网络流量分类领域的发展，被认为是当前的最佳解决方案。</p></li></ul></li><li><p>Conclusion:</p><ul><li><p>(1)意义：随着元宇宙概念的兴起，对网络流量管理提出了更高的要求。这篇论文研究的虚拟元宇宙网络流量分类器Discern-XR，对于互联网服务提供商（ISP）和路由器制造商提高服务质量（QoS）和用户体验质量（ QoE）具有重要意义。</p></li><li><p>(2)创新点、性能、工作量评价：<br>创新点：该文章提出了一种基于分段学习的元宇宙网络流量分类器Discern-XR，针对过去方法在非纯Metaverse网络流量数据的分类方法和准确性上存在的问题，进行了有效的改进。其贡献了一个包含虚拟现实游戏、虚拟现实视频、虚拟现实聊天、增强现实和混合现实流量的现实世界Metaverse数据集，为分类提供了全面的基准测试。<br>性能：Discern-XR相对于最新方法提高了约7%的分类准确性，同时提高了训练效率并降低了误报率，推动了元宇宙网络流量分类领域的发展。<br>工作量：文章进行了详尽的背景调研和文献综述，通过严谨的实验验证了所提出方法的有效性。同时，文章对所用数据集进行了详细的描述和处理，确保了实验结果的可靠性和可重复性。但工作量评价需要进一步了解实验的具体细节和数据处理量。</p></li></ul></li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-69855a95df20d78ce1b955ac62590360.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-4822286e484c3d81f5a2c93d5c436483.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-99ded90a4fb93e9bab4cf4e6ece82ef8.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-461a6827c598f04a56dd76318e36ef09.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-ba689f04aed5316f20a933a56e981a29.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-51dbbb893ae76e03917b97ebfa952299.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-6716b6c268c5b59669bf767a6e5375c3.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-5a67bb415e8b5f25ef646b68eb8754b4.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-aed7467a5826eb70f4fb32e33b8c7a57.jpg" align="middle"></details><h2 id="Joint-wireless-and-computing-resource-management-with-optimal-slice-selection-in-in-network-edge-metaverse-system"><a href="#Joint-wireless-and-computing-resource-management-with-optimal-slice-selection-in-in-network-edge-metaverse-system" class="headerlink" title="Joint wireless and computing resource management with optimal slice   selection in in-network-edge metaverse system"></a>Joint wireless and computing resource management with optimal slice selection in in-network-edge metaverse system</h2><p><strong>Authors:Sulaiman Muhammad Rashid, Ibrahim Aliyu, Abubakar Isah, Jihoon Lee, Sangwon Oh, Minsoo Hahn, Jinsul Kim</strong></p><p>This paper presents an approach to joint wireless and computing resource management in slice-enabled metaverse networks, addressing the challenges of inter-slice and intra-slice resource allocation in the presence of in-network computing. We formulate the problem as a mixed-integer nonlinear programming (MINLP) problem and derive an optimal solution using standard optimization techniques. Through extensive simulations, we demonstrate that our proposed method significantly improves system performance by effectively balancing the allocation of radio and computing resources across multiple slices. Our approach outperforms existing benchmarks, particularly in scenarios with high user demand and varying computational tasks.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.04561v1">PDF</a></p><p><strong>Summary</strong><br>针对元宇宙切片网络中的无线和计算资源管理问题，提出了一种优化方案，显著提升系统性能。</p><p><strong>Key Takeaways</strong></p><ol><li>研究针对元宇宙切片网络资源管理。</li><li>采用混合整数非线性规划（MINLP）建模问题。</li><li>应用标准优化技术求得最优解。</li><li>模拟结果表明方法有效提升系统性能。</li><li>方案在用户需求高、计算任务多样场景下表现优异。</li><li>超越现有基准方法。</li><li>优化无线与计算资源分配。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>标题：基于最优切片的联合无线和计算资源管理在边缘网络元宇宙系统中的应用<br>中文翻译：边缘网络元宇宙系统中基于最优切片的联合无线和计算资源管理</p></li><li><p>作者：Sulaiman Muhammad Rashid等</p></li><li><p>隶属机构：部分作者来自韩国光州庆南大学的智能电子与计算机工程系，部分作者来自哈萨克斯坦阿斯塔纳IT大学的计算与数据科学系。<br>中文翻译：部分作者隶属庆南大学智能电子与计算机工程系（韩国光州），部分作者隶属阿斯塔纳IT大学计算与数据科学系（哈萨克斯坦）。</p></li><li><p>关键词：元宇宙、切片、资源管理、网络内计算、6G网络</p></li><li><p>链接：论文链接无法确定，GitHub代码链接不可用。</p></li><li><p>总结：</p><ul><li><p>(1) 研究背景：随着技术的不断进步，网络正在向元宇宙方向发展，其中虚拟和增强现实与现实世界相融合。为了满足元宇宙的高资源需求，包括计算资源、存储资源和通信资源，需要有效的资源管理方法。</p></li><li><p>(2) 过去的方法及问题：过去的研究主要关注网络切片和资源分配，但缺乏针对网络内计算的资源管理的有效方法。现有的方法在网络切片和资源分配方面存在局限性，尤其是在处理动态用户需求和多变计算任务时效果不佳。</p></li><li><p>(3) 研究方法：本研究将问题表述为混合整数非线性规划（MINLP）问题，并使用标准优化技术求解。通过广泛的模拟仿真验证所提出方法的性能。</p></li><li><p>(4) 任务与性能：论文所提出的方法在平衡多个切片间的无线和计算资源分配方面表现出显著改进，特别是在高用户需求和多变计算任务的场景下。通过优化资源使用，提高了系统性能，超越了现有基准测试的性能。这种性能提升支持了研究目标的实现。</p></li></ul></li></ol><p>以上是对该文章的概括，希望对您有所帮助。</p><ol><li>Conclusion:</li></ol><p>(1) 这项研究的意义在于解决了基于最优切片的联合无线和计算资源在边缘网络元宇宙系统中的管理问题，这对于满足元宇宙的高资源需求，提高网络性能和用户体验具有重要意义。</p><p>(2) 创新性：该文章提出了一个基于混合整数非线性规划（MINLP）的问题表述，为网络内计算资源管理提供了有效方法，这在以前的研究中尚未得到充分解决。<br>性能：通过广泛的模拟仿真，验证了所提出方法的性能，显示其在平衡多个切片间的无线和计算资源分配方面表现出显著改进，系统性能超过了现有基准测试的性能。<br>工作量：文章对于问题的阐述和解决方案的提出较为简洁，但通过模拟仿真验证了所提出方法的性能，工作量适中。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-a8f29039b34e0143c278686459a68f8c.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-f8d8541ee439ff3abcb74a63630f0e4f.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-82093b84984fe8bc03c6740332d7a602.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-422acfd9959b1d6bd12c797b43a5dee4.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-85c50e0336ecc9f8ea1fe25031ca8751.jpg" align="middle"></details><h2 id="Diffusion-based-Auction-Mechanism-for-Efficient-Resource-Management-in-6G-enabled-Vehicular-Metaverses"><a href="#Diffusion-based-Auction-Mechanism-for-Efficient-Resource-Management-in-6G-enabled-Vehicular-Metaverses" class="headerlink" title="Diffusion-based Auction Mechanism for Efficient Resource Management in   6G-enabled Vehicular Metaverses"></a>Diffusion-based Auction Mechanism for Efficient Resource Management in 6G-enabled Vehicular Metaverses</h2><p><strong>Authors:Jiawen Kang, Yongju Tong, Yue Zhong, Junlong Chen, Minrui Xu, Dusit Niyato, Runrong Deng, Shiwen Mao</strong></p><p>The rise of 6G-enable Vehicular Metaverses is transforming the automotive industry by integrating immersive, real-time vehicular services through ultra-low latency and high bandwidth connectivity. In 6G-enable Vehicular Metaverses, vehicles are represented by Vehicle Twins (VTs), which serve as digital replicas of physical vehicles to support real-time vehicular applications such as large Artificial Intelligence (AI) model-based Augmented Reality (AR) navigation, called VT tasks. VT tasks are resource-intensive and need to be offloaded to ground Base Stations (BSs) for fast processing. However, high demand for VT tasks and limited resources of ground BSs, pose significant resource allocation challenges, particularly in densely populated urban areas like intersections. As a promising solution, Unmanned Aerial Vehicles (UAVs) act as aerial edge servers to dynamically assist ground BSs in handling VT tasks, relieving resource pressure on ground BSs. However, due to high mobility of UAVs, there exists information asymmetry regarding VT task demands between UAVs and ground BSs, resulting in inefficient resource allocation of UAVs. To address these challenges, we propose a learning-based Modified Second-Bid (MSB) auction mechanism to optimize resource allocation between ground BSs and UAVs by accounting for VT task latency and accuracy. Moreover, we design a diffusion-based reinforcement learning algorithm to optimize the price scaling factor, maximizing the total surplus of resource providers and minimizing VT task latency. Finally, simulation results demonstrate that the proposed diffusion-based MSB auction outperforms traditional baselines, providing better resource distribution and enhanced service quality for vehicular users.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.04139v1">PDF</a></p><p><strong>Summary</strong><br>6G车载元宇宙通过改进的MSB拍卖机制，利用无人机优化资源分配，提升AR导航性能。</p><p><strong>Key Takeaways</strong></p><ul><li>6G赋能的车载元宇宙革新了汽车行业。</li><li>车辆以虚拟双胞胎（VTs）形式参与实时应用。</li><li>VT任务资源密集，需地面基站处理。</li><li>高密度城市地区资源分配挑战显著。</li><li>无人机作为空中边缘服务器辅助基站。</li><li>信息不对称导致无人机资源分配低效。</li><li>提出基于学习的改进MSB拍卖机制优化资源分配。</li><li>设计扩散式强化学习算法优化价格缩放因子。</li><li>模拟结果显示MSB拍卖优于传统基准，提升服务质量和资源分配。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>Title: 基于扩散的拍卖机制在6G赋能的车辆元宇宙中的高效资源管理</li></ol><p>Authors: 贾文康, 童永炬, 钟悦, 陈俊龙, 徐敏睿, 倪塔杜斯, 邓润荣, 毛世文</p><ol><li><p>Affiliation:<br>第一作者贾文康的所属单位为广东工业大学自动化学院，广州市，510006，中国。</p></li><li><p>Keywords: 拍卖模型，扩散，资源分配，边缘智能，大型AI模型</p></li><li><p>Urls: 论文链接无法提供GitHub代码链接，如有需要请自行搜索相关资源。</p></li><li><p>Summary:</p><ul><li><p>(1)研究背景：<br>随着6G技术的发展，6G赋能的车辆元宇宙概念正在引领智能交通系统的革命。该文章主要探讨了在这一背景下，如何高效管理资源以提供优质的车辆服务。</p></li><li><p>(2)过去的方法及问题：<br>在6G赋能的车辆元宇宙中，车辆通过车辆孪生（VTs）来支持实时车辆应用。由于车辆服务的资源密集性，需要将这些任务卸载到地面基站（BSs）进行快速处理。然而，地面基站资源有限，特别是在人口密集的城市地区，资源分配面临巨大挑战。虽然无人机（UAVs）作为空中边缘服务器被提出作为解决方案，但由于其高移动性，存在与地面基站之间的信息不对称问题，导致资源分配效率低下。</p></li><li><p>(3)研究方法：<br>针对上述问题，文章提出了一种基于学习的改进型第二竞价（MSB）拍卖机制，以优化地面基站和无人机之间的资源分配。该机制考虑了任务延迟和准确性，并设计了一种基于扩散的强化学习算法来优化价格缩放因子，以最大化资源提供者的总盈余并最小化任务延迟。</p></li><li><p>(4)任务与性能：<br>文章通过仿真实验验证了所提出的基于扩散的MSB拍卖机制的性能。结果表明，与传统方法相比，该机制在资源分配和服务质量方面表现出更好的性能，为车辆用户提供了更好的资源分布和服务质量。</p></li></ul></li></ol><p>希望以上内容符合您的要求。</p><ol><li>Conclusion:</li></ol><ul><li>(1)意义：该研究工作在基于拍卖的资源分配方面对6G赋能的车辆元宇宙中的大型AI模型应用进行了深入探讨，具有重要的理论价值和实践意义。提出的基于扩散的拍卖机制为高效资源分配提供了新的思路和方法。</li><li>(2)创新点、性能、工作量维度评价：<ul><li>创新点：文章提出了一种基于学习的改进型第二竞价（MSB）拍卖机制，该机制结合了延迟和任务准确性作为共同价值，并采用扩散强化学习算法动态调整拍卖价格缩放因子，实现了资源的高效分配。这一创新点具有显著的技术创新性。</li><li>性能：通过仿真实验验证了所提出机制的性能，结果表明该机制在资源分配和服务质量方面表现出较好的性能，相比传统方法具有优越性。</li><li>工作量：文章详细阐述了研究背景、现状、方法及性能评价等方面，但未明确说明具体的工作量投入，如实验数据规模、计算复杂度等。需要在后续工作中进一步补充和完善相关细节。</li></ul></li></ul><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-dc7daa880b3bc7ccbd10eb71056febe9.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-c3a24ccf17dd602b2419a2c937bbc340.jpg" align="middle"></details><h2 id="LightAvatar-Efficient-Head-Avatar-as-Dynamic-Neural-Light-Field"><a href="#LightAvatar-Efficient-Head-Avatar-as-Dynamic-Neural-Light-Field" class="headerlink" title="LightAvatar: Efficient Head Avatar as Dynamic Neural Light Field"></a>LightAvatar: Efficient Head Avatar as Dynamic Neural Light Field</h2><p><strong>Authors:Huan Wang, Feitong Tan, Ziqian Bai, Yinda Zhang, Shichen Liu, Qiangeng Xu, Menglei Chai, Anish Prabhu, Rohit Pandey, Sean Fanello, Zeng Huang, Yun Fu</strong></p><p>Recent works have shown that neural radiance fields (NeRFs) on top of parametric models have reached SOTA quality to build photorealistic head avatars from a monocular video. However, one major limitation of the NeRF-based avatars is the slow rendering speed due to the dense point sampling of NeRF, preventing them from broader utility on resource-constrained devices. We introduce LightAvatar, the first head avatar model based on neural light fields (NeLFs). LightAvatar renders an image from 3DMM parameters and a camera pose via a single network forward pass, without using mesh or volume rendering. The proposed approach, while being conceptually appealing, poses a significant challenge towards real-time efficiency and training stability. To resolve them, we introduce dedicated network designs to obtain proper representations for the NeLF model and maintain a low FLOPs budget. Meanwhile, we tap into a distillation-based training strategy that uses a pretrained avatar model as teacher to synthesize abundant pseudo data for training. A warping field network is introduced to correct the fitting error in the real data so that the model can learn better. Extensive experiments suggest that our method can achieve new SOTA image quality quantitatively or qualitatively, while being significantly faster than the counterparts, reporting 174.1 FPS (512x512 resolution) on a consumer-grade GPU (RTX3090) with no customized optimization.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.18057v2">PDF</a> ECCV’24 CADL Workshop. Code: <a target="_blank" rel="noopener" href="https://github.com/MingSun-Tse/LightAvatar-TensorFlow">https://github.com/MingSun-Tse/LightAvatar-TensorFlow</a>. V2: Corrected speed benchmark with GaussianAvatar</p><p><strong>Summary</strong><br>基于NeRF的虚拟头像渲染技术受限，LightAvatar通过NeLFs实现高效渲染。</p><p><strong>Key Takeaways</strong></p><ol><li>NeRF在虚拟头像渲染中达到SOTA，但渲染速度慢。</li><li>LightAvatar基于NeLFs，无需网格或体积渲染。</li><li>模型在实时效率和训练稳定性上面临挑战。</li><li>专用网络设计降低FLOPs，提升效率。</li><li>使用预训练模型和伪数据训练。</li><li>引入变形场网络校正误差。</li><li>实验表明，LightAvatar在速度和图像质量上优于现有方法。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: LightAvatar: 基于神经光照场的高效人头虚拟化技术</p></li><li><p>Authors: 王欢, 谭飞彤, 白子谦, 张音达, 刘世琛, 徐强甬, 柴梦蕾, 普布哈 (Anish Prabhu), 潘德瑞 (Rohit Pandey), 范恩洛 (Sean Fanello), 黄增, 傅云等。</p></li><li><p>Affiliation: 作者来自东北大学（美国）和谷歌公司。</p></li><li><p>Keywords: LightAvatar, 神经光照场 (Neural Light Field), 人头虚拟化 (Head Avatar Virtualization), 渲染速度优化 (Rendering Speed Optimization), 深度学习计算机视觉 (Deep Learning Computer Vision)。</p></li><li><p>URLs: 具体链接未知（可以查阅相关的学术数据库或文献库以获取论文和代码）</p></li><li><p>Summary:</p><ul><li><p>(1) 研究背景：随着计算机视觉和深度学习的快速发展，基于视频的数字化虚拟化技术成为当前的研究热点。在娱乐、影视、游戏等领域，高质量的人头虚拟化技术具有广泛的应用前景。然而，现有的虚拟化技术存在渲染速度慢的问题，难以满足实时应用的需求。本文的研究背景是针对这一问题展开。</p></li><li><p>(2) 过去的方法及问题：传统的头像虚拟化技术主要基于三维模型和纹理映射，虽然质量较高但计算量大、渲染速度慢。近年来，基于神经辐射场（NeRF）的方法成为新的研究热点，但仍然存在速度慢的问题，限制了其在资源受限设备上的广泛应用。</p></li><li><p>(3) 研究方法：本文提出了基于神经光照场（NeLF）的LightAvatar模型，该模型直接从3DMM参数和相机姿态渲染图像，无需使用网格或体积渲染。为了解决实时效率和训练稳定性问题，研究团队引入了专门的网络设计来获得适当的NeLF模型表示，并维持了一个低的浮点运算量预算。同时，他们采用了一种基于蒸馏的训练策略，使用预训练的头像模型作为教师进行大量伪数据的合成用于训练。</p></li><li><p>(4) 任务与性能：本文的方法在头像虚拟化任务上取得了显著的性能提升，实现了快速的渲染速度并提高了图像质量。与现有的最快（性能较好）的头像虚拟化方法相比，LightAvatar达到了更高的帧率（174.1 FPS）和更好的LPIPS指标（Local Perceptual Image Similarity），从而支持了其方法的实际应用价值。</p></li></ul></li></ol><p>希望以上总结符合您的要求！</p><ol><li><p>Methods:</p><ul><li><p>(1) 研究团队首先介绍了当前计算机视觉和深度学习领域的人头虚拟化技术背景，指出传统方法和基于NeRF的方法存在的问题和挑战。</p></li><li><p>(2) 针对这些问题，研究团队提出了基于神经光照场（NeLF）的LightAvatar模型。该模型直接利用3DMM参数和相机姿态进行图像渲染，无需使用网格或体积渲染技术。</p></li><li><p>(3) 为了提高实时效率和训练稳定性，研究团队设计了专门的网络结构来获取适当的NeLF模型表示，并维持了一个较低的浮点运算量预算。</p></li><li><p>(4) 此外，研究团队采用了一种基于蒸馏的训练策略，使用预训练的头像模型作为教师，合成大量伪数据进行训练。这种策略有助于提高模型的性能和泛化能力。</p></li><li><p>(5) 最后，研究团队在头像虚拟化任务上进行了实验验证，与现有的最快（性能较好）的头像虚拟化方法相比，LightAvatar达到了更高的帧率（174.1 FPS）和更好的LPIPS指标（Local Perceptual Image Similarity），证明了其方法的有效性和实用性。</p></li></ul></li><li>Conclusion:</li></ol><p>(1) 该工作的意义在于提出了一种基于神经光照场（NeLF）的高效人头虚拟化技术，有效解决了传统虚拟化技术渲染速度慢的问题，提高了图像质量，在娱乐、影视、游戏等领域具有广泛的应用前景。此外，该研究还促进了计算机视觉和深度学习领域的技术发展。</p><p>(2) 创新点总结：本文提出的基于神经光照场（NeLF）的LightAvatar模型在头像虚拟化技术上实现了重要突破。其采用的新型网络结构和基于蒸馏的训练策略有效提高了实时效率和训练稳定性。与现有技术相比，LightAvatar在性能上取得了显著提升，达到了更高的帧率和更好的图像相似度指标。但受限于其技术和实施难度，实际应用中可能存在一定的挑战。性能上：LightAvatar在头像虚拟化任务上取得了显著的性能提升，实现了快速的渲染速度和高质量的图像。工作量上：该文章的研究团队进行了大量的实验验证和模型训练，工作量较大，但成果显著。</p><p>总的来说，该文章所提出的基于神经光照场的人头虚拟化技术具有重要的应用价值和发展前景，为相关领域的研究提供了新的思路和方法。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-c13b656c7a42614b6eb15d01a93cd2fc.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-8f0739cce843124abdd4f19bc6f3bff0.jpg" align="middle"></details><h2 id="Gaussian-Deja-vu-Creating-Controllable-3D-Gaussian-Head-Avatars-with-Enhanced-Generalization-and-Personalization-Abilities"><a href="#Gaussian-Deja-vu-Creating-Controllable-3D-Gaussian-Head-Avatars-with-Enhanced-Generalization-and-Personalization-Abilities" class="headerlink" title="Gaussian Deja-vu: Creating Controllable 3D Gaussian Head-Avatars with   Enhanced Generalization and Personalization Abilities"></a>Gaussian Deja-vu: Creating Controllable 3D Gaussian Head-Avatars with Enhanced Generalization and Personalization Abilities</h2><p><strong>Authors:Peizhi Yan, Rabab Ward, Qiang Tang, Shan Du</strong></p><p>Recent advancements in 3D Gaussian Splatting (3DGS) have unlocked significant potential for modeling 3D head avatars, providing greater flexibility than mesh-based methods and more efficient rendering compared to NeRF-based approaches. Despite these advancements, the creation of controllable 3DGS-based head avatars remains time-intensive, often requiring tens of minutes to hours. To expedite this process, we here introduce the “Gaussian Deja-vu” framework, which first obtains a generalized model of the head avatar and then personalizes the result. The generalized model is trained on large 2D (synthetic and real) image datasets. This model provides a well-initialized 3D Gaussian head that is further refined using a monocular video to achieve the personalized head avatar. For personalizing, we propose learnable expression-aware rectification blendmaps to correct the initial 3D Gaussians, ensuring rapid convergence without the reliance on neural networks. Experiments demonstrate that the proposed method meets its objectives. It outperforms state-of-the-art 3D Gaussian head avatars in terms of photorealistic quality as well as reduces training time consumption to at least a quarter of the existing methods, producing the avatar in minutes.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.16147v3">PDF</a> 11 pages, Accepted by WACV 2025 in Round 1</p><p><strong>Summary</strong><br>引入“高斯Deja-vu”框架，大幅缩短3D高斯分层头像创建时间。</p><p><strong>Key Takeaways</strong></p><ul><li>高斯分层分层（3DGS）在3D头像建模中展现出潜力。</li><li>3DGS头像创建耗时较长。</li><li>“高斯Deja-vu”框架通过通用模型与个性化调整加速头像制作。</li><li>通用模型基于大量2D图像数据集训练。</li><li>使用单目视频进行个性化调整。</li><li>提出可学习的表情感知混合映射。</li><li>方法在真实感与时间效率上优于现有技术。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: 高斯Dejavu：创建可控的3D高斯头部化身，增强通用性和个性化能力</p></li><li><p>Authors: Peizhi Yan, Rabab Ward, Qiang Tang, Shan Du</p></li><li><p>Affiliation: 第一作者Peizhi Yan的隶属单位为University of British Columbia。</p></li><li><p>Keywords: 3D Gaussian Head Avatars, Creation, Controllability, Efficient Rendering, Photorealistic Quality</p></li><li><p>Urls: 论文链接待补充，Github代码链接待补充（如果可用）。</p></li><li><p>Summary:</p><ul><li><p>(1) 研究背景：随着视频游戏、虚拟现实、增强现实、电影制作、远程存在等领域的快速发展，创建真实感强的3D头部化身变得越来越受欢迎。本文提出了高斯Dejavu方法，旨在创建一个可控的、高效的、高质量的3D高斯头部化身。</p></li><li><p>(2) 过去的方法及其问题：现有的创建3D头部化身的方法往往难以同时满足效率、质量和可控性的要求。一些方法虽然能够实现较高的质量，但训练时间过长，难以实现快速创建可控的化身。</p></li><li><p>(3) 研究方法：本文提出的高斯Dejavu框架首先通过大型2D图像数据集训练一个通用模型，然后个性化结果。通用模型使用合成的和真实的图像数据集进行训练，提供一个初始的3D高斯头部，再通过单目视频进行精细化处理，以实现个性化的头部化身。为了个性化，本文提出了可学习的表情感知校正混合图（blendmaps），以纠正初始的3D高斯模型，确保快速收敛，不依赖神经网络。</p></li><li><p>(4) 任务与性能：本文的方法在创建3D高斯头部化身的任务上取得了显著成果。实验表明，该方法在真实感质量方面优于现有的3D高斯头部化身方法，并将训练时间消耗减少了至少四分之一，能够在几分钟内生成化身。性能结果支持该方法的有效性。</p></li></ul></li><li><p>方法：</p><ul><li><p>(1) 首先，该研究通过大型2D图像数据集训练一个通用模型。数据集包括合成图像和真实图像，用于提供一个初始的3D高斯头部模型。</p></li><li><p>(2) 然后，利用单目视频对初始模型进行精细化处理，以实现个性化的头部化身。这一步的目的是纠正初始的3D高斯模型，使其更符合特定个体的特征。</p></li><li><p>(3) 为了实现个性化，该研究提出了可学习的表情感知校正混合图（blendmaps）。这种技术可以确保快速收敛，并且不依赖神经网络。通过调整blendmaps，研究能够根据不同的个体和表情对初始模型进行微调，生成具有真实感和个性化的3D头部化身。</p></li><li><p>(4) 最后，实验验证了该方法的有效性。与现有的3D高斯头部化身方法相比，该方法在真实感质量方面表现更优，并且显著减少了训练时间，能够在几分钟内生成高质量的化身。这些实验结果表明了该方法在实际应用中的潜力和价值。</p></li></ul></li><li>Conclusion:</li></ol><ul><li><p>(1) 这项工作的意义在于提出了一种创建可控的3D高斯头部化身的新方法，显著提高了通用性和个性化能力。它为视频游戏、虚拟现实、增强现实、电影制作和远程存在等领域提供了一种新的工具，能够创建真实感强的3D头部化身，有助于推动这些领域的进一步发展。</p></li><li><p>(2) 创新点：本文提出了高斯Dejavu框架，首次实现了仅通过单张图像输入重建3D高斯头部的能力，并且使用2D图像进行训练。此外，本文提出的可学习表情感知校正混合图（blendmaps）技术，能够在不依赖神经网络的情况下，实现快速收敛和调整头部表情。<br>性能：实验结果表明，该方法在渲染质量和训练速度方面均优于现有方法。与现有方法相比，该方法显著减少了训练时间，能够在几分钟内生成高质量的化身。<br>工作量：文章对方法的实现进行了详细的描述，从数据集的准备、模型的训练、到个性化调整等步骤均有详细的说明。但是，对于如何进一步优化模型以适应更广泛的面部表情，以及探索更多应用场景等方面，还需要进一步的研究和努力。</p></li></ul><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-beead99da582727cac14cb701ec01678.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-d9e3f1d2594022501a9f86c0116e76c6.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-6fcd3ef7a1064ac1787a3a9488d68df8.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-a65bfee69acf66c22c8ecbae533bebb8.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-b70ea7ba45b0d5f10b16c2dd3557a0ba.jpg" align="middle"></details></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io/2024/11/12/Paper/2024-11-12/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">https://kedreamix.github.io/2024/11/12/Paper/2024-11-12/元宇宙_虚拟人/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">元宇宙/虚拟人</a></div><div class="post_share"><div class="social-share" data-image="https://picx.zhimg.com/v2-461a6827c598f04a56dd76318e36ef09.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/11/12/Paper/2024-11-12/Talking%20Head%20Generation/" title="Talking Head Generation"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-4c5303324593840f3fc67192b765e7dc.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Talking Head Generation</div></div></a></div><div class="next-post pull-right"><a href="/2024/11/05/Paper/2024-11-05/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-ae8159aa1ab38ad341af1961f35ab00a.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Diffusion Models</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/01/24/Paper/2024-01-24/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-0f13a2b60bef4c886a3317754c99b456.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-24</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/04/01/Paper/2024-04-01/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-4f2a829065c463be027e4b423c4e43c8.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-01</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/04/06/Paper/2024-04-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-4f97970c093585e18e2db42fb96a6b75.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-06</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/05/06/Paper/2024-05-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-65e21e3a0a320adc36f81e6bfc7c5739.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-06</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/05/02/Paper/2024-05-02/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-37516691b13dabbabb3b74ea46b402d8.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-02</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/05/22/Paper/2024-05-22/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-b4943be56cf94e02149b50eec541d27f.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-22</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-11-12-%E6%9B%B4%E6%96%B0"><span class="toc-text">2024-11-12 更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Discern-XR-An-Online-Classifier-for-Metaverse-Network-Traffic"><span class="toc-text">Discern-XR: An Online Classifier for Metaverse Network Traffic</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Joint-wireless-and-computing-resource-management-with-optimal-slice-selection-in-in-network-edge-metaverse-system"><span class="toc-text">Joint wireless and computing resource management with optimal slice selection in in-network-edge metaverse system</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Diffusion-based-Auction-Mechanism-for-Efficient-Resource-Management-in-6G-enabled-Vehicular-Metaverses"><span class="toc-text">Diffusion-based Auction Mechanism for Efficient Resource Management in 6G-enabled Vehicular Metaverses</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LightAvatar-Efficient-Head-Avatar-as-Dynamic-Neural-Light-Field"><span class="toc-text">LightAvatar: Efficient Head Avatar as Dynamic Neural Light Field</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Gaussian-Deja-vu-Creating-Controllable-3D-Gaussian-Head-Avatars-with-Enhanced-Generalization-and-Personalization-Abilities"><span class="toc-text">Gaussian Deja-vu: Creating Controllable 3D Gaussian Head-Avatars with Enhanced Generalization and Personalization Abilities</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image:url('https://picx.zhimg.com/v2-461a6827c598f04a56dd76318e36ef09.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js").then((()=>{pangu.autoSpacingPage()}))}function panguInit(){panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.1},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script></div><script id="canvas_nest" defer color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><script data-pjax>function butterfly_swiper_injector_config(){var a=document.getElementById("recent-posts");console.log("已挂载butterfly_swiper"),a.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/25/Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="2024/01/25/Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="2024/01/25/Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/17/Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023/12/17/Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="2023/12/17/Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script></body></html>