<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>元宇宙/虚拟人 | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="元宇宙&#x2F;虚拟人 方向最新论文已更新，请持续关注 Update in 2024-12-06  AniGS Animatable Gaussian Avatar from a Single Image with Inconsistent   Gaussian Reconstruction"><meta property="og:type" content="article"><meta property="og:title" content="元宇宙&#x2F;虚拟人"><meta property="og:url" content="https://kedreamix.github.io/2024/12/06/Paper/2024-12-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/index.html"><meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World"><meta property="og:description" content="元宇宙&#x2F;虚拟人 方向最新论文已更新，请持续关注 Update in 2024-12-06  AniGS Animatable Gaussian Avatar from a Single Image with Inconsistent   Gaussian Reconstruction"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://picx.zhimg.com/v2-dc9887f1f37b328dfd11d4a1513a778b.jpg"><meta property="article:published_time" content="2024-12-06T14:40:19.000Z"><meta property="article:modified_time" content="2024-12-06T14:57:03.840Z"><meta property="article:author" content="Kedreamix"><meta property="article:tag" content="元宇宙&#x2F;虚拟人"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://picx.zhimg.com/v2-dc9887f1f37b328dfd11d4a1513a778b.jpg"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/2024/12/06/Paper/2024-12-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-74LZ5BEQQ1")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!0,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"简"},noticeOutdate:{limitDay:500,position:"top",messagePrev:"It has been",messageNext:"days since the last update, the content of the article may be outdated."},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:200},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!0,post:!0},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!0,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"元宇宙/虚拟人",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-12-06 22:57:03"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach((e=>{n.setAttribute(e,t[e])})),document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)})),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=24?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 7.0.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml"></head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{const d=document.getElementById("loading-box"),e=document.body,o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l(),window.addEventListener("load",(()=>{o()}))})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">304</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">17</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url('https://picx.zhimg.com/v2-dc9887f1f37b328dfd11d4a1513a778b.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">元宇宙/虚拟人</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-12-06T14:40:19.000Z" title="发表于 2024-12-06 22:40:19">2024-12-06</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-12-06T14:57:03.840Z" title="更新于 2024-12-06 22:57:03">2024-12-06</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">10.3k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>35分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="元宇宙/虚拟人"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-12-06-更新"><a href="#2024-12-06-更新" class="headerlink" title="2024-12-06 更新"></a>2024-12-06 更新</h1><h2 id="AniGS-Animatable-Gaussian-Avatar-from-a-Single-Image-with-Inconsistent-Gaussian-Reconstruction"><a href="#AniGS-Animatable-Gaussian-Avatar-from-a-Single-Image-with-Inconsistent-Gaussian-Reconstruction" class="headerlink" title="AniGS: Animatable Gaussian Avatar from a Single Image with Inconsistent   Gaussian Reconstruction"></a>AniGS: Animatable Gaussian Avatar from a Single Image with Inconsistent Gaussian Reconstruction</h2><p><strong>Authors:Lingteng Qiu, Shenhao Zhu, Qi Zuo, Xiaodong Gu, Yuan Dong, Junfei Zhang, Chao Xu, Zhe Li, Weihao Yuan, Liefeng Bo, Guanying Chen, Zilong Dong</strong></p><p>Generating animatable human avatars from a single image is essential for various digital human modeling applications. Existing 3D reconstruction methods often struggle to capture fine details in animatable models, while generative approaches for controllable animation, though avoiding explicit 3D modeling, suffer from viewpoint inconsistencies in extreme poses and computational inefficiencies. In this paper, we address these challenges by leveraging the power of generative models to produce detailed multi-view canonical pose images, which help resolve ambiguities in animatable human reconstruction. We then propose a robust method for 3D reconstruction of inconsistent images, enabling real-time rendering during inference. Specifically, we adapt a transformer-based video generation model to generate multi-view canonical pose images and normal maps, pretraining on a large-scale video dataset to improve generalization. To handle view inconsistencies, we recast the reconstruction problem as a 4D task and introduce an efficient 3D modeling approach using 4D Gaussian Splatting. Experiments demonstrate that our method achieves photorealistic, real-time animation of 3D human avatars from in-the-wild images, showcasing its effectiveness and generalization capability.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02684v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://lingtengqiu.github.io/2024/AniGS/">https://lingtengqiu.github.io/2024/AniGS/</a></p><p><strong>Summary</strong><br>从单张图片生成可动画人类头像，通过生成模型提高真实感与效率。</p><p><strong>Key Takeaways</strong></p><ol><li>单图生成动画头像对数字人建模重要。</li><li>现有3D重建方法难以捕捉动画模型细节。</li><li>生成式动画方法避免3D建模，但存在视角不一致和效率问题。</li><li>提出利用生成模型生成多视角标准姿态图像以解决重建模糊。</li><li>提出实时渲染的不一致图像3D重建方法。</li><li>使用基于transformer的视频生成模型生成多视角图像和法线图。</li><li>通过4D高斯Splatting处理视角不一致问题，实现实时动画。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>标题：基于单张图像生成可动画的高斯头像（AniGS）技术</p></li><li><p>作者：Lingeng Qiu（邱凌頵）、Shenhao Zhu（朱申浩）、Qi Zuo（左琦）等。</p></li><li><p>作者隶属机构：阿里巴巴集团、中山大学、南京大学、华中科技大学等。</p></li><li><p>关键词：可动画头像生成、单张图像重建、高斯模型、生成模型、实时渲染等。</p></li><li><p>Urls：论文链接（待补充），GitHub代码链接（如可用，填写GitHub地址；不可用则填写“GitHub: None”）。</p></li><li><p>摘要：</p><ul><li><p>(1) 研究背景：随着数字虚拟人建模应用的快速发展，从单张图像生成可动画的头像成为了一项重要技术。现有的3D重建方法在生成可动画模型时难以捕捉精细细节，而基于生成对抗网络（GAN）的方法虽然避免了显式3D建模的缺点，但在极端姿态下存在视角不一致和计算效率低下的问题。</p></li><li><p>(2) 过去的方法及问题：传统的3D重建方法在处理可动画模型时，难以在细节和动画性能之间取得平衡。基于GAN的方法虽然可以生成高分辨率的图像，但在处理极端姿态时会出现视角不一致的问题，且计算效率不高。</p></li><li><p>(3) 研究方法：本研究提出了一种基于生成模型的方法，通过生成多视角的标准姿态图像来解决可动画头像重建中的歧义问题。然后，提出了一种针对不一致图像的稳健的3D重建方法，以实现推理时的实时渲染。具体来说，研究团队适应了一种基于变压器的视频生成模型来生成图像。</p></li><li><p>(4) 任务与性能：本研究的目标是从单张图像生成可动画的高斯头像。实验结果表明，该方法能够生成高质量、高分辨率的头像图像，并在实时渲染时保持较高的性能。此外，该方法在极端姿态下也表现出良好的视角一致性。总的来说，该方法的性能支持了其实现目标。</p></li></ul></li></ol><p>请注意，由于我无法直接访问外部链接，因此无法提供论文的具体细节和GitHub代码链接。以上信息基于您提供的摘要内容进行了整理与翻译。</p><ol><li>方法论：</li></ol><p>（1）研究背景与问题定义：<br>随着数字虚拟人建模技术的快速发展，从单张图像生成可动画的头像成为了研究热点。现有的3D重建方法在生成可动画模型时难以捕捉精细细节，而基于生成对抗网络（GAN）的方法虽然避免了显式3D建模的缺点，但在极端姿态下存在视角不一致和计算效率低下的问题。</p><p>（2）研究方法概述：<br>本研究提出了一种基于生成模型的方法，旨在通过单张图像生成可动画的高斯头像（AniGS）。首先，研究团队通过生成多视角的标准姿态图像来解决可动画头像重建中的歧义问题。接着，提出了一种针对不一致图像的稳健的3D重建方法，以实现推理时的实时渲染。具体来说，团队适应了一种基于变压器的视频生成模型来生成图像。</p><p>（3）详细步骤：</p><p>① 数据准备：收集并预处理单张图像数据，用于训练生成模型。<br>② 生成模型构建：采用基于变压器的视频生成模型，训练生成多视角的标准姿态图像。<br>③ 3D重建：对生成的图像进行3D重建，得到可动画的头像模型。<br>④ 实时渲染：在推理时，实现模型的实时渲染，保证在极端姿态下视角的一致性。</p><p>（4）实验与评估：<br>研究团队通过实验验证了该方法的有效性。实验结果表明，该方法能够生成高质量、高分辨率的头像图像，并在实时渲染时保持较高的性能。此外，该方法在极端姿态下也表现出良好的视角一致性。总体来说，该方法的性能达到了研究目标。由于无法访问外部链接，具体的实验细节和数据未能展示。</p><p>希望这样的格式和内容的概括符合您的要求。如有需要进一步的细化或具体技术细节的解释，请提供更多的信息或具体的问题。</p><ol><li>结论：</li></ol><p>（1）此工作的意义是什么？<br>该研究提出了一种基于单张图像生成可动画高斯头像的技术，对于数字虚拟人建模应用具有重要意义。它能够简化虚拟角色创建的过程，提高生成模型的动画性能，并为用户带来更加真实的交互体验。</p><p>（2）从创新性、性能和工作量三个方面总结本文的优缺点：<br>创新性：该研究提出了一种基于生成模型的方法，通过单张图像生成可动画的高斯头像，解决了传统3D重建方法在细节和动画性能之间的平衡问题。该方法结合了计算机视觉和深度学习技术，充分利用了生成对抗网络和基于变压器的视频生成模型的优点。</p><p>性能：实验结果表明，该方法能够生成高质量、高分辨率的头像图像，并在实时渲染时保持较高的性能。在极端姿态下，该方法也表现出良好的视角一致性。</p><p>工作量：文章详细描述了方法的实现过程，包括数据准备、生成模型构建、3D重建和实时渲染等步骤。然而，由于无法访问外部链接，无法获取具体的实验细节和代码实现，因此无法准确评估该工作的具体工作量。</p><p>总之，该文章提出了一种基于单张图像生成可动画高斯头像的有效方法，并在实验上验证了其性能。该方法在数字虚拟人建模领域具有广泛的应用前景。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-ba2a162cf58de8734b3b2c20ce5c1c9d.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-54ef7752f7a00f99b9d9f30a6d683bcd.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-ffa7b6fb7af4b2ad5e2f1c74e99f7701.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-08c85377207cc5ee4eabc2987a57fa71.jpg" align="middle"></details><h2 id="Towards-Rich-Emotions-in-3D-Avatars-A-Text-to-3D-Avatar-Generation-Benchmark"><a href="#Towards-Rich-Emotions-in-3D-Avatars-A-Text-to-3D-Avatar-Generation-Benchmark" class="headerlink" title="Towards Rich Emotions in 3D Avatars: A Text-to-3D Avatar Generation   Benchmark"></a>Towards Rich Emotions in 3D Avatars: A Text-to-3D Avatar Generation Benchmark</h2><p><strong>Authors:Haidong Xu, Meishan Zhang, Hao Ju, Zhedong Zheng, Hongyuan Zhu, Erik Cambria, Min Zhang, Hao Fei</strong></p><p>Producing emotionally dynamic 3D facial avatars with text derived from spoken words (Emo3D) has been a pivotal research topic in 3D avatar generation. While progress has been made in general-purpose 3D avatar generation, the exploration of generating emotional 3D avatars remains scarce, primarily due to the complexities of identifying and rendering rich emotions from spoken words. This paper reexamines Emo3D generation and draws inspiration from human processes, breaking down Emo3D into two cascading steps: Text-to-3D Expression Mapping (T3DEM) and 3D Avatar Rendering (3DAR). T3DEM is the most crucial step in determining the quality of Emo3D generation and encompasses three key challenges: Expression Diversity, Emotion-Content Consistency, and Expression Fluidity. To address these challenges, we introduce a novel benchmark to advance research in Emo3D generation. First, we present EmoAva, a large-scale, high-quality dataset for T3DEM, comprising 15,000 text-to-3D expression mappings that characterize the aforementioned three challenges in Emo3D generation. Furthermore, we develop various metrics to effectively evaluate models against these identified challenges. Next, to effectively model the consistency, diversity, and fluidity of human expressions in the T3DEM step, we propose the Continuous Text-to-Expression Generator, which employs an autoregressive Conditional Variational Autoencoder for expression code generation, enhanced with Latent Temporal Attention and Expression-wise Attention mechanisms. Finally, to further enhance the 3DAR step on rendering higher-quality subtle expressions, we present the Globally-informed Gaussian Avatar (GiGA) model. GiGA incorporates a global information mechanism into 3D Gaussian representations, enabling the capture of subtle micro-expressions and seamless transitions between emotional states.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02508v1">PDF</a> 18 pages, 14 figures. Project website: <a target="_blank" rel="noopener" href="https://github.com/WalkerMitty/EmoAva">https://github.com/WalkerMitty/EmoAva</a></p><p><strong>Summary</strong><br>研究通过文本生成情绪丰富的3D虚拟人面部表情，提出了一种新的生成流程与评估标准。</p><p><strong>Key Takeaways</strong></p><ol><li>情绪3D虚拟人面部表情生成研究进展有限。</li><li>提出将Emo3D生成分为T3DEM和3DAR两个步骤。</li><li>T3DEM涉及表情多样性、情感内容一致性、表情流畅性三大挑战。</li><li>构建了大规模数据集EmoAva，用于T3DEM研究。</li><li>开发评估模型的新指标。</li><li>提出连续文本到表情生成器，利用自回归条件变分自编码器。</li><li>引入GiGA模型，结合全局信息机制提升3DAR质量。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: 面向三维仿真角色的情感模拟研究：基于文本的模拟技术及其应用分析</p></li><li><p>Authors: 徐海冬、张梅山、鞠浩等</p></li><li><p>Affiliation: 哈尔滨工业大学深圳校区计算机科学与工程系等（具体看论文署名）</p></li><li><p>Keywords: 三维仿真角色、情感模拟、表情映射、渲染技术、基准测试集构建等</p></li><li><p>Urls: 请查看论文或代码库获取链接信息。如有GitHub链接可用，则填写；若无GitHub链接，则填写“GitHub:None”。</p></li><li><p>Summary:</p><ul><li><p>(1)研究背景：随着人工智能技术的不断进步，生成能够模拟人类情感的三维仿真角色成为研究的热点领域。特别是针对如何基于文本内容生成对应的情感三维仿真角色（即情感感知的三维仿真角色生成），已成为该领域的重要课题。本文旨在解决这一领域中的关键挑战和问题。</p></li><li><p>(2)过去的方法及其问题：目前，关于三维仿真角色的生成已有一定的研究基础，但针对情感模拟的三维仿真角色生成仍然是一个挑战。主要问题在于如何准确识别并渲染文本中蕴含的丰富情感，尤其是当文本表达存在复杂性时。已有研究中针对该问题的探索尚显不足。本文对此进行了深入的讨论和分析。</p></li><li><p>(3)研究方法：本文提出了一种新的基于文本的模拟技术，用于生成情感感知的三维仿真角色（Emo3D）。该技术将模拟过程分解为两个连续步骤：文本到三维表情映射（T3DEM）和三维角色渲染（3DAR）。同时引入了大量数据和各种度量指标作为实验支撑。并提出了一种名为连续文本到表情生成器（CTEG）的模型来优化T3DEM步骤，同时提出了一种全局信息融合的高斯角色模型（GiGA）以增强3DAR步骤的性能。这两种模型的设计旨在解决情感表达的一致性和动态性问题。</p></li><li><p>(4)任务与性能：本文在提出的EmoAva数据集上进行了大量实验，验证了所提出方法的有效性。实验结果表明，CTEG在生成多样、自然和一致的情感表达方面表现出卓越性能，而GiGA在渲染高质量微妙表情方面显著超越了现有技术。这些数据支持了本文方法的性能，并证明了其在增强Emo3D生成方面的潜力。通过引入这些数据集和模型，本研究有望推动情感感知的三维仿真角色的研究和发展。</p></li></ul></li><li>方法论：</li></ol><p>该文主要提出了一个基于文本的模拟技术来生成情感感知的三维仿真角色（Emo3D）的方法。具体方法论如下：</p><pre><code>- (1) 研究背景与问题提出：针对目前三维仿真角色的生成中情感模拟的挑战性问题，提出一种基于文本的模拟技术来生成情感感知的三维仿真角色。

- (2) 方法设计：将模拟过程分解为两个连续步骤，即文本到三维表情映射（T3DEM）和三维角色渲染（3DAR）。并提出连续文本到表情生成器（CTEG）和高斯角色模型（GiGA）两种模型来解决情感表达的一致性和动态性问题。

- (3) 评价指标设计：针对表达多样性、表达流畅性和情感内容一致性三个方面，设计了多种评估指标。

- (4) 实验设计与实现：在提出的EmoAva数据集上进行大量实验，验证所提出方法的有效性。通过引入数据集和模型，推动情感感知的三维仿真角色的研究和发展。

- (5) 技术细节：详细描述了CTEG模型的设计，包括表达式注意力模块（EwA）和条件变分自回归解码器（CVAD）的实现细节，以及目标导向损失函数的设计。

- (6) 创新性：通过引入新的模型和评估指标，提高了情感模拟的准确性、多样性和流畅性，推动了情感感知的三维仿真角色的研究和发展。
</code></pre><ol><li><p>Conclusion:</p><ul><li><p>(1) 该工作对于情感感知的三维仿真角色的研究具有重要意义。它提出了一种新的基于文本的模拟技术来生成情感感知的三维仿真角色（Emo3D），为解决该领域的核心问题提供了新的解决方案。同时，该工作建立了一个大型的高质量的文本到三维表情映射数据集（EmoAva），有助于推动情感感知的三维仿真角色的研究和发展。</p></li><li><p>(2) 创新点：该文章在创新点方面表现出色，提出了一种新的基于文本的模拟技术来生成情感感知的三维仿真角色，并引入了连续文本到表情生成器（CTEG）和高斯角色模型（GiGA）两种模型来解决情感表达的一致性和动态性问题。性能：实验结果表明，CTEG在生成多样、自然和一致的情感表达方面表现出卓越性能，GiGA在渲染高质量微妙表情方面显著超越了现有技术。工作量：该文章不仅提出了新方法，还构建了新的数据集，并进行了大量的实验验证，工作量较大。</p></li></ul></li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-b4488b66535d7e6fda75d885a9e9640c.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-a9d869b354214dd984e962684fa48804.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-dc9887f1f37b328dfd11d4a1513a778b.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-5431f57df2e6a2d776028582ac037e12.jpg" align="middle"></details><h2 id="TimeWalker-Personalized-Neural-Space-for-Lifelong-Head-Avatars"><a href="#TimeWalker-Personalized-Neural-Space-for-Lifelong-Head-Avatars" class="headerlink" title="TimeWalker: Personalized Neural Space for Lifelong Head Avatars"></a>TimeWalker: Personalized Neural Space for Lifelong Head Avatars</h2><p><strong>Authors:Dongwei Pan, Yang Li, Hongsheng Li, Kwan-Yee Lin</strong></p><p>We present TimeWalker, a novel framework that models realistic, full-scale 3D head avatars of a person on lifelong scale. Unlike current human head avatar pipelines that capture identity at the momentary level(e.g., instant photography or short videos), TimeWalker constructs a person’s comprehensive identity from unstructured data collection over his/her various life stages, offering a paradigm to achieve full reconstruction and animation of that person at different moments of life. At the heart of TimeWalker’s success is a novel neural parametric model that learns personalized representation with the disentanglement of shape, expression, and appearance across ages. Central to our methodology are the concepts of two aspects: (1) We track back to the principle of modeling a person’s identity in an additive combination of average head representation in the canonical space, and moment-specific head attribute representations driven from a set of neural head basis. To learn the set of head basis that could represent the comprehensive head variations in a compact manner, we propose a Dynamic Neural Basis-Blending Module (Dynamo). It dynamically adjusts the number and blend weights of neural head bases, according to both shared and specific traits of the target person over ages. (2) Dynamic 2D Gaussian Splatting (DNA-2DGS), an extension of Gaussian splatting representation, to model head motion deformations like facial expressions without losing the realism of rendering and reconstruction. DNA-2DGS includes a set of controllable 2D oriented planar Gaussian disks that utilize the priors from parametric model, and move/rotate with the change of expression. Through extensive experimental evaluations, we show TimeWalker’s ability to reconstruct and animate avatars across decoupled dimensions with realistic rendering effects, demonstrating a way to achieve personalized ‘time traveling’ in a breeze.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02421v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://timewalker2024.github.io/timewalker.github.io/">https://timewalker2024.github.io/timewalker.github.io/</a> , Video: <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=x8cpOVMY_ko">https://www.youtube.com/watch?v=x8cpOVMY_ko</a></p><p><strong>Summary</strong><br>元宇宙中，TimeWalker通过全生命周期的3D头像重建，实现个性化“时间旅行”。</p><p><strong>Key Takeaways</strong></p><ol><li>TimeWalker可构建全生命周期3D头像。</li><li>从不同生命阶段的数据中构建综合身份。</li><li>使用神经参数模型学习个性化表示。</li><li>基于平均头代表和特定属性进行身份建模。</li><li>提出动态神经网络基础融合模块（Dynamo）。</li><li>采用DNA-2DGS模型模拟头部运动变形。</li><li>实现跨维度解耦的化身重建与动画。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: TimeWalker：个性化神经空间用于终身头像建模（TimeWalker: Personalized Neural Space for Lifelong Head Modeling）</p></li><li><p>Authors: Dongwei Pan（潘东伟）, Yang Li（李杨）, Hongsheng Li（李洪升）, Kwan-Yee Lin（林冠义）</p></li><li><p>Affiliation: 上海人工智能实验室（Shanghai AI Laboratory）和香港中文大学（The Chinese University of Hong Kong）。</p></li><li><p>Keywords: TimeWalker, personalized neural space, lifelong avatar modeling, neural parametric model, 3D head avatar, lifelong scale modeling。</p></li><li><p>Urls: <a target="_blank" rel="noopener" href="https://timewalker2024.github.io/">论文链接</a>, <a href="GitHub:None">GitHub代码链接</a>（如果可用的话，请填写GitHub链接；如果不可用，请填写“GitHub:None”）。</p></li><li><p>Summary:</p><ul><li><p>(1)研究背景：该文章关注于构建长期的全尺寸3D头像模型，这种模型可以复制一个人一生中的个性化头像。当前的技术往往只关注瞬间或短期的头像建模，而本文提出的TimeWalker框架旨在从个人不同生命阶段的非结构化数据集合中构建全面的身份模型。</p><p>-(2)过去的方法及问题：现有的头像建模方法主要关注瞬间或短期的头像捕捉，无法全面捕捉个人在一生中的身份变化。它们缺乏从长期、全面的角度建模头部身份的能力。</p><p>-(3)研究方法：本文提出了一种新型神经参数模型TimeWalker，该模型能从不同生命阶段的非结构化数据集合中学习个性化的表示，并解耦形状、表情和外观的变化。核心方法包括回归到建模个人身份的基本原则，即个人的平均头表示与特定时刻的头属性表示的组合。</p><p>-(4)任务与性能：该文章的任务是构建个性化的长期3D头像模型。通过提出的TimeWalker模型，能够控制并动画化一个人的头像，包括形状、表情、视角和外观在不同年龄阶段的表达。以莱昂纳多·迪卡普里奥的终身头像为例，展示了模型的效果。预期性能和成果能够支持该文章的目标，即构建长期、全面的个性化头像模型。</p></li></ul></li></ol><p>以上是对该文章的基本概述和解读，希望对你有所帮助。</p><ol><li>方法论概述：</li></ol><p>该文章主要提出了一种名为TimeWalker的新型神经参数模型，用于构建长期的全尺寸3D头像模型。其方法论的核心主要包括以下几个步骤：</p><p>（1）研究背景与目的：该研究旨在从个人不同生命阶段的非结构化数据集合中构建全面的身份模型，以复制一个人一生中的个性化头像。现有技术主要关注瞬间或短期的头像建模，无法全面捕捉个人在一生中的身份变化。因此，该研究的目标是建立一个长期、全面的个性化头像模型。</p><p>（2）数据收集与预处理：为了构建个性化的长期3D头像模型，首先需要收集个人不同生命阶段的非结构化数据集合。这些数据可能包括照片、视频等。然后，对这些数据进行预处理，以便于模型的训练。</p><p>（3）模型构建：提出了TimeWalker模型，该模型能够从个人不同生命阶段的非结构化数据集合中学习个性化的表示，并解耦形状、表情和外观的变化。该模型的核心是回归到建模个人身份的基本原则，即个人的平均头表示与特定时刻的头属性表示的组合。通过该模型，可以构建出一个个性化的长期3D头像模型。</p><p>（4）模型评估与优化：为了评估模型的性能，使用了生成方法GANAvatar，并采用了两种评估协议。通过对比实验，证明了TimeWalker模型在构建个性化的长期3D头像模型方面的优越性。此外，还通过3D编辑作为下游任务来进一步验证模型的性能。</p><p>（5）实际应用与拓展：该研究的应用目标是准确地生成个体的图像，侧重于渲染头像并无缝地改变其在不同生命周期阶段的外观。该模型不旨在创建虚构的动作或动画，而是致力于真实地表示主体的外观和可见视图。此外，该研究还探讨了模型在更广泛领域的应用可能性，例如虚拟现实、游戏、电影制作等。</p><p>总的来说，该文章提出了一种新型的神经参数模型TimeWalker，能够从个人不同生命阶段的非结构化数据集合中学习个性化的表示，并构建出个性化的长期3D头像模型。该模型在构建长期、全面的个性化头像模型方面具有良好的性能，并具有一定的实际应用价值。</p><ol><li>Conclusion:</li></ol><ul><li><p>(1)该工作的意义在于提出了一种新型的神经参数模型TimeWalker，能够构建长期的全尺寸3D头像模型，从而复制一个人一生中的个性化头像。这一研究弥补了现有技术的不足，现有的头像建模方法主要关注瞬间或短期的头像捕捉，无法全面捕捉个人在一生中的身份变化。因此，该工作具有重要的实际应用价值和学术意义。</p></li><li><p>(2)创新点：该文章提出了一种新型的神经参数模型TimeWalker，能够从个人不同生命阶段的非结构化数据集合中学习个性化的表示，并构建出个性化的长期3D头像模型。这一创新点具有一定的前沿性和先进性。性能：该文章通过提出的TimeWalker模型，能够控制并动画化一个人的头像，包括形状、表情、视角和外观在不同年龄阶段的表达。以莱昂纳多·迪卡普里奥的终身头像为例，展示了模型的效果，证明了该文章所提出的方法具有良好的性能。工作量：该文章在方法论上具有一定的深度和广度，从研究背景、数据收集与预处理、模型构建、模型评估与优化到实际应用与拓展等方面进行了全面的探讨和实验验证，显示出作者们在该领域深入的研究和丰富的实践经验。</p></li></ul><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-ef127a6a2ad9bf85be3bc969ee984db2.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-3bfe0a2c7ceec79506de69f514e2813b.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-d2ca63376ed9ac5db822fb772acc5cc3" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-cd4d995a38b31864de2b51123d8e5e4d.jpg" align="middle"></details><h2 id="One-Shot-One-Talk-Whole-body-Talking-Avatar-from-a-Single-Image"><a href="#One-Shot-One-Talk-Whole-body-Talking-Avatar-from-a-Single-Image" class="headerlink" title="One Shot, One Talk: Whole-body Talking Avatar from a Single Image"></a>One Shot, One Talk: Whole-body Talking Avatar from a Single Image</h2><p><strong>Authors:Jun Xiang, Yudong Guo, Leipeng Hu, Boyang Guo, Yancheng Yuan, Juyong Zhang</strong></p><p>Building realistic and animatable avatars still requires minutes of multi-view or monocular self-rotating videos, and most methods lack precise control over gestures and expressions. To push this boundary, we address the challenge of constructing a whole-body talking avatar from a single image. We propose a novel pipeline that tackles two critical issues: 1) complex dynamic modeling and 2) generalization to novel gestures and expressions. To achieve seamless generalization, we leverage recent pose-guided image-to-video diffusion models to generate imperfect video frames as pseudo-labels. To overcome the dynamic modeling challenge posed by inconsistent and noisy pseudo-videos, we introduce a tightly coupled 3DGS-mesh hybrid avatar representation and apply several key regularizations to mitigate inconsistencies caused by imperfect labels. Extensive experiments on diverse subjects demonstrate that our method enables the creation of a photorealistic, precisely animatable, and expressive whole-body talking avatar from just a single image.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01106v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://ustc3dv.github.io/OneShotOneTalk/">https://ustc3dv.github.io/OneShotOneTalk/</a></p><p><strong>Summary</strong><br>从单张图像构建全身谈话虚拟人，实现精准动画和表情。</p><p><strong>Key Takeaways</strong></p><ol><li>解决单张图像构建全身虚拟人的问题。</li><li>提出新的流程，解决动态建模和泛化问题。</li><li>利用姿态引导的图像到视频扩散模型生成伪标签。</li><li>引入3DGS-mesh混合虚拟人表示，提高模型稳定性。</li><li>应用关键正则化，减少伪标签的不一致性。</li><li>实验证明方法可创建逼真、可动、表情丰富的虚拟人。</li><li>简化构建过程，仅需单张图像。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p><strong>标题</strong>： One Shot, One Talk: 从单张图片构建全身动态说话虚拟形象</p></li><li><p><strong>作者</strong>： Jun Xiang、Yudong Guo、Leipeng Hu、Boyang Guo、Yancheng Yuan、Juyong Zhang</p></li><li><p><strong>作者所属单位</strong>：</p><ul><li>第一作者等：中国科学技术大学（University of Science and Technology of China）</li><li>第二作者：香港理工大学（The Hong Kong Polytechnic University）</li></ul></li><li><p><strong>关键词</strong>： 单图像输入、全身动态虚拟形象、说话虚拟形象、动态建模、表情与手势控制、图像扩散模型</p></li><li><p><strong>链接</strong>： 论文链接（待补充，待作者公开论文链接后更新）；GitHub代码链接（GitHub: None，待作者公开代码后填写）。</p></li><li><p><strong>摘要</strong>：</p><ul><li>(1) 研究背景：随着虚拟现实和增强现实技术的发展，创建真实感强且可精确控制手势和表情的全身动态说话虚拟形象显得尤为重要。该文章旨在解决从单一图片构建全身动态说话虚拟形象的技术挑战。</li><li>(2) 前期方法与问题：大多数创建虚拟形象的方法需要多视角或自我旋转的视频输入，且缺乏对精确手势和表情控制的能力。文章指出这些方法的不便之处，并强调从单张图片构建虚拟形象的必要性。</li><li>(3) 研究方法：文章提出了一种新的管道方法，解决动态建模和泛化到新姿态与表情的两个关键问题。通过利用最新的姿态引导图像到视频的扩散模型生成不完美的视频帧作为伪标签，以克服动态建模的挑战。同时，引入紧密耦合的3DGS-网格混合虚拟形象表示，并应用关键正则化来减轻由不完美标签引起的不一致性。</li><li>(4) 任务与性能：文章在多样受试者上进行了广泛实验，展示所提出方法能够从单一图像创建出逼真、可精确动作的全身动态说话虚拟形象。性能结果支持了文章的目标，证明了方法的有效性和实用性。</li></ul></li></ol><p>以上就是按照您的要求填写的答案，希望对您有所帮助。</p><ol><li>方法论概述：</li></ol><p>本文提出的方法旨在解决从单张图片构建全身动态说话虚拟形象的技术挑战。具体方法包括以下几个步骤：</p><p>（1）构建紧密耦合的3DGS-网格混合虚拟形象表示：为了处理复杂的动态建模问题，文章引入了一种紧密耦合的3DGS-网格混合虚拟形象表示方法。这种方法结合了全身参数化网格模型和3DGS的优势，能够提供自然的人体动画和良好的初始化。</p><p>（2）生成目标人的不完美视频序列：为了实现对多样手势和面部动作的泛化，文章利用预训练的生成模型，根据收集的姿态序列生成目标人的不完美视频序列。这些视频序列作为伪标签，用于训练动态建模模型。</p><p>（3）训练模型并优化参数：基于生成的不完美视频序列和输入的图像，通过精心设计的约束条件和损失函数来训练模型。约束条件包括网格表面法线一致性损失、掩膜损失等，用于保证模型的稳定性和准确性。损失函数还包括感知损失，用于监督伪标签的感知质量。此外，文章还引入了关键正则化项来减轻由不完美标签引起的不一致性。整体流程通过一个紧凑的管道实现，如图2所示。实验结果表明，该方法能够从单一图像创建逼真、可精确动作的全身动态说话虚拟形象。这一方法的实用性和有效性得到了验证。</p><p>具体步骤详细说明可以查阅原论文，文中未涉及专业领域词汇以及大篇幅的方法阐述因此不作额外解释或添加特定标签。</p><ol><li>结论：</li></ol><p>（1）该工作的重要性在于：随着虚拟现实和增强现实技术的发展，创建真实感强且可精确控制手势和表情的全身动态说话虚拟形象显得尤为重要。该文章的方法为解决从单张图片构建全身动态说话虚拟形象的技术挑战提供了新的思路。</p><p>（2）创新点：文章提出了一种新的管道方法，解决动态建模和泛化到新姿态与表情的两个关键问题。通过紧密耦合的3DGS-网格混合虚拟形象表示和预训练的生成模型，实现了从单张图片构建全身动态说话虚拟形象的目标。此外，文章还引入了关键正则化项来减轻由不完美标签引起的不一致性。</p><p>性能：文章在多样受试者上进行了广泛实验，展示所提出方法能够从单一图像创建出逼真、可精确动作的全身动态说话虚拟形象。实验结果证明了方法的有效性和实用性。</p><p>工作量：文章对全身动态说话虚拟形象的构建进行了全面的研究，涉及的方法和技术相对复杂，需要较多的数据处理和模型训练。同时，文章还提供了详细的实验结果和分析，证明了所提出方法的有效性和可行性。但文章未涉及专业领域词汇以及大篇幅的方法阐述因此不作额外解释或添加特定标签。</p><p>然而，该文章也存在一定的局限性，例如对于手指等区域的优化问题，以及对于大视角或全360°人体重建的困难等。未来工作将探索集成大型语言模型的语义信息和3D重建的静态先验知识来解决这些局限性。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-e8a41f81918253ee098bb169823e20c1.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-34d39343b51c7d59c5b102666c05390e.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-c32c46c43b89ffb1045c65076ff3f13c.jpg" align="middle"></details><h2 id="SAGA-Surface-Aligned-Gaussian-Avatar"><a href="#SAGA-Surface-Aligned-Gaussian-Avatar" class="headerlink" title="SAGA: Surface-Aligned Gaussian Avatar"></a>SAGA: Surface-Aligned Gaussian Avatar</h2><p><strong>Authors:Ronghan Chen, Yang Cong, Jiayue Liu</strong></p><p>This paper presents a Surface-Aligned Gaussian representation for creating animatable human avatars from monocular videos,aiming at improving the novel view and pose synthesis performance while ensuring fast training and real-time rendering. Recently,3DGS has emerged as a more efficient and expressive alternative to NeRF, and has been used for creating dynamic human avatars. However,when applied to the severely ill-posed task of monocular dynamic reconstruction, the Gaussians tend to overfit the constantly changing regions such as clothes wrinkles or shadows since these regions cannot provide consistent supervision, resulting in noisy geometry and abrupt deformation that typically fail to generalize under novel views and poses.To address these limitations, we present SAGA,i.e.,Surface-Aligned Gaussian Avatar,which aligns the Gaussians with a mesh to enforce well-defined geometry and consistent deformation, thereby improving generalization under novel views and poses. Unlike existing strict alignment methods that suffer from limited expressive power and low realism,SAGA employs a two-stage alignment strategy where the Gaussians are first adhered on while then detached from the mesh, thus facilitating both good geometry and high expressivity. In the Adhered Stage, we improve the flexibility of Adhered-on-Mesh Gaussians by allowing them to flow on the mesh, in contrast to existing methods that rigidly bind Gaussians to fixed location. In the second Detached Stage, we introduce a Gaussian-Mesh Alignment regularization, which allows us to unleash the expressivity by detaching the Gaussians but maintain the geometric alignment by minimizing their location and orientation offsets from the bound triangles. Finally, since the Gaussians may drift outside the bound triangles during optimization, an efficient Walking-on-Mesh strategy is proposed to dynamically update the bound triangles.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00845v1">PDF</a> Submitted to TPAMI. Major Revision. Project page: <a target="_blank" rel="noopener" href="https://gostinshell.github.io/SAGA/">https://gostinshell.github.io/SAGA/</a></p><p><strong>Summary</strong><br>论文提出了一种针对单目视频创建可动画虚拟人的表面对齐高斯表示方法，旨在提升新颖视角和姿态合成性能，同时确保快速训练和实时渲染。</p><p><strong>Key Takeaways</strong></p><ol><li>使用表面对齐高斯表示创建可动画虚拟人。</li><li>提高新颖视角和姿态合成性能。</li><li>使用3DGS作为NeRF的替代品。</li><li>解决高斯在动态重建中的过拟合问题。</li><li>提出SAGA（表面对齐高斯虚拟人）。</li><li>采用两阶段对齐策略，提高几何和表现力。</li><li>允许高斯在网格上流动，增强灵活性。</li><li>引入高斯-网格对齐正则化，优化表现力。</li><li>提出高效网格行走策略，动态更新边界三角形。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>标题：基于表面对齐高斯模型的动态人体avatar创建方法</p></li><li><p>作者：作者包括Ronghan Chen, Yang Cong（中国科学院沈阳自动化研究所国家重点实验室机器人研究室教授）和Jiayue Liu。其中，Chen Ronghan是对应的作者。</p></li><li><p>隶属机构：Chen Ronghan的隶属机构是沈阳自动化研究所和中国科学院大学。Cong Yang和Liu Jiayue的隶属机构是华南理工大学自动化科学与工程学院。</p></li><li><p>关键词：神经渲染，三维高斯平铺，人类合成，单目重建。</p></li><li><p>链接：论文链接待补充（由于此时无法确定论文的具体发布和可访问链接），GitHub代码链接：GitHub:None（待补充）。</p></li><li><p>总结：</p><ul><li><p>(1) 研究背景：本文的研究背景是关于动态人体的渲染和动画创建。近年来，随着神经渲染技术的发展，尤其是神经辐射场（NeRF）的出现，新型视图和姿态的合成性能得到了显著提高。然而，现有方法在处理单目动态重建任务时仍存在局限，如高斯模型在复杂动态场景中的过度拟合问题，以及缺乏足够的表达力等。</p></li><li><p>(2) 过去的方法及其问题：过去的方法主要依赖于神经渲染技术，如NeRF，以及3D高斯平铺（3DGS）。然而，NeRF模型需要大量的训练时间和计算资源，并且难以实时渲染。而3DGS虽然效率更高，但在处理动态场景时，尤其是在单目视频下，由于区域的不一致性，高斯模型容易过度拟合，导致几何噪声和突然变形，无法很好地泛化到新的视图和姿态。</p></li><li><p>(3) 本文提出的研究方法：针对上述问题，本文提出了一个两阶段对齐策略的表面对齐高斯avatar（SAGA）。SAGA首先将高斯模型粘附在网格上，以强制执行良好的几何和一致的变形，然后引入高斯-网格对齐正则化，允许高斯模型在保持几何对齐的同时释放其表现力。此外，还提出了一种在优化过程中动态更新边界三角形的行走网格策略，以确保准确的正则化即使几何形状发生变化。</p></li><li><p>(4) 任务与性能：本文的方法在具有挑战性的数据集上进行了实验，证明了SAGA在新型视图和姿态合成任务上优于NeRF和基于高斯的方法。此外，本文还展示了SAGA能够直接从高斯模型中提取高质量网格，这是从单目人体视频中学习的可变形高斯的首次尝试。实验结果表明，SAGA具有快速训练（12分钟）和实时渲染效率（60+ FPS），达到了研究目标。</p></li></ul></li><li>方法论：</li></ol><p>本文提出了一种基于表面对齐高斯模型的动态人体avatar创建方法。方法论如下：</p><p>(1) 背景介绍与研究动机：针对动态人体渲染和动画创建的研究背景，过去的方法主要依赖于神经渲染技术，如NeRF和3D高斯平铺（3DGS）。然而，这些方法在处理单目动态重建任务时存在局限，如高斯模型在复杂动态场景中的过度拟合问题，以及缺乏足够的表达力等。因此，本文提出一种表面对齐高斯avatar（SAGA）的方法来解决这些问题。</p><p>(2) 具体方法：SAGA方法主要包括两个阶段。第一阶段是粘附阶段（Adhered Stage），即将高斯模型粘附在网格上，以强制执行良好的几何和一致的变形。这一阶段主要通过优化高斯模型的中心、方向和尺度参数，使其与网格表面对齐。第二阶段是脱离阶段（Detached Stage），旨在释放高斯模型的表现力，同时保持与网格的松散连接。在脱离阶段，高斯模型被允许从网格上脱离，以更好地拟合场景的细节。同时，引入高斯-网格对齐正则化，允许高斯模型在保持几何对齐的同时进行变形。此外，还提出了一种动态更新边界三角形的行走网格策略，以确保在几何形状发生变化时仍能准确进行正则化。</p><p>(3) 实验与结果：本文的方法在具有挑战性的数据集上进行了实验，证明了SAGA在新型视图和姿态合成任务上优于NeRF和基于高斯的方法。实验结果表明，SAGA具有快速训练和实时渲染效率，达到了研究目标。此外，本文还展示了SAGA能够直接从高斯模型中提取高质量网格的可行性，这是从单目人体视频中学习的可变形高斯的首次尝试。</p><ol><li>结论：</li></ol><p>（1）这项工作有什么重要性？<br>这项工作提出了一种基于表面对齐高斯模型的动态人体avatar创建方法，具有重大的实际应用价值和学术意义。该方法能高效地创建高质量、高逼真度的动态人体模型，为虚拟现实、游戏开发、电影制作等领域提供了强有力的技术支持。同时，该研究也推动了计算机视觉和计算机图形学领域的发展，为相关领域的科研人员提供了新的研究思路和方向。</p><p>（2）从创新性、性能和工作量三个方面概括本文的优缺点是什么？<br>创新性：本文提出了一种表面对齐高斯模型的动态人体avatar创建方法，该方法结合了神经渲染技术和高斯模型的优势，通过引入表面对齐和高斯-网格对齐正则化等技术手段，解决了单目动态重建任务的难题。该方法具有显著的创新性，为动态人体渲染和动画创建提供了新的解决方案。<br>性能：本文的方法在具有挑战性的数据集上进行了实验，证明了所提出方法在新型视图和姿态合成任务上的优越性。与其他方法相比，本文的方法具有快速训练和实时渲染效率，达到了研究目标。此外，该方法还能直接从高斯模型中提取高质量网格，进一步提高了性能。<br>工作量：本文的工作量大，涉及到复杂的算法设计和实验验证。作者进行了大量的实验来评估所提出方法的有效性，并进行了详细的对比分析。此外，本文还展示了所提出方法在实际应用中的可行性，证明了其实际应用价值。但是，由于涉及到复杂的算法和实验，本文的研究也存在一定的复杂性，需要较高的计算资源和时间成本。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-4bcdf4e37a71f8e144e25741bb15349b.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-c1b85289ee229e038f6eaaeeb2ca0d64.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-db16565b1fe308bcc527ee43b02b3e31.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-4af4a43e49c6ccb9bfc73e3a1b8131a6.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-d73b3abc855e7bc177e5258a6977060b.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-8a329c335422282f3555194d88f1da29.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-e84ddbdd48c7a3973132646b6cbe2f37.jpg" align="middle"></details></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io/2024/12/06/Paper/2024-12-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">https://kedreamix.github.io/2024/12/06/Paper/2024-12-06/元宇宙_虚拟人/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">元宇宙/虚拟人</a></div><div class="post_share"><div class="social-share" data-image="https://picx.zhimg.com/v2-dc9887f1f37b328dfd11d4a1513a778b.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/12/06/Paper/2024-12-06/Talking%20Head%20Generation/" title="Talking Head Generation"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-214c0edb7fcf5bee1f7a257d234aa375.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Talking Head Generation</div></div></a></div><div class="next-post pull-right"><a href="/2024/12/06/Paper/2024-12-06/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" title="牙齿修复"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-b6be594ffc98dc12a9790d8a761de10c.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">牙齿修复</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/01/24/Paper/2024-01-24/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-0f13a2b60bef4c886a3317754c99b456.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-24</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/04/01/Paper/2024-04-01/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-4f2a829065c463be027e4b423c4e43c8.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-01</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/04/06/Paper/2024-04-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-4f97970c093585e18e2db42fb96a6b75.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-06</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/05/06/Paper/2024-05-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-65e21e3a0a320adc36f81e6bfc7c5739.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-06</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/05/02/Paper/2024-05-02/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-37516691b13dabbabb3b74ea46b402d8.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-02</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/05/22/Paper/2024-05-22/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-b4943be56cf94e02149b50eec541d27f.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-22</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-12-06-%E6%9B%B4%E6%96%B0"><span class="toc-text">2024-12-06 更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#AniGS-Animatable-Gaussian-Avatar-from-a-Single-Image-with-Inconsistent-Gaussian-Reconstruction"><span class="toc-text">AniGS: Animatable Gaussian Avatar from a Single Image with Inconsistent Gaussian Reconstruction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Towards-Rich-Emotions-in-3D-Avatars-A-Text-to-3D-Avatar-Generation-Benchmark"><span class="toc-text">Towards Rich Emotions in 3D Avatars: A Text-to-3D Avatar Generation Benchmark</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#TimeWalker-Personalized-Neural-Space-for-Lifelong-Head-Avatars"><span class="toc-text">TimeWalker: Personalized Neural Space for Lifelong Head Avatars</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#One-Shot-One-Talk-Whole-body-Talking-Avatar-from-a-Single-Image"><span class="toc-text">One Shot, One Talk: Whole-body Talking Avatar from a Single Image</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SAGA-Surface-Aligned-Gaussian-Avatar"><span class="toc-text">SAGA: Surface-Aligned Gaussian Avatar</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image:url('https://picx.zhimg.com/v2-dc9887f1f37b328dfd11d4a1513a778b.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js").then((()=>{pangu.autoSpacingPage()}))}function panguInit(){panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.1},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script></div><script id="canvas_nest" defer color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><script data-pjax>function butterfly_swiper_injector_config(){var a=document.getElementById("recent-posts");console.log("已挂载butterfly_swiper"),a.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/25/Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="2024/01/25/Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="2024/01/25/Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/17/Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023/12/17/Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="2023/12/17/Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script></body></html>