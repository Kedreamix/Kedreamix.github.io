<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>3DGS | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="3DGS 方向最新论文已更新，请持续关注 Update in 2024-12-07  Turbo3D Ultra-fast Text-to-3D Generation"><meta property="og:type" content="article"><meta property="og:title" content="3DGS"><meta property="og:url" content="https://kedreamix.github.io/2024/12/07/Paper/2024-12-07/3DGS/index.html"><meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World"><meta property="og:description" content="3DGS 方向最新论文已更新，请持续关注 Update in 2024-12-07  Turbo3D Ultra-fast Text-to-3D Generation"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://pica.zhimg.com/v2-f99336e9eaadb0ddb4c3dfffa1d84b60.jpg"><meta property="article:published_time" content="2024-12-07T06:10:54.000Z"><meta property="article:modified_time" content="2024-12-07T06:10:54.106Z"><meta property="article:author" content="Kedreamix"><meta property="article:tag" content="3DGS"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://pica.zhimg.com/v2-f99336e9eaadb0ddb4c3dfffa1d84b60.jpg"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/2024/12/07/Paper/2024-12-07/3DGS/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-74LZ5BEQQ1")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!0,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"简"},noticeOutdate:{limitDay:500,position:"top",messagePrev:"It has been",messageNext:"days since the last update, the content of the article may be outdated."},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:200},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!0,post:!0},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!0,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"3DGS",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-12-07 14:10:54"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach((e=>{n.setAttribute(e,t[e])})),document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)})),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=24?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 7.0.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml"></head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{const d=document.getElementById("loading-box"),e=document.body,o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l(),window.addEventListener("load",(()=>{o()}))})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">304</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">17</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url('https://pica.zhimg.com/v2-f99336e9eaadb0ddb4c3dfffa1d84b60.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">3DGS</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-12-07T06:10:54.000Z" title="发表于 2024-12-07 14:10:54">2024-12-07</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-12-07T06:10:54.106Z" title="更新于 2024-12-07 14:10:54">2024-12-07</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">22.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>76分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="3DGS"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-12-07-更新"><a href="#2024-12-07-更新" class="headerlink" title="2024-12-07 更新"></a>2024-12-07 更新</h1><h2 id="Turbo3D-Ultra-fast-Text-to-3D-Generation"><a href="#Turbo3D-Ultra-fast-Text-to-3D-Generation" class="headerlink" title="Turbo3D: Ultra-fast Text-to-3D Generation"></a>Turbo3D: Ultra-fast Text-to-3D Generation</h2><p><strong>Authors:Hanzhe Hu, Tianwei Yin, Fujun Luan, Yiwei Hu, Hao Tan, Zexiang Xu, Sai Bi, Shubham Tulsiani, Kai Zhang</strong></p><p>We present Turbo3D, an ultra-fast text-to-3D system capable of generating high-quality Gaussian splatting assets in under one second. Turbo3D employs a rapid 4-step, 4-view diffusion generator and an efficient feed-forward Gaussian reconstructor, both operating in latent space. The 4-step, 4-view generator is a student model distilled through a novel Dual-Teacher approach, which encourages the student to learn view consistency from a multi-view teacher and photo-realism from a single-view teacher. By shifting the Gaussian reconstructor’s inputs from pixel space to latent space, we eliminate the extra image decoding time and halve the transformer sequence length for maximum efficiency. Our method demonstrates superior 3D generation results compared to previous baselines, while operating in a fraction of their runtime.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04470v1">PDF</a> project page: <a target="_blank" rel="noopener" href="https://turbo-3d.github.io/">https://turbo-3d.github.io/</a></p><p><strong>Summary</strong><br>我们提出Turbo3D，一个能在1秒内生成高质量高斯分层资产的快速文本到3D系统。</p><p><strong>Key Takeaways</strong></p><ol><li>Turbo3D可在1秒内生成高质量的3D资产。</li><li>使用4步、4视图的扩散生成器和高效的前馈高斯重建器。</li><li>采用双重教师方法进行学生模型训练。</li><li>从多视图教师学习视角一致性，从单视图教师学习逼真度。</li><li>将高斯重建器输入从像素空间转换为潜在空间。</li><li>消除额外图像解码时间，缩短变换器序列长度。</li><li>性能优于现有基准，运行时间更短。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: Turbo3D：超快速文本到三维生成技术</p></li><li><p>Authors: 待查阅相关文献后补充</p></li><li><p>Affiliation: 待查阅相关文献后补充</p></li><li><p>Keywords: text-to-3D generation；Turbo3D；multi-step multi-view generation；high-resolution 3D generation；Ablation experiments；user study；A100 GPU</p></li><li><p>Urls: xxx（请自行查找该论文的链接及Github代码链接）</p></li><li><p>Summary:</p></li></ol><p>(1) 研究背景：本文研究了文本到三维生成技术，旨在解决现有方法生成速度慢、分辨率低、效果差等问题。</p><p>(2) 过去的方法及问题：当前文本到三维生成技术存在生成速度慢、分辨率不高、细节缺失等问题。一些方法虽然能够生成高质量的三维模型，但计算量大、耗时较长，难以满足实时应用需求。</p><p>(3) 研究方法：本文提出了一种基于深度学习的高效文本到三维生成方法Turbo3D。该方法通过对已有模型进行微调，实现了快速高效的文本到三维生成。具体地，通过采用一种四步四视图的扩散生成器和一个高效的前馈高斯重建器，以及操作在潜在空间中的策略，大大提高了生成速度和图像质量。此外，还提出了一种Dual-Teacher蒸馏方法，用于训练学生模型，提高其视图一致性和逼真度。</p><p>(4) 任务与性能：本文方法在文本到三维生成任务上取得了显著成果，与现有方法相比，具有更高的生成速度、更高的分辨率和更好的图像质量。实验结果表明，该方法能够支持高效、高质量的文本到三维生成。</p><ol><li>Methods:</li></ol><p>(1) 研究背景与动机：本文研究的主题是文本到三维生成技术，旨在解决现有技术存在生成速度慢、分辨率低、效果不理想等问题，提高该技术在实时应用中的实用性。</p><p>(2) 数据收集与预处理：此部分的内容尚未在摘要中提及，需要查阅原文详细了解。可能包括收集大量的文本和对应的三维模型数据，进行数据清洗、标注等工作。</p><p>(3) 方法介绍：本文提出了一种基于深度学习的高效文本到三维生成方法Turbo3D。该方法主要包括以下几个步骤：</p><ul><li>采用四步四视图的扩散生成器，通过扩散过程生成三维模型的各个视图。</li><li>使用高效的前馈高斯重建器，对生成的视图进行重建，提高图像质量。</li><li>在潜在空间中进行操作，通过微调已有模型，实现快速高效的文本到三维生成。</li><li>提出Dual-Teacher蒸馏方法，训练学生模型，提高其视图一致性和逼真度。</li></ul><p>(4) 实验设计与实施：本文进行了大量的实验来验证所提出方法的性能。可能包括与其他方法的对比实验、消融实验以及用户研究等。实验结果表明，该方法在文本到三维生成任务上取得了显著成果，具有更高的生成速度、更高的分辨率和更好的图像质量。</p><p>(5) 评估与分析：通过对比实验、定量和定性分析等方法，对所提出方法的性能进行了全面评估。证明了该方法在文本到三维生成任务上的优越性。</p><p>注：具体细节需要查阅原文进行确认，上述回答仅为根据摘要内容进行的推测。</p><ol><li><p>Conclusion:</p><ul><li><p>(1) 这项工作的意义在于提出了一种名为Turbo3D的高效文本到三维生成技术，该技术对于解决现有方法生成速度慢、分辨率低、效果差等问题具有重要意义，有望推动文本到三维生成技术的实时应用。</p></li><li><p>(2) Innovation point（创新点）：本文提出了基于深度学习的高效文本到三维生成方法Turbo3D，通过采用四步四视图的扩散生成器、高效的前馈高斯重建器以及在潜在空间中的操作策略，大大提高了生成速度和图像质量。此外，还创新性地提出了Dual-Teacher蒸馏方法，用于提高学生模型的视图一致性和逼真度。<br>Performance（性能）：与现有方法相比，Turbo3D在文本到三维生成任务上取得了显著成果，具有更高的生成速度、更高的分辨率和更好的图像质量。实验结果表明，该方法支持高效、高质量的文本到三维生成。<br>Workload（工作量）：虽然本文的工作取得了显著的成果，但关于数据收集与预处理的细节尚未在摘要中提及，这部分工作可能较为繁重。此外，文章还可能需要更多的实验来验证所提出方法的泛化性能和鲁棒性。</p></li></ul></li></ol><p>注意：以上结论仅根据摘要内容进行了推测，具体细节需要查阅原文进行确认。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-7d8fa913396b5df2698ef949f9fa46ad.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-cda2cf1453999ee8905dd85add59ab2e.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-0ae944cf0c716d06f5a5e2c743b525cc.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-78e5444d6c76898037a40bed8d46a931.jpg" align="middle"></details><h2 id="QUEEN-QUantized-Efficient-ENcoding-of-Dynamic-Gaussians-for-Streaming-Free-viewpoint-Videos"><a href="#QUEEN-QUantized-Efficient-ENcoding-of-Dynamic-Gaussians-for-Streaming-Free-viewpoint-Videos" class="headerlink" title="QUEEN: QUantized Efficient ENcoding of Dynamic Gaussians for Streaming   Free-viewpoint Videos"></a>QUEEN: QUantized Efficient ENcoding of Dynamic Gaussians for Streaming Free-viewpoint Videos</h2><p><strong>Authors:Sharath Girish, Tianye Li, Amrita Mazumdar, Abhinav Shrivastava, David Luebke, Shalini De Mello</strong></p><p>Online free-viewpoint video (FVV) streaming is a challenging problem, which is relatively under-explored. It requires incremental on-the-fly updates to a volumetric representation, fast training and rendering to satisfy real-time constraints and a small memory footprint for efficient transmission. If achieved, it can enhance user experience by enabling novel applications, e.g., 3D video conferencing and live volumetric video broadcast, among others. In this work, we propose a novel framework for QUantized and Efficient ENcoding (QUEEN) for streaming FVV using 3D Gaussian Splatting (3D-GS). QUEEN directly learns Gaussian attribute residuals between consecutive frames at each time-step without imposing any structural constraints on them, allowing for high quality reconstruction and generalizability. To efficiently store the residuals, we further propose a quantization-sparsity framework, which contains a learned latent-decoder for effectively quantizing attribute residuals other than Gaussian positions and a learned gating module to sparsify position residuals. We propose to use the Gaussian viewspace gradient difference vector as a signal to separate the static and dynamic content of the scene. It acts as a guide for effective sparsity learning and speeds up training. On diverse FVV benchmarks, QUEEN outperforms the state-of-the-art online FVV methods on all metrics. Notably, for several highly dynamic scenes, it reduces the model size to just 0.7 MB per frame while training in under 5 sec and rendering at 350 FPS. Project website is at <a target="_blank" rel="noopener" href="https://research.nvidia.com/labs/amri/projects/queen">https://research.nvidia.com/labs/amri/projects/queen</a></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04469v1">PDF</a> Accepted at NeurIPS 2024, Project website: <a target="_blank" rel="noopener" href="https://research.nvidia.com/labs/amri/projects/queen">https://research.nvidia.com/labs/amri/projects/queen</a></p><p><strong>Summary</strong><br>提出基于3D高斯分层（3D-GS）的QUEN框架，实现高效在线自由视点视频流。</p><p><strong>Key Takeaways</strong></p><ol><li>在线自由视点视频流（FVV）面临挑战，3D-GS编码潜力巨大。</li><li>QUEEN框架学习帧间高斯属性残差，无需结构约束。</li><li>量化稀疏框架有效存储残差，包括隐式解码器和门控模块。</li><li>使用高斯视场梯度差向量分离静态和动态内容。</li><li>高效学习稀疏性，加速训练。</li><li>在FVV基准上优于现有方法。</li><li>高动态场景模型仅0.7MB/帧，训练5秒内完成，渲染350FPS。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>标题：基于三维高斯贴图的动态场景在线自由视角视频量化高效编码研究（QUEEN: QUantized Efficient ENcoding of Dynamic）</p></li><li><p>作者：Sharath Girish，Amrita Mazumdar，Tianye Li等</p></li><li><p>作者单位：Girish来自于马里兰大学；Li和Mazumdar以及Shrivastava来自于NVIDIA；Luebke和De Mello同样来自NVIDIA。</p></li><li><p>关键词：在线自由视角视频、量化编码、高效编码、三维高斯贴图、动态场景处理、深度学习模型。</p></li><li><p>Urls：论文链接（待补充）；GitHub代码链接（待补充，如果没有可用信息可标记为“GitHub:None”）</p></li><li><p>总结：</p><ul><li><p>(1) 研究背景：在线自由视角视频（FVV）的流式传输是一个挑战性的问题，需要实现场景的动态重建并高效传输。然而，当前的研究方法在这方面仍有许多不足，本文旨在提出一种高效的解决方案。</p></li><li><p>(2) 过去的方法及其问题：目前的方法在在线FVV的流式传输方面存在性能不足的问题，尤其是在增量更新、实时约束满足和小内存占用方面。此外，它们通常无法有效地处理高度动态的场景。因此，有必要开发一种新的方法来解决这些问题。</p></li><li><p>(3) 研究方法：本文提出了一种基于三维高斯贴图（3D-GS）的量化高效编码框架（QUEEN）。该框架直接学习连续帧之间的高斯属性残差，并使用量化稀疏框架来有效存储这些残差。此外，还利用高斯视空间梯度差分向量来分离场景的静态和动态内容，从而加速训练过程。总体而言，这种方法实现了高质量的场景重建和泛化能力。</p></li><li><p>(4) 任务与性能：本文的方法在多种在线自由视角视频基准测试上表现出色，超过了现有方法的所有指标。特别是在处理高度动态场景时，模型大小被减少到每帧仅0.7 MB，训练时间不到5秒，渲染速度约为350 FPS。这些性能表明该方法在在线自由视角视频的流式传输方面具有巨大的潜力。实验结果支持了本文方法的有效性和优越性。</p></li></ul></li><li><p>方法：</p><ul><li><p>(1) 研究背景分析：针对在线自由视角视频（FVV）的流式传输问题，分析当前方法的不足，确定研究背景及必要性。</p></li><li><p>(2) 方法提出：提出了一种基于三维高斯贴图（3D-GS）的量化高效编码框架（QUEEN）。该框架通过直接学习连续帧之间的高斯属性残差，使用量化稀疏框架有效存储这些残差。同时，利用高斯视空间梯度差分向量来分离场景的静态和动态内容，从而优化训练过程。</p></li><li><p>(3) 实验设计与实施：在多种在线自由视角视频基准测试上进行实验，验证所提出方法的有效性。通过详细的数据分析和对比，证明该方法在性能上超越现有方法，特别是在处理高度动态场景时表现出色。</p></li><li><p>(4) 结果评估：实验结果支持所提出方法的有效性和优越性，模型大小被减少到每帧仅0.7 MB，训练时间不到5秒，渲染速度约为350 FPS。这些性能结果表明该方法在在线自由视角视频的流式传输方面具有巨大的潜力。</p></li></ul></li><li>Conclusion:</li></ol><ul><li>(1)该工作的意义在于提出了一种基于三维高斯贴图的量化高效编码框架（QUEEN），解决了在线自由视角视频（FVV）的流式传输问题。该方法能够实现场景的动态重建并高效传输，对于推动在线自由视角视频技术的发展具有重要意义。</li><li>(2)创新点：该文章提出了基于三维高斯贴图的量化高效编码框架，通过直接学习连续帧之间的高斯属性残差，使用量化稀疏框架有效存储这些残差，并利用高斯视空间梯度差分向量分离场景的静态和动态内容。其创新点在于结合了三维高斯贴图技术与量化编码，实现了高效的视频编码。</li><li>性能：该文章的方法在多种在线自由视角视频基准测试上表现出色，超过了现有方法的所有指标。特别是在处理高度动态场景时，模型大小被减少到每帧仅0.7 MB，训练时间不到5秒，渲染速度约为350 FPS。这些性能结果表明该方法在实际应用中具有巨大的潜力。</li><li>工作量：文章进行了详细的实验设计与实施，通过大量的实验验证了所提出方法的有效性。同时，文章对实验结果进行了详细的分析和讨论，证明了该方法的有效性和优越性。</li></ul><p>综上所述，该文章提出了一种基于三维高斯贴图的量化高效编码框架，实现了在线自由视角视频的高效编码和传输，具有重要的实际意义和创新性。同时，该方法在性能上表现出色，具有广泛的应用前景。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-11e4171fbf03b972c288ce8cbc0bbbb2.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-0b61cd2fb60595fd8ac83e423a6fff88.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-72b89ab3ff122acd8d808b6018a3218f.jpg" align="middle"></details><h2 id="Sparse-Voxels-Rasterization-Real-time-High-fidelity-Radiance-Field-Rendering"><a href="#Sparse-Voxels-Rasterization-Real-time-High-fidelity-Radiance-Field-Rendering" class="headerlink" title="Sparse Voxels Rasterization: Real-time High-fidelity Radiance Field   Rendering"></a>Sparse Voxels Rasterization: Real-time High-fidelity Radiance Field Rendering</h2><p><strong>Authors:Cheng Sun, Jaesung Choe, Charles Loop, Wei-Chiu Ma, Yu-Chiang Frank Wang</strong></p><p>We propose an efficient radiance field rendering algorithm that incorporates a rasterization process on sparse voxels without neural networks or 3D Gaussians. There are two key contributions coupled with the proposed system. The first is to render sparse voxels in the correct depth order along pixel rays by using dynamic Morton ordering. This avoids the well-known popping artifact found in Gaussian splatting. Second, we adaptively fit sparse voxels to different levels of detail within scenes, faithfully reproducing scene details while achieving high rendering frame rates. Our method improves the previous neural-free voxel grid representation by over 4db PSNR and more than 10x rendering FPS speedup, achieving state-of-the-art comparable novel-view synthesis results. Additionally, our neural-free sparse voxels are seamlessly compatible with grid-based 3D processing algorithms. We achieve promising mesh reconstruction accuracy by integrating TSDF-Fusion and Marching Cubes into our sparse grid system.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04459v1">PDF</a> Code release in progress</p><p><strong>Summary</strong><br>提出无需神经网络或3D高斯函数的稀疏体素渲染算法，提升渲染效率和效果。</p><p><strong>Key Takeaways</strong></p><ol><li>使用动态Morton排序渲染稀疏体素，避免高斯散布的抖动效果。</li><li>适应场景细节级别，高帧率渲染。</li><li>比神经网络体素网格提升4db PSNR和10倍渲染速度。</li><li>与基于网格的3D处理算法兼容。</li><li>通过TSDF-Fusion和Marching Cubes实现网格系统中的网格重建。</li><li>改善了稀疏体素的表现。</li><li>达到先进的视图合成效果。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p><strong>标题</strong>: 高效无神经网络稀疏体素渲染算法研究</p></li><li><p><strong>作者</strong>: 作者名需要查看原文提供的信息。</p></li><li><p><strong>隶属机构</strong>: 由于原文未提供第一作者隶属机构信息，故此项无法回答。</p></li><li><p><strong>关键词</strong>: 稀疏体素渲染、无神经网络、动态Morton排序、场景细节、渲染性能优化、新型视图合成。</p></li><li><p><strong>链接</strong>: 由于原文未提供GitHub代码链接，故此项填“GitHub: None”。</p></li><li><p><strong>摘要</strong>:</p><ul><li><p>(1) 研究背景：本文主要研究了计算机图形学中的稀疏体素渲染技术，针对现有技术存在的问题，提出了一种高效的无神经网络稀疏体素渲染算法。</p></li><li><p>(2) 现有方法及其问题：过去的方法在稀疏体素渲染中常常使用神经网络或3D高斯函数，但存在计算量大、渲染速度慢、细节表现不足等问题。本文提出的算法旨在解决这些问题。</p></li><li><p>(3) 研究方法：本文提出的算法结合了稀疏体素栅格化和动态Morton排序技术，通过动态Morton排序对稀疏体素进行正确的深度排序，避免了高斯喷溅中的弹跳伪影。同时，算法还实现了自适应场景细节层次的体素拟合，提高了渲染帧率和场景细节表现。</p></li><li><p>(4) 任务与性能：本文的方法在新型视图合成任务上取得了显著成果，相较于之前的无神经网络体素网格表示方法，提高了4db PSNR和超过10倍的渲染FPS速度。此外，该方法的稀疏体素能够无缝兼容基于网格的3D处理算法，实现了有前景的网格重建精度。</p></li></ul></li></ol><p>希望以上总结符合您的要求。如果有任何需要修改或补充的地方，请告诉我。</p><ol><li><p>方法论：</p><ul><li><p>(1) 研究背景：本文主要研究了计算机图形学中的稀疏体素渲染技术，针对现有技术存在的问题，提出了一种高效的无神经网络稀疏体素渲染算法。</p></li><li><p>(2) 方法概述：本文提出的算法结合了稀疏体素栅格化和动态Morton排序技术。首先，使用稀疏体素场景表示和渲染算法，然后引入了一种渐进场景优化策略，旨在从多视角图像中重建场景。</p></li><li><p>(3) 场景表示：采用稀疏体素表示三维场景，根据Octree空间分区规则分配体素。通过计算每个体素的alpha值和视差相关的颜色，实现场景的场景表示。其中，alpha值用于表示体素对像素射线的贡献，而颜色则用于表示体素的视差效果。</p></li><li><p>(4) 栅格化过程：在渲染过程中，算法将体素投影到图像空间，并按照正确的顺序进行渲染。这一过程中采用了动态Morton排序技术，避免了高斯喷溅中的弹跳伪影。同时，算法还实现了自适应场景细节层次的体素拟合，提高了渲染帧率和场景细节表现。</p></li><li><p>(5) 实验结果：本文的方法在新型视图合成任务上取得了显著成果，相较于之前的无神经网络体素网格表示方法，提高了4db PSNR和超过10倍的渲染FPS速度。此外，该方法的稀疏体素能够无缝兼容基于网格的3D处理算法，实现了有前景的网格重建精度。</p></li></ul></li><li><p>Conclusion:</p><ul><li><p>(1) 这项工作的意义在于提出了一种高效的无神经网络稀疏体素渲染算法，能够改善计算机图形学中的稀疏体素渲染技术，对于视图合成和场景重建等领域具有重要的应用价值。</p></li><li><p>(2) 创新点：该文章提出了一种新型的无神经网络稀疏体素渲染算法，结合了稀疏体素栅格化和动态Morton排序技术，实现了高效渲染和场景细节的优化。性能：在新型视图合成任务上，该方法相较于之前的无神经网络体素网格表示方法，提高了渲染帧率和图像质量。工作量：文章详细描述了算法的设计和实现过程，并通过实验验证了算法的有效性。同时，该方法的稀疏体素能够无缝兼容基于网格的3D处理算法，为网格重建提供了新思路。</p></li></ul></li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-f3c198636e7e6f01bef0ce6e0c639bc1.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-1e67387a52acc90720cab8db9dcbb9e4.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-f5ffe7a0d584080719e00c16399fc129.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-aaed7d1f8b988755acf557e77e9591bf.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-e8f6fe3d924c1c5facc15ecca5be6f7b.jpg" align="middle"></details><h2 id="Monocular-Dynamic-Gaussian-Splatting-is-Fast-and-Brittle-but-Smooth-Motion-Helps"><a href="#Monocular-Dynamic-Gaussian-Splatting-is-Fast-and-Brittle-but-Smooth-Motion-Helps" class="headerlink" title="Monocular Dynamic Gaussian Splatting is Fast and Brittle but Smooth   Motion Helps"></a>Monocular Dynamic Gaussian Splatting is Fast and Brittle but Smooth Motion Helps</h2><p><strong>Authors:Yiqing Liang, Mikhail Okunev, Mikaela Angelina Uy, Runfeng Li, Leonidas Guibas, James Tompkin, Adam W. Harley</strong></p><p>Gaussian splatting methods are emerging as a popular approach for converting multi-view image data into scene representations that allow view synthesis. In particular, there is interest in enabling view synthesis for dynamic scenes using only monocular input data — an ill-posed and challenging problem. The fast pace of work in this area has produced multiple simultaneous papers that claim to work best, which cannot all be true. In this work, we organize, benchmark, and analyze many Gaussian-splatting-based methods, providing apples-to-apples comparisons that prior works have lacked. We use multiple existing datasets and a new instructive synthetic dataset designed to isolate factors that affect reconstruction quality. We systematically categorize Gaussian splatting methods into specific motion representation types and quantify how their differences impact performance. Empirically, we find that their rank order is well-defined in synthetic data, but the complexity of real-world data currently overwhelms the differences. Furthermore, the fast rendering speed of all Gaussian-based methods comes at the cost of brittleness in optimization. We summarize our experiments into a list of findings that can help to further progress in this lively problem setting. Project Webpage: <a target="_blank" rel="noopener" href="https://lynl7130.github.io/MonoDyGauBench.github.io/">https://lynl7130.github.io/MonoDyGauBench.github.io/</a></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04457v1">PDF</a> 37 pages, 39 figures, 9 tables</p><p><strong>Summary</strong><br>利用高斯分层方法进行多视角图像数据转换，以实现动态场景的单目输入数据视图合成，并对比分析了多种方法。</p><p><strong>Key Takeaways</strong></p><ol><li>高斯分层方法在多视角图像数据转换为场景表示方面流行。</li><li>单目数据动态场景视图合成是一项挑战。</li><li>多种方法同时提出，但效果各异。</li><li>对高斯分层方法进行组织和基准测试。</li><li>使用现有数据集和合成数据集分析。</li><li>系统分类方法并量化其对性能的影响。</li><li>实验结果在合成数据中定义明确，但在真实数据中差异被复杂性淹没。</li><li>高斯方法的快速渲染速度以优化脆弱性为代价。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p><strong>标题</strong>： 单目动态高斯涂抹法<br>中文翻译：单目动态高斯渲染法研究</p></li><li><p><strong>作者</strong>： Yiqing Liang（梁一琦）, Mikhail Okunev（米哈伊尔·奥库涅夫）, Mikaela Angelina Uy（乌伊·米凯拉·安吉莉娜）, Runfeng Li（李润峰）, Leonidas Guibas（列奥尼达斯·吉巴斯）, James Tompkin（詹姆斯·汤姆金）, Adam W. Harley（亚当·W·哈雷）。</p></li><li><p><strong>作者所属机构</strong>： 梁一琦、米哈伊尔·奥库涅夫、李润峰为布朗大学；列奥尼达斯·吉巴斯和亚当·W·哈雷为斯坦福大学；乌伊·米凯拉·安吉莉娜为NVIDIA公司。其中，“Affiliation: 布朗大学；斯坦福大学；NVIDIA公司”表示这些作者分别来自这三个机构。</p></li><li><p><strong>关键词</strong>： Gaussian Splatting（高斯涂抹法）、Dynamic View Synthesis（动态视图合成）、Monocular Camera Data（单目相机数据）、Scene Representation（场景表示）。</p></li><li><p><strong>链接</strong>： 论文链接待补充；GitHub代码链接待补充（如果可用）。如果不可用，填写为“GitHub:None”。</p></li><li><p><strong>摘要</strong>：</p><ul><li><p>(1)研究背景：本文研究了基于单目相机数据的动态场景视图合成问题。由于单目输入数据的动态场景重建是一个病态问题，因此需要使用高斯涂抹法等方法进行多视角图像数据转换为场景表示。</p></li><li><p>(2)过去的方法及问题：过去的研究中出现了许多基于高斯涂抹法的方法，并应用于动态视图合成。但由于缺乏统一的评估基准和混乱的数据集分割，使得不同方法之间的公平比较变得困难。</p></li><li><p>(3)研究方法：本文组织、评估和分析了许多基于高斯涂抹法的方法，提供了以前工作所缺乏的“苹果对苹果”的比较。通过创建包含受控相机和场景运动的指导性合成数据集，本文揭示了影响性能的关键因素。同时，系统地分类了高斯涂抹法的运动表示类型，并量化了它们之间的差异如何影响性能。</p></li><li><p>(4)任务与性能：本文方法在动态视图合成的任务上取得了一定的性能，通过对现有数据集和指导性合成数据集的结果分析证明了方法的有效性。然而，由于现实世界的复杂性，目前仍难以充分展现方法的全部性能优势。尽管如此，本文的贡献在于通过一系列实验发现总结了有助于进一步推进这一活跃领域的关键信息。同时公开了代码和指导性合成数据集，为后续的进一步研究提供了基础。性能结果在一定程度上支持了其方法的实用性，但由于现实世界场景的复杂性，还需进一步的改进和优化才能达到更理想的效果。</p></li></ul></li></ol><p>以上就是该论文的概括，希望对您有所帮助！</p><ol><li>Methods:</li></ol><ul><li>(1) 研究背景与问题定义：文章主要研究了基于单目相机数据的动态场景视图合成问题。由于单目输入数据的动态场景重建是一个病态问题，因此需要寻找有效的解决方法。</li><li>(2) 方法综述：文章综述了现有的基于高斯涂抹法的方法，并分析了其优缺点，缺乏统一的评估基准和混乱的数据集分割使得公平比较变得困难。</li><li>(3) 指导性合成数据集的创建：为了进行公平的比较和分析，文章创建了一个包含受控相机和场景运动的指导性合成数据集。</li><li>(4) 高斯涂抹法的运动表示类型分类：文章系统地分类了高斯涂抹法的运动表示类型，并探讨了它们之间的差异如何影响性能。</li><li>(5) 实验与性能评估：文章通过一系列实验对动态视图合成的性能进行了评估，并通过分析现有数据集和指导性合成数据集的结果证明了方法的有效性。同时公开了代码和指导性合成数据集，为后续的进一步研究提供了基础。</li></ul><p>以上内容仅供参考，建议查阅该论文以获取更准确全面的信息。</p><ol><li>Conclusion:</li></ol><ul><li>(1)工作意义：该研究对于动态场景视图合成的领域具有重要的推动作用，特别是基于单目相机数据的处理方法。它有助于解决现实世界中动态场景的重建问题，对于计算机视觉、图形学以及虚拟现实等领域的发展都具有重要的意义。</li><li>(2)创新点、性能、工作量总结：<ul><li>创新点：文章创建了指导性合成数据集，为动态视图合成的方法提供了一个统一的评估基准。此外，文章系统地分类了高斯涂抹法的运动表示类型，并分析了它们对性能的影响，这是该领域的一个新的尝试和探索。</li><li>性能：文章的方法在动态视图合成的任务上取得了一定的性能提升，并通过实验证明了其有效性。然而，由于现实世界的复杂性，方法的性能还需要进一步的优化和提升。</li><li>工作量：文章进行了大量的实验和数据分析，对现有的高斯涂抹法进行了全面的综述和分析。同时，文章的代码和指导性合成数据集的公开为后续的进一步研究提供了基础，展现了作者的工作量和贡献。</li></ul></li></ul><p>以上就是对该论文的总结，希望对您有所帮助。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-1b1b7418ba4499f889e25e84c367bb83.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-d20a7218959b7bed6768f78876bd953d.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-1612ef329282f6e3b3ec7fff545223be.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-9183177eebd6dddf58dcda78c9a6835b.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-546eb6e30b278abf55c93dc350fed997.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-8f99ceb89f6f9005e23760069c489762.jpg" align="middle"></details><h2 id="PBDyG-Position-Based-Dynamic-Gaussians-for-Motion-Aware-Clothed-Human-Avatars"><a href="#PBDyG-Position-Based-Dynamic-Gaussians-for-Motion-Aware-Clothed-Human-Avatars" class="headerlink" title="PBDyG: Position Based Dynamic Gaussians for Motion-Aware Clothed Human   Avatars"></a>PBDyG: Position Based Dynamic Gaussians for Motion-Aware Clothed Human Avatars</h2><p><strong>Authors:Shota Sasaki, Jane Wu, Ko Nishino</strong></p><p>This paper introduces a novel clothed human model that can be learned from multiview RGB videos, with a particular emphasis on recovering physically accurate body and cloth movements. Our method, Position Based Dynamic Gaussians (PBDyG), realizes <code>movement-dependent'' cloth deformation via physical simulation, rather than merely relying on</code>pose-dependent’’ rigid transformations. We model the clothed human holistically but with two distinct physical entities in contact: clothing modeled as 3D Gaussians, which are attached to a skinned SMPL body that follows the movement of the person in the input videos. The articulation of the SMPL body also drives physically-based simulation of the clothes’ Gaussians to transform the avatar to novel poses. In order to run position based dynamics simulation, physical properties including mass and material stiffness are estimated from the RGB videos through Dynamic 3D Gaussian Splatting. Experiments demonstrate that our method not only accurately reproduces appearance but also enables the reconstruction of avatars wearing highly deformable garments, such as skirts or coats, which have been challenging to reconstruct using existing methods.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04433v1">PDF</a></p><p><strong>Summary</strong><br>本文提出一种基于多视角RGB视频学习的人体模型，通过物理模拟实现准确的衣物和身体运动。</p><p><strong>Key Takeaways</strong></p><ol><li>使用多视角RGB视频学习衣物人体模型。</li><li>PBDyG方法通过物理模拟实现运动依赖的衣物变形。</li><li>衣物作为3D高斯模型，附着在SMPL身体上。</li><li>SMPL身体驱动衣物高斯模型模拟，变换到新姿势。</li><li>利用RGB视频估计物理属性进行位置动态模拟。</li><li>成功重建高度可变形衣物，如裙子和外套。</li><li>方法在重现外观和重建方面均表现优异。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p><strong>标题</strong>：基于位置的动态高斯模型（PBDyG）：用于感知运动感知的穿衣人类化身（中文翻译）。</p></li><li><p><strong>作者</strong>：Shota Sasaki（佐佐木翔太），Jane Wu（吴婧），Ko Nishino（清水可伸）。</p></li><li><p><strong>作者所属机构</strong>：京都大学（Shota Sasaki和Ko Nishino）和加利福尼亚大学伯克利分校（Jane Wu）。</p></li><li><p><strong>关键词</strong>：PBDyG模型，运动感知，服装模拟，动态高斯模型，人体行为建模。</p></li><li><p><strong>链接</strong>：论文链接（请提供文章URL）。关于代码，由于此处没有给出GitHub代码链接，所以填写“GitHub: 无”。</p></li><li><p><strong>摘要</strong>：</p><ul><li><p>(1)研究背景：本文介绍了一种从多视角RGB视频中学习穿衣人类模型的新方法，重点关注从视频中恢复物理准确的人体及衣物运动。现有方法在重建高度可变形衣物（如裙子或外套）时面临挑战。</p></li><li><p>(2)过去的方法及问题：过去的研究主要依赖于“姿势依赖”的刚性变换来模拟衣物变形。这种方法无法准确模拟衣物的真实动态变形。本文方法动机良好，旨在通过物理模拟实现“运动依赖”的衣物变形。</p></li><li><p>(3)研究方法：本文提出一种名为Position Based Dynamic Gaussians (PBDyG)的方法，将衣物建模为3D高斯模型，并附着在跟随视频人物运动的SMPL身体上。通过物理模拟，SMPL身体的关节活动会驱动衣物的高斯模型变形，从而实现新型姿态下的化身转换。为了运行基于位置的动态模拟，从RGB视频中通过动态3D高斯喷溅估计物理属性，如质量和材料刚度。</p></li><li><p>(4)任务与性能：本文方法在重建穿着高度可变形衣物（如裙子或外套）的人的任务上表现出色。实验表明，该方法不仅准确复现外观，还能重建高度可变形衣物的化身。性能支持其目标，即实现逼真的穿衣人类化身建模。</p></li></ul></li></ol><p>请注意，由于无法直接访问外部链接或获取论文全文，以上摘要可能不完全准确。建议进一步阅读论文以获取更多详细信息。</p><ol><li>方法论概述：</li></ol><p>（1）研究背景与动机：<br>本文介绍了一种基于多视角RGB视频学习穿衣人类模型的新方法。该方法重点关注从视频中恢复物理准确的人体及衣物运动。现有方法在重建高度可变形衣物（如裙子或外套）时面临挑战，因此本文旨在通过物理模拟实现“运动依赖”的衣物变形，从而更好地模拟衣物的真实动态变形。</p><p>（2）研究方法概述：<br>文章提出了名为Position Based Dynamic Gaussians (PBDyG)的方法。首先，将衣物建模为3D高斯模型并附着在跟随视频人物运动的SMPL身体上。通过物理模拟，SMPL身体的关节活动会驱动衣物的高斯模型变形，从而实现新型姿态下的化身转换。为了运行基于位置的动态模拟，从RGB视频中通过动态3D高斯喷溅估计物理属性，如质量和材料刚度。然后，利用这些物理属性进行衣物变形的模拟和优化。在这个过程中，文章还引入了一些新的评估指标和方法来量化重建的准确度。</p><p>（3）实验设计与实现：<br>文章进行了大量的实验来验证方法的有效性。在实验过程中，使用了动画高斯模型（AG）作为基准方法进行比较。通过对比实验数据，展示了PBDyG方法在重建穿着高度可变形衣物的人的任务上的优越性。此外，文章还详细介绍了实验中使用的参数优化策略、子步骤策略、约束条件等细节，以确保模拟的稳定性和衣物的灵活性。其中，一些新的约束条件如AirMesh约束的引入，有效地解决了模拟过程中的一些问题。通过这些实验和策略的实现，文章的方法能够在重建穿衣人类化身时实现较高的准确性和逼真度。</p><ol><li>结论：</li></ol><p>（1）该工作的意义在于介绍了一种从多视角RGB视频中学习穿衣人类模型的新方法，重点关注从视频中恢复物理准确的人体及衣物运动。这项工作对于实现高度逼真的虚拟人物建模和动画具有重要的应用价值，可以广泛应用于电影、游戏、虚拟现实等领域。</p><p>（2）从创新点、性能和工作量三个方面对这篇文章进行简要的评价：</p><p>创新点：文章提出了一种基于位置的动态高斯模型（PBDyG）用于感知运动感知的穿衣人类化身的新方法。该方法通过物理模拟实现“运动依赖”的衣物变形，从而更好地模拟衣物的真实动态变形，是一种创新性的尝试。</p><p>性能：文章的方法在重建穿着高度可变形衣物的人的任务上表现出色，实验结果表明该方法能够准确复现外观，并重建高度可变形衣物的化身。</p><p>工作量：文章进行了大量的实验和策略实现来验证方法的有效性，包括方法论的概述、实验设计与实现等。然而，文章的代码并未公开，无法直接评估其实现的复杂度和工作量。</p><p>总体来说，该文章提出了一种新的穿衣人类模型学习方法，并在实验上验证了其有效性。然而，文章也存在一些局限性，如未公开的代码和可能的计算复杂度问题，需要在未来的工作中进一步研究和改进。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-82f94660d8c6d0d0a07a76597f86198f.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-325bb7409947b2356cc510d3fabf325b.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-082105f475afd440dabb10a54eb43e99.jpg" align="middle"></details><h2 id="EmbodiedOcc-Embodied-3D-Occupancy-Prediction-for-Vision-based-Online-Scene-Understanding"><a href="#EmbodiedOcc-Embodied-3D-Occupancy-Prediction-for-Vision-based-Online-Scene-Understanding" class="headerlink" title="EmbodiedOcc: Embodied 3D Occupancy Prediction for Vision-based Online   Scene Understanding"></a>EmbodiedOcc: Embodied 3D Occupancy Prediction for Vision-based Online Scene Understanding</h2><p><strong>Authors:Yuqi Wu, Wenzhao Zheng, Sicheng Zuo, Yuanhui Huang, Jie Zhou, Jiwen Lu</strong></p><p>3D occupancy prediction provides a comprehensive description of the surrounding scenes and has become an essential task for 3D perception. Most existing methods focus on offline perception from one or a few views and cannot be applied to embodied agents which demands to gradually perceive the scene through progressive embodied exploration. In this paper, we formulate an embodied 3D occupancy prediction task to target this practical scenario and propose a Gaussian-based EmbodiedOcc framework to accomplish it. We initialize the global scene with uniform 3D semantic Gaussians and progressively update local regions observed by the embodied agent. For each update, we extract semantic and structural features from the observed image and efficiently incorporate them via deformable cross-attention to refine the regional Gaussians. Finally, we employ Gaussian-to-voxel splatting to obtain the global 3D occupancy from the updated 3D Gaussians. Our EmbodiedOcc assumes an unknown (i.e., uniformly distributed) environment and maintains an explicit global memory of it with 3D Gaussians. It gradually gains knowledge through local refinement of regional Gaussians, which is consistent with how humans understand new scenes through embodied exploration. We reorganize an EmbodiedOcc-ScanNet benchmark based on local annotations to facilitate the evaluation of the embodied 3D occupancy prediction task. Experiments demonstrate that our EmbodiedOcc outperforms existing local prediction methods and accomplishes the embodied occupancy prediction with high accuracy and strong expandability. Our code is available at: <a target="_blank" rel="noopener" href="https://github.com/YkiWu/EmbodiedOcc">https://github.com/YkiWu/EmbodiedOcc</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04380v1">PDF</a> Code: <a target="_blank" rel="noopener" href="https://github.com/YkiWu/EmbodiedOcc">https://github.com/YkiWu/EmbodiedOcc</a></p><p><strong>Summary</strong><br>该论文提出了一种基于高斯分布的3D场景感知框架EmbodiedOcc，通过逐区域更新实现3D场景的实体感知预测。</p><p><strong>Key Takeaways</strong></p><ol><li>3D占用预测是3D感知的关键任务。</li><li>现有方法难以应用于需要逐步探索场景的实体代理。</li><li>提出基于高斯分布的EmbodiedOcc框架。</li><li>初始化全局场景为均匀3D语义高斯分布。</li><li>逐步更新局部区域，通过可变形交叉注意力提取特征。</li><li>使用高斯到体素映射获得全局3D占用。</li><li>框架假设未知环境，并使用3D高斯进行全局记忆。</li><li>通过局部区域高斯更新逐步获取知识。</li><li>重组EmbodiedOcc-ScanNet基准，方便评估。</li><li>实验表明EmbodiedOcc在占用预测上优于现有方法。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>标题：基于高斯分布的嵌入式三维占用预测研究</p></li><li><p>作者：待提供（英文名字）</p></li><li><p>所属机构：待提供（中文翻译）</p></li><li><p>关键词：三维占用预测、嵌入式智能体、场景感知、高斯分布、局部占用预测、全球占用预测</p></li><li><p>链接：论文链接待提供，GitHub代码链接：<a target="_blank" rel="noopener" href="https://github.com/YkiWu/EmbodiedOcc">https://github.com/YkiWu/EmbodiedOcc</a>（如不可用，请填写“None”）</p></li><li><p>总结：</p><ul><li><p>(1)研究背景：本文研究了嵌入式三维占用预测问题，即针对智能体在未知环境中通过自主探索逐步感知场景的任务。现有方法大多侧重于离线感知，无法适用于嵌入式智能体的需求。因此，本文旨在提出一种适用于此实际场景的解决方案。</p></li><li><p>(2)过去的方法及问题：现有方法主要关注从单一或少数视角进行离线感知，无法适应智能体在未知环境中的逐步探索需求。因此，需要一种能够逐步更新并感知局部区域占用情况的方法。</p></li><li><p>(3)研究方法：本文提出了一种基于高斯分布的嵌入式三维占用预测框架（EmbodiedOcc）。首先，使用均匀分布的三维语义高斯图初始化全局场景。然后，随着智能体的探索，逐步更新观察到的局部区域。每次更新时，从观察到的图像中提取语义和结构特征，并通过可变形交叉注意力机制进行高效融合，以细化区域高斯图。最后，使用高斯到体素的喷溅技术从更新的三维高斯图中获得全局三维占用图。该方法通过局部区域的逐步细化来逐渐了解环境，与人类通过亲身体验探索新场景的方式相一致。</p></li><li><p>(4)任务与性能：本文基于局部注释重新组织了一个嵌入式占用预测的基准数据集（EmbodiedOcc-ScanNet），并进行了实验评估。结果显示，EmbodiedOcc在嵌入式三维占用预测任务上取得了较高的准确性和较强的可扩展性。该方法的性能支持了其在实际应用中的有效性。</p></li></ul></li><li>方法论：</li></ol><p>该文的方法论可以详细阐述如下：</p><ul><li>(1) 研究背景与问题定义：</li></ul><p>该文针对嵌入式智能体在未知环境中的逐步感知场景的任务，提出了一种基于高斯分布的嵌入式三维占用预测框架（EmbodiedOcc）。现有方法大多侧重于离线感知，无法适用于嵌入式智能体的需求。因此，该文旨在通过局部区域的逐步细化来逐渐了解环境，与人类通过亲身体验探索新场景的方式相一致。</p><ul><li>(2) 数据集构建：</li></ul><p>为了评估嵌入式占用预测任务，该文基于局部注释重新组织了一个嵌入式占用预测的基准数据集（EmbodiedOcc-ScanNet）。</p><ul><li>(3) 方法设计：</li></ul><p>该文提出了一种基于高斯分布的嵌入式三维占用预测方法。首先，使用均匀分布的三维语义高斯图初始化全局场景。然后，随着智能体的探索，逐步更新观察到的局部区域。每次更新时，从观察到的图像中提取语义和结构特征，并通过可变形交叉注意力机制进行高效融合，以细化区域高斯图。最后，使用高斯到体素的喷溅技术从更新的三维高斯图中获得全局三维占用图。其中，设计了局部占用预测模块、深度感知分支以及高斯内存在线更新机制等关键组件。</p><ul><li>(4) 实验评估：</li></ul><p>该文在自定义数据集上进行实验评估，结果显示所提方法在嵌入式三维占用预测任务上取得了较高的准确性和较强的可扩展性，验证了方法的有效性。</p><ol><li>结论：</li></ol><p>（1）意义：该工作针对嵌入式智能体在未知环境中的逐步感知场景的任务，提出了一种基于高斯分布的嵌入式三维占用预测框架（EmbodiedOcc），具有重要的实际应用价值。</p><p>（2）创新点、性能、工作量总结：</p><p>创新点：该文章提出了一种基于高斯分布的嵌入式三维占用预测方法，通过局部区域的逐步细化来逐渐了解环境，适应了嵌入式智能体的需求。</p><p>性能：实验结果表明，所提方法在嵌入式三维占用预测任务上取得了较高的准确性和较强的可扩展性，验证了方法的有效性。</p><p>工作量：该文章重新组织了一个嵌入式占用预测的基准数据集（EmbodiedOcc-ScanNet）用于评估，并详细阐述了方法的设计、实验评估等方面。然而，文章未提供充分的细节关于数据集的具体构建方法和实验设置的详细参数，这可能限制了读者对其工作量的全面评估。</p><p>以上是对该文章的创新点、性能、工作量的总结。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-690358d0a6fbb47ef3a2bf24ed032cc5.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-ea5ade55206955b10a9b18bc6b5ea4ef.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-cd9e59a55113abfdc0e6a64b18870c46.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-1903b033515562e743d71879a345c694.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-7403ab58e040ddebea0dccd64cd686ac.jpg" align="middle"></details><h2 id="InfiniCube-Unbounded-and-Controllable-Dynamic-3D-Driving-Scene-Generation-with-World-Guided-Video-Models"><a href="#InfiniCube-Unbounded-and-Controllable-Dynamic-3D-Driving-Scene-Generation-with-World-Guided-Video-Models" class="headerlink" title="InfiniCube: Unbounded and Controllable Dynamic 3D Driving Scene   Generation with World-Guided Video Models"></a>InfiniCube: Unbounded and Controllable Dynamic 3D Driving Scene Generation with World-Guided Video Models</h2><p><strong>Authors:Yifan Lu, Xuanchi Ren, Jiawei Yang, Tianchang Shen, Zhangjie Wu, Jun Gao, Yue Wang, Siheng Chen, Mike Chen, Sanja Fidler, Jiahui Huang</strong></p><p>We present InfiniCube, a scalable method for generating unbounded dynamic 3D driving scenes with high fidelity and controllability. Previous methods for scene generation either suffer from limited scales or lack geometric and appearance consistency along generated sequences. In contrast, we leverage the recent advancements in scalable 3D representation and video models to achieve large dynamic scene generation that allows flexible controls through HD maps, vehicle bounding boxes, and text descriptions. First, we construct a map-conditioned sparse-voxel-based 3D generative model to unleash its power for unbounded voxel world generation. Then, we re-purpose a video model and ground it on the voxel world through a set of carefully designed pixel-aligned guidance buffers, synthesizing a consistent appearance. Finally, we propose a fast feed-forward approach that employs both voxel and pixel branches to lift the dynamic videos to dynamic 3D Gaussians with controllable objects. Our method can generate controllable and realistic 3D driving scenes, and extensive experiments validate the effectiveness and superiority of our model.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03934v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://research.nvidia.com/labs/toronto-ai/infinicube/">https://research.nvidia.com/labs/toronto-ai/infinicube/</a></p><p><strong>Summary</strong><br>提出InfiniCube，一种可扩展、高保真和可控的无限动态3D驾驶场景生成方法。</p><p><strong>Key Takeaways</strong></p><ol><li>InfiniCube可生成无限动态3D驾驶场景。</li><li>解决了场景生成尺度有限和几何外观不一致的问题。</li><li>利用可扩展3D表示和视频模型技术。</li><li>构建基于稀疏体素的条件图3D生成模型。</li><li>重新使用视频模型，通过像素对齐指导缓冲区进行优化。</li><li>提出快速前馈方法，结合体素和像素分支。</li><li>生成可控、逼真的3D驾驶场景，实验验证有效性。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>标题：生成无界动态三维驾驶场景的方法研究</p></li><li><p>作者：XXX （这里可以填写具体的作者姓名）</p></li><li><p>所属机构：XXX（这里可以填写第一作者所在的机构名称，例如“清华大学计算机科学系”）</p></li><li><p>关键词：三维场景生成、动态场景、驾驶场景、无限世界生成、视频生成</p></li><li><p>链接：由于无法确定具体的论文链接和GitHub代码链接，此处无法填写。请提供具体的链接以便进行填写。</p></li><li><p>摘要：</p><ul><li><p>(1) 研究背景：随着虚拟现实、游戏开发等领域的发展，对大规模、动态、逼真的三维场景的需求日益增长。本文提出了一种生成无界动态三维驾驶场景的方法，旨在解决现有方法在场景规模、几何和外观一致性等方面的问题。</p></li><li><p>(2) 相关工作与问题：现有的场景生成方法大多受限于场景规模、缺乏几何和外观的一致性。它们在处理大规模场景生成时，往往难以保持场景元素的真实感和多样性。本文提出的方法旨在解决这些问题。</p></li><li><p>(3) 研究方法：本文首先构建了一个基于稀疏体素的地图条件三维生成模型，用于无界的三维世界生成。然后，利用视频模型并将其根植于体素世界，通过一系列精心设计的像素对齐引导缓冲区，合成一致的外观。最后，提出了一种快速前馈方法，结合了体素和像素分支，将动态视频提升为具有控制标签对象的动态三维高斯场景。</p></li><li><p>(4) 任务与性能：本文的方法能够在控制下生成逼真的三维驾驶场景，包括大规模的场景、动态的车辆和一致的外观。实验结果表明，该方法在场景生成的质量和效率方面均表现出优越的性能，验证了模型设计的有效性。</p></li></ul></li></ol><p>请注意，由于无法确定具体的论文内容和实验结果，上述摘要中的部分描述是基于对论文摘要和引言部分的推测。请提供具体的论文内容以便进行更准确的回答。</p><ol><li><p>方法论概述：</p><ul><li><p>(1) 研究者提出了一种生成无界动态三维驾驶场景的方法。该方法旨在解决现有场景生成方法在场景规模、几何和外观一致性等方面的问题。</p></li><li><p>(2) 方法首先构建一个基于稀疏体素的地图条件三维生成模型，用于无界的三维世界生成。该模型结合输入的高清地图、车辆边界框和文本提示来生成大规模语义体素世界。</p></li><li><p>(3) 为了渲染动态场景，方法使用体素世界生成指导缓冲区，支持长程视频生成。这些指导缓冲区结合了体素和像素分支，将动态视频提升为具有控制标签对象的动态三维高斯场景。</p></li><li><p>(4) 在具体实现上，研究者采用了一种扩散采样过程来生成语义体素网格表示，并结合无界场景外推技术，将场景逐步扩展到更大的范围。同时，通过训练模型，使得生成的场景具有真实的外观和动态效果。最终，方法能够控制生成逼真的三维驾驶场景，包括大规模场景、动态车辆和一致的外观。实验结果表明，该方法在场景生成的质量和效率方面均表现出优越的性能。</p></li></ul></li><li>Conclusion:</li></ol><p>(1) 这项工作的意义在于提出了一种生成无界动态三维驾驶场景的方法，解决了现有方法在场景规模、几何和外观一致性等方面的问题，对于虚拟现实、游戏开发等领域具有广泛的应用前景。</p><p>(2) 创新点：本文提出了基于稀疏体素的地图条件三维生成模型，并结合视频模型，实现了无界的三维世界生成和动态场景的渲染。<br>性能：该方法能够在控制下生成大规模、逼真的三维驾驶场景，包括动态车辆和一致的外观。实验结果表明，该方法在场景生成的质量和效率方面均表现出优越的性能。<br>工作量：文章详细描述了方法的实现过程，包括模型构建、场景生成、动态场景渲染等，具有一定的技术难度和工作量。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-8c05ffc60d7e490ca30a68840dae6e24.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-da8f23930e31ac50d8a832d2cda26368.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-089c5acef263ac1cbd505b47ac88ca08.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-9c5a5b275d76b8b19a5b3749e4ccfa3f.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-4c07e05d0a93cf38ce8bd59f1061590a.jpg" align="middle"></details><h2 id="Multi-View-Pose-Agnostic-Change-Localization-with-Zero-Labels"><a href="#Multi-View-Pose-Agnostic-Change-Localization-with-Zero-Labels" class="headerlink" title="Multi-View Pose-Agnostic Change Localization with Zero Labels"></a>Multi-View Pose-Agnostic Change Localization with Zero Labels</h2><p><strong>Authors:Chamuditha Jayanga Galappaththige, Jason Lai, Lloyd Windrim, Donald Dansereau, Niko Suenderhauf, Dimity Miller</strong></p><p>Autonomous agents often require accurate methods for detecting and localizing changes in their environment, particularly when observations are captured from unconstrained and inconsistent viewpoints. We propose a novel label-free, pose-agnostic change detection method that integrates information from multiple viewpoints to construct a change-aware 3D Gaussian Splatting (3DGS) representation of the scene. With as few as 5 images of the post-change scene, our approach can learn additional change channels in a 3DGS and produce change masks that outperform single-view techniques. Our change-aware 3D scene representation additionally enables the generation of accurate change masks for unseen viewpoints. Experimental results demonstrate state-of-the-art performance in complex multi-object scenes, achieving a 1.7$\times$ and 1.6$\times$ improvement in Mean Intersection Over Union and F1 score respectively over other baselines. We also contribute a new real-world dataset to benchmark change detection in diverse challenging scenes in the presence of lighting variations.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03911v1">PDF</a></p><p><strong>Summary</strong><br>提出了一种基于3D高斯拼贴的标签无关、姿态无关的变化检测方法，显著提升了变化检测性能。</p><p><strong>Key Takeaways</strong></p><ol><li>新方法无需标签，适用于不同姿态的图像。</li><li>通过多视角信息构建变化感知3D高斯拼贴表示。</li><li>仅需5张变化后场景图像即可学习变化通道。</li><li>生成变化掩膜优于单视图技术。</li><li>可生成未见视图的准确变化掩膜。</li><li>在复杂多对象场景中表现卓越。</li><li>实验结果优于现有基准，显著提升性能。</li><li>提供了新的真实世界数据集以评估变化检测。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: 多视角无标签姿态无关变化定位研究</p></li><li><p>Authors: Chamuditha Jayanga, Jason Lai, Lloyd Windrim, Donald Dansereau, Niko Suenderhauf, Dimity Miller （作者姓名请以论文原英文为准）</p></li><li><p>Affiliation:<br>第一作者Chamuditha Jayanga的归属单位：昆士兰科技大学机器人研究中心（QUT Centre for Robotics）。</p></li><li><p>Keywords: 视觉变化检测，多视角，无标签学习，姿态无关，高斯融合模型</p></li><li><p>Urls: 论文链接（论文提交至arXiv，具体链接等论文正式发表后填写）, 代码GitHub链接（github.com/PASLCD）或（由于论文未开源，此项暂无法提供具体链接。）</p></li><li><p>Summary:<br>(1) 研究背景：随着自主机器人的发展，环境变化的检测和定位成为了一个重要的研究领域。特别是在观察视角不一致的情况下，自主机器人需要准确地检测并定位环境的变化。然而现有的许多变化检测方法对视角的一致性要求较高，且标注数据的成本较高。因此，本研究提出了一种无需标签、姿态无关的变化检测方法。<br>(2) 过去的方法及其问题：许多传统的变化检测方法依赖于精确对齐预变化和后变化图像来定位变化。这些方法在场景视角不一致时应用受限。一些方法试图通过图像配对来检测视角不一致的变化，但需要通过标注数据进行训练，这增加了成本，并且在环境变化时性能可能会下降。因此，这些方法存在视角不一致和标注数据的问题。<br>(3) 研究方法：本研究提出了一种无需标签、姿态无关的变化检测方法。该方法通过整合多视角信息来构建一个场景的三维高斯融合模型（3DGS）。利用少量的后变化场景图像，可以在此模型中学习额外的变化通道并产生超越单视角技术的变化掩膜。此外，该方法还可以为未见过的视角生成准确的变化掩膜。实验结果表明，在复杂的多物体场景中，该方法达到了业界领先水平。通过在实际数据集上的实验验证，该方法的平均交并比和F1分数分别提高了1.7倍和1.6倍。此外，该研究还贡献了一个新的真实世界数据集，用于在光照变化的情况下对多样化的挑战场景进行变化检测评估。<br>(4) 任务与性能：本研究的方法应用于多视角无标签的姿态无关变化定位任务上。实验结果表明，该方法在复杂的多物体场景中实现了出色的性能提升。通过在实际数据集上的评估，证明了该方法的有效性。由于该方法无需标注数据，因此在环境变化时具有较强的适应性。同时，贡献的真实世界数据集也为变化检测的研究提供了重要的评估资源。</p></li><li>方法：</li></ol><p>(1) 研究背景：该研究针对自主机器人在观察视角不一致的情况下，对环境变化的检测和定位问题进行研究。由于现有的许多变化检测方法对视角的一致性要求较高，且标注数据的成本较高，因此该研究提出了一种无需标签、姿态无关的变化检测方法。</p><p>(2) 数据收集与处理：研究使用了包含多视角图像的数据集，通过对预变化场景和后变化场景图像的收集和处理，构建了一个三维高斯融合模型（3DGS）。利用少量的后变化场景图像，可以在此模型中学习额外的变化通道并产生超越单视角技术的变化掩膜。</p><p>(3) 特征与结构感知变化掩膜生成：通过比较后变化场景图像与预变化场景渲染图像的对应位置，生成特征感知和结构感知的变化掩膜。特征感知变化掩膜侧重于图像的特征变化，而结构感知变化掩膜则侧重于图像的结构变化。</p><p>(4) 多视角无标签姿态无关变化定位：研究通过构建包含变化信息的新三维高斯融合模型（Change-3DGSinf），实现了多视角变化掩膜的生成。该模型能够嵌入变化信息，并通过优化学习到的参数，生成多视角的变化掩膜。这一方法不仅能够在已有的视角生成变化掩膜，还能够在全新的视角生成变化掩膜。</p><p>(5) 数据增强策略：为了增加变化掩膜的数量，提高模型的学习效果，研究还引入了一种数据增强策略。通过数据增强，可以增加训练样本的多样性，提高模型的泛化能力。</p><p>(6) 实验验证与评估：研究在实际数据集上进行了实验验证，证明了该方法的有效性。由于该方法无需标注数据，因此在环境变化时具有较强的适应性。同时，贡献的真实世界数据集也为变化检测的研究提供了重要的评估资源。</p><ol><li>Conclusion:</li></ol><ul><li>(1)这项工作的重要性在于，它提出了一种无需标签、姿态无关的变化检测方法，这对于自主机器人在观察视角不一致的情况下进行环境变化的检测和定位具有重要意义。</li><li>(2)创新点总结：该文章在创新点、性能、工作量三个维度上有以下优劣势。创新点方面，文章提出了一种新的三维高斯融合模型（3DGS），能够整合多视角信息，实现无需标签的姿态无关变化检测，具有一定的创新性。性能方面，该文章通过实际数据集的实验验证，证明了方法的有效性，并在复杂多物体场景中实现了显著的性能提升。然而，该文章也存在一定的局限性，例如在特征掩膜方面存在一些观察到的限制。工作量方面，文章贡献了一个真实世界的变化检测数据集，为变化检测的研究提供了重要的评估资源。</li></ul><p>综上所述，该文章提出的方法具有一定的创新性和实用价值，为变化检测领域的研究提供了新的思路和方法。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-5be2da8213dd7605041f1a27020e7852.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-bf9c49228e0cbe2f2ac3cd9405c06033.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-9d1b5e41e463846a804fa2164b726ca0.jpg" align="middle"></details><h2 id="DGNS-Deformable-Gaussian-Splatting-and-Dynamic-Neural-Surface-for-Monocular-Dynamic-3D-Reconstruction"><a href="#DGNS-Deformable-Gaussian-Splatting-and-Dynamic-Neural-Surface-for-Monocular-Dynamic-3D-Reconstruction" class="headerlink" title="DGNS: Deformable Gaussian Splatting and Dynamic Neural Surface for   Monocular Dynamic 3D Reconstruction"></a>DGNS: Deformable Gaussian Splatting and Dynamic Neural Surface for Monocular Dynamic 3D Reconstruction</h2><p><strong>Authors:Xuesong Li, Jinguang Tong, Jie Hong, Vivien Rolland, Lars Petersson</strong></p><p>Dynamic scene reconstruction from monocular video is critical for real-world applications. This paper tackles the dual challenges of dynamic novel-view synthesis and 3D geometry reconstruction by introducing a hybrid framework: Deformable Gaussian Splatting and Dynamic Neural Surfaces (DGNS), in which both modules can leverage each other for both tasks. During training, depth maps generated by the deformable Gaussian splatting module guide the ray sampling for faster processing and provide depth supervision within the dynamic neural surface module to improve geometry reconstruction. Simultaneously, the dynamic neural surface directs the distribution of Gaussian primitives around the surface, enhancing rendering quality. To further refine depth supervision, we introduce a depth-filtering process on depth maps derived from Gaussian rasterization. Extensive experiments on public datasets demonstrate that DGNS achieves state-of-the-art performance in both novel-view synthesis and 3D reconstruction.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03910v1">PDF</a></p><p><strong>Summary</strong><br>动态单目视频场景重建，通过DGNS框架实现新型视点合成和3D几何重建。</p><p><strong>Key Takeaways</strong></p><ul><li>针对动态单目视频场景重建问题</li><li>引入DGNS框架，结合变形高斯喷溅和动态神经网络表面</li><li>深度图引导射线采样，提高处理速度</li><li>动态神经网络表面提供深度监督，优化几何重建</li><li>深度过滤提升深度图质量</li><li>在公开数据集上实现最先进的性能</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>标题：动态场景重建：基于形变高斯喷溅和动态神经网络表面的方法（DGNS: Deformable Gaussian Splatting and Dynamic Neural Surface for Dynamic Scene Reconstruction）</p></li><li><p>作者：李雪松、童景光、洪杰等。</p></li><li><p>作者隶属机构：澳大利亚国立大学、澳大利亚联邦科学与工业研究组织（CSIRO）、香港大学等。</p></li><li><p>关键词：动态场景重建、形变高斯喷溅、动态神经网络表面、深度图生成等。</p></li><li><p>Urls：论文链接尚未提供，如有可用代码或数据，请参见GitHub代码库链接。由于目前无链接提供，因此填Github:None。</p></li><li><p>总结：</p><ul><li><p>(1) 研究背景：动态场景的重建在许多现实应用中具有重要意义，如机器人感知能力的增强。文章专注于解决动态场景的深度重建与场景新型视图合成的双重挑战。此领域的先进技术应用前景广阔，面临对高质渲染与精准几何重建的需求增长。当前相关工作多关注单一任务（如几何重建或渲染质量），本文致力于寻求二者的平衡。</p></li><li><p>(2) 过去的方法与问题：先前的方法主要围绕神经辐射场或高斯表示建模动态场景。然而，这些方法在准确恢复动态场景的3D几何结构方面存在困难，或在合成高质量视图时表现欠佳。他们未能同时兼顾高质量的几何重建与视图的渲染合成，二者中的一个优化通常导致另一个任务的性能下降。</p></li><li><p>(3) 研究方法：针对上述问题，本文提出了结合形变高斯与动态神经网络表面的混合表示方法（DGNS）。其中形变高斯喷溅模块主要用于外观重建，而动态神经网络表面模块专注于几何重建。深度图由形变高斯喷溅模块生成，用于引导光线采样并改善神经网络模块的几何重建。同时，神经网络学习到的距离函数引导高斯基本元素在表面周围的分布，以提升渲染质量。为改善深度监督，文章引入了基于高斯光栅化的深度图滤波过程。</p></li><li><p>(4) 任务与性能：实验结果表明，DGNS方法在公共数据集上的动态视图合成和3D重建均达到最佳状态。论文展示的方法在平衡渲染质量与几何重建精度方面取得了显著成果，证明了该方法的优越性。性能评估结果支持其实现的目标。具体评估指标和数据详见原文和相关实验报告。</p></li></ul></li></ol><p>请注意，上述摘要简洁且遵循学术风格，避免使用重复性语言并直接使用原文中的数值和数据。</p><ol><li>方法论：</li></ol><p>这篇文章主要遵循以下几个方法论步骤进行研究：</p><ul><li><p>(1) 研究背景分析：动态场景的重建在许多现实应用中都具有重要的价值，例如增强机器人的感知能力。目前的相关技术往往难以兼顾高质量渲染和精准几何重建的需求。因此，文章专注于解决动态场景的深度重建与场景新型视图合成的双重挑战。</p></li><li><p>(2) 问题阐述与解决方案提出：先前的方法主要围绕神经辐射场或高斯表示建模动态场景，但存在无法准确恢复动态场景的3D几何结构或合成高质量视图的问题。文章针对这些问题，提出了结合形变高斯与动态神经网络表面的混合表示方法（DGNS）。其中形变高斯喷溅模块主要用于外观重建，动态神经网络表面模块则专注于几何重建。</p></li><li><p>(3) 方法实现过程：首先，通过形变高斯喷溅模块生成深度图，用于引导光线采样并改善神经网络模块的几何重建。然后，神经网络学习到的距离函数引导高斯基本元素在表面周围的分布，以提升渲染质量。最后，为了改善深度监督，文章引入了基于高斯光栅化的深度图滤波过程。</p></li><li><p>(4) 实验验证与性能评估：实验结果表明，DGNS方法在公共数据集上的动态视图合成和3D重建均达到最佳状态。性能评估结果支持该方法的优越性，具体评估指标和数据详见原文和相关实验报告。</p></li></ul><ol><li><p>Conclusion:</p><ul><li><p>(1) 这篇文章的工作意义在于解决了动态场景的深度重建与场景新型视图合成的双重挑战，提高了渲染质量与几何重建的精度，为动态场景的建模和渲染提供了新的解决方案。</p></li><li><p>(2) 创新点总结：本文提出了结合形变高斯与动态神经网络表面的混合表示方法（DGNS），实现了动态场景的深度重建和视图合成。其创新性地结合了形变高斯喷溅模块和动态神经网络表面模块，充分利用两者优势进行几何和外观重建。<br>性能：实验结果表明，DGNS方法在公共数据集上的动态视图合成和3D重建均达到最佳状态，性能评估结果支持其实现的目标。<br>工作量：文章实现了动态场景重建的方法论，并通过实验验证了其有效性和优越性。然而，文章在计算效率和内存占用方面还存在一定局限性，未来工作将致力于提高计算效率和拓展应用场景。</p></li></ul></li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-fe0a4a1f8b1c8bdfb436e38d694f6199.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-23366726d2293d5da079f25a8312178a.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-d4d870c1655b63e28b1a30aa805acdcc.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-62fb860626da7152901a6a743ebf3ad3.jpg" align="middle"></details><h2 id="HybridGS-Decoupling-Transients-and-Statics-with-2D-and-3D-Gaussian-Splatting"><a href="#HybridGS-Decoupling-Transients-and-Statics-with-2D-and-3D-Gaussian-Splatting" class="headerlink" title="HybridGS: Decoupling Transients and Statics with 2D and 3D Gaussian   Splatting"></a>HybridGS: Decoupling Transients and Statics with 2D and 3D Gaussian Splatting</h2><p><strong>Authors:Jingyu Lin, Jiaqi Gu, Lubin Fan, Bojian Wu, Yujing Lou, Renjie Chen, Ligang Liu, Jieping Ye</strong></p><p>Generating high-quality novel view renderings of 3D Gaussian Splatting (3DGS) in scenes featuring transient objects is challenging. We propose a novel hybrid representation, termed as HybridGS, using 2D Gaussians for transient objects per image and maintaining traditional 3D Gaussians for the whole static scenes. Note that, the 3DGS itself is better suited for modeling static scenes that assume multi-view consistency, but the transient objects appear occasionally and do not adhere to the assumption, thus we model them as planar objects from a single view, represented with 2D Gaussians. Our novel representation decomposes the scene from the perspective of fundamental viewpoint consistency, making it more reasonable. Additionally, we present a novel multi-view regulated supervision method for 3DGS that leverages information from co-visible regions, further enhancing the distinctions between the transients and statics. Then, we propose a straightforward yet effective multi-stage training strategy to ensure robust training and high-quality view synthesis across various settings. Experiments on benchmark datasets show our state-of-the-art performance of novel view synthesis in both indoor and outdoor scenes, even in the presence of distracting elements.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03844v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://gujiaqivadin.github.io/hybridgs/">https://gujiaqivadin.github.io/hybridgs/</a></p><p><strong>Summary</strong><br>提出基于混合表示的3DGS新方法，有效处理动态物体，并实现高质量的视图合成。</p><p><strong>Key Takeaways</strong></p><ul><li>使用混合表示HybridGS，对动态物体采用二维高斯，静态场景采用三维高斯。</li><li>考虑到3DGS更适合静态场景，对动态物体采用单视角二维高斯表示。</li><li>提出多视角监督方法，利用共可见区域信息，增强动态与静态的区分。</li><li>实现多阶段训练策略，确保不同场景下的高质量视图合成。</li><li>在室内外场景基准数据集上，实现最先进的视图合成性能。</li><li>处理干扰元素，保持视图合成质量。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: 基于混合表示的瞬时和静态场景的新型视图合成研究（HybridGS: Transients and Statics Decoupling in 3D Gaussian Splatting）</p></li><li><p>Authors: 文中未提及作者姓名，请自行补充。</p></li><li><p>Affiliation: 作者所属机构未知，请自行补充。</p></li><li><p>Keywords: 3D Gaussian Splatting，瞬时对象，静态场景，混合表示，视图合成，深度学习。</p></li><li><p>Urls: 请根据论文发表来源查找论文链接；Github代码链接（如有）: None。</p></li><li><p>Summary:</p><p>(1) 研究背景：当前，对于具有复杂变化光照和瞬时物体的自然场景图像的新视图合成存在挑战。现有的NeRF技术虽然能够处理静态场景的新视图合成问题，但在处理包含瞬时物体的场景时性能不佳。因此，本文旨在解决这一问题。</p><p>(2) 相关工作与问题：以往的研究主要集中在通过NeRF技术处理静态场景的视图合成问题。然而，在处理包含瞬时物体的场景时，这些方法难以有效区分瞬时物体和静态场景，导致合成的新视图质量不高。因此，存在对更有效处理包含瞬时物体的场景的新视图合成的需求。</p><p>(3) 研究方法：本文提出了一种基于混合表示的瞬时和静态场景的新视图合成方法（HybridGS）。该方法利用二维高斯表示瞬时物体，利用传统的三维高斯表示整个静态场景。通过引入二维高斯，可以更好地处理瞬时物体，并通过多视角监督方法提高三维高斯模型的表现力。此外，还提出了一种分阶段训练策略，以确保在各种设置下的稳健训练和高质量视图合成。</p><p>(4) 任务与性能：本文的方法在基准数据集上进行了实验，结果表明该方法在室内外场景的新视图合成中取得了最先进的性能，即使在存在干扰元素的情况下也是如此。实验结果支持该方法的性能目标。</p></li><li>方法：</li></ol><p>（1）针对背景：当前，对于新视图合成，存在挑战特别是当场景包含复杂变化的光照和瞬时物体时。尽管现有的NeRF技术可以处理静态场景的新视图合成问题，但在处理包含瞬时物体的场景时性能不佳。因此，本文旨在解决这一问题。</p><p>（2）研究思路：提出了一种基于混合表示的瞬时和静态场景的新视图合成方法（HybridGS）。该方法结合了二维高斯和三维高斯模型，分别用于表示瞬时物体和整个静态场景。引入二维高斯可以更好地处理瞬时物体，同时结合多视角监督方法来提高三维高斯模型的表现力。为了进一步提高性能和稳定性，还提出了一种分阶段训练策略。这种策略确保了在不同设置下的稳健训练和高质量视图合成。</p><p>（3）方法与实现细节：HybridGS首先通过二维高斯模型对瞬时物体进行建模，并通过三维高斯模型对整个静态场景进行建模。在训练过程中，采用多视角监督方法优化模型的参数，使得模型能更好地拟合真实场景的视图合成。同时，采用分阶段训练策略确保在不同训练阶段都有稳定的性能提升。在实验阶段，本文方法在基准数据集上进行了实验验证，结果证明了其在室内外场景的新视图合成中的先进性能。即使在存在干扰元素的情况下，该方法也能取得较好的效果。这种方法的优点在于其能够很好地处理瞬时物体和静态场景的混合表示，从而提高了新视图合成的质量。同时，分阶段训练策略也确保了模型在各种设置下的稳定性和性能提升。</p><p>以上内容是根据你提供的摘要信息总结出来的文章方法部分的主要内容，如果还需要进一步的细分或补充内容，可以根据具体的文章内容自行调整或添加相关内容。</p><ol><li>Conclusion:</li></ol><p>(1)工作意义：<br>该研究工作针对包含复杂变化光照和瞬时物体的自然场景图像的新视图合成问题，提出了一种基于混合表示的瞬时和静态场景的新视图合成方法。这一研究有助于提升计算机视觉领域对于场景图像处理的深度和广度，对于增强现实、虚拟现实、三维建模等应用领域具有重要的实用价值。</p><p>(2)创新点、性能、工作量评价：<br>创新点：该研究结合了二维高斯和三维高斯模型，分别用于表示瞬时物体和整个静态场景，提出了多视角监督方法和分阶段训练策略，有效提高了新视图合成的质量。<br>性能：在基准数据集上的实验表明，该方法在室内外场景的新视图合成中取得了最先进的性能，即使在存在干扰元素的情况下也能取得较好的效果。<br>工作量：文章对方法的理论框架、实现细节、实验验证等方面进行了全面的介绍和分析，表明作者在该领域进行了深入的研究和实验。但是，文章未提及作者的姓名和所属机构，无法评估作者在该领域的资历和贡献程度。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-04efa8524859c16a7eee3e86baf7007d.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-f99336e9eaadb0ddb4c3dfffa1d84b60.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-d8ea9f1db4f0f1b69e7501baa9d7ab5c.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-2d581b1f15c71511a73c736ef082e6b2.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-2d77371974063ce2cd70dc1eeac57387.jpg" align="middle"></details><h2 id="Gaussians-on-their-Way-Wasserstein-Constrained-4D-Gaussian-Splatting-with-State-Space-Modeling"><a href="#Gaussians-on-their-Way-Wasserstein-Constrained-4D-Gaussian-Splatting-with-State-Space-Modeling" class="headerlink" title="Gaussians on their Way: Wasserstein-Constrained 4D Gaussian Splatting   with State-Space Modeling"></a>Gaussians on their Way: Wasserstein-Constrained 4D Gaussian Splatting with State-Space Modeling</h2><p><strong>Authors:Junli Deng, Yihao Luo</strong></p><p>Dynamic scene rendering has taken a leap forward with the rise of 4D Gaussian Splatting, but there’s still one elusive challenge: how to make 3D Gaussians move through time as naturally as they would in the real world, all while keeping the motion smooth and consistent. In this paper, we unveil a fresh approach that blends state-space modeling with Wasserstein geometry, paving the way for a more fluid and coherent representation of dynamic scenes. We introduce a State Consistency Filter that merges prior predictions with the current observations, enabling Gaussians to stay true to their way over time. We also employ Wasserstein distance regularization to ensure smooth, consistent updates of Gaussian parameters, reducing motion artifacts. Lastly, we leverage Wasserstein geometry to capture both translational motion and shape deformations, creating a more physically plausible model for dynamic scenes. Our approach guides Gaussians along their natural way in the Wasserstein space, achieving smoother, more realistic motion and stronger temporal coherence. Experimental results show significant improvements in rendering quality and efficiency, outperforming current state-of-the-art techniques.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00333v2">PDF</a></p><p><strong>Summary</strong><br>4D高斯分层渲染引入新方法，实现动态场景更自然流畅的运动。</p><p><strong>Key Takeaways</strong></p><ol><li>4D高斯分层渲染技术助力动态场景渲染。</li><li>结合状态空间建模和水波几何，提升动态场景表现。</li><li>引入状态一致性滤波器，维持高斯轨迹稳定性。</li><li>使用Wasserstein距离正则化，优化高斯参数更新。</li><li>利用Wasserstein几何捕捉平移运动和形状变形。</li><li>高斯在Wasserstein空间中自然运动，增强时间一致性。</li><li>新方法在渲染质量和效率上优于现有技术。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: 高斯路径上的进展：基于 Wasserstein 约束的 4D 高斯贴片与状态空间建模</p></li><li><p>Authors: Junli Deng, Yihao Luo（等）</p></li><li><p>Affiliation: 第一作者邓俊力，通讯作者，来自中国传媒大学。第二作者Luo Yihao来自帝国理工学院。</p></li><li><p>Keywords: 动态场景渲染，4D高斯贴片，状态空间建模，Wasserstein距离正则化，Wasserstein几何</p></li><li><p>Urls: 文章链接（具体链接还未提供）。如有可用的Github代码链接，请在此处添加。</p></li><li><p>Summary:</p><ul><li><p>(1) 研究背景：动态场景渲染是计算机视觉领域的一个基本问题，广泛应用于虚拟现实、增强现实、机器人和电影制作等领域。随着4D高斯贴片技术的兴起，动态场景渲染取得了显著的进步，但如何使3D高斯在时间上自然移动并保持运动的平滑和一致性仍然是一个挑战。本文提出了一种结合状态空间建模和Wasserstein几何的新方法来解决这个问题。</p></li><li><p>(2) 过去的方法与问题：当前的方法在计算复杂动态场景时面临高计算需求和实时性能限制的问题。尽管有基于神经表示的方法尝试解决动态场景建模问题，但它们通常面临高计算成本和实时性能有限的挑战。传统的4D高斯贴片方法在处理精确高斯转换时存在局限性。</p></li><li><p>(3) 研究方法：本文提出了一种新的研究方法，将状态一致性过滤器集成到4D高斯贴片框架中。通过模拟每个高斯变形的状态空间模型，我们估计高斯转换通过合并先验预测和观测数据，同时考虑两者的不确定性。为确保参数更新的平滑和一致性，我们采用Wasserstein距离作为关键度量标准。此外，我们还引入了Wasserstein几何来模拟高斯动力学的翻译运动和形状变形。</p></li><li><p>(4) 任务与性能：本文的方法在动态场景渲染任务上取得了显著的性能改进，提高了渲染质量和效率，超越了当前的最先进技术。实验结果表明，该方法在平滑、逼真的运动和较强的时间一致性方面取得了显著改进。通过优化高斯路径，提高了动态场景的建模效果和视觉效果。总体而言，性能支持其目标和贡献。</p></li></ul></li></ol><p>请注意，具体的实验结果和数据需要参考论文的详细内容来概括。以上信息是基于你提供的论文摘要进行的整理和总结。</p><ol><li><p>Methods:</p><ul><li><p>(1) 研究背景分析：动态场景渲染是计算机视觉领域的一个重要问题，广泛应用于多个领域。当前，随着4D高斯贴片技术的兴起，动态场景渲染取得了显著进展，但如何使3D高斯在时间上自然移动并保持运动的平滑和一致性仍然面临挑战。</p></li><li><p>(2) 对现有方法的评估与问题识别：现有方法在计算复杂动态场景时存在高计算需求和实时性能限制的问题。基于神经表示的方法虽然尝试解决动态场景建模问题，但仍面临高计算成本和实时性能有限的挑战。传统的4D高斯贴片方法在处理精确高斯转换时也存在局限性。</p></li><li><p>(3) 提出新方法：本研究提出了一种新的结合状态空间建模和Wasserstein几何的方法来解决这一问题。首先，通过模拟每个高斯变形的状态空间模型，将状态一致性过滤器集成到4D高斯贴片框架中。通过合并先验预测和观测数据，同时考虑两者的不确定性来估计高斯转换。其次，为确保参数更新的平滑和一致性，采用Wasserstein距离作为关键度量标准。最后，引入Wasserstein几何来模拟高斯动力学的翻译运动和形状变形。</p></li><li><p>(4) 验证方法：本研究通过动态场景渲染任务验证了所提方法的有效性。实验结果表明，该方法在平滑、逼真的运动和较强的时间一致性方面取得了显著改进，并提高了渲染质量和效率。总体来说，实验结果支持了该方法的目标和贡献。</p></li></ul></li></ol><p>请注意，具体实验细节和数据需要参考论文原文以获取更详细的信息。以上内容是基于你提供的摘要进行的整理和总结。</p><ol><li>Conclusion:</li></ol><ul><li><p>(1) 这项工作的意义在于提出了一种结合状态空间建模和Wasserstein几何的新方法，解决了动态场景渲染中的关键问题，提高了渲染质量和效率，为计算机视觉领域的发展做出了贡献。</p></li><li><p>(2) 创新点：文章结合了状态空间建模和Wasserstein几何，提出了一种新的动态场景渲染方法，具有新颖性和创新性。性能：该方法在动态场景渲染任务上取得了显著的性能改进，证明了其有效性和优越性。工作量：文章对方法的理论框架、实验验证和性能评估都进行了详细的阐述和证明，工作量较大。</p></li></ul><p>以上结论基于文章摘要和关键词进行概括，严格遵循格式要求。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-d480d67505b0b159151cd7af20a48a30.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-8b73be1c014002b4967b2bb77904a617.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-8073aa4d13173f4dffd7b839b1bf8e0f.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-4d202de85bf3896f2b47eae66bc243d4.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-862eb9dcccadc074ab5ed3fc598fe6e8.jpg" align="middle"></details><h2 id="Multimodal-LLM-Guided-Exploration-and-Active-Mapping-using-Fisher-Information"><a href="#Multimodal-LLM-Guided-Exploration-and-Active-Mapping-using-Fisher-Information" class="headerlink" title="Multimodal LLM Guided Exploration and Active Mapping using Fisher   Information"></a>Multimodal LLM Guided Exploration and Active Mapping using Fisher Information</h2><p><strong>Authors:Wen Jiang, Boshu Lei, Katrina Ashton, Kostas Daniilidis</strong></p><p>We present an active mapping system that could plan for long-horizon exploration goals and short-term actions with a 3D Gaussian Splatting (3DGS) representation. Existing methods either did not take advantage of recent developments in multimodal Large Language Models (LLM) or did not consider challenges in localization uncertainty, which is critical in embodied agents. We propose employing multimodal LLMs for long-horizon planning in conjunction with detailed motion planning using our information-based algorithm. By leveraging high-quality view synthesis from our 3DGS representation, our method employs a multimodal LLM as a zero-shot planner for long-horizon exploration goals from the semantic perspective. We also introduce an uncertainty-aware path proposal and selection algorithm that balances the dual objectives of maximizing the information gain for the environment while minimizing the cost of localization errors. Experiments conducted on the Gibson and Habitat-Matterport 3D datasets demonstrate state-of-the-art results of the proposed method.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.17422v2">PDF</a></p><p><strong>Summary</strong><br>基于3DGS表示的主动映射系统，利用多模态LLM进行长期目标规划，并平衡信息增益和定位误差成本。</p><p><strong>Key Takeaways</strong></p><ol><li>采用3DGS表示进行主动映射和长期目标规划。</li><li>结合多模态LLM进行长期目标语义规划。</li><li>利用信息算法进行详细运动规划。</li><li>3DGS提供高质量视图合成。</li><li>零样本规划用于长期探索。</li><li>引入不确定性感知路径提议和选择算法。</li><li>在Gibson和Habitat-Matterport数据集上实现最先进结果。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><strong>标题</strong>（含中文翻译）：主动映射系统：结合多模态大型语言模型和3D高斯融合表示进行长期规划探索</li></ol><p>中文翻译：Active Mapping System: Long-Horizon Planning Exploration with the Combination of Multimodal Large Language Models and 3D Gaussian Splatting Representation</p><ol><li><p><strong>作者名单</strong>（使用英文）：作者名单未提供。</p></li><li><p><strong>第一作者所属单位</strong>（中文翻译）：无信息。</p></li><li><p><strong>关键词</strong>（使用英文）：Active Mapping, Long-Horizon Planning, Multimodal Large Language Models, 3D Gaussian Splatting, Information-Based Path Planning, Uncertainty-Awareness。</p></li><li><p><strong>链接</strong>：论文链接未提供，GitHub代码链接（如有）：Github: None。</p></li><li><p><strong>摘要</strong>：</p><ul><li><p>(1) 研究背景：本文的研究背景是关于主动映射系统的长期规划探索，结合多模态大型语言模型和3D高斯融合表示方法，解决定位不确定性问题，提高环境探索效率。</p></li><li><p>(2) 过去的方法及问题：现有方法未充分利用多模态大型语言模型或未考虑定位不确定性的挑战。文章指出这些问题并进行了动机阐述。</p></li><li><p>(3) 研究方法：本文提出结合多模态大型语言模型进行长期规划，同时使用详细的运动规划算法。通过3D高斯融合表示的高质量视图合成，采用零射击方式为长期探索目标提供语义视角的规划。还引入了一个考虑定位误差成本和信息增益平衡的的不确定性感知路径提案和选择算法。</p></li><li><p>(4) 任务与性能：在Gibson和Habitat-Matterport 3D数据集上进行的实验证明了该方法在状态前沿的优异表现。文章的结果支持了他们方法的长期规划和高效探索的目标。</p></li></ul></li></ol><p>请注意，以上摘要中的内容是根据您提供的论文摘要和相关信息进行转写和总结的，原文中的具体细节和深入的内容需要在阅读原始论文后获得。</p><ol><li>方法：</li></ol><p>(1) 研究背景分析：该研究聚焦于主动映射系统的长期规划探索，针对现有方法的不足，结合多模态大型语言模型和3D高斯融合表示方法来解决定位不确定性问题，旨在提高环境探索效率。</p><p>(2) 方法论概述：研究采用多模态大型语言模型进行长期规划，结合详细的运动规划算法。通过3D高斯融合表示的高质量视图合成，采用零射击方式为长期探索目标提供语义视角的规划。同时，引入了考虑定位误差成本和信息增益平衡的的不确定性感知路径提案和选择算法。这些方法和技术的结合使用，使得系统能够在复杂的环境中进行高效的长期规划。</p><p>(3) 数据集实验：在Gibson和Habitat-Matterport 3D数据集上进行的实验验证了该方法的有效性。实验结果表明，该方法在状态前沿具有优异的表现，支持了其长期规划和高效探索的目标。</p><p>(4) 总结：该研究提出了一种结合多模态大型语言模型和3D高斯融合表示方法的主动映射系统，旨在解决长期规划中的定位不确定性问题。通过详细的运动规划算法和高质量视图合成，以及不确定性感知路径提案和选择算法的使用，该系统在复杂环境中表现出高效的长期规划能力。</p><ol><li>Conclusion:</li></ol><ul><li>(1) 工作意义：该研究关注主动映射系统的长期规划探索，解决了定位不确定性问题，提高了环境探索效率，对于智能机器人和自动化领域的长期发展具有重要意义。</li><li>(2) 优缺点总结：创新点方面，研究结合了多模态大型语言模型和3D高斯融合表示方法，为长期规划探索提供了新的思路和方法；性能上，实验证明该方法在状态前沿表现优异，有效实现了长期规划和高效探索的目标；工作量方面，研究涉及了多模态大型语言模型、3D高斯融合表示、运动规划算法、路径提案和选择算法等方面的内容，工作量较大，但结果具有实际应用价值。然而，文章未提供充分的细节和深入的内容，如GitHub代码链接等，对于完整评估其性能和工作量存在一定限制。</li></ul><p>综上所述，该文章在主动映射系统的长期规划探索方面取得了一定的进展，具有一定的创新性和实际应用价值。然而，由于缺少部分细节和深入内容，对于其全面评估存在一定困难。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-ff13bdb99d7102f6d037f9801c2d74c0.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-5f0b8ec723a6d80c205a438c142cf26e.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-b2bd33678afcf27edd36f7c95ae3dceb.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-42e53544ae20970bb2e2e214392ed5b5.jpg" align="middle"></details><h2 id="LUDVIG-Learning-free-Uplifting-of-2D-Visual-features-to-Gaussian-Splatting-scenes"><a href="#LUDVIG-Learning-free-Uplifting-of-2D-Visual-features-to-Gaussian-Splatting-scenes" class="headerlink" title="LUDVIG: Learning-free Uplifting of 2D Visual features to Gaussian   Splatting scenes"></a>LUDVIG: Learning-free Uplifting of 2D Visual features to Gaussian Splatting scenes</h2><p><strong>Authors:Juliette Marrie, Romain Menegaux, Michael Arbel, Diane Larlus, Julien Mairal</strong></p><p>We address the problem of extending the capabilities of vision foundation models such as DINO, SAM, and CLIP, to 3D tasks. Specifically, we introduce a novel method to uplift 2D image features into 3D Gaussian Splatting scenes. Unlike traditional approaches that rely on minimizing a reconstruction loss, our method employs a simpler and more efficient feature aggregation technique, augmented by a graph diffusion mechanism. Graph diffusion enriches features from a given model, such as CLIP, by leveraging pairwise similarities that encode 3D geometry or similarities induced by another embedding like DINOv2. Our approach achieves performance comparable to the state of the art on multiple downstream tasks while delivering significant speed-ups. Notably, we obtain competitive segmentation results using generic DINOv2 features, despite DINOv2 not being trained on millions of annotated segmentation masks like SAM. When applied to CLIP features, our method demonstrates strong performance in open-vocabulary, language-based object detection tasks, highlighting the versatility of our approach.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.14462v2">PDF</a></p><p><strong>Summary</strong><br>提出将二维图像特征提升至三维高斯分层场景的新方法，提升模型性能。</p><p><strong>Key Takeaways</strong></p><ol><li>提升DINO、SAM、CLIP等模型在3D任务上的能力。</li><li>创新地使用特征聚合技术提升2D图像至3D场景。</li><li>采用图扩散机制，增强模型特征。</li><li>获得与现有技术相当的性能，同时提高速度。</li><li>使用DINOv2特征实现与SAM相当的分割效果。</li><li>在CLIP特征上表现优异，特别是在开放式词汇物体检测任务中。</li><li>方法具有通用性和多任务处理能力。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p><strong>标题</strong>： LUDVIG: 利用无监督学习的二维视觉特征升维至高斯样条场景技术（LEARNING-FREE UPLIFTING OF 2D VISUAL FEATURES TO GAUSSIAN SPLATTING SCENES）</p></li><li><p><strong>作者</strong>： 朱丽叶·马丽（Juliette Marrie）、罗曼·梅纳克斯（Romain Menegaux）、迈克尔·阿尔贝尔（Michael Arbel）、黛安娜·拉鲁斯（Diane Larlus）、朱利安·梅拉尔（Julien Mairal）。</p></li><li><p><strong>作者隶属机构</strong>： 第一作者朱丽叶·马丽隶属于格勒诺布尔阿尔卑斯大学（Univ. Grenoble Alpes）、Inria、CNRS、Grenoble INP、LJK。</p></li><li><p><strong>关键词</strong>： 二维视觉特征、高斯样条场景、特征聚合、图扩散机制、场景理解、计算机视觉。</p></li><li><p><strong>链接</strong>： 由于无法直接提供链接，请通过学术搜索引擎查询相关论文。如有GitHub代码库，可通过论文中的链接或相关学术资源网站获取。GitHub链接：无。</p></li><li><p><strong>摘要</strong>：</p><ul><li>(1) 研究背景：本文研究如何将二维视觉特征扩展至三维任务，特别是在使用如DINO、SAM和CLIP等预训练模型时。文章探索了一种新颖的方法，将二维图像特征提升到三维高斯样条场景。</li><li>(2) 过去的方法与问题：大多数先前的方法依赖于优化过程，通过最小化重建损失来学习场景特定的三维表示。这种方法计算量大且效率不高。文章提出了一种更简单且高效的方法来解决这个问题。</li><li>(3) 研究方法：本文提出了一种无监督学习方法，通过特征聚合技术和图扩散机制来升维二维视觉特征至三维高斯样条场景。图扩散机制能够丰富给定模型（如CLIP）的特征，利用成对相似性来编码三维几何信息或特征嵌入的相似性。</li><li>(4) 任务与性能：文章在多个下游任务上验证了所提方法的性能，包括语义分割、语言引导的目标检索和场景编辑等。与现有技术相比，该方法实现了竞争性的性能，并且显著加速了计算过程。特别是，使用通用的DINOv2特征，即使在未经过数百万标注分割掩膜训练的情况下，仍能获得良好的分割结果。当应用于CLIP特征时，该方法在基于语言的开放词汇表目标检测任务中表现出强大的性能，凸显了方法的通用性。</li></ul></li></ol><p>希望以上内容符合您的要求！</p><ol><li><p>结论：</p><ul><li><p>(1)这篇工作的意义在于提出了一种新颖且高效的方法，将二维视觉特征扩展到三维场景任务，尤其是在使用预训练模型如DINO、SAM和CLIP时。它促进了计算机视觉领域中的场景理解，为相关任务提供了更有效的解决方案。</p></li><li><p>(2)创新点：本文提出了一个基于无监督学习的二维视觉特征升维方法，通过特征聚合技术和图扩散机制，将二维特征提升到三维高斯样条场景。该方法简化了计算过程，提高了效率，并在多个下游任务上实现了竞争性的性能。</p><p>性能：该文章在语义分割、语言引导的目标检索和场景编辑等任务上验证了所提方法的性能，并显示出强大的通用性。尤其是，使用通用的DINOv2特征时，即使在没有数百万标注分割掩膜训练的情况下，也能获得良好的分割结果。</p><p>工作量：文章进行了大量的实验来验证所提出方法的有效性，并在多个数据集上进行了评估。然而，关于该方法的实现细节和代码并未公开，这可能会限制其他研究者对其进行深入研究和验证。</p></li></ul></li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-d384c02bc5bb8f3cd5e857d0449747af.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-6e2d514ade6269c2983c2f12c1a69710.jpg" align="middle"></details></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io/2024/12/07/Paper/2024-12-07/3DGS/">https://kedreamix.github.io/2024/12/07/Paper/2024-12-07/3DGS/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/3DGS/">3DGS</a></div><div class="post_share"><div class="social-share" data-image="https://pica.zhimg.com/v2-f99336e9eaadb0ddb4c3dfffa1d84b60.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/12/07/Paper/2024-12-07/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-157b4d4c861c6793e5e81cb87637c2a2.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Diffusion Models</div></div></a></div><div class="next-post pull-right"><a href="/2024/12/07/Paper/2024-12-07/Talking%20Head%20Generation/" title="Talking Head Generation"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-1495909db6c3934be6b148d04c1c0a90.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Talking Head Generation</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/01/25/Paper/3DGS%20Survey/" title="3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-25</div><div class="title">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</div></div></a></div><div><a href="/2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" title="超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-01</div><div class="title">超赞的数字人生成知识库 Awesome-Talking-Head-Synthesis</div></div></a></div><div><a href="/2024/01/30/Paper/2024-01-30/3DGS/" title="3DGS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-2032721a60695f2d41ac96f75dec65a2.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-30</div><div class="title">3DGS</div></div></a></div><div><a href="/2024/01/24/Paper/2024-01-24/3DGS/" title="3DGS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-3d3dcd00c27bc3d320b23d4247ae79f3.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-24</div><div class="title">3DGS</div></div></a></div><div><a href="/2024/02/09/Paper/2024-02-09/3DGS/" title="3DGS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-28074a5f13fdf5a52c0d4de04dfb9406.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-09</div><div class="title">3DGS</div></div></a></div><div><a href="/2024/02/02/Paper/2024-02-02/3DGS/" title="3DGS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-e4e5570dfa99dfac9b297f7650c717c3.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-02</div><div class="title">3DGS</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-12-07-%E6%9B%B4%E6%96%B0"><span class="toc-text">2024-12-07 更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Turbo3D-Ultra-fast-Text-to-3D-Generation"><span class="toc-text">Turbo3D: Ultra-fast Text-to-3D Generation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#QUEEN-QUantized-Efficient-ENcoding-of-Dynamic-Gaussians-for-Streaming-Free-viewpoint-Videos"><span class="toc-text">QUEEN: QUantized Efficient ENcoding of Dynamic Gaussians for Streaming Free-viewpoint Videos</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Sparse-Voxels-Rasterization-Real-time-High-fidelity-Radiance-Field-Rendering"><span class="toc-text">Sparse Voxels Rasterization: Real-time High-fidelity Radiance Field Rendering</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Monocular-Dynamic-Gaussian-Splatting-is-Fast-and-Brittle-but-Smooth-Motion-Helps"><span class="toc-text">Monocular Dynamic Gaussian Splatting is Fast and Brittle but Smooth Motion Helps</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PBDyG-Position-Based-Dynamic-Gaussians-for-Motion-Aware-Clothed-Human-Avatars"><span class="toc-text">PBDyG: Position Based Dynamic Gaussians for Motion-Aware Clothed Human Avatars</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#EmbodiedOcc-Embodied-3D-Occupancy-Prediction-for-Vision-based-Online-Scene-Understanding"><span class="toc-text">EmbodiedOcc: Embodied 3D Occupancy Prediction for Vision-based Online Scene Understanding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#InfiniCube-Unbounded-and-Controllable-Dynamic-3D-Driving-Scene-Generation-with-World-Guided-Video-Models"><span class="toc-text">InfiniCube: Unbounded and Controllable Dynamic 3D Driving Scene Generation with World-Guided Video Models</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Multi-View-Pose-Agnostic-Change-Localization-with-Zero-Labels"><span class="toc-text">Multi-View Pose-Agnostic Change Localization with Zero Labels</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DGNS-Deformable-Gaussian-Splatting-and-Dynamic-Neural-Surface-for-Monocular-Dynamic-3D-Reconstruction"><span class="toc-text">DGNS: Deformable Gaussian Splatting and Dynamic Neural Surface for Monocular Dynamic 3D Reconstruction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HybridGS-Decoupling-Transients-and-Statics-with-2D-and-3D-Gaussian-Splatting"><span class="toc-text">HybridGS: Decoupling Transients and Statics with 2D and 3D Gaussian Splatting</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Gaussians-on-their-Way-Wasserstein-Constrained-4D-Gaussian-Splatting-with-State-Space-Modeling"><span class="toc-text">Gaussians on their Way: Wasserstein-Constrained 4D Gaussian Splatting with State-Space Modeling</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Multimodal-LLM-Guided-Exploration-and-Active-Mapping-using-Fisher-Information"><span class="toc-text">Multimodal LLM Guided Exploration and Active Mapping using Fisher Information</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LUDVIG-Learning-free-Uplifting-of-2D-Visual-features-to-Gaussian-Splatting-scenes"><span class="toc-text">LUDVIG: Learning-free Uplifting of 2D Visual features to Gaussian Splatting scenes</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image:url('https://pica.zhimg.com/v2-f99336e9eaadb0ddb4c3dfffa1d84b60.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js").then((()=>{pangu.autoSpacingPage()}))}function panguInit(){panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.1},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script></div><script id="canvas_nest" defer color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><script data-pjax>function butterfly_swiper_injector_config(){var a=document.getElementById("recent-posts");console.log("已挂载butterfly_swiper"),a.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/25/Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="2024/01/25/Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="2024/01/25/Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/17/Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023/12/17/Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="2023/12/17/Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script></body></html>