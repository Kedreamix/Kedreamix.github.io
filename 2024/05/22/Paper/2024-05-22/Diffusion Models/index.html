<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>Diffusion Models | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-05-22  Diffusion-RSCC Diffusion Probabilistic Model for Change Captioning in   Remote Sensing Images"><meta property="og:type" content="article"><meta property="og:title" content="Diffusion Models"><meta property="og:url" content="https://kedreamix.github.io/2024/05/22/Paper/2024-05-22/Diffusion%20Models/index.html"><meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World"><meta property="og:description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-05-22  Diffusion-RSCC Diffusion Probabilistic Model for Change Captioning in   Remote Sensing Images"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://picx.zhimg.com/v2-d222555f8f090be73c96a07a45af66c0.jpg"><meta property="article:published_time" content="2024-05-22T04:21:50.000Z"><meta property="article:modified_time" content="2024-05-22T04:21:50.550Z"><meta property="article:author" content="Kedreamix"><meta property="article:tag" content="Diffusion Models"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://picx.zhimg.com/v2-d222555f8f090be73c96a07a45af66c0.jpg"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/2024/05/22/Paper/2024-05-22/Diffusion%20Models/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-74LZ5BEQQ1")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!0,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"简"},noticeOutdate:{limitDay:500,position:"top",messagePrev:"It has been",messageNext:"days since the last update, the content of the article may be outdated."},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:200},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!0,post:!0},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!0,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"Diffusion Models",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-05-22 12:21:50"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach((e=>{n.setAttribute(e,t[e])})),document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)})),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=24?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 7.0.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml"></head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{const d=document.getElementById("loading-box"),e=document.body,o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l(),window.addEventListener("load",(()=>{o()}))})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">216</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url('https://picx.zhimg.com/v2-d222555f8f090be73c96a07a45af66c0.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Diffusion Models</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-05-22T04:21:50.000Z" title="发表于 2024-05-22 12:21:50">2024-05-22</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-05-22T04:21:50.550Z" title="更新于 2024-05-22 12:21:50">2024-05-22</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">10.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>38分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="Diffusion Models"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-05-22-更新"><a href="#2024-05-22-更新" class="headerlink" title="2024-05-22 更新"></a>2024-05-22 更新</h1><h2 id="Diffusion-RSCC-Diffusion-Probabilistic-Model-for-Change-Captioning-in-Remote-Sensing-Images"><a href="#Diffusion-RSCC-Diffusion-Probabilistic-Model-for-Change-Captioning-in-Remote-Sensing-Images" class="headerlink" title="Diffusion-RSCC: Diffusion Probabilistic Model for Change Captioning in   Remote Sensing Images"></a>Diffusion-RSCC: Diffusion Probabilistic Model for Change Captioning in Remote Sensing Images</h2><p><strong>Authors:Xiaofei Yu, Yitong Li, Jie Ma</strong></p><p>Remote sensing image change captioning (RSICC) aims at generating human-like language to describe the semantic changes between bi-temporal remote sensing image pairs. It provides valuable insights into environmental dynamics and land management. Unlike conventional change captioning task, RSICC involves not only retrieving relevant information across different modalities and generating fluent captions, but also mitigating the impact of pixel-level differences on terrain change localization. The pixel problem due to long time span decreases the accuracy of generated caption. Inspired by the remarkable generative power of diffusion model, we propose a probabilistic diffusion model for RSICC to solve the aforementioned problems. In training process, we construct a noise predictor conditioned on cross modal features to learn the distribution from the real caption distribution to the standard Gaussian distribution under the Markov chain. Meanwhile, a cross-mode fusion and a stacking self-attention module are designed for noise predictor in the reverse process. In testing phase, the well-trained noise predictor helps to estimate the mean value of the distribution and generate change captions step by step. Extensive experiments on the LEVIR-CC dataset demonstrate the effectiveness of our Diffusion-RSCC and its individual components. The quantitative results showcase superior performance over existing methods across both traditional and newly augmented metrics. The code and materials will be available online at <a target="_blank" rel="noopener" href="https://github.com/Fay-Y/Diffusion-RSCC">https://github.com/Fay-Y/Diffusion-RSCC</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.12875v1">PDF</a></p><p><strong>Summary</strong><br>扩散模型应用于遥感图像变化描述，有效减轻像素差异对地形变化定位的影响，提高描述精度。</p><p><strong>Key Takeaways</strong></p><ul><li>遥感图像变化描述旨在生成人类可理解的自然语言描述，以解释双时相遥感图像对之间的语义变化。</li><li>遥感图像变化描述不仅涉及跨模态相关信息的提取和流畅描述的生成，还需减轻像素级差异对地形变化定位的影响。</li><li>时间跨度长的像素问题会降低生成描述的准确度。</li><li>扩散模型具有杰出的生成能力，可用于遥感图像变化描述，解决上述问题。</li><li>在训练过程中，构建噪声预测器以学习从真实描述分布到标准高斯分布的分布。</li><li>在推理阶段，训练好的噪声预测器有助于估计分布的均值并逐步生成变化描述。</li><li>在 LEVIR-CC 数据集上的广泛实验表明了扩散模型在遥感图像变化描述中的有效性。</li><li>该方法在传统和新增加的指标上都优于现有方法。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>题目：基于扩散模型的遥感图像变化描述</p></li><li><p>作者：Xiaofei Yu, Yitong Li, Jie Ma</p></li><li><p>第一作者单位：北京外国语大学信息科学与技术学院</p></li><li><p>关键词：遥感，扩散模型，变化描述，注意力机制</p></li><li><p>论文链接：https://arxiv.org/abs/2302.07736, Github：None</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：遥感图像变化描述（RSICC）旨在生成类似人类语言的句子来描述双时相遥感图像对之间的语义变化。与传统的变化描述任务不同，RSICC 不仅涉及跨不同模态检索相关信息并生成流畅的描述，还要减轻像素级差异对地形变化定位的影响。</p><p>（2）：过去方法及问题：现有的 RSICC 方法通常采用编码器-解码器结构，但它们难以区分语义变化和伪变化，从而影响描述的准确性。</p><p>（3）：本文方法：本文提出了一种基于扩散模型的 RSICC 方法。该方法构造了一个条件为交叉模态特征的噪声预测器，学习从真实描述分布到马尔可夫链下的标准高斯分布的分布。同时，在逆过程中设计了一个跨模态融合和一个堆叠自注意力模块用于噪声预测器。</p><p>（4）：实验结果：在 LEVIR-CC 数据集上的广泛实验表明，本文方法在传统和新增加的指标上都优于现有方法。这些结果支持了本文方法区分语义变化和伪变化的能力，从而提高了描述的准确性。</p><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：</li></ol><p>（1）：本文工作的主要意义在于：提出了一个基于扩散模型的遥感图像变化描述方法，该方法通过构建条件为交叉模态特征的噪声预测器，学习从真实描述分布到马尔可夫链下的标准高斯分布的分布，并设计了跨模态融合和堆叠自注意力模块，有效区分语义变化和伪变化，提高了描述的准确性。</p><p>（2）：本文的优点和不足总结如下：</p><p>创新点： - 提出了一种基于扩散模型的遥感图像变化描述方法，该方法通过构建条件为交叉模态特征的噪声预测器，学习从真实描述分布到马尔可夫链下的标准高斯分布的分布，有效区分语义变化和伪变化，提高了描述的准确性。 - 设计了跨模态融合和堆叠自注意力模块，进一步增强了模型的语义理解能力和变化定位能力。</p><p>性能： - 在 LEVIR-CC 数据集上的广泛实验表明，本文方法在传统和新增加的指标上都优于现有方法，验证了其有效性。</p><p>工作量： - 本文方法需要较大的训练数据量和较长的训练时间。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-21dbd52d9fa2dfab9ed21bd713132601.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-ea4cb0070ada153d3948236792884ccd.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-f43b384f7a1cf699952513394080a478.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-559ce1394523d55dae45d360bd3b2838.jpg" align="middle"></details>## Diffusion for World Modeling: Visual Details Matter in Atari **Authors:Eloi Alonso, Adam Jelley, Vincent Micheli, Anssi Kanervisto, Amos Storkey, Tim Pearce, François Fleuret** World models constitute a promising approach for training reinforcement learning agents in a safe and sample-efficient manner. Recent world models predominantly operate on sequences of discrete latent variables to model environment dynamics. However, this compression into a compact discrete representation may ignore visual details that are important for reinforcement learning. Concurrently, diffusion models have become a dominant approach for image generation, challenging well-established methods modeling discrete latents. Motivated by this paradigm shift, we introduce DIAMOND (DIffusion As a Model Of eNvironment Dreams), a reinforcement learning agent trained in a diffusion world model. We analyze the key design choices that are required to make diffusion suitable for world modeling, and demonstrate how improved visual details can lead to improved agent performance. DIAMOND achieves a mean human normalized score of 1.46 on the competitive Atari 100k benchmark; a new best for agents trained entirely within a world model. To foster future research on diffusion for world modeling, we release our code, agents and playable world models at https://github.com/eloialonso/diamond. [PDF](http://arxiv.org/abs/2405.12399v1) 25 pages, 11 figures, 10 tables **Summary** 扩散模型的视觉细节提升可改善世界模型中强化学习代理的性能。 **Key Takeaways** - 世界模型为强化学习代理提供了一种安全、高效的训练方法。 - 扩散模型在图像生成领域取得了巨大成功。 - DIAMOND（DIffusion As a Model Of eNvironment Dreams）是第一个在扩散世界模型中训练的强化学习代理。 - DIAMOND在Atari 100k基准上达到1.46的人类归一化平均得分。 - 扩散模型可以捕获对强化学习重要的视觉细节。 - DIAMOND代码、代理和可玩世界模型已开源。 - 扩散模型在世界建模领域具有巨大的潜力。 **[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: 世界建模的扩散：Atari 中的视觉细节至关重要</p></li><li><p>Authors: Eloi Alonso, Adam Jelley, Vincent Micheli, Anssi Kanervisto, Amos Storkey, Tim Pearce, François Fleuret</p></li><li><p>Affiliation: 日内瓦大学</p></li><li><p>Keywords: Diffusion, World Modeling, Reinforcement Learning, Atari</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2405.12399, Github: https://github.com/eloialonso/diamond</p></li><li><p>Summary:</p><pre><code>            (1): 世界模型是一种有前途的方法，可用于以安全且样本高效的方式训练强化学习智能体。最近的世界模型主要对离散潜在变量序列进行操作以建模环境动态。然而，这种压缩成紧凑的离散表示可能会忽略对强化学习很重要的视觉细节。与此同时，扩散模型已成为图像生成的主导方法，挑战了对离散潜在变量建模的成熟方法。受这种范式转变的启发，我们引入了 DIAMOND（DIffusion As a Model Of eNvironment Dreams），一种在扩散世界模型中训练的强化学习智能体。我们分析了使扩散适用于世界建模所需的关键设计选择，并展示了改进的视觉细节如何提高智能体性能。DIAMOND 在具有竞争力的 Atari 100k 基准测试中获得了 1.46 的平均人类归一化分数；这是在世界模型中完全训练的智能体的最新成绩。为了促进未来对世界建模扩散的研究，我们在 https://github.com/eloialonso/diamond 上发布了我们的代码、智能体和可玩世界模型。

            (2): 最近的世界建模方法通常将环境动态建模为离散潜在变量序列。潜在空间的离散化有助于避免在多步时间范围内累积误差。然而，这种编码可能会丢失信息，从而导致泛化性和重建质量下降。这对于信息要求不太明确的更真实场景可能存在问题，例如训练自动驾驶汽车（Autonomous Vehicles）。在这种情况下，视觉输入中的小细节，例如远处的交通灯或行人，可能会改变智能体的策略。增加离散潜在变量的数量可以减轻这种有损压缩，但会增加计算成本。

            (3): 本文提出的研究方法是：分析使扩散适合世界建模所需的关键设计选择，并展示了改进的视觉细节如何提高智能体性能。

            (4): 在 Atari 100k 基准测试任务上，DIAMOND 取得了 1.46 的平均人类归一化分数。该性能支持了他们在世界模型中完全训练智能体的目标。
</code></pre></li><li><p>Methods:</p><pre><code>            (1):本文提出了一种名为 DIAMOND 的强化学习智能体，该智能体在扩散世界模型中进行训练。

            (2):DIAMOND 使用扩散模型来对环境动态进行建模，而不是离散潜在变量序列。

            (3):作者分析了使扩散适用于世界建模所需的关键设计选择，并展示了改进的视觉细节如何提高智能体性能。

            (4):DIAMOND 在 Atari 100k 基准测试任务上取得了 1.46 的平均人类归一化分数，这表明了扩散世界模型在强化学习中的潜力。
</code></pre></li><li><p>结论：</p><pre><code>            (1):本文提出了一种名为 DIAMOND 的强化学习智能体，该智能体在扩散世界模型中进行训练。DIAMOND 在 Atari 100k 基准测试任务上取得了 1.46 的平均人类归一化分数，这表明了扩散世界模型在强化学习中的潜力。

            (2):Innovation point: 本文提出了使用扩散模型对环境动态进行建模的方法，而不是离散潜在变量序列。 Performance: DIAMOND 在 Atari 100k 基准测试任务上取得了 1.46 的平均人类归一化分数，这表明了扩散世界模型在强化学习中的潜力。 Workload: DIAMOND 的训练成本高于使用离散潜在变量序列的世界模型的训练成本。
</code></pre></li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-72ac1259074913dc48248601ecb6050f.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-3f4d7aa4fb02351e901a1debcb4d39d9.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-3025a3d3200ab1611ab31f0968676023.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-d222555f8f090be73c96a07a45af66c0.jpg" align="middle"></details><h2 id="Images-that-Sound-Composing-Images-and-Sounds-on-a-Single-Canvas"><a href="#Images-that-Sound-Composing-Images-and-Sounds-on-a-Single-Canvas" class="headerlink" title="Images that Sound: Composing Images and Sounds on a Single Canvas"></a>Images that Sound: Composing Images and Sounds on a Single Canvas</h2><p><strong>Authors:Ziyang Chen, Daniel Geng, Andrew Owens</strong></p><p>Spectrograms are 2D representations of sound that look very different from the images found in our visual world. And natural images, when played as spectrograms, make unnatural sounds. In this paper, we show that it is possible to synthesize spectrograms that simultaneously look like natural images and sound like natural audio. We call these spectrograms images that sound. Our approach is simple and zero-shot, and it leverages pre-trained text-to-image and text-to-spectrogram diffusion models that operate in a shared latent space. During the reverse process, we denoise noisy latents with both the audio and image diffusion models in parallel, resulting in a sample that is likely under both models. Through quantitative evaluations and perceptual studies, we find that our method successfully generates spectrograms that align with a desired audio prompt while also taking the visual appearance of a desired image prompt. Please see our project page for video results: <a target="_blank" rel="noopener" href="https://ificl.github.io/images-that-sound/">https://ificl.github.io/images-that-sound/</a></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.12221v1">PDF</a> Project site: <a target="_blank" rel="noopener" href="https://ificl.github.io/images-that-sound/">https://ificl.github.io/images-that-sound/</a></p><p><strong>Summary</strong><br>自然图像的声谱图既能展现逼真的视觉效果，又能产生自然的声音。</p><p><strong>Key Takeaways</strong></p><ul><li>声谱图是声音的二维表示，其外观与我们视觉世界中的图像截然不同。</li><li>自然图像作为声谱图播放时，会产生不自然的声音。</li><li>本研究合成出同时具有自然图像外观和自然音频声音的声谱图，称为“可视化声音”。</li><li>该方法采用零样本学习，利用共享潜在空间中的预训练文本到图像和文本到声谱图扩散模型。</li><li>逆向过程中，通过音频和图像扩散模型并行对噪声潜在变量进行去噪，生成满足两个模型要求的样本。</li><li>定量评估和感知研究表明，该方法成功生成了与目标音频提示一致、同时具有目标图像提示视觉外观的声谱图。</li><li>更详细的研究结果请见项目主页：<a target="_blank" rel="noopener" href="https://ificl.github.io/images-that-sound/">https://ificl.github.io/images-that-sound/</a></li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>标题：图像即声音：在单一画布上合成图像和声音</p></li><li><p>作者：Ziyang Chen, Daniel Geng, Andrew Owens</p></li><li><p>隶属单位：密歇根大学</p></li><li><p>关键词：图像到声音，扩散模型，零样本学习</p></li><li><p>论文链接：https://ificl.github.io/images-that-sound/，Github 代码链接：None</p></li><li><p>摘要：</p><p>（1）：研究背景：声谱图是声音的二维表示，与我们视觉世界中的图像看起来非常不同。当自然图像以声谱图的形式播放时，会产生不自然的声音。</p><p>（2）：过去的方法和问题：以往的方法无法同时生成既像自然图像又像自然音频的声谱图。</p><p>（3）：本文提出的研究方法：本文提出了一种简单且零样本的方法，利用预训练的文本到图像和文本到声谱图扩散模型，在共享潜在空间中工作。在反向过程中，使用音频和图像扩散模型并行对噪声潜变量进行去噪，从而得到一个同时符合这两个模型的样本。</p><p>（4）：方法的性能：通过定量评估和感知研究，本文的方法成功生成了与所需音频提示一致，同时具有所需图像提示视觉外观的声谱图。</p></li><li><p>方法：</p><p>（1）：利用预训练的文本到图像和文本到声谱图扩散模型，在共享潜在空间中工作；</p><p>（2）：在反向过程中，使用音频和图像扩散模型并行对噪声潜变量进行去噪；</p><p>（3）：得到一个同时符合这两个模型的样本。</p></li><li><p>结论：</p><pre><code>            （1）：本工作表明，自然图像的分布与自然声谱图的分布之间存在非平凡的重叠。我们通过从这两个分布的交集中进行采样来证明这一点，从而得到看起来像真实图像但听起来像自然声音的声谱图。我们注意到，由于声码器本质上是有损的，因此通常无法实现完美的循环一致性。

            （2）：创新点：提出了一个简单且零样本的方法，利用预训练的文本到图像和文本到声谱图扩散模型，在共享潜在空间中工作，在反向过程中并行对噪声潜变量进行去噪，得到一个同时符合这两个模型的样本；性能：通过定量评估和感知研究，本文的方法成功生成了与所需音频提示一致，同时具有所需图像提示视觉外观的声谱图；工作量：该方法简单易用，不需要额外的训练数据或模型。
</code></pre></li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-44e9096dfe8b1eb6e7cbea03451f9e61.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-3f85b8a4d2b38d0e0dd599904b6101cd.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-030148a4e48570d9fe061e8cc613146d.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-8f3517b1b23ac9838a5e3355e6bbc727.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-01ecd2fc03770b0401757015953e2d0a.jpg" align="middle"></details><h2 id="Slicedit-Zero-Shot-Video-Editing-With-Text-to-Image-Diffusion-Models-Using-Spatio-Temporal-Slices"><a href="#Slicedit-Zero-Shot-Video-Editing-With-Text-to-Image-Diffusion-Models-Using-Spatio-Temporal-Slices" class="headerlink" title="Slicedit: Zero-Shot Video Editing With Text-to-Image Diffusion Models   Using Spatio-Temporal Slices"></a>Slicedit: Zero-Shot Video Editing With Text-to-Image Diffusion Models Using Spatio-Temporal Slices</h2><p><strong>Authors:Nathaniel Cohen, Vladimir Kulikov, Matan Kleiner, Inbar Huberman-Spiegelglas, Tomer Michaeli</strong></p><p>Text-to-image (T2I) diffusion models achieve state-of-the-art results in image synthesis and editing. However, leveraging such pretrained models for video editing is considered a major challenge. Many existing works attempt to enforce temporal consistency in the edited video through explicit correspondence mechanisms, either in pixel space or between deep features. These methods, however, struggle with strong nonrigid motion. In this paper, we introduce a fundamentally different approach, which is based on the observation that spatiotemporal slices of natural videos exhibit similar characteristics to natural images. Thus, the same T2I diffusion model that is normally used only as a prior on video frames, can also serve as a strong prior for enhancing temporal consistency by applying it on spatiotemporal slices. Based on this observation, we present Slicedit, a method for text-based video editing that utilizes a pretrained T2I diffusion model to process both spatial and spatiotemporal slices. Our method generates videos that retain the structure and motion of the original video while adhering to the target text. Through extensive experiments, we demonstrate Slicedit’s ability to edit a wide range of real-world videos, confirming its clear advantages compared to existing competing methods. Webpage: <a target="_blank" rel="noopener" href="https://matankleiner.github.io/slicedit/">https://matankleiner.github.io/slicedit/</a></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.12211v1">PDF</a> ICML 2024. Code and examples are available at <a target="_blank" rel="noopener" href="https://matankleiner.github.io/slicedit/">https://matankleiner.github.io/slicedit/</a></p><p><strong>Summary</strong><br>基于自然视频的时空切片与真实图像具有相似的特性，可利用预训练的 T2I 扩散模型对其进行处理，增强视频编辑中的时间一致性</p><p><strong>Key Takeaways</strong></p><ul><li>利用预训练的 T2I 扩散模型来增强时空一致性</li><li>Slicedit 方法同时处理空间和时空切片</li><li>生成视频保留原始视频的结构和运动，同时符合目标文本</li><li>在广泛实验中，证明 Slicedit 能够编辑各种真实视频</li><li>明显优于现有的竞争方法</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: Slicedit：基于文本到图像扩散模型和时空切片的零样本视频编辑</p></li><li><p>Authors: Nathaniel Cohen, Vladimir Kulikov, Matan Kleiner, Inbar Huberman-Spiegelglas, Tomer Michaeli</p></li><li><p>Affiliation: 巴黎矿业-PSL研究大学</p></li><li><p>Keywords: 文本到图像, 视频编辑, 扩散模型, 时空切片</p></li><li><p>Urls:</p></li><li><p>Summary:</p><p>(1): 文本到图像（T2I）扩散模型在图像合成和编辑中取得了最先进的结果。然而，将这些预训练模型用于视频编辑被认为是一个重大挑战。许多现有工作试图通过像素空间或深度特征之间的显式对应机制来增强编辑视频中的时间一致性。然而，这些方法难以处理强烈的非刚性运动。</p><p>(2): 本文提出了一种从根本上不同的方法，该方法基于以下观察：自然视频的时空切片表现出与自然图像相似的特征。因此，通常仅用作视频帧先验的相同 T2I 扩散模型也可以通过在时空切片上应用它来作为增强时间一致性的强先验。</p><p>(3): 基于这一观察，我们提出了 Slicedit，这是一种基于文本的视频编辑方法，它利用预训练的 T2I 扩散模型处理空间和时空切片。我们的方法生成的视频保留了原始视频的结构和运动，同时遵循目标文本。</p><p>(4): 通过广泛的实验，我们证明了 Slicedit 编辑各种真实世界视频的能力，证实了其与现有竞争方法相比的明显优势。</p></li><li><p>方法：</p><p>（1）：提出 Slicedit，这是一种基于文本的视频编辑方法，它利用预训练的 Text-to-Image（T2I）扩散模型处理空间和时空切片。</p><p>（2）：该方法将时空切片作为增强时间一致性的强先验，通过在时空切片上应用 T2I 扩散模型来生成视频。</p><p>（3）：Slicedit 编辑视频时保留了原始视频的结构和运动，同时遵循目标文本。</p></li><li><p>结论：</p></li></ol><p>（1）：本文提出了一种基于文本的零样本视频编辑方法 Slicedit，该方法利用预训练的文本到图像扩散模型。我们的方法对模型进行了修改，使其能够处理视频。最重要的是，它将最初设计用于图像的预训练去噪器也应用于视频的时空切片。为了编辑视频，我们在 DDPM 反演过程中使用我们膨胀的去噪器，同时将源视频的扩展注意力注入目标视频。我们的方法优于现有技术，在编辑视频时保留了未指定区域，同时不影响时间一致性。我们通过测量编辑保真度、结构保留和时间一致性指标对其进行了评估，并辅以用户研究。虽然我们的方法在保留输入视频的结构方面表现出色，但它在全局编辑任务中遇到了挑战，例如将自然视频的帧转换为绘画。此外，我们的方法仅限于保留结构的编辑。这是由于使用了带有注意力注入的 DDPM 反演。图 11 中显示了一个示例失败案例。</p><p>（2）：创新点：提出了一种基于文本的零样本视频编辑方法，该方法利用预训练的文本到图像扩散模型，并将其应用于视频的时空切片以增强时间一致性。 性能：我们的方法在编辑保真度、结构保留和时间一致性方面优于现有技术。 工作量：我们的方法需要预训练文本到图像扩散模型，并且编辑过程可能需要大量计算。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-7ad40d7ffd4fdfec179a13d80066e3bf.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-9fc0922570bcd1ad99da98532754eebb.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-db1b0305aeb4fd36b0e3253f5b88f485.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-7730aa9df76f69b4353b0e3ce05aaa74.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-e869c60df6712ebe7f060fa84c38f40e.jpg" align="middle"></details>## Evolving Storytelling: Benchmarks and Methods for New Character Customization with Diffusion Models **Authors:Xiyu Wang, Yufei Wang, Satoshi Tsutsui, Weisi Lin, Bihan Wen, Alex C. Kot** Diffusion-based models for story visualization have shown promise in generating content-coherent images for storytelling tasks. However, how to effectively integrate new characters into existing narratives while maintaining character consistency remains an open problem, particularly with limited data. Two major limitations hinder the progress: (1) the absence of a suitable benchmark due to potential character leakage and inconsistent text labeling, and (2) the challenge of distinguishing between new and old characters, leading to ambiguous results. To address these challenges, we introduce the NewEpisode benchmark, comprising refined datasets designed to evaluate generative models' adaptability in generating new stories with fresh characters using just a single example story. The refined dataset involves refined text prompts and eliminates character leakage. Additionally, to mitigate the character confusion of generated results, we propose EpicEvo, a method that customizes a diffusion-based visual story generation model with a single story featuring the new characters seamlessly integrating them into established character dynamics. EpicEvo introduces a novel adversarial character alignment module to align the generated images progressively in the diffusive process, with exemplar images of new characters, while applying knowledge distillation to prevent forgetting of characters and background details. Our evaluation quantitatively demonstrates that EpicEvo outperforms existing baselines on the NewEpisode benchmark, and qualitative studies confirm its superior customization of visual story generation in diffusion models. In summary, EpicEvo provides an effective way to incorporate new characters using only one example story, unlocking new possibilities for applications such as serialized cartoons. [PDF](http://arxiv.org/abs/2405.11852v1) **Summary** 扩散模型中引入新角色时，定制化方法EpicEvo可有效解决角色一致性问题，通过单个故事范例实现无缝整合。 **Key Takeaways** - NewEpisode基准建立，用于评估扩散生成模型在仅使用单一示例故事的情况下，生成具有新角色的内容连贯图像。 - 精炼数据集，消除字符泄露和文本标签不一致的问题。 - EpicEvo方法，通过单一故事定制基于扩散的可视化故事生成模型，无缝整合新角色。 - 加入对抗性字符对齐模块，在扩散过程中将生成图像与新角色示例图像对齐。 - 运用知识蒸馏，防止遗忘角色和背景细节。 - 评估结果表明，EpicEvo在NewEpisode基准上优于现有基线。 - EpicEvo可有效整合新角色，仅需单个示例故事，为连载漫画等应用开辟新可能。 **[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: 可演化的故事生成：用于新角色自定义的基准和方法</p></li><li><p>Authors: Xiyu Wang, Yufei Wang, Satoshi Tsutsui, Weisi Lin, Bihan Wen, Alex C. Kot</p></li><li><p>Affiliation: 南洋理工大学</p></li><li><p>Keywords: Generative Diffusion Model, Story Visualization, Generative Model Customization</p></li><li><p>Urls: Paper: https://arxiv.org/pdf/2405.11852.pdf , Github:None</p></li><li><p>Summary:</p><p>(1): 基于扩散的模型在故事可视化中展示了生成内容连贯图像的潜力。然而，如何在保持角色一致性的同时有效地将新角色融入现有叙事中仍然是一个难题，特别是在数据有限的情况下。有两个主要限制阻碍了进展：(1) 由于潜在的角色泄露和不一致的文本标记，缺少合适的基准；(2) 区分新角色和旧角色的挑战，导致结果模棱两可。</p><p>(2): 过去的方法包括：使用预训练的文本到图像生成模型来生成故事可视化。然而，这些方法存在以下问题：1）缺乏合适的基准来评估生成模型生成具有新角色的新故事的适应性。2）难以区分新角色和旧角色，导致生成结果模棱两可。</p><p>(3): 本文提出的研究方法包括：1）引入 NewEpisode 基准，该基准包含经过改进的数据集，旨在使用单个示例故事评估生成模型生成具有新角色的新故事的适应性。2）提出 EpicEvo，这是一种使用具有新角色的单个故事来自定义基于扩散的视觉故事生成模型的方法，将新角色无缝集成到既定的角色动态中。</p><p>(4): 本文方法在 NewEpisode 基准上取得了以下性能：1）定量评估表明，EpicEvo 在 NewEpisode 基准上优于现有的基线。2）定性研究证实了 EpicEvo 在扩散模型中对视觉故事生成的卓越定制。这些性能支持了本文的目标，即提供一种仅使用一个示例故事就能融合新角色的有效方法，为连载漫画等应用解锁了新的可能性。</p></li><li><p>方法：</p><p>(1): 提出 NewEpisode 基准，该基准包含经过改进的数据集，旨在使用单个示例故事评估生成模型生成具有新角色的新故事的适应性；</p><p>(2): 提出了 EpicEvo，这是一种使用具有新角色的单个故事来自定义基于扩散的视觉故事生成模型的方法，将新角色无缝集成到既定的角色动态中；</p><p>(3): 在 NewEpisode 基准上对 EpicEvo 进行了定量和定性评估，结果表明 EpicEvo 在生成具有新角色的新故事的适应性方面优于现有的基线，并且能够在扩散模型中对视觉故事生成进行卓越的定制。</p></li><li><p>结论：</p></li></ol><p>（1）：本文解决了故事角色定制的难题，提出 NewEpisode 基准和 EpicEvo 方法，使视觉故事生成模型能够生成从未见过的角色的新故事，为连载漫画等应用解锁了新的可能性。</p><p>（2）：创新点：提出 NewEpisode 基准和 EpicEvo 方法；性能：在 NewEpisode 基准上优于现有的基线，能够在扩散模型中对视觉故事生成进行卓越的定制；工作量：需要收集和整理 NewEpisode 基准数据，训练 EpicEvo 模型。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-1bcf790e86915883bf4c5491f4af0617.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-b7e49809f39744c919340eadd0a23302.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-7829327de314711f1e323c58084208a2.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-aea383233d966c6afec2db0d88be118b.jpg" align="middle"></details>## ViViD: Video Virtual Try-on using Diffusion Models **Authors:Zixun Fang, Wei Zhai, Aimin Su, Hongliang Song, Kai Zhu, Mao Wang, Yu Chen, Zhiheng Liu, Yang Cao, Zheng-Jun Zha** Video virtual try-on aims to transfer a clothing item onto the video of a target person. Directly applying the technique of image-based try-on to the video domain in a frame-wise manner will cause temporal-inconsistent outcomes while previous video-based try-on solutions can only generate low visual quality and blurring results. In this work, we present ViViD, a novel framework employing powerful diffusion models to tackle the task of video virtual try-on. Specifically, we design the Garment Encoder to extract fine-grained clothing semantic features, guiding the model to capture garment details and inject them into the target video through the proposed attention feature fusion mechanism. To ensure spatial-temporal consistency, we introduce a lightweight Pose Encoder to encode pose signals, enabling the model to learn the interactions between clothing and human posture and insert hierarchical Temporal Modules into the text-to-image stable diffusion model for more coherent and lifelike video synthesis. Furthermore, we collect a new dataset, which is the largest, with the most diverse types of garments and the highest resolution for the task of video virtual try-on to date. Extensive experiments demonstrate that our approach is able to yield satisfactory video try-on results. The dataset, codes, and weights will be publicly available. Project page: https://becauseimbatman0.github.io/ViViD. [PDF](http://arxiv.org/abs/2405.11794v1) **Summary** 视频虚拟试穿通过扩散模型实现服装在视频人体上的试穿，该框架包含服装编码器、姿态编码器和时间模块，并收集了用于视频虚拟试穿任务的最大、最具多样性服装类型和最高分辨率的新数据集。 **Key Takeaways** - 视频虚拟试穿将服装转移到目标人物视频上，但逐帧应用图像试穿会导致时间不一致。 - ViViD 框架使用扩散模型来解决视频虚拟试穿任务。 - 服装编码器提取服装语义特征，用于捕获服装细节并通过注意力特征融合机制注入目标视频。 - 姿势编码器编码姿势信号，使模型学习服装与人体姿势之间的交互。 - 文本到图像稳定的扩散模型中加入层次化时间模块，实现更连贯、逼真的视频合成。 - ViViD 收集了迄今为止用于视频虚拟试穿任务的最大、最具多样性服装类型和最高分辨率的新数据集。 - 实验表明，ViViD 能够产生令人满意的视频试穿结果。 - 数据集、代码和权重将公开。 **[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: ViViD: 使用扩散模型的视频虚拟试穿</p></li><li><p>Authors: Zixun Fang, Wei Zhai, Aimin Su, Hongliang Song, Kai Zhu, Mao Wang, Yu Chen, Zhiheng Liu, Yang Cao, Zheng-Jun Zha</p></li><li><p>Affiliation: 中国科学技术大学</p></li><li><p>Keywords: Video virtual try-on, Diffusion models, Pose encoding, Temporal consistency</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2405.11794, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 视频虚拟试穿旨在将一件衣服转移到目标人物的视频上。将基于图像的试穿技术逐帧应用于视频领域会导致时间不一致的结果，而之前的基于视频的试穿解决方案只能生成低视觉质量和模糊的结果。</p><p>(2): 过去的基于图像的虚拟试穿方法无法直接应用于视频，因为这会导致灾难性的结果。基于视频的试穿解决方案虽然可以解决时间一致性问题，但它们通常会产生低视觉质量和模糊的结果。</p><p>(3): 本文提出了 ViViD，一个使用强大的扩散模型来解决视频虚拟试穿任务的新框架。ViViD 包含一个服装编码器，用于提取细粒度的服装语义特征，指导模型捕捉服装细节并通过提出的注意力特征融合机制将其注入目标视频中。为了确保时空一致性，ViViD 引入了一个轻量级的姿势编码器来编码姿势信号，使模型能够学习服装和人体姿势之间的相互作用，并将分层的 Temporal 模块插入到文本到图像的稳定扩散模型中以实现更连贯和逼真的视频合成。此外，ViViD 还收集了一个新的数据集，这是迄今为止用于视频虚拟试穿任务的最大、服装类型最多、分辨率最高的数据集。</p><p>(4): 实验表明，ViViD 能够产生令人满意的视频试穿结果。在 ViViD 数据集上，ViViD 在 FID 和 LPIPS 度量方面优于最先进的方法。这些结果支持了 ViViD 在视频虚拟试穿任务中的有效性。</p><ol><li><p>方法：</p><pre><code>            (1):该方法将视频虚拟试穿任务视为视频修复问题，将服装粘贴到与服装无关的区域；

            (2):提出服装编码器提取服装语义特征，通过注意力特征融合机制注入目标视频中；

            (3):引入轻量级姿势编码器编码姿势信号，使模型学习服装和人体姿势之间的相互作用；

            (4):在文本到图像的稳定扩散模型中插入分层的 Temporal 模块，实现更连贯、逼真的视频合成；

            (5):收集新数据集 ViViD，包含迄今为止用于视频虚拟试穿任务的最大、服装类型最多、分辨率最高的视频数据。
</code></pre></li><li><p>结论：</p><pre><code>            (1): 本工作首次将强大的扩散模型应用于视频虚拟试穿任务，提出了 ViViD 框架，在视频虚拟试穿领域取得了显著进展。

            (2):Innovation point: 创新点：提出了服装编码器、注意力特征融合机制、轻量级姿势编码器和分层的 Temporal 模块，有效解决了视频虚拟试穿任务中的服装细节捕捉、时间一致性、服装与人体姿势交互建模等关键挑战。Performance: 性能：在 ViViD 数据集上，ViViD 在 FID 和 LPIPS 度量方面均优于最先进的方法，证明了其在视频虚拟试穿任务中的有效性。Workload: 工作量：ViViD 的实现相对复杂，需要大量的训练数据和计算资源。
</code></pre></li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-0355bce071e350207c70de02bda959ed.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-23ccaa5c8bb5673e1bef077ad2b7d22a.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-e7b35dffa1cf9920b89882397361f15f.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-1e3d8bba6d2cbf1178e36f754857920d.jpg" align="middle"></details><h2 id="HR-Human-Modeling-Human-Avatars-with-Triangular-Mesh-and-High-Resolution-Textures-from-Videos"><a href="#HR-Human-Modeling-Human-Avatars-with-Triangular-Mesh-and-High-Resolution-Textures-from-Videos" class="headerlink" title="HR Human: Modeling Human Avatars with Triangular Mesh and   High-Resolution Textures from Videos"></a>HR Human: Modeling Human Avatars with Triangular Mesh and High-Resolution Textures from Videos</h2><p><strong>Authors:Qifeng Chen, Rengan Xie, Kai Huang, Qi Wang, Wenting Zheng, Rong Li, Yuchi Huo</strong></p><p>Recently, implicit neural representation has been widely used to generate animatable human avatars. However, the materials and geometry of those representations are coupled in the neural network and hard to edit, which hinders their application in traditional graphics engines. We present a framework for acquiring human avatars that are attached with high-resolution physically-based material textures and triangular mesh from monocular video. Our method introduces a novel information fusion strategy to combine the information from the monocular video and synthesize virtual multi-view images to tackle the sparsity of the input view. We reconstruct humans as deformable neural implicit surfaces and extract triangle mesh in a well-behaved pose as the initial mesh of the next stage. In addition, we introduce an approach to correct the bias for the boundary and size of the coarse mesh extracted. Finally, we adapt prior knowledge of the latent diffusion model at super-resolution in multi-view to distill the decomposed texture. Experiments show that our approach outperforms previous representations in terms of high fidelity, and this explicit result supports deployment on common renderers.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.11270v1">PDF</a></p><p><strong>Summary</strong></p><p>用单目视频获取带有物理材质纹理和三角形网格的可变形人体模型。</p><p><strong>Key Takeaways</strong></p><ul><li>使用隐式神经表示生成可动画人体模型。</li><li>引入信息融合策略解决单目视频输入视图稀疏问题。</li><li>重建人体为可变形神经隐式曲面，提取三角形网格作为初始网格。</li><li>提出方法纠正粗糙网格边界和大小偏差。</li><li>采用多视图超分辨率潜扩散模型先验知识提取分解纹理。</li><li>实验表明该方法在高保真度方面优于以往表示，且显式结果支持在通用渲染器上部署。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: HR Human: 使用三角形网格和高分辨率纹理从视频中建模人体化身</p></li><li><p>Authors: Qifeng Chen, Rengan Xie, Kai Huang, Qi Wang, Wenting Zheng, Rong Li, Yuchi Huo</p></li><li><p>Affiliation: 中国杭州</p></li><li><p>Keywords: Human modeling;Rendering;Texture super resolution</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2405.11270</p></li></ol><p>Github: None</p><ol><li><p>Summary:</p><p>(1): 近期，隐式神经表示已被广泛用于生成可动画的人体化身。然而，这些表示中的材质和几何形状在神经网络中耦合，难以编辑，这阻碍了它们在传统图形引擎中的应用。</p><p>(2): 过去的方法主要有：Implicit animatable human reconstruction、Relighting4D、Relightavatar。这些方法存在的问题是：隐式几何和纹理难以编辑，产生的纹理清晰度低，无法应用于传统图形引擎。</p><p>(3): 本文提出了一种从单目视频中获取具有高分辨率基于物理的材质纹理和三角形网格的人体化身的方法。该方法引入了一种新颖的信息融合策略，将单目视频中的信息与合成的虚拟多视图图像相结合，以解决输入视图的稀疏性。我们将人体重建为可变形的神经隐式曲面，并在行为良好的姿态中提取三角形网格作为下一阶段的初始网格。此外，我们还引入了一种方法来纠正提取的粗糙网格的边界和大小偏差。最后，我们采用了多视图超分辨率中潜在扩散模型的先验知识来提取分解的纹理。</p><p>(4): 在人体建模任务上，该方法在高保真度方面优于以往的表示，并且这种显式结果支持在通用渲染器上的部署。</p></li><li><p>方法：</p><p>（1）：提出了一种从单目视频获取具有高分辨率基于物理的材质纹理和三角形网格的人体化身的方法；</p><p>（2）：引入了一种新颖的信息融合策略，将单目视频中的信息与合成的虚拟多视图图像相结合，以解决输入视图的稀疏性；</p><p>（3）：将人体重建为可变形的神经隐式曲面，并在行为良好的姿态中提取三角形网格作为下一阶段的初始网格；</p><p>（4）：引入了一种方法来纠正提取的粗糙网格的边界和大小偏差；</p><p>（5）：采用了多视图超分辨率中潜在扩散模型的先验知识来提取分解的纹理。</p></li><li><p>结论：</p></li></ol><p>（1）：本文提出了一种从单目视频中获取具有高分辨率基于物理的材质纹理和三角形网格的人体化身的方法，该方法在高保真度方面优于以往的表示，并且这种显式结果支持在通用渲染器上的部署。</p><p>（2）：创新点：提出了一种新颖的信息融合策略，将单目视频中的信息与合成的虚拟多视图图像相结合，以解决输入视图的稀疏性；引入了一种方法来纠正提取的粗糙网格的边界和大小偏差；采用了多视图超分辨率中潜在扩散模型的先验知识来提取分解的纹理。 性能：在人体建模任务上，该方法在高保真度方面优于以往的表示。 工作量：该方法需要合成虚拟多视图图像，这可能会增加计算成本。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-58f6be0321d44679e674675890fa61f4.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-d120932cc8da35e36223e213bf08ff48.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-e49edd99763ea96c13881d786d9a42af.jpg" align="middle"></details><h2 id="Deep-Data-Consistency-a-Fast-and-Robust-Diffusion-Model-based-Solver-for-Inverse-Problems"><a href="#Deep-Data-Consistency-a-Fast-and-Robust-Diffusion-Model-based-Solver-for-Inverse-Problems" class="headerlink" title="Deep Data Consistency: a Fast and Robust Diffusion Model-based Solver   for Inverse Problems"></a>Deep Data Consistency: a Fast and Robust Diffusion Model-based Solver for Inverse Problems</h2><p><strong>Authors:Hanyu Chen, Zhixiu Hao, Liying Xiao</strong></p><p>Diffusion models have become a successful approach for solving various image inverse problems by providing a powerful diffusion prior. Many studies tried to combine the measurement into diffusion by score function replacement, matrix decomposition, or optimization algorithms, but it is hard to balance the data consistency and realness. The slow sampling speed is also a main obstacle to its wide application. To address the challenges, we propose Deep Data Consistency (DDC) to update the data consistency step with a deep learning model when solving inverse problems with diffusion models. By analyzing existing methods, the variational bound training objective is used to maximize the conditional posterior and reduce its impact on the diffusion process. In comparison with state-of-the-art methods in linear and non-linear tasks, DDC demonstrates its outstanding performance of both similarity and realness metrics in generating high-quality solutions with only 5 inference steps in 0.77 seconds on average. In addition, the robustness of DDC is well illustrated in the experiments across datasets, with large noise and the capacity to solve multiple tasks in only one pre-trained model.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.10748v1">PDF</a> Codes: <a target="_blank" rel="noopener" href="https://github.com/Hanyu-Chen373/DeepDataConsistency">https://github.com/Hanyu-Chen373/DeepDataConsistency</a></p><p><strong>Summary:</strong><br>深度数据一致性通过深度学习模型更新数据一致性步骤，解决了扩散模型求解逆问题的挑战，展现了卓越的相似性和真实性表现。</p><p><strong>Key Takeaways:</strong></p><ul><li>提出深度数据一致性 (DDC) 方法，将数据一致性步骤用深度学习模型更新。</li><li>使用变分界训练目标，最大化条件后验，减少其对扩散过程的影响。</li><li>在线性和非线性任务中，DDC 在相似性和真实性指标上表现优异。</li><li>DDC 仅需 5 步推理，平均耗时 0.77 秒，生成高质量的解决方案。</li><li>DDC 在不同数据集、大噪声条件下表现稳健。</li><li>DDC 可以用一个预训练模型解决多个任务。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: 深度数据一致性：一种快速且鲁棒的扩散模型求解逆问题的模型</p></li><li><p>Authors: 陈瀚宇，郝志修，肖丽英</p></li><li><p>Affiliation: 清华大学</p></li><li><p>Keywords: 扩散模型，逆问题，数据一致性，真实性</p></li><li><p>Urls: https://arxiv.org/abs/2405.10748v1, Github:None</p></li><li><p>Summary:</p></li></ol><p>(1):扩散模型在解决图像逆问题方面取得了成功，但如何平衡数据一致性和真实性是一个挑战。</p><p>(2):现有方法包括替换得分函数、分解矩阵或使用优化算法，但它们在数据一致性和真实性之间难以平衡，且推理速度慢。</p><p>(3):本文提出深度数据一致性（DDC），使用神经网络更新扩散模型中的数据一致性步骤，通过变分界训练目标最大化条件后验概率，并减少其对扩散过程的影响。</p><p>(4):在图像超分辨率、修复、去模糊和JPEG恢复等任务上，DDC在仅需5个推理步骤且平均耗时0.77秒的情况下，在相似性和真实性指标上均取得了优异的性能，证明了其在平衡数据一致性和真实性方面的有效性。此外，DDC在不同数据集、大噪声和单一预训练模型解决多任务方面的鲁棒性也得到了证明。</p><ol><li>方法：</li></ol><p>（1）提出深度数据一致性（DDC），使用神经网络更新扩散模型中的数据一致性步骤，通过变分界训练目标最大化条件后验概率，并减少其对扩散过程的影响；</p><p>（2）利用神经网络拟合数据一致性项，并将其融入到扩散模型中，使得模型能够在保留数据一致性的同时，生成更真实的图像；</p><p>（3）在训练过程中，通过变分界训练目标最大化条件后验概率，使得模型能够专注于生成与条件数据一致的真实图像；</p><p>（4）通过减少数据一致性项对扩散过程的影响，使得模型能够在推理过程中快速生成图像，同时保持较高的真实性。</p><ol><li>结论：</li></ol><p>（1）本文提出的深度数据一致性（DDC）方法，在平衡数据一致性和真实性的同时，实现了快速推理，为扩散模型求解逆问题提供了新的思路和方法。</p><p>（2）创新点：提出深度数据一致性（DDC）方法，使用神经网络更新扩散模型中的数据一致性步骤，通过变分界训练目标最大化条件后验概率，并减少其对扩散过程的影响。</p><p>性能：在图像超分辨率、修复、去模糊和 JPEG 恢复等任务上，DDC 在仅需 5 个推理步骤且平均耗时 0.77 秒的情况下，在相似性和真实性指标上均取得了优异的性能。</p><p>工作量：DDC 的训练过程需要使用神经网络拟合数据一致性项，并将其融入到扩散模型中，这可能会增加训练时间和计算成本。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-825b9ef49219bfe90e547c36af6ae92e.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-d8f3559fc7f4e16bd5efc45f3e874012.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-833585aeca5f9fcecaa196677353c9fe.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-62c582c82243d4b2484dbc714bdede51.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-321a696bd3140a7780176a7ef30ec4fe.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-58f99fcf80754e3e0aae1cad41d5cfeb.jpg" align="middle"></details></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io/2024/05/22/Paper/2024-05-22/Diffusion%20Models/">https://kedreamix.github.io/2024/05/22/Paper/2024-05-22/Diffusion Models/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Diffusion-Models/">Diffusion Models</a></div><div class="post_share"><div class="social-share" data-image="https://picx.zhimg.com/v2-d222555f8f090be73c96a07a45af66c0.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/05/22/Paper/2024-05-22/Talking%20Head%20Generation/" title="Talking Head Generation"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-55d3ca2d04e45a757c657d4be241bba9.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Talking Head Generation</div></div></a></div><div class="next-post pull-right"><a href="/2024/05/22/Paper/2024-05-22/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙/虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-b4943be56cf94e02149b50eec541d27f.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">元宇宙/虚拟人</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/03/03/Paperscape/EMO/" title="EMO Emote Portrait Alive - 阿里HumanAIGC"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-6492e24fb03ffa98135dc584535ab7d9.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-03</div><div class="title">EMO Emote Portrait Alive - 阿里HumanAIGC</div></div></a></div><div><a href="/2024/01/24/Paper/2024-01-24/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-71a37c439c6714e8867560f580599d2f.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-24</div><div class="title">Diffusion Models</div></div></a></div><div><a href="/2024/02/02/Paper/2024-02-02/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-5920453c69c00995f18077b22d4a790e.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-02</div><div class="title">Diffusion Models</div></div></a></div><div><a href="/2024/01/30/Paper/2024-01-30/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-e55358c77a9d65f15701e8f33262e2a4.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-30</div><div class="title">Diffusion Models</div></div></a></div><div><a href="/2024/02/09/Paper/2024-02-09/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-32488f736ee10537497afccc3a1a1d76.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-09</div><div class="title">Diffusion Models</div></div></a></div><div><a href="/2024/02/23/Paper/2024-02-23/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-ff425802a32a4519e30b9044a3eed1e8.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-23</div><div class="title">Diffusion Models</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-05-22-%E6%9B%B4%E6%96%B0"><span class="toc-text">2024-05-22 更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Diffusion-RSCC-Diffusion-Probabilistic-Model-for-Change-Captioning-in-Remote-Sensing-Images"><span class="toc-text">Diffusion-RSCC: Diffusion Probabilistic Model for Change Captioning in Remote Sensing Images</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Images-that-Sound-Composing-Images-and-Sounds-on-a-Single-Canvas"><span class="toc-text">Images that Sound: Composing Images and Sounds on a Single Canvas</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Slicedit-Zero-Shot-Video-Editing-With-Text-to-Image-Diffusion-Models-Using-Spatio-Temporal-Slices"><span class="toc-text">Slicedit: Zero-Shot Video Editing With Text-to-Image Diffusion Models Using Spatio-Temporal Slices</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HR-Human-Modeling-Human-Avatars-with-Triangular-Mesh-and-High-Resolution-Textures-from-Videos"><span class="toc-text">HR Human: Modeling Human Avatars with Triangular Mesh and High-Resolution Textures from Videos</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Deep-Data-Consistency-a-Fast-and-Robust-Diffusion-Model-based-Solver-for-Inverse-Problems"><span class="toc-text">Deep Data Consistency: a Fast and Robust Diffusion Model-based Solver for Inverse Problems</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image:url('https://picx.zhimg.com/v2-d222555f8f090be73c96a07a45af66c0.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js").then((()=>{pangu.autoSpacingPage()}))}function panguInit(){panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.1},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script></div><script id="canvas_nest" defer color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><script data-pjax>function butterfly_swiper_injector_config(){var a=document.getElementById("recent-posts");console.log("已挂载butterfly_swiper"),a.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/25/Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="2024/01/25/Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="2024/01/25/Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/17/Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023/12/17/Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="2023/12/17/Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script></body></html>