<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>元宇宙/虚拟人 | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="元宇宙&#x2F;虚拟人 方向最新论文已更新，请持续关注 Update in 2024-05-22  GGAvatar Geometric Adjustment of Gaussian Head Avatar"><meta property="og:type" content="article"><meta property="og:title" content="元宇宙&#x2F;虚拟人"><meta property="og:url" content="https://kedreamix.github.io/2024/05/22/Paper/2024-05-22/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/index.html"><meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World"><meta property="og:description" content="元宇宙&#x2F;虚拟人 方向最新论文已更新，请持续关注 Update in 2024-05-22  GGAvatar Geometric Adjustment of Gaussian Head Avatar"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://pic1.zhimg.com/v2-b4943be56cf94e02149b50eec541d27f.jpg"><meta property="article:published_time" content="2024-05-22T04:00:21.000Z"><meta property="article:modified_time" content="2024-05-22T04:00:21.952Z"><meta property="article:author" content="Kedreamix"><meta property="article:tag" content="元宇宙&#x2F;虚拟人"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://pic1.zhimg.com/v2-b4943be56cf94e02149b50eec541d27f.jpg"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/2024/05/22/Paper/2024-05-22/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-74LZ5BEQQ1")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!0,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"简"},noticeOutdate:{limitDay:500,position:"top",messagePrev:"It has been",messageNext:"days since the last update, the content of the article may be outdated."},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:200},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!0,post:!0},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!0,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"元宇宙/虚拟人",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-05-22 12:00:21"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach((e=>{n.setAttribute(e,t[e])})),document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)})),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=24?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 7.0.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml"></head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{const d=document.getElementById("loading-box"),e=document.body,o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l(),window.addEventListener("load",(()=>{o()}))})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">239</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url('https://pic1.zhimg.com/v2-b4943be56cf94e02149b50eec541d27f.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">元宇宙/虚拟人</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-05-22T04:00:21.000Z" title="发表于 2024-05-22 12:00:21">2024-05-22</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-05-22T04:00:21.952Z" title="更新于 2024-05-22 12:00:21">2024-05-22</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">5.4k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>19分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="元宇宙/虚拟人"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-05-22-更新"><a href="#2024-05-22-更新" class="headerlink" title="2024-05-22 更新"></a>2024-05-22 更新</h1><h2 id="GGAvatar-Geometric-Adjustment-of-Gaussian-Head-Avatar"><a href="#GGAvatar-Geometric-Adjustment-of-Gaussian-Head-Avatar" class="headerlink" title="GGAvatar: Geometric Adjustment of Gaussian Head Avatar"></a>GGAvatar: Geometric Adjustment of Gaussian Head Avatar</h2><p><strong>Authors:Xinyang Li, Jiaxin Wang, Yixin Xuan, Gongxin Yao, Yu Pan</strong></p><p>We propose GGAvatar, a novel 3D avatar representation designed to robustly model dynamic head avatars with complex identities and deformations. GGAvatar employs a coarse-to-fine structure, featuring two core modules: Neutral Gaussian Initialization Module and Geometry Morph Adjuster. Neutral Gaussian Initialization Module pairs Gaussian primitives with deformable triangular meshes, employing an adaptive density control strategy to model the geometric structure of the target subject with neutral expressions. Geometry Morph Adjuster introduces deformation bases for each Gaussian in global space, creating fine-grained low-dimensional representations of deformation behaviors to address the Linear Blend Skinning formula’s limitations effectively. Extensive experiments show that GGAvatar can produce high-fidelity renderings, outperforming state-of-the-art methods in visual quality and quantitative metrics.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.11993v1">PDF</a> 9 pages, 5 figures</p><p><strong>Summary</strong><br>GGAvatar，一种新颖的3D虚拟形象表示，旨在稳健地塑造带有复杂特征和形变的动态头部虚拟形象。</p><p><strong>Key Takeaways</strong></p><ul><li>GGAvatar 采用了粗到细的结构，包括两个核心模块：中性高斯初始化模块和几何变形调节器。</li><li>中性高斯初始化模块将高斯基本体与可变形的三角形网格配对，采用自适应密度控制策略来模拟目标对象在中性表情下的几何结构。</li><li>几何变形调节器为全局空间中的每个高斯体引入形变基础，创建了细粒度、低维的形变行为表示，有效解决了线性混合蒙皮配方的局限性。</li><li>大量实验表明，GGAvatar 可以产生高保真渲染，在视觉质量和定量指标上优于最先进的方法。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>标题：GGAvatar：高斯头部头像的几何调整</p></li><li><p>作者：Xinyang Li, Jiaxin Wang, Yixin Xuan, Gongxin Yao, Yu Pan</p></li><li><p>单位：浙江大学</p></li><li><p>关键词：3D Avatar、Gaussian Primitives、Geometric Deformation、Neutral Expression Initialization、Morph Adjuster</p></li><li><p>论文链接：https://arxiv.org/abs/2405.11993v1，Github：None</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：创建高保真数字头像对于元宇宙和各种应用至关重要，但将这些头像推广到未见过的姿势或表情仍然是一个挑战。</p><p>（2）：过去方法：3DMM技术或神经隐式表示，但前者缺乏结构灵活性，后者训练和渲染效率低。</p><p>（3）：研究方法：提出GGAvatar，一种新的3D头像表示，它采用粗到细的结构，包括中性高斯初始化模块和几何变形调整器，分别用于对目标对象的中性表情几何结构建模和引入变形基以有效解决线性混合蒙皮公式的局限性。</p><p>（4）：任务和性能：在高保真渲染任务上，GGAvatar优于最先进的方法，在视觉质量和定量指标上都取得了更好的性能，支持其目标。</p><ol><li>方法：</li></ol><p>（1）：利用离散高斯基元（Gaussian primitives）对目标对象的中性表情几何结构建模，初始化3D头像；</p><p>（2）：引入变形基，有效解决线性混合蒙皮公式（Linear Blending Skinning）的局限性，实现精细变形；</p><p>（3）：通过多分辨率三平面存储头部周围的高频空间信息，学习额外的基，实现高精度变形；</p><p>（4）：使用L1损失、D-SSIM损失和感知损失监督训练，并添加位置和缩放正则化项，保证高斯基元的合理性。</p><ol><li>结论：</li></ol><p>（1）：本工作提出了一种新颖的3D头像表示GGAvatar，解决了现有方法在未见过的姿势或表情下的推广问题，为元宇宙和各种应用中的高保真数字头像创建提供了新的途径。</p><p>（2）：创新点：提出了一种粗到细的结构，包括中性高斯初始化模块和几何变形调整器，分别用于对目标对象的中性表情几何结构建模和引入变形基以有效解决线性混合蒙皮公式的局限性。 性能：在高保真渲染任务上优于最先进的方法，在视觉质量和定量指标上都取得了更好的性能。 工作量：需要离散高斯基元对目标对象的中性表情几何结构建模，引入变形基，学习额外的基，并使用L1损失、D-SSIM损失和感知损失监督训练，工作量较大。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-d0b37297e18948031e40fa8e18788ee1.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-c6d3d44ffb48165e82767bbe3166494f.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-c5178d3e1bae75cb38fb1b04f261f7cd.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-9b80139053ee885b883063414b7490a2.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-d72be260f2e841855af340127b381ef0.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-45a63ba45015397a4354d20b5e428a1a.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-5d8ef253cd8bfe45e75b2c2ecc8dd03f.jpg" align="middle"></details>## Motion Avatar: Generate Human and Animal Avatars with Arbitrary Motion **Authors:Zeyu Zhang, Yiran Wang, Biao Wu, Shuo Chen, Zhiyuan Zhang, Shiya Huang, Wenbo Zhang, Meng Fang, Ling Chen, Yang Zhao** In recent years, there has been significant interest in creating 3D avatars and motions, driven by their diverse applications in areas like film-making, video games, AR/VR, and human-robot interaction. However, current efforts primarily concentrate on either generating the 3D avatar mesh alone or producing motion sequences, with integrating these two aspects proving to be a persistent challenge. Additionally, while avatar and motion generation predominantly target humans, extending these techniques to animals remains a significant challenge due to inadequate training data and methods. To bridge these gaps, our paper presents three key contributions. Firstly, we proposed a novel agent-based approach named Motion Avatar, which allows for the automatic generation of high-quality customizable human and animal avatars with motions through text queries. The method significantly advanced the progress in dynamic 3D character generation. Secondly, we introduced a LLM planner that coordinates both motion and avatar generation, which transforms a discriminative planning into a customizable Q&amp;A fashion. Lastly, we presented an animal motion dataset named Zoo-300K, comprising approximately 300,000 text-motion pairs across 65 animal categories and its building pipeline ZooGen, which serves as a valuable resource for the community. See project website https://steve-zeyu-zhang.github.io/MotionAvatar/ [PDF](http://arxiv.org/abs/2405.11286v1) **Summary** 3D角色生成迈入新阶段， Motion Avatar实现海量动画数据动态生成，助推动物3D动画生成技术落地。 **Key Takeaways** - 提出 Motion Avatar，实现人类及动物3D高精可定制化形象生成与动作驱动。 - LLM规划器实现动作与形象生成协调统一，将生成式任务转换为可定制化问答交互。 - Zoo-300K 动物动作数据集包含约 30 万个跨 65 个动物种类的文本-动作对，极大丰富动物动画数据。 - ZooGen 数据集生成管道为动物动画生成任务提供宝贵的数据资源。 - Motion Avatar显著提升动态3D角色生成效率。 - 该技术为电影制作、游戏、AR/VR、人机交互等领域提供强大助力。 - 动物3D动画生成技术的落地有望突破数据和方法瓶颈。 **[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: 动作化身：生成具有任意动作的人类和动物化身</p></li><li><p>Authors: Zeyu Zhang, Yiran Wang, Biao Wu, Shuo Chen, Zhiyuan Zhang, Shiya Huang, Wenbo Zhang, Meng Fang, Ling Chen, Yang Zhao</p></li><li><p>Affiliation: 澳大利亚国立大学</p></li><li><p>Keywords: Motion Avatar, LLM planner, Zoo-300K, Animal motion dataset</p></li><li><p>Urls: https://steve-zeyu-zhang.github.io/MotionAvatar, Github: https://github.com/steve-zeyu-zhang/MotionAvatar</p></li><li><p>Summary:</p></li></ol><p>(1): 近年来，人们对创建 3D 化身和动作产生了浓厚的兴趣，这得益于它们在电影制作、视频游戏、AR/VR 和人机交互等领域的广泛应用。然而，目前的研究主要集中在生成 3D 化身网格或产生动作序列，而将这两个方面集成在一起仍然是一个持续的挑战。此外，虽然化身和动作生成主要针对人类，但由于缺乏训练数据和方法，将这些技术扩展到动物仍然是一个重大挑战。</p><p>(2): 过去的方法要么专注于生成 3D 化身网格，要么专注于生成动作序列，但将这两个方面集成在一起仍然是一个挑战。此外，现有的方法主要针对人类，而将这些技术扩展到动物仍然存在困难。</p><p>(3): 本文提出了一种名为 Motion Avatar 的基于代理的新方法，它允许通过文本查询自动生成具有动作的高质量可定制人类和动物化身。该方法极大地推进了动态 3D 角色生成的进展。此外，我们引入了一个 LLM 规划器来协调动作和化身生成，它将判别规划转换为可定制的问答方式。最后，我们提出了一个名为 Zoo-300K 的动物动作数据集，其中包含大约 300,000 个跨越 65 个动物类别的文本-动作对及其构建管道 ZooGen，它为社区提供了宝贵的资源。</p><p>(4): 在生成具有任意动作的人类和动物化身方面，Motion Avatar 实现了最先进的性能。该方法在各种任务上都取得了出色的效果，包括动作生成、化身生成和动作到化身的重定向。这些结果证明了该方法在创建逼真且可定制的 3D 角色方面的潜力。</p><ol><li>方法：</li></ol><p>（1）：基于代理的Motion Avatar方法，通过文本查询自动生成具有动作的高质量可定制人类和动物化身；</p><p>（2）：LLM规划器协调动作和化身生成，将判别规划转换为可定制的问答方式；</p><p>（3）：Zoo-300K动物动作数据集包含约300,000个跨越65个动物类别的文本-动作对；</p><p>（4）：Motion Avatar在生成具有任意动作的人类和动物化身方面实现了最先进的性能，在动作生成、化身生成和动作到化身的重定向等各种任务上取得了出色的效果。</p><ol><li>结论：</li></ol><p>（1）：本研究通过提出创新的解决方案，解决了动态 3D 化身生成中的持续挑战。我们引入了 Motion Avatar，一种新颖的基于代理的方法，它使我们能够根据文本查询自动创建具有动态动作的高质量可定制人类和动物化身，从而极大地推进了该领域的发展。此外，我们的 LLM 规划器促进了动作和化身生成的协调，增强了动态化身任务中的适应性和可用性。此外，Zoo-300K 数据集的开发以及 ZooGen 管道为研究人员提供了宝贵的资源，强调了我们致力于推进跨各种领域的动态化身生成。</p><p>（2）：创新点：提出 Motion Avatar 方法，实现了动作和化身生成的一体化，并引入了 LLM 规划器，增强了适应性和可用性。 性能：在动作生成、化身生成和动作到化身的重定向等各种任务上取得了最先进的性能。 工作量：需要收集和标注大量的数据，并且训练过程可能需要大量的时间和计算资源。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-b4943be56cf94e02149b50eec541d27f.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-4d4b6d7293dfe420ebcd255a83e215e5.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-178d2f15bc2899950bcba3dc7e32fcaa.jpg" align="middle"></details>## HR Human: Modeling Human Avatars with Triangular Mesh and High-Resolution Textures from Videos **Authors:Qifeng Chen, Rengan Xie, Kai Huang, Qi Wang, Wenting Zheng, Rong Li, Yuchi Huo** Recently, implicit neural representation has been widely used to generate animatable human avatars. However, the materials and geometry of those representations are coupled in the neural network and hard to edit, which hinders their application in traditional graphics engines. We present a framework for acquiring human avatars that are attached with high-resolution physically-based material textures and triangular mesh from monocular video. Our method introduces a novel information fusion strategy to combine the information from the monocular video and synthesize virtual multi-view images to tackle the sparsity of the input view. We reconstruct humans as deformable neural implicit surfaces and extract triangle mesh in a well-behaved pose as the initial mesh of the next stage. In addition, we introduce an approach to correct the bias for the boundary and size of the coarse mesh extracted. Finally, we adapt prior knowledge of the latent diffusion model at super-resolution in multi-view to distill the decomposed texture. Experiments show that our approach outperforms previous representations in terms of high fidelity, and this explicit result supports deployment on common renderers. [PDF](http://arxiv.org/abs/2405.11270v1) **Summary** 利用单目视频，实现带有高分辨率物理材质纹理和三角形网格的可动画人的获取。 **Key Takeaways** - 创新性信息融合策略，结合单目视频信息，合成虚拟多视角图像，解决输入视角稀疏问题。 - 将人物重建为可变形神经隐式曲面，并提取行为良好的姿势中的三角形网格作为下一阶段的初始网格。 - 提出一种方法来纠正粗糙网格边界和尺寸的偏差。 - 采用多视角超分辨率潜扩散模型的先验知识，蒸馏分解纹理。 - 实验表明，该方法在高保真方面优于之前的表示，显式结果支持在常见渲染器上部署。 **[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: HR Human: 利用三角形网格和高分辨率纹理从视频中建模人类化身</p></li><li><p>Authors: Qifeng Chen, Rengan Xie, Kai Huang, Qi Wang, Wenting Zheng, Rong Li, Yuchi Huo</p></li><li><p>Affiliation: 浙江大学</p></li><li><p>Keywords: 人体建模、渲染、材质纹理</p></li><li><p>Urls: Paper: https://arxiv.org/pdf/2405.11270.pdf, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 近期，隐式神经表示已被广泛用于生成可动画的人类化身。然而，这些表示中的材质和几何形状在神经网络中耦合，难以编辑，这阻碍了它们在传统图形引擎中的应用。</p><p>(2): 现有方法： - 隐式神经表示：几何和材质耦合，难以编辑。 - Relighting4D 和 Relightavatar：尝试用隐式表示恢复具有分离几何和材质的人类化身，但隐式几何和纹理难以编辑，且纹理清晰度较低。 - Nvdiffrec：专注于重建显式的一般静态对象，但无法处理非刚性物体和皮肤的动态运动。</p><p>(3): 本文提出了一种从单目视频中获取附有高分辨率基于物理的材质纹理和三角形网格的人类化身的方法。该方法引入了信息融合策略，结合单目视频信息和合成虚拟多视图图像，解决了输入视图的稀疏性。将人类重建为可变形神经隐式曲面，并提取良好姿势下的三角形网格作为下一阶段的初始网格。此外，还引入了一种方法来校正提取的粗网格的边界和尺寸偏差。最后，将多视图超分辨率中潜在扩散模型的先验知识用于分解纹理。</p><p>(4): 本文方法在高保真度方面优于以往的表示，显式结果支持在常见渲染器上部署。</p><ol><li>方法：</li></ol><p>（1）：提出了一种从单目视频中获取附有高分辨率基于物理的材质纹理和三角形网格的人类化身的方法；</p><p>（2）：引入信息融合策略，结合单目视频信息和合成虚拟多视图图像，解决了输入视图的稀疏性；</p><p>（3）：将人类重建为可变形神经隐式曲面，并提取良好姿势下的三角形网格作为下一阶段的初始网格；</p><p>（4）：引入了一种方法来校正提取的粗网格的边界和尺寸偏差；</p><p>（5）：将多视图超分辨率中潜在扩散模型的先验知识用于分解纹理。</p><ol><li>结论：</li></ol><p>（1）本文提出的 HR Human 框架的意义在于，它能够从单目视频中重建出带有三角形网格和对应的 PBR 材质纹理的数字化身。我们引入了一种新颖的信息融合策略，将单目视频的信息与合成的虚拟多视图图像相结合，以弥补缺失的空间视图信息。此外，我们还修正了从隐式场中提取的网格的边界和尺寸偏差。最后，我们引入了一个经过预训练的潜在扩散模型，用于分解纹理。</p><p>（2）创新点：本文提出了一种从单目视频中重建带有三角形网格和 PBR 材质纹理的人类化身的方法，该方法具有以下创新点： - 引入信息融合策略，结合单目视频信息和合成虚拟多视图图像，解决了输入视图的稀疏性问题。 - 提出了一种方法来校正提取的粗网格的边界和尺寸偏差。 - 将多视图超分辨率中潜在扩散模型的先验知识用于分解纹理。</p><p>性能：本文方法在高保真度方面优于以往的表示，显式结果支持在常见渲染器上部署。</p><p>工作量：本文方法的工作量相对较大，需要训练一个潜在扩散模型来分解纹理。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-58f6be0321d44679e674675890fa61f4.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-d120932cc8da35e36223e213bf08ff48.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-e49edd99763ea96c13881d786d9a42af.jpg" align="middle"></details>## LayGA: Layered Gaussian Avatars for Animatable Clothing Transfer **Authors:Siyou Lin, Zhe Li, Zhaoqi Su, Zerong Zheng, Hongwen Zhang, Yebin Liu** Animatable clothing transfer, aiming at dressing and animating garments across characters, is a challenging problem. Most human avatar works entangle the representations of the human body and clothing together, which leads to difficulties for virtual try-on across identities. What's worse, the entangled representations usually fail to exactly track the sliding motion of garments. To overcome these limitations, we present Layered Gaussian Avatars (LayGA), a new representation that formulates body and clothing as two separate layers for photorealistic animatable clothing transfer from multi-view videos. Our representation is built upon the Gaussian map-based avatar for its excellent representation power of garment details. However, the Gaussian map produces unstructured 3D Gaussians distributed around the actual surface. The absence of a smooth explicit surface raises challenges in accurate garment tracking and collision handling between body and garments. Therefore, we propose two-stage training involving single-layer reconstruction and multi-layer fitting. In the single-layer reconstruction stage, we propose a series of geometric constraints to reconstruct smooth surfaces and simultaneously obtain the segmentation between body and clothing. Next, in the multi-layer fitting stage, we train two separate models to represent body and clothing and utilize the reconstructed clothing geometries as 3D supervision for more accurate garment tracking. Furthermore, we propose geometry and rendering layers for both high-quality geometric reconstruction and high-fidelity rendering. Overall, the proposed LayGA realizes photorealistic animations and virtual try-on, and outperforms other baseline methods. Our project page is https://jsnln.github.io/layga/index.html. [PDF](http://arxiv.org/abs/2405.07319v1) SIGGRAPH 2024 conference track **Summary** 多层高斯化身（LayGA）：一种用于从多视角视频中进行逼真可动画服装转移的新型表示，它将身体和服装表述为两个独立的层。 **Key Takeaways** - 提出了一种名为 Layered Gaussian Avatars (LayGA) 的新表示，它将身体和服装表述为可动画服装转移的两个独立层。 - LayGA 基于以高斯图为基础的化身，因其对服装细节的出色表示能力。 - 使用了一系列几何约束来重建光滑表面，并同时获得了身体和服装之间的分割。 - 训练了两个独立模型来表示身体和服装，并利用重建的服装几何作为更准确的服装跟踪的 3D 监督。 - 提出了一种几何和渲染层，用于高质量的几何重建和高保真的渲染。 - 总的来说，提出的 LayGA 实现了逼真的动画和虚拟试穿，并且优于其他基线方法。 **[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: Layered Gaussian Avatars for Animatable Clothing (基于分层高斯分布的动画服装化身)</p></li><li><p>Authors: Siyou Lin, Zhe Li, Zhaoqi Su, Zerong Zheng, Hongwen Zhang, Yebin Liu</p></li><li><p>Affiliation: 清华大学</p></li><li><p>Keywords: Animatable avatar, clothing transfer, human reconstruction</p></li><li><p>Urls: Paper: https://arxiv.org/pdf/2405.07319.pdf, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 动画服装传输旨在跨角色穿衣和动画服装，是一个具有挑战性的问题。大多数人体化身工作将人体和服装的表示纠缠在一起，这给不同身份的虚拟试穿带来了困难。更糟糕的是，纠缠的表示通常无法准确跟踪服装的滑动运动。</p><p>(2): 过去的方法将身体和服装表示为一个整体，这使得虚拟试穿跨身份困难，并且无法准确跟踪服装的滑动运动。</p><p>(3): 本文提出了一种新的表示形式——分层高斯化身 (LayGA)，它将身体和服装表述为两个独立的层，用于从多视图视频中进行逼真的动画服装传输。LayGA 在单层重建阶段提出了一系列几何约束，以重建平滑曲面并同时获得身体和服装之间的分割。在多层拟合阶段，训练了两个独立的模型来表示身体和服装，并利用重建的服装几何作为 3D 监督，以实现更准确的服装跟踪。此外，还提出了几何层和渲染层，用于高质量的几何重建和高保真渲染。</p><p>(4): 在服装转移任务上，LayGA 实现了逼真的动画和虚拟试穿，并且优于其他基线方法。</p><ol><li>方法：</li></ol><p>（1）：提出分层高斯化身（LayGA）表示，将身体和服装表示为两个独立的层，用于从多视图视频中进行逼真的动画服装传输。</p><p>（2）：在单层重建阶段，提出了一系列几何约束，以重建平滑曲面并同时获得身体和服装之间的分割。</p><p>（3）：在多层拟合阶段，训练了两个独立的模型来表示身体和服装，并利用重建的服装几何作为3D监督，以实现更准确的服装跟踪。</p><p>（4）：此外，还提出了几何层和渲染层，用于高质量的几何重建和高保真渲染。</p><ol><li>结论：<pre><code>            (1):该工作提出了分层高斯化身（LayGA）表示，用于从多视图视频中进行逼真的动画服装传输。LayGA 将身体和服装表示为两个独立的层，并提出了一系列几何约束和多层拟合策略，以实现更准确的服装跟踪和高质量的几何重建。

            (2):创新点：提出了 LayGA 表示，将身体和服装表示为两个独立的层，并提出了几何约束和多层拟合策略，以实现更准确的服装跟踪和高质量的几何重建；

            性能：在服装转移任务上，LayGA 实现了逼真的动画和虚拟试穿，并且优于其他基线方法；

            工作量：该方法需要多视图视频输入，并且训练过程需要大量的数据和计算资源。
</code></pre></li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-cbea179fd85983d0e759d4be018fb59a.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-68aa80c2ba44dfde97867ba03ebc2814.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-80f17e9e8af3606ee233b1b0ca1da60c.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-923a67fcbe4586a3709ea7a21a673f85.jpg" align="middle"></details></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io/2024/05/22/Paper/2024-05-22/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">https://kedreamix.github.io/2024/05/22/Paper/2024-05-22/元宇宙_虚拟人/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">元宇宙/虚拟人</a></div><div class="post_share"><div class="social-share" data-image="https://pic1.zhimg.com/v2-b4943be56cf94e02149b50eec541d27f.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/05/22/Paper/2024-05-22/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-d222555f8f090be73c96a07a45af66c0.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Diffusion Models</div></div></a></div><div class="next-post pull-right"><a href="/2024/05/13/Paper/2024-05-13/NeRF/" title="NeRF"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-7575292a1dc220679b8e9c4fa1e7bb9b.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">NeRF</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/01/24/Paper/2024-01-24/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-0f13a2b60bef4c886a3317754c99b456.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-24</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/04/01/Paper/2024-04-01/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-4f2a829065c463be027e4b423c4e43c8.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-01</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/04/06/Paper/2024-04-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-4f97970c093585e18e2db42fb96a6b75.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-06</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/05/02/Paper/2024-05-02/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-37516691b13dabbabb3b74ea46b402d8.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-02</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/05/06/Paper/2024-05-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-65e21e3a0a320adc36f81e6bfc7c5739.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-06</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/05/28/Paper/2024-05-28/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-dc27e0e81b6be96603dd90e8aa23e081.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-28</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-05-22-%E6%9B%B4%E6%96%B0"><span class="toc-text">2024-05-22 更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#GGAvatar-Geometric-Adjustment-of-Gaussian-Head-Avatar"><span class="toc-text">GGAvatar: Geometric Adjustment of Gaussian Head Avatar</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image:url('https://pic1.zhimg.com/v2-b4943be56cf94e02149b50eec541d27f.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js").then((()=>{pangu.autoSpacingPage()}))}function panguInit(){panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.1},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script></div><script id="canvas_nest" defer color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><script data-pjax>function butterfly_swiper_injector_config(){var a=document.getElementById("recent-posts");console.log("已挂载butterfly_swiper"),a.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/25/Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="2024/01/25/Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="2024/01/25/Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/17/Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023/12/17/Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="2023/12/17/Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script></body></html>