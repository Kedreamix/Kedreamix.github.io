<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>Diffusion Models | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-05-02  Probing Unlearned Diffusion Models A Transferable Adversarial Attack   Perspective"><meta property="og:type" content="article"><meta property="og:title" content="Diffusion Models"><meta property="og:url" content="https://kedreamix.github.io/2024/05/02/Paper/2024-05-02/Diffusion%20Models/index.html"><meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World"><meta property="og:description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-05-02  Probing Unlearned Diffusion Models A Transferable Adversarial Attack   Perspective"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://pic1.zhimg.com/v2-abb4a9d2f1e50d8adfd617a0dc760606.jpg"><meta property="article:published_time" content="2024-05-02T02:21:37.000Z"><meta property="article:modified_time" content="2024-05-02T02:21:37.091Z"><meta property="article:author" content="Kedreamix"><meta property="article:tag" content="Diffusion Models"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://pic1.zhimg.com/v2-abb4a9d2f1e50d8adfd617a0dc760606.jpg"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/2024/05/02/Paper/2024-05-02/Diffusion%20Models/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-74LZ5BEQQ1")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!0,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"简"},noticeOutdate:{limitDay:500,position:"top",messagePrev:"It has been",messageNext:"days since the last update, the content of the article may be outdated."},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:200},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!0,post:!0},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!0,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"Diffusion Models",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-05-02 10:21:37"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach((e=>{n.setAttribute(e,t[e])})),document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)})),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=24?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 7.0.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml"></head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{const d=document.getElementById("loading-box"),e=document.body,o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l(),window.addEventListener("load",(()=>{o()}))})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">226</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url('https://pic1.zhimg.com/v2-abb4a9d2f1e50d8adfd617a0dc760606.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Diffusion Models</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-05-02T02:21:37.000Z" title="发表于 2024-05-02 10:21:37">2024-05-02</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-05-02T02:21:37.091Z" title="更新于 2024-05-02 10:21:37">2024-05-02</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">9.5k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>35分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="Diffusion Models"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-05-02-更新"><a href="#2024-05-02-更新" class="headerlink" title="2024-05-02 更新"></a>2024-05-02 更新</h1><h2 id="Probing-Unlearned-Diffusion-Models-A-Transferable-Adversarial-Attack-Perspective"><a href="#Probing-Unlearned-Diffusion-Models-A-Transferable-Adversarial-Attack-Perspective" class="headerlink" title="Probing Unlearned Diffusion Models: A Transferable Adversarial Attack   Perspective"></a>Probing Unlearned Diffusion Models: A Transferable Adversarial Attack Perspective</h2><p><strong>Authors:Xiaoxuan Han, Songlin Yang, Wei Wang, Yang Li, Jing Dong</strong></p><p>Advanced text-to-image diffusion models raise safety concerns regarding identity privacy violation, copyright infringement, and Not Safe For Work content generation. Towards this, unlearning methods have been developed to erase these involved concepts from diffusion models. However, these unlearning methods only shift the text-to-image mapping and preserve the visual content within the generative space of diffusion models, leaving a fatal flaw for restoring these erased concepts. This erasure trustworthiness problem needs probe, but previous methods are sub-optimal from two perspectives: (1) Lack of transferability: Some methods operate within a white-box setting, requiring access to the unlearned model. And the learned adversarial input often fails to transfer to other unlearned models for concept restoration; (2) Limited attack: The prompt-level methods struggle to restore narrow concepts from unlearned models, such as celebrity identity. Therefore, this paper aims to leverage the transferability of the adversarial attack to probe the unlearning robustness under a black-box setting. This challenging scenario assumes that the unlearning method is unknown and the unlearned model is inaccessible for optimization, requiring the attack to be capable of transferring across different unlearned models. Specifically, we employ an adversarial search strategy to search for the adversarial embedding which can transfer across different unlearned models. This strategy adopts the original Stable Diffusion model as a surrogate model to iteratively erase and search for embeddings, enabling it to find the embedding that can restore the target concept for different unlearning methods. Extensive experiments demonstrate the transferability of the searched adversarial embedding across several state-of-the-art unlearning methods and its effectiveness for different levels of concepts.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.19382v1">PDF</a></p><p><strong>Summary</strong><br>扩散模型的反学习研究存在安全隐患，本文提出了一种迁移性的对抗攻击方法来探测反学习鲁棒性，在黑盒设置下有效恢复被擦除的概念。</p><p><strong>Key Takeaways</strong></p><ul><li>反学习方法会将文本到图像的映射关系进行重新分布，但保留了扩散模型生成空间中的视觉内容。</li><li>目前探测反学习鲁棒性的方法缺乏可迁移性和攻击力。</li><li>本文提出了一个对抗性搜索策略，可以在黑盒设置下跨不同的反学习模型迁移对抗嵌入。</li><li>采用原始 Stable Diffusion 模型作为代理模型来迭代擦除和搜索嵌入。</li><li>实验表明，搜索的对抗嵌入可以跨多个最先进的反学习方法迁移，并且对不同层次的概念都能有效恢复。</li><li>本文提出的方法弥补了当前探测反学习鲁棒性方法的不足，为评估和提高反学习的有效性提供了新的思路。</li><li>该方法可以用于评估反学习的鲁棒性，并为提高反学习的有效性提供指导。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>论文标题：探测未学习扩散模型：可转移对抗攻击视角</p></li><li><p>作者：</p><ul><li>Xiaoxuan Han</li><li>Songlin Yang</li><li>Wei Wang</li><li>Yang Li</li><li>Jing Dong</li></ul></li><li><p>第一作者单位：中国科学院大学</p></li><li><p>关键词：</p><ul><li>Diffusion Model</li><li>Machine Unlearning</li><li>Adversarial Attack</li></ul></li><li><p>论文链接：https://arxiv.org/abs/2404.19382 Github 链接：https://github.com/hxxdtd/PUND</p></li><li><p>摘要：</p><p>(1) 研究背景： 文本到图像（T2I）扩散模型在生成高质量图像方面取得了显著进展，但也带来了身份隐私、版权和不安全内容等安全问题。为了缓解这些问题，提出了概念擦除方法来消除涉及的概念。</p><p>(2) 过往方法与问题： 现有的概念擦除方法通过改变文本到图像的映射来实现“擦除”任务，但未能擦除扩散模型生成空间内的视觉内容，为恢复这些擦除的概念留下了致命缺陷。此外，这些方法缺乏可转移性，并且在恢复狭窄概念（例如名人身份）方面存在局限性。</p><p>(3) 本文方法： 本文提出了一种可转移的对抗攻击来探测未学习扩散模型的鲁棒性。该攻击采用对抗搜索策略，在原始 Stable Diffusion 模型上迭代擦除和搜索嵌入，以找到可以在不同的未学习模型之间转移的对抗嵌入。</p><p>(4) 实验结果： 实验表明，所搜索的对抗嵌入在各种最先进的未学习方法中具有可转移性，并且在从宽泛到狭窄的不同概念级别上都表现出有效性。这些结果支持了本文提出的方法能够探测未学习扩散模型的鲁棒性，并为恢复擦除的概念提供了新的思路。</p></li><li><p>方法：</p><p>（1）：提出一种可转移的对抗攻击方法，探测未学习扩散模型的鲁棒性；</p><p>（2）：采用对抗搜索策略，在原始 Stable Diffusion 模型上迭代擦除和搜索嵌入，找到可以在不同未学习模型之间转移的对抗嵌入；</p><p>（3）：使用对抗嵌入作为攻击源，在不同的未学习扩散模型上进行攻击，评估模型的鲁棒性；</p><p>（4）：通过实验验证所搜索的对抗嵌入在各种最先进的未学习方法中具有可转移性，并且在从宽泛到狭窄的不同概念级别上都表现出有效性。</p></li><li><p>结论：</p><pre><code>            (1):本文提出了一种可转移的对抗攻击方法，探测未学习扩散模型的鲁棒性，为恢复擦除的概念提供了新的思路。

            (2):Innovation point: 提出了一种可转移的对抗攻击方法，该方法在不同未学习模型之间具有可转移性，并且在从宽泛到狭窄的不同概念级别上都表现出有效性。Performance: 实验结果表明，所搜索的对抗嵌入在各种最先进的未学习方法中具有可转移性，并且在从宽泛到狭窄的不同概念级别上都表现出有效性。Workload: 该方法需要使用对抗搜索策略，在原始 Stable Diffusion 模型上迭代擦除和搜索嵌入，这可能需要大量的计算资源。
</code></pre></li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-aeb2a7c17e04ea32837496f134911073.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-3cfab2dba37aac49c7649b71ac867d79.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-0aa15fec53d79c3279c72f74772273b9.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-8177a4229cfdb42440835f0ee9e56c19.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-67758faf074d7114c762f4a57a5d1403.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-471f68a1caec8d9bae0fe6402d798203.jpg" align="middle"></details><h2 id="TheaterGen-Character-Management-with-LLM-for-Consistent-Multi-turn-Image-Generation"><a href="#TheaterGen-Character-Management-with-LLM-for-Consistent-Multi-turn-Image-Generation" class="headerlink" title="TheaterGen: Character Management with LLM for Consistent Multi-turn   Image Generation"></a>TheaterGen: Character Management with LLM for Consistent Multi-turn Image Generation</h2><p><strong>Authors:Junhao Cheng, Baiqiao Yin, Kaixin Cai, Minbin Huang, Hanhui Li, Yuxin He, Xi Lu, Yue Li, Yifei Li, Yuhao Cheng, Yiqiang Yan, Xiaodan Liang</strong></p><p>Recent advances in diffusion models can generate high-quality and stunning images from text. However, multi-turn image generation, which is of high demand in real-world scenarios, still faces challenges in maintaining semantic consistency between images and texts, as well as contextual consistency of the same subject across multiple interactive turns. To address this issue, we introduce TheaterGen, a training-free framework that integrates large language models (LLMs) and text-to-image (T2I) models to provide the capability of multi-turn image generation. Within this framework, LLMs, acting as a “Screenwriter”, engage in multi-turn interaction, generating and managing a standardized prompt book that encompasses prompts and layout designs for each character in the target image. Based on these, Theatergen generate a list of character images and extract guidance information, akin to the “Rehearsal”. Subsequently, through incorporating the prompt book and guidance information into the reverse denoising process of T2I diffusion models, Theatergen generate the final image, as conducting the “Final Performance”. With the effective management of prompt books and character images, TheaterGen significantly improves semantic and contextual consistency in synthesized images. Furthermore, we introduce a dedicated benchmark, CMIGBench (Consistent Multi-turn Image Generation Benchmark) with 8000 multi-turn instructions. Different from previous multi-turn benchmarks, CMIGBench does not define characters in advance. Both the tasks of story generation and multi-turn editing are included on CMIGBench for comprehensive evaluation. Extensive experimental results show that TheaterGen outperforms state-of-the-art methods significantly. It raises the performance bar of the cutting-edge Mini DALLE 3 model by 21% in average character-character similarity and 19% in average text-image similarity.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.18919v1">PDF</a></p><p><strong>Summary</strong><br>对话生成模型 TheaterGen 无需额外训练，即可实现文本到图像的多轮生成，提升图像语义和上下文一致性。</p><p><strong>Key Takeaways</strong></p><ul><li>TheaterGen 创新性地将大语言模型融入文本到图像模型，实现多轮图像生成。</li><li>大语言模型作为“编剧”，生成标准化提示手册，管理角色提示和设计。</li><li>TheaterGen 基于手册生成角色图像，提取指导信息。</li><li>反向去噪过程将手册和指导信息融入扩散模型，生成图像。</li><li>CMIGBench 是首个不预先定义角色的多轮图像生成基准测试。</li><li>TheaterGen 显著优于其他方法，在 Mini DALLE 3 模型上提升平均角色相似度 21%，平均文本图像相似度 19%。</li><li>TheaterGen 可用于故事生成和多轮编辑任务。</li><li>TheaterGen 为文本到图像生成领域带来了突破。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: TheaterGen：基于 LLM 的多轮图像生成中的角色管理</p></li><li><p>Authors: Junhao Cheng, Baiqiao Yin, Kaixin Cai, Minbin Huang, Hanhui Li, Yuxin He, Xi Lu, Yue Li, Yifei Li, Yuhao Cheng, Yiqiang Yan, Xiaodan Liang</p></li><li><p>Affiliation: 中山大学深圳校区</p></li><li><p>Keywords: Diffusion models · Consistency · Multi-turn image generation</p></li><li><p>Urls: https://howe140.github.io/theatergen.io/ , Github: https://github.com/donahowe/Theatergen</p></li><li><p>Summary:</p><pre><code>           (1): 当前的扩散模型在文本生成图像方面取得了显著进展。然而，在实际场景中需求较高的多轮图像生成仍然面临着图像和文本之间的语义一致性以及同一主题在多个交互轮次中的上下文一致性等挑战。

           (2): 现有的方法主要集中在文本提示的改进和扩散模型的训练上，但对于多轮图像生成中的角色管理和一致性问题关注较少。

           (3): 本文提出 TheaterGen，这是一个无训练框架，它集成了大语言模型 (LLM) 和文本到图像 (T2I) 模型，以提供多轮图像生成的能力。在这个框架中，LLM 充当“编剧”，参与多轮交互，生成和管理一个标准化的提示手册，其中包含目标图像中每个角色的提示和布局设计。基于角色提示和布局，生成角色图像列表并从中提取指导信息，类似于“排练”。随后，通过将提示手册和指导信息融入 T2I 扩散模型的反向去噪过程中，生成最终图像，即进行“最终表演”。通过有效管理提示手册和角色图像，TheaterGen 显着提高了合成图像中的语义和上下文一致性。此外，我们引入了包含 8000 个多轮指令的专门基准 CMIGBench（一致的多轮图像生成基准）。与以前的多轮基准不同，CMIGBench 不预先定义角色，因此具有很大的多样性。CMIGBench 上包含故事生成和多轮编辑任务，以进行综合评估。

           (4): 广泛的实验结果表明，TheaterGen 明显优于最先进的方法。例如，它将尖端的 Mini DALL·E 3 模型在平均字符-字符相似度方面的性能提高了 21%，在平均文本-图像相似度方面的性能提高了 19%。
</code></pre></li><li><p>方法：</p></li></ol><p>（1）：提出基于大语言模型（LLM）的角色设计师，生成包含角色提示和布局的结构化提示手册；</p><p>（2）：设计角色图像管理器，生成舞台角色图像及其对应的线条引导和潜在引导；</p><p>（3）：将提示手册和两种引导输入基于角色的生成器中，合成最终图像。</p><ol><li>结论：</li></ol><p>（1）：本文提出了 TheaterGen，一个无训练框架，它集成了大语言模型 (LLM) 和文本到图像 (T2I) 模型，以提供多轮图像生成的能力。通过有效管理提示手册和角色图像，TheaterGen 显着提高了合成图像中的语义和上下文一致性。此外，我们引入了包含 8000 个多轮指令的专门基准 CMIGBench（一致的多轮图像生成基准）。与以前的多轮基准不同，CMIGBench 不预先定义角色，因此具有很大的多样性。CMIGBench 上包含故事生成和多轮编辑任务，以进行综合评估。</p><p>（2）：创新点：提出基于大语言模型（LLM）的角色设计师，生成包含角色提示和布局的结构化提示手册；设计角色图像管理器，生成舞台角色图像及其对应的线条引导和潜在引导；将提示手册和两种引导输入基于角色的生成器中，合成最终图像。性能：广泛的实验结果表明，TheaterGen 明显优于最先进的方法。例如，它将尖端的 Mini DALL·E 3 模型在平均字符-字符相似度方面的性能提高了 21%，在平均文本-图像相似度方面的性能提高了 19%。工作量：TheaterGen 是一个无训练框架，不需要额外的训练成本。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-9ba365cfc612e009b79d484c29a30149.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-fce5b92ae3c1c7350697723f803ec2cb.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-568b0d8a34639fe3e5425bc5cb460f4b.jpg" align="middle"></details>## FlexiFilm: Long Video Generation with Flexible Conditions **Authors:Yichen Ouyang, jianhao Yuan, Hao Zhao, Gaoang Wang, Bo zhao** Generating long and consistent videos has emerged as a significant yet challenging problem. While most existing diffusion-based video generation models, derived from image generation models, demonstrate promising performance in generating short videos, their simple conditioning mechanism and sampling strategy-originally designed for image generation-cause severe performance degradation when adapted to long video generation. This results in prominent temporal inconsistency and overexposure. Thus, in this work, we introduce FlexiFilm, a new diffusion model tailored for long video generation. Our framework incorporates a temporal conditioner to establish a more consistent relationship between generation and multi-modal conditions, and a resampling strategy to tackle overexposure. Empirical results demonstrate FlexiFilm generates long and consistent videos, each over 30 seconds in length, outperforming competitors in qualitative and quantitative analyses. Project page: https://y-ichen.github.io/FlexiFilm-Page/ [PDF](http://arxiv.org/abs/2404.18620v1) 9 pages, 9 figures **Summary** 针对长视频生成这一重大挑战，本文提出 FlexiFilm，一种专为长视频生成而设计的扩散模型。 **Key Takeaways** - 现有基于扩散的视频生成模型在生成长视频时性能下降。 - FlexiFilm 引入了时间条件器，以建立生成与多模态条件之间更一致的关系。 - FlexiFilm 使用再采样策略来解决过度曝光问题。 - FlexiFilm 生成的长视频超过 30 秒，时间一致性好。 - FlexiFilm 在定性和定量分析中均优于竞争对手。 - FlexiFilm 能生成内容丰富、具有时间连贯性的长视频。 - FlexiFilm 为长视频生成提供了新的思路。 **[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: FlexiFilm: 长视频生成中的时空一致性 (FlexiFilm: Temporal Coherence in Long Video Generation)</p></li><li><p>Authors: Yichen Ouyang, Jianhao Yuan, Hao Zhao, Tiejun Huang, Gaoang Wang, Bo Zhao</p></li><li><p>Affiliation: 南京大学 (Nanjing University)</p></li><li><p>Keywords: Long video generation, Diffusion models, Temporal conditioner, Resampling strategy</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2302.09413, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 长视频生成面临时空一致性挑战，现有基于扩散的视频生成模型在长视频生成中表现不佳。</p><p>(2): 过去的方法采用简单的条件机制和采样策略，导致时空不一致和过曝问题。</p><p>(3): 本文提出 FlexiFilm，一种针对长视频生成量身定制的扩散模型。FlexiFilm 引入时间条件器，建立生成和多模态条件之间更一致的关系，并采用重采样策略解决过曝问题。</p><p>(4): 在长视频生成任务上，FlexiFilm 优于竞争对手，生成长度超过 30 秒的长且一致的视频。定量和定性分析都支持其目标。</p><ol><li>方法：</li></ol><p>（1）：提出 FlexiFilm，一种针对长视频生成量身定制的扩散模型，引入时间条件器，建立生成和多模态条件之间更一致的关系；</p><p>（2）：采用重采样策略解决过曝问题，在长视频生成任务上优于竞争对手，生成长度超过 30 秒的长且一致的视频。</p><ol><li>结论：<pre><code>            (1):本工作针对长视频生成中时空一致性问题，提出 FlexiFilm 模型，有效提升了长视频生成质量；

            (2):创新点：引入时间条件器和重采样策略，增强时空一致性；性能：在长视频生成任务上优于竞争对手，可生成长度超过 30 秒的长且一致的视频；工作量：模型设计和训练复杂度较高。
</code></pre></li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-89f7187f1074067e636b6cefcd03214c.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-fc7b975f21081a9007db0c1ec2d26248.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-dcfbf96f6700552f8cbb6108717b928b.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-26f03063378af5e36436e73a3bc39c46.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-3bf4fac4e6634e90aecfc106469774e4.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-639e75c0d686596adb3d0c89cc48bb9c.jpg" align="middle"></details><h2 id="Anywhere-A-Multi-Agent-Framework-for-Reliable-and-Diverse-Foreground-Conditioned-Image-Inpainting"><a href="#Anywhere-A-Multi-Agent-Framework-for-Reliable-and-Diverse-Foreground-Conditioned-Image-Inpainting" class="headerlink" title="Anywhere: A Multi-Agent Framework for Reliable and Diverse   Foreground-Conditioned Image Inpainting"></a>Anywhere: A Multi-Agent Framework for Reliable and Diverse Foreground-Conditioned Image Inpainting</h2><p><strong>Authors:Tianyidan Xie, Rui Ma, Qian Wang, Xiaoqian Ye, Feixuan Liu, Ying Tai, Zhenyu Zhang, Zili Yi</strong></p><p>Recent advancements in image inpainting, particularly through diffusion modeling, have yielded promising outcomes. However, when tested in scenarios involving the completion of images based on the foreground objects, current methods that aim to inpaint an image in an end-to-end manner encounter challenges such as “over-imagination”, inconsistency between foreground and background, and limited diversity. In response, we introduce Anywhere, a pioneering multi-agent framework designed to address these issues. Anywhere utilizes a sophisticated pipeline framework comprising various agents such as Visual Language Model (VLM), Large Language Model (LLM), and image generation models. This framework consists of three principal components: the prompt generation module, the image generation module, and the outcome analyzer. The prompt generation module conducts a semantic analysis of the input foreground image, leveraging VLM to predict relevant language descriptions and LLM to recommend optimal language prompts. In the image generation module, we employ a text-guided canny-to-image generation model to create a template image based on the edge map of the foreground image and language prompts, and an image refiner to produce the outcome by blending the input foreground and the template image. The outcome analyzer employs VLM to evaluate image content rationality, aesthetic score, and foreground-background relevance, triggering prompt and image regeneration as needed. Extensive experiments demonstrate that our Anywhere framework excels in foreground-conditioned image inpainting, mitigating “over-imagination”, resolving foreground-background discrepancies, and enhancing diversity. It successfully elevates foreground-conditioned image inpainting to produce more reliable and diverse results.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.18598v1">PDF</a> 16 pages, 9 figures, project page: <a target="_blank" rel="noopener" href="https://anywheremultiagent.github.io">https://anywheremultiagent.github.io</a></p><p><strong>Summary</strong><br>通过 Anywhere 多智能体框架，利用 VLM、LLM 和图像生成模型，通过语义分析、文本引导的图像生成和结果分析器，实现前景调控图像修复，解决过度想象、前景背景不一致和多样性差的问题。</p><p><strong>Key Takeaways</strong></p><ul><li>引入 Anywhere，一种用于前景调控图像修复的多智能体框架。</li><li>利用 VLM 和 LLM 进行语义分析，生成最佳语言提示。</li><li>使用文本引导的 Canny-to-Image 生成模型创建模板图像。</li><li>使用图像精炼器融合输入前景和模板图像以生成输出。</li><li>使用 VLM 进行结果分析，评估图像内容合理性、美学分数和前景背景相关性。</li><li>触发提示和图像再生，以解决过度想象、前景背景差异和多样性差的问题。</li><li>Anywhere 框架在前景调控图像修复中表现出色，生成更可靠、更多样化的结果。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: ANYWHERE：基于前景条件的图像修复的多智能体框架</p></li><li><p>Authors: Tianyidan Xie, Rui Ma, Qian Wang, Xiaoqian Ye, Feixuan Liu, Ying Tai, Zhenyu Zhang, Zili Yi</p></li><li><p>Affiliation: 南京大学新软件技术国家重点实验室</p></li><li><p>Keywords: Image inpainting, Multi-agent framework, Foreground-conditioned, Diversity, Reliability</p></li><li><p>Urls: https://arxiv.org/abs/2404.18598v1, https://github.com/anywheremultiagent/anywhere</p></li><li><p>Summary:</p><pre><code>            (1):图像修复在图像处理中有着重要的应用，近年来基于扩散模型的图像修复方法取得了显著进展。然而，当应用于基于前景对象完成图像的任务时，现有的端到端图像修复方法面临着“过度想象”、“前景与背景不一致”以及多样性受限等挑战。

            (2):针对上述问题，本文提出了一个开创性的多智能体框架——Anywhere。Anywhere采用了一个复杂的管道框架，包括视觉语言模型（VLM）、大语言模型（LLM）和图像生成模型等多种智能体。该框架由三个主要组件组成：提示生成模块、图像生成模块和结果分析器。提示生成模块对输入的前景图像进行语义分析，利用VLM预测相关的语言描述，并利用LLM推荐最优的语言提示。在图像生成模块中，我们采用了一个文本引导的Canny-to-Image生成模型，基于前景图像的边缘图和语言提示创建模板图像，并使用图像精修器通过融合输入前景和模板图像生成结果。结果分析器利用VLM评估图像内容合理性、美学分数和前景-背景相关性，并根据需要触发提示和图像再生。

            (3):广泛的实验表明，我们的Anywhere框架在基于前景条件的图像修复任务中表现出色，它缓解了“过度想象”，解决了前景与背景的不一致性，并增强了多样性。它成功地将基于前景条件的图像修复提升到了一个新的水平，产生了更加可靠和多样化的结果。

            (4):在基于前景条件的图像修复任务上，Anywhere框架取得了比现有方法更好的性能，证明了其方法的有效性。
</code></pre></li><li><p>方法：</p></li></ol><p>（1）：提出一个多智能体框架——Anywhere，该框架由提示生成模块、图像生成模块和结果分析器组成；</p><p>（2）：提示生成模块利用视觉语言模型（VLM）和大语言模型（LLM）预测相关的语言描述和推荐最优的语言提示；</p><p>（3）：图像生成模块采用文本引导的Canny-to-Image生成模型，基于前景图像的边缘图和语言提示创建模板图像，并使用图像精修器生成结果；</p><p>（4）：结果分析器利用VLM评估图像内容合理性、美学分数和前景-背景相关性，并根据需要触发提示和图像再生。</p><ol><li>结论：</li></ol><p>（1）本工作提出了一个多智能体框架 Anywhere，该框架有效解决了基于前景条件的图像修复任务中存在的“过度想象”、“前景与背景不一致”以及多样性受限等挑战，将基于前景条件的图像修复提升到了一个新的水平；</p><p>（2）创新点：Anywhere 采用了多智能体框架，结合了视觉语言模型、大语言模型和图像生成模型，实现了基于前景条件的图像修复任务的端到端完成；</p><p>性能：Anywhere 在基于前景条件的图像修复任务上取得了比现有方法更好的性能，证明了其方法的有效性；</p><p>工作量：Anywhere 框架的实现相对复杂，需要训练多个智能体模型，工作量较大。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-b60b160bb6aabb892081fb4dd065859c.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-0b5d570ae9275bbd4b3e2d5946151c0d.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-ba6b19fe809bc2888d9a6c4f365915d1.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-7e07715dcf6b24c4a172db98d4808c7b.jpg" align="middle"></details>## Grounded Compositional and Diverse Text-to-3D with Pretrained Multi-View Diffusion Model **Authors:Xiaolong Li, Jiawei Mo, Ying Wang, Chethan Parameshwara, Xiaohan Fei, Ashwin Swaminathan, CJ Taylor, Zhuowen Tu, Paolo Favaro, Stefano Soatto** In this paper, we propose an effective two-stage approach named Grounded-Dreamer to generate 3D assets that can accurately follow complex, compositional text prompts while achieving high fidelity by using a pre-trained multi-view diffusion model. Multi-view diffusion models, such as MVDream, have shown to generate high-fidelity 3D assets using score distillation sampling (SDS). However, applied naively, these methods often fail to comprehend compositional text prompts, and may often entirely omit certain subjects or parts. To address this issue, we first advocate leveraging text-guided 4-view images as the bottleneck in the text-to-3D pipeline. We then introduce an attention refocusing mechanism to encourage text-aligned 4-view image generation, without the necessity to re-train the multi-view diffusion model or craft a high-quality compositional 3D dataset. We further propose a hybrid optimization strategy to encourage synergy between the SDS loss and the sparse RGB reference images. Our method consistently outperforms previous state-of-the-art (SOTA) methods in generating compositional 3D assets, excelling in both quality and accuracy, and enabling diverse 3D from the same text prompt. [PDF](http://arxiv.org/abs/2404.18065v1) 9 pages, 10 figures **摘要：** 融合文本引导的中间表示和混合优化策略，从复合文本提示生成高质量且准确的3D资产。 **要点：** * 利用文本引导的4视图图像作为文本到3D生成中的bottleneck。 * 引入注意力重新聚焦机制，促进文本对齐的4视图图像生成。 * 提出混合优化策略，鼓励SDS损失函数和稀疏RGB参考图像之间的协同作用。 * 大幅优于现有的SOTA方法，在合成3D资产的质量和准确性上都表现出色。 * 支持根据同一文本提示生成多样化的3D资产。 **[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>论文标题：基于预训练多视图的接地式组合式和多样化文本到 3D</li><li>作者：Xudong Zhang, Huchuan Lu, Yinda Zhang, Xiaoguang Han, Joshua B. Tenenbaum, Jiajun Wu</li><li>单位：麻省理工学院（Massachusetts Institute of Technology）</li><li>关键词：文本到 3D、组合式生成、多视图扩散模型、接地式合成</li><li>论文链接：https://arxiv.org/pdf/2302.04742.pdf，Github：无</li><li><p>摘要： （1）：研究背景：现有的文本到 3D 方法在生成组合式文本提示时存在困难，经常遗漏某些主体或部分。 （2）：过去方法：MVDream 等多视图扩散模型可以生成高保真 3D 资产，但无法理解组合式文本提示。 （3）：研究方法：提出了一种两阶段方法 Grounded-Dreamer，利用预训练的多视图扩散模型，通过注意重新聚焦机制和混合优化策略，鼓励文本对齐的 4 视图图像生成，并促进 SDS 损失和稀疏 RGB 参考图像之间的协同作用。 （4）：方法性能：方法在生成组合式 3D 资产方面优于现有技术，在质量和准确性方面均表现出色，并能够从相同的文本提示中生成多样化的 3D。</p></li><li><p>Methods:</p></li></ol><p>（1）：利用预训练的多视图扩散模型 MVDream，通过注意重新聚焦机制和混合优化策略，生成文本对齐的 4 视图图像。</p><p>（2）：采用 SDS 损失和稀疏 RGB 参考图像之间的协同作用，促进 3D 资产的生成。</p><p>（3）：使用文本提示作为条件，生成多样化的 3D 资产。</p><ol><li>结论：<pre><code>            (1):本文提出了一种新颖的文本到 3D 合成两阶段框架，有效克服了组合准确性和多样性的挑战。第一阶段利用多视图扩散模型从文本生成空间相干视图，而第二阶段将稀疏视图 NeRF 与文本引导扩散先验协同起来，用于精细的 3D 重建。这种方法不仅提高了复杂文本提示生成 3D 模型的保真度和组合完整性，还为未来无缝 2D 到 3D 过渡和模型多功能性方面的探索铺平了道路。我们的方法展示了文本到 3D 合成方面的重大飞跃，为该不断发展的领域的进一步进步提供了坚实的基础。

            (2):创新点：利用预训练多视图扩散模型，通过注意力重新聚焦机制和混合优化策略，鼓励文本对齐的 4 视图图像生成，并促进 SDS 损失和稀疏 RGB 参考图像之间的协同作用，促进 3D 资产的生成；性能：在生成组合式 3D 资产方面优于现有技术，在质量和准确性方面均表现出色，并能够从相同的文本提示中生成多样化的 3D；工作量：利用了预训练的多视图扩散模型，减少了训练时间和计算资源的消耗。
</code></pre></li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-0d3bc1be854ed564fddf7ab8d560de5f.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-abb4a9d2f1e50d8adfd617a0dc760606.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-abe084fa386e134319b922f3543204fa.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-ae1872641ac814aff738475c08d64d2b.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-bab3fbf4b4460d73196cb6abddbb1b4f.jpg" align="middle"></details><h2 id="Unsupervised-Anomaly-Detection-via-Masked-Diffusion-Posterior-Sampling"><a href="#Unsupervised-Anomaly-Detection-via-Masked-Diffusion-Posterior-Sampling" class="headerlink" title="Unsupervised Anomaly Detection via Masked Diffusion Posterior Sampling"></a>Unsupervised Anomaly Detection via Masked Diffusion Posterior Sampling</h2><p><strong>Authors:Di Wu, Shicai Fan, Xue Zhou, Li Yu, Yuzhong Deng, Jianxiao Zou, Baihong Lin</strong></p><p>Reconstruction-based methods have been commonly used for unsupervised anomaly detection, in which a normal image is reconstructed and compared with the given test image to detect and locate anomalies. Recently, diffusion models have shown promising applications for anomaly detection due to their powerful generative ability. However, these models lack strict mathematical support for normal image reconstruction and unexpectedly suffer from low reconstruction quality. To address these issues, this paper proposes a novel and highly-interpretable method named Masked Diffusion Posterior Sampling (MDPS). In MDPS, the problem of normal image reconstruction is mathematically modeled as multiple diffusion posterior sampling for normal images based on the devised masked noisy observation model and the diffusion-based normal image prior under Bayesian framework. Using a metric designed from pixel-level and perceptual-level perspectives, MDPS can effectively compute the difference map between each normal posterior sample and the given test image. Anomaly scores are obtained by averaging all difference maps for multiple posterior samples. Exhaustive experiments on MVTec and BTAD datasets demonstrate that MDPS can achieve state-of-the-art performance in normal image reconstruction quality as well as anomaly detection and localization.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.17900v1">PDF</a></p><p><strong>Summary</strong><br>利用扩散模型对正态图像采样，并通过差分映射计算异常分数。</p><p><strong>Key Takeaways</strong></p><ul><li>扩散模型在异常检测中展现出了良好的应用前景。</li><li>MDPS 方法将图像重建问题数学建模为基于掩码噪声观测模型和基于贝叶斯框架的扩散图像先验的正态图像后验采样。</li><li>MDPS 可以有效地计算每次正态后验样本和给定测试图像之间的差分映射。</li><li>异常分数通过对多个后验样本的差分映射进行平均得到。</li><li>MDPS 在图像重建质量和异常检测与定位方面均取得了最先进的效果。</li><li>MDPS 具有较高的可解释性，为异常检测提供了新的思路。</li><li>MDPS 在 MVTec 和 BTAD 数据集上进行了全面实验，证明了其优越性。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: 无监督异常检测的掩码扩散后验采样</p></li><li><p>Authors: Di Wu, Shicai Fan, Xue Zhou, Li Yu, Yuzhong Deng, Jianxiao Zou, Baihong Lin</p></li><li><p>Affiliation: 电子科技大学自动化工程学院</p></li><li><p>Keywords: 无监督异常检测, 扩散模型, 后验采样, 掩码噪声观测模型</p></li><li><p>Urls: https://arxiv.org/abs/2404.17900, Github: https://github.com/KevinBHLin/</p></li><li><p>Summary:</p></li></ol><p>(1): 异常检测是计算机视觉领域的一项基本任务，在医学诊断、智能制造、自动驾驶等领域有着广泛的应用。由于异常样本的稀有性和多样性，近年来研究主要集中在无监督异常检测上，即模型只从正常样本中学习，但可以检测异常数据。</p><p>(2): 现有的无监督异常检测方法主要有重建方法、生成对抗网络方法、扩散模型方法等。其中，重建方法是较早且最常用的神经网络方法之一。然而，传统的重建模型如自动编码器和生成对抗网络存在重建质量较差、训练不稳定等问题。</p><p>(3): 本文提出了一种基于贝叶斯框架的无监督异常检测新方法，称为掩码扩散后验采样(MDPS)。该方法将正常图像重建问题数学建模为基于掩码噪声观测模型和扩散模型的正常图像后验采样，并设计了一种从像素级和感知级角度设计的度量，可以有效计算每个正常后验样本与给定测试图像之间的差异图。异常分数通过对多个后验样本的差异图求平均得到。</p><p>(4): 在 MVTec 和 BTAD 数据集上进行的详尽实验表明，MDPS 在正常图像重建质量、异常检测和定位方面均取得了最先进的性能。这些性能足以支持作者提出的目标。</p><ol><li><p>Methods:</p><p>(1):本文提出了一种基于贝叶斯框架的无监督异常检测新方法，称为掩码扩散后验采样(MDPS)。</p><p>(2):MDPS将正常图像重建问题数学建模为基于掩码噪声观测模型和扩散模型的正常图像后验采样。</p><p>(3):MDPS设计了一种从像素级和感知级角度设计的度量，可以有效计算每个正常后验样本与给定测试图像之间的差异图。</p><p>(4):异常分数通过对多个后验样本的差异图求平均得到。</p></li><li><p>结论：</p></li></ol><p>（1）：本文提出了一种基于贝叶斯框架的无监督异常检测新方法 MDPS，该方法将正常图像重建问题数学建模为基于掩码噪声观测模型和扩散模型的正常图像后验采样，并设计了一种从像素级和感知级角度设计的度量，可以有效计算每个正常后验样本与给定测试图像之间的差异图，异常分数通过对多个后验样本的差异图求平均得到。</p><p>（2）：创新点：MDPS基于贝叶斯框架，将正常图像重建问题数学建模为基于掩码噪声观测模型和扩散模型的正常图像后验采样，并设计了一种从像素级和感知级角度设计的度量；性能：在 MVTec 和 BTAD 数据集上进行的详尽实验表明，MDPS 在正常图像重建质量、异常检测和定位方面均取得了最先进的性能；工作量：MDPS 由于扩散后验采样而导致较高的计算成本。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-9016eceac1926a6b34927f7b8fe1c178.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-4b0adf35bda326a41bfa8fc38c5b7545.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-5e8db92be0dd5370c893fb23a6f36582.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-3954ca6b4dc2d3e932f8670609df6e6f.jpg" align="middle"></details><h2 id="Causal-Diffusion-Autoencoders-Toward-Counterfactual-Generation-via-Diffusion-Probabilistic-Models"><a href="#Causal-Diffusion-Autoencoders-Toward-Counterfactual-Generation-via-Diffusion-Probabilistic-Models" class="headerlink" title="Causal Diffusion Autoencoders: Toward Counterfactual Generation via   Diffusion Probabilistic Models"></a>Causal Diffusion Autoencoders: Toward Counterfactual Generation via Diffusion Probabilistic Models</h2><p><strong>Authors:Aneesh Komanduri, Chen Zhao, Feng Chen, Xintao Wu</strong></p><p>Diffusion probabilistic models (DPMs) have become the state-of-the-art in high-quality image generation. However, DPMs have an arbitrary noisy latent space with no interpretable or controllable semantics. Although there has been significant research effort to improve image sample quality, there is little work on representation-controlled generation using diffusion models. Specifically, causal modeling and controllable counterfactual generation using DPMs is an underexplored area. In this work, we propose CausalDiffAE, a diffusion-based causal representation learning framework to enable counterfactual generation according to a specified causal model. Our key idea is to use an encoder to extract high-level semantically meaningful causal variables from high-dimensional data and model stochastic variation using reverse diffusion. We propose a causal encoding mechanism that maps high-dimensional data to causally related latent factors and parameterize the causal mechanisms among latent factors using neural networks. To enforce the disentanglement of causal variables, we formulate a variational objective and leverage auxiliary label information in a prior to regularize the latent space. We propose a DDIM-based counterfactual generation procedure subject to do-interventions. Finally, to address the limited label supervision scenario, we also study the application of CausalDiffAE when a part of the training data is unlabeled, which also enables granular control over the strength of interventions in generating counterfactuals during inference. We empirically show that CausalDiffAE learns a disentangled latent space and is capable of generating high-quality counterfactual images.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.17735v1">PDF</a></p><p><strong>摘要：</strong></p><p>基于扩散的因果表征学习框架 CausalDiffAE，通过指定因果模型实现反事实生成。</p><p><strong>关键要点：</strong></p><ul><li>提出 CausalDiffAE，将编码器与逆扩散相结合，从高维数据中提取因果表征。</li><li>通过因果编码机制将数据映射到因果相关潜在因子。</li><li>使用神经网络参数化潜在因子之间的因果机制。</li><li>提出变分目标和辅助标签信息来解纠缠因果变量。</li><li>基于 DDIM 提出受干预影响的反事实生成程序。</li><li>研究了仅部分训练数据有标签的情况，可在推理中细粒度控制反事实的干预强度。</li><li>实证表明 CausalDiffAE 学习到了解纠缠的潜在空间，能够生成高质量的反事实图像。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>标题：因果扩散自编码器：基于扩散概率模型的反事实生成</li><p></p><p></p><li>作者：Aneesh Komanduri、Chen Zhao、Feng Chen、Xintao Wu</li><p></p><p></p><li>第一作者单位：阿肯色大学</li><p></p><p></p><li>关键词：扩散概率模型、因果建模、反事实生成、表示学习</li><p></p><p></p><li>论文链接：https://arxiv.org/abs/2404.17735, Github代码链接：无</li><p></p><p></p><li>摘要： (1) 研究背景：扩散概率模型（DPMs）在高质量图像生成方面已成为最先进的技术。然而，DPMs 具有任意噪声潜在空间，没有可解释或可控制的语义。尽管在提高图像样本质量方面进行了大量的研究，但使用扩散模型进行表示控制生成的工作却很少。具体来说，使用 DPMs 进行因果建模和可控反事实生成是一个尚未探索的领域。 (2) 过去的方法及其问题：目前还没有关于使用 DPMs 进行因果建模和反事实生成的工作。这限制了 DPMs 在需要因果推理和反事实生成的任务中的应用。 (3) 本文提出的研究方法：本文提出了 CausalDiffAE，一个基于扩散的因果表示学习框架，以根据指定的因果模型生成反事实。CausalDiffAE 使用编码器从高维数据中提取高级语义因果变量，并使用反向扩散对随机变化进行建模。它提出了一个因果编码机制，将高维数据映射到因果相关的潜在因子，并使用神经网络参数化潜在因子之间的因果机制。为了强制因果变量的解纠缠，CausalDiffAE 制定了变分目标并利用先验中的辅助标签信息来正则化潜在空间。它提出了一个基于 DDIM 的反事实生成过程，该过程受 do 干预的影响。最后，为了解决标签监督有限的情况，CausalDiffAE 还研究了在训练数据的一部分未标记时的应用，这也使得在推理过程中对生成反事实的干预强度进行精细控制。 (4) 本文方法在什么任务上取得了什么性能：实验表明，CausalDiffAE 学习了一个解纠缠的潜在空间，并且能够生成高质量的反事实图像。该性能支持了本文提出的因果表示学习框架在反事实生成任务中的有效性。</li><br>&lt;/ol&gt;<p></p><p>Some Error for method(比如是不是没有Methods这个章节)</p><p><strong>8. 结论：</strong></p><p><strong>(1)：本工作的意义是什么？</strong> 本工作提出了 CausalDiffAE，这是一个基于扩散的因果表示学习和反事实生成框架。我们提出了一个因果编码机制，将图像映射到因果相关的因子。我们通过神经网络学习因子之间的因果机制。我们制定了一个基于变分的扩散目标来强制潜在空间的解纠缠，以实现潜在空间操作。我们提出了一个基于 DDIM 的反事实生成算法，该算法受 do 干预的影响。对于有限监督的情况，我们提出了我们模型的弱监督扩展，它联合学习了一个无条件模型和一个条件模型。该目标还使得能够对生成的 counterfactuals 进行细粒度控制。我们使用定性和定量指标实证地展示了我们模型的能力。未来的工作包括探索文本到图像扩散模型中的反事实生成。</p><p><strong>(2)：总结本文在创新点、性能和工作量三个维度上的优缺点：</strong> <strong>创新点：</strong> * 提出了一种基于扩散的因果表示学习框架 CausalDiffAE。 * 提出了一种因果编码机制，将高维数据映射到因果相关的潜在因子。 * 提出了一种基于变分的扩散目标来强制潜在空间的解纠缠。 * 提出了一种基于 DDIM 的反事实生成算法，该算法受 do 干预的影响。</p><p><strong>性能：</strong> * 实验表明，CausalDiffAE 学习了一个解纠缠的潜在空间，并且能够生成高质量的反事实图像。</p><p><strong>工作量：</strong> * 该方法需要大量的数据和计算资源来训练。 * 该方法在训练过程中需要大量的超参数调整。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-89030cb7b49450895338abca619e996e.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-0118f1cc7ce9364c178c9f49ae8f2863.jpg" align="middle"></details></ol></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io/2024/05/02/Paper/2024-05-02/Diffusion%20Models/">https://kedreamix.github.io/2024/05/02/Paper/2024-05-02/Diffusion Models/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Diffusion-Models/">Diffusion Models</a></div><div class="post_share"><div class="social-share" data-image="https://pic1.zhimg.com/v2-abb4a9d2f1e50d8adfd617a0dc760606.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/05/02/Paper/2024-05-02/Talking%20Head%20Generation/" title="Talking Head Generation"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-00e62c0d66ff2641b9803987918d6fd0.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Talking Head Generation</div></div></a></div><div class="next-post pull-right"><a href="/2024/05/02/Paper/2024-05-02/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙/虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-37516691b13dabbabb3b74ea46b402d8.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">元宇宙/虚拟人</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/03/03/Paperscape/EMO/" title="EMO Emote Portrait Alive - 阿里HumanAIGC"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-6492e24fb03ffa98135dc584535ab7d9.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-03</div><div class="title">EMO Emote Portrait Alive - 阿里HumanAIGC</div></div></a></div><div><a href="/2024/01/24/Paper/2024-01-24/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-71a37c439c6714e8867560f580599d2f.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-24</div><div class="title">Diffusion Models</div></div></a></div><div><a href="/2024/02/02/Paper/2024-02-02/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-5920453c69c00995f18077b22d4a790e.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-02</div><div class="title">Diffusion Models</div></div></a></div><div><a href="/2024/01/30/Paper/2024-01-30/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-e55358c77a9d65f15701e8f33262e2a4.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-30</div><div class="title">Diffusion Models</div></div></a></div><div><a href="/2024/02/13/Paper/2024-02-13/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-3709a9941aada6c4d3ed35934e311765.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-13</div><div class="title">Diffusion Models</div></div></a></div><div><a href="/2024/02/09/Paper/2024-02-09/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-32488f736ee10537497afccc3a1a1d76.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-09</div><div class="title">Diffusion Models</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-05-02-%E6%9B%B4%E6%96%B0"><span class="toc-text">2024-05-02 更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Probing-Unlearned-Diffusion-Models-A-Transferable-Adversarial-Attack-Perspective"><span class="toc-text">Probing Unlearned Diffusion Models: A Transferable Adversarial Attack Perspective</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#TheaterGen-Character-Management-with-LLM-for-Consistent-Multi-turn-Image-Generation"><span class="toc-text">TheaterGen: Character Management with LLM for Consistent Multi-turn Image Generation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Anywhere-A-Multi-Agent-Framework-for-Reliable-and-Diverse-Foreground-Conditioned-Image-Inpainting"><span class="toc-text">Anywhere: A Multi-Agent Framework for Reliable and Diverse Foreground-Conditioned Image Inpainting</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Unsupervised-Anomaly-Detection-via-Masked-Diffusion-Posterior-Sampling"><span class="toc-text">Unsupervised Anomaly Detection via Masked Diffusion Posterior Sampling</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Causal-Diffusion-Autoencoders-Toward-Counterfactual-Generation-via-Diffusion-Probabilistic-Models"><span class="toc-text">Causal Diffusion Autoencoders: Toward Counterfactual Generation via Diffusion Probabilistic Models</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image:url('https://pic1.zhimg.com/v2-abb4a9d2f1e50d8adfd617a0dc760606.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js").then((()=>{pangu.autoSpacingPage()}))}function panguInit(){panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.1},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script></div><script id="canvas_nest" defer color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><script data-pjax>function butterfly_swiper_injector_config(){var a=document.getElementById("recent-posts");console.log("已挂载butterfly_swiper"),a.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/25/Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="2024/01/25/Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="2024/01/25/Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/17/Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023/12/17/Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="2023/12/17/Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script></body></html>