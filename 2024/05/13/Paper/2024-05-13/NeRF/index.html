<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>NeRF | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="NeRF 方向最新论文已更新，请持续关注 Update in 2024-05-13  OneTo3D One Image to Re-editable Dynamic 3D Model and Video Generation"><meta property="og:type" content="article"><meta property="og:title" content="NeRF"><meta property="og:url" content="https://kedreamix.github.io/2024/05/13/Paper/2024-05-13/NeRF/index.html"><meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World"><meta property="og:description" content="NeRF 方向最新论文已更新，请持续关注 Update in 2024-05-13  OneTo3D One Image to Re-editable Dynamic 3D Model and Video Generation"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://pic1.zhimg.com/v2-7575292a1dc220679b8e9c4fa1e7bb9b.jpg"><meta property="article:published_time" content="2024-05-13T08:45:28.000Z"><meta property="article:modified_time" content="2024-05-13T08:45:28.621Z"><meta property="article:author" content="Kedreamix"><meta property="article:tag" content="NeRF"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://pic1.zhimg.com/v2-7575292a1dc220679b8e9c4fa1e7bb9b.jpg"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/2024/05/13/Paper/2024-05-13/NeRF/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-74LZ5BEQQ1")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!0,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"简"},noticeOutdate:{limitDay:500,position:"top",messagePrev:"It has been",messageNext:"days since the last update, the content of the article may be outdated."},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:200},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!0,post:!0},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!0,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"NeRF",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-05-13 16:45:28"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach((e=>{n.setAttribute(e,t[e])})),document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)})),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=24?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 7.0.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml"></head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{const d=document.getElementById("loading-box"),e=document.body,o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l(),window.addEventListener("load",(()=>{o()}))})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">269</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url('https://pic1.zhimg.com/v2-7575292a1dc220679b8e9c4fa1e7bb9b.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">NeRF</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-05-13T08:45:28.000Z" title="发表于 2024-05-13 16:45:28">2024-05-13</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-05-13T08:45:28.621Z" title="更新于 2024-05-13 16:45:28">2024-05-13</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">14.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>53分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="NeRF"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-05-13-更新"><a href="#2024-05-13-更新" class="headerlink" title="2024-05-13 更新"></a>2024-05-13 更新</h1><h2 id="OneTo3D-One-Image-to-Re-editable-Dynamic-3D-Model-and-Video-Generation"><a href="#OneTo3D-One-Image-to-Re-editable-Dynamic-3D-Model-and-Video-Generation" class="headerlink" title="OneTo3D: One Image to Re-editable Dynamic 3D Model and Video Generation"></a>OneTo3D: One Image to Re-editable Dynamic 3D Model and Video Generation</h2><p><strong>Authors:Jinwei Lin</strong></p><p>One image to editable dynamic 3D model and video generation is novel direction and change in the research area of single image to 3D representation or 3D reconstruction of image. Gaussian Splatting has demonstrated its advantages in implicit 3D reconstruction, compared with the original Neural Radiance Fields. As the rapid development of technologies and principles, people tried to used the Stable Diffusion models to generate targeted models with text instructions. However, using the normal implicit machine learning methods is hard to gain the precise motions and actions control, further more, it is difficult to generate a long content and semantic continuous 3D video. To address this issue, we propose the OneTo3D, a method and theory to used one single image to generate the editable 3D model and generate the targeted semantic continuous time-unlimited 3D video. We used a normal basic Gaussian Splatting model to generate the 3D model from a single image, which requires less volume of video memory and computer calculation ability. Subsequently, we designed an automatic generation and self-adaptive binding mechanism for the object armature. Combined with the re-editable motions and actions analyzing and controlling algorithm we proposed, we can achieve a better performance than the SOTA projects in the area of building the 3D model precise motions and actions control, and generating a stable semantic continuous time-unlimited 3D video with the input text instructions. Here we will analyze the detailed implementation methods and theories analyses. Relative comparisons and conclusions will be presented. The project code is open source.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.06547v1">PDF</a> 24 pages, 13 figures, 2 tables</p><p><strong>Summary</strong><br>一键图像生成可编辑动态3D模型和视频，是图像到3D表示或图像3D重建研究领域的新方向和变革。</p><p><strong>Key Takeaways</strong></p><ul><li>相比于原始神经辐射场，高斯溅射在隐式3D重建中表现出优势。</li><li>稳定扩散模型可用于根据文本指令生成目标模型。</li><li>传统的隐式机器学习方法难以获得精确的运动和动作控制。</li><li>难以生成长内容和语义连续的3D视频。</li><li>OneTo3D方法可使用单张图像生成可编辑3D模型和生成目标语义连续且时间无限的3D视频。</li><li>OneTo3D使用基本的高斯溅射模型从单张图像生成3D模型，减少了视频内存和计算机计算需求。</li><li>OneTo3D设计了自动生成和自适应绑定机制，用于对象骨架。</li><li>结合OneTo3D提出的可重新编辑的运动和动作分析与控制算法，在3D模型精确定位运动和动作控制以及根据输入文本指令生成稳定的语义连续且时间无限的3D视频方面，OneTo3D的性能优于该领域的SOTA项目。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: 一张图像到可重新编辑的动态 3D 模型和视频生成</p></li><li><p>Authors: JINWEI LIN</p></li><li><p>Affiliation: 澳大利亚莫纳什大学</p></li><li><p>Keywords: 3D, One image, Editable, Dynamic, Generation, Automation, Video, Self-adaption, Armature</p></li><li><p>Urls: Paper: xxx, Github: None</p></li><li><p>Summary:</p><p>(1): 3D 表征或 3D 重建长期以来一直是计算机视觉领域的研究难题。</p><p>(2): 现有的 3D 重建方法可分为显式方法和隐式方法。显式方法直接设计和完成 3D 重建或建模；隐式方法使用机器学习方法和理论实现这些目标。近年来，Neural Radiance Fields (NeRF) 在隐式 3D 表征或重建方面取得了突出成就。</p><p>(3): 本文提出 OneTo3D 方法，使用单张图像生成可编辑的 3D 模型和语义连续的 3D 视频。该方法使用基本的 Gaussian Splatting 模型从单张图像生成 3D 模型，并设计了一种自动生成和自适应绑定机制来绑定对象骨架。结合重新编辑的运动和动作分析与控制算法，OneTo3D 在 3D 模型精确运动和动作控制以及生成稳定的语义连续无时间限制的 3D 视频方面取得了优于现有方法的性能。</p><p>(4): 本文方法在任务和性能上取得了以下成就：使用单张图像生成可编辑的 3D 模型；生成语义连续的 3D 视频；精确控制 3D 模型的运动和动作。这些性能支持了本文的目标，即实现从单张图像到可重新编辑的动态 3D 模型和视频的生成。</p></li><li><p>方法：</p><pre><code>            (1):生成初始3D模型，使用DreamGaussian模型和Zero-1-to-3方法；

            (2):生成并绑定自适应骨架，设计基本骨架，分析初始3D模型的几何参数信息，微调骨架参数以使其适合对象的身体；

            (3):文本到动作和动作，分析用户文本指令的命令意图，将命令转换为特定动作和骨架相对骨骼的修改数据，控制特定骨骼在Blender中实现相对运动；

            (4):背景去除，使用Dreamgaussian的process.py脚本或其他方法，可选使用图像检测或语义分割机器学习方法；

            (5):颜色分组去除背景，计算图像中每个主要颜色项的比例，将颜色值范围内的颜色项划分为不同的颜色组，去除配置比例范围内的颜色组。
</code></pre></li><li><p>结论：</p></li></ol><p>（1）本篇工作提出了一种从单张图像生成可编辑的动态 3D 模型和视频的方法，具有生成可编辑 3D 模型、生成语义连续的 3D 视频、精确控制 3D 模型的运动和动作等优点，在任务和性能上取得了创新。</p><p>（2）创新点：OneTo3D 方法首次实现了从单张图像到可重新编辑的动态 3D 模型和视频的生成；性能：OneTo3D 方法在生成 3D 模型的精度、视频的语义连续性、动作控制的精确性等方面优于现有方法；工作量：OneTo3D 方法的实现需要较大的计算资源和时间，需要进一步优化算法和设计。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-8729865363a1dfddc21dff54a70072f2.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-02dad34b1d632546ae26f127a58c9c0f.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-f635130d270abd57752edb234d2c8a48.jpg" align="middle"></details>## Aerial-NeRF: Adaptive Spatial Partitioning and Sampling for Large-Scale Aerial Rendering **Authors:Xiaohan Zhang, Yukui Qiu, Zhenyu Sun, Qi Liu** Recent progress in large-scale scene rendering has yielded Neural Radiance Fields (NeRF)-based models with an impressive ability to synthesize scenes across small objects and indoor scenes. Nevertheless, extending this idea to large-scale aerial rendering poses two critical problems. Firstly, a single NeRF cannot render the entire scene with high-precision for complex large-scale aerial datasets since the sampling range along each view ray is insufficient to cover buildings adequately. Secondly, traditional NeRFs are infeasible to train on one GPU to enable interactive fly-throughs for modeling massive images. Instead, existing methods typically separate the whole scene into multiple regions and train a NeRF on each region, which are unaccustomed to different flight trajectories and difficult to achieve fast rendering. To that end, we propose Aerial-NeRF with three innovative modifications for jointly adapting NeRF in large-scale aerial rendering: (1) Designing an adaptive spatial partitioning and selection method based on drones' poses to adapt different flight trajectories; (2) Using similarity of poses instead of (expert) network for rendering speedup to determine which region a new viewpoint belongs to; (3) Developing an adaptive sampling approach for rendering performance improvement to cover the entire buildings at different heights. Extensive experiments have conducted to verify the effectiveness and efficiency of Aerial-NeRF, and new state-of-the-art results have been achieved on two public large-scale aerial datasets and presented SCUTic dataset. Note that our model allows us to perform rendering over 4 times as fast as compared to multiple competitors. Our dataset, code, and model are publicly available at https://drliuqi.github.io/. [PDF](http://arxiv.org/abs/2405.06214v1) **Summary** 针对大规模航拍场景，我们提出 Aerial-NeRF，它针对 NeRF 进行三项创新性修改，以联合实现 NeRF 在大规模航拍渲染中的自适应：自适应空间分区和选择方法、基于姿态相似性的快速渲染和自适应采样方法。 **Key Takeaways** - 提出 Aerial-NeRF，针对大规模航拍场景对 NeRF 进行三项创新性修改。 - 使用自适应空间分区和选择方法，根据无人机姿态自适应不同的飞行轨迹。 - 使用姿态相似性代替（专家）网络进行渲染加速，以确定新视点属于哪个区域。 - 开发自适应采样方法，以提高渲染性能，覆盖不同高度的整座建筑。 - 大量实验验证了 Aerial-NeRF 的有效性和效率，并在两个公开的大规模航拍数据集和 SCUTic 数据集上取得了新的最先进结果。 - 与多个竞争对手相比，我们的模型允许我们以超过 4 倍的速度进行渲染。 - 我们模型、代码和数据集已公开获取。 **[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: 航拍NeRF：大规模航拍渲染的自适应空间划分和采样</p></li><li><p>Authors: Xiaohan Zhang, Yukui Qiu, Zhenyu Sun, Qi Liu</p></li><li><p>Affiliation: 华南理工大学未来技术学院</p></li><li><p>Keywords: View synthesis, large-scale scene rendering, neural radiance fields, fast rendering</p></li><li><p>Urls: https://arxiv.org/abs/2405.06214 , https://github.com/drliuqi/Aerial-NeRF</p></li><li><p>Summary:</p><p>(1):NeRF模型在小物体和室内场景渲染中取得了成功，但将其扩展到航拍渲染中面临两个挑战：单个NeRF无法渲染大规模航拍数据集中的整个场景，传统NeRF无法在单个GPU上训练以实现交互式浏览。</p><p>(2):以往方法将场景划分为多个区域，并在每个区域训练一个NeRF，但这些方法无法适应不同的飞行轨迹，渲染速度也较慢。</p><p>(3):本文提出Aerial-NeRF，通过自适应空间划分和选择、基于姿态相似性确定区域归属、自适应采样等方法，解决了上述问题。</p><p>(4):Aerial-NeRF在两个公开大规模航拍数据集和一个自建数据集上取得了最优性能，渲染速度比其他方法快4倍以上。</p></li><li><p>方法：</p><p>（1）：自适应空间划分：根据航拍数据集的特征，提出了一种自适应空间划分方法，将大规模场景划分为多个小区域，每个区域使用一个NeRF进行渲染；</p><p>（2）：基于姿态相似性确定区域归属：设计了一种基于姿态相似性的区域归属确定算法，根据相机的姿态信息将航拍图像分配到不同的区域；</p><p>（3）：自适应采样：提出了一种自适应采样算法，根据不同区域的复杂程度和渲染速度要求，动态调整采样点数，提高渲染效率；</p><p>（4）：基于神经网络的区域融合：使用神经网络将不同区域的渲染结果融合成最终图像，保证渲染结果的连续性和准确性；</p><p>（5）：基于概率密度函数的加速采样：利用概率密度函数对采样点进行优化，进一步提高渲染速度。</p></li><li><p>结论：</p></li></ol><p>（1）：本工作提出了 Aerial-NeRF，一种用于处理大规模航拍数据集的高效且鲁棒的渲染方法，在渲染速度上大幅优于现有的同类方法，几乎达到 4 倍。此外，在适当的划分区域数量下，Aerial-NeRF 可以使用单个 GPU 渲染任意大的场景。同时，我们为航拍场景引入了一种新颖的采样策略，该策略能够通过不同高度相机的采样范围覆盖建筑物。与 SOTA 模型进行更广泛的比较时，我们的方法明显更有效（仅使用 1/4 的采样点和 2 GB 的 GPU 内存节省），并且在多个常用指标方面具有可比性。最后，我们提出了 SCUTic，这是一个用于大规模大学校园场景的新型航拍数据集，具有不均匀的相机轨迹，可以验证渲染方法的鲁棒性。</p><p>（2）：创新点：自适应空间划分和采样；性能：渲染速度快，内存占用低；工作量：数据集构建和模型训练复杂度较高。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-3f9706ee7489efbc0fffd098a133920f.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-b5cd3300322f846160033228b8f55d45.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-7172b9e2d3611b5ec9915962744d54fa.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-07281c002ad9f4eaef4b0c58ebbaf426.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-a2778deaeb7d026e2fd79cf4c5e6e409.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-d55c494ec8f956acc30da13f5d75881b.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-ac7e04c419fb74632e6c7f9332f81960.jpg" align="middle"></details>## Residual-NeRF: Learning Residual NeRFs for Transparent Object Manipulation **Authors:Bardienus P. Duisterhof, Yuemin Mao, Si Heng Teng, Jeffrey Ichnowski** Transparent objects are ubiquitous in industry, pharmaceuticals, and households. Grasping and manipulating these objects is a significant challenge for robots. Existing methods have difficulty reconstructing complete depth maps for challenging transparent objects, leaving holes in the depth reconstruction. Recent work has shown neural radiance fields (NeRFs) work well for depth perception in scenes with transparent objects, and these depth maps can be used to grasp transparent objects with high accuracy. NeRF-based depth reconstruction can still struggle with especially challenging transparent objects and lighting conditions. In this work, we propose Residual-NeRF, a method to improve depth perception and training speed for transparent objects. Robots often operate in the same area, such as a kitchen. By first learning a background NeRF of the scene without transparent objects to be manipulated, we reduce the ambiguity faced by learning the changes with the new object. We propose training two additional networks: a residual NeRF learns to infer residual RGB values and densities, and a Mixnet learns how to combine background and residual NeRFs. We contribute synthetic and real experiments that suggest Residual-NeRF improves depth perception of transparent objects. The results on synthetic data suggest Residual-NeRF outperforms the baselines with a 46.1% lower RMSE and a 29.5% lower MAE. Real-world qualitative experiments suggest Residual-NeRF leads to more robust depth maps with less noise and fewer holes. Website: https://residual-nerf.github.io [PDF](http://arxiv.org/abs/2405.06181v1) **Summary** 透明物体在工业、医药和家庭中无处不在，机器人在抓取和操作这些物体时面临重大挑战。 **Key Takeaways** - NeRFs 对包含透明物体的场景中的深度感知效果很好。 - NeRFs 在处理极具挑战性的透明物体和光照条件时仍然存在困难。 - Residual-NeRF 提出了一种改善透明物体深度感知和训练速度的方法。 - 首先学习场景中不包含待操作透明物体的背景 NeRF，可以减少学习新物体变化带来的歧义。 - Residual-NeRF 学习推断残差 RGB 值和密度，Mixnet 学习如何组合背景和残差 NeRF。 - 在合成数据上的结果表明，Residual-NeRF 的 RMSE 低 46.1%，MAE 低 29.5%。 - 真实世界的定性实验表明，Residual-NeRF 能够生成更鲁棒的深度图，噪声更少，孔洞更少。 **[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>论文标题：Residual-NeRF：学习残差 NeRF 以实现透明物体操作</p></li><li><p>作者：Bardienus P. Duisterhof, Yuemin Mao, Si Heng Teng, Jeffrey Ichnowski</p></li><li><p>第一作者单位：卡内基梅隆大学机器人研究所</p></li><li><p>关键词：神经辐射场（NeRF）、深度感知、透明物体、残差学习、背景先验</p></li><li><p>论文链接：https://arxiv.org/abs/2405.06181 Github 链接：None</p></li><li><p>摘要：</p></li></ol><p>(1)：研究背景： 透明物体在工业、医药和家庭中无处不在。抓取和操纵这些物体对机器人来说是一个重大挑战。现有的方法难以重建具有挑战性的透明物体的完整深度图，从而在深度重建中留下孔洞。最近的研究表明，神经辐射场（NeRF）非常适合在有透明物体的场景中进行深度感知，并且这些深度图可用于高精度地抓取透明物体。基于 NeRF 的深度重建仍然难以处理特别具有挑战性的透明物体和照明条件。</p><p>(2)：过去的方法和问题： Dex-NeRF 和 Evo-NeRF 等方法表明，NeRF 在透明物体的深度感知方面是有效的。然而，这些方法还表明，NeRF 往往难以处理特别具有挑战性的透明物体，例如具有挑战性光照条件的酒杯或厨房锡箔。透明物体的挑战源于缺乏特征以及外观中很大的视点依赖性变化。</p><p>(3)：本文提出的研究方法： 为了提高透明物体的深度感知并加快训练速度，我们提出了 Residual-NeRF。在许多情况下，机器人的工作区域的几何形状主要是静态且不透明的，例如架子、桌子和桌子。Residual-NeRF 利用场景的静态和不透明部分作为先验，以减少歧义并提高深度感知。Residual-NeRF 首先通过训练不包含透明物体的图像来学习整个场景的背景 NeRF。然后，Residual-NeRF 使用包含透明物体的完整场景的图像来学习残差 NeRF 和 Mixnet。</p><p>(4)：方法的应用任务和性能： 我们对合成和真实数据进行了实验，表明 Residual-NeRF 提高了透明物体的深度感知。合成数据上的结果表明，Residual-NeRF 在 RMSE 上比基线低 46.1%，在 MAE 上低 29.5%。真实世界的定性实验表明，Residual-NeRF 产生了更稳健的深度图，噪点更少，孔洞更少。</p><ol><li>方法：</li></ol><p>（1）：首先训练不包含透明物体的图像，学习整个场景的背景 NeRF；</p><p>（2）：然后使用包含透明物体的完整场景的图像，学习残差 NeRF 和 Mixnet；</p><p>（3）：利用场景的静态和不透明部分作为先验，减少歧义并提高深度感知；</p><p>（4）：Residual-NeRF 提高了透明物体的深度感知。</p><p><strong>结论</strong></p><p>(1): 本工作通过提出 Residual-NeRF，提高了透明物体的深度感知，并加快了训练速度。</p><p>(2): 创新点： - 利用场景的静态和不透明部分作为先验，减少歧义并提高深度感知； - 提出了一种两阶段训练方法，首先学习背景 NeRF，然后学习残差 NeRF 和 Mixnet。 - 提出了一种新的 Mixnet，可以有效地融合背景 NeRF 和残差 NeRF 的输出。</p><pre><code>性能：
- 在合成数据上，Residual-NeRF 在 RMSE 上比基线低 46.1%，在 MAE 上低 29.5%。
- 在真实世界的定性实验中，Residual-NeRF 产生了更稳健的深度图，噪点更少，孔洞更少。

工作量：
- 训练 Residual-NeRF 需要两个阶段的训练，这比基线方法更复杂。
- Residual-NeRF 需要额外的内存来存储背景 NeRF 和残差 NeRF 的权重。
</code></pre><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-b7d7618a421bd8b0947856c3ea91116f.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-1b6857403dd3f1eeafdb70f45e5b92e4.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-4d0e4ee50f9ead394b9fcd552ae92106.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-4b39c7bfcb8005d9c40b2eac38f3ed56.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-0aaa94e0b48b25617be15c8888555cae.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-a7c646c3420664f87093b1eb3a62bfba.jpg" align="middle"></details>## NeRFFaceSpeech: One-shot Audio-driven 3D Talking Head Synthesis via Generative Prior **Authors:Gihoon Kim, Kwanggyoon Seo, Sihun Cha, Junyong Noh** Audio-driven talking head generation is advancing from 2D to 3D content. Notably, Neural Radiance Field (NeRF) is in the spotlight as a means to synthesize high-quality 3D talking head outputs. Unfortunately, this NeRF-based approach typically requires a large number of paired audio-visual data for each identity, thereby limiting the scalability of the method. Although there have been attempts to generate audio-driven 3D talking head animations with a single image, the results are often unsatisfactory due to insufficient information on obscured regions in the image. In this paper, we mainly focus on addressing the overlooked aspect of 3D consistency in the one-shot, audio-driven domain, where facial animations are synthesized primarily in front-facing perspectives. We propose a novel method, NeRFFaceSpeech, which enables to produce high-quality 3D-aware talking head. Using prior knowledge of generative models combined with NeRF, our method can craft a 3D-consistent facial feature space corresponding to a single image. Our spatial synchronization method employs audio-correlated vertex dynamics of a parametric face model to transform static image features into dynamic visuals through ray deformation, ensuring realistic 3D facial motion. Moreover, we introduce LipaintNet that can replenish the lacking information in the inner-mouth area, which can not be obtained from a given single image. The network is trained in a self-supervised manner by utilizing the generative capabilities without additional data. The comprehensive experiments demonstrate the superiority of our method in generating audio-driven talking heads from a single image with enhanced 3D consistency compared to previous approaches. In addition, we introduce a quantitative way of measuring the robustness of a model against pose changes for the first time, which has been possible only qualitatively. [PDF](http://arxiv.org/abs/2405.05749v2) 11 pages, 5 figures **Summary** 通过解决单张图像音频驱动3D Talking Head生成中的3D一致性问题，NeRFFaceSpeech 方法能够生成高质量 3D感知的 Talking Head。 **Key Takeaways** - 解决单张图像音频驱动 3D Talking Head 生成的 3D 一致性问题。 - 使用生成模型的先验知识与 NeRF 相结合，构建对应于单张图像的 3D 一致的面部特征空间。 - 引入空间同步方法，利用参数化人脸模型的音频相关顶点动态，通过射线变形将静态图像特征转换为动态视觉效果，确保逼真的 3D 面部运动。 - 引入 LipaintNet 补充内嘴区域中缺失的信息，该信息无法从给定的单张图像中获得。 - 以自监督的方式训练网络，利用生成能力而无需额外数据。 - 提出一种定量方法来衡量模型对姿势变化的鲁棒性，这在以前只能通过定性方式进行。 **[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: NeRFFaceSpeech: 一次性音频驱动 3D 说话人头部合成，通过生成先验</p></li><li><p>Authors: Gihoon Kim, Kwanggyoon Seo, Sihun Cha, Junyong Noh</p></li><li><p>Affiliation: 首尔国立大学</p></li><li><p>Keywords: 音频驱动, 3D 说话人头部, 神经辐射场, 生成先验, 一次性学习</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2405.05749, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 音频驱动说话人头部生成正从 2D 内容转向 3D 内容。值得注意的是，神经辐射场 (NeRF) 作为合成高质量 3D 说话人头部输出的一种手段而备受关注。不幸的是，这种基于 NeRF 的方法通常需要大量配对的每个身份的音频视觉数据，从而限制了该方法的可扩展性。尽管已经尝试使用单张图像生成音频驱动的 3D 说话人头部动画，但由于图像中遮挡区域的信息不足，结果通常不令人满意。在本文中，我们主要关注解决一次性音频驱动域中被忽视的 3D 一致性方面，其中面部动画主要在正面视角合成。</p><p>(2): 现有的方法： - 基于 NeRF 的方法通常需要大量配对的每个身份的音频视觉数据。 - 使用单张图像生成音频驱动的 3D 说话人头部动画的方法由于图像中遮挡区域的信息不足，结果通常不令人满意。</p><p>问题： - 可扩展性受限。 - 3D 一致性不足。</p><p>(3): 本文提出的研究方法： - 提出了一种新方法 NeRFFaceSpeech，它能够生成高质量的 3D 感知说话人头部。 - 使用生成模型的先验知识与 NeRF 相结合，我们的方法可以构建一个与单张图像相对应的 3D 一致的面部特征空间。 - 我们的空间同步方法采用参数化人脸模型的音频相关顶点动态，通过光线变形将静态图像特征转换为动态视觉效果，确保逼真的 3D 面部运动。 - 此外，我们引入了 LipaintNet，它可以补充单张给定图像中无法获得的内口区域中缺少的信息。该网络以自监督的方式进行训练，利用生成能力而无需额外数据。</p><p>(4): 在任务和性能上，本文方法取得了以下成就： - 全面的实验表明，与以前的方法相比，我们的方法在从单张图像生成音频驱动的说话人头部方面具有出色的性能，并增强了 3D 一致性。 - 此外，我们首次引入了一种衡量模型对姿势变化鲁棒性的定量方法，而以前只能定性地进行。</p><ol><li>Methods:</li></ol><p>(1): 提出 NeRFFaceSpeech 方法，该方法结合了生成模型的先验知识与 NeRF，构建与单张图像相对应的 3D 一致的面部特征空间；</p><p>(2): 采用参数化人脸模型的音频相关顶点动态，通过光线变形将静态图像特征转换为动态视觉效果，确保逼真的 3D 面部运动；</p><p>(3): 引入 LipaintNet，补充单张给定图像中无法获得的内口区域中缺少的信息，该网络以自监督的方式进行训练，利用生成能力而无需额外数据；</p><p>(4): 设计定量方法衡量模型对姿势变化的鲁棒性。</p><ol><li>结论：</li></ol><p>（1）：本文提出了 NeRFFaceSpeech，一种通过利用生成先验构建和操纵 3D 特征，从单幅图像生成 3D 感知音频驱动说话人头部动画的新方法；</p><p>（2）：创新点：将生成模型先验与神经辐射场相结合，构建与单幅图像相对应的 3D 一致的面部特征空间；通过光线变形，采用参数化人脸模型的音频相关顶点动态，将静态图像特征转换为动态视觉效果，确保逼真的 3D 面部运动；引入了 LipaintNet，一个自监督学习框架，利用生成模型的能力合成隐藏的内口区域，补充变形场以产生可行结果；设计了定量方法来衡量模型对姿势变化的鲁棒性。性能：与以前的方法相比，我们的方法在从单幅图像生成音频驱动的说话人头部方面具有出色的性能，并增强了 3D 一致性；首次引入了一种衡量模型对姿势变化鲁棒性的定量方法，而以前只能定性地进行。工作量：本文提出的方法需要较大的计算资源和较长的训练时间。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-2a60d3f8bc167b5a06ffeda10f57dfc8.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-d422ea4050244e053b7e4851bb4a9ade.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-e65d136edc8fc7443ae44525f2b6db77.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-4e5fb53c0c038366d8c74e34f9bffdfb.jpg" align="middle"></details>## Tactile-Augmented Radiance Fields **Authors:Yiming Dou, Fengyu Yang, Yi Liu, Antonio Loquercio, Andrew Owens** We present a scene representation, which we call a tactile-augmented radiance field (TaRF), that brings vision and touch into a shared 3D space. This representation can be used to estimate the visual and tactile signals for a given 3D position within a scene. We capture a scene's TaRF from a collection of photos and sparsely sampled touch probes. Our approach makes use of two insights: (i) common vision-based touch sensors are built on ordinary cameras and thus can be registered to images using methods from multi-view geometry, and (ii) visually and structurally similar regions of a scene share the same tactile features. We use these insights to register touch signals to a captured visual scene, and to train a conditional diffusion model that, provided with an RGB-D image rendered from a neural radiance field, generates its corresponding tactile signal. To evaluate our approach, we collect a dataset of TaRFs. This dataset contains more touch samples than previous real-world datasets, and it provides spatially aligned visual signals for each captured touch signal. We demonstrate the accuracy of our cross-modal generative model and the utility of the captured visual-tactile data on several downstream tasks. Project page: https://dou-yiming.github.io/TaRF [PDF](http://arxiv.org/abs/2405.04534v1) CVPR 2024, Project page: https://dou-yiming.github.io/TaRF, Code: https://github.com/Dou-Yiming/TaRF/ **Summary** 视觉触觉增强辐射场将视觉和触觉带入共享的 3D 空间，能够估计场景中给定 3D 位置的视觉和触觉信号。 **Key Takeaways** - 视觉触觉增强辐射场 (TaRF) 结合了视觉和触觉信号，用于估计场景中给定 3D 位置的视觉和触觉信号。 - TaRF 由照片和稀疏采样的触觉探针采集而成。 - 视觉触觉增强辐射场利用了视觉触觉传感器可与图像配准以及场景中视觉和结构相似的区域共享相同触觉特征的见解。 - 触觉信号通过配准和条件扩散模型与捕获的视觉场景相关联。 - TaRF 数据集包含比以往真实世界数据集更多的触觉样本，并为每个捕获的触觉信号提供了空间对齐的视觉信号。 - 跨模态生成模型的准确性已得到验证，且已在多项下游任务中证明了捕获的视觉触觉数据的实用性。 - 项目主页：https://dou-yiming.github.io/TaRF **[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: 触觉增强辐射场：一种用于跨模态感知和生成的新型场景表示</p></li><li><p>Authors: Douyi Ming, Srinath Sridhar, Jiajun Wu, Angjoo Kanazawa, Peter Anderson, Wojciech Matusik, Jonathan Ragan-Kelley</p></li><li><p>Affiliation: 麻省理工学院</p></li><li><p>Keywords: Cross-modal perception, generative models, multi-view geometry, neural radiance fields, tactile sensing, vision</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2301.09422, Github: https://github.com/douyiming/TaRF</p></li><li><p>Summary:</p></li></ol><p>(1): 本文的研究背景是视觉和触觉感知在共享的 3D 空间中的表示问题。</p><p>(2): 过去的方法通常是将视觉和触觉信号视为独立的模态，这限制了跨模态感知和生成的任务。</p><p>(3): 本文提出了一种新的场景表示形式——触觉增强辐射场 (TaRF)，它将视觉和触觉信号统一到一个 3D 空间中。TaRF 的构建利用了基于视觉的触觉传感器与图像之间的几何对应关系，以及场景中视觉和结构相似区域具有相同触觉特征的假设。</p><p>(4): 在 TaRF 数据集上，本文提出的方法在跨模态生成、触觉信号估计和触觉引导的视觉探索等任务上取得了良好的性能，证明了其在跨模态感知和生成方面的有效性。</p><ol><li>方法：</li></ol><p>（1）：构建视觉和触觉增强辐射场（TaRF），将视觉和触觉信号统一到一个 3D 空间中；</p><p>（2）：通过视觉-触觉对应关系和场景中视觉和结构相似区域具有相同触觉特征的假设，建立 TaRF；</p><p>（3）：使用基于视觉的触觉传感器和图像之间的几何对应关系，估计 TaRF 中的视觉和触觉信号；</p><p>（4）：利用生成模型，估计场景中其他位置的触觉信号；</p><p>（5）：使用条件潜在扩散模型，从渲染的视觉信号中预测触觉信号；</p><p>（6）：收集包含 19.3k 个图像对的视觉-触觉数据集，用于训练和评估 TaRF。</p><ol><li>结论：<pre><code>            （1）：本文提出了一种新的场景表示形式——触觉增强辐射场（TaRF），首次将视觉和触觉信号统一到一个共享的 3D 空间中，为跨模态感知和生成提供了新的可能性。

            （2）：创新点：提出了一种将视觉和触觉信号统一到一个 3D 空间中的场景表示形式 TaRF；提出了一种基于视觉-触觉对应关系和场景中视觉和结构相似区域具有相同触觉特征的假设，建立 TaRF 的方法；提出了一种使用生成模型，估计场景中其他位置的触觉信号的方法。性能：在跨模态生成、触觉信号估计和触觉引导的视觉探索等任务上取得了良好的性能；收集了包含 19.3k 个图像对的视觉-触觉数据集，为 TaRF 的训练和评估提供了丰富的素材。 workload：TaRF 的构建需要基于视觉的触觉传感器和图像之间的几何对应关系，以及场景中视觉和结构相似区域具有相同触觉特征的假设，这在某些情况下可能存在挑战；TaRF 的训练需要大量的视觉-触觉数据，这可能会增加数据收集和标注的工作量。
</code></pre></li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-5322ab124785e1ed8207592748379b4a.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-2a5022ff2c6b665ce2b96a8b7b9f166a.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-929d0d52fcc6b94f08fe05a010b4ea04.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-288d198e1ad4f685777631680ccf4209.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-59b2afe338ddf3888c68d0443ec0d04f.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-7ec71c15419dcda442892e2bc1a105da.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-432dac236eec2115922c4f0698f51eec.jpg" align="middle"></details><h2 id="DistGrid-Scalable-Scene-Reconstruction-with-Distributed-Multi-resolution-Hash-Grid"><a href="#DistGrid-Scalable-Scene-Reconstruction-with-Distributed-Multi-resolution-Hash-Grid" class="headerlink" title="DistGrid: Scalable Scene Reconstruction with Distributed   Multi-resolution Hash Grid"></a>DistGrid: Scalable Scene Reconstruction with Distributed Multi-resolution Hash Grid</h2><p><strong>Authors:Sidun Liu, Peng Qiao, Zongxin Ye, Wenyu Li, Yong Dou</strong></p><p>Neural Radiance Field~(NeRF) achieves extremely high quality in object-scaled and indoor scene reconstruction. However, there exist some challenges when reconstructing large-scale scenes. MLP-based NeRFs suffer from limited network capacity, while volume-based NeRFs are heavily memory-consuming when the scene resolution increases. Recent approaches propose to geographically partition the scene and learn each sub-region using an individual NeRF. Such partitioning strategies help volume-based NeRF exceed the single GPU memory limit and scale to larger scenes. However, this approach requires multiple background NeRF to handle out-of-partition rays, which leads to redundancy of learning. Inspired by the fact that the background of current partition is the foreground of adjacent partition, we propose a scalable scene reconstruction method based on joint Multi-resolution Hash Grids, named DistGrid. In this method, the scene is divided into multiple closely-paved yet non-overlapped Axis-Aligned Bounding Boxes, and a novel segmented volume rendering method is proposed to handle cross-boundary rays, thereby eliminating the need for background NeRFs. The experiments demonstrate that our method outperforms existing methods on all evaluated large-scale scenes, and provides visually plausible scene reconstruction. The scalability of our method on reconstruction quality is further evaluated qualitatively and quantitatively.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.04416v2">PDF</a> Originally submitted to Siggraph Asia 2023</p><p><strong>Summary</strong><br>大规模场景只需用单一NeRF，通过多级Hash网格，而无需单独的背景NeRF，即可实现场景重建。</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF 在对象和室内场景重建中表现出色，但在重建大型场景时存在问题。</li><li>基于 MLP 的 NeRF 网络容量有限，而基于体积的 NeRF 会随着场景分辨率的增加占用大量内存。</li><li>最近的方法将场景地理分区，并使用单独的 NeRF 学习每个分区。</li><li>这有助于基于体积的 NeRF 突破单 GPU 内存限制，并扩展到更大的场景。</li><li>但这种方法需要多个背景 NeRF 来处理分区外的光线，这导致学习冗余。</li><li>本文提出了 DistGrid，基于多级散列网格的分布式场景重建方法。</li><li>该方法将场景划分为多个紧密排列但不重叠的轴对齐包围盒，并提出了一种分割体积渲染方法来处理跨边界光线，从而消除了对背景 NeRF 的需求。</li><li>实验表明，该方法在所有评估的大型场景上都优于现有方法，并提供了视觉上合理的效果。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: DistGrid: 基于分布式多分辨率哈希网格的大规模场景重建</p></li><li><p>Authors: Sidun Liu, Peng Qiao, Zongxin Ye, Wenyu Li, Yong Dou</p></li><li><p>Affiliation: 国防科技大学</p></li><li><p>Keywords: Neural Radiance Field, Distributed Algorithm, Large-scale Scene Reconstruction, Neural Rendering</p></li><li><p>Urls: Paper: https://arxiv.org/pdf/2405.04416.pdf, Github: None</p></li><li><p>Summary:</p><p>(1): 神经渲染技术自 NeRF 提出以来取得了重大进展，NeRF 旨在解决新视角合成任务。它基于体积渲染，使用称为神经场的多分层感知器（MLP）隐式表示场景。该神经场接受 3D 坐标和观察方向作为输入，并预测其对应的密度和颜色。NeRF 在新视角合成任务上产生了令人印象深刻的视觉质量。其学习的神经场也可用于场景重建。</p><p>(2): NeRF 的后续工作旨在提高效率和质量。一种方法是将场景地理分区，并使用单独的 NeRF 学习每个子区域。这种分区策略帮助基于体积的 NeRF 超过单个 GPU 的内存限制，并扩展到更大的场景。但是，这种方法需要多个背景 NeRF 来处理分区外的光线，这导致了学习的冗余。</p><p>(3): 本文提出了一种基于联合多分辨率哈希网格的可扩展场景重建方法，称为 DistGrid。在此方法中，场景被划分为多个紧密相邻但非重叠的轴对齐边界框，并提出了一种新颖的分段体积渲染方法来处理跨边界光线，从而消除了对背景 NeRF 的需求。</p><p>(4): 实验表明，该方法在所有评估的大规模场景上都优于现有方法，并提供了视觉上合理的场景重建。该方法在重建质量上的可扩展性还通过定性和定量的方式进行了进一步评估。</p></li><li><p>方法：</p><p>(1): 本文提出了一种基于联合多分辨率哈希网格的可扩展场景重建方法，称为 DistGrid。</p><p>(2): DistGrid 将场景划分为多个紧密相邻但非重叠的轴对齐边界框 (AABB)，并使用新颖的分段体积渲染方法来处理跨边界光线，从而消除了对背景 NeRF 的需求。</p><p>(3): DistGrid 采用两级级联结构，其中细粒度 NeRF 使用内层边界框作为其边界框，而粗粒度 NeRF 使用外层边界框。</p><p>(4): DistGrid 使用分段体积渲染方法来处理跨区域光线，该方法将体积渲染积分分解为多个部分，并使用部分颜色和部分透射率来计算渲染颜色和最终透射率。</p></li><li><p>结论：</p><pre><code>            (1):本文提出了 DistGrid，一种基于联合多分辨率哈希网格的可扩展场景重建方法，该方法在视觉质量和效率方面都优于现有方法。

            (2):创新点：提出了一种新颖的分段体积渲染方法来处理跨边界光线，无需背景 NeRF，提高了效率；采用两级级联结构，细粒度 NeRF 和粗粒度 NeRF 协同工作，提高了重建质量。

            性能：在所有评估的大规模场景上都优于现有方法，并提供了视觉上合理的场景重建。

            工作量：与现有方法相比，DistGrid 在重建质量上的可扩展性得到了定性和定量的方式的进一步评估。
</code></pre></li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-c94130b609d19ed2e706304ecfbbdde4.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-c20db83d0a269f077a389b38e5b01349.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-1e4dd6ce979c2f2e18008fc25e085162.jpg" align="middle"></details><h2 id="Blending-Distributed-NeRFs-with-Tri-stage-Robust-Pose-Optimization"><a href="#Blending-Distributed-NeRFs-with-Tri-stage-Robust-Pose-Optimization" class="headerlink" title="Blending Distributed NeRFs with Tri-stage Robust Pose Optimization"></a>Blending Distributed NeRFs with Tri-stage Robust Pose Optimization</h2><p><strong>Authors:Baijun Ye, Caiyun Liu, Xiaoyu Ye, Yuantao Chen, Yuhai Wang, Zike Yan, Yongliang Shi, Hao Zhao, Guyue Zhou</strong></p><p>Due to the limited model capacity, leveraging distributed Neural Radiance Fields (NeRFs) for modeling extensive urban environments has become a necessity. However, current distributed NeRF registration approaches encounter aliasing artifacts, arising from discrepancies in rendering resolutions and suboptimal pose precision. These factors collectively deteriorate the fidelity of pose estimation within NeRF frameworks, resulting in occlusion artifacts during the NeRF blending stage. In this paper, we present a distributed NeRF system with tri-stage pose optimization. In the first stage, precise poses of images are achieved by bundle adjusting Mip-NeRF 360 with a coarse-to-fine strategy. In the second stage, we incorporate the inverting Mip-NeRF 360, coupled with the truncated dynamic low-pass filter, to enable the achievement of robust and precise poses, termed Frame2Model optimization. On top of this, we obtain a coarse transformation between NeRFs in different coordinate systems. In the third stage, we fine-tune the transformation between NeRFs by Model2Model pose optimization. After obtaining precise transformation parameters, we proceed to implement NeRF blending, showcasing superior performance metrics in both real-world and simulation scenarios. Codes and data will be publicly available at <a target="_blank" rel="noopener" href="https://github.com/boilcy/Distributed-NeRF">https://github.com/boilcy/Distributed-NeRF</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.02880v1">PDF</a></p><p><strong>Summary</strong><br>使用三阶段姿态优化对分布式NeRF进行精确对齐，以缓解建模大规模城市环境时出现的混叠伪影和姿态精度不足的问题。</p><p><strong>Key Takeaways</strong></p><ul><li>分布式NeRF建模城市环境面临混叠伪影和姿态精度问题。</li><li>采用分阶段姿态优化解决问题，包括Mip-NeRF 360束调整、反向Mip-NeRF 360和Frame2Model优化。</li><li>利用Model2Model优化进一步细化不同NeRF之间的转换。</li><li>精确的姿态优化有效消除NeRF融合中的遮挡伪影。</li><li>在真实和模拟场景中展示出优越的NeRF融合性能。</li><li>代码和数据将在GitHub上公开。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>题目：基于三阶段鲁棒位姿优化融合分布式NeRFs</p></li><li><p>作者：Baijun Ye∗1,2, Caiyun Liu∗1, Xiaoyu Ye1,2, Yuantao Chen1,3, Yuhai Wang4, Zike Yan1, Yongliang Shi1†, Hao Zhao1, Guyue Zhou1</p></li><li><p>第一作者单位：清华大学人工智能产业研究院（AIR）</p></li><li><p>关键词：分布式NeRF、位姿优化、NeRF融合、Mip-NeRF 360、iNeRF</p></li><li><p>论文链接：https://arxiv.org/abs/2405.02880 Github：None</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景： 在大规模场景建模领域，NeRF因其能够在保持紧凑模型结构的同时实现逼真的渲染而备受关注。然而，当前的分布式NeRF配准方法存在混叠伪影，这源于渲染分辨率差异和次优位姿精度。这些因素共同降低了NeRF框架内位姿估计的保真度，导致NeRF融合阶段出现遮挡伪影。</p><p>（2）：以往方法及其问题： 以往方法主要有两种：批处理学习和增量学习。批处理学习需要大量的计算资源，而增量学习容易出现遗忘问题。此外，当前使用显式编码方法（如网格和八叉树）进行实时性能的NeRF方法，面临着随着场景规模的增加，编码组件呈指数级扩展的挑战，导致存储需求大幅增加。</p><p>（3）：本文提出的研究方法： 本文提出了一种基于三阶段鲁棒位姿优化的分布式NeRF框架。在第一阶段，通过捆绑调整Mip-NeRF 360并采用由粗到精的策略，实现了图像的精确位姿。在第二阶段，借鉴LATITUDE，利用截断动态低通滤波（TDLF）的原理对反向Mip-NeRF 360进行了优化，称为iMNeRF。此方法类似于模糊图像以使优化过程更加鲁棒，从而实现帧到模型的位姿优化。随后，采用协视图区域检索方法来搜索不同NeRF实例中最相似的图像，进而确定其关联的位姿。给定关联的位姿，利用iMNeRF通过渲染图像和观察图像之间的光度损失来优化这些位姿，从而获得可靠的帧到模型转换。在第三阶段，通过不同的帧到模型转换获得了NeRF之间粗略的模型到模型转换。然后，将不同的NeRF模型投影到一个统一的坐标系中，并使用渲染图像作为观测值进一步优化NeRF之间的相对转换，即通过模型到模型优化来获得NeRF之间的精确转换。</p><p>（4）：方法在什么任务上取得了怎样的性能： 利用三阶段位姿优化，实现了NeRF融合并获得了更好的性能。为了验证本文方法，同时发布了真实世界和模拟数据集，展示了本文方法在性能上的优越性。</p><ol><li>方法：</li></ol><p>（1）：提出了一种基于三阶段鲁棒位姿优化的分布式NeRF框架；</p><p>（2）：第一阶段，通过捆绑调整Mip-NeRF 360并采用由粗到精的策略，实现了图像的精确位姿；</p><p>（3）：第二阶段，借鉴LATITUDE，利用截断动态低通滤波（TDLF）的原理对反向Mip-NeRF 360进行了优化，称为iMNeRF；</p><p>（4）：给定关联的位姿，利用iMNeRF通过渲染图像和观察图像之间的光度损失来优化这些位姿，从而获得可靠的帧到模型转换；</p><p>（5）：第三阶段，通过不同的帧到模型转换获得了NeRF之间粗略的模型到模型转换；</p><p>（6）：将不同的NeRF模型投影到一个统一的坐标系中，并使用渲染图像作为观测值进一步优化NeRF之间的相对转换，即通过模型到模型优化来获得NeRF之间的精确转换。</p><ol><li>结论：</li></ol><p>（1）：本文提出了一种基于三阶段鲁棒位姿优化的分布式NeRF框架，解决了当前分布式NeRF配准方法中存在的混叠伪影问题，提高了NeRF融合的保真度；</p><p>（2）：创新点：提出了三阶段鲁棒位姿优化方法，包括图像精确位姿估计、帧到模型位姿优化和模型到模型位姿优化；性能：在真实世界和模拟数据集上验证了本文方法的优越性；工作量：本文方法需要额外的计算资源进行位姿优化，但可以实现更好的NeRF融合效果。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-19421ede11ee24694424a6e2329cbd82.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-c3320829396c5cee81241227bf678e63.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-4f9d4086bd525c9621ef04e939f0ee92.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-cce8d4e7f7b76ee03713ad33dc0da96e.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-f2f0b44124a450d85749c232dce8a310.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-8b718f4974ac6aac1d1a62a7beb2d681.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-767f74cc31852d4c6f28717ac5e03f47.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-5929c8babaf3c740cf4c2d6f2886bf8e.jpg" align="middle"></details>## TK-Planes: Tiered K-Planes with High Dimensional Feature Vectors for Dynamic UAV-based Scenes **Authors:Christopher Maxey, Jaehoon Choi, Yonghan Lee, Hyungtae Lee, Dinesh Manocha, Heesung Kwon** In this paper, we present a new approach to bridge the domain gap between synthetic and real-world data for un- manned aerial vehicle (UAV)-based perception. Our formu- lation is designed for dynamic scenes, consisting of moving objects or human actions, where the goal is to recognize the pose or actions. We propose an extension of K-Planes Neural Radiance Field (NeRF), wherein our algorithm stores a set of tiered feature vectors. The tiered feature vectors are generated to effectively model conceptual information about a scene as well as an image decoder that transforms output feature maps into RGB images. Our technique leverages the information amongst both static and dynamic objects within a scene and is able to capture salient scene attributes of high altitude videos. We evaluate its performance on challenging datasets, including Okutama Action and UG2, and observe considerable improvement in accuracy over state of the art aerial perception algorithms. [PDF](http://arxiv.org/abs/2405.02762v1) 8 pages, submitted to IROS2024 **Summary** 无人机实时感知任务中，该文将静态特征与动态特征相结合，提高了神经辐射场（NeRF）模型在合成和真实世界数据之间的域适应性。 **Key Takeaways** - 提出了一种分层特征向量的神经辐射场（NeRF）扩展版本，用于捕获动态场景中的概念信息。 - 扩展的NeRF模型通过图像解码器将输出特征图转换为RGB图像。 - 该模型同时利用静态和动态对象的信息，从而捕获高空视频中的显著场景属性。 - 将静态和动态特征相结合，提高了模型在合成和真实世界数据之间的域适应性。 - 在Okutama Action和UG2等具有挑战性的数据集上评估了该模型的性能。 - 与最先进的无人机感知算法相比，该模型在准确性方面有显著提高。 - 该模型可以应用于无人机实时感知任务，例如动作识别和姿态估计。 **[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: TK-Planes：具有高维特征向量的分层 K-Planes</p></li><li><p>Authors: Christopher Maxey, Jaehoon Choi, Yonghan Lee, Hyungtae Lee, Dinesh Manocha, Heesung Kwon</p></li><li><p>Affiliation: 美国陆军研究实验室</p></li><li><p>Keywords: Neural Radiance Fields, Synthetic Data, UAV Perception, Dynamic Scenes, Feature Vectors</p></li><li><p>Urls: https://arxiv.org/abs/2405.02762, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 本文研究背景是合成数据在无人机感知中的应用，特别是动态场景的识别，如姿势或动作识别。</p><p>(2): 过去的方法主要基于 K-Planes 神经辐射场，但存在问题：动态对象建模困难、静态和动态元素分离困难、动态对象稀疏、姿态多样性受限。</p><p>(3): 本文提出的研究方法是 TK-Planes，一种分层 K-Planes 算法，输出和操作特征向量而不是 RGB 像素值。这些特征向量可以存储场景中特定对象或位置的概念信息，并为多个相应的相机光线输出时形成特征图，然后解码为最终图像。</p><p>(4): 该方法在 Okutama Action 和 UG2 等具有挑战性的无人机数据集上进行了评估，结果表明，与现有算法相比，基于 TK-Planes 的 NeRF 模型可以生成补充的无人机数据，从而提高动态场景的整体识别准确性。</p><ol><li>方法：</li></ol><p>（1）：使用 NeRF（神经辐射场）在特征空间中生成新视角，以更好地捕获场景中的动态对象，如人物。</p><p>（2）：采用基于网格的 NeRF，网格类似于 K-Planes，但存储的特征向量不直接编码 RGB 值，而是编码场景中的更高层次概念信息，如地面、树木和人物。</p><p>（3）：使用分层网格在特征空间中操作，将场景分解为静态和动态特征，并使用图像解码器将特征图解码为最终图像。</p><p>（4）：在具有挑战性的无人机数据集上评估模型，结果表明基于 TK-Planes 的 NeRF 模型可以生成补充的无人机数据，从而提高动态场景的识别准确性。</p><ol><li>结论：<pre><code>            (1):本文提出的分层 K-Planes（TK-Planes）算法，通过在特征空间中操作特征向量，有效地解决了合成数据在无人机感知中的动态场景识别问题。

            (2):创新点：TK-Planes 算法将场景分解为静态和动态特征，并使用分层网格在特征空间中操作，从而更好地捕获动态对象；性能：在 Okutama Action 和 UG2 等具有挑战性的无人机数据集上的评估结果表明，基于 TK-Planes 的 NeRF 模型可以生成补充的无人机数据，从而提高动态场景的识别准确性；工作量：TK-Planes 算法的实现和在无人机数据集上的评估需要一定的技术投入和计算资源。
</code></pre></li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-49ebb6a345fe5bed0d70468dcdf8fd84.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-63139de16b603f02b54ce2804a9bad9f.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-af95031ec70f26ba5024a7788a09ddff.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-c99084999876941f9618dfb5d99f367b.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-63f6561485ccf42232344f25cf43bc8a.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-a606ade4cb1fcbf9e4da91086ed92ad1.jpg" align="middle"></details><h2 id="ActiveNeuS-Active-3D-Reconstruction-using-Neural-Implicit-Surface-Uncertainty"><a href="#ActiveNeuS-Active-3D-Reconstruction-using-Neural-Implicit-Surface-Uncertainty" class="headerlink" title="ActiveNeuS: Active 3D Reconstruction using Neural Implicit Surface   Uncertainty"></a>ActiveNeuS: Active 3D Reconstruction using Neural Implicit Surface Uncertainty</h2><p><strong>Authors:Hyunseo Kim, Hyeonseo Yang, Taekyung Kim, YoonSung Kim, Jin-Hwa Kim, Byoung-Tak Zhang</strong></p><p>Active learning in 3D scene reconstruction has been widely studied, as selecting informative training views is critical for the reconstruction. Recently, Neural Radiance Fields (NeRF) variants have shown performance increases in active 3D reconstruction using image rendering or geometric uncertainty. However, the simultaneous consideration of both uncertainties in selecting informative views remains unexplored, while utilizing different types of uncertainty can reduce the bias that arises in the early training stage with sparse inputs. In this paper, we propose ActiveNeuS, which evaluates candidate views considering both uncertainties. ActiveNeuS provides a way to accumulate image rendering uncertainty while avoiding the bias that the estimated densities can introduce. ActiveNeuS computes the neural implicit surface uncertainty, providing the color uncertainty along with the surface information. It efficiently handles the bias by using the surface information and a grid, enabling the fast selection of diverse viewpoints. Our method outperforms previous works on popular datasets, Blender and DTU, showing that the views selected by ActiveNeuS significantly improve performance.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.02568v1">PDF</a></p><p><strong>Summary</strong><br>主动神经重建同时考虑图像渲染和几何不确定性，以选择信息丰富的训练视图来提高 3D 场景重建性能。</p><p><strong>Key Takeaways</strong></p><ul><li>主动学习在 3D 场景重建中至关重要。</li><li>NeRF 变体使用图像渲染或几何不确定性提高了主动 3D 重建的性能。</li><li>ActiveNeuS 同时考虑了图像渲染和几何不确定性来选择信息丰富的视图。</li><li>ActiveNeuS 积累图像渲染不确定性，同时避免估计密度引入的偏差。</li><li>ActiveNeuS 计算神经隐式表面不确定性，提供颜色不确定性和表面信息。</li><li>ActiveNeuS 使用表面信息和网格有效处理偏差，从而快速选择多样化的视点。</li><li>ActiveNeuS 在 Blender 和 DTU 数据集上的表现优于以往的工作，表明 ActiveNeuS 选择的视图显著提高了性能。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>标题：ActiveNeuS：基于神经隐式曲面不确定性的主动三维重建</li><p></p><p></p><li>作者：Hyunseo Kim, Hyeonseo Yang, Taekyung Kim, YoonSung Kim, Jin-Hwa Kim, Byoung-Tak Zhang</li><p></p><p></p><li>单位：首尔大学</li><p></p><p></p><li>关键词：主动学习、神经隐式曲面不确定性、曲面网格</li><p></p><p></p><li>论文链接：https://arxiv.org/pdf/2405.02568.pdf，Github代码链接：无</li><p></p><p></p><li>摘要：</li><br>&lt;/ol&gt;<p></p><p>（1）研究背景：三维场景重建中的主动学习已被广泛研究，因为选择有信息的训练视图对于重建至关重要。最近，神经辐射场（NeRF）变体在使用图像渲染或几何不确定性进行主动三维重建方面表现出性能提升。然而，在选择信息视图时同时考虑两种不确定性仍然未被探索，而利用不同类型的不确定性可以减少在早期训练阶段由于输入稀疏而产生的偏差。</p><p>（2）过去的方法及其问题：传统的NeRF主动学习方法通常估计其输出中的不确定性：三维点的密度和颜色。Martin等人和Pan等人估计了颜色预测中的不确定性，方法是将颜色建模为高斯概率分布。然而，这些方法在早期训练阶段容易受到密度估计偏差的影响，这可能会导致信息视图选择不佳。</p><p>（3）提出的研究方法：本文提出了ActiveNeuS，它在评估候选视图时考虑了图像渲染不确定性和神经隐式曲面不确定性。ActiveNeuS提供了一种积累图像渲染不确定性的方法，同时避免了估计密度可能引入的偏差。ActiveNeuS计算神经隐式曲面不确定性，提供颜色不确定性和曲面信息。它通过使用曲面信息和网格有效地处理偏差，从而能够快速选择不同的视点。</p><p>（4）任务和性能：在流行的数据集Blender和DTU上，ActiveNeuS的性能优于以前的工作，表明ActiveNeuS选择的视图显著提高了性能。这些结果支持了作者的目标，即开发一种主动学习方法，该方法可以有效地选择信息视图以提高三维重建的性能。</p><ol><li><p>方法：</p><pre><code>            (1): ActiveNeuS 提出了一种新的采集函数，该函数结合了几何重建和图像渲染的视角。

            (2): ActiveNeuS 估计颜色预测的不确定性，以获取有关图像渲染质量的信息。

            (3): 采集函数集成了估计的不确定性，同时不丢失几何属性。

            (4): 首先，在第 4.1 节中，我们介绍了我们的采集函数，并解释了在积分过程中如何考虑曲面。

            (5): 在第 4.2 节中，我们定义了 ActiveNeuS 中估计的神经隐式表面不确定性，并描述了如何在采集函数中利用不确定性。

            (6): 然后，为了进行高效且稳健的计算，我们引入了存储曲面信息的曲面网格和选择多个次优视图 (NBV) 的策略（第 4.3 节）。
</code></pre></li><li><p>结论：</p></li></ol><p>（1）：本文提出了一种有效的信息视图选择方法 ActiveNeuS，该方法同时考虑了几何重建和图像渲染的保真度。ActiveNeuS 引入了一种新的采集函数，该函数利用不确定性网格有效且稳健地利用神经隐式曲面不确定性。采集函数通过使用曲面网格并根据曲面的存在应用不同的积分策略来计算神经隐式曲面不确定性的积分。我们展示了 ActiveNeuS 的下一个最佳视图选择，与其他方法相比，它改进了网格重建和图像渲染质量。对于未来的工作，我们建议研究一种有效的方法来连接不同网络的不确定性以进行信息视图选择。此外，将 ActiveNeuS 应用于机器人主动 3D 重建中也很有趣，其中机器人手臂移动并收集数据。</p><p>（2）：创新点：本文提出了一种新的采集函数，该函数同时考虑了图像渲染不确定性和神经隐式曲面不确定性，有效地提高了信息视图的选择。性能：在 Blender 和 DTU 等流行数据集上，ActiveNeuS 的性能优于以前的工作，表明 ActiveNeuS 选择的视图显着提高了性能。工作量：ActiveNeuS 的计算成本相对较高，因为它需要估计神经隐式曲面不确定性并使用曲面网格进行积分。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-0eb6f5097fe2312cfce57f04da637606.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-824537082b5290b565ea1f0da3946351.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-7939474a53153965a29e673124ea4a9e.jpg" align="middle"></details><h2 id="Rip-NeRF-Anti-aliasing-Radiance-Fields-with-Ripmap-Encoded-Platonic-Solids"><a href="#Rip-NeRF-Anti-aliasing-Radiance-Fields-with-Ripmap-Encoded-Platonic-Solids" class="headerlink" title="Rip-NeRF: Anti-aliasing Radiance Fields with Ripmap-Encoded Platonic   Solids"></a>Rip-NeRF: Anti-aliasing Radiance Fields with Ripmap-Encoded Platonic Solids</h2><p><strong>Authors:Junchen Liu, Wenbo Hu, Zhuo Yang, Jianteng Chen, Guoliang Wang, Xiaoxue Chen, Yantong Cai, Huan-ang Gao, Hao Zhao</strong></p><p>Despite significant advancements in Neural Radiance Fields (NeRFs), the renderings may still suffer from aliasing and blurring artifacts, since it remains a fundamental challenge to effectively and efficiently characterize anisotropic areas induced by the cone-casting procedure. This paper introduces a Ripmap-Encoded Platonic Solid representation to precisely and efficiently featurize 3D anisotropic areas, achieving high-fidelity anti-aliasing renderings. Central to our approach are two key components: Platonic Solid Projection and Ripmap encoding. The Platonic Solid Projection factorizes the 3D space onto the unparalleled faces of a certain Platonic solid, such that the anisotropic 3D areas can be projected onto planes with distinguishable characterization. Meanwhile, each face of the Platonic solid is encoded by the Ripmap encoding, which is constructed by anisotropically pre-filtering a learnable feature grid, to enable featurzing the projected anisotropic areas both precisely and efficiently by the anisotropic area-sampling. Extensive experiments on both well-established synthetic datasets and a newly captured real-world dataset demonstrate that our Rip-NeRF attains state-of-the-art rendering quality, particularly excelling in the fine details of repetitive structures and textures, while maintaining relatively swift training times.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.02386v1">PDF</a> SIGGRAPH 2024, Project page: <a target="_blank" rel="noopener" href="https://junchenliu77.github.io/Rip-NeRF">https://junchenliu77.github.io/Rip-NeRF</a> , Code: <a target="_blank" rel="noopener" href="https://github.com/JunchenLiu77/Rip-NeRF">https://github.com/JunchenLiu77/Rip-NeRF</a></p><p><strong>Summary</strong><br>神经辐射场（NeRF）通过极坐标投影将三维各向异性区域射影到平面，再利用Ripmap编码对各平面进行编码，进而解决NeRF抗锯齿渲染中的混叠和模糊问题。</p><p><strong>Key Takeaways</strong></p><ul><li>极坐标投影将三维各向异性区域射影到平面，便于特征化。</li><li>Ripmap编码通过各向异性预滤波可学习特征网格，对射影各向异性区域进行精确高效的特征化。</li><li>方法在合成数据集和实景数据集上均取得了最优渲染质量。</li><li>方法在重复结构和纹理的精细细节上表现优异。</li><li>方法训练时间相对较短。</li><li>该方法依赖于可学习特征网格。</li><li>该方法目前仅适用于静态场景。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p><strong>标题：</strong>Rip-NeRF：基于Ripmap编码的Platonic实体的反走样辐射场</p></li><li><p><strong>作者：</strong>Junchen Liu, Wenbo Hu, Zhuo Yang, Jianteng Chen, Guoliang Wang, Xiaoxue Chen, Yantong Cai, Huan-ang Gao, Hao Zhao</p></li><li><p><strong>第一作者单位：</strong>北京航空航天大学</p></li><li><p><strong>关键词：</strong>神经辐射场（NeRFs）、反走样、Ripmap编码、Platonic实体</p></li><li><p><strong>论文链接：</strong>https://arxiv.org/pdf/2405.02386.pdf，<strong>Github链接：</strong>None</p></li><li><p><strong>摘要：</strong></p><p>(1) <strong>研究背景：</strong>尽管神经辐射场（NeRFs）取得了重大进展，但其渲染结果仍可能存在走样和模糊伪影，因为有效且高效地表征锥形投射过程产生的各向异性区域仍然是一个基本挑战。</p><p>(2) <strong>过去的方法和问题：</strong>现有方法要么无法精确地表征各向异性区域，要么效率低下。</p><p>(3) <strong>本文提出的研究方法：</strong>本文提出了一种基于Ripmap编码的Platonic实体表示，用于精确高效地表征3D各向异性区域，从而实现高保真反走样渲染。该方法的核心是两个关键组件：Platonic实体投影和Ripmap编码。Platonic实体投影将3D空间分解到特定Platonic实体的不平行面上，使得各向异性的3D区域可以投影到具有可区分特征的平面上。同时，Platonic实体的每个面都由Ripmap编码编码，该编码通过各向异性预过滤可学习的特征网格构建，以实现对投影各向异性区域的精确和高效表征。</p><p>(4) <strong>方法性能：</strong>在多尺度Blender数据集和新捕获的真实世界数据集上，该方法在渲染质量和效率方面均优于现有方法。</p></li><li><p>方法：</p></li></ol><p>（1）：基于Ripmap编码的Platonic实体投影，将3D空间分解到Platonic实体的不平行面上，将各向异性的3D区域投影到具有可区分特征的平面上。</p><p>（2）：Platonic实体的每个面由Ripmap编码编码，该编码通过各向异性预过滤可学习的特征网格构建，以实现对投影各向异性区域的精确和高效表征。</p><p>（3）：采用混合表示，包括显式和隐式表示，既能保证效率，又能保证灵活性。</p><p>（4）：采用多采样和面积采样对圆锥截体进行特征化，其中面积采样采用各向异性3D高斯函数对圆锥截体进行表征，再利用提出的Platonic实体投影和Ripmap编码进行特征化。</p><p>（5）：利用MLP估计圆锥截体的颜色和密度，并通过体积渲染渲染像素颜色。</p><p>（6）：采用光度损失函数，对渲染图像和观测图像进行端到端优化。</p><p><strong>8. 结论：</strong></p><p>（1）：本工作提出了一种基于Ripmap编码的Platonic实体表示，用于神经辐射场，称为Rip-NeRF。Rip-NeRF可以渲染高保真抗锯齿图像，同时保持效率，这得益于提出的Platonic实体投影和Ripmap编码。Platonic实体投影将3D空间分解到特定Platonic实体的不平行面上，使得各向异性的3D区域可以投影到具有可区分特征的平面上。Ripmap编码通过各向异性预过滤可学习的特征网格构建，能够对投影的各向异性区域进行精确高效的特征化。这两个组件协同工作，对各向异性的3D区域进行精确高效的特征化。它在合成数据集和真实世界捕捉上都实现了最先进的渲染质量，特别是在结构和纹理的精细细节方面表现出色，这验证了所提出的Platonic实体投影和Ripmap编码的有效性。</p><p>（2）：创新点：提出了基于Ripmap编码的Platonic实体投影和Ripmap编码，用于对各向异性的3D区域进行精确高效的特征化。</p><p>性能：在渲染质量和效率方面均优于现有方法，特别是在精细细节方面表现出色。</p><p>工作量：与现有方法相比，工作量略大，因为需要对Platonic实体投影和Ripmap编码进行预处理。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-7575292a1dc220679b8e9c4fa1e7bb9b.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-d766632fb875e5fe770b7fee4ed1ae6b.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-43f9b3bed63eef23fb98c456f7077574.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-7b2442c515601a7e17fca7b5a8e2166b.jpg" align="middle"></details></ol></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io/2024/05/13/Paper/2024-05-13/NeRF/">https://kedreamix.github.io/2024/05/13/Paper/2024-05-13/NeRF/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/NeRF/">NeRF</a></div><div class="post_share"><div class="social-share" data-image="https://pic1.zhimg.com/v2-7575292a1dc220679b8e9c4fa1e7bb9b.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/05/22/Paper/2024-05-22/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙/虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-b4943be56cf94e02149b50eec541d27f.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">元宇宙/虚拟人</div></div></a></div><div class="next-post pull-right"><a href="/2024/05/13/Paper/2024-05-13/3DGS/" title="3DGS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-b3dd2127ce2dbe6cdafc1b40d9cc2fb2.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">3DGS</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/03/07/Paperscape/SyncTalk/" title="SyncTalk The Devil is in the Synchronization for Talking Head Synthesis"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-a57e0937b2f452009023394a59529dfb.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-07</div><div class="title">SyncTalk The Devil is in the Synchronization for Talking Head Synthesis</div></div></a></div><div><a href="/2024/03/15/Paperscape/Real3D-Portrait/" title="REAL3D-PORTRAIT ONE-SHOT REALISTIC 3D TALKING PORTRAIT SYNTHESIS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-68585b79de5f83b0dfa23304f41b9b98.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-15</div><div class="title">REAL3D-PORTRAIT ONE-SHOT REALISTIC 3D TALKING PORTRAIT SYNTHESIS</div></div></a></div><div><a href="/2024/01/25/Paper/3DGS%20Survey/" title="3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-25</div><div class="title">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</div></div></a></div><div><a href="/2024/03/05/Paperscape/VividTalk/" title="VividTalk One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-8521b04f82075cc27b5e95148dba9792.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-05</div><div class="title">VividTalk One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior</div></div></a></div><div><a href="/2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" title="超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-01</div><div class="title">超赞的数字人生成知识库 Awesome-Talking-Head-Synthesis</div></div></a></div><div><a href="/2024/01/30/Paper/2024-01-30/NeRF/" title="NeRF"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-b6cd7f525efd45ad04614d4ae868c5ff.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-30</div><div class="title">NeRF</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-05-13-%E6%9B%B4%E6%96%B0"><span class="toc-text">2024-05-13 更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#OneTo3D-One-Image-to-Re-editable-Dynamic-3D-Model-and-Video-Generation"><span class="toc-text">OneTo3D: One Image to Re-editable Dynamic 3D Model and Video Generation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DistGrid-Scalable-Scene-Reconstruction-with-Distributed-Multi-resolution-Hash-Grid"><span class="toc-text">DistGrid: Scalable Scene Reconstruction with Distributed Multi-resolution Hash Grid</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Blending-Distributed-NeRFs-with-Tri-stage-Robust-Pose-Optimization"><span class="toc-text">Blending Distributed NeRFs with Tri-stage Robust Pose Optimization</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ActiveNeuS-Active-3D-Reconstruction-using-Neural-Implicit-Surface-Uncertainty"><span class="toc-text">ActiveNeuS: Active 3D Reconstruction using Neural Implicit Surface Uncertainty</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Rip-NeRF-Anti-aliasing-Radiance-Fields-with-Ripmap-Encoded-Platonic-Solids"><span class="toc-text">Rip-NeRF: Anti-aliasing Radiance Fields with Ripmap-Encoded Platonic Solids</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image:url('https://pic1.zhimg.com/v2-7575292a1dc220679b8e9c4fa1e7bb9b.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js").then((()=>{pangu.autoSpacingPage()}))}function panguInit(){panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.1},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script></div><script id="canvas_nest" defer color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><script data-pjax>function butterfly_swiper_injector_config(){var a=document.getElementById("recent-posts");console.log("已挂载butterfly_swiper"),a.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/25/Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="2024/01/25/Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="2024/01/25/Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/17/Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023/12/17/Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="2023/12/17/Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script></body></html>