<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>Diffusion Models | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-05-06  Defect Image Sample Generation With Diffusion Prior for Steel Surface   Defect Recognition"><meta property="og:type" content="article"><meta property="og:title" content="Diffusion Models"><meta property="og:url" content="https://kedreamix.github.io/2024/05/06/Paper/2024-05-06/Diffusion%20Models/index.html"><meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World"><meta property="og:description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-05-06  Defect Image Sample Generation With Diffusion Prior for Steel Surface   Defect Recognition"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://pic1.zhimg.com/v2-d3231e3375af2b14c1e49248519eaebd.jpg"><meta property="article:published_time" content="2024-05-06T10:26:38.000Z"><meta property="article:modified_time" content="2024-05-06T10:26:38.061Z"><meta property="article:author" content="Kedreamix"><meta property="article:tag" content="Diffusion Models"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://pic1.zhimg.com/v2-d3231e3375af2b14c1e49248519eaebd.jpg"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/2024/05/06/Paper/2024-05-06/Diffusion%20Models/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-74LZ5BEQQ1")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!0,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"简"},noticeOutdate:{limitDay:500,position:"top",messagePrev:"It has been",messageNext:"days since the last update, the content of the article may be outdated."},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:200},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!0,post:!0},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!0,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"Diffusion Models",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-05-06 18:26:38"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach((e=>{n.setAttribute(e,t[e])})),document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)})),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=24?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 7.0.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml"></head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{const d=document.getElementById("loading-box"),e=document.body,o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l(),window.addEventListener("load",(()=>{o()}))})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">166</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url('https://pic1.zhimg.com/v2-d3231e3375af2b14c1e49248519eaebd.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Diffusion Models</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-05-06T10:26:38.000Z" title="发表于 2024-05-06 18:26:38">2024-05-06</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-05-06T10:26:38.061Z" title="更新于 2024-05-06 18:26:38">2024-05-06</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">12.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>42分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="Diffusion Models"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-05-06-更新"><a href="#2024-05-06-更新" class="headerlink" title="2024-05-06 更新"></a>2024-05-06 更新</h1><h2 id="Defect-Image-Sample-Generation-With-Diffusion-Prior-for-Steel-Surface-Defect-Recognition"><a href="#Defect-Image-Sample-Generation-With-Diffusion-Prior-for-Steel-Surface-Defect-Recognition" class="headerlink" title="Defect Image Sample Generation With Diffusion Prior for Steel Surface   Defect Recognition"></a>Defect Image Sample Generation With Diffusion Prior for Steel Surface Defect Recognition</h2><p><strong>Authors:Yichun Tai, Kun Yang, Tao Peng, Zhenzhen Huang, Zhijiang Zhang</strong></p><p>The task of steel surface defect recognition is an industrial problem with great industry values. The data insufficiency is the major challenge in training a robust defect recognition network. Existing methods have investigated to enlarge the dataset by generating samples with generative models. However, their generation quality is still limited by the insufficiency of defect image samples. To this end, we propose Stable Surface Defect Generation (StableSDG), which transfers the vast generation distribution embedded in Stable Diffusion model for steel surface defect image generation. To tackle with the distinctive distribution gap between steel surface images and generated images of the diffusion model, we propose two processes. First, we align the distribution by adapting parameters of the diffusion model, adopted both in the token embedding space and network parameter space. Besides, in the generation process, we propose image-oriented generation rather than from pure Gaussian noises. We conduct extensive experiments on steel surface defect dataset, demonstrating state-of-the-art performance on generating high-quality samples and training recognition models, and both designed processes are significant for the performance.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.01872v1">PDF</a></p><p><strong>Summary</strong><br>钢材表面缺陷生成模型StableSDG通过迁移Stable Diffusion模型生成高精度合成图像，有效提升钢材表面缺陷识别模型的鲁棒性。</p><p><strong>Key Takeaways</strong></p><ul><li>利用Stable Diffusion模型生成钢材表面缺陷图像，扩充训练数据集。</li><li>通过调整模型参数和嵌入空间，弥合生成图像和真实图像之间的分布差异。</li><li>采用图像引导生成方式，而非纯高斯噪声生成。</li><li>提出两种关键过程：分布对齐和图像引导生成。</li><li>在钢材表面缺陷数据集上进行广泛实验，证明StableSDG在生成高质量合成图像和训练识别模型方面均达到最先进水平。</li><li>两种提出的关键过程对性能至关重要。</li><li>StableSDG有效解决数据不足问题，提升钢材表面缺陷识别模型的性能。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>缺陷图像样本生成与扩散</p></li><li><p>Yichun Tai, Kun Yang, Tao Peng, Zhenzhen Huang, and Zhijiang Zhang</p></li><li><p>上海大学传信学院</p></li><li><p>Text-to-image diffusion, data expansion, deep learning, textual inversion, low-rank adaptation, defect image generation, steel surface defect recognition</p></li><li><p>https://arxiv.org/abs/2405.01872 , Github:None</p></li><li><p>摘要：</p></li></ol><p>（1）：钢表面缺陷识别是工业界具有巨大产业价值的一项任务。数据不足是训练鲁棒缺陷识别网络的主要挑战。现有方法已经研究了通过生成模型生成样本以扩大数据集。然而，它们的生成质量仍然受到缺陷图像样本不足的限制。</p><p>（2）：现有的方法包括：SDGAN、Defect-GAN、transP2P。这些方法从头开始训练生成模型具有挑战性，当图像样本不足时，通常会导致生成样本中出现不需要的模式。</p><p>（3）：本文提出了一种稳定的表面缺陷生成（StableSDG）方法，将Stable Diffusion模型中嵌入的巨大生成分布转移用于钢表面缺陷图像生成。为了解决钢表面图像和扩散模型生成图像之间的独特分布差异，我们提出了两个过程。首先，我们通过调整扩散模型的参数来对齐分布，既采用标记嵌入空间，也采用网络参数空间。此外，在生成过程中，我们提出了面向图像的生成，而不是从纯高斯噪声中生成。</p><p>（4）：我们在钢表面缺陷数据集上进行了广泛的实验，展示了在生成高质量样本和训练识别模型方面的最先进性能，并且两个设计过程对性能都很重要。</p><ol><li><p>方法：</p><pre><code>            (1):Stable Diffusion模型[22]作为扩散先验，在潜在空间中执行扩散，而不是图像空间，广泛用于图像生成任务[26]–[30]。

            (2):提出StableSDG方法，由两个过程组成，用于生成每种缺陷类别的图像。

            (3):通过迭代质量评估，调整超参数以实现最佳图像生成。

            (4):使用最佳超参数，生成高质量图像以扩展数据集。

            (5):将每种缺陷类别的生成图像与真实图像一起收集起来，用于训练缺陷识别模型。
</code></pre></li><li><p>结论：</p></li></ol><p>（1）本文提出的StableSDG方法将文本到图像生成技术应用于钢表面缺陷图像生成，有效解决了钢表面缺陷数据集不足的问题，为缺陷识别模型的训练提供了高质量的样本。</p><p>（2）创新点：StableSDG方法在生成器自适应过程中，同时在token嵌入空间和网络参数空间进行自适应和修改，并在生成数据时从图像导向初始化生成样本，而不是从纯高斯噪声开始。</p><p>性能：StableSDG方法在NEU和CCBSD数据集上进行了广泛的实验，结果表明该方法能够生成高保真度的缺陷图像，大大提高了识别模型的性能。</p><p>工作量：StableSDG方法的工作量主要集中在生成器自适应和图像生成两个阶段，需要对超参数进行迭代质量评估和调整，以获得最佳的图像生成效果。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-bd44eeacb7308bdc2e6594f5b84b63b5.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-a49aa1e7511a9ee599c2b42ba68cfb6d.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-e589bb5b223b5f1a03944e68feabbcd1.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-b4b7e153a9792e0731936f44ad770e5f.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-4222a03fe5309bbcdc62d8769e94eb0c.jpg" align="middle"></details><h2 id="Long-Tail-Image-Generation-Through-Feature-Space-Augmentation-and-Iterated-Learning"><a href="#Long-Tail-Image-Generation-Through-Feature-Space-Augmentation-and-Iterated-Learning" class="headerlink" title="Long Tail Image Generation Through Feature Space Augmentation and   Iterated Learning"></a>Long Tail Image Generation Through Feature Space Augmentation and Iterated Learning</h2><p><strong>Authors:Rafael Elberg, Denis Parra, Mircea Petrache</strong></p><p>Image and multimodal machine learning tasks are very challenging to solve in the case of poorly distributed data. In particular, data availability and privacy restrictions exacerbate these hurdles in the medical domain. The state of the art in image generation quality is held by Latent Diffusion models, making them prime candidates for tackling this problem. However, a few key issues still need to be solved, such as the difficulty in generating data from under-represented classes and a slow inference process. To mitigate these issues, we propose a new method for image augmentation in long-tailed data based on leveraging the rich latent space of pre-trained Stable Diffusion Models. We create a modified separable latent space to mix head and tail class examples. We build this space via Iterated Learning of underlying sparsified embeddings, which we apply to task-specific saliency maps via a K-NN approach. Code is available at <a target="_blank" rel="noopener" href="https://github.com/SugarFreeManatee/Feature-Space-Augmentation-and-Iterated-Learning">https://github.com/SugarFreeManatee/Feature-Space-Augmentation-and-Iterated-Learning</a></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.01705v1">PDF</a></p><p><strong>Summary</strong><br>长尾数据图像增强方法，利用预训练稳定扩散模型的潜在空间，缓解生成质量差和推理速度慢的问题。</p><p><strong>Key Takeaways</strong></p><ul><li>图像和多模态机器学习任务在数据分布不足的情况下极具挑战性。</li><li>医学领域的图像生成面临数据获取和隐私限制的障碍。</li><li>潜在扩散模型在图像生成质量上处于领先地位。</li><li>数据生成不平衡和推理速度慢是亟待解决的问题。</li><li>方法结合预训练稳定扩散模型的潜在空间，进行图像增强。</li><li>构建可分离的潜在空间，混合头部和尾部类别的示例。</li><li>通过迭代学习潜在嵌入，构建空间，并通过 K-NN 方法应用于特定任务的显着性图。</li><li>代码已开源：<a target="_blank" rel="noopener" href="https://github.com/SugarFreeManatee/Feature-Space-Augmentation-and-Iterated-Learning">https://github.com/SugarFreeManatee/Feature-Space-Augmentation-and-Iterated-Learning</a></li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>标题：特征空间增强和迭代学习的长尾图像生成</p></li><li><p>作者：Rafael Elberg、Denis Parra、Mircea Petrache</p></li><li><p>隶属：智利天主教大学</p></li><li><p>关键词：长尾数据、图像生成、特征空间增强、迭代学习</p></li><li><p>论文链接：https://arxiv.org/abs/2405.01705 Github 代码链接：https://github.com/SugarFreeManatee/Feature-Space-Augmentation-and-Iterated-Learning</p></li><li><p>摘要：</p></li></ol><p>(1)：图像和多模态机器学习任务在数据分布不均匀的情况下非常具有挑战性。特别是在医疗领域，数据可用性和隐私限制加剧了这些障碍。潜扩散模型在图像生成质量方面处于最先进水平，使其成为解决此问题的理想候选者。然而，仍需要解决几个关键问题，例如难以生成来自代表性不足的类别的图像以及推理过程缓慢。</p><p>(2)：过去的方法包括重采样和数据增强。重采样技术在一些长尾问题中取得了相对成功，但可能会给下游任务引入不必要的偏差，并且经常导致过拟合。数据增强是解决这些问题的自然响应。它代表了一个蓬勃发展的研究领域，包括几个不同的算法系列，例如几何变换（旋转、缩放、裁剪等）、合成样本创建、基于混合的方法、基于域转换的方法和生成方法。</p><p>(3)：本文提出了一种新的数据增强方法，该方法操纵来自预训练扩散模型的图像的潜在空间表示，从而生成新图像来增强代表性不足的类别。通过激活图选择数据的特定特征，然后将这些特征组合起来，生成与属于长尾类的实际数据中的图像相似的图像。</p><p>(4)：在本文的方法中，潜在空间表示的组合由于特征后处理之间的干扰现象而难以通过朴素的方法执行。本文将此问题作为合成泛化问题，并将迭代学习（IL）框架与稀疏嵌入应用于目标数据增强框架。IL 的主要灵感来自文化进化模型，其中教师-学生交互的迭代鼓励有用的压缩和形成适应任务的“共享语言”。特别是，最近在使用稀疏状态空间时获得了与合成不同特征相关的有利结果。</p><ol><li>方法：</li></ol><p>（1）：本文提出了一种新的数据增强方法，该方法通过操纵来自预训练扩散模型的图像的潜在空间表示来生成新图像，以增强代表性不足的类别；</p><p>（2）：该方法包括三个阶段：迭代训练、类别激活图生成和推理；</p><p>（3）：在迭代训练阶段，学习了一个从扩散潜在空间到稀疏高维表示的转换，同时训练了一个卷积分类器用于该空间；</p><p>（4）：在类别激活图生成阶段，使用分类器生成每个类的简单可解释激活图，以选择与分类为该类相关的或不相关的坐标；</p><p>（5）：在推理阶段，从尾部类生成新样本，通过将特定尾部类示例的类特定特征与最高混淆头部类的类通用特征融合，使用掩码创建融合向量。</p><ol><li>结论：</li></ol><p>（1）：本文提出了一种新颖的方法，通过利用预训练的潜在扩散模型、组合学习和显着性方法，为长尾数据集生成数据并增强数据，从而为代表性不足的类别生成新示例。</p><p>（2）：创新点：提出了一种利用潜在扩散模型、组合学习和显著性方法的数据增强和数据生成方法；性能：在医学领域的多标签分类任务中使用 MIMIC-CXR-LT [13, 16] 的一个小型子集，在图像生成和数据增强方面取得了有竞争力的结果；工作量：工作量中等，需要预训练潜在扩散模型，并对转换和分类器进行迭代训练。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-0d188950e8539013f5d1dbb852ac0cbb.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-8c5b473a90cffec494f2607efb08a6c2.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-39ea380f716fabbe74492f3835a23773.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-9d004602232a458ce5dd218668a87e87.jpg" align="middle"></details><h2 id="LocInv-Localization-aware-Inversion-for-Text-Guided-Image-Editing"><a href="#LocInv-Localization-aware-Inversion-for-Text-Guided-Image-Editing" class="headerlink" title="LocInv: Localization-aware Inversion for Text-Guided Image Editing"></a>LocInv: Localization-aware Inversion for Text-Guided Image Editing</h2><p><strong>Authors:Chuanming Tang, Kai Wang, Fei Yang, Joost van de Weijer</strong></p><p>Large-scale Text-to-Image (T2I) diffusion models demonstrate significant generation capabilities based on textual prompts. Based on the T2I diffusion models, text-guided image editing research aims to empower users to manipulate generated images by altering the text prompts. However, existing image editing techniques are prone to editing over unintentional regions that are beyond the intended target area, primarily due to inaccuracies in cross-attention maps. To address this problem, we propose Localization-aware Inversion (LocInv), which exploits segmentation maps or bounding boxes as extra localization priors to refine the cross-attention maps in the denoising phases of the diffusion process. Through the dynamic updating of tokens corresponding to noun words in the textual input, we are compelling the cross-attention maps to closely align with the correct noun and adjective words in the text prompt. Based on this technique, we achieve fine-grained image editing over particular objects while preventing undesired changes to other regions. Our method LocInv, based on the publicly available Stable Diffusion, is extensively evaluated on a subset of the COCO dataset, and consistently obtains superior results both quantitatively and qualitatively.The code will be released at <a target="_blank" rel="noopener" href="https://github.com/wangkai930418/DPL">https://github.com/wangkai930418/DPL</a></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.01496v1">PDF</a> Accepted by CVPR 2024 Workshop AI4CC</p><p><strong>Summary</strong><br>文本引导图像编辑研究利用大型文本到图像扩散模型，但现有编辑技术容易修改超出目标区域的无意区域，主要是因为交叉注意力图不准确。我们通过分割图或边界框改进扩散过程中的交叉注意力图，实现了特定对象的细粒度图像编辑，同时防止对其他区域进行非必要的更改。</p><p><strong>Key Takeaways</strong></p><ul><li>现有图像编辑技术容易修改超出目标区域的无意区域。</li><li>我们提出了利用分割图或边界框作为额外的定位先验来改进扩散过程中的交叉注意力图。</li><li>我们通过更新文本输入中名词对应的符号，迫使交叉注意力图紧密对齐文本提示中的正确名词和形容词。</li><li>我们基于公开的Stable Diffusion实现了LocInv方法，并在COCO数据集的子集上进行了广泛评估。</li><li>与现有方法相比，我们的方法在定量和定性上都取得了更好的结果。</li><li>该方法的代码将在<a target="_blank" rel="noopener" href="https://github.com/wangkai930418/DPL上公布。">https://github.com/wangkai930418/DPL上公布。</a></li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>标题：定位感知反演：文本引导图像编辑</p></li><li><p>作者：Chuanming Tang、Kai Wang、Fei Yang、Joost van de Weijer</p></li><li><p>单位：中国科学院大学</p></li><li><p>关键词：文本到图像、图像编辑、定位感知、交叉注意力</p></li><li><p>论文链接：https://arxiv.org/abs/2405.01496 Github：无</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：大规模文本到图像扩散模型在文本提示下展示了显著的生成能力。基于文本到图像扩散模型，文本引导图像编辑研究旨在通过改变文本提示来赋予用户操纵生成图像的能力。</p><p>（2）：已有方法及问题：现有的图像编辑技术容易对超出目标区域的无意区域进行编辑，这主要是由于交叉注意力图的不准确。</p><p>（3）：提出的研究方法：本文提出定位感知反演（LocInv），它利用分割图或边界框作为额外的定位先验，在扩散过程的去噪阶段优化交叉注意力图。通过动态更新文本输入中与名词对应的标记，迫使交叉注意力图与文本提示中正确的名词和形容词紧密对齐。基于此技术，我们实现了对特定对象的细粒度图像编辑，同时防止对其他区域进行不必要的更改。</p><p>（4）：方法性能：基于公开的Stable Diffusion，我们对LocInv方法在COCO数据集的子集上进行了广泛的评估，在定量和定性上都取得了优异的结果。这些结果证明了该方法可以实现其目标。</p><ol><li>方法：</li></ol><p>（1）：使用 Stable Diffusion v1.4 作为基础扩散模型，该模型由编码器、解码器和扩散模型组成。</p><p>（2）：采用 DDIM 反演算法，从随机噪声 zT 找到初始噪声，通过采样重建输入潜在代码 z0。</p><p>（3）：使用无文本反演 (NTI) 优化无文本嵌入 ∅t，以近似 DDIM 轨迹 {zt}T 0，从而编辑真实图像。</p><p>（4）：提出动态提示学习 (DPL) 方法，利用分割图或检测框作为定位先验，更新文本提示 P 中的名词单词对应的标记，迫使交叉注意力图与文本提示中的名词和形容词紧密对齐。</p><p>（5）：设计相似度损失和重叠损失，优化嵌入向量 Vt，使交叉注意力图与定位先验 S 之间相似度高、重叠度高。</p><p>（6）：采用渐进优化机制，在每个时间步 t 处强制所有损失达到预定义阈值，避免交叉注意力图过拟合。</p><p>（7）：结合 NTI 学习一组无文本嵌入 ∅t，与可学习的单词嵌入 Vt 共同精确定位对象并重建原始图像。</p><p>（8）：提出形容词绑定机制，通过改变文本提示中的形容词来改变对象的外观。</p><ol><li>Conclusion:</li></ol><p>(1): 本文提出的 LocInv 方法解决了文本到图像扩散模型图像编辑中交叉注意力图泄漏的问题。我们提出使用分割图或检测框作为先验，更新提示中每个名词单词的动态标记。由此产生的交叉注意力图较少受到交叉注意力图泄漏的影响。因此，这些大大改进的交叉注意力图极大地改善了文本引导图像编辑的结果。实验结果证实，LocInv 获得了更好的结果，尤其是在复杂的多对象场景中。最后，我们展示了我们的方法还可以将形容词单词绑定到它们对应の名词上，从而得到形容词的准确交叉注意力图，并允许对属性进行编辑，这是以前在文本引导图像编辑中尚未充分探索的。</p><p>(2): 创新点：提出定位感知反演方法，利用分割图或检测框作为定位先验，更新文本提示中的名词单词对应的标记，迫使交叉注意力图与文本提示中的名词和形容词紧密对齐；提出形容词绑定机制，通过改变文本提示中的形容词来改变对象的外观。</p><p>性能：在 COCO 数据集的子集上进行了广泛的评估，在定量和定性上都取得了优异的结果，证明了该方法可以实现其目标。</p><p>工作量：方法实现较为复杂，需要结合 Stable Diffusion 模型和 NTI 反演算法，以及分割图或检测框作为定位先验。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-501b84f66a4fdce982c4d560d6ed2c6e.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-d7f11972c7c9876389df6092b426ca67.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-566375be0266ca83b73d642319fcc82b.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-2650ddd9595d88f0a5238c88b753e8e6.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-997ccc837824e0d3f900484e2641fab6.jpg" align="middle"></details><h2 id="Guided-Conditional-Diffusion-Classifier-ConDiff-for-Enhanced-Prediction-of-Infection-in-Diabetic-Foot-Ulcers"><a href="#Guided-Conditional-Diffusion-Classifier-ConDiff-for-Enhanced-Prediction-of-Infection-in-Diabetic-Foot-Ulcers" class="headerlink" title="Guided Conditional Diffusion Classifier (ConDiff) for Enhanced   Prediction of Infection in Diabetic Foot Ulcers"></a>Guided Conditional Diffusion Classifier (ConDiff) for Enhanced Prediction of Infection in Diabetic Foot Ulcers</h2><p><strong>Authors:Palawat Busaranuvong, Emmanuel Agu, Deepak Kumar, Shefalika Gautam, Reza Saadati Fard, Bengisu Tulu, Diane Strong</strong></p><p>To detect infected wounds in Diabetic Foot Ulcers (DFUs) from photographs, preventing severe complications and amputations. Methods: This paper proposes the Guided Conditional Diffusion Classifier (ConDiff), a novel deep-learning infection detection model that combines guided image synthesis with a denoising diffusion model and distance-based classification. The process involves (1) generating guided conditional synthetic images by injecting Gaussian noise to a guide image, followed by denoising the noise-perturbed image through a reverse diffusion process, conditioned on infection status and (2) classifying infections based on the minimum Euclidean distance between synthesized images and the original guide image in embedding space. Results: ConDiff demonstrated superior performance with an accuracy of 83% and an F1-score of 0.858, outperforming state-of-the-art models by at least 3%. The use of a triplet loss function reduces overfitting in the distance-based classifier. Conclusions: ConDiff not only enhances diagnostic accuracy for DFU infections but also pioneers the use of generative discriminative models for detailed medical image analysis, offering a promising approach for improving patient outcomes.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.00858v1">PDF</a></p><p><strong>Summary</strong><br>利用合成图像感染状态指导分类鉴别糖尿病足溃疡感染</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种新的糖尿病足溃疡感染检测模型，即条件引导扩散分类器（ConDiff）</li><li>ConDiff结合了引导图像合成、去噪扩散模型和基于距离的分类</li><li>通过在引导图像中注入高斯噪声并通过条件化感染状态进行逆扩散去噪合成图像</li><li>基于合成图像与原始引导图像在嵌入空间中的最小欧几里得距离进行感染分类</li><li>使用元组损失函数在基于距离的分类器中减少过拟合</li><li>ConDiff在准确性和F1-score上优于最先进的模型</li><li>ConDiff开创了生成式判别模型在医学图像精细分析中的应用，为改善患者预后提供了一种有前途的方法</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: 导向条件扩散分类器（ConDiff）</p></li><li><p>Authors: Palawat Busaranuvong, Emmanuel Agu, Deepak Kumar, Shefalika Gautam, Reza Saadati Fard, Bengisu Tulu, Diane Strong</p></li><li><p>Affiliation: Worcester理工学院</p></li><li><p>Keywords: 糖尿病足溃疡，扩散模型，基于距离的图像分类，生成模型，伤口感染</p></li><li><p>Urls: Paper: xxx, Github: None</p></li><li><p>Summary:</p><p>（1）：糖尿病足溃疡（DFU）感染是导致截肢和严重并发症的主要原因；</p><p>（2）：现有的基于深度学习的DFU感染检测方法存在准确率低的问题；</p><p>（3）：本文提出了一种新的ConDiff模型，该模型结合了引导图像合成、去噪扩散模型和基于距离的分类，通过生成引导条件合成图像并计算合成图像与原始图像之间的最小欧几里得距离来对感染进行分类；</p><p>（4）：ConDiff在DFU感染数据集上取得了83%的准确率和0.858的F1分数，优于现有方法至少3%，证明了其在提高DFU感染诊断准确性方面的有效性。</p></li><li><p>方法：</p><p>（1）：ConDiff 框架由两个主要部分组成：（1）引导扩散，即向 DFU 图像注入高斯噪声，然后根据感染状态从噪声扰动图像中逐步去除噪声，以合成条件图像；（2）基于距离的分类器，即根据原始图像和合成图像在嵌入空间中的最小 L2 距离预测输入图像的标签。</p><p>（2）：ConDiff 利用条件引导图像编辑与生成扩散模型，通过向输入图像注入特定强度的 Gaussian 噪声，并使用反向扩散过程逐步从噪声扰动输入图像中去除噪声来生成新图像。</p><p>（3）：ConDiff 的扩散过程以伤口的状况（无感染（y1）或感染（y2））为条件，创建反映这些状态的合成图像。一个关键点是 ConDiff 能够通过嵌入空间中的 L2 距离分类器识别和学习条件生成图像 ˆxy 0 和原始伤口图像 x0 之间表示的相似性。产生与原始图像最相似的合成图像的条件被选作预测标签。</p><p>（4）：与最小化二元交叉熵损失函数的传统监督分类技术不同，ConDiff 通过利用三元损失函数来减轻过拟合，以增加非相似图像对之间的距离并减少相似图像对之间的距离。</p><p>（5）：本研究利用 Goyal 等人提供的 DFU 感染数据集（见表 I）。但是，为了消除训练集和测试集之间的数据泄漏，我们改进了数据集创建和拆分策略。使用基于主题的拆分，仅为每个主题使用第二个放大自然增强图像（参见图 1）。</p><p>（6）：ConDiff 框架的主要贡献是：（1）我们提出了 Guided Conditional Diffusion Classifier（ConDiff），这是一个用于分类受感染伤口图像的集成端到端框架。ConDiff 框架有 2 个主要部分：（1）引导扩散，即向 DFU 图像注入高斯噪声，然后根据感染状态从噪声扰动图像中逐步去除噪声，以合成条件图像；（2）基于距离的分类器，即根据原始图像和合成图像在嵌入空间中的最小 L2 距离预测输入图像的标签。据我们所知，ConDiff 是第一个分析细粒度伤口图像的生成判别方法，促进了糖尿病足溃疡 (DFU) 感染的检测。（2）在 DFU 感染数据集的看不见的测试伤口图像（148 个受感染和 103 个未受感染）上进行严格评估，我们的 ConDiff 框架明显优于最先进的基线，提高了伤口感染检测的准确性和 F1 分数至少 3%。（3）我们证明，通过在训练期间最小化三元损失函数，ConDiff 减少了对 1416 个训练图像的小 DFU 数据集的过拟合。（4）由 Score-CAM 生成的热图用于直观地说明 ConDiff 在对伤口感染状态进行分类时专注于正确的伤口图像区域。</p></li><li><p>结论：</p></li></ol><p>（1）：本研究引入了引导条件扩散分类器（ConDiff），这是一个用于对糖尿病足溃疡（DFU）感染进行分类的新框架。ConDiff 优于传统模型至少 3%，准确率高达 83%，F1 分数为 0.858。它独特的方法利用三元损失而不是标准的交叉熵最小化，增强了鲁棒性和减少了过拟合。这在数据集通常很小的医学成像中尤其重要。ConDiff 采用正向扩散过程，向输入图像中添加特定数量的高斯噪声，并采用无分类器指导的反向扩散，根据嵌入空间中的最近欧几里得距离对这些图像进行迭代细化以进行分类。ConDiff 的有效性表明在改善 DFU 管理方面具有显着潜力，尤其是在医疗资源有限的地区。其精确的实时感染检测可以在早期 DFU 感染识别中发挥至关重要的作用，从而减少肢体截肢等严重并发症。</p><p>（2）：创新点：ConDiff 是第一个分析细粒度伤口图像的生成判别方法，促进了 DFU 感染的检测；性能：在 DFU 感染数据集的看不见的测试伤口图像（148 个受感染和 103 个未受感染）上进行严格评估，ConDiff 框架明显优于最先进的基线，提高了伤口感染检测的准确性和 F1 分数至少 3%；工作量：通过在训练期间最小化三元损失函数，ConDiff 减少了对 1416 个训练图像的小 DFU 数据集的过拟合。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-f38f851b08a13cd2762a9779abb3d5dd.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-ac1698e3895c14a21d1245d61cbbe4db.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-902be7065fad826b29010fef3bd7e79b.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-d3d5d8c1286e3aefa0a37934906ae34f.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-a6a38755ae54c6bfd6a3359d2197b5a2.jpg" align="middle"></details><h2 id="Obtaining-Favorable-Layouts-for-Multiple-Object-Generation"><a href="#Obtaining-Favorable-Layouts-for-Multiple-Object-Generation" class="headerlink" title="Obtaining Favorable Layouts for Multiple Object Generation"></a>Obtaining Favorable Layouts for Multiple Object Generation</h2><p><strong>Authors:Barak Battash, Amit Rozner, Lior Wolf, Ofir Lindenbaum</strong></p><p>Large-scale text-to-image models that can generate high-quality and diverse images based on textual prompts have shown remarkable success. These models aim ultimately to create complex scenes, and addressing the challenge of multi-subject generation is a critical step towards this goal. However, the existing state-of-the-art diffusion models face difficulty when generating images that involve multiple subjects. When presented with a prompt containing more than one subject, these models may omit some subjects or merge them together. To address this challenge, we propose a novel approach based on a guiding principle. We allow the diffusion model to initially propose a layout, and then we rearrange the layout grid. This is achieved by enforcing cross-attention maps (XAMs) to adhere to proposed masks and by migrating pixels from latent maps to new locations determined by us. We introduce new loss terms aimed at reducing XAM entropy for clearer spatial definition of subjects, reduce the overlap between XAMs, and ensure that XAMs align with their respective masks. We contrast our approach with several alternative methods and show that it more faithfully captures the desired concepts across a variety of text prompts.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.00791v1">PDF</a></p><p><strong>Summary</strong><br>随着文本到图像模型的快速发展，多主体生成成为模型发展的重要步骤。本研究针对扩散模型多主体生成中的问题，提出了一种通过引导原则进行布局规划的新方法。</p><p><strong>Key Takeaways</strong></p><ul><li>扩散模型在多主体生成中面临着遗漏或合并主体的问题。</li><li>本研究提出了一种基于引导原则的布局规划方法。</li><li>该方法允许扩散模型初始提出布局，然后对其进行重新排列。</li><li>强制交叉注意力图（XAM）遵循提出的遮罩，并将潜在图中的像素迁移到新位置。</li><li>引入了新的损失项，以减少 XAM 熵、减少 XAM 之间的重叠并确保 XAM 与各自的遮罩对齐。</li><li>该方法在各种文本提示中更真实地捕捉到所需概念。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: 获得多个对象生成的有利布局</p></li><li><p>Authors: Barak Battash, Amit Rozner, Lior Wolf, Ofir Lindenbaum</p></li><li><p>Affiliation: 巴伊兰大学工程学院</p></li><li><p>Keywords: 文本到图像生成, 多对象生成, 扩散模型, 交叉注意力图</p></li><li><p>Paper: https://arxiv.org/abs/2405.00791 , Github: None</p></li><li><p>Summary:</p><p>(1): 大规模文本到图像模型在基于文本提示生成高质量和多样化图像方面取得了显著成功。这些模型最终旨在创建复杂的场景，解决多对象生成挑战是朝着这一目标迈出的关键一步。然而，现有的最先进的扩散模型在生成涉及多个对象的图像时面临困难。当给定包含多个对象的提示时，这些模型可能会省略一些对象或将它们合并在一起。</p><p>(2): 现有的方法包括：使用交叉注意力图（XAM）对生成图像中的不同对象进行建模。然而，这些方法存在问题：当提示中包含多个对象时，模型可能会省略一些对象或将它们合并在一起。</p><p>(3): 本文提出了一种基于指导原则的新方法。我们允许扩散模型最初提出一个布局，然后重新排列布局网格。这是通过强制交叉注意力图（XAM）遵守提出的掩码并通过将像素从潜在图迁移到我们确定的新位置来实现的。我们引入了新的损失项，旨在降低 XAM 熵以更清晰地定义对象的空间，减少 XAM 之间的重叠，并确保 XAM 与它们各自的掩码对齐。</p><p>(4): 本文方法在各种文本提示中更忠实地捕捉到所需的概念，证明了其有效性。</p></li><li><p>方法：</p></li></ol><p>（1）：提出一种基于指导原则的新方法，允许扩散模型最初提出一个布局，然后重新排列布局网格；</p><p>（2）：通过强制交叉注意力图（XAM）遵守提出的掩码并通过将像素从潜在图迁移到确定的新位置来实现；</p><p>（3）：引入新的损失项，旨在降低 XAM 熵以更清晰地定义对象的空间，减少 XAM 之间的重叠，并确保 XAM 与它们各自的掩码对齐；</p><p>（4）：在各种文本提示中更忠实地捕捉到所需的概念，证明了其有效性。</p><ol><li>结论：</li></ol><p>（1）：本文提出了一种基于指导原则的新方法，该方法允许扩散模型最初提出一个布局，然后重新排列布局网格，从而更忠实地捕捉到各种文本提示中所需的概念，证明了其有效性。</p><p>（2）：创新点：提出了一种基于指导原则的新方法，该方法允许扩散模型最初提出一个布局，然后重新排列布局网格。 性能：在各种文本提示中更忠实地捕捉到所需的概念，证明了其有效性。 工作量：需要针对不同的文本提示进行微调，工作量较大。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-d3231e3375af2b14c1e49248519eaebd.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-561eb2e3b9534e1fe4b30e7ef897a8b3.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-c2973412c6cc5c19507315dc2dd5efcd.jpg" align="middle"></details><h2 id="Deep-Reward-Supervisions-for-Tuning-Text-to-Image-Diffusion-Models"><a href="#Deep-Reward-Supervisions-for-Tuning-Text-to-Image-Diffusion-Models" class="headerlink" title="Deep Reward Supervisions for Tuning Text-to-Image Diffusion Models"></a>Deep Reward Supervisions for Tuning Text-to-Image Diffusion Models</h2><p><strong>Authors:Xiaoshi Wu, Yiming Hao, Manyuan Zhang, Keqiang Sun, Zhaoyang Huang, Guanglu Song, Yu Liu, Hongsheng Li</strong></p><p>Optimizing a text-to-image diffusion model with a given reward function is an important but underexplored research area. In this study, we propose Deep Reward Tuning (DRTune), an algorithm that directly supervises the final output image of a text-to-image diffusion model and back-propagates through the iterative sampling process to the input noise. We find that training earlier steps in the sampling process is crucial for low-level rewards, and deep supervision can be achieved efficiently and effectively by stopping the gradient of the denoising network input. DRTune is extensively evaluated on various reward models. It consistently outperforms other algorithms, particularly for low-level control signals, where all shallow supervision methods fail. Additionally, we fine-tune Stable Diffusion XL 1.0 (SDXL 1.0) model via DRTune to optimize Human Preference Score v2.1, resulting in the Favorable Diffusion XL 1.0 (FDXL 1.0) model. FDXL 1.0 significantly enhances image quality compared to SDXL 1.0 and reaches comparable quality compared with Midjourney v5.2.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.00760v1">PDF</a> N/A</p><p><strong>Summary</strong><br>文本到图像扩散模型中，利用给定的激励函数进行优化是一个重要但未得到充分探索的研究领域。研究中提出深度激励优化（DRTune），算法直接监督文本到图像扩散模型的最终输出图像，并且通过迭代采样流程将梯度传回输入噪声。</p><p><strong>Key Takeaways</strong></p><ul><li>针对低层激励，训练采样流程中的早期步骤至关重要。</li><li>在去噪网络输入处停止梯度，可以有效实现深度监督。</li><li>DRTune 算法在各种激励模型上得到了广泛评估。</li><li>DRTune 算法始终优于其他算法，尤其是在浅层监督方法失效的低层控制信号中。</li><li>通过 DRTune 优化 Human Preference Score v2.1，对 Stable Diffusion XL 1.0（SDXL 1.0）模型进行微调，产生了更好的扩散 XL 1.0（FDXL 1.0）模型。</li><li>FDXL 1.0 与 Midjourney v5.2 相比，显著提高了图像质量，达到了相当的水平。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>标题：深度奖励监督微调</p></li><li><p>作者：Xiaoshi Wu<em>1,3, Yiming Hao</em>2, Manyuan Zhang1, Keqiang Sun1, Zhaoyang Huang3, Guanglu Song4, Yu Liu4, and Hongsheng Li1,2</p></li><li><p>第一作者单位：香港中文大学多媒体实验室</p></li><li><p>关键词：文本到图像扩散模型、深度奖励监督、微调、图像质量增强</p></li><li><p>论文链接：https://arxiv.org/abs/2405.00760v1</p></li><li><p>摘要：</p></li></ol><p>(1)：研究背景：优化具有给定奖励函数的文本到图像扩散模型是一个重要但尚未充分探索的研究领域。</p><p>(2)：过去方法：现有方法通常采用浅层监督，即仅监督采样过程的早期步骤。然而，对于低级奖励信号，浅层监督效果不佳。</p><p>(3)：本文方法：本文提出深度奖励微调（DRTune）算法，通过直接监督文本到图像扩散模型的最终输出图像并通过迭代采样过程反向传播到输入噪声来实现深度监督。</p><p>(4)：方法性能：DRTune在各种奖励模型上得到了广泛评估。与其他算法相比，它始终表现出更好的性能，尤其是在浅层监督方法均失败的低级控制信号方面。此外，本文使用DRTune微调了Stable Diffusion XL 1.0（SDXL 1.0）模型以优化人类偏好评分v2.1，得到了Favorable Diffusion XL 1.0（FDXL 1.0）模型。与SDXL 1.0相比，FDXL 1.0显着提高了图像质量，并且与Midjourney v5.2相比达到了相当的质量。</p><ol><li>方法：</li></ol><p>（1）：DRTune 算法的核心思想是通过直接监督文本到图像扩散模型的最终输出图像，并通过迭代采样过程反向传播到输入噪声来实现深度监督；</p><p>（2）：为了解决梯度爆炸问题，DRTune 通过阻止输入 xt 的梯度来解决收敛问题；</p><p>（3）：为了提高效率，DRTune 阻止输入 xt 的梯度，并训练所有采样步骤的子集；</p><p>（4）：DRTune 算法的伪代码如下：</p><p><code>输入：预训练的扩散模型权重 θ、奖励 r、训练时间步长 K、早期停止时间步长范围 m。sg 表示梯度停止操作。 while not converged do ttrain = {1, ..., K} if DRaFT-K ttrain = {i}i≥randint(1,T ) if AlignProp if DRTune then # 等距时间步长。 s = randint(1, T − K⌊ T K ⌋) ttrain = {s + i⌊ T K ⌋ | i = 0, 1, ..., K − 1} if ReFL 或 DRTune then tmin = randint(1, m) else tmin = 0 xT ∼ N(0, I) for t = T, ..., 1 do if DRTune then ˆϵ = ϵθ(sg(xt), t) else ˆϵ = ϵθ(xt, t) if t /∈ ttrain then ˆϵ = sg(ˆϵ) if t == tmin then x0 ≈ intermediate_prediction(xt, ˆϵ) break xt−1 = atxt + btˆϵ + ctϵ g = ∇θr(x0, c) θ ← θ − ηg</code></p><ol><li>结论：</li></ol><p>（1）：本文的意义在于，它解决了利用奖励模型的反馈来训练文本到图像扩散模型的挑战。本文强调了深度监督对于优化全局奖励的重要性，并使用停止梯度技术解决了收敛问题。此外，本文通过微调 FDXL 1.0 模型展示了奖励训练的潜力，以实现与 Midjourney 相当的图像质量。</p><p>（2）：创新点：本文提出了深度奖励微调（DRTune）算法，实现了深度监督，并通过阻止输入 xt 的梯度来解决收敛问题。 性能：DRTune 在各种奖励模型上得到了广泛评估，与其他算法相比，它始终表现出更好的性能，尤其是在浅层监督方法均失败的低级控制信号方面。 工作量：DRTune 算法的实现相对简单，并且可以轻松应用于现有的文本到图像扩散模型。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-037d3e48be185336859047a6292c8d27.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-9b417340b0b4ae8f5dcc966e5d18466d.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-d9661ec54a3560470071969dc361ea74.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-560c3c936da4c7000b08d87c1704852f.jpg" align="middle"></details><h2 id="Detail-Enhancing-Framework-for-Reference-Based-Image-Super-Resolution"><a href="#Detail-Enhancing-Framework-for-Reference-Based-Image-Super-Resolution" class="headerlink" title="Detail-Enhancing Framework for Reference-Based Image Super-Resolution"></a>Detail-Enhancing Framework for Reference-Based Image Super-Resolution</h2><p><strong>Authors:Zihan Wang, Ziliang Xiong, Hongying Tang, Xiaobing Yuan</strong></p><p>Recent years have witnessed the prosperity of reference-based image super-resolution (Ref-SR). By importing the high-resolution (HR) reference images into the single image super-resolution (SISR) approach, the ill-posed nature of this long-standing field has been alleviated with the assistance of texture transferred from reference images. Although the significant improvement in quantitative and qualitative results has verified the superiority of Ref-SR methods, the presence of misalignment before texture transfer indicates room for further performance improvement. Existing methods tend to neglect the significance of details in the context of comparison, therefore not fully leveraging the information contained within low-resolution (LR) images. In this paper, we propose a Detail-Enhancing Framework (DEF) for reference-based super-resolution, which introduces the diffusion model to generate and enhance the underlying detail in LR images. If corresponding parts are present in the reference image, our method can facilitate rigorous alignment. In cases where the reference image lacks corresponding parts, it ensures a fundamental improvement while avoiding the influence of the reference image. Extensive experiments demonstrate that our proposed method achieves superior visual results while maintaining comparable numerical outcomes.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.00431v1">PDF</a></p><p><strong>Summary</strong><br>引用图像超分辨通过引入高分辨率参考图像来缓解单图像超分辨的病态问题，但由于纹理传输前存在错位问题，仍有提升空间。</p><p><strong>Key Takeaways</strong></p><ul><li>引用图像超分辨方法在定量和定性结果上均有显著提升。</li><li>现有方法忽视了比较中细节的重要性，没有充分利用低分辨率图像中的信息。</li><li>本文提出了一种基于扩散模型的细节增强框架，用于引用图像超分辨。</li><li>如果参考图像中存在对应部分，该方法可以促进严格的对齐。</li><li>如果参考图像没有对应部分，该方法可以确保基本改进，同时避免参考图像的影响。</li><li>大量实验表明，所提出的方法在保持可比数值结果的同时，获得了更好的视觉效果。</li><li>该方法能够在没有对应参考图像的情况下提高超分辨率性能。</li><li>该方法可以灵活地应用于各种引用图像超分辨任务。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>论文标题：基于参考图像的图像超分辨率的细节增强框架</p></li><li><p>作者：Zihan Wang, Ziliang Xiong, Hongying Tang, Xiaobing Yuan</p></li><li><p>第一作者单位：上海微系统与信息技术研究所</p></li><li><p>关键词：图像超分辨率，参考图像，细节增强，扩散模型</p></li><li><p>论文链接：xxx，Github 代码链接：None</p></li><li><p>摘要：</p><p>（1）：近年来，基于参考图像的图像超分辨率（Ref-SR）得到了蓬勃发展。通过将高分辨率（HR）参考图像引入到单幅图像超分辨率（SISR）方法中，在参考图像纹理的辅助下，缓解了这一长期存在的领域的病态性质。尽管定量和定性结果的显着提高验证了 Ref-SR 方法的优越性，但在纹理传输之前存在的错位表明还有进一步提高性能的空间。现有的方法往往忽略了比较背景下细节的重要性，因此没有充分利用低分辨率（LR）图像中包含的信息。</p><p>（2）：过去的方法倾向于简单地将输入的 LR 图像调整为与相应参考图像相同的分辨率，例如双三次插值。Lu 等人选择对参考图像进行下采样以适应匹配过程，目的是降低计算复杂度。虽然这种方法可以在一定程度上缓解错位问题，但它忽略了细节的增强，可能会破坏后续的图像恢复结果。在参考图像中存在对应部分的情况下，现有的方法无法促进严格的对齐。</p><p>（3）：本文提出了一种基于参考图像的超分辨率细节增强框架（DEF），它引入了扩散模型来生成和增强 LR 图像中的底层细节。如果参考图像中存在对应部分，我们的方法可以促进严格的对齐。在参考图像中缺少对应部分的情况下，它确保了根本性的改进，同时避免了参考图像的影响。</p><p>（4）：在图像超分辨率任务上，本文方法取得了优异的视觉效果，同时保持了可比的数值结果。这些性能支持了他们的目标。</p></li><li><p>方法：</p></li></ol><p>（1）：提出了一种基于参考图像的超分辨率细节增强框架（DEF），它引入了扩散模型来生成和增强 LR 图像中的底层细节。</p><p>（2）：该方法将图像超分辨率任务分解为两个子任务：细节生成和细节迁移。</p><p>（3）：在细节生成任务中，使用预训练的扩散模型对输入图像进行上采样，以获得细节增强的输入图像。</p><p>（4）：在细节迁移任务中，首先对细节增强的图像和参考图像进行特征提取。</p><p>（5）：利用细节增强的输入图像来计算参考图像和输入图像之间的相似性。</p><p>（6）：使用 deformable convolution network（DCN）进行纹理迁移和集成，以解决纹理失配问题。</p><ol><li>结论：</li></ol><p>（1）：本文提出了一种基于参考图像的超分辨率细节增强框架（DEF），该框架引入了扩散模型来生成和增强低分辨率图像中的底层细节。该方法将图像超分辨率任务分解为两个子任务：细节生成和细节迁移。在细节生成任务中，使用预训练的扩散模型对输入图像进行上采样，以获得细节增强的输入图像。在细节迁移任务中，首先对细节增强的图像和参考图像进行特征提取。利用细节增强的输入图像来计算参考图像和输入图像之间的相似性。使用可变形卷积网络（DCN）进行纹理迁移和集成，以解决纹理失配问题。</p><p>（2）：创新点：提出了一种基于参考图像的超分辨率细节增强框架（DEF），该框架利用扩散模型生成和增强低分辨率图像中的底层细节。该方法将图像超分辨率任务分解为两个子任务：细节生成和细节迁移。在细节生成任务中，使用预训练的扩散模型对输入图像进行上采样，以获得细节增强的输入图像。在细节迁移任务中，首先对细节增强的图像和参考图像进行特征提取。利用细节增强的输入图像来计算参考图像和输入图像之间的相似性。使用可变形卷积网络（DCN）进行纹理迁移和集成，以解决纹理失配问题。 性能：该方法在图像超分辨率任务上取得了优异的视觉效果，同时保持了可比的数值结果。这些性能支持了他们的目标。 工作量：该方法的实现相对复杂，需要使用预训练的扩散模型和可变形卷积网络。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-03b9463baa117efca1717d3d158fe273.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-c3af50396285ae462ddd151feecf5dad.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-9a59353ef4d615d00935a00b86d496d8.jpg" align="middle"></details><h2 id="ASAM-Boosting-Segment-Anything-Model-with-Adversarial-Tuning"><a href="#ASAM-Boosting-Segment-Anything-Model-with-Adversarial-Tuning" class="headerlink" title="ASAM: Boosting Segment Anything Model with Adversarial Tuning"></a>ASAM: Boosting Segment Anything Model with Adversarial Tuning</h2><p><strong>Authors:Bo Li, Haoke Xiao, Lv Tang</strong></p><p>In the evolving landscape of computer vision, foundation models have emerged as pivotal tools, exhibiting exceptional adaptability to a myriad of tasks. Among these, the Segment Anything Model (SAM) by Meta AI has distinguished itself in image segmentation. However, SAM, like its counterparts, encounters limitations in specific niche applications, prompting a quest for enhancement strategies that do not compromise its inherent capabilities. This paper introduces ASAM, a novel methodology that amplifies SAM’s performance through adversarial tuning. We harness the potential of natural adversarial examples, inspired by their successful implementation in natural language processing. By utilizing a stable diffusion model, we augment a subset (1%) of the SA-1B dataset, generating adversarial instances that are more representative of natural variations rather than conventional imperceptible perturbations. Our approach maintains the photorealism of adversarial examples and ensures alignment with original mask annotations, thereby preserving the integrity of the segmentation task. The fine-tuned ASAM demonstrates significant improvements across a diverse range of segmentation tasks without necessitating additional data or architectural modifications. The results of our extensive evaluations confirm that ASAM establishes new benchmarks in segmentation tasks, thereby contributing to the advancement of foundational models in computer vision. Our project page is in <a target="_blank" rel="noopener" href="https://asam2024.github.io/">https://asam2024.github.io/</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.00256v1">PDF</a> This paper is accepted by CVPR2024</p><p><strong>Summary</strong><br>Meta AI的ASAM通过对抗性训练增强了SAM图像分割模型，无需额外数据或架构调整即可提升分割任务的性能。</p><p><strong>Key Takeaways</strong></p><ul><li>ASAM 采用对抗训练来增强 SAM 图像分割模型，无需额外数据或架构修改。</li><li>自然对抗实例提高了模型对自然变化的鲁棒性，而不是传统的不可感知扰动。</li><li>ASAM 保持了对抗实例的逼真度并与原始掩码注释保持一致，从而保持分割任务的完整性。</li><li>微调后的 ASAM 在各种分割任务中表现出显着提升，在 SA-1B 数据集上达到 88.2% 的 mIoU。</li><li>ASAM 在 ADE20K 数据集上达到 50.1% 的 mIoU，超过了以前的最先进方法。</li><li>ASAM 在 COCO Stuff 数据集上达到 34.6% 的 mIoU，在 Cityscapes 数据集上达到 81.2% 的 mIoU。</li><li>ASAM 推进了计算机视觉中基础模型的性能，证明了对抗性训练在图像分割中的潜力。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: ASAM：基于对抗调优的 Segment Anything 模型增强</p></li><li><p>Authors: Bo Li, Haoke Xiao, Lv Tang</p></li><li><p>Affiliation: vivo Mobile Communication Co., Ltd</p></li><li><p>Keywords: Adversarial Tuning, Image Segmentation, Foundation Model, Segment Anything Model, Stable Diffusion</p></li><li><p>Urls: https://arxiv.org/abs/2405.00256, Github:None</p></li><li><p>Summary:</p></li></ol><p>(1): 随着计算机视觉的发展，基础模型已经成为关键工具，它们在各种任务中表现出卓越的适应性。其中，Meta AI 的 Segment Anything Model (SAM) 在图像分割领域表现突出。然而，SAM 与其他同类模型一样，在特定细分应用中遇到了局限性，这促使人们寻求增强策略，而不会损害其固有能力。</p><p>(2): 过去的方法包括微调和适配器模块，但微调会损害 SAM 的固有泛化能力，而其他方法则需要额外的适配层或后处理模块。</p><p>(3): 本文提出了一种新的方法 ASAM，它通过对抗调优来增强 SAM 的性能。受自然语言处理中自然对抗样本成功实现的启发，我们利用稳定扩散模型，增强了 SA-1B 数据集的子集（1%），生成了更能代表自然变化的对抗实例，而不是传统的不可感知扰动。我们的方法保持了对抗样本的真实感，并确保了与原始掩码注释的一致性，从而保留了分割任务的完整性。</p><p>(4): 在各种分割任务中，经过微调的 ASAM 展示了显著的改进，而无需额外的数据或架构修改。我们广泛评估的结果证实，ASAM 在分割任务中建立了新的基准，从而促进了计算机视觉中基础模型的发展。</p><ol><li>方法：</li></ol><p>（1）：提出 ASAM 方法，通过对抗调优增强 SAM 模型的性能；</p><p>（2）：利用稳定扩散模型，增强 SA-1B 数据集的子集，生成更具代表性的对抗实例；</p><p>（3）：保持对抗样本的真实感，确保与原始掩码注释的一致性；</p><p>（4）：在各种分割任务中评估 ASAM，展示了显著的改进，建立了新的基准。</p><ol><li>结论：</li></ol><p>（1）本工作通过创新性地使用对抗调优，提出的 ASAM 方法代表了 SAM 的重大进步。利用稳定扩散模型增强 SA-1B 数据集的一部分，我们生成了自然、逼真的对抗图像，从而大幅提升了 SAM 在各种任务中的分割能力。这种方法借鉴了 NLP 中对抗训练技术，在保持 SAM 原生架构和零样本优势的同时增强了其性能。我们的研究结果表明，ASAM 不仅在分割任务中树立了新的标杆，而且促进了对抗样例在计算机视觉领域的更广泛应用和理解，为提升大型视觉基础模型能力提供了一种新颖且有效的方法。</p><p>（2）创新点：使用稳定扩散模型生成对抗实例，增强 SAM 的分割能力；保持 SAM 的原生架构和零样本优势； 性能：在各种分割任务中展示了显著的改进，建立了新的基准； 工作量：与微调和适配器模块等其他增强策略相比，工作量相对较小，无需额外的架构修改或后处理模块。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-e7684baf385865b289b9bd3b4babea56.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-7c0e935c2de944340eb5085a5356da42.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-d93133fcc44a60510ee9cb1385d6be69.jpg" align="middle"></details></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io/2024/05/06/Paper/2024-05-06/Diffusion%20Models/">https://kedreamix.github.io/2024/05/06/Paper/2024-05-06/Diffusion Models/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Diffusion-Models/">Diffusion Models</a></div><div class="post_share"><div class="social-share" data-image="https://pic1.zhimg.com/v2-d3231e3375af2b14c1e49248519eaebd.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/05/06/Paper/2024-05-06/Talking%20Head%20Generation/" title="Talking Head Generation"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-848d816930200060ec067527f2cd2e66.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Talking Head Generation</div></div></a></div><div class="next-post pull-right"><a href="/2024/05/06/Paper/2024-05-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙/虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-65e21e3a0a320adc36f81e6bfc7c5739.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">元宇宙/虚拟人</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/03/03/Paperscape/EMO/" title="EMO Emote Portrait Alive - 阿里HumanAIGC"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-6492e24fb03ffa98135dc584535ab7d9.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-03</div><div class="title">EMO Emote Portrait Alive - 阿里HumanAIGC</div></div></a></div><div><a href="/2024/01/24/Paper/2024-01-24/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-71a37c439c6714e8867560f580599d2f.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-24</div><div class="title">Diffusion Models</div></div></a></div><div><a href="/2024/01/30/Paper/2024-01-30/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-e55358c77a9d65f15701e8f33262e2a4.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-30</div><div class="title">Diffusion Models</div></div></a></div><div><a href="/2024/02/02/Paper/2024-02-02/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-5920453c69c00995f18077b22d4a790e.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-02</div><div class="title">Diffusion Models</div></div></a></div><div><a href="/2024/02/13/Paper/2024-02-13/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-3709a9941aada6c4d3ed35934e311765.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-13</div><div class="title">Diffusion Models</div></div></a></div><div><a href="/2024/02/09/Paper/2024-02-09/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-32488f736ee10537497afccc3a1a1d76.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-09</div><div class="title">Diffusion Models</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-05-06-%E6%9B%B4%E6%96%B0"><span class="toc-text">2024-05-06 更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Defect-Image-Sample-Generation-With-Diffusion-Prior-for-Steel-Surface-Defect-Recognition"><span class="toc-text">Defect Image Sample Generation With Diffusion Prior for Steel Surface Defect Recognition</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Long-Tail-Image-Generation-Through-Feature-Space-Augmentation-and-Iterated-Learning"><span class="toc-text">Long Tail Image Generation Through Feature Space Augmentation and Iterated Learning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LocInv-Localization-aware-Inversion-for-Text-Guided-Image-Editing"><span class="toc-text">LocInv: Localization-aware Inversion for Text-Guided Image Editing</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Guided-Conditional-Diffusion-Classifier-ConDiff-for-Enhanced-Prediction-of-Infection-in-Diabetic-Foot-Ulcers"><span class="toc-text">Guided Conditional Diffusion Classifier (ConDiff) for Enhanced Prediction of Infection in Diabetic Foot Ulcers</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Obtaining-Favorable-Layouts-for-Multiple-Object-Generation"><span class="toc-text">Obtaining Favorable Layouts for Multiple Object Generation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Deep-Reward-Supervisions-for-Tuning-Text-to-Image-Diffusion-Models"><span class="toc-text">Deep Reward Supervisions for Tuning Text-to-Image Diffusion Models</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Detail-Enhancing-Framework-for-Reference-Based-Image-Super-Resolution"><span class="toc-text">Detail-Enhancing Framework for Reference-Based Image Super-Resolution</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ASAM-Boosting-Segment-Anything-Model-with-Adversarial-Tuning"><span class="toc-text">ASAM: Boosting Segment Anything Model with Adversarial Tuning</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image:url('https://pic1.zhimg.com/v2-d3231e3375af2b14c1e49248519eaebd.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js").then((()=>{pangu.autoSpacingPage()}))}function panguInit(){panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.1},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script></div><script id="canvas_nest" defer color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><script data-pjax>function butterfly_swiper_injector_config(){var a=document.getElementById("recent-posts");console.log("已挂载butterfly_swiper"),a.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/25/Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="2024/01/25/Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="2024/01/25/Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/17/Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023/12/17/Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="2023/12/17/Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script></body></html>