<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>Diffusion Models | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-05-28  DiffCalib Reformulating Monocular Camera Calibration as Diffusion-Based   Dense Incident Map Generation"><meta property="og:type" content="article"><meta property="og:title" content="Diffusion Models"><meta property="og:url" content="https://kedreamix.github.io/2024/05/28/Paper/2024-05-28/Diffusion%20Models/index.html"><meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World"><meta property="og:description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-05-28  DiffCalib Reformulating Monocular Camera Calibration as Diffusion-Based   Dense Incident Map Generation"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://pic1.zhimg.com/v2-36a0effe69414b2ffa084f4cd6a69d06.jpg"><meta property="article:published_time" content="2024-05-27T17:19:08.000Z"><meta property="article:modified_time" content="2024-05-28T08:34:32.613Z"><meta property="article:author" content="Kedreamix"><meta property="article:tag" content="Diffusion Models"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://pic1.zhimg.com/v2-36a0effe69414b2ffa084f4cd6a69d06.jpg"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/2024/05/28/Paper/2024-05-28/Diffusion%20Models/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-74LZ5BEQQ1")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!0,languages:{hits_empty:"æ‰¾ä¸åˆ°æ‚¨æŸ¥è¯¢çš„å†…å®¹ï¼š${query}",hits_stats:"å…±æ‰¾åˆ° ${hits} ç¯‡æ–‡ç« "}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"ç¹",msgToSimplifiedChinese:"ç®€"},noticeOutdate:{limitDay:500,position:"top",messagePrev:"It has been",messageNext:"days since the last update, the content of the article may be outdated."},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:200},copy:{success:"å¤åˆ¶æˆåŠŸ",error:"å¤åˆ¶é”™è¯¯",noSupport:"æµè§ˆå™¨ä¸æ”¯æŒ"},relativeDate:{homepage:!0,post:!0},runtime:"å¤©",dateSuffix:{just:"åˆšåˆš",min:"åˆ†é’Ÿå‰",hour:"å°æ—¶å‰",day:"å¤©å‰",month:"ä¸ªæœˆå‰"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js",buttonText:"åŠ è½½æ›´å¤š"},isPhotoFigcaption:!0,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"Diffusion Models",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-05-28 16:34:32"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach((e=>{n.setAttribute(e,t[e])})),document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)})),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=24?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 7.0.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml"></head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{const d=document.getElementById("loading-box"),e=document.body,o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l(),window.addEventListener("load",(()=>{o()}))})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">æ–‡ç« </div><div class="length-num">181</div></a><a href="/tags/"><div class="headline">æ ‡ç­¾</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">åˆ†ç±»</div><div class="length-num">6</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>é¦–é¡µ</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>æ—¶é—´è½´</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>æ ‡ç­¾</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>åˆ†ç±»</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>å‹é“¾</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url('https://pic1.zhimg.com/v2-36a0effe69414b2ffa084f4cd6a69d06.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i> <span>æœç´¢</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>é¦–é¡µ</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>æ—¶é—´è½´</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>æ ‡ç­¾</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>åˆ†ç±»</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>å‹é“¾</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Diffusion Models</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">å‘è¡¨äº</span><time class="post-meta-date-created" datetime="2024-05-27T17:19:08.000Z" title="å‘è¡¨äº 2024-05-28 01:19:08">2024-05-28</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">æ›´æ–°äº</span><time class="post-meta-date-updated" datetime="2024-05-28T08:34:32.613Z" title="æ›´æ–°äº 2024-05-28 16:34:32">2024-05-28</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">å­—æ•°æ€»è®¡:</span><span class="word-count">6.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">é˜…è¯»æ—¶é•¿:</span><span>23åˆ†é’Ÿ</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="Diffusion Models"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">é˜…è¯»é‡:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-05-28-æ›´æ–°"><a href="#2024-05-28-æ›´æ–°" class="headerlink" title="2024-05-28 æ›´æ–°"></a>2024-05-28 æ›´æ–°</h1><h2 id="DiffCalib-Reformulating-Monocular-Camera-Calibration-as-Diffusion-Based-Dense-Incident-Map-Generation"><a href="#DiffCalib-Reformulating-Monocular-Camera-Calibration-as-Diffusion-Based-Dense-Incident-Map-Generation" class="headerlink" title="DiffCalib: Reformulating Monocular Camera Calibration as Diffusion-Based   Dense Incident Map Generation"></a>DiffCalib: Reformulating Monocular Camera Calibration as Diffusion-Based Dense Incident Map Generation</h2><p><strong>Authors:Xiankang He, Guangkai Xu, Bo Zhang, Hao Chen, Ying Cui, Dongyan Guo</strong></p><p>Monocular camera calibration is a key precondition for numerous 3D vision applications. Despite considerable advancements, existing methods often hinge on specific assumptions and struggle to generalize across varied real-world scenarios, and the performance is limited by insufficient training data. Recently, diffusion models trained on expansive datasets have been confirmed to maintain the capability to generate diverse, high-quality images. This success suggests a strong potential of the models to effectively understand varied visual information. In this work, we leverage the comprehensive visual knowledge embedded in pre-trained diffusion models to enable more robust and accurate monocular camera intrinsic estimation. Specifically, we reformulate the problem of estimating the four degrees of freedom (4-DoF) of camera intrinsic parameters as a dense incident map generation task. The map details the angle of incidence for each pixel in the RGB image, and its format aligns well with the paradigm of diffusion models. The camera intrinsic then can be derived from the incident map with a simple non-learning RANSAC algorithm during inference. Moreover, to further enhance the performance, we jointly estimate a depth map to provide extra geometric information for the incident map estimation. Extensive experiments on multiple testing datasets demonstrate that our model achieves state-of-the-art performance, gaining up to a 40% reduction in prediction errors. Besides, the experiments also show that the precise camera intrinsic and depth maps estimated by our pipeline can greatly benefit practical applications such as 3D reconstruction from a single in-the-wild image.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.15619v1">PDF</a></p><p><strong>Summary</strong><br>å•ç›®ç›¸æœºæ ¡å‡†æ˜¯ä¼—å¤š3Dè§†è§‰åº”ç”¨çš„å…³é”®å…ˆå†³æ¡ä»¶ã€‚æœ€è¿‘ï¼ŒåŸºäºå¤§è§„æ¨¡æ•°æ®é›†è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è¢«è¯å®èƒ½å¤Ÿç”Ÿæˆå¤šæ ·ä¸”é«˜è´¨é‡çš„å›¾åƒï¼Œä¸ºå•ç›®ç›¸æœºå†…åœ¨ä¼°è®¡æä¾›æ›´å¼ºå¤§å’Œå‡†ç¡®çš„æ”¯æŒã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>å•ç›®ç›¸æœºæ ¡å‡†å¯¹äºå¤šç§3Dè§†è§‰åº”ç”¨è‡³å…³é‡è¦</li><li>æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆå¤šæ ·ä¸”é«˜è´¨é‡çš„å›¾åƒ</li><li>é€šè¿‡åˆ©ç”¨æ‰©æ•£æ¨¡å‹ä¸­çš„è§†è§‰çŸ¥è¯†ï¼Œèƒ½å¤Ÿå®ç°æ›´ç¨³å¥å’Œå‡†ç¡®çš„å•ç›®ç›¸æœºå†…åœ¨ä¼°è®¡</li><li>é€šè¿‡å°†ä¼°è®¡ç›¸æœºå†…åœ¨å‚æ•°çš„é—®é¢˜é‡æ–°æ„å»ºä¸ºå¯†é›†å…¥å°„å›¾ç”Ÿæˆä»»åŠ¡ï¼Œèƒ½å¤Ÿå®ç°æ›´ç®€å•çš„æ¨æ–­è¿‡ç¨‹</li><li>è”åˆä¼°è®¡æ·±åº¦å›¾èƒ½å¤Ÿè¿›ä¸€æ­¥æå‡æ€§èƒ½</li><li>å®éªŒè¯æ˜è¯¥æ¨¡å‹è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œé¢„æµ‹è¯¯å·®é™ä½äº†40%</li><li>ç²¾ç¡®çš„ç›¸æœºå†…åœ¨å’Œæ·±åº¦å›¾èƒ½å¤Ÿæå¤§åœ°ä¿ƒè¿›ä»å•å¼ é‡å¤–å›¾åƒè¿›è¡Œçš„3Dé‡å»ºç­‰å®é™…åº”ç”¨</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: DiffCalib: Reformulating Monocular Camera Calibration as Diffusion-Based Dense Incident Map Generation (DiffCalibï¼šå°†å•ç›®æ‘„åƒæœºæ ¡å‡†é‡æ„ä¸ºåŸºäºæ‰©æ•£çš„å¯†é›†å…¥å°„å›¾ç”Ÿæˆ)</p></li><li><p>Authors: Xiankang He, Guangkai Xu, Bo Zhang, Hao Chen, Ying Cui, Dongyan Guo</p></li><li><p>Affiliation: æµ™æ±Ÿå·¥ä¸šå¤§å­¦</p></li><li><p>Keywords: monocular camera calibration, diffusion models, incident map generation</p></li><li><p>Urls: arXiv:2405.15619v1, Github:None</p></li><li><p>Summary:</p></li><li><p>(1):æœ¬æ–‡çš„ç ”ç©¶èƒŒæ™¯æ˜¯å•ç›®æ‘„åƒæœºæ ¡å‡†ï¼Œè¿™æ˜¯è®¸å¤šä¸‰ç»´è§†è§‰åº”ç”¨çš„å…³é”®å‰ææ¡ä»¶ã€‚</p></li><li><p>(2):è¿‡å»çš„æ–¹æ³•å­˜åœ¨ä¸€äº›å‡è®¾å’Œé™åˆ¶ï¼Œæ— æ³•åœ¨ä¸åŒçš„çœŸå®ä¸–ç•Œåœºæ™¯ä¸­æ³›åŒ–ï¼Œå¹¶ä¸”å—é™äºè®­ç»ƒæ•°æ®çš„ä¸è¶³ã€‚æœ€è¿‘ï¼Œæ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢å–å¾—äº†æˆåŠŸï¼Œè¿™å¯å‘äº†æˆ‘ä»¬ä½¿ç”¨æ‰©æ•£æ¨¡å‹æ¥å®ç°æ›´é²æ£’å’Œå‡†ç¡®çš„å•ç›®æ‘„åƒæœºæ ¡å‡†ã€‚</p></li><li><p>(3):æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•æ˜¯å°†å•ç›®æ‘„åƒæœºæ ¡å‡†é—®é¢˜é‡æ„ä¸ºåŸºäºæ‰©æ•£çš„å¯†é›†å…¥å°„å›¾ç”Ÿæˆä»»åŠ¡ï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ç”Ÿæˆå…¥å°„å›¾ï¼Œç„¶åä½¿ç”¨RANSACç®—æ³•æ¨æ–­æ‘„åƒæœºå‚ã€‚</p></li><li><p>(4):æœ¬æ–‡çš„æ–¹æ³•åœ¨å•ç›®æ‘„åƒæœºæ ¡å‡†ä»»åŠ¡ä¸Šå–å¾—äº†å¾ˆå¥½çš„æ€§èƒ½ï¼Œè¯æ˜äº†æ‰©æ•£æ¨¡å‹åœ¨ç†è§£è§†è§‰ä¿¡æ¯æ–¹é¢çš„æ½œåŠ›ï¼Œå¹¶ä¸”å¯ä»¥ç”¨äºåœ¨é‡ä¸‰ç»´é‡å»ºä»»åŠ¡ä¸­ã€‚</p></li><li><p>æ–¹æ³•ï¼š</p></li><li><p>(1)ï¼šå°†å•ç›®æ‘„åƒæœºæ ¡å‡†é—®é¢˜é‡æ„ä¸ºåŸºäºæ‰©æ•£çš„å¯†é›†å…¥å°„å›¾ç”Ÿæˆä»»åŠ¡ï¼Œä»¥ä¾¿èƒ½å¤Ÿåˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ç”Ÿæˆå…¥å°„å›¾ã€‚</p></li><li><p>(2)ï¼šä½¿ç”¨Stable Diffusion v2.1æ¨¡å‹å¯¹å…¥å°„å›¾è¿›è¡Œç¼–ç å’Œè§£ç ï¼Œç”Ÿæˆå™ªå£°åçš„å…¥å°„å›¾latent codesï¼Œå¹¶è®­ç»ƒU-Netæ¨¡å‹æ¥é¢„æµ‹å™ªå£°ã€‚</p></li><li><p>(3)ï¼šå°†æ·±åº¦å›¾å’Œå…¥å°„å›¾è”åˆå­¦ä¹ ï¼Œä»¥æé«˜å…¥å°„å›¾ç”Ÿæˆçš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚</p></li><li><p>(4)ï¼šä½¿ç”¨RANSACç®—æ³•ä»ç”Ÿæˆçš„å…¥å°„å›¾ä¸­æ¢å¤æ‘„åƒæœºçš„å†…å‚æ•°çŸ©é˜µKã€‚</p></li><li><p>(5)ï¼šä½¿ç”¨ensembleæ–¹æ³•æ¥æé«˜å…¥å°„å›¾ç”Ÿæˆçš„å‡†ç¡®æ€§å’Œç¨³å®šæ€§ã€‚</p></li><li><p>(6)ï¼šä½¿ç”¨æ¢å¤çš„æ‘„åƒæœºå†…å‚æ•°çŸ©é˜µKæ¥è¿›è¡Œå•ç›®æ‘„åƒæœºæ ¡å‡†ã€‚</p></li><li><p>Conclusion:</p></li><li><p>(1): è¿™ç¯‡æ–‡ç« çš„æ„ä¹‰åœ¨äºæå‡ºäº†å¯¹äº[é¢†åŸŸ]çš„æ–°æ€è·¯ï¼Œä¸ºè¯¥é¢†åŸŸçš„ç ”ç©¶å’Œå‘å±•å¸¦æ¥äº†æ–°çš„å¯å‘å’Œæ–¹å‘ï¼›</p></li><li>(2): Innovation point: è¯¥æ–‡ç« çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ä¸€ç§å…¨æ–°çš„[åˆ›æ–°ç‚¹]ï¼Œçªç ´äº†ä¼ ç»Ÿçš„[åˆ›æ–°ç‚¹]æ–¹å¼ï¼› Performance: è¯¥æ–‡ç« åœ¨å®éªŒè¡¨ç°æ–¹é¢å±•ç°å‡ºäº†è¾ƒé«˜çš„å‡†ç¡®æ€§å’Œç¨³å®šæ€§ï¼Œä½†ä»æœ‰å¾…è¿›ä¸€æ­¥æå‡ï¼› Workload: è¯¥æ–‡ç« çš„å·¥ä½œé‡è¾ƒå¤§ï¼Œéœ€è¦æ›´å¤šçš„å®éªŒæ•°æ®å’Œåˆ†ææ¥æ”¯æ’‘å…¶ç»“è®ºã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-02a306a749ab4f7167af1ae9e9bd38f3.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-3354b1c0f182b11d7a2fe0d1f53745ed.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-a3bcd389775a3247ad6697fadd1fd9cd.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-8a6244aa42d8f424a5319ca260b17f35.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-36a0effe69414b2ffa084f4cd6a69d06.jpg" align="middle"></details><h2 id="Defensive-Unlearning-with-Adversarial-Training-for-Robust-Concept-Erasure-in-Diffusion-Models"><a href="#Defensive-Unlearning-with-Adversarial-Training-for-Robust-Concept-Erasure-in-Diffusion-Models" class="headerlink" title="Defensive Unlearning with Adversarial Training for Robust Concept   Erasure in Diffusion Models"></a>Defensive Unlearning with Adversarial Training for Robust Concept Erasure in Diffusion Models</h2><p><strong>Authors:Yimeng Zhang, Xin Chen, Jinghan Jia, Yihua Zhang, Chongyu Fan, Jiancheng Liu, Mingyi Hong, Ke Ding, Sijia Liu</strong></p><p>Diffusion models (DMs) have achieved remarkable success in text-to-image generation, but they also pose safety risks, such as the potential generation of harmful content and copyright violations. The techniques of machine unlearning, also known as concept erasing, have been developed to address these risks. However, these techniques remain vulnerable to adversarial prompt attacks, which can prompt DMs post-unlearning to regenerate undesired images containing concepts (such as nudity) meant to be erased. This work aims to enhance the robustness of concept erasing by integrating the principle of adversarial training (AT) into machine unlearning, resulting in the robust unlearning framework referred to as AdvUnlearn. However, achieving this effectively and efficiently is highly nontrivial. First, we find that a straightforward implementation of AT compromises DMsâ€™ image generation quality post-unlearning. To address this, we develop a utility-retaining regularization on an additional retain set, optimizing the trade-off between concept erasure robustness and model utility in AdvUnlearn. Moreover, we identify the text encoder as a more suitable module for robustification compared to UNet, ensuring unlearning effectiveness. And the acquired text encoder can serve as a plug-and-play robust unlearner for various DM types. Empirically, we perform extensive experiments to demonstrate the robustness advantage of AdvUnlearn across various DM unlearning scenarios, including the erasure of nudity, objects, and style concepts. In addition to robustness, AdvUnlearn also achieves a balanced tradeoff with model utility. To our knowledge, this is the first work to systematically explore robust DM unlearning through AT, setting it apart from existing methods that overlook robustness in concept erasing. Codes are available at: <a target="_blank" rel="noopener" href="https://github.com/OPTML-Group/AdvUnlearn">https://github.com/OPTML-Group/AdvUnlearn</a></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.15234v1">PDF</a> Codes are available at <a target="_blank" rel="noopener" href="https://github.com/OPTML-Group/AdvUnlearn">https://github.com/OPTML-Group/AdvUnlearn</a></p><p><strong>Summary</strong><br>åŸºäºå¯¹æŠ—è®­ç»ƒå¢å¼ºæœºå™¨unlearningï¼Œæå‡ºAdvUnlearnæ¡†æ¶ï¼Œä»¥æé«˜æ¦‚å¿µæ“¦é™¤çš„é²æ£’æ€§ã€‚</p><p><strong>Key Takeaways</strong><br>â€¢ Diffusionæ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†ä¹Ÿå­˜åœ¨å®‰å…¨é£é™©ï¼Œå¦‚ç”Ÿæˆæœ‰å®³å†…å®¹å’Œç‰ˆæƒè¿è§„ã€‚<br>â€¢ æœºå™¨unlearningæŠ€æœ¯å¯ä»¥è§£å†³è¿™äº›é£é™©ï¼Œä½†æ˜“å—åˆ°å¯¹æŠ—promptæ”»å‡»ã€‚<br>â€¢ æœ¬å·¥ä½œæå‡ºAdvUnlearnæ¡†æ¶ï¼Œé€šè¿‡å°†å¯¹æŠ—è®­ç»ƒåŸåˆ™é›†æˆåˆ°æœºå™¨unlearningä¸­ï¼Œä»¥æé«˜æ¦‚å¿µæ“¦é™¤çš„é²æ£’æ€§ã€‚<br>â€¢ AdvUnlearnæ¡†æ¶ä½¿ç”¨utility-retaining regularizationæ¥å¹³è¡¡æ¦‚å¿µæ“¦é™¤é²æ£’æ€§å’Œæ¨¡å‹å®ç”¨æ€§ã€‚<br>â€¢ æ–‡æœ¬ç¼–ç å™¨æ˜¯å®ç°æœºå™¨unlearningçš„æ›´é€‚åˆæ¨¡å—ã€‚<br>â€¢ AdvUnlearnæ¡†æ¶å¯ä»¥åœ¨å„ç§Diffusionæ¨¡å‹unlearningåœºæ™¯ä¸‹å®ç°é²æ£’çš„æ¦‚å¿µæ“¦é™¤ã€‚<br>â€¢ æœ¬å·¥ä½œæ˜¯é¦–æ¬¡ç³»ç»Ÿåœ°æ¢ç´¢é€šè¿‡å¯¹æŠ—è®­ç»ƒå®ç°é²æ£’çš„Diffusionæ¨¡å‹unlearningã€‚</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: AdvUnlearn: Robust Unlearning for Diffusion Models (Diffusionæ¨¡å‹çš„é²æ£’unlearning)</p></li><li><p>Authors: (no authors listed)</p></li><li><p>Affiliation: æ— </p></li><li><p>Keywords: Diffusion Models, Machine Unlearning, Adversarial Training, Text-to-Image Generation</p></li><li><p>Urls: https://github.com/OPTML-Group/AdvUnlearn</p></li><li><p>Summary:</p><ul><li><p>(1):éšç€Diffusionæ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­çš„æˆåŠŸï¼Œå®ƒä»¬ä¹Ÿå¸¦æ¥äº†å®‰å…¨é£é™©ï¼Œå¦‚ç”Ÿæˆæœ‰å®³å†…å®¹å’Œç‰ˆæƒè¿åã€‚ä¸ºè§£å†³è¿™äº›é£é™©ï¼Œæœºå™¨unlearningæŠ€æœ¯è¢«å¼€å‘å‡ºæ¥ï¼Œä½†æ˜¯è¿™äº›æŠ€æœ¯ä»æ˜“å—å¯¹æŠ—æ€§promptæ”»å‡»çš„å½±å“ã€‚</p></li><li><p>(2):è¿‡å»çš„æ–¹æ³•ï¼Œå¦‚ScissorHandså’ŒEraseDiffï¼Œè™½ç„¶å¯ä»¥å®ç°é«˜çš„unlearning robustnessï¼Œä½†æ˜¯å®ƒä»¬å›¾åƒç”Ÿæˆè´¨é‡ä¸‹é™æ˜æ˜¾ã€‚è¿™äº›æ–¹æ³•çš„motivationä¸è¶³ï¼Œæ— æ³•è§£å†³æœºå™¨unlearningä¸­çš„å®‰å…¨é£é™©ã€‚</p></li><li><p>(3):æœ¬æ–‡æå‡ºäº†AdvUnlearnæ¡†æ¶ï¼Œç»“åˆå¯¹æŠ—æ€§è®­ç»ƒæ¥å¢å¼ºæœºå™¨unlearningçš„robustnessã€‚è¯¥æ¡†æ¶ä½¿ç”¨utility-retaining regularizationæ¥å¹³è¡¡æ¦‚å¿µæ“¦é™¤çš„robustnesså’Œæ¨¡å®ç”¨æ€§ï¼Œå¹¶å°†æ–‡æœ¬ç¼–ç å™¨ä½œä¸ºrobustificationçš„æ¨¡å—ã€‚</p></li><li><p>(4):æœ¬æ–‡åœ¨å¤šä¸ªDiffusionæ¨¡å‹unlearningåœºæ™¯ä¸­è¿›è¡Œäº†å®éªŒï¼ŒåŒ…æ‹¬è£¸ä½“ã€å¯¹è±¡å’Œé£æ ¼æ¦‚å¿µçš„æ“¦é™¤ã€‚ç»“æœè¡¨æ˜ï¼ŒAdvUnlearnæ¡†æ¶å¯ä»¥å®ç°robustçš„æœºå™¨unlearningï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„å®ç”¨æ€§ã€‚</p></li><li>æ–¹æ³•ï¼š</li></ul></li><li><p>(1):æå‡ºAdvUnlearnæ¡†æ¶ï¼Œç»“åˆå¯¹æŠ—æ€§è®­ç»ƒæ¥å¢å¼ºæœºå™¨unlearningçš„robustnessï¼Œä½¿ç”¨utility-retaining regularizationæ¥å¹³è¡¡æ¦‚å¿µæ“¦é™¤çš„robustnesså’Œæ¨¡å®ç”¨ï¼Œå¹¶å°†æ–‡æœ¬ç¼–ç å™¨ä½œä¸ºrobustificationçš„æ¨¡å—ã€‚</p></li><li><p>(2):ä½¿ç”¨large language model (LLM)ä½œä¸ºjudgeæ¥ç­›é€‰ä¿ç•™promptï¼Œæ’é™¤ä¸ç›®æ ‡æ¦‚å¿µæ“¦é™¤ç›¸å…³çš„promptï¼Œä»è€Œç¡®ä¿å›¾åƒç”Ÿæˆè´¨é‡ä¸å—æŸå®³ã€‚</p></li><li><p>(3):å®šä¹‰utility-retaining regularizationæŸå¤±å‡½æ•°â„“ESDï¼Œpenalizeså›¾åƒç”Ÿæˆè´¨é‡çš„ä¸‹é™ï¼Œä½¿ç”¨å½“å‰Diffusionæ¨¡å‹Î¸ä¸åŸå§‹Î¸oä¸‹çš„ä¿ç•™æ¦‚å¿µËœcæ¥è®¡ç®—ã€‚</p></li><li><p>(4):ä½¿ç”¨fast attack generationæ–¹æ³•æ¥ç®€åŒ–AdvUnlearnçš„lower-levelä¼˜åŒ–ï¼Œä½¿ç”¨fast gradient sign method (FGSM)æ¥è§£å†³quadratic programï¼Œå¹¶ç”Ÿæˆå¯¹æŠ—æ€§promptã€‚</p></li><li><p>(5):å°†AdvUnlearnåº”ç”¨äºä¸åŒçš„Diffusionæ¨¡å‹unlearningåœºæ™¯ï¼ŒåŒ…æ‹¬è£¸ä½“ã€å¯¹è±¡å’Œé£æ ¼æ¦‚å¿µçš„æ“¦é™¤ï¼Œå¹¶è¯„ä¼°å…¶robustnesså’Œå›¾åƒç”Ÿæˆè´¨é‡ã€‚</p></li><li><p>(6):æ¯”è¾ƒAdvUnlearnä¸å…¶æ–¹æ³•ï¼ˆå¦‚ESDå’ŒAT-ESDï¼‰çš„æ€§èƒ½ï¼Œè¯æ˜AdvUnlearnå¯ä»¥å®ç°robustçš„æœºå™¨unlearningï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„å®ç”¨æ€§</p></li><li><p>(7):æ¢ç´¢AdvUnlearnçš„æ¨¡å—åŒ–è®¾è®¡ï¼Œè®¨è®ºå°†æ–‡æœ¬ç¼–ç å™¨ä½œä¸ºplug-in unlearnerçš„å¯èƒ½æ€§ï¼Œä»¥æé«˜æœºå™¨unlearningçš„æ•ˆç‡å’Œæ™®é€‚æ€§ã€‚</p></li><li><p>Conclusion:</p></li><li><p>(1):æœ¬æ–‡æå‡ºçš„AdvUnlearnæ¡†æ¶å¯¹Diffusionæ¨¡å‹çš„æœºå™¨unlearningé¢†åŸŸå…·æœ‰é‡è¦æ„ä¹‰ï¼Œå› ä¸ºå®ƒå¯ä»¥å¢å¼ºæœºå™¨unlearningçš„robustnessï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„å®ç”¨æ€§ã€‚</p></li><li><p>(2):Innovation point: æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æœºå™¨unlearningæ–¹æ³•ï¼Œç»“åˆå¯¹æŠ—æ€§è®­ç»ƒå’Œutility-retaining regularizationæ¥å¢å¼ºæœºå™¨unlearningçš„robustnessï¼›Performance: AdvUnlearnæ¡†æ¶åœ¨å¤šä¸ªDiffusionæ¨¡å‹unlearningåœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ï¼Œå®ç°äº†robustçš„æœºå™¨unlearningï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„å®ç”¨æ€§ï¼›Workload: æœ¬æ–‡çš„å®éªŒè®¾è®¡å’Œå®ç°ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºå’Œæ—¶é—´ã€‚</p></li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-12bc7afe95c87708c06799dd505c46da.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-c3f86497a08db26b9953f1bc30dad1c3.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-7ef67ded1db4d01263a65cdacd20797a.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-202a39b4f890f5df5c6e0f34c4f7a6a7.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-89575cd27c93753bf34b1aebf5ce8aef.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-005e6d2cd8b93a64b356e1bd2dd224c9.jpg" align="middle"></details><h2 id="DEEM-Diffusion-Models-Serve-as-the-Eyes-of-Large-Language-Models-for-Image-Perception"><a href="#DEEM-Diffusion-Models-Serve-as-the-Eyes-of-Large-Language-Models-for-Image-Perception" class="headerlink" title="DEEM: Diffusion Models Serve as the Eyes of Large Language Models for   Image Perception"></a>DEEM: Diffusion Models Serve as the Eyes of Large Language Models for Image Perception</h2><p><strong>Authors:Run Luo, Yunshui Li, Longze Chen, Wanwei He, Ting-En Lin, Ziqiang Liu, Lei Zhang, Zikai Song, Xiaobo Xia, Tongliang Liu, Min Yang, Binyuan Hui</strong></p><p>The development of large language models (LLMs) has significantly advanced the emergence of large multimodal models (LMMs). While LMMs have achieved tremendous success by promoting the synergy between multimodal comprehension and creation, they often face challenges when confronted with out-of-distribution data. This is primarily due to their reliance on image encoders trained to encode images into task-relevant features, which may lead them to disregard irrelevant details. Delving into the modeling capabilities of diffusion models for images naturally prompts the question: Can diffusion models serve as the eyes of large language models for image perception? In this paper, we propose DEEM, a simple and effective approach that utilizes the generative feedback of diffusion models to align the semantic distributions of the image encoder. This addresses the drawbacks of previous methods that solely relied on image encoders like ViT, thereby enhancing the modelâ€™s resilience against out-of-distribution samples and reducing visual hallucinations. Importantly, this is achieved without requiring additional training modules and with fewer training parameters. We extensively evaluated DEEM on both our newly constructed RobustVQA benchmark and another well-known benchmark, POPE, for object hallucination. Compared to the state-of-the-art interleaved content generation models, DEEM exhibits enhanced robustness and a superior capacity to alleviate model hallucinations while utilizing fewer trainable parameters, less pre-training data (10%), and a smaller base model size.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.15232v1">PDF</a> 25 pages</p><p><strong>Summary</strong><br>é€šè¿‡ä½¿ç”¨æ‰©æ•£æ¨¡å‹ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºDEEMçš„ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆåé¦ˆæ¥è°ƒæ•´å›¾åƒç¼–ç å™¨çš„è¯­ä¹‰åˆ†å¸ƒï¼Œä»è€Œå¢å¼ºäº†æ¨¡å‹å¯¹äºè¶…å‡ºåˆ†å¸ƒæ•°æ®çš„é²æ£’æ€§ï¼Œå‡å°‘äº†è§†è§‰å¹»è§‰ï¼ŒåŒæ—¶æ— éœ€é¢å¤–çš„è®­ç»ƒæ¨¡å—å’Œæ›´å°‘çš„è®­ç»ƒå‚æ•°ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‘å±•æ¨åŠ¨äº†å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„å‡ºç°ï¼›</li><li>LMMsåœ¨ä¿ƒè¿›å¤šæ¨¡æ€ç†è§£å’Œåˆ›ä½œæ–¹é¢å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä½†åœ¨å¤„ç†è¶…å‡ºåˆ†å¸ƒæ•°æ®æ—¶é¢ä¸´æŒ‘æˆ˜ï¼›</li><li>DEEMåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆåé¦ˆæ¥è°ƒæ•´å›¾åƒç¼–ç å™¨çš„è¯­ä¹‰åˆ†å¸ƒï¼Œè§£å†³äº†ä»¥å¾€ä»…ä¾èµ–äºå›¾åƒç¼–ç å™¨çš„æ–¹æ³•çš„ç¼ºé™·ï¼›</li><li>DEEMåœ¨RobustVQAåŸºå‡†å’ŒPOPEåŸºå‡†ä¸Šå¾—åˆ°äº†å¹¿æ³›è¯„ä¼°ï¼Œè¡¨ç°å‡ºå“è¶Šçš„é²æ£’æ€§å’Œå‡å°‘æ¨¡å‹å¹»è§‰çš„èƒ½åŠ›ï¼›</li><li>DEEMç›¸è¾ƒäºæœ€å…ˆè¿›çš„äº¤æ›¿å†…å®¹ç”Ÿæˆæ¨¡å‹ï¼Œå±•ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ï¼Œå¹¶åˆ©ç”¨æ›´å°‘çš„å¯è®­ç»ƒå‚æ•°ã€æ›´å°‘çš„é¢„è®­ç»ƒæ•°æ®ï¼ˆ10%ï¼‰å’Œæ›´å°çš„åŸºç¡€æ¨¡å‹å°ºå¯¸ã€‚</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><h2>Paper:1</h2><p></p><ol><li><p>Title: DEEMï¼šä½¿ç”¨æ‰©æ•£æ¨¡å‹å¯¹å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„å›¾åƒæ„ŸçŸ¥è¿›è¡Œå¢å¼º (DEEM: Enhancing Image Perception of Large Multimodal Models with Diffusion Models)</p></li><li><p>Authors: (no author names provided)</p></li><li><p>Affiliation: æ—  (no affiliation provided)</p></li><li><p>Keywords: large language models, large multimodal models, diffusion models, image perception, robustness, hallucination</p></li><li><p>Urls: arXiv:2405.15232v1, Github: None</p></li><li><p>Summary:</p><ul><li><p>(1):è¯¥è®ºæ–‡çš„ç ”ç©¶èƒŒæ™¯æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„å‘å±•ï¼Œåè€…é€šè¿‡ç®€å•çš„æ˜ å°„æ¨¡å—å°†LLMsä¸å›¾åƒç¼–ç å™¨è¿æ¥èµ·æ¥ï¼Œå®ç°å¤šæ¨¡æ€ç†è§£ä»»åŠ¡ã€‚</p></li><li><p>(2):è¿‡å»çš„æ–¹æ³•ä¸»è¦ä¾èµ–å›¾åƒç¼–ç å™¨æ¥å°†å›¾åƒç¼–ç ä¸ºä»»åŠ¡ç›¸å…³ç‰¹å¾ï¼Œå¯èƒ½å¿½è§†æ— å…³ç»†èŠ‚ï¼Œä»è€Œå¯¼è‡´æ¨¡å‹å¯¹å¤–åˆ†å¸ƒæ•°æ®çš„robustnesså’Œhallucinationé—®é¢˜ã€‚</p></li><li><p>(3):æœ¬æ–‡æå‡ºçš„æ–¹æ³•æ˜¯DEEMï¼Œå®ƒä½¿ç”¨æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆåé¦ˆæ¥å¯¹é½å›¾åƒç¼–ç å™¨çš„è¯­ä¹‰åˆ†å¸ƒï¼Œæé«˜æ¨¡å‹å¯¹å¤–åˆ†å¸ƒæ•°æ®çš„robustnesså’Œå‡å°‘hallucinationã€‚</p></li><li><p>(4):è¯¥æ–¹æ³•åœ¨RobustVQAå’ŒPOPEä¸¤ä¸ªåŸºå‡†æµ‹è¯•æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜DEEMç›¸æ¯”äºå½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹å…·æœ‰æ›´å¥½çš„robustnesså’Œå‡å°‘hallucinationèƒ½åŠ›ï¼ŒåŒæ—¶è¿˜å¯ä»¥åœ¨å¤šæ¨¡æ€ä»»åŠ¡å¦‚è§†è§‰é—®ç­”ã€å›¾åƒå­—å¹•ç”Ÿæˆå’Œæ–‡æœ¬æ¡ä»¶å›¾åƒåˆæˆç­‰æ–¹é¢å–å¾—ç«äº‰æ€§çš„ç»“æœã€‚</p></li><li>æ–¹æ³•ï¼š</li></ul></li><li><p>(1)ï¼šé¦–å…ˆï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºæ–‡æœ¬ç¼–ç å™¨ï¼Œç”Ÿæˆå›¾åƒç›¸å…³çš„æ–‡æœ¬ç‰¹å¾ï¼Œä»¥ä¾¿ä¸å›¾åƒç¼–ç å™¨è¿›è¡Œå¯¹é½ã€‚</p></li><li><p>(2)ï¼šç„¶åï¼Œä½¿ç”¨æ‰©æ•£æ¨¡å‹ï¼ˆDiffusion Modelï¼‰å¯¹å›¾åƒç¼–ç å™¨çš„è¾“å‡ºè¿›è¡Œç”Ÿæˆåé¦ˆï¼Œä»¥è°ƒæ•´å›¾åƒç¼–ç å™¨è¯­ä¹‰åˆ†å¸ƒï¼Œæé«˜æ¨¡å‹å¯¹å¤–åˆ†å¸ƒæ•°æ®çš„robustnessã€‚</p></li><li><p>(3)ï¼šåœ¨ç”Ÿæˆåé¦ˆè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨å¯¹æŠ—è®­ç»ƒï¼ˆAdversarial Trainingï¼‰æ¥é¼“åŠ±å›¾åƒç¼–ç å™¨ç”Ÿæˆæ›´åŠ robustçš„ç‰¹å¾ï¼Œå‡å°‘hallucinationçš„å¯èƒ½æ€§ã€‚</p></li><li><p>(4)ï¼šæ¥ç€ï¼Œå¯¹DEEMæ¨¡å‹è¿›è¡Œå¤šæ¨¡æ€ä»»åŠ¡çš„fine-tuningï¼Œä¾‹å¦‚è§†è§‰é—®ç­”ã€å›¾åƒå­—å¹•ç”Ÿæˆå’Œæ–‡æœ¬æ¡ä»¶å›¾åƒåˆæˆç­‰ï¼Œä»¥æé«˜æ¨¡å‹åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</p></li><li><p>(5)ï¼šæœ€åï¼Œåœ¨RobustVQAå’ŒPOPEä¸¤ä¸ªåŸºå‡†æµ‹è¯•æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œè¯„ä¼°DEEMæ¨¡å‹çš„robustnesså’Œhallucinationèƒ½åŠ›ï¼Œä¸¦ä¸å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚</p></li><li><p>Conclusion:</p></li><li><p>(1): æœ¬ç ”ç©¶çš„æ„ä¹‰åœ¨äºæå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼ˆDEEMï¼‰ï¼Œé€šè¿‡ä½¿ç”¨æ‰©æ•£æ¨¡å‹å¯¹å¤§å‹å¤šæ¨¡æ€æ¨¡å‹è¿›è¡Œå›¾åƒæ„ŸçŸ¥å¢å¼ºï¼Œæœ‰æ•ˆæé«˜äº†æ¨¡å‹çš„é²æ£’æ€§å’Œå‡å°‘äº†è™šå‡æ„ŸçŸ¥ï¼Œä¸ºå¤šæ¨¡æ€ä»»åŠ¡çš„æ€§èƒ½æå‡æä¾›äº†æ–°çš„æ€è·¯ã€‚</p></li><li><p>(2): åˆ›æ–°ç‚¹ï¼šDEEMæ–¹æ³•åˆ©ç”¨æ‰©æ•£æ¨¡å‹å¯¹å›¾åƒç¼–ç å™¨çš„è¯­ä¹‰åˆ†å¸ƒè¿›è¡Œè°ƒæ•´ï¼Œåœ¨æé«˜æ¨¡å‹é²æ£’æ€§å’Œå‡å°‘è™šå‡æ„ŸçŸ¥æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ã€‚æ€§èƒ½ï¼šDEEMåœ¨RobustVQAå’ŒPOPEä¸¤ä¸ªåŸºå‡†æµ‹è¯•æ•°æ®é›†ä¸Šç›¸æ¯”å½“å‰æœ€å…ˆè¿›æ¨¡å‹å…·æœ‰æ›´å¥½çš„é²æ£’æ€§å’Œå‡å°‘è™šå‡æ„ŸçŸ¥èƒ½åŠ›ï¼Œå¹¶åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸Šå–å¾—äº†ç«äº‰æ€§çš„ç»“æœã€‚å·¥ä½œé‡ï¼šè®ºæ–‡æ‰€æå‡ºçš„DEEMæ–¹æ³•éœ€è¦è¿›ä¸€æ­¥å®éªŒå’ŒéªŒè¯ï¼Œä»¥ç¡®ä¿å…¶åœ¨ä¸åŒé¢†åŸŸçš„æ³›åŒ–æ€§èƒ½ï¼Œè¿™å¯èƒ½éœ€è¦æ›´å¤šçš„å·¥ä½œé‡æ¥æ”¯æŒã€‚</p></li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-c0b6103bc7ef9889b013616a33153dac.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-5911a832e2f068efcd4f1c57fb6c0989.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-2f388f04ad9850dd89191f6903b1cf64.jpg" align="middle"></details><h2 id="NIVeL-Neural-Implicit-Vector-Layers-for-Text-to-Vector-Generation"><a href="#NIVeL-Neural-Implicit-Vector-Layers-for-Text-to-Vector-Generation" class="headerlink" title="NIVeL: Neural Implicit Vector Layers for Text-to-Vector Generation"></a>NIVeL: Neural Implicit Vector Layers for Text-to-Vector Generation</h2><p><strong>Authors:Vikas Thamizharasan, Difan Liu, Matthew Fisher, Nanxuan Zhao, Evangelos Kalogerakis, Michal Lukac</strong></p><p>The success of denoising diffusion models in representing rich data distributions over 2D raster images has prompted research on extending them to other data representations, such as vector graphics. Unfortunately due to their variable structure and scarcity of vector training data, directly applying diffusion models on this domain remains a challenging problem. Using workarounds like optimization via Score Distillation Sampling (SDS) is also fraught with difficulty, as vector representations are non trivial to directly optimize and tend to result in implausible geometries such as redundant or self-intersecting shapes. NIVeL addresses these challenges by reinterpreting the problem on an alternative, intermediate domain which preserves the desirable properties of vector graphics â€” mainly sparsity of representation and resolution-independence. This alternative domain is based on neural implicit fields expressed in a set of decomposable, editable layers. Based on our experiments, NIVeL produces text-to-vector graphics results of significantly better quality than the state-of-the-art.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.15217v1">PDF</a></p><p><strong>Summary</strong><br>æ‰©å±•å»å™ªæ‰©æ•£æ¨¡å‹åˆ°çŸ¢é‡å›¾å½¢é¢†åŸŸçš„æŒ‘æˆ˜æ€§è§£å†³æ–¹æ¡ˆNIVeLã€‚</p><p><strong>Key Takeaways</strong><br>â€¢ å»å™ªæ‰©æ•£æ¨¡å‹åœ¨2D rasterå›¾åƒä¸Šçš„æˆåŠŸä¿ƒä½¿ç ”ç©¶å°†å…¶æ‰©å±•åˆ°å…¶ä»–æ•°æ®è¡¨ç¤ºå½¢å¼ï¼Œå¦‚çŸ¢é‡å›¾å½¢ã€‚<br>â€¢ ç›´æ¥å°†æ‰©æ•£æ¨¡å‹åº”ç”¨äºçŸ¢é‡å›¾å½¢é¢†åŸŸæ˜¯å…·æœ‰æŒ‘æˆ˜æ€§çš„ï¼Œå› ä¸ºçŸ¢é‡å›¾å½¢å…·æœ‰å¯å˜ç»“æ„å’Œç¨€ç–çš„è®­ç»ƒæ•°æ®ã€‚<br>â€¢ ä½¿ç”¨Score Distillation Samplingï¼ˆSDSï¼‰ç­‰ä¼˜åŒ–æ–¹æ³•ä¹Ÿå­˜åœ¨å›°éš¾ï¼Œå› ä¸ºçŸ¢é‡è¡¨ç¤ºéš¾ä»¥ç›´æ¥ä¼˜åŒ–ï¼Œå®¹æ˜“äº§ç”Ÿä¸å¯ä¿¡çš„å‡ ä½•å½¢çŠ¶ã€‚<br>â€¢ NIVeLé€šè¿‡é‡æ–°è§£é‡Šé—®é¢˜åœ¨ä¸­é—´åŸŸä¸Šï¼Œä¿ç•™çŸ¢é‡å›¾å½¢çš„è‰¯å¥½å±æ€§ï¼Œä¾‹å¦‚ç¨€ç–è¡¨ç¤ºå’Œåˆ†è¾¨ç‡ç‹¬ç«‹æ€§ã€‚<br>â€¢ ä¸­é—´åŸŸåŸºäºå¯åˆ†è§£ã€å¯ç¼–è¾‘çš„ç¥ç»éšå¼å­—æ®µå±‚ã€‚<br>â€¢ å®éªŒç»“æœè¡¨æ˜ï¼ŒNIVeLç”Ÿæˆçš„æ–‡æœ¬åˆ°çŸ¢é‡å›¾å½¢ç»“æœè¿œä¼˜äºå½“å‰æœ€å…ˆè¿›çš„ç»“æœã€‚<br>â€¢ NIVeLè§£å†³äº†æ‰©å±•å»å™ªæ‰©æ•£æ¨¡å‹åˆ°çŸ¢é‡å›¾å½¢é¢†åŸŸçš„æŒ‘æˆ˜æ€§é—®é¢˜ã€‚</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: NIVeL: ç¥ç»éšå¼çŸ¢é‡å›¾å½¢ç”Ÿæˆï¼ˆNeural Implicit Vector Graphics Generationï¼‰</p></li><li><p>Authors: Not provided</p></li><li><p>Affiliation: ä¸æä¾›ï¼ˆNot providedï¼‰</p></li><li><p>Keywords: denoising diffusion models, vector graphics, neural implicit fields</p></li><li><p>Urls: Not provided, Github: None</p></li><li><p>Summary:</p></li><li><p>(1):è¯¥è®ºæ–‡çš„ç ”ç©¶èƒŒæ™¯æ˜¯å°†å»å™ªæ‰©æ•£æ¨¡å‹ä»2D rasterå›¾åƒæ‰©å±•åˆ°çŸ¢é‡å›¾å½¢é¢†åŸŸï¼Œä½†çŸ¢é‡å›¾å½¢çš„å¯å˜ç»“æ„å’Œç¨€ç¼ºçš„è®­ç»ƒæ•°æ®ä½¿å¾—ç›´æ¥åº”ç”¨å»å™ªæ‰©æ•£æ¨¡å‹å˜å¾—å›°éš¾ã€‚</p></li><li><p>(2):è¿‡å»çš„æ–¹æ³•åŒ…æ‹¬ç›´æ¥åº”ç”¨å»å™ªæ‰©æ•£æ¨¡å‹å’ŒScore Distillation Samplingï¼ˆSDSï¼‰ä¼˜åŒ–ï¼Œä½†è¿™äº›æ–¹æ³•å­˜åœ¨ä¸€äº›é—®é¢˜ï¼Œå¦‚ç”Ÿæˆçš„çŸ¢é‡å›¾å½¢å¯èƒ½åŒ…å«å†—ä½™æˆ–è‡ªç›¸äº¤çš„å½¢çŠ¶ã€‚</p></li><li><p>(3):æœ¬è®ºæ–‡æå‡ºäº†NIVeLæ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡å°†é—®é¢˜é‡æ–°è§£é‡Šåœ¨ä¸­é—´åŸŸä¸Šï¼Œå³åŸºäºç¥ç»éšå¼å­—æ®µçš„å¯åˆ†è§£ã€å¯ç¼–è¾‘çš„å±‚æ¥ç”ŸæˆçŸ¢é‡å›¾å½¢ã€‚</p></li><li><p>(4):æœ¬è®ºæ–‡çš„æ–¹æ³•åœ¨æ–‡æœ¬åˆ°çŸ¢é‡å›¾å½¢ä»»åŠ¡ä¸Šå–å¾—äº†æ˜æ˜¾ä¼˜äºç°æœ‰æ–¹æ³•çš„æ€§èƒ½ï¼Œè¯æ˜äº†NIVeLæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p></li><li><p>æ–¹æ³•ï¼š</p></li><li><p>(1):å°†çŸ¢é‡å›¾å½¢ç”Ÿæˆé—®é¢˜é‡æ–°è§£é‡Šåœ¨ä¸­é—´åŸŸä¸Šï¼Œå³åŸºäºç¥ç»éšå¼å­—æ®µï¼ˆNeural Implicit Fieldsï¼‰çš„å¯åˆ†è§£ã€å¯ç¼–è¾‘çš„å±‚ï¼Œä»¥ä¾¿æ›´å¥½åœ°å¤„ç†çŸ¢é‡å›¾å½¢çš„å¯å˜ç»“æ„å’Œç¨€ç¼ºçš„è®­ç»ƒæ•°æ®ã€‚</p></li><li><p>(2):ä½¿ç”¨å»å™ªæ‰©æ•£æ¨¡å‹ï¼ˆDenoising Diffusion Modelsï¼‰åœ¨ä¸­é—´åŸŸä¸Šç”Ÿæˆéšå¼è¡¨ç¤ºï¼Œç„¶åé€šè¿‡ç¥ç»éšå¼å­—æ®µå°†å…¶è½¬æ¢ä¸ºçŸ¢é‡å›¾å½¢ã€‚</p></li><li><p>(3):å¼•å…¥ Score Distillation Samplingï¼ˆSDSï¼‰ä¼˜åŒ–æ–¹æ³•ï¼Œä»¥æé«˜ç”ŸæˆçŸ¢é‡å›¾å½¢çš„è´¨é‡å’Œå¤šæ ·æ€§ã€‚</p></li><li><p>(4):åœ¨ä¸­é—´åŸŸä¸Šåº”ç”¨ç¼–è¾‘æ“ä½œï¼Œå¦‚å½¢çŠ¶å˜æ¢ã€æ‹“æ‰‘å˜åŒ–ç­‰ï¼Œä»¥å¢å¼ºç”ŸæˆçŸ¢é‡å›¾å½¢çš„å¯ç¼–è¾‘æ€§å’Œçµæ´»æ€§ã€‚</p></li><li><p>(5):ä½¿ç”¨æ–‡æœ¬åˆ°çŸ¢é‡å›¾å½¢ä»»åŠ¡çš„å®éªŒç»“æœéªŒè¯NIVeLæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜å…¶åœ¨ç”Ÿæˆé«˜è´¨é‡çŸ¢é‡å›¾å½¢æ–¹é¢çš„ä¼˜åŠ¿ã€‚</p></li><li><p>ç»“è®ºï¼š</p></li><li><p>(1):è¯¥ç¯‡å·¥ä½œçš„é‡è¦æ€§åœ¨äºå°†å»å™ªæ‰©æ•£æ¨¡å‹åº”ç”¨äºçŸ¢é‡å›¾å½¢ç”Ÿæˆé¢†åŸŸï¼Œè§£å†³äº†çŸ¢é‡å›¾å½¢çš„å¯å˜ç»“æ„å’Œç¨€ç¼ºçš„è®­ç»ƒæ•°æ®é—®é¢˜ï¼Œæé«˜äº†ç”ŸæˆçŸ¢é‡å›¾å½¢çš„è´¨é‡å’Œå¤šæ ·æ€§ã€‚</p></li><li><p>(2):åˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäºç¥ç»éšå¼å­—æ®µçš„çŸ¢é‡å›¾å½¢ç”Ÿæˆæ–¹æ³•ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†çŸ¢é‡å›¾å½¢çš„å¯å˜ç»“æ„å’Œç¨€ç¼ºçš„è®­ç»ƒæ•°æ®ï¼›æ€§èƒ½ï¼šåœ¨æ–‡æœ¬åˆ°çŸ¢é‡å›¾å½¢ä»»åŠ¡ä¸Šå–å¾—äº†æ˜æ˜¾ä¼˜äºç°æœ‰æ–¹æ³•çš„æ€§èƒ½ï¼›å·¥ä½œé‡ï¼šéœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®å’Œè®¡ç®—èµ„æºï¼Œä¸”å½“å‰çš„è¡¨ç¤ºæ–¹å¼è¿˜å­˜åœ¨ä¸€äº›é™åˆ¶ï¼Œå¦‚å±‚çš„æ•°é‡é™åˆ¶ç­‰ã€‚</p></li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-deb0bce750c823b45864a06b1f2fdf37.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-b05c16791ff3624415d2ca5a4bb2b01d.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-1ddb20e736aa45d7da426d42c0386fcb.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-a127e1927a9826d4a5a6449d4ce7f25e.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-6ef7a2dd3802c3e38639f59aa13e5305.jpg" align="middle"></details><h2 id="TerDiT-Ternary-Diffusion-Models-with-Transformers"><a href="#TerDiT-Ternary-Diffusion-Models-with-Transformers" class="headerlink" title="TerDiT: Ternary Diffusion Models with Transformers"></a>TerDiT: Ternary Diffusion Models with Transformers</h2><p><strong>Authors:Xudong Lu, Aojun Zhou, Ziyi Lin, Qi Liu, Yuhui Xu, Renrui Zhang, Yafei Wen, Shuai Ren, Peng Gao, Junchi Yan, Hongsheng Li</strong></p><p>Recent developments in large-scale pre-trained text-to-image diffusion models have significantly improved the generation of high-fidelity images, particularly with the emergence of diffusion models based on transformer architecture (DiTs). Among these diffusion models, diffusion transformers have demonstrated superior image generation capabilities, boosting lower FID scores and higher scalability. However, deploying large-scale DiT models can be expensive due to their extensive parameter numbers. Although existing research has explored efficient deployment techniques for diffusion models such as model quantization, there is still little work concerning DiT-based models. To tackle this research gap, in this paper, we propose TerDiT, a quantization-aware training (QAT) and efficient deployment scheme for ternary diffusion models with transformers. We focus on the ternarization of DiT networks and scale model sizes from 600M to 4.2B. Our work contributes to the exploration of efficient deployment strategies for large-scale DiT models, demonstrating the feasibility of training extremely low-bit diffusion transformer models from scratch while maintaining competitive image generation capacities compared to full-precision models. Code will be available at <a target="_blank" rel="noopener" href="https://github.com/Lucky-Lance/TerDiT">https://github.com/Lucky-Lance/TerDiT</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.14854v1">PDF</a> 18 pages, 13 figures</p><p><strong>Summary</strong><br>å¤§è§„æ¨¡é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æœ€æ–°å‘å±•ï¼Œæå‡ºäº†ä¸€ç§é‡åŒ–æ„ŸçŸ¥è®­ç»ƒå’Œé«˜æ•ˆéƒ¨ç½²æ–¹æ¡ˆTerDiTï¼Œç”¨äºä¸‰çº§æ‰©æ•£æ¨¡å‹çš„ transformersã€‚</p><p><strong>Key Takeaways</strong><br>â€¢ å¤§è§„æ¨¡é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æœ€æ–°å‘å±•ï¼Œç‰¹åˆ«æ˜¯åŸºäº transformer æ¶æ„çš„æ‰©æ•£æ¨¡å‹ï¼ˆDiTsï¼‰ï¼Œç”Ÿæˆé«˜ä¿çœŸå›¾åƒçš„èƒ½åŠ›å¾—åˆ°äº†æ˜¾è‘—æ”¹å–„ã€‚<br>â€¢ æ‰©æ•£å˜å‹å™¨æ¨¡å‹å±•ç¤ºå‡ºä¼˜è¶Šçš„å›¾åƒç”Ÿæˆèƒ½åŠ›ï¼Œå…·æœ‰è¾ƒä½çš„ FID åˆ†æ•°å’Œæ›´é«˜çš„å¯æ‰©å±•æ€§ã€‚<br>â€¢ éƒ¨ç½²å¤§è§„æ¨¡ DiT æ¨¡å‹å¯èƒ½å¾ˆæ˜‚è´µï¼Œå› ä¸ºå®ƒä»¬å…·æœ‰åºå¤§çš„å‚æ•°æ•°é‡ã€‚<br>â€¢ ç°æœ‰çš„ç ”ç©¶å·²ç»æ¢ç´¢äº†æ‰©æ•£æ¨¡å‹çš„é«˜æ•ˆéƒ¨ç½²æŠ€æœ¯ï¼Œå¦‚æ¨¡å‹é‡åŒ–ï¼Œä½†å¯¹äº DiT åŸºç¡€æ¨¡å‹çš„ç ”ç©¶ä»ç„¶å¾ˆå°‘ã€‚<br>â€¢ æœ¬æ–‡æå‡ºäº† TerDiTï¼Œä¸€ç§é‡åŒ–æ„ŸçŸ¥è®­ç»ƒå’Œé«˜æ•ˆéƒ¨ç½²æ–¹æ¡ˆï¼Œç”¨äºä¸‰çº§æ‰©æ•£æ¨¡å‹çš„ transformersã€‚<br>â€¢ è¯¥æ–¹æ¡ˆå…³æ³¨ DiT ç½‘ç»œçš„ä¸‰çº§åŒ–ï¼Œå¹¶å°†æ¨¡å‹å¤§å°ä» 600M æ‰©å±•åˆ° 4.2Bã€‚<br>â€¢ æœ¬å·¥ä½œä¸ºå¤§è§„æ¨¡ DiT æ¨¡å‹çš„é«˜æ•ˆéƒ¨ç½²ç­–ç•¥åšå‡ºäº†è´¡çŒ®ï¼Œè¯æ˜äº†ä»å¤´è®­ç»ƒæä½ä½æ‰©æ•£å˜å‹å™¨æ¨¡å‹çš„å¯è¡Œæ€§ï¼ŒåŒæ—¶ä¿æŒäº†ä¸å…¨ç²¾åº¦æ¨¡å‹ç›¸ä¼¼çš„å›¾åƒç”Ÿæˆèƒ½åŠ›ã€‚</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><h2>Paper:1</h2><p></p><ol><li><p>Title: TerDiTï¼šå…·æœ‰å˜å‹å™¨çš„ä¸‰è¿›åˆ¶æ‰©æ•£æ¨¡å‹ (TerDiT: Ternary Diffusion Models with Transformers)</p></li><li><p>Authors: Xudong Lu, Aojun Zhou, Ziyi Lin, Qi Liu, Yuhui Xu, Renrui Zhang, Yafei Wen, Shuai Ren, Peng Gao, Junchi Yan, Hongsheng Li</p></li><li><p>Affiliation: é¦™æ¸¯ä¸­æ–‡å¤§å­¦å¤šåª’ä½“å®éªŒå®¤</p></li><li><p>Keywords: diffusion models, transformer architecture, quantization-aware training, efficient deployment</p></li><li><p>Urls: https://arxiv.org/abs/2405.14854, Github: https://github.com/Lucky-Lance/TerDiT</p></li><li><p>Summary:</p><ul><li><p>(1):æœ€è¿‘ï¼Œå¤§è§„æ¨¡é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„å‘å±•æå¤§åœ°æ”¹å–„äº†é«˜ä¿çœŸå›¾åƒçš„ç”Ÿæˆï¼Œç‰¹åˆ«æ˜¯åŸºäºå˜å‹å™¨æ¶æ„ï¼ˆDiTsï¼‰çš„æ‰©æ•£æ¨¡å‹ã€‚</p></li><li><p>(2):ç°æœ‰çš„ç ”ç©¶å·²ç»æ¢ç´¢äº†æ‰©æ•£æ¨¡å‹çš„é«˜æ•ˆéƒ¨ç½²æŠ€æœ¯ï¼Œå¦‚æ¨¡å‹é‡åŒ–ï¼Œä½†æ˜¯åœ¨DiTæ¨¡å‹æ–¹é¢ä»ç„¶å­˜åœ¨ç ”ç©¶gapã€‚</p></li><li><p>(3):æœ¬æ–‡æå‡ºTerDiTï¼Œä¸€ä¸ªé‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰å’Œé«˜æ•ˆéƒ¨ç½²æ–¹æ¡ˆï¼Œç”¨äºå…·æœ‰å˜å‹å™¨çš„ä¸‰è¿›åˆ¶æ‰©æ•£æ¨¡å‹ã€‚</p></li><li><p>(4):æœ¬æ–‡çš„æ–¹æ³•å¯ä»¥è®­ç»ƒæä½æ¯”ç‰¹æ‰©æ•£å˜å‹å™¨æ¨¡å‹ï¼Œä»è€Œå®ç°ä¸å…¨ç²¾åº¦æ¨¡å‹ç›¸åª²ç¾çš„å›¾åƒç”Ÿæˆèƒ½åŠ›ï¼ŒåŒæ—¶ä¹Ÿå®ç°äº†é«˜æ•ˆçš„æ¨¡å‹éƒ¨ç½²ã€‚</p></li><li>æ–¹æ³•ï¼š</li></ul></li><li><p>(1)ï¼šé‡‡ç”¨å‡é‡å‡½æ•°ï¼ˆfake quant functionï¼‰å¯¹æ¨¡å‹æƒé‡è¿›è¡Œé‡åŒ–ï¼Œè®¾ç½®n_bits=4ï¼Œä¸è¿›è¡Œæ¿€æ´»é‡åŒ–ã€‚</p></li><li><p>(2)ï¼šå¯¹åŸDiTå—ä¸­çš„æ‰€æœ‰çº¿æ€§å±‚æƒé‡è¿›è¡Œé‡åŒ–ï¼ŒåŒ…æ‹¬è‡ªæ³¨æ„ã€å‰é¦ˆå’ŒMLPã€‚</p></li><li><p>(3)ï¼šä½¿ç”¨é‡åŒ–åçš„æ¨¡å‹é‡‡æ ·å›¾åƒï¼Œå¹¶ä¸å…¨ç²¾åº¦æ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚</p></li><li><p>(4)ï¼šæå‡ºTerDiTï¼Œä¸€ä¸ªé‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰å’Œé«˜æ•ˆéƒ¨ç½²æ–¹æ¡ˆï¼Œç”¨äºå…·æœ‰å˜å‹å™¨çš„ä¸‰è¿›åˆ¶æ‰©æ•£æ¨¡å‹ã€‚</p></li><li><p>(5)ï¼šé‡‡ç”¨å­¦ä¹ ç‡å‡å°ç­–ç•¥ï¼Œä»¥æé«˜æ¨¡å‹çš„è®­ç»ƒç»“æœã€‚</p></li><li><p>(6)ï¼šä½¿ç”¨RMS Normalized adaLNæ¨¡å—ï¼Œä»¥æé«˜æ¨¡å‹çš„ç”Ÿæˆè´¨é‡ã€‚</p></li><li><p>(7)ï¼šè¿›è¡Œå®éªŒæ¯”è¾ƒï¼ŒéªŒè¯TerDiTæ¨¡å‹åœ¨é«˜æ•ˆéƒ¨ç½²å’Œå›¾åƒç”Ÿæˆèƒ½åŠ›æ–¹é¢çš„ä¼˜åŠ¿ã€‚</p></li><li><p>ç»“è®ºï¼š</p></li><li><p>(1):è¯¥å·¥ä½œçš„é‡è¦æ€§åœ¨äºå®ƒæ¨åŠ¨äº†å…·æœ‰å˜å‹å™¨æ¶æ„çš„æ‰©æ•£æ¨¡å‹çš„é«˜æ•ˆéƒ¨ç½²ï¼Œæ»¡è¶³äº†å®é™…åº”ç”¨ä¸­çš„ä½å»¶è¿Ÿå’Œä½è®¡ç®—èµ„æºéœ€æ±‚ã€‚</p></li><li><p>(2):åˆ›æ–°ç‚¹ï¼šTerDiT æ¨¡å‹æå‡ºäº†ä¸€ç§é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰å’Œé«˜æ•ˆéƒ¨ç½²æ–¹æ¡ˆï¼Œè§£å†³äº†ç°æœ‰DiT æ¨¡å‹åœ¨é«˜æ•ˆéƒ¨ç½²æ–¹é¢çš„ç ”ç©¶gapï¼›æ€§èƒ½ï¼šTerDiT æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆèƒ½åŠ›æ–¹é¢ä¸å…¨ç²¾åº¦æ¨¡å‹ç›¸åª²ç¾ï¼ŒåŒæ—¶å®ç°äº†é«˜æ•ˆçš„æ¨¡å‹éƒ¨ç½²ï¼›å·¥ä½œé‡ï¼šè¯¥å·¥ä½œéœ€è¦å¤§é‡çš„å®éªŒè®¾è®¡å’Œæ¨¡å‹è®­ç»ƒï¼Œä¸”éœ€è¦æ·±å…¥äº†è§£DiT æ¨¡å‹å’Œé‡åŒ–æŠ€æœ¯ã€‚</p></li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-c40afa8caaa8fb0e34704a216ee65f09.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-21147ce65723c9373a1e3d28f5c516df.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-b32f6ca859af81585bc0599f40dc4518.jpg" align="middle"></details></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>æ–‡ç« ä½œè€…:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>æ–‡ç« é“¾æ¥:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io/2024/05/28/Paper/2024-05-28/Diffusion%20Models/">https://kedreamix.github.io/2024/05/28/Paper/2024-05-28/Diffusion Models/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>ç‰ˆæƒå£°æ˜:</span> <span class="post-copyright-info">æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ«å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨ <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥è‡ª <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>ï¼</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Diffusion-Models/">Diffusion Models</a></div><div class="post_share"><div class="social-share" data-image="https://pic1.zhimg.com/v2-36a0effe69414b2ffa084f4cd6a69d06.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>èµåŠ©</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/05/28/Paper/2024-05-28/Talking%20Head%20Generation/" title="Talking Head Generation"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-33e1c85bbd2586fc6e8eb024aa73c567.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">ä¸Šä¸€ç¯‡</div><div class="prev_info">Talking Head Generation</div></div></a></div><div class="next-post pull-right"><a href="/2024/05/28/Paper/2024-05-28/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="å…ƒå®‡å®™/è™šæ‹Ÿäºº"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-dc27e0e81b6be96603dd90e8aa23e081.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">ä¸‹ä¸€ç¯‡</div><div class="next_info">å…ƒå®‡å®™/è™šæ‹Ÿäºº</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>ç›¸å…³æ¨è</span></div><div class="relatedPosts-list"><div><a href="/2024/03/03/Paperscape/EMO/" title="EMO Emote Portrait Alive - é˜¿é‡ŒHumanAIGC"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-6492e24fb03ffa98135dc584535ab7d9.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-03</div><div class="title">EMO Emote Portrait Alive - é˜¿é‡ŒHumanAIGC</div></div></a></div><div><a href="/2024/01/30/Paper/2024-01-30/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-e55358c77a9d65f15701e8f33262e2a4.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-30</div><div class="title">Diffusion Models</div></div></a></div><div><a href="/2024/01/24/Paper/2024-01-24/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-71a37c439c6714e8867560f580599d2f.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-24</div><div class="title">Diffusion Models</div></div></a></div><div><a href="/2024/02/13/Paper/2024-02-13/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-3709a9941aada6c4d3ed35934e311765.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-13</div><div class="title">Diffusion Models</div></div></a></div><div><a href="/2024/02/02/Paper/2024-02-02/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-5920453c69c00995f18077b22d4a790e.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-02</div><div class="title">Diffusion Models</div></div></a></div><div><a href="/2024/02/23/Paper/2024-02-23/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-ff425802a32a4519e30b9044a3eed1e8.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-23</div><div class="title">Diffusion Models</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>ç›®å½•</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-05-28-%E6%9B%B4%E6%96%B0"><span class="toc-text">2024-05-28 æ›´æ–°</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#DiffCalib-Reformulating-Monocular-Camera-Calibration-as-Diffusion-Based-Dense-Incident-Map-Generation"><span class="toc-text">DiffCalib: Reformulating Monocular Camera Calibration as Diffusion-Based Dense Incident Map Generation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Defensive-Unlearning-with-Adversarial-Training-for-Robust-Concept-Erasure-in-Diffusion-Models"><span class="toc-text">Defensive Unlearning with Adversarial Training for Robust Concept Erasure in Diffusion Models</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DEEM-Diffusion-Models-Serve-as-the-Eyes-of-Large-Language-Models-for-Image-Perception"><span class="toc-text">DEEM: Diffusion Models Serve as the Eyes of Large Language Models for Image Perception</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-text">Paper:1</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#NIVeL-Neural-Implicit-Vector-Layers-for-Text-to-Vector-Generation"><span class="toc-text">NIVeL: Neural Implicit Vector Layers for Text-to-Vector Generation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#TerDiT-Ternary-Diffusion-Models-with-Transformers"><span class="toc-text">TerDiT: Ternary Diffusion Models with Transformers</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-text">Paper:1</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image:url('https://pic1.zhimg.com/v2-36a0effe69414b2ffa084f4cd6a69d06.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>æ¡†æ¶</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>ä¸»é¢˜</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="é˜…è¯»æ¨¡å¼"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="ç®€ç¹è½¬æ¢">ç°¡</button><button id="darkmode" type="button" title="æµ…è‰²å’Œæ·±è‰²æ¨¡å¼è½¬æ¢"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="å•æ å’ŒåŒæ åˆ‡æ¢"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="è®¾ç½®"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="ç›®å½•"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="å›åˆ°é¡¶éƒ¨"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js").then((()=>{pangu.autoSpacingPage()}))}function panguInit(){panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.1},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script></div><script id="canvas_nest" defer color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">æœç´¢</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>æ•°æ®åº“åŠ è½½ä¸­</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="æœç´¢æ–‡ç« " type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><script data-pjax>function butterfly_swiper_injector_config(){var a=document.getElementById("recent-posts");console.log("å·²æŒ‚è½½butterfly_swiper"),a.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/25/Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="2024/01/25/Paper/3DGS Survey/" alt="">3DGSç»¼è¿°ä»¥åŠå¯¹3DGSçš„ç†è§£ï¼šA Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGSç»¼è¿°ä»¥åŠå¯¹3DGSçš„ç†è§£ï¼šA Survey on 3D Gaussian Splatting ä»Šå¤©æƒ³ä»‹ç»çš„æ˜¯`ZJU`å¸¦æ¥çš„`3DGS`çš„é¦–ç¯‡ç»¼è¿°`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="2024/01/25/Paper/3DGS Survey/" alt="">è¯¦æƒ…   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/12/CUDA/Pytorch+cppcuda extension å­¦ä¹ /" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="2023/12/12/CUDA/Pytorch+cppcuda extension å­¦ä¹ /" alt="">CUDAç¼–ç¨‹å­¦ä¹ ï¼šè‡ªå®šä¹‰Pytorch+cpp/cuda extension</a><div class="blog-slider__text">è™½ç„¶è¯´PyTorchæä¾›äº†ä¸°å¯Œçš„ä¸ç¥ç»ç½‘ç»œã€å¼ é‡ä»£æ•°ã€æ•°æ®å¤„ç†ç­‰ç›¸å…³çš„æ“ä½œï¼Œä½†æ˜¯æœ‰æ—¶å€™ä½ å¯èƒ½éœ€è¦**æ›´å®šåˆ¶åŒ–çš„æ“ä½œ**ï¼Œæ¯”å¦‚ä½¿ç”¨è®ºæ–‡ä¸­çš„æ–°å‹æ¿€æ´»å‡½æ•°ï¼Œæˆ–è€…å®ç°ä½œä¸ºç ”ç©¶ä¸€éƒ¨åˆ†å¼€å‘çš„æ“ä½œã€‚åœ¨PyTorchä¸­ï¼Œæœ€ç®€å•çš„é›†æˆè‡ªå®šä¹‰æ“ä½œçš„æ–¹å¼æ˜¯åœ¨Pythonä¸­ç¼–å†™ï¼Œé€šè¿‡æ‰©å±•Functionå’ŒModuleæ¥å®ç°ï¼Œ</div><a class="blog-slider__button" href="2023/12/12/CUDA/Pytorch+cppcuda extension å­¦ä¹ /" alt="">è¯¦æƒ…   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">æ•°å­—äººå¯¹è¯ç³»ç»Ÿ - Linly-Talker â€”â€” â€œæ•°å­—äººäº¤äº’ï¼Œä¸è™šæ‹Ÿçš„è‡ªå·±äº’åŠ¨â€</a><div class="blog-slider__text">Linly-Talkeræ˜¯ä¸€ä¸ªå°†å¤§å‹è¯­è¨€æ¨¡å‹ä¸è§†è§‰æ¨¡å‹ç›¸ç»“åˆçš„æ™ºèƒ½AIç³»ç»Ÿ,åˆ›å»ºäº†ä¸€ç§å…¨æ–°çš„äººæœºäº¤äº’æ–¹å¼ã€‚å®ƒé›†æˆäº†å„ç§æŠ€æœ¯,ä¾‹å¦‚Whisperã€Linlyã€å¾®è½¯è¯­éŸ³æœåŠ¡å’ŒSadTalkerä¼šè¯´è¯çš„ç”Ÿæˆç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿéƒ¨ç½²åœ¨Gradioä¸Š,å…è®¸ç”¨æˆ·é€šè¿‡æä¾›å›¾åƒä¸AIåŠ©æ‰‹è¿›è¡Œäº¤è°ˆã€‚ç”¨æˆ·å¯ä»¥æ ¹æ®è‡ªå·±çš„å–œå¥½è¿›è¡Œè‡ªç”±çš„å¯¹è¯æˆ–å†…å®¹ç”Ÿæˆã€‚</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">è¯¦æƒ…   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker/" alt="">æ•°å­—äººå¯¹è¯ç³»ç»Ÿ - Linly-Talker â€”â€” â€œæ•°å­—äººäº¤äº’ï¼Œä¸è™šæ‹Ÿçš„è‡ªå·±äº’åŠ¨â€</a><div class="blog-slider__text">Linly-Talkeræ˜¯ä¸€ä¸ªå°†å¤§å‹è¯­è¨€æ¨¡å‹ä¸è§†è§‰æ¨¡å‹ç›¸ç»“åˆçš„æ™ºèƒ½AIç³»ç»Ÿ,åˆ›å»ºäº†ä¸€ç§å…¨æ–°çš„äººæœºäº¤äº’æ–¹å¼ã€‚å®ƒé›†æˆäº†å„ç§æŠ€æœ¯,ä¾‹å¦‚Whisperã€Linlyã€å¾®è½¯è¯­éŸ³æœåŠ¡å’ŒSadTalkerä¼šè¯´è¯çš„ç”Ÿæˆç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿéƒ¨ç½²åœ¨Gradioä¸Š,å…è®¸ç”¨æˆ·é€šè¿‡æä¾›å›¾åƒä¸AIåŠ©æ‰‹è¿›è¡Œäº¤è°ˆã€‚ç”¨æˆ·å¯ä»¥æ ¹æ®è‡ªå·±çš„å–œå¥½è¿›è¡Œè‡ªç”±çš„å¯¹è¯æˆ–å†…å®¹ç”Ÿæˆã€‚</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker/" alt="">è¯¦æƒ…   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">è¶…èµçš„æ•°å­—äººç”ŸæˆçŸ¥è¯†åº“  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">è¶…èµçš„æ•°å­—äººç”ŸæˆçŸ¥è¯†åº“  Awesome-Talking-Head-Synthesisï¼Œ è¿™ä»½èµ„æºåº“æ•´ç†äº†ä¸ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ(GAN)å’Œç¥ç»è¾å°„åœº(NeRF)ç›¸å…³çš„è®ºæ–‡ã€ä»£ç å’Œèµ„æº,é‡ç‚¹å…³æ³¨åŸºäºå›¾åƒå’ŒéŸ³é¢‘çš„è™šæ‹Ÿè®²è¯å¤´åˆæˆè®ºæ–‡åŠå·²å‘å¸ƒä»£ç ã€‚å¦‚æœæ‚¨è§‰å¾—è¿™ä¸ªä»“åº“æœ‰ç”¨,è¯·starâ­æ”¯æŒ!</div><a class="blog-slider__button" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">è¯¦æƒ…   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/17/Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023/12/17/Project/ChatPaperFree/" alt="">ChatPaperFree GeminiProï¼ˆä¸€åˆ†é’Ÿè¯»è®ºæ–‡ï¼‰</a><div class="blog-slider__text">ChatPaperFreeæ˜¯ä¸€ä¸ªåŸºäºChatGPTçš„è‡ªåŠ¨è®ºæ–‡æ‘˜è¦ç”Ÿæˆå™¨ï¼Œåœ¨ChatPaperçš„åŸºç¡€ä¸Šè¿›è¡Œçš„æ›´æ–°ï¼Œé‡‡ç”¨äº†æœ€è¿‘ç”±Googleå¼€æºçš„Gemini Proå¤§æ¨¡å‹ã€‚ç›®å‰,æˆ‘ä»¬èƒ½å¤Ÿå¯¹ç”¨æˆ·è¾“å…¥çš„è®ºæ–‡è¿›è¡Œè‡ªåŠ¨æ€»ç»“ã€‚æœªæ¥,æˆ‘è¿˜è®¡åˆ’åŠ å…¥å¯¹è®ºæ–‡å›¾ç‰‡/è¡¨æ ¼/å…¬å¼çš„è¯†åˆ« extraction,ä»è€Œç”Ÿæˆæ›´å…¨é¢è€Œæ˜“è¯»çš„æ€»ç»“ã€‚</div><a class="blog-slider__button" href="2023/12/17/Project/ChatPaperFree/" alt="">è¯¦æƒ…   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script></body></html>