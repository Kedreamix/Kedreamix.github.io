<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>3DGS | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="3DGS 方向最新论文已更新，请持续关注 Update in 2024-05-28  Feature Splatting for Better Novel View Synthesis with Low Overlap"><meta property="og:type" content="article"><meta property="og:title" content="3DGS"><meta property="og:url" content="https://kedreamix.github.io/2024/05/28/Paper/2024-05-28/3DGS/index.html"><meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World"><meta property="og:description" content="3DGS 方向最新论文已更新，请持续关注 Update in 2024-05-28  Feature Splatting for Better Novel View Synthesis with Low Overlap"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://pic1.zhimg.com/v2-8db132ec3c58c945a06898a8758b7480.jpg"><meta property="article:published_time" content="2024-05-27T17:55:43.000Z"><meta property="article:modified_time" content="2024-05-28T08:35:16.157Z"><meta property="article:author" content="Kedreamix"><meta property="article:tag" content="3DGS"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://pic1.zhimg.com/v2-8db132ec3c58c945a06898a8758b7480.jpg"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/2024/05/28/Paper/2024-05-28/3DGS/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-74LZ5BEQQ1")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!0,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"简"},noticeOutdate:{limitDay:500,position:"top",messagePrev:"It has been",messageNext:"days since the last update, the content of the article may be outdated."},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:200},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!0,post:!0},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!0,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"3DGS",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-05-28 16:35:16"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach((e=>{n.setAttribute(e,t[e])})),document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)})),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=24?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 7.0.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml"></head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{const d=document.getElementById("loading-box"),e=document.body,o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l(),window.addEventListener("load",(()=>{o()}))})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">269</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url('https://pic1.zhimg.com/v2-8db132ec3c58c945a06898a8758b7480.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">3DGS</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-05-27T17:55:43.000Z" title="发表于 2024-05-28 01:55:43">2024-05-28</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-05-28T08:35:16.157Z" title="更新于 2024-05-28 16:35:16">2024-05-28</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>27分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="3DGS"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-05-28-更新"><a href="#2024-05-28-更新" class="headerlink" title="2024-05-28 更新"></a>2024-05-28 更新</h1><h2 id="Feature-Splatting-for-Better-Novel-View-Synthesis-with-Low-Overlap"><a href="#Feature-Splatting-for-Better-Novel-View-Synthesis-with-Low-Overlap" class="headerlink" title="Feature Splatting for Better Novel View Synthesis with Low Overlap"></a>Feature Splatting for Better Novel View Synthesis with Low Overlap</h2><p><strong>Authors:T. Berriel Martins, Javier Civera</strong></p><p>3D Gaussian Splatting has emerged as a very promising scene representation, achieving state-of-the-art quality in novel view synthesis significantly faster than competing alternatives. However, its use of spherical harmonics to represent scene colors limits the expressivity of 3D Gaussians and, as a consequence, the capability of the representation to generalize as we move away from the training views. In this paper, we propose to encode the color information of 3D Gaussians into per-Gaussian feature vectors, which we denote as Feature Splatting (FeatSplat). To synthesize a novel view, Gaussians are first “splatted” into the image plane, then the corresponding feature vectors are alpha-blended, and finally the blended vector is decoded by a small MLP to render the RGB pixel values. To further inform the model, we concatenate a camera embedding to the blended feature vector, to condition the decoding also on the viewpoint information. Our experiments show that these novel model for encoding the radiance considerably improves novel view synthesis for low overlap views that are distant from the training views. Finally, we also show the capacity and convenience of our feature vector representation, demonstrating its capability not only to generate RGB values for novel views, but also their per-pixel semantic labels. We will release the code upon acceptance. Keywords: Gaussian Splatting, Novel View Synthesis, Feature Splatting</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.15518v1">PDF</a></p><p><strong>Summary</strong><br>使用特征splattering（FeatSplat）将3D高斯体的颜色信息编码到每个高斯体的特征向量中，提高了新视图合成的质量和泛化能力。</p><p><strong>Key Takeaways</strong><br>• 3D高斯splattering在新视图合成中取得了state-of-the-art的质量，但其使用球谐函数表达场景颜色限制了3D高斯体的表达能力。<br>• 本文提出将颜色信息编码到每个高斯体的特征向量中，以提高表达能力和泛化能力。<br>• 特征splattering（FeatSplat）模型包括高斯体的splattering、alpha-blending和解码三个步骤。<br>• 模型中还加入了相机embedding，以条件解码也基于视点信息。<br>• 实验结果表明，FeatSplat模型显著提高了低重叠视图的新视图合成质量。<br>• FeatSplat模型不仅可以生成RGB值，还可以生成每像素的语义标签。<br>• 将发布代码。</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: 特征Splattering用于低重叠视图的新视图合成 (Feature Splatting for Better Novel View Synthesis with Low Overlap)</p></li><li><p>Authors: Tomas Berriel Martins, Javier Civera</p></li><li><p>Affiliation: 扎拉戈萨大学(I3A)</p></li><li><p>Keywords: Gaussian Splatting, Novel View Synthesis, Feature Splatting</p></li><li><p>Urls: arXiv:2405.15518v1, Github:None</p></li><li><p>Summary:</p></li></ol><pre><code>- (1):该论文的研究背景是寻找适合三维场景表示，以便在机器人、虚拟现实和增强现实应用中使用。


- (2):过去的方法包括Neural Radiance Fields（NeRFs）和三维高斯Splattering（3DGS），但它们存在一些缺陷，例如NeRFs计算开销高、3DGS使用球谐函数表示场景颜色限制了其表达能力。


- (3):本文提出了一种新的方法，称为特征Splattering（FeatSplat），它将三维高斯的颜色信息编为每个高斯的特征向量，然后将这些征向量混合并解码以生成RGB像素值。


- (4):实验结果表明，FeatSplat方法可以显著改善低重叠视图的新视图合成性能，并且可以生成每像素的语义标签，以支持机器人等应用。
</code></pre><ol><li>Conclusion:</li></ol><ul><li><p>(1):本文的工作对于三维场景表示和新视图合成具有重要意义，可以应用于机器人、虚拟现实和增强现实等领域。</p></li><li><p>(2):Innovation point: 本文提出了一种新的特征Splattering（FeatSplat）方法，弥补了Neural Radiance Fields（NeRFs）和三维高斯Splattering（3DGS）的不足之处； Performance: FeatSplat方法可以生成高质量的新视图，并且可以生成每像素的语义标签； Workload: 本文的方法计算开销相对较低，适合实时应用。</p></li></ul><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-af9ac9b1d0d353f31971a8ace9ae132b.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-eaee1c783ee42cdf998fdd81f98539e2.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-922abaae68f73855cac3e6cd2f6fb3d0.jpg" align="middle"></details><h2 id="HDR-GS-Efficient-High-Dynamic-Range-Novel-View-Synthesis-at-1000x-Speed-via-Gaussian-Splatting"><a href="#HDR-GS-Efficient-High-Dynamic-Range-Novel-View-Synthesis-at-1000x-Speed-via-Gaussian-Splatting" class="headerlink" title="HDR-GS: Efficient High Dynamic Range Novel View Synthesis at 1000x Speed   via Gaussian Splatting"></a>HDR-GS: Efficient High Dynamic Range Novel View Synthesis at 1000x Speed via Gaussian Splatting</h2><p><strong>Authors:Yuanhao Cai, Zihao Xiao, Yixun Liang, Yulun Zhang, Xiaokang Yang, Yaoyao Liu, Alan Yuille</strong></p><p>High dynamic range (HDR) novel view synthesis (NVS) aims to create photorealistic images from novel viewpoints using HDR imaging techniques. The rendered HDR images capture a wider range of brightness levels containing more details of the scene than normal low dynamic range (LDR) images. Existing HDR NVS methods are mainly based on NeRF. They suffer from long training time and slow inference speed. In this paper, we propose a new framework, High Dynamic Range Gaussian Splatting (HDR-GS), which can efficiently render novel HDR views and reconstruct LDR images with a user input exposure time. Specifically, we design a Dual Dynamic Range (DDR) Gaussian point cloud model that uses spherical harmonics to fit HDR color and employs an MLP-based tone-mapper to render LDR color. The HDR and LDR colors are then fed into two Parallel Differentiable Rasterization (PDR) processes to reconstruct HDR and LDR views. To establish the data foundation for the research of 3D Gaussian splatting-based methods in HDR NVS, we recalibrate the camera parameters and compute the initial positions for Gaussian point clouds. Experiments demonstrate that our HDR-GS surpasses the state-of-the-art NeRF-based method by 3.84 and 1.91 dB on LDR and HDR NVS while enjoying 1000x inference speed and only requiring 6.3% training time.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.15125v1">PDF</a> The first 3D Gaussian Splatting-based method for HDR imaging</p><p><strong>Summary</strong><br>提出高动态范围Gaussian Splatting（HDR-GS）框架，实现高效 novel view synthesis 和曝光时间可控的低动态范围图像重建。</p><p><strong>Key Takeaways</strong><br>• 高动态范围 novel view synthesis（HDR NVS）旨在使用HDR成像技术从新视点生成逼真的图像。<br>• 现有的HDR NVS方法主要基于NeRF，存在长训练时间和慢推理速度的问题。<br>• 本文提出高动态范围Gaussian Splatting（HDR-GS）框架，实现高效 novel view synthesis 和曝光时间可控的低动态范围图像重建。<br>• HDR-GS使用双动态范围（DDR）高斯点云模型和基于MLP的tone-mapper来渲染HDR和LDR颜色。<br>• 该方法在LDR和HDR NVS任务上超过基于NeRF的方法，且具有1000倍的推理速度和仅需6.3%的训练时间<br>• 实验结果表明HDR-GS在HDR NVS任务上具有明显的优势。<br>• 本文为基于3D高斯splattting的HDR NVS方法奠定了数据基础。</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: 高动态范围新视图合成（HDR-GS）：基于高斯抹除的高效HDR新视图合成（High Dynamic Range Gaussian Splatting: Efficient HDR Novel View Synthesis via Gaussian Splatting）</p></li><li><p>Authors: Yuanhao Cai, Zihao Xiao, Yixun Liang, Minghan Qin, Yulun Zhang, Xiaokang Yang, Yaoyao Liu, Alan Yuille</p></li><li><p>Affiliation: 约翰斯·霍普金斯大学</p></li><li><p>Keywords: 高动态范围, 新视图合成, 高斯抹除, Novel View Synthesis, HDR, Gaussian Splatting</p></li><li><p>Urls: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.15125v1">https://arxiv.org/abs/2405.15125v1</a>, Github: <a target="_blank" rel="noopener" href="https://github.com/caiyuanhao1998/HDR-GS">https://github.com/caiyuanhao1998/HDR-GS</a></p></li><li><p>Summary:</p></li></ol><ul><li>(1):本文研究背景是高动态范围（HDR）新视图合成（NVS），旨在使用HDR成像技术从新视点生成逼真的图像。</li></ul><ul><li>(2):过去的方法主要基于NeRF，但这些方法存在长训练时间和慢推理速度的问题。</li></ul><ul><li>(3):本文提出的研究方法是High Dynamic Range Gaussian Splatting（HDR-GS），它使用双动态范围（DDR）高斯点云模型和平行可微分光栅化（PDR）过程来高效地渲染HDR和LDR视图。</li></ul><ul><li>(4):本文方法在HDR和LDR新视图合成任务上优于基于NeRF的方法，达到了3.84和1.91 dB的PSNR性能，并且具有1000倍的推理速度和仅需6.3%的训练时间</li></ul><ol><li>方法：</li></ol><ul><li><p>(1):提出双动态范围（DDR）高斯点云模型，用于表示高动态范围（HDR）图像的颜色和深度信息，该模型由高斯分布函数和点云数据组成。</p></li><li><p>(2):使用平行可微分光栅化（PDR）过程将DDR高斯点云模型转换为高效的渲染表示，以便快速生成HDR和LDR视图。</p></li><li><p>(3):设计高斯抹除（Gaussian Splatting）算法，用于将DDR高斯点云模型投影到目标视图平面上，生成高质量的HDR和LDR图像。</p></li><li><p>(4):提出基于高斯抹除的新视图合成（Novel View Synthesis）方法，用于从给定的HDR图像中生成意视点的HDR和LDR图像。</p></li><li><p>(5):使用基于NeRF的方法作为基线，比较HDR-GS方法在HDR和LDR新视图合成任务上的性能，结果表明HDR-GS方法具有更高的PSNR性能和更快的推理速度。</p></li><li><p>(6):通过实验验证HDR-GS方法的有效性和高效性，结果表明HDR-GS方法能够生成高质量的HDR和LDR图像，并且具有实时渲染的能力。</p></li></ul><ol><li>Conclusion:</li></ol><ul><li>(1):该研究工作的重要性在于解决了高动态范围（HDR）新视图合成中的效率问题，实现了高质量的HDR图像渲染和快速推理速度，具有广泛的应用前景在计算机视觉、图形学和机器学习等领域。</li></ul><ul><li>(2):创新点：提出了一种基于高斯抹除的高效HDR新视图合成方法HDR-GS，解决了基于NeRF方法的长训练时间和慢推理速度问题；性能：在HDR和LDR新视图合成任务上，HDR-GS方法具有更高的PSNR性能和更快的推理速度；工作量：HDR-GS方法仅需6.3%的训练时间和1000倍的推理速度，具有实时渲染的能力。</li></ul><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-62274faaed9878e5e0161dea6f18dbbe.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-1eb56bf3e6d513a6248b50e7a8d0c539.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-a6cf6e245e96bb903d2b486b7727c24e.jpg" align="middle"></details><h2 id="GS-Hider-Hiding-Messages-into-3D-Gaussian-Splatting"><a href="#GS-Hider-Hiding-Messages-into-3D-Gaussian-Splatting" class="headerlink" title="GS-Hider: Hiding Messages into 3D Gaussian Splatting"></a>GS-Hider: Hiding Messages into 3D Gaussian Splatting</h2><p><strong>Authors:Xuanyu Zhang, Jiarui Meng, Runyi Li, Zhipei Xu, Yongbing Zhang, Jian Zhang</strong></p><p>3D Gaussian Splatting (3DGS) has already become the emerging research focus in the fields of 3D scene reconstruction and novel view synthesis. Given that training a 3DGS requires a significant amount of time and computational cost, it is crucial to protect the copyright, integrity, and privacy of such 3D assets. Steganography, as a crucial technique for encrypted transmission and copyright protection, has been extensively studied. However, it still lacks profound exploration targeted at 3DGS. Unlike its predecessor NeRF, 3DGS possesses two distinct features: 1) explicit 3D representation; and 2) real-time rendering speeds. These characteristics result in the 3DGS point cloud files being public and transparent, with each Gaussian point having a clear physical significance. Therefore, ensuring the security and fidelity of the original 3D scene while embedding information into the 3DGS point cloud files is an extremely challenging task. To solve the above-mentioned issue, we first propose a steganography framework for 3DGS, dubbed GS-Hider, which can embed 3D scenes and images into original GS point clouds in an invisible manner and accurately extract the hidden messages. Specifically, we design a coupled secured feature attribute to replace the original 3DGS’s spherical harmonics coefficients and then use a scene decoder and a message decoder to disentangle the original RGB scene and the hidden message. Extensive experiments demonstrated that the proposed GS-Hider can effectively conceal multimodal messages without compromising rendering quality and possesses exceptional security, robustness, capacity, and flexibility. Our project is available at: <a target="_blank" rel="noopener" href="https://xuanyuzhang21.github.io/project/gshider">https://xuanyuzhang21.github.io/project/gshider</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.15118v1">PDF</a> 3DGS steganography</p><p><strong>Summary</strong><br>三维高斯分裂（3DGS）隐写术框架GS-Hider，实现了对原始3DGS点云文件的隐写和提取。</p><p><strong>Key Takeaways</strong><br>• 3DGS需要保护版权、完整性和隐私，因为训练需要大量时间和计算成本。<br>• 3DGS具有显式3D表示和实时渲染速度，导致点云文件公开透明，具有明确的物理意义。<br>• GS-Hider框架可以将3D场景和图像嵌入到原始GS点云中，以不可见的方式提取隐藏的消息。<br>• GS-Hider使用耦合安全特征属性替换原始3DGS的球谐系数，并使用场景解码器和消解码器来分离原始RGB场景和隐藏消息。<br>• 实验表明，GS-Hider可以有效地隐藏多模式消息，而不影响渲染质量，具有异常的安全性、鲁棒性、容量和灵活性。<br>• GS-Hider项目可在<a target="_blank" rel="noopener" href="https://xuanyuzhang21.github.io/project/gshider上访问。">https://xuanyuzhang21.github.io/project/gshider上访问。</a><br>• GS-Hider框架可以保护3DGS的版权、完整性和隐私。</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: GS-Hider：隐藏消息到3D高斯点云（GS-Hider: Hiding Messages into 3D Gaussian Splatting）</p></li><li><p>Authors: Xuanyu Zhang, Jiarui Meng, Runyi Li, Zhipei Xu, Yongbing Zhang, Jian Zhang</p></li><li><p>Affiliation: 电子与计算机工程学院，北京大学（School of Electronic and Computer Engineering, Peking University）</p></li><li><p>Keywords: 3D高斯点云、隐写术、数字水印、copyright protection</p></li><li><p>Urls: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.15118">https://arxiv.org/abs/2405.15118</a>, Github: <a target="_blank" rel="noopener" href="https://xuanyuzhang21.github.io/project/gshider/">https://xuanyuzhang21.github.io/project/gshider/</a></p></li><li><p>Summary:</p></li></ol><ul><li><p>(1):本文的研究背景是保护3D场景重建和新视图合成中的数字资产的版权和隐私，特别是基于3D高斯点云（3DGS）的方法。</p></li><li><p>(2):过去的隐写术方法主要使用傅里叶和小波变换来嵌入消息，但是这些方法不能很好地适应3DGS的特点，例如明确的3D表示和实时渲染速度。</p></li><li><p>(3):本文提出了一个名为GS-Hider的隐写术框架，使用耦合的安全特征属性来替换原始3DGS的球谐系数，然后使用场景解码器和消息解码器来分离原始RGB场景和隐藏的消息。</p></li><li><p>(4):实验结果表明，GS-Hider可以在不影响渲染质量的情况下隐藏多模态消息，并且具有非常高的安全性、鲁棒性、容量和灵活性。</p></li></ul><ol><li>方法：</li></ol><ul><li><p>(1)：首先，作者们提出了基于耦合安全特征属性的隐写术框架GS-Hider，该框架可以将消息隐藏在3D高斯点云（3DGS）中。</p></li><li><p>(2)：在GS-Hider框架中，作者们使用耦合的安全特征属性来替换原始3DGS的球谐系数，具体来说，就是将消息嵌入到球谐系数中。</p></li><li><p>(3)：然后，作者们使用场景解码器和消息解码器来分离原始RGB场景和隐藏的消息，这两个解码器都是基于深度学习的神经网络。</p></li><li><p>(4)：在消息嵌入过程中，作者们使用了anisotropic Gaussians表示场景，通过splattin技术将3D高斯点云投影到图像平面上，并使用经点基于渲染来生成图像。</p></li><li><p>(5)：为了提高消息的安全性和鲁棒性，作者们使用了多种技术，包括DIFFusion-based方法和Frequency-based方法来保护消息抵抗攻击。</p></li><li><p>(6)：在实验中，作者们使用了多种数据集和评估指标来评估GS-Hider的性能，结果表明GS-Hider可以在不影响渲染质量的情况下隐藏多模态消息，并且具有非常高的安全性、鲁棒性、容量和灵活性。</p></li></ul><ol><li>Conclusion:</li></ol><ul><li><p>(1): 本文的工作意义在于提出了一种高保真、安全、大容量和多功能的3D高斯点云隐写术框架，即GS-Hider，为保护3D场景重建和新视图合成中的数字资产版权和隐私提供了有效的技术支持。</p></li><li><p>(2): 创新点：GS-Hider框架利用耦合的安全特征表示和双解码器解码技术，实现了在3D高斯点云中隐藏消息，具有很高的安全性、鲁棒性和灵活性；性能：实验结果表明GS-Hider在不影响渲染质量的情况下可以隐藏多模态消息，且具有高容量；工作量：文章未详细说明具体的工作量评估，需要进一步补充和完善。</p></li></ul><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-44535b4dc9ae919b2dce80a4be050e9a.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-bbb3c977263acb314ebe7c8c3a9043c9.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-9e7d4ae3f321d6e860ec2da2743463f2.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-8db132ec3c58c945a06898a8758b7480.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-51183cc617b206934e4fdaaba05fdc46.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-c5422ed30935cd238fd580f363ae7ec2.jpg" align="middle"></details><h2 id="DoGaussian-Distributed-Oriented-Gaussian-Splatting-for-Large-Scale-3D-Reconstruction-Via-Gaussian-Consensus"><a href="#DoGaussian-Distributed-Oriented-Gaussian-Splatting-for-Large-Scale-3D-Reconstruction-Via-Gaussian-Consensus" class="headerlink" title="DoGaussian: Distributed-Oriented Gaussian Splatting for Large-Scale 3D   Reconstruction Via Gaussian Consensus"></a>DoGaussian: Distributed-Oriented Gaussian Splatting for Large-Scale 3D Reconstruction Via Gaussian Consensus</h2><p><strong>Authors:Yu Chen, Gim Hee Lee</strong></p><p>The recent advances in 3D Gaussian Splatting (3DGS) show promising results on the novel view synthesis (NVS) task. With its superior rendering performance and high-fidelity rendering quality, 3DGS is excelling at its previous NeRF counterparts. The most recent 3DGS method focuses either on improving the instability of rendering efficiency or reducing the model size. On the other hand, the training efficiency of 3DGS on large-scale scenes has not gained much attention. In this work, we propose DoGaussian, a method that trains 3DGS distributedly. Our method first decomposes a scene into K blocks and then introduces the Alternating Direction Method of Multipliers (ADMM) into the training procedure of 3DGS. During training, our DoGaussian maintains one global 3DGS model on the master node and K local 3DGS models on the slave nodes. The K local 3DGS models are dropped after training and we only query the global 3DGS model during inference. The training time is reduced by scene decomposition, and the training convergence and stability are guaranteed through the consensus on the shared 3D Gaussians. Our method accelerates the training of 3DGS by 6+ times when evaluated on large-scale scenes while concurrently achieving state-of-the-art rendering quality. Our project page is available at <a target="_blank" rel="noopener" href="https://aibluefisher.github.io/DoGaussian">https://aibluefisher.github.io/DoGaussian</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.13943v1">PDF</a></p><p><strong>Summary</strong><br>最近对3D高斯点云（3DGS）的研究显示了在新视图合成（NVS）任务上取得了令人期待的成果。</p><p><strong>Key Takeaways</strong></p><ul><li>3DGS在渲染性能和保真度方面表现优越，优于以往的NeRF方法。</li><li>最近的3DGS方法要么专注于改善渲染效率的不稳定性，要么减小模型尺寸。</li><li>本文提出了DoGaussian方法，该方法通过将场景分解为K个块，并引入交替方向乘子法（ADMM）来分布式训练3DGS。</li><li>DoGaussian方法通过场景分解缩短了训练时间，同时确保了训练的收敛性和稳定性。</li><li>训练时间缩短了6倍以上，同时在大规模场景上实现了最先进的渲染质量。</li><li>项目页面链接：<a target="_blank" rel="noopener" href="https://aibluefisher.github.io/DoGaussian。">https://aibluefisher.github.io/DoGaussian。</a></li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>Title: DoGaussian：分布式面向高斯斯普拉特（Distributed-Oriented Gaussian Splatting）</li></ol><ol><li>Authors: Yu Chen, Gim Hee Lee</li></ol><ol><li>Affiliation: 新加坡国立大学</li></ol><ol><li>Keywords: 3D Gaussian Splatting, Novel View Synthesis, Distributed Training</li></ol><ol><li>Urls: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.13943v1">https://arxiv.org/abs/2405.13943v1</a>, Github: None</li></ol><ol><li>Summary:</li></ol><pre><code>- (1):近年来，三维高斯斯普拉特（3DGS）在新视图合成（NVS）任务中取得了良好的结果，然而，当前3DGS方法的训练效率在大规模场景下尚未受到足够的关注。


- (2):之前的方法主要集中在提高渲染效率的不稳定性或减少模型大小，但这些方法忽视了大规模场景下的训练效率问题。


- (3):本文提出了DoGaussian方法，该方法将场景分解成K个块，然后引入交替方向乘子法（ADMM）到3DGS的训练过程中。在训练过程中，DoGaussian在主节点上维护一个全局的3DGS模型，在从节点上维护K个局部的3DGS模型


- (4):DoGaussian方在大规模场景下加速了3DGS的训练速度，达到了6倍以上的加速，同时也获得了最先进的渲染质量。
</code></pre><ol><li>方法：</li></ol><ul><li><p>(1)：将场景分解成 K 个块，以便分布式训练。在每个块中，分配训练视图和点云数据。</p></li><li><p>(2)：引入 Alternating Direction Method of Multipliers（ADMM）算法，在分布式训练中实现全局一致的 3D Gaussian Splatting 模型。在每个块中，维护一个局部的 3D Gaussian Splatting 模型，并与主节点上的全局模型进行交互。</p></li><li><p>(3)：在每个块中，使用 ADMM 算法更新局部模型，并将更新后的模型与主节点上的全局模型进行平均，以实现模型的一致性。</p></li><li><p>(4)：在训练过程中，使用 Penalty Parameter 和 Over-relaxation 技术来提高 ADMM 算法的收敛速度。</p></li><li><p>(5)：使用场景分割算法，以确保每个块的大小相似，并且相邻块之间有足够的重叠区域，以促进训练的收敛。</p></li><li><p>(6)：在训练完成后，使用全局模型来合成新视图，以实现高质量的渲染结果。</p></li><li><p>(7)：实验结果表明，提出的 DoGaussian 方法可以在大规模场景下加速 3D Gaussian Splatting 的训练速度，达到了 6 倍以上的加速，同时也获得了最先进的渲染质量。</p></li></ul><ol><li>Conclusion:</li></ol><ul><li><p>(1):本文的贡献在于解决了三维高斯斯普拉特（3DGS）在大规模场景下的训效率问题，提高了新视图合成（NVS）的实时性和质量。</p></li><li><p>(2):创新点：提出了一种分布式训练方法DoGaussian，使用Alternating Direction Method of Multipliers（ADMM）算法实现全局一致的3DGS模型；性能：加速了3DGS的训练速度，达到了6倍以上的加速，同时也获得了最先进的渲染质量；工作量：需要大量的计算资源和场景分割算法来实现分布式训练。</p></li></ul><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-22c8c9dbbe8897a84779859d7460a6eb.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-261a3638b92396cc85c1385cc6c53581.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-4e3e352a0325ce88ecaee52f7e182708.jpg" align="middle"></details><h2 id="Gaussian-Time-Machine-A-Real-Time-Rendering-Methodology-for-Time-Variant-Appearances"><a href="#Gaussian-Time-Machine-A-Real-Time-Rendering-Methodology-for-Time-Variant-Appearances" class="headerlink" title="Gaussian Time Machine: A Real-Time Rendering Methodology for   Time-Variant Appearances"></a>Gaussian Time Machine: A Real-Time Rendering Methodology for Time-Variant Appearances</h2><p><strong>Authors:Licheng Shen, Ho Ngai Chow, Lingyun Wang, Tong Zhang, Mengqiu Wang, Yuxing Han</strong></p><p>Recent advancements in neural rendering techniques have significantly enhanced the fidelity of 3D reconstruction. Notably, the emergence of 3D Gaussian Splatting (3DGS) has marked a significant milestone by adopting a discrete scene representation, facilitating efficient training and real-time rendering. Several studies have successfully extended the real-time rendering capability of 3DGS to dynamic scenes. However, a challenge arises when training images are captured under vastly differing weather and lighting conditions. This scenario poses a challenge for 3DGS and its variants in achieving accurate reconstructions. Although NeRF-based methods (NeRF-W, CLNeRF) have shown promise in handling such challenging conditions, their computational demands hinder real-time rendering capabilities. In this paper, we present Gaussian Time Machine (GTM) which models the time-dependent attributes of Gaussian primitives with discrete time embedding vectors decoded by a lightweight Multi-Layer-Perceptron(MLP). By adjusting the opacity of Gaussian primitives, we can reconstruct visibility changes of objects. We further propose a decomposed color model for improved geometric consistency. GTM achieved state-of-the-art rendering fidelity on 3 datasets and is 100 times faster than NeRF-based counterparts in rendering. Moreover, GTM successfully disentangles the appearance changes and renders smooth appearance interpolation.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.13694v1">PDF</a> 14 pages, 6 figures</p><p><strong>Summary</strong><br>利用高斯时间机GTM实现实时三维重建，解决weather和lighting条件变化带来的挑战。</p><p><strong>Key Takeaways</strong><br>• 三维高斯Splatting（3DGS）技术的出现标志着三维重建的重要里程碑。<br>• 3DGS及其变体在实时渲染动态场景方面取得了成功，但是在不同天气和照明条件下训练图像时存在挑战。<br>• NeRF-based方法（NeRF-W、CLNeRF）可以处理这种挑战，但计算需求高，影响实时渲染能力。<br>• 高斯时间机GTM使用轻量级MLP模型时间嵌入矢量来模拟高斯primitive的时间依赖属性。<br>• GTM可以重建对象的可见性变化，并且具有更好的几何一致性。<br>• GTM在三个数据集上的渲染保真度达到最好，并且染速度是NeRF-based方法的100倍。<br>• GTM成功地分离了外观变化，并实现了平滑的外观插值。</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>Title: 高斯时间机器：实时渲染时间变换外观 (Gaussian Time Machine: A Real-Time Rendering Methodology for Time-Variant Appearances)</li></ol><ol><li>Authors: Licheng Shen, Ho Ngai Chow, Lingyun Wang, Tong Zhang, Mengqiu Wang, Yuxing Han</li></ol><ol><li>Affiliation: 清华大学深圳国际研究生院</li></ol><ol><li>Keywords: Neural Rendering · 3D Gaussian Splatting · Varying Appearance</li></ol><ol><li>Urls: arXiv:2405.13694v1, Github:None</li></ol><ol><li>Summary:</li></ol><pre><code>- (1):近年来，神经渲染技术的发展极大地提高了三维重建的保真度。特别是，三维高斯点绘制（3DGS）提出了离散场景表示，提高了训练速度和实时渲染质量。

- (2):过去的方法如NeRF-W和CLNeRF可以处理复杂的天气和照明条件，但是它们的计算需求限制了实时渲染能力。3DGS和其变体也存在着准确重建的挑战。

- (3):本文提出了高斯时间机器（GTM），它使用离散时间嵌入向量和轻量级多层感知器（MLP）来建模高斯primitive的时间相关属性。通过调整高斯primitive的不透明度，可以重建对象的可见性变化。

- (4):GTM在三个数据集上实现了最先进的渲染保真度，渲染速度是NeRF-based方法的100倍。此外，GTM还成功地分离了外观变化并实现了平滑的外观插值。
</code></pre><ol><li>Methods:</li></ol><ul><li><p>(1): 本文提出的高斯时间机器（Gaussian Time Machine，GTM）采用离散时间嵌入向量和轻量级多层感知器（MLP）来建模高斯primitive的时间相关属性。</p></li><li><p>(2): GTM通过调整高斯primitive的不透明度，实现了对象可见性的变化，并成功地分离了外观变化。</p></li><li><p>(3): 在三个数据集上，GTM展现出了最先进的渲染保真度，且渲染速度是基于NeRF的方法的100倍。此外，GTM还能够实现平滑的外观插值。</p></li></ul><ol><li>Conclusion:</li></ol><ul><li><p>(1):本文提出的高斯时间机器（Gaussian Time Machine，GTM）在解决时间变换外观问题方面具有重要意义，可以应用于虚拟现实、数字孪生等领域。</p></li><li><p>(2):创新点：GTM 提出了离散时间嵌入向量和轻量级多层感知器（MLP）来建模高斯primitive的时间相关属性，实现了对象可见性的变化和外观变化的分离；性能：GTM 在三个数据集上实现了最先进的渲染保真度，渲染速度是 NeRF-based 方法的 100 倍；工作量：GTM 需要较少的计算资源和训练时间，能够实现实时渲染。</p></li></ul><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-e37e39f80d95d9753e062031ea071292.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-8d45eb05bc11e95b4d1a05a781ee482b.jpg" align="middle"></details><h2 id="GaussianVTON-3D-Human-Virtual-Try-ON-via-Multi-Stage-Gaussian-Splatting-Editing-with-Image-Prompting"><a href="#GaussianVTON-3D-Human-Virtual-Try-ON-via-Multi-Stage-Gaussian-Splatting-Editing-with-Image-Prompting" class="headerlink" title="GaussianVTON: 3D Human Virtual Try-ON via Multi-Stage Gaussian Splatting   Editing with Image Prompting"></a>GaussianVTON: 3D Human Virtual Try-ON via Multi-Stage Gaussian Splatting Editing with Image Prompting</h2><p><strong>Authors:Haodong Chen, Yongle Huang, Haojian Huang, Xiangsheng Ge, Dian Shao</strong></p><p>The increasing prominence of e-commerce has underscored the importance of Virtual Try-On (VTON). However, previous studies predominantly focus on the 2D realm and rely heavily on extensive data for training. Research on 3D VTON primarily centers on garment-body shape compatibility, a topic extensively covered in 2D VTON. Thanks to advances in 3D scene editing, a 2D diffusion model has now been adapted for 3D editing via multi-viewpoint editing. In this work, we propose GaussianVTON, an innovative 3D VTON pipeline integrating Gaussian Splatting (GS) editing with 2D VTON. To facilitate a seamless transition from 2D to 3D VTON, we propose, for the first time, the use of only images as editing prompts for 3D editing. To further address issues, e.g., face blurring, garment inaccuracy, and degraded viewpoint quality during editing, we devise a three-stage refinement strategy to gradually mitigate potential issues. Furthermore, we introduce a new editing strategy termed Edit Recall Reconstruction (ERR) to tackle the limitations of previous editing strategies in leading to complex geometric changes. Our comprehensive experiments demonstrate the superiority of GaussianVTON, offering a novel perspective on 3D VTON while also establishing a novel starting point for image-prompting 3D scene editing.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.07472v2">PDF</a> On-going work</p><p><strong>Summary</strong><br>电子商务的日益突出彰显了虚拟试穿（VTON）的重要性。本文提出了GaussianVTON，将高斯点绘制（GS）编辑与2D VTON相结合，首次提出使用图像作为3D编辑提示，以及引入了ERR编辑策略，为3D VTON提供了新视角。</p><p><strong>Key Takeaways</strong></p><ul><li>电子商务的日益突出彰显了虚拟试穿（VTON）的重要性。</li><li>GaussianVTON将高斯点绘制（GS）编辑与2D VTON相结合，首次提出使用图像作为3D编辑提示。</li><li>通过三阶段的精细化策略逐步缓解潜在问题，进一步解决了面部模糊、服装不准确和编辑过程中视角质量下降等问题。</li><li>引入了ERR编辑策略来应对之前编辑策略的局限性，解决了复杂几何变化带来的问题。</li><li>实验结果显示，GaussianVTON具有卓越性能，为3D VTON提供了新视角，并建立了图像提示3D场景编辑的新起点。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: 高斯 Virtual Try-On：基于多阶段高斯 Splatting 的 3D 人体虚拟试衣（GaussianVTON: 3D Human Virtual Try-ON via Multi-Stage Gaussian Splatting）</p></li><li><p>Authors: Haodong Chen, Yongle Huang, Haojian Huang, Xiangsheng Ge, Dian Shao</p></li><li><p>Affiliation: 西北工业大学</p></li><li><p>Keywords: Virtual Try-On, 3D Human, Gaussian Splatting, Image Prompting</p></li><li><p>Urls: <a target="_blank" rel="noopener" href="https://haroldchen19.github.io/gsvton/">https://haroldchen19.github.io/gsvton/</a>, Github:None</p></li><li><p>Summary:</p><ul><li><p>(1):随着电子商务的兴起，虚拟试衣（Virtual Try-On, VTON）变得越来越重要。然而，之前的研究主要集中在 2D 领域，并且需要大量的训练数据。</p></li><li><p>(2):过去的方法主要集中在 2D VTON 领域，并且需要大量的训练数据。这些方法无法很好地解决 3D VTON 问题，例如服装形状与人体形状的不兼容问题</p></li><li><p>(3):本文提出了 GaussianVTON，一种基于多阶段高斯 Splatting 的 3D VTON 管道。该方法使用图像作为编辑提示，实现了从 2D 到 3D VTON 的无缝过渡。</p></li><li><p>(4):实验结果表明，GaussianVTON 方法在 3D VTON 任务上取得了优异的性能，证明了该方法的有效性。</p></li></ul></li><li>方法：</li></ol><ul><li><p>(1)：输入重建的 3D 场景和相应的数据，包括一系列拍摄的图像、相应的相机姿态和相机标定参数。</p></li><li><p>(2)：使用图像编辑提示来指导 3D 场景的编辑过程，以实现虚拟试衣。首先，引入 3D 高斯 Splatting 模型和基于扩散的 2D VTON 模型。</p></li><li><p>(3)：提出了 Editing Recall Reconstruction (ERR) 策略，该策略在编辑过程中渲染整个数据集，以解决编辑不一致的问题。</p></li><li><p>(4)：采用三阶段细化策略，包括人脸一致性、层次稀疏编辑和图像质量改进三个阶段，以解决编辑过程中遇到的各种问题。</p></li><li><p>(5)：在 ERR 策略中，对整个数据集进行编辑和细化，然后对数据集进行更新，以确保编辑的一致性。</p></li><li><p>(6)：使用 LaDI-VTON 模型对每个图像进行编辑，并将编辑结果与原始图像进行比较，以评估编辑的效果。</p></li><li><p>(7)：对编辑结果进行可视化和评估，以验证 GaussianVTON 方法的有效性。</p></li></ul><ol><li><p>Conclusion:</p><pre><code>             - (1):本文的工作对电子商务虚拟试衣领域的发展具有重要意义，可以为用户提供更加真实的试衣体验。

             - (2):创新点：本文提出了一种基于多阶段高斯 Splatting 的 3D 人体虚拟试衣方法，解决了 2D 到 3D 虚拟试衣的技术瓶颈；性能：实验结果表明，GaussianVTON 方法在 3D VTON 任务上取得了优异的性能；工作量：本文的方法需要大量的训练数据和计算资源，限制了其在实际应用中的普及性。
</code></pre></li></ol><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-e12873404001a9a09d996899cdfe1fc3.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-c590805a84c00f53de63efe5b169e438.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-28127860f8d303f51aff59430d547019.jpg" align="middle"></details></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io/2024/05/28/Paper/2024-05-28/3DGS/">https://kedreamix.github.io/2024/05/28/Paper/2024-05-28/3DGS/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/3DGS/">3DGS</a></div><div class="post_share"><div class="social-share" data-image="https://pic1.zhimg.com/v2-8db132ec3c58c945a06898a8758b7480.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/05/28/Paper/2024-05-28/NeRF/" title="NeRF"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-46b90894aa28846d98c1eef5c5a89f0c.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">NeRF</div></div></a></div><div class="next-post pull-right"><a href="/2024/05/28/Paper/2024-05-28/Talking%20Head%20Generation/" title="Talking Head Generation"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-33e1c85bbd2586fc6e8eb024aa73c567.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Talking Head Generation</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/01/25/Paper/3DGS%20Survey/" title="3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-25</div><div class="title">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</div></div></a></div><div><a href="/2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" title="超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-01</div><div class="title">超赞的数字人生成知识库 Awesome-Talking-Head-Synthesis</div></div></a></div><div><a href="/2024/02/02/Paper/2024-02-02/3DGS/" title="3DGS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-e4e5570dfa99dfac9b297f7650c717c3.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-02</div><div class="title">3DGS</div></div></a></div><div><a href="/2024/01/24/Paper/2024-01-24/3DGS/" title="3DGS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-3d3dcd00c27bc3d320b23d4247ae79f3.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-24</div><div class="title">3DGS</div></div></a></div><div><a href="/2024/01/30/Paper/2024-01-30/3DGS/" title="3DGS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-2032721a60695f2d41ac96f75dec65a2.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-30</div><div class="title">3DGS</div></div></a></div><div><a href="/2024/02/09/Paper/2024-02-09/3DGS/" title="3DGS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-28074a5f13fdf5a52c0d4de04dfb9406.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-09</div><div class="title">3DGS</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-05-28-%E6%9B%B4%E6%96%B0"><span class="toc-text">2024-05-28 更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Feature-Splatting-for-Better-Novel-View-Synthesis-with-Low-Overlap"><span class="toc-text">Feature Splatting for Better Novel View Synthesis with Low Overlap</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HDR-GS-Efficient-High-Dynamic-Range-Novel-View-Synthesis-at-1000x-Speed-via-Gaussian-Splatting"><span class="toc-text">HDR-GS: Efficient High Dynamic Range Novel View Synthesis at 1000x Speed via Gaussian Splatting</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#GS-Hider-Hiding-Messages-into-3D-Gaussian-Splatting"><span class="toc-text">GS-Hider: Hiding Messages into 3D Gaussian Splatting</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DoGaussian-Distributed-Oriented-Gaussian-Splatting-for-Large-Scale-3D-Reconstruction-Via-Gaussian-Consensus"><span class="toc-text">DoGaussian: Distributed-Oriented Gaussian Splatting for Large-Scale 3D Reconstruction Via Gaussian Consensus</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Gaussian-Time-Machine-A-Real-Time-Rendering-Methodology-for-Time-Variant-Appearances"><span class="toc-text">Gaussian Time Machine: A Real-Time Rendering Methodology for Time-Variant Appearances</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#GaussianVTON-3D-Human-Virtual-Try-ON-via-Multi-Stage-Gaussian-Splatting-Editing-with-Image-Prompting"><span class="toc-text">GaussianVTON: 3D Human Virtual Try-ON via Multi-Stage Gaussian Splatting Editing with Image Prompting</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image:url('https://pic1.zhimg.com/v2-8db132ec3c58c945a06898a8758b7480.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js").then((()=>{pangu.autoSpacingPage()}))}function panguInit(){panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.1},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script></div><script id="canvas_nest" defer color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><script data-pjax>function butterfly_swiper_injector_config(){var a=document.getElementById("recent-posts");console.log("已挂载butterfly_swiper"),a.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/25/Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="2024/01/25/Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="2024/01/25/Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/17/Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023/12/17/Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="2023/12/17/Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script></body></html>