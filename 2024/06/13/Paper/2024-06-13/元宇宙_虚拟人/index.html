<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>å…ƒå®‡å®™/è™šæ‹Ÿäºº | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="å…ƒå®‡å®™&#x2F;è™šæ‹Ÿäºº æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-06-13  Human 3Diffusion Realistic Avatar Creation via Explicit 3D Consistent   Diffusion Models"><meta property="og:type" content="article"><meta property="og:title" content="å…ƒå®‡å®™&#x2F;è™šæ‹Ÿäºº"><meta property="og:url" content="https://kedreamix.github.io/2024/06/13/Paper/2024-06-13/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/index.html"><meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World"><meta property="og:description" content="å…ƒå®‡å®™&#x2F;è™šæ‹Ÿäºº æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-06-13  Human 3Diffusion Realistic Avatar Creation via Explicit 3D Consistent   Diffusion Models"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://picx.zhimg.com/v2-f32bae00bb972a17d554c58569365817.jpg"><meta property="article:published_time" content="2024-06-13T08:47:48.000Z"><meta property="article:modified_time" content="2024-06-13T08:47:48.975Z"><meta property="article:author" content="Kedreamix"><meta property="article:tag" content="å…ƒå®‡å®™&#x2F;è™šæ‹Ÿäºº"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://picx.zhimg.com/v2-f32bae00bb972a17d554c58569365817.jpg"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/2024/06/13/Paper/2024-06-13/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-74LZ5BEQQ1")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!0,languages:{hits_empty:"æ‰¾ä¸åˆ°æ‚¨æŸ¥è¯¢çš„å†…å®¹ï¼š${query}",hits_stats:"å…±æ‰¾åˆ° ${hits} ç¯‡æ–‡ç« "}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"ç¹",msgToSimplifiedChinese:"ç®€"},noticeOutdate:{limitDay:500,position:"top",messagePrev:"It has been",messageNext:"days since the last update, the content of the article may be outdated."},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:200},copy:{success:"å¤åˆ¶æˆåŠŸ",error:"å¤åˆ¶é”™è¯¯",noSupport:"æµè§ˆå™¨ä¸æ”¯æŒ"},relativeDate:{homepage:!0,post:!0},runtime:"å¤©",dateSuffix:{just:"åˆšåˆš",min:"åˆ†é’Ÿå‰",hour:"å°æ—¶å‰",day:"å¤©å‰",month:"ä¸ªæœˆå‰"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js",buttonText:"åŠ è½½æ›´å¤š"},isPhotoFigcaption:!0,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"å…ƒå®‡å®™/è™šæ‹Ÿäºº",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-06-13 16:47:48"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach((e=>{n.setAttribute(e,t[e])})),document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)})),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=24?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 7.0.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml"></head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{const d=document.getElementById("loading-box"),e=document.body,o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l(),window.addEventListener("load",(()=>{o()}))})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">æ–‡ç« </div><div class="length-num">153</div></a><a href="/tags/"><div class="headline">æ ‡ç­¾</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">åˆ†ç±»</div><div class="length-num">6</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>é¦–é¡µ</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>æ—¶é—´è½´</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>æ ‡ç­¾</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>åˆ†ç±»</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>å‹é“¾</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url('https://picx.zhimg.com/v2-f32bae00bb972a17d554c58569365817.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i> <span>æœç´¢</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>é¦–é¡µ</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>æ—¶é—´è½´</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>æ ‡ç­¾</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>åˆ†ç±»</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>å‹é“¾</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">å…ƒå®‡å®™/è™šæ‹Ÿäºº</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">å‘è¡¨äº</span><time class="post-meta-date-created" datetime="2024-06-13T08:47:48.000Z" title="å‘è¡¨äº 2024-06-13 16:47:48">2024-06-13</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">æ›´æ–°äº</span><time class="post-meta-date-updated" datetime="2024-06-13T08:47:48.975Z" title="æ›´æ–°äº 2024-06-13 16:47:48">2024-06-13</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">å­—æ•°æ€»è®¡:</span><span class="word-count">5.5k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">é˜…è¯»æ—¶é•¿:</span><span>29åˆ†é’Ÿ</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="å…ƒå®‡å®™/è™šæ‹Ÿäºº"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">é˜…è¯»é‡:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-06-13-æ›´æ–°"><a href="#2024-06-13-æ›´æ–°" class="headerlink" title="2024-06-13 æ›´æ–°"></a>2024-06-13 æ›´æ–°</h1><h2 id="Human-3Diffusion-Realistic-Avatar-Creation-via-Explicit-3D-Consistent-Diffusion-Models"><a href="#Human-3Diffusion-Realistic-Avatar-Creation-via-Explicit-3D-Consistent-Diffusion-Models" class="headerlink" title="Human 3Diffusion: Realistic Avatar Creation via Explicit 3D Consistent   Diffusion Models"></a>Human 3Diffusion: Realistic Avatar Creation via Explicit 3D Consistent Diffusion Models</h2><p><strong>Authors:Yuxuan Xue, Xianghui Xie, Riccardo Marin, Gerard Pons-Moll</strong></p><p>Creating realistic avatars from a single RGB image is an attractive yet challenging problem. Due to its ill-posed nature, recent works leverage powerful prior from 2D diffusion models pretrained on large datasets. Although 2D diffusion models demonstrate strong generalization capability, they cannot provide multi-view shape priors with guaranteed 3D consistency. We propose Human 3Diffusion: Realistic Avatar Creation via Explicit 3D Consistent Diffusion. Our key insight is that 2D multi-view diffusion and 3D reconstruction models provide complementary information for each other, and by coupling them in a tight manner, we can fully leverage the potential of both models. We introduce a novel image-conditioned generative 3D Gaussian Splats reconstruction model that leverages the priors from 2D multi-view diffusion models, and provides an explicit 3D representation, which further guides the 2D reverse sampling process to have better 3D consistency. Experiments show that our proposed framework outperforms state-of-the-art methods and enables the creation of realistic avatars from a single RGB image, achieving high-fidelity in both geometry and appearance. Extensive ablations also validate the efficacy of our design, (1) multi-view 2D priors conditioning in generative 3D reconstruction and (2) consistency refinement of sampling trajectory via the explicit 3D representation. Our code and models will be released on <a target="_blank" rel="noopener" href="https://yuxuan-xue.com/human-3diffusion">https://yuxuan-xue.com/human-3diffusion</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.08475v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://yuxuan-xue.com/human-3diffusion">https://yuxuan-xue.com/human-3diffusion</a></p><p><strong>Summary</strong><br>é€šè¿‡ç»“åˆ2Då¤šè§†è§’æ‰©æ•£æ¨¡å‹å’Œ3Dé‡å»ºæ¨¡å‹ï¼Œæˆ‘ä»¬æå‡ºäº†Human 3Diffusionï¼Œå®ç°äº†ä»å•ä¸ªRGBå›¾åƒåˆ›å»ºé€¼çœŸå¤´åƒçš„æ–¹æ³•ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>åˆ›é€ é€¼çœŸå¤´åƒçš„æŒ‘æˆ˜æ˜¯ä¸€ä¸ªå¸å¼•äººä½†å…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚</li><li>æœ€è¿‘çš„ç ”ç©¶åˆ©ç”¨é¢„å…ˆåœ¨å¤§å‹æ•°æ®é›†ä¸Šè®­ç»ƒçš„2Dæ‰©æ•£æ¨¡å‹çš„å¼ºå¤§å…ˆéªŒçŸ¥è¯†ã€‚</li><li>2Dæ‰©æ•£æ¨¡å‹å±•ç¤ºäº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†ä¸èƒ½æä¾›å…·æœ‰ä¿è¯çš„3Dä¸€è‡´æ€§çš„å¤šè§†è§’å½¢çŠ¶å…ˆéªŒã€‚</li><li>æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†2Då¤šè§†è§’æ‰©æ•£å’Œ3Dé‡å»ºæ¨¡å‹ï¼Œå……åˆ†åˆ©ç”¨äº†ä¸¤ç§æ¨¡å‹çš„æ½œåŠ›ã€‚</li><li>æå‡ºäº†ä¸€ç§æ–°çš„å›¾åƒæ¡ä»¶ç”Ÿæˆçš„3Dé«˜æ–¯æ–‘ç‚¹é‡å»ºæ¨¡å‹ï¼Œåˆ©ç”¨äº†2Då¤šè§†è§’æ‰©æ•£æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶æä¾›æ˜¾å¼çš„3Dè¡¨ç¤ºã€‚</li><li>å®éªŒæ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨å‡ ä½•å’Œå¤–è§‚ä¸Šéƒ½å®ç°äº†é«˜ä¿çœŸåº¦ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li><li>å¤§é‡æ¶ˆèå®éªŒè¯æ˜äº†æˆ‘ä»¬è®¾è®¡çš„æœ‰æ•ˆæ€§ã€‚</li><li>æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://yuxuan-xue.com/human-3diffusion">https://yuxuan-xue.com/human-3diffusion</a> ä¸Šå‘å¸ƒã€‚</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: Human 3Diffusion: Realistic Avatar Creation (äººç‰©3æ‰©æ•£ï¼šé€¼çœŸåŒ–èº«åˆ›å»º)</p></li><li><p>Authors: Yuxuan Xue, Xianghui Xie, Riccardo Marin, Gerard Pons-Moll</p></li><li><p>Affiliation: å¾·å›½å›¾å®¾æ ¹å¤§å­¦</p></li><li><p>Keywords: realistic avatars, 3D consistent diffusion, generative models, image reconstruction</p></li><li><p>Urls: <a target="_blank" rel="noopener" href="https://yuxuan-xue.com/human-3diffusion/">Paper</a>, Github: None</p></li><li><p>Summary:</p></li></ol><ul><li><p>(1): æœ¬æ–‡ç ”ç©¶èƒŒæ™¯æ˜¯åˆ›å»ºä»å•ä¸€RGBå›¾åƒç”Ÿæˆé€¼çœŸåŒ–èº«çš„é—®é¢˜ã€‚</p></li><li><p>(2): è¿‡å»çš„æ–¹æ³•ä¸»è¦ä¾èµ–äºé¢„è®­ç»ƒäºå¤§å‹æ•°æ®é›†çš„2Dæ‰©æ•£æ¨¡å‹ï¼Œå°½ç®¡è¿™äº›æ¨¡å‹å±•ç¤ºäº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†å®ƒä»¬æ— æ³•æä¾›å…·å¤‡3Dä¸€è‡´æ€§çš„å¤šè§†è§’å½¢çŠ¶å…ˆéªŒã€‚æœ¬æ–‡çš„æ–¹æ³•åœ¨äºç»“åˆ2Då¤šè§†è§’æ‰©æ•£æ¨¡å‹ä¸3Dé‡å»ºæ¨¡å‹ï¼Œä»¥ç´§å¯†è€¦åˆçš„æ–¹å¼åˆ©ç”¨ä¸¤è€…çš„æ½œåŠ›ï¼Œä»è€Œæ›´å¥½åœ°å®ç°å•å›¾åƒåˆ°é€¼çœŸåŒ–èº«çš„è½¬æ¢ä»»åŠ¡ã€‚</p></li><li><p>(3): æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŸºäºå›¾åƒæ¡ä»¶çš„ç”Ÿæˆ3Dé«˜æ–¯æ–‘ç‚¹é‡å»ºæ¨¡å‹ï¼Œåˆ©ç”¨äº†2Då¤šè§†è§’æ‰©æ•£æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶æä¾›æ˜ç¡®çš„3Dè¡¨ç¤ºï¼Œè¿›ä¸€æ­¥æŒ‡å¯¼2Dåå‘é‡‡æ ·è¿‡ç¨‹ä»¥è·å¾—æ›´å¥½çš„3Dä¸€è‡´æ€§ã€‚</p></li><li><p>(4): è¯¥æ–¹æ³•åœ¨å•ä¸€RGBå›¾åƒåˆ°é€¼çœŸåŒ–èº«çš„ç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æ€§èƒ½æå‡ï¼Œå®ç°äº†å‡ ä½•å’Œå¤–è§‚ä¸Šçš„é«˜ä¿çœŸåº¦ï¼Œå®éªŒè¯æ˜å…¶è®¾è®¡çš„æœ‰æ•ˆæ€§ã€‚</p></li></ul><ol><li>Methods:</li></ol><ul><li><p>(1): æœ¬æ–‡æ–¹æ³•ä¸»è¦ç»“åˆäº†2Då¤šè§†è§’æ‰©æ•£æ¨¡å‹å’Œ3Dé‡å»ºæ¨¡å‹ï¼Œé€šè¿‡å›¾åƒæ¡ä»¶çš„ç”Ÿæˆ3Dé«˜æ–¯æ–‘ç‚¹é‡å»ºï¼Œä»¥æå‡å•ä¸€RGBå›¾åƒåˆ°é€¼çœŸåŒ–èº«çš„ç”Ÿæˆè´¨é‡ã€‚</p></li><li><p>(2): æå‡ºäº†ä¸€ç§æ–°é¢–çš„é‡‡æ ·è½¨è¿¹ä¼˜åŒ–æ–¹æ³•ï¼Œç¡®ä¿å¤šè§†è§’ä¸€è‡´æ€§ï¼Œä»è€Œæ”¹å–„æœ€ç»ˆ3Dé‡å»ºç»“æœçš„è´¨é‡ã€‚</p></li><li><p>(3): å¼•å…¥äº†åŸºäºå¤§è§„æ¨¡æ•°æ®é¢„è®­ç»ƒçš„2Då¤šè§†è§’å…ˆéªŒæ¡ä»¶ï¼Œç”¨äºå¢å¼º3Dç”Ÿæˆæ¨¡å‹çš„é‡å»ºèƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†ä¸åŒæ•°æ®åˆ†å¸ƒå’Œå¯¹è±¡æ—¶è¡¨ç°å‡ºè‰²ã€‚</p></li></ul><ol><li>Conclusion:</li></ol><ul><li><p>(1): This paper introduces Human 3Diffusion, a significant advancement in creating realistic avatars from single RGB images, leveraging both 2D and 3D models effectively.</p></li><li><p>(2): Innovation point: The paper innovates by combining 2D multi-view diffusion models with a novel 3D Gaussian Splats reconstruction method, enhancing 3D consistency and fidelity in avatar generation. Performance: Demonstrates superior performance in both geometric accuracy and visual appearance compared to previous methods. Workload: The method introduces additional computational workload due to the integration of 3D reconstruction techniques alongside existing 2D models, but the results justify the computational cost.</p></li></ul><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-098d04fb98a7383be9fefedaf341e49d.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-4bb8393065d5933b2cfa0352a5506572.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-35bb47b846f5731cc9a4e3be005d1b01.jpg" align="middle"></details><h2 id="Instant-3D-Human-Avatar-Generation-using-Image-Diffusion-Models"><a href="#Instant-3D-Human-Avatar-Generation-using-Image-Diffusion-Models" class="headerlink" title="Instant 3D Human Avatar Generation using Image Diffusion Models"></a>Instant 3D Human Avatar Generation using Image Diffusion Models</h2><p><strong>Authors:Nikos Kolotouros, Thiemo Alldieck, Enric Corona, Eduard Gabriel Bazavan, Cristian Sminchisescu</strong></p><p>We present AvatarPopUp, a method for fast, high quality 3D human avatar generation from different input modalities, such as images and text prompts and with control over the generated pose and shape. The common theme is the use of diffusion-based image generation networks that are specialized for each particular task, followed by a 3D lifting network. We purposefully decouple the generation from the 3D modeling which allow us to leverage powerful image synthesis priors, trained on billions of text-image pairs. We fine-tune latent diffusion networks with additional image conditioning to solve tasks such as image generation and back-view prediction, and to support qualitatively different multiple 3D hypotheses. Our partial fine-tuning approach allows to adapt the networks for each task without inducing catastrophic forgetting. In our experiments, we demonstrate that our method produces accurate, high-quality 3D avatars with diverse appearance that respect the multimodal text, image, and body control signals. Our approach can produce a 3D model in as few as 2 seconds, a four orders of magnitude speedup w.r.t. the vast majority of existing methods, most of which solve only a subset of our tasks, and with fewer controls, thus enabling applications that require the controlled 3D generation of human avatars at scale. The project website can be found at <a target="_blank" rel="noopener" href="https://www.nikoskolot.com/avatarpopup/">https://www.nikoskolot.com/avatarpopup/</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.07516v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://www.nikoskolot.com/avatarpopup/">https://www.nikoskolot.com/avatarpopup/</a></p><p><strong>Summary</strong><br>AvatarPopUpæå‡ºäº†ä¸€ç§å¿«é€Ÿã€é«˜è´¨é‡çš„3Däººç±»åŒ–èº«ç”Ÿæˆæ–¹æ³•ï¼Œèƒ½ä»ä¸åŒè¾“å…¥æ¨¡å¼ï¼ˆå¦‚å›¾åƒå’Œæ–‡æœ¬æç¤ºï¼‰ç”Ÿæˆï¼Œå¹¶æ§åˆ¶ç”Ÿæˆçš„å§¿åŠ¿å’Œå½¢çŠ¶ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ä½¿ç”¨åŸºäºæ‰©æ•£çš„å›¾åƒç”Ÿæˆç½‘ç»œï¼Œé’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œä¸“é—¨ä¼˜åŒ–ã€‚</li><li>å¼•å…¥3Dæå‡ç½‘ç»œï¼Œå°†ç”Ÿæˆè¿‡ç¨‹ä¸3Då»ºæ¨¡åˆ†ç¦»ï¼Œåˆ©ç”¨æ•°åäº¿æ–‡æœ¬å›¾åƒå¯¹è®­ç»ƒçš„å¼ºå¤§å›¾åƒåˆæˆå…ˆéªŒçŸ¥è¯†ã€‚</li><li>é€šè¿‡éƒ¨åˆ†å¾®è°ƒæ‰©æ•£ç½‘ç»œå’Œé¢å¤–å›¾åƒè°ƒèŠ‚ï¼Œå®ç°å›¾åƒç”Ÿæˆå’ŒèƒŒé¢é¢„æµ‹ç­‰ä»»åŠ¡ã€‚</li><li>æ”¯æŒå¤šç§ä¸åŒçš„3Då‡è®¾ï¼Œä¿ç•™è´¨é‡å’Œå¤šæ¨¡æ€ä¿¡å·ã€‚</li><li>æ–¹æ³•èƒ½åœ¨çŸ­çŸ­2ç§’å†…ç”Ÿæˆ3Dæ¨¡å‹ï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•æé€Ÿäº†å››ä¸ªæ•°é‡çº§ã€‚</li><li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„3DåŒ–èº«å‡†ç¡®ä¸”é«˜è´¨é‡ï¼Œå¤–è§‚å¤šæ ·åŒ–ï¼Œå¹¶èƒ½æ§åˆ¶æ–‡æœ¬ã€å›¾åƒå’Œèº«ä½“ä¿¡å·ã€‚</li><li>æ”¯æŒå¤§è§„æ¨¡æ§åˆ¶çš„äººç±»åŒ–èº«ç”Ÿæˆï¼Œé€‚ç”¨äºå„ç§åº”ç”¨åœºæ™¯ã€‚</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: Instant 3D Human Avatar Generation using</p></li><li><p>Authors: Nikos Kolotouros, Thiemo Alldieck, Enric Corona, Eduard Gabriel Bazavan, and Cristian Sminchisescu</p></li><li><p>Affiliation: Google Research (è°·æ­Œç ”ç©¶)</p></li><li><p>Keywords: 3D human avatar generation, image diffusion models, multimodal generation, deep learning</p></li><li><p>Urls: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.07516v1">Paper</a>, Github: None</p></li><li><p>Summary:</p></li></ol><ul><li><p>(1): æœ¬æ–‡çš„ç ”ç©¶èƒŒæ™¯æ˜¯å¿«é€Ÿã€é«˜è´¨é‡çš„3Däººç±»åŒ–èº«ç”Ÿæˆï¼Œæ¶‰åŠå¤šæ¨¡æ€è¾“å…¥ï¼ˆå¦‚å›¾åƒå’Œæ–‡æœ¬æç¤ºï¼‰åŠå§¿åŠ¿ä¸å½¢çŠ¶çš„æ§åˆ¶ã€‚</p></li><li><p>(2): è¿‡å»çš„æ–¹æ³•ä¸»è¦å­˜åœ¨äºé€Ÿåº¦æ…¢ã€è´¨é‡ä¸é«˜ã€æ§åˆ¶ç²’åº¦è¾ƒä½ç­‰é—®é¢˜ã€‚æœ¬æ–‡çš„æ–¹æ³•åŠ¨æœºå……åˆ†ã€‚</p></li><li><p>(3): æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ä¸»è¦åŒ…æ‹¬ä½¿ç”¨åŸºäºæ‰©æ•£çš„å›¾åƒç”Ÿæˆç½‘ç»œä»¥åŠ3Dæå‡ç½‘ç»œï¼Œå¹¶ä¸”å°†ç”Ÿæˆä¸3Då»ºæ¨¡åˆ†ç¦»å¼€æ¥ï¼Œå¯ä»¥åˆ©ç”¨å¼ºå¤§çš„å›¾åƒåˆæˆå…ˆéªŒã€‚é€šè¿‡å±€éƒ¨å¾®è°ƒæ‰©æ•£ç½‘ç»œï¼ŒåŠ ä¸Šé¢å¤–çš„å›¾åƒæ¡ä»¶ï¼Œè§£å†³äº†å›¾åƒç”Ÿæˆå’ŒèƒŒæ™¯é¢„æµ‹ç­‰ä»»åŠ¡ã€‚</p></li><li><p>(4): æœ¬æ–‡æ–¹æ³•åœ¨ä»»åŠ¡ä¸Šè¾¾åˆ°äº†å¿«é€Ÿç”Ÿæˆé«˜è´¨é‡ã€å¤šæ ·åŒ–å¤–è§‚çš„3DåŒ–èº«ï¼ŒåŒæ—¶ä¿æŒäº†æ–‡æœ¬ã€å›¾åƒå’Œèº«ä½“æ§åˆ¶ä¿¡å·çš„å¤šæ¨¡æ€æ€§ã€‚å…¶æ€§èƒ½èƒ½å¤Ÿæ”¯æŒå…¶æ—¨åœ¨å®ç°çš„ç›®æ ‡ã€‚</p></li></ul><ol><li>Methods:</li></ol><ul><li><p>(1): æœ¬æ–‡çš„æ–¹æ³•æ—¨åœ¨å­¦ä¹ åŸºäºæ¡ä»¶ä¿¡å· c çš„çº¹ç†3Då½¢çŠ¶åˆ†å¸ƒ p(X|c)ï¼Œå…¶ä¸­ c è¢«åˆ†è§£ä¸ºå‰æ™¯å’ŒèƒŒæ™¯å›¾åƒè§‚å¯Ÿ If å’Œ Ibï¼Œä»¥åŠå…¶ä»–æ§åˆ¶ä¿¡å·ã€‚</p></li><li><p>(2): ä¸ºäº†ä»åˆ†å¸ƒä¸­ç”Ÿæˆæ ·æœ¬ï¼Œæˆ‘ä»¬ä½¿ç”¨ç¥–å…ˆé‡‡æ ·æ–¹æ³•ã€‚é¦–å…ˆæ ¹æ®æ¡ä»¶ä¿¡å· c ç”Ÿæˆå‰æ™¯å›¾åƒ Ifï¼Œæ¥ç€æ ¹æ® If å’Œ c ç”ŸæˆèƒŒæ™¯å›¾åƒ Ibï¼Œæœ€ååŸºäºæ•´ä½“ä¸Šä¸‹æ–‡ç”Ÿæˆ3Dé‡å»ºã€‚</p></li><li><p>(3): åœ¨å®é™…æ“ä½œä¸­ï¼Œä½¿ç”¨æ½œå˜æ‰©æ•£æ¨¡å‹å®ç° p(If|c) å’Œ p(Ib|If, c)ï¼Œè€Œ p(X|If, Ib, c) åˆ™ä½¿ç”¨å•æ¨¡æ€ç¥ç»éšå¼åœºç”Ÿæˆå™¨ã€‚å¯¹äºå•å›¾åƒ3Dé‡å»ºï¼Œæ¡ä»¶ä¿¡å· c æ˜¯ Ifï¼Œå› æ­¤å¯ä»¥çœç•¥ç¬¬ä¸€æ­¥ã€‚</p></li></ul><ol><li>Conclusion:</li></ol><ul><li><p>(1): The significance of this piece of work lies in its advancement of rapid, high-quality 3D human avatar generation through multimodal inputs and precise control over poses and shapes.</p></li><li><p>(2): Innovation point: The article innovates by separating image generation from 3D modeling using diffusion-based models, leveraging strong image synthesis priors, and achieving diverse 3D avatars with high efficiency.</p></li></ul><p>Performance: The method demonstrates fast generation of high-quality, diverse 3D avatars while maintaining multimodal signals from text, images, and body controls.</p><p>Workload: The approach addresses previous limitations in speed and quality by efficiently integrating multimodal inputs and enhancing control granularity, thus reducing the computational workload associated with 3D avatar generation.</p><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-53e914e263fac557c769b471b978934a.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-a6fc30677f4da16e92d0d3f3ca221eab.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-044126db3f1d005a12c07ede0d3c0aa0.jpg" align="middle"></details><h2 id="Multi-attribute-Auction-based-Resource-Allocation-for-Twins-Migration-in-Vehicular-Metaverses-A-GPT-based-DRL-Approach"><a href="#Multi-attribute-Auction-based-Resource-Allocation-for-Twins-Migration-in-Vehicular-Metaverses-A-GPT-based-DRL-Approach" class="headerlink" title="Multi-attribute Auction-based Resource Allocation for Twins Migration in   Vehicular Metaverses: A GPT-based DRL Approach"></a>Multi-attribute Auction-based Resource Allocation for Twins Migration in Vehicular Metaverses: A GPT-based DRL Approach</h2><p><strong>Authors:Yongju Tong, Junlong Chen, Minrui Xu, Jiawen Kang, Zehui Xiong, Dusit Niyato, Chau Yuen, Zhu Han</strong></p><p>Vehicular Metaverses are developed to enhance the modern automotive industry with an immersive and safe experience among connected vehicles and roadside infrastructures, e.g., RoadSide Units (RSUs). For seamless synchronization with virtual spaces, Vehicle Twins (VTs) are constructed as digital representations of physical entities. However, resource-intensive VTs updating and high mobility of vehicles require intensive computation, communication, and storage resources, especially for their migration among RSUs with limited coverages. To address these issues, we propose an attribute-aware auction-based mechanism to optimize resource allocation during VTs migration by considering both price and non-monetary attributes, e.g., location and reputation. In this mechanism, we propose a two-stage matching for vehicular users and Metaverse service providers in multi-attribute resource markets. First, the resource attributes matching algorithm obtains the resource attributes perfect matching, namely, buyers and sellers can participate in a double Dutch auction (DDA). Then, we train a DDA auctioneer using a generative pre-trained transformer (GPT)-based deep reinforcement learning (DRL) algorithm to adjust the auction clocks efficiently during the auction process. We compare the performance of social welfare and auction information exchange costs with state-of-the-art baselines under different settings. Simulation results show that our proposed GPT-based DRL auction schemes have better performance than others.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.05418v1">PDF</a> 16 pages, 6 figures, 3 tables</p><p>Vehicular Metaverses are developed to enhance the modern automotive industry with an immersive and safe experience among connected vehicles and roadside infrastructures, e.g., RoadSide Units (RSUs). For seamless synchronization with virtual spaces, Vehicle Twins (VTs) are constructed as digital representations of physical entities. However, resource-intensive VTs updating and high mobility of vehicles require intensive computation, communication, and storage resources, especially for their migration among RSUs with limited coverages. To address these issues, we propose an attribute-aware auction-based mechanism to optimize resource allocation during VTs migration by considering both price and non-monetary attributes, e.g., location and reputation. In this mechanism, we propose a two-stage matching for vehicular users and Metaverse service providers in multi-attribute resource markets. First, the resource attributes matching algorithm obtains the resource attributes perfect matching, namely, buyers and sellers can participate in a double Dutch auction (DDA). Then, we train a DDA auctioneer using a generative pre-trained transformer (GPT)-based deep reinforcement learning (DRL) algorithm to adjust the auction clocks efficiently during the auction process. We compare the performance of social welfare and auction information exchange costs with state-of-the-art baselines under different settings. Simulation results show that our proposed GPT-based DRL auction schemes have better performance than others.</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-90a795de2f09036800632e527abe26fd.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-19a2ab165aecd504845c925572391140.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-b134c3270d5107d23ca7c9bb13ae4c18.jpg" align="middle"></details><h2 id="STAR-Skeleton-aware-Text-based-4D-Avatar-Generation-with-In-Network-Motion-Retargeting"><a href="#STAR-Skeleton-aware-Text-based-4D-Avatar-Generation-with-In-Network-Motion-Retargeting" class="headerlink" title="STAR: Skeleton-aware Text-based 4D Avatar Generation with In-Network   Motion Retargeting"></a>STAR: Skeleton-aware Text-based 4D Avatar Generation with In-Network Motion Retargeting</h2><p><strong>Authors:Zenghao Chai, Chen Tang, Yongkang Wong, Mohan Kankanhalli</strong></p><p>The creation of 4D avatars (i.e., animated 3D avatars) from text description typically uses text-to-image (T2I) diffusion models to synthesize 3D avatars in the canonical space and subsequently applies animation with target motions. However, such an optimization-by-animation paradigm has several drawbacks. (1) For pose-agnostic optimization, the rendered images in canonical pose for naive Score Distillation Sampling (SDS) exhibit domain gap and cannot preserve view-consistency using only T2I priors, and (2) For post hoc animation, simply applying the source motions to target 3D avatars yields translation artifacts and misalignment. To address these issues, we propose Skeleton-aware Text-based 4D Avatar generation with in-network motion Retargeting (STAR). STAR considers the geometry and skeleton differences between the template mesh and target avatar, and corrects the mismatched source motion by resorting to the pretrained motion retargeting techniques. With the informatively retargeted and occlusion-aware skeleton, we embrace the skeleton-conditioned T2I and text-to-video (T2V) priors, and propose a hybrid SDS module to coherently provide multi-view and frame-consistent supervision signals. Hence, STAR can progressively optimize the geometry, texture, and motion in an end-to-end manner. The quantitative and qualitative experiments demonstrate our proposed STAR can synthesize high-quality 4D avatars with vivid animations that align well with the text description. Additional ablation studies shows the contributions of each component in STAR. The source code and demos are available at: \href{<a target="_blank" rel="noopener" href="https://star-avatar.github.io}{https://star-avatar.github.io}">https://star-avatar.github.io}{https://star-avatar.github.io}</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.04629v1">PDF</a> Tech report</p><p>The creation of 4D avatars (i.e., animated 3D avatars) from text description typically uses text-to-image (T2I) diffusion models to synthesize 3D avatars in the canonical space and subsequently applies animation with target motions. However, such an optimization-by-animation paradigm has several drawbacks. (1) For pose-agnostic optimization, the rendered images in canonical pose for naive Score Distillation Sampling (SDS) exhibit domain gap and cannot preserve view-consistency using only T2I priors, and (2) For post hoc animation, simply applying the source motions to target 3D avatars yields translation artifacts and misalignment. To address these issues, we propose Skeleton-aware Text-based 4D Avatar generation with in-network motion Retargeting (STAR). STAR considers the geometry and skeleton differences between the template mesh and target avatar, and corrects the mismatched source motion by resorting to the pretrained motion retargeting techniques. With the informatively retargeted and occlusion-aware skeleton, we embrace the skeleton-conditioned T2I and text-to-video (T2V) priors, and propose a hybrid SDS module to coherently provide multi-view and frame-consistent supervision signals. Hence, STAR can progressively optimize the geometry, texture, and motion in an end-to-end manner. The quantitative and qualitative experiments demonstrate our proposed STAR can synthesize high-quality 4D avatars with vivid animations that align well with the text description. Additional ablation studies shows the contributions of each component in STAR. The source code and demos are available at: \href{<a target="_blank" rel="noopener" href="https://star-avatar.github.io}{https://star-avatar.github.io}">https://star-avatar.github.io}{https://star-avatar.github.io}</a>.</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-3afa9e67f614d591989be2744ada9ff8.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-99ad3776d54c2d5b79964eb333ea879d.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-d423f555ca9632e58a48c373d35c07cb.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-eac38f57757596750c602ce3d36d327f.jpg" align="middle"></details><h2 id="A-Survey-on-3D-Human-Avatar-Modeling-â€”-From-Reconstruction-to-Generation"><a href="#A-Survey-on-3D-Human-Avatar-Modeling-â€”-From-Reconstruction-to-Generation" class="headerlink" title="A Survey on 3D Human Avatar Modeling â€” From Reconstruction to   Generation"></a>A Survey on 3D Human Avatar Modeling â€” From Reconstruction to Generation</h2><p><strong>Authors:Ruihe Wang, Yukang Cao, Kai Han, Kwan-Yee K. Wong</strong></p><p>3D modeling has long been an important area in computer vision and computer graphics. Recently, thanks to the breakthroughs in neural representations and generative models, we witnessed a rapid development of 3D modeling. 3D human modeling, lying at the core of many real-world applications, such as gaming and animation, has attracted significant attention. Over the past few years, a large body of work on creating 3D human avatars has been introduced, forming a new and abundant knowledge base for 3D human modeling. The scale of the literature makes it difficult for individuals to keep track of all the works. This survey aims to provide a comprehensive overview of these emerging techniques for 3D human avatar modeling, from both reconstruction and generation perspectives. Firstly, we review representative methods for 3D human reconstruction, including methods based on pixel-aligned implicit function, neural radiance field, and 3D Gaussian Splatting, etc. We then summarize representative methods for 3D human generation, especially those using large language models like CLIP, diffusion models, and various 3D representations, which demonstrate state-of-the-art performance. Finally, we discuss our reflection on existing methods and open challenges for 3D human avatar modeling, shedding light on future research.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.04253v1">PDF</a> 30 pages, 21 figures</p><p>3D modeling has long been an important area in computer vision and computer graphics. Recently, thanks to the breakthroughs in neural representations and generative models, we witnessed a rapid development of 3D modeling. 3D human modeling, lying at the core of many real-world applications, such as gaming and animation, has attracted significant attention. Over the past few years, a large body of work on creating 3D human avatars has been introduced, forming a new and abundant knowledge base for 3D human modeling. The scale of the literature makes it difficult for individuals to keep track of all the works. This survey aims to provide a comprehensive overview of these emerging techniques for 3D human avatar modeling, from both reconstruction and generation perspectives. Firstly, we review representative methods for 3D human reconstruction, including methods based on pixel-aligned implicit function, neural radiance field, and 3D Gaussian Splatting, etc. We then summarize representative methods for 3D human generation, especially those using large language models like CLIP, diffusion models, and various 3D representations, which demonstrate state-of-the-art performance. Finally, we discuss our reflection on existing methods and open challenges for 3D human avatar modeling, shedding light on future research.</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-189f13a886085b96b7aab578c707d2c9.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-258fb1920e8ece7e1b5a39ce9a8e24d5.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-72e63546703f4e6580e6e45c851ab7b3.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-7164ec75ca92b9dd7f6d6d67ae9924f2.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-f32bae00bb972a17d554c58569365817.jpg" align="middle"></details><h2 id="Representing-Animatable-Avatar-via-Factorized-Neural-Fields"><a href="#Representing-Animatable-Avatar-via-Factorized-Neural-Fields" class="headerlink" title="Representing Animatable Avatar via Factorized Neural Fields"></a>Representing Animatable Avatar via Factorized Neural Fields</h2><p><strong>Authors:Chunjin Song, Zhijie Wu, Bastian Wandt, Leonid Sigal, Helge Rhodin</strong></p><p>For reconstructing high-fidelity human 3D models from monocular videos, it is crucial to maintain consistent large-scale body shapes along with finely matched subtle wrinkles. This paper explores the observation that the per-frame rendering results can be factorized into a pose-independent component and a corresponding pose-dependent equivalent to facilitate frame consistency. Pose adaptive textures can be further improved by restricting frequency bands of these two components. In detail, pose-independent outputs are expected to be low-frequency, while highfrequency information is linked to pose-dependent factors. We achieve a coherent preservation of both coarse body contours across the entire input video and finegrained texture features that are time variant with a dual-branch network with distinct frequency components. The first branch takes coordinates in canonical space as input, while the second branch additionally considers features outputted by the first branch and pose information of each frame. Our network integrates the information predicted by both branches and utilizes volume rendering to generate photo-realistic 3D human images. Through experiments, we demonstrate that our network surpasses the neural radiance fields (NeRF) based state-of-the-art methods in preserving high-frequency details and ensuring consistent body contours.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.00637v1">PDF</a></p><p>For reconstructing high-fidelity human 3D models from monocular videos, it is crucial to maintain consistent large-scale body shapes along with finely matched subtle wrinkles. This paper explores the observation that the per-frame rendering results can be factorized into a pose-independent component and a corresponding pose-dependent equivalent to facilitate frame consistency. Pose adaptive textures can be further improved by restricting frequency bands of these two components. In detail, pose-independent outputs are expected to be low-frequency, while highfrequency information is linked to pose-dependent factors. We achieve a coherent preservation of both coarse body contours across the entire input video and finegrained texture features that are time variant with a dual-branch network with distinct frequency components. The first branch takes coordinates in canonical space as input, while the second branch additionally considers features outputted by the first branch and pose information of each frame. Our network integrates the information predicted by both branches and utilizes volume rendering to generate photo-realistic 3D human images. Through experiments, we demonstrate that our network surpasses the neural radiance fields (NeRF) based state-of-the-art methods in preserving high-frequency details and ensuring consistent body contours.</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-be445208f1ee2628483db32fdc93d722.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-3c15b019d856bd2f642fcf55e1d51564.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-d0b285fd3e4f7897ddc630c6547b8d53.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-282652b0f4632a60aba7736ffa4efcbf.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-d52191abc88dd83d7ef752e089b16d0f.jpg" align="middle"></details><h2 id="Stratified-Avatar-Generation-from-Sparse-Observations"><a href="#Stratified-Avatar-Generation-from-Sparse-Observations" class="headerlink" title="Stratified Avatar Generation from Sparse Observations"></a>Stratified Avatar Generation from Sparse Observations</h2><p><strong>Authors:Han Feng, Wenchao Ma, Quankai Gao, Xianwei Zheng, Nan Xue, Huijuan Xu</strong></p><p>Estimating 3D full-body avatars from AR/VR devices is essential for creating immersive experiences in AR/VR applications. This task is challenging due to the limited input from Head Mounted Devices, which capture only sparse observations from the head and hands. Predicting the full-body avatars, particularly the lower body, from these sparse observations presents significant difficulties. In this paper, we are inspired by the inherent property of the kinematic tree defined in the Skinned Multi-Person Linear (SMPL) model, where the upper body and lower body share only one common ancestor node, bringing the potential of decoupled reconstruction. We propose a stratified approach to decouple the conventional full-body avatar reconstruction pipeline into two stages, with the reconstruction of the upper body first and a subsequent reconstruction of the lower body conditioned on the previous stage. To implement this straightforward idea, we leverage the latent diffusion model as a powerful probabilistic generator, and train it to follow the latent distribution of decoupled motions explored by a VQ-VAE encoder-decoder model. Extensive experiments on AMASS mocap dataset demonstrate our state-of-the-art performance in the reconstruction of full-body motions.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.20786v2">PDF</a> Accepted by CVPR 2024 (Oral)</p><p>Estimating 3D full-body avatars from AR/VR devices is essential for creating immersive experiences in AR/VR applications. This task is challenging due to the limited input from Head Mounted Devices, which capture only sparse observations from the head and hands. Predicting the full-body avatars, particularly the lower body, from these sparse observations presents significant difficulties. In this paper, we are inspired by the inherent property of the kinematic tree defined in the Skinned Multi-Person Linear (SMPL) model, where the upper body and lower body share only one common ancestor node, bringing the potential of decoupled reconstruction. We propose a stratified approach to decouple the conventional full-body avatar reconstruction pipeline into two stages, with the reconstruction of the upper body first and a subsequent reconstruction of the lower body conditioned on the previous stage. To implement this straightforward idea, we leverage the latent diffusion model as a powerful probabilistic generator, and train it to follow the latent distribution of decoupled motions explored by a VQ-VAE encoder-decoder model. Extensive experiments on AMASS mocap dataset demonstrate our state-of-the-art performance in the reconstruction of full-body motions.</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-aa4ca91ff252ea86d12ad5871b7009af.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-24b9b86d9b0d5696c1a0c735c8924fbe.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-c8a2b47478ab54fd70177fe7d9980759.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-b52588ad35259227918390e2cf8cb5b2.jpg" align="middle"></details><h2 id="NPGA-Neural-Parametric-Gaussian-Avatars"><a href="#NPGA-Neural-Parametric-Gaussian-Avatars" class="headerlink" title="NPGA: Neural Parametric Gaussian Avatars"></a>NPGA: Neural Parametric Gaussian Avatars</h2><p><strong>Authors:Simon Giebenhain, Tobias Kirschstein, Martin RÃ¼nz, Lourdes Agapito, Matthias NieÃŸner</strong></p><p>The creation of high-fidelity, digital versions of human heads is an important stepping stone in the process of further integrating virtual components into our everyday lives. Constructing such avatars is a challenging research problem, due to a high demand for photo-realism and real-time rendering performance. In this work, we propose Neural Parametric Gaussian Avatars (NPGA), a data-driven approach to create high-fidelity, controllable avatars from multi-view video recordings. We build our method around 3D Gaussian Splatting for its highly efficient rendering and to inherit the topological flexibility of point clouds. In contrast to previous work, we condition our avatarsâ€™ dynamics on the rich expression space of neural parametric head models (NPHM), instead of mesh-based 3DMMs. To this end, we distill the backward deformation field of our underlying NPHM into forward deformations which are compatible with rasterization-based rendering. All remaining fine-scale, expression-dependent details are learned from the multi-view videos. To increase the representational capacity of our avatars, we augment the canonical Gaussian point cloud using per-primitive latent features which govern its dynamic behavior. To regularize this increased dynamic expressivity, we propose Laplacian terms on the latent features and predicted dynamics. We evaluate our method on the public NeRSemble dataset, demonstrating that NPGA significantly outperforms the previous state-of-the-art avatars on the self-reenactment task by 2.6 PSNR. Furthermore, we demonstrate accurate animation capabilities from real-world monocular videos.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.19331v1">PDF</a> Project Page: see <a target="_blank" rel="noopener" href="https://simongiebenhain.github.io/NPGA/">https://simongiebenhain.github.io/NPGA/</a> ; Youtube Video: see <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=NGRxAYbIkus">https://www.youtube.com/watch?v=NGRxAYbIkus</a></p><p>The creation of high-fidelity, digital versions of human heads is an important stepping stone in the process of further integrating virtual components into our everyday lives. Constructing such avatars is a challenging research problem, due to a high demand for photo-realism and real-time rendering performance. In this work, we propose Neural Parametric Gaussian Avatars (NPGA), a data-driven approach to create high-fidelity, controllable avatars from multi-view video recordings. We build our method around 3D Gaussian Splatting for its highly efficient rendering and to inherit the topological flexibility of point clouds. In contrast to previous work, we condition our avatarsâ€™ dynamics on the rich expression space of neural parametric head models (NPHM), instead of mesh-based 3DMMs. To this end, we distill the backward deformation field of our underlying NPHM into forward deformations which are compatible with rasterization-based rendering. All remaining fine-scale, expression-dependent details are learned from the multi-view videos. To increase the representational capacity of our avatars, we augment the canonical Gaussian point cloud using per-primitive latent features which govern its dynamic behavior. To regularize this increased dynamic expressivity, we propose Laplacian terms on the latent features and predicted dynamics. We evaluate our method on the public NeRSemble dataset, demonstrating that NPGA significantly outperforms the previous state-of-the-art avatars on the self-reenactment task by 2.6 PSNR. Furthermore, we demonstrate accurate animation capabilities from real-world monocular videos.</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-e1ebdb40880659f3f276da0e13675a00.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-fc4ed51dc083b8b6a51414491a73d806.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-f81503095d5f9b2100c356802a0daa7c.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-3db991818ec4bced433235a789fd7993.jpg" align="middle"></details><h2 id="E-3-Gen-Efficient-Expressive-and-Editable-Avatars-Generation"><a href="#E-3-Gen-Efficient-Expressive-and-Editable-Avatars-Generation" class="headerlink" title="$E^{3}$Gen: Efficient, Expressive and Editable Avatars Generation"></a>$E^{3}$Gen: Efficient, Expressive and Editable Avatars Generation</h2><p><strong>Authors:Weitian Zhang, Yichao Yan, Yunhui Liu, Xingdong Sheng, Xiaokang Yang</strong></p><p>This paper aims to introduce 3D Gaussian for efficient, expressive, and editable digital avatar generation. This task faces two major challenges: (1) The unstructured nature of 3D Gaussian makes it incompatible with current generation pipelines; (2) the expressive animation of 3D Gaussian in a generative setting that involves training with multiple subjects remains unexplored. In this paper, we propose a novel avatar generation method named $E^3$Gen, to effectively address these challenges. First, we propose a novel generative UV features plane representation that encodes unstructured 3D Gaussian onto a structured 2D UV space defined by the SMPL-X parametric model. This novel representation not only preserves the representation ability of the original 3D Gaussian but also introduces a shared structure among subjects to enable generative learning of the diffusion model. To tackle the second challenge, we propose a part-aware deformation module to achieve robust and accurate full-body expressive pose control. Extensive experiments demonstrate that our method achieves superior performance in avatar generation and enables expressive full-body pose control and editing. Our project page is <a target="_blank" rel="noopener" href="https://olivia23333.github.io/E3Gen">https://olivia23333.github.io/E3Gen</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.19203v2">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://olivia23333.github.io/E3Gen">https://olivia23333.github.io/E3Gen</a></p><p>This paper aims to introduce 3D Gaussian for efficient, expressive, and editable digital avatar generation. This task faces two major challenges: (1) The unstructured nature of 3D Gaussian makes it incompatible with current generation pipelines; (2) the expressive animation of 3D Gaussian in a generative setting that involves training with multiple subjects remains unexplored. In this paper, we propose a novel avatar generation method named $E^3$Gen, to effectively address these challenges. First, we propose a novel generative UV features plane representation that encodes unstructured 3D Gaussian onto a structured 2D UV space defined by the SMPL-X parametric model. This novel representation not only preserves the representation ability of the original 3D Gaussian but also introduces a shared structure among subjects to enable generative learning of the diffusion model. To tackle the second challenge, we propose a part-aware deformation module to achieve robust and accurate full-body expressive pose control. Extensive experiments demonstrate that our method achieves superior performance in avatar generation and enables expressive full-body pose control and editing. Our project page is <a target="_blank" rel="noopener" href="https://olivia23333.github.io/E3Gen">https://olivia23333.github.io/E3Gen</a>.</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-64c32658bd72d754d038262a495e2f0a.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-f28144d42c4a6824d648e9585b86557d.jpg" align="middle"></details></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>æ–‡ç« ä½œè€…:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>æ–‡ç« é“¾æ¥:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io/2024/06/13/Paper/2024-06-13/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">https://kedreamix.github.io/2024/06/13/Paper/2024-06-13/å…ƒå®‡å®™_è™šæ‹Ÿäºº/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>ç‰ˆæƒå£°æ˜:</span> <span class="post-copyright-info">æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ«å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨ <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥è‡ª <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>ï¼</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">å…ƒå®‡å®™/è™šæ‹Ÿäºº</a></div><div class="post_share"><div class="social-share" data-image="https://picx.zhimg.com/v2-f32bae00bb972a17d554c58569365817.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>èµåŠ©</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/06/13/Paper/2024-06-13/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-64c781f372976074f0321aafd21d7539.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">ä¸Šä¸€ç¯‡</div><div class="prev_info">Diffusion Models</div></div></a></div><div class="next-post pull-right"><a href="/2024/06/12/Paper/2024-06-12/Talking%20Head%20Generation/" title="Talking Head Generation"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-ed17fbefb852c404fb59baa6acfabeb2.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">ä¸‹ä¸€ç¯‡</div><div class="next_info">Talking Head Generation</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>ç›¸å…³æ¨è</span></div><div class="relatedPosts-list"><div><a href="/2024/01/24/Paper/2024-01-24/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="å…ƒå®‡å®™&#x2F;è™šæ‹Ÿäºº"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-0f13a2b60bef4c886a3317754c99b456.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-24</div><div class="title">å…ƒå®‡å®™&#x2F;è™šæ‹Ÿäºº</div></div></a></div><div><a href="/2024/04/06/Paper/2024-04-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="å…ƒå®‡å®™&#x2F;è™šæ‹Ÿäºº"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-4f97970c093585e18e2db42fb96a6b75.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-06</div><div class="title">å…ƒå®‡å®™&#x2F;è™šæ‹Ÿäºº</div></div></a></div><div><a href="/2024/04/01/Paper/2024-04-01/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="å…ƒå®‡å®™&#x2F;è™šæ‹Ÿäºº"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-4f2a829065c463be027e4b423c4e43c8.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-01</div><div class="title">å…ƒå®‡å®™&#x2F;è™šæ‹Ÿäºº</div></div></a></div><div><a href="/2024/05/02/Paper/2024-05-02/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="å…ƒå®‡å®™&#x2F;è™šæ‹Ÿäºº"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-37516691b13dabbabb3b74ea46b402d8.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-02</div><div class="title">å…ƒå®‡å®™&#x2F;è™šæ‹Ÿäºº</div></div></a></div><div><a href="/2024/05/06/Paper/2024-05-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="å…ƒå®‡å®™&#x2F;è™šæ‹Ÿäºº"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-65e21e3a0a320adc36f81e6bfc7c5739.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-06</div><div class="title">å…ƒå®‡å®™&#x2F;è™šæ‹Ÿäºº</div></div></a></div><div><a href="/2024/05/22/Paper/2024-05-22/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="å…ƒå®‡å®™&#x2F;è™šæ‹Ÿäºº"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-b4943be56cf94e02149b50eec541d27f.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-22</div><div class="title">å…ƒå®‡å®™&#x2F;è™šæ‹Ÿäºº</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>ç›®å½•</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-06-13-%E6%9B%B4%E6%96%B0"><span class="toc-text">2024-06-13 æ›´æ–°</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Human-3Diffusion-Realistic-Avatar-Creation-via-Explicit-3D-Consistent-Diffusion-Models"><span class="toc-text">Human 3Diffusion: Realistic Avatar Creation via Explicit 3D Consistent Diffusion Models</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Instant-3D-Human-Avatar-Generation-using-Image-Diffusion-Models"><span class="toc-text">Instant 3D Human Avatar Generation using Image Diffusion Models</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Multi-attribute-Auction-based-Resource-Allocation-for-Twins-Migration-in-Vehicular-Metaverses-A-GPT-based-DRL-Approach"><span class="toc-text">Multi-attribute Auction-based Resource Allocation for Twins Migration in Vehicular Metaverses: A GPT-based DRL Approach</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#STAR-Skeleton-aware-Text-based-4D-Avatar-Generation-with-In-Network-Motion-Retargeting"><span class="toc-text">STAR: Skeleton-aware Text-based 4D Avatar Generation with In-Network Motion Retargeting</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#A-Survey-on-3D-Human-Avatar-Modeling-%E2%80%94-From-Reconstruction-to-Generation"><span class="toc-text">A Survey on 3D Human Avatar Modeling â€” From Reconstruction to Generation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Representing-Animatable-Avatar-via-Factorized-Neural-Fields"><span class="toc-text">Representing Animatable Avatar via Factorized Neural Fields</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Stratified-Avatar-Generation-from-Sparse-Observations"><span class="toc-text">Stratified Avatar Generation from Sparse Observations</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#NPGA-Neural-Parametric-Gaussian-Avatars"><span class="toc-text">NPGA: Neural Parametric Gaussian Avatars</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#E-3-Gen-Efficient-Expressive-and-Editable-Avatars-Generation"><span class="toc-text">$E^{3}$Gen: Efficient, Expressive and Editable Avatars Generation</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image:url('https://picx.zhimg.com/v2-f32bae00bb972a17d554c58569365817.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>æ¡†æ¶</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>ä¸»é¢˜</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="é˜…è¯»æ¨¡å¼"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="ç®€ç¹è½¬æ¢">ç°¡</button><button id="darkmode" type="button" title="æµ…è‰²å’Œæ·±è‰²æ¨¡å¼è½¬æ¢"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="å•æ å’ŒåŒæ åˆ‡æ¢"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="è®¾ç½®"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="ç›®å½•"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="å›åˆ°é¡¶éƒ¨"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js").then((()=>{pangu.autoSpacingPage()}))}function panguInit(){panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.1},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script></div><script id="canvas_nest" defer color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">æœç´¢</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>æ•°æ®åº“åŠ è½½ä¸­</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="æœç´¢æ–‡ç« " type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><script data-pjax>function butterfly_swiper_injector_config(){var a=document.getElementById("recent-posts");console.log("å·²æŒ‚è½½butterfly_swiper"),a.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/25/Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="2024/01/25/Paper/3DGS Survey/" alt="">3DGSç»¼è¿°ä»¥åŠå¯¹3DGSçš„ç†è§£ï¼šA Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGSç»¼è¿°ä»¥åŠå¯¹3DGSçš„ç†è§£ï¼šA Survey on 3D Gaussian Splatting ä»Šå¤©æƒ³ä»‹ç»çš„æ˜¯`ZJU`å¸¦æ¥çš„`3DGS`çš„é¦–ç¯‡ç»¼è¿°`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="2024/01/25/Paper/3DGS Survey/" alt="">è¯¦æƒ…   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/12/CUDA/Pytorch+cppcuda extension å­¦ä¹ /" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="2023/12/12/CUDA/Pytorch+cppcuda extension å­¦ä¹ /" alt="">CUDAç¼–ç¨‹å­¦ä¹ ï¼šè‡ªå®šä¹‰Pytorch+cpp/cuda extension</a><div class="blog-slider__text">è™½ç„¶è¯´PyTorchæä¾›äº†ä¸°å¯Œçš„ä¸ç¥ç»ç½‘ç»œã€å¼ é‡ä»£æ•°ã€æ•°æ®å¤„ç†ç­‰ç›¸å…³çš„æ“ä½œï¼Œä½†æ˜¯æœ‰æ—¶å€™ä½ å¯èƒ½éœ€è¦**æ›´å®šåˆ¶åŒ–çš„æ“ä½œ**ï¼Œæ¯”å¦‚ä½¿ç”¨è®ºæ–‡ä¸­çš„æ–°å‹æ¿€æ´»å‡½æ•°ï¼Œæˆ–è€…å®ç°ä½œä¸ºç ”ç©¶ä¸€éƒ¨åˆ†å¼€å‘çš„æ“ä½œã€‚åœ¨PyTorchä¸­ï¼Œæœ€ç®€å•çš„é›†æˆè‡ªå®šä¹‰æ“ä½œçš„æ–¹å¼æ˜¯åœ¨Pythonä¸­ç¼–å†™ï¼Œé€šè¿‡æ‰©å±•Functionå’ŒModuleæ¥å®ç°ï¼Œ</div><a class="blog-slider__button" href="2023/12/12/CUDA/Pytorch+cppcuda extension å­¦ä¹ /" alt="">è¯¦æƒ…   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker/" alt="">æ•°å­—äººå¯¹è¯ç³»ç»Ÿ - Linly-Talker â€”â€” â€œæ•°å­—äººäº¤äº’ï¼Œä¸è™šæ‹Ÿçš„è‡ªå·±äº’åŠ¨â€</a><div class="blog-slider__text">Linly-Talkeræ˜¯ä¸€ä¸ªå°†å¤§å‹è¯­è¨€æ¨¡å‹ä¸è§†è§‰æ¨¡å‹ç›¸ç»“åˆçš„æ™ºèƒ½AIç³»ç»Ÿ,åˆ›å»ºäº†ä¸€ç§å…¨æ–°çš„äººæœºäº¤äº’æ–¹å¼ã€‚å®ƒé›†æˆäº†å„ç§æŠ€æœ¯,ä¾‹å¦‚Whisperã€Linlyã€å¾®è½¯è¯­éŸ³æœåŠ¡å’ŒSadTalkerä¼šè¯´è¯çš„ç”Ÿæˆç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿéƒ¨ç½²åœ¨Gradioä¸Š,å…è®¸ç”¨æˆ·é€šè¿‡æä¾›å›¾åƒä¸AIåŠ©æ‰‹è¿›è¡Œäº¤è°ˆã€‚ç”¨æˆ·å¯ä»¥æ ¹æ®è‡ªå·±çš„å–œå¥½è¿›è¡Œè‡ªç”±çš„å¯¹è¯æˆ–å†…å®¹ç”Ÿæˆã€‚</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker/" alt="">è¯¦æƒ…   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">æ•°å­—äººå¯¹è¯ç³»ç»Ÿ - Linly-Talker â€”â€” â€œæ•°å­—äººäº¤äº’ï¼Œä¸è™šæ‹Ÿçš„è‡ªå·±äº’åŠ¨â€</a><div class="blog-slider__text">Linly-Talkeræ˜¯ä¸€ä¸ªå°†å¤§å‹è¯­è¨€æ¨¡å‹ä¸è§†è§‰æ¨¡å‹ç›¸ç»“åˆçš„æ™ºèƒ½AIç³»ç»Ÿ,åˆ›å»ºäº†ä¸€ç§å…¨æ–°çš„äººæœºäº¤äº’æ–¹å¼ã€‚å®ƒé›†æˆäº†å„ç§æŠ€æœ¯,ä¾‹å¦‚Whisperã€Linlyã€å¾®è½¯è¯­éŸ³æœåŠ¡å’ŒSadTalkerä¼šè¯´è¯çš„ç”Ÿæˆç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿéƒ¨ç½²åœ¨Gradioä¸Š,å…è®¸ç”¨æˆ·é€šè¿‡æä¾›å›¾åƒä¸AIåŠ©æ‰‹è¿›è¡Œäº¤è°ˆã€‚ç”¨æˆ·å¯ä»¥æ ¹æ®è‡ªå·±çš„å–œå¥½è¿›è¡Œè‡ªç”±çš„å¯¹è¯æˆ–å†…å®¹ç”Ÿæˆã€‚</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">è¯¦æƒ…   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">è¶…èµçš„æ•°å­—äººç”ŸæˆçŸ¥è¯†åº“  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">è¶…èµçš„æ•°å­—äººç”ŸæˆçŸ¥è¯†åº“  Awesome-Talking-Head-Synthesisï¼Œ è¿™ä»½èµ„æºåº“æ•´ç†äº†ä¸ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ(GAN)å’Œç¥ç»è¾å°„åœº(NeRF)ç›¸å…³çš„è®ºæ–‡ã€ä»£ç å’Œèµ„æº,é‡ç‚¹å…³æ³¨åŸºäºå›¾åƒå’ŒéŸ³é¢‘çš„è™šæ‹Ÿè®²è¯å¤´åˆæˆè®ºæ–‡åŠå·²å‘å¸ƒä»£ç ã€‚å¦‚æœæ‚¨è§‰å¾—è¿™ä¸ªä»“åº“æœ‰ç”¨,è¯·starâ­æ”¯æŒ!</div><a class="blog-slider__button" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">è¯¦æƒ…   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/17/Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023/12/17/Project/ChatPaperFree/" alt="">ChatPaperFree GeminiProï¼ˆä¸€åˆ†é’Ÿè¯»è®ºæ–‡ï¼‰</a><div class="blog-slider__text">ChatPaperFreeæ˜¯ä¸€ä¸ªåŸºäºChatGPTçš„è‡ªåŠ¨è®ºæ–‡æ‘˜è¦ç”Ÿæˆå™¨ï¼Œåœ¨ChatPaperçš„åŸºç¡€ä¸Šè¿›è¡Œçš„æ›´æ–°ï¼Œé‡‡ç”¨äº†æœ€è¿‘ç”±Googleå¼€æºçš„Gemini Proå¤§æ¨¡å‹ã€‚ç›®å‰,æˆ‘ä»¬èƒ½å¤Ÿå¯¹ç”¨æˆ·è¾“å…¥çš„è®ºæ–‡è¿›è¡Œè‡ªåŠ¨æ€»ç»“ã€‚æœªæ¥,æˆ‘è¿˜è®¡åˆ’åŠ å…¥å¯¹è®ºæ–‡å›¾ç‰‡/è¡¨æ ¼/å…¬å¼çš„è¯†åˆ« extraction,ä»è€Œç”Ÿæˆæ›´å…¨é¢è€Œæ˜“è¯»çš„æ€»ç»“ã€‚</div><a class="blog-slider__button" href="2023/12/17/Project/ChatPaperFree/" alt="">è¯¦æƒ…   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script></body></html>