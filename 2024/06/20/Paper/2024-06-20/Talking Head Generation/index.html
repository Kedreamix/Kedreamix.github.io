<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>Talking Head Generation | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-06-20  Talk With Human-like Agents Empathetic Dialogue Through Perceptible   Acoustic Reception and Reaction"><meta property="og:type" content="article"><meta property="og:title" content="Talking Head Generation"><meta property="og:url" content="https://kedreamix.github.io/2024/06/20/Paper/2024-06-20/Talking%20Head%20Generation/index.html"><meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World"><meta property="og:description" content="Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-06-20  Talk With Human-like Agents Empathetic Dialogue Through Perceptible   Acoustic Reception and Reaction"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://picx.zhimg.com/v2-de6a5bba250b1dcfc57b3ad385bdae26.jpg"><meta property="article:published_time" content="2024-06-20T13:10:31.000Z"><meta property="article:modified_time" content="2024-06-20T14:51:08.969Z"><meta property="article:author" content="Kedreamix"><meta property="article:tag" content="Talking Head Generation"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://picx.zhimg.com/v2-de6a5bba250b1dcfc57b3ad385bdae26.jpg"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/2024/06/20/Paper/2024-06-20/Talking%20Head%20Generation/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-74LZ5BEQQ1")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!0,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"简"},noticeOutdate:{limitDay:500,position:"top",messagePrev:"It has been",messageNext:"days since the last update, the content of the article may be outdated."},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:200},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!0,post:!0},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!0,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"Talking Head Generation",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-06-20 22:51:08"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach((e=>{n.setAttribute(e,t[e])})),document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)})),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=24?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 7.0.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml"></head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{const d=document.getElementById("loading-box"),e=document.body,o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l(),window.addEventListener("load",(()=>{o()}))})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">269</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url('https://picx.zhimg.com/v2-de6a5bba250b1dcfc57b3ad385bdae26.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Talking Head Generation</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-06-20T13:10:31.000Z" title="发表于 2024-06-20 21:10:31">2024-06-20</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-06-20T14:51:08.969Z" title="更新于 2024-06-20 22:51:08">2024-06-20</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">6.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>20分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="Talking Head Generation"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-06-20-更新"><a href="#2024-06-20-更新" class="headerlink" title="2024-06-20 更新"></a>2024-06-20 更新</h1><h2 id="Talk-With-Human-like-Agents-Empathetic-Dialogue-Through-Perceptible-Acoustic-Reception-and-Reaction"><a href="#Talk-With-Human-like-Agents-Empathetic-Dialogue-Through-Perceptible-Acoustic-Reception-and-Reaction" class="headerlink" title="Talk With Human-like Agents: Empathetic Dialogue Through Perceptible   Acoustic Reception and Reaction"></a>Talk With Human-like Agents: Empathetic Dialogue Through Perceptible Acoustic Reception and Reaction</h2><p><strong>Authors:Haoqiu Yan, Yongxin Zhu, Kai Zheng, Bing Liu, Haoyu Cao, Deqiang Jiang, Linli Xu</strong></p><p>Large Language Model (LLM)-enhanced agents become increasingly prevalent in Human-AI communication, offering vast potential from entertainment to professional domains. However, current multi-modal dialogue systems overlook the acoustic information present in speech, which is crucial for understanding human communication nuances. This oversight can lead to misinterpretations of speakers’ intentions, resulting in inconsistent or even contradictory responses within dialogues. To bridge this gap, in this paper, we propose PerceptiveAgent, an empathetic multi-modal dialogue system designed to discern deeper or more subtle meanings beyond the literal interpretations of words through the integration of speech modality perception. Employing LLMs as a cognitive core, PerceptiveAgent perceives acoustic information from input speech and generates empathetic responses based on speaking styles described in natural language. Experimental results indicate that PerceptiveAgent excels in contextual understanding by accurately discerning the speakers’ true intentions in scenarios where the linguistic meaning is either contrary to or inconsistent with the speaker’s true feelings, producing more nuanced and expressive spoken dialogues. Code is publicly available at: \url{<a target="_blank" rel="noopener" href="https://github.com/Haoqiu-Yan/PerceptiveAgent}">https://github.com/Haoqiu-Yan/PerceptiveAgent}</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.12707v1">PDF</a> 9 pages, 3 figures, ACL24 accepted</p><p><strong>Summary</strong><br>大型语言模型增强的代理在人工智能与人类交流中日益普及，提供从娱乐到专业领域的广泛潜力，但当前的多模态对话系统忽视了语音中的声学信息，这对理解人类交流的微妙差异至关重要。</p><p><strong>Key Takeaways</strong></p><ul><li>大型语言模型（LLM）增强的代理在人工智能与人类交流中应用广泛。</li><li>目前的多模态对话系统忽视了语音中的声学信息。</li><li>声学信息对于理解人类交流的微妙差异至关重要。</li><li>这种忽视可能导致对说话者意图的误解。</li><li>提出了PerceptiveAgent，旨在通过集成语音模态感知来解决这一问题。</li><li>PerceptiveAgent利用LLM作为认知核心，能够准确识别说话者真实意图。</li><li>实验结果显示，PerceptiveAgent在理解上下文和表达细微情感方面表现出色。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>好的，我将根据您提供的摘要和介绍，总结这篇论文的关键信息和核心内容。下面是按照要求的格式输出的答案：</p><ol><li><p>标题：感知代理：通过感知理解人类对话中的同情心（Talk With Human-like Agents: Empathetic Dialogue Through Perceptible）</p></li><li><p>作者：Haoqiu Yan（颜浩秋）, Yongxin Zhu（朱永新）, Kai Zheng（郑凯）, Bing Liu（刘斌）, Haoyu Cao（曹浩宇）, Deqiang Jiang（姜德强）, Linli Xu（徐林利）等。</p></li><li><p>所属机构：中国科学技术大学计算机科学和技术学院、数据科学学院，以及与腾讯优图实验室合作。</p></li><li><p>关键词：大型语言模型、感知代理、人工智能对话系统、多模态对话系统、语音感知、情感计算。</p></li><li><p>Urls：论文链接暂未提供；代码仓库链接：<a target="_blank" rel="noopener" href="https://github.com/Haoqiu-Yan/PerceptiveAgent">Github链接</a>（如实际有公开代码的话，请替换为实际链接）。</p></li><li><p>摘要：</p><ul><li><p>(1)研究背景：随着人工智能对话系统的普及，人们越来越期待系统能够理解和回应人类的情感和意图。然而，现有的多模态对话系统往往忽略了语音中的声学信息，这对于理解人类沟通的细微差别至关重要。本文旨在通过整合语音感知来弥补这一差距，提出一个能够感知更深层含义的多模态对话系统。</p></li><li><p>(2)过去的方法与问题：现有的对话系统主要依赖文本交互，忽略了语音中的声学信息，这可能导致对说话人意图的误解。因此，需要一种新的方法来解决这一问题。</p></li><li><p>(3)研究方法：本文提出了PerceptiveAgent，一个以大型语言模型为核心的多模态对话系统。该系统能够感知输入语音的声学信息，并通过自然语言描述生成基于说话风格的同情回应。实验结果表明，PerceptiveAgent在理解语境和准确识别说话人的真实意图方面表现出色，特别是在语言意义与说话人的真实感受相悖或不一致的情况下。</p></li><li><p>(4)任务与性能：PerceptiveAgent在多模态对话任务上进行了测试，并通过准确识别说话人的意图和情感，生成更富有同情心的回应，证明了其有效性和性能。这些结果支持了该方法的目标，即提高对话系统的情感智能和语境理解能力。</p></li></ul></li></ol><p>希望以上总结符合您的要求！</p><ol><li>方法介绍：</li></ol><p>这篇论文的核心方法是设计一个能够感知语音情感和意图的多模态对话系统。这一方法主要包括以下几个步骤：</p><ul><li>(1)语音感知：通过语音捕获模型（Speech Caption Model）捕捉语音中的声学信息，并将其转化为文本描述。这一模型通过编码语音输入并结合预训练的语言模型生成描述说话风格的文本。这是理解说话人意图的关键步骤。此外，论文还通过微调策略优化该模型，使其更好地适应多模态嵌入对齐和指令调节任务。</li><li>(2)意图辨识与理解：利用上一步骤生成的文本描述，结合大型语言模型（LLM），感知对话语境并理解说话人的真实意图。这一过程依赖于大型语言模型的上下文理解能力。设计好的提示可以更有效地利用语言模型的这一功能。这是多模态对话系统的核心环节之一。</li><li>(3)表达性语音合成：利用多说话人多属性合成器（MSMA-Synthesizer）合成富有同情心的音频响应。该合成器根据对话内容和风格描述生成语音响应，实现了语音的精细控制。通过引入多种韵律属性，如音调、速度和能量等，合成器能够生成更自然的语音响应。这一步是多模态对话系统的最终输出环节，使得系统能够模拟人类的情感表达和交流方式。论文中的实验证明了该系统的有效性和性能。</li></ul><p>总的来说，该方法通过整合语音感知和大型语言模型技术，实现了多模态对话系统的情感智能和语境理解能力提升，为人工智能对话系统的进一步发展提供了新的思路和方法。</p><p>结论：</p><p>（1）这篇论文的研究意义在于提出了一种感知代理（PerceptiveAgent）的多模态对话系统，该系统能够感知语音中的声学信息并结合大型语言模型（LLM）来理解和回应人类的情感和意图。这项研究推动了人工智能对话系统在情感智能和语境理解能力方面的进展，有助于提高人工智能系统的用户交互体验，使机器更好地理解和模拟人类的情感表达和交流方式。此外，该研究在理解和准确识别说话人意图方面取得了显著成果，特别是在语言意义与说话人的真实感受相悖或不一致的情况下。这些成果对于开发更智能、更人性化的对话系统具有重要的应用价值。</p><p>（2）创新点：该论文的创新之处在于整合了语音感知和大型语言模型技术，提出了一个能够感知语音情感和意图的多模态对话系统。该系统通过捕捉语音中的声学信息，结合文本描述和大型语言模型来生成基于说话风格的同情回应。这一创新方法提高了对话系统的情感智能和语境理解能力。<br>性能：实验结果表明，PerceptiveAgent在多模态对话任务上表现出良好的性能。通过准确识别说话人的意图和情感，该系统能够生成更富有同情心的回应。这些结果支持了该方法的目标，即提高对话系统的情感智能和语境理解能力。<br>工作量：该论文在构建PerceptiveAgent系统方面投入了大量的工作，包括设计语音感知模型、意图辨识与理解模块以及表达性语音合成器等。此外，论文还进行了大量的实验验证和性能评估，证明了该系统的有效性和性能。然而，论文未提及代码仓库链接的实际可用性，这可能是一个潜在的工作不足。总体而言，该论文在构建多模态对话系统方面取得了重要的进展和成果。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-6d6ed15644b9de007c349ee10520a26d.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-de6a5bba250b1dcfc57b3ad385bdae26.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-a9d446b3eef7d75be3e920901f7d974e.jpg" align="middle"></details><h2 id="NLDF-Neural-Light-Dynamic-Fields-for-Efficient-3D-Talking-Head-Generation"><a href="#NLDF-Neural-Light-Dynamic-Fields-for-Efficient-3D-Talking-Head-Generation" class="headerlink" title="NLDF: Neural Light Dynamic Fields for Efficient 3D Talking Head   Generation"></a>NLDF: Neural Light Dynamic Fields for Efficient 3D Talking Head Generation</h2><p><strong>Authors:Niu Guanchen</strong></p><p>Talking head generation based on the neural radiation fields model has shown promising visual effects. However, the slow rendering speed of NeRF seriously limits its application, due to the burdensome calculation process over hundreds of sampled points to synthesize one pixel. In this work, a novel Neural Light Dynamic Fields model is proposed aiming to achieve generating high quality 3D talking face with significant speedup. The NLDF represents light fields based on light segments, and a deep network is used to learn the entire light beam’s information at once. In learning the knowledge distillation is applied and the NeRF based synthesized result is used to guide the correct coloration of light segments in NLDF. Furthermore, a novel active pool training strategy is proposed to focus on high frequency movements, particularly on the speaker mouth and eyebrows. The propose method effectively represents the facial light dynamics in 3D talking video generation, and it achieves approximately 30 times faster speed compared to state of the art NeRF based method, with comparable generation visual quality.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.11259v1">PDF</a></p><p><strong>Summary</strong><br>基于神经辐射场模型的说话头像生成显示了良好的视觉效果，但NeRF的渲染速度过慢严重限制了其应用。</p><p><strong>Key Takeaways</strong></p><ul><li>神经光动态场模型（NLDF）旨在通过光段生成高质量的3D说话面部，并显著加快速度。</li><li>NLDF使用深度网络一次性学习整个光束的信息，采用知识蒸馏并使用NeRF合成结果指导正确的光段颜色。</li><li>提出了新的主动池训练策略，重点关注高频运动，特别是演讲者的嘴部和眉毛。</li><li>该方法有效地表现了3D说话视频生成中的面部光动态。</li><li>NLDF比NeRF基于方法快大约30倍，并且具有可比较的生成视觉质量。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>Title: NLDF：基于神经网络光照动态场的3D说话人头像高效生成方法</p><p>Authors: 牛冠晨及其他未列出的合作者</p><p>Affiliation: 作者的隶属机构未提及。</p><p>Keywords: Neural Light Dynamic Fields, Talking Head Generation, NeRF, Light Segmentation, Active Pool Training</p><p>Urls: <a target="_blank" rel="noopener" href="https://github.com/XXX/XXX">https://github.com/XXX/XXX</a> （如果没有GitHub代码链接，请填写“GitHub:None”）</p><p>Summary:</p><p>(1) 研究背景：<br>随着计算机图形学和深度学习技术的发展，3D说话人头像生成已经成为一个热门的研究领域。然而，现有的基于神经网络辐射场（NeRF）的方法虽然生成效果出色，但渲染速度较慢，难以满足实时应用的需求。本文提出的神经网络光照动态场（NLDF）模型旨在解决这一问题，实现高质量3D说话头像的快速生成。</p><p>(2) 过去的方法及问题：<br>过去的方法主要基于NeRF模型进行3D说话头像的生成，虽然视觉效果出色，但由于需要对数百个采样点进行繁琐的计算过程来合成一个像素，导致渲染速度较慢。此外，过去的方法未能有效地对光照动态进行建模，影响了生成结果的逼真度。</p><p>(3) 研究方法：<br>本文提出了基于光照分段的NLDF模型，使用深度网络一次性学习整个光束的信息。同时，采用知识蒸馏技术进行学习，并使用NeRF合成的结果引导光分段的正确着色。此外，本文还提出了一种新的主动池训练策略，专注于高频移动，特别是说话者的嘴巴和眉毛，以提高生成结果的动态效果。</p><p>(4) 任务与性能：<br>本文的方法应用于3D说话视频生成任务，实现了与基于NeRF的方法相比，约30倍的加速效果，同时保持相当的生成视觉质量。通过实验验证，本文方法能够在保证生成质量的同时，显著提高渲染速度，从而满足实时应用的需求。性能结果表明，该方法能够达到研究目标。<br>好的，我将按照您的要求，用中文详细阐述这篇论文的方法论。以下为详细内容：</p><ol><li>方法论：</li></ol><p>(1) 研究首先基于现有的神经网络辐射场（NeRF）方法存在渲染速度较慢的问题出发，针对该问题，本文提出了一种基于神经网络光照动态场（NLDF）的模型，用于实现高质量3D说话头像的快速生成。这是解决该问题的核心思路。</p><p>(2) 为了构建光照分段模型，本研究引入了光照分段的概念并利用深度网络进行光束信息学习。在此基础上使用知识蒸馏技术进行高效学习。这是其技术的第一步创新点。研究接着引入了NeRF合成结果作为引导，确保光照分段的正确着色。这是其技术的第二步创新点。此外，为了进一步提高生成结果的动态效果，研究还提出了一种新的主动池训练策略，该策略特别关注高频移动区域（如嘴巴和眉毛）。这部分内容是论文的主要贡献和创新点之一。</p><p>(3) 在实际应用中，本文的方法被应用于3D说话视频生成任务。通过对比实验验证，该方法的渲染速度相较于基于NeRF的方法实现了约30倍的加速效果，同时保持了相当的视觉质量。这一部分的实验数据和结果证明了该方法的实用性和优越性。最后，通过性能结果分析，验证了该方法达到了研究目标。这部分内容是论文的核心成果和贡献所在。</p><ol><li>Conclusion:</li></ol><p>(1) 这项工作的意义在于提出了一种基于神经网络光照动态场（NLDF）的3D说话人头像高效生成方法，解决了现有方法渲染速度慢、光照动态建模不足的问题，为实时应用提供了可能。</p><p>(2) 创新点方面，本文引入了光照分段的概念并利用深度网络进行光束信息学习，同时采用知识蒸馏技术和主动池训练策略，提高了生成结果的动态效果和渲染速度。性能上，本文方法实现了与基于NeRF的方法相比约30倍的加速效果，同时保持了相当的视觉质量。工作量方面，本文实现了3D说话视频生成任务的应用，并通过实验验证了方法的有效性。</p><p>总体来说，本文提出的基于神经网络光照动态场的3D说话人头像高效生成方法具有显著的创新性和实用性，为相关领域的研究提供了有益的参考和启示。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-0d5f496fb1cc5f2aa7988a95b302e626.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-4f1ec26ae4a93cc432503fa4842a7007.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-9f82dd9ef776058f584bdd8f88b44efc.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-070e3169781f9800bdeb41a6d90d09de.jpg" align="middle"></details><h2 id="Make-Your-Actor-Talk-Generalizable-and-High-Fidelity-Lip-Sync-with-Motion-and-Appearance-Disentanglement"><a href="#Make-Your-Actor-Talk-Generalizable-and-High-Fidelity-Lip-Sync-with-Motion-and-Appearance-Disentanglement" class="headerlink" title="Make Your Actor Talk: Generalizable and High-Fidelity Lip Sync with   Motion and Appearance Disentanglement"></a>Make Your Actor Talk: Generalizable and High-Fidelity Lip Sync with Motion and Appearance Disentanglement</h2><p><strong>Authors:Runyi Yu, Tianyu He, Ailing Zhang, Yuchi Wang, Junliang Guo, Xu Tan, Chang Liu, Jie Chen, Jiang Bian</strong></p><p>We aim to edit the lip movements in talking video according to the given speech while preserving the personal identity and visual details. The task can be decomposed into two sub-problems: (1) speech-driven lip motion generation and (2) visual appearance synthesis. Current solutions handle the two sub-problems within a single generative model, resulting in a challenging trade-off between lip-sync quality and visual details preservation. Instead, we propose to disentangle the motion and appearance, and then generate them one by one with a speech-to-motion diffusion model and a motion-conditioned appearance generation model. However, there still remain challenges in each stage, such as motion-aware identity preservation in (1) and visual details preservation in (2). Therefore, to preserve personal identity, we adopt landmarks to represent the motion, and further employ a landmark-based identity loss. To capture motion-agnostic visual details, we use separate encoders to encode the lip, non-lip appearance and motion, and then integrate them with a learned fusion module. We train MyTalk on a large-scale and diverse dataset. Experiments show that our method generalizes well to the unknown, even out-of-domain person, in terms of both lip sync and visual detail preservation. We encourage the readers to watch the videos on our project page (<a target="_blank" rel="noopener" href="https://Ingrid789.github.io/MyTalk/">https://Ingrid789.github.io/MyTalk/</a>).</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.08096v2">PDF</a> 14 pages of main text, 23 pages in total, 9 figures</p><p><strong>Summary</strong><br>我们旨在根据给定的语音编辑说话视频中的唇部运动，同时保留个人身份和视觉细节。</p><p><strong>Key Takeaways</strong></p><ul><li>任务分解为语音驱动的唇部运动生成和视觉外观合成两个子问题。</li><li>目前的解决方案在单一生成模型中处理两个子问题，导致在唇同步质量和视觉细节保留之间存在挑战性的权衡。</li><li>我们提议通过分离运动和外观，采用语音到运动扩散模型和运动条件的外观生成模型分别生成它们。</li><li>每个阶段仍然存在挑战，如在第一阶段中的运动感知身份保留和第二阶段的视觉细节保留。</li><li>为了保留个人身份，我们采用地标来表示运动，并进一步采用基于地标的身份损失。</li><li>为了捕捉与运动无关的视觉细节，我们使用单独的编码器对唇部、非唇部外观和运动进行编码，然后用学习的融合模块进行集成。</li><li>我们在大规模和多样化数据集上训练了MyTalk模型，实验表明我们的方法在唇同步和视觉细节保留方面对未知、甚至是超领域的人物都具有很好的泛化能力。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>好的，我将会根据您给出的要求来回答。</p><ol><li>Title: 基于运动与外观解耦的说话人唇动生成技术</li><li>Authors: 于润义、何天宇、张艾玲、王宇池、郭俊良、谭煦、刘畅、陈洁和边江</li><li>Affiliation: 第一作者于润义是北京大学的。</li><li>Keywords: 说话视频生成、唇同步、面部动画、扩散模型</li><li>Urls: 论文链接：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.08096v2">论文链接</a>。Github代码链接：GitHub:暂无可用的代码链接。</li><li>Summary:</li></ol><p>(1) 研究背景：<br>随着人工智能技术的发展，生成说话视频的需求越来越高。本文旨在编辑说话视频的唇动，使其能够根据给定的语音进行同步，同时保留个人身份和视觉细节。这对于电影特效、虚拟现实和游戏等领域有广泛应用。</p><p>(2) 过去的方法及其问题：<br>当前的方法通常在一个生成模型中处理唇动生成和视觉外观合成两个子问题，导致唇同步质量和视觉细节保留之间的权衡困难。此外，这些方法在应对未知或域外人物时表现不佳。</p><p>(3) 研究方法：<br>本文提出一种将运动和外观解耦的方法，通过扩散模型和基于运动的外观生成模型来分别生成运动和外观。为解决身份保留和视觉细节捕捉的问题，采用基于特征点的身份损失和融合模块来整合唇部和非唇部区域的视觉细节。</p><p>(4) 任务与性能：<br>本文的方法在大型和多样化的数据集上进行训练，实验表明，该方法在未知甚至域外人物上的唇同步和视觉细节保留方面都具有良好的泛化能力。性能评估支持了该方法的有效性。</p><p>请注意，由于无法直接观看视频或访问GitHub代码链接，无法进一步验证上述信息的准确性。建议您直接通过提供的链接查看论文和相关资源以获取更多详细信息。<br>好的，根据您的要求，我会按照给定的格式来详细阐述这篇论文的方法论。</p><ol><li>Methods:</li></ol><p>(1) 研究背景和意义：随着多媒体技术的发展，生成说话视频的需求越来越高。特别是在电影特效、虚拟现实和游戏等领域，唇动编辑技术具有重要的应用价值。因此，本文旨在开发一种能够编辑说话视频的唇动，使其能够根据给定的语音进行同步，同时保留个人身份和视觉细节的技术。</p><p>(2) 对现有技术的问题进行分析：当前的方法大多在一个生成模型中同时处理唇动生成和视觉外观合成两个子问题，这导致在唇同步质量和视觉细节保留之间难以取得平衡。此外，这些方法在应对未知或域外人物时表现不佳。</p><p>(3) 方法论的主要思路：针对上述问题，本文提出了一种将运动和外观解耦的方法，通过扩散模型和基于运动的外观生成模型来分别生成运动和外观。首先，利用扩散模型学习唇部运动模式；然后，通过基于运动的外观生成模型来生成与唇部运动相匹配的外观。</p><p>(4) 具体实现步骤：</p><ul><li>a. 数据准备：收集并预处理大量的说话视频数据，包括面部图像和对应的语音信号。</li><li>b. 训练和模型构建：采用扩散模型学习唇部运动模式，并构建基于运动的外观生成模型。</li><li>c. 唇同步和视觉细节保留：通过融合模块整合唇部和非唇部区域的视觉细节，实现唇同步和视觉细节保留。</li><li>d. 评估与测试：在大型和多样化的数据集上进行训练，并通过实验验证该方法在未知甚至域外人物上的泛化能力。</li></ul><p>(5) 方法和效果评估：实验结果表明，本文提出的方法在唇同步和视觉细节保留方面均表现出良好的性能。通过对比实验和性能评估，验证了该方法的有效性和优越性。</p><p>以上就是这篇论文的方法论介绍。</p><ol><li>Conclusion:</li></ol><p>(1)这篇工作的意义在于开发了一种能够编辑说话视频的唇动技术，使视频能够根据给定的语音进行同步，同时保留个人身份和视觉细节。这项技术在电影特效、虚拟现实和游戏等领域具有重要的应用价值。</p><p>(2)创新点：本文提出了一种基于运动与外观解耦的说话人唇动生成技术，通过扩散模型和基于运动的外观生成模型分别生成运动和外观，实现了唇同步和视觉细节保留。<br>性能：实验结果表明，该方法在唇同步和视觉细节保留方面均表现出良好的性能，并且在未知或域外人物上具有良好的泛化能力。<br>工作量：本文收集并预处理了大量的说话视频数据，构建了基于扩散模型和基于运动的外观生成模型，通过实验验证了方法的有效性。</p><p>总体来说，本文提出的说话人唇动生成技术具有创新性和实用性，为电影特效、虚拟现实和游戏等领域的唇动编辑提供了有效的解决方案。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-8a6a7c5f91f913dcbb728f71012a7a25.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-ffacd6f931293748617a8f14a08c763e.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-7aef5cf3d8645ae9194bd3559c9139ed.jpg" align="middle"></details></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io/2024/06/20/Paper/2024-06-20/Talking%20Head%20Generation/">https://kedreamix.github.io/2024/06/20/Paper/2024-06-20/Talking Head Generation/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Talking-Head-Generation/">Talking Head Generation</a></div><div class="post_share"><div class="social-share" data-image="https://picx.zhimg.com/v2-de6a5bba250b1dcfc57b3ad385bdae26.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/06/20/Paper/2024-06-20/3DGS/" title="3DGS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-e47e1891444d84986d52eede4b830aec.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">3DGS</div></div></a></div><div class="next-post pull-right"><a href="/2024/06/20/Paper/2024-06-20/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-e47e1891444d84986d52eede4b830aec.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Diffusion Models</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/03/11/Note/BlendShape/" title="Blendshape学习笔记"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://p6-sign.toutiaoimg.com/pgc-image/2c8cbd123e00470e95500a8ae62da605~noop.image?_iz=58558&from=article.pc_detail&lk3s=953192f4&x-expires=1710668214&x-signature=UHPhjWP4v96kbtfJzF97Z%2Bp3klc%3D" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-11</div><div class="title">Blendshape学习笔记</div></div></a></div><div><a href="/2024/03/03/Paperscape/EMO/" title="EMO Emote Portrait Alive - 阿里HumanAIGC"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-6492e24fb03ffa98135dc584535ab7d9.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-03</div><div class="title">EMO Emote Portrait Alive - 阿里HumanAIGC</div></div></a></div><div><a href="/2024/03/15/Paperscape/Real3D-Portrait/" title="REAL3D-PORTRAIT ONE-SHOT REALISTIC 3D TALKING PORTRAIT SYNTHESIS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-68585b79de5f83b0dfa23304f41b9b98.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-15</div><div class="title">REAL3D-PORTRAIT ONE-SHOT REALISTIC 3D TALKING PORTRAIT SYNTHESIS</div></div></a></div><div><a href="/2024/03/05/Paperscape/VividTalk/" title="VividTalk One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-8521b04f82075cc27b5e95148dba9792.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-05</div><div class="title">VividTalk One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior</div></div></a></div><div><a href="/2024/03/07/Paperscape/SyncTalk/" title="SyncTalk The Devil is in the Synchronization for Talking Head Synthesis"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-a57e0937b2f452009023394a59529dfb.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-07</div><div class="title">SyncTalk The Devil is in the Synchronization for Talking Head Synthesis</div></div></a></div><div><a href="/2024/03/18/Project/SyncTalk/" title="SyncTalk实验笔记"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-3866dff2d07194c235eefab923f694c5.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-18</div><div class="title">SyncTalk实验笔记</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-06-20-%E6%9B%B4%E6%96%B0"><span class="toc-text">2024-06-20 更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Talk-With-Human-like-Agents-Empathetic-Dialogue-Through-Perceptible-Acoustic-Reception-and-Reaction"><span class="toc-text">Talk With Human-like Agents: Empathetic Dialogue Through Perceptible Acoustic Reception and Reaction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#NLDF-Neural-Light-Dynamic-Fields-for-Efficient-3D-Talking-Head-Generation"><span class="toc-text">NLDF: Neural Light Dynamic Fields for Efficient 3D Talking Head Generation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Make-Your-Actor-Talk-Generalizable-and-High-Fidelity-Lip-Sync-with-Motion-and-Appearance-Disentanglement"><span class="toc-text">Make Your Actor Talk: Generalizable and High-Fidelity Lip Sync with Motion and Appearance Disentanglement</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image:url('https://picx.zhimg.com/v2-de6a5bba250b1dcfc57b3ad385bdae26.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js").then((()=>{pangu.autoSpacingPage()}))}function panguInit(){panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.1},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script></div><script id="canvas_nest" defer color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><script data-pjax>function butterfly_swiper_injector_config(){var a=document.getElementById("recent-posts");console.log("已挂载butterfly_swiper"),a.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/25/Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="2024/01/25/Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="2024/01/25/Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/17/Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023/12/17/Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="2023/12/17/Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script></body></html>