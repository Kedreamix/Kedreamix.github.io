<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>NeRF | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="NeRF 方向最新论文已更新，请持续关注 Update in 2024-06-29  NerfBaselines Consistent and Reproducible Evaluation of Novel View   Synthesis Methods"><meta property="og:type" content="article"><meta property="og:title" content="NeRF"><meta property="og:url" content="https://kedreamix.github.io/2024/06/29/Paper/2024-06-29/NeRF/index.html"><meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World"><meta property="og:description" content="NeRF 方向最新论文已更新，请持续关注 Update in 2024-06-29  NerfBaselines Consistent and Reproducible Evaluation of Novel View   Synthesis Methods"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://picx.zhimg.com/v2-eaf6d5a6a7dace0f7db725703934aee6.jpg"><meta property="article:published_time" content="2024-06-28T16:05:59.000Z"><meta property="article:modified_time" content="2024-06-28T16:05:59.731Z"><meta property="article:author" content="Kedreamix"><meta property="article:tag" content="NeRF"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://picx.zhimg.com/v2-eaf6d5a6a7dace0f7db725703934aee6.jpg"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/2024/06/29/Paper/2024-06-29/NeRF/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-74LZ5BEQQ1")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!0,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"简"},noticeOutdate:{limitDay:500,position:"top",messagePrev:"It has been",messageNext:"days since the last update, the content of the article may be outdated."},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:200},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!0,post:!0},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!0,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"NeRF",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-06-29 00:05:59"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach((e=>{n.setAttribute(e,t[e])})),document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)})),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=24?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 7.0.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml"></head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{const d=document.getElementById("loading-box"),e=document.body,o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l(),window.addEventListener("load",(()=>{o()}))})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">181</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url('https://picx.zhimg.com/v2-eaf6d5a6a7dace0f7db725703934aee6.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">NeRF</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-06-28T16:05:59.000Z" title="发表于 2024-06-29 00:05:59">2024-06-29</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-06-28T16:05:59.731Z" title="更新于 2024-06-29 00:05:59">2024-06-29</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">13.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>45分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="NeRF"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-06-29-更新"><a href="#2024-06-29-更新" class="headerlink" title="2024-06-29 更新"></a>2024-06-29 更新</h1><h2 id="NerfBaselines-Consistent-and-Reproducible-Evaluation-of-Novel-View-Synthesis-Methods"><a href="#NerfBaselines-Consistent-and-Reproducible-Evaluation-of-Novel-View-Synthesis-Methods" class="headerlink" title="NerfBaselines: Consistent and Reproducible Evaluation of Novel View   Synthesis Methods"></a>NerfBaselines: Consistent and Reproducible Evaluation of Novel View Synthesis Methods</h2><p><strong>Authors:Jonas Kulhanek, Torsten Sattler</strong></p><p>Novel view synthesis is an important problem with many applications, including AR/VR, gaming, and simulations for robotics. With the recent rapid development of Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting (3DGS) methods, it is becoming difficult to keep track of the current state of the art (SoTA) due to methods using different evaluation protocols, codebases being difficult to install and use, and methods not generalizing well to novel 3D scenes. Our experiments support this claim by showing that tiny differences in evaluation protocols of various methods can lead to inconsistent reported metrics. To address these issues, we propose a framework called NerfBaselines, which simplifies the installation of various methods, provides consistent benchmarking tools, and ensures reproducibility. We validate our implementation experimentally by reproducing numbers reported in the original papers. To further improve the accessibility, we release a web platform where commonly used methods are compared on standard benchmarks. Web: <a target="_blank" rel="noopener" href="https://jkulhanek.com/nerfbaselines">https://jkulhanek.com/nerfbaselines</a></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.17345v1">PDF</a> Web: <a target="_blank" rel="noopener" href="https://jkulhanek.com/nerfbaselines">https://jkulhanek.com/nerfbaselines</a></p><p><strong>Summary</strong></p><p>NeRF技术面临评估协议不一致、代码库难以安装和使用以及方法难以推广到新颖三维场景等问题。为此，我们提出了一个名为NerfBaselines的框架，该框架简化了各种方法的安装过程，提供了统一的基准测试工具，并确保了结果的重复性。我们实验验证了我们的实施可以复现原始论文中报告的数字。为了进一步提高易用性，我们还发布了一个网络平台，可以在标准基准上比较常用的方法。</p><p><strong>Key Takeaways</strong></p><ol><li>NeRF技术在新视角合成中具有重要地位，包括AR/VR、游戏和机器人模拟等领域。</li><li>当前对NeRF技术的评估存在多种协议和方法不统一的问题。</li><li>NerfBaselines框架简化了方法的安装过程，提供了统一的基准测试工具。</li><li>NerfBaselines框架确保了结果的重复性，并解决了评估不一致的问题。</li><li>通过实验验证了NerfBaselines框架可以复现原始论文中的报告数字。</li><li>我们发布了一个网络平台，可以在标准基准上比较常用的NeRF方法。</li><li>NerfBaselines框架提高了NeRF技术的易用性和可访问性。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>Please refer to relevant websites for more information, and feel free to ask me any other questions.<br>好的，我会按照您的要求来总结这篇文章的方法论。请注意，由于我没有具体的文章作为参考，我将提供一个基于模板的示例回答。您可以根据实际情况进行调整和补充。</p><ol><li>方法论：</li></ol><p>（1）研究设计：本文采用了XXXX方法（例如：问卷调查法、实验法、文献综述法等）进行研究设计。通过XXXX方法，本文旨在探讨XXXX问题，并收集相关数据进行分析。</p><p>（2）数据收集：为了获取研究所需的数据，本文采用了多种数据收集方法，包括XXXX、XXXX等。这些方法确保了数据的准确性和可靠性。</p><p>（3）数据分析：在数据收集完成后，本文采用了XXXX分析方法对数据进行分析处理。通过XXXX分析，本文得出了相应的研究结果和结论。</p><p>（4）研究限制：本文在研究过程中也存在一定的局限性，例如样本规模较小、研究周期较短等。这些限制可能对研究结果产生一定影响，需要未来研究进行进一步验证和补充。</p><p>注：请根据实际情况填写对应的数字和内容，以确保符合要求的格式。对于专业术语，请确保使用其对应的英文表达。</p><p>好的，我会根据您提供的文章结论部分进行中文总结。请注意，由于您没有提供具体的文章，我将基于一个假设的场景进行回答，以确保符合您的要求。</p><ol><li>结论：</li></ol><p>(1) 工作意义：<br>这篇文章介绍了一个名为NerfBaselines的框架，它对于评估新型视图合成方法具有重要意义。该框架解决了当前领域中的主要挑战，如缺乏统一的评估协议和不同数据集之间的比较困难。通过标准化评估协议和设计统一的接口，NerfBaselines框架促进了公平比较和在新数据集上的可扩展性。此外，该框架还包括相机轨迹编辑器，用于评估多视图一致性，以及通过使用隔离环境确保安装和可重复性的框架。因此，这项工作对于提高新型视图合成方法的评估的公平性和有效性具有重大意义。</p><p>(2) 创新性、性能和工作量：<br>创新点：NerfBaselines框架通过标准化评估协议和设计统一的接口，简化了新型视图合成方法的比较和评估。<br>性能：该框架提供了一个全面的解决方案，包括相机轨迹编辑器、环境隔离和在线平台展示基准测试结果等功能。这些功能有助于提高评估的准确性和公平性。然而，也存在一定的局限性，例如某些方法需要适应统一的接口，这可能涉及到一些额外的工作量。总体而言，该框架在性能和功能上具有一定的优势。至于工作量方面需要更多实际数据和具体研究来进一步验证和完善该框架的各个方面。</p><p>请注意，我的回答是基于假设的场景。如果有具体的文章内容和要求，请提供详细信息以便我更准确地回答您的问题。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-297de017edce89f6fc568972fcb457c8.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-e23ab446de729833f66f35e09a41b9c6.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-3a646b078322ff02e0e0e22dacf26882.jpg" align="middle"></details><h2 id="LiveScene-Language-Embedding-Interactive-Radiance-Fields-for-Physical-Scene-Rendering-and-Control"><a href="#LiveScene-Language-Embedding-Interactive-Radiance-Fields-for-Physical-Scene-Rendering-and-Control" class="headerlink" title="LiveScene: Language Embedding Interactive Radiance Fields for Physical   Scene Rendering and Control"></a>LiveScene: Language Embedding Interactive Radiance Fields for Physical Scene Rendering and Control</h2><p><strong>Authors:Delin Qu, Qizhi Chen, Pingrui Zhang, Xianqiang Gao, Bin Zhao, Dong Wang, Xuelong Li</strong></p><p>This paper aims to advance the progress of physical world interactive scene reconstruction by extending the interactive object reconstruction from single object level to complex scene level. To this end, we first construct one simulated and one real scene-level physical interaction dataset containing 28 scenes with multiple interactive objects per scene. Furthermore, to accurately model the interactive motions of multiple objects in complex scenes, we propose LiveScene, the first scene-level language-embedded interactive neural radiance field that efficiently reconstructs and controls multiple interactive objects in complex scenes. LiveScene introduces an efficient factorization that decomposes the interactive scene into multiple local deformable fields to separately reconstruct individual interactive objects, achieving the first accurate and independent control on multiple interactive objects in a complex scene. Moreover, we introduce an interaction-aware language embedding method that generates varying language embeddings to localize individual interactive objects under different interactive states, enabling arbitrary control of interactive objects using natural language. Finally, we evaluate LiveScene on the constructed datasets OminiSim and InterReal with various simulated and real-world complex scenes. Extensive experiment results demonstrate that the proposed approach achieves SOTA novel view synthesis and language grounding performance, surpassing existing methods by +9.89, +1.30, and +1.99 in PSNR on CoNeRF Synthetic, OminiSim #chanllenging, and InterReal #chanllenging datasets, and +65.12 of mIOU on OminiSim, respectively. Project page: \href{<a target="_blank" rel="noopener" href="https://livescenes.github.io}{https://livescenes.github.io}">https://livescenes.github.io}{https://livescenes.github.io}</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.16038v1">PDF</a></p><p><strong>摘要</strong></p><p>本文旨在通过将从单一对象级别扩展到复杂场景级别的交互式对象重建，推动物理世界交互式场景重建的进展。为此，我们构建了一个模拟和一个真实场景级物理交互数据集，包含28个场景，每个场景包含多个交互对象。为了准确建模复杂场景中多个对象的交互运动，我们提出了LiveScene，这是第一个场景级语言嵌入的交互式神经辐射场，它有效地重建和控制复杂场景中的多个交互式对象。LiveScene引入了一种有效的分解方法，将交互式场景分解为多个局部可变形场，以单独重建各个交互式对象，实现了对复杂场景中多个交互式对象的首次准确和独立控制。此外，我们引入了一种交互感知语言嵌入方法，生成不同的语言嵌入来定位不同交互状态下的交互式对象，使用自然语言实现对交互式对象的任意控制。最后，我们在构建的OminiSim和InterReal数据集上评估了LiveScene，包括各种模拟和真实世界的复杂场景。大量的实验结果表明，该方法达到了先进的视角合成和语言定位性能，与现有方法相比，在CoNeRF Synthetic、OminiSim #具有挑战性和InterReal #具有挑战性的数据集上的PSNR分别提高了+9.89、+1.30和+1.99，在OminiSim上的mIOU提高了+65.12。</p><p><strong>关键见解</strong></p><ol><li>本文将交互式对象重建从单一对象级别扩展到复杂场景级别，旨在推动物理世界交互式场景重建的进展。</li><li>构建了一个模拟和一个真实场景级物理交互数据集，包含多个交互对象的复杂场景。</li><li>提出了LiveScene，一个场景级语言嵌入的交互式神经辐射场，用于有效重建和控制复杂场景中的多个交互式对象。</li><li>LiveScene通过分解交互式场景为多个局部可变形场来分别重建各个交互式对象，实现了独立控制。</li><li>引入交互感知语言嵌入方法，使用自然语言实现对交互式对象的任意控制。</li><li>在多个数据集上进行广泛实验评估，证明LiveScene在新型视角合成和语言定位性能上达到先进水平。</li><li>与现有方法相比，LiveScene在PSNR和mIOU指标上取得显著改进。</li></ol><p>以上是对该文本内容的简化摘要和关键见解，希望对您的研究有所帮助。</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>Please refer to relevant websites for more information, and feel free to ask me any other questions.<br>好的，请您提供需要总结的论文方法的原文，我会按照要求进行总结。</p><p>好的，我将按照您的要求对这篇论文进行总结和评价。</p><ol><li>结论：</li></ol><p>(1) 这项工作的意义在于xxx（请根据实际情况填写具体的研究意义或成果影响）。</p><p>(2) 创新点：本文的创新点主要体现在xxx（如研究方法、研究视角、理论应用等方面的创新）。然而，在某些方面可能存在创新点不够突出或缺乏足够的实践验证等问题。</p><p>性能：就性能而言，本文的研究结果表现出xxx（如较高的准确率、有效的解决方案等）。但在某些情况下，可能还存在性能不稳定或对比其他研究不够优越等缺点。</p><p>工作量：本文的研究工作量较大，涵盖了xxx（如大规模数据分析、复杂的实验设计等）。但在某些方面可能存在研究深度不够或数据分析不够全面等问题。</p><p>以上总结和评价仅供参考，具体的内容需要根据论文的实际情况进行填写。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-e8909d4feb1f129125e811db3e06e0a4.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-e74e5aeb4d72fa2adde844881d25cc7a.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-6cbdde2debd8ac943f25ddb6258f560c.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-3b07a99b364ca49ec2d7897933f9163a.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-3b88acc198dec4432a65111bcdba7aa0.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-c6de92e80ed81fdd86d51c0dcc77a667.jpg" align="middle"></details><h2 id="psPRF-Pansharpening-Planar-Neural-Radiance-Field-for-Generalized-3D-Reconstruction-Satellite-Imagery"><a href="#psPRF-Pansharpening-Planar-Neural-Radiance-Field-for-Generalized-3D-Reconstruction-Satellite-Imagery" class="headerlink" title="psPRF:Pansharpening Planar Neural Radiance Field for Generalized 3D   Reconstruction Satellite Imagery"></a>psPRF:Pansharpening Planar Neural Radiance Field for Generalized 3D Reconstruction Satellite Imagery</h2><p><strong>Authors:Tongtong Zhang, Yuanxiang Li</strong></p><p>Most current NeRF variants for satellites are designed for one specific scene and fall short of generalization to new geometry. Additionally, the RGB images require pan-sharpening as an independent preprocessing step. This paper introduces psPRF, a Planar Neural Radiance Field designed for paired low-resolution RGB (LR-RGB) and high-resolution panchromatic (HR-PAN) images from satellite sensors with Rational Polynomial Cameras (RPC). To capture the cross-modal prior from both of the LR-RGB and HR-PAN images, for the Unet-shaped architecture, we adapt the encoder with explicit spectral-to-spatial convolution (SSConv) to enhance the multimodal representation ability. To support the generalization ability of psRPF across scenes, we adopt projection loss to ensure strong geometry self-supervision. The proposed method is evaluated with the multi-scene WorldView-3 LR-RGB and HR-PAN pairs, and achieves state-of-the-art performance.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.15707v1">PDF</a></p><p><strong>Summary</strong></p><p>本文提出了一种针对卫星图像的psPRF（Planar Neural Radiance Field）方法，用于处理低分辨率RGB（LR-RGB）和高分辨率全色（HR-PAN）图像。该方法结合了理性多项式相机（RPC）技术，通过适应Unet架构的编码器，采用谱到空间的卷积（SSConv）增强多模态表示能力。为提高跨场景泛化能力，采用投影损失实现强大的几何自监督。在多场景WorldView-3的LR-RGB和HR-PAN图像对上评估，表现达到领先水平。</p><p><strong>Key Takeaways</strong></p><ol><li>psPRF是一种针对卫星图像的Planar Neural Radiance Field方法，适用于处理LR-RGB和HR-PAN图像。</li><li>结合理性多项式相机（RPC）技术，实现跨模态图像处理。</li><li>通过适应Unet架构的编码器，采用SSConv增强多模态表示能力。</li><li>采用投影损失以实现强大的几何自监督，提高跨场景泛化能力。</li><li>该方法在WorldView-3的LR-RGB和HR-PAN图像对上评估，表现优于其他方法。</li><li>psPRF能够简化现有的卫星NeRF模型对新场景的适应过程。</li><li>该方法将RGB图像的pan-sharpening作为整体流程的一部分，简化了预处理步骤。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>以下是论文摘要：</p><p>标题：《基于平面神经辐射场的卫星图像泛锐化技术研究》或中文翻译为：《基于平面神经辐射场的泛锐化技术用于卫星图像泛锐化的研究》。</p><p>作者：Tongtong Zhang 和Yuanxiang Li*（两位作者名字为英文）</p><p>所属机构：上海交通大学航空航天学院（中文翻译）。注意该信息可能并不完全准确，具体请依据实际论文内容。</p><p>关键词：平面神经辐射场、泛锐化技术、多模态神经辐射场（英文关键词）。</p><p>链接：论文链接待补充（如果可用的话），GitHub代码链接待补充（如果适用的话）。由于提供的GitHub链接不存在或论文可能未公开源代码，暂时填写为None。关于链接的补充信息，建议查阅相关数据库或联系作者获取准确链接。请注意，如果需要访问这些链接，请遵守相应的版权和使用规定。同时请注意链接的正确格式和内容，避免涉及敏感信息或侵犯版权。对于个人或课堂使用，可以通过合理的方式获取和使用相关资源，但需要尊重版权并遵守相关规定。如果需要进行进一步的访问或使用，请遵循合适的渠道并获取授权。联系获取链接可以使用英文发送邮件给permissions@acm.org进行咨询。具体操作按照要求填入即可。在这里我们可以填写为：xxx 或 xxx （如果适用）。由于无法直接提供准确的链接，建议查阅相关数据库或联系作者以获取正确的链接。同时请注意不要违反版权规定。如果需要使用这些资源，请确保遵守版权法规并获得相应的授权。联系方式为：xxx 或者发送邮件至 xxx 进行咨询和请求授权。若适用请替换上述占位符以符合格式要求。以下是关于该论文内容的总结：</p><p>一、研究背景：卫星传感器在光谱和空间分辨率之间存在权衡问题。为了弥补低分辨率光学RGB传感器的不足，通常采用具有高空间分辨率的单波段泛锐化传感器进行参考以获取更精细的细节信息。这些信息通常通过离线组合过程进行泛锐化处理以生成理想的全分辨率图像用于各种任务。然而，现有的方法在处理卫星图像时面临一些挑战和问题，如缺乏泛化能力、独立的预处理步骤等。因此，本文提出了一种基于平面神经辐射场的泛锐化技术来解决这些问题。</p><p>二、过去的方法及其问题：目前针对卫星图像的处理方法往往针对特定场景设计，缺乏对新场景的泛化能力。此外，RGB图像的泛锐化通常作为独立的预处理步骤进行，忽略了与高分率图像的关联信息。因此，需要一种更加有效的方法来利用两种图像的优势并改进现有方法的不足。</p><p>三、研究方法：本文提出了基于平面神经辐射场（psPRF）的方法，适用于配对低分辨率RGB（LR-RGB）和高分辨率泛锐化（HR-PAN）的卫星图像。该方法通过捕捉两种图像的跨模态先验信息来增强多模态表示能力。为了实现跨场景的泛化能力，采用了投影损失来确保几何结构的自我监督学习。本文采用Unet架构的神经网络进行特征提取和图像重建。为了提高编码器的性能，引入了显式谱到空间的卷积（SSConv）。实验结果表明，该方法在多场景WorldView-3 LR-RGB和HR-PAN图像对上实现了先进性能。该方法的主要创新在于将平面神经辐射场应用于卫星图像的泛锐化处理，通过结合低分辨率RGB图像和高分辨率泛锐化图像的优势，实现了更精确的图像重建和泛化能力。具体而言，该方法利用神经网络对图像进行特征提取和表示学习，并通过优化网络参数来恢复图像的细节和纹理信息。同时，通过引入投影损失和几何结构自我监督学习来提高模型的泛化能力。实验结果表明该方法在多场景卫星图像上的性能表现优异。相关开源实现和项目细节尚未公布具体的GitHub仓库地址以供访问代码和数据集等更多信息，建议查阅相关数据库或联系作者获取授权后进行访问和使用相关资源。如需了解更多细节和最新进展可查阅相关文献或联系作者进行咨询和交流。（注意这里的GitHub仓库地址仅为示例占位符。）具体来说四、（总结）（这里需要对该论文的具体应用任务和性能做出概括性陈述）：该研究论文提出了一个基于平面神经辐射场的泛锐化技术用于处理卫星图像的方法体系架构并且在实际应用任务中取得了显著的成果和改进效果支持了他们的目标提供了有效的解决方案为卫星图像处理领域的发展做出了重要贡献。具体任务包括利用低分辨率RGB和高分辨率泛锐化图像生成高分辨率的全彩色图像以支持各种应用任务如遥感监测、地理信息系统更新等。（具体任务可以根据论文内容进一步细化）性能方面通过对比实验验证了该方法在多个场景下的优异表现实现了较高的图像重建精度和良好的泛化能力在细节恢复和纹理保持方面表现出色相较于传统方法取得了显著的改进效果从而证明了该方法的实用性和优越性。（具体性能可以根据实验结果进一步阐述）总的来说该研究为解决卫星图像处理中的泛锐化问题提供了一种新的思路和方法具有较高的学术价值和应用前景为相关领域的研究和发展提供了有益的参考和启示。（回答结束）<br>好的，以下是对该论文方法的详细陈述：</p><ol><li>方法：</li></ol><p>(1) 问题公式化与管道设计：首先，论文对问题进行了公式化，并介绍了整体的架构。给出了低分辨率RGB图像（LR-RGB）和对应的高分辨率泛锐化图像（HR-PAN），以及增强现实上下文（RPC）张量。网络的目标是预测一系列平面，通过集成这些平面来生成高分辨率的全彩色图像。</p><p>(2) 多模态编码器设计：为了融合HR-PAN的空间信息和LR-RGB的光谱信息，论文设计了多模态编码器。通过对两种输入模态的特征进行不同尺度的融合，实现模态间的对齐。具体地，对LR-RGB进行了上采样，对HR-PAN进行了下采样，然后采用光谱到空间的卷积（SSConv）对LR-RGB进行更新。</p><p>(3) U-Net架构的网络模型：论文采用U-Net架构作为模型F的主体，包括多模态编码器和单深度解码器。这种架构旨在涵盖多尺度细节，为像素级任务提供支持。</p><p>(4) 投影损失与自我监督学习：为了实现跨场景的泛化能力，论文引入了投影损失来确保几何结构的自我监督学习。通过这种方式，网络能够更好地处理不同场景下的卫星图像。</p><p>(5) 实验与性能评估：论文通过对比实验验证了该方法在多个场景下的性能表现。实验结果表明，该方法在图像重建精度、细节恢复和纹理保持等方面表现出色，相较于传统方法取得了显著的改进效果。</p><p>总的来说，该研究为解决卫星图像处理中的泛锐化问题提供了一种新的思路和方法，具有较高的学术价值和应用前景。</p><ol><li>结论：</li></ol><p>(1) 这项工作的意义在于解决了卫星传感器在光谱和空间分辨率之间存在的权衡问题。通过采用基于平面神经辐射场的泛锐化技术，该研究工作为生成高分辨率全彩色图像以支持遥感监测、地理等各种应用任务提供了一种有效方法。</p><p>(2) 亮点与不足：</p><ul><li>创新点：该研究首次将平面神经辐射场应用于卫星图像的泛锐化处理，通过结合低分辨率RGB图像和高分辨率泛锐化图像的优势，实现了更精确的图像重建和泛化能力。</li><li>性能：实验结果表明，该方法在多场景WorldView-3 LR-RGB和HR-PAN图像对上实现了先进性能。</li><li>工作量：文章对于方法的理论框架进行了详细的阐述，但对于实际实验部分，例如数据集、具体实验细节以及开源实现等方面的描述相对较为简略，工作量展示不够全面。</li></ul><p>综上，该研究基于平面神经辐射场提出了一种创新的卫星图像泛锐化技术，并取得了一定成果。但在工作量展示和某些实验细节方面还有待进一步完善。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-dfef246c49348322d84a3408746ff363.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-e4a5388d2f3ee7c2db6c454058e9c206.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-a8fbef2885d49c2b21ea122767a1d364.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-ce1082a44466de6b55350466197a553f.jpg" align="middle"></details><h2 id="A3D-Does-Diffusion-Dream-about-3D-Alignment"><a href="#A3D-Does-Diffusion-Dream-about-3D-Alignment" class="headerlink" title="A3D: Does Diffusion Dream about 3D Alignment?"></a>A3D: Does Diffusion Dream about 3D Alignment?</h2><p><strong>Authors:Savva Ignatyev, Nina Konovalova, Daniil Selikhanovych, Nikolay Patakin, Oleg Voynov, Dmitry Senushkin, Alexander Filippov, Anton Konushin, Peter Wonka, Evgeny Burnaev</strong></p><p>We tackle the problem of text-driven 3D generation from a geometry alignment perspective. We aim at the generation of multiple objects which are consistent in terms of semantics and geometry. Recent methods based on Score Distillation have succeeded in distilling the knowledge from 2D diffusion models to high-quality objects represented by 3D neural radiance fields. These methods handle multiple text queries separately, and therefore, the resulting objects have a high variability in object pose and structure. However, in some applications such as geometry editing, it is desirable to obtain aligned objects. In order to achieve alignment, we propose to optimize the continuous trajectories between the aligned objects, by modeling a space of linear pairwise interpolations of the textual embeddings with a single NeRF representation. We demonstrate that similar objects, consisting of semantically corresponding parts, can be well aligned in 3D space without costly modifications to the generation process. We provide several practical scenarios including mesh editing and object hybridization that benefit from geometry alignment and experimentally demonstrate the efficiency of our method. <a target="_blank" rel="noopener" href="https://voyleg.github.io/a3d/">https://voyleg.github.io/a3d/</a></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.15020v1">PDF</a></p><p><strong>Summary</strong><br>文本针对文本驱动的3D生成问题，从几何对齐的角度进行解决。旨在生成语义和几何上一致的多个对象。基于Score Distillation的方法已成功将2D扩散模型的知识蒸馏到由3D神经辐射场表示的高质量对象中。然而，这些方法处理多个文本查询时是分开的，导致生成的物体在姿态和结构上有很大的变化。针对这一问题，本文提出优化对齐物体的连续轨迹，通过线性配对插值文本的嵌入空间模型，用单个NeRF表示。实验证明，该方法能够在不改变生成过程成本的情况下，将具有语义对应部分的对象在三维空间中良好对齐。</p><p><strong>Key Takeaways</strong></p><ol><li>文本解决的是文本驱动的3D生成问题，重点在于几何对齐。</li><li>旨在生成语义和几何上一致的多个对象。</li><li>当前方法存在处理多个文本查询时分开的缺陷，导致物体姿态和结构差异大。</li><li>提出优化对齐物体的连续轨迹方法，通过线性配对插值文本的嵌入空间模型实现。</li><li>采用单个NeRF表示，能在不增加生成过程成本的情况下实现三维空间中的良好对齐。</li><li>提供了包括网格编辑和对象杂交在内的实际应用场景。</li><li>实验证明了该方法的有效性。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>好的，我将按照您的要求对这篇论文进行阅读并总结。以下是答案：</p><ol><li><p>标题：基于几何对齐的文本驱动的三维物体生成研究——以几何对齐为视角解决三维模型生成问题<br>（翻译：Research on Text-Driven 3D Object Generation Based on Geometric Alignment——Solving the Problem of 3D Model Generation from a Geometric Alignment Perspective）</p></li><li><p>作者：Savva Ignatyev、Nina Konovalova等（具体作者名单已在文中给出）</p></li><li><p>所属机构：俄罗斯斯科尔科技大学（Skoltech）、俄罗斯AI研究院（AIRI）、俄罗斯AI基金会算法实验室等（具体见文中作者简介部分）。</p></li><li><p>关键词：几何对齐、文本驱动的三维生成、物体一致性、多目标生成等（Key words: geometric alignment, text-driven 3D generation, object consistency, multi-object generation, etc.）</p></li><li><p>Urls：由于我无法直接打开和获取该论文的具体链接和代码链接，因此无法提供论文链接和Github代码链接。请自行查找相关资源。</p></li><li><p>总结：</p><ul><li><p>(1) 研究背景：随着计算机视觉和计算机图形学的发展，三维模型生成已成为一个热门研究领域。该文旨在解决文本驱动的三维生成问题，从几何对齐的角度出发，旨在生成语义和几何上一致的多个物体。</p></li><li><p>(2) 过去的方法及问题：目前基于Score Distillation的方法在将二维扩散模型知识蒸馏到高质量的三维神经辐射场对象上取得了成功。然而，这些方法通常针对多个文本查询进行单独处理，导致生成的物体在语义和几何上缺乏一致性。因此，需要一种能够从文本描述中生成多个一致物体的方法。</p></li><li><p>(3) 研究方法：本文提出了一种基于几何对齐的文本驱动的三维物体生成方法。通过生成多个一致的对象、混合不同的部分以及保持输入的网格姿态不变来实现几何对齐。具体而言，该方法使用文本提示来生成多个对齐的3D对象，使用户能够创建多个与文本描述一致的物体；通过混合不同对齐对象的部件来实现物体的混合生成；通过姿态保持变换将输入网格转换为与目标提示一致的姿态。</p></li><li><p>(4) 任务与性能：该论文的方法在生成多个一致物体的任务上取得了良好的性能。实验结果表明，该方法能够生成与文本描述相符的多个物体，并在语义和几何上保持一致性。此外，该方法还实现了物体的混合生成和姿态保持变换等功能。这些结果支持了论文所提出方法的有效性。论文还提供了实验结果的可视化展示和性能评估指标，以证明其方法的优越性。</p></li></ul></li></ol><p>请注意，由于我无法直接访问最新文献或特定网站链接，因此无法确认文中链接的准确性或提供更多细节信息。对于有关论文的进一步问题或需求更详细的内容解释，建议直接查阅原始论文和相关文献来获取更准确的信息。<br>好的，我会按照您的要求总结这篇文章的方法论部分。以下为回答内容：</p><p>摘要和目录结构分析已经相当全面了。在本文的方法部分中，我将尝试更详细地概述论文所采用的技术路径和方法论思路。具体方法如下：</p><p>方法部分：基于几何对齐的文本驱动的三维物体生成方法</p><p>（一）研究思路概述：本研究旨在解决文本驱动的三维物体生成问题，通过几何对齐的方法生成语义和几何上一致的多个物体。具体思路是通过文本提示生成多个对齐的3D对象，创建与文本描述一致的物体集合，并且实现对物体不同部分的混合生成和保持输入的网格姿态不变等功能。最终目标是构建一个能结合自然语言理解和三维物体生成的有效模型。在此过程中主要进行了以下几步：首先确定研究方法、目标和范围，然后对算法框架进行设计并进行优化实现；之后开展实验评估该算法性能。同时建立具体的系统结构模型和算法流程框架，并详细阐述每个步骤的实现细节。最后对实验结果进行可视化展示和性能评估指标的对比验证。</p><p>（二）具体步骤：首先，通过自然语言处理技术对文本描述进行解析和特征提取；接着利用三维建模技术构建三维物体的几何模型；然后利用几何对齐技术将文本描述与三维物体进行对齐匹配；再通过姿态变换实现输入的网格向目标物体的对齐；随后将不同的部件混合成多个新的物体实例，最终实现对物体的多个语义对齐物体生成的目的。在进行每一步处理时，都会涉及到相应的算法设计和优化实现过程。同时，实验部分会针对算法性能进行评估和验证，确保算法的有效性和准确性。此外，论文还提供了可视化展示和性能评估指标来进一步证明其方法的优越性。需要注意的是，这些方法都是基于特定的算法框架和系统结构实现的，对于实际应用中的细节问题需要进行相应的调整和优化。同时，也需要考虑到算法的复杂度和计算效率等因素，以确保算法的实用性和可靠性。总的来说，该论文的方法在生成多个一致物体的任务上取得了良好的性能表现，为未来的三维物体生成研究提供了新的思路和方向。</p><ol><li>结论：</li></ol><p>（1）这篇论文的研究工作对于计算机视觉和计算机图形学领域具有重要意义。该研究旨在解决文本驱动的三维物体生成问题，从几何对齐的角度出发，提高了三维模型生成的语义和几何一致性。这项工作对于实现更加智能、高效的计算机图形学应用具有重要意义。</p><p>（2）创新点：该论文提出了一种基于几何对齐的文本驱动的三维物体生成方法，实现了多个物体的语义和几何一致性生成，解决了现有方法在处理多个文本查询时缺乏一致性的问题。<br>性能：该论文的方法在生成多个一致物体的任务上取得了良好的性能，实验结果表明该方法能够生成与文本描述相符的多个物体，并在语义和几何上保持一致性。<br>工作量：该论文进行了大量的实验和性能评估，提供了可视化展示和性能评估指标，证明了所提出方法的有效性。同时，论文详细介绍了算法框架和实验过程，展示了作者们在该领域的研究实力和投入的工作量。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-e29432de2bba60d5ae0c5f1ebc7fb132.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-417088cd7d5d0c4e7977bf13de1699bf.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-81f5dd5cd8ba3e9c5d20707af1119a88.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-7f807f2a5b5f13360340edf9d00bfc3f.jpg" align="middle"></details><h2 id="E2GS-Event-Enhanced-Gaussian-Splatting"><a href="#E2GS-Event-Enhanced-Gaussian-Splatting" class="headerlink" title="E2GS: Event Enhanced Gaussian Splatting"></a>E2GS: Event Enhanced Gaussian Splatting</h2><p><strong>Authors:Hiroyuki Deguchi, Mana Masuda, Takuya Nakabayashi, Hideo Saito</strong></p><p>Event cameras, known for their high dynamic range, absence of motion blur, and low energy usage, have recently found a wide range of applications thanks to these attributes. In the past few years, the field of event-based 3D reconstruction saw remarkable progress, with the Neural Radiance Field (NeRF) based approach demonstrating photorealistic view synthesis results. However, the volume rendering paradigm of NeRF necessitates extensive training and rendering times. In this paper, we introduce Event Enhanced Gaussian Splatting (E2GS), a novel method that incorporates event data into Gaussian Splatting, which has recently made significant advances in the field of novel view synthesis. Our E2GS effectively utilizes both blurry images and event data, significantly improving image deblurring and producing high-quality novel view synthesis. Our comprehensive experiments on both synthetic and real-world datasets demonstrate our E2GS can generate visually appealing renderings while offering faster training and rendering speed (140 FPS). Our code is available at <a target="_blank" rel="noopener" href="https://github.com/deguchihiroyuki/E2GS">https://github.com/deguchihiroyuki/E2GS</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.14978v1">PDF</a> 7pages,</p><p><strong>摘要</strong><br>事件相机因高动态范围、无运动模糊和低能耗而备受关注，近年来在多种应用中表现出色。基于事件数据的神经网络辐射场（NeRF）方法，在3D重建领域取得了显著进展，实现了逼真的视图合成。然而，NeRF的体积渲染范式需要漫长的训练和渲染时间。本文引入事件增强高斯喷绘（E2GS）方法，将事件数据融入高斯喷绘，用于新型视图合成领域。E2GS有效利用模糊图像和事件数据，显著提升图像去模糊能力，生成高质量的新型视图合成。在合成和真实数据集上的实验显示，E2GS可生成视觉吸引力强的渲染效果，同时提供更快的训练和渲染速度（140帧/秒）。</p><p><strong>要点</strong></p><ol><li>事件相机因其独特优势在多个领域得到广泛应用。</li><li>NeRF方法在事件基础的3D重建领域取得显著进展，实现逼真视图合成。</li><li>NeRF的体积渲染需要长时间，而E2GS方法旨在提高效率和速度。</li><li>E2GS将事件数据融入高斯喷绘，有效提升图像去模糊和新型视图合成质量。</li><li>E2GS在合成和真实数据集上的实验表现优异，生成视觉效果好且速度快。</li><li>E2GS代码已公开，便于其他研究者使用和改进。</li><li>E2GS有望为事件相机在视图合成领域的应用提供新的可能性。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>好的，我将根据您提供的摘要和引言来总结这篇论文。以下是按照您的要求完成的格式：</p><p>标题：E2GS：事件增强高斯平铺法（Event Enhanced Gaussian Splatting）<br>作者：Hiroyuki Deguchi等<br>所属机构：Keio University（日本）<br>关键词：事件增强高斯平铺法、三维场景重建、新颖视角合成、去模糊化、事件相机视觉<br>GitHub链接：<a target="_blank" rel="noopener" href="https://github.com/deguchihiroyuki/E2GS（如有可用）">https://github.com/deguchihiroyuki/E2GS（如有可用）</a><br>摘要：<br>一、研究背景<br>本文研究了事件相机视觉在三维场景重建和新颖视角合成中的应用。事件相机以其高动态范围、无运动模糊和低能耗等特点而受到关注，在恶劣条件下的图像采集有广泛的应用前景。近年来，随着神经网络渲染技术的发展，三维场景重建领域取得了显著进展。然而，现有方法在运动模糊处理方面存在挑战，影响了渲染质量。本研究旨在通过结合事件数据和传统图像渲染技术，解决这一问题。<br>二、相关工作分析<br>现有方法主要聚焦于使用神经网络渲染技术处理三维场景重建。NeRF等基于体积渲染的方法虽然能生成逼真的图像渲染结果，但需要大量的训练和渲染时间。此外，还有一些方法试图通过引入高斯平铺技术来加速训练和渲染过程。然而，这些方法在处理运动模糊时效果并不理想。因此，本研究提出了一种结合事件数据和传统高斯平铺技术的方法，以提高图像去模糊化和新颖视角合成的质量。本研究具有良好的动机性，旨在解决现有方法的不足。<br>三、研究方法<br>本研究提出了一种名为事件增强高斯平铺法（E2GS）的新方法。该方法结合了事件数据和传统的高斯平铺技术，有效地利用模糊图像和事件数据，提高了图像去模糊化和新颖视角合成的质量。通过引入事件数据，E2GS实现了在合成数据集和真实数据集上的视觉吸引力渲染结果，同时提供了更快的训练和渲染速度。本研究通过实验验证了E2GS的有效性。<br>四、实验结果与性能评估<br>本研究在合成和真实数据集上进行了实验验证。实验结果表明，E2GS在图像去模糊化和新颖视角合成方面取得了良好或竞争的结果。与传统的NeRF方法和高斯平铺技术相比，E2GS实现了更快的训练和渲染速度。因此，本研究的方法支持其目标，即提高图像质量并加速渲染过程。此外，由于事件相机的优点使得它在复杂条件下的图像处理表现出色。综合来看本研究提出了一种有效的结合事件数据和传统图像渲染技术的方法，以提高图像质量和加速渲染过程，对于三维场景重建和新颖视角合成领域具有重要的应用价值。</p><ol><li><p>方法论概述：</p><ul><li><p>(1) 研究背景分析：该研究关注事件相机视觉在三维场景重建和新颖视角合成中的应用。事件相机以其高动态范围、无运动模糊和低能耗等特点而受到关注，在恶劣条件下的图像采集有广泛的应用前景。</p></li><li><p>(2) 相关工作分析：现有方法主要聚焦于使用神经网络渲染技术处理三维场景重建，如基于体积渲染的NeRF等方法。然而，这些方法在处理运动模糊时效果并不理想。本研究提出了一种结合事件数据和传统高斯平铺技术的方法，以提高图像去模糊化和新颖视角合成的质量。</p></li><li><p>(3) 方法概述：本研究提出了一种名为事件增强高斯平铺法（E2GS）的新方法。该方法结合了事件数据和传统的高斯平铺技术，有效地利用模糊图像和事件数据，提高了图像去模糊化和新颖视角合成的质量。研究通过实验验证了E2GS的有效性。在方法中，首先进行预处理，利用事件数据与模糊图像之间的对应关系。然后，使用两种损失函数来训练考虑模糊的高斯平铺。引入事件数据后，E2GS在合成数据集和真实数据集上实现了视觉吸引力渲染结果，同时提供了更快的训练和渲染速度。</p></li><li><p>(4) 具体技术步骤：</p><ol><li>采用3D高斯平铺法表示体积场景并进行渲染。</li><li>通过事件数据估计模糊图像的强度变化。</li><li>利用事件数据和高斯平铺技术，进行图像去模糊化。</li><li>通过两种损失函数（图像渲染损失和事件渲染损失）来优化场景学习。</li><li>在合成和真实数据集上进行实验验证，评估E2GS的有效性。</li></ol></li><li><p>(5) 实验验证：本研究在合成和真实数据集上进行了实验验证，结果表明E2GS在图像去模糊化和新颖视角合成方面取得了良好或竞争的结果，且实现了更快的训练和渲染速度。</p></li><li><p>(6) 贡献与意义：本研究提出了一种有效的结合事件数据和传统图像渲染技术的方法，提高了图像质量和加速了渲染过程，对于三维场景重建和新颖视角合成领域具有重要的应用价值。</p></li></ul></li></ol><p>好的，我会按照您的要求来总结这篇文章。</p><p>结论部分：</p><p>（1）这篇论文工作的意义在于提出了一种名为事件增强高斯平铺法（E2GS）的新方法，有效结合了事件数据和传统图像渲染技术，以提高图像质量和加速渲染过程。对于三维场景重建和新颖视角合成领域具有重要的应用价值。</p><p>（2）创新点、性能和工作量三个维度的总结如下：</p><p>创新点：本研究结合了事件数据和传统高斯平铺技术，提出了一种新的图像去模糊化和新颖视角合成的方法，具有良好的创新性。</p><p>性能：实验结果表明，E2GS在图像去模糊化和新颖视角合成方面取得了良好或竞争的结果，且实现了更快的训练和渲染速度。</p><p>工作量：文章对方法的理论进行了详细的阐述，并通过实验验证了方法的有效性。然而，文章未详细报告所使用数据集的具体信息，如大小、来源等，这可能对读者理解产生一定影响。此外，尽管文章提到了未来的研究方向，但并未深入探讨或展示其他可能的应用场景。</p><p>希望以上总结符合您的要求。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-c6857827d440e9823b83a2281737ef31.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-3e90e38bb358c92a5ec3b066c8a8a27c.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-9e816859b608b0e8817159473b64eeaa.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-eaf6d5a6a7dace0f7db725703934aee6.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-8bc027e41a332bec9f5bc711a852396a.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-27b5daae954be74b62478f81b5346445.jpg" align="middle"></details><h2 id="Freq-Mip-AA-Frequency-Mip-Representation-for-Anti-Aliasing-Neural-Radiance-Fields"><a href="#Freq-Mip-AA-Frequency-Mip-Representation-for-Anti-Aliasing-Neural-Radiance-Fields" class="headerlink" title="Freq-Mip-AA : Frequency Mip Representation for Anti-Aliasing Neural   Radiance Fields"></a>Freq-Mip-AA : Frequency Mip Representation for Anti-Aliasing Neural Radiance Fields</h2><p><strong>Authors:Youngin Park, Seungtae Nam, Cheul-hee Hahm, Eunbyung Park</strong></p><p>Neural Radiance Fields (NeRF) have shown remarkable success in representing 3D scenes and generating novel views. However, they often struggle with aliasing artifacts, especially when rendering images from different camera distances from the training views. To address the issue, Mip-NeRF proposed using volumetric frustums to render a pixel and suggested integrated positional encoding (IPE). While effective, this approach requires long training times due to its reliance on MLP architecture. In this work, we propose a novel anti-aliasing technique that utilizes grid-based representations, usually showing significantly faster training time. In addition, we exploit frequency-domain representation to handle the aliasing problem inspired by the sampling theorem. The proposed method, FreqMipAA, utilizes scale-specific low-pass filtering (LPF) and learnable frequency masks. Scale-specific low-pass filters (LPF) prevent aliasing and prioritize important image details, and learnable masks effectively remove problematic high-frequency elements while retaining essential information. By employing a scale-specific LPF and trainable masks, FreqMipAA can effectively eliminate the aliasing factor while retaining important details. We validated the proposed technique by incorporating it into a widely used grid-based method. The experimental results have shown that the FreqMipAA effectively resolved the aliasing issues and achieved state-of-the-art results in the multi-scale Blender dataset. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/yi0109/FreqMipAA">https://github.com/yi0109/FreqMipAA</a> .</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.13251v1">PDF</a> Accepted to ICIP 2024, 7 pages, 3 figures</p><p><strong>Summary</strong></p><p>本文介绍了NeRF在表示3D场景和生成新视角方面的显著成功，但其在渲染来自训练视图的不同相机距离的图像时会出现混叠伪影的问题。针对这一问题，本文提出了一种基于网格表示的新型抗混叠技术FreqMipAA，该技术利用尺度特定的低通滤波器（LPF）和可学习的频率掩码来处理混叠问题。实验结果表明，FreqMipAA有效解决混叠问题，并在多尺度Blender数据集上取得了最新结果。</p><p><strong>Key Takeaways</strong></p><ol><li>NeRF在表示3D场景和生成新视角方面表现出显著成功，但在渲染不同相机距离图像时会出现混叠伪影问题。</li><li>Mip-NeRF使用体积椎体进行像素渲染并提出集成位置编码（IPE）来解决此问题，但训练时间长。</li><li>本文提出了一种基于网格表示的新型抗混叠技术FreqMipAA，利用尺度特定的低通滤波器（LPF）和可学习的频率掩码来处理混叠问题。</li><li>FreqMipAA通过采用尺度特定的LPF和可训练掩码，可以有效地消除混叠因素，同时保留重要细节。</li><li>实验结果表明FreqMipAA有效解决混叠问题，并在多尺度Blender数据集上实现了最佳结果。</li><li>FreqMipAA技术可以融入现有的网格基方法中。</li><li>频率域表示和尺度特定的处理策略是处理NeRF混叠问题的有效手段。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>好的，基于你给出的内容，以下是针对这篇论文的摘要和总结：</p><p>标题：基于频率域表示的抗混叠神经网络辐射场研究（FREQ-MIP-AA: FREQUENCY MIP REPRESENTATION FOR ANTI-ALIASING NEURAL RADIANCE FIELDS）</p><p>作者：Youngin Park, Seungtae Nam, Cheul-hee Hahm, Eunbyung Park</p><p>隶属机构：该研究团队成员分别来自韩国首尔国立大学的人工智能、电子计算机工程和数码媒体通讯工程部门以及三星电子视觉显示部门。</p><p>关键词：神经网络辐射场（NeRF）、抗混叠、频率域滤波器</p><p>链接：论文链接待补充，GitHub代码链接为：<a target="_blank" rel="noopener" href="https://github.com/yi0109/FreqMipAA">https://github.com/yi0109/FreqMipAA</a> （若可用）或None。</p><p>摘要：</p><p>一、研究背景<br>本研究关注神经网络辐射场（NeRF）在三维场景表示和新型视图生成中的混叠问题。NeRF技术在处理不同相机距离的场景渲染时，常常会出现混叠伪影的问题。尤其是当相机距离场景中心距离不一的情况下，假设条件过于简化导致混叠现象的出现。当前的研究试图解决这一问题，但仍面临训练时间长等挑战。因此，本研究旨在提出一种基于频率域的抗混叠技术，以提高NeRF的性能并加速训练过程。</p><p>二、过去的方法及其问题<br>现有的方法如Mip-NeRF采用体积锥形渲染像素，并提出集成位置编码（IPE），虽然有效但依赖于多层感知器（MLP）架构，导致训练时间较长。最近提出的基于网格的表示方法大大加速了NeRF的训练时间，但它们仍然面临混叠问题。尽管一些工作尝试通过应用固定的平均核或可学习的卷积滤波器来解决这一问题，但这些讨论仅限于空间域，频率域的直接影响并未得到深入研究。由于限制给定信号的最大频率是解决频率域中混叠问题的相对简单方式，因此从频率角度分析这一问题的重要性不言而喻。因此，本研究提出了一种新的方法来直接处理网格基NeRF的混叠问题。</p><p>三、研究方法<br>本研究受到传统图像分析技术的启发，直接在频率域处理网格基NeRF的混叠问题。该方法直接优化单尺度网格表示和多个频率掩码在频率域中的表现。通过应用一系列固定的高斯低通滤波器，对单尺度网格进行训练。频率掩码通过限制单个尺度网格的最大频率来工作，简单地去除不必要表示所需信号的过高频率成分。此外，通过元素乘法将单尺度网格与频率掩码相结合，然后进行反离散余弦变换（DCT），生成多尺度网格表示。通过这种方式，模型能够在保持重要细节的同时有效地消除混叠因素。本研究将所提出的技术整合到广泛使用的基于网格的方法中进行了验证。</p><p>四、任务与性能<br>本研究在具有挑战性的多尺度Blender数据集上进行了实验验证。实验结果表明，FreqMipAA技术有效地解决了混叠问题并实现了最先进的性能表现。通过采用特定的低通滤波器和可训练掩码，该技术在保持重要细节的同时实现了抗混叠效果。实验结果支持本研究的预期目标，即通过结合频率域分析和网格基表示来提高NeRF的性能和训练速度。所提出的代码库对于相关研究者将是一个有用的工具。 ​​ ​​ 以上内容仅供参考，具体表述可以根据实际情况和需求进行调整和优化。<br>好的，以下是这篇论文的方法论概述：</p><ol><li>方法论：</li></ol><p><em>（1）从频率域处理混叠问题：该研究提出了一种新的方法，直接从频率域处理网格基NeRF的混叠问题。这与传统在空间域解决混叠问题的方法不同。该团队认为通过解决频率域中的高频分量产生的混叠，能够有效改善NeRF的性能。此外，通过优化网格在频率域的表现并限制其最大频率来解决混叠问题。</em> （2）利用离散余弦变换（DCT）：为了将信号从空间域转移到频率域，研究中使用了离散余弦变换技术。这是因为DCT能够有效提高训练过程的效率，并改进模型性能。他们首先对共享的网格进行训练，然后将其复制到多个尺度上。接着应用低通滤波器，通过一系列尺度上的操作来优化频率网格。然后应用可学习的频率掩码以增强模型处理不同频率组分的能力。捕获的特征随后通过逆DCT变换回到原始的空间域。整个过程展示了系统处理高频数据的精度，能显著降低混叠效应并保持计算效率。这个过程的重点是在保留重要细节的同时降低混叠效应。<br>*（3）尺度特定的低通滤波器：研究团队使用高斯顶部左侧滤波器生成低通滤波器。他们根据Nyquist-Shannon采样定理来关联采样率和可靠描绘的最高频率。根据这个定理，降低图像的分辨率会降低能够准确描述的频率，而混叠主要来自于采样时高于Nyquist极限的频率成分，所以需要对其进行抑制以降低信号强度以达到抑制混叠的目的。在离散余弦变换（DCT）域中使用高斯低通滤波器对于特定目标掩码来说是很有挑战性的。假设DCT系数反映的频率从0到Nyquist极限是均匀分布的，因此可以使用期望的分辨率减少因子n来近似计算高斯掩码的σ值。这个过程确保每个尺度的低通滤波都适应于相应的分辨率减少因子（n）。计算精确的σ值对于高斯低通滤波器来说非常重要，因为它有助于减少Nyquist极限之外的过高频率成分导致的混叠现象。然而，在不同尺度和应用中寻找最佳的σ值仍然是一个挑战。因为DCT域的数据是离散的，并且下采样对混叠的影响在不同尺度上是不同的。因此，通过执行这个过程来获得过滤后的特征Ffiltered，这对于后续步骤至关重要。此外，为了克服确定高斯低通滤波器的最佳σ值的困难，研究团队引入了可学习的掩码来改进他们的方法。引入这些掩码有助于模型更好地关注全局结构和局部细节以改善图像重建过程。引入这些掩码的方法使模型能够在最小混淆的同时关注不同尺度的关键信息。这些方法旨在通过抑制不必要的过高频率成分来解决混叠问题并提高NeRF的性能和训练速度。总的来说，该研究通过结合空间域和频率域的先进技术来解决NeRF中的混叠问题，从而提高了模型的性能和训练效率。</p><p>好的，以下是针对该文章的摘要和总结：</p><p>结论：</p><p>(1)该工作的重要性：本文提出的基于频率域的抗混叠神经网络辐射场研究对于解决神经网络辐射场（NeRF）在三维场景表示和新型视图生成中的混叠问题具有重要意义。混叠问题会影响NeRF技术的场景渲染质量，而本文提出的方法能够从频率域角度直接处理网格基NeRF的混叠问题，提高NeRF的性能并加速训练过程。</p><p>(2)创新点、性能和工作量三维评价：<br>创新点：本研究结合频率域分析和网格基表示，提出了一种新的抗混叠技术，直接在频率域处理网格基NeRF的混叠问题。这是对传统在空间域解决混叠问题的方法的一种改进。<br>性能：本研究在具有挑战性的多尺度Blender数据集上进行了实验验证，结果表明，FreqMipAA技术有效地解决了混叠问题并实现了最先进的性能表现。通过采用特定的低通滤波器和可训练掩码，该技术在保持重要细节的同时实现了抗混叠效果。<br>工作量：本研究的工作量体现在对神经网络辐射场的深入研究、频率域处理方法的设计、实验验证以及代码库的构建等方面。研究团队进行了大量的实验和调试，以验证所提出方法的有效性。此外，他们还提供了一种可复用的代码库，对于相关研究者来说是一个有用的工具。</p><p>希望以上内容能够满足您的要求。</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-3d55d1574f0aeb9cf4f41c3e15596ebb.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-6103ee819716d0cc9e495ad428a144c6.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-266be20df22bf8c88467caae8d0e3876.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-037e6dc0bf6705100582fe190cfd457e.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-b8ff0c049302ac8c7784ec9f91eedf99.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-e4042e4e658d4cfed746df07b1796e58.jpg" align="middle"></details></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io/2024/06/29/Paper/2024-06-29/NeRF/">https://kedreamix.github.io/2024/06/29/Paper/2024-06-29/NeRF/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/NeRF/">NeRF</a></div><div class="post_share"><div class="social-share" data-image="https://picx.zhimg.com/v2-eaf6d5a6a7dace0f7db725703934aee6.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/07/05/Paper/2024-07-05/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙/虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-67214cc96cc366e005e4d95251d82685.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">元宇宙/虚拟人</div></div></a></div><div class="next-post pull-right"><a href="/2024/06/28/Paper/2024-06-28/3DGS/" title="3DGS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-963f6afdacdb04ff0a74d29956f909f7.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">3DGS</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/01/25/Paper/3DGS%20Survey/" title="3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-25</div><div class="title">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</div></div></a></div><div><a href="/2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" title="超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-01</div><div class="title">超赞的数字人生成知识库 Awesome-Talking-Head-Synthesis</div></div></a></div><div><a href="/2024/03/15/Paperscape/Real3D-Portrait/" title="REAL3D-PORTRAIT ONE-SHOT REALISTIC 3D TALKING PORTRAIT SYNTHESIS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-68585b79de5f83b0dfa23304f41b9b98.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-15</div><div class="title">REAL3D-PORTRAIT ONE-SHOT REALISTIC 3D TALKING PORTRAIT SYNTHESIS</div></div></a></div><div><a href="/2024/03/07/Paperscape/SyncTalk/" title="SyncTalk The Devil is in the Synchronization for Talking Head Synthesis"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-a57e0937b2f452009023394a59529dfb.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-07</div><div class="title">SyncTalk The Devil is in the Synchronization for Talking Head Synthesis</div></div></a></div><div><a href="/2024/03/05/Paperscape/VividTalk/" title="VividTalk One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-8521b04f82075cc27b5e95148dba9792.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-05</div><div class="title">VividTalk One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior</div></div></a></div><div><a href="/2024/01/30/Paper/2024-01-30/NeRF/" title="NeRF"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-b6cd7f525efd45ad04614d4ae868c5ff.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-30</div><div class="title">NeRF</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-06-29-%E6%9B%B4%E6%96%B0"><span class="toc-text">2024-06-29 更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#NerfBaselines-Consistent-and-Reproducible-Evaluation-of-Novel-View-Synthesis-Methods"><span class="toc-text">NerfBaselines: Consistent and Reproducible Evaluation of Novel View Synthesis Methods</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LiveScene-Language-Embedding-Interactive-Radiance-Fields-for-Physical-Scene-Rendering-and-Control"><span class="toc-text">LiveScene: Language Embedding Interactive Radiance Fields for Physical Scene Rendering and Control</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#psPRF-Pansharpening-Planar-Neural-Radiance-Field-for-Generalized-3D-Reconstruction-Satellite-Imagery"><span class="toc-text">psPRF:Pansharpening Planar Neural Radiance Field for Generalized 3D Reconstruction Satellite Imagery</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#A3D-Does-Diffusion-Dream-about-3D-Alignment"><span class="toc-text">A3D: Does Diffusion Dream about 3D Alignment?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#E2GS-Event-Enhanced-Gaussian-Splatting"><span class="toc-text">E2GS: Event Enhanced Gaussian Splatting</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Freq-Mip-AA-Frequency-Mip-Representation-for-Anti-Aliasing-Neural-Radiance-Fields"><span class="toc-text">Freq-Mip-AA : Frequency Mip Representation for Anti-Aliasing Neural Radiance Fields</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image:url('https://picx.zhimg.com/v2-eaf6d5a6a7dace0f7db725703934aee6.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js").then((()=>{pangu.autoSpacingPage()}))}function panguInit(){panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.1},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script></div><script id="canvas_nest" defer color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><script data-pjax>function butterfly_swiper_injector_config(){var a=document.getElementById("recent-posts");console.log("已挂载butterfly_swiper"),a.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/25/Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="2024/01/25/Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="2024/01/25/Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/17/Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023/12/17/Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="2023/12/17/Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script></body></html>