<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>元宇宙/虚拟人 | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="元宇宙&#x2F;虚拟人 方向最新论文已更新，请持续关注 Update in 2024-06-05  Representing Animatable Avatar via Factorized Neural Fields"><meta property="og:type" content="article"><meta property="og:title" content="元宇宙&#x2F;虚拟人"><meta property="og:url" content="https://kedreamix.github.io/2024/06/05/Paper/2024-06-05/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/index.html"><meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World"><meta property="og:description" content="元宇宙&#x2F;虚拟人 方向最新论文已更新，请持续关注 Update in 2024-06-05  Representing Animatable Avatar via Factorized Neural Fields"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://picx.zhimg.com/80/v2-2bdb0ecbbc3a0a2420781e472b68ba52.png"><meta property="article:published_time" content="2024-06-05T09:23:37.000Z"><meta property="article:modified_time" content="2024-06-05T09:23:37.857Z"><meta property="article:author" content="Kedreamix"><meta property="article:tag" content="元宇宙&#x2F;虚拟人"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://picx.zhimg.com/80/v2-2bdb0ecbbc3a0a2420781e472b68ba52.png"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/2024/06/05/Paper/2024-06-05/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-74LZ5BEQQ1")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!0,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"简"},noticeOutdate:{limitDay:500,position:"top",messagePrev:"It has been",messageNext:"days since the last update, the content of the article may be outdated."},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:200},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!0,post:!0},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!0,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"元宇宙/虚拟人",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-06-05 17:23:37"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach((e=>{n.setAttribute(e,t[e])})),document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)})),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=24?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 7.0.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml"></head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{const d=document.getElementById("loading-box"),e=document.body,o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l(),window.addEventListener("load",(()=>{o()}))})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">146</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url('https://picx.zhimg.com/80/v2-2bdb0ecbbc3a0a2420781e472b68ba52.png')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">元宇宙/虚拟人</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-06-05T09:23:37.000Z" title="发表于 2024-06-05 17:23:37">2024-06-05</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-06-05T09:23:37.857Z" title="更新于 2024-06-05 17:23:37">2024-06-05</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">5k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>19分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="元宇宙/虚拟人"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-06-05-更新"><a href="#2024-06-05-更新" class="headerlink" title="2024-06-05 更新"></a>2024-06-05 更新</h1><h2 id="Representing-Animatable-Avatar-via-Factorized-Neural-Fields"><a href="#Representing-Animatable-Avatar-via-Factorized-Neural-Fields" class="headerlink" title="Representing Animatable Avatar via Factorized Neural Fields"></a>Representing Animatable Avatar via Factorized Neural Fields</h2><p><strong>Authors:Chunjin Song, Zhijie Wu, Bastian Wandt, Leonid Sigal, Helge Rhodin</strong></p><p>For reconstructing high-fidelity human 3D models from monocular videos, it is crucial to maintain consistent large-scale body shapes along with finely matched subtle wrinkles. This paper explores the observation that the per-frame rendering results can be factorized into a pose-independent component and a corresponding pose-dependent equivalent to facilitate frame consistency. Pose adaptive textures can be further improved by restricting frequency bands of these two components. In detail, pose-independent outputs are expected to be low-frequency, while highfrequency information is linked to pose-dependent factors. We achieve a coherent preservation of both coarse body contours across the entire input video and finegrained texture features that are time variant with a dual-branch network with distinct frequency components. The first branch takes coordinates in canonical space as input, while the second branch additionally considers features outputted by the first branch and pose information of each frame. Our network integrates the information predicted by both branches and utilizes volume rendering to generate photo-realistic 3D human images. Through experiments, we demonstrate that our network surpasses the neural radiance fields (NeRF) based state-of-the-art methods in preserving high-frequency details and ensuring consistent body contours.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.00637v1">PDF</a></p><p><strong>Summary</strong><br>单目视频重建高保真人体3D模型，提出基于双分支网络的方法，分离姿态独立和姿态相关频率成分，实现大尺度身形和细腻皱纹的一致性重建。</p><p><strong>Key Takeaways</strong><br>• 单目视频重建高保真人体3D模型需要维持大尺度身形和细腻皱纹的一致性。<br>• 将per-frame渲染结果分解为姿态独立和姿态相关两部分，以实现帧一致性。<br>• 姿态adaptive纹理可以通过限制频率带来改善。<br>• 姿态独立输出应为低频率，而高频率信息与姿态相关。<br>• 双分支网络可以同时保留大尺度身形和细腻纹理特征。<br>• 网络集成了两分支预测的信息，并使用体渲染生成逼真的3D人像。<br>• 实验结果表明，所提方法超过基于NeRF的最新方法，在保留高频细节和确保身形一致性方面。</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: 通过因子化神经场表示可动画化Avatar / Representing Animatable Avatar via Factorized Neural Fields</p></li><li><p>Authors: Chunjin Song, Zhijie Wu, Bastian Wandt, Leonid Sigal, Helge Rhodin</p></li><li><p>Affiliation: 英属哥伦比亚大学</p></li><li><p>Keywords: neural radiance fields, 3D human avatars, monocular videos, pose-independent component, pose-dependent equivalent</p></li><li><p>Urls: arXiv:2406.00637v1 , Github: None</p></li><li><p>Summary:</p></li></ol><ul><li><p>(1):本文研究背景是从单目视频重建高保真的人类3D模型，需要保持一致的大尺度身体形状和精细的细节纹理。</p></li><li><p>(2):过去的方法是学习神经辐射场（NeRF）模型，并通过骨骼和限制姿态依赖的变化来约束模型。但是，这些方法存在过拟合的风险，无法保留高频细节，导致形状和纹理出现artifact。</p></li><li><p>(3):本文提出了一种基于双分支网络的方法，第一个分支输入canonical空间坐标，第二个分支考虑第一个分支输出和每帧的姿态信息，然后将两个分支的信息集成起来，并使用体积渲染生成照片-realistic 3D人像。</p></li><li><p>(4):本文的方法在保持一致的大尺度身体形状和精细的细节纹理方面超越了基于NeRF的state-of-the-art方法。</p></li></ul><ol><li>方法：</li></ol><ul><li><p>(1)：首先，估计输入帧的身体姿态，表示为关节角度序列[θk]N k=1，然后使用骨骼变形将观察空间中的查询点xo变形到规范空间中的坐标xc。</p></li><li><p>(2)：然后，将计算的坐标xc输入双支网络中，输出 pose-independent SDF 值{s1}和 RGB 值{c1}，以及 pose-dependent 对应值{s2}和{c2}。</p></li><li><p>(3)：接着，将 pose-independent 和 pose-dependent 输出合并，计算最终的 SDF 和颜色输出{s, c}，用于后续处理。</p></li><li><p>(4)：最后，使用体积渲染生成照片-realistic 3D 人像，计算损失函数并优化模型参数。</p></li><li><p>(5)：为了免上支网络退化和下支网络占据主要信息，我们还使用 pose-independent SDF 和颜色组件{s1, c1}渲染人像，计算共同损失函数Lcom。</p></li><li><p>(6)：此，我们还使用了感知损失LPIPS，以提供对轻微 Misalignments 和阴影变化的robustness，并改善重建中的细节。</p></li><li><p>(7)：最后，我们将所有损失函数组合成一个整体损失函数L，以优化模型参数。</p></li></ul><ol><li>Conclusion:</li></ol><ul><li><p>(1):本文的工作对从单目视频重建高保真的人类3D模型具有重要意义，能够在虚拟现实、电影制作、游戏等领域具有广泛的应用前景。</p></li><li><p>(2):Innovation point: 本文提出的基于双分支网络的方法创新地解决了NeRF模型的过拟合问题，能够保持一致的大尺度身体形状和精细的细节纹理；Performance: 本文的方法在保持一致的大尺度身体形状和精细的细节纹理方面超越了基于NeRF的state-of-the-art方法；Workload: 本文的方法需要大量的计算资源和数据集，限制了其在实际应用中的普及。</p></li></ul><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/f6474da82b1fcbd38b06a4fbe40f11d9241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/9d14e76216861610dcab63ff7e0dac16241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/9f3a4e3ed0db96a39537058e63e5b076241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/906ecc42153b2a728d3ba358bb463c70241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/bd677906cc26e45a40b64066f0d23e40241286257.jpg" align="middle"></details><h2 id="Stratified-Avatar-Generation-from-Sparse-Observations"><a href="#Stratified-Avatar-Generation-from-Sparse-Observations" class="headerlink" title="Stratified Avatar Generation from Sparse Observations"></a>Stratified Avatar Generation from Sparse Observations</h2><p><strong>Authors:Han Feng, Wenchao Ma, Quankai Gao, Xianwei Zheng, Nan Xue, Huijuan Xu</strong></p><p>Estimating 3D full-body avatars from AR/VR devices is essential for creating immersive experiences in AR/VR applications. This task is challenging due to the limited input from Head Mounted Devices, which capture only sparse observations from the head and hands. Predicting the full-body avatars, particularly the lower body, from these sparse observations presents significant difficulties. In this paper, we are inspired by the inherent property of the kinematic tree defined in the Skinned Multi-Person Linear (SMPL) model, where the upper body and lower body share only one common ancestor node, bringing the potential of decoupled reconstruction. We propose a stratified approach to decouple the conventional full-body avatar reconstruction pipeline into two stages, with the reconstruction of the upper body first and a subsequent reconstruction of the lower body conditioned on the previous stage. To implement this straightforward idea, we leverage the latent diffusion model as a powerful probabilistic generator, and train it to follow the latent distribution of decoupled motions explored by a VQ-VAE encoder-decoder model. Extensive experiments on AMASS mocap dataset demonstrate our state-of-the-art performance in the reconstruction of full-body motions.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.20786v2">PDF</a> Accepted by CVPR 2024 (Oral)</p><p><strong>Summary</strong><br>基于AR/VR设备估计3D全身avatar，提出分层方法，先重建上半身，然后重建下半身，获得state-of-the-art性能。</p><p><strong>Key Takeaways</strong><br>• AR/VR应用中，估计3D全身avatar是创建沉浸式体验的关键。<br>• Head Mounted Devices只能捕捉头部和手部的稀疏观察，预测全身avatar尤其是下半非常困难。<br>• 本文提出分层方法，先重建上半身，然后重建下半身，以解决这个问题。<br>• 使用latent diffusion model和VQ-VAE encoder-decoder模型来实现分层重建。<br>• 在AMASS mocap数据集上的实验表明，所提出的方法获得state-of-the-art性能。<br>• 分层方法可以 decouple 上半身和下半身的重建，提高重建精度。<br>• 本文的方法有助于创建更加逼真的AR/VR体验。</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>Title: 分层化avatar生成从稀疏观察（Stratified Avatar Generation from Sparse Observations）</li></ol><ol><li>Authors: (No author list provided)</li></ol><ol><li>Affiliation: 武汉大学计算机学院</li></ol><ol><li>Keywords: Avatar Generation, Sparse Observations, Latent Diffusion Model, VQ-VAE</li></ol><ol><li>Urls: (No link provided), Github:None</li></ol><ol><li>Summary:</li></ol><pre><code>- (1):该论文的研究背景是从AR/VR设备中估计3D全身avatar，这项任务具有挑战性，因为Head Mounted Devices只能捕捉到头部和手部的稀疏观察。

- (2):过去的方法尝试使用单个模型来重建整个身体avatar，但是这种方法存在问题，例如难以重建下半身。该论文的方法是motivated的，因为它借鉴了SMPL模型中kinematic树的性质，提出了一种分层化的方法来重建avatar。

- (3):该论文提出的研究方法是使用latent diffusion model来生成avatar，并使用VQ-VAE encoder-decoder型来探索decoupled motions的潜在分布。

- (4):该论文的方法在AMASS mocap数据集上获得了state-of-the-art性能，在重建全身运动方面取得了优异的成果，支持了该论文的目标。
</code></pre><p>sorry, 您的ip已由于触发防滥用检测而被封禁,可能是因为使用云服务器如腾讯云或者国外代理(vpn)访问本网站，如果使用了vpn，关闭vpn或代理即可继续使用,本服务网址是<a target="_blank" rel="noopener" href="https://chat18.aichatos8.com">https://chat18.aichatos8.com</a> 如需合作接口调用请联系微信chatkf123 或者前往 <a target="_blank" rel="noopener" href="https://binjie09.shop">https://binjie09.shop</a> 自助购买key, 认为是误封需要解封的请前往<a target="_blank" rel="noopener" href="https://www.ip.cn/">https://www.ip.cn/</a> 查询ip信息,并发送信息至邮件 gpt33@binjie.site ，站长会定期看邮件并处理解封和合作问题，如需调用接口请见接口文档<a target="_blank" rel="noopener" href="https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123">https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123</a> 或者前往 <a target="_blank" rel="noopener" href="https://cat.gptxyy.cn">https://cat.gptxyy.cn</a> 注册使用（可付费使用gpt4 注册可免费使用3.5）</p><ol><li>Conclusion:</li></ol><ul><li>(1):该论文的工作意义在于提出了一种新颖的分层化avatar生成方法，从稀疏观察中重建3D全身avatar，具有重要的理论和实践价值，能够推动AR/VR技术的发展。</li></ul><ul><li>(2):Innovation point: 该论文的创��点在于借鉴了SMPL模型中kinematic树的性质，提出了一种分层化的方法来重建avatar，实现了从稀疏观察到全身avatar的生成；Performance: 该论文的方法在AMASS mocap数据集上获得了state-of-the-art性能，验证了该方法的有效性；Workload: 该论文的方法需要较高的计算资源和数据集支持，限制了其在实际应用中的使用。</li></ul><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/a8d570b14cd9ddf9717130c93c4b112a241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/ad45b61ccb3d12c899be07950d3f3931241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/13d9d975cb0ececfa240beb4467270ab241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/0e4790e06b92fc46158ce93ad6d7868f241286257.jpg" align="middle"></details><h2 id="NPGA-Neural-Parametric-Gaussian-Avatars"><a href="#NPGA-Neural-Parametric-Gaussian-Avatars" class="headerlink" title="NPGA: Neural Parametric Gaussian Avatars"></a>NPGA: Neural Parametric Gaussian Avatars</h2><p><strong>Authors:Simon Giebenhain, Tobias Kirschstein, Martin Rünz, Lourdes Agapito, Matthias Nießner</strong></p><p>The creation of high-fidelity, digital versions of human heads is an important stepping stone in the process of further integrating virtual components into our everyday lives. Constructing such avatars is a challenging research problem, due to a high demand for photo-realism and real-time rendering performance. In this work, we propose Neural Parametric Gaussian Avatars (NPGA), a data-driven approach to create high-fidelity, controllable avatars from multi-view video recordings. We build our method around 3D Gaussian Splatting for its highly efficient rendering and to inherit the topological flexibility of point clouds. In contrast to previous work, we condition our avatars’ dynamics on the rich expression space of neural parametric head models (NPHM), instead of mesh-based 3DMMs. To this end, we distill the backward deformation field of our underlying NPHM into forward deformations which are compatible with rasterization-based rendering. All remaining fine-scale, expression-dependent details are learned from the multi-view videos. To increase the representational capacity of our avatars, we augment the canonical Gaussian point cloud using per-primitive latent features which govern its dynamic behavior. To regularize this increased dynamic expressivity, we propose Laplacian terms on the latent features and predicted dynamics. We evaluate our method on the public NeRSemble dataset, demonstrating that NPGA significantly outperforms the previous state-of-the-art avatars on the self-reenactment task by 2.6 PSNR. Furthermore, we demonstrate accurate animation capabilities from real-world monocular videos.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.19331v1">PDF</a> Project Page: see <a target="_blank" rel="noopener" href="https://simongiebenhain.github.io/NPGA/">https://simongiebenhain.github.io/NPGA/</a> ; Youtube Video: see <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=NGRxAYbIkus">https://www.youtube.com/watch?v=NGRxAYbIkus</a></p><p>sorry, 您的ip已由于触发防滥用检测而被封禁,可能是因为使用云服务器如腾讯云或者国外代理(vpn)访问本网站，如果使用了vpn，关闭vpn或代理即可继续使用,本服务网址是<a target="_blank" rel="noopener" href="https://chat18.aichatos8.com">https://chat18.aichatos8.com</a> 如需合作接口调用请联系微信chatkf123 或者前往 <a target="_blank" rel="noopener" href="https://binjie09.shop">https://binjie09.shop</a> 自助购买key, 认为是误封需要解封的请前往<a target="_blank" rel="noopener" href="https://www.ip.cn/">https://www.ip.cn/</a> 查询ip信息,并发送信息至邮件 gpt33@binjie.site ，站长会定期看邮件并处理解封和合作问题，如需调用接口请见接口文档<a target="_blank" rel="noopener" href="https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123">https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123</a> 或者前往 <a target="_blank" rel="noopener" href="https://cat.gptxyy.cn">https://cat.gptxyy.cn</a> 注册使用（可付费使用gpt4 注册可免费使用3.5）</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>Title: NPGA：神经参数高斯头像_avatar（Neural Parametric Gaussian Avatars）</li></ol><ol><li>Authors: SIMON GIEBENHAIN, TOBIAS KIRSCHSTEIN, MARTIN RÜNZ, LOURDES AGAPITO, MATTHIAS NIESSNER</li></ol><ol><li>Affiliation: 德国慕尼黑工业大学</li></ol><ol><li>Keywords: Neural Parametric Gaussian Avatars, 3D Gaussian Splatting, digital humans, avatar research</li></ol><ol><li>Urls: <a target="_blank" rel="noopener" href="https://simongiebenhain.github.io/NPGA/">https://simongiebenhain.github.io/NPGA/</a>, Github: None</li></ol><ol><li>Summary:</li></ol><pre><code>- (1):本文的研究背景是创建高保真度的数字头像avatar，用于电影、游戏、AR/VR_teleconferencing和metaverse等应用场景中。


- (2):过去的方法主要基于mesh-based 3DMMs，但是这些方法存在一些问题，如不够灵活、计算复杂等。本文的方法motivated by最近的研究进展，旨在解决这些问题。


- (3):本文提出的方法是基于神经参数高斯头像avatar（NPGA），该方法使用3D Gaussian Splatting来实现高效的渲染，并使用神经参数头模型（NPHM）来控制头像的表情。


- (4):本文的方法在NeRSemble数据集上的自reenactment任务中取得了良好的性能，超过了之前的state-of-the-art方法约2.6 PSNR。此外，本文还展示了从真实世界单目视频中进行精准动画的能力。
</code></pre><ol><li>方法：</li></ol><ul><li><p>(1):本文提出的NPGA方法基于神经参数头模型（NPHM），该模型使用多层感知机（MLP）来学习人头的参数化表示，捕捉头部的几何和表面信息。</p></li><li><p>(2):在NPHM的基础上，本文使用3D Gaussian Splatting来实现高效的渲���。该方法将头部表面分割成多个小块，每个小块被表示为一个高斯分布，从而实现高效的渲染。</p></li><li><p>(3):为了控制头像的表情，本文使用神经参数头模型（NPHM）来学习表情参数。该参数将被用于驱动头像的表情变化。</p></li><li><p>(4):在头像渲染过程中，本文使用NeRF（ Neural Radiance Fields）来模拟光照和阴影，从而实现更加真实的头像渲染。</p></li><li><p>(5):为了提高头像的真实，本文使用对抗损失函数来优化头像的生成过程，该函数鼓励头像生成器生成更加真实的头像。</p></li><li><p>(6):在实验中，本文使用NeRSemble数据集来评估NPGA方法的性能，并与之前的state-of-the-art方法进行比较。</p></li></ul><ol><li>Conclusion:</li></ol><ul><li>(1):该研究工作的重要性在于提出了一种基于神经参数高斯头像avatar（NPGA）的方法，实现了高保真度的数字头像avatar，具有广泛的应用前景在电影、游戏、AR/VR_teleconferencing和metaverse等领域中。</li></ul><ul><li>(2):Innovation point: 本文提出的NPGA方法将神经参数头模型（NPHM）与3D Gaussian Splatting相结合，实现了高效的渲染和控制头像的表情； Performance: 在NeRSemble数据集上的自reenactment任务中取得了良好的性能，超过了之前的state-of-the-art方法约2.6 PSNR； Workload: 该方法需要大量的计算资源和数据支持，且需要进一步优化和改进以满足实际应用的需求。</li></ul><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/a849a0d01c37753156a2a2d6b99162aa241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/2b6430b199d349a8ab6cd4ed84cfa7e1241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/6fc7a16a32260081d79304542cbf7a28241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/a7bd6e0457b4be281b181c23a57c526e241286257.jpg" align="middle"></details><h2 id="E-3-Gen-Efficient-Expressive-and-Editable-Avatars-Generation"><a href="#E-3-Gen-Efficient-Expressive-and-Editable-Avatars-Generation" class="headerlink" title="$E^{3}$Gen: Efficient, Expressive and Editable Avatars Generation"></a>$E^{3}$Gen: Efficient, Expressive and Editable Avatars Generation</h2><p><strong>Authors:Weitian Zhang, Yichao Yan, Yunhui Liu, Xingdong Sheng, Xiaokang Yang</strong></p><p>This paper aims to introduce 3D Gaussian for efficient, expressive, and editable digital avatar generation. This task faces two major challenges: (1) The unstructured nature of 3D Gaussian makes it incompatible with current generation pipelines; (2) the expressive animation of 3D Gaussian in a generative setting that involves training with multiple subjects remains unexplored. In this paper, we propose a novel avatar generation method named $E^3$Gen, to effectively address these challenges. First, we propose a novel generative UV features plane representation that encodes unstructured 3D Gaussian onto a structured 2D UV space defined by the SMPL-X parametric model. This novel representation not only preserves the representation ability of the original 3D Gaussian but also introduces a shared structure among subjects to enable generative learning of the diffusion model. To tackle the second challenge, we propose a part-aware deformation module to achieve robust and accurate full-body expressive pose control. Extensive experiments demonstrate that our method achieves superior performance in avatar generation and enables expressive full-body pose control and editing. Our project page is <a target="_blank" rel="noopener" href="https://olivia23333.github.io/E3Gen">https://olivia23333.github.io/E3Gen</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.19203v2">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://olivia23333.github.io/E3Gen">https://olivia23333.github.io/E3Gen</a></p><p>sorry, 您的ip已由于触发防滥用检测而被封禁,可能是因为使用云服务器如腾讯云或者国外代理(vpn)访问本网站，如果使用了vpn，关闭vpn或代理即可继续使用,本服务网址是<a target="_blank" rel="noopener" href="https://chat18.aichatos8.com">https://chat18.aichatos8.com</a> 如需合作接口调用请联系微信chatkf123 或者前往 <a target="_blank" rel="noopener" href="https://binjie09.shop">https://binjie09.shop</a> 自助购买key, 认为是误封需要解封的请前往<a target="_blank" rel="noopener" href="https://www.ip.cn/">https://www.ip.cn/</a> 查询ip信息,并发送信息至邮件 gpt33@binjie.site ，站长会定期看邮件并处理解封和合作问题，如需调用接口请见接口文档<a target="_blank" rel="noopener" href="https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123">https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123</a> 或者前往 <a target="_blank" rel="noopener" href="https://cat.gptxyy.cn">https://cat.gptxyy.cn</a> 注册使用（可付费使用gpt4 注册可免费使用3.5）</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: 𝐸3Gen：Efficient, Expressive and Editable Avatars Generation（高效、表达性强且可编辑的数字化_avatar生成）</p></li><li><p>Authors: Weitian Zhang, Yichao Yan, Yunhui Liu, Xingdong Sheng, Xiaokang Yang</p></li><li><p>Affiliation: 上海交通大学</p></li><li><p>Keywords: Avatar Generation, 3D Gaussian, Diffusion Model, Expressive Pose Control</p></li><li><p>Urls: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.19203v2">https://arxiv.org/abs/2405.19203v2</a> , Github:None</p></li><li><p>Summary:</p></li></ol><pre><code>- (1):本文研究��景是数字化_avatar生成，旨在实现高效、表达性强且可编辑的_avatar生成。

- (2):过去的方法存在两个主要挑战：一是三维高斯分布的非结构化特性使其与当前生成流水线不兼容；二是三维高斯分布在生成设置中的表达性动画仍未被探索。本文motivated by解决这些挑战。

- (3):本文提出了一种名为𝐸3Gen的avatar生成方法，首先将三维高斯分布编码到结构化的二维UV空间中，然后使用部分感知形变模块实现鲁棒而准确的全身pose控制。

- (4):本文的方法在_avatar生成任务上取得了优异的性能，实现了高保真度的_avatar生成、实时渲染和编辑能力，支持多种视角和pose控制，证明了其方法的有效性。
</code></pre><ol><li>方法：</li></ol><ul><li><p>(1):提出了一种名为𝐸3Gen的avatar生成方法，该方法首先将三高斯分布编码到结构化的二维UV空间中，然后使用部分感知形变模块实现鲁棒而准确的全身pose控制。</p></li><li><p>(2):在UV特征平面表示中，引入了三维高斯分布，实现高效、表达性强且可编辑的数字化_avatar生成。</p></li><li><p>(3):使用单阶段扩散模型同时训练去噪和拟合过程，UV特征平面𝑥𝑖被随机初始化并由去噪和拟合过程共同优化。</p></li><li><p>(4):在去噪过程中，添加噪声到UV特征平面，然后使用基于v-parameterization方案的去噪UNet对其进行去噪。</p></li><li><p>(5):在拟合过程中，UV特征平面被解码成高斯属性图，然后使用这些属性图生成三维高斯分布-based avatar在规范pose空间中。</p></li><li><p>(6):使用部分感知形变模块将avatar变形到目标pose空间中，以实现全身pose控制包括面部表情和手势动画。</p></li><li><p>(7):在训练过程中，使用了light-weight共享解码器𝐷𝑎和𝐷𝑔来预测高斯属性图，实现了快速推理和高质量生成。</p></li><li><p>(8):使用biliniear插值来查高斯primitive的属性，从而获取了数字化avatar的表示。</p></li><li><p>(9):在变形模块中，使用了线性blend skinning技术来实现avatar的变形，并计算了皮肤权重场来确定每个关节对avatar的影响。</p></li><li><p>(10):使用了低分辨率体积来表示皮肤权重场，以实现鲁棒和平滑的变形。</p></li></ul><ol><li>Conclusion:</li></ol><ul><li><p>(1):本文的工作对于数字化_avatar生成领域具有重要的意义，因为它解决了三维高斯分布的非结构化特性和表达性动画的问题，实现了高效、表达性强且可编辑的_avatar生成。</p></li><li><p>(2):Innovation point: 本文提出了一种创新性的UV特征平面表示方法，实现了三维高斯分布的结构化编码，并引入了部分感知形变模块实现棒而准确的全身pose控制；<br>Performance: 本文的方法在_avatar生成任务上取得了优异的性能，实现了高保真的_avatar生成、实时渲染和编辑能力，支持多种视角和pose控制；<br>Workload: 本文的方法需要大量的数据和计算资源来训练扩散模型和去噪UNet，但可以实现快速推理和高质量生成。</p></li></ul><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/be8861336dd5c32e1556032f03ce9f9d241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/13e4caf12e93db0b8bcb907899743af4241286257.jpg" align="middle"></details></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io/2024/06/05/Paper/2024-06-05/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">https://kedreamix.github.io/2024/06/05/Paper/2024-06-05/元宇宙_虚拟人/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">元宇宙/虚拟人</a></div><div class="post_share"><div class="social-share" data-image="https://picx.zhimg.com/80/v2-2bdb0ecbbc3a0a2420781e472b68ba52.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/06/05/Paper/2024-06-05/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/80/v2-890676236f48f9a7d915a0c42c40aa38.png" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Diffusion Models</div></div></a></div><div class="next-post pull-right"><a href="/2024/05/28/Paper/2024-05-28/NeRF/" title="NeRF"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-46b90894aa28846d98c1eef5c5a89f0c.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">NeRF</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/01/24/Paper/2024-01-24/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-0f13a2b60bef4c886a3317754c99b456.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-24</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/04/01/Paper/2024-04-01/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-4f2a829065c463be027e4b423c4e43c8.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-01</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/04/06/Paper/2024-04-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-4f97970c093585e18e2db42fb96a6b75.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-06</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/05/02/Paper/2024-05-02/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-37516691b13dabbabb3b74ea46b402d8.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-02</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/05/06/Paper/2024-05-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-65e21e3a0a320adc36f81e6bfc7c5739.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-06</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div><div><a href="/2024/05/22/Paper/2024-05-22/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙&#x2F;虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-b4943be56cf94e02149b50eec541d27f.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-22</div><div class="title">元宇宙&#x2F;虚拟人</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-06-05-%E6%9B%B4%E6%96%B0"><span class="toc-text">2024-06-05 更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Representing-Animatable-Avatar-via-Factorized-Neural-Fields"><span class="toc-text">Representing Animatable Avatar via Factorized Neural Fields</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Stratified-Avatar-Generation-from-Sparse-Observations"><span class="toc-text">Stratified Avatar Generation from Sparse Observations</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#NPGA-Neural-Parametric-Gaussian-Avatars"><span class="toc-text">NPGA: Neural Parametric Gaussian Avatars</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#E-3-Gen-Efficient-Expressive-and-Editable-Avatars-Generation"><span class="toc-text">$E^{3}$Gen: Efficient, Expressive and Editable Avatars Generation</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image:url('https://picx.zhimg.com/80/v2-2bdb0ecbbc3a0a2420781e472b68ba52.png')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js").then((()=>{pangu.autoSpacingPage()}))}function panguInit(){panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.1},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script></div><script id="canvas_nest" defer color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><script data-pjax>function butterfly_swiper_injector_config(){var a=document.getElementById("recent-posts");console.log("已挂载butterfly_swiper"),a.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/25/Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="2024/01/25/Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="2024/01/25/Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/17/Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023/12/17/Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="2023/12/17/Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script></body></html>