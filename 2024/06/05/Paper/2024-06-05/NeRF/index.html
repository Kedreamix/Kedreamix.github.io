<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>NeRF | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="NeRF 方向最新论文已更新，请持续关注 Update in 2024-06-05  Self-Calibrating 4D Novel View Synthesis from Monocular Videos Using   Gaussian Splatting"><meta property="og:type" content="article"><meta property="og:title" content="NeRF"><meta property="og:url" content="https://kedreamix.github.io/2024/06/05/Paper/2024-06-05/NeRF/index.html"><meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World"><meta property="og:description" content="NeRF 方向最新论文已更新，请持续关注 Update in 2024-06-05  Self-Calibrating 4D Novel View Synthesis from Monocular Videos Using   Gaussian Splatting"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://picx.zhimg.com/80/v2-2bdb0ecbbc3a0a2420781e472b68ba52.png"><meta property="article:published_time" content="2024-06-05T11:22:30.000Z"><meta property="article:modified_time" content="2024-06-05T11:22:30.939Z"><meta property="article:author" content="Kedreamix"><meta property="article:tag" content="NeRF"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://picx.zhimg.com/80/v2-2bdb0ecbbc3a0a2420781e472b68ba52.png"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/2024/06/05/Paper/2024-06-05/NeRF/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-74LZ5BEQQ1")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!0,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"简"},noticeOutdate:{limitDay:500,position:"top",messagePrev:"It has been",messageNext:"days since the last update, the content of the article may be outdated."},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:200},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!0,post:!0},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!0,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"NeRF",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-06-05 19:22:30"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach((e=>{n.setAttribute(e,t[e])})),document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)})),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=24?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 7.0.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml"></head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{const d=document.getElementById("loading-box"),e=document.body,o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l(),window.addEventListener("load",(()=>{o()}))})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">146</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url('https://picx.zhimg.com/80/v2-2bdb0ecbbc3a0a2420781e472b68ba52.png')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">NeRF</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-06-05T11:22:30.000Z" title="发表于 2024-06-05 19:22:30">2024-06-05</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-06-05T11:22:30.939Z" title="更新于 2024-06-05 19:22:30">2024-06-05</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">7.4k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>28分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="NeRF"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-06-05-更新"><a href="#2024-06-05-更新" class="headerlink" title="2024-06-05 更新"></a>2024-06-05 更新</h1><h2 id="Self-Calibrating-4D-Novel-View-Synthesis-from-Monocular-Videos-Using-Gaussian-Splatting"><a href="#Self-Calibrating-4D-Novel-View-Synthesis-from-Monocular-Videos-Using-Gaussian-Splatting" class="headerlink" title="Self-Calibrating 4D Novel View Synthesis from Monocular Videos Using   Gaussian Splatting"></a>Self-Calibrating 4D Novel View Synthesis from Monocular Videos Using Gaussian Splatting</h2><p><strong>Authors:Fang Li, Hao Zhang, Narendra Ahuja</strong></p><p>Gaussian Splatting (GS) has significantly elevated scene reconstruction efficiency and novel view synthesis (NVS) accuracy compared to Neural Radiance Fields (NeRF), particularly for dynamic scenes. However, current 4D NVS methods, whether based on GS or NeRF, primarily rely on camera parameters provided by COLMAP and even utilize sparse point clouds generated by COLMAP for initialization, which lack accuracy as well are time-consuming. This sometimes results in poor dynamic scene representation, especially in scenes with large object movements, or extreme camera conditions e.g. small translations combined with large rotations. Some studies simultaneously optimize the estimation of camera parameters and scenes, supervised by additional information like depth, optical flow, etc. obtained from off-the-shelf models. Using this unverified information as ground truth can reduce robustness and accuracy, which does frequently occur for long monocular videos (with e.g. &gt; hundreds of frames). We propose a novel approach that learns a high-fidelity 4D GS scene representation with self-calibration of camera parameters. It includes the extraction of 2D point features that robustly represent 3D structure, and their use for subsequent joint optimization of camera parameters and 3D structure towards overall 4D scene optimization. We demonstrate the accuracy and time efficiency of our method through extensive quantitative and qualitative experimental results on several standard benchmarks. The results show significant improvements over state-of-the-art methods for 4D novel view synthesis. The source code will be released soon at <a target="_blank" rel="noopener" href="https://github.com/fangli333/SC-4DGS">https://github.com/fangli333/SC-4DGS</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.01042v1">PDF</a> GitHub Page: <a target="_blank" rel="noopener" href="https://github.com/fangli333/SC-4DGS">https://github.com/fangli333/SC-4DGS</a></p><p><strong>Summary</strong><br>GS 方法提升了场景重建和新视图合成的效率和准确性，但现有的 4D NVS 方法依赖 COLMAP 提供的相机参数和稀��点云，存在准确性和时间效率问题。</p><p><strong>Key Takeaways</strong><br>• GS 方法相比 NeRF 提高了场景重建和新视图合成的效率和准确性，特别是在动态场景中。<br>• 现有的 4D NVS 方法依赖 COLMAP 提供的相机参数和稀疏点云，存在准确性和时间效率问题。<br>• 部分研究同时优化相机参数和场景估计，但使用未经验证的信息作为ground truth可能会降低鲁棒性和准确性。<br>• 本文提出了一种新的方法，学习高保真 4D GS 场景表示，并自行校准相机参数。<br>• 该方法包括提取 robustly 表示 3D 结构的 2D 点特征，并将其用于联合优化相机参数和 3D 结构。<br>• 实验结果表明该方法在多个标准基准测试中取得了显著的改进。<br>• 代码将在 <a target="_blank" rel="noopener" href="https://github.com/fangli333/SC-4DGS">https://github.com/fangli333/SC-4DGS</a> 上发布。</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: 自适应4D新视图合成从单ocular视频使用高斯点描绑定（Self-Calibrating 4D Novel View Synthesis from Monocular Videos Using Gaussian Splatting）</p></li><li><p>Authors: Fang Li, Hao Zhang, Narendra Ahuja</p></li><li><p>Affiliation: 伊利诺伊大学厄巴纳-香槟分校</p></li><li><p>Keywords: Novel View Synthesis, Gaussian Splatting, Self-Calibration, Camera Parameters, 4D Scene Representation</p></li><li><p>Urls: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01042v1">https://arxiv.org/abs/2406.01042v1</a> , Github: <a target="_blank" rel="noopener" href="https://github.com/fangli333/SC-4DGS">https://github.com/fangli333/SC-4DGS</a></p></li><li><p>Summary:</p></li></ol><ul><li><p>(1):本文研究背景是四维新视图合成（4D Novel View Synthesis），特别是动态场景的重建和新视图合成。</p></li><li><p>(2):过去的方法主要基于NeRF和Gaussian Splatting，但这些方法依赖COLMAP提供的摄像机参数和稀疏点云，存在准确性和时间效率问题。同时，某些研究使用额外信息（如深度、光流等）监督摄像机参数和场景估计，但这种方法可能会降低鲁棒性和准确性。</p></li><li><p>(3):本文提出了一种新的方法，使用高斯点描绑定（Gaussian Splatting）学习四维高保真场景表示，同时进行摄像机参数的自适应校准。该方法包括提取二维点特征、联合优化摄像机参数和三维结构以实现四维场景优化。</p></li><li><p>(4):实验结果表明，本文方法在四维新视图合成任务上取得了显著的改进，超过了当前最先进的方法。</p></li></ul><ol><li>方法：</li></ol><ul><li><p>(1): 提取二维点特征：从单ocular视频中提取二维点特征，使用基于角点检测的方法（如SIFT、ORB等）或基于深度学习的方法（如SuperPoint等）;</p></li><li><p>(2): 高斯点描绑定（Gaussian Splatting）：使用高斯点描绑定方法将维点特征投影到三维空间，获得初始的三维点云;</p></li><li><p>(3): 联合优化摄像机参数和三维结构：使用 Bundle Adjustment 算法联合优化摄像机参数和三维结构，以最小化重投影误差和点云误差;</p></li><li><p>(4): 四维场景优化：使用化后的摄像机参数和三维结构，学习四维高保真场景表示，包场景几何和反射率信息;</p></li><li><p>(5): 自适应校准：在优化过程中，自适应地校准摄像机参数，以适应动态场景的变化;</p></li><li><p>(6): 新视图合成：使用学习到的四维场景表示，合成视图，实现四维新视图合成任务。</p></li></ul><ol><li>Conclusion:</li></ol><ul><li><p>(1):本文的工作对四维新视图合成领域具有重要意义，因为它解决了摄像机参数的自适应校准问题，提高了四维场景表示的准确性和鲁棒性。</p></li><li><p>(2):Innovation point: 本文提出了一种新的方法，使用高斯点描绑定学习四维高保真场景表示，同时进行摄像机参数的自适配校准，具有创新性和原创性；Performance: 实验结果表明，本文方法在四维新视图合成任务上取得了显著的改进，超过了当前最先进的方法；Workload: 本文方法需要大量的计算资源和时间，且需要高质量的视频输入，以确保四维场景表示的准确性。</p></li></ul><p>Note: Please fill in the corresponding content according to the actual requirements.</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/e828338e21222261fe938fb0df79eb64241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/a8ab4f92c604312081326135aac48003241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/e60020bb965ee67ddf7e10fe9d0d833a241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/9028323f469c7b9a82e97a9c1593cc97241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/d8089002972843214017e5370b2107d4241286257.jpg" align="middle"></details><h2 id="PruNeRF-Segment-Centric-Dataset-Pruning-via-3D-Spatial-Consistency"><a href="#PruNeRF-Segment-Centric-Dataset-Pruning-via-3D-Spatial-Consistency" class="headerlink" title="PruNeRF: Segment-Centric Dataset Pruning via 3D Spatial Consistency"></a>PruNeRF: Segment-Centric Dataset Pruning via 3D Spatial Consistency</h2><p><strong>Authors:Yeonsung Jung, Heecheol Yun, Joonhyung Park, Jin-Hwa Kim, Eunho Yang</strong></p><p>Neural Radiance Fields (NeRF) have shown remarkable performance in learning 3D scenes. However, NeRF exhibits vulnerability when confronted with distractors in the training images — unexpected objects are present only within specific views, such as moving entities like pedestrians or birds. Excluding distractors during dataset construction is a straightforward solution, but without prior knowledge of their types and quantities, it becomes prohibitively expensive. In this paper, we propose PruNeRF, a segment-centric dataset pruning framework via 3D spatial consistency, that effectively identifies and prunes the distractors. We first examine existing metrics for measuring pixel-wise distraction and introduce Influence Functions for more accurate measurements. Then, we assess 3D spatial consistency using a depth-based reprojection technique to obtain 3D-aware distraction. Furthermore, we incorporate segmentation for pixel-to-segment refinement, enabling more precise identification. Our experiments on benchmark datasets demonstrate that PruNeRF consistently outperforms state-of-the-art methods in robustness against distractors.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.00798v1">PDF</a></p><p><strong>Summary</strong><br>NeRF prune框架PruNeRF，通过3D空间一致性和分割技术，有效地识别和删除训练图像中的干扰物体。</p><p><strong>Key Takeaways</strong><br>• NeRF存在对训练图像中的干扰物体的脆弱性，例如移动的行人或鸟类。<br>• PruNeRF框架通过3D空间一致性和分割技术来识别和删除干扰物体。<br>• PruNeRF使用影响函数来准确测量像素级别的干扰度。<br>• PruNeRF使用基于深度的重投影技术来评估3D空间一致性。<br>• PruNeRF结合分割技术来实现像素到分割的细化，提高干扰物体的识别精度。<br>• PruNeRF在基准数据集上的实验结果表明，它比现有的方法更好地抵御干扰物体。<br>• PruNeRF可以提高NeRF在学习3D场景中的鲁棒性。</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>Title: PruNeRF：基于3D空间一致性的分段中心数据修剪框架（PruNeRF: Segment-Centric Dataset Pruning via 3D Spatial Consistency）</li></ol><ol><li>Authors: Yeonsung Jung, Heecheol Yun, Joonhyung Park, Jin-Hwa Kim, Eunho Yang</li></ol><ol><li>Affiliation: 韩国科学技术院（KAIST）</li></ol><ol><li>Keywords: Neural Radiance Fields, 3D Scene Learning, Dataset Pruning, 3D Spatial Consistency</li></ol><ol><li>Urls: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.00798">https://arxiv.org/abs/2406.00798</a> , Github: None</li></ol><ol><li>Summary:</li></ol><pre><code>- (1):这篇论文的研究背景是Neural Radiance Fields（NeRF）在学习3D场景中的应用，但是NeRF模型对训练图像中的干扰物体非常敏感，影响模型的性能。


- (2):过去的方法包括NeRF-W、RobustNeRF等，但是这些方法存在一些问题，如需要大量的人工干预、模型架构复杂、参数选择困难等。


- (3):这篇论文提出了PruNeRF框架，通过3D空间一致性来评估图像中的干扰物体，然后使用分段技术来精准地识别和修剪干扰物体。


- (4):实验结果表明，PruNeRF框架在多个基准数据集上都取得了state-of-the-art的性能，证明了该方法的有效性。
</code></pre><ol><li>方法：</li></ol><ul><li><p>(1)：定义干扰物体评估指标：本文提出了基于3D空间一致性的评估指标，以评估图像中的干扰物体，该指标能够量化图像中的干扰程度。</p></li><li><p>(2)：分段中心数据修剪：根据评估指标，对图像进行分段，识别出干扰物体，然后对其进行修剪，以减少干扰对对模型的影响。</p></li><li><p>(3)：PruNeRF框架的建立：将分段中心数据修剪技术与Neural Radiance Fields（NeRF）模型集成，形成PruNeRF框架，该框架能够学习3D场景同时避免干扰物体的影响。</p></li><li><p>(4)：损失函数设计：设计了一个损失函数，以惩罚模型对干扰物体敏感度，并鼓励模型学习到干扰物体无关的特征。</p></li><li><p>(5)：模型训练：使用PruNeRF框架和损失函数对模型进行训练，以学习到干扰物体无关的特征和3D场景的表示。</p></li><li><p>(6)：实验评估：在多个基准数据集上对PruNeRF框架进行评估，验证其在3D场景学习任务中的有效性。</p></li></ul><ol><li>Conclusion:</li></ol><ul><li><p>(1):这篇论文的贡献在于提出了一种基于3D空间一致性的分段中心数据修剪框架PruNeRF，有效地解决了NeRF模型对训练图像中的干扰物体敏感的问题，提高了模型的泛化能力和鲁棒性。</p></li><li><p>(2):创新点：PruNeRF框架的提出，基于3D空间一致性的评估指标和分段中心数据修剪技术的结合，解决了NeRF模型对干扰物体的敏感问题；性能：实验结果表明，PruNeRF框架在多个基准数据集上取得了state-of-the-art的性能；工作量：该方法需要设计损失函数和模型架构的修改，但整体来说工作量可控的。</p></li></ul><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/f0e570acab5e53be3c43e6586eda8e10241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/5a3922dbb4f41bc3ecf340c4e37bcf8d241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/2f85e97e8b1f4ecf6a7cc19e2b5541e9241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/9d77d1593aa8e12fa5a4c4effca042eb241286257.jpg" align="middle"></details><h2 id="Representing-Animatable-Avatar-via-Factorized-Neural-Fields"><a href="#Representing-Animatable-Avatar-via-Factorized-Neural-Fields" class="headerlink" title="Representing Animatable Avatar via Factorized Neural Fields"></a>Representing Animatable Avatar via Factorized Neural Fields</h2><p><strong>Authors:Chunjin Song, Zhijie Wu, Bastian Wandt, Leonid Sigal, Helge Rhodin</strong></p><p>For reconstructing high-fidelity human 3D models from monocular videos, it is crucial to maintain consistent large-scale body shapes along with finely matched subtle wrinkles. This paper explores the observation that the per-frame rendering results can be factorized into a pose-independent component and a corresponding pose-dependent equivalent to facilitate frame consistency. Pose adaptive textures can be further improved by restricting frequency bands of these two components. In detail, pose-independent outputs are expected to be low-frequency, while highfrequency information is linked to pose-dependent factors. We achieve a coherent preservation of both coarse body contours across the entire input video and finegrained texture features that are time variant with a dual-branch network with distinct frequency components. The first branch takes coordinates in canonical space as input, while the second branch additionally considers features outputted by the first branch and pose information of each frame. Our network integrates the information predicted by both branches and utilizes volume rendering to generate photo-realistic 3D human images. Through experiments, we demonstrate that our network surpasses the neural radiance fields (NeRF) based state-of-the-art methods in preserving high-frequency details and ensuring consistent body contours.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.00637v1">PDF</a></p><p><strong>Summary</strong><br>从单目视频重建高保真人体3D模型，提出分解pose独立和pose相关组件以保持帧一致性，并使用双支网络和体积渲染生成照片般的3D人体图像。</p><p><strong>Key Takeaways</strong><br>�� 为了重建高保真人体3D模型，需要保持大规模身体形状的一致性和细腻皱纹的匹配。<br>• 将每帧渲染结果分解为pose独立和pose相关组件，以提高帧一致性。<br>• pose独立输出预计为低频率，而高频率信息与pose相关因素相链接。<br>• 双支网络可以分别处理不同频率组件，生成高保真3D人体图像。<br>• 网络集成了两个分支的预测信息，并使用体积渲染生成照片般的3D人体图像。<br>• 该方法在保留高频率细节和保持身体轮廓的一致性方面超过了基于NeRF的最新方法。<br>• 实验结果证明了该方法有效性。</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>Title: 通过因子化神经场表示可动漫化Avatar / Representing Animatable Avatar via Factorized Neural Fields</li></ol><ol><li>Authors: Chunjin Song, Zhijie Wu, Bastian Wandt, Leonid Sigal, Helge Rhodin</li></ol><ol><li>Affiliation: 英属哥伦比亚大学</li></ol><ol><li>Keywords: Neural Radiance Fields, 3D human avatar, monocular video, pose-independent component, pose-dependent component</li></ol><ol><li>Urls: arXiv:2406.00637v1 , Github:None</li></ol><ol><li>Summary:</li></ol><pre><code>- (1):本文研究背景是从单目视频重建高保真的人类3D模型，需要同时保持大规模身体形状和细腻的纹理信息。


- (2):过去的方法主要是学习 Neural Radiance Field（NeRF）模型，但这些方法存在过拟合风险，难以保持高频细节信息，导致模型参数化平滑，出现形状和纹理 artifacts。


- (3):本文提出了一种双分支网络，分别学习姿态独立和姿态相关的频率成分，通过限制频率带来提高姿态适应性纹理质量。


- (4):本文方法在单目视频的人类3D模型重建任务上取得了state-of-the-art性能，保留了高频细节信息和一致的身体形状，支持了研究目标。
</code></pre><ol><li>方法：</li></ol><ul><li><p>(1)：从单目视频重建高保真的人类3D模型，需要同时保持大规模身体形状和细腻的纹理信息。</p></li><li><p>(2)：提出了一种双分支网络，分别学习姿态独立和姿态相关的频率成分，通过限制频率带来提高姿态适应性纹理质量。</p></li><li><p>(3)：使用 skeletal deformation 模型将观察空间中的 query 点 xo 转换到 canonical 空间中的坐标 xc。</p></li><li><p>(4)：将坐标 xc 输入到双分支网络中，输出姿态独立 SDF 值和颜色值以及姿态相关的对应值。</p></li><li><p>(5)：使用低频 positional encoding 来输出姿态独立的 SDF 值和颜色值，使用高频 positional encoding 来输出姿态相关的 SDF 值和颜色值。</p></li><li><p>(6)：将姿态独立和姿态相关的输出值合并，得到最终的 SDF 值和颜色值。</p></li><li><p>(7)：使用 SDF-based volume rendering 来生成图像，并计算 L1 损失函数和 Eikonal 损失函数。</p></li><li><p>(8)：使用 perceptual loss 来提供对 slight misalignments 和 shading variation 的 robustness，并改进重建的细节。</p></li><li><p>(9)：将所有损失函数组合，得到最终的损失函数，并使用反向传播算法来优化网络参数。</p></li><li><p>(10)：使用 ZJU-Mocap 数据集和 YouTube 序列来评估方法的有效性，并与 state-of-the-art 方法进行比较。</p></li></ul><p>sorry, 您的ip已由于触发防滥用检测而被封禁,可能是因为使用云服务器如腾讯云或者国外代理(vpn)访问本网站，如果使用了vpn，关闭vpn或代理即可继续使用,本服务网址是<a target="_blank" rel="noopener" href="https://chat18.aichatos8.com">https://chat18.aichatos8.com</a> 如需合作接口调用请联系微信chatkf123 或者前往 <a target="_blank" rel="noopener" href="https://binjie09.shop">https://binjie09.shop</a> 自助购买key, 认为是误封需要解封的请前往<a target="_blank" rel="noopener" href="https://www.ip.cn/">https://www.ip.cn/</a> 查询ip信息,并发送信息至邮件 gpt33@binjie.site ，站长会定期看邮件并处理解封和合作问题，如需调用接口请见接口文档<a target="_blank" rel="noopener" href="https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123">https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123</a> 或者前往 <a target="_blank" rel="noopener" href="https://cat.gptxyy.cn">https://cat.gptxyy.cn</a> 注册使用（可付费使用gpt4 注册可免费使用3.5）</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/0a0949896b8f2717b410a739d8f6d907241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/842e717182499acfd98431859dde9501241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/d17a97b75fa02b842f5a29a557fc54a5241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/7ebc614d1fba3159cc07b3499e2e7ab1241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/1faa5b873edc115dc7c919f8383ee41c241286257.jpg" align="middle"></details><h2 id="Bilateral-Guided-Radiance-Field-Processing"><a href="#Bilateral-Guided-Radiance-Field-Processing" class="headerlink" title="Bilateral Guided Radiance Field Processing"></a>Bilateral Guided Radiance Field Processing</h2><p><strong>Authors:Yuehao Wang, Chaoyi Wang, Bingchen Gong, Tianfan Xue</strong></p><p>Neural Radiance Fields (NeRF) achieves unprecedented performance in synthesizing novel view synthesis, utilizing multi-view consistency. When capturing multiple inputs, image signal processing (ISP) in modern cameras will independently enhance them, including exposure adjustment, color correction, local tone mapping, etc. While these processings greatly improve image quality, they often break the multi-view consistency assumption, leading to “floaters” in the reconstructed radiance fields. To address this concern without compromising visual aesthetics, we aim to first disentangle the enhancement by ISP at the NeRF training stage and re-apply user-desired enhancements to the reconstructed radiance fields at the finishing stage. Furthermore, to make the re-applied enhancements consistent between novel views, we need to perform imaging signal processing in 3D space (i.e. “3D ISP”). For this goal, we adopt the bilateral grid, a locally-affine model, as a generalized representation of ISP processing. Specifically, we optimize per-view 3D bilateral grids with radiance fields to approximate the effects of camera pipelines for each input view. To achieve user-adjustable 3D finishing, we propose to learn a low-rank 4D bilateral grid from a given single view edit, lifting photo enhancements to the whole 3D scene. We demonstrate our approach can boost the visual quality of novel view synthesis by effectively removing floaters and performing enhancements from user retouching. The source code and our data are available at: <a target="_blank" rel="noopener" href="https://bilarfpro.github.io">https://bilarfpro.github.io</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.00448v1">PDF</a> SIGGRAPH (ACM TOG), 2024. Project page: <a target="_blank" rel="noopener" href="https://bilarfpro.github.io">https://bilarfpro.github.io</a></p><p><strong>Summary</strong><br>NeRF合成新视图时，考虑到现代摄像机ISP处理对多视图一致性的影响，提出了一种基于3D ISP的方法来重新应用用户所需的图像增强。</p><p><strong>Key Takeaways</strong><br>• NeRF在新视图合成中受到现代摄像机ISP处理的影响，导致radiance fields重建中的“浮动”问题。<br>• 为了解决这个问题，需要在NeRF训练阶段分离ISP增强，并在最后阶段重新应用用户所需的增强。<br>• 本文提出使用3D ISP来重新应用图像增强，并使用双边网格来表示ISP处理。<br>• 双边网格可以近似摄像机管道的效果，并且可以根据单视图编辑学习低秩4D双边网格来实现用户可调整的3D图像增强。<br>• 该方法可以有效地消除radiance fields重建中的“浮动”问题，并提高新视图合成的视觉质量。<br>• 该方法可以应用于用户retouching，抬升整个3D场景的图像质量。<br>• 源代码和数据可以在<a target="_blank" rel="noopener" href="https://bilarfpro.github.io中找到。">https://bilarfpro.github.io中找到。</a></p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>Title: 双向引导辐射场处理 (Bilateral Guided Radiance Field Processing)</li></ol><ol><li>Authors: Yuehao Wang, Chaoyi Wang, Bingchen Gong, Tianfan Xue</li></ol><ol><li>Affiliation: 香港中文大学</li></ol><ol><li>Keywords: Neural Radiance Fields, Neural Rendering, Bilateral Grid, 3D Editing</li></ol><ol><li>Urls: <a target="_blank" rel="noopener" href="https://bilarfpro.github.io">https://bilarfpro.github.io</a>, Github: None</li></ol><ol><li>Summary:</li></ol><ul><li>(1):本文研究了Neural Radiance Fields（NeRF）在多视图图像合成中的应用，但摄像机的图像信号处理（ISP）会破坏多视图一致性，导致“浮动物体”出现。</li></ul><ul><li>(2):过去的方法包括使用Raw图像训练NeRF、采用GLO向量近似多视图不一致，但这些方法会降低渲染质量或不能准确地模拟ISP处理。</li></ul><ul><li>(3):本文提出了双向引导辐射场处理方法，使用双向网格近似ISP处理，并将其与NeRF训练相结合，消除浮动物体，并实现用户可调整的3D图像编辑。</li></ul><ul><li>(4):本文的方法在多个夜间拍摄场景、RawNeRF数据集和mip-NeRF 360数据集上取得了state-of-the-art性能，证明了该方法的有效性。</li></ul><p>sorry, 您的ip已由于触发防滥用检测而被封禁,可能是因为使用云服务器如腾讯云或者国外代理(vpn)访问本网站，如果使用了vpn，关闭vpn或代理即可继续使用,本服务网址是<a target="_blank" rel="noopener" href="https://chat18.aichatos8.com">https://chat18.aichatos8.com</a> 如需合作接口调用请联系微信chatkf123 或者前往 <a target="_blank" rel="noopener" href="https://binjie09.shop">https://binjie09.shop</a> 自助购买key, 认为是误封需要解封的请前往<a target="_blank" rel="noopener" href="https://www.ip.cn/">https://www.ip.cn/</a> 查询ip信息,并发送信息至邮件 gpt33@binjie.site ，站长会定期看邮件并处理解封和合作问题，如需调用接口请见接口文档<a target="_blank" rel="noopener" href="https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123">https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123</a> 或者前往 <a target="_blank" rel="noopener" href="https://cat.gptxyy.cn">https://cat.gptxyy.cn</a> 注册使用（可付费使用gpt4 注册可免费使用3.5）</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/2c6a114ca938d788acecc5d70c5c6a01241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/d11265710865186bc135a20b1772294f241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/c5fb01334ccd448ed0c271aa4aebfbc3241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/d49748d5da2bb7f5d85cbc2b900b92ce241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/6131d7e103f3e4a03e7e451fcb022e0c241286257.jpg" align="middle"></details><h2 id="NeRF-On-the-go-Exploiting-Uncertainty-for-Distractor-free-NeRFs-in-the-Wild"><a href="#NeRF-On-the-go-Exploiting-Uncertainty-for-Distractor-free-NeRFs-in-the-Wild" class="headerlink" title="NeRF On-the-go: Exploiting Uncertainty for Distractor-free NeRFs in the   Wild"></a>NeRF On-the-go: Exploiting Uncertainty for Distractor-free NeRFs in the Wild</h2><p><strong>Authors:Weining Ren, Zihan Zhu, Boyang Sun, Jiaqi Chen, Marc Pollefeys, Songyou Peng</strong></p><p>Neural Radiance Fields (NeRFs) have shown remarkable success in synthesizing photorealistic views from multi-view images of static scenes, but face challenges in dynamic, real-world environments with distractors like moving objects, shadows, and lighting changes. Existing methods manage controlled environments and low occlusion ratios but fall short in render quality, especially under high occlusion scenarios. In this paper, we introduce NeRF On-the-go, a simple yet effective approach that enables the robust synthesis of novel views in complex, in-the-wild scenes from only casually captured image sequences. Delving into uncertainty, our method not only efficiently eliminates distractors, even when they are predominant in captures, but also achieves a notably faster convergence speed. Through comprehensive experiments on various scenes, our method demonstrates a significant improvement over state-of-the-art techniques. This advancement opens new avenues for NeRF in diverse and dynamic real-world applications.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.18715v2">PDF</a> CVPR 2024, first two authors contributed equally. Project Page: <a target="_blank" rel="noopener" href="https://rwn17.github.io/nerf-on-the-go/">https://rwn17.github.io/nerf-on-the-go/</a></p><p>sorry, 您的ip已由于触发防滥用检测而被封禁,可能是因为使用云服务器如腾讯云或者国外代理(vpn)访问本网站，如果使用了vpn，关闭vpn或代理即可继续使用,本服务网址是<a target="_blank" rel="noopener" href="https://chat18.aichatos8.com">https://chat18.aichatos8.com</a> 如需合作接口调用请联系微信chatkf123 或者前往 <a target="_blank" rel="noopener" href="https://binjie09.shop">https://binjie09.shop</a> 自助购买key, 认为是误封需要解封的请前往<a target="_blank" rel="noopener" href="https://www.ip.cn/">https://www.ip.cn/</a> 查询ip信息,并发送信息至邮件 gpt33@binjie.site ，站长会定期看邮件并处理解封和合作问题，如需调用接口请见接口文档<a target="_blank" rel="noopener" href="https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123">https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123</a> 或者前往 <a target="_blank" rel="noopener" href="https://cat.gptxyy.cn">https://cat.gptxyy.cn</a> 注册使用（可付费使用gpt4 注册可免费使用3.5）</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>Title: NeRF On-the-go：在野外场景中消除干扰的 NeRF 方法 / NeRF On-the-go: Exploiting Uncertainty for Distractor-free NeRFs in the Wild</li></ol><ol><li>Authors: (not provided in the text)</li></ol><ol><li>Affiliation: (not provided in the text)</li></ol><ol><li>Keywords: NeRF, Neural Radiance Fields, Novel View Synthesis, Distractor Elimination</li></ol><ol><li>Urls: (not provided in the text), Github: None</li></ol><ol><li>Summary:</li></ol><pre><code>- (1):近年来，Neural Radiance Fields（NeRF）在静态场景中的新视图合成方面取得了remarkable success。但是在野外场景中，存在着移动对象、阴影和光照变化等干扰，限制了 NeRF 的应用。

- (2):现有的方法只能在控制环境和低遮挡率下取得不错的结果，但是在高遮挡率场景下，渲染质量较差。这些方法无法有效地消除干扰，且收敛速度较慢。

- (3):本文提出了一种名为 NeRF On-the-go 的方法，该方法可以在野外场景中robustly合成新视图。该方法通过探索不确定性，能够高效地消除干扰，且具有较快的收敛速度。

- (4):通过在多个场景下的实验，本文的方法在新视图合成任务上取得了state-of-the-art 的性能，证明了该方法的有效性。
</code></pre><ol><li>方法：</li></ol><ul><li><p>(1): 提取 DINOv2 特征：使用预训练的 DINOv2 特征取器从输入 RGB 图像中提取 per-pixel 特征，接着将这些特征作为输入到小型 MLP 中，以预测每个像素的不确定性值。</p></li><li><p>(2): 不确定性预测：使用小型 MLP 测每个像素的不确定性值，并引入正则项以强制不确定性预测的空间一致性。</p></li><li><p>(3): 不确定性正则化：引入基于 cosine 相似度的正则项，以强制不确定性预测的空间一致性，并计算每个像素的不确定性值。</p></li><li><p>(4): NeRF 模型训练：使用预测的不确定性值作为权重函数，训练 NeRF 模型，以实现新视图合成。</p></li><li><p>(5): SSIM-Based Loss：提出基于 SSIM 的损失函数，以增强不确定性学，并更好地区分动态元素和静态背景。</p></li><li><p>(6): Dilated Patch 采样：使用 Dilated Patch 采样策略，以增加训练迭代中的上下文信息，提高训练效率和不确定性学习的效果。</p></li><li><p>(7): 并行优化：并行训练不确定性预测 MLP 和 NeRF 模型，以确保不确定性学习的鲁棒性。</p></li></ul><p>sorry, 您的ip已由于触发防滥用检测而被封禁,可能是因为使用云服务器如腾讯云或者国外代理(vpn)访问本网站，如果使用了vpn，关闭vpn或代理即可继续使用,本服务网址是<a target="_blank" rel="noopener" href="https://chat18.aichatos8.com">https://chat18.aichatos8.com</a> 如需合作接口调用请联系微信chatkf123 或者前往 <a target="_blank" rel="noopener" href="https://binjie09.shop">https://binjie09.shop</a> 自助购买key, 认为是误封需要解封的请前往<a target="_blank" rel="noopener" href="https://www.ip.cn/">https://www.ip.cn/</a> 查询ip信息,并发送信息至邮件 gpt33@binjie.site ，站长会定期看邮件并处理解封和合作问题，如需调用接口请见接口文档<a target="_blank" rel="noopener" href="https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123">https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123</a> 或者前往 <a target="_blank" rel="noopener" href="https://cat.gptxyy.cn">https://cat.gptxyy.cn</a> 注册使用（可付费使用gpt4 注册可免费使用3.5）</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/81f93428bbbec8fd40924af0fdbb877d241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/201d88f74a36935b1e92502602a27e8b241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/7ed24d4e315d9e834296edef3ebeb1aa241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/9d327bd010b29ddc9e71da79b5b58b04241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/72e7a41c36b54711203fc693f66e17c2241286257.jpg" align="middle"></details><h2 id="PyGS-Large-scale-Scene-Representation-with-Pyramidal-3D-Gaussian-Splatting"><a href="#PyGS-Large-scale-Scene-Representation-with-Pyramidal-3D-Gaussian-Splatting" class="headerlink" title="PyGS: Large-scale Scene Representation with Pyramidal 3D Gaussian   Splatting"></a>PyGS: Large-scale Scene Representation with Pyramidal 3D Gaussian Splatting</h2><p><strong>Authors:Zipeng Wang, Dan Xu</strong></p><p>Neural Radiance Fields (NeRFs) have demonstrated remarkable proficiency in synthesizing photorealistic images of large-scale scenes. However, they are often plagued by a loss of fine details and long rendering durations. 3D Gaussian Splatting has recently been introduced as a potent alternative, achieving both high-fidelity visual results and accelerated rendering performance. Nonetheless, scaling 3D Gaussian Splatting is fraught with challenges. Specifically, large-scale scenes grapples with the integration of objects across multiple scales and disparate viewpoints, which often leads to compromised efficacy as the Gaussians need to balance between detail levels. Furthermore, the generation of initialization points via COLMAP from large-scale dataset is both computationally demanding and prone to incomplete reconstructions. To address these challenges, we present Pyramidal 3D Gaussian Splatting (PyGS) with NeRF Initialization. Our approach represent the scene with a hierarchical assembly of Gaussians arranged in a pyramidal fashion. The top level of the pyramid is composed of a few large Gaussians, while each subsequent layer accommodates a denser collection of smaller Gaussians. We effectively initialize these pyramidal Gaussians through sampling a rapidly trained grid-based NeRF at various frequencies. We group these pyramidal Gaussians into clusters and use a compact weighting network to dynamically determine the influence of each pyramid level of each cluster considering camera viewpoint during rendering. Our method achieves a significant performance leap across multiple large-scale datasets and attains a rendering time that is over 400 times faster than current state-of-the-art approaches.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.16829v3">PDF</a></p><p>sorry, 您的ip已由于触发防滥用检测而被封禁,可能是因为使用云服务器如腾讯云或者国外代理(vpn)访问本网站，如果使用了vpn，关闭vpn或代理即可继续使用,本服务网址是<a target="_blank" rel="noopener" href="https://chat18.aichatos8.com">https://chat18.aichatos8.com</a> 如需合作接口调用请联系微信chatkf123 或者前往 <a target="_blank" rel="noopener" href="https://binjie09.shop">https://binjie09.shop</a> 自助购买key, 认为是误封需要解封的请前往<a target="_blank" rel="noopener" href="https://www.ip.cn/">https://www.ip.cn/</a> 查询ip信息,并发送信息至邮件 gpt33@binjie.site ，站长会定期看邮件并处理解封和合作问题，如需调用接口请见接口文档<a target="_blank" rel="noopener" href="https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123">https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123</a> 或者前往 <a target="_blank" rel="noopener" href="https://cat.gptxyy.cn">https://cat.gptxyy.cn</a> 注册使用（可付费使用gpt4 注册可免费使用3.5）</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>Title: PYGS：大规模场景表示与 Pyramid 3D 高斯分裂</li></ol><ol><li>Authors: Zipeng Wang, Dan Xu</li></ol><ol><li>Affiliation: 香港科技大学计算机科学与工程系</li></ol><ol><li>Keywords: Neural Radiance Fields, 3D Gaussian Splatting, Scene Representation</li></ol><ol><li>Urls: <a target="_blank" rel="noopener" href="https://wzpscott.github.io/pyramid_gaussian_splatting/">https://wzpscott.github.io/pyramid_gaussian_splatting/</a>, Github:None</li></ol><ol><li>Summary:</li></ol><ul><li>(1): 本文研究背景是神经辐射场（NeRF）在大规模场景表示中局限性，例如细节损失和渲染时间长的问题。</li></ul><ul><li>(2): 过去的方法包括NeRF和其变体，如MegaNeRF、BlockNeRF和SwitchNeRF，但这些方法存在学习场景低频元素的倾向，无法捕捉细节，并且渲染时间长。最近，3D 高斯分裂（3DGS）方法出现，能够实时渲染高质量图像，但其扩展到大规模场景时存在挑战。</li></ul><ul><li>(3): 本文提出的方法是 Pyramid 3D 高斯分裂（PyGS），它使用金字塔结构的高斯分布来表示场景，每个高斯分布都有可学习的颜色和形状参数。同时，使用NeRF初始化高斯分布，并使用紧凑的权重网络来动态确定每个高斯分布的影响。</li></ul><ul><li>(4): 本文方法在多个大规模数据集上取得了显著的性能提升，渲染时间比当前最先进的方法快了400倍以上，达到了实时渲染的要求。</li></ul><ol><li>方法：</li></ol><ul><li><p>(1): 使用金字塔结构的高斯分布来表示场景，每个高斯分布都有可学习的颜色和状参数，以便捕捉场景的细节和结构。</p></li><li><p>(2): 使用NeRF初始化高斯分布，并使用紧凑的权重网络来动态确定每个高斯分布的影响，以实实时渲染。</p></li><li><p>(3): 将点云从NeRF生成，并将其分为多个子集，以便初始化Pyramidal Gaussians，形成一个多尺度的金字塔结构。</p></li><li><p>(4): 使用K-means算法将Pyramidal Gaussians分为簇，并为每个赋予唯一的权重，以实现基于簇的权重计算。</p></li><li><p>(5): 在渲染过程中，使用紧凑的权重网络来计算每个簇的权重，并将其应于Pyramidal Gaussians，以实现实时渲染。</p></li><li><p>(6): 使用颜色校正网络对Pyramidal Gaussians进行颜色调整，以适应复杂的光照变化。</p></li><li><p>(7): 通过投影和光栅化将Pyramidal Gaussians投影到图像平面上，以生成高质量的图像。</p></li></ul><p>sorry, 您的ip已由于触发防滥用检测而被封禁,可能是因为使用云服务器如腾讯云或者国外代理(vpn)访问本网站，如果使用了vpn，关闭vpn或代理即可继续使用,本服务网址是<a target="_blank" rel="noopener" href="https://chat18.aichatos8.com">https://chat18.aichatos8.com</a> 如需合作接口调用请联系微信chatkf123 或者前往 <a target="_blank" rel="noopener" href="https://binjie09.shop">https://binjie09.shop</a> 自助购买key, 认为是误封需要解封的请前往<a target="_blank" rel="noopener" href="https://www.ip.cn/">https://www.ip.cn/</a> 查询ip信息,并发送信息至邮件 gpt33@binjie.site ，站长会定期看邮件并处理解封和合作问题，如需调用接口请见接口文档<a target="_blank" rel="noopener" href="https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123">https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123</a> 或者前往 <a target="_blank" rel="noopener" href="https://cat.gptxyy.cn">https://cat.gptxyy.cn</a> 注册使用（可付费使用gpt4 注册可免费使用3.5）</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/9ff3c7fbe871418e3ac793dadf4fd5fe241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/c283e23120eb3eca240b844e87b24421241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/7d38859f61ef1701d335c147d7f0d813241286257.jpg" align="middle"></details></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io/2024/06/05/Paper/2024-06-05/NeRF/">https://kedreamix.github.io/2024/06/05/Paper/2024-06-05/NeRF/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/NeRF/">NeRF</a></div><div class="post_share"><div class="social-share" data-image="https://picx.zhimg.com/80/v2-2bdb0ecbbc3a0a2420781e472b68ba52.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2024/06/05/Paper/2024-06-05/3DGS/" title="3DGS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-2196bb11e15de7b5d3440823b4c92ca5.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">3DGS</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/03/15/Paperscape/Real3D-Portrait/" title="REAL3D-PORTRAIT ONE-SHOT REALISTIC 3D TALKING PORTRAIT SYNTHESIS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-68585b79de5f83b0dfa23304f41b9b98.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-15</div><div class="title">REAL3D-PORTRAIT ONE-SHOT REALISTIC 3D TALKING PORTRAIT SYNTHESIS</div></div></a></div><div><a href="/2024/03/07/Paperscape/SyncTalk/" title="SyncTalk The Devil is in the Synchronization for Talking Head Synthesis"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-a57e0937b2f452009023394a59529dfb.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-07</div><div class="title">SyncTalk The Devil is in the Synchronization for Talking Head Synthesis</div></div></a></div><div><a href="/2024/03/05/Paperscape/VividTalk/" title="VividTalk One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-8521b04f82075cc27b5e95148dba9792.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-05</div><div class="title">VividTalk One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior</div></div></a></div><div><a href="/2024/01/25/Paper/3DGS%20Survey/" title="3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-25</div><div class="title">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</div></div></a></div><div><a href="/2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" title="超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-01</div><div class="title">超赞的数字人生成知识库 Awesome-Talking-Head-Synthesis</div></div></a></div><div><a href="/2024/01/24/Paper/2024-01-24/NeRF/" title="NeRF"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-eaae707aaf894e22e54246edd91e6dce.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-24</div><div class="title">NeRF</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-06-05-%E6%9B%B4%E6%96%B0"><span class="toc-text">2024-06-05 更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Self-Calibrating-4D-Novel-View-Synthesis-from-Monocular-Videos-Using-Gaussian-Splatting"><span class="toc-text">Self-Calibrating 4D Novel View Synthesis from Monocular Videos Using Gaussian Splatting</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PruNeRF-Segment-Centric-Dataset-Pruning-via-3D-Spatial-Consistency"><span class="toc-text">PruNeRF: Segment-Centric Dataset Pruning via 3D Spatial Consistency</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Representing-Animatable-Avatar-via-Factorized-Neural-Fields"><span class="toc-text">Representing Animatable Avatar via Factorized Neural Fields</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Bilateral-Guided-Radiance-Field-Processing"><span class="toc-text">Bilateral Guided Radiance Field Processing</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#NeRF-On-the-go-Exploiting-Uncertainty-for-Distractor-free-NeRFs-in-the-Wild"><span class="toc-text">NeRF On-the-go: Exploiting Uncertainty for Distractor-free NeRFs in the Wild</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PyGS-Large-scale-Scene-Representation-with-Pyramidal-3D-Gaussian-Splatting"><span class="toc-text">PyGS: Large-scale Scene Representation with Pyramidal 3D Gaussian Splatting</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image:url('https://picx.zhimg.com/80/v2-2bdb0ecbbc3a0a2420781e472b68ba52.png')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js").then((()=>{pangu.autoSpacingPage()}))}function panguInit(){panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.1},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script></div><script id="canvas_nest" defer color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><script data-pjax>function butterfly_swiper_injector_config(){var a=document.getElementById("recent-posts");console.log("已挂载butterfly_swiper"),a.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/25/Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="2024/01/25/Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="2024/01/25/Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/17/Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023/12/17/Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="2023/12/17/Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script></body></html>