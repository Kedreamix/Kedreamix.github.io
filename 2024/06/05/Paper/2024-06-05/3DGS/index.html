<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>3DGS | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="3DGS 方向最新论文已更新，请持续关注 Update in 2024-06-05  DDGS-CT Direction-Disentangled Gaussian Splatting for Realistic Volume   Rendering"><meta property="og:type" content="article"><meta property="og:title" content="3DGS"><meta property="og:url" content="https://kedreamix.github.io/2024/06/05/Paper/2024-06-05/3DGS/index.html"><meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World"><meta property="og:description" content="3DGS 方向最新论文已更新，请持续关注 Update in 2024-06-05  DDGS-CT Direction-Disentangled Gaussian Splatting for Realistic Volume   Rendering"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://picx.zhimg.com/v2-2196bb11e15de7b5d3440823b4c92ca5.jpg"><meta property="article:published_time" content="2024-06-05T10:59:51.000Z"><meta property="article:modified_time" content="2024-06-05T10:59:51.809Z"><meta property="article:author" content="Kedreamix"><meta property="article:tag" content="3DGS"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://picx.zhimg.com/v2-2196bb11e15de7b5d3440823b4c92ca5.jpg"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/2024/06/05/Paper/2024-06-05/3DGS/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-74LZ5BEQQ1")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!0,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"简"},noticeOutdate:{limitDay:500,position:"top",messagePrev:"It has been",messageNext:"days since the last update, the content of the article may be outdated."},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:200},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!0,post:!0},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!0,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"3DGS",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-06-05 18:59:51"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach((e=>{n.setAttribute(e,t[e])})),document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)})),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=24?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 7.0.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml"></head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{const d=document.getElementById("loading-box"),e=document.body,o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l(),window.addEventListener("load",(()=>{o()}))})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">146</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url('https://picx.zhimg.com/v2-2196bb11e15de7b5d3440823b4c92ca5.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">3DGS</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-06-05T10:59:51.000Z" title="发表于 2024-06-05 18:59:51">2024-06-05</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-06-05T10:59:51.809Z" title="更新于 2024-06-05 18:59:51">2024-06-05</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">12.4k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>48分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="3DGS"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-06-05-更新"><a href="#2024-06-05-更新" class="headerlink" title="2024-06-05 更新"></a>2024-06-05 更新</h1><h2 id="DDGS-CT-Direction-Disentangled-Gaussian-Splatting-for-Realistic-Volume-Rendering"><a href="#DDGS-CT-Direction-Disentangled-Gaussian-Splatting-for-Realistic-Volume-Rendering" class="headerlink" title="DDGS-CT: Direction-Disentangled Gaussian Splatting for Realistic Volume   Rendering"></a>DDGS-CT: Direction-Disentangled Gaussian Splatting for Realistic Volume Rendering</h2><p><strong>Authors:Zhongpai Gao, Benjamin Planche, Meng Zheng, Xiao Chen, Terrence Chen, Ziyan Wu</strong></p><p>Digitally reconstructed radiographs (DRRs) are simulated 2D X-ray images generated from 3D CT volumes, widely used in preoperative settings but limited in intraoperative applications due to computational bottlenecks, especially for accurate but heavy physics-based Monte Carlo methods. While analytical DRR renderers offer greater efficiency, they overlook anisotropic X-ray image formation phenomena, such as Compton scattering. We present a novel approach that marries realistic physics-inspired X-ray simulation with efficient, differentiable DRR generation using 3D Gaussian splatting (3DGS). Our direction-disentangled 3DGS (DDGS) method separates the radiosity contribution into isotropic and direction-dependent components, approximating complex anisotropic interactions without intricate runtime simulations. Additionally, we adapt the 3DGS initialization to account for tomography data properties, enhancing accuracy and efficiency. Our method outperforms state-of-the-art techniques in image accuracy. Furthermore, our DDGS shows promise for intraoperative applications and inverse problems such as pose registration, delivering superior registration accuracy and runtime performance compared to analytical DRR methods.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.02518v1">PDF</a></p><p><strong>Summary</strong><br>使用基于物理的3D高斯点渲染（3DGS）方法生成数字重建射线图像（DRR），实现高效和高精度的X射线模拟。</p><p><strong>Key Takeaways</strong><br>• 数字重建射线图像（DRR）在preoperative settings中广泛应用，但在intraoperative应用中受到计算瓶颈的限制。<br>• 物理基于蒙特卡罗方法准确但计算密集，而analytical DRR renderer-efficient但忽视非各向同性X射线成像现象。<br>• 本文提出了一种novel方法，使用3D高斯点渲染（3DGS）实现高效和高精度的X射线模拟。<br>• 本方法将radiosity贡献分离为各向同性和方向依赖组件，近似复杂非各向同性相互作用。<br>• 本方法在图像准确性上优于state-of-the-art技术。<br>• 本方法适用于intraoperative应用和逆问题，如pose registration，提供更高的注册准确性和运行性能。</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: DDGS-CT: Direction-Disentangled Gaussian Splatting (基于方向解耦高斯点渲染的DRR生成)</p></li><li><p>Authors: Zhongpai Gao, Benjamin Planche, Meng Zheng, Xiao Chen, Terrence Chen, Ziyan Wu</p></li><li><p>Affiliation:波士顿美国成像智能公司 (United Imaging Intelligence, Boston, MA)</p></li><li><p>Keywords: Digitally Reconstructed Radiographs, 3D Gaussian Splatting, Direction-Disentangled Gaussian Splatting, X-ray Simulation</p></li><li><p>Urls: arXiv:2406.02518v1, Github: None</p></li><li><p>Summary:</p><ul><li><p>(1):本文研究的背景是生成Digitally Reconstructed Radiographs (DRRs)，即从3D CT体数据生成2D X射线图像。这种技术广泛应用于外科手术前规划，但是在外科手术中应用受到计算瓶颈的限制。</p></li><li><p>(2):过去的方法有基于蒙特卡罗方法的物理模型和基于射线追踪的分析方法。然而，物理模型计算复杂，而分析方法忽视了X射线成像中的各向异性效应，如Compton散射。</p></li><li><p>(3):本文提出了一种新的方法，称为Direction-Disentangled Gaussian Splatting (DDGS)，该方法将辐射度贡献分解为各向同性和方向相关的组件，近似复杂的各向异性相互作用，而不需要复杂的runtime模拟。</p></li><li><p>(4):本文的方法在图像准确性方面优于state-of-the-art技术，并且在外科手术应用和逆问题（如姿态注册）中显示出良好的性能和注册准确性。</p></li></ul></li><li>方法：</li></ol><ul><li><p>(1):基于3D高斯点渲染（3D Gaussian Splatting，3DGS）技术，提出了一种新的方法，称为方向解耦高斯点渲染（Direction-Disentangled Gaussian Splatting，DDGS），将辐射度贡献分解为各向同性和方向相关的组件。</p></li><li><p>(2):将高斯点云初始化策略改革为考虑CT体数据的几何和材料属性，从而提高模型收敛速度和图像准确性。</p></li><li><p>(3):引入方向依赖的辐射函数，近似复杂的各向异性相互作用，而不需要复杂的runtime模拟。</p></li><li><p>(4):将辐射函数分解为各向同性和方向相关的组件，分别用sigmoid函数和球谐函数表示，提高模型的模块化和准确性。</p></li><li><p>(5):使用DDGS方法生成Digitally Reconstructed Radiographs（DRRs），实现快速和准确的X射线图像生成。</p></li><li><p>(6):通过实验验证，DDGS方法在图像准确性方面优于state-of-the-art技术，并且在外科手术应用和逆问题（如姿态注册）中显示出良好的性能和注册准确性。</p></li></ul><ol><li>结论：</li></ol><ul><li><p>(1):本文提出的Direction-Disentangled Gaussian Splatting（DDGS）方法对于Digitally Reconstructed Radiographs（DRRs）的生成具有重要的意义，可以提高外科手术前规划和逆问题解决的效率和准确性。</p></li><li><p>(2):创新点：DDGS方法将辐射度贡献分解为各向同性和方向相关的组件，似复杂的各向异性相互作用；性能：DDGS方法在图像准确性方面优于state-of-the-art技术，并且在外科手术应用和逆问题中显示出良好的性能和注册准确性；工作量：DDGS方法可以减少计算瓶颈的限制，提高模型收敛速度和图像准确性，但需要offline DRR renderer来产生真实的ground truth以训练DDGS。</p></li></ul><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-8b3318acd0f86deae773881806828424.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-ba0d027e2ea11b7e2dabed9efbf6103a.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-940fd606a5c57a9e6fa75ff759d3c33a.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-5cb52f82edf660328efd02928ca40b9c.jpg" align="middle"></details><h2 id="WE-GS-An-In-the-wild-Efficient-3D-Gaussian-Representation-for-Unconstrained-Photo-Collections"><a href="#WE-GS-An-In-the-wild-Efficient-3D-Gaussian-Representation-for-Unconstrained-Photo-Collections" class="headerlink" title="WE-GS: An In-the-wild Efficient 3D Gaussian Representation for   Unconstrained Photo Collections"></a>WE-GS: An In-the-wild Efficient 3D Gaussian Representation for Unconstrained Photo Collections</h2><p><strong>Authors:Yuze Wang, Junyi Wang, Yue Qi</strong></p><p>Novel View Synthesis (NVS) from unconstrained photo collections is challenging in computer graphics. Recently, 3D Gaussian Splatting (3DGS) has shown promise for photorealistic and real-time NVS of static scenes. Building on 3DGS, we propose an efficient point-based differentiable rendering framework for scene reconstruction from photo collections. Our key innovation is a residual-based spherical harmonic coefficients transfer module that adapts 3DGS to varying lighting conditions and photometric post-processing. This lightweight module can be pre-computed and ensures efficient gradient propagation from rendered images to 3D Gaussian attributes. Additionally, we observe that the appearance encoder and the transient mask predictor, the two most critical parts of NVS from unconstrained photo collections, can be mutually beneficial. We introduce a plug-and-play lightweight spatial attention module to simultaneously predict transient occluders and latent appearance representation for each image. After training and preprocessing, our method aligns with the standard 3DGS format and rendering pipeline, facilitating seamlessly integration into various 3DGS applications. Extensive experiments on diverse datasets show our approach outperforms existing approaches on the rendering quality of novel view and appearance synthesis with high converge and rendering speed.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.02407v1">PDF</a> Our project page is available at <a target="_blank" rel="noopener" href="https://yuzewang1998.github.io/we-gs.github.io/">https://yuzewang1998.github.io/we-gs.github.io/</a></p><p><strong>Summary</strong><br>基于3D高斯涂抹技术，提出了一种点云基于可微分渲染框架，用于从照片集合重建场景，并实现了实时的新视图合成和外观合成。</p><p><strong>Key Takeaways</strong><br>• 基于3D高斯涂抹技术，提出了一种点云基于可微分渲染框架，用于从照片集合重建场景。<br>• 提出了残差基于球谐系数传输模块，适应变化的照明条件和光度后处理。<br>• 引入了轻量级空间注意力模块，预测瞬态遮挡物和潜在外观表示。<br>• 方法与标准3DGS格式和渲染管道兼容，易于集成到各种3DGS应用中。<br>• 实验结果表明，方法在新视图和外观合成的渲染质量上优于现有的方法，同时具有高收敛速和渲染速度。<br>• 方法能够时生成高质量的新视图和外观图像。<br>• 方法可以广泛应用于计算机图形学和虚拟现实等领域。</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>Title: WE-GS：一种在野高效3D高斯表示Novel View Synthesis for Unconstrained Photo Collections</li></ol><ol><li>Authors: Yuze Wang, Junyi Wang, Yue Qi</li></ol><ol><li>Affiliation: 未提供</li></ol><ol><li>Keywords: Novel View Synthesis, Unconstrained Photo Collection, Appearance Modeling, Real-time Rendering, 3D Gaussian Splatting</li></ol><ol><li>Urls: <a target="_blank" rel="noopener" href="https://yuzewang1998.github.io/we-gs.github.io/">https://yuzewang1998.github.io/we-gs.github.io/</a>, Github:None</li></ol><ol><li>Summary:</li></ol><pre><code>- (1):本文研究背景是从不受约束的照片集合中重建真实世界场景，并实现新视图合成（Novel View Synthesis），这是计算机图形学和计算机视觉领域中的一个挑战性任务。


- (2):过去的方法包括NeRF、3DGS等，但这些方法存在一些问题，例如需要大量计算资源、模型参数多、训练时长等。此外，NeRF-W、SWAG和GS-W等方法虽然可以处理不受约束的照片集合，但它们也存在一些缺陷，例如计算效率低、模型参数多、无法实时渲染等。


- (3):本文提出了一个基于点的可微分渲染框架，使用残差基于球谐函数系数传输模块来适应变化的照明条件和光度后处理。此外，本文还引入了一个轻量级的空间_attention模块来同时预测瞬态遮挡和潜在外观表示。


- (4):本文的方法在多个数据集上的实验结果表明，可以实现高质量的新视图和外观合成，同时具有高收敛速度和渲染速度，证明了方法的有效性。
</code></pre><ol><li>方法：</li></ol><ul><li><p>(1): 提出了一个基于点的可微分渲染框架，使用残差基于球谐函数系数传输模块来适应变化的照明条件和光度后处理。</p></li><li><p>(2): 引入了一个轻量级的空间_attention模块来同时预测瞬态遮挡和潜在外观表示。</p></li><li><p>(3): 使用残差基于球谐函数系数传输模块将每个3D高斯Gj学习到图像特定的残差球谐系数∆shjk，以适应不受约束的照片集合中的外观变化。</p></li><li><p>(4): 使用轻量级的空间_attention模块来预测瞬态遮挡和潜在外观表示，实现对瞬态遮挡和外观的同时预测。</p></li><li><p>(5): 通过优化过程，学习每个3D高斯的radiance field和瞬态遮挡，实现对不受约束的照片集合的新视图合成。</p></li><li><p>(6): 使用WE-GS方法在多个数据集上的实验结果表明，可以实现高质量的新视图和外观合成，同时具有高收敛速度和渲染速度，证明了方法的有效性。</p></li><li><p>(7): 通过对照实验，验证了WE-GS方法的优越性和鲁棒性，证明了其在不受约束的照片集合中的新视图合成任务中的应用价值。</p></li></ul><ol><li>Conclusion:</li></ol><ul><li>(1):本文的工作对计算机图形学和计算机视觉领域中的新视图合成任务具有重要意义，可以实现高质量的新视图和外观合成，同时具有高收敛速度和渲染速度，证明了方法的有效性。</li></ul><ul><li>(2):Innovation point: 本文提出的基于点的可微分渲染框架和轻量级的空_attention模块是该领域的创新点，解决了过去方法中的计算效率低、模型参数多、无法实时渲染等问题。Performance: 本文的方法在多个数据集上的实验结果表明，可以实现高质量的新视图和外观合成，同时具有高收敛速度和渲染速度。Workload: 本文的方法可以实时渲染，具有高收敛速度和渲染速度，证明了方法的实时性和高效性。</li></ul><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-97ed0af94488d838444ce09b850244d2.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-560238e8fd8e314b5603a7e532c6b0c1.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-5d888159ce7037f6f69e8e35f606741f.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-3bc8e67380a38526e522265c420f422b.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-2196bb11e15de7b5d3440823b4c92ca5.jpg" align="middle"></details><h2 id="OpenGaussian-Towards-Point-Level-3D-Gaussian-based-Open-Vocabulary-Understanding"><a href="#OpenGaussian-Towards-Point-Level-3D-Gaussian-based-Open-Vocabulary-Understanding" class="headerlink" title="OpenGaussian: Towards Point-Level 3D Gaussian-based Open Vocabulary   Understanding"></a>OpenGaussian: Towards Point-Level 3D Gaussian-based Open Vocabulary Understanding</h2><p><strong>Authors:Yanmin Wu, Jiarui Meng, Haijie Li, Chenming Wu, Yahao Shi, Xinhua Cheng, Chen Zhao, Haocheng Feng, Errui Ding, Jingdong Wang, Jian Zhang</strong></p><p>This paper introduces OpenGaussian, a method based on 3D Gaussian Splatting (3DGS) capable of 3D point-level open vocabulary understanding. Our primary motivation stems from observing that existing 3DGS-based open vocabulary methods mainly focus on 2D pixel-level parsing. These methods struggle with 3D point-level tasks due to weak feature expressiveness and inaccurate 2D-3D feature associations. To ensure robust feature presentation and 3D point-level understanding, we first employ SAM masks without cross-frame associations to train instance features with 3D consistency. These features exhibit both intra-object consistency and inter-object distinction. Then, we propose a two-stage codebook to discretize these features from coarse to fine levels. At the coarse level, we consider the positional information of 3D points to achieve location-based clustering, which is then refined at the fine level. Finally, we introduce an instance-level 3D-2D feature association method that links 3D points to 2D masks, which are further associated with 2D CLIP features. Extensive experiments, including open vocabulary-based 3D object selection, 3D point cloud understanding, click-based 3D object selection, and ablation studies, demonstrate the effectiveness of our proposed method. Project page: <a target="_blank" rel="noopener" href="https://3d-aigc.github.io/OpenGaussian">https://3d-aigc.github.io/OpenGaussian</a></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.02058v1">PDF</a> technical report, 15 pages</p><p><strong>Summary</strong><br>本文提出OpenGaussian方法，基于3D Gaussian Splatting（3DGS），实现3D点级开放词汇理解。</p><p><strong>Key Takeaways</strong><br>• OpenGaussian方法基于3D Gaussian Splatting（3DGS），实现3D点级开放词汇理解。<br>• 现有的3DGS-based开放词汇方法主关注2D像素级解析，无法进行3D点级任务。<br>• 本方法使用SAM masks训练实例特征，具有3D一致性和对象间区分性。<br>• 该方法采用两阶段代码簿将特征从粗到细级别离散化。<br>• 实验结果表明OpenGaussian方法在开放词汇基于3D对象选择、3D点云理解、点击基于3D对象选择等任务中具有很高的有效性。</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: OpenGaussian：点级3D高斯理解（OpenGaussian: Towards Point-Level 3D Gaussian-based Open Vocabulary Understanding）</p></li><li><p>Authors: Yanmin Wu, Jiarui Meng, Haijie Li, Chenming Wu, Yahao Shi, Xinhua Cheng, Chen Zhao, Haocheng Feng, Errui Ding, Jingdong Wang, Jian Zhang</p></li><li><p>Affiliation: 北京大学（Peking University）</p></li><li><p>Keywords: 3D Gaussian Splatting, Open Vocabulary Understanding, Point-Level Understanding, 3D Scene Understanding</p></li><li><p>Urls: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02058v1">https://arxiv.org/abs/2406.02058v1</a>, Github:None</p></li><li><p>Summary:</p></li></ol><ul><li><p>(1):本文的研究背景是基于3D Gaussian Splatting（3DGS）框架的3D场景理解，旨在实现点级开放词汇理解。</p></li><li><p>(2):过去的方法主要集中在2D像素级别的开放词汇理解，存在特征表达能力弱和2D-3D特征关联不准确的问题，本文的方法motivated by这些问题。</p></li><li><p>(3):本文提出的方法首先使用SAM masks训练实例特征，以确保特征的robustness和3D点级别的理解。然后，提出一个两阶段的codebook来离散这些特征，从粗糙到细腻的级别。最后，引入实例级别的3D-2D特征关联方法，将3D点关联到2D masks，并与2D CLIP特征关联。</p></li><li><p>(4):本文的方法在开放词汇基于3D对象选择、3D点云理解、点击基于3D对象选择等任务上取得了良好的性能，支持了论文的目标。</p></li></ul><ol><li>方法：</li></ol><ul><li><p>(1):使用 SAM 掩膜训练实例特征，以确保特征的鲁棒性和 3D 点级别的理解。</p></li><li><p>(2):提出一个两阶段的 codebook 来离散这些特征，从粗糙到细腻的级别。</p></li><li><p>(3):引入实例级别的 3D-2D 特征关联方法，将 3D 点关联到 2D 掩膜，并与 2D CLIP 特征关联。</p></li><li><p>(4):使用两种损失函数：intra-mask 平滑损失和 inter-mask 对比损失，来约束实例特征的学习。</p></li><li><p>(5):使用 codebook 离散化来确保同一实例的高斯点具有相同的特征，从而提高实例特征的 distinctiveness。</p></li><li><p>(6):提出两级 codebook 离散化方法，首先使用高斯点的 3D 坐标和实例特征进行粗糙级别的离散化，然后在每个粗糙簇中进一步离散化实例特征。</p></li><li><p>(7):使用伪特征损失函数来优化 codebook 的学习过程，提高实例特征的准确性。</p></li></ul><ol><li>Conclusion:</li></ol><ul><li><p>(1):本文的工作意义在于推动三维场景理解的发展，实现点级开放词汇理解，提高三维点云理解的精度和泛化能力。</p></li><li><p>(2):创新点：本文提出了一种基于三维高斯点的开放词汇理解方法引入了实例级别的三维-二维特��关联方法，提高了三维点云理解的精度和robustness；性能：本文的方法在开放词汇基于三维对象选择、 trois点云理解、点击基于三维对象选择等任务上取得了良好的性能；工作负载：本文的方法需要大量的计资源和数据支持，且需要进一步优化代码实现以提高效率。</p></li></ul><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-2be9af96961d59f11698f2c5bcd330a2.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-a911eb9df51d9d27ce9eaa83b8c2dc27.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-156984d4ba895cb35d5d6806f6c11e48.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-ee276cabd836736f8ee107c9dee10a2e.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-469659c7608d57f9e72a22a8abc4c5d0.jpg" align="middle"></details><h2 id="FastLGS-Speeding-up-Language-Embedded-Gaussians-with-Feature-Grid-Mapping"><a href="#FastLGS-Speeding-up-Language-Embedded-Gaussians-with-Feature-Grid-Mapping" class="headerlink" title="FastLGS: Speeding up Language Embedded Gaussians with Feature Grid   Mapping"></a>FastLGS: Speeding up Language Embedded Gaussians with Feature Grid Mapping</h2><p><strong>Authors:Yuzhou Ji, He Zhu, Junshu Tang, Wuyi Liu, Zhizhong Zhang, Yuan Xie, Lizhuang Ma, Xin Tan</strong></p><p>The semantically interactive radiance field has always been an appealing task for its potential to facilitate user-friendly and automated real-world 3D scene understanding applications. However, it is a challenging task to achieve high quality, efficiency and zero-shot ability at the same time with semantics in radiance fields. In this work, we present FastLGS, an approach that supports real-time open-vocabulary query within 3D Gaussian Splatting (3DGS) under high resolution. We propose the semantic feature grid to save multi-view CLIP features which are extracted based on Segment Anything Model (SAM) masks, and map the grids to low dimensional features for semantic field training through 3DGS. Once trained, we can restore pixel-aligned CLIP embeddings through feature grids from rendered features for open-vocabulary queries. Comparisons with other state-of-the-art methods prove that FastLGS can achieve the first place performance concerning both speed and accuracy, where FastLGS is 98x faster than LERF and 4x faster than LangSplat. Meanwhile, experiments show that FastLGS is adaptive and compatible with many downstream tasks, such as 3D segmentation and 3D object inpainting, which can be easily applied to other 3D manipulation systems.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.01916v1">PDF</a></p><p>sorry, 您的ip已由于触发防滥用检测而被封禁,可能是因为使用云服务器如腾讯云或者国外代理(vpn)访问本网站，如果使用了vpn，关闭vpn或代理即可继续使用,本服务网址是<a target="_blank" rel="noopener" href="https://chat18.aichatos8.com">https://chat18.aichatos8.com</a> 如需合作接口调用请联系微信chatkf123 或者前往 <a target="_blank" rel="noopener" href="https://binjie09.shop">https://binjie09.shop</a> 自助购买key, 认为是误封需要解封的请前往<a target="_blank" rel="noopener" href="https://www.ip.cn/">https://www.ip.cn/</a> 查询ip信息,并发送信息至邮件 gpt33@binjie.site ，站长会定期看邮件并处理解封和合作问题，如需调用接口请见接口文档<a target="_blank" rel="noopener" href="https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123">https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123</a> 或者前往 <a target="_blank" rel="noopener" href="https://cat.gptxyy.cn">https://cat.gptxyy.cn</a> 注册使用（可付费使用gpt4 注册可免费使用3.5）</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>Title: FastLGS：Speeding up Language Embedded Gaussians with Feature Grid / FastLGS：使用特征网格加速语言嵌入高斯</li></ol><ol><li>Authors: Yuzhou Ji, He Zhu, Junshu Tang, Wuyi Liu, Zhizhong Zhang, Yuan Xie, Lizhuang Ma, Xin Tan</li></ol><ol><li>Affiliation: 东华师范大学</li></ol><ol><li>Keywords: open-vocabulary detection, zero-shot learning, semantic 3D field</li></ol><ol><li>Urls: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01916">https://arxiv.org/abs/2406.01916</a> , Github:None</li></ol><ol><li>Summary:</li></ol><pre><code>- (1):研究背景是三维场景理解，目标是学习高效、准确的三维语义表示，并支持自然语言查询和交互式场景操作。


- (2):过去的方法，如Semantic-NeRF、N3F、Panoptic Lifting等，主要关注对象的分割和识别，但这些方法存在一些问题，如速度慢、准确性不高、不支持零样本学习等。此外，LERF和LangSplat等方法可以实现零样本学习和开放词汇查询，但它们也存在一些问题，如结果不够准确、不支持交互式查询等。


- (3):本文提出的方法是FastLGS，使用特征网格映射策略在三维高斯splatted（3DGS）中构建三维语义场，首先提取对象遮罩，然后使用CLIP encoder提取对象级图像特征，并将其映射到低维特征空间，最后在推理阶段恢复语义特征以生成查询结果。


- (4):本文方法在三维场景理解任务上取得了竞争性的性能，支持实时交互式查询，速度比LERF快98倍、比LangSplat快4倍，同时也支持多对象查询和调整。
</code></pre><p>sorry, 您的ip已由于触发防滥用检测而被封禁,可能是因为使用云服务器如腾讯云或者国外代理(vpn)访问本网站，如果使用了vpn，关闭vpn或代理即可继续使用,本服务网址是<a target="_blank" rel="noopener" href="https://chat18.aichatos8.com">https://chat18.aichatos8.com</a> 如需合作接口调用请联系微信chatkf123 或者前往 <a target="_blank" rel="noopener" href="https://binjie09.shop">https://binjie09.shop</a> 自助购买key, 认为是误封需要解封的请前往<a target="_blank" rel="noopener" href="https://www.ip.cn/">https://www.ip.cn/</a> 查询ip信息,并发送信息至邮件 gpt33@binjie.site ，站长会定期看邮件并处理解封和合作问题，如需调用接口请见接口文档<a target="_blank" rel="noopener" href="https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123">https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123</a> 或者前往 <a target="_blank" rel="noopener" href="https://cat.gptxyy.cn">https://cat.gptxyy.cn</a> 注册使用（可付费使用gpt4 注册可免费使用3.5）</p><ol><li>Conclusion:</li></ol><ul><li>(1):本文提出的FastLGS方法在三维场景理解领域具有重要的研究价值，能够实现高效、准确的三维语义表示，并支持自然语言查询和交互式场景操作，具有广阔的应用前景。</li></ul><ul><li>(2):创新点：FastLGS方法提出了特征网格映射策略，在三维高斯splatted（3DGS）中构建三维语义场，实现了实时交互式查询和零样本学习；性能：FastLGS方法在三维场景理解任务上取得了竞争性的性能，速度比LERF快98倍、比LangSplat快4倍，同时也支持多对象查询和调整；工作量：FastLGS方法的计算复杂度较低，可以实时处理大规模三维场景数据。</li></ul><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-d4170698dae988cb090141684c9112f2.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-540d2a4b100c2bf1d6286f366b6c2a8f.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-022dd2df1dd7f0502d71536cf740dac3.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-aa2aac627a35c0864283eead0500c353.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-bf66f18dd9ece3d86d3323a4a75057f1.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-080db73849e866a47f863184c0b8e203.jpg" align="middle"></details><h2 id="DreamPhysics-Learning-Physical-Properties-of-Dynamic-3D-Gaussians-with-Video-Diffusion-Priors"><a href="#DreamPhysics-Learning-Physical-Properties-of-Dynamic-3D-Gaussians-with-Video-Diffusion-Priors" class="headerlink" title="DreamPhysics: Learning Physical Properties of Dynamic 3D Gaussians with   Video Diffusion Priors"></a>DreamPhysics: Learning Physical Properties of Dynamic 3D Gaussians with Video Diffusion Priors</h2><p><strong>Authors:Tianyu Huang, Yihan Zeng, Hui Li, Wangmeng Zuo, Rynson W. H. Lau</strong></p><p>Dynamic 3D interaction has witnessed great interest in recent works, while creating such 4D content remains challenging. One solution is to animate 3D scenes with physics-based simulation, and the other is to learn the deformation of static 3D objects with the distillation of video generative models. The former one requires assigning precise physical properties to the target object, otherwise the simulated results would become unnatural. The latter tends to formulate the video with minor motions and discontinuous frames, due to the absence of physical constraints in deformation learning. We think that video generative models are trained with real-world captured data, capable of judging physical phenomenon in simulation environments. To this end, we propose DreamPhysics in this work, which estimates physical properties of 3D Gaussian Splatting with video diffusion priors. DreamPhysics supports both image- and text-conditioned guidance, optimizing physical parameters via score distillation sampling with frame interpolation and log gradient. Based on a material point method simulator with proper physical parameters, our method can generate 4D content with realistic motions. Experimental results demonstrate that, by distilling the prior knowledge of video diffusion models, inaccurate physical properties can be gradually refined for high-quality simulation. Codes are released at: <a target="_blank" rel="noopener" href="https://github.com/tyhuang0428/DreamPhysics">https://github.com/tyhuang0428/DreamPhysics</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.01476v1">PDF</a> Technical report. Codes are released at: <a target="_blank" rel="noopener" href="https://github.com/tyhuang0428/DreamPhysics">https://github.com/tyhuang0428/DreamPhysics</a></p><p>sorry, 您的ip已由于触发防滥用检测而被封禁,可能是因为使用云服务器如腾讯云或者国外代理(vpn)访问本网站，如果使用了vpn，关闭vpn或代理即可继续使用,本服务网址是<a target="_blank" rel="noopener" href="https://chat18.aichatos8.com">https://chat18.aichatos8.com</a> 如需合作接口调用请联系微信chatkf123 或者前往 <a target="_blank" rel="noopener" href="https://binjie09.shop">https://binjie09.shop</a> 自助购买key, 认为是误封需要解封的请前往<a target="_blank" rel="noopener" href="https://www.ip.cn/">https://www.ip.cn/</a> 查询ip信息,并发送信息至邮件 gpt33@binjie.site ，站长会定期看邮件并处理解封和合作问题，如需调用接口请见接口文档<a target="_blank" rel="noopener" href="https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123">https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123</a> 或者前往 <a target="_blank" rel="noopener" href="https://cat.gptxyy.cn">https://cat.gptxyy.cn</a> 注册使用（可付费使用gpt4 注册可免费使用3.5）</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>Title: DreamPhysics: Learning Physical Properties of Dynamic 3D Gaussians with Video Diffusion Priors (DreamPhysics：使用视频扩散先验学习动态3D高斯体的物理属性)</li></ol><ol><li>Authors: Tianyu Huang, Yihan Zeng, Hui Li, Wangmeng Zuo, Rynson W. H. Lau</li></ol><ol><li>Affiliation: 香港城市大学</li></ol><ol><li>Keywords: Dynamic 3D interaction, physics-based simulation, video generative models, physical properties estimation</li></ol><ol><li>Urls: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01476v1">https://arxiv.org/abs/2406.01476v1</a> , Github: <a target="_blank" rel="noopener" href="https://github.com/tyhuang0428/DreamPhysics">https://github.com/tyhuang0428/DreamPhysics</a></li></ol><ol><li>Summary:</li></ol><pre><code>- (1):该论文的研究背景是动态3D交互，旨在生成逼真的4D内容。


- (2):过去的方法有两种：物理基于模拟和视频生成模型，但前者需要手动指定物理参数，则模拟结果不自然；后者生成的视频呈现小幅度运动和不连续的帧，缺乏物理约束。


- (3):本文提出了DreamPhysics方法，使视频扩散先验估计3D高斯体的物理属性，支持图像和文本条件指导，通过分数蒸馏采样和帧插值和对数梯度优化物理参数。


- (4):实验结果表明，通过馏视频扩散模型的先验知识，可以逐步改进物理参数的不准确性，从而生成高质量的模拟结果。
</code></pre><ol><li>方法：</li></ol><ul><li><p>(1): DreamPhysics 方法的第一步是初始化一系列参数 $\theta^{(0)}_G$，并通过基于 Material Point Method（MPM）的模拟器渲染一个长度为 T 的视频 $V^{(0)} = {I^{(0)}_1, I^{(0)}_2, …, I^{(0)}_T}$。</p></li><li><p>(2): 然后，将渲染的视频 $V^{(0)}$ 送入 Score Distillation Sampling���SDS-T）优化器，distill 视频扩散先验到参数 $\theta^{(1)}_G$。</p></li><li><p>(3): 对于每个训练 epoch k，我们可以通过 SDS-T 优化器来获得一个优化的参数 $\theta^{(k+1)}_G$，通过 distill 视频 $V^{(k)}$。</p></li><li><p>(4): 通过多次迭代优化，参数 $\theta_G$ 将收敛到一个合理的范围，实现物理参数的估计。</p></li><li><p>(5): 在估计物理参数的同时，DreamPhysics 方法还支持图像和文本条件指导，通过对数梯度优化物理参数。</p></li><li><p>(6): 最终，DreamPhysics 方法可以生成高质量的模拟结果，实现动态 3D 交互的逼真模拟。</p></li></ul><p>sorry, 您的ip已由于触发防滥用检测而被封禁,可能是因为使用云服务器如腾讯云或者国外代理(vpn)访问本网站，如果使用了vpn，关闭vpn或代理即可继续使用,本服务网址是<a target="_blank" rel="noopener" href="https://chat18.aichatos8.com">https://chat18.aichatos8.com</a> 如需合作接口调用请联系微信chatkf123 或者前往 <a target="_blank" rel="noopener" href="https://binjie09.shop">https://binjie09.shop</a> 自助购买key, 认为是误封需要解封的请前往<a target="_blank" rel="noopener" href="https://www.ip.cn/">https://www.ip.cn/</a> 查询ip信息,并发送信息至邮件 gpt33@binjie.site ，站长会定期看邮件并处理解封和合作问题，如需调用接口请见接口文档<a target="_blank" rel="noopener" href="https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123">https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123</a> 或者前往 <a target="_blank" rel="noopener" href="https://cat.gptxyy.cn">https://cat.gptxyy.cn</a> 注册使用（可付费使用gpt4 注册可免费使用3.5）</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-3b5f7a74c5f7cfa121fbdc0c6a5c2e55.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-0396fa459472e29b46aa5af33ea5941e.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-a40536c6f67882b579ec742261eacada.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-f82f9a57aa613a59ab05429bfbaaa47c.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-af6bdc5f2b23662bbbbc8a2da5064695.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-32fb319f07fc7904ec45c112004295ae.jpg" align="middle"></details><h2 id="Self-Calibrating-4D-Novel-View-Synthesis-from-Monocular-Videos-Using-Gaussian-Splatting"><a href="#Self-Calibrating-4D-Novel-View-Synthesis-from-Monocular-Videos-Using-Gaussian-Splatting" class="headerlink" title="Self-Calibrating 4D Novel View Synthesis from Monocular Videos Using   Gaussian Splatting"></a>Self-Calibrating 4D Novel View Synthesis from Monocular Videos Using Gaussian Splatting</h2><p><strong>Authors:Fang Li, Hao Zhang, Narendra Ahuja</strong></p><p>Gaussian Splatting (GS) has significantly elevated scene reconstruction efficiency and novel view synthesis (NVS) accuracy compared to Neural Radiance Fields (NeRF), particularly for dynamic scenes. However, current 4D NVS methods, whether based on GS or NeRF, primarily rely on camera parameters provided by COLMAP and even utilize sparse point clouds generated by COLMAP for initialization, which lack accuracy as well are time-consuming. This sometimes results in poor dynamic scene representation, especially in scenes with large object movements, or extreme camera conditions e.g. small translations combined with large rotations. Some studies simultaneously optimize the estimation of camera parameters and scenes, supervised by additional information like depth, optical flow, etc. obtained from off-the-shelf models. Using this unverified information as ground truth can reduce robustness and accuracy, which does frequently occur for long monocular videos (with e.g. &gt; hundreds of frames). We propose a novel approach that learns a high-fidelity 4D GS scene representation with self-calibration of camera parameters. It includes the extraction of 2D point features that robustly represent 3D structure, and their use for subsequent joint optimization of camera parameters and 3D structure towards overall 4D scene optimization. We demonstrate the accuracy and time efficiency of our method through extensive quantitative and qualitative experimental results on several standard benchmarks. The results show significant improvements over state-of-the-art methods for 4D novel view synthesis. The source code will be released soon at <a target="_blank" rel="noopener" href="https://github.com/fangli333/SC-4DGS">https://github.com/fangli333/SC-4DGS</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.01042v1">PDF</a> GitHub Page: <a target="_blank" rel="noopener" href="https://github.com/fangli333/SC-4DGS">https://github.com/fangli333/SC-4DGS</a></p><p>sorry, 您的ip已由于触发防滥用检测而被封禁,可能是因为使用云服务器如腾讯云或者国外代理(vpn)访问本网站，如果使用了vpn，关闭vpn或代理即可继续使用,本服务网址是<a target="_blank" rel="noopener" href="https://chat18.aichatos8.com">https://chat18.aichatos8.com</a> 如需合作接口调用请联系微信chatkf123 或者前往 <a target="_blank" rel="noopener" href="https://binjie09.shop">https://binjie09.shop</a> 自助购买key, 认为是误封需要解封的请前往<a target="_blank" rel="noopener" href="https://www.ip.cn/">https://www.ip.cn/</a> 查询ip信息,并发送信息至邮件 gpt33@binjie.site ，站长会定期看邮件并处理解封和合作问题，如需调用接口请见接口文档<a target="_blank" rel="noopener" href="https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123">https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123</a> 或者前往 <a target="_blank" rel="noopener" href="https://cat.gptxyy.cn">https://cat.gptxyy.cn</a> 注册使用（可付费使用gpt4 注册可免费使用3.5）</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: 自适应4D新视图合成来自单目视频的高斯斑点（Self-Calibrating 4D Novel View Synthesis from Monocular Videos Using Gaussian Splatting）</p></li><li><p>Authors: Fang Li, Hao Zhang, Narendra Ahuja</p></li><li><p>Affiliation: 伊利诺伊大学厄巴纳-香槟分校</p></li><li><p>Keywords: Novel View Synthesis, Gaussian Splatting, 4D Reconstruction, Self-Calibration</p></li><li><p>Urls: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01042v1">https://arxiv.org/abs/2406.01042v1</a> , Github: <a target="_blank" rel="noopener" href="https://github.com/fangli333/SC-4DGS">https://github.com/fangli333/SC-4DGS</a></p></li><li><p>Summary:</p></li></ol><pre><code>- (1):本文研究的是从单目视频中生成高保真4D新视图的任务，当前方法存在一些限制，例如依赖COLMAP提供的摄像机参数和稀疏点云初始化，缺乏准确性和时间效率。

- (2):过去的方法主要基于NeRF和Gaussian Splatting，然而这些方法存在一些问题，如依赖外部信息、长时间的预处理和训练时间、对动态场景的表示不够准确等。

- (3):本文提出了一个自适应的方法，学习高保真4D场景表示，并自适应摄像机参数。该方法包括提取2D点特征来表示3D结构，然后联合优化摄像机参数和3D结构来实现4D场景优化。

- (4):本文的方法在多个标准基准测试上取得了显著的改进，证明了其在4D新视图合成任务上的优越性。
</code></pre><p>sorry, 您的ip已由于触发防滥用检测而被封禁,可能是因为使用云服务器如腾讯云或者国外代理(vpn)访问本网站，如果使用了vpn，关闭vpn或代理即可继续使用,本服务网址是<a target="_blank" rel="noopener" href="https://chat18.aichatos8.com">https://chat18.aichatos8.com</a> 如需合作接口调用请联系微信chatkf123 或者前往 <a target="_blank" rel="noopener" href="https://binjie09.shop">https://binjie09.shop</a> 自助购买key, 认为是误封需要解封的请前往<a target="_blank" rel="noopener" href="https://www.ip.cn/">https://www.ip.cn/</a> 查询ip信息,并发送信息至邮件 gpt33@binjie.site ，站长会定期看邮件并处理解封和合作问题，如需调用接口请见接口文档<a target="_blank" rel="noopener" href="https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123">https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123</a> 或者前往 <a target="_blank" rel="noopener" href="https://cat.gptxyy.cn">https://cat.gptxyy.cn</a> 注册使用（可付费使用gpt4 注册可免费使用3.5）</p><p>sorry, 您的ip已由于触发防滥用检测而被封禁,可能是因为使用云服务器如腾讯云或者国外代理(vpn)访问本网站，如果使用了vpn，关闭vpn或代理即可继续使用,本服务网址是<a target="_blank" rel="noopener" href="https://chat18.aichatos8.com">https://chat18.aichatos8.com</a> 如需合作接口调用请联系微信chatkf123 或者前往 <a target="_blank" rel="noopener" href="https://binjie09.shop">https://binjie09.shop</a> 自助购买key, 认为是误封需要解封的请前往<a target="_blank" rel="noopener" href="https://www.ip.cn/">https://www.ip.cn/</a> 查询ip信息,并发送信息至邮件 gpt33@binjie.site ，站长会定期看邮件并处理解封和合作问题，如需调用接口请见接口文档<a target="_blank" rel="noopener" href="https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123">https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123</a> 或者前往 <a target="_blank" rel="noopener" href="https://cat.gptxyy.cn">https://cat.gptxyy.cn</a> 注册使用（可付费使用gpt4 注册可免费使用3.5）</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-301d49be2f4e8a22c8b77021a373d934.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-943f2448f91afb60a72f6a93466398e1.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-46743dd4fd6de272e99deea3ad94b4ac.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-cb752928248c98233389d6a68f5cbb84.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-94b1a6297fd9bb1be9cdc3fbe53fc163.jpg" align="middle"></details><h2 id="Topo4D-Topology-Preserving-Gaussian-Splatting-for-High-Fidelity-4D-Head-Capture"><a href="#Topo4D-Topology-Preserving-Gaussian-Splatting-for-High-Fidelity-4D-Head-Capture" class="headerlink" title="Topo4D: Topology-Preserving Gaussian Splatting for High-Fidelity 4D Head   Capture"></a>Topo4D: Topology-Preserving Gaussian Splatting for High-Fidelity 4D Head Capture</h2><p><strong>Authors:X. Li, Y. Cheng, X. Ren, H. Jia, D. Xu, W. Zhu, Y. Yan</strong></p><p>4D head capture aims to generate dynamic topological meshes and corresponding texture maps from videos, which is widely utilized in movies and games for its ability to simulate facial muscle movements and recover dynamic textures in pore-squeezing. The industry often adopts the method involving multi-view stereo and non-rigid alignment. However, this approach is prone to errors and heavily reliant on time-consuming manual processing by artists. To simplify this process, we propose Topo4D, a novel framework for automatic geometry and texture generation, which optimizes densely aligned 4D heads and 8K texture maps directly from calibrated multi-view time-series images. Specifically, we first represent the time-series faces as a set of dynamic 3D Gaussians with fixed topology in which the Gaussian centers are bound to the mesh vertices. Afterward, we perform alternative geometry and texture optimization frame-by-frame for high-quality geometry and texture learning while maintaining temporal topology stability. Finally, we can extract dynamic facial meshes in regular wiring arrangement and high-fidelity textures with pore-level details from the learned Gaussians. Extensive experiments show that our method achieves superior results than the current SOTA face reconstruction methods both in the quality of meshes and textures. Project page: <a target="_blank" rel="noopener" href="https://xuanchenli.github.io/Topo4D/">https://xuanchenli.github.io/Topo4D/</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.00440v1">PDF</a></p><p><strong>Summary</strong><br>使用Topo4D框架自动生成4D头部网格和8K纹理图，从而简化电影和游戏industry中的面部捕捉过程。</p><p><strong>Key Takeaways</strong><br>• Topo4D框架��以自动生成高质量的4D头部网格和8K纹理图。<br>• 该方法可以直接从calibrated multi-view time-series images中学习高质量的几何形状和纹理。<br>• Topo4D使用动态3D高斯分布来表示时间序列面部，以保持时间拓扑稳定性。<br>• 方法可以提取具有规则wireframe排列的动态面部网格和高保真度纹理。<br>• 实验结果明，Topo4D方法优于当前最先进的面部重建方法<br>• 该方法可以应用于电影和游戏industry中的面部捕捉过程。<br>• 项目页面为<a target="_blank" rel="noopener" href="https://xuanchenli.github.io/Topo4D/。">https://xuanchenli.github.io/Topo4D/。</a></p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: Topo4D：Topology-Preserving Gaussian Splatting（拓扑保持高斯Splating）</p></li><li><p>Authors: Xuanchen Li, Yuhao Cheng, Xingyu Ren, Haozhe Jia, Di Xu, Wenhan Zhu, Yichao Yan</p></li><li><p>Affiliation: 上海交通大学人工智能研究所</p></li><li><p>Keywords: 4D Face Modeling · High Resolution Texture Generation</p></li><li><p>Urls: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.00440v1">https://arxiv.org/abs/2406.00440v1</a> , Github:None</p></li><li><p>Summary:</p></li></ol><ul><li><p>(1):本文研究的背景是四维头部捕捉，旨在从视频中生���动态拓扑网格和对应的纹理图，广泛应用于电影和游戏中模拟面部肌肉运动和恢复动态纹理细节。</p></li><li><p>(2):过去的方法主要采用多视图立体和非刚性对齐，但是这种方法易出错且需要艺术家耗时_manual_processing。因此，本文提出的方法旨在简化这个过程。</p></li><li><p>(3):本文提出的方法是Topo4D框架，首先将时序面部表示为一组具有固定拓扑结构的三维高斯函数，然后逐帧进行几何和纹理优化，以学习高质量的几何和纹理同时保持时域拓扑稳定性。</p></li><li><p>(4):本文的方法在四维头部捕捉任务上取得了优于当前最优方法的结果，生成的网格和纹理质量高支持了电影和游戏中的应用目标。<br>sorry, 您的ip已由于触发防滥用检测而被封禁,可能是因为使用云服务器如腾讯云或者国外代理(vpn)访问本网站，如果使用了vpn，关闭vpn或代理即可继续使用,本服务网址是<a target="_blank" rel="noopener" href="https://chat18.aichatos8.com">https://chat18.aichatos8.com</a> 如需合作接口调用请联系微信chatkf123 或者前往 <a target="_blank" rel="noopener" href="https://binjie09.shop">https://binjie09.shop</a> 自助购买key, 认为是误封需要解封的请前往<a target="_blank" rel="noopener" href="https://www.ip.cn/">https://www.ip.cn/</a> 查询ip信息,并发送信息至邮件 gpt33@binjie.site ，站长会定期看邮件并处理解封和合作问题，如需调用接口请见接口文档<a target="_blank" rel="noopener" href="https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123">https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123</a> 或者前往 <a target="_blank" rel="noopener" href="https://cat.gptxyy.cn">https://cat.gptxyy.cn</a> 注册使用（可付费使用gpt4 注册可免费使用3.5）</p></li></ul><p>sorry, 您的ip已由于触发防滥用检测而被封禁,可能是因为使用云服务器如腾讯云或者国外代理(vpn)访问本网站，如果使用了vpn，关闭vpn或代理即可继续使用,本服务网址是<a target="_blank" rel="noopener" href="https://chat18.aichatos8.com">https://chat18.aichatos8.com</a> 如需合作接口调用请联系微信chatkf123 或者前往 <a target="_blank" rel="noopener" href="https://binjie09.shop">https://binjie09.shop</a> 自助购买key, 认为是误封需要解封的请前往<a target="_blank" rel="noopener" href="https://www.ip.cn/">https://www.ip.cn/</a> 查询ip信息,并发送信息至邮件 gpt33@binjie.site ，站长会定期看邮件并处理解封和合作问题，如需调用接口请见接口文档<a target="_blank" rel="noopener" href="https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123">https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123</a> 或者前往 <a target="_blank" rel="noopener" href="https://cat.gptxyy.cn">https://cat.gptxyy.cn</a> 注册使用（可付费使用gpt4 注册可免费使用3.5）</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-044930c455fa1fcb8db237a77e2f901e.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-81f2cfd9126d74c5f6a8c92db3a7a1b9.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-3eba255e0bedcac1c79c02965998ba33.jpg" align="middle"></details><h2 id="MoDGS-Dynamic-Gaussian-Splatting-from-Causually-captured-Monocular-Videos"><a href="#MoDGS-Dynamic-Gaussian-Splatting-from-Causually-captured-Monocular-Videos" class="headerlink" title="MoDGS: Dynamic Gaussian Splatting from Causually-captured Monocular   Videos"></a>MoDGS: Dynamic Gaussian Splatting from Causually-captured Monocular Videos</h2><p><strong>Authors:Qingming Liu, Yuan Liu, Jiepeng Wang, Xianqiang Lv, Peng Wang, Wenping Wang, Junhui Hou</strong></p><p>In this paper, we propose MoDGS, a new pipeline to render novel-view images in dynamic scenes using only casually captured monocular videos. Previous monocular dynamic NeRF or Gaussian Splatting methods strongly rely on the rapid movement of input cameras to construct multiview consistency but fail to reconstruct dynamic scenes on casually captured input videos whose cameras are static or move slowly. To address this challenging task, MoDGS adopts recent single-view depth estimation methods to guide the learning of the dynamic scene. Then, a novel 3D-aware initialization method is proposed to learn a reasonable deformation field and a new robust depth loss is proposed to guide the learning of dynamic scene geometry. Comprehensive experiments demonstrate that MoDGS is able to render high-quality novel view images of dynamic scenes from just a casually captured monocular video, which outperforms baseline methods by a significant margin.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.00434v1">PDF</a></p><p><strong>Summary</strong><br>使用MoDGS.pipeline，仅需单目视频即可渲染动态场景中的新视图图像。</p><p><strong>Key Takeaways</strong><br>• MoDGS.pipeline可以渲染动态场景中的新视图图像，仅需单目视频。<br>• 之前的单目动态NeRF或高斯喷射方法需要快速移动的输入摄像机，但MoDGS可以处理静态或慢速移动摄像机的输入视频。<br>• MoDGS采用单视图深度估计方法指导动态场景的学习。<br>• MoDGS提出了一种新型的三维感知初始化方法来学习合理的变形场和新的鲁棒深度损失来指导动态场景几何学的学习。<br>• 实验结果表明，MoDGS可以渲染高质量的动态场景新视图图像，远超baseline方法。<br>• MoDGS可以处理casually captured monocular videos。<br>• MoDGS的方法可以学习动态场景的几何学和变形场。</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: Monocular Dynamic Gaussian Splatting (单目动态高斯涂抹)</p></li><li><p>Authors: Liu et al.</p></li><li><p>Affiliation: 未提供</p></li><li><p>Keywords: Novel View Synthesis, Dynamic Scene, Monocular Video, Gaussian Splatting</p></li><li><p>Urls: None, Github:None</p></li><li><p>Summary:</p></li></ol><ul><li><p>(1):该论文的研究背景是新视图合成（Novel View Synthesis）在动态场景中的应用，而现有的方法在处理单目视频时存在限制。</p></li><li><p>(2):过去的方法需要多视图视频或具有极大运动的单目视频来实现高质量的新视图合成，但这些方法在处理常规拍摄的单目视频时效果不佳。</p></li><li><p>(3):本文提出了Monocular Dynamic Gaussian Splatting（单目动态高斯涂抹）方法，该方法通过采用单目深度估计三维aware初始化来学习时间相关的变形场，并使用 ordinal depth loss 来监督渲染深度图。</p></li><li><p>(4):本文的方法在两种数据集和一个自收集的数据集上进行了实验结果表明该方法可以实现高质量的新视图合成，并且比现有的方法有大的性能优势。</p></li></ul><ol><li>方法：</li></ol><ul><li><p>(1)：给定一段随意拍摄的单目视频，我们的目标是从该视频中合成新视图图像。为此，我们将视频拆分成一系列图像{𝐼𝑡 |𝑡 = 1, …,𝑇 }，并假设所有图像的相机位姿都是已知的。</p></li><li><p>(2)：对于每个图像 𝐼𝑡，我们使用单视图深度估算器 GeoWizard [Fu et al. 2024] 估算一个深度图 𝐷𝑡，并使用流估算方法 RAFT [Teed and Deng 2020] 估算一个二维光流 𝐹𝑡𝑖→𝑡𝑗 之间 𝐼𝑡𝑖 和 𝐼𝑡𝑗 。</p></li><li><p>(3)：我们初始化变形场 𝑇𝑡 通过一个三维感知初始化方案，如 Sec. 3.2 所述。然后，我们使用一个渲染损失和一个新引入的深度损失来训练高斯体和变形场，如 Sec. 3.3 所述。</p></li><li><p>(4)：在训练过程中，我们定义了一组高斯体在典型空间中，每个高斯体都有一个三维位置、一个尺度向量、一个旋转和一个颜色，以球谐函数表示。</p></li><li><p>(5)：我们使用变形场 𝑇𝑡 将高斯体从典型空间变形到特定的时间步 𝑡，然后使用分裂技术来渲染图像。</p></li><li><p>(6)：在渲染过程中，我们遵循 3D GS [Kerbl et al. 2023] 中的分裂技术来渲染图像。</p></li></ul><ol><li>Conclusion:</li></ol><ul><li><p>(1):该篇工作的意义在于提出了一种新的单目动态高斯涂抹（Monocular Dynamic Gaussian Splatting）方法，能够从随意拍摄的单目视频中合成高质量的新视图图像，这对新视图合成技术的发展具有重要意义。</p></li><li><p>(2):创新点：该方法引入了三维感知初始化方案和ordinal depth loss，解决了单目视频新视图合成中的深度估计和变形场优化问题；性能：实验结果表明该方法在三个数据集上的性能优于基线方法；工作量：该方法需要大量的计算资源和数据集来训练高斯体和变形场。</p></li></ul><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-1549178cb60d7f194174a1a27981ffde.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-037f8718c0a69bdd6927b8e59f439157.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-31d5b80b1fb6a6d144c84378c897ddf5.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-ac6f4ffe1bf67aaa2e7f9e977ced2e27.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-29da24b0057e96725725e8333552d5e6.jpg" align="middle"></details><h2 id="ContextGS-Compact-3D-Gaussian-Splatting-with-Anchor-Level-Context-Model"><a href="#ContextGS-Compact-3D-Gaussian-Splatting-with-Anchor-Level-Context-Model" class="headerlink" title="ContextGS: Compact 3D Gaussian Splatting with Anchor Level Context Model"></a>ContextGS: Compact 3D Gaussian Splatting with Anchor Level Context Model</h2><p><strong>Authors:Yufei Wang, Zhihao Li, Lanqing Guo, Wenhan Yang, Alex C. Kot, Bihan Wen</strong></p><p>Recently, 3D Gaussian Splatting (3DGS) has become a promising framework for novel view synthesis, offering fast rendering speeds and high fidelity. However, the large number of Gaussians and their associated attributes require effective compression techniques. Existing methods primarily compress neural Gaussians individually and independently, i.e., coding all the neural Gaussians at the same time, with little design for their interactions and spatial dependence. Inspired by the effectiveness of the context model in image compression, we propose the first autoregressive model at the anchor level for 3DGS compression in this work. We divide anchors into different levels and the anchors that are not coded yet can be predicted based on the already coded ones in all the coarser levels, leading to more accurate modeling and higher coding efficiency. To further improve the efficiency of entropy coding, e.g., to code the coarsest level with no already coded anchors, we propose to introduce a low-dimensional quantized feature as the hyperprior for each anchor, which can be effectively compressed. Our work pioneers the context model in the anchor level for 3DGS representation, yielding an impressive size reduction of over 100 times compared to vanilla 3DGS and 15 times compared to the most recent state-of-the-art work Scaffold-GS, while achieving comparable or even higher rendering quality.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.20721v1">PDF</a></p><p>sorry, 您的ip已由于触发防滥用检测而被封禁,可能是因为使用云服务器如腾讯云或者国外代理(vpn)访问本网站，如果使用了vpn，关闭vpn或代理即可继续使用,本服务网址是<a target="_blank" rel="noopener" href="https://chat18.aichatos8.com">https://chat18.aichatos8.com</a> 如需合作接口调用请联系微信chatkf123 或者前往 <a target="_blank" rel="noopener" href="https://binjie09.shop">https://binjie09.shop</a> 自助购买key, 认为是误封需要解封的请前往<a target="_blank" rel="noopener" href="https://www.ip.cn/">https://www.ip.cn/</a> 查询ip信息,并发送信息至邮件 gpt33@binjie.site ，站长会定期看邮件并处理解封和合作问题，如需调用接口请见接口文档<a target="_blank" rel="noopener" href="https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123">https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123</a> 或者前往 <a target="_blank" rel="noopener" href="https://cat.gptxyy.cn">https://cat.gptxyy.cn</a> 注册使用（可付费使用gpt4 注册可免费使用3.5）</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>Title: ContextGS：紧凑的三维高斯 Splatting 带锚点级别上下文模型 (Compact 3D Gaussian Splatting with Anchor Level Context Model)</li></ol><ol><li>Authors: Yufei Wang, Zhihao Li, Lanqing Guo, Wenhan Yang, Alex C. Kot, Bihan Wen</li></ol><ol><li>Affiliation: 南洋理工大学</li></ol><ol><li>Keywords: 3D Gaussian Splatting, novel view synthesis, compression, autoregressive model</li></ol><ol><li>Urls: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.20721">https://arxiv.org/abs/2405.20721</a>, Github: <a target="_blank" rel="noopener" href="https://github.com/wyf0912/ContextGS">https://github.com/wyf0912/ContextGS</a></li></ol><ol><li>Summary:</li></ol><pre><code>- (1):最近，三维高斯 Splatting（3DGS）框架在新视图合成方面展现出很高的潜力，提供了快速的渲染速度和高保真度。然而，高斯体的数量及其关联属性需要有效的压缩技术。


- (2):现有的方法主要独立地压缩神经高斯体，忽视了它们之间的交互和空间依赖性。这种方法存在空间冗余的问题，影响了压缩效率。


- (3):本文提出了一种锚点级别的自回归模型，用于压缩三维高斯 Splatting。我们将锚点分为不同的级别，并使用已经编码的锚点来预测未编码的锚点，提高了压缩效率。


- (4):本文的方法在新视图合成任务取得了很好的性能，渲染质量和速度都得到了提，同时也实现了高达15倍的压缩比。
</code></pre><ol><li>方法：</li></ol><ul><li>(1)：将锚点分为不同的级别，使用可追溯的映射关系在相邻级别之间（anchor partitioning strategy），以实现高效的压缩和解压缩。</li></ul><ul><li>(2)：使用自回归模型对锚点进行编码，以预测未编码的锚点提高压缩效率（autoregressive model for anchor coding）；</li></ul><ul><li>(3)：使用基于hyperprior的上下文模型来测锚点的属性，进一步提高压缩效率（hyperprior-based context model for anchor attribute prediction）；</li></ul><ul><li>(4)：使用基于voxel size的方法对锚点进行分区，实现高效的压缩和解压缩（voxel size-based method for anchor partitioning）；</li></ul><ul><li>(5)：使用 binary search 来确定voxel size的参数，以避免对每个场景的参数微调（binary search for voxel size parameter determination）；</li></ul><ul><li>(6)：使用基于 entropy 的方法对锚点的属性进行编码，实现高效的压缩（entropy-based method for anchor attribute coding）；</li></ul><ul><li>(7)：使用joint optimization 的方法同时优化 bitrate 和 rendering loss，以实现高质量的新视图合成（joint optimization for bitrate and rendering loss）</li></ul><ul><li>(8)：使用 Scaffold-GS 框架来实现新视图合成，并在测试阶段不需要引入额外的开销（Scaffold-GS framework for novel view synthesis）.</li></ul><p>sorry, 您的ip已由于触发防滥用检测而被封禁,可能是因为使用云服务器如腾讯云或者国外代理(vpn)访问本网站，如果使用了vpn，关闭vpn或代理即可继续使用,本服务网址是<a target="_blank" rel="noopener" href="https://chat18.aichatos8.com">https://chat18.aichatos8.com</a> 如需合作接口调用请联系微信chatkf123 或者前往 <a target="_blank" rel="noopener" href="https://binjie09.shop">https://binjie09.shop</a> 自助购买key, 认为是误封需要解封的请前往<a target="_blank" rel="noopener" href="https://www.ip.cn/">https://www.ip.cn/</a> 查询ip信息,并发送信息至邮件 gpt33@binjie.site ，站长会定期看邮件并处理解封和合作问题，如需调用接口请见接口文档<a target="_blank" rel="noopener" href="https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123">https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123</a> 或者前往 <a target="_blank" rel="noopener" href="https://cat.gptxyy.cn">https://cat.gptxyy.cn</a> 注册使用（可付费使用gpt4 注册可免费使用3.5）</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-6a81c98ff4646b8801ce48fd00017484.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-4f113e2e6fa2e780c2e467c1b0df0909.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-ea99a9dead64e65ce89c907d223689ca.jpg" align="middle"></details><h2 id="NPGA-Neural-Parametric-Gaussian-Avatars"><a href="#NPGA-Neural-Parametric-Gaussian-Avatars" class="headerlink" title="NPGA: Neural Parametric Gaussian Avatars"></a>NPGA: Neural Parametric Gaussian Avatars</h2><p><strong>Authors:Simon Giebenhain, Tobias Kirschstein, Martin Rünz, Lourdes Agapito, Matthias Nießner</strong></p><p>The creation of high-fidelity, digital versions of human heads is an important stepping stone in the process of further integrating virtual components into our everyday lives. Constructing such avatars is a challenging research problem, due to a high demand for photo-realism and real-time rendering performance. In this work, we propose Neural Parametric Gaussian Avatars (NPGA), a data-driven approach to create high-fidelity, controllable avatars from multi-view video recordings. We build our method around 3D Gaussian Splatting for its highly efficient rendering and to inherit the topological flexibility of point clouds. In contrast to previous work, we condition our avatars’ dynamics on the rich expression space of neural parametric head models (NPHM), instead of mesh-based 3DMMs. To this end, we distill the backward deformation field of our underlying NPHM into forward deformations which are compatible with rasterization-based rendering. All remaining fine-scale, expression-dependent details are learned from the multi-view videos. To increase the representational capacity of our avatars, we augment the canonical Gaussian point cloud using per-primitive latent features which govern its dynamic behavior. To regularize this increased dynamic expressivity, we propose Laplacian terms on the latent features and predicted dynamics. We evaluate our method on the public NeRSemble dataset, demonstrating that NPGA significantly outperforms the previous state-of-the-art avatars on the self-reenactment task by 2.6 PSNR. Furthermore, we demonstrate accurate animation capabilities from real-world monocular videos.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.19331v1">PDF</a> Project Page: see <a target="_blank" rel="noopener" href="https://simongiebenhain.github.io/NPGA/">https://simongiebenhain.github.io/NPGA/</a> ; Youtube Video: see <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=NGRxAYbIkus">https://www.youtube.com/watch?v=NGRxAYbIkus</a></p><p>sorry, 您的ip已由于触发防滥用检测而被封禁,可能是因为使用云服务器如腾讯云或者国外代理(vpn)访问本网站，如果使用了vpn，关闭vpn或代理即可继续使用,本服务网址是<a target="_blank" rel="noopener" href="https://chat18.aichatos8.com">https://chat18.aichatos8.com</a> 如需合作接口调用请联系微信chatkf123 或者前往 <a target="_blank" rel="noopener" href="https://binjie09.shop">https://binjie09.shop</a> 自助购买key, 认为是误封需要解封的请前往<a target="_blank" rel="noopener" href="https://www.ip.cn/">https://www.ip.cn/</a> 查询ip信息,并发送信息至邮件 gpt33@binjie.site ，站长会定期看邮件并处理解封和合作问题，如需调用接口请见接口文档<a target="_blank" rel="noopener" href="https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123">https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123</a> 或者前往 <a target="_blank" rel="noopener" href="https://cat.gptxyy.cn">https://cat.gptxyy.cn</a> 注册使用（可付费使用gpt4 注册可免费使用3.5）</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: NPGA： Neural Parametric Gaussian Avatars（NPGA：神经参数高斯头像）</p></li><li><p>Authors: SIMON GIEBENHAIN, TOBIAS KIRSCHSTEIN, MARTIN RÜNZ, LOURDES AGAPITO, MATTHIAS NIESSNER</p></li><li><p>Affiliation: 德国慕尼黑工业大学</p></li><li><p>Keywords: Neural Parametric Gaussian Avatars, 3D Gaussian Splatting, Digital Humans, Avatar Reconstruction</p></li><li><p>Urls: <a target="_blank" rel="noopener" href="https://simongiebenhain.github.io/NPGA/">https://simongiebenhain.github.io/NPGA/</a>, Github:None</p></li><li><p>Summary:</p></li></ol><ul><li>(1):本文的研究背景是创建高度逼真的数字头像，用于电影、游戏、AR/VR 电信会议和元宇宙等领域。</li></ul><ul><li>(2):过去的方法主要基于三维可变形模型（3DMM），但是这些方法存在一些问题，例如细节不够丰富、身份和表情描述不够分离等。本文的方法motivated by这些问题，旨在创建更加逼真的数字头像。</li></ul><ul><li>(3):本文提出的研究方法是基于神经参数高斯头像（NPGA），该方法使用三维高斯点云和神经参数头模型（NPHM）来创建数字头像，并使用per-primitive特征来增强头像的动态表达能力。</li></ul><ul><li>(4):本文的方法在NeRSemble数据集上的自reenactment任务上取得了约2.6 PSNR的性能提升，证明了该方法的有效性。同时，本文还展示了从真实世界单目视频的精准动画能力。</li></ul><ol><li>方法：</li></ol><ul><li><p>(1):首先，作者使用三维高斯点云（3D Gaussian Splatting）将头像表面表示为一组高斯分布，这些分布可以捕捉头像的局部几何形状和细节特征。</p></li><li><p>(2):然后，作者引入神经参数头模型（NPHM），该模型使用神经网络学习头像的参数表示，包括身份、表情和姿态信息。</p></li><li><p>(3):接着，作者使用三维高斯点云和神经参数头模型结合，生成数字头像的per-primitive特征，这些特征可以增强头像的动态表达能力。</p></li><li><p>(4):在头像重建阶段，作者使用per-primitive特征和三维高斯点云进行头像重建，生成高质量的数字头像。</p></li><li><p>(5):最后，作者在NeRSemble数据集上进行自reenactment任务的实验，证明了NPGA方法的有效性，并展示了从真实世界单目视频的精准动画能力。</p></li><li><p>(6):作者还讨论了NPGA方法的优点，包括可以生成高质量的数字头像、能够捕捉头像的细节特征和动态表达能力等。</p></li></ul><ol><li>结论：</li></ol><ul><li><p>(1):本文的研究工作对于创建高度逼真的数字头像具有重要意义，可以应用于电影、游戏、AR/VR 电信会议和元宇宙等领域，提高数字头像的表达能力和逼真度。</p></li><li><p>(2):创新点：本文提出的NPGA方法将三维高斯点云和神经参数头模型结合，生成数字头像的per-primitive特征，提高了头像的动态表达能力和细节特征捕捉能力；性能：本文的方法在NeRSemble数据集上的自reenactment任务上取得了约2.6 PSNR的性能提升，证明了该方法的有效性；工作量：本文的方法需要大量的数据和计算资源，限制了其在实际应用中的使用范围。</p></li></ul><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-e1ebdb40880659f3f276da0e13675a00.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-fc4ed51dc083b8b6a51414491a73d806.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-f81503095d5f9b2100c356802a0daa7c.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-3db991818ec4bced433235a789fd7993.jpg" align="middle"></details></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io/2024/06/05/Paper/2024-06-05/3DGS/">https://kedreamix.github.io/2024/06/05/Paper/2024-06-05/3DGS/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/3DGS/">3DGS</a></div><div class="post_share"><div class="social-share" data-image="https://picx.zhimg.com/v2-2196bb11e15de7b5d3440823b4c92ca5.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/06/05/Paper/2024-06-05/NeRF/" title="NeRF"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2bdb0ecbbc3a0a2420781e472b68ba52.png" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">NeRF</div></div></a></div><div class="next-post pull-right"><a href="/2024/06/05/Paper/2024-06-05/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/80/v2-890676236f48f9a7d915a0c42c40aa38.png" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Diffusion Models</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/01/25/Paper/3DGS%20Survey/" title="3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-25</div><div class="title">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</div></div></a></div><div><a href="/2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" title="超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-01</div><div class="title">超赞的数字人生成知识库 Awesome-Talking-Head-Synthesis</div></div></a></div><div><a href="/2024/01/24/Paper/2024-01-24/3DGS/" title="3DGS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-3d3dcd00c27bc3d320b23d4247ae79f3.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-24</div><div class="title">3DGS</div></div></a></div><div><a href="/2024/01/30/Paper/2024-01-30/3DGS/" title="3DGS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-2032721a60695f2d41ac96f75dec65a2.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-30</div><div class="title">3DGS</div></div></a></div><div><a href="/2024/02/13/Paper/2024-02-13/3DGS/" title="3DGS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-785f0dd46228bdf108d1677b776eeb58.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-13</div><div class="title">3DGS</div></div></a></div><div><a href="/2024/02/02/Paper/2024-02-02/3DGS/" title="3DGS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-e4e5570dfa99dfac9b297f7650c717c3.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-02</div><div class="title">3DGS</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-06-05-%E6%9B%B4%E6%96%B0"><span class="toc-text">2024-06-05 更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#DDGS-CT-Direction-Disentangled-Gaussian-Splatting-for-Realistic-Volume-Rendering"><span class="toc-text">DDGS-CT: Direction-Disentangled Gaussian Splatting for Realistic Volume Rendering</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#WE-GS-An-In-the-wild-Efficient-3D-Gaussian-Representation-for-Unconstrained-Photo-Collections"><span class="toc-text">WE-GS: An In-the-wild Efficient 3D Gaussian Representation for Unconstrained Photo Collections</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#OpenGaussian-Towards-Point-Level-3D-Gaussian-based-Open-Vocabulary-Understanding"><span class="toc-text">OpenGaussian: Towards Point-Level 3D Gaussian-based Open Vocabulary Understanding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#FastLGS-Speeding-up-Language-Embedded-Gaussians-with-Feature-Grid-Mapping"><span class="toc-text">FastLGS: Speeding up Language Embedded Gaussians with Feature Grid Mapping</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DreamPhysics-Learning-Physical-Properties-of-Dynamic-3D-Gaussians-with-Video-Diffusion-Priors"><span class="toc-text">DreamPhysics: Learning Physical Properties of Dynamic 3D Gaussians with Video Diffusion Priors</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Self-Calibrating-4D-Novel-View-Synthesis-from-Monocular-Videos-Using-Gaussian-Splatting"><span class="toc-text">Self-Calibrating 4D Novel View Synthesis from Monocular Videos Using Gaussian Splatting</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Topo4D-Topology-Preserving-Gaussian-Splatting-for-High-Fidelity-4D-Head-Capture"><span class="toc-text">Topo4D: Topology-Preserving Gaussian Splatting for High-Fidelity 4D Head Capture</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MoDGS-Dynamic-Gaussian-Splatting-from-Causually-captured-Monocular-Videos"><span class="toc-text">MoDGS: Dynamic Gaussian Splatting from Causually-captured Monocular Videos</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ContextGS-Compact-3D-Gaussian-Splatting-with-Anchor-Level-Context-Model"><span class="toc-text">ContextGS: Compact 3D Gaussian Splatting with Anchor Level Context Model</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#NPGA-Neural-Parametric-Gaussian-Avatars"><span class="toc-text">NPGA: Neural Parametric Gaussian Avatars</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image:url('https://picx.zhimg.com/v2-2196bb11e15de7b5d3440823b4c92ca5.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js").then((()=>{pangu.autoSpacingPage()}))}function panguInit(){panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.1},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script></div><script id="canvas_nest" defer color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><script data-pjax>function butterfly_swiper_injector_config(){var a=document.getElementById("recent-posts");console.log("已挂载butterfly_swiper"),a.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/25/Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="2024/01/25/Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="2024/01/25/Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/17/Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023/12/17/Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="2023/12/17/Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script></body></html>