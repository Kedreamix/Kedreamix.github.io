<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>Diffusion Models | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-06-05  CamCo Camera-Controllable 3D-Consistent Image-to-Video Generation"><meta property="og:type" content="article"><meta property="og:title" content="Diffusion Models"><meta property="og:url" content="https://kedreamix.github.io/2024/06/05/Paper/2024-06-05/Diffusion%20Models/index.html"><meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World"><meta property="og:description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-06-05  CamCo Camera-Controllable 3D-Consistent Image-to-Video Generation"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://pica.zhimg.com/80/v2-890676236f48f9a7d915a0c42c40aa38.png"><meta property="article:published_time" content="2024-06-05T10:15:50.000Z"><meta property="article:modified_time" content="2024-06-05T10:15:50.765Z"><meta property="article:author" content="Kedreamix"><meta property="article:tag" content="Diffusion Models"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://pica.zhimg.com/80/v2-890676236f48f9a7d915a0c42c40aa38.png"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/2024/06/05/Paper/2024-06-05/Diffusion%20Models/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-74LZ5BEQQ1")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!0,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"简"},noticeOutdate:{limitDay:500,position:"top",messagePrev:"It has been",messageNext:"days since the last update, the content of the article may be outdated."},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:200},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!0,post:!0},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!0,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"Diffusion Models",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-06-05 18:15:50"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach((e=>{n.setAttribute(e,t[e])})),document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)})),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=24?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 7.0.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml"></head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{const d=document.getElementById("loading-box"),e=document.body,o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l(),window.addEventListener("load",(()=>{o()}))})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">146</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url('https://pica.zhimg.com/80/v2-890676236f48f9a7d915a0c42c40aa38.png')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Diffusion Models</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-06-05T10:15:50.000Z" title="发表于 2024-06-05 18:15:50">2024-06-05</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-06-05T10:15:50.765Z" title="更新于 2024-06-05 18:15:50">2024-06-05</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">15.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>60分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="Diffusion Models"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-06-05-更新"><a href="#2024-06-05-更新" class="headerlink" title="2024-06-05 更新"></a>2024-06-05 更新</h1><h2 id="CamCo-Camera-Controllable-3D-Consistent-Image-to-Video-Generation"><a href="#CamCo-Camera-Controllable-3D-Consistent-Image-to-Video-Generation" class="headerlink" title="CamCo: Camera-Controllable 3D-Consistent Image-to-Video Generation"></a>CamCo: Camera-Controllable 3D-Consistent Image-to-Video Generation</h2><p><strong>Authors:Dejia Xu, Weili Nie, Chao Liu, Sifei Liu, Jan Kautz, Zhangyang Wang, Arash Vahdat</strong></p><p>Recently video diffusion models have emerged as expressive generative tools for high-quality video content creation readily available to general users. However, these models often do not offer precise control over camera poses for video generation, limiting the expression of cinematic language and user control. To address this issue, we introduce CamCo, which allows fine-grained Camera pose Control for image-to-video generation. We equip a pre-trained image-to-video generator with accurately parameterized camera pose input using Pl\”ucker coordinates. To enhance 3D consistency in the videos produced, we integrate an epipolar attention module in each attention block that enforces epipolar constraints to the feature maps. Additionally, we fine-tune CamCo on real-world videos with camera poses estimated through structure-from-motion algorithms to better synthesize object motion. Our experiments show that CamCo significantly improves 3D consistency and camera control capabilities compared to previous models while effectively generating plausible object motion. Project page: <a target="_blank" rel="noopener" href="https://ir1d.github.io/CamCo/">https://ir1d.github.io/CamCo/</a></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.02509v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://ir1d.github.io/CamCo/">https://ir1d.github.io/CamCo/</a></p><p><strong>Summary</strong><br>视频扩散模型CamCo实现了图像到视频生成的精细相机姿态控制，提高了视频生成的三维一致性和摄像机控制能力。</p><p><strong>Key Takeaways</strong><br>• 视频扩散模型可以生成高质量的视频内容，但缺乏精细的摄像机姿态控制。<br>• CamCo模型通过Pl\”ucker坐标准确地参数化摄像机姿态输入，实现了图像到视频生成的精细控制。<br>• epipolar attention模块可以强制执行特征图的极线约束，从而提高视频生成的三维一致性。<br>• CamCo模型在真实世界视频上进行微调，以更好地合成物体运动。<br>• 实验结果表明，CamCo模型相比之前的模型具有更好的三维一致性和摄像机控制能力。<br>• CamCo模型可以生成可靠的物体运动。<br>• CamCo项目页面：<a target="_blank" rel="noopener" href="https://ir1d.github.io/CamCo/">https://ir1d.github.io/CamCo/</a></p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>Title: CamCo: Camera-Controllable 3D-Consistent图像到视频生成 (Camera-Controllable 3D-Consistent Image-to-Video Generation)</li></ol><ol><li>Authors: Dejia Xu, Weili Nie, Chao Liu, Sifei Liu, Jan Kautz, Zhangyang Wang, Arash Vahdat</li></ol><ol><li>Affiliation: 德克萨斯大学奥斯汀分校 (University of Texas at Austin)</li></ol><ol><li>Keywords: video diffusion models, image-to-video generation, camera pose control, 3D consistency</li></ol><ol><li>Urls: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02509v1">https://arxiv.org/abs/2406.02509v1</a>, Github:None</li></ol><ol><li>Summary:</li></ol><pre><code>- (1):近年来，视频扩散模型已经成为高质量视频内容创建的有表达力生成工具，但这些模型通常不提供精准的摄像机姿态控制，限制了电影语言和用户控制的表达。

- (2):过去的方法不提供摄像机姿态控制，限制了电影语言和用户控制的表达。 CamCo 模型的提出是为了解决这个问题。

- (3):本文提出 CamCo 模型，该模型可以精准控制摄像机姿态，生成 3D 一致的视频。我们使用 Plücker 坐标来参数化摄像机姿态，并集成 epipolar 注意力模块来强制执行 epipolar 约束。

- (4):实验结果表明，CamCo 模型可以生成 3D 一致的视频，且能够精准控制摄像机姿态，达到高质量的视频生成效果。
</code></pre><p>sorry, 您的ip已由于触发防滥用检测而被封禁,请勿滥用本站,本服务网址是<a target="_blank" rel="noopener" href="https://chat18.aichatos8.com">https://chat18.aichatos8.com</a> 如需合作接口调用请联系微信chatkf123 或者前往 <a target="_blank" rel="noopener" href="https://binjie09.shop">https://binjie09.shop</a> 自助购买key, 认为是误封需要解封的请前往<a target="_blank" rel="noopener" href="https://www.ip.cn/">https://www.ip.cn/</a> 查询ip信息,并发送信息至邮件 gpt33@binjie.site ，站长会定期看邮件并处理解封和合作问题，如需调用接口请见接口文档<a target="_blank" rel="noopener" href="https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82">https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82</a> 如需合作接口调用请联系微信chatkf123 或者前往 <a target="_blank" rel="noopener" href="https://cat.gptxyy.cn">https://cat.gptxyy.cn</a> 注册使用（可付费使用gpt4 注册可免费使用3.5）</p><p>sorry, 您的ip已由于触发防滥用检测而被封禁,可能是因为使用云服务器如腾讯云或者国外代理(vpn)访问本网站，如果使用了vpn，关闭vpn或代理即可继续使用,本服务网址是<a target="_blank" rel="noopener" href="https://chat18.aichatos8.com">https://chat18.aichatos8.com</a> 如需合作接口调用请联系微信chatkf123 或者前往 <a target="_blank" rel="noopener" href="https://binjie09.shop">https://binjie09.shop</a> 自助购买key, 认为是误封需要解封的请前往<a target="_blank" rel="noopener" href="https://www.ip.cn/">https://www.ip.cn/</a> 查询ip信息,并发送信息至邮件 gpt33@binjie.site ，站长会定期看邮件并处理解封和合作问题，如需调用接口请见接口文档<a target="_blank" rel="noopener" href="https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123">https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123</a> 或者前往 <a target="_blank" rel="noopener" href="https://cat.gptxyy.cn">https://cat.gptxyy.cn</a> 注册使用（可付费使用gpt4 注册可免费使用3.5）</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/1c8be5285001bbf933e19f41ce9137e5241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/4d92a8d29d3149c75770b9b4c963a79a241286257.jpg" align="middle"></details><h2 id="Guiding-a-Diffusion-Model-with-a-Bad-Version-of-Itself"><a href="#Guiding-a-Diffusion-Model-with-a-Bad-Version-of-Itself" class="headerlink" title="Guiding a Diffusion Model with a Bad Version of Itself"></a>Guiding a Diffusion Model with a Bad Version of Itself</h2><p><strong>Authors:Tero Karras, Miika Aittala, Tuomas Kynkäänniemi, Jaakko Lehtinen, Timo Aila, Samuli Laine</strong></p><p>The primary axes of interest in image-generating diffusion models are image quality, the amount of variation in the results, and how well the results align with a given condition, e.g., a class label or a text prompt. The popular classifier-free guidance approach uses an unconditional model to guide a conditional model, leading to simultaneously better prompt alignment and higher-quality images at the cost of reduced variation. These effects seem inherently entangled, and thus hard to control. We make the surprising observation that it is possible to obtain disentangled control over image quality without compromising the amount of variation by guiding generation using a smaller, less-trained version of the model itself rather than an unconditional model. This leads to significant improvements in ImageNet generation, setting record FIDs of 1.01 for 64x64 and 1.25 for 512x512, using publicly available networks. Furthermore, the method is also applicable to unconditional diffusion models, drastically improving their quality.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.02507v1">PDF</a></p><p>sorry, 您的ip已由于触发防滥用检测而被封禁,可能是因为使用云服务器如腾讯云或者国外代理(vpn)访问本网站，如果使用了vpn，关闭vpn或代理即可继续使用,本服务网址是<a target="_blank" rel="noopener" href="https://chat18.aichatos8.com">https://chat18.aichatos8.com</a> 如需合作接口调用请联系微信chatkf123 或者前往 <a target="_blank" rel="noopener" href="https://binjie09.shop">https://binjie09.shop</a> 自助购买key, 认为是误封需要解封的请前往<a target="_blank" rel="noopener" href="https://www.ip.cn/">https://www.ip.cn/</a> 查询ip信息,并发送信息至邮件 gpt33@binjie.site ，站长会定期看邮件并处理解封和合作问题，如需调用接口请见接口文档<a target="_blank" rel="noopener" href="https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123">https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123</a> 或者前往 <a target="_blank" rel="noopener" href="https://cat.gptxyy.cn">https://cat.gptxyy.cn</a> 注册使用（可付费使用gpt4 注册可免费使用3.5）</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: 使用不太好的自己版本来指导扩散模型（Guiding a Diffusion Model with a Bad Version of Itself）</p></li><li><p>Authors: (no author list provided)</p></li><li><p>Affiliation: (no affiliation provided)</p></li><li><p>Keywords: diffusion model, image generation, classifier-free guidance</p></li><li><p>Urls: (no URL provided), Github: None</p></li><li><p>Summary:</p><ul><li><p>(1):本文研究的是图像生成扩散模型中控制图像质量、多样性和条件对齐的三个主要axes问题。</p></li><li><p>(2):过去的方法中，classifier-free指导方法使用无条件模型来指导条件模型，提高了图像质量和条件对齐，但减少了多样性。这些效果似乎是耦合的，难以控制。</p></li><li><p>(3):本文提出了一种新的方法，即使用模型自身的较小、较少训练的版本来指导生成，而不是使用无条件模型。这使得可以独立控制图像质量，而不牺牲多样性。</p></li><li><p>(4):本文的方法在ImageNet生成任务上取得了最好的FID记录，分别为64×64和512×512的1.01和1.25。此外，该方法也适用于无条件扩散模型，大幅提高了其质量。</p></li></ul></li><li>方法：</li></ol><ul><li><p>(1)：提出了一种新的指导扩散模型的方法，即使用模型自身的较小、较少训练的版本来指导生成，而不是使用无条件模型（Classifier-Free Guidance）;</p></li><li><p>(2)：该方法可以独立控制图像质量，而不牺牲多样性，解决了过去方法中图像质量、多样性和条件对齐三个主要axes问题的耦合问题;</p></li><li><p>(3)：在指导模型的选择上，作者发现使用较小的模型容量和较少的训练次数可以获得更好的结果，且这两个因素的影响是orthogonal的;</p></li><li><p>(4)：作者还��究了指导权重、EMA长度参数等超参数对模型性能的影响，发现模型对这些参数的敏感性较高;</p></li><li><p>(5)：在实验中，作者使用了ImageNet-512和ImageNet-64两个数据集，结果表明该方法可以获得 state-of-the-art 的性能，特别是在图像质量和多样性方面;</p></li><li><p>(6)：作者还将该方法应用于无条件扩散模型，结果表明可以大幅提高其质量;</p></li><li><p>(7)：最后，作者讨论了该方法的未来发展方向，包括将其应用于大规模图像生成模型和探究其在艺术控制方面的可能性。</p></li></ul><ol><li>Conclusion:</li></ol><ul><li><p>(1): 本文的工作对于图像生成扩散模型的控制和优化具有重要意义，可以独立控制图像质量和多样性，解决了过去方法中的耦合问题。</p></li><li><p>(2): Innovation point: 本文提出的使用模型自身的较小、较少训练的版本来指导生成的方法是创新性的，解决了过去方法中的耦合问题； Performance: 本文的方法在ImageNet生成任务上取得了最好的FID记录，分别为64×64和512×512的1.01和1.25，证明了其优越的性能； Workload: 本文的实验使用了ImageNet-512和ImageNet-64两个数据集，实验设计和结果分析较为充分，但超参数的调整和选择可能需要更多的实验和分析。</p></li></ul><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/ece422492bfcb0ed72fed53bc4f94053241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/79b8e3c566552b5ee83993b5efabfbef241286257.jpg" align="middle"></details><h2 id="Stable-Pose-Leveraging-Transformers-for-Pose-Guided-Text-to-Image-Generation"><a href="#Stable-Pose-Leveraging-Transformers-for-Pose-Guided-Text-to-Image-Generation" class="headerlink" title="Stable-Pose: Leveraging Transformers for Pose-Guided Text-to-Image   Generation"></a>Stable-Pose: Leveraging Transformers for Pose-Guided Text-to-Image Generation</h2><p><strong>Authors:Jiajun Wang, Morteza Ghahremani, Yitong Li, Björn Ommer, Christian Wachinger</strong></p><p>Controllable text-to-image (T2I) diffusion models have shown impressive performance in generating high-quality visual content through the incorporation of various conditions. Current methods, however, exhibit limited performance when guided by skeleton human poses, especially in complex pose conditions such as side or rear perspectives of human figures. To address this issue, we present Stable-Pose, a novel adapter model that introduces a coarse-to-fine attention masking strategy into a vision Transformer (ViT) to gain accurate pose guidance for T2I models. Stable-Pose is designed to adeptly handle pose conditions within pre-trained Stable Diffusion, providing a refined and efficient way of aligning pose representation during image synthesis. We leverage the query-key self-attention mechanism of ViTs to explore the interconnections among different anatomical parts in human pose skeletons. Masked pose images are used to smoothly refine the attention maps based on target pose-related features in a hierarchical manner, transitioning from coarse to fine levels. Additionally, our loss function is formulated to allocate increased emphasis to the pose region, thereby augmenting the model’s precision in capturing intricate pose details. We assessed the performance of Stable-Pose across five public datasets under a wide range of indoor and outdoor human pose scenarios. Stable-Pose achieved an AP score of 57.1 in the LAION-Human dataset, marking around 13% improvement over the established technique ControlNet. The project link and code is available at <a target="_blank" rel="noopener" href="https://github.com/ai-med/StablePose">https://github.com/ai-med/StablePose</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.02485v1">PDF</a></p><p><strong>Summary</strong><br>提出Stable-Pose模型，引入粗到细的注意遮罩策略，提高文本到图像生成模型对人体姿势的控制能力。</p><p><strong>Key Takeaways</strong><br>• Stable-Pose模型引入粗到细的注意遮罩策略，提高了文本到图像生成模型对人体姿势的控制能力。<br>• 该模型基于视觉Transformer（ViT），使用查询键自注意机制来探索人体姿势骨架中不同解剖部分之间的交叉连接。<br>• 掩码姿势图像被用于平滑地细化注意图，基于目标姿势相关特征，以层次方式从粗到细。<br>• 损失函数被设为将更多的权重分配给姿势区域，以增强模型捕捉细腻姿势细节的精度。<br>• Stable-Pose模型在五个公共数据集下的广泛室内和室外人体姿势场景中进行评估，取得了优异的结果。<br>• 在LAION-Human数据集上，Stable-Pose模型获得了57.1的AP分数，相比于ControlNet技术提高了约13%。<br>• 项目链接和代码可在<a target="_blank" rel="noopener" href="https://github.com/ai-med/StablePose上获取。">https://github.com/ai-med/StablePose上获取。</a></p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: Stable-Pose：Pose-Guided Text-to-Image Generation（基于姿态的文本到图像生成）</p></li><li><p>Authors: Jiajun Wang, Morteza Ghahremani, Yitong Li, Björn Ommer, Christian Wachinger</p></li><li><p>Affiliation: 德国慕尼黑工业大学（Technical University of Munich）</p></li><li><p>Keywords: Pose-Guided Text-to-Image Generation, Vision Transformer, Stable Diffusion</p></li><li><p>Urls: <a target="_blank" rel="noopener" href="https://github.com/ai-med/StablePose">https://github.com/ai-med/StablePose</a></p></li><li><p>Summary:</p><ul><li><p>(1):本文研究的背景是基于文本和姿态信息生成高质量图像的技术，具有广泛的应用前景。</p></li><li><p>(2):过去的方法需要源图像来指导生成的图像风格，而这些方法复杂姿态条件下生成图像的性能不佳。 Stable-Pose 模型旨在解决这些问题。</p></li><li><p>(3):本文提出的研究方法是引入糙到细粒度的注意力masking策略到视觉Transformer（ViT）中，以获取准确的姿态指导信息。同时，使用masked姿态图像来平滑地细化注意力图，提高模型对姿态细节的捕捉能力。</p></li><li><p>(4):本文的方法在五个公共数据集下的性能评估结果表明，Stable-Pose 模型在 LAION-Human 数据集上取得了 57.1 的 AP 分数，相比 ControlNet 方法提高了约 13%。这表明 Stable-Pose 模型能够生成高质量的像，满足应用需求。</p></li></ul></li><li>方法：</li></ol><ul><li><p>(1):引入基于文本和姿态信息的Pose-Guided Text-to-Image Generation任务，旨在生成高质量图像。</p></li><li><p>(2):提出使用Vision Transformer（ViT）作为基础架构，以获取准确的姿态指导信息。</p></li><li><p>(3):引入糙到细粒度的注意力masking策略到ViT中，以获取准确的姿态指导信息。</p></li><li><p>(4):使用masked姿态图像来平滑地细化注意力图，提高模型对姿态细节的捕捉能力。</p></li><li><p>(5):采用Stable Diffusion方法来生成图像，确保生成的图像具有高质量和真性。</p></li><li><p>(6):使用五个公共数据集（包括LAION-Human数据集）来评估Stable-Pose模型的性能，结果表明该模型能够生成高质量的图像，满足应用需求。</p></li></ul><ol><li>Conclusion:</li></ol><ul><li><p>(1):本文提出的Stable-Pose模型在 Pose-Guided Text-to-Image Generation领域具有重要意义，它可以生成高质量的图像，满足应用需求，具有广泛的应用前景。</p></li><li><p>(2):创新点：Stable-Pose模型引入了基于文本和姿态信息的Pose-Guided Text-to-Image Generation任务，并提出使用Vision Transformer（ViT）作为基础架构，引入糙到细粒度的注意力masking策略来获取准确的姿态指导信息；性能：Stable-Pose模型在五个公共数据集下的性评估结果表明，它能够生成高质量的图像，满足应用需求，具有高生成鲁棒性；工作量：Stable-Pose模型的推理时间略长，主要是由于ViT中的self-attention机制的集成，但其设计允许对各种外部条件的直接适应，具有高应用潜力。</p></li></ul><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/ff3baa25e7d57dc0d3d150bd122a1406241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/9310f7f5bf930aab6265d4b0a26ca10f241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/0715a783340ef4b332910748478d64d9241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/666669f9054ec22d9219286330d08a97241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/186ec1358a6ffa6ae4d3d38deeeebdc7241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/4fe5adfd9504bedddd0dbdbeb362124f241286257.jpg" align="middle"></details><h2 id="Learning-Image-Priors-through-Patch-based-Diffusion-Models-for-Solving-Inverse-Problems"><a href="#Learning-Image-Priors-through-Patch-based-Diffusion-Models-for-Solving-Inverse-Problems" class="headerlink" title="Learning Image Priors through Patch-based Diffusion Models for Solving   Inverse Problems"></a>Learning Image Priors through Patch-based Diffusion Models for Solving Inverse Problems</h2><p><strong>Authors:Jason Hu, Bowen Song, Xiaojian Xu, Liyue Shen, Jeffrey A. Fessler</strong></p><p>Diffusion models can learn strong image priors from underlying data distribution and use them to solve inverse problems, but the training process is computationally expensive and requires lots of data. Such bottlenecks prevent most existing works from being feasible for high-dimensional and high-resolution data such as 3D images. This paper proposes a method to learn an efficient data prior for the entire image by training diffusion models only on patches of images. Specifically, we propose a patch-based position-aware diffusion inverse solver, called PaDIS, where we obtain the score function of the whole image through scores of patches and their positional encoding and utilize this as the prior for solving inverse problems. First of all, we show that this diffusion model achieves an improved memory efficiency and data efficiency while still maintaining the capability to generate entire images via positional encoding. Additionally, the proposed PaDIS model is highly flexible and can be plugged in with different diffusion inverse solvers (DIS). We demonstrate that the proposed PaDIS approach enables solving various inverse problems in both natural and medical image domains, including CT reconstruction, deblurring, and superresolution, given only patch-based priors. Notably, PaDIS outperforms previous DIS methods trained on entire image priors in the case of limited training data, demonstrating the data efficiency of our proposed approach by learning patch-based prior.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.02462v1">PDF</a></p><p>sorry, 您的ip已由于触发防滥用检测而被封禁,可能是因为使用云服务器如腾讯云或者国外代理(vpn)访问本网站，如果使用了vpn，关闭vpn或代理即可继续使用,本服务网址是<a target="_blank" rel="noopener" href="https://chat18.aichatos8.com">https://chat18.aichatos8.com</a> 如需合作接口调用请联系微信chatkf123 或者前往 <a target="_blank" rel="noopener" href="https://binjie09.shop">https://binjie09.shop</a> 自助购买key, 认为是误封需要解封的请前往<a target="_blank" rel="noopener" href="https://www.ip.cn/">https://www.ip.cn/</a> 查询ip信息,并发送信息至邮件 gpt33@binjie.site ，站长会定期看邮件并处理解封和合作问题，如需调用接口请见接口文档<a target="_blank" rel="noopener" href="https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123">https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123</a> 或者前往 <a target="_blank" rel="noopener" href="https://cat.gptxyy.cn">https://cat.gptxyy.cn</a> 注册使用（可付费使用gpt4 注册可免费使用3.5）</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: 通过基于patch的扩散学习图像prior（Learning Image Priors through Patch-based Diffusion）</p></li><li><p>Authors: Jason Hu, Bowen Song, Xiaojian Xu, Liyue Shen, Jeffrey A. Fessler</p></li><li><p>Affiliation: 密歇根大学电子与计算机工程系（University of Michigan, Department of Electrical and Computer Engineering）</p></li><li><p>Keywords: 扩散模型, 图像prior, 逆问题, Patch-based</p></li><li><p>Urls: arXiv:2406.02462v1 , Github:None</p></li><li><p>Summary:</p><ul><li><p>(1):现有的扩散模型可以从数据分布中学习强的图像prior，并用于解决逆问题，但是训练过程计算昂贵，需要大量数据，这限制了它们在高维和高分辨率数据（如3D图像）上的应用。</p></li><li><p>(2):过去的方法需要在整个图像上训练扩模型，这要求大量计算资源和数据，限制了它们的实用性。这些方法没有充分考虑图像的patch结构和位置信息。</p></li><li><p>(3):本文提出了一种基于patch的扩散逆问题求解器（PaDIS），通过训练patch-based扩散模型，学习整个图像prior，并使用位置编码来生成整个图像。</p></li><li><p>(4):实验结果表明，PaDIS模型可以在保持成整个图像能力的同时，提高内存效率和数据效率，证明了该方法的有效性。</p></li></ul></li><li>方法：</li></ol><ul><li><p>(1): 将图像x分割成非重叠的patch，学习每个patch的分布，然后将它们组合以获得整个图像的分布。</p></li><li><p>(2): 为了避免patch之间的边界-artifact，使用零填充将图像x扩展到更大的尺寸，然后将分割成多个region，每个region包含一个中心patch和一个边界patch</p></li><li><p>(3): 对于每个region��学习中心patch的分布和边界patch的分布，然后将它们组合以获得整个region的分布。</p></li><li><p>(4): 通过score matching方法训练神经网络，以学习patch的分布。</p></li><li><p>(5): 使用UNet架构的神经网络，可以处理不同大小的patch输入。</p></li><li><p>(6): 在训练过程中，随机选择patch的大小和位置，以提高模型的泛化能力。</p></li><li><p>(7): 在 sampling 和 reconstruction 阶段，使用 Langevin dynamics 或 DDPM 等方法，以生成整个图像。</p></li><li><p>(8): 对于逆问题的求解，使用 Patch Diffusion Inverse Solver (PaDIS) 方法，该方法可以将整个图像的score函数分解为多个patch的score函数，然后使用 Langevin dynamics 或 DDPM 等方法来求解逆问题。</p></li></ul><ol><li>Conclusion:</li></ol><ul><li><p>(1): 本文工作的意义在于提出了一种基于patch的扩散学习图像prior方法，解决了传统扩散模型高维和高分辨率数据上的计算效率和数据效率问题，从而扩展了扩散模型在图像处理领域的应用范围。</p></li><li><p>(2): Innovation point: 本文的创新点在于将patch-based扩散模型应用于图像prior学习，充分考虑了图像的patch结构和位置信息； Performance: 本文的实验结果表明，PaDIS模型可以在保持成整个图像能力的同时，提高内存效率和数据效率； Workload: 本文的工作负载主要集中在扩散模型的设计和实现上，需要大量的计算资源和数据支持。</p></li></ul><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/9c6dfd9d2a3b403856c0c71ca91e3c59241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/7827914460cd503e82f43b5c4841752f241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/780e08470b895237445fa4a5328c634d241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/252612531903a78b63f685326a39cf81241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/22fc4b3649f72d4778e80367f62cf3c0241286257.jpg" align="middle"></details><h2 id="RoomTex-Texturing-Compositional-Indoor-Scenes-via-Iterative-Inpainting"><a href="#RoomTex-Texturing-Compositional-Indoor-Scenes-via-Iterative-Inpainting" class="headerlink" title="RoomTex: Texturing Compositional Indoor Scenes via Iterative Inpainting"></a>RoomTex: Texturing Compositional Indoor Scenes via Iterative Inpainting</h2><p><strong>Authors:Qi Wang, Ruijie Lu, Xudong Xu, Jingbo Wang, Michael Yu Wang, Bo Dai, Gang Zeng, Dan Xu</strong></p><p>The advancement of diffusion models has pushed the boundary of text-to-3D object generation. While it is straightforward to composite objects into a scene with reasonable geometry, it is nontrivial to texture such a scene perfectly due to style inconsistency and occlusions between objects. To tackle these problems, we propose a coarse-to-fine 3D scene texturing framework, referred to as RoomTex, to generate high-fidelity and style-consistent textures for untextured compositional scene meshes. In the coarse stage, RoomTex first unwraps the scene mesh to a panoramic depth map and leverages ControlNet to generate a room panorama, which is regarded as the coarse reference to ensure the global texture consistency. In the fine stage, based on the panoramic image and perspective depth maps, RoomTex will refine and texture every single object in the room iteratively along a series of selected camera views, until this object is completely painted. Moreover, we propose to maintain superior alignment between RGB and depth spaces via subtle edge detection methods. Extensive experiments show our method is capable of generating high-quality and diverse room textures, and more importantly, supporting interactive fine-grained texture control and flexible scene editing thanks to our inpainting-based framework and compositional mesh input. Our project page is available at <a target="_blank" rel="noopener" href="https://qwang666.github.io/RoomTex/">https://qwang666.github.io/RoomTex/</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.02461v1">PDF</a></p><p><strong>Summary</strong><br>RoomTex：一种 coarse-to-fine 的 3D 场景纹理生成框架，能够生成高保真和风格一致的纹理。</p><p><strong>Key Takeaways</strong><br>• RoomTex 框架通过 coarse-to-fine 两阶段生成高质量的 3D 场景纹理。<br>• coarse 阶段生成全局一致的 room panorama，作为后续纹理生成的参考。<br>• fine 阶段迭代地对每个对象进行纹理生成和细化，直到完全绘制。<br>• RoomTex 通过精细的边缘检测方法维护 RGB 和深度空间之间的对齐。<br>• 该方法支持交互式的细粒度纹理控制和灵活的场景编辑。<br>• RoomTex 能够生成高质量和多样化的房间纹理。<br>• 项目页面：<a target="_blank" rel="noopener" href="https://qwang666.github.io/RoomTex/。">https://qwang666.github.io/RoomTex/。</a></p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>Title: RoomTex：基于迭代 inpainting 的组合室内场景纹理生成 (RoomTex: Texturing Compositional Indoor Scenes via Iterative Inpainting)</li></ol><ol><li>Authors: Qi Wang, Ruijie Lu, Xudong Xu, Jingbo Wang, Michael Yu Wang, Bo Dai, Gang Zeng, Dan Xu</li></ol><ol><li>Affiliation: 香港科技大学</li></ol><ol><li>Keywords: Scene Texturing · Scene Generation · Texture Synthesis</li></ol><ol><li>Urls: <a target="_blank" rel="noopener" href="https://qwang666.github.io/RoomTex/">https://qwang666.github.io/RoomTex/</a> , Github:None</li></ol><ol><li>Summary:</li></ol><pre><code>- (1):研究背景是生成高质量纹理的3D室内场景模型，对于游戏、电影和AR/VR等工业应用非常重要。


- (2):过去的方法可以生成3D对象，但是这些对象的纹理风格不一致，且对象之间存在遮挡问题。这些方法不能生成高质量的纹理。


- (3):本文提出了一个 coarse-to-fine 的3D场景纹理生成框架RoomTex，首先将场景mesh unwrap到全景深度图，然后使用ControlNet生成房间全景图，最后基于全景图和视角深度图，对每个对象进行迭代 inpainting，直到对象完全被绘制。


- (4):本文的方法可以生成高质量和多样化的房间纹理，并且支持交互式的细粒度纹理控制和灵活的场景编辑。
</code></pre><p>sorry, 您的ip已由于触发防滥用检测而被封禁,可能是因为使用云服务器如腾讯云或者国外代理(vpn)访问本网站，如果使用了vpn，关闭vpn或代理即可继续使用,本服务网址是<a target="_blank" rel="noopener" href="https://chat18.aichatos8.com">https://chat18.aichatos8.com</a> 如需合作接口调用请联系微信chatkf123 或者前往 <a target="_blank" rel="noopener" href="https://binjie09.shop">https://binjie09.shop</a> 自助购买key, 认为是误封需要解封的请前往<a target="_blank" rel="noopener" href="https://www.ip.cn/">https://www.ip.cn/</a> 查询ip信息,并发送信息至邮件 gpt33@binjie.site ，站长会定期看邮件并处理解封和合作问题，如需调用接口请见接口文档<a target="_blank" rel="noopener" href="https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123">https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123</a> 或者前往 <a target="_blank" rel="noopener" href="https://cat.gptxyy.cn">https://cat.gptxyy.cn</a> 注册使用（可付费使用gpt4 注册可免费使用3.5）</p><p>sorry, 您的ip已由于触发防滥用检测而被封禁,可能是因为使用云服务器如腾讯云或者国外代理(vpn)访问本网站，如果使用了vpn，关闭vpn或代理即可继续使用,本服务网址是<a target="_blank" rel="noopener" href="https://chat18.aichatos8.com">https://chat18.aichatos8.com</a> 如需合作接口调用请联系微信chatkf123 或者前往 <a target="_blank" rel="noopener" href="https://binjie09.shop">https://binjie09.shop</a> 自助购买key, 认为是误封需要解封的请前往<a target="_blank" rel="noopener" href="https://www.ip.cn/">https://www.ip.cn/</a> 查询ip信息,并发送信息至邮件 gpt33@binjie.site ，站长会定期看邮件并处理解封和合作问题，如需调用接口请见接口文档<a target="_blank" rel="noopener" href="https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123">https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123</a> 或者前往 <a target="_blank" rel="noopener" href="https://cat.gptxyy.cn">https://cat.gptxyy.cn</a> 注册使用（可付费使用gpt4 注册可免费使用3.5）</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/e596a578ebc0d3ac4051ac1db16c06e5241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/421ef6ff4d11d1376ed152b170e96685241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/8355176fb1f4e6929e3df8cc7fd0027f241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/8298da4924c73a41dbe8947ac953ec05241286257.jpg" align="middle"></details><h2 id="Flash-Diffusion-Accelerating-Any-Conditional-Diffusion-Model-for-Few-Steps-Image-Generation"><a href="#Flash-Diffusion-Accelerating-Any-Conditional-Diffusion-Model-for-Few-Steps-Image-Generation" class="headerlink" title="Flash Diffusion: Accelerating Any Conditional Diffusion Model for Few   Steps Image Generation"></a>Flash Diffusion: Accelerating Any Conditional Diffusion Model for Few Steps Image Generation</h2><p><strong>Authors:Clement Chadebec, Onur Tasar, Eyal Benaroche, Benjamin Aubin</strong></p><p>In this paper, we propose an efficient, fast, and versatile distillation method to accelerate the generation of pre-trained diffusion models: Flash Diffusion. The method reaches state-of-the-art performances in terms of FID and CLIP-Score for few steps image generation on the COCO2014 and COCO2017 datasets, while requiring only several GPU hours of training and fewer trainable parameters than existing methods. In addition to its efficiency, the versatility of the method is also exposed across several tasks such as text-to-image, inpainting, face-swapping, super-resolution and using different backbones such as UNet-based denoisers (SD1.5, SDXL) or DiT (Pixart-$\alpha$), as well as adapters. In all cases, the method allowed to reduce drastically the number of sampling steps while maintaining very high-quality image generation. The official implementation is available at <a target="_blank" rel="noopener" href="https://github.com/gojasper/flash-diffusion">https://github.com/gojasper/flash-diffusion</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.02347v1">PDF</a> 16 pages + 16 pages appendices</p><p>sorry, 您的ip已由于触发防滥用检测而被封禁,可能是因为使用云服务器如腾讯云或者国外代理(vpn)访问本网站，如果使用了vpn，关闭vpn或代理即可继续使用,本服务网址是<a target="_blank" rel="noopener" href="https://chat18.aichatos8.com">https://chat18.aichatos8.com</a> 如需合作接口调用请联系微信chatkf123 或者前往 <a target="_blank" rel="noopener" href="https://binjie09.shop">https://binjie09.shop</a> 自助购买key, 认为是误封需要解封的请前往<a target="_blank" rel="noopener" href="https://www.ip.cn/">https://www.ip.cn/</a> 查询ip信息,并发送信息至邮件 gpt33@binjie.site ，站长会定期看邮件并处理解封和合作问题，如需调用接口请见接口文档<a target="_blank" rel="noopener" href="https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123">https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123</a> 或者前往 <a target="_blank" rel="noopener" href="https://cat.gptxyy.cn">https://cat.gptxyy.cn</a> 注册使用（可付费使用gpt4 注册可免费使用3.5）</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: Flash Diffusion: Accelerating Any Conditional Diffusion Model for Few Steps Image Generation (闪光扩散：加速任何条件扩散模型的少步图像生成)</p></li><li><p>Authors: Clement Chadebec, Onur Tasar, Eyal Benaroche, Benjamin Aubin</p></li><li><p>Affiliation:.jasper Research (.jasper研究)</p></li><li><p>Keywords: diffusion model, few steps image generation, Flash Diffusion</p></li><li><p>Urls: arXiv:2406.02347v1, Github: <a target="_blank" rel="noopener" href="https://github.com/gojasper/flash-diffusion">https://github.com/gojasper/flash-diffusion</a></p></li><li><p>Summary:</p></li></ol><ul><li><p>(1):这篇论文的研究背景是 diffusion 模型在图像生成任务中的应用，特别是 few steps 图像生成任务， existing 方法需要大量的采样步骤和计算资源。</p></li><li><p>(2):过去的方法需要大量的采样步骤和计算资源，限制了它们在实际应用中的效率和可扩展性。 Flash Diffusion 方法的提出是为了解决这个问题，提高 diffusion 模型的生成效率和质量。</p></li><li><p>(3):这篇论文提出了一种名为 Flash Diffusion 的 distillation 方法，旨在加速预训练的 diffusion 模型的生成速度。该方法可以在少步图像生成任务中达到 state-of-the-art 的性能，同时需要的计算资源和采样步骤都较少。</p></li><li><p>(4):该方法在 COCO2014 和 COCO2017 数据集上的 few steps 图像生成任务中达到了 state-of-the-art 的性能，FID 和 CLIP-Score 都取得了很高的分数，同时只需要几小时的 GPU 训练时间和较少的可训练参数。<br>sorry, 您的ip已由于触发防滥用检测而被封禁,可能是因为使用云服务器如腾讯云或者国外代理(vpn)访问本网站，如果使用了vpn，关闭vpn或代理即可继续使用,本服务网址是<a target="_blank" rel="noopener" href="https://chat18.aichatos8.com">https://chat18.aichatos8.com</a> 如需合作接口调用请联系微信chatkf123 或者前往 <a target="_blank" rel="noopener" href="https://binjie09.shop">https://binjie09.shop</a> 自助购买key, 认为是误封需要解封的请前往<a target="_blank" rel="noopener" href="https://www.ip.cn/">https://www.ip.cn/</a> 查询ip信息,并发送信息至邮件 gpt33@binjie.site ，站长会定期看邮件并处理解封和合作问题，如需调用接口请见接口文档<a target="_blank" rel="noopener" href="https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123">https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123</a> 或者前往 <a target="_blank" rel="noopener" href="https://cat.gptxyy.cn">https://cat.gptxyy.cn</a> 注册使用（可付费使用gpt4 注册可免费使用3.5）</p></li></ul><ol><li>Conclusion:</li></ol><ul><li><p>(1):本文的研究工作对于diffusion模型在图像生成任务中的应用具有重要意义，能够极大地提高图像生成的效率和质量。</p></li><li><p>(2):创新点：提出了Flash Diffusion方法，能够加速预训练的diffusion模型的生成速度；性能：在COCO2014和COCO2017数据集上的few steps图像生成任务中达到了state-of-the-art的性能；工作负载：需要的计算资源和采样步骤都较少，可以在短时间内完成训练。</p></li></ul><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/e7563b1fa7c8536ad51a2c69f80de705241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/018b45169d7cd7b816f3b4572b5d2d6d241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/968d37ccfe6674d2a2d12755024b75db241286257.jpg" align="middle"></details><h2 id="A-Survey-of-Transformer-Enabled-Time-Series-Synthesis"><a href="#A-Survey-of-Transformer-Enabled-Time-Series-Synthesis" class="headerlink" title="A Survey of Transformer Enabled Time Series Synthesis"></a>A Survey of Transformer Enabled Time Series Synthesis</h2><p><strong>Authors:Alexander Sommers, Logan Cummins, Sudip Mittal, Shahram Rahimi, Maria Seale, Joseph Jaboure, Thomas Arnold</strong></p><p>Generative AI has received much attention in the image and language domains, with the transformer neural network continuing to dominate the state of the art. Application of these models to time series generation is less explored, however, and is of great utility to machine learning, privacy preservation, and explainability research. The present survey identifies this gap at the intersection of the transformer, generative AI, and time series data, and reviews works in this sparsely populated subdomain. The reviewed works show great variety in approach, and have not yet converged on a conclusive answer to the problems the domain poses. GANs, diffusion models, state space models, and autoencoders were all encountered alongside or surrounding the transformers which originally motivated the survey. While too open a domain to offer conclusive insights, the works surveyed are quite suggestive, and several recommendations for best practice, and suggestions of valuable future work, are provided.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.02322v1">PDF</a></p><p>sorry, 您的ip已由于触发防滥用检测而被封禁,可能是因为使用云服务器如腾讯云或者国外代理(vpn)访问本网站，如果使用了vpn，关闭vpn或代理即可继续使用,本服务网址是<a target="_blank" rel="noopener" href="https://chat18.aichatos8.com">https://chat18.aichatos8.com</a> 如需合作接口调用请联系微信chatkf123 或者前往 <a target="_blank" rel="noopener" href="https://binjie09.shop">https://binjie09.shop</a> 自助购买key, 认为是误封需要解封的请前往<a target="_blank" rel="noopener" href="https://www.ip.cn/">https://www.ip.cn/</a> 查询ip信息,并发送信息至邮件 gpt33@binjie.site ，站长会定期看邮件并处理解封和合作问题，如需调用接口请见接口文档<a target="_blank" rel="noopener" href="https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123">https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123</a> 或者前往 <a target="_blank" rel="noopener" href="https://cat.gptxyy.cn">https://cat.gptxyy.cn</a> 注册使用（可付费使用gpt4 注册可免费使用3.5）</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>Title: 时间序列合成Transformer综述 (A Survey of Transformer Enabled Time Series)</li></ol><ol><li>Authors: Alexander Sommers, Logan Cummins, Sudip Mittal, Shahram Rahimi, Maria Seale, Joseph Jaboure, Thomas Arnold</li></ol><ol><li>Affiliation: 密西西比州立大学计算机科学与工程学院</li></ol><ol><li>Keywords: 时间序列合成, Transformer, 数据增强,综述</li></ol><ol><li>Urls: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02322v1">https://arxiv.org/abs/2406.02322v1</a> , Github:None</li></ol><ol><li>Summary:</li></ol><pre><code>- (1):该论文的研究背景是生成式AI在图像和语言领域取得了很大进展，而在时间序列生成领域的应用较少，Transformer经网络在时间序列生成领域的应用尤其不足。

- (2):过去的方法包括GAN、扩散模型、状态空间模型和自动编码器等，但是这些方法还没有在时间序列生成领域取得明确的结论。

- (3):本论文对Transformer神经网络在时间序列生成领域的应用进行了综述，并总结了十二篇相关论文。

- (4):该论文的方法可以用于时间序列生成、数据增强、隐私保护和模解释等领域，但由于时间序列生成领域还没有明确的结论，该论文的性能还需要进一步验证。
</code></pre><ol><li>方法：</li></ol><ul><li><p>(1)：该论文对Transformer神经网络在时间序列生成领域的应用进行了综述，总结了十二篇相关论文。</p></li><li><p>(2)：论文首先对间序列生成领域的背景和挑战进行了介绍，然后对Transformer神经网络的基本结构和特点进行了描述。</p></li><li><p>(3)：然后，论文对十二篇相关论文进行了分和总结，包括基于生成对抗网络（GAN）的方法、基于扩散模型的方法、基于状态空间模型的方法和基于自动编码器的方法等。</p></li><li><p>(4)：论文对每种方法的优缺点和应用场景进行了分析，并对其在时间序列生成领域的应用前景进行了讨论。</p></li><li><p>(5)：最后，论文对Transformer神经网络在时间序列生成领域的应用前景和挑战进行了总结，并提出了未来研究方向。</p></li><li><p>(6)：论文还对时间序列生成领域的评估指标和评估方法进行了讨论，并对其在实际应用中的重要性进行了调。</p></li><li><p>(7)：论文的方法可以用于时间序列生成、数据增强、隐私保护和模解释等领域，但由于时间序列生成领域还没有明确的结论，该论文的性能还需要进一步验证。</p></li><li><p>(8)：论文的方法可以与其他机器学习方法结合使用，以提高时间序列生成的准确性和效率。</p></li><li><p>(9)：论文的结果可以用于指导实际应用中的时间序列生成任务，并对相关领域的研究和应用产生影响。</p></li></ul><ol><li>Conclusion:</li></ol><ul><li>(1):该论文对Transformer神经网络在时间序列生成领域的应用进行了综述，总结了十二篇相关论文，为时间序列生成领域的研究提供了有价值的参考和指导意义。</li></ul><ul><li>(2):Innovation point: 本论文对Transformer神经网络在时间序列生成领域的应用进行了系统的综述，总结了十二篇相关论文，对时间序列生成领域的研究提供了新的视角和思路；Performance: 本论文对时间序列生成领域的评估指标和评估方法进行了讨论，为时间序列生成领域的研究提供了有价值的参考和指导意义；Workload: 本论文对Transformer神经网络在时间序列生成领域的应用前景和挑战进行了总结，为时间序列生成领域的研究提供了有价值的参考和指导意义。</li></ul><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/a0c55b0527377194a053702dd3225eca241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/60bf5af7b9eccbcc3f9a4ba81f4f908d241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/cdd32cd34afcef5f08ff31a45b9ee889241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/ab6bd348754ac1cc2583d470123aee82241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/329c6d0622234384361f47461191f595241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/595995bfacb8d86452d7e0e6768bb75e241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/c7d01e5476ca9cab42d43ff36c8f5608241286257.jpg" align="middle"></details><h2 id="GraVITON-Graph-based-garment-warping-with-attention-guided-inversion-for-Virtual-tryon"><a href="#GraVITON-Graph-based-garment-warping-with-attention-guided-inversion-for-Virtual-tryon" class="headerlink" title="GraVITON: Graph based garment warping with attention guided inversion   for Virtual-tryon"></a>GraVITON: Graph based garment warping with attention guided inversion for Virtual-tryon</h2><p><strong>Authors:Sanhita Pathak, Vinay Kaushik, Brejesh Lall</strong></p><p>Virtual try-on, a rapidly evolving field in computer vision, is transforming e-commerce by improving customer experiences through precise garment warping and seamless integration onto the human body. While existing methods such as TPS and flow address the garment warping but overlook the finer contextual details. In this paper, we introduce a novel graph based warping technique which emphasizes the value of context in garment flow. Our graph based warping module generates warped garment as well as a coarse person image, which is utilised by a simple refinement network to give a coarse virtual tryon image. The proposed work exploits latent diffusion model to generate the final tryon, treating garment transfer as an inpainting task. The diffusion model is conditioned with decoupled cross attention based inversion of visual and textual information. We introduce an occlusion aware warping constraint that generates dense warped garment, without any holes and occlusion. Our method, validated on VITON-HD and Dresscode datasets, showcases substantial state-of-the-art qualitative and quantitative results showing considerable improvement in garment warping, texture preservation, and overall realism.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.02184v1">PDF</a> 18 pages, 7 Figures and 6 Tables</p><p><strong>Summary</strong><br>虚拟试衣技术通过图形变形和人体融合，提高电子商务客户体验，本文提出了一种基于图形的变形技术，强调上下文细节的重要性。</p><p><strong>Key Takeaways</strong><br>• 虚拟试衣技术可以改善电子商务客户体验。<br>• 现有的变形方法（如TPS和流）忽视了上下文细节。<br>• 本文提出的图形变形技术可以生成高质量的变形服装和粗糙的人体图像。<br>• 提出的方法使用潜在扩散模型生成最终的试衣图像，将服装传输视为inpainting任务。<br>• 该方法使用去耦合的交叉注意力反转视觉和文本信息。<br>• 本文提出了一种遮挡感知变形约束，生成密集的变形服装，without holes and occlusion。<br>• 该方法在VITON-HD和Dresscode数据集上的实验结果显示了显著的state-of-the-art qualitative and quantitative结果。</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: GraVITON：基于图的服装变形虚拟试衣（Graph-based Garment Warping for Virtual Try-on）</p></li><li><p>Authors: Sanhita Pathak, Vinay Kaushik, Brejesh Lall</p></li><li><p>Affiliation: 未提供（No affiliation provided）</p></li><li><p>Keywords: Virtual try-on · Optical Flow · Graph · Latent Diffusion models</p></li><li><p>Urls: arXiv:2406.02184v1 , Github:None</p></li><li><p>Summary:</p></li></ol><ul><li><p>(1):本文研究的背景是虚拟试衣技术的发展，它能够为电子商务提供更好的客户体验。</p></li><li><p>(2):过去的方法包括TPS warping和flow-based approaches，但这些方法忽视了服装流的细节信息，无法生成真实的试衣结果。</p></li><li><p>(3):本文提出了一种基于图的服装变形方法，使用图基于的变形模块生成变形后的服装和糙的人体图像，然后使用简单的 tinhancement网络生成粗糙的虚拟试衣图像。最后，使用潜伏扩散模型生成最终的试衣结果。</p></li><li><p>(4):本文的方法在VITON-HD和Dresscode数据集上进行了验证，取得了state-of-the-art的定性和定量结果，证明了该方法在服装变形、纹理保留和总体真实性方面的改进。</p></li></ul><ol><li>方法：</li></ol><ul><li><p>(1)：提出了一种基于图的服装变形方法，使用图基于的变形模块生成变形后的服装和糙的人体图像。</p></li><li><p>(2)：该方法包括两个阶段：第一阶段是变形阶段，使用图流网络（GraphNet）和精炼网络（RefineNet）生成变形后的服装和糙的人体图像；第二阶段是生成阶段，使用潜伏扩散模型生成最终的试衣结果。</p></li><li><p>(3)：在变形阶段，使用图流网络（GraphNet）计算密集流，以生成变形后的服装和糙的人体图像。</p></li><li><p>(4)：然后，使用精炼网络（RefineNet）计算最终的变形指令，以生成变形后的服装和糙的人体图像。</p></li><li><p>(5)：在生成阶段，使用潜伏扩散模型生成最终的试衣结果，条件于ertextual embedding和服装试衣图像的纹理嵌入。</p></li><li><p>(6)：为了提高生成结果的真实性，引入了一个去耦合注意力适配器（Decoupled Cross Attention Adaptor，DCAA），以条件潜伏扩散模型生成更为逼真的试衣结果。</p></li><li><p>(7)：最后，使用 Occlusion Aware warp Loss（OWL）损失函数来学习变形后的服装，以避免自遮挡问题。</p></li></ul><ol><li>Conclusion:</li></ol><ul><li><p>(1):本文的研究工作对虚拟试衣技术的发展具有重要意义，可以为电子商务提供更好的客户体验。</p></li><li><p>(2):创新点：提出了一种基于图的服装变形方法，使用图流网络（GraphNet）和潜伏扩散模型生成高质量的虚拟试衣结果；性能：在VITON-HD和Dresscode数据集上取得了state-of-the-art的定性和定量结果；工作量：需要大量的数据和计算资源来训练模型，且算法复杂度较高。</p></li></ul><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/d8b7026ce91b28c27a140c63af83d58e241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/b2845c22971c07776331dbad3278d3fe241286257.jpg" align="middle"></details><h2 id="Follow-Your-Emoji-Fine-Controllable-and-Expressive-Freestyle-Portrait-Animation"><a href="#Follow-Your-Emoji-Fine-Controllable-and-Expressive-Freestyle-Portrait-Animation" class="headerlink" title="Follow-Your-Emoji: Fine-Controllable and Expressive Freestyle Portrait   Animation"></a>Follow-Your-Emoji: Fine-Controllable and Expressive Freestyle Portrait Animation</h2><p><strong>Authors:Yue Ma, Hongyu Liu, Hongfa Wang, Heng Pan, Yingqing He, Junkun Yuan, Ailing Zeng, Chengfei Cai, Heung-Yeung Shum, Wei Liu, Qifeng Chen</strong></p><p>We present Follow-Your-Emoji, a diffusion-based framework for portrait animation, which animates a reference portrait with target landmark sequences. The main challenge of portrait animation is to preserve the identity of the reference portrait and transfer the target expression to this portrait while maintaining temporal consistency and fidelity. To address these challenges, Follow-Your-Emoji equipped the powerful Stable Diffusion model with two well-designed technologies. Specifically, we first adopt a new explicit motion signal, namely expression-aware landmark, to guide the animation process. We discover this landmark can not only ensure the accurate motion alignment between the reference portrait and target motion during inference but also increase the ability to portray exaggerated expressions (i.e., large pupil movements) and avoid identity leakage. Then, we propose a facial fine-grained loss to improve the model’s ability of subtle expression perception and reference portrait appearance reconstruction by using both expression and facial masks. Accordingly, our method demonstrates significant performance in controlling the expression of freestyle portraits, including real humans, cartoons, sculptures, and even animals. By leveraging a simple and effective progressive generation strategy, we extend our model to stable long-term animation, thus increasing its potential application value. To address the lack of a benchmark for this field, we introduce EmojiBench, a comprehensive benchmark comprising diverse portrait images, driving videos, and landmarks. We show extensive evaluations on EmojiBench to verify the superiority of Follow-Your-Emoji.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.01900v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://follow-your-emoji.github.io/">https://follow-your-emoji.github.io/</a></p><p><strong>Summary</strong><br>Follow-Your-Emoji：基于扩散模型的肖像动画框架，通过标志序列动画参考肖像。</p><p><strong>Key Takeaways</strong><br>• Follow-Your-Emoji 框架使用 Stable Diffusion 模型和两种技术来解决肖像动画挑战。<br>• 表情感知标志信号指导动画过程，确保动作对齐和身份保持。<br>• 脸部细粒度损失函数提高模型对细微表情和参考肖像外观的重建能力。<br>• 该方法在控制自由肖像表情方面显示出显著性能，包括真人、卡通、雕塑和动物。<br>• 进步生成策略使模型能够生成长期稳定的动画，增加其应用价值。<br>• EmojiBench 基准测试集用于评估 Follow-Your-Emoji 的优越性。<br>• 该方法在 EmojiBench 上进行了广泛的评估，验证了其优越性。</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>Title: 细致可控的自由式肖像动画（Fine-Controllable and Expressive Freestyle Portrait Animation）</li></ol><ol><li>Authors: Yue Ma, Hongyu Liu, Hongfa Wang, Heng Pan, Yingqing He, Junkun Yuan, Ailing Zeng, Chengfei Cai, Heung-Yeung Shum, Wei Liu, Qifeng Chen</li></ol><ol><li>Affiliation: 香港科技大学</li></ol><ol><li>Keywords: Portrait Animation, Diffusion Model, Expression-Aware Landmark, Facial Fine-Grained Loss</li></ol><ol><li>Urls: <a target="_blank" rel="noopener" href="https://follow-your-emoji.github.io/">https://follow-your-emoji.github.io/</a> , Github:None</li></ol><ol><li>Summary:</li></ol><pre><code>- (1):本文研究的是肖像动画任务，即将目标序列的姿势和表情从驱动视频转移到参考肖像中。


- (2):过去的方法基于生成对抗网络（GAN）和扩散模型，但它们存在一些问题，如身份泄露、不真实的内容和显著的artifact。


- (3):本文提出了Follow-Your-Emoji框架，该框架采用扩散模型并引入了两种技术：表情感知标志和面部细粒度损失函数。


- (4):本文的方法在自由式肖像动画任务上取得了显著的性能，能够控制肖像的表情，包括人、卡通、雕塑和动物等。同时，本文还提出了一个名为EmojiBench的基准测试集，以评估肖像动画方法的性能。
</code></pre><p>sorry, 您的ip已由于触发防滥用检测而被封禁,可能是因为使用云服务器如腾讯云或者国外代理(vpn)访问本网站，如果使用了vpn，关闭vpn或代理即可继续使用,本服务网址是<a target="_blank" rel="noopener" href="https://chat18.aichatos8.com">https://chat18.aichatos8.com</a> 如需合作接口调用请联系微信chatkf123 或者前往 <a target="_blank" rel="noopener" href="https://binjie09.shop">https://binjie09.shop</a> 自助购买key, 认为是误封需要解封的请前往<a target="_blank" rel="noopener" href="https://www.ip.cn/">https://www.ip.cn/</a> 查询ip信息,并发送信息至邮件 gpt33@binjie.site ，站长会定期看邮件并处理解封和合作问题，如需调用接口请见接口文档<a target="_blank" rel="noopener" href="https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123">https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123</a> 或者前往 <a target="_blank" rel="noopener" href="https://cat.gptxyy.cn">https://cat.gptxyy.cn</a> 注册使用（可付费使用gpt4 注册可免费使用3.5）</p><ol><li>Conclusion:</li></ol><ul><li>(1):本文的工作对于肖像动画任务具有重要意义，因为它解决了过去方法中存在的身份泄露、不真实的内容和显著的artifact问题，提高了肖像动画的可控性和表达性。</li></ul><ul><li>(2):创新点：本文提出的Follow-Your-Emoji框架采用扩散模型并引入了表情感知标志和面部细粒度损失函数，解决了肖像动画任务中的多个挑战；性能：本文的方法在自由式肖像动画任务上取得了显著的性能，能够控制肖像的表情，包括人、卡通、雕塑和动物等；工作量：本文的方法需要大量的训练数据和计算资源，但其提出的 EmojiBench基准测试集可以为未来肖像动画研究提供重要的参考价值。</li></ul><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/40bb4d009765bfeafad0de4e983ca97b241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/2a6f923f4899d3ecd2aca72132a3edf1241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/aa82ff249b702946ffff2f4be643318b241286257.jpg" align="middle"></details><h2 id="Cross-Domain-Graph-Data-Scaling-A-Showcase-with-Diffusion-Models"><a href="#Cross-Domain-Graph-Data-Scaling-A-Showcase-with-Diffusion-Models" class="headerlink" title="Cross-Domain Graph Data Scaling: A Showcase with Diffusion Models"></a>Cross-Domain Graph Data Scaling: A Showcase with Diffusion Models</h2><p><strong>Authors:Wenzhuo Tang, Haitao Mao, Danial Dervovic, Ivan Brugere, Saumitra Mishra, Yuying Xie, Jiliang Tang</strong></p><p>Models for natural language and images benefit from data scaling behavior: the more data fed into the model, the better they perform. This ‘better with more’ phenomenon enables the effectiveness of large-scale pre-training on vast amounts of data. However, current graph pre-training methods struggle to scale up data due to heterogeneity across graphs. To achieve effective data scaling, we aim to develop a general model that is able to capture diverse data patterns of graphs and can be utilized to adaptively help the downstream tasks. To this end, we propose UniAug, a universal graph structure augmentor built on a diffusion model. We first pre-train a discrete diffusion model on thousands of graphs across domains to learn the graph structural patterns. In the downstream phase, we provide adaptive enhancement by conducting graph structure augmentation with the help of the pre-trained diffusion model via guided generation. By leveraging the pre-trained diffusion model for structure augmentation, we consistently achieve performance improvements across various downstream tasks in a plug-and-play manner. To the best of our knowledge, this study represents the first demonstration of a data-scaling graph structure augmentor on graphs across domains.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.01899v1">PDF</a></p><p><strong>Summary</strong><br>提出了一种通用的图结构增强器UniAug，基于扩散模型，能够捕捉多样化的图数据模式，提高下游任务的性能。</p><p><strong>Key Takeaways</strong><br>• 图模型的性能可以通过数据规模的增加来提高，但当前的图预训练方法难以扩展数据规模。<br>• 提出了一种基于扩散模型的用图结构增强器UniAug，可以捕捉多样化的图数据模式。<br>• UniAug 通过预训练扩散模型来学习图结构模式，然后在下游阶段进行图结构增强。<br>• 使用预训练的扩散模型进行结构增强可以在多种下游任务中带来性能改进。<br>• 本研究是一个在跨域图上展示数据扩展图结构增强器的示例。<br>• UniAug 可以在插件式方式下提高下游任务的性能。<br>• 本方法可以帮助下游任务自适应地选择合适的图结构增强策略。</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>Title: 跨域图数据扩展：基于扩散模型的展示 (Cross-Domain Graph Data Scaling: A Showcase with Diffusion Models)</li></ol><ol><li>Authors: Wenzhuo Tang, Haitao Mao, Danial Dervovic, Ivan Brugere, Saumitra Mishra, Yuying Xie, Jiliang Tang</li></ol><ol><li>Affiliation: 密歇根州立大学</li></ol><ol><li>Keywords: graph pre-training, data scaling, diffusion models, graph structure augmentation</li></ol><ol><li>Urls: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01899">https://arxiv.org/abs/2406.01899</a>, Github: <a target="_blank" rel="noopener" href="https://github.com/WenzhuoTang/UniAug">https://github.com/WenzhuoTang/UniAug</a></li></ol><ol><li>Summary:</li></ol><pre><code>- (1):研究发现自然语言和图像模型的性能随着数据规模的增加而提高，但当前的图预训练方法难以扩展数据规模，因为图之间存在异质性。


- (2):过去的图预训练方法无法很好地处理图之间的异质性，无法实现数据扩展；本文提出的方法 UniAug 通过扩散模型来学习图结构模式，解决了这个问题。


- (3):本文提出了一种基于扩散模型的通用图结构增强器 UniAug，首先在数千个跨域图上预训练离散扩散模型，然后在下游任务中使用预训练的扩散模型进行图结构增强。


- (4):实验结果表明，UniAug 方法在多个下游任务中取得了性能改进，证明了该方法的有效性。
</code></pre><ol><li>方法：</li></ol><ul><li><p>(1)：收集多个领域的图数据，包括生物网络、化学网络、社交网络等，共计数千个图，用于训练扩散模型。</p></li><li><p>(2)：对收集的图数据进行预处理，包括计算图的结构特征，如节点数、密度、网络熵、平均度、度方差、无标度指数等，用于构建图级表示。</p></li><li><p>(3)：采用自监督标注策略，通过聚类算法对图级表示进行聚类，获取自监督标签，用于训练扩散模型。</p></li><li><p>(4)：构建离散扩散模型，用于学习图结构模式，模型包括denoising 网络和图变换器（GT），denoising 网络用于恢复图的邻接矩阵，图变换器用于学习图的结构表示。</p></li><li><p>(5)：训练扩散模型，使用variational lower bound优化目标，用于学习图结构模式。</p></li><li><p>(6)：在下游任务中，使用预训练的扩散模型进行图结构增强，通过guided generation生成合成图结构，并与原始节点特征组合，用于训练下游任务的GNN模型。</p></li><li><p>(7)：选择合适的指导目标，包括节点级、边级和图级的指导目标，用于bridging the gap between pre-training distribution and downstream dataset。</p></li><li><p>(8)：使用梯度基于方法，例如NOS方法，进行guided generation，生成合成图结构。</p></li><li><p>(9)：使用augmented图数据训练下游任务的GNN模型，用于提高下游任务的性能。</p></li></ul><p>sorry, 您的ip已由于触发防滥用检测而被封禁,可能是因为使用云服务器如腾讯云或者国外代理(vpn)访问本网站，如果使用了vpn，关闭vpn或代理即可继续使用,本服务网址是<a target="_blank" rel="noopener" href="https://chat18.aichatos8.com">https://chat18.aichatos8.com</a> 如需合作接口调用请联系微信chatkf123 或者前往 <a target="_blank" rel="noopener" href="https://binjie09.shop">https://binjie09.shop</a> 自助购买key, 认为是误封需要解封的请前往<a target="_blank" rel="noopener" href="https://www.ip.cn/">https://www.ip.cn/</a> 查询ip信息,并发送信息至邮件 gpt33@binjie.site ，站长会定期看邮件并处理解封和合作问题，如需调用接口请见接口文档<a target="_blank" rel="noopener" href="https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123">https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123</a> 或者前往 <a target="_blank" rel="noopener" href="https://cat.gptxyy.cn">https://cat.gptxyy.cn</a> 注册使用（可付费使用gpt4 注册可免费使用3.5）</p><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/98b3bdaaf6267f0a53ae7d2491a660c2241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/32c460eff661bc8ebe78818d82409486241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/65febcdec0bc56574cdb8d9263cd3dcf241286257.jpg" align="middle"></details><h2 id="L-MAGIC-Language-Model-Assisted-Generation-of-Images-with-Coherence"><a href="#L-MAGIC-Language-Model-Assisted-Generation-of-Images-with-Coherence" class="headerlink" title="L-MAGIC: Language Model Assisted Generation of Images with Coherence"></a>L-MAGIC: Language Model Assisted Generation of Images with Coherence</h2><p><strong>Authors:Zhipeng Cai, Matthias Mueller, Reiner Birkl, Diana Wofk, Shao-Yen Tseng, JunDa Cheng, Gabriela Ben-Melech Stan, Vasudev Lal, Michael Paulitsch</strong></p><p>In the current era of generative AI breakthroughs, generating panoramic scenes from a single input image remains a key challenge. Most existing methods use diffusion-based iterative or simultaneous multi-view inpainting. However, the lack of global scene layout priors leads to subpar outputs with duplicated objects (e.g., multiple beds in a bedroom) or requires time-consuming human text inputs for each view. We propose L-MAGIC, a novel method leveraging large language models for guidance while diffusing multiple coherent views of 360 degree panoramic scenes. L-MAGIC harnesses pre-trained diffusion and language models without fine-tuning, ensuring zero-shot performance. The output quality is further enhanced by super-resolution and multi-view fusion techniques. Extensive experiments demonstrate that the resulting panoramic scenes feature better scene layouts and perspective view rendering quality compared to related works, with &gt;70% preference in human evaluations. Combined with conditional diffusion models, L-MAGIC can accept various input modalities, including but not limited to text, depth maps, sketches, and colored scripts. Applying depth estimation further enables 3D point cloud generation and dynamic scene exploration with fluid camera motion. Code is available at <a target="_blank" rel="noopener" href="https://github.com/IntelLabs/MMPano">https://github.com/IntelLabs/MMPano</a>. The video presentation is available at <a target="_blank" rel="noopener" href="https://youtu.be/XDMNEzH4-Ec?list=PLG9Zyvu7iBa0-a7ccNLO8LjcVRAoMn57s">https://youtu.be/XDMNEzH4-Ec?list=PLG9Zyvu7iBa0-a7ccNLO8LjcVRAoMn57s</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.01843v1">PDF</a> accepted to CVPR 2024</p><p><strong>Summary</strong><br>提出L-MAGIC方法，利用大语言模型指导生成高质量360度全景场景图像。</p><p><strong>Key Takeaways</strong><br>• 当前的生成全景场景图像方法存在局限性，例如重复对象或需要人工输入。<br>• L-MAGIC方法利用大语言模型和扩散模型生成高质量全景场景图像，无需微调。<br>• 该方法结合超分辨率和多视图融合技术，提高了输出质量。<br>• 实验结果显示，L-MAGIC方法生成的全景场景图像拥有更好的场景布局和视角渲染质量。<br>• ���方法支持多种输入模式，包括文本、深度图、草图和彩色脚本。<br>• 结合深度估计，L-MAGIC方法可以生成3D点云和实现流畅的摄像机运动。<br>• 代码和视频演示分别发布在GitHub和YouTube上。</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: L-MAGIC：Language Model Assisted Generation of Images with Coherence（L-MAGIC：语言模型辅助的一致图像生成）</p></li><li><p>Authors: Zhipeng Cai, Matthias Mueller, Reiner Birkl, Diana Wofk, Shao-Yen Tseng, Junda Cheng, Gabriela Ben-Melech Stan, Vasudev Lai, Michael Paulitsch</p></li><li><p>Affiliation: 英特尔实验室</p></li><li><p>Keywords: 360° panoramic scenes, language models, diffusion models, image generation</p></li><li><p>Urls: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01843v1">https://arxiv.org/abs/2406.01843v1</a>, Github: <a target="_blank" rel="noopener" href="https://github.com/IntelLabs/MMPano">https://github.com/IntelLabs/MMPano</a></p></li><li><p>Summary:</p></li></ol><ul><li><p>(1):当前生成AI领域的突破，仍然存在从单个输入图像生成全景景的挑战。</p></li><li><p>(2):过去的方法使用基于扩散的迭代或同时多视图 inpainting，但是缺乏全局场景布局先验，导致输出中存在重复对象或需要时间-consuming的人类文本输入。</p></li><li><p>(3):本文提出L-MAGIC方法，利用大型语言模型指导多视图扩散，生成一致的360°全景场景。</p></li><li><p>(4):实验结果表明，L-MAGIC生成的全景场景具有更好的场景布局和视图渲染质量，人类评估中超过70%的偏好率。同时，L-MAGIC还可以与条件扩散模型结合，接受各种输入模式，包括但不限于文本、深度图草图和彩色脚本。</p></li></ul><ol><li>Conclusion:</li></ol><ul><li><p>(1):本文工作的重要性在于提出了一种新颖的全景场景生成方法，能够从单个输入图像生成高质量的一致360°全景场景，具有广泛的应用前景。</p></li><li><p>(2):创新点：L-MAGIC方法引入了大型语言模型指导扩散模型，实现了全局场景布局的生成和局部场景内容的平滑扩展；性能：实验结果表明，L-MAGIC生成全景场景具有更好场景布局和视图渲染质量，人类评估中超过70%的偏好率；工作量：本文提出的方法需要大量的计算资源和数据支持，且需要与其他计算机视觉技术结合使用，增加了实现的难度和工作量。</p></li></ul><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/2cef8fafa1f262effcf9fe39395105f6241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/2b24aac14c8714cae85b320f1bf43dfc241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/8c079cf7eb6001aa8e2c200418660fee241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/988e794d7b687bb6bf87a36175ed49a8241286257.jpg" align="middle"></details><h2 id="DEFT-Efficient-Finetuning-of-Conditional-Diffusion-Models-by-Learning-the-Generalised-h-transform"><a href="#DEFT-Efficient-Finetuning-of-Conditional-Diffusion-Models-by-Learning-the-Generalised-h-transform" class="headerlink" title="DEFT: Efficient Finetuning of Conditional Diffusion Models by Learning   the Generalised $h$-transform"></a>DEFT: Efficient Finetuning of Conditional Diffusion Models by Learning the Generalised $h$-transform</h2><p><strong>Authors:Alexander Denker, Francisco Vargas, Shreyas Padhy, Kieran Didi, Simon Mathis, Vincent Dutordoir, Riccardo Barbano, Emile Mathieu, Urszula Julia Komorowska, Pietro Lio</strong></p><p>Generative modelling paradigms based on denoising diffusion processes have emerged as a leading candidate for conditional sampling in inverse problems. In many real-world applications, we often have access to large, expensively trained unconditional diffusion models, which we aim to exploit for improving conditional sampling. Most recent approaches are motivated heuristically and lack a unifying framework, obscuring connections between them. Further, they often suffer from issues such as being very sensitive to hyperparameters, being expensive to train or needing access to weights hidden behind a closed API. In this work, we unify conditional training and sampling using the mathematically well-understood Doob’s h-transform. This new perspective allows us to unify many existing methods under a common umbrella. Under this framework, we propose DEFT (Doob’s h-transform Efficient FineTuning), a new approach for conditional generation that simply fine-tunes a very small network to quickly learn the conditional $h$-transform, while keeping the larger unconditional network unchanged. DEFT is much faster than existing baselines while achieving state-of-the-art performance across a variety of linear and non-linear benchmarks. On image reconstruction tasks, we achieve speedups of up to 1.6$\times$, while having the best perceptual quality on natural images and reconstruction performance on medical images.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.01781v1">PDF</a> arXiv admin note: text overlap with arXiv:2312.09236</p><p><strong>Summary</strong><br>基于Doob’s h-transform的条件生成方法DEFT，实现了快速高效的条件采样。</p><p><strong>Key Takeaways</strong><br>• 基于denoising diffusion过程的生成模型在逆问题中的条件采样中居领导地位。<br>• 现有方法缺乏统一的框架，且存在超参数敏感、训练成本高和封闭API等问题。<br>• 本工作提出了基于Doob’s h-transform的统一框架，统一了条件训练和采样。<br>• 提出了DEFT方法，快速fine-tune小网络以学习条件h-transform，而保持大型无条件网络不变。<br>• DEFT方法比现有baseline快得多，且在多种线性和非线性基准测试中取得了最好的性能。<br>• 在图像重建任务中，DEFT方法实现了高达1.6倍的速度提升，同时具有自然图像的最佳感知质量和医疗图像的最佳重建性能。</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: DEFT：基于条件扩散模型的高效微调（Efficient Finetuning of Conditional Diffusion Models）</p></li><li><p>Authors: Alexander Denker, Francisco Vargas, Shreyas Padhy, Kieran Didi, Simon Mathis, Vincent Dutordoir, Riccardo Barbano, Emile Mathieu, Urszula Julia Komorowska, Pietro Lio</p></li><li><p>Affiliation: 伦敦大学学院（University College London）</p></li><li><p>Keywords: Conditional Diffusion Models, DEFT, Doob’s h-transform, Generative Modelling</p></li><li><p>Urls: arXiv:2406.01781v1 , Github:None</p></li><li><p>Summary:</p></li></ol><ul><li><p>(1):该论文研究背景是基于扩散过程的生成模型在逆问题中的条件采样，利用已经训练好的无条件扩散模型来改进条件采样。</p></li><li><p>(2):过去的方法缺乏统一的框架，存在超参数敏感、训练昂贵或需要访问闭源API的权重的问题。这些方法的motivation也不是很充分。</p></li><li><p>(3):本文提出了一种新的方法DEFT，使用Doob’s h-transform来统一条件训练和采样。DEFT只需要微调一个小的网络来学习条件h-transform，同时保持大型无条件网络不变。</p></li><li><p>(4):论文中的方法在图像重建任务上取得了最好的感知质量和重建性能，速度快达1.6倍。这些性能支持了论文的目。</p></li></ul><ol><li>方法：</li></ol><ul><li><p>(1)：提出了一种新的方法DEFT，使用Doob’s h-transform来统一条件训练和采样，解决了传统方法的缺陷，如超参数敏感、训练昂贵或需要访问闭源API的权重的问题。</p></li><li><p>(2)：DEFT方法只需要微调一个小的网络来学习条件h-transform，同时保持大型无条件网络不变，从而提高了模型的训练效率和泛化能力。</p></li><li><p>(3)：在图像重建任务上，DEFT方法取得了最好的感知质量和重建性能，速度快达1.6倍，验证了该方法的有效性。</p></li><li><p>(4)：通过形式化的框架，DEFT方法提供了一种学习条件Score的VI目标，能够学习条件SDE从无条件SDE中，无需进行斯近似。</p></li><li><p>(5)：在非线去模糊任务中，DEFT方法也取得了良好的性能，验证了该方法在不同任务中的泛化能力。</p></li></ul><ol><li>Conclusion:</li></ol><pre><code>                - (1):该研究工作的意义在于提出了一个统一的数学框架，解决了基于扩散过程的生成模型在逆问题中的条件采样问题提高了模型的训练效率和泛化能力。

                - (2):创新点：提出了基于Doob's h-transform的条件扩散模型微调方法DEFT，解决了传统方法的缺陷；性能：在图像重建任务上取得了最好的感知质量和重建性能，速度快达1.6倍；工作量：只需要微调一个小的网络来学习条件h-transform，同时保持大型无条件网络不变，提高了模型的训练效率。
</code></pre><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/1c631e15a39c81660b3afb7dea0a5956241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/c18ab4b4f8e015a4e3e573135a023ca5241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/3fddddbd0dbd3bef9659308276311521241286257.jpg" align="middle"></details><h2 id="Long-and-Short-Guidance-in-Score-identity-Distillation-for-One-Step-Text-to-Image-Generation"><a href="#Long-and-Short-Guidance-in-Score-identity-Distillation-for-One-Step-Text-to-Image-Generation" class="headerlink" title="Long and Short Guidance in Score identity Distillation for One-Step   Text-to-Image Generation"></a>Long and Short Guidance in Score identity Distillation for One-Step Text-to-Image Generation</h2><p><strong>Authors:Mingyuan Zhou, Zhendong Wang, Huangjie Zheng, Hai Huang</strong></p><p>Diffusion-based text-to-image generation models trained on extensive text-image pairs have shown the capacity to generate photorealistic images consistent with textual descriptions. However, a significant limitation of these models is their slow sample generation, which requires iterative refinement through the same network. In this paper, we enhance Score identity Distillation (SiD) by developing long and short classifier-free guidance (LSG) to efficiently distill pretrained Stable Diffusion models without using real training data. SiD aims to optimize a model-based explicit score matching loss, utilizing a score-identity-based approximation alongside the proposed LSG for practical computation. By training exclusively with fake images synthesized with its one-step generator, SiD equipped with LSG rapidly improves FID and CLIP scores, achieving state-of-the-art FID performance while maintaining a competitive CLIP score. Specifically, its data-free distillation of Stable Diffusion 1.5 achieves a record low FID of 8.15 on the COCO-2014 validation set, with a CLIP score of 0.304 at an LSG scale of 1.5, and a FID of 9.56 with a CLIP score of 0.313 at an LSG scale of 2. We will make our PyTorch implementation and distilled Stable Diffusion one-step generators available at <a target="_blank" rel="noopener" href="https://github.com/mingyuanzhou/SiD-LSG">https://github.com/mingyuanzhou/SiD-LSG</a></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.01561v1">PDF</a></p><p>sorry, 您的ip已由于触发防滥用检测而被封禁,可能是因为使用云服务器如腾讯云或者国外代理(vpn)访问本网站，如果使用了vpn，关闭vpn或代理即可继续使用,本服务网址是<a target="_blank" rel="noopener" href="https://chat18.aichatos8.com">https://chat18.aichatos8.com</a> 如需合作接口调用请联系微信chatkf123 或者前往 <a target="_blank" rel="noopener" href="https://binjie09.shop">https://binjie09.shop</a> 自助购买key, 认为是误封需要解封的请前往<a target="_blank" rel="noopener" href="https://www.ip.cn/">https://www.ip.cn/</a> 查询ip信息,并发送信息至邮件 gpt33@binjie.site ，站长会定期看邮件并处理解封和合作问题，如需调用接口请见接口文档<a target="_blank" rel="noopener" href="https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123">https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123</a> 或者前往 <a target="_blank" rel="noopener" href="https://cat.gptxyy.cn">https://cat.gptxyy.cn</a> 注册使用（可付费使用gpt4 注册可免费使用3.5）</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: 一步文本到图像生成中的长短指导分数身份 Distillation (Long and Short Guidance in Score Identity Distillation for One-Step Text-to-Image Generation)</p></li><li><p>Authors: Mingyuan Zhou, Zhendong Wang, Huangjie Zheng, Hai Huang</p></li><li><p>Affiliation: 德克萨斯大学奥斯汀分校</p></li><li><p>Keywords: text-to-image generation, Score identity Distillation, Long-Short Guidance, Stable Diffusion</p></li><li><p>Urls: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01561v1">https://arxiv.org/abs/2406.01561v1</a>, Github: <a target="_blank" rel="noopener" href="https://github.com/mingyuanzhou/SiD-LSG">https://github.com/mingyuanzhou/SiD-LSG</a></p></li><li><p>Summary:</p><ul><li><p>(1):本文研究的是基于扩散模型的文本到图像生成方法，该方法可以生成 photorealistic 图像，但其生成速度慢，需要迭代 refinement。</p></li><li><p>(2):过去的方法是使用 Score identity Distillation（SiD）来 distill 训练的扩散模型，但是这些方法需要使用真实的训练数据，并且计算复杂。该方法的motivation是提高生成速度和质量。</p></li><li><p>(3):本文提出了一种新的方法，使用长短指导分数身份 Distillation（LSG）来 distill 预训练的 Stable Diffusion 模型，该方法不需要使用真实的训练数据，并且可以快速提高 FID 和 CLIP 评分。</p></li><li><p>(4):本文的方法在 COCO-2014 验证集上达到 state-of-the-art 的 FID 性能（8.15），同时保持竞争性的 CLIP 评分（0.304），证明了该方法的有效性。</p></li></ul></li><li>方法：</li></ol><ul><li><p>(1):本文提出了一种新的文本到图像生成方法，基于扩散模型的Score identity Distillation（SiD），使用长短指导分数身份 Distillation（LSG）来 distill 预训练的 Stable Diffusion 模型。</p></li><li><p>(2):该方法首先使用预训练的 Stable Diffusion 模型生成图像，然后使用LSG策略来指导生成图像的分数，提高生成图像的质量和速度。</p></li><li><p>(3):在LSG策略中，使用了不同的指导 scales（κ），例如1.5、2、3、4.5等，来控制生成图像的质量和速度。</p></li><li><p>(4):为了评估该方法的性能，本文使用了多种评估指标，包括FID、CLIP、HPSv2等，并与其他state-of-the-art方法进行了比较。</p></li><li><p>(5):实验结果表明，该方法能够生成高质量的图像，且具有较快的生成速度，达到了state-of-the-art的性能。</p></li><li><p>(6):为了进一步探究该方法的性能，本文还进行了些消融实验，例如改变批大小、学习率、指 scales 等，来评估该方法的鲁棒性和泛化能力。</p></li><li><p>(7):实验结果表明，该方法具有较好的鲁棒性和泛化能力，可以在不同的实验设置下生成高质量的图像。</p></li></ul><ol><li>Conclusion:</li></ol><ul><li><p>(1):本文的工作对基于扩散模型的文本到图像生成领域具有重要意义，可以提高生成速度和质量，并且具有良好的泛化能力和鲁棒性。</p></li><li><p>(2):创新点：提出了一种新的长短指导分数身份Distillation（LSG）策略，使用预训练的Stable Diffusion模型生成图像，并且不需要使用真实的训练数据；性能：在COCO-2014验证集上达到了state-of-the-art��FID性能（8.15），同时保持竞争性的CLIP评分（0.304）；工作量：实验结果表明，该方法具有较好的鲁棒性和泛化能力，可以在不同的实验设置下生成高质量的图像，但需要进一步探究该方法的可扩展性和实际应用价值。</p></li></ul><details><summary>点此查看论文截图</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/7d504c9b651f2ab6fc7f9ce6d5e64623241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/21fbd0387701d9ae2e365df8cc945589241286257.jpg" align="middle"></details></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io/2024/06/05/Paper/2024-06-05/Diffusion%20Models/">https://kedreamix.github.io/2024/06/05/Paper/2024-06-05/Diffusion Models/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Diffusion-Models/">Diffusion Models</a></div><div class="post_share"><div class="social-share" data-image="https://pica.zhimg.com/80/v2-890676236f48f9a7d915a0c42c40aa38.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/06/05/Paper/2024-06-05/3DGS/" title="3DGS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-2196bb11e15de7b5d3440823b4c92ca5.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">3DGS</div></div></a></div><div class="next-post pull-right"><a href="/2024/06/05/Paper/2024-06-05/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="元宇宙/虚拟人"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2bdb0ecbbc3a0a2420781e472b68ba52.png" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">元宇宙/虚拟人</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/03/03/Paperscape/EMO/" title="EMO Emote Portrait Alive - 阿里HumanAIGC"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-6492e24fb03ffa98135dc584535ab7d9.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-03</div><div class="title">EMO Emote Portrait Alive - 阿里HumanAIGC</div></div></a></div><div><a href="/2024/01/24/Paper/2024-01-24/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-71a37c439c6714e8867560f580599d2f.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-24</div><div class="title">Diffusion Models</div></div></a></div><div><a href="/2024/01/30/Paper/2024-01-30/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-e55358c77a9d65f15701e8f33262e2a4.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-30</div><div class="title">Diffusion Models</div></div></a></div><div><a href="/2024/02/13/Paper/2024-02-13/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-3709a9941aada6c4d3ed35934e311765.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-13</div><div class="title">Diffusion Models</div></div></a></div><div><a href="/2024/02/02/Paper/2024-02-02/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-5920453c69c00995f18077b22d4a790e.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-02</div><div class="title">Diffusion Models</div></div></a></div><div><a href="/2024/02/23/Paper/2024-02-23/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-ff425802a32a4519e30b9044a3eed1e8.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-23</div><div class="title">Diffusion Models</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-06-05-%E6%9B%B4%E6%96%B0"><span class="toc-text">2024-06-05 更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#CamCo-Camera-Controllable-3D-Consistent-Image-to-Video-Generation"><span class="toc-text">CamCo: Camera-Controllable 3D-Consistent Image-to-Video Generation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Guiding-a-Diffusion-Model-with-a-Bad-Version-of-Itself"><span class="toc-text">Guiding a Diffusion Model with a Bad Version of Itself</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Stable-Pose-Leveraging-Transformers-for-Pose-Guided-Text-to-Image-Generation"><span class="toc-text">Stable-Pose: Leveraging Transformers for Pose-Guided Text-to-Image Generation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Learning-Image-Priors-through-Patch-based-Diffusion-Models-for-Solving-Inverse-Problems"><span class="toc-text">Learning Image Priors through Patch-based Diffusion Models for Solving Inverse Problems</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RoomTex-Texturing-Compositional-Indoor-Scenes-via-Iterative-Inpainting"><span class="toc-text">RoomTex: Texturing Compositional Indoor Scenes via Iterative Inpainting</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Flash-Diffusion-Accelerating-Any-Conditional-Diffusion-Model-for-Few-Steps-Image-Generation"><span class="toc-text">Flash Diffusion: Accelerating Any Conditional Diffusion Model for Few Steps Image Generation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#A-Survey-of-Transformer-Enabled-Time-Series-Synthesis"><span class="toc-text">A Survey of Transformer Enabled Time Series Synthesis</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#GraVITON-Graph-based-garment-warping-with-attention-guided-inversion-for-Virtual-tryon"><span class="toc-text">GraVITON: Graph based garment warping with attention guided inversion for Virtual-tryon</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Follow-Your-Emoji-Fine-Controllable-and-Expressive-Freestyle-Portrait-Animation"><span class="toc-text">Follow-Your-Emoji: Fine-Controllable and Expressive Freestyle Portrait Animation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Cross-Domain-Graph-Data-Scaling-A-Showcase-with-Diffusion-Models"><span class="toc-text">Cross-Domain Graph Data Scaling: A Showcase with Diffusion Models</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#L-MAGIC-Language-Model-Assisted-Generation-of-Images-with-Coherence"><span class="toc-text">L-MAGIC: Language Model Assisted Generation of Images with Coherence</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DEFT-Efficient-Finetuning-of-Conditional-Diffusion-Models-by-Learning-the-Generalised-h-transform"><span class="toc-text">DEFT: Efficient Finetuning of Conditional Diffusion Models by Learning the Generalised $h$-transform</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Long-and-Short-Guidance-in-Score-identity-Distillation-for-One-Step-Text-to-Image-Generation"><span class="toc-text">Long and Short Guidance in Score identity Distillation for One-Step Text-to-Image Generation</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image:url('https://pica.zhimg.com/80/v2-890676236f48f9a7d915a0c42c40aa38.png')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js").then((()=>{pangu.autoSpacingPage()}))}function panguInit(){panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.1},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script></div><script id="canvas_nest" defer color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><script data-pjax>function butterfly_swiper_injector_config(){var a=document.getElementById("recent-posts");console.log("已挂载butterfly_swiper"),a.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/25/Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="2024/01/25/Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="2024/01/25/Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/17/Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023/12/17/Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="2023/12/17/Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script></body></html>