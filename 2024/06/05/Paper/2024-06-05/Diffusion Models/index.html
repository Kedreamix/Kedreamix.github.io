<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>Diffusion Models | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-06-05  CamCo Camera-Controllable 3D-Consistent Image-to-Video Generation"><meta property="og:type" content="article"><meta property="og:title" content="Diffusion Models"><meta property="og:url" content="https://kedreamix.github.io/2024/06/05/Paper/2024-06-05/Diffusion%20Models/index.html"><meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World"><meta property="og:description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-06-05  CamCo Camera-Controllable 3D-Consistent Image-to-Video Generation"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://pica.zhimg.com/80/v2-890676236f48f9a7d915a0c42c40aa38.png"><meta property="article:published_time" content="2024-06-05T10:15:50.000Z"><meta property="article:modified_time" content="2024-06-05T10:15:50.765Z"><meta property="article:author" content="Kedreamix"><meta property="article:tag" content="Diffusion Models"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://pica.zhimg.com/80/v2-890676236f48f9a7d915a0c42c40aa38.png"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/2024/06/05/Paper/2024-06-05/Diffusion%20Models/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-74LZ5BEQQ1")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!0,languages:{hits_empty:"æ‰¾ä¸åˆ°æ‚¨æŸ¥è¯¢çš„å†…å®¹ï¼š${query}",hits_stats:"å…±æ‰¾åˆ° ${hits} ç¯‡æ–‡ç« "}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"ç¹",msgToSimplifiedChinese:"ç®€"},noticeOutdate:{limitDay:500,position:"top",messagePrev:"It has been",messageNext:"days since the last update, the content of the article may be outdated."},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:200},copy:{success:"å¤åˆ¶æˆåŠŸ",error:"å¤åˆ¶é”™è¯¯",noSupport:"æµè§ˆå™¨ä¸æ”¯æŒ"},relativeDate:{homepage:!0,post:!0},runtime:"å¤©",dateSuffix:{just:"åˆšåˆš",min:"åˆ†é’Ÿå‰",hour:"å°æ—¶å‰",day:"å¤©å‰",month:"ä¸ªæœˆå‰"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js",buttonText:"åŠ è½½æ›´å¤š"},isPhotoFigcaption:!0,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"Diffusion Models",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-06-05 18:15:50"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach((e=>{n.setAttribute(e,t[e])})),document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)})),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=24?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 7.0.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml"></head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{const d=document.getElementById("loading-box"),e=document.body,o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l(),window.addEventListener("load",(()=>{o()}))})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">æ–‡ç« </div><div class="length-num">146</div></a><a href="/tags/"><div class="headline">æ ‡ç­¾</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">åˆ†ç±»</div><div class="length-num">6</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>é¦–é¡µ</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>æ—¶é—´è½´</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>æ ‡ç­¾</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>åˆ†ç±»</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>å‹é“¾</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url('https://pica.zhimg.com/80/v2-890676236f48f9a7d915a0c42c40aa38.png')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i> <span>æœç´¢</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>é¦–é¡µ</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>æ—¶é—´è½´</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>æ ‡ç­¾</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>åˆ†ç±»</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>å‹é“¾</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Diffusion Models</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">å‘è¡¨äº</span><time class="post-meta-date-created" datetime="2024-06-05T10:15:50.000Z" title="å‘è¡¨äº 2024-06-05 18:15:50">2024-06-05</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">æ›´æ–°äº</span><time class="post-meta-date-updated" datetime="2024-06-05T10:15:50.765Z" title="æ›´æ–°äº 2024-06-05 18:15:50">2024-06-05</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">å­—æ•°æ€»è®¡:</span><span class="word-count">15.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">é˜…è¯»æ—¶é•¿:</span><span>60åˆ†é’Ÿ</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="Diffusion Models"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">é˜…è¯»é‡:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-06-05-æ›´æ–°"><a href="#2024-06-05-æ›´æ–°" class="headerlink" title="2024-06-05 æ›´æ–°"></a>2024-06-05 æ›´æ–°</h1><h2 id="CamCo-Camera-Controllable-3D-Consistent-Image-to-Video-Generation"><a href="#CamCo-Camera-Controllable-3D-Consistent-Image-to-Video-Generation" class="headerlink" title="CamCo: Camera-Controllable 3D-Consistent Image-to-Video Generation"></a>CamCo: Camera-Controllable 3D-Consistent Image-to-Video Generation</h2><p><strong>Authors:Dejia Xu, Weili Nie, Chao Liu, Sifei Liu, Jan Kautz, Zhangyang Wang, Arash Vahdat</strong></p><p>Recently video diffusion models have emerged as expressive generative tools for high-quality video content creation readily available to general users. However, these models often do not offer precise control over camera poses for video generation, limiting the expression of cinematic language and user control. To address this issue, we introduce CamCo, which allows fine-grained Camera pose Control for image-to-video generation. We equip a pre-trained image-to-video generator with accurately parameterized camera pose input using Pl\â€ucker coordinates. To enhance 3D consistency in the videos produced, we integrate an epipolar attention module in each attention block that enforces epipolar constraints to the feature maps. Additionally, we fine-tune CamCo on real-world videos with camera poses estimated through structure-from-motion algorithms to better synthesize object motion. Our experiments show that CamCo significantly improves 3D consistency and camera control capabilities compared to previous models while effectively generating plausible object motion. Project page: <a target="_blank" rel="noopener" href="https://ir1d.github.io/CamCo/">https://ir1d.github.io/CamCo/</a></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.02509v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://ir1d.github.io/CamCo/">https://ir1d.github.io/CamCo/</a></p><p><strong>Summary</strong><br>è§†é¢‘æ‰©æ•£æ¨¡å‹CamCoå®ç°äº†å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆçš„ç²¾ç»†ç›¸æœºå§¿æ€æ§åˆ¶ï¼Œæé«˜äº†è§†é¢‘ç”Ÿæˆçš„ä¸‰ç»´ä¸€è‡´æ€§å’Œæ‘„åƒæœºæ§åˆ¶èƒ½åŠ›ã€‚</p><p><strong>Key Takeaways</strong><br>â€¢ è§†é¢‘æ‰©æ•£æ¨¡å‹å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„è§†é¢‘å†…å®¹ï¼Œä½†ç¼ºä¹ç²¾ç»†çš„æ‘„åƒæœºå§¿æ€æ§åˆ¶ã€‚<br>â€¢ CamCoæ¨¡å‹é€šè¿‡Pl\â€uckeråæ ‡å‡†ç¡®åœ°å‚æ•°åŒ–æ‘„åƒæœºå§¿æ€è¾“å…¥ï¼Œå®ç°äº†å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆçš„ç²¾ç»†æ§åˆ¶ã€‚<br>â€¢ epipolar attentionæ¨¡å—å¯ä»¥å¼ºåˆ¶æ‰§è¡Œç‰¹å¾å›¾çš„æçº¿çº¦æŸï¼Œä»è€Œæé«˜è§†é¢‘ç”Ÿæˆçš„ä¸‰ç»´ä¸€è‡´æ€§ã€‚<br>â€¢ CamCoæ¨¡å‹åœ¨çœŸå®ä¸–ç•Œè§†é¢‘ä¸Šè¿›è¡Œå¾®è°ƒï¼Œä»¥æ›´å¥½åœ°åˆæˆç‰©ä½“è¿åŠ¨ã€‚<br>â€¢ å®éªŒç»“æœè¡¨æ˜ï¼ŒCamCoæ¨¡å‹ç›¸æ¯”ä¹‹å‰çš„æ¨¡å‹å…·æœ‰æ›´å¥½çš„ä¸‰ç»´ä¸€è‡´æ€§å’Œæ‘„åƒæœºæ§åˆ¶èƒ½åŠ›ã€‚<br>â€¢ CamCoæ¨¡å‹å¯ä»¥ç”Ÿæˆå¯é çš„ç‰©ä½“è¿åŠ¨ã€‚<br>â€¢ CamCoé¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://ir1d.github.io/CamCo/">https://ir1d.github.io/CamCo/</a></p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>Title: CamCo: Camera-Controllable 3D-Consistentå›¾åƒåˆ°è§†é¢‘ç”Ÿæˆ (Camera-Controllable 3D-Consistent Image-to-Video Generation)</li></ol><ol><li>Authors: Dejia Xu, Weili Nie, Chao Liu, Sifei Liu, Jan Kautz, Zhangyang Wang, Arash Vahdat</li></ol><ol><li>Affiliation: å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡ (University of Texas at Austin)</li></ol><ol><li>Keywords: video diffusion models, image-to-video generation, camera pose control, 3D consistency</li></ol><ol><li>Urls: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02509v1">https://arxiv.org/abs/2406.02509v1</a>, Github:None</li></ol><ol><li>Summary:</li></ol><pre><code>- (1):è¿‘å¹´æ¥ï¼Œè§†é¢‘æ‰©æ•£æ¨¡å‹å·²ç»æˆä¸ºé«˜è´¨é‡è§†é¢‘å†…å®¹åˆ›å»ºçš„æœ‰è¡¨è¾¾åŠ›ç”Ÿæˆå·¥å…·ï¼Œä½†è¿™äº›æ¨¡å‹é€šå¸¸ä¸æä¾›ç²¾å‡†çš„æ‘„åƒæœºå§¿æ€æ§åˆ¶ï¼Œé™åˆ¶äº†ç”µå½±è¯­è¨€å’Œç”¨æˆ·æ§åˆ¶çš„è¡¨è¾¾ã€‚

- (2):è¿‡å»çš„æ–¹æ³•ä¸æä¾›æ‘„åƒæœºå§¿æ€æ§åˆ¶ï¼Œé™åˆ¶äº†ç”µå½±è¯­è¨€å’Œç”¨æˆ·æ§åˆ¶çš„è¡¨è¾¾ã€‚ CamCo æ¨¡å‹çš„æå‡ºæ˜¯ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ã€‚

- (3):æœ¬æ–‡æå‡º CamCo æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥ç²¾å‡†æ§åˆ¶æ‘„åƒæœºå§¿æ€ï¼Œç”Ÿæˆ 3D ä¸€è‡´çš„è§†é¢‘ã€‚æˆ‘ä»¬ä½¿ç”¨ PlÃ¼cker åæ ‡æ¥å‚æ•°åŒ–æ‘„åƒæœºå§¿æ€ï¼Œå¹¶é›†æˆ epipolar æ³¨æ„åŠ›æ¨¡å—æ¥å¼ºåˆ¶æ‰§è¡Œ epipolar çº¦æŸã€‚

- (4):å®éªŒç»“æœè¡¨æ˜ï¼ŒCamCo æ¨¡å‹å¯ä»¥ç”Ÿæˆ 3D ä¸€è‡´çš„è§†é¢‘ï¼Œä¸”èƒ½å¤Ÿç²¾å‡†æ§åˆ¶æ‘„åƒæœºå§¿æ€ï¼Œè¾¾åˆ°é«˜è´¨é‡çš„è§†é¢‘ç”Ÿæˆæ•ˆæœã€‚
</code></pre><p>sorry, æ‚¨çš„ipå·²ç”±äºè§¦å‘é˜²æ»¥ç”¨æ£€æµ‹è€Œè¢«å°ç¦,è¯·å‹¿æ»¥ç”¨æœ¬ç«™,æœ¬æœåŠ¡ç½‘å€æ˜¯<a target="_blank" rel="noopener" href="https://chat18.aichatos8.com">https://chat18.aichatos8.com</a> å¦‚éœ€åˆä½œæ¥å£è°ƒç”¨è¯·è”ç³»å¾®ä¿¡chatkf123 æˆ–è€…å‰å¾€ <a target="_blank" rel="noopener" href="https://binjie09.shop">https://binjie09.shop</a> è‡ªåŠ©è´­ä¹°key, è®¤ä¸ºæ˜¯è¯¯å°éœ€è¦è§£å°çš„è¯·å‰å¾€<a target="_blank" rel="noopener" href="https://www.ip.cn/">https://www.ip.cn/</a> æŸ¥è¯¢ipä¿¡æ¯,å¹¶å‘é€ä¿¡æ¯è‡³é‚®ä»¶ gpt33@binjie.site ï¼Œç«™é•¿ä¼šå®šæœŸçœ‹é‚®ä»¶å¹¶å¤„ç†è§£å°å’Œåˆä½œé—®é¢˜ï¼Œå¦‚éœ€è°ƒç”¨æ¥å£è¯·è§æ¥å£æ–‡æ¡£<a target="_blank" rel="noopener" href="https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82">https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82</a> å¦‚éœ€åˆä½œæ¥å£è°ƒç”¨è¯·è”ç³»å¾®ä¿¡chatkf123 æˆ–è€…å‰å¾€ <a target="_blank" rel="noopener" href="https://cat.gptxyy.cn">https://cat.gptxyy.cn</a> æ³¨å†Œä½¿ç”¨ï¼ˆå¯ä»˜è´¹ä½¿ç”¨gpt4 æ³¨å†Œå¯å…è´¹ä½¿ç”¨3.5ï¼‰</p><p>sorry, æ‚¨çš„ipå·²ç”±äºè§¦å‘é˜²æ»¥ç”¨æ£€æµ‹è€Œè¢«å°ç¦,å¯èƒ½æ˜¯å› ä¸ºä½¿ç”¨äº‘æœåŠ¡å™¨å¦‚è…¾è®¯äº‘æˆ–è€…å›½å¤–ä»£ç†(vpn)è®¿é—®æœ¬ç½‘ç«™ï¼Œå¦‚æœä½¿ç”¨äº†vpnï¼Œå…³é—­vpnæˆ–ä»£ç†å³å¯ç»§ç»­ä½¿ç”¨,æœ¬æœåŠ¡ç½‘å€æ˜¯<a target="_blank" rel="noopener" href="https://chat18.aichatos8.com">https://chat18.aichatos8.com</a> å¦‚éœ€åˆä½œæ¥å£è°ƒç”¨è¯·è”ç³»å¾®ä¿¡chatkf123 æˆ–è€…å‰å¾€ <a target="_blank" rel="noopener" href="https://binjie09.shop">https://binjie09.shop</a> è‡ªåŠ©è´­ä¹°key, è®¤ä¸ºæ˜¯è¯¯å°éœ€è¦è§£å°çš„è¯·å‰å¾€<a target="_blank" rel="noopener" href="https://www.ip.cn/">https://www.ip.cn/</a> æŸ¥è¯¢ipä¿¡æ¯,å¹¶å‘é€ä¿¡æ¯è‡³é‚®ä»¶ gpt33@binjie.site ï¼Œç«™é•¿ä¼šå®šæœŸçœ‹é‚®ä»¶å¹¶å¤„ç†è§£å°å’Œåˆä½œé—®é¢˜ï¼Œå¦‚éœ€è°ƒç”¨æ¥å£è¯·è§æ¥å£æ–‡æ¡£<a target="_blank" rel="noopener" href="https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82ï¼Œå¦‚éœ€åˆä½œæ¥å£è°ƒç”¨è¯·è”ç³»å¾®ä¿¡chatkf123">https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82ï¼Œå¦‚éœ€åˆä½œæ¥å£è°ƒç”¨è¯·è”ç³»å¾®ä¿¡chatkf123</a> æˆ–è€…å‰å¾€ <a target="_blank" rel="noopener" href="https://cat.gptxyy.cn">https://cat.gptxyy.cn</a> æ³¨å†Œä½¿ç”¨ï¼ˆå¯ä»˜è´¹ä½¿ç”¨gpt4 æ³¨å†Œå¯å…è´¹ä½¿ç”¨3.5ï¼‰</p><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/1c8be5285001bbf933e19f41ce9137e5241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/4d92a8d29d3149c75770b9b4c963a79a241286257.jpg" align="middle"></details><h2 id="Guiding-a-Diffusion-Model-with-a-Bad-Version-of-Itself"><a href="#Guiding-a-Diffusion-Model-with-a-Bad-Version-of-Itself" class="headerlink" title="Guiding a Diffusion Model with a Bad Version of Itself"></a>Guiding a Diffusion Model with a Bad Version of Itself</h2><p><strong>Authors:Tero Karras, Miika Aittala, Tuomas KynkÃ¤Ã¤nniemi, Jaakko Lehtinen, Timo Aila, Samuli Laine</strong></p><p>The primary axes of interest in image-generating diffusion models are image quality, the amount of variation in the results, and how well the results align with a given condition, e.g., a class label or a text prompt. The popular classifier-free guidance approach uses an unconditional model to guide a conditional model, leading to simultaneously better prompt alignment and higher-quality images at the cost of reduced variation. These effects seem inherently entangled, and thus hard to control. We make the surprising observation that it is possible to obtain disentangled control over image quality without compromising the amount of variation by guiding generation using a smaller, less-trained version of the model itself rather than an unconditional model. This leads to significant improvements in ImageNet generation, setting record FIDs of 1.01 for 64x64 and 1.25 for 512x512, using publicly available networks. Furthermore, the method is also applicable to unconditional diffusion models, drastically improving their quality.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.02507v1">PDF</a></p><p>sorry, æ‚¨çš„ipå·²ç”±äºè§¦å‘é˜²æ»¥ç”¨æ£€æµ‹è€Œè¢«å°ç¦,å¯èƒ½æ˜¯å› ä¸ºä½¿ç”¨äº‘æœåŠ¡å™¨å¦‚è…¾è®¯äº‘æˆ–è€…å›½å¤–ä»£ç†(vpn)è®¿é—®æœ¬ç½‘ç«™ï¼Œå¦‚æœä½¿ç”¨äº†vpnï¼Œå…³é—­vpnæˆ–ä»£ç†å³å¯ç»§ç»­ä½¿ç”¨,æœ¬æœåŠ¡ç½‘å€æ˜¯<a target="_blank" rel="noopener" href="https://chat18.aichatos8.com">https://chat18.aichatos8.com</a> å¦‚éœ€åˆä½œæ¥å£è°ƒç”¨è¯·è”ç³»å¾®ä¿¡chatkf123 æˆ–è€…å‰å¾€ <a target="_blank" rel="noopener" href="https://binjie09.shop">https://binjie09.shop</a> è‡ªåŠ©è´­ä¹°key, è®¤ä¸ºæ˜¯è¯¯å°éœ€è¦è§£å°çš„è¯·å‰å¾€<a target="_blank" rel="noopener" href="https://www.ip.cn/">https://www.ip.cn/</a> æŸ¥è¯¢ipä¿¡æ¯,å¹¶å‘é€ä¿¡æ¯è‡³é‚®ä»¶ gpt33@binjie.site ï¼Œç«™é•¿ä¼šå®šæœŸçœ‹é‚®ä»¶å¹¶å¤„ç†è§£å°å’Œåˆä½œé—®é¢˜ï¼Œå¦‚éœ€è°ƒç”¨æ¥å£è¯·è§æ¥å£æ–‡æ¡£<a target="_blank" rel="noopener" href="https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82ï¼Œå¦‚éœ€åˆä½œæ¥å£è°ƒç”¨è¯·è”ç³»å¾®ä¿¡chatkf123">https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82ï¼Œå¦‚éœ€åˆä½œæ¥å£è°ƒç”¨è¯·è”ç³»å¾®ä¿¡chatkf123</a> æˆ–è€…å‰å¾€ <a target="_blank" rel="noopener" href="https://cat.gptxyy.cn">https://cat.gptxyy.cn</a> æ³¨å†Œä½¿ç”¨ï¼ˆå¯ä»˜è´¹ä½¿ç”¨gpt4 æ³¨å†Œå¯å…è´¹ä½¿ç”¨3.5ï¼‰</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: ä½¿ç”¨ä¸å¤ªå¥½çš„è‡ªå·±ç‰ˆæœ¬æ¥æŒ‡å¯¼æ‰©æ•£æ¨¡å‹ï¼ˆGuiding a Diffusion Model with a Bad Version of Itselfï¼‰</p></li><li><p>Authors: (no author list provided)</p></li><li><p>Affiliation: (no affiliation provided)</p></li><li><p>Keywords: diffusion model, image generation, classifier-free guidance</p></li><li><p>Urls: (no URL provided), Github: None</p></li><li><p>Summary:</p><ul><li><p>(1):æœ¬æ–‡ç ”ç©¶çš„æ˜¯å›¾åƒç”Ÿæˆæ‰©æ•£æ¨¡å‹ä¸­æ§åˆ¶å›¾åƒè´¨é‡ã€å¤šæ ·æ€§å’Œæ¡ä»¶å¯¹é½çš„ä¸‰ä¸ªä¸»è¦axesé—®é¢˜ã€‚</p></li><li><p>(2):è¿‡å»çš„æ–¹æ³•ä¸­ï¼Œclassifier-freeæŒ‡å¯¼æ–¹æ³•ä½¿ç”¨æ— æ¡ä»¶æ¨¡å‹æ¥æŒ‡å¯¼æ¡ä»¶æ¨¡å‹ï¼Œæé«˜äº†å›¾åƒè´¨é‡å’Œæ¡ä»¶å¯¹é½ï¼Œä½†å‡å°‘äº†å¤šæ ·æ€§ã€‚è¿™äº›æ•ˆæœä¼¼ä¹æ˜¯è€¦åˆçš„ï¼Œéš¾ä»¥æ§åˆ¶ã€‚</p></li><li><p>(3):æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå³ä½¿ç”¨æ¨¡å‹è‡ªèº«çš„è¾ƒå°ã€è¾ƒå°‘è®­ç»ƒçš„ç‰ˆæœ¬æ¥æŒ‡å¯¼ç”Ÿæˆï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ— æ¡ä»¶æ¨¡å‹ã€‚è¿™ä½¿å¾—å¯ä»¥ç‹¬ç«‹æ§åˆ¶å›¾åƒè´¨é‡ï¼Œè€Œä¸ç‰ºç‰²å¤šæ ·æ€§ã€‚</p></li><li><p>(4):æœ¬æ–‡çš„æ–¹æ³•åœ¨ImageNetç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†æœ€å¥½çš„FIDè®°å½•ï¼Œåˆ†åˆ«ä¸º64Ã—64å’Œ512Ã—512çš„1.01å’Œ1.25ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•ä¹Ÿé€‚ç”¨äºæ— æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œå¤§å¹…æé«˜äº†å…¶è´¨é‡ã€‚</p></li></ul></li><li>æ–¹æ³•ï¼š</li></ol><ul><li><p>(1)ï¼šæå‡ºäº†ä¸€ç§æ–°çš„æŒ‡å¯¼æ‰©æ•£æ¨¡å‹çš„æ–¹æ³•ï¼Œå³ä½¿ç”¨æ¨¡å‹è‡ªèº«çš„è¾ƒå°ã€è¾ƒå°‘è®­ç»ƒçš„ç‰ˆæœ¬æ¥æŒ‡å¯¼ç”Ÿæˆï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ— æ¡ä»¶æ¨¡å‹ï¼ˆClassifier-Free Guidanceï¼‰;</p></li><li><p>(2)ï¼šè¯¥æ–¹æ³•å¯ä»¥ç‹¬ç«‹æ§åˆ¶å›¾åƒè´¨é‡ï¼Œè€Œä¸ç‰ºç‰²å¤šæ ·æ€§ï¼Œè§£å†³äº†è¿‡å»æ–¹æ³•ä¸­å›¾åƒè´¨é‡ã€å¤šæ ·æ€§å’Œæ¡ä»¶å¯¹é½ä¸‰ä¸ªä¸»è¦axesé—®é¢˜çš„è€¦åˆé—®é¢˜;</p></li><li><p>(3)ï¼šåœ¨æŒ‡å¯¼æ¨¡å‹çš„é€‰æ‹©ä¸Šï¼Œä½œè€…å‘ç°ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹å®¹é‡å’Œè¾ƒå°‘çš„è®­ç»ƒæ¬¡æ•°å¯ä»¥è·å¾—æ›´å¥½çš„ç»“æœï¼Œä¸”è¿™ä¸¤ä¸ªå› ç´ çš„å½±å“æ˜¯orthogonalçš„;</p></li><li><p>(4)ï¼šä½œè€…è¿˜ï¿½ï¿½ç©¶äº†æŒ‡å¯¼æƒé‡ã€EMAé•¿åº¦å‚æ•°ç­‰è¶…å‚æ•°å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œå‘ç°æ¨¡å‹å¯¹è¿™äº›å‚æ•°çš„æ•æ„Ÿæ€§è¾ƒé«˜;</p></li><li><p>(5)ï¼šåœ¨å®éªŒä¸­ï¼Œä½œè€…ä½¿ç”¨äº†ImageNet-512å’ŒImageNet-64ä¸¤ä¸ªæ•°æ®é›†ï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•å¯ä»¥è·å¾— state-of-the-art çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å›¾åƒè´¨é‡å’Œå¤šæ ·æ€§æ–¹é¢;</p></li><li><p>(6)ï¼šä½œè€…è¿˜å°†è¯¥æ–¹æ³•åº”ç”¨äºæ— æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œç»“æœè¡¨æ˜å¯ä»¥å¤§å¹…æé«˜å…¶è´¨é‡;</p></li><li><p>(7)ï¼šæœ€åï¼Œä½œè€…è®¨è®ºäº†è¯¥æ–¹æ³•çš„æœªæ¥å‘å±•æ–¹å‘ï¼ŒåŒ…æ‹¬å°†å…¶åº”ç”¨äºå¤§è§„æ¨¡å›¾åƒç”Ÿæˆæ¨¡å‹å’Œæ¢ç©¶å…¶åœ¨è‰ºæœ¯æ§åˆ¶æ–¹é¢çš„å¯èƒ½æ€§ã€‚</p></li></ul><ol><li>Conclusion:</li></ol><ul><li><p>(1): æœ¬æ–‡çš„å·¥ä½œå¯¹äºå›¾åƒç”Ÿæˆæ‰©æ•£æ¨¡å‹çš„æ§åˆ¶å’Œä¼˜åŒ–å…·æœ‰é‡è¦æ„ä¹‰ï¼Œå¯ä»¥ç‹¬ç«‹æ§åˆ¶å›¾åƒè´¨é‡å’Œå¤šæ ·æ€§ï¼Œè§£å†³äº†è¿‡å»æ–¹æ³•ä¸­çš„è€¦åˆé—®é¢˜ã€‚</p></li><li><p>(2): Innovation point: æœ¬æ–‡æå‡ºçš„ä½¿ç”¨æ¨¡å‹è‡ªèº«çš„è¾ƒå°ã€è¾ƒå°‘è®­ç»ƒçš„ç‰ˆæœ¬æ¥æŒ‡å¯¼ç”Ÿæˆçš„æ–¹æ³•æ˜¯åˆ›æ–°æ€§çš„ï¼Œè§£å†³äº†è¿‡å»æ–¹æ³•ä¸­çš„è€¦åˆé—®é¢˜ï¼› Performance: æœ¬æ–‡çš„æ–¹æ³•åœ¨ImageNetç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†æœ€å¥½çš„FIDè®°å½•ï¼Œåˆ†åˆ«ä¸º64Ã—64å’Œ512Ã—512çš„1.01å’Œ1.25ï¼Œè¯æ˜äº†å…¶ä¼˜è¶Šçš„æ€§èƒ½ï¼› Workload: æœ¬æ–‡çš„å®éªŒä½¿ç”¨äº†ImageNet-512å’ŒImageNet-64ä¸¤ä¸ªæ•°æ®é›†ï¼Œå®éªŒè®¾è®¡å’Œç»“æœåˆ†æè¾ƒä¸ºå……åˆ†ï¼Œä½†è¶…å‚æ•°çš„è°ƒæ•´å’Œé€‰æ‹©å¯èƒ½éœ€è¦æ›´å¤šçš„å®éªŒå’Œåˆ†æã€‚</p></li></ul><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/ece422492bfcb0ed72fed53bc4f94053241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/79b8e3c566552b5ee83993b5efabfbef241286257.jpg" align="middle"></details><h2 id="Stable-Pose-Leveraging-Transformers-for-Pose-Guided-Text-to-Image-Generation"><a href="#Stable-Pose-Leveraging-Transformers-for-Pose-Guided-Text-to-Image-Generation" class="headerlink" title="Stable-Pose: Leveraging Transformers for Pose-Guided Text-to-Image   Generation"></a>Stable-Pose: Leveraging Transformers for Pose-Guided Text-to-Image Generation</h2><p><strong>Authors:Jiajun Wang, Morteza Ghahremani, Yitong Li, BjÃ¶rn Ommer, Christian Wachinger</strong></p><p>Controllable text-to-image (T2I) diffusion models have shown impressive performance in generating high-quality visual content through the incorporation of various conditions. Current methods, however, exhibit limited performance when guided by skeleton human poses, especially in complex pose conditions such as side or rear perspectives of human figures. To address this issue, we present Stable-Pose, a novel adapter model that introduces a coarse-to-fine attention masking strategy into a vision Transformer (ViT) to gain accurate pose guidance for T2I models. Stable-Pose is designed to adeptly handle pose conditions within pre-trained Stable Diffusion, providing a refined and efficient way of aligning pose representation during image synthesis. We leverage the query-key self-attention mechanism of ViTs to explore the interconnections among different anatomical parts in human pose skeletons. Masked pose images are used to smoothly refine the attention maps based on target pose-related features in a hierarchical manner, transitioning from coarse to fine levels. Additionally, our loss function is formulated to allocate increased emphasis to the pose region, thereby augmenting the modelâ€™s precision in capturing intricate pose details. We assessed the performance of Stable-Pose across five public datasets under a wide range of indoor and outdoor human pose scenarios. Stable-Pose achieved an AP score of 57.1 in the LAION-Human dataset, marking around 13% improvement over the established technique ControlNet. The project link and code is available at <a target="_blank" rel="noopener" href="https://github.com/ai-med/StablePose">https://github.com/ai-med/StablePose</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.02485v1">PDF</a></p><p><strong>Summary</strong><br>æå‡ºStable-Poseæ¨¡å‹ï¼Œå¼•å…¥ç²—åˆ°ç»†çš„æ³¨æ„é®ç½©ç­–ç•¥ï¼Œæé«˜æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹å¯¹äººä½“å§¿åŠ¿çš„æ§åˆ¶èƒ½åŠ›ã€‚</p><p><strong>Key Takeaways</strong><br>â€¢ Stable-Poseæ¨¡å‹å¼•å…¥ç²—åˆ°ç»†çš„æ³¨æ„é®ç½©ç­–ç•¥ï¼Œæé«˜äº†æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹å¯¹äººä½“å§¿åŠ¿çš„æ§åˆ¶èƒ½åŠ›ã€‚<br>â€¢ è¯¥æ¨¡å‹åŸºäºè§†è§‰Transformerï¼ˆViTï¼‰ï¼Œä½¿ç”¨æŸ¥è¯¢é”®è‡ªæ³¨æ„æœºåˆ¶æ¥æ¢ç´¢äººä½“å§¿åŠ¿éª¨æ¶ä¸­ä¸åŒè§£å‰–éƒ¨åˆ†ä¹‹é—´çš„äº¤å‰è¿æ¥ã€‚<br>â€¢ æ©ç å§¿åŠ¿å›¾åƒè¢«ç”¨äºå¹³æ»‘åœ°ç»†åŒ–æ³¨æ„å›¾ï¼ŒåŸºäºç›®æ ‡å§¿åŠ¿ç›¸å…³ç‰¹å¾ï¼Œä»¥å±‚æ¬¡æ–¹å¼ä»ç²—åˆ°ç»†ã€‚<br>â€¢ æŸå¤±å‡½æ•°è¢«è®¾ä¸ºå°†æ›´å¤šçš„æƒé‡åˆ†é…ç»™å§¿åŠ¿åŒºåŸŸï¼Œä»¥å¢å¼ºæ¨¡å‹æ•æ‰ç»†è…»å§¿åŠ¿ç»†èŠ‚çš„ç²¾åº¦ã€‚<br>â€¢ Stable-Poseæ¨¡å‹åœ¨äº”ä¸ªå…¬å…±æ•°æ®é›†ä¸‹çš„å¹¿æ³›å®¤å†…å’Œå®¤å¤–äººä½“å§¿åŠ¿åœºæ™¯ä¸­è¿›è¡Œè¯„ä¼°ï¼Œå–å¾—äº†ä¼˜å¼‚çš„ç»“æœã€‚<br>â€¢ åœ¨LAION-Humanæ•°æ®é›†ä¸Šï¼ŒStable-Poseæ¨¡å‹è·å¾—äº†57.1çš„APåˆ†æ•°ï¼Œç›¸æ¯”äºControlNetæŠ€æœ¯æé«˜äº†çº¦13%ã€‚<br>â€¢ é¡¹ç›®é“¾æ¥å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ai-med/StablePoseä¸Šè·å–ã€‚">https://github.com/ai-med/StablePoseä¸Šè·å–ã€‚</a></p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: Stable-Poseï¼šPose-Guided Text-to-Image Generationï¼ˆåŸºäºå§¿æ€çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆï¼‰</p></li><li><p>Authors: Jiajun Wang, Morteza Ghahremani, Yitong Li, BjÃ¶rn Ommer, Christian Wachinger</p></li><li><p>Affiliation: å¾·å›½æ…•å°¼é»‘å·¥ä¸šå¤§å­¦ï¼ˆTechnical University of Munichï¼‰</p></li><li><p>Keywords: Pose-Guided Text-to-Image Generation, Vision Transformer, Stable Diffusion</p></li><li><p>Urls: <a target="_blank" rel="noopener" href="https://github.com/ai-med/StablePose">https://github.com/ai-med/StablePose</a></p></li><li><p>Summary:</p><ul><li><p>(1):æœ¬æ–‡ç ”ç©¶çš„èƒŒæ™¯æ˜¯åŸºäºæ–‡æœ¬å’Œå§¿æ€ä¿¡æ¯ç”Ÿæˆé«˜è´¨é‡å›¾åƒçš„æŠ€æœ¯ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚</p></li><li><p>(2):è¿‡å»çš„æ–¹æ³•éœ€è¦æºå›¾åƒæ¥æŒ‡å¯¼ç”Ÿæˆçš„å›¾åƒé£æ ¼ï¼Œè€Œè¿™äº›æ–¹æ³•å¤æ‚å§¿æ€æ¡ä»¶ä¸‹ç”Ÿæˆå›¾åƒçš„æ€§èƒ½ä¸ä½³ã€‚ Stable-Pose æ¨¡å‹æ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ã€‚</p></li><li><p>(3):æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•æ˜¯å¼•å…¥ç³™åˆ°ç»†ç²’åº¦çš„æ³¨æ„åŠ›maskingç­–ç•¥åˆ°è§†è§‰Transformerï¼ˆViTï¼‰ä¸­ï¼Œä»¥è·å–å‡†ç¡®çš„å§¿æ€æŒ‡å¯¼ä¿¡æ¯ã€‚åŒæ—¶ï¼Œä½¿ç”¨maskedå§¿æ€å›¾åƒæ¥å¹³æ»‘åœ°ç»†åŒ–æ³¨æ„åŠ›å›¾ï¼Œæé«˜æ¨¡å‹å¯¹å§¿æ€ç»†èŠ‚çš„æ•æ‰èƒ½åŠ›ã€‚</p></li><li><p>(4):æœ¬æ–‡çš„æ–¹æ³•åœ¨äº”ä¸ªå…¬å…±æ•°æ®é›†ä¸‹çš„æ€§èƒ½è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒStable-Pose æ¨¡å‹åœ¨ LAION-Human æ•°æ®é›†ä¸Šå–å¾—äº† 57.1 çš„ AP åˆ†æ•°ï¼Œç›¸æ¯” ControlNet æ–¹æ³•æé«˜äº†çº¦ 13%ã€‚è¿™è¡¨æ˜ Stable-Pose æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„åƒï¼Œæ»¡è¶³åº”ç”¨éœ€æ±‚ã€‚</p></li></ul></li><li>æ–¹æ³•ï¼š</li></ol><ul><li><p>(1):å¼•å…¥åŸºäºæ–‡æœ¬å’Œå§¿æ€ä¿¡æ¯çš„Pose-Guided Text-to-Image Generationä»»åŠ¡ï¼Œæ—¨åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒã€‚</p></li><li><p>(2):æå‡ºä½¿ç”¨Vision Transformerï¼ˆViTï¼‰ä½œä¸ºåŸºç¡€æ¶æ„ï¼Œä»¥è·å–å‡†ç¡®çš„å§¿æ€æŒ‡å¯¼ä¿¡æ¯ã€‚</p></li><li><p>(3):å¼•å…¥ç³™åˆ°ç»†ç²’åº¦çš„æ³¨æ„åŠ›maskingç­–ç•¥åˆ°ViTä¸­ï¼Œä»¥è·å–å‡†ç¡®çš„å§¿æ€æŒ‡å¯¼ä¿¡æ¯ã€‚</p></li><li><p>(4):ä½¿ç”¨maskedå§¿æ€å›¾åƒæ¥å¹³æ»‘åœ°ç»†åŒ–æ³¨æ„åŠ›å›¾ï¼Œæé«˜æ¨¡å‹å¯¹å§¿æ€ç»†èŠ‚çš„æ•æ‰èƒ½åŠ›ã€‚</p></li><li><p>(5):é‡‡ç”¨Stable Diffusionæ–¹æ³•æ¥ç”Ÿæˆå›¾åƒï¼Œç¡®ä¿ç”Ÿæˆçš„å›¾åƒå…·æœ‰é«˜è´¨é‡å’ŒçœŸæ€§ã€‚</p></li><li><p>(6):ä½¿ç”¨äº”ä¸ªå…¬å…±æ•°æ®é›†ï¼ˆåŒ…æ‹¬LAION-Humanæ•°æ®é›†ï¼‰æ¥è¯„ä¼°Stable-Poseæ¨¡å‹çš„æ€§èƒ½ï¼Œç»“æœè¡¨æ˜è¯¥æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒï¼Œæ»¡è¶³åº”ç”¨éœ€æ±‚ã€‚</p></li></ul><ol><li>Conclusion:</li></ol><ul><li><p>(1):æœ¬æ–‡æå‡ºçš„Stable-Poseæ¨¡å‹åœ¨ Pose-Guided Text-to-Image Generationé¢†åŸŸå…·æœ‰é‡è¦æ„ä¹‰ï¼Œå®ƒå¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒï¼Œæ»¡è¶³åº”ç”¨éœ€æ±‚ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚</p></li><li><p>(2):åˆ›æ–°ç‚¹ï¼šStable-Poseæ¨¡å‹å¼•å…¥äº†åŸºäºæ–‡æœ¬å’Œå§¿æ€ä¿¡æ¯çš„Pose-Guided Text-to-Image Generationä»»åŠ¡ï¼Œå¹¶æå‡ºä½¿ç”¨Vision Transformerï¼ˆViTï¼‰ä½œä¸ºåŸºç¡€æ¶æ„ï¼Œå¼•å…¥ç³™åˆ°ç»†ç²’åº¦çš„æ³¨æ„åŠ›maskingç­–ç•¥æ¥è·å–å‡†ç¡®çš„å§¿æ€æŒ‡å¯¼ä¿¡æ¯ï¼›æ€§èƒ½ï¼šStable-Poseæ¨¡å‹åœ¨äº”ä¸ªå…¬å…±æ•°æ®é›†ä¸‹çš„æ€§è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œå®ƒèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒï¼Œæ»¡è¶³åº”ç”¨éœ€æ±‚ï¼Œå…·æœ‰é«˜ç”Ÿæˆé²æ£’æ€§ï¼›å·¥ä½œé‡ï¼šStable-Poseæ¨¡å‹çš„æ¨ç†æ—¶é—´ç•¥é•¿ï¼Œä¸»è¦æ˜¯ç”±äºViTä¸­çš„self-attentionæœºåˆ¶çš„é›†æˆï¼Œä½†å…¶è®¾è®¡å…è®¸å¯¹å„ç§å¤–éƒ¨æ¡ä»¶çš„ç›´æ¥é€‚åº”ï¼Œå…·æœ‰é«˜åº”ç”¨æ½œåŠ›ã€‚</p></li></ul><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/ff3baa25e7d57dc0d3d150bd122a1406241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/9310f7f5bf930aab6265d4b0a26ca10f241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/0715a783340ef4b332910748478d64d9241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/666669f9054ec22d9219286330d08a97241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/186ec1358a6ffa6ae4d3d38deeeebdc7241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/4fe5adfd9504bedddd0dbdbeb362124f241286257.jpg" align="middle"></details><h2 id="Learning-Image-Priors-through-Patch-based-Diffusion-Models-for-Solving-Inverse-Problems"><a href="#Learning-Image-Priors-through-Patch-based-Diffusion-Models-for-Solving-Inverse-Problems" class="headerlink" title="Learning Image Priors through Patch-based Diffusion Models for Solving   Inverse Problems"></a>Learning Image Priors through Patch-based Diffusion Models for Solving Inverse Problems</h2><p><strong>Authors:Jason Hu, Bowen Song, Xiaojian Xu, Liyue Shen, Jeffrey A. Fessler</strong></p><p>Diffusion models can learn strong image priors from underlying data distribution and use them to solve inverse problems, but the training process is computationally expensive and requires lots of data. Such bottlenecks prevent most existing works from being feasible for high-dimensional and high-resolution data such as 3D images. This paper proposes a method to learn an efficient data prior for the entire image by training diffusion models only on patches of images. Specifically, we propose a patch-based position-aware diffusion inverse solver, called PaDIS, where we obtain the score function of the whole image through scores of patches and their positional encoding and utilize this as the prior for solving inverse problems. First of all, we show that this diffusion model achieves an improved memory efficiency and data efficiency while still maintaining the capability to generate entire images via positional encoding. Additionally, the proposed PaDIS model is highly flexible and can be plugged in with different diffusion inverse solvers (DIS). We demonstrate that the proposed PaDIS approach enables solving various inverse problems in both natural and medical image domains, including CT reconstruction, deblurring, and superresolution, given only patch-based priors. Notably, PaDIS outperforms previous DIS methods trained on entire image priors in the case of limited training data, demonstrating the data efficiency of our proposed approach by learning patch-based prior.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.02462v1">PDF</a></p><p>sorry, æ‚¨çš„ipå·²ç”±äºè§¦å‘é˜²æ»¥ç”¨æ£€æµ‹è€Œè¢«å°ç¦,å¯èƒ½æ˜¯å› ä¸ºä½¿ç”¨äº‘æœåŠ¡å™¨å¦‚è…¾è®¯äº‘æˆ–è€…å›½å¤–ä»£ç†(vpn)è®¿é—®æœ¬ç½‘ç«™ï¼Œå¦‚æœä½¿ç”¨äº†vpnï¼Œå…³é—­vpnæˆ–ä»£ç†å³å¯ç»§ç»­ä½¿ç”¨,æœ¬æœåŠ¡ç½‘å€æ˜¯<a target="_blank" rel="noopener" href="https://chat18.aichatos8.com">https://chat18.aichatos8.com</a> å¦‚éœ€åˆä½œæ¥å£è°ƒç”¨è¯·è”ç³»å¾®ä¿¡chatkf123 æˆ–è€…å‰å¾€ <a target="_blank" rel="noopener" href="https://binjie09.shop">https://binjie09.shop</a> è‡ªåŠ©è´­ä¹°key, è®¤ä¸ºæ˜¯è¯¯å°éœ€è¦è§£å°çš„è¯·å‰å¾€<a target="_blank" rel="noopener" href="https://www.ip.cn/">https://www.ip.cn/</a> æŸ¥è¯¢ipä¿¡æ¯,å¹¶å‘é€ä¿¡æ¯è‡³é‚®ä»¶ gpt33@binjie.site ï¼Œç«™é•¿ä¼šå®šæœŸçœ‹é‚®ä»¶å¹¶å¤„ç†è§£å°å’Œåˆä½œé—®é¢˜ï¼Œå¦‚éœ€è°ƒç”¨æ¥å£è¯·è§æ¥å£æ–‡æ¡£<a target="_blank" rel="noopener" href="https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82ï¼Œå¦‚éœ€åˆä½œæ¥å£è°ƒç”¨è¯·è”ç³»å¾®ä¿¡chatkf123">https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82ï¼Œå¦‚éœ€åˆä½œæ¥å£è°ƒç”¨è¯·è”ç³»å¾®ä¿¡chatkf123</a> æˆ–è€…å‰å¾€ <a target="_blank" rel="noopener" href="https://cat.gptxyy.cn">https://cat.gptxyy.cn</a> æ³¨å†Œä½¿ç”¨ï¼ˆå¯ä»˜è´¹ä½¿ç”¨gpt4 æ³¨å†Œå¯å…è´¹ä½¿ç”¨3.5ï¼‰</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: é€šè¿‡åŸºäºpatchçš„æ‰©æ•£å­¦ä¹ å›¾åƒpriorï¼ˆLearning Image Priors through Patch-based Diffusionï¼‰</p></li><li><p>Authors: Jason Hu, Bowen Song, Xiaojian Xu, Liyue Shen, Jeffrey A. Fessler</p></li><li><p>Affiliation: å¯†æ­‡æ ¹å¤§å­¦ç”µå­ä¸è®¡ç®—æœºå·¥ç¨‹ç³»ï¼ˆUniversity of Michigan, Department of Electrical and Computer Engineeringï¼‰</p></li><li><p>Keywords: æ‰©æ•£æ¨¡å‹, å›¾åƒprior, é€†é—®é¢˜, Patch-based</p></li><li><p>Urls: arXiv:2406.02462v1 , Github:None</p></li><li><p>Summary:</p><ul><li><p>(1):ç°æœ‰çš„æ‰©æ•£æ¨¡å‹å¯ä»¥ä»æ•°æ®åˆ†å¸ƒä¸­å­¦ä¹ å¼ºçš„å›¾åƒpriorï¼Œå¹¶ç”¨äºè§£å†³é€†é—®é¢˜ï¼Œä½†æ˜¯è®­ç»ƒè¿‡ç¨‹è®¡ç®—æ˜‚è´µï¼Œéœ€è¦å¤§é‡æ•°æ®ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨é«˜ç»´å’Œé«˜åˆ†è¾¨ç‡æ•°æ®ï¼ˆå¦‚3Då›¾åƒï¼‰ä¸Šçš„åº”ç”¨ã€‚</p></li><li><p>(2):è¿‡å»çš„æ–¹æ³•éœ€è¦åœ¨æ•´ä¸ªå›¾åƒä¸Šè®­ç»ƒæ‰©æ¨¡å‹ï¼Œè¿™è¦æ±‚å¤§é‡è®¡ç®—èµ„æºå’Œæ•°æ®ï¼Œé™åˆ¶äº†å®ƒä»¬çš„å®ç”¨æ€§ã€‚è¿™äº›æ–¹æ³•æ²¡æœ‰å……åˆ†è€ƒè™‘å›¾åƒçš„patchç»“æ„å’Œä½ç½®ä¿¡æ¯ã€‚</p></li><li><p>(3):æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºpatchçš„æ‰©æ•£é€†é—®é¢˜æ±‚è§£å™¨ï¼ˆPaDISï¼‰ï¼Œé€šè¿‡è®­ç»ƒpatch-basedæ‰©æ•£æ¨¡å‹ï¼Œå­¦ä¹ æ•´ä¸ªå›¾åƒpriorï¼Œå¹¶ä½¿ç”¨ä½ç½®ç¼–ç æ¥ç”Ÿæˆæ•´ä¸ªå›¾åƒã€‚</p></li><li><p>(4):å®éªŒç»“æœè¡¨æ˜ï¼ŒPaDISæ¨¡å‹å¯ä»¥åœ¨ä¿æŒæˆæ•´ä¸ªå›¾åƒèƒ½åŠ›çš„åŒæ—¶ï¼Œæé«˜å†…å­˜æ•ˆç‡å’Œæ•°æ®æ•ˆç‡ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p></li></ul></li><li>æ–¹æ³•ï¼š</li></ol><ul><li><p>(1): å°†å›¾åƒxåˆ†å‰²æˆéé‡å çš„patchï¼Œå­¦ä¹ æ¯ä¸ªpatchçš„åˆ†å¸ƒï¼Œç„¶åå°†å®ƒä»¬ç»„åˆä»¥è·å¾—æ•´ä¸ªå›¾åƒçš„åˆ†å¸ƒã€‚</p></li><li><p>(2): ä¸ºäº†é¿å…patchä¹‹é—´çš„è¾¹ç•Œ-artifactï¼Œä½¿ç”¨é›¶å¡«å……å°†å›¾åƒxæ‰©å±•åˆ°æ›´å¤§çš„å°ºå¯¸ï¼Œç„¶åå°†åˆ†å‰²æˆå¤šä¸ªregionï¼Œæ¯ä¸ªregionåŒ…å«ä¸€ä¸ªä¸­å¿ƒpatchå’Œä¸€ä¸ªè¾¹ç•Œpatch</p></li><li><p>(3): å¯¹äºæ¯ä¸ªregionï¿½ï¿½å­¦ä¹ ä¸­å¿ƒpatchçš„åˆ†å¸ƒå’Œè¾¹ç•Œpatchçš„åˆ†å¸ƒï¼Œç„¶åå°†å®ƒä»¬ç»„åˆä»¥è·å¾—æ•´ä¸ªregionçš„åˆ†å¸ƒã€‚</p></li><li><p>(4): é€šè¿‡score matchingæ–¹æ³•è®­ç»ƒç¥ç»ç½‘ç»œï¼Œä»¥å­¦ä¹ patchçš„åˆ†å¸ƒã€‚</p></li><li><p>(5): ä½¿ç”¨UNetæ¶æ„çš„ç¥ç»ç½‘ç»œï¼Œå¯ä»¥å¤„ç†ä¸åŒå¤§å°çš„patchè¾“å…¥ã€‚</p></li><li><p>(6): åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œéšæœºé€‰æ‹©patchçš„å¤§å°å’Œä½ç½®ï¼Œä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</p></li><li><p>(7): åœ¨ sampling å’Œ reconstruction é˜¶æ®µï¼Œä½¿ç”¨ Langevin dynamics æˆ– DDPM ç­‰æ–¹æ³•ï¼Œä»¥ç”Ÿæˆæ•´ä¸ªå›¾åƒã€‚</p></li><li><p>(8): å¯¹äºé€†é—®é¢˜çš„æ±‚è§£ï¼Œä½¿ç”¨ Patch Diffusion Inverse Solver (PaDIS) æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥å°†æ•´ä¸ªå›¾åƒçš„scoreå‡½æ•°åˆ†è§£ä¸ºå¤šä¸ªpatchçš„scoreå‡½æ•°ï¼Œç„¶åä½¿ç”¨ Langevin dynamics æˆ– DDPM ç­‰æ–¹æ³•æ¥æ±‚è§£é€†é—®é¢˜ã€‚</p></li></ul><ol><li>Conclusion:</li></ol><ul><li><p>(1): æœ¬æ–‡å·¥ä½œçš„æ„ä¹‰åœ¨äºæå‡ºäº†ä¸€ç§åŸºäºpatchçš„æ‰©æ•£å­¦ä¹ å›¾åƒprioræ–¹æ³•ï¼Œè§£å†³äº†ä¼ ç»Ÿæ‰©æ•£æ¨¡å‹é«˜ç»´å’Œé«˜åˆ†è¾¨ç‡æ•°æ®ä¸Šçš„è®¡ç®—æ•ˆç‡å’Œæ•°æ®æ•ˆç‡é—®é¢˜ï¼Œä»è€Œæ‰©å±•äº†æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå¤„ç†é¢†åŸŸçš„åº”ç”¨èŒƒå›´ã€‚</p></li><li><p>(2): Innovation point: æœ¬æ–‡çš„åˆ›æ–°ç‚¹åœ¨äºå°†patch-basedæ‰©æ•£æ¨¡å‹åº”ç”¨äºå›¾åƒpriorå­¦ä¹ ï¼Œå……åˆ†è€ƒè™‘äº†å›¾åƒçš„patchç»“æ„å’Œä½ç½®ä¿¡æ¯ï¼› Performance: æœ¬æ–‡çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒPaDISæ¨¡å‹å¯ä»¥åœ¨ä¿æŒæˆæ•´ä¸ªå›¾åƒèƒ½åŠ›çš„åŒæ—¶ï¼Œæé«˜å†…å­˜æ•ˆç‡å’Œæ•°æ®æ•ˆç‡ï¼› Workload: æœ¬æ–‡çš„å·¥ä½œè´Ÿè½½ä¸»è¦é›†ä¸­åœ¨æ‰©æ•£æ¨¡å‹çš„è®¾è®¡å’Œå®ç°ä¸Šï¼Œéœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºå’Œæ•°æ®æ”¯æŒã€‚</p></li></ul><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/9c6dfd9d2a3b403856c0c71ca91e3c59241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/7827914460cd503e82f43b5c4841752f241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/780e08470b895237445fa4a5328c634d241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/252612531903a78b63f685326a39cf81241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/22fc4b3649f72d4778e80367f62cf3c0241286257.jpg" align="middle"></details><h2 id="RoomTex-Texturing-Compositional-Indoor-Scenes-via-Iterative-Inpainting"><a href="#RoomTex-Texturing-Compositional-Indoor-Scenes-via-Iterative-Inpainting" class="headerlink" title="RoomTex: Texturing Compositional Indoor Scenes via Iterative Inpainting"></a>RoomTex: Texturing Compositional Indoor Scenes via Iterative Inpainting</h2><p><strong>Authors:Qi Wang, Ruijie Lu, Xudong Xu, Jingbo Wang, Michael Yu Wang, Bo Dai, Gang Zeng, Dan Xu</strong></p><p>The advancement of diffusion models has pushed the boundary of text-to-3D object generation. While it is straightforward to composite objects into a scene with reasonable geometry, it is nontrivial to texture such a scene perfectly due to style inconsistency and occlusions between objects. To tackle these problems, we propose a coarse-to-fine 3D scene texturing framework, referred to as RoomTex, to generate high-fidelity and style-consistent textures for untextured compositional scene meshes. In the coarse stage, RoomTex first unwraps the scene mesh to a panoramic depth map and leverages ControlNet to generate a room panorama, which is regarded as the coarse reference to ensure the global texture consistency. In the fine stage, based on the panoramic image and perspective depth maps, RoomTex will refine and texture every single object in the room iteratively along a series of selected camera views, until this object is completely painted. Moreover, we propose to maintain superior alignment between RGB and depth spaces via subtle edge detection methods. Extensive experiments show our method is capable of generating high-quality and diverse room textures, and more importantly, supporting interactive fine-grained texture control and flexible scene editing thanks to our inpainting-based framework and compositional mesh input. Our project page is available at <a target="_blank" rel="noopener" href="https://qwang666.github.io/RoomTex/">https://qwang666.github.io/RoomTex/</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.02461v1">PDF</a></p><p><strong>Summary</strong><br>RoomTexï¼šä¸€ç§ coarse-to-fine çš„ 3D åœºæ™¯çº¹ç†ç”Ÿæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜ä¿çœŸå’Œé£æ ¼ä¸€è‡´çš„çº¹ç†ã€‚</p><p><strong>Key Takeaways</strong><br>â€¢ RoomTex æ¡†æ¶é€šè¿‡ coarse-to-fine ä¸¤é˜¶æ®µç”Ÿæˆé«˜è´¨é‡çš„ 3D åœºæ™¯çº¹ç†ã€‚<br>â€¢ coarse é˜¶æ®µç”Ÿæˆå…¨å±€ä¸€è‡´çš„ room panoramaï¼Œä½œä¸ºåç»­çº¹ç†ç”Ÿæˆçš„å‚è€ƒã€‚<br>â€¢ fine é˜¶æ®µè¿­ä»£åœ°å¯¹æ¯ä¸ªå¯¹è±¡è¿›è¡Œçº¹ç†ç”Ÿæˆå’Œç»†åŒ–ï¼Œç›´åˆ°å®Œå…¨ç»˜åˆ¶ã€‚<br>â€¢ RoomTex é€šè¿‡ç²¾ç»†çš„è¾¹ç¼˜æ£€æµ‹æ–¹æ³•ç»´æŠ¤ RGB å’Œæ·±åº¦ç©ºé—´ä¹‹é—´çš„å¯¹é½ã€‚<br>â€¢ è¯¥æ–¹æ³•æ”¯æŒäº¤äº’å¼çš„ç»†ç²’åº¦çº¹ç†æ§åˆ¶å’Œçµæ´»çš„åœºæ™¯ç¼–è¾‘ã€‚<br>â€¢ RoomTex èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡å’Œå¤šæ ·åŒ–çš„æˆ¿é—´çº¹ç†ã€‚<br>â€¢ é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://qwang666.github.io/RoomTex/ã€‚">https://qwang666.github.io/RoomTex/ã€‚</a></p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>Title: RoomTexï¼šåŸºäºè¿­ä»£ inpainting çš„ç»„åˆå®¤å†…åœºæ™¯çº¹ç†ç”Ÿæˆ (RoomTex: Texturing Compositional Indoor Scenes via Iterative Inpainting)</li></ol><ol><li>Authors: Qi Wang, Ruijie Lu, Xudong Xu, Jingbo Wang, Michael Yu Wang, Bo Dai, Gang Zeng, Dan Xu</li></ol><ol><li>Affiliation: é¦™æ¸¯ç§‘æŠ€å¤§å­¦</li></ol><ol><li>Keywords: Scene Texturing Â· Scene Generation Â· Texture Synthesis</li></ol><ol><li>Urls: <a target="_blank" rel="noopener" href="https://qwang666.github.io/RoomTex/">https://qwang666.github.io/RoomTex/</a> , Github:None</li></ol><ol><li>Summary:</li></ol><pre><code>- (1):ç ”ç©¶èƒŒæ™¯æ˜¯ç”Ÿæˆé«˜è´¨é‡çº¹ç†çš„3Då®¤å†…åœºæ™¯æ¨¡å‹ï¼Œå¯¹äºæ¸¸æˆã€ç”µå½±å’ŒAR/VRç­‰å·¥ä¸šåº”ç”¨éå¸¸é‡è¦ã€‚


- (2):è¿‡å»çš„æ–¹æ³•å¯ä»¥ç”Ÿæˆ3Då¯¹è±¡ï¼Œä½†æ˜¯è¿™äº›å¯¹è±¡çš„çº¹ç†é£æ ¼ä¸ä¸€è‡´ï¼Œä¸”å¯¹è±¡ä¹‹é—´å­˜åœ¨é®æŒ¡é—®é¢˜ã€‚è¿™äº›æ–¹æ³•ä¸èƒ½ç”Ÿæˆé«˜è´¨é‡çš„çº¹ç†ã€‚


- (3):æœ¬æ–‡æå‡ºäº†ä¸€ä¸ª coarse-to-fine çš„3Dåœºæ™¯çº¹ç†ç”Ÿæˆæ¡†æ¶RoomTexï¼Œé¦–å…ˆå°†åœºæ™¯mesh unwrapåˆ°å…¨æ™¯æ·±åº¦å›¾ï¼Œç„¶åä½¿ç”¨ControlNetç”Ÿæˆæˆ¿é—´å…¨æ™¯å›¾ï¼Œæœ€ååŸºäºå…¨æ™¯å›¾å’Œè§†è§’æ·±åº¦å›¾ï¼Œå¯¹æ¯ä¸ªå¯¹è±¡è¿›è¡Œè¿­ä»£ inpaintingï¼Œç›´åˆ°å¯¹è±¡å®Œå…¨è¢«ç»˜åˆ¶ã€‚


- (4):æœ¬æ–‡çš„æ–¹æ³•å¯ä»¥ç”Ÿæˆé«˜è´¨é‡å’Œå¤šæ ·åŒ–çš„æˆ¿é—´çº¹ç†ï¼Œå¹¶ä¸”æ”¯æŒäº¤äº’å¼çš„ç»†ç²’åº¦çº¹ç†æ§åˆ¶å’Œçµæ´»çš„åœºæ™¯ç¼–è¾‘ã€‚
</code></pre><p>sorry, æ‚¨çš„ipå·²ç”±äºè§¦å‘é˜²æ»¥ç”¨æ£€æµ‹è€Œè¢«å°ç¦,å¯èƒ½æ˜¯å› ä¸ºä½¿ç”¨äº‘æœåŠ¡å™¨å¦‚è…¾è®¯äº‘æˆ–è€…å›½å¤–ä»£ç†(vpn)è®¿é—®æœ¬ç½‘ç«™ï¼Œå¦‚æœä½¿ç”¨äº†vpnï¼Œå…³é—­vpnæˆ–ä»£ç†å³å¯ç»§ç»­ä½¿ç”¨,æœ¬æœåŠ¡ç½‘å€æ˜¯<a target="_blank" rel="noopener" href="https://chat18.aichatos8.com">https://chat18.aichatos8.com</a> å¦‚éœ€åˆä½œæ¥å£è°ƒç”¨è¯·è”ç³»å¾®ä¿¡chatkf123 æˆ–è€…å‰å¾€ <a target="_blank" rel="noopener" href="https://binjie09.shop">https://binjie09.shop</a> è‡ªåŠ©è´­ä¹°key, è®¤ä¸ºæ˜¯è¯¯å°éœ€è¦è§£å°çš„è¯·å‰å¾€<a target="_blank" rel="noopener" href="https://www.ip.cn/">https://www.ip.cn/</a> æŸ¥è¯¢ipä¿¡æ¯,å¹¶å‘é€ä¿¡æ¯è‡³é‚®ä»¶ gpt33@binjie.site ï¼Œç«™é•¿ä¼šå®šæœŸçœ‹é‚®ä»¶å¹¶å¤„ç†è§£å°å’Œåˆä½œé—®é¢˜ï¼Œå¦‚éœ€è°ƒç”¨æ¥å£è¯·è§æ¥å£æ–‡æ¡£<a target="_blank" rel="noopener" href="https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82ï¼Œå¦‚éœ€åˆä½œæ¥å£è°ƒç”¨è¯·è”ç³»å¾®ä¿¡chatkf123">https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82ï¼Œå¦‚éœ€åˆä½œæ¥å£è°ƒç”¨è¯·è”ç³»å¾®ä¿¡chatkf123</a> æˆ–è€…å‰å¾€ <a target="_blank" rel="noopener" href="https://cat.gptxyy.cn">https://cat.gptxyy.cn</a> æ³¨å†Œä½¿ç”¨ï¼ˆå¯ä»˜è´¹ä½¿ç”¨gpt4 æ³¨å†Œå¯å…è´¹ä½¿ç”¨3.5ï¼‰</p><p>sorry, æ‚¨çš„ipå·²ç”±äºè§¦å‘é˜²æ»¥ç”¨æ£€æµ‹è€Œè¢«å°ç¦,å¯èƒ½æ˜¯å› ä¸ºä½¿ç”¨äº‘æœåŠ¡å™¨å¦‚è…¾è®¯äº‘æˆ–è€…å›½å¤–ä»£ç†(vpn)è®¿é—®æœ¬ç½‘ç«™ï¼Œå¦‚æœä½¿ç”¨äº†vpnï¼Œå…³é—­vpnæˆ–ä»£ç†å³å¯ç»§ç»­ä½¿ç”¨,æœ¬æœåŠ¡ç½‘å€æ˜¯<a target="_blank" rel="noopener" href="https://chat18.aichatos8.com">https://chat18.aichatos8.com</a> å¦‚éœ€åˆä½œæ¥å£è°ƒç”¨è¯·è”ç³»å¾®ä¿¡chatkf123 æˆ–è€…å‰å¾€ <a target="_blank" rel="noopener" href="https://binjie09.shop">https://binjie09.shop</a> è‡ªåŠ©è´­ä¹°key, è®¤ä¸ºæ˜¯è¯¯å°éœ€è¦è§£å°çš„è¯·å‰å¾€<a target="_blank" rel="noopener" href="https://www.ip.cn/">https://www.ip.cn/</a> æŸ¥è¯¢ipä¿¡æ¯,å¹¶å‘é€ä¿¡æ¯è‡³é‚®ä»¶ gpt33@binjie.site ï¼Œç«™é•¿ä¼šå®šæœŸçœ‹é‚®ä»¶å¹¶å¤„ç†è§£å°å’Œåˆä½œé—®é¢˜ï¼Œå¦‚éœ€è°ƒç”¨æ¥å£è¯·è§æ¥å£æ–‡æ¡£<a target="_blank" rel="noopener" href="https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82ï¼Œå¦‚éœ€åˆä½œæ¥å£è°ƒç”¨è¯·è”ç³»å¾®ä¿¡chatkf123">https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82ï¼Œå¦‚éœ€åˆä½œæ¥å£è°ƒç”¨è¯·è”ç³»å¾®ä¿¡chatkf123</a> æˆ–è€…å‰å¾€ <a target="_blank" rel="noopener" href="https://cat.gptxyy.cn">https://cat.gptxyy.cn</a> æ³¨å†Œä½¿ç”¨ï¼ˆå¯ä»˜è´¹ä½¿ç”¨gpt4 æ³¨å†Œå¯å…è´¹ä½¿ç”¨3.5ï¼‰</p><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/e596a578ebc0d3ac4051ac1db16c06e5241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/421ef6ff4d11d1376ed152b170e96685241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/8355176fb1f4e6929e3df8cc7fd0027f241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/8298da4924c73a41dbe8947ac953ec05241286257.jpg" align="middle"></details><h2 id="Flash-Diffusion-Accelerating-Any-Conditional-Diffusion-Model-for-Few-Steps-Image-Generation"><a href="#Flash-Diffusion-Accelerating-Any-Conditional-Diffusion-Model-for-Few-Steps-Image-Generation" class="headerlink" title="Flash Diffusion: Accelerating Any Conditional Diffusion Model for Few   Steps Image Generation"></a>Flash Diffusion: Accelerating Any Conditional Diffusion Model for Few Steps Image Generation</h2><p><strong>Authors:Clement Chadebec, Onur Tasar, Eyal Benaroche, Benjamin Aubin</strong></p><p>In this paper, we propose an efficient, fast, and versatile distillation method to accelerate the generation of pre-trained diffusion models: Flash Diffusion. The method reaches state-of-the-art performances in terms of FID and CLIP-Score for few steps image generation on the COCO2014 and COCO2017 datasets, while requiring only several GPU hours of training and fewer trainable parameters than existing methods. In addition to its efficiency, the versatility of the method is also exposed across several tasks such as text-to-image, inpainting, face-swapping, super-resolution and using different backbones such as UNet-based denoisers (SD1.5, SDXL) or DiT (Pixart-$\alpha$), as well as adapters. In all cases, the method allowed to reduce drastically the number of sampling steps while maintaining very high-quality image generation. The official implementation is available at <a target="_blank" rel="noopener" href="https://github.com/gojasper/flash-diffusion">https://github.com/gojasper/flash-diffusion</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.02347v1">PDF</a> 16 pages + 16 pages appendices</p><p>sorry, æ‚¨çš„ipå·²ç”±äºè§¦å‘é˜²æ»¥ç”¨æ£€æµ‹è€Œè¢«å°ç¦,å¯èƒ½æ˜¯å› ä¸ºä½¿ç”¨äº‘æœåŠ¡å™¨å¦‚è…¾è®¯äº‘æˆ–è€…å›½å¤–ä»£ç†(vpn)è®¿é—®æœ¬ç½‘ç«™ï¼Œå¦‚æœä½¿ç”¨äº†vpnï¼Œå…³é—­vpnæˆ–ä»£ç†å³å¯ç»§ç»­ä½¿ç”¨,æœ¬æœåŠ¡ç½‘å€æ˜¯<a target="_blank" rel="noopener" href="https://chat18.aichatos8.com">https://chat18.aichatos8.com</a> å¦‚éœ€åˆä½œæ¥å£è°ƒç”¨è¯·è”ç³»å¾®ä¿¡chatkf123 æˆ–è€…å‰å¾€ <a target="_blank" rel="noopener" href="https://binjie09.shop">https://binjie09.shop</a> è‡ªåŠ©è´­ä¹°key, è®¤ä¸ºæ˜¯è¯¯å°éœ€è¦è§£å°çš„è¯·å‰å¾€<a target="_blank" rel="noopener" href="https://www.ip.cn/">https://www.ip.cn/</a> æŸ¥è¯¢ipä¿¡æ¯,å¹¶å‘é€ä¿¡æ¯è‡³é‚®ä»¶ gpt33@binjie.site ï¼Œç«™é•¿ä¼šå®šæœŸçœ‹é‚®ä»¶å¹¶å¤„ç†è§£å°å’Œåˆä½œé—®é¢˜ï¼Œå¦‚éœ€è°ƒç”¨æ¥å£è¯·è§æ¥å£æ–‡æ¡£<a target="_blank" rel="noopener" href="https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82ï¼Œå¦‚éœ€åˆä½œæ¥å£è°ƒç”¨è¯·è”ç³»å¾®ä¿¡chatkf123">https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82ï¼Œå¦‚éœ€åˆä½œæ¥å£è°ƒç”¨è¯·è”ç³»å¾®ä¿¡chatkf123</a> æˆ–è€…å‰å¾€ <a target="_blank" rel="noopener" href="https://cat.gptxyy.cn">https://cat.gptxyy.cn</a> æ³¨å†Œä½¿ç”¨ï¼ˆå¯ä»˜è´¹ä½¿ç”¨gpt4 æ³¨å†Œå¯å…è´¹ä½¿ç”¨3.5ï¼‰</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: Flash Diffusion: Accelerating Any Conditional Diffusion Model for Few Steps Image Generation (é—ªå…‰æ‰©æ•£ï¼šåŠ é€Ÿä»»ä½•æ¡ä»¶æ‰©æ•£æ¨¡å‹çš„å°‘æ­¥å›¾åƒç”Ÿæˆ)</p></li><li><p>Authors: Clement Chadebec, Onur Tasar, Eyal Benaroche, Benjamin Aubin</p></li><li><p>Affiliation:.jasper Research (.jasperç ”ç©¶)</p></li><li><p>Keywords: diffusion model, few steps image generation, Flash Diffusion</p></li><li><p>Urls: arXiv:2406.02347v1, Github: <a target="_blank" rel="noopener" href="https://github.com/gojasper/flash-diffusion">https://github.com/gojasper/flash-diffusion</a></p></li><li><p>Summary:</p></li></ol><ul><li><p>(1):è¿™ç¯‡è®ºæ–‡çš„ç ”ç©¶èƒŒæ™¯æ˜¯ diffusion æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯ few steps å›¾åƒç”Ÿæˆä»»åŠ¡ï¼Œ existing æ–¹æ³•éœ€è¦å¤§é‡çš„é‡‡æ ·æ­¥éª¤å’Œè®¡ç®—èµ„æºã€‚</p></li><li><p>(2):è¿‡å»çš„æ–¹æ³•éœ€è¦å¤§é‡çš„é‡‡æ ·æ­¥éª¤å’Œè®¡ç®—èµ„æºï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨å®é™…åº”ç”¨ä¸­çš„æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚ Flash Diffusion æ–¹æ³•çš„æå‡ºæ˜¯ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæé«˜ diffusion æ¨¡å‹çš„ç”Ÿæˆæ•ˆç‡å’Œè´¨é‡ã€‚</p></li><li><p>(3):è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸º Flash Diffusion çš„ distillation æ–¹æ³•ï¼Œæ—¨åœ¨åŠ é€Ÿé¢„è®­ç»ƒçš„ diffusion æ¨¡å‹çš„ç”Ÿæˆé€Ÿåº¦ã€‚è¯¥æ–¹æ³•å¯ä»¥åœ¨å°‘æ­¥å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­è¾¾åˆ° state-of-the-art çš„æ€§èƒ½ï¼ŒåŒæ—¶éœ€è¦çš„è®¡ç®—èµ„æºå’Œé‡‡æ ·æ­¥éª¤éƒ½è¾ƒå°‘ã€‚</p></li><li><p>(4):è¯¥æ–¹æ³•åœ¨ COCO2014 å’Œ COCO2017 æ•°æ®é›†ä¸Šçš„ few steps å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­è¾¾åˆ°äº† state-of-the-art çš„æ€§èƒ½ï¼ŒFID å’Œ CLIP-Score éƒ½å–å¾—äº†å¾ˆé«˜çš„åˆ†æ•°ï¼ŒåŒæ—¶åªéœ€è¦å‡ å°æ—¶çš„ GPU è®­ç»ƒæ—¶é—´å’Œè¾ƒå°‘çš„å¯è®­ç»ƒå‚æ•°ã€‚<br>sorry, æ‚¨çš„ipå·²ç”±äºè§¦å‘é˜²æ»¥ç”¨æ£€æµ‹è€Œè¢«å°ç¦,å¯èƒ½æ˜¯å› ä¸ºä½¿ç”¨äº‘æœåŠ¡å™¨å¦‚è…¾è®¯äº‘æˆ–è€…å›½å¤–ä»£ç†(vpn)è®¿é—®æœ¬ç½‘ç«™ï¼Œå¦‚æœä½¿ç”¨äº†vpnï¼Œå…³é—­vpnæˆ–ä»£ç†å³å¯ç»§ç»­ä½¿ç”¨,æœ¬æœåŠ¡ç½‘å€æ˜¯<a target="_blank" rel="noopener" href="https://chat18.aichatos8.com">https://chat18.aichatos8.com</a> å¦‚éœ€åˆä½œæ¥å£è°ƒç”¨è¯·è”ç³»å¾®ä¿¡chatkf123 æˆ–è€…å‰å¾€ <a target="_blank" rel="noopener" href="https://binjie09.shop">https://binjie09.shop</a> è‡ªåŠ©è´­ä¹°key, è®¤ä¸ºæ˜¯è¯¯å°éœ€è¦è§£å°çš„è¯·å‰å¾€<a target="_blank" rel="noopener" href="https://www.ip.cn/">https://www.ip.cn/</a> æŸ¥è¯¢ipä¿¡æ¯,å¹¶å‘é€ä¿¡æ¯è‡³é‚®ä»¶ gpt33@binjie.site ï¼Œç«™é•¿ä¼šå®šæœŸçœ‹é‚®ä»¶å¹¶å¤„ç†è§£å°å’Œåˆä½œé—®é¢˜ï¼Œå¦‚éœ€è°ƒç”¨æ¥å£è¯·è§æ¥å£æ–‡æ¡£<a target="_blank" rel="noopener" href="https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82ï¼Œå¦‚éœ€åˆä½œæ¥å£è°ƒç”¨è¯·è”ç³»å¾®ä¿¡chatkf123">https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82ï¼Œå¦‚éœ€åˆä½œæ¥å£è°ƒç”¨è¯·è”ç³»å¾®ä¿¡chatkf123</a> æˆ–è€…å‰å¾€ <a target="_blank" rel="noopener" href="https://cat.gptxyy.cn">https://cat.gptxyy.cn</a> æ³¨å†Œä½¿ç”¨ï¼ˆå¯ä»˜è´¹ä½¿ç”¨gpt4 æ³¨å†Œå¯å…è´¹ä½¿ç”¨3.5ï¼‰</p></li></ul><ol><li>Conclusion:</li></ol><ul><li><p>(1):æœ¬æ–‡çš„ç ”ç©¶å·¥ä½œå¯¹äºdiffusionæ¨¡å‹åœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­çš„åº”ç”¨å…·æœ‰é‡è¦æ„ä¹‰ï¼Œèƒ½å¤Ÿæå¤§åœ°æé«˜å›¾åƒç”Ÿæˆçš„æ•ˆç‡å’Œè´¨é‡ã€‚</p></li><li><p>(2):åˆ›æ–°ç‚¹ï¼šæå‡ºäº†Flash Diffusionæ–¹æ³•ï¼Œèƒ½å¤ŸåŠ é€Ÿé¢„è®­ç»ƒçš„diffusionæ¨¡å‹çš„ç”Ÿæˆé€Ÿåº¦ï¼›æ€§èƒ½ï¼šåœ¨COCO2014å’ŒCOCO2017æ•°æ®é›†ä¸Šçš„few stepså›¾åƒç”Ÿæˆä»»åŠ¡ä¸­è¾¾åˆ°äº†state-of-the-artçš„æ€§èƒ½ï¼›å·¥ä½œè´Ÿè½½ï¼šéœ€è¦çš„è®¡ç®—èµ„æºå’Œé‡‡æ ·æ­¥éª¤éƒ½è¾ƒå°‘ï¼Œå¯ä»¥åœ¨çŸ­æ—¶é—´å†…å®Œæˆè®­ç»ƒã€‚</p></li></ul><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/e7563b1fa7c8536ad51a2c69f80de705241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/018b45169d7cd7b816f3b4572b5d2d6d241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/968d37ccfe6674d2a2d12755024b75db241286257.jpg" align="middle"></details><h2 id="A-Survey-of-Transformer-Enabled-Time-Series-Synthesis"><a href="#A-Survey-of-Transformer-Enabled-Time-Series-Synthesis" class="headerlink" title="A Survey of Transformer Enabled Time Series Synthesis"></a>A Survey of Transformer Enabled Time Series Synthesis</h2><p><strong>Authors:Alexander Sommers, Logan Cummins, Sudip Mittal, Shahram Rahimi, Maria Seale, Joseph Jaboure, Thomas Arnold</strong></p><p>Generative AI has received much attention in the image and language domains, with the transformer neural network continuing to dominate the state of the art. Application of these models to time series generation is less explored, however, and is of great utility to machine learning, privacy preservation, and explainability research. The present survey identifies this gap at the intersection of the transformer, generative AI, and time series data, and reviews works in this sparsely populated subdomain. The reviewed works show great variety in approach, and have not yet converged on a conclusive answer to the problems the domain poses. GANs, diffusion models, state space models, and autoencoders were all encountered alongside or surrounding the transformers which originally motivated the survey. While too open a domain to offer conclusive insights, the works surveyed are quite suggestive, and several recommendations for best practice, and suggestions of valuable future work, are provided.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.02322v1">PDF</a></p><p>sorry, æ‚¨çš„ipå·²ç”±äºè§¦å‘é˜²æ»¥ç”¨æ£€æµ‹è€Œè¢«å°ç¦,å¯èƒ½æ˜¯å› ä¸ºä½¿ç”¨äº‘æœåŠ¡å™¨å¦‚è…¾è®¯äº‘æˆ–è€…å›½å¤–ä»£ç†(vpn)è®¿é—®æœ¬ç½‘ç«™ï¼Œå¦‚æœä½¿ç”¨äº†vpnï¼Œå…³é—­vpnæˆ–ä»£ç†å³å¯ç»§ç»­ä½¿ç”¨,æœ¬æœåŠ¡ç½‘å€æ˜¯<a target="_blank" rel="noopener" href="https://chat18.aichatos8.com">https://chat18.aichatos8.com</a> å¦‚éœ€åˆä½œæ¥å£è°ƒç”¨è¯·è”ç³»å¾®ä¿¡chatkf123 æˆ–è€…å‰å¾€ <a target="_blank" rel="noopener" href="https://binjie09.shop">https://binjie09.shop</a> è‡ªåŠ©è´­ä¹°key, è®¤ä¸ºæ˜¯è¯¯å°éœ€è¦è§£å°çš„è¯·å‰å¾€<a target="_blank" rel="noopener" href="https://www.ip.cn/">https://www.ip.cn/</a> æŸ¥è¯¢ipä¿¡æ¯,å¹¶å‘é€ä¿¡æ¯è‡³é‚®ä»¶ gpt33@binjie.site ï¼Œç«™é•¿ä¼šå®šæœŸçœ‹é‚®ä»¶å¹¶å¤„ç†è§£å°å’Œåˆä½œé—®é¢˜ï¼Œå¦‚éœ€è°ƒç”¨æ¥å£è¯·è§æ¥å£æ–‡æ¡£<a target="_blank" rel="noopener" href="https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82ï¼Œå¦‚éœ€åˆä½œæ¥å£è°ƒç”¨è¯·è”ç³»å¾®ä¿¡chatkf123">https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82ï¼Œå¦‚éœ€åˆä½œæ¥å£è°ƒç”¨è¯·è”ç³»å¾®ä¿¡chatkf123</a> æˆ–è€…å‰å¾€ <a target="_blank" rel="noopener" href="https://cat.gptxyy.cn">https://cat.gptxyy.cn</a> æ³¨å†Œä½¿ç”¨ï¼ˆå¯ä»˜è´¹ä½¿ç”¨gpt4 æ³¨å†Œå¯å…è´¹ä½¿ç”¨3.5ï¼‰</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>Title: æ—¶é—´åºåˆ—åˆæˆTransformerç»¼è¿° (A Survey of Transformer Enabled Time Series)</li></ol><ol><li>Authors: Alexander Sommers, Logan Cummins, Sudip Mittal, Shahram Rahimi, Maria Seale, Joseph Jaboure, Thomas Arnold</li></ol><ol><li>Affiliation: å¯†è¥¿è¥¿æ¯”å·ç«‹å¤§å­¦è®¡ç®—æœºç§‘å­¦ä¸å·¥ç¨‹å­¦é™¢</li></ol><ol><li>Keywords: æ—¶é—´åºåˆ—åˆæˆ, Transformer, æ•°æ®å¢å¼º,ç»¼è¿°</li></ol><ol><li>Urls: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02322v1">https://arxiv.org/abs/2406.02322v1</a> , Github:None</li></ol><ol><li>Summary:</li></ol><pre><code>- (1):è¯¥è®ºæ–‡çš„ç ”ç©¶èƒŒæ™¯æ˜¯ç”Ÿæˆå¼AIåœ¨å›¾åƒå’Œè¯­è¨€é¢†åŸŸå–å¾—äº†å¾ˆå¤§è¿›å±•ï¼Œè€Œåœ¨æ—¶é—´åºåˆ—ç”Ÿæˆé¢†åŸŸçš„åº”ç”¨è¾ƒå°‘ï¼ŒTransformerç»ç½‘ç»œåœ¨æ—¶é—´åºåˆ—ç”Ÿæˆé¢†åŸŸçš„åº”ç”¨å°¤å…¶ä¸è¶³ã€‚

- (2):è¿‡å»çš„æ–¹æ³•åŒ…æ‹¬GANã€æ‰©æ•£æ¨¡å‹ã€çŠ¶æ€ç©ºé—´æ¨¡å‹å’Œè‡ªåŠ¨ç¼–ç å™¨ç­‰ï¼Œä½†æ˜¯è¿™äº›æ–¹æ³•è¿˜æ²¡æœ‰åœ¨æ—¶é—´åºåˆ—ç”Ÿæˆé¢†åŸŸå–å¾—æ˜ç¡®çš„ç»“è®ºã€‚

- (3):æœ¬è®ºæ–‡å¯¹Transformerç¥ç»ç½‘ç»œåœ¨æ—¶é—´åºåˆ—ç”Ÿæˆé¢†åŸŸçš„åº”ç”¨è¿›è¡Œäº†ç»¼è¿°ï¼Œå¹¶æ€»ç»“äº†åäºŒç¯‡ç›¸å…³è®ºæ–‡ã€‚

- (4):è¯¥è®ºæ–‡çš„æ–¹æ³•å¯ä»¥ç”¨äºæ—¶é—´åºåˆ—ç”Ÿæˆã€æ•°æ®å¢å¼ºã€éšç§ä¿æŠ¤å’Œæ¨¡è§£é‡Šç­‰é¢†åŸŸï¼Œä½†ç”±äºæ—¶é—´åºåˆ—ç”Ÿæˆé¢†åŸŸè¿˜æ²¡æœ‰æ˜ç¡®çš„ç»“è®ºï¼Œè¯¥è®ºæ–‡çš„æ€§èƒ½è¿˜éœ€è¦è¿›ä¸€æ­¥éªŒè¯ã€‚
</code></pre><ol><li>æ–¹æ³•ï¼š</li></ol><ul><li><p>(1)ï¼šè¯¥è®ºæ–‡å¯¹Transformerç¥ç»ç½‘ç»œåœ¨æ—¶é—´åºåˆ—ç”Ÿæˆé¢†åŸŸçš„åº”ç”¨è¿›è¡Œäº†ç»¼è¿°ï¼Œæ€»ç»“äº†åäºŒç¯‡ç›¸å…³è®ºæ–‡ã€‚</p></li><li><p>(2)ï¼šè®ºæ–‡é¦–å…ˆå¯¹é—´åºåˆ—ç”Ÿæˆé¢†åŸŸçš„èƒŒæ™¯å’ŒæŒ‘æˆ˜è¿›è¡Œäº†ä»‹ç»ï¼Œç„¶åå¯¹Transformerç¥ç»ç½‘ç»œçš„åŸºæœ¬ç»“æ„å’Œç‰¹ç‚¹è¿›è¡Œäº†æè¿°ã€‚</p></li><li><p>(3)ï¼šç„¶åï¼Œè®ºæ–‡å¯¹åäºŒç¯‡ç›¸å…³è®ºæ–‡è¿›è¡Œäº†åˆ†å’Œæ€»ç»“ï¼ŒåŒ…æ‹¬åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„æ–¹æ³•ã€åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–¹æ³•ã€åŸºäºçŠ¶æ€ç©ºé—´æ¨¡å‹çš„æ–¹æ³•å’ŒåŸºäºè‡ªåŠ¨ç¼–ç å™¨çš„æ–¹æ³•ç­‰ã€‚</p></li><li><p>(4)ï¼šè®ºæ–‡å¯¹æ¯ç§æ–¹æ³•çš„ä¼˜ç¼ºç‚¹å’Œåº”ç”¨åœºæ™¯è¿›è¡Œäº†åˆ†æï¼Œå¹¶å¯¹å…¶åœ¨æ—¶é—´åºåˆ—ç”Ÿæˆé¢†åŸŸçš„åº”ç”¨å‰æ™¯è¿›è¡Œäº†è®¨è®ºã€‚</p></li><li><p>(5)ï¼šæœ€åï¼Œè®ºæ–‡å¯¹Transformerç¥ç»ç½‘ç»œåœ¨æ—¶é—´åºåˆ—ç”Ÿæˆé¢†åŸŸçš„åº”ç”¨å‰æ™¯å’ŒæŒ‘æˆ˜è¿›è¡Œäº†æ€»ç»“ï¼Œå¹¶æå‡ºäº†æœªæ¥ç ”ç©¶æ–¹å‘ã€‚</p></li><li><p>(6)ï¼šè®ºæ–‡è¿˜å¯¹æ—¶é—´åºåˆ—ç”Ÿæˆé¢†åŸŸçš„è¯„ä¼°æŒ‡æ ‡å’Œè¯„ä¼°æ–¹æ³•è¿›è¡Œäº†è®¨è®ºï¼Œå¹¶å¯¹å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„é‡è¦æ€§è¿›è¡Œäº†è°ƒã€‚</p></li><li><p>(7)ï¼šè®ºæ–‡çš„æ–¹æ³•å¯ä»¥ç”¨äºæ—¶é—´åºåˆ—ç”Ÿæˆã€æ•°æ®å¢å¼ºã€éšç§ä¿æŠ¤å’Œæ¨¡è§£é‡Šç­‰é¢†åŸŸï¼Œä½†ç”±äºæ—¶é—´åºåˆ—ç”Ÿæˆé¢†åŸŸè¿˜æ²¡æœ‰æ˜ç¡®çš„ç»“è®ºï¼Œè¯¥è®ºæ–‡çš„æ€§èƒ½è¿˜éœ€è¦è¿›ä¸€æ­¥éªŒè¯ã€‚</p></li><li><p>(8)ï¼šè®ºæ–‡çš„æ–¹æ³•å¯ä»¥ä¸å…¶ä»–æœºå™¨å­¦ä¹ æ–¹æ³•ç»“åˆä½¿ç”¨ï¼Œä»¥æé«˜æ—¶é—´åºåˆ—ç”Ÿæˆçš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</p></li><li><p>(9)ï¼šè®ºæ–‡çš„ç»“æœå¯ä»¥ç”¨äºæŒ‡å¯¼å®é™…åº”ç”¨ä¸­çš„æ—¶é—´åºåˆ—ç”Ÿæˆä»»åŠ¡ï¼Œå¹¶å¯¹ç›¸å…³é¢†åŸŸçš„ç ”ç©¶å’Œåº”ç”¨äº§ç”Ÿå½±å“ã€‚</p></li></ul><ol><li>Conclusion:</li></ol><ul><li>(1):è¯¥è®ºæ–‡å¯¹Transformerç¥ç»ç½‘ç»œåœ¨æ—¶é—´åºåˆ—ç”Ÿæˆé¢†åŸŸçš„åº”ç”¨è¿›è¡Œäº†ç»¼è¿°ï¼Œæ€»ç»“äº†åäºŒç¯‡ç›¸å…³è®ºæ–‡ï¼Œä¸ºæ—¶é—´åºåˆ—ç”Ÿæˆé¢†åŸŸçš„ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒå’ŒæŒ‡å¯¼æ„ä¹‰ã€‚</li></ul><ul><li>(2):Innovation point: æœ¬è®ºæ–‡å¯¹Transformerç¥ç»ç½‘ç»œåœ¨æ—¶é—´åºåˆ—ç”Ÿæˆé¢†åŸŸçš„åº”ç”¨è¿›è¡Œäº†ç³»ç»Ÿçš„ç»¼è¿°ï¼Œæ€»ç»“äº†åäºŒç¯‡ç›¸å…³è®ºæ–‡ï¼Œå¯¹æ—¶é—´åºåˆ—ç”Ÿæˆé¢†åŸŸçš„ç ”ç©¶æä¾›äº†æ–°çš„è§†è§’å’Œæ€è·¯ï¼›Performance: æœ¬è®ºæ–‡å¯¹æ—¶é—´åºåˆ—ç”Ÿæˆé¢†åŸŸçš„è¯„ä¼°æŒ‡æ ‡å’Œè¯„ä¼°æ–¹æ³•è¿›è¡Œäº†è®¨è®ºï¼Œä¸ºæ—¶é—´åºåˆ—ç”Ÿæˆé¢†åŸŸçš„ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒå’ŒæŒ‡å¯¼æ„ä¹‰ï¼›Workload: æœ¬è®ºæ–‡å¯¹Transformerç¥ç»ç½‘ç»œåœ¨æ—¶é—´åºåˆ—ç”Ÿæˆé¢†åŸŸçš„åº”ç”¨å‰æ™¯å’ŒæŒ‘æˆ˜è¿›è¡Œäº†æ€»ç»“ï¼Œä¸ºæ—¶é—´åºåˆ—ç”Ÿæˆé¢†åŸŸçš„ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒå’ŒæŒ‡å¯¼æ„ä¹‰ã€‚</li></ul><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/a0c55b0527377194a053702dd3225eca241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/60bf5af7b9eccbcc3f9a4ba81f4f908d241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/cdd32cd34afcef5f08ff31a45b9ee889241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/ab6bd348754ac1cc2583d470123aee82241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/329c6d0622234384361f47461191f595241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/595995bfacb8d86452d7e0e6768bb75e241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/c7d01e5476ca9cab42d43ff36c8f5608241286257.jpg" align="middle"></details><h2 id="GraVITON-Graph-based-garment-warping-with-attention-guided-inversion-for-Virtual-tryon"><a href="#GraVITON-Graph-based-garment-warping-with-attention-guided-inversion-for-Virtual-tryon" class="headerlink" title="GraVITON: Graph based garment warping with attention guided inversion   for Virtual-tryon"></a>GraVITON: Graph based garment warping with attention guided inversion for Virtual-tryon</h2><p><strong>Authors:Sanhita Pathak, Vinay Kaushik, Brejesh Lall</strong></p><p>Virtual try-on, a rapidly evolving field in computer vision, is transforming e-commerce by improving customer experiences through precise garment warping and seamless integration onto the human body. While existing methods such as TPS and flow address the garment warping but overlook the finer contextual details. In this paper, we introduce a novel graph based warping technique which emphasizes the value of context in garment flow. Our graph based warping module generates warped garment as well as a coarse person image, which is utilised by a simple refinement network to give a coarse virtual tryon image. The proposed work exploits latent diffusion model to generate the final tryon, treating garment transfer as an inpainting task. The diffusion model is conditioned with decoupled cross attention based inversion of visual and textual information. We introduce an occlusion aware warping constraint that generates dense warped garment, without any holes and occlusion. Our method, validated on VITON-HD and Dresscode datasets, showcases substantial state-of-the-art qualitative and quantitative results showing considerable improvement in garment warping, texture preservation, and overall realism.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.02184v1">PDF</a> 18 pages, 7 Figures and 6 Tables</p><p><strong>Summary</strong><br>è™šæ‹Ÿè¯•è¡£æŠ€æœ¯é€šè¿‡å›¾å½¢å˜å½¢å’Œäººä½“èåˆï¼Œæé«˜ç”µå­å•†åŠ¡å®¢æˆ·ä½“éªŒï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå›¾å½¢çš„å˜å½¢æŠ€æœ¯ï¼Œå¼ºè°ƒä¸Šä¸‹æ–‡ç»†èŠ‚çš„é‡è¦æ€§ã€‚</p><p><strong>Key Takeaways</strong><br>â€¢ è™šæ‹Ÿè¯•è¡£æŠ€æœ¯å¯ä»¥æ”¹å–„ç”µå­å•†åŠ¡å®¢æˆ·ä½“éªŒã€‚<br>â€¢ ç°æœ‰çš„å˜å½¢æ–¹æ³•ï¼ˆå¦‚TPSå’Œæµï¼‰å¿½è§†äº†ä¸Šä¸‹æ–‡ç»†èŠ‚ã€‚<br>â€¢ æœ¬æ–‡æå‡ºçš„å›¾å½¢å˜å½¢æŠ€æœ¯å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„å˜å½¢æœè£…å’Œç²—ç³™çš„äººä½“å›¾åƒã€‚<br>â€¢ æå‡ºçš„æ–¹æ³•ä½¿ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆæœ€ç»ˆçš„è¯•è¡£å›¾åƒï¼Œå°†æœè£…ä¼ è¾“è§†ä¸ºinpaintingä»»åŠ¡ã€‚<br>â€¢ è¯¥æ–¹æ³•ä½¿ç”¨å»è€¦åˆçš„äº¤å‰æ³¨æ„åŠ›åè½¬è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯ã€‚<br>â€¢ æœ¬æ–‡æå‡ºäº†ä¸€ç§é®æŒ¡æ„ŸçŸ¥å˜å½¢çº¦æŸï¼Œç”Ÿæˆå¯†é›†çš„å˜å½¢æœè£…ï¼Œwithout holes and occlusionã€‚<br>â€¢ è¯¥æ–¹æ³•åœ¨VITON-HDå’ŒDresscodeæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºäº†æ˜¾è‘—çš„state-of-the-art qualitative and quantitativeç»“æœã€‚</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: GraVITONï¼šåŸºäºå›¾çš„æœè£…å˜å½¢è™šæ‹Ÿè¯•è¡£ï¼ˆGraph-based Garment Warping for Virtual Try-onï¼‰</p></li><li><p>Authors: Sanhita Pathak, Vinay Kaushik, Brejesh Lall</p></li><li><p>Affiliation: æœªæä¾›ï¼ˆNo affiliation providedï¼‰</p></li><li><p>Keywords: Virtual try-on Â· Optical Flow Â· Graph Â· Latent Diffusion models</p></li><li><p>Urls: arXiv:2406.02184v1 , Github:None</p></li><li><p>Summary:</p></li></ol><ul><li><p>(1):æœ¬æ–‡ç ”ç©¶çš„èƒŒæ™¯æ˜¯è™šæ‹Ÿè¯•è¡£æŠ€æœ¯çš„å‘å±•ï¼Œå®ƒèƒ½å¤Ÿä¸ºç”µå­å•†åŠ¡æä¾›æ›´å¥½çš„å®¢æˆ·ä½“éªŒã€‚</p></li><li><p>(2):è¿‡å»çš„æ–¹æ³•åŒ…æ‹¬TPS warpingå’Œflow-based approachesï¼Œä½†è¿™äº›æ–¹æ³•å¿½è§†äº†æœè£…æµçš„ç»†èŠ‚ä¿¡æ¯ï¼Œæ— æ³•ç”ŸæˆçœŸå®çš„è¯•è¡£ç»“æœã€‚</p></li><li><p>(3):æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå›¾çš„æœè£…å˜å½¢æ–¹æ³•ï¼Œä½¿ç”¨å›¾åŸºäºçš„å˜å½¢æ¨¡å—ç”Ÿæˆå˜å½¢åçš„æœè£…å’Œç³™çš„äººä½“å›¾åƒï¼Œç„¶åä½¿ç”¨ç®€å•çš„ tinhancementç½‘ç»œç”Ÿæˆç²—ç³™çš„è™šæ‹Ÿè¯•è¡£å›¾åƒã€‚æœ€åï¼Œä½¿ç”¨æ½œä¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆæœ€ç»ˆçš„è¯•è¡£ç»“æœã€‚</p></li><li><p>(4):æœ¬æ–‡çš„æ–¹æ³•åœ¨VITON-HDå’ŒDresscodeæ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œå–å¾—äº†state-of-the-artçš„å®šæ€§å’Œå®šé‡ç»“æœï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨æœè£…å˜å½¢ã€çº¹ç†ä¿ç•™å’Œæ€»ä½“çœŸå®æ€§æ–¹é¢çš„æ”¹è¿›ã€‚</p></li></ul><ol><li>æ–¹æ³•ï¼š</li></ol><ul><li><p>(1)ï¼šæå‡ºäº†ä¸€ç§åŸºäºå›¾çš„æœè£…å˜å½¢æ–¹æ³•ï¼Œä½¿ç”¨å›¾åŸºäºçš„å˜å½¢æ¨¡å—ç”Ÿæˆå˜å½¢åçš„æœè£…å’Œç³™çš„äººä½“å›¾åƒã€‚</p></li><li><p>(2)ï¼šè¯¥æ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µæ˜¯å˜å½¢é˜¶æ®µï¼Œä½¿ç”¨å›¾æµç½‘ç»œï¼ˆGraphNetï¼‰å’Œç²¾ç‚¼ç½‘ç»œï¼ˆRefineNetï¼‰ç”Ÿæˆå˜å½¢åçš„æœè£…å’Œç³™çš„äººä½“å›¾åƒï¼›ç¬¬äºŒé˜¶æ®µæ˜¯ç”Ÿæˆé˜¶æ®µï¼Œä½¿ç”¨æ½œä¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆæœ€ç»ˆçš„è¯•è¡£ç»“æœã€‚</p></li><li><p>(3)ï¼šåœ¨å˜å½¢é˜¶æ®µï¼Œä½¿ç”¨å›¾æµç½‘ç»œï¼ˆGraphNetï¼‰è®¡ç®—å¯†é›†æµï¼Œä»¥ç”Ÿæˆå˜å½¢åçš„æœè£…å’Œç³™çš„äººä½“å›¾åƒã€‚</p></li><li><p>(4)ï¼šç„¶åï¼Œä½¿ç”¨ç²¾ç‚¼ç½‘ç»œï¼ˆRefineNetï¼‰è®¡ç®—æœ€ç»ˆçš„å˜å½¢æŒ‡ä»¤ï¼Œä»¥ç”Ÿæˆå˜å½¢åçš„æœè£…å’Œç³™çš„äººä½“å›¾åƒã€‚</p></li><li><p>(5)ï¼šåœ¨ç”Ÿæˆé˜¶æ®µï¼Œä½¿ç”¨æ½œä¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆæœ€ç»ˆçš„è¯•è¡£ç»“æœï¼Œæ¡ä»¶äºertextual embeddingå’Œæœè£…è¯•è¡£å›¾åƒçš„çº¹ç†åµŒå…¥ã€‚</p></li><li><p>(6)ï¼šä¸ºäº†æé«˜ç”Ÿæˆç»“æœçš„çœŸå®æ€§ï¼Œå¼•å…¥äº†ä¸€ä¸ªå»è€¦åˆæ³¨æ„åŠ›é€‚é…å™¨ï¼ˆDecoupled Cross Attention Adaptorï¼ŒDCAAï¼‰ï¼Œä»¥æ¡ä»¶æ½œä¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ›´ä¸ºé€¼çœŸçš„è¯•è¡£ç»“æœã€‚</p></li><li><p>(7)ï¼šæœ€åï¼Œä½¿ç”¨ Occlusion Aware warp Lossï¼ˆOWLï¼‰æŸå¤±å‡½æ•°æ¥å­¦ä¹ å˜å½¢åçš„æœè£…ï¼Œä»¥é¿å…è‡ªé®æŒ¡é—®é¢˜ã€‚</p></li></ul><ol><li>Conclusion:</li></ol><ul><li><p>(1):æœ¬æ–‡çš„ç ”ç©¶å·¥ä½œå¯¹è™šæ‹Ÿè¯•è¡£æŠ€æœ¯çš„å‘å±•å…·æœ‰é‡è¦æ„ä¹‰ï¼Œå¯ä»¥ä¸ºç”µå­å•†åŠ¡æä¾›æ›´å¥½çš„å®¢æˆ·ä½“éªŒã€‚</p></li><li><p>(2):åˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäºå›¾çš„æœè£…å˜å½¢æ–¹æ³•ï¼Œä½¿ç”¨å›¾æµç½‘ç»œï¼ˆGraphNetï¼‰å’Œæ½œä¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡çš„è™šæ‹Ÿè¯•è¡£ç»“æœï¼›æ€§èƒ½ï¼šåœ¨VITON-HDå’ŒDresscodeæ•°æ®é›†ä¸Šå–å¾—äº†state-of-the-artçš„å®šæ€§å’Œå®šé‡ç»“æœï¼›å·¥ä½œé‡ï¼šéœ€è¦å¤§é‡çš„æ•°æ®å’Œè®¡ç®—èµ„æºæ¥è®­ç»ƒæ¨¡å‹ï¼Œä¸”ç®—æ³•å¤æ‚åº¦è¾ƒé«˜ã€‚</p></li></ul><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/d8b7026ce91b28c27a140c63af83d58e241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/b2845c22971c07776331dbad3278d3fe241286257.jpg" align="middle"></details><h2 id="Follow-Your-Emoji-Fine-Controllable-and-Expressive-Freestyle-Portrait-Animation"><a href="#Follow-Your-Emoji-Fine-Controllable-and-Expressive-Freestyle-Portrait-Animation" class="headerlink" title="Follow-Your-Emoji: Fine-Controllable and Expressive Freestyle Portrait   Animation"></a>Follow-Your-Emoji: Fine-Controllable and Expressive Freestyle Portrait Animation</h2><p><strong>Authors:Yue Ma, Hongyu Liu, Hongfa Wang, Heng Pan, Yingqing He, Junkun Yuan, Ailing Zeng, Chengfei Cai, Heung-Yeung Shum, Wei Liu, Qifeng Chen</strong></p><p>We present Follow-Your-Emoji, a diffusion-based framework for portrait animation, which animates a reference portrait with target landmark sequences. The main challenge of portrait animation is to preserve the identity of the reference portrait and transfer the target expression to this portrait while maintaining temporal consistency and fidelity. To address these challenges, Follow-Your-Emoji equipped the powerful Stable Diffusion model with two well-designed technologies. Specifically, we first adopt a new explicit motion signal, namely expression-aware landmark, to guide the animation process. We discover this landmark can not only ensure the accurate motion alignment between the reference portrait and target motion during inference but also increase the ability to portray exaggerated expressions (i.e., large pupil movements) and avoid identity leakage. Then, we propose a facial fine-grained loss to improve the modelâ€™s ability of subtle expression perception and reference portrait appearance reconstruction by using both expression and facial masks. Accordingly, our method demonstrates significant performance in controlling the expression of freestyle portraits, including real humans, cartoons, sculptures, and even animals. By leveraging a simple and effective progressive generation strategy, we extend our model to stable long-term animation, thus increasing its potential application value. To address the lack of a benchmark for this field, we introduce EmojiBench, a comprehensive benchmark comprising diverse portrait images, driving videos, and landmarks. We show extensive evaluations on EmojiBench to verify the superiority of Follow-Your-Emoji.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.01900v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://follow-your-emoji.github.io/">https://follow-your-emoji.github.io/</a></p><p><strong>Summary</strong><br>Follow-Your-Emojiï¼šåŸºäºæ‰©æ•£æ¨¡å‹çš„è‚–åƒåŠ¨ç”»æ¡†æ¶ï¼Œé€šè¿‡æ ‡å¿—åºåˆ—åŠ¨ç”»å‚è€ƒè‚–åƒã€‚</p><p><strong>Key Takeaways</strong><br>â€¢ Follow-Your-Emoji æ¡†æ¶ä½¿ç”¨ Stable Diffusion æ¨¡å‹å’Œä¸¤ç§æŠ€æœ¯æ¥è§£å†³è‚–åƒåŠ¨ç”»æŒ‘æˆ˜ã€‚<br>â€¢ è¡¨æƒ…æ„ŸçŸ¥æ ‡å¿—ä¿¡å·æŒ‡å¯¼åŠ¨ç”»è¿‡ç¨‹ï¼Œç¡®ä¿åŠ¨ä½œå¯¹é½å’Œèº«ä»½ä¿æŒã€‚<br>â€¢ è„¸éƒ¨ç»†ç²’åº¦æŸå¤±å‡½æ•°æé«˜æ¨¡å‹å¯¹ç»†å¾®è¡¨æƒ…å’Œå‚è€ƒè‚–åƒå¤–è§‚çš„é‡å»ºèƒ½åŠ›ã€‚<br>â€¢ è¯¥æ–¹æ³•åœ¨æ§åˆ¶è‡ªç”±è‚–åƒè¡¨æƒ…æ–¹é¢æ˜¾ç¤ºå‡ºæ˜¾è‘—æ€§èƒ½ï¼ŒåŒ…æ‹¬çœŸäººã€å¡é€šã€é›•å¡‘å’ŒåŠ¨ç‰©ã€‚<br>â€¢ è¿›æ­¥ç”Ÿæˆç­–ç•¥ä½¿æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆé•¿æœŸç¨³å®šçš„åŠ¨ç”»ï¼Œå¢åŠ å…¶åº”ç”¨ä»·å€¼ã€‚<br>â€¢ EmojiBench åŸºå‡†æµ‹è¯•é›†ç”¨äºè¯„ä¼° Follow-Your-Emoji çš„ä¼˜è¶Šæ€§ã€‚<br>â€¢ è¯¥æ–¹æ³•åœ¨ EmojiBench ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„è¯„ä¼°ï¼ŒéªŒè¯äº†å…¶ä¼˜è¶Šæ€§ã€‚</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>Title: ç»†è‡´å¯æ§çš„è‡ªç”±å¼è‚–åƒåŠ¨ç”»ï¼ˆFine-Controllable and Expressive Freestyle Portrait Animationï¼‰</li></ol><ol><li>Authors: Yue Ma, Hongyu Liu, Hongfa Wang, Heng Pan, Yingqing He, Junkun Yuan, Ailing Zeng, Chengfei Cai, Heung-Yeung Shum, Wei Liu, Qifeng Chen</li></ol><ol><li>Affiliation: é¦™æ¸¯ç§‘æŠ€å¤§å­¦</li></ol><ol><li>Keywords: Portrait Animation, Diffusion Model, Expression-Aware Landmark, Facial Fine-Grained Loss</li></ol><ol><li>Urls: <a target="_blank" rel="noopener" href="https://follow-your-emoji.github.io/">https://follow-your-emoji.github.io/</a> , Github:None</li></ol><ol><li>Summary:</li></ol><pre><code>- (1):æœ¬æ–‡ç ”ç©¶çš„æ˜¯è‚–åƒåŠ¨ç”»ä»»åŠ¡ï¼Œå³å°†ç›®æ ‡åºåˆ—çš„å§¿åŠ¿å’Œè¡¨æƒ…ä»é©±åŠ¨è§†é¢‘è½¬ç§»åˆ°å‚è€ƒè‚–åƒä¸­ã€‚


- (2):è¿‡å»çš„æ–¹æ³•åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰å’Œæ‰©æ•£æ¨¡å‹ï¼Œä½†å®ƒä»¬å­˜åœ¨ä¸€äº›é—®é¢˜ï¼Œå¦‚èº«ä»½æ³„éœ²ã€ä¸çœŸå®çš„å†…å®¹å’Œæ˜¾è‘—çš„artifactã€‚


- (3):æœ¬æ–‡æå‡ºäº†Follow-Your-Emojiæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨æ‰©æ•£æ¨¡å‹å¹¶å¼•å…¥äº†ä¸¤ç§æŠ€æœ¯ï¼šè¡¨æƒ…æ„ŸçŸ¥æ ‡å¿—å’Œé¢éƒ¨ç»†ç²’åº¦æŸå¤±å‡½æ•°ã€‚


- (4):æœ¬æ–‡çš„æ–¹æ³•åœ¨è‡ªç”±å¼è‚–åƒåŠ¨ç”»ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½ï¼Œèƒ½å¤Ÿæ§åˆ¶è‚–åƒçš„è¡¨æƒ…ï¼ŒåŒ…æ‹¬äººã€å¡é€šã€é›•å¡‘å’ŒåŠ¨ç‰©ç­‰ã€‚åŒæ—¶ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ä¸ªåä¸ºEmojiBenchçš„åŸºå‡†æµ‹è¯•é›†ï¼Œä»¥è¯„ä¼°è‚–åƒåŠ¨ç”»æ–¹æ³•çš„æ€§èƒ½ã€‚
</code></pre><p>sorry, æ‚¨çš„ipå·²ç”±äºè§¦å‘é˜²æ»¥ç”¨æ£€æµ‹è€Œè¢«å°ç¦,å¯èƒ½æ˜¯å› ä¸ºä½¿ç”¨äº‘æœåŠ¡å™¨å¦‚è…¾è®¯äº‘æˆ–è€…å›½å¤–ä»£ç†(vpn)è®¿é—®æœ¬ç½‘ç«™ï¼Œå¦‚æœä½¿ç”¨äº†vpnï¼Œå…³é—­vpnæˆ–ä»£ç†å³å¯ç»§ç»­ä½¿ç”¨,æœ¬æœåŠ¡ç½‘å€æ˜¯<a target="_blank" rel="noopener" href="https://chat18.aichatos8.com">https://chat18.aichatos8.com</a> å¦‚éœ€åˆä½œæ¥å£è°ƒç”¨è¯·è”ç³»å¾®ä¿¡chatkf123 æˆ–è€…å‰å¾€ <a target="_blank" rel="noopener" href="https://binjie09.shop">https://binjie09.shop</a> è‡ªåŠ©è´­ä¹°key, è®¤ä¸ºæ˜¯è¯¯å°éœ€è¦è§£å°çš„è¯·å‰å¾€<a target="_blank" rel="noopener" href="https://www.ip.cn/">https://www.ip.cn/</a> æŸ¥è¯¢ipä¿¡æ¯,å¹¶å‘é€ä¿¡æ¯è‡³é‚®ä»¶ gpt33@binjie.site ï¼Œç«™é•¿ä¼šå®šæœŸçœ‹é‚®ä»¶å¹¶å¤„ç†è§£å°å’Œåˆä½œé—®é¢˜ï¼Œå¦‚éœ€è°ƒç”¨æ¥å£è¯·è§æ¥å£æ–‡æ¡£<a target="_blank" rel="noopener" href="https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82ï¼Œå¦‚éœ€åˆä½œæ¥å£è°ƒç”¨è¯·è”ç³»å¾®ä¿¡chatkf123">https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82ï¼Œå¦‚éœ€åˆä½œæ¥å£è°ƒç”¨è¯·è”ç³»å¾®ä¿¡chatkf123</a> æˆ–è€…å‰å¾€ <a target="_blank" rel="noopener" href="https://cat.gptxyy.cn">https://cat.gptxyy.cn</a> æ³¨å†Œä½¿ç”¨ï¼ˆå¯ä»˜è´¹ä½¿ç”¨gpt4 æ³¨å†Œå¯å…è´¹ä½¿ç”¨3.5ï¼‰</p><ol><li>Conclusion:</li></ol><ul><li>(1):æœ¬æ–‡çš„å·¥ä½œå¯¹äºè‚–åƒåŠ¨ç”»ä»»åŠ¡å…·æœ‰é‡è¦æ„ä¹‰ï¼Œå› ä¸ºå®ƒè§£å†³äº†è¿‡å»æ–¹æ³•ä¸­å­˜åœ¨çš„èº«ä»½æ³„éœ²ã€ä¸çœŸå®çš„å†…å®¹å’Œæ˜¾è‘—çš„artifacté—®é¢˜ï¼Œæé«˜äº†è‚–åƒåŠ¨ç”»çš„å¯æ§æ€§å’Œè¡¨è¾¾æ€§ã€‚</li></ul><ul><li>(2):åˆ›æ–°ç‚¹ï¼šæœ¬æ–‡æå‡ºçš„Follow-Your-Emojiæ¡†æ¶é‡‡ç”¨æ‰©æ•£æ¨¡å‹å¹¶å¼•å…¥äº†è¡¨æƒ…æ„ŸçŸ¥æ ‡å¿—å’Œé¢éƒ¨ç»†ç²’åº¦æŸå¤±å‡½æ•°ï¼Œè§£å†³äº†è‚–åƒåŠ¨ç”»ä»»åŠ¡ä¸­çš„å¤šä¸ªæŒ‘æˆ˜ï¼›æ€§èƒ½ï¼šæœ¬æ–‡çš„æ–¹æ³•åœ¨è‡ªç”±å¼è‚–åƒåŠ¨ç”»ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½ï¼Œèƒ½å¤Ÿæ§åˆ¶è‚–åƒçš„è¡¨æƒ…ï¼ŒåŒ…æ‹¬äººã€å¡é€šã€é›•å¡‘å’ŒåŠ¨ç‰©ç­‰ï¼›å·¥ä½œé‡ï¼šæœ¬æ–‡çš„æ–¹æ³•éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®å’Œè®¡ç®—èµ„æºï¼Œä½†å…¶æå‡ºçš„ EmojiBenchåŸºå‡†æµ‹è¯•é›†å¯ä»¥ä¸ºæœªæ¥è‚–åƒåŠ¨ç”»ç ”ç©¶æä¾›é‡è¦çš„å‚è€ƒä»·å€¼ã€‚</li></ul><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/40bb4d009765bfeafad0de4e983ca97b241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/2a6f923f4899d3ecd2aca72132a3edf1241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/aa82ff249b702946ffff2f4be643318b241286257.jpg" align="middle"></details><h2 id="Cross-Domain-Graph-Data-Scaling-A-Showcase-with-Diffusion-Models"><a href="#Cross-Domain-Graph-Data-Scaling-A-Showcase-with-Diffusion-Models" class="headerlink" title="Cross-Domain Graph Data Scaling: A Showcase with Diffusion Models"></a>Cross-Domain Graph Data Scaling: A Showcase with Diffusion Models</h2><p><strong>Authors:Wenzhuo Tang, Haitao Mao, Danial Dervovic, Ivan Brugere, Saumitra Mishra, Yuying Xie, Jiliang Tang</strong></p><p>Models for natural language and images benefit from data scaling behavior: the more data fed into the model, the better they perform. This â€˜better with moreâ€™ phenomenon enables the effectiveness of large-scale pre-training on vast amounts of data. However, current graph pre-training methods struggle to scale up data due to heterogeneity across graphs. To achieve effective data scaling, we aim to develop a general model that is able to capture diverse data patterns of graphs and can be utilized to adaptively help the downstream tasks. To this end, we propose UniAug, a universal graph structure augmentor built on a diffusion model. We first pre-train a discrete diffusion model on thousands of graphs across domains to learn the graph structural patterns. In the downstream phase, we provide adaptive enhancement by conducting graph structure augmentation with the help of the pre-trained diffusion model via guided generation. By leveraging the pre-trained diffusion model for structure augmentation, we consistently achieve performance improvements across various downstream tasks in a plug-and-play manner. To the best of our knowledge, this study represents the first demonstration of a data-scaling graph structure augmentor on graphs across domains.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.01899v1">PDF</a></p><p><strong>Summary</strong><br>æå‡ºäº†ä¸€ç§é€šç”¨çš„å›¾ç»“æ„å¢å¼ºå™¨UniAugï¼ŒåŸºäºæ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿæ•æ‰å¤šæ ·åŒ–çš„å›¾æ•°æ®æ¨¡å¼ï¼Œæé«˜ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚</p><p><strong>Key Takeaways</strong><br>â€¢ å›¾æ¨¡å‹çš„æ€§èƒ½å¯ä»¥é€šè¿‡æ•°æ®è§„æ¨¡çš„å¢åŠ æ¥æé«˜ï¼Œä½†å½“å‰çš„å›¾é¢„è®­ç»ƒæ–¹æ³•éš¾ä»¥æ‰©å±•æ•°æ®è§„æ¨¡ã€‚<br>â€¢ æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„ç”¨å›¾ç»“æ„å¢å¼ºå™¨UniAugï¼Œå¯ä»¥æ•æ‰å¤šæ ·åŒ–çš„å›¾æ•°æ®æ¨¡å¼ã€‚<br>â€¢ UniAug é€šè¿‡é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹æ¥å­¦ä¹ å›¾ç»“æ„æ¨¡å¼ï¼Œç„¶ååœ¨ä¸‹æ¸¸é˜¶æ®µè¿›è¡Œå›¾ç»“æ„å¢å¼ºã€‚<br>â€¢ ä½¿ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è¿›è¡Œç»“æ„å¢å¼ºå¯ä»¥åœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸­å¸¦æ¥æ€§èƒ½æ”¹è¿›ã€‚<br>â€¢ æœ¬ç ”ç©¶æ˜¯ä¸€ä¸ªåœ¨è·¨åŸŸå›¾ä¸Šå±•ç¤ºæ•°æ®æ‰©å±•å›¾ç»“æ„å¢å¼ºå™¨çš„ç¤ºä¾‹ã€‚<br>â€¢ UniAug å¯ä»¥åœ¨æ’ä»¶å¼æ–¹å¼ä¸‹æé«˜ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚<br>â€¢ æœ¬æ–¹æ³•å¯ä»¥å¸®åŠ©ä¸‹æ¸¸ä»»åŠ¡è‡ªé€‚åº”åœ°é€‰æ‹©åˆé€‚çš„å›¾ç»“æ„å¢å¼ºç­–ç•¥ã€‚</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>Title: è·¨åŸŸå›¾æ•°æ®æ‰©å±•ï¼šåŸºäºæ‰©æ•£æ¨¡å‹çš„å±•ç¤º (Cross-Domain Graph Data Scaling: A Showcase with Diffusion Models)</li></ol><ol><li>Authors: Wenzhuo Tang, Haitao Mao, Danial Dervovic, Ivan Brugere, Saumitra Mishra, Yuying Xie, Jiliang Tang</li></ol><ol><li>Affiliation: å¯†æ­‡æ ¹å·ç«‹å¤§å­¦</li></ol><ol><li>Keywords: graph pre-training, data scaling, diffusion models, graph structure augmentation</li></ol><ol><li>Urls: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01899">https://arxiv.org/abs/2406.01899</a>, Github: <a target="_blank" rel="noopener" href="https://github.com/WenzhuoTang/UniAug">https://github.com/WenzhuoTang/UniAug</a></li></ol><ol><li>Summary:</li></ol><pre><code>- (1):ç ”ç©¶å‘ç°è‡ªç„¶è¯­è¨€å’Œå›¾åƒæ¨¡å‹çš„æ€§èƒ½éšç€æ•°æ®è§„æ¨¡çš„å¢åŠ è€Œæé«˜ï¼Œä½†å½“å‰çš„å›¾é¢„è®­ç»ƒæ–¹æ³•éš¾ä»¥æ‰©å±•æ•°æ®è§„æ¨¡ï¼Œå› ä¸ºå›¾ä¹‹é—´å­˜åœ¨å¼‚è´¨æ€§ã€‚


- (2):è¿‡å»çš„å›¾é¢„è®­ç»ƒæ–¹æ³•æ— æ³•å¾ˆå¥½åœ°å¤„ç†å›¾ä¹‹é—´çš„å¼‚è´¨æ€§ï¼Œæ— æ³•å®ç°æ•°æ®æ‰©å±•ï¼›æœ¬æ–‡æå‡ºçš„æ–¹æ³• UniAug é€šè¿‡æ‰©æ•£æ¨¡å‹æ¥å­¦ä¹ å›¾ç»“æ„æ¨¡å¼ï¼Œè§£å†³äº†è¿™ä¸ªé—®é¢˜ã€‚


- (3):æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„é€šç”¨å›¾ç»“æ„å¢å¼ºå™¨ UniAugï¼Œé¦–å…ˆåœ¨æ•°åƒä¸ªè·¨åŸŸå›¾ä¸Šé¢„è®­ç»ƒç¦»æ•£æ‰©æ•£æ¨¡å‹ï¼Œç„¶ååœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­ä½¿ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è¿›è¡Œå›¾ç»“æ„å¢å¼ºã€‚


- (4):å®éªŒç»“æœè¡¨æ˜ï¼ŒUniAug æ–¹æ³•åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­å–å¾—äº†æ€§èƒ½æ”¹è¿›ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚
</code></pre><ol><li>æ–¹æ³•ï¼š</li></ol><ul><li><p>(1)ï¼šæ”¶é›†å¤šä¸ªé¢†åŸŸçš„å›¾æ•°æ®ï¼ŒåŒ…æ‹¬ç”Ÿç‰©ç½‘ç»œã€åŒ–å­¦ç½‘ç»œã€ç¤¾äº¤ç½‘ç»œç­‰ï¼Œå…±è®¡æ•°åƒä¸ªå›¾ï¼Œç”¨äºè®­ç»ƒæ‰©æ•£æ¨¡å‹ã€‚</p></li><li><p>(2)ï¼šå¯¹æ”¶é›†çš„å›¾æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼ŒåŒ…æ‹¬è®¡ç®—å›¾çš„ç»“æ„ç‰¹å¾ï¼Œå¦‚èŠ‚ç‚¹æ•°ã€å¯†åº¦ã€ç½‘ç»œç†µã€å¹³å‡åº¦ã€åº¦æ–¹å·®ã€æ— æ ‡åº¦æŒ‡æ•°ç­‰ï¼Œç”¨äºæ„å»ºå›¾çº§è¡¨ç¤ºã€‚</p></li><li><p>(3)ï¼šé‡‡ç”¨è‡ªç›‘ç£æ ‡æ³¨ç­–ç•¥ï¼Œé€šè¿‡èšç±»ç®—æ³•å¯¹å›¾çº§è¡¨ç¤ºè¿›è¡Œèšç±»ï¼Œè·å–è‡ªç›‘ç£æ ‡ç­¾ï¼Œç”¨äºè®­ç»ƒæ‰©æ•£æ¨¡å‹ã€‚</p></li><li><p>(4)ï¼šæ„å»ºç¦»æ•£æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºå­¦ä¹ å›¾ç»“æ„æ¨¡å¼ï¼Œæ¨¡å‹åŒ…æ‹¬denoising ç½‘ç»œå’Œå›¾å˜æ¢å™¨ï¼ˆGTï¼‰ï¼Œdenoising ç½‘ç»œç”¨äºæ¢å¤å›¾çš„é‚»æ¥çŸ©é˜µï¼Œå›¾å˜æ¢å™¨ç”¨äºå­¦ä¹ å›¾çš„ç»“æ„è¡¨ç¤ºã€‚</p></li><li><p>(5)ï¼šè®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼Œä½¿ç”¨variational lower boundä¼˜åŒ–ç›®æ ‡ï¼Œç”¨äºå­¦ä¹ å›¾ç»“æ„æ¨¡å¼ã€‚</p></li><li><p>(6)ï¼šåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è¿›è¡Œå›¾ç»“æ„å¢å¼ºï¼Œé€šè¿‡guided generationç”Ÿæˆåˆæˆå›¾ç»“æ„ï¼Œå¹¶ä¸åŸå§‹èŠ‚ç‚¹ç‰¹å¾ç»„åˆï¼Œç”¨äºè®­ç»ƒä¸‹æ¸¸ä»»åŠ¡çš„GNNæ¨¡å‹ã€‚</p></li><li><p>(7)ï¼šé€‰æ‹©åˆé€‚çš„æŒ‡å¯¼ç›®æ ‡ï¼ŒåŒ…æ‹¬èŠ‚ç‚¹çº§ã€è¾¹çº§å’Œå›¾çº§çš„æŒ‡å¯¼ç›®æ ‡ï¼Œç”¨äºbridging the gap between pre-training distribution and downstream datasetã€‚</p></li><li><p>(8)ï¼šä½¿ç”¨æ¢¯åº¦åŸºäºæ–¹æ³•ï¼Œä¾‹å¦‚NOSæ–¹æ³•ï¼Œè¿›è¡Œguided generationï¼Œç”Ÿæˆåˆæˆå›¾ç»“æ„ã€‚</p></li><li><p>(9)ï¼šä½¿ç”¨augmentedå›¾æ•°æ®è®­ç»ƒä¸‹æ¸¸ä»»åŠ¡çš„GNNæ¨¡å‹ï¼Œç”¨äºæé«˜ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚</p></li></ul><p>sorry, æ‚¨çš„ipå·²ç”±äºè§¦å‘é˜²æ»¥ç”¨æ£€æµ‹è€Œè¢«å°ç¦,å¯èƒ½æ˜¯å› ä¸ºä½¿ç”¨äº‘æœåŠ¡å™¨å¦‚è…¾è®¯äº‘æˆ–è€…å›½å¤–ä»£ç†(vpn)è®¿é—®æœ¬ç½‘ç«™ï¼Œå¦‚æœä½¿ç”¨äº†vpnï¼Œå…³é—­vpnæˆ–ä»£ç†å³å¯ç»§ç»­ä½¿ç”¨,æœ¬æœåŠ¡ç½‘å€æ˜¯<a target="_blank" rel="noopener" href="https://chat18.aichatos8.com">https://chat18.aichatos8.com</a> å¦‚éœ€åˆä½œæ¥å£è°ƒç”¨è¯·è”ç³»å¾®ä¿¡chatkf123 æˆ–è€…å‰å¾€ <a target="_blank" rel="noopener" href="https://binjie09.shop">https://binjie09.shop</a> è‡ªåŠ©è´­ä¹°key, è®¤ä¸ºæ˜¯è¯¯å°éœ€è¦è§£å°çš„è¯·å‰å¾€<a target="_blank" rel="noopener" href="https://www.ip.cn/">https://www.ip.cn/</a> æŸ¥è¯¢ipä¿¡æ¯,å¹¶å‘é€ä¿¡æ¯è‡³é‚®ä»¶ gpt33@binjie.site ï¼Œç«™é•¿ä¼šå®šæœŸçœ‹é‚®ä»¶å¹¶å¤„ç†è§£å°å’Œåˆä½œé—®é¢˜ï¼Œå¦‚éœ€è°ƒç”¨æ¥å£è¯·è§æ¥å£æ–‡æ¡£<a target="_blank" rel="noopener" href="https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82ï¼Œå¦‚éœ€åˆä½œæ¥å£è°ƒç”¨è¯·è”ç³»å¾®ä¿¡chatkf123">https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82ï¼Œå¦‚éœ€åˆä½œæ¥å£è°ƒç”¨è¯·è”ç³»å¾®ä¿¡chatkf123</a> æˆ–è€…å‰å¾€ <a target="_blank" rel="noopener" href="https://cat.gptxyy.cn">https://cat.gptxyy.cn</a> æ³¨å†Œä½¿ç”¨ï¼ˆå¯ä»˜è´¹ä½¿ç”¨gpt4 æ³¨å†Œå¯å…è´¹ä½¿ç”¨3.5ï¼‰</p><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/98b3bdaaf6267f0a53ae7d2491a660c2241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/32c460eff661bc8ebe78818d82409486241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/65febcdec0bc56574cdb8d9263cd3dcf241286257.jpg" align="middle"></details><h2 id="L-MAGIC-Language-Model-Assisted-Generation-of-Images-with-Coherence"><a href="#L-MAGIC-Language-Model-Assisted-Generation-of-Images-with-Coherence" class="headerlink" title="L-MAGIC: Language Model Assisted Generation of Images with Coherence"></a>L-MAGIC: Language Model Assisted Generation of Images with Coherence</h2><p><strong>Authors:Zhipeng Cai, Matthias Mueller, Reiner Birkl, Diana Wofk, Shao-Yen Tseng, JunDa Cheng, Gabriela Ben-Melech Stan, Vasudev Lal, Michael Paulitsch</strong></p><p>In the current era of generative AI breakthroughs, generating panoramic scenes from a single input image remains a key challenge. Most existing methods use diffusion-based iterative or simultaneous multi-view inpainting. However, the lack of global scene layout priors leads to subpar outputs with duplicated objects (e.g., multiple beds in a bedroom) or requires time-consuming human text inputs for each view. We propose L-MAGIC, a novel method leveraging large language models for guidance while diffusing multiple coherent views of 360 degree panoramic scenes. L-MAGIC harnesses pre-trained diffusion and language models without fine-tuning, ensuring zero-shot performance. The output quality is further enhanced by super-resolution and multi-view fusion techniques. Extensive experiments demonstrate that the resulting panoramic scenes feature better scene layouts and perspective view rendering quality compared to related works, with &gt;70% preference in human evaluations. Combined with conditional diffusion models, L-MAGIC can accept various input modalities, including but not limited to text, depth maps, sketches, and colored scripts. Applying depth estimation further enables 3D point cloud generation and dynamic scene exploration with fluid camera motion. Code is available at <a target="_blank" rel="noopener" href="https://github.com/IntelLabs/MMPano">https://github.com/IntelLabs/MMPano</a>. The video presentation is available at <a target="_blank" rel="noopener" href="https://youtu.be/XDMNEzH4-Ec?list=PLG9Zyvu7iBa0-a7ccNLO8LjcVRAoMn57s">https://youtu.be/XDMNEzH4-Ec?list=PLG9Zyvu7iBa0-a7ccNLO8LjcVRAoMn57s</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.01843v1">PDF</a> accepted to CVPR 2024</p><p><strong>Summary</strong><br>æå‡ºL-MAGICæ–¹æ³•ï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹æŒ‡å¯¼ç”Ÿæˆé«˜è´¨é‡360åº¦å…¨æ™¯åœºæ™¯å›¾åƒã€‚</p><p><strong>Key Takeaways</strong><br>â€¢ å½“å‰çš„ç”Ÿæˆå…¨æ™¯åœºæ™¯å›¾åƒæ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œä¾‹å¦‚é‡å¤å¯¹è±¡æˆ–éœ€è¦äººå·¥è¾“å…¥ã€‚<br>â€¢ L-MAGICæ–¹æ³•åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡å…¨æ™¯åœºæ™¯å›¾åƒï¼Œæ— éœ€å¾®è°ƒã€‚<br>â€¢ è¯¥æ–¹æ³•ç»“åˆè¶…åˆ†è¾¨ç‡å’Œå¤šè§†å›¾èåˆæŠ€æœ¯ï¼Œæé«˜äº†è¾“å‡ºè´¨é‡ã€‚<br>â€¢ å®éªŒç»“æœæ˜¾ç¤ºï¼ŒL-MAGICæ–¹æ³•ç”Ÿæˆçš„å…¨æ™¯åœºæ™¯å›¾åƒæ‹¥æœ‰æ›´å¥½çš„åœºæ™¯å¸ƒå±€å’Œè§†è§’æ¸²æŸ“è´¨é‡ã€‚<br>â€¢ ï¿½ï¿½ï¿½æ–¹æ³•æ”¯æŒå¤šç§è¾“å…¥æ¨¡å¼ï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€æ·±åº¦å›¾ã€è‰å›¾å’Œå½©è‰²è„šæœ¬ã€‚<br>â€¢ ç»“åˆæ·±åº¦ä¼°è®¡ï¼ŒL-MAGICæ–¹æ³•å¯ä»¥ç”Ÿæˆ3Dç‚¹äº‘å’Œå®ç°æµç•…çš„æ‘„åƒæœºè¿åŠ¨ã€‚<br>â€¢ ä»£ç å’Œè§†é¢‘æ¼”ç¤ºåˆ†åˆ«å‘å¸ƒåœ¨GitHubå’ŒYouTubeä¸Šã€‚</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: L-MAGICï¼šLanguage Model Assisted Generation of Images with Coherenceï¼ˆL-MAGICï¼šè¯­è¨€æ¨¡å‹è¾…åŠ©çš„ä¸€è‡´å›¾åƒç”Ÿæˆï¼‰</p></li><li><p>Authors: Zhipeng Cai, Matthias Mueller, Reiner Birkl, Diana Wofk, Shao-Yen Tseng, Junda Cheng, Gabriela Ben-Melech Stan, Vasudev Lai, Michael Paulitsch</p></li><li><p>Affiliation: è‹±ç‰¹å°”å®éªŒå®¤</p></li><li><p>Keywords: 360Â° panoramic scenes, language models, diffusion models, image generation</p></li><li><p>Urls: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01843v1">https://arxiv.org/abs/2406.01843v1</a>, Github: <a target="_blank" rel="noopener" href="https://github.com/IntelLabs/MMPano">https://github.com/IntelLabs/MMPano</a></p></li><li><p>Summary:</p></li></ol><ul><li><p>(1):å½“å‰ç”ŸæˆAIé¢†åŸŸçš„çªç ´ï¼Œä»ç„¶å­˜åœ¨ä»å•ä¸ªè¾“å…¥å›¾åƒç”Ÿæˆå…¨æ™¯æ™¯çš„æŒ‘æˆ˜ã€‚</p></li><li><p>(2):è¿‡å»çš„æ–¹æ³•ä½¿ç”¨åŸºäºæ‰©æ•£çš„è¿­ä»£æˆ–åŒæ—¶å¤šè§†å›¾ inpaintingï¼Œä½†æ˜¯ç¼ºä¹å…¨å±€åœºæ™¯å¸ƒå±€å…ˆéªŒï¼Œå¯¼è‡´è¾“å‡ºä¸­å­˜åœ¨é‡å¤å¯¹è±¡æˆ–éœ€è¦æ—¶é—´-consumingçš„äººç±»æ–‡æœ¬è¾“å…¥ã€‚</p></li><li><p>(3):æœ¬æ–‡æå‡ºL-MAGICæ–¹æ³•ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æŒ‡å¯¼å¤šè§†å›¾æ‰©æ•£ï¼Œç”Ÿæˆä¸€è‡´çš„360Â°å…¨æ™¯åœºæ™¯ã€‚</p></li><li><p>(4):å®éªŒç»“æœè¡¨æ˜ï¼ŒL-MAGICç”Ÿæˆçš„å…¨æ™¯åœºæ™¯å…·æœ‰æ›´å¥½çš„åœºæ™¯å¸ƒå±€å’Œè§†å›¾æ¸²æŸ“è´¨é‡ï¼Œäººç±»è¯„ä¼°ä¸­è¶…è¿‡70%çš„åå¥½ç‡ã€‚åŒæ—¶ï¼ŒL-MAGICè¿˜å¯ä»¥ä¸æ¡ä»¶æ‰©æ•£æ¨¡å‹ç»“åˆï¼Œæ¥å—å„ç§è¾“å…¥æ¨¡å¼ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºæ–‡æœ¬ã€æ·±åº¦å›¾è‰å›¾å’Œå½©è‰²è„šæœ¬ã€‚</p></li></ul><ol><li>Conclusion:</li></ol><ul><li><p>(1):æœ¬æ–‡å·¥ä½œçš„é‡è¦æ€§åœ¨äºæå‡ºäº†ä¸€ç§æ–°é¢–çš„å…¨æ™¯åœºæ™¯ç”Ÿæˆæ–¹æ³•ï¼Œèƒ½å¤Ÿä»å•ä¸ªè¾“å…¥å›¾åƒç”Ÿæˆé«˜è´¨é‡çš„ä¸€è‡´360Â°å…¨æ™¯åœºæ™¯ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚</p></li><li><p>(2):åˆ›æ–°ç‚¹ï¼šL-MAGICæ–¹æ³•å¼•å…¥äº†å¤§å‹è¯­è¨€æ¨¡å‹æŒ‡å¯¼æ‰©æ•£æ¨¡å‹ï¼Œå®ç°äº†å…¨å±€åœºæ™¯å¸ƒå±€çš„ç”Ÿæˆå’Œå±€éƒ¨åœºæ™¯å†…å®¹çš„å¹³æ»‘æ‰©å±•ï¼›æ€§èƒ½ï¼šå®éªŒç»“æœè¡¨æ˜ï¼ŒL-MAGICç”Ÿæˆå…¨æ™¯åœºæ™¯å…·æœ‰æ›´å¥½åœºæ™¯å¸ƒå±€å’Œè§†å›¾æ¸²æŸ“è´¨é‡ï¼Œäººç±»è¯„ä¼°ä¸­è¶…è¿‡70%çš„åå¥½ç‡ï¼›å·¥ä½œé‡ï¼šæœ¬æ–‡æå‡ºçš„æ–¹æ³•éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºå’Œæ•°æ®æ”¯æŒï¼Œä¸”éœ€è¦ä¸å…¶ä»–è®¡ç®—æœºè§†è§‰æŠ€æœ¯ç»“åˆä½¿ç”¨ï¼Œå¢åŠ äº†å®ç°çš„éš¾åº¦å’Œå·¥ä½œé‡ã€‚</p></li></ul><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/2cef8fafa1f262effcf9fe39395105f6241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/2b24aac14c8714cae85b320f1bf43dfc241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/8c079cf7eb6001aa8e2c200418660fee241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/988e794d7b687bb6bf87a36175ed49a8241286257.jpg" align="middle"></details><h2 id="DEFT-Efficient-Finetuning-of-Conditional-Diffusion-Models-by-Learning-the-Generalised-h-transform"><a href="#DEFT-Efficient-Finetuning-of-Conditional-Diffusion-Models-by-Learning-the-Generalised-h-transform" class="headerlink" title="DEFT: Efficient Finetuning of Conditional Diffusion Models by Learning   the Generalised $h$-transform"></a>DEFT: Efficient Finetuning of Conditional Diffusion Models by Learning the Generalised $h$-transform</h2><p><strong>Authors:Alexander Denker, Francisco Vargas, Shreyas Padhy, Kieran Didi, Simon Mathis, Vincent Dutordoir, Riccardo Barbano, Emile Mathieu, Urszula Julia Komorowska, Pietro Lio</strong></p><p>Generative modelling paradigms based on denoising diffusion processes have emerged as a leading candidate for conditional sampling in inverse problems. In many real-world applications, we often have access to large, expensively trained unconditional diffusion models, which we aim to exploit for improving conditional sampling. Most recent approaches are motivated heuristically and lack a unifying framework, obscuring connections between them. Further, they often suffer from issues such as being very sensitive to hyperparameters, being expensive to train or needing access to weights hidden behind a closed API. In this work, we unify conditional training and sampling using the mathematically well-understood Doobâ€™s h-transform. This new perspective allows us to unify many existing methods under a common umbrella. Under this framework, we propose DEFT (Doobâ€™s h-transform Efficient FineTuning), a new approach for conditional generation that simply fine-tunes a very small network to quickly learn the conditional $h$-transform, while keeping the larger unconditional network unchanged. DEFT is much faster than existing baselines while achieving state-of-the-art performance across a variety of linear and non-linear benchmarks. On image reconstruction tasks, we achieve speedups of up to 1.6$\times$, while having the best perceptual quality on natural images and reconstruction performance on medical images.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.01781v1">PDF</a> arXiv admin note: text overlap with arXiv:2312.09236</p><p><strong>Summary</strong><br>åŸºäºDoobâ€™s h-transformçš„æ¡ä»¶ç”Ÿæˆæ–¹æ³•DEFTï¼Œå®ç°äº†å¿«é€Ÿé«˜æ•ˆçš„æ¡ä»¶é‡‡æ ·ã€‚</p><p><strong>Key Takeaways</strong><br>â€¢ åŸºäºdenoising diffusionè¿‡ç¨‹çš„ç”Ÿæˆæ¨¡å‹åœ¨é€†é—®é¢˜ä¸­çš„æ¡ä»¶é‡‡æ ·ä¸­å±…é¢†å¯¼åœ°ä½ã€‚<br>â€¢ ç°æœ‰æ–¹æ³•ç¼ºä¹ç»Ÿä¸€çš„æ¡†æ¶ï¼Œä¸”å­˜åœ¨è¶…å‚æ•°æ•æ„Ÿã€è®­ç»ƒæˆæœ¬é«˜å’Œå°é—­APIç­‰é—®é¢˜ã€‚<br>â€¢ æœ¬å·¥ä½œæå‡ºäº†åŸºäºDoobâ€™s h-transformçš„ç»Ÿä¸€æ¡†æ¶ï¼Œç»Ÿä¸€äº†æ¡ä»¶è®­ç»ƒå’Œé‡‡æ ·ã€‚<br>â€¢ æå‡ºäº†DEFTæ–¹æ³•ï¼Œå¿«é€Ÿfine-tuneå°ç½‘ç»œä»¥å­¦ä¹ æ¡ä»¶h-transformï¼Œè€Œä¿æŒå¤§å‹æ— æ¡ä»¶ç½‘ç»œä¸å˜ã€‚<br>â€¢ DEFTæ–¹æ³•æ¯”ç°æœ‰baselineå¿«å¾—å¤šï¼Œä¸”åœ¨å¤šç§çº¿æ€§å’Œéçº¿æ€§åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å¥½çš„æ€§èƒ½ã€‚<br>â€¢ åœ¨å›¾åƒé‡å»ºä»»åŠ¡ä¸­ï¼ŒDEFTæ–¹æ³•å®ç°äº†é«˜è¾¾1.6å€çš„é€Ÿåº¦æå‡ï¼ŒåŒæ—¶å…·æœ‰è‡ªç„¶å›¾åƒçš„æœ€ä½³æ„ŸçŸ¥è´¨é‡å’ŒåŒ»ç–—å›¾åƒçš„æœ€ä½³é‡å»ºæ€§èƒ½ã€‚</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: DEFTï¼šåŸºäºæ¡ä»¶æ‰©æ•£æ¨¡å‹çš„é«˜æ•ˆå¾®è°ƒï¼ˆEfficient Finetuning of Conditional Diffusion Modelsï¼‰</p></li><li><p>Authors: Alexander Denker, Francisco Vargas, Shreyas Padhy, Kieran Didi, Simon Mathis, Vincent Dutordoir, Riccardo Barbano, Emile Mathieu, Urszula Julia Komorowska, Pietro Lio</p></li><li><p>Affiliation: ä¼¦æ•¦å¤§å­¦å­¦é™¢ï¼ˆUniversity College Londonï¼‰</p></li><li><p>Keywords: Conditional Diffusion Models, DEFT, Doobâ€™s h-transform, Generative Modelling</p></li><li><p>Urls: arXiv:2406.01781v1 , Github:None</p></li><li><p>Summary:</p></li></ol><ul><li><p>(1):è¯¥è®ºæ–‡ç ”ç©¶èƒŒæ™¯æ˜¯åŸºäºæ‰©æ•£è¿‡ç¨‹çš„ç”Ÿæˆæ¨¡å‹åœ¨é€†é—®é¢˜ä¸­çš„æ¡ä»¶é‡‡æ ·ï¼Œåˆ©ç”¨å·²ç»è®­ç»ƒå¥½çš„æ— æ¡ä»¶æ‰©æ•£æ¨¡å‹æ¥æ”¹è¿›æ¡ä»¶é‡‡æ ·ã€‚</p></li><li><p>(2):è¿‡å»çš„æ–¹æ³•ç¼ºä¹ç»Ÿä¸€çš„æ¡†æ¶ï¼Œå­˜åœ¨è¶…å‚æ•°æ•æ„Ÿã€è®­ç»ƒæ˜‚è´µæˆ–éœ€è¦è®¿é—®é—­æºAPIçš„æƒé‡çš„é—®é¢˜ã€‚è¿™äº›æ–¹æ³•çš„motivationä¹Ÿä¸æ˜¯å¾ˆå……åˆ†ã€‚</p></li><li><p>(3):æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•DEFTï¼Œä½¿ç”¨Doobâ€™s h-transformæ¥ç»Ÿä¸€æ¡ä»¶è®­ç»ƒå’Œé‡‡æ ·ã€‚DEFTåªéœ€è¦å¾®è°ƒä¸€ä¸ªå°çš„ç½‘ç»œæ¥å­¦ä¹ æ¡ä»¶h-transformï¼ŒåŒæ—¶ä¿æŒå¤§å‹æ— æ¡ä»¶ç½‘ç»œä¸å˜ã€‚</p></li><li><p>(4):è®ºæ–‡ä¸­çš„æ–¹æ³•åœ¨å›¾åƒé‡å»ºä»»åŠ¡ä¸Šå–å¾—äº†æœ€å¥½çš„æ„ŸçŸ¥è´¨é‡å’Œé‡å»ºæ€§èƒ½ï¼Œé€Ÿåº¦å¿«è¾¾1.6å€ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†è®ºæ–‡çš„ç›®ã€‚</p></li></ul><ol><li>æ–¹æ³•ï¼š</li></ol><ul><li><p>(1)ï¼šæå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•DEFTï¼Œä½¿ç”¨Doobâ€™s h-transformæ¥ç»Ÿä¸€æ¡ä»¶è®­ç»ƒå’Œé‡‡æ ·ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•çš„ç¼ºé™·ï¼Œå¦‚è¶…å‚æ•°æ•æ„Ÿã€è®­ç»ƒæ˜‚è´µæˆ–éœ€è¦è®¿é—®é—­æºAPIçš„æƒé‡çš„é—®é¢˜ã€‚</p></li><li><p>(2)ï¼šDEFTæ–¹æ³•åªéœ€è¦å¾®è°ƒä¸€ä¸ªå°çš„ç½‘ç»œæ¥å­¦ä¹ æ¡ä»¶h-transformï¼ŒåŒæ—¶ä¿æŒå¤§å‹æ— æ¡ä»¶ç½‘ç»œä¸å˜ï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p></li><li><p>(3)ï¼šåœ¨å›¾åƒé‡å»ºä»»åŠ¡ä¸Šï¼ŒDEFTæ–¹æ³•å–å¾—äº†æœ€å¥½çš„æ„ŸçŸ¥è´¨é‡å’Œé‡å»ºæ€§èƒ½ï¼Œé€Ÿåº¦å¿«è¾¾1.6å€ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p></li><li><p>(4)ï¼šé€šè¿‡å½¢å¼åŒ–çš„æ¡†æ¶ï¼ŒDEFTæ–¹æ³•æä¾›äº†ä¸€ç§å­¦ä¹ æ¡ä»¶Scoreçš„VIç›®æ ‡ï¼Œèƒ½å¤Ÿå­¦ä¹ æ¡ä»¶SDEä»æ— æ¡ä»¶SDEä¸­ï¼Œæ— éœ€è¿›è¡Œæ–¯è¿‘ä¼¼ã€‚</p></li><li><p>(5)ï¼šåœ¨éçº¿å»æ¨¡ç³Šä»»åŠ¡ä¸­ï¼ŒDEFTæ–¹æ³•ä¹Ÿå–å¾—äº†è‰¯å¥½çš„æ€§èƒ½ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨ä¸åŒä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚</p></li></ul><ol><li>Conclusion:</li></ol><pre><code>                - (1):è¯¥ç ”ç©¶å·¥ä½œçš„æ„ä¹‰åœ¨äºæå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ•°å­¦æ¡†æ¶ï¼Œè§£å†³äº†åŸºäºæ‰©æ•£è¿‡ç¨‹çš„ç”Ÿæˆæ¨¡å‹åœ¨é€†é—®é¢˜ä¸­çš„æ¡ä»¶é‡‡æ ·é—®é¢˜æé«˜äº†æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚

                - (2):åˆ›æ–°ç‚¹ï¼šæå‡ºäº†åŸºäºDoob's h-transformçš„æ¡ä»¶æ‰©æ•£æ¨¡å‹å¾®è°ƒæ–¹æ³•DEFTï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•çš„ç¼ºé™·ï¼›æ€§èƒ½ï¼šåœ¨å›¾åƒé‡å»ºä»»åŠ¡ä¸Šå–å¾—äº†æœ€å¥½çš„æ„ŸçŸ¥è´¨é‡å’Œé‡å»ºæ€§èƒ½ï¼Œé€Ÿåº¦å¿«è¾¾1.6å€ï¼›å·¥ä½œé‡ï¼šåªéœ€è¦å¾®è°ƒä¸€ä¸ªå°çš„ç½‘ç»œæ¥å­¦ä¹ æ¡ä»¶h-transformï¼ŒåŒæ—¶ä¿æŒå¤§å‹æ— æ¡ä»¶ç½‘ç»œä¸å˜ï¼Œæé«˜äº†æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡ã€‚
</code></pre><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/1c631e15a39c81660b3afb7dea0a5956241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/c18ab4b4f8e015a4e3e573135a023ca5241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/3fddddbd0dbd3bef9659308276311521241286257.jpg" align="middle"></details><h2 id="Long-and-Short-Guidance-in-Score-identity-Distillation-for-One-Step-Text-to-Image-Generation"><a href="#Long-and-Short-Guidance-in-Score-identity-Distillation-for-One-Step-Text-to-Image-Generation" class="headerlink" title="Long and Short Guidance in Score identity Distillation for One-Step   Text-to-Image Generation"></a>Long and Short Guidance in Score identity Distillation for One-Step Text-to-Image Generation</h2><p><strong>Authors:Mingyuan Zhou, Zhendong Wang, Huangjie Zheng, Hai Huang</strong></p><p>Diffusion-based text-to-image generation models trained on extensive text-image pairs have shown the capacity to generate photorealistic images consistent with textual descriptions. However, a significant limitation of these models is their slow sample generation, which requires iterative refinement through the same network. In this paper, we enhance Score identity Distillation (SiD) by developing long and short classifier-free guidance (LSG) to efficiently distill pretrained Stable Diffusion models without using real training data. SiD aims to optimize a model-based explicit score matching loss, utilizing a score-identity-based approximation alongside the proposed LSG for practical computation. By training exclusively with fake images synthesized with its one-step generator, SiD equipped with LSG rapidly improves FID and CLIP scores, achieving state-of-the-art FID performance while maintaining a competitive CLIP score. Specifically, its data-free distillation of Stable Diffusion 1.5 achieves a record low FID of 8.15 on the COCO-2014 validation set, with a CLIP score of 0.304 at an LSG scale of 1.5, and a FID of 9.56 with a CLIP score of 0.313 at an LSG scale of 2. We will make our PyTorch implementation and distilled Stable Diffusion one-step generators available at <a target="_blank" rel="noopener" href="https://github.com/mingyuanzhou/SiD-LSG">https://github.com/mingyuanzhou/SiD-LSG</a></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.01561v1">PDF</a></p><p>sorry, æ‚¨çš„ipå·²ç”±äºè§¦å‘é˜²æ»¥ç”¨æ£€æµ‹è€Œè¢«å°ç¦,å¯èƒ½æ˜¯å› ä¸ºä½¿ç”¨äº‘æœåŠ¡å™¨å¦‚è…¾è®¯äº‘æˆ–è€…å›½å¤–ä»£ç†(vpn)è®¿é—®æœ¬ç½‘ç«™ï¼Œå¦‚æœä½¿ç”¨äº†vpnï¼Œå…³é—­vpnæˆ–ä»£ç†å³å¯ç»§ç»­ä½¿ç”¨,æœ¬æœåŠ¡ç½‘å€æ˜¯<a target="_blank" rel="noopener" href="https://chat18.aichatos8.com">https://chat18.aichatos8.com</a> å¦‚éœ€åˆä½œæ¥å£è°ƒç”¨è¯·è”ç³»å¾®ä¿¡chatkf123 æˆ–è€…å‰å¾€ <a target="_blank" rel="noopener" href="https://binjie09.shop">https://binjie09.shop</a> è‡ªåŠ©è´­ä¹°key, è®¤ä¸ºæ˜¯è¯¯å°éœ€è¦è§£å°çš„è¯·å‰å¾€<a target="_blank" rel="noopener" href="https://www.ip.cn/">https://www.ip.cn/</a> æŸ¥è¯¢ipä¿¡æ¯,å¹¶å‘é€ä¿¡æ¯è‡³é‚®ä»¶ gpt33@binjie.site ï¼Œç«™é•¿ä¼šå®šæœŸçœ‹é‚®ä»¶å¹¶å¤„ç†è§£å°å’Œåˆä½œé—®é¢˜ï¼Œå¦‚éœ€è°ƒç”¨æ¥å£è¯·è§æ¥å£æ–‡æ¡£<a target="_blank" rel="noopener" href="https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82ï¼Œå¦‚éœ€åˆä½œæ¥å£è°ƒç”¨è¯·è”ç³»å¾®ä¿¡chatkf123">https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82ï¼Œå¦‚éœ€åˆä½œæ¥å£è°ƒç”¨è¯·è”ç³»å¾®ä¿¡chatkf123</a> æˆ–è€…å‰å¾€ <a target="_blank" rel="noopener" href="https://cat.gptxyy.cn">https://cat.gptxyy.cn</a> æ³¨å†Œä½¿ç”¨ï¼ˆå¯ä»˜è´¹ä½¿ç”¨gpt4 æ³¨å†Œå¯å…è´¹ä½¿ç”¨3.5ï¼‰</p><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: ä¸€æ­¥æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„é•¿çŸ­æŒ‡å¯¼åˆ†æ•°èº«ä»½ Distillation (Long and Short Guidance in Score Identity Distillation for One-Step Text-to-Image Generation)</p></li><li><p>Authors: Mingyuan Zhou, Zhendong Wang, Huangjie Zheng, Hai Huang</p></li><li><p>Affiliation: å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡</p></li><li><p>Keywords: text-to-image generation, Score identity Distillation, Long-Short Guidance, Stable Diffusion</p></li><li><p>Urls: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01561v1">https://arxiv.org/abs/2406.01561v1</a>, Github: <a target="_blank" rel="noopener" href="https://github.com/mingyuanzhou/SiD-LSG">https://github.com/mingyuanzhou/SiD-LSG</a></p></li><li><p>Summary:</p><ul><li><p>(1):æœ¬æ–‡ç ”ç©¶çš„æ˜¯åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥ç”Ÿæˆ photorealistic å›¾åƒï¼Œä½†å…¶ç”Ÿæˆé€Ÿåº¦æ…¢ï¼Œéœ€è¦è¿­ä»£ refinementã€‚</p></li><li><p>(2):è¿‡å»çš„æ–¹æ³•æ˜¯ä½¿ç”¨ Score identity Distillationï¼ˆSiDï¼‰æ¥ distill è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œä½†æ˜¯è¿™äº›æ–¹æ³•éœ€è¦ä½¿ç”¨çœŸå®çš„è®­ç»ƒæ•°æ®ï¼Œå¹¶ä¸”è®¡ç®—å¤æ‚ã€‚è¯¥æ–¹æ³•çš„motivationæ˜¯æé«˜ç”Ÿæˆé€Ÿåº¦å’Œè´¨é‡ã€‚</p></li><li><p>(3):æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œä½¿ç”¨é•¿çŸ­æŒ‡å¯¼åˆ†æ•°èº«ä»½ Distillationï¼ˆLSGï¼‰æ¥ distill é¢„è®­ç»ƒçš„ Stable Diffusion æ¨¡å‹ï¼Œè¯¥æ–¹æ³•ä¸éœ€è¦ä½¿ç”¨çœŸå®çš„è®­ç»ƒæ•°æ®ï¼Œå¹¶ä¸”å¯ä»¥å¿«é€Ÿæé«˜ FID å’Œ CLIP è¯„åˆ†ã€‚</p></li><li><p>(4):æœ¬æ–‡çš„æ–¹æ³•åœ¨ COCO-2014 éªŒè¯é›†ä¸Šè¾¾åˆ° state-of-the-art çš„ FID æ€§èƒ½ï¼ˆ8.15ï¼‰ï¼ŒåŒæ—¶ä¿æŒç«äº‰æ€§çš„ CLIP è¯„åˆ†ï¼ˆ0.304ï¼‰ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p></li></ul></li><li>æ–¹æ³•ï¼š</li></ol><ul><li><p>(1):æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹æ³•ï¼ŒåŸºäºæ‰©æ•£æ¨¡å‹çš„Score identity Distillationï¼ˆSiDï¼‰ï¼Œä½¿ç”¨é•¿çŸ­æŒ‡å¯¼åˆ†æ•°èº«ä»½ Distillationï¼ˆLSGï¼‰æ¥ distill é¢„è®­ç»ƒçš„ Stable Diffusion æ¨¡å‹ã€‚</p></li><li><p>(2):è¯¥æ–¹æ³•é¦–å…ˆä½¿ç”¨é¢„è®­ç»ƒçš„ Stable Diffusion æ¨¡å‹ç”Ÿæˆå›¾åƒï¼Œç„¶åä½¿ç”¨LSGç­–ç•¥æ¥æŒ‡å¯¼ç”Ÿæˆå›¾åƒçš„åˆ†æ•°ï¼Œæé«˜ç”Ÿæˆå›¾åƒçš„è´¨é‡å’Œé€Ÿåº¦ã€‚</p></li><li><p>(3):åœ¨LSGç­–ç•¥ä¸­ï¼Œä½¿ç”¨äº†ä¸åŒçš„æŒ‡å¯¼ scalesï¼ˆÎºï¼‰ï¼Œä¾‹å¦‚1.5ã€2ã€3ã€4.5ç­‰ï¼Œæ¥æ§åˆ¶ç”Ÿæˆå›¾åƒçš„è´¨é‡å’Œé€Ÿåº¦ã€‚</p></li><li><p>(4):ä¸ºäº†è¯„ä¼°è¯¥æ–¹æ³•çš„æ€§èƒ½ï¼Œæœ¬æ–‡ä½¿ç”¨äº†å¤šç§è¯„ä¼°æŒ‡æ ‡ï¼ŒåŒ…æ‹¬FIDã€CLIPã€HPSv2ç­‰ï¼Œå¹¶ä¸å…¶ä»–state-of-the-artæ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚</p></li><li><p>(5):å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒï¼Œä¸”å…·æœ‰è¾ƒå¿«çš„ç”Ÿæˆé€Ÿåº¦ï¼Œè¾¾åˆ°äº†state-of-the-artçš„æ€§èƒ½ã€‚</p></li><li><p>(6):ä¸ºäº†è¿›ä¸€æ­¥æ¢ç©¶è¯¥æ–¹æ³•çš„æ€§èƒ½ï¼Œæœ¬æ–‡è¿˜è¿›è¡Œäº†äº›æ¶ˆèå®éªŒï¼Œä¾‹å¦‚æ”¹å˜æ‰¹å¤§å°ã€å­¦ä¹ ç‡ã€æŒ‡ scales ç­‰ï¼Œæ¥è¯„ä¼°è¯¥æ–¹æ³•çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p></li><li><p>(7):å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å…·æœ‰è¾ƒå¥½çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œå¯ä»¥åœ¨ä¸åŒçš„å®éªŒè®¾ç½®ä¸‹ç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒã€‚</p></li></ul><ol><li>Conclusion:</li></ol><ul><li><p>(1):æœ¬æ–‡çš„å·¥ä½œå¯¹åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆé¢†åŸŸå…·æœ‰é‡è¦æ„ä¹‰ï¼Œå¯ä»¥æé«˜ç”Ÿæˆé€Ÿåº¦å’Œè´¨é‡ï¼Œå¹¶ä¸”å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚</p></li><li><p>(2):åˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§æ–°çš„é•¿çŸ­æŒ‡å¯¼åˆ†æ•°èº«ä»½Distillationï¼ˆLSGï¼‰ç­–ç•¥ï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„Stable Diffusionæ¨¡å‹ç”Ÿæˆå›¾åƒï¼Œå¹¶ä¸”ä¸éœ€è¦ä½¿ç”¨çœŸå®çš„è®­ç»ƒæ•°æ®ï¼›æ€§èƒ½ï¼šåœ¨COCO-2014éªŒè¯é›†ä¸Šè¾¾åˆ°äº†state-of-the-artï¿½ï¿½FIDæ€§èƒ½ï¼ˆ8.15ï¼‰ï¼ŒåŒæ—¶ä¿æŒç«äº‰æ€§çš„CLIPè¯„åˆ†ï¼ˆ0.304ï¼‰ï¼›å·¥ä½œé‡ï¼šå®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å…·æœ‰è¾ƒå¥½çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œå¯ä»¥åœ¨ä¸åŒçš„å®éªŒè®¾ç½®ä¸‹ç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒï¼Œä½†éœ€è¦è¿›ä¸€æ­¥æ¢ç©¶è¯¥æ–¹æ³•çš„å¯æ‰©å±•æ€§å’Œå®é™…åº”ç”¨ä»·å€¼ã€‚</p></li></ul><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/7d504c9b651f2ab6fc7f9ce6d5e64623241286257.jpg" align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://article.biliimg.com/bfs/new_dyn/21fbd0387701d9ae2e365df8cc945589241286257.jpg" align="middle"></details></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>æ–‡ç« ä½œè€…:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>æ–‡ç« é“¾æ¥:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io/2024/06/05/Paper/2024-06-05/Diffusion%20Models/">https://kedreamix.github.io/2024/06/05/Paper/2024-06-05/Diffusion Models/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>ç‰ˆæƒå£°æ˜:</span> <span class="post-copyright-info">æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ«å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨ <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥è‡ª <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>ï¼</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Diffusion-Models/">Diffusion Models</a></div><div class="post_share"><div class="social-share" data-image="https://pica.zhimg.com/80/v2-890676236f48f9a7d915a0c42c40aa38.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>èµåŠ©</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/06/05/Paper/2024-06-05/3DGS/" title="3DGS"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-2196bb11e15de7b5d3440823b4c92ca5.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">ä¸Šä¸€ç¯‡</div><div class="prev_info">3DGS</div></div></a></div><div class="next-post pull-right"><a href="/2024/06/05/Paper/2024-06-05/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/" title="å…ƒå®‡å®™/è™šæ‹Ÿäºº"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2bdb0ecbbc3a0a2420781e472b68ba52.png" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">ä¸‹ä¸€ç¯‡</div><div class="next_info">å…ƒå®‡å®™/è™šæ‹Ÿäºº</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>ç›¸å…³æ¨è</span></div><div class="relatedPosts-list"><div><a href="/2024/03/03/Paperscape/EMO/" title="EMO Emote Portrait Alive - é˜¿é‡ŒHumanAIGC"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-6492e24fb03ffa98135dc584535ab7d9.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-03</div><div class="title">EMO Emote Portrait Alive - é˜¿é‡ŒHumanAIGC</div></div></a></div><div><a href="/2024/01/24/Paper/2024-01-24/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-71a37c439c6714e8867560f580599d2f.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-24</div><div class="title">Diffusion Models</div></div></a></div><div><a href="/2024/01/30/Paper/2024-01-30/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-e55358c77a9d65f15701e8f33262e2a4.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-30</div><div class="title">Diffusion Models</div></div></a></div><div><a href="/2024/02/13/Paper/2024-02-13/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-3709a9941aada6c4d3ed35934e311765.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-13</div><div class="title">Diffusion Models</div></div></a></div><div><a href="/2024/02/02/Paper/2024-02-02/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-5920453c69c00995f18077b22d4a790e.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-02</div><div class="title">Diffusion Models</div></div></a></div><div><a href="/2024/02/23/Paper/2024-02-23/Diffusion%20Models/" title="Diffusion Models"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-ff425802a32a4519e30b9044a3eed1e8.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-23</div><div class="title">Diffusion Models</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>ç›®å½•</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024-06-05-%E6%9B%B4%E6%96%B0"><span class="toc-text">2024-06-05 æ›´æ–°</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#CamCo-Camera-Controllable-3D-Consistent-Image-to-Video-Generation"><span class="toc-text">CamCo: Camera-Controllable 3D-Consistent Image-to-Video Generation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Guiding-a-Diffusion-Model-with-a-Bad-Version-of-Itself"><span class="toc-text">Guiding a Diffusion Model with a Bad Version of Itself</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Stable-Pose-Leveraging-Transformers-for-Pose-Guided-Text-to-Image-Generation"><span class="toc-text">Stable-Pose: Leveraging Transformers for Pose-Guided Text-to-Image Generation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Learning-Image-Priors-through-Patch-based-Diffusion-Models-for-Solving-Inverse-Problems"><span class="toc-text">Learning Image Priors through Patch-based Diffusion Models for Solving Inverse Problems</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RoomTex-Texturing-Compositional-Indoor-Scenes-via-Iterative-Inpainting"><span class="toc-text">RoomTex: Texturing Compositional Indoor Scenes via Iterative Inpainting</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Flash-Diffusion-Accelerating-Any-Conditional-Diffusion-Model-for-Few-Steps-Image-Generation"><span class="toc-text">Flash Diffusion: Accelerating Any Conditional Diffusion Model for Few Steps Image Generation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#A-Survey-of-Transformer-Enabled-Time-Series-Synthesis"><span class="toc-text">A Survey of Transformer Enabled Time Series Synthesis</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#GraVITON-Graph-based-garment-warping-with-attention-guided-inversion-for-Virtual-tryon"><span class="toc-text">GraVITON: Graph based garment warping with attention guided inversion for Virtual-tryon</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Follow-Your-Emoji-Fine-Controllable-and-Expressive-Freestyle-Portrait-Animation"><span class="toc-text">Follow-Your-Emoji: Fine-Controllable and Expressive Freestyle Portrait Animation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Cross-Domain-Graph-Data-Scaling-A-Showcase-with-Diffusion-Models"><span class="toc-text">Cross-Domain Graph Data Scaling: A Showcase with Diffusion Models</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#L-MAGIC-Language-Model-Assisted-Generation-of-Images-with-Coherence"><span class="toc-text">L-MAGIC: Language Model Assisted Generation of Images with Coherence</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DEFT-Efficient-Finetuning-of-Conditional-Diffusion-Models-by-Learning-the-Generalised-h-transform"><span class="toc-text">DEFT: Efficient Finetuning of Conditional Diffusion Models by Learning the Generalised $h$-transform</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Long-and-Short-Guidance-in-Score-identity-Distillation-for-One-Step-Text-to-Image-Generation"><span class="toc-text">Long and Short Guidance in Score identity Distillation for One-Step Text-to-Image Generation</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image:url('https://pica.zhimg.com/80/v2-890676236f48f9a7d915a0c42c40aa38.png')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>æ¡†æ¶</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>ä¸»é¢˜</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="é˜…è¯»æ¨¡å¼"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="ç®€ç¹è½¬æ¢">ç°¡</button><button id="darkmode" type="button" title="æµ…è‰²å’Œæ·±è‰²æ¨¡å¼è½¬æ¢"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="å•æ å’ŒåŒæ åˆ‡æ¢"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="è®¾ç½®"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="ç›®å½•"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="å›åˆ°é¡¶éƒ¨"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js").then((()=>{pangu.autoSpacingPage()}))}function panguInit(){panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.1},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script></div><script id="canvas_nest" defer color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">æœç´¢</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>æ•°æ®åº“åŠ è½½ä¸­</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="æœç´¢æ–‡ç« " type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><script data-pjax>function butterfly_swiper_injector_config(){var a=document.getElementById("recent-posts");console.log("å·²æŒ‚è½½butterfly_swiper"),a.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/25/Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="2024/01/25/Paper/3DGS Survey/" alt="">3DGSç»¼è¿°ä»¥åŠå¯¹3DGSçš„ç†è§£ï¼šA Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGSç»¼è¿°ä»¥åŠå¯¹3DGSçš„ç†è§£ï¼šA Survey on 3D Gaussian Splatting ä»Šå¤©æƒ³ä»‹ç»çš„æ˜¯`ZJU`å¸¦æ¥çš„`3DGS`çš„é¦–ç¯‡ç»¼è¿°`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="2024/01/25/Paper/3DGS Survey/" alt="">è¯¦æƒ…   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/12/CUDA/Pytorch+cppcuda extension å­¦ä¹ /" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="2023/12/12/CUDA/Pytorch+cppcuda extension å­¦ä¹ /" alt="">CUDAç¼–ç¨‹å­¦ä¹ ï¼šè‡ªå®šä¹‰Pytorch+cpp/cuda extension</a><div class="blog-slider__text">è™½ç„¶è¯´PyTorchæä¾›äº†ä¸°å¯Œçš„ä¸ç¥ç»ç½‘ç»œã€å¼ é‡ä»£æ•°ã€æ•°æ®å¤„ç†ç­‰ç›¸å…³çš„æ“ä½œï¼Œä½†æ˜¯æœ‰æ—¶å€™ä½ å¯èƒ½éœ€è¦**æ›´å®šåˆ¶åŒ–çš„æ“ä½œ**ï¼Œæ¯”å¦‚ä½¿ç”¨è®ºæ–‡ä¸­çš„æ–°å‹æ¿€æ´»å‡½æ•°ï¼Œæˆ–è€…å®ç°ä½œä¸ºç ”ç©¶ä¸€éƒ¨åˆ†å¼€å‘çš„æ“ä½œã€‚åœ¨PyTorchä¸­ï¼Œæœ€ç®€å•çš„é›†æˆè‡ªå®šä¹‰æ“ä½œçš„æ–¹å¼æ˜¯åœ¨Pythonä¸­ç¼–å†™ï¼Œé€šè¿‡æ‰©å±•Functionå’ŒModuleæ¥å®ç°ï¼Œ</div><a class="blog-slider__button" href="2023/12/12/CUDA/Pytorch+cppcuda extension å­¦ä¹ /" alt="">è¯¦æƒ…   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">æ•°å­—äººå¯¹è¯ç³»ç»Ÿ - Linly-Talker â€”â€” â€œæ•°å­—äººäº¤äº’ï¼Œä¸è™šæ‹Ÿçš„è‡ªå·±äº’åŠ¨â€</a><div class="blog-slider__text">Linly-Talkeræ˜¯ä¸€ä¸ªå°†å¤§å‹è¯­è¨€æ¨¡å‹ä¸è§†è§‰æ¨¡å‹ç›¸ç»“åˆçš„æ™ºèƒ½AIç³»ç»Ÿ,åˆ›å»ºäº†ä¸€ç§å…¨æ–°çš„äººæœºäº¤äº’æ–¹å¼ã€‚å®ƒé›†æˆäº†å„ç§æŠ€æœ¯,ä¾‹å¦‚Whisperã€Linlyã€å¾®è½¯è¯­éŸ³æœåŠ¡å’ŒSadTalkerä¼šè¯´è¯çš„ç”Ÿæˆç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿéƒ¨ç½²åœ¨Gradioä¸Š,å…è®¸ç”¨æˆ·é€šè¿‡æä¾›å›¾åƒä¸AIåŠ©æ‰‹è¿›è¡Œäº¤è°ˆã€‚ç”¨æˆ·å¯ä»¥æ ¹æ®è‡ªå·±çš„å–œå¥½è¿›è¡Œè‡ªç”±çš„å¯¹è¯æˆ–å†…å®¹ç”Ÿæˆã€‚</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">è¯¦æƒ…   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker/" alt="">æ•°å­—äººå¯¹è¯ç³»ç»Ÿ - Linly-Talker â€”â€” â€œæ•°å­—äººäº¤äº’ï¼Œä¸è™šæ‹Ÿçš„è‡ªå·±äº’åŠ¨â€</a><div class="blog-slider__text">Linly-Talkeræ˜¯ä¸€ä¸ªå°†å¤§å‹è¯­è¨€æ¨¡å‹ä¸è§†è§‰æ¨¡å‹ç›¸ç»“åˆçš„æ™ºèƒ½AIç³»ç»Ÿ,åˆ›å»ºäº†ä¸€ç§å…¨æ–°çš„äººæœºäº¤äº’æ–¹å¼ã€‚å®ƒé›†æˆäº†å„ç§æŠ€æœ¯,ä¾‹å¦‚Whisperã€Linlyã€å¾®è½¯è¯­éŸ³æœåŠ¡å’ŒSadTalkerä¼šè¯´è¯çš„ç”Ÿæˆç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿéƒ¨ç½²åœ¨Gradioä¸Š,å…è®¸ç”¨æˆ·é€šè¿‡æä¾›å›¾åƒä¸AIåŠ©æ‰‹è¿›è¡Œäº¤è°ˆã€‚ç”¨æˆ·å¯ä»¥æ ¹æ®è‡ªå·±çš„å–œå¥½è¿›è¡Œè‡ªç”±çš„å¯¹è¯æˆ–å†…å®¹ç”Ÿæˆã€‚</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker/" alt="">è¯¦æƒ…   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">è¶…èµçš„æ•°å­—äººç”ŸæˆçŸ¥è¯†åº“  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">è¶…èµçš„æ•°å­—äººç”ŸæˆçŸ¥è¯†åº“  Awesome-Talking-Head-Synthesisï¼Œ è¿™ä»½èµ„æºåº“æ•´ç†äº†ä¸ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ(GAN)å’Œç¥ç»è¾å°„åœº(NeRF)ç›¸å…³çš„è®ºæ–‡ã€ä»£ç å’Œèµ„æº,é‡ç‚¹å…³æ³¨åŸºäºå›¾åƒå’ŒéŸ³é¢‘çš„è™šæ‹Ÿè®²è¯å¤´åˆæˆè®ºæ–‡åŠå·²å‘å¸ƒä»£ç ã€‚å¦‚æœæ‚¨è§‰å¾—è¿™ä¸ªä»“åº“æœ‰ç”¨,è¯·starâ­æ”¯æŒ!</div><a class="blog-slider__button" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">è¯¦æƒ…   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/17/Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023/12/17/Project/ChatPaperFree/" alt="">ChatPaperFree GeminiProï¼ˆä¸€åˆ†é’Ÿè¯»è®ºæ–‡ï¼‰</a><div class="blog-slider__text">ChatPaperFreeæ˜¯ä¸€ä¸ªåŸºäºChatGPTçš„è‡ªåŠ¨è®ºæ–‡æ‘˜è¦ç”Ÿæˆå™¨ï¼Œåœ¨ChatPaperçš„åŸºç¡€ä¸Šè¿›è¡Œçš„æ›´æ–°ï¼Œé‡‡ç”¨äº†æœ€è¿‘ç”±Googleå¼€æºçš„Gemini Proå¤§æ¨¡å‹ã€‚ç›®å‰,æˆ‘ä»¬èƒ½å¤Ÿå¯¹ç”¨æˆ·è¾“å…¥çš„è®ºæ–‡è¿›è¡Œè‡ªåŠ¨æ€»ç»“ã€‚æœªæ¥,æˆ‘è¿˜è®¡åˆ’åŠ å…¥å¯¹è®ºæ–‡å›¾ç‰‡/è¡¨æ ¼/å…¬å¼çš„è¯†åˆ« extraction,ä»è€Œç”Ÿæˆæ›´å…¨é¢è€Œæ˜“è¯»çš„æ€»ç»“ã€‚</div><a class="blog-slider__button" href="2023/12/17/Project/ChatPaperFree/" alt="">è¯¦æƒ…   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script></body></html>