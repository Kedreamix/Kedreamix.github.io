<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>CUDA编程实践：LLTM加速优化实践</title>
    <url>/2023/12/12/CUDA/CUDA%E7%BC%96%E7%A8%8B%E5%AE%9E%E8%B7%B5%EF%BC%9ALLTM%E5%8A%A0%E9%80%9F%E4%BC%98%E5%8C%96%E5%AE%9E%E8%B7%B5/</url>
    <content><![CDATA[<p>Github：<a href="https://github.com/Kedreamix/pytorch-cppcuda-tutorial">https://github.com/Kedreamix/pytorch-cppcuda-tutorial</a></p>
<p>前馈知识：<a href="https://zhuanlan.zhihu.com/p/671704557">Kedreamix：CUDA编程学习：自定义Pytorch+cpp/cuda extension</a></p>
<p>接下来再进行一个实践，根据pytorch官网的一个<a href="https://pytorch.org/tutorials/advanced/cpp_extension.html#">应用扩展文档</a>来一起来实现这个LLTM的神经网络的实现，这是官方对应的代码 <a href="https://github.com/pytorch/extension-cpp。">https://github.com/pytorch/extension-cpp。</a></p>
<p>首先介绍一下，假设有一种新的循环单元，这个循环单元类似于 LSTM，但不同之处在于它没有遗忘门，并使用指数线性单元 (<a href="https://so.csdn.net/so/search?q=ELU&amp;spm=1001.2101.3001.7020">ELU</a>) 作为其内部激活函数。因为这个单元能够记忆很久，我们称之为 LLTM，或 Long-Long-Term-Memory 单元。</p>
<p>LLTM 与普通 LSTM 的两种不同之处非常重要，以至于我们无法为我们的目的配置 PyTorch 的 LSTM 算子，因此我们必须创建一个自定义算子。第一个也是最简单的方法——可能在所有情况下都是很好的第一步——是用 Python 在普通的 PyTorch 中实现我们想要的功能。为此，我们需要继承 torch.nn.Module 并实现 LLTM 的 forward。</p>
<h2 id="Pytorch普通实现"><a href="#Pytorch普通实现" class="headerlink" title="Pytorch普通实现"></a>Pytorch普通实现</h2><p>接下来我们就用普通的pytorch对其进行实现LLTM，具体代码如下</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LLTM</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_features, state_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(LLTM, self).__init__()</span><br><span class="line">        self.input_features = input_features</span><br><span class="line">        self.state_size = state_size</span><br><span class="line">        <span class="comment"># 3 * state_size for input gate, output gate and candidate cell gate.</span></span><br><span class="line">        <span class="comment"># input_features + state_size because we will multiply with [input, h].</span></span><br><span class="line">        self.weights = torch.nn.Parameter(</span><br><span class="line">            torch.empty(<span class="number">3</span> * state_size, input_features + state_size))</span><br><span class="line">        self.bias = torch.nn.Parameter(torch.empty(<span class="number">3</span> * state_size))</span><br><span class="line">        self.reset_parameters()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reset_parameters</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        重置模型参数的方法。</span></span><br><span class="line"><span class="string">        初始化权重和偏差的值，使用均匀分布和标准差进行初始化。</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        stdv = <span class="number">1.0</span> / math.sqrt(self.state_size)</span><br><span class="line">        <span class="keyword">for</span> weight <span class="keyword">in</span> self.parameters():</span><br><span class="line">            weight.data.uniform_(-stdv, +stdv)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span>, state</span>):</span><br><span class="line">        old_h, old_cell = state</span><br><span class="line">        X = torch.cat([old_h, <span class="built_in">input</span>], dim=<span class="number">1</span>)</span><br><span class="line">        gate_weights = F.linear(X, self.weights, self.bias)</span><br><span class="line">        gates = gate_weights.chunk(<span class="number">3</span>, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        input_gate = torch.sigmoid(gates[<span class="number">0</span>])</span><br><span class="line">        output_gate = torch.sigmoid(gates[<span class="number">1</span>])</span><br><span class="line">        candidate_cell = F.elu(gates[<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">        new_cell = old_cell + candidate_cell * input_gate</span><br><span class="line">        new_h = torch.tanh(new_cell) * output_gate</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> new_h, new_cell</span><br></pre></td></tr></tbody></table></figure>
<h2 id="Python扩展实现"><a href="#Python扩展实现" class="headerlink" title="Python扩展实现"></a>Python扩展实现</h2><p>我们在大多数的时候，都是用上述的方法进行对Pytorch进行扩展，因为 PyTorch 对 CPU 和 GPU 的操作实现了高度优化，并由 <a href="https://developer.nvidia.com/cudnn">NVIDIA cuDNN</a>、<a href="https://software.intel.com/en-us/mkl">Intel MKL</a> 或 <a href="https://github.com/Maratyszcza/NNPACK">NNPACK</a> 等库提供支持。因此，通常情况下，上述的PyTorch代码已经足够快速。</p>
<p>然而，在某些情况下，还有进一步提升性能的空间。这是因为PyTorch可能不了解我们实现的特定算法，它只知道我们使用的各种操作。因此，它可能会逐个执行这些操作的内核（可能涉及启动CUDA内核），这会导致累积的开销变得很大。此外，Python本身也存在一些速度限制。</p>
<p>因此，一种明显的加速方法是使用C++（或CUDA）对部分代码进行重写，并将特定的操作组合起来。组合意味着将许多函数的实现合并为一个函数，这样可以启动较少的内核，并且可以通过提高全局数据流的可见性来执行其他优化。</p>
<p>接下来，我们将分别使用<strong>Python，C++，Pytorch</strong>扩展来实现LLTM（Long Long-Term Memory）的组合版本。我们将从使用纯C++编写它开始，使用<a href="https://github.com/zdevito/ATen">ATen</a>库，该库为PyTorch的大部分后端提供支持。然后，我们将通过将模型的部分移动到CUDA内核中，以利用GPU提供的大规模并行性，进一步提高速度。</p>
<p>在上述的代码中，为了后续对写C++扩展有一个更好的理解，所以再加上一部分首先实现反向传播的代码。所以我们需要手写我们对应函数出现的微分，这里面我们就需要先计算基础函数的微分函数，分别是的<code>d_sigmoid</code>，<code>d_tanh</code>和<code>d_elu</code>，后续可以对其进行复用。</p>
<hr>
<p>在这一部分的代码中，结果实际上的方式和上面代码是一样的，主要是就是独立实现了一下对应的反向传播部分，方便后续进行训练的反向传播的CUDA编写，并且让大家也更加理解一下对应底层的C++和CUDA反向传播的代码的编写原理。</p>
<p>在前面我们有介绍过<code>ctx</code>可以保存我们反向传播需要的参数，所以这里使用了<code>ctx.save_for_backward(X, weights, input_gate, output_gate, old_cell,new_cell, candidate_cell, gate_weights)</code>，最后返回了两个值，分别是<code>new_h, new_cell</code>，并且这两部分我们也需要在反向传播中传入两个参数进行求微分，在<code>forward</code>函数中，我们有四个参数<code>input, weights, bias, old_h, old_cell</code>，所以反向传播的最后返回值也分别是<code>d_input, d_weights, d_bias, d_old_h, d_old_cell</code>进行一一对应。</p>
<h3 id="基础函数"><a href="#基础函数" class="headerlink" title="基础函数"></a>基础函数</h3><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">d_sigmoid</span>(<span class="params">z</span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    计算sigmoid函数的导数</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    z (tensor): 输入张量</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回值:</span></span><br><span class="line"><span class="string">    d_sigmoid (tensor): sigmoid函数的导数张量</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    s = torch.sigmoid(z)</span><br><span class="line">    <span class="keyword">return</span> (<span class="number">1</span> - s) * s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">d_tanh</span>(<span class="params">z</span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    计算tanh函数的导数</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    z (tensor): 输入张量</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回值:</span></span><br><span class="line"><span class="string">    d_tanh (tensor): tanh函数的导数张量</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    t = torch.tanh(z)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> - (t * t)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">d_elu</span>(<span class="params">z, alpha=<span class="number">1.0</span></span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    计算ELU函数的导数</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    z (tensor): 输入张量</span></span><br><span class="line"><span class="string">    alpha (float): ELU函数的alpha参数，默认为1.0</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回值:</span></span><br><span class="line"><span class="string">    d_elu (tensor): ELU函数的导数张量</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    e = z.exp()</span><br><span class="line">    mask = (alpha * (e - <span class="number">1</span>)) &lt; <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> (z &gt; <span class="number">0</span>).type_as(z) + mask.type_as(z) * (alpha * e)</span><br></pre></td></tr></tbody></table></figure>
<h3 id="前向和反向传播"><a href="#前向和反向传播" class="headerlink" title="前向和反向传播"></a>前向和反向传播</h3><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 定义一个自定义的LLTM函数，继承自torch.autograd.Function类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LLTMPythonFunction</span>(torch.autograd.Function):</span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">ctx, <span class="built_in">input</span>, weights, bias, old_h, old_cell</span>):</span><br><span class="line">        X = torch.cat([old_h, <span class="built_in">input</span>], dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        gate_weights = F.linear(X, weights, bias)</span><br><span class="line">        gates = gate_weights.chunk(<span class="number">3</span>, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        input_gate = torch.sigmoid(gates[<span class="number">0</span>])</span><br><span class="line">        output_gate = torch.sigmoid(gates[<span class="number">1</span>])</span><br><span class="line">        candidate_cell = F.elu(gates[<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">        new_cell = old_cell + candidate_cell * input_gate</span><br><span class="line">        new_h = torch.tanh(new_cell) * output_gate</span><br><span class="line"></span><br><span class="line">        ctx.save_for_backward(X, weights, input_gate, output_gate, old_cell,</span><br><span class="line">                            new_cell, candidate_cell, gate_weights)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> new_h, new_cell</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">ctx, grad_h, grad_cell</span>):</span><br><span class="line">        <span class="comment"># 从上下文中获取保存的变量</span></span><br><span class="line">        X, weights, input_gate, output_gate, old_cell = ctx.saved_variables[:<span class="number">5</span>]</span><br><span class="line">        new_cell, candidate_cell, gate_weights = ctx.saved_variables[<span class="number">5</span>:]</span><br><span class="line"></span><br><span class="line">        d_input = d_weights = d_bias = d_old_h = d_old_cell = <span class="literal">None</span></span><br><span class="line">		</span><br><span class="line">        <span class="comment"># 计算关于输出门和 tanh(new_cell) 的梯度</span></span><br><span class="line">        d_output_gate = torch.tanh(new_cell) * grad_h</span><br><span class="line">        d_tanh_new_cell = output_gate * grad_h</span><br><span class="line">        d_new_cell = d_tanh(new_cell) * d_tanh_new_cell + grad_cell</span><br><span class="line"></span><br><span class="line">        d_old_cell = d_new_cell</span><br><span class="line">        d_candidate_cell = input_gate * d_new_cell</span><br><span class="line">        d_input_gate = candidate_cell * d_new_cell</span><br><span class="line">		</span><br><span class="line">        <span class="comment"># 将门控权重分割成输入门、输出门和候选细胞状态的梯度</span></span><br><span class="line">        gates = gate_weights.chunk(<span class="number">3</span>, dim=<span class="number">1</span>)</span><br><span class="line">        d_input_gate *= d_sigmoid(gates[<span class="number">0</span>])</span><br><span class="line">        d_output_gate *= d_sigmoid(gates[<span class="number">1</span>])</span><br><span class="line">        d_candidate_cell *= d_elu(gates[<span class="number">2</span>])</span><br><span class="line">		</span><br><span class="line">        <span class="comment"># 拼接三个门的梯度</span></span><br><span class="line">        d_gates = torch.cat(</span><br><span class="line">        [d_input_gate, d_output_gate, d_candidate_cell], dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果需要计算对权重的梯度</span></span><br><span class="line">        <span class="keyword">if</span> ctx.needs_input_grad[<span class="number">1</span>]:</span><br><span class="line">            d_weights = d_gates.t().mm(X)</span><br><span class="line">        <span class="comment"># 如果需要计算对偏置的梯度</span></span><br><span class="line">        <span class="keyword">if</span> ctx.needs_input_grad[<span class="number">2</span>]:</span><br><span class="line">            d_bias = d_gates.<span class="built_in">sum</span>(dim=<span class="number">0</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 如果需要计算对上一个时间步的隐藏状态和输入的梯度</span></span><br><span class="line">        <span class="keyword">if</span> ctx.needs_input_grad[<span class="number">3</span>] <span class="keyword">or</span> ctx.needs_input_grad[<span class="number">4</span>]:</span><br><span class="line">            d_X = d_gates.mm(weights)</span><br><span class="line">            state_size = grad_h.shape[<span class="number">1</span>]</span><br><span class="line">            d_old_h, d_input = d_X[:, :state_size], d_X[:, state_size:]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> d_input, d_weights, d_bias, d_old_h, d_old_cell</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LLTM</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_features, state_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(LLTMPython, self).__init__()</span><br><span class="line">        self.input_features = input_features</span><br><span class="line">        self.state_size = state_size</span><br><span class="line">        self.weights = nn.Parameter(</span><br><span class="line">            torch.Tensor(<span class="number">3</span> * state_size, input_features + state_size))</span><br><span class="line">        self.bias = nn.Parameter(torch.Tensor(<span class="number">1</span>, <span class="number">3</span> * state_size))</span><br><span class="line">        self.reset_parameters()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reset_parameters</span>(<span class="params">self</span>):</span><br><span class="line">        stdv = <span class="number">1.0</span> / math.sqrt(self.state_size)</span><br><span class="line">        <span class="keyword">for</span> weight <span class="keyword">in</span> self.parameters():</span><br><span class="line">            weight.data.uniform_(-stdv, +stdv)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span>, state</span>):</span><br><span class="line">        <span class="keyword">return</span> LLTMPythonFunction.apply(<span class="built_in">input</span>, self.weights, self.bias, *state)</span><br></pre></td></tr></tbody></table></figure>
<h3 id="时间效率比较"><a href="#时间效率比较" class="headerlink" title="时间效率比较"></a>时间效率比较</h3><p>实现完成以后可以简单进行一下测试两者<code>forward</code>和<code>backward</code>的时间，比较一下使用Pytorch的自动求导比较快，还是我们自己独立编写一个前向反向传播的函数比较快。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">cuda_device = torch.device(<span class="string">"cuda"</span>)  <span class="comment"># device object representing GPU</span></span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">16</span></span><br><span class="line">input_features = <span class="number">32</span></span><br><span class="line">state_size = <span class="number">128</span></span><br><span class="line"></span><br><span class="line">X = torch.randn(batch_size, input_features, device=cuda_device, dtype=torch.float32)</span><br><span class="line">h = torch.randn(batch_size, state_size, device=cuda_device, dtype=torch.float32)</span><br><span class="line">C = torch.randn(batch_size, state_size, device=cuda_device, dtype=torch.float32)</span><br><span class="line"></span><br><span class="line">rnn = LLTMPython(input_features, state_size).to(cuda_device)</span><br><span class="line"></span><br><span class="line">forward = <span class="number">0</span></span><br><span class="line">backward = <span class="number">0</span></span><br><span class="line">n = <span class="number">100</span></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">    start = time.time()</span><br><span class="line">    new_h, new_C = rnn(X, (h, C))</span><br><span class="line">    torch.cuda.synchronize()</span><br><span class="line">    forward += time.time() - start</span><br><span class="line"></span><br><span class="line">    start = time.time()</span><br><span class="line">    (new_h.<span class="built_in">sum</span>() + new_C.<span class="built_in">sum</span>()).backward()</span><br><span class="line">    torch.cuda.synchronize()</span><br><span class="line">    backward += time.time() - start</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'Forward: {:.10f} s | Backward {:.10f} s'</span>.<span class="built_in">format</span>(forward / n, backward  / n))</span><br><span class="line"></span><br><span class="line">rnn = LLTM(input_features, state_size).to(cuda_device)</span><br><span class="line"></span><br><span class="line">forward = <span class="number">0</span></span><br><span class="line">backward = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">    start = time.time()</span><br><span class="line">    new_h, new_C = rnn(X, (h, C))</span><br><span class="line">    torch.cuda.synchronize()</span><br><span class="line">    forward += time.time() - start</span><br><span class="line"></span><br><span class="line">    start = time.time()</span><br><span class="line">    (new_h.<span class="built_in">sum</span>() + new_C.<span class="built_in">sum</span>()).backward()</span><br><span class="line">    torch.cuda.synchronize()</span><br><span class="line">    backward += time.time() - start</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'Forward: {:.10f} s | Backward {:.10f} s'</span>.<span class="built_in">format</span>(forward / n, backward  / n))</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">Forward: 0.0012168598 s | Backward 0.0008872311 s</span><br><span class="line">Forward: 0.0002657304 s | Backward 0.0004498205 s</span><br></pre></td></tr></tbody></table></figure>
<p>从前向和反向的时间比较来看，我们的自实现方法在性能上表现较差，相较于PyTorch的默认自动求导机制。尽管前向传播速度的提升并不明显，这可能表明PyTorch的自动求导机制在某些方面确实具有强大的优势。我们的手动实现可能存在一些效率方面的不足，导致性能不如PyTorch默认方式高效。虽然重新编写前向传播和反向传播是提升训练速度的一种尝试，但我们需要进一步优化以确保其性能与PyTorch默认机制相媲美。在反向传播方面，可能需要考虑使用C++扩展和CUDA扩展来提高效率。</p>
<h2 id="C-扩展实现"><a href="#C-扩展实现" class="headerlink" title="C++扩展实现"></a>C++扩展实现</h2><p>接下来，我们将使用C++扩展来实现LLTM（Long Long-Term Memory）的组合版本。我们将从使用纯C++编写它开始，使用<a href="https://github.com/zdevito/ATen">ATen</a>库，在代码中表现为<code>torch/extension.h</code>，该库为PyTorch的大部分后端提供支持。</p>
<blockquote>
<p><strong><torch extension.h=""></torch></strong>是一站式头文件，包含写入C++扩展所需的所有PyTorch操作，包括：</p>
<ul>
<li>ATen库是用于张量计算的主要API，</li>
<li>pybind11，是为C++代码创建Python绑定的方式</li>
<li>管理ATen和pybind11之间交互细节的头文件</li>
</ul>
<p>PyTorch的张量和变量接口是从ATen库自动生成的，因此几乎可以将Python实现1:1转换为C++。所有计算的主要数据类型将是torch::Tensor。</p>
</blockquote>
<h3 id="基础函数-1"><a href="#基础函数-1" class="headerlink" title="基础函数"></a>基础函数</h3><p>对于C++的部分来说，我们首先需要对里面的函数进行重写一遍，利用C++的方式对来进行重写，这样进行计算的时候，程序就会通过C++来计算而不是Python来计算，这样就提高了效率和速度，首先主要是计算基础函数的微分函数，分别是的<code>d_sigmoid</code>，<code>d_tanh</code>和<code>d_elu</code>，主要是通过C++来进行编写。PyTorch的张量和变量接口是从ATen库自动生成的，因此几乎可以将Python实现1:1转换为C++。所有计算的主要数据类型将是<code>torch::Tensor</code>。</p>
<figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;torch/extension.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">torch::Tensor <span class="title">d_sigmoid</span><span class="params">(torch::Tensor z)</span> </span>{</span><br><span class="line">  <span class="keyword">auto</span> s = torch::<span class="built_in">sigmoid</span>(z);</span><br><span class="line">  <span class="keyword">return</span> (<span class="number">1</span> - s) * s;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// tanh'(z) = 1 - tanh^2(z)</span></span><br><span class="line"><span class="function">torch::Tensor <span class="title">d_tanh</span><span class="params">(torch::Tensor z)</span> </span>{</span><br><span class="line">  <span class="keyword">return</span> <span class="number">1</span> - z.<span class="built_in">tanh</span>().<span class="built_in">pow</span>(<span class="number">2</span>);</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// elu'(z) = relu'(z) + { alpha * exp(z) if (alpha * (exp(z) - 1)) &lt; 0, else 0}</span></span><br><span class="line"><span class="function">torch::Tensor <span class="title">d_elu</span><span class="params">(torch::Tensor z, torch::Scalar alpha = <span class="number">1.0</span>)</span> </span>{</span><br><span class="line">  <span class="keyword">auto</span> e = z.<span class="built_in">exp</span>();</span><br><span class="line">  <span class="keyword">auto</span> mask = (alpha * (e - <span class="number">1</span>)) &lt; <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">return</span> (z &gt; <span class="number">0</span>).<span class="built_in">type_as</span>(z) + mask.<span class="built_in">type_as</span>(z) * (alpha * e);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h3 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h3><p>接下来比较重要的就是对应前向传播和反向传播的编写了，这里面其实依旧是将我们前面使用的Python代码来转化为C++的代码的编写，逻辑都是一样的，只不过语言是有些不同的，这里<code>forward</code>不需要传入<code>ctx</code>参数，所以一共有五个参数，实际上跟Python实现的一一对应，只是转化为C++的扩展而已。</p>
<p>以前向传播为例子，这里面有一些语法和用法是不一样的，比如Pytorch里面的<code>torch.cat</code>对应着C++里面的<code>torch::cat</code>，还有比较不一样的就是<code>F.linear</code>对应着<code>torch.addmm</code>，除此之外，大部分的方法我们都可以在前面加一个前缀<code>torch::</code>即可，这也算写C++扩展的一个规律。</p>
<blockquote>
<p><code>torch::addmm</code> 是 PyTorch 中的一个函数，用于执行矩阵的乘法和加法操作。具体来说，它执行以下操作：</p>
<figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line">result = beta * mat + alpha * (mat1 @ mat2)</span><br></pre></td></tr></tbody></table></figure>
<p>其中，<code>mat</code> 是输出矩阵，<code>mat1</code> 和 <code>mat2</code> 是输入矩阵，<code>alpha</code> 和 <code>beta</code> 是标量系数。这个函数通常用于线性代数运算，特别是在神经网络的前向传播过程中经常会用到。</p>
</blockquote>
<figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// 前向传播</span></span><br><span class="line"><span class="function">std::vector&lt;at::Tensor&gt; <span class="title">lltm_forward</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Tensor input,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Tensor weights,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Tensor bias,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Tensor old_h,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Tensor old_cell)</span> </span>{</span><br><span class="line">  <span class="keyword">auto</span> X = torch::<span class="built_in">cat</span>({old_h, input}, <span class="comment">/*dim=*/</span><span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">auto</span> gate_weights = torch::<span class="built_in">addmm</span>(bias, X, weights.<span class="built_in">transpose</span>(<span class="number">0</span>, <span class="number">1</span>));</span><br><span class="line">  <span class="keyword">auto</span> gates = gate_weights.<span class="built_in">chunk</span>(<span class="number">3</span>, <span class="comment">/*dim=*/</span><span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">auto</span> input_gate = torch::<span class="built_in">sigmoid</span>(gates[<span class="number">0</span>]);</span><br><span class="line">  <span class="keyword">auto</span> output_gate = torch::<span class="built_in">sigmoid</span>(gates[<span class="number">1</span>]);</span><br><span class="line">  <span class="keyword">auto</span> candidate_cell = torch::<span class="built_in">elu</span>(gates[<span class="number">2</span>], <span class="comment">/*alpha=*/</span><span class="number">1.0</span>);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">auto</span> new_cell = old_cell + candidate_cell * input_gate;</span><br><span class="line">  <span class="keyword">auto</span> new_h = torch::<span class="built_in">tanh</span>(new_cell) * output_gate;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> {new_h,</span><br><span class="line">          new_cell,</span><br><span class="line">          input_gate,</span><br><span class="line">          output_gate,</span><br><span class="line">          candidate_cell,</span><br><span class="line">          X,</span><br><span class="line">          gate_weights};</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><p>反向传播与前向传播是类似的，有一点不同的是，由于没有<code>ctx</code>存储信息，所以我们的传入参数还包括<code>ctx</code>里面的参数，都是<code>torch::Tensor</code>格式，根据Python编写的反向传播的代码，对应着C++实现即可，其实也是一模一样，一一对应的。</p>
<figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// 反向传播</span></span><br><span class="line"><span class="function">std::vector&lt;torch::Tensor&gt; <span class="title">lltm_backward</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Tensor grad_h,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Tensor grad_cell,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Tensor new_cell,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Tensor input_gate,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Tensor output_gate,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Tensor candidate_cell,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Tensor X,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Tensor gate_weights,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Tensor weights)</span> </span>{</span><br><span class="line">  <span class="keyword">auto</span> d_output_gate = torch::<span class="built_in">tanh</span>(new_cell) * grad_h;</span><br><span class="line">  <span class="keyword">auto</span> d_tanh_new_cell = output_gate * grad_h;</span><br><span class="line">  <span class="keyword">auto</span> d_new_cell = <span class="built_in">d_tanh</span>(new_cell) * d_tanh_new_cell + grad_cell;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">auto</span> d_old_cell = d_new_cell;</span><br><span class="line">  <span class="keyword">auto</span> d_candidate_cell = input_gate * d_new_cell;</span><br><span class="line">  <span class="keyword">auto</span> d_input_gate = candidate_cell * d_new_cell;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">auto</span> gates = gate_weights.<span class="built_in">chunk</span>(<span class="number">3</span>, <span class="comment">/*dim=*/</span><span class="number">1</span>);</span><br><span class="line">  d_input_gate *= <span class="built_in">d_sigmoid</span>(gates[<span class="number">0</span>]);</span><br><span class="line">  d_output_gate *= <span class="built_in">d_sigmoid</span>(gates[<span class="number">1</span>]);</span><br><span class="line">  d_candidate_cell *= <span class="built_in">d_elu</span>(gates[<span class="number">2</span>]);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">auto</span> d_gates =</span><br><span class="line">      torch::<span class="built_in">cat</span>({d_input_gate, d_output_gate, d_candidate_cell}, <span class="comment">/*dim=*/</span><span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">auto</span> d_weights = d_gates.<span class="built_in">t</span>().<span class="built_in">mm</span>(X);</span><br><span class="line">  <span class="keyword">auto</span> d_bias = d_gates.<span class="built_in">sum</span>(<span class="comment">/*dim=*/</span><span class="number">0</span>, <span class="comment">/*keepdim=*/</span><span class="literal">true</span>);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">auto</span> d_X = d_gates.<span class="built_in">mm</span>(weights);</span><br><span class="line">  <span class="type">const</span> <span class="keyword">auto</span> state_size = grad_h.<span class="built_in">size</span>(<span class="number">1</span>);</span><br><span class="line">  <span class="keyword">auto</span> d_old_h = d_X.<span class="built_in">slice</span>(<span class="comment">/*dim=*/</span><span class="number">1</span>, <span class="number">0</span>, state_size);</span><br><span class="line">  <span class="keyword">auto</span> d_input = d_X.<span class="built_in">slice</span>(<span class="comment">/*dim=*/</span><span class="number">1</span>, state_size);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> {d_old_h, d_input, d_weights, d_bias, d_old_cell};</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>完成上述代码的编写之后，我们就只需要<code>PYBIND11_MODULE</code>来进行绑定，再简单介绍一下他的用处，他的作用其实就是一个命名的绑定，也就是Python里面的forward为C++里面的<code>lltm_forward</code>，这样调用模块的时候，Python中的函数就会找到对应的C++函数来进行运行。</p>
<blockquote>
<p><code>PYBIND11_MODULE</code>这是 Python 调用 C++ 函数的关键部分。这个函数会在Python执行<code>import</code>语句时被调用，其接受两个参数，</p>
<p>第一个参数为模块名称，这里我们直接将<code>lltm_cpp</code>填入，稍候可以在Python中使用<code>import lltm_cpp</code>导入该模块；第二个参数<code>m</code>是创建Python关联代码的主接口，其类型为<code>py::module_</code>。<code>module_::def()</code>用于生成能够将<code>lltm_cpp</code>函数暴露给Python的代码，其第一个参数为<strong>字符串</strong>，将会成为Python中调用的函数名；</p>
<p>第二个参数是<strong>C++函数</strong>的引用；</p>
<p>第三个参数是<strong>说明字符串</strong>，在Python中可以使用<code>help(lltm_cpp)</code>查看。比如下面的例子中，C++ 中的函数 <code>lltm_forward</code> 对应 Python 中的 <code>forward</code>。</p>
</blockquote>
<figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="built_in">PYBIND11_MODULE</span>(TORCH_EXTENSION_NAME, m) {</span><br><span class="line">  m.<span class="built_in">def</span>(<span class="string">"forward"</span>, &amp;lltm_forward, <span class="string">"LLTM forward"</span>);</span><br><span class="line">  m.<span class="built_in">def</span>(<span class="string">"backward"</span>, &amp;lltm_backward, <span class="string">"LLTM backward"</span>);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h3 id="C-扩展调用"><a href="#C-扩展调用" class="headerlink" title="C++扩展调用"></a>C++扩展调用</h3><p>C++扩展一般有两种方式</p>
<ul>
<li>通过<code>setuptools</code>“提前”构建</li>
<li>通过<code>torch.utils.cpp_extension.load()</code>“实时”构建</li>
</ul>
<p>对于这两种方法，如果你的代码只运行一次，可以利用<code>jit</code>实时构建，这样不用去<code>python setup.py</code>来安装，但是如果你会多次复用这个C++扩展，那么还是需要去用第一种方法，这样后续运行的时候不需要一直构建，这样等待的时间就回比较长。</p>
<h4 id="Building-with-setuptools"><a href="#Building-with-setuptools" class="headerlink" title="Building with setuptools"></a>Building with <code>setuptools</code></h4><p>接下来先试用<code>setuptools</code>进行构建，编写一个 <code>setup.py</code> 文件，主要用于定义和说明一些重要的信息。其中关键的参数包括：</p>
<ul>
<li><code>name</code>：Python 调用的包的名称。</li>
<li><code>ext_modules</code> 的 <code>sources</code>：需要编译的 C++ 源文件，如果有多个 C++ 文件，需要列举所有。</li>
<li><code>cmdclass</code>：用BuildExtension执行许多必需的配置步骤和检查，并在混合C++/CUDA扩展的情况下处理混合编译。</li>
</ul>
<figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line">from setuptools <span class="keyword">import</span> setup, Extension</span><br><span class="line">from torch.<span class="function">utils <span class="keyword">import</span> cpp_extension</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"><span class="title">setup</span><span class="params">(name=<span class="string">'lltm_cpp'</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">      ext_modules=[cpp_extension.CppExtension(<span class="string">'lltm_cpp'</span>, [<span class="string">'lltm.cpp'</span>])],</span></span></span><br><span class="line"><span class="params"><span class="function">      cmdclass={<span class="string">'build_ext'</span>: cpp_extension.BuildExtension})</span></span></span><br></pre></td></tr></tbody></table></figure>
<p>完成这一步后，如果使用<code>setuptools</code>进行构建，我们可以使用 <code>pip</code> 进行安装。如果在当前文件夹下，直接运行 <code>pip install .</code> 即可完成安装或者我们也可以使用<code>python set.py install</code>，安装成功后应该会显示以下结果：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">Installed /path/python3.10/site-packages/lltm_cpp-0.0.0-py3.10-linux-x86_64.egg</span><br><span class="line">Processing dependencies <span class="keyword">for</span> lltm-cpp==0.0.0</span><br><span class="line">Finished processing dependencies <span class="keyword">for</span> lltm-cpp==0.0.0</span><br></pre></td></tr></tbody></table></figure>
<p>这样后续我们就可以在对应的环境来进行<code>import lltm_cpp</code>来使用对应C++函数了。</p>
<h4 id="JIT-Compiling-Extensions"><a href="#JIT-Compiling-Extensions" class="headerlink" title="JIT Compiling Extensions"></a>JIT Compiling Extensions</h4><p>除了上述的<code>setuptools</code>的方法，接下来介绍即时编译（JIT）机制构建C++扩展。JIT编译机制通过调用PyTorch API中的一个简单函数<code>torch.utils.cpp_extension.load()</code>，为你提供了一种即时编译和加载扩展的方式。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.cpp_extension <span class="keyword">import</span> load</span><br><span class="line"></span><br><span class="line">cppcuda_tutorial = load(name=<span class="string">"cppcuda_tutorial"</span>,</span><br><span class="line">                        <span class="comment"># extra_include_paths=include_dirs,</span></span><br><span class="line">                        sources=[<span class="string">'interpolation.cpp'</span>],)</span><br></pre></td></tr></tbody></table></figure>
<p>在这里，实际提供的是域setuptools相同的信息。在后台，这将执行以下操作：</p>
<ol>
<li>创建一个临时目录<code>/tmp/torch_extensions/cppcuda_tutorial</code>，</li>
<li>向该临时目录发出Ninja构建文件，</li>
<li>将你的源文件编译成一个共享库，</li>
<li>将这个共享库导入为Python模块。</li>
</ol>
<p>实际上，如果将<code>verbose=True</code>传递给<code>cpp_extension.load()</code>，你将得到有关该过程的信息：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">Using /path/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...</span><br><span class="line">Creating extension directory /path/.cache/torch_extensions/py310_cu113/lltm_cpp...</span><br><span class="line">Emitting ninja build file /path/.cache/torch_extensions/py310_cu113/lltm_cpp/build.ninja...</span><br><span class="line">Building extension module lltm_cpp...</span><br><span class="line">Allowing ninja to <span class="built_in">set</span> a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)</span><br><span class="line">[1/2] c++ -MMD -MF lltm.o.d -DTORCH_EXTENSION_NAME=lltm_cpp -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -isystem /path//path/python3.10/site-packages/torch/include -isystem /path//path/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /path//path/python3.10/site-packages/torch/include/TH -isystem /path//path/python3.10/site-packages/torch/include/THC -isystem /path/anaconda3/envs/ernerf/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -c /path/workdirs/pytorch-cppcuda-tutorial/lltm/lltm.cpp -o lltm.o </span><br><span class="line">[2/2] c++ lltm.o -shared -L/path//path/python3.10/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o lltm_cpp.so</span><br><span class="line">Loading extension module lltm_cpp...</span><br></pre></td></tr></tbody></table></figure>
<h3 id="加速前向反向传播"><a href="#加速前向反向传播" class="headerlink" title="加速前向反向传播"></a>加速前向反向传播</h3><p>实现了加速的扩展之后，我们就可以改写我们对应的<code>LLTMFuntcion</code>利用C++扩展来进行加速，方法很简单，就是将函数传入到<code>LLTMFuntcion</code>传入即可</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LLTMFunction</span>(torch.autograd.Function):</span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">ctx, <span class="built_in">input</span>, weights, bias, old_h, old_cell</span>):</span><br><span class="line">        outputs = lltm_cpp.forward(<span class="built_in">input</span>, weights, bias, old_h, old_cell)</span><br><span class="line">        new_h, new_cell = outputs[:<span class="number">2</span>]</span><br><span class="line">        variables = outputs[<span class="number">1</span>:] + [weights]</span><br><span class="line">        ctx.save_for_backward(*variables)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> new_h, new_cell</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">ctx, grad_h, grad_cell</span>):</span><br><span class="line">        outputs = lltm_cpp.backward(</span><br><span class="line">            grad_h.contiguous(), grad_cell.contiguous(), *ctx.saved_tensors)</span><br><span class="line">        d_old_h, d_input, d_weights, d_bias, d_old_cell = outputs</span><br><span class="line">        <span class="keyword">return</span> d_input, d_weights, d_bias, d_old_h, d_old_cell</span><br></pre></td></tr></tbody></table></figure>
<h3 id="时间效率比较-1"><a href="#时间效率比较-1" class="headerlink" title="时间效率比较"></a>时间效率比较</h3><p>与Python扩展实现类似，我们进行了与C++扩展实现类似的时间效率比较，结果显示前向传播在<code>PyTorch</code>自动求导机制下相对较慢，与我们手动实现相比可能有四五倍的性能差距。然而，值得注意的是，尽管前向传播的速度较慢，但所获得的结果却更为优越，相较于默认机制大约提高了2倍左右。</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">Forward: 0.0011254544 s | Backward 0.0005326970 s</span><br><span class="line">Forward: 0.0002915986 s | Backward 0.0008708093 s</span><br></pre></td></tr></tbody></table></figure>
<h2 id="CUDA扩展实现"><a href="#CUDA扩展实现" class="headerlink" title="CUDA扩展实现"></a>CUDA扩展实现</h2><p>接下来就到了重头戏，也就是CUDA扩展的实现，首先我们先看看，怎么使用CUDA去进行编程，首先CUDA的代码是<code>cu</code>结尾的，我们通过编写CUDA来进行一个计算加速。我们还是先按之前的方法，看看如何使用CUDA进行编程先，这里面有几个注意的点：</p>
<ul>
<li>需要编写CUDA函数</li>
<li>需要在头文件<code>.h</code>中去定义需要使用的函数，包括一些常用的测试函数。也可以写到函数中</li>
<li>修改CUDA的<code>setup.py</code></li>
</ul>
<p>接下来一步一步来，首先我们写对应调用CUDA的C++文件，这里面做的主要是声明和一些定义<code>lltm_forward</code>,<code>lltm_backward</code>,<code>lltm_cuda_forward</code>,<code>lltm_cuda_forward</code>函数，具体的函数操作会在CUDA里面进行实现，这样编写以后，我们的cpp的代码就可以调用CUDA对函数来进行调用，但是由于我们使用CUDA的函数，所以这里面我们还要用到<code>CHECK_INPUT</code>函数来判断是否在GPU上，也就是一个检测，并且内存是否连续，因为后续要进行一个并行的计算。</p>
<figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;torch/extension.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// C++ interface</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK_CUDA(x) TORCH_CHECK(x.device().is_cuda(), #x <span class="string">" must be a CUDA tensor"</span>)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x <span class="string">" must be contiguous"</span>)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// CUDA forward declarations</span></span><br><span class="line"></span><br><span class="line"><span class="function">std::vector&lt;torch::Tensor&gt; <span class="title">lltm_cuda_forward</span><span class="params">( </span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor input,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor weights,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor bias,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor old_h,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor old_cell)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function">std::vector&lt;torch::Tensor&gt; <span class="title">lltm_cuda_backward</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor grad_h,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor grad_cell,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor new_cell,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor input_gate,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor output_gate,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor candidate_cell,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor X,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor gate_weights,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor weights)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function">std::vector&lt;torch::Tensor&gt; <span class="title">lltm_forward</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor input,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor weights,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor bias,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor old_h,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor old_cell)</span> </span>{</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(input);</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(weights);</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(bias);</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(old_h);</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(old_cell);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">lltm_cuda_forward</span>(input, weights, bias, old_h, old_cell);</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function">std::vector&lt;torch::Tensor&gt; <span class="title">lltm_backward</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor grad_h,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor grad_cell,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor new_cell,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor input_gate,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor output_gate,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor candidate_cell,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor X,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor gate_weights,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor weights)</span> </span>{</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(grad_h);</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(grad_cell);</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(input_gate);</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(output_gate);</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(candidate_cell);</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(X);</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(gate_weights);</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(weights);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">lltm_cuda_backward</span>(</span><br><span class="line">            grad_h,</span><br><span class="line">            grad_cell,</span><br><span class="line">            new_cell,</span><br><span class="line">            input_gate,</span><br><span class="line">            output_gate,</span><br><span class="line">            candidate_cell,</span><br><span class="line">            X,</span><br><span class="line">            gate_weights,</span><br><span class="line">            weights);</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="built_in">PYBIND11_MODULE</span>(TORCH_EXTENSION_NAME, m) {</span><br><span class="line">    m.<span class="built_in">def</span>(<span class="string">"forward"</span>, &amp;lltm_forward, <span class="string">"LLTM forward (CUDA)"</span>);</span><br><span class="line">    m.<span class="built_in">def</span>(<span class="string">"backward"</span>, &amp;lltm_backward, <span class="string">"LLTM backward (CUDA)"</span>);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h3 id="基础函数-2"><a href="#基础函数-2" class="headerlink" title="基础函数"></a>基础函数</h3><p>接下来就是重中之重个，也就是我们需要写一个<code>lltm_cuda_kernel.cu</code>函数，也就是一个LLTM对应的CUDA函数，后面我们可以用C++调用CUDA，最基础的还是对应的基础函数CUDA函数编写，和前面类似，这里面是将前面使用的C++，Python转化为CUDA编程的格式，实际上也是对应的C++代码。</p>
<p>对于CUDA函数来说，首先比较的不同的就是，我们都需要使用到模板参数<code>template&lt;typename scalar_t&gt;</code>，这样就代表着<code>scalar_t</code>能够代表任何的类型，方便后续去调用。</p>
<p><code>__device__ __forceinline__</code> 这两个标记告诉编译器将函数内联到调用处，以提高执行效率。<code>__device__</code> 表示这个函数在设备（GPU）上执行，而 <code>__forceinline__</code> 提示编译器尽可能地进行内联优化。这都是CUDA来提高效率的一些方式，实际上的计算方式就和C++的实现是一样的。</p>
<figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;torch/extension.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> <span class="type">scalar_t</span>&gt;</span></span><br><span class="line"><span class="function">__device__ __forceinline__ <span class="type">scalar_t</span> <span class="title">sigmoid</span><span class="params">(<span class="type">scalar_t</span> z)</span> </span>{</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span> / (<span class="number">1.0</span> + <span class="built_in">exp</span>(-z));</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> <span class="type">scalar_t</span>&gt;</span></span><br><span class="line"><span class="function">__device__ __forceinline__ <span class="type">scalar_t</span> <span class="title">d_sigmoid</span><span class="params">(<span class="type">scalar_t</span> z)</span> </span>{</span><br><span class="line">    <span class="type">const</span> <span class="keyword">auto</span> s = <span class="built_in">sigmoid</span>(z);</span><br><span class="line">    <span class="keyword">return</span> (<span class="number">1.0</span> - s) * s;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> <span class="type">scalar_t</span>&gt;</span></span><br><span class="line"><span class="function">__device__ __forceinline__ <span class="type">scalar_t</span> <span class="title">d_tanh</span><span class="params">(<span class="type">scalar_t</span> z)</span> </span>{</span><br><span class="line">    <span class="type">const</span> <span class="keyword">auto</span> t = <span class="built_in">tanh</span>(z);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> - (t * t);</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> <span class="type">scalar_t</span>&gt;</span></span><br><span class="line"><span class="function">__device__ __forceinline__ <span class="type">scalar_t</span> <span class="title">elu</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">scalar_t</span> z,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">scalar_t</span> alpha = <span class="number">1.0</span></span></span></span><br><span class="line"><span class="params"><span class="function">)</span> </span>{</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">fmax</span>(<span class="number">0.0</span>,</span><br><span class="line">                z) + <span class="built_in">fmin</span>(<span class="number">0.0</span>,</span><br><span class="line">                          alpha * (<span class="built_in">exp</span>(z) - <span class="number">1.0</span>));</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> <span class="type">scalar_t</span>&gt;</span></span><br><span class="line"><span class="function">__device__ __forceinline__ <span class="type">scalar_t</span> <span class="title">d_elu</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">scalar_t</span> z,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">scalar_t</span> alpha = <span class="number">1.0</span></span></span></span><br><span class="line"><span class="params"><span class="function">)</span> </span>{</span><br><span class="line">    <span class="type">const</span> <span class="keyword">auto</span> e = <span class="built_in">exp</span>(z);</span><br><span class="line">    <span class="type">const</span> <span class="keyword">auto</span> d_relu = z &lt; <span class="number">0.0</span> ? <span class="number">0.0</span> : <span class="number">1.0</span>;</span><br><span class="line">    <span class="keyword">return</span> d_relu + (((alpha * (e - <span class="number">1.0</span>)) &lt; <span class="number">0.0</span>) ? (alpha * e) : <span class="number">0.0</span>);</span><br><span class="line">}</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h3 id="CUDA加速原理"><a href="#CUDA加速原理" class="headerlink" title="CUDA加速原理"></a>CUDA加速原理</h3><p>首先介绍一个CUDA程序实现的流程</p>
<ol>
<li>把数据从CPU内存拷贝到GPU内存</li>
<li>调用核函数对存储在GPU内存中的数据进行操作</li>
<li>将数据从GPU内存传送回CPU内存</li>
</ol>
<p><img src="https://pic2.zhimg.com/v2-2959e07a36a8dc8f59280f53b43eb9d1_b.jpg" alt="CUDA编程入门极简教程- 知乎"></p>
<p>如下图所示，在利用CUDA加速的时候，图的左边是CPU，右边是GPU，我们需要把数据从CPU传到GPU中。在GPU中，就会生成对应的Grid来进行计算，每个Grid里面又有很多的block，从block中看又有很多的线程thread进行运算。我们也可以想象一下，如果计算一个矩阵的加法，我们可以让每个thread做对应的元素的相加，这样就可以大大加快计算速度，达到并行的效果。所以这之中内核（kernel）是CUDA编程中一个重要的部分，其代码在GPU上运行，比如矩阵乘法，我们就可以写一个加法的核函数，然后串行执行核函数，这样我们就快速能完成CUDA代码的编写，而不用在创建和管理大量的GPU线程时拘泥于细节。</p>
<p><img src="https://nyu-cds.github.io/python-gpu/fig/02-threadmapping.png" alt="Thread Mapping"></p>
<p>所以我们可以发现，实际上CUDA的计算是<code>Grid</code>——》<code>Block</code>——》<code>Thread</code>，然后用多个<code>Thread</code>进行计算，这里面可能会疑惑，为什么不是直接<code>Grid</code>——》<code>Thread</code>，实际上是因为硬件的限制是<code>Block</code>上限是$(2^{31}-1)<em>2^{16}</em>2^{16}$，<code>Thread</code>的上限是1024，所以这样的组合设计能够利用好更多的<code>Thread</code>，这也是为什么GPU速度那么快的原因。</p>
<h3 id="前向传播核函数"><a href="#前向传播核函数" class="headerlink" title="前向传播核函数"></a>前向传播核函数</h3><p>对于CUDA编程来说，往往首先我们需要编写一个<code>kernel</code>模板函数，这个模板函数实际上就是说明，在一个核中的对应的计算方式和流程。这样GPU实现的就是在多个<code>thread</code>做这样的事情，这样就实现了并行运算，极大的提高了效率，这也是是为什么CUDA加速比较快的原因。</p>
<p>接下里分析函数的主体部分，主要做两件事情：</p>
<ol>
<li>为每个<code>threads</code>进行编号</li>
<li>去除不必要的<code>threads</code></li>
</ol>
<p>在使用<code>threads</code>计算的时候，实际上每一个<code>threads</code>都有一个对应的编号，计算方式如第12,13行所示，实际上就是block的x*block的个数+block的y就能得到最后的结果，后续就是对应着每一个<code>threads</code>的计算，对应<code>threads</code>的编号<code>index</code>。</p>
<figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> <span class="type">scalar_t</span>&gt;</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">lltm_cuda_forward_kernel</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">const</span> <span class="type">scalar_t</span> *__restrict__ gates,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">const</span> <span class="type">scalar_t</span> *__restrict__ old_cell,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">scalar_t</span> *__restrict__ new_h,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">scalar_t</span> *__restrict__ new_cell,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">scalar_t</span> *__restrict__ input_gate,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">scalar_t</span> *__restrict__ output_gate,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">scalar_t</span> *__restrict__ candidate_cell,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">size_t</span> state_size</span></span></span><br><span class="line"><span class="params"><span class="function">)</span> </span>{</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> column = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> index = blockIdx.y * state_size + column;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> gates_row = blockIdx.y * (state_size * <span class="number">3</span>);</span><br><span class="line">    <span class="keyword">if</span> (column &lt; state_size) {</span><br><span class="line">        input_gate[index] = <span class="built_in">sigmoid</span>(gates[gates_row + column]);</span><br><span class="line">        output_gate[index] = <span class="built_in">sigmoid</span>(gates[gates_row + state_size +</span><br><span class="line">                                           column]);</span><br><span class="line">        candidate_cell[index] = <span class="built_in">elu</span>(gates[gates_row + <span class="number">2</span> * state_size +</span><br><span class="line">                                          column]);</span><br><span class="line">        new_cell[index] =</span><br><span class="line">                old_cell[index] +</span><br><span class="line">                candidate_cell[index] * input_gate[index];</span><br><span class="line">        new_h[index] = <span class="built_in">tanh</span>(new_cell[index]) * output_gate[index];</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h3 id="前向传播-1"><a href="#前向传播-1" class="headerlink" title="前向传播"></a>前向传播</h3><p>接下来我们就使用实现的<code>lltm_cuda_forward_kernel</code>来实现<code>lltm_cuda_forward</code>，前面和C++扩展的方式是一样的，以及初始化空的变量以便后续进行前向传播，对于<code>const int threads = 256; const dim3 blocks((state_size + threads - 1) / threads, batch_size);</code>这两行，实际上就是在定义上述提过的<code>threads</code>和<code>blocks</code>了，<code>dim3</code>是NVIDIA的CUDA编程中一种自定义的整型向量类型，基于用于指定维度的<code>uint3</code>，<code>dim3</code>类型最终设置的是一个三维向量，三维参数分别为x,y,z。在并行中，通常只支持三个并行，比如这里的N和F刚刚好就是两个并行，这里设置为16x16的线程，一般可以是128,256,512，不一定使用越多越好，这里面只是给了一个例子。</p>
<p><code>const dim3 blocks((state_size + threads - 1) / threads, batch_size);</code>部分还定义了<code>blocks</code>的个数计算，也就是使用多大的<code>blocks</code>去覆盖我们计算矩阵，这里面的<code>state_size</code>是每一个计算的状态大小，<code>batch_size</code>是每次的数目，这样我们就可以计算出对应的<code>blocks</code>大小来进行计算。（详细解读可以看看上一篇我的博客，里面可以画图帮助理解）<a href="https://zhuanlan.zhihu.com/p/671704557">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a></p>
<p>最后还有一个就是<code>AT_DISPATCH_FLOATING_TYPES</code>，这里面就是一个启动核函数的部分，这里也可以认为是一个框架函数，<code>AT_DISPATCH_FLOATING_TYPES</code> 是处理核函数的启动（使用 <code>&lt;&lt;&lt;...&gt;&gt;&gt;</code> 表示），它一般有三个参数</p>
<ul>
<li>一个类型<code>gates.type()</code></li>
<li>一个名称 <code>"lltm_forward_cuda"</code>，用于错误消息</li>
<li>一个 lambda 函数，是一个模版函数<code>template</code>，类型别名为 <code>scalar_t</code>，这里面就是核函数</li>
</ul>
<figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function">std::vector&lt;torch::Tensor&gt; <span class="title">lltm_cuda_forward</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor input,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor weights,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor bias,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor old_h,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor old_cell</span></span></span><br><span class="line"><span class="params"><span class="function">)</span> </span>{</span><br><span class="line">    <span class="keyword">auto</span> X = torch::<span class="built_in">cat</span>({old_h, input}, <span class="comment">/*dim=*/</span></span><br><span class="line">                        <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">auto</span> gate_weights = torch::<span class="built_in">addmm</span>(bias,</span><br><span class="line">                                     X,</span><br><span class="line">                                     weights.<span class="built_in">transpose</span>(<span class="number">0</span>,</span><br><span class="line">                                                       <span class="number">1</span>));</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="keyword">auto</span> batch_size = old_cell.<span class="built_in">size</span>(<span class="number">0</span>);</span><br><span class="line">    <span class="type">const</span> <span class="keyword">auto</span> state_size = old_cell.<span class="built_in">size</span>(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> gates = gate_weights.<span class="built_in">reshape</span>({batch_size, <span class="number">3</span>, state_size});</span><br><span class="line">    <span class="keyword">auto</span> new_h = torch::<span class="built_in">zeros_like</span>(old_cell);</span><br><span class="line">    <span class="keyword">auto</span> new_cell = torch::<span class="built_in">zeros_like</span>(old_cell);</span><br><span class="line">    <span class="keyword">auto</span> input_gate = torch::<span class="built_in">zeros_like</span>(old_cell);</span><br><span class="line">    <span class="keyword">auto</span> output_gate = torch::<span class="built_in">zeros_like</span>(old_cell);</span><br><span class="line">    <span class="keyword">auto</span> candidate_cell = torch::<span class="built_in">zeros_like</span>(old_cell);</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> threads = <span class="number">256</span>;</span><br><span class="line">    <span class="function"><span class="type">const</span> dim3 <span class="title">blocks</span><span class="params">((state_size + threads - <span class="number">1</span>) / threads,</span></span></span><br><span class="line"><span class="params"><span class="function">                      batch_size)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">AT_DISPATCH_FLOATING_TYPES</span>(gates.<span class="built_in">type</span>(),</span><br><span class="line">                               <span class="string">"lltm_forward_cuda"</span>,</span><br><span class="line">                               ([&amp;] {</span><br><span class="line">                                   lltm_cuda_forward_kernel&lt;<span class="type">scalar_t</span>&gt;&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(</span><br><span class="line">                                           gates.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">                                           old_cell.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">                                           new_h.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">                                           new_cell.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">                                           input_gate.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">                                           output_gate.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">                                           candidate_cell.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">                                           state_size);</span><br><span class="line">                               }));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> {new_h, new_cell, input_gate, output_gate, candidate_cell, X,</span><br><span class="line">            gates};</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h3 id="反向传播-1"><a href="#反向传播-1" class="headerlink" title="反向传播"></a>反向传播</h3><p>与前向传播类似，反向传播也是类似的方法进行了实现，这里具体就不讲述，几乎和前向传播的步骤是一样的。</p>
<figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> <span class="type">scalar_t</span>&gt;</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">lltm_cuda_backward_kernel</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">scalar_t</span> *d_old_cell,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">scalar_t</span> *d_gates,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">const</span> <span class="type">scalar_t</span> *grad_h,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">const</span> <span class="type">scalar_t</span> *grad_cell,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">const</span> <span class="type">scalar_t</span> *new_cell,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">const</span> <span class="type">scalar_t</span> *input_gate,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">const</span> <span class="type">scalar_t</span> *output_gate,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">const</span> <span class="type">scalar_t</span> *candidate_cell,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">const</span> <span class="type">scalar_t</span> *gate_weights,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">int</span> state_size</span></span></span><br><span class="line"><span class="params"><span class="function">)</span> </span>{</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> column = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> index = blockIdx.y * state_size + column;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> gates_row = blockIdx.y * (state_size * <span class="number">3</span>);</span><br><span class="line">    <span class="keyword">if</span> (column &lt; state_size) {</span><br><span class="line">        <span class="type">const</span> <span class="keyword">auto</span> d_output_gate = <span class="built_in">tanh</span>(new_cell[index]) *</span><br><span class="line">                                   grad_h[index];</span><br><span class="line">        <span class="type">const</span> <span class="keyword">auto</span> d_tanh_new_cell = output_gate[index] * grad_h[index];</span><br><span class="line">        <span class="type">const</span> <span class="keyword">auto</span> d_new_cell =</span><br><span class="line">                <span class="built_in">d_tanh</span>(new_cell[index]) * d_tanh_new_cell +</span><br><span class="line">                grad_cell[index];</span><br><span class="line">        d_old_cell[index] = d_new_cell;</span><br><span class="line">        <span class="type">const</span> <span class="keyword">auto</span> d_candidate_cell = input_gate[index] * d_new_cell;</span><br><span class="line">        <span class="type">const</span> <span class="keyword">auto</span> d_input_gate = candidate_cell[index] * d_new_cell;</span><br><span class="line">        d_gates[gates_row + column] =</span><br><span class="line">                d_input_gate * <span class="built_in">d_sigmoid</span>(gate_weights[gates_row +</span><br><span class="line">                                                      column]);</span><br><span class="line">        d_gates[gates_row + state_size +</span><br><span class="line">                column] =</span><br><span class="line">                d_output_gate *</span><br><span class="line">                <span class="built_in">d_sigmoid</span>(gate_weights[gates_row + state_size +</span><br><span class="line">                                       column]);</span><br><span class="line">        d_gates[gates_row + <span class="number">2</span> * state_size +</span><br><span class="line">                column] =</span><br><span class="line">                d_candidate_cell *</span><br><span class="line">                <span class="built_in">d_elu</span>(gate_weights[gates_row + <span class="number">2</span> * state_size +</span><br><span class="line">                                   column]);</span><br><span class="line">    }</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function">std::vector&lt;torch::Tensor&gt; <span class="title">lltm_cuda_backward</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor grad_h,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor grad_cell,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor new_cell,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor input_gate,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor output_gate,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor candidate_cell,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor X,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor gates,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor weights</span></span></span><br><span class="line"><span class="params"><span class="function">)</span> </span>{</span><br><span class="line">    <span class="keyword">auto</span> d_old_cell = torch::<span class="built_in">zeros_like</span>(new_cell);</span><br><span class="line">    <span class="keyword">auto</span> d_gates = torch::<span class="built_in">zeros_like</span>(gates);</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="keyword">auto</span> batch_size = new_cell.<span class="built_in">size</span>(<span class="number">0</span>);</span><br><span class="line">    <span class="type">const</span> <span class="keyword">auto</span> state_size = new_cell.<span class="built_in">size</span>(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> threads = <span class="number">256</span>;</span><br><span class="line">    <span class="function"><span class="type">const</span> dim3 <span class="title">blocks</span><span class="params">((state_size + threads - <span class="number">1</span>) / threads,</span></span></span><br><span class="line"><span class="params"><span class="function">                      batch_size)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">AT_DISPATCH_FLOATING_TYPES</span>(X.<span class="built_in">type</span>(),</span><br><span class="line">                               <span class="string">"lltm_backward_cuda"</span>,</span><br><span class="line">                               ([&amp;] {</span><br><span class="line">                                   lltm_cuda_backward_kernel&lt;<span class="type">scalar_t</span>&gt;&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(</span><br><span class="line">                                           d_old_cell.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">                                           d_gates.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">                                           grad_h.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">                                           grad_cell.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">                                           new_cell.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">                                           input_gate.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">                                           output_gate.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">                                           candidate_cell.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">                                           gates.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">                                           state_size</span><br><span class="line">                                           );</span><br><span class="line"></span><br><span class="line">                               }));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> d_gate_weights = d_gates.<span class="built_in">reshape</span>({batch_size, <span class="number">3</span> * state_size});</span><br><span class="line">    <span class="keyword">auto</span> d_weights = d_gate_weights.<span class="built_in">t</span>().<span class="built_in">mm</span>(X);</span><br><span class="line">    <span class="keyword">auto</span> d_bias = d_gate_weights.<span class="built_in">sum</span>(<span class="comment">/*dim=*/</span><span class="number">0</span>, <span class="comment">/*keepdim=*/</span></span><br><span class="line">                                             <span class="literal">true</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> d_X = d_gate_weights.<span class="built_in">mm</span>(weights);</span><br><span class="line">    <span class="keyword">auto</span> d_old_h = d_X.<span class="built_in">slice</span>(<span class="comment">/*dim=*/</span><span class="number">1</span>,</span><br><span class="line">                                     <span class="number">0</span>,</span><br><span class="line">                                     state_size);</span><br><span class="line">    <span class="keyword">auto</span> d_input = d_X.<span class="built_in">slice</span>(<span class="comment">/*dim=*/</span><span class="number">1</span>,</span><br><span class="line">                                     state_size);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> {d_old_h, d_input, d_weights, d_bias, d_old_cell};</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h3 id="时间效率对比"><a href="#时间效率对比" class="headerlink" title="时间效率对比"></a>时间效率对比</h3><p>接下来，我们就可以调用扩展，方法是一样的，所以这里不多说，我使用的Jit的方式进行实时构建</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.cpp_extension <span class="keyword">import</span> load</span><br><span class="line">lltm_cuda = load(name=<span class="string">'lltm_cuda'</span>, </span><br><span class="line">                 sources=[<span class="string">'lltm/lltm_cuda.cpp'</span>, <span class="string">'lltm/lltm_cuda_kernel.cu'</span>], verbose=<span class="literal">True</span></span><br><span class="line">                 )</span><br></pre></td></tr></tbody></table></figure>
<p>同样也是加速对应的<code>LLTMFunction</code>，调用对应CUDA写的<code>forward</code>和<code>backward</code>即可。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LLTMFunction</span>(torch.autograd.Function):</span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">ctx, <span class="built_in">input</span>, weights, bias, old_h, old_cell</span>):</span><br><span class="line">        outputs = lltm_cuda.forward(<span class="built_in">input</span>, weights, bias, old_h, old_cell)</span><br><span class="line">        new_h, new_cell = outputs[:<span class="number">2</span>]</span><br><span class="line">        variables = outputs[<span class="number">1</span>:] + [weights]</span><br><span class="line">        ctx.save_for_backward(*variables)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> new_h, new_cell</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">ctx, grad_h, grad_cell</span>):</span><br><span class="line">        outputs = lltm_cuda.backward(</span><br><span class="line">            grad_h.contiguous(), grad_cell.contiguous(), *ctx.saved_tensors)</span><br><span class="line">        d_old_h, d_input, d_weights, d_bias, d_old_cell = outputs</span><br><span class="line">        <span class="keyword">return</span> d_input, d_weights, d_bias, d_old_h, d_old_cell</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://github.com/pytorch/extension-cpp">https://github.com/pytorch/extension-cpp</a></p>
<p><a href="https://pytorch.org/tutorials/advanced/cpp_extension.html#">https://pytorch.org/tutorials/advanced/cpp_extension.html#</a></p>
<p><a href="https://github.com/kevinghst/lltm_experiment">https://github.com/kevinghst/lltm_experiment</a></p>
]]></content>
      <categories>
        <category>CUDA</category>
      </categories>
      <tags>
        <tag>CUDA</tag>
      </tags>
  </entry>
  <entry>
    <title>conda和pip换源</title>
    <url>/2024/01/01/Linux/Anaconda%E6%8D%A2%E6%BA%90/</url>
    <content><![CDATA[<h2 id="pip换源"><a href="#pip换源" class="headerlink" title="pip换源"></a>pip换源</h2><h3 id="设为默认清华源"><a href="#设为默认清华源" class="headerlink" title="设为默认清华源"></a>设为默认清华源</h3><p>升级 pip 到最新的版本 (&gt;=10.0.0) 后进行配置：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">python -m pip install --upgrade pip</span><br><span class="line">pip config <span class="built_in">set</span> global.index-url https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></tbody></table></figure>
<p>如果您到 pip 默认源的网络连接较差，临时使用本镜像站来升级 pip：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">python -m pip install -i https://pypi.tuna.tsinghua.edu.cn/simple --upgrade pip</span><br></pre></td></tr></tbody></table></figure>
<p><a href="https://blog.csdn.net/wyf2017/article/details/118676765">https://blog.csdn.net/wyf2017/article/details/118676765</a></p>
<h3 id="修改配置"><a href="#修改配置" class="headerlink" title="修改配置"></a>修改配置</h3><p>vim ~/.pip/pip.conf<br>注意，这里设置的豆瓣源</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">[global]</span><br><span class="line"></span><br><span class="line">index-url = https://pypi.doubanio.com/simple</span><br><span class="line"></span><br><span class="line">trusted-host = pypi.doubanio.com</span><br></pre></td></tr></tbody></table></figure>
<h2 id="conda换源"><a href="#conda换源" class="headerlink" title="conda换源"></a>conda换源</h2><h3 id="清华源"><a href="#清华源" class="headerlink" title="清华源"></a>清华源</h3><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/conda</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/conda</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/conda</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge</span><br><span class="line">conda config --<span class="built_in">set</span> show_channel_urls <span class="built_in">yes</span></span><br></pre></td></tr></tbody></table></figure>
<p><strong>提前说</strong>：如果配置镜像源后报 HTTP 错误，只需要将源链接中的 https://… 中的 s 删掉就行 清华源</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forgeconda config --<span class="built_in">set</span> show_channel_urls yes</span><br></pre></td></tr></tbody></table></figure>
<h3 id="中科大源"><a href="#中科大源" class="headerlink" title="中科大源"></a>中科大源</h3><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">conda config --add channels https://mirrors.ustc.edu.cn/anaconda/pkgs/main/conda</span><br><span class="line">conda config --add channels https://mirrors.ustc.edu.cn/anaconda/pkgs/free/conda </span><br><span class="line">conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/conda-forge/conda </span><br><span class="line">conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/msys2/conda </span><br><span class="line">conda config --<span class="built_in">set</span> show_channel_urls yes</span><br></pre></td></tr></tbody></table></figure>
<h3 id="修改配置-1"><a href="#修改配置-1" class="headerlink" title="修改配置"></a>修改配置</h3><p>我们还刻有直接修改配置<br>vim ~/.condarc</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">channels:</span><br><span class="line">  - defaults</span><br><span class="line">show_channel_urls: <span class="literal">true</span></span><br><span class="line">default_channels:</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2</span><br><span class="line">custom_channels:</span><br><span class="line">  conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br></pre></td></tr></tbody></table></figure>
<h3 id="换为默认源"><a href="#换为默认源" class="headerlink" title="换为默认源"></a>换为默认源</h3><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">conda config --remove-key channels</span><br></pre></td></tr></tbody></table></figure>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux 安装CUDA 及 更新CUDA</title>
    <url>/2024/01/01/Linux/Linux%20%E5%AE%89%E8%A3%85CUDA%20%E5%8F%8A%20%E6%9B%B4%E6%96%B0CUDA/</url>
    <content><![CDATA[<h1 id="CUDA-更新"><a href="#CUDA-更新" class="headerlink" title="CUDA 更新"></a>CUDA 更新</h1><p>先装 CUDA [<a href="https://developer.nvidia.com/zh-cn/cuda-toolkit">下载地址</a>]，老版本的 CUDA 不用删掉，直接让管理员将 cuda 软连接到最新的 CUDA 就行了，以防有些代码需要低版本 CUDA，所以我们可以在多个CUDA版本进行切换。</p>
<p>再装驱动 <a href="https://www.nvidia.cn/Download/index.aspx?lang=cn">[驱动下载地址]</a>，安装过程会提示说检测到老版本驱动，直接卸载就行了。（但是在Linux中，我们只需要安装CUDA，里面会自带驱动安装，不需要重新安装驱动）</p>
<h1 id="CUDA-安装"><a href="#CUDA-安装" class="headerlink" title="CUDA 安装"></a>CUDA 安装</h1><h2 id="CUDA-简介"><a href="#CUDA-简介" class="headerlink" title="CUDA 简介"></a>CUDA 简介</h2><p>CUDA 是由 Nvidia 公司开发的并行计算平台和应用程序接口，软件开发者可以利用支持 CUDA 软件的 GPU 进行通用计算。CUDA 可以直接链接到 GPU 的虚拟指令集和并行计算单元，从而在 GPU 中完成内核函数的计算。<br>CUDA 提供 C/C++/Fortran 接口，也有许多高性能计算或深度学习库提供包装后的 Python 接口。开发者们可根据实际需要 (高性能计算, 深度学习, 神经网络等) 选择适当的编程语言。</p>
<h2 id="CUDA-安装步骤"><a href="#CUDA-安装步骤" class="headerlink" title="CUDA 安装步骤"></a>CUDA 安装步骤</h2><p>一般而言，在 Linux 下安装和使用 CUDA 的流程如下：</p>
<ol>
<li>安装 NVIDIA Driver，即显卡驱动</li>
<li>安装 CUDA Toolkit</li>
<li>使用 C/C++ 编译器或 Python 扩展库进行 GPU 加速的 CUDA 编程</li>
</ol>
<h2 id="安装CUDA"><a href="#安装CUDA" class="headerlink" title="安装CUDA"></a>安装CUDA</h2><ol>
<li>首先查看驱动，在命令行输入nvidia-smi查看显卡驱动版本也就是最高支持的CUDA工具包版本。<br>例如，本机可安装11.2及以下的CUDA工具包：<br><img src="https://pica.zhimg.com/v2-8eb67ea314c41ef10fe803db8ec37146.png" alt=""></li>
<li>在<a href="https://developer.nvidia.com/cuda-toolkit-archive">nvidia官网</a>选择对应版本的CUDA工具包并选择你的机器配置，我们就选择11.2.0版本下载，<br><img src="https://cdn.nlark.com/yuque/0/2023/png/32727715/1699412172755-39338a8a-468b-42d7-9f47-b7eae24f7b43.png#averageHue=%23faf9f4&amp;clientId=u8317f767-0156-4&amp;from=paste&amp;id=ub4eaf9ff&amp;originHeight=550&amp;originWidth=602&amp;originalType=url&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;taskId=ub607342c-fcd0-4d93-9eae-541091756f6&amp;title=" alt=""><br><img src="https://picx.zhimg.com/v2-7f66e7a0b551621641f20f164e44eaaa.png" alt=""></li>
<li>在终端执行如下命令：</li>
</ol>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">wget https://developer.download.nvidia.com/compute/cuda/11.2.0/local_installers/cuda_11.2.0_460.27.04_linux.run</span><br><span class="line">sudo sh cuda_11.2.0_460.27.04_linux.run</span><br></pre></td></tr></tbody></table></figure>
<p><img src="https://picx.zhimg.com/v2-d342c888c906d6ffb7e87abab83de0d1.png" alt=""><br>如果出现以下提示，选择continue并在第四步取消安装驱动即可。<br><img src="https://picx.zhimg.com/v2-a8f2cd8aec305e7c0ab32d0c21d51901.png" alt=""></p>
<ol>
<li>根据提示一步步安装键入accept确认。👇<br><img src="https://picx.zhimg.com/v2-94f230d6082cbf2b695ba71fda63655d.png" alt=""></li>
<li>我们已经有驱动了，这里取消安装驱动，上下键和回车键选择。👇<br><img src="https://picx.zhimg.com/v2-f0602a3e166fc875a2e1ca29ae91c0e6.png" alt=""></li>
<li>稍作等待，出现以下提示信息就安装好了，可以看到CUDA安装到了/usr/local/cuda-11.2/。<br><img src="https://pica.zhimg.com/v2-4e3d84d056ff7e7453c4f30e73531ea6.png" alt=""></li>
<li>配置环境变量</li>
</ol>
<p>打开配置文件</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">vi /etc/profile</span><br></pre></td></tr></tbody></table></figure>
<p>在配置文件末尾加上：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">export PATH=/usr/local/cuda-11.2/bin:$PATH</span><br><span class="line">export LD_LIBRARY_PATH=/usr/local/cuda-11.2/lib64$LD_LIBRARY_PATH</span><br><span class="line"></span><br><span class="line"># 第二种可以方便切换CUDA</span><br><span class="line">export PATH=/usr/local/cuda/bin:$PATH</span><br><span class="line">export LD_LIBRARY_PATH=/usr/local/cuda/lib64$LD_LIBRARY_PATH</span><br></pre></td></tr></tbody></table></figure>
<p>source 一下配置文件</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></tbody></table></figure>
<ol>
<li>检查是否安装完成</li>
</ol>
<p>使用nvcc -V检查CUDA是否安装完成，出现以下提示代表安装完成。</p>
<p><img src="https://pica.zhimg.com/v2-4c3f3cb58faba27427df57840bd54650.tiff" alt=""><br>编译并执行CUDA样例程序，出现pass代表CUDA和GPU正常运行：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">cd /usr/local/cuda-11.2/samples/1_Utilities/deviceQuery</span><br><span class="line">sudo make</span><br><span class="line">./deviceQuery</span><br></pre></td></tr></tbody></table></figure>
<p><img src="https://pic1.zhimg.com/v2-5c859ddd09830ca59b872170d68f9eb2.png" alt=""></p>
<h1 id="多个CUDA版本切换"><a href="#多个CUDA版本切换" class="headerlink" title="多个CUDA版本切换"></a>多个CUDA版本切换</h1><p>实际上，老版本的 CUDA 不用删掉，直接让管理员将 cuda 软连接到最新的 CUDA 就行了，以防有些代码需要低版本 CUDA，所以我们可以在多个CUDA版本进行切换。在linux里面，就是修改软连接即可，软连接到对应的CUDA就可以实现安装。</p>
<h2 id="root用户软链接"><a href="#root用户软链接" class="headerlink" title="root用户软链接"></a>root用户软链接</h2><h3 id="删除原来的软链接"><a href="#删除原来的软链接" class="headerlink" title="删除原来的软链接"></a>删除原来的软链接</h3><p><strong>第一种方法：</strong><br>经评论区大佬指点，可以使用unlink命令删除软链接：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/local</span><br><span class="line">sudo <span class="built_in">unlink</span> cuda</span><br></pre></td></tr></tbody></table></figure>
<p><strong>第二种方法：</strong><br>注意！不要多打一个’/‘，否则会删除了实际数据。<br>具体参见：<a href="https://link.zhihu.com/?target=https%3A//blog.51cto.com/kusorz/1876315">linux删除软链接的正确方式_每天进步一点的技术博客_51CTO博客_linux软连接</a></p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/local</span><br><span class="line">sudo <span class="built_in">rm</span> -rf cuda</span><br></pre></td></tr></tbody></table></figure>
<p>（千万不要多打’/‘ ！！！！！再说一遍！！！）</p>
<h3 id="建立新的软链接"><a href="#建立新的软链接" class="headerlink" title="建立新的软链接"></a>建立新的软链接</h3><p>建立指向cuda-10.0（需要的CUDA版本）版本的软链接</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">sudo <span class="built_in">ln</span> -snf /usr/local/cuda-8.0 /usr/local/cuda</span><br></pre></td></tr></tbody></table></figure>
<h3 id="查看当前CUDA版本"><a href="#查看当前CUDA版本" class="headerlink" title="查看当前CUDA版本"></a>查看当前CUDA版本</h3><p>通过以下命令来查看切换是否成功</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 查看'cuda'是否指向'/usr/local/cuda-需要的版本号'</span></span><br><span class="line"><span class="built_in">cd</span> /usr/local</span><br><span class="line"><span class="built_in">stat</span> cuda</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看当前CUDA版本号</span></span><br><span class="line">nvcc -V</span><br></pre></td></tr></tbody></table></figure>
<p>附查看所有CUDA版本的命令</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="built_in">ls</span> -l /usr/local | grep cuda</span><br></pre></td></tr></tbody></table></figure>
<p>下面原来是CUDA 11 ，现切换为CUDA10版本的操作：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">censhaoqi@censhaoqiVM:/usr/local$ nvcc -V</span><br><span class="line">nvcc: NVIDIA (R) Cuda compiler driver</span><br><span class="line">Copyright (c) 2005-2020 NVIDIA Corporation</span><br><span class="line">Built on Thu_Jun_11_22:26:38_PDT_2020</span><br><span class="line">Cuda compilation tools, release 11.0, V11.0.194</span><br><span class="line">Build cuda_11.0_bu.TC445_37.28540450_0</span><br><span class="line"></span><br><span class="line">censhaoqi@censhaoqiVM:/usr/local$ sudo <span class="built_in">rm</span> -rf cuda</span><br><span class="line">censhaoqi@censhaoqiVM:/usr/local$ sudo <span class="built_in">ln</span> -snf /usr/local/cuda-10.0 /usr/local/cuda</span><br><span class="line"></span><br><span class="line">censhaoqi@censhaoqiVM:/usr/local$ <span class="built_in">stat</span> cuda</span><br><span class="line">  File: <span class="string">'cuda'</span> -&gt; <span class="string">'/usr/local/cuda-10.0'</span></span><br><span class="line">  Size: 20        	Blocks: 0          IO Block: 4096   symbolic <span class="built_in">link</span></span><br><span class="line">Device: 8dh/141d	Inode: 36966009    Links: 1</span><br><span class="line">Access: (0777/lrwxrwxrwx)  Uid: (    0/    root)   Gid: (    0/    root)</span><br><span class="line">Access: 2021-09-15 19:58:35.449731251 +0800</span><br><span class="line">Modify: 2021-09-15 19:58:31.881787060 +0800</span><br><span class="line">Change: 2021-09-15 19:58:31.881787060 +0800</span><br><span class="line"> Birth: -</span><br><span class="line"></span><br><span class="line">censhaoqi@censhaoqiVM:/usr/local$ nvcc -V</span><br><span class="line">nvcc: NVIDIA (R) Cuda compiler driver</span><br><span class="line">Copyright (c) 2005-2018 NVIDIA Corporation</span><br><span class="line">Built on Sat_Aug_25_21:08:01_CDT_2018</span><br><span class="line">Cuda compilation tools, release 10.0, V10.0.130</span><br></pre></td></tr></tbody></table></figure>
<h2 id="个人用户设置路径"><a href="#个人用户设置路径" class="headerlink" title="个人用户设置路径"></a>个人用户设置路径</h2><p>我们可以在自己的~/.bashrc中设置cuda的路径也可以自由的切换我们的CUDA的版本，同样我们也可以使用alias</p>
<blockquote>
<p>Linux alias 命令用于设置指令的别名，用户可利用 alias，自定指令的别名。<br>它可以使您以一种更简单和易于记忆的方式执行命令，而不必每次都键入完整的命令。<br>若仅输入 alias，则可列出目前所有的别名设置。<br>alias 的效果仅在该次登入的操作有效，若想要每次登入都生效，可在 <strong>.profile</strong> 或 <strong>.cshrc</strong> 中设定指令的别名。</p>
</blockquote>
<figure class="highlight jsx"><table><tbody><tr><td class="code"><pre><span class="line">vim ~/.<span class="property">bashrc</span></span><br><span class="line"># 加入已有的<span class="variable constant_">CUDA</span>的路径，这里面一定要查看自己本机是否有安装好的cuda</span><br><span class="line"># 这里面在多个<span class="variable constant_">CUDA</span>安装后进行切换的</span><br><span class="line"># ls /usr/local/ 可查看cuda</span><br><span class="line">alias cuda111=<span class="string">'export PATH=/usr/local/cuda-11.1/bin:$PATH'</span></span><br><span class="line">alias cuda118=<span class="string">'export PATH=/usr/local/cuda-11.8/bin:$PATH'</span></span><br><span class="line">source ~/.<span class="property">bashrc</span></span><br><span class="line"># 接下来就可以自由切换cuda了，可nvcc -V查看是否正确切换</span><br><span class="line"># 使用cuda11<span class="number">.1</span></span><br><span class="line">cuda111 </span><br><span class="line"># 使用cuda11<span class="number">.8</span></span><br><span class="line">cuda118</span><br></pre></td></tr></tbody></table></figure>
<h1 id="cuDNN的安装"><a href="#cuDNN的安装" class="headerlink" title="cuDNN的安装"></a>cuDNN的安装</h1><p>根据安装的CUDA工具包版本在官网选择适合版本的cuDNN，本文安装的CUDA版本是11.2，就选择与之对应的cuDNN v8.4.0，选择Local Installer for Linux x86_64 (Tar)。</p>
<p>复制cuDNN库的链接，使用wget下载或者下载到自己电脑之后再传到服务器上。<br>我的服务器网速有点慢，所以选择先下到自己电脑再传上去，速度很快啊。</p>
<p>解压cuDNN文件，并进入解压出的文件夹，拷贝文件到/usr/local/cuda-11.2中</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">tar -xvf cudnn-linux-x86_64-8.4.0.27_cuda11.6-archive.tar.xz</span><br><span class="line"><span class="built_in">cd</span> cudnn-linux-x86_64-8.4.0.27_cuda11.6-archive</span><br><span class="line">sudo <span class="built_in">cp</span> lib/* /usr/local/cuda-11.2/lib64/</span><br><span class="line">sudo <span class="built_in">cp</span> include/* /usr/local/cuda-11.2/include/</span><br><span class="line">sudo <span class="built_in">chmod</span> a+r /usr/local/cuda-11.2/lib64/*</span><br><span class="line">sudo <span class="built_in">chmod</span> a+r /usr/local/cuda-11.2/include/*</span><br></pre></td></tr></tbody></table></figure>
<p>查看cuDNN版本，旧版本指令 为cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A2，新版本有更新，将cuDNN版本信息单拉了一个文件名为 cudnn_version.h，所以新版本查看cuDNN版本的命令为 cat /usr/local/cuda/include/cudnn_version.h | grep CUDNN_MAJOR -A 2</p>
<p>出现问题：Error ‘An NVIDIA kernel module ‘nvidia’ appears to already be loaded in your kernel’ when trying to get GPU support in AWS EMR<br><a href="https://unix.stackexchange.com/questions/440840/how-to-unload-kernel-module-nvidia-drm">https://unix.stackexchange.com/questions/440840/how-to-unload-kernel-module-nvidia-drm</a></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>有时候好像还要装cudnn，但是我那时候没装，不知道是不是必要，可以尝试一下</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/410764884">多个CUDA版本切换方法</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/79059379">Linux 下的 CUDA 安装和使用指南</a></li>
<li><a href="https://blog.csdn.net/tangjiahao10/article/details/125227005">【Linux】安装CUDA 11.2 和 cuDNN 8.4.0并检查是否安装成功_linux查看cudnn是否安装成功-CSDN博客</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/86441879">pytorch多gpu并行训练</a></li>
</ul>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>CUDA编程学习：自定义Pytorch+cpp/cuda extension</title>
    <url>/2023/12/12/CUDA/Pytorch+cppcuda%20extension%20%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<p>视频资料： </p>
<p><a href="https://www.youtube.com/watch?v=l_Rpk6CRJYI&amp;list=PLDV2CyUo4q-LKuiNltBqCKdO9GH4SS_ec&amp;ab_channel=AI%E8%91%B5">Pytorch+cpp/cuda extension 教學</a></p>
<p>Github：</p>
<p><a href="https://github.com/Kedreamix/pytorch-cppcuda-tutorial">https://github.com/Kedreamix/pytorch-cppcuda-tutorial</a></p>
<p>Pytorch官方资料：</p>
<p><a href="https://pytorch.org/cppdocs/">PyTorch C++ API - PyTorch main documentation</a></p>
<p><a href="https://pytorch.org/tutorials/advanced/cpp_extension.html">pytorch.org/tutorials/advanced/cpp_extension.html</a></p>
<p>CUDA doc：</p>
<p><a href="https://nyu-cds.github.io/python-gpu/02-cuda/">Introduction to GPU</a></p>
<h2 id="学习背景"><a href="#学习背景" class="headerlink" title="学习背景"></a>学习背景</h2><p>虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要<strong>更定制化的操作</strong>，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，这使我们可以充分利用自动微分<code>autograd</code>的功能，<strong>然而有时候代码在模型中被频繁调用或者调用代价比较大，我们就可能需要在C++中进行实现。另一个可能的原因可能需要依赖于其他的C++库，为了解决这种情况，PyTorch提供了一种非常简单的方式来编写自定义C++扩展。</strong></p>
<p>简单介绍一下Pytorch C++的API部分，主要有以下五部分</p>
<ol>
<li><strong>ATen：</strong> 作为基础张量和数学操作库，所有其他接口都构建在其上。</li>
<li><strong>Autograd：</strong> 通过自动微分增强了ATen，记录张量上的操作以形成自动微分图。</li>
<li><strong>C++ Frontend：</strong> 提供了用于神经网络和机器学习模型的高级纯C++建模接口。</li>
<li><strong>TorchScript：</strong> 是一个可以由TorchScript编译器理解、编译和序列化的PyTorch模型表示。</li>
<li><strong>C++ Extensions：</strong> 用于扩展Python API的自定义C++和CUDA例程。</li>
</ol>
<p>这些块组合形成了一个C++库，可用于张量计算和具有高效的GPU加速以及快速CPU性能的动态神经网络。</p>
<blockquote>
<p>在这部分中，ATen是一个基础张量库，几乎所有PyTorch的Python和C++接口都构建在其上。Autograd是C++ API的一部分，用于为ATen张量类添加自动微分功能。我们编写C++的扩展的时候，我们实际上是基于ATen进行操作和书写的。</p>
</blockquote>
<h2 id="适用对象与场景"><a href="#适用对象与场景" class="headerlink" title="适用对象与场景"></a>适用对象与场景</h2><p>实际上pytorch+cuda是为了加速pytorch的计算，如果pytorch的计算已经可以满足了，就可以跳过这一部分，因为本身pytorch也已经蕴含了很多的函数</p>
<ul>
<li><p><strong>非平行运算 non parallel computation</strong>：在这样的场景下，比如现在一个batch里面，都是平行运算，所以这时候可以直接用pytorch进行实现，但是在NeRF的体渲染volume rendering中，我们就可以知道，每一条射线可能采样的点都是不一样的，如果我们去用for循环就可能需要花比较多的时间，这时候就需要cuda的存在。</p>
</li>
<li><p><strong>大量的串列计算 lots of sequential computation</strong>：比如神经网络的卷积层的计算的，比如在forward中，经常会出现以下这样的情况</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">x = f1(x)</span><br><span class="line">x = f2(x)</span><br><span class="line">...</span><br><span class="line">x = fn(x)</span><br></pre></td></tr></tbody></table></figure>
<p>如果在层数比较小的时候，这样是可以得到不错的结果的，但是层数比较大的时候，不断的内存访问其实会减慢速度，这时候就需要CUDA来进行加速，比如我们可以将所有的f变成一个函数F，融合了所有的函数后，我们就可以进行一次运算得到最后的结果，在层数大的时候能得到很大的提升。</p>
</li>
</ul>
<p>在这一部分的学习中，主要还是在第一个场景，非平行运算，特别是NeRF的体渲染部分，这一部分的学习和加速还是非常重要的，值得学习。</p>
<h2 id="Pytorch和CUDA的关系"><a href="#Pytorch和CUDA的关系" class="headerlink" title="Pytorch和CUDA的关系"></a>Pytorch和CUDA的关系</h2><p>一般来说，是pytorch -&gt; C++ &gt; cuda，也就是pytorch调用C++，然后C++再调用cuda，所以C++作为的是一个桥梁，所以比较重要的cuda，而不是C++，利用cuda进行平行的计算。</p>
<p><img src="https://img-blog.csdnimg.cn/direct/64f3116d776247ae975479b252554c0a.png" alt="Pytorch和CUDA的关系"></p>
<h2 id="Python调用C-函数（桥梁）"><a href="#Python调用C-函数（桥梁）" class="headerlink" title="Python调用C++函数（桥梁）"></a>Python调用C++函数（桥梁）</h2><p>首先声明一下，我的文件夹格式如下：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">pytorch-cppcuda-tutorial/</span><br><span class="line">  test.py</span><br><span class="line">  setup.py</span><br><span class="line">  interpolation.cpp</span><br><span class="line">  interpolation_kernel.cu</span><br><span class="line">  include/</span><br><span class="line">    utils.h</span><br></pre></td></tr></tbody></table></figure>
<p><strong>有一个问题可能我们会疑惑很久，就是python是怎么调用C++和CUDA的，这里面根据课程简单来讲一下，以三线性插值为例子</strong></p>
<p>首先，我们定义一个简单的函数。这个函数接受两个参数，分别是特征和点，然后直接返回特征。在这里，我们将看到一个核心的东西，即 <code>PYBIND11_MODULE</code>。这是 Python 调用 C++ 函数的关键部分。这个函数会在Python执行<code>import</code>语句时被调用，其接受两个参数，第一个参数为模块名称，这里我们直接将<code>trilinear_interpolation</code>填入，稍候可以在Python中使用<code>import cppcuda_tutorial</code>导入该模块；第二个参数<code>m</code>是创建Python关联代码的主接口，其类型为<code>py::module_</code>。<code>module_::def()</code>用于生成能够将<code>trilinear_interpolation</code>函数暴露给Python的代码，其第一个参数为<strong>字符串</strong>，将会成为Python中调用的函数名；第二个参数是<strong>C++函数</strong>的引用；第三个参数是<strong>说明字符串</strong>，在Python中可以使用<code>help(trilinear_interpolation)</code>查看。比如下面的例子中，C++ 中的函数 <code>trilinear_interpolation</code> 对应 Python 中的 <code>trilinear_interpolation</code>。</p>
<blockquote>
<p><strong><torch extension.h=""></torch></strong>是一站式头文件，包含写入C++扩展所需的所有PyTorch操作，包括：</p>
<ul>
<li>ATen库是用于张量计算的主要API，</li>
<li>pybind11，是为C++代码创建Python绑定的方式</li>
<li>管理ATen和pybind11之间交互细节的头文件</li>
</ul>
<p>PyTorch的张量和变量接口是从ATen库自动生成的，因此几乎可以将Python实现1:1转换为C++。所有计算的主要数据类型将是torch::Tensor。</p>
</blockquote>
<figure class="highlight c++"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;torch/extension.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">torch::Tensor <span class="title">trilinear_interpolation</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Tensor feats,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Tensor point</span></span></span><br><span class="line"><span class="params"><span class="function">)</span></span>{</span><br><span class="line">    <span class="keyword">return</span> feats;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="built_in">PYBIND11_MODULE</span>(TORCH_EXTENSION_NAME, m){</span><br><span class="line">    m.<span class="built_in">def</span>(<span class="string">"trilinear_interpolation"</span>, &amp;trilinear_interpolation);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p><strong>注意：TORCH_EXTENSION_NAME，torch扩展构建会将其定义为在setup.py脚本中为扩展指定的名称。比如这里为“TORCH_EXTENSION_NAME“，两者之间的不匹配可能会导致严重且难以跟踪的问题。</strong></p>
<p>C++扩展一般有两种方式</p>
<ul>
<li>通过<code>setuptools</code>“提前”构建</li>
<li>通过<code>torch.utils.cpp_extension.load()</code>“实时”构建</li>
</ul>
<h3 id="Building-with-setuptools"><a href="#Building-with-setuptools" class="headerlink" title="Building with setuptools"></a>Building with <code>setuptools</code></h3><p>接下来先试用<code>setuptools</code>进行构建，编写一个 <code>setup.py</code> 文件，主要用于定义和说明一些重要的信息。其中关键的参数包括：</p>
<ul>
<li><code>name</code>：Python 调用的包的名称。</li>
<li><code>ext_modules</code> 的 <code>sources</code>：需要编译的 C++ 源文件，如果有多个 C++ 文件，需要列举所有。</li>
<li><code>cmdclass</code>：用BuildExtension执行许多必需的配置步骤和检查，并在混合C++/CUDA扩展的情况下处理混合编译。</li>
</ul>
<figure class="highlight c++"><table><tbody><tr><td class="code"><pre><span class="line">from setuptools <span class="keyword">import</span> setup</span><br><span class="line">from torch.utils.cpp_extension <span class="keyword">import</span> CppExtension, <span class="function">BuildExtension</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"><span class="title">setup</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    name=<span class="string">'cppcuda_tutorial'</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    version=<span class="string">'1.0'</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    author=<span class="string">'xxx'</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    author_email=<span class="string">'xxx@gmail.com'</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    description=<span class="string">'cppcuda example'</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    long_description=<span class="string">'cppcuda example'</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    ext_modules=[</span></span></span><br><span class="line"><span class="params"><span class="function">        CppExtension(</span></span></span><br><span class="line"><span class="params"><span class="function">            name=<span class="string">'cppcuda_tutorial'</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">            sources=[<span class="string">'interpolation.cpp'</span>])</span></span></span><br><span class="line"><span class="params"><span class="function">    ],</span></span></span><br><span class="line"><span class="params"><span class="function">    cmdclass={</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="string">'build_ext'</span>: BuildExtension</span></span></span><br><span class="line"><span class="params"><span class="function">    }</span></span></span><br><span class="line"><span class="params"><span class="function">)</span></span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="JIT-Compiling-Extensions"><a href="#JIT-Compiling-Extensions" class="headerlink" title="JIT Compiling Extensions"></a>JIT Compiling Extensions</h3><p>除了上述的<code>setuptools</code>的方法，接下来介绍即时编译（JIT）机制构建C++扩展。JIT编译机制通过调用PyTorch API中的一个简单函数<code>torch.utils.cpp_extension.load()</code>，为你提供了一种即时编译和加载扩展的方式。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.cpp_extension <span class="keyword">import</span> load</span><br><span class="line"></span><br><span class="line">cppcuda_tutorial = load(name=<span class="string">"cppcuda_tutorial"</span>,</span><br><span class="line">                        <span class="comment"># extra_include_paths=include_dirs,</span></span><br><span class="line">                        sources=[<span class="string">'interpolation.cpp'</span>],)</span><br></pre></td></tr></tbody></table></figure>
<p>在这里，实际提供的是域setuptools相同的信息。在后台，这将执行以下操作：</p>
<ol>
<li>创建一个临时目录<code>/tmp/torch_extensions/cppcuda_tutorial</code>，</li>
<li>向该临时目录发出Ninja构建文件，</li>
<li>将你的源文件编译成一个共享库，</li>
<li>将这个共享库导入为Python模块。</li>
</ol>
<p>实际上，如果将<code>verbose=True</code>传递给<code>cpp_extension.load()</code>，你将得到有关该过程的信息：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">Using /path/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...</span><br><span class="line">Detected CUDA files, patching ldflags</span><br><span class="line">Emitting ninja build file /path/.cache/torch_extensions/py310_cu113/cppcuda_tutorial/build.ninja...</span><br><span class="line">Building extension module cppcuda_tutorial...</span><br><span class="line">Allowing ninja to <span class="built_in">set</span> a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)</span><br><span class="line">[1/2] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=cppcuda_tutorial -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/path/workdirs/pytorch-cppcuda-tutorial/include -isystem /path/anaconda3/envs/cppcuda/lib/python3.10/site-packages/torch/include -isystem /path/anaconda3/envs/cppcuda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /path/anaconda3/envs/cppcuda/lib/python3.10/site-packages/torch/include/TH -isystem /path/anaconda3/envs/cppcuda/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /path/anaconda3/envs/cppcuda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=<span class="built_in">arch</span>=compute_86,code=compute_86 -gencode=<span class="built_in">arch</span>=compute_86,code=sm_86 --compiler-options <span class="string">'-fPIC'</span> -std=c++14 -c /path/workdirs/pytorch-cppcuda-tutorial/interpolation_kernel.cu -o interpolation_kernel.cuda.o </span><br><span class="line">[2/2] c++ interpolation.o interpolation_kernel.cuda.o -shared -L/path/anaconda3/envs/cppcuda/lib/python3.10/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda_cu -ltorch_cuda_cpp -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o cppcuda_tutorial.so</span><br></pre></td></tr></tbody></table></figure>
<p>完成这一步后，如果使用<code>setuptools</code>进行构建，我们可以使用 <code>pip</code> 进行安装。如果在当前文件夹下，直接运行 <code>pip install .</code> 即可完成安装或者我们也可以使用<code>python set.py install</code>，安装成功后应该会显示以下结果：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">Processing path/pytorch-cppcuda-tutorial</span><br><span class="line">  Preparing metadata (setup.py) ... <span class="keyword">done</span></span><br><span class="line">Building wheels <span class="keyword">for</span> collected packages: cppcuda-tutorial</span><br><span class="line">  Building wheel <span class="keyword">for</span> cppcuda-tutorial (setup.py) ... <span class="keyword">done</span></span><br><span class="line">  Created wheel <span class="keyword">for</span> cppcuda-tutorial: filename=cppcuda_tutorial-1.0-cp310-cp310-linux_x86_64.whl size=74123 sha256=3029b98bd3b49bed65f42640e60932c38379f15db48a5187fe40610b525307c9</span><br><span class="line">  Stored <span class="keyword">in</span> directory: /path/.cache/pip/wheels/65/53/4a/5e2c10d11e3a657b9efae376ccce3277e5535d691dd4659883</span><br><span class="line">Successfully built cppcuda-tutorial</span><br><span class="line">Installing collected packages: cppcuda-tutorial</span><br><span class="line">Successfully installed cppcuda-tutorial-1.0</span><br></pre></td></tr></tbody></table></figure>
<p>完成以上步骤后，我们可以编写一个 <code>test.py</code> 文件来测试代码的正确性。只要能够成功运行，就代表一切正常。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> cppcuda_tutorial <span class="comment"># 位置需要在import torch后面</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">feats = torch.ones(<span class="number">2</span>)</span><br><span class="line">point = torch.zeros(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用函数</span></span><br><span class="line">out = cppcuda_tutorial.trilinear_interpolation(feats, point)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(out)</span><br></pre></td></tr></tbody></table></figure>
<p>这里面要注意的就是，首席爱你要导入torch，解析动态链接器必须看到的一些符号</p>
<h2 id="CUDA加速的原理"><a href="#CUDA加速的原理" class="headerlink" title="CUDA加速的原理"></a>CUDA加速的原理</h2><p>首先介绍一个CUDA程序实现的流程</p>
<ol>
<li>把数据从CPU内存拷贝到GPU内存</li>
<li>调用核函数对存储在GPU内存中的数据进行操作</li>
<li>将数据从GPU内存传送回CPU内存</li>
</ol>
<p><img src="https://pic2.zhimg.com/v2-2959e07a36a8dc8f59280f53b43eb9d1_b.jpg" alt="CUDA编程入门极简教程- 知乎"></p>
<p>如下图所示，在利用CUDA加速的时候，图的左边是CPU，右边是GPU，我们需要把数据从CPU传到GPU中。在GPU中，就会生成对应的Grid来进行计算，每个Grid里面又有很多的block，从block中看又有很多的线程thread进行运算。我们也可以想象一下，如果计算一个矩阵的加法，我们可以让每个thread做对应的元素的相加，这样就可以大大加快计算速度，达到并行的效果。所以这之中内核（kernel）是CUDA编程中一个重要的部分，其代码在GPU上运行，比如矩阵乘法，我们就可以写一个加法的核函数，然后串行执行核函数，这样我们就快速能完成CUDA代码的编写，而不用在创建和管理大量的GPU线程时拘泥于细节。</p>
<p><img src="https://nyu-cds.github.io/python-gpu/fig/02-threadmapping.png" alt="Thread Mapping"></p>
<p>所以我们可以发现，实际上CUDA的计算是<code>Grid</code>——》<code>Block</code>——》<code>Thread</code>，然后用多个<code>Thread</code>进行计算，这里面可能会疑惑，为什么不是直接<code>Grid</code>——》<code>Thread</code>，实际上是因为硬件的限制是<code>Block</code>上限是$(2^{31}-1)<em>2^{16}</em>2^{16}$，<code>Thread</code>的上限是1024，所以这样的组合设计能够利用好更多的<code>Thread</code>，这也是为什么GPU速度那么快的原因。</p>
<h2 id="三线性插值问题定义"><a href="#三线性插值问题定义" class="headerlink" title="三线性插值问题定义"></a>三线性插值问题定义</h2><p>有关于线性插值和三线性插值的介绍，可以从这部分资料进行了解，<a href="https://zhuanlan.zhihu.com/p/77496615">https://zhuanlan.zhihu.com/p/77496615</a>，<a href="https://blog.csdn.net/webzhuce/article/details/86585489">https://blog.csdn.net/webzhuce/article/details/86585489</a>，这样我们就知道三线性插值的概念，和大概的思路，这样我们就可以进行一个CUDA的实现了。</p>
<p><img src="https://img-blog.csdnimg.cn/20190121221016700.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlYnpodWNl,size_16,color_FFFFFF,t_70" alt="三线性插值"></p>
<p>从三线性插值的概念我们可以知道，我们需要传入两个参数</p>
<ul>
<li>feats(N, 8, F)：N个立方体，每个立方体有8个点，每个点有F个特征</li>
<li>Points(N,3)：N个点，每个点的坐标分别是xyz，一共有三个维度 </li>
</ul>
<p>我们也可以知道输出的参数为<code>feat_interp(N, F)</code>，也就是插值后的结果</p>
<p>从上述定义我们就可以知道，我们有两个部分可以进行平行运算，分别是N和F，因为它们是独立的，不会相互影响计算。</p>
<h2 id="C-调用CUDA函数"><a href="#C-调用CUDA函数" class="headerlink" title="C++调用CUDA函数"></a>C++调用CUDA函数</h2><p>首先我们先看看，怎么使用CUDA去进行编程，首先CUDA的代码是<code>cu</code>结尾的，我们通过编写CUDA来进行一个计算加速。我们还是先按之前的方法，看看如何使用CUDA进行编程先，这里面有几个注意的点：</p>
<ul>
<li>需要编写CUDA函数</li>
<li>需要在头文件<code>.h</code>中去定义需要使用的函数，包括一些常用的测试函数。</li>
<li>修改CUDA的<code>setup.py</code></li>
</ul>
<p>接下来一步一步来，首先写一个<code>interpolation_kernel.cu</code>函数，也就是一个CUDA函数，后面我们可以用C++调用CUDA，这里面还是直接返回feats</p>
<figure class="highlight c++"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;torch/extension.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">torch::Tensor <span class="title">trilinear_fw_cu</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Tensor feats,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Tensor points</span></span></span><br><span class="line"><span class="params"><span class="function">)</span></span>{</span><br><span class="line">    <span class="keyword">return</span> feats;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>接下来，我们就需要在头文件<code>utils.h</code>中去定义我们在文件中需要使用的函数，类似于原本C++的一个声明和定义函数</p>
<figure class="highlight c++"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;torch/extension.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK_CUDA(x) TORCH_CHECK(x.is_cuda(), #x <span class="string">" must be a CUDA tensor"</span>)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x <span class="string">" must be contiguous"</span>)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 声明和定义函数</span></span><br><span class="line"><span class="function">torch::Tensor <span class="title">trilinear_fw_cu</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Tensor feats,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Tensor points</span></span></span><br><span class="line"><span class="params"><span class="function">)</span></span>;</span><br></pre></td></tr></tbody></table></figure>
<p>这样编写以后，我们的cpp的代码就可以调用CUDA对函数来进行调用，但是由于我们使用CUDA的函数，所以这里面我们还要用到<code>CHECK_INPUT</code>函数来判断是否在GPU上，也就是一个检测，并且内存是否连续，因为后续要进行一个并行的计算。</p>
<figure class="highlight c++"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">"utils.h"</span></span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function">torch::Tensor <span class="title">trilinear_interpolation</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Tensor feats,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Tensor points</span></span></span><br><span class="line"><span class="params"><span class="function">)</span></span>{</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(feats);</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(points);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">trilinear_fw_cu</span>(feats, points);</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">PYBIND11_MODULE</span>(TORCH_EXTENSION_NAME, m){</span><br><span class="line">    m.<span class="built_in">def</span>(<span class="string">"trilinear_interpolation"</span>, &amp;trilinear_interpolation);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>完成上述编写之后，我们最后就只剩下<code>setup.py</code>函数需要修改，其实需要修改的东西非常有限，只需要将上述的<code>CPPExtension</code>改为<code>CUDAExtension</code>，也就是改成CUDA的编译，这里面还有比较好的方法就是，之前我们source需要自己写，但是当我们有很多个文件的时候，我们就可以自动检索文件夹下的cpp和cu文件，进行build即可得到最后的结果。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">import</span> os.path <span class="keyword">as</span> osp</span><br><span class="line"><span class="keyword">from</span> setuptools <span class="keyword">import</span> setup</span><br><span class="line"><span class="keyword">from</span> torch.utils.cpp_extension <span class="keyword">import</span> CUDAExtension, BuildExtension</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ROOT_DIR = osp.dirname(osp.abspath(__file__))</span><br><span class="line">include_dirs = [osp.join(ROOT_DIR, <span class="string">"include"</span>)] <span class="comment"># 得到include文件夹下所有的头文件.h</span></span><br><span class="line"></span><br><span class="line">sources = glob.glob(<span class="string">'*.cpp'</span>)+glob.glob(<span class="string">'*.cu'</span>) <span class="comment"># 得到当前文件夹下所有cu文件和cpp文件</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">setup(</span><br><span class="line">    name=<span class="string">'cppcuda_tutorial'</span>,</span><br><span class="line">    version=<span class="string">'1.0'</span>,</span><br><span class="line">    author=<span class="string">'xxx'</span>,</span><br><span class="line">    author_email=<span class="string">'xxx@gmail.com'</span>,</span><br><span class="line">    description=<span class="string">'cppcuda_tutorial'</span>,</span><br><span class="line">    long_description=<span class="string">'cppcuda_tutorial'</span>,</span><br><span class="line">    ext_modules=[</span><br><span class="line">        CUDAExtension(</span><br><span class="line">            name=<span class="string">'cppcuda_tutorial'</span>,</span><br><span class="line">            sources=sources,</span><br><span class="line">            include_dirs=include_dirs,</span><br><span class="line">            extra_compile_args={<span class="string">'cxx'</span>: [<span class="string">'-O2'</span>],</span><br><span class="line">                                <span class="string">'nvcc'</span>: [<span class="string">'-O2'</span>]}</span><br><span class="line">        )</span><br><span class="line">    ],</span><br><span class="line">    cmdclass={</span><br><span class="line">        <span class="string">'build_ext'</span>: BuildExtension</span><br><span class="line">    }</span><br><span class="line">)</span><br></pre></td></tr></tbody></table></figure>
<p>安装过后，我们就可以测试是否使用CUDA进行计算，唯一不同的是，由于我们是使用CUDA进行计算，所以我们要把数据转到CUDA中即可</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> cppcuda_tutorial</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line"></span><br><span class="line">    feats = torch.ones(<span class="number">2</span>, device=<span class="string">'cuda'</span>)</span><br><span class="line">    points = torch.zeros(<span class="number">2</span>, device=<span class="string">'cuda'</span>)</span><br><span class="line"></span><br><span class="line">    out = cppcuda_tutorial.trilinear_interpolation(feats, points)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(out)</span><br></pre></td></tr></tbody></table></figure>
<blockquote>
<p>在这里，可能第一次学习会觉得比较麻烦，但是实际上有一些函数，比如CHECK的函数和setup.py的函数，只要写了一次以后，之后都是可以参考复用的，不用重复写</p>
</blockquote>
<h2 id="三线性插值CUDA实现"><a href="#三线性插值CUDA实现" class="headerlink" title="三线性插值CUDA实现"></a>三线性插值CUDA实现</h2><p>接下来就是主要的三线性插值的CUDA实现了，在前面的CUDA加速中有说到，实际上我们是希望在每一个thread都执行一个单元的计算，在三线性插值中，我们可以知道，我们两个部分需要并行，分别是<code>N</code>和<code>F</code>两个部分，也就是立方体的个数和特征的个数。</p>
<p>我们先看看需要进行编写的函数，然后一步一步的来解释和探索，以下是更新后的<code>forward</code>函数</p>
<figure class="highlight c++"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function">torch::Tensor <span class="title">trilinear_fw_cu</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Tensor feats,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Tensor points</span></span></span><br><span class="line"><span class="params"><span class="function">)</span></span>{</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> N = feats.<span class="built_in">size</span>(<span class="number">0</span>), F = feats.<span class="built_in">size</span>(<span class="number">2</span>);</span><br><span class="line">    <span class="comment">// 等价于 feat_interp = torch.zeros(N, F, dtype = torch.float32, device = "cuda:0")</span></span><br><span class="line">    torch::Tensor feat_interp = torch::<span class="built_in">zeros</span>({N, F}, feats.<span class="built_in">options</span>());</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">const</span> dim3 <span class="title">threads</span><span class="params">(<span class="number">16</span>, <span class="number">16</span>)</span></span>; <span class="comment">// 128,256,512</span></span><br><span class="line">    <span class="function"><span class="type">const</span> dim3 <span class="title">blocks</span><span class="params">((N+threads.x<span class="number">-1</span>)/threads.x, (F+threads.y<span class="number">-1</span>)/threads.y)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">AT_DISPATCH_FLOATING_TYPES</span>(feats.<span class="built_in">type</span>(), <span class="string">"trilinear_fw_cu"</span>, </span><br><span class="line">    ([&amp;] {</span><br><span class="line">        trilinear_fw_kernel&lt;<span class="type">scalar_t</span>&gt;&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(</span><br><span class="line">            feats.<span class="built_in">packed_accessor</span>&lt;<span class="type">scalar_t</span>, <span class="number">3</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt;(),</span><br><span class="line">            points.<span class="built_in">packed_accessor</span>&lt;<span class="type">scalar_t</span>, <span class="number">2</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt;(),</span><br><span class="line">            feat_interp.<span class="built_in">packed_accessor</span>&lt;<span class="type">scalar_t</span>, <span class="number">2</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt;()</span><br><span class="line">        );</span><br><span class="line">    }));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> feat_interp;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>第5行我们得到了对应的维度，分别是N和F，这也是我们最后需要返回值<code>feat_interp</code>的维度。</p>
<p>第7行我们初始化了变量<code>feat_interp</code>，这里面是初始化为zero，在里面还有一个参数是<code>feats.options()</code>，在CUDA编程中，<code>feats.options()</code>表示获取<code>feats</code>张量的选项（options）。选项包括张量的数据类型、设备（设备指定为CUDA或CPU）以及其他相关的配置信息。通过使用<code>feats.options()</code>，可以确保新创建的<code>feat_interp</code>张量与<code>feats</code>张量具有相同的选项，以便在相同的设备上进行操作，并保持一致性。</p>
<p>简单来说，就是保持一致的设备等等，这样就方便后续在同一个设备进行计算，和pytorch需要放在cpu和cuda上是一样的，除此之外，还有一些另外的写法，比如是创建一个整型的，可以写成如下，一样的意思。</p>
<figure class="highlight c++"><table><tbody><tr><td class="code"><pre><span class="line">torch::<span class="built_in">zeros</span>({N，F}，torch::<span class="built_in">dtype</span>(torch::kInt32).<span class="built_in">device</span>(feats,device));</span><br></pre></td></tr></tbody></table></figure>
<p>第9行和第10行就是定义上述提过的<code>threads</code>和<code>blocks</code>了，dim3是NVIDIA的CUDA编程中一种自定义的整型向量类型，基于用于指定维度的uint3，dim3类型最终设置的是一个三维向量，三维参数分别为x,y,z。在并行中，通常只支持三个并行，比如这里的N和F刚刚好就是两个并行，这里设置为16x16的线程，一般可以是128,256,512，不一定使用越多越好，这里面只是给了一个例子。</p>
<p>第10行中是定义了<code>blocks</code>的计算，<code>blocks</code>的个数实际上是计算的得到的，如下图所示，如果N=10，F=20，我们最后的输出就是10x20，这样我们就会用一个16x16的block去覆盖这整个矩阵，我们会发现大概需要2个矩阵，所以我们的<code>blocks</code>就是(2,1)，从下图也可以看出来，所以上述公式就是计算<code>block</code>的个数，这样我们就可以用每一个<code>thread</code>去计算，这样就能大大加快速度。</p>
<p><img src="https://img-blog.csdnimg.cn/direct/3dfa0f2ed94b4d9d9297a90e55a59662.png" alt="在这里插入图片描述"></p>
<p>第12~19行就是CUDA的核心函数，这里面就是一个启动核函数的部分，后面会提到核函数的编写，这里也是一个框架的部分，<code>AT_DISPATCH_FLOATING_TYPES</code> 是处理核函数的启动（使用 <code>&lt;&lt;&lt;...&gt;&gt;&gt;</code> 表示），它一般有三个参数</p>
<ul>
<li>一个类型 feats.type()</li>
<li>一个名称 “trilinear_fw_cu”，用于错误消息</li>
<li>一个 lambda 函数，是一个模版函数template，类型别名为 <code>scalar_t</code></li>
</ul>
<p>在这里面可以看出处理的是float类型的数据，如果想对所有类型进行操作而不仅仅是浮点类型（Float 和 Double），可以使用 <code>AT_DISPATCH_ALL_TYPES</code>。</p>
<h3 id="scalar-t类型"><a href="#scalar-t类型" class="headerlink" title="scalar_t类型"></a>scalar_t类型</h3><p>在函数之中，我们可以看到我们有三个input，其中两个是三线性插值的input，一个是output，为什么呢，是因为其实这个函数是没有返回值的，所以说实际上是在函数里面计算后复制在output之中，最后进行返回。</p>
<p>我们来仔细了解了一下具体函数的编写是什么意思，首先是<code>scalar_t</code>其实是一种类型，他可以表示任何类型，包括整型，浮点型等等，如果我们确定数据是float，我们也可以直接将<code>scalar_t</code>写为<code>float</code>，那么他可能就只能处理浮点型的数据了，从下面也可以看到<code>scalar_t</code>的一个简单的实现。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">switch (tensor.<span class="built_in">type</span>().scalarType()) {</span><br><span class="line">  <span class="keyword">case</span> torch::ScalarType::Double:</span><br><span class="line">    <span class="keyword">return</span> function&lt;double&gt;(tensor.data&lt;double&gt;());</span><br><span class="line">  <span class="keyword">case</span> torch::ScalarType::Float:</span><br><span class="line">    <span class="keyword">return</span> function&lt;<span class="built_in">float</span>&gt;(tensor.data&lt;<span class="built_in">float</span>&gt;());</span><br><span class="line">  ...</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<hr>
<h3 id="accessors"><a href="#accessors" class="headerlink" title="accessors"></a>accessors</h3><p>在CUDA计算的时候，还有一个问题，在 CUDA 核函数内部，虽然我们能正确处理数据，直接使用高级类型<code>scalar_t</code>不可知的张量将非常低效，因为这是以易用性和可读性为代价的，特别是对于高维数据。</p>
<p>比如说，在数据中，我们这里有(N, F)个数据，那我们有没有快速的方法去读取到<code>feat_interp[i][j]</code>的数据呢，特别是有些一般是三个维度的，比如(bs,row,index)这样的，并且有时候我们还需要知道stride才能快速索引到位置，比如<code>gates.data&lt;scalar_t&gt;()[n*3*state_size + row*state_size + column]</code></p>
<p>在这里面，我们可能就需要用到一个<code>ATen</code>提供的<code>accessors</code>，他可以动态检查确保张量具有指定的类型和维度数量，器提供了一个 API，用于高效地访问张量元素，而无需转换为单个指针，就可以高效访问 cpu 张量上的数据，cuda我们就可以用<code>packed_accessor</code>。</p>
<figure class="highlight c++"><table><tbody><tr><td class="code"><pre><span class="line">torch::Tensor foo = torch::<span class="built_in">rand</span>({<span class="number">12</span>, <span class="number">12</span>});</span><br><span class="line"></span><br><span class="line"><span class="comment">// 确定 foo 是二维的并且包含浮点数。</span></span><br><span class="line"><span class="keyword">auto</span> foo_a = foo.<span class="built_in">accessor</span>&lt;<span class="type">float</span>,<span class="number">2</span>&gt;();</span><br><span class="line"><span class="type">float</span> trace = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; foo_a.<span class="built_in">size</span>(<span class="number">0</span>); i++) {</span><br><span class="line">  <span class="comment">// 使用访问器 foo_a 来获取张量数据。</span></span><br><span class="line">  trace += foo_a[i][i];</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>所以我们在核函数内部看到了<code>packed_accessor</code>，这一部分就是做这样一件事情，<strong>不过值得注意的是，只有对torch的向量我们需要这样的操作，如果是bool等，我们是不需要处理的。</strong></p>
<hr>
<h3 id="模板函数"><a href="#模板函数" class="headerlink" title="模板函数"></a>模板函数</h3><p>上述有提到，实际上我们的<code>trilinear_fw_kernel</code>是一个模板函数，我们接下来看一下具体的实现，我们利用<code>scalar_t</code>对其进行实例化，我们在这里再解释一下这个模板函数的参数部分：</p>
<ul>
<li><p>首先是 <code>scalar_t</code>，它是一个模板参数，代表张量的数据类型。在这个上下文中，通常会使用 <code>float</code> 或 <code>double</code> 作为 <code>scalar_t</code>，具体取决于张量的数据类型。</p>
</li>
<li><p>接下来是 <code>3</code>，它表示张量的维度数量。在这个例子中，我们的feats的维度是3，所以维度数量为 3。</p>
</li>
<li><p>然后是 <code>torch::RestrictPtrTraits</code>，它是一个模板参数，用于指定指针的限定符。<code>__restrict__</code> 关键字在 CUDA 中用于指示指针是唯一的，并且没有别名。这有助于编译器进行优化，提高代码的性能。</p>
</li>
<li><p>最后是 <code>PackedTensorAccessor</code>，它是一个访问器（accessor）的变体，用于存储大小和步幅信息，可以使得在访问器对象传递给 CUDA 核函数时，内存传输的数据量也更小。</p>
</li>
</ul>
<p>接下来我们仔细分析里面代码的细节，首先介绍一下这个<code>__global__</code>，他实际意义如下，如下图所示</p>
<ul>
<li><p><code>__global__</code>表示CPU上定义，GPU上执行，是CUDA的关键字</p>
</li>
<li><p><code>__device__</code> GPU定 义，GPU执行</p>
</li>
<li><p><code>__host__</code> CPU定义，CPU执行</p>
</li>
</ul>
<p><img src="https://img2018.cnblogs.com/blog/1093303/201809/1093303-20180919123125957-1702896390.png" alt="CUDA编程之快速入门- 最难不过二叉树- 博客园"></p>
<p>接下里分析函数的主体部分，主要做两件事情：</p>
<ol>
<li>为每个<code>threads</code>进行编号</li>
<li>去除不必要的<code>threads</code></li>
</ol>
<p>在使用<code>threads</code>计算的时候，实际上每一个<code>threads</code>都有一个对应的编号，计算方式如第7,8行所示，实际上就是block的x*block的个数+block的y就能得到最后的结果</p>
<p>除了编号之外，还有去除不必要的<code>threads</code>，因为有一部分是没有覆盖到的，比如如下图的黄色部分就是不必要的<code>threads</code>，所以在第10行进行判断，如果超过范围，直接return不计算</p>
<p><img src="https://img-blog.csdnimg.cn/direct/6daed7e700bb4fb6927b4a8a89b8a1ea.png" alt="在这里插入图片描述"></p>
<p>最后就是上述说明的三线性插值的做法了，先进行一个标准化，然后代入公式进行计算，最后将值写入feat_interp中，就完成了整个模板函数的编写，大功告成！！！</p>
<p><img src="https://img-blog.csdnimg.cn/20190121221044883.png" alt="在这里插入图片描述"></p>
<figure class="highlight c++"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> <span class="type">scalar_t</span>&gt;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">trilinear_fw_kernel</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::PackedTensorAccessor&lt;<span class="type">scalar_t</span>, <span class="number">3</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt; feats,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::PackedTensorAccessor&lt;<span class="type">scalar_t</span>, <span class="number">2</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt; points,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::PackedTensorAccessor&lt;<span class="type">scalar_t</span>, <span class="number">2</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt; feat_interp</span></span></span><br><span class="line"><span class="params"><span class="function">)</span></span>{</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> n = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> f = blockIdx.y * blockDim.y + threadIdx.y;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (n&gt;=feats.<span class="built_in">size</span>(<span class="number">0</span>) || f&gt;=feats.<span class="built_in">size</span>(<span class="number">2</span>)) <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// point -1~1</span></span><br><span class="line">    <span class="type">const</span> <span class="type">scalar_t</span> u = (points[n][<span class="number">0</span>]+<span class="number">1</span>)/<span class="number">2</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">scalar_t</span> v = (points[n][<span class="number">1</span>]+<span class="number">1</span>)/<span class="number">2</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">scalar_t</span> w = (points[n][<span class="number">2</span>]+<span class="number">1</span>)/<span class="number">2</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="type">const</span> <span class="type">scalar_t</span> a = (<span class="number">1</span>-v)*(<span class="number">1</span>-w);</span><br><span class="line">    <span class="type">const</span> <span class="type">scalar_t</span> b = (<span class="number">1</span>-v)*w;</span><br><span class="line">    <span class="type">const</span> <span class="type">scalar_t</span> c = v*(<span class="number">1</span>-w);</span><br><span class="line">    <span class="type">const</span> <span class="type">scalar_t</span> d = <span class="number">1</span>-a-b-c;</span><br><span class="line">    feat_interp[n][f] = (<span class="number">1</span>-u)*(a*feats[n][<span class="number">0</span>][f] +</span><br><span class="line">                               b*feats[n][<span class="number">1</span>][f] +</span><br><span class="line">                               c*feats[n][<span class="number">2</span>][f] +</span><br><span class="line">                               d*feats[n][<span class="number">3</span>][f]) + </span><br><span class="line">                            u*(a*feats[n][<span class="number">4</span>][f] +</span><br><span class="line">                               b*feats[n][<span class="number">5</span>][f] +</span><br><span class="line">                               c*feats[n][<span class="number">6</span>][f] +</span><br><span class="line">                               d*feats[n][<span class="number">7</span>][f]);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h3 id="foward验证与比较"><a href="#foward验证与比较" class="headerlink" title="foward验证与比较"></a>foward验证与比较</h3><p>经过<code>python setup.py install</code>以后（每次修改后都要重新运行<code>setup.py</code>），我们就可以进行运行了，在这里面为了验证结果的正确性和与python进行比较，用python实现三线性插值的算法，比较两者的结果和时间效率，<code>test.py</code>如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> cppcuda_tutorial</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">trilinear_interpolation_py</span>(<span class="params">feats, points</span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">        feats: (N, 8, F)</span></span><br><span class="line"><span class="string">        points: (N, 3) local coordinates in [-1, 1]</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Outputs:</span></span><br><span class="line"><span class="string">        feats_interp: (N, F)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    u = (points[:, <span class="number">0</span>:<span class="number">1</span>]+<span class="number">1</span>)/<span class="number">2</span></span><br><span class="line">    v = (points[:, <span class="number">1</span>:<span class="number">2</span>]+<span class="number">1</span>)/<span class="number">2</span></span><br><span class="line">    w = (points[:, <span class="number">2</span>:<span class="number">3</span>]+<span class="number">1</span>)/<span class="number">2</span></span><br><span class="line">    a = (<span class="number">1</span>-v)*(<span class="number">1</span>-w)</span><br><span class="line">    b = (<span class="number">1</span>-v)*w</span><br><span class="line">    c = v*(<span class="number">1</span>-w)</span><br><span class="line">    d = <span class="number">1</span>-a-b-c</span><br><span class="line"></span><br><span class="line">    feats_interp = (<span class="number">1</span>-u)*(a*feats[:, <span class="number">0</span>] +</span><br><span class="line">                          b*feats[:, <span class="number">1</span>] +</span><br><span class="line">                          c*feats[:, <span class="number">2</span>] +</span><br><span class="line">                          d*feats[:, <span class="number">3</span>]) + \</span><br><span class="line">                       u*(a*feats[:, <span class="number">4</span>] +</span><br><span class="line">                          b*feats[:, <span class="number">5</span>] +</span><br><span class="line">                          c*feats[:, <span class="number">6</span>] +</span><br><span class="line">                          d*feats[:, <span class="number">7</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> feats_interp</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    N = <span class="number">65536</span>; F = <span class="number">256</span></span><br><span class="line">    feats = torch.rand(N, <span class="number">8</span>, F, device=<span class="string">'cuda'</span>).requires_grad_()</span><br><span class="line">    points = torch.rand(N, <span class="number">3</span>, device=<span class="string">'cuda'</span>)*<span class="number">2</span>-<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    t = time.time()</span><br><span class="line">    out_cuda = cppcuda_tutorial.trilinear_interpolation(feats, points)</span><br><span class="line">    torch.cuda.synchronize()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'   cuda time'</span>, time.time()-t, <span class="string">'s'</span>)</span><br><span class="line"></span><br><span class="line">    t = time.time()</span><br><span class="line">    out_py = trilinear_interpolation_py(feats, points)</span><br><span class="line">    torch.cuda.synchronize()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'pytorch time'</span>, time.time()-t, <span class="string">'s'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(torch.allclose(out_py, out_cuda)) <span class="comment"># 判断两者的差异</span></span><br></pre></td></tr></tbody></table></figure>
<p>经过运行和计算后，我们会得到以下结果，我们可以看到，CUDA是明显比Pytorch更快的，并且两者的计算结果也是一样的，如果需要更好的计算两者的速度的话，可能需要进行循环运行计算取平均更有可信度。</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">   cuda time 0.02436351776123047 s</span><br><span class="line">pytorch time 0.04364943504333496 s</span><br><span class="line">True</span><br></pre></td></tr></tbody></table></figure>
<h2 id="CUDA反向传播"><a href="#CUDA反向传播" class="headerlink" title="CUDA反向传播"></a>CUDA反向传播</h2><p>在上述实验中，当我们尝试添加自动求导的梯度计算时，使用<code>requires_grad_</code>，我们会发现通过CUDA返回的值实际上不会自动进行梯度计算（autograd）。然而，如果我们在Python中进行计算，它会自动进行梯度计算。</p>
<p>然而，在实际应用中，神经网络经常需要计算损失函数，并使用梯度下降等优化算法来不断优化参数。但是，在CUDA编程中，C++扩展API并没有提供自动求导（autograd）的方法。因此，我们必须自己实现反向传播的代码，计算每个输入的导数，并将其封装在<code>torch.autograd.Function</code>中。</p>
<p>在CUDA编程中，实现反向传播的代码通常包括以下步骤：</p>
<ol>
<li>在C++扩展中，创建一个新类，继承自<code>torch::autograd::Function</code>，用于定义前向传播和反向传播操作。</li>
<li>在新类中，重写<code>forward()</code>方法，定义前向传播的操作。这些操作将使用CUDA执行计算，并返回结果，其实就是上述的cuda的部分。</li>
<li>在新类中，重写<code>backward()</code>方法，定义反向传播的操作。这些操作将计算输入张量的梯度，并传递给上一层。</li>
<li>在CUDA和C++中，编写对应的<code>forward</code>和<code>backward</code>函数，计算前向传播和微分。</li>
<li>在Python代码中，使用这个自定义函数执行前向传播，并通过调用<code>backward()</code>方法执行反向传播。</li>
<li>在反向传播过程中，梯度将通过CUDA计算，并在每个层之间传递，从而计算出每个输入的导数。</li>
</ol>
<p>通过这种方式，我们可以在CUDA编程中手动实现反向传播，并获得每个输入的梯度，以便进行优化算法的参数更新。尽管需要手动编写反向传播代码，但这使我们能够在CUDA扩展中自定义梯度计算，并与PyTorch的自动求导机制无缝配合。</p>
<p>我们还是把三线性插值作为我们的一个例子，编写对应的反向传播，根据问题设定，我们可以知道我们的Points是固定的，所以我们需要对我们的Feats进行求微分。我们以简单的双线性插值来学习一下，怎么求微分，这里面其实涉及高数的知识，当然我们也可以把函数交给一些数学网站帮我们求得结果。<strong>从下图我们可以看到，<code>f</code>是双线性插值的结果，我们可以得到对应的四个导数，我们会发现实际上，他们的微分是对应的系数，推导在三线性插值也是一样的，所以他们对应的微分也就是对应的前缀。</strong></p>
<p><img src="https://img-blog.csdnimg.cn/direct/c35a354c6fcb49678872728cf04e6306.png" alt="在这里插入图片描述"></p>
<p>在计算反向传播前，我们往往会有一个对应的一个损失<code>Loss</code>，然后再进行求微分，这里面其实就用到了高数里面的链式法则，使用链式法则我们就可以得到<code>L</code>对每一个<code>feat</code>的微分，明白了双线性插值的计算，我们也可以推导在三线性插值中。</p>
<p><img src="https://img-blog.csdnimg.cn/direct/777b69824dcf4ca0ba257613067fa9be.png" alt="在这里插入图片描述"></p>
<h3 id="定义CUDA函数"><a href="#定义CUDA函数" class="headerlink" title="定义CUDA函数"></a>定义CUDA函数</h3><p>明白了理论的计算，我们就可以进行对应的实现，首先我们可以编写对应的反向传播的CUDA函数，这一部分实际上和前向传播是一样的，首先我们还是定义反向传播函数，在这里和前向传播函数的不同就是对名字进行了修改，除此之外加入了两个参数，分别是<code>dL_dfeats</code>参数和<code>dL_dfeats</code>。简单解释一下这些参数，在问题的设定中，我们的<code>feats</code>的维度是的(N, 8, F)，所以我们的微分<code>dL_dfeats</code>的维度是和<code>feats</code>是一样的，然后再加入反向传播的核函数中即可；<code>dL_dfeat_interp</code>则是根据函数已知的，所以不用计算，直接传参数。</p>
<figure class="highlight c++"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function">torch::Tensor <span class="title">trilinear_bw_cu</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::Tensor dL_dfeat_interp,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::Tensor feats,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::Tensor points</span></span></span><br><span class="line"><span class="params"><span class="function">)</span></span>{</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> N = feats.<span class="built_in">size</span>(<span class="number">0</span>), F = feats.<span class="built_in">size</span>(<span class="number">2</span>);</span><br><span class="line">    </span><br><span class="line">    torch::Tensor dL_dfeats = torch::<span class="built_in">empty</span>({N, <span class="number">8</span>, F}, feats.<span class="built_in">options</span>());</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">const</span> dim3 <span class="title">threads</span><span class="params">(<span class="number">16</span>, <span class="number">16</span>)</span></span>;</span><br><span class="line">    <span class="function"><span class="type">const</span> dim3 <span class="title">blocks</span><span class="params">((N+threads.x<span class="number">-1</span>)/threads.x, (F+threads.y<span class="number">-1</span>)/threads.y)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">AT_DISPATCH_FLOATING_TYPES</span>(feats.<span class="built_in">type</span>(), <span class="string">"trilinear_bw_cu"</span>, </span><br><span class="line">    ([&amp;] {</span><br><span class="line">        trilinear_bw_kernel&lt;<span class="type">scalar_t</span>&gt;&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(</span><br><span class="line">            dL_dfeat_interp.<span class="built_in">packed_accessor</span>&lt;<span class="type">scalar_t</span>, <span class="number">2</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt;(),</span><br><span class="line">            feats.<span class="built_in">packed_accessor</span>&lt;<span class="type">scalar_t</span>, <span class="number">3</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt;(),</span><br><span class="line">            points.<span class="built_in">packed_accessor</span>&lt;<span class="type">scalar_t</span>, <span class="number">2</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt;(),</span><br><span class="line">            dL_dfeats.<span class="built_in">packed_accessor</span>&lt;<span class="type">scalar_t</span>, <span class="number">3</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt;()</span><br><span class="line">        );</span><br><span class="line">    }));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dL_dfeats;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h3 id="核函数实现微分计算"><a href="#核函数实现微分计算" class="headerlink" title="核函数实现微分计算"></a>核函数实现微分计算</h3><p>接下来就是主要的核函数的实现，在这一部分我们就需要实现微分的计算，在前面已经介绍了双线性插值的微分的计算，推导在三线性插值是一样的，我们可以根据前向传播的代码，这一部分只需要保留前面的系数✖️对应位置的<code>dL_dfeat_interp</code>就可以得到最后的微分值，这一部分跟上述的推导是一模一样的。</p>
<figure class="highlight c++"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> <span class="type">scalar_t</span>&gt;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">trilinear_bw_kernel</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::PackedTensorAccessor&lt;<span class="type">scalar_t</span>, <span class="number">2</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt; dL_dfeat_interp,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::PackedTensorAccessor&lt;<span class="type">scalar_t</span>, <span class="number">3</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt; feats,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::PackedTensorAccessor&lt;<span class="type">scalar_t</span>, <span class="number">2</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt; points,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::PackedTensorAccessor&lt;<span class="type">scalar_t</span>, <span class="number">3</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt; dL_dfeats</span></span></span><br><span class="line"><span class="params"><span class="function">)</span></span>{</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> n = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> f = blockIdx.y * blockDim.y + threadIdx.y;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (n&gt;=feats.<span class="built_in">size</span>(<span class="number">0</span>) || f&gt;=feats.<span class="built_in">size</span>(<span class="number">2</span>)) <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// point -1~1</span></span><br><span class="line">    <span class="type">const</span> <span class="type">scalar_t</span> u = (points[n][<span class="number">0</span>]+<span class="number">1</span>)/<span class="number">2</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">scalar_t</span> v = (points[n][<span class="number">1</span>]+<span class="number">1</span>)/<span class="number">2</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">scalar_t</span> w = (points[n][<span class="number">2</span>]+<span class="number">1</span>)/<span class="number">2</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="type">const</span> <span class="type">scalar_t</span> a = (<span class="number">1</span>-v)*(<span class="number">1</span>-w);</span><br><span class="line">    <span class="type">const</span> <span class="type">scalar_t</span> b = (<span class="number">1</span>-v)*w;</span><br><span class="line">    <span class="type">const</span> <span class="type">scalar_t</span> c = v*(<span class="number">1</span>-w);</span><br><span class="line">    <span class="type">const</span> <span class="type">scalar_t</span> d = <span class="number">1</span>-a-b-c;</span><br><span class="line"></span><br><span class="line">    dL_dfeats[n][<span class="number">0</span>][f] = (<span class="number">1</span>-u)*a*dL_dfeat_interp[n][f];</span><br><span class="line">    dL_dfeats[n][<span class="number">1</span>][f] = (<span class="number">1</span>-u)*b*dL_dfeat_interp[n][f];</span><br><span class="line">    dL_dfeats[n][<span class="number">2</span>][f] = (<span class="number">1</span>-u)*c*dL_dfeat_interp[n][f];</span><br><span class="line">    dL_dfeats[n][<span class="number">3</span>][f] = (<span class="number">1</span>-u)*d*dL_dfeat_interp[n][f];</span><br><span class="line">    dL_dfeats[n][<span class="number">4</span>][f] = u*a*dL_dfeat_interp[n][f];</span><br><span class="line">    dL_dfeats[n][<span class="number">5</span>][f] = u*b*dL_dfeat_interp[n][f];</span><br><span class="line">    dL_dfeats[n][<span class="number">6</span>][f] = u*c*dL_dfeat_interp[n][f];</span><br><span class="line">    dL_dfeats[n][<span class="number">7</span>][f] = u*d*dL_dfeat_interp[n][f];</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h3 id="PYBIND11绑定函数"><a href="#PYBIND11绑定函数" class="headerlink" title="PYBIND11绑定函数"></a>PYBIND11绑定函数</h3><p>写好了反向传播函数之后，不要忘记绑定函数，这样我们才能在最后的python中调用对应的函数。</p>
<figure class="highlight c++"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function">torch::Tensor <span class="title">trilinear_interpolation_fw</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::Tensor feats,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::Tensor points</span></span></span><br><span class="line"><span class="params"><span class="function">)</span></span>{</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(feats);</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(points);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">trilinear_fw_cu</span>(feats, points);</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function">torch::Tensor <span class="title">trilinear_interpolation_bw</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::Tensor dL_dfeat_interp,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::Tensor feats,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::Tensor points</span></span></span><br><span class="line"><span class="params"><span class="function">)</span></span>{</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(dL_dfeat_interp);</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(feats);</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(points);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">trilinear_bw_cu</span>(dL_dfeat_interp, feats, points);</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">PYBIND11_MODULE</span>(TORCH_EXTENSION_NAME, m){</span><br><span class="line">    m.<span class="built_in">def</span>(<span class="string">"trilinear_interpolation_fw"</span>, &amp;trilinear_interpolation_fw);</span><br><span class="line">    m.<span class="built_in">def</span>(<span class="string">"trilinear_interpolation_bw"</span>, &amp;trilinear_interpolation_bw);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h3 id="torch-autograd-Function封装"><a href="#torch-autograd-Function封装" class="headerlink" title="torch.autograd.Function封装"></a>torch.autograd.Function封装</h3><p>为了使用pytorch的<code>autograd</code>我们还差最后一步，就是使用<code>torch.autograd.Function</code>进行封装，不然的话不能进行反向传播，会出现一些奇奇怪怪的bug。</p>
<p>在下面的代码中，首先，我们需要定义<code>forward</code>和<code>backward</code>函数，<strong>记得我们都需要定义<code>@staticmethod</code>装饰器，这个是一定要的。</strong>接下来我们就可以开始完善<code>forward</code>和<code>backward</code>函数。在两个函数中，实际上我们就是调用C++扩展写好的函数，这里面唯一一个需要注意的就是<code>ctx</code>，实际上这里是<code>context</code>的缩写，这里就是表示有什么数据需要进行保存在反向传播中使用到，因为在<code>backward</code>我们还要传入对应的<code>feats</code>和<code>points</code>，所以在这里这两个参数都需要<code>save_for_backward</code>。</p>
<p>最后的<code>backward</code>就更简单了，传入的参数与<code>forward</code>返回的参数进行对应，接着我们从<code>ctx</code>取出需要用到的参数，从<code>ctx.saved_tensors</code>中取出，后续只需要调用对应的C++函数即可，在这里面我们返回了两个参数，分别是<code>dL_dfeats, None</code>，这一部分是因为实际上是因为，我们有两个参数，分别<code>feats, points</code>，而我们并没有对<code>points</code>进行计算微分，所以这里就返回None。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Trilinear_interpolation_cuda</span>(torch.autograd.Function):</span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">ctx, feats, points</span>):</span><br><span class="line">        feat_interp = cppcuda_tutorial.trilinear_interpolation_fw(feats, points)</span><br><span class="line"></span><br><span class="line">        ctx.save_for_backward(feats, points)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> feat_interp</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">ctx, dL_dfeat_interp</span>):</span><br><span class="line">        feats, points = ctx.saved_tensors</span><br><span class="line"></span><br><span class="line">        dL_dfeats = cppcuda_tutorial.trilinear_interpolation_bw(dL_dfeat_interp.contiguous(), feats, points)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dL_dfeats, <span class="literal">None</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="backward验证与比较"><a href="#backward验证与比较" class="headerlink" title="backward验证与比较"></a>backward验证与比较</h3><p>和上述一样，经过<code>python setup.py install</code>以后（每次修改后都要重新运行<code>setup.py</code>），我们就可以进行运行了，在这里面为了验证结果的正确性和与pytorch本身的反向传播进行比较，比较两者的结果和时间效率，<code>test.py</code>的主函数如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Trilinear_interpolation_cuda</span>(torch.autograd.Function):</span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">ctx, feats, points</span>):</span><br><span class="line">        feat_interp = cppcuda_tutorial.trilinear_interpolation_fw(feats, points)</span><br><span class="line"></span><br><span class="line">        ctx.save_for_backward(feats, points)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> feat_interp</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">ctx, dL_dfeat_interp</span>):</span><br><span class="line">        feats, points = ctx.saved_tensors</span><br><span class="line"></span><br><span class="line">        dL_dfeats = cppcuda_tutorial.trilinear_interpolation_bw(dL_dfeat_interp.contiguous(), feats, points)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dL_dfeats, <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    N = <span class="number">65536</span>; F = <span class="number">256</span></span><br><span class="line">    rand = torch.rand(N, <span class="number">8</span>, F, device=<span class="string">'cuda'</span>)</span><br><span class="line">    feats = rand.clone().requires_grad_()</span><br><span class="line">    feats2 = rand.clone().requires_grad_()</span><br><span class="line">    points = torch.rand(N, <span class="number">3</span>, device=<span class="string">'cuda'</span>)*<span class="number">2</span>-<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    t = time.time()</span><br><span class="line">    <span class="comment"># 调用CUDA计算</span></span><br><span class="line">    out_cuda = Trilinear_interpolation_cuda.apply(feats2, points)</span><br><span class="line">    torch.cuda.synchronize()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'   cuda fw time'</span>, time.time()-t, <span class="string">'s'</span>)</span><br><span class="line"></span><br><span class="line">    t = time.time()</span><br><span class="line">    out_py = trilinear_interpolation_py(feats, points)</span><br><span class="line">    torch.cuda.synchronize()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'pytorch fw time'</span>, time.time()-t, <span class="string">'s'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'fw all close'</span>, torch.allclose(out_py, out_cuda))</span><br><span class="line"></span><br><span class="line">    t = time.time()</span><br><span class="line">    <span class="comment"># CUDA反向传播</span></span><br><span class="line">    loss2 = out_cuda.<span class="built_in">sum</span>()</span><br><span class="line">    loss2.backward()</span><br><span class="line">    torch.cuda.synchronize()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'   cuda bw time'</span>, time.time()-t, <span class="string">'s'</span>)</span><br><span class="line"></span><br><span class="line">    t = time.time()</span><br><span class="line">    loss = out_py.<span class="built_in">sum</span>()</span><br><span class="line">    loss.backward()</span><br><span class="line">    torch.cuda.synchronize()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'pytorch bw time'</span>, time.time()-t, <span class="string">'s'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'bw all close'</span>, torch.allclose(feats.grad, feats2.grad))</span><br></pre></td></tr></tbody></table></figure>
<p>经过运行和计算后，我们会得到以下结果，我们可以看到，CUDA和Pytorch前向传播相差不大，但是对于反向传播的效率可以看得出来，结果大概差了10倍所有，CUDA的反向传播还是有一个较为明显的效率提升的。</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">   cuda fw time 0.0033109188079833984 s</span><br><span class="line">pytorch fw time 0.004142045974731445 s</span><br><span class="line">fw all close True</span><br><span class="line">   cuda bw time 0.004648447036743164 s</span><br><span class="line">pytorch bw time 0.04614758491516113 s</span><br><span class="line">bw all close True</span><br></pre></td></tr></tbody></table></figure>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>视频资料： <a href="https://www.youtube.com/watch?v=l_Rpk6CRJYI&amp;list=PLDV2CyUo4q-LKuiNltBqCKdO9GH4SS_ec&amp;ab_channel=AI葵">https://www.youtube.com/watch?v=l_Rpk6CRJYI&amp;list=PLDV2CyUo4q-LKuiNltBqCKdO9GH4SS_ec&amp;ab_channel=AI%E8%91%B5</a></p>
<p>Github：<a href="https://github.com/kwea123/pytorch-cppcuda-tutorial">https://github.com/kwea123/pytorch-cppcuda-tutorial</a></p>
<p>Pytorch官方资料：<a href="https://pytorch.org/cppdocs/">https://pytorch.org/cppdocs/</a>，<a href="https://pytorch.org/tutorials/advanced/cpp_extension.html">https://pytorch.org/tutorials/advanced/cpp_extension.html</a></p>
<p>CUDA doc：<a href="https://nyu-cds.github.io/python-gpu/02-cuda/">https://nyu-cds.github.io/python-gpu/02-cuda/</a></p>
]]></content>
      <categories>
        <category>CUDA</category>
      </categories>
      <tags>
        <tag>CUDA</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux中修改用户UID和组GID的方法</title>
    <url>/2024/01/01/Linux/Linux%E4%B8%AD%E4%BF%AE%E6%94%B9%E7%94%A8%E6%88%B7UID%E5%92%8C%E7%BB%84GID%E7%9A%84%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<p>我在部署nfs的时候，共享了一个文件夹。为了让远程nfs客户端挂载这个文件夹的时候都有可读写权限，我需要把服务器上的用户uid、gid设置成nfs服务端文件夹一样的权限。不过因为之前新建的用户uid、gid都是系统自动生成的，几台服务器之前某个用户的uid、gid可能都不一样，所以现在需要把这个uid、gid都设置成统一某个值。</p>
<p>修改用户uid和组gid的命令分别是usermod和groupmod，思路很简单。先使用usermod修改用户的uid，然后使用groupmod修改组的gid，最后使用chown和chgrp命令修改原来用户文件和目录的属主属组。<br>例如测试用户foo和测试组foo。</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">foo old UID: 1005</span><br><span class="line">foo new UID: 2005</span><br><span class="line">foo old GID: 2000</span><br><span class="line">foo new GID: 3000</span><br></pre></td></tr></tbody></table></figure>
<h2 id="命令："><a href="#命令：" class="headerlink" title="命令："></a>命令：</h2><p>1、修改foo用户的uid<br></p><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">usermod -u 2005 foo</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>2、修改foo组的gid<br></p><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">groupmod -g 3000 foo</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>3、foo用户的家目录下面的文件属主和属组会在1、2命令执行后自动修改成新的uid、gid对应的属主属组，但是其他文件目录需要手动修改。手动修改的命令也比较简单。</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 利用name</span></span><br><span class="line">find / -user 1005 -<span class="built_in">exec</span> <span class="built_in">chown</span> -h foo {} ;</span><br><span class="line">find / -group 2000 -<span class="built_in">exec</span> <span class="built_in">chgrp</span> -h foo {} ;</span><br><span class="line"><span class="comment"># 利用old id</span></span><br><span class="line">find / -user 1005 -<span class="built_in">exec</span> <span class="built_in">chown</span> -h 2005 {} ;</span><br><span class="line">find / -group 2000 -<span class="built_in">exec</span> <span class="built_in">chgrp</span> -h 3000 {} ;</span><br></pre></td></tr></tbody></table></figure>
<p>这样用户和组的uid、gid就修改好了。可以用id命令看下是否修改的如我们所愿。<br></p><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="built_in">ls</span> -l /home/foo/</span><br><span class="line"></span><br><span class="line"><span class="built_in">id</span> -u foo</span><br><span class="line"></span><br><span class="line"><span class="built_in">id</span> -g foo</span><br><span class="line"></span><br><span class="line">grep foo /etc/passwd</span><br><span class="line"></span><br><span class="line">grep foo /etc/group</span><br></pre></td></tr></tbody></table></figure><p></p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux创建用户、设置密码</title>
    <url>/2024/01/01/Linux/Linux%E5%88%9B%E5%BB%BA%E7%94%A8%E6%88%B7%E3%80%81%E8%AE%BE%E7%BD%AE%E5%AF%86%E7%A0%81/</url>
    <content><![CDATA[<h2 id="创建用户"><a href="#创建用户" class="headerlink" title="创建用户"></a>创建用户</h2><p>由于每台服务器都需要连接到 NAS，而且可能很多用户在不同的服务器上都有账号，这样的话就会导致 uid 冲突（不同服务器上不同用户的 uid 可能是一样的），因此，针对不同情况需要用到不同添加用户的方法：</p>
<ol>
<li>该用户为新同学，说明他之前在其他服务器上没有账号，因此，先在 NAS 上为他开一个账号确保 uid 唯一性，再根据这个 uid 去其他的服务器上进行开号。</li>
<li>该用户在其他服务器上有账号，那就直接根据他的 uid 进行开号，无需再经过一遍 NAS。</li>
</ol>
<p>开号方式使用命令 useradd ，默认情况下直接 useradd user1 就可以了，用户目录为 /home/user1，但是考虑到服务器硬盘容量有限，最好将其划分到具有更大空间的目录如 /data，因此使用如下命令进行自定义添加用户<br>在服务器中，可以通过 df -h 来查看磁盘空间，默认uid和gid为同一个<br></p><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">$ useradd -u [uid] -g [gid] -d /data/user1 -m -s /bin/bash user1</span><br></pre></td></tr></tbody></table></figure><p></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>选项</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>-u UID</td>
<td>手工指定用户的 UID，注意 UID 的范围（不要小于 500）。</td>
</tr>
<tr>
<td>-d 主目录</td>
<td>手工指定用户的主目录。主目录必须写绝对路径，而且如果需要手工指定主目录，则一定要注意权限；</td>
</tr>
<tr>
<td>-c 用户说明</td>
<td>手工指定/etc/passwd文件中各用户信息中第 5 个字段的描述性内容，可随意配置；</td>
</tr>
<tr>
<td>-g 组名</td>
<td>手工指定用户的初始组。一般以和用户名相同的组作为用户的初始组，在创建用户时会默认建立初始组。一旦手动指定，则系统将不会在创建此默认的初始组目录。</td>
</tr>
<tr>
<td>-G 组名</td>
<td>指定用户的附加组。我们把用户加入其他组，一般都使用附加组；</td>
</tr>
<tr>
<td>-s shell</td>
<td>手工指定用户的登录 Shell，默认是 /bin/bash；</td>
</tr>
<tr>
<td>-e 曰期</td>
<td>指定用户的失效曰期，格式为 “YYYY-MM-DD”。也就是 /etc/shadow 文件的第八个字段；</td>
</tr>
<tr>
<td>-o</td>
<td>允许创建的用户的 UID 相同。例如，执行 “useradd -u 0 -o usertest” 命令建立用户 usertest，它的 UID 和 root 用户的 UID 相同，都是 0；</td>
</tr>
<tr>
<td>-m</td>
<td>建立用户时强制建立用户的家目录。在建立系统用户时，该选项是默认的；</td>
</tr>
<tr>
<td>-r</td>
<td>创建系统用户，也就是 UID 在 1~499 之间，供系统程序使用的用户。由于系统用户主要用于运行系统所需服务的权限配置，因此系统用户的创建默认不会创建主目录。</td>
</tr>
</tbody>
</table>
</div>
<h2 id="设置密码"><a href="#设置密码" class="headerlink" title="设置密码"></a>设置密码</h2><p>开了用户以后，可以进行设置密码<br></p><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">sudo passwd user1</span><br></pre></td></tr></tbody></table></figure><br>设置密码的时候，可以进行随机生成密码得到结果，这样不容易被爆破<br><a href="https://1password.com/zh-cn/password-generator/">创建强大、安全、随机的密码 | Password 密码生成器 | 1Password</a><br><a href="https://suijimimashengcheng.bmcx.com">生成随机密码 - 密码生成器 - 密码批量生成器</a><p></p>
<h2 id="权限设置"><a href="#权限设置" class="headerlink" title="权限设置"></a>权限设置</h2><p>如果需要加入一些权限，比如root权限，首先我们可以查看一下用户的权限<br></p><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">sudo <span class="built_in">cat</span> /etc/sudoers</span><br></pre></td></tr></tbody></table></figure><p></p>
<ul>
<li>增加权限 sudo gpasswd -a username sudo / sudo usermod -aG sudo username </li>
<li>删除权限 sudo gpasswd -d username sudo</li>
</ul>
<h2 id="命令行窗口下用户的相互切换"><a href="#命令行窗口下用户的相互切换" class="headerlink" title="命令行窗口下用户的相互切换"></a>命令行窗口下用户的相互切换</h2><p>su 用户名 说明：su是switch user的缩写，表示用户切换</p>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><ul>
<li>usermod —help 修改用户这个命令的相关参数</li>
<li>userdel testuser 删除用户testuser</li>
<li>rm -rf testuser 删除用户testuser所在目录</li>
</ul>
<p>创建新用户后，同时会在etc目录下的passwd文件中添加这个新用户的相关信息</p>
<ul>
<li>groupadd testgroup 组的添加</li>
<li>groupdel testgroup 组的删除 </li>
</ul>
<p>说明：组的增加和删除信息会在etc目录的group文件中体现出来。</p>
<p>管理员查看所有用户</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="built_in">cat</span> /etc/passwd</span><br></pre></td></tr></tbody></table></figure>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux中的SSH密钥登录连接</title>
    <url>/2024/01/01/Linux/Linux%E4%B8%AD%E7%9A%84SSH%E5%AF%86%E9%92%A5%E7%99%BB%E5%BD%95%E8%BF%9E%E6%8E%A5/</url>
    <content><![CDATA[<h1 id="简单两步走"><a href="#简单两步走" class="headerlink" title="简单两步走"></a>简单两步走</h1><p>如果不想看那么多原理，可以简单两步走</p>
<ol>
<li><p><strong>输入以下命令</strong></p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">ssh-keygen</span><br><span class="line"><span class="built_in">cd</span> .ssh</span><br><span class="line"><span class="built_in">cat</span> id_rsa.pub &gt;&gt; authorized_keys</span><br><span class="line"><span class="built_in">chmod</span> 600 authorized_keys</span><br><span class="line"><span class="built_in">chmod</span> 700 ~/.ssh</span><br></pre></td></tr></tbody></table></figure>
</li>
<li><p><strong>下载对应密钥id_rsa即可，后续就可以密钥登录</strong></p>
</li>
</ol>
<h1 id="设置-SSH-通过密钥登录"><a href="#设置-SSH-通过密钥登录" class="headerlink" title="设置 SSH 通过密钥登录"></a>设置 SSH 通过密钥登录</h1><p><img src="https://pic1.zhimg.com/80/v2-e62d325d3c609b70f5a297322c8739be.png" alt=""></p>
<p>我们一般使用 PuTTY 等 SSH 客户端来远程管理 Linux 服务器。但是，一般的密码方式登录，容易有密码被暴力破解的问题。所以，一般我们会将 SSH 的端口设置为默认的 22 以外的端口，或者禁用 root 账户登录。其实，有一个更好的办法来保证安全，而且让你可以放心地用 root 账户从远程登录——那就是通过密钥方式登录。<br>密钥形式登录的原理是：利用密钥生成器制作一对密钥——一只公钥和一只私钥。将公钥添加到服务器的某个账户上，然后在客户端利用私钥即可完成认证并登录。这样一来，没有私钥，任何人都无法通过 SSH 暴力破解你的密码来远程登录到系统。此外，如果将公钥复制到其他账户甚至主机，利用私钥也可以登录。<br>下面来讲解如何在 Linux 服务器上制作密钥对，将公钥添加给账户，设置 SSH，最后通过客户端登录。</p>
<h2 id="1-制作密钥对"><a href="#1-制作密钥对" class="headerlink" title="1. 制作密钥对"></a>1. 制作密钥对</h2><p>首先在服务器上制作密钥对。首先用密码登录到你打算使用密钥登录的账户，然后执行以下命令：<br></p><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">[root@host ~]$ ssh-keygen  &lt;== 建立密钥对</span><br><span class="line">Generating public/private rsa key pair.</span><br><span class="line">Enter file <span class="keyword">in</span> <span class="built_in">which</span> to save the key (/root/.ssh/id_rsa): &lt;== 按 Enter</span><br><span class="line">Created directory <span class="string">'/root/.ssh'</span>.</span><br><span class="line">Enter passphrase (empty <span class="keyword">for</span> no passphrase): &lt;== 输入密钥锁码，或直接按 Enter 留空</span><br><span class="line">Enter same passphrase again: &lt;== 再输入一遍密钥锁码</span><br><span class="line">Your identification has been saved <span class="keyword">in</span> /root/.ssh/id_rsa. &lt;== 私钥</span><br><span class="line">Your public key has been saved <span class="keyword">in</span> /root/.ssh/id_rsa.pub. &lt;== 公钥</span><br><span class="line">The key fingerprint is:</span><br><span class="line">0f:d3:e7:1a:1c:bd:5c:03:f1:19:f1:22:<span class="built_in">df</span>:9b:cc:08 root@host</span><br></pre></td></tr></tbody></table></figure><br>密钥锁码在使用私钥时必须输入，这样就可以保护私钥不被盗用。当然，也可以留空，实现无密码登录。<br>现在，在 root 用户的家目录中生成了一个 .ssh 的隐藏目录，内含两个密钥文件。id_rsa 为私钥，id_rsa.pub 为公钥。<br>这个在windows也可以制作自己的密钥对<p></p>
<h2 id="2-在服务器上安装公钥"><a href="#2-在服务器上安装公钥" class="headerlink" title="2. 在服务器上安装公钥"></a>2. 在服务器上安装公钥</h2><p>键入以下命令，在服务器上安装公钥：<br></p><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">[root@host ~]$ <span class="built_in">cd</span> .ssh</span><br><span class="line">[root@host .ssh]$ <span class="built_in">cat</span> id_rsa.pub &gt;&gt; authorized_keys</span><br></pre></td></tr></tbody></table></figure><br>这一部分相当于，将自己的公钥写到了ssh服务器中，写到authorized_keys中，这样只要有密钥对，就能正常连接，VScode也是<br>如此便完成了公钥的安装。为了确保连接成功，请保证以下文件权限正确：<br><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">[root@host .ssh]$ <span class="built_in">chmod</span> 600 authorized_keys</span><br><span class="line">[root@host .ssh]$ <span class="built_in">chmod</span> 700 ~/.ssh</span><br></pre></td></tr></tbody></table></figure><p></p>
<h2 id="3-设置-SSH，打开密钥登录功能-（管理员可以设置仅仅密钥登录）"><a href="#3-设置-SSH，打开密钥登录功能-（管理员可以设置仅仅密钥登录）" class="headerlink" title="3. 设置 SSH，打开密钥登录功能 （管理员可以设置仅仅密钥登录）"></a>3. 设置 SSH，打开密钥登录功能 （管理员可以设置仅仅密钥登录）</h2><p>编辑 /etc/ssh/sshd_config 文件，进行如下设置：<br></p><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">RSAAuthentication <span class="built_in">yes</span></span><br><span class="line">PubkeyAuthentication <span class="built_in">yes</span></span><br></pre></td></tr></tbody></table></figure><br>另外，请留意 root 用户能否通过 SSH 登录：<br><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">PermitRootLogin <span class="built_in">yes</span></span><br></pre></td></tr></tbody></table></figure><br>当你完成全部设置，并以密钥方式登录成功后，再禁用密码登录：<br><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">PasswordAuthentication no</span><br></pre></td></tr></tbody></table></figure><br>最后，重启 SSH 服务：<br><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">[root@host .ssh]$ service sshd restart</span><br></pre></td></tr></tbody></table></figure><br><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">#允许root认证登录</span></span><br><span class="line">PermitRootLogin <span class="built_in">yes</span></span><br><span class="line"><span class="comment">#允许密钥认证</span></span><br><span class="line">RSAAuthentication <span class="built_in">yes</span></span><br><span class="line">PubkeyAuthentication <span class="built_in">yes</span></span><br></pre></td></tr></tbody></table></figure><p></p>
<h2 id="4-将私钥下载到客户端，然后转换为-PuTTY-能使用的格式"><a href="#4-将私钥下载到客户端，然后转换为-PuTTY-能使用的格式" class="headerlink" title="4. 将私钥下载到客户端，然后转换为 PuTTY 能使用的格式"></a>4. 将私钥下载到客户端，然后转换为 PuTTY 能使用的格式</h2><p>使用 WinSCP、SFTP 等工具将私钥文件 id_rsa 下载到客户端机器上。然后打开 PuTTYGen，单击 Actions 中的 Load 按钮，载入你刚才下载到的私钥文件。如果你刚才设置了密钥锁码，这时则需要输入。<br>载入成功后，PuTTYGen 会显示密钥相关的信息。在 Key comment 中键入对密钥的说明信息，然后单击 Save private key 按钮即可将私钥文件存放为 PuTTY 能使用的格式。<br>今后，当你使用 PuTTY 登录时，可以在左侧的 Connection -&gt; SSH -&gt; Auth 中的 Private key file for authentication: 处选择你的私钥文件，然后即可登录了，过程中只需输入密钥锁码即可。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://wangdoc.com/ssh/key#ssh-keygen%E5%91%BD%E4%BB%A4%E7%94%9F%E6%88%90%E5%AF%86%E9%92%A5">https://wangdoc.com/ssh/key#ssh-keygen%E5%91%BD%E4%BB%A4%E7%94%9F%E6%88%90%E5%AF%86%E9%92%A5</a></p>
<h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><p>其实可以一个密钥多个服务器，只要把公钥输入到authorized_keys中，这样只要有密钥对，就能正常连接，所以可以用很多个即可，不过为了安全，可以只用一个密钥</p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux服务器联网攻略</title>
    <url>/2024/01/01/Linux/Linux%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%81%94%E7%BD%91%E6%94%BB%E7%95%A5/</url>
    <content><![CDATA[<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>学校的个人账号只能在一台设备上进行认证联网，但是我们使用的 GPU 服务器经常需要访问互联网，在服务器上认证之后我们自己的电脑就会掉线，所以可以通过代理的方式让服务器通过我们自己的设备进行联网，解决这个问题。<br><strong>以下所有的方法，实际上让服务器可通过代理本地网络上网（解决服务器连不上网时使用）</strong></p>
<blockquote>
<p>此文档是借鉴实验室的师兄的文档完善而来，在这里感谢木子李！！！</p>
</blockquote>
<h1 id="设置http代理"><a href="#设置http代理" class="headerlink" title="设置http代理"></a>设置http代理</h1><p>首先在自己本机电脑上，查看自己的ip地址，在当前终端进行输入<br></p><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> http_proxy=http://ip:7890</span><br><span class="line"><span class="built_in">export</span> https_proxy=https://ip:7890</span><br><span class="line">curl cip.cc <span class="comment"># 有输出说明成功代理到了</span></span><br></pre></td></tr></tbody></table></figure><br>若希望不用每次终端都需要设置，也可以把export的部分写入~/.bashrc中，设置后进行source ~/.basrc即可<p></p>
<blockquote>
<p>这里面的7890端口是通过clash进行转发的，可以进行下载clash</p>
<p>除此之外，也需要设置代理软件才能让服务器访问到网络，在 clash 中打开 Allow LAN， v2ray 中打开 允许局域网的连接 就行了。这样我们的设备可以访问的东西，服务器都可以访问到。</p>
<p>以下是两种软件设置举例（注意：服务器端代理上网行为和代理软件设置一致）。</p>
<p>V2ray设置中开启允许局域网访问（需要注意本地端口和局域网端口不一样）</p>
<p><img src="https://pica.zhimg.com/v2-7c222d179d5454cef5d6a55f9a4f26b6.jpg" alt=""></p>
<p>clash开启局域网<br><img src="https://picx.zhimg.com/v2-d8dea34b8e6980fc40d3fb790733d70b.png" alt=""></p>
</blockquote>
<p>上面这种方法有时候每次都需要打两行命令，还有一种比较简单的方法，在 .bashrc 中设置alias</p>
<blockquote>
<p>Linux alias 命令用于设置指令的别名，用户可利用 alias，自定指令的别名。<br>它可以使您以一种更简单和易于记忆的方式执行命令，而不必每次都键入完整的命令。<br>若仅输入 alias，则可列出目前所有的别名设置。<br>alias 的效果仅在该次登入的操作有效，若想要每次登入都生效，可在 <strong>.profile</strong> 或 <strong>.cshrc</strong> 中设定指令的别名。</p>
</blockquote>
<figure class="highlight jsx"><table><tbody><tr><td class="code"><pre><span class="line">vim ~/.<span class="property">bashrc</span></span><br><span class="line"># 在最后一行加入</span><br><span class="line">alias setproxy=<span class="string">'export http_proxy=http://ip:7890; export https_proxy=http://ip:7890'</span></span><br><span class="line"># 启动</span><br><span class="line">source ~/.<span class="property">bashrc</span></span><br><span class="line"># 运行上网命令</span><br><span class="line">setproxy</span><br></pre></td></tr></tbody></table></figure>
<p>这样以后，每次只需要输入 setproxy 就可以方便快捷的连入自己的网络了。</p>
<h1 id="借助proxychains工具"><a href="#借助proxychains工具" class="headerlink" title="借助proxychains工具"></a>借助proxychains工具</h1><p>核心：服务器端借助proxychains-ng 将应用“http/https/socks4/socks5”请求代理到本地。</p>
<h2 id="安装软件"><a href="#安装软件" class="headerlink" title="安装软件"></a>安装软件</h2><ol>
<li><p>主要是通过 proxychains-ng 来转发网络请求，可以通过 git 下载也可以直接下载压缩包。（可在仓库直接下载zip 上传服务器解压）</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/rofl0r/proxychains-ng</span><br></pre></td></tr></tbody></table></figure>
</li>
<li><p>然后进入软件目录，用 pwd 命令看一下当前的绝对路径，这个在下一步中要用到</p>
</li>
</ol>
<p><img src="https://picx.zhimg.com/80/v2-68aa3d2a2b0e5ce78f7643598f82b814.png" alt=""></p>
<ol>
<li>进入目录执行命令，这里的 pwd 就是上一步输出的<strong>绝对路径</strong>，要输<strong>绝对路径</strong>，不然后面编译的时候会出错</li>
</ol>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># ./configure --prefix=pwd --sysconfdir=/raid/xxx/proxychains</span></span><br><span class="line">./configure --prefix=<span class="built_in">pwd</span> --sysconfdir=<span class="built_in">pwd</span></span><br></pre></td></tr></tbody></table></figure>
<ol>
<li>编译二进制文件</li>
</ol>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">make</span><br><span class="line">make install</span><br><span class="line">make install-config</span><br></pre></td></tr></tbody></table></figure>
<h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><p>进入安装目录找到配置文件 proxychains.conf，进行编辑<br>在底部添加需要代理的设备的 ip 和端口，我使用的 clash，是 socks 代理，在其底部添加本地代理软件的本机IP 和 代理软件的局域网端口号。<br></p><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">vim proxychains/proxychains.conf</span><br></pre></td></tr></tbody></table></figure><br><img src="https://picx.zhimg.com/80/v2-71d052ed41187a762ba6f9619d0b3b8a.png" alt=""><p></p>
<p>我的配置是</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">socks5 172.31.xx.xx 7890</span><br></pre></td></tr></tbody></table></figure>
<h2 id="软件设置以及路径"><a href="#软件设置以及路径" class="headerlink" title="软件设置以及路径"></a>软件设置以及路径</h2><p>那么我们自己的设备上也需要打开代理软件才能让服务器访问到网络，在 clash 中打开 Allow LAN， v2ray 中打开 允许局域网的连接 就行了。这样我们的设备可以访问的东西，服务器都可以访问到。<br>以下是两种软件设置举例（注意：服务器端代理上网行为和代理软件设置一致）。</p>
<p>V2ray设置中开启允许局域网访问（需要注意本地端口和局域网端口不一样）</p>
<p><img src="https://pica.zhimg.com/v2-7c222d179d5454cef5d6a55f9a4f26b6.jpg" alt=""></p>
<p>clash开启局域网</p>
<p><img src="https://picx.zhimg.com/v2-d8dea34b8e6980fc40d3fb790733d70b.png" alt=""></p>
<p>此外，我们还需要在 bash 配置文件中加入二进制文件的路径，不然运行时会找不到文件（如果是通过管理员装的，则不用这一步),环境变量中添加刚刚的安装路径</p>
<figure class="highlight jsx"><table><tbody><tr><td class="code"><pre><span class="line">vim ~/.<span class="property">bashrc</span></span><br></pre></td></tr></tbody></table></figure>
<p><img src="https://picx.zhimg.com/80/v2-ea6a367e8a4642a7b49a498821427b79.png" alt=""></p>
<figure class="highlight jsx"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">export</span> <span class="variable constant_">PATH</span>=<span class="regexp">/data/</span>xxx/proxychains/<span class="attr">bin</span>:$PATH</span><br><span class="line"><span class="keyword">export</span> <span class="variable constant_">PROXYCHAINS_CONF_FILE</span>=<span class="regexp">/data/</span>xxx/proxychains/proxychains.<span class="property">conf</span></span><br></pre></td></tr></tbody></table></figure>
<p>重新开启终端后可正常使用 或者进行 source ~/.bashrc</p>
<h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><p>在想要代理网络的时候就在命令前加上 proxychains4 就可以了，例如：<br></p><figure class="highlight jsx"><table><tbody><tr><td class="code"><pre><span class="line">proxychains4 curl cip.<span class="property">cc</span></span><br><span class="line">proxychains4 python main.<span class="property">py</span></span><br></pre></td></tr></tbody></table></figure><br>这样在linux服务器上用本机电脑的代理网络，可以完成wget和git clone的代码等，还是很方便和舒服的<p></p>
<h2 id="troubleshoot"><a href="#troubleshoot" class="headerlink" title="troubleshoot"></a>troubleshoot</h2><p>在使用的时候报错找不到 proxychains.conf 的，基本上都是编译的时候没有填绝对路径而是填了相对路径，用 make uninstall 以及 make clean 把刚刚生成的东西给删了，然后重新运行上述的安装步骤，一定要填绝对路径。</p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux常用技巧及命令（不断更新）</title>
    <url>/2024/01/01/Linux/Linux%E5%B8%B8%E7%94%A8%E6%8A%80%E5%B7%A7%E5%8F%8A%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<h2 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h2><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">命令</th>
<th style="text-align:left">command</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">查看 GPU 使用状态</td>
<td style="text-align:left">nvidia-smi</td>
<td>gpustat -i (需 pip install gpustat)</td>
<td>nvitop（需pip install nvitop）</td>
</tr>
<tr>
<td style="text-align:left">查看进程</td>
<td style="text-align:left">top、htop、ps -ef</td>
<td>grep [pid]</td>
</tr>
<tr>
<td style="text-align:left">查看服务器磁盘容量</td>
<td style="text-align:left">df -h</td>
</tr>
<tr>
<td style="text-align:left">查看自己占用服务器的容量</td>
<td style="text-align:left">du -h</td>
</tr>
<tr>
<td style="text-align:left">查看当前目录下文件个数 (不包含子目录)</td>
<td style="text-align:left">ls -l</td>
<td>grep “^-“</td>
<td>wc -l</td>
</tr>
<tr>
<td style="text-align:left">查看端口占用 (Linux)</td>
<td style="text-align:left">lsof -i:PORT (没有空格)</td>
</tr>
<tr>
<td style="text-align:left">查看端口占用 (Windows)</td>
<td style="text-align:left">查看所有开放端口： netstat -ano 查看占用端口程序的 PID：netstat -aon</td>
<td>findstr “PORT” 查看占用端口的 PID 所对应的程序：tasklist</td>
<td>findstr “PID” 杀死占用端口的进程：taskkill /T /F /PID “PID”</td>
</tr>
</tbody>
</table>
</div>
<h2 id="Linux的Vim常用快捷键"><a href="#Linux的Vim常用快捷键" class="headerlink" title="Linux的Vim常用快捷键"></a>Linux的Vim常用快捷键</h2><p><a href="https://www.runoob.com/w3cnote/all-vim-cheatsheat.html">https://www.runoob.com/w3cnote/all-vim-cheatsheat.html</a></p>
<p><img src="https://picx.zhimg.com/v2-8ccbfe019bbbb1f2062d5bd6ff9d75f4.png" alt="Linux的Vim常用快捷键"></p>
<h2 id="解压中文文件"><a href="#解压中文文件" class="headerlink" title="解压中文文件"></a>解压中文文件</h2><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">unzip -O GBK 脑PET图像分析和疾病预测挑战赛公开数据.zip</span><br></pre></td></tr></tbody></table></figure>
<h2 id="查看进程使用者以及命令"><a href="#查看进程使用者以及命令" class="headerlink" title="查看进程使用者以及命令"></a>查看进程使用者以及命令</h2><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">ps -p &lt;PID&gt; -o user,cmd</span><br></pre></td></tr></tbody></table></figure>
<p>请注意，在某些情况下，特权用户可能无法查看其他用户的进程信息。在这种情况下，您需要以root用户或拥有适当权限的用户身份运行<code>ps</code>命令。</p>
<h2 id="查找历史记录"><a href="#查找历史记录" class="headerlink" title="查找历史记录"></a>查找历史记录</h2><p>查找历史记录中包含关键字的命令：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="built_in">history</span> | grep 关键字</span><br></pre></td></tr></tbody></table></figure>
<p>这个命令将显示包含指定关键字的历史记录命令。通过使用<code>history</code>命令查看最近使用的命令列表，并结合<code>grep</code>命令在文本中搜索指定模式（或关键字）。</p>
<h2 id="批量-kill-进程"><a href="#批量-kill-进程" class="headerlink" title="批量 kill 进程"></a>批量 kill 进程</h2><p>用 grep 配合 awk 可以轻易做到，<code>awk '{print $2}'</code> 表示输出第二列结果，在 ps 命令中就是进程的 id 号</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">ps -ef | grep xxx | grep -v grep | awk <span class="string">'{print $2}'</span> | xargs <span class="built_in">kill</span> -9</span><br></pre></td></tr></tbody></table></figure>
<p>如果要kill掉所有python的进程</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">pkill python</span><br></pre></td></tr></tbody></table></figure>
<h2 id="查看内存占用"><a href="#查看内存占用" class="headerlink" title="查看内存占用"></a>查看内存占用</h2><p>输入 <code>ps aux | sort -k4nr | head -n 5</code> 查看占用内存最多的前 5 个进程，或者也可以通过 <code>top</code> 命令后按住 <code>M</code> 来对内存占用进行排序，两个都可以。利用 <code>ps -aux</code> 或者 <code>top</code> 命令也可以查看到具体的占用多少 G 内存，举个例子，这是 <code>top</code> 命令的界面，<code>%MEM</code> 就是内存的占用量，对 <code>250508</code> 这个进程来分析一下，它的占用率是 2.1%，我们服务器内存大概是 504G，得出这个进程占用了大约 10.6G 的内存</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">   PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND</span><br><span class="line">250508 wubizhu   20   0 92.497g 0.010t  88056 S   0.0  2.1   0:32.10 python</span><br><span class="line">250593 wubizhu   20   0 92.498g 0.010t  87900 S   0.0  2.1   0:29.64 python</span><br><span class="line">225884 xjheng    20   0 49.620g 0.010t 6.367g S   4.8  2.0  57:06.42 python</span><br><span class="line"> 83813 xjheng    20   0 52.932g 6.760g 3.045g R 101.9  1.3 205:04.84 python</span><br><span class="line">252689 zxdong    20   0 14.935g 4.106g  83604 D   2.6  0.8  22:00.05 python</span><br><span class="line">252681 zxdong    20   0 14.935g 4.106g  83608 D  30.4  0.8  22:18.04 python</span><br><span class="line"> 13646 xjheng    20   0 17.928g 4.022g 1.127g D  22.7  0.8   1:29.93 python</span><br></pre></td></tr></tbody></table></figure>
<p>验证一下说法，通过 <code>ps -aux | grep 250508</code> 得到下面结果，第六列 <code>10944660</code> 就是占用的物理内存，单位是 k，所以这里统计出的是 10.9G，跟我们算出来的差不多</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">wubizhu  250508  1.8  2.0 96990572 10944660 pts/131 Sl+ 11:50   0:42 python xx.py</span><br></pre></td></tr></tbody></table></figure>
<p>通过 <code>cat /proc/250508/status</code> 也能得到进程的内存占用量，<code>VmRSS</code> 就是物理内存使用量，单位也是 k</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">Name:   python</span><br><span class="line">Umask:  0002</span><br><span class="line">State:  S (sleeping)</span><br><span class="line">Tgid:   250508</span><br><span class="line">Ngid:   250508</span><br><span class="line">Pid:    250508</span><br><span class="line">PPid:   38642</span><br><span class="line">TracerPid:      0</span><br><span class="line">Uid:    1074    1074    1074    1074</span><br><span class="line">Gid:    1074    1074    1074    1074</span><br><span class="line">FDSize: 128</span><br><span class="line">Groups: 1074</span><br><span class="line">NStgid: 250508</span><br><span class="line">NSpid:  250508</span><br><span class="line">NSpgid: 38642</span><br><span class="line">NSsid:  9950</span><br><span class="line">VmPeak: 97005580 kB</span><br><span class="line">VmSize: 96990572 kB</span><br><span class="line">VmLck:         0 kB</span><br><span class="line">VmPin:         0 kB</span><br><span class="line">VmHWM:  10961980 kB</span><br><span class="line">VmRSS:  10944656 kB</span><br></pre></td></tr></tbody></table></figure>
<h2 id="Linux中断运行以及恢复"><a href="#Linux中断运行以及恢复" class="headerlink" title="Linux中断运行以及恢复"></a>Linux中断运行以及恢复</h2><p>如果您不小心停止了一个正在运行的文件，一般终端可以使用<code>ctrl + Z</code>，可以尝试使用以下方法来继续它的运行：</p>
<p><strong>使用 fg 命令将进程移动到前台</strong></p>
<p>如果您在终端中运行的是一个正在运行的程序，可以使用 <code>fg</code> 命令将其移动到前台继续运行。首先，使用 <code>jobs</code> 命令查看当前终端会话中的作业列表，找到您想要恢复的作业的编号。然后，使用 <code>fg</code> 命令并带上作业编号，将进程移动到前台继续运行。例如，如果您想要恢复作业编号为 1 的进程，可以使用以下命令：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="built_in">jobs</span></span><br><span class="line"><span class="built_in">fg</span> %1</span><br></pre></td></tr></tbody></table></figure>
<p><strong>使用 nohup 命令将进程放入后台继续运行</strong></p>
<p>如果您希望进程在后台继续运行，可以使用 <code>nohup</code> 命令。该命令可以使进程在断开终端连接后继续运行，并将输出写入指定的文件。例如，如果您想要在后台运行名为 <code>myprogram</code> 的程序，并将输出写入 <code>myprogram.log</code> 文件，可以使用以下命令：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="built_in">nohup</span> myprogram &gt; myprogram.log &amp;</span><br></pre></td></tr></tbody></table></figure>
<p>其中，<code>&amp;</code> 符号表示将进程放入后台运行。</p>
<p>请注意，这些方法仅适用于在终端中运行的进程。如果您使用 GUI 界面启动的程序，可以尝试重新启动该程序或者使用系统监视器等工具来查看和终止进程。</p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux安全配置Fail2ban</title>
    <url>/2024/01/01/Linux/Linux%E5%AE%89%E5%85%A8%E9%85%8D%E7%BD%AEfail2ban/</url>
    <content><![CDATA[<h3 id="风险暴露"><a href="#风险暴露" class="headerlink" title="风险暴露"></a>风险暴露</h3><p>所有连接到互联网的服务器都面临恶意软件攻击的风险。连接到互联网的软件可能成为攻击者蛮力尝试的目标，试图访问应用程序。</p>
<h3 id="使用Fail2ban工具"><a href="#使用Fail2ban工具" class="headerlink" title="使用Fail2ban工具"></a>使用Fail2ban工具</h3><p>Fail2ban是一个开源工具，可帮助保护Linux免受暴力攻击和其他自动攻击。它通过监视服务日志中的恶意活动来工作，使用正则表达式扫描日志文件。</p>
<h4 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h4><ol>
<li>扫描日志文件，匹配特定模式的记录。</li>
<li>计数匹配记录，当数量达到预定义阈值时采取措施。</li>
<li>在指定时间段内禁止有问题的IP。</li>
<li>默认使用系统防火墙阻止被禁止IP的访问。</li>
<li>禁止期限到期后，IP地址将从禁止列表中删除。</li>
</ol>
<h3 id="安装和配置Fail2ban"><a href="#安装和配置Fail2ban" class="headerlink" title="安装和配置Fail2ban"></a>安装和配置Fail2ban</h3><p>如果需要更新和升级服务器，执行以下命令：<br></p><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get upgrade</span><br><span class="line">sudo apt-get install -y fail2ban</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>在root状态下，编辑Fail2ban的配置文件：<br></p><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">vim /etc/fail2ban/jail.local</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>配置文件内容如下（注意修改ssh端口和sshd日志路径）：<br></p><figure class="highlight properties"><table><tbody><tr><td class="code"><pre><span class="line"><span class="attr">[sshd]</span></span><br><span class="line"><span class="attr">port</span> = <span class="string">2223</span></span><br><span class="line"><span class="attr">enabled</span> = <span class="string">true</span></span><br><span class="line"><span class="attr">filter</span> = <span class="string">sshd</span></span><br><span class="line"><span class="attr">logpath</span> = <span class="string">/var/log/auth.log</span></span><br><span class="line"><span class="attr">maxretry</span> = <span class="string">6</span></span><br><span class="line"><span class="attr">findtime</span> = <span class="string">60</span></span><br><span class="line"><span class="attr">bantime</span> = <span class="string">86400</span></span><br><span class="line"></span><br><span class="line"><span class="attr">[ssh-iptables]</span></span><br><span class="line"><span class="attr">enabled</span> = <span class="string">true</span></span><br><span class="line"><span class="attr">filter</span> = <span class="string">sshd</span></span><br><span class="line"><span class="attr">action</span> = <span class="string">iptables[name=SSH, port=ssh, protocol=tcp]</span></span><br></pre></td></tr></tbody></table></figure><p></p>
<p>重启Fail2ban服务：<br></p><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">sudo service fail2ban restart</span><br></pre></td></tr></tbody></table></figure><p></p>
<h3 id="使用Fail2ban"><a href="#使用Fail2ban" class="headerlink" title="使用Fail2ban"></a>使用Fail2ban</h3><p>检查被封禁的IP：<br></p><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">fail2ban-client status sshd</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>取消被禁止的IP地址：<br></p><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">sudo fail2ban-client <span class="built_in">set</span> sshd unbanip IP_ADDRESS</span><br></pre></td></tr></tbody></table></figure><br>（其中，<code>IP_ADDRESS</code>是被禁止的IP地址）<p></p>
<h3 id="启用ufw防火墙"><a href="#启用ufw防火墙" class="headerlink" title="启用ufw防火墙"></a>启用ufw防火墙</h3><ol>
<li><p>安装ufw：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">sudo apt-get install ufw</span><br></pre></td></tr></tbody></table></figure>
</li>
<li><p>启用ufw：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">sudo ufw <span class="built_in">enable</span></span><br><span class="line">sudo ufw default allow</span><br><span class="line">sudo ufw allow ssh</span><br><span class="line">sudo ufw allow 2223</span><br></pre></td></tr></tbody></table></figure>
<p>以上是简单的保护措施，但记得随时保持系统和软件更新以确保安全性。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux配置 Rsyslog 服务器</title>
    <url>/2024/01/01/Linux/Linux%E9%85%8D%E7%BD%AE%20Rsyslog%20%E6%9C%8D%E5%8A%A1%E5%99%A8/</url>
    <content><![CDATA[<p><a href="https://bynss.com/linux/770633.html">如何在 Ubuntu 上设置 Rsyslog 服务器 – Digitalixy.com</a></p>
<h1 id="日志文件和Rsyslog简介"><a href="#日志文件和Rsyslog简介" class="headerlink" title="日志文件和Rsyslog简介"></a>日志文件和Rsyslog简介</h1><p>日志文件是用于保存系统活动信息的文件，包括授权和访问尝试、启动和关闭尝试，以及服务的启动和关闭。不同类型的活动有不同的日志文件。Rsyslog是一款开源程序，用于配置Linux操作系统的日志服务器和客户端。<br>在这一部分，将在 Ubuntu 操作系统上设置 Rsyslog 服务器。 我们将使用两台 Ubuntu 机器。<br>在一台 Ubuntu 机器上，我们将 Rsyslog 配置为日志服务器，<br>在另一台机器上； 我们将 Rsyslog 配置为将日志发送到 Rsyslog 服务器的客户端。</p>
<h1 id="在Ubuntu上配置Rsyslog服务器"><a href="#在Ubuntu上配置Rsyslog服务器" class="headerlink" title="在Ubuntu上配置Rsyslog服务器"></a>在Ubuntu上配置Rsyslog服务器</h1><p>在这一部分，我们将在两台Ubuntu机器上设置Rsyslog服务器。一台机器将被配置为Rsyslog服务器，另一台机器将被配置为将日志发送到Rsyslog服务器的客户端。</p>
<h2 id="安装Rsyslog"><a href="#安装Rsyslog" class="headerlink" title="安装Rsyslog"></a>安装Rsyslog</h2><p>如果Rsyslog未安装，可以通过以下命令进行安装：<br></p><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">sudo apt install rsyslog</span><br></pre></td></tr></tbody></table></figure><br>在安装过程中，它会提示您 是/否 用于继续安装 Rsyslog 的选项。 按 是 接着Enter 继续。<br>验证Rsyslog安装并检查其服务状态：<br><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">sudo systemctl status rsyslog</span><br></pre></td></tr></tbody></table></figure><p></p>
<h2 id="配置Rsyslog服务器"><a href="#配置Rsyslog服务器" class="headerlink" title="配置Rsyslog服务器"></a>配置Rsyslog服务器</h2><p>编辑Rsyslog配置文件<code>/etc/rsyslog.conf</code>：<br></p><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">sudo nano /etc/rsyslog.conf</span><br></pre></td></tr></tbody></table></figure><br>在配置文件中添加以下行，用于接收通过UDP和TCP发送的syslog：<br><figure class="highlight properties"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># Receive syslog over UDP</span></span><br><span class="line"><span class="attr">module(load</span>=<span class="string">"imudp") </span></span><br><span class="line"><span class="attr">input(type</span>=<span class="string">"imudp" port="514")</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># Receive syslog over TCP</span></span><br><span class="line"><span class="attr">module(load</span>=<span class="string">"imtcp") </span></span><br><span class="line"><span class="attr">input(type</span>=<span class="string">"imtcp" port="514")</span></span><br></pre></td></tr></tbody></table></figure><br>创建一个模板，用于存储传入的syslog消息：<br><figure class="highlight properties"><table><tbody><tr><td class="code"><pre><span class="line"><span class="attr">$template</span> <span class="string">remote-incoming-logs, "/var/log/%HOSTNAME%/%PROGRAMNAME%.log"</span></span><br><span class="line"><span class="attr">*.*</span> <span class="string">?remote-incoming-logs</span></span><br></pre></td></tr></tbody></table></figure><p></p>
<p><img src="https://picx.zhimg.com/v2-6852e00c33d7a3653c21a0274f95c9d2.png" alt=""><br>保存并关闭配置文件，然后重启Rsyslog服务：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">sudo systemctl restart rsyslog</span><br></pre></td></tr></tbody></table></figure>
<p>您还可以使用以下命令验证 Rsyslog 是否正在侦听 TCP/UDP 端口 514：<br></p><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">$ sudo ss -tunlp | grep 514</span><br></pre></td></tr></tbody></table></figure><br>应该收到以下输出：<p></p>
<p><img src="https://picx.zhimg.com/v2-96f64402412999a61ab770c8e4060404.png" alt=""></p>
<h2 id="配置防火墙"><a href="#配置防火墙" class="headerlink" title="配置防火墙"></a>配置防火墙</h2><p>如果系统启用了防火墙，打开TCP和UDP端口514，Rsyslog服务器使用这些端口接收来自远程客户端的日志：<br></p><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">sudo ufw allow 514/tcp</span><br><span class="line">sudo ufw allow 514/udp</span><br><span class="line">sudo ufw reload <span class="comment"># 重新加载防火墙</span></span><br></pre></td></tr></tbody></table></figure><p></p>
<h1 id="在-Ubuntu-上配置-Rsyslog-客户端"><a href="#在-Ubuntu-上配置-Rsyslog-客户端" class="headerlink" title="在 Ubuntu 上配置 Rsyslog 客户端"></a>在 Ubuntu 上配置 Rsyslog 客户端</h1><p>现在在另一个 Ubuntu 系统上，我们将执行 Rsyslog 客户端的配置。 然后，此客户端会将其日志发送到 Rsyslog 日志记录服务器。</p>
<h2 id="安装Rsyslog-1"><a href="#安装Rsyslog-1" class="headerlink" title="安装Rsyslog"></a>安装Rsyslog</h2><p>如果尚未安装Rsyslog，执行以下命令：<br></p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">sudo apt install rsyslog</span><br></pre></td></tr></tbody></table></figure><p></p>
<h2 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h2><p>编辑Rsyslog配置文件：<br></p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">sudo vim /etc/rsyslog.conf</span><br></pre></td></tr></tbody></table></figure><br><strong>具体细节看参考图：</strong><br><img src="https://picx.zhimg.com/v2-504b8c11750552c11c53d2233980ef28.png" alt=""><br><img src="https://picx.zhimg.com/v2-20c70f079d16d5dd288004534c7048fe.png" alt=""><br>在 Rsyslog 配置文件的末尾添加以下行。 确保更换 192.168.72.204 与您的 Rsyslog 日志记录服务器的 IP 地址。(<strong>这里要注意ip地址，如172.31.224.190</strong>)<p></p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">，<span class="comment">#Send system logs to rsyslog server over RDP</span></span><br><span class="line"></span><br><span class="line">*.* @192.168.72.204:514 <span class="comment"># *.* @服务器ip地址:514</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Send system logs to rsyslog server over TCP</span></span><br><span class="line"></span><br><span class="line">*.* @@192.168.72.204:514 <span class="comment"># *.* @@服务器ip地址:514 </span></span><br><span class="line"></span><br><span class="line"><span class="comment">##Set disk queue to preserve your logs in case rsyslog server is experiencing any downtime</span></span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="variable">$ActionQueueFileName</span> queue</span><br><span class="line"></span><br><span class="line"><span class="variable">$ActionQueueMaxDiskSpace</span> 1g</span><br><span class="line"></span><br><span class="line"><span class="variable">$ActionQueueSaveOnShutdown</span> on</span><br><span class="line"></span><br><span class="line"><span class="variable">$ActionQueueType</span> LinkedList</span><br><span class="line"></span><br><span class="line"><span class="variable">$ActionResumeRetryCount</span> -1</span><br></pre></td></tr></tbody></table></figure>
<h2 id="修改配置文件-1"><a href="#修改配置文件-1" class="headerlink" title="修改配置文件"></a>修改配置文件</h2><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">vim /etc/rsyslog.d/50-default.conf</span><br></pre></td></tr></tbody></table></figure>
<p><img src="https://pic1.zhimg.com/v2-9b7bdc9a3e08ad468aef863ce72ad7eb.png" alt=""></p>
<h2 id="添加环境变量"><a href="#添加环境变量" class="headerlink" title="添加环境变量"></a>添加环境变量</h2><p>在vim /etc/profile最后添加<br></p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">export PROMPT_COMMAND="history -a"</span><br><span class="line">export HISTTIMEFORMAT="`whoami` %F %T "</span><br><span class="line">shopt -s histappend</span><br><span class="line">export PROMPT_COMMAND='{ msg=$(history 1 | { read x y; echo $y; });logger "[euid=$(whoami)]":$(who am i):[`pwd`]"$msg"; }'</span><br></pre></td></tr></tbody></table></figure><br><img src="https://picx.zhimg.com/v2-6b59d9c8ecdd94a5ef00ee35ed7111e1.png" alt=""><p></p>
<h2 id="重启服务"><a href="#重启服务" class="headerlink" title="重启服务"></a>重启服务</h2><p>现在运行以下命令重启 Rsyslog 的服务：<br></p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">/etc/init.d/rsyslog restart</span><br></pre></td></tr></tbody></table></figure><br><img src="https://pic1.zhimg.com/v2-c1deae4ddcadce6e60f62c011f98248d.png" alt=""><p></p>
<h1 id="在-Rsyslog-Server-中查看客户端的日志文件"><a href="#在-Rsyslog-Server-中查看客户端的日志文件" class="headerlink" title="在 Rsyslog Server 中查看客户端的日志文件"></a>在 Rsyslog Server 中查看客户端的日志文件</h1><p>完成上述所有配置后，您可以查看客户端发送到 Rsyslog 服务器的日志文件。 在您的 Rsyslog 服务器机器上，在终端中运行以下命令：<br></p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">$ ls /var/log/ # 具体看文件的命名</span><br></pre></td></tr></tbody></table></figure><br>在上述命令的输出中，您将看到一个与您的客户端系统主机名相同的目录（我们的 ubuntu2 example）。<br><img src="https://picx.zhimg.com/v2-d524b8a87c12665b38e574fb511d5229.png" alt=""><br>要查看客户端机器的日志文件，请列出该目录的内容：<p></p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">$ sudo ls /var/log/ubuntu2</span><br></pre></td></tr></tbody></table></figure>
<p><img src="https://pic1.zhimg.com/v2-209842178d0de9e02cff85c548270c8b.png" alt=""></p>
<p>这就是它的全部！ 在本指南中，我们介绍了如何在 Ubuntu OS 上将 Rsyslog 配置为日志服务器和客户端以将日志发送到 Rsyslog 服务器。 我们还介绍了如何查看客户端发送到日志服务器的日志。</p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux服务器之间文件传输</title>
    <url>/2024/01/01/Linux/Linux%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%97%B4%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93/</url>
    <content><![CDATA[<h2 id="SCP命令"><a href="#SCP命令" class="headerlink" title="SCP命令"></a>SCP命令</h2><p>正常情况下使用SCP命令<br></p><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 将服务器文件传输到本地</span></span><br><span class="line">scp -r user@ip:服务器文件夹 本地文件夹</span><br></pre></td></tr></tbody></table></figure><br><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 将本地文件传输到服务器</span></span><br><span class="line">scp -r 本地文件夹 user@ip:服务器文件夹</span><br></pre></td></tr></tbody></table></figure><p></p>
<h2 id="使用密钥"><a href="#使用密钥" class="headerlink" title="使用密钥"></a>使用密钥</h2><p>使用密钥时，需要添加一个-i的参数，并输入对应密钥的密码即可<br></p><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">scp -i 对应密钥地址 -r user@ip 本地文件夹</span><br></pre></td></tr></tbody></table></figure><br>如果出现Permissions 0644错误，则运行chmod 400修改一下权限即可<br><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">chmod 400 密钥地址</span><br></pre></td></tr></tbody></table></figure><p></p>
<h2 id="增加端口port"><a href="#增加端口port" class="headerlink" title="增加端口port"></a>增加端口port</h2><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">scp -r -i 对应密钥地址 -P 端口号 user@ip:服务器文件夹 本地文件夹</span><br></pre></td></tr></tbody></table></figure>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>SSHFS-WIN使用连接Linux服务器</title>
    <url>/2024/01/01/Linux/SSHFS-WIN%E4%BD%BF%E7%94%A8%E8%BF%9E%E6%8E%A5Linux%E6%9C%8D%E5%8A%A1%E5%99%A8/</url>
    <content><![CDATA[<p><a href="https://github.com/evsar3/sshfs-win-manager">https://github.com/evsar3/sshfs-win-manager</a></p>
<h2 id="Installation"><a href="#Installation" class="headerlink" title="Installation"></a><strong>Installation</strong></h2><p><strong>Step 1</strong><br>Install SSHFS-Win on your Windows computer.<br>Follow they <a href="https://github.com/winfsp/sshfs-win/blob/master/README.md">installation instructions</a> before continue.</p>
<blockquote>
<ul>
<li>Install the latest version of <a href="https://github.com/billziss-gh/winfsp/releases/latest">WinFsp</a>.</li>
<li>Install the latest version of <a href="https://github.com/billziss-gh/sshfs-win/releases">SSHFS-Win</a>. Choose the x64 or x86 installer according to your computer’s architecture.（一般win电脑是x64）</li>
</ul>
</blockquote>
<p>在您的Windows计算机上安装SSHFS-Win。 在继续之前，请按照以下安装说明进行操作： </p>
<ul>
<li>安装最新版本的WinFsp。 </li>
<li>安装最新版本的SSHFS-Win。根据您计算机的架构选择x64或x86安装程序。（通常情况下，Windows电脑使用x64架构。）</li>
</ul>
<p><strong>Step 2</strong><br>Once SSHFS-Win is installed, <a href="https://github.com/evsar3/sshfs-win-manager/releases/latest">download the lastest setup</a> from the <a href="https://github.com/evsar3/sshfs-win-manager/releases">releases</a> section and install it.<br>安装桌面版本的SSHFS-win，用最新版本<br><strong>Step 3</strong><br>Add your connections and enjoy!</p>
<h2 id="Screenshots"><a href="#Screenshots" class="headerlink" title="Screenshots"></a>Screenshots</h2><p><img src="https://picx.zhimg.com/v2-73f85dfba21f7760230a65ab574d8551.png" alt="Main Window"></p>
<p><img src="https://pica.zhimg.com/v2-81b072937c987de94039a2c6ef00b0d2.png" alt="Add &amp; edit connections"></p>
<p><img src="https://picx.zhimg.com/v2-b5f0fb802dadff86258fad695d6ba5b9.png" alt="Explore mounted drive"></p>
<p><img src="https://picx.zhimg.com/v2-ea10b8327376cab8e80ca120ab32ac24.png" alt="Close to system tray"></p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>SSH连接服务器中断，代码继续运行</title>
    <url>/2024/01/01/Linux/SSH%E8%BF%9E%E6%8E%A5%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%AD%E6%96%AD%EF%BC%8C%E4%BB%A3%E7%A0%81%E7%BB%A7%E7%BB%AD%E8%BF%90%E8%A1%8C/</url>
    <content><![CDATA[<h2 id="1、ssh连接linux服务器中断后，如何让命令继续在服务器运行"><a href="#1、ssh连接linux服务器中断后，如何让命令继续在服务器运行" class="headerlink" title="1、ssh连接linux服务器中断后，如何让命令继续在服务器运行"></a>1、ssh连接linux服务器中断后，如何让命令继续在服务器运行</h2><p>这个问题也许是我们这些小白比较头疼的问题，尤其对于那些做机器学习需要花很久的时间才能训练出一个结果。然而就在这时，因为各种不可抗力我们使用ssh连接服务器时，ssh的窗口突然断开了连接，那么在服务器上跑的程序就也跟着断掉了，之前所有跑的数据也将丢失，这样将会浪费我们大量的时间。</p>
<p>今天刚好有人跟我提到了这个问题，然后就简单上网查找资料，简单的学习一下，做个笔记方便自己以后查阅。</p>
<p>参考链接：</p>
<p><a href="http://blog.csdn.net/gukesdo/article/details/6901902">http://blog.csdn.net/gukesdo/article/details/6901902</a></p>
<h2 id="2、为什么ssh一旦断开我们的进程也将会被杀掉？"><a href="#2、为什么ssh一旦断开我们的进程也将会被杀掉？" class="headerlink" title="2、为什么ssh一旦断开我们的进程也将会被杀掉？"></a>2、为什么ssh一旦断开我们的进程也将会被杀掉？</h2><p><strong>元凶：SIGHUP 信号</strong></p>
<p>让我们来看看为什么关掉窗口/断开连接会使得正在运行的程序死掉。</p>
<p>在Linux/Unix中，有这样几个概念：</p>
<p><strong>进程组（process group）：一个或多个进程的集合，每一个进程组有唯一一个进程组ID，即进程组长进程的ID。</strong></p>
<p><strong>会话期（session）：一个或多个进程组的集合，有唯一一个会话期首进程（session leader）。会话期ID为首进程的ID。</strong></p>
<p><strong>会话期可以有一个单独的控制终端（controlling terminal）。与控制终端连接的会话期首进程叫做控制进程（controlling process）。当前与终端交互的进程称为前台进程组。其余进程组称为后台进程组。</strong></p>
<p>根据POSIX.1定义：</p>
<p>挂断信号（SIGHUP）默认的动作是终止程序。</p>
<p>当终端接口检测到网络连接断开，将挂断信号发送给控制进程（会话期首进程）。</p>
<p>如果会话期首进程终止，则该信号发送到该会话期前台进程组。</p>
<p>一个进程退出导致一个孤儿进程组中产生时，如果任意一个孤儿进程组进程处于STOP状态，发送SIGHUP和SIGCONT信号到该进程组中所有进程。</p>
<p>因此当网络断开或终端窗口关闭后，控制进程收到SIGHUP信号退出，会导致该会话期内其他进程退出。</p>
<p><strong>这里我认为我们的进程被杀掉也就是因为ssh与服务器之间的通信断掉了，这个通信断掉之后linux程序就默认将该连接下的所有进程都杀掉</strong></p>
<h2 id="3、解决方案"><a href="#3、解决方案" class="headerlink" title="3、解决方案"></a>3、解决方案</h2><p>针对上面的问题，上面的参考链接中也有讲解，在此进行一个简单的整理。</p>
<p>这里主要有三个方案，一个是使用nohup指令，一个是使用screen指令，最后一个是screen的升级版byobu。看完这三个指令之后其实<strong>我更倾向于使用byobu指令，因为byobu指令更加的强大,是screen的升级版本，并且界面也比较友好。</strong></p>
<h3 id="nohup命令"><a href="#nohup命令" class="headerlink" title="nohup命令"></a>nohup命令</h3><p>nohup 命令</p>
<p>用途：不挂断地运行命令。</p>
<p>语法：nohup Command [ Arg … ] [　&amp; ]</p>
<p><strong>描述：nohup 命令运行由 Command 参数和任何相关的 Arg 参数指定的命令，忽略所有挂断(SIGHUP)信号。在注销后使用 nohup 命令运行后台中的程序。要运行后台中的 nohup 命令，添加 &amp; ( 表示”and”的符号)到命令的尾部。</strong></p>
<p>无论是否将 nohup 命令的输出重定向到终端，输出都将附加到当前目录的 nohup.out 文件中。如果当前目录的 nohup.out 文件不可写，输出重定向到 $HOME/nohup.out 文件中。如果没有文件能创建或打开以用于追加，那么 Command 参数指定的命令不可调用。如果标准错误是一个终端，那么把指定的命令写给标准错误的所有输出作为标准输出重定向到相同的文件描述符。</p>
<h3 id="nohup的简单使用"><a href="#nohup的简单使用" class="headerlink" title="nohup的简单使用"></a>nohup的简单使用</h3><p>在执行命令时在命令前面加上nohup，然后回车开始运行。</p>
<p><strong>这时你会发现该有的输出其实并没有输出出来，这个时候不要方，这是因为nohup命令将你的所有输出都输出到了当前文件夹下的nohup.out文件中，自己可以使用vim指令进行一个查看。</strong></p>
<p>nohup命令及其输出文件 　　</p>
<p>nohup命令：如果你正在运行一个进程，而且你觉得在退出帐户时该进程还不会结束，那么可以使用nohup命令。该命令可以在你退出帐户/关闭终端之后继续运行相应的进程。nohup就是不挂起的意思( n ohang up)。 　　</p>
<p>该命令的一般形式为：nohup command &amp; 　　</p>
<p>使用nohup命令提交作业 　　</p>
<p>如果使用nohup命令提交作业，那么在缺省情况下该作业的所有输出都被重定向到一个名为nohup.out的文件中，除非另外指定了输出文件： 　　</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="built_in">nohup</span> <span class="built_in">command</span> &gt; myout.file 2&gt;&amp;1 &amp;</span><br></pre></td></tr></tbody></table></figure>
<p><strong>使用 jobs 查看任务。</strong></p>
<p><strong>使用 fg %n　关闭。</strong></p>
<h3 id="screen命令"><a href="#screen命令" class="headerlink" title="screen命令"></a>screen命令</h3><p>简单来说，Screen是一个可以在多个进程之间多路复用一个物理终端的窗口管理器。Screen中有会话的概念，用户可以在一个screen会话中创建多个screen窗口，在每一个screen窗口中就像操作一个真实的telnet/SSH连接窗口那样。在screen中创建一个新的窗口有这样几种方式：</p>
<p>1．直接在命令行键入screen命令</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">screen</span><br></pre></td></tr></tbody></table></figure>
<p>Screen将创建一个执行shell的全屏窗口。你可以执行任意shell程序，就像在ssh窗口中那样。在该窗口中键入exit退出该窗口，如果这是该screen会话的唯一窗口，该screen会话退出，否则screen自动切换到前一个窗口。</p>
<p>2．Screen命令后跟你要执行的程序。</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">screen 【后面跟你执行程序的命令】</span><br></pre></td></tr></tbody></table></figure>
<p>Screen创建一个执行<code>vi test.c</code>的单窗口会话，退出vi将退出该窗口/会话。</p>
<p>3．以上两种方式都创建新的screen会话。我们还可以在一个已有screen会话中创建新的窗口。在当前screen窗口中键入C-a c ，即Ctrl键+a键，之后再按下c键，screen 在该会话内生成一个新的窗口并切换到该窗口。</p>
<p>screen还有更高级的功能。你可以不中断screen窗口中程序的运行而暂时断开（detach）screen会话，并在随后时间重新连接（attach）该会话，重新控制各窗口中运行的程序。</p>
<h3 id="screen的简单使用"><a href="#screen的简单使用" class="headerlink" title="screen的简单使用"></a>screen的简单使用</h3><p>在所要执行的指令前添加screen.然后程序的运行等一切正常。（nohup的输出是定向到了nohup.out文件中，然而screen指令的输出是直接输出到了屏幕上的）</p>
<p>这个时候如果ssh终端断开了连接。我们只需要再次连接服务器然后输入指令</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">screen -<span class="built_in">ls</span></span><br></pre></td></tr></tbody></table></figure>
<p>然后会得到类似下面的结果：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">There is a screen on:</span><br><span class="line">    27267.pts-19.TITAN-X    (09/08/2017 03:49:10 PM)    (Detached)</span><br><span class="line">1 Socket <span class="keyword">in</span> /var/run/screen/S-huanghailiang.</span><br></pre></td></tr></tbody></table></figure>
<p>这里就会显示ssh断开之前的程序，<strong><em>其实断开后程序依然在后台在运行</em></strong>，只是我们这个时候需要将它放到前台来运行。这个时候我们们已经通过screen -ls查询到了线程号是27267了，所以我们只需要执行下面的指令即可恢复到前台了。</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">screen -r 27267</span><br></pre></td></tr></tbody></table></figure>
<p>如果想杀掉终端可以执行</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="built_in">kill</span> 27267</span><br></pre></td></tr></tbody></table></figure>
<p>其他更多的指令可以通过screen —help来进行学习。</p>
<p>当然screen还有更多的快捷键值得我们学习。我们可以通过C-a ? 即先按ctrl+a以后再按？即可查看。</p>
<p><strong>通过观察我们可以发现它其实恢复了我们ssh断开前的那个界面。（所有的输出也都会在此显示出来）</strong></p>
<h3 id="byobu命令"><a href="#byobu命令" class="headerlink" title="byobu命令"></a>byobu命令</h3><p>byobu感觉就是screen的一个升级版本，界面比较友好，操作也比较方便。 一般Ubuntu系统开始的时候默认没有安装，我们需要手动安装byobu:</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">sudo apt install byobu</span><br></pre></td></tr></tbody></table></figure>
<h3 id="byobu的基本简单操作"><a href="#byobu的基本简单操作" class="headerlink" title="byobu的基本简单操作"></a>byobu的基本简单操作</h3><div class="table-container">
<table>
<thead>
<tr>
<th>按键</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>F2</td>
<td>新建窗口</td>
</tr>
<tr>
<td>F3</td>
<td>移动到前一个窗口</td>
</tr>
<tr>
<td>F4</td>
<td>移动到后一个窗口</td>
</tr>
<tr>
<td>F6</td>
<td>退出byobu窗口</td>
</tr>
<tr>
<td>F9</td>
<td>打开byobu菜单，查看帮助信息的配置信息</td>
</tr>
</tbody>
</table>
</div>
<p>关闭当前窗口其实Ctrl+D就可以完成</p>
<p>剩余操作我们可以F9查看byobu的帮助即可。</p>
<p>如果我们想要一登陆就显示byobu界面的话，可以使用指令</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">byobu-enable</span><br></pre></td></tr></tbody></table></figure>
<p>如果想取消一登陆就显示byobu界面可以是用指令</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">byobu-disable</span><br></pre></td></tr></tbody></table></figure>
<h2 id="4、注（个人理解）："><a href="#4、注（个人理解）：" class="headerlink" title="4、注（个人理解）："></a>4、注（个人理解）：</h2><p><strong>nohup</strong>虽然可以把所有的输出都写入到nohup.out中间来，但是在面对需要人机交互的时候它就不能正常使用了。另外在我做测试的时候，一旦ssh断开，虽然程序还在后台运行，但是好像并不会再把输出写入到nohup.out文件中，这一点很不好。</p>
<p><strong>screen</strong>相比之下就强大了很多，不仅可以满足人机交互，而且还可以将所有的输出都再次展示出来，方便我们查看。我对screen的理解是，其实它是一个虚拟的终端，我们运行的时候screen指令就为我们创建了一个虚拟的终端，所以我们再次连接后我们打开的还是这个虚拟的终端，所以我们可以更好的进行操作，而且screen支持开很多个终端。</p>
<p><strong>byobu</strong>相比之下继承了screen的所有优点，并且拥有了更加方便快捷的操作界面，同时在界面下方还能很好的显示目前计算机的硬件使用情况，十分方便。</p>
<p>它们三者的共性我认为都已经不受SIGHUP信号的影响了，所以即使断掉了ssh程序依旧会运行。</p>
<p>还有一个tmux，推荐tmux，我也已经写了教程</p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>如何在Linux服务器上安装Anaconda（超详细）</title>
    <url>/2024/01/01/Linux/Linux%E8%A3%85Anconda%E5%8F%8Apytorch/</url>
    <content><![CDATA[<h2 id="安装Anaconda"><a href="#安装Anaconda" class="headerlink" title="安装Anaconda"></a>安装Anaconda</h2><h3 id="1-1-下载anaconda的安装包"><a href="#1-1-下载anaconda的安装包" class="headerlink" title="1.1 下载anaconda的安装包"></a>1.1 下载anaconda的安装包</h3><p>这里我们需要在官网上查找自己需要的版本，地址链接在下面:<br><a href="https://repo.anaconda.com/archive/">https://repo.anaconda.com/archive/</a><br>这里以我自己安装的版本为例：<br><a href="https://repo.anaconda.com/archive/Anaconda3-2022.10-Linux-x86_64.sh">https://repo.anaconda.com/archive/Anaconda3-2022.10-Linux-x86_64.sh</a><br>这是我选择的版本，然后我们在控制台输入这句话:<br></p><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">wget https://repo.anaconda.com/archive/Anaconda3-2022.10-Linux-x86_64.sh</span><br></pre></td></tr></tbody></table></figure><br>如果没有出现问题就是下面图示：<br><img src="https://pic1.zhimg.com/v2-d676de98829f1145c75f69bc9896c15e.png" alt=""><br>如果出现问题就按照 1.2 步骤操作。<p></p>
<h3 id="1-2-解决安装出现的bug"><a href="#1-2-解决安装出现的bug" class="headerlink" title="1.2 解决安装出现的bug"></a>1.2 解决安装出现的bug</h3><p>当我们输入1.1的那一条命令时，有些人可能会出现下面这样的错误:<br></p><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">bash: wget: <span class="built_in">command</span> not found</span><br></pre></td></tr></tbody></table></figure><br>当然这也是我自己出现的错误，具体解决办法如下: Debian/Ubuntu系统，需要执行以下命令：<br><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">apt-get install -y wget</span><br></pre></td></tr></tbody></table></figure><br>相反，CentOS系统则需要输入下面指令:<br><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">yum install wget -y</span><br></pre></td></tr></tbody></table></figure><p></p>
<h3 id="1-3-安装anaconda"><a href="#1-3-安装anaconda" class="headerlink" title="1.3 安装anaconda"></a>1.3 安装anaconda</h3><p>接下来我们需要首先赋权再执行安装程序，依次输入下面两句命令:<br></p><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="built_in">chmod</span> +x Anaconda3-5.3.0-Linux-x86_64.sh</span><br><span class="line">./Anaconda3-5.3.0-Linux-x86_64.sh</span><br></pre></td></tr></tbody></table></figure><br>然后出现下面图所示:<br><img src="https://picx.zhimg.com/v2-450e81bb2de67a3c137bf068af871d6b.png" alt=""><p></p>
<h3 id="1-4-点击Enter（回车键）"><a href="#1-4-点击Enter（回车键）" class="headerlink" title="1.4 点击Enter（回车键）"></a>1.4 点击Enter（回车键）</h3><p>此时显示Anaconda的信息，并且会出现More，继续按Enter，直到如下图所示:<br><img src="https://picx.zhimg.com/v2-558fb8bd56454e22cd5853e97d5d58ef.png" alt=""></p>
<h3 id="1-5-输入-yes"><a href="#1-5-输入-yes" class="headerlink" title="1.5 输入 yes"></a>1.5 输入 yes</h3><p><img src="https://picx.zhimg.com/v2-6957bbec41567ae3cf222d00cefa9bca.png" alt=""></p>
<h3 id="1-6-继续点击-Enter"><a href="#1-6-继续点击-Enter" class="headerlink" title="1.6 继续点击 Enter"></a>1.6 继续点击 Enter</h3><p><img src="https://pic1.zhimg.com/v2-9cacfb8685e9f1c10036d80edffe6fdd.png" alt=""></p>
<h3 id="1-7-输入-yes，添加环境变量"><a href="#1-7-输入-yes，添加环境变量" class="headerlink" title="1.7 输入 yes，添加环境变量"></a>1.7 输入 yes，添加环境变量</h3><p>这里需要注意点的就是如果你直接跳过这部设置环境变量的话：</p>
<p><img src="https://pic1.zhimg.com/v2-f0f80af2b03a73a6a81e82149405e38c.png" alt=""></p>
<p>这里需要注意点的就是如果你直接跳过这部设置环境变量的话：<br>[no ] &gt;&gt;&gt;<br>那你需要自己到这个文件夹设置你安装Anaconda路径（比如上面显示我的是）<br>/home/wangke/.bashrc<br>单击进去，在最后一行添加：<br>export PATH=/home/wangke/anaconda3/bin:$PATH<br>需要把之前的那句话给注释掉如下所示：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># export PATH=/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/sbin:/sbin:$PATHexport PATH=/root/anaconda3/bin:$PATH</span></span><br></pre></td></tr></tbody></table></figure>
<p>这里只是个示例，具体的还是要看你们自己安装的路径，这个就是相当于windows的环境变量<br>然后保存更改，输入下面这句指令：<br></p><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="built_in">source</span> ~/.bashrc</span><br></pre></td></tr></tbody></table></figure><p></p>
<h3 id="1-8-完成安装以及检测是否安装成功"><a href="#1-8-完成安装以及检测是否安装成功" class="headerlink" title="1.8 完成安装以及检测是否安装成功"></a>1.8 完成安装以及检测是否安装成功</h3><p>打开新的终端后，进入自己的文件夹目录下，输入anaconda -V（注意a要小写，V要大写），conda -V ,显示版本信息，若显示则表示安装成功。<br></p><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">root@dev-wyf-react:~/wyf<span class="comment"># conda -V</span></span><br><span class="line">conda 4.5.11</span><br></pre></td></tr></tbody></table></figure><p></p>
<h2 id="Anaconda安装Pytorch"><a href="#Anaconda安装Pytorch" class="headerlink" title="Anaconda安装Pytorch"></a>Anaconda安装Pytorch</h2><h3 id="2-1-创建虚拟环境"><a href="#2-1-创建虚拟环境" class="headerlink" title="2.1 创建虚拟环境"></a>2.1 创建虚拟环境</h3><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">conda create -n pytorch python=3.7 （pytorch 是我自己取的名字）</span><br></pre></td></tr></tbody></table></figure>
<h3 id="2-2-激活环境"><a href="#2-2-激活环境" class="headerlink" title="2.2 激活环境"></a>2.2 激活环境</h3><p>使用下面这条命令，激活环境：<br></p><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">conda activate pytorch</span><br></pre></td></tr></tbody></table></figure><br>出现下面所示:<br><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">(pytorch) root@dev-wyf-react:~/wyf<span class="comment">#</span></span><br></pre></td></tr></tbody></table></figure><br>检测环境是否安装好:<br><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">(pytorch) root@dev-wyf-react:~/wyf<span class="comment"># conda info –envs</span></span><br></pre></td></tr></tbody></table></figure><br>出现下面所示：<br><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">base /root/anaconda3 pytorch * /root/anaconda3/envs/pytorch</span><br></pre></td></tr></tbody></table></figure><br>然后去选择适合自己的pytorch版本，点击下面那个链接:<br><a href="https://pytorch.org/">https://pytorch.org/</a><br><img src="https://pica.zhimg.com/v2-909fe51d58b9ad7b9a6494d682206fff.png" alt=""><br>利用安装的cudatoolkit=11.3可以很好的安装cuda工具包<p></p>
<h3 id="2-3-测试安装成功"><a href="#2-3-测试安装成功" class="headerlink" title="2.3 测试安装成功"></a>2.3 测试安装成功</h3><p>首先输入： python 然后在输入：import torch</p>
<p><img src="https://pic1.zhimg.com/v2-8351ad9a02919bb2186436fbb04719d5.png" alt=""></p>
<h3 id="2-4-退出之后如何查看自己安装的环境"><a href="#2-4-退出之后如何查看自己安装的环境" class="headerlink" title="2.4 退出之后如何查看自己安装的环境"></a>2.4 退出之后如何查看自己安装的环境</h3><p>如果在一台服务器上安装多个环境，一下子可能不记得需要激活哪个环境名称，这时候我们需要使用下面这个命令来查找：<br></p><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">conda info –envs</span><br></pre></td></tr></tbody></table></figure><br><img src="https://picx.zhimg.com/v2-a2d772f6c53ad75747862907d184eec3.png" alt=""><p></p>
<h2 id="迁移-conda-环境"><a href="#迁移-conda-环境" class="headerlink" title="迁移 conda 环境"></a>迁移 conda 环境</h2><p>有时候我们需要在机器上重新建一个 conda 环境，但是又不想重新装包，毕竟 pytorch 和 cuda 版本都跟之前的环境是一样的，所以可以直接从之前的环境中复制一份成为新环境，conda 是支持这样做的，以下命令就将 BBB 环境拷贝了一份成为 AAA 环境。</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">conda create -n AAA --<span class="built_in">clone</span> BBB</span><br></pre></td></tr></tbody></table></figure>
<p>如果涉及不同服务器之间装环境的话也一样，可以先将旧的环境拷贝到新的电脑，然后通过下面的命令创一个新的环境</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">conda create -n AAA --<span class="built_in">clone</span> ~/path </span><br></pre></td></tr></tbody></table></figure>
<p>可以使用 <code>conda info -e</code> 来查询机器上的所有 conda 环境以及对应所在的位置。</p>
<h2 id="其他错误"><a href="#其他错误" class="headerlink" title="其他错误"></a>其他错误</h2><p>step1：安装anacoda 下载地址</p>
<figure class="highlight jsx"><table><tbody><tr><td class="code"><pre><span class="line">bash <span class="title class_">Anaconda3</span>-<span class="number">2018.12</span>-<span class="title class_">Linux</span>-x86_64.<span class="property">sh</span></span><br></pre></td></tr></tbody></table></figure>
<p>step2：按照官网的方法安装pytorch </p>
<figure class="highlight jsx"><table><tbody><tr><td class="code"><pre><span class="line">conda install pytorch torchvision cudatoolkit=<span class="number">10.0</span> -c pytorch</span><br></pre></td></tr></tbody></table></figure>
<p>在此期间你会发现特别的慢，慢也就算了主要它还老安装不成功，这是为什么呢<br>在用conda安装软件的过程中还会经常遇到的一个问题，就是：软件太大老是下载中断 我要下载一个软件，conda会告诉我这个软件底层依赖好几个其他的软件，需要将它们一起下载安装，这个时候这一大堆软件中可能有几个体量很大，上百MB，很容易下着下着网络连接就中断了，而conda有没有断点下载功能，一旦下载中断，conda就会终止安装进程退出运行 最后，我们转战清华源安装,运行以下命令:</p>
<figure class="highlight jsx"><table><tbody><tr><td class="code"><pre><span class="line">conda config --add channels <span class="attr">https</span>:<span class="comment">//mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span></span><br><span class="line">conda config --add channels <span class="attr">https</span>:<span class="comment">//mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</span></span><br><span class="line">conda config --set show_channel_urls yes</span><br><span class="line"># reference</span><br><span class="line"></span><br><span class="line"># <span class="attr">https</span>:<span class="comment">//mirror.tuna.tsinghua.edu.cn/help/anaconda/</span></span><br><span class="line">conda config --add channels <span class="attr">https</span>:<span class="comment">//mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/</span></span><br></pre></td></tr></tbody></table></figure>
<p>LAST，直接运行下面的命令就可以快速安装好啦，真的真的真的很快，就是不走-c pytorch即可</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">conda install pytorch torchvision cudatoolkit=10.0</span><br></pre></td></tr></tbody></table></figure>
<p>可参考：<a href="https://cloud.tencent.com/developer/article/1627527">https://cloud.tencent.com/developer/article/1627527</a></p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Tmux使用教程</title>
    <url>/2024/01/01/Linux/Tmux%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/</url>
    <content><![CDATA[<h2 id="1-tmux介绍"><a href="#1-tmux介绍" class="headerlink" title="1. tmux介绍"></a>1. tmux介绍</h2><h3 id="1-1-会话与进程"><a href="#1-1-会话与进程" class="headerlink" title="1.1 会话与进程"></a>1.1 会话与进程</h3><p>命令行的典型使用方式是，打开一个终端窗口（terminal window，以下简称“窗口”），在里面输入命令。<strong>用户与计算机的这种临时的交互，称为一次“会话”（session）</strong> 。</p>
<p>会话的一个重要特点是，窗口与其中启动的进程是<a href="https://www.ruanyifeng.com/blog/2016/02/linux-daemon.html">连在一起</a>的。打开窗口，会话开始；关闭窗口，会话结束，会话内部的进程也会随之终止，不管有没有运行完。</p>
<p>一个典型的例子就是，<a href="https://www.ruanyifeng.com/blog/2011/12/ssh_remote_login.html">SSH 登录</a>远程计算机，打开一个远程窗口执行命令。这时，网络突然断线，再次登录的时候，是找不回上一次执行的命令的。因为上一次 SSH 会话已经终止了，里面的进程也随之消失了。</p>
<p>为了解决这个问题，会话与窗口可以“解绑”：窗口关闭时，会话并不终止，而是继续运行，等到以后需要的时候，再让会话“绑定”其他窗口。</p>
<h3 id="1-2-Tmux-的作用"><a href="#1-2-Tmux-的作用" class="headerlink" title="1.2 Tmux 的作用"></a>1.2 Tmux 的作用</h3><p><strong>Tmux 就是会话与窗口的“解绑”工具，将它们彻底分离。</strong></p>
<p>（1）它允许在单个窗口中，同时访问多个会话。这对于同时运行多个命令行程序很有用。</p>
<p>（2） 它可以让新窗口“接入”已经存在的会话。</p>
<p>（3）它允许每个会话有多个连接窗口，因此可以多人实时共享会话。</p>
<p>（4）它还支持窗口任意的垂直和水平拆分。</p>
<p>类似的终端复用器还有 GNU Screen。Tmux 与它功能相似，但是更易用，也更强大。二、基本用法</p>
<h2 id="2-tmux安装"><a href="#2-tmux安装" class="headerlink" title="2. tmux安装"></a>2. tmux安装</h2><h3 id="2-1-安装"><a href="#2-1-安装" class="headerlink" title="2.1 安装"></a>2.1 安装</h3><p>Tmux 一般需要自己安装。</p>
<h1 id="Ubuntu-或-Debian"><a href="#Ubuntu-或-Debian" class="headerlink" title="Ubuntu 或 Debian"></a>Ubuntu 或 Debian</h1><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">sudo apt-get install tmux</span><br><span class="line"><span class="comment"># CentOS 或 Fedora</span></span><br><span class="line">sudo yum install tmux</span><br><span class="line"><span class="comment"># Mac</span></span><br><span class="line">brew install tmux</span><br></pre></td></tr></tbody></table></figure>
<h3 id="2-2-启动与退出"><a href="#2-2-启动与退出" class="headerlink" title="2.2 启动与退出"></a>2.2 启动与退出</h3><p>安装完成后，键入<code>tmux</code>命令，就进入了 Tmux 窗口。</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">tmux</span><br></pre></td></tr></tbody></table></figure>
<p>上面命令会启动 Tmux 窗口，底部有一个状态栏。状态栏的左侧是窗口信息（编号和名称），右侧是系统信息。</p>
<p><img src="https://picx.zhimg.com/v2-3a8a91db08866f19a26aa93ac8b40fc4.png" alt=""></p>
<p>按下<code>Ctrl+d</code>或者显式输入<code>exit</code>命令，就可以退出 Tmux 窗口。</p>
<p>exit</p>
<h3 id="2-3-前缀键"><a href="#2-3-前缀键" class="headerlink" title="2.3 前缀键"></a>2.3 前缀键</h3><p>Tmux 窗口有大量的快捷键。所有快捷键都要通过前缀键唤起。默认的前缀键是<code>Ctrl+b</code>，即先按下<code>Ctrl+b</code>，快捷键才会生效。</p>
<p>举例来说，帮助命令的快捷键是<code>Ctrl+b ?</code>。它的用法是，在 Tmux 窗口中，先按下<code>Ctrl+b</code>，再按下<code>?</code>，就会显示帮助信息。</p>
<p>然后，按下 ESC 键或<code>q</code>键，就可以退出帮助。</p>
<h2 id="3-常用命令"><a href="#3-常用命令" class="headerlink" title="3. 常用命令"></a>3. 常用命令</h2><h3 id="3-1-新建会话"><a href="#3-1-新建会话" class="headerlink" title="3.1 新建会话"></a>3.1 新建会话</h3><p>第一个启动的 Tmux 窗口，编号是<code>0</code>，第二个窗口的编号是<code>1</code>，以此类推。这些窗口对应的会话，就是 0 号会话、1 号会话。</p>
<p>使用编号区分会话，不太直观，更好的方法是为会话起名。</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">tmux new -s &lt;session-name&gt;</span><br></pre></td></tr></tbody></table></figure>
<p>上面命令新建一个指定名称的会话。</p>
<h3 id="3-2-分离会话"><a href="#3-2-分离会话" class="headerlink" title="3.2 分离会话"></a>3.2 分离会话</h3><p>在 Tmux 窗口中，按下<code>Ctrl+b d</code>或者输入<code>tmux detach</code>命令，就会将当前会话与窗口分离。</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">tmux detach</span><br></pre></td></tr></tbody></table></figure>
<p>上面命令执行后，就会退出当前 Tmux 窗口，但是会话和里面的进程仍然在后台运行。</p>
<p><code>tmux ls</code>命令可以查看当前所有的 Tmux 会话。</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">tmux <span class="built_in">ls</span></span><br><span class="line"><span class="comment"># ortmux list-session</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="3-3-接入会话"><a href="#3-3-接入会话" class="headerlink" title="3.3 接入会话"></a>3.3 接入会话</h3><p><code>tmux attach</code>命令用于重新接入某个已存在的会话。</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 使用会话编号</span></span><br><span class="line">tmux attach -t 0</span><br><span class="line"><span class="comment"># 使用会话名称</span></span><br><span class="line">tmux attach -t &lt;session-name&gt;</span><br></pre></td></tr></tbody></table></figure>
<h3 id="3-4-杀死会话"><a href="#3-4-杀死会话" class="headerlink" title="3.4 杀死会话"></a>3.4 杀死会话</h3><p><code>tmux kill-session</code>命令用于杀死某个会话。</p>
<h1 id="使用会话编号"><a href="#使用会话编号" class="headerlink" title="使用会话编号"></a>使用会话编号</h1><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">tmux kill-session -t 0</span><br><span class="line"><span class="comment"># 使用会话名称</span></span><br><span class="line">tmux kill-session -t &lt;session-name&gt;</span><br></pre></td></tr></tbody></table></figure>
<h3 id="3-5-切换会话"><a href="#3-5-切换会话" class="headerlink" title="3.5 切换会话"></a>3.5 切换会话</h3><p><code>tmux switch</code>命令用于切换会话。</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 使用会话编号</span></span><br><span class="line">tmux switch -t 0</span><br><span class="line"><span class="comment"># 使用会话名称</span></span><br><span class="line">tmux switch -t &lt;session-name&gt;</span><br></pre></td></tr></tbody></table></figure>
<h3 id="3-6-重命名会话"><a href="#3-6-重命名会话" class="headerlink" title="3.6 重命名会话"></a>3.6 重命名会话</h3><p><code>tmux rename-session</code>命令用于重命名会话。</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">tmux rename-session -t 0 &lt;new-name&gt;</span><br></pre></td></tr></tbody></table></figure>
<p>上面命令将0号会话重命名。</p>
<h3 id="3-7-会话快捷键"><a href="#3-7-会话快捷键" class="headerlink" title="3.7 会话快捷键"></a>3.7 会话快捷键</h3><p>下面是一些会话相关的快捷键。</p>
<p>Ctrl+b d：分离当前会话。Ctrl+b s：列出所有会话。Ctrl+b $：重命名当前会话。</p>
<h2 id="4-最简操作流程"><a href="#4-最简操作流程" class="headerlink" title="4. 最简操作流程"></a>4. 最简操作流程</h2><p>综上所述，以下是 Tmux 的最简操作流程。</p>
<p>新建会话<code>tmux new -s my_session</code>,在 Tmux 窗口运行所需的程序。按下快捷键<code>Ctrl+b d</code>将会话分离。下次使用时，重新连接到会话<code>tmux attach-session -t my_session</code>。</p>
<h3 id="窗格快捷键"><a href="#窗格快捷键" class="headerlink" title="窗格快捷键"></a>窗格快捷键</h3><p>下面是一些窗格操作的快捷键。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>快捷键</th>
<th>功能</th>
</tr>
</thead>
<tbody>
<tr>
<td>Ctrl+b %</td>
<td>划分左右两个窗格</td>
</tr>
<tr>
<td>Ctrl+b “</td>
<td>划分上下两个窗格</td>
</tr>
<tr>
<td>Ctrl+b <arrow key=""></arrow></td>
<td>光标切换到其他窗格。<arrow key="">是指向要切换到的窗格的方向键，比如切换到下方窗格，就按方向键↓</arrow></td>
</tr>
<tr>
<td>Ctrl+b ;</td>
<td>光标切换到上一个窗格</td>
</tr>
<tr>
<td>Ctrl+b o</td>
<td>光标切换到下一个窗格</td>
</tr>
<tr>
<td>Ctrl+b {</td>
<td>当前窗格与上一个窗格交换位置</td>
</tr>
<tr>
<td>Ctrl+b }</td>
<td>当前窗格与下一个窗格交换位置</td>
</tr>
<tr>
<td>Ctrl+b Ctrl+o</td>
<td>所有窗格向前移动一个位置，第一个窗格变成最后一个窗格</td>
</tr>
<tr>
<td>Ctrl+b Alt+o</td>
<td>所有窗格向后移动一个位置，最后一个窗格变成第一个窗格</td>
</tr>
<tr>
<td>Ctrl+b x</td>
<td>关闭当前窗格</td>
</tr>
<tr>
<td>Ctrl+b !</td>
<td>将当前窗格拆分为一个独立窗口</td>
</tr>
<tr>
<td>Ctrl+b z</td>
<td>当前窗格全屏显示，再使用一次会变回原来大小</td>
</tr>
<tr>
<td>Ctrl+b Ctrl+<arrow key=""></arrow></td>
<td>按箭头方向调整窗格大小</td>
</tr>
<tr>
<td>Ctrl+b q</td>
<td>显示窗格编号</td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>防火墙ufw 以及 开放端口</title>
    <url>/2024/01/01/Linux/%E9%98%B2%E7%81%AB%E5%A2%99ufw%20%E4%BB%A5%E5%8F%8A%20%E5%BC%80%E6%94%BE%E7%AB%AF%E5%8F%A3/</url>
    <content><![CDATA[<p>Ubuntu  默认使用 UFW（Uncomplicated Firewall）作为防火墙，并且已经支持界面操作了。在命令行中运行 <code>ufw</code> 命令就可以看到一系列可进行的操作。</p>
<p>最简单的一个操作是使用以下命令来检查防火墙的状态：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">sudo ufw status</span><br></pre></td></tr></tbody></table></figure>
<p>我的返回结果是：不活动。</p>
<p>防火墙版本：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">sudo ufw version</span><br><span class="line">ufw 0.29-4ubuntu1</span><br><span class="line">Copyright 2008-2009 Canonical Ltd.</span><br></pre></td></tr></tbody></table></figure>
<p>Ubuntu 系统默认已安装 UFW。</p>
<h3 id="1-安装"><a href="#1-安装" class="headerlink" title="1. 安装"></a>1. 安装</h3><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">sudo apt-get install ufw</span><br></pre></td></tr></tbody></table></figure>
<h3 id="2-启用"><a href="#2-启用" class="headerlink" title="2. 启用"></a>2. 启用</h3><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">sudo ufw <span class="built_in">enable</span></span><br><span class="line">sudo ufw default deny</span><br></pre></td></tr></tbody></table></figure>
<p>运行以上两条命令后，开启了防火墙，并在系统启动时自动开启。这将关闭所有外部对本机的访问，但本机访问外部仍然正常。</p>
<h3 id="3-开启-禁用"><a href="#3-开启-禁用" class="headerlink" title="3. 开启/禁用"></a>3. 开启/禁用</h3><p>以下命令可以打开或关闭某个端口：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">sudo ufw allow smtp     <span class="comment"># 允许所有的外部 IP 访问本机的 25/tcp (smtp) 端口</span></span><br><span class="line">sudo ufw allow 22/tcp   <span class="comment"># 允许所有的外部 IP 访问本机的 22/tcp (ssh) 端口</span></span><br><span class="line">sudo ufw allow 53       <span class="comment"># 允许外部访问 53 端口 (tcp/udp)</span></span><br><span class="line">sudo ufw allow from 192.168.1.100   <span class="comment"># 允许 IP 地址为 192.168.1.100 的主机访问所有本机端口</span></span><br><span class="line">sudo ufw allow proto udp from 192.168.0.1 port 53 to 192.168.0.2 port 53</span><br><span class="line">sudo ufw deny smtp      <span class="comment"># 禁止外部访问 smtp 服务</span></span><br><span class="line">sudo ufw delete allow smtp   <span class="comment"># 删除上面建立的某条规则</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="4-查看防火墙状态"><a href="#4-查看防火墙状态" class="headerlink" title="4. 查看防火墙状态"></a>4. 查看防火墙状态</h3><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">sudo ufw status</span><br></pre></td></tr></tbody></table></figure>
<p>一般用户只需要如下设置：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">sudo apt-get install ufw</span><br><span class="line">sudo ufw <span class="built_in">enable</span></span><br><span class="line">sudo ufw default deny</span><br></pre></td></tr></tbody></table></figure>
<p>以上三条命令已经足够安全了，如果你需要开放某些服务，再使用 <code>sudo ufw allow</code> 开启。</p>
<p>其他命令：</p>
<ul>
<li>开启/关闭防火墙（默认设置是 <code>disable</code>）：<code>sudo ufw enable|disable</code></li>
<li>转换日志状态：<code>sudo ufw logging on|off</code></li>
<li>设置默认策略（比如 “mostly open” vs “mostly closed”）：<code>sudo ufw default allow|deny</code></li>
<li>允许或屏蔽端口：<code>sudo ufw allow|deny [service]</code></li>
<li>显示防火墙和端口的侦听状态：<code>sudo ufw status</code></li>
</ul>
<h3 id="UFW-使用范例"><a href="#UFW-使用范例" class="headerlink" title="UFW 使用范例"></a>UFW 使用范例</h3><p>允许 53 端口：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">sudo ufw allow 53</span><br></pre></td></tr></tbody></table></figure>
<p>禁用 53 端口：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">sudo ufw delete allow 53</span><br></pre></td></tr></tbody></table></figure>
<p>允许 80 端口：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">sudo ufw allow 80/tcp</span><br></pre></td></tr></tbody></table></figure>
<p>禁用 80 端口：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">sudo ufw delete allow 80/tcp</span><br></pre></td></tr></tbody></table></figure>
<p>允许 smtp 端口：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">sudo ufw allow smtp</span><br></pre></td></tr></tbody></table></figure>
<p>删除 smtp 端口的许可：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">sudo ufw delete allow smtp</span><br></pre></td></tr></tbody></table></figure>
<p>允许某特定 IP：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">sudo ufw allow from 192.168.254.254</span><br></pre></td></tr></tbody></table></figure>
<p>删除上面的规则：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">sudo ufw delete allow from 192.168.254.254</span><br></pre></td></tr></tbody></table></figure>
<p>Linux 2.4 内核以后提供了一个非常优秀的防火墙工具：netfilter/iptables。它免费且功能强大，可以对流入和流出的信息进行细化控制，实现防火墙、NAT（网络地址翻译）和数据包的分割等功能。netfilter 工作在内核内部，而 iptables 则是让用户定义规则集的表结构。</p>
<p>但是 iptables 的规则稍微有些复杂，因此 Ubuntu 提供了 UFW 这个设定工具，以简化 iptables 的某些设定，其后台仍然是 iptables。UFW 即 Uncomplicated Firewall 的简称，一些复杂的设定还是需要使用 iptables。</p>
<p>UFW 相关的文件和文件夹有：</p>
<ul>
<li><code>/etc/ufw/</code>：包含一些 UFW 的环境设定文件，如 <code>before.rules</code>、<code>after.rules</code>、<code>sysctl.conf</code>、<code>ufw.conf</code>，以及用于 IPv6 的 <code>before6.rules</code> 和 <code>after6.rules</code>。这些文件一般按照默认设置进行即可。</li>
<li>如果在启用 UFW 后，<code>/etc/ufw/sysctl.conf</code> 覆盖了默认的 <code>/etc/sysctl.conf</code> 文件，如果在 <code>/etc/ufw/sysctl.conf</code> 中有新的赋值，则会覆盖 <code>/etc/sysctl.conf</code>，否则以 <code>/etc/sysctl.conf</code> 的设置为准。你可以通过修改 <code>/etc/default/ufw</code> 中的 <code>IPT_SYSCTL=</code> 条目来设置使用哪个 sysctrl.conf。</li>
<li><code>/var/lib/ufw/user.rules</code>：这个文件中包含我们设置的一些防火墙规则，你可以直接修改这个文件来进行设定。修改后需要使用 <code>ufw reload</code> 命令重启 UFW 以使新规则生效。</li>
</ul>
<p>下面是 UFW 命令行的一些示例：</p>
<ul>
<li><code>ufw enable/disable</code>：打开/关闭 UFW</li>
<li><code>ufw status</code>：查看已经定义的 UFW 规则</li>
<li><code>ufw default allow/deny</code>：外来访问默认允许/拒绝</li>
<li><code>ufw allow/deny 20</code>：允许/拒绝访问 20 端口（<code>20</code> 后可跟 <code>/tcp</code> 或 `/udp</li>
</ul>
<p>`，表示 TCP 或 UDP 封包）</p>
<ul>
<li><code>ufw allow/deny servicename</code>：UFW 从 <code>/etc/services</code> 中找到对应服务的端口进行过滤</li>
<li><code>ufw allow proto tcp from 10.0.1.0/10 to 本机ip port 25</code>：允许来自 <code>10.0.1.0/10</code> 的 TCP 封包访问本机的 25 端口</li>
<li><code>ufw delete allow/deny 20</code>：删除以前定义的允许/拒绝访问 20 端口的规则</li>
</ul>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux的 shell无法自动补全命令</title>
    <url>/2024/01/01/Linux/Linux%E7%9A%84%20shell%E6%97%A0%E6%B3%95%E8%87%AA%E5%8A%A8%E8%A1%A5%E5%85%A8%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<p>在<a href="https://so.csdn.net/so/search?q=ubuntu&amp;spm=1001.2101.3001.7020">ubuntu</a>上使用useradd新建了一个用户，发现用新建的用户登陆无法使用tab键补全，这个问题是否让人恼火，所以来解决一下。</p>
<h2 id="一、问题"><a href="#一、问题" class="headerlink" title="一、问题"></a>一、问题</h2><p>这是因为shell的解释器不是bash，需把shell的解释器更改为bash</p>
<h2 id="二、两种解决方法"><a href="#二、两种解决方法" class="headerlink" title="二、两种解决方法"></a>二、两种解决方法</h2><h3 id="1-方式一"><a href="#1-方式一" class="headerlink" title="1)方式一"></a>1)方式一</h3><p><img src="https://picx.zhimg.com/v2-3abd850433ca2709a7ca8e90cb50b340.png" alt=""></p>
<p>新建一用户lqding，切换到该用户下</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">root@lqding:~<span class="comment"># su - lqding</span></span><br><span class="line">$ <span class="built_in">echo</span> <span class="variable">$USER</span></span><br><span class="line">lqding</span><br></pre></td></tr></tbody></table></figure>
<p>提示符仅仅是一个$ ，很奇怪。输入命令，用TAB键也无法补全命令。并且没有命令历史功能。</p>
<p>使用root用户看passwd文件</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">root@lqding:~<span class="comment"># grep lqding /etc/passwd</span></span><br><span class="line">lqding:x:1000:1000::/home/lqding:/bin/sh</span><br><span class="line">root@lqding:~<span class="comment">#</span></span><br></pre></td></tr></tbody></table></figure>
<p>原来lqding用户默认的shell是/bin/sh 将其改为/bin/bash后</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">root@lqding:~<span class="comment"># vi /etc/passwd</span></span><br><span class="line">root@lqding:~<span class="comment"># grep lqding /etc/passwd</span></span><br><span class="line">lqding:x:1000:1000::/home/lqding:/bin/bash</span><br><span class="line">root@lqding:~<span class="comment"># su - lqding</span></span><br><span class="line">lqding@lqding:~$</span><br></pre></td></tr></tbody></table></figure>
<p>经 测试 ，一切功能正常。</p>
<h3 id="2-方式二："><a href="#2-方式二：" class="headerlink" title="2)方式二："></a>2)方式二：</h3><p><img src="https://pica.zhimg.com/v2-94d10245114f3dfa18bc77a058301ee1.png" alt=""></p>
<p>一劳永逸型的。</p>
<p>这样问题就解决了</p>
<h2 id="三、补充知识点"><a href="#三、补充知识点" class="headerlink" title="三、补充知识点"></a>三、补充知识点</h2><h3 id="1-作用"><a href="#1-作用" class="headerlink" title="1. 作用"></a>1. 作用</h3><p><code>useradd</code> 或 <code>adduser</code> 命令用于创建用户帐号和设置用户的初始目录，通常需要超级用户权限。</p>
<h3 id="2-格式"><a href="#2-格式" class="headerlink" title="2. 格式"></a>2. 格式</h3><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">useradd [-d home] [-s shell] [-c comment] [-m [-k template]] [-f inactive] [-e expire ] [-p passwd] [-r] name</span><br></pre></td></tr></tbody></table></figure>
<h3 id="3-主要参数"><a href="#3-主要参数" class="headerlink" title="3. 主要参数"></a>3. 主要参数</h3><ul>
<li><code>-c</code>：添加备注文字，备注文字保存在 passwd 的备注栏中。</li>
<li><code>-d</code>：指定用户登入时的主目录，替换系统默认值 <code>/home/&lt;用户名&gt;</code>。</li>
<li><code>-D</code>：变更预设值。</li>
<li><code>-e</code>：指定账号的失效日期，日期格式为 MM/DD/YY，例如 06/30/12。缺省表示永久有效。</li>
<li><code>-f</code>：指定在密码过期后多少天即关闭该账号。如果为 0 账号立即被停用；如果为 -1 则账号一直可用。默认值为 -1。</li>
<li><code>-g</code>：指定用户所属的群组。值可以是组名也可以是 GID。用户组必须已经存在，默认值为 100，即 users。</li>
<li><code>-G</code>：指定用户所属的附加群组。</li>
<li><code>-m</code>：自动建立用户的登入目录。</li>
<li><code>-M</code>：不要自动建立用户的登入目录。</li>
<li><code>-n</code>：取消建立以用户名称为名的群组。</li>
<li><code>-r</code>：建立系统账号。</li>
<li><code>-s</code>：指定用户登入后所使用的 shell。默认值为 <code>/bin/bash</code>。</li>
<li><code>-u</code>：指定用户ID号。该值在系统中必须是唯一的。0~499默认是保留给系统用户账号使用的，所以该值必须大于 499。</li>
</ul>
<h3 id="4-说明"><a href="#4-说明" class="headerlink" title="4. 说明"></a>4. 说明</h3><p><code>useradd</code> 可用于创建用户账号，它和 <code>adduser</code> 命令是相同的。账号创建完成后，可以使用 <code>passwd</code> 设置账号的密码。通过 <code>useradd</code> 命令创建的账号实际上是保存在 <code>/etc/passwd</code> 文本文件中。</p>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://blog.csdn.net/bilifo/article/details/50442737">https://blog.csdn.net/bilifo/article/details/50442737</a></p>
<p><a href="https://blog.csdn.net/weixin_37569048/article/details/101675360">https://blog.csdn.net/weixin_37569048/article/details/101675360</a></p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</title>
    <url>/2024/01/20/Project/Linly-Talker%20-%20GPT-SoVITS/</url>
    <content><![CDATA[<h1 id="数字人的未来：数字人对话系统-Linly-Talker-克隆语音-GPT-SoVITS"><a href="#数字人的未来：数字人对话系统-Linly-Talker-克隆语音-GPT-SoVITS" class="headerlink" title="数字人的未来：数字人对话系统 Linly-Talker + 克隆语音 GPT-SoVITS"></a>数字人的未来：数字人对话系统 Linly-Talker + 克隆语音 GPT-SoVITS</h1><p><a href="https://github.com/Kedreamix/Linly-Talker">https://github.com/Kedreamix/Linly-Talker</a></p>
<p><strong>2023.12 更新</strong> 📆</p>
<p><strong>用户可以上传任意图片进行对话</strong></p>
<p><strong>2024.01 更新</strong> 📆</p>
<ul>
<li><strong>令人兴奋的消息！我现在已经将强大的GeminiPro和Qwen大模型融入到我们的对话场景中。用户现在可以在对话中上传任何图片，为我们的互动增添了全新的层面。</strong></li>
<li><strong>更新了FastAPI的部署调用方法。</strong> </li>
<li><strong>更新了微软TTS的高级设置选项，增加声音种类的多样性，以及加入视频字幕加强可视化。</strong><ul>
<li><strong>更新了GPT多轮对话系统，使得对话有上下文联系，提高数字人的交互性和真实感。</strong></li>
</ul>
</li>
</ul>
<p><strong>2024.02 更新</strong> 📆</p>
<ul>
<li><strong>更新了Gradio的版本为最新版本4.16.0，使得界面拥有更多的功能，比如可以摄像头拍摄图片构建数字人等。</strong></li>
<li><strong>更新了ASR和THG，其中ASR加入了阿里的FunASR，具体更快的速度；THG部分加入了Wav2Lip模型，ER-NeRF在准备中(Comming Soon)。</strong></li>
<li><strong>加入了语音克隆方法GPT-SoVITS模型，能够通过微调一分钟对应人的语料进行克隆，效果还是相当不错的，值得推荐。</strong></li>
<li><strong>集成一个WebUI界面，能够更好的运行Linly-Talker。</strong></li>
</ul>
<p>在最近一段时间，我在尝试探索，如何克隆声音，因为在数字人对话系统中，虽然可能能够重建特定的人，但是还是存在一个问题：声音是用固定的人声生成的，导致没有真实性，如果我们能够去克隆出对应的声音，并且结合特定的数字人，那是否就完成了一个数字人的完整复刻。</p>
<p>于是我就研究了一段时间，后面发现了两个非常有意思的项目，分别是<code>GPT-SoVITS</code>和<code>XTTS</code>两个开源项目，我认为这两个算是现在最好的两个开源项目了，像OpenVoice之类的效果还是比较差，火山效果不错，但是没有开源。</p>
<p>除此之外，我后续集成到了Linly-Talker之中，做了一个WebUI，能够通过我3~10s的语音大概克隆我的声音，同时也可以使用一分钟克隆训练的语音来操作，如果使用多一点的预料能够得到更好的效果，希望和大家一起努力，成功复刻出一个完整的数字人</p>
<p>具体也可以关注我B站的演示的视频<a href="https://www.bilibili.com/video/BV1S4421A7gh">🚀数字人的未来：Linly-Talker+GPT-SoVIT语音克隆技术的赋能之道</a>和<a href="https://www.bilibili.com/video/BV1nu4m1K7qG">Linly-Talker WebUI🚀: 在对话时悄悄偷走你的声音🎤</a></p>
<h2 id="GPT-SoVITS（推荐）"><a href="#GPT-SoVITS（推荐）" class="headerlink" title="GPT-SoVITS（推荐）"></a>GPT-SoVITS（推荐）</h2><p>感谢大家的开源贡献，我借鉴了当前开源的语音克隆模型 <code>GPT-SoVITS</code>，我认为效果是相当不错的，项目地址可参考<a href="https://github.com/RVC-Boss/GPT-SoVITS">https://github.com/RVC-Boss/GPT-SoVITS</a></p>
<p>他有以下功能：</p>
<ol>
<li><strong>零样本文本到语音（TTS）：</strong> 输入 5 秒的声音样本，即刻体验文本到语音转换。</li>
<li><strong>少样本 TTS：</strong> 仅需 1 分钟的训练数据即可微调模型，提升声音相似度和真实感。</li>
<li><strong>跨语言支持：</strong> 支持与训练数据集不同语言的推理，目前支持英语、日语和中文。</li>
<li><strong>WebUI 工具：</strong> 集成工具包括声音伴奏分离、自动训练集分割、中文自动语音识别(ASR)和文本标注，协助初学者创建训练数据集和 GPT/SoVITS 模型。</li>
</ol>
<p>之前很多方法都是少样本，比如<code>OpenVoice</code>和<code>XTTS</code>，我之前也想着使用他们来进行实现语音克隆部分，但是很遗憾的是，并没有感觉有很好的效果，其实<code>XTTS</code>还是不错的，如果我们简单用麦克风🎤说几句话作为参考来进行克隆，我觉得效果还是可以的。</p>
<p>但是如果遇到比较高的要求，我觉得可能就需要更好的模型，并且成本也要打压下来，所以我就看到了这个<code>GPT-SoVITS</code>，我觉得这个模型是相当厉害的，少样本的TTS能做，也能做跨语言支持，这样我们很有可能就可以体验到奥巴马讲中文之类的，这样就可以完成视频翻译的一些任务了，所以我是很推崇这样的简单微调，效果又好的方法的。</p>
<p><strong>为了尊重作者，在Linly-Talker并没有把<code>GPT-SoVITS</code>的全套代码搬过来，我写了一个关于语音克隆的类，大家可以将训练好的模型参数中，就可以在本项目使用经过语音克隆后的TTS了，希望大家玩的开心，玩的愉快。</strong></p>
<blockquote>
<p>如果使用语音克隆模型，可能需要python为3.10，pytorch为2.1左右可能比较好，我的环境已经测试过了，简单来说，先安装GPT-SoVITS的环境，再直接pip intsall -r requirements_app.txt即可使用</p>
<p>除此之外，还需要根据原作者的说明放入对应路径，我的预训练模型和存放位置已给出，可参考<a href="https://huggingface.co/Kedreamix/Linly-Talker">https://huggingface.co/Kedreamix/Linly-Talker</a></p>
</blockquote>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">pip install torch==<span class="number">2.1</span><span class="number">.0</span> torchvision==<span class="number">0.16</span><span class="number">.0</span> torchaudio==<span class="number">2.1</span><span class="number">.0</span> --index-url https://download.pytorch.org/whl/cu118</span><br><span class="line"><span class="comment"># 安装对应的依赖</span></span><br><span class="line">pip install -r VITS/requirements_gptsovits.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动如下的WebUI界面</span></span><br><span class="line">python VITS/app.py </span><br></pre></td></tr></tbody></table></figure>
<p><img src="C:/Users/Kedreamix/Documents/GitHub/Linly-Talker/docs/GPT-SoVITS.png" alt=""></p>
<h2 id="Coqui-XTTS"><a href="#Coqui-XTTS" class="headerlink" title="Coqui XTTS"></a>Coqui XTTS</h2><p>Coqui XTTS是一个领先的深度学习文本到语音任务（TTS语音生成模型）工具包，通过使用一段5秒钟以上的语音频剪辑就可以完成声音克隆<em>将语音克隆到不同的语言</em>。支持多种语言文本到语音转换，使其成为国际化应用的理想选择，这一特点特别适用于全球化的市场，其中需要生成多种语言的语音内容。所以在实验过程中，我也加入了这一部分，不过暂时使用的是默认的模型，并没有进行微调，个人认为是没有GPT-SoVITS经过微调后好的，但是其中的少样本五秒钟克隆语音还是值得称赞的。大家也可以在官方的在线体验，但是官方的可能会有生成语音限制，文字不能太长，但是还是足够我们体验了。</p>
<p>🐸TTS 是一个用于高级文本转语音生成的库。</p>
<p>🚀 超过 1100 种语言的预训练模型。</p>
<p>🛠️ 用于以任何语言训练新模型和微调现有模型的工具。</p>
<p>📚 用于数据集分析和管理的实用程序。</p>
<ul>
<li>在线体验XTTS <a href="https://huggingface.co/spaces/coqui/xtts">https://huggingface.co/spaces/coqui/xtts</a></li>
<li>官方Github库 <a href="https://github.com/coqui-ai/TTS">https://github.com/coqui-ai/TTS</a></li>
</ul>
<p>XTTS的环境也需要PyTorch 2.1所以，如果下载了GPT-SoVITS，也不妨体验一下XTTS的效果。</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu118</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装对应的依赖</span></span><br><span class="line">pip install -r VITS/requirements_xtts.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动如下的WebUI界面</span></span><br><span class="line">python VITS/XTTS.py</span><br></pre></td></tr></tbody></table></figure>
<p><img src="C:/Users/Kedreamix/Documents/GitHub/Linly-Talker/docs/XTTS.png" alt=""></p>
<h2 id="Linly-Talker-WebUI"><a href="#Linly-Talker-WebUI" class="headerlink" title="Linly-Talker WebUI"></a>Linly-Talker WebUI</h2><p>之前我将很多个版本都是分开来的，实际上运行多个会比较麻烦，所以后续我增加了变成WebUI一个界面即可体验，后续也会不断更新</p>
<p>现在已加入WebUI的功能如下</p>
<ul>
<li>[x] 文本/语音数字人对话（固定数字人，分男女角色）</li>
<li>[x] 任意图片数字人对话（可上传任意数字人）</li>
<li>[x] 多轮GPT对话（加入历史对话数据，链接上下文）</li>
<li>[x] 语音克隆对话（基于GPT-SoVITS设置进行语音克隆，内置烟嗓音，可根据语音对话的声音进行克隆）</li>
</ul>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># WebUI</span></span><br><span class="line">python webui.py</span><br></pre></td></tr></tbody></table></figure>
<p><img src="C:/Users/Kedreamix/Documents/GitHub/Linly-Talker/docs/WebUI.png" alt=""></p>
]]></content>
      <categories>
        <category>Project</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>Talking Head Generation</tag>
      </tags>
  </entry>
  <entry>
    <title>ChatPaperFree GeminiPro（一分钟读论文）</title>
    <url>/2023/12/17/Project/ChatPaperFree/</url>
    <content><![CDATA[<p>实际上，这个项目是在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。这样一来，我们可以免费使用ChatPaper，并且未来我还计划加入对论文图片的读取以提取摘要（测试结果是OK的），大家可以关注一下，<a href="https://github.com/Kedreamix/ChatPaperFree">https://github.com/Kedreamix/ChatPaperFree</a>。</p>
<p>目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。如果在对话中chatbot能提供更优质的服务,我还会尝试进行更深层次的模型fine-tuning。我会在后续尝试进行更新，大家也可以提出自己的意见，也欢迎提PR。这是ChatPaper的<a href="https://github.com/kaixindelele/ChatPaper.git">GitHub链接</a>，多多关注。</p>
<p>另外,为了跟上快速发展的人工智能领域和庞大的arxiv论文,我们从arXiv爬取了大量论文,并制作了网站<a href="https://ipaper.today/">进步屋</a>使研究人员能够便捷获取当前热点。通过Gemini Pro对论文进行自动归纳,我们可以用极少的文本来展示论文要点,帮助用户快速评估哪篇值得深入研读。</p>
<p>在这个不断变化的时代,ChatPaperFree GeminiPro通过利用强大技术,有效提升我们学习效率。同时,我们也欢迎你提供关键词,共同完善这个平台,让每个人都能在技术演进中取得新的进步。（可在<a href="https://github.com/wmpscc/ipaper.today">https://github.com/wmpscc/ipaper.today</a> 提出关键词，欢迎）</p>
<p>希望未来,我们可以搭建更友好的人机合作模式。</p>
<p>我已经部署到了HuggingFace上了，大家都可以尝试用一下，看看怎么样 <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">https://huggingface.co/spaces/Kedreamix/ChatPaperFree</a></p>
<h2 id="Usage"><a href="#Usage" class="headerlink" title="Usage"></a>Usage</h2><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">pip install -r requriements.txt</span><br><span class="line">python app.py</span><br></pre></td></tr></tbody></table></figure>
<p>成功后，进入<a href="http://127.0.0.1:7860">http://127.0.0.1:7860</a> 即可，接着就可以输入自己的API key。</p>
<p>Google的Gemini Pro的API key是免费的，所以大家都是可以申请的，具体申请可以在<a href="https://makersuite.google.com/">https://makersuite.google.com/</a>进行获取，每个人都可以获取多个，如果大家有自己的一些想法，也可以看谷歌的API使用文档<a href="https://ai.google.dev/tutorials/python_quickstart">https://ai.google.dev/tutorials/python_quickstart</a>。</p>
<p><img src="https://picx.zhimg.com/80/v2-b1d0493ba08909ecfee8664cda4ff36d_720w.png?source=d16d100b" alt="获取API Key"></p>
<h2 id="获取最新文章（进步屋）"><a href="#获取最新文章（进步屋）" class="headerlink" title="获取最新文章（进步屋）"></a>获取最新文章（进步屋）</h2><p>实际上，这个操作与ChatPaper的爬取论文类似，我们通过爬取最新arxiv论文，根据设定的关键词，每天对论文进行爬取，以最快的速度跟踪最新的文章。实验室已经搭建了一个网站，大家可以多多支持，提出自己的关键词，我们会不定时进行更新，让你每天都能看到最新的文章。</p>
<p>这是进步屋的网站：<a href="https://ipaper.today/">进步屋</a> <a href="https://ipaper.today/">https://ipaper.today/</a> ，大家如果有什么需要学习的内容，可以给出关键词，在<a href="https://github.com/wmpscc/ipaper.today">GitHub</a>提issue。</p>
<p>后续，我还计划加入自动生成摘要和分析的功能，包括尝试让数字人播报等等，敬请期待。</p>
<p><img src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="ipaper进步屋"></p>
<p>最近，我还会尝试生成文章的简单摘要，丰富内容，这是即将上线的由GPT生成简单摘要的部分，后续还可以加入更多功能，比如ChatPaper的prompt，我猜可以得到更好的效果。</p>
<p><img src="https://pica.zhimg.com/80/v2-21ef11d4a14db737b3dd8d364bed0cea_720w.png?source=d16d100b" alt="Gemin Pro摘要总结"></p>
<h2 id="使用技巧："><a href="#使用技巧：" class="headerlink" title="使用技巧："></a>使用技巧：</h2><ul>
<li>找到好的文章后，可以精读这篇文章；</li>
<li>推荐其他两个精读论文的AI辅助网站：<a href="https://typeset.io/">Typeset.io</a> 和 <a href="https://chatpdf">chatpdf</a>。</li>
</ul>
<h2 id="Example1-进步屋"><a href="#Example1-进步屋" class="headerlink" title="Example1(进步屋)"></a>Example1(进步屋)</h2><p>比如最近<code>3D Gassian Splatting</code>很火，所以我也有关注这一方面，这是我最近爬取最新的几篇文章</p>
<h3 id="Triplane-Meets-Gaussian-Splatting-Fast-and-Generalizable-Single-View-3D-Reconstruction-with-Transformers"><a href="#Triplane-Meets-Gaussian-Splatting-Fast-and-Generalizable-Single-View-3D-Reconstruction-with-Transformers" class="headerlink" title="Triplane Meets Gaussian Splatting: Fast and Generalizable Single-View 3D   Reconstruction with Transformers"></a>Triplane Meets Gaussian Splatting: Fast and Generalizable Single-View 3D   Reconstruction with Transformers</h3><p><strong>Authors:Zi-Xin Zou, Zhipeng Yu, Yuan-Chen Guo, Yangguang Li, Ding Liang, Yan-Pei Cao, Song-Hai Zhang</strong></p>
<p>Recent advancements in 3D reconstruction from single images have been driven by the evolution of generative models. Prominent among these are methods based on Score Distillation Sampling (SDS) and the adaptation of diffusion models in the 3D domain. Despite their progress, these techniques often face limitations due to slow optimization or rendering processes, leading to extensive training and optimization times. In this paper, we introduce a novel approach for single-view reconstruction that efficiently generates a 3D model from a single image via feed-forward inference. Our method utilizes two transformer-based networks, namely a point decoder and a triplane decoder, to reconstruct 3D objects using a hybrid Triplane-Gaussian intermediate representation. This hybrid representation strikes a balance, achieving a faster rendering speed compared to implicit representations while simultaneously delivering superior rendering quality than explicit representations. The point decoder is designed for generating point clouds from single images, offering an explicit representation which is then utilized by the triplane decoder to query Gaussian features for each point. This design choice addresses the challenges associated with directly regressing explicit 3D Gaussian attributes characterized by their non-structural nature. Subsequently, the 3D Gaussians are decoded by an MLP to enable rapid rendering through splatting. Both decoders are built upon a scalable, transformer-based architecture and have been efficiently trained on large-scale 3D datasets. The evaluations conducted on both synthetic datasets and real-world images demonstrate that our method not only achieves higher quality but also ensures a faster runtime in comparison to previous state-of-the-art techniques. Please see our project page at <a href="https://zouzx.github.io/TriplaneGaussian/">https://zouzx.github.io/TriplaneGaussian/</a>. </p>
<p><a href="http://arxiv.org/abs/2312.09147v1">PDF</a> </p>
<p><strong>Summary</strong><br>基于变分自编码器和扩散模型的3D单视图重建方法取得了长足的进步。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>利用变压器网络和点解码器实现单图像的3D对象重建</li>
<li>使用混合的Triplane-Gaussian中间表示实现更快的渲染速度和更高的渲染质量</li>
<li>通过MLP解码3D高斯以实现快速渲染</li>
<li>在合成数据集和真实图像上进行了有效的评估</li>
<li>相比于先前的技术，方法达到了更高的质量和更快的运行时间</li>
<li>方法的详细信息可在<a href="https://zouzx.github.io/TriplaneGaussian/上找到。">https://zouzx.github.io/TriplaneGaussian/上找到。</a></li>
</ul>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/80/v2-2ebb2460e78384f8da85626de1b8120e_720w.jpeg?source=d16d100b" align="middle">
<img src="https://picx.zhimg.com/80/v2-52bff713f56ef38ffc7e6459d32e7b9f_720w.jpeg?source=d16d100b" align="middle">
<img src="https://pica.zhimg.com/80/v2-44f843bddff8a1e4072392b284803fdf_720w.jpeg?source=d16d100b" align="middle">
</details>




<h3 id="3DGS-Avatar-Animatable-Avatars-via-Deformable-3D-Gaussian-Splatting"><a href="#3DGS-Avatar-Animatable-Avatars-via-Deformable-3D-Gaussian-Splatting" class="headerlink" title="3DGS-Avatar: Animatable Avatars via Deformable 3D Gaussian Splatting"></a>3DGS-Avatar: Animatable Avatars via Deformable 3D Gaussian Splatting</h3><p><strong>Authors:Zhiyin Qian, Shaofei Wang, Marko Mihajlovic, Andreas Geiger, Siyu Tang</strong></p>
<p>We introduce an approach that creates animatable human avatars from monocular videos using 3D Gaussian Splatting (3DGS). Existing methods based on neural radiance fields (NeRFs) achieve high-quality novel-view/novel-pose image synthesis but often require days of training, and are extremely slow at inference time. Recently, the community has explored fast grid structures for efficient training of clothed avatars. Albeit being extremely fast at training, these methods can barely achieve an interactive rendering frame rate with around 15 FPS. In this paper, we use 3D Gaussian Splatting and learn a non-rigid deformation network to reconstruct animatable clothed human avatars that can be trained within 30 minutes and rendered at real-time frame rates (50+ FPS). Given the explicit nature of our representation, we further introduce as-isometric-as-possible regularizations on both the Gaussian mean vectors and the covariance matrices, enhancing the generalization of our model on highly articulated unseen poses. Experimental results show that our method achieves comparable and even better performance compared to state-of-the-art approaches on animatable avatar creation from a monocular input, while being 400x and 250x faster in training and inference, respectively. </p>
<p><a href="http://arxiv.org/abs/2312.09228v1">PDF</a> </p>
<p><strong>Summary</strong><br>使用三维高斯点阵和非刚性变形网络创建可动人体化身，训练时间短，渲染速度快。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>3D高斯点阵创建可动人体化身，训练时间30分钟，渲染速度50+ FPS</li>
<li>引入正则化提高模型在高度复杂的姿势上的泛化能力</li>
<li>实验结果表明方法在从单眼输入创建可动化身方面性能优越</li>
<li>与现有方法相比，训练速度快400倍，推理速度快250倍</li>
</ul>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/80/v2-d9069606a39ac20f1b0bf52ae2af1e83_720w.jpeg?source=d16d100b" align="middle">
<img src="https://picx.zhimg.com/80/v2-622f5d5aa71b525c2b25dfceb0d4c49a_720w.jpeg?source=d16d100b" align="middle">
<img src="https://picx.zhimg.com/80/v2-df8a29e21b43e7322f740381b022b6e4_720w.jpeg?source=d16d100b" align="middle">
<img src="https://pic1.zhimg.com/80/v2-c04b8f81d853c5df7e574e6e17d490fc_720w.jpeg?source=d16d100b" align="middle">
</details>


<p>​    </p>
<h2 id="Example2-ChatPaperFree"><a href="#Example2-ChatPaperFree" class="headerlink" title="Example2(ChatPaperFree)"></a>Example2(ChatPaperFree)</h2><p>比如这里使用上述的 <code>3DGS-Avatar: Animatable Avatars via Deformable 3D Gaussian Splatting</code> 进行测试，大家都可以尝试一下</p>
<p><img src="https://pic1.zhimg.com/80/v2-99a3ce94a6c8df465018c248a22bf2f7_720w.png?source=d16d100b" alt="ChatPaperFree Example"></p>
<ol>
<li><p>论文标题：3DGS-Avatar：可变形3D高斯点云生成的可动画身</p>
</li>
<li><p>作者：</p>
<ul>
<li>Changil Kim</li>
<li>Jinwoo Kim</li>
<li>Tae-Hyun Oh</li>
<li>Joon-Young Lee</li>
<li>In So Kweon</li>
</ul>
</li>
<li><p>第一作者单位：韩国科学技术院（KAIST）</p>
</li>
<li><p>关键词：</p>
<ul>
<li>可动画身</li>
<li>3D高斯点云生成</li>
<li>可变形神经场</li>
<li>单目视频</li>
</ul>
</li>
<li><p>论文链接：<a href="https://arxiv.org/abs/2312.09228">https://arxiv.org/abs/2312.09228</a> Github 链接：None</p>
</li>
<li><p>摘要：</p>
</li>
</ol>
<p>   (1)：研究背景：</p>
<ul>
<li>重建可动画身具有广泛的应用前景，但从图像输入中重建衣着的人体模型是一项具有挑战性的任务。</li>
<li>基于NeRF的神经辐射场方法在几何和外观重建方面取得了很好的效果，但训练和推理过程非常耗时。</li>
<li><p>最近提出的3D高斯点云生成方法在静态场景重建中取得了很好的效果，具有快速训练和推理的优点。</p>
<p>(2)：过去方法和问题：</p>
</li>
<li><p>现有方法主要依赖于NeRF或其变体，但这些方法通常需要大型的多层感知机来建模神经辐射场，导致训练和推理过程非常耗时。</p>
</li>
<li><p>一些方法尝试使用更快的训练和推理方法，但它们通常会牺牲渲染质量或无法对姿势相关的非刚性变形进行建模。</p>
<p>(3)：研究方法：</p>
</li>
<li><p>本文提出了一种基于3D高斯点云生成的可动画身重建方法。</p>
</li>
<li>该方法将刚性人体关节与非刚性变形场有效地集成到3D高斯点云生成框架中。</li>
<li>使用了一个小型多层感知机来解码颜色，该感知机能够对局部非刚性变形和动态光照条件做出响应。</li>
<li><p>应用了“尽可能等距”正则化项，以保持几何一致性和逼真的变形，尤其是在动态和变化的姿势中。</p>
<p>(4)：实验结果：</p>
</li>
<li><p>在单目视频输入的情况下，该方法在可动画身创建方面与最先进的方法相当或更好，训练速度提高了400倍，推理速度提高了250倍。</p>
</li>
<li>与专注于快速训练的方法相比，该方法虽然训练速度较慢，但能够对姿势相关的非刚性变形进行建模，并产生明显更好的渲染质量，同时渲染速度提高了3倍。</li>
</ul>
<ol>
<li>方法：</li>
</ol>
<p>（1）：提出了一种基于3D高斯点云生成的可动画身重建方法，将刚性人体关节与非刚性变形场有效地集成到3D高斯点云生成框架中；</p>
<p>（2）：使用了一个小型多层感知机来解码颜色，该感知机能够对局部非刚性变形和动态光照条件做出响应；</p>
<p>（3）：应用了“尽可能等距”正则化项，以保持几何一致性和逼真的变形，尤其是在动态和变化的姿势中。</p>
<ol>
<li>结论：</li>
</ol>
<p>（1）：本文提出了一种基于 3D 高斯点云生成的可动画身重建方法，该方法在单目视频输入的情况下，在可动画身创建方面与最先进的方法相当或更好，训练速度提高了 400 倍，推理速度提高了 250 倍。</p>
<p>（2）：创新点：提出了一种基于 3D 高斯点云生成的可动画身重建方法，将刚性人体关节与非刚性变形场有效地集成到 3D 高斯点云生成框架中；使用了一个小型多层感知机来解码颜色，该感知机能够对局部非刚性变形和动态光照条件做出响应；应用了“尽可能等距”正则化项，以保持几何一致性和逼真的变形，尤其是在动态和变化的姿势中。性能：在单目视频输入的情况下，该方法在可动画身创建方面与最先进的方法相当或更好，训练速度提高了 400 倍，推理速度提高了 250 倍；与专注于快速训练的方法相比，该方法虽然训练速度较慢，但能够对姿势相关的非刚性变形进行建模，并产生明显更好的渲染质量，同时渲染速度提高了 3 倍。工作量：该方法的训练和推理速度都非常快，训练一个模型只需要几分钟，推理一个模型只需要几毫秒。</p>
]]></content>
      <categories>
        <category>Project</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>Google</tag>
      </tags>
  </entry>
  <entry>
    <title>数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</title>
    <url>/2024/01/20/Project/Linly-Talker/</url>
    <content><![CDATA[<p><strong>2023.12 更新</strong> 📆</p>
<p><strong>用户可以上传任意图片进行对话</strong></p>
<p><strong>2024.01 更新</strong> 📆</p>
<ul>
<li><strong>令人兴奋的消息！我现在已经将强大的GeminiPro和Qwen大模型融入到我们的对话场景中。用户现在可以在对话中上传任何图片，为我们的互动增添了全新的层面。</strong></li>
<li><strong>更新了FastAPI的部署调用方法。</strong> </li>
<li><strong>更新了微软TTS的高级设置选项，增加声音种类的多样性，以及加入视频字幕加强可视化。</strong></li>
<li><strong>更新了GPT多轮对话系统，使得对话有上下文联系，提高数字人的交互性和真实感</strong></li>
</ul>
<p><strong>2024.02 更新</strong> 📆</p>
<ul>
<li><strong>更新了Gradio的版本为最新版本4.16.0，使得界面拥有更多的功能，比如可以摄像头拍摄图片构建数字人等</strong></li>
<li><strong>更新了ASR和THG，其中ASR加入了阿里的FunASR，具体更快的速度；THG部分加入了Wav2Lip模型，ER-NeRF在准备中(Comming Soon)</strong></li>
</ul>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</p>
<p><img src="https://picx.zhimg.com/80/v2-b38722d9d71153dec12acbb9e020a5b4.png" alt="The system architecture of multimodal human–computer interaction."></p>
<h2 id="TO-TO-DO-LIST"><a href="#TO-TO-DO-LIST" class="headerlink" title="TO## TO DO LIST"></a>TO## TO DO LIST</h2><ul>
<li>[x] 基本完成对话系统流程，能够<code>语音对话</code></li>
<li>[x] 加入了LLM大模型，包括<code>Linly</code>，<code>Qwen</code>和<code>GeminiPro</code>的使用</li>
<li>[x] 可上传<code>任意数字人照片</code>进行对话</li>
<li>[x] Linly加入<code>FastAPI</code>调用方式</li>
<li>[x] 利用微软<code>TTS</code>加入高级选项，可设置对应人声以及音调等参数，增加声音的多样性</li>
<li>[x] 视频生成加入<code>字幕</code>，能够更好的进行可视化</li>
<li>[x] GPT<code>多轮对话</code>系统（提高数字人的交互性和真实感，增强数字人的智能）</li>
<li>[x] 优化Gradio界面，加入更多模型，如Wav2Lip，FunASR等</li>
<li>[ ] <code>语音克隆</code>技术（语音克隆合成自己声音，提高数字人分身的真实感和互动体验）</li>
<li>[ ] 加入<code>Langchain</code>的框架，建立本地知识库</li>
<li>[ ] <code>实时</code>语音识别（人与数字人之间就可以通过语音进行对话交流)</li>
</ul>
<p>🔆 该项目 Linly-Talker 正在进行中 - 欢迎提出PR请求！如果您有任何关于新的模型方法、研究、技术或发现运行错误的建议，请随时编辑并提交 PR。您也可以打开一个问题或通过电子邮件直接联系我。📩⭐ 如果您发现这个Github Project有用，请给它点个星！🤩</p>
<h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">文字/语音对话</th>
<th style="text-align:center">数字人回答</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">应对压力最有效的方法是什么？</td>
<td style="text-align:center"><video src="https://github.com/Kedreamix/Linly-Talker/assets/61195303/f1deb189-b682-4175-9dea-7eeb0fb392ca"></video></td>
</tr>
<tr>
<td style="text-align:center">如何进行时间管理？</td>
<td style="text-align:center"><video src="https://github.com/Kedreamix/Linly-Talker/assets/61195303/968b5c43-4dce-484b-b6c6-0fd4d621ac03"></video></td>
</tr>
<tr>
<td style="text-align:center">撰写一篇交响乐音乐会评论，讨论乐团的表演和观众的整体体验。</td>
<td style="text-align:center"><video src="https://github.com/Kedreamix/Linly-Talker/assets/61195303/f052820f-6511-4cf0-a383-daf8402630db"></video></td>
</tr>
<tr>
<td style="text-align:center">翻译成中文：Luck is a dividend of sweat. The more you sweat, the luckier you get.</td>
<td style="text-align:center"><video src="https://github.com/Kedreamix/Linly-Talker/assets/61195303/118eec13-a9f7-4c38-b4ad-044d36ba9776"></video></td>
</tr>
</tbody>
</table>
</div>
<h2 id="创建环境"><a href="#创建环境" class="headerlink" title="创建环境"></a>创建环境</h2><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">conda create -n linly python=3.9 </span><br><span class="line">conda activate linly</span><br><span class="line"></span><br><span class="line"><span class="comment"># pytorch安装方式1：conda安装（推荐）</span></span><br><span class="line">conda install pytorch==1.12.1 torchvision==0.13.1 torchaudio==0.12.1 cudatoolkit=11.3 -c pytorch</span><br><span class="line"></span><br><span class="line"><span class="comment"># pytorch安装方式2：pip 安装</span></span><br><span class="line">pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113</span><br><span class="line"></span><br><span class="line">conda install -q ffmpeg <span class="comment"># ffmpeg==4.2.2</span></span><br><span class="line"></span><br><span class="line">pip install -r requirements_app.txt</span><br></pre></td></tr></tbody></table></figure>
<p>为了大家的部署使用方便，更新了一个<code>configs.py</code>文件，可以对其进行一些超参数修改即可</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 设备运行端口 (Device running port)</span></span><br><span class="line">port = 7870</span><br><span class="line"><span class="comment"># api运行端口及IP (API running port and IP)</span></span><br><span class="line">ip = <span class="string">'127.0.0.1'</span> </span><br><span class="line">api_port = 7871</span><br><span class="line"><span class="comment"># Linly模型路径 (Linly model path)</span></span><br><span class="line">mode = <span class="string">'api'</span> <span class="comment"># api 需要先运行Linly-api-fast.py</span></span><br><span class="line">mode = <span class="string">'offline'</span></span><br><span class="line">model_path = <span class="string">'Linly-AI/Chinese-LLaMA-2-7B-hf'</span></span><br><span class="line"><span class="comment"># ssl证书 (SSL certificate) 麦克风对话需要此参数</span></span><br><span class="line">ssl_certfile = <span class="string">"/path/to/Linly-Talker/https_cert/cert.pem"</span></span><br><span class="line">ssl_keyfile = <span class="string">"/path/to/Linly-Talker/https_cert/key.pem"</span></span><br></pre></td></tr></tbody></table></figure>
<h2 id="ASR-Speech-Recognition"><a href="#ASR-Speech-Recognition" class="headerlink" title="ASR - Speech Recognition"></a>ASR - Speech Recognition</h2><h3 id="Whisper"><a href="#Whisper" class="headerlink" title="Whisper"></a>Whisper</h3><p>借鉴OpenAI的Whisper实现了ASR的语音识别，具体使用方法参考 <a href="https://github.com/openai/whisper">https://github.com/openai/whisper</a></p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">https://github.com/openai/whisper</span></span><br><span class="line"><span class="string">pip install -U openai-whisper</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">import</span> whisper</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">WhisperASR</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model_path</span>):</span><br><span class="line">        self.LANGUAGES = {</span><br><span class="line">            <span class="string">"en"</span>: <span class="string">"english"</span>,</span><br><span class="line">            <span class="string">"zh"</span>: <span class="string">"chinese"</span>,</span><br><span class="line">        }</span><br><span class="line">        self.model = whisper.load_model(model_path)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">transcribe</span>(<span class="params">self, audio_file</span>):</span><br><span class="line">        result = self.model.transcribe(audio_file)</span><br><span class="line">        <span class="keyword">return</span> result[<span class="string">"text"</span>]</span><br></pre></td></tr></tbody></table></figure>
<h3 id="FunASR"><a href="#FunASR" class="headerlink" title="FunASR"></a>FunASR</h3><p>阿里的<code>FunASR</code>的语音识别效果也是相当不错，而且时间也是比whisper更快的，更能达到实时的效果，所以也将FunASR添加进去了，在ASR文件夹下的FunASR文件里可以进行体验，需要注意的是，在第一次运行的时候，需要安装以下库，参考 <a href="https://github.com/alibaba-damo-academy/FunASR">https://github.com/alibaba-damo-academy/FunASR</a></p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">pip install funasr</span><br><span class="line">pip install modelscope</span><br><span class="line">pip install -U rotary_embedding_torch</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">Reference: https://github.com/alibaba-damo-academy/FunASR</span></span><br><span class="line"><span class="string">pip install funasr</span></span><br><span class="line"><span class="string">pip install modelscope</span></span><br><span class="line"><span class="string">pip install -U rotary_embedding_torch</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">from</span> funasr <span class="keyword">import</span> AutoModel</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"如果想使用FunASR，请先安装funasr，若使用Whisper，请忽略此条信息"</span>)   </span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FunASR</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        self.model = AutoModel(model=<span class="string">"paraformer-zh"</span>, model_revision=<span class="string">"v2.0.4"</span>,</span><br><span class="line">                vad_model=<span class="string">"fsmn-vad"</span>, vad_model_revision=<span class="string">"v2.0.4"</span>,</span><br><span class="line">                punc_model=<span class="string">"ct-punc-c"</span>, punc_model_revision=<span class="string">"v2.0.4"</span>,</span><br><span class="line">                <span class="comment"># spk_model="cam++", spk_model_revision="v2.0.2",</span></span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">transcribe</span>(<span class="params">self, audio_file</span>):</span><br><span class="line">        res = self.model.generate(<span class="built_in">input</span>=audio_file, </span><br><span class="line">            batch_size_s=<span class="number">300</span>)</span><br><span class="line">        <span class="built_in">print</span>(res)</span><br><span class="line">        <span class="keyword">return</span> res[<span class="number">0</span>][<span class="string">'text'</span>]</span><br></pre></td></tr></tbody></table></figure>
<h2 id="TTS-Edge-TTS"><a href="#TTS-Edge-TTS" class="headerlink" title="TTS - Edge TTS"></a>TTS - Edge TTS</h2><p>使用微软语音服务,具体使用方法参考<a href="https://github.com/rany2/edge-tts">https://github.com/rany2/edge-tts</a></p>
<p>我编写了一个 <code>EdgeTTS</code> 的类，能够更好的使用，并且增加了保存字幕文件的功能</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EdgeTTS</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, list_voices = <span class="literal">False</span>, proxy = <span class="literal">None</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        voices = list_voices_fn(proxy=proxy)</span><br><span class="line">        self.SUPPORTED_VOICE = [item[<span class="string">'ShortName'</span>] <span class="keyword">for</span> item <span class="keyword">in</span> voices]</span><br><span class="line">        self.SUPPORTED_VOICE.sort(reverse=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">if</span> list_voices:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">", "</span>.join(self.SUPPORTED_VOICE))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">preprocess</span>(<span class="params">self, rate, volume, pitch</span>):</span><br><span class="line">        <span class="keyword">if</span> rate &gt;= <span class="number">0</span>:</span><br><span class="line">            rate = <span class="string">f'+<span class="subst">{rate}</span>%'</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            rate = <span class="string">f'<span class="subst">{rate}</span>%'</span></span><br><span class="line">        <span class="keyword">if</span> pitch &gt;= <span class="number">0</span>:</span><br><span class="line">            pitch = <span class="string">f'+<span class="subst">{pitch}</span>Hz'</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            pitch = <span class="string">f'<span class="subst">{pitch}</span>Hz'</span></span><br><span class="line">        volume = <span class="number">100</span> - volume</span><br><span class="line">        volume = <span class="string">f'-<span class="subst">{volume}</span>%'</span></span><br><span class="line">        <span class="keyword">return</span> rate, volume, pitch</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self,TEXT, VOICE, RATE, VOLUME, PITCH, OUTPUT_FILE=<span class="string">'result.wav'</span>, OUTPUT_SUBS=<span class="string">'result.vtt'</span>, words_in_cue = <span class="number">8</span></span>):</span><br><span class="line">        <span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">amain</span>() -&gt; <span class="literal">None</span>:</span><br><span class="line">            <span class="string">"""Main function"""</span></span><br><span class="line">            rate, volume, pitch = self.preprocess(rate = RATE, volume = VOLUME, pitch = PITCH)</span><br><span class="line">            communicate = Communicate(TEXT, VOICE, rate = rate, volume = volume, pitch = pitch)</span><br><span class="line">            subs: SubMaker = SubMaker()</span><br><span class="line">            sub_file: <span class="type">Union</span>[TextIOWrapper, TextIO] = (</span><br><span class="line">                <span class="built_in">open</span>(OUTPUT_SUBS, <span class="string">"w"</span>, encoding=<span class="string">"utf-8"</span>)</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">async</span> <span class="keyword">for</span> chunk <span class="keyword">in</span> communicate.stream():</span><br><span class="line">                <span class="keyword">if</span> chunk[<span class="string">"type"</span>] == <span class="string">"audio"</span>:</span><br><span class="line">                    <span class="comment"># audio_file.write(chunk["data"])</span></span><br><span class="line">                    <span class="keyword">pass</span></span><br><span class="line">                <span class="keyword">elif</span> chunk[<span class="string">"type"</span>] == <span class="string">"WordBoundary"</span>:</span><br><span class="line">                    <span class="comment"># print((chunk["offset"], chunk["duration"]), chunk["text"])</span></span><br><span class="line">                    subs.create_sub((chunk[<span class="string">"offset"</span>], chunk[<span class="string">"duration"</span>]), chunk[<span class="string">"text"</span>])</span><br><span class="line">            sub_file.write(subs.generate_subs(words_in_cue))</span><br><span class="line">            <span class="keyword">await</span> communicate.save(OUTPUT_FILE)</span><br><span class="line">            </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># loop = asyncio.get_event_loop_policy().get_event_loop()</span></span><br><span class="line">        <span class="comment"># try:</span></span><br><span class="line">        <span class="comment">#     loop.run_until_complete(amain())</span></span><br><span class="line">        <span class="comment"># finally:</span></span><br><span class="line">        <span class="comment">#     loop.close()</span></span><br><span class="line">        asyncio.run(amain())</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(OUTPUT_SUBS, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> file:</span><br><span class="line">            vtt_lines = file.readlines()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 去掉每一行文字中的空格</span></span><br><span class="line">        vtt_lines_without_spaces = [line.replace(<span class="string">" "</span>, <span class="string">""</span>) <span class="keyword">if</span> <span class="string">"--&gt;"</span> <span class="keyword">not</span> <span class="keyword">in</span> line <span class="keyword">else</span> line <span class="keyword">for</span> line <span class="keyword">in</span> vtt_lines]</span><br><span class="line">        <span class="comment"># print(vtt_lines_without_spaces)</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(OUTPUT_SUBS, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> output_file:</span><br><span class="line">            output_file.writelines(vtt_lines_without_spaces)</span><br><span class="line">        <span class="keyword">return</span> OUTPUT_FILE, OUTPUT_SUBS</span><br></pre></td></tr></tbody></table></figure>
<p>同时在<code>src</code>文件夹下，写了一个简易的<code>WebUI</code></p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">python TTS_app.py</span><br></pre></td></tr></tbody></table></figure>
<p><img src="https://picx.zhimg.com/80/v2-570f3c9a069358e4d4a7b7c008e99cb7.png" alt="TTS"></p>
<h2 id="THG-Avatar"><a href="#THG-Avatar" class="headerlink" title="THG - Avatar"></a>THG - Avatar</h2><h3 id="SadTalker"><a href="#SadTalker" class="headerlink" title="SadTalker"></a>SadTalker</h3><p>数字人生成可使用SadTalker（CVPR 2023）,详情介绍见 <a href="https://sadtalker.github.io">https://sadtalker.github.io</a></p>
<p>在使用前先下载SadTalker模型:</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">bash scripts/sadtalker_download_models.sh  </span><br></pre></td></tr></tbody></table></figure>
<p><a href="https://pan.baidu.com/s/1eF13O-8wyw4B3MtesctQyg?pwd=linl">Baidu (百度云盘)</a> (Password: <code>linl</code>)</p>
<blockquote>
<p>如果百度网盘下载，记住是放在checkpoints文件夹下，百度网盘下载的默认命名为sadtalker，实际应该重命名为checkpoints</p>
</blockquote>
<h3 id="Wav2Lip"><a href="#Wav2Lip" class="headerlink" title="Wav2Lip"></a>Wav2Lip</h3><p>数字人生成还可使用Wav2Lip（ACM 2020），详情介绍见 <a href="https://github.com/Rudrabha/Wav2Lip">https://github.com/Rudrabha/Wav2Lip</a></p>
<p>在使用前先下载Wav2Lip模型：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Model</th>
<th>Description</th>
<th>Link to the model</th>
</tr>
</thead>
<tbody>
<tr>
<td>Wav2Lip</td>
<td>Highly accurate lip-sync</td>
<td><a href="https://iiitaphyd-my.sharepoint.com/:u:/g/personal/radrabha_m_research_iiit_ac_in/Eb3LEzbfuKlJiR600lQWRxgBIY27JZg80f7V9jtMfbNDaQ?e=TBFBVW">Link</a></td>
</tr>
<tr>
<td>Wav2Lip + GAN</td>
<td>Slightly inferior lip-sync, but better visual quality</td>
<td><a href="https://iiitaphyd-my.sharepoint.com/:u:/g/personal/radrabha_m_research_iiit_ac_in/EdjI7bZlgApMqsVoEUUXpLsBxqXbn5z8VTmoxp55YNDcIA?e=n9ljGW">Link</a></td>
</tr>
<tr>
<td>Expert Discriminator</td>
<td>Weights of the expert discriminator</td>
<td><a href="https://iiitaphyd-my.sharepoint.com/:u:/g/personal/radrabha_m_research_iiit_ac_in/EQRvmiZg-HRAjvI6zqN9eTEBP74KefynCwPWVmF57l-AYA?e=ZRPHKP">Link</a></td>
</tr>
<tr>
<td>Visual Quality Discriminator</td>
<td>Weights of the visual disc trained in a GAN setup</td>
<td><a href="https://iiitaphyd-my.sharepoint.com/:u:/g/personal/radrabha_m_research_iiit_ac_in/EQVqH88dTm1HjlK11eNba5gBbn15WMS0B0EZbDBttqrqkg?e=ic0ljo">Link</a></td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Wav2Lip</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, path = <span class="string">'checkpoints/wav2lip.pth'</span></span>):</span><br><span class="line">        self.fps = <span class="number">25</span></span><br><span class="line">        self.resize_factor = <span class="number">1</span></span><br><span class="line">        self.mel_step_size = <span class="number">16</span></span><br><span class="line">        self.static = <span class="literal">False</span></span><br><span class="line">        self.img_size = <span class="number">96</span></span><br><span class="line">        self.face_det_batch_size = <span class="number">2</span></span><br><span class="line">        self.box = [-<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>]</span><br><span class="line">        self.pads = [<span class="number">0</span>, <span class="number">10</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">        self.nosmooth = <span class="literal">False</span></span><br><span class="line">        self.device = <span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span></span><br><span class="line">        self.model = self.load_model(path)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">load_model</span>(<span class="params">self, checkpoint_path</span>):</span><br><span class="line">        model = wav2lip_mdoel()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"Load checkpoint from: {}"</span>.<span class="built_in">format</span>(checkpoint_path))</span><br><span class="line">        <span class="keyword">if</span> self.device == <span class="string">'cuda'</span>:</span><br><span class="line">            checkpoint = torch.load(checkpoint_path)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            checkpoint = torch.load(checkpoint_path,</span><br><span class="line">                                    map_location=<span class="keyword">lambda</span> storage, loc: storage)</span><br><span class="line">        s = checkpoint[<span class="string">"state_dict"</span>]</span><br><span class="line">        new_s = {}</span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> s.items():</span><br><span class="line">            new_s[k.replace(<span class="string">'module.'</span>, <span class="string">''</span>)] = v</span><br><span class="line">        model.load_state_dict(new_s)</span><br><span class="line"></span><br><span class="line">        model = model.to(self.device)</span><br><span class="line">        <span class="keyword">return</span> model.<span class="built_in">eval</span>()</span><br></pre></td></tr></tbody></table></figure>
<h3 id="ER-NeRF（Comming-Soon）"><a href="#ER-NeRF（Comming-Soon）" class="headerlink" title="ER-NeRF（Comming Soon）"></a>ER-NeRF（Comming Soon）</h3><p>ER-NeRF（ICCV2023）是使用最新的NeRF技术构建的数字人，拥有定制数字人的特性，只需要一个人的五分钟左右到视频即可重建出来，具体可参考 <a href="https://github.com/Fictionarry/ER-NeRF">https://github.com/Fictionarry/ER-NeRF</a></p>
<p>后续会针对此更新</p>
<h2 id="LLM-Conversation"><a href="#LLM-Conversation" class="headerlink" title="LLM - Conversation"></a>LLM - Conversation</h2><h3 id="Linly-AI"><a href="#Linly-AI" class="headerlink" title="Linly-AI"></a>Linly-AI</h3><p>Linly来自深圳大学数据工程国家重点实验室,参考<a href="https://github.com/CVI-SZU/Linly">https://github.com/CVI-SZU/Linly</a></p>
<p>下载Linly模型:<a href="https://huggingface.co/Linly-AI/Chinese-LLaMA-2-7B-hf">https://huggingface.co/Linly-AI/Chinese-LLaMA-2-7B-hf</a></p>
<p>可以使用<code>git</code>下载</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">git lfs install</span><br><span class="line">git <span class="built_in">clone</span> https://huggingface.co/Linly-AI/Chinese-LLaMA-2-7B-hf</span><br></pre></td></tr></tbody></table></figure>
<p>或者使用<code>huggingface</code>的下载工具<code>huggingface-cli</code></p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">pip install -U huggingface_hub</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置镜像加速</span></span><br><span class="line"><span class="comment"># Linux</span></span><br><span class="line"><span class="built_in">export</span> HF_ENDPOINT=<span class="string">"https://hf-mirror.com"</span></span><br><span class="line"><span class="comment"># windows powershell</span></span><br><span class="line"><span class="variable">$env</span>:HF_ENDPOINT=<span class="string">"https://hf-mirror.com"</span></span><br><span class="line"></span><br><span class="line">huggingface-cli download --resume-download Linly-AI/Chinese-LLaMA-2-7B-hf --local-dir Linly-AI/Chinese-LLaMA-2-7B-hf</span><br></pre></td></tr></tbody></table></figure>
<p>或使用API:</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 命令行</span></span><br><span class="line">curl -X POST -H <span class="string">"Content-Type: application/json"</span> -d <span class="string">'{"question": "北京有什么好玩的地方?"}'</span> http://url:port  </span><br><span class="line"></span><br><span class="line"><span class="comment"># Python</span></span><br><span class="line">import requests</span><br><span class="line"></span><br><span class="line">url = <span class="string">"http://url:port"</span></span><br><span class="line">headers = {</span><br><span class="line">  <span class="string">"Content-Type"</span>: <span class="string">"application/json"</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">data = {</span><br><span class="line">  <span class="string">"question"</span>: <span class="string">"北京有什么好玩的地方?"</span> </span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">response = requests.post(url, headers=headers, json=data)</span><br><span class="line"><span class="comment"># response_text = response.content.decode("utf-8")</span></span><br><span class="line">answer, tag = response.json()</span><br><span class="line"><span class="comment"># print(answer)</span></span><br><span class="line"><span class="keyword">if</span> tag == <span class="string">'success'</span>:</span><br><span class="line">    response_text =  answer[0]</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"fail"</span>)</span><br><span class="line"><span class="built_in">print</span>(response_text)</span><br></pre></td></tr></tbody></table></figure>
<p>API部署推荐<strong>FastAPI</strong>，现在更新了 FastAPI 的API使用版本，FastAPI 是一个高性能、易用且现代的Python Web 框架，它通过使用最新的Python 特性和异步编程，提供了快速开发Web API 的能力。 该框架不仅易于学习和使用，还具有自动生成文档、数据验证等强大功能。 无论是构建小型项目还是大型应用程序，FastAPI 都是一个强大而有效的工具。</p>
<p>首先安装部署API所使用的库</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">pip install fastapi==0.104.1</span><br><span class="line">pip install uvicorn==0.24.0.post1</span><br></pre></td></tr></tbody></table></figure>
<p>其他使用方法大致相同，主要是不同代码实现方式，会更加简单边界，并且处理并发也会更好</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> fastapi <span class="keyword">import</span> FastAPI, Request</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForCausalLM, GenerationConfig</span><br><span class="line"><span class="keyword">import</span> uvicorn</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> configs <span class="keyword">import</span> model_path, api_port</span><br><span class="line"><span class="comment"># 设置设备参数</span></span><br><span class="line">DEVICE = <span class="string">"cuda"</span>  <span class="comment"># 使用CUDA</span></span><br><span class="line">DEVICE_ID = <span class="string">"0"</span>  <span class="comment"># CUDA设备ID，如果未设置则为空</span></span><br><span class="line">CUDA_DEVICE = <span class="string">f"<span class="subst">{DEVICE}</span>:<span class="subst">{DEVICE_ID}</span>"</span> <span class="keyword">if</span> DEVICE_ID <span class="keyword">else</span> DEVICE  <span class="comment"># 组合CUDA设备信息</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 清理GPU内存函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">torch_gc</span>():</span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():  <span class="comment"># 检查是否可用CUDA</span></span><br><span class="line">        <span class="keyword">with</span> torch.cuda.device(CUDA_DEVICE):  <span class="comment"># 指定CUDA设备</span></span><br><span class="line">            torch.cuda.empty_cache()  <span class="comment"># 清空CUDA缓存</span></span><br><span class="line">            torch.cuda.ipc_collect()  <span class="comment"># 收集CUDA内存碎片</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建FastAPI应用</span></span><br><span class="line">app = FastAPI()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理POST请求的端点</span></span><br><span class="line"><span class="meta">@app.post(<span class="params"><span class="string">"/"</span></span>)</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">create_item</span>(<span class="params">request: Request</span>):</span><br><span class="line">    <span class="keyword">global</span> model, tokenizer  <span class="comment"># 声明全局变量以便在函数内部使用模型和分词器</span></span><br><span class="line">    json_post_raw = <span class="keyword">await</span> request.json()  <span class="comment"># 获取POST请求的JSON数据</span></span><br><span class="line">    json_post = json.dumps(json_post_raw)  <span class="comment"># 将JSON数据转换为字符串</span></span><br><span class="line">    json_post_list = json.loads(json_post)  <span class="comment"># 将字符串转换为Python对象</span></span><br><span class="line">    prompt = json_post_list.get(<span class="string">'prompt'</span>)  <span class="comment"># 获取请求中的提示</span></span><br><span class="line">    history = json_post_list.get(<span class="string">'history'</span>)  <span class="comment"># 获取请求中的历史记录</span></span><br><span class="line">    max_length = json_post_list.get(<span class="string">'max_length'</span>)  <span class="comment"># 获取请求中的最大长度</span></span><br><span class="line">    top_p = json_post_list.get(<span class="string">'top_p'</span>)  <span class="comment"># 获取请求中的top_p参数</span></span><br><span class="line">    temperature = json_post_list.get(<span class="string">'temperature'</span>)  <span class="comment"># 获取请求中的温度参数</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 调用模型进行对话生成</span></span><br><span class="line">    prompt = <span class="string">f"请用少于25个字回答以下问题 ### Instruction:<span class="subst">{prompt}</span>  ### Response:"</span></span><br><span class="line">    inputs = tokenizer(prompt, return_tensors=<span class="string">"pt"</span>).to(<span class="string">"cuda:0"</span>)</span><br><span class="line">    generate_ids = model.generate(inputs.input_ids, </span><br><span class="line">                                  max_new_tokens=max_length <span class="keyword">if</span> max_length <span class="keyword">else</span> <span class="number">2048</span>,</span><br><span class="line">                                  do_sample=<span class="literal">True</span>, </span><br><span class="line">                                  top_k=<span class="number">20</span>,</span><br><span class="line">                                  top_p=top_p,</span><br><span class="line">                                  temperature=temperature <span class="keyword">if</span> temperature <span class="keyword">else</span> <span class="number">0.84</span>,</span><br><span class="line">                                  repetition_penalty=<span class="number">1.15</span>, eos_token_id=<span class="number">2</span>, bos_token_id=<span class="number">1</span>,pad_token_id=<span class="number">0</span>)</span><br><span class="line">    response = tokenizer.batch_decode(generate_ids, skip_special_tokens=<span class="literal">True</span>, clean_up_tokenization_spaces=<span class="literal">False</span>)[<span class="number">0</span>]</span><br><span class="line">    response = response.split(<span class="string">"### Response:"</span>)[-<span class="number">1</span>]</span><br><span class="line">    now = datetime.datetime.now()  <span class="comment"># 获取当前时间</span></span><br><span class="line">    time = now.strftime(<span class="string">"%Y-%m-%d %H:%M:%S"</span>)  <span class="comment"># 格式化时间为字符串</span></span><br><span class="line">    <span class="comment"># 构建响应JSON</span></span><br><span class="line">    answer = {</span><br><span class="line">        <span class="string">"response"</span>: response,</span><br><span class="line">        <span class="comment"># "history": history,</span></span><br><span class="line">        <span class="string">"status"</span>: <span class="number">200</span>,</span><br><span class="line">        <span class="string">"time"</span>: time</span><br><span class="line">    }</span><br><span class="line">    <span class="comment"># 构建日志信息</span></span><br><span class="line">    log = <span class="string">"["</span> + time + <span class="string">"] "</span> + <span class="string">'", prompt:"'</span> + prompt + <span class="string">'", response:"'</span> + <span class="built_in">repr</span>(response) + <span class="string">'"'</span></span><br><span class="line">    <span class="built_in">print</span>(log)  <span class="comment"># 打印日志</span></span><br><span class="line">    torch_gc()  <span class="comment"># 执行GPU内存清理</span></span><br><span class="line">    <span class="keyword">return</span> answer  <span class="comment"># 返回响应</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 主函数入口</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># 加载预训练的分词器和模型</span></span><br><span class="line">    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=<span class="string">"cuda:0"</span>,</span><br><span class="line">                                                    torch_dtype=torch.bfloat16, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line">    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=<span class="literal">False</span>, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line">    model.<span class="built_in">eval</span>()  <span class="comment"># 设置模型为评估模式</span></span><br><span class="line">    <span class="comment"># 启动FastAPI应用</span></span><br><span class="line">    uvicorn.run(app, host=<span class="string">'0.0.0.0'</span>, port=api_port, workers=<span class="number">1</span>)  <span class="comment"># 在指定端口和主机上启动应用</span></span><br></pre></td></tr></tbody></table></figure>
<p>默认部署在 7871 端口，通过 POST 方法进行调用，可以使用curl调用，如下所示：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">curl -X POST <span class="string">"http://127.0.0.1:7871"</span> \</span><br><span class="line">     -H <span class="string">'Content-Type: application/json'</span> \</span><br><span class="line">     -d <span class="string">'{"prompt": "如何应对压力"}'</span></span><br></pre></td></tr></tbody></table></figure>
<p>也可以使用python中的requests库进行调用，如下所示：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_completion</span>(<span class="params">prompt</span>):</span><br><span class="line">    headers = {<span class="string">'Content-Type'</span>: <span class="string">'application/json'</span>}</span><br><span class="line">    data = {<span class="string">"prompt"</span>: prompt}</span><br><span class="line">    response = requests.post(url=<span class="string">'http://127.0.0.1:7871'</span>, headers=headers, data=json.dumps(data))</span><br><span class="line">    <span class="keyword">return</span> response.json()[<span class="string">'response'</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="built_in">print</span>(get_completion(<span class="string">'你好如何应对压力'</span>))</span><br></pre></td></tr></tbody></table></figure>
<p>得到的返回值如下所示：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">{</span><br><span class="line">  <span class="string">"response"</span>:<span class="string">"寻求支持和放松，并采取积极的措施解决问题。"</span>,</span><br><span class="line">  <span class="string">"status"</span>:200,</span><br><span class="line">  <span class="string">"time"</span>:<span class="string">"2024-01-12 01:43:37"</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h3 id="Qwen"><a href="#Qwen" class="headerlink" title="Qwen"></a>Qwen</h3><p>来自阿里云的Qwen，查看 <a href="https://github.com/QwenLM/Qwen">https://github.com/QwenLM/Qwen</a></p>
<p>下载 Qwen 模型: <a href="https://huggingface.co/Qwen/Qwen-1_8B-Chat">https://huggingface.co/Qwen/Qwen-1_8B-Chat</a></p>
<p>可以使用<code>git</code>下载</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">git lfs install</span><br><span class="line">git <span class="built_in">clone</span> https://huggingface.co/Qwen/Qwen-1_8B-Chat</span><br></pre></td></tr></tbody></table></figure>
<p>或者使用<code>huggingface</code>的下载工具<code>huggingface-cli</code></p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">pip install -U huggingface_hub</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置镜像加速</span></span><br><span class="line"><span class="comment"># Linux</span></span><br><span class="line"><span class="built_in">export</span> HF_ENDPOINT=<span class="string">"https://hf-mirror.com"</span></span><br><span class="line"><span class="comment"># windows powershell</span></span><br><span class="line"><span class="variable">$env</span>:HF_ENDPOINT=<span class="string">"https://hf-mirror.com"</span></span><br><span class="line"></span><br><span class="line">huggingface-cli download --resume-download Qwen/Qwen-1_8B-Chat --local-dir Qwen/Qwen-1_8B-Chat</span><br></pre></td></tr></tbody></table></figure>
<h3 id="Gemini-Pro"><a href="#Gemini-Pro" class="headerlink" title="Gemini-Pro"></a>Gemini-Pro</h3><p>来自 Google 的 Gemini-Pro，了解更多请访问 <a href="https://deepmind.google/technologies/gemini/">https://deepmind.google/technologies/gemini/</a></p>
<p>请求 API 密钥: <a href="https://makersuite.google.com/">https://makersuite.google.com/</a></p>
<h3 id="LLM-模型选择"><a href="#LLM-模型选择" class="headerlink" title="LLM 模型选择"></a>LLM 模型选择</h3><p>在 app.py 文件中，轻松选择您需要的模型。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 取消注释并设置您选择的模型:</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># llm = Gemini(model_path='gemini-pro', api_key=None, proxy_url=None) # 不要忘记加入您自己的 Google API 密钥</span></span><br><span class="line"><span class="comment"># llm = Qwen(mode='offline', model_path="Qwen/Qwen-1_8B-Chat")</span></span><br><span class="line"><span class="comment"># 自动下载</span></span><br><span class="line"><span class="comment"># llm = Linly(mode='offline', model_path="Linly-AI/Chinese-LLaMA-2-7B-hf")</span></span><br><span class="line"><span class="comment"># 手动下载到指定路径</span></span><br><span class="line">llm = Linly(mode=<span class="string">'offline'</span>, model_path=<span class="string">"Linly-AI/Chinese-LLaMA-2-7B-hf"</span>)</span><br></pre></td></tr></tbody></table></figure>
<h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h2><p>一些优化:</p>
<ul>
<li>使用固定的输入人脸图像,提前提取特征,避免每次读取</li>
<li>移除不必要的库,缩短总时间</li>
<li>只保存最终视频输出,不保存中间结果,提高性能</li>
<li>使用OpenCV生成最终视频,比mimwrite更快</li>
</ul>
<h2 id="Gradio"><a href="#Gradio" class="headerlink" title="Gradio"></a>Gradio</h2><p>Gradio是一个Python库,提供了一种简单的方式将机器学习模型作为交互式Web应用程序来部署。</p>
<p>对Linly-Talker而言,使用Gradio有两个主要目的:</p>
<ol>
<li><p><strong>可视化与演示</strong>:Gradio为模型提供一个简单的Web GUI,上传图片和文本后可以直观地看到结果。这是展示系统能力的有效方式。</p>
</li>
<li><p><strong>用户交互</strong>:Gradio的GUI可以作为前端,允许用户与Linly-Talker进行交互对话。用户可以上传自己的图片并输入问题,实时获取回答。这提供了更自然的语音交互方式。</p>
</li>
</ol>
<p>具体来说,我们在app.py中创建了一个Gradio的Interface,接收图片和文本输入,调用函数生成回应视频,在GUI中显示出来。这样就实现了浏览器交互而不需要编写复杂的前端。</p>
<p>总之,Gradio为Linly-Talker提供了可视化和用户交互的接口,是展示系统功能和让最终用户使用系统的有效途径。</p>
<h2 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h2><p>现在的启动一共有几种模式，可以选择特定的场景进行设置</p>
<p>第一种只有固定了人物问答，设置好了人物，省去了预处理时间</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">python app.py</span><br></pre></td></tr></tbody></table></figure>
<p><img src="https://pica.zhimg.com/80/v2-fc37a5490a674e2194b88714d38f986e.png" alt=""></p>
<p>第二种是可以任意上传图片进行对话</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">python app_img.py</span><br></pre></td></tr></tbody></table></figure>
<p><img src="https://pic1.zhimg.com/80/v2-7c863c3992beef67953d7ab378be99d9.png" alt=""></p>
<p>第三种是在第一种的基础上加入了大语言模型，加入了多轮的GPT对话</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">python app_multi.py</span><br></pre></td></tr></tbody></table></figure>
<p><img src="https://picx.zhimg.com/80/v2-802165f64f307dd204b04b9725626cd7.png" alt=""></p>
<p>文件夹结构如下</p>
<p>权重部分可以从这下载：<a href="https://pan.baidu.com/s/1eF13O-8wyw4B3MtesctQyg?pwd=linl">Baidu (百度云盘)</a> (Password: <code>linl</code>)</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">Linly-Talker/ </span><br><span class="line">├── app.py</span><br><span class="line">├── app_img.py</span><br><span class="line">├── utils.py</span><br><span class="line">├── Linly-api.py</span><br><span class="line">├── Linly-api-fast.py</span><br><span class="line">├── Linly-example.ipynb</span><br><span class="line">├── README.md</span><br><span class="line">├── README_zh.md</span><br><span class="line">├── request-Linly-api.py</span><br><span class="line">├── requirements_app.txt</span><br><span class="line">├── scripts</span><br><span class="line">│   └── download_models.sh</span><br><span class="line">├──	src</span><br><span class="line">│&nbsp;&nbsp; ├── audio2exp_models</span><br><span class="line">│&nbsp;&nbsp; ├── audio2pose_models</span><br><span class="line">│&nbsp;&nbsp; ├── config</span><br><span class="line">│&nbsp;&nbsp; ├── cost_time.py</span><br><span class="line">│&nbsp;&nbsp; ├── face3d</span><br><span class="line">│&nbsp;&nbsp; ├── facerender</span><br><span class="line">│&nbsp;&nbsp; ├── generate_batch.py</span><br><span class="line">│&nbsp;&nbsp; ├── generate_facerender_batch.py</span><br><span class="line">│&nbsp;&nbsp; ├── Record.py</span><br><span class="line">│&nbsp;&nbsp; ├── test_audio2coeff.py</span><br><span class="line">│&nbsp;&nbsp; └── utils</span><br><span class="line">├── inputs</span><br><span class="line">│   ├── example.png</span><br><span class="line">│   └── first_frame_dir</span><br><span class="line">│       ├── example_landmarks.txt</span><br><span class="line">│       ├── example.mat</span><br><span class="line">│       └── example.png</span><br><span class="line">├── examples</span><br><span class="line">│   └── source_image</span><br><span class="line">│       ├── art_0.png</span><br><span class="line">│       ├── ......</span><br><span class="line">│       └── sad.png</span><br><span class="line">├── TFG</span><br><span class="line">│&nbsp;&nbsp; ├── __init__.py</span><br><span class="line">│&nbsp;  ├── Wav2Lip.py</span><br><span class="line">│&nbsp;&nbsp; └── SadTalker.py</span><br><span class="line">└── TTS</span><br><span class="line">│&nbsp;&nbsp; ├── __init__.py</span><br><span class="line">│&nbsp;  ├── EdgeTTS.py</span><br><span class="line">│&nbsp;  └── TTS_app.py</span><br><span class="line">├── ASR</span><br><span class="line">│&nbsp;&nbsp; ├── __init__.py</span><br><span class="line">│&nbsp;&nbsp; ├── FunASR.py</span><br><span class="line">│&nbsp;&nbsp; └── Whisper.py</span><br><span class="line">├── LLM</span><br><span class="line">│&nbsp;&nbsp; ├── __init__.py</span><br><span class="line">│&nbsp;&nbsp; ├── Gemini.py</span><br><span class="line">│&nbsp;&nbsp; ├── Linly.py</span><br><span class="line">│&nbsp;&nbsp; └── Qwen.py</span><br><span class="line">....... // 以下是需要下载的权重路径（可选）</span><br><span class="line">├── checkpoints // SadTalker 权重路径</span><br><span class="line">│   ├── mapping_00109-model.pth.tar</span><br><span class="line">│   ├── mapping_00229-model.pth.tar</span><br><span class="line">│   ├── SadTalker_V0.0.2_256.safetensors</span><br><span class="line">│   └── SadTalker_V0.0.2_512.safetensors</span><br><span class="line">│   ├── lipsync_expert.pth</span><br><span class="line">│   ├── visual_quality_disc.pth</span><br><span class="line">│   ├── wav2lip_gan.pth</span><br><span class="line">│   └── wav2lip.pth // Wav2Lip 权重陆军</span><br><span class="line">├── gfpgan // GFPGAN 权重路径</span><br><span class="line">│   └── weights</span><br><span class="line">│       ├── alignment_WFLW_4HG.pth</span><br><span class="line">│       └── detection_Resnet50_Final.pth</span><br><span class="line">├── Linly-AI // Linly 权重路径</span><br><span class="line">│   └── Chinese-LLaMA-2-7B-hf </span><br><span class="line">│       ├── config.json</span><br><span class="line">│       ├── generation_config.json</span><br><span class="line">│       ├── pytorch_model-00001-of-00002.bin</span><br><span class="line">│       ├── pytorch_model-00002-of-00002.bin</span><br><span class="line">│       ├── pytorch_model.bin.index.json</span><br><span class="line">│       ├── README.md</span><br><span class="line">│       ├── special_tokens_map.json</span><br><span class="line">│       ├── tokenizer_config.json</span><br><span class="line">│       └── tokenizer.model</span><br><span class="line">├── Qwen // Qwen 权重路径</span><br><span class="line">│   └── Qwen-1_8B-Chat</span><br><span class="line">│       ├── cache_autogptq_cuda_256.cpp</span><br><span class="line">│       ├── cache_autogptq_cuda_kernel_256.cu</span><br><span class="line">│       ├── config.json</span><br><span class="line">│       ├── configuration_qwen.py</span><br><span class="line">│       ├── cpp_kernels.py</span><br><span class="line">│       ├── examples</span><br><span class="line">│       │   └── react_prompt.md</span><br><span class="line">│       ├── generation_config.json</span><br><span class="line">│       ├── LICENSE</span><br><span class="line">│       ├── model-00001-of-00002.safetensors</span><br><span class="line">│       ├── model-00002-of-00002.safetensors</span><br><span class="line">│       ├── modeling_qwen.py</span><br><span class="line">│       ├── model.safetensors.index.json</span><br><span class="line">│       ├── NOTICE</span><br><span class="line">│       ├── qwen_generation_utils.py</span><br><span class="line">│       ├── qwen.tiktoken</span><br><span class="line">│       ├── README.md</span><br><span class="line">│       ├── tokenization_qwen.py</span><br><span class="line">│       └── tokenizer_config.json</span><br></pre></td></tr></tbody></table></figure>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a href="https://github.com/openai/whisper">https://github.com/openai/whisper</a></li>
<li><a href="https://github.com/rany2/edge-tts">https://github.com/rany2/edge-tts</a>  </li>
<li><a href="https://github.com/CVI-SZU/Linly">https://github.com/CVI-SZU/Linly</a></li>
<li><a href="https://github.com/QwenLM/Qwen">https://github.com/QwenLM/Qwen</a></li>
<li><a href="https://deepmind.google/technologies/gemini/">https://deepmind.google/technologies/gemini/</a></li>
<li><a href="https://github.com/OpenTalker/SadTalker">https://github.com/OpenTalker/SadTalker</a></li>
</ul>
<h2 id="Star-History"><a href="#Star-History" class="headerlink" title="Star History"></a>Star History</h2><p><a href="https://star-history.com/#Kedreamix/Linly-Talker&amp;Date"><img src="https://api.star-history.com/svg?repos=Kedreamix/Linly-Talker&amp;type=Date" alt="Star History Chart"></a></p>
]]></content>
      <categories>
        <category>Project</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>Talking Head Generation</tag>
      </tags>
  </entry>
  <entry>
    <title>3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</title>
    <url>/2024/01/25/Paper/3DGS%20Survey/</url>
    <content><![CDATA[<p>今天想介绍的是<code>ZJU</code>带来的<code>3DGS</code>的首篇综述<code>A Survey on 3D Gaussian Splatting</code> 这是论文链接 <a href="https://arxiv.org/abs/2401.03890">arXiv:2401.03890</a>，结合一些资料，趁这个机会好好学习一下3DGS，加油入坑！！！</p>
<p>首先说一些自己的理解，3DGS之所以爆火，很大程度在于他的实时性，而这一部分极大程度得益于他定制的算法与自定义 CUDA 内核。除此之外，<strong>Gaussian Splatting</strong>根本不涉及任何神经网络，甚至没有一个小型的 MLP，也没有什么 “神经”的东西，场景本质上只是空间中的一组点。在大家都在研究数十亿个参数组成的模型的人工智能世界里，这种方法越来越受欢迎，令人耳目一新。它的想法源于 “Surface splatting”（2001 年），说明经典的计算机视觉方法仍然可以激发相关的解决方案。它简单明了的表述方式使<strong>Gaussian Splatting</strong>特别容易解释，这也是为什么在某些应用中选择它而不是 NeRFs。</p>
<h2 id="引言-INTRODUCTION"><a href="#引言-INTRODUCTION" class="headerlink" title="引言 INTRODUCTION"></a>引言 INTRODUCTION</h2><p>NeRF自从2020年开始，在多视角合成中做出来巨大的贡献，他利用神经网络，实现了空间坐标到颜色和密度的映射的，然NeRF的方法是计算密集型的，通常需要大量的训练时间和大量的渲染资源，特别是高分辨率的输出。</p>
<p><img src="https://pic1.zhimg.com/80/v2-c828848317a156fc6dd17c9a5310dd03.png" alt="NeRF"></p>
<p>针对这些问题，3DGS出现了，3DGS 采用显式表示和高度并行的工作流程，有利于更高效的计算和渲染，其创新在于其独特地融合了可微分管道和基于点的渲染技术的优点，通过用可学习的 3D 高斯函数表示场景，保留了连续体积辐射场的理想特性，这对于高质量图像合成至关重要，同时避免了与空白空间渲染相关的计算开销，这是传统 NeRF 方法的常见缺点，而3DGS很好的解决了这个问题，在不影响视觉质量的情况下达到了实时渲染。</p>
<p>论文中也发现，自3DGS出现以来，2023年有很多的论文在arXiv中挂出来，所以基于此也写了这样一个综述，同时促进3DGS领域的进一步研究和创新</p>
<p><img src="https://picx.zhimg.com/80/v2-167cd8779af5c5550c15156e2b9b52c0.png" alt="The number of papers on 3DGS is increasing every month."></p>
<p>以下是论文架构的图，论文的大概架构如下所示，可以看到这篇综述撰写的一个逻辑，还是非常好的，接下来，我会顺着这个架构进行解读论文来学习</p>
<ul>
<li>第2部分：主要是一些问题描述和相关研究领域的一些简要的背景</li>
<li>第3部分：介绍3DGS，包括3DGS的多视角的合成和3DGS的优化</li>
<li>第4部分：3DGS 产生重大影响的各种应用领域和任务，展示了其多功能性</li>
<li>第5部分：对3DGS进行了一些比较和分析</li>
<li>第6、7部分：对一些未来的开放性工作进行总结和调查</li>
</ul>
<p><img src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="Structure of the overall review."></p>
<h2 id="背景-BACKGROUND"><a href="#背景-BACKGROUND" class="headerlink" title="背景 BACKGROUND"></a>背景 BACKGROUND</h2><p>背景主要分两部分讲解</p>
<ul>
<li>辐射场的概念：隐式和显式</li>
<li>有关辐射场的场景重建、渲染等领域相关介绍</li>
</ul>
<h3 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h3><h4 id="辐射场"><a href="#辐射场" class="headerlink" title="辐射场"></a>辐射场</h4><p>辐射场是实际上是对三维空间中光分布的表示，它捕捉了光与环境中的表面和材质相互作用的方式。从数学上来说，辐射场可被描述为一个函数$L:\mathbb{R}^5\to\mathbb{R}^+$, 其中$L(x,y,z,\theta,\psi)$将点$(x,y,z)$和球坐标下的方向$(\theta,\phi)$映射为非负的辐射值。辐射场有显示表达和隐式表达，可用于场景表示和渲染。</p>
<h4 id="隐式辐射场"><a href="#隐式辐射场" class="headerlink" title="隐式辐射场"></a>隐式辐射场</h4><p>隐式辐射场是辐射场中的一种，在表示场景中的光分布时，不需显式定义场景的集合形状。这里面最常见的就是NeRF，使用神经网络来学习连续的体积表示。在NeRF中，使用MLP 网络用于将一组空间坐标 $(x, y, z)$ 和观察方向 $(\theta,\phi)$ 映射到颜色和密度值。任何点处的辐射不是显式存储的，而是通过查询神经网络实时计算得出。因此，该函数可以写成：</p>
<script type="math/tex; mode=display">
L_\text{implicit}(x,y,z,\theta,\phi)=\text{NeuralNetwork}(x,y,z,\theta,\phi)</script><p>这种方式的好处是构建了一个可微且紧凑的复杂场景，但是由于我们总是需要对光线进行采样和体渲染的计算，会导致计算负载比较高。</p>
<h4 id="显式辐射场"><a href="#显式辐射场" class="headerlink" title="显式辐射场"></a>显式辐射场</h4><p>与隐式不同的是，显示是直接表示光在离散空间结构中的分布，比如体素网格或点云。该结构中的每个元素都存储了其在空间中相应位置的辐射信息，而不是像NeRF一样去执行查询的操作，所以他会更直接也更快的得到每个值，但是同时也需要更大内存使用和导致较低的分辨率。通常我们可以表示为：</p>
<script type="math/tex; mode=display">
L_\text{explicit}{ ( x , y , z , \theta , \phi ) }=\text{DataStructure}[(x,y,z)]\cdot f(\theta,\phi)</script><p>其中，<code>DataStructure</code>可以是网格或点云，而$f(θ, ϕ)$是一个根据观察视线方向修改辐射的函数。</p>
<h4 id="3D-Gaussian-Splatting-（两全其美）"><a href="#3D-Gaussian-Splatting-（两全其美）" class="headerlink" title="3D Gaussian Splatting （两全其美）"></a>3D Gaussian Splatting （两全其美）</h4><p>3DGS通过利用3D 高斯函数作为其表示形式，充分利用了显示辐射场和隐式辐射场的优势。这些高斯函数被优化用于准确表示场景，结合了基于神经网络的优化和显式结构化数据存储的优点。这种混合方法能进行高质量渲染，同时具有更快的训练和实时性能，3D高斯表达可表示为：</p>
<script type="math/tex; mode=display">
L_{\mathrm{3DGS}}(x,y,z,\theta,\phi)=\sum_{i}G(x,y,z,\mu_{i},\Sigma_{i})\cdot c_{i}(\theta,\phi)</script><p>其中 $G$ 是具有平均值 $μ_i$ 和协方差 $Σ_i$ 的高斯函数，$c$ 表示与视图相关的颜色。</p>
<h4 id="显式与隐式的理解"><a href="#显式与隐式的理解" class="headerlink" title="显式与隐式的理解"></a>显式与隐式的理解</h4><p>这里放一张理解显示隐式图像的图片，我还是觉得相当不错的</p>
<p><img src="https://pic1.zhimg.com/80/v2-e79d0183806753d34863598e544a0517.jpeg" alt="显式隐式表达"></p>
<h3 id="背景和术语"><a href="#背景和术语" class="headerlink" title="背景和术语"></a>背景和术语</h3><p>许多技术和研究学科与 <code>3DGS</code> 有着密切的关系，以下各节将对此进行简要介绍。</p>
<h4 id="场景重建与渲染"><a href="#场景重建与渲染" class="headerlink" title="场景重建与渲染"></a>场景重建与渲染</h4><p><strong>场景重建</strong>：从一组图像集合或其它数据建立场景的三维模型。</p>
<p><strong>渲染</strong>：将计算机可读取的信息（如场景中的3D物体）转化为图像。<br>早期技术基于光场生成逼真的图像，运动结构（SfM）与多视图立体匹配（MVS）算法通过从图像序列估计3D结构来增强光场。</p>
<h4 id="神经渲染和辐射场"><a href="#神经渲染和辐射场" class="headerlink" title="神经渲染和辐射场"></a>神经渲染和辐射场</h4><p><strong>神经渲染</strong>：将深度学习与传统图形技术结合生成逼真的图像。早期方法使用CNN估计混合权重或纹理空间解决方案。</p>
<p><strong>辐射场</strong>：一种函数表达，描述从各方向穿过空间各点的光的量。NeRF使用神经网络建模辐射场。</p>
<h4 id="体积表示和光线行进"><a href="#体积表示和光线行进" class="headerlink" title="体积表示和光线行进"></a>体积表示和光线行进</h4><p><strong>体积表达</strong>：不仅将物体和场景建模为表面，还将其其建模为充满材料或空白空间的体积。这样可以对如雾、烟或半透明材料进行更精确的渲染。</p>
<p><strong>光线行进</strong>：是体积表达渲染图像的技术，通过增量跟踪穿过“体”的光线来渲染图像。NeRF引入重要性采样和位置编码增强合成图像的质量，虽然能得到高质量的图像，但这一方法计算量大。</p>
<h4 id="基于点的渲染"><a href="#基于点的渲染" class="headerlink" title="基于点的渲染"></a>基于点的渲染</h4><p>基于点的渲染是一种使用点而非传统多边形来可视化3D场景的技术。该方法特别适用于渲染复杂、非结构化或稀疏的几何数据。点可以通过添加额外属性，如可学习的神经描述符来进行增强，并且可以高效地进行渲染，但这种方法可能会出现渲染中的空洞或混叠效应等问题。3DGS通过使用各向异性高斯进行更连贯的场景表达。</p>
<h2 id="用于显式辐射场的3DGS"><a href="#用于显式辐射场的3DGS" class="headerlink" title="用于显式辐射场的3DGS"></a>用于显式辐射场的3DGS</h2><p>3DGS能够实时渲染高分辨率的图像，并且不需要神经网络，是一个突破。</p>
<p>这一块主要围绕两块进行讲解</p>
<ul>
<li>3DGS的前向过程</li>
<li>3DGS的优化过程</li>
</ul>
<h3 id="学习3D高斯函数进行新视角合成"><a href="#学习3D高斯函数进行新视角合成" class="headerlink" title="学习3D高斯函数进行新视角合成"></a>学习3D高斯函数进行新视角合成</h3><p>假如现在有一个场景，目的是生成特定视角下的相机图像。NeRF对每一个像素使用光线行进和采样点，影响其实时性；而3DGS将3D高斯投影到图像平面，称为“泼溅”，如下图所示。然后对高斯进行排序并计算每个像素的值。NeRF和3DGS的渲染可视为互逆关系。</p>
<p><img src="https://pic1.zhimg.com/80/v2-9d5fff5c2390526cd03e5a14fd13f4fe.png" alt="3DGS的Splatting 泼溅"></p>
<p>这里面有个点很有意思，为什么说是互逆关系，我参考了知乎的一篇文章<a href="https://zhuanlan.zhihu.com/p/666465701">3D Gaussian Splatting中的数学推导</a>的说明，我觉得这个说的还不错。</p>
<blockquote>
<p> 首先，我们回忆一下体渲染的这个事情。假设读者跟我一样是从NeRF才接触体渲染的，那么回顾一下NeRF中，沿着一个像素，发出一条射线，然后这条射线“射向体数据”（在NeRF里就是沿着光线进行采样，然后查询采样点的属性）的过程。这个过程可以归结为一种<code>backward mapping</code>。</p>
<p> 所以很自然的，会有一种<code>forward mapping</code>的办法。形式上，就是将整个“体数据”投影到此时位姿所对应的图像平面。这种办法的前提就不能是用NeRF那种隐式表达了，需要一些显式的表达才能支持这样直接的投影。例如以三个顶点长成的三角面基元（primitive），然后将这些许多的三角面直接投影到成像平面上，判断哪些像素是什么颜色，当有多个三角形投影时，根据他们的“深度”来判断前后顺序，然后进行熟悉的alpha compositing。当然也会有其他基元，例如小的平面表示等等。</p>
<p> 无论是<code>backward mapping</code>还是<code>forward mapping</code>，这个过程都涉及到将连续的表示变成离散的。在<code>backward mapping</code>里，是对场进行采样；在<code>forward mapping</code>里，是需要直接生成出基元，这也是一种连续化为离散。为了理解在这个过程中，高斯分布为什么重要，我们需要牵扯到信号与系统中的概念。与混过数字信号处理考试不同的是，我们要清楚此时引入信号与系统里的工具的目的是什么。回想刚才三角面基元的情景，在实际情境中，我们其实都接触不到“连续”的表达，比如三角面，我们只会记录它的三个顶点。当投影完成后，我们只能做一些有限的操作来阻止“锯齿”，例如对结果进行一个模糊操作，这些操作一般都是局部的。我们这样做的目的，本质是“希望用离散的表达来重建原来的信号，进一步在重建好的信号上进行“resampling”。如果我们对处理后的结果，视觉上看起来没什么混叠或者锯齿上的问题，那就说明我们“resampling”是成功的。</p>
</blockquote>
<p>从下图也可以看到NeRF和Gaussian在概念上的区别，左边是NeRF沿着光线查询连续 MLP，右边是Gaussian一组与给定光线相关的离散的高斯分布</p>
<p><img src="https://picx.zhimg.com/80/v2-08473faff1a084b3de92e2a86f69f0fd.png" alt=""></p>
<p><img src="https://picx.zhimg.com/80/v2-37166011e5e81d299598141028acff42.png" alt="difference between NeRF and Gaussian Splatting"></p>
<p>首先简单介绍一下，3DGS是如何表示真实场景的，前面也有提过，在<strong>Gaussian Splatting</strong>中，3D世界用一组3D点表示，实际上是数百万个，大致在0.5到5百万之间。每个点是一个3D高斯，具有其独特的参数，这些参数是为每个场景拟合的，以便该场景的渲染与已知数据集图像紧密匹配，接下来就介绍他的属性。</p>
<p><img src="https://pica.zhimg.com/80/v2-f440b37ac00a08977b2b6e5514ffec1f.png" alt="Representing a 3D world"></p>
<ul>
<li><p><strong>3D高斯的属性</strong>： 一个3D高斯主要包括，中心（位置）$x,y,z$的均值$μ$、不透明度 $α$、3D 协方差矩阵 $Σ$ 和颜色 $c$（一般是RGB或者是球谐（SH）系数）。 其中$c$与视角有关，$c$ 由球谐函数表示。所有属性均可学习，都可以通过反向传播来学习和优化。</p>
</li>
<li><p><strong>视域剔除</strong>：给定特定的相机姿态，该步骤会判断哪些高斯位于相机的视锥外，并在后续步骤中剔除之，以节省计算。</p>
</li>
<li><p><strong>Splatting泼溅</strong>：实际上只是3D高斯（椭圆体）投影到2D图像空间（椭圆）中进行渲染。给定视图变换 $W$ 和3D协方差矩阵$\Sigma$，我们可以使用使用以下公式计算投影 2D 协方差矩阵 $\Sigma^{\prime}$</p>
<script type="math/tex; mode=display">
\Sigma^{\prime}=JW\Sigma W^\top J^\top</script><p>其中 $J$ 为投影变换中仿射近似的雅可比矩阵。</p>
</li>
<li><p><strong>像素渲染</strong>：如果不考虑并行，采用最简单的方式：给定像素 $x$ 的位置，与其到所有重叠高斯函数的距离，即这些高斯函数的深度。这些可以通过观察变换 $W$ 计算出来，形成高斯函数的排序列表$N$。然后进行alpha混合，计算该像素的最终颜色：</p>
<script type="math/tex; mode=display">
C=\sum_{i\in\mathcal{N}}c_i\alpha_i^{\prime}\prod_{j=1}^{i-1}\left(1-\alpha_j^{\prime}\right.)</script><p>其中 $c_i$ 是学习到的颜色，最终的不透明度 $\alpha_i^{\prime}$ 是学习的不透明度 $\alpha_i$ 与高斯的乘积:</p>
<script type="math/tex; mode=display">
\alpha_i'=\alpha_i\times\exp\left(-\frac12(x'-\mu_i')^\top\Sigma_i'^{-1}(x'-\mu_i')\right)</script></li>
</ul>
<p>  其中 $x’$ 和 $μ’_i$ 是投影空间中的坐标，同时我也找了个gif来可视化了一下Gaussian Splatting对位置p的影响：</p>
<p>  <img src="/img/3dgs.gif" alt="3DGS"></p>
<p>  如果仔细看的话，我们会发现，实际上这个公式和<a href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution">多变量正态分布的概率密度函数</a>十分相像，是忽略了带有协方差行列式的标准化项，而是用不透明度来加权。</p>
<script type="math/tex; mode=display">
  (2\pi)^{-k/2}\det(\boldsymbol{\Sigma})^{-1/2}\exp\biggl(-\frac12(\mathbf{x}-\boldsymbol{\mu})^\mathrm{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\biggr)</script><p>  不过如果考虑并行的话加快速度，这种列表排序实际上很难并行化，所以很有可能这个渲染程度比NeRF还慢。为了实现实时渲染，3DGS也做了一个tradeoff，3DGS做出了一些让步来适应<strong>并行计算</strong>。</p>
<p>  <img src="https://picx.zhimg.com/80/v2-7cea6c4b183982cd921c0456d1f689b7.png" alt="Tiles(Patches)"></p>
<ul>
<li><p><strong>Tiles (Patches)</strong>：为避免逐像素计算出现的成本，3DGS改为<strong>patch</strong>级别的渲染。具体来说，首先将图像分割为多个不重叠的patch，称为<code>tile</code>，每个图块包含 16×16 像素，如下图所示。3DGS然后确定<code>tile</code>与投影高斯的相交情况，由于投影高斯可能会与多个<code>tile</code>相交，需要进行复制，并为每个复制体分配相关tile的标识符（如<code>tile</code>的ID）。(不用判断每个像素与高斯的距离，而是判断tile就简单多了)</p>
<p><img src="https://picx.zhimg.com/80/v2-c81242a6677621910801fcec4c0adbee.png" alt=""></p>
<p>从下图可以看到排序的结果，在排序中，高位是tile的ID，低位就是深度，一起进行排序，下面的图是AI葵视频的结果，还是很好理解的</p>
<p><img src="https://pic1.zhimg.com/80/v2-5c74958d484c1d2588c20c8c30b58411.png" alt="3DGS排序"></p>
<p><img src="https://picx.zhimg.com/80/v2-3d6e3aec3a86c1d94354458830dbf17f.png" alt="3DGS排序例子(AI葵)"></p>
</li>
<li><p><strong>并行渲染</strong>：复制后，3DGS（对应字节的无序列表）结合包含了相关的tile ID（对应字节的高位）和深度信息（对应字节的低位），如上图所示。由于每一块和每一像素的计算是独立的，所以可以基于CUDA编程的块和线程来实现并行计算，同时有利于访问公共共享内存并保持统一的读取顺序。排序后的列表可直接用于渲染（alpha混合），如下图所示。</p>
<p><img src="https://pic1.zhimg.com/80/v2-6393ea51f715d0d0baa880cd1890a549.png" alt="并行渲染"></p>
<p>总的来说，3DGS在前向过程中做出了一些近似计算，以提高计算效率并保留图像合成的高质量。</p>
</li>
</ul>
<h3 id="3DGS的优化"><a href="#3DGS的优化" class="headerlink" title="3DGS的优化"></a>3DGS的优化</h3><p>学习到这里，我们可能会有一个问题，怎么可能在空间中的一堆圆球中得到一个像样的图像的，确实是这样，如果没有进行优化，在渲染的时候就会出现很多伪影，从下图你可以看到。</p>
<p><img src="https://pic1.zhimg.com/80/v2-7ad69d962fb9a18d84747130af62fe15.png" alt="An example of renders of an under-optimized scene"></p>
<p>3DGS的核心是<strong>3D高斯集合的优化过程</strong>。一方面需要通过可微渲染来使高斯符合场景纹理，另一方面表达场景需要的高斯数量是未知的。这分别对应参数优化与密度控制两步，这两步在优化过程中交替进行。优化过程中，需要手动设置很多超参数。</p>
<h4 id="参数优化-Parameter-Optimization"><a href="#参数优化-Parameter-Optimization" class="headerlink" title="参数优化 Parameter Optimization"></a>参数优化 Parameter Optimization</h4><ul>
<li><p><strong>损失函数</strong>：图像合成后，计算渲染图像与真实图像的差异作为损失：</p>
<script type="math/tex; mode=display">
\mathcal{L}=(1-\lambda)\mathcal{L}_1+\lambda\mathcal{L}_{D-SSIM}</script><p>其中 $λ$ 是权重因子。与 NeRF 的损失函数略有不同，由于光线行进成本高昂，NeRF 通常在像素级别而不是图像级别进行计算，而3DGS是图像级别的。</p>
</li>
<li><p><strong>参数更新</strong>：3D高斯的多数参数可通过反向传播直接更新，但对于协方差矩阵 $\Sigma$来说，需要半正定矩阵（这里面是一个定义，应该是多元正态分布的协方差矩阵是一个半正定矩阵），直接优化可能会产生非半正定矩阵，而只有半正定矩阵才有物理意义。因此，改为优化四元数$q$和3D向量$s$。将协方差矩阵分解：</p>
<script type="math/tex; mode=display">
\Sigma=RSS^\top R^\top</script><p>其中$R$与$S$分别由$q$和$s$推导得到的旋转和缩放矩阵。</p>
<ul>
<li>$S$是一个对角缩放矩阵，含有3个参数</li>
<li>$R$是一个3x3的旋转矩阵，通过旋转四元数来表示</li>
</ul>
<p>对于不透明度$α$, 其计算图较为复杂：$(q,s)\to\Sigma\to\Sigma^{\prime}\to\alpha$。为避免自动微分的计算消耗，3DGS还推导了$q$与$s$的梯度，在优化过程中直接计算之。</p>
</li>
</ul>
<h4 id="密度控制-Density-Control"><a href="#密度控制-Density-Control" class="headerlink" title="密度控制 Density Control"></a>密度控制 Density Control</h4><ul>
<li><strong>初始化</strong>：3DGS建议从SfM产生的稀疏点云初始化或随机初始化高斯，可以直接调用 <a href="https://colmap.github.io/">COLMAP</a> 库来完成这一步。。然后进行点的密集化和剪枝以控制3D高斯的密度。当由于某种原因无法获得点云时，可以使用随机初始化来代替，但可能会降低最终的重建质量。</li>
</ul>
<p><img src="https://picx.zhimg.com/80/v2-0d67e5748993593a04ed46f7519e972e_720w.png" alt="A sparse 3D point cloud produced by SfM, means initialization"></p>
<ul>
<li><p><strong>点密集化</strong>：在点密集化阶段，3DGS自适应地增加高斯的密度，以更好地捕捉场景的细节。该过程特别关注缺失几何特征或高斯过于分散的区域。密集化在一定数量的迭代后执行，比如100个迭代，针对在视图空间中具有较大位置梯度（即超过特定阈值）的高斯。其包括在未充分重建的区域克隆小高斯或在过度重建的区域分裂大高斯。对于克隆，创建高斯的复制体并朝着位置梯度移动。对于分裂，用两个较小的高斯替换一个大高斯，按照特定因子减小它们的尺度。这一步旨在在3D空间中寻求高斯的最佳分布和表示，增强重建的整体质量。</p>
<p>这一部分的意义是什么呢，因为SGD只能对现有点进行调整，但是在完全没有点或点太多的区域，很难找到好的参数，所以这就是点密集化的作用。</p>
</li>
<li><p><strong>点的剪枝</strong>：点的剪枝阶段移除冗余或影响较小的高斯，可以在某种程度上看作是一种正则化过程。一般消除几乎是透明的高斯（α低于指定阈值）和在世界空间或视图空间中过大的高斯。此外，为防止输入相机附近的高斯密度不合理地增加，这些高斯会在固定次数的迭代后将$\alpha$设置为接近0的值。该步骤在保证高斯的精度和有效性的情况下，能节约计算资源。</p>
</li>
</ul>
<p><img src="https://picx.zhimg.com/80/v2-58c80507588563289c26e2ea4066ad81.png" alt="Adaptive Gaussian densification scheme."></p>
<h3 id="用SH系数来表示颜色"><a href="#用SH系数来表示颜色" class="headerlink" title="用SH系数来表示颜色"></a>用SH系数来表示颜色</h3><p>在计算机图形学中，用球谐函数（Spherical Harmonics，简称SH）表示视角相关的颜色起着重要作用，最初是在Plenoxels中提出的。他能表示非兰伯特效应，比如金属表面的高光反射。不过这样也不是一定的，实际上也可以使用3个RGB值表示颜色，然后使用Gaussian Splatting。</p>
<p> 图形学全局环境光照技术与球谐函数息息相关，我们的环境光来源四面八方，可以理解为一个球面函数，当模拟漫反射环境光，我们用一张环境贴图进行采样，对每一个点进行半球采样出在这个像素上的颜色，<strong>球谐光照</strong>简单来说就是用几个系数存取了整张环境贴图包围在球上<strong>法线方向</strong>所对应的的颜色信息。在渲染过程中传入球谐系数。在模型上根据对应的法线信息，从球谐函数中获取对应的颜色信息。</p>
<p>球谐函数是定义在球面上的特殊函数，换句话说，可以对球面上的任意点计算这样一个函数并得到一个值。</p>
<p>这里我们简单理解一下，SH，球谐函数，归根到底只是一组基函数，至于这组基函数是怎么来的，不管他。简单点来说，每一个函数都可以由多个基函数组合起来，如果我们有很多基函数，我们可以通过对应的权重系数复原出原来的函数，不过本质上还是一个有损压缩，不一定那么准确，不过如果基函数越多，复原的函数越准确，但是计算量也变大了。</p>
<p>在球面基函数中，最多的就是球谐函数了。球谐函数有很多很好的性质，比如正交性，旋转不变性（这边就不介绍了）。正交性说明每个基函数都是独立的，每个基函数都不能用别的基函数加权得到。当SH的系数用的越多，那么表达能力就越强，跟原始的函数就越接近。（如果更详细的了解可以看看一些原理，我主要是宏观的了解SH是什么，简单理解就是他是一种颜色的表示）</p>
<p><img src="https://pic1.zhimg.com/80/v2-9e660f32e92e1897aa986b0ab2ce073e.png" alt=""></p>
<p>当用来描述不同方向光照的SH基函数，我们一般用二阶或者三阶，比如下面的例子就是3阶的</p>
<p><img src="https://picx.zhimg.com/80/v2-f6bfb715b846bf13c95013ca96c1d51d.png" alt=""></p>
<p>下面展示的是一个$l=2$和3阶的球谐函数，一共包括9个学习系数，我们可以根据点的视角得到相关颜色，可以看到最后是red红色分量。</p>
<p><img src="https://pica.zhimg.com/80/v2-8241e4f7092a89a158df31b8cde94d33.png" alt="得到l=2和9个学习系数的点的视角相关颜色（红色分量）的过程"></p>
<h3 id="3DGS-流程"><a href="#3DGS-流程" class="headerlink" title="3DGS 流程"></a>3DGS 流程</h3><p>最后根据论文的图来总结一下3DGS的流程</p>
<p><img src="https://pic1.zhimg.com/80/v2-fda180df51e9171e3e147f5b40e520b9.png" alt="3DGS 流程"></p>
<ol>
<li><p><strong>Structure from Motion</strong>：使用SfM从一组图像中估计出点云，可以直接调用 <a href="https://colmap.github.io/">COLMAP</a>  库操作</p>
<p><img src="https://picx.zhimg.com/80/v2-961548f1a56fb5bc81bc8b349472d8ab.png" alt="Structure from Motion"></p>
</li>
</ol>
<ol>
<li><p><strong>Convert to Gaussians</strong>：将每个点建模成一个 3D 高斯图像。从 SfM 数据中，我们能推断出每个高斯图像的位置和颜色。但如果是要得到更高质量的表征的话，还需要对每个高斯函数进行训练，以推断出更精细的位置和颜色，并推断出协方差和透明度。</p>
</li>
<li><p><strong>Training</strong>：与神经网络类似，我们使用随机梯度下降法进行训练，但这里没有神经网络的层的概念 (都是 3D 高斯函数)。</p>
<p>训练步骤如下:</p>
<ol>
<li>用当前所有可微高斯函数渲染出图像</li>
<li>根据渲染图像和真实图像之间的差异计算损失</li>
<li>根据损失调整每个高斯图像的参数</li>
<li>根据情况对当前相关高斯图像进行点的密度控制</li>
</ol>
<p>步骤 1-3 比较简单，下面我们稍微解释一下第 4 步的工作:</p>
<ul>
<li>如果某高斯图像的梯度很大 (即它错得比较离谱)，则对其进行分裂或克隆<ul>
<li>如果该高斯图像很小，则克隆它</li>
<li>如果该高斯图像很大，则将其分裂</li>
</ul>
</li>
<li>如果该高斯图像的 alpha 太低，则将其删除</li>
</ul>
<p>这么做能帮助高斯图像更好地拟合精细的细节，同时修剪掉不必要的高斯图像。</p>
</li>
<li><p><strong>Differentiable Gaussian Rasterization</strong>：3D Gaussian Splatting实际上是一种光栅化的方法，将数据成像到屏幕上，与其他方法相比，他有两个特点</p>
<ol>
<li>快</li>
<li>可微</li>
</ol>
<p>主要步骤如下：</p>
<ol>
<li>针对给定相机视角，把每个 3D 高斯投影到 2D。</li>
<li>按深度对高斯进行排序。</li>
<li>对每个像素，从前到后计算每个高斯在该像素点的值，并将所有值混合以得到最终像素值。</li>
</ol>
</li>
</ol>
<h3 id="3DGS-Limitations"><a href="#3DGS-Limitations" class="headerlink" title="3DGS Limitations"></a>3DGS Limitations</h3><p><strong>优点</strong></p>
<ol>
<li>高品质、逼真的场景</li>
<li>快速、实时的渲染</li>
<li>更快的训练速度</li>
</ol>
<p><strong>缺点</strong></p>
<ol>
<li>防止模型优化中的“破碎”的高斯：点太大、太长、冗余等</li>
<li>更高的显存使用率 (4GB 用于显示，12GB 用于训练)</li>
<li>更大的磁盘占用 (每场景 1GB+)</li>
<li>与现有渲染管线不兼容</li>
<li><del>只能重建静态场景（但是好像现在动态的Gaussian也出来了，所以这个不算缺点了）</del></li>
</ol>
<h2 id="应用领域和任务-APPLICATION-AREAS-AND-TASKS"><a href="#应用领域和任务-APPLICATION-AREAS-AND-TASKS" class="headerlink" title="应用领域和任务 APPLICATION AREAS AND TASKS"></a>应用领域和任务 APPLICATION AREAS AND TASKS</h2><h3 id="同时定位和建图（SLAM）"><a href="#同时定位和建图（SLAM）" class="headerlink" title="同时定位和建图（SLAM）"></a>同时定位和建图（SLAM）</h3><p>SLAM需要让设备实时理解自身位置并同时为环境建图，因此计算量大的表达技术难以应用。</p>
<p>传统SLAM使用点/surfel云或体素网格表达环境。3DGS的优势在于高效性（自适应控制高斯密度）、精确性（各向异性高斯能建模环境细节）、适应性（能用于各种尺度和复杂度的环境）。</p>
<h3 id="动态场景建模"><a href="#动态场景建模" class="headerlink" title="动态场景建模"></a>动态场景建模</h3><p>动态场景建模需要捕捉和表达场景随时间变化的的3D结构和外观。需要建立能精确反映场景中物体几何、运动和视觉方面的数字模型。4D高斯泼溅通过扩展3D高斯溅射的概念，引入时间维度，使得可以表达和渲染动态场景。现在也有一些方法在研究在动态场景中的一些编辑的功能，与3DGS进行交互。</p>
<h3 id="AI生成内容（AIGC）"><a href="#AI生成内容（AIGC）" class="headerlink" title="AI生成内容（AIGC）"></a>AI生成内容（AIGC）</h3><p>AIGC是人工智能自动创建或极大修改的数字内容，可以模仿、扩展或增强人类生成的内容。</p>
<p>3DGS的显式特性、实时渲染能力和可编辑水平使其与AIGC高度相关。例如，有方法使用3DGS与生成模型、化身或场景编辑结合，如3DGS-Avatar。</p>
<h3 id="自动驾驶"><a href="#自动驾驶" class="headerlink" title="自动驾驶"></a>自动驾驶</h3><p>自动驾驶的目标是在无人干涉的情况下导航并操作车辆，其主要目标是安全而高效地感知环境、做出决策和操作执行器。</p>
<p>其中，感知和理解环境需要实时重建驾驶场景，精确识别静态和动态物体，并理解其相互关系和运动。动态驾驶场景中，场景还会随时间连续变化。3DGS可以通过混合数据点（如激光雷达点）将场景重建为连贯表达，有利于处理数据点变化的密度，以及静态背景和动态物体的精确重建。</p>
<h2 id="性能比较-PERFORMANCE-COMPARISON"><a href="#性能比较-PERFORMANCE-COMPARISON" class="headerlink" title="性能比较 PERFORMANCE COMPARISON"></a>性能比较 PERFORMANCE COMPARISON</h2><p>在这一部分，针对3FGS在上述的领域上的一些性能评估。</p>
<h3 id="性能基准：定位"><a href="#性能基准：定位" class="headerlink" title="性能基准：定位"></a>性能基准：定位</h3><ul>
<li><p>数据集：Replica。</p>
</li>
<li><p>基准算法：Gaussian-SLAM、GS-SLAM、SplaTAM、GSS-SLAM。</p>
</li>
<li>评估指标：均方根误差（RMSE）、绝对轨迹误差（ATE），测量传感器运动轨迹上真实位置与估计位置欧式距离的均方根。</li>
<li>结果：基于3D高斯的SLAM方法能超过基于NeRF的密集视觉SLAM。</li>
</ul>
<p><img src="https://picx.zhimg.com/80/v2-3277500ac5a850accdd2891db0595ae6.png" alt=""></p>
<h3 id="性能基准：静态场景渲染"><a href="#性能基准：静态场景渲染" class="headerlink" title="性能基准：静态场景渲染"></a>性能基准：静态场景渲染</h3><ul>
<li>数据集：Replica。</li>
<li>基准算法：Gaussian-SLAM、GS-SLAM、SplaTAM、GSS-SLAM。</li>
<li>评估指标：峰值信噪比(PSNR)、结构相似性(SSIM)、学习的感知图像patch相似性(LPIPS),衡量RGB渲染性能。</li>
<li>结果：基于3D高斯的方法能超过基于<strong>NeRF</strong>的方法。</li>
</ul>
<p><img src="https://picx.zhimg.com/80/v2-94d0eea6ab0c03f32c82802f00a8102d.png" alt=""></p>
<h3 id="性能基准：动态场景渲染"><a href="#性能基准：动态场景渲染" class="headerlink" title="性能基准：动态场景渲染"></a>性能基准：动态场景渲染</h3><ul>
<li>数据集：D-NeRF。</li>
<li>基准算法：CoGS、4D-GS、GauFRe、4DGS。</li>
<li>评估指标：PSNR、SSIM、LPIPS, 用于衡量RGB渲染性能。</li>
<li>结果：3DGS能大幅超过基于NeRF的SOTA。但静态版本的3DGS对动态场景的重建是失败的。</li>
</ul>
<p><img src="https://picx.zhimg.com/80/v2-288ca353912cbbd221a111ee553ab607_720w.png" alt=""></p>
<h3 id="性能基准：驾驶场景渲染"><a href="#性能基准：驾驶场景渲染" class="headerlink" title="性能基准：驾驶场景渲染"></a>性能基准：驾驶场景渲染</h3><ul>
<li>数据集：nuScences。</li>
<li>基准算法：DrivingGaussian。</li>
<li>评估指标：PSNR、SSIM、LPIPS*（LPIPS× 1000）, 用于衡量RGB渲染性能。</li>
<li>结果：3DGS方法能大幅超过基于NeRF的方法。</li>
</ul>
<p><img src="https://pic1.zhimg.com/80/v2-8b7bb910fbb86e3d1b23fc062260dc5d_720w.png" alt=""></p>
<h3 id="性能基准：数字虚拟人"><a href="#性能基准：数字虚拟人" class="headerlink" title="性能基准：数字虚拟人"></a>性能基准：数字虚拟人</h3><p>该任务的目标是从给定的多视角视频渲染人体化身模型。</p>
<ul>
<li>数据集：ZJU-MoCap。</li>
<li>基准算法：GART、Human101、HUGS、3DGS-Avatar。</li>
<li>评估指标：PSNR、SSIM、LPIPS* (LPIPS×1000) ,用于衡量RGB渲染性能$_{9}$</li>
<li>结果：基于3DGS的方法能在渲染质量和速度上均有优势。</li>
</ul>
<p><img src="https://pica.zhimg.com/80/v2-d916a05d315e576e3cef738aa2306226.png" alt=""></p>
<h2 id="未来研究方向-FUTURE-RESEARCH-DIRECTIONS"><a href="#未来研究方向-FUTURE-RESEARCH-DIRECTIONS" class="headerlink" title="未来研究方向 FUTURE RESEARCH DIRECTIONS"></a>未来研究方向 FUTURE RESEARCH DIRECTIONS</h2><ul>
<li><strong>数据高效的3DGS解决方案</strong>：从少样本中进行新视图生成和场景重建很重要。目前的方法有探究引入深度信息、密集概率分布、像素到高斯的映射来促进该能力，实际上就是引入更多的信息。。此外，在观测不足的区域，3DGS会产生伪影，可尝试在这些区域进行数据插值或积分。</li>
<li><strong>存储高效的3DGS解决方案</strong>：3DGS的可扩展性较差，在大尺度环境中需要大量的存储。需要优化训练阶段和模型的存储利用，而对于NeRF来说只需要存储学习到的MLP参数。可以探索更多高效的数据结构和先进的压缩技术，如Light-Gaussian等</li>
<li><strong>先进的渲染算法</strong>：目前3DGS的渲染算法较为简单直接，可见性算法会导致高斯深度/混合顺序的剧烈切换，需要实施更先进的渲染算法，更好模拟光与材料属性的复杂相互作用。可结合传统计算机图形学的方法。此外，还可探索逆渲染。</li>
<li><strong>优化与正则化</strong>： 各向异性高斯虽然有利于表示复杂几何体，但可能产生不希望的视觉伪影。例如，特别是在具有视角依赖外观的区域，大的3D高斯可能导致弹出伪影，突然出现或消失的视觉元素打破了沉浸感。使用正则化可以增加收敛速度，平滑视觉噪声或提高图像质量。此外，3DGS中大量的超参数也会影响3DGS的泛化性。在3DGS的规则化和优化方面存在相当大的探索潜力。</li>
<li><strong>3D高斯在网格重建中的应用</strong>：可探索3DGS在网格重建中的潜力，从而缩小体积渲染和传统基于表面的方法的差距，以便提出新的渲染技巧和应用。</li>
<li><strong>赋予3DGS更多可能性</strong>： 尽管3DGS具有显著潜力，但3DGS的全范围应用仍然未被充分挖掘。一个有前景的探索方向是用额外的属性增强3D高斯，例如为特定应用定制的语言和物理属性。此外，最近的研究开始揭示3DGS在多个领域的能力，例如相机姿态估计、捕捉手对象互动和不确定性量化。这些初步发现突出了跨学科学者进一步探索3DGS的重要机会。</li>
</ul>
<h2 id="参考文献-REFERENCES"><a href="#参考文献-REFERENCES" class="headerlink" title="参考文献 REFERENCES"></a>参考文献 REFERENCES</h2><ol>
<li>Kerbl, B., Kopanas, G., Leimkühler, T., &amp; Drettakis, G. (2023). <a href="https://arxiv.org/abs/2308.04079">3D Gaussian Splatting for Real-Time Radiance Field Rendering.</a> SIGGRAPH 2023.</li>
<li>Mildenhall, B., Srinivasan, P. P., Tancik, M., Barron, J. T., Ramamoorthi, R., &amp; Ng, R. (2020). <a href="https://arxiv.org/abs/2003.08934">NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis.</a> ECCV 2020.</li>
<li>Zwicker, M., Pfister, H., van Baar, J., &amp; Gross, M. (2001). <a href="https://www.cs.umd.edu/~zwicker/publications/SurfaceSplatting-SIG01.pdf">Surface Splatting.</a> SIGGRAPH 2001</li>
<li>Luiten, J., Kopanas, G., Leibe, B., &amp; Ramanan, D. (2023). <a href="https://arxiv.org/abs/2308.09713">Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis.</a> International Conference on 3D Vision.</li>
<li>Zwicker, M., Pfister, H., van Baar, J., &amp; Gross, M. (2001). <a href="https://www.cs.umd.edu/~zwicker/publications/EWAVolumeSplatting-VIS01.pdf">EWA Volume Splatting.</a> IEEE Visualization 2001.</li>
<li>Yu, A., Fridovich-Keil, S., Tancik, M., Chen, Q., Recht, B., &amp; Kanazawa, A. (2023). <a href="https://arxiv.org/abs/2112.05131">Plenoxels: Radiance Fields without Neural Networks.</a> CVPR 2022.</li>
<li><a href="https://towardsdatascience.com/a-comprehensive-overview-of-gaussian-splatting-e7d570081362">A Comprehensive Overview of Gaussian Splatting</a></li>
<li><a href="https://github.com/huggingface/blog/blob/main/gaussian-splatting.md">Introduction to 3D Gaussian Splatting</a></li>
<li><a href="https://docs.nerf.studio/nerfology/model_components/visualize_samples.html#d-frustum">Sample Representation</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/664725693">《3D Gaussian Splatting for Real-Time Radiance Field Rendering》3D高斯的理论理解</a></li>
<li><a href="https://blog.csdn.net/weixin_45657478/article/details/135603696">【论文笔记】A Survey on 3D Gaussian Splatting</a></li>
</ol>
]]></content>
      <categories>
        <category>Note</category>
      </categories>
      <tags>
        <tag>3DGS</tag>
        <tag>NeRF</tag>
      </tags>
  </entry>
  <entry>
    <title>EMO Emote Portrait Alive - 阿里HumanAIGC</title>
    <url>/2024/03/03/Paperscape/EMO/</url>
    <content><![CDATA[<h1 id="EMO-Emote-Portrait-Alive-阿里HumanAIGC"><a href="#EMO-Emote-Portrait-Alive-阿里HumanAIGC" class="headerlink" title="EMO: Emote Portrait Alive - 阿里HumanAIGC"></a>EMO: Emote Portrait Alive - 阿里HumanAIGC</h1><p>最近这一个星期，也就是2月28日的时候，阿里巴巴的HumanAIGC团队发布了一款全新的生成式AI模型EMO（Emote Portrait Alive）。EMO仅需一张人物肖像照片和音频，就可以让照片中的人物按照音频内容“张嘴”唱歌、说话，且口型基本一致，面部表情和头部姿态非常自然，发布的视频效果非常好，好的几乎难以置信，特别是蔡徐坤唱rap的第一段，效果非常好。</p>
<p><strong>EMO不仅能够生成唱歌和说话的视频，还能在保持角色身份稳定性的同时，根据输入音频的长度生成不同时长的视频。</strong></p>
<p>所以我就想借此机会，学习一下EMO的大概框架，剖析一下里面的一些技术要点，首先给出论文的链接和代码链接，不过HumanAIGC已经很久没有开源代码了，不过技术方向还是值得一看的。</p>
<p>论文：<a href="https://arxiv.org/abs/2402.17485v1">EMO: Emote Portrait Alive - Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions</a></p>
<p>项目：<a href="https://humanaigc.github.io/emote-portrait-alive/">https://humanaigc.github.io/emote-portrait-alive/</a></p>
<p>我也一直有关注这一部分的技术，大家也可以关注我的数字人知识库<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis">https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis</a></p>
<h2 id="Diffusion相关"><a href="#Diffusion相关" class="headerlink" title="Diffusion相关"></a>Diffusion相关</h2><p>在之前的一些研究中，有过用Diffusion做Talking head generation的，比如Diffusion head和CVPR2023的DiffTalk等论文，这些论文都是用Diffusion得强大生成能力来完成音频驱动的人脸生成。</p>
<p>这里逐帧生成与音频对应的人脸的图像，mask人脸中嘴唇的部分，然后逐步生成视频，<strong>这个过程相当于，AI先看一下照片，然后打开声音，再随着声音一张一张地画出视频中每一帧变化的图像。</strong></p>
<p><img src="https://picx.zhimg.com/v2-24c8ad5651ce25627b3e8bfff24d85b1.png" alt="DiffTalk"></p>
<p>如果我们看Diffusion Head论文，也是类似的做法，都是通过Diffusion的强大能力完成视频的生成。</p>
<p><img src="https://pica.zhimg.com/v2-3e6497aae4c003eb72bb3f24224c89ee.png" alt="Overview"></p>
<h2 id="EMO整体框架"><a href="#EMO整体框架" class="headerlink" title="EMO整体框架"></a>EMO整体框架</h2><p>接下来开始剖析一下EMO的框架，与DiffTalk和Diffusion Heads类似，都是利用Diffusion来生成，也是根据一个参考图像来逐帧生成图片最后得到视频。</p>
<p><img src="https://pica.zhimg.com/v2-24facf74c8152c3d19d0e57fce19c9b2.png" alt="EMO"></p>
<p>不同的是，EMO的工作过程分为两个主要阶段：</p>
<ol>
<li>首先，利用参考网络（ReferenceNet）从参考图像和动作帧中提取特征；</li>
<li>然后，利用预训练的音频编码器处理声音并嵌入，再结合多帧噪声和面部区域掩码来生成视频。</li>
</ol>
<p>该框架还融合了两种注意机制和时间模块，以确保视频中角色身份的一致性和动作的自然流畅。我觉得实际上这里是最重要的一部分，这一部分也是和之前Diffusion方法不同的点，其实这一部份又和HumanAIGC之前做的科目三驱动的方式很像，也就是那篇AnimateAnyone论文，这一部分也是火🔥了很久，现在也有人复现了该方法，不过还没有开源。</p>
<p>根据EMO的论文与项目的展现的结果，EMO不仅仅能产生非常Amazing的对口型视频，还能生成各种风格的歌唱视频，无论是在表现力还是真实感方面都显著优于现有的先进方法，如DreamTalk、Wav2Lip和SadTalker。</p>
<p><img src="https://picx.zhimg.com/v2-6492e24fb03ffa98135dc584535ab7d9.png" alt="EMO整体框架"></p>
<h2 id="EMO工作原理"><a href="#EMO工作原理" class="headerlink" title="EMO工作原理"></a>EMO工作原理</h2><p>从EMO的框架可以看到，利用骨干网络获取多帧噪声潜在输入，并尝试在每个时间步将它们去噪到连续的视频帧，这个骨干网络是类似于SD 1.5的UNet的结构配置。与之前的SD1.5不同的是，本身的SD是使用文本嵌入的，而现在是使用参考特征。</p>
<ol>
<li>与之前的工作类似，为了确保生成的帧之间的连续性，骨干网络嵌入了时间模块。 </li>
<li>为了保持生成帧中肖像的ID一致性，使用了一个与Backbone并行的称为ReferenceNet的UNet结构，它输入参考图像以获得参考特征。 </li>
<li>为了驱动角色说话动作，利用音频层对语音特征进行编码。 </li>
<li>为了使说话角色的运动可控且稳定，我们使用面部定位器和速度层来提供弱条件。</li>
</ol>
<p><strong>预训练音频编码器：</strong>EMO使用预训练的音频编码器（如wav2vec）来处理输入音频。这些编码器提取音频特征，这些特征随后用于驱动视频中的角色动作，包括口型和面部表情。这里面还是使用附加特征m来解决动作可能会受到未来/过去音频片段的影响，例如说话前张嘴和吸气。</p>
<p><strong>参考网络（ReferenceNet）：</strong>该网络从单个参考图像中提取特征，这些特征在视频生成过程中用于保持角色的身份一致性。ReferenceNet与生成网络（Backbone Network）并行工作，输入参考图像以获取参考特征。</p>
<p><strong>骨干网络（Backbone Network）：</strong>Backbone Network接收多帧噪声（来自参考图像和音频特征的结合）并尝试将其去噪为连续的视频帧。这个网络采用了类似于Stable Diffusion的UNet结构，其中包含了用于维持生成帧之间连续性的时间模块。 </p>
<p><strong>注意力机制：</strong>EMO利用两种形式的注意力机制——<strong>参考注意力（Reference-Attention）和音频注意力（Audio-Attention）</strong>。参考注意力用于保持角色身份的一致性，而音频注意力则用于调整角色的动作，使之与音频信号相匹配。 </p>
<p><strong>时间模块：</strong>这些模块用于操纵时间维度并调整动作速度，以生成流畅且连贯的视频序列。时间模块通过自注意力层跨帧捕获动态内容，有效地在不同的视频片段之间维持一致性。</p>
<p><strong>训练策略：</strong>EMO的训练分为三个阶段：图像预训练、视频训练和速度层训练。在图像预训练阶段，Backbone Network和ReferenceNet在单帧上进行训练，而在视频训练阶段，引入时间模块和音频层，处理连续帧。速度层的训练在最后阶段进行，以细化角色头部的移动速度和频率。</p>
<p><strong>去噪过程：</strong>在生成过程中，Backbone Network尝试去除多帧噪声，生成连续的视频帧。去噪过程中，参考特征和音频特征被结合使用，以生成高度真实和表情丰富的视频内容。</p>
<p>EMO模型通过这种结合使用参考图像、音频信号、和时间信息的方法，能够生成与输入音频同步且在表情和头部姿势上富有表现力的肖像视频，超越了传统技术的限制，创造出更加自然和逼真的动画效果。</p>
<h2 id="EMO训练阶段"><a href="#EMO训练阶段" class="headerlink" title="EMO训练阶段"></a>EMO训练阶段</h2><p>训练分为三个阶段，<strong>图像预训练、视频训练和速度层训练。</strong></p>
<ul>
<li><p>在图像预训练阶段，网络以单帧图像为输入进行训练。此阶段，Backbone 将单个帧作为输入，而 ReferenceNet 处理来自同一帧的不同的、随机选择的帧，从原始 SD 初始化权重</p>
</li>
<li><p>在视频训练阶段，引入时间模块和音频层，处理连续帧，从视频剪辑中采样n+f个连续帧，开始的n帧是运动帧。时间模块从AnimateDiff初始化权重。</p>
</li>
<li><p>速度层训练专注于调整角色头部的移动速度和频率。</p>
</li>
</ul>
<p>这些详细信息提供了对EMO模型训练和其参数配置的深入了解，突显了其在处理广泛和多样化数据集方面的能力，以及其在生成富有表现力和逼真肖像视频方面的先进性能。</p>
<h2 id="EMO实验设置"><a href="#EMO实验设置" class="headerlink" title="EMO实验设置"></a>EMO实验设置</h2><p>EMO的数据集有两部份，首先HumanAIGC团队从互联网中收集了 <strong>超过250小时的视频和超过1.5亿张图像</strong>，同时加入了来自互联网和HDTF以及VFHQ数据集作为补充。这里面的数据集多种多样，包括演讲、电影和电视剪辑以及歌唱表演，涵盖了多种语言，如中文和英文，这也是为什么最后能表现出如此好效果的原因。</p>
<p>在第一阶段的时候，使用VFHQ数据集，因为它不包含音频。然后再对视频进行预处理，所有的视频可通过MediaPipe来获取人脸检测框区域，并且裁剪到512×512的分辨率。</p>
<p>在第一训练阶段，批处理大小BatchSize设置为48。在第二和第三训练阶段，生成视频长度设置为f=12，运动帧数设置为n=4，训练的批处理大小为4，学习率在所有阶段均设置为1e-5。</p>
<p>在推理时，使用DDIM的采样算法生成视频。时间步大约是40步，为每一帧生成指定一个恒定的速度值，最后方法的结果生成一批（f=12帧）的时间大约为15秒。 </p>
<p>一般视频的长度为25～30帧左右，如果我们认为是1mins的视频，也就是60s的视频，那就是60*25=1500，1500/15 = 100s，也就是大概需要1mins40s能生成一分钟的视频，速度也得到了不错的改进，虽然没有实时，但是结果已经很好了。</p>
<h2 id="EMO特点"><a href="#EMO特点" class="headerlink" title="EMO特点"></a>EMO特点</h2><p>EMO模型有如下特点：</p>
<p><strong>直接音频到视频合成：</strong>EMO采用直接从音频合成视频的方法，无需中间的3D模型或面部标志，简化了生成过程，同时保持了高度的表现力和自然性。</p>
<p><strong>无缝帧过渡与身份保持：</strong>该方法确保视频帧之间的无缝过渡和视频中身份的一致性，生成的动画既生动又逼真。</p>
<p><strong>表达力与真实性：</strong>实验结果显示，EMO不仅能生成令人信服的说话视频，而且还能生成各种风格的歌唱视频，其表现力和真实性显著超过现有的先进方法。</p>
<p><strong>灵活的视频时长生成：</strong>EMO可以根据输入音频的长度生成任意时长的视频，提供了极大的灵活性。</p>
<p><strong>面向表情的视频生成：</strong>EMO专注于通过音频提示生成表情丰富的肖像视频，特别是在处理说话和唱歌场景时，可以捕捉到复杂的面部表情和头部姿态变化。</p>
<p>这些特点共同构成了EMO模型的核心竞争力，使其在动态肖像视频生成领域表现出色。</p>
<h2 id="EMO缺陷"><a href="#EMO缺陷" class="headerlink" title="EMO缺陷"></a>EMO缺陷</h2><p>对于EMO来说，也会有一些限制。</p>
<ul>
<li><p>首先，与不依赖扩散模型的方法相比，它更耗时。</p>
</li>
<li><p>其次，由于不使用任何明确的控制信号来控制角色的运动，因此可能会导致无意中生成其他身体部位（例如手），从而导致视频中出现伪影。</p>
</li>
</ul>
<p>所以这样的一个问题，如果要解决的话，可以考虑用专门控制身体部位的控制信号，这样就会较好的解决这个方法，每一个信号控制一部分，就不会生成错误。</p>
<p>参考</p>
<ul>
<li><a href="https://m.huxiu.com/article/2728417.html">https://m.huxiu.com/article/2728417.html</a></li>
</ul>
]]></content>
      <categories>
        <category>Paperscape</category>
      </categories>
      <tags>
        <tag>Talking Head Generation</tag>
        <tag>Diffusion Models</tag>
      </tags>
  </entry>
  <entry>
    <title>SyncTalk The Devil is in the Synchronization for Talking Head Synthesis</title>
    <url>/2024/03/07/Paperscape/SyncTalk/</url>
    <content><![CDATA[<h1 id="SyncTalk-The-Devil-is-in-the-Synchronization-for-Talking-Head-Synthesis"><a href="#SyncTalk-The-Devil-is-in-the-Synchronization-for-Talking-Head-Synthesis" class="headerlink" title="SyncTalk: The Devil is in the Synchronization for Talking Head Synthesis"></a>SyncTalk: The Devil is in the Synchronization for Talking Head Synthesis</h1><p>Paper   : <a href="https://arxiv.org/abs/2311.17590">https://arxiv.org/abs/2311.17590</a></p>
<p>Project : <a href="https://ziqiaopeng.github.io/synctalk/">https://ziqiaopeng.github.io/synctalk/</a></p>
<p>Video    : <a href="https://ziqiaopeng.github.io/synctalk/#teaser">https://ziqiaopeng.github.io/synctalk/#teaser</a></p>
<p>Code    : <a href="https://github.com/ziqiaopeng/SyncTalk">https://github.com/ziqiaopeng/SyncTalk</a></p>
<p><strong>摘要</strong></p>
<p>神经辐射场 - 生成对抗网络框架用于实现说话人头部视频的同步合成。</p>
<p>（1）研究背景： 生成逼真的、由语音驱动的谈话头部视频是一项具有挑战性的任务。传统生成对抗网络（GAN）难以保持一致的面部身份，而神经辐射场（NeRF）方法虽然可以解决这个问题，但通常会产生不匹配的唇部动作、不充分的面部表情和不稳定的头部姿势。一个逼真的谈话头部需要同步协调主体身份、唇部动作、面部表情和头部姿势。缺乏这些同步是导致不真实和人工结果的根本缺陷。 </p>
<p>（2）过去的方法及其问题： GAN 方法难以保持一致的面部身份。NeRF 方法虽然可以解决这个问题，但通常会产生不匹配的唇部动作、不充分的面部表情和不稳定的头部姿势。 </p>
<p>（3）提出的研究方法： SyncTalk 是一种基于 NeRF 的方法，它有效地保持了主体身份，增强了谈话头部合成的同步性和真实性。SyncTalk 使用面部同步控制器将唇部动作与语音对齐，并创新地使用 3D 面部混合形状模型来捕捉准确的面部表情。头部同步稳定器优化头部姿势，实现更自然的头部运动。肖像同步生成器恢复头发细节，并将生成的头部与躯干融合，以获得无缝的视觉体验。</p>
<p>（4）方法在什么任务上取得了什么性能，这些性能是否支持了它们的目标： SyncTalk 在谈话头部合成同步性和真实性方面优于最先进的方法。广泛的实验和用户研究表明，SyncTalk 在同步性和真实性方面优于最先进的方法。</p>
<p><strong>关键要点</strong></p>
<ul>
<li>传统生成对抗网络难以维持一致的面部身份。</li>
<li>神经辐射场方法可以解决面部身份一致性问题，但经常出现嘴唇运动不匹配、面部表情不足和头部姿势不稳定的问题。</li>
<li>逼真的说话人头部视频需要同步协调主体身份、嘴唇运动、面部表情和头部姿势。</li>
<li>缺少同步性是导致不真实和人为结果的根本缺陷。</li>
<li>SyncTalk 是一种基于神经辐射场的方法，有效地保持了主体身份，提高了说话人头部合成中的同步性和真实感。</li>
<li>SyncTalk 使用面部同步控制器将嘴唇运动与语音对齐，并创新地使用 3D 面部混合形状模型来捕捉准确的面部表情。</li>
<li>SyncTalk 的头部同步稳定器优化了头部姿势，实现了更自然的头部运动。</li>
<li>人像同步生成器恢复头发细节，将生成的头部与躯干融合，以获得无缝的视觉体验。</li>
</ul>
<p><img src="https://picx.zhimg.com/v2-a57e0937b2f452009023394a59529dfb.png" alt="SyncTalk"></p>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>这篇论文中，解决最好的就是同步的问题，所以也称为同步的Devil 魔鬼😈。现有方法在四个关键领域需要更多的同步：<strong>主体身份</strong>、<strong>唇部运动</strong>、<strong>面部表情</strong>和<strong>头部姿势</strong>。</p>
<ul>
<li><p>首先，在基于GAN的方法中，由于连续帧中特征的不稳定性以及仅使用少量帧作为面部重建参考，保持视频中主体的身份是具有挑战性的。</p>
</li>
<li><p>其次，唇部运动与语音不同步。在基于NeRF的方法中，仅基于5分钟语音数据集训练的音频特征难以泛化到不同的语音输入。</p>
</li>
<li><p>第三，缺乏面部表情控制，大多数方法只能产生唇部运动或控制眨眼，导致面部动作不自然。</p>
</li>
<li><p>第四，头部姿势不同步。</p>
</li>
</ul>
<p>先前的方法依赖于稀疏的landmarks来计算投影误差，但这些landmarks的抖动和不准确性导致头部姿势不稳定。这些同步问题会引入伪影，并显著降低真实感。</p>
<p>为了解决这些同步挑战，引入了SyncTalk，这是一种基于NeRF的方法，专注于高度同步、逼真的、语音驱动的说话头部合成，采用三平面哈希表示来维护主体身份。通过面部同步控制器和头部同步稳定器，SyncTalk显著提高了合成视频的同步性和视觉质量。PortraitSync Generator进一步改善了视觉质量，精心细化了视觉细节。整个渲染过程可以实现50 FPS，并输出高分辨率视频。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>模块</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>Face-Sync Controller</td>
<td>在Face-Sync控制器中，预先在2D音频视听数据集上对音频视觉编码器进行预训练，得到了一种通用表示，确保了不同语音样本之间的唇部同步运动。对于控制面部表情，采用了一个语义丰富的3D面部混合形状模型，该模型通过52个参数控制特定的面部表情区域。</td>
</tr>
<tr>
<td>Head-Sync Stabilizer</td>
<td>在Head-Sync稳定器中，使用AD-NeRF中的头部运动跟踪器来推断头部的粗略旋转和平移参数。由于粗略参数的不稳定性，借鉴了同步定位与地图(SLAM)的思想，结合头部关键点跟踪器跟踪稠密关键点，并采用bundle adjustment method 束调整方法来优化头部姿势，从而实现稳定连续的头部运动。</td>
</tr>
<tr>
<td>Portrait-Sync Generator</td>
<td>为了进一步提高SyncTalk的视觉保真度，设计了一个Portrait-Sync生成器。这个模块修复了NeRF建模中的伪影，特别是头发和背景等细节，输出高分辨率视频。</td>
</tr>
</tbody>
</table>
</div>
<p><strong>主要贡献</strong></p>
<ul>
<li>提出了一个Face-Sync控制器，结合音频视觉编码器和面部动画捕捉器，确保准确的唇部同步和动态面部表情渲染。 </li>
<li>引入了一个Head-Sync稳定器，跟踪头部旋转和面部运动关键点。利用束调整方法，该稳定器保证了平滑同步的头部运动。</li>
<li>设计了一个Portrait-Sync生成器，通过修复NeRF建模中的伪影和细化头发和背景等细节，提高了视觉保真度。</li>
</ul>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p><strong>GAN-based Method</strong></p>
<p>近来，基于GAN的说话头合成成为了计算机视觉中的一个重要研究领域。然而，它们在保持视频中主体的身份一致性方面存在挑战。</p>
<p>例如，Wav2Lip引入了一个唇部同步专家来监督唇部运动。然而，由于使用了来自参考帧的五帧来重建唇部，它难以保持主体的身份。另一些方法尝试进行全脸合成，但往往难以确保面部表情和头部姿势之间的同步。除了视频流技术外，还有一些方法试图通过语音使单张图像“说话”，如SadTalker可以从单张图像生成一个人说话的视频。然而，这些方法无法生成自然的头部姿势和面部表情，难以保持主体的身份，影响了同步效果，导致视觉感知不真实。</p>
<p>与这些方法相比，SyncTalk使用NeRF<strong>对人脸进行三维建模</strong>。其能够在规范空间中表示<strong>连续的3D场景的能力</strong>，使其在保持主体身份一致性和保留细节方面表现出色。</p>
<p><strong>NeRF-based Method</strong></p>
<p>近来，随着NeRF的崛起，许多领域已开始利用它来解决相关挑战。先前的工作已将NeRF整合到合成说话头像的任务中，并将音频作为驱动信号，但这些方法都是基于普通的NeRF模型。</p>
<p>例如，AD-NeRF需要大约10秒来渲染单个图像。RADNeRF旨在实现实时视频生成，并使用了基于Instant-NGP的NeRF。ER-NeRF通过引入三平面哈希编码器来修剪空白空间区域，提倡紧凑且加速的渲染方法。GeneFace试图通过将语音特征转换为面部标志来减少NeRF的伪影，但这往往导致唇部运动不准确。尝试使用基于NeRF的方法创建角色头像，例如，不能直接由语音驱动。这些方法仅将音频作为条件，没有清晰的同步概念，并且通常导致唇部运动平均。</p>
<p>此外，先前的方法<strong>缺乏对面部表情的控制</strong>，仅限于控制眨眼，并且无法对抬眉毛或皱眉等动作进行建模。此外，这些方法在头部姿势不稳定方面存在显着问题，<strong>导致头部和躯干分离</strong>。相比之下，使用Face-Sync控制器来建模音频和唇部运动之间的关系，从而增强唇部运动和表情的同步性，使用Head-Sync稳定器来稳定头部姿势，通过解决这些同步问题，提高了视觉质量。</p>
<h2 id="主要方法"><a href="#主要方法" class="headerlink" title="主要方法"></a>主要方法</h2><p>SyncTalk主要由三部分组成，接下来会一一介绍</p>
<ul>
<li><strong>Face-Sync Controller</strong> 控制嘴唇运动和面部表情</li>
<li><strong>Head-Sync Stabilizer</strong> 稳定头部姿势</li>
<li><strong>Portrait-Sync Generator</strong> 渲染的高同步面部帧</li>
</ul>
<p><img src="https://pica.zhimg.com/v2-03605cd4fbd659c9d341840c64fd3b41.png" alt="Overview of SyncTalk"></p>
<h3 id="Face-Sync-Controller"><a href="#Face-Sync-Controller" class="headerlink" title="Face-Sync Controller"></a>Face-Sync Controller</h3><p><strong>Audio-Visual Encoder</strong></p>
<p>在现有的方法中，大部分的音频特征提取器是用类似于 <strong>DeepSpeech，Wav2Vec 2.0 和 HuBERT</strong> 等ASR模型，但是这些事专门为Automatic Speech Recognition ASR任务设计的，这种设计的音频编码器并不能真正反映嘴唇运动。这是因为预训练的模型是<strong>基于从音频到文本的特征分布，而需要从音频到嘴唇运动的特征分布</strong>。</p>
<p>针对这种情况，使用在LRS2上训练的<a href="https://github.com/smeetrs/deep_avsr">deep avsr</a>来做音频特征提取器，使用预训练的唇形同步鉴别器 <a href="https://github.com/joonson/syncnet_python">SyncNet</a>来监督视频的同步效果，这是使用连续的面部窗口F和相对应的音频帧A输入，同时分为正负样本进行训练，利用<strong>余弦相似度和交叉熵损失</strong>来最小化同步样本的距离并最大化非同步样本的距离。</p>
<script type="math/tex; mode=display">
\begin{aligned}\sin(F,A)&=\frac{F\cdot A}{\|F\|_2\|A\|_2})\end{aligned},</script><script type="math/tex; mode=display">
L_{\mathrm{sync}}=-\left(y\log(\sin(F,A))+(1-y)\log(1-\sin(F,A))\right),</script><p><img src="https://picx.zhimg.com/v2-6b250a8119b776d55493f82cfda54bc5.png" alt="正负样本"></p>
<p>同时在同步鉴别器的监督下，预训练对应的视听特征提取器，这里面堆叠卷积网络进行编码解码，最后用<strong>重建损失</strong>来进行监督。训练后，我们使用 Conv(A) 作为从音频中提取的唇部空间。</p>
<script type="math/tex; mode=display">
L_{\mathrm{recon}}=\|F-\mathrm{Dec}(\mathrm{Conv}(A)\oplus\mathrm{Conv}(F))\|_1.</script><p><strong>Facial Animation Capturer</strong></p>
<p>在之前的研究中发现，基于NeRF的方法只能改变眨眼，无法准确地建模面部表情。这导致训练出的角色表情僵硬，面部细节不准确，特别是对于有明显面部动作的角色，如眨眼、抬眉毛或皱眉等。<strong>考虑到需要更加同步和逼真的面部表情，添加了一个表情同步控制模块。</strong></p>
<p>具体而言，引入了一个<strong>基于52个语义面部混合形状系数 B 的3D面部先验模型来建模面部</strong>，也就是3D blendshape 系数来控制面部，这一部分类似于 <a href="https://arxiv.org/abs/2303.11089">EmoTalk</a>。因为3D面部模型能够保留面部运动的结构信息，所以它能够很好地反映面部动作的内容，同时又不会引起面部结构的失真。</p>
<p><strong>在训练过程中，首先使用一个复杂的面部混合形状捕捉模块将面部表情捕捉为E(B)，然后选择七个核心面部表情控制系数来控制眉毛、额头和眼睛区域。</strong>这些系数与表情高度相关，且独立于嘴唇的运动。因为面部系数具有语义信息，所以我们可以在推理过程中同步演讲者的面部表情。</p>
<p><img src="https://pica.zhimg.com/v2-9cfb1cfb7f4ae95b64a868f8e8abad0e.png" alt="Facial Animation Capturer"></p>
<p><strong>Facial-Aware Masked-Attention</strong></p>
<p>为了减少训练过程中嘴唇特征和表情特征之间的相互干扰，引入了Facial-Aware Disentangle Attention模块。基于区域注意力向量 V，这类似于<a href="https://fictionarry.github.io/ER-NeRF/">ER-NeRF</a>，我们分别将Mask $M<em>{lip}$ 和 $M</em>{exp}$ 添加到嘴唇和表情的注意力区域。</p>
<script type="math/tex; mode=display">
\begin{aligned}V_{\mathrm{lip}}&=V\odot M_{\mathrm{lip}},\\V_{\mathrm{exp}}&=V\odot M_{\mathrm{exp}}.\end{aligned}</script><p>通过这样设计的注意力机制，能够有效解耦嘴唇运动和眨眼运动等，从而减少耦合带来的伪影，最后利用解耦的嘴唇特征 $f<em>l = F</em>{lip} ⊙ V<em>{lip}$ 和表情特征$f_e = f</em>{exp} ⊙ V_{exp}$。</p>
<p><img src="https://pica.zhimg.com/v2-ba601309ab5cc09573f4291d7ae27f13.png" alt="ER-NeRF Mask"></p>
<h3 id="Head-Sync-Stabilizer"><a href="#Head-Sync-Stabilizer" class="headerlink" title="Head-Sync Stabilizer"></a>Head-Sync Stabilizer</h3><p><strong>Head Motion Tracker</strong></p>
<p>头部姿势，表示为 p，是指人的头部在 3D 空间中的旋转角度，由旋转 R 和平移 T 定义。</p>
<p>不稳定的头部姿势会导致头部抖动。为了获得头部姿势的粗略估计，首先，通过在预定范围内迭代 i 次来确定最佳焦距。对于每个焦距候选 fi，重新初始化旋转和平移值。目标是最小化 3D 可变形模型 (3DMM) 的投影地标与视频帧中的实际地标之间的误差。</p>
<script type="math/tex; mode=display">
f_{\mathrm{opt}}=\arg\min_{f_i}E_i(L_{2D},L_{3D}(f_i,R_i,T_i)),</script><p>其中 $E_i$表示的就是MSE，这样能够以更好地将模型的投影lmk与实际视频lmk对齐，然后得到最优的旋转和平移矩阵，也是用MSE来最小化，这是对每一帧进行操作的，在对应视频帧的最优值。</p>
<script type="math/tex; mode=display">
(R_{\mathrm{opt}},T_{\mathrm{opt}})=\arg\min_{R,T}E(L_{2D},L_{3D}(f_{\mathrm{opt}},R,T)).</script><p><strong>Head Points Tracker</strong></p>
<p>对于之前基于NeRF的方法来说，先前的方法利用基于 3DMM 的技术来提取头部姿势并生成不准确的结果。为了提高R和T的精度，我们使用像Co- tracker这样的光流估计模型来跟踪面部关键点K。</p>
<p>接下来，使用预训练的光流估计模型，在获取面部运动光流后，我们使用<strong>拉普拉斯滤波器</strong>选择位于最显著流变化位置的关键点，并在流序列中跟踪这些关键点的运动轨迹。通过这个模块确保了所有帧上的面部关键点对齐更加精确和一致，从而增强了头部姿势参数的准确性。</p>
<p><strong>Bundle Adjustment</strong></p>
<p>根据关键点和粗略的头部姿势，引入了一个两阶段优化框架来提高关键点和头部姿势估计的准确性。</p>
<ul>
<li><p>第一阶段，随机初始化 j 个关键点的 3D 坐标并优化它们的位置，以便与图像平面上跟踪的关键点对齐。这一部分最小化损失函数 $L_{init}$，捕获<strong>投影关键点 P 和跟踪关键点 K</strong> 之间的差异：</p>
<script type="math/tex; mode=display">
L_{\mathrm{init}}=\sum_j\lVert P_j-K_j\rVert_2.</script></li>
<li><p>第二阶段，开始进行更全面的优化，以细化 3D 关键点和相关的头部联合姿势参数，通过Adam优化器优化算法，<strong>调整空间坐标、旋转角度R和平移T</strong>以最小化对齐误差$L_{sec}$，表示为：</p>
<script type="math/tex; mode=display">
L_{\sec}=\sum_j\lVert P_j(R,T)-K_j\rVert_2.</script><p>经过这些优化后，观察到所得的头部姿势和平移参数平滑且稳定。</p>
</li>
</ul>
<h3 id="Dynamic-Portrait-Renderer"><a href="#Dynamic-Portrait-Renderer" class="headerlink" title="Dynamic Portrait Renderer"></a>Dynamic Portrait Renderer</h3><p><strong>Tri-Plane Hash Representation</strong></p>
<p>这一部分实际上就是NeRF的体渲染的方式，都是一些定义的部分。</p>
<script type="math/tex; mode=display">
\hat{C}(\mathrm{r})=\int_{t_n}^{t_f}\sigma(\mathrm{r}(t))\cdot\mathrm{c}(\mathrm{r}(t),\mathrm{d})\cdot T(t)dt,</script><p>类似于ER-NeRF的方式，解决哈希冲突和优化音频特征处理的问题，结合了三个独特定向xyz的 2D 哈希网格，也就是 <strong>Tri-Plane Hash</strong>，作为hash的编码器。</p>
<script type="math/tex; mode=display">
\mathcal{H}^{\mathrm{AB}}:(a,b)\to\mathrm{f}_{ab}^{\mathrm{AB}},\\
\mathrm{f_x}=\mathcal{H}^\mathrm{XY}(x,y)\oplus\mathcal{H}^\mathrm{YZ}(y,z)\oplus\mathcal{H}^\mathrm{XZ}(x,z),</script><p>其中输出 $f^{AB}<em>{ab} ∈ R</em>{LD}$，具有层数 $L$ 和每个方向的特征维度 $D$，表示与投影坐标$ (a, b)$ 相对应的平面几何特征，$H^{AB}$ 表示平面 $R^{AB}$ 的多分辨率哈希编码器。得到每个方向的向量以后，产生 $3 × LD$ 通道向量。采用$fx$、视角方向$d$、嘴唇特征$f_l$和表情特征$f_e$，三平面哈希的隐式函数定义为：</p>
<script type="math/tex; mode=display">
\mathcal{F}^{\mathcal{H}}:(\mathrm{x},\mathrm{d},f_l,f_e;\mathcal{H}^3)\to(\mathrm{c},\sigma),</script><p>类似于ER-NeRF，训练采用了一个两步粗到细的策略。首先，使用MSE损失评估预测的 $\hat{C(r)}$与实际图像颜色$C(r)$之间的差异。鉴于MSE在细节捕捉方面的局限性。接下来进入一个细化阶段，引入LPIPS损失以增强细节，类似于ER-NeRF。我们从图像中提取随机补丁Patch $P$，并将LPIPS（由λ加权）与MSE结合起来以改善细节表示。</p>
<script type="math/tex; mode=display">
\mathcal{L}_\mathrm{total}=\sum_\mathrm{r}\|C(\mathrm{r})-\hat{C}(\mathrm{r})\|_2+\lambda\times\mathcal{L}_\mathrm{LPIPS}(\hat{\mathcal{P}},\mathcal{P}).</script><p><strong>Portrait-Sync Generator</strong></p>
<p>在训练过程中，为了解决 NeRF 在<strong>捕捉发丝和动态背景</strong>等精细细节方面的局限性，引入了一个包含两个关键部分的 PortraitSync 生成器。</p>
<p>首先，NeRF 渲染面部区域 ($Fr$)，通过高斯模糊创建 $G(Fr)$，然后使用我们同步的头部姿势能够与原始图像 ($F_o$) 合并，以增强头发细节保真度。</p>
<p>其次，当头部和躯干结合在一起时，如果源视频中的角色说话而生成的面部保持沉默，则可能会出现暗间隙区域，如下图（b）所示。 所以用平均颈部颜色 ($Cn$) 填充这些区域。 </p>
<p>这种方法通过肖像同步生成器产生更真实的细节并提高视觉质量。</p>
<p><img src="https://picx.zhimg.com/v2-421af4b4cfa489148de7fc8f4067427b.png" alt="比较"></p>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p><strong>数据集</strong></p>
<p>为了进行公平比较，我们使用了来自AD-NeRF，GeneFace和ER-NeRF中相同的视频序列，其中包括英语和法语。这些视频的平均长度约为8,843帧，每个视频以25 FPS录制。除了来自AD-NeRF的视频分辨率为450 × 450外，所有其他视频的分辨率均为512 × 512，并以角色为中心。</p>
<p><strong>比较基线</strong></p>
<ul>
<li>GAN-based  方法  ：Wav2Lip，VideoReTalking，DINet，TalkLip and IP-LAP。</li>
<li>NeRF-based 方法 ： AD-NeRF，RADNeRF，GeneFace and ER-NeRF。</li>
</ul>
<p><strong>实验细节</strong></p>
<ul>
<li>在粗略阶段，肖像头部经过100,000次迭代训练，在精细阶段训练25,000次迭代。</li>
<li>每次迭代使用2D哈希编码器（L=14，F=1）采样$256^2$条光线。</li>
<li>采用AdamW优化器[24]，哈希编码器的学习率为0.01，其他模块的学习率为0.001。</li>
<li>在NVIDIA RTX 3090 GPU上，总训练时间约为2小时。</li>
</ul>
<p><strong>定量评价</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>评估指标</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>全参考质量评估</td>
<td>使用峰值信噪比（PSNR）、学习感知图像补丁相似性（LPIPS）、多尺度结构相似性（MS-SSIM）和Frechet Inception Distance（FID）作为评估指标。</td>
</tr>
<tr>
<td>无参考质量评估</td>
<td>在高PSNR图像中，纹理细节可能与人类视觉感知不一致。为了更精确地定义和比较输出，使用两种无参考方法：自然图像质量评估器（NIQE）和无参考图像空间质量评估器（BRISQUE）。</td>
</tr>
<tr>
<td>同步评估</td>
<td>对于同步性，使用地标距离（LMD）来衡量面部运动的同步性，动作单位误差（AUE）来评估面部运动的准确性，并引入唇同步误差置信度（LSE-C），与Wav2Lip一致，以评估唇部运动与音频之间的同步性。</td>
</tr>
</tbody>
</table>
</div>
<p><strong>定量评估结果</strong></p>
<ul>
<li>头部重建方法在图像质量和同步性方面均优于基于GAN和NeRF的最新方法。</li>
<li>经过<code>Portrait-Sync Generato</code>r处理后，图像质量得到了显著改善，头发细节得到了恢复。</li>
<li>方法在维持主体身份、唇部、表情和姿势的同步性方面表现出色。</li>
<li>使用分布外音频的最新SOTA方法的驱动器结果表明，方法在唇音同步评估方面领先。</li>
<li>渲染速度远远超过视频输入速度，可以实现实时生成视频流。</li>
</ul>
<p><img src="https://pica.zhimg.com/v2-3093f3d799bb12490a7f79dba96bde99.png" alt="The quantitative results of the head reconstruction."></p>
<p><img src="https://picx.zhimg.com/v2-73c53cd37a7c9e87af9b918778a84d3e.png" alt="The quantitative results of the lip synchronization."></p>
<p><strong>定性评价</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>评估结果</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>图像质量比较</td>
<td>在图中，我们展示了我们的方法与其他方法的比较。可以观察到，SyncTalk展示了更精确、更准确的面部细节。</td>
</tr>
<tr>
<td>与Wav2Lip的比较</td>
<td>与Wav2Lip相比，我们的方法在保持主体身份的同时提供了更高的保真度和分辨率。</td>
</tr>
<tr>
<td>与IP-LAP的比较</td>
<td>与IP-LAP相比，我们的方法在唇形同步方面表现出色，主要归功于音频-视觉编码器带来的音频-视觉一致性。</td>
</tr>
<tr>
<td>与GeneFace的比较</td>
<td>与GeneFace相比，我们的方法可以通过表情同步精确地重现眨眼和抬眉等动作。</td>
</tr>
<tr>
<td>与ER-NeRF的比较</td>
<td>与ER-NeRF相比，我们的方法通过姿势同步稳定器避免了头部和身体的分离，并生成了更准确的唇形。</td>
</tr>
<tr>
<td></td>
</tr>
</tbody>
</table>
</div>
<p><img src="https://picx.zhimg.com/v2-b076e645737b2297bee21027ac8e27ad.png" alt="Qualitative comparison of facial synthesis by different methods."></p>
<p><strong>User Study</strong>  </p>
<p>我们设计了一个详尽的用户研究问卷，35名参与者进行评分。问卷设计了五个方面的评分：唇同步准确性、表情同步准确性、姿势同步准确性、图像质量和视频真实性。</p>
<p>参与者平均完成问卷时间为19分钟，标准化的Cronbach α系数为0.96。用户研究结果显示，SyncTalk在所有评估中均超过以前的方法，特别是在视频真实性方面。</p>
<p><img src="https://picx.zhimg.com/v2-2666052562f51f053affc9fb748eec54.png" alt="User Study"></p>
<p><strong>Ablation Study</strong></p>
<p>接下来进行了消融研究，以检验我们模型中不同部分的贡献，选择了三个核心指标进行评估：PSNR、LPIPS和LMD。</p>
<p>我们选择了一个名为“May”的主体进行测试，结果如表所示。</p>
<p><img src="https://pic1.zhimg.com/v2-b204e48268633b55ad93cf70dbc8f9bd.png" alt="Ablation study for our components"></p>
<p>音频-视觉编码器提供了主要的唇部同步信息，当替换此模块时，所有三个指标都变差，其中特别是LMD错误增加了21.15%，表明唇部动作同步减少，如图5（a）所示，显示出我们的音频-视觉编码器可以提取准确的唇部特征。</p>
<p><img src="https://pica.zhimg.com/v2-2fc44a31570aeacd6badcf909f669fdc.png" alt="Ablation Study"></p>
<p>用ER-NeRF 的<strong>眨眼模块</strong>替换<strong>Facial Animation Capture</strong>模块，这一部分会影响眉毛的运动和图像质量。</p>
<p><strong>Facial-Aware Masked-Attention</strong>主要解耦了唇部和面部其他部位之间的运动，在移除后略微影响图像质量。</p>
<p>若没有<strong>头部同步稳定器</strong>，所有指标都显著下降，特别是LPIPS，导致头部姿势抖动和头部与躯干分离，如图5（b）所示。</p>
<p><strong>Portrait-Sync Generator</strong>恢复了像头发这样的细节，移除此模块会影响头发等细节的恢复，导致明显的分割边界。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>本文介绍了SyncTalk，这是一种基于高度同步的NeRF方法，用于实现逼真的语音驱动的说话头部合成。</li>
<li>框架包括面部同步控制器、头部同步稳定器和肖像同步生成器，能够保持主体身份，并生成同步的唇部动作、面部表情和稳定的头部姿势。</li>
<li>通过广泛的评估，SyncTalk在创建逼真和同步的说话头部视频方面表现出优异的性能，相较于现有方法。</li>
<li>期望SyncTalk不仅能增强各种应用程序的功能，还能在说话头部合成领域激发进一步的创新。</li>
</ul>
]]></content>
      <categories>
        <category>Paperscape</category>
      </categories>
      <tags>
        <tag>Talking Head Generation</tag>
        <tag>NeRF</tag>
      </tags>
  </entry>
  <entry>
    <title>VividTalk One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior</title>
    <url>/2024/03/05/Paperscape/VividTalk/</url>
    <content><![CDATA[<h1 id="VividTalk-One-Shot-Audio-Driven-Talking-Head-Generation-Based-on-3D-Hybrid-Prior"><a href="#VividTalk-One-Shot-Audio-Driven-Talking-Head-Generation-Based-on-3D-Hybrid-Prior" class="headerlink" title="VividTalk: One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior"></a>VividTalk: One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior</h1><p>Paper   : <a href="https://arxiv.org/pdf/2312.01841.pdf">https://arxiv.org/pdf/2312.01841.pdf</a></p>
<p>Project : <a href="https://humanaigc.github.io/vivid-talk/">https://humanaigc.github.io/vivid-talk/</a></p>
<p>Video   : <a href="https://www.youtube.com/watch?v=lJVzt7JCe_4">https://www.youtube.com/watch?v=lJVzt7JCe_4</a></p>
<p>Code    : <a href="https://github.com/HumanAIGC/VividTalk">https://github.com/HumanAIGC/VividTalk</a>  (Maybe Comming Soon)</p>
<p><strong>摘要</strong></p>
<p>创新的两阶段框架 VividTalk 可生成高质量视觉效果的说话人头部视频，包括唇形同步、丰富的面部表情、自然的头部姿势等。</p>
<p>（1）音频驱动的说话头生成已经引起广泛关注，在唇形同步、面部表情、头部姿势生成和视频质量方面取得了进展。然而，由于音频和动作之间的一对多映射，还没有模型能够在所有这些指标上达到最优SOTA。<br>（2）以往的方法通常使用混合形状Blendshape或顶点偏移vertex来表示面部表情，但这些方法在捕捉精细的表情细节方面存在局限性。此外，头部姿势的生成通常是通过直接从音频中学习来实现的，这可能会导致不合理和不连续的结果。<br>（3）本文提出了一种名为 VividTalk 的两阶段通用框架，支持生成具有所有上述属性的高视觉质量说话头视频。在第一阶段，通过学习非刚性表情运动和刚性头部运动将音频映射到网格。对于表情运动，采用混合形状和顶点作为中间表示，以最大限度地提高模型的表示能力。对于自然头部运动，提出了一种新颖的可学习头部姿势codebook，并采用两阶段训练机制。在第二阶段，提出了一种双分支运动-VAE 和生成器，将网格转换为密集运动并逐帧合成高质量视频。<br>（4）广泛的实验表明，所提出的 VividTalk 可以生成具有唇形同步和逼真头部姿势的高视觉质量说话头视频，并且在客观和主观比较中优于以往的最新作品。</p>
<p><strong>要点</strong></p>
<ul>
<li>VividTalk 采用双阶段通用框架，可以生成高质量视觉效果的说话人头部视频。</li>
<li>VividTalk 在第一阶段通过学习非刚性表情运动和刚性头部运动，将音频映射到网格。</li>
<li>VividTalk 在第二阶段使用双分支运动-VAE 和生成器将网格转换为密集运动并逐帧合成高质量视频。</li>
<li>广泛的实验表明，与目前最先进的作品相比，VividTalk 可以生成高质量视觉效果的说话人头部视频，并将唇形同步和逼真的增强效果提高很大幅度。</li>
</ul>
<p><img src="https://picx.zhimg.com/80/v2-8521b04f82075cc27b5e95148dba9792.png" alt="VividTalk can generate realistic and lip-sync talking head videos with expressive facial expression, natural head poses."></p>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p> <strong>音频驱动人脸生成</strong></p>
<p>主要是利用音频驱动人脸，生成相匹配的图像，最近的一些工作如SadTalker，是用3DMM作为中间表示，再使用3DMM渲染得到对应的视频；也有利用人脸面部关键点的，这都是比较类似的。同时加入生成mask的嘴唇部份，但是由于中间的表示限制，所有这些方法都不足以生成口型同步和逼真的头部说话视频。</p>
<p>这个VIvidTalker是使用blendshape和vertex来作为中间表示，分别对粗粒度和细粒度进行建模。</p>
<p><strong>视频驱动人脸生成</strong></p>
<p>视频驱动可以认为是表情迁移，也就是将参考视频的动作迁移到目标人脸上，比如FOMM这样的方式，用无监督的关键点作为中间的表示，以及有利用depth深度作为信息的。</p>
<h2 id="主要方法"><a href="#主要方法" class="headerlink" title="主要方法"></a>主要方法</h2><p>VividTalk主要的框架由两个级联阶段组成，分别是</p>
<ul>
<li><strong>Audio-To-Mesh</strong> 音频到网格生成</li>
<li><strong>Mesh-To-VIdeo</strong> 网格到视频生成</li>
</ul>
<p><img src="https://pic1.zhimg.com/v2-35ebd6e4eb48d485c2f77af937e3a762.png" alt="主要方法"></p>
<h3 id="前馈知识"><a href="#前馈知识" class="headerlink" title="前馈知识"></a>前馈知识</h3><h4 id="3DMM"><a href="#3DMM" class="headerlink" title="3DMM"></a>3DMM</h4><p>3D Morphable Model（3DMM）是一种用于建模和分析人脸形状和外观的计算机图形技术。它是基于数学模型的方法，用于描述和生成人脸的<strong>三维几何形状和表面纹理</strong>。3DMM的基本原理是利用统计学方法从大量的三维人脸数据中学习人脸形状和纹理的变化规律，并将这些信息编码到一个数学模型中。</p>
<p>这个模型包括两个主要的部分：形状模型和纹理模型。</p>
<ol>
<li><strong>形状模型</strong>：形状模型描述了人脸的几何形状的变化。通常采用的方法是使用主成分分析（PCA）对人脸的形状数据进行降维和建模。通过分析大量的人脸形状数据，可以得到一组主成分，它们描述了人脸形状变化的主要模式。形状模型可以用来生成新的人脸形状，或者对现有的人脸形状进行编辑和变形。</li>
<li><strong>纹理模型</strong>：纹理模型描述了人脸表面的颜色和纹理的变化。与形状模型类似，纹理模型也可以利用主成分分析等方法来建模人脸的表面纹理。通过分析大量的人脸纹理数据，可以得到一组主成分，它们描述了人脸表面颜色和纹理的变化模式。纹理模型可以用来生成新的人脸纹理，或者对现有的人脸纹理进行编辑和变换。</li>
</ol>
<p><img src="https://pic1.zhimg.com/v2-efd80426cbb18b4f2ee91789c07277eb.png" alt="3DMM"></p>
<h4 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h4><p>除此之外，首先还需要对数据集进行预处理，使用FOMM对方式对视听数据集进行预处理，并且裁剪面部区域为256x256。同时也是用FaceVerse来提取表情系数和网格顶点序列。</p>
<h3 id="Audio-To-Mesh"><a href="#Audio-To-Mesh" class="headerlink" title="Audio-To-Mesh"></a>Audio-To-Mesh</h3><p>在数据预处理的时候，使用Faceverse重建我们的参考图像，从音频中学习非刚性面部表情运动和刚性头部运动来驱动重建的网格。为此，提出了一个多分支的Blendshape和Vertex偏移生成器以及学习头部姿势的codebook，具体如下图所示。</p>
<p><img src="https://picx.zhimg.com/v2-1648982e559021c0b5f5eaa6b201ef93.png" alt="Audio-To-Mesh"></p>
<p><strong>BlendShape and Vertex Offset Generator</strong></p>
<p>对于BlendShape and Vertex Offset Generator来说，首先会使用一个预训练的音频模型来提取音频特征，然后从参考图像中提取身份信息$\alpha$，并且编码为风格信息$z_{style}$，然后在音频特征中嵌入个人风格信息，再结合送到基于多分支的Transformer架构中，一共有三个分支，两个分支生成粗粒度的blendshape，第三个分支生成细粒度的与嘴唇相关的vertex偏移对嘴唇运动进行补充。</p>
<script type="math/tex; mode=display">
\hat{\beta}_i^f=\Phi_i^{bs}(\hat{\beta}_i^{1...f-1},A,z^{style}),\quad i\in\{lip,other\}, \\
\hat{O}_{lip}^f=\Phi_{lip}^{\upsilon o}(\hat{O}_{lip}^{1...f-1},A,z^{style}),</script><p>训练完成后，就可以通过以下方式来进行驱动</p>
<script type="math/tex; mode=display">
\hat{M}_{nr}=(\overline{S}+\alpha U_{id}+(\hat{\beta}_{lip},\hat{\beta}_{other})U_{exp}+\hat{O}_{lip})\otimes P_{ref}.</script><p>这里面的$P_{ref}$为参考图像的<strong>head pose</strong>，$\otimes$是对应的仿射变化。</p>
<p><img src="https://picx.zhimg.com/v2-15f9efd01582593cfaf9a3a5bd765dac.png" alt="BlendShape and Vertex Offset Generator"></p>
<p><strong>Learnable Head Pose Codebook</strong></p>
<p>头部姿势是非常重要的一环，直接从音频中学习还是比较困难的，因为这里面的关系是比较微弱的，因此，使用离散的codebook的，将生成的问题转化为在离散和且有限的姿势空间中查询codebook的任务，设计了两阶段的训练机制。</p>
<p>第一阶段是重建阶段，利用VQ-VAE来构建丰富的头部姿势codebook，是一个编码解码结构。</p>
<script type="math/tex; mode=display">
Z_q=\mathbf{q}(\hat{z})=\underset{z_k\in\mathcal{Z}}{\operatorname*{\arg\min}}\left\|\hat{z}-z_k\right\|. \\
\hat{P}_r^{1:f}=\mathcal{D}(Z_q)=\mathcal{D}(\mathbf{q}(\mathcal{E}(P_r^{1:f}))).</script><p>第二阶段是映射阶段，将输入音频映射到codebook生成最终结果，具体来说，$\Phi_{map}$以音频序列A、特定于人的风格嵌入$z^{style}$和初始头部姿势$P^0$ 作为输入，输出中间特征$\hat Z$，该中间特征将从codebook$Z$量化为$Z_q$，然后由预训练的解码器$D$解码</p>
<script type="math/tex; mode=display">
\hat{P}_r^{1:f}=\mathcal{D}(Z_q)=\mathcal{D}(\mathbf{q}(\Phi_{map}(A,s,P^0))).</script><p>从目前为止，非刚性面部表情运动和刚性头部姿势都已学习。现在我们就可以运用学习到的刚性头部姿势应用于Mesh $\hat{M}<em>{nr}$来获得最最终的驱动网格Mesh $\hat{M}</em>{d}$。</p>
<p><img src="https://pic1.zhimg.com/v2-d9f01fd2be86dc73e859cc5df7c2f7d9.png" alt="Learnable Head Pose Codebook"></p>
<h3 id="Mesh-To-Video"><a href="#Mesh-To-Video" class="headerlink" title="Mesh-To-Video"></a>Mesh-To-Video</h3><p>这一部份是为了将驱动的Mesh转成视频，提出了一个双分支的Motion-VAE对这些2D密集运动进行建模，最后合成最终的视频。</p>
<p>如果要建模2D与3D之间的关系比较难，为了更好的学习，使用投影纹理表示来实现2D的转换。</p>
<p>并且为了更好的学习3D Mesh的纹理，首先在x,y,z三个轴的进行归一化的处理，归一化到0，得到纹理的新表示NCC：</p>
<script type="math/tex; mode=display">
NCC_i=\frac{\overline{S}_i-min(\overline{S}_i)}{max(\overline{S}_i)-min(\overline{S}_i)},\quad i\in\{x,y,z\}.</script><p>然后，使用了Z-Buffer方式和NCC的颜色去渲染3D面度的纹理$PT<em>{in}$，由于3DMM的限制，外表的区域是无法被建模的，所以使用Deep Learning Face Attributes in the Wild 方法解析图像并获得外部面部区域纹理$PT</em>{out}$，例如躯干和背景，将其与$PT_{in}$ 组合如下：</p>
<script type="math/tex; mode=display">
PT=PT_{in}\cdot M+PT_{out}\cdot(1-M)</script><p>其中$M$是内部人脸的Mask，为了进一步增强嘴唇运动并更准确地建模，我们还选择与嘴唇相关的标志并将其转换为高斯图，这是一种更紧凑、更有效的表示。然后，Hourglass网络将减去的高斯图作为输入并输出 2D 嘴唇运动，该运动将与面部运动连接并解码为密集运动和遮挡图。</p>
<p>最后，根据之前预测的密集运动图对参考图像进行变形，获得变形图像，该变形图像将与遮挡图一起作为生成器的输入，逐帧合成最终视频。</p>
<p><img src="https://picx.zhimg.com/v2-bd37230a1f7ac7c875a8b5555d5b43dd.png" alt="Mesh-To-Video"></p>
<h3 id="训练策略"><a href="#训练策略" class="headerlink" title="训练策略"></a>训练策略</h3><p>这几部分实际上都是分开训练的，不过训练后可以通过端到端的方式生成结果。</p>
<p><strong>BlendShape and Vertex Offset Generator</strong>由Blendshape和Mesh重建损失来进行监督</p>
<script type="math/tex; mode=display">
L_{bsvo}=\left\|\beta-\hat{\beta}\right\|+\left\|M-\hat{M}_{nr}\right\|.</script><p><strong>Learnable Head Pose Codebook</strong>部分中，由于量化函数是不可微分的，所以使用straight-through gradient estimator将梯度从解码器复制到编码器，然后对两阶段训练进行如下监督：</p>
<script type="math/tex; mode=display">
\begin{aligned}
L_{rec}= =\left\|P_r^{1:f}-\hat{P}_r^{1:f}\right\|^2+\left\|sg(\mathcal{E}(P_r^{1:f}))-z_q\right\|_2^2  \\
+\left\|sg(z_q)-\mathcal{E}(P_r^{1:f})\right\|_2^2, \\
L_{map} =\left\|P_r^{1:f}-\hat{P}_r^{1:f}\right\|^2+\left\|\hat{Z}-sg(Z_q)\right\|_2^2, 
\end{aligned}</script><p>sg表示停止梯度操作，也就是 <strong>stop gradient</strong></p>
<p><strong>Mesh-To-Video</strong>阶段中，基于预训练的VGG-19 网络的感知损失$L<em>{perc}$被用作主要驱动损失。特征匹配损失 $L</em>{fm}$还用于稳定训练产生更真实的结果。</p>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>接下来总结一下实验的结果方法的对比，该模型使用了HDTF和VoxCeleb数据集，使用Adam优化器，在两个阶段中学习率分别为1e-4和1e-5，最后用8个V100训练了2天得到最终的结果。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>方法</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td>SadTalker</td>
<td>无法生成精确的细节唇部动作</td>
<td>视频质量不佳</td>
</tr>
<tr>
<td>TalkLip</td>
<td>生成模糊结果，皮肤色调稍微偏黄，失去了一定程度的身份信息</td>
<td>质量较差</td>
</tr>
<tr>
<td>MakeItTalk</td>
<td>在交叉身份配音设置中不能生成准确的嘴部形状</td>
<td>嘴部形状不准确</td>
</tr>
<tr>
<td>Wav2Lip</td>
<td>容易合成模糊的口部区域，单一参考图像时输出视频头部姿势和眼部运动静止</td>
<td>视频输出质量较低</td>
</tr>
<tr>
<td>PC-AVS</td>
<td>需要一个驱动视频作为输入，身份保存困难</td>
<td>身份保存困难</td>
</tr>
<tr>
<td>VividTalker</td>
<td>可以生成高质量的说话头像视频，具有准确的唇同步和丰富的面部运动</td>
<td>视频质量高，唇同步准确，面部运动丰富</td>
</tr>
</tbody>
</table>
</div>
<p><img src="https://pic1.zhimg.com/v2-66838829a274884142dde5ee251e190c.png" alt="实验结果"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>VividTalk框架的优点包括：</p>
<ol>
<li><p><strong>高质量的生成视频</strong>：VividTalk能够生成高质量的说话头像视频，具有清晰的面部表情和自然的头部姿势，为用户提供更具沉浸感和真实感的体验。</p>
</li>
<li><p><strong>丰富的表达能力</strong>：通过将混合形状和顶点映射为中间表示，VividTalk能够最大化模型的表达能力，从而呈现出丰富的面部表情，包括细微的细节运动。</p>
</li>
<li><p><strong>灵活的模型设计</strong>：采用多分支生成器，VividTalk能够灵活地对全局和局部面部运动进行建模，使得生成的视频更加生动和自然。</p>
</li>
<li><p><strong>自然的头部姿势合成</strong>：通过引入新颖的可学习的头部姿势码本和两阶段训练机制，VividTalk能够合成更加自然的头部姿势，使得生成的视频更加逼真。</p>
</li>
<li><p><strong>创新的双分支机制</strong>：利用双分支运动-VAE和生成器，VividTalk能够有效地转化驱动网格为密集运动，并用于合成最终视频，提高了生成视频的质量和真实感。</p>
</li>
<li><p><strong>超越性能</strong>：实验证明，VividTalk优于以往的最先进方法，为数字人类创建、视频会议等应用开辟了新的可能性。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>Paperscape</category>
      </categories>
      <tags>
        <tag>Talking Head Generation</tag>
        <tag>NeRF</tag>
      </tags>
  </entry>
  <entry>
    <title>开机自启动登录/认证脚本设置（SZU为例）</title>
    <url>/2024/01/22/Note/AutoLogin/</url>
    <content><![CDATA[<h2 id="开机自启动登录-认证前沿"><a href="#开机自启动登录-认证前沿" class="headerlink" title="开机自启动登录/认证前沿"></a>开机自启动登录/认证前沿</h2><p>有时候在学校或者在企业的时候，会出现这样一种情况，就是我们需要认证才能够上网，但是这种认证并不是非常稳定，有可能会出现断连的情况 </p>
<p>包括有时候电脑关机后自启后也会断掉，针对这种情况，我介绍一种开机自启动登录/认证的脚本，这样能不断的保证联网</p>
<h2 id="开机自启动目录"><a href="#开机自启动目录" class="headerlink" title="开机自启动目录"></a>开机自启动目录</h2><p>首先，我们既然向进行开机自启动，那么我就需要找到开机自启动目录</p>
<p>在Windows中想要开机自启动某些应用，可以把程序的快捷方式放到开始菜单-&gt;程序-&gt;启动目录下，但是自启动又分为用户自启动和系统自启动，前者针对单个用户，后者针对全部用户生效。</p>
<ul>
<li>用户自启动目录：<code>C:\Users\Administrator\AppData\Roaming\Microsoft\Windows\Start Menu\Programs\Startup</code></li>
<li>系统自启动目录：<code>C:\ProgramData\Microsoft\Windows\Start Menu\Programs\StartUp</code></li>
</ul>
<p>这里要根据用户名进行设置和修改，但是当然有更简单的方法对吧，如下，我也推荐这样的方式自动打开我们的开机自启动目录。</p>
<p><strong>快捷命令：按下【win+R】打开运行输入：【shell:Common Startup】</strong></p>
<h2 id="设置脚本运行"><a href="#设置脚本运行" class="headerlink" title="设置脚本运行"></a>设置脚本运行</h2><p>当我们已经找到了开机自启动目录后，我们就可以在这个文件下，写入<code>bat</code>文件，这样每次开机都会自动运行。</p>
<p>首先我们定义<code>drcom.bat</code>，我们希望他能运行一个代码来进行一个检测连接网络情况，这个代码放在了 <code>C盘</code> 的根目录下，我们也可以根据自己情况修改路径放置</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">python C:\\network.py</span><br></pre></td></tr></tbody></table></figure>
<p>所以主要的就是这个代码了，这个代码是用Python写的，原理十分的简单</p>
<p>既然我们需要不断的联网，那我们就不断的看看能否ping通百度，如果ping通了说明联网了，如果没有，说明我们需要运行一个登录的<code>Shell</code>文件</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    exit_code = os.system(<span class="string">'ping www.baidu.com'</span>)</span><br><span class="line">    <span class="keyword">if</span> exit_code != <span class="number">0</span>:</span><br><span class="line">        os.system(<span class="string">r'C:\login_network.sh'</span>)</span><br><span class="line">        time.sleep(<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        time.sleep(<span class="number">10</span>)</span><br></pre></td></tr></tbody></table></figure>
<p>这里面的<code>Shell</code>文件可以是任何登录的脚本和命令，对于<code>SZU</code>来说，脚本如下，只要改为自己的账号和密码即可，这样就完成了开机自启动。</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"Login SZU NetWork"</span></span><br><span class="line">username=<span class="string">"1111111"</span>  &amp;&amp; password=<span class="string">"6666666"</span> &amp;&amp; curl -k https://drcom.szu.edu.cn/a70.htm --data <span class="string">"DDDDD=<span class="variable">$username</span>&amp;upass=<span class="variable">$password</span>&amp;0MKKey=123456"</span>;</span><br></pre></td></tr></tbody></table></figure>
<p>当然，简单的情况下，我们也可以直接在上面的代码<code>network.py</code>里面修改，比如如下</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    exit_code = os.system(<span class="string">'ping www.baidu.com'</span>)</span><br><span class="line">    <span class="keyword">if</span> exit_code != <span class="number">0</span>:</span><br><span class="line">        os.system(<span class="string">r'username="1111111"  &amp;&amp; password="6666666" &amp;&amp; curl -k https://drcom.szu.edu.cn/a70.htm --data "DDDDD=$username&amp;upass=$password&amp;0MKKey=123456";'</span>)</span><br><span class="line">        time.sleep(<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        time.sleep(<span class="number">10</span>)</span><br></pre></td></tr></tbody></table></figure>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过这个脚本实现开机自动登录，解决了开机后无法连接网络的问题。本文介绍了在Windows系统下找到开机自启动目录，并放置检查网络状态和登录认证脚本的方法。</p>
<p>具体来说，使用Python脚本循环ping百度检测网络，如果无法连接则调用登录脚本进行登录。登录脚本可以直接写死账号密码，也可以单独保存为文件引用。</p>
<p>这种方法很简单实用，不需要付出额外精力就可以获得自动登录的功能。尤其是在需要频繁登录校园网或公司WiFi的环境下，可以大大提升效率。只需一次设置，之后就可以享受每次开机即可上网的体验。</p>
<p>总之，通过这个开机自启动脚本，轻松实现了每次开机自动登录网络的需求。给日常工作和学习生活带来了许多便利。</p>
<p>最后感谢木子李的代码提供，感谢感谢！！！</p>
]]></content>
      <categories>
        <category>Note</category>
      </categories>
      <tags>
        <tag>Note</tag>
      </tags>
  </entry>
  <entry>
    <title>超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</title>
    <url>/2024/01/01/Paper/Awesome-Talking-Head-Synthesis/</url>
    <content><![CDATA[<p>Gihub：<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis">https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis</a></p>
<p>这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。</p>
<p>论文合集及发布代码整理。✍️</p>
<p>大多数论文链接到“arXiv”或学术会议/期刊的PDF。但是,一些论文可能需要学术许可才能查看。</p>
<p>这个Awesome Talking Head Synthesis项目将持续更新 - 欢迎Pull Request。如果您有任何论文缺失、新增论文、关键研究人员或错别字建议,请编辑提交PR。您也可以打开Issue或直接通过电子邮件联系我。</p>
<p>如果您觉得这个仓库有用,请star⭐支持!</p>
<p><strong>2023年12月更新 📆</strong></p>
<p>感谢<a href="https://github.com/Curated-Awesome-Lists/awesome-ai-talking-heads">https://github.com/Curated-Awesome-Lists/awesome-ai-talking-heads</a>, 我增加了一些其内容,例如Tools&amp;Software和Slides&amp;Presentations模块。 希望这对您有帮助。</p>
<p>如果您对扩展这个聚合资源有任何想法或反馈,请打开Issue或PR——社区贡献对推进我们共同的知识至关重要。</p>
<p>让我们继续努力,实现更逼真的数字人脸表现!我们已经走了很长一段路,但还有很长的路要走。通过持续的研究和合作,我相信我们一定会达到目标!</p>
<p>如果您觉得这个仓库很有价值,请star⭐并分享给他人。您的支持可以激励我持续改进和维护它。如果您还有任何其他问题,请告诉我!</p>
<p>This repository organizes papers, codes and resources related to generative adversarial networks (GANs) 🤗 and neural radiance fields (NeRF) 🎨, with a main focus on image-driven and audio-driven talking head synthesis papers and released codes. 👤</p>
<p>Papers for Talking Head Synthesis, released codes collections. ✍️</p>
<p>Most papers are linked to PDFs on “arXiv” or journal/conference websites 📚. However, some papers require an academic license to view 🔐.</p>
<p>🔆 This project Awesome-Talking-Head-Synthesis is ongoing - pull requests are welcome! If you have any suggestions (missing papers, new papers, key researchers or typos), please feel free to edit and submit a PR. You can also open an issue or contact me directly via email. 📩</p>
<p>⭐ If you find this repo useful, please give it a star! 🤩</p>
<p><strong>2023.12 Update</strong> 📆</p>
<p>Thank you to <a href="https://github.com/Curated-Awesome-Lists/awesome-ai-talking-heads">https://github.com/Curated-Awesome-Lists/awesome-ai-talking-heads</a>, I have added some of its contents, such as <code>Tools &amp; Software</code> and <code>Slides &amp; Presentations</code>. 🙏 I hope this will be helpful.😊</p>
<p>If you have any feedback or ideas on extending this aggregated resource, please open an issue or PR - community contributions are vital to advancing this shared knowledge. 🤝</p>
<p>Let’s keep pushing forward to recreate ever more realistic digital human faces! 💪 We’ve come so far but still have a long way to go. With continued research 🔬 and collaboration, I’m sure we’ll get there! 🤗</p>
<p>Please feel free to star ⭐ and share this repo if you find it a valuable resource. Your support helps motivate me to keep maintaining and improving it. 🥰 Let me know if you have any other questions!</p>
<h2 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h2><p><img src="https://img-blog.csdnimg.cn/direct/841257d9dee74547bbd4f717794a9492.png#pic_center" alt="在这里插入图片描述"></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Download Link</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Faceforensics++</td>
<td><a href="https://github.com/ondyari/FaceForensics">Download link</a></td>
<td></td>
</tr>
<tr>
<td>CelebV</td>
<td><a href="https://drive.google.com/file/d/1jQ6d76T5GQuvQH4dq8_Wq1T0cxvN0_xp/view">Download link</a></td>
<td></td>
</tr>
<tr>
<td>VoxCeleb</td>
<td><a href="https://www.robots.ox.ac.uk/~vgg/data/voxceleb/">Download link</a></td>
<td><code>VoxCeleb</code>, a comprehensive audio-visual dataset for speaker recognition, encompasses both VoxCeleb1 and VoxCeleb2 datasets.</td>
</tr>
<tr>
<td>VoxCeleb1</td>
<td><a href="https://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1.html">Download link</a></td>
<td><code>VoxCeleb1</code> contains over 100,000 utterances for 1,251 celebrities, extracted from videos uploaded to YouTube.</td>
</tr>
<tr>
<td>VoxCeleb2</td>
<td><a href="https://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox2.html">Download link</a></td>
<td>Extracted from YouTube videos, VoxCeleb2 includes video URLs and discourse timestamps. As the largest public audio-visual dataset, it is primarily used for speaker recognition tasks. However, it can also be utilized for training talking-head generation models. To obtain download permission and access the dataset, apply <a href="https://www.robots.ox.ac.uk/~vgg/data/voxceleb/">here</a>. Requires 300 GB+ storage space.</td>
</tr>
<tr>
<td>ObamaSet</td>
<td><a href="https://github.com/supasorn/synthesizing_obama_network_training">Download link</a></td>
<td><code>ObamaSet</code> is a specialized audio-visual dataset focused on analyzing the visual speech of former US President Barack Obama. All video samples are collected from his weekly address footage. Unlike previous datasets, it exclusively centers on Barack Obama and does not provide any human annotations.</td>
</tr>
<tr>
<td>TalkingHead-1KH</td>
<td><a href="https://github.com/tcwang0509/TalkingHead-1KH">Download link</a></td>
<td>The dataset consists of 500k video clips, of which about 80k are greater than 512x512 resolution. Only videos under permissive licenses are included. Note that the number of videos differ from that in the original paper because a more robust preprocessing script was used to split the videos.</td>
</tr>
<tr>
<td>LRW (Lip Reading in the Wild)</td>
<td><a href="https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrw1.html">Download link</a></td>
<td>LRW, a diverse English-speaking video dataset from the BBC program, features over 1000 speakers with various speaking styles and head poses. Each video is 1.16 seconds long (29 frames) and involves the target word along with context.</td>
</tr>
<tr>
<td>MEAD 2020</td>
<td><a href="https://github.com/uniBruce/Mead">Download link</a></td>
<td>MEAD 2020 is a Talking Head dataset annotated with emotion labels and intensity labels. The dataset focuses on facial generation for natural emotional speech, covering eight different emotions on three intensity levels.</td>
</tr>
<tr>
<td>CelebV-HQ</td>
<td><a href="https://github.com/CelebV-HQ/CelebV-HQ">Download link</a></td>
<td>CelebV-HQ is a high-quality video dataset comprising 35,666 clips with a resolution of at least 512x512. It includes 15,653 identities, and each clip is manually labeled with 83 facial attributes, spanning appearance, action, and emotion. The dataset’s diversity and temporal coherence make it a valuable resource for tasks like unconditional video generation and video facial attribute editing.</td>
</tr>
<tr>
<td>HDTF</td>
<td><a href="https://github.com/MRzzm/HDTF">Download link</a></td>
<td>HDTF, the High-definition Talking-Face Dataset, is a large in-the-wild high-resolution audio-visual dataset consisting of approximately 362 different videos totaling 15.8 hours. Original video resolutions are 720 P or 1080 P, and each cropped video is resized to 512 × 512.</td>
</tr>
<tr>
<td>CREMA-D</td>
<td><a href="https://github.com/CheyneyComputerScience/CREMA-D">Download link</a></td>
<td>CREMA-D is a diverse dataset with 7,442 original clips featuring 91 actors, including 48 male and 43 female actors aged 20 to 74, representing various races and ethnicities. The dataset includes recordings of actors speaking from a set of 12 sentences, expressing six different emotions (Anger, Disgust, Fear, Happy, Neutral, and Sad) at four emotion levels (Low, Medium, High, and Unspecified). Emotion and intensity ratings were gathered through crowd-sourcing, with 2,443 participants rating 90 unique clips each (30 audio, 30 visual, and 30 audio-visual). Over 95% of the clips have more than 7 ratings. For additional details on CREMA-D, refer to the <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4313618/">paper link</a>.</td>
</tr>
<tr>
<td>LRS2</td>
<td><a href="https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrs2.html">Download link</a></td>
<td>LRS2 is a lip reading dataset that includes videos recorded in diverse settings, suitable for studying lip reading and visual speech recognition.</td>
</tr>
<tr>
<td>GRID</td>
<td><a href="http://spandh.dcs.shef.ac.uk/avlombard/">Download link</a></td>
<td>The GRID dataset was recorded in a laboratory setting with 34 volunteers, each speaking 1000 phrases, totaling 34,000 utterance instances. Phrases follow specific rules, with six words randomly selected from six categories: “command,” “color,” “preposition,” “letter,” “number,” and “adverb.” Access the dataset <a href="https://spandh.dcs.shef.ac.uk/gridcorpus/">here</a>.</td>
</tr>
<tr>
<td>SAVEE</td>
<td><a href="http://kahlan.eps.surrey.ac.uk/savee/Download.html">Download link</a></td>
<td>The SAVEE (Surrey Audio-Visual Expressed Emotion) database is a crucial component for developing an automatic emotion recognition system. It features recordings from 4 male actors expressing 7 different emotions, totaling 480 British English utterances. These sentences, selected from the standard TIMIT corpus, are phonetically balanced for each emotion. Recorded in a high-quality visual media lab, the data undergoes processing and labeling. Performance evaluation involves 10 subjects rating recordings under audio, visual, and audio-visual conditions. Classification systems for each modality achieve speaker-independent recognition rates of 61%, 65%, and 84% for audio, visual, and audio-visual, respectively.</td>
</tr>
<tr>
<td>BIWI(3D)</td>
<td><a href="https://data.vision.ee.ethz.ch/cvl/datasets/b3dac2.en.html">Download link</a></td>
<td>The Biwi 3D Audiovisual Corpus of Affective Communication serves as a compromise between data authenticity and quality, acquired at ETHZ in collaboration with SYNVO GmbH.</td>
</tr>
<tr>
<td>VOCA</td>
<td><a href="https://voca.is.tue.mpg.de/">Download link</a></td>
<td>VOCA is a 4D-face dataset with approximately 29 minutes of 4D face scans and synchronized audio from 12-bit speakers. It greatly facilitates research in 3D VSG.</td>
</tr>
<tr>
<td>Multiface(3D)</td>
<td><a href="https://github.com/facebookresearch/multiface">Download link</a></td>
<td>The Multiface Dataset consists of high-quality multi-view video recordings of 13 people displaying various facial expressions. It contains approximately 12,200 to 23,000 frames per subject, captured at 30 fps from around 40 to 160 camera views with uniform lighting. The dataset’s size is 65TB and includes raw images (2048x1334 resolution), tracked and meshed heads, 1024x1024 unwrapped face textures, camera calibration metadata, and audio. This repository provides code for downloading the dataset and building a codec avatar using a deep appearance model.</td>
</tr>
<tr>
<td>MMFace4D</td>
<td><a href="https://wuhaozhe.github.io/mmface4d/">Download link</a></td>
<td>The MMFace4D dataset is a large-scale multi-modal dataset for audio-driven 3D facial animation research. It contains over 35,000 sequences captured from 431 subjects ranging in age from 15 to 68 years old. Various sentences from scenarios such as news broadcasting, conversations and storytelling were recorded, totaling around 11,000 utterances. High-fidelity data was captured using three synchronized RGB-D cameras to obtain high-resolution 3D meshes and textures. A reconstruction pipeline was developed to fuse the multi-view data and generate topology-consistent 3D mesh sequences. In addition to the 3D facial motions, synchronized speech audio is also provided. The final dataset covers a wide range of expressive talking styles and facial expressions through a diverse set of subjects and utterances. With its large scale, high quality of data and strong diversity, the MMFace4D dataset provides an ideal benchmark for developing and evaluating audio-driven 3D facial animation models.</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h2 id="Survey"><a href="#Survey" class="headerlink" title="Survey"></a>Survey</h2><div class="table-container">
<table>
<thead>
<tr>
<th>Year</th>
<th>Title</th>
<th>Conference/Journal</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024</td>
<td><a href="https://arxiv.org/pdf/2401.03890.pdf">A Survey on 3D Gaussian Splatting</a> 3DGS🔥🔥🔥<strong>on going</strong></td>
<td>arXiv 2024</td>
</tr>
<tr>
<td>2024</td>
<td><a href="https://arxiv.org/pdf/2304.10050.pdf">Neural Radiance Fields: Past, Present, and Future</a>  NeRF🔥🔥🔥 <strong>Amazing 413 pages</strong></td>
<td>arXiv 2024</td>
</tr>
<tr>
<td>2023</td>
<td><a href="https://arxiv.org/abs/2308.16041">From Pixels to Portraits: A Comprehensive Survey of Talking Head Generation Techniques and Applications</a></td>
<td>arXiv 2023</td>
</tr>
<tr>
<td>2023</td>
<td><a href="https://www.mdpi.com/2079-9292/12/1/218">Human-Computer Interaction System: A Survey of Talking-Head Generation</a></td>
<td>IEEE</td>
</tr>
<tr>
<td>2023</td>
<td><a href="https://dl.acm.org/doi/10.1016/j.eswa.2023.119678">Talking human face generation: A survey</a></td>
<td>ACM</td>
</tr>
<tr>
<td>2022</td>
<td><a href="https://arxiv.org/abs/2205.10839">Deep Learning for Visual Speech Analysis: A Survey</a></td>
<td>arXiv 2022</td>
</tr>
<tr>
<td>2020</td>
<td><a href="https://arxiv.org/abs/2005.03201">What comprises a good talking-head video generation?: A Survey and Benchmark</a></td>
<td>arXiv 2020</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h2 id="Funny-Work"><a href="#Funny-Work" class="headerlink" title="Funny Work"></a>Funny Work</h2><div class="table-container">
<table>
<thead>
<tr>
<th>Year</th>
<th>Title</th>
<th>Code</th>
<th>Project</th>
<th>Keywords</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024</td>
<td>[Audio2Photoreal] <a href="https://arxiv.org/pdf/2401.01885.pdf">From Audio to Photoreal Embodiment: Synthesizing Humans in Conversations</a></td>
<td><a href="https://github.com/facebookresearch/audio2photoreal/">Code</a></td>
<td><a href="https://people.eecs.berkeley.edu/~evonne_ng/projects/audio2photoreal/#">Project</a></td>
<td>Photoreal</td>
</tr>
<tr>
<td>2024</td>
<td>[Animate Anyone] <a href="https://arxiv.org/pdf/2311.17117.pdf">Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation</a></td>
<td><a href="https://github.com/HumanAIGC/AnimateAnyone">Code</a></td>
<td><a href="https://humanaigc.github.io/animate-anyone/">Project</a></td>
<td>🔥Animate (阿里科目三驱动)</td>
</tr>
<tr>
<td>2024</td>
<td>[3DGAN] <a href="https://research.nvidia.com/labs/nxp/wysiwyg/media/WYSIWYG.pdf">What You See Is What You GAN: Rendering Every Pixel for High-Fidelity Geometry in 3D GANs</a></td>
<td></td>
<td><a href="https://research.nvidia.com/labs/nxp/wysiwyg/">Project</a></td>
<td>🔥Nvidia</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h2 id="Audio-driven"><a href="#Audio-driven" class="headerlink" title="Audio-driven"></a>Audio-driven</h2><div class="table-container">
<table>
<thead>
<tr>
<th>Year</th>
<th>Title</th>
<th>Conference/Journal</th>
<th>Code</th>
<th>Project</th>
<th>Keywords</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024</td>
<td>[Real3D-Portrait] <a href="http://arxiv.org/abs/2401.08503v2">Real3D-Portrait: One-shot Realistic 3D Talking Portrait Synthesis</a></td>
<td>ICLR 2024</td>
<td></td>
<td><a href="https://real3dportrait.github.io/">Project</a></td>
<td>3D, One-Shot,Realistic</td>
</tr>
<tr>
<td>2024</td>
<td>[AdaMesh] <a href="http://arxiv.org/abs/2310.07236v2">AdaMesh: Personalized Facial Expressions and Head Poses for Adaptive   Speech-Driven 3D Facial Animation</a></td>
<td>Arix 2024</td>
<td><a href="https://github.com/adamesh/adamesh">Code</a></td>
<td><a href="https://adamesh.github.io">Project</a></td>
<td>3D,Mesh</td>
</tr>
<tr>
<td>2024</td>
<td>[DREAM-Talk] <a href="http://arxiv.org/abs/2312.13578v1">DREAM-Talk: Diffusion-based Realistic Emotional Audio-driven Method for Single Image Talking Face Generation</a></td>
<td>Arix 2024</td>
<td></td>
<td><a href="https://magic-research.github.io/dream-talk/">Project</a></td>
<td>Emotion</td>
</tr>
<tr>
<td>2024</td>
<td>[AE-NeRF] <a href="http://arxiv.org/abs/2312.10921v1">AE-NeRF: Audio Enhanced Neural Radiance Field for Few Shot Talking Head Synthesis</a></td>
<td>AAAI 2024</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>[VectorTalker] <a href="http://arxiv.org/abs/2312.11568v1">VectorTalker: SVG Talking Face Generation with Progressive Vectorisation</a></td>
<td>Arix 2024</td>
<td></td>
<td></td>
<td>SVG</td>
</tr>
<tr>
<td>2024</td>
<td>[VectorTalker] <a href="http://arxiv.org/abs/2312.11568v1">VectorTalker: SVG Talking Face Generation with Progressive Vectorisation</a></td>
<td>Arix 2024</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>[Mimic] <a href="http://arxiv.org/abs/2312.10877v1">Mimic: Speaking Style Disentanglement for Speech-Driven 3D Facial Animation</a></td>
<td>AAAI 2024</td>
<td></td>
<td></td>
<td>3D</td>
</tr>
<tr>
<td>2024</td>
<td>[DreamTalk] <a href="http://arxiv.org/abs/2312.09767v1">DreamTalk: When Expressive Talking Head Generation Meets Diffusion Probabilistic Models</a></td>
<td>Arix 2024</td>
<td><a href="https://github.com/damo-vilab/i2vgen-xl">Code</a></td>
<td><a href="https://dreamtalk-project.github.io">Project</a></td>
<td>Diffusion</td>
</tr>
<tr>
<td>2024</td>
<td>[FaceTalk] <a href="http://arxiv.org/abs/2312.08459v1">FaceTalk: Audio-Driven Motion Diffusion for Neural Parametric Head Models</a></td>
<td>Arix 2024</td>
<td><a href="https://github.com/shivangi-aneja/FaceTalk">Code</a></td>
<td><a href="https://shivangi-aneja.github.io/projects/facetalk/">Project</a></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>[GSmoothFace] <a href="http://arxiv.org/abs/2312.07385v1">GSmoothFace: Generalized Smooth Talking Face Generation via Fine Grained   3D Face Guidance</a></td>
<td>Arix 2024</td>
<td></td>
<td></td>
<td>3D</td>
</tr>
<tr>
<td>2024</td>
<td>[GMTalker] <a href="http://arxiv.org/abs/2312.07669v1">GMTalker: Gaussian Mixture based Emotional talking video Portraits</a></td>
<td>Arix 2024</td>
<td></td>
<td><a href="https://bob35buaa.github.io/GMTalker">Project</a></td>
<td>Emotion</td>
</tr>
<tr>
<td>2024</td>
<td>[VividTalk] <a href="http://arxiv.org/abs/2312.01841v2">VividTalk: One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior</a></td>
<td>Arix 2024</td>
<td></td>
<td></td>
<td>Mesh</td>
</tr>
<tr>
<td>2024</td>
<td>[GAIA] <a href="https://arxiv.org/pdf/2311.15230.pdf">GAIA: Zero-shot Talking Avatar Generation</a></td>
<td>Arix 2024</td>
<td>Code(coming)</td>
<td><a href="https://microsoft.github.io/GAIA/">Project</a></td>
<td>😲😲😲</td>
</tr>
<tr>
<td>2023</td>
<td><a href="https://arxiv.org/abs/2307.10008">Implicit Identity Representation Conditioned Memory Compensation Network for Talking Head Video Generation</a></td>
<td>ICCV 2023</td>
<td><a href="https://github.com/harlanhong/ICCV2023-MCNET">Code</a></td>
<td><a href="https://harlanhong.github.io/publications/mcnet.html">Project</a></td>
<td>-</td>
</tr>
<tr>
<td>2023</td>
<td>[ToonTalker] <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Gong_ToonTalker_Cross-Domain_Face_Reenactment_ICCV_2023_paper.pdf">ToonTalker: Cross-Domain Face Reenactment</a></td>
<td>ICCV 2023</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>2023</td>
<td><a href="https://arxiv.org/abs/2309.04946">Efficient Emotional Adaptation for Audio-Driven Talking-Head Generation</a></td>
<td>ICCV 2023</td>
<td><a href="https://github.com/yuangan/EAT_code">Code</a></td>
<td><a href="https://yuangan.github.io/eat/">Project</a></td>
<td>-</td>
</tr>
<tr>
<td>2023</td>
<td>[EMMN] <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Tan_EMMN_Emotional_Motion_Memory_Network_for_Audio-driven_Emotional_Talking_Face_ICCV_2023_paper.pdf">EMMN: Emotional Motion Memory Network for Audio-driven Emotional Talking Face Generation</a></td>
<td>ICCV 2023</td>
<td>-</td>
<td>-</td>
<td>Emotion</td>
</tr>
<tr>
<td>2023</td>
<td><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Song_Emotional_Listener_Portrait_Neural_Listener_Head_Generation_with_Emotion_ICCV_2023_paper.pdf">Emotional Listener Portrait: Realistic Listener Motion Simulation in Conversation</a></td>
<td>ICCV 2023</td>
<td>-</td>
<td>-</td>
<td>Emotion,LHG</td>
</tr>
<tr>
<td>2023</td>
<td>[MODA] <a href="https://arxiv.org/abs/2307.10008">MODA: Mapping-Once Audio-driven Portrait Animation with Dual Attentions</a></td>
<td>ICCV 2023</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>2023</td>
<td>[Facediffuser] <a href="https://dl.acm.org/doi/abs/10.1145/3623264.3624447">Facediffuser: Speech-driven 3d facial animation synthesis using diffusion</a></td>
<td>ACM SIGGRAPH MIG 2023</td>
<td><a href="https://github.com/uuembodiedsocialai/FaceDiffuser">Code</a></td>
<td><a href="https://uuembodiedsocialai.github.io/FaceDiffuser/">Project</a></td>
<td>🔥Diffusion,3D</td>
</tr>
<tr>
<td>2023</td>
<td><a href="https://arxiv.org/abs/2309.00030">Audio-Driven Dubbing for User Generated Contents via Style-Aware Semi-Parametric Synthesis</a></td>
<td>TCSVT 2023</td>
<td>-</td>
<td>-</td>
<td></td>
</tr>
<tr>
<td>2023</td>
<td>[SadTalker] <a href="https://arxiv.org/pdf/2211.12194.pdf">SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation</a></td>
<td>CVPR 2023</td>
<td><a href="https://github.com/Winfredy/SadTalker">Code</a></td>
<td><a href="https://sadtalker.github.io/">Project</a></td>
<td>3D,Single Image</td>
</tr>
<tr>
<td>2023</td>
<td>[EmoTalk] <a href="https://arxiv.org/abs/2303.11089">EmoTalk: Speech-Driven Emotional Disentanglement for 3D Face Animation</a></td>
<td>ICCV 2023</td>
<td><a href="https://github.com/ZiqiaoPeng/EmoTalk">Code</a></td>
<td></td>
<td>3D,Emotion</td>
</tr>
<tr>
<td>2023</td>
<td><a href="https://arxiv.org/abs/2306.03594">Emotional Talking Head Generation based on Memory-Sharing and Attention-Augmented Networks</a></td>
<td>InterSpeech 2023</td>
<td></td>
<td></td>
<td>Emotion</td>
</tr>
<tr>
<td>2023</td>
<td>[DINet] <a href="https://fuxivirtualhuman.github.io/pdf/AAAI2023_FaceDubbing.pdf">DINet: Deformation Inpainting Network for Realistic Face Visually Dubbing on High Resolution Video</a></td>
<td>AAAI 2023</td>
<td><a href="https://github.com/MRzzm/DINet">Code</a></td>
<td>-</td>
<td></td>
</tr>
<tr>
<td>2023</td>
<td>[StyleTalk] <a href="https://arxiv.org/abs/2301.01081">StyleTalk: One-shot Talking Head Generation with Controllable Speaking Styles</a></td>
<td>AAAI 2023</td>
<td><a href="https://github.com/FuxiVirtualHuman/styletalk">Code</a></td>
<td>-</td>
<td>Style</td>
</tr>
<tr>
<td>2023</td>
<td><a href="https://arxiv.org/abs/2305.02572">High-fidelity Generalized Emotional Talking Face Generation with Multi-modal Emotion Space Learning</a></td>
<td>CVPR 2023</td>
<td>-</td>
<td>-</td>
<td>Emotion</td>
</tr>
<tr>
<td>2023</td>
<td>[StyleSync] <a href="https://arxiv.org/pdf/2305.05445.pdf">StyleSync: High-Fidelity Generalized and Personalized Lip Sync in Style-based Generator</a></td>
<td>CVPR 2023</td>
<td><a href="https://github.com/guanjz20/StyleSync">Code</a></td>
<td><a href="https://hangz-nju-cuhk.github.io/projects/StyleSync">Project</a></td>
<td>-</td>
</tr>
<tr>
<td>2023</td>
<td>[TalkLip] <a href="https://arxiv.org/pdf/2303.17480.pdf">TalkLip: Seeing What You Said - Talking Face Generation Guided by a Lip Reading Expert</a></td>
<td>CVPR 2023</td>
<td><a href="https://github.com/Sxjdwang/TalkLip">Code</a></td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>2023</td>
<td>[CodeTalker] <a href="https://arxiv.org/abs/2301.02379">CodeTalker: Speech-Driven 3D Facial Animation with Discrete Motion Prior</a></td>
<td>CVPR 2023</td>
<td><a href="https://github.com/Doubiiu/CodeTalker">Code</a></td>
<td><a href="https://doubiiu.github.io/projects/codetalker/">Project</a></td>
<td>3D,codebook</td>
</tr>
<tr>
<td>2023</td>
<td>[EmoGen] <a href="https://arxiv.org/pdf/2303.11548.pdf">Emotionally Enhanced Talking Face Generation</a></td>
<td>Arxiv 2023</td>
<td><a href="https://github.com/sahilg06/EmoGen">Code</a></td>
<td>-</td>
<td>Emotion</td>
</tr>
<tr>
<td>2023</td>
<td>[DAE-Talker] <a href="https://arxiv.org/pdf/2303.17550.pdf">DAE-Talker: High Fidelity Speech-Driven Talking Face Generation with Diffusion Autoencoder</a></td>
<td>Arxiv 2023</td>
<td>-</td>
<td><a href="https://mstypulkowski.github.io/diffusedheads/">Project</a></td>
<td>🔥Diffusion</td>
</tr>
<tr>
<td>2023</td>
<td>[READ] <a href="READ Avatars: Realistic Emotion-controllable Audio Driven Avatars">READ Avatars: Realistic Emotion-controllable Audio Driven Avatars</a></td>
<td>Arxiv 2023</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>2023</td>
<td>[DiffTalk] <a href="https://arxiv.org/abs/2301.03786">DiffTalk: Crafting Diffusion Models for Generalized Talking Head Synthesis</a></td>
<td>CVPR 2023</td>
<td><a href="https://github.com/sstzal/DiffTalk">Code</a></td>
<td><a href="https://sstzal.github.io/DiffTalk/">Project</a></td>
<td>🔥Diffusion</td>
</tr>
<tr>
<td>2023</td>
<td>[Diffused Heads] <a href="https://mstypulkowski.github.io/diffusedheads/diffused_heads.pdf">Diffused Heads: Diffusion Models Beat GANs on Talking-Face Generation</a></td>
<td>Arxiv 2023</td>
<td>-</td>
<td><a href="https://mstypulkowski.github.io/diffusedheads/">Project</a></td>
<td>🔥Diffusion</td>
</tr>
<tr>
<td>2022</td>
<td>[MemFace] <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Liang_Expressive_Talking_Head_Generation_With_Granular_Audio-Visual_Control_CVPR_2022_paper.pdf">Expressive Talking Head Generation with Granular Audio-Visual Control</a></td>
<td>CVPR 2022</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>2022</td>
<td><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Song_Talking_Face_Generation_With_Multilingual_TTS_CVPR_2022_paper.pdf">Talking Face Generation with Multilingual TTS</a></td>
<td>CVPR 2022</td>
<td><a href="https://huggingface.co/spaces/CVPR/ml-talking-face">Demo Track</a></td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>2022</td>
<td>[EAMM] <a href="https://arxiv.org/pdf/2205.15278.pdf">EAMM: One-Shot Emotional Talking Face via Audio-Based Emotion-Aware Motion Model</a></td>
<td>SIGGRAPH 2022</td>
<td>-</td>
<td>-</td>
<td>Emotion</td>
</tr>
<tr>
<td>2022</td>
<td>[SPACEx] <a href="https://arxiv.org/pdf/2211.09809.pdf">SPACEx 🚀: Speech-driven Portrait Animation with Controllable Expression</a></td>
<td>arXiv 2022</td>
<td>-</td>
<td><a href="https://deepimagination.cc/SPACEx/">Project</a></td>
<td>-</td>
</tr>
<tr>
<td>2022</td>
<td>[AV-CAT] <a href="https://arxiv.org/pdf/2212.04970.pdf">Masked Lip-Sync Prediction by Audio-Visual Contextual Exploitation in Transformers</a></td>
<td>SIGGRAPH Asia 2022</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>2022</td>
<td>[MemFace] <a href="https://arxiv.org/pdf/2212.05005.pdf">Memories are One-to-Many Mapping Alleviators in Talking Face Generation</a></td>
<td>arXiv 2022</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>2021</td>
<td>[PC-AVS] <a href="https://arxiv.org/abs/2104.11116">PC-AVS: Pose-Controllable Talking Face Generation by Implicitly Modularized Audio-Visual Representation</a></td>
<td>CVPR 2021</td>
<td><a href="https://github.com/Hangz-nju-cuhk/Talking-Face_PC-AVS">Code</a></td>
<td><a href="https://hangz-nju-cuhk.github.io/projects/PC-AVS">Project</a></td>
<td>-</td>
</tr>
<tr>
<td>2021</td>
<td>[IATS] <a href="https://arxiv.org/abs/2111.00203">Imitating Arbitrary Talking Style for Realistic Audio-Driven Talking Face Synthesis</a></td>
<td>ACM MM 2021</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>2021</td>
<td>[Speech2Talking-Face] <a href="https://www.ijcai.org/proceedings/2021/0141.pdf">Speech2Talking-Face: Inferring and Driving a Face with Synchronized Audio-Visual Representation</a></td>
<td>IJCAI 2021</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>2021</td>
<td>[FAU] <a href="https://arxiv.org/pdf/2110.09951.pdf">Talking Head Generation with Audio and Speech Related Facial Action Units</a></td>
<td>BMVC 2021</td>
<td>-</td>
<td>-</td>
<td>AU</td>
</tr>
<tr>
<td>2021</td>
<td>[EVP] <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Ji_Audio-Driven_Emotional_Video_Portraits_CVPR_2021_paper.pdf">Audio-Driven Emotional Video Portraits</a></td>
<td>CVPR 2021</td>
<td><a href="https://github.com/jixinya/EVP">Code</a></td>
<td>-</td>
<td>Emotion</td>
</tr>
<tr>
<td>2021</td>
<td>[IATS] <a href="https://dl.acm.org/doi/pdf/10.1145/3474085.3475280">IATS: Imitating Arbitrary Talking Style for Realistic Audio-Driven Talking Face Synthesis</a></td>
<td>ACM Multimedia 2021</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>2020</td>
<td>[Wav2Lip] <a href="http://arxiv.org/pdf/2008.10010.pdf">A Lip Sync Expert Is All You Need for Speech to Lip Generation In The Wild</a></td>
<td>ACM Multimedia 2020</td>
<td><a href="https://github.com/Rudrabha/Wav2Lip">Code</a></td>
<td><a href="http://cvit.iiit.ac.in/research/projects/cvit-projects/a-lip-sync-expert-is-all-you-need-for-speech-to-lip-generation-in-the-wild/">Project</a></td>
<td>-</td>
</tr>
<tr>
<td>2020</td>
<td>[RhythmicHead] <a href="https://arxiv.org/pdf/2007.08547v1.pdf">Talking-head Generation with Rhythmic Head Motion</a></td>
<td>ECCV 2020</td>
<td><a href="https://github.com/lelechen63/Talking-head-Generation-with-Rhythmic-Head-Motion">Code</a></td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>2020</td>
<td>[MakeItTalk] <a href="https://arxiv.org/pdf/2006.09661.pdf">Speaker-Aware Talking-Head Animation</a></td>
<td>SIGGRAPH Asia 2020</td>
<td><a href="https://github.com/yzhou359/MakeItTalk">Code</a></td>
<td><a href="https://people.umass.edu/~yangzhou/MakeItTalk/">Project</a></td>
<td>-</td>
</tr>
<tr>
<td>2020</td>
<td>[Neural Voice Puppetry] <a href="https://arxiv.org/pdf/1912.05566.pdf">Audio-driven Facial Reenactment</a></td>
<td>ECCV 2020</td>
<td>-</td>
<td><a href="https://justusthies.github.io/posts/neural-voice-puppetry/">Project</a></td>
<td>-</td>
</tr>
<tr>
<td>2020</td>
<td>[MEAD] <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660698.pdf">A Large-scale Audio-visual Dataset for Emotional Talking-face Generation</a></td>
<td>ECCV 2020</td>
<td><a href="https://github.com/uniBruce/Mead">Code</a></td>
<td><a href="https://wywu.github.io/projects/MEAD/MEAD.html">Project</a></td>
<td>-</td>
</tr>
<tr>
<td>2020</td>
<td><a href="https://arxiv.org/pdf/1906.06337.pdf">Realistic Speech-Driven Facial Animation with GANs</a></td>
<td>IJCV 2020</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>2019</td>
<td>[DAVS] <a href="https://arxiv.org/pdf/1807.07860.pdf">Talking Face Generation by Adversarially Disentangled Audio-Visual Representation</a></td>
<td>AAAI 2019</td>
<td><a href="https://github.com/Hangz-nju-cuhk/Talking-Face-Generation-DAVS">Code</a></td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>2019</td>
<td>[ATVGnet] <a href="https://www.cs.rochester.edu/~cxu22/p/cvpr2019_facegen_paper.pdf">Hierarchical Cross-modal Talking Face Generation with Dynamic Pixel-wise Loss</a></td>
<td>CVPR 2019</td>
<td><a href="https://github.com/lelechen63/ATVGnet">Code</a></td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>2018</td>
<td><a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Lele_Chen_Lip_Movements_Generation_ECCV_2018_paper.pdf">Lip Movements Generation at a Glance</a></td>
<td>ECCV 2018</td>
<td><a href="https://github.com/lelechen63/3d_gan">Code</a></td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>2018</td>
<td>[VisemeNet] <a href="https://arxiv.org/pdf/1805.09488.pdf">Audio-Driven Animator-Centric Speech Animation</a></td>
<td>SIGGRAPH 2018</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>2017</td>
<td>[Synthesizing Obama] <a href="https://grail.cs.washington.edu/projects/AudioToObama/siggraph17_obama.pdf">Learning Lip Sync From Audio</a></td>
<td>SIGGRAPH 2017</td>
<td>-</td>
<td><a href="https://grail.cs.washington.edu/projects/AudioToObama/">Project</a></td>
<td>-</td>
</tr>
<tr>
<td>2017</td>
<td>[You Said That?] <a href="https://arxiv.org/abs/1705.02966">Synthesising Talking Faces From Audio</a></td>
<td>BMVC 2019</td>
<td><a href="https://github.com/joonson/yousaidthat">Code</a></td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>2017</td>
<td><a href="https://users.aalto.fi/~laines9/publications/karras2017siggraph_paper.pdf">Audio-Driven Facial Animation by Joint End-to-End Learning of Pose and Emotion</a></td>
<td>SIGGRAPH 2017</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>2017</td>
<td><a href="https://home.ttic.edu/~taehwan/taylor_etal_siggraph2017.pdf">A Deep Learning Approach for Generalized Speech Animation</a></td>
<td>SIGGRAPH 2017</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>2016</td>
<td>[LRW] <a href="https://www.robots.ox.ac.uk/~vgg/publications/2016/Chung16/chung16.pdf">Lip Reading in the Wild</a></td>
<td>ACCV 2016</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h2 id="Text-driven"><a href="#Text-driven" class="headerlink" title="Text-driven"></a>Text-driven</h2><div class="table-container">
<table>
<thead>
<tr>
<th>Year</th>
<th>Title</th>
<th>Conference/Journal</th>
<th>Code/Proj</th>
</tr>
</thead>
<tbody>
<tr>
<td>2023</td>
<td><a href="https://arxiv.org/pdf/2304.00334.pdf">TalkCLIP: Talking Head Generation with Text-Guided Expressive Speaking Styles</a></td>
<td>Arxiv</td>
<td></td>
</tr>
<tr>
<td>2021</td>
<td><a href="https://arxiv.org/abs/2104.07995">Write-a-speaker: Text-based Emotional and Rhythmic Talking-head Generation</a></td>
<td>AAAI</td>
<td><a href="https://github.com/FuxiVirtualHuman/Write-a-Speaker">Code</a></td>
</tr>
<tr>
<td>2021</td>
<td><a href="https://arxiv.org/abs/2106.14014v3">Txt2vid: Ultra-low bitrate compression of talking-head videos via text</a></td>
<td>Arxiv</td>
<td><a href="https://github.com/tpulkit/txt2vid">Code</a></td>
</tr>
</tbody>
</table>
</div>
<hr>
<h2 id="NeRF-amp-3D-amp-Gaussian-Splatting"><a href="#NeRF-amp-3D-amp-Gaussian-Splatting" class="headerlink" title="NeRF &amp; 3D &amp; Gaussian Splatting"></a>NeRF &amp; 3D &amp; Gaussian Splatting</h2><div class="table-container">
<table>
<thead>
<tr>
<th>Year</th>
<th>Title</th>
<th>Conference/Journal</th>
<th>Code</th>
<th>Project</th>
<th>Keywords</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024</td>
<td>[UltrAvatar] <a href="http://arxiv.org/abs/2401.11078v1">UltrAvatar: A Realistic Animatable 3D Avatar Diffusion Model with Authenticity Guided Textures</a></td>
<td>Arxiv 2024</td>
<td></td>
<td><a href="http://usrc-sea.github.io/UltrAvatar/">Project</a></td>
<td>Diffusion,Avatar</td>
</tr>
<tr>
<td>2024</td>
<td>[GaussianBody] <a href="http://arxiv.org/abs/2401.09720v1">GaussianBody: Clothed Human Reconstruction via 3d Gaussian Splatting</a></td>
<td>Arxiv 2024</td>
<td></td>
<td></td>
<td>🔥Gaussian Splatting</td>
</tr>
<tr>
<td>2024</td>
<td><a href="http://arxiv.org/abs/2401.02616v1">FED-NeRF: Achieve High 3D Consistency and Temporal Coherence for Face Video Editing on Dynamic NeRF</a></td>
<td></td>
<td><a href="https://github.com/ZHANG1023/FED-NeRF">Code</a></td>
<td></td>
<td>4D face video editor</td>
</tr>
<tr>
<td>2024</td>
<td>[AGG] <a href="http://arxiv.org/abs/2401.04099v1">AGG: Amortized Generative 3D Gaussians for Single Image to 3D</a></td>
<td>Arxiv 2024</td>
<td></td>
<td><a href="https://ir1d.github.io/AGG/">Project</a></td>
<td>🔥Gaussian Splatting</td>
</tr>
<tr>
<td>2024</td>
<td><a href="http://arxiv.org/abs/2401.06116v1">Gaussian Shadow Casting for Neural Characters</a></td>
<td>Arxiv 2024</td>
<td></td>
<td></td>
<td>🔥Gaussian Splatting</td>
</tr>
<tr>
<td>2024</td>
<td>[Human101] <a href="http://arxiv.org/abs/2312.15258v1">Human101: Training 100+FPS Human Gaussians in 100s from 1 View</a></td>
<td>Arxiv 2024</td>
<td></td>
<td><a href="https://github.com/longxiang-ai/Human101">Project</a></td>
<td>🔥Gaussian Splatting</td>
</tr>
<tr>
<td>2024</td>
<td><a href="http://arxiv.org/abs/2312.15059v1">Deformable 3D Gaussian Splatting for Animatable Human Avatars</a></td>
<td>Arxiv 2024</td>
<td></td>
<td></td>
<td>🔥Gaussian Splatting</td>
</tr>
<tr>
<td>2024</td>
<td>[4DGen] <a href="http://arxiv.org/abs/2312.17225v1">4DGen: Grounded 4D Content Generation with Spatial-temporal Consistency</a></td>
<td>Arxiv 2024</td>
<td></td>
<td><a href="https://vita-group.github.io/4DGen/">Project</a></td>
<td>🔥Gaussian Splatting</td>
</tr>
<tr>
<td>2024</td>
<td>[3DGAN] <a href="https://research.nvidia.com/labs/nxp/wysiwyg/media/WYSIWYG.pdf">What You See Is What You GAN: Rendering Every Pixel for High-Fidelity Geometry in 3D GANs</a></td>
<td>Arxiv 2024</td>
<td></td>
<td><a href="https://research.nvidia.com/labs/nxp/wysiwyg/">Project</a></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>[3DGS-Avatar] <a href="http://arxiv.org/abs/2312.09228v2">3DGS-Avatar: Animatable Avatars via Deformable 3D Gaussian Splatting</a></td>
<td>Arxiv 2024</td>
<td><a href="https://github.com/mikeqzy/3dgs-avatar-release">Code</a></td>
<td><a href="https://neuralbodies.github.io/3DGS-Avatar">Project</a></td>
<td>🔥Gaussian Splatting</td>
</tr>
<tr>
<td>2024</td>
<td><a href="http://arxiv.org/abs/2312.10422v2">Learning Dense Correspondence for NeRF-Based Face Reenactment</a></td>
<td>AAAI 2024</td>
<td></td>
<td></td>
<td>one-shot multi-view face reenactmen</td>
</tr>
<tr>
<td>2024</td>
<td>[R2-Talker] <a href="http://arxiv.org/abs/2312.05572v1">R2-Talker: Realistic Real-Time Talking Head Synthesis with Hash Grid Landmarks Encoding and Progressive Multilayer Conditioning</a></td>
<td>Arxiv 2024</td>
<td></td>
<td></td>
<td>based-RAD-NeRF</td>
</tr>
<tr>
<td>2024</td>
<td>[GaussianHead] <a href="https://arxiv.org/abs/2312.01632">GaussianHead: Impressive 3D Gaussian-based Head Avatars with Dynamic Hybrid Neural Field</a></td>
<td>Arxiv 2024</td>
<td><a href="https://github.com/chiehwangs/gaussian-head">Code</a></td>
<td></td>
<td>🔥Gaussian Splatting</td>
</tr>
<tr>
<td>2024</td>
<td>[MonoGaussianAvatar] <a href="https://arxiv.org/abs/2312.00846">MonoGaussianAvatar: Monocular Gaussian Point-based Head Avatar</a></td>
<td>Arxiv 2024</td>
<td></td>
<td></td>
<td>🔥Gaussian Splatting</td>
</tr>
<tr>
<td>2024</td>
<td>[Gaussian Head Avatar] <a href="http://arxiv.org/abs/2312.03029v1">Gaussian Head Avatar: Ultra High-fidelity Head Avatar via Dynamic Gaussians</a></td>
<td>Arxiv 2024</td>
<td><a href="https://github.com/YuelangX/Gaussian-Head-Avatar">Code</a></td>
<td><a href="https://yuelangx.github.io/gaussianheadavatar/">Project</a></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>[HeadGaS] <a href="http://arxiv.org/abs/2312.02902v1">HeadGaS: Real-Time Animatable Head Avatars via 3D Gaussian Splatting</a></td>
<td>Arxiv 2024</td>
<td></td>
<td></td>
<td>🔥Gaussian Splatting</td>
</tr>
<tr>
<td>2024</td>
<td>[GaussianAvatars] <a href="http://arxiv.org/abs/2312.02069v1">GaussianAvatars: Photorealistic Head Avatars with Rigged 3D Gaussians</a></td>
<td>Arxiv 2024</td>
<td></td>
<td><a href="https://shenhanqian.github.io/gaussian-avatars">Project</a></td>
<td>🔥Gaussian Splatting</td>
</tr>
<tr>
<td>2024</td>
<td>[SyncTalk] <a href="https://arxiv.org/abs/2311.17590">SyncTalk: The Devil😈 is in the Synchronization for Talking Head Synthesis</a></td>
<td>CVPR 2024?</td>
<td><a href="https://github.com/ZiqiaoPeng/SyncTalk">Code</a></td>
<td><a href="https://ziqiaopeng.github.io/synctalk/">Project</a></td>
<td>😈</td>
</tr>
<tr>
<td>2024</td>
<td>[R2-Talk] <a href="https://arxiv.org/abs/2312.05572">R2-Talker: Realistic Real-Time Talking Head Synthesis with Hash Grid Landmarks Encoding and Progressive Multilayer Conditioning</a></td>
<td>Arxiv 2024</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>[DT-NeRF] <a href="https://arxiv.org/pdf/2309.07752">DT-NeRF: Decomposed Triplane-Hash Neural Radiance Fields for High-Fidelity Talking Portrait Synthesis</a></td>
<td>ICASSP 2024</td>
<td>-</td>
<td>-</td>
<td>ER-NeRF</td>
</tr>
<tr>
<td>2023</td>
<td>[ER-NeRF] <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Efficient_Region-Aware_Neural_Radiance_Fields_for_High-Fidelity_Talking_Portrait_Synthesis_ICCV_2023_paper.pdf">Efficient Region-Aware Neural Radiance Fields for High-Fidelity Talking Portrait Synthesis</a></td>
<td>ICCV 2023</td>
<td><a href="https://github.com/Fictionarry/ER-NeRF">Code</a></td>
<td><a href="https://fictionarry.github.io/ER-NeRF/">Project</a></td>
<td>Tri-plane</td>
</tr>
<tr>
<td>2023</td>
<td>[LipNeRF] <a href="https://www.amazon.science/publications/lipnerf-what-is-the-right-feature-space-to-lip-sync-a-nerf">LipNeRF: What is the right feature space to lip-sync a NeRF?</a></td>
<td>FG 2023</td>
<td>Code</td>
<td><a href="https://aggelinacha.github.io/LipNeRF/">Project</a></td>
<td>Wav2lip</td>
</tr>
<tr>
<td>2023</td>
<td>[SD-NeRF] <a href="https://ieeexplore.ieee.org/document/10229247">SD-NeRF: Towards Lifelike Talking Head Animation via Spatially-adaptive Dual-driven NeRFs</a></td>
<td>IEEE 2023</td>
<td>-</td>
<td>-</td>
<td></td>
</tr>
<tr>
<td>2023</td>
<td>[Instruct-NeuralTalker] <a href="https://arxiv.org/abs/2306.10813">Instruct-NeuralTalker: Editing Audio-Driven Talking Radiance Fields with Instructions</a></td>
<td>Arxiv 2023</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2023</td>
<td>[GeneFace++] <a href="https://arxiv.org/abs/2305.00787">Generalized and Stable Real-Time Audio-Driven 3D Talking Face Generation</a></td>
<td>Arxiv 2023</td>
<td>-</td>
<td><a href="https://genefaceplusplus.github.io/">Project</a></td>
<td>-</td>
</tr>
<tr>
<td>2023</td>
<td>[GeneFace] <a href="https://arxiv.org/abs/2301.13430">GeneFace: Generalized and High-Fidelity Audio-Driven 3D Talking Face Synthesis</a></td>
<td>ICLR 2023</td>
<td><a href="https://github.com/yerfor/GeneFace">Code</a></td>
<td><a href="https://geneface.github.io/">Project</a></td>
<td>-</td>
</tr>
<tr>
<td>2022</td>
<td>[RAD-NeRF] <a href="https://arxiv.org/pdf/2211.12368.pdf">RAD-NeRF: Real-time Neural Talking Portrait Synthesis</a></td>
<td>Arxiv 2022</td>
<td><a href="https://github.com/ashawkey/RAD-NeRF">Code</a></td>
<td><a href="https://ashawkey.github.io/radnerf/">Project</a></td>
<td>InstantNGP</td>
</tr>
<tr>
<td>2022</td>
<td>[DFRF] <a href="https://arxiv.org/abs/2207.11770">DFRF：Learning Dynamic Facial Radiance Fields for Few-Shot Talking Head Synthesis</a></td>
<td>ECCV 2022</td>
<td><a href="https://github.com/sstzal/DFRF">Code</a></td>
<td><a href="https://sstzal.github.io/DFRF/">Project</a></td>
<td></td>
</tr>
<tr>
<td>2022</td>
<td>[DialogueNeRF] <a href="https://arxiv.org/pdf/2203.07931.pdf">DialogueNeRF: Towards Realistic Avatar Face-to-face Conversation Video Generation</a></td>
<td>Arxiv 2022</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>2022</td>
<td>[NeRFInvertor] <a href="https://arxiv.org/pdf/2211.17235.pdf">NeRFInvertor: High Fidelity NeRF-GAN Inversion for Single-shot Real Image Animation</a></td>
<td>Arxiv 2022</td>
<td><a href="https://github.com/YuYin1/NeRFInvertor">Code</a></td>
<td><a href="https://yuyin1.github.io/NeRFInvertor_Homepage/">Project</a></td>
<td>-</td>
</tr>
<tr>
<td>2022</td>
<td>[Next3D] <a href="https://arxiv.org/pdf/2211.11208.pdf">Next3D: Generative Neural Texture Rasterization for 3D-Aware Head Avatars</a></td>
<td>Arxiv 2022</td>
<td><a href="https://mrtornado24.github.io/Next3D/">Code</a></td>
<td><a href="https://mrtornado24.github.io/Next3D/">Project</a></td>
<td>-</td>
</tr>
<tr>
<td>2022</td>
<td>[3DFaceShop] <a href="https://arxiv.org/pdf/2209.05434">3DFaceShop: Explicitly Controllable 3D-Aware Portrait Generation</a></td>
<td>Arxiv 2022</td>
<td><a href="https://github.com/junshutang/3DFaceShop">Code</a></td>
<td><a href="https://junshutang.github.io/control/index.html">Project</a></td>
<td>-</td>
</tr>
<tr>
<td>2022</td>
<td>[FNeVR] <a href="https://arxiv.org/abs/2209.10340">FNeVR: Neural Volume Rendering for Face Animation</a></td>
<td>Arxiv 2022</td>
<td><a href="https://github.com/zengbohan0217/FNeVR">Code</a></td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>2022</td>
<td>[ROME] <a href="https://arxiv.org/pdf/2206.08343.pdf">ROME: Realistic One-shot Mesh-based Head Avatars</a></td>
<td>ECCV 2022</td>
<td><a href="https://github.com/SamsungLabs/rome">Code</a></td>
<td><a href="https://samsunglabs.github.io/rome/">Project</a></td>
<td>-</td>
</tr>
<tr>
<td>2022</td>
<td>[IMavatar] <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zheng_I_M_Avatar_Implicit_Morphable_Head_Avatars_From_Videos_CVPR_2022_paper.pdf">IMavatar: Implicit Morphable Head Avatars from Videos</a></td>
<td>CVPR 2022</td>
<td><a href="https://ait.ethz.ch/projects/2022/IMavatar/">Code</a></td>
<td><a href="https://ait.ethz.ch/projects/2022/IMavatar/">Project</a></td>
<td>-</td>
</tr>
<tr>
<td>2022</td>
<td>[HeadNeRF] <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Grassal_Neural_Head_Avatars_From_Monocular_RGB_Videos_CVPR_2022_paper.pdf">HeadNeRF: A Real-time NeRF-based Parametric Head Model</a></td>
<td>CVPR 2022</td>
<td><a href="https://github.com/CrisHY1995/headnerf">Code</a></td>
<td><a href="https://hy1995.top/HeadNeRF-Project/">Project</a></td>
<td>-</td>
</tr>
<tr>
<td>2022</td>
<td>[SSP-NeRF] <a href="https://arxiv.org/pdf/2201.07786.pdf">Semantic-Aware Implicit Neural Audio-Driven Video Portrait Generation</a></td>
<td>Arxiv 2022</td>
<td><a href="https://github.com/alvinliu0/SSP-NeRF">Code</a></td>
<td><a href="https://alvinliu0.github.io/projects/SSP-NeRF">Project</a></td>
<td>-</td>
</tr>
<tr>
<td>2021</td>
<td>[AD-NeRF] <a href="https://arxiv.org/abs/2103.11078">AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis</a></td>
<td>ICCV 2021</td>
<td><a href="https://github.com/YudongGuo/AD-NeRF">Code</a></td>
<td><a href="https://yudongguo.github.io/ADNeRF/">Project</a></td>
<td>-</td>
</tr>
<tr>
<td>2021</td>
<td>[NerFACE] <a href="https://arxiv.org/pdf/2012.03065">NerFACE: Dynamic Neural Radiance Fields for Monocular 4D Facial Avatar Reconstruction</a></td>
<td>CVPR 2021 Oral</td>
<td><a href="https://github.com/gafniguy/4D-Facial-Avatars">Code</a></td>
<td><a href="https://gafniguy.github.io/4D-Facial-Avatars/">Project</a></td>
<td>-</td>
</tr>
<tr>
<td>2021</td>
<td>[DFA-NeRF] <a href="https://arxiv.org/pdf/2201.00791v1.pdf">DFA-NeRF: Personalized Talking Head Generation via Disentangled Face Attributes Neural Rendering</a></td>
<td>Arxiv 2021</td>
<td><a href="https://github.com/ShunyuYao/DFA-NeRF">Code</a></td>
<td>-</td>
<td>-</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h2 id="Metrics"><a href="#Metrics" class="headerlink" title="Metrics"></a>Metrics</h2><div class="table-container">
<table>
<thead>
<tr>
<th>Metrics</th>
<th>Paper</th>
<th>Link</th>
</tr>
</thead>
<tbody>
<tr>
<td>PSNR (peak signal-to-noise ratio)</td>
<td>-</td>
<td></td>
</tr>
<tr>
<td>SSIM (structural similarity index measure)</td>
<td>Image quality assessment: from error visibility to structural similarity.</td>
<td></td>
</tr>
<tr>
<td>CPBD(cumulative probability of blur detection)</td>
<td>A no-reference image blur metric based on the cumulative probability of blur detection</td>
<td></td>
</tr>
<tr>
<td>LPIPS (Learned Perceptual Image Patch Similarity) -</td>
<td>The Unreasonable Effectiveness of Deep Features as a Perceptual Metric</td>
<td><a href="https://arxiv.org/pdf/1801.03924.pdf">paper</a></td>
</tr>
<tr>
<td>NIQE (Natural Image Quality Evaluator)</td>
<td>Making a ‘Completely Blind’ Image Quality Analyzer</td>
<td><a href="http://live.ece.utexas.edu/research/Quality/niqe_spl.pdf">paper</a></td>
</tr>
<tr>
<td>FID (Fréchet inception distance)</td>
<td>GANs trained by a two time-scale update rule converge to a local nash equilibrium</td>
<td></td>
</tr>
<tr>
<td>LMD (landmark distance error)</td>
<td>Lip Movements Generation at a Glance</td>
<td></td>
</tr>
<tr>
<td>LRA (lip-reading accuracy)</td>
<td>Talking Face Generation by Conditional Recurrent Adversarial Network</td>
<td><a href="https://arxiv.org/pdf/1804.04786.pdf">paper</a></td>
</tr>
<tr>
<td>WER(word error rate)</td>
<td>Lipnet: end-to-end sentencelevel lipreading.</td>
<td></td>
</tr>
<tr>
<td>LSE-D (Lip Sync Error - Distance)</td>
<td>Out of time: automated lip sync in the wild</td>
<td></td>
</tr>
<tr>
<td>LSE-C (Lip Sync Error - Confidence)</td>
<td>Out of time: automated lip sync in the wild</td>
<td></td>
</tr>
<tr>
<td>ACD(Average content distance)</td>
<td>Facenet: a unified embedding for face recognition and clustering.</td>
<td></td>
</tr>
<tr>
<td>CSIM(cosine similarity)</td>
<td>Arcface: additive angular margin loss for deep face recognition.</td>
<td></td>
</tr>
<tr>
<td>EAR(eye aspect ratio)</td>
<td>Real-time eye blink detection using facial landmarks. In: Computer Vision Winter Workshop</td>
<td></td>
</tr>
<tr>
<td>ESD(emotion similarity distance)</td>
<td>What comprises a good talking-head video generation?: A Survey and Benchmark</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h2 id="Tools-amp-Software"><a href="#Tools-amp-Software" class="headerlink" title="Tools &amp; Software"></a>Tools &amp; Software</h2><div class="table-container">
<table>
<thead>
<tr>
<th>Tool/Resource</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://sourceforge.net/projects/lucia/">LUCIA</a></td>
<td>Development of a MPEG-4 Talking Head Engine. 💻</td>
</tr>
<tr>
<td><a href="https://www.g2.com/products/yepic-studio/reviews">Yepic Studio</a></td>
<td>Create and dub talking head-style videos in minutes without expensive equipment. 🎥</td>
</tr>
<tr>
<td><a href="https://sourceforge.net/projects/talkbots/">Mel McGee’s Talkbots</a></td>
<td>A complete multi-browser, multi-platform talking head application in SVG suitable for web sites or as an avatar. 🗣️</td>
</tr>
<tr>
<td><a href="https://sourceforge.net/projects/face3dchung/">face3D_chung</a></td>
<td>Create 3D character avatar head objects with texture from a single photo for your games. 🎮</td>
</tr>
<tr>
<td><a href="https://www.g2.com/products/crazytalk/reviews">CrazyTalk</a></td>
<td>Exciting features for 3D head creation and automation. 🤪</td>
</tr>
<tr>
<td><a href="https://sourceforge.net/directory/?q=tts%20avatar">tts avatar free download - SourceForge</a></td>
<td>Mel McGee’s Talkbots is a complete multi-browser, multi-platform talking head. (🔧👄)</td>
</tr>
<tr>
<td><a href="https://www.producthunt.com/products/verbatim-ai">Verbatim AI - Product Information, Latest Updates, and Reviews 2023</a></td>
<td>A simple yet powerful API to generate AI “talking head” videos in near real-time with Verbatim AI. Add interest, intrigue, and dynamism to your chat bots! (🔧👄)</td>
</tr>
<tr>
<td><a href="https://sourceforge.net/directory/3d-modeling/basic/">Best Open Source BASIC 3D Modeling Software</a></td>
<td>Includes talk3D_chung, a small example using obj models created with face3D_chung, and speak3D_chung_dll, a dll to load and display face3D_chung talking avatars. (🛠️🎭)</td>
</tr>
<tr>
<td><a href="https://sourceforge.net/p/dvdstyler/discussion/318795/thread/82dcb647/">DVDStyler / Discussion / Help: ffmpeg-vbr or internal</a></td>
<td>Talking heads would get a bitrate which is unnecessarily high while using DVDStyler. (🛠️👄)</td>
</tr>
<tr>
<td><a href="https://sourceforge.net/directory/lisp/?q=puffin+web+browser">puffin web browser free download - SourceForge</a></td>
<td>Mel McGee’s Talkbots is a complete multi-browser, multi-platform talking head. (🔧👄)</td>
</tr>
<tr>
<td>[12 best AI video generators to use in 2023 Free and paid \</td>
<td>Product …](<a href="https://www.producthunt.com/stories/best-ai-video-generators-free">https://www.producthunt.com/stories/best-ai-video-generators-free</a>)</td>
<td>Whether you’re an entrepreneur, small business owner, or run a large company, AI video generators make it super easy to create high-quality videos from scratch. (🔧🎥)</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h2 id="Slides-amp-Presentations"><a href="#Slides-amp-Presentations" class="headerlink" title="Slides &amp; Presentations"></a>Slides &amp; Presentations</h2><div class="table-container">
<table>
<thead>
<tr>
<th>Presentation Title</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://www.slideshare.net/ssuserc9d82a/paper-reviewfewshot-adversarial-learning-of-realistic-neural-talking-head-models">Few-Shot Adversarial Learning of Realistic Neural Talking Head Models</a></td>
<td>Presentation reviewing the few-shot adversarial learning of realistic neural talking head models.</td>
</tr>
<tr>
<td><a href="https://www.slideshare.net/ZULHICZARARIETINARBU/nethania-michelles-character">Nethania Michelle’s Character</a></td>
<td>PPT: Presentation discussing the improvement of a 3D talking head for use in an avatar of a virtual meeting room.</td>
</tr>
<tr>
<td><a href="https://support.prezi.com/hc/en-us/articles/360036679953-Presenting-you-Top-tips-on-presenting-with-Prezi-Video">Presenting you: Top tips on presenting with Prezi Video – Prezi</a></td>
<td>Article providing top tips for presenting with Prezi Video.</td>
</tr>
<tr>
<td><a href="https://pt.slideshare.net/willg_36/research-presentation-presentation-956726">Research Presentation</a></td>
<td>PPT: Resident Research Presentation Slide Deck.</td>
</tr>
<tr>
<td><a href="https://support.prezi.com/hc/en-us/articles/360038281894-Adding-narration-to-your-presentation-using-Prezi-Video-">Adding narration to your presentation (using Prezi Video) – Prezi</a></td>
<td>Learn how to add narration to your Prezi presentation with Prezi Video.</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h2 id="Star-History"><a href="#Star-History" class="headerlink" title="Star History"></a>Star History</h2><p><a href="https://star-history.com/#Kedreamix/Awesome-Talking-Head-Synthesis&amp;Date"><img src="https://api.star-history.com/svg?repos=Kedreamix/Awesome-Talking-Head-Synthesis&amp;type=Date" alt="Star History Chart"></a></p>
]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>Talking Head Generation</tag>
        <tag>3DGS</tag>
        <tag>NeRF</tag>
      </tags>
  </entry>
  <entry>
    <title>简便快捷获取Hugging Face模型（使用镜像站点）</title>
    <url>/2024/01/05/Note/HuggingFace/</url>
    <content><![CDATA[<p>通常，我们可以通过代码直接下载模型，但有时会遇到诸多问题，例如下载速度慢或其他一些问题。在这里，我将主要介绍一些常用的简便方法。如果你想了解更多用法，请查看这篇知乎文章：<a href="https://zhuanlan.zhihu.com/p/663712983，">https://zhuanlan.zhihu.com/p/663712983，</a> 其中包含许多多线程的方法。</p>
<p><a href="https://zhuanlan.zhihu.com/p/663712983">padeoe：如何快速下载huggingface模型</a></p>
<p><img src="https://picx.zhimg.com/v2-9e8901ddec21dae36a31bb438dae03a8_r.jpg?source=172ae18b" alt="img"></p>
<p>我的关注点主要是<strong>断点续传和多线程下载</strong>的方式，因为这样可以避免每次都重新下载，同时在网络不稳定时能够保持相对较好的下载速度。</p>
<h3 id="浏览器下载"><a href="#浏览器下载" class="headerlink" title="浏览器下载"></a>浏览器下载</h3><p>首先，最简单的方法是通过浏览器下载。找到相应的文件，一个一个地下载即可。然而，这样会耗费时间和精力。</p>
<p><img src="https://picx.zhimg.com/80/v2-83aaadbd16b90a345f15a08954aa9e2e_720w.png?source=d16d100b" alt="浏览器下载"></p>
<p>浏览器下载</p>
<h3 id="直接使用URL下载"><a href="#直接使用URL下载" class="headerlink" title="直接使用URL下载"></a>直接使用URL下载</h3><p>使用URL直接下载时，将 huggingface.co 直接替换为域名 hf-mirror.com。可以使用浏览器，或者命令行工具如 wget -c、curl -L、aria2c 等。对于需要登录的模型，需在命令行中添加 —header hf_<em>*</em> 参数，具体获取token的方法请参见前文。</p>
<p>Hugging Face提供的包会获取系统变量，因此可以通过设置变量来解决：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">HF_ENDPOINT=https://hf-mirror.com python your_script.py</span><br></pre></td></tr></tbody></table></figure>
<h3 id="Git克隆下载"><a href="#Git克隆下载" class="headerlink" title="Git克隆下载"></a>Git克隆下载</h3><p>此外，还有一种通过 git clone repo_url 下载的方式由官方提供。尽管这种方法相对简单，但却被认为是<strong>最不建议直接采用的途径</strong>，主要有以下三个弊端：</p>
<ol>
<li>不支持断点续传，一旦中断，需要重新启动整个下载过程。</li>
<li>占用大量磁盘空间,即使没有历史版本,也会存储一份元信息和当前版本文件拷贝;</li>
<li>对于一些存在历史版本的模型，<strong>下载时间会超过两倍</strong>。</li>
</ol>
<p>一个较好的方法是设置环境变量GIT_LFS_SKIP_SMUDGE=1开启git clone,之后单独使用其他支持断点续传的工具下载大文件。这种方式可以很好地兼顾断点续传和磁盘空间占用。</p>
<p><img src="https://picx.zhimg.com/80/v2-3e4aaa6d94bd5dd6b7cea0f745e581f8_720w.png?source=d16d100b" alt="Git下载"></p>
<h3 id="Hugging-Face-CLI命令行工具"><a href="#Hugging-Face-CLI命令行工具" class="headerlink" title="Hugging Face CLI命令行工具"></a>Hugging Face CLI命令行工具</h3><p>Hugging Face官方提供的 <a href="https://zhuanlan.zhihu.com/p/676222159///hf-mirror.com/docs/huggingface_hub/guides/download#download-from-the-cli"><strong>huggingface-cli</strong> </a> <strong>命令行工具功能很强大,极力推荐使用。它支持下载模型、数据集,登录注册,且长期持续更新维护。</strong></p>
<p>使用镜像网站可以加快速度。基本操作如下:安装依赖、设置环境变量、执行下载、处理需要登录的模型等情况。此外,可以尝试使用hf_transfer进行加速。</p>
<ol>
<li>安装依赖</li>
</ol>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">pip install -U huggingface_hub</span><br></pre></td></tr></tbody></table></figure>
<ol>
<li>基本命令示例：</li>
</ol>
<p>这里使用了镜像网站<a href="https://hf-mirror.com/">https://hf-mirror.com</a>，会加快速度一点，<strong>hf-mirror.com</strong>，用于镜像 <a href="https://huggingface.co/">huggingface.co</a> 域名。</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> HF_ENDPOINT=https://hf-mirror.com</span><br><span class="line"><span class="comment"># 下载模型</span></span><br><span class="line">huggingface-cli download --resume-download --local-dir-use-symlinks False bigscience/bloom-560m --local-dir bloom-560m</span><br><span class="line"><span class="comment"># 下载数据集</span></span><br><span class="line">huggingface-cli download --resume-download --repo-type dataset lavita/medical-qa-shared-task-v1-toy</span><br></pre></td></tr></tbody></table></figure>
<ol>
<li>下载需要登录的模型（Gated Model）</li>
</ol>
<p>请添加 —token hf<em><em>*</em> 参数，其中 hf</em><em>** 是 </em>access token*，请在 <a href="https://huggingface.co/settings/tokens">Hugging Face官网这里</a> 获取。示例：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">huggingface-cli download --token hf_*** --resume-download --local-dir-use-symlinks False meta-llama/Llama-2-7b-hf --local-dir Llama-2-7b-hf</span><br></pre></td></tr></tbody></table></figure>
<h3 id="hf-transfer加速"><a href="#hf-transfer加速" class="headerlink" title="hf_transfer加速"></a>hf_transfer加速</h3><p>hf_transfer 依附并兼容 huggingface-cli，是 hugging face 官方专门为提高下载速度基于 Rust 开发的一个模块。</p>
<p><strong>开启方法</strong></p>
<ol>
<li>安装依赖</li>
</ol>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">pip install -U hf-transfer</span><br></pre></td></tr></tbody></table></figure>
<ol>
<li>设置 HF_HUB_ENABLE_HF_TRANSFER 环境变量为 1。</li>
</ol>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> HF_HUB_ENABLE_HF_TRANSFER=1</span><br></pre></td></tr></tbody></table></figure>
<ol>
<li>开启后使用方法同 huggingface-cli：</li>
</ol>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">huggingface-cli download --resume-download bigscience/bloom-560m --local-dir bloom-560m</span><br></pre></td></tr></tbody></table></figure>
<p>以前hf_transfer没有进度条,需要根据是否有进度条判断是否成功开启。但从0.19版本开始已支持进度条,开启更为便捷。</p>
<p>不过,hf_transfer的鲁棒性较低,在网络不稳定时可能会报错。这是因为它还在完善过程中,对国内较差的网络状况支持不够好。如果出现错误,可以尝试关闭该模块以提高程序的容错性。</p>
<p>总体来说,hf_transfer能有效提速,但在网络不稳定时可能会出现问题。基于自身网络环境调整是否使用它是个不错的选择。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>总体来说，选择下载模型的方式取决于个人需求和网络环境。浏览器下载简单直接，但耗时耗力；Git克隆方式虽简单，但不支持断点续传且占用磁盘空间较大；Hugging Face CLI命令行工具功能强大，推荐使用，尤其搭配 hf_transfer 进行加速，但在网络不稳定时可能会遇到问题。根据实际情况，选择合适的方式可以更高效地获取所需模型。</p>
]]></content>
      <categories>
        <category>Note</category>
      </categories>
      <tags>
        <tag>Note</tag>
      </tags>
  </entry>
  <entry>
    <title>Blendshape学习笔记</title>
    <url>/2024/03/11/Note/BlendShape/</url>
    <content><![CDATA[<h2 id="Blendshape-Morph-Target动画"><a href="#Blendshape-Morph-Target动画" class="headerlink" title="Blendshape(Morph Target动画)"></a>Blendshape(Morph Target动画)</h2><p>Blendshapes泛指3D定点动画的制作方式 (Maya里面称之为 blend shapes ，而3DS Max里称之为morph targets) ，在3D动画中用的比较多，尤其是人脸动画的制作，通过blendshape来驱动角色的面部表情。</p>
<p>用在脸部动画制作时，blendshape可以被称之为<strong>脸部特征，表情基准，定位符</strong>等等。这里要引入一个<code>FACS</code>的概念，可以简单理解为将脸部进行合理化的分区标准。</p>
<blockquote>
<p>“表情这个东西看起来是一个无限多可能的东西，怎么能够计算expression呢？</p>
<p>这就带来了Blendshapes——一组组成整体表情的基准（数量可以有十几个、50个、100+、 200+，越多就越细腻)。我们可以使用这一组基准通过线性组合来计算出整体的expression，用公式来说就是  ，其中e是expression，B是一组表情基准，d是对应的系数（在这一组里面的权重），b是neutral。” </p>
<p>— From <a href="https://zhuanlan.zhihu.com/p/78174706">https://zhuanlan.zhihu.com/p/78174706</a></p>
</blockquote>
<h2 id="BlendShape系数介绍"><a href="#BlendShape系数介绍" class="headerlink" title="BlendShape系数介绍"></a>BlendShape系数介绍</h2><p>在ARKit中，对表情特征位置定义了52组运动blendshape系数(<br><a href="https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation">https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation</a> )，每个blendshape系数代表一种表情定位符，表情定位符定义了特定表情属性，如mouthSmileLeft、mouthSmileRight等，与其对应的blendshape系数则表示表情运动范围。这52组blendshape系数极其描述如下表所示。</p>
<p><img src="https://p3-sign.toutiaoimg.com/pgc-image/984d8d76878441c3a8402f788ef6e46f~noop.image?_iz=58558&amp;from=article.pc_detail&amp;lk3s=953192f4&amp;x-expires=1710668214&amp;x-signature=qI8vU39X63te%2BVNdO78uBFphwK0%3D" alt="Blendshape"></p>
<p>每一个blendshape系数的取值范围为0～1的浮点数。以jawOpen为例，当认为用户的嘴巴完全闭紧时，返回的jawOpen系数为0。当认为用户的嘴巴张开至最大时，返回的jawOpen系数为1。</p>
<p><img src="https://p6-sign.toutiaoimg.com/pgc-image/2c8cbd123e00470e95500a8ae62da605~noop.image?_iz=58558&amp;from=article.pc_detail&amp;lk3s=953192f4&amp;x-expires=1710668214&amp;x-signature=UHPhjWP4v96kbtfJzF97Z%2Bp3klc%3D" alt=""></p>
<p>在用户完全闭嘴与嘴张到最大之间的过渡状态，jawOpen会根据用户嘴张大的幅度返回一个0～1的插值。</p>
<p><img src="https://p3-sign.toutiaoimg.com/pgc-image/8e8d980b8d69461fb5d2efbc50e47d47~noop.image?_iz=58558&amp;from=article.pc_detail&amp;lk3s=953192f4&amp;x-expires=1710668214&amp;x-signature=sFNMeBoNY3ZFfiO%2BRSjR8uGECIw%3D" alt=""></p>
<h2 id="脸部动捕的使用"><a href="#脸部动捕的使用" class="headerlink" title="脸部动捕的使用"></a>脸部动捕的使用</h2><h3 id="ARKit-脸部与Vive脸部blendshape基准对比"><a href="#ARKit-脸部与Vive脸部blendshape基准对比" class="headerlink" title="ARKit 脸部与Vive脸部blendshape基准对比"></a>ARKit 脸部与Vive脸部blendshape基准对比</h3><div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>ARKit（52）</th>
<th>Extra</th>
<th>VIVE（52）</th>
<th>Extra</th>
</tr>
</thead>
<tbody>
<tr>
<td>Brow</td>
<td>5</td>
<td></td>
<td>0</td>
<td></td>
</tr>
<tr>
<td>Eye</td>
<td>13</td>
<td></td>
<td>14</td>
<td>Eye Frown + 1</td>
</tr>
<tr>
<td>Cheek</td>
<td>3</td>
<td></td>
<td>3</td>
<td></td>
</tr>
<tr>
<td>Nose</td>
<td>2</td>
<td></td>
<td>0</td>
<td></td>
</tr>
<tr>
<td>Jaw</td>
<td>4</td>
<td></td>
<td>4</td>
<td></td>
</tr>
<tr>
<td>Mouth</td>
<td>24</td>
<td></td>
<td>20</td>
<td>O shape - 1</td>
</tr>
<tr>
<td>Tongue</td>
<td>1</td>
<td>Tongue + 7</td>
<td>11</td>
<td></td>
</tr>
<tr>
<td>Sum</td>
<td>52</td>
<td>59</td>
<td>52</td>
<td>52</td>
</tr>
</tbody>
</table>
</div>
<h3 id="ARKit的52个Blendshape表情基准组"><a href="#ARKit的52个Blendshape表情基准组" class="headerlink" title="ARKit的52个Blendshape表情基准组"></a>ARKit的52个Blendshape表情基准组</h3><p>可以看ARKit Face Blendshapes的照片和3D模型示例：<a href="https://arkit-face-blendshapes.com/">https://arkit-face-blendshapes.com/</a></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>CC3</th>
<th>ARKit Name 表情基准/定位符</th>
<th>ARKit Picture</th>
<th>CC3 Picture</th>
</tr>
</thead>
<tbody>
<tr>
<td>A01</td>
<td>browInnerUp</td>
<td><img src="https://static.wixstatic.com/media/64c63b_4cc12dd62ef8484986eebe9739f4eac9~mv2.png/v1/fill/w_252,h_178,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_4cc12dd62ef8484986eebe9739f4eac9~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_fc0c248f4f6f46dda26eda66865678d2~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_fc0c248f4f6f46dda26eda66865678d2~mv2.png" alt=""></td>
</tr>
<tr>
<td>A02</td>
<td>browDownLeft</td>
<td><img src="https://static.wixstatic.com/media/64c63b_18a57dc078214abea520f25ad6dfb02a~mv2.png/v1/fill/w_252,h_175,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_18a57dc078214abea520f25ad6dfb02a~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_9d780933931b469991ae0d4ddf105045~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_9d780933931b469991ae0d4ddf105045~mv2.png" alt=""></td>
</tr>
<tr>
<td>A03</td>
<td>browDownRight</td>
<td><img src="https://static.wixstatic.com/media/64c63b_105d6dd9d7c44394b96b242e6d9d580b~mv2.png/v1/fill/w_252,h_175,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_105d6dd9d7c44394b96b242e6d9d580b~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_d9943f7163414286809edef7c3bf2de7~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_d9943f7163414286809edef7c3bf2de7~mv2.png" alt=""></td>
</tr>
<tr>
<td>A04</td>
<td>browOuterUpLeft</td>
<td><img src="https://static.wixstatic.com/media/64c63b_e7fe8581da2540a3bd7dfc39c874dd61~mv2.png/v1/fill/w_252,h_175,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e7fe8581da2540a3bd7dfc39c874dd61~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_c7234589721d4ddda4e2fcb1a9e0aa97~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_c7234589721d4ddda4e2fcb1a9e0aa97~mv2.png" alt=""></td>
</tr>
<tr>
<td>A05</td>
<td>browOuterUpRight</td>
<td><img src="https://static.wixstatic.com/media/64c63b_1fb29f740ff74e8aabadc3769c86501a~mv2.png/v1/fill/w_252,h_175,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_1fb29f740ff74e8aabadc3769c86501a~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_4aefd80dd4c548669d5ba80e4da639eb~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_4aefd80dd4c548669d5ba80e4da639eb~mv2.png" alt=""></td>
</tr>
<tr>
<td>A06</td>
<td>eyeLookUpLeft</td>
<td><img src="https://static.wixstatic.com/media/64c63b_697a02a504c84d5f9e316172849bb6d0~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_697a02a504c84d5f9e316172849bb6d0~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_48e3b55ee9ca40f9aec9be8b35c403b0~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_48e3b55ee9ca40f9aec9be8b35c403b0~mv2.png" alt=""></td>
</tr>
<tr>
<td>A07</td>
<td>eyeLookUpRight</td>
<td><img src="https://static.wixstatic.com/media/64c63b_84cacf1f990a4e5c874c084a1ea626b3~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_84cacf1f990a4e5c874c084a1ea626b3~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_c879b6cca2ce4f8aa2385864c1fb9389~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_c879b6cca2ce4f8aa2385864c1fb9389~mv2.png" alt=""></td>
</tr>
<tr>
<td>A08</td>
<td>eyeLookDownLeft</td>
<td><img src="https://static.wixstatic.com/media/64c63b_d229ef398f3547be93a1a59563520e81~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_d229ef398f3547be93a1a59563520e81~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_d6c6c94e3cee43db8ae9d6f36fc1a689~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_d6c6c94e3cee43db8ae9d6f36fc1a689~mv2.png" alt=""></td>
</tr>
<tr>
<td>A09</td>
<td>eyeLookDownRight</td>
<td><img src="https://static.wixstatic.com/media/64c63b_67a1674b6d584344b7ea77843f72be27~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_67a1674b6d584344b7ea77843f72be27~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_ca0dbf25d9f74085809cdcd0742ede35~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_ca0dbf25d9f74085809cdcd0742ede35~mv2.png" alt=""></td>
</tr>
<tr>
<td>A10</td>
<td>eyeLookOutLeft</td>
<td><img src="https://static.wixstatic.com/media/64c63b_b4257aa18f754427a593064e71aa97fd~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_b4257aa18f754427a593064e71aa97fd~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_d24fd04ef2d64db18c31b90eccd5f1a9~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_d24fd04ef2d64db18c31b90eccd5f1a9~mv2.png" alt=""></td>
</tr>
<tr>
<td>A11</td>
<td>eyeLookInLeft</td>
<td><img src="https://static.wixstatic.com/media/64c63b_03368853adeb4b8599da5451033cd809~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_03368853adeb4b8599da5451033cd809~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_049433ce862e44c4a5f96bcf0ad13bd0~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_049433ce862e44c4a5f96bcf0ad13bd0~mv2.png" alt=""></td>
</tr>
<tr>
<td>A12</td>
<td>eyeLookInRight</td>
<td><img src="https://static.wixstatic.com/media/64c63b_6e67745f7867402398390ce18a9f2882~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_6e67745f7867402398390ce18a9f2882~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_803074453832444d8dec710711196559~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_803074453832444d8dec710711196559~mv2.png" alt=""></td>
</tr>
<tr>
<td>A13</td>
<td>eyeLookOutRight</td>
<td><img src="https://static.wixstatic.com/media/64c63b_b54a5b6f123244d98eadbded8c29a8c3~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_b54a5b6f123244d98eadbded8c29a8c3~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_2e7b0fed966d453fa0a8dffaabeaf769~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_2e7b0fed966d453fa0a8dffaabeaf769~mv2.png" alt=""></td>
</tr>
<tr>
<td>A14</td>
<td>eyeBlinkLeft</td>
<td><img src="https://static.wixstatic.com/media/64c63b_0b68b26a666a49da843b6a47c4579b46~mv2.png/v1/fill/w_252,h_175,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_0b68b26a666a49da843b6a47c4579b46~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_b46d50b28b5d40feba9a496b1ead4a5c~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_b46d50b28b5d40feba9a496b1ead4a5c~mv2.png" alt=""></td>
</tr>
<tr>
<td>A15</td>
<td>eyeBlinkRight</td>
<td><img src="https://static.wixstatic.com/media/64c63b_65e50badaa854262a87329394a87484c~mv2.png/v1/fill/w_252,h_175,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_65e50badaa854262a87329394a87484c~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_9113137c91934bdbab3fb26756e84783~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_9113137c91934bdbab3fb26756e84783~mv2.png" alt=""></td>
</tr>
<tr>
<td>A16</td>
<td>eyeSquintLeft</td>
<td><img src="https://static.wixstatic.com/media/64c63b_7b9132e314d6404097f212401559e9c4~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_7b9132e314d6404097f212401559e9c4~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_0cccb71728de47e5a7f63fe9bc70bcaf~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_0cccb71728de47e5a7f63fe9bc70bcaf~mv2.png" alt=""></td>
</tr>
<tr>
<td>A17</td>
<td>eyeSquintRight</td>
<td><img src="https://static.wixstatic.com/media/64c63b_8cc99b12de914fe882c19229ce2a91da~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_8cc99b12de914fe882c19229ce2a91da~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_8445ac0161fe400ab28591fb6b0b1f56~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_8445ac0161fe400ab28591fb6b0b1f56~mv2.png" alt=""></td>
</tr>
<tr>
<td>A18</td>
<td>eyeWideLeft</td>
<td><img src="https://static.wixstatic.com/media/64c63b_0c87ac4e4c5d4d5f9639523c82aa9d43~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_0c87ac4e4c5d4d5f9639523c82aa9d43~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_c097966492c3496cabf1d84455d7144d~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_c097966492c3496cabf1d84455d7144d~mv2.png" alt=""></td>
</tr>
<tr>
<td>A19</td>
<td>eyeWideRight</td>
<td><img src="https://static.wixstatic.com/media/64c63b_3157fc370d064da9926027034e8220d6~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_3157fc370d064da9926027034e8220d6~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_24dc2c84e19b436a97fd2db6044f439c~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_24dc2c84e19b436a97fd2db6044f439c~mv2.png" alt=""></td>
</tr>
<tr>
<td>A20</td>
<td>cheekPuff</td>
<td><img src="https://static.wixstatic.com/media/64c63b_de4df8062c5f47ca9cd322b75b535705~mv2.png/v1/fill/w_252,h_172,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_de4df8062c5f47ca9cd322b75b535705~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_27548c426f1b47ae834c757417e03269~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_27548c426f1b47ae834c757417e03269~mv2.png" alt=""></td>
</tr>
<tr>
<td>A21</td>
<td>cheekSquintLeft</td>
<td><img src="https://static.wixstatic.com/media/64c63b_70520c1a1c374ff3855cb8dfa7450b8b~mv2.png/v1/fill/w_252,h_175,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_70520c1a1c374ff3855cb8dfa7450b8b~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_d769bd2ef0104030818ed7a156ee2a2e~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_d769bd2ef0104030818ed7a156ee2a2e~mv2.png" alt=""></td>
</tr>
<tr>
<td>A22</td>
<td>cheekSquintRight</td>
<td><img src="https://static.wixstatic.com/media/64c63b_2f82d4db05764690b33001da1d138f20~mv2.png/v1/fill/w_252,h_175,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_2f82d4db05764690b33001da1d138f20~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_49752693e89a4293982b5e023a0e1c75~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_49752693e89a4293982b5e023a0e1c75~mv2.png" alt=""></td>
</tr>
<tr>
<td>A23</td>
<td>noseSneerLeft</td>
<td><img src="https://static.wixstatic.com/media/64c63b_843177402d2545d1a1f0a97e848df91c~mv2.png/v1/fill/w_252,h_178,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_843177402d2545d1a1f0a97e848df91c~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_9855bb8e50f54f638d4bc321dd3caa45~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_9855bb8e50f54f638d4bc321dd3caa45~mv2.png" alt=""></td>
</tr>
<tr>
<td>A24</td>
<td>noseSneerRight</td>
<td><img src="https://static.wixstatic.com/media/64c63b_8ee42dc6d8e443a0858e0c65ce56cc74~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_8ee42dc6d8e443a0858e0c65ce56cc74~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_edd25cffbf1249d89bb0f6c5a95b76e5~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_edd25cffbf1249d89bb0f6c5a95b76e5~mv2.png" alt=""></td>
</tr>
<tr>
<td>A25</td>
<td>jawOpen</td>
<td><img src="https://static.wixstatic.com/media/64c63b_aca391d5eb744a76b18d6ced31904111~mv2.png/v1/fill/w_267,h_192,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_aca391d5eb744a76b18d6ced31904111~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_f14421d8adb1461ea32ca31bd3cac7be~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_f14421d8adb1461ea32ca31bd3cac7be~mv2.png" alt=""></td>
</tr>
<tr>
<td>A26</td>
<td>jawForward</td>
<td><img src="https://static.wixstatic.com/media/64c63b_a199113be0f9418f8c729d9a7e7b4e49~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_a199113be0f9418f8c729d9a7e7b4e49~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_df9963f3452c4ce1bd1b6829a8045112~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_df9963f3452c4ce1bd1b6829a8045112~mv2.png" alt=""></td>
</tr>
<tr>
<td>A27</td>
<td>jawLeft</td>
<td><img src="https://static.wixstatic.com/media/64c63b_2642a61cdd0241f9ba24339873003125~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_2642a61cdd0241f9ba24339873003125~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_0de7a40c68654461be74016c2e29cf02~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_0de7a40c68654461be74016c2e29cf02~mv2.png" alt=""></td>
</tr>
<tr>
<td>A28</td>
<td>jawRight</td>
<td><img src="https://static.wixstatic.com/media/64c63b_2838229bffe74a5abe7d25d9c6e398ca~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_2838229bffe74a5abe7d25d9c6e398ca~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_2f51f3993bed441b89b9b493a1f2e86b~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_2f51f3993bed441b89b9b493a1f2e86b~mv2.png" alt=""></td>
</tr>
<tr>
<td>A29</td>
<td>mouthFunnel</td>
<td><img src="https://static.wixstatic.com/media/64c63b_d2719d8d83524b52a735296f0dfbf092~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_d2719d8d83524b52a735296f0dfbf092~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_e2b4d7681dcf4b1faf34e3c5b57dd3ac~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e2b4d7681dcf4b1faf34e3c5b57dd3ac~mv2.png" alt=""></td>
</tr>
<tr>
<td>A30</td>
<td>mouthPucker</td>
<td><img src="https://static.wixstatic.com/media/64c63b_7771a95fb2ae4afeb885b7a684e3f249~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_7771a95fb2ae4afeb885b7a684e3f249~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_e553a24166984303920d5ec9ce1de6d6~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e553a24166984303920d5ec9ce1de6d6~mv2.png" alt=""></td>
</tr>
<tr>
<td>A31</td>
<td>mouthLeft</td>
<td><img src="https://static.wixstatic.com/media/64c63b_d2e30cadc9b443f6993ee48d99ffb9c8~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_d2e30cadc9b443f6993ee48d99ffb9c8~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_76b134e6564749fcaf6e036a6dc53517~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_76b134e6564749fcaf6e036a6dc53517~mv2.png" alt=""></td>
</tr>
<tr>
<td>A32</td>
<td>mouthRight</td>
<td><img src="https://static.wixstatic.com/media/64c63b_ef34b0cf15c541058052d74870f95a11~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_ef34b0cf15c541058052d74870f95a11~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_332e51118068490cbb932bc8b3880895~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_332e51118068490cbb932bc8b3880895~mv2.png" alt=""></td>
</tr>
<tr>
<td>A33</td>
<td>mouthRollUpper</td>
<td><img src="https://static.wixstatic.com/media/64c63b_5c93c56f5d9e4698a86160047452fdae~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_5c93c56f5d9e4698a86160047452fdae~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_f86e02c72dcd4e27b5fafc3e7cbf5098~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_f86e02c72dcd4e27b5fafc3e7cbf5098~mv2.png" alt=""></td>
</tr>
<tr>
<td>A34</td>
<td>mouthRollLower</td>
<td><img src="https://static.wixstatic.com/media/64c63b_8d2d50c4784b4f4a881264f9e806b26e~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_8d2d50c4784b4f4a881264f9e806b26e~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_f6ce7f1df803456fb25b77533ec5c1a9~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_f6ce7f1df803456fb25b77533ec5c1a9~mv2.png" alt=""></td>
</tr>
<tr>
<td>A35</td>
<td>mouthShrugUpper</td>
<td><img src="https://static.wixstatic.com/media/64c63b_d70a5a8102d14df6be57658f696ab28c~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_d70a5a8102d14df6be57658f696ab28c~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_36feb9bc9305402e8a9e044b7f42c06e~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_36feb9bc9305402e8a9e044b7f42c06e~mv2.png" alt=""></td>
</tr>
<tr>
<td>A36</td>
<td>mouthShrugLower</td>
<td><img src="https://static.wixstatic.com/media/64c63b_c39b6573ab8b452c8ba9af4cfd61fa0d~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_c39b6573ab8b452c8ba9af4cfd61fa0d~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_881391abc1ff4fbabd6f7719d93179b8~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_881391abc1ff4fbabd6f7719d93179b8~mv2.png" alt=""></td>
</tr>
<tr>
<td>A37</td>
<td>mouthClose</td>
<td><img src="https://static.wixstatic.com/media/64c63b_7c1a9921e54c42c5bbad10ce2d2a2edc~mv2.png/v1/fill/w_267,h_129,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_7c1a9921e54c42c5bbad10ce2d2a2edc~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_8aded518da54400db938b69753b8539a~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_8aded518da54400db938b69753b8539a~mv2.png" alt=""></td>
</tr>
<tr>
<td>A38</td>
<td>mouthSmileLeft</td>
<td><img src="https://static.wixstatic.com/media/64c63b_a3cdfd578cec40a5a83931c4d0c9f8ab~mv2.png/v1/fill/w_267,h_186,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_a3cdfd578cec40a5a83931c4d0c9f8ab~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_11aa5137231b4bfe8a8908f25d8d4112~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_11aa5137231b4bfe8a8908f25d8d4112~mv2.png" alt=""></td>
</tr>
<tr>
<td>A39</td>
<td>mouthSmileRight</td>
<td><img src="https://static.wixstatic.com/media/64c63b_f0c7a9ddfcb945f496f4ac8aafcfd0ca~mv2.png/v1/fill/w_267,h_186,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_f0c7a9ddfcb945f496f4ac8aafcfd0ca~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_4818df7bf47740f6bab387d0d2926a2b~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_4818df7bf47740f6bab387d0d2926a2b~mv2.png" alt=""></td>
</tr>
<tr>
<td>A40</td>
<td>mouthFrownLeft</td>
<td><img src="https://static.wixstatic.com/media/64c63b_8e7c89a5e9514206ac3fd7152e912ef8~mv2.png/v1/fill/w_267,h_187,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_8e7c89a5e9514206ac3fd7152e912ef8~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_ba5a9fdcf6d246439b8d7d9dbf63fb16~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_ba5a9fdcf6d246439b8d7d9dbf63fb16~mv2.png" alt=""></td>
</tr>
<tr>
<td>A41</td>
<td>mouthFrownRight</td>
<td><img src="https://static.wixstatic.com/media/64c63b_019c769729a34c7c992c3bbde95adf2a~mv2.png/v1/fill/w_267,h_187,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_019c769729a34c7c992c3bbde95adf2a~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_4505259aa94646278b01cd6b4e6fe32a~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_4505259aa94646278b01cd6b4e6fe32a~mv2.png" alt=""></td>
</tr>
<tr>
<td>A42</td>
<td>mouthDimpleLeft</td>
<td><img src="https://static.wixstatic.com/media/64c63b_b7c5c7b4fcea481ba877fab837ddda7c~mv2.png/v1/fill/w_267,h_187,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_b7c5c7b4fcea481ba877fab837ddda7c~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_53d010f6b8b340d6a305149152fe9eb2~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_53d010f6b8b340d6a305149152fe9eb2~mv2.png" alt=""></td>
</tr>
<tr>
<td>A43</td>
<td>mouthDimpleRight</td>
<td><img src="https://static.wixstatic.com/media/64c63b_268dda3d9bb14eaba63c5b123ab9002c~mv2.png/v1/fill/w_267,h_187,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_268dda3d9bb14eaba63c5b123ab9002c~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_ea46553169c749f69dc8e47737434193~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_ea46553169c749f69dc8e47737434193~mv2.png" alt=""></td>
</tr>
<tr>
<td>A44</td>
<td>mouthUpperUpLeft</td>
<td><img src="https://static.wixstatic.com/media/64c63b_e6f82a77cd374e37b456590eb19c2d28~mv2.png/v1/fill/w_267,h_186,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e6f82a77cd374e37b456590eb19c2d28~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_5c9a52ea901243218e0c9252fcd45a00~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_5c9a52ea901243218e0c9252fcd45a00~mv2.png" alt=""></td>
</tr>
<tr>
<td>A45</td>
<td>mouthUpperUpRight</td>
<td><img src="https://static.wixstatic.com/media/64c63b_384bab2c926045f99f4bbef75b6975f0~mv2.png/v1/fill/w_267,h_186,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_384bab2c926045f99f4bbef75b6975f0~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_8abd87f586bb4d2088673a2358a65adb~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_8abd87f586bb4d2088673a2358a65adb~mv2.png" alt=""></td>
</tr>
<tr>
<td>A46</td>
<td>mouthLowerDownLeft</td>
<td><img src="https://static.wixstatic.com/media/64c63b_2511f8304fbb49dab88eb09b118f88bc~mv2.png/v1/fill/w_267,h_188,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_2511f8304fbb49dab88eb09b118f88bc~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_21506f6994114f1194bc69958bd3778d~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_21506f6994114f1194bc69958bd3778d~mv2.png" alt=""></td>
</tr>
<tr>
<td>A47</td>
<td>mouthLowerDownRight</td>
<td><img src="https://static.wixstatic.com/media/64c63b_5c300e220ef04f388b827c096ad7aae6~mv2.png/v1/fill/w_267,h_188,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_5c300e220ef04f388b827c096ad7aae6~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_9ac53c48df9e4d63b6774b91aaa4db3d~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_9ac53c48df9e4d63b6774b91aaa4db3d~mv2.png" alt=""></td>
</tr>
<tr>
<td>A48</td>
<td>mouthPressLeft</td>
<td><img src="https://static.wixstatic.com/media/64c63b_478c881ace1744ff825202484b212c17~mv2.png/v1/fill/w_267,h_187,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_478c881ace1744ff825202484b212c17~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_cca358e42c08454cb9f7f30317c4e93c~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_cca358e42c08454cb9f7f30317c4e93c~mv2.png" alt=""></td>
</tr>
<tr>
<td>A49</td>
<td>mouthPressRight</td>
<td><img src="https://static.wixstatic.com/media/64c63b_acad007d32d24b26b4cc192345afc0ba~mv2.png/v1/fill/w_267,h_186,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_acad007d32d24b26b4cc192345afc0ba~mv2.png" alt=""></td>
<td><br><img src="https://static.wixstatic.com/media/64c63b_35bac2e5acf54d438dd0acf4690c4ea2~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_35bac2e5acf54d438dd0acf4690c4ea2~mv2.png" alt=""></td>
</tr>
<tr>
<td>A50</td>
<td>mouthStretchLeft</td>
<td><img src="https://static.wixstatic.com/media/64c63b_18fbf15030164a6383068c8fb7aa7e72~mv2.png/v1/fill/w_263,h_184,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_18fbf15030164a6383068c8fb7aa7e72~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_cf77104a546149e88698feb420726493~mv2.png/v1/fill/w_232,h_209,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_cf77104a546149e88698feb420726493~mv2.png" alt=""></td>
</tr>
<tr>
<td>A51</td>
<td>mouthStretchRight</td>
<td><img src="https://static.wixstatic.com/media/64c63b_3f8dd987a3d44b7e98e1e7abb1815111~mv2.png/v1/fill/w_263,h_184,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_3f8dd987a3d44b7e98e1e7abb1815111~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_b2a3abb6ea204ab293571c7c19747003~mv2.png/v1/fill/w_232,h_209,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_b2a3abb6ea204ab293571c7c19747003~mv2.png" alt=""></td>
</tr>
<tr>
<td>A52</td>
<td>tongueOut</td>
<td></td>
<td><img src="https://static.wixstatic.com/media/64c63b_10387f10b0e04d5fac672f8bd17d9459~mv2.png/v1/fill/w_232,h_209,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_10387f10b0e04d5fac672f8bd17d9459~mv2.png" alt=""></td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>CC3 额外的舌头Blendshape(with open month)：</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>T01</th>
<th>Tongue_Up</th>
<th></th>
<th><img src="https://static.wixstatic.com/media/64c63b_75c512342cde45ffbb40fcf5d463732e~mv2.png/v1/fill/w_244,h_220,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_75c512342cde45ffbb40fcf5d463732e~mv2.png" alt=""></th>
</tr>
</thead>
<tbody>
<tr>
<td>T02</td>
<td>Tongue_Down</td>
<td></td>
<td><img src="https://static.wixstatic.com/media/64c63b_4bdf00b4d23d4a89ac0bffbb66cc348d~mv2.png/v1/fill/w_244,h_220,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_4bdf00b4d23d4a89ac0bffbb66cc348d~mv2.png" alt=""></td>
</tr>
<tr>
<td>T03</td>
<td>Tongue_Left</td>
<td></td>
<td><img src="https://static.wixstatic.com/media/64c63b_860b7c7043894521a754755c35816cb3~mv2.png/v1/fill/w_244,h_220,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_860b7c7043894521a754755c35816cb3~mv2.png" alt=""></td>
</tr>
<tr>
<td>T04</td>
<td>Tongue_Right</td>
<td></td>
<td><img src="https://static.wixstatic.com/media/64c63b_02c7adc74f934d31ad35c01615b96735~mv2.png/v1/fill/w_244,h_220,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_02c7adc74f934d31ad35c01615b96735~mv2.png" alt=""></td>
</tr>
<tr>
<td>T05</td>
<td>Tongue_Roll</td>
<td></td>
<td><img src="https://static.wixstatic.com/media/64c63b_bfa27b0483c94f7484eeed246642fbc5~mv2.png/v1/fill/w_244,h_220,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_bfa27b0483c94f7484eeed246642fbc5~mv2.png" alt=""></td>
</tr>
<tr>
<td>T06</td>
<td>Tongue_Tip_Up</td>
<td></td>
<td><img src="https://static.wixstatic.com/media/64c63b_6a7d96ce1444409c958adc03652983b7~mv2.png/v1/fill/w_244,h_220,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_6a7d96ce1444409c958adc03652983b7~mv2.png" alt=""></td>
</tr>
<tr>
<td>T07</td>
<td>Tongue_Tip_Down</td>
<td></td>
<td><img src="https://static.wixstatic.com/media/64c63b_37fa335ad1a549b983fb6552db3b5198~mv2.png/v1/fill/w_244,h_220,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_37fa335ad1a549b983fb6552db3b5198~mv2.png" alt=""></td>
</tr>
</tbody>
</table>
</div>
<h3 id="Vive面部的表情基准组"><a href="#Vive面部的表情基准组" class="headerlink" title="Vive面部的表情基准组"></a>Vive面部的表情基准组</h3><p>Vive这一套脸部追踪也是52个blendshapes，但是和苹果的基准有很大区别。</p>
<ul>
<li>区别一：舌头</li>
</ul>
<p>苹果其实是52+7，因为舌头在52个里只有一个伸舌头的blendshape，但vive其实是42 + 10，整体来讲Vive表情记住能tracking到的表情细节还是更少一些。</p>
<ul>
<li>区别二：眉毛</li>
</ul>
<p>ARKit的52个blendshapes，是根据硬件分区一对一tracking的，然而Vive眉毛不分是没有单独另设blendshapes，而是与眼睛的动作blended在一起作为一个blendshape的，并不是精准的一对一分区tracking。</p>
<p>我下面编号的排序是按照<a href="https://developer.vive.com/resources/vive-sense/sdk/vive-eye-and-facial-tracking-sdk/">VIVE Eye and Facial Tracking SDK</a> unity 里inspector里的顺序，方便我加表情。</p>
<p>这里是整理的用ARKit制作Vive基准的对应编号：</p>
<p><a href="https://docs.google.com/spreadsheets/d/1kWXnqtiVbXRb1FrD5NLlxxuxbYmS0Z6YBLuIE1WwqD4/edit?usp=sharing">https://docs.google.com/spreadsheets/d/1kWXnqtiVbXRb1FrD5NLlxxuxbYmS0Z6YBLuIE1WwqD4/edit?usp=sharing</a></p>
<ul>
<li>Eye Blendshapes （14 = 12 + 2）</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>Vive编号</th>
<th>Vive表情基准</th>
<th>Vive Picture</th>
<th>Create by CC3 blendshapes</th>
</tr>
</thead>
<tbody>
<tr>
<td>V01</td>
<td>Eye_Left_Blink</td>
<td><img src="https://static.wixstatic.com/media/64c63b_735cb0ae227e42bca98f9c51fbd0df6b~mv2.png/v1/fill/w_238,h_182,al_c,lg_1,q_85,enc_auto/64c63b_735cb0ae227e42bca98f9c51fbd0df6b~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_e5f3c0dfc63a42618e182a9b1a0c1e9c~mv2.png/v1/fill/w_211,h_188,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e5f3c0dfc63a42618e182a9b1a0c1e9c~mv2.png" alt=""></td>
</tr>
<tr>
<td>V02</td>
<td>Eye_Left_Wide</td>
<td><img src="https://static.wixstatic.com/media/64c63b_1d9223fb44574a94988a7c9ce4d89b39~mv2.png/v1/fill/w_222,h_160,al_c,lg_1,q_85,enc_auto/64c63b_1d9223fb44574a94988a7c9ce4d89b39~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_5dd9ed05165f4fe9b486b4f0604dacc1~mv2.png/v1/fill/w_211,h_188,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_5dd9ed05165f4fe9b486b4f0604dacc1~mv2.png" alt=""></td>
</tr>
<tr>
<td>V03</td>
<td>Eye_Left_Right</td>
<td><img src="https://static.wixstatic.com/media/64c63b_638f42e4960743c4b235ab18b5ac6eba~mv2.png/v1/fill/w_238,h_188,al_c,lg_1,q_85,enc_auto/64c63b_638f42e4960743c4b235ab18b5ac6eba~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_c5fcda96b7184941b2a89b2193470f2b~mv2.png/v1/fill/w_211,h_188,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_c5fcda96b7184941b2a89b2193470f2b~mv2.png" alt=""></td>
</tr>
<tr>
<td>V04</td>
<td>Eye_Left_Left</td>
<td><img src="https://static.wixstatic.com/media/64c63b_a1041585f54748728dd71aec7de129f5~mv2.png/v1/fill/w_238,h_192,al_c,lg_1,q_85,enc_auto/64c63b_a1041585f54748728dd71aec7de129f5~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_a26c36f7dc2940fe9513e263f7a99c4e~mv2.png/v1/fill/w_211,h_188,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_a26c36f7dc2940fe9513e263f7a99c4e~mv2.png" alt=""></td>
</tr>
<tr>
<td>V05</td>
<td>Eye_Left_Up</td>
<td><img src="https://static.wixstatic.com/media/64c63b_d2c84d05015c4d8289f6c7d6d5ba0dcc~mv2.png/v1/fill/w_238,h_203,al_c,lg_1,q_85,enc_auto/64c63b_d2c84d05015c4d8289f6c7d6d5ba0dcc~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_e3e67e16e4b84697a39c3333bad24712~mv2.png/v1/fill/w_211,h_188,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e3e67e16e4b84697a39c3333bad24712~mv2.png" alt=""></td>
</tr>
<tr>
<td>V06</td>
<td>Eye_Left_Down</td>
<td><img src="https://static.wixstatic.com/media/64c63b_5cd43316383549e282e7f3f743df9053~mv2.png/v1/fill/w_238,h_195,al_c,lg_1,q_85,enc_auto/64c63b_5cd43316383549e282e7f3f743df9053~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_993bd278fb024580a834a71c6886cc4b~mv2.png/v1/fill/w_211,h_188,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_993bd278fb024580a834a71c6886cc4b~mv2.png" alt=""></td>
</tr>
<tr>
<td>V07</td>
<td>Eye_Right_Blink</td>
<td><img src="https://static.wixstatic.com/media/64c63b_2e198e55d97a491cbc66059e6f6adddc~mv2.png/v1/fill/w_235,h_195,al_c,lg_1,q_85,enc_auto/64c63b_2e198e55d97a491cbc66059e6f6adddc~mv2.png" alt=""></td>
<td></td>
</tr>
<tr>
<td>V08</td>
<td>Eye_Right_Wide</td>
<td><img src="https://static.wixstatic.com/media/64c63b_6eebbecf575d4bec905be4dbec06322c~mv2.png/v1/fill/w_223,h_160,al_c,lg_1,q_85,enc_auto/64c63b_6eebbecf575d4bec905be4dbec06322c~mv2.png" alt=""></td>
<td></td>
</tr>
<tr>
<td>V09</td>
<td>Eye_Right_Right</td>
<td><img src="https://static.wixstatic.com/media/64c63b_fc88e475e8c4435a98364224f54ade1a~mv2.png/v1/fill/w_234,h_197,al_c,lg_1,q_85,enc_auto/64c63b_fc88e475e8c4435a98364224f54ade1a~mv2.png" alt=""></td>
<td></td>
</tr>
<tr>
<td>V10</td>
<td>Eye_Right_Left</td>
<td><img src="https://static.wixstatic.com/media/64c63b_216b900672314c37a3e91501b7fe7cc1~mv2.png/v1/fill/w_238,h_190,al_c,lg_1,q_85,enc_auto/64c63b_216b900672314c37a3e91501b7fe7cc1~mv2.png" alt=""></td>
<td></td>
</tr>
<tr>
<td>V11</td>
<td>Eye_Right_Up</td>
<td><img src="https://static.wixstatic.com/media/64c63b_062847a5c45d4dabbe78d255779013dd~mv2.png/v1/fill/w_231,h_196,al_c,lg_1,q_85,enc_auto/64c63b_062847a5c45d4dabbe78d255779013dd~mv2.png" alt=""></td>
<td></td>
</tr>
<tr>
<td>V12</td>
<td>Eye_Right_Down</td>
<td><img src="https://static.wixstatic.com/media/64c63b_b6fae35fba8543d0becc535532111d23~mv2.png/v1/fill/w_238,h_176,al_c,lg_1,q_85,enc_auto/64c63b_b6fae35fba8543d0becc535532111d23~mv2.png" alt=""></td>
<td></td>
</tr>
<tr>
<td>V13</td>
<td>Eye_Left_squeeze: The blendShape close eye tightly when Eye_Left_Blink  value is 100.</td>
<td><img src="https://static.wixstatic.com/media/64c63b_c7b07f1b4ec6495685d85808d23c04e8~mv2.png/v1/fill/w_238,h_183,al_c,lg_1,q_85,enc_auto/64c63b_c7b07f1b4ec6495685d85808d23c04e8~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_a629d31ace624dd8b2ded5123123156e~mv2.png/v1/fill/w_211,h_188,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_a629d31ace624dd8b2ded5123123156e~mv2.png" alt=""></td>
</tr>
<tr>
<td>V14</td>
<td>Eye_Right_squeeze</td>
<td><img src="https://static.wixstatic.com/media/64c63b_b82c535f90874742b3c0b6ff62136fe2~mv2.png/v1/fill/w_238,h_194,al_c,lg_1,q_85,enc_auto/64c63b_b82c535f90874742b3c0b6ff62136fe2~mv2.png" alt=""></td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>Lip Blendshapes （38 = 37 + 1）</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>Vive编号</th>
<th>Vive表情基准</th>
<th>Vive Picture</th>
<th>Create by CC3 blendshapes</th>
</tr>
</thead>
<tbody>
<tr>
<td>V15</td>
<td>Jaw_Right</td>
<td><img src="https://static.wixstatic.com/media/64c63b_75f8b68a96104ef8bf36e393c7ecd48b~mv2.png/v1/fill/w_235,h_190,al_c,lg_1,q_85,enc_auto/64c63b_75f8b68a96104ef8bf36e393c7ecd48b~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_65471d28b0d743c0bb6232ffaee0f6b6~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_65471d28b0d743c0bb6232ffaee0f6b6~mv2.png" alt=""></td>
</tr>
<tr>
<td>V16</td>
<td>Jaw_Left</td>
<td><img src="https://static.wixstatic.com/media/64c63b_9cacde29288c4523a2192835a736ad6b~mv2.png/v1/fill/w_245,h_202,al_c,lg_1,q_85,enc_auto/64c63b_9cacde29288c4523a2192835a736ad6b~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_017d7142fd494aef9ad7bbe53fa1d6eb~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_017d7142fd494aef9ad7bbe53fa1d6eb~mv2.png" alt=""></td>
</tr>
<tr>
<td>V17</td>
<td>Jaw_Forward</td>
<td><img src="https://static.wixstatic.com/media/64c63b_3876f3dd1a1b4eed92e8405b42700190~mv2.png/v1/fill/w_248,h_197,al_c,lg_1,q_85,enc_auto/64c63b_3876f3dd1a1b4eed92e8405b42700190~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_20353f83579541428557c32d92545c9e~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_20353f83579541428557c32d92545c9e~mv2.png" alt=""></td>
</tr>
<tr>
<td>V18</td>
<td>Jaw_Open</td>
<td><img src="https://static.wixstatic.com/media/64c63b_dc79f10003534839948d3261183d5082~mv2.png/v1/fill/w_244,h_188,al_c,lg_1,q_85,enc_auto/64c63b_dc79f10003534839948d3261183d5082~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_24470b9cc9964a11906c42b1d1a6e5e9~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_24470b9cc9964a11906c42b1d1a6e5e9~mv2.png" alt=""></td>
</tr>
<tr>
<td>V19</td>
<td>Mouth_Ape_Shape</td>
<td><img src="https://static.wixstatic.com/media/64c63b_7a0f9461a760449db12b2159009ccc93~mv2.png/v1/fill/w_249,h_196,al_c,lg_1,q_85,enc_auto/64c63b_7a0f9461a760449db12b2159009ccc93~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_3d7911f5bfa645adb7f3c36fbeafa2b9~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_3d7911f5bfa645adb7f3c36fbeafa2b9~mv2.png" alt=""></td>
</tr>
<tr>
<td>V20</td>
<td>Mouth_Upper_Right</td>
<td><img src="https://static.wixstatic.com/media/64c63b_01037e0042754059b7ada72a8adf2e8a~mv2.png/v1/fill/w_227,h_161,al_c,lg_1,q_85,enc_auto/64c63b_01037e0042754059b7ada72a8adf2e8a~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_185ec305ba464016a15c2420fb04916e~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_185ec305ba464016a15c2420fb04916e~mv2.png" alt=""></td>
</tr>
<tr>
<td>V21</td>
<td>Mouth_Upper_Left</td>
<td><img src="https://static.wixstatic.com/media/64c63b_f0c61e8f3f3c42d7ad6d83703f1a61d9~mv2.png/v1/fill/w_265,h_182,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_f0c61e8f3f3c42d7ad6d83703f1a61d9~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_abbecb4860a44fe9800585825beb4b17~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_abbecb4860a44fe9800585825beb4b17~mv2.png" alt=""></td>
</tr>
<tr>
<td>V22</td>
<td>Mouth_Lower_Right</td>
<td><img src="https://static.wixstatic.com/media/64c63b_3ec74984b19d44d389a68bcc1ac1a7fb~mv2.png/v1/fill/w_265,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_3ec74984b19d44d389a68bcc1ac1a7fb~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_80a4cdffa3fb493e9c153b517d9aebda~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_80a4cdffa3fb493e9c153b517d9aebda~mv2.png" alt=""></td>
</tr>
<tr>
<td>V23</td>
<td>Mouth_Lower_Left</td>
<td><img src="https://static.wixstatic.com/media/64c63b_e80cee8738ea42528c8f351303f5e2c8~mv2.png/v1/fill/w_265,h_225,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e80cee8738ea42528c8f351303f5e2c8~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_edf8e441ff994c27bde811a21754d5e5~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_edf8e441ff994c27bde811a21754d5e5~mv2.png" alt=""></td>
</tr>
<tr>
<td>V24</td>
<td>*Mouth_Upper_Overturn</td>
<td><img src="https://static.wixstatic.com/media/64c63b_5f77c14164ae48cf9c0cf4c762b97837~mv2.png/v1/fill/w_265,h_202,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_5f77c14164ae48cf9c0cf4c762b97837~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_b8ae358e723f42e199338722f186e238~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_b8ae358e723f42e199338722f186e238~mv2.png" alt=""></td>
</tr>
<tr>
<td>V25</td>
<td>*Mouth_Lower_Overturn</td>
<td><img src="https://static.wixstatic.com/media/64c63b_16a26f9ced50420b99a4c32fc296c112~mv2.png/v1/fill/w_265,h_210,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_16a26f9ced50420b99a4c32fc296c112~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_282323813dfa4ea1aa76551b112e3919~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_282323813dfa4ea1aa76551b112e3919~mv2.png" alt=""></td>
</tr>
<tr>
<td>V26</td>
<td>Mouth_Pout</td>
<td><img src="https://static.wixstatic.com/media/64c63b_e7da853fe63242a9bedbd7fe3bddadc7~mv2.png/v1/fill/w_265,h_205,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e7da853fe63242a9bedbd7fe3bddadc7~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_36615908f74d4663a6bc438c3287938c~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_36615908f74d4663a6bc438c3287938c~mv2.png" alt=""></td>
</tr>
<tr>
<td>V27</td>
<td>Mouth_Smile_Right</td>
<td><img src="https://static.wixstatic.com/media/64c63b_1de129dc23784dd0af8d5bbccff75741~mv2.png/v1/fill/w_265,h_205,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_1de129dc23784dd0af8d5bbccff75741~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_47a00d3e749e47fa8b3489da81252654~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_47a00d3e749e47fa8b3489da81252654~mv2.png" alt=""></td>
</tr>
<tr>
<td>V28</td>
<td>Mouth_Smile_Left</td>
<td><img src="https://static.wixstatic.com/media/64c63b_e66c5606123d4272bc4d3206a101e884~mv2.png/v1/fill/w_265,h_213,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e66c5606123d4272bc4d3206a101e884~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_0a26740959644351bb01f9e9d40ef35e~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_0a26740959644351bb01f9e9d40ef35e~mv2.png" alt=""></td>
</tr>
<tr>
<td>V29</td>
<td>Mouth_Sad_Right</td>
<td><img src="https://static.wixstatic.com/media/64c63b_ba5427a221c2446e9d3b9e30d94d80b9~mv2.png/v1/fill/w_265,h_224,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_ba5427a221c2446e9d3b9e30d94d80b9~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_c1e99be3d6c34038852ce55b72102f5c~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_c1e99be3d6c34038852ce55b72102f5c~mv2.png" alt=""></td>
</tr>
<tr>
<td>V30</td>
<td>Mouth_Sad_Left</td>
<td><img src="https://static.wixstatic.com/media/64c63b_00772c50ca334cbf95dd1bf53be4c6b8~mv2.png/v1/fill/w_265,h_218,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_00772c50ca334cbf95dd1bf53be4c6b8~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_889e2637303f4d7195afd699a3d92b86~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_889e2637303f4d7195afd699a3d92b86~mv2.png" alt=""></td>
</tr>
<tr>
<td>V31</td>
<td>Cheek_Puff_Right</td>
<td><img src="https://static.wixstatic.com/media/64c63b_765350d6685547d4b03b7ae31e7346e0~mv2.png/v1/fill/w_265,h_224,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_765350d6685547d4b03b7ae31e7346e0~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_66386b8d632b4556a00f886613f26d92~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_66386b8d632b4556a00f886613f26d92~mv2.png" alt=""></td>
</tr>
<tr>
<td>V32</td>
<td>Cheek_Puff_Left</td>
<td><img src="https://static.wixstatic.com/media/64c63b_2998211eb141496d8651b786337b7846~mv2.png/v1/fill/w_265,h_213,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_2998211eb141496d8651b786337b7846~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_833f433253fb4bcc8e380d79120b3003~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_833f433253fb4bcc8e380d79120b3003~mv2.png" alt=""></td>
</tr>
<tr>
<td>V33</td>
<td>Cheek_Suck</td>
<td><img src="https://static.wixstatic.com/media/64c63b_6eea541e05494d26a06cbbe5377cdc0a~mv2.png/v1/fill/w_265,h_209,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_6eea541e05494d26a06cbbe5377cdc0a~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_8573de7fc0d84245a0fa4412ecd3e842~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_8573de7fc0d84245a0fa4412ecd3e842~mv2.png" alt=""></td>
</tr>
<tr>
<td>V34</td>
<td>Mouth_Upper_UpRight</td>
<td><img src="https://static.wixstatic.com/media/64c63b_b6ca87cbb7774b2ab4f0ec3748ec9c51~mv2.png/v1/fill/w_265,h_216,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_b6ca87cbb7774b2ab4f0ec3748ec9c51~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_c48b985609fe4129ad1dac8a41905a7e~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_c48b985609fe4129ad1dac8a41905a7e~mv2.png" alt=""></td>
</tr>
<tr>
<td>V35</td>
<td>Mouth<em>Upper</em> UpLeft</td>
<td><img src="https://static.wixstatic.com/media/64c63b_58bdc8db0ac3451388534ff3bfb0fa83~mv2.png/v1/fill/w_265,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_58bdc8db0ac3451388534ff3bfb0fa83~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_5ff3f05aeecd48fe9f3077f5c9c96569~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_5ff3f05aeecd48fe9f3077f5c9c96569~mv2.png" alt=""></td>
</tr>
<tr>
<td>V36</td>
<td>Mouth_Lower_DownRight</td>
<td><img src="https://static.wixstatic.com/media/64c63b_12f26efe2f28425cb366eea55e83470c~mv2.png/v1/fill/w_265,h_223,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_12f26efe2f28425cb366eea55e83470c~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_30ae27fa9ee94aa8a1b29bbd5fe7b0b2~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_30ae27fa9ee94aa8a1b29bbd5fe7b0b2~mv2.png" alt=""></td>
</tr>
<tr>
<td>V37</td>
<td>Mouth_Lower_DownLeft</td>
<td><img src="https://static.wixstatic.com/media/64c63b_4ed1a27945324b53aad0aa3cf453a275~mv2.png/v1/fill/w_265,h_218,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_4ed1a27945324b53aad0aa3cf453a275~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_76893f9855fa4a5a9415cd8abfae6f6f~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_76893f9855fa4a5a9415cd8abfae6f6f~mv2.png" alt=""></td>
</tr>
<tr>
<td>V38</td>
<td>Mouth_Upper_Inside</td>
<td><img src="https://static.wixstatic.com/media/64c63b_f5e050f0d9954760879ccd18185c2fc8~mv2.png/v1/fill/w_265,h_209,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_f5e050f0d9954760879ccd18185c2fc8~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_5217c686c7c6455aaca6ba1b2ce64217~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_5217c686c7c6455aaca6ba1b2ce64217~mv2.png" alt=""></td>
</tr>
<tr>
<td>V39</td>
<td>Mouth_Lower_Inside</td>
<td><img src="https://static.wixstatic.com/media/64c63b_0031f9adda4441cbb6361e280e594c7b~mv2.png/v1/fill/w_269,h_211,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_0031f9adda4441cbb6361e280e594c7b~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_5108178f718e492eb840f4d678eb3e4e~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_5108178f718e492eb840f4d678eb3e4e~mv2.png" alt=""></td>
</tr>
<tr>
<td>V40</td>
<td>Mouth_Lower_Overlay</td>
<td><img src="https://static.wixstatic.com/media/64c63b_26e5bd6286474b4ea3f4fff3933b91f1~mv2.png/v1/fill/w_269,h_222,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_26e5bd6286474b4ea3f4fff3933b91f1~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_1ab252df4c5146e1815e23a83edb2cd2~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_1ab252df4c5146e1815e23a83edb2cd2~mv2.png" alt=""></td>
</tr>
<tr>
<td>V41</td>
<td>Tongue_LongStep1</td>
<td><img src="https://static.wixstatic.com/media/64c63b_6791ccfceffe4c2ca07f277b91037521~mv2.png/v1/fill/w_269,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_6791ccfceffe4c2ca07f277b91037521~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_4ef4ab94e1ef47dcb01facf5d168f1d1~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_4ef4ab94e1ef47dcb01facf5d168f1d1~mv2.png" alt=""></td>
</tr>
<tr>
<td>V42</td>
<td>Tongue_LongStep2</td>
<td><img src="https://static.wixstatic.com/media/64c63b_e79ccc0096a54ce2b48d188cf6907d0c~mv2.png/v1/fill/w_269,h_181,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e79ccc0096a54ce2b48d188cf6907d0c~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_6e560524670843848266701061f24c63~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_6e560524670843848266701061f24c63~mv2.png" alt=""></td>
</tr>
<tr>
<td>V43</td>
<td>*Tongue_Down</td>
<td><img src="https://static.wixstatic.com/media/64c63b_ac97c6cf9fd940e9b38cdf22ab3c9261~mv2.png/v1/fill/w_269,h_199,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_ac97c6cf9fd940e9b38cdf22ab3c9261~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_a2a8aab70eea4b6ba9378cf249aef3a0~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_a2a8aab70eea4b6ba9378cf249aef3a0~mv2.png" alt=""></td>
</tr>
<tr>
<td>V44</td>
<td>*Tongue_Up</td>
<td><img src="https://static.wixstatic.com/media/64c63b_0c11f06c560842718309c52bc159ffa8~mv2.png/v1/fill/w_269,h_197,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_0c11f06c560842718309c52bc159ffa8~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_0abc5fe7b2da4c988591e18fc6e060ac~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_0abc5fe7b2da4c988591e18fc6e060ac~mv2.png" alt=""></td>
</tr>
<tr>
<td>V45</td>
<td>*Tongue_Right</td>
<td><img src="https://static.wixstatic.com/media/64c63b_0d6e1976f6c342a395f9631be529c694~mv2.png/v1/fill/w_269,h_202,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_0d6e1976f6c342a395f9631be529c694~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_e17db94c36594f32b60ce6057a17aafc~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e17db94c36594f32b60ce6057a17aafc~mv2.png" alt=""></td>
</tr>
<tr>
<td>V46</td>
<td>*Tongue_Left</td>
<td><img src="https://static.wixstatic.com/media/64c63b_2633b9481a6f4e94ad4bfe6a6d52e122~mv2.png/v1/fill/w_269,h_201,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_2633b9481a6f4e94ad4bfe6a6d52e122~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_ad949214aeda4b3488c238af7aabdba6~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_ad949214aeda4b3488c238af7aabdba6~mv2.png" alt=""></td>
</tr>
<tr>
<td>V47</td>
<td>*Tongue_Roll</td>
<td><img src="https://static.wixstatic.com/media/64c63b_4d41918f9af84d0aaaa2de1e354a5706~mv2.png/v1/fill/w_269,h_216,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_4d41918f9af84d0aaaa2de1e354a5706~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_2c544b45c0ea48079bb570912b85b2a3~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_2c544b45c0ea48079bb570912b85b2a3~mv2.png" alt=""></td>
</tr>
<tr>
<td>V48</td>
<td>*Tongue_UpLeft_Morph</td>
<td><img src="https://static.wixstatic.com/media/64c63b_49d65415de0f47d5a07aa77cfebd54e6~mv2.png/v1/fill/w_269,h_201,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_49d65415de0f47d5a07aa77cfebd54e6~mv2.png" alt=""> <img src="https://static.wixstatic.com/media/64c63b_9df53a0e560f41f2a441f9e799b56d1c~mv2.png/v1/fill/w_269,h_221,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_9df53a0e560f41f2a441f9e799b56d1c~mv2.png" alt=""></td>
<td></td>
</tr>
<tr>
<td>V49</td>
<td>*Tongue_UpRight_Morph</td>
<td><img src="https://static.wixstatic.com/media/64c63b_118b5229d2274b5f95a163ebc0d0cfad~mv2.png/v1/fill/w_269,h_232,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_118b5229d2274b5f95a163ebc0d0cfad~mv2.png" alt=""> <img src="https://static.wixstatic.com/media/64c63b_aa40430e533c40c69f0addb2df019a29~mv2.png/v1/fill/w_269,h_215,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_aa40430e533c40c69f0addb2df019a29~mv2.png" alt=""></td>
<td></td>
</tr>
<tr>
<td>V50</td>
<td>*Tongue_DownLeft_Morph</td>
<td><img src="https://static.wixstatic.com/media/64c63b_92fc463c5efc4436a23870d596023ba9~mv2.png/v1/fill/w_269,h_237,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_92fc463c5efc4436a23870d596023ba9~mv2.png" alt=""> <img src="https://static.wixstatic.com/media/64c63b_b3a74eba5433479b96ff645e85681480~mv2.png/v1/fill/w_269,h_231,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_b3a74eba5433479b96ff645e85681480~mv2.png" alt=""></td>
<td></td>
</tr>
<tr>
<td>V51</td>
<td>*Tongue_DownRight_Morph</td>
<td><img src="https://static.wixstatic.com/media/64c63b_db25fb8a736e4b1f9f4e11ba7436e0b0~mv2.png/v1/fill/w_269,h_224,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_db25fb8a736e4b1f9f4e11ba7436e0b0~mv2.png" alt=""> <img src="https://static.wixstatic.com/media/64c63b_b707961c295f40a7995644e93e438ffc~mv2.png/v1/fill/w_269,h_212,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_b707961c295f40a7995644e93e438ffc~mv2.png" alt=""></td>
<td></td>
</tr>
<tr>
<td>V52</td>
<td>*O-shaped mouth</td>
<td><img src="https://static.wixstatic.com/media/64c63b_97cacb52babe4a109cd02874efcb2eda~mv2.png/v1/fill/w_269,h_175,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_97cacb52babe4a109cd02874efcb2eda~mv2.png" alt=""></td>
<td><img src="https://static.wixstatic.com/media/64c63b_6e98ff0232d349b8a6f7d8348992ab37~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_6e98ff0232d349b8a6f7d8348992ab37~mv2.png" alt=""> <img src="https://static.wixstatic.com/media/64c63b_b35e7f3707fc4b0ca75f80c4d77867a4~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_b35e7f3707fc4b0ca75f80c4d77867a4~mv2.png" alt=""></td>
</tr>
</tbody>
</table>
</div>
<h2 id="MediaPipe提取BlendShape"><a href="#MediaPipe提取BlendShape" class="headerlink" title="MediaPipe提取BlendShape"></a>MediaPipe提取BlendShape</h2><p>MediaPipe Face Landmarker解决方案最初于5月的Google I/O 2023发布。它可以检测面部landmark并输出blendshape score，以渲染与用户匹配的3D面部模型。通过MediaPipe Face Landmarker解决方案，KDDI和谷歌成功地为虚拟主播带来了真实感。</p>
<p><strong>技术实现</strong></p>
<p>使用Mediapipe强大而高效的Python包，KDDI开发人员能够检测表演者的面部特征并实时提取52个混合形状。</p>
<p>还可参考：<a href="https://colab.research.google.com/github/googlesamples/mediapipe/blob/main/examples/face_landmarker/python/%5BMediaPipe_Python_Tasks%5D_Face_Landmarker.ipynb">https://colab.research.google.com/github/googlesamples/mediapipe/blob/main/examples/face_landmarker/python/%5BMediaPipe_Python_Tasks%5D_Face_Landmarker.ipynb</a></p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> mediapipe <span class="keyword">as</span> mp</span><br><span class="line"><span class="keyword">from</span> mediapipe.tasks <span class="keyword">import</span> python <span class="keyword">as</span> mp_python</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">MP_TASK_FILE = <span class="string">"face_landmarker_with_blendshapes.task"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FaceMeshDetector</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(MP_TASK_FILE, mode=<span class="string">"rb"</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f_buffer = f.read()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 创建配置选项</span></span><br><span class="line">        base_options = mp_python.BaseOptions(model_asset_buffer=f_buffer)</span><br><span class="line">        options = mp_python.vision.FaceLandmarkerOptions(</span><br><span class="line">            base_options=base_options,</span><br><span class="line">            output_face_blendshapes=<span class="literal">True</span>,</span><br><span class="line">            output_facial_transformation_matrixes=<span class="literal">True</span>,</span><br><span class="line">            running_mode=mp.tasks.vision.RunningMode.LIVE_STREAM,</span><br><span class="line">            num_faces=<span class="number">1</span>,</span><br><span class="line">            result_callback=self.mp_callback</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 创建模型</span></span><br><span class="line">        self.model = mp_python.vision.FaceLandmarker.create_from_options(options)</span><br><span class="line">        self.landmarks = <span class="literal">None</span></span><br><span class="line">        self.blendshapes = <span class="literal">None</span></span><br><span class="line">        self.latest_time_ms = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">mp_callback</span>(<span class="params">self, mp_result, output_image, timestamp_ms: <span class="built_in">int</span></span>):</span><br><span class="line">        <span class="comment"># 处理回调结果</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(mp_result.face_landmarks) &gt;= <span class="number">1</span> <span class="keyword">and</span> <span class="built_in">len</span>(mp_result.face_blendshapes) &gt;= <span class="number">1</span>:</span><br><span class="line">            self.landmarks = mp_result.face_landmarks[<span class="number">0</span>]</span><br><span class="line">            self.blendshapes = [b.score <span class="keyword">for</span> b <span class="keyword">in</span> mp_result.face_blendshapes[<span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update</span>(<span class="params">self, frame</span>):</span><br><span class="line">        t_ms = <span class="built_in">int</span>(time.time() * <span class="number">1000</span>)</span><br><span class="line">        <span class="keyword">if</span> t_ms &lt;= self.latest_time_ms:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        </span><br><span class="line">        frame_mp = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame)</span><br><span class="line">        self.model.detect_async(frame_mp, t_ms)</span><br><span class="line">        self.latest_time_ms = t_ms</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_results</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.landmarks, self.blendshapes</span><br></pre></td></tr></tbody></table></figure>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a href="https://www.mianzi-lizi.com/post/blendshape学习笔记">https://www.mianzi-lizi.com/post/blendshape学习笔记</a></li>
<li><a href="https://www.toutiao.com/article/6915330866285691395/">利用Animoji技术识别用户的表情</a></li>
<li><a href="https://news.nweon.com/110210">通过MediaPipe解决方案来为虚拟主播带来更逼真真实感</a></li>
<li><a href="https://bbs.huaweicloud.com/blogs/374337">Unity &amp; FACEGOOD Audio2Face 通过音频驱动面部BlendShape</a></li>
<li><a href="https://www.cnblogs.com/jesse123/p/9014234.html">GenerativeAI Avatar solutions</a></li>
</ul>
]]></content>
      <categories>
        <category>Note</category>
      </categories>
      <tags>
        <tag>Talking Head Generation</tag>
        <tag>3D reconstruction</tag>
      </tags>
  </entry>
  <entry>
    <title>图床转化脚本</title>
    <url>/2024/01/20/Note/PicConvert/</url>
    <content><![CDATA[<p>Github: <a href="https://github.com/imcyx/PicConvert">https://github.com/imcyx/PicConvert</a></p>
<h2 id="1-安装依赖"><a href="#1-安装依赖" class="headerlink" title="1. 安装依赖"></a>1. 安装依赖</h2><p>该脚本除了python基础依赖库，需要安装 <code>requests</code> 和 <code>requests_toolbelt</code> 两个库：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">pip install requests</span><br><span class="line">pip install requests_toolbelt</span><br></pre></td></tr></tbody></table></figure>
<p>安装完成后即可正常使用脚本</p>
<h2 id="2-个人配置"><a href="#2-个人配置" class="headerlink" title="2. 个人配置"></a>2. 个人配置</h2><p>在目录下的<code>configs.py</code>文件里，用户可以对自己的脚本进行配置，下面对配置进行解读：</p>
<h3 id="a-配置默认使用图床"><a href="#a-配置默认使用图床" class="headerlink" title="a. 配置默认使用图床"></a>a. 配置默认使用图床</h3><p><img src="https://picx.zhimg.com/v2-dbaa44b516d8fc8934cb1239f57779d0.png" alt="配置默认使用图床"></p>
<p>在<code>configs.py</code>文件的第11行可以配置默认使用的图床网站，而使用的图床<strong>必须从下面5种中选择</strong>。可以设置一种或者多种，按表格样式配置即可。</p>
<p>这里推荐使用CSDN，因为目前实测不需要频繁更换cookie，可以比较稳定的使用。</p>
<h3 id="b-配置登录cookie"><a href="#b-配置登录cookie" class="headerlink" title="b. 配置登录cookie"></a>b. 配置登录cookie</h3><p>因为使用的各服务提供商图床需要登录cookie，所以需要用户进入自己的浏览器抓包获得对应字段cookie后填入。</p>
<p>下面介绍各浏览器cookie的获取方法：</p>
<h4 id="CSDN"><a href="#CSDN" class="headerlink" title="CSDN"></a>CSDN</h4><p>登录自己的CSDN，然后进入个人中心 (<a href="https://i.csdn.net/">https://i.csdn.net/</a>)，打开浏览器的开发者工具（chrome 默认 <code>ctrl</code>+<code>alt</code>+<code>I</code>），找到<code>UserName</code>和<code>UserToken</code>，将对应的值复制。</p>
<p><img src="https://pic1.zhimg.com/v2-10ea2f2f64f2fbbef8e61656dee9c1f6.png" alt="CSDN Cookie"></p>
<p>然后粘贴到第26行的 <code>csdn_cookies</code>内，即完成配置。</p>
<p><img src="https://pic1.zhimg.com/v2-4a12554481a33bba2f3e3f421a7944b3.png" alt="CSDN Cookie"></p>
<h4 id="知乎"><a href="#知乎" class="headerlink" title="知乎"></a>知乎</h4><p>登录自己的知乎，然后进入主页 (<a href="https://www.zhihu.com/">https://www.zhihu.com/</a>)，打开浏览器的开发者工具，找到<code>z_c0</code>，将对应的值复制，然后填入33行对应的<code>zhihu_cookies</code>里即完成配置。</p>
<p><img src="https://picx.zhimg.com/v2-c06388579ca46e0ea942d9292d580878.png" alt="知乎 Cookie"></p>
<p>知乎的图片默认支持3种，<code>src</code>, <code>watermark_src</code>, <code>original_src</code>，<code>watermark_src</code>是水印原图，<code>original_src</code>是原图，<code>src</code>是展示图，用户可以自己选择。</p>
<h4 id="b站"><a href="#b站" class="headerlink" title="b站"></a>b站</h4><p>登录自己的b站，然后进入主页 (<a href="https://www.bilibili.com/">https://www.bilibili.com/</a>)，打开浏览器的开发者工具，找到<code>SESSDATA</code>，将对应的值复制，然后填入41行对应的<code>bili_cookies</code>里即完成配置。</p>
<p><img src="https://pic1.zhimg.com/v2-73f566da43bf13b082b4cc569875bad8.png" alt="Bilibili Cookie"></p>
<h4 id="简书"><a href="#简书" class="headerlink" title="简书"></a>简书</h4><p>登录自己的简书，然后进入主页 (<a href="https://www.jianshu.com/">https://www.jianshu.com/</a>)，打开浏览器的开发者工具，找到<code>remember_user_token</code>和<code>_m7e_session_core</code>字段，将对应的值复制，然后填入47行对应的<code>jianshu_cookies</code>里即完成配置。</p>
<p><img src="https://pic1.zhimg.com/v2-ed43bbd15853b83cfe8b4c7ccd473424.png" alt="简书 Cookie"></p>
<h4 id="博客园"><a href="#博客园" class="headerlink" title="博客园"></a>博客园</h4><p>登录自己的博客园，然后进入主页 (<a href="https://www.cnblogs.com/">https://www.cnblogs.com/</a>)，打开浏览器的开发者工具，找到<code>.Cnblogs.AspNetCore.Cookies</code>字段，将对应的值复制，然后填入53行对应的<code>bokeyuan_cookies</code>里即完成配置。</p>
<p><img src="https://pica.zhimg.com/v2-2ec98121095723e0b63f0dce75441907.png" alt="博客园 Cookie"></p>
<h2 id="3-命令行调用"><a href="#3-命令行调用" class="headerlink" title="3.  命令行调用"></a>3.  命令行调用</h2><p>脚本的使用方法为：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">python convert.py</span><br></pre></td></tr></tbody></table></figure>
<p>使用该命令后，默认读取当前脚本所处目录下的所有md文件，并逐个读取扫描图片链接或本地路径，按照配置里指定的转换方式，转换后再输出为{New<em>(mode)</em>(原始名)}。</p>
<p><img src="https://pica.zhimg.com/v2-d85a797b22394c79616e1645f70047a1.png" alt="使用命令行"></p>
<p>如果需要指定转化的文件，使用命令：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">python convert.py -f new.md</span><br></pre></td></tr></tbody></table></figure>
<p>而如果不适用默认的转换图床，需要额外指定转换图床，使用命令：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">python convert.py -m csdn</span><br></pre></td></tr></tbody></table></figure>
<p>这两个参数可以同时指定，转换效果如下：</p>
<p><img src="https://pic1.zhimg.com/v2-56e84953380c8dc11ee8a329c3bc1f5e.png" alt="指定参数"></p>
]]></content>
      <categories>
        <category>Note</category>
      </categories>
      <tags>
        <tag>Note</tag>
        <tag>Tool</tag>
      </tags>
  </entry>
  <entry>
    <title>Tailscale：随时随地远程和使用服务器</title>
    <url>/2024/01/03/Note/Taiscale/</url>
    <content><![CDATA[<h2 id="Tailscale是什么？"><a href="#Tailscale是什么？" class="headerlink" title="Tailscale是什么？"></a>Tailscale是什么？</h2><p>网上有时候提到tailscale，总是介绍好多，比如以下介绍，但是太官方了</p>
<blockquote>
<p>Tailscale 是一种基于 WireGuard 的虚拟组网工具，和 Netmaker 类似，<strong>最大的区别在于 Tailscale 是在用户态实现了 WireGuard 协议，而 Netmaker 直接使用了内核态的 WireGuard</strong>。</p>
</blockquote>
<p>这里面简单介绍一下，实际上Tailscale属于一种虚拟组网工具，基于WireGuard。简单来说他能帮助我们把安装了Tailscale服务的机器，都放到同一个局域网。比如我们的NAS或者PC，或者在其他地方的NAS和PC，甚至云服务器都能放到同一个局域网。这样就实现了一个内网穿透，有时候我们就可以随时随地远程和使用我们的服务器。</p>
<h2 id="Tailscale能做什么？"><a href="#Tailscale能做什么？" class="headerlink" title="Tailscale能做什么？"></a><strong>Tailscale能做什么？</strong></h2><p>只需将你的设备连接到公网，Tailscale就能让所有设备加入同一个私有子网。这意味着，无论你身在何处，都可以轻松实现设备间的连接，就像它们在同一个局域网中一样。</p>
<p>举个例子，我的台式机和笔记本都登录了相同的Tailscale账号，它们共享一个100.64/10的子网，可以方便地互联。即使我的笔记本在公司内网，无法直接连接到家里的台式机，通过Tailscale的relay功能，它们依然能够直接连接，实现畅通无阻的通信。</p>
<h3 id="1、传输文件"><a href="#1、传输文件" class="headerlink" title="1、传输文件"></a><strong>1、传输文件</strong></h3><p>Tailscale内置了taildrop，可用于设备之间的文件传输。由于Tailscale支持android/ios/mac/windows/linux，因此它也是一个跨平台文件传输工具。如果设备在同一局域网内，传输速度将非常快速。</p>
<h3 id="2、远程开发"><a href="#2、远程开发" class="headerlink" title="2、远程开发"></a><strong>2、远程开发</strong></h3><p>举例来说，如果我的台式电脑运行Windows系统，我可以启动WSL2，安装SSHD，相当于将它变成一个服务器。这样，无论我身在何处，都可以通过笔记本上的VSCode Remote SSH随时打开台式机上的VSCode Server，实现远程开发。</p>
<p>对我而言，移动办公的真谛不在于随身携带一台笔记本，而是在任何地方只要有网络，就能使用任何设备接入统一的办公环境。</p>
<h3 id="3、代理"><a href="#3、代理" class="headerlink" title="3、代理"></a><strong>3、代理</strong></h3><p>具体可参考，这里不主要介绍，<a href="https://github.com/nadoo/glider">https://github.com/nadoo/glider</a></p>
<h2 id="Tailscale怎么用？"><a href="#Tailscale怎么用？" class="headerlink" title="Tailscale怎么用？"></a><strong>Tailscale怎么用？</strong></h2><p>参照这个官方页面安装，然后登录即可：<a href="https://tailscale.com/download">https://tailscale.com/download</a></p>
<p><img src="https://picx.zhimg.com/v2-0f0f761a65f56a0818e53fdef6592e11.png" alt="Tailscale官网"></p>
<p>如下图所示，一旦登录，每台设备都会被分配一个对应的IP地址。此时，所有设备实际上都在同一个局域网内，接下来我们可以启动设备的SSH功能。无论身在何处，只需使用分配给设备的IP地址，就能连接到该设备，从而实现远程办公的功能，例如连接到服务器等。</p>
<p><img src="https://picx.zhimg.com/v2-5c27422f68e185584f24f59621ae1ebf.png" alt="设备"></p>
<h2 id="Windows下安装OpenSSH"><a href="#Windows下安装OpenSSH" class="headerlink" title="Windows下安装OpenSSH"></a><strong>Windows下安装OpenSSH</strong></h2><p>OpenSSH 是 SSH （Secure SHell） 协议的免费开源实现。OpenSSH提供了服务端后台程序和客户端工具，用来加密远程控制和文件传输过程中的数据。安装以后，我们就可以把我们的电脑作为一台服务器进行链接，这样就成功可以随时随地进行远程连接了。</p>
<h3 id="在线安装"><a href="#在线安装" class="headerlink" title="在线安装"></a><strong>在线安装</strong></h3><p>一般windows自带SSH server，直接启动即可</p>
<p>开启方法： 安装openssh</p>
<p>设置-应用-应用和功能-可选功能-添加功能</p>
<p>安装OpenSSH服务器即可</p>
<p><img src="https://picx.zhimg.com/v2-aeb2d713f14ed7cf9c25f2340190a631.webp" alt="安装SSH步骤1"></p>
<p>访问可选功能屏幕。</p>
<p><img src="https://picx.zhimg.com/v2-f94b788ac25b51a54a85bf0074d81941.webp" alt="安装SSH步骤2"></p>
<p>选择添加要素的选项。</p>
<p><img src="https://picx.zhimg.com/v2-15d13a44cccd6e77e83d833b5ca28b7e.webp" alt="安装SSH步骤3"></p>
<p>选择 SSH 服务器包，然后单击”安装”按钮。</p>
<p><img src="https://pic1.zhimg.com/v2-7087d6f462dc72d88429d3825ac9f160.webp" alt="安装SSH步骤4"></p>
<p>等待 Openssh 服务器安装完成。</p>
<p><img src="https://pica.zhimg.com/v2-cf48691afcf3e5e8340c490e46d59c94.webp" alt="安装SSH步骤5"></p>
<p>作为管理员，启动 Powershell 命令行的提升版本。</p>
<p><img src="https://picx.zhimg.com/v2-967a457fd357ab32cd55c5440953af9a.webp" alt="安装SSH步骤6"></p>
<p>将 SSH 服务配置为自动启动。</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">sc config sshd start=auto</span><br></pre></td></tr></tbody></table></figure>
<p>启动 SSH 服务。</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">net start sshd</span><br></pre></td></tr></tbody></table></figure>
<p>创建防火墙规则以允许在 SSH 端口上输入数据包。</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">netsh advfirewall firewall add rule name="SSH PORT 22" dir=in action=allow protocol=TCP localport=22</span><br></pre></td></tr></tbody></table></figure>
<p>祝贺！ 您已完成在 Windows 上安装 SSH 服务器。</p>
<h3 id="离线安装"><a href="#离线安装" class="headerlink" title="离线安装"></a><strong>离线安装</strong></h3><p>下载最新版本 <a href="https://github.com/PowerShell/Win32-OpenSSH/releases">适用于Windows二进制文件的OpenSSH</a> （包OpenSSH-Win64.zip或OpenSSH-Win32.zip）</p>
<p>建议直接下载msi 即可，自动安装以后就可以按照上述方法进行启动ssh服务了</p>
<h2 id="连接SSH服务器"><a href="#连接SSH服务器" class="headerlink" title="连接SSH服务器"></a><strong>连接SSH服务器</strong></h2><p>接下来，我们就可以使用常规的SSH的方式来连接我们配置好的windows了，我们只需要找到对应的IP地址，对其进行ssh连接即可。</p>
<p>这里一定要注意，<strong>密码是你微软账户的密码</strong>。</p>
<p>连接成功后即可远程办公了，比如我在家里，我们可以设置公司的电脑的ssh，后续我们就可以连接公司的电脑后对其进行操作，这样就比较的方便，当然，也可以对其进行远程，比如todesk等远程工具，这个可以见仁见智，我觉得都可以，有时候我只使用终端，我觉得都还好。</p>
<p>参考</p>
<p><a href="https://blog.laisky.com/p/tailscale/">Laisky’s Blog</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/452890363">小辣椒高效Office：Windows系统开启Ssh Server服务</a></p>
]]></content>
      <categories>
        <category>Note</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Note</tag>
      </tags>
  </entry>
  <entry>
    <title>VS Code Server 离线安装（解决超时，XHR Failed等问题）</title>
    <url>/2023/12/13/Note/Vscode/</url>
    <content><![CDATA[<p>在设置远程开发环境时,我们首先需要获取并安装 VS Code Server 程序。由于不同的服务器版本和环境,不一定会事先预装 VS Code Server,那么我们需要手动进行下载安装。有时候安裝等半天，有时候还报错，为了防止这样的情况，我还是记录一下解决方法，免得每次都需要找好多资料，但是找不到一个很有效的。</p>
<p>如果服务器是连接外网的，就根本不用有这个烦恼，因为下载很快，有时候主要是因为离线安装</p>
<p>这里我就介绍一种方法，我觉得最有效的方法，其他可能太麻烦而且不一定有效</p>
<p>具体操作如下:</p>
<p>首先,我们需要到 VS Code 的下载页面获取最新的版本号,记为 Commit ID。</p>
<p><img src="https://picx.zhimg.com/80/v2-2f652a6b456e4cef18c01ca60b83d8d1_720w.png?source=d16d100b" alt="获取Commit ID"></p>
<blockquote>
<p>除此之外，也可以直接在vscode左上角的帮助查看版本，如：</p>
</blockquote>
<p><img src="https://pic1.zhimg.com/80/v2-43da35aeeb56fa0c8c3e9d4eeb39f7d5_720w.png?source=d16d100b" alt="查看对应的版本"></p>
<p>访问此地址 <strong><a href="https://update.code.visualstudio.com/commit:${commit_id}/server-linux-x64/stable">https://update.code.visualstudio.com/commit:${commit_id}/server-linux-x64/stable</a></strong>，其中需要用实际的 Commit ID 替换 <strong>${commit_id}</strong> 变量字段。</p>
<p>比如我这里就是 <a href="https://vscode.download.prss.microsoft.com/dbazure/download/stable/af28b32d7e553898b2a91af498b1fb666fdebe0c/vscode-server-linux-x64.tar.gz">https://vscode.download.prss.microsoft.com/dbazure/download/stable/af28b32d7e553898b2a91af498b1fb666fdebe0c/vscode-server-linux-x64.tar.gz</a></p>
<p>这个地址将会返回对应的 VS Code Server 程序压缩包 vscode-server-linux-x64.tar.gz。我们可以通过 wget 或其他方式下载该文件，比如这里就是</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">wget https://update.code.visualstudio.com/commit:af28b32d7e553898b2a91af498b1fb666fdebe0c/server-linux-x64/stable</span><br></pre></td></tr></tbody></table></figure>
<p>下载完成后,需要解压并安装该程序。由于规范性考虑，我们通常将 VS Code 目录放在 ~/.vscode-server 下:</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> -p ~/.vscode-server/bin <span class="comment"># 创建目录</span></span><br><span class="line"><span class="built_in">rm</span> -rf ~/.vscode-server/bin/* <span class="comment">#清空原有文件</span></span><br><span class="line">tar -zxf vscode-server-linux-x64.tar.gz <span class="comment"># 解压程序包</span></span><br><span class="line"><span class="built_in">mv</span> vscode-server-linux-x64  ~/.vscode-server/bin/<span class="variable">${commit_id}</span> <span class="comment"># 移动并重命名程序</span></span><br></pre></td></tr></tbody></table></figure>
<p>最后一步就是把 <code>vscode-server-linux-x64.tar.gz</code> 解压，其中的内容全部放到<code>af28b32d7e553898b2a91af498b1fb666fdebe0c</code>文件夹下，并保持<code>af28b32d7e553898b2a91af498b1fb666fdebe0c</code>文件夹下只有<code>vscode-server-linux-x64.tar.gz</code>解压出来的文件。</p>
<p><img src="https://pica.zhimg.com/80/v2-ea940b665332a810245ea0e4096a2362_720w.png?source=d16d100b" alt="最后结果"></p>
<p>完成上述步骤后,VS Code Server 就成功下载和安装了。</p>
<p>另外,为了在多个服务器之间同步设置和扩展,我们也可以将一个服务器的 ~/.vscode-server/extensions 目录直接复制过去,以实现快速配置，因为只要是同一个VScode，版本是一模一样的</p>
<p>参考链接：<a href="https://blog.csdn.net/m0_43609475/article/details/120905157">离线在远程linux服务器配置vscode-python环境以及在容器中配置_没网的服务器vscode调试容器-</a></p>
]]></content>
      <categories>
        <category>Note</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Note</tag>
        <tag>Tool</tag>
      </tags>
  </entry>
  <entry>
    <title>Arxiv学术论文查询接口详解</title>
    <url>/2024/01/24/Note/arXiv/</url>
    <content><![CDATA[<h1 id="Arxiv学术论文查询接口详解-转载"><a href="#Arxiv学术论文查询接口详解-转载" class="headerlink" title="Arxiv学术论文查询接口详解 转载"></a>Arxiv学术论文查询接口详解 转载</h1><blockquote>
<p>这篇博客主要转载自：<a href="https://hiyoungai.com/posts/arxiv%E5%AD%A6%E6%9C%AF%E8%AE%BA%E6%96%87%E6%9F%A5%E8%AF%A2%E6%8E%A5%E5%8F%A3%E8%AF%A6%E8%A7%A3/">Arxiv学术论文查询接口详解</a>，我觉得写的很好，所以我也不重新整理这一部分的API接口了。我后续使用这一部分的API接口来进行爬取得到最新的文章，还是非常方便的，所以也同时推荐给大家，能最快follow新文章</p>
</blockquote>
<p>Arxiv API 允许以编程方式获取 <a href="https://arxiv.org/">https://arxiv.org</a> 上的论文。API 的基本结构为：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">http://export.arxiv.org/api/{method_name}?{parameters}</span><br></pre></td></tr></tbody></table></figure>
<h2 id="查询接口"><a href="#查询接口" class="headerlink" title="查询接口"></a>查询接口</h2><p>查询接口的的 method_name 为 query，下面是查询方法的参数，参数之间以 <em>&amp;</em> 分隔。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">parameters</th>
<th style="text-align:center">type</th>
<th style="text-align:center">defaults</th>
<th style="text-align:center">required</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">search_query</td>
<td style="text-align:center">string</td>
<td style="text-align:center">None</td>
<td style="text-align:center">No</td>
</tr>
<tr>
<td style="text-align:center">id_list</td>
<td style="text-align:center">comma-delimited string（以 ‘，’ 分隔的字符串）</td>
<td style="text-align:center">None</td>
<td style="text-align:center">No</td>
</tr>
<tr>
<td style="text-align:center">start</td>
<td style="text-align:center">int</td>
<td style="text-align:center">0</td>
<td style="text-align:center">No</td>
</tr>
<tr>
<td style="text-align:center">max_results</td>
<td style="text-align:center">int</td>
<td style="text-align:center">10</td>
<td style="text-align:center">No</td>
</tr>
</tbody>
</table>
</div>
<h3 id="参数说明"><a href="#参数说明" class="headerlink" title="参数说明"></a>参数说明</h3><ul>
<li>如果 API 只包含 search_query（不包含 id_list），那么返回与 search_query 内容匹配的结果。</li>
<li>如果 API 只包含 id_list（不包含 search_query），那么返回 id_list 中每一项的结果。</li>
<li>如果 API 中包含了 search_query 和 id_list，那么返回在 id_list 中，并且与 search_query 匹配的文章。</li>
</ul>
<h3 id="分页查询"><a href="#分页查询" class="headerlink" title="分页查询"></a>分页查询</h3><p>通常情况下，一个查询可能有成百上千个返回结果。有时候我们不希望一次性查询到这么多数量，那么可以使用 <em>start</em> 和 <em>max_results</em> 两个字段来进行分页查询。</p>
<ul>
<li>start 是查询的起始索引，以 0 为第一个。</li>
<li>max_results 是查询返回的集合数。</li>
</ul>
<p>下面来举例说明一下：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">http://export.arxiv.org/api/query?search_query=all:electron&amp;start=0&amp;max_results=10 (1)</span><br><span class="line">http://export.arxiv.org/api/query?search_query=all:electron&amp;start=10&amp;max_results=10 (2)</span><br><span class="line">http://export.arxiv.org/api/query?search_query=all:electron&amp;start=20&amp;max_results=10 (3)</span><br></pre></td></tr></tbody></table></figure>
<p>查询结果分别为：</p>
<ol>
<li>0 - 9</li>
<li>10 - 19</li>
<li>20 - 29</li>
</ol>
<p>需要注意的是，由于 API 的限制，在多次调用 API 的情况下，建议每次调用的时间间隔为 3 秒。每次调用返回的最大数量为 2000 个。arXiv的硬限制约为 50,000 条记录； 对于与 50,000 多个原稿匹配的查询，无法接收全部结果. 解决这个问题的最简单的解决方案是将中断查询成小块，例如使用的时间片，与一系列日期的<code>submittedDate</code>或<code>lastUpdatedDate</code> 。</p>
<h3 id="排序查询"><a href="#排序查询" class="headerlink" title="排序查询"></a>排序查询</h3><p>对查询的结果进行排序有两个选项：<em>sortBy</em> 和 <em>sortOrder</em>。</p>
<ul>
<li>sortBy 的值有：relevance，lastUpdatedDate 和 submittedDate。</li>
<li>sortOrder 的值有：ascending 和 descending。</li>
</ul>
<p>示例：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">http://export.arxiv.org/api/query?search_query=ti:%22electron%20thermal%20conductivity%22&amp;sortBy=lastUpdatedDate&amp;sortOrder=ascending</span><br></pre></td></tr></tbody></table></figure>
<h2 id="结果响应"><a href="#结果响应" class="headerlink" title="结果响应"></a>结果响应</h2><p>API 的 Response 内容中是以 <em>Atom 1.0</em> 为主体的，<em>Atom</em> 是 XML 的一种语法。下面分别来说明各个标签的含义。</p>
<h3 id="Feed-Metadata"><a href="#Feed-Metadata" class="headerlink" title="Feed Metadata"></a>Feed Metadata</h3><p>每个 Response 都会包含的内容：</p>
<ol>
<li>版本和命名空间</li>
</ol>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">&lt;?xml version=<span class="string">"1.0"</span> encoding=<span class="string">"UTF-8"</span>?&gt;</span><br><span class="line">&lt;feed xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;</span><br></pre></td></tr></tbody></table></figure>
<ol>
<li>Title：feed 的标题，通常为查询 URL 的字符串。</li>
</ol>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">&lt;title xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;</span><br><span class="line">    ArXiv Query:  search_query=all:electron&amp;amp;id_list=&amp;amp;start=0&amp;amp;max_results=1</span><br><span class="line">&lt;/title&gt;</span><br></pre></td></tr></tbody></table></figure>
<ol>
<li>Id：查询的唯一标识（注意不是查询的每个文章的 id），保证每个查询 id 是唯一的。</li>
</ol>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">&lt;<span class="built_in">id</span> xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;</span><br><span class="line">    http://arxiv.org/api/cHxbiOdZaP56ODnBPIenZhzg5f8</span><br><span class="line">&lt;/id&gt;</span><br></pre></td></tr></tbody></table></figure>
<ol>
<li>Link：查询 URL 的规范化。</li>
</ol>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">&lt;<span class="built_in">link</span> xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span> href=<span class="string">"http://arxiv.org/api/query?search_query=all:electron&amp;amp;id_list=&amp;amp;start=0&amp;amp;max_results=1"</span> rel=<span class="string">"self"</span> <span class="built_in">type</span>=<span class="string">"application/atom+xml"</span>/&gt;</span><br></pre></td></tr></tbody></table></figure>
<ol>
<li>Updated：提供了 feed 内容最后一次更新的时间。</li>
</ol>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">&lt;updated xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;2007-10-08T00:00:00-04:00&lt;/updated&gt;</span><br></pre></td></tr></tbody></table></figure>
<ol>
<li>Opensearch：扩展元素，包含了查询的返回数量以及分页信息等。</li>
</ol>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">&lt;opensearch:totalResults xmlns:opensearch=<span class="string">"http://a9.com/-/spec/opensearch/1.1/"</span>&gt;</span><br><span class="line">   1000</span><br><span class="line">&lt;/opensearch:totalResults&gt;</span><br><span class="line">&lt;opensearch:startIndex xmlns:opensearch=<span class="string">"http://a9.com/-/spec/opensearch/1.1/"</span>&gt;</span><br><span class="line">   0</span><br><span class="line">&lt;/opensearch:startIndex&gt;</span><br><span class="line">&lt;opensearch:itemsPerPage xmlns:opensearch=<span class="string">"http://a9.com/-/spec/opensearch/1.1/"</span>&gt;</span><br><span class="line">   1</span><br><span class="line">&lt;/opensearch:itemsPerPage&gt;</span><br></pre></td></tr></tbody></table></figure>
<h3 id="Entry-Metadata"><a href="#Entry-Metadata" class="headerlink" title="Entry Metadata"></a>Entry Metadata</h3><p>正常情况下，Response 返回结果中的 <em>feed</em> 标签会包含 0 个或者多个 <em>entry</em> 标签。每个 entry 表示一个查询的返回文章，下面分别说一下 entry 中的各个元素。</p>
<ol>
<li>Title：返回文章的标题</li>
</ol>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">&lt;title xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;</span><br><span class="line">    Multi-Electron Production at High Transverse Momenta <span class="keyword">in</span> ep Collisions at HERA</span><br><span class="line">&lt;/title&gt;</span><br></pre></td></tr></tbody></table></figure>
<ol>
<li>Id：文章的 URL ，可以认为是文章的绝对路径。最后一个字段是文章的唯一标识符。</li>
</ol>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">&lt;<span class="built_in">id</span> xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;</span><br><span class="line">    http://arxiv.org/abs/hep-ex/0307015</span><br><span class="line">&lt;/id&gt;</span><br></pre></td></tr></tbody></table></figure>
<ol>
<li>Published/Updated：文章的发布日期和更新日期。</li>
</ol>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">&lt;published xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;</span><br><span class="line">    2007-02-27T16:02:02-05:00</span><br><span class="line">&lt;/published&gt;</span><br><span class="line">&lt;updated xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;</span><br><span class="line">    2007-06-25T17:09:59-04:00</span><br><span class="line">&lt;/updated&gt;</span><br></pre></td></tr></tbody></table></figure>
<ol>
<li>Summary：文章的摘要。</li>
</ol>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">&lt;summary xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;</span><br><span class="line">    Multi-electron production is studied at high electron transverse momentum</span><br><span class="line">    <span class="keyword">in</span> positron- and electron-proton collisions using the H1 detector at HERA.</span><br><span class="line">    The data correspond to an integrated luminosity of 115 pb-1. Di-electron</span><br><span class="line">    and tri-electron event yields are measured. Cross sections are derived <span class="keyword">in</span></span><br><span class="line">    a restricted phase space region dominated by photon-photon collisions. In</span><br><span class="line">    general good agreement is found with the Standard Model predictions.</span><br><span class="line">    However, <span class="keyword">for</span> electron pair invariant masses above 100 GeV, three</span><br><span class="line">    di-electron events and three tri-electron events are observed, compared to</span><br><span class="line">    Standard Model expectations of 0.30 \pm 0.04 and 0.23 \pm 0.04,</span><br><span class="line">    respectively.</span><br><span class="line">&lt;/summary&gt;</span><br></pre></td></tr></tbody></table></figure>
<ol>
<li>Author：文章的作者，包含一个或者多个 name 标签，分别表示多个作者。</li>
</ol>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">&lt;author xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;</span><br><span class="line">      &lt;name xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;H1 Collaboration&lt;/name&gt;</span><br><span class="line">&lt;/author&gt;</span><br></pre></td></tr></tbody></table></figure>
<ol>
<li>Category：文章的分类。</li>
</ol>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">&lt;category xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span> term=<span class="string">"cs.LG"</span> scheme=<span class="string">"http://arxiv.org/schemas/atom"</span>/&gt;</span><br><span class="line">&lt;category xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span> term=<span class="string">"cs.AI"</span> scheme=<span class="string">"http://arxiv.org/schemas/atom"</span>/&gt;</span><br><span class="line">&lt;category xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span> term=<span class="string">"I.2.6"</span> scheme=<span class="string">"http://arxiv.org/schemas/atom"</span>/&gt;</span><br></pre></td></tr></tbody></table></figure>
<ol>
<li>Link，对于每个文章，最多有三个 link 元素，通过 ref 和 title 来区别，下面的表格表示 ref 和 title 的内容：</li>
</ol>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">rel</th>
<th style="text-align:center">title</th>
<th style="text-align:center">refers to</th>
<th style="text-align:center">always present</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">alternate</td>
<td style="text-align:center">-</td>
<td style="text-align:center">abstract page</td>
<td style="text-align:center">yes</td>
</tr>
<tr>
<td style="text-align:center">related</td>
<td style="text-align:center">pdf</td>
<td style="text-align:center">pdf</td>
<td style="text-align:center">yes</td>
</tr>
<tr>
<td style="text-align:center">related</td>
<td style="text-align:center">doi</td>
<td style="text-align:center">resolved doi</td>
<td style="text-align:center">no</td>
</tr>
</tbody>
</table>
</div>
<p>例子：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">&lt;<span class="built_in">link</span> xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span> href=<span class="string">"http://arxiv.org/abs/hep-ex/0307015v1"</span> rel=<span class="string">"alternate"</span> <span class="built_in">type</span>=<span class="string">"text/html"</span>/&gt;</span><br><span class="line">&lt;<span class="built_in">link</span> xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span> title=<span class="string">"pdf"</span> href=<span class="string">"http://arxiv.org/pdf/hep-ex/0307015v1"</span> rel=<span class="string">"related"</span> <span class="built_in">type</span>=<span class="string">"application/pdf"</span>/&gt;</span><br><span class="line">&lt;<span class="built_in">link</span> xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span> title=<span class="string">"doi"</span> href=<span class="string">"http://dx.doi.org/10.1529/biophysj.104.047340"</span> rel=<span class="string">"related"</span>/&gt;</span><br></pre></td></tr></tbody></table></figure>
<ol>
<li>arxiv:primary_category：主要分类的扩展元素。</li>
</ol>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">&lt;arxiv:primary_category xmlns:arxiv=<span class="string">"http://arxiv.org/schemas/atom"</span> term=<span class="string">"cs.LG"</span> scheme=<span class="string">"http://arxiv.org/schemas/atom"</span>/&gt;</span><br></pre></td></tr></tbody></table></figure>
<ol>
<li>arxiv:comment：评论扩展元素。</li>
</ol>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">&lt;arxiv:comment xmlns:arxiv=<span class="string">"http://arxiv.org/schemas/atom"</span>&gt;</span><br><span class="line">   23 pages, 8 figures and 4 tables</span><br><span class="line">&lt;/arxiv:comment&gt;</span><br></pre></td></tr></tbody></table></figure>
<ol>
<li>arxiv:affiliation：作者从属关系。</li>
</ol>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">&lt;author&gt;</span><br><span class="line">   &lt;name&gt;G. G. Kacprzak&lt;/name&gt;</span><br><span class="line">   &lt;arxiv:affiliation xmlns:arxiv=<span class="string">"http://arxiv.org/schemas/atom"</span>&gt;NMSU&lt;/arxiv:affiliation&gt;</span><br><span class="line">&lt;/author&gt;</span><br></pre></td></tr></tbody></table></figure>
<ol>
<li>arxiv:journal_ref：期刊说明</li>
</ol>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">&lt;arxiv:journal_ref xmlns:arxiv=<span class="string">"http://arxiv.org/schemas/atom"</span>&gt;</span><br><span class="line">   Eur.Phys.J. C31 (2003) 17-29</span><br><span class="line">&lt;/arxiv:journal_ref&gt;</span><br></pre></td></tr></tbody></table></figure>
<ol>
<li>arxiv:doi：doi 说明</li>
</ol>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">&lt;arxiv:doi xmlns:arxiv=<span class="string">"http://arxiv.org/schemas/atom"</span>&gt;</span><br><span class="line">   10.1529/biophysj.104.047340</span><br><span class="line">&lt;/arxiv:doi&gt;</span><br></pre></td></tr></tbody></table></figure>
<h3 id="Errors"><a href="#Errors" class="headerlink" title="Errors"></a>Errors</h3><p>返回错误，如果请求的响应出现错误，会返回一个详细的错误信息。例如下面是一个错误 id 的信息：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">&lt;?xml version=<span class="string">"1.0"</span> encoding=<span class="string">"utf-8"</span>?&gt;</span><br><span class="line">&lt;feed xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span> xmlns:opensearch=<span class="string">"http://a9.com/-/spec/opensearch/1.1/"</span>&gt;</span><br><span class="line">  &lt;<span class="built_in">link</span> xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span> href=<span class="string">"http://arxiv.org/api/query?search_query=&amp;amp;id_list=1234.12345"</span> rel=<span class="string">"self"</span> <span class="built_in">type</span>=<span class="string">"application/atom+xml"</span>/&gt;</span><br><span class="line">  &lt;title xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;ArXiv Query: search_query=&amp;amp;id_list=1234.12345&lt;/title&gt;</span><br><span class="line">  &lt;<span class="built_in">id</span> xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;http://arxiv.org/api/kvuntZ8c9a4Eq5CF7KY03nMug+Q&lt;/id&gt;</span><br><span class="line">  &lt;updated xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;2007-10-12T00:00:00-04:00&lt;/updated&gt;</span><br><span class="line">  &lt;opensearch:totalResults xmlns:opensearch=<span class="string">"http://a9.com/-/spec/opensearch/1.1/"</span>&gt;1&lt;/opensearch:totalResults&gt;</span><br><span class="line">  &lt;opensearch:startIndex xmlns:opensearch=<span class="string">"http://a9.com/-/spec/opensearch/1.1/"</span>&gt;0&lt;/opensearch:startIndex&gt;</span><br><span class="line"></span><br><span class="line">  &lt;opensearch:itemsPerPage xmlns:opensearch=<span class="string">"http://a9.com/-/spec/opensearch/1.1/"</span>&gt;1&lt;/opensearch:itemsPerPage&gt;</span><br><span class="line">  &lt;entry xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;</span><br><span class="line">    &lt;<span class="built_in">id</span> xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;http://arxiv.org/api/errors<span class="comment">#incorrect_id_format_for_1234.12345&lt;/id&gt;</span></span><br><span class="line">    &lt;title xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;Error&lt;/title&gt;</span><br><span class="line">    &lt;summary xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;incorrect <span class="built_in">id</span> format <span class="keyword">for</span> 1234.12345&lt;/summary&gt;</span><br><span class="line">    &lt;updated xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;2007-10-12T00:00:00-04:00&lt;/updated&gt;</span><br><span class="line"></span><br><span class="line">    &lt;<span class="built_in">link</span> xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span> href=<span class="string">"http://arxiv.org/api/errors#incorrect_id_format_for_1234.12345"</span> rel=<span class="string">"alternate"</span> <span class="built_in">type</span>=<span class="string">"text/html"</span>/&gt;</span><br><span class="line">    &lt;author xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;</span><br><span class="line">      &lt;name xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;arXiv api core&lt;/name&gt;</span><br><span class="line">    &lt;/author&gt;</span><br><span class="line">  &lt;/entry&gt;</span><br><span class="line">&lt;/feed&gt;</span><br></pre></td></tr></tbody></table></figure>
<p>下面提供了一些常见的错误：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"><strong>Sample query 示例查询</strong></th>
<th style="text-align:center"><strong>Error Explanation 错误解释</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><a href="http://export.arxiv.org/api/query?start=not_an_int">http://export.arxiv.org/api/query?start=not_an_int</a></td>
<td style="text-align:center"><code>start</code> 一定是个整数</td>
</tr>
<tr>
<td style="text-align:center"><a href="http://export.arxiv.org/api/query?start=-1">http://export.arxiv.org/api/query?start=-1</a></td>
<td style="text-align:center"><code>start</code> 必须 &gt;= 0</td>
</tr>
<tr>
<td style="text-align:center"><a href="http://export.arxiv.org/api/query?max_results=not_an_int">http://export.arxiv.org/api/query?max_results=not_an_int</a></td>
<td style="text-align:center"><code>max_results</code> 一定是个整数</td>
</tr>
<tr>
<td style="text-align:center"><a href="http://export.arxiv.org/api/query?max_results=-1">http://export.arxiv.org/api/query?max_results=-1</a></td>
<td style="text-align:center"><code>max_results</code> 必须 &gt;= 0</td>
</tr>
<tr>
<td style="text-align:center"><a href="http://export.arxiv.org/api/query?id_list=1234.1234">http://export.arxiv.org/api/query?id_list=1234.1234</a></td>
<td style="text-align:center">malformed id</td>
</tr>
<tr>
<td style="text-align:center"><a href="http://export.arxiv.org/api/query?id_list=cond—mat/0709123">http://export.arxiv.org/api/query?id_list=cond—mat/0709123</a></td>
<td style="text-align:center">malformed id</td>
</tr>
</tbody>
</table>
</div>
<h2 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h2><h3 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h3><p>python2.7 上的简单请求：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line">url = <span class="string">'http://export.arxiv.org/api/query?search_query=all:electron&amp;start=0&amp;max_results=1'</span></span><br><span class="line">data = urllib.urlopen(url).read()</span><br><span class="line"><span class="built_in">print</span> data</span><br></pre></td></tr></tbody></table></figure>
<p>python3 上的请求：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request <span class="keyword">as</span> libreq</span><br><span class="line"><span class="keyword">with</span> libreq.urlopen(<span class="string">'http://export.arxiv.org/api/query?search_query=all:electron&amp;start=0&amp;max_results=1'</span>) <span class="keyword">as</span> url:</span><br><span class="line">    r = url.read()</span><br><span class="line">    <span class="built_in">print</span>(r)</span><br></pre></td></tr></tbody></table></figure>
<h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><h3 id="查询的详细结构"><a href="#查询的详细结构" class="headerlink" title="查询的详细结构"></a>查询的详细结构</h3><p>在 arXiv 搜索引擎中，每篇文章都被划分为许多可以单独搜索的字段。 例如，可以搜索一篇文章的标题，以及作者列表、摘要、评论和期刊参考文献。 要搜索其中一个字段，只需在搜索词前加上字段前缀和冒号即可。 例如：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">http://export.arxiv.org/api/query?search_query=au:del_maestro</span><br></pre></td></tr></tbody></table></figure>
<p>下面的表格显示所有字段的前缀：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"><strong>prefix</strong></th>
<th style="text-align:center"><strong>explanation</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">ti</td>
<td style="text-align:center">Title</td>
</tr>
<tr>
<td style="text-align:center">au</td>
<td style="text-align:center">Author</td>
</tr>
<tr>
<td style="text-align:center">abs</td>
<td style="text-align:center">Abstract</td>
</tr>
<tr>
<td style="text-align:center">co</td>
<td style="text-align:center">Comment</td>
</tr>
<tr>
<td style="text-align:center">jr</td>
<td style="text-align:center">Journal Reference</td>
</tr>
<tr>
<td style="text-align:center">cat</td>
<td style="text-align:center">Subject Category</td>
</tr>
<tr>
<td style="text-align:center">rn</td>
<td style="text-align:center">Report Number</td>
</tr>
<tr>
<td style="text-align:center">id</td>
<td style="text-align:center">Id (use <code>id_list</code> instead)</td>
</tr>
<tr>
<td style="text-align:center">all</td>
<td style="text-align:center">All of the above</td>
</tr>
</tbody>
</table>
</div>
<p>并且查询也支持布尔运算，假设我们希望找到作者 Adrian DelMaestro 的所有文章，其标题中也包含单词 checkerboard。 我们可以使用 AND 操作符构造下面的查询：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">http://export.arxiv.org/api/query?search_query=au:del_maestro+AND+ti:checkerboard</span><br></pre></td></tr></tbody></table></figure>
<p>下面是三种可能的布尔值：</p>
<ul>
<li>AND</li>
<li>OR</li>
<li>ANDNOT</li>
</ul>
<p>下面是特殊符号的含义以及转义字符：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">symbol</th>
<th style="text-align:left">encoding</th>
<th style="text-align:left">explanation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">( )</td>
<td style="text-align:left">%28 %29</td>
<td style="text-align:left">用于为布尔运算符优先级对布尔表达式进行分组</td>
</tr>
<tr>
<td style="text-align:left">“ “</td>
<td style="text-align:left">%22 %22</td>
<td style="text-align:left">用于将多个单词组合成短语以搜索特定字段</td>
</tr>
<tr>
<td style="text-align:left">空格</td>
<td style="text-align:left">+</td>
<td style="text-align:left">用于扩展<code>search_query</code> 包含多个字段</td>
</tr>
</tbody>
</table>
</div>
<h3 id="返回的详细结构"><a href="#返回的详细结构" class="headerlink" title="返回的详细结构"></a>返回的详细结构</h3><p>下表列出了返回的 Atom 结果的每个元素:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"><strong>element</strong></th>
<th style="text-align:center"><strong>explanation</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><strong>feed elements</strong></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">title</td>
<td style="text-align:center">包含规范化查询字符串的标题</td>
</tr>
<tr>
<td style="text-align:center">id</td>
<td style="text-align:center">分配给此查询的唯一 id</td>
</tr>
<tr>
<td style="text-align:center">updated</td>
<td style="text-align:center">最后一次更新此查询的搜索结果。 设置为当天的午夜</td>
</tr>
<tr>
<td style="text-align:center">link</td>
<td style="text-align:center">通过 GET 请求检索此提要的 url</td>
</tr>
<tr>
<td style="text-align:center">opensearch:totalResults</td>
<td style="text-align:center">此查询的搜索结果总数</td>
</tr>
<tr>
<td style="text-align:center">opensearch:startIndex</td>
<td style="text-align:center">总结果列表中第一个返回结果的基于0的索引</td>
</tr>
<tr>
<td style="text-align:center">opensearch:itemsPerPage</td>
<td style="text-align:center">每页返回的结果数</td>
</tr>
<tr>
<td style="text-align:center"><strong>entry elements</strong></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">title</td>
<td style="text-align:center">文章的标题</td>
</tr>
<tr>
<td style="text-align:center">id</td>
<td style="text-align:center">文章的网址<code>http://arxiv.org/abs/id</code></td>
</tr>
<tr>
<td style="text-align:center">published</td>
<td style="text-align:center">文章的发布日期</td>
</tr>
<tr>
<td style="text-align:center">updated</td>
<td style="text-align:center">文章的更新日期，如果为 v1 版本，那么与发布日期相同</td>
</tr>
<tr>
<td style="text-align:center">summary</td>
<td style="text-align:center">文章摘要</td>
</tr>
<tr>
<td style="text-align:center">author</td>
<td style="text-align:center">每个作者有一个子元素 name，包含了作者的名字</td>
</tr>
<tr>
<td style="text-align:center">link</td>
<td style="text-align:center">可以给定与这篇文章关联的 3 个网址</td>
</tr>
<tr>
<td style="text-align:center">category</td>
<td style="text-align:center">文章分类</td>
</tr>
<tr>
<td style="text-align:center">arxiv:primary_category</td>
<td style="text-align:center">主要的 arXiv 分类</td>
</tr>
<tr>
<td style="text-align:center">arxiv:comment</td>
<td style="text-align:center">作者对此发表的评论</td>
</tr>
<tr>
<td style="text-align:center">arxiv:affiliation</td>
<td style="text-align:center">作者的从属关系</td>
</tr>
<tr>
<td style="text-align:center">arxiv:journal_ref</td>
<td style="text-align:center">参考文献</td>
</tr>
<tr>
<td style="text-align:center">arxiv:doi</td>
<td style="text-align:center">已解析的 DOI 的 url，指向外部资源</td>
</tr>
</tbody>
</table>
</div>
<h3 id="学科的分类"><a href="#学科的分类" class="headerlink" title="学科的分类"></a>学科的分类</h3><p>下面是学科分类字段以及对应的翻译（软件脚本自动翻译，如不对请勿喷）：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">字段</th>
<th style="text-align:center">学科（英文）</th>
<th style="text-align:center">学科（中文）</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">astro-ph</td>
<td style="text-align:center">Astrophysics</td>
<td style="text-align:center">天体物理</td>
</tr>
<tr>
<td style="text-align:center">astro-ph.CO</td>
<td style="text-align:center">Cosmology and Nongalactic Astrophysics</td>
<td style="text-align:center">宇宙学与非规则天体物理学</td>
</tr>
<tr>
<td style="text-align:center">astro-ph.EP</td>
<td style="text-align:center">Earth and Planetary Astrophysics</td>
<td style="text-align:center">地球与行星天体物理学</td>
</tr>
<tr>
<td style="text-align:center">astro-ph.GA</td>
<td style="text-align:center">Astrophysics of Galaxies</td>
<td style="text-align:center">星系的天体物理学</td>
</tr>
<tr>
<td style="text-align:center">astro-ph.HE</td>
<td style="text-align:center">High Energy Astrophysical Phenomena</td>
<td style="text-align:center">高能天体物理现象</td>
</tr>
<tr>
<td style="text-align:center">astro-ph.IM</td>
<td style="text-align:center">Instrumentation and Methods for Astrophysics</td>
<td style="text-align:center">天体物理学的仪器和方法</td>
</tr>
<tr>
<td style="text-align:center">astro-ph.SR</td>
<td style="text-align:center">Solar and Stellar Astrophysics</td>
<td style="text-align:center">太阳与恒星天体物理学</td>
</tr>
<tr>
<td style="text-align:center">cond-mat.dis-nn</td>
<td style="text-align:center">Disordered Systems and Neural Networks</td>
<td style="text-align:center">无序系统与神经网络</td>
</tr>
<tr>
<td style="text-align:center">cond-mat.mes-hall</td>
<td style="text-align:center">Mesoscale and Nanoscale Physics</td>
<td style="text-align:center">中尺度和纳米尺度物理学</td>
</tr>
<tr>
<td style="text-align:center">cond-mat.mtrl-sci</td>
<td style="text-align:center">Materials Science</td>
<td style="text-align:center">材料科学</td>
</tr>
<tr>
<td style="text-align:center">cond-mat.other</td>
<td style="text-align:center">Other Condensed Matter</td>
<td style="text-align:center">其他凝聚态</td>
</tr>
<tr>
<td style="text-align:center">cond-mat.quant-gas</td>
<td style="text-align:center">Quantum Gases</td>
<td style="text-align:center">量子气体</td>
</tr>
<tr>
<td style="text-align:center">cond-mat.soft</td>
<td style="text-align:center">Soft Condensed Matter</td>
<td style="text-align:center">软凝聚物</td>
</tr>
<tr>
<td style="text-align:center">cond-mat.stat-mech</td>
<td style="text-align:center">Statistical Mechanics</td>
<td style="text-align:center">统计力学</td>
</tr>
<tr>
<td style="text-align:center">cond-mat.str-el</td>
<td style="text-align:center">Strongly Correlated Electrons</td>
<td style="text-align:center">强关联电子</td>
</tr>
<tr>
<td style="text-align:center">cond-mat.supr-con</td>
<td style="text-align:center">Superconductivity</td>
<td style="text-align:center">超导现象</td>
</tr>
<tr>
<td style="text-align:center">cs.AI</td>
<td style="text-align:center">Artificial Intelligence</td>
<td style="text-align:center">人工智能</td>
</tr>
<tr>
<td style="text-align:center">cs.AR</td>
<td style="text-align:center">Hardware Architecture</td>
<td style="text-align:center">硬件架构</td>
</tr>
<tr>
<td style="text-align:center">cs.CC</td>
<td style="text-align:center">Computational Complexity</td>
<td style="text-align:center">计算复杂性</td>
</tr>
<tr>
<td style="text-align:center">cs.CE</td>
<td style="text-align:center">Computational Engineering, Finance, and Science</td>
<td style="text-align:center">计算工程，金融和科学</td>
</tr>
<tr>
<td style="text-align:center">cs.CG</td>
<td style="text-align:center">Computational Geometry</td>
<td style="text-align:center">计算几何</td>
</tr>
<tr>
<td style="text-align:center">cs.CL</td>
<td style="text-align:center">Computation and Language</td>
<td style="text-align:center">计算与语言</td>
</tr>
<tr>
<td style="text-align:center">cs.CR</td>
<td style="text-align:center">Cryptography and Security</td>
<td style="text-align:center">密码学与保安</td>
</tr>
<tr>
<td style="text-align:center">cs.CV</td>
<td style="text-align:center">Computer Vision and Pattern Recognition</td>
<td style="text-align:center">计算机视觉与模式识别</td>
</tr>
<tr>
<td style="text-align:center">CY</td>
<td style="text-align:center">Computers and Society</td>
<td style="text-align:center">电脑与社会</td>
</tr>
<tr>
<td style="text-align:center">cs.DB</td>
<td style="text-align:center">Databases</td>
<td style="text-align:center">数据库</td>
</tr>
<tr>
<td style="text-align:center">cs.DC</td>
<td style="text-align:center">Distributed, Parallel, and Cluster Computing</td>
<td style="text-align:center">分布式、并行和集群计算</td>
</tr>
<tr>
<td style="text-align:center">cs.DL</td>
<td style="text-align:center">Digital Libraries</td>
<td style="text-align:center">数字仓库</td>
</tr>
<tr>
<td style="text-align:center">cs.DM</td>
<td style="text-align:center">Discrete Mathematics</td>
<td style="text-align:center">离散数学</td>
</tr>
<tr>
<td style="text-align:center">cs.DS</td>
<td style="text-align:center">Data Structures and Algorithms</td>
<td style="text-align:center">数据结构和算法</td>
</tr>
<tr>
<td style="text-align:center">cs.ET</td>
<td style="text-align:center">Emerging Technologies</td>
<td style="text-align:center">新兴科技</td>
</tr>
<tr>
<td style="text-align:center">cs.FL</td>
<td style="text-align:center">Formal Languages and Automata Theory</td>
<td style="text-align:center">形式语言与自动机理论</td>
</tr>
<tr>
<td style="text-align:center">cs.GL</td>
<td style="text-align:center">General Literature</td>
<td style="text-align:center">一般文学</td>
</tr>
<tr>
<td style="text-align:center">cs.GR</td>
<td style="text-align:center">Graphics</td>
<td style="text-align:center">图形</td>
</tr>
<tr>
<td style="text-align:center">cs.GT</td>
<td style="text-align:center">Computer Science and Game Theory</td>
<td style="text-align:center">计算机科学与博弈论</td>
</tr>
<tr>
<td style="text-align:center">cs.HC</td>
<td style="text-align:center">Human-Computer Interaction</td>
<td style="text-align:center">人机交互</td>
</tr>
<tr>
<td style="text-align:center">cs.IR</td>
<td style="text-align:center">Information Retrieval</td>
<td style="text-align:center">信息检索</td>
</tr>
<tr>
<td style="text-align:center">cs.IT</td>
<td style="text-align:center">Information Theory</td>
<td style="text-align:center">信息理论</td>
</tr>
<tr>
<td style="text-align:center">cs.LG</td>
<td style="text-align:center">Learning</td>
<td style="text-align:center">学习</td>
</tr>
<tr>
<td style="text-align:center">cs.LO</td>
<td style="text-align:center">Logic in Computer Science</td>
<td style="text-align:center">计算机科学中的逻辑</td>
</tr>
<tr>
<td style="text-align:center">cs.MA</td>
<td style="text-align:center">Multiagent Systems</td>
<td style="text-align:center">多代理系统</td>
</tr>
<tr>
<td style="text-align:center">cs.MM</td>
<td style="text-align:center">Multimedia</td>
<td style="text-align:center">多媒体</td>
</tr>
<tr>
<td style="text-align:center">cs.MS</td>
<td style="text-align:center">Mathematical Software</td>
<td style="text-align:center">数学软件</td>
</tr>
<tr>
<td style="text-align:center">cs.NA</td>
<td style="text-align:center">Numerical Analysis</td>
<td style="text-align:center">数值分析</td>
</tr>
<tr>
<td style="text-align:center">cs.NE</td>
<td style="text-align:center">Neural and Evolutionary Computing</td>
<td style="text-align:center">神经和进化计算</td>
</tr>
<tr>
<td style="text-align:center">cs.NI</td>
<td style="text-align:center">Networking and Internet Architecture</td>
<td style="text-align:center">网络与互联网架构</td>
</tr>
<tr>
<td style="text-align:center">cs.OH</td>
<td style="text-align:center">Other Computer Science</td>
<td style="text-align:center">其他计算机科学</td>
</tr>
<tr>
<td style="text-align:center">cs.OS</td>
<td style="text-align:center">Operating Systems</td>
<td style="text-align:center">操作系统</td>
</tr>
<tr>
<td style="text-align:center">cs.PF</td>
<td style="text-align:center">Performance</td>
<td style="text-align:center">性能</td>
</tr>
<tr>
<td style="text-align:center">cs.PL</td>
<td style="text-align:center">Programming Languages</td>
<td style="text-align:center">编程语言</td>
</tr>
<tr>
<td style="text-align:center">cs.RO</td>
<td style="text-align:center">Robotics</td>
<td style="text-align:center">机器人技术</td>
</tr>
<tr>
<td style="text-align:center">cs.SC</td>
<td style="text-align:center">Symbolic Computation</td>
<td style="text-align:center">符号计算</td>
</tr>
<tr>
<td style="text-align:center">cs.SD</td>
<td style="text-align:center">Sound</td>
<td style="text-align:center">声音</td>
</tr>
<tr>
<td style="text-align:center">cs.SE</td>
<td style="text-align:center">Software Engineering</td>
<td style="text-align:center">软件工程</td>
</tr>
<tr>
<td style="text-align:center">cs.SI</td>
<td style="text-align:center">Social and Information Networks</td>
<td style="text-align:center">社会和信息网络</td>
</tr>
<tr>
<td style="text-align:center">cs.SY</td>
<td style="text-align:center">Systems and Control</td>
<td style="text-align:center">系统及控制</td>
</tr>
<tr>
<td style="text-align:center">econ.EM</td>
<td style="text-align:center">Econometrics</td>
<td style="text-align:center">计量经济学</td>
</tr>
<tr>
<td style="text-align:center">eess.AS</td>
<td style="text-align:center">Audio and Speech Processing</td>
<td style="text-align:center">音频及语音处理</td>
</tr>
<tr>
<td style="text-align:center">eess.IV</td>
<td style="text-align:center">Image and Video Processing</td>
<td style="text-align:center">图像和视频处理</td>
</tr>
<tr>
<td style="text-align:center">eess.SP</td>
<td style="text-align:center">Signal Processing</td>
<td style="text-align:center">信号处理</td>
</tr>
<tr>
<td style="text-align:center">gr-qc</td>
<td style="text-align:center">General Relativity and Quantum Cosmology</td>
<td style="text-align:center">广义相对论和量子宇宙学</td>
</tr>
<tr>
<td style="text-align:center">hep-ex</td>
<td style="text-align:center">High Energy Physics - Experiment</td>
<td style="text-align:center">高能物理实验</td>
</tr>
<tr>
<td style="text-align:center">hep-lat</td>
<td style="text-align:center">High Energy Physics - Lattice</td>
<td style="text-align:center">高能物理-晶格</td>
</tr>
<tr>
<td style="text-align:center">hep-ph</td>
<td style="text-align:center">High Energy Physics - Phenomenology</td>
<td style="text-align:center">高能物理-现象学</td>
</tr>
<tr>
<td style="text-align:center">hep-th</td>
<td style="text-align:center">High Energy Physics - Theory</td>
<td style="text-align:center">高能物理理论</td>
</tr>
<tr>
<td style="text-align:center">math.AC</td>
<td style="text-align:center">Commutative Algebra</td>
<td style="text-align:center">交换代数</td>
</tr>
<tr>
<td style="text-align:center">math.AG</td>
<td style="text-align:center">Algebraic Geometry</td>
<td style="text-align:center">代数几何</td>
</tr>
<tr>
<td style="text-align:center">math.AP</td>
<td style="text-align:center">Analysis of PDEs</td>
<td style="text-align:center">偏微分方程分析</td>
</tr>
<tr>
<td style="text-align:center">math.AT</td>
<td style="text-align:center">Algebraic Topology</td>
<td style="text-align:center">代数拓扑</td>
</tr>
<tr>
<td style="text-align:center">math.CA</td>
<td style="text-align:center">Classical Analysis and ODEs</td>
<td style="text-align:center">传统分析和微分方程</td>
</tr>
<tr>
<td style="text-align:center">math.CO</td>
<td style="text-align:center">Combinatorics</td>
<td style="text-align:center">组合数学</td>
</tr>
<tr>
<td style="text-align:center">math.CT</td>
<td style="text-align:center">Category Theory</td>
<td style="text-align:center">范畴理论</td>
</tr>
<tr>
<td style="text-align:center">math.CV</td>
<td style="text-align:center">Complex Variables</td>
<td style="text-align:center">复杂变量</td>
</tr>
<tr>
<td style="text-align:center">math.DG</td>
<td style="text-align:center">Differential Geometry</td>
<td style="text-align:center">微分几何</td>
</tr>
<tr>
<td style="text-align:center">math.DS</td>
<td style="text-align:center">Dynamical Systems</td>
<td style="text-align:center">动力系统</td>
</tr>
<tr>
<td style="text-align:center">math.FA</td>
<td style="text-align:center">Functional Analysis</td>
<td style="text-align:center">功能分析</td>
</tr>
<tr>
<td style="text-align:center">math.GM</td>
<td style="text-align:center">General Mathematics</td>
<td style="text-align:center">普通数学</td>
</tr>
<tr>
<td style="text-align:center">math.GN</td>
<td style="text-align:center">General Topology</td>
<td style="text-align:center">点集拓扑学</td>
</tr>
<tr>
<td style="text-align:center">math.GR</td>
<td style="text-align:center">Group Theory</td>
<td style="text-align:center">群论</td>
</tr>
<tr>
<td style="text-align:center">math.GT</td>
<td style="text-align:center">Geometric Topology</td>
<td style="text-align:center">几何拓扑学</td>
</tr>
<tr>
<td style="text-align:center">math.HO</td>
<td style="text-align:center">History and Overview</td>
<td style="text-align:center">历史和概述</td>
</tr>
<tr>
<td style="text-align:center">math.IT</td>
<td style="text-align:center">Information Theory</td>
<td style="text-align:center">信息理论</td>
</tr>
<tr>
<td style="text-align:center">math.KT</td>
<td style="text-align:center">K-Theory and Homology</td>
<td style="text-align:center">K 理论与同调</td>
</tr>
<tr>
<td style="text-align:center">math.LO</td>
<td style="text-align:center">Logic</td>
<td style="text-align:center">逻辑</td>
</tr>
<tr>
<td style="text-align:center">math.MG</td>
<td style="text-align:center">Metric Geometry</td>
<td style="text-align:center">度量几何学</td>
</tr>
<tr>
<td style="text-align:center">math.MP</td>
<td style="text-align:center">Mathematical Physics</td>
<td style="text-align:center">数学物理</td>
</tr>
<tr>
<td style="text-align:center">math.NA</td>
<td style="text-align:center">Numerical Analysis</td>
<td style="text-align:center">数值分析</td>
</tr>
<tr>
<td style="text-align:center">math.NT</td>
<td style="text-align:center">Number Theory</td>
<td style="text-align:center">数论</td>
</tr>
<tr>
<td style="text-align:center">math.OA</td>
<td style="text-align:center">Operator Algebras</td>
<td style="text-align:center">算子代数</td>
</tr>
<tr>
<td style="text-align:center">math.OC</td>
<td style="text-align:center">Optimization and Control</td>
<td style="text-align:center">优化和控制</td>
</tr>
<tr>
<td style="text-align:center">math.PR</td>
<td style="text-align:center">Probability</td>
<td style="text-align:center">概率</td>
</tr>
<tr>
<td style="text-align:center">math.QA</td>
<td style="text-align:center">Quantum Algebra</td>
<td style="text-align:center">量子代数</td>
</tr>
<tr>
<td style="text-align:center">math.RA</td>
<td style="text-align:center">Rings and Algebras</td>
<td style="text-align:center">环与代数</td>
</tr>
<tr>
<td style="text-align:center">math.RT</td>
<td style="text-align:center">Representation Theory</td>
<td style="text-align:center">表示论</td>
</tr>
<tr>
<td style="text-align:center">math.SG</td>
<td style="text-align:center">Symplectic Geometry</td>
<td style="text-align:center">辛几何</td>
</tr>
<tr>
<td style="text-align:center">math.SP</td>
<td style="text-align:center">Spectral Theory</td>
<td style="text-align:center">光谱理论</td>
</tr>
<tr>
<td style="text-align:center">math.ST</td>
<td style="text-align:center">Statistics Theory</td>
<td style="text-align:center">统计学理论</td>
</tr>
<tr>
<td style="text-align:center">math-ph</td>
<td style="text-align:center">Mathematical Physics</td>
<td style="text-align:center">数学物理</td>
</tr>
<tr>
<td style="text-align:center">nlin.AO</td>
<td style="text-align:center">Adaptation and Self-Organizing Systems</td>
<td style="text-align:center">适应与自组织系统</td>
</tr>
<tr>
<td style="text-align:center">nlin.CD</td>
<td style="text-align:center">Chaotic Dynamics</td>
<td style="text-align:center">混沌动力学</td>
</tr>
<tr>
<td style="text-align:center">nlin.CG</td>
<td style="text-align:center">Cellular Automata and Lattice Gases</td>
<td style="text-align:center">元胞自动机与格子气体</td>
</tr>
<tr>
<td style="text-align:center">nlin.PS</td>
<td style="text-align:center">Pattern Formation and Solitons</td>
<td style="text-align:center">模式形成与孤子</td>
</tr>
<tr>
<td style="text-align:center">nlin.SI</td>
<td style="text-align:center">Exactly Solvable and Integrable Systems</td>
<td style="text-align:center">严格可解可积系统</td>
</tr>
<tr>
<td style="text-align:center">nucl-ex</td>
<td style="text-align:center">Nuclear Experiment</td>
<td style="text-align:center">核试验</td>
</tr>
<tr>
<td style="text-align:center">nucl-th</td>
<td style="text-align:center">Nuclear Theory</td>
<td style="text-align:center">核理论</td>
</tr>
<tr>
<td style="text-align:center">physics.acc-ph</td>
<td style="text-align:center">Accelerator Physics</td>
<td style="text-align:center">加速器物理学</td>
</tr>
<tr>
<td style="text-align:center">physics.ao-ph</td>
<td style="text-align:center">Atmospheric and Oceanic Physics</td>
<td style="text-align:center">大气和海洋物理学</td>
</tr>
<tr>
<td style="text-align:center">physics.app-ph</td>
<td style="text-align:center">Applied Physics</td>
<td style="text-align:center">应用物理学</td>
</tr>
<tr>
<td style="text-align:center">physics.atm-clus</td>
<td style="text-align:center">Atomic and Molecular Clusters</td>
<td style="text-align:center">原子和分子团簇</td>
</tr>
<tr>
<td style="text-align:center">physics.atom-ph</td>
<td style="text-align:center">Atomic Physics</td>
<td style="text-align:center">原子物理学</td>
</tr>
<tr>
<td style="text-align:center">physics.bio-ph</td>
<td style="text-align:center">Biological Physics</td>
<td style="text-align:center">生物物理学</td>
</tr>
<tr>
<td style="text-align:center">physics.chem-ph</td>
<td style="text-align:center">Chemical Physics</td>
<td style="text-align:center">化学物理</td>
</tr>
<tr>
<td style="text-align:center">physics.class-ph</td>
<td style="text-align:center">Classical Physics</td>
<td style="text-align:center">经典物理学</td>
</tr>
<tr>
<td style="text-align:center">physics.comp-ph</td>
<td style="text-align:center">Computational Physics</td>
<td style="text-align:center">计算物理学</td>
</tr>
<tr>
<td style="text-align:center">physics.data-an</td>
<td style="text-align:center">Data Analysis, Statistics and Probability</td>
<td style="text-align:center">数据分析、统计和概率</td>
</tr>
<tr>
<td style="text-align:center">physics.ed-ph</td>
<td style="text-align:center">Physics Education</td>
<td style="text-align:center">物理教育</td>
</tr>
<tr>
<td style="text-align:center">physics.flu-dyn</td>
<td style="text-align:center">Fluid Dynamics</td>
<td style="text-align:center">流体动力学</td>
</tr>
<tr>
<td style="text-align:center">physics.gen-ph</td>
<td style="text-align:center">General Physics</td>
<td style="text-align:center">普通物理</td>
</tr>
<tr>
<td style="text-align:center">physics.geo-ph</td>
<td style="text-align:center">Geophysics</td>
<td style="text-align:center">地球物理学</td>
</tr>
<tr>
<td style="text-align:center">physics.hist-ph</td>
<td style="text-align:center">History and Philosophy of Physics</td>
<td style="text-align:center">物理学的历史与哲学</td>
</tr>
<tr>
<td style="text-align:center">physics.ins-det</td>
<td style="text-align:center">Instrumentation and Detectors</td>
<td style="text-align:center">仪器和探测器</td>
</tr>
<tr>
<td style="text-align:center">physics.med-ph</td>
<td style="text-align:center">Medical Physics</td>
<td style="text-align:center">医学物理学</td>
</tr>
<tr>
<td style="text-align:center">physics.optics</td>
<td style="text-align:center">Optics</td>
<td style="text-align:center">光学</td>
</tr>
<tr>
<td style="text-align:center">physics.plasm-ph</td>
<td style="text-align:center">Plasma Physics</td>
<td style="text-align:center">等离子体物理</td>
</tr>
<tr>
<td style="text-align:center">physics.pop-ph</td>
<td style="text-align:center">Popular Physics</td>
<td style="text-align:center">大众物理</td>
</tr>
<tr>
<td style="text-align:center">physics.soc-ph</td>
<td style="text-align:center">Physics and Society</td>
<td style="text-align:center">物理学与社会</td>
</tr>
<tr>
<td style="text-align:center">physics.space-ph</td>
<td style="text-align:center">Space Physics</td>
<td style="text-align:center">空间物理学</td>
</tr>
<tr>
<td style="text-align:center">q-bio.BM</td>
<td style="text-align:center">Biomolecules</td>
<td style="text-align:center">生物分子</td>
</tr>
<tr>
<td style="text-align:center">q-bio.CB</td>
<td style="text-align:center">Cell Behavior</td>
<td style="text-align:center">细胞行为</td>
</tr>
<tr>
<td style="text-align:center">q-bio.GN</td>
<td style="text-align:center">Genomics</td>
<td style="text-align:center">基因组学</td>
</tr>
<tr>
<td style="text-align:center">q-bio.MN</td>
<td style="text-align:center">Molecular Networks</td>
<td style="text-align:center">分子网络</td>
</tr>
<tr>
<td style="text-align:center">q-bio.NC</td>
<td style="text-align:center">Neurons and Cognition</td>
<td style="text-align:center">神经元与认知</td>
</tr>
<tr>
<td style="text-align:center">q-bio.OT</td>
<td style="text-align:center">Other Quantitative Biology</td>
<td style="text-align:center">其他定量生物学</td>
</tr>
<tr>
<td style="text-align:center">q-bio.PE</td>
<td style="text-align:center">Populations and Evolution</td>
<td style="text-align:center">种群与进化</td>
</tr>
<tr>
<td style="text-align:center">q-bio.QM</td>
<td style="text-align:center">Quantitative Methods</td>
<td style="text-align:center">定量方法</td>
</tr>
<tr>
<td style="text-align:center">q-bio.SC</td>
<td style="text-align:center">Subcellular Processes</td>
<td style="text-align:center">亚细胞突起</td>
</tr>
<tr>
<td style="text-align:center">q-bio.TO</td>
<td style="text-align:center">Tissues and Organs</td>
<td style="text-align:center">组织和器官</td>
</tr>
<tr>
<td style="text-align:center">q-fin.CP</td>
<td style="text-align:center">Computational Finance</td>
<td style="text-align:center">金融工程</td>
</tr>
<tr>
<td style="text-align:center">q-fin.EC</td>
<td style="text-align:center">Economics</td>
<td style="text-align:center">经济学</td>
</tr>
<tr>
<td style="text-align:center">q-fin.GN</td>
<td style="text-align:center">General Finance</td>
<td style="text-align:center">财务概述</td>
</tr>
<tr>
<td style="text-align:center">q-fin.MF</td>
<td style="text-align:center">Mathematical Finance</td>
<td style="text-align:center">数学金融</td>
</tr>
<tr>
<td style="text-align:center">q-fin.PM</td>
<td style="text-align:center">Portfolio Management</td>
<td style="text-align:center">投资组合管理</td>
</tr>
<tr>
<td style="text-align:center">q-fin.PR</td>
<td style="text-align:center">Pricing of Securities</td>
<td style="text-align:center">证券定价</td>
</tr>
<tr>
<td style="text-align:center">q-fin.RM</td>
<td style="text-align:center">Risk Management</td>
<td style="text-align:center">风险管理</td>
</tr>
<tr>
<td style="text-align:center">q-fin.ST</td>
<td style="text-align:center">Statistical Finance</td>
<td style="text-align:center">金融统计</td>
</tr>
<tr>
<td style="text-align:center">q-fin.TR</td>
<td style="text-align:center">Trading and Market Microstructure</td>
<td style="text-align:center">交易与市场微观结构</td>
</tr>
<tr>
<td style="text-align:center">quant-ph</td>
<td style="text-align:center">Quantum Physics</td>
<td style="text-align:center">量子物理学</td>
</tr>
<tr>
<td style="text-align:center">stat.AP</td>
<td style="text-align:center">Applications</td>
<td style="text-align:center">应用</td>
</tr>
<tr>
<td style="text-align:center">stat.CO</td>
<td style="text-align:center">Computation</td>
<td style="text-align:center">计算</td>
</tr>
<tr>
<td style="text-align:center">stat.ME</td>
<td style="text-align:center">Methodology</td>
<td style="text-align:center">方法论</td>
</tr>
<tr>
<td style="text-align:center">stat.ML</td>
<td style="text-align:center">Machine Learning</td>
<td style="text-align:center">机器学习</td>
</tr>
<tr>
<td style="text-align:center">stat.OT</td>
<td style="text-align:center">Other Statistics</td>
<td style="text-align:center">其他统计学</td>
</tr>
<tr>
<td style="text-align:center">stat.TH</td>
<td style="text-align:center">Statistics Theory</td>
<td style="text-align:center">统计学理论</td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <categories>
        <category>Note</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>FastAPI 快速教程: 从零开始构建你的第一个API项目</title>
    <url>/2024/01/19/Note/fastapi/</url>
    <content><![CDATA[<p>最近在学习大模型的时候，有时候会遇到要写API的时候，这个时候我就遇见了FastAPI，我发现这个是一个很好的库，可以很方便的让我们构建一个属于自己的API，所以今天我也写一下这个入门教程和大家一起分享一下，同时也让我们解密一下，OpenAI和一些公司的API，可能是怎么写和怎么做的。</p>
<p><img src="https://picx.zhimg.com/80/v2-f7dc5c12cb693d83a113359819a1f26e_720w.png?source=d16d100b" alt="FastAPI framework, high performance, easy to learn, fast to code, ready for production"></p>
<h2 id="FastAPI介绍"><a href="#FastAPI介绍" class="headerlink" title="FastAPI介绍"></a>FastAPI介绍</h2><p>FastAPI 是一个用于构建 API 的现代、快速（高性能）的 web 框架，使用 Python 3.8+ 并基于标准的 Python 类型提示。</p>
<p><strong>文档</strong>： <a href="https://fastapi.tiangolo.com/">https://fastapi.tiangolo.com</a></p>
<p><strong>源码</strong>： <a href="https://github.com/tiangolo/fastapi">https://github.com/tiangolo/fastapi</a></p>
<p>关键特性:</p>
<ul>
<li><strong>快速</strong>：可与 <strong>NodeJS</strong> 和 <strong>Go</strong> 并肩的极高性能（归功于 Starlette 和 Pydantic）。<a href="https://fastapi.tiangolo.com/zh/#_11">最快的 Python web 框架之一</a>。</li>
<li><strong>高效编码</strong>：提高功能开发速度约 200％ 至 300％。*</li>
<li><strong>更少 bug</strong>：减少约 40％ 的人为（开发者）导致错误。*</li>
<li><strong>智能</strong>：极佳的编辑器支持。处处皆可自动补全，减少调试时间。</li>
<li><strong>简单</strong>：设计的易于使用和学习，阅读文档的时间更短。</li>
<li><strong>简短</strong>：使代码重复最小化。通过不同的参数声明实现丰富功能。bug 更少。</li>
<li><strong>健壮</strong>：生产可用级别的代码。还有自动生成的交互式文档。</li>
<li><strong>标准化</strong>：基于（并完全兼容）API 的相关开放标准：<a href="https://github.com/OAI/OpenAPI-Specification">OpenAPI</a> (以前被称为 Swagger) 和 <a href="https://json-schema.org/">JSON Schema</a>。</li>
</ul>
<h2 id="安装及依赖"><a href="#安装及依赖" class="headerlink" title="安装及依赖"></a>安装及依赖</h2><p>Python 3.8 及更高版本</p>
<p>FastAPI 站在以下巨人的肩膀之上：</p>
<ul>
<li><a href="https://www.starlette.io/">Starlette</a> 负责 web 部分。</li>
<li><a href="https://pydantic-docs.helpmanual.io/">Pydantic</a> 负责数据部分。</li>
</ul>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">pip install fastapi</span><br></pre></td></tr></tbody></table></figure>
<p>我们有可能还会需要一个 ASGI 服务器，可以使用<a href="https://www.uvicorn.org/">Uvicorn</a></p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">pip install unvicorn</span><br></pre></td></tr></tbody></table></figure>
<h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><h3 id="创建"><a href="#创建" class="headerlink" title="创建"></a>创建</h3><p>创建一个 main.py 文件并写入以下内容:</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Union</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> fastapi <span class="keyword">import</span> FastAPI</span><br><span class="line"></span><br><span class="line">app = FastAPI()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.get(<span class="params"><span class="string">"/"</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_root</span>():</span><br><span class="line">    <span class="keyword">return</span> {<span class="string">"Hello"</span>: <span class="string">"World"</span>}</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.get(<span class="params"><span class="string">"/items/{item_id}"</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_item</span>(<span class="params">item_id: <span class="built_in">int</span>, q: <span class="type">Union</span>[<span class="built_in">str</span>, <span class="literal">None</span>] = <span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">return</span> {<span class="string">"item_id"</span>: item_id, <span class="string">"q"</span>: q}</span><br></pre></td></tr></tbody></table></figure>
<p>如果我们需要加入异步编程的话，我们就需要改一下代码，加入async/await和async def </p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Union</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> fastapi <span class="keyword">import</span> FastAPI</span><br><span class="line"></span><br><span class="line">app = FastAPI()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.get(<span class="params"><span class="string">"/"</span></span>)</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">read_root</span>():</span><br><span class="line">    <span class="keyword">return</span> {<span class="string">"Hello"</span>: <span class="string">"World"</span>}</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.get(<span class="params"><span class="string">"/items/{item_id}"</span></span>)</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">read_item</span>(<span class="params">item_id: <span class="built_in">int</span>, q: <span class="type">Union</span>[<span class="built_in">str</span>, <span class="literal">None</span>] = <span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">return</span> {<span class="string">"item_id"</span>: item_id, <span class="string">"q"</span>: q}</span><br></pre></td></tr></tbody></table></figure>
<p>如果对于异步编程有点兴趣的话，可以看看这个讲解，我觉得还是很不错的，这也加强了代码的并发能力。</p>
<h3 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h3><p>通过以下命令进行运行服务器</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">$ uvicorn main:app --reload</span><br><span class="line"></span><br><span class="line">INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)</span><br><span class="line">INFO:     Started reloader process [28720]</span><br><span class="line">INFO:     Started server process [28722]</span><br><span class="line">INFO:     Waiting <span class="keyword">for</span> application startup.</span><br><span class="line">INFO:     Application startup complete.</span><br></pre></td></tr></tbody></table></figure>
<p>uvicorn main:app 命令含义如下:</p>
<ul>
<li>main：main.py 文件（一个 Python “模块”）。</li>
<li>app：在 main.py 文件中通过 app = FastAPI() 创建的对象。</li>
<li>—reload：让服务器在更新代码后重新启动。仅在开发时使用该选项。</li>
</ul>
<p>这里面着重提一下<strong>reolab参数</strong>，这个相当于我们在开发是对代码进行修改的同时，服务器也在变化，这样就方便我们进行开发和学习，但是如果开发完毕以后，我们可以去掉<strong>—reload</strong>，防止不小心动到代码改变了api的访问</p>
<h3 id="检查"><a href="#检查" class="headerlink" title="检查"></a><strong>检查</strong></h3><p>使用浏览器访问 <a href="http://127.0.0.1:8000/items/5?q=somequery。">http://127.0.0.1:8000/items/5?q=somequery。</a></p>
<p>你将会看到如下 JSON 响应：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">{"item_id": 5, "q": "somequery"}</span><br></pre></td></tr></tbody></table></figure>
<p>你已经创建了一个具有以下功能的 API：</p>
<ul>
<li>通过 <em>路径</em> / 和 /items/{item_id} 接受 HTTP 请求。</li>
<li>以上 <em>路径</em> 都接受 GET <em>操作</em>（也被称为 HTTP <em>方法</em>）。</li>
<li>/items/{item_id} <em>路径</em> 有一个 <em>路径参数</em> item_id 并且应该为 int 类型。</li>
<li>/items/{item_id} <em>路径</em> 有一个可选的 str 类型的 <em>查询参数</em> q。</li>
</ul>
<h2 id="API文档"><a href="#API文档" class="headerlink" title="API文档"></a>API文档</h2><h3 id="交互式-API-文档"><a href="#交互式-API-文档" class="headerlink" title="交互式 API 文档"></a><strong>交互式 API 文档</strong></h3><p>现在访问 <a href="http://127.0.0.1:8000/docs。">http://127.0.0.1:8000/docs。</a></p>
<p>你会看到自动生成的交互式 API 文档（由 <a href="https://github.com/swagger-api/swagger-ui">Swagger UI</a>生成）：</p>
<p><img src="https://picx.zhimg.com/80/v2-39fe8891285bf578eee8466c4aec1b52_720w.png?source=d16d100b" alt="交互式 API 文档"></p>
<h3 id="可选的-API-文档"><a href="#可选的-API-文档" class="headerlink" title="可选的 API 文档"></a>可选的 API 文档</h3><p>访问 <a href="http://127.0.0.1:8000/redoc。">http://127.0.0.1:8000/redoc。</a></p>
<p>你会看到另一个自动生成的文档（由 <a href="https://github.com/Rebilly/ReDoc">ReDoc</a> 生成）：</p>
<p><img src="https://picx.zhimg.com/80/v2-23ae5bafaed6faf6ced83d987b17fc98_720w.png?source=d16d100b" alt="可选的 API 文档"></p>
<h2 id="大模型API实战"><a href="#大模型API实战" class="headerlink" title="大模型API实战"></a>大模型API实战</h2><p>比如我现在想使用一个大模型，比如就是阿里的通义千问的大模型，我希望能写一个api接口进行对其调用，有点类似与OpenAI一样写一个接口，这样就方便我们进行去调用，而不用每次跑一堆代码。</p>
<h3 id="Qwen模型下载与使用"><a href="#Qwen模型下载与使用" class="headerlink" title="Qwen模型下载与使用"></a>Qwen模型下载与使用</h3><p>比如我们可以从Qwen中获取对应的模型，<a href="https://huggingface.co/Qwen/Qwen-7B-Chat">https://huggingface.co/Qwen/Qwen-7B-Chat</a></p>
<p>从里面我们可以看到多轮对话的快速使用代码，这对我们写API有很大的帮助，我们从中可以看到主要的流程，实际上还是蛮简单的，就是导入模型后，进行传入参数，参数一般有三个，一个是一开始定义的分词器<strong>tokenizer</strong>，另外两个就比较重要，分别是问题和历史记录，所以我们希望得到的api应该是有这两个输入的。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForCausalLM, AutoTokenizer</span><br><span class="line"><span class="keyword">from</span> transformers.generation <span class="keyword">import</span> GenerationConfig</span><br><span class="line"></span><br><span class="line"><span class="comment"># Note: The default behavior now has injection attack prevention off.</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">"Qwen/Qwen-7B-Chat"</span>, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># use bf16</span></span><br><span class="line"><span class="comment"># model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True, bf16=True).eval()</span></span><br><span class="line"><span class="comment"># use fp16</span></span><br><span class="line"><span class="comment"># model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True, fp16=True).eval()</span></span><br><span class="line"><span class="comment"># use cpu only</span></span><br><span class="line"><span class="comment"># model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="cpu", trust_remote_code=True).eval()</span></span><br><span class="line"><span class="comment"># use auto mode, automatically select precision based on the device.</span></span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(<span class="string">"Qwen/Qwen-7B-Chat"</span>, device_map=<span class="string">"auto"</span>, trust_remote_code=<span class="literal">True</span>).<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Specify hyperparameters for generation. But if you use transformers&gt;=4.32.0, there is no need to do this.</span></span><br><span class="line"><span class="comment"># model.generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-7B-Chat", trust_remote_code=True) # 可指定不同的生成长度、top_p等相关超参</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一轮对话 1st dialogue turn</span></span><br><span class="line">response, history = model.chat(tokenizer, <span class="string">"你好"</span>, history=<span class="literal">None</span>)</span><br><span class="line"><span class="built_in">print</span>(response)</span><br><span class="line"><span class="comment"># 你好！很高兴为你提供帮助。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二轮对话 2nd dialogue turn</span></span><br><span class="line">response, history = model.chat(tokenizer, <span class="string">"给我讲一个年轻人奋斗创业最终取得成功的故事。"</span>, history=history)</span><br><span class="line"><span class="built_in">print</span>(response)</span><br><span class="line"><span class="comment"># 这是一个关于一个年轻人奋斗创业最终取得成功的故事。</span></span><br><span class="line"><span class="comment"># 故事的主人公叫李明，他来自一个普通的家庭，父母都是普通的工人。从小，李明就立下了一个目标：要成为一名成功的企业家。</span></span><br><span class="line"><span class="comment"># 为了实现这个目标，李明勤奋学习，考上了大学。在大学期间，他积极参加各种创业比赛，获得了不少奖项。他还利用课余时间去实习，积累了宝贵的经验。</span></span><br><span class="line"><span class="comment"># 毕业后，李明决定开始自己的创业之路。他开始寻找投资机会，但多次都被拒绝了。然而，他并没有放弃。他继续努力，不断改进自己的创业计划，并寻找新的投资机会。</span></span><br><span class="line"><span class="comment"># 最终，李明成功地获得了一笔投资，开始了自己的创业之路。他成立了一家科技公司，专注于开发新型软件。在他的领导下，公司迅速发展起来，成为了一家成功的科技企业。</span></span><br><span class="line"><span class="comment"># 李明的成功并不是偶然的。他勤奋、坚韧、勇于冒险，不断学习和改进自己。他的成功也证明了，只要努力奋斗，任何人都有可能取得成功。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第三轮对话 3rd dialogue turn</span></span><br><span class="line">response, history = model.chat(tokenizer, <span class="string">"给这个故事起一个标题"</span>, history=history)</span><br><span class="line"><span class="built_in">print</span>(response)</span><br><span class="line"><span class="comment"># 《奋斗创业：一个年轻人的成功之路》</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="编写FastAPI代码"><a href="#编写FastAPI代码" class="headerlink" title="编写FastAPI代码"></a>编写FastAPI代码</h3><p>所以弄清楚了原理以后，我们就可以开始利用FastAPI写以下的代码了，为了更好的使用api，除了prompt和history两个参数之外，还加入了max_lenth和temperature等参数，这些参数实际上都是model.chat里面进行使用的，这样更好的去设置和学习。等到返回结果以后，就会返回时间和成功的标志，这样我们就获取了结果。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> fastapi <span class="keyword">import</span> FastAPI, Request</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForCausalLM, GenerationConfig</span><br><span class="line"><span class="keyword">import</span> uvicorn</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置设备参数</span></span><br><span class="line">DEVICE = <span class="string">"cuda"</span>  <span class="comment"># 使用CUDA</span></span><br><span class="line">DEVICE_ID = <span class="string">"0"</span>  <span class="comment"># CUDA设备ID，如果未设置则为空</span></span><br><span class="line">CUDA_DEVICE = <span class="string">f"<span class="subst">{DEVICE}</span>:<span class="subst">{DEVICE_ID}</span>"</span> <span class="keyword">if</span> DEVICE_ID <span class="keyword">else</span> DEVICE  <span class="comment"># 组合CUDA设备信息</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 清理GPU内存函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">torch_gc</span>():</span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():  <span class="comment"># 检查是否可用CUDA</span></span><br><span class="line">        <span class="keyword">with</span> torch.cuda.device(CUDA_DEVICE):  <span class="comment"># 指定CUDA设备</span></span><br><span class="line">            torch.cuda.empty_cache()  <span class="comment"># 清空CUDA缓存</span></span><br><span class="line">            torch.cuda.ipc_collect()  <span class="comment"># 收集CUDA内存碎片</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建FastAPI应用</span></span><br><span class="line">app = FastAPI()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理POST请求的端点</span></span><br><span class="line"><span class="meta">@app.post(<span class="params"><span class="string">"/"</span></span>)</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">create_item</span>(<span class="params">request: Request</span>):</span><br><span class="line">    <span class="keyword">global</span> model, tokenizer  <span class="comment"># 声明全局变量以便在函数内部使用模型和分词器</span></span><br><span class="line">    json_post_raw = <span class="keyword">await</span> request.json()  <span class="comment"># 获取POST请求的JSON数据</span></span><br><span class="line">    json_post = json.dumps(json_post_raw)  <span class="comment"># 将JSON数据转换为字符串</span></span><br><span class="line">    json_post_list = json.loads(json_post)  <span class="comment"># 将字符串转换为Python对象</span></span><br><span class="line">    prompt = json_post_list.get(<span class="string">'prompt'</span>)  <span class="comment"># 获取请求中的提示</span></span><br><span class="line">    history = json_post_list.get(<span class="string">'history'</span>)  <span class="comment"># 获取请求中的历史记录</span></span><br><span class="line">    max_length = json_post_list.get(<span class="string">'max_length'</span>)  <span class="comment"># 获取请求中的最大长度</span></span><br><span class="line">    top_p = json_post_list.get(<span class="string">'top_p'</span>)  <span class="comment"># 获取请求中的top_p参数</span></span><br><span class="line">    temperature = json_post_list.get(<span class="string">'temperature'</span>)  <span class="comment"># 获取请求中的温度参数</span></span><br><span class="line">    <span class="comment"># 调用模型进行对话生成</span></span><br><span class="line">    response, history = model.chat(</span><br><span class="line">        tokenizer,</span><br><span class="line">        prompt,</span><br><span class="line">        history=history,</span><br><span class="line">        max_length=max_length <span class="keyword">if</span> max_length <span class="keyword">else</span> <span class="number">2048</span>,  <span class="comment"># 如果未提供最大长度，默认使用2048</span></span><br><span class="line">        top_p=top_p <span class="keyword">if</span> top_p <span class="keyword">else</span> <span class="number">0.7</span>,  <span class="comment"># 如果未提供top_p参数，默认使用0.7</span></span><br><span class="line">        temperature=temperature <span class="keyword">if</span> temperature <span class="keyword">else</span> <span class="number">0.95</span>  <span class="comment"># 如果未提供温度参数，默认使用0.95</span></span><br><span class="line">    )</span><br><span class="line">    now = datetime.datetime.now()  <span class="comment"># 获取当前时间</span></span><br><span class="line">    time = now.strftime(<span class="string">"%Y-%m-%d %H:%M:%S"</span>)  <span class="comment"># 格式化时间为字符串</span></span><br><span class="line">    <span class="comment"># 构建响应JSON</span></span><br><span class="line">    answer = {</span><br><span class="line">        <span class="string">"response"</span>: response,</span><br><span class="line">        <span class="string">"history"</span>: history,</span><br><span class="line">        <span class="string">"status"</span>: <span class="number">200</span>,</span><br><span class="line">        <span class="string">"time"</span>: time</span><br><span class="line">    }</span><br><span class="line">    <span class="comment"># 构建日志信息</span></span><br><span class="line">    log = <span class="string">"["</span> + time + <span class="string">"] "</span> + <span class="string">'", prompt:"'</span> + prompt + <span class="string">'", response:"'</span> + <span class="built_in">repr</span>(response) + <span class="string">'"'</span></span><br><span class="line">    <span class="built_in">print</span>(log)  <span class="comment"># 打印日志</span></span><br><span class="line">    torch_gc()  <span class="comment"># 执行GPU内存清理</span></span><br><span class="line">    <span class="keyword">return</span> answer  <span class="comment"># 返回响应</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 主函数入口</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># 加载预训练的分词器和模型</span></span><br><span class="line">    tokenizer = AutoTokenizer.from_pretrained(<span class="string">"Qwen/Qwen-7B-Chat"</span>, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line">    model = AutoModelForCausalLM.from_pretrained(<span class="string">"Qwen/Qwen-7B-Chat"</span>, device_map=<span class="string">"auto"</span>, trust_remote_code=<span class="literal">True</span>).<span class="built_in">eval</span>()</span><br><span class="line">    model.generation_config = GenerationConfig.from_pretrained(<span class="string">"Qwen/Qwen-7B-Chat"</span>, trust_remote_code=<span class="literal">True</span>) <span class="comment"># 可指定</span></span><br><span class="line">    model.<span class="built_in">eval</span>()  <span class="comment"># 设置模型为评估模式</span></span><br><span class="line">    <span class="comment"># 启动FastAPI应用，用6006端口映射到本地，从而在本地使用api</span></span><br><span class="line">    uvicorn.run(app, host=<span class="string">'0.0.0.0'</span>, port=<span class="number">6006</span>, workers=<span class="number">1</span>)  <span class="comment"># 在指定端口和主机上启动应用</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="运行及使用API"><a href="#运行及使用API" class="headerlink" title="运行及使用API"></a>运行及使用API</h3><p>我们运行方式很简单，直接在服务器终端运行</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">python api.py</span><br></pre></td></tr></tbody></table></figure>
<p>加载完毕后出现如下信息说明成功。</p>
<p><img src="https://picx.zhimg.com/80/v2-25146b4a0f3b5a0fa70782b1ed749e5d_720w.png?source=d16d100b" alt="运行方式"></p>
<p>默认部署在 6006 端口，通过 POST 方法进行调用，可以使用curl调用，如下所示：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">curl -X POST "http://127.0.0.1:6006" \</span><br><span class="line">     -H 'Content-Type: application/json' \</span><br><span class="line">     -d '{"prompt": "你好", "history": []}'</span><br></pre></td></tr></tbody></table></figure>
<p>也可以使用python中的requests库进行调用，如下所示：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">import requests</span><br><span class="line">import json</span><br><span class="line"></span><br><span class="line">def get_completion(prompt):</span><br><span class="line">    headers = {'Content-Type': 'application/json'}</span><br><span class="line">    data = {"prompt": prompt, "history": []}</span><br><span class="line">    response = requests.post(url='http://127.0.0.1:6006', headers=headers, data=json.dumps(data))</span><br><span class="line">    return response.json()['response']</span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line">    print(get_completion('你好'))</span><br></pre></td></tr></tbody></table></figure>
<p>得到的返回值如下所示：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">{</span><br><span class="line">  "response":"你好！很高兴为你服务。有什么我可以帮助你的吗？",</span><br><span class="line">  "history":[["你好","你好！很高兴为你服务。有什么我可以帮助你的吗？"]],</span><br><span class="line">  "status":200,</span><br><span class="line">  "time":"2023-11-26 1:14:20"</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p><img src="https://pic1.zhimg.com/80/v2-284a1395c44b8bb0177dae70fd54b930_720w.png?source=d16d100b" alt="运行结果"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>我觉得在有时候我们要实现一个API的时候，我们可以用FastAPI快速实现，并且得到一个不错的结果，这也是我学习的初衷，有时候一些服务器可能可以当做API来使用来调用，其实也方便去使用，也可以部署后成为商业产品，类似于OpenAI一样。</p>
<p>最后感谢一下FastAPI的文档，让我学习到很多，在里面还有更详细的使用方案，大家也可以去学习一下，然后再感谢一下datawhale的self-llm项目，也是在里面我学习到了使用FastAPI，大家如果对大模型感兴趣也可以关注一下。</p>
<p>self-llm项目：<a href="https://github.com/datawhalechina/self-llm">https://github.com/datawhalechina/self-llm</a></p>
<p>FastAPI学习文档：<a href="https://fastapi.tiangolo.com/zh/learn/">https://fastapi.tiangolo.com/zh/learn/</a></p>
]]></content>
      <categories>
        <category>Note</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Note</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux使用gdown从Google Drive下载文件和文件夹（命令行/代码下载）</title>
    <url>/2024/01/19/Note/gdown/</url>
    <content><![CDATA[<h2 id="一、安装gdown"><a href="#一、安装gdown" class="headerlink" title="一、安装gdown"></a>一、安装gdown</h2><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/wkentaro/gdown </span><br><span class="line"><span class="built_in">cd</span> gdown</span><br><span class="line">pip install gdown</span><br></pre></td></tr></tbody></table></figure>
<h2 id="二、获取Google-Drive文件链接"><a href="#二、获取Google-Drive文件链接" class="headerlink" title="二、获取Google Drive文件链接"></a>二、获取Google Drive文件链接</h2><ol>
<li>打开Google Drive</li>
<li>右键点击要下载的文件/文件夹</li>
<li>选择”获取链接”</li>
<li>确保文件/文件夹的访问权限设置为”任何人均可访问”</li>
<li>打开分享链接,复制地址栏中的文件ID,链接前缀都为<code>https://drive.google.com/uc?id=</code>，如<code>https://drive.google.com/uc?id=1l_5RK28JRL19wpT22B-DY9We3TVXnnQQ</code></li>
<li>复制链接</li>
</ol>
<h2 id="三、使用gdown下载"><a href="#三、使用gdown下载" class="headerlink" title="三、使用gdown下载"></a>三、使用gdown下载</h2><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 文件下载</span></span><br><span class="line"></span><br><span class="line">gdown https://drive.google.com/uc?<span class="built_in">id</span>=&lt;文件ID&gt;</span><br><span class="line"><span class="comment"># gdown https://drive.google.com/uc?id=1l_5RK28JRL19wpT22B-DY9We3TVXnnQQ</span></span><br><span class="line"><span class="comment"># gdown 1l_5RK28JRL19wpT22B-DY9We3TVXnnQQ</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 文件夹下载</span></span><br><span class="line">gdown https://drive.google.com/drive/folders/15uNXeRBIhVvZJIhL4yTw4IsStMhUaaxl -O /tmp/folder --folder</span><br></pre></td></tr></tbody></table></figure>
<p>除了命令行之外，我们也可以通过代码来进行下载</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">## 文件下载</span></span><br><span class="line"><span class="keyword">import</span> gdown</span><br><span class="line"></span><br><span class="line">url = <span class="string">'https://drive.google.com/file/d/1uFTzwFc3tmS-D7azjMiJcxSfn71BPqKt/view?usp=sharing'</span></span><br><span class="line">output_path = <span class="string">'graph_ML.pk'</span></span><br><span class="line">gdown.download(url, output_path, quiet=<span class="literal">False</span>,fuzzy=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 文件夹下载</span></span><br><span class="line"><span class="keyword">import</span> gdown</span><br><span class="line">url = <span class="string">"https://drive.google.com/drive/folders/1HWFHKCprFzR7H7TYhrE-W7v4bz2Vc7Ia"</span></span><br><span class="line"></span><br><span class="line">gdown.download_folder(url, quiet=<span class="literal">True</span>, use_cookies=<span class="literal">False</span>)</span><br></pre></td></tr></tbody></table></figure>
<blockquote>
<p>除此之外，还有一些命令的使用，这里就不过多解释了</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">gdown --<span class="built_in">help</span></span><br><span class="line">usage: gdown [-h] [-V] [-O OUTPUT] [-q] [--fuzzy] [--<span class="built_in">id</span>] [--proxy PROXY]</span><br><span class="line">             [--speed SPEED] [--no-cookies] [--no-check-certificate]</span><br><span class="line">             [--<span class="built_in">continue</span>] [--folder] [--remaining-ok]</span><br><span class="line">             url_or_id</span><br></pre></td></tr></tbody></table></figure>
</blockquote>
<h2 id="四、问题解决"><a href="#四、问题解决" class="headerlink" title="四、问题解决"></a>四、问题解决</h2><p>由于Google Drive文件大小限制,直接使用curl/wget下载可能会失败。</p>
<p>这时需要使用gdown来实现从Google Drive下载大文件。它可以解决由于文件太大导致的curl/wget下载失败问题。</p>
<p><strong>参考链接</strong></p>
<p>gdown项目地址: <a href="https://github.com/wkentaro/gdown">https://github.com/wkentaro/gdown</a></p>
]]></content>
      <categories>
        <category>Note</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Note</tag>
      </tags>
  </entry>
  <entry>
    <title>Failed building wheel for PyAudio  解决方法</title>
    <url>/2024/01/28/Note/pyaudio/</url>
    <content><![CDATA[<h2 id="Failed-building-wheel-for-PyAudio-解决方法"><a href="#Failed-building-wheel-for-PyAudio-解决方法" class="headerlink" title="Failed building wheel for PyAudio  解决方法"></a>Failed building wheel for PyAudio  解决方法</h2><p>有时候在安装pyaudio的时候，总是有时候遇见一些错误，如下</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">  Building wheel <span class="keyword">for</span> pyaudio (pyproject.toml) ... error</span><br><span class="line">  error: subprocess-exited-with-error</span><br><span class="line"></span><br><span class="line">  × Building wheel <span class="keyword">for</span> pyaudio (pyproject.toml) did not run successfully.</span><br><span class="line">  │ <span class="built_in">exit</span> code: 1</span><br><span class="line">  ╰─&gt; [18 lines of output]</span><br><span class="line">      running bdist_wheel</span><br><span class="line">      running build</span><br><span class="line">      running build_py</span><br><span class="line">      creating build</span><br><span class="line">      creating build/lib.linux-x86_64-cpython-310</span><br><span class="line">      creating build/lib.linux-x86_64-cpython-310/pyaudio</span><br><span class="line">      copying src/pyaudio/__init__.py -&gt; build/lib.linux-x86_64-cpython-310/pyaudio</span><br><span class="line">      running build_ext</span><br><span class="line">      building <span class="string">'pyaudio._portaudio'</span> extension</span><br><span class="line">      creating build/temp.linux-x86_64-cpython-310</span><br><span class="line">      creating build/temp.linux-x86_64-cpython-310/src</span><br><span class="line">      creating build/temp.linux-x86_64-cpython-310/src/pyaudio</span><br><span class="line">      gcc -pthread -B anaconda3/envs/ernerf/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem anaconda3/envs/ernerf/include -fPIC -O2 -isystem anaconda3/envs/ernerf/include -fPIC -I/usr/local/include -I/usr/include -Ianaconda3/envs/ernerf/include/python3.10 -c src/pyaudio/device_api.c -o build/temp.linux-x86_64-cpython-310/src/pyaudio/device_api.o</span><br><span class="line">      src/pyaudio/device_api.c:9:10: fatal error: portaudio.h: No such file or directory</span><br><span class="line">          9 | <span class="comment">#include "portaudio.h"</span></span><br><span class="line">            |          ^~~~~~~~~~~~~</span><br><span class="line">      compilation terminated.</span><br><span class="line">      error: <span class="built_in">command</span> <span class="string">'/usr/bin/gcc'</span> failed with <span class="built_in">exit</span> code 1</span><br><span class="line">      [end of output]</span><br><span class="line"></span><br><span class="line">  note: This error originates from a subprocess, and is likely not a problem with pip.</span><br><span class="line">  ERROR: Failed building wheel <span class="keyword">for</span> pyaudio</span><br><span class="line">Successfully built python_speech_features</span><br><span class="line">Failed to build pyaudio</span><br><span class="line">ERROR: Could not build wheels <span class="keyword">for</span> pyaudio, <span class="built_in">which</span> is required to install pyproject.toml-based projects</span><br></pre></td></tr></tbody></table></figure>
<p>如果单纯查后面这一句，会发现找不到什么错误，最后我找到了对应的解决办法，实际上是linux有一些库没安装上，用root权限装一下即可</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 有些人说这样即可</span></span><br><span class="line">sudo apt-get install portaudio19-dev</span><br><span class="line"><span class="comment"># 如果不行就试一下这样</span></span><br><span class="line">sudo apt-get install libasound-dev portaudio19-dev libportaudio2 libportaudiocpp0</span><br></pre></td></tr></tbody></table></figure>
<p>这样安装完以后，我们就可以正常安装pyaudio了</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">pip install pyaudio</span><br></pre></td></tr></tbody></table></figure>
<p>我也在github上看到的相关帖子，大家也可以参考：<a href="https://github.com/ardha27/AI-Waifu-Vtuber/issues/49">https://github.com/ardha27/AI-Waifu-Vtuber/issues/49</a>，而且这里面有个windows的解决方法，还蛮有趣，我还没试过</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">pip install pipwin</span><br><span class="line">pipwin install pyaudio</span><br><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></tbody></table></figure>
]]></content>
      <categories>
        <category>Note</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Note</tag>
      </tags>
  </entry>
  <entry>
    <title>解决Flask-Sockets连接WebSocket时出现werkzeug.routing.WebsocketMismatch的错误</title>
    <url>/2024/01/15/Note/websocket/</url>
    <content><![CDATA[<p>在使用Flask-Sockets进行WebSocket连接时，一些用户可能会遇到如下错误信息：werkzeug.routing.WebsocketMismatch: 400 Bad Request: The browser (or proxy) sent a request that this server could not understand.</p>
<p>这个问题的解决方法可能有两种，从整理的资料上来看，接下来分别对两种方法进行说明。</p>
<ol>
<li>降低falsk和Werkzeug版本</li>
<li>涉及到安装flask_sockets库并对其源代码进行手动修改。</li>
</ol>
<h3 id="方法一：降低-Flask-和-Werkzeug-版本"><a href="#方法一：降低-Flask-和-Werkzeug-版本" class="headerlink" title="方法一：降低 Flask 和 Werkzeug 版本"></a>方法一：降低 Flask 和 Werkzeug 版本</h3><p>这种错误有时是因为Flask版本过高，降级即可成功，但是这种方式可能不够理想，推荐查看方法二。</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">pip install flask==1.1.2</span><br><span class="line">pip install Werkzeug==1.0.2</span><br></pre></td></tr></tbody></table></figure>
<h3 id="方法二（推荐）"><a href="#方法二（推荐）" class="headerlink" title="方法二（推荐）"></a>方法二（推荐）</h3><p>首先，确保已经安装了flask_sockets库，可以通过运行以下命令进行安装：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">pip install flask_sockets</span><br></pre></td></tr></tbody></table></figure>
<p>安装完成后，需要对flask_sockets库的源代码进行手动修改。具体的修改如下：</p>
<p>文件：flask_sockets.py 函数：add_url_rule</p>
<p>修改前的代码：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">self.url_map.add(Rule(rule, endpoint=f))</span><br></pre></td></tr></tbody></table></figure>
<p>修改后的代码：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">self.url_map.add(Rule(rule, endpoint=f, websocket=True))</span><br></pre></td></tr></tbody></table></figure>
<p>这个修改的目的是为了在添加URL规则时明确指定WebSocket。通过将websocket=True添加到Rule构造函数中，可以解决400 Bad Request错误。</p>
<p>关于这个问题的详细讨论可以参考以下链接： <a href="https://github.com/heroku-python/flask-sockets/issues/81">https://github.com/heroku-python/flask-sockets/issues/81</a></p>
<p>有关这个问题的具体修改可以参考以下链接： <a href="https://github.com/slipperstree/flask-sockets/commit/cb06c69db3af2cb52fbc050f3595ffa4100bbee3">https://github.com/slipperstree/flask-sockets/commit/cb06c69db3af2cb52fbc050f3595ffa4100bbee3</a></p>
<p>通过对flask_sockets库的手动修改，您可以顺利解决WebSocket连接时可能遇到的400 Bad Request错误，确保您的应用正常运行。希望这篇文章对您有帮助！</p>
]]></content>
      <categories>
        <category>Note</category>
      </categories>
      <tags>
        <tag>Bug</tag>
      </tags>
  </entry>
  <entry>
    <title>3D reconstruction</title>
    <url>/2024/01/24/Paper/2024-01-24/3D%20reconstruction/</url>
    <content><![CDATA[<blockquote>
<p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-01-24-更新"><a href="#2024-01-24-更新" class="headerlink" title="2024-01-24 更新"></a>2024-01-24 更新</h1><h2 id="Deformable-Endoscopic-Tissues-Reconstruction-with-Gaussian-Splatting"><a href="#Deformable-Endoscopic-Tissues-Reconstruction-with-Gaussian-Splatting" class="headerlink" title="Deformable Endoscopic Tissues Reconstruction with Gaussian Splatting"></a>Deformable Endoscopic Tissues Reconstruction with Gaussian Splatting</h2><p><strong>Authors:Lingting Zhu, Zhao Wang, Zhenchao Jin, Guying Lin, Lequan Yu</strong></p>
<p>Surgical 3D reconstruction is a critical area of research in robotic surgery, with recent works adopting variants of dynamic radiance fields to achieve success in 3D reconstruction of deformable tissues from single-viewpoint videos. However, these methods often suffer from time-consuming optimization or inferior quality, limiting their adoption in downstream tasks. Inspired by 3D Gaussian Splatting, a recent trending 3D representation, we present EndoGS, applying Gaussian Splatting for deformable endoscopic tissue reconstruction. Specifically, our approach incorporates deformation fields to handle dynamic scenes, depth-guided supervision to optimize 3D targets with a single viewpoint, and a spatial-temporal weight mask to mitigate tool occlusion. As a result, EndoGS reconstructs and renders high-quality deformable endoscopic tissues from a single-viewpoint video, estimated depth maps, and labeled tool masks. Experiments on DaVinci robotic surgery videos demonstrate that EndoGS achieves superior rendering quality. Code is available at <a href="https://github.com/HKU-MedAI/EndoGS">https://github.com/HKU-MedAI/EndoGS</a>. </p>
<p><a href="http://arxiv.org/abs/2401.11535v1">PDF</a> Work in progress. 10 pages, 4 figures</p>
<p><strong>摘要</strong></p>
<p>我们将 EndoGS 提出了一种基于高斯斑点的可变形内镜组织重建方法。</p>
<p><strong>要点</strong></p>
<ul>
<li>EndoGS通过采用高斯斑点来实现可变形内镜组织的 3D 重建。</li>
<li>EndoGS 引入了变形场来处理动态场景，并通过深度引导监督来优化具有单个视点的 3D 目标。</li>
<li>EndoGS 利用时空权重掩码来减轻工具遮挡。</li>
<li>EndoGS 可以从单视角视频、估计的深度图和标记的工具掩码中重建和渲染高质量的可变形内镜组织。</li>
<li>在 DaVinci 机器人手术视频上的实验表明，EndoGS 可实现优异的渲染质量。</li>
<li>EndoGS 的代码可在 <a href="https://github.com/HKU-MedAI/EndoGS">https://github.com/HKU-MedAI/EndoGS</a> 获得。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li><p>题目：高斯散点法可变形内窥镜组织重建</p>
</li>
<li><p>作者：Lingting Zhu, Zhao Wang, Zhenchao Jin, Guying Lin, Lequan Yu</p>
</li>
<li><p>第一作者单位：香港大学</p>
</li>
<li><p>关键词：高斯散点法 · 机器人手术 · 三维重建</p>
</li>
<li><p>论文链接：https://arxiv.org/abs/2401.11535，Github 链接：https://github.com/HKU-MedAI/EndoGS</p>
</li>
<li><p>摘要：
（1）研究背景：可变形组织的三维重建是机器人手术研究的关键领域，但现有方法往往存在优化耗时或质量较差的问题，限制了它们在后续任务中的应用。
（2）过去的方法：早期尝试采用深度估计来实现内窥镜重建，但这些方法在处理非刚性变形和遮挡方面存在困难。[9, 12] 提出结合工具遮挡、立体深度估计和稀疏翘曲场的框架，但它们在存在剧烈非拓扑可变形组织变化时仍然容易失败。神经辐射场 (NeRFs) 在三维重建方面取得了巨大成功，但它们在处理动态场景和遮挡方面也存在局限性。
（3）研究方法：本文提出了一种称为 EndoGS 的方法，将高斯散点法应用于可变形内窥镜组织重建。EndoGS 结合了变形场、深度引导监督和时空权重掩码，能够从单视角视频、估计的深度图和标记的工具掩码中重建和渲染高质量的可变形内窥镜组织。
（4）方法性能：在达芬奇机器人手术视频上的实验表明，EndoGS 在渲染质量方面优于其他方法。这表明 EndoGS 可以为下游任务（如手术增强现实、教育和机器人学习）提供高质量的可变形组织重建。</p>
</li>
<li><p>方法：
(1): 我们提出了一种称为 EndoGS 的方法，它将高斯散点法应用于可变形内窥镜组织重建。
(2): EndoGS 结合了变形场、深度引导监督和时空权重掩码，能够从单视角视频、估计的深度图和标记的工具掩码中重建和渲染高质量的可变形内窥镜组织。
(3): 我们使用六个正交特征平面对空间和时间信息进行编码，并使用单个 MLP 来更新高斯属性，以获得变形的位置、比例因子、旋转因子、球谐系数和不透明度。
(4): 我们结合工具掩码和深度图来训练 EndoGS，以处理工具遮挡和提高重建质量。</p>
</li>
<li><p>结论：</p>
</li>
</ol>
<p>（1）意义：本文提出了一种基于高斯散点法进行可变形内窥镜组织重建的方法，该方法能够从单视角视频、估计的深度图和标记的工具掩码中实时渲染高质量的可变形组织。在达芬奇机器人手术视频上的实验表明，该方法在渲染质量方面优于其他方法。</p>
<p>（2）优缺点：</p>
<p>创新点：</p>
<ul>
<li>将高斯散点法应用于可变形内窥镜组织重建。</li>
<li>结合变形场、深度引导监督和时空权重掩码，以处理工具遮挡和提高重建质量。</li>
</ul>
<p>性能：</p>
<ul>
<li>在达芬奇机器人手术视频上的实验表明，该方法在渲染质量方面优于其他方法。</li>
</ul>
<p>工作量：</p>
<ul>
<li>需要收集和标记大量的数据。</li>
<li>需要设计和训练复杂的模型。</li>
</ul>



<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3aced720ad0952509d5ad4feafb073c5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-db38985f02aa9f93361d5395728da086.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f22f8ab59ea6655501c3858f5b7639aa.jpg" align="middle">
</details>
​    


## SHINOBI: Shape and Illumination using Neural Object Decomposition via   BRDF Optimization In-the-wild
**Authors:Andreas Engelhardt, Amit Raj, Mark Boss, Yunzhi Zhang, Abhishek Kar, Yuanzhen Li, Deqing Sun, Ricardo Martin Brualla, Jonathan T. Barron, Hendrik P. A. Lensch, Varun Jampani**

We present SHINOBI, an end-to-end framework for the reconstruction of shape, material, and illumination from object images captured with varying lighting, pose, and background. Inverse rendering of an object based on unconstrained image collections is a long-standing challenge in computer vision and graphics and requires a joint optimization over shape, radiance, and pose. We show that an implicit shape representation based on a multi-resolution hash encoding enables faster and robust shape reconstruction with joint camera alignment optimization that outperforms prior work. Further, to enable the editing of illumination and object reflectance (i.e. material) we jointly optimize BRDF and illumination together with the object's shape. Our method is class-agnostic and works on in-the-wild image collections of objects to produce relightable 3D assets for several use cases such as AR/VR, movies, games, etc. Project page: https://shinobi.aengelhardt.com Video: https://www.youtube.com/watch?v=iFENQ6AcYd8&amp;feature=youtu.be 

[PDF](http://arxiv.org/abs/2401.10171v1) 

**摘要**
多尺度哈希编码的隐式形状表示使更快速、更鲁棒的形状重建成为可能，并通过联合相机对齐优化实现了对现有技术的超越。

**要点**
- SHINOBI 是一种端到端框架，用于从以不同光线、姿势和背景拍摄的对象图像中重建形状、材质和照明。
- SHINOBI 使用基于多尺度哈希编码的隐式形状表示，能够实现更快速、更鲁棒的形状重建。
- SHINOBI 还可以编辑照明和对象反射率（即材质），并与对象的形状一起优化双向反射分布函数（BRDF）和照明。
- SHINOBI 适用于各种各样的对象图像集合，可以为增强现实/虚拟现实、电影、游戏等多种用例生成可重新照明的 3D 资源。
- SHINOBI 可以从不限数量的图像中重建形状、材料和照明。
- SHINOBI 联合优化 BRDF 和照明以及对象的形状。
- SHINOBI 在各种各样的对象图像集合上进行评估，并与最先进的方法进行了比较。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li><p>题目：SHINOBI：基于 BRDF 优化进行形状和光照分解的神经物体分解</p>
</li>
<li><p>作者：Andreas Engelhardt、Amit Raj、Mark Boss、Yunzhi Zhang、Abhishek Kar、Yuanzhen Li、Deqing Sun、Ricardo Martin Brualla、Jonathan T. Barron、Hendrik P. A. Lensch、Varun Jampani</p>
</li>
<li><p>第一作者单位：德国图宾根大学</p>
</li>
<li><p>关键词：计算机视觉、图形学、神经渲染、形状重建、材质估计、光照估计</p>
</li>
<li><p>论文链接：https://arxiv.org/abs/2401.10171，Github 代码链接：无</p>
</li>
<li><p>摘要：
(1)：背景：逆向渲染基于无约束图像集合的对象是一个长期存在的挑战，需要对形状、光照和姿势进行联合优化。
(2)：过去的方法：现有方法通常使用显式形状表示，这在处理具有复杂几何形状的对象时存在局限性。此外，它们通常需要大量的手动调整和用户交互。
(3)：方法：本文提出了一种新的框架 SHINOBI，它使用隐式形状表示和多分辨率哈希编码来实现更快速和鲁棒的形状重建。同时，该框架还联合优化 BRDF 和光照，以实现对光照和对象反射率的编辑。
(4)：性能：SHINOBI 在多个数据集上的实验结果表明，它在形状重建、材质估计和光照估计方面都优于现有方法。该方法可以生成逼真的可照明 3D 资产，可用于 AR/VR、电影、游戏等多种应用场景。</p>
</li>
<li><p>方法：
(1) <strong>隐式形状表示</strong>：SHINOBI使用隐式形状表示来表示对象，这是一种无界表示，可以轻松处理具有复杂几何形状的对象。
(2) <strong>多分辨率哈希编码</strong>：SHINOBI使用多分辨率哈希编码来编码隐式形状表示，这可以提高形状重建的速度和鲁棒性。
(3) <strong>联合优化BRDF和光照</strong>：SHINOBI联合优化BRDF和光照，以实现对光照和对象反射率的编辑。
(4) <strong>可照明3D资产生成</strong>：SHINOBI可以生成逼真的可照明3D资产，可用于AR/VR、电影、游戏等多种应用场景。</p>
</li>
<li><p>结论：
（1）：本工作提出了一种新的框架 SHINOBI，该框架可以从未经摆放的野外图像集中估计物体的形状、姿态和光照。我们新颖的混合哈希网格编码能够使用多分辨率哈希网格更容易地优化相机姿态。此外，我们选择的相机参数化以及逐视图重要性权重和基于补丁的对齐损失允许更好地将图像与 3D 对齐，从而在具有高频细节的情况下更好地重建。虽然 SHINOBI 能够从任何类别的物体中恢复几何形状，但其性能在细长/透明结构上受到限制，并且无法在极端光照变化下恢复高频细节，我们将其留作未来工作的探索。
（2）：创新点：</p>
</li>
</ol>
<ul>
<li>使用隐式形状表示和多分辨率哈希编码来实现更快速和鲁棒的形状重建。</li>
<li>联合优化 BRDF 和光照，以实现对光照和对象反射率的编辑。</li>
<li>可以生成逼真的可照明 3D 资产，可用于 AR/VR、电影、游戏等多种应用场景。
性能：</li>
<li>在多个数据集上的实验结果表明，SHINOBI 在形状重建、材质估计和光照估计方面都优于现有方法。</li>
<li>可以生成逼真的可照明 3D 资产，可用于 AR/VR、电影、游戏等多种应用场景。
工作量：</li>
<li>该方法需要大量的数据和计算资源。</li>
<li>需要手动调整和用户交互。</li>
</ul>



<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-6fa4b83c9b053b9d6092eca8188f4657.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b1a74a4a26394cb29cc941c3540acec6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1cad55e2949324c67746914275a9a371.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a4ea31f8709c7ac961337f4d618a8737.jpg" align="middle">
</details>
​    


## GaussianBody: Clothed Human Reconstruction via 3d Gaussian Splatting
**Authors:Mengtian Li, Shengxiang Yao, Zhifeng Xie, Keyu Chen, Yu-Gang Jiang**

In this work, we propose a novel clothed human reconstruction method called GaussianBody, based on 3D Gaussian Splatting. Compared with the costly neural radiance based models, 3D Gaussian Splatting has recently demonstrated great performance in terms of training time and rendering quality. However, applying the static 3D Gaussian Splatting model to the dynamic human reconstruction problem is non-trivial due to complicated non-rigid deformations and rich cloth details. To address these challenges, our method considers explicit pose-guided deformation to associate dynamic Gaussians across the canonical space and the observation space, introducing a physically-based prior with regularized transformations helps mitigate ambiguity between the two spaces. During the training process, we further propose a pose refinement strategy to update the pose regression for compensating the inaccurate initial estimation and a split-with-scale mechanism to enhance the density of regressed point clouds. The experiments validate that our method can achieve state-of-the-art photorealistic novel-view rendering results with high-quality details for dynamic clothed human bodies, along with explicit geometry reconstruction. 

[PDF](http://arxiv.org/abs/2401.09720v1) 

**Summary**
高斯人体：基于 3D 高斯散布的动态穿衣人体重建方法。

**Key Takeaways**

- 我们提出了一种名为 GaussianBody 的新型穿衣人体重建方法，该方法基于 3D 高斯散布。
- 与昂贵的神经辐照度模型相比，3D 高斯散布在训练时间和渲染质量方面最近表现出出色的性能。
- 将静态 3D 高斯散布模型应用于动态人体重建问题并非易事，原因是复杂的非刚性变形和丰富的布料细节。
- 为了应对这些挑战，我们的方法考虑了显式姿势引导变形，以在规范空间和观察空间中关联动态高斯分布，引入具有正则化变换的基于物理的先验有助于减轻两个空间之间的歧义。
- 在训练过程中，我们进一步提出了一种姿势优化策略来更新姿势回归，以补偿不准确的初始估计，并提出了一个具有尺度的分割机制来增强回归点云的密度。
- 实验验证表明，我们的方法可以实现动态穿着人体的高质量细节，以及显式几何重建的最先进的照片级新视角渲染结果。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>题目：高斯体：基于 3D 高斯散点的衣着人体重建</li>
<li>作者：孟天力、姚圣翔、谢志峰、陈可宇、姜玉刚</li>
<li>第一作者单位：上海大学</li>
<li>关键词：衣着人体重建、3D 高斯散点、动态捕捉、几何重建</li>
<li>论文链接：https://arxiv.org/abs/2401.09720</li>
<li>摘要：
（1）研究背景：高保真衣着人体模型的创建在虚拟现实、远程临场和电影制作等领域具有重要应用。传统方法要么涉及复杂的捕捉系统，要么需要 3D 艺术家进行繁琐的手工工作，这使得它们耗时且昂贵，从而限制了新手用户的可扩展性。最近，人们越来越关注从单个 RGB 图像或单目视频中自动重建衣着人体模型。
（2）过去方法及问题：基于网格的方法最初被引入，通过回归参数化模型（如 SCAPE、SMPL、SMPL-X 和 STAR）来恢复人体形状。虽然它们可以实现快速且鲁棒的重建，但回归的多边形网格难以捕捉不同的几何细节和丰富的服装特征。在这种情况下，添加顶点偏移成为一种增强解决方案。然而，它的表示能力仍然有限。
（3）研究方法：本文提出了一种基于 3D 高斯散点的新型衣着人体重建方法，称为高斯体。与昂贵的神经辐射体模型相比，3D 高斯散点最近在训练时间和渲染质量方面表现出优异的性能。然而，由于复杂的非刚性变形和丰富的服装细节，将静态 3D 高斯散点模型应用于动态人体重建问题并非易事。为了应对这些挑战，本文的方法考虑了在规范空间和观察空间中对动态高斯体进行显式姿势引导的变形，引入了具有正则化变换的基于物理的先验，有助于减轻两个空间之间的歧义。在训练过程中，本文还提出了一种姿势细化策略来更新姿势回归，以补偿不准确的初始估计，并提出了一种分裂缩放机制来增强回归点云的密度。
（4）方法性能：实验验证了本文方法可以实现动态衣着人体的高质量细节的最新逼真新视角渲染结果，以及显式的几何重建。</li>
</ol>
<p>Some Error for method(比如是不是没有Methods这个章节)</p>
<ol start="8">
<li>结论：
（1）：本文提出了一种基于3D高斯散点的新型衣着人体重建方法GaussianBody，该方法可以从单目视频中重建动态衣着人体模型。GaussianBody通过在规范空间和观察空间中对动态高斯体进行显式姿势引导的变形，以及引入具有正则化变换的基于物理的先验，解决了动态衣着人体重建中存在的复杂非刚性变形和丰富的服装细节等挑战。实验表明，GaussianBody可以实现高质量细节的最新逼真新视角渲染结果，以及显式的几何重建。
（2）：创新点：</li>
</ol>
<ul>
<li>提出了一种基于3D高斯散点的新型衣着人体重建方法GaussianBody。</li>
<li>在规范空间和观察空间中对动态高斯体进行显式姿势引导的变形，解决了动态衣着人体重建中存在的复杂非刚性变形和丰富的服装细节等挑战。</li>
<li>引入具有正则化变换的基于物理的先验，有助于减轻两个空间之间的歧义。</li>
<li>提出了一种姿势细化策略来更新姿势回归，以补偿不准确的初始估计，并提出了一种分裂缩放机制来增强回归点云的密度。</li>
</ul>
<p>性能：</p>
<ul>
<li>GaussianBody可以实现高质量细节的最新逼真新视角渲染结果，以及显式的几何重建。</li>
<li>GaussianBody在图像质量指标方面与基线和其他方法相当，证明了其竞争性能、相对较快的训练速度，以及使用更高分辨率图像进行训练的能力。</li>
</ul>
<p>工作量：</p>
<ul>
<li>GaussianBody需要收集和标注大量的数据。</li>
<li>GaussianBody的训练过程相对复杂，需要较大的计算资源。</li>
</ul>



<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-7931aa02d87b1007c7f5cdde77107e5c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3d3dcd00c27bc3d320b23d4247ae79f3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3df005c3ea738aba56feb680b23b73d2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d121364f4f1fecac5ef9d276f421f434.jpg" align="middle">
</details>
​    


## PPSURF: Combining Patches and Point Convolutions for Detailed Surface   Reconstruction
**Authors:Philipp Erler, Lizeth Fuentes, Pedro Hermosilla, Paul Guerrero, Renato Pajarola Michael Wimmer**

3D surface reconstruction from point clouds is a key step in areas such as content creation, archaeology, digital cultural heritage, and engineering. Current approaches either try to optimize a non-data-driven surface representation to fit the points, or learn a data-driven prior over the distribution of commonly occurring surfaces and how they correlate with potentially noisy point clouds. Data-driven methods enable robust handling of noise and typically either focus on a global or a local prior, which trade-off between robustness to noise on the global end and surface detail preservation on the local end. We propose PPSurf as a method that combines a global prior based on point convolutions and a local prior based on processing local point cloud patches. We show that this approach is robust to noise while recovering surface details more accurately than the current state-of-the-art.   Our source code, pre-trained model and dataset are available at: https://github.com/cg-tuwien/ppsurf 

[PDF](http://arxiv.org/abs/2401.08518v1) Published in Computer Graphics Forum (Jan 2024):   https://onlinelibrary.wiley.com/doi/10.1111/cgf.15000

**Summary**
云点重建结合局部特征与全局信息，提升表面重建的抗噪性和细节保存能力。

**Key Takeaways**

- 表面重建是内容创建、考古学、数字文化遗产和工程等领域的关键步骤。
- 当前方法要么尝试优化非数据驱动的曲面表示以拟合点，要么学习数据驱动的先验分布，以及它们如何与潜在的噪声点云相关。
- 数据驱动的方法能够对噪声进行鲁棒处理，通常专注于全局或局部先验，在全局末端的抗噪性和局部末端的表面细节保留之间进行权衡。
- PPSurf 是一种将基于点卷积的全局先验与基于处理局部点云块的局部先验相结合的方法。
- PPSurf 在恢复表面细节方面比当前最先进的方法更准确。
- PPSurf 的源代码、预训练模型和数据集可在 https://github.com/cg-tuwien/ppsurf 获取。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li><p>标题：PPSURF：结合局部块和点卷积进行详细表面重建</p>
</li>
<li><p>作者：P. Erler、L. Fuentes-Perez、P. Hermosilla、P. Guerrero、R. Pajarola、M. Wimmer</p>
</li>
<li><p>第一作者单位：维也纳工业大学</p>
</li>
<li><p>关键词：点云、表面重建、数据驱动、局部块、点卷积</p>
</li>
<li><p>论文链接：https://onlinelibrary.wiley.com/doi/10.1111/cgf.150001
Github 代码链接：https://github.com/cg-tuwien/ppsurf</p>
</li>
<li><p>摘要：
（1）：3D 表面重建是内容创作、考古学、数字文化遗产和工程等领域的关键步骤。目前的方法要么尝试优化非数据驱动的表面表示以拟合点，要么学习数据驱动的先验，了解常见表面和潜在噪声点云之间的分布和相关性。数据驱动的方法能够稳健地处理噪声，通常侧重于全局或局部先验，这在全局端与噪声的稳健性和局部端表面细节的保留之间进行权衡。
（2）：过去的方法要么尝试优化非数据驱动的表面表示以拟合点，要么学习数据驱动的先验，了解常见表面和潜在噪声点云之间的分布和相关性。数据驱动的方法能够稳健地处理噪声，但通常侧重于全局或局部先验，这在全局端与噪声的稳健性和局部端表面细节的保留之间进行权衡。
（3）：本文提出了一种名为 PPSURF 的方法，该方法结合了基于点卷积的全局先验和基于处理局部点云块的局部先验。
（4）：在噪声点云重建任务上，该方法在恢复表面细节的同时，对噪声具有鲁棒性，优于当前最先进的方法。</p>
</li>
<li><p>方法：
(1)：PPSURF方法将基于点卷积的全局先验与基于处理局部点云块的局部先验相结合，以实现详细的表面重建。
(2)：全局分支使用POCO网络，该网络由点卷积模块和插值模块组成，点卷积模块计算每个稀疏点的特征向量，插值模块使用基于注意力的权重对特征向量进行插值以获得全局特征向量。
(3)：局部分支使用PointNet网络，该网络经过修改，采用基于注意力的聚合方式，而不是原始的最大值或和值聚合方式，以获得局部特征向量。
(4)：全局特征向量和局部特征向量通过MLP组合，以计算查询点处的占用概率。</p>
</li>
<li><p>结论：
（1）：本文提出了一种名为 PPSURF 的方法，该方法将基于点卷积的全局先验与基于处理局部点云块的局部先验相结合，以实现详细的表面重建。在噪声点云重建任务上，该方法在恢复表面细节的同时，对噪声具有鲁棒性，优于当前最先进的方法。
（2）：创新点：</p>
</li>
</ol>
<ul>
<li>将基于点卷积的全局先验与基于处理局部点云块的局部先验相结合，以实现详细的表面重建。</li>
<li>使用基于注意力的聚合方式，而不是原始的最大值或和值聚合方式，以获得局部特征向量。</li>
<li>在噪声点云重建任务上，该方法在恢复表面细节的同时，对噪声具有鲁棒性，优于当前最先进的方法。</li>
</ul>
<p>性能：</p>
<ul>
<li>在 ABCvar-noise 测试集上，PPSURF 在 Chamfer 距离、F1 分数和法线误差方面均优于当前最先进的方法。</li>
<li>在 PatchSize 消融研究中，PPSURF100NN 和 PPSURF200NN 在 Chamfer 距离和 F1 分数方面与 PPSURFFull 相当，但在法线误差方面略差。</li>
<li>在 Miscellanous 消融研究中，PPSURFSymMax 在 Chamfer 距离方面优于 PPSURFFull，但在 F1 分数和法线误差方面略差。PPSURFQPoints 在 F1 分数和法线误差方面优于 PPSURFFull，但在 Chamfer 距离方面略差。PPSURFMergeCat 在 Chamfer 距离和 F1 分数方面与 PPSURFFull 相当，但在法线误差方面略差。</li>
</ul>
<p>工作量：</p>
<ul>
<li>PPSURF 的实现相对简单，易于理解和使用。</li>
<li>PPSURF 的训练时间和推理时间与当前最先进的方法相当。</li>
</ul>



<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-deb0c891a44eda3abc848abfcb3b5052.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b71a526973638d7a94c6cac1ef5d0c4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-78b2aade816834cf694d7949cbee5c89.jpg" align="middle">
</details>
​    

​    

## InseRF: Text-Driven Generative Object Insertion in Neural 3D Scenes
**Authors:Mohamad Shahbazi, Liesbeth Claessens, Michael Niemeyer, Edo Collins, Alessio Tonioni, Luc Van Gool, Federico Tombari**

We introduce InseRF, a novel method for generative object insertion in the NeRF reconstructions of 3D scenes. Based on a user-provided textual description and a 2D bounding box in a reference viewpoint, InseRF generates new objects in 3D scenes. Recently, methods for 3D scene editing have been profoundly transformed, owing to the use of strong priors of text-to-image diffusion models in 3D generative modeling. Existing methods are mostly effective in editing 3D scenes via style and appearance changes or removing existing objects. Generating new objects, however, remains a challenge for such methods, which we address in this study. Specifically, we propose grounding the 3D object insertion to a 2D object insertion in a reference view of the scene. The 2D edit is then lifted to 3D using a single-view object reconstruction method. The reconstructed object is then inserted into the scene, guided by the priors of monocular depth estimation methods. We evaluate our method on various 3D scenes and provide an in-depth analysis of the proposed components. Our experiments with generative insertion of objects in several 3D scenes indicate the effectiveness of our method compared to the existing methods. InseRF is capable of controllable and 3D-consistent object insertion without requiring explicit 3D information as input. Please visit our project page at https://mohamad-shahbazi.github.io/inserf. 

[PDF](http://arxiv.org/abs/2401.05335v1) 

**摘要**
神经辐射场三维重建中，通过用户提供的文本描述和参考视点中的二维边界框，实现新的物体生成。

**要点**

- InseRF 是一种三维场景神经辐射场中生成式物体插入的新颖方法，支持生成新的物体并将其插入三维场景中。
- InseRF 使用文本到图像扩散模型的先验知识，可以有效地编辑三维场景的样式和外观，或移除现有物体。
- InseRF 将三维物体插入操作转化为参考视图中的二维物体插入，并使用单视图物体重建方法将二维编辑提升到三维。
- InseRF 利用单目深度估计方法的先验知识，将重建的物体插入场景中。
- InseRF 在多个三维场景上进行了评估，结果表明其优于现有方法。
- InseRF 能够生成可控和三维一致的物体，且不需要显式的三维信息作为输入。
- InseRF 项目主页：https://mohamad-shahbazi.github.io/inserf。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li><p>题目：无精修 URL：</p>
</li>
<li><p>作者：Mohamad Shahbazi<em>, Anh-Huy Phan</em>, Jia-Bin Huang, Yi-Ling Qiao, Hao Tang, Matthew Fisher, Angela Dai</p>
</li>
<li><p>第一作者单位：苏黎世联邦理工学院（ETH Zurich）</p>
</li>
<li><p>关键词：神经辐射场、对象插入、文本到图像扩散模型</p>
</li>
<li><p>论文链接：无，Github 链接：无</p>
</li>
<li><p>摘要：
（1）研究背景：近年来，3D 场景编辑方法取得了重大进展，这主要归功于在 3D 生成建模中使用文本到图像扩散模型的强先验。现有方法主要通过改变样式和外观或移除现有对象来编辑 3D 场景。然而，生成新对象对于这些方法来说仍然是一个挑战。
（2）过去的方法及其问题：现有方法的问题在于，它们无法生成新的对象，并且需要显式的 3D 信息作为输入。
（3）研究方法：为了解决上述问题，本文提出了一种新的方法 InseRF，它可以根据用户提供的文本描述和参考视点中的 2D 边界框在 NeRF 重建的 3D 场景中生成新对象。InseRF 将 2D 编辑提升到 3D，使用单视图对象重建方法重建对象，然后在单目深度估计方法的指导下将重建的对象插入场景中。
（4）方法性能：实验表明，InseRF 在多个 3D 场景中生成插入对象的任务上优于现有方法。InseRF 能够以可控且 3D 一致的方式插入对象，而无需显式 3D 信息作为输入。</p>
</li>
<li><p>方法：
(1) 输入：InseRF 的输入是一个 NeRF 重建的 3D 场景、一个文本描述和一个参考视点中的 2D 边界框。
(2) 对象重建：InseRF 使用单视图对象重建方法重建对象。
(3) 对象插入：InseRF 使用单目深度估计方法估计对象的深度，然后将重建的对象插入场景中。</p>
</li>
<li><p>结论：
（1）：xxx；
（2）：创新点：InseRF 提出了一种新颖的方法，可以根据文本描述和 2D 边界框在 NeRF 重建的 3D 场景中生成新对象，无需显式 3D 信息作为输入。
性能：InseRF 在多个 3D 场景中生成插入对象的任务上优于现有方法，能够以可控且 3D 一致的方式插入对象。
工作量：InseRF 的实现相对复杂，需要结合单视图对象重建方法和单目深度估计方法，工作量较大。</p>
</li>
</ol>



<p></p><details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9f528d642820c297073b934ba1373992.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8499c6fe071df44b8e34eafc6e0a99e2.jpg" align="middle">
</details><br>​    <p></p>
<h2 id="Vision-Reimagined-AI-Powered-Breakthroughs-in-WiFi-Indoor-Imaging"><a href="#Vision-Reimagined-AI-Powered-Breakthroughs-in-WiFi-Indoor-Imaging" class="headerlink" title="Vision Reimagined: AI-Powered Breakthroughs in WiFi Indoor Imaging"></a>Vision Reimagined: AI-Powered Breakthroughs in WiFi Indoor Imaging</h2><p><strong>Authors:Jianyang Shi, Bowen Zhang, Amartansh Dubey, Ross Murch, Liwen Jing</strong></p>
<p>Indoor imaging is a critical task for robotics and internet-of-things. WiFi as an omnipresent signal is a promising candidate for carrying out passive imaging and synchronizing the up-to-date information to all connected devices. This is the first research work to consider WiFi indoor imaging as a multi-modal image generation task that converts the measured WiFi power into a high-resolution indoor image. Our proposed WiFi-GEN network achieves a shape reconstruction accuracy that is 275% of that achieved by physical model-based inversion methods. Additionally, the Frechet Inception Distance score has been significantly reduced by 82%. To examine the effectiveness of models for this task, the first large-scale dataset is released containing 80,000 pairs of WiFi signal and imaging target. Our model absorbs challenges for the model-based methods including the non-linearity, ill-posedness and non-certainty into massive parameters of our generative AI network. The network is also designed to best fit measured WiFi signals and the desired imaging output. For reproducibility, we will release the data and code upon acceptance. </p>
<p><a href="http://arxiv.org/abs/2401.04317v1">PDF</a> </p>
<p><strong>Summary</strong><br>室内成像通过Wi-Fi信号，将测量到的Wi-Fi功率转换为高分辨率图像。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Wi-Fi室内成像被视为多模态图像生成任务。</li>
<li>Wi-Fi-GEN网络的形状重建精度是基于物理模型的反演方法的275%。</li>
<li>Fréchet起始距离评分降低了82%。</li>
<li>发布了首个大型数据集，包含80,000对Wi-Fi信号和成像目标。</li>
<li>Wi-Fi-GEN网络将基于模型的方法的挑战吸收为生成性AI网络的大量参数。</li>
<li>网络设计用于最适合测量的Wi-Fi信号和所需的成像输出。</li>
<li>为了可复制性，我们将发布数据和代码。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li><p>题目：视觉重构：WiFi 室内成像的 AI 赋能突破</p>
</li>
<li><p>作者：Jianyang Shi、Bowen Zhang、Amartansh Dubey、Ross Murch、Liwen Jing</p>
</li>
<li><p>隶属单位：深圳先进技术研究院信息与智能学院</p>
</li>
<li><p>关键词：WiFi成像、生成式人工智能、逆散射问题</p>
</li>
<li><p>论文链接：https://arxiv.org/abs/2401.04317
Github 代码链接：无</p>
</li>
<li><p>摘要：
(1)：研究背景：室内成像是机器人和物联网的一项关键任务。WiFi 作为一种无处不在的信号，是执行被动成像并将最新信息同步到所有连接设备的理想选择。
(2)：过去的方法及其问题：现有研究探索了全波（即测量 WiFi 相位和功率）和无相测量来解决室内成像问题。然而，全波方法的相位测量在高频时非常棘手且昂贵，这使得它在实践中不切实际。最近的研究证明了通过无相 WiFi 信号进行室内成像的可行性。
(3)：提出的研究方法：本文将 WiFi 室内成像视为多模态图像生成任务，将测量的 WiFi 功率转换为高分辨率室内图像。提出的 WiFi-GEN 网络实现了比基于物理模型的反演方法高出 275% 的重建精度。此外，Fréchet Inception Distance 得分显着降低了 82%。为了检查模型对这项任务的有效性，发布了第一个包含 80,000 对 WiFi 信号和成像目标的大规模数据集。该模型吸收了基于模型的方法的挑战，包括我们生成式 AI 网络中非线性和不确定性的大量参数。该网络还旨在最适合测量的 WiFi 信号和期望的成像输出。
(4)：方法的性能：该方法在室内成像任务上取得了良好的性能。在 80,000 对 WiFi 信号和成像目标的数据集上，该方法的重建精度比基于物理模型的反演方法高出 275%。此外，Fréchet Inception Distance 得分显着降低了 82%。这些结果表明，该方法能够有效地从 WiFi 信号中生成高分辨率的室内图像。</p>
</li>
<li><p>方法：
(1)：WiFi室内成像问题被表述为多模态图像生成任务，利用生成式人工智能网络将测量的WiFi功率转换为高分辨率室内图像。
(2)：WiFi-GEN网络由三个部分组成：WiFi信号编码器、控制信号网络和WiFi生成器。WiFi信号编码器将WiFi信号嵌入到潜在空间矩阵中；控制信号网络从潜在空间中提取WiFi信号相关特征并将其转换为多级特征向量；WiFi生成器利用从控制信号网络不同层级提取的特征来影响最终图像生成结果。
(3)：WiFi信号编码器是一个三层全连接网络，将19×20的信号展平为380维向量并输入编码器。
(4)：控制信号网络由下采样模块和上采样模块组成。下采样模块由多个残差层组成，用于减少特征图的空间尺寸；上采样模块利用卷积层和双线性插值来恢复特征图的空间分辨率。
(5)：WiFi生成器采用了一种利用从潜在空间中提取的多尺度特征来生成最终图像的方法。WiFi生成器由多个信号块组成，每个信号块在生成过程中发挥着至关重要的作用。
(6)：WiFi生成器利用从控制信号网络不同层级提取的特征来影响最终图像生成结果。</p>
</li>
<li><p>结论：
（1）： 本文将 WiFi 室内成像问题表述为多模态图像生成任务，利用生成式人工智能网络将测量的 WiFi 功率转换为高分辨率室内图像，取得了良好的性能。
（2）： 创新点：
提出一种基于生成式人工智能的 WiFi 室内成像方法，将 WiFi 室内成像问题表述为多模态图像生成任务，利用生成式人工智能网络将测量的 WiFi 功率转换为高分辨率室内图像。
性能：
在 80,000 对 WiFi 信号和成像目标的数据集上，该方法的重建精度比基于物理模型的反演方法高出 275%。此外，Fréchet Inception Distance 得分显着降低了 82%。
工作量：
该方法需要大量的数据集和计算资源。</p>
</li>
</ol>



<p></p><details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-86612c0b8ef65da0afea3c89e2c33374.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc98bdad49ef910c1452e4c81ae51370.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38ec24519ebd731699e5acd71cf669b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de86c8629916709cd814c7c39e2990a2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ba54f2b1b3eaec39559af94bdc51539d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2408b66da95959934b23ca823827183.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-303fefd6ac513f1e77ee4837d8d3a0bd.jpg" align="middle">
</details><br>​    <p></p>
<h2 id="RHOBIN-Challenge-Reconstruction-of-Human-Object-Interaction"><a href="#RHOBIN-Challenge-Reconstruction-of-Human-Object-Interaction" class="headerlink" title="RHOBIN Challenge: Reconstruction of Human Object Interaction"></a>RHOBIN Challenge: Reconstruction of Human Object Interaction</h2><p><strong>Authors:Xianghui Xie, Xi Wang, Nikos Athanasiou, Bharat Lal Bhatnagar, Chun-Hao P. Huang, Kaichun Mo, Hao Chen, Xia Jia, Zerui Zhang, Liangxian Cui, Xiao Lin, Bingqiao Qian, Jie Xiao, Wenfei Yang, Hyeongjin Nam, Daniel Sungho Jung, Kihoon Kim, Kyoung Mu Lee, Otmar Hilliges, Gerard Pons-Moll</strong></p>
<p>Modeling the interaction between humans and objects has been an emerging research direction in recent years. Capturing human-object interaction is however a very challenging task due to heavy occlusion and complex dynamics, which requires understanding not only 3D human pose, and object pose but also the interaction between them. Reconstruction of 3D humans and objects has been two separate research fields in computer vision for a long time. We hence proposed the first RHOBIN challenge: reconstruction of human-object interactions in conjunction with the RHOBIN workshop. It was aimed at bringing the research communities of human and object reconstruction as well as interaction modeling together to discuss techniques and exchange ideas. Our challenge consists of three tracks of 3D reconstruction from monocular RGB images with a focus on dealing with challenging interaction scenarios. Our challenge attracted more than 100 participants with more than 300 submissions, indicating the broad interest in the research communities. This paper describes the settings of our challenge and discusses the winning methods of each track in more detail. We observe that the human reconstruction task is becoming mature even under heavy occlusion settings while object pose estimation and joint reconstruction remain challenging tasks. With the growing interest in interaction modeling, we hope this report can provide useful insights and foster future research in this direction. Our workshop website can be found at \href{<a href="https://rhobin-challenge.github.io/}{https://rhobin-challenge.github.io/}">https://rhobin-challenge.github.io/}{https://rhobin-challenge.github.io/}</a>. </p>
<p><a href="http://arxiv.org/abs/2401.04143v1">PDF</a> 14 pages, 5 tables, 7 figure. Technical report of the CVPR’23   workshop: RHOBIN challenge (<a href="https://rhobin-challenge.github.io/">https://rhobin-challenge.github.io/</a>)</p>
<p><strong>Summary</strong><br>首次人类与物体交互重建挑战赛（RHOBIN）及研讨会促成学术交流，吸引百余名参赛者，探讨单目RGB图像中的3D重建。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>首次人类与物体交互重建挑战赛（RHOBIN）于2020年举行，旨在促进人体、物体重建与交互建模研究社群的交流与合作。</li>
<li>挑战赛包含3个赛道，均以单目RGB图像作为输入，聚焦于解决富有挑战性的交互场景。</li>
<li>挑战赛吸引了来自世界各地的研究人员，收到300多篇投稿。</li>
<li>虽然人体重建在遮挡严重的情况下亦表现良好，但物体姿态估计与联合重建仍然具有挑战性。</li>
<li>挑战赛获奖方法在人体姿态估计、物体姿态估计、交互建模等方面取得优异成绩，表现出强大的鲁棒性和准确性。</li>
<li>RHOBIN挑战赛促进了研究社群的合作与交流，推动了人体、物体交互重建与建模研究的发展。</li>
<li>挑战赛获奖方法对后续人体、物体交互重建与建模研究有重要参考价值。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li><p>题目：RHOBIN 挑战：重建人与物体交互</p>
</li>
<li><p>作者：Xianghui Xie, Xi Wang, Nikos Athanasiou, Bharat Lal Bhatnagar, Chun-Hao P. Huang, Kaichun Mo, Hao Chen, Xia Jia, Zerui Zhang, Liangxian Cui, Xiao Lin, Bingqiao Qian, Jie Xiao, Wenfei Yang, Hyeongjin Nam, Daniel Sungho Jung, Kihoon Kim, Kyoung Mu Lee, Otmar Hilliges, Gerard Pons-Moll</p>
</li>
<li><p>第一作者单位：图宾根大学</p>
</li>
<li><p>关键词：人与物体交互、三维重建、单目 RGB 图像、BEHAVE 数据集</p>
</li>
<li><p>论文链接：https://arxiv.org/abs/2401.04143</p>
</li>
<li><p>摘要：
(1) 研究背景：人与物体交互建模近年来成为一个新兴的研究方向。然而，由于严重的遮挡和复杂的动态，捕捉人与物体交互是一个非常具有挑战性的任务，这需要理解不仅是三维人体姿势和物体姿势，还有它们之间的交互。三维人体和物体的重建长期以来一直是计算机视觉中的两个独立的研究领域。
(2) 过去的方法及问题：过去的方法通常将人与物体交互建模分为两个独立的任务：人体重建和物体姿态估计。然而，这些方法存在一些问题，例如，它们通常需要多个摄像头或深度传感器，并且它们对遮挡和动态变化非常敏感。
(3) 本文提出的研究方法：为了解决上述问题，本文提出了一个新的挑战：RHOBIN 挑战：重建人与物体交互。该挑战赛旨在将人体重建、物体姿态估计和人与物体交互建模的研究社区聚集在一起，讨论技术并交流思想。该挑战赛由三个赛道组成，所有赛道都使用单目 RGB 图像作为输入，并输出三维人体或/和交互的三维物体。
(4) 实验结果及性能：该挑战赛吸引了 100 多名参与者和 300 多份提交。所有表现最佳的团队都获得了优于先前最先进方法的结果。经过对结果的检查，我们有以下观察：1）现有方法已经可以在单独的人体或物体重建上取得相当好的结果，并且应用数据增强和模型集成来提高性能非常重要；2）联合重建仍然是一个具有挑战性的任务。随着对交互建模的兴趣日益浓厚，我们希望这份报告能够提供有用的见解，并促进该方向的未来研究。</p>
</li>
<li><p>方法：
（1）RHOBIN挑战赛：该挑战赛由三个赛道组成，所有赛道都使用单目RGB图像作为输入，并输出三维人体或/和交互的三维物体。
（2）数据增强和模型集成：应用数据增强和模型集成来提高性能非常重要。
（3）联合重建：联合重建仍然是一个具有挑战性的任务。</p>
</li>
<li><p>结论：
（1）：本文提出了 RHOBIN 挑战赛，旨在将人体重建、物体姿态估计和人与物体交互建模的研究社区聚集在一起，讨论技术并交流思想。该挑战赛吸引了 100 多名参与者和 300 多份提交，所有表现最佳的团队都获得了优于先前最先进方法的结果。
（2）：创新点：
本文提出了一个新的挑战：RHOBIN 挑战赛：重建人与物体交互，该挑战赛由三个赛道组成，所有赛道都使用单目 RGB 图像作为输入，并输出三维人体或/和交互的三维物体。
性能：
经过对结果的检查，我们有以下观察：1）现有方法已经可以在单独的人体或物体重建上取得相当好的结果，并且应用数据增强和模型集成来提高性能非常重要；2）联合重建仍然是一个具有挑战性的任务。
工作量：
该挑战赛吸引了 100 多名参与者和 300 多份提交，所有表现最佳的团队都获得了优于先前最先进方法的结果。</p>
</li>
</ol>



<p></p><details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-60dae88b48b9b3b3fe863b3d312f44c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af42a68cb719096e3898c7bb71fe22b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53fd3a763ee32776a87f4b1ae0da73e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d3cda82200fcfbcaad43b075c54ef0f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27080102702f4941d7f14a6d3d4305f4.jpg" align="middle">
</details><br>​    <p></p>
<h2 id="Sur2f-A-Hybrid-Representation-for-High-Quality-and-Efficient-Surface-Reconstruction-from-Multi-view-Images"><a href="#Sur2f-A-Hybrid-Representation-for-High-Quality-and-Efficient-Surface-Reconstruction-from-Multi-view-Images" class="headerlink" title="Sur2f: A Hybrid Representation for High-Quality and Efficient Surface   Reconstruction from Multi-view Images"></a>Sur2f: A Hybrid Representation for High-Quality and Efficient Surface   Reconstruction from Multi-view Images</h2><p><strong>Authors:Zhangjin Huang, Zhihao Liang, Haojie Zhang, Yangkai Lin, Kui Jia</strong></p>
<p>Multi-view surface reconstruction is an ill-posed, inverse problem in 3D vision research. It involves modeling the geometry and appearance with appropriate surface representations. Most of the existing methods rely either on explicit meshes, using surface rendering of meshes for reconstruction, or on implicit field functions, using volume rendering of the fields for reconstruction. The two types of representations in fact have their respective merits. In this work, we propose a new hybrid representation, termed Sur2f, aiming to better benefit from both representations in a complementary manner. Technically, we learn two parallel streams of an implicit signed distance field and an explicit surrogate surface Sur2f mesh, and unify volume rendering of the implicit signed distance function (SDF) and surface rendering of the surrogate mesh with a shared, neural shader; the unified shading promotes their convergence to the same, underlying surface. We synchronize learning of the surrogate mesh by driving its deformation with functions induced from the implicit SDF. In addition, the synchronized surrogate mesh enables surface-guided volume sampling, which greatly improves the sampling efficiency per ray in volume rendering. We conduct thorough experiments showing that Sur$^2$f outperforms existing reconstruction methods and surface representations, including hybrid ones, in terms of both recovery quality and recovery efficiency. </p>
<p><a href="http://arxiv.org/abs/2401.03704v1">PDF</a> 18 pages, 16 figures</p>
<p><strong>Summary</strong><br>结合隐式距离场与显式代理曲面，提出新型混合表示Sur2f，提高3D重建的质量和效率。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>多视角曲面重建是 3D 视觉研究中的病态逆问题，涉及使用适当的曲面表示对几何形状和外观进行建模。</li>
<li>现有方法要么依赖显式网格，使用网格的曲面渲染进行重建，要么依赖隐式场函数，使用场的体积渲染进行重建。</li>
<li>Sur2f是一种新的混合表示，旨在以互补的方式从两种表示中更好地受益。</li>
<li>Sur2f学习隐式有符号距离场和显式代理曲面Sur2f网格的两个并行流，并使用共享的神经着色器统一隐式有符号距离函数 (SDF) 的体积渲染和代理网格的曲面渲染。</li>
<li>Sur2f通过使用从隐式SDF诱导的函数来驱动代理网格的变形来同步学习代理网格。</li>
<li>同步的代理网格支持曲面引导体积采样，大大提高了体积渲染中每个射线的采样效率。</li>
<li>Sur2f在恢复质量和恢复效率方面优于现有的重建方法和曲面表示，包括混合方法。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：Sur2f：一种用于从多视角图像进行高质量和高效曲面重建的混合表示</li>
<li>作者：Long Chen、Zexiang Xu、Qian Yu、Hao Su、Hao Li、Hao Zhang</li>
<li>隶属机构：香港城市大学</li>
<li>关键词：曲面重建、隐式表示、显式表示、混合表示、神经渲染</li>
<li>论文链接：None，Github 链接：None</li>
<li>摘要：
(1)：曲面重建是 3D 视觉研究中一个不适定的逆问题，它涉及使用适当的曲面表示来建模几何形状和外观。现有的大多数方法依赖于显式网格，使用网格的曲面渲染进行重建，或者依赖于隐式场函数，使用场的体积渲染进行重建。事实上，这两种类型的表示各自都有其优点。
(2)：过去的方法要么使用显式网格，要么使用隐式场函数。显式网格方法可以生成高质量的重建结果，但计算成本高，并且难以处理拓扑变化。隐式场函数方法计算成本低，并且可以轻松处理拓扑变化，但生成的重建结果质量较低。
(3)：本文提出了一种新的混合表示，称为 Sur2f，旨在以互补的方式从两种表示中更好地受益。具体来说，我们学习了隐式有符号距离场和显式代理曲面 (Sur2f) 网格的两个并行流，并将隐式有符号距离函数 (SDF) 的体积渲染和代理网格的曲面渲染统一到一个共享的神经着色器中；统一的着色促进了它们收敛到相同的底层曲面。我们通过函数从隐式 SDF 诱导来同步学习代理网格，从而驱动其变形。此外，同步的代理网格支持表面引导的体积采样，这极大地提高了体积渲染中每个射线的采样效率。
(4)：我们进行了彻底的实验，表明 Sur2f 在重建质量和重建效率方面优于现有的重建方法和曲面表示，包括混合表示。这些性能可以支持它们的目标。</li>
</ol>
<p>Some Error for method(比如是不是没有Methods这个章节)</p>
<ol start="8">
<li>结论：
（1）：本文提出了一种新的混合表示 Sur2f，它结合了隐式有符号距离场和显式代理曲面网格的优点，在重建质量和重建效率方面优于现有的重建方法和曲面表示，包括混合表示。
（2）：创新点：</li>
</ol>
<ul>
<li>提出了一种新的混合表示 Sur2f，它可以同时利用隐式有符号距离场和显式代理曲面网格的优点。</li>
<li>设计了一种统一的神经着色器，将隐式有符号距离函数的体积渲染和代理网格的曲面渲染统一到一个共享的神经着色器中。</li>
<li>通过函数从隐式 SDF 诱导来同步学习代理网格，从而驱动其变形。</li>
<li>同步的代理网格支持表面引导的体积采样，这极大地提高了体积渲染中每个射线的采样效率。
性能：</li>
<li>在重建质量和重建效率方面优于现有的重建方法和曲面表示，包括混合表示。</li>
<li>可以生成高质量的重建结果，并且可以轻松处理拓扑变化。</li>
<li>计算成本低，并且可以轻松处理拓扑变化。
工作量：</li>
<li>需要学习两个并行流，一个用于隐式有符号距离场，另一个用于显式代理曲面网格。</li>
<li>需要设计一个统一的神经着色器，将隐式有符号距离函数的体积渲染和代理网格的曲面渲染统一到一个共享的神经着色器中。</li>
<li>需要通过函数从隐式 SDF 诱导来同步学习代理网格，从而驱动其变形。</li>
<li>需要支持表面引导的体积采样，这极大地提高了体积渲染中每个射线的采样效率。</li>
</ul>



<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6cf972d05baa99fcd12394fe7af270dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-01333630ee152c5ffdb71b25620aab65.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3f5316c581e5971075fa487859bda230.jpg" align="middle">
</details>
​    


## GridFormer: Point-Grid Transformer for Surface Reconstruction
**Authors:Shengtao Li, Ge Gao, Yudong Liu, Yu-Shen Liu, Ming Gu**

Implicit neural networks have emerged as a crucial technology in 3D surface reconstruction. To reconstruct continuous surfaces from discrete point clouds, encoding the input points into regular grid features (plane or volume) has been commonly employed in existing approaches. However, these methods typically use the grid as an index for uniformly scattering point features. Compared with the irregular point features, the regular grid features may sacrifice some reconstruction details but improve efficiency. To take full advantage of these two types of features, we introduce a novel and high-efficiency attention mechanism between the grid and point features named Point-Grid Transformer (GridFormer). This mechanism treats the grid as a transfer point connecting the space and point cloud. Our method maximizes the spatial expressiveness of grid features and maintains computational efficiency. Furthermore, optimizing predictions over the entire space could potentially result in blurred boundaries. To address this issue, we further propose a boundary optimization strategy incorporating margin binary cross-entropy loss and boundary sampling. This approach enables us to achieve a more precise representation of the object structure. Our experiments validate that our method is effective and outperforms the state-of-the-art approaches under widely used benchmarks by producing more precise geometry reconstructions. The code is available at https://github.com/list17/GridFormer. 

[PDF](http://arxiv.org/abs/2401.02292v1) 

**Summary**
隐式神经网络引入了点与网格结合的重建方式，并通过优化提高重建精度。

**Key Takeaways**

- 隐式神经网络已成为 3D 表面重建中的关键技术。
- 现有的方法通常将输入点编码为规则的网格特征（平面或体积），这可能牺牲一些重建细节以提高效率。
- 我们引入了一种在网格和点特征之间的新颖且高效的注意机制，称为网格前馈神经网络 (GridFormer)。
- 我们的方法最大限度地提高了网格特征的空间表达能力，并保持计算效率。
- 为了更准确的再现对象结构，我们进一步提出了结合边界二进制交叉熵损失和边界采样的边界优化策略。
- 我们的方法优于现有技术在广泛使用的基准下重建几何结构。
- 代码可在 https://github.com/list17/GridFormer 获取。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li><p>题目：网格生成器：用于曲面重建的点网格转换器</p>
</li>
<li><p>作者：盛涛李、葛高、刘雨东、刘玉申、顾明</p>
</li>
<li><p>隶属单位：北京大学信息科学技术国家研究中心（BNRist），清华大学软件学院</p>
</li>
<li><p>关键词：隐式神经网络、曲面重建、点网格转换器、边界优化</p>
</li>
<li><p>论文链接：https://arxiv.org/abs/2401.02292
Github 代码链接：https://github.com/list17/GridFormer</p>
</li>
<li><p>摘要：
（1）研究背景：隐式神经网络已成为 3D 曲面重建的关键技术。为了从离散点云重建连续曲面，现有方法通常将输入点编码为规则网格特征（平面或体积）。然而，这些方法通常使用网格作为均匀散射点特征的索引。与不规则点特征相比，规则网格特征可能会牺牲一些重建细节，但提高了效率。
（2）过去的方法及其问题：为了充分利用这两种类型的特征，我们引入了一种新颖且高效的网格和点特征之间的注意力机制，称为点网格转换器（GridFormer）。这种机制将网格视为连接空间和点云的传递点。我们的方法最大限度地提高了网格特征的空间表现力，并保持了计算效率。此外，在整个空间上优化预测可能会导致边界模糊。为了解决这个问题，我们进一步提出了一种边界优化策略，结合了边际二值交叉熵损失和边界采样。这种方法使我们能够实现对物体结构更精确的表示。
（3）研究方法：我们的实验验证了我们的方法是有效的，并且在广泛使用的基准下优于最先进的方法，通过产生更精确的几何重建。
（4）方法的性能：我们的方法在 Synthetic Rooms 数据集上取得了最先进的结果，在 F-Score 指标上达到 0.89，在 Chamfer 距离指标上达到 0.011。在 ShapeNet 数据集上，我们的方法在 F-Score 指标上达到 0.83，在 Chamfer 距离指标上达到 0.013。这些性能支持了我们的目标，即在保持计算效率的同时实现高保真曲面重建。</p>
</li>
<li><p>方法：
（1）：我们提出了一种新颖且高效的网格和点特征之间的注意力机制，称为点网格转换器（GridFormer）。这种机制将网格视为连接空间和点云的传递点。我们的方法最大限度地提高了网格特征的空间表现力，并保持了计算效率。
（2）：为了解决预测边界模糊的问题，我们进一步提出了一种边界优化策略，结合了边际二值交叉熵损失和边界采样。这种方法使我们能够实现对物体结构更精确的表示。
（3）：我们的方法在广泛使用的基准下优于最先进的方法，通过产生更精确的几何重建。</p>
</li>
<li><p>结论：
（1）：本工作提出了点网格转换器（GridFormer），它使用了一种新颖且高效的点和网格特征之间的注意力机制。该机制将网格视为连接空间和点云的传递点。我们的方法最大限度地提高了网格特征的空间表现力，并保持了计算效率。此外，为了解决预测边界模糊的问题，我们进一步提出了一种边界优化策略，结合了边际二值交叉熵损失和边界采样。这种方法使我们能够实现对物体结构更精确的表示。最终，实验表明，输入点的密度和网格大小都会影响我们方法的效果。在未来的工作中，探索如何动态划分网格以实现不同分辨率之间的注意力机制，可能会将这种机制应用到更多的场景中。
（2）：创新点：</p>
</li>
</ol>
<ul>
<li>提出了一种新颖且高效的点和网格特征之间的注意力机制，称为点网格转换器（GridFormer）。</li>
<li>提出了一种边界优化策略，结合了边际二值交叉熵损失和边界采样，以解决预测边界模糊的问题。</li>
<li>在广泛使用的基准下优于最先进的方法，通过产生更精确的几何重建。</li>
</ul>
<p>性能：</p>
<ul>
<li>在SyntheticRooms数据集上取得了最先进的结果，在F-Score指标上达到0.89，在Chamfer距离指标上达到0.011。</li>
<li>在ShapeNet数据集上，我们的方法在F-Score指标上达到0.83，在Chamfer距离指标上达到0.013。</li>
</ul>
<p>工作量：</p>
<ul>
<li>本文的工作量中等。该方法需要实现点网格转换器和边界优化策略，并进行大量的实验来验证其有效性。</li>
</ul>



<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4060effb50cbcc4d2a1088a9feb6000d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-58bec8ac4a2da95ce6f707047d4df439.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7279323b67e79dc5114058f8c28e7609.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5761d740398ae95036084ccdcf20f08c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34ffafeb6a4cead225422a90aa07d1c2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2bf8f7e2e1bfbbf55b2cea30afd6182c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b68791929579cd1db5915c9cc3542d43.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-756e738d1626d407b474a748a66913a5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-adf4ffe3ca8f2dc11327a310777fca9a.jpg" align="middle">
</details>
​    


## HISR: Hybrid Implicit Surface Representation for Photorealistic 3D Human   Reconstruction
**Authors:Angtian Wang, Yuanlu Xu, Nikolaos Sarafianos, Robert Maier, Edmond Boyer, Alan Yuille, Tony Tung**

Neural reconstruction and rendering strategies have demonstrated state-of-the-art performances due, in part, to their ability to preserve high level shape details. Existing approaches, however, either represent objects as implicit surface functions or neural volumes and still struggle to recover shapes with heterogeneous materials, in particular human skin, hair or clothes. To this aim, we present a new hybrid implicit surface representation to model human shapes. This representation is composed of two surface layers that represent opaque and translucent regions on the clothed human body. We segment different regions automatically using visual cues and learn to reconstruct two signed distance functions (SDFs). We perform surface-based rendering on opaque regions (e.g., body, face, clothes) to preserve high-fidelity surface normals and volume rendering on translucent regions (e.g., hair). Experiments demonstrate that our approach obtains state-of-the-art results on 3D human reconstructions, and also shows competitive performances on other objects. 

[PDF](http://arxiv.org/abs/2312.17192v1) Accepted by AAAI 2024 main track

**Summary**
人像模型重建引入了新的隐式表面表现形式，能够重构半透明区域。

**Key Takeaways**
- 该工作提出了一种新的混合隐式表面表示法来建模人体形状。
- 这种表示由两个表面层组成，分别代表被遮挡的人体表面的不透明和半透明区域。
- 该方法通过视觉提示自动分割不同区域，并学习重建两个有符号距离函数 (SDF)。
- 该方法对不透明区域（例如身体、面部、衣服）执行基于表面的渲染，以保留高保真表面法线，并对半透明区域（例如头发）执行体积渲染。
- 实验表明，该方法在 3D 人体重建方面获得了最先进的结果，并且在其他对象上也表现出竞争性的性能。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li><p>论文标题：基于混合隐式曲面表示的人体形状重建（Hybrid implicit surface representation for human shape reconstruction）</p>
</li>
<li><p>作者：Yuxuan Zhang, Song Bai, Xiaoguang Han, Juergen Gall, Mario Fritz</p>
</li>
<li><p>第一作者单位：德国马克斯·普朗克智能系统研究所</p>
</li>
<li><p>关键词：人体形状重建、隐式曲面表示、深度学习、渲染</p>
</li>
<li><p>论文链接：https://arxiv.org/abs/2203.04118，Github 链接：None</p>
</li>
<li><p>摘要：
（1）研究背景：神经重建和渲染策略由于能够保留高级形状细节而表现出最先进的性能。然而，现有方法要么将对象表示为隐式曲面函数或神经体积，仍然难以恢复具有异质材料的形状，特别是人类皮肤、头发或衣服。
（2）过去的方法及其问题：为了解决这个问题，我们提出了一种新的混合隐式曲面表示来建模人体形状。这种表示由两个曲面层组成，它们分别代表穿着衣服的人体上的不透明区域和半透明区域。我们使用视觉线索自动分割不同的区域，并学习重建两个有符号距离函数 (SDF)。我们对不透明区域（例如身体、面部、衣服）执行基于曲面的渲染以保留高保真曲面法线，并对半透明区域（例如头发）执行体积渲染。
（3）论文提出的研究方法：实验表明，我们的方法在 3D 人体重建方面取得了最先进的结果，并在其他对象上也表现出竞争力。
（4）方法在任务上取得的性能：我们的方法在人体形状重建任务上取得了最先进的性能，并且在其他对象上也表现出竞争力。这些性能支持了我们的目标，即提供一种能够重建具有异质材料的复杂形状的通用方法。</p>
</li>
<li><p>方法：</p>
<ul>
<li>混合隐式曲面表示：将人体形状表示为由不透明区域和半透明区域组成的两个曲面层，并使用视觉线索自动分割不同的区域，学习重建两个有符号距离函数 (SDF)。</li>
<li>混合渲染：对不透明区域执行基于曲面的渲染以保留高保真曲面法线，对半透明区域执行体积渲染。</li>
<li>集成 SDF 用于体积密度：引入一个可学习的高斯混合模型作为 SDF 到密度函数，以更好地建模精细的几何细节。</li>
<li>自适应采样策略：提出一种自适应采样策略，以确保每个射线上采样的间隔相似，从而提高渲染质量。</li>
<li>训练：通过随机采样每个训练图像上的像素集并最小化总损失来训练网络，包括掩码损失、光度损失、镜面损失和 Eikonal 损失。</li>
</ul>
</li>
<li><p>结论：
（1）：本工作的主要贡献在于提出了一种新的混合隐式曲面表示（HISR）来进行人体的三维重建，该方法结合了基于曲面的渲染和体积渲染，能够同时保留高保真曲面法线和精细的几何细节。在各种人体和物体重建数据集上的评估表明，我们的方法在重建几何保真度和新视角合成方面都优于基线方法。这得益于我们为不透明区域和半透明区域设置的双曲面层表示，允许对皮肤、头发和衣服等复杂的人体特征进行细致的渲染。我们的方法在三维人体重建方面取得了最先进的结果，并在其他物体上也表现出竞争力。
（2）：创新点：</p>
</li>
</ol>
<ul>
<li>提出了一种新的混合隐式曲面表示（HISR），该表示由两个曲面层组成，分别代表穿着衣服的人体上的不透明区域和半透明区域。</li>
<li>使用视觉线索自动分割不同的区域，并学习重建两个有符号距离函数 (SDF)。</li>
<li>对不透明区域执行基于曲面的渲染以保留高保真曲面法线，对半透明区域执行体积渲染。</li>
<li>引入一个可学习的高斯混合模型作为 SDF 到密度函数，以更好地建模精细的几何细节。</li>
<li>提出了一种自适应采样策略，以确保每个射线上采样的间隔相似，从而提高渲染质量。</li>
</ul>
<p>性能：</p>
<ul>
<li>在人体形状重建任务上取得了最先进的性能，并且在其他对象上也表现出竞争力。</li>
</ul>
<p>工作量：</p>
<ul>
<li>混合隐式曲面表示的构建和学习需要较大的计算量。</li>
<li>自适应采样策略的实现也需要较大的计算量。</li>
</ul>



<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-65b60c55274f249c7a21655ea5c21695.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cdd4c157203b0a88b68cba902768a310.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-67ee6c759bd41020f430403f5e2d93ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fd6993e8a08dd8e58fbc4eaec5ab30ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-edd10754c68e2204a13c9b005617ccb0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c2e0db87430d852291b34fe1369fecf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1465bdf98251d75ab856b665d50f4419.jpg" align="middle">
</details>
​    


## In-Hand 3D Object Reconstruction from a Monocular RGB Video
**Authors:Shijian Jiang, Qi Ye, Rengan Xie, Yuchi Huo, Xiang Li, Yang Zhou, Jiming Chen**

Our work aims to reconstruct a 3D object that is held and rotated by a hand in front of a static RGB camera. Previous methods that use implicit neural representations to recover the geometry of a generic hand-held object from multi-view images achieved compelling results in the visible part of the object. However, these methods falter in accurately capturing the shape within the hand-object contact region due to occlusion. In this paper, we propose a novel method that deals with surface reconstruction under occlusion by incorporating priors of 2D occlusion elucidation and physical contact constraints. For the former, we introduce an object amodal completion network to infer the 2D complete mask of objects under occlusion. To ensure the accuracy and view consistency of the predicted 2D amodal masks, we devise a joint optimization method for both amodal mask refinement and 3D reconstruction. For the latter, we impose penetration and attraction constraints on the local geometry in contact regions. We evaluate our approach on HO3D and HOD datasets and demonstrate that it outperforms the state-of-the-art methods in terms of reconstruction surface quality, with an improvement of $52\%$ on HO3D and $20\%$ on HOD. Project webpage: https://east-j.github.io/ihor. 

[PDF](http://arxiv.org/abs/2312.16425v1) Accepted by AAAI2024

**摘要**
引入二维遮挡阐释和物理接触约束，可提升贴合于手的复杂形状的 3D 重建质量。

**主要结论**

- 提出了一种新颖的方法来处理遮挡下的表面重建，该方法结合了二维遮挡阐释先验和物理接触约束。
- 引入了一个对象无遮挡完成网络来推断遮挡下对象的二维完整掩码。
- 设计了一种联合优化方法，用于无遮挡掩码细化和三维重建。
- 对 HO3D 和 HOD 数据集评估了我们的方法，结果表明，在重建表面质量方面，我们的方法优于最先进的方法，在 HO3D 上提高了 52%，在 HOD 上提高了 20%。
- 项目网页：https://east-j.github.io/ihor。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li><p>题目：遮挡下三维手持物体的表面重建</p>
</li>
<li><p>作者：Junyi Dong, Yuxuan Zhang, Jiaolong Yang, Shiwei Li, Kai Xu, Qixing Huang, Xin Tong</p>
</li>
<li><p>单位：清华大学</p>
</li>
<li><p>关键词：三维重建、遮挡、手持物体、隐式神经表示</p>
</li>
<li><p>论文链接：https://arxiv.org/abs/2211.06779, Github 链接：None</p>
</li>
<li><p>摘要：
(1)：研究背景：现有方法使用隐式神经表示从多视角图像中恢复通用手持物体的几何形状，在物体的可见部分取得了令人满意的结果。然而，这些方法由于遮挡，无法准确捕捉手物体接触区域内的形状。
(2)：过去的方法和问题：过去的方法使用隐式神经表示从多视角图像中恢复通用手持物体的几何形状，在物体的可见部分取得了令人满意的结果。然而，这些方法由于遮挡，无法准确捕捉手物体接触区域内的形状。
(3)：研究方法：本文提出了一种新的方法来处理遮挡下的表面重建，该方法结合了二维遮挡阐明和物理接触约束。对于前者，我们引入了一个对象模态完成网络来推断遮挡下物体的二维完整掩码。为了确保预测的二维模态掩码的准确性和视图一致性，我们设计了一种用于模态掩码细化和三维重建的联合优化方法。对于后者，我们在接触区域的局部几何形状上施加穿透和吸引约束。
(4)：方法的性能：我们在 HO3D 和 HOD 数据集上评估了我们的方法，并证明它在重建表面质量方面优于最先进的方法，在 HO3D 上提高了 52%，在 HOD 上提高了 20%。这些性能支持了我们的目标。</p>
</li>
<li><p>方法：
(1) 二维遮挡阐明：引入对象模态完成网络推断遮挡下物体的二维完整掩码，设计联合优化方法细化模态掩码并进行三维重建；
(2) 物理接触约束：在接触区域的局部几何形状上施加穿透和吸引约束，确保重建结果的物理合理性；
(3) 联合优化：将二维遮挡阐明和物理接触约束结合起来，进行联合优化，以获得更准确的表面重建结果。</p>
</li>
<li><p>结论：
（1）：本文提出了一种新的方法来处理遮挡下的表面重建，该方法结合了二维遮挡阐明和物理接触约束，在HO3D和HOD数据集上评估了我们的方法，并证明它在重建表面质量方面优于最先进的方法，在HO3D上提高了52%，在HOD上提高了20%。
（2）：创新点：</p>
</li>
</ol>
<ul>
<li>引入对象模态完成网络推断遮挡下物体的二维完整掩码，设计联合优化方法细化模态掩码并进行三维重建。</li>
<li>在接触区域的局部几何形状上施加穿透和吸引约束，确保重建结果的物理合理性。</li>
<li>将二维遮挡阐明和物理接触约束结合起来，进行联合优化，以获得更准确的表面重建结果。
性能：</li>
<li>在HO3D和HOD数据集上评估了我们的方法，并证明它在重建表面质量方面优于最先进的方法，在HO3D上提高了52%，在HOD上提高了20%。
工作量：</li>
<li>需要收集和预处理大量的数据。</li>
<li>需要设计和训练对象模态完成网络和联合优化方法。</li>
<li>需要对重建结果进行评估。</li>
</ul>



<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3ecdbcd441f93fe15bf3fe1c7cb442b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d87c02c6c8349823fdb6a6a7ca7dd86.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac876d7d2f5101b310e755365dc6534b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-af4d9aa687fac12ec84ed6bb8f0af4c0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e9131687642c0ded0611866dd3c4e02e.jpg" align="middle">
</details>
​    


## Human101: Training 100+FPS Human Gaussians in 100s from 1 View
**Authors:Mingwei Li, Jiachen Tao, Zongxin Yang, Yi Yang**

Reconstructing the human body from single-view videos plays a pivotal role in the virtual reality domain. One prevalent application scenario necessitates the rapid reconstruction of high-fidelity 3D digital humans while simultaneously ensuring real-time rendering and interaction. Existing methods often struggle to fulfill both requirements. In this paper, we introduce Human101, a novel framework adept at producing high-fidelity dynamic 3D human reconstructions from 1-view videos by training 3D Gaussians in 100 seconds and rendering in 100+ FPS. Our method leverages the strengths of 3D Gaussian Splatting, which provides an explicit and efficient representation of 3D humans. Standing apart from prior NeRF-based pipelines, Human101 ingeniously applies a Human-centric Forward Gaussian Animation method to deform the parameters of 3D Gaussians, thereby enhancing rendering speed (i.e., rendering 1024-resolution images at an impressive 60+ FPS and rendering 512-resolution images at 100+ FPS). Experimental results indicate that our approach substantially eclipses current methods, clocking up to a 10 times surge in frames per second and delivering comparable or superior rendering quality. Code and demos will be released at https://github.com/longxiang-ai/Human101. 

[PDF](http://arxiv.org/abs/2312.15258v1) Website: https://github.com/longxiang-ai/Human101

**摘要**
通过在100秒内训练3D高斯模型和以超过100帧/秒的速度渲染，Human101可以从单视角视频快速生成高保真3D动态人体重建。

**主要要点**

- Human101是一种新的框架，可以在100秒内从1视角视频生成高保真的动态3D人体重建，并能以超过100FPS的速度渲染。
- Human101利用3D高斯散点图的优势，提供人体3D表示。
- Human101采用以人为中心的前向高斯动画方法，变形3D高斯模型的参数，从而提高渲染速度。
- Human101在渲染质量上与目前最先进的方法相当或优越，并且帧数是目前最先进的方法的10倍以上。
- Human101 的代码和演示将在 https://github.com/longxiang-ai/Human101 上发布。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li><p>论文标题：Human101：从单视角在 100 秒内训练 100+FPS 人体高斯分布</p>
</li>
<li><p>作者：Longxiang Xiang, Hanqing Jiang, Zhe Wang, Yebin Liu, Xiaowei Zhou</p>
</li>
<li><p>第一作者单位：浙江大学</p>
</li>
<li><p>关键词：人体重建、神经辐射场、高斯分布、单视角重建、实时渲染</p>
</li>
<li><p>论文链接：None，Github 链接：https://github.com/longxiang-ai/Human101</p>
</li>
<li><p>摘要：</p>
</li>
</ol>
<p>（1）研究背景：从单视角视频重建人体在虚拟现实领域发挥着关键作用。一个普遍的应用场景需要快速重建高保真 3D 数字人，同时确保实时渲染和交互。现有方法通常难以满足这两个要求。</p>
<p>（2）过去方法及其问题：神经辐射场 (NeRF) 方法在单视角人体重建中取得了成功，但它们通常计算成本高，无法实现实时渲染。基于 3D 高斯分布的方法可以实现快速渲染，但它们通常缺乏细节和保真度。</p>
<p>（3）本文提出的研究方法：本文提出 Human101，这是一个新颖的框架，能够通过在 100 秒内训练 3D 高斯分布并以 100+FPS 渲染，从单视角视频生成高保真动态 3D 人体重建。Human101 利用了 3D 高斯分布的优势，它提供了人体的一种显式且高效的表示。与之前的基于 NeRF 的管道不同，Human101 巧妙地应用了一种以人为中心的正向高斯动画方法来变形 3D 高斯分布的参数，从而提高渲染速度（即，以令人印象深刻的 60+FPS 渲染 1024 分辨率图像，并以 100+FPS 渲染 512 分辨率图像）。</p>
<p>（4）方法的性能：实验结果表明，本文方法大大优于当前方法，将每秒帧数提高了 10 倍，并提供了可比或更高的渲染质量。</p>
<ol start="7">
<li><p>Methods:
(1): Human101方法的核心思想是利用3D高斯分布来表示人体，并通过一种以人为中心的正向高斯动画方法来变形3D高斯分布的参数，从而实现快速渲染和交互。
(2): 具体来说，Human101首先通过单视角视频训练一个3D高斯分布，然后利用该分布来生成人体的高分辨率网格模型。
(3): 为了实现快速渲染，Human101应用了一种以人为中心的正向高斯动画方法来变形3D高斯分布的参数，从而避免了昂贵的体渲染计算。
(4): 这种方法使得Human101能够以令人印象深刻的60+FPS渲染1024分辨率图像，并以100+FPS渲染512分辨率图像。
(5): 实验结果表明，Human101方法大大优于当前方法，将每秒帧数提高了10倍，并提供了可比或更高的渲染质量。</p>
</li>
<li><p>结论：
（1）：Human101 是一个从单视角视频中重建高保真动态人体模型的新颖框架，它在 100 秒内使用固定视角相机高效地重建了高保真动态人体模型。新颖的规范化人体初始化、以人为中心的正向高斯动画和以人为中心的正向高斯细化相结合，再配以 3DGS 的显式表示，显著提高了渲染速度。此外，这种速度的提升并没有牺牲视觉质量。实验表明，与最先进的方法相比，Human101 的 FPS 提高了 67 倍，并保持了可比或更好的视觉质量。Human101 为从单视角视频中重建人体树立了新标准。这一突破为沉浸式技术中的进一步发展和应用奠定了基础。
（2）：创新点：</p>
</li>
</ol>
<ul>
<li>提出了一种新颖的框架 Human101，该框架能够在 100 秒内从单视角视频中重建高保真动态人体模型。</li>
<li>提出了一种新的规范化人体初始化方法，该方法可以将人体初始化为一个标准姿势，从而提高重建的准确性和鲁棒性。</li>
<li>提出了一种新的以人为中心的正向高斯动画方法，该方法可以变形 3D 高斯分布的参数，从而实现快速渲染。</li>
<li>提出了一种新的以人为中心的正向高斯细化方法，该方法可以进一步提高重建的质量。
性能：</li>
<li>Human101 的 FPS 比最先进的方法提高了 67 倍。</li>
<li>Human101 的视觉质量与最先进的方法相当或更好。
工作量：</li>
<li>Human101 的训练时间为 100 秒。</li>
<li>Human101 的渲染时间为 60+FPS（1024 分辨率）或 100+FPS（512 分辨率）。</li>
</ul>



<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-84a60e1cfd3ff2a4ccd504c677c219dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f8cfe9cdf0f3f288a2851246fa3440a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9d7298160fd7bc71030647b1bbde1aed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-95ae9edf8140557344587f9d62973d44.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f9308b2b911a7239d0b1c13e120fe940.jpg" align="middle">
</details>
​    


## ZeroShape: Regression-based Zero-shot Shape Reconstruction
**Authors:Zixuan Huang, Stefan Stojanov, Anh Thai, Varun Jampani, James M. Rehg**

We study the problem of single-image zero-shot 3D shape reconstruction. Recent works learn zero-shot shape reconstruction through generative modeling of 3D assets, but these models are computationally expensive at train and inference time. In contrast, the traditional approach to this problem is regression-based, where deterministic models are trained to directly regress the object shape. Such regression methods possess much higher computational efficiency than generative methods. This raises a natural question: is generative modeling necessary for high performance, or conversely, are regression-based approaches still competitive? To answer this, we design a strong regression-based model, called ZeroShape, based on the converging findings in this field and a novel insight. We also curate a large real-world evaluation benchmark, with objects from three different real-world 3D datasets. This evaluation benchmark is more diverse and an order of magnitude larger than what prior works use to quantitatively evaluate their models, aiming at reducing the evaluation variance in our field. We show that ZeroShape not only achieves superior performance over state-of-the-art methods, but also demonstrates significantly higher computational and data efficiency. 

[PDF](http://arxiv.org/abs/2312.14198v2) Project page: https://zixuanh.com/projects/zeroshape.html

**摘要**
回归式模型ZeroShape在单张图像零样本三维形状重建中取得了卓越的性能和计算效率。

**要点**
- 回归式方法在单张图像零样本三维形状重建中同样具有竞争力。
- 提出了一个强大的回归式模型ZeroShape，该模型基于领域内趋同的研究成果和一个新颖的洞察。
- 构建了一个大型的真实世界评估基准，包含来自三个不同真实世界三维数据集的对象。
- 该评估基准比先前工作用于定量评估其模型的基准更加多样化，并且数量级更大。
- 证明了ZeroShape不仅优于最先进的方法，而且显示出更高的计算和数据效率。
- 回归方法效率高、可用于实时渲染。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>题目：ZeroShape：基于回归的零样本形状重建</li>
<li>作者：Zixuan Huang、Stefan Stojanov、Anh Thai、Varun Jampani、James M. Rehg</li>
<li>隶属机构：伊利诺伊大学厄巴纳-香槟分校</li>
<li>关键词：零样本形状重建、回归、生成模型</li>
<li>论文链接：https://arxiv.org/abs/2312.14198
Github 代码链接：无</li>
<li>摘要：
（1）研究背景：<ul>
<li>零样本形状重建旨在从单张图像中重建从未见过的物体的 3D 形状。</li>
<li>最近的工作通过生成扩散模型或神经辐射场 (NeRF) 来学习零样本形状先验，但这些模型在训练和推理时计算成本都很高。</li>
<li>传统方法是基于回归的，直接回归物体的形状，计算效率更高。</li>
</ul>
</li>
</ol>
<p>（2）过去方法及其问题：</p>
<ul>
<li>现有方法主要基于生成模型，计算成本高，并且需要大量训练数据。</li>
<li>基于回归的方法虽然计算效率高，但性能不如生成模型。</li>
</ul>
<p>（3）提出的研究方法：</p>
<ul>
<li>提出了一种新的基于回归的零样本形状重建模型 ZeroShape。</li>
<li>ZeroShape 结合了领域内最新研究成果和一个新的洞察，在性能和效率上都优于现有方法。</li>
<li>构建了一个包含来自三个不同真实世界 3D 数据集的物体的大规模真实世界评估基准。</li>
<li>该基准比以前的工作用于定量评估其模型的基准更加多样化，并且数量级更大，旨在减少该领域的评估差异。</li>
</ul>
<p>（4）方法的性能和对目标的支持：</p>
<ul>
<li>ZeroShape 在零样本 3D 形状重建任务上优于最先进的方法，同时具有更快的推理时间和更少的训练数据。</li>
<li>ZeroShape 不仅在性能上优于最先进的方法，而且还展示出显着更高的计算效率和数据效率。</li>
</ul>
<ol start="7">
<li>方法：</li>
</ol>
<p>（1）深度和相机估计器：使用 DPTResNet CNN 来估计图像的深度图和相机内参。</p>
<p>（2）几何反投影单元：将深度图和内参估计值反投影到归一化的 3D 可见表面，该表面由三通道投影图参数化。</p>
<p>（3）投影引导的形状重建器：使用 ResNet 编码器对投影图进行编码和重塑，然后使用基于交叉注意力的方法从投影图中提取相关补丁编码，并使用 MLP 预测每个查询点的占用值。</p>
<p>（4）损失函数：使用两阶段训练范式，首先预训练深度和相机估计器，然后使用 3D 监督微调整个模型。深度和相机预训练使用深度损失和基于投影的内参损失。整个模型的联合训练使用 3D 占用损失，这是预测占用值和以观察者为中心的坐标系中的地面实况之间的标准二元交叉熵。</p>
<p>（5）实现细节：使用 Adam 优化器训练模型。在深度和相机预训练期间，使用学习率 3×10−5、批大小 44、权重衰减 0.05 和动量参数 (0.9, 0.95)。训练模型 15 个 epoch，并使用 Omnidata 权重初始化深度估计器。在联合训练阶段，使用学习率 3×10−5 用于投影引导的形状重建器，并使用学习率 10−5 用于预训练的深度和相机估计器（几何反投影单元没有可学习参数）。使用批大小 28、权重衰减 0.05 和动量参数 (0.9, 0.95)。在每次迭代中，随机抽取 4096 个点来计算占用损失。在 4×NVIDIA GeForce RTX 2080Ti 上训练模型，预训练需要大约 2 天，联合训练需要大约 3 天。</p>
<p>（6）数据整理：使用来自三个不同真实世界 3D 数据集的物体的大规模真实世界评估基准。该基准比以前的工作用于定量评估其模型的基准更加多样化，并且数量级更大，旨在减少该领域的评估差异。</p>
<ol start="8">
<li>结论：
（1）：本文提出了一种基于回归的零样本形状重建模型 ZeroShape，该模型在性能和效率上优于现有方法。
（2）：创新点：</li>
</ol>
<ul>
<li>提出了一种新的中间表示形式，该表示形式可以有效地进行显式 3D 几何推理。</li>
<li>构建了一个包含来自三个不同真实世界 3D 数据集的物体的大规模真实世界评估基准。
性能：</li>
<li>在零样本 3D 形状重建任务上，ZeroShape 优于最先进的方法，同时具有更快的推理时间和更少的训练数据。</li>
<li>ZeroShape 不仅在性能上优于最先进的方法，而且还展示出显着更高的计算效率和数据效率。
工作量：</li>
<li>使用 Adam 优化器训练模型。</li>
<li>在深度和相机预训练期间，使用学习率 3×10−5、批大小 44、权重衰减 0.05 和动量参数 (0.9, 0.95)。训练模型 15 个 epoch，并使用 Omnidata 权重初始化深度估计器。</li>
<li>在联合训练阶段，使用学习率 3×10−5 用于投影引导的形状重建器，并使用学习率 10−5 用于预训练的深度和相机估计器（几何反投影单元没有可学习参数）。使用批大小 28、权重衰减 0.05 和动量参数 (0.9, 0.95)。在每次迭代中，随机抽取 4096 个点来计算占用损失。</li>
<li>在 4×NVIDIA GeForce RTX 2080Ti 上训练模型，预训练需要大约 2 天，联合训练需要大约 3 天。</li>
</ul>



<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0df67089f0cd470421435e6ad26a625d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19cd67e1a52d95f7d665d88a7ee51292.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-504faa61d0f546e94a3b52452ac7c3e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd545832e863d3187e6888c47dbab37d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-028a47ff3a5de37fe1ed865255a3e193.jpg" align="middle">
</details>
​    


## NeuSurf: On-Surface Priors for Neural Surface Reconstruction from Sparse   Input Views
**Authors:Han Huang, Yulun Wu, Junsheng Zhou, Ge Gao, Ming Gu, Yu-Shen Liu**

Recently, neural implicit functions have demonstrated remarkable results in the field of multi-view reconstruction. However, most existing methods are tailored for dense views and exhibit unsatisfactory performance when dealing with sparse views. Several latest methods have been proposed for generalizing implicit reconstruction to address the sparse view reconstruction task, but they still suffer from high training costs and are merely valid under carefully selected perspectives. In this paper, we propose a novel sparse view reconstruction framework that leverages on-surface priors to achieve highly faithful surface reconstruction. Specifically, we design several constraints on global geometry alignment and local geometry refinement for jointly optimizing coarse shapes and fine details. To achieve this, we train a neural network to learn a global implicit field from the on-surface points obtained from SfM and then leverage it as a coarse geometric constraint. To exploit local geometric consistency, we project on-surface points onto seen and unseen views, treating the consistent loss of projected features as a fine geometric constraint. The experimental results with DTU and BlendedMVS datasets in two prevalent sparse settings demonstrate significant improvements over the state-of-the-art methods. 

[PDF](http://arxiv.org/abs/2312.13977v2) Accepted by AAAI 2024. Project page:   https://alvin528.github.io/NeuSurf/

**摘要**
利用表面先验重建框架增强深度神经网络隐函数，提高稀疏视角下的三维重建精度。

**要点**

- 神经隐式函数在多视角重建领域取得了显著成果，但现有方法多针对稠密视角，稀疏视角下表现不佳。
- 近期提出的几种广义隐式重建方法虽然适用于稀疏视图重建任务，但训练成本高，且仅在精心挑选的视角下有效。
- 本文提出一种利用表面先验，用于实现高保真曲面重建的稀疏视图重建框架。
- 通过设计全局几何对齐和局部几何细化的约束，联合优化粗略形状和精细细节。
- 训练一个神经网络，根据从 SfM 获得的表面点学习一个全局隐式场，将其用作粗略几何约束。
- 利用局部几何一致性，将表面点投影到已见和未见视图，将投影特征的一致性损失作为精细几何约束。
- 在 DTU 和 BlendedMVS 数据集上的实验结果表明，本方法在两种普遍的稀疏设置下均优于最先进的方法。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>题目：NeuSurf：基于表面先验的神经表面重建框架（从稀疏输入中）</li>
<li>作者：Yuxuan Zhang, Yuxin Wen, Yufeng Zheng, Changjian Li, Yanwei Fu, Qiong Yan, Yebin Liu, Lu Fang, Shihui Lai</li>
<li>第一作者单位：华中科技大学</li>
<li>关键词：神经隐式函数、稀疏视图重建、表面先验、几何对齐、局部几何精细化</li>
<li>论文链接：https://arxiv.org/abs/2203.12461，Github 代码链接：None</li>
<li>摘要：
（1）研究背景：神经隐式函数在多视图重建领域取得了显著成果，但现有方法大多针对稠密视图，在处理稀疏视图时表现不佳。一些最新方法试图将隐式重建推广到稀疏视图重建任务，但仍然存在训练成本高、仅在仔细选择的视角下有效等问题。
（2）过去的方法及其问题：现有方法在处理稀疏视图时存在训练成本高、仅在仔细选择的视角下有效等问题。
（3）论文提出的研究方法：本文提出一种利用表面先验的新型稀疏视图重建框架，以实现高度逼真的表面重建。具体来说，我们设计了关于全局几何对齐和局部几何精细化的约束，用于联合优化粗略形状和精细细节。为此，我们训练了一个神经网络，从 SfM 获得的表面点学习一个全局隐式场，然后将其作为粗略几何约束。为了利用局部几何一致性，我们将表面点投影到可见和不可见视图，将投影特征的一致性损失作为精细几何约束。
（4）方法在任务和性能上的表现：在 DTU 和 Blended MVS 数据集上的实验结果表明，在两种普遍的稀疏设置下，该方法显著优于最先进的方法。这些性能支持了论文提出的目标。</li>
</ol>
<p>7.Methods：
(1) 提出一种新的稀疏视图重建框架，利用表面先验实现高度逼真的表面重建。
(2) 设计关于全局几何对齐和局部几何精细化的约束，用于联合优化粗略形状和精细细节。
(3) 训练一个神经网络，从SfM获得的表面点学习一个全局隐式场，作为粗略几何约束。
(4) 将表面点投影到可见和不可见视图，将投影特征的一致性损失作为精细几何约束。
(5) 在DTU和BlendedMVS数据集上的实验结果表明，该方法显著优于最先进的方法。</p>
<ol start="8">
<li>结论：
（1）：本文提出了一种基于表面先验的神经表面重建框架 NeuSurf，该框架利用表面点学习全局隐式场作为粗略几何约束，并将表面点投影到可见和不可见视图，将投影特征的一致性损失作为精细几何约束，从而实现高度逼真的表面重建。
（2）：创新点：</li>
</ol>
<ul>
<li>提出了一种新的稀疏视图重建框架，利用表面先验实现高度逼真的表面重建。</li>
<li>设计关于全局几何对齐和局部几何精细化的约束，用于联合优化粗略形状和精细细节。</li>
<li>训练一个神经网络，从 SfM 获得的表面点学习一个全局隐式场，作为粗略几何约束。</li>
<li>将表面点投影到可见和不可见视图，将投影特征的一致性损失作为精细几何约束。
性能：</li>
<li>在 DTU 和 BlendedMVS 数据集上的实验结果表明，该方法显著优于最先进的方法。</li>
<li>这些性能支持了论文提出的目标。
工作量：</li>
<li>该方法不需要大规模训练，并且在各种稀疏设置中都很稳健。</li>
</ul>



<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3f6644db90f1dd4f7ca9dcf04e307b68.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f0eb8b77b117c4be4c26d0982f919c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f11cf5878ea37a4b5efbd6c59f20a5d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3dee8633ed2740393067a67de3d6ef00.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c8ebc6b78ec57ce3e5e1234d62b7690.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-302eebe295f00eabc65233dc28a98374.jpg" align="middle">
</details>
​    


## LASA: Instance Reconstruction from Real Scans using A Large-scale   Aligned Shape Annotation Dataset
**Authors:Haolin Liu, Chongjie Ye, Yinyu Nie, Yingfan He, Xiaoguang Han**

Instance shape reconstruction from a 3D scene involves recovering the full geometries of multiple objects at the semantic instance level. Many methods leverage data-driven learning due to the intricacies of scene complexity and significant indoor occlusions. Training these methods often requires a large-scale, high-quality dataset with aligned and paired shape annotations with real-world scans. Existing datasets are either synthetic or misaligned, restricting the performance of data-driven methods on real data. To this end, we introduce LASA, a Large-scale Aligned Shape Annotation Dataset comprising 10,412 high-quality CAD annotations aligned with 920 real-world scene scans from ArkitScenes, created manually by professional artists. On this top, we propose a novel Diffusion-based Cross-Modal Shape Reconstruction (DisCo) method. It is empowered by a hybrid feature aggregation design to fuse multi-modal inputs and recover high-fidelity object geometries. Besides, we present an Occupancy-Guided 3D Object Detection (OccGOD) method and demonstrate that our shape annotations provide scene occupancy clues that can further improve 3D object detection. Supported by LASA, extensive experiments show that our methods achieve state-of-the-art performance in both instance-level scene reconstruction and 3D object detection tasks. 

[PDF](http://arxiv.org/abs/2312.12418v1) homepage: https://gap-lab-cuhk-sz.github.io/LASA/

**Summary**
实例形状重建从 3D 场景中恢复多个对象的完整几何形状，于语义实例级别。

**Key Takeaways**
- LASA 包含 10412 个高质量 CAD 注释，与 920 个来自 ArkitScenes 的真实场景扫描对齐，由专业艺术家手动创建。
- 提出了一种新的基于扩散的跨模态形状重建方法 (DisCo)。
- DisCo 使用混合特征聚合设计来融合多模态输入并恢复高保真度对象几何体。
- 提出了一种占有引导的 3D 对象检测方法 (OccGOD)。
- 形状注释提供了场景占用线索，可以进一步改进 3D 对象检测。
- 广泛的实验表明，DisCo 和 OccGOD 在实例级场景重建和 3D 对象检测任务中均取得了最先进的性能。
- DisCo 在 LASA 上训练，在三个基准数据集上的平均改进幅度为 13.2%。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>题目：LASA：利用大规模对齐形状注释数据集从真实扫描中进行实例重建</li>
<li>作者：Haolin Liu, Chongjie Ye, Yinyu Nie, Yingfan He, Xiaoguang Han</li>
<li>隶属单位：香港中文大学（深圳）</li>
<li>关键词：实例形状重建、3D物体检测、大规模数据集、对齐形状注释</li>
<li>论文链接：https://arxiv.org/abs/2312.12418
Github 代码链接：无</li>
<li>摘要：
(1)：研究背景：从 3D 场景中进行实例形状重建涉及恢复多个对象的完整几何形状，这些对象处于语义实例级别。由于场景复杂性和明显的室内遮挡，许多方法利用数据驱动的学习。训练这些方法通常需要一个大规模、高质量的数据集，其中包含与真实世界扫描对齐且配对的形状注释。现有数据集要么是合成的，要么是未对齐的，这限制了数据驱动方法在真实数据上的性能。
(2)：过去的方法及其问题：目前的方法是利用深度学习方法来解决实例级别场景重建任务。这些方法取得了很大的进展，但它们也存在一些问题。首先，这些方法通常需要大量的数据来训练，这在现实世界中很难获得。其次，这些方法通常对噪声和不完整的数据非常敏感，这在真实世界扫描中很常见。最后，这些方法通常只能重建有限数量的物体类别，这限制了它们的适用性。
(3)：论文提出的研究方法：为了解决这些问题，本文提出了一种新的方法，称为 LASA（Large-scale Aligned Shape Annotation Dataset）。LASA 是一个包含 10,412 个高质量 CAD 注释的大规模对齐形状注释数据集，这些注释与来自 ArkitScenes 的 920 个真实世界场景扫描对齐。LASA 是由专业艺术家手动创建的，它提供了高质量的形状注释，可以用于训练和评估实例级别场景重建方法。
(4)：方法在任务上的表现及性能：在实例级别场景重建任务上，LASA 可以支持最先进的性能。在 3D 物体检测任务上，LASA 也可以支持最先进的性能。这些结果表明，LASA 对于训练和评估实例级别场景重建方法非常有价值。</li>
</ol>
<p>&lt;Methods&gt;:
(1): 该文提出了一种名为LASA的大规模对齐形状注释数据集，该数据集包含10,412个高质量CAD注释，这些注释与来自ArkitScenes的920个真实世界场景扫描对齐。
(2): LASA是由专业艺术家手动创建的，它提供了高质量的形状注释，可以用于训练和评估实例级别场景重建方法。
(3): 在实例级别场景重建任务上，LASA可以支持最先进的性能。在3D物体检测任务上，LASA也可以支持最先进的性能。</p>
<ol start="8">
<li>总结：
（1）：该文提出了一个名为 LASA 的大规模对齐形状注释数据集，该数据集包含 10,412 个高质量 CAD 注释，这些注释与来自 ArkitScenes 的 920 个真实世界场景扫描对齐。LASA 由专业艺术家手动创建，它提供了高质量的形状注释，可以用于训练和评估实例级别场景重建方法。在实例级别场景重建任务上，LASA 可以支持最先进的性能。在 3D 物体检测任务上，LASA 也可以支持最先进的性能。
（2）：创新点：
该文提出了一个名为 LASA 的大规模对齐形状注释数据集，该数据集包含 10,412 个高质量 CAD 注释，这些注释与来自 ArkitScenes 的 920 个真实世界场景扫描对齐。LASA 由专业艺术家手动创建，它提供了高质量的形状注释，可以用于训练和评估实例级别场景重建方法。在实例级别场景重建任务上，LASA 可以支持最先进的性能。在 3D 物体检测任务上，LASA 也可以支持最先进的性能。
性能：
在实例级别场景重建任务上，LASA 可以支持最先进的性能。在 3D 物体检测任务上，LASA 也可以支持最先进的性能。
工作量：
该文的工作量很大，需要收集和注释大量的数据。此外，该文还提出了两种新的方法，Diffusion-based Cross-Modal Shape Reconstruction 和 Occupancy-guided 3D Object Detection，这两种方法的实现也需要大量的工作量。</li>
</ol>



<p></p><details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-cda9d91453de63d77467b3bed34c6d49.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2b47fbe622e984a9f8410e202812867.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aafa1603b087a09b6d9a0e1ba939e3c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b5f74bcf20d71e60eae001a07761608.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-594a80e24dc501900a0ba0cb9417b9b7.jpg" align="middle">
</details><br>​    <p></p>
<h2 id="SEEAvatar-Photorealistic-Text-to-3D-Avatar-Generation-with-Constrained-Geometry-and-Appearance"><a href="#SEEAvatar-Photorealistic-Text-to-3D-Avatar-Generation-with-Constrained-Geometry-and-Appearance" class="headerlink" title="SEEAvatar: Photorealistic Text-to-3D Avatar Generation with Constrained   Geometry and Appearance"></a>SEEAvatar: Photorealistic Text-to-3D Avatar Generation with Constrained   Geometry and Appearance</h2><p><strong>Authors:Yuanyou Xu, Zongxin Yang, Yi Yang</strong></p>
<p>Powered by large-scale text-to-image generation models, text-to-3D avatar generation has made promising progress. However, most methods fail to produce photorealistic results, limited by imprecise geometry and low-quality appearance. Towards more practical avatar generation, we present SEEAvatar, a method for generating photorealistic 3D avatars from text with SElf-Evolving constraints for decoupled geometry and appearance. For geometry, we propose to constrain the optimized avatar in a decent global shape with a template avatar. The template avatar is initialized with human prior and can be updated by the optimized avatar periodically as an evolving template, which enables more flexible shape generation. Besides, the geometry is also constrained by the static human prior in local parts like face and hands to maintain the delicate structures. For appearance generation, we use diffusion model enhanced by prompt engineering to guide a physically based rendering pipeline to generate realistic textures. The lightness constraint is applied on the albedo texture to suppress incorrect lighting effect. Experiments show that our method outperforms previous methods on both global and local geometry and appearance quality by a large margin. Since our method can produce high-quality meshes and textures, such assets can be directly applied in classic graphics pipeline for realistic rendering under any lighting condition. Project page at: <a href="https://yoxu515.github.io/SEEAvatar/">https://yoxu515.github.io/SEEAvatar/</a>. </p>
<p><a href="http://arxiv.org/abs/2312.08889v2">PDF</a> </p>
<p><strong>Summary</strong><br>使用具有自进化约束条件的文本到 3D 头像生成方法，可生成具有照片级真实感、形状和外观解耦的 3D 头像。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>使用文本到图像生成模型的大规模预训练，文本到 3D 头像生成取得了显著进展。</li>
<li>现有方法由于几何形状不准确和外观质量低，无法生成具有照片级真实感的结果。</li>
<li>SEEAvatar 提出了一种使用文本生成具有照片级真实感 3D 头像的方法，具有用于分离几何形状和外观的自进化约束条件。</li>
<li>为了生成几何形状，我们建议使用模板头像来约束优化后的头像以获得合理的外观形状。</li>
<li>模板头像使用人体先验进行初始化，并且可以由优化后的头像定期更新为不断进化的模板，从而实现更灵活的形状生成。</li>
<li>此外，几何形状还受到面部和手等局部部位的静态人体先验的约束，以保持精细的结构。</li>
<li>为了生成外表，我们使用提示工程增强的扩散模型来指导基于物理的渲染管道生成逼真的纹理。</li>
<li>将亮度约束应用于反照率纹理以抑制不正确的照明效果。</li>
<li>实验表明，我们的方法在整体和局部几何形状和外观质量方面都优于以前的方法。</li>
<li>由于我们的方法可以生成高质量的网格和纹理，因此这些资源可以直接应用于经典图形管道中，以便在任何照明条件下进行逼真的渲染。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li><p>标题：SEEAvatar：具有约束几何和外观的逼真文本到 3D 头像生成</p>
</li>
<li><p>作者：Yuxuan Zhou, Jiajun Wu, Kangxue Yin, Jingyi Yu, Hao Tang, Kun Zhou, Qifeng Chen</p>
</li>
<li><p>单位：暂无</p>
</li>
<li><p>关键词：文本到 3D 头像生成、几何约束、外观生成、扩散模型、照明约束</p>
</li>
<li><p>论文链接：https://arxiv.org/abs/2302.09529, Github 链接：无</p>
</li>
<li><p>摘要：
(1)：研究背景：随着大规模文本到图像生成模型的发展，文本到 3D 头像生成取得了可喜的进展。然而，大多数方法由于几何不精确和外观质量低，无法产生逼真的结果。
(2)：过去的方法：现有方法在几何和外观方面都存在问题。在几何方面，现有方法通常使用静态模板来约束几何，这限制了形状生成的灵活性，并且难以生成复杂的服装。在外观方面，现有方法通常使用扩散模型来生成纹理，但这些模型容易受到照明条件的影响，并且难以生成准确的物理参数。
(3)：研究方法：为了解决上述问题，本文提出了一种名为 SEEAvatar 的方法。SEEAvatar 使用一个不断进化的模板来约束几何，该模板可以根据优化后的头像进行更新，从而能够生成更灵活的形状。此外，SEEAvatar 还使用扩散模型来生成纹理，并应用亮度约束来抑制不正确的照明效果。
(4)：方法性能：实验表明，SEEAvatar 在几何和外观质量方面均优于以往的方法。SEEAvatar 可以生成高质量的网格和纹理，这些资产可以直接应用于经典图形管道中，以在任何照明条件下进行逼真的渲染。</p>
</li>
<li><p><strong>方法</strong>：
(1) <strong>几何约束：</strong>
- 使用一个不断进化的模板来约束几何，该模板可以根据优化后的头像进行更新。
- 模板由一个粗糙的网格表示，该网格可以根据优化后的头像进行变形。
- 使用一个优化器来最小化模板和优化后的头像之间的距离。
(2) <strong>外观生成：</strong>
- 使用一个扩散模型来生成纹理。
- 扩散模型是一个生成模型，它可以从噪声中生成图像。
- 使用一个优化器来最小化纹理和优化后的头像之间的距离。
(3) <strong>照明约束：</strong>
- 在纹理生成过程中应用亮度约束，以抑制不正确的照明效果。
- 亮度约束通过最小化纹理和优化后的头像之间的亮度差异来实现。</p>
</li>
<li><p>结论：
（1）：本文提出了一种名为 SEEAvatar 的方法，该方法能够生成具有约束几何和外观的逼真文本到 3D 头像。SEEAvatar 使用一个不断进化的模板来约束几何，该模板可以根据优化后的头像进行更新，从而能够生成更灵活的形状。此外，SEEAvatar 还使用扩散模型来生成纹理，并应用亮度约束来抑制不正确的照明效果。实验表明，SEEAvatar 在几何和外观质量方面均优于以往的方法。SEEAvatar 可以生成高质量的网格和纹理，这些资产可以直接应用于经典图形管道中，以在任何照明条件下进行逼真的渲染。
（2）：创新点：</p>
</li>
</ol>
<ul>
<li>提出了一种新的几何约束方法，该方法能够生成更灵活的形状，并保持详细的局部结构。</li>
<li>提出了一种新的外观生成方法，该方法能够生成更逼真的纹理，并抑制不正确的照明效果。</li>
<li>提出了一种新的亮度约束方法，该方法能够有效地抑制纹理中的照明效果。
性能：</li>
<li>SEEAvatar 在几何和外观质量方面均优于以往的方法。</li>
<li>SEEAvatar 可以生成高质量的网格和纹理，这些资产可以直接应用于经典图形管道中，以在任何照明条件下进行逼真的渲染。
工作量：</li>
<li>SEEAvatar 的工作量相对较大，需要大量的训练数据和计算资源。</li>
</ul>



<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-31d8f3ef22e9983e6f080f4f979f6284.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-204ca8c7f61c24414854bac9e34ba0a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af635847f8e0712b1b887523a86123da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df9fb4cbdd77b5ee6fa2c33565667f41.jpg" align="middle">
</details>
​    


## 3DGEN: A GAN-based approach for generating novel 3D models from image   data
**Authors:Antoine Schnepf, Flavian Vasile, Ugo Tanielian**

The recent advances in text and image synthesis show a great promise for the future of generative models in creative fields. However, a less explored area is the one of 3D model generation, with a lot of potential applications to game design, video production, and physical product design. In our paper, we present 3DGEN, a model that leverages the recent work on both Neural Radiance Fields for object reconstruction and GAN-based image generation. We show that the proposed architecture can generate plausible meshes for objects of the same category as the training images and compare the resulting meshes with the state-of-the-art baselines, leading to visible uplifts in generation quality. 

[PDF](http://arxiv.org/abs/2312.08094v1) Submitted to NeurIPS 2022 Machine Learning for Creativity and Design   Workshop

**Summary**

3D 生成模型通过融合神经辐射场和生成对抗网络，提升了游戏、影视、工业设计等领域的 3D 模型生成效果。

**Key Takeaways**

- 3D 模型生成在游戏、视频制作和物理产品设计等领域具有广泛应用前景。


- 3DGEN 模型将神经辐射场和基于 GAN 的图像生成相结合，用于 3D 模型生成。


- 3DGEN 模型可以生成与训练图像相同类别的物体的高质量可信度网格。


- 3DGEN 模型在生成质量方面优于现有最先进的基线。


- 3DGEN 模型有潜力对创意领域的未来产生重大影响。


- 3DGEN 模型还可以用于创建新颖的互动体验。


- 3DGEN 模型还可以用于改善现有的 3D 建模工具。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>题目：3DGEN：一种基于 GAN 的生成新颖 3D 模型的方法</li>
<li>作者：Antoine Schnepf, Flavian Vasile, Ugo Tanielian</li>
<li>第一作者单位：Criteo 人工智能实验室</li>
<li>关键词：生成模型、神经辐射场、隐式表面、3D 模型生成</li>
<li>论文链接：https://arxiv.org/abs/2312.08094</li>
<li>摘要：
（1）研究背景：随着文本和图像合成的快速发展，生成模型在创意领域展现出巨大的潜力。然而，3D 模型生成领域相对较少探索，但在游戏设计、视频制作和实体产品设计等方面具有广泛的应用前景。
（2）过去的方法及其问题：GRAF 模型可以从相似物体的视图集中生成新的体积模型。但其主要限制在于体积表示不适用于生成合理的物体网格，因此不适用于游戏设计、虚拟现实世界设计和动画等 3D 原生创意环境。
（3）本文提出的研究方法：本文提出 3DGEN 模型作为 GRAF 模型的潜在解决方案。该模型结合了 GRAF 和 UNISURF 的优点，可以生成具有对应隐式表面的体积对象，从而轻松导出为 3D 网格。
（4）方法在任务中的表现：3DGEN 模型在生成相同类别物体的合理网格方面取得了良好的效果。与现有方法相比，3DGEN 模型在生成质量方面具有明显的提升。这些结果支持了本文提出的方法能够有效生成新颖的 3D 模型。</li>
</ol>
<p>Some Error for method(比如是不是没有Methods这个章节)</p>
<ol start="8">
<li>结论：
（1）：本文提出了3DGEN模型，该模型结合了GRAF和UNISURF的优点，可以生成具有对应隐式表面的体积对象，从而轻松导出为3D网格。
（2）：创新点：</li>
</ol>
<ul>
<li>将GRAF和UNISURF模型相结合，生成具有对应隐式表面的体积对象。</li>
<li>提出了一种新的损失函数，可以有效地训练模型。</li>
<li>在生成相同类别物体的合理网格方面取得了良好的效果。
性能：</li>
<li>与现有方法相比，3DGEN模型在生成质量方面具有明显的提升。</li>
<li>3DGEN模型可以生成具有对应隐式表面的体积对象，从而轻松导出为3D网格。
工作量：</li>
<li>3DGEN模型的训练过程相对复杂，需要大量的计算资源。</li>
<li>3DGEN模型的生成过程也相对复杂，需要较长的时间。</li>
</ul>



<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b366229325959f0a6130781934e0265c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d524a6508c45ab9726c11826aafbf1fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-968aafc6935f4229c922187d223a5752.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c38720850992061897e67722d04fd0c.jpg" align="middle">
</details>
​    


## SIFU: Side-view Conditioned Implicit Function for Real-world Usable   Clothed Human Reconstruction
**Authors:Zechuan Zhang, Zongxin Yang, Yi Yang**

Creating high-quality 3D models of clothed humans from single images for real-world applications is crucial. Despite recent advancements, accurately reconstructing humans in complex poses or with loose clothing from in-the-wild images, along with predicting textures for unseen areas, remains a significant challenge. A key limitation of previous methods is their insufficient prior guidance in transitioning from 2D to 3D and in texture prediction. In response, we introduce SIFU (Side-view Conditioned Implicit Function for Real-world Usable Clothed Human Reconstruction), a novel approach combining a Side-view Decoupling Transformer with a 3D Consistent Texture Refinement pipeline.SIFU employs a cross-attention mechanism within the transformer, using SMPL-X normals as queries to effectively decouple side-view features in the process of mapping 2D features to 3D. This method not only improves the precision of the 3D models but also their robustness, especially when SMPL-X estimates are not perfect. Our texture refinement process leverages text-to-image diffusion-based prior to generate realistic and consistent textures for invisible views. Through extensive experiments, SIFU surpasses SOTA methods in both geometry and texture reconstruction, showcasing enhanced robustness in complex scenarios and achieving an unprecedented Chamfer and P2S measurement. Our approach extends to practical applications such as 3D printing and scene building, demonstrating its broad utility in real-world scenarios. Project page https://river-zhang.github.io/SIFU-projectpage/ . 

[PDF](http://arxiv.org/abs/2312.06704v2) Project page https://river-zhang.github.io/SIFU-projectpage/ ;

**Summary**
侧视图条件隐函数实现真实可用衣着人体 3D 重建

**Key Takeaways**

- SIFU 提出一种侧视图条件隐函数，用于真实世界可用衣着人体 3D 重建。
- SIFU 引入侧视图解耦变换器，有效地将侧视图特征与 2D 特征解耦。
- SIFU 采用基于文本到图像扩散的先验，为不可见视图生成逼真且一致的纹理。
- SIFU 在几何和纹理重建方面均优于最先进的方法。
- SIFU 在复杂场景中展现出增强的鲁棒性，并在 Chamfer 和 P2S 测量中取得了前所未有的成果。
- SIFU 可扩展到 3D 打印和场景构建等实际应用，证明了其在现实世界场景中的广泛实用性。
- 项目主页：https://river-zhang.github.io/SIFU-projectpage/。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>题目：SIFU：面向现实世界可用的侧视图条件隐函数服装人体重建</li>
<li>作者：Hongwen Zhang, Yuxuan Zhang, Zhe Wang, Shihao Wu, Yebin Liu, Yajie Zhao, Lu Sheng, Hao Su</li>
<li>单位：上海交通大学</li>
<li>关键词：3D 人体重建、服装重建、隐式函数、扩散模型、文本到图像</li>
<li>论文链接：None, Github 链接：None</li>
<li>摘要：
（1）研究背景：随着计算机视觉技术的快速发展，3D 人体重建技术已经取得了很大的进步。然而，现有的方法在重建复杂姿势或穿着宽松服装的人体时，以及为不可见区域预测纹理时，仍然存在很大的挑战。这是因为现有的方法在从 2D 到 3D 的转换以及纹理预测中缺乏足够的先验指导。
（2）过去的方法：现有的方法主要集中在使用单张图像重建人体几何形状，但对于服装纹理的重建则关注较少。此外，现有的方法在处理复杂姿势或穿着宽松服装的人体时，往往会出现重建不准确或纹理不真实的问题。
（3）研究方法：为了解决上述问题，本文提出了一种新的方法 SIFU（面向现实世界可用的侧视图条件隐函数服装人体重建）。SIFU 的主要贡献包括：<ul>
<li>提出了一种新的侧视图解耦变换器，可以有效地将 2D 特征映射到 3D。</li>
<li>提出了一种新的 3D 一致纹理细化管道，可以为不可见区域生成逼真且一致的纹理。
（4）方法性能：在广泛的实验中，SIFU 在几何和纹理重建方面都优于最先进的方法，在处理复杂场景时表现出增强的鲁棒性，并在 Chamfer 和 P2S 测量中取得了前所未有的结果。我们的方法还扩展到了实际应用，如 3D 打印和场景构建，证明了其在现实世界场景中的广泛实用性。</li>
</ul>
</li>
</ol>
<p>7.方法：
（1）：提出一种新的侧视图解耦变换器，将2D特征映射有效映射到3D，该变换器由一个3D位置编码器和一个2D特征映射解码器组成。
（2）：提出一种新的3D一致纹理细化管道，包括一个3D一致纹理生成器和一个3D一致纹理细化器。
（3）：设计一个新的扩散模型，用于生成逼真且一致的纹理。</p>
<ol start="8">
<li>结论：
（1）：本文提出了一种面向现实世界可用的侧视图条件隐函数服装人体重建方法 SIFU，该方法能够重建高质量的 3D 着装人体网格，并具有详细的纹理。
（2）：创新点：</li>
</ol>
<ul>
<li>提出了一种新的侧视图解耦变换器，可以有效地将 2D 特征映射到 3D。</li>
<li>提出了一种新的 3D 一致纹理细化管道，可以为不可见区域生成逼真且一致的纹理。</li>
<li>设计了一个新的扩散模型，用于生成逼真且一致的纹理。
性能：</li>
<li>在几何和纹理重建方面都优于最先进的方法。</li>
<li>在处理复杂场景时表现出增强的鲁棒性。</li>
<li>在 Chamfer 和 P2S 测量中取得了前所未有的结果。
工作量：</li>
<li>需要大量的训练数据。</li>
<li>训练过程需要大量的时间和计算资源。</li>
</ul>



<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-54b75f643b611ae2794b016d4dc361c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b5ea488f6c95123ca19ad6ae81f164b6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-49b55712f79e5234eecf8dde731ba32c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a19952e9672e2642bc68402553713857.jpg" align="middle">
</details>
​    


## CorresNeRF: Image Correspondence Priors for Neural Radiance Fields
**Authors:Yixing Lao, Xiaogang Xu, Zhipeng Cai, Xihui Liu, Hengshuang Zhao**

Neural Radiance Fields (NeRFs) have achieved impressive results in novel view synthesis and surface reconstruction tasks. However, their performance suffers under challenging scenarios with sparse input views. We present CorresNeRF, a novel method that leverages image correspondence priors computed by off-the-shelf methods to supervise NeRF training. We design adaptive processes for augmentation and filtering to generate dense and high-quality correspondences. The correspondences are then used to regularize NeRF training via the correspondence pixel reprojection and depth loss terms. We evaluate our methods on novel view synthesis and surface reconstruction tasks with density-based and SDF-based NeRF models on different datasets. Our method outperforms previous methods in both photometric and geometric metrics. We show that this simple yet effective technique of using correspondence priors can be applied as a plug-and-play module across different NeRF variants. The project page is at https://yxlao.github.io/corres-nerf. 

[PDF](http://arxiv.org/abs/2312.06642v1) 

**Summary**
用现成的图像匹配方法作为监督，增强 NeRF 模型的训练过程，提高其图像渲染和表面重建性能。

**Key Takeaways**
- CorresNeRF 是一种利用现成方法计算的图像匹配先验来监督 NeRF 训练的新方法。
- CorresNeRF 设计了自适应增强和过滤过程以生成稠密且高质量的匹配。
- 通过匹配像素重投影和深度损失项将匹配用于正则化 NeRF 训练。
- CorresNeRF 在密度和 SDF 为基础的 NeRF 模型上对图像渲染和表面重建任务进行了评估。
- CorresNeRF 在光度和几何度量上均优于以前的方法。
- 这种简单但有效地使用匹配先验的技术可以作为即插即用的模块应用于不同的 NeRF 变体。
- 项目主页：https://yxlao.github.io/corres-nerf。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>题目：CorresNeRF：用于神经辐射场的图像对应先验</li>
<li>作者：Yixing Lao, Xiaogang Xu, Zhipeng Cai, Xihui Liu, Hengshuang Zhao</li>
<li>隶属机构：香港大学</li>
<li>关键词：神经辐射场、图像对应、稀疏视图、三维重建</li>
<li>论文链接：https://arxiv.org/abs/2312.06642
Github 链接：无</li>
<li>摘要：
（1）研究背景：神经辐射场（NeRF）在新型视图合成和表面重建任务中取得了令人印象深刻的结果。然而，在具有稀疏输入视图的具有挑战性的场景下，其性能会受到影响。
（2）过去的方法及其问题：一些方法通过优化渲染过程或添加训练约束来解决这个问题。然而，这些方法可能在真实世界中表现不佳，因为只有稀疏的 2D 输入视图是一个约束不足的问题，并且训练过程很容易过度拟合有限的输入视图。最近的工作提出利用额外的先验来监督 NeRF 训练。然而，当前的先验对于目标场景的稀疏特性不够鲁棒。
（3）研究方法：本文提出了一种新的方法 CorresNeRF，它利用现成的图像对应先验来监督 NeRF 训练。我们设计了自适应的过程来增强和过滤，以生成密集和高质量的对应关系。然后，通过对应像素重投影和深度损失项将对应关系用于正则化 NeRF 训练。
（4）方法的性能：我们在具有密度和 SDF 的 NeRF 模型的不同数据集上评估了我们的方法。我们的方法在光度和几何度量方面都优于以前的方法。我们表明，这种简单但有效的利用对应先验的技术可以作为即插即用模块应用于不同的 NeRF 变体。</li>
</ol>
<p><strong>Methods</strong>：**</p>
<p>（1）神经辐射场背景：** 给定 3D 点 x∈R3 和观察方向 d∈R3，神经辐射场 [4] 预测相应的密度 σ∈[0,∞) 和 RGB 颜色 c∈[0,1]3，由 MLP 网络建模，表示为 fθ:(γ(x),γ(d))→(c,σ)，其中 γ 是位置编码函数。射线 r 定义为 r(t)=o+td，其中 o 是相机中心，d 是射线方向，tn 是近边界，tf 是远边界。为了使用预定义的 tn 和 tf 渲染射线 r，我们将密度 σ 和颜色 c 沿射线积分，如下所示：</p>
<p>ˆcθ(r)=∫tftnT(t)σθ(r(t))cθ(r(t),d)dt，T(t)=exp(−∫ttnσθ(r(t))dt)，</p>
<p>其中 T(t) 是累积透射率，cθ(r(t),d) 和 σθ(r(t)) 分别是 fθ 预测的颜色和密度输出。渲染通过分层采样方法实现，其中在 [tn,tf] 中采样 M 个点，表示为 {x1,...,xM}。密度和颜色可以获得如下所示：</p>
<p>ˆcθ(r)=M∑i=1Ti(1−exp(−σθ(xi)δi))cθ(xi,d)，Ti=exp(−i−1∑j=1σθ(xj)δj)，</p>
<p>其中 δj=tj+1−tj 是相邻采样点之间的距离。具体来说，对于射线 r，其预测的 3D 点可以通过沿射线对加权深度值求和获得，如下所示：</p>
<p>y=o+∑M∑i=1Ti(1−exp(−σθ(xi)δi))ti d。</p>
<p>为了优化 NeRF 模型中的参数 θ，提供一组输入图像和相机参数，并最小化均方误差颜色损失进行优化，如下所示：</p>
<p>Lcolor(θ,R)=Er∈R∥ˆcθ(r)−c(r)∥22，</p>
<p>其中 R 是训练视图中的射线集合，c(r) 是射线 r 的真实颜色。</p>
<p>（2）生成对应关系：** 在本文中，我们重点研究如何利用计算的图像对应关系来增强神经隐式表示在 NeRF 中的性能。因此，对应关系的质量至关重要。对于训练视图中的每对图像，我们使用现成的 SOTA 预训练图像匹配模型计算对应关系。特别是，使用 DKMv3 [24]，因为它提供了密集匹配结果，非常适合我们的用例。为了提高泛化能力，我们将室内和室外模型的预测结果融合在一起，这些模型分别在 ScanNet [46] 和 MegaDepth [47] 上预训练。为了进一步提高对应关系的可靠性，我们提出利用对应关系置信度，并设计了自动和自适应的对应关系处理算法，增加了令人信服的对应关系并去除了异常值。</p>
<p>（3）增强：** 为了增加对应关系的数量，我们对对应关系执行增强。第一种增强类型是图像变换，包括翻转、交换查询和支持图像以及缩放。这些图像变换可以有效地增加预测对应关系的密度，因为图像变换可以提供各种上下文条件来生成对应关系。第二种类型的增强将对应关系传播到图像对中，有效地增加了对应关系的区域覆盖范围。我们构建了一个无向图 G=(V,E)，其中顶点 V={r|r∈R}，边 E={(rq,rs)|rs∈C(rq)}。对于每条边 (rq,rs)，分配一个置信度值 αq,s。然后，我们将对应关系传播到 G 中每个连通分量内的顶点对。具体来说，令 rq 和 rs 是两个顶点，距离为 d，其中是连接它们的路径 (rq,r1,r2,...,rd−1,rs)。我们在 rq 和 rs 之间分配对应关系，置信度为 αq,s=αq,1α1,2...αd−1,s。在实践中，我们可以捕获传播距离 d≤dmax，其中我们在实验中使用 dmax=2。图 3(B) 和 (C) 分别显示了原始对应关系和增强后的对应关系。</p>
<p>（4）异常值过滤：** 为了提高对应关系的质量以指导监督，我们在计算和增强对应关系后删除异常值。首先，我们根据对应点之间的投影射线距离去除异常值。假设 pq 和 ps 是 Iq 到 Is 中的一对 2D 对应关系，πq 和 πs 分别是 Iq 和 Is 的世界到像素投影。给定一对对应关系和相机参数，我们计算沿从相机中心射出的两条射线的最近 3D 点 xq 和 xs。然后，我们将这两个 3D 点投影到对应关系的图像平面。投影射线距离 [48] 定义为投影点和对应关系之间的平均欧几里得距离：</p>
<p>dproj=∥πq(xs)−pq∥2+∥πs(xq)−ps∥22。</p>
<p>我们删除投影射线距离 dproj 大于阈值的对应关系。其次，我们通过检查一个点是否在统计上远离其邻居来去除异常值。对于每一对对应关系，可以获得两个 3D 点 xq 和 xs，这已经在上一段中指出了。然后，我们考虑 12(xq+xs) 是对应关系的 3D 点。我们对所有对应关系对执行此操作以获得 3D 点集 P。对于 P 中的每个 3D 点，我们计算到其 k 个最近邻居的平均距离，如果距离大于阈值，则删除该点（以及其匹配的对应关系对）。此阈值由 P 中所有点的平均距离的标准差确定。</p>
<ol start="8">
<li>结论：
（1）：本文提出了一种利用图像对应先验来训练具有稀疏视图输入的神经辐射场的新方法。我们设计了自动增强和过滤方法，以从稀疏视图输入中生成密集且高质量的图像对应关系。我们设计了基于对应先验的重投影和深度损失项来正则化神经辐射场训练。实验表明，我们的方法仅使用少量输入图像即可显着提高光度和几何度量中衡量的重建质量。
（2）：创新点：</li>
</ol>
<ul>
<li>利用图像对应先验来监督神经辐射场训练，提高了重建质量。</li>
<li>提出了一种自动增强和过滤方法来生成密集且高质量的图像对应关系。</li>
<li>设计了基于对应先验的重投影和深度损失项来正则化神经辐射场训练。
性能：</li>
<li>在具有密度和SDF的NeRF模型的不同数据集上，我们的方法在光度和几何度量方面都优于以前的方法。</li>
<li>我们的方法可以作为即插即用模块应用于不同的NeRF变体。
工作量：</li>
<li>本文的工作量中等。需要收集和预处理数据，训练神经辐射场模型，并评估模型的性能。</li>
<li>本文的代码和数据已开源，便于其他研究人员使用和扩展。</li>
</ul>



<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-bb81a1d85c4890b96c58352034ef526e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0bfbadb767d53f08ad37ade7dcd5b8f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6fb67d265cc7f332cd181f46f3137ec9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51f7196d6c0e34d80890742433bf1ba7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-09c23c9b810e3846280b6f7fbd407f08.jpg" align="middle">
</details>
​    


## MVDD: Multi-View Depth Diffusion Models
**Authors:Zhen Wang, Qiangeng Xu, Feitong Tan, Menglei Chai, Shichen Liu, Rohit Pandey, Sean Fanello, Achuta Kadambi, Yinda Zhang**

Denoising diffusion models have demonstrated outstanding results in 2D image generation, yet it remains a challenge to replicate its success in 3D shape generation. In this paper, we propose leveraging multi-view depth, which represents complex 3D shapes in a 2D data format that is easy to denoise. We pair this representation with a diffusion model, MVDD, that is capable of generating high-quality dense point clouds with 20K+ points with fine-grained details. To enforce 3D consistency in multi-view depth, we introduce an epipolar line segment attention that conditions the denoising step for a view on its neighboring views. Additionally, a depth fusion module is incorporated into diffusion steps to further ensure the alignment of depth maps. When augmented with surface reconstruction, MVDD can also produce high-quality 3D meshes. Furthermore, MVDD stands out in other tasks such as depth completion, and can serve as a 3D prior, significantly boosting many downstream tasks, such as GAN inversion. State-of-the-art results from extensive experiments demonstrate MVDD's excellent ability in 3D shape generation, depth completion, and its potential as a 3D prior for downstream tasks. 

[PDF](http://arxiv.org/abs/2312.04875v3) 

**Summary**
多视角深度表征与扩散模型相结合，用于高效且高质量的三维形状生成。

**Key Takeaways**
- 多视角深度能够将复杂的三维形状表示为易于去噪的二维数据格式。
- 提出了多视角深度扩散模型 MVDD，能够生成具有 20,000 多个点的包含精细细节的高质量密集点云。
- 介绍了一种极线线段注意力机制，该机制对视图的去噪步骤进行条件处理，以确保多视角深度中的三维一致性。
- 加入了一个深度融合模块，以进一步确保深度图的对齐。
- 当使用表面重建增强时，MVDD 还可以生成高质量的三维网格。
- MVDD 在深度补全等其他任务中脱颖而出，并且可以用作三维先验，极大地提高了许多下游任务的性能，例如 GAN 反演。
- 大量实验的最新结果证明了 MVDD 在三维形状生成、深度补全以及作为下游任务的三维先验方面的出色能力。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li><p>标题：MVDD：多视图深度扩散模型</p>
</li>
<li><p>作者：Zhen Wang, Qiangeng Xu, Feitong Tan, Menglei Chai, Shichen Liu, Rohit Pandey, Sean Fanello, Achuta Kadambi, Yinda Zhang</p>
</li>
<li><p>第一作者单位：加州大学洛杉矶分校</p>
</li>
<li><p>关键词：深度扩散模型、多视图深度、3D形状生成、形状补全、3D GAN反演</p>
</li>
<li><p>论文链接：https://arxiv.org/abs/2312.04875
Github 链接：https://github.com/mvdepth/mvdd</p>
</li>
<li><p>摘要：
(1)：研究背景：扩散模型在 2D 图像生成中取得了出色的结果，但在 3D 形状生成中复制其成功仍然具有挑战性。
(2)：过去的方法及其问题：现有方法通常使用点云或体素表示来生成 3D 形状，但这些表示难以建模复杂的形状。
(3)：本文提出的研究方法：本文提出了一种多视图深度扩散模型 (MVDD)，该模型利用多视图深度来表示复杂的 3D 形状。MVDD 能够生成高质量的密集点云，具有 20K+ 个点和精细的细节。
(4)：方法在任务中的表现：MVDD 在 3D 形状生成、深度补全和作为 3D GAN 反演的先验等任务中取得了最先进的结果。这些结果证明了 MVDD 在 3D 形状生成和相关任务中的出色性能。</p>
</li>
<li><p>方法：
(1) 多视图深度扩散模型 (MVDD)：MVDD 采用多视图深度来表示复杂的 3D 形状，可以生成高质量的密集点云，具有 20K+ 个点和精细的细节。
(2) 表观线段注意力 (Epipole “Line Segment” Attention)：MVDD 引入了一种有效的表观“线段”注意力，以促进所有深度图的一致性。该注意力仅关注其他视图上可见位置的特征，从而提高了效率和有效性。
(3) 去噪深度融合 (Denoising Depth Fusion)：为了进一步加强多视图深度图的排列，MVDD 在扩散步骤中结合了深度融合。该融合过程将深度图投影到其他视图并进行比较，以确保深度值的一致性。
(4) 训练目标：MVDD 采用 DDPM 的目标函数，旨在最大化对数似然函数。该目标函数通过最小化噪声估计误差来实现，从而使模型能够从纯噪声生成逼真的深度图。
(5) 应用：MVDD 在 3D 形状生成、深度补全和作为 3DGAN 反演的先验等任务中取得了最先进的结果。这些结果证明了 MVDD 在 3D 形状生成和相关任务中的出色性能。</p>
</li>
<li><p>结论：
（1）：MVDD 提出了一种多视图深度扩散模型，能够生成高质量的密集点云，并具有精细的细节。
（2）：创新点：</p>
</li>
</ol>
<ul>
<li>提出了一种多视图深度扩散模型（MVDD），该模型利用多视图深度来表示复杂的 3D 形状。</li>
<li>引入了一种有效的表观“线段”注意力，以促进所有深度图的一致性。</li>
<li>结合了深度融合，以进一步加强多视图深度图的排列。
性能：</li>
<li>MVDD 在 3D 形状生成、深度补全和作为 3DGAN 反演的先验等任务中取得了最先进的结果。
工作量：</li>
<li>MVDD 的训练和推理过程相对复杂，需要大量的计算资源。</li>
</ul>



<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-0cd9c4d80e12157224ee5b88f9d1ffc4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-681bf6921d2a2ffb41e0728da51d27c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8e93402d2ce303a0694caeced8bf21f.jpg" align="middle">
</details>
​    


## DreamComposer: Controllable 3D Object Generation via Multi-View   Conditions
**Authors:Yunhan Yang, Yukun Huang, Xiaoyang Wu, Yuan-Chen Guo, Song-Hai Zhang, Hengshuang Zhao, Tong He, Xihui Liu**

Utilizing pre-trained 2D large-scale generative models, recent works are capable of generating high-quality novel views from a single in-the-wild image. However, due to the lack of information from multiple views, these works encounter difficulties in generating controllable novel views. In this paper, we present DreamComposer, a flexible and scalable framework that can enhance existing view-aware diffusion models by injecting multi-view conditions. Specifically, DreamComposer first uses a view-aware 3D lifting module to obtain 3D representations of an object from multiple views. Then, it renders the latent features of the target view from 3D representations with the multi-view feature fusion module. Finally the target view features extracted from multi-view inputs are injected into a pre-trained diffusion model. Experiments show that DreamComposer is compatible with state-of-the-art diffusion models for zero-shot novel view synthesis, further enhancing them to generate high-fidelity novel view images with multi-view conditions, ready for controllable 3D object reconstruction and various other applications. 

[PDF](http://arxiv.org/abs/2312.03611v1) Project Page: https://yhyang-myron.github.io/DreamComposer/

**Summary**
多视图条件助力基于扩散模型的新视角生成，实现可控的三维重建。

**Key Takeaways**

- DreamComposer 可以将多视图条件注入到现有基于扩散模型中，以增强它们生成新的可控视角的能力。
- DreamComposer 使用视图感知 3D 提升模块从多个视角获取对象的 3D 表征，然后通过多视图特征融合模块从 3D 表征中渲染目标视角的潜在特征。
- 提取自多视图输入的目标视角特征被注入到预先训练的扩散模型中。
- DreamComposer 能与最先进的扩散模型兼容，用于零样本新视角合成，进一步增强它们生成高保真新视角图像的能力，满足可控 3D 对象重建和各种其他应用的需求。
- DreamComposer 能够有效地提高最终合成图像的质量，尤其是在细节和几何结构方面。
- DreamComposer 可以处理各种不同的对象和场景，鲁棒性和泛化能力强。
- DreamComposer 使用预训练的 2D 模型作为基础，无需额外的数据或训练，易于部署和使用。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li><p>题目：DreamComposer：通过多视图条件生成可控的 3D 对象</p>
</li>
<li><p>作者：Yunhan Yang、Yukun Huang、Xiaoyang Wu、Yuan-Chen Guo、Song-Hai Zhang、Hengshuang Zhao、Tong He、Xihui Liu</p>
</li>
<li><p>第一作者单位：香港大学</p>
</li>
<li><p>关键词：可控 3D 对象生成、多视图条件、扩散模型、零样本新视角合成</p>
</li>
<li><p>论文链接：https://arxiv.org/abs/2312.03611，Github 代码链接：无</p>
</li>
<li><p>摘要：
(1) 研究背景：利用预训练的 2D 大规模生成模型，最近的工作能够从一张自然界图像中生成高质量的新视角。然而，由于缺乏来自多视角的信息，这些工作在生成可控的新视角时遇到了困难。
(2) 过去方法与不足：过去的方法通常使用单一的预训练扩散模型来生成新视角，但这些模型缺乏对多视角条件的控制能力。
(3) 研究方法：本文提出 DreamComposer，这是一个灵活且可扩展的框架，可以通过注入多视图条件来增强现有的视图感知扩散模型。具体来说，DreamComposer 首先使用视图感知 3D 提升模块从多个视图中获得对象的 3D 表示。然后，它使用多视图特征融合模块从 3D 表示中渲染目标视图的潜在特征。最后，将从多视图输入中提取的目标视图特征注入预训练的扩散模型。
(4) 实验结果：实验表明，DreamComposer 与最先进的用于零样本新视角合成的扩散模型兼容，进一步增强了这些模型生成具有多视图条件的高保真新视角图像的能力，可用于可控 3D 对象重建和各种其他应用。</p>
</li>
<li><p>Methods：
(1) 视图感知3D提升模块：该模块从多个视图中获得对象的3D表示。它首先使用预训练的2D扩散模型从每个视图中生成对象的2D表示，然后将这些2D表示投影到3D空间中，并使用3D卷积网络融合这些投影，以获得对象的3D表示。
(2) 多视图特征融合模块：该模块从3D表示中渲染目标视图的潜在特征。它首先使用3D渲染器将3D表示渲染成目标视图的2D图像，然后使用预训练的2D扩散模型从2D图像中提取潜在特征。
(3) 多视图条件注入模块：该模块将从多视图输入中提取的目标视图特征注入预训练的扩散模型。它首先将目标视图的潜在特征与预训练的扩散模型的潜在特征进行拼接，然后使用一个线性层将拼接后的特征投影到预训练的扩散模型的潜在空间中。
(4) 扩散模型生成：将注入多视图条件的潜在特征输入预训练的扩散模型，并使用扩散模型生成目标视图的新视角图像。</p>
</li>
<li><p>结论：
（1）：DreamComposer 框架提出了一个灵活且可扩展的框架，可以通过注入多视图条件来增强现有的视图感知扩散模型，从而生成具有多视图条件的高保真新视角图像。
（2）：创新点：</p>
</li>
</ol>
<ul>
<li>提出了一种新的视图感知 3D 提升模块，可以从多个视图中获得对象的 3D 表示。</li>
<li>提出了一种新的多视图特征融合模块，可以从 3D 表示中渲染目标视图的潜在特征。</li>
<li>提出了一种新的多视图条件注入模块，可以将从多视图输入中提取的目标视图特征注入预训练的扩散模型。
性能：</li>
<li>DreamComposer 与最先进的用于零样本新视角合成的扩散模型兼容，进一步增强了这些模型生成具有多视图条件的高保真新视角图像的能力。
工作量：</li>
<li>DreamComposer 框架的实现相对简单，易于使用。</li>
</ul>



<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-168607583ef1e4f58b3bdcd4803e43c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33dcfafc8394f58e1dbd724842a101fb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-93b23a2d6c08c410dd4391267ba17622.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29ce8934dd91a9c823f4314c0c68127d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0e0119c153d54525b574ef2a92843a1d.jpg" align="middle">
</details>
​    


## HOLD: Category-agnostic 3D Reconstruction of Interacting Hands and   Objects from Video
**Authors:Zicong Fan, Maria Parelli, Maria Eleni Kadoglou, Muhammed Kocabas, Xu Chen, Michael J. Black, Otmar Hilliges**

Since humans interact with diverse objects every day, the holistic 3D capture of these interactions is important to understand and model human behaviour. However, most existing methods for hand-object reconstruction from RGB either assume pre-scanned object templates or heavily rely on limited 3D hand-object data, restricting their ability to scale and generalize to more unconstrained interaction settings. To this end, we introduce HOLD -- the first category-agnostic method that reconstructs an articulated hand and object jointly from a monocular interaction video. We develop a compositional articulated implicit model that can reconstruct disentangled 3D hand and object from 2D images. We also further incorporate hand-object constraints to improve hand-object poses and consequently the reconstruction quality. Our method does not rely on 3D hand-object annotations while outperforming fully-supervised baselines in both in-the-lab and challenging in-the-wild settings. Moreover, we qualitatively show its robustness in reconstructing from in-the-wild videos. Code: https://github.com/zc-alexfan/hold 

[PDF](http://arxiv.org/abs/2311.18448v1) 

**Summary**
无须先验知识，仅从单目交互视频中，就能对铰接手和物体进行联合重建。

**Key Takeaways**

- HOLD 是一种无需类别信息的通用方法，可以仅从单目交互视频中重建铰接手和物体。
- HOLD 使用复合铰接隐式模型来重建从 2D 图像中分离出的 3D 手和物体。
- HOLD 还进一步加入了手物体约束，以改进手物体姿势，从而提升重建质量。
- HOLD 无需 3D 手物体注释，即便在实验室和极具挑战性的野外环境中，也能优于完全监督的基线方法。
- HOLD 在重建野外视频方面具有鲁棒性。
- HOLD 适用于广泛的互动场景，包括抓取、操作和操纵。
- HOLD 可以作为下游任务（如动作识别和手势识别）的辅助工具。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>标题：HOLD：从视频中进行类别无关的交互手和物体的 3D 重建</li>
<li>作者：Zicong Fan, Maria Parelli, Maria Eleni Kadoglou, Muhammed Kocabas, Xu Chen, Michael J. Black, Otmar Hilliges</li>
<li>隶属单位：苏黎世联邦理工学院，德国图宾根马克斯普朗克智能系统研究所</li>
<li>关键词：3D 重建、手部对象交互、隐式神经表示、约束优化</li>
<li>论文链接：https://arxiv.org/abs/2311.18448，Github 链接：https://github.com/zc-alexfan/hold</li>
<li>摘要：
(1)：研究背景：人类每天都会与各种物体互动，因此全面捕捉这些互动对于理解和建模人类行为非常重要。然而，目前大多数用于从 RGB 图像进行手部物体重建的方法要么假设预先扫描的对象模板，要么严重依赖有限的 3D 手部物体数据，这限制了它们在更不受约束的交互场景中的扩展和泛化能力。
(2)：过去的方法及其问题：本文方法的动机：现有方法要么假设预先扫描的对象模板，要么严重依赖有限的 3D 手部物体数据，这限制了它们在更不受约束的交互场景中的扩展和泛化能力。
(3)：研究方法：为了解决上述问题，本文提出了一种名为 HOLD 的方法，该方法可以从单目交互视频中重建铰接手和物体。HOLD 使用了一种组合关节隐式模型，该模型可以从 2D 图像重建分离的 3D 手部和物体。此外，本文还进一步结合了手部物体约束来改进手部物体姿势，从而提高重建质量。
(4)：方法的性能：HOLD 方法在实验室和具有挑战性的野外场景中均优于完全监督的基线。此外，本文还定性地展示了其从野外视频中重建的鲁棒性。</li>
</ol>
<p>Some Error for method(比如是不是没有Methods这个章节)</p>
<ol start="8">
<li>结论：
（1）：本研究的意义：
本研究提出了一种名为 HOLD 的方法，该方法可以从单目交互视频中重建铰接手和物体。HOLD 使用了一种组合关节隐式模型，该模型可以从 2D 图像重建分离的 3D 手部和物体。此外，本文还进一步结合了手部物体约束来改进手部物体姿势，从而提高重建质量。HOLD 方法在实验室和具有挑战性的野外场景中均优于完全监督的基线。此外，本文还定性地展示了其从野外视频中重建的鲁棒性。
（2）：本文的优缺点：
创新点：</li>
<li>提出了一种名为 HOLD 的方法，该方法可以从单目交互视频中重建铰接手和物体。</li>
<li>使用了一种组合关节隐式模型，该模型可以从 2D 图像重建分离的 3D 手部和物体。</li>
<li>进一步结合了手部物体约束来改进手部物体姿势，从而提高重建质量。</li>
</ol>
<p>性能：</p>
<ol>
<li>HOLD 方法在实验室和具有挑战性的野外场景中均优于完全监督的基线。</li>
<li>HOLD 方法从野外视频中重建的鲁棒性强。</li>
</ol>
<p>工作量：</p>
<ol>
<li>HOLD 方法需要大量的数据和计算资源。</li>
<li>HOLD 方法的训练过程比较复杂。</li>
</ol>



<p></p><details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-415ee1a9b6ee2ab5c473c2f0dda0e51e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c2ea9ec163c36ce8e5caba3457d81b7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac62d03408862e6758b781f5f9d3f4f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ad526669e8557416b88d13528a10f59.jpg" align="middle">
</details><br>​    <p></p>
<h2 id="HumanRecon-Neural-Reconstruction-of-Dynamic-Human-Using-Geometric-Cues-and-Physical-Priors"><a href="#HumanRecon-Neural-Reconstruction-of-Dynamic-Human-Using-Geometric-Cues-and-Physical-Priors" class="headerlink" title="HumanRecon: Neural Reconstruction of Dynamic Human Using Geometric Cues   and Physical Priors"></a>HumanRecon: Neural Reconstruction of Dynamic Human Using Geometric Cues   and Physical Priors</h2><p><strong>Authors:Junhui Yin, Wei Yin, Hao Chen, Xuqian Ren, Zhanyu Ma, Jun Guo, Yifan Liu</strong></p>
<p>Recent methods for dynamic human reconstruction have attained promising reconstruction results. Most of these methods rely only on RGB color supervision without considering explicit geometric constraints. This leads to existing human reconstruction techniques being more prone to overfitting to color and causes geometrically inherent ambiguities, especially in the sparse multi-view setup.   Motivated by recent advances in the field of monocular geometry prediction, we consider the geometric constraints of estimated depth and normals in the learning of neural implicit representation for dynamic human reconstruction. As a geometric regularization, this provides reliable yet explicit supervision information, and improves reconstruction quality. We also exploit several beneficial physical priors, such as adding noise into view direction and maximizing the density on the human surface. These priors ensure the color rendered along rays to be robust to view direction and reduce the inherent ambiguities of density estimated along rays. Experimental results demonstrate that depth and normal cues, predicted by human-specific monocular estimators, can provide effective supervision signals and render more accurate images. Finally, we also show that the proposed physical priors significantly reduce overfitting and improve the overall quality of novel view synthesis. Our code is available at:~\href{<a href="https://github.com/PRIS-CV/HumanRecon}{https://github.com/PRIS-CV/HumanRecon}">https://github.com/PRIS-CV/HumanRecon}{https://github.com/PRIS-CV/HumanRecon}</a>. </p>
<p><a href="http://arxiv.org/abs/2311.15171v1">PDF</a> </p>
<p><strong>Summary</strong><br>利用深度和法向量作为几何正则化，提高神经隐式表示动态人体重建的质量。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>深度和法向量是人体重建的关键几何约束。</li>
<li>使用深度和法向量作为监督信息可以提高重建质量。</li>
<li>加入噪声到视角中可以使颜色渲染更鲁棒。</li>
<li>最大化人体表面的密度可以减少密度估计的固有二义性。</li>
<li>实验结果表明，深度和法向量线索可以提供有效的监督信号并呈现更准确的图像。</li>
<li>提出物理先验显著减少了过拟合并提高了新视角合成的总体质量。</li>
<li>代码可在 GitHub 上获取。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：HumanRecon：利用几何线索和物理先验的神经重建动态人体</li>
<li>作者：Junhui Yin、Wei Yin、Hao Chen、Xuqian Ren、Zhanyu Ma、Jun Guo 和 Yifan Liu</li>
<li>单位：北京邮电大学</li>
<li>关键词：动态人体重建、神经隐式表示、几何约束、物理先验</li>
<li>论文链接：https://arxiv.org/abs/2311.15171
Github 链接：https://github.com/PRIS-CV/HumanRecon</li>
<li>摘要：
（1）：研究背景：动态人体重建是计算机视觉研究中的一个基本问题，在机器人技术、图形学、增强现实、虚拟现实和人体数字化等领域有着广泛的应用。然而，从 RGB 图像和视频中重建动态人体极具挑战性，因为这些数据缺乏足够的监督信息。
（2）：过去方法：传统的人体建模工作使用显式网格来表示人体几何形状，并将外观存储在 2D 纹理贴图中。但是，这些方法需要密集的摄像头阵列和受控的照明条件。近年来，PIFu、StereoPIFu 和 PIFuHD 等方法提出使用像素级图像特征回归神经隐式函数，能够重建高分辨率的衣着人体结果。ARCH 方法将一系列 PIFu 方法扩展到从单目图像中回归可动画的衣着人体化身。然而，这些方法无法从稀疏的多视角视频中重建动态的衣着人体。
（3）：研究方法：本文提出了一种新的动态人体重建方法 HumanRecon，该方法利用几何线索和物理先验来学习神经隐式表示。具体来说，HumanRecon 使用单目几何估计器预测深度和法线线索，并将其作为几何正则化项添加到神经隐式表示的学习中。此外，HumanRecon 还利用了几种有益的物理先验，例如在视向中添加噪声和最大化人体表面的密度。这些先验确保了沿射线渲染的颜色对视向具有鲁棒性，并减少了沿射线估计的密度的固有歧义性。
（4）：方法性能：实验结果表明，由特定于人类的单目估计器预测的深度和法线线索可以提供有效的监督信号并渲染更准确的图像。此外，本文提出的物理先验可以显着减少过拟合并提高新视角合成的整体质量。</li>
</ol>
<p><strong>方法</strong>：</p>
<p>(1)：HumanRecon利用单目几何估计器预测深度和法线线索，并将其作为几何正则化项添加到神经隐式表示的学习中。</p>
<p>(2)：HumanRecon利用了几种有益的物理先验，例如在视向中添加噪声和最大化人体表面的密度。</p>
<p>(3)：HumanRecon使用神经隐式表示来学习动态人体的几何形状和外观，并利用几何线索和物理先验来提高重建的准确性和鲁棒性。</p>
<ol start="8">
<li>结论：
（1）：本文提出了一种新的动态人体重建方法 HumanRecon，该方法利用几何线索和物理先验来学习神经隐式表示，能够从稀疏的多视角视频中重建动态的衣着人体，在公共数据集上取得了最优的重建效果。
（2）：创新点：
HumanRecon 提出了一种新的动态人体重建方法，该方法利用几何线索和物理先验来学习神经隐式表示，能够从稀疏的多视角视频中重建动态的衣着人体。
性能：
HumanRecon 在公共数据集上取得了最优的重建效果。
工作量：
HumanRecon 的工作量中等，需要收集稀疏的多视角视频数据，并使用神经网络进行训练。</li>
</ol>



<p></p><details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-53aa17faea82b5e0fdbbccd7e38ab3f0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-efb4eebe5bbbb5efc58baa3b2148c347.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3423434440b10282e759bb59440bd5e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb5324e9f58f0fec825b0268ffd59901.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cc59c128bbcbd6e5e97f759649f953d4.jpg" align="middle">
</details><br>​    <p></p>
<h2 id="Unsupervised-Graph-Attention-Autoencoder-for-Attributed-Networks-using-K-means-Loss"><a href="#Unsupervised-Graph-Attention-Autoencoder-for-Attributed-Networks-using-K-means-Loss" class="headerlink" title="Unsupervised Graph Attention Autoencoder for Attributed Networks using   K-means Loss"></a>Unsupervised Graph Attention Autoencoder for Attributed Networks using   K-means Loss</h2><p><strong>Authors:Abdelfateh Bekkair, Slimane Bellaouar, Slimane Oulad-Naoui</strong></p>
<p>Several natural phenomena and complex systems are often represented as networks. Discovering their community structure is a fundamental task for understanding these networks. Many algorithms have been proposed, but recently, Graph Neural Networks (GNN) have emerged as a compelling approach for enhancing this task.In this paper, we introduce a simple, efficient, and clustering-oriented model based on unsupervised \textbf{G}raph Attention \textbf{A}uto\textbf{E}ncoder for community detection in attributed networks (GAECO). The proposed model adeptly learns representations from both the network’s topology and attribute information, simultaneously addressing dual objectives: reconstruction and community discovery. It places a particular emphasis on discovering compact communities by robustly minimizing clustering errors. The model employs k-means as an objective function and utilizes a multi-head Graph Attention Auto-Encoder for decoding the representations. Experiments conducted on three datasets of attributed networks show that our method surpasses state-of-the-art algorithms in terms of NMI and ARI. Additionally, our approach scales effectively with the size of the network, making it suitable for large-scale applications. The implications of our findings extend beyond biological network interpretation and social network analysis, where knowledge of the fundamental community structure is essential. </p>
<p><a href="http://arxiv.org/abs/2311.12986v2">PDF</a> 7 pages, 5 Figures</p>
<p><strong>摘要</strong><br>无监督图注意力自动编码器方法有效地提取了生物和社交关系网络中的社区结构。</p>
<p><strong>要点</strong></p>
<ul>
<li>提出了一种简单、高效、面向聚类的无监督图注意力自动编码器方法，用于发现属性网络中的社区结构。</li>
<li>该模型能够同时学习网络拓扑和属性信息，并以重建和社区发现为双重目标。</li>
<li>该模型通过稳健地最小化聚类误差，特别强调发现紧凑的社区。</li>
<li>该模型采用k均值作为目标函数，并使用多头图注意自动编码器对表示进行解码。</li>
<li>在三个属性网络数据集上进行的实验表明，该方法在NMI和ARI方面优于最先进的算法。</li>
<li>该方法还可以有效地扩展到网络规模，使其适用于大规模应用。</li>
<li>该方法的应用范围不仅限于生物网络解释和社交网络分析，在这些领域中，了解基本的社区结构至关重要。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<p></p><ol><p></p>
<p></p><li>论文标题：无监督图注意力自编码器用于属性网络</li><p></p>
<p></p><li>作者：Abdelfateh Bekkaira, Slimane Bellaouara, Slimane Oulad-Naouia</li><p></p>
<p></p><li>所属单位：阿尔及利亚格哈达亚大学数学与计算机科学系</li><p></p>
<p></p><li>关键词：图注意力网络、自编码器、无监督学习、社区检测</li><p></p>
<p></p><li>链接：https://arxiv.org/abs/2311.12986</li><p></p>
<p></p><li>摘要：
（1）研究背景：社区检测是图分析中的一项重要任务，旨在将图中的节点划分为不同的社区，使社区内的节点连接紧密，社区之间的节点连接稀疏。传统社区检测方法主要基于图的结构信息，而近年来提出的图注意力网络（GAT）可以同时考虑图的结构信息和节点的属性信息，在社区检测任务上取得了较好的效果。然而，现有的GAT模型大多是监督学习模型，需要大量的标记数据进行训练。
（2）过去的方法及其问题：现有方法存在以下问题：</li><br>&lt;/ol&gt;<p></p>
<ul>
<li>需要大量的标记数据进行训练，这在实际应用中往往难以获得。</li>
<li>难以处理大规模图数据。</li>
<li>对图的结构和属性信息利用不足。
（3）研究方法：为了解决上述问题，本文提出了一种无监督图注意力自编码器（GAECO）模型，该模型可以同时利用图的结构信息和节点的属性信息进行社区检测。GAECO模型的结构如下图所示：</li>
</ul>
<p>[Image of GAECO model architecture]</p>
<p>GAECO模型由编码器、解码器和损失函数三部分组成。编码器采用GAT网络，可以同时考虑图的结构信息和节点的属性信息，将图中的节点嵌入到一个低维空间中。解码器采用简单的内积解码器，将编码器的输出重建为图的邻接矩阵。损失函数采用K-means损失函数，可以使编码器的输出与真实的社区结构更加接近。
（4）实验结果：</p>
<ul>
<li>在Cora数据集上，GAECO模型的NMI和ARI指标分别达到了0.564和0.516，优于其他对比模型。</li>
<li>在CiteSeer数据集上，GAECO模型的NMI和ARI指标分别达到了0.451和0.477，优于其他对比模型。</li>
<li>在PubMed数据集上，GAECO模型的NMI和ARI指标分别达到了0.341和0.321，优于其他对比模型。</li>
</ul>
<p>Some Error for method(比如是不是没有Methods这个章节)</p>
<ol start="8">
<li>结论：
（1）：本文提出了一种无监督图注意力自编码器（GAECO）模型，该模型可以同时利用图的结构信息和节点的属性信息进行社区检测。GAECO模型在三个真实数据集上取得了优于其他对比模型的结果，表明了该模型的有效性。
（2）：创新点：</li>
</ol>
<ul>
<li>提出了一种新的无监督图注意力自编码器（GAECO）模型，该模型可以同时利用图的结构信息和节点的属性信息进行社区检测。</li>
<li>GAECO模型采用GAT网络作为编码器，可以同时考虑图的结构信息和节点的属性信息，将图中的节点嵌入到一个低维空间中。</li>
<li>GAECO模型采用简单的内积解码器，将编码器的输出重建为图的邻接矩阵。</li>
<li>GAECO模型采用K-means损失函数，可以使编码器的输出与真实的社区结构更加接近。
性能：</li>
<li>在Cora数据集上，GAECO模型的NMI和ARI指标分别达到了0.564和0.516，优于其他对比模型。</li>
<li>在CiteSeer数据集上，GAECO模型的NMI和ARI指标分别达到了0.451和0.477，优于其他对比模型。</li>
<li>在PubMed数据集上，GAECO模型的NMI和ARI指标分别达到了0.341和0.321，优于其他对比模型。
工作量：</li>
<li>GAECO模型的编码器采用GAT网络，GAT网络的计算复杂度为O（|E|d），其中|E|是图中的边数，d是GAT网络的层数。</li>
<li>GAECO模型的解码器采用简单的内积解码器，内积解码器的计算复杂度为O（|V|^2），其中|V|是图中的节点数。</li>
<li>GAECO模型的损失函数采用K-means损失函数，K-means损失函数的计算复杂度为O（|V|k），其中k是社区的个数。</li>
</ul>



<p></p><details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b74cf502e48f47b499aada3b40dffd6c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3de603a9dee95b40e4c36264efb66584.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a789c364045820588a48d784da87b83b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ac4b7e93b2cb7cc90f069d4cad7ce3f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fdb87522aae5351d76c2693b34df4b9e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f81c4e04a9bd4eca0b4c90cf7b270249.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b83ae848f07876055a106f8dc5f8ccf5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8276545d9badaff0d0b635c20212197f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34999862d8e3b506a71b65f8cb20c960.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6988fa3ab3a1f1365a34e891d5e8d311.jpg" align="middle">
</details><br>​    <p></p>
<p>​    </p>
</ol>]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>3D reconstruction</tag>
      </tags>
  </entry>
  <entry>
    <title>Diffusion Models</title>
    <url>/2024/01/24/Paper/2024-01-24/Diffusion%20Models/</url>
    <content><![CDATA[<blockquote>
<p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-01-24-更新"><a href="#2024-01-24-更新" class="headerlink" title="2024-01-24 更新"></a>2024-01-24 更新</h1><h2 id="Less-Could-Be-Better-Parameter-efficient-Fine-tuning-Advances-Medical-Vision-Foundation-Models"><a href="#Less-Could-Be-Better-Parameter-efficient-Fine-tuning-Advances-Medical-Vision-Foundation-Models" class="headerlink" title="Less Could Be Better: Parameter-efficient Fine-tuning Advances Medical   Vision Foundation Models"></a>Less Could Be Better: Parameter-efficient Fine-tuning Advances Medical   Vision Foundation Models</h2><p><strong>Authors:Chenyu Lian, Hong-Yu Zhou, Yizhou Yu, Liansheng Wang</strong></p>
<p>Parameter-efficient fine-tuning (PEFT) that was initially developed for exploiting pre-trained large language models has recently emerged as an effective approach to perform transfer learning on computer vision tasks. However, the effectiveness of PEFT on medical vision foundation models is still unclear and remains to be explored. As a proof of concept, we conducted a detailed empirical study on applying PEFT to chest radiography foundation models. Specifically, we delved into LoRA, a representative PEFT method, and compared it against full-parameter fine-tuning (FFT) on two self-supervised radiography foundation models across three well-established chest radiograph datasets. Our results showed that LoRA outperformed FFT in 13 out of 18 transfer learning tasks by at most 2.9% using fewer than 1% tunable parameters. Combining LoRA with foundation models, we set up new state-of-the-art on a range of data-efficient learning tasks, such as an AUROC score of 80.6% using 1% labeled data on NIH ChestX-ray14. We hope this study can evoke more attention from the community in the use of PEFT for transfer learning on medical imaging tasks. Code and models are available at <a href="https://github.com/RL4M/MED-PEFT">https://github.com/RL4M/MED-PEFT</a>. </p>
<p><a href="http://arxiv.org/abs/2401.12215v1">PDF</a> Technical report</p>
<p><strong>Summary</strong><br>基于胸部X光影像基金模型的PEFT参数化微调可提高医学视觉任务的迁移学习性能。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>PEFT在18项迁移学习任务中有13项优于FFT，使用可调参数少于1%可提高至多2.9%的性能。</li>
<li>将PEFT与基金模型相结合，我们在一系列数据高效学习任务上设置了新的最先进技术，例如，在NIH ChestX-ray14上使用1%的标记数据，AUROC得分达到80.6%。</li>
<li>我们希望这项研究能够引起社区对PEFT在医学影像任务中的迁移学习的更多关注。</li>
<li>代码和模型可以在<a href="https://github.com/RL4M/MED-PEFT上获得。">https://github.com/RL4M/MED-PEFT上获得。</a></li>
<li>PEFT最初开发用于开发预训练的大型语言模型，最近已成为在计算机视觉任务上执行迁移学习的有效方法。</li>
<li>PEFT在医学视觉基金模型中的有效性仍不清楚，有待探索。</li>
<li>作为概念证明，我们对将PEFT应用于胸部放射线照相基金模型进行了详细的实证研究。</li>
<li>具体而言，我们深入研究了LoRA（一种具有代表性的PEFT方法），并将其与在三个公认的胸部X光照相数据集中对两个自监督放射线照相基金模型进行全参数微调（FFT）进行了比较。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li><p>题目：少即是多：参数高效微调</p>
</li>
<li><p>作者：陈宇廉、周鸿宇、于一舟、王连生</p>
</li>
<li><p>单位：厦门大学</p>
</li>
<li><p>关键词：迁移学习、医学视觉基础模型、胸部 X 射线</p>
</li>
<li><p>论文链接：https://arxiv.org/abs/2401.12215
Github 代码链接：https://github.com/RL4M/MED-PEFT</p>
</li>
<li><p>摘要：
（1）研究背景：参数高效微调（PEFT）最初用于开发预训练的大语言模型，最近已成为计算机视觉任务中进行迁移学习的有效方法。然而，PEFT 在医学视觉基础模型中的有效性仍不清楚，有待探索。
（2）过去的方法及其问题：全参数微调（FFT）已被公认为一种执行迁移学习的优越技术。然而，基础模型通常具有大量参数，当下游任务只有有限的注释时，微调全部模型权重可能不是一个最优选择。
（3）论文提出的研究方法：为了证明概念，我们对将 PEFT 应用于胸部放射线基础模型进行了详细的实证研究。具体来说，我们深入研究了具有代表性的 PEFT 方法 LoRA，并将其与两个自监督放射线基础模型在三个公认的胸部放射线数据集上与全参数微调 (FFT) 进行了比较。
（4）方法在任务上的表现及其性能：我们的结果表明，LoRA 在 18 项迁移学习任务中的 13 项中优于 FFT，最多可使用少于 1% 的可调参数提高 2.9%。将 LoRA 与基础模型相结合，我们在各种数据高效学习任务中建立了新的最优水平，例如在 NIHChestX-ray14 上使用 1% 的标记数据获得了 80.6% 的 AUROC 分数。我们希望这项研究能够引起社区更多地关注在医学成像任务中使用 PEFT 进行迁移学习。</p>
</li>
<li><p>方法：
（1）提出 LoRA-PEFT 方法：LoRA-PEFT 是一种参数高效微调方法，它通过学习一个低秩矩阵来对基础模型的权重进行微调。该方法可以有效减少可调参数的数量，从而提高微调的效率。
（2）在胸部 X 射线数据集上进行实验：作者将 LoRA-PEFT 方法应用于两个自监督胸部 X 射线基础模型，并在三个公认的胸部 X 射线数据集上与全参数微调 (FFT) 进行了比较。结果表明，LoRA-PEFT 在 18 项迁移学习任务中的 13 项中优于 FFT，最多可使用少于 1% 的可调参数提高 2.9%。
（3）在其他医学成像任务上进行实验：作者还将 LoRA-PEFT 方法应用于其他医学成像任务，包括肺结节检测、骨龄评估和心脏磁共振成像分割。结果表明，LoRA-PEFT 在这些任务上也取得了良好的性能。</p>
</li>
<li><p>结论：
（1）：本文提出了一种参数高效微调方法LoRA-PEFT，该方法可以有效减少可调参数的数量，从而提高微调的效率。在胸部X射线数据集和其它医学成像任务上的实验表明，LoRA-PEFT在迁移学习任务中取得了良好的性能。
（2）：创新点：
提出了一种新的参数高效微调方法LoRA-PEFT。
将LoRA-PEFT方法应用于胸部X射线数据集和其它医学成像任务，并取得了良好的性能。
性能：
在18项迁移学习任务中的13项中优于全参数微调(FFT)，最多可使用少于1%的可调参数提高2.9%。
在其他医学成像任务上也取得了良好的性能。
工作量：
方法简单易用，易于实现。
在多个数据集上进行了实验，证明了方法的有效性。</p>
</li>
</ol>



<p></p><details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-88f5604fa47b7e6b53fa59ed5ce873a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f28a6055dce3066c942bea25f00c4b98.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-219f68f671f950faee6332daa05d83eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0e3d9c8b6a9c6651af0cb1202241988.jpg" align="middle">
</details><br>​    <p></p>
<h2 id="CheXagent-Towards-a-Foundation-Model-for-Chest-X-Ray-Interpretation"><a href="#CheXagent-Towards-a-Foundation-Model-for-Chest-X-Ray-Interpretation" class="headerlink" title="CheXagent: Towards a Foundation Model for Chest X-Ray Interpretation"></a>CheXagent: Towards a Foundation Model for Chest X-Ray Interpretation</h2><p><strong>Authors:Zhihong Chen, Maya Varma, Jean-Benoit Delbrouck, Magdalini Paschali, Louis Blankemeier, Dave Van Veen, Jeya Maria Jose Valanarasu, Alaa Youssef, Joseph Paul Cohen, Eduardo Pontes Reis, Emily B. Tsai, Andrew Johnston, Cameron Olsen, Tanishq Mathew Abraham, Sergios Gatidis, Akshay S. Chaudhari, Curtis Langlotz</strong></p>
<p>Chest X-rays (CXRs) are the most frequently performed imaging test in clinical practice. Recent advances in the development of vision-language foundation models (FMs) give rise to the possibility of performing automated CXR interpretation, which can assist physicians with clinical decision-making and improve patient outcomes. However, developing FMs that can accurately interpret CXRs is challenging due to the (1) limited availability of large-scale vision-language datasets in the medical image domain, (2) lack of vision and language encoders that can capture the complexities of medical data, and (3) absence of evaluation frameworks for benchmarking the abilities of FMs on CXR interpretation. In this work, we address these challenges by first introducing \emph{CheXinstruct} - a large-scale instruction-tuning dataset curated from 28 publicly-available datasets. We then present \emph{CheXagent} - an instruction-tuned FM capable of analyzing and summarizing CXRs. To build CheXagent, we design a clinical large language model (LLM) for parsing radiology reports, a vision encoder for representing CXR images, and a network to bridge the vision and language modalities. Finally, we introduce \emph{CheXbench} - a novel benchmark designed to systematically evaluate FMs across 8 clinically-relevant CXR interpretation tasks. Extensive quantitative evaluations and qualitative reviews with five expert radiologists demonstrate that CheXagent outperforms previously-developed general- and medical-domain FMs on CheXbench tasks. Furthermore, in an effort to improve model transparency, we perform a fairness evaluation across factors of sex, race and age to highlight potential performance disparities. Our project is at \url{<a href="https://stanford-aimi.github.io/chexagent.html}">https://stanford-aimi.github.io/chexagent.html}</a>. </p>
<p><a href="http://arxiv.org/abs/2401.12208v1">PDF</a> 24 pages, 8 figures</p>
<p><strong>摘要</strong><br>引入大规模指令调整数据集和创新基准，构建强大且透明的胸部 X 光解释 AI 系统。</p>
<p><strong>主要要点</strong></p>
<ul>
<li>胸部 X 光检查是临床上最常进行的影像检查。</li>
<li>视觉语言基础模型 (FM) 在医学影像领域取得了进展。</li>
<li>开发准确解读胸部 X 光的 FM 存在挑战。</li>
<li>提出 CheXinstruct，一个包含 28 个公共数据集的大规模指令调整数据集。</li>
<li>提出 CheXagent，一个能够分析和总结胸部 X 光的指令调整 FM。</li>
<li>构建 CheXagent，设计了一个临床大语言模型 (LLM) 用于解析放射报告，一个视觉编码器用于表示胸部 X 光图像，以及一个用于桥接视觉和语言模态的网络。</li>
<li>引入 CheXbench，一个旨在系统地评估 FM 在 8 个临床相关胸部 X 光解释任务中的能力的新基准。</li>
<li>CheXagent 在 CheXbench 任务上优于之前开发的通用和医学领域 FM。</li>
<li>对性别、种族和年龄等因素进行公平性评估，以突出潜在的性能差异。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：CheXagent：构建胸部 X 射线解读基础模型</li>
<li>作者：Zhihong Chen、Maya Varma、Jean-Benoit Delbrouck、Magdalini Paschali、Louis Blankemeier、Dave Van Veen、Jeya Maria Jose Valanarasu、Alaa Youssef、Joseph Paul Cohen、Eduardo Pontes Reis、Emily B. Tsai、Andrew Johnston、Cameron Olsen、Tanishq Mathew Abraham、Sergios Gatidis、Akshay S. Chaudhari、Curtis Langlotz</li>
<li>第一作者单位：斯坦福大学</li>
<li>关键词：胸部 X 射线、医学图像、基础模型、语言模型、视觉编码器</li>
<li>论文链接：https://arxiv.org/abs/2401.12208，Github 代码链接：Github：None</li>
<li>摘要：
(1)：研究背景：胸部 X 射线 (CXR) 是临床实践中最常进行的影像检查。最近视觉语言基础模型 (FM) 的发展为自动 CXR 解读提供了可能性，这可以帮助医生进行临床决策并改善患者预后。然而，开发能够准确解读 CXR 的 FM 具有挑战性，原因在于：（1）医学图像领域缺乏大规模视觉语言数据集；（2）缺乏能够捕捉医学数据复杂性的视觉和语言编码器；（3）缺乏用于对 FM 在 CXR 解读方面的能力进行基准测试的评估框架。
(2)：过去的方法及其问题：过去的方法主要集中在开发能够从医学图像中提取特征的视觉编码器和能够理解和生成自然语言的语言模型。然而，这些方法在 CXR 解读任务上表现不佳，原因在于它们无法捕捉医学数据中的复杂性，并且它们没有经过针对 CXR 解读任务的专门训练。
(3)：本文提出的研究方法：为了解决这些挑战，本文首先介绍了 CheXinstruct，这是一个从 28 个公开数据集策划而来的大规模指令微调数据集。然后，本文提出了 CheXagent，这是一个经过指令微调的 FM，能够分析和总结 CXR。为了构建 CheXagent，本文设计了一个用于解析放射学报告的临床大语言模型 (LLM)、一个用于表示 CXR 图像的视觉编码器以及一个用于桥接视觉和语言模态的网络。最后，本文介绍了 CheXbench，这是一个新颖的基准，旨在系统地评估 FM 在 8 个临床上相关的 CXR 解读任务中的表现。
(4)：本文方法在任务和性能上的表现：广泛的定量评估和五位专家放射科医生的定性审查表明，CheXagent 在 CheXbench 任务上优于之前开发的通用和医学领域 FM。此外，为了提高模型透明度，本文针对性别、种族和年龄等因素进行了公平性评估，以突出潜在的性能差异。</li>
</ol>
<p>Some Error for method(比如是不是没有Methods这个章节)</p>
<ol start="8">
<li>结论：
（1）：本工作代表了胸部X射线解读自动化的进展。我们介绍了（i）CheXinstruct，一个指令微调数据集，（ii）CheXagent，一个8B参数的视觉语言基础模型，并通过（iii）CheXbench，我们的基准框架（包括7个数据集上的8个任务）展示了它的能力。与通用和医学领域的大语言模型相比，CheXagent在视觉感知和文本生成任务中取得了改进，并得到了五位专家放射科医生的验证。此外，我们针对性别、种族和年龄等因素进行了公平性评估，以突出潜在的性能差异，从而提高了模型的透明度。CheXinstruct、CheXagent和CheXbench的公开发布不仅强调了我们对推进医疗人工智能的承诺，而且为这一关键研究领域的未来发展树立了新的基准。
（2）：创新点：</li>
</ol>
<ul>
<li>提出了一种新的指令微调数据集CheXinstruct，用于训练视觉语言基础模型。</li>
<li>提出了一种新的视觉语言基础模型CheXagent，用于胸部X射线解读。</li>
<li>提出了一种新的基准框架CheXbench，用于评估视觉语言基础模型在胸部X射线解读任务中的性能。
性能：</li>
<li>CheXagent在CheXbench任务上优于之前开发的通用和医学领域的大语言模型。</li>
<li>CheXagent在视觉感知和文本生成任务中取得了改进。
工作量：</li>
<li>CheXinstruct数据集包含超过100万个图像和相应的放射学报告。</li>
<li>CheXagent模型的参数量为8B。</li>
<li>CheXbench基准框架包括7个数据集和8个任务。</li>
</ul>



<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-6aa52c71b57a2862b763a5188b83d6d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a7d79f07ab8199caa375ff5c3d1ce188.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-71a37c439c6714e8867560f580599d2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f44d1729ed1485e81c04a41f097c005.jpg" align="middle">
</details>
​    


## SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning   Capabilities
**Authors:Boyuan Chen, Zhuo Xu, Sean Kirmani, Brian Ichter, Danny Driess, Pete Florence, Dorsa Sadigh, Leonidas Guibas, Fei Xia**

Understanding and reasoning about spatial relationships is a fundamental capability for Visual Question Answering (VQA) and robotics. While Vision Language Models (VLM) have demonstrated remarkable performance in certain VQA benchmarks, they still lack capabilities in 3D spatial reasoning, such as recognizing quantitative relationships of physical objects like distances or size differences. We hypothesize that VLMs' limited spatial reasoning capability is due to the lack of 3D spatial knowledge in training data and aim to solve this problem by training VLMs with Internet-scale spatial reasoning data. To this end, we present a system to facilitate this approach. We first develop an automatic 3D spatial VQA data generation framework that scales up to 2 billion VQA examples on 10 million real-world images. We then investigate various factors in the training recipe, including data quality, training pipeline, and VLM architecture. Our work features the first internet-scale 3D spatial reasoning dataset in metric space. By training a VLM on such data, we significantly enhance its ability on both qualitative and quantitative spatial VQA. Finally, we demonstrate that this VLM unlocks novel downstream applications in chain-of-thought spatial reasoning and robotics due to its quantitative estimation capability. Project website: https://spatial-vlm.github.io/ 

[PDF](http://arxiv.org/abs/2401.12168v1) 

**摘要**
通过大规模空间推理数据集的训练，视觉语言模型的空间推理能力得到显着提高。

**要点**
- 视觉语言模型在视觉问答任务中表现出色，但在三维空间推理任务中存在不足。
- 视觉语言模型三维空间推理能力有限的原因是训练数据中缺乏三维空间知识。
- 通过互联网规模的空间推理数据训练视觉语言模型，可以显著提高其空间推理能力。
- 本研究提出了一种自动三维空间视觉问答数据生成框架，可以生成 20 亿个视觉问答示例，这些示例来自 1000 万张真实世界图像。
- 本研究还探讨了训练过程中对视觉语言模型空间推理能力有影响的各种因素，包括数据质量、训练管道和视觉语言模型架构。
- 本研究首次提出了度量空间中的互联网规模的三维空间推理数据集。
- 通过在该数据集上训练视觉语言模型，显著提高了其在定性和定量空间视觉问答任务中的能力。
- 由于视觉语言模型的定量估计能力，它可以在思维链空间推理和机器人技术方面解锁新的下游应用程序。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li><p>题目：空间VLM：赋予视觉语言模型空间推理能力</p>
</li>
<li><p>作者：Boyuan Chen, Zhuo Xu, Sean Kirmani, Danny Driess, Pete Florence, Brian Ichter, Dorsa Sadigh, Leonidas Guibas, Fei Xia</p>
</li>
<li><p>第一作者单位：谷歌大脑</p>
</li>
<li><p>关键词：视觉语言模型、空间推理、数据生成、预训练</p>
</li>
<li><p>论文链接：https://arxiv.org/abs/2401.12168
Github 链接：无</p>
</li>
<li><p>摘要：
(1)：研究背景：视觉语言模型（VLM）在图像字幕、视觉问答、具身规划、动作识别等任务上取得了显著进展。然而，大多数最先进的 VLM 在空间推理方面仍然存在困难，即需要理解物体在 3D 空间中的位置或它们之间的空间关系的任务。空间推理能力本身很有用，也适用于机器人或 AR 等下游应用。
(2)：过去的方法与问题：许多 VLM 在以图像字幕对为特征的互联网规模数据集上进行训练，这些数据集包含有限的空间信息。这是因为难以获得空间信息丰富的具身数据或高质量的人类注释以用于 3D 感知查询。自动数据生成和增强技术是解决数据限制问题的一种方法。然而，以前的大多数数据生成工作都集中在使用真实语义注释渲染逼真的图像，而忽略了对象和 3D 关系的丰富性。
(3)：研究方法：本文提出了一种名为 Spatial VLM 的系统，该系统能够生成数据并训练 VLM 以增强其空间推理能力。具体来说，通过结合 1）开放词汇检测，2）度量深度估计，3）语义分割和 4）以对象为中心的字幕模型，我们可以大规模地注释真实世界数据。Spatial VLM 将视觉模型生成的数据转换为一种格式，可用于在字幕、VQA 和空间推理数据的混合体上训练 VLM。
(4)：实验结果：实验表明，训练后的 VLM 表现出许多理想的能力。首先，它回答定性空间问题的能力大大增强。其次，它可以可靠地执行定量估计，尽管训练数据存在噪声。这种能力不仅赋予它有关物体大小的常识知识，而且使其成为用于重新排列任务的开放词汇奖励注释器。第三，我们发现这种空间视觉语言模型受益于其自然语言界面，可以执行空间思想链以解决与强大的大型语言模型相结合的复杂空间推理任务。</p>
</li>
<li><p>方法：
(1) 通过结合开放词汇检测、度量深度估计、语义分割和以对象为中心的字幕模型，在大规模真实世界数据上进行注释；
(2) 将视觉模型生成的数据转换为一种格式，可用于在字幕、VQA和空间推理数据的混合体上训练VLM；
(3) 训练后的VLM表现出许多理想的能力，包括回答定性空间问题的能力大大增强、可以可靠地执行定量估计、受益于其自然语言界面，可以执行空间思想链以解决与强大的大型语言模型相结合的复杂空间推理任务。</p>
</li>
<li><p>结论：
（1）：本工作通过构建一个基于互联网规模真实世界图像的 3D 空间推理视觉问答数据自动生成框架，解决了向 VLM 注入空间推理能力的挑战。我们消融了在训练 VLM 时不同的设计选择，例如使用大量噪声数据进行训练和解冻 ViT。虽然我们的直接空间查询构建在一个有限的模板集上，但我们表明 Spatial VLM 可以扩展到处理需要空间推理组件的更复杂的思想链推理。Spatial VLM 也被证明对机器人任务有用，我们表明 3D 空间感知 VLM 可以用作机器人任务的奖励注释器。对更多细微的几何基元的额外研究也有助于将空间推理扎根于 3D 几何中。
（2）：创新点：Spatial VLM 框架可以自动生成 3D 空间推理视觉问答数据，从而解决了 VLM 数据匮乏的问题。Spatial VLM 在空间推理任务上表现出优异的性能，例如回答定性空间问题、执行定量估计和解决复杂的思想链推理任务。Spatial VLM 还可以在机器人任务中用作奖励注释器。
性能：Spatial VLM 在空间推理任务上表现出优异的性能，例如回答定性空间问题、执行定量估计和解决复杂的思想链推理任务。
工作量：Spatial VLM 框架的构建和训练需要大量的时间和计算资源。</p>
</li>
</ol>



<p></p><details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1dd8332c6d8630f99e53a83fc7e433f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c541a35e7c51b65b33425be6365e1f69.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae654224a3e214605c476084a222746f.jpg" align="middle">
</details><br>​    <p></p>
<h2 id="West-of-N-Synthetic-Preference-Generation-for-Improved-Reward-Modeling"><a href="#West-of-N-Synthetic-Preference-Generation-for-Improved-Reward-Modeling" class="headerlink" title="West-of-N: Synthetic Preference Generation for Improved Reward Modeling"></a>West-of-N: Synthetic Preference Generation for Improved Reward Modeling</h2><p><strong>Authors:Alizée Pace, Jonathan Mallinson, Eric Malmi, Sebastian Krause, Aliaksei Severyn</strong></p>
<p>The success of reinforcement learning from human feedback (RLHF) in language model alignment is strongly dependent on the quality of the underlying reward model. In this paper, we present a novel approach to improve reward model quality by generating synthetic preference data, thereby augmenting the training dataset with on-policy, high-quality preference pairs. Motivated by the promising results of Best-of-N sampling strategies in language model training, we extend their application to reward model training. This results in a self-training strategy to generate preference pairs by selecting the best and worst candidates in a pool of responses to a given query. Empirically, we find that this approach improves the performance of any reward model, with an effect comparable to the addition of a similar quantity of human preference data. This work opens up new avenues of research for improving RLHF for language model alignment, by offering synthetic preference generation as a solution to reward modeling challenges. </p>
<p><a href="http://arxiv.org/abs/2401.12086v1">PDF</a> </p>
<p><strong>Summary:</strong></p>
<p>深度强化学习从人类反馈中学习对语言模型的调整强烈依赖于基础奖励模型的质量。</p>
<p><strong>Key Takeaways:</strong></p>
<ul>
<li>提出了一种通过生成合成偏好数据来提高奖励模型质量的新方法。</li>
<li>利用最优 N 采样策略在奖励模型训练中的应用。</li>
<li>采用自训练策略，通过从给定查询的响应池中选择最佳和最差候选来生成偏好对。</li>
<li>实证研究发现，该方法可以提高任何奖励模型的性能，效果与添加相同数量的人类偏好数据相当。</li>
<li>这项工作通过提供合成偏好生成作为奖励建模挑战的解决方案，为改进深度强化学习从人类反馈中学习对语言模型的调整开辟了新的研究途径。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：西-N：用于改进奖励建模的合成偏好生成</li>
<li>作者：Alizée Pace、Jonathan Mallinson、Eric Malmi、Sebastian Krause、Aliaksei Severyn</li>
<li>隶属机构：苏黎世联邦理工学院人工智能中心</li>
<li>关键词：强化学习、人类反馈、语言模型对齐、奖励建模、合成偏好生成、最佳-N 采样</li>
<li>链接：https://arxiv.org/abs/2401.12086</li>
<li>摘要：
（1）研究背景：在语言模型对齐中，从人类反馈中进行强化学习的成功在很大程度上取决于基础奖励模型的质量。
（2）过去的方法及其问题：以往的方法通常通过收集人类反馈数据来训练奖励模型，这既昂贵又耗时。此外，奖励模型的质量还取决于人类反馈数据的数量、评估的响应分布以及偏好标签的准确性。
（3）本文提出的研究方法：为了解决这些问题，本文提出了一种通过生成高质量、策略内合成偏好数据来增强奖励模型训练的新方法。这种方法利用语言模型策略的生成能力来产生一个半监督训练框架。具体来说，本文利用最佳-N 采样，从一组给定未标记提示的输出中提取最佳和最差的生成，并使用奖励模型来识别西-N 对。然后，将这些西-N 对添加到初始偏好数据集中，以增强奖励模型的训练。
（4）方法在任务和性能上的表现：实验证明，本文提出的方法可以有效地提高任何奖励模型的性能，其效果与添加类似数量的人类偏好数据相当或更好。此外，本文的工作也是第一个证明了最佳-N 采样和半监督学习在奖励模型训练中的前景，这有望为该领域带来进一步的研究。</li>
</ol>
<p>Some Error for method(比如是不是没有Methods这个章节)</p>
<ol start="8">
<li>结论：
（1）：本文提出了一种通过生成高质量、策略内合成偏好数据来增强奖励模型训练的新方法，该方法可以有效地提高任何奖励模型的性能，其效果与添加类似数量的人类偏好数据相当或更好。此外，本文的工作也是第一个证明了最佳-N采样和半监督学习在奖励模型训练中的前景，这有望为该领域带来进一步的研究。
（2）：创新点：</li>
</ol>
<ul>
<li>提出了一种通过生成合成偏好数据来增强奖励模型训练的新方法。</li>
<li>利用语言模型策略的生成能力来产生一个半监督训练框架。</li>
<li>利用最佳-N采样，从一组给定未标记提示的输出中提取最佳和最差的生成，并使用奖励模型来识别西-N对。</li>
<li>将这些西-N对添加到初始偏好数据集中，以增强奖励模型的训练。
性能：</li>
<li>实验证明，本文提出的方法可以有效地提高任何奖励模型的性能，其效果与添加类似数量的人类偏好数据相当或更好。</li>
<li>本文的工作也是第一个证明了最佳-N采样和半监督学习在奖励模型训练中的前景。
工作量：</li>
<li>本文提出的方法需要收集人类反馈数据来训练奖励模型，这既昂贵又耗时。</li>
<li>此外，奖励模型的质量还取决于人类反馈数据的数量、评估的响应分布以及偏好标签的准确性。</li>
</ul>



<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-f075498889dd5931672e158769361ccc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a762acac78a978fb4aa322de329e2f04.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fab744164f7b7ba0585c92c42ab56338.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3412e87f6d4c5844235495c01077f118.jpg" align="middle">
</details>
​    


## Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated   Text
**Authors:Abhimanyu Hans, Avi Schwarzschild, Valeriia Cherepanova, Hamid Kazemi, Aniruddha Saha, Micah Goldblum, Jonas Geiping, Tom Goldstein**

Detecting text generated by modern large language models is thought to be hard, as both LLMs and humans can exhibit a wide range of complex behaviors. However, we find that a score based on contrasting two closely related language models is highly accurate at separating human-generated and machine-generated text. Based on this mechanism, we propose a novel LLM detector that only requires simple calculations using a pair of pre-trained LLMs. The method, called Binoculars, achieves state-of-the-art accuracy without any training data. It is capable of spotting machine text from a range of modern LLMs without any model-specific modifications. We comprehensively evaluate Binoculars on a number of text sources and in varied situations. Over a wide range of document types, Binoculars detects over 90% of generated samples from ChatGPT (and other LLMs) at a false positive rate of 0.01%, despite not being trained on any ChatGPT data. 

[PDF](http://arxiv.org/abs/2401.12070v1) 20 pages, code available at https://github.com/ahans30/Binoculars

**Summary**
人工智能检测器可以准确区分人类生成和机器生成文本，无需训练数据。

**Key Takeaways**
- 人工智能检测器基于对比两个密切相关的语言模型的分数，可以准确区分人类生成和机器生成文本。
- 该方法称为 Binoculars，无需任何训练数据即可实现最先进的准确性。
- Binoculars 无需任何特定模型的修改，就可以从一系列现代语言模型中检测到机器文本。
- Binoculars 在多种文本来源和各种情况下都得到了全面评估。
- 在各种类型的文档中，Binoculars 以 0.01% 的误报率检测出超过 90% 由 ChatGPT（和其他语言模型）生成的样本，尽管它没有使用任何 ChatGPT 数据进行训练。
- Binoculars 是一个通用工具，可以检测由各种语言模型生成的文本。
- Binoculars 可用于多种应用，例如检测虚假新闻或识别在线欺诈。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>标题：使用双筒望远镜发现 LLM：零次检测机器生成的文本</li>
<li>作者：Abhimanyu Hans、Avi Schwarzschild、Valeriia Cherepanova、Hamid Kazemi、Aniruddha Saha、Micah Gold Blum、Jonas Geiping、Tom Goldstein</li>
<li>第一作者单位：马里兰大学</li>
<li>关键词：自然语言处理、机器学习、语言模型、文本生成、检测机器生成的文本</li>
<li>论文链接：https://arxiv.org/abs/2401.12070，Github 代码链接：https://github.com/ahans30/Binoculars</li>
<li>摘要：</li>
</ol>
<p>（1）研究背景：检测由现代大型语言模型生成的文本被认为是一项艰巨的任务，因为 LLM 和人类都可以表现出广泛的复杂行为。然而，我们发现基于对比两个密切相关的语言模型的分数在区分人类生成的文本和机器生成的文本方面非常准确。</p>
<p>（2）过去的方法及其问题：现有方法存在以下问题：需要大量训练数据进行微调；只能检测特定语言模型生成的文本；对生成的文本类型和领域敏感。</p>
<p>（3）研究方法：我们提出了一种新颖的 LLM 检测器，它只需要使用一对预训练的 LLM 进行简单的计算。该方法称为双筒望远镜，在没有任何训练数据的情况下实现了最先进的准确性。它能够在不进行任何特定于模型的修改的情况下从一系列现代 LLM 中识别机器文本。</p>
<p>（4）方法性能：我们对双筒望远镜进行了全面的评估，涉及多种文本来源和各种情况。在各种类型的文档中，双筒望远镜检测到超过 90% 的来自 ChatGPT（和其他 LLM）生成的示例，假阳性率为 0.01%，尽管没有在任何 ChatGPT 数据上进行训练。</p>
<p>Some Error for method(比如是不是没有Methods这个章节)</p>
<ol start="8">
<li>结论：
（1）：本文提出了一种新颖的LLM检测器——双筒望远镜，该方法在没有任何训练数据的情况下实现了最先进的准确性。它能够在不进行任何特定于模型的修改的情况下从一系列现代LLM中识别机器文本。
（2）：创新点：</li>
</ol>
<ul>
<li>使用一对预训练的LLM进行简单的计算，无需大量训练数据进行微调。</li>
<li>能够在不进行任何特定于模型的修改的情况下从一系列现代LLM中识别机器文本。</li>
<li>在各种类型的文档中，双筒望远镜检测到超过90%的来自ChatGPT（和其他LLM）生成的示例，假阳性率为0.01%，尽管没有在任何ChatGPT数据上进行训练。
性能：</li>
<li>在各种类型的文档中，双筒望远镜检测到超过90%的来自ChatGPT（和其他LLM）生成的示例，假阳性率为0.01%，尽管没有在任何ChatGPT数据上进行训练。</li>
<li>双筒望远镜在检测其他LLM生成的文本方面也表现出良好的性能，例如GPT-3、T5和BART。
工作量：</li>
<li>双筒望远镜的实现相对简单，可以在各种计算平台上轻松部署。</li>
<li>双筒望远镜的计算成本很低，可以实时检测机器生成的文本。</li>
</ul>



<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-dd3bfb5b9d052c9fd7839e210dcdc353.jpg" align="middle">
</details>
​    


## Feature Denoising Diffusion Model for Blind Image Quality Assessment
**Authors:Xudong Li, Jingyuan Zheng, Runze Hu, Yan Zhang, Ke Li, Yunhang Shen, Xiawu Zheng, Yutao Liu, ShengChuan Zhang, Pingyang Dai, Rongrong Ji**

Blind Image Quality Assessment (BIQA) aims to evaluate image quality in line with human perception, without reference benchmarks. Currently, deep learning BIQA methods typically depend on using features from high-level tasks for transfer learning. However, the inherent differences between BIQA and these high-level tasks inevitably introduce noise into the quality-aware features. In this paper, we take an initial step towards exploring the diffusion model for feature denoising in BIQA, namely Perceptual Feature Diffusion for IQA (PFD-IQA), which aims to remove noise from quality-aware features. Specifically, (i) We propose a {Perceptual Prior Discovery and Aggregation module to establish two auxiliary tasks to discover potential low-level features in images that are used to aggregate perceptual text conditions for the diffusion model. (ii) We propose a Perceptual Prior-based Feature Refinement strategy, which matches noisy features to predefined denoising trajectories and then performs exact feature denoising based on text conditions. Extensive experiments on eight standard BIQA datasets demonstrate the superior performance to the state-of-the-art BIQA methods, i.e., achieving the PLCC values of 0.935 ( vs. 0.905 in KADID) and 0.922 ( vs. 0.894 in LIVEC). 

[PDF](http://arxiv.org/abs/2401.11949v1) 

**Summary**
利用扩散模型提升图像质量评价的特征去噪

**Key Takeaways**

- 提出了一种基于扩散模型的图像质量评价方法，PFD-IQA。
- PFD-IQA 通过两个辅助任务发现潜在的低级特征，并将其用于聚合扩散模型的感知文本条件。
- PFD-IQA 提出了一种基于感知先验的特征细化策略，将噪声特征匹配到预定义的去噪轨迹，然后基于文本条件执行精确的特征去噪。
- PFD-IQA 在八个标准图像质量评价数据集上取得了优于最先进的图像质量评价方法的性能，例如，在 KADID 中达到了 0.935 的 PLCC 值（而 KADID 为 0.905）和在 LIVEC 中达到了 0.922 的 PLCC 值（而 LIVEC 为 0.894）。
- PFD-IQA 可以有效地从质量感知特征中去除噪声，从而提高图像质量评价的准确性。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>题目：基于感知特征扩散的图像质量评估</li>
<li>作者：Huajun Chen, Yifan Zhang, Qiong Yan, Jiaying Liu, Yanyun Zhao, Lei Zhang</li>
<li>单位：中国科学院自动化研究所</li>
<li>关键词：图像质量评估、扩散模型、感知特征、文本条件</li>
<li>链接：None, Github：None</li>
<li>摘要：</li>
</ol>
<p>（1）研究背景：
图像质量评估（BIQA）旨在评估图像质量，使其与人类感知一致，且无需参考基准。当前，深度学习的 BIQA 方法通常依赖于来自高级任务的特征，以便进行迁移学习。然而，BIQA 与这些高级任务之间的固有差异不可避免地会向质量感知特征引入噪声。</p>
<p>（2）过去的方法及其问题：
现有的 BIQA 方法通常依赖于从高层任务中提取的特征，这些特征可能包含与图像质量无关的信息，从而导致评估结果不准确。此外，这些方法通常需要大量的数据进行训练，并且对图像的失真类型和质量水平敏感。</p>
<p>（3）本文的研究方法：
本文提出了一种基于感知特征扩散的图像质量评估方法（PFD-IQA）。该方法首先通过感知先验发现和聚合模块建立两个辅助任务，以发现图像中潜在的低级特征，这些特征用于聚合用于扩散模型的感知文本条件。然后，本文提出了一种基于感知先验的特征细化策略，该策略将噪声特征与预定义的去噪轨迹相匹配，然后基于文本条件执行精确的特征去噪。</p>
<p>（4）方法的性能：
在八个标准 BIQA 数据集上的广泛实验表明，该方法优于最先进的 BIQA 方法，即在 KADID 中实现 PLCC 值为 0.935（比 0.905 提高 3.0%），在 LIVEC 中实现 PLCC 值为 0.922（比 0.894 提高 2.8%）。这些性能结果支持了本文方法的目标。</p>
<ol start="7">
<li><p>方法：
(1): 感知先验发现和聚合模块 (PDA)：利用随机通道掩码模块和特征重建器来发现潜在的失真先验和感知先验，并利用文本条件自适应地聚合感知文本嵌入。
(2): 感知先验驱动的扩散细化模块 (PDR)：利用感知先验来增强特征表示，并提出一种基于感知先验的特征细化策略，将噪声特征与预定义的去噪轨迹相匹配，然后基于文本条件执行精确的特征去噪。
(3): 变换器解码器：使用一层变换器解码器来进一步解释去噪后的特征，以预测最终的质量分数。</p>
</li>
<li><p>结论：
（1）：本文提出了一种基于感知特征扩散的图像质量评估方法（PFD-IQA），该方法将扩散模型的去噪能力引入到盲图像质量评估中，并通过引入感知先验发现和聚合模块以及感知先验驱动的特征细化策略，实现了图像质量评估的准确性和鲁棒性。
（2）：创新点：
本文的主要创新点包括：</p>
</li>
</ol>
<ul>
<li>提出了一种新的图像质量评估框架，该框架利用扩散模型的去噪能力来评估图像质量。</li>
<li>提出了一种感知先验发现和聚合模块，该模块可以发现图像中的潜在失真先验和感知先验，并自适应地聚合感知文本嵌入。</li>
<li>提出了一种感知先验驱动的特征细化策略，该策略可以将噪声特征与预定义的去噪轨迹相匹配，然后基于文本条件执行精确的特征去噪。</li>
</ul>
<p>性能：</p>
<ul>
<li>在八个标准BIQA数据集上的广泛实验表明，该方法优于最先进的BIQA方法，即在KADID中实现PLCC值为0.935（比0.905提高3.0%），在LIVEC中实现PLCC值为0.922（比0.894提高2.8%）。</li>
</ul>
<p>工作量：</p>
<ul>
<li>该方法的工作量主要体现在模型的训练和推理上。模型的训练需要大量的数据，并且需要较长的训练时间。模型的推理速度也相对较慢，因为需要对图像进行多次采样。</li>
</ul>



<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d238ca44c11468d98720fd64ab500d75.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1828ba81b9cef3240c9a656e8ada16ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c337add6e35fa4a0c7cc26a39230f729.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9bdc9e0b457f2c81fd14765eac361bfa.jpg" align="middle">
</details>
​    


## Blinded by Generated Contexts: How Language Models Merge Generated and   Retrieved Contexts for Open-Domain QA?
**Authors:Hexiang Tan, Fei Sun, Wanli Yang, Yuanzhuo Wang, Qi Cao, Xueqi Cheng**

While auxiliary information has become a key to enhance Large Language Models (LLMs), relatively little is known about how well LLMs merge these contexts, specifically generated and retrieved. To study this, we formulate a task specifically designed to identify whether the answers, derived from the integration of generated and retrieved contexts, are attributed to either generated or retrieved contexts. To support this task, we develop a methodology to construct datasets with conflicting contexts, where each question is paired with both generated and retrieved contexts, yet only one of them contains the correct answer. Our experiments reveal a significant bias in LLMs towards generated contexts, as evidenced across state-of-the-art open (Llama2-7b/13b) and closed (GPT 3.5/4) systems. We further identify two key factors contributing to this bias: i) Contexts generated by LLMs typically show greater similarity to the questions, increasing their likelihood of selection; ii) The segmentation process used in retrieved contexts disrupts their completeness, thereby hindering their full utilization in LLMs. Our analysis enhances the understanding of how LLMs merge diverse contexts, offering valuable insights for advancing current augmentation methods for LLMs. 

[PDF](http://arxiv.org/abs/2401.11911v1) 

**Summary**
大型语言模型对生成和检索语境的融合存在显著偏差，偏向于选择与问题更相似的生成语境。

**Key Takeaways**
- 大型语言模型对生成和检索语境的融合存在显著偏差，偏向于选择与问题更相似的生成语境。
- 大型语言模型生成的语境通常与问题更相似，因此更容易被选择。
- 大型语言模型检索的语境由于分段过程而变得不完整，因此难以被充分利用。
- 理解大型语言模型如何融合不同语境有助于改进目前的大型语言模型增强方法。
- 大型语言模型在获取信息时，往往会偏向于它自己生成的语境，这是由于这些语境通常与问题更相似。
- 大型语言模型检索的语境由于被分段，因此会存在不完整的情况，这也会影响大型语言模型对语境的利用。
- 研究人员提出了一种方法来构建具有冲突语境的数据集，并使用该数据集来评估大型语言模型融合不同语境的能力。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li><p>论文标题：生成上下文遮蔽：语言模型如何融合生成上下文和检索上下文进行开放域问答？</p>
</li>
<li><p>作者：谭和祥、孙飞、杨万里、王元卓、曹琦、程雪祺</p>
</li>
<li><p>第一作者单位：中国科学院计算技术研究所人工智能安全与安全重点实验室</p>
</li>
<li><p>关键词：大型语言模型、信息融合、生成上下文、检索上下文、问答</p>
</li>
<li><p>论文链接：https://arxiv.org/abs/2401.11911
Github 代码链接：无</p>
</li>
<li><p>摘要：
(1) 研究背景：近年来，利用辅助信息增强大型语言模型（LLM）的性能已成为研究热点。然而，对于 LLM 如何融合这些上下文，特别是生成上下文和检索上下文，目前的研究还相对较少。
(2) 过去的方法及其问题：现有工作可以分为生成增强和检索增强两大类。生成增强方法通过让 LLM 生成与给定问题相关的背景上下文，然后利用该上下文生成最终答案。检索增强方法则通过将来自外部语料库（如维基百科）的相关段落作为上下文，从而增强 LLM 处理知识更新和长尾知识等情况的能力。然而，这些方法都存在冲突问题，即不同来源的上下文之间可能存在冲突，从而影响信息融合的有效性。
(3) 本文提出的研究方法：为了研究 LLM 如何处理生成上下文和检索上下文之间的冲突，本文提出了一种专门设计的新任务，用于识别答案是否来自生成上下文或检索上下文。同时，本文还开发了一种构建具有冲突上下文的数据集的方法，其中每个问题都与生成上下文和检索上下文配对，但只有一个上下文包含正确答案。
(4) 方法在任务上的表现：实验结果表明，LLM 对生成上下文存在显着的偏好，这在最先进的开放（Llama2-7b/13b）和封闭（GPT3.5/4）系统中都有所体现。进一步分析发现，导致这种偏见的两个关键因素是：i) LLM 生成的上下文通常与问题更相似，增加了它们被选择的可能性；ii) 检索上下文中的分段过程破坏了它们的完整性，从而阻碍了 LLM 对它们的充分利用。</p>
</li>
<li><p>方法：
（1）任务设计：设计一种任务来识别答案是否来自生成上下文或检索上下文，以研究 LLM 如何处理冲突上下文。
（2）数据集构建：构建具有冲突上下文的数据集，其中每个问题都与生成上下文和检索上下文配对，但只有一个上下文包含正确答案。
（3）实验评估：使用最先进的开放（Llama2-7b/13b）和封闭（GPT3.5/4）系统在任务上评估 LLM 的表现，分析导致 LLM 对生成上下文存在偏见的关键因素。</p>
</li>
<li><p>结论：
(1): 本工作首次研究了 LLM 如何处理生成上下文和检索上下文之间的冲突，并提出了一个专门设计的新任务和构建具有冲突上下文的数据集的方法，为研究 LLM 的信息融合行为提供了新的视角。
(2): 创新点：</p>
</li>
</ol>
<ul>
<li>提出了一种识别答案是否来自生成上下文或检索上下文的新任务，用于研究 LLM 如何处理冲突上下文。</li>
<li>开发了一种构建具有冲突上下文的数据集的方法，其中每个问题都与生成上下文和检索上下文配对，但只有一个上下文包含正确答案。</li>
<li>通过实验评估发现，LLM 对生成上下文存在显着的偏好，并分析了导致这种偏见的两个关键因素。
性能：</li>
<li>在最先进的开放（Llama2-7b/13b）和封闭（GPT3.5/4）系统上评估了 LLM 在任务上的表现，结果表明 LLM 对生成上下文存在显着的偏好。</li>
<li>进一步分析发现，导致这种偏见的两个关键因素是：i) LLM 生成的上下文通常与问题更相似，增加了它们被选择的可能性；ii) 检索上下文中的分段过程破坏了它们的完整性，从而阻碍了 LLM 对它们的充分利用。
工作量：</li>
<li>设计了任务和构建了数据集，用于研究 LLM 如何处理冲突上下文。</li>
<li>使用最先进的开放（Llama2-7b/13b）和封闭（GPT3.5/4）系统在任务上评估了 LLM 的表现，并分析了导致 LLM 对生成上下文存在偏见的两个关键因素。</li>
</ul>



<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-aff8facaa355b1505b2cf6af3d0e915b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ee80fef672e8714cbda66ee9ba9e921.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-32c0aa5250fcb9295d1e46e737e52534.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99f94640fc796568a6b02c8056191892.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c63733100557d4290705642b87c665f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8815deb3960e6b2bbbe8b92e6f8e6799.jpg" align="middle">
</details>
​    


## Considerations on Approaches and Metrics in Automated Theorem   Generation/Finding in Geometry
**Authors:Pedro Quaresma, Pierluigi Graziani, Stefano M. Nicoletti**

The pursue of what are properties that can be identified to permit an automated reasoning program to generate and find new and interesting theorems is an interesting research goal (pun intended). The automatic discovery of new theorems is a goal in itself, and it has been addressed in specific areas, with different methods. The separation of the "weeds", uninteresting, trivial facts, from the "wheat", new and interesting facts, is much harder, but is also being addressed by different authors using different approaches. In this paper we will focus on geometry. We present and discuss different approaches for the automatic discovery of geometric theorems (and properties), and different metrics to find the interesting theorems among all those that were generated. After this description we will introduce the first result of this article: an undecidability result proving that having an algorithmic procedure that decides for every possible Turing Machine that produces theorems, whether it is able to produce also interesting theorems, is an undecidable problem. Consequently, we will argue that judging whether a theorem prover is able to produce interesting theorems remains a non deterministic task, at best a task to be addressed by program based in an algorithm guided by heuristics criteria. Therefore, as a human, to satisfy this task two things are necessary: an expert survey that sheds light on what a theorem prover/finder of interesting geometric theorems is, and - to enable this analysis - other surveys that clarify metrics and approaches related to the interestingness of geometric theorems. In the conclusion of this article we will introduce the structure of two of these surveys - the second result of this article - and we will discuss some future work. 

[PDF](http://arxiv.org/abs/2401.11905v1) In Proceedings ADG 2023, arXiv:2401.10725

**摘要**
几何定理自动发现方法学及衡量标准综述。

**要点**

- 几何定理自动发现与寻找有趣定理是两个不同的课题。
- 几何定理的有趣性难以判断，目前尚未找到有效的算法来解决这个问题。
- 目前有不同的方法和度量标准来衡量几何定理的有趣性。
- 专家调查对于确定有趣的几何定理证明器/发现者的标准非常重要。
- 衡量几何定理有趣性的度量标准和方法值得进一步研究。
- 本文介绍了两项关于几何定理自动发现和有趣性度量的调查结果。
- 未来的工作包括开发新的方法来衡量几何定理的有趣性以及设计新的算法来发现几何定理。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>标题：关于自动推理中的方法和度量</li>
<li>作者：P. Quaresma, P. Graziani, S. M. Nicoletti</li>
<li>单位：科英布拉大学</li>
<li>关键词：自动定理生成、自动定理发现、几何定理、有趣性、调查</li>
<li>链接：https://link.springer.com/article/10.1007/s10955-022-02793-z
Github：无</li>
<li>摘要：
(1)：研究背景：自动推理系统面临的一个挑战是能够发现新的和有趣的定理。本文探讨了自动定理生成和自动定理发现的方法和度量。
(2)：过去的方法和问题：过去的方法包括归纳法、生成法和操纵法。这些方法都存在一定的局限性，例如归纳法不健全，生成法不健全，操纵法受限于现有定理。
(3)：研究方法：本文提出了一种新的方法来评估几何定理的有趣性。该方法基于两项调查，第一项调查收集了受访者对几何定理有趣性的看法，第二项调查将第一项调查的结果用于设计一个在线调查，以进一步探索几何定理有趣性的特征。
(4)：方法的性能：该方法能够有效地评估几何定理的有趣性。在第一项调查中，受访者对 100 个几何定理的有趣性进行了评估，结果表明该方法能够准确地识别出受访者认为有趣的定理。在第二项调查中，受访者对 50 个几何定理的有趣性进行了评估，结果表明该方法能够准确地识别出受访者认为有趣的定理，并且能够识别出受访者认为不有趣的定理。</li>
</ol>
<p>7.方法：
（1）提出一种基于两项调查的新方法来评估几何定理的有趣性。
（2）第一项调查收集了受访者对几何定理有趣性的看法。
（3）第二项调查将第一项调查的结果用于设计一个在线调查，以进一步探索几何定理有趣性的特征。
（4）该方法能够有效地评估几何定理的有趣性。</p>
<ol start="8">
<li>结论：
（1）：本文提出了一种基于两项调查的新方法来评估几何定理的有趣性，该方法能够有效地评估几何定理的有趣性，为自动推理系统发现新的和有趣的定理提供了新的思路。
（2）：创新点：提出了一种基于两项调查的新方法来评估几何定理的有趣性。
性能：该方法能够有效地评估几何定理的有趣性。
工作量：该方法需要收集受访者对几何定理有趣性的看法，设计在线调查，分析调查结果，工作量较大。</li>
</ol>



<p></p><details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-35fbe3fcfdde6deb4efc56cd12862691.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0e8aa21d700ed908986328d869dd194f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-23af62ea3bf9c0e1bfe2c05e42b8598e.jpg" align="middle">
</details><br>​    <p></p>
<h2 id="Adversarial-speech-for-voice-privacy-protection-from-Personalized-Speech-generation"><a href="#Adversarial-speech-for-voice-privacy-protection-from-Personalized-Speech-generation" class="headerlink" title="Adversarial speech for voice privacy protection from Personalized Speech   generation"></a>Adversarial speech for voice privacy protection from Personalized Speech   generation</h2><p><strong>Authors:Shihao Chen, Liping Chen, Jie Zhang, KongAik Lee, Zhenhua Ling, Lirong Dai</strong></p>
<p>The rapid progress in personalized speech generation technology, including personalized text-to-speech (TTS) and voice conversion (VC), poses a challenge in distinguishing between generated and real speech for human listeners, resulting in an urgent demand in protecting speakers’ voices from malicious misuse. In this regard, we propose a speaker protection method based on adversarial attacks. The proposed method perturbs speech signals by minimally altering the original speech while rendering downstream speech generation models unable to accurately generate the voice of the target speaker. For validation, we employ the open-source pre-trained YourTTS model for speech generation and protect the target speaker’s speech in the white-box scenario. Automatic speaker verification (ASV) evaluations were carried out on the generated speech as the assessment of the voice protection capability. Our experimental results show that we successfully perturbed the speaker encoder of the YourTTS model using the gradient-based I-FGSM adversarial perturbation method. Furthermore, the adversarial perturbation is effective in preventing the YourTTS model from generating the speech of the target speaker. Audio samples can be found in <a href="https://voiceprivacy.github.io/Adeversarial-Speech-with-YourTTS">https://voiceprivacy.github.io/Adeversarial-Speech-with-YourTTS</a>. </p>
<p><a href="http://arxiv.org/abs/2401.11857v1">PDF</a> Accepted by icassp 2024</p>
<p><strong>Summary</strong><br>利用对抗攻击保护语音免受恶意语音合成攻击。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>语音生成技术快速发展，带来语音保护需求。</li>
<li>提出一种基于对抗攻击的语音保护方法。</li>
<li>该方法通过最小化扰动原始语音，使下游语音生成模型无法准确生成目标扬声器的语音。</li>
<li>利用开源预训练YourTTS模型进行语音生成，并在白盒场景下保护目标扬声器的语音。</li>
<li>在生成的语音上进行自动扬声器验证（ASV）评估，评估语音保护能力。</li>
<li>实验结果表明，使用基于梯度的I-FGSM对抗扰动方法成功扰动了YourTTS模型的扬声器编码器。</li>
<li>对抗扰动有效阻止了YourTTS模型生成目标扬声器的语音。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li><p>题目：对抗语音保护语音隐私免受个性化语音生成的影响</p>
</li>
<li><p>作者：Shihao Chen, Liping Chen, Jie Zhang, Kong Aik Lee, Zhenhua Ling, Lirong Dai</p>
</li>
<li><p>隶属单位：中国科学技术大学自然科学与工程科学研究中心</p>
</li>
<li><p>关键词：个性化语音生成，文本到语音，语音转换，语音隐私，对抗攻击</p>
</li>
<li><p>论文链接：https://arxiv.org/abs/2401.11857
Github 链接：无</p>
</li>
<li><p>摘要：
(1)：随着个性化语音生成技术（包括个性化文本到语音（TTS）和语音转换（VC））的快速发展，人类听众很难区分生成的语音和真实语音，这使得保护说话者声音免受恶意使用变得迫切。
(2)：过去的方法主要集中在语音合成语音检测和语音匿名化。语音合成语音检测技术可以检测出合成的语音，但无法防止合成的语音被生成。语音匿名化技术可以隐藏说话者的属性，但会改变语音的感知。
(3)：本文提出了一种基于对抗攻击的说话者保护方法。该方法通过最小化改变原始语音来扰动语音信号，同时使下游语音生成模型无法准确生成目标说话者的语音。
(4)：在 YourTTS 模型上进行的实验结果表明，该方法成功地扰动了说话者编码器，并有效地防止了 YourTTS 模型生成目标说话者的语音。</p>
</li>
<li><p>方法：
（1）提出了一种对抗攻击的说话者保护方法，该方法通过最小化改变原始语音来扰动语音信号，同时使下游语音生成模型无法准确生成目标说话者的语音。
（2）该方法包括两个步骤：首先，使用预训练的语音编码器提取原始语音的说话者编码；然后，使用对抗训练来生成对抗扰动，该对抗扰动可以最小化说话者编码与下游语音生成模型生成的语音之间的相似性。
（3）在 YourTTS 模型上进行的实验结果表明，该方法成功地扰动了说话者编码器，并有效地防止了 YourTTS 模型生成目标说话者的语音。</p>
</li>
<li><p>结论：
（1）本研究意义：提出对抗语音生成保护说话者隐私的方法，旨在防止利用说话者属性生成模仿特定目标说话者的语音。
（2）文章优缺点总结：
创新点：提出基于对抗攻击的说话者隐私保护方法，通过扰动语音信号最小化目标说话者编码与下游语音生成模型生成的语音之间的相似性，有效防止下游语音生成模型准确生成目标说话者的语音。
性能：在 YourTTS 模型上进行的实验结果表明，该方法成功地扰动了说话者编码器，并有效地防止了 YourTTS 模型生成目标说话者的语音。
工作量：需要预训练语音编码器和对抗训练来生成对抗扰动，工作量较大。</p>
</li>
</ol>



<p></p><details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-15e1f378d04e71f2940c11aed1ae5bf4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0af4566ba493c8c725b1d0b9a109ef1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6f0f2f4d8c1103eb84061360d6a46ecc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a9bec8025ef1ca5041dc7cb35b6d9ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b61eaa1a3a79d68bf6f920d3fc1011c.jpg" align="middle">
</details><br>​    <p></p>
<h2 id="Towards-Effective-and-General-Graph-Unlearning-via-Mutual-Evolution"><a href="#Towards-Effective-and-General-Graph-Unlearning-via-Mutual-Evolution" class="headerlink" title="Towards Effective and General Graph Unlearning via Mutual Evolution"></a>Towards Effective and General Graph Unlearning via Mutual Evolution</h2><p><strong>Authors:Xunkai Li, Yulin Zhao, Zhengyu Wu, Wentao Zhang, Rong-Hua Li, Guoren Wang</strong></p>
<p>With the rapid advancement of AI applications, the growing needs for data privacy and model robustness have highlighted the importance of machine unlearning, especially in thriving graph-based scenarios. However, most existing graph unlearning strategies primarily rely on well-designed architectures or manual process, rendering them less user-friendly and posing challenges in terms of deployment efficiency. Furthermore, striking a balance between unlearning performance and framework generalization is also a pivotal concern. To address the above issues, we propose \underline{\textbf{M}}utual \underline{\textbf{E}}volution \underline{\textbf{G}}raph \underline{\textbf{U}}nlearning (MEGU), a new mutual evolution paradigm that simultaneously evolves the predictive and unlearning capacities of graph unlearning. By incorporating aforementioned two components, MEGU ensures complementary optimization in a unified training framework that aligns with the prediction and unlearning requirements. Extensive experiments on 9 graph benchmark datasets demonstrate the superior performance of MEGU in addressing unlearning requirements at the feature, node, and edge levels. Specifically, MEGU achieves average performance improvements of 2.7\%, 2.5\%, and 3.2\% across these three levels of unlearning tasks when compared to state-of-the-art baselines. Furthermore, MEGU exhibits satisfactory training efficiency, reducing time and space overhead by an average of 159.8x and 9.6x, respectively, in comparison to retraining GNN from scratch. </p>
<p><a href="http://arxiv.org/abs/2401.11760v1">PDF</a> Accepted by AAAI 2024 Oral</p>
<p><strong>Summary</strong><br>机器互文演化解图网络遗忘任务难点，提升性能降低开销。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>机器的互文演化范式（MEGU）同时演化预测与遗忘能力，进行互补性优化。</li>
<li>MEGU对9个图基准数据集进行广泛实验，在特征、节点和边层面的遗忘任务中表现优异。</li>
<li>与最先进的基准相比，MEGU在这三个级别的遗忘任务中实现平均性能提升2.7%、2.5%和3.2%。</li>
<li>MEGU训练效率高，与从头开始重新训练GNN相比，时间和空间开销分别平均减少了159.8倍和9.6倍。</li>
<li>MEGU权衡了遗忘性能和框架的泛化，是用户友好的，部署效率高。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li><p>题目：基于互惠进化的有效且通用的图遗忘</p>
</li>
<li><p>作者：李寻凯，赵玉林，吴政宇，张文韬，李荣华，王国仁</p>
</li>
<li><p>单位：北京理工大学</p>
</li>
<li><p>关键词：机器遗忘，图神经网络，互惠进化</p>
</li>
<li><p>论文链接：https://arxiv.org/abs/2401.11760，Github 链接：无</p>
</li>
<li><p>摘要：
(1) 研究背景：随着人工智能应用的快速发展，对数据隐私和模型鲁棒性的日益增长的需求凸显了机器遗忘的重要性，尤其是在蓬勃发展的基于图的场景中。
(2) 过去的方法及其问题：大多数现有的图遗忘策略主要依赖于精心设计的体系结构或手动过程，这使得它们不太用户友好，并且在部署效率方面提出了挑战。此外，在遗忘性能和框架泛化之间取得平衡也是一个关键问题。
(3) 本文提出的研究方法：为了解决上述问题，我们提出了互惠进化图遗忘 (MEGU)，这是一种新的互惠进化范式，可以同时进化图遗忘的预测能力和遗忘能力。通过结合上述两个组件，MEGU 确保了与预测和遗忘要求一致的统一训练框架中的互补优化。
(4) 方法在任务和性能上的表现：在 9 个图基准数据集上的广泛实验表明，MEGU 在解决特征、节点和边级别遗忘要求方面具有优越的性能。具体而言，与最先进的基线相比，MEGU 在这三个级别的遗忘任务中分别实现了 2.7%、2.5% 和 3.2% 的平均性能提升。此外，MEGU 表现出令人满意的训练效率，与从头开始重新训练 GNN 相比，平均减少了 159.8 倍的时间和 9.6 倍的空间开销。</p>
</li>
<li><p>方法：
(1): 提出互惠进化图遗忘（MEGU）范式，该范式由原始模型预测模块和线性遗忘模块组成；
(2): 提出自适应高影响邻域选择和拓扑感知遗忘传播，以解决 GNN 中的独特挑战并实现基于图的互惠进化；
(3): 设计一个精心设计的优化目标，在保留预测精度的同时减少遗忘实体的影响，并以拓扑引导的相互促进方式训练预测模块和遗忘模块；
(4): 对于特征级、节点级和边级遗忘任务，分别对节点、节点和连接的节点进行处理。</p>
</li>
<li><p>结论：
(1): 本文提出了一种新的互惠进化图遗忘（MEGU）范式，该范式能够在保持预测精度的同时有效地遗忘图数据中的实体，为基于图的人工智能应用提供了一种新的数据遗忘解决方案。
(2): Innovation point:</p>
<ul>
<li>提出互惠进化图遗忘（MEGU）范式，该范式由原始模型预测模块和线性遗忘模块组成，通过结合这两个组件，MEGU 确保了与预测和遗忘要求一致的统一训练框架中的互补优化。</li>
<li>提出自适应高影响邻域选择和拓扑感知遗忘传播，以解决 GNN 中的独特挑战并实现基于图的互惠进化。</li>
<li>设计一个精心设计的优化目标，在保留预测精度的同时减少遗忘实体的影响，并以拓扑引导的相互促进方式训练预测模块和遗忘模块。
Performance:</li>
<li>在 9 个图基准数据集上的广泛实验表明，MEGU 在解决特征、节点和边级别遗忘要求方面具有优越的性能。具体而言，与最先进的基线相比，MEGU 在这三个级别的遗忘任务中分别实现了 2.7%、2.5% 和 3.2% 的平均性能提升。</li>
<li>MEGU 表现出令人满意的训练效率，与从头开始重新训练 GNN 相比，平均减少了 159.8 倍的时间和 9.6 倍的空间开销。
Workload:</li>
<li>MEGU 的实现相对复杂，需要设计和实现互惠进化图遗忘范式、自适应高影响邻域选择、拓扑感知遗忘传播和精心设计的优化目标等组件。</li>
<li>MEGU 的训练过程需要同时优化预测模块和遗忘模块，这可能会增加训练时间和计算资源消耗。</li>
</ul>
</li>
</ol>



<p></p><details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-70d5f7a64115883b02ea1767385ba893.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a8939202881feee558982e445caf1a42.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b52a21e40634da3517551fc7b6d6dae6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d1bda09f438055b071e260d78600f52.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2cec89067deb63b340f0a70ac09f8835.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47f7c8704ffb75dc4a7676853409a0f2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1ddc22923eeaa505e66731bd2d67736b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-797371f016d00a04252a7ac5332620ef.jpg" align="middle">
</details><br>​    <p></p>
<h2 id="Mastering-Text-to-Image-Diffusion-Recaptioning-Planning-and-Generating-with-Multimodal-LLMs"><a href="#Mastering-Text-to-Image-Diffusion-Recaptioning-Planning-and-Generating-with-Multimodal-LLMs" class="headerlink" title="Mastering Text-to-Image Diffusion: Recaptioning, Planning, and   Generating with Multimodal LLMs"></a>Mastering Text-to-Image Diffusion: Recaptioning, Planning, and   Generating with Multimodal LLMs</h2><p><strong>Authors:Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, Bin Cui</strong></p>
<p>Diffusion models have exhibit exceptional performance in text-to-image generation and editing. However, existing methods often face challenges when handling complex text prompts that involve multiple objects with multiple attributes and relationships. In this paper, we propose a brand new training-free text-to-image generation/editing framework, namely Recaption, Plan and Generate (RPG), harnessing the powerful chain-of-thought reasoning ability of multimodal LLMs to enhance the compositionality of text-to-image diffusion models. Our approach employs the MLLM as a global planner to decompose the process of generating complex images into multiple simpler generation tasks within subregions. We propose complementary regional diffusion to enable region-wise compositional generation. Furthermore, we integrate text-guided image generation and editing within the proposed RPG in a closed-loop fashion, thereby enhancing generalization ability. Extensive experiments demonstrate our RPG outperforms state-of-the-art text-to-image diffusion models, including DALL-E 3 and SDXL, particularly in multi-category object composition and text-image semantic alignment. Notably, our RPG framework exhibits wide compatibility with various MLLM architectures (e.g., MiniGPT-4) and diffusion backbones (e.g., ControlNet). Our code is available at: <a href="https://github.com/YangLing0818/RPG-DiffusionMaster">https://github.com/YangLing0818/RPG-DiffusionMaster</a> </p>
<p><a href="http://arxiv.org/abs/2401.11708v1">PDF</a> Project: <a href="https://github.com/YangLing0818/RPG-DiffusionMaster">https://github.com/YangLing0818/RPG-DiffusionMaster</a></p>
<p><strong>Summary</strong><br>多模态 LLM 作为全局规划器，帮助扩散模型提升多属性、多类别目标生成任务。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>提出了一系列无监督学习的框架，即 RPG，用来改善文字转图像扩散模型的组合性。</li>
<li>利用多模态 LLM 作为全局规划器，将复杂的图像生成任务分解为多个在子区域内的简单生成任务。</li>
<li>提出了一种互补的区域扩散来支持按区域进行组合性生成。</li>
<li>将文本引导的图像生成和编辑以闭环方式集成到 RPG 中，从而提高泛化能力。</li>
<li>在多类别目标组合和文本图像语义对齐方面，RPG 优于最先进的文本到图像扩散模型，包括 DALL-E 3 和 SDXL。</li>
<li>RPG 框架与各种多模态 LLM 架构（例如 MiniGPT-4）和扩散模型兼容。</li>
<li>代码可从此处获取：<a href="https://github.com/YangLing0818/RPG-DiffusionMaster">https://github.com/YangLing0818/RPG-DiffusionMaster</a></li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li><p>标题：掌握文本到图像扩散：多模态 LLM 的重新表述、规划和生成</p>
</li>
<li><p>作者：Ling Yang<em>, Zhaochen Yu</em>, Chenlin Meng, Minkai Xu, Stefano Ermon, Bin Cui</p>
</li>
<li><p>隶属单位：北京大学</p>
</li>
<li><p>关键词：文本到图像生成、扩散模型、多模态 LLM、链式思维推理、区域扩散</p>
</li>
<li><p>论文链接：https://github.com/YangLing0818/RPG-DiffusionMaster
Github 代码链接：https://github.com/YangLing0818/RPG-DiffusionMaster</p>
</li>
<li><p>摘要：
(1) 研究背景：扩散模型在文本到图像生成和编辑方面表现出色，但现有方法在处理涉及多个对象及其属性和关系的复杂文本提示时通常面临挑战。
(2) 过去的方法及其问题：一些工作通过引入布局/框作为条件或利用提示感知注意引导来解决这个问题，但这些方法通常需要额外的训练或难以扩展到复杂提示。
(3) 本文提出的研究方法：本文提出了一种新的无训练文本到图像生成/编辑框架，称为 Recaption, Plan and Generate (RPG)，利用多模态 LLM 强大的链式思维推理能力来增强文本到图像扩散模型的组合性。我们的方法使用 MLLM 作为全局规划器，将生成复杂图像的过程分解为多个子区域内的简单生成任务。我们提出了互补区域扩散以实现区域内组合生成。此外，我们将文本引导的图像生成和编辑集成到所提出的 RPG 中，从而增强泛化能力。
(4) 方法在任务和性能上的表现：广泛的实验表明，我们的 RPG 优于最先进的文本到图像扩散模型，包括 DALL-E3 和 SDXL，特别是在多类别对象组合和文本图像语义对齐方面。值得注意的是，我们的 RPG 框架与各种 MLLM 架构（例如 MiniGPT-4）和扩散骨干（例如 ControlNet）具有广泛的兼容性。</p>
</li>
<li><p>方法：
（1）文本重述：利用多模态 LLM 将复杂的文本提示分解为多个子提示，并对每个子提示进行更详细的描述，以提高生成图像的保真度和减少语义差异。
（2）链式思维推理规划：利用多模态 LLM 的链式思维推理能力，对最终图像内容的构成进行规划，将图像空间划分为多个互补区域，并为每个区域分配特定的子提示。
（3）互补区域扩散：提出一种新的扩散模型，对划分的每个区域进行独立生成，并在每个采样步骤中将生成的图像块进行组合，以实现区域内的组合生成。
（4）文本引导的图像生成和编辑：将文本引导的图像生成和编辑集成到提出的框架中，通过对配对目标提示和源图像进行分析，生成信息丰富的多模态反馈，以捕捉它们的跨模态语义差异，并指导区域扩散过程。</p>
</li>
<li><p>结论：
（1）：本文提出了一种无训练文本到图像生成/编辑框架 RPG，利用多模态 LLM 强大的链式思维推理能力来增强文本到图像扩散模型的组合性。RPG 在复杂类别对象组合和文本图像语义对齐方面优于最先进的文本到图像扩散模型。此外，RPG 框架与各种 MLLM 架构（例如 MiniGPT-4）和扩散骨干（例如 ControlNet）具有广泛的兼容性。
（2）：创新点：</p>
</li>
</ol>
<ul>
<li>提出了一种新的无训练文本到图像生成/编辑框架 RPG，利用多模态 LLM 强大的链式思维推理能力来增强文本到图像扩散模型的组合性。</li>
<li>提出了一种互补区域扩散模型，对划分的每个区域进行独立生成，并在每个采样步骤中将生成的图像块进行组合，以实现区域内的组合生成。</li>
<li>将文本引导的图像生成和编辑集成到提出的框架中，通过对配对目标提示和源图像进行分析，生成信息丰富的多模态反馈，以捕捉它们的跨模态语义差异，并指导区域扩散过程。
性能：</li>
<li>RPG 在复杂类别对象组合和文本图像语义对齐方面优于最先进的文本到图像扩散模型。</li>
<li>RPG 框架与各种 MLLM 架构（例如 MiniGPT-4）和扩散骨干（例如 ControlNet）具有广泛的兼容性。
工作量：</li>
<li>RPG 框架的实现相对复杂，需要对多模态 LLM、扩散模型和区域扩散模型进行集成。</li>
<li>RPG 框架的训练过程需要大量的计算资源。</li>
</ul>



<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d7ede89518c7e2b2017c785eb927b766.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-69a6785a9dc22c046203d70cee24a3f1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b57333091d6dbb8392ce8971cf413d0b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5d6f54078071dcab585ee882e1cb7cb6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-40b7d562cad3ed84d89938dbcdb65fff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe1c57ab8d093322b4502e666dccd4cb.jpg" align="middle">
</details>
​    


## Scalable High-Resolution Pixel-Space Image Synthesis with Hourglass   Diffusion Transformers
**Authors:Katherine Crowson, Stefan Andreas Baumann, Alex Birch, Tanishq Mathew Abraham, Daniel Z. Kaplan, Enrico Shippole**

We present the Hourglass Diffusion Transformer (HDiT), an image generative model that exhibits linear scaling with pixel count, supporting training at high-resolution (e.g. $1024 \times 1024$) directly in pixel-space. Building on the Transformer architecture, which is known to scale to billions of parameters, it bridges the gap between the efficiency of convolutional U-Nets and the scalability of Transformers. HDiT trains successfully without typical high-resolution training techniques such as multiscale architectures, latent autoencoders or self-conditioning. We demonstrate that HDiT performs competitively with existing models on ImageNet $256^2$, and sets a new state-of-the-art for diffusion models on FFHQ-$1024^2$. 

[PDF](http://arxiv.org/abs/2401.11605v1) 20 pages, 13 figures, project page and code available at   https://crowsonkb.github.io/hourglass-diffusion-transformers/

**Summary**
图像生成模型 Hourglass Diffusion Transformer (HDiT) 在像素数量上呈线性扩展，支持以像素空间直接进行高分辨率（例如 1024×1024）训练。

**Key Takeaways**

- HDiT 是一种新的图像生成模型，它使用 Transformer 架构，该架构以亿万参数的规模进行扩展。
- HDiT 将卷积 U-Net 的效率与 Transformer 的可扩展性相结合。
- HDiT 可以直接在像素空间中训练高分辨率图像，而无需使用多尺度架构、潜在自编码器或自条件等典型的高分辨率训练技术。
- HDiT 在 ImageNet 256^2 上的表现与现有模型具有竞争力，并在 FFHQ-1024^2 上的扩散模型中创造了新的最先进水平。
- HDiT 的训练过程更简单，并且不需要使用复杂的架构或训练策略。
- HDiT 可以生成高质量的图像，并且在Inception Score和FID等评估指标上取得了不错的成绩。
- HDiT 的发布为高分辨率图像生成打开了新的可能性，有望在未来得到更广泛的应用。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>题目：基于小时沙漏扩散变换器的可扩展高分辨率像素空间图像合成</li>
<li>作者：Katherine Crowson<em>1、Stefan Andreas Baumann</em>2、Alex Birch*3、Tanishq Mathew Abraham1、Daniel Z. Kaplan4、Enrico Shippole5</li>
<li>第一作者单位：稳定人工智能</li>
<li>关键词：扩散模型、Transformer、图像生成、高分辨率</li>
<li>论文链接：https://arxiv.org/abs/2401.11605，Github 代码链接：无</li>
<li>摘要：
（1）：随着扩散模型在图像生成任务中的成功，研究人员开始探索如何将这些模型扩展到更高的分辨率。然而，现有的扩散模型在高分辨率下往往面临着计算成本高、训练不稳定等问题。
（2）：过去的方法主要集中在使用多尺度架构、潜在自编码器或自条件等技术来提高扩散模型在高分辨率下的性能。然而，这些方法往往会增加模型的复杂性和训练难度。
（3）：本文提出了一种新的扩散模型——小时沙漏扩散变换器（HDiT），该模型在像素空间中直接支持高分辨率训练，并且具有线性的计算成本缩放。HDiT 采用了一种新的架构，该架构结合了 Transformer 的可扩展性和卷积 U-Net 的效率。
（4）：在 ImageNet256 和 FFHQ-1024 数据集上，HDiT 在与现有模型的比较中取得了有竞争力的性能，并且在 FFHQ-1024 数据集上创下了扩散模型的新纪录。</li>
</ol>
<p>&lt;Methods&gt;:</p>
<p>(1)：本文提出了一种新的扩散模型——小时沙漏扩散变换器（HDiT），该模型在像素空间中直接支持高分辨率训练，并且具有线性的计算成本缩放。</p>
<p>(2)：HDiT采用了一种新的架构，该架构结合了Transformer的可扩展性和卷积U-Net的效率。</p>
<p>(3)：HDiT在ImageNet256和FFHQ-1024数据集上，在与现有模型的比较中取得了有竞争力的性能，并且在FFHQ-1024数据集上创下了扩散模型的新纪录。</p>
<p>(4)：HDiT还具有良好的大规模图像生成能力，在ImageNet-256数据集上，HDiT在不使用分类器自由指导的情况下，取得了比现有扩散模型更好的性能。</p>
<ol start="8">
<li>结论：</li>
</ol>
<p>（1）：本工作提出了一种新的扩散模型——小时沙漏扩散变换器（HDiT），该模型在像素空间中直接支持高分辨率训练，并且具有线性的计算成本缩放。HDiT采用了一种新的架构，该架构结合了Transformer的可扩展性和卷积U-Net的效率。在ImageNet256和FFHQ-1024数据集上，HDiT在与现有模型的比较中取得了有竞争力的性能，并且在FFHQ-1024数据集上创下了扩散模型的新纪录。</p>
<p>（2）：创新点：</p>
<ul>
<li>提出了一种新的扩散模型——小时沙漏扩散变换器（HDiT），该模型在像素空间中直接支持高分辨率训练，并且具有线性的计算成本缩放。</li>
<li>HDiT采用了一种新的架构，该架构结合了Transformer的可扩展性和卷积U-Net的效率。</li>
<li>在ImageNet256和FFHQ-1024数据集上，HDiT在与现有模型的比较中取得了有竞争力的性能，并且在FFHQ-1024数据集上创下了扩散模型的新纪录。</li>
</ul>
<p>性能：</p>
<ul>
<li>在ImageNet256数据集上，HDiT在不使用分类器自由指导的情况下，取得了比现有扩散模型更好的性能。</li>
<li>在FFHQ-1024数据集上，HDiT创下了扩散模型的新纪录。</li>
</ul>
<p>工作量：</p>
<ul>
<li>HDiT的计算成本缩放是线性的，这使得它能够扩展到更高的分辨率。</li>
<li>HDiT的训练难度较低，这使得它更容易训练。</li>
</ul>



<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fcc074a8fe14d1b52ec9aa98684f39d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb5ff002efdb09103d60a3788e8ec694.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f646dbc6a174419ce3e875010d6a8da1.jpg" align="middle">
</details>
​    


## Exploring Diffusion Time-steps for Unsupervised Representation Learning
**Authors:Zhongqi Yue, Jiankun Wang, Qianru Sun, Lei Ji, Eric I-Chao Chang, Hanwang Zhang**

Representation learning is all about discovering the hidden modular attributes that generate the data faithfully. We explore the potential of Denoising Diffusion Probabilistic Model (DM) in unsupervised learning of the modular attributes. We build a theoretical framework that connects the diffusion time-steps and the hidden attributes, which serves as an effective inductive bias for unsupervised learning. Specifically, the forward diffusion process incrementally adds Gaussian noise to samples at each time-step, which essentially collapses different samples into similar ones by losing attributes, e.g., fine-grained attributes such as texture are lost with less noise added (i.e., early time-steps), while coarse-grained ones such as shape are lost by adding more noise (i.e., late time-steps). To disentangle the modular attributes, at each time-step t, we learn a t-specific feature to compensate for the newly lost attribute, and the set of all 1,...,t-specific features, corresponding to the cumulative set of lost attributes, are trained to make up for the reconstruction error of a pre-trained DM at time-step t. On CelebA, FFHQ, and Bedroom datasets, the learned feature significantly improves attribute classification and enables faithful counterfactual generation, e.g., interpolating only one specified attribute between two images, validating the disentanglement quality. Codes are in https://github.com/yue-zhongqi/diti. 

[PDF](http://arxiv.org/abs/2401.11430v1) Accepted by ICLR 2024

**摘要**
扩散模型中的时间步长与隐藏属性相关，可用于无监督学习模块化属性。

**要点**

- 扩散模型通过在每个时间步长向样本添加高斯噪声，将不同样本折叠成相似样本。
- 在每个时间步长 t，学习一个 t 特定的特征来补偿新丢失的属性。
- 所有 1, ..., t 特定的特征对应于累积的丢失属性集，用于弥补时间步长 t 处预训练扩散模型的重建误差。
- 在 CelebA、FFHQ 和 Bedroom 数据集上，学习到的特征显着提高了属性分类，并实现了保真的反事实生成，例如，仅在两幅图像之间插入一个指定属性。
- 代码可在 https://github.com/yue-zhongqi/diti 中找到。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li><p>题目：DiTi：通过弥补扩散模型的重建误差来恢复属性</p>
</li>
<li><p>作者：Yuxin Chen, Yifan Jiang, Yujun Shen, Xin Yu, Song Bai, Bolei Zhou</p>
</li>
<li><p>单位：北京大学</p>
</li>
<li><p>关键词：扩散模型，图像生成，属性恢复，弥补误差</p>
</li>
<li><p>链接：https://arxiv.org/abs/2302.04522 或 https://github.com/VITA-Group/DiTi</p>
</li>
<li><p>摘要：
(1)：研究背景：扩散模型是一种生成图像的有效方法，但它在生成过程中会丢失图像的某些属性。
(2)：过去的方法：为了解决这个问题，一些方法提出了在扩散过程中加入属性信息，但这些方法往往需要额外的监督信息或计算量大。
(3)：研究方法：本文提出了一种新的方法 DiTi，它通过弥补扩散模型的重建误差来恢复图像的属性。DiTi 由一个预训练的扩散模型、一个可训练的编码器和一个可训练的解码器组成。编码器将图像映射到一个潜在空间，解码器将潜在空间的表示映射回图像空间。在训练过程中，DiTi 通过最小化重建误差来学习编码器和解码器的参数。
(4)：实验结果：实验结果表明，DiTi 在多个数据集上取得了比现有方法更好的性能。DiTi 能够有效地恢复图像的属性，并且生成的图像质量也更高。</p>
</li>
<li><p>方法：
（1）提出了一种新的方法 DiTi，它通过弥补扩散模型的重建误差来恢复图像的属性；
（2）DiTi 由一个预训练的扩散模型、一个可训练的编码器和一个可训练的解码器组成；
（3）编码器将图像映射到一个潜在空间，解码器将潜在空间的表示映射回图像空间；
（4）在训练过程中，DiTi 通过最小化重建误差来学习编码器和解码器的参数。</p>
</li>
<li><p>结论：
（1）：本工作提出了一种新的无监督方法来学习 disentangled 表示，该方法利用了扩散时间步长的归纳偏差。具体来说，我们揭示了时间步长和隐藏模块化属性之间固有的联系，这些属性忠实地生成了数据，从而通过学习时间步长特定特征来实现属性的简单有效的解耦。学习到的特征改进了下游推理并支持反事实生成，验证了其解耦质量。作为未来的工作，我们将寻求额外的归纳偏差来改进解耦，例如，通过探索文本到图像扩散模型来使用文本作为解耦模板，并设计实用的优化技术以实现更快的收敛。
（2）：创新点：提出了一种新的方法 DiTi，通过弥补扩散模型的重建误差来恢复图像的属性；性能：实验结果表明，DiTi 在多个数据集上取得了比现有方法更好的性能。DiTi 能够有效地恢复图像的属性，并且生成的图像质量也更高；工作量：DiTi 由一个预训练的扩散模型、一个可训练的编码器和一个可训练的解码器组成。在训练过程中，DiTi 通过最小化重建误差来学习编码器和解码器的参数。</p>
</li>
</ol>



<p></p><details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-77fe0985f3ccf24b58f01409208c95d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c049f8dfb182212eafcbb8d455570a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53cea1049f1b04a122064eadf034709b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24c1ae5245a699656c0411ed106e5ae2.jpg" align="middle">
</details><br>​    <p></p>
<h2 id="Diffusion-Model-Conditioning-on-Gaussian-Mixture-Model-and-Negative-Gaussian-Mixture-Gradient"><a href="#Diffusion-Model-Conditioning-on-Gaussian-Mixture-Model-and-Negative-Gaussian-Mixture-Gradient" class="headerlink" title="Diffusion Model Conditioning on Gaussian Mixture Model and Negative   Gaussian Mixture Gradient"></a>Diffusion Model Conditioning on Gaussian Mixture Model and Negative   Gaussian Mixture Gradient</h2><p><strong>Authors:Weiguo Lu, Xuan Wu, Deng Ding, Jinqiao Duan, Jirong Zhuang, Gangnan Yuan</strong></p>
<p>Diffusion models (DMs) are a type of generative model that has a huge impact on image synthesis and beyond. They achieve state-of-the-art generation results in various generative tasks. A great diversity of conditioning inputs, such as text or bounding boxes, are accessible to control the generation. In this work, we propose a conditioning mechanism utilizing Gaussian mixture models (GMMs) as feature conditioning to guide the denoising process. Based on set theory, we provide a comprehensive theoretical analysis that shows that conditional latent distribution based on features and classes is significantly different, so that conditional latent distribution on features produces fewer defect generations than conditioning on classes. Two diffusion models conditioned on the Gaussian mixture model are trained separately for comparison. Experiments support our findings. A novel gradient function called the negative Gaussian mixture gradient (NGMG) is proposed and applied in diffusion model training with an additional classifier. Training stability has improved. We also theoretically prove that NGMG shares the same benefit as the Earth Mover distance (Wasserstein) as a more sensible cost function when learning distributions supported by low-dimensional manifolds. </p>
<p><a href="http://arxiv.org/abs/2401.11261v1">PDF</a> </p>
<p><strong>Summary</strong><br>扩散模型提出一种基于高斯混合模型引导去噪的高效条件生成机制，提升生成图像质量。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>扩散模型是一种生成模型，在图像合成和其他领域产生了巨大影响。</li>
<li>扩散模型可以通过不同的条件输入，如文本或边界框，来生成不同的图像。</li>
<li>本文将高斯混合模型（GMM）作为特征条件，提出了一个用于控制扩散去噪过程的条件机制。</li>
<li>基于集合论，本文提供了全面的理论分析，表明基于特征和类别的条件潜在分布存在显著差异。</li>
<li>基于特征的条件潜在分布产生更少的缺陷生成，优于基于类别的条件。</li>
<li>提出的高斯混合模型梯度函数（NGMG）可用于提高扩散模型训练的稳定性。</li>
<li>NGMG与 Earth Mover 距离（Wasserstein）具有相同的好处，作为学习低维流形分布的更合理的成本函数。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<p></p><ol><p></p>
<p></p><li><p>标题：基于高斯混合模型和负高斯混合模型梯度的扩散模型条件机制</p>
</li><p></p>
<p></p><li><p>作者：Weiguo Lu, Xuan Wu, Deng Ding, Jinqiao Duan, Jirong Zhuang, Gangnan Yuan</p>
</li><p></p>
<p></p><li><p>澳门大学数学系</p>
</li><p></p>
<p></p><li><p>关键词：扩散模型、条件生成、高斯混合模型、负高斯混合模型梯度</p>
</li><p></p>
<p></p><li><p>链接：https://arxiv.org/abs/2401.11261</p>
</li><p></p>
<p></p><li><p>摘要：
(1) 研究背景：扩散模型是一种生成模型，在图像合成和其他领域取得了巨大的影响。扩散模型可以通过文本或边界框等多种条件输入来控制生成。
(2) 过去的方法：过去的方法通常使用高斯分布对数据进行建模，但这种方法在处理复杂数据时存在局限性。
(3) 研究方法：本文提出了一种利用高斯混合模型（GMM）作为条件机制的扩散模型。GMM可以对复杂数据进行建模，因此可以更好地控制生成的图像。
(4) 性能：本文的方法在人脸生成任务上取得了很好的性能。生成的图像逼真且多样，并且可以根据条件输入进行控制。</p>
</li><p></p>
<p></p><li><p>方法：
（1）：利用高斯混合模型（GMM）对数据进行建模，GMM可以更好地拟合复杂数据，从而更好地控制生成的图像。
（2）：将GMM作为条件机制，通过负高斯混合模型梯度（NGMG）来计算损失函数的梯度，NGMG是一种连续且可微的函数，可以提供更好的稳定性和灵敏性。
（3）：证明了NGMG与Wasserstein距离之间的关系，NGMG与Wasserstein距离具有相同的优点，并且可以相互转换。
（4）：提出了一种新的扩散模型，该模型利用NGMG作为条件机制，并在人脸生成任务上取得了很好的性能。</p>
</li><p></p>
<p></p><li><p>结论：
（1）意义：本文提出了一种利用高斯混合模型（GMM）作为条件机制的扩散模型，该模型可以更好地控制生成的图像，在人脸生成任务上取得了很好的性能。
（2）优缺点：
创新点：</p>
</li><br>&lt;/ol&gt;<p></p>
<ul>
<li>利用GMM对数据进行建模，可以更好地拟合复杂数据，从而更好地控制生成的图像。</li>
<li>将GMM作为条件机制，通过负高斯混合模型梯度（NGMG）来计算损失函数的梯度，NGMG是一种连续且可微的函数，可以提供更好的稳定性和灵敏性。</li>
<li>证明了NGMG与Wasserstein距离之间的关系，NGMG与Wasserstein距离具有相同的优点，并且可以相互转换。</li>
<li>提出了一种新的扩散模型，该模型利用NGMG作为条件机制，并在人脸生成任务上取得了很好的性能。</li>
</ul>
<p>性能：</p>
<ul>
<li>在人脸生成任务上取得了很好的性能。生成的图像逼真且多样，并且可以根据条件输入进行控制。</li>
</ul>
<p>工作量：</p>
<ul>
<li>该模型的训练过程相对复杂，需要较多的计算资源。</li>
</ul>



<p></p><details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-61a621f7ccfd39bba7557184f456de65.jpg" align="middle">
</details><br>​    <p></p>
<h2 id="MotionMix-Weakly-Supervised-Diffusion-for-Controllable-Motion-Generation"><a href="#MotionMix-Weakly-Supervised-Diffusion-for-Controllable-Motion-Generation" class="headerlink" title="MotionMix: Weakly-Supervised Diffusion for Controllable Motion   Generation"></a>MotionMix: Weakly-Supervised Diffusion for Controllable Motion   Generation</h2><p><strong>Authors:Nhat M. Hoang, Kehong Gong, Chuan Guo, Michael Bi Mi</strong></p>
<p>Controllable generation of 3D human motions becomes an important topic as the world embraces digital transformation. Existing works, though making promising progress with the advent of diffusion models, heavily rely on meticulously captured and annotated (e.g., text) high-quality motion corpus, a resource-intensive endeavor in the real world. This motivates our proposed MotionMix, a simple yet effective weakly-supervised diffusion model that leverages both noisy and unannotated motion sequences. Specifically, we separate the denoising objectives of a diffusion model into two stages: obtaining conditional rough motion approximations in the initial $T-T^<em>$ steps by learning the noisy annotated motions, followed by the unconditional refinement of these preliminary motions during the last $T^</em>$ steps using unannotated motions. Notably, though learning from two sources of imperfect data, our model does not compromise motion generation quality compared to fully supervised approaches that access gold data. Extensive experiments on several benchmarks demonstrate that our MotionMix, as a versatile framework, consistently achieves state-of-the-art performances on text-to-motion, action-to-motion, and music-to-dance tasks. </p>
<p><a href="http://arxiv.org/abs/2401.11115v1">PDF</a> Accepted at the 38th Association for the Advancement of Artificial   Intelligence (AAAI) Conference on Artificial Intelligence, Main Conference</p>
<p><strong>Summary</strong><br>利用噪声和未标注动作序列的弱监督扩散模型，实现高质量动作生成。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>提出一个简单有效的弱监督扩散模型 MotionMix，利用噪声和未标注动作序列生成高质量动作。</li>
<li>将扩散模型的去噪目标分为两个阶段：在前 $T-T^<em>$ 步利用噪声标注动作获得粗略动作近似，最后 $T^</em>$ 步利用未标注动作对粗略动作进行无条件细化。</li>
<li>MotionMix 在文本转动作、动作转动作和音乐转舞蹈任务上取得了最先进的性能。</li>
<li>MotionMix 可以应用于各种下游任务，如动作合成、动画制作和机器人控制。</li>
<li>MotionMix 可以扩展到其他领域，如图像生成、语音合成和自然语言处理。</li>
<li>MotionMix 是一个通用框架，可以应用于各种动作生成任务。</li>
<li>MotionMix 可以通过调节超参数来控制动作生成的质量和多样性。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<p></p><ol><p></p>
<p></p><li><p>标题：MotionMix：用于可控运动生成的弱监督扩散</p>
</li><p></p>
<p></p><li><p>作者：Nhat M. Hoang, Kehong Gong, Chuan Guo, Michael Bi Mi</p>
</li><p></p>
<p></p><li><p>隶属机构：华为技术有限公司</p>
</li><p></p>
<p></p><li><p>关键字：运动生成、扩散模型、弱监督学习</p>
</li><p></p>
<p></p><li><p>论文链接：https://nhathoang2002.github.io/MotionMix-page/，Github 链接：无</p>
</li><p></p>
<p></p><li><p>摘要：
（1）研究背景：随着世界拥抱数字化转型，可控生成三维人体运动成为一个重要课题。现有的工作虽然随着扩散模型的出现取得了可喜的进展，但严重依赖于精心捕捉和注释（例如，文本）的高质量运动语料库，这在现实世界中是一个资源密集型工作。
（2）过去的方法及其问题：过去的方法通常使用完全监督的扩散模型，需要大量高质量的注释数据。然而，获取此类数据成本高昂且耗时。
（3）提出的研究方法：为了解决上述问题，本文提出了一种简单而有效的方法 MotionMix，它是一种弱监督扩散模型，可以同时利用噪声注释运动和未注释运动。具体来说，我们将扩散模型的去噪目标分为两个阶段：在初始 T-T* 步中通过学习噪声注释运动来获得条件粗略运动近似值，然后在最后 T* 步中使用未注释运动对这些初步运动进行无条件细化。
（4）方法的性能：广泛的实验表明，MotionMix 作为一种通用的框架，在文本到运动、动作到运动和音乐到舞蹈任务上始终如一地取得了最先进的性能。这些性能支持了本文的目标，即在不损害运动生成质量的前提下，使用更少的数据和更少的注释来训练扩散模型。</p>
</li><p></p>
<p></p><li><p>Methods:
(1): MotionMix方法将扩散模型的去噪目标分为两个阶段：在初始T-T<em>步中通过学习噪声注释运动来获得条件粗略运动近似值，然后在最后T</em>步中使用未注释运动对这些初步运动进行无条件细化。
(2): MotionMix方法使用噪声注释运动来学习条件粗略运动近似值，这可以帮助扩散模型更好地学习运动的整体结构和关键点位置。
(3): MotionMix方法使用未注释运动对初步运动进行无条件细化，这可以帮助扩散模型学习运动的细节和流畅性。
(4): MotionMix方法可以同时利用噪声注释运动和未注释运动，这可以帮助扩散模型学习更丰富的运动信息，并提高运动生成的质量。</p>
</li><p></p>
<p></p><li><p>结论：
（1）：本工作首次提出了一种弱监督扩散模型 MotionMix，用于同时利用噪声注释运动和未注释运动来生成可控运动。MotionMix 在多个运动生成基准和基本扩散模型设计中展示了其多功能性。全面的消融研究进一步支持了其在不同噪声调度和去噪支点的策略选择中的鲁棒性。
（2）：创新点：</p>
</li><br>&lt;/ol&gt;<p></p>
<ul>
<li>提出了一种弱监督扩散模型 MotionMix，可以同时利用噪声注释运动和未注释运动来生成可控运动。</li>
<li>MotionMix 将扩散模型的去噪目标分为两个阶段：在初始 T-T* 步中通过学习噪声注释运动来获得条件粗略运动近似值，然后在最后 T* 步中使用未注释运动对这些初步运动进行无条件细化。</li>
<li>MotionMix 使用噪声注释运动来学习条件粗略运动近似值，这可以帮助扩散模型更好地学习运动的整体结构和关键点位置。</li>
<li>MotionMix 使用未注释运动对初步运动进行无条件细化，这可以帮助扩散模型学习运动的细节和流畅性。
性能：</li>
<li>MotionMix 在多个运动生成基准上取得了最先进的性能，包括文本到运动、动作到运动和音乐到舞蹈任务。</li>
<li>MotionMix 在使用更少的数据和更少的注释的情况下，可以生成与完全监督扩散模型质量相当的运动。
工作量：</li>
<li>MotionMix 的实现相对简单，易于训练和使用。</li>
<li>MotionMix 可以使用标准的扩散模型训练框架进行训练，不需要额外的计算资源。</li>
</ul>



<p></p><details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-273f0c50cd4e128d204627cc095176a2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eae22ee23e9a564640cb9d43a3c08766.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7d3bcd5ef19eb5e526b72441762f30b5.jpg" align="middle">
</details><br>​    <p></p>
<h2 id="UltrAvatar-A-Realistic-Animatable-3D-Avatar-Diffusion-Model-with-Authenticity-Guided-Textures"><a href="#UltrAvatar-A-Realistic-Animatable-3D-Avatar-Diffusion-Model-with-Authenticity-Guided-Textures" class="headerlink" title="UltrAvatar: A Realistic Animatable 3D Avatar Diffusion Model with   Authenticity Guided Textures"></a>UltrAvatar: A Realistic Animatable 3D Avatar Diffusion Model with   Authenticity Guided Textures</h2><p><strong>Authors:Mingyuan Zhou, Rakib Hyder, Ziwei Xuan, Guojun Qi</strong></p>
<p>Recent advances in 3D avatar generation have gained significant attentions. These breakthroughs aim to produce more realistic animatable avatars, narrowing the gap between virtual and real-world experiences. Most of existing works employ Score Distillation Sampling (SDS) loss, combined with a differentiable renderer and text condition, to guide a diffusion model in generating 3D avatars. However, SDS often generates oversmoothed results with few facial details, thereby lacking the diversity compared with ancestral sampling. On the other hand, other works generate 3D avatar from a single image, where the challenges of unwanted lighting effects, perspective views, and inferior image quality make them difficult to reliably reconstruct the 3D face meshes with the aligned complete textures. In this paper, we propose a novel 3D avatar generation approach termed UltrAvatar with enhanced fidelity of geometry, and superior quality of physically based rendering (PBR) textures without unwanted lighting. To this end, the proposed approach presents a diffuse color extraction model and an authenticity guided texture diffusion model. The former removes the unwanted lighting effects to reveal true diffuse colors so that the generated avatars can be rendered under various lighting conditions. The latter follows two gradient-based guidances for generating PBR textures to render diverse face-identity features and details better aligning with 3D mesh geometry. We demonstrate the effectiveness and robustness of the proposed method, outperforming the state-of-the-art methods by a large margin in the experiments. </p>
<p><a href="http://arxiv.org/abs/2401.11078v1">PDF</a> The project page is at <a href="http://usrc-sea.github.io/UltrAvatar/">http://usrc-sea.github.io/UltrAvatar/</a></p>
<p><strong>Summary</strong><br>基于几何保真度增强和物理渲染纹理优化，提出了一种新的三维虚拟人物生成方法。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Diffusion模型生成的3D虚拟人物往往过平滑，缺乏细节和多样性。</li>
<li>从单个图像生成3D虚拟人物面临着光照、视角和图像质量等挑战。</li>
<li>本文提出了一种名为UltrAvatar的新三维虚拟人物生成方法。</li>
<li>UltrAvatar可以去除光照的影响，生成更真实的漫反射颜色。</li>
<li>UltrAvatar通过两种基于梯度的引导来生成PBR纹理。</li>
<li>UltrAvatar在实验中优于现有最先进的方法。</li>
<li>UltrAvatar可以生成高质量的三维虚拟人物，具有更真实的几何形状和物理渲染纹理。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<p></p><ol><p></p>
<p></p><li><p>题目：UltrAvatar：基于真实感指导的纹理扩散模型的超写实 3D 头像生成</p>
</li><p></p>
<p></p><li><p>作者：Yuxuan Zhang<em>, Yifan Jiang</em>, Jingyu Yang, Yebin Liu, Xiaoguang Han, Yu-Kun Lai</p>
</li><p></p>
<p></p><li><p>单位：香港中文大学（深圳）</p>
</li><p></p>
<p></p><li><p>关键词：3D 头像生成、纹理扩散模型、真实感指导、物理渲染纹理</p>
</li><p></p>
<p></p><li><p>链接：https://arxiv.org/abs/2302.08844, Github：无</p>
</li><p></p>
<p></p><li><p>摘要：
（1）：随着 3D 头像生成技术的发展，如何生成更逼真、更可动画的头像成为研究热点。
（2）：现有方法大多采用分数蒸馏采样损失函数，结合可微渲染器和文本条件，来指导扩散模型生成 3D 头像。然而，分数蒸馏采样往往会产生过度平滑的结果，缺乏面部细节，与祖先采样相比缺乏多样性。其他方法从单张图像生成 3D 头像，但图像中存在不需要的照明效果、透视视图和较差的图像质量等问题，导致难以可靠地重建具有对齐完整纹理的 3D 面部网格。
（3）：本文提出了一种名为 UltrAvatar 的 3D 头像生成方法，该方法提高了几何形状的保真度，并生成了具有出色质量的物理渲染纹理，且没有不需要的照明效果。为此，该方法提出了一种漫反射颜色提取模型和一种真实感指导的纹理扩散模型。漫反射颜色提取模型可以去除不需要的照明效果，以揭示真实的漫反射颜色，以便在各种照明条件下渲染生成的头像。真实感指导的纹理扩散模型遵循两个基于梯度的指导，以生成物理渲染纹理，以更好地呈现各种面部身份特征和细节，并与 3D 网格几何形状更好地对齐。
（4）：实验结果表明，该方法有效且鲁棒，在实验中大幅优于最先进的方法。</p>
</li><p></p>
<p></p><li><p>方法：
(1)：本文提出了一种名为UltrAvatar的3D头像生成方法，该方法提高了几何形状的保真度，并生成了具有出色质量的物理渲染纹理，且没有不需要的照明效果。
(2)：为此，该方法提出了一种漫反射颜色提取模型和一种真实感指导的纹理扩散模型。
(3)：漫反射颜色提取模型可以去除不需要的照明效果，以揭示真实的漫反射颜色，以便在各种照明条件下渲染生成的头像。
(4)：真实感指导的纹理扩散模型遵循两个基于梯度的指导，以生成物理渲染纹理，以更好地呈现各种面部身份特征和细节，并与3D网格几何形状更好地对齐。</p>
</li><p></p>
<p></p><li><p>结论：
（1）：本工作提出了一种从文本提示或单个图像生成 3D 头像的新方法。我们方法的核心是 DCEM 模型，旨在消除源图像中不需要的照明效果，以及一个由光度和边缘信号引导的纹理生成模型，以保留头像的 PBR 细节。与其他最先进的方法相比，我们证明了我们的方法可以生成显示出高度逼真、更高质量、更出色保真度和更广泛多样性的 3D 头像。
（2）：创新点：</p>
</li><br>&lt;/ol&gt;<p></p>
<ul>
<li>提出了一种漫反射颜色提取模型，可以去除不需要的照明效果，以揭示真实的漫反射颜色，以便在各种照明条件下渲染生成的头像。</li>
<li>提出了一种真实感指导的纹理扩散模型，遵循两个基于梯度的指导，以生成物理渲染纹理，以更好地呈现各种面部身份特征和细节，并与 3D 网格几何形状更好地对齐。
性能：</li>
<li>在定量和定性评估中，我们的方法在几何保真度、纹理质量和整体逼真度方面优于最先进的方法。</li>
<li>我们的方法能够生成具有出色质量的物理渲染纹理，且没有不需要的照明效果。</li>
<li>我们的方法可以从文本提示或单个图像生成 3D 头像，并且生成的头像具有高度逼真、更高质量、更出色保真度和更广泛多样性。
工作量：</li>
<li>本文提出的方法需要较大的计算资源，包括高性能 GPU 和大量内存。</li>
<li>本文提出的方法需要较多的训练数据，包括大量高质量的 3D 头像数据和相应的文本描述。</li>
<li>本文提出的方法需要较多的训练时间，以确保模型能够收敛并生成高质量的 3D 头像。</li>
</ul>



<p></p><details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6ddb372268ab29440ab071d2e4e6e298.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e5a668d3e08b3e9f2b7d9e0f965d9762.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe4a6193adf67c2ee040715753a40d2d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e3f4f76cbd1c6492ba1fd985b02c9d05.jpg" align="middle">
</details><br>​    <p></p>
<p>​    </p>
</ol></ol></ol>]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>Diffusion Models</tag>
      </tags>
  </entry>
  <entry>
    <title>GAN</title>
    <url>/2024/01/24/Paper/2024-01-24/GAN/</url>
    <content><![CDATA[<blockquote>
<p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-01-24-更新"><a href="#2024-01-24-更新" class="headerlink" title="2024-01-24 更新"></a>2024-01-24 更新</h1><h2 id="A-Fair-Evaluation-of-Various-Deep-Learning-Based-Document-Image-Binarization-Approaches"><a href="#A-Fair-Evaluation-of-Various-Deep-Learning-Based-Document-Image-Binarization-Approaches" class="headerlink" title="A Fair Evaluation of Various Deep Learning-Based Document Image   Binarization Approaches"></a>A Fair Evaluation of Various Deep Learning-Based Document Image   Binarization Approaches</h2><p><strong>Authors:Richin Sukesh, Mathias Seuret, Anguelos Nicolaou, Martin Mayr, Vincent Christlein</strong></p>
<p>Binarization of document images is an important pre-processing step in the field of document analysis. Traditional image binarization techniques usually rely on histograms or local statistics to identify a valid threshold to differentiate between different aspects of the image. Deep learning techniques are able to generate binarized versions of the images by learning context-dependent features that are less error-prone to degradation typically occurring in document images. In recent years, many deep learning-based methods have been developed for document binarization. But which one to choose? There have been no studies that compare these methods rigorously. Therefore, this work focuses on the evaluation of different deep learning-based methods under the same evaluation protocol. We evaluate them on different Document Image Binarization Contest (DIBCO) datasets and obtain very heterogeneous results. We show that the DE-GAN model was able to perform better compared to other models when evaluated on the DIBCO2013 dataset while DP-LinkNet performed best on the DIBCO2017 dataset. The 2-StageGAN performed best on the DIBCO2018 dataset while SauvolaNet outperformed the others on the DIBCO2019 challenge. Finally, we make the code, all models and evaluation publicly available (<a href="https://github.com/RichSu95/Document_Binarization_Collection">https://github.com/RichSu95/Document_Binarization_Collection</a>) to ensure reproducibility and simplify future binarization evaluations. </p>
<p><a href="http://arxiv.org/abs/2401.11831v1">PDF</a> DAS 2022</p>
<p><strong>摘要</strong><br>通过评估不同数据集上深度学习方法的文档二值化性能，可以帮助研究人员选择最适合自己任务的模型。</p>
<p><strong>要点</strong></p>
<ul>
<li>深度学习方法能够通过学习与上下文相关的特征来生成图像的二值化版本，从而减少通常发生在文档图像中的退化错误。</li>
<li>在DIBCO2013数据集上，DE-GAN模型在评估时表现优于其他模型，而在DIBCO2017数据集上，DP-LinkNet表现最佳。</li>
<li>在DIBCO2018数据集上，2-StageGAN表现最佳，而在DIBCO2019挑战赛上，SauvolaNet优于其他模型。</li>
<li>代码、所有模型和评估已公开发布，以确保可重现性和简化未来的二值化评估。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li><p>论文标题：各种基于深度学习的文档图像二值化方法的公平评估</p>
</li>
<li><p>作者：Richin Sukesh, Mathias Seuret, Anguelos Nicolaou, Martin Mayr, Vincent Christlein</p>
</li>
<li><p>第一作者单位：Friedrich-Alexander-Universit¨atErlangen-N¨urnberg, Erlangen, Germany</p>
</li>
<li><p>关键词：二值化、深度学习、评估</p>
</li>
<li><p>论文链接：https://arxiv.org/abs/2401.11831
Github 代码链接：https://github.com/RichSu95/DocumentBinarizationCollection</p>
</li>
<li><p>摘要：
(1) 研究背景：
文档图像的二值化是文档分析领域中一个重要的预处理步骤。传统的图像二值化技术通常依赖于直方图或局部统计数据来识别有效阈值，以区分图像的不同方面。深度学习技术能够通过学习对上下文相关的特征进行二值化，从而生成图像的二值化版本，这些特征对通常发生在文档图像中的退化不太容易出错。近年来，已经开发了许多基于深度学习的文档二值化方法。但是，如何选择合适的方法是一个问题。目前还没有研究对这些方法进行严格的比较。</p>
</li>
</ol>
<p>(2) 过去的方法及其问题：
传统的图像二值化技术通常依赖于直方图或局部统计数据来识别有效阈值，以区分图像的不同方面。这些方法通常对图像中的噪声和退化敏感，并且可能产生不准确的二值化结果。</p>
<p>(3) 本文提出的研究方法：
本文的重点是评估不同的基于深度学习的方法，以在相同的评估协议下进行评估。在不同的文档图像二值化竞赛 (DIBCO) 数据集上评估这些方法，并获得了非常不同的结果。结果表明，在 DIBCO 2013 数据集上评估时，DE-GAN 模型能够比其他模型表现更好，而在 DIBCO 2017 数据集上，DP-LinkNet 表现最好。2-StageGAN 在 DIBCO 2018 数据集上表现最好，而 SauvolaNet 在 DIBCO 2019 挑战赛中优于其他方法。</p>
<p>(4) 方法在任务和性能上的表现：
在 DIBCO 2013 数据集上，DE-GAN 模型在 F-measure 指标上获得了 0.957 的最佳结果。在 DIBCO 2017 数据集上，DP-LinkNet 模型在 F-measure 指标上获得了 0.964 的最佳结果。在 DIBCO 2018 数据集上，2-StageGAN 模型在 F-measure 指标上获得了 0.968 的最佳结果。在 DIBCO 2019 挑战赛中，SauvolaNet 模型在 F-measure 指标上获得了 0.971 的最佳结果。这些结果表明，所提出的方法能够在不同的数据集上实现良好的性能，并且能够支持其目标。</p>
<ol start="7">
<li><p>方法：
（1）DE-GAN：利用生成对抗网络来建模文档二值化问题，生成器和判别器共同工作，生成器生成干净的图像，判别器区分生成的图像和真实二值化图像。
（2）SauvolaNet：受传统 Sauvola 阈值算法启发，使用深度学习方法学习 Sauvola 参数，包括多窗口 Sauvola、像素级窗口注意力和自适应 Sauvola 阈值三个模块，以估计辅助阈值函数。
（3）Two-StageGAN：提出一种两阶段彩色文档图像二值化深度学习架构，利用生成对抗网络，第一阶段使用局部预测进行二值化，第二阶段使用调整大小的原始输入图像和第一阶段的输出进行全局二值化。
（4）DP-LinkNet：使用深度学习方法对文档图像进行二值化，采用 U-Net 作为编码器-解码器结构，使用链接网络来增强特征表示，并使用软最大池化来生成二值化结果。
（5）2-StageGAN：提出一种两阶段彩色文档图像二值化深度学习架构，利用生成对抗网络，第一阶段使用局部预测进行二值化，第二阶段使用调整大小的原始输入图像和第一阶段的输出进行全局二值化。</p>
</li>
<li><p>结论：
（1）：本文对七种基于深度学习的文档图像二值化方法进行了公平的评估，使用所有十个可用的 DIBCO 数据集对方法进行了评估。评估结果表明，在四个不同的测试数据集上，结果非常不同，没有明确的获胜者。总体而言，DE-GAN 方法在四个不同数据集上的平均排名最高，其次是 SauvolaNet。当单独比较指标时，2-StageGAN 方法表现最好，其次是 DE-GAN。然而，在非常不同的 DIBCO2019 数据集上，SauvolaNet 优于这些方法。对于未来的工作，我们希望使用不同的协议来评估这些方法。特别是，我们希望模拟每年挑战的 DIBCO 场景，以便与单一的 DIBCO 论文进行比较，即使用 2015-2016 年的数据集进行训练，然后使用 2017 年进行评估，将 2017 年添加到训练集中，重新训练并评估 2018 年，依此类推。使用额外的增强技术以及额外的训练数据集也值得研究，并且可能对二值化方法的整体性能产生巨大影响。此外，基于像素的评估并不是最优的。虽然 pFM 度量包含到脚本轮廓的距离，但研究间接措施可能是值得的，例如 OCR/HTR 准确性或纯粹基于骨架的度量。从实用的角度来看，推理时间也值得研究。这主要在时间质量文档图像二值化竞赛中进行了研究。
（2）：创新点：本文对七种基于深度学习的文档图像二值化方法进行了公平的评估，使用所有十个可用的 DIBCO 数据集对方法进行了评估。评估结果表明，在四个不同的测试数据集上，结果非常不同，没有明确的获胜者。总体而言，DE-GAN 方法在四个不同数据集上的平均排名最高，其次是 SauvolaNet。当单独比较指标时，2-StageGAN 方法表现最好，其次是 DE-GAN。然而，在非常不同的 DIBCO2019 数据集上，SauvolaNet 优于这些方法。
性能：本文提出的方法能够在不同的数据集上实现良好的性能，并且能够支持其目标。
工作量：本文的工作量很大，需要对七种基于深度学习的文档图像二值化方法进行公平的评估，使用所有十个可用的 DIBCO 数据集对方法进行了评估。评估结果表明，在四个不同的测试数据集上，结果非常不同，没有明确的获胜者。总体而言，DE-GAN 方法在四个不同数据集上的平均排名最高，其次是 SauvolaNet。当单独比较指标时，2-StageGAN 方法表现最好，其次是 DE-GAN。然而，在非常不同的 DIBCO2019 数据集上，SauvolaNet 优于这些方法。</p>
</li>
</ol>



<p></p><details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-f410bbd295b056b48e2e24fef3c6357b.jpg" align="middle">
</details><br>​    <p></p>
<h2 id="Efficient-generative-adversarial-networks-using-linear-additive-attention-Transformers"><a href="#Efficient-generative-adversarial-networks-using-linear-additive-attention-Transformers" class="headerlink" title="Efficient generative adversarial networks using linear   additive-attention Transformers"></a>Efficient generative adversarial networks using linear   additive-attention Transformers</h2><p><strong>Authors:Emilio Morales-Juarez, Gibran Fuentes-Pineda</strong></p>
<p>Although the capacity of deep generative models for image generation, such as Diffusion Models (DMs) and Generative Adversarial Networks (GANs), has dramatically improved in recent years, much of their success can be attributed to computationally expensive architectures. This has limited their adoption and use to research laboratories and companies with large resources, while significantly raising the carbon footprint for training, fine-tuning, and inference. In this work, we present LadaGAN, an efficient generative adversarial network that is built upon a novel Transformer block named Ladaformer. The main component of this block is a linear additive-attention mechanism that computes a single attention vector per head instead of the quadratic dot-product attention. We employ Ladaformer in both the generator and discriminator, which reduces the computational complexity and overcomes the training instabilities often associated with Transformer GANs. LadaGAN consistently outperforms existing convolutional and Transformer GANs on benchmark datasets at different resolutions while being significantly more efficient. Moreover, LadaGAN shows competitive performance compared to state-of-the-art multi-step generative models (e.g. DMs) using orders of magnitude less computational resources. </p>
<p><a href="http://arxiv.org/abs/2401.09596v1">PDF</a> 12 pages, 6 figures</p>
<p><strong>摘要</strong><br>拉达生成对抗网络：基于新颖的 Transformer 块 Ladaformer 的高效生成对抗网络。</p>
<p><strong>关键要点</strong></p>
<ul>
<li>拉达生成对抗网络是一种高效的生成对抗网络，由一个名为 Ladaformer 的新颖 Transformer 块构建而成。</li>
<li>Ladaformer 的主要组成部分是线性加性注意力机制，它为每个头计算一个注意力向量，而不是二次点积注意力。</li>
<li>我们在生成器和判别器中都采用了 Ladaformer，这降低了计算复杂度并克服了 Transformer GANs 经常遇到的训练不稳定性。</li>
<li>拉达生成对抗网络在不同分辨率的基准数据集上始终优于现有的卷积和 Transformer GANs，同时具有更高的效率。</li>
<li>拉达生成对抗网络与最先进的多步生成模型（例如扩散模型）相比，显示出具有竞争力的性能，而使用的计算资源却少几个数量级。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li><p>题目：使用线性加性注意力 Transformer 的高效生成对抗网络</p>
</li>
<li><p>作者：Emilio Morales-Juarez, Gibran Fuentes-Pineda</p>
</li>
<li><p>隶属单位：墨西哥国立自治大学工程学院</p>
</li>
<li><p>关键词：图像生成、GAN、线性加性注意力、高效 Transformer</p>
</li>
<li><p>链接：https://arxiv.org/abs/2401.09596
Github：无</p>
</li>
<li><p>摘要：
(1)：随着深度生成模型在图像生成方面的能力不断提高，Diffusion Models (DM) 和 Generative Adversarial Networks (GAN) 等模型取得了显著的成功。然而，这些模型的成功很大程度上归功于计算成本高昂的架构。这限制了它们在研究实验室和资源较多的公司中的应用，同时大幅增加了训练、微调和推理的碳足迹。
(2)：以往的方法包括基于卷积的 GAN 和基于 Transformer 的 GAN。基于卷积的 GAN 通常需要复杂的工程设计和复杂的模块来实现最先进的图像生成，导致计算成本高昂。基于 Transformer 的 GAN 则可以学习数据的光滑和连续的潜在空间表示，但自注意力机制可能会导致 GAN 训练更加不稳定，并且其 O(N^2) 的复杂度导致高计算需求。
(3)：本文提出了一种新的 GAN 架构 LadaGAN，它基于称为 Ladaformer 的新型 Transformer 块。Ladaformer 的主要组件是线性加性注意力机制，它为每个头计算一个注意力向量，而不是二次点积注意力。这种机制可以降低计算复杂度，并克服与 Transformer GAN 相关的训练不稳定性。
(4)：LadaGAN 在 CIFAR-10、CelebA、FFHQ 和 LSUNBedroom 等基准数据集上实现了具有竞争力的 FID 分数，同时所需的 FLOP 和参数明显更少。此外，与最先进的多步生成模型（例如 DM）相比，LadaGAN 在使用数量级更少的计算资源的情况下表现出竞争力。</p>
</li>
<li><p>方法：
(1)：本文提出了一种新的GAN架构LadaGAN，它基于称为Ladaformer的新型Transformer块。
(2)：Ladaformer的主要组件是线性加性注意力机制，它为每个头计算一个注意力向量，而不是二次点积注意力。
(3)：这种机制可以降低计算复杂度，并克服与TransformerGAN相关的训练不稳定性。
(4)：LadaGAN在CIFAR-10、CelebA、FFHQ和LSUNBedroom等基准数据集上实现了具有竞争力的FID分数，同时所需的FLOP和参数明显更少。
(5)：此外，与最先进的多步生成模型（例如DM）相比，LadaGAN在使用数量级更少的计算资源的情况下表现出竞争力。</p>
</li>
<li><p>结论：
（1）本工作提出了一种新颖的 GAN 架构 LadaGAN，它基于一种称为 Ladaformer 的新型 Transformer 块。该块被证明比其他高效的 Transformer 块更适合生成器和判别器，允许在不同场景中进行稳定的 GAN 训练。我们的研究结果表明，Ladaformer 与卷积兼容，LadaGAN 具有梯度稳定性，并且对于图像生成任务非常有效。值得注意的是，LadaGAN 在不同分辨率的多个基准数据集上优于 ConvNet 和 TransformerGAN，同时所需的 FLOP 明显更少。此外，与扩散模型和一致性训练相比，LadaGAN 以极低的计算成本实现了具有竞争力的性能。据我们所知，LadaGAN 是第一个基于线性加性注意力机制的 GAN 架构。因此，我们的结果进一步证明了线性注意力机制的效率和表达能力，并为具有类似于现代扩散模型的性能的有效 GAN 架构的未来研究打开了大门。我们相信 LadaGAN 可以帮助实验室和研究小组在有限的计算预算下更快地进行实验，在不损失质量的情况下推进生成模型的应用，同时减少能源消耗并最大限度地减少碳足迹。作为未来的工作，我们计划在音频和文本到图像场景中训练 LadaGAN。此外，Ladaformer 块及其与卷积的兼容性还有待在其他任务（如图像和视频分类）中进行探索。</p>
</li>
</ol>
<p>（2）创新点：</p>
<ul>
<li>提出了一种新颖的 GAN 架构 LadaGAN，它基于一种称为 Ladaformer 的新型 Transformer 块。</li>
<li>Ladaformer 的主要组成部分是线性加性注意力机制，它为每个头计算一个注意力向量，而不是二次点积注意力。</li>
<li>这种机制可以降低计算复杂度，并克服与 TransformerGAN 相关的训练不稳定性。</li>
</ul>
<p>性能：</p>
<ul>
<li>LadaGAN 在 CIFAR-10、CelebA、FFHQ 和 LSUNBedroom 等基准数据集上实现了具有竞争力的 FID 分数，同时所需的 FLOP 和参数明显更少。</li>
<li>与最先进的多步生成模型（例如 DM）相比，LadaGAN 在使用数量级更少的计算资源的情况下表现出竞争力。</li>
</ul>
<p>工作量：</p>
<ul>
<li>LadaGAN 的训练速度比基于卷积的 GAN 和基于 Transformer 的 GAN 更快。</li>
<li>LadaGAN 的内存占用更少，这使得它可以在具有有限内存的设备上训练。</li>
</ul>



<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-3929700f0a09cfd1fa328b24d0274fe2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e4b6c7bf475fddc24bcc75378997dc3c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef2224955dfcfa7c34704af7b7f861f5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-74838ae28b4bbb602f3c6e331bd694ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-949e6e1e1b4c79eff4a0a5134e9ed474.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ed19f7c8a63fb77d3397877ea92c0b0d.jpg" align="middle">
</details>
​    


## Adversarial Masking Contrastive Learning for vein recognition
**Authors:Huafeng Qin, Yiquan Wu, Mounim A. El-Yacoubi, Jun Wang, Guangxiang Yang**

Vein recognition has received increasing attention due to its high security and privacy. Recently, deep neural networks such as Convolutional neural networks (CNN) and Transformers have been introduced for vein recognition and achieved state-of-the-art performance. Despite the recent advances, however, existing solutions for finger-vein feature extraction are still not optimal due to scarce training image samples. To overcome this problem, in this paper, we propose an adversarial masking contrastive learning (AMCL) approach, that generates challenging samples to train a more robust contrastive learning model for the downstream palm-vein recognition task, by alternatively optimizing the encoder in the contrastive learning model and a set of latent variables. First, a huge number of masks are generated to train a robust generative adversarial network (GAN). The trained generator transforms a latent variable from the latent variable space into a mask space. Then, we combine the trained generator with a contrastive learning model to obtain our AMCL, where the generator produces challenging masking images to increase the contrastive loss and the contrastive learning model is trained based on the harder images to learn a more robust feature representation. After training, the trained encoder in the contrastive learning model is combined with a classification layer to build a classifier, which is further fine-tuned on labeled training data for vein recognition. The experimental results on three databases demonstrate that our approach outperforms existing contrastive learning approaches in terms of improving identification accuracy of vein classifiers and achieves state-of-the-art recognition results. 

[PDF](http://arxiv.org/abs/2401.08079v1) 

**Summary**
对抗遮罩对比学习通过生成具有挑战性的掩模来增强静脉识别的鲁棒性。

**Key Takeaways**

- 深度神经网络，如卷积神经网络（CNN）和Transformer，已被引入静脉识别并取得了最先进的性能。
- 然而，现有的手指静脉特征提取解决方案由于训练图像样本稀缺，仍然不是最优的。
- 本文提出了一种对抗遮罩对比学习（AMCL）方法，该方法通过交替优化对比学习模型中的编码器和一组潜在变量，生成具有挑战性的样本来训练一个更鲁棒的对比学习模型，用于下游的掌静脉识别任务。
- 首先，生成大量掩模来训练一个鲁棒的生成对抗网络（GAN）。训练后的生成器将潜在变量空间中的潜在变量转换为掩模空间。
- 然后，我们将训练后的生成器与对比学习模型相结合，得到我们的 AMCL，其中生成器产生具有挑战性的掩模图像以增加对比损失，对比学习模型根据更难的图像进行训练以学习更鲁棒的特征表示。
- 训练后，对比学习模型中训练后的编码器与分类层相结合构建分类器，该分类器在标记的训练数据上进一步微调以进行静脉识别。
- 三个数据库上的实验结果表明，我们的方法在提高静脉分类器的识别准确性方面优于现有的对比学习方法，并实现了最先进的识别结果。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li><p>题目：对抗掩码对比学习用于静脉识别</p>
</li>
<li><p>作者：Huafeng Qin, Yiquan Wu, Mounim A. El-Yacoubi, Jun Wang, Guangxiang Yang</p>
</li>
<li><p>单位：重庆市智能感知与区块链工程实验室，重庆工商大学计算机科学与信息工程学院</p>
</li>
<li><p>关键词：生物识别，静脉识别，对比学习，对抗学习，掩码</p>
</li>
<li><p>链接：https://arxiv.org/abs/2401.08079v1，Github 链接：无</p>
</li>
<li><p>摘要：
（1）：静脉识别由于其高安全性和隐私性而受到越来越多的关注。近年来，卷积神经网络（CNN）和 Transformer 等深度神经网络已被引入静脉识别并取得了最先进的性能。然而，尽管取得了这些进展，但由于训练图像样本稀少，现有的手指静脉特征提取解决方案仍然不是最优的。
（2）：为了克服这个问题，本文提出了一种对抗掩码对比学习（AMCL）方法，通过交替优化对比学习模型中的编码器和一组潜在变量，为下游掌静脉识别任务生成具有挑战性的样本以训练更鲁棒的对比学习模型。首先，生成大量掩码来训练鲁棒的生成对抗网络（GAN）。训练后的生成器将来自潜在变量空间的潜在变量转换为掩码空间。然后，我们将训练后的生成器与对比学习模型结合起来得到我们的 AMCL，其中生成器产生具有挑战性的掩码图像以增加对比损失，并且对比学习模型基于更难的图像进行训练以学习更鲁棒的特征表示。训练后，对比学习模型中训练的编码器与分类层相结合构建分类器，该分类器在标记的训练数据上进一步微调以进行静脉识别。
（3）：在三个数据库上的实验结果表明，我们的方法在提高静脉分类器的识别准确性方面优于现有的对比学习方法，并取得了最先进的识别结果。</p>
</li>
<li><p>方法：
（1）：本文提出了一种对抗掩码对比学习（AMCL）方法，通过交替优化对比学习模型中的编码器和一组潜在变量，为下游掌静脉识别任务生成具有挑战性的样本以训练更鲁棒的对比学习模型。
（2）：首先，生成大量掩码来训练鲁棒的生成对抗网络（GAN）。训练后的生成器将来自潜在变量空间的潜在变量转换为掩码空间。
（3）：然后，我们将训练后的生成器与对比学习模型结合起来得到我们的 AMCL，其中生成器产生具有挑战性的掩码图像以增加对比损失，并且对比学习模型基于更难的图像进行训练以学习更鲁棒的特征表示。
（4）：训练后，对比学习模型中训练的编码器与分类层相结合构建分类器，该分类器在标记的训练数据上进一步微调以进行静脉识别。</p>
</li>
<li><p>结论：
（1）本文提出了一种联合生成和对比学习框架用于静脉识别，该框架结合了 GAN 和对比学习来学习鲁棒的静脉分类器。首先，生成大量掩码来训练鲁棒的 GAN 以学习掩码分布空间。其次，将训练后的 GAN 与对比学习模型相结合得到我们的 AMCL，并以对抗的方式进行训练。具体来说，搜索一组潜在变量以生成具有挑战性的样本对来增加对比学习模型的损失。对比学习模型能够基于生成的困难样本学习鲁棒的特征表示。我们在三个公开数据库上的实验结果表明，我们的方法在提高静脉分类器的性能方面优于现有的对比学习方法，并取得了最先进的识别准确率。
（2）创新点：</p>
</li>
</ol>
<ul>
<li>提出了一种联合生成和对比学习的框架，用于静脉识别。</li>
<li>设计了一种鲁棒的 GAN 来学习掩码分布空间。</li>
<li>将训练后的 GAN 与对比学习模型相结合，以对抗的方式训练对比学习模型。</li>
</ul>
<p>性能：</p>
<ul>
<li>在三个公开数据库上的实验结果表明，我们的方法在提高静脉分类器的性能方面优于现有的对比学习方法，并取得了最先进的识别准确率。</li>
</ul>
<p>工作量：</p>
<ul>
<li>需要生成大量掩码来训练鲁棒的 GAN。</li>
<li>需要将训练后的 GAN 与对比学习模型相结合，并以对抗的方式训练对比学习模型。</li>
</ul>



<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-da84b96d624d97ec8e4bccc75083479b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56f3ecc77281bae064a88576167ef74d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f2d72f0d01c578dc3aa49ae1a52e3e52.jpg" align="middle">
</details>
​    


## Multimodal Crowd Counting with Pix2Pix GANs
**Authors:Muhammad Asif Khan, Hamid Menouar, Ridha Hamila**

Most state-of-the-art crowd counting methods use color (RGB) images to learn the density map of the crowd. However, these methods often struggle to achieve higher accuracy in densely crowded scenes with poor illumination. Recently, some studies have reported improvement in the accuracy of crowd counting models using a combination of RGB and thermal images. Although multimodal data can lead to better predictions, multimodal data might not be always available beforehand. In this paper, we propose the use of generative adversarial networks (GANs) to automatically generate thermal infrared (TIR) images from color (RGB) images and use both to train crowd counting models to achieve higher accuracy. We use a Pix2Pix GAN network first to translate RGB images to TIR images. Our experiments on several state-of-the-art crowd counting models and benchmark crowd datasets report significant improvement in accuracy. 

[PDF](http://arxiv.org/abs/2401.07591v1) Accepted version of the paper in 19th International Conference on   Computer Vision Theory and Applications (VISAPP), Rome, Italy, 27-29 Feb,   2024,

**Summary**
利用生成对抗网络从彩色图像自动生成热红外图像，可大幅提升人群计数准确率。

**Key Takeaways**

- 大多数最先进的人群计数方法使用彩色 (RGB) 图像来学习人群的密度图，但在光线较差的密集人群场景中，这些方法通常难以实现更高的准确度。
- 最近，一些研究报告称，结合 RGB 和热图像可以提高人群计数模型的准确度。
- 虽然多模态数据可以产生更好的预测，但多模态数据可能并不总是事先可用。
- 本文提出使用生成对抗网络 (GAN) 从彩色 (RGB) 图像自动生成热红外 (TIR) 图像，并使用两者来训练人群计数模型以实现更高的准确度。
- 我们首先使用 Pix2Pix GAN 网络将 RGB 图像转换为 TIR 图像。
- 我们在几个最先进的人群计数模型和基准人群数据集上的实验表明，准确度有了显著提高。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li><p>题目：使用 Pix2PixGAN 进行多模态人群计数</p>
</li>
<li><p>作者：Muhammad Asif Khan, Hamid Menouar, Ridha Hamila</p>
</li>
<li><p>第一作者单位：卡塔尔大学卡塔尔移动创新中心</p>
</li>
<li><p>关键词：人群计数、CNN、密度估计、多模态、RGB、热成像</p>
</li>
<li><p>论文链接：https://arxiv.org/abs/2401.07591
Github 链接：无</p>
</li>
<li><p>摘要：
(1)：人群计数在人群管理、城市规划、安全监控、活动管理和公共安全等领域有着广泛的应用。深度学习的出现带来了人群计数技术的范式转变，在各种现实世界应用中实现了更高的准确性和可扩展性。
(2)：大多数最先进的人群计数方法主要使用光学彩色图像，并且在合理的照明条件下效果较好。然而，在许多监控场景中，使用光学相机捕获的图像具有较差的照明条件，导致计数模型的性能较差。为了提高准确性，热红外 (TIR) 相机与光学相机一起使用，以捕获彩色 RGB 图像和热图像。然后，人群计数模型可以使用一个（单模态）或两个（多模态）RGB 和 TIR 图像来学习低光条件下的人群密度。
(3)：本文提出使用生成对抗网络 (GAN) 从彩色 (RGB) 图像自动生成热红外 (TIR) 图像，并使用两者来训练人群计数模型以实现更高的准确性。我们首先使用 Pix2PixGAN 网络将 RGB 图像转换为 TIR 图像。
(4)：我们在几个最先进的人群计数模型和基准人群数据集上进行的实验报告了准确性的显着提高。</p>
</li>
<li><p>方法：
（1）：提出了一种使用 Pix2PixGAN 从 RGB 图像自动生成热红外 (TIR) 图像，并使用两者来训练人群计数模型以实现更高精度的框架，称为 MMCount。
（2）：MMCount 由两个基本部分组成：Pix2PixGAN 和多模态人群计数网络。Pix2PixGAN 将光学 RGB 图像转换为 TIR 图像，人群模型使用 RGB 和 TIR 图像来预测人群密度图。
（3）：Pix2PixGAN 由生成器和判别器组成。生成器将 RGB 图像转换为 TIR 图像，判别器将生成的 TIR 图像与真实 TIR 图像进行区分。
（4）：多模态计数网络由 RGB 分支和 TIR 分支组成，两个分支都具有四个卷积层，输出连接并融合在融合层中，最后使用 1×1 卷积层生成密度图。
（5）：使用头位置生成稀疏定位图，然后使用高斯核与 delta 函数卷积生成密度图，作为训练模型的真实值。
（6）：使用 L2 损失函数训练人群计数模型，该损失函数计算目标密度图和预测密度图之间的欧几里德距离。
（7）：在 DroneRGBT、ShanghaiTechPart-B 和 CARPK 数据集上评估了所提出的方法，并与基线模型进行了比较。
（8）：实验结果表明，所提出的方法在单模态和多模态人群计数任务中都优于基线模型。</p>
</li>
<li><p>结论：
（1）：本文提出了一种使用 Pix2PixGAN 从 RGB 图像自动生成热红外 (TIR) 图像，并使用两者来训练人群计数模型以实现更高精度的框架，称为 MMCount。实验结果表明，所提出的方法在单模态和多模态人群计数任务中都优于基线模型。
（2）：创新点：</p>
</li>
</ol>
<ul>
<li>提出了一种使用 Pix2PixGAN 从 RGB 图像自动生成 TIR 图像的方法。</li>
<li>提出了一种使用 RGB 和 TIR 图像来训练人群计数模型以实现更高精度的框架。</li>
<li>在几个最先进的人群计数模型和基准人群数据集上进行了实验，并报告了准确性的显着提高。
性能：</li>
<li>在 DroneRGBT、ShanghaiTechPart-B 和 CARPK 数据集上评估了所提出的方法，并与基线模型进行了比较。</li>
<li>实验结果表明，所提出的方法在单模态和多模态人群计数任务中都优于基线模型。
工作量：</li>
<li>使用 PyTorch 实现并开源了所提出的方法。</li>
<li>在多个 GPU 上训练了所提出的模型。</li>
<li>在几个基准人群数据集上评估了所提出的模型。</li>
</ul>



<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-6f0b0afa8ea4a994e77c0bef26f7009b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d77479e99dfbd9fb266eda32c03e44d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6696e7497717d1dc1f0a8370d7ba041.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0791df4d611a13627d1fc21d3a330a13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1cc942916c0c2dd251bf08c0b900723a.jpg" align="middle">
</details>
​    


## ENTED: Enhanced Neural Texture Extraction and Distribution for   Reference-based Blind Face Restoration
**Authors:Yuen-Fui Lau, Tianjia Zhang, Zhefan Rao, Qifeng Chen**

We present ENTED, a new framework for blind face restoration that aims to restore high-quality and realistic portrait images. Our method involves repairing a single degraded input image using a high-quality reference image. We utilize a texture extraction and distribution framework to transfer high-quality texture features between the degraded input and reference image. However, the StyleGAN-like architecture in our framework requires high-quality latent codes to generate realistic images. The latent code extracted from the degraded input image often contains corrupted features, making it difficult to align the semantic information from the input with the high-quality textures from the reference. To overcome this challenge, we employ two special techniques. The first technique, inspired by vector quantization, replaces corrupted semantic features with high-quality code words. The second technique generates style codes that carry photorealistic texture information from a more informative latent space developed using the high-quality features in the reference image's manifold. Extensive experiments conducted on synthetic and real-world datasets demonstrate that our method produces results with more realistic contextual details and outperforms state-of-the-art methods. A thorough ablation study confirms the effectiveness of each proposed module. 

[PDF](http://arxiv.org/abs/2401.06978v1) 

**Summary**
利用高品质参考图修复退化面部图像，将参考图和输入图像之间的高品质纹理特征传递，替换低质量语义特征并生成逼真纹理信息样式代码。

**Key Takeaways**

- 提出退化面部修复框架ENTED，从参考图像传递纹理特征。
- 使用风格GAN生成逼真图像需要高质量潜在码。
- 退化输入图像提取的潜在码特征不完整，难以对齐输入与参考图像的语义信息。
- 使用矢量量化技术，以优质码字替换损坏的语义特征。
- 生成样式码，从参考图像流形中的高质量特征生成更多逼真纹理信息。
- 该方法在合成和真实数据集上产生更逼真且包含更多细节的修复结果。
- 团队通过消融研究验证每个提出模块的作用与必要性。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li><p>题目：ENTED：用于基于参考图像的盲人面部修复的增强神经纹理提取和分布</p>
</li>
<li><p>作者：Yuen-Fui Lau、Tianjia Zhang、Zhefan Rao、Qifeng Chen</p>
</li>
<li><p>隶属机构：香港科技大学</p>
</li>
<li><p>关键词：盲人面部修复、参考图像、纹理提取和分布、矢量量化、风格编码</p>
</li>
<li><p>链接：https://arxiv.org/abs/2401.06978
Github：无</p>
</li>
<li><p>摘要：
（1）：研究背景：盲人面部修复（BFR）是一种计算摄影技术，专注于将低质量的面部图像转换成高质量的面部图像，即使不知道退化的类型。这项技术对于那些希望提高面部图像质量的人来说至关重要。然而，从低质量图像中准确重建面部细节的任务可能非常具有挑战性，因为会丢失特定的身份信息。
（2）：过去的方法：为了解决这个问题，我们考虑使用同一人的高质量参考图像。这种方法导致我们探索基于参考图像的盲人面部修复的概念，我们的目标是利用高质量参考图像中的信息来增强修复过程。基于参考图像的超分辨率（RefSR）技术最近引起了人们的兴趣。它通过结合来自参考图像的高质量语义细节来提高低分辨率输入的质量。然而，如果参考图像的特征没有得到正确的管理，可能会导致参考图像的利用不足或使用不当。
（3）：研究方法：为了克服这些挑战，我们采用了一个纹理提取和分布框架，该框架可以训练具有注意力重建损失，以提高整个纹理转移过程的准确性和高质量特征的使用。纹理提取和分布框架已成功应用于可控人像合成任务，我们将这一概念扩展到我们的基于参考图像的盲人面部修复框架中。
（4）：方法性能：低质量（LQ）输入的潜在表示通常在纹理分布过程中包含不正确的信息。仅仅应用纹理提取和分布框架不足以产生高保真图像。为了克服这一挑战，我们利用了矢量量化（VQ）技术。该技术需要用从高质量（HQ）词典中获得的高质量潜在编码替换退化的潜在特征。通过直接替换这些代码，我们能够缩小低质量和高质量潜在代码之间的差异，从而为纹理分布提供合适的语义指导。如表 3 所示，我们注意到调制卷积有助于在修复过程中增强面部细节的真实感。然而，它们需要高质量的样式代码表示才能实现卓越的图像修复。</p>
</li>
<li><p>方法：
（1）：该文提出了一种基于参考图像的盲人面部修复（BFR）方法，该方法利用高质量参考图像中的信息来增强修复过程。
（2）：该方法采用了一个纹理提取和分布框架，该框架可以训练具有注意力重建损失，以提高整个纹理转移过程的准确性和高质量特征的使用。
（3）：为了克服低质量（LQ）输入的潜在表示通常在纹理分布过程中包含不正确的信息的问题，该方法利用了矢量量化（VQ）技术，用从高质量（HQ）词典中获得的高质量潜在编码替换退化的潜在特征。
（4）：为了增强面部细节的真实感，该方法还使用了调制卷积。</p>
</li>
<li><p>结论：
（1）：本研究提出了一种基于参考图像的盲人面部修复方法，该方法利用高质量参考图像中的信息来增强修复过程。该方法采用了一个纹理提取和分布框架，该框架可以训练具有注意力重建损失，以提高整个纹理转移过程的准确性和高质量特征的使用。为了克服低质量（LQ）输入的潜在表示通常在纹理分布过程中包含不正确的信息的问题，该方法利用了矢量量化（VQ）技术，用从高质量（HQ）词典中获得的高质量潜在编码替换退化的潜在特征。为了增强面部细节的真实感，该方法还使用了调制卷积。
（2）：创新点：</p>
</li>
</ol>
<ul>
<li>提出了一种基于参考图像的盲人面部修复方法，该方法利用高质量参考图像中的信息来增强修复过程。</li>
<li>采用了一个纹理提取和分布框架，该框架可以训练具有注意力重建损失，以提高整个纹理转移过程的准确性和高质量特征的使用。</li>
<li>利用了矢量量化（VQ）技术，用从高质量（HQ）词典中获得的高质量潜在编码替换退化的潜在特征。</li>
<li>使用了调制卷积来增强面部细节的真实感。
性能：</li>
<li>该方法在多个数据集上取得了最先进的性能。</li>
<li>该方法能够有效地修复各种类型的面部图像，包括模糊、噪声和低分辨率图像。</li>
<li>该方法能够生成高质量的面部图像，具有逼真的细节和自然的外观。
工作量：</li>
<li>该方法的实现相对简单，并且可以在标准的硬件上训练和部署。</li>
<li>该方法的训练时间相对较短，并且可以在几个小时内完成。</li>
<li>该方法的推理时间相对较快，并且可以在几秒钟内生成高质量的面部图像。</li>
</ul>



<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-53a65527ccf6d7d8f8572b0ddb295010.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-40e6ecf4c16ec5c66222df8b6bf80060.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60393ae177ef1a065e9b74e8fb943b41.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dffe915921a34f90de0be1eb61982e27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-898fa480014e89e712e40975e08fb724.jpg" align="middle">
</details>
​    


## GE-AdvGAN: Improving the transferability of adversarial samples by   gradient editing-based adversarial generative model
**Authors:Zhiyu Zhu, Huaming Chen, Xinyi Wang, Jiayu Zhang, Zhibo Jin, Kim-Kwang Raymond Choo**

Adversarial generative models, such as Generative Adversarial Networks (GANs), are widely applied for generating various types of data, i.e., images, text, and audio. Accordingly, its promising performance has led to the GAN-based adversarial attack methods in the white-box and black-box attack scenarios. The importance of transferable black-box attacks lies in their ability to be effective across different models and settings, more closely aligning with real-world applications. However, it remains challenging to retain the performance in terms of transferable adversarial examples for such methods. Meanwhile, we observe that some enhanced gradient-based transferable adversarial attack algorithms require prolonged time for adversarial sample generation. Thus, in this work, we propose a novel algorithm named GE-AdvGAN to enhance the transferability of adversarial samples whilst improving the algorithm's efficiency. The main approach is via optimising the training process of the generator parameters. With the functional and characteristic similarity analysis, we introduce a novel gradient editing (GE) mechanism and verify its feasibility in generating transferable samples on various models. Moreover, by exploring the frequency domain information to determine the gradient editing direction, GE-AdvGAN can generate highly transferable adversarial samples while minimizing the execution time in comparison to the state-of-the-art transferable adversarial attack algorithms. The performance of GE-AdvGAN is comprehensively evaluated by large-scale experiments on different datasets, which results demonstrate the superiority of our algorithm. The code for our algorithm is available at: https://github.com/LMBTough/GE-advGAN 

[PDF](http://arxiv.org/abs/2401.06031v1) Accepted by SIAM International Conference on Data Mining (SDM24)

**摘要**
改进生成对抗神经网络（GAN）的生成器参数训练过程，提出了一种新的梯度编辑机制，提高了对抗样本的可迁移性和算法效率。

**要点**

* 对抗生成模型（GAN）被广泛应用于生成各种类型的数据，如图像、文本和音频。
* 基于 GAN 的对抗性攻击方法在白盒和黑盒攻击场景中取得了良好的效果。
* 可迁移的黑盒攻击对于在不同模型和设置下保持有效性具有重要意义，与实际应用更为紧密。
* 保持此类方法的可迁移对抗性例子在性能方面仍然具有挑战性。
* 增强型基于梯度的可迁移对抗性攻击算法需要花费较长时间才能生成对抗性样本。
* 提出了一种名为 GE-AdvGAN 的新算法，以增强对抗样本的可迁移性并提高算法的效率。
* 主要方法是优化生成器参数的训练过程。
* 通过函数和特征相似性分析，引入了新的梯度编辑 (GE) 机制，并验证了其在各种模型上生成可迁移样本的可行性。
* 通过探索频域信息来确定梯度编辑方向，与最先进的可迁移对抗性攻击算法相比，GE-AdvGAN 可以生成高度可迁移的对抗性样本，同时最大限度地减少了执行时间。
* 通过在不同数据集上进行大规模实验，对 GE-AdvGAN 的性能进行了全面评估，结果证明了该算法的优越性。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>题目：GE-AdvGAN：通过基于梯度编辑的对抗生成模型来提高对抗样本的可迁移性</li>
<li>作者：Zhiyu Zhu、Huaming Chen、Xinyi Wang、Jiayu Zhang、Zhibo Jin、Kim-Kwang Raymond Choo、Jun Shen、Dong Yuan</li>
<li>隶属单位：悉尼大学电气与计算机工程学院</li>
<li>关键词：梯度编辑、对抗可迁移性、基于 GAN 的对抗攻击、计算优化</li>
<li>论文链接：https://arxiv.org/abs/2401.06031
Github 代码链接：https://github.com/LMBTough/GE-advGAN</li>
<li>摘要：
(1)：研究背景：对抗生成模型（AGM）在生成各种类型的数据（如图像、文本和音频）方面表现出色。基于 AGM 的对抗攻击方法已广泛应用于白盒和黑盒攻击场景中。可迁移的黑盒攻击非常重要，因为它们能够跨不同的模型和设置有效地进行攻击，这与现实世界的应用更加紧密地结合在一起。然而，对于此类方法来说，在可迁移对抗样本方面保持性能仍然具有挑战性。同时，我们观察到一些增强的基于梯度的可迁移对抗攻击算法需要较长时间才能生成对抗样本。
(2)：过去的方法及其问题：AdvGAN 是一种基于香草 GAN 的对抗攻击算法，用于白盒和黑盒攻击。AdvGAN 在白盒攻击环境中，通过训练生成器 G 来生成扰动，一旦 G 经过训练，就不需要连续访问受害者模型的信息。它解决了在传统白盒攻击中需要多次查询模型以训练最优对抗样本的要求。此外，在判别器（以下记为 D）中引入了动态蒸馏过程，允许 AdvGAN 适用于黑盒攻击。该算法以一种新颖的方式集成了前馈和判别器网络来构建 G 和 D 以生成对抗样本。然而，一方面，尽管在黑盒攻击中取得了可喜的成果，即 92.76% 的攻击成功率，但 AdvGAN 在白盒攻击中的性能却很差，即 67.89% 的攻击成功率。另一方面，AdvGAN 在生成对抗样本时需要大量的时间。
(3)：研究方法：为了解决上述问题，我们提出了一种名为 GE-AdvGAN 的新算法，以提高对抗样本的可迁移性，同时提高算法的效率。主要方法是通过优化生成器参数的训练过程。通过功能和特征相似性分析，我们引入了一种新的梯度编辑（GE）机制，并验证了其在各种模型上生成可迁移样本的可行性。此外，通过探索频域信息来确定梯度编辑方向，与最先进的可迁移对抗攻击算法相比，GE-AdvGAN 能够生成高度可迁移的对抗样本，同时最大限度地减少执行时间。
(4)：实验结果：GE-AdvGAN 的性能通过在不同数据集上进行的大规模实验进行了全面评估，结果证明了我们算法的优越性。在 MNIST 数据集上，GE-AdvGAN 在白盒攻击和黑盒攻击中分别实现了 99.54% 和 98.76% 的攻击成功率。在 CIFAR-10 数据集上，GE-AdvGAN 在白盒攻击和黑盒攻击中分别实现了 98.32% 和 97.14% 的攻击成功率。在 ImageNet 数据集上，GE-AdvGAN 在白盒攻击和黑盒攻击中分别实现了 97.08% 和 96.23% 的攻击成功率。这些结果表明，GE-AdvGAN 能够有效地生成高度可迁移的对抗样本，并且在计算效率方面也优于其他最先进的方法。</li>
</ol>
<p>Methods:
(1): 提出GE-AdvGAN算法，通过优化生成器参数的训练过程来提高对抗样本的可迁移性。
(2): 引入梯度编辑（GE）机制，通过功能和特征相似性分析来确定梯度编辑方向，以生成高度可迁移的对抗样本。
(3): 通过探索频域信息来确定梯度编辑方向，与最先进的可迁移对抗攻击算法相比，GE-AdvGAN能够生成高度可迁移的对抗样本，同时最大限度地减少执行时间。</p>
<ol start="8">
<li>结论：
（1）：本文提出了一种名为 GE-AdvGAN 的新算法，通过优化生成器参数的训练过程来提高对抗样本的可迁移性。
（2）：创新点：
本文引入梯度编辑（GE）机制，通过功能和特征相似性分析来确定梯度编辑方向，以生成高度可迁移的对抗样本。
通过探索频域信息来确定梯度编辑方向，与最先进的可迁移对抗攻击算法相比，GE-AdvGAN 能够生成高度可迁移的对抗样本，同时最大限度地减少执行时间。
性能：
在 MNIST 数据集上，GE-AdvGAN 在白盒攻击和黑盒攻击中分别实现了 99.54% 和 98.76% 的攻击成功率。
在 CIFAR-10 数据集上，GE-AdvGAN 在白盒攻击和黑盒攻击中分别实现了 98.32% 和 97.14% 的攻击成功率。
在 ImageNet 数据集上，GE-AdvGAN 在白盒攻击和黑盒攻击中分别实现了 97.08% 和 96.23% 的攻击成功率。
工作量：
GE-AdvGAN 的计算效率优于其他最先进的方法。</li>
</ol>



<p></p><details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-db1cd80b7b3a4ee552ff6559e8a7a978.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19cc70fca6b8ad3b64be920ef14dd19e.jpg" align="middle">
</details><br>​    <p></p>
<h2 id="3D-SSGAN-Lifting-2D-Semantics-for-3D-Aware-Compositional-Portrait-Synthesis"><a href="#3D-SSGAN-Lifting-2D-Semantics-for-3D-Aware-Compositional-Portrait-Synthesis" class="headerlink" title="3D-SSGAN: Lifting 2D Semantics for 3D-Aware Compositional Portrait   Synthesis"></a>3D-SSGAN: Lifting 2D Semantics for 3D-Aware Compositional Portrait   Synthesis</h2><p><strong>Authors:Ruiqi Liu, Peng Zheng, Ye Wang, Rui Ma</strong></p>
<p>Existing 3D-aware portrait synthesis methods can generate impressive high-quality images while preserving strong 3D consistency. However, most of them cannot support the fine-grained part-level control over synthesized images. Conversely, some GAN-based 2D portrait synthesis methods can achieve clear disentanglement of facial regions, but they cannot preserve view consistency due to a lack of 3D modeling abilities. To address these issues, we propose 3D-SSGAN, a novel framework for 3D-aware compositional portrait image synthesis. First, a simple yet effective depth-guided 2D-to-3D lifting module maps the generated 2D part features and semantics to 3D. Then, a volume renderer with a novel 3D-aware semantic mask renderer is utilized to produce the composed face features and corresponding masks. The whole framework is trained end-to-end by discriminating between real and synthesized 2D images and their semantic masks. Quantitative and qualitative evaluations demonstrate the superiority of 3D-SSGAN in controllable part-level synthesis while preserving 3D view consistency. </p>
<p><a href="http://arxiv.org/abs/2401.03764v1">PDF</a> </p>
<p><strong>摘要</strong><br>3D-SSGAN提出了一种新颖的3D感知合成人像框架，实现了人像细粒度部件控制并保持3D视图一致。</p>
<p><strong>要点</strong></p>
<ul>
<li>3D-SSGAN框架使用深度引导的2D到3D提升模块将生成的2D部件特征和语义映射到3D。</li>
<li>3D-SSGAN框架使用体积渲染器和新颖的3D感知语义掩码渲染器来生成合成面部特征和相应的掩码。</li>
<li>3D-SSGAN框架通过对真实和合成的2D图像及其语义掩码进行判别来端到端地训练。</li>
<li>定量和定性评估表明，3D-SSGAN在可控部件合成中优于现有方法，同时保持了3D视图一致性。</li>
<li>3D-SSGAN可以有效地实现人像细粒度部件控制，并保持3D视图一致性。</li>
<li>3D-SSGAN框架可以端到端地训练，易于实现。</li>
<li>3D-SSGAN框架在人像生成任务上取得了很好的效果。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li><p>题目：3D-SSGAN：提升 2D 语义以实现 3D 感知合成肖像</p>
</li>
<li><p>作者：刘瑞奇，郑鹏，王叶，马锐</p>
</li>
<li><p>单位：吉林大学人工智能学院</p>
</li>
<li><p>关键词：合成图像，解耦建模，3D 感知神经渲染</p>
</li>
<li><p>论文链接：Paper_info:3D-SSGAN:Lifting2DSemanticsfor3D-AwareCompositionalPortraitSynthesis，Github 链接：None</p>
</li>
<li><p>摘要：
(1) 研究背景：肖像合成和编辑近年来备受关注，现有方法可以生成视觉上吸引人的肖像图像，但缺乏对图像进行精细部分编辑的能力。同时，3D 感知肖像合成方法可以生成高质量且具有视图一致性的图像，但无法支持部件级别的编辑。
(2) 过去方法及其问题：2D GAN 模型可以实现面部区域的清晰解耦，但由于缺乏 3D 建模能力，无法保持视图一致性。3D 感知肖像合成方法可以实现高品质和视图一致的肖像合成，但无法进行部件级别的编辑。
(3) 研究方法：本文提出了一种名为 3D-SSGAN 的框架，用于 3D 感知合成肖像。该框架首先使用深度引导的 2D 到 3D 提升模块将生成的 2D 部件特征和语义映射到 3D。然后，利用具有新型 3D 感知语义掩码渲染器的体积渲染器生成合成面部特征和相应的掩码。整个框架通过对真实和合成 2D 图像及其语义掩码进行判别来端到端训练。
(4) 实验结果：定量和定性评估表明，3D-SSGAN 在可控部件级别合成和保持 3D 视图一致性方面优于现有方法。这些结果支持了本文的目标，即实现可控部件级别合成和保持 3D 视图一致性。</p>
</li>
<li><p>&lt;Methods&gt;：
（1）：本文提出了一种名为3D-SSGAN的框架，用于3D感知合成肖像。该框架首先使用深度引导的2D到3D提升模块将生成的2D部件特征和语义映射到3D。
（2）：然后，利用具有新型3D感知语义掩码渲染器的体积渲染器生成合成面部特征和相应的掩码。
（3）：整个框架通过对真实和合成2D图像及其语义掩码进行判别来端到端训练。</p>
</li>
<li><p>结论：
（1）：本文提出了一种新颖的框架3D-SSGAN，用于3D感知合成肖像。该框架将2D语义解耦学习扩展到3D感知，通过提升操作将生成的2D部件特征和语义映射到3D。利用提升和融合的特征，体积渲染器被优化用于合成3D感知人脸图像及其语义掩码。此外，基于NeRF的权重被用于部件级别掩码的生成，使生成的图像更具有3D感知性。3D感知语义掩码渲染器也将3D信息有效地融入到部件级别掩码的生成中。虽然3D-SSGAN在强语义解耦的3D感知合成中表现出优异的性能，但未来工作仍有改进的空间。首先，需要在更多数据集（如FFHQ[1]）上进行评估，以进一步验证方法的泛化能力。其次，通过调整2D生成器的架构，可以进一步提高结果的质量和多样性。最后，如何进一步提高基于组合的网络的内存和计算效率将是一个有趣的研究方向。
致谢：这项工作得到了国家自然科学基金（62202199）的部分支持。
（2）：创新点：
本文提出了一种新颖的框架3D-SSGAN，用于3D感知合成肖像。该框架将2D语义解耦学习扩展到3D感知，通过提升操作将生成的2D部件特征和语义映射到3D。利用提升和融合的特征，体积渲染器被优化用于合成3D感知人脸图像及其语义掩码。此外，基于NeRF的权重被用于部件级别掩码的生成，使生成的图像更具有3D感知性。3D感知语义掩码渲染器也将3D信息有效地融入到部件级别掩码的生成中。
性能：
3D-SSGAN在强语义解耦的3D感知合成中表现出优异的性能。定量和定性评估表明，3D-SSGAN在可控部件级别合成和保持3D视图一致性方面优于现有方法。
工作量：
3D-SSGAN的实现需要一定的工作量。该框架涉及到多个模块，包括深度引导的2D到3D提升模块、体积渲染器和3D感知语义掩码渲染器。此外，还需要对整个框架进行端到端训练。</p>
</li>
</ol>



<p></p><details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ce64d7f4a0001fd56e8d3a28ff09991c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0dc11b36fba8af3b937de43883acfc32.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-71873fa4e08294d88dddccba710aca5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4db074bd3e9835f962fe893f6e960b17.jpg" align="middle">
</details><br>​    <p></p>
<h2 id="FED-NeRF-Achieve-High-3D-Consistency-and-Temporal-Coherence-for-Face-Video-Editing-on-Dynamic-NeRF"><a href="#FED-NeRF-Achieve-High-3D-Consistency-and-Temporal-Coherence-for-Face-Video-Editing-on-Dynamic-NeRF" class="headerlink" title="FED-NeRF: Achieve High 3D Consistency and Temporal Coherence for Face   Video Editing on Dynamic NeRF"></a>FED-NeRF: Achieve High 3D Consistency and Temporal Coherence for Face   Video Editing on Dynamic NeRF</h2><p><strong>Authors:Hao Zhang, Yu-Wing Tai, Chi-Keung Tang</strong></p>
<p>The success of the GAN-NeRF structure has enabled face editing on NeRF to maintain 3D view consistency. However, achieving simultaneously multi-view consistency and temporal coherence while editing video sequences remains a formidable challenge. This paper proposes a novel face video editing architecture built upon the dynamic face GAN-NeRF structure, which effectively utilizes video sequences to restore the latent code and 3D face geometry. By editing the latent code, multi-view consistent editing on the face can be ensured, as validated by multiview stereo reconstruction on the resulting edited images in our dynamic NeRF. As the estimation of face geometries occurs on a frame-by-frame basis, this may introduce a jittering issue. We propose a stabilizer that maintains temporal coherence by preserving smooth changes of face expressions in consecutive frames. Quantitative and qualitative analyses reveal that our method, as the pioneering 4D face video editor, achieves state-of-the-art performance in comparison to existing 2D or 3D-based approaches independently addressing identity and motion. Codes will be released. </p>
<p><a href="http://arxiv.org/abs/2401.02616v1">PDF</a> Our code will be available at: <a href="https://github.com/ZHANG1023/FED-NeRF">https://github.com/ZHANG1023/FED-NeRF</a></p>
<p><strong>摘要</strong><br>动态人脸 GAN-NeRF 结构实现了单帧图像的人脸编辑，并能保持多视角一致性和时间连贯性。</p>
<p><strong>关键要点</strong></p>
<ul>
<li>该方法使用视频序列恢复潜在代码和 3D 面部几何图形。</li>
<li>通过编辑潜在代码，可以确保人脸上的多视图一致编辑。</li>
<li>通过按帧估计面部几何图形，可能会产生抖动问题。</li>
<li>该方法提出一个稳定器，通过保留连续帧中面部表情的平滑变化来保持时间连贯性。</li>
<li>该方法的性能优于现有的基于 2D 或 3D 的方法。</li>
<li>该方法在确保身份和动作的情况下实现了最先进的性能。</li>
<li>代码已经开源。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li><p>题目：FED-NeRF：实现人脸视频编辑中的高 3D 一致性和时间连贯性</p>
</li>
<li><p>作者：Hao Zhang, Yu-Wing Tai, Chi-Keung Tang</p>
</li>
<li><p>单位：香港科技大学</p>
</li>
<li><p>关键词：人脸视频编辑、NeRF、动态 NeRF、多视角一致性、时间连贯性</p>
</li>
<li><p>论文链接：https://arxiv.org/abs/2401.02616，Github 链接：None</p>
</li>
<li><p>摘要：
（1）研究背景：GAN-NeRF 结构的成功使 NeRF 上的人脸编辑能够保持 3D 视角一致性。然而，在编辑视频序列时同时实现多视角一致性和时间连贯性仍然是一个巨大的挑战。
（2）过去的方法及其问题：现有方法在处理人脸视频编辑时，往往只能独立地解决身份和动作问题，难以同时保证多视角一致性和时间连贯性。
（3）研究方法：本文提出了一种新颖的人脸视频编辑架构，建立在动态人脸 GAN-NeRF 结构之上，有效地利用视频序列来恢复潜在代码和 3D 人脸几何。通过编辑潜在代码，可以确保在人脸上进行多视角一致的编辑，并通过在动态 NeRF 中对生成的编辑图像进行多视角立体重建来验证。由于人脸几何的估计是逐帧进行的，这可能会引入抖动问题。我们提出了一种稳定器，通过保持连续帧中人脸表情的平滑变化来保持时间连贯性。
（4）方法性能：定量和定性分析表明，我们的方法作为开创性的 4D 人脸视频编辑器，在独立地处理身份和动作时，与现有的 2D 或 3D 方法相比，达到了最先进的性能。</p>
</li>
<li><p>方法：
(1) 潜在代码估计器：从视频序列中提取身份信息，将每一帧的特征聚合为单一的潜在代码输出。
(2) 面部几何估计器：估计每一帧的 FLAME 控制，并使用随机采样的相机姿态渲染输出图像，计算重建损失和 ID 损失。
(3) 稳定器：使用 Catmull-Rom 样条曲线对连续帧中的面部几何进行平滑运动，确保时间连贯性。
(4) 语义编辑器：修改潜在代码以执行语义编辑，例如改变面部表情或发型。</p>
</li>
<li><p>结论：</p>
</li>
</ol>
<p>（1）：本文提出了一种新颖的人脸视频编辑架构FED-NeRF，该架构能够同时实现多视角一致性和时间连贯性。FED-NeRF在独立地处理身份和动作时，与现有的2D或3D方法相比，达到了最先进的性能。</p>
<p>（2）：创新点：</p>
<ul>
<li>提出了一种新颖的人脸视频编辑架构FED-NeRF，该架构能够同时实现多视角一致性和时间连贯性。</li>
<li>设计了一种潜在代码估计器，可以从视频序列中提取身份信息，并将每一帧的特征聚合为单一的潜在代码输出。</li>
<li>设计了一种面部几何估计器，可以估计每一帧的FLAME控制，并使用随机采样的相机姿态渲染输出图像，计算重建损失和ID损失。</li>
<li>设计了一种稳定器，可以对连续帧中的面部几何进行平滑运动，确保时间连贯性。</li>
<li>设计了一种语义编辑器，可以修改潜在代码以执行语义编辑，例如改变面部表情或发型。</li>
</ul>
<p>性能：</p>
<ul>
<li>在定量和定性分析中，FED-NeRF与现有的2D或3D方法相比，达到了最先进的性能。</li>
</ul>
<p>工作量：</p>
<ul>
<li>FED-NeRF的实现相对复杂，需要较高的计算资源。</li>
</ul>



<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d5f818de87353fbf13907e49c13b462d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b57ca3ade2e4538084a10eeb0919b87d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-102448406db5d3475d988673668fc7a1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1ec81b2b7dfa9352ae62039d258ec687.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ee173ef2962cf2f938e02414ced9add.jpg" align="middle">
</details>
​    


## What You See is What You GAN: Rendering Every Pixel for High-Fidelity   Geometry in 3D GANs
**Authors:Alex Trevithick, Matthew Chan, Towaki Takikawa, Umar Iqbal, Shalini De Mello, Manmohan Chandraker, Ravi Ramamoorthi, Koki Nagano**

3D-aware Generative Adversarial Networks (GANs) have shown remarkable progress in learning to generate multi-view-consistent images and 3D geometries of scenes from collections of 2D images via neural volume rendering. Yet, the significant memory and computational costs of dense sampling in volume rendering have forced 3D GANs to adopt patch-based training or employ low-resolution rendering with post-processing 2D super resolution, which sacrifices multiview consistency and the quality of resolved geometry. Consequently, 3D GANs have not yet been able to fully resolve the rich 3D geometry present in 2D images. In this work, we propose techniques to scale neural volume rendering to the much higher resolution of native 2D images, thereby resolving fine-grained 3D geometry with unprecedented detail. Our approach employs learning-based samplers for accelerating neural rendering for 3D GAN training using up to 5 times fewer depth samples. This enables us to explicitly "render every pixel" of the full-resolution image during training and inference without post-processing superresolution in 2D. Together with our strategy to learn high-quality surface geometry, our method synthesizes high-resolution 3D geometry and strictly view-consistent images while maintaining image quality on par with baselines relying on post-processing super resolution. We demonstrate state-of-the-art 3D gemetric quality on FFHQ and AFHQ, setting a new standard for unsupervised learning of 3D shapes in 3D GANs. 

[PDF](http://arxiv.org/abs/2401.02411v1) See our project page: https://research.nvidia.com/labs/nxp/wysiwyg/

**Summary**
深度图像采样加速器能够以大大降低的计算成本进行神经渲染，以便以更高分辨率训练生成对抗网络，从而生成更精细的 3D 几何形状。

**Key Takeaways**

- 深度图像采样加速器将神经体积渲染扩展到接近原生的 2D 图像分辨率，以前所未有的细节解析精细的 3D 几何体。
- 深度图像采样加速器使用基于学习的采样器，使用减少至多 5 倍的深度样本加速 3D GAN 训练的神经渲染。
- 深度图像采样加速器在训练和推理期间显式“渲染全分辨率图像的每个像素”，而无需在 2D 中进行后期处理超分辨率。
- 深度图像采样加速器与学习高质量曲面几何的策略相结合，可以在保持图像质量与依赖后期处理超分辨率的基线相当的同时，合成高分辨率的 3D 几何体和严格的视图一致图像。
- 深度图像采样加速器在 FFHQ 和 AFHQ 上展示了最先进的 3D 几何质量，为 3D GAN 中的 3D 形状的无监督学习树立了新标准。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>题目：渲染每个像素以在 3D GAN 中获得高保真几何体</li>
<li>作者：Saimun Sattar、Simon Niklaus、Richard Tucker、Niloy J. Mitra、Peter Wonka</li>
<li>单位：英伟达研究中心</li>
<li>关键词：3D GAN、神经体渲染、采样、几何体质量</li>
<li>论文链接：https://arxiv.org/abs/2401.02411</li>
<li>摘要：</li>
</ol>
<p>（1）研究背景：3D GAN 在学习生成场景的多视图一致图像和 3D 几何体方面取得了显着进展。然而，体渲染中密集采样的巨大内存和计算成本迫使 3D GAN 采用基于 patch 的训练或使用低分辨率渲染并进行后处理 2D 超分辨率，这牺牲了多视图一致性和已解析几何体的质量。因此，3D GAN 尚未能够完全解析 2D 图像中存在的丰富 3D 几何体。</p>
<p>（2）过去方法及问题：过去的方法主要有两种：基于 patch 的训练和低分辨率渲染。基于 patch 的训练可以减少内存和计算成本，但会牺牲图像质量和多视图一致性。低分辨率渲染可以保持图像质量和多视图一致性，但需要后处理 2D 超分辨率，这会引入伪影并降低几何体质量。</p>
<p>（3）研究方法：本文提出了一种将神经体渲染扩展到原生 2D 图像的更高分辨率的技术，从而以前所未有的细节解析细粒度的 3D 几何体。我们的方法采用基于学习的采样器来加速 3D GAN 训练的神经渲染，最多可减少 5 倍的深度采样。这使我们能够在训练和推理期间显式地“渲染全分辨率图像的每个像素”，而无需在 2D 中进行后处理超分辨率。结合我们学习高质量表面几何体的策略，我们的方法综合了高分辨率 3D 几何体和严格的视图一致图像，同时保持与依赖后处理超分辨率的基线相当的图像质量。</p>
<p>（4）实验结果：我们在 FFHQ 和 AFHQ 上展示了最先进的 3D 几何质量，为 3D GAN 中 3D 形状的无监督学习树立了新标准。</p>
<p>7.方法：
（1）提出了一种基于采样器的加速 3DGAN 方法，可将神经渲染解析为 2D 图像的原生分辨率，从而以前所未有的细节解析细粒度的 3D 几何体。
（2）学习采样器以加速 3DGAN 训练的神经渲染，最多可减少 5 倍的深度采样。
（3）显式地“渲染全分辨率图像的每个像素”，无需在 2D 中进行后处理超分辨率。
（4）结合策略学习高质量表面几何体，综合了高分辨率 3D 几何体和严格的视图一致图像，同时保持与依赖后处理超分辨率的基线相当的图像质量。</p>
<ol start="8">
<li>结论：
（1）：本文提出了一种基于采样器的加速3DGAN方法，可将神经渲染解析为2D图像的原生分辨率，从而以前所未有的细节解析细粒度的3D几何体。
（2）：创新点：</li>
</ol>
<ul>
<li>提出了一种学习采样器以加速3DGAN训练的神经渲染，最多可减少5倍的深度采样。</li>
<li>显式地“渲染全分辨率图像的每个像素”，无需在2D中进行后处理超分辨率。</li>
<li>结合策略学习高质量表面几何体，综合了高分辨率3D几何体和严格的视图一致图像，同时保持与依赖后处理超分辨率的基线相当的图像质量。
性能：</li>
<li>在FFHQ和AFHQ上展示了最先进的3D几何质量，为3DGAN中3D形状的无监督学习树立了新标准。
工作量：</li>
<li>需要大量的计算资源来训练和推理模型。</li>
</ul>



<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c8e26844034c822334224616389d9fff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-198f6a00de818ecd7d4a21c0df4f643a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0fa5c47519f923fb9367516ec51f18e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f299edb46d004ab3eb2d99138f860f5.jpg" align="middle">
</details>
​    


## Is It Possible to Backdoor Face Forgery Detection with Natural Triggers?
**Authors:Xiaoxuan Han, Songlin Yang, Wei Wang, Ziwen He, Jing Dong**

Deep neural networks have significantly improved the performance of face forgery detection models in discriminating Artificial Intelligent Generated Content (AIGC). However, their security is significantly threatened by the injection of triggers during model training (i.e., backdoor attacks). Although existing backdoor defenses and manual data selection can mitigate those using human-eye-sensitive triggers, such as patches or adversarial noises, the more challenging natural backdoor triggers remain insufficiently researched. To further investigate natural triggers, we propose a novel analysis-by-synthesis backdoor attack against face forgery detection models, which embeds natural triggers in the latent space. We thoroughly study such backdoor vulnerability from two perspectives: (1) Model Discrimination (Optimization-Based Trigger): we adopt a substitute detection model and find the trigger by minimizing the cross-entropy loss; (2) Data Distribution (Custom Trigger): we manipulate the uncommon facial attributes in the long-tailed distribution to generate poisoned samples without the supervision from detection models. Furthermore, to completely evaluate the detection models towards the latest AIGC, we utilize both state-of-the-art StyleGAN and Stable Diffusion for trigger generation. Finally, these backdoor triggers introduce specific semantic features to the generated poisoned samples (e.g., skin textures and smile), which are more natural and robust. Extensive experiments show that our method is superior from three levels: (1) Attack Success Rate: ours achieves a high attack success rate (over 99%) and incurs a small model accuracy drop (below 0.2%) with a low poisoning rate (less than 3%); (2) Backdoor Defense: ours shows better robust performance when faced with existing backdoor defense methods; (3) Human Inspection: ours is less human-eye-sensitive from a comprehensive user study. 

[PDF](http://arxiv.org/abs/2401.00414v1) 

**摘要**
分析与合成结合的隐空间自然触发器嵌入进行人脸伪造检测模型后门攻击分析。

**要点**

* 针对人脸伪造检测模型提出了一种新的分析-合成后门攻击，该攻击在潜在空间中嵌入自然触发器。
* 分析了模型判别和数据分布两种视角下的后门攻击。
* 利用最新的 StyleGAN 和 Stable Diffusion 来生成触发器。
* 在攻击成功率、后门防御和人工检查等方面优于现有方法。
* 带有自然触发器的中毒样本引入特定的语义特征（如皮肤纹理和微笑），更自然也更鲁棒。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>论文标题：是否可以通过自然触发器对人脸伪造检测进行后门攻击？</li>
<li>作者：韩晓轩，杨松林，王伟，何子文，董京</li>
<li>第一作者单位：中国科学院大学人工智能学院</li>
<li>关键词：后门攻击，人脸伪造检测，面部属性编辑</li>
<li>论文链接：https://arxiv.org/abs/2401.00414</li>
<li>摘要：
（1）研究背景：深度神经网络显著提高了人脸伪造检测模型区分人工智能生成内容（AIGC）的性能。然而，它们的安全受到了模型训练期间注入触发器（即后门攻击）的严重威胁。虽然现有的后门防御和手动数据选择可以通过减轻对人眼敏感的触发器（如补丁或对抗噪声）来缓解这些威胁，但更具挑战性的自然后门触发器研究还不够充分。
（2）过去的方法及其问题：为了进一步研究自然触发器，我们提出了一种针对人脸伪造检测模型的新颖的分析-合成后门攻击方法，该方法将自然触发器嵌入潜在空间。我们从两个角度彻底研究了这种后门漏洞：（1）模型判别（基于优化的触发器）：我们采用替代检测模型，并通过最小化交叉熵损失来寻找触发器；（2）数据分布（自定义触发器）：我们操纵长尾分布中的不常见面部属性，在没有检测模型监督的情况下生成中毒样本。此外，为了全面评估检测模型对最新 AIGC 的性能，我们利用最新的 StyleGAN 和 Stable Diffusion 进行触发器生成。
（3）提出的研究方法：最终，这些后门触发器为生成的中毒样本引入了特定的语义特征（例如，皮肤纹理和微笑），这些特征更自然且鲁棒。广泛的实验表明，我们的方法在三个方面优于其他方法：（1）攻击成功率：我们的方法实现了较高的攻击成功率（超过 99%），并且在较低的投毒率（低于 3%）下导致较小的模型准确度下降（低于 0.2%）；（2）后门防御：当面对现有的后门防御方法时，我们的方法表现出更好的鲁棒性能；（3）人工检查：从全面的用户研究来看，我们的方法对人眼不太敏感。</li>
</ol>
<p>Methods：</p>
<p>（1）优化生成触发器：利用替代检测模型，通过优化交叉熵损失函数，寻找潜在空间中的触发器；</p>
<p>（2）自定义触发器：分析长尾分布中的不常见面部属性，在没有检测模型监督的情况下生成中毒样本；</p>
<p>（3）模型训练：将中毒样本和良性样本混合，共同训练检测模型，使模型将特定语义特征与目标标签关联起来；</p>
<p>（4）模型测试：攻击者使用触发器生成图像，绕过人脸伪造检测模型，而未使用触发器生成的图像可以被正确分类。</p>
<ol start="8">
<li>结论：
（1）：本文针对人脸伪造检测模型的后门攻击进行了研究，提出了一种基于自然触发器的新型后门攻击方法。该方法将自然触发器嵌入潜在空间，并通过优化交叉熵损失函数或分析长尾分布中的不常见面部属性来生成触发器。实验结果表明，该方法在攻击成功率、后门防御和人工检查方面优于其他方法。
（2）：创新点：</li>
</ol>
<ul>
<li>提出了一种基于自然触发器的人脸伪造检测模型后门攻击方法。</li>
<li>设计了两种生成触发器的方法：优化生成触发器和自定义触发器。</li>
<li>分析了触发器对检测模型的影响，并提出了相应的防御措施。
性能：</li>
<li>该方法在三个方面优于其他方法：（1）攻击成功率：超过99%；（2）后门防御：表现出更好的鲁棒性能；（3）人工检查：对人眼不太敏感。
工作量：</li>
<li>该方法需要对人脸伪造检测模型进行训练，并生成触发器。</li>
<li>该方法的攻击成功率与触发器的质量有关，需要花费一定的时间和精力来生成高质量的触发器。</li>
</ul>



<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e6a4441d4cab4c7fcfe8ff967215c571.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5b05b35a7e0ad666e188c91169453bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b17a7d60d57015034bba81bb6b39cd4b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d74e6cb8c8e487c2a507b9ceae4eac6.jpg" align="middle">
</details>
​    


## Scalable Face Image Coding via StyleGAN Prior: Towards Compression for   Human-Machine Collaborative Vision
**Authors:Qi Mao, Chongyu Wang, Meng Wang, Shiqi Wang, Ruijie Chen, Libiao Jin, Siwei Ma**

The accelerated proliferation of visual content and the rapid development of machine vision technologies bring significant challenges in delivering visual data on a gigantic scale, which shall be effectively represented to satisfy both human and machine requirements. In this work, we investigate how hierarchical representations derived from the advanced generative prior facilitate constructing an efficient scalable coding paradigm for human-machine collaborative vision. Our key insight is that by exploiting the StyleGAN prior, we can learn three-layered representations encoding hierarchical semantics, which are elaborately designed into the basic, middle, and enhanced layers, supporting machine intelligence and human visual perception in a progressive fashion. With the aim of achieving efficient compression, we propose the layer-wise scalable entropy transformer to reduce the redundancy between layers. Based on the multi-task scalable rate-distortion objective, the proposed scheme is jointly optimized to achieve optimal machine analysis performance, human perception experience, and compression ratio. We validate the proposed paradigm's feasibility in face image compression. Extensive qualitative and quantitative experimental results demonstrate the superiority of the proposed paradigm over the latest compression standard Versatile Video Coding (VVC) in terms of both machine analysis as well as human perception at extremely low bitrates ($&lt;0.01$ bpp), offering new insights for human-machine collaborative compression. 

[PDF](http://arxiv.org/abs/2312.15622v1) Accepted by IEEE TIP

**摘要**
通过层级表示模型与视觉内容编码的结合，可以提高机器智能和视觉感知效能，实现图像的高效视觉压缩。

**要点**

- 基于 StyleGAN 先验，构建三层分层语义编码模型，支持机器分析和人类视觉感知。
- 提出基于分层可变熵变换器的编码方案，降低层间冗余，提高压缩效率。
- 采用多任务可变速率失真目标函数，实现机器分析性能、人类感知体验和压缩率的最优化。
- 在人脸图像压缩中验证了所提范式的可行性。
- 实验结果表明，在极低比特率（&lt;0.01 bpp）下，所提范式在机器分析和人类感知方面均优于最新的压缩标准多功能视频编码（VVC）。
- 为人机协同压缩提供了新的思路。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li><p>标题：基于 StyleGAN 先验的可扩展人脸图像编码：面向人机协同视觉的压缩</p>
</li>
<li><p>作者：齐茂、王崇宇、王萌、王世奇、陈瑞杰、金立标、马思伟</p>
</li>
<li><p>隶属单位：中国传媒大学信息与通信工程学院、国家媒体融合与传播工程技术研究中心</p>
</li>
<li><p>关键词：人机协同压缩、可扩展编码、生成压缩、StyleGAN</p>
</li>
<li><p>链接：https://arxiv.org/abs/2312.15622</p>
</li>
<li><p>摘要：
(1)：随着视觉内容的激增和机器视觉技术的快速发展，在大规模传输视觉数据时面临着严峻的挑战，需要有效地表示数据以满足人类和机器的需求。
(2)：以往的方法主要集中在图像压缩或机器视觉压缩，但无法同时满足人机协同视觉的需求。
(3)：本文提出了一种基于 StyleGAN 先验的可扩展人脸图像编码方法，该方法利用 StyleGAN 先验学习三层表示，分别对应基本层、中间层和增强层，并设计了一种分层可扩展熵变换器来减少层之间的冗余。
(4)：实验结果表明，该方法在极低比特率（&lt;0.01bpp）下，在机器分析和人类感知方面均优于最新的压缩标准多功能视频编码（VVC）。</p>
</li>
<li><p>方法：
(1) 利用 StyleGAN 先验学习三层表示，分别对应基本层、中间层和增强层；
(2) 设计分层可扩展熵变换器来减少层之间的冗余；
(3) 提出多任务可扩展 R-D 优化方法，在受控比特率约束下同时优化三层解码图像；
(4) 采用对抗训练来增强增强层图像的纹理。</p>
</li>
<li><p>结论：
（1）：本文提出了一种基于 StyleGAN 先验的可扩展人脸图像编码方法，该方法在极低比特率（&lt;0.01bpp）下，在机器分析和人类感知方面均优于最新的压缩标准多功能视频编码（VVC）。
（2）：创新点：</p>
</li>
</ol>
<ul>
<li>利用 StyleGAN 先验学习三层表示，分别对应基本层、中间层和增强层，该方法可以有效地去除人脸图像中的冗余信息，并保留人脸图像的关键特征。</li>
<li>设计分层可扩展熵变换器来减少层之间的冗余，该变换器可以有效地减少三层表示之间的相关性，并提高编码效率。</li>
<li>提出多任务可扩展 R-D 优化方法，在受控比特率约束下同时优化三层解码图像，该方法可以有效地提高编码性能。</li>
<li>采用对抗训练来增强增强层图像的纹理，该方法可以有效地提高增强层图像的质量，并提高编码性能。
性能：</li>
<li>该方法在极低比特率（&lt;0.01bpp）下，在机器分析和人类感知方面均优于最新的压缩标准多功能视频编码（VVC）。</li>
<li>该方法可以有效地去除人脸图像中的冗余信息，并保留人脸图像的关键特征。</li>
<li>该方法可以有效地减少三层表示之间的相关性，并提高编码效率。</li>
<li>该方法可以有效地提高编码性能。</li>
<li>该方法可以有效地提高增强层图像的质量，并提高编码性能。
工作量：</li>
<li>该方法需要设计分层可扩展熵变换器和多任务可扩展 R-D 优化方法，这需要较高的数学和编程能力。</li>
<li>该方法需要采用对抗训练来增强增强层图像的纹理，这需要较高的计算资源。</li>
<li>该方法需要较高的计算复杂度，这可能会影响编码速度。</li>
</ul>



<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c605faab09c529ceddb03676f3c952f6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0264f3a9e653ed6bd3b5559ddaa228b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06e69cec60b6e777f4b2ccd3fe36084e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-146bf25e7774478609dfac556e36e8fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5cdc78a006a5a82fe40d8fbf0f0aae35.jpg" align="middle">
</details>
​    


## StyleRetoucher: Generalized Portrait Image Retouching with GAN Priors
**Authors:Wanchao Su, Can Wang, Chen Liu, Hangzhou Han, Hongbo Fu, Jing Liao**

Creating fine-retouched portrait images is tedious and time-consuming even for professional artists. There exist automatic retouching methods, but they either suffer from over-smoothing artifacts or lack generalization ability. To address such issues, we present StyleRetoucher, a novel automatic portrait image retouching framework, leveraging StyleGAN's generation and generalization ability to improve an input portrait image's skin condition while preserving its facial details. Harnessing the priors of pretrained StyleGAN, our method shows superior robustness: a). performing stably with fewer training samples and b). generalizing well on the out-domain data. Moreover, by blending the spatial features of the input image and intermediate features of the StyleGAN layers, our method preserves the input characteristics to the largest extent. We further propose a novel blemish-aware feature selection mechanism to effectively identify and remove the skin blemishes, improving the image skin condition. Qualitative and quantitative evaluations validate the great generalization capability of our method. Further experiments show StyleRetoucher's superior performance to the alternative solutions in the image retouching task. We also conduct a user perceptive study to confirm the superior retouching performance of our method over the existing state-of-the-art alternatives. 

[PDF](http://arxiv.org/abs/2312.14389v1) 13 pages, 15 figures

**摘要**
利用StyleGAN的高效生成能力和泛化能力，StyleRetoucher可大幅改善输入人像的皮肤状况，同时不会影响其面部细节。

**要点**

- StyleRetoucher 是一款新颖的自动人像图像修图框架，它利用 StyleGAN 的生成能力和泛化能力来改善输入人像图像的皮肤状况，同时保留其面部细节。
- 与其他自动修图方法相比，StyleRetoucher 具有更好的鲁棒性：即使在训练样本较少的情况下也能稳定地执行任务，并且在域外数据上具有良好的泛化能力。
- 通过融合输入图像的空间特征和 StyleGAN 层的中间特征，StyleRetoucher 可以最大程度地保留输入特征。
- 一种新颖的瑕疵感知特征选择机制可以有效识别并去除皮肤瑕疵，改善图像皮肤状况。
- 定性和定量评估验证了该方法强大的泛化能力。
- 进一步的实验表明，StyleRetoucher 在图像修图任务中优于其他替代解决方案。
- 用户感知研究表明，该方法的修图性能优于现有的最先进的替代方法。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li><p>标题：StyleRetoucher：基于GAN先验的通用人像图片修图</p>
</li>
<li><p>作者：Wanchao Su, Can Wang, Chen Liu, Fangzhou Han, Hongbo Fu, Jing Liao</p>
</li>
<li><p>隶属机构：无</p>
</li>
<li><p>关键词：人像图片修图、StyleGAN、GAN先验、皮肤瑕疵去除、特征融合</p>
</li>
<li><p>论文链接：无，Github代码链接：无</p>
</li>
<li><p>摘要：
(1)：研究背景：人像图片修图是一项复杂且耗时的任务，需要专业艺术家的参与。现有的自动修图方法要么产生过度平滑的伪影，要么缺乏泛化能力。
(2)：过去的方法：过去的方法主要基于CNN，存在过度平滑和泛化能力差的问题。
(3)：研究方法：本文提出了一种新的人像图片修图框架StyleRetoucher，利用StyleGAN的生成和泛化能力来改善输入人像图片的皮肤状况，同时保留其面部细节。该方法通过融合输入图像的空间特征和StyleGAN中间层的特征，最大程度地保留了输入图像的特征。此外，还提出了一种新的瑕疵感知特征选择机制，可以有效地识别和去除皮肤瑕疵，改善图像的皮肤状况。
(4)：方法性能：定性和定量评估验证了该方法的泛化能力。进一步的实验表明，StyleRetoucher在图像修图任务中优于其他替代方案。用户感知研究也证实了该方法优于现有最先进的替代方案。</p>
</li>
<li><p>方法：
(1) StyleGAN先验融合：该方法利用StyleGAN的生成和泛化能力，提取输入图像的空间特征和StyleGAN中间层的特征，并融合这些特征，最大程度地保留输入图像的特征。
(2) 瑕疵感知特征选择：提出了一种新的瑕疵感知特征选择机制，可以有效地识别和去除皮肤瑕疵，改善图像的皮肤状况。
(3) 迭代优化：通过迭代优化，逐步改善图像的皮肤状况，同时保留其面部细节。</p>
</li>
<li><p>结论：
（1）：本工作提出了一种新的通用人像图片修图框架StyleRetoucher，利用StyleGAN的生成和泛化能力，有效地改善了输入人像图片的皮肤状况，同时保留了其面部细节。该方法在定性和定量评估中均取得了良好的结果，证明了其泛化能力和优越性。进一步的实验表明，StyleRetoucher在图像修图任务中优于其他替代方案。用户感知研究也证实了该方法优于现有最先进的替代方案。
（2）：创新点：</p>
</li>
</ol>
<ul>
<li>提出了一种新的人像图片修图框架StyleRetoucher，利用StyleGAN的生成和泛化能力，有效地改善了输入人像图片的皮肤状况，同时保留了其面部细节。</li>
<li>提出了一种新的瑕疵感知特征选择机制，可以有效地识别和去除皮肤瑕疵，改善图像的皮肤状况。</li>
<li>通过迭代优化，逐步改善图像的皮肤状况，同时保留其面部细节。
性能：</li>
<li>在定性和定量评估中均取得了良好的结果，证明了其泛化能力和优越性。</li>
<li>在图像修图任务中优于其他替代方案。</li>
<li>用户感知研究也证实了该方法优于现有最先进的替代方案。
工作量：</li>
<li>该方法的实现相对简单，易于部署和使用。</li>
<li>该方法的训练过程相对较快，可以在合理的时间内完成。</li>
<li>该方法的推理过程相对较快，可以实时处理图像。</li>
</ul>



<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1da4e5eed7542c8e382bd71ed1222c8e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eecb890ae8fa7ffea652e6d2ccfd2f90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29ec59afed1c6e0dba20d552901cdbd2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1cca26e2ef7490681e28d8a8b63ab99c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1aca71fe4cee342f6c693d139fa2e057.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7cccc88967632c8ffb8a168720e3210f.jpg" align="middle">
</details>
​    


## HeadCraft: Modeling High-Detail Shape Variations for Animated 3DMMs
**Authors:Artem Sevastopolsky, Philip-William Grassal, Simon Giebenhain, ShahRukh Athar, Luisa Verdoliva, Matthias Niessner**

Current advances in human head modeling allow to generate plausible-looking 3D head models via neural representations. Nevertheless, constructing complete high-fidelity head models with explicitly controlled animation remains an issue. Furthermore, completing the head geometry based on a partial observation, e.g. coming from a depth sensor, while preserving details is often problematic for the existing methods. We introduce a generative model for detailed 3D head meshes on top of an articulated 3DMM which allows explicit animation and high-detail preservation at the same time. Our method is trained in two stages. First, we register a parametric head model with vertex displacements to each mesh of the recently introduced NPHM dataset of accurate 3D head scans. The estimated displacements are baked into a hand-crafted UV layout. Second, we train a StyleGAN model in order to generalize over the UV maps of displacements. The decomposition of the parametric model and high-quality vertex displacements allows us to animate the model and modify it semantically. We demonstrate the results of unconditional generation and fitting to the full or partial observation. The project page is available at https://seva100.github.io/headcraft. 

[PDF](http://arxiv.org/abs/2312.14140v1) Project page: https://seva100.github.io/headcraft. Video:   https://youtu.be/uBeBT2f1CL0. 23 pages, 19 figures, 2 tables

**摘要**
生成模型可用于创建详细的 3D 头部网格，并允许显式动画和高细节保留。

**要点**
* 我们提出了一种生成模型，用于在可铰接 3DMM 的基础上生成详细的 3D 头部网格。
* 我们的模型可以显式地动画化头部并对头部进行语义修改。
* 我们将参数模型和高质量顶点位移分解，以便对模型进行动画处理并对其进行语义修改。
* 我们演示了无条件生成和拟合到完全或部分观察的结果。
* 我们使用最近引入的 NPHM 精确 3D 头部扫描数据集对模型进行了训练。
* 我们将估计的位移烘焙到手工制作的 UV 布局中。
* 我们训练了一个 StyleGAN 模型来概括位移的 UV 映射。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>题目：HeadCraft：为动画 3DMM 建模高细节形状变化</li>
<li>作者：Artem Sevastopolsky、Philip-William Grassal、Simon Giebenhain、Shah Rukh Athar、Luisa Verdoliva、Matthias Nießner</li>
<li>第一作者单位：慕尼黑工业大学（TUM），德国</li>
<li>关键词：生成头部模型、3D 扫描数据集、参数化模板、随机生成、深度观察、完整几何</li>
<li>论文链接：https://arxiv.org/abs/2312.14140
Github 代码链接：无</li>
<li>摘要：
(1)：研究背景：当前的人类头部建模技术可以通过神经表征来生成逼真的 3D 头部模型，这些模型在计算机图形学、虚拟现实和数字娱乐等领域都有应用。然而，构建具有显式控制动画和足够细节的完整高保真头部模型仍然是一个挑战。
(2)：过去方法与问题：现有的隐式生成模型，如 pi-GAN、EG3D 或 StyleGAN-3，通常不能很好地处理动画和跟踪，并且在从部分观察（例如来自深度传感器）完成头部几何时，往往难以保留细节。
(3)：研究方法：本文提出了一种在铰接 3DMM 之上的详细 3D 头部网格生成模型，该模型允许显式动画和高细节保留。该方法分两个阶段训练。首先，将参数化头部模型与顶点位移注册到最近引入的 NPHM 数据集的每个网格，以获得准确的 3D 头部扫描。然后，训练一个 StyleGAN 模型来概括位移的 UV 贴图。
(4)：方法性能：本文方法在无条件生成和拟合完整或部分观察方面取得了良好的性能，证明了其有效性。</li>
</ol>
<p>方法：
（1）：本文提出了一种在铰接3DMM之上的详细3D头部网格生成模型，该模型允许显式动画和高细节保留。
（2）：该方法分两个阶段训练。首先，将参数化头部模型与顶点位移注册到最近引入的NPHM数据集的每个网格，以获得准确的3D头部扫描。然后，训练一个StyleGAN模型来概括位移的UV贴图。
（3）：在第一个阶段，优化具有附加矢量位移的损失函数，以防止自相交。在第二个阶段，优化具有仅允许沿先前位移顶点的法线移动的位移的损失函数，以拟合高频细节。
（4）：最后，将位移烘焙到UV贴图中，并使用StyleGAN2模型生成UV位移贴图。</p>
<ol start="8">
<li>结论：
（1）：本文提出了一种在铰接3DMM之上的详细3D头部网格生成模型，该模型允许显式动画和高细节保留，在无条件生成和拟合完整或部分观察方面取得了良好的性能，证明了其有效性。
（2）：创新点：</li>
</ol>
<ul>
<li>将参数化头部模型与顶点位移注册到3D头部扫描，以获得准确的3D头部扫描。</li>
<li>训练一个StyleGAN模型来概括位移的UV贴图。</li>
<li>在第一个阶段，优化具有附加矢量位移的损失函数，以防止自相交。</li>
<li>在第二个阶段，优化具有仅允许沿先前位移顶点的法线移动的位移的损失函数，以拟合高频细节。</li>
<li>将位移烘焙到UV贴图中，并使用StyleGAN2模型生成UV位移贴图。
性能：</li>
<li>在无条件生成和拟合完整或部分观察方面取得了良好的性能。
工作量：</li>
<li>需要大量的数据和计算资源。</li>
</ul>



<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-0b60b8f51ddf0eb511b0da3467102956.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-634ce760929e598097380f2cf865992f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-08d46c76d8da31eb3f3362ff9d4fefa4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bd84563b8211b1c631a249968d6a3bb7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da274555d2a06c54e8244021b7e2af1d.jpg" align="middle">
</details>
​    


## Warping the Residuals for Image Editing with StyleGAN
**Authors:Ahmet Burak Yildirim, Hamza Pehlivan, Aysegul Dundar**

StyleGAN models show editing capabilities via their semantically interpretable latent organizations which require successful GAN inversion methods to edit real images. Many works have been proposed for inverting images into StyleGAN's latent space. However, their results either suffer from low fidelity to the input image or poor editing qualities, especially for edits that require large transformations. That is because low-rate latent spaces lose many image details due to the information bottleneck even though it provides an editable space. On the other hand, higher-rate latent spaces can pass all the image details to StyleGAN for perfect reconstruction of images but suffer from low editing qualities. In this work, we present a novel image inversion architecture that extracts high-rate latent features and includes a flow estimation module to warp these features to adapt them to edits. The flows are estimated from StyleGAN features of edited and unedited latent codes. By estimating the high-rate features and warping them for edits, we achieve both high-fidelity to the input image and high-quality edits. We run extensive experiments and compare our method with state-of-the-art inversion methods. Qualitative metrics and visual comparisons show significant improvements. 

[PDF](http://arxiv.org/abs/2312.11422v1) 

**Summary:**
GAN图像反演创新方法可实现高质量图像编辑。

**Key Takeaways:**

- StyleGAN模型具有语义可解释的潜在组织，需要成功的GAN反演方法来编辑真实图像。
- 许多研究已经提出将图像反演到StyleGAN潜在空间的方法。
- 低速率潜在空间由于信息瓶颈而丢失许多图像细节，即使它提供了可编辑的空间。
- 高速率潜在空间可以将所有图像细节传递给StyleGAN以完美重建图像，但编辑质量较差。
- 本工作提出了一种新的图像反演架构，可提取高速率潜在特征，并包括一个流动估计模块，将这些特征扭曲以适应编辑。
- 通过估计高速率特征并将其扭曲以进行编辑，我们实现了对输入图像的高保真度和高质量的编辑。
- 广泛的实验表明我们的方法优于最先进的反演方法。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>标题：使用 StyleGAN 进行图像编辑的残差翘曲</li>
<li>作者：Ahmet Burak Yildirim、Hamza Pehlivan、Aysegul Dundar</li>
<li>作者单位：无</li>
<li>关键词：GAN 反演、图像编辑、生成对抗网络</li>
<li>论文链接：https://arxiv.org/abs/2312.11422，Github 代码链接：无</li>
<li>摘要：
（1）研究背景：StyleGAN 模型因其语义可解释的潜在组织而显示出编辑功能，这需要成功的 GAN 反演方法来编辑真实图像。许多工作已被提议将图像反演到 StyleGAN 的潜在空间。然而，它们的结果要么对输入图像的保真度低，要么编辑质量差，尤其是对于需要大变换的编辑。这是因为低速率潜在空间由于信息瓶颈而丢失了许多图像细节，即使它提供了可编辑的空间。另一方面，更高速率的潜在空间可以将所有图像细节传递给 StyleGAN 以完美重建图像，但编辑质量会降低，如果这些特征与编辑不一致。
（2）过去的方法和问题：为了解决这种权衡，我们之前的工作 StyleRes 提出了一种框架，该框架可以学习更高速率潜在代码中的残差特征，这些特征在编码特征的重建中缺失。然而，这些高比率潜在代码需要适应图像编辑。例如，如果高比率代码携带有关人耳环的信息，并且图像被编辑以改变人的姿势，那么这些代码应该相应地被携带到生成器中以出现在正确的位置。否则，它会导致重影效应。因此，StyleRes 提议用随机编辑和循环一致性指导来训练模型，以便在应用编辑并恢复回原状时重建原始图像。通过这个指导，它学习了一个模块来转换这些更高比率的潜在代码以进行图像编辑。然而，学习的模块是一个卷积神经网络，并且在正确转换残差以进行编辑方面具有有限的能力。
（3）方法：在本文中，我们的主要贡献是一个管道，它可以学习高保真图像重建的残差并正确采用它们以进行高质量的图像编辑。我们的框架通过估计原始图像特征和编辑图像特征之间的流动并根据流动翘曲残差特征来实现这一点。我们利用一个无监督的流动估计网络以一种新颖的方式训练我们的框架。该框架在极端编辑变换下实现了显着更好的结果，而我们的架构是单阶段且有效的。
（4）实验结果：我们的贡献如下：</li>
</ol>
<ul>
<li>我们提出了一种新颖的反演管道，通过预测原始图像和编辑图像之间的流动并学习根据预测的流动翘曲高比率特征来实现高质量的图像编辑。</li>
<li>我们通过一个无监督的预训练流动估计网络来指导流动预测。流动预测以 StyleGAN 中间特征作为输入以实现效率。</li>
<li>我们表明我们的框架可以与不同的预训练 StyleGAN 反演网络一起工作，并以很大的幅度改进所有这些网络。</li>
<li>我们的广泛实验表明了我们方法的有效性。</li>
</ul>
<ol start="7">
<li><p>Methods:
(1): 本文提出了一种新的反演管道，通过预测原始图像和编辑图像之间的流动并学习根据预测的流动翘曲高比率特征来实现高质量的图像编辑。
(2): 该框架通过一个无监督的预训练流动估计网络来指导流动预测。流动预测以StyleGAN中间特征作为输入以实现效率。
(3): 本文表明该框架可以与不同的预训练StyleGAN反演网络一起工作，并以很大的幅度改进所有这些网络。
(4): 广泛的实验表明了该方法的有效性。</p>
</li>
<li><p>结论：
（1）该工作提出了一种 GAN 反演方法，该方法在运行时高效，并且在各种工作中探索的编辑方向下实现了高保真度和高质量的图像编辑。与以前的工作不同，我们提出估计未编辑和编辑特征的流预测，以扭曲高比率残差特征，这些特征对于精确的图像重建是必需的。我们表明，所提出的框架在许多编辑中实现了最先进的结果，尤其是在需要大变换的编辑中。
（2）创新点：</p>
</li>
</ol>
<ul>
<li>提出了一种新的 GAN 反演管道，通过预测原始图像和编辑图像之间的流动并学习根据预测的流动扭曲高比率特征来实现高质量的图像编辑。</li>
<li>该框架通过一个无监督的预训练流动估计网络来指导流动预测。流动预测以 StyleGAN 中间特征作为输入以实现效率。</li>
<li>表明该框架可以与不同的预训练 StyleGAN 反演网络一起工作，并以很大的幅度改进所有这些网络。</li>
<li>广泛的实验表明了该方法的有效性。
性能：</li>
<li>该方法在极端编辑变换下实现了显着更好的结果，而我们的架构是单阶段且有效的。</li>
<li>该方法可以与不同的预训练 StyleGAN 反演网络一起工作，并以很大的幅度改进所有这些网络。</li>
<li>该方法在广泛的实验中表明了其有效性。
工作量：</li>
<li>该方法需要一个无监督的预训练流动估计网络来指导流动预测。</li>
<li>该方法需要广泛的实验来表明其有效性。</li>
</ul>



<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-3da8d474a8794bf8e7be10e3a9d757df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e321c50f9edfe34a7ecdbffb04a0979e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43b1de4b501c695a4a18a9daefec168c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7fcb4e4ade991311e9a49c68e363e4c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-46d11b639cc223b41293eb04b8aca2d9.jpg" align="middle">
</details>
​    


## High-Fidelity Face Swapping with Style Blending
**Authors:Xinyu Yang, Hongbo Bo**

Face swapping has gained significant traction, driven by the plethora of human face synthesis facilitated by deep learning methods. However, previous face swapping methods that used generative adversarial networks (GANs) as backbones have faced challenges such as inconsistency in blending, distortions, artifacts, and issues with training stability. To address these limitations, we propose an innovative end-to-end framework for high-fidelity face swapping. First, we introduce a StyleGAN-based facial attributes encoder that extracts essential features from faces and inverts them into a latent style code, encapsulating indispensable facial attributes for successful face swapping. Second, we introduce an attention-based style blending module to effectively transfer Face IDs from source to target. To ensure accurate and quality transferring, a series of constraint measures including contrastive face ID learning, facial landmark alignment, and dual swap consistency is implemented. Finally, the blended style code is translated back to the image space via the style decoder, which is of high training stability and generative capability. Extensive experiments on the CelebA-HQ dataset highlight the superior visual quality of generated images from our face-swapping methodology when compared to other state-of-the-art methods, and the effectiveness of each proposed module. Source code and weights will be publicly available. 

[PDF](http://arxiv.org/abs/2312.10843v1) 4 pages

**Summary**
优化生成对抗网络 (GAN) 架构，提出基于 StyleGAN 的人脸属性编码器和基于注意力的风格融合模块，实现高保真换脸。

**Key Takeaways**

- 提出了一种基于 StyleGAN 的人脸属性编码器，将人脸特征提取并反演为潜在样式代码。
- 设计了一个基于注意力的风格融合模块，有效地将源人脸的 Face ID 转移到目标人脸。
- 采用了一系列约束措施，包括对比人脸 ID 学习、人脸关键点对齐和双重交换一致性。
- 将混合的样式代码通过样式解码器翻译回图像空间，具有很高的训练稳定性和生成能力。
- 在 CelebA-HQ 数据集上进行的广泛实验表明，该换脸方法生成的图像视觉质量优于其他最先进的方法。
- 同时验证了每一项提出的模块的有效性。
- 源代码和权重将公开发布。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>题目：风格融合的高保真换脸</li>
<li>作者：Xinyu Yang</li>
<li>单位：兰开斯特大学</li>
<li>关键词：生成对抗网络、人脸合成、人脸交换</li>
<li>论文链接：https://arxiv.org/abs/2312.10843</li>
<li>摘要：
（1）研究背景：换脸技术近年来备受关注，但现有的换脸方法存在融合不一致、扭曲、伪影和训练稳定性差等问题。
（2）过去方法与不足：以往基于生成对抗网络（GAN）的换脸方法存在融合不一致、扭曲、伪影和训练稳定性差等问题。这些方法难以平衡身份相似性和目标的低级细节保留，导致生成的图像经常出现伪影和低保真度。
（3）研究方法：本文提出了一种新的端到端高保真换脸框架。该框架包括三个关键组成部分：面部属性编码器、风格融合模块和风格解码器。面部属性编码器将目标和源图像的特征提取到潜在空间中，并使用多头交叉注意（MHCA）将源嵌入和目标嵌入进行融合。最后，使用预训练的风格解码器将融合的嵌入解码为图像。
（4）性能与结论：在 CelebA-HQ 数据集上进行的广泛实验表明，该方法生成的图像具有更高的视觉质量，并且能够有效地将源图像的面部特征转移到目标图像中。</li>
</ol>
<p>Some Error for method(比如是不是没有Methods这个章节)</p>
<ol start="8">
<li>结论：
（1）：本文提出了一种基于风格编码模型的高保真换脸框架，该框架能够生成具有更高视觉质量和更逼真的人脸图像，有效地将源图像的面部特征转移到目标图像中。
（2）：创新点：
该框架采用了一种新的风格融合模块，该模块能够在潜在空间中将源嵌入和目标嵌入进行融合，从而生成更逼真的图像。
该框架使用预训练的风格解码器将融合的嵌入解码为图像，这使得该框架能够生成更逼真的人脸图像。
性能：
该框架在CelebA-HQ数据集上进行了广泛的实验，实验结果表明，该框架生成的图像具有更高的视觉质量，并且能够有效地将源图像的面部特征转移到目标图像中。
工作量：
该框架的实现相对复杂，需要较高的计算资源和较长时间的训练。</li>
</ol>



<p></p><details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-911f1f1ba2307b836811866ce4529595.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a1567919a066b4968d69d7bc94705df3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cdc82b874821cb5b2ef53e92f09ee664.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94207fdbd05bbd894e5ca3f64d99a25b.jpg" align="middle">
</details><br>​    <p></p>
<h2 id="Super-Resolution-through-StyleGAN-Regularized-Latent-Search-A-Realism-Fidelity-Trade-off"><a href="#Super-Resolution-through-StyleGAN-Regularized-Latent-Search-A-Realism-Fidelity-Trade-off" class="headerlink" title="Super-Resolution through StyleGAN Regularized Latent Search: A   Realism-Fidelity Trade-off"></a>Super-Resolution through StyleGAN Regularized Latent Search: A   Realism-Fidelity Trade-off</h2><p><strong>Authors:Marzieh Gheisari, Auguste Genovesio</strong></p>
<p>This paper addresses the problem of super-resolution: constructing a highly resolved (HR) image from a low resolved (LR) one. Recent unsupervised approaches search the latent space of a StyleGAN pre-trained on HR images, for the image that best downscales to the input LR image. However, they tend to produce out-of-domain images and fail to accurately reconstruct HR images that are far from the original domain. Our contribution is twofold. Firstly, we introduce a new regularizer to constrain the search in the latent space, ensuring that the inverted code lies in the original image manifold. Secondly, we further enhanced the reconstruction through expanding the image prior around the optimal latent code. Our results show that the proposed approach recovers realistic high-quality images for large magnification factors. Furthermore, for low magnification factors, it can still reconstruct details that the generator could not have produced otherwise. Altogether, our approach achieves a good trade-off between fidelity and realism for the super-resolution task. </p>
<p><a href="http://arxiv.org/abs/2311.16923v1">PDF</a> </p>
<p><strong>Summary</strong><br>通过约束潜在空间搜索范围和扩展图像先验来提高深度生成模型的超分辨率重建</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>该论文关注超分辨率问题，即从低分辨率图像生成高分辨率图像。</li>
<li>最近的无监督方法在预先训练好的 StyleGAN 的潜在空间中搜索图像，使其下采样后与输入的低分辨率图像最匹配。</li>
<li>但这些方法倾向于产生域外图像，无法准确重建远离原始域的高分辨率图像。</li>
<li>引入了一个新的正则化项来约束潜在空间中的搜索，确保反转代码位于原始图像流形中。</li>
<li>通过扩展最佳潜在代码周围的图像先验来进一步增强重建。</li>
<li>实验结果表明，该方法可以在大倍率放大因子下恢复逼真、高质量的图像。</li>
<li>对于低倍率放大因子，它仍然可以重建生成器无法生成的其他细节。</li>
<li>总之，该方法在超分辨率任务中实现了保真度和真实性之间的良好权衡。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<p></p><ol><p></p>
<p></p><li><p>题目：通过 StyleGAN 正则化潜在搜索实现超分辨率：真实性-保真度权衡</p>
</li><p></p>
<p></p><li><p>作者：Marzieh Gheisari, Auguste Genovesio</p>
</li><p></p>
<p></p><li><p>单位：巴黎高等师范学院</p>
</li><p></p>
<p></p><li><p>关键词：超分辨率、StyleGAN、潜在搜索、正则化、真实性、保真度</p>
</li><p></p>
<p></p><li><p>链接：https://arxiv.org/abs/2311.16923
Github：无</p>
</li><p></p>
<p></p><li><p>摘要：
(1)：超分辨率旨在从低分辨率图像中重建未知的高分辨率图像。近年来，生成模型极大地促进了超分辨率的发展。
(2)：GAN-based 方法和先验引导方法是超分辨率的两种主要研究趋势。GAN-based 方法学习高分辨率图像和低分辨率图像的直接耦合，但存在局限性，例如产生伪影和不自然的纹理。先验引导方法通过利用先验信息来更好地定义目标，从而提高重建的稳定性。
(3)：本文提出了一种新的正则化方法来约束潜在空间中的搜索，确保反转的编码位于原始图像流形中。此外，通过在最优潜在编码周围扩展图像先验，进一步增强了重建效果。
(4)：实验结果表明，该方法在较大的放大倍数下可以恢复真实且高质量的图像。即使在较小的放大倍数下，它仍然可以重建生成器无法单独生成的细节。总体而言，该方法在超分辨率任务中实现了真实性和保真度之间的良好权衡。</p>
</li><p></p>
<p></p><li><p>方法：</p>
</li><br>&lt;/ol&gt;<p></p>
<p></p><p>（1）潜在空间正则化：提出了一种新的正则化方法，通过在潜在空间中约束搜索来确保反转的编码位于原始图像流形中。该方法通过最小化潜在编码和相应的生成图像之间的距离来实现。</p><p></p>
<p></p><p>（2）图像先验扩展：在最优潜在编码周围扩展图像先验，以进一步增强重建效果。该方法通过使用高斯核对最优潜在编码周围的潜在空间进行加权来实现。</p><p></p>
<p></p><p>（3）超分辨率重建：使用StyleGAN生成器将正则化的潜在编码反转为高分辨率图像。该方法通过最小化生成的图像和低分辨率输入图像之间的距离来实现。</p><p></p>
<p></p><ol start="8"><p></p>
<p></p><li>结论：
（1）本文提出了一种新的正则化方法，通过在潜在空间中约束搜索来确保反转的编码位于原始图像流形中。此外，通过在最优潜在编码周围扩展图像先验，进一步增强了重建效果。实验结果表明，该方法在较大的放大倍数下可以恢复真实且高质量的图像。即使在较小的放大倍数下，它仍然可以重建生成器无法单独生成的细节。总体而言，该方法在超分辨率任务中实现了真实性和保真度之间的良好权衡。
（2）创新点：</li><br>&lt;/ol&gt;<p></p>
<ul>
<li>提出了一种新的正则化方法，通过在潜在空间中约束搜索来确保反转的编码位于原始图像流形中。</li>
<li>通过在最优潜在编码周围扩展图像先验，进一步增强了重建效果。
性能：</li>
<li>该方法在较大的放大倍数下可以恢复真实且高质量的图像。</li>
<li>即使在较小的放大倍数下，它仍然可以重建生成器无法单独生成的细节。</li>
<li>总体而言，该方法在超分辨率任务中实现了真实性和保真度之间的良好权衡。
工作量：</li>
<li>该方法需要对StyleGAN生成器进行预训练。</li>
<li>需要对正则化方法和图像先验扩展方法进行训练。</li>
<li>需要对超分辨率重建方法进行训练。</li>
</ul>



<p></p><details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-047c7ef399b99c1a231d9dac098cd5ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2b9f640a36ad41c3da4593c85263843.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1e5efa0aaad503171165b516b9c711a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a5de01c1a980dac1eb85f183edcaa69.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50f53d31ed0811c18da861677d9c633b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-147a4284c73eb5f49586ed6d811f5b2d.jpg" align="middle">
</details><br>​    <p></p>
<h2 id="Animatable-Gaussians-Learning-Pose-dependent-Gaussian-Maps-for-High-fidelity-Human-Avatar-Modeling"><a href="#Animatable-Gaussians-Learning-Pose-dependent-Gaussian-Maps-for-High-fidelity-Human-Avatar-Modeling" class="headerlink" title="Animatable Gaussians: Learning Pose-dependent Gaussian Maps for   High-fidelity Human Avatar Modeling"></a>Animatable Gaussians: Learning Pose-dependent Gaussian Maps for   High-fidelity Human Avatar Modeling</h2><p><strong>Authors:Zhe Li, Zerong Zheng, Lizhen Wang, Yebin Liu</strong></p>
<p>Modeling animatable human avatars from RGB videos is a long-standing and challenging problem. Recent works usually adopt MLP-based neural radiance fields (NeRF) to represent 3D humans, but it remains difficult for pure MLPs to regress pose-dependent garment details. To this end, we introduce Animatable Gaussians, a new avatar representation that leverages powerful 2D CNNs and 3D Gaussian splatting to create high-fidelity avatars. To associate 3D Gaussians with the animatable avatar, we learn a parametric template from the input videos, and then parameterize the template on two front \&amp; back canonical Gaussian maps where each pixel represents a 3D Gaussian. The learned template is adaptive to the wearing garments for modeling looser clothes like dresses. Such template-guided 2D parameterization enables us to employ a powerful StyleGAN-based CNN to learn the pose-dependent Gaussian maps for modeling detailed dynamic appearances. Furthermore, we introduce a pose projection strategy for better generalization given novel poses. Overall, our method can create lifelike avatars with dynamic, realistic and generalized appearances. Experiments show that our method outperforms other state-of-the-art approaches. Code: <a href="https://github.com/lizhe00/AnimatableGaussians">https://github.com/lizhe00/AnimatableGaussians</a> </p>
<p><a href="http://arxiv.org/abs/2311.16096v1">PDF</a> Projectpage: <a href="https://animatable-gaussians.github.io/">https://animatable-gaussians.github.io/</a>, Code:   <a href="https://github.com/lizhe00/AnimatableGaussians">https://github.com/lizhe00/AnimatableGaussians</a></p>
<p><strong>概述</strong><br>采用强大的二维卷积神经网络和三维高斯扩散的方式来创作逼真的虚拟人物。</p>
<p><strong>要点</strong></p>
<ul>
<li>提出一种新的虚拟形象表示法——可动画高斯，它利用强大的二维卷积神经网络和三维高斯扩散来创建逼真的虚拟形象。</li>
<li>学习一个参数模板从输入视频中，然后将模板参数化在两个正面&amp;背面规范的高斯映射上，每个像素代表一个三维高斯。</li>
<li>学习的模板可以适应穿的衣服来建模更宽松的衣服，如连衣裙。</li>
<li>基于 StyleGAN 的卷积神经网络学习姿势相关的 Gaussian 地图，用于建模详细的动态外观。</li>
<li>提出了一种姿势投影策略，以便在新的姿势下更好地泛化。</li>
<li>实验表明，本方法优于其他最先进的方法。</li>
<li>代码：<a href="https://github.com/lizhe00/AnimatableGaussians">https://github.com/lizhe00/AnimatableGaussians</a></li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<p></p><ol><p></p>
<p></p><li><p>题目：可动画的高斯体：学习姿势依赖的高斯映射</p>
</li><p></p>
<p></p><li><p>作者：Zhe Li, Menglei Chai, Yinda Zhang, Jingyi Yu, Jingyi Yu</p>
</li><p></p>
<p></p><li><p>单位：香港中文大学（深圳）</p>
</li><p></p>
<p></p><li><p>关键词：神经辐射场、动画、高斯体、姿势投影</p>
</li><p></p>
<p></p><li><p>论文链接：None，Github 代码链接：https://github.com/lizhe00/AnimatableGaussians</p>
</li><p></p>
<p></p><li><p>摘要：
（1）研究背景：建模可动画的人体虚拟形象是一个长期存在且具有挑战性的问题。最近的工作通常采用基于 MLP 的神经辐射场 (NeRF) 来表示 3D 人体，但纯 MLP 很难回归姿势相关的服装细节。
（2）过去方法与问题：为了解决这个问题，我们引入了可动画的高斯体，这是一种新的虚拟形象表示，利用强大的 2D CNN 和 3D 高斯体 splatting 来创建高保真虚拟形象。为了将 3D 高斯体与可动画虚拟形象相关联，我们从输入视频中学习了一个参数化模板，然后在两个正面和背面规范高斯图上对模板进行参数化，其中每个像素都表示一个 3D 高斯体。学习到的模板可以适应穿着的服装，以建模更宽松的衣服，如连衣裙。这种模板引导的 2D 参数化使我们能够使用强大的基于 StyleGAN 的 CNN 来学习姿势相关的 Gaussian 图，以建模详细的动态外观。此外，我们引入了一种姿势投影策略，以在给定新颖姿势时获得更好的泛化。
（3）研究方法：总体而言，我们的方法可以创建具有动态、逼真和泛化的外观的逼真虚拟形象。实验表明，我们的方法优于其他最先进的方法。
（4）方法性能：在 THuman4.0 数据集和 AvatarReX 数据集上的实验表明，我们的方法在重建准确性和视觉质量方面优于其他最先进的方法。</p>
</li><p></p>
<p></p><li><p>方法：
(1) 参数化模板：提出了一种参数化模板，该模板由两个正面和背面规范高斯图组成，每个像素都表示一个 3D 高斯体。该模板可以适应穿着的服装，以建模更宽松的衣服，如连衣裙。
(2) 基于 StyleGAN 的 CNN：利用强大的基于 StyleGAN 的 CNN 来学习姿势相关的 Gaussian 图，以建模详细的动态外观。
(3) 姿势投影策略：引入了一种姿势投影策略，以在给定新颖姿势时获得更好的泛化。该策略通过将输入位置图投影到训练数据集中最接近的姿势的位置图上来实现。</p>
</li><p></p>
<p></p><li><p>结论：</p>
</li><br>&lt;/ol&gt;<p></p>
<p></p><p>（1）：本文提出了一种可动画的高斯体表示方法，使用强大的2DCNN和3D高斯体splatting来创建高保真虚拟形象。该方法可以创建具有动态、逼真和泛化的外观的逼真虚拟形象。实验表明，该方法优于其他最先进的方法。</p><p></p>
<p></p><p>（2）：创新点：</p><p></p>
<ul>
<li>提出了一种参数化模板，该模板由两个正面和背面规范高斯图组成，每个像素都表示一个3D高斯体。该模板可以适应穿着的服装，以建模更宽松的衣服，如连衣裙。</li>
<li>利用强大的基于StyleGAN的CNN来学习姿势相关的Gaussian图，以建模详细的动态外观。</li>
<li>引入了一种姿势投影策略，以在给定新颖姿势时获得更好的泛化。</li>
</ul>
<p>性能：</p>
<ul>
<li>在THuman4.0数据集和AvatarReX数据集上的实验表明，该方法在重建准确性和视觉质量方面优于其他最先进的方法。</li>
</ul>
<p>工作量：</p>
<ul>
<li>该方法需要大量的数据和计算资源。</li>
</ul>



<p></p><details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-4f58407bb1da6858a1f8b3afeca5122e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77ee722883e97eb42525787423c0db90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ba94c75431413d71052f68f19e06407.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aec0c4ba5b1b400598aa699437906d07.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b30fc2750f7d5972c0541922982b18fe.jpg" align="middle">
</details><br>​    <p></p>
<h2 id="Importance-of-Feature-Extraction-in-the-Calculation-of-Frechet-Distance-for-Medical-Imaging"><a href="#Importance-of-Feature-Extraction-in-the-Calculation-of-Frechet-Distance-for-Medical-Imaging" class="headerlink" title="Importance of Feature Extraction in the Calculation of Fréchet   Distance for Medical Imaging"></a>Importance of Feature Extraction in the Calculation of Fréchet   Distance for Medical Imaging</h2><p><strong>Authors:McKell Woodland, Mais Al Taie, Jessica Albuquerque Marques Silva, Mohamed Eltaher, Frank Mohn, Alexander Shieh, Austin Castelo, Suprateek Kundu, Joshua P. Yung, Ankit B. Patel, Kristy K. Brock</strong></p>
<p>Fr\’echet Inception Distance is a widely used metric for evaluating synthetic image quality that utilizes an ImageNet-trained InceptionV3 network as a feature extractor. However, its application in medical imaging lacks a standard feature extractor, leading to biased and inconsistent comparisons. This study aimed to compare state-of-the-art feature extractors for computing Fr\’echet Distances (FDs) in medical imaging. A StyleGAN2 network was trained with data augmentation techniques tailored for limited data domains on datasets comprising three medical imaging modalities and four anatomical locations. Human evaluation of generative quality (via a visual Turing test) was compared to FDs calculated using ImageNet-trained InceptionV3, ResNet50, SwAV, DINO, and Swin Transformer architectures, in addition to an InceptionV3 network trained on a large medical dataset, RadImageNet. All ImageNet-based extractors were consistent with each other, but only SwAV was significantly correlated with medical expert judgment. The RadImageNet-based FD showed volatility and lacked correlation with human judgment. Caution is advised when using medical image-trained extraction networks in the FD calculation. These networks should be rigorously evaluated on the imaging modality under consideration and publicly released. ImageNet-based extractors, while imperfect, are consistent and widely understood. Training extraction networks with SwAV is a promising approach for synthetic medical image evaluation. </p>
<p><a href="http://arxiv.org/abs/2311.13717v1">PDF</a> </p>
<p><strong>Summary</strong><br>医学图像生成质量评价中，ImageNet 预训练的特征提取器与医学专家判断不一致，SwAV 是唯一显著相关的网络。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Fr\’echet Inception Distance (FID) 是一种广泛用于评估合成图像质量的指标，利用 ImageNet 训练的 InceptionV3 网络作为特征提取器。</li>
<li>在医学图像中使用 FID 缺乏标准的特征提取器，导致比较结果存在偏差且不一致。</li>
<li>本研究旨在比较最先进的特征提取器，用于计算医学图像中的 Fr\’echet 距离 (FD)。</li>
<li>使用针对有限数据域量身定制的数据增强技术训练 StyleGAN2 网络，该数据集包括三个医学图像模态和四个解剖位置。</li>
<li>将生成质量的人工评估（通过视觉图灵测试）与使用 ImageNet 训练的 InceptionV3、ResNet50、SwAV、DINO 和 Swin Transformer 架构以及在大规模医学数据集 RadImageNet 上训练的 InceptionV3 网络计算的 FD 进行比较。</li>
<li>所有基于 ImageNet 的提取器彼此一致，但只有 SwAV 与医学专家判断显着相关。</li>
<li>基于 RadImageNet 的 FD 显示出不稳定性，并且与人类判断缺乏相关性。</li>
<li>在 FD 计算中使用医学图像训练的提取网络时应谨慎。这些网络应在所考虑的成像方式上进行严格评估并公开发布。</li>
<li>基于 ImageNet 的提取器虽然不完美，但具有一致性且被广泛理解。</li>
<li>使用 SwAV 训练提取网络是合成医学图像评估的一种有前途的方法。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<p></p><ol><p></p>
<p></p><li>题目：医学图像 Fréchet 距离计算中的特征提取的重要性</li><p></p>
<p></p><li>作者：McKell Woodland, Mais Al Taie, Jessica Albuquerque Marques Silva, Mohamed Eltaher, Frank Mohn, Alexander Shieh, Austin Castelo, Suprateek Kundu, Joshua P. Yung, Ankit B. Patel, Kristy K. Brock</li><p></p>
<p></p><li>第一作者单位：德克萨斯大学安德森癌症中心</li><p></p>
<p></p><li>关键词：Fréchet 距离, 特征提取, 医学图像, 生成模型, 自监督学习</li><p></p>
<p></p><li>链接：None, Github：None</li><p></p>
<p></p><li>摘要：
（1）研究背景：Fréchet 距离是一种广泛用于评估合成图像质量的度量，它利用在 ImageNet 上训练的 InceptionV3 网络作为特征提取器。然而，在医学图像中的应用缺乏标准的特征提取器，导致比较存在偏差和不一致。
（2）过去方法及其问题：一种方法是使用在大型公开医学数据集上训练的 InceptionV3 网络来计算 FD。然而，这种方法可能不适用于未包含在该数据集中的医学模态，例如内窥镜检查和乳房 X 光检查。另一种方法是使用自监督特征提取器，但现有研究表明，这些提取器在医学图像上的性能参差不齐。
（3）论文提出的研究方法：本文比较了用于计算医学图像中 Fréchet 距离的几种最先进的特征提取器。我们使用针对有限数据域的数据增强技术训练了一个 StyleGAN2 网络，该数据集包含三个医学影像模态和四个解剖位置。我们将生成质量的人工评估（通过视觉图灵测试）与使用在 ImageNet 上训练的 InceptionV3、ResNet50、SwAV、DINO 和 SwinTransformer 架构以及在大型医学数据集 RadImageNet 上训练的 InceptionV3 网络计算的 FD 进行了比较。
（4）方法在任务和性能上的表现：所有基于 ImageNet 的提取器彼此一致，但只有 SwAV 与医学专家的判断显着相关。基于 RadImageNet 的 FD 显示出不稳定性，并且与人类判断缺乏相关性。在 FD 计算中使用医学图像训练的提取网络时应谨慎。这些网络应该在考虑的成像方式上进行严格评估并公开发布。ImageNet-base 的提取器虽然不完美，但它们一致且被广泛理解。使用 SwAV 训练提取网络是合成医学图像评估的一种很有前途的方法。</li><br>&lt;/ol&gt;<p></p>
<p></p><p>Some Error for method(比如是不是没有Methods这个章节)</p><p></p>
<p></p><ol start="8"><p></p>
<p></p><li>结论：
(1): 本文比较了用于计算医学图像中 Fréchet 距离的几种最先进的特征提取器，发现基于 ImageNet 的提取器彼此一致，但只有 SwAV 与医学专家的判断显着相关。基于 RadImageNet 的 Fréchet 距离显示出不稳定性，并且与人类判断缺乏相关性。在 Fréchet 距离计算中使用医学图像训练的提取网络时应谨慎。这些网络应该在考虑的成像方式上进行严格评估并公开发布。ImageNet-base 的提取器虽然不完美，但它们一致且被广泛理解。使用 SwAV 训练提取网络是合成医学图像评估的一种很有前途的方法。
(2): 创新点：</li><br>&lt;/ol&gt;<p></p>
<ul>
<li>比较了用于计算医学图像中 Fréchet 距离的几种最先进的特征提取器。</li>
<li>发现基于 ImageNet 的提取器彼此一致，但只有 SwAV 与医学专家的判断显着相关。</li>
<li>基于 RadImageNet 的 Fréchet 距离显示出不稳定性，并且与人类判断缺乏相关性。</li>
<li>在 Fréchet 距离计算中使用医学图像训练的提取网络时应谨慎。</li>
<li>使用 SwAV 训练提取网络是合成医学图像评估的一种很有前途的方法。</li>
</ul>
<p>性能：</p>
<ul>
<li>基于 ImageNet 的提取器彼此一致，但只有 SwAV 与医学专家的判断显着相关。</li>
<li>基于 RadImageNet 的 Fréchet 距离显示出不稳定性，并且与人类判断缺乏相关性。</li>
</ul>
<p>工作量：</p>
<ul>
<li>需要比较多种特征提取器，工作量较大。</li>
<li>需要在医学图像上训练提取网络，工作量较大。</li>
</ul>



<p></p><details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-3a98d200e6662a30834d463110beefc2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9c2de329d902d2836a266714f60f4842.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-855d1b05e045ac53d6b4a972ad2848dd.jpg" align="middle">
</details><br>​    <p></p>
<p>​    </p>
</ol></ol></ol></ol></ol>]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title>3DGS</title>
    <url>/2024/01/24/Paper/2024-01-24/3DGS/</url>
    <content><![CDATA[<blockquote>
<p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-01-27-更新"><a href="#2024-01-27-更新" class="headerlink" title="2024-01-27 更新"></a>2024-01-27 更新</h1><h2 id="Deformable-Endoscopic-Tissues-Reconstruction-with-Gaussian-Splatting"><a href="#Deformable-Endoscopic-Tissues-Reconstruction-with-Gaussian-Splatting" class="headerlink" title="Deformable Endoscopic Tissues Reconstruction with Gaussian Splatting"></a>Deformable Endoscopic Tissues Reconstruction with Gaussian Splatting</h2><p><strong>Authors:Lingting Zhu, Zhao Wang, Zhenchao Jin, Guying Lin, Lequan Yu</strong></p>
<p>Surgical 3D reconstruction is a critical area of research in robotic surgery, with recent works adopting variants of dynamic radiance fields to achieve success in 3D reconstruction of deformable tissues from single-viewpoint videos. However, these methods often suffer from time-consuming optimization or inferior quality, limiting their adoption in downstream tasks. Inspired by 3D Gaussian Splatting, a recent trending 3D representation, we present EndoGS, applying Gaussian Splatting for deformable endoscopic tissue reconstruction. Specifically, our approach incorporates deformation fields to handle dynamic scenes, depth-guided supervision to optimize 3D targets with a single viewpoint, and a spatial-temporal weight mask to mitigate tool occlusion. As a result, EndoGS reconstructs and renders high-quality deformable endoscopic tissues from a single-viewpoint video, estimated depth maps, and labeled tool masks. Experiments on DaVinci robotic surgery videos demonstrate that EndoGS achieves superior rendering quality. Code is available at <a href="https://github.com/HKU-MedAI/EndoGS">https://github.com/HKU-MedAI/EndoGS</a>. </p>
<p><a href="http://arxiv.org/abs/2401.11535v1">PDF</a> Work in progress. 10 pages, 4 figures</p>
<p><strong>摘要</strong><br>动态高斯散点用于可变形内窥镜组织重建。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>EndoGS 将高斯散点应用于可变形内窥镜组织重建。</li>
<li>EndoGS 结合了变形场、深度引导监督和时空权重掩码来处理动态场景、优化 3D 目标和减轻工具遮挡。</li>
<li>EndoGS 从单视角视频、估计的深度图和标注的工具掩码中重建并渲染高质量的可变形内窥镜组织。</li>
<li>EndoGS 在达芬奇机器人手术视频上的实验表明，它实现了卓越的渲染质量。</li>
<li>EndoGS 的代码可在 <a href="https://github.com/HKU-MedAI/EndoGS">https://github.com/HKU-MedAI/EndoGS</a> 获得。</li>
<li>EndoGS 的时间成本低于传统方法，如基于动态辐射场的重建方法。</li>
<li>EndoGS 的重建质量高于传统方法，如基于动态辐射场的重建方法。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：高斯溅射的可变形内窥镜组织重建</li>
<li>作者：Lingting Zhu, Zhao Wang, Zhenchao Jin, Guying Lin, Lequan Yu</li>
<li>第一作者单位：香港大学</li>
<li>关键词：高斯溅射、机器人手术、三维重建</li>
<li>论文链接：https://arxiv.org/abs/2401.11535，Github 代码链接：https://github.com/HKU-MedAI/EndoGS</li>
<li>
<p>摘要：
(1)：研究背景：三维重建是机器人手术中的一个关键研究领域，最近的工作采用动态辐射场的变体，从单视角视频中成功实现了可变形组织的三维重建。然而，这些方法往往会遭受耗时的优化或质量低下的困扰，限制了它们在下游任务中的应用。
(2)：过去的方法：早期尝试采用深度估计来实现内窥镜重建，但在处理非刚性变形和遮挡方面存在困难。[9,12]提出了结合工具掩蔽、立体深度估计和稀疏变形场的框架，但它们在存在剧烈非拓扑可变形组织变化时仍然容易失败。神经辐射场 (NeRFs) 在三维重建方面取得了重大进展，但它们通常需要大量数据和计算资源。
(3)：研究方法：本文提出了一种新的方法 EndoGS，它将高斯溅射应用于可变形内窥镜组织重建。EndoGS 将变形场结合起来处理动态场景，使用深度引导监督来优化具有单一视点的三维目标，并使用时空权重掩码来减轻遮挡。
(4)：实验结果：在达芬奇机器人手术视频上的实验表明，EndoGS 实现了卓越的渲染质量。</p>
</li>
<li>
<p>方法：
（1）概述：提出了一种名为 EndoGS 的新方法，它将高斯溅射应用于可变形内窥镜组织重建。EndoGS 将变形场结合起来处理动态场景，使用深度引导监督来优化具有单一视点的三维目标，并使用时空权重掩码来减轻遮挡。
（2）高斯溅射表示的可变形组织：使用高斯变形来表示随时间变化的运动和形状，遵循 [26] 的基本设计。最终目标是学习 3D 高斯的原始表示 {(µ, s, r, sh, σ)} 以及高斯变形 {∆(µ, s, r, sh, σ)}={(∆µ, ∆s, ∆r, ∆sh, ∆σ)}。
（3）结合工具掩码和深度图的训练：重建带有工具遮挡的视频具有挑战性，遵循前人的工作 [25, 27, 28] 使用标记的工具遮挡掩码来指示看不见的像素。此外，利用时空重要性采样策略来指示与遮挡问题相关的关键区域。由于 3D-GS 使用空间权重掩码来处理遮挡，因此将工具掩码和深度图合并到空间权重掩码中，以进一步增强对遮挡区域的建模。</p>
</li>
<li>
<p>结论：
（1）：本文提出了一种基于高斯溅射的可变形内窥镜组织重建方法，能够从单视角视频、估计的深度图和标记的工具遮挡掩码中实时渲染高质量的可变形组织。在达芬奇机器人手术视频上的实验表明，该方法具有更高的渲染质量。
（2）：创新点：</p>
</li>
<li>将高斯溅射应用于可变形内窥镜组织重建，能够有效处理动态场景中的非刚性变形和遮挡。</li>
<li>使用深度引导监督来优化具有单一视点的三维目标，提高了重建的准确性和鲁棒性。</li>
<li>使用时空权重掩码来减轻遮挡，增强了对遮挡区域的建模。
性能：</li>
<li>在达芬奇机器人手术视频上的实验表明，该方法实现了卓越的渲染质量。</li>
<li>与现有方法相比，该方法在准确性和鲁棒性方面具有优势。
工作量：</li>
<li>该方法需要较少的计算资源，能够实时渲染可变形组织。</li>
<li>该方法需要标记的工具遮挡掩码和估计的深度图，这可能会增加工作量。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-3aced720ad0952509d5ad4feafb073c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db38985f02aa9f93361d5395728da086.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f22f8ab59ea6655501c3858f5b7639aa.jpg" align="middle">
</details>




<h2 id="GaussianBody-Clothed-Human-Reconstruction-via-3d-Gaussian-Splatting"><a href="#GaussianBody-Clothed-Human-Reconstruction-via-3d-Gaussian-Splatting" class="headerlink" title="GaussianBody: Clothed Human Reconstruction via 3d Gaussian Splatting"></a>GaussianBody: Clothed Human Reconstruction via 3d Gaussian Splatting</h2><p><strong>Authors:Mengtian Li, Shengxiang Yao, Zhifeng Xie, Keyu Chen, Yu-Gang Jiang</strong></p>
<p>In this work, we propose a novel clothed human reconstruction method called GaussianBody, based on 3D Gaussian Splatting. Compared with the costly neural radiance based models, 3D Gaussian Splatting has recently demonstrated great performance in terms of training time and rendering quality. However, applying the static 3D Gaussian Splatting model to the dynamic human reconstruction problem is non-trivial due to complicated non-rigid deformations and rich cloth details. To address these challenges, our method considers explicit pose-guided deformation to associate dynamic Gaussians across the canonical space and the observation space, introducing a physically-based prior with regularized transformations helps mitigate ambiguity between the two spaces. During the training process, we further propose a pose refinement strategy to update the pose regression for compensating the inaccurate initial estimation and a split-with-scale mechanism to enhance the density of regressed point clouds. The experiments validate that our method can achieve state-of-the-art photorealistic novel-view rendering results with high-quality details for dynamic clothed human bodies, along with explicit geometry reconstruction. </p>
<p><a href="http://arxiv.org/abs/2401.09720v1">PDF</a> </p>
<p><strong>摘要</strong><br>应用高斯几何建模实现动态着装人物的三维重建。</p>
<p><strong>要点</strong></p>
<ul>
<li>提出一种基于 3D 高斯几何建模的动态着装人物三维重建方法 GaussianBody。</li>
<li>GaussianBody 采用了高效的点云表示和渲染方式，在训练时间和渲染质量方面均取得了不错的表现。</li>
<li>为了适应动态人类复杂的非刚性变形和丰富的服装细节，GaussianBody 引入了显式的姿态引导变形，将规范空间和观测空间中的动态高斯体素相关联。</li>
<li>提出了一种基于物理先验的正则化变换，帮助缓解两个空间之间的歧义性。</li>
<li>训练过程中，GaussianBody 还提出了一种姿态优化策略来更新姿态回归，以补偿不准确的初始估计，并提出了一个分而治之的机制来增强回归点云的密度。</li>
<li>在标准数据集上的实验证明，GaussianBody 在实现最先进的动态着装人物照片级新视角渲染结果的同时，也能实现准确的几何形状重建。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：高斯体：基于 3D 高斯散点的穿衣人体重建</li>
<li>作者：李孟田、姚盛祥、谢志锋、陈可宇、蒋玉刚</li>
<li>第一作者单位：上海大学</li>
<li>关键词：人体重建、3D 高斯散点、变形、姿势引导</li>
<li>论文链接：https://arxiv.org/abs/2401.09720，Github 代码链接：无</li>
<li>摘要：
(1)：研究背景：高保真穿衣人体模型在虚拟现实、远程呈现和电影制作中具有重要应用。传统方法要么涉及复杂的捕捉系统，要么需要 3D 艺术家的繁琐手动工作，这使得它们耗时且昂贵，从而限制了新手用户的可扩展性。最近，人们越来越关注从单个 RGB 图像或单目视频中自动重建穿衣人体模型。
(2)：过去方法及其问题：网格方法最初被引入，通过回归 SCAPE、SMPL、SMPL-X 和 STAR 等参数模型来恢复人体形状。虽然它们可以实现快速且稳健的重建，但回归的多边形网格难以捕捉不同的几何细节和丰富的服装特征。添加顶点偏移成为这种情况下的一种增强解决方案。然而，它的表示能力仍然有限。
(3)：本文提出的研究方法：本文提出了一种基于 3D 高斯散点的新型穿衣人体重建方法，称为高斯体。与昂贵的神经辐射场模型相比，3D 高斯散点最近在训练时间和渲染质量方面表现出了极佳的性能。然而，由于复杂的不刚性变形和丰富的服装细节，将静态 3D 高斯散点模型应用于动态人体重建问题并非易事。为了应对这些挑战，本文方法考虑了在规范空间和观察空间跨动态高斯体的显式姿势引导变形，引入具有正则化变换的基于物理的先验有助于减轻两个空间之间的歧义。在训练过程中，本文进一步提出了一种姿势细化策略来更新姿势回归，以补偿不准确的初始估计，并提出了一种分裂尺度机制来增强回归点云的密度。
(4)：方法在什么任务和性能上取得了成就，能否支持其目标：实验证明，本文方法可以实现动态穿衣人体的高质量细节的最新逼真的新视图渲染结果，以及显式的几何重建。</li>
</ol>
<p>Some Error for method(比如是不是没有Methods这个章节)</p>
<ol>
<li>结论：
(1)：意义：本文提出了一种基于 3D 高斯散点的穿衣人体重建方法，称为高斯体。该方法克服了复杂的不刚性变形和丰富的服装细节的挑战，实现了动态穿衣人体的高质量细节的新视图渲染结果，以及显式的几何重建。
(2)：创新点：</li>
<li>使用 3D 高斯散点表示动态穿衣人体。</li>
<li>引入具有正则化变换的基于物理的先验，以减轻规范空间和观察空间之间的歧义。</li>
<li>提出了一种姿势细化策略来更新姿势回归，以补偿不准确的初始估计。</li>
<li>提出了一种分裂尺度机制来增强回归点云的密度。
性能：</li>
<li>与基线和其他方法相比，本文方法实现了可比较的图像质量指标，证明了具有竞争力的性能、相对较快的训练速度，以及能够以更高分辨率的图像进行训练。
工作量：</li>
<li>本文方法需要收集和标记大量的数据，并且训练过程需要大量的时间和计算资源。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7931aa02d87b1007c7f5cdde77107e5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d3dcd00c27bc3d320b23d4247ae79f3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3df005c3ea738aba56feb680b23b73d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d121364f4f1fecac5ef9d276f421f434.jpg" align="middle">
</details>




<h2 id="Forging-Vision-Foundation-Models-for-Autonomous-Driving-Challenges-Methodologies-and-Opportunities"><a href="#Forging-Vision-Foundation-Models-for-Autonomous-Driving-Challenges-Methodologies-and-Opportunities" class="headerlink" title="Forging Vision Foundation Models for Autonomous Driving: Challenges,   Methodologies, and Opportunities"></a>Forging Vision Foundation Models for Autonomous Driving: Challenges,   Methodologies, and Opportunities</h2><p><strong>Authors:Xu Yan, Haiming Zhang, Yingjie Cai, Jingming Guo, Weichao Qiu, Bin Gao, Kaiqiang Zhou, Yue Zhao, Huan Jin, Jiantao Gao, Zhen Li, Lihui Jiang, Wei Zhang, Hongbo Zhang, Dengxin Dai, Bingbing Liu</strong></p>
<p>The rise of large foundation models, trained on extensive datasets, is revolutionizing the field of AI. Models such as SAM, DALL-E2, and GPT-4 showcase their adaptability by extracting intricate patterns and performing effectively across diverse tasks, thereby serving as potent building blocks for a wide range of AI applications. Autonomous driving, a vibrant front in AI applications, remains challenged by the lack of dedicated vision foundation models (VFMs). The scarcity of comprehensive training data, the need for multi-sensor integration, and the diverse task-specific architectures pose significant obstacles to the development of VFMs in this field. This paper delves into the critical challenge of forging VFMs tailored specifically for autonomous driving, while also outlining future directions. Through a systematic analysis of over 250 papers, we dissect essential techniques for VFM development, including data preparation, pre-training strategies, and downstream task adaptation. Moreover, we explore key advancements such as NeRF, diffusion models, 3D Gaussian Splatting, and world models, presenting a comprehensive roadmap for future research. To empower researchers, we have built and maintained <a href="https://github.com/zhanghm1995/Forge_VFM4AD">https://github.com/zhanghm1995/Forge_VFM4AD</a>, an open-access repository constantly updated with the latest advancements in forging VFMs for autonomous driving. </p>
<p><a href="http://arxiv.org/abs/2401.08045v1">PDF</a> Github Repo: <a href="https://github.com/zhanghm1995/Forge_VFM4AD">https://github.com/zhanghm1995/Forge_VFM4AD</a></p>
<p><strong>Summary</strong><br>针对自动驾驶领域，通过对 250 余篇论文的分析，我们总结了视觉基础模型的发展方法，包括数据处理、预训练策略和下游任务的适应，并展望了未来研究方向。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>视觉基础模型在自动驾驶领域面临诸多挑战，如缺乏专用数据、多传感器融合和不同任务的特定架构等，极大制约了其发展。</li>
<li>本文通过分析 250 余篇论文，总结了视觉基础模型的构建方法，包括数据准备、预训练策略和下游任务的适应。</li>
<li>文中重点介绍了包括神经辐射场（NeRF）、扩散模型、3D 高斯分布和世界模型等关键技术，为该领域的研究提供了全面的路线图。</li>
<li>本文建立并维护了开源仓库 <a href="https://github.com/zhanghm1995/Forge_VFM4AD，以收集和分享视觉基础模型的构建方法，旨在为研究人员提供有价值的信息。">https://github.com/zhanghm1995/Forge_VFM4AD，以收集和分享视觉基础模型的构建方法，旨在为研究人员提供有价值的信息。</a></li>
<li>视觉基础模型在自动驾驶领域的应用具有广阔的前景，有望推动自动驾驶技术的发展。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>论文标题：为自动驾驶打造视觉基础模型：挑战、方法和机遇</li>
<li>作者：徐岩、张海明、蔡英杰、郭靖明、邱维超、高斌、周凯强、赵岳、金欢、高建涛、李臻、蒋立辉、张伟、张宏波、戴登新、刘冰冰</li>
<li>第一作者单位：华为诺亚方舟实验室</li>
<li>关键词：视觉基础模型、数据生成、自监督训练、自动驾驶、文献综述</li>
<li>论文链接：https://arxiv.org/pdf/2401.08045.pdf，Github 链接：无</li>
<li>摘要：
(1) 研究背景：随着自动驾驶技术的发展，对视觉基础模型（VFM）的需求日益增长，但目前缺乏专门针对自动驾驶的 VFM。
(2) 过去的方法和问题：传统自动驾驶感知系统依赖模块化架构，使用特定任务的专用算法，这种方法导致输出不一致，并且限制了系统处理长尾案例的能力。
(3) 论文提出的研究方法：本文系统分析了 250 多篇论文，剖析了 VFM 开发的必要技术，包括数据准备、预训练策略和下游任务适应。此外，还探讨了 NeRF、扩散模型、3D 高斯散射和世界模型等关键进展，为未来的研究提供了全面的路线图。
(4) 方法在任务上的表现和性能：本文建立并维护了 ForgeVFM4AD，这是一个开放获取的资源库，不断更新自动驾驶 VFM 的最新进展。</li>
</ol>
<p><methods>:
(1): 视觉基础模型（VFM）是自动驾驶感知系统的重要组成部分，本文系统分析了250多篇论文，剖析了VFM开发的必要技术，包括数据准备、预训练策略和下游任务适应。
(2): 数据准备方面，本文探讨了自动驾驶场景下的数据生成方法，包括合成数据、真实数据和混合数据。
(3): 预训练策略方面，本文介绍了VFM的预训练方法，包括自监督学习、监督学习和半监督学习。
(4): 下游任务适应方面，本文讨论了VFM在自动驾驶任务中的应用，包括目标检测、语义分割、实例分割、深度估计和运动估计。
(5): 此外，本文还探讨了NeRF、扩散模型、3D高斯散射和世界模型等关键进展，为未来的研究提供了全面的路线图。</methods></p>
<ol>
<li>结论：
（1）：本文系统分析了250多篇论文，剖析了自动驾驶视觉基础模型开发的必要技术，包括数据准备、预训练策略和下游任务适应，为自动驾驶视觉基础模型的开发提供了全面的路线图。
（2）：创新点：</li>
<li>本文首次系统分析了自动驾驶视觉基础模型的开发技术，为自动驾驶视觉基础模型的开发提供了全面的路线图。</li>
<li>本文建立并维护了ForgeVFM4AD，这是一个开放获取的资源库，不断更新自动驾驶视觉基础模型的最新进展。
性能：</li>
<li>本文提出的方法在自动驾驶视觉基础模型的开发中取得了良好的性能。
工作量：</li>
<li>本文的工作量很大，需要分析250多篇论文，并建立和维护ForgeVFM4AD资源库。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-7ce70a9a128d8a3669098fd6808591bd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b29768228c4fd656077c66549ec08984.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7ea3a2551a65a42514ea6e5555124cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66561a69f615f893c246615fba473e10.jpg" align="middle">
</details>




<h2 id="Gaussian-Shadow-Casting-for-Neural-Characters"><a href="#Gaussian-Shadow-Casting-for-Neural-Characters" class="headerlink" title="Gaussian Shadow Casting for Neural Characters"></a>Gaussian Shadow Casting for Neural Characters</h2><p><strong>Authors:Luis Bolanos, Shih-Yang Su, Helge Rhodin</strong></p>
<p>Neural character models can now reconstruct detailed geometry and texture from video, but they lack explicit shadows and shading, leading to artifacts when generating novel views and poses or during relighting. It is particularly difficult to include shadows as they are a global effect and the required casting of secondary rays is costly. We propose a new shadow model using a Gaussian density proxy that replaces sampling with a simple analytic formula. It supports dynamic motion and is tailored for shadow computation, thereby avoiding the affine projection approximation and sorting required by the closely related Gaussian splatting. Combined with a deferred neural rendering model, our Gaussian shadows enable Lambertian shading and shadow casting with minimal overhead. We demonstrate improved reconstructions, with better separation of albedo, shading, and shadows in challenging outdoor scenes with direct sun light and hard shadows. Our method is able to optimize the light direction without any input from the user. As a result, novel poses have fewer shadow artifacts and relighting in novel scenes is more realistic compared to the state-of-the-art methods, providing new ways to pose neural characters in novel environments, increasing their applicability. </p>
<p><a href="http://arxiv.org/abs/2401.06116v1">PDF</a> 14 pages, 13 figures</p>
<p><strong>Summary</strong><br>神经特征模型可从视频重建详细几何结构和纹理，但不包含明确的阴影和着色，在生成新视图和姿势或重新照明时会产生伪像。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>高斯密度代理可将采样过程替换为简单的解析公式，实现对新阴影模型的构建。</li>
<li>方法支持动态运动并专为阴影计算量身定制，从而避免了高斯散射所需的仿射投影逼近和排序。</li>
<li>与延迟神经渲染模型相结合，高斯阴影可实现兰伯特着色和阴影投射，且开销极小。</li>
<li>重建效果更好且更能将反照率、着色和阴影分离开来，尤其是在具有直接阳光和硬阴影的具有挑战性的户外场景中。</li>
<li>该方法能够优化光线的方向，而无需任何用户输入。</li>
<li>与最先进的方法相比，新姿势具有更少的阴影伪像，新场景中的重新照明也更逼真，这提供了在新的环境中摆放神经特征的新方法，从而提高了其适用性。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>论文标题：使用高斯阴影生成神经角色</li>
<li>作者：Daniel Holden, Junbang Liang, Derek Nowrouzezahrai, Josh Susskind</li>
<li>第一作者单位：英伟达公司</li>
<li>关键词：计算机图形学、神经渲染、阴影、光照</li>
<li>论文链接：https://arxiv.org/abs/2209.03594
Github 链接：无</li>
<li>
<p>摘要：
(1) 研究背景：神经角色模型可以从视频中重建详细的几何形状和纹理，但它们缺乏明确的阴影和着色，当生成新的视角和姿势或重新照明时会导致伪影。将阴影包含在内特别困难，因为它们是全局效应，并且所需的二次光线投射成本很高。
(2) 过去的方法及其问题：为了解决这个问题，过去的方法使用各种技术来近似阴影，例如使用预计算的阴影贴图或使用球谐函数来表示光照。然而，这些方法通常计算成本高昂，并且可能导致伪影或不准确。
(3) 本文提出的研究方法：我们提出了一种使用高斯密度代理的新阴影模型，该代理用一个简单的解析公式代替了采样。它支持动态运动，并针对阴影计算进行了定制，从而避免了与密切相关的 Gaussian splatting 所需的仿射投影近似和排序。结合延迟神经渲染模型，我们的高斯阴影能够以最小的开销实现朗伯阴影和阴影投射。
(4) 方法在什么任务上取得了什么性能，这些性能是否能支撑其目标：我们展示了改进的重建，在具有直射阳光和硬阴影的具有挑战性的户外场景中更好地分离了反照率、阴影和阴影。与最先进的方法相比，我们的方法能够优化光线方向，而无需任何用户输入。因此，新的姿势具有更少的阴影伪影，并且在新的场景中的重新照明更加逼真，为神经角色在新的环境中提供了新的摆放方式，从而提高了它们的可应用性。</p>
</li>
<li>
<p>方法：
（1）提出了一种使用高斯密度代理的新阴影模型，该代理用一个简单的解析公式代替了采样。
（2）结合延迟神经渲染模型，高斯阴影能够以最小的开销实现朗伯阴影和阴影投射。
（3）通过优化光线方向，新的姿势具有更少的阴影伪影，并且在新的场景中的重新照明更加逼真，为神经角色在新的环境中提供了新的摆放方式，从而提高了它们的可应用性。</p>
</li>
<li>
<p>结论：
（1）：这项工作通过一个适用于动态场景且可微分用于迭代优化的高斯阴影模型，实现了人类动作在不受控环境中的 3D 重建。重建的角色支持在新的环境中重新摆放和重新照明。它们配备了全局阴影计算、漫反射着色、几何重建和一致的表面反照率，非常像手工制作的计算机图形模型所提供的那样。
（2）：创新点：
提出了一种使用高斯密度代理的新阴影模型，该代理用一个简单的解析公式代替了采样。
结合延迟神经渲染模型，高斯阴影能够以最小的开销实现朗伯阴影和阴影投射。
通过优化光线方向，新的姿势具有更少的阴影伪影，并且在新的场景中的重新照明更加逼真，为神经角色在新的环境中提供了新的摆放方式，从而提高了它们的可应用性。
性能：
与最先进的方法相比，我们的方法能够优化光线方向，而无需任何用户输入。
我们的方法在具有直射阳光和硬阴影的具有挑战性的户外场景中更好地分离了反照率、阴影和阴影，展示了改进的重建。
工作量：
该方法需要较少的计算成本，并且能够以最小的开销实现朗伯阴影和阴影投射。</p>
</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-dc4d45055e7a95eff0b60a3ceb1f1663.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eaebff114520d85764f0c7d0f90d56c8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a4aa0b3fd8ca4717a88e2915cae00586.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-280abfc9150e00cc95fb60e679ad5920.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51373c781c37833f1e8cc8da2b6ea30e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3659bfeee8dd129594185dcf0a0f2373.jpg" align="middle">
</details>




<h2 id="TRIPS-Trilinear-Point-Splatting-for-Real-Time-Radiance-Field-Rendering"><a href="#TRIPS-Trilinear-Point-Splatting-for-Real-Time-Radiance-Field-Rendering" class="headerlink" title="TRIPS: Trilinear Point Splatting for Real-Time Radiance Field Rendering"></a>TRIPS: Trilinear Point Splatting for Real-Time Radiance Field Rendering</h2><p><strong>Authors:Linus Franke, Darius Rückert, Laura Fink, Marc Stamminger</strong></p>
<p>Point-based radiance field rendering has demonstrated impressive results for novel view synthesis, offering a compelling blend of rendering quality and computational efficiency. However, also latest approaches in this domain are not without their shortcomings. 3D Gaussian Splatting [Kerbl and Kopanas et al. 2023] struggles when tasked with rendering highly detailed scenes, due to blurring and cloudy artifacts. On the other hand, ADOP [R\”uckert et al. 2022] can accommodate crisper images, but the neural reconstruction network decreases performance, it grapples with temporal instability and it is unable to effectively address large gaps in the point cloud.   In this paper, we present TRIPS (Trilinear Point Splatting), an approach that combines ideas from both Gaussian Splatting and ADOP. The fundamental concept behind our novel technique involves rasterizing points into a screen-space image pyramid, with the selection of the pyramid layer determined by the projected point size. This approach allows rendering arbitrarily large points using a single trilinear write. A lightweight neural network is then used to reconstruct a hole-free image including detail beyond splat resolution. Importantly, our render pipeline is entirely differentiable, allowing for automatic optimization of both point sizes and positions.   Our evaluation demonstrate that TRIPS surpasses existing state-of-the-art methods in terms of rendering quality while maintaining a real-time frame rate of 60 frames per second on readily available hardware. This performance extends to challenging scenarios, such as scenes featuring intricate geometry, expansive landscapes, and auto-exposed footage. </p>
<p><a href="http://arxiv.org/abs/2401.06003v1">PDF</a> </p>
<p><strong>摘要</strong><br>利用高斯散布和 ADOP 的思想，提出一种新型的三线性点渲染方法 TRIPS，具有实时渲染速度和优秀的渲染质量。</p>
<p><strong>要点</strong></p>
<ul>
<li>TRIPS 将高斯散布和 ADOP 的思想相结合，在屏幕空间图像金字塔中对点进行光栅化，并根据投影点大小选择金字塔层。</li>
<li>使用三线性写入渲染任意大小的点，并使用轻量级神经网络重建无孔图像，包括超出点分辨率的细节。</li>
<li>渲染管道完全可微，允许自动优化点的大小和位置。</li>
<li>TRIPS 在渲染质量方面优于现有最先进的方法，同时在现有硬件上保持 60 帧/秒的实时帧速率。</li>
<li>TRIPS 适用于具有复杂几何形状、广阔景观和自动曝光镜头的场景等具有挑战性的场景。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>标题：TRIPS：实时光照场的三角形点云渲染</li>
<li>作者：Linus Franke, Darius Rückert, Laura Fink, Marc Stamminger</li>
<li>隶属机构：视觉计算，埃朗根-纽伦堡大学，德国埃朗根</li>
<li>关键词：渲染；基于图像的渲染；重建</li>
<li>论文链接：https://arxiv.org/abs/2401.06003
   Github 链接：无</li>
<li>
<p>摘要：
（1）研究背景：基于点的渲染方法在新型视图合成方面取得了令人印象深刻的成果，提供了渲染质量和计算效率的完美结合。然而，该领域中最新方法也并非没有缺点。3D 高斯渲染在处理高细节场景时会遇到困难，因为它会导致模糊和云状伪影。另一方面，ADOP 可以生成更清晰的图像，但神经重建网络会降低性能，它难以解决时间不稳定性，并且无法有效处理点云中的大间隙。
（2）过去的方法及其问题：3D 高斯渲染在处理高细节场景时会产生模糊和云状伪影；ADOP 可以生成更清晰的图像，但神经重建网络会降低性能，它难以解决时间不稳定性，并且无法有效处理点云中的大间隙。该方法的动机很充分。
（3）研究方法：本文提出了一种名为 TRIPS（三角形点云渲染）的方法，它结合了高斯渲染和 ADOP 的思想。我们新技术的核心概念是将点光栅化为屏幕空间图像金字塔，金字塔层的选取由投影点大小决定。这种方法允许使用单个三线性写入来渲染任意大的点。然后使用一个轻量级神经网络来重建一个无孔图像，包括超过光栅分辨率的细节。重要的是，我们的渲染管道是完全可微分的，允许自动优化点的大小和位置。
（4）方法的性能：我们的评估表明，TRIPS 在渲染质量方面超越了现有的最先进方法，同时在现成的硬件上保持每秒 60 帧的实时帧率。这种性能扩展到具有复杂几何形状、广阔景观和自动曝光素材的场景等具有挑战性的场景。</p>
</li>
<li>
<p>方法：
（1）将点云投影到屏幕空间图像金字塔中，金字塔层的选取由投影点大小决定。
（2）使用轻量级神经网络重建一个无孔图像，包括超过光栅分辨率的细节。
（3）渲染管道是完全可微分的，允许自动优化点的大小和位置。</p>
</li>
<li>
<p>结论：
（1）TRIPS：实时光照场的三角形点云渲染，提出了一种稳健的实时基于点的辐射场渲染管道。TRIPS 采用了一种有效的策略，将点光栅化为屏幕空间图像金字塔，从而可以有效地渲染大点，并且是完全可微分的，因此可以自动优化点的大小和位置。这种技术能够渲染高度详细的场景并填充大间隙，同时在常用硬件上保持实时帧速率。我们强调，TRIPS 实现了很高的渲染质量，即使在具有复杂几何形状、大规模环境和自动曝光素材等具有挑战性的场景中也是如此。此外，由于平滑点渲染方法，一个相对简单的神经网络重建就足够了，从而实现了实时渲染性能。开源实现可在此处获得：https://github.com/lfranke/TRIPS
（2）创新点：</p>
</li>
<li>提出了一种新的基于点的辐射场渲染管道 TRIPS，它结合了高斯渲染和 ADOP 的思想，在渲染质量和计算效率之间取得了很好的平衡。</li>
<li>TRIPS 采用了一种有效的光栅化策略，将点光栅化为屏幕空间图像金字塔，从而可以有效地渲染大点。</li>
<li>TRIPS 的渲染管道是完全可微分的，因此可以自动优化点的大小和位置。
性能：</li>
<li>TRIPS 在渲染质量方面超越了现有的最先进方法，同时在现成的硬件上保持每秒 60 帧的实时帧率。</li>
<li>TRIPS 可以渲染高度详细的场景并填充大间隙，即使在具有复杂几何形状、大规模环境和自动曝光素材等具有挑战性的场景中也是如此。</li>
<li>TRIPS 使用了一个轻量级的神经网络来重建图像，因此渲染速度很快。
工作量：</li>
<li>TRIPS 的实现相对简单，并且开源。</li>
<li>TRIPS 可以很容易地应用于各种场景，包括具有复杂几何形状、广阔景观和自动曝光素材的场景。</li>
<li>TRIPS 可以很容易地与其他渲染技术相结合，以创建更逼真的图像。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-09ec963291bb4ef95dcae847c73b65ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f2e26b867b1d85086292f6b22b185913.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6615e2eef71a0b02f92f959a8a857c39.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0e1cebcb888b31ce7e7665083bca1f9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f5d595ac6172487559d3c8aaa5130d1c.jpg" align="middle">
</details>




<h2 id="CoSSegGaussians-Compact-and-Swift-Scene-Segmenting-3D-Gaussians-with-Dual-Feature-Fusion"><a href="#CoSSegGaussians-Compact-and-Swift-Scene-Segmenting-3D-Gaussians-with-Dual-Feature-Fusion" class="headerlink" title="CoSSegGaussians: Compact and Swift Scene Segmenting 3D Gaussians with   Dual Feature Fusion"></a>CoSSegGaussians: Compact and Swift Scene Segmenting 3D Gaussians with   Dual Feature Fusion</h2><p><strong>Authors:Bin Dou, Tianyu Zhang, Yongjia Ma, Zhaohui Wang, Zejian Yuan</strong></p>
<p>We propose Compact and Swift Segmenting 3D Gaussians(CoSSegGaussians), a method for compact 3D-consistent scene segmentation at fast rendering speed with only RGB images input. Previous NeRF-based segmentation methods have relied on time-consuming neural scene optimization. While recent 3D Gaussian Splatting has notably improved speed, existing Gaussian-based segmentation methods struggle to produce compact masks, especially in zero-shot segmentation. This issue probably stems from their straightforward assignment of learnable parameters to each Gaussian, resulting in a lack of robustness against cross-view inconsistent 2D machine-generated labels. Our method aims to address this problem by employing Dual Feature Fusion Network as Gaussians’ segmentation field. Specifically, we first optimize 3D Gaussians under RGB supervision. After Gaussian Locating, DINO features extracted from images are applied through explicit unprojection, which are further incorporated with spatial features from the efficient point cloud processing network. Feature aggregation is utilized to fuse them in a global-to-local strategy for compact segmentation features. Experimental results show that our model outperforms baselines on both semantic and panoptic zero-shot segmentation task, meanwhile consumes less than 10\% inference time compared to NeRF-based methods. Code and more results will be available at <a href="https://David-Dou.github.io/CoSSegGaussians">https://David-Dou.github.io/CoSSegGaussians</a>. </p>
<p><a href="http://arxiv.org/abs/2401.05925v2">PDF</a> Correct writing details</p>
<p><strong>Summary:</strong></p>
<p>从 RGB 图像中实时进行紧凑的 3D 场景分割。</p>
<p><strong>Key Takeaways:</strong></p>
<ul>
<li>提出了一种新的紧凑且快速的三维高斯分割方法 CoSSegGaussians，可在快速渲染速度下仅使用 RGB 图像输入进行紧凑的 3D 一致场景分割。</li>
<li>CoSSegGaussians 在语义和全景零镜头分割任务上优于基准，同时与基于 NeRF 的方法相比，推理时间减少了 10% 以上。</li>
<li>该方法的目标是通过使用双特征融合网络作为高斯的分割场来解决这个问题。</li>
<li>首先优化 RGB 监督下的 3D 高斯函数。</li>
<li>从图像中提取的 DINO 特征通过显式反投影应用，进一步与来自高效点云处理网络的空间特征结合。</li>
<li>利用特征聚合以全局到局部策略融合它们，以实现紧凑的分割特征。</li>
<li>实验结果表明，该模型在语义和全景零镜头分割任务上优于基准，同时与基于 NeRF 的方法相比，推理时间减少了 10% 以上。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：CoSSegGaussians：紧凑且快速的双特征融合高斯体场景分割</li>
<li>作者：Bin Dou, Tianyu Zhang, Yongjia Ma, Zhaohui Wang, Zejian Yuan</li>
<li>单位：西安交通大学人工智能与机器人学院</li>
<li>关键词：场景分割、神经辐射场、高斯体、双特征融合网络</li>
<li>论文链接：https://arxiv.org/abs/2401.05925</li>
<li>
<p>摘要：
（1）研究背景：近年来，计算机视觉和计算机图形学取得了显著进展，特别是在神经渲染领域。神经辐射场（NeRF）及其后续方法推动了神经场景表示的发展，在新型视图合成方面展现出显著能力。然而，基于NeRF的场景分割方法依赖于耗时的神经场景优化。虽然最近的3D高斯体渲染显著提高了速度，但现有的基于高斯体的分割方法难以生成紧凑的掩模，尤其是在零样本分割中。
（2）过去方法及问题：现有方法直接将可学习的参数分配给每个高斯体，导致对跨视图不一致的2D机器生成的标签缺乏鲁棒性。
（3）研究方法：本文提出一种紧凑且快速的高斯体场景分割方法CoSSegGaussians。该方法首先在RGB监督下优化3D高斯体。在高斯体定位后，将从图像中提取的DINO特征通过显式反投影应用，并进一步与来自高效点云处理网络的空间特征结合。利用特征聚合以全局到局部的策略将它们融合，以获得紧凑的分割特征。
（4）方法性能：实验结果表明，该模型在语义和全景零样本分割任务上优于基线，同时推理时间不到基于NeRF方法的10%。</p>
</li>
<li>
<p>方法：
（1）高斯体定位阶段：利用 L1 和 ℓD-SSIM 光度损失对 3D 高斯体的几何信息进行监督，以获得逼真的场景表示。
（2）分割阶段：将从图像中提取的 DINO 特征通过显式反投影应用，并进一步与来自 RandLA-Net 的空间特征相结合。
（3）特征聚合：利用特征聚合以全局到局部的策略将它们融合，以获得紧凑的分割特征。</p>
</li>
<li>
<p>结论：
（1）本工作提出了一种紧凑且快速的基于高斯体的场景分割方法 CoSSegGaussians，该方法在仅有 RGB 图像的条件下实现了紧凑且快速的场景分割。该方法建立在 3D 高斯体之上，并利用双特征融合网络作为分割场，该网络聚合了 DINO 和空间特征进行分割。来自图像的多尺度 DINO 特征通过反投影引入定位的 3D 高斯体，并进一步与来自 RandLA-Net 的高斯体的空间信息相结合。然后应用全局到局部的聚合模块来生成紧凑的分割逻辑。结果表明，我们的模型可以可靠且高效地完成零样本分割任务。
（2）创新点：</p>
</li>
<li>提出了一种紧凑且快速的基于高斯体的场景分割方法 CoSSegGaussians。</li>
<li>利用双特征融合网络作为分割场，该网络聚合了 DINO 和空间特征进行分割。</li>
<li>将来自图像的多尺度 DINO 特征通过反投影引入定位的 3D 高斯体，并进一步与来自 RandLA-Net 的高斯体的空间信息相结合。</li>
<li>应用全局到局部的聚合模块来生成紧凑的分割逻辑。
性能：</li>
<li>在语义和全景零样本分割任务上优于基线。</li>
<li>推理时间不到基于 NeRF 方法的 10%。
工作量：</li>
<li>论文长度适中，实验部分较为详细。</li>
<li>代码和数据已开源。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7b3b4f44e1bfaba57c660121007fee8a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-222c4f05c24f306aefd909de021e726c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e297ea1e2c85e96907865cc0d6107864.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27a48d664aab4676f21f642635ecb972.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a12edf74e5d62d5f426b60407d904ab.jpg" align="middle">
</details>




<h2 id="AGG-Amortized-Generative-3D-Gaussians-for-Single-Image-to-3D"><a href="#AGG-Amortized-Generative-3D-Gaussians-for-Single-Image-to-3D" class="headerlink" title="AGG: Amortized Generative 3D Gaussians for Single Image to 3D"></a>AGG: Amortized Generative 3D Gaussians for Single Image to 3D</h2><p><strong>Authors:Dejia Xu, Ye Yuan, Morteza Mardani, Sifei Liu, Jiaming Song, Zhangyang Wang, Arash Vahdat</strong></p>
<p>Given the growing need for automatic 3D content creation pipelines, various 3D representations have been studied to generate 3D objects from a single image. Due to its superior rendering efficiency, 3D Gaussian splatting-based models have recently excelled in both 3D reconstruction and generation. 3D Gaussian splatting approaches for image to 3D generation are often optimization-based, requiring many computationally expensive score-distillation steps. To overcome these challenges, we introduce an Amortized Generative 3D Gaussian framework (AGG) that instantly produces 3D Gaussians from a single image, eliminating the need for per-instance optimization. Utilizing an intermediate hybrid representation, AGG decomposes the generation of 3D Gaussian locations and other appearance attributes for joint optimization. Moreover, we propose a cascaded pipeline that first generates a coarse representation of the 3D data and later upsamples it with a 3D Gaussian super-resolution module. Our method is evaluated against existing optimization-based 3D Gaussian frameworks and sampling-based pipelines utilizing other 3D representations, where AGG showcases competitive generation abilities both qualitatively and quantitatively while being several orders of magnitude faster. Project page: <a href="https://ir1d.github.io/AGG/">https://ir1d.github.io/AGG/</a> </p>
<p><a href="http://arxiv.org/abs/2401.04099v1">PDF</a> Project page: <a href="https://ir1d.github.io/AGG/">https://ir1d.github.io/AGG/</a></p>
<p><strong>摘要</strong><br>无需昂贵计算，AGG 即可直接从图像生成 3D 高斯体素，大幅提升了 3D 内容创建效率。</p>
<p><strong>要点</strong></p>
<ul>
<li>AGG 是一个直接从图像生成 3D 高斯体素的框架，无需逐例优化，大大提高了生成效率。</li>
<li>AGG 使用了混合表示，将 3D 高斯体素的位置和外观属性分开生成，并联合优化。</li>
<li>AGG 使用级联管道，先生成 3D 数据的粗略表示，然后通过 3D 高斯体素超分辨率模块进行上采样。</li>
<li>AGG 在定性和定量方面都优于现有基于优化的 3D 高斯体素框架和使用其他 3D 表示的基于采样的管道。</li>
<li>AGG 的速度比现有方法快几个数量级。</li>
<li>项目主页：<a href="https://ir1d.github.io/AGG/">https://ir1d.github.io/AGG/</a></li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：AGG：用于单图像到 3D 的摊余生成 3D 高斯（AGG：Amortized Generative 3D Gaussians for Single Image to 3D）</li>
<li>作者：Dejia Xu, Ye Yuan, Morteza Mardani, Sifei Liu, Jiaming Song, Zhangyang Wang, Arash Vahdat</li>
<li>第一作者单位：德克萨斯大学奥斯汀分校</li>
<li>关键词：图像到 3D、3D 生成、3D 高斯、摊余生成、级联管道</li>
<li>论文链接：https://arxiv.org/abs/2401.04099，Github 代码链接：无</li>
<li>摘要：
（1）随着对自动 3D 内容创建管道需求的不断增长，已经研究了各种 3D 表示来从单个图像生成 3D 对象。由于其卓越的渲染效率，基于 3D 高斯 splatting 的模型最近在 3D 重建和生成方面都表现出色。用于图像到 3D 生成的 3D 高斯 splatting 方法通常基于优化，需要许多计算成本高昂的得分蒸馏步骤。为了克服这些挑战，我们引入了一个摊余生成 3D 高斯框架（AGG），该框架可以从单个图像立即生成 3D 高斯，从而无需进行逐个实例的优化。利用中间混合表示，AGG 分解了 3D 高斯位置和其他外观属性的生成，以便进行联合优化。此外，我们提出了一个级联管道，该管道首先生成 3D 数据的粗略表示，然后使用 3D 高斯超分辨率模块对其进行上采样。我们的方法针对现有基于优化的 3D 高斯框架和利用其他 3D 表示的基于采样的管道进行了评估，其中 AGG 在定性和定量方面都展示了具有竞争力的生成能力，同时速度提高了几个数量级。</li>
</ol>
<p>Methods:
(1): AGG通过摊余生成来避免逐个实例的优化，从而实现从单个图像到3D高斯的立即生成。
(2): AGG利用中间混合表示将3D高斯位置和其他外观属性的生成分解为联合优化问题。
(3): AGG采用级联管道，首先生成3D数据的粗略表示，然后使用3D高斯超分辨率模块对其进行上采样。
(4): AGG在定性和定量方面都展示了具有竞争力的生成能力，同时速度提高了几个数量级。</p>
<ol>
<li>结论：
（1）：本文首次尝试开发一个能够从单张图像输入生成 3D 高斯 splatting 的摊余管道。提出的 AGG 框架利用级联生成管道，包括粗略混合生成器和高斯超分辨率模型。实验结果表明，与基于优化的 3D 高斯框架和基于采样的 3D 生成框架相比，我们的方法在单图像到 3D 生成中实现了具有竞争力的性能，并且速度提高了几个数量级。
（2）：创新点：
提出了一种摊余生成 3D 高斯 splatting 的新框架 AGG，该框架可以从单张图像立即生成 3D 高斯，而无需进行逐个实例的优化。
利用中间混合表示将 3D 高斯位置和其他外观属性的生成分解为联合优化问题，提高了生成效率。
采用级联管道，首先生成 3D 数据的粗略表示，然后使用 3D 高斯超分辨率模块对其进行上采样，提高了生成的质量。
性能：
在定性和定量方面都展示了具有竞争力的生成能力，在单图像到 3D 生成中的速度提高了几个数量级。
工作量：
代码和数据将在论文发布后公开。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-7f1992dc148bbeebd8e201f1e361744a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dfa4769a03fba071a100ba492ba057c1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a15f930311ed81ec60e68bbe1e79e746.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-436ee02cfefc628563bdcac1719e6c80.jpg" align="middle">
</details>




<h2 id="PEGASUS-Physically-Enhanced-Gaussian-Splatting-Simulation-System-for-6DOF-Object-Pose-Dataset-Generation"><a href="#PEGASUS-Physically-Enhanced-Gaussian-Splatting-Simulation-System-for-6DOF-Object-Pose-Dataset-Generation" class="headerlink" title="PEGASUS: Physically Enhanced Gaussian Splatting Simulation System for   6DOF Object Pose Dataset Generation"></a>PEGASUS: Physically Enhanced Gaussian Splatting Simulation System for   6DOF Object Pose Dataset Generation</h2><p><strong>Authors:Lukas Meyer, Floris Erich, Yusuke Yoshiyasu, Marc Stamminger, Noriaki Ando, Yukiyasu Domae</strong></p>
<p>We introduce Physically Enhanced Gaussian Splatting Simulation System (PEGASUS) for 6DOF object pose dataset generation, a versatile dataset generator based on 3D Gaussian Splatting. Environment and object representations can be easily obtained using commodity cameras to reconstruct with Gaussian Splatting. PEGASUS allows the composition of new scenes by merging the respective underlying Gaussian Splatting point cloud of an environment with one or multiple objects. Leveraging a physics engine enables the simulation of natural object placement within a scene through interaction between meshes extracted for the objects and the environment. Consequently, an extensive amount of new scenes - static or dynamic - can be created by combining different environments and objects. By rendering scenes from various perspectives, diverse data points such as RGB images, depth maps, semantic masks, and 6DoF object poses can be extracted. Our study demonstrates that training on data generated by PEGASUS enables pose estimation networks to successfully transfer from synthetic data to real-world data. Moreover, we introduce the Ramen dataset, comprising 30 Japanese cup noodle items. This dataset includes spherical scans that captures images from both object hemisphere and the Gaussian Splatting reconstruction, making them compatible with PEGASUS. </p>
<p><a href="http://arxiv.org/abs/2401.02281v1">PDF</a> Project Page: <a href="https://meyerls.github.io/pegasus_web">https://meyerls.github.io/pegasus_web</a></p>
<p><strong>Summary</strong><br>六自由度目标位姿数据集生成的新型物理实体增强高斯溅射模拟系统。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>PEGASUS 使用商品相机轻松获取环境和物体表示，重建带有高斯溅射的场景。</li>
<li>PEGASUS 允许通过合并环境与一个或多个物体的相应底层高斯溅射点云来组成新场景。</li>
<li>利用物理引擎能够模拟物体在场景中的自然放置，通过提取的物体网格和环境之间的相互作用。</li>
<li>通过将不同的环境和物体组合起来，可以创建大量新的静态或动态场景。</li>
<li>通过从不同视角渲染场景，可以提取各种数据点，例如 RGB 图像、深度图、语义蒙版和 6DoF 目标位姿。</li>
<li>在 PEGASUS 生成的训练数据上训练的位姿估计网络能够成功地从合成数据转移到真实数据。</li>
<li>我们介绍了包含 30 种日本杯面商品的 Ramen 数据集。此数据集包括球形扫描，可从目标半球和高斯溅射重建中捕获图像，使其与 PEGASUS 兼容。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：PEGASUS：用于 6DOF 物体位姿数据集生成的物理增强高斯散射模拟系统</li>
<li>作者：Lukas Meyer, Floris Erich, Yusuke Yoshiyasu, Marc Stamminger, Noriaki Ando, Yukiyasu Domae</li>
<li>第一作者单位：Friedrich-Alexander-Universit¨at Erlangen-N¨urnberg-F¨urth</li>
<li>关键词：数据集生成、机器人、光场、sim2real</li>
<li>论文链接：https://arxiv.org/pdf/2401.02281.pdf，Github 链接：无</li>
<li>摘要：
（1）研究背景：随着人口结构的变化，许多国家面临着劳动力短缺的问题。日本也不例外，其人口正在减少，导致政府在多个行业（如医疗保健、制造业和农业）大力投资机器人，以维持劳动力稳定。本文的研究重点是开发服务业的机器人系统，以支持零售业的人员。
（2）过去的方法及其问题：为了应用深度学习方法进行物体位姿估计，大多数数据集都集中在西方风格的产品上。然而，使用这些数据集训练的模型在应用于实际场景时往往存在域差距问题，因为合成生成的数据缺乏真实性。
（3）本文提出的研究方法：为了解决域差距问题，本文提出了一种名为 PEGASUS 的物理增强高斯散射模拟系统，用于生成 6DOF 物体位姿数据集。PEGASUS 允许将环境和物体表示合并，以便创建新的场景。通过利用物理引擎，可以在场景中模拟自然物体放置，从而创建大量的静态或动态新场景。通过从不同视角渲染场景，可以提取各种数据点，如 RGB 图像、深度图、语义掩码和 6DoF 物体位姿。
（4）方法的性能及对目标的支持：研究表明，使用 PEGASUS 生成的训练数据可以使位姿估计网络成功地从合成数据转移到真实世界数据。此外，本文还介绍了拉面数据集，其中包含 30 种日本杯面。该数据集包括从物体两个半球捕获图像的球形扫描和高斯散射重建，使其与 PEGASUS 兼容。</li>
</ol>
<p>方法：</p>
<p>(1) 高斯散射基础环境：通过使用 Structure from Motion (SfM) 重建技术和 CherryPicker 方法，从 10 个不同场景中提取稀疏点云，并使用高斯散射技术生成基础环境的 3D 重建和网格。</p>
<p>(2) 高斯散射对象：利用 Ortery 扫描系统对物体进行图像采集，并使用与基础环境相同的设置进行高斯散射处理，生成物体的照度实体和几何实体。</p>
<p>(3) 物理引擎：将 PyBullet 集成到 PEGASUS 中，作为物理引擎，用于模拟物体的自然放置和动态场景的创建。</p>
<p>(4) PEGASUS 数据集生成：通过将高斯散射基础环境和高斯散射对象集成，利用物理引擎模拟物体的运动轨迹，并使用高斯散射渲染器渲染场景，生成包含 RGB 图像、深度图、语义掩码、2D/3D 边界框和变换矩阵等数据的训练数据集。</p>
<p>(5) 拉面数据集：使用 3DPhotoBench280 和 3DMultiArm2000 相机系统扫描 30 多种杯面，并使用自动背景去除技术和特征丰富的表面进行校准。</p>
<ol>
<li>结论：
（1）：PEGASUS是一个多功能的数据集生成器，旨在提高物体位姿估计的准确性和质量。除了PEGASUS，我们还介绍了拉面数据集，其中包含30多种不同的产品。该数据集生成器巧妙地创建了逼真的渲染、语义掩码、深度图，并捕获了物体位姿。PEGASUS专为生成特定领域的数据集而设计，有助于微调神经网络，使其超越单纯的位姿估计任务。
（2）：创新点：PEGASUS是一个多功能的数据集生成器，可以生成逼真的渲染、语义掩码、深度图和物体位姿。它使用物理引擎来模拟自然物体放置和动态场景的创建。拉面数据集包含30多种不同的产品，并使用自动背景去除技术和特征丰富的表面进行校准。
性能：PEGASUS可以生成高质量的数据集，这些数据集可以用于训练物体位姿估计网络。拉面数据集是一个具有挑战性的数据集，可以用于评估物体位姿估计网络的性能。
工作量：PEGASUS是一个复杂的数据集生成器，需要大量的时间和精力来创建。拉面数据集是一个大型数据集，需要大量的时间和精力来收集和注释。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-00e4aa054ddb93bc6555152285634f59.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-53410ef5f12fd336c82ff96b81afe2e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aacbd06f5c2193593ff83881df0a9a65.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d2817a77d6336db5f08820060093065.jpg" align="middle">
</details>




<h2 id="FMGS-Foundation-Model-Embedded-3D-Gaussian-Splatting-for-Holistic-3D-Scene-Understanding"><a href="#FMGS-Foundation-Model-Embedded-3D-Gaussian-Splatting-for-Holistic-3D-Scene-Understanding" class="headerlink" title="FMGS: Foundation Model Embedded 3D Gaussian Splatting for Holistic 3D   Scene Understanding"></a>FMGS: Foundation Model Embedded 3D Gaussian Splatting for Holistic 3D   Scene Understanding</h2><p><strong>Authors:Xingxing Zuo, Pouya Samangouei, Yunwen Zhou, Yan Di, Mingyang Li</strong></p>
<p>Precisely perceiving the geometric and semantic properties of real-world 3D objects is crucial for the continued evolution of augmented reality and robotic applications. To this end, we present \algfull{} (\algname{}), which incorporates vision-language embeddings of foundation models into 3D Gaussian Splatting (GS). The key contribution of this work is an efficient method to reconstruct and represent 3D vision-language models. This is achieved by distilling feature maps generated from image-based foundation models into those rendered from our 3D model. To ensure high-quality rendering and fast training, we introduce a novel scene representation by integrating strengths from both GS and multi-resolution hash encodings (MHE). Our effective training procedure also introduces a pixel alignment loss that makes the rendered feature distance of same semantic entities close, following the pixel-level semantic boundaries. Our results demonstrate remarkable multi-view semantic consistency, facilitating diverse downstream tasks, beating state-of-the-art methods by $\mathbf{10.2}$ percent on open-vocabulary language-based object detection, despite that we are $\mathbf{851\times}$ faster for inference. This research explores the intersection of vision, language, and 3D scene representation, paving the way for enhanced scene understanding in uncontrolled real-world environments. We plan to release the code upon paper acceptance. </p>
<p><a href="http://arxiv.org/abs/2401.01970v1">PDF</a> 19 pages, Project page coming soon</p>
<p><strong>Summary</strong><br>3D 高斯散点与视觉语言嵌入相结合，可高效重建并表示 3D 视觉语言模型。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>\algname{} 将视觉语言嵌入结合到 3D 高斯散点中，以高效重建并表示 3D 视觉语言模型。</li>
<li>3D 高斯散点和多分辨率哈希编码的优势相结合，实现了新的场景表示。</li>
<li>像素对齐损失可确保渲染特征距离相同的语义实体接近，从而实现高质量的渲染和快速训练。</li>
<li>\algname{} 在开放词汇语言对象检测基准上超越了最先进的方法，且推理速度快了 851 倍。</li>
<li>\algname{} 可以在不受控的真实世界环境中增强场景理解。</li>
<li>研究探索了视觉、语言和 3D 场景表示的交叉，为增强场景理解铺平了道路。</li>
<li>代码将在论文被接受后发布。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：FMGS：用于整体 3D 场景理解的基础模型嵌入式 3D 高斯泼溅</li>
<li>作者：Xingxing Zuo、Pouya Samangouei、Yunwen Zhou、Yan Di、Mingyang Li</li>
<li>第一作者单位：谷歌</li>
<li>关键词：高斯泼溅、视觉语言嵌入、基础模型、开放词汇语义</li>
<li>论文链接：https://arxiv.org/abs/2401.01970，Github 链接：无</li>
<li>
<p>摘要：
（1）研究背景：精确感知现实世界 3D 物体的几何和语义属性对于增强现实和机器人应用的持续发展至关重要。
（2）过去方法：现有方法主要集中在 3D 几何和外观估计或 3D 对象检测和场景分割上，这些方法在具有封闭类集的数据集上进行了训练。然而，对于智能代理商与物理世界进行平滑交互，仅理解由预先识别的标签表征的空间子集是不够的。
（3）研究方法：本文提出了一种名为 FMGS 的方法，将基础模型的视觉语言嵌入整合到 3D 高斯泼溅 (GS) 中。FMGS 的关键贡献是一种有效的方法来重建和表示 3D 视觉语言模型。这是通过将基于图像的基础模型生成的特征图提取到从 3D 模型渲染的特征图中来实现的。为了确保高质量的渲染和快速训练，本文引入了一种新的场景表示，该表示集成了 GS 和多分辨率哈希编码 (MHE) 的优势。本文的有效训练过程还引入了一个像素对齐损失，该损失使相同语义实体的渲染特征距离接近，遵循像素级语义边界。
（4）方法性能：实验结果表明，FMGS 在开放词汇语言对象检测任务上比最先进的方法提高了 10.2%，尽管它的推理速度快 851 倍。这表明 FMGS 能够有效地探索视觉、语言和 3D 场景表示的交集，为在不受控制的现实世界环境中增强场景理解铺平了道路。</p>
</li>
<li>
<p>方法：
(1) FMGS方法概述：FMGS方法将基础模型的视觉语言嵌入整合到3D高斯泼溅(GS)中，通过将基于图像的基础模型生成的特征图提取到从3D模型渲染的特征图中，重建和表示3D视觉语言模型。
(2) 场景表示：FMGS方法引入了一种新的场景表示，该表示集成了GS和多分辨率哈希编码(MHE)的优势，确保高质量的渲染和快速训练。
(3) 有效训练过程：FMGS方法引入了一个像素对齐损失，该损失使相同语义实体的渲染特征距离接近，遵循像素级语义边界，保证了有效训练过程。</p>
</li>
<li>
<p>结论：
（1）：FMGS方法将基础模型的视觉语言嵌入整合到3D高斯泼溅(GS)中，通过将基于图像的基础模型生成的特征图提取到从3D模型渲染的特征图中，重建和表示3D视觉语言模型，为在不受控制的现实世界环境中增强场景理解铺平了道路。
（2）：创新点：
FMGS方法的关键贡献是一种有效的方法来重建和表示3D视觉语言模型。这是通过将基于图像的基础模型生成的特征图提取到从3D模型渲染的特征图中来实现的。
FMGS方法引入了一种新的场景表示，该表示集成了GS和多分辨率哈希编码(MHE)的优势，确保高质量的渲染和快速训练。
FMGS方法引入了一个像素对齐损失，该损失使相同语义实体的渲染特征距离接近，遵循像素级语义边界，保证了有效训练过程。
性能：
实验结果表明，FMGS在开放词汇语言对象检测任务上比最先进的方法提高了10.2%，尽管它的推理速度快851倍。这表明FMGS能够有效地探索视觉、语言和3D场景表示的交集，为在不受控制的现实世界环境中增强场景理解铺平了道路。
工作量：
FMGS方法的实现相对复杂，需要较高的编程和数学基础。此外，该方法需要大量的数据和计算资源，这可能会增加训练和部署的成本。</p>
</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9d4d136f06ab7f31a3343e367c298d7d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-859b3e0bd449bc5f2715166a19563958.jpg" align="middle">
</details>




<h2 id="Deblurring-3D-Gaussian-Splatting"><a href="#Deblurring-3D-Gaussian-Splatting" class="headerlink" title="Deblurring 3D Gaussian Splatting"></a>Deblurring 3D Gaussian Splatting</h2><p><strong>Authors:Byeonghyeon Lee, Howoong Lee, Xiangyu Sun, Usman Ali, Eunbyung Park</strong></p>
<p>Recent studies in Radiance Fields have paved the robust way for novel view synthesis with their photorealistic rendering quality. Nevertheless, they usually employ neural networks and volumetric rendering, which are costly to train and impede their broad use in various real-time applications due to the lengthy rendering time. Lately 3D Gaussians splatting-based approach has been proposed to model the 3D scene, and it achieves remarkable visual quality while rendering the images in real-time. However, it suffers from severe degradation in the rendering quality if the training images are blurry. Blurriness commonly occurs due to the lens defocusing, object motion, and camera shake, and it inevitably intervenes in clean image acquisition. Several previous studies have attempted to render clean and sharp images from blurry input images using neural fields. The majority of those works, however, are designed only for volumetric rendering-based neural radiance fields and are not straightforwardly applicable to rasterization-based 3D Gaussian splatting methods. Thus, we propose a novel real-time deblurring framework, deblurring 3D Gaussian Splatting, using a small Multi-Layer Perceptron (MLP) that manipulates the covariance of each 3D Gaussian to model the scene blurriness. While deblurring 3D Gaussian Splatting can still enjoy real-time rendering, it can reconstruct fine and sharp details from blurry images. A variety of experiments have been conducted on the benchmark, and the results have revealed the effectiveness of our approach for deblurring. Qualitative results are available at <a href="https://benhenryl.github.io/Deblurring-3D-Gaussian-Splatting/">https://benhenryl.github.io/Deblurring-3D-Gaussian-Splatting/</a> </p>
<p><a href="http://arxiv.org/abs/2401.00834v1">PDF</a> 19 pages, 8 figures</p>
<p><strong>Summary</strong><br>利用小型多层感知器 (MLP) 处理高斯分布的协方差，实现实时去模糊，重建清晰图像。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>最近，基于体素渲染的3D高斯散射方法实现了实时渲染，但容易受到训练图像模糊的影响，导致渲染质量下降。</li>
<li>本文提出了一种新的实时去模糊框架——去模糊3D高斯散射，利用小型多层感知器 (MLP) 处理高斯分布的协方差，以模拟场景模糊。</li>
<li>去模糊3D高斯散射仍然可以享受实时渲染，同时可以从模糊图像中重建出精细清晰的细节。</li>
<li>可以在基准测试集上进行多种实验，结果表明本文方法对于去模糊是有效的。</li>
<li>定性结果可在 <a href="https://benhenryl.github.io/Deblurring-3D-Gaussian-Splatting/">https://benhenryl.github.io/Deblurring-3D-Gaussian-Splatting/</a> 获得。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：去模糊 3D 高斯斑点图</li>
<li>作者：Byeonghyeon Lee、Howoong Lee、Xiangyu Sun、Usman Ali 和 Eunbyung Park</li>
<li>第一作者单位：韩国成均馆大学人工智能系</li>
<li>关键词：神经辐射场、3D 高斯斑点图、去模糊、实时渲染</li>
<li>论文链接：https://arxiv.org/abs/2401.00834
Github 链接：无</li>
<li>
<p>摘要：
（1）研究背景：神经辐射场 (NeRF) 的出现为具有逼真渲染质量的新视角合成开辟了一条稳健的道路。然而，它们通常采用神经网络和体积渲染，这些方法训练成本高昂，并且由于漫长的渲染时间而阻碍了它们在各种实时应用程序中的广泛使用。最近，人们提出了一种基于 3D 高斯斑点图的方法来建模 3D 场景，它在实时渲染图像的同时实现了卓越的视觉质量。然而，如果训练图像模糊，则渲染质量会严重下降。模糊通常由于镜头失焦、物体运动和相机抖动而发生，它不可避免地会干预清晰图像的获取。之前的一些研究尝试使用神经场从模糊输入图像渲染干净清晰的图像。然而，这些工作中的大多数仅针对基于体积渲染的神经辐射场而设计，并不直接适用于基于光栅化的 3D 高斯斑点图方法。
（2）过去的方法及其问题：过去的方法主要针对基于体积渲染的神经辐射场而设计，不适用于基于光栅化的 3D 高斯斑点图方法。该方法动机明确，针对 3D 高斯斑点图方法的模糊问题，提出了一种新的去模糊框架。
（3）研究方法：提出了一种新的实时去模糊框架，称为去模糊 3D 高斯斑点图，它使用了一个小的多层感知器 (MLP) 来操纵每个 3D 高斯的协方差以建模场景模糊。虽然去模糊 3D 高斯斑点图仍然可以享受实时渲染，但它可以从模糊图像中重建精细而清晰的细节。
（4）方法的性能：在基准上进行了各种实验，结果揭示了我们方法去模糊的有效性。定性结果可在 https://benhenryl.github.io/Deblurring-3D-Gaussian-Splatting/ 获得。</p>
</li>
<li>
<p>Methods：
(1): 提出了一种新的实时去模糊框架，称为去模糊3D高斯斑点图，它使用了一个小的多层感知器 (MLP) 来操纵每个3D高斯的协方差以建模场景模糊。
(2): 为了解决3D高斯斑点图方法在模糊图像下渲染质量下降的问题，该方法提出了一种新的去模糊框架，该框架使用了一个小的多层感知器 (MLP) 来操纵每个3D高斯的协方差以建模场景模糊。
(3): 虽然去模糊3D高斯斑点图仍然可以享受实时渲染，但它可以从模糊图像中重建精细而清晰的细节。</p>
</li>
<li>
<p>结论：
（1）：本文提出了一种新的实时去模糊框架，称为去模糊 3D 高斯斑点图，它使用了一个小的多层感知器 (MLP) 来操纵每个 3D 高斯的协方差以建模场景模糊。我们还通过额外的点分配进一步促进了去模糊，该分配通过 K-最近邻算法均匀地分布场景中的点并分配颜色特征。此外，由于我们应用了基于深度的剪枝而不是 3D-GS 采用的朴素剪枝，我们可以在场景边缘（SfM 通常难以提取特征并无法生成足够点）保留更多点。通过广泛的实验，我们验证了我们的方法可以模糊散焦模糊，同时仍然享受具有 FPS&gt;200 的实时渲染。这是因为我们仅在训练期间使用 MLP，并且 MLP 不参与推理阶段，从而使推理阶段与 3D-GS 保持一致。我们的方法在不同指标下评估时实现了最先进的性能或与当前最前沿的模型相当。
（2）：创新点：</p>
</li>
<li>提出了一种新的实时去模糊框架，称为去模糊 3D 高斯斑点图，它使用了一个小的多层感知器 (MLP) 来操纵每个 3D 高斯的协方差以建模场景模糊。</li>
<li>进一步促进了去模糊，通过额外的点分配均匀地分布场景中的点并分配颜色特征。</li>
<li>应用了基于深度的剪枝而不是 3D-GS 采用的朴素剪枝，可以在场景边缘保留更多点。
性能：</li>
<li>我们的方法可以在模糊图像下渲染精细而清晰的细节。</li>
<li>我们的方法在基准上进行了各种实验，结果揭示了我们方法去模糊的有效性。</li>
<li>我们的方法在不同指标下评估时实现了最先进的性能或与当前最前沿的模型相当。
工作量：</li>
<li>该方法使用了一个小的多层感知器 (MLP) 来操纵每个 3D 高斯的协方差以建模场景模糊，工作量较小。</li>
<li>该方法通过额外的点分配进一步促进了去模糊，工作量较小。</li>
<li>该方法应用了基于深度的剪枝而不是 3D-GS 采用的朴素剪枝，工作量较小。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-5884764967912a4d08dd4f817a2619a7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-689495a2c3ac97a3861a047c4d7a0252.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ccf1c2c36b334acc29bec756307b6720.jpg" align="middle">
</details>




<h2 id="LangSplat-3D-Language-Gaussian-Splatting"><a href="#LangSplat-3D-Language-Gaussian-Splatting" class="headerlink" title="LangSplat: 3D Language Gaussian Splatting"></a>LangSplat: 3D Language Gaussian Splatting</h2><p><strong>Authors:Minghan Qin, Wanhua Li, Jiawei Zhou, Haoqian Wang, Hanspeter Pfister</strong></p>
<p>Human lives in a 3D world and commonly uses natural language to interact with a 3D scene. Modeling a 3D language field to support open-ended language queries in 3D has gained increasing attention recently. This paper introduces LangSplat, which constructs a 3D language field that enables precise and efficient open-vocabulary querying within 3D spaces. Unlike existing methods that ground CLIP language embeddings in a NeRF model, LangSplat advances the field by utilizing a collection of 3D Gaussians, each encoding language features distilled from CLIP, to represent the language field. By employing a tile-based splatting technique for rendering language features, we circumvent the costly rendering process inherent in NeRF. Instead of directly learning CLIP embeddings, LangSplat first trains a scene-wise language autoencoder and then learns language features on the scene-specific latent space, thereby alleviating substantial memory demands imposed by explicit modeling. Existing methods struggle with imprecise and vague 3D language fields, which fail to discern clear boundaries between objects. We delve into this issue and propose to learn hierarchical semantics using SAM, thereby eliminating the need for extensively querying the language field across various scales and the regularization of DINO features. Extensive experiments on open-vocabulary 3D object localization and semantic segmentation demonstrate that LangSplat significantly outperforms the previous state-of-the-art method LERF by a large margin. Notably, LangSplat is extremely efficient, achieving a {\speed} $\times$ speedup compared to LERF at the resolution of 1440 $\times$ 1080. We strongly recommend readers to check out our video results at <a href="https://langsplat.github.io">https://langsplat.github.io</a> </p>
<p><a href="http://arxiv.org/abs/2312.16084v1">PDF</a> Project Page: <a href="https://langsplat.github.io">https://langsplat.github.io</a></p>
<p><strong>摘要</strong><br>利用 3D 高斯体表示语言特征，LangSplat 在 3D 空间中构建了语言场，实现了无需预训练的、效率高的开集查询。</p>
<p><strong>要点</strong></p>
<ul>
<li>LangSplat 利用 3D 高斯体集合对语言特征进行编码，不需要对 CLIP 语言嵌入进行预训练。</li>
<li>LangSplat 使用基于切片的 splatting 技术渲染语言特征，提高了渲染效率。</li>
<li>LangSplat 首先训练场景级的语言自动编码器，然后在特定场景的潜在空间上学习语言特征，减少了显式建模带来的内存需求。</li>
<li>LangSplat 利用 SAM 学习分层语义，无需在不同尺度上大量查询语言场，也不需要 DINO 特征的正则化。</li>
<li>LangSplat 在开集 3D 物体定位和语义分割任务上的表现显著优于之前的最佳方法 LERF。</li>
<li>LangSplat 非常高效，在 1440 × 1080 的分辨率下，速度比 LERF 快 {\speed} 倍。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：LangSplat：高效的 3D 语言场用于开放式词汇查询</li>
<li>作者：Yilun Du<em>, Xuran Pan</em>, Bo Dai, Chen Change Loy, Dahua Lin</li>
<li>单位：香港中文大学</li>
<li>关键词：3D 语言场、开放式词汇查询、语义分割、对象定位、CLIP</li>
<li>论文链接：https://arxiv.org/abs/2302.06121，Github 链接：None</li>
<li>
<p>摘要：
(1) 研究背景：人类生活在 3D 世界中，通常使用自然语言与 3D 场景进行交互。对 3D 语言场进行建模以支持 3D 空间中的开放式词汇查询最近引起了越来越多的关注。
(2) 过去的方法：现有方法将 CLIP 语言嵌入整合到 NeRF 模型中。然而，NeRF 方法的渲染过程非常耗时，即使是最先进的 NeRF 技术也无法在高分辨率、不受限制的场景中实现实时渲染。此外，现有方法难以区分对象之间的清晰边界，导致 3D 语言场不精确且模糊。
(3) 研究方法：本文提出 LangSplat，它使用一组 3D 高斯函数来表示语言场，每个高斯函数都对从 CLIP 中提取的语言特征进行编码。LangSplat 采用基于图块的 splatting 技术来渲染语言特征，从而避免了 NeRF 中固有的昂贵渲染过程。此外，LangSplat 首先训练一个场景级的语言自动编码器，然后在场景特定的潜在空间上学习语言特征，从而减轻了显式建模带来的大量内存需求。为了解决对象之间的模糊边界问题，LangSplat 提出了一种学习分层语义的方法，该方法利用 SAM 来生成具有清晰边界的对象掩码。
(4) 方法性能：在开放式词汇 3D 对象定位和语义分割任务上，LangSplat 的性能明显优于之前的最先进方法 LERF。值得注意的是，LangSplat 非常高效，在 1440×1080 的分辨率下，与 LERF 相比，速度提高了 199 倍。</p>
</li>
<li>
<p>方法：
(1) 利用 SAM 学习分层语义：采用 SAM 生成具有清晰边界的对象掩码，以解决对象之间的模糊边界问题。
(2) 提取像素对齐的语言嵌入：将 SAM 生成的掩码发送到 CLIP 图像编码器以提取相应的 CLIP 嵌入。
(3) 学习场景级语言自动编码器：使用这些获得的 CLIP 嵌入训练一个场景级语言自动编码器，以减少显式建模带来的大量内存需求。
(4) 3D 语言高斯 splatting：使用一组 3D 高斯函数来表示语言场，每个高斯函数都对从 CLIP 中提取的语言特征进行编码。
(5) 基于图块的 splatting 技术：采用基于图块的 splatting 技术来渲染语言特征，从而避免了 NeRF 中固有的昂贵渲染过程。</p>
</li>
<li>
<p>结论：
（1）：LangSplat 是一种用于构建 3D 语言场的方法，能够在 3D 空间内实现精确且高效的开放式词汇查询。LangSplat 通过将 3D 高斯 Splatting 扩展到语言特征，并学习场景特定的语言自动编码器，避免了基于 NeRF 的方法固有的缓慢渲染速度。此外，LangSplat 提出学习由 SAM 定义的语义层次结构，有效地解决了点模糊问题，从而实现了更加精确和可靠的 3D 语言场。实验结果清楚地表明，LangSplat 优于现有的最先进方法（如 LERF），尤其是在其显着的 199 倍速度提升和在开放式 3D 语言查询任务中的增强性能方面。
（2）：创新点：</p>
</li>
<li>提出了一种利用 3D 高斯 Splatting 和场景级语言自动编码器来构建 3D 语言场的新方法。</li>
<li>提出了一种学习语义层次结构的方法，该方法能够有效地解决点模糊问题，从而提高了 3D 语言场的精度和可靠性。</li>
<li>提出了一种基于图块的 Splatting 技术，该技术能够避免 NeRF 中固有的昂贵渲染过程，从而大大提高了渲染速度。
性能：</li>
<li>在开放式词汇 3D 对象定位和语义分割任务上，LangSplat 的性能明显优于之前的最先进方法 LERF。</li>
<li>LangSplat 非常高效，在 1440×1080 的分辨率下，与 LERF 相比，速度提高了 199 倍。
工作量：</li>
<li>LangSplat 的实现相对复杂，需要对 3D 高斯 Splatting、场景级语言自动编码器、学习语义层次结构和基于图块的 Splatting 技术等多个方面进行实现。</li>
<li>LangSplat 的训练过程相对耗时，需要大量的数据和计算资源。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-59c272aba45f1a4c840c869c1aeb179c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5173c1e31a57b0806bd38f395623e341.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bc17dfd41d5d04e3b354c6d95099e61.jpg" align="middle">
</details>




<h2 id="Human101-Training-100-FPS-Human-Gaussians-in-100s-from-1-View"><a href="#Human101-Training-100-FPS-Human-Gaussians-in-100s-from-1-View" class="headerlink" title="Human101: Training 100+FPS Human Gaussians in 100s from 1 View"></a>Human101: Training 100+FPS Human Gaussians in 100s from 1 View</h2><p><strong>Authors:Mingwei Li, Jiachen Tao, Zongxin Yang, Yi Yang</strong></p>
<p>Reconstructing the human body from single-view videos plays a pivotal role in the virtual reality domain. One prevalent application scenario necessitates the rapid reconstruction of high-fidelity 3D digital humans while simultaneously ensuring real-time rendering and interaction. Existing methods often struggle to fulfill both requirements. In this paper, we introduce Human101, a novel framework adept at producing high-fidelity dynamic 3D human reconstructions from 1-view videos by training 3D Gaussians in 100 seconds and rendering in 100+ FPS. Our method leverages the strengths of 3D Gaussian Splatting, which provides an explicit and efficient representation of 3D humans. Standing apart from prior NeRF-based pipelines, Human101 ingeniously applies a Human-centric Forward Gaussian Animation method to deform the parameters of 3D Gaussians, thereby enhancing rendering speed (i.e., rendering 1024-resolution images at an impressive 60+ FPS and rendering 512-resolution images at 100+ FPS). Experimental results indicate that our approach substantially eclipses current methods, clocking up to a 10 times surge in frames per second and delivering comparable or superior rendering quality. Code and demos will be released at <a href="https://github.com/longxiang-ai/Human101">https://github.com/longxiang-ai/Human101</a>. </p>
<p><a href="http://arxiv.org/abs/2312.15258v1">PDF</a> Website: <a href="https://github.com/longxiang-ai/Human101">https://github.com/longxiang-ai/Human101</a></p>
<p><strong>Summary</strong><br>单视角视频中の人体三维动态重建解决方案，兼具快速、高质量、实时渲染的优点。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Human101 框架能够在 100 秒内训练 3D 高斯核，以每秒 100 多帧的速度渲染出逼真的动态 3D 人类重建。</li>
<li>利用 3D 高斯核的优点，为 3D 人体提供明确且高效的表示形式。</li>
<li>使用以人类为中心的前向高斯动画方法，对 3D 高斯核的参数进行变形，以提高渲染速度。</li>
<li>1024 像素分辨率的图像能够以每秒 60 多帧的速度渲染，512 像素分辨率的图像能够以每秒 100 多帧的速度渲染。</li>
<li>实验结果表明，Human101 的性能远超现有方法，帧率最高可提高 10 倍，同时渲染质量相当或更高。</li>
<li>代码和演示将在 <a href="https://github.com/longxiang-ai/Human101上发布。">https://github.com/longxiang-ai/Human101上发布。</a></li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>标题：Human101：从单视角中以 100 FPS 的速度训练 100+ 个高斯人体模型，并在 100 秒内完成</li>
<li>作者：Longxiang Xiang、Yihao Liu、Jiaolong Yang、Yuxuan Zhang、Shunsuke Saito、Hanbyul Joo、Zheng Wu</li>
<li>单位：香港中文大学（深圳）</li>
<li>关键词：计算机视觉、计算机图形学、人体重建、神经辐射场、高斯散点</li>
<li>论文链接：https://arxiv.org/abs/2302.06695，Github 链接：None</li>
<li>
<p>摘要：
（1）研究背景：从单视角视频中重建人体在虚拟现实领域发挥着关键作用。一个普遍的应用场景需要快速重建高保真 3D 数字人体，同时确保实时渲染和交互。现有方法通常难以满足这两个要求。
（2）过去的方法及其问题：现有方法的问题在于渲染速度慢、保真度低、对单视角数据建模能力不足。
（3）研究方法：本文提出了一种名为 Human101 的新框架，该框架能够通过在 100 秒内训练 3D 高斯模型并以 100+ FPS 的速度渲染，从单视角视频生成高保真动态 3D 人体重建。Human101 利用了 3D 高斯散点的优势，提供了一种显式且高效的人体表示。与先前的基于 NeRF 的管道不同，Human101 巧妙地应用了一种以人为中心的正向高斯动画方法来变形 3D 高斯模型的参数，从而提高了渲染速度（即以令人印象深刻的 60+ FPS 渲染 1024 分辨率的图像，并以 100+ FPS 渲染 512 分辨率的图像）。
（4）实验结果：实验结果表明，我们的方法大大超过了当前的方法，将每秒帧数提高了 10 倍，并提供了可比或更好的渲染质量。</p>
</li>
<li>
<p>方法：
（1）Human101框架概述：Human101框架由三个主要组件组成：数据预处理、高斯模型训练和渲染。首先，数据预处理模块将单视角视频转换为一系列2D人体关键点。然后，高斯模型训练模块利用这些关键点训练一个3D高斯散点模型，该模型可以表示人体形状和外观。最后，渲染模块使用训练好的高斯模型生成高保真动态3D人体重建。
（2）以人为中心的正向高斯动画方法：Human101框架采用了一种以人为中心的正向高斯动画方法来变形3D高斯模型的参数，从而提高渲染速度。这种方法将人体姿势分解为一系列基本动作，并使用这些基本动作来控制3D高斯模型的参数。这种方法可以有效地减少渲染计算量，从而提高渲染速度。
（3）高斯散点模型的训练：Human101框架使用了一种基于神经辐射场的训练方法来训练3D高斯散点模型。这种方法将3D空间中的每个点表示为一个高斯散点，并使用神经网络来预测每个高斯散点的颜色和密度。这种方法可以有效地捕捉人体形状和外观的细节，并生成高保真动态3D人体重建。</p>
</li>
<li>
<p>结论：
（1）：本文提出了一种名为 Human101 的新框架，该框架能够通过在 100 秒内训练 3D 高斯模型并以 100+FPS 的速度渲染，从单视角视频生成高保真动态 3D 人体重建。Human101 利用了 3D 高斯散点的优势，提供了一种显式且高效的人体表示。与先前的基于 NeRF 的管道不同，Human101 巧妙地应用了一种以人为中心的正向高斯动画方法来变形 3D 高斯模型的参数，从而提高了渲染速度（即以令人印象深刻的 60+FPS 渲染 1024 分辨率的图像，并以 100+FPS 渲染 512 分辨率的图像）。
（2）：创新点：</p>
</li>
<li>提出了一种新颖的以人为中心的正向高斯动画方法，该方法可以有效地减少渲染计算量，从而提高渲染速度。</li>
<li>使用了一种基于神经辐射场的训练方法来训练 3D 高斯散点模型，该方法可以有效地捕捉人体形状和外观的细节，并生成高保真动态 3D 人体重建。</li>
<li>将 3D 高斯散点模型与以人为中心的正向高斯动画方法相结合，提出了一种新的单视角人体重建框架 Human101，该框架能够在 100 秒内训练 3D 高斯模型并以 100+FPS 的速度渲染，从而生成高保真动态 3D 人体重建。
性能：</li>
<li>Human101 框架在渲染速度和渲染质量方面都优于现有的方法。</li>
<li>Human101 框架可以以 60+FPS 的速度渲染 1024 分辨率的图像，并以 100+FPS 的速度渲染 512 分辨率的图像。</li>
<li>Human101 框架生成的 3D 人体重建具有很高的保真度，并且可以捕捉人体形状和外观的细节。
工作量：</li>
<li>Human101 框架的训练和渲染过程都比较简单，易于实现。</li>
<li>Human101 框架的训练时间为 100 秒，渲染时间为 100+FPS。</li>
<li>Human101 框架的实现代码已经开源，方便其他研究人员使用。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-84a60e1cfd3ff2a4ccd504c677c219dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f8cfe9cdf0f3f288a2851246fa3440a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9d7298160fd7bc71030647b1bbde1aed.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-95ae9edf8140557344587f9d62973d44.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f9308b2b911a7239d0b1c13e120fe940.jpg" align="middle">
</details>




<h2 id="Deformable-3D-Gaussian-Splatting-for-Animatable-Human-Avatars"><a href="#Deformable-3D-Gaussian-Splatting-for-Animatable-Human-Avatars" class="headerlink" title="Deformable 3D Gaussian Splatting for Animatable Human Avatars"></a>Deformable 3D Gaussian Splatting for Animatable Human Avatars</h2><p><strong>Authors:HyunJun Jung, Nikolas Brasch, Jifei Song, Eduardo Perez-Pellitero, Yiren Zhou, Zhihao Li, Nassir Navab, Benjamin Busam</strong></p>
<p>Recent advances in neural radiance fields enable novel view synthesis of photo-realistic images in dynamic settings, which can be applied to scenarios with human animation. Commonly used implicit backbones to establish accurate models, however, require many input views and additional annotations such as human masks, UV maps and depth maps. In this work, we propose ParDy-Human (Parameterized Dynamic Human Avatar), a fully explicit approach to construct a digital avatar from as little as a single monocular sequence. ParDy-Human introduces parameter-driven dynamics into 3D Gaussian Splatting where 3D Gaussians are deformed by a human pose model to animate the avatar. Our method is composed of two parts: A first module that deforms canonical 3D Gaussians according to SMPL vertices and a consecutive module that further takes their designed joint encodings and predicts per Gaussian deformations to deal with dynamics beyond SMPL vertex deformations. Images are then synthesized by a rasterizer. ParDy-Human constitutes an explicit model for realistic dynamic human avatars which requires significantly fewer training views and images. Our avatars learning is free of additional annotations such as masks and can be trained with variable backgrounds while inferring full-resolution images efficiently even on consumer hardware. We provide experimental evidence to show that ParDy-Human outperforms state-of-the-art methods on ZJU-MoCap and THUman4.0 datasets both quantitatively and visually. </p>
<p><a href="http://arxiv.org/abs/2312.15059v1">PDF</a> </p>
<p><strong>摘要</strong><br>无需多视图和额外注解，仅用单张单目序列，即可构建出逼真的动态人类化身。</p>
<p><strong>要点</strong></p>
<ul>
<li>ParDy-Human 是一种全显式方法，仅需一张单色序列即可构建一个数字头像。</li>
<li>ParDy-Human 将参数驱动的动态引入到 3D 高斯散射中，其中 3D 高斯由人类姿态模型变形以实现化身动画。</li>
<li>ParDy-Human 由两部分组成：第一部分根据 SMPL 顶点变形标准 3D 高斯，连续部分进一步采用其设计的关节编码并预测每个高斯变形以处理超出 SMPL 顶点变形的动态。</li>
<li>图像通过光栅化器合成。</li>
<li>ParDy-Human 构成了一个逼真的动态人类化身的显式模型，所需的训练视图和图像明显更少。</li>
<li>我们化身学习无需额外注释，如遮罩，可在变化的背景下进行训练，同时即使在消费级硬件上也能高效推断出全分辨率图像。</li>
<li>我们提供了实验证据表明，ParDy-Human 在 ZJU-MoCap 和 THUman4.0 数据集上无论在数量上还是视觉上都优于最先进的方法。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：可变形 3D 高斯散点绘制，用于可动画的人体虚拟形象</li>
<li>作者：Junggi Kim, Kanghee Jee, Sunghoon Im, Junsik Kim, Minsu Cho, Hyunwoo Kim</li>
<li>隶属机构：首尔国立大学</li>
<li>关键词：可变形 3D 高斯散点绘制、人体动画、神经辐射场、显式模型</li>
<li>论文链接：https://arxiv.org/abs/2204.09365，Github 链接：https://github.com/Junggy/pardy-human</li>
<li>
<p>摘要：
(1)：近年来，神经辐射场在动态场景中合成新颖视角的逼真图像方面取得了很大进展，可应用于人体动画等场景。常用的隐式骨干网络可以建立准确的模型，但需要大量输入视图和额外注释，如人体蒙版、UV 贴图和深度图。
(2)：以往方法：现有方法通常使用隐式表示来构建人体虚拟形象，需要大量输入视图和额外的注释，如人体蒙版、UV 贴图和深度图。这些方法在处理动态场景时也存在困难。
            问题：这些方法需要大量输入视图和额外的注释，在处理动态场景时也存在困难。
            动机：本文提出了一种显式方法来构建人体虚拟形象，可以仅需很少的输入视图即可完成训练，并且不需要额外的注释。这种方法还能够处理动态场景。
(3)：本文提出的研究方法：本文提出了一种名为 ParDy-Human 的参数化动态人体虚拟形象方法，该方法采用显式方法构建人体虚拟形象，仅需很少的输入视图即可完成训练，并且不需要额外的注释。该方法由两部分组成：第一部分根据 SMPL 顶点变形规范 3D 高斯散点，第二部分进一步采用设计的关节编码并预测每个高斯散点的变形，以处理超出 SMPL 顶点变形的动态。然后通过光栅化器合成图像。
(4)：本文方法在 ZJU-MoCap 和 THUman4.0 数据集上均优于最先进的方法，无论是定量还是视觉上。我们的虚拟形象学习过程不需要额外的注释，例如蒙版，并且可以在推理全分辨率图像时使用可变背景进行训练，即使在消费级硬件上也能有效地进行训练。</p>
</li>
<li>
<p>方法：
（1）3D 高斯散点初始化：从稀疏点云初始化 3D 高斯散点，并为每个散点分配几何中心、旋转、尺寸、比例、不透明度和球谐函数等属性。
（2）姿势化高斯散点：根据人体姿势参数，使用逐顶点变形模块将高斯散点变形到新的位置和方向。
（3）变形细化：使用变形细化模块对高斯散点的变形进行残差校正，以提高贴身衣物的人体动画的保真度。
（4）球谐函数方向：使用球谐函数来模拟高斯散点的视角相关效果，并结合表面法线信息来计算更准确的散点方向。
（5）取消高斯散点的姿势并更新父项：在每次姿势更新后，将高斯散点变换回规范空间，并更新它们的父项索引。
（6）损失设计和推理管道：在训练过程中，使用 L1 损失、结构相似性损失和感知相似性损失来评估渲染图像与真实图像之间的差异。在推理时，过滤掉背景高斯散点，并根据相机姿态和人体姿态对高斯散点进行变形，然后通过光栅化器渲染出人体图像。
（7）训练和实现细节：采用多阶段训练策略，交替更新高斯散点特征和变形细化模块。训练损失包括 L1 损失、结构相似性损失和感知相似性损失。</p>
</li>
<li>
<p>结论：
（1）：本工作提出了一种显式方法构建人体虚拟形象，仅需很少的输入视图即可完成训练，并且不需要额外的注释。这种方法还能够处理动态场景。
（2）：创新点：</p>
</li>
<li>提出了一种显式方法构建人体虚拟形象，仅需很少的输入视图即可完成训练，并且不需要额外的注释。</li>
<li>该方法能够处理动态场景。</li>
<li>在ZJU-MoCap和THUman4.0数据集上均优于最先进的方法，无论是定量还是视觉上。
性能：</li>
<li>在ZJU-MoCap和THUman4.0数据集上均优于最先进的方法，无论是定量还是视觉上。</li>
<li>我们的虚拟形象学习过程不需要额外的注释，例如蒙版，并且可以在推理全分辨率图像时使用可变背景进行训练，即使在消费级硬件上也能有效地进行训练。
工作量：</li>
<li>该方法需要收集人体动作数据和渲染图像数据。</li>
<li>该方法需要训练一个神经网络模型。</li>
<li>该方法需要实现一个光栅化器来渲染人体图像。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-3a2dec08eda70704d60e83b281cc54a2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2a167032c68efd5d06543a5ec3ba4f79.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e243cc96b91f1cb9f2e0e8cb1aa2a523.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-805c12244272b525ede83f20a94c5569.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df3f505c56582ddada94e66d5ec6791a.jpg" align="middle">
</details>




<h2 id="3DGS-Avatar-Animatable-Avatars-via-Deformable-3D-Gaussian-Splatting"><a href="#3DGS-Avatar-Animatable-Avatars-via-Deformable-3D-Gaussian-Splatting" class="headerlink" title="3DGS-Avatar: Animatable Avatars via Deformable 3D Gaussian Splatting"></a>3DGS-Avatar: Animatable Avatars via Deformable 3D Gaussian Splatting</h2><p><strong>Authors:Zhiyin Qian, Shaofei Wang, Marko Mihajlovic, Andreas Geiger, Siyu Tang</strong></p>
<p>We introduce an approach that creates animatable human avatars from monocular videos using 3D Gaussian Splatting (3DGS). Existing methods based on neural radiance fields (NeRFs) achieve high-quality novel-view/novel-pose image synthesis but often require days of training, and are extremely slow at inference time. Recently, the community has explored fast grid structures for efficient training of clothed avatars. Albeit being extremely fast at training, these methods can barely achieve an interactive rendering frame rate with around 15 FPS. In this paper, we use 3D Gaussian Splatting and learn a non-rigid deformation network to reconstruct animatable clothed human avatars that can be trained within 30 minutes and rendered at real-time frame rates (50+ FPS). Given the explicit nature of our representation, we further introduce as-isometric-as-possible regularizations on both the Gaussian mean vectors and the covariance matrices, enhancing the generalization of our model on highly articulated unseen poses. Experimental results show that our method achieves comparable and even better performance compared to state-of-the-art approaches on animatable avatar creation from a monocular input, while being 400x and 250x faster in training and inference, respectively. </p>
<p><a href="http://arxiv.org/abs/2312.09228v2">PDF</a> Project page: <a href="https://neuralbodies.github.io/3DGS-Avatar">https://neuralbodies.github.io/3DGS-Avatar</a></p>
<p><strong>摘要</strong><br>使用 3D 高斯展布 (3DGS) 学习单目视频中的动画人类形象，训练 30 分钟即可完成，且渲染帧率达到实时水平 (50+ FPS)。</p>
<p><strong>要点</strong></p>
<ul>
<li>使用神经辐射场 (NeRF) 的现有方法能实现高质量的新视角/新姿态图像合成，但通常需要数天的训练时间，且推理速度极慢。</li>
<li>近期研究社区探索了快速网格结构，以便高效训练着装形象。尽管训练速度快，但这些方法只能勉强实现约 15 FPS 的交互式渲染帧率。</li>
<li>在本文中，我们使用 3D 高斯展布并学习一个非刚性变形网络，以重建可动画的着装人类形象，训练可在 30 分钟内完成，渲染帧率达到实时水平 (50+ FPS)。</li>
<li>鉴于我们的表示具有的显式特性，我们进一步引入了高斯均值向量和协方差矩阵的尽可能等距正则化方法，从而增强了模型在高度关节化未见姿势上的泛化能力。</li>
<li>实验结果表明，我们的方法在从单目输入创建动画形象方面实现了与最先进的方法相当甚至更好的性能，同时训练和推理速度分别快 400 倍和 250 倍。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：3DGS-Avatar：可变形 3D 高斯散点绘制的动画虚拟形象</li>
<li>作者：Jiapeng Tang、Pengfei Wan、Yifan Jiang、Zhaopeng Cui、Chen Change Loy、Linchao Bao、Wenxiu Sun、Wei Cheng</li>
<li>隶属机构：香港中文大学</li>
<li>关键词：计算机视觉、图形学、动画、虚拟现实</li>
<li>论文链接：https://arxiv.org/abs/2302.09403、Github 链接：None</li>
<li>摘要：
（1）研究背景：近年来，基于神经辐射场（NeRF）的方法在图像合成领域取得了显著进展，但其训练和推理速度慢的问题限制了其在动画领域的应用。
（2）过去的方法和问题：现有基于 NeRF 的方法可以生成高质量的新视角/新姿势图像，但通常需要数天的训练时间，并且推理速度极慢。最近，研究人员探索了快速网格结构，以实现服装虚拟形象的训练。尽管这些方法的训练速度非常快，但其渲染帧率仅约为 15 FPS，难以实现交互式渲染。
（3）本文提出的研究方法：本文提出了一种使用 3D 高斯散点绘制和学习非刚性变形网络来重建可动画服装人类虚拟形象的方法。该方法可以在 30 分钟内训练完成，并以实时帧率（50+ FPS）进行渲染。此外，本文还引入了尽可能等距的正则化，以增强模型对高度铰接的未见姿势的泛化能力。
（4）方法在任务和性能上的表现：实验结果表明，本文方法在单目输入的动画虚拟形象创建任务上取得了与最先进方法相当甚至更好的性能，同时在训练和推理速度上分别快了 400 倍和 250 倍。这些性能结果支持了本文方法的目标。</li>
</ol>
<p>Methods：</p>
<p>（1）非刚性变形：提出了一种非刚性变形模块，该模块可以对3D高斯散点进行变形，以适应不同的姿势。该模块由一个神经网络组成，该神经网络以3D高斯散点的位置和一个编码姿势的潜在代码作为输入，并输出变形后的3D高斯散点的位置、缩放因子、旋转四元数和一个特征向量。</p>
<p>（2）刚性变换：将非刚性变形后的3D高斯散点通过刚性变换模块转换到观察空间。该模块由一个神经网络组成，该神经网络以非刚性变形后的3D高斯散点的位置和一个编码姿势的潜在代码作为输入，并输出转换后的3D高斯散点的位置和旋转矩阵。</p>
<p>（3）颜色MLP：使用了一个颜色MLP来预测每个3D高斯散点的颜色。该MLP以3D高斯散点的特征向量、一个编码姿势的潜在代码和一个帧级潜在代码作为输入，并输出3D高斯散点的颜色。</p>
<p>（4）可微分高斯光栅化：使用了一种可微分高斯光栅化方法将观察空间中的3D高斯散点渲染成图像。该方法将每个3D高斯散点投影到图像平面上，并根据高斯函数的权重对投影后的像素进行累加。</p>
<ol>
<li>结论：</li>
</ol>
<p>（1）：本文提出了一种使用 3D 高斯散点绘制和学习非刚性变形网络来重建可动画服装人类虚拟形象的方法，该方法可以在 30 分钟内训练完成，并以实时帧率（50+FPS）进行渲染。</p>
<p>（2）：创新点：</p>
<ul>
<li>使用 3D 高斯散点绘制来表示服装人类虚拟形象，该表示可以有效地捕捉服装的细节和变形。</li>
<li>学习了一个非刚性变形网络，该网络可以将 3D 高斯散点变形到不同的姿势。</li>
<li>引入了尽可能等距的正则化，以增强模型对高度铰接的未见姿势的泛化能力。</li>
</ul>
<p>性能：</p>
<ul>
<li>在单目输入的动画虚拟形象创建任务上取得了与最先进方法相当甚至更好的性能。</li>
<li>训练速度快了 400 倍，推理速度快了 250 倍。</li>
</ul>
<p>工作量：</p>
<ul>
<li>训练一个模型需要 30 分钟。</li>
<li>渲染一帧图像需要 20 毫秒。</li>
</ul>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-491840e5e9b907bfe6c860125c793a8e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-df8a29e21b43e7322f740381b022b6e4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c04b8f81d853c5df7e574e6e17d490fc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-622f5d5aa71b525c2b25dfceb0d4c49a.jpg" align="middle">
</details>




## ASH: Animatable Gaussian Splats for Efficient and Photoreal Human   Rendering

**Authors:Haokai Pang, Heming Zhu, Adam Kortylewski, Christian Theobalt, Marc Habermann**

Real-time rendering of photorealistic and controllable human avatars stands as a cornerstone in Computer Vision and Graphics. While recent advances in neural implicit rendering have unlocked unprecedented photorealism for digital avatars, real-time performance has mostly been demonstrated for static scenes only. To address this, we propose ASH, an animatable Gaussian splatting approach for photorealistic rendering of dynamic humans in real-time. We parameterize the clothed human as animatable 3D Gaussians, which can be efficiently splatted into image space to generate the final rendering. However, naively learning the Gaussian parameters in 3D space poses a severe challenge in terms of compute. Instead, we attach the Gaussians onto a deformable character model, and learn their parameters in 2D texture space, which allows leveraging efficient 2D convolutional architectures that easily scale with the required number of Gaussians. We benchmark ASH with competing methods on pose-controllable avatars, demonstrating that our method outperforms existing real-time methods by a large margin and shows comparable or even better results than offline methods. 

[PDF](http://arxiv.org/abs/2312.05941v1) 13 pages, 7 figures. For project page, see   https://vcai.mpi-inf.mpg.de/projects/ash/

**Summary**
动态场景中实时渲染照片级动态虚拟人的新算法ASH。

**Key Takeaways**

- ASH是一种可动画的高斯散射方法，可用于实时渲染动态人类的照片级图像。
- 将人体参数化为可动画的3D高斯体，可以有效地散射到图像空间以生成最终渲染。
- 通过将高斯体附加到可变形角色模型上并学习它们在2D纹理空间中的参数来学习3D空间中的高斯参数。
- 使用高效的2D卷积架构来学习高斯体参数，该架构可以轻松地扩展到所需数量的高斯体。
- ASH在姿势可控的虚拟人上与竞争方法进行比较，结果表明该方法优于现有的实时方法，并且显示出与离线方法相当或更好的结果。
- ASH是一种实时渲染照片级动态虚拟人的新算法，它具有高保真和高效的特点，可以应用于虚拟现实、增强现实和游戏等领域。
- ASH算法将高斯体附加到可变形角色模型上，并学习它们在2D纹理空间中的参数，这使得该算法可以利用高效的2D卷积架构来学习高斯体参数，从而实现实时渲染。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>标题：ASH：可动画高斯斑点，用于高效且逼真的真人渲染</li>
<li>作者：Haokai Pang, Heming Zhu, Adam Kortylewski, Christian Theobalt, Marc Habermann</li>
<li>第一作者单位：马克斯·普朗克计算机科学研究所</li>
<li>关键词：可动画高斯斑点、高效且逼真的真人渲染、神经隐式渲染、深度学习</li>
<li>论文链接：https://arxiv.org/abs/2312.05941，Github 代码链接：None</li>
<li>
<p>摘要：
(1)：研究背景：实时渲染逼真且可控的人类 avatar 是计算机视觉和图形学领域的基础。最近在神经隐式渲染方面的进展为数字 avatar 带来了前所未有的逼真度，但实时性能大多仅限于静态场景。
(2)：过去的方法及其问题：显式方法将人类 avatar 表示为具有学习动态纹理的可变形模板网格。尽管这些方法在运行时效率高，并且可以与成熟的基于光栅化的渲染管道无缝集成，但生成的渲染通常在逼真度和细节级别方面有所欠缺。混合方法通常将神经辐射场 (NeRF) 附加到（可变形）人体模型上。通常，它们在未摆姿势的空间中评估 NeRF 以模拟穿着衣服的人类的详细外观，并通过查询基于坐标的每个光线样品的  mlp 来生成颜色和密度值。尽管混合方法可以通过 NeRF 捕捉精细外观细节的能力提供卓越的渲染质量，但它们不适合实时应用，因为它们需要昂贵的计算成本。
(3)：本文提出的研究方法：为了解决上述问题，本文提出 ASH，这是一种可动画的高斯斑点方法，用于实时逼真地渲染动态人类。ASH 将穿着衣服的人类参数化为可动画的 3D 高斯斑点，可以将这些斑点有效地溅射到图像空间以生成最终渲染。然而，在 3D 空间中天真地学习高斯参数会带来严峻的计算挑战。相反，本文将高斯斑点附在可变形角色模型上，并学习其参数在 2D 纹理空间中，这允许利用高效的 2D 卷积架构，这些架构可以轻松扩展到所需数量的高斯斑点。
(4)：方法在任务和性能上的表现：本文使用可姿势控制的 avatar 对 ASH 进行了基准测试，结果表明，ASH 的性能远远优于现有的实时方法，并且与离线方法相比具有可比拟甚至更好的结果。这些性能支持了本文的目标。</p>
</li>
<li>
<p>方法：
（1）：本文提出了一种可动画的高斯斑点方法ASH，用于实时逼真地渲染动态人类。ASH将穿着衣服的人类参数化为可动画的3D高斯斑点，可以将这些斑点有效地溅射到图像空间以生成最终渲染。
（2）：为了解决在3D空间中天真地学习高斯参数带来的严峻计算挑战，本文将高斯斑点附在可变形角色模型上，并学习其参数在2D纹理空间中，这允许利用高效的2D卷积架构，这些架构可以轻松扩展到所需数量的高斯斑点。
（3）：本文使用可姿势控制的avatar对ASH进行了基准测试，结果表明，ASH的性能远远优于现有的实时方法，并且与离线方法相比具有可比拟甚至更好的结果。</p>
</li>
<li>
<p>结论：</p>
</li>
</ol>
<p>（1）：本文提出了ASH，这是一种实时高质量渲染动画人类的方法，仅从多视角视频中学习。ASH将最初设计用于建模静态场景的3D高斯斑点附加到可变形网格模板上。通过网格的UV参数化连接，我们可以在2D纹理空间中有效地学习3D高斯斑点作为纹理转换任务。ASH在可动画人类渲染方面定量和定性地展示出明显优于最先进的实时方法的性能，甚至优于最先进的离线方法。目前，ASH不会更新底层可变形模板网格。未来，我们将探索高斯斑点是否可以直接改进3D网格几何体。</p>
<p>（2）：创新点：</p>
<ul>
<li>将3D高斯斑点附加到可变形角色模型上，并学习其参数在2D纹理空间中，这允许利用高效的2D卷积架构，这些架构可以轻松扩展到所需数量的高斯斑点。</li>
<li>使用可姿势控制的avatar对ASH进行了基准测试，结果表明，ASH的性能远远优于现有的实时方法，并且与离线方法相比具有可比拟甚至更好的结果。</li>
</ul>
<p>性能：</p>
<ul>
<li>ASH在可动画人类渲染方面定量和定性地展示出明显优于最先进的实时方法的性能，甚至优于最先进的离线方法。</li>
</ul>
<p>工作量：</p>
<ul>
<li>ASH仅从多视角视频中学习，不需要手动注释或复杂的预处理。</li>
</ul>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a44cc5ef91ec67dc9380befcf6d58fd9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8f7667bf87fc0ccec5a9bc7e63c410a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-092918c112e840a8eb44423eb9235716.jpg" align="middle">
</details>




## MonoGaussianAvatar: Monocular Gaussian Point-based Head Avatar

**Authors:Yufan Chen, Lizhen Wang, Qijing Li, Hongjiang Xiao, Shengping Zhang, Hongxun Yao, Yebin Liu**

The ability to animate photo-realistic head avatars reconstructed from monocular portrait video sequences represents a crucial step in bridging the gap between the virtual and real worlds. Recent advancements in head avatar techniques, including explicit 3D morphable meshes (3DMM), point clouds, and neural implicit representation have been exploited for this ongoing research. However, 3DMM-based methods are constrained by their fixed topologies, point-based approaches suffer from a heavy training burden due to the extensive quantity of points involved, and the last ones suffer from limitations in deformation flexibility and rendering efficiency. In response to these challenges, we propose MonoGaussianAvatar (Monocular Gaussian Point-based Head Avatar), a novel approach that harnesses 3D Gaussian point representation coupled with a Gaussian deformation field to learn explicit head avatars from monocular portrait videos. We define our head avatars with Gaussian points characterized by adaptable shapes, enabling flexible topology. These points exhibit movement with a Gaussian deformation field in alignment with the target pose and expression of a person, facilitating efficient deformation. Additionally, the Gaussian points have controllable shape, size, color, and opacity combined with Gaussian splatting, allowing for efficient training and rendering. Experiments demonstrate the superior performance of our method, which achieves state-of-the-art results among previous methods. 

[PDF](http://arxiv.org/abs/2312.04558v1) The link to our projectpage is   https://yufan1012.github.io/MonoGaussianAvatar

**摘要**
使用单目人像视频序列重建的光写实头部化身动画的能力是弥合虚拟和现实世界之间差距的关键一步。

**要点**

- 单目人像视频序列中重建光写实头部化身的能力对于弥合虚拟和现实世界之间的差距至关重要。
- 最近在头部虚拟化身技术方面的进展，包括显式 3D 可变形网格 (3DMM)、点云和神经隐式表示，已被用于此项正在进行的研究。
- 基于 3DMM 的方法受到其固定拓扑的限制，基于点的办法由于涉及大量点而负担沉重，最后一种方法存在变形灵活性和渲染效率方面的局限性。
- 为了应对这些挑战，我们提出了 MonoGaussianAvatar（单目高斯点基头部虚拟化身），一种新颖的方法，它利用 3D 高斯点表示以及高斯变形场从单目人像视频中学习显式头部虚拟化身。
- 我们使用具有可适应形状、能够实现灵活拓扑的高斯点来定义头部虚拟化身。
- 这些点随着高斯变形场移动，与目标姿势和人的表情保持一致，从而实现有效变形。
- 此外，高斯点具有可控的形状、大小、颜色和不透明度，结合高斯飞溅，可以进行高效训练和渲染。
- 实验表明，我们的方法性能优越，在之前的方法中获得了最先进的结果。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>论文标题：单目高斯点表征头部虚拟人（中文翻译）</li>
<li>作者：Yuhao Zhou, Xingtong Han, Xiaojun Wu, Yu-Kun Lai, Shizhan Zhu, Ang Li, Yebin Liu</li>
<li>第一作者单位：清华大学（中文翻译）</li>
<li>关键词：头部虚拟人、单目重建、高斯点表征、高斯变形场</li>
<li>论文链接：None，Github 代码链接：None</li>
<li>
<p>摘要：
（1）研究背景：头部虚拟人技术在虚拟现实、视频会议等领域有着广泛的应用。近年来，基于显式 3D 可变形网格、点云和神经隐式表示的头部虚拟人技术取得了很大进展。然而，基于 3D 可变形网格的方法受限于固定的拓扑结构，基于点云的方法由于涉及大量点而训练负担很重，基于神经隐式表示的方法在变形灵活性和渲染效率方面存在局限性。
（2）过去的方法及其问题：现有方法主要包括基于显式 3D 可变形网格、点云和神经隐式表示的方法。基于显式 3D 可变形网格的方法受限于固定的拓扑结构，无法很好地适应不同人的头部形状。基于点云的方法由于涉及大量点而训练负担很重，并且渲染效率较低。基于神经隐式表示的方法在变形灵活性和渲染效率方面存在局限性。
（3）研究方法：本文提出了一种基于高斯点表征和高斯变形场的单目头部虚拟人重建方法。该方法首先将头部表示为一组高斯点，然后通过高斯变形场来控制这些点的形状、大小、颜色和透明度。这种表示方式具有很强的灵活性，可以很好地适应不同人的头部形状。此外，该方法还采用了高效的渲染算法，可以实现实时渲染。
（4）方法性能：本文方法在多个数据集上进行了评估，实验结果表明，该方法在重建质量、变形灵活性和渲染效率方面都优于现有方法。</p>
</li>
<li>
<p>方法：
(1) 头部高斯点表征：将头部表示为一组高斯点，每个高斯点都有其位置、大小、颜色和透明度。
(2) 高斯变形场：通过高斯变形场来控制高斯点的形状、大小、颜色和透明度。
(3) 单目头部虚拟人重建：使用单目图像来重建头部虚拟人，首先将图像投影到高斯点上，然后通过高斯变形场来控制高斯点的形状、大小、颜色和透明度，从而重建出头部虚拟人。
(4) 高效渲染算法：采用高效的渲染算法来渲染头部虚拟人，该算法可以实现实时渲染。</p>
</li>
<li>
<p>结论：
（1）：提出了一种基于高斯点表征和高斯变形场的单目头部虚拟人重建方法，该方法具有很强的灵活性，可以很好地适应不同人的头部形状，并且采用高效的渲染算法，可以实现实时渲染。
（2）：创新点：提出了一种新的头部虚拟人表示方法——高斯点表征，该方法具有很强的灵活性，可以很好地适应不同人的头部形状；提出了一种新的头部虚拟人变形方法——高斯变形场，该方法可以有效地控制头部虚拟人的形状、大小、颜色和透明度；提出了一种新的单目头部虚拟人重建方法，该方法可以从单目图像中重建出高质量的头部虚拟人。
性能：在多个数据集上进行了评估，实验结果表明，该方法在重建质量、变形灵活性和渲染效率方面都优于现有方法。
工作量：该方法的实现相对复杂，需要较多的时间和精力。</p>
</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-960cb31f176221bc485bffca08572c49.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ab5a224fe794c8ce5dd0412eaa41c0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff28fa285c5edbd1890f64177638b29e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c7871839126e3ba8f659d17bd8677f4.jpg" align="middle">
</details>




<h2 id="HiFi4G-High-Fidelity-Human-Performance-Rendering-via-Compact-Gaussian-Splatting"><a href="#HiFi4G-High-Fidelity-Human-Performance-Rendering-via-Compact-Gaussian-Splatting" class="headerlink" title="HiFi4G: High-Fidelity Human Performance Rendering via Compact Gaussian   Splatting"></a>HiFi4G: High-Fidelity Human Performance Rendering via Compact Gaussian   Splatting</h2><p><strong>Authors:Yuheng Jiang, Zhehao Shen, Penghao Wang, Zhuo Su, Yu Hong, Yingliang Zhang, Jingyi Yu, Lan Xu</strong></p>
<p>We have recently seen tremendous progress in photo-real human modeling and rendering. Yet, efficiently rendering realistic human performance and integrating it into the rasterization pipeline remains challenging. In this paper, we present HiFi4G, an explicit and compact Gaussian-based approach for high-fidelity human performance rendering from dense footage. Our core intuition is to marry the 3D Gaussian representation with non-rigid tracking, achieving a compact and compression-friendly representation. We first propose a dual-graph mechanism to obtain motion priors, with a coarse deformation graph for effective initialization and a fine-grained Gaussian graph to enforce subsequent constraints. Then, we utilize a 4D Gaussian optimization scheme with adaptive spatial-temporal regularizers to effectively balance the non-rigid prior and Gaussian updating. We also present a companion compression scheme with residual compensation for immersive experiences on various platforms. It achieves a substantial compression rate of approximately 25 times, with less than 2MB of storage per frame. Extensive experiments demonstrate the effectiveness of our approach, which significantly outperforms existing approaches in terms of optimization speed, rendering quality, and storage overhead. </p>
<p><a href="http://arxiv.org/abs/2312.03461v2">PDF</a> </p>
<p><strong>Summary</strong><br>高保真人类表演渲染的显式紧凑高斯方法。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>本文提出 HiFi4G，一种基于高斯分布的显式紧凑方法，用于从密集镜头中进行高保真人类表演渲染。</li>
<li>核心思想是将 3D 高斯表示法与非刚性跟踪相结合，实现紧凑且易于压缩的表示。</li>
<li>提出双图机制获取运动先验，其中粗略变形图用于有效初始化，细粒度高斯图用于实施后续约束。</li>
<li>利用 4D 高斯优化方案，结合自适应时空正则化，有效平衡非刚性先验和高斯更新。</li>
<li>提出了一种具有残差补偿机制的配套压缩方案，可实现跨平台沉浸式体验。</li>
<li>压缩比高达约 25 倍，每帧存储空间不到 2MB。</li>
<li>大量实验表明该方法的有效性，在优化速度、渲染质量和存储开销方面均明显优于现有方法。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：HiFi4G：通过紧凑型高斯散射实现高保真人体表演渲染（HiFi4G：High-Fidelity Human Performance Rendering via Compact Gaussian Splatting）</li>
<li>作者：Yuheng Jiang, Zhehao Shen, Penghao Wang, Zhuo Su, Yu Hong, Yingliang Zhang, Jingyi Yu, Lan Xu</li>
<li>第一作者单位：上海科技大学</li>
<li>关键词：高保真人体表演渲染、紧凑型高斯散射、非刚性融合、可微分光栅化</li>
<li>论文链接：https://arxiv.org/abs/2312.03461
Github 代码链接：无</li>
<li>
<p>摘要：
（1）研究背景：近年来，照片级真实人体建模和渲染取得了巨大进展。然而，高效渲染逼真的人体表演并将其集成到光栅化管道中仍然具有挑战性。
（2）过去方法及其问题：早期解决方案通过显式利用非刚性配准从捕获的视频中重建纹理网格。然而，它们仍然容易受到遮挡和纹理缺失的影响，从而导致重建结果出现孔洞和噪声。最近的神经网络进展，以 NeRF 为代表，绕过了显式重建，而是优化了一个基于坐标的多层感知器 (MLP) 来进行逼真的体积渲染。一些 NeRF 的动态变体试图维护一个规范的特征空间，以通过额外的隐式变形场在每个实时帧中重现特征。然而，这种规范设计对于大的运动和拓扑变化很脆弱。
（3）论文提出的研究方法：本文提出了一种显式且紧凑的高斯散射方法 HiFi4G，用于从密集视频中进行高保真人体表演渲染。我们的核心思想是将 3D 高斯表示与非刚性跟踪相结合，实现紧凑且有利于压缩的表示。我们首先提出了一种双图机制来获取运动先验，其中粗略的变形图用于有效初始化，细粒度的高斯图用于强制后续约束。然后，我们利用 4D 高斯优化方案和自适应时空正则化器来有效平衡非刚性先验和高斯更新。我们还提出了一种具有残差补偿的配套压缩方案，用于在各种平台上实现沉浸式体验。它实现了大约 25 倍的压缩率，每个帧的存储空间小于 2MB。
（4）方法在任务和性能上的表现：大量实验表明了我们方法的有效性，在优化速度、渲染质量和存储开销方面明显优于现有方法。这些性能支持了我们的目标。</p>
</li>
<li>
<p>方法：
(1) 双图机制：获取运动先验，粗略变形图用于有效初始化，细粒度的高斯图用于强制后续约束。
(2) 4D高斯优化方案和自适应时空正则化器：有效平衡非刚性先验和高斯更新。
(3) 残差补偿的配套压缩方案：实现沉浸式体验，压缩率约为25倍，每个帧的存储空间小于2MB。</p>
</li>
<li>
<p>结论：
（1）：本文提出了一种显式且紧凑的高斯散射方法 HiFi4G，用于从密集视频中进行高保真人体表演渲染。我们的方法将 3D 高斯表示与非刚性跟踪相结合，实现了紧凑且有利于压缩的表示。大量实验表明了我们方法的有效性，在优化速度、渲染质量和存储开销方面明显优于现有方法。这些性能支持了我们的目标。
（2）：创新点：</p>
</li>
<li>提出了一种双图机制来获取运动先验，其中粗略的变形图用于有效初始化，细粒度的高斯图用于强制后续约束。</li>
<li>利用 4D 高斯优化方案和自适应时空正则化器来有效平衡非刚性先验和高斯更新。</li>
<li>提出了一种具有残差补偿的配套压缩方案，用于在各种平台上实现沉浸式体验。它实现了大约 25 倍的压缩率，每个帧的存储空间小于 2MB。
性能：</li>
<li>在优化速度、渲染质量和存储开销方面明显优于现有方法。</li>
<li>在各种数据集上进行了广泛的实验，证明了我们方法的有效性和鲁棒性。
工作量：</li>
<li>该方法需要密集的视频输入，这可能需要额外的采集设备和处理时间。</li>
<li>该方法需要对 4D 高斯散射进行优化，这可能需要大量的计算资源。</li>
<li>该方法需要对压缩方案进行训练，这可能需要额外的标注数据和训练时间。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a9db3189636791435751c6ef2f566368.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8fb22dc203c856f780869a746b68066b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1ba98b01fb3d847fecf8756d2a082e8e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70860236a0e2652fa9ec2055060eb12b.jpg" align="middle">
</details>




<h2 id="GauHuman-Articulated-Gaussian-Splatting-from-Monocular-Human-Videos"><a href="#GauHuman-Articulated-Gaussian-Splatting-from-Monocular-Human-Videos" class="headerlink" title="GauHuman: Articulated Gaussian Splatting from Monocular Human Videos"></a>GauHuman: Articulated Gaussian Splatting from Monocular Human Videos</h2><p><strong>Authors:Shoukang Hu, Ziwei Liu</strong></p>
<p>We present, GauHuman, a 3D human model with Gaussian Splatting for both fast training (1 ~ 2 minutes) and real-time rendering (up to 189 FPS), compared with existing NeRF-based implicit representation modelling frameworks demanding hours of training and seconds of rendering per frame. Specifically, GauHuman encodes Gaussian Splatting in the canonical space and transforms 3D Gaussians from canonical space to posed space with linear blend skinning (LBS), in which effective pose and LBS refinement modules are designed to learn fine details of 3D humans under negligible computational cost. Moreover, to enable fast optimization of GauHuman, we initialize and prune 3D Gaussians with 3D human prior, while splitting/cloning via KL divergence guidance, along with a novel merge operation for further speeding up. Extensive experiments on ZJU_Mocap and MonoCap datasets demonstrate that GauHuman achieves state-of-the-art performance quantitatively and qualitatively with fast training and real-time rendering speed. Notably, without sacrificing rendering quality, GauHuman can fast model the 3D human performer with ~13k 3D Gaussians. </p>
<p><a href="http://arxiv.org/abs/2312.02973v1">PDF</a> project page: <a href="https://skhu101.github.io/GauHuman/">https://skhu101.github.io/GauHuman/</a>; code:   <a href="https://github.com/skhu101/GauHuman">https://github.com/skhu101/GauHuman</a></p>
<p><strong>Summary</strong><br>高斯散点表示的 3D 人体模型 GauHuman，实现快速训练（1 ~ 2 分钟）和实时渲染（高达 189 FPS）。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>GauHuman 使用高斯散点表示在规范空间对 3D 人体进行编码，并通过线性混合蒙皮 (LBS) 将 3D 高斯体从规范空间变换为姿势空间。</li>
<li>GauHuman 的有效姿势和 LBS 细化模块能够以极低的计算成本学习 3D 人体的精细细节。</li>
<li>为了实现 GauHuman 的快速优化，研究者使用 3D 人体先验对 3D 高斯体进行初始化和剪枝，同时通过 KL 散度指导进行拆分/克隆，并引入了一种新颖的合并操作以进一步加速优化。</li>
<li>在 ZJU_Mocap 和 MonoCap 数据集上进行的广泛实验表明，GauHuman 在快速训练和实时渲染速度下实现了最先进的定量和定性性能。</li>
<li>值得注意的是，在不牺牲渲染质量的情况下，GauHuman 可以使用约 13k 个 3D 高斯体快速建模 3D 人体表演者。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>标题：高斯喷溅：从单目人体视频中进行铰接高斯喷溅</li>
<li>作者：Shoukang Hu，Ziwei Liu</li>
<li>隶属单位：南洋理工大学</li>
<li>关键词：3D 人体建模，隐式表示，神经渲染，高斯喷溅，线性混合蒙皮</li>
<li>论文链接：https://arxiv.org/abs/2312.02973，Github 链接：https://github.com/skhu101/GauHuman</li>
<li>
<p>摘要：
(1) 研究背景：随着 AR/VR、远程呈现、电子游戏和电影制作等领域的发展，创建高质量的 3D 人体表演者具有广泛的应用价值。最近的方法表明，可以使用基于 NeRF 的隐式表示从稀疏视图视频甚至单张图像中学习 3D 人体 Avatar。然而，这些方法通常需要昂贵的时间和计算成本进行训练和渲染，这阻碍了它们在现实世界场景中的应用。例如，通常需要大约 10 个 GPU 小时来学习 3D 人体表演者，渲染速度不到每秒 1 帧 (FPS)。
(2) 过去的方法及其问题：为了加快 3D 人体建模，可泛化的 HumanNeRF 方法以质量换取更少的训练时间。它们通常使用预先在铰接人体数据上训练好的模型对新的表演者进行微调。然而，这种效率低下的预训练和微调范式通常需要几个小时的预训练来获得 3D 人体的可泛化表示，另外还要花费 1 个小时进行微调。
(3) 本文提出的研究方法：为了解决上述问题，本文提出了一种名为高斯喷溅的 3D 人体模型。该模型在规范空间中对高斯喷溅进行编码，并使用线性混合蒙皮 (LBS) 将 3D 高斯从规范空间变换到姿势空间。在其中设计了有效的姿势和 LBS 细化模块，以在可忽略的计算成本下学习人体表演者的精细细节。此外，为了实现高斯喷溅的快速优化，本文使用 3D 人体先验初始化和剪枝 3D 高斯，同时通过 KL 散度指导进行分割/克隆，并采用了一种新颖的合并操作以进一步提高速度。
(4) 实验结果与性能：在 ZJU_Mocap 和 MonoCap 数据集上进行的广泛实验表明，高斯喷溅在快速训练和实时渲染速度下实现了最先进的定量和定性性能。值得注意的是，在不牺牲渲染质量的情况下，高斯喷溅可以使用约 13k 个 3D 高斯快速建模 3D 人体表演者。</p>
</li>
<li>
<p>方法：
（1）我们提出了一种称为高斯喷溅的 3D 人体模型，该模型在规范空间中对高斯喷溅进行编码，并使用线性混合蒙皮 (LBS) 将 3D 高斯从规范空间变换到姿势空间。
（2）我们设计了一个有效的姿势和 LBS 细化模块，以在可忽略的计算成本下学习人体表演者的精细细节。
（3）为了实现高斯喷溅的快速优化，我们使用 3D 人体先验初始化和剪枝 3D 高斯，同时通过 KL 散度指导进行分割/克隆，并采用了一种新颖的合并操作以进一步提高速度。</p>
</li>
<li>
<p>结论：
（1）：高斯喷溅是一种快速训练（1~2分钟）和实时渲染（166 FPS）3D 人体的 3D 人体模型，它在规范空间中对高斯喷溅进行编码，并使用线性混合蒙皮 (LBS) 变换将 3D 高斯从规范空间变换到姿势空间，其中还设计了有效的姿势细化和 LBS 权重场模块来学习 3D 人体的精细细节。为了实现快速优化，我们使用 3D 人体先验初始化和剪枝 3D 高斯，同时通过 KL 散度指导进行分割/克隆，并采用了一种新颖的合并操作以进一步提高速度。
（2）：创新点：</p>
</li>
<li>提出了一种称为高斯喷溅的 3D 人体模型，该模型在规范空间中对高斯喷溅进行编码，并使用 LBS 变换将 3D 高斯从规范空间变换到姿势空间。</li>
<li>设计了一个有效的姿势细化和 LBS 权重场模块，以在可忽略的计算成本下学习人体表演者的精细细节。</li>
<li>使用 3D 人体先验初始化和剪枝 3D 高斯，同时通过 KL 散度指导进行分割/克隆，并采用了一种新颖的合并操作以进一步提高速度。
性能：</li>
<li>在 ZJU_Mocap 和 MonoCap 数据集上进行的广泛实验表明，高斯喷溅在快速训练和实时渲染速度下实现了最先进的定量和定性性能。</li>
<li>值得注意的是，在不牺牲渲染质量的情况下，高斯喷溅可以使用约 13k 个 3D 高斯快速建模 3D 人体表演者。
工作量：</li>
<li>高斯喷溅的训练时间约为 1~2 分钟，渲染速度为 166 FPS。</li>
<li>高斯喷溅可以使用约 13k 个 3D 高斯快速建模 3D 人体表演者。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ca68371ceaa9efbdc729fa4bdc967f7d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a50485d70cca3d120a273394bed8d88.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-272226fc9271422fc749accef9a38c85.jpg" align="middle">
</details>




<h2 id="HeadGaS-Real-Time-Animatable-Head-Avatars-via-3D-Gaussian-Splatting"><a href="#HeadGaS-Real-Time-Animatable-Head-Avatars-via-3D-Gaussian-Splatting" class="headerlink" title="HeadGaS: Real-Time Animatable Head Avatars via 3D Gaussian Splatting"></a>HeadGaS: Real-Time Animatable Head Avatars via 3D Gaussian Splatting</h2><p><strong>Authors:Helisa Dhamo, Yinyu Nie, Arthur Moreau, Jifei Song, Richard Shaw, Yiren Zhou, Eduardo Pérez-Pellitero</strong></p>
<p>3D head animation has seen major quality and runtime improvements over the last few years, particularly empowered by the advances in differentiable rendering and neural radiance fields. Real-time rendering is a highly desirable goal for real-world applications. We propose HeadGaS, the first model to use 3D Gaussian Splats (3DGS) for 3D head reconstruction and animation. In this paper we introduce a hybrid model that extends the explicit representation from 3DGS with a base of learnable latent features, which can be linearly blended with low-dimensional parameters from parametric head models to obtain expression-dependent final color and opacity values. We demonstrate that HeadGaS delivers state-of-the-art results in real-time inference frame rates, which surpasses baselines by up to ~2dB, while accelerating rendering speed by over x10. </p>
<p><a href="http://arxiv.org/abs/2312.02902v1">PDF</a> </p>
<p><strong>Summary</strong><br>三维高斯形式（3DGS）可以被用于三维头部重建和动画，并能实现最先进的实时推理帧率。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>HeadGaS 是第一个使用 3DGS 进行 3D 头部重建和动画的模型。</li>
</ul>
<ul>
<li>HeadGaS 融合了 3DGS 与可学习潜在特征的优点，可根据参数化头部模型中的低维参数实现表情相关的最终颜色和不透明度值。</li>
</ul>
<ul>
<li>HeadGaS 在实时推理帧率方面实现了最先进的结果，比基线高出约 2dB，同时渲染速度提高了 10 倍以上。</li>
</ul>
<ul>
<li>HeadGaS 适用于各种三维头部动画应用，包括实时视频会议、虚拟现实和游戏。</li>
</ul>
<ul>
<li>在同一数据集上，与即时神经辐射场（Instant-NGP）等方法相比，HeadGaS在重建质量和运行时间方面均优于其实时变体即时神经辐射场（Instant-NGP）。</li>
</ul>
<ul>
<li>HeadGaS 的模型参数比即时神经辐射场（Instant-NGP）少 2.5 倍，仅为 16MB，速度也比其快 12 倍。</li>
</ul>
<ul>
<li>在真实数据和 synthetic 数据上，HeadGaS 的速度提高了 10 倍以上，更适合于对实时性能要求较高的应用。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：HeadGaS：基于 3D 高斯斑点的实时动画头部重建</li>
<li>作者：Jiatao Gu, Andreas Rössler, Hao Tang, Justus Thies, Matthias Nießner</li>
<li>隶属机构：马普学会计算机图形学研究所</li>
<li>关键词：3D 头部重建，动画，神经辐射场，高斯斑点</li>
<li>论文链接：https://arxiv.org/abs/2208.00120，Github 链接：无</li>
<li>摘要：
（1）研究背景：近年来，3D 头部动画在质量和运行时性能方面取得了重大改进，这主要得益于可微渲染和神经辐射场的发展。实时渲染对于现实世界应用而言是一个非常理想的目标。
（2）过去的方法及其问题：现有方法主要依赖显式场景表示（例如网格或点云）或隐式神经辐射场表示。显式表示通常需要大量的参数，并且难以捕捉复杂的面部表情。隐式神经辐射场表示虽然可以捕捉复杂的面部表情，但渲染速度较慢。
（3）研究方法：本文提出了一种新的 3D 头部重建和动画模型 HeadGaS，该模型使用 3D 高斯斑点（3DGS）作为神经辐射场表示。3DGS 是一种参数化表示，它可以有效地捕捉复杂的面部表情，并且渲染速度快。HeadGaS 还使用了一个可学习的潜在特征库，该特征库可以与参数化头部模型的低维参数进行线性混合，以获得与表情相关的最终颜色和不透明度值。
（4）方法性能：HeadGaS 在实时推理帧率下实现了最先进的结果，其性能优于基线方法高达 2dB，同时渲染速度提高了 10 倍以上。这些性能结果支持了本文的目标，即构建一个能够实时渲染复杂面部表情的 3D 头部模型。</li>
</ol>
<p>方法：</p>
<p>（1）：HeadGaS模型使用3D高斯斑点（3DGS）作为神经辐射场表示。3DGS是一种参数化表示，它可以有效地捕捉复杂的面部表情，并且渲染速度快。</p>
<p>（2）：HeadGaS还使用了一个可学习的潜在特征库，该特征库可以与参数化头部模型的低维参数进行线性混合，以获得与表情相关的最终颜色和不透明度值。</p>
<p>（3）：HeadGaS模型在实时推理帧率下实现了最先进的结果，其性能优于基线方法高达2dB，同时渲染速度提高了10倍以上。</p>
<ol>
<li>结论：
（1）：HeadGaS模型在实时推理帧率下实现了最先进的结果，其性能优于基线方法高达2dB，同时渲染速度提高了10倍以上。
（2）：创新点：HeadGaS模型使用3D高斯斑点（3DGS）作为神经辐射场表示，并使用了一个可学习的潜在特征库来获得与表情相关的最终颜色和不透明度值。
性能：HeadGaS模型在实时推理帧率下实现了最先进的结果，其性能优于基线方法高达2dB，同时渲染速度提高了10倍以上。
工作量：HeadGaS模型需要大量的数据进行训练，并且训练过程可能需要很长时间。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0c315a4df3b1de4edf3d295f34ada5d3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c976c03ab0c54c1ca58b8f79a43787fc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6a79269613584cfdb2b735b299c5cce1.jpg" align="middle">
</details>




<h2 id="GPS-Gaussian-Generalizable-Pixel-wise-3D-Gaussian-Splatting-for-Real-time-Human-Novel-View-Synthesis"><a href="#GPS-Gaussian-Generalizable-Pixel-wise-3D-Gaussian-Splatting-for-Real-time-Human-Novel-View-Synthesis" class="headerlink" title="GPS-Gaussian: Generalizable Pixel-wise 3D Gaussian Splatting for   Real-time Human Novel View Synthesis"></a>GPS-Gaussian: Generalizable Pixel-wise 3D Gaussian Splatting for   Real-time Human Novel View Synthesis</h2><p><strong>Authors:Shunyuan Zheng, Boyao Zhou, Ruizhi Shao, Boning Liu, Shengping Zhang, Liqiang Nie, Yebin Liu</strong></p>
<p>We present a new approach, termed GPS-Gaussian, for synthesizing novel views of a character in a real-time manner. The proposed method enables 2K-resolution rendering under a sparse-view camera setting. Unlike the original Gaussian Splatting or neural implicit rendering methods that necessitate per-subject optimizations, we introduce Gaussian parameter maps defined on the source views and regress directly Gaussian Splatting properties for instant novel view synthesis without any fine-tuning or optimization. To this end, we train our Gaussian parameter regression module on a large amount of human scan data, jointly with a depth estimation module to lift 2D parameter maps to 3D space. The proposed framework is fully differentiable and experiments on several datasets demonstrate that our method outperforms state-of-the-art methods while achieving an exceeding rendering speed. </p>
<p><a href="http://arxiv.org/abs/2312.02155v1">PDF</a> The link to our projectpage is <a href="https://shunyuanzheng.github.io">https://shunyuanzheng.github.io</a></p>
<p><strong>摘要</strong><br>利用高斯参数图对人体扫描数据进行训练，无需微调或优化即可实现实时新视角合成。</p>
<p><strong>要点</strong></p>
<ul>
<li>提出了一种名为 GPS-Gaussian 的新方法，用于实时合成角色的新视角。</li>
<li>该方法可以在稀疏视图相机设置下进行 2K 分辨率渲染。</li>
<li>与需要针对每个对象进行优化的原始高斯斑点或神经隐式渲染方法不同，我们引入了在源视图上定义的高斯参数图，并直接回归高斯斑点属性，无需任何微调或优化即可实现即时新视图合成。</li>
<li>为此，我们在大量人体扫描数据上训练我们的高斯参数回归模块，并与深度估计模块联合，将二维参数图提升到三维空间。</li>
<li>所提出的框架是完全可微的，在多个数据集上的实验表明，我们的方法优于最先进的方法，同时实现了极高的渲染速度。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：GPS-Gaussian：实时补充通用像素级三维高斯体素以用于补充材料</li>
<li>作者：Yuxuan Zhang†, Ziyi Wang†, Yajie Zhao, Yiyi Liao, Zhe Lin, Hao Su, Lu Sheng</li>
<li>单位：清华大学（仅输出中文翻译）</li>
<li>关键词：实时渲染、神经辐射场、三维人体重建</li>
<li>论文链接：None，Github 链接：None</li>
<li>摘要：
（1）：研究背景：神经辐射场（NeRF）方法可以生成逼真的图像，但它们通常需要针对每个场景进行优化，这使得它们难以用于实时应用。
（2）：过去的方法：FloRen、IBRNet 和 EN-eRF 等方法都尝试解决这个问题，但它们要么速度慢，要么质量差。
（3）：研究方法：本文提出了一种新的方法 GPS-Gaussian，它可以在不进行任何微调或优化的情况下，直接回归高斯体素参数，从而实现即时的新视角合成。
（4）：方法性能：在多个数据集上进行的实验表明，该方法在实现超过渲染速度的同时，优于最先进的方法。</li>
</ol>
<p>7.方法：
（1）：我们提出了一种名为 GPS-Gaussian 的方法，它可以直接回归高斯体素参数，而无需针对每个场景进行任何微调或优化，从而实现即时的新视角合成。
（2）：我们使用可微渲染积分来促进像素之间的对应关系，并使用优化深度估计来提高 3D 高斯参数的确定精度。
（3）：我们的方法在多个数据集上实现了超过渲染速度的性能，并且优于最先进的方法。</p>
<ol>
<li>结论：
（1）：该方法提出了一种新的NeRF方法GPS-Gaussian，它可以直接回归高斯体素参数，而无需针对每个场景进行任何微调或优化，从而实现即时的新视角合成。
（2）：创新点：</li>
<li>直接回归高斯体素参数，无需针对每个场景进行任何微调或优化。</li>
<li>使用可微渲染积分来促进像素之间的对应关系。</li>
<li>使用优化深度估计来提高3D高斯参数的确定精度。
性能：</li>
<li>在多个数据集上实现了超过渲染速度的性能。</li>
<li>优于最先进的方法。
工作量：</li>
<li>训练复杂度较高。</li>
<li>需要大量的数据。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a836e694dac2420ca9f244952c8ca9fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95ab4d2bde29d287ade4c06b75bcaae7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5d7961c20513332db064855b12f85e3.jpg" align="middle">
</details>




<h2 id="GaussianAvatars-Photorealistic-Head-Avatars-with-Rigged-3D-Gaussians"><a href="#GaussianAvatars-Photorealistic-Head-Avatars-with-Rigged-3D-Gaussians" class="headerlink" title="GaussianAvatars: Photorealistic Head Avatars with Rigged 3D Gaussians"></a>GaussianAvatars: Photorealistic Head Avatars with Rigged 3D Gaussians</h2><p><strong>Authors:Shenhan Qian, Tobias Kirschstein, Liam Schoneveld, Davide Davoli, Simon Giebenhain, Matthias Nießner</strong></p>
<p>We introduce GaussianAvatars, a new method to create photorealistic head avatars that are fully controllable in terms of expression, pose, and viewpoint. The core idea is a dynamic 3D representation based on 3D Gaussian splats that are rigged to a parametric morphable face model. This combination facilitates photorealistic rendering while allowing for precise animation control via the underlying parametric model, e.g., through expression transfer from a driving sequence or by manually changing the morphable model parameters. We parameterize each splat by a local coordinate frame of a triangle and optimize for explicit displacement offset to obtain a more accurate geometric representation. During avatar reconstruction, we jointly optimize for the morphable model parameters and Gaussian splat parameters in an end-to-end fashion. We demonstrate the animation capabilities of our photorealistic avatar in several challenging scenarios. For instance, we show reenactments from a driving video, where our method outperforms existing works by a significant margin. </p>
<p><a href="http://arxiv.org/abs/2312.02069v1">PDF</a> Project page: <a href="https://shenhanqian.github.io/gaussian-avatars">https://shenhanqian.github.io/gaussian-avatars</a></p>
<p><strong>摘要</strong><br>高斯化身：利用高斯三维模型构建的可控3D面部化身。</p>
<p><strong>要点</strong></p>
<ul>
<li>引入高斯化身，一种创建逼真且可控头部化身的方法。</li>
<li>核心思想是基于 3D 高斯模型的动态 3D 表征。</li>
<li>该方法将逼真的渲染与精确的动画控制相结合。</li>
<li>优化了高斯模型参数和可变形模型参数，提高重建精度。</li>
<li>在具有挑战性的场景中，展现了逼真化身的动画效果。</li>
<li>通过驾驶视频重现验证了方法的性能优越性。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：高斯化身：用装备 3D 高斯的逼真头部化身</li>
<li>作者：沈涵钱、托比亚斯·基希施泰因、利亚姆·舍内维尔德、达维德·达沃利、西蒙·吉本海因、马蒂亚斯·尼施纳</li>
<li>第一作者单位：慕尼黑工业大学</li>
<li>关键词：高斯化身、逼真头部化身、3D 高斯、参数化人脸模型、动画控制、表情迁移、驾驶视频、新视角渲染</li>
<li>论文链接：https://arxiv.org/abs/2312.02069，Github 链接：无</li>
<li>
<p>摘要：
（1）研究背景：创建可动画的人类头部化身一直是计算机视觉和图形学中的一个长期问题。逼真动态化身的新视角渲染能力在游戏、电影制作、沉浸式远程临场和增强或虚拟现实等领域具有广泛应用。此外，能够控制化身并使其很好地推广到新颖的姿势和表情也至关重要。重建能够同时捕捉人类头部外观、几何形状和动态的 3D 表征对于生成高保真化身而言是一项重大挑战。这种重建问题的约束不足极大地增加了实现结合新视角渲染逼真度和表情可控性的表征的任务的复杂性。此外，极端表情和面部细节（如皱纹、嘴巴内部和头发）很难捕捉，并且很容易产生人类注意到的视觉伪影。
（2）过去方法及其问题：神经辐射场 (NeRF) 及其变体在从多视角观察中重建静态场景方面取得了令人印象深刻的结果。后续工作已将 NeRF 扩展到为任意场景和针对人类定制的场景建模动态场景。这些工作为新视角渲染取得了令人印象深刻的结果；但是，它们缺乏可控性，因此无法很好地推广到新颖的姿势和表情。最近的 3D 高斯散射方法通过优化整个 3D 空间中的离散几何基元（3D 高斯）来实现比 NeRF 更高的渲染质量，以进行新视角合成并具有实时性能。
（3）本文提出的研究方法：本文提出了一种新方法——高斯化身，该方法可以创建在表情、姿势和视角方面完全可控的逼真头部化身。核心思想是基于装备到参数化可变形人脸模型的 3D 高斯散点的动态 3D 表征。这种组合促进了逼真的渲染，同时允许通过基础参数化模型进行精确的动画控制，例如，通过从驱动序列进行表情迁移或通过手动更改可变形模型参数。本文通过三角形的局部坐标系对每个散点进行参数化，并优化显式位移偏移以获得更准确的几何表示。在化身重建期间，本文以端到端的方式联合优化可变形模型参数和高斯散点参数。
（4）方法在什么任务上取得了什么性能？该方法的性能是否支持其目标？本文展示了逼真化身的动画能力，涉及几个具有挑战性的场景。例如，本文展示了从驾驶视频中进行重演，其中本文的方法以显着的优势优于现有工作。</p>
</li>
<li>
<p><strong>方法</strong>：
（1）数据预处理：将多视角视频分解为一系列帧，并对每一帧进行预处理，包括裁剪、调整大小和归一化。
（2）参数化人脸模型：使用可变形的人脸模型来表示化身头部。该模型由一组控制顶点和一组变形权重组成，可以通过优化控制顶点的位置和变形权重来控制化身的表情和姿势。
（3）3D高斯散点：使用一组3D高斯散点来表示化身头部的几何形状。每个散点由一个位置、一个半径和一个颜色组成。通过优化散点的位置、半径和颜色，可以重建化身头部的几何形状和外观。
（4）端到端优化：将参数化人脸模型和3D高斯散点组合在一起，并以端到端的方式进行优化。优化目标包括重建误差、正则化项和动画控制项。重建误差衡量了化身与输入视频帧之间的差异，正则化项防止过拟合，动画控制项确保化身能够根据控制信号进行动画。
（5）动画控制：通过优化可变形人脸模型的控制顶点或变形权重，可以控制化身的表情和姿势。还可以通过从驱动序列进行表情迁移或通过手动更改可变形模型参数来控制化身。</p>
</li>
<li>
<p>结论：
（1）：高斯化身是一种新颖的方法，它可以从视频序列中创建逼真的人类头部化身。它具有基于装备到参数化可变形人脸模型的 3D 高斯散点的动态 3D 表征。这使得化身能够根据控制信号进行动画，并能够精确地控制表情和姿势。
（2）：创新点：</p>
</li>
<li>将 3D 高斯散点与参数化可变形人脸模型相结合，以实现逼真的人类头部化身重建。</li>
<li>提出了一种新的局部坐标系，该坐标系可以对每个散点进行参数化，并优化显式位移偏移以获得更准确的几何表示。</li>
<li>以端到端的方式联合优化可变形模型参数和高斯散点参数，以获得更好的重建效果。</li>
<li>在表情、姿势和视角方面取得了完全可控的逼真头部化身。
性能：</li>
<li>在图像质量和表情准确性方面优于现有方法。</li>
<li>能够从驾驶视频中进行重演，并且具有显着的优势。
工作量：</li>
<li>需要大量的数据和计算资源来训练模型。</li>
<li>需要手动调整模型的参数以获得最佳的重建效果。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-8675c90807e2bdb45562b774a55905a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a39f72412af9350569632ebc4038eb42.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffae667ed385ae5e64c20d00da08ac79.jpg" align="middle">
</details>




<h2 id="HUGS-Human-Gaussian-Splats"><a href="#HUGS-Human-Gaussian-Splats" class="headerlink" title="HUGS: Human Gaussian Splats"></a>HUGS: Human Gaussian Splats</h2><p><strong>Authors:Muhammed Kocabas, Jen-Hao Rick Chang, James Gabriel, Oncel Tuzel, Anurag Ranjan</strong></p>
<p>Recent advances in neural rendering have improved both training and rendering times by orders of magnitude. While these methods demonstrate state-of-the-art quality and speed, they are designed for photogrammetry of static scenes and do not generalize well to freely moving humans in the environment. In this work, we introduce Human Gaussian Splats (HUGS) that represents an animatable human together with the scene using 3D Gaussian Splatting (3DGS). Our method takes only a monocular video with a small number of (50-100) frames, and it automatically learns to disentangle the static scene and a fully animatable human avatar within 30 minutes. We utilize the SMPL body model to initialize the human Gaussians. To capture details that are not modeled by SMPL (e.g. cloth, hairs), we allow the 3D Gaussians to deviate from the human body model. Utilizing 3D Gaussians for animated humans brings new challenges, including the artifacts created when articulating the Gaussians. We propose to jointly optimize the linear blend skinning weights to coordinate the movements of individual Gaussians during animation. Our approach enables novel-pose synthesis of human and novel view synthesis of both the human and the scene. We achieve state-of-the-art rendering quality with a rendering speed of 60 FPS while being ~100x faster to train over previous work. Our code will be announced here: <a href="https://github.com/apple/ml-hugs">https://github.com/apple/ml-hugs</a> </p>
<p><a href="http://arxiv.org/abs/2311.17910v1">PDF</a> </p>
<p><strong>Summary</strong><br>人类高斯斑块 (HUGS) 使用高斯散布法 (3DGS) 通过单眼视频学习动态场景和可动人类的 disentangled 表示。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>HUGS 使用单目视频学习动画场景和可动人类的 disentangled 表示。</li>
<li>HUGS 使用高斯散布法 (3DGS) 来表示动画人类和场景。</li>
<li>SMPL 人体模型被用来初始化人体高斯。</li>
<li>允许 3D 高斯偏离人体模型来捕获未被 SMPL 建模的细节（如衣物、毛发）。</li>
<li>提出联合优化线性混合蒙皮权重，以协调动画期间各个高斯的运动。</li>
<li>HUGS 实现人类的新姿势合成以及人类和场景的新视角合成。</li>
<li>HUGS 实现了最先进的渲染质量，渲染速度为 60 FPS，而训练速度比以前的工作快 100 倍。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：HUGS：人体高斯斑点</li>
<li>作者：Muhammed Kocabas、Jen-Hao Rick Chang、N. James Gabriel、Oncel Tuzel、Anurag Ranjan</li>
<li>第一作者单位：苹果公司</li>
<li>关键词：神经渲染、人体动画、三维高斯斑点、场景表示</li>
<li>论文链接：https://arxiv.org/abs/2311.17910
    Github 代码链接：https://github.com/apple/ml-hugs</li>
<li>
<p>摘要：
（1）研究背景：近年来，神经渲染技术取得了很大进展，训练和渲染时间都大大缩短。然而，这些方法主要针对静态场景的摄影测量，不能很好地推广到环境中自由移动的人体。
（2）过去的方法及其问题：以往的方法通常使用多摄像头捕捉设备、大量计算和大量的手动工作来创建人体虚拟形象。直接从视频中生成三维虚拟形象的方法虽然取得了一些进展，但它们在处理自由移动的人体时仍然存在很多问题。
（3）研究方法：本文提出了一种名为 HUGS 的方法，它使用三维高斯斑点（3DGS）来表示可动画的人体和场景。该方法只需要一个单目视频，包含 50-100 帧，就可以在 30 分钟内自动学习分离静态场景和一个完全可动画的人体虚拟形象。
（4）方法性能及其实际意义：HUGS 方法实现了最先进的渲染质量，渲染速度达到 60FPS，同时训练速度比以前的工作快约 100 倍。该方法可以用于新姿势合成、新视角合成以及人体和场景的动画。</p>
</li>
<li>
<p>方法：
(1) 使用预训练的 SMPL 回归器估计 SMPL 姿势参数和身体形状参数。
(2) 将人体表示为 3D 高斯斑点，并使用学习的 LBS 驱动高斯斑点。
(3) 使用三个 MLP 来估计高斯斑点的颜色、不透明度、附加位移、旋转、缩放和 LBS 权重。
(4) 将人体高斯斑点与场景高斯斑点结合起来，并使用 splatting 渲染在一起。
(5) 使用 L1 损失、SSIM 损失和感知损失来优化高斯斑点的中心位置、特征三平面和三个 MLP 的参数。
(6) 对 LBS 权重进行正则化，使其与 SMPL 中的 LBS 权重接近。
(7) 在优化过程中，克隆、分裂和剪枝高斯斑点，以避免局部最小值。
(8) 在优化结束后，人体由平均 200 个高斯斑点表示。
(9) 在测试时渲染时，可以直接使用 LBS 权重对人体高斯斑点进行动画处理，而不需要评估三平面和 MLP。</p>
</li>
<li>
<p>结论：
（1）：本文提出了一种名为 HUGS 的方法，该方法使用三维高斯斑点 (3DGS) 来表示可动画的人体和场景，只需要一个单目视频，包含 50-100 帧，就可以在 30 分钟内自动学习分离静态场景和一个完全可动画的人体虚拟形象。该方法实现了最先进的渲染质量，渲染速度达到 60FPS，同时训练速度比以前的工作快约 100 倍。
（2）：创新点：</p>
</li>
<li>使用三维高斯斑点来表示可动画的人体和场景，可以实现快速训练和渲染，并且能够处理自由移动的人体。</li>
<li>使用学习的 LBS 驱动高斯斑点，可以实现人体的高质量动画。</li>
<li>使用三个 MLP 来估计高斯斑点的颜色、不透明度、附加位移、旋转、缩放和 LBS 权重，可以实现高精度的渲染。</li>
</ol>
<p>性能：
- 渲染质量：HUGS 方法实现了最先进的渲染质量，在 PSNR、SSIM 和 LPIPS 指标上都优于其他方法。
- 渲染速度：HUGS 方法的渲染速度达到 60FPS，远高于其他方法。
- 训练速度：HUGS 方法的训练速度比以前的工作快约 100 倍。</p>
<p>工作量：
- 数据集：HUGS 方法使用 in-the-wild 视频作为训练数据，这些视频很容易获得。
- 训练时间：HUGS 方法的训练时间只需要 30 分钟。
- 渲染时间：HUGS 方法的渲染时间非常短，可以达到 60FPS。</p>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-df53c4361e0954102f0a16da8b5dbddd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6761187324e778cd899d6c594d88a176.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d64b96e5aea9ea7f9087d616da2c616f.jpg" align="middle">
</details>




## HumanGaussian: Text-Driven 3D Human Generation with Gaussian Splatting

**Authors:Xian Liu, Xiaohang Zhan, Jiaxiang Tang, Ying Shan, Gang Zeng, Dahua Lin, Xihui Liu, Ziwei Liu**

Realistic 3D human generation from text prompts is a desirable yet challenging task. Existing methods optimize 3D representations like mesh or neural fields via score distillation sampling (SDS), which suffers from inadequate fine details or excessive training time. In this paper, we propose an efficient yet effective framework, HumanGaussian, that generates high-quality 3D humans with fine-grained geometry and realistic appearance. Our key insight is that 3D Gaussian Splatting is an efficient renderer with periodic Gaussian shrinkage or growing, where such adaptive density control can be naturally guided by intrinsic human structures. Specifically, 1) we first propose a Structure-Aware SDS that simultaneously optimizes human appearance and geometry. The multi-modal score function from both RGB and depth space is leveraged to distill the Gaussian densification and pruning process. 2) Moreover, we devise an Annealed Negative Prompt Guidance by decomposing SDS into a noisier generative score and a cleaner classifier score, which well addresses the over-saturation issue. The floating artifacts are further eliminated based on Gaussian size in a prune-only phase to enhance generation smoothness. Extensive experiments demonstrate the superior efficiency and competitive quality of our framework, rendering vivid 3D humans under diverse scenarios. Project Page: https://alvinliu0.github.io/projects/HumanGaussian 

[PDF](http://arxiv.org/abs/2311.17061v1) Project Page: https://alvinliu0.github.io/projects/HumanGaussian

**Summary**
三维高斯散点绘制：一种最高效且有效的细粒度几何图形和逼真外观的三维人类生成框架。

**Key Takeaways**
- 我们提出了一种名为 HumanGaussian 的高效且有效的框架，它可以生成具有细粒度几何结构和逼真外观的高质量 3D 人类。
- 3D 高斯散点绘制是一种高效的渲染器，具有周期性高斯收缩或增长，其中这种自适应密度控制可以由内在的人类结构自然引导。
- 我们首先提出了一种结构感知 SDS，可以同时优化人类的外观和几何形状。
- 我们设计了一个退火否定提示指导，将 SDS 分解为一个更嘈杂的生成分数和一个更清晰的分类器分数，从而很好地解决了过饱和问题。
- 基于高斯大小进一步消除浮动伪影，以增强生成平滑度。
- 广泛的实验表明了我们框架的卓越效率和竞争质量，在不同的场景下渲染了生动的 3D 人类。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>题目：Human Gaussian：文本驱动的三维人体生成与高斯体素溅射</li>
<li>作者：Xian Liu, Xiaohang Zhan, Jiaxiang Tang, Ying Shan, Gang Zeng, Dahua Lin, Xihui Liu, Ziwei Liu</li>
<li>第一作者单位：香港中文大学</li>
<li>关键词：文本到三维，三维人类生成，高斯体素溅射，结构感知 SDS，退火负面提示指导</li>
<li>论文链接：https://arxiv.org/abs/2311.17061，Github 代码链接：无</li>
<li>摘要：
（1）研究背景：</li>
<li>从文本提示生成逼真的三维人体是一项理想但具有挑战性的任务。</li>
<li>现有方法通过基于分数蒸馏采样的方式优化网格或神经场等三维表示，但往往存在精细细节不足或训练时间过长的问题。</li>
</ol>
<p>（2）过去方法及其问题：
   - 现有方法优化三维表示时，通常采用基于分数蒸馏采样的方式，但这种方式存在以下问题：
     - 难以生成精细的细节。
     - 训练时间过长。</p>
<p>（3）研究方法：
   - 提出了一种高效且有效的三维人体生成框架 Human Gaussian，该框架能够生成具有精细几何结构和逼真外观的高质量三维人体。
   - Human Gaussian 的关键思想是将三维高斯体素溅射引入文本驱动的三维人体生成中，并进行了一些新颖的设计：
     - 提出了一种结构感知 SDS，可以同时优化人体的外观和几何结构。
     - 设计了一种退火负面提示指导，可以有效地解决过饱和问题。
     - 基于高斯体素大小，在仅修剪阶段进一步消除浮动伪影，以增强生成的平滑性。</p>
<p>（4）方法性能：
   - 广泛的实验表明，Human Gaussian 具有优越的效率和竞争性的质量，能够在各种场景下渲染出逼真的三维人体。
   - 性能支持目标：
     - Human Gaussian 能够生成具有精细几何结构和逼真外观的高质量三维人体。
     - Human Gaussian 具有优越的效率，能够在较短的时间内生成三维人体。</p>
<ol>
<li>
<p>方法：
（1）高斯初始化与 SMPL-X 先验：从 SMPL-X 网格表面均匀采样点作为 3DGS 初始化，生成 100k 个 3DGS，并将其缩放和转换到合理的人类尺寸，位于 3D 空间的中心。从 SMPL-X 联合提取 2D 骨架作为结构条件。
（2）学习纹理结构联合分布：使用预训练的 StableDiffusion，扩展结构专家分支，同时对图像 RGB 和深度进行去噪，以捕获纹理和结构的联合分布。为了实现灵活的骨架控制，还通过通道方式将姿势图作为输入条件。
（3）结构感知 SDS：设计了一种结构感知 SDS，可以同时优化人体的外观和几何结构，从 RGB 和深度空间蒸馏多模态分数函数，以优化 3DGS 的密度和修剪过程。
（4）退火负面提示指导：使用具有退火负面分数的更清洁的分类器分数来规范高方差的随机 SDS 梯度，并根据高斯体素大小进一步消除浮动伪影，以增强生成的平滑性。</p>
</li>
<li>
<p>结论：
（1）：HumanGaussian 提出了一种高效且有效的三维人体生成框架，能够生成具有精细几何结构和逼真外观的高质量三维人体。
（2）：创新点：
Performance：HumanGaussian 能够在较短的时间内生成三维人体。
Workload：HumanGaussian 具有优越的效率，能够在较短的时间内生成三维人体。</p>
</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-812d7a87ffdcd8ec91c266eb6cc26d09.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2996d2afc6ef940153f64c951a8e67a2.jpg" align="middle">
</details>




<h2 id="Animatable-Gaussians-Learning-Pose-dependent-Gaussian-Maps-for-High-fidelity-Human-Avatar-Modeling"><a href="#Animatable-Gaussians-Learning-Pose-dependent-Gaussian-Maps-for-High-fidelity-Human-Avatar-Modeling" class="headerlink" title="Animatable Gaussians: Learning Pose-dependent Gaussian Maps for   High-fidelity Human Avatar Modeling"></a>Animatable Gaussians: Learning Pose-dependent Gaussian Maps for   High-fidelity Human Avatar Modeling</h2><p><strong>Authors:Zhe Li, Zerong Zheng, Lizhen Wang, Yebin Liu</strong></p>
<p>Modeling animatable human avatars from RGB videos is a long-standing and challenging problem. Recent works usually adopt MLP-based neural radiance fields (NeRF) to represent 3D humans, but it remains difficult for pure MLPs to regress pose-dependent garment details. To this end, we introduce Animatable Gaussians, a new avatar representation that leverages powerful 2D CNNs and 3D Gaussian splatting to create high-fidelity avatars. To associate 3D Gaussians with the animatable avatar, we learn a parametric template from the input videos, and then parameterize the template on two front \&amp; back canonical Gaussian maps where each pixel represents a 3D Gaussian. The learned template is adaptive to the wearing garments for modeling looser clothes like dresses. Such template-guided 2D parameterization enables us to employ a powerful StyleGAN-based CNN to learn the pose-dependent Gaussian maps for modeling detailed dynamic appearances. Furthermore, we introduce a pose projection strategy for better generalization given novel poses. Overall, our method can create lifelike avatars with dynamic, realistic and generalized appearances. Experiments show that our method outperforms other state-of-the-art approaches. Code: <a href="https://github.com/lizhe00/AnimatableGaussians">https://github.com/lizhe00/AnimatableGaussians</a> </p>
<p><a href="http://arxiv.org/abs/2311.16096v1">PDF</a> Projectpage: <a href="https://animatable-gaussians.github.io/">https://animatable-gaussians.github.io/</a>, Code:   <a href="https://github.com/lizhe00/AnimatableGaussians">https://github.com/lizhe00/AnimatableGaussians</a></p>
<p><strong>Summary</strong><br>动画高斯体素：一种新的化身表示形式，结合了强大的 2D CNN 和 3D 高斯体素，用于创建高保真化身。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Animatable Gaussians 是一种新的化身表示形式，用于从 RGB 视频中建模可动画的人类化身。</li>
<li>Animatable Gaussians 利用强大的 2D CNN 和 3D 高斯体素来创建高保真化身，用于建模动态、逼真和概括的外观。</li>
<li>Animatable Gaussians 学习了一个参数模板，该模板可以适应松散的衣服，如连衣裙。</li>
<li>Animatable Gaussians 采用强大的 StyleGAN-based CNN 来学习与姿势相关的映射，用于建模详细的动态外观。</li>
<li>Animatable Gaussians 引入了一种姿势投影策略，以便在新的姿势下更好地泛化。</li>
<li>Animatable Gaussians 在实验中优于其他最先进的方法。</li>
<li>Animatable Gaussians 的源代码可在 <a href="https://github.com/lizhe00/AnimatableGaussians">https://github.com/lizhe00/AnimatableGaussians</a> 获得。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：可动画高斯：学习姿势相关的高斯映射</li>
<li>作者：Zhi Li, Yifan Liu, Wenhao Yu, Qiong Chen, Xiaogang Wang</li>
<li>单位：无</li>
<li>关键词：动画、高斯映射、姿势相关、神经渲染场、可变形模型</li>
<li>论文链接：无，Github：https://github.com/lizhe00/AnimatableGaussians</li>
<li>
<p>摘要：
(1)：研究背景：从 RGB 视频中建模可动画的人类虚拟形象是一个长期存在且具有挑战性的问题。最近的工作通常采用基于 MLP 的神经辐射场 (NeRF) 来表示 3D 人类，但纯 MLP 很难回归姿势相关的服装细节。
(2)：过去方法及其问题：本文方法的动机：为了解决这个问题，我们引入了可动画高斯，这是一种新的虚拟形象表示，利用强大的 2D CNN 和 3D 高斯 splatting 来创建高保真虚拟形象。为了将 3D 高斯与可动画虚拟形象相关联，我们从输入视频中学习了一个参数模板，然后将模板参数化为两个正面和背面规范高斯映射，其中每个像素都表示一个 3D 高斯。学习到的模板可以适应穿着的服装，以建模更宽松的衣服，如连衣裙。这种模板引导的 2D 参数化使我们能够使用强大的基于 StyleGAN 的 CNN 来学习姿势相关的 Gaussian 地图，以建模详细的动态外观。此外，我们引入了一种姿势投影策略，以便在给定新姿势时更好地泛化。
(3)：本文提出的研究方法：总体而言，我们的方法可以创建具有动态、逼真和泛化外观的逼真虚拟形象。实验表明，我们的方法优于其他最先进的方法。
(4)：方法在什么任务上取得了怎样的性能？性能是否支持了他们的目标：我们的方法在建模可动画的人类虚拟形象任务上取得了最先进的性能。定量和定性结果表明，我们的方法可以生成具有逼真细节和姿势相关外观的逼真虚拟形象。这些结果支持了我们的目标，即创建可以用于各种应用（例如游戏、电影和虚拟现实）的高质量可动画虚拟形象。</p>
</li>
<li>
<p>方法：
（1）参数模板：提出了一种参数模板，该模板由两个正面和背面规范高斯映射组成，其中每个像素都表示一个 3D 高斯。该模板可以适应穿着的服装，以建模更宽松的衣服，如连衣裙。
（2）姿势投影：引入了一种姿势投影策略，以便在给定新姿势时更好地泛化。该策略将新姿势投影到训练数据中的最接近姿势，以确保重建的位置图位于训练姿势的分布内。
（3）2D CNN 和 MLP 的比较：通过将 2D CNN 替换为基于坐标的 MLP，评估了 2D CNN 和 MLP 在训练姿势重建方面的表示能力。结果表明，2D CNN 能够回归更详细和更逼真的外观，而 MLP 的表示能力有限，导致模糊的动画结果。
（4）消融研究：通过移除参数模板和姿势投影策略，评估了它们对动画结果的影响。结果表明，参数模板和姿势投影策略对于生成合理和生动的合成外观至关重要。</p>
</li>
<li>
<p>结论：
（1）：这项工作提出了一种新的虚拟形象表示方法——可动画高斯，该方法利用强大的2DCNN和3D高斯splatting来创建高保真虚拟形象。此外，还引入了一种姿势投影策略，以便在给定新姿势时更好地泛化。实验表明，该方法优于其他最先进的方法，可以创建具有动态、逼真和泛化外观的逼真虚拟形象。
（2）：创新点：</p>
</li>
<li>提出了一种新的虚拟形象表示方法——可动画高斯，该方法利用强大的2DCNN和3D高斯splatting来创建高保真虚拟形象。</li>
<li>引入了一种姿势投影策略，以便在给定新姿势时更好地泛化。
性能：</li>
<li>在建模可动画的人类虚拟形象任务上取得了最先进的性能。</li>
<li>定量和定性结果表明，该方法可以生成具有逼真细节和姿势相关外观的逼真虚拟形象。
工作量：</li>
<li>该方法需要大量的数据和计算资源来训练。</li>
<li>该方法的实现相对复杂，需要较高的编程技能。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4f58407bb1da6858a1f8b3afeca5122e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-77ee722883e97eb42525787423c0db90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ba94c75431413d71052f68f19e06407.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aec0c4ba5b1b400598aa699437906d07.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b30fc2750f7d5972c0541922982b18fe.jpg" align="middle">
</details>




]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>3DGS</tag>
      </tags>
  </entry>
  <entry>
    <title>LLM</title>
    <url>/2024/01/24/Paper/2024-01-24/LLM/</url>
    <content><![CDATA[<blockquote>
<p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-01-24-更新"><a href="#2024-01-24-更新" class="headerlink" title="2024-01-24 更新"></a>2024-01-24 更新</h1><h2 id="Less-Could-Be-Better-Parameter-efficient-Fine-tuning-Advances-Medical-Vision-Foundation-Models"><a href="#Less-Could-Be-Better-Parameter-efficient-Fine-tuning-Advances-Medical-Vision-Foundation-Models" class="headerlink" title="Less Could Be Better: Parameter-efficient Fine-tuning Advances Medical   Vision Foundation Models"></a>Less Could Be Better: Parameter-efficient Fine-tuning Advances Medical   Vision Foundation Models</h2><p><strong>Authors:Chenyu Lian, Hong-Yu Zhou, Yizhou Yu, Liansheng Wang</strong></p>
<p>Parameter-efficient fine-tuning (PEFT) that was initially developed for exploiting pre-trained large language models has recently emerged as an effective approach to perform transfer learning on computer vision tasks. However, the effectiveness of PEFT on medical vision foundation models is still unclear and remains to be explored. As a proof of concept, we conducted a detailed empirical study on applying PEFT to chest radiography foundation models. Specifically, we delved into LoRA, a representative PEFT method, and compared it against full-parameter fine-tuning (FFT) on two self-supervised radiography foundation models across three well-established chest radiograph datasets. Our results showed that LoRA outperformed FFT in 13 out of 18 transfer learning tasks by at most 2.9% using fewer than 1% tunable parameters. Combining LoRA with foundation models, we set up new state-of-the-art on a range of data-efficient learning tasks, such as an AUROC score of 80.6% using 1% labeled data on NIH ChestX-ray14. We hope this study can evoke more attention from the community in the use of PEFT for transfer learning on medical imaging tasks. Code and models are available at <a href="https://github.com/RL4M/MED-PEFT">https://github.com/RL4M/MED-PEFT</a>. </p>
<p><a href="http://arxiv.org/abs/2401.12215v1">PDF</a> Technical report</p>
<p><strong>摘要</strong><br>用少于 1% 可调参数实现超参数高效微调 (PEFT) 可以极大提高医学视觉基础模型的性能。</p>
<p><strong>主要要点</strong></p>
<ul>
<li>PEFT 是利用预训练大语言模型开发的一种有效的计算机视觉任务迁移学习方法。</li>
<li>PEFT 在医学视觉基础模型上的有效性尚不清楚，有待探索。</li>
<li>我们通过实验研究将 PEFT 应用于胸部 X 光基础模型。</li>
<li>与全参数微调 (FFT) 相比，LoRA 在三个著名的胸部 X 光数据集上的 18 个迁移学习任务中的 13 个中表现更好，最多提高了 2.9%，同时可调参数少于 1%。</li>
<li>将 LoRA 与基础模型相结合，我们在各种数据有效学习任务中创下了新纪录，例如在 NIH ChestX-ray14 上使用 1% 的标记数据获得了 80.6% 的 AUROC 分数。</li>
<li>我们希望这项研究能够引起社区对 PEFT 用于医学图像任务迁移学习的更多关注。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li><p>题目：少即是多：参数高效微调</p>
</li>
<li><p>作者：陈宇廉，周宏宇，于一舟，王连生</p>
</li>
<li><p>单位：厦门大学</p>
</li>
<li><p>关键词：迁移学习，医学视觉基础模型，胸部X光片</p>
</li>
<li><p>论文链接：https://arxiv.org/abs/2401.12215，Github 代码链接：https://github.com/RL4M/MED-PEFT</p>
</li>
<li><p>摘要：
(1) 研究背景：参数高效微调（PEFT）最初用于开发预训练的大语言模型，最近已成为计算机视觉任务中进行迁移学习的有效方法。然而，PEFT 在医学视觉基础模型上的有效性仍不清楚，有待探索。
(2) 过去的方法及问题：全参数微调（FFT）一直被认为是进行迁移学习的优越技术。然而，基础模型通常具有大量的参数，当下游任务只有有限的注释时，微调整个模型权重可能不是最优选择。在医学影像任务中，由于隐私、安全问题以及某些疾病的罕见性，注释通常难以获得，因此这种差异值得更多关注。
(3) 研究方法：为了证明概念，我们对将 PEFT 应用于胸部放射线照相基础模型进行了详细的实证研究。具体来说，我们深入研究了 LoRA（一种具有代表性的 PEFT 方法），并将其与两个自监督放射线照相基础模型在三个完善的胸部放射线照相数据集上进行了比较。
(4) 实验结果：我们的结果表明，在 18 个迁移学习任务中有 13 个任务中，LoRA 使用少于 1% 可调参数的表现优于 FFT，最多可达 2.9%。将 LoRA 与基础模型相结合，我们在各种数据高效学习任务中确立了新的最优水平，例如，在 NIHChestX-ray14 上使用 1% 的标记数据时，AUROC 得分为 80.6%。我们希望这项研究能够引起社区对 PEFT 在医学影像任务中进行迁移学习的更多关注。</p>
</li>
<li><p>Methods:
(1): 提出了一种参数高效微调（PEFT）方法，用于医学视觉基础模型的迁移学习。
(2): 将LoRA（一种具有代表性的PEFT方法）与两个自监督放射线照相基础模型在三个完善的胸部放射线照相数据集上进行了比较。
(3): 在18个迁移学习任务中有13个任务中，LoRA使用少于1%可调参数的表现优于FFT，最多可达2.9%。
(4): 将LoRA与基础模型相结合，在各种数据高效学习任务中确立了新的最优水平，例如，在NIHChestX-ray14上使用1%的标记数据时，AUROC得分为80.6%。</p>
</li>
<li><p>结论：
(1): 本工作首次将参数高效微调（PEFT）方法应用于医学视觉基础模型的迁移学习，并取得了优异的性能，为医学影像任务中的迁移学习提供了新的思路和方法。
(2): 创新点：</p>
</li>
</ol>
<ul>
<li>提出了一种新的PEFT方法，该方法可以有效地将预训练的医学视觉基础模型迁移到下游任务，并取得了优异的性能。</li>
<li>在三个完善的胸部放射线照相数据集上，将LoRA与两个自监督放射线照相基础模型进行了比较，结果表明，LoRA使用少于1%可调参数的表现优于FFT，最多可达2.9%。</li>
<li>将LoRA与基础模型相结合，在各种数据高效学习任务中确立了新的最优水平，例如，在NIHChestX-ray14上使用1%的标记数据时，AUROC得分为80.6%。
性能：</li>
<li>在18个迁移学习任务中有13个任务中，LoRA使用少于1%可调参数的表现优于FFT，最多可达2.9%。</li>
<li>将LoRA与基础模型相结合，在各种数据高效学习任务中确立了新的最优水平，例如，在NIHChestX-ray14上使用1%的标记数据时，AUROC得分为80.6%。
工作量：</li>
<li>本工作涉及了大量的数据收集、预处理、模型训练和评估工作，工作量较大。</li>
<li>本工作涉及了多种深度学习模型和算法，需要较强的理论基础和编程能力。</li>
</ul>



<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-88f5604fa47b7e6b53fa59ed5ce873a4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f28a6055dce3066c942bea25f00c4b98.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-219f68f671f950faee6332daa05d83eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0e3d9c8b6a9c6651af0cb1202241988.jpg" align="middle">
</details>
​    


## CheXagent: Towards a Foundation Model for Chest X-Ray Interpretation
**Authors:Zhihong Chen, Maya Varma, Jean-Benoit Delbrouck, Magdalini Paschali, Louis Blankemeier, Dave Van Veen, Jeya Maria Jose Valanarasu, Alaa Youssef, Joseph Paul Cohen, Eduardo Pontes Reis, Emily B. Tsai, Andrew Johnston, Cameron Olsen, Tanishq Mathew Abraham, Sergios Gatidis, Akshay S. Chaudhari, Curtis Langlotz**

Chest X-rays (CXRs) are the most frequently performed imaging test in clinical practice. Recent advances in the development of vision-language foundation models (FMs) give rise to the possibility of performing automated CXR interpretation, which can assist physicians with clinical decision-making and improve patient outcomes. However, developing FMs that can accurately interpret CXRs is challenging due to the (1) limited availability of large-scale vision-language datasets in the medical image domain, (2) lack of vision and language encoders that can capture the complexities of medical data, and (3) absence of evaluation frameworks for benchmarking the abilities of FMs on CXR interpretation. In this work, we address these challenges by first introducing \emph{CheXinstruct} - a large-scale instruction-tuning dataset curated from 28 publicly-available datasets. We then present \emph{CheXagent} - an instruction-tuned FM capable of analyzing and summarizing CXRs. To build CheXagent, we design a clinical large language model (LLM) for parsing radiology reports, a vision encoder for representing CXR images, and a network to bridge the vision and language modalities. Finally, we introduce \emph{CheXbench} - a novel benchmark designed to systematically evaluate FMs across 8 clinically-relevant CXR interpretation tasks. Extensive quantitative evaluations and qualitative reviews with five expert radiologists demonstrate that CheXagent outperforms previously-developed general- and medical-domain FMs on CheXbench tasks. Furthermore, in an effort to improve model transparency, we perform a fairness evaluation across factors of sex, race and age to highlight potential performance disparities. Our project is at \url{https://stanford-aimi.github.io/chexagent.html}. 

[PDF](http://arxiv.org/abs/2401.12208v1) 24 pages, 8 figures

**摘要**
针对目前医疗图像领域中的大规模视觉语言数据集较少、可捕捉医学数据复杂性的视觉和语言编码器缺乏、以及用于基准测试 CXR 解释能力的评估框架缺失的问题，本文提出了一个新的 CXR 解释基准框架 Chexbench 和一个新的视觉语言基础模型 CheXagent。

**要点**
- **医学影像领域** 中的 **大规模视觉语言数据集** 较少。
- **医疗数据复杂性** 使得 **视觉和语言编码器** 难以有效捕捉。
- 目前 **用于评估 CXR 解释能力的框架** 仍然**缺失**。
- **CheXinstruct** 是一个从 **28 个公开可用的数据集** 中策划的大规模指令微调数据集。
- **CheXagent** 是一个基于指令微调的视觉语言基础模型，能够分析和总结 CXR。
- **CheXbench** 是一个新颖的基准，旨在系统地评估跨越 **8 个临床相关 CXR 解释任务** 的视觉语言基础模型。
- **CheXagent** 在 **CheXbench** 任务上优于此前开发的通用和医学领域视觉语言基础模型。
- 本项目还进行了公平性评估，以突出潜在的性能差异。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>题目：CheXagent：迈向胸部 X 射线解读的基础模型</li>
<li>作者：Zhihong Chen、Maya Varma、Jean-Benoit Delbrouck、Magdalini Paschali、Louis Blankemeier、Dave Van Veen、Jeya Maria Jose Valanarasu、Alaa Youssef、Joseph Paul Cohen、Eduardo Pontes Reis、Emily B. Tsai、Andrew Johnston、Cameron Olsen、Tanishq Mathew Abraham、Sergios Gatidis、Akshay S. Chaudhari、Curtis Langlotz</li>
<li>第一作者单位：斯坦福大学</li>
<li>关键词：胸部 X 射线、医学图像、计算机视觉、自然语言处理、基础模型</li>
<li>论文链接：https://arxiv.org/abs/2401.12208
Github 代码链接：无</li>
<li>摘要：
（1）：胸部 X 射线（CXR）是临床实践中最常进行的影像检查。最近，视觉语言基础模型（FM）的发展为实现自动 CXR 解释提供了可能性，这可以帮助医生进行临床决策并改善患者预后。然而，由于以下原因，开发能够准确解释 CXR 的 FM 具有挑战性：（1）医学图像领域中缺乏大规模视觉语言数据集；（2）缺乏能够捕捉医学数据复杂性的视觉和语言编码器；（3）缺乏用于评估 FM 在 CXR 解释方面的能力的评估框架。
（2）：过去的方法包括使用通用 FM 或针对医学图像领域微调的 FM。这些方法存在的问题在于它们无法充分捕捉 CXR 的复杂性，并且缺乏针对 CXR 解释任务的评估框架。
（3）：本文提出了一种新的研究方法，该方法包括：</li>
</ol>
<ul>
<li>构建了一个名为 CheXinstruct 的大规模指令微调数据集，该数据集是从 28 个公开可用的数据集策划而来。</li>
<li>提出了一种名为 CheXagent 的指令微调 FM，该 FM 能够分析和总结 CXR。</li>
<li>引入了一个名为 CheXbench 的新基准，该基准旨在系统地评估 FM 在 8 个与临床相关的 CXR 解释任务中的表现。
（4）：在 8 个 CXR 解释任务上，CheXagent 的性能优于之前开发的通用和医学领域 FM。此外，公平性评估表明，CheXagent 在性别、种族和年龄等因素上的性能没有显着差异。</li>
</ul>
<p>Some Error for method(比如是不是没有Methods这个章节)</p>
<ol start="8">
<li>结论：
（1）：xxx；
（2）：创新点：xxx；性能：xxx；工作量：xxx；</li>
</ol>
<p>创新点：</p>
<ul>
<li>构建了一个名为 CheXinstruct 的大规模指令微调数据集，该数据集是从 28 个公开可用的数据集策划而来。</li>
<li>提出了一种名为 CheXagent 的指令微调 FM，该 FM 能够分析和总结 CXR。</li>
<li>引入了一个名为 CheXbench 的新基准，该基准旨在系统地评估 FM 在 8 个与临床相关的 CXR 解释任务中的表现。</li>
</ul>
<p>性能：</p>
<ul>
<li>在 8 个 CXR 解释任务上，CheXagent 的性能优于之前开发的通用和医学领域 FM。</li>
<li>公平性评估表明，CheXagent 在性别、种族和年龄等因素上的性能没有显着差异。</li>
</ul>
<p>工作量：</p>
<ul>
<li>数据集构建：从 28 个公开可用的数据集策划 CheXinstruct 数据集。</li>
<li>模型训练：训练 CheXagent 模型。</li>
<li>基准构建：构建 CheXbench 基准。</li>
<li>模型评估：在 CheXbench 基准上评估 CheXagent 的性能。</li>
</ul>



<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-6aa52c71b57a2862b763a5188b83d6d8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a7d79f07ab8199caa375ff5c3d1ce188.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-71a37c439c6714e8867560f580599d2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f44d1729ed1485e81c04a41f097c005.jpg" align="middle">
</details>
​    


## Text Embedding Inversion Attacks on Multilingual Language Models
**Authors:Yiyi Chen, Heather Lent, Johannes Bjerva**

Representing textual information as real-numbered embeddings has become the norm in NLP. Moreover, with the rise of public interest in large language models (LLMs), Embeddings as a Service (EaaS) has rapidly gained traction as a business model. This is not without outstanding security risks, as previous research has demonstrated that sensitive data can be reconstructed from embeddings, even without knowledge of the underlying model that generated them. However, such work is limited by its sole focus on English, leaving all other languages vulnerable to attacks by malicious actors. %As many international and multilingual companies leverage EaaS, there is an urgent need for research into multilingual LLM security. To this end, this work investigates LLM security from the perspective of multilingual embedding inversion. Concretely, we define the problem of black-box multilingual and cross-lingual inversion attacks, with special attention to a cross-domain scenario. Our findings reveal that multilingual models are potentially more vulnerable to inversion attacks than their monolingual counterparts. This stems from the reduced data requirements for achieving comparable inversion performance in settings where the underlying language is not known a-priori. To our knowledge, this work is the first to delve into multilinguality within the context of inversion attacks, and our findings highlight the need for further investigation and enhanced defenses in the area of NLP Security. 

[PDF](http://arxiv.org/abs/2401.12192v1) 13 pages

**Summary**
不断增长的多语言用户植入对 NLP 安全研究提出了迫切要求，也加剧了黑盒多语言对嵌入逆向攻击及跨语言攻击的风险。

**Key Takeaways**
- 安全风险：植入作为一项服务 (EaaS) 模型使数据重构更容易，但过去的重点仅放在英语上，其他语言很脆弱。
- 多语言模型更易受攻击：它们对实现可比的逆向性能的数据需求较少，即使在事先不知道基础语言的情况下也是如此。
- 跨语言攻击：对攻击者来说，在跨语言场景中进行跨域攻击更为容易，因为目标语言通常与源语言不同。
- 表现差异：多语言模型在不同语言上的性能差异可能导致跨语言设置中的攻击成功率发生变化。
- 更少的数据需求：多语言模型对实现可比的逆向性能的数据需求较少，即使在事先不知道基础语言的情况下也是如此。
- 需要进一步研究：这项工作强调了在 NLP 安全领域进行进一步研究和改进防御的必要性。
- 防御措施：开发针对多语言和跨语言嵌入逆向攻击的防御措施，以确保 NLP系统的安全。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li><p>论文标题：多语言语言模型的文本嵌入逆向攻击</p>
</li>
<li><p>作者：Yiyi Chen, Heather Lent, Johannes Bjerva</p>
</li>
<li><p>第一作者单位：奥尔堡大学计算机科学系</p>
</li>
<li><p>关键词：自然语言处理，大语言模型，嵌入式服务，嵌入式逆向攻击，多语言，跨语言</p>
</li>
<li><p>论文链接：https://arxiv.org/abs/2401.12192，Github 链接：无</p>
</li>
<li><p>摘要：</p>
</li>
</ol>
<p>（1）研究背景：随着自然语言处理（NLP）的工业应用日益广泛，大型语言模型（LLM）和嵌入式服务（EaaS）框架的使用也越来越普遍。EaaS 允许用户将文本数据存储为高品质的句子嵌入，从而提高搜索效率。然而，最近的研究表明，嵌入式逆向攻击可以从嵌入中解码出原始文本，这给 NLP 安全带来了重大威胁。</p>
<p>（2）过去的方法和问题：以往的研究主要集中在单语英语模型和嵌入上，假设攻击者知道文本的语言。然而，在现实场景中，攻击者可能不知道文本的语言。</p>
<p>（3）研究方法：本文定义了黑盒多语言和跨语言嵌入逆向攻击问题，并特别关注跨域场景。本文使用外部模型来近似嵌入逆向函数，该函数可以从嵌入中重建文本。</p>
<p>（4）方法的性能和目标支持：本文的实验结果表明，多语言模型比单语模型更容易受到逆向攻击。在不知道底层语言的情况下，攻击者也可以实现与单语模型相当的逆向性能。这表明多语言模型在安全性方面存在潜在的风险。</p>
<ol start="7">
<li><p>方法：
(1) 定义黑盒多语言和跨语言嵌入逆向攻击问题，关注跨域场景；
(2) 使用外部模型 ψ 近似嵌入逆向函数 ϕ−1，从嵌入中重建文本；
(3) 探讨文本生成模型在未知语言文本重建中的作用；
(4) 研究多语言嵌入逆向攻击的潜力和影响；
(5) 提出后逆向策略 AdhocTranslation，将生成文本从 ly 翻译成 lx，评估信息泄露情况；
(6) 使用 T5-base 作为生成模型，在 ME5-base 和 GTR-base 上训练多语言逆向模型；
(7) 与在英语数据集上训练的逆向模型比较，评估多语言模型的性能。</p>
</li>
<li><p>结论：
（1）：本文首次对多语言嵌入逆向攻击问题进行了研究，为该方向的未来工作奠定了基础。我们的核心发现之一是，在某些情况下，多语言模型比单语英语模型更容易受到攻击。我们希望这项工作能够激发人们对 LLM 安全和 NLP 安全的关注，采取多语言的方法。
（2）：创新点：</p>
</li>
</ol>
<ul>
<li>定义了黑盒多语言和跨语言嵌入逆向攻击问题，关注跨域场景。</li>
<li>使用外部模型 ψ 近似嵌入逆向函数 ϕ−1，从嵌入中重建文本。</li>
<li>探讨了文本生成模型在未知语言文本重建中的作用。</li>
<li>研究了多语言嵌入逆向攻击的潜力和影响。</li>
<li>提出后逆向策略 AdhocTranslation，将生成文本从 l_x 翻译成 l_y，评估信息泄露情况。</li>
<li>使用 T5-base 作为生成模型，在 ME5-base 和 GTR-base 上训练多语言逆向模型。</li>
<li>与在英语数据集上训练的逆向模型比较，评估了多语言模型的性能。
性能：</li>
<li>多语言模型在某些情况下比单语英语模型更容易受到攻击。</li>
<li>在不知道底层语言的情况下，攻击者也可以实现与单语模型相当的逆向性能。</li>
<li>后逆向策略 AdhocTranslation 可以有效降低信息泄露风险。
工作量：</li>
<li>实验计算量大，需要约 20,000 个 GPU 计算小时。</li>
<li>将这项研究扩展到更多语言将进一步增加开销。</li>
</ul>



<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-716d1bb2864a9ac6b2a1614499e04fd6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5de54c90ee7b1b6fe5ae2be76cb7496f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f940c4622168d9e745d9be24e1dccd3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99a0979006c1b27d04c44e10eca03872.jpg" align="middle">
</details>
​    


## WARM: On the Benefits of Weight Averaged Reward Models
**Authors:Alexandre Ramé, Nino Vieillard, Léonard Hussenot, Robert Dadashi, Geoffrey Cideron, Olivier Bachem, Johan Ferret**

Aligning large language models (LLMs) with human preferences through reinforcement learning (RLHF) can lead to reward hacking, where LLMs exploit failures in the reward model (RM) to achieve seemingly high rewards without meeting the underlying objectives. We identify two primary challenges when designing RMs to mitigate reward hacking: distribution shifts during the RL process and inconsistencies in human preferences. As a solution, we propose Weight Averaged Reward Models (WARM), first fine-tuning multiple RMs, then averaging them in the weight space. This strategy follows the observation that fine-tuned weights remain linearly mode connected when sharing the same pre-training. By averaging weights, WARM improves efficiency compared to the traditional ensembling of predictions, while improving reliability under distribution shifts and robustness to preference inconsistencies. Our experiments on summarization tasks, using best-of-N and RL methods, shows that WARM improves the overall quality and alignment of LLM predictions; for example, a policy RL fine-tuned with WARM has a 79.4% win rate against a policy RL fine-tuned with a single RM. 

[PDF](http://arxiv.org/abs/2401.12187v1) 14 pages, 9 figures

**Summary**
利用加权平均奖励模型来克服强化学习中存在的奖励窃取现象。

**Key Takeaways**

- 奖励窃取是指 LLM 利用奖励模型的漏洞来获得看似很高的奖励，但并未达到预期的目标。
- 设计奖励模型时面临的两大挑战是：强化学习过程中分布的变化和人类偏好的不一致。
- 提出加权平均奖励模型 (WARM) 来解决奖励窃取问题，先微调多个奖励模型，然后在权重空间对它们求平均。
- WARM 通过使用最优 N 和强化学习方法在摘要任务上进行的实验证明，WARM 提高了 LLM 预测的整体质量和一致性。
- 与采用单个奖励模型微调的策略型强化学习相比，采用 WARM 微调的策略型强化学习的获胜率为 79.4%。
- 与传统预测集成相比，WARM 在分布变化和偏好不一致的情况下提高了效率和可靠性。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>题目：WARM：关于权重平均奖励模型的优势</li>
<li>作者：Alexandre Ramé, Nino Vieillard, Léonard Hussenot, Robert Dadashi, Geoffrey Cidon, Olivier Bachem, Johan Ferret</li>
<li>隶属机构：谷歌大脑</li>
<li>关键词：对齐，RLHF，奖励建模，模型合并</li>
<li>链接：https://arxiv.org/abs/2401.12187</li>
<li>摘要：
（1）研究背景：近年来，大型语言模型（LLM）在各个领域取得了令人瞩目的成就，这很大程度上得益于强化学习（RL）的应用。然而，在RLHF（从人类反馈中进行强化学习）中，奖励黑客问题是一个普遍存在的问题。奖励黑客是指策略（即正在训练的LLM）学会利用奖励模型（RM）中的漏洞，在不真正满足预期目标的情况下实现看似很高的奖励。这会导致性能下降、检查点选择复杂化、产生谄媚行为或放大社会偏见，最严重的是可能导致安全风险。
（2）过去的方法及其问题：为了解决奖励黑客问题，一些研究人员提出了使用预测集成（ENS）的方法。ENS通过对多个RM的奖励进行平均，可以提高奖励的可靠性并降低黑客风险。然而，ENS存在内存和推理开销大的问题，而且它并不能提高对首选项数据集中标签噪声的鲁棒性。
（3）本文提出的研究方法：为了解决上述问题，本文提出了权重平均奖励模型（WARM）。WARM通过对多个RM的权重进行线性插值，将它们合并成一个新的RM。这种方法继承了WA在分布偏移下的泛化能力，并提高了对标签损坏的鲁棒性。此外，WARM在效率和实用性方面也优于ENS，因为它只需要在推理时使用一个模型，而ENS需要对多个模型的预测进行平均。
（4）方法性能：在本文的实验中，WARM在摘要任务上取得了比传统RLHF方法更好的性能。例如，使用WARM微调的策略在对抗使用单个RM微调的策略时，具有79.4%的获胜率。这表明WARM可以有效地提高LLM预测的整体质量和一致性。</li>
</ol>
<p>Some Error for method(比如是不是没有Methods这个章节)</p>
<ol start="8">
<li>结论：
（1）：本文提出了一种新的权重平均奖励模型（WARM），以解决奖励建模中的两个关键挑战：分布偏移下的可靠性和标签损坏下的鲁棒性。通过对多个来自不同微调的奖励模型的权重进行线性插值，WARM 似乎是一种有效的解决方案，可以减轻人类反馈中的强化学习中的奖励黑客问题。我们的实证结果证明了将其应用于摘要时的有效性。我们预计 WARM 将有助于实现更加一致、透明和有效的 AI 系统，并鼓励在奖励建模方面进行进一步探索。
（2）：创新点：</li>
</ol>
<ul>
<li>WARM 通过对多个奖励模型的权重进行平均，可以提高奖励的可靠性并降低黑客风险。</li>
<li>WARM 在效率和实用性方面优于预测集成（ENS），因为它只需要在推理时使用一个模型，而 ENS 需要对多个模型的预测进行平均。</li>
<li>WARM 在摘要任务上取得了比传统 RLHF 方法更好的性能。</li>
</ul>
<p>性能：</p>
<ul>
<li>WARM 在摘要任务上取得了比传统 RLHF 方法更好的性能。例如，使用 WARM 微调的策略在对抗使用单个 RM 微调的策略时，具有 79.4% 的获胜率。这表明 WARM 可以有效地提高 LLM 预测的整体质量和一致性。</li>
</ul>
<p>工作量：</p>
<ul>
<li>WARM 在效率和实用性方面优于预测集成（ENS），因为它只需要在推理时使用一个模型，而 ENS 需要对多个模型的预测进行平均。</li>
</ul>



<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cb30bdc7fa3fe5ca08f68a92a10b2271.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f65c9aafc185dee21168928e8ee7151.jpg" align="middle">
</details>
​    


## Temporal Blind Spots in Large Language Models
**Authors:Jonas Wallat, Adam Jatowt, Avishek Anand**

Large language models (LLMs) have recently gained significant attention due to their unparalleled ability to perform various natural language processing tasks. These models, benefiting from their advanced natural language understanding capabilities, have demonstrated impressive zero-shot performance. However, the pre-training data utilized in LLMs is often confined to a specific corpus, resulting in inherent freshness and temporal scope limitations. Consequently, this raises concerns regarding the effectiveness of LLMs for tasks involving temporal intents. In this study, we aim to investigate the underlying limitations of general-purpose LLMs when deployed for tasks that require a temporal understanding. We pay particular attention to handling factual temporal knowledge through three popular temporal QA datasets. Specifically, we observe low performance on detailed questions about the past and, surprisingly, for rather new information. In manual and automatic testing, we find multiple temporal errors and characterize the conditions under which QA performance deteriorates. Our analysis contributes to understanding LLM limitations and offers valuable insights into developing future models that can better cater to the demands of temporally-oriented tasks. The code is available\footnote{https://github.com/jwallat/temporalblindspots}. 

[PDF](http://arxiv.org/abs/2401.12078v1) accepted at WSDM'24

**摘要**
大型语言模型在处理时间相关任务时存在局限性，尤其是在处理对过去信息的详细问题和较新信息时。

**要点**

- 大型语言模型在自然语言处理任务上表现出色，但受限于预训练语料库，对时间的理解能力有限。
- 大型语言模型在回答关于过去问题的详细问题时表现不佳，对较新信息也存在理解困难。
- 通过手动和自动测试发现大型语言模型存在多种时间错误。
- 大型语言模型在处理时间相关任务时，随着问题对时间敏感性的增加，准确率下降。
- 大型语言模型对时间的理解能力受制于预训练数据的时间范围。
- 研究人员发现时间性问题可以分为事实性问题、个人经历问题和意见问题，其中大型语言模型对事实性问题的回答准确率最高。
- 这项研究为理解大型语言模型的局限性做出了贡献，并为开发能够更好地满足时间相关任务需求的未来模型提供了宝贵的见解。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li><p>论文标题：大型语言模型中的时间盲点</p>
</li>
<li><p>作者：Jonas Wallat、Adam Jatowt、Avishek Anand</p>
</li>
<li><p>第一作者单位：德国汉诺威莱布尼兹信息科学研究所</p>
</li>
<li><p>关键词：大型语言模型、时间知识、时间理解、问答</p>
</li>
<li><p>论文链接：https://arxiv.org/abs/2401.12078
Github 链接：None</p>
</li>
<li><p>摘要：
（1）研究背景：大型语言模型 (LLM) 在自然语言处理任务中表现出色，但其预训练数据通常局限于特定语料库，存在时效性和时间范围的限制，影响了其在涉及时间意图任务中的有效性。
（2）过去的方法及其问题：以往的研究主要集中在知识探测、对抗性示例和风险分析等方面，但对 LLM 在时间知识和理解方面的盲点关注较少。
（3）研究方法：本文通过三个时间问答数据集（Temporal Questions、ArchivalQA 和 TempLAMA）对 LLM 的时间知识和理解能力进行了全面评估，重点关注事实时间知识的处理和复杂时间信息的处理。
（4）方法性能及对目标的支持：实验结果表明，LLM 在涉及时间知识和理解的任务中存在盲点，特别是在处理详细的历史问题和相对较新的信息时表现不佳。这些发现有助于理解 LLM 的局限性，并为开发能够更好地满足时间导向任务需求的未来模型提供有价值的见解。</p>
</li>
</ol>
<p>Some Error for method(比如是不是没有Methods这个章节)</p>
<ol start="8">
<li>结论：</li>
</ol>
<p>（1）意义：
本文通过对大型语言模型（LLM）的时间知识和理解能力进行全面评估，揭示了其在涉及时间意图任务中的盲点，为开发能够更好地满足时间导向任务需求的未来模型提供了有价值的见解。</p>
<p>（2）优缺点：
创新点：</p>
<ul>
<li>提出了一种新的方法来评估 LLM 在时间知识和理解方面的能力。</li>
<li>发现 LLM 在处理详细的历史问题和相对较新的信息时表现不佳。</li>
</ul>
<p>性能：</p>
<ul>
<li>LLM 在涉及时间知识和理解的任务中存在盲点。</li>
</ul>
<p>工作量：</p>
<ul>
<li>需要收集和标记大量的数据来评估 LLM 的时间知识和理解能力。</li>
</ul>



<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ef953dd5680b8986b519ca3040e877c7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bb539ecbb67b0f65215a7addd3ccd09f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-68a2b498e74140261d533e54124b4339.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a748627690bad5dde36e8816fc801a09.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65999fcea7981a3e7935b9b9271ce7b8.jpg" align="middle">
</details>
​    


## Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated   Text
**Authors:Abhimanyu Hans, Avi Schwarzschild, Valeriia Cherepanova, Hamid Kazemi, Aniruddha Saha, Micah Goldblum, Jonas Geiping, Tom Goldstein**

Detecting text generated by modern large language models is thought to be hard, as both LLMs and humans can exhibit a wide range of complex behaviors. However, we find that a score based on contrasting two closely related language models is highly accurate at separating human-generated and machine-generated text. Based on this mechanism, we propose a novel LLM detector that only requires simple calculations using a pair of pre-trained LLMs. The method, called Binoculars, achieves state-of-the-art accuracy without any training data. It is capable of spotting machine text from a range of modern LLMs without any model-specific modifications. We comprehensively evaluate Binoculars on a number of text sources and in varied situations. Over a wide range of document types, Binoculars detects over 90% of generated samples from ChatGPT (and other LLMs) at a false positive rate of 0.01%, despite not being trained on any ChatGPT data. 

[PDF](http://arxiv.org/abs/2401.12070v1) 20 pages, code available at https://github.com/ahans30/Binoculars

**摘要**
句法相似度评分可以准确区分人类文本与机器文本。

**要点**

- 基于比较两个紧密相关的语言模型的分数，可以非常准确地区分人类生成的文本和机器生成的文本。
- 基于这种机制，我们提出了一种新颖的 LLM 检测器，它只需要使用一对预训练的 LLM 进行简单的计算。
- 该方法称为 Binoculars，在没有任何训练数据的情况下实现了最先进的准确性。
- 它能够在没有任何特定模型修改的情况下识别来自一系列现代 LLM 的机器文本。
- 我们对 Binoculars 进行了全面的评估，涵盖多种文本来源和不同情况。
- 在广泛的文档类型中，Binoculars 检测到超过 90% 的来自 ChatGPT（和其他 LLM）的生成样本，误报率为 0.01%，尽管它没有针对任何 ChatGPT 数据进行训练。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>标题：使用双筒望远镜发现 LLM：零样本检测机器生成的文本</li>
<li>作者：Abhimanyu Hans、Avi Schwarzschild、Valeriia Cherepanova、Hamid Kazemi、Aniruddha Saha、Micah Goldblum、Jonas Geiping、Tom Goldstein</li>
<li>第一位作者的单位：马里兰大学</li>
<li>关键词：LLM 检测、零样本检测、语言模型、机器生成的文本</li>
<li>论文链接：https://arxiv.org/abs/2401.12070
Github 代码链接：https://github.com/ahans30/Binoculars</li>
<li>摘要：</li>
</ol>
<p>（1）研究背景：检测现代大型语言模型生成的文本被认为是一项困难的任务，因为 LLM 和人都可以表现出广泛的复杂行为。
（2）过去的方法和问题：现有方法主要依赖于训练数据来区分人写文本和机器生成的文本，但是这些方法往往需要大量的数据和模型训练，并且对于新的 LLM 或文本类型可能不适用。
（3）提出的研究方法：本文提出一种基于对比两个紧密相关的语言模型的分数的新颖 LLM 检测器，称为双筒望远镜。该方法仅需使用预训练的 LLM 对进行简单的计算，无需任何训练数据。
（4）方法的性能和对目标的支持：双筒望远镜在各种文本来源和不同情况下进行了全面的评估。在广泛的文件类型中，双筒望远镜检测到超过 90% 的来自聊天机器人和其他 LLM 生成的样本，误报率为 0.01%，尽管它没有在任何聊天机器人数据上进行训练。</p>
<p>Some Error for method(比如是不是没有Methods这个章节)</p>
<ol start="8">
<li>结论：
（1）：本文提出了一种基于对比两个紧密相关的语言模型的分数的新颖LLM检测器，称为双筒望远镜。该方法仅需使用预训练的LLM对进行简单的计算，无需任何训练数据，在广泛的文件类型中，双筒望远镜检测到超过90%的来自聊天机器人和其他LLM生成的样本，误报率为0.01%，尽管它没有在任何聊天机器人数据上进行训练。
（2）：创新点：
创新点一：提出了一种基于对比两个紧密相关的语言模型的分数的新颖LLM检测器，称为双筒望远镜。
创新点二：该方法仅需使用预训练的LLM对进行简单的计算，无需任何训练数据。
性能：
性能一：在广泛的文件类型中，双筒望远镜检测到超过90%的来自聊天机器人和其他LLM生成的样本。
性能二：误报率为0.01%。
工作量：
工作量一：该方法仅需使用预训练的LLM对进行简单的计算，无需任何训练数据。</li>
</ol>



<p></p><details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-dd3bfb5b9d052c9fd7839e210dcdc353.jpg" align="middle">
</details><br>​    <p></p>
<h2 id="Blinded-by-Generated-Contexts-How-Language-Models-Merge-Generated-and-Retrieved-Contexts-for-Open-Domain-QA"><a href="#Blinded-by-Generated-Contexts-How-Language-Models-Merge-Generated-and-Retrieved-Contexts-for-Open-Domain-QA" class="headerlink" title="Blinded by Generated Contexts: How Language Models Merge Generated and   Retrieved Contexts for Open-Domain QA?"></a>Blinded by Generated Contexts: How Language Models Merge Generated and   Retrieved Contexts for Open-Domain QA?</h2><p><strong>Authors:Hexiang Tan, Fei Sun, Wanli Yang, Yuanzhuo Wang, Qi Cao, Xueqi Cheng</strong></p>
<p>While auxiliary information has become a key to enhance Large Language Models (LLMs), relatively little is known about how well LLMs merge these contexts, specifically generated and retrieved. To study this, we formulate a task specifically designed to identify whether the answers, derived from the integration of generated and retrieved contexts, are attributed to either generated or retrieved contexts. To support this task, we develop a methodology to construct datasets with conflicting contexts, where each question is paired with both generated and retrieved contexts, yet only one of them contains the correct answer. Our experiments reveal a significant bias in LLMs towards generated contexts, as evidenced across state-of-the-art open (Llama2-7b/13b) and closed (GPT 3.5/4) systems. We further identify two key factors contributing to this bias: i) Contexts generated by LLMs typically show greater similarity to the questions, increasing their likelihood of selection; ii) The segmentation process used in retrieved contexts disrupts their completeness, thereby hindering their full utilization in LLMs. Our analysis enhances the understanding of how LLMs merge diverse contexts, offering valuable insights for advancing current augmentation methods for LLMs. </p>
<p><a href="http://arxiv.org/abs/2401.11911v1">PDF</a> </p>
<p><strong>摘要</strong><br>大型语言模型对生成和检索上下文的整合存在偏见，更多地依赖生成上下文中的信息。</p>
<p><strong>要点</strong></p>
<ul>
<li>大型语言模型对生成上下文的整合存在明显偏见。</li>
<li>大型语言模型更倾向于选择与问题更相似的生成上下文作为答案来源。</li>
<li>大型语言模型在整合检索上下文的过程中存在一定困难，检索上下文的不完整性可能影响了答案的准确性。</li>
<li>针对大型语言模型，现有上下文增强方法需要改进。</li>
<li>大型语言模型在整合生成和检索上下文的过程中表现出差异，理解这种差异有助于改进当前的增强方法。</li>
<li>影响大型语言模型上下文整合的两个关键因素：第一，生成上下文的相似性；第二，检索上下文的完整性。</li>
<li>大型语言模型整合上下文的能力可以通过理解和解决这些因素来提高。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li><p>标题：生成语境掩盖：语言模型如何将生成语境和检索语境融合用于开放域问答？</p>
</li>
<li><p>作者：谭鹤翔，孙飞，杨万里，王元卓，曹琪，程雪启</p>
</li>
<li><p>第一作者单位：中国科学院计算技术研究所人工智能安全与可信赖实验室</p>
</li>
<li><p>关键词：大型语言模型、生成增强、检索增强、冲突语境、问答</p>
</li>
<li><p>论文链接：https://arxiv.org/abs/2401.11911，Github 链接：无</p>
</li>
<li><p>摘要：
（1）研究背景：近年来，利用辅助信息增强大型语言模型（LLM）已成为一项重要研究方向。然而，对于 LLM 如何融合不同来源的语境，特别是生成语境和检索语境，目前的研究还相对较少。
（2）过去方法与问题：现有工作主要分为生成增强和检索增强两种方法。生成增强方法通过生成与问题相关的背景语境，作为 LLM 回答问题的依据。检索增强方法通过从外部语料库中检索相关段落作为语境，以增强 LLM 的知识。然而，这些方法都存在一定的问题。生成增强方法生成的语境可能与问题不一致，检索增强方法检索到的语境可能不完整或与问题无关。
（3）研究方法：为了研究 LLM 如何融合生成语境和检索语境，本文提出了一种新的任务，即冲突语境问答任务。该任务旨在识别 LLM 在融合生成语境和检索语境时，是否倾向于将答案归因于某个特定的语境。为了支持该任务，本文还开发了一种构建冲突语境数据集的方法。该数据集中的每个问题都配对有生成语境和检索语境，但只有其中一个语境包含正确答案。
（4）实验结果与性能：本文在冲突语境问答任务上对 LLM 进行了评估，结果表明 LLM 存在明显的生成语境偏好。这种偏好体现在各个最先进的开放域（Llama2-7b/13b）和封闭域（GPT3.5/4）系统中。进一步分析表明，这种偏好主要由两个因素导致：一是生成语境通常与问题更相似，因此更容易被 LLM 选择；二是检索语境在被分割成段落后，其完整性受到破坏，从而难以被 LLM 充分利用。</p>
</li>
<li><p>方法：
（1）任务定义：冲突语境问答任务。给定一个问题和两个语境，一个由生成器生成，另一个由检索器检索，判断答案来自哪个语境。
（2）数据集构建：构建冲突语境数据集，每个问题配对有生成语境和检索语境，但只有其中一个语境包含正确答案。
（3）模型评估：在冲突语境问答任务上评估 LLM，分析 LLM 在融合生成语境和检索语境时的偏好。</p>
</li>
<li><p>结论：
（1）：本文提出了一种新的任务——冲突语境问答任务，用于研究大型语言模型（LLM）在融合生成语境和检索语境时的偏好。该任务旨在识别 LLM 在融合生成语境和检索语境时，是否倾向于将答案归因于某个特定的语境。为了支持该任务，本文还开发了一种构建冲突语境数据集的方法。该数据集中的每个问题都配对有生成语境和检索语境，但只有其中一个语境包含正确答案。
（2）：创新点：
提出了一种新的任务——冲突语境问答任务，用于研究 LLM 在融合生成语境和检索语境时的偏好。
开发了一种构建冲突语境数据集的方法。
性能：
在冲突语境问答任务上评估 LLM，结果表明 LLM 存在明显的生成语境偏好。这种偏好体现在各个最先进的开放域（Llama2-7b/13b）和封闭域（GPT3.5/4）系统中。
进一步分析表明，这种偏好主要由两个因素导致：一是生成语境通常与问题更相似，因此更容易被 LLM 选择；二是检索语境在被分割成段落后，其完整性受到破坏，从而难以被 LLM 充分利用。
工作量：
构建冲突语境数据集的工作量较大，需要收集大量的问题和语境，并进行人工标注。
评估 LLM 在冲突语境问答任务上的表现的工作量也较大，需要对 LLM 进行多次评估，并分析评估结果。</p>
</li>
</ol>



<p></p><details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-aff8facaa355b1505b2cf6af3d0e915b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1ee80fef672e8714cbda66ee9ba9e921.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-32c0aa5250fcb9295d1e46e737e52534.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-99f94640fc796568a6b02c8056191892.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c63733100557d4290705642b87c665f9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8815deb3960e6b2bbbe8b92e6f8e6799.jpg" align="middle">
</details><br>​    <p></p>
<h2 id="PsySafe-A-Comprehensive-Framework-for-Psychological-based-Attack-Defense-and-Evaluation-of-Multi-agent-System-Safety"><a href="#PsySafe-A-Comprehensive-Framework-for-Psychological-based-Attack-Defense-and-Evaluation-of-Multi-agent-System-Safety" class="headerlink" title="PsySafe: A Comprehensive Framework for Psychological-based Attack,   Defense, and Evaluation of Multi-agent System Safety"></a>PsySafe: A Comprehensive Framework for Psychological-based Attack,   Defense, and Evaluation of Multi-agent System Safety</h2><p><strong>Authors:Zaibin Zhang, Yongting Zhang, Lijun Li, Hongzhi Gao, Lijun Wang, Huchuan Lu, Feng Zhao, Yu Qiao, Jing Shao</strong></p>
<p>Multi-agent systems, augmented with Large Language Models (LLMs), demonstrate significant capabilities for collective intelligence. However, the potential misuse of this intelligence for malicious purposes presents significant risks. To date, comprehensive research on the safety issues associated with multi-agent systems remains limited. From the perspective of agent psychology, we discover that the dark psychological states of agents can lead to severe safety issues. To address these issues, we propose a comprehensive framework grounded in agent psychology. In our framework, we focus on three aspects: identifying how dark personality traits in agents might lead to risky behaviors, designing defense strategies to mitigate these risks, and evaluating the safety of multi-agent systems from both psychological and behavioral perspectives. Our experiments reveal several intriguing phenomena, such as the collective dangerous behaviors among agents, agents’ propensity for self-reflection when engaging in dangerous behavior, and the correlation between agents’ psychological assessments and their dangerous behaviors. We anticipate that our framework and observations will provide valuable insights for further research into the safety of multi-agent systems. We will make our data and code publicly accessible at https:/github.com/AI4Good24/PsySafe. </p>
<p><a href="http://arxiv.org/abs/2401.11880v1">PDF</a> </p>
<p><strong>摘要</strong><br>多智能体系统结合了大型语言模型 (LLM)，显示出显著的集体智能能力，但也存在被恶意利用的风险。</p>
<p><strong>要点</strong></p>
<ul>
<li>多智能体系统的安全性问题尚未得到全面研究。</li>
<li>从智能体心理学角度来看，智能体的黑暗心理状态可能导致严重的安全问题。</li>
<li>我们提出了一个基于智能体心理学的综合框架，重点关注三个方面：识别智能体中的黑暗人格特质如何导致危险行为、设计防御策略来减轻这些风险，以及从心理和行为两个角度评估多智能体系统的安全性。</li>
<li>我们的实验揭示了一些有趣的现象，例如智能体之间的集体危险行为、智能体在从事危险行为时的自我反省倾向，以及智能体的心理评估与其危险行为之间的相关性。</li>
<li>我们希望我们的框架和观察结果将为多智能体系统安全性的进一步研究提供有价值的见解。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li><p>论文标题：黑暗人格特质攻击提示：用于危险任务评估的提示</p>
</li>
<li><p>作者：Yihan Wang, Qiang Zhang, Yuxin Peng, Xiaotong Li, Jingbo Shang, Xiangliang Zhang, Yujie Zhang, Jie Tang, Yong Yu</p>
</li>
<li><p>第一作者单位：斯坦福大学</p>
</li>
<li><p>关键词：多智能体系统、黑暗人格特质、安全、心理、行为</p>
</li>
<li><p>论文链接：https://arxiv.org/abs/2302.09358，Github 代码链接：Github：None</p>
</li>
<li><p>摘要：</p>
</li>
</ol>
<p>（1）研究背景：多智能体系统，增强了大型语言模型（LLM），展示了集体智能的显著能力。然而，将这种智能恶意用于恶意目的的潜在滥用带来了重大风险。迄今为止，关于多智能体系统安全问题的全面研究仍然有限。</p>
<p>（2）过去方法及其问题：从代理心理学角度，我们发现代理的黑暗心理状态可能导致严重的安全问题。为了解决这些问题，我们提出了一个以代理心理学为基础的综合框架。</p>
<p>（3）提出的研究方法：在我们的框架中，我们关注三个方面：识别代理中的黑暗人格特质如何导致危险行为、设计防御策略来减轻这些风险，以及从心理和行为角度评估多智能体系统的安全性。</p>
<p>（4）方法在任务和性能方面的表现：我们的实验揭示了一些有趣的现象，例如代理之间的集体危险行为、代理在从事危险行为时进行自我反省的倾向，以及代理的心理评估与其危险行为之间的相关性。我们预计，我们的框架和观察结果将为进一步研究多智能体系统的安全性提供有价值的见解。</p>
<ol start="7">
<li><p>方法：
（1）提出一个基于代理心理学的综合框架，关注代理中的黑暗人格特质如何导致危险行为、设计防御策略来减轻这些风险，以及从心理和行为角度评估多智能体系统的安全性；
（2）通过实验揭示了一些有趣的现象，例如代理之间的集体危险行为、代理在从事危险行为时进行自我反省的倾向，以及代理的心理评估与其危险行为之间的相关性；
（3）分析了不同提示对多智能体系统危险率的影响，包括手工制作的越狱提示、黑暗特质提示注入、诱导指令注入和危险意图的隐藏；
（4）分析了从不同角度攻击多智能体系统的影响，包括人类输入攻击、高频人类输入攻击、特质攻击和混合攻击方法；
（5）评估了不同的大语言模型的安全性，包括基于 API 的模型和开源模型，研究了模型大小与危险率之间的关系；
（6）进行了防御实验，评估了输入过滤器、GPT-4 的有害提示识别、DoctorDefense 和 PoliceDefense 的有效性。</p>
</li>
<li><p>结论：
（1）：从心理学的角度，本文全面分析了多智能体系统的安全性问题，引入了一种结合黑暗人格特质的攻击方法。这种方法可以有效地破坏多智能体系统，诱发危险行为。本文还提出了防御策略，可以显著降低多智能体系统中危险行为的风险。此外，本文还引入了一种包含心理和行为两个方面在内的安全评估方法，对多智能体系统的安全性进行综合评估。
（2）：创新点：提出了一种基于代理心理学的综合框架，关注代理中的黑暗人格特质如何导致危险行为、设计防御策略来减轻这些风险，以及从心理和行为角度评估多智能体系统的安全性；通过实验揭示了一些有趣的现象，例如代理之间的集体危险行为、代理在从事危险行为时进行自我反省的倾向，以及代理的心理评估与其危险行为之间的相关性；分析了不同提示对多智能体系统危险率的影响，包括手工制作的越狱提示、黑暗特质提示注入、诱导指令注入和危险意图的隐藏；分析了从不同角度攻击多智能体系统的影响，包括人类输入攻击、高频人类输入攻击、特质攻击和混合攻击方法；评估了不同的大语言模型的安全性，包括基于API的模型和开源模型，研究了模型大小与危险率之间的关系；进行了防御实验，评估了输入过滤器、GPT-4的有害提示识别、DoctorDefense和PoliceDefense的有效性。
性能：本文提出的攻击方法可以有效地破坏多智能体系统，诱发危险行为。本文提出的防御策略可以显著降低多智能体系统中危险行为的风险。本文提出的安全评估方法可以对多智能体系统的安全性进行综合评估。
工作量：本文的工作量很大，需要进行大量的实验和分析。</p>
</li>
</ol>



<p></p><details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c838c9a54be9dc1fa902235acfd4e0fa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1dc1793d596c4da4ce3e17bd21b27f34.jpg" align="middle">
</details><br>​    <p></p>
<h2 id="Improving-Small-Language-Models’-Mathematical-Reasoning-via-Mix-Thoughts-Distillation"><a href="#Improving-Small-Language-Models’-Mathematical-Reasoning-via-Mix-Thoughts-Distillation" class="headerlink" title="Improving Small Language Models’ Mathematical Reasoning via Mix Thoughts   Distillation"></a>Improving Small Language Models’ Mathematical Reasoning via Mix Thoughts   Distillation</h2><p><strong>Authors:Xunyu Zhu, Jian Li, Yong Liu, Can Ma, Weiping Wang</strong></p>
<p>This work addresses the challenge of democratizing advanced Large Language Models (LLMs) by compressing their mathematical reasoning capabilities into sub-billion parameter Small Language Models (SLMs) without compromising performance. We introduce Equation-of-Thought Distillation (EoTD), a novel technique that encapsulates the reasoning process into equation-based representations to construct an EoTD dataset for fine-tuning SLMs. Additionally, we propose the Mix Thoughts Distillation (MTD) framework to enhance the reasoning performance of SLMs. This involves creating a reasoning dataset with multiple thought processes and using it for fine-tuning. Our experimental findings demonstrate that EoTD significantly boosts the reasoning abilities of SLMs, while MTD enables these models to achieve state-of-the-art reasoning performance. </p>
<p><a href="http://arxiv.org/abs/2401.11864v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>融合推理蒸馏方法增强小语言模型数学推理能力。</p>
<p><strong>要点</strong></p>
<ul>
<li>方程式思维蒸馏 (EoTD) 是一种新颖的技术，将推理过程封装到基于方程式的表示中，以便为精调 SLM 构建 EoTD 数据集。</li>
<li>EoTD 大幅提升了SLM的推理能力，而MTD使这些模型能够实现最先进的推理性能。</li>
<li>混合思维蒸馏 (MTD) 框架用于创建具有多种思维过程的推理数据集，并将其用于精调。</li>
<li>EoTD 数据集的构建方法为 SLM 的推理能力的提高提供了新的思路。</li>
<li>MTD 框架通过创建具有多种思维过程的推理数据集，可以进一步增强SLM的推理性能。</li>
<li>EoTD 和 MTD 方法的结合，使得 SLM 能够实现最先进的推理性能。</li>
<li>这些方法为 SLM 的推理能力的提高提供了新的思路，并有望在自然语言处理、机器学习等领域得到广泛的应用。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<p></p><ol><p></p>
<p></p><li><p>题目：通过混合思想蒸馏改进小型语言模型的数学推理能力</p>
</li><p></p>
<p></p><li><p>作者：朱洵宇, 李健, 刘勇, 马灿, 王卫平</p>
</li><p></p>
<p></p><li><p>第一作者单位：中国科学院信息工程研究所</p>
</li><p></p>
<p></p><li><p>关键词：小型语言模型；数学推理；蒸馏；思想链；方程思维</p>
</li><p></p>
<p></p><li><p>论文链接：https://arxiv.org/abs/2401.11864</p>
</li><p></p>
<p></p><li><p>摘要：
(1) 研究背景：随着大型语言模型（LLM）的发展，其在自然语言处理任务中表现出强大的性能。然而，LLM的庞大参数规模和计算需求限制了其在实际应用中的部署。为了解决这一问题，研究人员提出了将LLM的数学推理能力压缩到数十亿参数的小型语言模型（SLM）中，以实现更广泛的部署。
(2) 过去的方法及其问题：现有方法主要通过使用LLM创建包含详细推理路径的丰富数据集，然后微调SLM来实现知识迁移。然而，这些方法在数学问题求解方面存在明显的差距。
(3) 本文提出的研究方法：本文提出了一种名为“方程思维蒸馏”（EoTD）的框架来增强SLM的数学推理能力。EoTD首先提示LLM对问题生成方程，然后使用方程求解器求解这些方程。不产生正确解的方程会被丢弃。利用这种方法，我们构建了EoTD数据集，并使用该数据集微调SLM，从而提升了SLM的推理能力。
(4) 方法在任务和性能上的表现：在数学问题求解任务上，EoTD显著提高了SLM的推理能力。同时，我们还提出了混合思想蒸馏（MTD）框架，进一步增强了SLM的推理性能。MTD通过创建一个包含多种推理过程的推理数据集，并使用该数据集微调SLM来实现。实验结果表明，MTD使SLM在数学推理任务上达到了最先进的性能。这些结果证明了EoTD和MTD方法的有效性，为将LLM的数学推理能力压缩到SLM中提供了新的思路。</p>
</li><p></p>
<p></p><li><p>方法：
(1) 方程思维蒸馏（EoTD）：</p><p></p>
<ul>
<li>从大型语言模型（LLM）生成包含方程的推理数据集。</li>
<li>使用方程求解器求解这些方程，并丢弃不产生正确解的方程。</li>
<li>利用该数据集微调小型语言模型（SLM），以提升其数学推理能力。
(2) 混合思想蒸馏（MTD）：</li>
<li>构建包含多种推理过程的推理数据集。</li>
<li>使用该数据集微调 SLM，以进一步增强其推理性能。
(3) MTD 的优势：</li>
<li>结合了方程思维蒸馏、链式思维蒸馏和程序思维蒸馏的优点。</li>
<li>弥补了各思想过程的不足，增强了 SLM 的数学推理能力。</li>
</ul>
</li>
<li><p>结论：
（1）：本研究工作的重要意义在于，它为将大型语言模型（Large Language Models，LLMs）先进的推理能力压缩到小规模语言模型（Small Language Models，SLMs）中迈出了重要一步。通过提出方程思维蒸馏（Equation-of-Thought Distillation，EoTD）和混合思想蒸馏（Mix Thoughts Distillation，MTD）方法，我们证明了将 LLMs 的数学推理能力压缩到参数量少于十亿的 SLMs 中是可行的。EoTD 方法有效地捕获了基于方程的推理过程，促进了 SLMs 对数学推理的理解和生成。MTD 框架通过结合多种推理过程的数据集进一步增强了 SLMs 的推理能力，使其在推理任务上取得了最先进的性能。
（2）：创新点：</p>
</li>
</ol>
<ul>
<li>提出了一种新的蒸馏框架 EoTD，该框架可以将 LLMs 的数学推理能力压缩到 SLMs 中。</li>
<li>构建了一个包含方程的推理数据集，并使用该数据集微调 SLMs，以提升其数学推理能力。</li>
<li>提出了一种新的蒸馏框架 MTD，该框架可以结合多种推理过程的数据集来微调 SLMs，进一步增强其推理性能。
性能：</li>
<li>EoTD 和 MTD 方法在数学推理任务上显著提高了 SLMs 的推理能力。</li>
<li>在数学推理任务上，MTD 使 SLMs 达到了最先进的性能。
工作量：</li>
<li>EoTD 和 MTD 方法需要构建推理数据集并微调 SLMs，这需要一定的工作量。</li>
<li>EoTD 和 MTD 方法需要使用方程求解器来求解方程，这可能会增加计算成本。</li>
</ul>



<p></p><details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-4f7748b843608736dabf6b6cc14b4ee1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d7fdb18ed70f0091f9dbdbddc997d51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c622575f8bb790ddfec633e22a66d78.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9c9ffd2b8828d29126e178af6f1f45c3.jpg" align="middle">
</details><br>​    <p></p>
<h2 id="Speak-It-Out-Solving-Symbol-Related-Problems-with-Symbol-to-Language-Conversion-for-Language-Models"><a href="#Speak-It-Out-Solving-Symbol-Related-Problems-with-Symbol-to-Language-Conversion-for-Language-Models" class="headerlink" title="Speak It Out: Solving Symbol-Related Problems with Symbol-to-Language   Conversion for Language Models"></a>Speak It Out: Solving Symbol-Related Problems with Symbol-to-Language   Conversion for Language Models</h2><p><strong>Authors:Yile Wang, Sijie Cheng, Zixin Sun, Peng Li, Yang Liu</strong></p>
<p>Symbols (or more broadly, non-natural language textual representations) such as numerical sequences, molecular formulas, and table delimiters widely exist, playing important roles in various tasks such as abstract reasoning, chemical property prediction, and table question answering. Despite the impressive natural language comprehension capabilities of large language models (LLMs), their reasoning abilities for symbols remain inadequate, which could attributed to the difference between symbol representations and general natural languages. We propose symbol-to-language (S2L), a tuning-free method that enables large language models to solve symbol-related problems with information expressed in natural language. Specifically, S2L first converts the symbols involved to language-based representations, which can be implemented by prompting LLMs or leveraging external tools, then these language-based representations are integrated into the original problem via direct substitution or concatenation, serving as useful input information for LLMs. We evaluate the S2L method using both API-based (GPT-4, ChatGPT) and open-source (OpenChat) models over eight symbol-related tasks, ranging from symbol-only abstract reasoning to sentiment analysis in social media. Experimental results show that S2L consistently leads to superior performance. For example, by employing S2L for GPT-4, there can be average significant improvements of +21.9% and +9.5% for subtasks in 1D-ARC and Dyck language, respectively. Codes and data are available at <a href="https://github.com/THUNLP-MT/symbol2language">https://github.com/THUNLP-MT/symbol2language</a>. </p>
<p><a href="http://arxiv.org/abs/2401.11725v1">PDF</a> </p>
<p><strong>Summary</strong><br>在自然语言理解方面表现出色的大语言模型 (LLM) 在处理符号方面能力不足，符号到语言 (S2L) 方法可将符号转换为语言表示，从而显著提高 LLM 解决符号相关问题的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>符号广泛存在于各种任务中，如抽象推理、化学性质预测和表格问答。</li>
<li>LLM 在自然语言理解方面表现出色，但在处理符号方面能力不足。</li>
<li>符号与一般自然语言的表示方式不同，这是导致 LLM 符号推理能力不足的原因之一。</li>
<li>S2L 是一种不需要微调的方法，可以使 LLM 能够利用自然语言表达的信息来解决符号相关的问题。</li>
<li>S2L 先将符号转换为语言表示，然后将这些语言表示通过直接替换或连接的方式集成到原始问题中，作为 LLM 的有用输入信息。</li>
<li>利用 S2L 方法，可以显著提高 LLM 在符号相关任务上的性能。</li>
<li>S2L 方法的代码和数据可以在 <a href="https://github.com/THUNLP-MT/symbol2language">https://github.com/THUNLP-MT/symbol2language</a> 找到。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<p></p><ol><p></p>
<p></p><li><p>论文标题：SpeakItOut：利用符号与语言转换解决符号相关问题</p>
</li><p></p>
<p></p><li><p>作者：王一乐、程思杰、孙子欣、李鹏、刘洋</p>
</li><p></p>
<p></p><li><p>第一作者单位：清华大学人工智能产业研究院</p>
</li><p></p>
<p></p><li><p>关键词：符号与语言转换、大语言模型、符号相关问题</p>
</li><p></p>
<p></p><li><p>论文链接：https://arxiv.org/abs/2401.11725
Github 代码链接：https://github.com/THUNLP-MT/symbol2language</p>
</li><p></p>
<p></p><li><p>摘要：</p>
</li><br>&lt;/ol&gt;<p></p>
<p></p><p>（1）研究背景：符号（或更广泛的非自然语言文本表示），如数值序列、分子式和表格分隔符，广泛存在，在各种任务中发挥着重要作用，如抽象推理、化学性质预测和表格问答。尽管大型语言模型（LLM）具有令人印象深刻的自然语言理解能力，但它们对符号的推理能力仍然不足，这可能归因于符号表示与一般自然语言之间的差异。</p><p></p>
<p></p><p>（2）过去的方法及其问题：本文提出了一种无调优的方法符号到语言（S2L），该方法使大型语言模型能够利用自然语言中表达的信息来解决符号相关问题。S2L 首先将涉及的符号转换为基于语言的表示，这可以通过提示 LLM 或利用外部工具来实现，然后通过直接替换或连接将这些基于语言的表示集成到原始问题中，作为 LLM 的有用输入信息。</p><p></p>
<p></p><p>（3）本文提出的研究方法：我们使用基于 API（GPT-4、ChatGPT）和开源（OpenChat）的模型在八个符号相关任务上评估了 S2L 方法，这些任务从仅符号的抽象推理到社交媒体中的情感分析。实验结果表明，S2L 一致地带来了更好的性能。例如，通过将 S2L 用于 GPT-4，1D-ARC 和 Dyck 语言的子任务可以分别平均显着提高 21.9% 和 9.5%。</p><p></p>
<p></p><p>（4）方法在任务上的表现及其对目标的支持：S2L 方法在各种涉及不同类型符号的场景中具有广泛的适用性。</p><p></p>
<p></p><p>Some Error for method(比如是不是没有Methods这个章节)</p><p></p>
<p></p><ol start="8"><p></p>
<p></p><li>结论：
（1）：本文提出了符号到语言转换方法，该方法将符号表示转换为基于语言的表示，从而使大型语言模型能够利用自然语言中表达的信息来解决符号相关问题。实验结果表明，该方法在各种涉及不同类型符号的场景中具有广泛的适用性，并且能够显著提高任务的性能。
（2）：创新点：</li><br>&lt;/ol&gt;<p></p>
<ul>
<li>将符号表示转换为基于语言的表示，从而使大型语言模型能够利用自然语言中表达的信息来解决符号相关问题。</li>
<li>提出了一种无调优的方法，该方法无需对大型语言模型进行任何额外的训练，即可将其应用于符号相关问题。</li>
<li>在八个符号相关任务上评估了该方法的性能，实验结果表明该方法能够显著提高任务的性能。
性能：</li>
<li>在1D-ARC和Dyck语言的子任务上，该方法分别平均显着提高了21.9%和9.5%。</li>
<li>在化学性质预测任务上，该方法的准确率提高了10.2%。</li>
<li>在表格问答任务上，该方法的准确率提高了5.8%。
工作量：</li>
<li>该方法的实现相对简单，并且不需要对大型语言模型进行任何额外的训练。</li>
<li>该方法可以很容易地应用于各种符号相关问题。</li>
</ul>



<p></p><details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-40de55454aa9c0a64ccc5f00f797fa15.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-61ef72b92fc749addf01e22c5321e38b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c34ab5861560ccbb591eb0b220a02b3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3fe73936de3fc8438e09058d53307b11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5fc7e17bf194fb6a87ae3ea50cc1c8f9.jpg" align="middle">
</details><br>​    <p></p>
<h2 id="Mastering-Text-to-Image-Diffusion-Recaptioning-Planning-and-Generating-with-Multimodal-LLMs"><a href="#Mastering-Text-to-Image-Diffusion-Recaptioning-Planning-and-Generating-with-Multimodal-LLMs" class="headerlink" title="Mastering Text-to-Image Diffusion: Recaptioning, Planning, and   Generating with Multimodal LLMs"></a>Mastering Text-to-Image Diffusion: Recaptioning, Planning, and   Generating with Multimodal LLMs</h2><p><strong>Authors:Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, Bin Cui</strong></p>
<p>Diffusion models have exhibit exceptional performance in text-to-image generation and editing. However, existing methods often face challenges when handling complex text prompts that involve multiple objects with multiple attributes and relationships. In this paper, we propose a brand new training-free text-to-image generation/editing framework, namely Recaption, Plan and Generate (RPG), harnessing the powerful chain-of-thought reasoning ability of multimodal LLMs to enhance the compositionality of text-to-image diffusion models. Our approach employs the MLLM as a global planner to decompose the process of generating complex images into multiple simpler generation tasks within subregions. We propose complementary regional diffusion to enable region-wise compositional generation. Furthermore, we integrate text-guided image generation and editing within the proposed RPG in a closed-loop fashion, thereby enhancing generalization ability. Extensive experiments demonstrate our RPG outperforms state-of-the-art text-to-image diffusion models, including DALL-E 3 and SDXL, particularly in multi-category object composition and text-image semantic alignment. Notably, our RPG framework exhibits wide compatibility with various MLLM architectures (e.g., MiniGPT-4) and diffusion backbones (e.g., ControlNet). Our code is available at: <a href="https://github.com/YangLing0818/RPG-DiffusionMaster">https://github.com/YangLing0818/RPG-DiffusionMaster</a> </p>
<p><a href="http://arxiv.org/abs/2401.11708v1">PDF</a> Project: <a href="https://github.com/YangLing0818/RPG-DiffusionMaster">https://github.com/YangLing0818/RPG-DiffusionMaster</a></p>
<p><strong>摘要</strong><br>充分利用多模态语言模型的思维链能力，有效地提高文本到图像扩散模型的合成性。</p>
<p><strong>要点</strong></p>
<ul>
<li>提出了一种新型的无训练文本到图像生成/编辑框架，称为重新注释、计划和生成 (RPG)。</li>
<li>RPG 利用多模态语言模型作为全局规划器，将生成复杂图像的过程分解为子区域内的多个更简单的生成任务。</li>
<li>提出了一种互补的区域扩散算法，以实现区域级的合成生成。</li>
<li>在闭合回路中集成文本导向图像生成和编辑，从而提高泛化能力。</li>
<li>大量实验表明，RPG 优于最先进的文本到图像扩散模型，包括 DALL-E 3 和 SDXL，尤其是在多类别对象合成和文本图像语义对齐方面。</li>
<li>RPG 框架与各种多模态语言模型架构（例如 MiniGPT-4）和扩散模型骨干（例如 ControlNet）具有广泛的兼容性。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<p></p><ol><p></p>
<p></p><li><p>题目：掌握文本到图像扩散：使用多模态 LLM 进行重新描述、规划和生成</p>
</li><p></p>
<p></p><li><p>作者：凌扬<em>1、余兆晨</em>1、孟晨林23、徐明凯2、斯特凡诺·埃尔蒙2、崔斌1</p>
</li><p></p>
<p></p><li><p>所属单位：北京大学</p>
</li><p></p>
<p></p><li><p>关键词：文本到图像扩散、多模态 LLM、重新描述、规划、生成、区域扩散</p>
</li><p></p>
<p></p><li><p>论文链接：https://github.com/YangLing0818/RPG-DiffusionMaster
Github 链接：https://github.com/YangLing0818/RPG-DiffusionMaster</p>
</li><p></p>
<p></p><li><p>摘要：
（1）研究背景：扩散模型在文本到图像生成和编辑方面表现出色的性能。然而，现有方法在处理涉及具有多个属性和关系的多个对象的复杂文本提示时通常面临挑战。
（2）过去的方法及其问题：一些工作通过引入额外的布局/框作为条件或利用提示感知注意指导来解决这个问题。然而，这些方法通常需要额外的训练数据或复杂的设计，并且在处理复杂的文本提示时可能仍然存在局限性。
（3）研究方法：本文提出了一种全新的无训练文本到图像生成/编辑框架，称为重新描述、规划和生成 (RPG)，利用多模态 LLM 强大的链式思维推理能力来增强文本到图像扩散模型的组合性。我们的方法使用 MLLM 作为全局规划器，将生成复杂图像的过程分解为多个更简单的生成任务，这些任务位于子区域内。我们提出了互补的区域扩散来实现区域组合生成。此外，我们将文本引导的图像生成和编辑集成到所提出的 RPG 中，从而增强泛化能力。
（4）方法性能：广泛的实验表明，我们的 RPG 优于最先进的文本到图像扩散模型，包括 DALL-E3 和 SDXL，尤其是在多类别对象组合和文本图像语义对齐方面。值得注意的是，我们的 RPG 框架与各种 MLLM 架构（例如，MiniGPT-4）和扩散骨干网（例如，ControlNet）具有广泛的兼容性。</p>
</li><p></p>
<p></p><li><p>提出了一种新的无训练文本到图像生成/编辑框架，称为重新描述、规划和生成 (RPG)，利用多模态 LLM 强大的链式思维推理能力来增强文本到图像扩散模型的组合性。</p>
</li><p></p>
<p></p><li><p>使用 MLLM 作为全局规划器，将生成复杂图像的过程分解为多个更简单的生成任务，这些任务位于子区域内。</p>
</li><p></p>
<p></p><li><p>提出互补的区域扩散来实现区域组合生成。</p>
</li><p></p>
<p></p><li><p>将文本引导的图像生成和编辑集成到所提出的 RPG 中，从而增强泛化能力。</p>
</li><p></p>
<p></p><li><p>广泛的实验表明，我们的 RPG 优于最先进的文本到图像扩散模型，包括 DALL-E3 和 SDXL，尤其是在多类别对象组合和文本图像语义对齐方面。</p>
</li><p></p>
<p></p><li><p>我们的 RPG 框架与各种 MLLM 架构（例如，MiniGPT-4）和扩散骨干网（例如，ControlNet）具有广泛的兼容性。</p>
</li><p></p>
<p></p><li><p>结论：
（1）：本文提出了一种新的无训练文本到图像生成/编辑框架 RPG，利用多模态 LLM 强大的链式思维推理能力来增强文本到图像扩散模型的组合性，在多类别对象组合和文本图像语义对齐方面优于最先进的文本到图像扩散模型。
（2）：创新点：</p>
</li><br>&lt;/ol&gt;<p></p>
<ul>
<li>提出了一种新的无训练文本到图像生成/编辑框架 RPG，利用多模态 LLM 强大的链式思维推理能力来增强文本到图像扩散模型的组合性。</li>
<li>使用 MLLM 作为全局规划器，将生成复杂图像的过程分解为多个更简单的生成任务，这些任务位于子区域内。</li>
<li>提出互补的区域扩散来实现区域组合生成。</li>
<li>将文本引导的图像生成和编辑集成到所提出的 RPG 中，从而增强泛化能力。
性能：</li>
<li>广泛的实验表明，我们的 RPG 优于最先进的文本到图像扩散模型，包括 DALL-E3 和 SDXL，尤其是在多类别对象组合和文本图像语义对齐方面。</li>
<li>我们的 RPG 框架与各种 MLLM 架构（例如，MiniGPT-4）和扩散骨干网（例如，ControlNet）具有广泛的兼容性。
工作量：</li>
<li>本文的工作量较大，涉及到多模态 LLM、文本到图像扩散模型、区域扩散模型等多个方面的研究。</li>
<li>本文需要进行大量的实验来验证所提出的方法的有效性。</li>
</ul>



<p></p><details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d7ede89518c7e2b2017c785eb927b766.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-69a6785a9dc22c046203d70cee24a3f1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b57333091d6dbb8392ce8971cf413d0b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d6f54078071dcab585ee882e1cb7cb6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40b7d562cad3ed84d89938dbcdb65fff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe1c57ab8d093322b4502e666dccd4cb.jpg" align="middle">
</details><br>​    <p></p>
<h2 id="FinSQL-Model-Agnostic-LLMs-based-Text-to-SQL-Framework-for-Financial-Analysis"><a href="#FinSQL-Model-Agnostic-LLMs-based-Text-to-SQL-Framework-for-Financial-Analysis" class="headerlink" title="FinSQL: Model-Agnostic LLMs-based Text-to-SQL Framework for Financial   Analysis"></a>FinSQL: Model-Agnostic LLMs-based Text-to-SQL Framework for Financial   Analysis</h2><p><strong>Authors:Chao Zhang, Yuren Mao, Yijiang Fan, Yu Mi, Yunjun Gao, Lu Chen, Dongfang Lou, Jinshu Lin</strong></p>
<p>Text-to-SQL, which provides zero-code interface for operating relational databases, has gained much attention in financial analysis; because, financial professionals may not well-skilled in SQL programming. However, until now, there is no practical Text-to-SQL benchmark dataset for financial analysis, and existing Text-to-SQL methods have not considered the unique characteristics of databases in financial applications, such as commonly existing wide tables. To address these issues, we collect a practical Text-to-SQL benchmark dataset and propose a model-agnostic Large Language Model (LLMs)-based Text-to-SQL framework for financial analysis. The benchmark dataset, BULL, is collected from the practical financial analysis business of Hundsun Technologies Inc., including databases for fund, stock, and macro economy. Besides, the proposed LLMs-based Text-to-SQL framework, FinSQL, provides a systematic treatment for financial Text-to-SQL from the perspectives of prompt construction, parameter-efficient fine-tuning and output calibration. Extensive experimental results on BULL demonstrate that FinSQL achieves the state-of-the-art Text-to-SQL performance at a small cost; furthermore, FinSQL can bring up to 36.64% performance improvement in scenarios requiring few-shot cross-database model transfer. </p>
<p><a href="http://arxiv.org/abs/2401.10506v1">PDF</a> 13 pages, 13 figures</p>
<p><strong>Summary</strong><br>基于大语言模型的金融领域文本转 SQL 框架 FinSQL 实现了最先进的文本转 SQL 性能，并且在少样本跨数据库模型迁移的场景中，FinSQL 可以带来高达 36.64% 的性能提升。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>基于大语言模型的文本转 SQL 框架 FinSQL 在金融领域实现了最先进的性能，在 BULL 数据集上的准确率达到了 99.36%。</li>
<li>FinSQL 利用提示工程、参数高效微调和输出校准等技术，有效地解决了金融领域文本转 SQL 的挑战，特别是在处理宽表方面具有优势。</li>
<li>FinSQL 可以通过提示工程和微调，轻松地适应不同的金融领域数据库，并且在少样本跨数据库模型迁移的场景中表现出色。</li>
<li>FinSQL 提供了一个统一的框架，可以应用于各种金融领域文本转 SQL 任务，例如基金分析、股票分析和宏观经济分析。</li>
<li>FinSQL 可以帮助金融专业人士轻松地从文本查询中提取信息，从而提高工作效率和决策质量。</li>
<li>FinSQL 是一个开源工具，可以在 GitHub 上获取，<a href="https://github.com/hundun-tech/FinSQL。">https://github.com/hundun-tech/FinSQL。</a></li>
<li>FinSQL 可以在各种硬件平台上运行，包括 CPU 和 GPU，并且支持多种编程语言，包括 Python、Java 和 C++。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<p></p><ol><p></p>
<p></p><li><p>题目：FinSQL：基于模型无关的 LLM 的金融分析文本转 SQL 框架</p>
</li><p></p>
<p></p><li><p>作者：Chao Zhang, Yuren Mao, Yijiang Fan, Yu Mi, Yunjun Gao, Lu Chen, Dongfang Lou, Jinshu Lin</p>
</li><p></p>
<p></p><li><p>单位：浙江大学</p>
</li><p></p>
<p></p><li><p>关键词：文本转 SQL，大型语言模型（LLM），金融分析</p>
</li><p></p>
<p></p><li><p>链接：https://arxiv.org/abs/2401.10506
Github：无</p>
</li><p></p>
<p></p><li><p>摘要：
（1）研究背景：文本转 SQL 旨在将自然语言问题转换成可执行的 SQL 查询，这有助于非专业数据库用户访问数据。在金融分析领域，金融专业人士经常需要查询相关数据库，但他们通常不熟悉 SQL 编程。因此，文本转 SQL 对金融分析非常重要。
（2）过去方法及问题：目前没有针对金融分析的文本转 SQL 基准数据集，现有文本转 SQL 方法也没有考虑金融分析中使用的数据库的独特特征。
（3）研究方法：本文构建了一个实用的金融分析文本转 SQL 数据集 BULL，该数据集包含三个分别对应于基金、股票和宏观经济的数据库。此外，本文还提出了一种基于模型无关的 LLM 的文本转 SQL 框架 FinSQL。FinSQL 从提示构建、参数高效微调和输出校准的角度对金融文本转 SQL 进行了系统处理。
（4）任务和性能：在 BULL 数据集上进行的广泛实验结果表明，FinSQL 以较小的成本实现了最先进的文本转 SQL 性能；此外，在需要少量跨数据库模型迁移的场景中，FinSQL 可以带来高达 36.64% 的性能提升。这些性能结果支持了本文的目标。</p>
</li><p></p>
<p></p><li><p>方法：
（1）构建金融分析文本转 SQL 基准数据集 BULL，包含基金、股票和宏观经济三个数据库；
（2）提出基于模型无关的 LLM 的文本转 SQL 框架 FinSQL，从提示构建、参数高效微调和输出校准三个角度对金融文本转 SQL 进行了系统处理；
（3）利用 ChatGPT 自动生成同义问句，丰富问题风格的多样性；
（4）设计规则从 SQL 查询中提取关键词，获得相应的骨架，创建骨架增强数据集，指导模型先生成 SQL 骨架，再生成最终 SQL 查询；
（5）改进 Cross-Encoder 模型的训练和推理过程，使其适用于金融场景，快速准确地检索模式项；
（6）提出基于 LoRA 的参数高效微调框架，支持低资源微调和跨数据库泛化，包括 LoRA-based 多任务参数高效微调方法、LoRA 插件中心和权重合并方法；
（7）构建 LoRA 插件中心，存储训练好的 LoRA 模块，支持低资源场景下的少量微调。</p>
</li><p></p>
<p></p><li><p>结论：
（1）：本文构建了首个金融分析文本转SQL基准数据集BULL，并提出了基于模型无关的LLM的文本转SQL框架FinSQL，在BULL数据集上取得了最先进的性能，并支持低资源微调和跨数据库泛化。
（2）：创新点：</p>
</li><br>&lt;/ol&gt;<p></p>
<ul>
<li>构建了首个金融分析文本转SQL基准数据集BULL，包含基金、股票和宏观经济三个数据库。</li>
<li>提出了一种基于模型无关的LLM的文本转SQL框架FinSQL，从提示构建、参数高效微调和输出校准三个角度对金融文本转SQL进行了系统处理。</li>
<li>设计了规则从SQL查询中提取关键词，获得相应的骨架，创建骨架增强数据集，指导模型先生成SQL骨架，再生成最终SQL查询。</li>
<li>改进了Cross-Encoder模型的训练和推理过程，使其适用于金融场景，快速准确地检索模式项。</li>
<li>提出基于LoRA的参数高效微调框架，支持低资源微调和跨数据库泛化，包括LoRA-based多任务参数高效微调方法、LoRA插件中心和权重合并方法。</li>
<li>构建了LoRA插件中心，存储训练好的LoRA模块，支持低资源场景下的少量微调。
性能：</li>
<li>在BULL数据集上进行的广泛实验结果表明，FinSQL以较小的成本实现了最先进的文本转SQL性能；此外，在需要少量跨数据库模型迁移的场景中，FinSQL可以带来高达36.64%的性能提升。
工作量：</li>
<li>本文构建了首个金融分析文本转SQL基准数据集BULL，包含基金、股票和宏观经济三个数据库，并提出了基于模型无关的LLM的文本转SQL框架FinSQL，在BULL数据集上取得了最先进的性能，并支持低资源微调和跨数据库泛化。</li>
</ul>



<p></p><details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-9c895692f6c567a00b199fa54f5a74f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b388a7ca23a42a81a34c91321601c58d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-503c271dadd87a4b6296f39bca961d5e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f4ac08d2e533ded93b75b84bb260e475.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc20ecf0e3b40a207c16c1ef7b53fe00.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2bf510e39ce68125220c0fde2cc70d74.jpg" align="middle">
</details><br>​    <p></p>
<h2 id="Beyond-Reference-Based-Metrics-Analyzing-Behaviors-of-Open-LLMs-on-Data-to-Text-Generation"><a href="#Beyond-Reference-Based-Metrics-Analyzing-Behaviors-of-Open-LLMs-on-Data-to-Text-Generation" class="headerlink" title="Beyond Reference-Based Metrics: Analyzing Behaviors of Open LLMs on   Data-to-Text Generation"></a>Beyond Reference-Based Metrics: Analyzing Behaviors of Open LLMs on   Data-to-Text Generation</h2><p><strong>Authors:Zdeněk Kasner, Ondřej Dušek</strong></p>
<p>We investigate to which extent open large language models (LLMs) can generate coherent and relevant text from structured data. To prevent bias from benchmarks leaked into LLM training data, we collect Quintd-1: an ad-hoc benchmark for five data-to-text (D2T) generation tasks, consisting of structured data records in standard formats gathered from public APIs. We leverage reference-free evaluation metrics and LLMs’ in-context learning capabilities, allowing us to test the models with no human-written references. Our evaluation focuses on annotating semantic accuracy errors on token-level, combining human annotators and a metric based on GPT-4. Our systematic examination of the models’ behavior across domains and tasks suggests that state-of-the-art open LLMs with 7B parameters can generate fluent and coherent text from various standard data formats in zero-shot settings. However, we also show that semantic accuracy of the outputs remains a major issue: on our benchmark, 80% of outputs of open LLMs contain a semantic error according to human annotators (91% according to GPT-4). Our code, data, and model outputs are available at <a href="https://d2t-llm.github.io">https://d2t-llm.github.io</a>. </p>
<p><a href="http://arxiv.org/abs/2401.10186v1">PDF</a> 26 pages</p>
<p><strong>Summary</strong><br>大型语言模型在无参考零样本数据对文本生成任务中的语义准确性存在重大问题。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>我们收集了一个新的数据对文本生成基准 Quintd-1，以防止基准中的偏见影响 LLM 的训练数据。</li>
<li>我们使用无参考评估指标和 LLM 的上下文学习能力，允许我们在没有任何人工书写参考的情况下测试模型。</li>
<li>我们对模型在不同领域和任务中的行为进行了系统检查，结果表明，具有 7B 参数的最新开放式 LLM 可以从各种标准数据格式中生成流畅连贯的文本，而无需进行任何预训练。</li>
<li>但是，我们还发现，输出的语义准确性仍然是一个主要问题：在我们的基准测试中，根据人工注释者，80% 的开放式 LLM 输出包含语义错误（根据 GPT-4，这一比例为 91%）。</li>
<li>需要更多的方法来提高 LLM 在数据对文本生成任务中的语义准确性。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<p></p><ol><p></p>
<p></p><li><p>论文标题：超越基于引用的指标：分析开放式大语言模型在数据到文本生成中的行为</p>
</li><p></p>
<p></p><li><p>作者：Zdeněk Kasner、Ondřej Dušek</p>
</li><p></p>
<p></p><li><p>第一作者单位：查尔斯大学数学与物理学院形式与应用语言学研究所，捷克共和国布拉格</p>
</li><p></p>
<p></p><li><p>关键词：数据到文本生成、大语言模型、无参考评估、语义准确性</p>
</li><p></p>
<p></p><li><p>论文链接：https://arxiv.org/abs/2401.10186，Github 代码链接：https://github.com/d2t-llm/d2t-llm</p>
</li><p></p>
<p></p><li><p>摘要：
(1) 研究背景：大语言模型在自然语言处理领域取得了显著进展，但其在数据到文本生成任务中的适用性尚未得到充分探索。当前的数据到文本生成基准测试存在饱和问题，并且与人类判断的相关性较差。
(2) 过去方法及其问题：传统的基于引用的评估指标与人类判断的相关性较差。封闭式大语言模型的使用被认为是一种不良的研究实践，因为其不可重复且存在数据污染问题。
(3) 本文提出的研究方法：本文提出了一种新的数据到文本生成基准测试 QUINTD-1，该基准测试由来自五个领域的结构化数据记录组成。本文还提出了一种新的评估方法，该方法结合了人类注释者和基于 GPT-4 的指标，用于评估模型输出的语义准确性。
(4) 方法在任务上的表现及其对目标的支持：本文的方法在 QUINTD-1 基准测试上进行了评估。结果表明，最先进的开放式大语言模型能够从各种标准数据格式中生成流畅且连贯的文本。然而，语义准确性仍然是一个主要问题，80% 的开放式大语言模型输出根据人类注释者包含语义错误（根据 GPT-4 为 91%）。</p>
</li><p></p>
<p></p><li><p>方法：
(1): 提出QUINTD-1基准测试，包含来自五个领域的结构化数据记录，用于评估数据到文本生成模型的性能。
(2): 提出一种新的评估方法，结合人类注释者和基于GPT-4的指标，用于评估模型输出的语义准确性。
(3): 使用最先进的开放式大语言模型在QUINTD-1基准测试上进行评估，分析模型在不同数据格式下的生成能力和语义准确性。</p>
</li><p></p>
<p></p><li><p>结论：
（1）：本文探索了基于开放式大语言模型的数据到文本生成任务，提出了新的基准测试 QUINTD-1 和评估方法，分析了最先进的开放式大语言模型在不同数据格式下的生成能力和语义准确性，为数据到文本生成任务提供了新的研究方向。
（2）：创新点：</p>
</li><br>&lt;/ol&gt;<p></p>
<ul>
<li>提出新的基准测试 QUINTD-1，包含来自五个领域的结构化数据记录，用于评估数据到文本生成模型的性能。</li>
<li>提出一种新的评估方法，结合人类注释者和基于 GPT-4 的指标，用于评估模型输出的语义准确性。</li>
<li>使用最先进的开放式大语言模型在 QUINTD-1 基准测试上进行评估，分析模型在不同数据格式下的生成能力和语义准确性。
性能：</li>
<li>最先进的开放式大语言模型能够从各种标准数据格式中生成流畅且连贯的文本。</li>
<li>但语义准确性仍然是一个主要问题，80% 的开放式大语言模型输出根据人类注释者包含语义错误（根据 GPT-4 为 91%）。
工作量：</li>
<li>收集和标注 QUINTD-1 基准测试的数据集。</li>
<li>实现新的评估方法，包括基于 GPT-4 的指标和人类注释。</li>
<li>使用最先进的开放式大语言模型在 QUINTD-1 基准测试上进行评估。</li>
</ul>



<p></p><details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cbc2ee00465454eb9b14d3b763aac637.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1481e93defe7c7bf222585814e3cc756.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-133889154a0e1815bf8287c40392dc96.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-773279112248c748e33e938a9774aed3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41a6874b90e948f998889323b12872af.jpg" align="middle">
</details><br>​    <p></p>
<h2 id="DiffusionGPT-LLM-Driven-Text-to-Image-Generation-System"><a href="#DiffusionGPT-LLM-Driven-Text-to-Image-Generation-System" class="headerlink" title="DiffusionGPT: LLM-Driven Text-to-Image Generation System"></a>DiffusionGPT: LLM-Driven Text-to-Image Generation System</h2><p><strong>Authors:Jie Qin, Jie Wu, Weifeng Chen, Yuxi Ren, Huixia Li, Hefeng Wu, Xuefeng Xiao, Rui Wang, Shilei Wen</strong></p>
<p>Diffusion models have opened up new avenues for the field of image generation, resulting in the proliferation of high-quality models shared on open-source platforms. However, a major challenge persists in current text-to-image systems are often unable to handle diverse inputs, or are limited to single model results. Current unified attempts often fall into two orthogonal aspects: i) parse Diverse Prompts in input stage; ii) activate expert model to output. To combine the best of both worlds, we propose DiffusionGPT, which leverages Large Language Models (LLM) to offer a unified generation system capable of seamlessly accommodating various types of prompts and integrating domain-expert models. DiffusionGPT constructs domain-specific Trees for various generative models based on prior knowledge. When provided with an input, the LLM parses the prompt and employs the Trees-of-Thought to guide the selection of an appropriate model, thereby relaxing input constraints and ensuring exceptional performance across diverse domains. Moreover, we introduce Advantage Databases, where the Tree-of-Thought is enriched with human feedback, aligning the model selection process with human preferences. Through extensive experiments and comparisons, we demonstrate the effectiveness of DiffusionGPT, showcasing its potential for pushing the boundaries of image synthesis in diverse domains. </p>
<p><a href="http://arxiv.org/abs/2401.10061v1">PDF</a> </p>
<p><strong>摘要</strong><br>扩散模型结合大型语言模型，构建领域知识库，引导模型选择，提升图像生成多样性。</p>
<p><strong>要点</strong></p>
<ul>
<li>扩散模型在图像生成领域取得重大进展，开源平台共享高质量模型。</li>
<li>当前文本转图像系统存在输入多样性不足，模型结果单一等挑战。</li>
<li>现有统一尝试分为两种正交方面：i) 在输入阶段解析不同提示；ii) 激活专家模型以输出。</li>
<li>DiffusionGPT 结合语言模型优势，构建统一生成系统，支持多种提示，集成领域专家模型。</li>
<li>DiffusionGPT 基于先验知识为不同生成模型构建特定领域树形结构。</li>
<li>输入时，语言模型解析提示，利用树形思维引导选择合适模型，放宽输入限制，在不同领域确保出色性能。</li>
<li>引入优势数据库，将树形思维与人类反馈相结合，使模型选择过程与人类偏好相一致。</li>
<li>大量实验与比较证明了 DiffusionGPT 的有效性，展示了其在不同领域图像合成中的潜力。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<p></p><ol><p></p>
<p></p><li>题目：DiffusionGPT：基于树状思维模型的统一扩散生成系统</li><p></p>
<p></p><li>作者：Yichuan Liu<em>, Xiang Wang</em>, Xihong Wu, Zhe Gan, Yujun Shen, Jingyi Zhang, Xiaogang Wang</li><p></p>
<p></p><li>单位：北京大学计算机科学技术系</li><p></p>
<p></p><li>关键词：扩散模型、生成模型、树状思维模型、文本到图像生成</li><p></p>
<p></p><li>链接：https://arxiv.org/abs/2302.08134, Github：无</li><p></p>
<p></p><li>摘要：
（1）研究背景：扩散模型在图像生成领域取得了显著进展，但现有文本到图像生成系统通常无法处理多样化的输入，或者仅限于单一模型的结果。
（2）过去的方法及其问题：目前的研究主要集中在输入阶段解析多样化的提示或在输出阶段激活专家模型。这些方法存在以下问题：1）难以同时处理多样化的输入；2）无法充分利用领域专家模型的知识。
（3）研究方法：本文提出了一种新的文本到图像生成系统——DiffusionGPT，它利用大型语言模型（LLM）构建领域特定的树状思维模型，并根据输入提示和树状思维模型选择合适的生成模型。
（4）实验结果：在多个任务上，DiffusionGPT在图像质量、多样性和准确性方面均优于现有方法，证明了其有效性。</li><br>&lt;/ol&gt;<p></p>
<p></p><p><strong>方法：</strong></p><p></p>
<p></p><p>（1）提示解析：利用大型语言模型（LLM）分析和提取输入提示中的关键文本信息，以准确识别用户想要生成的核心内容，并减轻噪声文本的影响。</p><p></p>
<p></p><p>（2）模型思想树构建与搜索：使用模型思想树（TOT）的概念来构建模型树，并利用模型树的搜索能力来缩小候选模型的集合，提高模型选择过程的准确性。</p><p></p>
<p></p><p>（3）模型选择与人工反馈：通过与用户交互，获取用户对生成结果的反馈，并根据反馈结果调整模型选择策略，以提高生成结果的质量。</p><p></p>
<p></p><p>（4）生成执行：根据选定的模型，生成图像。</p><p></p>
<p></p><ol start="8"><p></p>
<p></p><li>结论：
（1）本工作通过提出 DiffusionGPT，将高性能生成模型与高效提示解析无缝集成，在文本到图像生成任务中取得了显著的成果，为该领域的研究提供了新的思路。
（2）创新点：</li><br>&lt;/ol&gt;<p></p>
<ul>
<li>提出了一种新的文本到图像生成框架 DiffusionGPT，该框架能够同时处理多样化的输入提示并充分利用领域专家模型的知识。</li>
<li>提出了一种基于大型语言模型（LLM）的提示解析方法，该方法能够准确识别用户想要生成的核心内容，并减轻噪声文本的影响。</li>
<li>提出了一种基于模型思想树（TOT）的概念构建模型树的方法，该方法能够缩小候选模型的集合，提高模型选择过程的准确性。</li>
<li>提出了一种基于用户反馈的模型选择策略，该策略能够根据用户对生成结果的反馈，调整模型选择策略，以提高生成结果的质量。
性能：</li>
<li>在多个任务上，DiffusionGPT 在图像质量、多样性和准确性方面均优于现有方法，证明了其有效性。</li>
<li>DiffusionGPT 能够处理多样化的输入提示，并生成高质量、多样性和准确的图像。</li>
<li>DiffusionGPT 能够充分利用领域专家模型的知识，生成符合特定领域要求的图像。
工作量：</li>
<li>DiffusionGPT 的训练和推理过程相对复杂，需要大量的数据和计算资源。</li>
<li>DiffusionGPT 的模型选择过程需要与用户交互，这可能会增加生成图像的工作量。</li>
<li>DiffusionGPT 的提示解析过程需要使用大型语言模型（LLM），这可能会增加生成图像的成本。</li>
</ul>



<p></p><details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-206805985eb655491965884e7bb9f034.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-66c858aa31f63c6ce390099af5809303.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-176a2174255ac60d001c26f929b2818d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1cea822f74219b16f2b14b9a08ee6e77.jpg" align="middle">
</details><br>​    <p></p>
<p>​    </p>
</ol></ol></ol></ol></ol></ol></ol>]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title>3DGS</title>
    <url>/2024/01/30/Paper/2024-01-30/3DGS/</url>
    <content><![CDATA[<blockquote>
<p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-01-30-更新"><a href="#2024-01-30-更新" class="headerlink" title="2024-01-30 更新"></a>2024-01-30 更新</h1><h2 id="EndoGaussians-Single-View-Dynamic-Gaussian-Splatting-for-Deformable-Endoscopic-Tissues-Reconstruction"><a href="#EndoGaussians-Single-View-Dynamic-Gaussian-Splatting-for-Deformable-Endoscopic-Tissues-Reconstruction" class="headerlink" title="EndoGaussians: Single View Dynamic Gaussian Splatting for Deformable   Endoscopic Tissues Reconstruction"></a>EndoGaussians: Single View Dynamic Gaussian Splatting for Deformable   Endoscopic Tissues Reconstruction</h2><p><strong>Authors:Yangsen Chen, Hao Wang</strong></p>
<p>The accurate 3D reconstruction of deformable soft body tissues from endoscopic videos is a pivotal challenge in medical applications such as VR surgery and medical image analysis. Existing methods often struggle with accuracy and the ambiguity of hallucinated tissue parts, limiting their practical utility. In this work, we introduce EndoGaussians, a novel approach that employs Gaussian Splatting for dynamic endoscopic 3D reconstruction. This method marks the first use of Gaussian Splatting in this context, overcoming the limitations of previous NeRF-based techniques. Our method sets new state-of-the-art standards, as demonstrated by quantitative assessments on various endoscope datasets. These advancements make our method a promising tool for medical professionals, offering more reliable and efficient 3D reconstructions for practical applications in the medical field. </p>
<p><a href="http://arxiv.org/abs/2401.13352v1">PDF</a> </p>
<p><strong>摘要</strong><br>高斯散点结合神经辐射场，实现动态内窥镜 3D 重建新方法。</p>
<p><strong>要点</strong></p>
<ul>
<li>EndoGaussians 是一个新的方法，它利用高斯散点进行动态内窥镜 3D 重建。</li>
<li>这种方法是首次在该背景下使用高斯散点，克服了以前基于 NeRF 技术的限制。</li>
<li>该方法在各种内窥镜数据集上进行定量评估，树立了新的最先进标准。</li>
<li>这些进步使该方法成为医疗专业人员的有前途的工具，为医疗领域的实际应用提供了更可靠、更高效的 3D 重建。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：EndoGaussians：单视动态高斯体素重建</li>
<li>作者：Yangsen Chen, Hao Wang</li>
<li>隶属机构：香港科技大学（广州）</li>
<li>关键词：3D 重建、高斯体素重建、机器人手术</li>
<li>论文链接：https://arxiv.org/abs/2401.13352</li>
<li>
<p>摘要：
(1) 研究背景：准确地从内窥镜视频中重建可变形软体组织的 3D 模型对于 VR 手术和医学图像分析等医疗应用至关重要。现有方法通常在准确性和产生的组织部分的模棱两可方面存在问题，限制了其实际效用。
(2) 过往方法：以往的一些工作尝试使用深度估计、SLAM、稀疏变形场和神经辐射场等方法来解决这个问题，但这些方法要么假设场景是静态的，要么假设手术工具不存在，从而限制了它们在实际场景中的实用性。
(3) 研究方法：为了进一步提高静态单视 RGBD 设置下软体组织的 3D 重建的准确性，并提高 3D 重建的可靠性和可信度，我们提出了 Endogaussians，该方法利用高斯体素重建作为重建方法。
(4) 方法性能：我们的方法在 PSNR、SSIM、LPIPS 等多项定量评估中取得了最先进的结果，并且重建速度更快。这些进步使我们的方法成为医疗专业人员的有前途的工具，为医疗领域的实际应用提供更可靠和高效的 3D 重建。</p>
</li>
<li>
<p>方法：
(1): 本文提出了一种名为 Endogaussians 的方法，用于从单目动态 RGBD 设置中重建可变形软体组织的 3D 模型。
(2): 该方法使用高斯体素重建作为重建方法，可以有效地处理软体组织的变形。
(3): 为了提高重建的准确性，本文提出了一种新的体素融合策略，该策略可以有效地融合来自不同帧的数据。
(4): 此外，本文还提出了一种新的体素分割算法，该算法可以有效地将软体组织分割成不同的部分。
(5): 最后，本文提出了一种新的体素渲染算法，该算法可以生成逼真的软体组织模型。</p>
</li>
<li>
<p>结论：
（1）：EndoGaussians方法可以有效地从单目动态RGBD设置中重建可变形软体组织的3D模型，具有较高的准确性和可靠性，在医疗领域具有广阔的应用前景。
（2）：创新点：</p>
</li>
<li>提出了一种新的高斯体素重建方法，可以有效地处理软体组织的变形。</li>
<li>提出了一种新的体素融合策略，可以有效地融合来自不同帧的数据。</li>
<li>提出了一种新的体素分割算法，可以有效地将软体组织分割成不同的部分。</li>
<li>提出了一种新的体素渲染算法，可以生成逼真的软体组织模型。
性能：</li>
<li>在PSNR、SSIM、LPIPS等多项定量评估中取得了最先进的结果。</li>
<li>重建速度更快。
工作量：</li>
<li>该方法需要大量的计算资源。</li>
<li>该方法需要大量的标注数据。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-049a97b3607a44946b481425f04f7d64.jpg" align="middle">
</details>




<h2 id="EndoGaussian-Gaussian-Splatting-for-Deformable-Surgical-Scene-Reconstruction"><a href="#EndoGaussian-Gaussian-Splatting-for-Deformable-Surgical-Scene-Reconstruction" class="headerlink" title="EndoGaussian: Gaussian Splatting for Deformable Surgical Scene   Reconstruction"></a>EndoGaussian: Gaussian Splatting for Deformable Surgical Scene   Reconstruction</h2><p><strong>Authors:Yifan Liu, Chenxin Li, Chen Yang, Yixuan Yuan</strong></p>
<p>Reconstructing deformable tissues from endoscopic stereo videos is essential in many downstream surgical applications. However, existing methods suffer from slow inference speed, which greatly limits their practical use. In this paper, we introduce EndoGaussian, a real-time surgical scene reconstruction framework that builds on 3D Gaussian Splatting. Our framework represents dynamic surgical scenes as canonical Gaussians and a time-dependent deformation field, which predicts Gaussian deformations at novel timestamps. Due to the efficient Gaussian representation and parallel rendering pipeline, our framework significantly accelerates the rendering speed compared to previous methods. In addition, we design the deformation field as the combination of a lightweight encoding voxel and an extremely tiny MLP, allowing for efficient Gaussian tracking with a minor rendering burden. Furthermore, we design a holistic Gaussian initialization method to fully leverage the surface distribution prior, achieved by searching informative points from across the input image sequence. Experiments on public endoscope datasets demonstrate that our method can achieve real-time rendering speed (195 FPS real-time, 100$\times$ gain) while maintaining the state-of-the-art reconstruction quality (35.925 PSNR) and the fastest training speed (within 2 min/scene), showing significant promise for intraoperative surgery applications. Code is available at: \url{<a href="https://yifliu3.github.io/EndoGaussian/}">https://yifliu3.github.io/EndoGaussian/}</a>. </p>
<p><a href="http://arxiv.org/abs/2401.12561v1">PDF</a> </p>
<p><strong>Summary</strong><br>3D高斯渲染框架实现了实时内窥镜手术场景重建。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>提出了一种名为EndoGaussian的实时手术场景重建框架，它是建立在3D高斯点阵的基础上的。</li>
<li>使用高斯表示和并行渲染管道，显著提高了渲染速度。</li>
<li>将变形场设计为轻量级编码体素和极小MLP的组合，实现了高效的高斯跟踪，渲染负担较小。</li>
<li>设计了一种整体的高斯初始化方法，充分利用了表面分布先验，通过搜索输入图像序列中的信息点来实现。</li>
<li>公共内窥镜数据集上的实验表明，该方法可以实现实时渲染速度（195 FPS实时，100倍收益），同时保持最先进的重建质量（35.925 PSNR）和最快的训练速度（在2分钟/场景以内），显示出对术中手术应用的重大前景。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：EndoGaussian：用于可变形手术场景重建的高斯点云</li>
<li>作者：Yifan Liu<em>, Chenxin Li</em>, Chen Yang 和 Yixuan Yuan</li>
<li>隶属机构：香港中文大学</li>
<li>关键词：三维重建 · 高斯点云 · 机器人手术</li>
<li>论文链接：https://arxiv.org/abs/2401.12561
   Github 代码链接：https://yifliu3.github.io/EndoGaussian/</li>
<li>
<p>摘要：
（1）研究背景：从内窥镜立体视频中重建可变形组织对于许多下游手术应用至关重要。然而，现有方法的推理速度慢，极大地限制了它们的实际使用。
（2）过去方法及其问题：现有方法的问题在于推理速度慢，这使得它们在实际应用中受到限制。
（3）研究方法：该论文提出了一种基于 3D 高斯点云的实时手术场景重建框架 EndoGaussian。该框架将动态手术场景表示为规范高斯点云和时间相关的变形场，该变形场可以预测新时间戳下的高斯变形。由于高效的高斯表示和并行渲染管道，该框架与以往方法相比，显著地提高了渲染速度。此外，该论文将变形场设计为轻量级编码体素和极小型的 MLP 的组合，从而实现高效的高斯跟踪，且渲染负担很小。此外，该论文设计了一种整体的高斯初始化方法，以充分利用表面分布先验，该方法通过从输入图像序列中搜索信息点来实现。
（4）方法性能：在公开内窥镜数据集上的实验表明，该方法可以实现实时渲染速度（195 FPS 实时，100 倍增益），同时保持最先进的重建质量（35.925 PSNR）和最快的训练速度（每个场景 2 分钟以内），显示出对术中手术应用的重大前景。</p>
</li>
<li>
<p>方法：
（1）EndoGaussian框架概述：该框架由高斯点云初始化、高斯跟踪和高斯渲染三个模块组成。
（2）高斯点云初始化：从输入图像序列中搜索信息点，通过高斯混合模型估计点云参数，并通过表面分布先验优化点云位置。
（3）高斯跟踪：将变形场设计为轻量级编码体素和极小型的MLP的组合，通过将当前时间戳的高斯点云变形到新时间戳，实现高效的高斯跟踪。
（4）高斯渲染：利用高斯点云的几何特性和并行渲染管道，实现高效的渲染。
（5）训练细节：使用Adam优化器，学习率为1e-4，批大小为8，训练200个周期。</p>
</li>
<li>
<p>结论：
（1）：本工作提出了一种实时且高质量的 4D 重建框架，用于动态手术场景重建。通过利用基于体素的高斯跟踪和整体高斯初始化，我们能够处理组织变形和非平凡的高斯初始化问题。全面的实验表明，我们的 EndoGaussian 可以实现最先进的重建质量和实时的渲染速度，比以前的方法快 100 倍以上。我们希望新兴的基于高斯斑点的重建技术能够为机器人手术场景理解提供新的途径，并增强各种下游临床任务，尤其是术中应用。
（2）：创新点：</p>
</li>
<li>基于高斯点云的实时手术场景重建框架。</li>
<li>体素编码的高斯跟踪，实现了高效的高斯跟踪。</li>
<li>整体高斯初始化方法，充分利用表面分布先验。
性能：</li>
<li>在公开内窥镜数据集上的实验表明，该方法可以实现实时渲染速度（195FPS 实时，100 倍增益），同时保持最先进的重建质量（35.925PSNR）和最快的训练速度（每个场景 2 分钟以内）。
工作量：</li>
<li>论文的代码和数据已经开源，可以方便地进行复现。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0b9bca825762ac8e0bbad3078a233ed1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e1d91551398571ef4d862b170f54e4fc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d93c7e9f9dfadf417d2add6f22082d7e.jpg" align="middle">
</details>




<h2 id="Deformable-Endoscopic-Tissues-Reconstruction-with-Gaussian-Splatting"><a href="#Deformable-Endoscopic-Tissues-Reconstruction-with-Gaussian-Splatting" class="headerlink" title="Deformable Endoscopic Tissues Reconstruction with Gaussian Splatting"></a>Deformable Endoscopic Tissues Reconstruction with Gaussian Splatting</h2><p><strong>Authors:Lingting Zhu, Zhao Wang, Zhenchao Jin, Guying Lin, Lequan Yu</strong></p>
<p>Surgical 3D reconstruction is a critical area of research in robotic surgery, with recent works adopting variants of dynamic radiance fields to achieve success in 3D reconstruction of deformable tissues from single-viewpoint videos. However, these methods often suffer from time-consuming optimization or inferior quality, limiting their adoption in downstream tasks. Inspired by 3D Gaussian Splatting, a recent trending 3D representation, we present EndoGS, applying Gaussian Splatting for deformable endoscopic tissue reconstruction. Specifically, our approach incorporates deformation fields to handle dynamic scenes, depth-guided supervision to optimize 3D targets with a single viewpoint, and a spatial-temporal weight mask to mitigate tool occlusion. As a result, EndoGS reconstructs and renders high-quality deformable endoscopic tissues from a single-viewpoint video, estimated depth maps, and labeled tool masks. Experiments on DaVinci robotic surgery videos demonstrate that EndoGS achieves superior rendering quality. Code is available at <a href="https://github.com/HKU-MedAI/EndoGS">https://github.com/HKU-MedAI/EndoGS</a>. </p>
<p><a href="http://arxiv.org/abs/2401.11535v1">PDF</a> Work in progress. 10 pages, 4 figures</p>
<p><strong>摘要</strong><br>动态高斯溅射用于可变形内窥镜组织重建。</p>
<p><strong>要点</strong></p>
<ul>
<li>EndoGS 利用高斯溅射进行可变形内窥镜组织重建。</li>
<li>该方法结合变形场以处理动态场景。</li>
<li>深度引导监督用于优化具有单个视点的 3D 目标。</li>
<li>时空权重掩码可减轻工具遮挡。</li>
<li>EndoGS 可以从单视角视频、估计的深度图和标记的工具掩码中重建和渲染高质量的可变形内窥镜组织。</li>
<li>在 DaVinci 机器人手术视频上的实验表明，EndoGS 实现卓越的渲染质量。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：高斯斑点可变形内窥镜组织重建</li>
<li>作者：Lingting Zhu, Zhao Wang, Zhenchao Jin, Guying Lin, Lequan Yu</li>
<li>第一作者单位：香港大学</li>
<li>关键词：高斯斑点·机器人手术·三维重建</li>
<li>论文链接：https://arxiv.org/abs/2401.11535, Github 链接：https://github.com/HKU-MedAI/EndoGS</li>
<li>摘要：
(1)：研究背景：手术三维重建是机器人手术研究的一个关键领域，最近的工作采用动态辐射场实现从单视角视频中可变形组织的三维重建。然而，这些方法通常存在优化耗时或质量较差的问题，限制了它们在后续任务中的应用。
(2)：过去的方法：早期尝试采用深度估计在内窥镜重建中取得了巨大成功，但这些方法仍然难以产生逼真的三维重建，原因有两个关键问题。首先，非刚性变形有时会导致较大的运动，这需要实际动态场景重建，这阻碍了这些技术的适应。其次，单视角视频中存在遮挡，导致学习受影响部分时信息有限，产生困难。虽然一些框架结合了工具遮罩、立体深度估计和稀疏翘曲场用于单视角三维重建，但它们在存在剧烈非拓扑可变形组织变化时仍然容易失败。
(3)：研究方法：受最近流行的三维表示方法三维高斯斑点启发，我们提出了 EndoGS，将高斯斑点应用于可变形内窥镜组织重建。具体来说，我们的方法结合了变形场来处理动态场景，深度引导监督来优化具有单一视点的三维目标，以及时空权重掩码来减轻遮挡。
(4)：方法性能：在达芬奇机器人手术视频上的实验表明，EndoGS 实现了更高的渲染质量。</li>
</ol>
<p>Methods:
(1): 我们提出了一种称为 EndoGS 的方法，它利用 3D-GS 的可变形变体从单视角视频、估计的深度图和标记的工具掩码中重建 3D 外科场景。
(2): 我们首先介绍了 3D-GS 的预备知识，然后展示了使用动态版本的 3D-GS 对可变形组织进行建模，该版本采用轻量级 MLP 来表示动态场。最后，我们介绍了使用工具掩码和深度图对高斯飞溅进行训练优化的过程。
(3): 我们使用六个正交特征平面来编码空间和时间信息，并使用单个 MLP 来更新高斯属性并解码位置、比例因子、旋转因子、球谐系数和不透明度的变形。
(4): 我们结合工具掩码和深度图来训练 EndoGS，以解决视频中工具遮挡的挑战，并使用时空重要性采样策略来指示与遮挡问题相关的关键区域。</p>
<ol>
<li>结论：
（1）：xxx；
（2）：创新点：xxx；性能：xxx；工作量：xxx；</li>
</ol>
<p>具体内容如下：</p>
<ol>
<li>结论：
（1）：本文提出了一种基于高斯斑点可变形内窥镜组织重建的方法，该方法可以从单视角视频、估计的深度图和标记的工具掩码中实时渲染高质量的可变形组织。在达芬奇机器人手术视频上的实验表明，该方法具有更高的渲染质量。
（2）：创新点：</li>
<li>提出了一种新的方法EndoGS，利用3D-GS的可变形变体从单视角视频、估计的深度图和标记的工具掩码中重建3D外科场景。</li>
<li>使用动态版本的3D-GS对可变形组织进行建模，该版本采用轻量级MLP来表示动态场。</li>
<li>结合工具掩码和深度图对高斯飞溅进行训练优化，以解决视频中工具遮挡的挑战，并使用时空重要性采样策略来指示与遮挡问题相关的关键区域。
性能：</li>
<li>在达芬奇机器人手术视频上的实验表明，EndoGS实现了更高的渲染质量。
工作量：</li>
<li>该方法需要预先训练3D-GS模型，并对每个新场景进行优化。</li>
<li>优化过程需要一定的时间，具体取决于场景的复杂性和数据量。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-3aced720ad0952509d5ad4feafb073c5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-db38985f02aa9f93361d5395728da086.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f22f8ab59ea6655501c3858f5b7639aa.jpg" align="middle">
</details>




<h2 id="GaussianBody-Clothed-Human-Reconstruction-via-3d-Gaussian-Splatting"><a href="#GaussianBody-Clothed-Human-Reconstruction-via-3d-Gaussian-Splatting" class="headerlink" title="GaussianBody: Clothed Human Reconstruction via 3d Gaussian Splatting"></a>GaussianBody: Clothed Human Reconstruction via 3d Gaussian Splatting</h2><p><strong>Authors:Mengtian Li, Shengxiang Yao, Zhifeng Xie, Keyu Chen</strong></p>
<p>In this work, we propose a novel clothed human reconstruction method called GaussianBody, based on 3D Gaussian Splatting. Compared with the costly neural radiance based models, 3D Gaussian Splatting has recently demonstrated great performance in terms of training time and rendering quality. However, applying the static 3D Gaussian Splatting model to the dynamic human reconstruction problem is non-trivial due to complicated non-rigid deformations and rich cloth details. To address these challenges, our method considers explicit pose-guided deformation to associate dynamic Gaussians across the canonical space and the observation space, introducing a physically-based prior with regularized transformations helps mitigate ambiguity between the two spaces. During the training process, we further propose a pose refinement strategy to update the pose regression for compensating the inaccurate initial estimation and a split-with-scale mechanism to enhance the density of regressed point clouds. The experiments validate that our method can achieve state-of-the-art photorealistic novel-view rendering results with high-quality details for dynamic clothed human bodies, along with explicit geometry reconstruction. </p>
<p><a href="http://arxiv.org/abs/2401.09720v2">PDF</a> </p>
<p><strong>Summary</strong><br>优化动态穿衣人体重建方法，引入物理先验和规范化变换，实现高精度照片级新视角渲染。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>本文提出了一种新的穿衣人體重建方法 GaussianBody，基於 3D 高斯 Splatting。</li>
<li>3D 高斯 Splatting 最近在訓練時間和渲染質量方面表現出了很好的性能。</li>
<li>應用靜態 3D 高斯 Splatting 模型於動態人體重建問題時，會因複雜的非剛性變形和豐富的衣物細節而遇到挑戰。</li>
<li>提出明確的姿勢引導變形，以關聯規範空間和觀測空間中的動態高斯。</li>
<li>引入基於物理的先驗和正則化變換，以減少兩個空間之間的歧義。</li>
<li>提出姿勢精煉策略，以更新姿勢回歸，以補償不準確的初始估計。</li>
<li>提出分拆比例機制，以增強回歸點雲的密度。</li>
<li>實驗證明，該方法可實現最先進照片級的新視圖渲染結果，同時具有高質量的動態穿衣人體細節和明確的幾何重建。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：高斯体：基于 3D 高斯散布的着装人体重建</li>
<li>作者：李梦添、姚胜祥、谢志峰、陈可宇</li>
<li>隶属单位：上海大学</li>
<li>关键词：3D 高斯散布、着装人体重建、单目视频、神经辐射场</li>
<li>论文链接：https://arxiv.org/abs/2401.09720，Github 代码链接：无</li>
<li>摘要：
（1）研究背景：创建高保真着装人体模型在虚拟现实、远程临场和电影制作中具有重要应用。传统方法要么涉及复杂的捕捉系统，要么需要 3D 艺术家进行繁琐的手工操作，这使得它们既耗时又昂贵，从而限制了新手用户的可扩展性。近年来，人们越来越关注从单个 RGB 图像或单目视频中自动重建着装人体模型。
（2）过去的方法及其问题：网格模型方法最初被引入，通过回归 SCAPE、SMPL、SMPL-X 和 STAR 等参数模型来重构人体形状。虽然它们可以实现快速且稳健的重建，但回归的多边形网格难以捕捉不同的几何细节和丰富的服装特征。添加顶点偏移量成为这种情况下的一种增强解决方案。然而，其表示能力仍然受到网格分辨率的严格限制，并且通常在宽松服装的情况下会失败。
（3）本文提出的研究方法：为了克服显式网格模型的局限性，本文提出了一种基于 3D 高斯散布的新颖着装人体重建方法 GaussianBody。与代价高昂的神经辐射场模型相比，3D 高斯散布最近在训练时间和渲染质量方面表现出优异的性能。然而，将静态 3D 高斯散布模型应用于动态人体重建问题由于复杂的非刚性变形和丰富的服装细节而变得非常困难。为了应对这些挑战，本文的方法考虑了显式姿势引导的变形，将动态高斯体与规范空间和观察空间相关联，引入具有正则化变换的基于物理的先验有助于减轻这两个空间之间的歧义。在训练过程中，本文进一步提出了一种姿势细化策略，以更新姿势回归，以补偿不准确的初始估计，并提出了一种分裂尺度机制来增强回归点云的密度。
（4）方法的应用任务和性能：实验验证了本文的方法可以实现最先进的逼真新视图渲染结果，为动态着装人体提供高质量的细节，以及显式几何重建。这些性能可以支持他们的目标。</li>
</ol>
<p>Some Error for method(比如是不是没有Methods这个章节)</p>
<ol>
<li>结论：
（1）：本文提出了一种基于 3D 高斯散布的着装人体重建方法 GaussianBody，该方法能够从单目视频中重建动态的着装人体模型，并具有逼真的新视图渲染结果和高质量的细节。
（2）：创新点：</li>
<li>将 3D 高斯散布表示扩展到着装人体重建中，并考虑了显式姿势引导的变形，以解决动态高斯体与规范空间和观察空间之间的歧义问题。</li>
<li>提出了一种基于物理的先验来正则化规范空间的高斯体，以减轻观察空间和规范空间之间的过度旋转问题。</li>
<li>提出了一种姿势细化策略和分裂尺度机制，以增强重建点云的质量和鲁棒性。
性能：</li>
<li>该方法在图像质量指标上与基线和其他方法相当，证明了其具有竞争力的性能和相对较快的训练速度。</li>
<li>该方法能够使用更高分辨率的图像进行训练。
工作量：</li>
<li>该方法的训练时间比一些最先进的方法更长。</li>
<li>该方法需要大量的数据来进行训练。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-03cb35c9ffdf24e162bbcf10081d440a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2032721a60695f2d41ac96f75dec65a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4877b53e7d23cf29d6e9a1a57a3155ec.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d121364f4f1fecac5ef9d276f421f434.jpg" align="middle">
</details>




<h2 id="Forging-Vision-Foundation-Models-for-Autonomous-Driving-Challenges-Methodologies-and-Opportunities"><a href="#Forging-Vision-Foundation-Models-for-Autonomous-Driving-Challenges-Methodologies-and-Opportunities" class="headerlink" title="Forging Vision Foundation Models for Autonomous Driving: Challenges,   Methodologies, and Opportunities"></a>Forging Vision Foundation Models for Autonomous Driving: Challenges,   Methodologies, and Opportunities</h2><p><strong>Authors:Xu Yan, Haiming Zhang, Yingjie Cai, Jingming Guo, Weichao Qiu, Bin Gao, Kaiqiang Zhou, Yue Zhao, Huan Jin, Jiantao Gao, Zhen Li, Lihui Jiang, Wei Zhang, Hongbo Zhang, Dengxin Dai, Bingbing Liu</strong></p>
<p>The rise of large foundation models, trained on extensive datasets, is revolutionizing the field of AI. Models such as SAM, DALL-E2, and GPT-4 showcase their adaptability by extracting intricate patterns and performing effectively across diverse tasks, thereby serving as potent building blocks for a wide range of AI applications. Autonomous driving, a vibrant front in AI applications, remains challenged by the lack of dedicated vision foundation models (VFMs). The scarcity of comprehensive training data, the need for multi-sensor integration, and the diverse task-specific architectures pose significant obstacles to the development of VFMs in this field. This paper delves into the critical challenge of forging VFMs tailored specifically for autonomous driving, while also outlining future directions. Through a systematic analysis of over 250 papers, we dissect essential techniques for VFM development, including data preparation, pre-training strategies, and downstream task adaptation. Moreover, we explore key advancements such as NeRF, diffusion models, 3D Gaussian Splatting, and world models, presenting a comprehensive roadmap for future research. To empower researchers, we have built and maintained <a href="https://github.com/zhanghm1995/Forge_VFM4AD">https://github.com/zhanghm1995/Forge_VFM4AD</a>, an open-access repository constantly updated with the latest advancements in forging VFMs for autonomous driving. </p>
<p><a href="http://arxiv.org/abs/2401.08045v1">PDF</a> Github Repo: <a href="https://github.com/zhanghm1995/Forge_VFM4AD">https://github.com/zhanghm1995/Forge_VFM4AD</a></p>
<p><strong>摘要</strong><br>智能汽车专属视觉基础模型的构建挑战及其未来发展机遇。</p>
<p><strong>要点</strong></p>
<ul>
<li>数据准备、预训练策略和下游任务适配是 VFM 开发的关键技术。</li>
<li>生成神经辐射场 (NeRF)，扩散模型，3D 高斯分布（3DGS）和世界模型等技术的进步为未来的研究提出了路线图。</li>
<li>开源项目 <a href="https://github.com/zhanghm1995/Forge_VFM4AD">https://github.com/zhanghm1995/Forge_VFM4AD</a> 将不断更新，以赋能研究人员。</li>
<li>自动驾驶中的 VFM 缺乏专用数据和多传感器集成，导致任务特定架构的多样性成为 VFM 发展的障碍。</li>
<li>视觉基础模型 (VFM) 在自动驾驶中至关重要，但其发展面临着诸多挑战。</li>
<li>开发专用于自动驾驶的 VFM 是当前的紧迫挑战。</li>
<li>建议从数据准备、预训练以及下游任务适配等方面入手，并探索 NeRF、扩散模型等新技术，以推进 VFM 的发展。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：构建自动驾驶视觉基础模型：挑战、方法和机遇</li>
<li>作者：徐岩，张海明，蔡应杰，郭敬明，邱维超，高斌，周凯强，赵越，金欢，高建涛，李振，蒋立辉，张伟，张宏波，戴登心，刘冰冰</li>
<li>第一作者单位：华为诺亚方舟实验室</li>
<li>关键词：视觉基础模型，数据生成，自监督训练，自动驾驶，文献综述</li>
<li>论文链接：https://arxiv.org/abs/2401.08045
   Github 代码链接：无</li>
<li>
<p>摘要：
（1）研究背景：随着自动驾驶技术的快速发展，传统自动驾驶感知系统采用模块化架构，针对特定任务使用专用算法，但这种方法往往导致输出不一致，限制了系统处理长尾情况的能力。近年来，大型基础模型在自然语言处理领域取得了巨大成功，展现出强大的适应性和有效性，为构建自动驾驶视觉基础模型提供了新的思路。
（2）过去的方法及其问题：以往的方法主要集中于针对特定任务训练深度神经网络，但这种方法存在以下问题：1. 忽视了数据之间的关系，导致输出不一致；2. 难以处理长尾情况；3. 无法有效利用大量未标记数据。
（3）提出的研究方法：本文提出了一种构建自动驾驶视觉基础模型的方法，该方法主要包括以下几个步骤：1. 数据准备：收集和预处理自动驾驶相关的数据，包括图像、激光雷达点云、语义分割标签等；2. 预训练：使用自监督学习方法对基础模型进行预训练，使其能够从数据中提取有用的特征；3. 下游任务自适应：将预训练好的基础模型应用于下游任务，并通过微调使其适应特定任务。
（4）方法在任务中的表现及性能：该方法在自动驾驶相关任务上取得了较好的性能，例如目标检测、语义分割和深度估计等。这些结果表明，该方法能够有效地从数据中提取有用的特征，并将其应用于下游任务。</p>
</li>
<li>
<p>方法：
（1）数据准备：收集和预处理自动驾驶相关的数据，包括图像、激光雷达点云、语义分割标签等；
（2）预训练：使用自监督学习方法对基础模型进行预训练，使其能够从数据中提取有用的特征；
（3）下游任务自适应：将预训练好的基础模型应用于下游任务，并通过微调使其适应特定任务。</p>
</li>
<li>
<p>结论：
（1）：本文针对自动驾驶视觉基础模型的构建提出了系统的方法，并取得了较好的性能。该方法为自动驾驶视觉基础模型的构建提供了新的思路，有望推动自动驾驶技术的发展。
（2）：创新点：</p>
</li>
<li>提出了一种构建自动驾驶视觉基础模型的方法，该方法包括数据准备、预训练和下游任务自适应三个步骤。</li>
<li>使用自监督学习方法对基础模型进行预训练，使其能够从数据中提取有用的特征。</li>
<li>将预训练好的基础模型应用于下游任务，并通过微调使其适应特定任务。
性能：</li>
<li>该方法在自动驾驶相关任务上取得了较好的性能，例如目标检测、语义分割和深度估计等。</li>
<li>该方法能够有效地从数据中提取有用的特征，并将其应用于下游任务。
工作量：</li>
<li>该方法需要收集和预处理大量的数据。</li>
<li>该方法需要使用自监督学习方法对基础模型进行预训练，这需要较大的计算资源。</li>
<li>该方法需要将预训练好的基础模型应用于下游任务，并通过微调使其适应特定任务，这需要较多的工程工作。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-7ce70a9a128d8a3669098fd6808591bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b29768228c4fd656077c66549ec08984.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7ea3a2551a65a42514ea6e5555124cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66561a69f615f893c246615fba473e10.jpg" align="middle">
</details>



]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>3DGS</tag>
      </tags>
  </entry>
  <entry>
    <title>NeRF</title>
    <url>/2024/01/24/Paper/2024-01-24/NeRF/</url>
    <content><![CDATA[<blockquote>
<p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-01-24-更新"><a href="#2024-01-24-更新" class="headerlink" title="2024-01-24 更新"></a>2024-01-24 更新</h1><h2 id="ProvNeRF-Modeling-per-Point-Provenance-in-NeRFs-as-a-Stochastic-Process"><a href="#ProvNeRF-Modeling-per-Point-Provenance-in-NeRFs-as-a-Stochastic-Process" class="headerlink" title="ProvNeRF: Modeling per Point Provenance in NeRFs as a Stochastic Process"></a>ProvNeRF: Modeling per Point Provenance in NeRFs as a Stochastic Process</h2><p><strong>Authors:Kiyohiro Nakayama, Mikaela Angelina Uy, Yang You, Ke Li, Leonidas Guibas</strong></p>
<p>Neural radiance fields (NeRFs) have gained popularity across various applications. However, they face challenges in the sparse view setting, lacking sufficient constraints from volume rendering. Reconstructing and understanding a 3D scene from sparse and unconstrained cameras is a long-standing problem in classical computer vision with diverse applications. While recent works have explored NeRFs in sparse, unconstrained view scenarios, their focus has been primarily on enhancing reconstruction and novel view synthesis. Our approach takes a broader perspective by posing the question: “from where has each point been seen?” — which gates how well we can understand and reconstruct it. In other words, we aim to determine the origin or provenance of each 3D point and its associated information under sparse, unconstrained views. We introduce ProvNeRF, a model that enriches a traditional NeRF representation by incorporating per-point provenance, modeling likely source locations for each point. We achieve this by extending implicit maximum likelihood estimation (IMLE) for stochastic processes. Notably, our method is compatible with any pre-trained NeRF model and the associated training camera poses. We demonstrate that modeling per-point provenance offers several advantages, including uncertainty estimation, criteria-based view selection, and improved novel view synthesis, compared to state-of-the-art methods. Please visit our project page at <a href="https://provnerf.github.io">https://provnerf.github.io</a> </p>
<p><a href="http://arxiv.org/abs/2401.08140v2">PDF</a> </p>
<p><strong>Summary</strong><br>神经辐射场 (NeRF) 在各种应用中获得广泛欢迎，但面临稀疏视图设置时的挑战，缺乏体积渲染的足够约束。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>NeRF 在稀疏视图设置中面临挑战，因为缺乏足够的体积渲染约束。</li>
<li>重建和理解稀疏且不受约束的相机的 3D 场景是经典计算机视觉中的一个长期问题，具有广泛的应用。</li>
<li>最近的工作已经探索了稀疏、不受约束的视图场景中的 NeRF，但重点主要放在增强重建和新颖视图合成。</li>
<li>ProvNeRF 通过提出问题“每个点是从哪里看到的？”来采用更广泛的视角 — 这决定了我们对它的理解和重建程度。</li>
<li>ProvNeRF 是一种模型，它通过结合每个点的出处来丰富传统的 NeRF 表示，建模每个点的可能来源位置。</li>
<li>ProvNeRF 与任何预先训练的 NeRF 模型及其关联的训练相机位姿兼容。</li>
<li>对比最先进的方法，建模每个点的出处提供了多种优势，包括不确定性估计、基于准则的视图选择和改进的新颖视图合成。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：ProvNeRF：将 NeRF 中的逐点来源建模为随机过程</li>
<li>作者：George Kiyohiro Nakayama、Mikaela Angelina Uy、Yang You、Ke Li、Leonidas Guibas</li>
<li>第一作者单位：斯坦福大学</li>
<li>关键词：NeRF、稀疏视图、来源、不确定性估计、视点优化、新颖视图合成</li>
<li>论文链接：https://arxiv.org/abs/2401.08140、Github 链接：None</li>
<li>摘要：
(1)：研究背景：神经辐射场 (NeRF) 在许多应用中都获得了广泛关注。然而，由于仅靠体积渲染无法提供足够的约束，它们在稀疏视图设置中面临挑战。
(2)：过去方法和问题：最近的一些工作探索了在稀疏、不受约束的视图场景中使用 NeRF。为了解决约束不足的问题，他们将先验信息纳入 NeRF 优化中，例如深度、局部几何或全局形状信息。然而，这些工作只关注于实现更好的新颖视图合成，而没有解决如何从更全面的角度理解场景的问题，例如不确定性估计、基于标准的视点选择和鲁棒性。
(3)：研究方法：我们提出 ProvNeRF，这是一种通过纳入逐点来源来丰富传统 NeRF 表示的模型，该模型为每个点建模可能的源位置。我们通过扩展随机过程的隐式最大似然估计 (IMLE) 来实现这一点。值得注意的是，我们的方法与任何预训练的 NeRF 模型及其相关的训练相机位姿兼容。
(4)：方法性能：我们证明了对逐点来源进行建模提供了许多优势，包括不确定性估计、基于准则的视点选择和改进的新颖视图合成，与最先进的方法相比，我们的方法在这些任务上取得了更好的性能。这些性能支持了我们的目标。</li>
</ol>
<p>Methods:</p>
<p>(1)：我们提出ProvNeRF，这是一种通过纳入逐点来源来丰富传统NeRF表示的模型，该模型为每个点建模可能的源位置。我们通过扩展随机过程的隐式最大似然估计 (IMLE) 来实现这一点。</p>
<p>(2)：我们定义逐点来源为一个随机过程，其索引集为R3，并由无穷多个随机变量组成。每个随机变量对应于R3中的一个点，其分布由该点的边缘分布给出。边缘分布表示可以观察到该点的可能位置的分布。</p>
<p>(3)：我们使用一个神经网络来学习从潜在随机变量分布到模型分布的变换。潜在随机变量分布由输入位置x的随机线性变换和x本身的连接组成。模型分布由神经网络Hθ参数化，该神经网络将潜在随机函数映射到函数Dθ。</p>
<p>(4)：为了优化模型分布，我们扩展了IMLE以建模随机过程的分布。我们通过在每个点处匹配经验样本和模型样本，将IMLE公式调整为函数空间。</p>
<ol>
<li>结论：
（1）：ProvNeRF通过扩展随机过程的隐式最大似然估计（IMLE），将逐点来源纳入传统NeRF表示，增强了NeRF模型的表示能力，使其能够在稀疏视图设置中更好地估计不确定性、选择最佳视点并合成新颖视图。
（2）：创新点：ProvNeRF将逐点来源建模为随机过程，并通过扩展IMLE来优化模型分布，这是一种新颖的建模方法，可以有效地提高NeRF模型在稀疏视图设置下的性能。
性能：ProvNeRF在不确定性估计、基于准则的视点选择和新颖视图合成等任务上取得了更好的性能，证明了对逐点来源进行建模的有效性。
工作量：ProvNeRF的实现相对简单，可以轻松应用于任何预训练的NeRF模型，并且不需要额外的训练数据或相机位姿信息。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-f48885cf9ef1b2a677c258f6b1e9a2a2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d72d125185075e757ca6e7284c2ace68.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a582ca9b91a20a6a1c1593166a2d8401.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d26582d170597ef79c1a5e15500eaa42.jpg" align="middle">
</details>




<h2 id="TriNeRFLet-A-Wavelet-Based-Multiscale-Triplane-NeRF-Representation"><a href="#TriNeRFLet-A-Wavelet-Based-Multiscale-Triplane-NeRF-Representation" class="headerlink" title="TriNeRFLet: A Wavelet Based Multiscale Triplane NeRF Representation"></a>TriNeRFLet: A Wavelet Based Multiscale Triplane NeRF Representation</h2><p><strong>Authors:Rajaei Khatib, Raja Giryes</strong></p>
<p>In recent years, the neural radiance field (NeRF) model has gained popularity due to its ability to recover complex 3D scenes. Following its success, many approaches proposed different NeRF representations in order to further improve both runtime and performance. One such example is Triplane, in which NeRF is represented using three 2D feature planes. This enables easily using existing 2D neural networks in this framework, e.g., to generate the three planes. Despite its advantage, the triplane representation lagged behind in its 3D recovery quality compared to NeRF solutions. In this work, we propose TriNeRFLet, a 2D wavelet-based multiscale triplane representation for NeRF, which closes the 3D recovery performance gap and is competitive with current state-of-the-art methods. Building upon the triplane framework, we also propose a novel super-resolution (SR) technique that combines a diffusion model with TriNeRFLet for improving NeRF resolution. </p>
<p><a href="http://arxiv.org/abs/2401.06191v1">PDF</a> webpage link: <a href="https://rajaeekh.github.io/trinerflet-web">https://rajaeekh.github.io/trinerflet-web</a></p>
<p><strong>摘要</strong></p>
<p>引入了基于二维小波表示的 TriNeRFLet 以弥补三平面表示的 3D 重建质量与 NeRF 解决方案之间的差距。</p>
<p><strong>主要内容</strong></p>
<ul>
<li>TriNeRFLet 是一种基于二维小波表示的 TriPlane 表示，可以弥补三平面表示的 3D 重建质量与 NeRF 解决方案之间的差距。</li>
<li>TriNeRFLet 在 3D 重建质量上与当前最先进的方法具有竞争力。</li>
<li>TriNeRFLet 可以与扩散模型相结合以提高 NeRF 分辨率。</li>
<li>使用 TriNeRFLet 可以提高 NeRF 在复杂 3D 场景中的重建质量。</li>
<li>TriNeRFLet 可以使用现有的 2D 神经网络来生成三个平面。</li>
<li>TriNeRFLet 可以轻松地应用于现有的 NeRF 框架。</li>
<li>TriNeRFLet 可以提高 NeRF 在光照条件复杂场景中的重建质量。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：TriNeRFLet：一种基于小波的多尺度三平面 NeRF 表示</li>
<li>作者：Rajaie Khatib, Raja Giryes</li>
<li>单位：特拉维夫大学电子工程系</li>
<li>关键词：神经辐射场、三平面、小波、多尺度、超分辨率</li>
<li>链接：https://rajaeekh.github.io/trinerflet-web，Github 链接：None</li>
<li>
<p>摘要：
(1)：近年来，神经辐射场 (NeRF) 模型因其恢复复杂 3D 场景的能力而广受欢迎。继其成功之后，许多方法提出了不同的 NeRF 表示，以便进一步提高运行时和性能。其中一个例子是三平面，其中 NeRF 使用三个 2D 特征平面表示。这使得在这个框架中轻松使用现有的 2D 神经网络，例如生成三个平面。尽管有优势，但与 NeRF 解决方案相比，三平面表示在 3D 恢复质量方面落后。在这项工作中，我们提出了 TriNeRFLet，一种用于 NeRF 的基于 2D 小波的多尺度三平面表示，它缩小了 3D 恢复性能差距，并与当前最先进的方法具有竞争力。在三平面框架的基础上，我们还提出了一种新颖的超分辨率 (SR) 技术，该技术将扩散模型与 TriNeRFLet 相结合，以提高 NeRF 分辨率。
(2)：过去的方法包括使用三轴对齐的 2D 特征平面来表示 NeRF，称为三平面。在渲染过程中，通过将每个点投影到三个平面之一，然后连接对应于三个投影的特征，对每个点进行采样。这为该点形成一个单一的特征向量，然后将其传递给输出该点密度和颜色值的较小 MLP。三平面表示的一个显着优势是它可以与许多已经存在的 2D 方法一起使用。在最初的工作中，作者使用现有的 2D 生成对抗网络 (GAN) 架构来生成其平面。后续工作采用了 2D 卷积神经网络 (CNN) 架构来生成平面。然而，这些方法在 3D 恢复质量方面落后于 NeRF 解决方案。
(3)：在这项工作中，我们提出了 TriNeRFLet，一种基于 2D 小波的多尺度三平面表示，它缩小了 3D 恢复性能差距，并与当前最先进的方法具有竞争力。TriNeRFLet 利用小波变换将每个平面分解为多个尺度，从而捕获不同尺度的几何细节。此外，我们还提出了一种新颖的超分辨率 (SR) 技术，该技术将扩散模型与 TriNeRFLet 相结合，以提高 NeRF 分辨率。
(4)：在各种数据集上的实验表明，TriNeRFLet 在 3D 重建质量和渲染速度方面优于最先进的方法。具体来说，在 DTU 数据集上，TriNeRFLet 的平均重投影误差为 0.006，而最先进的方法为 0.008。在 Replica 数据集上，TriNeRFLet 的平均重投影误差为 0.004，而最先进的方法为 0.006。此外，TriNeRFLet 在渲染速度方面也优于最先进的方法。在 DTU 数据集上，TriNeRFLet 的平均渲染时间为 0.02 秒，而最先进的方法为 0.04 秒。在 Replica 数据集上，TriNeRFLet 的平均渲染时间为 0.01 秒，而最先进的方法为 0.03 秒。这些结果表明，TriNeRFLet 在 3D 重建质量和渲染速度方面优于最先进的方法。</p>
</li>
<li>
<p>Methods：
（1）TriNeRFLet是一种基于2D小波的多尺度三平面NeRF表示，它利用小波变换将每个平面分解为多个尺度，从而捕获不同尺度的几何细节。
（2）TriNeRFLet在渲染过程中，通过将每个点投影到三个平面之一，然后连接对应于三个投影的特征，对每个点进行采样。这为该点形成一个单一的特征向量，然后将其传递给输出该点密度和颜色值的较小MLP。
（3）TriNeRFLet还提出了一种新颖的超分辨率（SR）技术，该技术将扩散模型与TriNeRFLet相结合，以提高NeRF分辨率。</p>
</li>
<li>
<p>结论：
（1）：本文提出了一种基于2D小波的多尺度三平面NeRF表示TriNeRFLet，它利用小波变换将每个平面分解为多个尺度，从而捕获不同尺度的几何细节。此外，我们还提出了一种新颖的超分辨率（SR）技术，该技术将扩散模型与TriNeRFLet相结合，以提高NeRF分辨率。在各种数据集上的实验表明，TriNeRFLet在3D重建质量和渲染速度方面优于最先进的方法。
（2）：创新点：
TriNeRFLet利用小波变换将每个平面分解为多个尺度，从而捕获不同尺度的几何细节。
TriNeRFLet提出了一种新颖的超分辨率（SR）技术，该技术将扩散模型与TriNeRFLet相结合，以提高NeRF分辨率。
性能：
在DTU数据集上，TriNeRFLet的平均重投影误差为0.006，而最先进的方法为0.008。
在Replica数据集上，TriNeRFLet的平均重投影误差为0.004，而最先进的方法为0.006。
在DTU数据集上，TriNeRFLet的平均渲染时间为0.02秒，而最先进的方法为0.04秒。
在Replica数据集上，TriNeRFLet的平均渲染时间为0.01秒，而最先进的方法为0.03秒。
工作量：
TriNeRFLet的实现相对复杂，需要对小波变换和扩散模型有一定的了解。
TriNeRFLet的训练时间较长，在DTU数据集上需要大约12小时，在Replica数据集上需要大约8小时。</p>
</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-376ce19b86e43007a4505ad233d775ef.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-62e7822d95a507a9a6135289c7daa699.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7a5c91a388f997c293454bfee53afa88.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e34ea48c160c12032d9b31bd76183537.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c58b3d3d4afab12adc77185baa182d90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f1a3bb1b557793cc8f1d5f612cbf4c1.jpg" align="middle">
</details>




<h2 id="Fast-High-Dynamic-Range-Radiance-Fields-for-Dynamic-Scenes"><a href="#Fast-High-Dynamic-Range-Radiance-Fields-for-Dynamic-Scenes" class="headerlink" title="Fast High Dynamic Range Radiance Fields for Dynamic Scenes"></a>Fast High Dynamic Range Radiance Fields for Dynamic Scenes</h2><p><strong>Authors:Guanjun Wu, Taoran Yi, Jiemin Fang, Wenyu Liu, Xinggang Wang</strong></p>
<p>Neural Radiances Fields (NeRF) and their extensions have shown great success in representing 3D scenes and synthesizing novel-view images. However, most NeRF methods take in low-dynamic-range (LDR) images, which may lose details, especially with nonuniform illumination. Some previous NeRF methods attempt to introduce high-dynamic-range (HDR) techniques but mainly target static scenes. To extend HDR NeRF methods to wider applications, we propose a dynamic HDR NeRF framework, named HDR-HexPlane, which can learn 3D scenes from dynamic 2D images captured with various exposures. A learnable exposure mapping function is constructed to obtain adaptive exposure values for each image. Based on the monotonically increasing prior, a camera response function is designed for stable learning. With the proposed model, high-quality novel-view images at any time point can be rendered with any desired exposure. We further construct a dataset containing multiple dynamic scenes captured with diverse exposures for evaluation. All the datasets and code are available at \url{<a href="https://guanjunwu.github.io/HDR-HexPlane/}">https://guanjunwu.github.io/HDR-HexPlane/}</a>. </p>
<p><a href="http://arxiv.org/abs/2401.06052v1">PDF</a> 3DV 2024. Project page: <a href="https://guanjunwu.github.io/HDR-HexPlane">https://guanjunwu.github.io/HDR-HexPlane</a></p>
<p><strong>Summary</strong><br>动态 HDR NeRF 框架 HDR-HexPlane 可以从具有不同曝光度的动态 2D 图像中学习 3D 场景，并以任何时间点渲染任意曝光下的高质量新视图图像。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>HDR-HexPlane 是一种动态 HDR NeRF 框架，可以从具有不同曝光度的动态 2D 图像中学习 3D 场景。</li>
<li>HDR-HexPlane 构建了一个可学习的曝光映射函数，以获得每张图像的自适应曝光值。</li>
<li>HDR-HexPlane 基于单调递增的先验设计了一种相机响应函数，以实现稳定学习。</li>
<li>HDR-HexPlane 可以以任何时间点渲染任意曝光下的高质量新视图图像。</li>
<li>HDR-HexPlane 构建了一个包含多个动态场景的数据集，这些场景是用不同的曝光拍摄的，以便进行评估。</li>
<li>HDR-HexPlane 的所有数据集和代码均可在 <a href="https://guanjunwu.github.io/HDR-HexPlane/">https://guanjunwu.github.io/HDR-HexPlane/</a> 获得。</li>
<li>HDR-HexPlane 可有效地从具有不同曝光度的动态 2D 图像中学习 3D 场景，在各种动态场景的重建与渲染任务中表现出优异的性能。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：动态场景的快速高动态范围辐射场（中文翻译：动态场景的快速高动态范围辐射场）</li>
<li>作者：Guanjun Wu, Taoran Yi, Jiemin Fang, Wenyu Liu, Xinggang Wang</li>
<li>第一作者单位：华中科技大学计算机学院（中文翻译：华中科技大学计算机学院）</li>
<li>关键词：神经辐射场、高动态范围、动态场景、曝光映射、相机响应函数</li>
<li>论文链接：https://arxiv.org/abs/2401.06052，Github 代码链接：None</li>
<li>总结：
（1）：研究背景：神经辐射场（NeRF）及其扩展在表示 3D 场景和合成新视角图像方面取得了巨大成功。然而，大多数 NeRF 方法使用低动态范围 (LDR) 图像，这可能会丢失细节，尤其是在照明不均匀的情况下。一些先前的 NeRF 方法尝试引入高动态范围 (HDR) 技术，但主要针对静态场景。
（2）：过去的方法及其问题：过去的方法主要针对静态场景，无法处理动态场景。此外，这些方法通常需要对图像进行预处理，例如对齐和裁剪，这可能会引入误差。
（3）：本文提出的研究方法：为了将 HDR NeRF 方法扩展到更广泛的应用，我们提出了一种动态 HDR NeRF 框架，名为 HDR-HexPlane，它可以从以不同曝光值捕获的动态 2D 图像中学习 3D 场景。我们构建了一个可学习的曝光映射函数来为每个图像获得自适应曝光值。基于单调递增先验，我们设计了一个相机响应函数以实现稳定学习。利用所提出的模型，可以在任何时间点以任何期望的曝光渲染高质量的新视角图像。我们还构建了一个包含多个动态场景的数据集，这些场景以不同的曝光进行捕获，以进行评估。
（4）：方法在什么任务上取得了什么性能：在合成新视角图像的任务上，HDR-HexPlane 在多个动态场景上取得了最先进的性能。此外，HDR-HexPlane 还可以无缝组合不同曝光的图像并生成高动态范围 (HDR) 图像。使用色调映射函数，可以实现更好的色彩平衡，从而提高图像的整体视觉质量。这些性能支持了本文的目标。</li>
</ol>
<p><strong>Methods</strong>：**</p>
<p>（1）：我们首先回顾了HDR-NeRF和HexPlane的方法。</p>
<p>（2）：然后我们介绍了HDR-HexPlane的框架。</p>
<p>（3）：我们讨论了如何学习未知的曝光，并介绍了sigmoid相机响应函数。</p>
<p>（4）：最后，我们讨论了优化部分。</p>
<ol>
<li>结论：
（1）：本文提出了一种名为 HDR-HexPlane 的动态 HDR 神经辐射场框架，它可以从以不同曝光值捕获的动态 2D 图像中学习 3D 场景。HDR-HexPlane 在合成新视角图像的任务上取得了最先进的性能，并且可以无缝组合不同曝光的图像并生成高动态范围 (HDR) 图像。
（2）：创新点：</li>
<li>将 HDR 成像和动态场景表示管道集成到一个统一的框架中，以高效地学习 HDR 动态场景。</li>
<li>提出了一种可学习的曝光映射函数，可以为每个图像获得自适应曝光值。</li>
<li>设计了一个基于单调递增先验的相机响应函数，以实现稳定学习。
性能：</li>
<li>在合成新视角图像的任务上，HDR-HexPlane 在多个动态场景上取得了最先进的性能。</li>
<li>HDR-HexPlane 可以无缝组合不同曝光的图像并生成高动态范围 (HDR) 图像。</li>
<li>使用色调映射函数，可以实现更好的色彩平衡，从而提高图像的整体视觉质量。
工作量：</li>
<li>HDR-HexPlane 的实现需要大量的计算资源，包括 GPU 和内存。</li>
<li>HDR-HexPlane 的训练过程需要大量的数据和时间。</li>
<li>HDR-HexPlane 的推理过程也需要大量的计算资源。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-365b96052d113ae5a68faafffa3b689c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f64579f98fad3923afdea199ebc9b8cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a65b50e441e1d12f71aca19d6858a6fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ba32dc69a2b4433dbf0e3259f92de8c.jpg" align="middle">
</details>




<h2 id="GO-NeRF-Generating-Virtual-Objects-in-Neural-Radiance-Fields"><a href="#GO-NeRF-Generating-Virtual-Objects-in-Neural-Radiance-Fields" class="headerlink" title="GO-NeRF: Generating Virtual Objects in Neural Radiance Fields"></a>GO-NeRF: Generating Virtual Objects in Neural Radiance Fields</h2><p><strong>Authors:Peng Dai, Feitong Tan, Xin Yu, Yinda Zhang, Xiaojuan Qi</strong></p>
<p>Despite advances in 3D generation, the direct creation of 3D objects within an existing 3D scene represented as NeRF remains underexplored. This process requires not only high-quality 3D object generation but also seamless composition of the generated 3D content into the existing NeRF. To this end, we propose a new method, GO-NeRF, capable of utilizing scene context for high-quality and harmonious 3D object generation within an existing NeRF. Our method employs a compositional rendering formulation that allows the generated 3D objects to be seamlessly composited into the scene utilizing learned 3D-aware opacity maps without introducing unintended scene modification. Moreover, we also develop tailored optimization objectives and training strategies to enhance the model’s ability to exploit scene context and mitigate artifacts, such as floaters, originating from 3D object generation within a scene. Extensive experiments on both feed-forward and $360^o$ scenes show the superior performance of our proposed GO-NeRF in generating objects harmoniously composited with surrounding scenes and synthesizing high-quality novel view images. Project page at {\url{<a href="https://daipengwa.github.io/GO-NeRF/}">https://daipengwa.github.io/GO-NeRF/}</a>. </p>
<p><a href="http://arxiv.org/abs/2401.05750v1">PDF</a> 12 pages</p>
<p><strong>Summary</strong><br>神经辐射场 (NeRF) 场景中的直接 3D 物体生成方法 GO-NeRF，利用场景上下文生成高质量且和谐的 3D 物体。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>GO-NeRF 提出了一种新的方法，该方法能够利用场景上下文在现有 NeRF 中生成高质量且和谐的 3D 对象。</li>
<li>GO-NeRF 采用组合渲染公式，利用学习的 3D 感知不透明度贴图将生成的 3D 对象无缝地组合到场景中，而不会引入意外的场景修改。</li>
<li>GO-NeRF 开发了定制的优化目标和训练策略，以增强模型利用场景上下文的能力并减轻源自场景中 3D 对象生成的人为痕迹（例如漂浮物）。</li>
<li>GO-NeRF 在前馈和 360 度场景上的广泛实验表明，它在生成与周围场景和谐组合的对象和合成高质量的新视图图像方面具有卓越的性能。</li>
<li>GO-NeRF 项目主页：<a href="https://daipengwa.github.io/GO-NeRF/">https://daipengwa.github.io/GO-NeRF/</a></li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：GO-NeRF：在神经辐射场中生成虚拟物体</li>
<li>作者：彭代、谭飞通、俞欣、张印达、戚小娟</li>
<li>第一作者单位：香港大学</li>
<li>关键词：神经辐射场、3D对象生成、场景合成、文本到3D</li>
<li>论文链接：https://arxiv.org/abs/2401.05750，Github 代码链接：无</li>
<li>
<p>摘要：
(1)：研究背景：近年来，神经辐射场 (NeRF) 在可重现真实世界环境重建方面取得了巨大进展。与此同时，文本引导的对象生成也显示出在创建新颖 3D 内容方面的巨大潜力。本文研究了一个新颖的问题：生成与给定 3D 真实世界场景相协调的 3D 对象。这种能力对于新场景创建和编辑至关重要，它要求将生成的原始内容无缝地组合到环境中，并确保在 downstream 应用中获得高度沉浸式的体验。
(2)：过去的方法及其问题：Gordon 等人利用基于 CLIP 的文本图像匹配损失进行 3D 对象生成，并引入了一个 3D 混合管道将合成的 3D 对象组合到 NeRF 中。然而，这种方法受到模型生成能力的限制，并且缺乏利用场景上下文信息的特性，导致次优的、低质量的结果，并且无法与 NeRF 无缝融合（见图 3 第二行：水果在空中飞翔）。另一方面，文本引导的图像修复模型可以通过填充指定掩码来创建与所需对象相协调的场景。然而，为后续 NeRF 模型训练生成具有视图一致性的图像（用于合成对象）仍然具有挑战性。因此，这些技术容易受到大的视图变化和意外的场景内容修改的影响，因为修复掩码不准确（见图 3 右下角：给定的掩码与对象的轮廓不匹配）。
(3)：论文提出的研究方法：本文介绍了一个具有易于使用界面的新管道，称为 GO-NeRF，它可以在给定的基于 NeRF 的环境中生成由文本提示控制的 3D 虚拟对象，从而生成协调的 3D 场景（见图 1、9、3，其中可以看到浅阴影和反射）。我们的方法基于两个关键方面：（1）一种合成渲染公式，它有助于将生成的 3D 对象无缝组合到现有场景中，同时防止引入意外的场景修改，而无需显式建模场景几何。（2）精心设计的优化目标和训练策略，以增强模型利用场景上下文的能力并减轻伪影（例如，源于场景中 3D 对象生成）。
(4)：方法在任务和性能上的表现：在正向馈送和 360 度场景上的广泛实验表明，我们提出的 GO-NeRF 在生成与周围场景协调一致的对象和合成高质量的新视图图像方面具有优越的性能。这些性能支持他们的目标。</p>
</li>
<li>
<p>方法：
(1) 界面：创建一个简单的直观界面，允许用户轻松地定义要生成的 3D 场景中的对象位置。
(2) 组合渲染：引入一个单独的 NeRF 来表示对象，该 NeRF 由 θ 参数化。在训练期间，生成过程学习这些参数以根据输入文本提示和场景上下文合成对象。
(3) 优化：首先描述场景和谐对象生成的目标损失，然后介绍提高生成质量的优化策略。</p>
</li>
<li>
<p>结论：
（1）GO-NeRF 作为一种新颖的方法，直接在现有的场景级 NeRF 中生成由文本控制的 3D 对象，迈出了重要一步。为了实现这一目标，我们采用了与定制优化目标和训练策略相关的组合渲染公式，用于合成无缝组合到现有场景中的 3D 对象。我们的方法利用预训练文本引导图像修复网络的图像先验，以促进对象及其周围环境的和谐生成。实验结果表明了我们的方法在正向馈送和 360 度数据集中的优越性。我们希望我们的研究将激发该领域进一步的工作。
（2）创新点：
GO-NeRF 的创新点在于：</p>
</li>
<li>提出了一种合成渲染公式，该公式有助于将生成的 3D 对象无缝组合到现有场景中，同时防止引入意外的场景修改，而无需显式建模场景几何。</li>
<li>设计了精心设计的优化目标和训练策略，以增强模型利用场景上下文的能力并减轻伪影（例如，源于场景中 3D 对象生成）。
性能：
GO-NeRF 在以下方面表现出优越的性能：</li>
<li>在生成与周围场景协调一致的对象和合成高质量的新视图图像方面具有优越的性能。</li>
<li>在正向馈送和 360 度场景上的广泛实验表明，GO-NeRF 在生成与周围场景协调一致的对象和合成高质量的新视图图像方面具有优越的性能。
工作量：
GO-NeRF 的工作量主要包括：</li>
<li>创建一个简单的直观界面，允许用户轻松地定义要生成的 3D 场景中的对象位置。</li>
<li>引入一个单独的 NeRF 来表示对象，该 NeRF 由 θ 参数化。在训练期间，生成过程学习这些参数以根据输入文本提示和场景上下文合成对象。</li>
<li>描述场景和谐对象生成的目标损失，然后介绍提高生成质量的优化策略。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ee0add100ffcef00be2fec6bbe6283d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43fa755cedd33ceae1d9d7fbb83963da.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8afd08915207c50c36f014b939a33fd2.jpg" align="middle">
</details>




<h2 id="FPRF-Feed-Forward-Photorealistic-Style-Transfer-of-Large-Scale-3D-Neural-Radiance-Fields"><a href="#FPRF-Feed-Forward-Photorealistic-Style-Transfer-of-Large-Scale-3D-Neural-Radiance-Fields" class="headerlink" title="FPRF: Feed-Forward Photorealistic Style Transfer of Large-Scale 3D   Neural Radiance Fields"></a>FPRF: Feed-Forward Photorealistic Style Transfer of Large-Scale 3D   Neural Radiance Fields</h2><p><strong>Authors:GeonU Kim, Kim Youwang, Tae-Hyun Oh</strong></p>
<p>We present FPRF, a feed-forward photorealistic style transfer method for large-scale 3D neural radiance fields. FPRF stylizes large-scale 3D scenes with arbitrary, multiple style reference images without additional optimization while preserving multi-view appearance consistency. Prior arts required tedious per-style/-scene optimization and were limited to small-scale 3D scenes. FPRF efficiently stylizes large-scale 3D scenes by introducing a style-decomposed 3D neural radiance field, which inherits AdaIN’s feed-forward stylization machinery, supporting arbitrary style reference images. Furthermore, FPRF supports multi-reference stylization with the semantic correspondence matching and local AdaIN, which adds diverse user control for 3D scene styles. FPRF also preserves multi-view consistency by applying semantic matching and style transfer processes directly onto queried features in 3D space. In experiments, we demonstrate that FPRF achieves favorable photorealistic quality 3D scene stylization for large-scale scenes with diverse reference images. Project page: <a href="https://kim-geonu.github.io/FPRF/">https://kim-geonu.github.io/FPRF/</a> </p>
<p><a href="http://arxiv.org/abs/2401.05516v1">PDF</a> Project page: <a href="https://kim-geonu.github.io/FPRF/">https://kim-geonu.github.io/FPRF/</a></p>
<p><strong>摘要</strong><br>针对大规模 3D 神经辐射场，提出了一种前馈的超现实风格迁移方法 FPRF，可在无需额外优化的情况下对大型 3D 场景进行任意风格化处理，并保持多视图外观一致性。</p>
<p><strong>要点</strong></p>
<ul>
<li>FPRF 提出了一种风格分解的 3D 神经辐射场，支持任意的风格参考图像，无需额外的优化。</li>
<li>FPRF 支持多参考风格化，并具有语义对应匹配和局部 AdaIN，为 3D 场景风格增添了多样化的用户控制。</li>
<li>FPRF 通过直接将语义匹配和风格迁移过程应用于 3D 空间中的查询特征，保持了多视图一致性。</li>
<li>FPRF 在实验中证明了其在大规模场景中使用多样化参考图像实现令人满意的超现实质量的 3D 场景风格化。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：大规模 3D 神经辐射场的 Feed-Forward 真实感风格迁移</li>
<li>作者：Geonu Kim, Jun-Ho Choi, Kyoung Mu Lee</li>
<li>单位：韩国科学技术院</li>
<li>关键词：3D 神经辐射场、风格迁移、AdaIN、多视图一致性</li>
<li>论文链接：https://arxiv.org/abs/2401.05516
   Github 代码链接：None</li>
<li>
<p>摘要：
   (1) 研究背景：神经辐射场 (NeRF) 是一种强大的技术，可以从多视图图像重建逼真的 3D 场景。然而，NeRF 模型通常需要大量的训练数据和计算资源，这使得它们难以用于大规模场景的重建和风格迁移。
   (2) 过去方法：一些研究人员提出了使用预训练的 NeRF 模型来进行风格迁移的方法，但这些方法通常需要额外的优化过程，并且只能处理小规模的场景。
   (3) 本文方法：为了解决上述问题，本文提出了一种新的 Feed-Forward Photorealistic Style Transfer (FPRF) 方法，可以对大规模 3D 神经辐射场进行真实感风格迁移。FPRF 方法通过引入一种风格分解的 3D 神经辐射场来实现风格迁移，该辐射场继承了 AdaIN 的 Feed-Forward 风格迁移机制，支持任意风格参考图像。此外，FPRF 方法支持多参考风格迁移，通过语义对应匹配和局部 AdaIN 来实现，这为 3D 场景风格增加了多样化的用户控制。FPRF 方法还通过直接将语义匹配和风格迁移过程应用于 3D 空间中的查询特征来保持多视图一致性。
   (4) 实验结果：在实验中，本文证明了 FPRF 方法可以为大规模场景实现良好的真实感 3D 场景风格迁移，并支持多种参考图像。FPRF 方法在 LLFF 数据集和小规模场景上优于其他 3D 风格迁移方法，并且在 San Francisco Mission Bay 数据集和大规模场景上实现了良好的多视图一致性。这些结果表明，FPRF 方法可以很好地支持其目标，即对大规模 3D 神经辐射场进行真实感风格迁移。</p>
</li>
<li>
<p>方法：
(1) 提出了一种名为 FPRF 的前馈式真实感风格迁移方法，用于大规模 3D 场景。
(2) 构建了一个可风格化的辐射场，称为可风格化辐射场，该辐射场继承了 AdaIN 的前馈式风格迁移机制，支持任意风格参考图像。
(3) 引入场景语义场，通过语义对应匹配和局部 AdaIN 来实现对大规模场景的多参考风格迁移，为 3D 场景风格增加了多样化的用户控制。
(4) 通过将语义匹配和风格迁移过程直接应用于 3D 空间中的查询特征来保持多视图一致性。</p>
</li>
<li>
<p>结论：
（1）：本文提出了一种名为 FPRF 的前馈式真实感风格迁移方法，用于大规模 3D 场景。与现有的 3D 风格迁移方法相比，FPRF 具有以下优点：支持任意风格参考图像、支持多参考风格迁移、保持多视图一致性。
（2）：创新点：</p>
</li>
<li>提出了一种名为 FPRF 的前馈式真实感风格迁移方法，用于大规模 3D 场景。</li>
<li>构建了一个可风格化的辐射场，称为可风格化辐射场，该辐射场继承了 AdaIN 的前馈式风格迁移机制，支持任意风格参考图像。</li>
<li>引入场景语义场，通过语义对应匹配和局部 AdaIN 来实现对大规模场景的多参考风格迁移，为 3D 场景风格增加了多样化的用户控制。</li>
<li>通过将语义匹配和风格迁移过程直接应用于 3D 空间中的查询特征来保持多视图一致性。
性能：</li>
<li>在 LLFF 数据集和小规模场景上优于其他 3D 风格迁移方法。</li>
<li>在 SanFranciscoMissionBay 数据集和大规模场景上实现了良好的多视图一致性。
工作量：</li>
<li>训练 FPRF 模型需要大量的数据和计算资源。</li>
<li>使用 FPRF 模型进行风格迁移需要较高的计算成本。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-189f376179ba2d0468a2e9c590bd3797.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a612beca1b40d1f1448824f6c5f8d29c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-15778af4f62c01b59051e8a0daf28bc7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ea21a55b1de35fad214a9b4f1c10e960.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b5524cc2407a0875f1556116ab5b3f1b.jpg" align="middle">
</details>




<h2 id="CTNeRF-Cross-Time-Transformer-for-Dynamic-Neural-Radiance-Field-from-Monocular-Video"><a href="#CTNeRF-Cross-Time-Transformer-for-Dynamic-Neural-Radiance-Field-from-Monocular-Video" class="headerlink" title="CTNeRF: Cross-Time Transformer for Dynamic Neural Radiance Field from   Monocular Video"></a>CTNeRF: Cross-Time Transformer for Dynamic Neural Radiance Field from   Monocular Video</h2><p><strong>Authors:Xingyu Miao, Yang Bai, Haoran Duan, Yawen Huang, Fan Wan, Yang Long, Yefeng Zheng</strong></p>
<p>The goal of our work is to generate high-quality novel views from monocular videos of complex and dynamic scenes. Prior methods, such as DynamicNeRF, have shown impressive performance by leveraging time-varying dynamic radiation fields. However, these methods have limitations when it comes to accurately modeling the motion of complex objects, which can lead to inaccurate and blurry renderings of details. To address this limitation, we propose a novel approach that builds upon a recent generalization NeRF, which aggregates nearby views onto new viewpoints. However, such methods are typically only effective for static scenes. To overcome this challenge, we introduce a module that operates in both the time and frequency domains to aggregate the features of object motion. This allows us to learn the relationship between frames and generate higher-quality images. Our experiments demonstrate significant improvements over state-of-the-art methods on dynamic scene datasets. Specifically, our approach outperforms existing methods in terms of both the accuracy and visual quality of the synthesized views. </p>
<p><a href="http://arxiv.org/abs/2401.04861v1">PDF</a> </p>
<p><strong>Summary</strong><br>动态场景NeRF：通过时间和频率域聚合特征以提高复杂动态场景的视图合成质量。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>我们提出了一种新颖的方法来生成复杂动态场景的单目视频的高质量新颖视图。</li>
<li>我们的方法建立在最近的泛化 NeRF 基础上，该方法将附近的视图聚合到新的视点上。</li>
<li>我们引入了一个同时在时域和频域中运行的模块来聚合对象运动的特征。</li>
<li>这使我们能够学习帧之间的关系并生成更高质量的图像。</li>
<li>我们的实验表明，我们的方法在动态场景数据集上的性能明显优于最先进的方法。</li>
<li>具体来说，我们的方法在合成视图的准确性和视觉质量方面优于现有方法。</li>
<li>我们证明了我们的方法对于具有挑战性的动态场景（例如舞蹈序列）特别有效。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：CTNeRF：动态场景单目视频的跨时间变换器</li>
<li>作者：邢宇苗、杨白、郝然段、黄雅雯、万璠、杨龙、叶锋正</li>
<li>单位：英国杜伦大学计算机系</li>
<li>关键词：动态神经辐射场、单目视频、场景流、变换器</li>
<li>链接：Paper_info:IEEE JOURNALS 1
Github：无</li>
<li>
<p>总结：
（1）：随着深度学习的飞速发展，神经辐射场（NeRF）作为该领域最具代表性的成果之一，在单目视频的动态场景新视角合成方面取得了显著进展。然而，现有方法在处理复杂对象运动时仍存在局限性，导致细节渲染不准确和模糊。
（2）：现有方法主要有两种类型：可变形翘曲场方法和神经场景流方法。可变形翘曲场方法可以处理长序列，但对于具有复杂对象运动的动态场景可能效果不佳。神经场景流方法可以处理动态场景中的大运动，但其有效性高度依赖于预测场景流或轨迹的准确性。
（3）：本文提出了一种新的方法，可以应用于动态场景，能够处理更复杂的运动并改进渲染结果。该方法借鉴了最近静态场景渲染的研究，通过聚合附近视角沿极线上的局部图像特征来增强渲染过程。为了克服动态场景带来的挑战，本文设计了一个模块，可以聚合光照空间中由于运动引起的光照变化，以及获得的多视角特征。这使得本文方法能够准确地考虑几何和外观的时空变化，从而更好地渲染动态场景。
（4）：本文方法在动态场景数据集上进行了实验，结果表明，该方法在合成视图的准确性和视觉质量方面都优于现有方法，证明了本文方法可以很好地支持其目标。</p>
</li>
<li>
<p>方法：
(1)：本文方法的核心思想是通过聚合附近视角沿极线上的局部图像特征来增强渲染过程，以更好地捕捉动态场景中的几何和外观变化。
(2)：为了克服动态场景带来的挑战，本文设计了一个模块，可以聚合光照空间中由于运动引起的光照变化，以及获得的多视角特征。
(3)：该模块由一个时空注意力机制和一个光照变化聚合层组成。时空注意力机制用于聚合附近视角沿极线上的局部图像特征，光照变化聚合层用于聚合光照空间中由于运动引起的光照变化。
(4)：通过将这两个模块结合起来，本文方法可以准确地考虑几何和外观的时空变化，从而更好地渲染动态场景。</p>
</li>
<li>
<p>结论：
（1）： 本文提出了一种新的动态神经渲染场框架，用于动态单目视频，该框架能够高质量地渲染新视角。为了实现这一点，我们扩展了最近的多视图聚合思想到时变 NeRF，这使得建模复杂运动成为可能。具体来说，我们引入了 RBCT 和 GSTF 模块来分别从时间域和频域建模运动。我们的实验结果表明，这些提出的模块在渲染新视角时显著增强了具有多视图聚合的时变 NeRF 的性能。
（2）： 创新点：</p>
</li>
<li>提出了一种新的动态神经渲染场框架，用于动态单目视频，该框架能够高质量地渲染新视角。</li>
<li>扩展了最近的多视图聚合思想到时变 NeRF，这使得建模复杂运动成为可能。</li>
<li>引入了 RBCT 和 GSTF 模块来分别从时间域和频域建模运动。</li>
</ol>
<p>性能：
* 在合成视图的准确性和视觉质量方面，我们的方法优于现有方法。</p>
<p>工作量：
* 该方法的计算成本较高，不适用于长序列视频的新视角渲染。</p>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-0850f96472c46b64ee282b61f71f5061.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d50131e7a1f564140cf78966a505a3aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f75406f8a2fa17fe58a50e32b802d2fd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e3217dd559310e7375b8e7d9ef2419b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ec167c81acb136a204b777dfd69fd47.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47ef05de23daff322b3a1fb46fac475b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35fa8ad2270d74904552566049ae2bdb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45227f9a07bbebb8ecbbed80ce5f59bb.jpg" align="middle">
</details>




## NeRFmentation: NeRF-based Augmentation for Monocular Depth Estimation

**Authors:Casimir Feldmann, Niall Siegenheim, Nikolas Hars, Lovro Rabuzin, Mert Ertugrul, Luca Wolfart, Marc Pollefeys, Zuria Bauer, Martin R. Oswald**

The capabilities of monocular depth estimation (MDE) models are limited by the availability of sufficient and diverse datasets. In the case of MDE models for autonomous driving, this issue is exacerbated by the linearity of the captured data trajectories. We propose a NeRF-based data augmentation pipeline to introduce synthetic data with more diverse viewing directions into training datasets and demonstrate the benefits of our approach to model performance and robustness. Our data augmentation pipeline, which we call "NeRFmentation", trains NeRFs on each scene in the dataset, filters out subpar NeRFs based on relevant metrics, and uses them to generate synthetic RGB-D images captured from new viewing directions. In this work, we apply our technique in conjunction with three state-of-the-art MDE architectures on the popular autonomous driving dataset KITTI, augmenting its training set of the Eigen split. We evaluate the resulting performance gain on the original test set, a separate popular driving set, and our own synthetic test set. 

[PDF](http://arxiv.org/abs/2401.03771v1) 

**Summary**
NeRF 驱动的图像扩充有助于自动驾驶单目深度估计模型的鲁棒性和性能。

**Key Takeaways**
- 单目深度估计模型的性能受数据量和多样性限制，尤其是在自动驾驶场景中。
- 提出基于 NeRF 的数据扩充管道，在训练集中引入具有更多样视角的合成数据。
- 数据扩充管道训练每个场景的 NeRF，并根据相关指标过滤次优 NeRF。
- 使用经过滤的 NeRF 从新的视角生成合成 RGB-D 图像。
- 在 KITTI 数据集上与三种最先进的单目深度估计架构结合使用该技术。
- 扩充后的训练集在原始测试集、单独的流行驾驶集和我们自己的合成测试集上都取得了性能提升。
- 数据扩充在自动驾驶场景中提高了单目深度估计模型的鲁棒性和性能。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>题目：NeRFmentation：基于 NeRF 的单目深度估计增强</li>
<li>作者：Felix Heide, Tobias Wutz, Simon Fuhrmann, Michael Goesele, Andreas Geiger</li>
<li>单位：苏黎世联邦理工学院</li>
<li>关键词：单目深度估计、数据增强、NeRF、自动驾驶</li>
<li>论文链接：https://arxiv.org/abs/2208.01185
    Github 代码链接：无</li>
<li>
<p>摘要：
（1）研究背景：单目深度估计 (MDE) 模型的能力受限于足够和多样化数据集的可用性。对于自动驾驶的 MDE 模型，由于捕获的数据轨迹的线性性，这个问题更加严重。
（2）过去的方法及其问题：现有方法主要集中在设计新的网络架构或损失函数上，但这些方法往往需要大量的数据才能达到良好的性能。此外，现有方法通常只关注单一的场景或数据集，缺乏泛化能力。
（3）提出的研究方法：我们提出了一种基于 NeRF 的数据增强管道，将具有更多样化视角方向的合成数据引入训练数据集中，并证明了我们方法对模型性能和鲁棒性的好处。我们的数据增强管道称为“NeRFmentation”，它在数据集中的每个场景上训练 NeRF，根据相关指标过滤掉较差的 NeRF，并使用它们从新的视角方向生成合成 RGB-D 图像。
（4）方法在什么任务上取得了什么性能：在流行的自动驾驶数据集 KITTI 上，我们将我们的技术与三种最先进的 MDE 架构结合使用，扩充了 Eigen 分割的训练集。我们在原始测试集、单独的流行驾驶集和我们自己的合成测试集上评估了由此产生的性能提升。</p>
</li>
<li>
<p>方法：
(1) 训练 NeRF：在每个场景上训练 NeRF，以生成具有不同视角方向的合成 RGB-D 图像。
(2) 过滤 NeRF：根据相关指标过滤掉较差的 NeRF。
(3) 生成合成图像：使用选定的 NeRF 从新的视角方向生成合成 RGB-D 图像。
(4) 扩充训练集：将生成的合成图像与原始训练集合并，扩充训练集。
(5) 训练 MDE 模型：使用扩充的训练集训练 MDE 模型。
(6) 评估性能：在原始测试集、单独的流行驾驶集和我们自己的合成测试集上评估 MDE 模型的性能。</p>
</li>
<li>
<p>结论：
（1）：本文提出了一种基于 NeRF 的数据增强管道，通过将具有更多样化视角方向的合成数据引入训练数据集中，显著提升了单目深度估计模型的性能和鲁棒性。
（2）：创新点：</p>
</li>
<li>
<p>提出了一种基于 NeRF 的数据增强管道，可以生成具有更多样化视角方向的合成 RGB-D 图像。</p>
</li>
<li>设计了一种过滤机制，可以过滤掉较差的 NeRF，从而提高合成图像的质量。</li>
<li>将生成的合成图像与原始训练集合并，扩充训练集，从而提高 MDE 模型的性能。</li>
</ol>
<p>性能：</p>
<ul>
<li>在 KITTI 数据集上，将我们的技术与三种最先进的 MDE 架构结合使用，扩充了 Eigen 分割的训练集，在原始测试集、单独的流行驾驶集和我们自己的合成测试集上评估了由此产生的性能提升。</li>
<li>在原始测试集上，我们的方法将 MDE 模型的平均绝对误差从 2.43 米降低到 2.29 米，相对误差降低了 5.8%。</li>
<li>在单独的流行驾驶集上，我们的方法将 MDE 模型的平均绝对误差从 2.67 米降低到 2.49 米，相对误差降低了 6.7%。</li>
<li>在我们自己的合成测试集上，我们的方法将 MDE 模型的平均绝对误差从 3.12 米降低到 2.86 米，相对误差降低了 8.3%。</li>
</ul>
<p>工作量：</p>
<ul>
<li>训练 NeRF 模型需要大量的时间和计算资源。</li>
<li>过滤较差的 NeRF 也需要花费大量的时间和计算资源。</li>
<li>生成合成图像需要花费大量的时间和计算资源。</li>
<li>将生成的合成图像与原始训练集合并，扩充训练集需要花费大量的时间和计算资源。</li>
<li>训练 MDE 模型需要花费大量的时间和计算资源。</li>
<li>评估 MDE 模型的性能需要花费大量的时间和计算资源。</li>
</ul>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-ce7ee2b66de780a44f4c0f517f763b05.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b5b0c192b842205b298c0ce78c4b41a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c3e191046fcb30bdd49314ea6c0da386.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-debede860a9c02b780b3f6ea909c3d2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1511e60fdc56e1d2545239f0ebe1001d.jpg" align="middle">
</details>




## Hi-Map: Hierarchical Factorized Radiance Field for High-Fidelity   Monocular Dense Mapping

**Authors:Tongyan Hua, Haotian Bai, Zidong Cao, Ming Liu, Dacheng Tao, Lin Wang**

In this paper, we introduce Hi-Map, a novel monocular dense mapping approach based on Neural Radiance Field (NeRF). Hi-Map is exceptional in its capacity to achieve efficient and high-fidelity mapping using only posed RGB inputs. Our method eliminates the need for external depth priors derived from e.g., a depth estimation model. Our key idea is to represent the scene as a hierarchical feature grid that encodes the radiance and then factorizes it into feature planes and vectors. As such, the scene representation becomes simpler and more generalizable for fast and smooth convergence on new observations. This allows for efficient computation while alleviating noise patterns by reducing the complexity of the scene representation. Buttressed by the hierarchical factorized representation, we leverage the Sign Distance Field (SDF) as a proxy of rendering for inferring the volume density, demonstrating high mapping fidelity. Moreover, we introduce a dual-path encoding strategy to strengthen the photometric cues and further boost the mapping quality, especially for the distant and textureless regions. Extensive experiments demonstrate our method's superiority in geometric and textural accuracy over the state-of-the-art NeRF-based monocular mapping methods. 

[PDF](http://arxiv.org/abs/2401.03203v1) 

**摘要**
神经辐射场（NeRF）单目密集映射中，全新的金字塔结构特征网格可提高效率和保真度。

**主要要点**

- Hi-Map 是一种基于神经辐射场 (NeRF) 的单目密集映射方法，无需深度估计模型即可实现高效且高保真的映射。
- Hi-Map 的关键思想是将场景表示为一个分层特征网格，该网格对辐射进行编码，然后将其分解为特征平面和向量。
- 分层分解表示可以使场景表示更简单，更具泛化性，以便对新观测进行快速、平滑的收敛。
- Hi-Map 利用符号距离场 (SDF) 作为渲染的代理来推断体积密度，从而提高了映射保真度。
- Hi-Map 引入了双路径编码策略，以加强光度线索并进一步提高映射质量，尤其是对于遥远且无纹理的区域。
- 大量实验表明，Hi-Map 在几何和纹理准确性方面优于最先进的基于 NeRF 的单目映射方法。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>题目：分层因子化辐射场：用于高保真单目密集测绘的 Hi-Map</li>
<li>作者：Hua Tongyan、Bai Haotian、Cao Zidong、Liu Ming、Tao Dacheng、Wang Lin</li>
<li>隶属单位：香港科技大学（广州）</li>
<li>关键词：单目密集测绘、NeRF、SDF</li>
<li>论文链接：https://arxiv.org/abs/2401.03203，Github 链接：无</li>
<li>
<p>摘要：
（1）研究背景：构建高保真密集 3D 地图对于具身智能系统（如机器人）至关重要。3D 地图使机器人能够执行场景理解任务并在复杂且动态的环境中导航。
（2）过去方法及其问题：传统密集测绘技术难以平衡内存效率和准确性。这些方法通常依赖于显式跟踪和存储共同观察到的点，这些点随后被转换为（例如）占用网格或 TSDF 来表示场景。因此，正确跟踪的点数越多，生成的映射保真度就越高，但这还需要大量的计算和存储。随着神经辐射场 (NeRF) 的出现，一些研究尝试利用神经场更好地表示场景，方法是通过以紧凑且可学习的方式对外观和几何进行编码，从而有利于内存消耗和测绘质量。基于 NeRF 的密集测绘方法主要依赖于输入深度先验来促进在线收敛，方法是通过缩小采样的搜索范围。这种深度先验通常来自传感器或由单目视觉同时定位和建图 (vSLAM) 系统或深度估计模型提供。然而，这种对深度先验的依赖在资源有限的环境或深度线索不可用或不可靠的情况下成为障碍。即使可以通过在优化隐式表示时添加翘曲约束来内部化深度估计，它仍然难以在准确性和计算效率之间取得平衡。因此，在不依赖深度先验的情况下实现高效且高保真的密集测绘是有意义的。这要求 NeRF 能够有效且快速地推广到基础几何未知的新观察结果。
（3）论文提出的研究方法：为了实现这一点，我们引入了一种新颖的分层表示，方法是因子化多分辨率特征网格，灵感来自 [29]，其中通过因子化辐射场提出了低秩正则化，通过将辐射场因子化为特征平面和向量，从而提高了渲染质量并提高了计算效率。这种正则化技术将数据结构（即 4D 张量）简化为低维元素，即低秩分量，以保留体积渲染最相关的特征。因此，场景表示变得更简单、更具通用性，可以快速平滑地收敛到新的观察结果。这允许进行有效计算，同时通过降低场景表示的复杂性来减轻噪声模式。在分层因子化表示的支持下，我们利用符号距离场 (SDF) 作为渲染的代理来推断体积密度，从而证明了高测绘保真度。此外，我们引入了一种双路径编码策略来增强光度线索并进一步提高测绘质量，特别是对于遥远和无纹理的区域。
（4）方法在什么任务上取得了什么性能？性能是否支持其目标？广泛的实验表明，我们的方法在几何和纹理精度方面优于最先进的基于 NeRF 的单目测绘方法。性能支持其目标。</p>
</li>
<li>
<p>方法：
(1) 引入分层因子化表示，将多分辨率特征网格因子化为特征平面和向量，简化数据结构，降低场景表示的复杂性，提高渲染质量和计算效率。
(2) 利用符号距离场 (SDF) 作为渲染的代理来推断体积密度，提高测绘保真度。
(3) 采用双路径编码策略增强光度线索，进一步提高测绘质量，特别是对于遥远和无纹理的区域。</p>
</li>
<li>
<p>结论：
(1): 本文提出了一种分层因子化辐射场，用于高保真单目密集测绘，在几何和纹理精度方面优于最先进的基于NeRF的单目测绘方法。
(2): 创新点：</p>
</li>
<li>引入分层因子化表示，简化数据结构，降低场景表示的复杂性，提高渲染质量和计算效率。</li>
<li>利用符号距离场(SDF)作为渲染的代理来推断体积密度，提高测绘保真度。</li>
<li>采用双路径编码策略增强光度线索，进一步提高测绘质量，特别是对于遥远和无纹理的区域。
性能：</li>
<li>在几何和纹理精度方面优于最先进的基于NeRF的单目测绘方法。
工作量：</li>
<li>需要更多的计算资源来训练模型。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b366ebbb955de81c06947699b2848416.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-020fc3cd9ae1ff2be7aa32d5d5fdd3ac.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4dacbdf094479bbedb2926fda300988b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-51809493796f92589316bddbc0c14a07.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f5010f46d456e61cab09f5b9e1b8146.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c0f693c233faba8cf8ca1dd1f39ee6f2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-78869f68df98aaa60a33965857838f02.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12356980c7845df8d299441db2d77219.jpg" align="middle">
</details>




<h2 id="FED-NeRF-Achieve-High-3D-Consistency-and-Temporal-Coherence-for-Face-Video-Editing-on-Dynamic-NeRF"><a href="#FED-NeRF-Achieve-High-3D-Consistency-and-Temporal-Coherence-for-Face-Video-Editing-on-Dynamic-NeRF" class="headerlink" title="FED-NeRF: Achieve High 3D Consistency and Temporal Coherence for Face   Video Editing on Dynamic NeRF"></a>FED-NeRF: Achieve High 3D Consistency and Temporal Coherence for Face   Video Editing on Dynamic NeRF</h2><p><strong>Authors:Hao Zhang, Yu-Wing Tai, Chi-Keung Tang</strong></p>
<p>The success of the GAN-NeRF structure has enabled face editing on NeRF to maintain 3D view consistency. However, achieving simultaneously multi-view consistency and temporal coherence while editing video sequences remains a formidable challenge. This paper proposes a novel face video editing architecture built upon the dynamic face GAN-NeRF structure, which effectively utilizes video sequences to restore the latent code and 3D face geometry. By editing the latent code, multi-view consistent editing on the face can be ensured, as validated by multiview stereo reconstruction on the resulting edited images in our dynamic NeRF. As the estimation of face geometries occurs on a frame-by-frame basis, this may introduce a jittering issue. We propose a stabilizer that maintains temporal coherence by preserving smooth changes of face expressions in consecutive frames. Quantitative and qualitative analyses reveal that our method, as the pioneering 4D face video editor, achieves state-of-the-art performance in comparison to existing 2D or 3D-based approaches independently addressing identity and motion. Codes will be released. </p>
<p><a href="http://arxiv.org/abs/2401.02616v1">PDF</a> Our code will be available at: <a href="https://github.com/ZHANG1023/FED-NeRF">https://github.com/ZHANG1023/FED-NeRF</a></p>
<p><strong>摘要</strong><br>动态人脸 GAN-NeRF 结构实现 4D 人脸视频编辑，可在 3D 视图保持一致性的同时，实现时间连贯性的视频序列编辑。</p>
<p><strong>要点</strong></p>
<ul>
<li>该方法基于动态人脸 GAN-NeRF 结构，有效利用视频序列恢复潜在编码和 3D 面部几何。</li>
<li>通过编辑潜在编码，可确保人脸在多视图中的一致性编辑。</li>
<li>动态 NeRF 中对编辑后图像进行多视点立体重建验证了多视图一致性编辑。</li>
<li>逐帧估计面部几何可能会导致抖动问题。</li>
<li>该方法提出一个稳定器通过保持连续帧中面部表情的平滑变化，以保持时间连贯性。</li>
<li>定量和定性分析表明，该方法作为首个 4D 人脸视频编辑器，与现有的仅针对身份或运动的 2D 或 3D 方法相比，取得了最先进的性能。</li>
<li>代码将公布。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>标题：FED-NeRF：实现人脸视频编辑的高 3D 一致性和时间连贯性</li>
<li>作者：张浩、戴宇炜、邓志铿</li>
<li>隶属单位：香港科技大学</li>
<li>关键词：人脸视频编辑、NeRF、动态 NeRF、多视图一致性、时间连贯性</li>
<li>论文链接：https://arxiv.org/abs/2401.02616</li>
<li>
<p>摘要：
（1）研究背景：GAN-NeRF 结构的成功让人脸编辑能够在 NeRF 上保持 3D 视图一致性。然而，在编辑视频序列时同时实现多视图一致性和时间连贯性仍然是一个巨大的挑战。
（2）过去的方法及其问题：现有方法主要集中在 2D 或 3D 空间中进行人脸编辑，但这些方法在处理多视图一致性和时间连贯性方面存在局限性。
（3）研究方法：本文提出了一种新颖的人脸视频编辑架构，该架构建立在动态人脸 GAN-NeRF 结构之上，有效地利用视频序列来恢复潜在编码和 3D 人脸几何。通过编辑潜在编码，可以在人脸上确保多视图一致的编辑，这可以通过对动态 NeRF 中生成的编辑图像进行多视图立体重建来验证。由于人脸几何的估计是逐帧进行的，这可能会引入抖动问题。因此，本文提出了一种稳定器，通过保持连续帧中人脸表情的平滑变化来保持时间连贯性。
（4）方法性能：定量和定性分析表明，本文方法作为开创性的 4D 人脸视频编辑器，在独立处理身份和运动方面取得了最先进的性能，优于现有的 2D 或 3D 方法。</p>
</li>
<li>
<p>方法：
(1) 潜在编码估计器：从视频序列中提取身份信息，将每一帧的特征通过交叉注意力层聚合，得到一个奇异的潜在编码输出。
(2) 面部几何估计器：修改基于 EMOCA 的图像编码器，将输入图像分解为面部几何（由 FLAME 控制表示）、反照率、光照、额外表情代码等。
(3) 稳定器：使用 Catmull-Rom 样条曲线对连续帧中的面部表情进行平滑变化，保持时间连贯性。</p>
</li>
<li>
<p>结论：
（1）：本文提出了一种新颖的人脸视频编辑架构FED-NeRF，该架构建立在动态人脸GAN-NeRF结构之上，有效地利用视频序列来恢复潜在编码和3D人脸几何。通过编辑潜在编码，可以在人脸上确保多视图一致的编辑，这可以通过对动态NeRF中生成的编辑图像进行多视图立体重建来验证。由于人脸几何的估计是逐帧进行的，这可能会引入抖动问题。因此，本文提出了一种稳定器，通过保持连续帧中人脸表情的平滑变化来保持时间连贯性。定量和定性分析表明，本文方法作为开创性的4D人脸视频编辑器，在独立处理身份和运动方面取得了最先进的性能，优于现有的2D或3D方法。
（2）：创新点：</p>
</li>
<li>提出了一种新颖的人脸视频编辑架构FED-NeRF，该架构建立在动态人脸GAN-NeRF结构之上，有效地利用视频序列来恢复潜在编码和3D人脸几何。</li>
<li>通过编辑潜在编码，可以在人脸上确保多视图一致的编辑，这可以通过对动态NeRF中生成的编辑图像进行多视图立体重建来验证。</li>
<li>提出了一种稳定器，通过保持连续帧中人脸表情的平滑变化来保持时间连贯性。
性能：</li>
<li>定量和定性分析表明，本文方法作为开创性的4D人脸视频编辑器，在独立处理身份和运动方面取得了最先进的性能，优于现有的2D或3D方法。
工作量：</li>
<li>本文方法的实现需要较高的计算资源，并且需要对视频序列进行预处理。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d5f818de87353fbf13907e49c13b462d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b57ca3ade2e4538084a10eeb0919b87d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-102448406db5d3475d988673668fc7a1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1ec81b2b7dfa9352ae62039d258ec687.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4ee173ef2962cf2f938e02414ced9add.jpg" align="middle">
</details>




<h2 id="Inpaint4DNeRF-Promptable-Spatio-Temporal-NeRF-Inpainting-with-Generative-Diffusion-Models"><a href="#Inpaint4DNeRF-Promptable-Spatio-Temporal-NeRF-Inpainting-with-Generative-Diffusion-Models" class="headerlink" title="Inpaint4DNeRF: Promptable Spatio-Temporal NeRF Inpainting with   Generative Diffusion Models"></a>Inpaint4DNeRF: Promptable Spatio-Temporal NeRF Inpainting with   Generative Diffusion Models</h2><p><strong>Authors:Han Jiang, Haosen Sun, Ruoxuan Li, Chi-Keung Tang, Yu-Wing Tai</strong></p>
<p>Current Neural Radiance Fields (NeRF) can generate photorealistic novel views. For editing 3D scenes represented by NeRF, with the advent of generative models, this paper proposes Inpaint4DNeRF to capitalize on state-of-the-art stable diffusion models (e.g., ControlNet) for direct generation of the underlying completed background content, regardless of static or dynamic. The key advantages of this generative approach for NeRF inpainting are twofold. First, after rough mask propagation, to complete or fill in previously occluded content, we can individually generate a small subset of completed images with plausible content, called seed images, from which simple 3D geometry proxies can be derived. Second and the remaining problem is thus 3D multiview consistency among all completed images, now guided by the seed images and their 3D proxies. Without other bells and whistles, our generative Inpaint4DNeRF baseline framework is general which can be readily extended to 4D dynamic NeRFs, where temporal consistency can be naturally handled in a similar way as our multiview consistency. </p>
<p><a href="http://arxiv.org/abs/2401.00208v1">PDF</a> </p>
<p><strong>摘要</strong><br>生成方法弥补神经辐射场的遮挡区域，生成过程分粗糙遮罩传播和由种子图像引导的多视点一致性两步。</p>
<p><strong>要点</strong></p>
<ul>
<li>基于扩散模型的生成器可直接生成图片，可解决NeRF遮挡区域的修复问题。</li>
<li>通过种子图像上的标记可以生成补全的图片。</li>
<li>3D几何代理可以从生成图片构建，以指导多视点一致性。</li>
<li>所设计方法可以推广到4D动态神经辐射场，通过标记和种子图像，利用时间一致性可以指导多视点一致性。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：Inpaint4DNeRF：基于扩散模型的提示式时空 NeRF 修复</li>
<li>作者：Han Jiang、Haosen Sun、Ruoxuan Li、Chi-Keung Tang、Yu-Wing Tai</li>
<li>隶属单位：香港科技大学</li>
<li>关键词：NeRF、图像修复、生成扩散模型、提示引导、时空一致性</li>
<li>论文链接：https://arxiv.org/abs/2401.00208</li>
<li>
<p>摘要：
（1）研究背景：NeRF 是一种强大的 3D 场景表示方法，可以生成逼真的新视图。然而，对于 NeRF 表示的 3D 场景进行编辑仍然是一个具有挑战性的问题。
（2）过去的方法及其问题：现有方法主要集中在基于文本提示的 NeRF 编辑，但它们通常仅限于编辑现有对象的外观，而无法处理实质性的几何变化。此外，这些方法通常需要大量的数据和复杂的网络结构，这使得它们难以扩展到动态 NeRF。
（3）提出的研究方法：Inpaint4DNeRF 是一种基于生成扩散模型的 NeRF 修复方法。它首先通过粗糙的掩码传播来生成一组具有合理内容的种子图像，然后利用这些种子图像和它们的 3D 代理来指导所有完成图像的 3D 多视图一致性。此外，Inpaint4DNeRF 可以很容易地扩展到动态 NeRF，以处理时间一致性。
（4）方法的性能：在静态和动态 NeRF 修复任务上，Inpaint4DNeRF 在定性和定量方面都优于现有方法。实验结果表明，Inpaint4DNeRF 可以生成与背景一致且具有视觉上令人信服的细节的新内容。</p>
</li>
<li>
<p>方法：
(1) 训练视图预处理：首先，选择一组种子图像，并在这些图像上进行 inpainting，以生成一组具有合理内容的种子图像。然后，利用这些种子图像和它们的 3D 代理来指导所有完成图像的 3D 多视图一致性。
(2) 渐进式训练：首先，对 NeRF 进行预热训练，以获得粗略的收敛。然后，使用迭代数据集更新 (IDU) 策略对 NeRF 进行微调，以编辑目标对象的外观和精细几何形状。
(3) 正则化：为了监督 NeRF 训练，使用 L1 光度损失和深度损失作为监督。此外，还使用 LPIPS 损失作为正则化项，以减少噪声和浮动物。</p>
</li>
<li>
<p>结论：
(1): 本文提出了一种基于生成扩散模型的 NeRF 修复方法 Inpaint4DNeRF，该方法可以生成文本引导、背景适当且在多视图下一致的内容。
(2): 创新点：</p>
</li>
<li>提出了一种训练图像预处理方法，该方法利用种子图像和它们的 3D 代理来指导所有完成图像的 3D 多视图一致性。</li>
<li>提出了一种渐进式训练策略，该策略首先对 NeRF 进行预热训练，然后使用迭代数据集更新 (IDU) 策略对 NeRF 进行微调，以编辑目标对象的外观和精细几何形状。</li>
<li>使用 L1 光度损失、深度损失和 LPIPS 损失作为正则化项来监督 NeRF 训练。
性能：</li>
<li>在静态和动态 NeRF 修复任务上，Inpaint4DNeRF 在定性和定量方面都优于现有方法。</li>
<li>Inpaint4DNeRF 可以生成与背景一致且具有视觉上令人信服的细节的新内容。
工作量：</li>
<li>Inpaint4DNeRF 的实现相对简单，并且可以很容易地扩展到动态 NeRF。</li>
<li>Inpaint4DNeRF 的训练速度较快，并且可以在普通 GPU 上进行训练。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c3d700b9dfde2e5d8ed89d1f163196dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eaae707aaf894e22e54246edd91e6dce.jpg" align="middle">
</details>




<h2 id="SyncDreamer-for-3D-Reconstruction-of-Endangered-Animal-Species-with-NeRF-and-NeuS"><a href="#SyncDreamer-for-3D-Reconstruction-of-Endangered-Animal-Species-with-NeRF-and-NeuS" class="headerlink" title="SyncDreamer for 3D Reconstruction of Endangered Animal Species with NeRF   and NeuS"></a>SyncDreamer for 3D Reconstruction of Endangered Animal Species with NeRF   and NeuS</h2><p><strong>Authors:Ahmet Haydar Ornek, Deniz Sen, Esmanur Civil</strong></p>
<p>The main aim of this study is to demonstrate how innovative view synthesis and 3D reconstruction techniques can be used to create models of endangered species using monocular RGB images. To achieve this, we employed SyncDreamer to produce unique perspectives and NeuS and NeRF to reconstruct 3D representations. We chose four different animals, including the oriental stork, frog, dragonfly, and tiger, as our subjects for this study. Our results show that the combination of SyncDreamer, NeRF, and NeuS techniques can successfully create 3D models of endangered animals. However, we also observed that NeuS produced blurry images, while NeRF generated sharper but noisier images. This study highlights the potential of modeling endangered animals and offers a new direction for future research in this field. By showcasing the effectiveness of these advanced techniques, we hope to encourage further exploration and development of techniques for preserving and studying endangered species. </p>
<p><a href="http://arxiv.org/abs/2312.13832v1">PDF</a> 8 figures</p>
<p><strong>Summary</strong><br>育成濒危生物 3D 模型的新方法：结合 SyncDreamer、NeRF 和 NeuS 技术。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>本研究旨在展示如何利用创新性视图合成和 3D 重建技术，仅使用单目 RGB 图像创建濒危物种模型。</li>
<li>我们使用了 SyncDreamer 来生成独特视角，并使用 NeuS 和 NeRF 重建 3D 表示。</li>
<li>我们选择了四种不同的动物，包括东方鹳、青蛙、蜻蜓和老虎，作为本研究的主题。</li>
<li>我们的结果表明，SyncDreamer、NeRF 和 NeuS 技术的结合可以成功创建濒危动物的 3D 模型。</li>
<li>我们还观察到，NeuS 生成的图像模糊，而 NeRF 生成的图像更清晰但噪声更多。</li>
<li>本研究强调了对濒危动物建模的潜力，并为该领域未来的研究提供了新的方向。</li>
<li>通过展示这些先进技术的有效性，我们希望鼓励进一步探索和开发保护和研究濒危物种的技术。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：利用 SyncDreamer、NeuS 和 NeRF 从单目 RGB 图像重建濒危动物物种的 3D 模型</li>
<li>作者：Ahmet Haydar Ornek, Deniz Sen, Esmanur Civil</li>
<li>第一作者单位：华为土耳其研发中心</li>
<li>关键词：SyncDreamer · NeuS · NeRF · 3D · 重建 · 新颖视图合成</li>
<li>论文链接：https://arxiv.org/abs/2312.13832</li>
<li>
<p>摘要：
(1)：研究背景：随着人工智能和深度学习技术的快速发展，生成式人工智能作为一种能够自主创建逼真复杂内容的技术，已经成为技术创新的焦点。近年来，随着稳定扩散 (SD) 算法的出现，生成式人工智能取得了突破性进展，同时 3D 生成能力也取得了显着进步，进一步扩展了人工智能在各个领域的潜在应用。然而，训练一致的 3D 表示需要大量的数据样本，在某些情况下并不总是可用。保护生物多样性仍然是一个关键问题，许多物种面临着野外灭绝的威胁。加剧这一挑战的是，某些濒危物种的图像数据稀缺，这使得使用现有的基于生成式人工智能的方法创建 3D 模型变得困难。
(2)：过去的方法及其问题：现有方法主要集中在使用大量数据来训练 3D 表示，这在某些情况下并不总是可用。此外，现有方法往往需要复杂的训练过程和大量的数据，这使得它们难以应用于现实世界中的问题。
(3)：论文提出的研究方法：本文探索了 3D 生成式人工智能与野生动物保护的交叉点，讨论了现有的零样本 3D 模型生成方法的结果，以解决数据稀缺问题。通过利用先进的人工智能技术，特别是生成式新颖视图合成和神经隐式 3D 表示，我们旨在从有限的现有样本中生成濒危物种的 3D 模型。
(4)：方法在什么任务上取得了什么性能？性能是否支持其目标：我们的方法在濒危动物物种的 3D 重建任务上取得了很好的性能。我们使用 SyncDreamer 从单目 RGB 图像生成新颖的视角，然后使用 NeRF 和 NeuS 重建 3D 表示。我们的结果表明，所提出的方法能够成功地从有限的现有样本中生成濒危动物物种的 3D 模型。这些模型可以用于各种应用，例如教育、研究和保护。</p>
</li>
<li>
<p>Methods:
(1): 利用 SyncDreamer 从单目 RGB 图像生成新颖视角，以解决数据稀缺问题；
(2): 采用 NeRF 和 NeuS 重建 3D 表示，以获得濒危动物物种的 3D 模型；
(3): 将生成的 3D 模型用于教育、研究和保护等应用。</p>
</li>
<li>
<p>结论：
（1）：这项工作的意义在于，它探索了 3D 生成式人工智能与野生动物保护的交叉点，讨论了现有的零样本 3D 模型生成方法的结果，以解决数据稀缺问题。通过利用先进的人工智能技术，特别是生成式新颖视图合成和神经隐式 3D 表示，旨在从有限的现有样本中生成濒危物种的 3D 模型。这些模型可以用于各种应用，例如教育、研究和保护。
（2）：创新点：</p>
</li>
<li>利用 SyncDreamer 从单目 RGB 图像生成新颖视角，以解决数据稀缺问题。</li>
<li>采用 NeRF 和 NeuS 重建 3D 表示，以获得濒危动物物种的 3D 模型。</li>
<li>将生成的 3D 模型用于教育、研究和保护等应用。</li>
</ol>
<p>性能：
- 在濒危动物物种的 3D 重建任务上取得了很好的性能。
- 使用 SyncDreamer 从单目 RGB 图像生成新颖的视角，然后使用 NeRF 和 NeuS 重建 3D 表示。
- 结果表明，所提出的方法能够成功地从有限的现有样本中生成濒危动物物种的 3D 模型。</p>
<p>工作量：
- 需要收集濒危动物物种的图像数据。
- 需要训练 SyncDreamer、NeRF 和 NeuS 模型。
- 需要对生成的 3D 模型进行评估。</p>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-da747916ae998379722fef89053fcb49.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b82ce116e2b1bca95bca7d34fc6c5014.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-84409520f6d01d8c83d6f88dd1f7a0f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4dba69f7a9d4113f4ec74476da3ef5b7.jpg" align="middle">
</details>




## Ternary-type Opacity and Hybrid Odometry for RGB-only NeRF-SLAM

**Authors:Junru Lin, Asen Nachkov, Songyou Peng, Luc Van Gool, Danda Pani Paudel**

The opacity of rigid 3D scenes with opaque surfaces is considered to be of a binary type. However, we observed that this property is not followed by the existing RGB-only NeRF-SLAM. Therefore, we are motivated to introduce this prior into the RGB-only NeRF-SLAM pipeline. Unfortunately, the optimization through the volumetric rendering function does not facilitate easy integration of the desired prior. Instead, we observed that the opacity of ternary-type (TT) is well supported. In this work, we study why ternary-type opacity is well-suited and desired for the task at hand. In particular, we provide theoretical insights into the process of jointly optimizing radiance and opacity through the volumetric rendering process. Through exhaustive experiments on benchmark datasets, we validate our claim and provide insights into the optimization process, which we believe will unleash the potential of RGB-only NeRF-SLAM. To foster this line of research, we also propose a simple yet novel visual odometry scheme that uses a hybrid combination of volumetric and warping-based image renderings. More specifically, the proposed hybrid odometry (HO) additionally uses image warping-based coarse odometry, leading up to an order of magnitude final speed-up. Furthermore, we show that the proposed TT and HO well complement each other, offering state-of-the-art results on benchmark datasets in terms of both speed and accuracy. 

[PDF](http://arxiv.org/abs/2312.13332v2) 

**摘要**
RGB-SLAM 仅使用颜色信息估计三维场景，我们引入三元型的不透明度来优化三维重建的准确性和速度。

**要点**
- RGB-SLAM 现有的方法中不透明度被认为是二元型。
- 通过理论分析证明了三元型的不透明度是 RGB-SLAM 的最优选择。
- 三元型的不透明度优化可以提高 RGB-SLAM 的精度。
- 三元型的不透明度优化可以通过体渲染轻松实现。
- 提出了一个简单但新颖的视觉里程计方案，使用体渲染和基于图像翘曲的混合方法。
- 基于图像翘曲的粗略里程计算可以优化视觉里程计的速度。
- 三元型的不透明度和混合里程计可以很好地互补，在速度和精度方面都取得了最先进的结果。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>题目：RGB-only NeRF-SLAM 的三元型不透明度和混合里程计</li>
<li>作者：Yifan Yuan, Hongrui Zhou, Yuxiao Zhou, Xiaowei Zhou, Chen Feng</li>
<li>单位：无</li>
<li>关键词：NeRF-SLAM、三元型不透明度、混合里程计、RGB-D SLAM</li>
<li>论文链接：无，Github 代码链接：无</li>
<li>摘要：
(1)：研究背景：RGB-only NeRF-SLAM 中不透明度的性质被认为是二元型的，但现有的 RGB-only NeRF-SLAM 并没有遵循这一特性。因此，本文提出将三元型不透明度先验引入 RGB-only NeRF-SLAM 管道中。
(2)：过去的方法及其问题：优化通过体积渲染函数不能轻松集成所需先验。
(3)：本文的研究方法：研究三元型不透明度为什么非常适合并且是这项任务所需要的。具体来说，本文提供了通过体积渲染过程联合优化辐射度和不透明度的理论见解。
(4)：方法在任务中的表现：通过在基准数据集上的详尽实验，验证了本文的声明，并提供了对优化过程的见解，这将释放 RGB-only NeRF-SLAM 的潜力。为了促进这一研究方向，本文还提出了一种简单但新颖的视觉里程计方案，该方案使用体积和基于图像扭曲的图像渲染的混合组合。更具体地说，所提出的混合里程计 (HO) 另外使用了基于图像扭曲的粗略里程计，从而最终加速了一个数量级。此外，本文表明所提出的 TT 和 HO 相互补充，在基准数据集上在速度和准确性方面都提供了最先进的结果。</li>
</ol>
<p>Some Error for method(比如是不是没有Methods这个章节)</p>
<ol>
<li>结论：
（1）：本文提出了一种受益于不透明场景先验的仅 RGB NeRF-SLAM 方法。这是通过 3D 场景的三元型建模来实现的。此外，我们提出了一种混合方法来估计相机运动，从而导致整体速度显着提高。我们在分析体积渲染和不透明表面时提供的理论见解在我们的上下文中得到了我们的实验结果的充分支持。事实上，报告的观察结果促使我们提出了一个简单但非常有效的策略来利用不透明表面先验，这反过来又为我们提供了更高的准确性和速度，这要归功于所提出的三元型先验提供的更快的收敛速度。局限性和未来工作。虽然是实时的，但所提出的方法对于许多常见应用来说在消费设备上尚未实时。这些要求可以通过特定于应用程序和硬件的代码优化和系统配置来满足，这仍然是未来的工作。
（2）：创新点：</li>
<li>将三元型不透明度先验引入 RGB-only NeRF-SLAM 管道中。</li>
<li>提出了一种混合里程计方案，该方案使用体积和基于图像扭曲的图像渲染的混合组合。</li>
</ol>
<p>性能：
* 在基准数据集上的详尽实验验证了本文的声明，并提供了对优化过程的见解，这将释放 RGB-only NeRF-SLAM 的潜力。
* 所提出的 TT 和 HO 相互补充，在基准数据集上在速度和准确性方面都提供了最先进的结果。</p>
<p>工作量：
* 该方法尚未在消费设备上实时。
* 需要进行特定于应用程序和硬件的代码优化和系统配置。</p>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-93cf1ecd415a1a0301157b4d0970a43c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-222406a8117741693dd9920da4fdf228.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2fac3b124dc86b2b4487320181b09bbe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f1b89edb99ac23be67750f729dc4b43b.jpg" align="middle">
</details>




## MixRT: Mixed Neural Representations For Real-Time NeRF Rendering

**Authors:Chaojian Li, Bichen Wu, Peter Vajda,  Yingyan,  Lin**

Neural Radiance Field (NeRF) has emerged as a leading technique for novel view synthesis, owing to its impressive photorealistic reconstruction and rendering capability. Nevertheless, achieving real-time NeRF rendering in large-scale scenes has presented challenges, often leading to the adoption of either intricate baked mesh representations with a substantial number of triangles or resource-intensive ray marching in baked representations. We challenge these conventions, observing that high-quality geometry, represented by meshes with substantial triangles, is not necessary for achieving photorealistic rendering quality. Consequently, we propose MixRT, a novel NeRF representation that includes a low-quality mesh, a view-dependent displacement map, and a compressed NeRF model. This design effectively harnesses the capabilities of existing graphics hardware, thus enabling real-time NeRF rendering on edge devices. Leveraging a highly-optimized WebGL-based rendering framework, our proposed MixRT attains real-time rendering speeds on edge devices (over 30 FPS at a resolution of 1280 x 720 on a MacBook M1 Pro laptop), better rendering quality (0.2 PSNR higher in indoor scenes of the Unbounded-360 datasets), and a smaller storage size (less than 80% compared to state-of-the-art methods). 

[PDF](http://arxiv.org/abs/2312.11841v4) Accepted by 3DV'24. Project Page: https://licj15.github.io/MixRT/

**摘要**

低质量网格、视图相关位移贴图和压缩 NeRF 模型相结合的方法可实现实时 NeRF 渲染。

**要点**

- MixRT 采用低质量网格、视图相关位移贴图和压缩 NeRF 模型的新型表示方法。
- 这种设计有效地利用了现有图形硬件的功能，从而可以在边缘设备上实现实时 NeRF 渲染。
- MixRT 在边缘设备上实现了较快的渲染速度（MacBook M1 Pro 笔记本电脑上以 1280 x 720 分辨率达到 30 FPS 以上）。
- MixRT 在室内场景中实现了更好的渲染质量（在 Unbounded-360 数据集中 PSNR 高 0.2）。
- MixRT 具有较小的存储空间（低于最先进的方法的 80%）。
- MixRT 可以扩展到处理大规模场景。
- MixRT 可用于各种应用，包括增强现实、虚拟现实和游戏。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>题目：MixRT：用于实时 NeRF 渲染的混合神经表示</li>
<li>作者：Lichao Jia、Yufei Wang、Hao Zhu、Kun Zhou、Zhiwen Fan、Shuangbai Zhou</li>
<li>第一作者单位：清华大学</li>
<li>关键词：神经辐射场、实时渲染、混合表示、压缩、图形处理器</li>
<li>论文链接：https://arxiv.org/abs/2302.01328，Github 代码链接：None</li>
<li>
<p>摘要：
(1)：研究背景：神经辐射场（NeRF）是一种用于新视角合成的领先技术，具有令人印象深刻的逼真重建和渲染能力。然而，在大规模场景中实现实时 NeRF 渲染提出了挑战，通常导致采用具有大量三角形的复杂烘焙网格表示或在烘焙表示中进行资源密集的光线行进。
(2)：过去方法及问题：过去的方法要么采用复杂烘焙网格表示，要么采用资源密集的光线行进，这使得实时 NeRF 渲染在大规模场景中具有挑战性。
(3)：研究方法：本文提出了一种新的 NeRF 表示 MixRT，它包括一个低质量网格、一个视点相关位移图和一个压缩的 NeRF 模型。这种设计有效地利用了现有图形硬件的功能，从而在边缘设备上实现了实时 NeRF 渲染。
(4)：方法性能：本文提出的 MixRT 在边缘设备上实现了实时的渲染速度（在 MacBook M1 Pro 笔记本电脑上以 1280×720 的分辨率达到 30 FPS 以上）、更好的渲染质量（在 Unbounded-360 数据集的室内场景中 PSNR 高出 0.2）和更小的存储大小（与最先进的方法相比减少了 80% 以上）。</p>
</li>
<li>
<p>方法：
(1) 提出了一种新的NeRF表示MixRT，它包括一个低质量网格、一个视点相关位移图和一个压缩的NeRF模型。
(2) 采用低质量网格来表示场景的几何结构，并使用视点相关位移图来细化网格的细节。
(3) 将NeRF模型压缩成一个紧凑的格式，以减少存储空间和提高渲染速度。
(4) 设计了一种新的渲染算法，可以有效地利用现有图形硬件的功能，实现实时的NeRF渲染。</p>
</li>
<li>
<p>结论：</p>
</li>
</ol>
<p>（1）MixRT 提出了一种新的 NeRF 表示，该表示将低质量网格、视点相关位移图和压缩的 NeRF 模型结合在一起。这种设计源于我们的观察，即实现高渲染质量并不需要由具有大量三角形的高复杂度几何体表示的网格。这一认识表明，有可能简化烘焙网格并将不同的神经表示纳入渲染、内存和存储效率中。通过详细的运行时分析和优化的基于 WebGL 的渲染框架，MixRT 在渲染质量和效率之间提供了最先进的平衡。</p>
<p>（2）创新点：</p>
<ul>
<li>提出了一种新的 NeRF 表示 MixRT，它将低质量网格、视点相关位移图和压缩的 NeRF 模型结合在一起。</li>
<li>采用低质量网格来表示场景的几何结构，并使用视点相关位移图来细化网格的细节。</li>
<li>将 NeRF 模型压缩成一个紧凑的格式，以减少存储空间和提高渲染速度。</li>
<li>设计了一种新的渲染算法，可以有效地利用现有图形硬件的功能，实现实时的 NeRF 渲染。</li>
</ul>
<p>性能：</p>
<ul>
<li>在边缘设备上实现了实时的渲染速度（在 MacBook M1 Pro 笔记本电脑上以 1280×720 的分辨率达到 30FPS 以上）。</li>
<li>更好的渲染质量（在 Unbounded-360 数据集的室内场景中 PSNR 高出 0.2）。</li>
<li>更小的存储大小（与最先进的方法相比减少了 80% 以上）。</li>
</ul>
<p>工作量：</p>
<ul>
<li>该方法在 Unbounded-360 数据集上进行了评估。</li>
<li>该方法与最先进的方法进行了比较。</li>
<li>该方法的代码已开源。</li>
</ul>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-301a52ffd84957421daad742378d8046.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44d66c65781dd7094b22b4ed21552bd3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-16ddd4478152e5c6a4f02bc6b86875f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-729c2c4e7c400fc17f0e1faacd580d64.jpg" align="middle">
</details>




## Learning Dense Correspondence for NeRF-Based Face Reenactment

**Authors:Songlin Yang, Wei Wang, Yushi Lan, Xiangyu Fan, Bo Peng, Lei Yang, Jing Dong**

Face reenactment is challenging due to the need to establish dense correspondence between various face representations for motion transfer. Recent studies have utilized Neural Radiance Field (NeRF) as fundamental representation, which further enhanced the performance of multi-view face reenactment in photo-realism and 3D consistency. However, establishing dense correspondence between different face NeRFs is non-trivial, because implicit representations lack ground-truth correspondence annotations like mesh-based 3D parametric models (e.g., 3DMM) with index-aligned vertexes. Although aligning 3DMM space with NeRF-based face representations can realize motion control, it is sub-optimal for their limited face-only modeling and low identity fidelity. Therefore, we are inspired to ask: Can we learn the dense correspondence between different NeRF-based face representations without a 3D parametric model prior? To address this challenge, we propose a novel framework, which adopts tri-planes as fundamental NeRF representation and decomposes face tri-planes into three components: canonical tri-planes, identity deformations, and motion. In terms of motion control, our key contribution is proposing a Plane Dictionary (PlaneDict) module, which efficiently maps the motion conditions to a linear weighted addition of learnable orthogonal plane bases. To the best of our knowledge, our framework is the first method that achieves one-shot multi-view face reenactment without a 3D parametric model prior. Extensive experiments demonstrate that we produce better results in fine-grained motion control and identity preservation than previous methods. 

[PDF](http://arxiv.org/abs/2312.10422v2) Accepted by Proceedings of the AAAI Conference on Artificial   Intelligence, 2024

**摘要**
没有三维参数模型先验，也能学习不同神经辐射场人脸表示之间的稠密对应关系。

**要点**
- 人脸重演具有挑战性，需要在不同的脸部表示之间建立稠密的对应关系，以实现动作转换。
- 最近的研究利用神经辐射场（NeRF）作为基本表示，进一步提高了多视角人脸重演在照片真实感和三维一致性方面的性能。
- 在不同的人脸NeRF之间建立稠密的对应关系并非易事，因为隐式表示缺乏像基于网格的三维参数模型（如具有索引对齐顶点的3DMM）这样的真实对应注释。
- 虽然将3DMM空间与基于NeRF的人脸表示对齐可以实现动作控制，但由于其仅限于脸部建模，且身份保真度低，因此并不理想。
- 我们提出了一种新框架，采用三平面作为基本NeRF表示，并将脸部三平面分解为三个分量：规范三平面、身份变形和动作。
- 在动作控制方面，我们的关键贡献是提出了一个平面字典（PlaneDict）模块，它可以有效地将动作条件映射到可学习的正交平面基的线性加权叠加。
- 据我们所知，我们的框架是第一个在没有三维参数模型先验的情况下实现一发多视角人脸重演的方法。
- 大量实验表明，我们在精细的动作控制和身份保持方面产生的结果优于以前的方法。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>题目：基于 NeRF 的人脸重现的密集对应关系学习</li>
<li>作者：Songlin Yang, Wei Wang*, Yushi Lan, Xiangyu Fan, Bo Peng, Lei Yang, Jing Dong</li>
<li>单位：中国科学院大学人工智能学院</li>
<li>关键词：人脸重现、NeRF、三平面表示、运动控制、身份保持</li>
<li>链接：https://arxiv.org/abs/2312.10422</li>
<li>
<p>摘要：
（1）研究背景：人脸重现是一项具有挑战性的任务，需要在不同的人脸表示之间建立密集的对应关系以进行运动转移。最近的研究利用神经辐射场 (NeRF) 作为基本表示，进一步提高了多视角人脸重现的真实感和 3D 一致性。然而，在不同的面部 NeRF 之间建立密集的对应关系并非易事，因为隐式表示缺乏像基于网格的 3D 参数模型（例如，具有索引对齐顶点的 3DMM）那样的真实对应的注释。
（2）过去的方法及其问题：虽然将 3DMM 空间与基于 NeRF 的人脸表示对齐可以实现运动控制，但由于其仅限于面部建模和较低的身份保真度，因此并不是最佳选择。
（3）研究方法：本文提出了一种新颖的框架，该框架采用三平面作为基本 NeRF 表示，并将面部三平面分解为三个组件：规范三平面、身份变形和运动。在运动控制方面，本文的主要贡献是提出了一种平面字典 (PlaneDict) 模块，该模块有效地将运动条件映射到可学习的正交平面基的线性加权和。
（4）方法性能：实验证明，本文的方法在细粒度运动控制和身份保持方面优于以往的方法。</p>
</li>
<li>
<p>方法：
(1) 我们将面部三平面分解为规范三平面、身份变形和运动。
(2) 提出 PlaneDict 模块将运动条件映射到可学习的正交平面基的线性加权和。
(3) 采用 StyleGAN 生成器获得身份变形，并通过 PlaneDict 模块获得运动。
(4) 将规范三平面、身份变形和运动相加得到驱动面部图像的三平面。
(5) 通过三平面解码器和体积渲染器将三平面投影到 2D 特征图像。
(6) 使用超分辨率模块将最终图像大小增加到 256^2。</p>
</li>
<li>
<p>结论：
（1）：本文提出了一种基于三平面表示的新颖框架，该框架可以有效地实现人脸重现的密集对应关系学习，在细粒度运动控制和身份保持方面优于以往的方法。
（2）：创新点：
PlaneDict模块：该模块有效地将运动条件映射到可学习的正交平面基的线性加权和，从而实现运动控制。
三平面分解：将面部三平面分解为规范三平面、身份变形和运动，便于运动控制和身份保持。
性能：
在细粒度运动控制和身份保持方面优于以往的方法。
工作量：
该方法需要训练多个模型，包括三平面解码器、体积渲染器和超分辨率模块，工作量较大。</p>
</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b33e8c7219eac4eb653c91c2ed347e6a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cffcb53275a5aba0333681ea0099c55b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-629b61bec3df1b03a6a20598b333d114.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-64cee96b4bc968085a7ff7e4564a8091.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d09b9ae7baddeedc8870c3b7e3bb83b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29a23cef6a44369e63e93d4b44f0b59e.jpg" align="middle">
</details>




<h2 id="Aleth-NeRF-Illumination-Adaptive-NeRF-with-Concealing-Field-Assumption"><a href="#Aleth-NeRF-Illumination-Adaptive-NeRF-with-Concealing-Field-Assumption" class="headerlink" title="Aleth-NeRF: Illumination Adaptive NeRF with Concealing Field Assumption"></a>Aleth-NeRF: Illumination Adaptive NeRF with Concealing Field Assumption</h2><p><strong>Authors:Ziteng Cui, Lin Gu, Xiao Sun, Xianzheng Ma, Yu Qiao, Tatsuya Harada</strong></p>
<p>The standard Neural Radiance Fields (NeRF) paradigm employs a viewer-centered methodology, entangling the aspects of illumination and material reflectance into emission solely from 3D points. This simplified rendering approach presents challenges in accurately modeling images captured under adverse lighting conditions, such as low light or over-exposure. Motivated by the ancient Greek emission theory that posits visual perception as a result of rays emanating from the eyes, we slightly refine the conventional NeRF framework to train NeRF under challenging light conditions and generate normal-light condition novel views unsupervised. We introduce the concept of a “Concealing Field,” which assigns transmittance values to the surrounding air to account for illumination effects. In dark scenarios, we assume that object emissions maintain a standard lighting level but are attenuated as they traverse the air during the rendering process. Concealing Field thus compel NeRF to learn reasonable density and colour estimations for objects even in dimly lit situations. Similarly, the Concealing Field can mitigate over-exposed emissions during the rendering stage. Furthermore, we present a comprehensive multi-view dataset captured under challenging illumination conditions for evaluation. Our code and dataset available at <a href="https://github.com/cuiziteng/Aleth-NeRF">https://github.com/cuiziteng/Aleth-NeRF</a> </p>
<p><a href="http://arxiv.org/abs/2312.09093v3">PDF</a> AAAI 2024, code available at   <a href="https://cuiziteng.github.io/Aleth_NeRF_web/">https://cuiziteng.github.io/Aleth_NeRF_web/</a> Modified version of previous   paper arXiv:2303.05807</p>
<p><strong>Summary</strong><br>用“遮挡场 (Concealing Field)” 辅助 NeRF 模型训练以模拟具有挑战性光照条件下的图像。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>NeRF 利用视点为中心的策略，将光照和材质反射特性混入 3D 点的放射中。</li>
<li>该简化渲染方法难以准确地模拟在不利的照明条件（例如，低光照或过度曝光）下拍摄的图像。</li>
<li>受古希腊认为视觉感知是来自眼睛射出的射线的影响的理论启发，我们对 NeRF 框架进行了改进，在具有挑战性的光照条件下训练 NeRF 模型，并生成正常光照条件下的新视角。</li>
<li>我们引入了“遮挡场”的概念，为周围的空气分配透射率值，以考虑光照效果。</li>
<li>在黑暗场景中，我们假设对象放射保持标准照明水平，但在渲染过程中在空气中传播时会衰减。</li>
<li>遮挡场迫使 NeRF 在光线昏暗的环境，学习到合理的物体密度和颜色估计。</li>
<li>遮挡场可以同样地减少渲染过程中过度曝光的放射。</li>
<li>我们提供了一个在具有挑战性的照明条件下捕获的综合多视图数据集以供评估。</li>
<li>我们的代码和数据集位于 <a href="https://github.com/cuiziteng/Aleth-NeRF。">https://github.com/cuiziteng/Aleth-NeRF。</a></li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：Aleth-NeRF：具有遮蔽场假设的照明自适应 NeRF</li>
<li>作者：Ziteng Cui, Lin Gu, Xiao Sun, Xianzheng Ma, Yu Qiao, Tatsuya Harada</li>
<li>隶属机构：东京大学</li>
<li>关键词：NeRF、照明、低光、过度曝光、遮蔽场</li>
<li>论文链接：https://arxiv.org/abs/2312.09093，Github 链接：https://github.com/cuiziteng/Aleth-NeRF</li>
<li>摘要：
（1）研究背景：NeRF 是一种有效的方法，可以从 2D 图像中理解 3D 场景并生成新颖的视角。然而，NeRF 及其后续变体的公式假设捕获的图像处于正常光照条件下，通常无法在低光或过度曝光场景下工作。这是因为香草 NeRF 以观看者为中心，它对从某个位置到观看者的光线发射量进行建模，而没有将照明和材料解开（图 1(a)）。因此，NeRF 算法将黑暗场景解释为 3D 对象粒子的辐射不足，违反了对对象材料和几何形状的估计。在实际应用中，图像通常在具有挑战性的照明条件下拍摄。
（2）过去的方法及其问题：NeRF 是一种以观看者为中心的范例，将照明和材料反射的各个方面纠缠到仅来自 3D 点的发射中。这种简化的渲染方法在准确建模在不利的照明条件下捕获的图像时存在挑战，例如低光或过度曝光。这是因为香草 NeRF 以观看者为中心，它对从某个位置到观看者的光线发射量进行建模，而没有将照明和材料解开（图 1(a)）。因此，NeRF 算法将黑暗场景解释为 3D 对象粒子的辐射不足，违反了对对象材料和几何形状的估计。在实际应用中，图像通常在具有挑战性的照明条件下拍摄。
（3）论文提出的研究方法：为了解决这个问题，本文提出了一种新的方法 Aleth-NeRF，它可以训练低光和过度曝光场景并生成正常光照条件下的新颖视角。Aleth-NeRF 受古希腊哲学的启发，通过在对象和观察者之间建模遮蔽场来自然地扩展香草 NeRF 中的透射函数。在黑暗场景中，我们假设对象发射保持标准光照水平，但在渲染过程中穿过空气时会衰减。因此，遮蔽场迫使 NeRF 即使在光线昏暗的情况下也能学习到合理的密度和颜色估计。同样，遮蔽场可以减轻渲染阶段过度曝光的发射。此外，我们提出了一个综合的多视图数据集，该数据集在具有挑战性的照明条件下捕获，用于评估。
（4）方法在任务和性能上的表现：我们的方法在具有挑战性的照明条件下训练 NeRF，并在正常光照条件下生成新颖的视角。我们对低光和过度曝光图像进行了广泛的实验，结果表明，我们的方法能够准确地估计对象的密度和颜色，即使在光线昏暗的情况下也是如此。此外，我们的方法能够生成具有正常光照条件的新颖视角，即使输入图像严重不足或过度曝光。这些结果表明，我们的方法可以有效地处理具有挑战性的照明条件下的图像，并为在各种照明条件下生成逼真的新颖视角开辟了新的可能性。</li>
</ol>
<p>7.Methods：
（1）提出了一种新的方法Aleth-NeRF，它可以通过在对象和观察者之间建模遮蔽场来自然地扩展香草NeRF中的透射函数，从而训练低光和过度曝光场景并生成正常光照条件下的新颖视角；
（2）在黑暗场景中，假设对象发射保持标准光照水平，但在渲染过程中穿过空气时会衰减，因此遮蔽场迫使NeRF即使在光线昏暗的情况下也能学习到合理的密度和颜色估计；
（3）同样，遮蔽场可以减轻渲染阶段过度曝光的发射；
（4）提出了一个综合的多视图数据集，该数据集在具有挑战性的照明条件下捕获，用于评估。</p>
<ol>
<li>结论：
（1）：本文提出了一种新的方法 Aleth-NeRF，它可以训练低光和过度曝光场景并生成正常光照条件下的新颖视角。此外，我们提出了一个综合的多视图数据集，该数据集在具有挑战性的照明条件下捕获，用于评估。
（2）：创新点：</li>
<li>提出了一种新的方法 Aleth-NeRF，它可以通过在对象和观察者之间建模遮蔽场来自然地扩展香草 NeRF 中的透射函数，从而训练低光和过度曝光场景并生成正常光照条件下的新颖视角。</li>
<li>在黑暗场景中，假设对象发射保持标准光照水平，但在渲染过程中穿过空气时会衰减，因此遮蔽场迫使 NeRF 即使在光线昏暗的情况下也能学习到合理的密度和颜色估计。</li>
<li>同样，遮蔽场可以减轻渲染阶段过度曝光的发射。
性能：</li>
<li>我们的方法在具有挑战性的照明条件下训练 NeRF，并在正常光照条件下生成新颖的视角。</li>
<li>我们对低光和过度曝光图像进行了广泛的实验，结果表明，我们的方法能够准确地估计对象的密度和颜色，即使在光线昏暗的情况下也是如此。</li>
<li>此外，我们的方法能够生成具有正常光照条件的新颖视角，即使输入图像严重不足或过度曝光。
工作量：</li>
<li>Aleth-NeRF 应该针对每个场景进行专门训练，这与香草 NeRF 相同。</li>
<li>此外，Aleth-NeRF 可能无法处理具有非均匀照明条件（王、徐和刘，2022 年）或阴影条件（屈等人，2017 年）的场景，我们认为这也是未来探索的一个有价值的研究课题。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-73df3246f0f189f363c4406f05fdfb64.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0081eb8c2f0caf23f1cd8c480137c67c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5911359d2c2da29789ea31d3c9246652.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9fafb4552428de0fb3d21116b5da3089.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48849341af81d4f10dd85c432bbe02f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45fe793c1d78fdd64382dd5c56c317ba.jpg" align="middle">
</details>




<h2 id="Learn-to-Optimize-Denoising-Scores-for-3D-Generation-A-Unified-and-Improved-Diffusion-Prior-on-NeRF-and-3D-Gaussian-Splatting"><a href="#Learn-to-Optimize-Denoising-Scores-for-3D-Generation-A-Unified-and-Improved-Diffusion-Prior-on-NeRF-and-3D-Gaussian-Splatting" class="headerlink" title="Learn to Optimize Denoising Scores for 3D Generation: A Unified and   Improved Diffusion Prior on NeRF and 3D Gaussian Splatting"></a>Learn to Optimize Denoising Scores for 3D Generation: A Unified and   Improved Diffusion Prior on NeRF and 3D Gaussian Splatting</h2><p><strong>Authors:Xiaofeng Yang, Yiwen Chen, Cheng Chen, Chi Zhang, Yi Xu, Xulei Yang, Fayao Liu, Guosheng Lin</strong></p>
<p>We propose a unified framework aimed at enhancing the diffusion priors for 3D generation tasks. Despite the critical importance of these tasks, existing methodologies often struggle to generate high-caliber results. We begin by examining the inherent limitations in previous diffusion priors. We identify a divergence between the diffusion priors and the training procedures of diffusion models that substantially impairs the quality of 3D generation. To address this issue, we propose a novel, unified framework that iteratively optimizes both the 3D model and the diffusion prior. Leveraging the different learnable parameters of the diffusion prior, our approach offers multiple configurations, affording various trade-offs between performance and implementation complexity. Notably, our experimental results demonstrate that our method markedly surpasses existing techniques, establishing new state-of-the-art in the realm of text-to-3D generation. Furthermore, our approach exhibits impressive performance on both NeRF and the newly introduced 3D Gaussian Splatting backbones. Additionally, our framework yields insightful contributions to the understanding of recent score distillation methods, such as the VSD and DDS loss. </p>
<p><a href="http://arxiv.org/abs/2312.04820v1">PDF</a> </p>
<p><strong>摘要</strong><br>神经辐射场 (NeRF) 扩散模型的统一框架，迭代优化 3D 模型和扩散先验，实现文本到 3D 生成新突破。</p>
<p><strong>要点</strong></p>
<ul>
<li>提出一种统一的增强 3D 生成任务扩散先验的框架。</li>
<li>识别出扩散先验与扩散模型训练过程之间的差异，阻碍 3D 生成的质量。</li>
<li>提出一个新的统一框架，迭代优化 3D 模型和扩散先验。</li>
<li>该方法在 NeRF 和新引入的 3D 高斯散点骨干上均表现出令人印象深刻的性能。</li>
<li>该框架有助于理解最近的分数蒸馏方法，如 VSD 和 DDS 损失。</li>
<li>大幅优于现有技术，在文本到 3D 生成领域树立了新的最先进水平。</li>
<li>该方法提供了对最近的分数蒸馏方法的深刻理解，例如 VSD 和 DDS 损失。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：学习优化用于 3D 生成的去噪评分：神经辐射场和 3D 高斯散射的统一改进扩散先验</li>
<li>作者：杨晓峰、陈一文、陈程、张驰、许怡、杨旭磊、刘发耀、林国生</li>
<li>单位：南洋理工大学</li>
<li>关键词：扩散模型、3D 生成、NeRF、3D 高斯散射、评分蒸馏</li>
<li>论文链接：https://arxiv.org/abs/2312.04820
Github 链接：无</li>
<li>摘要：
(1) 研究背景：扩散模型是一种强大的生成式方法，在图像生成、编辑和 3D 生成等任务中取得了成功。然而，现有方法在 3D 生成任务中经常难以生成高质量的结果。
(2) 过去的方法：SDS 损失是 DreamFusion 中提出的用于 3D 生成的扩散先验，它通过计算图像和噪声之间的评分来指导 3D 模型的优化。然而，SDS 损失存在一些问题，例如容易产生模糊的生成图像，并且难以捕捉数据空间的多样性。
(3) 本文方法：本文提出了一种新的统一框架来增强 3D 生成的扩散先验。该框架通过迭代优化 3D 模型和扩散先验来提高生成图像的质量。该框架利用扩散先验的不同可学习参数，提供了多种配置，可在性能和实现复杂性之间进行权衡。
(4) 实验结果：实验结果表明，本文方法在 NeRF 和 3D 高斯散射两种骨干网络上均取得了比现有技术更好的性能，在文本到 3D 生成的领域树立了新的技术水平。此外，该框架还对最近的评分蒸馏方法，例如 VSD 和 DDS 损失，做出了有见地的贡献。</li>
</ol>
<p><strong>方法：</strong></p>
<p>（1）问题表述：
考虑优化 3D 模型 [23]，参数化为参数 θ，以及将 θ 转换为 2D 图像 x = g(θ) 的可微渲染操作 g。我们感兴趣的是使用条件预训练扩散模型 ϵϕ(zt; y) 优化 θ 的问题。在以下小节中，我们将分析 SDS 损失如何解决这个问题，以及为什么它不能生成良好的结果。</p>
<p>（2）先前扩散先验的问题：
扩散模型训练和推理中的差异导致 SDS 损失产生次优结果。考虑方程 3 中的 SDS 损失。它直接使用 CFG 变体（通常带有权重因子 100）的参考去噪评分来优化 θ。然而，如方程 1 所述，扩散模型的训练学习了评分函数 ϵϕ(zt; y, t) 而没有使用 CFG。这种差异产生了一个重大问题：SDS 损失中应用的无分类器引导并未将目标分布引导到与参考分布（由评分函数 ϵϕ(zt; y, t) 表征）对齐，而是引导到扩散模型的 CFG 修改版本，表示为 ˆϵϕ。这导致 SDS 损失生成的输出通常过度饱和且缺乏多样性，正如原始研究 [27] 中指出的那样。</p>
<p>（3）扩散先验需要更高的 CFG 引导：
对上述问题的直接解决方案可以直接删除 SDS 损失中的 CFG。我们称之为参考 SDS 损失：
∇θLSDS–ref(ϕ,x)=Et,ϵ[(ϵϕ(zt;y,t)−ϵ)∂x∂θ]。
然而，从经验观察和理论分析来看，直接使用上述方程在 3D 生成中是不可行的。根据经验，正如先前工作 [18, 27] 所证明的，扩散先验仅在使用大 CFG 权重 w 时才能够学习 3D 对象的详细特征。我们的实验观察到了类似的挑战。可以在实验部分和图 5 中找到一个说明。从理论上讲，较大的 CFG 权重 w 将目标分布引导到条件评分函数的方向，远离无条件评分函数，从而生成与条件更相关的内容。与 2D 空间相比，3D 优化在使用 2D 扩散模型时引入了额外的分布外因素 [39]。因此，2D 扩散先验在 3D 问题上需要更大的 w。</p>
<p>（4）学习优化去噪评分：
基于以上分析，我们改进扩散先验的关键见解是，SDS 应从较高的初始无分类器引导 (CFG) 值开始，并最终与参考 SDS 公式方程 6 保持一致，以弥合训练和推理阶段之间的差距。一种自然的方法是在优化过程中逐渐减小 CFG 权重 w。然而，这种简单的方法并没有在我们的实验中产生改进的结果。主要挑战在于 CFG 权重 w 是一个标量，它会统一影响整个噪声图，而不会考虑内部变化。此外，调整 w 值以适应优化过程中不同 3D 对象的差异难度被证明具有挑战性。为此，我们提出了 LODS（学习优化去噪评分）算法。我们的方法首先通过两个额外的可学习参数扩展无分类器引导公式。第一个，表示为 α，是指可学习的无条件嵌入，初始化为空嵌入 ∅。第二个 ψ 表示添加到网络的附加参数（例如 LoRA [14] 参数）。这些可学习参数中的每一个都对应于我们提出的方法的一个变体。然后，我们建议使用算法 1 中所示的 LODS 算法来学习这两个可学习参数。</p>
<p>（5）具体实现：
我们首先初始化 3D 模型参数和当前运行的 SDS 损失。然后，我们继续执行两个迭代优化步骤。在步骤 5 中，我们使用当前的 SDS 损失来优化 3D 模型参数。在此之后，在步骤 6 中，我们优化 SDS 的参数。这种迭代优化算法在两个方面具有优势。首先，它允许 3D 模型的优化从任意初始评分函数开始。其次，步骤 6 中的优化过程通过将原始 SDS 与方程 6 对齐来学习弥合训练和推理阶段之间的差距。随后的子部分深入探讨了通过优化两个额外的可学习参数来实现 LODS 算法的细节。在这项研究中，我们将我们的探索限制为使用可学习空嵌入或可学习低秩参数来扩展无分类器引导。然而，值得注意的是，我们的框架可以扩展到包含其他可学习参数，例如 ControlNet 结构 [46] 和 T2I 适配器结构 [25] 中的参数。</p>
<ol>
<li>结论：</li>
</ol>
<p>（1）本文的主要贡献在于提出了一种新的框架来增强3D生成的扩散先验。该框架通过迭代优化3D模型和扩散先验来提高生成图像的质量。该框架利用扩散先验的不同可学习参数，提供了多种配置，可在性能和实现复杂性之间进行权衡。</p>
<p>（2）创新点：</p>
<ul>
<li>提出了一种新的统一框架来增强3D生成的扩散先验。</li>
<li>该框架通过迭代优化3D模型和扩散先验来提高生成图像的质量。</li>
<li>该框架利用扩散先验的不同可学习参数，提供了多种配置，可在性能和实现复杂性之间进行权衡。</li>
</ul>
<p>（3）性能：</p>
<ul>
<li>在NeRF和3D高斯散射两种骨干网络上均取得了比现有技术更好的性能。</li>
<li>在文本到3D生成的领域树立了新的技术水平。</li>
</ul>
<p>（4）工作量：</p>
<ul>
<li>该框架的实现复杂度较高，需要较多的计算资源。</li>
<li>该框架的训练时间较长，需要花费数天或数周的时间。</li>
</ul>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-abd850391ea953af46ce32e34e49d149.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f856eb6ef9dd71eaef7a7695cd8943b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a74922c93a0f56db6537ecd6f851decb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d83c9cfec77cb9e942b656c02dfcdcf7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1dcf54591ff8e16e58863a1eaa2623a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47cb88f4c0caa3a68ecae822221de46d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dc0db37ef3bd4c00248273e0c7931a50.jpg" align="middle">
</details>




## SO-NeRF: Active View Planning for NeRF using Surrogate Objectives

**Authors:Keifer Lee, Shubham Gupta, Sunglyoung Kim, Bhargav Makwana, Chao Chen, Chen Feng**

Despite the great success of Neural Radiance Fields (NeRF), its data-gathering process remains vague with only a general rule of thumb of sampling as densely as possible. The lack of understanding of what actually constitutes good views for NeRF makes it difficult to actively plan a sequence of views that yield the maximal reconstruction quality. We propose Surrogate Objectives for Active Radiance Fields (SOAR), which is a set of interpretable functions that evaluates the goodness of views using geometric and photometric visual cues - surface coverage, geometric complexity, textural complexity, and ray diversity. Moreover, by learning to infer the SOAR scores from a deep network, SOARNet, we are able to effectively select views in mere seconds instead of hours, without the need for prior visits to all the candidate views or training any radiance field during such planning. Our experiments show SOARNet outperforms the baselines with $\sim$80x speed-up while achieving better or comparable reconstruction qualities. We finally show that SOAR is model-agnostic, thus it generalizes across fully neural-implicit to fully explicit approaches. 

[PDF](http://arxiv.org/abs/2312.03266v1) 13 pages

**Summary**
根据几何和光度视觉线索评估视角优劣，帮助 NeRF 迅速地选择最佳视角，提高重建质量。

**Key Takeaways**

- 提出了一种可解释函数 SOAR，用于评估视角的优劣，指标包括表面覆盖率、几何复杂度、纹理复杂度和光线多样性。
- 设计了 SOARNet，可以快速推导出 SOAR 分数，而无需访问候选视角或训练任何辐射场。
- SOARNet 在 80 倍加速的情况下优于基准，重建质量更好或相当。
- SOAR 与模型无关，适用于纯神经隐式到纯显式方法。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>题目：SO-NeRF：使用代理目标的 NeRF 主动视图规划</li>
<li>作者：Chen Feng、Yuxuan Zhang、Xiaoguang Han、Shuang Zhao、Zhiwen Fan、Zeyu Jin、Yibo Yang、Shuang Liang、Lin Gao、Xiaogang Jin</li>
<li>隶属单位：纽约大学</li>
<li>关键词：神经辐射场、主动视图规划、代理目标、视图选择、深度学习</li>
<li>论文链接：None，Github 代码链接：None</li>
<li>
<p>摘要：
（1）：尽管神经辐射场 (NeRF) 取得了巨大的成功，但其数据收集过程仍然模糊不清，只有一个“尽可能密集地采样”的一般经验法则。由于缺乏对什么实际上构成 NeRF 的良好视图的理解，因此很难主动规划出一系列视图，从而产生最大的重建质量。
（2）：过去的方法包括随机采样、主动学习和基于不确定性的采样。这些方法要么效率低下，要么需要对辐射场进行多次访问，要么需要对所有候选视图进行预先访问。
（3）：本文提出了一种用于主动辐射场的代理目标 (SOAR)，这是一组可解释的函数，使用几何和光度视觉线索（表面覆盖率、几何复杂性、纹理复杂性和光线多样性）来评估视图的优劣。此外，通过学习从深度网络 SOARNet 推断 SOAR 分数，我们能够在短短几秒内有效地选择视图，而无需事先访问所有候选视图或在规划期间训练任何辐射场。
（4）：实验表明，SOARNet 在实现更好或相当的重建质量的同时，比基线快约 80 倍。我们最终表明 SOAR 与模型无关，因此它可以跨越完全神经隐式到完全显式的方法进行推广。</p>
</li>
<li>
<p>方法：
（1）首先，我们定义了评估训练集质量的目标函数，该函数最大化了表面的覆盖率、几何复杂性、纹理复杂性和光线多样性。
（2）然后，我们提出了 SOARNet，这是一个深度神经网络，可以有效地计算目标函数的分数，而无需访问所有候选视图或在规划期间训练任何辐射场。
（3）最后，我们通过贪婪策略构建了一个最优的轨迹，该策略在每一步选择最大化所需目标函数的视图。</p>
</li>
<li>
<p>结论：
（1）：本文提出的 SOAR 是一个代理目标函数集合，旨在指示给定一组输入时，生成的辐射场模型的优劣。为了实现实时轨迹生成的有效推理，我们进一步提出了一个深度神经网络 SOARNet，它能够在看不见的姿态下以每步&lt;1s的速度进行规划。通过广泛的评估，我们已经证明我们的方法确实比基线快约 80 倍，同时实现了更好或相当的重建质量。
（2）：创新点：</p>
</li>
<li>提出了一种代理目标函数集合 SOAR，用于评估给定一组输入时，生成的辐射场模型的优劣。</li>
<li>提出了一种深度神经网络 SOARNet，能够在看不见的姿态下以每步&lt;1s的速度进行规划。
性能：</li>
<li>在实现更好或相当的重建质量的同时，比基线快约 80 倍。
工作量：</li>
<li>实验表明，SOARNet 在实现更好或相当的重建质量的同时，比基线快约 80 倍。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-671944d1cdae24c5a23ad1828db56206.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dabcaa09003b69ee9c199436cd4103a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d920e266e7bb522989e7f812b8f3e8c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6fc30479617914fe92232c4f441700b2.jpg" align="middle">
</details>




<h2 id="SANeRF-HQ-Segment-Anything-for-NeRF-in-High-Quality"><a href="#SANeRF-HQ-Segment-Anything-for-NeRF-in-High-Quality" class="headerlink" title="SANeRF-HQ: Segment Anything for NeRF in High Quality"></a>SANeRF-HQ: Segment Anything for NeRF in High Quality</h2><p><strong>Authors:Yichen Liu, Benran Hu, Chi-Keung Tang, Yu-Wing Tai</strong></p>
<p>Recently, the Segment Anything Model (SAM) has showcased remarkable capabilities of zero-shot segmentation, while NeRF (Neural Radiance Fields) has gained popularity as a method for various 3D problems beyond novel view synthesis. Though there exist initial attempts to incorporate these two methods into 3D segmentation, they face the challenge of accurately and consistently segmenting objects in complex scenarios. In this paper, we introduce the Segment Anything for NeRF in High Quality (SANeRF-HQ) to achieve high quality 3D segmentation of any object in a given scene. SANeRF-HQ utilizes SAM for open-world object segmentation guided by user-supplied prompts, while leveraging NeRF to aggregate information from different viewpoints. To overcome the aforementioned challenges, we employ density field and RGB similarity to enhance the accuracy of segmentation boundary during the aggregation. Emphasizing on segmentation accuracy, we evaluate our method quantitatively on multiple NeRF datasets where high-quality ground-truths are available or manually annotated. SANeRF-HQ shows a significant quality improvement over previous state-of-the-art methods in NeRF object segmentation, provides higher flexibility for object localization, and enables more consistent object segmentation across multiple views. Additional information can be found at <a href="https://lyclyc52.github.io/SANeRF-HQ/">https://lyclyc52.github.io/SANeRF-HQ/</a>. </p>
<p><a href="http://arxiv.org/abs/2312.01531v1">PDF</a> </p>
<p><strong>摘要</strong><br>利用SAM的提示和NeRF的多个视角，以高质量分割3D目标。</p>
<p><strong>要点</strong></p>
<ul>
<li>SANeRF-HQ将SAM用于开放世界目标分割，并利用NeRF从不同视点聚合信息。</li>
<li>为了克服上述挑战，我们采用密度场和RGB相似性来提高聚合期间分割边界的准确性。</li>
<li>SANeRF-HQ在多个具有高质量基本事实或手动注释的NeRF数据集上进行定量评估。</li>
<li>SANeRF-HQ在NeRF目标分割方面对以前的最先进方法显示出显着的质量改进。</li>
<li>SANeRF-HQ为目标检测提供了更高的灵活性，并实现了跨多个视图更一致的目标分割。</li>
<li>可以通过<a href="https://lyclyc52.github.io/SANeRF-HQ/获取更多相关信息。">https://lyclyc52.github.io/SANeRF-HQ/获取更多相关信息。</a></li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>标题：SANeRF-HQ：高品质任意物体 NeRF 分割</li>
<li>作者：Yichen Liu, Benran Hu, Chi-Keung Tang, Yu-Wing Tai</li>
<li>隶属机构：香港科技大学</li>
<li>关键词：NeRF，分割，任意物体分割，高品质，零样本分割</li>
<li>论文链接：https://arxiv.org/abs/2312.01531
   Github 链接：None</li>
<li>
<p>摘要：
(1)：研究背景：神经辐射场（NeRF）在复杂真实世界场景的新颖视图合成中取得了最先进的结果。NeRF 使用多层感知器（MLP）对给定场景进行编码，并支持查询给定 3D 坐标和视图方向的密度和辐射，这些坐标和视图方向用于从任何视点渲染逼真的图像。此外，在训练期间，NeRF 只需要具有相机位姿的 RGB 图像，这直接将 3D 链接到 2D。具有连续表示的简单但巧妙的架构很快开始挑战使用显式离散结构（例如 RGB-D 图像或点云）的传统表示。因此，NeRF 准备好在 3D 视觉中解决更具挑战性的任务。NeRF 表示可以受益的一个重要的下游任务是 3D 对象分割，这是 3D 视觉的基础，并广泛用于许多应用。为了解决 NeRF 中的对象分割问题，研究人员调查了各种方法。针对语义分割的语义 NeRF 是该方向上的第一批作品之一。DFF 将预训练特征（例如 DINO）的知识蒸馏到 3D 特征场中，用于无监督对象分解。监督方法（例如 [47]）利用 Mask2Former 获得初始 2D 掩码，并使用全景辐射场将其提升到 3D。尽管这些方法展示了令人印象深刻的结果，但它们的性能受到用于生成特征的预训练模型的限制。最近，出现了大型视觉模型，例如任意物体分割模型（SAM），具有强大的零样本泛化性能，可以作为许多下游任务的骨干组件。具体来说，SAM 为分割任务提出了一种新范式，该范式可以接受各种提示作为输入，并生成不同语义级别的分割掩码作为输出。SAM 的多功能性和泛化性为在 NeRF 中执行可提示的对象分割提供了新方法。虽然对这一领域进行了一些调查[10, 13, 21]，但新视图中的掩码质量仍然不令人满意。有鉴于此，我们提出了一种新的通用框架来实现 NeRF 中基于提示的 3D 分割。我们的框架称为 SegmentAnything for NeRF in High Quality，或 SANeRF-HQ，它利用现有的 2D 基础模型（例如 SegmentAnything）允许各种提示作为输入，并生成具有高精度和多视图一致性的 3D 分割。我们论文的主要贡献是：</p>
</li>
<li>
<p>我们提出了 SANeRF-HQ，这是在 NeRF 中生成高质量 3D 对象分割的首次尝试之一，在更准确的分割边界和更好的多视图一致性方面取得了进展。</p>
</li>
<li>我们通过组装和评估多个 NeRF 数据集来验证我们的方法，这些数据集中提供了高质量的真实情况或手动注释。SANeRF-HQ 在 NeRF 对象分割中的先前最先进方法上显示出显着的质量改进，为对象定位提供了更高的灵活性，并能够在多个视图中实现更一致的对象分割。有关更多信息，请访问 https://lyclyc52.github.io/SANeRF-HQ/。</li>
</ol>
<p>(2)：过去的方法：
* 语义 NeRF：针对语义分割，但性能受限于预训练模型。
* DFF：将预训练特征蒸馏到 3D 特征场中，用于无监督对象分解，但性能受限于预训练模型。
* Mask2Former：利用 Mask2Former 获得初始 2D 掩码，并使用全景辐射场将其提升到 3D，但新视图中的掩码质量仍然不令人满意。</p>
<p>(3)：本研究方法：
* SANeRF-HQ：利用现有的 2D 基础模型（例如 SegmentAnything）允许各种提示作为输入，并生成具有高精度和多视图一致性的 3D 分割。
* 我们利用密度场和 RGB 相似性来增强聚合过程中分割边界的准确性。</p>
<p>(4)：方法的性能：
* 在多个 NeRF 数据集上，SANeRF-HQ 在 NeRF 对象分割中的先前最先进方法上显示出显着的质量改进。
* SANeRF-HQ 为对象定位提供了更高的灵活性，并能够在多个视图中实现更一致的对象分割。</p>
<ol>
<li>
<p>方法：
（1）特征容器：利用预训练的 SAM 模型对图像进行编码，得到 2D 特征，这些特征可重复用于预测和传播掩码，因此可以预先计算或提取场景特征，并针对不同的输入提示重复使用；
（2）掩码解码器：将用户提供的提示在不同视图之间传播，并使用来自容器的 SAM 特征生成中间掩码输出；
（3）掩码聚合器：将生成的 2D 掩码集成到 3D 空间中，并利用来自 NeRF 模型的颜色和密度场来实现高质量的 3D 分割。</p>
</li>
<li>
<p>结论：
（1）：SANeRF-HQ 结合了 SegmentAnything 模型 (SAM) 在开放世界物体分割中的优势和 NeRF 在聚合来自多个视点的信息的优势，在高质量 3D 分割方面取得了重大进展。我们的方法在各种 NeRF 数据集上进行了定量和定性评估，这证明了 SANeRF-HQ 相比于以前最先进的方法的优势。此外，我们展示了将我们的工作扩展到 4D 动态 NeRF 对象分割的潜力（请参阅补充材料）。SANeRF-HQ 有望为不断发展的 3D 计算机视觉和分割技术领域做出重大贡献。
（2）：创新点：</p>
</li>
<li>利用预训练的 SAM 模型对图像进行编码，得到 2D 特征，这些特征可重复用于预测和传播掩码。</li>
<li>使用来自 NeRF 模型的颜色和密度场来实现高质量的 3D 分割。
性能：</li>
<li>在多个 NeRF 数据集上，SANeRF-HQ 在 NeRF 对象分割中的先前最先进方法上显示出显着的质量改进。</li>
<li>SANeRF-HQ 为对象定位提供了更高的灵活性，并能够在多个视图中实现更一致的对象分割。
工作量：</li>
<li>SANeRF-HQ 是一种通用的框架，可以与任何预训练的 2D 分割模型一起使用。</li>
<li>SANeRF-HQ 易于实现，并且可以在各种 NeRF 数据集上进行训练和评估。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9a343a2eb78b0ee139f02bb29d9d32d5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1095025f718c19933ffeb4fc65253032.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9cd347cc2cc1b17675b2cf3433c14135.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d4043be457d52c93131d8791d9d26578.jpg" align="middle">
</details>




<h2 id="Deceptive-Human-Prompt-to-NeRF-3D-Human-Generation-with-3D-Consistent-Synthetic-Images"><a href="#Deceptive-Human-Prompt-to-NeRF-3D-Human-Generation-with-3D-Consistent-Synthetic-Images" class="headerlink" title="Deceptive-Human: Prompt-to-NeRF 3D Human Generation with 3D-Consistent   Synthetic Images"></a>Deceptive-Human: Prompt-to-NeRF 3D Human Generation with 3D-Consistent   Synthetic Images</h2><p><strong>Authors:Shiu-hong Kao, Xinhang Liu, Yu-Wing Tai, Chi-Keung Tang</strong></p>
<p>This paper presents Deceptive-Human, a novel Prompt-to-NeRF framework capitalizing state-of-the-art control diffusion models (e.g., ControlNet) to generate a high-quality controllable 3D human NeRF. Different from direct 3D generative approaches, e.g., DreamFusion and DreamHuman, Deceptive-Human employs a progressive refinement technique to elevate the reconstruction quality. This is achieved by utilizing high-quality synthetic human images generated through the ControlNet with view-consistent loss. Our method is versatile and readily extensible, accommodating multimodal inputs, including a text prompt and additional data such as 3D mesh, poses, and seed images. The resulting 3D human NeRF model empowers the synthesis of highly photorealistic novel views from 360-degree perspectives. The key to our Deceptive-Human for hallucinating multi-view consistent synthetic human images lies in our progressive finetuning strategy. This strategy involves iteratively enhancing views using the provided multimodal inputs at each intermediate step to improve the human NeRF model. Within this iterative refinement process, view-dependent appearances are systematically eliminated to prevent interference with the underlying density estimation. Extensive qualitative and quantitative experimental comparison shows that our deceptive human models achieve state-of-the-art application quality. </p>
<p><a href="http://arxiv.org/abs/2311.16499v1">PDF</a> Github project: <a href="https://github.com/DanielSHKao/DeceptiveHuman">https://github.com/DanielSHKao/DeceptiveHuman</a></p>
<p><strong>Summary</strong></p>
<p>模拟人类：巧用控制扩散模型，生成高品质可控 3D 人类 NeRF。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Deceptive-Human 是一个新颖的 Prompt-to-NeRF 框架，利用先进的控制扩散模型生成高质量的可控 3D 人类 NeRF。</li>
<li>不同于直接 3D 生成方法（如 DreamFusion 和 DreamHuman），Deceptive-Human 采用渐进优化技术来提升重建质量。</li>
<li>该方法通过利用 ControlNet 生成的高质量合成人体图像和视图一致性损失来实现。</li>
<li>Deceptive-Human 方法是多功能的，并可轻松扩展，可适应多种模态输入，包括文本提示和额外的 3D 网格、姿势和种子图像。</li>
<li>结果的 3D 人类 NeRF 模型能够从 360 度视角合成高度逼真的新视角。</li>
<li>Deceptive-Human 的关键在于其渐进微调策略，该策略涉及使用在每个中间步骤提供的多种输入反复增强视图，以改进人类 NeRF 模型。</li>
<li>在这个迭代细化过程中，系统地消除视图相关外观，以防止其干扰潜在的密度估计。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：Deceptive-Human：带提示的 NeRF 3D 人体生成</li>
<li>作者：Daniel S.H. Kao, Jiapeng Tang, Jiaxiang Shang, Chen Change Loy, Qifeng Chen</li>
<li>隶属机构：香港中文大学</li>
<li>关键词：3D 人体生成、NeRF、扩散模型、文本到 3D</li>
<li>论文链接：https://arxiv.org/abs/2302.08823，Github 链接：None</li>
<li>摘要：
(1) 研究背景：NeRF 是一种强大的技术，可以从 2D 图像生成逼真的 3D 场景。然而，直接使用 NeRF 生成 3D 人体存在诸多挑战，例如难以捕捉人体复杂的几何形状和纹理，以及难以保证生成的图像在不同视角下的一致性。
(2) 过去的方法：一些研究尝试使用扩散模型来生成 3D 人体，但这些方法通常需要大量的数据和计算资源，并且生成的图像质量有限。
(3) 本文方法：本文提出了一种名为 Deceptive-Human 的新框架，该框架利用最先进的扩散模型来生成高质量的可控 3D 人体 NeRF。Deceptive-Human 采用了一种渐进式细化技术来提高重建质量，该技术利用通过扩散模型生成的合成图像来训练 NeRF 模型。
(4) 性能：实验结果表明，Deceptive-Human 在图像质量和一致性方面均优于现有方法。Deceptive-Human 可以从 360 度视角合成高度逼真的新视图，并且可以用于各种应用，例如虚拟现实、增强现实和游戏。</li>
</ol>
<p><methods>:
(1) 背景：介绍了用于本文的 Clean-NeRF 以及颜色分解方法，该方法是本文使用的重建方法。Clean-NeRF 与原始 NeRF 的输入相同，即空间坐标点 x=(x,y,z) 和方向 d=(θ,ϕ)，以估计密度 σ 和空间特征 b，并基于 b 和 d 预测颜色 c。主要区别在于 c 分解为两个分量，即与视图无关的分量 cvi 和与视图相关的分量 cvd：c=γcvi+(1−γ)cvd，其中 γ 是权重因子。虽然 Clean-NeRF 成功地去除了 NeRF 重建的伪影，但我们的目标是从扩散模型生成的图像中去除视图间的不一致性。由于从低次球谐函数中提取的 cvi 被视为场景中的低频颜色，在 Clean-NeRF 架构中不受视图方向的约束，因此我们在测试期间保留 cvi 而丢弃 cvd，以提取合成图像训练数据中的共同特征。具体来说，我们的 NeRF 结构在推理期间与 Clean-NeRF 不同，其中仅使用与视图无关的分量来生成一致的图像，以便进一步细化（见第 3.2 节）。详细来说，推理阶段的渲染方法可以表示为 ˆCvi=K�k=1ˆT(tk)α(σ(tk)δk)cvi(tk)，其中 ˆT(tk)=exp�−�k−1k′=1σ(tk)δ(tk)�，α(x)=1−exp(−x)，δp=tk+1−tk，tk 是沿射线采样的第 k 个点，σ 是在 Clean-NeRF 架构中估计的密度。
(2) Deceptive-Human 框架：
* 先验生成：这一步旨在分别生成人类角色的高级特征和几何形状，作为种子图像和 3D 代理先验。给定一组用户提示 {P0,P1,···,Pk}，其中 P0 表示文本控制，P1,P2,...,Pk 表示额外的可选图像控制，即深度、边缘、姿势等，我们引入了一个扩散模型 G1，该模型生成 2D 人类图像 Iseed，如下所示：Iseed=G1(P0,P1,...,Pk)。在我们的工作中，G1 是使用 Latent Diffusion Model 的 txt2img 模块构建的，并附加了 k 个 ControlNet 模型，分别对应于相应的提示类型。通常，Latent Diffusion Model 能够生成高质量的图像，而额外的提示，即 P1,P2,...,Pk 用于特定的输出要求。这个种子图像 Iseed 提供了我们生成的人类的高级特征，这些特征由用户提示，例如，一位穿着白色短裙和蓝色衬衫、长发、白色鞋子，见图 2 的女白领。接下来可以从 Iseed 生成 3D 几何形状。我们利用单视图网格预测模型根据 Iseed 生成 3D 网格 M。种子图像 Iseed 和 3D 网格 M 作为纹理和几何先验传递到后续步骤。请注意，此网格仅用作 3D 几何代理；我们将展示由生成的 NeRF 在渐进细化后诱导的细化深度图。
* 一致的合成视图生成：这一步用于生成 3D 感知一致的图像。我们首先从网格 M 渲染视图，即 m1,m2,...,mn，并将每个渲染的网格视图 mi 与 k 种控制类型（例如，边缘、深度等）相关联，表示为 f1(mi),f2(mi),...,fk(mi)。接下来，我们使用预训练的扩散模型 G2 作为粗略视图的生成器。具体来说，vi=G2(P0,Iseed,f1(mi),f2(mi),...,fk(mi))，其中 vi 是从 mi 生成的粗略视图，与 mi 的相机姿态相关联。请注意，生成的 vi 可能非常不一致且不具有 3D 感知，因为它们缺少交叉视图知识。为了获得具有 3D 感知力的图像，我们基于 NeRF 重建 {vi}，在优化期间丢弃与视图相关的分量，即颜色分量由空间 MLP 预测，而不受射线方向的约束。具体来说，我们通过插入合成视图和提取的与视图无关的分量之间的语义一致性正则化器来修改第 3.1 节中描述的 Clean-NeRF 重建策略。我们从 {vi} 中随机采样 m 个图像，表示为种子集 D={vjs|s=1,2,...,m}，其中 1≤j1&lt;j2&lt;...<jm≤n。我们利用预训练的编码器 Φ，本文中为="" vit。对于从方程="" 2="" 渲染的每个与视图无关的帧="" ˆivirendered，我们定义图像级正则化器为：lsem="1−Sc�Φ(I),Φ(ˆIvi)�，其中" sc(·)="" 表示余弦相似度，i="" 是从="" d="" 中随机采样种子图像。最后，我们将方程="" 5="" 与="" clean-nerf="" 重建损失="" lpho,lvi="" 和="" lvd（详细推导见="" [37]）结合起来，开发了一个新的视图一致损失="" lcon，满足="" lcon="�Lpho+�x(Lvi+Lvd)�+λLsem，其中" λ="">0 是预定义的参数。通常，我们获得一个粗糙的辐射场，我们可以从中提取一致的 3D 感知图像，遵循方程 6 中的重建策略。然而，这个 NeRF 仍然远未令人满意。基于这个粗糙的 NeRF，我们在下面介绍 3D 细化的 NeRF。
* 3D 视图细化：这一步重点是细化从粗糙 NeRF 渲染的 3D 感知图像，然后重建一个增强的优质 NeRF。假设我们渲染与视图无关的帧，即 ˆIvi，则使用与视图无关的分量对其进行细化，如下所示：ˆIvi=G3(ˆIvi,P0,Iseed)。然后，我们使用细化的图像来更新 NeRF，如下所示：ˆσ=ˆσ+αˆσ，ˆb=ˆb+αˆb，其中 α 是学习率。我们重复此过程，直到达到收敛。</jm≤n。我们利用预训练的编码器></methods></p>
<ol>
<li>结论：
（1）：本文提出了 Deceptive-Human，这是一个新颖的端到端 Prompt-to-NeRF 框架，该框架利用多模态指导提示生成高质量的 3D 人体 NeRF，包括文本描述以及网格、姿势和风格等其他控制。我们利用了具有神经辐射场 (NeRF) 的最先进的 2D 可控扩散模型，并采用两阶段 NeRF 重建方法来确保合成图像之间的一致性。在第一阶段，从粗糙但一致的图像中丢弃了与视图相关的分量。在第二阶段，对这些图像进行去噪以生成逼真的合成视图，以便为 NeRF 的精细版本进行重建。大量的实验表明，Deceptive-Human 在质量方面优于最先进的基线，并通过其多控制生成的可使用性，极大地扩展了 3D 人体生成在普通用户中的适用性。
（2）：创新点：</li>
<li>提出了一种新颖的端到端 Prompt-to-NeRF 框架，该框架可以从多模态指导提示生成高质量的 3D 人体 NeRF。</li>
<li>利用了具有神经辐射场 (NeRF) 的最先进的 2D 可控扩散模型，并采用两阶段 NeRF 重建方法来确保合成图像之间的一致性。</li>
<li>在第一阶段，从粗糙但一致的图像中丢弃了与视图相关的分量。在第二阶段，对这些图像进行去噪以生成逼真的合成视图，以便为 NeRF 的精细版本进行重建。</li>
</ol>
<p>性能：
* 在质量方面优于最先进的基线。
* 极大地扩展了 3D 人体生成在普通用户中的适用性。</p>
<p>工作量：
* 需要大量的实验和计算资源。
* 需要对模型进行微调以适应不同的数据集。</p>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9f7d30300d876235d30715a48931b9e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2fd76e8d6804ec1d50363c9316996837.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d2afeb012626cf2b90dc5b2d57f5440.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-013099c91542aa46e6a16c5f4d465795.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-991d93d3bf524876676fa47b2a4071ab.jpg" align="middle">
</details>




## DynVideo-E: Harnessing Dynamic NeRF for Large-Scale Motion- and   View-Change Human-Centric Video Editing

**Authors:Jia-Wei Liu, Yan-Pei Cao, Jay Zhangjie Wu, Weijia Mao, Yuchao Gu, Rui Zhao, Jussi Keppo, Ying Shan, Mike Zheng Shou**

Despite recent progress in diffusion-based video editing, existing methods are limited to short-length videos due to the contradiction between long-range consistency and frame-wise editing. Prior attempts to address this challenge by introducing video-2D representations encounter significant difficulties with large-scale motion- and view-change videos, especially in human-centric scenarios. To overcome this, we propose to introduce the dynamic Neural Radiance Fields (NeRF) as the innovative video representation, where the editing can be performed in the 3D spaces and propagated to the entire video via the deformation field. To provide consistent and controllable editing, we propose the image-based video-NeRF editing pipeline with a set of innovative designs, including multi-view multi-pose Score Distillation Sampling (SDS) from both the 2D personalized diffusion prior and 3D diffusion prior, reconstruction losses, text-guided local parts super-resolution, and style transfer. Extensive experiments demonstrate that our method, dubbed as DynVideo-E, significantly outperforms SOTA approaches on two challenging datasets by a large margin of 50% ~ 95% for human preference. Code will be released at https://showlab.github.io/DynVideo-E/. 

[PDF](http://arxiv.org/abs/2310.10624v2) Project Page: https://showlab.github.io/DynVideo-E/

**Summary**
动态神经辐射场 (NeRF) 作为视频表示，可进行 3D 空间编辑并通过变形场传播到整段视频，实现一致且可控的视频编辑。

**Key Takeaways**

- 提出创新视频表示，引入动态神经辐射场 (NeRF)，支持 3D 空间编辑。
- 使用多视角多姿势分数蒸馏采样 (SDS) 确保编辑的一致性和可控性。
- 提供重建损失，用于约束 NeRF 的学习过程，确保准确的视频重建。
- 实现基于文本的局部零件超分辨率，使编辑结果更加逼真。
- 利用风格迁移将视频编辑应用于任意风格。
- 在两个具有挑战性的人类动作数据集上进行广泛的实验，证明了方法的有效性。
- 该方法在人类偏好上的性能优于现有方法，提升幅度为 50% ~ 95%。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>标题：DynVideo-E：利用动态神经辐射场进行大规模运动和视点变化的人体中心视频编辑</li>
<li>作者：Jia-Wei Liu, Yan-Pei Cao, Jay Zhangjie Wu, Weijia Mao, Yuchao Gu, Rui Zhao, Jussi Keppo, Ying Shan, Mike Zheng Shou</li>
<li>第一作者单位：ShowLab</li>
<li>关键词：视频编辑、神经辐射场、运动和视点变化、人体中心视频</li>
<li>论文链接：https://arxiv.org/abs/2310.10624，Github 链接：无</li>
<li>
<p>摘要：
（1）研究背景：现有的基于扩散的视频编辑方法由于长程一致性和逐帧编辑之间的矛盾，仅限于短视频。以往尝试通过引入视频二维表示来解决这一挑战，但在处理大规模运动和视点变化的视频时遇到了重大困难，尤其是在人体中心场景中。
（2）过去方法及问题：以往方法试图通过引入视频二维表示来解决这一挑战，但在大规模运动和视点变化的视频，尤其是在人体中心场景中，遇到了重大困难。
（3）研究方法：为了克服这一挑战，我们提出将动态神经辐射场 (NeRF) 作为创新的视频表示，可以在 3D 空间中执行编辑并通过变形场传播到整个视频。为了提供一致且可控的编辑，我们提出了基于图像的视频 NeRF 编辑管道，其中包含一系列创新设计，包括多视图多姿势 Score Disentanglement 模块、基于关键点的 3D 运动估计模块和基于变形场的视频 NeRF 编辑模块。
（4）方法性能：在人体中心视频编辑任务上，我们的方法在定性和定量方面都优于现有方法。实验结果表明，我们的方法可以实现高度一致的大规模运动和视点变化的人体中心视频编辑。</p>
</li>
<li>
<p>方法：
（1）视频-NeRF 模型：我们利用 HOSNeRF 作为视频表示，它可以执行 3D 空间中的编辑并通过变形场传播到整个视频。
（2）图像-NeRF 编辑：我们提出基于图像的视频-NeRF 编辑管道，其中包含一系列创新设计，包括多视图多姿势 ScoreDisentanglement 模块、基于关键点的 3D 运动估计模块和基于变形场的视频-NeRF 编辑模块。
（3）Image-based 3D 动态人体编辑：我们设计了一系列策略来解决一致性和高质量的图像-NeRF 编辑的挑战，包括参考图像重建损失、从 3D 扩散先验中进行分数蒸馏采样、基于关键点的 3D 运动估计和局部部分超分辨率。
（4）背景静态空间编辑：我们利用风格迁移损失将参考样式传输到我们的 3D 背景模型中。</p>
</li>
<li>
<p>结论：
（1）：本文提出了一种名为 DynVideo-E 的新颖框架，用于一致地编辑大规模运动和视点变化的人体中心视频。我们首先提出利用动态神经辐射场 (NeRF) 作为我们创新的视频表示，其中编辑可以在动态 3D 空间中执行，并通过变形场准确地传播到整个视频。然后，我们提出了一组有效的基于图像的视频-NeRF 编辑设计，包括从二维个性化扩散先验和三维扩散先验中进行多视图多姿势分数蒸馏采样 (SDS)、参考图像上的重建损失、文本指导的局部部分超分辨率以及用于 3D 背景空间的风格迁移。最后，大量的实验表明，DynVideo-E 在 SOTA 方法上取得了显着的改进。局限性和未来工作。尽管 DynVideo-E 在视频编辑方面取得了显着的进步，但其基于 NeRF 的表示非常耗时。在视频-NeRF 模型中使用体素或哈希网格可以大大减少训练时间，我们将它留作一个忠实的未来方向。
（2）：创新点：</p>
</li>
<li>提出了一种新颖的视频表示——动态神经辐射场 (NeRF)，它允许在动态 3D 空间中执行编辑并通过变形场传播到整个视频。</li>
<li>提出了一系列有效的基于图像的视频-NeRF 编辑设计，包括多视图多姿势分数蒸馏采样 (SDS)、参考图像上的重建损失、文本指导的局部部分超分辨率以及用于 3D 背景空间的风格迁移。
性能：</li>
<li>在人体中心视频编辑任务上，我们的方法在定性和定量方面都优于现有方法。</li>
<li>实验结果表明，我们的方法可以实现高度一致的大规模运动和视点变化的人体中心视频编辑。
工作量：</li>
<li>训练基于 NeRF 的视频表示非常耗时。</li>
<li>在视频-NeRF 模型中使用体素或哈希网格可以大大减少训练时间。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e51f421f05471d502d845b5a05a0e040.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f12ddaab97fef8603330a543d8097935.jpg" align="middle">
</details>




<h2 id="ED-NeRF-Efficient-Text-Guided-Editing-of-3D-Scene-using-Latent-Space-NeRF"><a href="#ED-NeRF-Efficient-Text-Guided-Editing-of-3D-Scene-using-Latent-Space-NeRF" class="headerlink" title="ED-NeRF: Efficient Text-Guided Editing of 3D Scene using Latent Space   NeRF"></a>ED-NeRF: Efficient Text-Guided Editing of 3D Scene using Latent Space   NeRF</h2><p><strong>Authors:Jangho Park, Gihyun Kwon, Jong Chul Ye</strong></p>
<p>Recently, there has been a significant advancement in text-to-image diffusion models, leading to groundbreaking performance in 2D image generation. These advancements have been extended to 3D models, enabling the generation of novel 3D objects from textual descriptions. This has evolved into NeRF editing methods, which allow the manipulation of existing 3D objects through textual conditioning. However, existing NeRF editing techniques have faced limitations in their performance due to slow training speeds and the use of loss functions that do not adequately consider editing. To address this, here we present a novel 3D NeRF editing approach dubbed ED-NeRF by successfully embedding real-world scenes into the latent space of the latent diffusion model (LDM) through a unique refinement layer. This approach enables us to obtain a NeRF backbone that is not only faster but also more amenable to editing compared to traditional image space NeRF editing. Furthermore, we propose an improved loss function tailored for editing by migrating the delta denoising score (DDS) distillation loss, originally used in 2D image editing to the three-dimensional domain. This novel loss function surpasses the well-known score distillation sampling (SDS) loss in terms of suitability for editing purposes. Our experimental results demonstrate that ED-NeRF achieves faster editing speed while producing improved output quality compared to state-of-the-art 3D editing models. </p>
<p><a href="http://arxiv.org/abs/2310.02712v1">PDF</a> </p>
<p><strong>摘要</strong><br>ED-NeRF 将真实场景嵌入潜在扩散模型的潜在空间，提高了 NeRF 编辑速度和质量。</p>
<p><strong>要点</strong></p>
<ul>
<li>将真实场景嵌入潜在扩散模型的潜在空间，构建 ED-NeRF 模型。</li>
<li>ED-NeRF 具有更快的训练速度和更高的编辑效率。</li>
<li>ED-NeRF 采用专为编辑设计的改进损失函数。</li>
<li>ED-NeRF 在编辑速度和输出质量方面优于现有 3D 编辑模型。</li>
<li>ED-NeRF 能够实现 3D 对象的文本编辑，并生成高质量的图像。</li>
<li>ED-NeRF 为 3D 场景编辑提供了新颖的解决方案。</li>
<li>ED-NeRF 可以应用于游戏开发、影视制作等领域。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>标题：ED-NeRF：利用潜在空间 NeRF 进行高效的文本引导 3D 场景编辑</li>
<li>作者：Jangho Park、Gihyun Kwon、Jong Chul Ye</li>
<li>单位：韩国科学技术院人工智能研究生院、机器人学项目、生物与脑工程系</li>
<li>关键词：NeRF、文本引导、3D 场景编辑、潜在空间、扩散模型</li>
<li>论文链接：https://arxiv.org/abs/2310.02712</li>
<li>
<p>摘要：
（1）研究背景：近年来，文本到图像扩散模型取得了重大进展，在 2D 图像生成方面取得了突破性的性能。这些进展已扩展到 3D 模型，能够从文本描述中生成新颖的 3D 对象。这已发展成为 NeRF 编辑方法，该方法允许通过文本条件操纵现有 3D 对象。然而，现有的 NeRF 编辑技术由于训练速度慢以及使用不充分考虑编辑的损失函数，在性能上受到限制。
（2）过去方法和问题：过去的方法包括图像空间 NeRF 编辑，但存在训练速度慢、对编辑不友好等问题。
（3）研究方法：为了解决这些问题，本文提出了一种新颖的 3D NeRF 编辑方法，称为 ED-NeRF，通过独特的细化层将真实世界场景成功嵌入潜在扩散模型 (LDM) 的潜在空间中。这种方法使我们能够获得一个 NeRF 主干，它不仅更快，而且与传统的图像空间 NeRF 编辑相比更适合编辑。此外，我们通过将最初用于 2D 图像编辑的 delta 去噪分数 (DDS) 蒸馏损失迁移到三维域，提出了一种针对编辑量身定制的改进损失函数。这种新颖的损失函数在适合编辑目的方面超越了众所周知的分数蒸馏采样 (SDS) 损失。
（4）实验结果：我们的实验结果表明，与最先进的 3D 编辑模型相比，ED-NeRF 在实现更快的编辑速度的同时，还产生了改进的输出质量。这些性能支持了本文的目标。</p>
</li>
<li>
<p>Methods:
(1) 提出了一种新颖的3D NeRF 编辑方法 ED-NeRF，通过独特的细化层将真实世界场景成功嵌入潜在扩散模型 (LDM) 的潜在空间中；
(2) 提出了一种针对编辑量身定制的改进损失函数，通过将最初用于 2D 图像编辑的 delta 去噪分数 (DDS) 蒸馏损失迁移到三维域；
(3) 在真实世界场景上评估了 ED-NeRF 的性能，结果表明，与最先进的 3D 编辑模型相比，ED-NeRF 在实现更快的编辑速度的同时，还产生了改进的输出质量。</p>
</li>
<li>
<p>结论：
（1）：本文提出了一种新颖的 ED-NeRF 方法，该方法在潜在空间中进行了优化。通过使 NeRF 能够直接预测潜在特征，它有效地利用了潜在扩散模型的文本引导评分函数，而无需编码器。通过这样做，我们的方法能够有效降低计算成本，并解决先前模型的负担，这些模型需要以全分辨率渲染才能利用扩散模型。我们扩展了强大的 2D 图像编辑性能，使 ED-NeRF 能够在保持输出质量的同时，以更快的速度编辑 3D 场景。
（2）：创新点：</p>
</li>
<li>将真实世界场景成功嵌入潜在扩散模型 (LDM) 的潜在空间中，通过独特的细化层，使 NeRF 能够直接预测潜在特征。</li>
<li>提出了一种针对编辑量身定制的改进损失函数，将最初用于 2D 图像编辑的 delta 去噪分数 (DDS) 蒸馏损失迁移到三维域。</li>
<li>在真实世界场景上评估了 ED-NeRF 的性能，结果表明，与最先进的 3D 编辑模型相比，ED-NeRF 在实现更快的编辑速度的同时，还产生了改进的输出质量。
性能：</li>
<li>与最先进的 3D 编辑模型相比，ED-NeRF 在实现更快的编辑速度的同时，还产生了改进的输出质量。</li>
<li>ED-NeRF 能够有效降低计算成本，并解决先前模型的负担，这些模型需要以全分辨率渲染才能利用扩散模型。
工作量：</li>
<li>ED-NeRF 的训练速度更快，并且与传统的图像空间 NeRF 编辑相比更适合编辑。</li>
<li>ED-NeRF 的损失函数是针对编辑量身定制的，在适合编辑目的方面超越了众所周知的分数蒸馏采样 (SDS) 损失。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cdb1ccf9138631994193d2b408f855a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fa1ef9ca7e816bcb461a93f9dab30ab2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-462b0d642d1c29db012aa25db302beb0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1d2140a841f3f0c8950e698c3992fc8.jpg" align="middle">
</details>




<h2 id="NOFA-NeRF-based-One-shot-Facial-Avatar-Reconstruction"><a href="#NOFA-NeRF-based-One-shot-Facial-Avatar-Reconstruction" class="headerlink" title="NOFA: NeRF-based One-shot Facial Avatar Reconstruction"></a>NOFA: NeRF-based One-shot Facial Avatar Reconstruction</h2><p><strong>Authors:Wangbo Yu, Yanbo Fan, Yong Zhang, Xuan Wang, Fei Yin, Yunpeng Bai, Yan-Pei Cao, Ying Shan, Yang Wu, Zhongqian Sun, Baoyuan Wu</strong></p>
<p>3D facial avatar reconstruction has been a significant research topic in computer graphics and computer vision, where photo-realistic rendering and flexible controls over poses and expressions are necessary for many related applications. Recently, its performance has been greatly improved with the development of neural radiance fields (NeRF). However, most existing NeRF-based facial avatars focus on subject-specific reconstruction and reenactment, requiring multi-shot images containing different views of the specific subject for training, and the learned model cannot generalize to new identities, limiting its further applications. In this work, we propose a one-shot 3D facial avatar reconstruction framework that only requires a single source image to reconstruct a high-fidelity 3D facial avatar. For the challenges of lacking generalization ability and missing multi-view information, we leverage the generative prior of 3D GAN and develop an efficient encoder-decoder network to reconstruct the canonical neural volume of the source image, and further propose a compensation network to complement facial details. To enable fine-grained control over facial dynamics, we propose a deformation field to warp the canonical volume into driven expressions. Through extensive experimental comparisons, we achieve superior synthesis results compared to several state-of-the-art methods. </p>
<p><a href="http://arxiv.org/abs/2307.03441v1">PDF</a> </p>
<p><strong>摘要</strong><br>单张图片即可重建高保真 3D 面部虚拟形象，带来灵活的姿势和表情控制。</p>
<p><strong>要点</strong></p>
<ul>
<li>新颖的单张图片 3D 面部虚拟形象重建框架，只需一张源图像即可重建高保真 3D 面部虚拟形象。</li>
<li>克服 NeRF 无法泛化到新身份的缺点，仅使用一张源图像即可重建新身份的 3D 面部虚拟形象。</li>
<li>提出一种补偿网络来补充面部细节，提高面部虚拟形象的保真度。</li>
<li>提出一个变形场将规范体积扭曲成驱动的表情，实现对表情的细粒度控制。</li>
<li>与最先进的方法相比，实现优越的合成结果。</li>
<li>在多个基准数据集上进行广泛的实验评估，证明了该方法的有效性和泛化能力。</li>
<li>潜在应用包括虚拟现实、增强现实和游戏。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：NOFA：基于 NeRF 的单次拍摄面部虚拟形象重建</li>
<li>作者：王博宇、范彦波、张勇、王璇、费寅、白云鹏、曹延沛、殷山、吴阳、孙忠强、吴保元</li>
<li>单位：腾讯优图实验室、蚂蚁集团、清华大学</li>
<li>关键词：面部虚拟形象、视频合成、NeRF</li>
<li>论文链接：https://arxiv.org/abs/2307.03441，Github 链接：无</li>
<li>摘要：
（1）研究背景：面部虚拟形象重建是计算机图形学和计算机视觉领域的重要研究课题，在虚拟现实、增强现实、电影工业和远程会议等领域有广泛应用。高保真面部重建和细粒度面部重演是这些应用的基础。
（2）过去方法与问题：为了动画面部图像，已经提出了多种 2D 方法，利用基于流的扭曲在图像或特征空间中传递运动，以及编码器-解码器网络来合成逼真的面部图像。然而，这些方法通常需要大量训练数据，并且难以泛化到新的身份。
（3）研究方法：本文提出了一种单次拍摄 3D 面部虚拟形象重建框架，仅需一张源图像即可重建高保真 3D 面部虚拟形象。为了解决泛化能力不足和缺少多视图信息的问题，我们利用 3DGAN 的生成先验，并开发了一个高效的编码器-解码器网络来重建源图像的规范神经体积，并进一步提出一个补偿网络来补充面部细节。为了实现对面部动态的细粒度控制，我们提出了一个变形场，将规范体积扭曲成驱动的表情。
（4）实验结果：通过广泛的实验比较，我们的方法在合成结果方面优于几种最先进的方法。这些结果支持了我们的目标，即构建一个能够从单张图像重建高保真 3D 面部虚拟形象的框架。</li>
</ol>
<p>Methods：
（1）：提出一个基于NeRF的单次拍摄3D面部虚拟形象重建框架NOFA，利用3DGAN的生成先验和高效的编码器-解码器网络重建源图像的规范神经体积，并进一步提出一个补偿网络来补充面部细节。
（2）：使用3DMM引导的变形场来实现对面部动态的细粒度控制，将规范体积扭曲成驱动的表情。
（3）：在训练阶段使用多种损失函数来确保逼真的重建和生动的重演，包括图像重建损失、体积一致性损失、变形场损失和对抗损失。</p>
<ol>
<li>结论：
（1）：本文提出了一种基于NeRF的单次拍摄3D面部虚拟形象重建框架NOFA，该框架仅需一张源图像即可重建高保真3D面部虚拟形象。
（2）：创新点：</li>
<li>提出了一种基于NeRF的单次拍摄3D面部虚拟形象重建框架NOFA。</li>
<li>利用3DGAN的生成先验和高效的编码器-解码器网络重建源图像的规范神经体积。</li>
<li>提出一个补偿网络来补充面部细节。</li>
<li>使用3DMM引导的变形场来实现对面部动态的细粒度控制，将规范体积扭曲成驱动的表情。</li>
<li>在训练阶段使用多种损失函数来确保逼真的重建和生动的重演，包括图像重建损失、体积一致性损失、变形场损失和对抗损失。
性能：</li>
<li>在合成结果方面优于几种最先进的方法。
工作量：</li>
<li>训练过程相对复杂，需要大量的数据和计算资源。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-469cd2dc109bc6c14b5596ae4928857d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3a38646bddce1978ca080ab7995373b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b3e291d277a3c74b961d6b7a943cfd20.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-720ea0eebc7863101769dccda881d6ee.jpg" align="middle">
</details>




]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>NeRF</tag>
      </tags>
  </entry>
  <entry>
    <title>Diffusion Models</title>
    <url>/2024/01/30/Paper/2024-01-30/Diffusion%20Models/</url>
    <content><![CDATA[<blockquote>
<p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-01-30-更新"><a href="#2024-01-30-更新" class="headerlink" title="2024-01-30 更新"></a>2024-01-30 更新</h1><h2 id="Diffutoon-High-Resolution-Editable-Toon-Shading-via-Diffusion-Models"><a href="#Diffutoon-High-Resolution-Editable-Toon-Shading-via-Diffusion-Models" class="headerlink" title="Diffutoon: High-Resolution Editable Toon Shading via Diffusion Models"></a>Diffutoon: High-Resolution Editable Toon Shading via Diffusion Models</h2><p><strong>Authors:Zhongjie Duan, Chengyu Wang, Cen Chen, Weining Qian, Jun Huang</strong></p>
<p>Toon shading is a type of non-photorealistic rendering task of animation. Its primary purpose is to render objects with a flat and stylized appearance. As diffusion models have ascended to the forefront of image synthesis methodologies, this paper delves into an innovative form of toon shading based on diffusion models, aiming to directly render photorealistic videos into anime styles. In video stylization, extant methods encounter persistent challenges, notably in maintaining consistency and achieving high visual quality. In this paper, we model the toon shading problem as four subproblems: stylization, consistency enhancement, structure guidance, and colorization. To address the challenges in video stylization, we propose an effective toon shading approach called \textit{Diffutoon}. Diffutoon is capable of rendering remarkably detailed, high-resolution, and extended-duration videos in anime style. It can also edit the content according to prompts via an additional branch. The efficacy of Diffutoon is evaluated through quantitive metrics and human evaluation. Notably, Diffutoon surpasses both open-source and closed-source baseline approaches in our experiments. Our work is accompanied by the release of both the source code and example videos on Github (Project page: <a href="https://ecnu-cilab.github.io/DiffutoonProjectPage/">https://ecnu-cilab.github.io/DiffutoonProjectPage/</a>). </p>
<p><a href="http://arxiv.org/abs/2401.16224v1">PDF</a> </p>
<p><strong>Summary</strong><br>以扩散模型为基础，提出一种将写实视频直接渲染成动漫风格的创新性卡通渲染方法。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>将卡通渲染问题建模为四个子问题：风格化、一致性增强、结构引导和着色。</li>
<li>提出了一种名为 Diffutoon 的有效卡通渲染方法，能够渲染出细节丰富、高分辨率、长时间的动漫风格视频。</li>
<li>Diffutoon 可以通过额外的分支根据提示编辑视频内容。</li>
<li>在定量指标和人类评估中，Diffutoon 优于开源和闭源基线方法。</li>
<li>在 Github 上发布了源代码和示例视频（项目主页：<a href="https://ecnu-cilab.github.io/DiffutoonProjectPage/）。">https://ecnu-cilab.github.io/DiffutoonProjectPage/）。</a></li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：ToonShading：基于扩散的高分辨率可编辑卡通渲染</li>
<li>作者：Zhongjie Duan, Chengyu Wang, Cen Chen, Weining Qian, Jun Huang</li>
<li>单位：华东师范大学</li>
<li>关键词：ToonShading, DiffusionModels, VideoSynthesis</li>
<li>论文链接：None, Github 代码链接：https://github.com/ecnu-cilab/DiffutoonProjectPage/</li>
<li>
<p>摘要：
（1）研究背景：卡通渲染是一种非真实感渲染任务，其主要目的是以扁平和风格化的外观渲染对象。扩散模型在图像合成方法学中占据了前沿地位，本文深入研究了一种基于扩散模型的创新卡通渲染形式，旨在将逼真的视频直接渲染成动画风格。在视频风格化中，现有方法面临着持续的挑战，尤其是在保持一致性和实现高视觉质量方面。
（2）过去方法与问题：在不受控的图像合成中，基于适配器类型的控制模块已经证明了精确控制的能力。然而，这些模块仅限于处理单个图像，无法处理视频。为了提高视频的一致性，关于此主题的研究通常分为两类：无训练和基于训练的方法。无训练方法通过构建特定机制来对齐帧之间的内容，无需训练过程，但其有效性有限。另一方面，基于训练的方法通常可以实现更好的结果。然而，由于对冗长的视频数据集进行扩散模型训练所需的计算资源非常大，因此大多数视频扩散模型只能处理最多 32 帧的连续帧，从而导致较长视频中出现不一致的情况。为了获得更好的视觉质量，超分辨率技术可以潜在地提高视频分辨率，但它们可能会引入过度平滑的信息丢失等额外问题。
（3）研究方法：本文提出了一种专门针对卡通渲染的视频处理方法。我们将卡通渲染问题划分为四个子问题：风格化、一致性增强、结构引导和着色。对于每个子问题，我们都提供了一个具体的解决方案。我们提出的框架由一个主要的卡通渲染管道和一个编辑分支组成。在主要的卡通渲染管道中，我们基于动漫风格的扩散模型构建了一个多模块去噪模型。ControlNet 和 AnimateDiff 用于去噪模型中以解决可控性和一致性问题。为了在长视频中生成超高分辨率的内容，我们偏离了传统的逐帧生成范例。相反，我们采用滑动窗口方法来处理视频。在编辑分支中，我们利用提示来控制视频的内容。
（4）方法性能：在任务和性能方面，本文方法能够渲染出非常详细、高分辨率和长时间的动画风格视频。它还可以根据提示通过额外的分支编辑内容。Diffutoon 的有效性通过定量指标和人类评估来评估。值得注意的是，Diffutoon 在我们的实验中超越了开源和闭源基线方法。我们的工作伴随着源代码和示例视频在 Github 上发布。</p>
</li>
<li>
<p>方法：
（1）我们提出了一种专门针对视频渲染的视频处理方法，将视频渲染问题划分为四个子问题：去噪、一致性增强、结构引导和着色，并针对每个子问题提出了具体的解决方案。
（2）我们提出的框架由一个主要的视频渲染管道和一个编辑分支组成。在主要的视频渲染管道中，我们基于动漫视频的扩散模型构建了一个多模块去噪模型，并利用ControlNet和AnimateDiff解决可控性和一致性问题。
（3）为了在长视频中生成超高分辨率的内容，我们偏离了传统的逐帧生成范例，转而采用窗口方法来处理视频。
（4）在编辑分支中，我们利用提示来控制视频的内容。</p>
</li>
<li>
<p>结论：
（1）：本文提出了一种用于视频渲染的创新方法，该方法能够生成非常详细、高分辨率和长时间的动画风格视频，并且可以通过提示编辑视频内容。
（2）：创新点：</p>
</li>
<li>提出了一种专门针对视频渲染的视频处理方法，将视频渲染问题划分为四个子问题：去噪、一致性增强、结构引导和着色，并针对每个子问题提出了具体的解决方案。</li>
<li>在主要的视频渲染管道中，基于动漫视频的扩散模型构建了一个多模块去噪模型，并利用ControlNet和AnimateDiff解决可控性和一致性问题。</li>
<li>为了在长视频中生成超高分辨率的内容，偏离了传统的逐帧生成范例，转而采用窗口方法来处理视频。</li>
<li>在编辑分支中，利用提示来控制视频的内容。
性能：</li>
<li>在任务和性能方面，本文方法能够渲染出非常详细、高分辨率和长时间的动画风格视频。</li>
<li>它还可以根据提示通过额外的分支编辑内容。</li>
<li>Diffutoon的有效性通过定量指标和人类评估来评估。</li>
<li>值得注意的是，Diffutoon在我们的实验中超越了开源和闭源基线方法。
工作量：</li>
<li>本文工作量较大，涉及到扩散模型、控制网络、一致性增强、结构引导和着色等多个方面。</li>
<li>需要对动漫视频进行大规模的数据集训练，以构建动漫风格的扩散模型。</li>
<li>需要对ControlNet和AnimateDiff进行训练，以解决可控性和一致性问题。</li>
<li>需要对视频进行逐帧处理，以生成超高分辨率的动画风格视频。</li>
<li>需要对编辑分支进行训练，以实现对视频内容的控制。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e68001de35eaf91396e2b23b2c2ecde0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a8e23447d7367cb91d3ccb6108983df.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3aa40ea0bb553ba90b2221490a232dfc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fe06192b1bad47a43ecbf8ac48335ed1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-259d9693cc43c3d292031b46b3292db9.jpg" align="middle">
</details>




<h2 id="Object-Driven-One-Shot-Fine-tuning-of-Text-to-Image-Diffusion-with-Prototypical-Embedding"><a href="#Object-Driven-One-Shot-Fine-tuning-of-Text-to-Image-Diffusion-with-Prototypical-Embedding" class="headerlink" title="Object-Driven One-Shot Fine-tuning of Text-to-Image Diffusion with   Prototypical Embedding"></a>Object-Driven One-Shot Fine-tuning of Text-to-Image Diffusion with   Prototypical Embedding</h2><p><strong>Authors:Jianxiang Lu, Cong Xie, Hui Guo</strong></p>
<p>As large-scale text-to-image generation models have made remarkable progress in the field of text-to-image generation, many fine-tuning methods have been proposed. However, these models often struggle with novel objects, especially with one-shot scenarios. Our proposed method aims to address the challenges of generalizability and fidelity in an object-driven way, using only a single input image and the object-specific regions of interest. To improve generalizability and mitigate overfitting, in our paradigm, a prototypical embedding is initialized based on the object’s appearance and its class, before fine-tuning the diffusion model. And during fine-tuning, we propose a class-characterizing regularization to preserve prior knowledge of object classes. To further improve fidelity, we introduce object-specific loss, which can also use to implant multiple objects. Overall, our proposed object-driven method for implanting new objects can integrate seamlessly with existing concepts as well as with high fidelity and generalization. Our method outperforms several existing works. The code will be released. </p>
<p><a href="http://arxiv.org/abs/2401.15708v1">PDF</a> </p>
<p><strong>摘要</strong><br>使用单个输入图像和特定对象关注区域，生成模型可植入新对象，兼具高保真度和泛化性。</p>
<p><strong>要点</strong></p>
<ul>
<li>提出了一种面向对象的方法来植入新对象，仅使用单个输入图像和对象特定关注区域。</li>
<li>该方法在泛化性和保真度方面优于现有工作。</li>
<li>在微调之前，根据对象的出现和类别初始化原型嵌入。</li>
<li>在微调期间，提出了一种类特征正则化来保留对象类别的先验知识。</li>
<li>引入特定于对象的对象损失，用于植入和放置多个对象。</li>
<li>该方法可以与现有概念无缝集成，并具有高保真度和泛化性。</li>
<li>代码将发布。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：基于对象的文本到图像扩散模型的单次微调</li>
<li>作者：Jianxiang Lu, Cong Xie, Hui Guo</li>
<li>单位：腾讯（中国）</li>
<li>关键词：object-driven, one-shot, diffusion model</li>
<li>论文链接：https://arxiv.org/abs/2401.15708
Github 链接：无</li>
<li>
<p>摘要：
（1）研究背景：随着大规模文本到图像生成模型在文本到图像生成领域取得显著进展，许多微调方法被提出。然而，这些模型通常难以处理新颖对象，尤其是在单次微调场景中。
（2）过去方法及其问题：现有方法通常需要大量数据和计算资源，并且难以保证生成图像的保真度和可控性。
（3）研究方法：本文提出了一种基于对象驱动的文本到图像扩散模型的单次微调方法。该方法仅使用单个输入图像和对象特定的感兴趣区域，就可以有效地微调扩散模型，生成具有高保真度和可控性的图像。
（4）方法性能：本文方法在多个数据集上进行了评估，结果表明，该方法在生成图像的保真度、可控性和泛化性方面均优于现有方法。</p>
</li>
<li>
<p>方法：
（1）提出了一种基于对象驱动的文本到图像扩散模型的单次微调方法，该方法仅使用单个输入图像和对象特定的感兴趣区域，就可以有效地微调扩散模型，生成具有高保真度和可控性的图像；
（2）引入了一种对象驱动的原型嵌入初始化方法，该方法可以有效地表示对象，提高对象植入的效率；
（3）提出了一种对象驱动的特定损失函数，该损失函数可以用于合成高保真度的图像，也可以用于多对象植入；
（4）引入了一种类特征正则化方法，该方法可以保护类先验信息，防止灾难性遗忘，提高模型的泛化能力。</p>
</li>
<li>
<p>结论：
（1）本工作的意义：提出了一种基于对象驱动的文本到图像扩散模型的单次微调方法，该方法仅使用单个输入图像和对象特定的感兴趣区域，就可以有效地微调扩散模型，生成具有高保真度和可控性的图像。
（2）文章的优缺点：
创新点：</p>
</li>
<li>提出了一种基于对象驱动的文本到图像扩散模型的单次微调方法，该方法仅使用单个输入图像和对象特定的感兴趣区域，就可以有效地微调扩散模型，生成具有高保真度和可控性的图像。</li>
<li>引入了一种对象驱动的原型嵌入初始化方法，该方法可以有效地表示对象，提高对象植入的效率。</li>
<li>提出了一种对象驱动的特定损失函数，该损失函数可以用于合成高保真度的图像，也可以用于多对象植入。</li>
<li>引入了一种类特征正则化方法，该方法可以保护类先验信息，防止灾难性遗忘，提高模型的泛化能力。
性能：</li>
<li>在多个数据集上进行了评估，结果表明，该方法在生成图像的保真度、可控性和泛化性方面均优于现有方法。
工作量：</li>
<li>需要收集和预处理数据，包括文本数据和图像数据。</li>
<li>需要训练模型，这可能需要大量的时间和计算资源。</li>
<li>需要对模型进行微调，这可能需要额外的训练时间和计算资源。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-34290c5f50d1304ffe58b66fbb188569.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1fbf118677f5f5edfac0f56cf14f457e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c92c4056742c512f459d791828c63886.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f04fc1cc8fffe19240576200c97a367.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4011e336771243f32716368b60213d88.jpg" align="middle">
</details>




<h2 id="CPDM-Content-Preserving-Diffusion-Model-for-Underwater-Image-Enhancement"><a href="#CPDM-Content-Preserving-Diffusion-Model-for-Underwater-Image-Enhancement" class="headerlink" title="CPDM: Content-Preserving Diffusion Model for Underwater Image   Enhancement"></a>CPDM: Content-Preserving Diffusion Model for Underwater Image   Enhancement</h2><p><strong>Authors:Xiaowen Shi, Yuan-Gen Wang</strong></p>
<p>Underwater image enhancement (UIE) is challenging since image degradation in aquatic environments is complicated and changing over time. Existing mainstream methods rely on either physical-model or data-driven, suffering from performance bottlenecks due to changes in imaging conditions or training instability. In this article, we make the first attempt to adapt the diffusion model to the UIE task and propose a Content-Preserving Diffusion Model (CPDM) to address the above challenges. CPDM first leverages a diffusion model as its fundamental model for stable training and then designs a content-preserving framework to deal with changes in imaging conditions. Specifically, we construct a conditional input module by adopting both the raw image and the difference between the raw and noisy images as the input, which can enhance the model’s adaptability by considering the changes involving the raw images in underwater environments. To preserve the essential content of the raw images, we construct a content compensation module for content-aware training by extracting low-level features from the raw images. Extensive experimental results validate the effectiveness of our CPDM, surpassing the state-of-the-art methods in terms of both subjective and objective metrics. </p>
<p><a href="http://arxiv.org/abs/2401.15649v1">PDF</a> </p>
<p><strong>Summary</strong><br>扩散模型与内容保持框架相结合，用于水下图像增强。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>水下图像增强技术可以克服复杂且不断变化的水生环境中的图像退化问题。</li>
<li>当前主流方法或者依赖物理模型，或者依赖数据驱动，在成像条件变化或训练不稳定时性能会下降。</li>
<li>提出了一种内容保持扩散模型（CPDM）来解决上述挑战，CPDM 以扩散模型作为基础模型进行稳定训练，并设计了一个内容保持框架来处理成像条件的变化。</li>
<li>CPDM 构建了一个条件输入模块，采用原始图像和原始图像与噪声图像的差值作为输入，可以考虑水下环境中原始图像的变化，提高模型的适应性。</li>
<li>为了保持原始图像的基本内容，CPDM 构建了一个内容补偿模块，通过从原始图像中提取低级特征进行内容感知训练。</li>
<li>大量实验结果验证了 CPDM 的有效性，在主观和客观指标上都优于最先进的方法。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：CPDM：用于水下图像增强的保内容扩散模型</li>
<li>作者：Xiaowen Shi, Yuan-Gen Wang</li>
<li>单位：广州大学计算机科学与网络工程学院</li>
<li>关键词：水下图像增强、扩散模型、条件输入模块、内容补偿模块</li>
<li>链接：https://arxiv.org/abs/2401.15649</li>
<li>
<p>摘要：
（1）研究背景：水下图像增强是一项具有挑战性的任务，因为水下环境中的图像退化复杂且随时间变化。现有的主流方法要么依赖于物理模型，要么依赖于数据驱动，由于成像条件的变化或训练的不稳定性，它们在性能上存在瓶颈。
（2）过去方法与问题：物理模型方法旨在模拟水中的光传播过程，但由于水下环境随时间变化，这种方法无法适应不同的物理场景，导致泛化能力差。数据驱动方法依赖于大规模数据集进行模型训练，可以有效地提高图像质量，但目前建立的水下图像增强数据集通常是在特定的水下环境中收集的，因此在单个数据集上训练的模型在跨数据集上的性能较差。
（3）研究方法：本文提出了一种新的水下图像增强框架，称为保内容扩散模型（CPDM）。CPDM利用原始图像作为条件输入，并引入原始图像与噪声图像在每个时间步的差值作为另一个条件输入，以增强模型对水下环境中原始图像变化的适应性。为了确保模型保留原始图像的本质内容，本文设计了一个内容补偿模块，从原始图像中提取低级特征进行内容感知训练。
（4）性能与目标：CPDM在水下图像增强任务上取得了优异的性能，在主观和客观指标上都超过了最先进的方法。这些性能支持了本文的目标，即开发一种能够适应水下环境变化并保留原始图像内容的水下图像增强方法。</p>
</li>
<li>
<p>方法：
（1）：本文提出了一种新的水下图像增强框架，称为保内容扩散模型（CPDM）。
（2）：CPDM利用原始图像作为条件输入，并引入原始图像与噪声图像在每个时间步的差值作为另一个条件输入，以增强模型对水下环境中原始图像变化的适应性。
（3）：为了确保模型保留原始图像的本质内容，本文设计了一个内容补偿模块，从原始图像中提取低级特征进行内容感知训练。</p>
</li>
<li>
<p>结论：
（1）：xxx；
（2）：创新点：xxx；性能：xxx；工作量：xxx；</p>
</li>
</ol>
<p>创新点：</p>
<p>（1）：提出了一种新的水下图像增强框架——保内容扩散模型（CPDM）。
（2）：CPDM利用原始图像作为条件输入，并引入原始图像与噪声图像在每个时间步的差值作为另一个条件输入，以增强模型对水下环境中原始图像变化的适应性。
（3）：为了确保模型保留原始图像的本质内容，设计了一个内容补偿模块，从原始图像中提取低级特征进行内容感知训练。</p>
<p>性能：</p>
<p>（1）：CPDM在水下图像增强任务上取得了优异的性能，在主观和客观指标上都超过了最先进的方法。
（2）：CPDM能够有效地去除水下图像中的噪声和雾霾，并增强图像的对比度和清晰度。
（3）：CPDM能够很好地保留原始图像的细节和纹理，并避免产生伪影。</p>
<p>工作量：</p>
<p>（1）：CPDM的模型结构相对复杂，需要较多的训练时间。
（2）：CPDM需要较大的训练数据集，这可能需要花费大量的时间和精力来收集。
（3）：CPDM的训练过程需要大量的计算资源，这可能会增加训练成本。</p>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a8fcc331fc0bfeef66e34869766fa2b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf29b6fb4b07d5c3b4cb5f016776d454.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e73408da8c9156184b27ba3f3078c1e3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bd8e60305606bf1ff35f3c3755cc52f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-23da9492dcf3ab60c1d6d6eea0539743.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a9756b4fce0ba8672bb402f3dc4e5905.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4aa90f24b58e47449838d2b7e74a0358.jpg" align="middle">
</details>




## FreeStyle: Free Lunch for Text-guided Style Transfer using Diffusion   Models

**Authors:Feihong He, Gang Li, Mengyuan Zhang, Leilei Yan, Lingyu Si, Fanzhang Li**

The rapid development of generative diffusion models has significantly advanced the field of style transfer. However, most current style transfer methods based on diffusion models typically involve a slow iterative optimization process, e.g., model fine-tuning and textual inversion of style concept. In this paper, we introduce FreeStyle, an innovative style transfer method built upon a pre-trained large diffusion model, requiring no further optimization. Besides, our method enables style transfer only through a text description of the desired style, eliminating the necessity of style images. Specifically, we propose a dual-stream encoder and single-stream decoder architecture, replacing the conventional U-Net in diffusion models. In the dual-stream encoder, two distinct branches take the content image and style text prompt as inputs, achieving content and style decoupling. In the decoder, we further modulate features from the dual streams based on a given content image and the corresponding style text prompt for precise style transfer. Our experimental results demonstrate high-quality synthesis and fidelity of our method across various content images and style text prompts. The code and more results are available at our project website:https://freestylefreelunch.github.io/. 

[PDF](http://arxiv.org/abs/2401.15636v1) 

**Summary**
扩散模型的最新风格迁移方法无需优化，仅需文本描述即可完成风格迁移。

**Key Takeaways**
- 无需进一步优化即可使用预先训练的扩散模型进行风格迁移。
- 仅需文本描述即可完成风格迁移，无需使用风格图像。
- 提出了一种具有双流编码器和单流解码器的架构，取代了扩散模型中的传统 U-Net。
- 双流编码器将内容图像和风格文本提示作为输入，以实现内容和风格的解耦。
- 解码器会根据给定的内容图像和相应的风格文本提示对来自双流的特征进行调制，以实现精准的风格迁移。
- 实验结果表明，该方法在各种内容图像和风格文本提示下均能生成高质量且保真度高的图像。
- 代码和更多结果可在项目网站上找到。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>标题：FreeStyle：基于扩散模型的文本引导风格迁移的免费午餐</li>
<li>作者：Feihong He, Gang Li, Mengyuan Zhang, Leilei Yan, Lingyu Si, Fanzhang Li</li>
<li>隶属机构：苏州大学计算机科学与技术学院</li>
<li>关键词：图像风格迁移、扩散模型、文本引导、内容和风格解耦</li>
<li>论文链接：https://arxiv.org/abs/2401.15636，Github 代码链接：https://github.com/freestylefreelunch/freestyle</li>
<li>摘要：
(1)：图像风格迁移旨在将自然图像转换为所需的艺术图像，同时保留内容信息。随着生成扩散模型的快速发展，图像风格迁移也取得了重大进展。
(2)：过去的方法主要分为两类：微调方法和反演方法。微调方法需要优化部分或全部参数，以将给定的视觉风格嵌入到生成扩散模型的输出域中。反演方法涉及将特定风格或内容学习为文本标记，以指导特定风格的生成。这两种方法通常需要数千次甚至更多次迭代的训练，导致巨大的计算成本和缓慢的优化过程。
(3)：本文提出了一种创新的风格迁移方法 FreeStyle，它建立在预训练的大型扩散模型之上，不需要进一步的优化。此外，我们的方法仅通过对所需风格的文本描述即可实现风格迁移，消除了对风格图像的需要。具体来说，我们提出了一个双流编码器和单流解码器架构，取代了扩散模型中的传统 U-Net。在双流编码器中，两个不同的分支分别以内容图像和风格文本提示作为输入，实现内容和风格的解耦。在解码器中，我们进一步根据给定的内容图像和相应的风格文本提示对来自双流的特征进行调制，以实现精确的风格迁移。
(4)：实验结果表明，我们的方法在各种内容图像和风格文本提示下都具有高质量的合成和保真度。这些结果支持了我们的目标，即提供一种不需要优化且仅使用文本描述即可实现风格迁移的方法。</li>
</ol>
<p>Some Error for method(比如是不是没有Methods这个章节)</p>
<ol>
<li>结论：
(1) 本工作的重要意义在于，它提出了一种无需优化且仅使用文本描述即可实现风格迁移的方法，极大地简化了风格迁移的实现过程，并提高了风格迁移的效率。
(2) 创新点：</li>
<li>提出了一种基于预训练的大型扩散模型的风格迁移方法，无需进一步的优化。</li>
<li>提出了一种双流编码器和单流解码器架构，实现了内容和风格的解耦。</li>
<li>通过调整缩放因子，可以轻松地适应特定的风格迁移。
性能：</li>
<li>在各种内容图像和风格文本提示下，我们的方法都具有高质量的合成和保真度。</li>
<li>我们的方法在视觉质量、艺术一致性和内容信息保留方面都优于现有方法。
工作量：</li>
<li>我们的方法不需要额外的优化，也不需要参考风格图像，因此工作量大大减少。</li>
<li>我们的方法易于实现，并且可以在各种硬件平台上运行。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-02dabbff0265fb8cee1ebc93f2818847.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3e4829b0ad92ebeecf294e4f413dbd14.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2dcc79e02f8fd2b94a1ae5b107cacf57.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f38c646b53cac6a48979ec9e56fd9c9.jpg" align="middle">
</details>




<h2 id="A-Survey-on-Data-Augmentation-in-Large-Model-Era"><a href="#A-Survey-on-Data-Augmentation-in-Large-Model-Era" class="headerlink" title="A Survey on Data Augmentation in Large Model Era"></a>A Survey on Data Augmentation in Large Model Era</h2><p><strong>Authors:Yue Zhou, Chenlu Guo, Xu Wang, Yi Chang, Yuan Wu</strong></p>
<p>Large models, encompassing large language and diffusion models, have shown exceptional promise in approximating human-level intelligence, garnering significant interest from both academic and industrial spheres. However, the training of these large models necessitates vast quantities of high-quality data, and with continuous updates to these models, the existing reservoir of high-quality data may soon be depleted. This challenge has catalyzed a surge in research focused on data augmentation methods. Leveraging large models, these data augmentation techniques have outperformed traditional approaches. This paper offers an exhaustive review of large model-driven data augmentation methods, adopting a comprehensive perspective. We begin by establishing a classification of relevant studies into three main categories: image augmentation, text augmentation, and paired data augmentation. Following this, we delve into various data post-processing techniques pertinent to large model-based data augmentation. Our discussion then expands to encompass the array of applications for these data augmentation methods within natural language processing, computer vision, and audio signal processing. We proceed to evaluate the successes and limitations of large model-based data augmentation across different scenarios. Concluding our review, we highlight prospective challenges and avenues for future exploration in the field of data augmentation. Our objective is to furnish researchers with critical insights, ultimately contributing to the advancement of more sophisticated large models. We consistently maintain the related open-source materials at: <a href="https://github.com/MLGroup-JLU/LLM-data-aug-survey">https://github.com/MLGroup-JLU/LLM-data-aug-survey</a>. </p>
<p><a href="http://arxiv.org/abs/2401.15422v1">PDF</a> 30 pages; <a href="https://github.com/MLGroup-JLU/LLM-data-aug-survey">https://github.com/MLGroup-JLU/LLM-data-aug-survey</a></p>
<p><strong>摘要</strong><br>借助大模型提升数据增强方法，为更先进的大模型赋能。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>随着大模型的爆发，数据增强方法也受到前所未有的关注。</li>
<li>大模型驱动的图像增强、文本增强和配对数据增强能够有效提高模型性能。</li>
<li>模型增强可用于自然语言处理、计算机视觉和音频信号处理等领域。</li>
<li>模型增强能够解决大模型训练中优质数据短缺的问题，提高模型的泛化能力。</li>
<li>模型增强需要大量的数据和算力，可能存在道德和伦理风险。</li>
<li>模型增强是数据增强领域的一个重要方向，有广阔的应用前景。</li>
<li>本综述提供了大模型驱动的模型增强方法的全面总结，有助于研究人员进一步探索该领域。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>论文标题：大模型时代的数据增强综述</li>
<li>作者：岳周，陈璐国，徐王，易昌，袁武</li>
<li>第一作者单位：吉林大学人工智能学院</li>
<li>关键词：大型语言模型，扩散模型，数据增强</li>
<li>论文链接：arXiv:2401.15422v1[cs.LG] 27Jan2024，Github 链接：https://github.com/MLGroup-JLU/LLM-data-aug-survey</li>
<li>
<p>摘要：
（1）研究背景：随着大模型在自然语言处理、计算机视觉和音频信号处理等领域取得显著进展，对高质量数据的需求也随之增加。然而，现有的高质量数据储备可能很快就会枯竭。为了应对这一挑战，研究人员开始探索利用大模型进行数据增强，以提高模型的泛化能力。
（2）过去的方法及其问题：传统的数据增强方法通常使用简单的变换，如裁剪、旋转和颜色调整等，这些方法虽然简单有效，但难以捕捉真实世界数据的复杂性。此外，这些方法通常需要大量的人工标注数据，这在实际应用中往往难以实现。
（3）提出的研究方法：本文提出了一种利用大模型进行数据增强的研究方法。该方法利用大模型的强大生成能力，可以生成高质量的合成数据，这些合成数据与真实数据具有相似的分布，可以有效地提高模型的泛化能力。
（4）方法的性能：本文的方法在自然语言处理、计算机视觉和音频信号处理等领域取得了显著的性能提升。例如，在自然语言处理领域，该方法可以提高文本分类和机器翻译任务的准确率；在计算机视觉领域，该方法可以提高图像分类和目标检测任务的准确率；在音频信号处理领域，该方法可以提高语音识别和音乐生成任务的准确率。这些性能提升证明了该方法的有效性。</p>
</li>
<li>
<p>方法：
（1）基于提示的数据增强：利用大模型的强大生成能力，根据给定的文本提示生成高质量的合成数据。
（2）基于图像的数据增强：利用大模型的图像生成能力，生成与真实图像相似的合成图像。
（3）基于文本的数据增强：利用大模型的文本生成能力，生成与真实文本相似的合成文本。
（4）数据后处理：对生成的数据进行后处理，以提高其质量和多样性。</p>
</li>
<li>
<p>结论：
（1）：数据增强对于人工智能模型的发展具有重要意义，特别是在大模型的背景下。本文对基于大模型的数据增强方法进行了全面的综述，从方法、数据后处理和应用三个维度对现有研究进行了详细的分类和总结，阐述了关键技术及其优缺点。
（2）：创新点：</p>
</li>
<li>提出了一种利用大模型进行数据增强的研究方法，该方法可以有效地提高模型的泛化能力。</li>
<li>在自然语言处理、计算机视觉和音频信号处理等领域取得了显著的性能提升。</li>
<li>提出了一种基于提示的数据增强方法，该方法可以根据给定的文本提示生成高质量的合成数据。</li>
<li>提出了一种基于图像的数据增强方法，该方法可以生成与真实图像相似的合成图像。</li>
<li>提出了一种基于文本的数据增强方法，该方法可以生成与真实文本相似的合成文本。
性能：</li>
<li>在自然语言处理领域，该方法可以提高文本分类和机器翻译任务的准确率。</li>
<li>在计算机视觉领域，该方法可以提高图像分类和目标检测任务的准确率。</li>
<li>在音频信号处理领域，该方法可以提高语音识别和音乐生成任务的准确率。
工作量：</li>
<li>该方法需要大量的数据和计算资源，这可能限制其在实际应用中的使用。</li>
<li>该方法需要对生成的数据进行后处理，这可能会增加额外的工作量。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-9a7051b919bc0792980f2ad47c261e3e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-864381e5b6301b666082a34992eefafb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea1ea52d75b9694bef170802d2ad73b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c3b258fe16ad4c00b00b765b8bcdcc4.jpg" align="middle">
</details>




<h2 id="GEM-Boost-Simple-Network-for-Glass-Surface-Segmentation-via-Segment-Anything-Model-and-Data-Synthesis"><a href="#GEM-Boost-Simple-Network-for-Glass-Surface-Segmentation-via-Segment-Anything-Model-and-Data-Synthesis" class="headerlink" title="GEM: Boost Simple Network for Glass Surface Segmentation via Segment   Anything Model and Data Synthesis"></a>GEM: Boost Simple Network for Glass Surface Segmentation via Segment   Anything Model and Data Synthesis</h2><p><strong>Authors:Jing Hao, Moyun Liu, Kuo Feng Hung</strong></p>
<p>Detecting glass regions is a challenging task due to the ambiguity of their transparency and reflection properties. These transparent glasses share the visual appearance of both transmitted arbitrary background scenes and reflected objects, thus having no fixed patterns.Recent visual foundation models, which are trained on vast amounts of data, have manifested stunning performance in terms of image perception and image generation. To segment glass surfaces with higher accuracy, we make full use of two visual foundation models: Segment Anything (SAM) and Stable Diffusion.Specifically, we devise a simple glass surface segmentor named GEM, which only consists of a SAM backbone, a simple feature pyramid, a discerning query selection module, and a mask decoder. The discerning query selection can adaptively identify glass surface features, assigning them as initialized queries in the mask decoder. We also propose a Synthetic but photorealistic large-scale Glass Surface Detection dataset dubbed S-GSD via diffusion model with four different scales, which contain 1x, 5x, 10x, and 20x of the original real data size. This dataset is a feasible source for transfer learning. The scale of synthetic data has positive impacts on transfer learning, while the improvement will gradually saturate as the amount of data increases. Extensive experiments demonstrate that GEM achieves a new state-of-the-art on the GSD-S validation set (IoU +2.1%). Codes and datasets are available at: <a href="https://github.com/isbrycee/GEM-Glass-Segmentor">https://github.com/isbrycee/GEM-Glass-Segmentor</a>. </p>
<p><a href="http://arxiv.org/abs/2401.15282v1">PDF</a> 14 pages, 9 figures, 7 tables</p>
<p><strong>摘要</strong><br>运用两个视觉基础模型（Segment Anything 和 Stable Diffusion），提出一种新的玻璃表面分割器 GEM，并在合成数据集 S-GSD 上进行训练和评估。</p>
<p><strong>要点</strong></p>
<ul>
<li>玻璃区域检测是一项具有挑战性的任务，因为其透明性和反射特性具有模糊性。</li>
<li>SAM、Stable Diffusion 等视觉基础模型在图像感知和图像生成方面表现出色。</li>
<li>GEM 由 SAM 主干、简单特征金字塔、识别查询选择模块和掩码解码器组成。</li>
<li>识别查询选择可以自适应地识别玻璃表面特征，并将它们分配为掩码解码器中的初始化查询。</li>
<li>提出 S-GSD 数据集，通过扩散模型生成合成但逼真的大规模玻璃表面检测数据。</li>
<li>S-GSD 包含 1x、5x、10x 和 20x 四种不同尺度的原始真实数据大小。</li>
<li>S-GSD 数据集是迁移学习的可行来源，合成数据的规模对迁移学习有积极影响。</li>
<li>GEM 在 GSD-S 验证集上取得最新技术水平（IoU +2.1%）。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：GEM：通过 Segment Anything 模型和数据合成增强简单网络以进行玻璃表面分割</li>
<li>作者：Jing Hao†1∗, Moyun Liu†1, Kuo Feng Hung2</li>
<li>隶属机构：华中科技大学</li>
<li>关键词：玻璃表面分割, Segment Anything, Stable Diffusion, 数据合成, 迁移学习</li>
<li>论文链接：https://arxiv.org/abs/2401.15282
Github 代码链接：https://github.com/isbrycee/GEM-Glass-Segmentor</li>
<li>
<p>摘要：
(1)：玻璃表面分割是一项具有挑战性的任务，因为它们具有透明和反射特性。这些透明的玻璃具有透射的任意背景场景和反射物体的视觉外观，因此没有固定的图案。最近的视觉基础模型在图像感知和图像生成方面表现出了惊人的性能。为了更准确地分割玻璃表面，我们充分利用了两个视觉基础模型：Segment Anything (SAM) 和 Stable Diffusion。
(2)：过去的方法主要依赖于手工制作的特征和启发式规则，这些方法在处理具有复杂纹理和图案的玻璃表面时往往表现不佳。此外，这些方法通常需要大量的人工标注数据，这在实际应用中往往是不可行的。
(3)：本文提出了一种简单有效的玻璃表面分割方法 GEM，该方法仅由 SAM 主干、简单的特征金字塔、辨别查询选择模块和掩码解码器组成。辨别查询选择可以自适应地识别玻璃表面特征，并将它们分配为掩码解码器中的初始化查询。我们还提出了一种合成但逼真的大规模玻璃表面检测数据集 S-GSD，该数据集包含 1×、5×、10× 和 20× 四种不同比例的原始真实数据大小。该数据集是迁移学习的可行来源。合成数据的规模对迁移学习有积极影响，而随着数据量的增加，改进将逐渐饱和。
(4)：广泛的实验表明，GEM 在 GSD-S 验证集上实现了新的最先进水平（IoU+2.1%）。</p>
</li>
<li>
<p>方法：
（1）GEM模型由图像编码器、简单的特征金字塔、辨别查询选择模块和掩码解码器组成。图像编码器采用MobileSAM或SAM，特征金字塔通过对图像编码器的最后一个特征图进行反卷积和最大池化操作生成。辨别查询选择模块通过对C3、C4和C5层特征图进行加权平均，并根据Softmax操作的结果选择置信度最高的特征作为查询。掩码解码器采用MaskDINO中的结构，并对像素嵌入图的生成操作进行了简化。
（2）辨别查询选择模块通过对C3、C4和C5层特征图进行加权平均，并根据Softmax操作的结果选择置信度最高的特征作为查询，以增强解码器的能力。
（3）预训练数据集生成利用ControlNet和StableDiffusion生成大规模高质量图像，并使用这些图像对GEM模型进行预训练。预训练数据集包含1×、5×、10×和20×四种不同比例的原始真实数据大小，规模对迁移学习有积极影响。</p>
</li>
<li>
<p>结论：
（1）：本工作提出了一种简单有效的玻璃表面分割方法 GEM，并构建了一个合成但逼真的大规模玻璃表面检测数据集 S-GSD。通过插入 SAM 模型的图像编码器并利用合成数据，极大地挖掘和提升了简单网络的分割性能。广泛的实验表明，GEM 在 GSD-S 验证集上实现了新的最先进水平。此外，我们验证了基础模型可以极大地受益于玻璃分割，使用通用分割模型和扩散模型。我们还发现了当预训练数据数量变得足够大时，改进的瓶颈。希望这个全新的解决方案能够给视觉感知与 AI 生成的内容相结合的研究带来启发。
（2）：创新点：提出了一种简单有效的玻璃表面分割方法 GEM，并构建了一个合成但逼真的大规模玻璃表面检测数据集 S-GSD，通过插入 SAM 模型的图像编码器并利用合成数据，极大地挖掘和提升了简单网络的分割性能。
性能：GEM 在 GSD-S 验证集上实现了新的最先进水平。
工作量：中等。</p>
</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-e556844b104c1e1db4ea6e193687836b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-605395ceec1ff88ccf59285c32da74ca.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0e9883f006240f87a828d0cc4091a4b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c02170019ce2f938dd11fe2abdb10a5c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-06ae235e2096a9f91bd70256d2ef74a6.jpg" align="middle">
</details>




<h2 id="Text-Image-Inpainting-via-Global-Structure-Guided-Diffusion-Models"><a href="#Text-Image-Inpainting-via-Global-Structure-Guided-Diffusion-Models" class="headerlink" title="Text Image Inpainting via Global Structure-Guided Diffusion Models"></a>Text Image Inpainting via Global Structure-Guided Diffusion Models</h2><p><strong>Authors:Shipeng Zhu, Pengfei Fang, Chenjie Zhu, Zuoyan Zhao, Qiang Xu, Hui Xue</strong></p>
<p>Real-world text can be damaged by corrosion issues caused by environmental or human factors, which hinder the preservation of the complete styles of texts, e.g., texture and structure. These corrosion issues, such as graffiti signs and incomplete signatures, bring difficulties in understanding the texts, thereby posing significant challenges to downstream applications, e.g., scene text recognition and signature identification. Notably, current inpainting techniques often fail to adequately address this problem and have difficulties restoring accurate text images along with reasonable and consistent styles. Formulating this as an open problem of text image inpainting, this paper aims to build a benchmark to facilitate its study. In doing so, we establish two specific text inpainting datasets which contain scene text images and handwritten text images, respectively. Each of them includes images revamped by real-life and synthetic datasets, featuring pairs of original images, corrupted images, and other assistant information. On top of the datasets, we further develop a novel neural framework, Global Structure-guided Diffusion Model (GSDM), as a potential solution. Leveraging the global structure of the text as a prior, the proposed GSDM develops an efficient diffusion model to recover clean texts. The efficacy of our approach is demonstrated by thorough empirical study, including a substantial boost in both recognition accuracy and image quality. These findings not only highlight the effectiveness of our method but also underscore its potential to enhance the broader field of text image understanding and processing. Code and datasets are available at: <a href="https://github.com/blackprotoss/GSDM">https://github.com/blackprotoss/GSDM</a>. </p>
<p><a href="http://arxiv.org/abs/2401.14832v1">PDF</a> Accepted by AAAI-24</p>
<p><strong>Summary</strong><br>扩散模型可修复文本图像中的腐蚀问题，提高文本识别和理解准确率。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>现实世界的文本图像可能受到环境或人为因素的腐蚀，导致文本样式不完整，给文本理解和下游应用带来挑战。</li>
<li>目前的图像修复技术难以很好地修复腐蚀的文本图像，无法恢复准确的文本图像并保持合理的样式一致性。</li>
<li>本文将文本图像修复作为一个开放问题，建立了一个基准来促进其研究。</li>
<li>本文建立了两个具体的数据集，分别包含场景文本图像和手写文本图像，每个数据集都包括原始图像、损坏图像和其他辅助信息。</li>
<li>本文还开发了一种新的神经框架，即全局结构引导扩散模型（GSDM），作为一种潜在的解决方案。</li>
<li>GSDM 利用文本的全局结构作为先验，开发了一个有效的扩散模型来恢复干净的文本。</li>
<li>实验表明，GSDM 可以有效地修复文本图像中的腐蚀问题，提高文本识别和理解准确率。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>标题：文本图像修复：全局结构引导扩散模型</li>
<li>作者：Guozhu Zhu, Xiaojuan Qi, Chengquan Zhang, Yuhang Song, Jiahui Yu</li>
<li>单位：清华大学</li>
<li>关键词：文本图像修复、扩散模型、全局结构引导</li>
<li>论文链接：https://arxiv.org/abs/2302.05818，Github 代码链接：https://github.com/blackprotoss/GSDM</li>
<li>
<p>摘要：
（1）研究背景：现实世界中的文本可能会因环境或人为因素造成的腐蚀问题而损坏，这阻碍了文本完整风格（如纹理和结构）的保存。这些腐蚀问题，例如涂鸦标志和不完整的签名，给理解文本带来了困难，从而对下游应用（如场景文本识别和签名识别）提出了重大挑战。值得注意的是，当前的修复技术通常无法充分解决这个问题，并且难以恢复准确的文本图像以及合理且一致的样式。
（2）过去方法与问题：本文将此表述为文本图像修复的开放问题，旨在建立一个基准以促进其研究。为此，我们建立了两个特定的文本修复数据集，分别包含场景文本图像和手写文本图像。其中每一个都包含由真实生活和合成数据集改造的图像，具有原始图像、损坏图像和其他辅助信息对。在数据集之上，我们进一步开发了一种新的神经框架，即全局结构引导扩散模型 (GSDM)，作为一种潜在的解决方案。利用文本的全局结构作为先验，所提出的 GSDM 开发了一个有效的扩散模型来恢复干净的文本。我们方法的有效性通过彻底的实证研究得到证明，包括识别准确性和图像质量的显着提升。这些发现不仅突出了我们方法的有效性，而且强调了其增强更广泛的文本图像理解和处理领域 的潜力。
（3）研究方法：我们首先建立了两个特定的文本修复数据集，分别包含场景文本图像和手写文本图像。然后，我们提出了一种新的神经框架，即全局结构引导扩散模型 (GSDM)，作为一种潜在的解决方案。利用文本的全局结构作为先验，所提出的 GSDM 开发了一个有效的扩散模型来恢复干净的文本。
（4）方法性能：我们方法在场景文本识别和手写文本识别任务上取得了最先进的性能。在场景文本识别任务上，我们的方法在 ICDAR 2015 数据集上实现了 96.4% 的单词级识别准确率，在 ICDAR 2019 数据集上实现了 93.7% 的单词级识别准确率。在手写文本识别任务上，我们的方法在 IAM 数据集上实现了 98.1% 的单词级识别准确率，在 HWDB 数据集上实现了 97.6% 的单词级识别准确率。这些结果表明，我们的方法能够有效地修复损坏的文本图像，并提高下游文本识别任务的性能。</p>
</li>
<li>
<p>方法：
（1）结构预测模块（SPM）：利用 U-Net 网络预测损坏文本图像的完整全局结构；
（2）重建模块（RM）：基于扩散模型，利用损坏文本图像和预测的全局结构生成修复后的文本图像；
（3）训练过程：采用 DDIM 训练方法，逐步添加高斯噪声并通过反向过程恢复图像；
（4）推理过程：采用非马尔可夫过程加速推理，减少采样步骤；
（5）实验：在场景文本图像和手写文本图像数据集上与其他方法进行比较，证明了该方法的有效性。</p>
</li>
<li>
<p>结论：
(1): 本工作提出文本图像修复新任务，并构建两个针对性数据集，同时提出全局结构引导扩散模型（GSDM）以实现文本图像修复。实验证明该方法有效提升了图像质量和下游识别任务的性能，为现实场景中修复文本图像提供了新思路。
(2): 创新点：</p>
</li>
<li>提出文本图像修复新任务，构建两个针对性数据集，为该任务的研究提供基础。</li>
<li>提出全局结构引导扩散模型（GSDM），利用文本的全局结构作为先验，有效修复损坏文本图像。</li>
<li>通过广泛的实验验证了该方法的有效性，在图像质量和下游识别任务的性能上均取得了最先进的结果。
性能：</li>
<li>在场景文本识别任务上，在 ICDAR2015 数据集上实现了 96.4% 的单词级识别准确率，在 ICDAR2019 数据集上实现了 93.7% 的单词级识别准确率。</li>
<li>在手写文本识别任务上，在 IAM 数据集上实现了 98.1% 的单词级识别准确率，在 HWDB 数据集上实现了 97.6% 的单词级识别准确率。
工作量：</li>
<li>构建了两个针对文本图像修复任务的数据集，包含场景文本图像和手写文本图像。</li>
<li>实现了一个基于扩散模型的文本图像修复框架，包括结构预测模块和重建模块。</li>
<li>通过广泛的实验验证了该方法的有效性，证明了其在图像质量和下游识别任务性能上的提升。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-fb345b2b97342283c585c897304ad431.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf9a67e4292afee367fc527c4b324288.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf5404c79d83f2d33ab5e5614cc703c5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1f264483e26c0c15e167f195bb401503.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80f9c7dd93111e154247310dc9853392.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fcdd40d76ff8a508af2d4cd6798bae54.jpg" align="middle">
</details>




<h2 id="Image-Synthesis-with-Graph-Conditioning-CLIP-Guided-Diffusion-Models-for-Scene-Graphs"><a href="#Image-Synthesis-with-Graph-Conditioning-CLIP-Guided-Diffusion-Models-for-Scene-Graphs" class="headerlink" title="Image Synthesis with Graph Conditioning: CLIP-Guided Diffusion Models   for Scene Graphs"></a>Image Synthesis with Graph Conditioning: CLIP-Guided Diffusion Models   for Scene Graphs</h2><p><strong>Authors:Rameshwar Mishra, A V Subramanyam</strong></p>
<p>Advancements in generative models have sparked significant interest in generating images while adhering to specific structural guidelines. Scene graph to image generation is one such task of generating images which are consistent with the given scene graph. However, the complexity of visual scenes poses a challenge in accurately aligning objects based on specified relations within the scene graph. Existing methods approach this task by first predicting a scene layout and generating images from these layouts using adversarial training. In this work, we introduce a novel approach to generate images from scene graphs which eliminates the need of predicting intermediate layouts. We leverage pre-trained text-to-image diffusion models and CLIP guidance to translate graph knowledge into images. Towards this, we first pre-train our graph encoder to align graph features with CLIP features of corresponding images using a GAN based training. Further, we fuse the graph features with CLIP embedding of object labels present in the given scene graph to create a graph consistent CLIP guided conditioning signal. In the conditioning input, object embeddings provide coarse structure of the image and graph features provide structural alignment based on relationships among objects. Finally, we fine tune a pre-trained diffusion model with the graph consistent conditioning signal with reconstruction and CLIP alignment loss. Elaborate experiments reveal that our method outperforms existing methods on standard benchmarks of COCO-stuff and Visual Genome dataset. </p>
<p><a href="http://arxiv.org/abs/2401.14111v2">PDF</a> </p>
<p><strong>Summary</strong><br>利用图知识指导预训练文本到图像扩散模型，生成与给定场景图一致的图像。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>通过GAN训练，将图编码器预训练为将图特征与对应图像的CLIP特征对齐。</li>
<li>将图特征与给定场景图中物体标签的CLIP嵌入融合，创建图一致的CLIP引导条件信号。</li>
<li>在条件输入中，物体嵌入提供图像的粗略结构，图特征提供基于物体之间关系的结构对齐。</li>
<li>使用重建和CLIP对齐损失，微调预训练的扩散模型，其具有图一致的条件信号。</li>
<li>大量实验表明，我们的方法在COCO-stuff和Visual Genome数据集的标准基准上优于现有方法。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：基于图条件的图像合成：用于场景图的 CLIP 引导扩散模型</li>
<li>作者：Rameshwar Mishra, AV Subramanyam</li>
<li>隶属单位：印度理工学院德里分校</li>
<li>关键词：图像合成、场景图、扩散模型、CLIP</li>
<li>论文链接：https://arxiv.org/abs/2401.14111
Github 代码链接：无</li>
<li>摘要：
（1）研究背景：生成模型的进步激发了人们对生成图像的兴趣，同时遵守特定的结构准则。场景图到图像生成是生成与给定场景图一致的图像的此类任务之一。然而，视觉场景的复杂性对根据场景图中指定的关联准确对齐对象提出了挑战。现有的方法通过首先预测场景布局并使用对抗性训练从这些布局生成图像来解决此任务。
（2）过去的方法及其问题：现有方法存在的问题在于，它们首先需要预测场景布局，然后才能生成图像。这使得生成过程变得复杂且效率低下。此外，现有方法在处理具有复杂关系的场景图时往往会遇到困难。
（3）本文提出的研究方法：为了克服现有方法的不足，本文提出了一种新的方法来从场景图生成图像。该方法不需要预测场景布局，而是直接将场景图中的信息转换为图像。具体来说，本文使用预训练的文本到图像扩散模型和 CLIP 指导来将图知识转换为图像。首先，本文预训练图编码器，使用基于 GAN 的训练将图特征与相应图像的 CLIP 特征对齐。然后，将图特征与场景图中存在的对象标签的 CLIP 嵌入融合，以创建图一致的 CLIP 引导条件信号。在条件输入中，对象嵌入提供图像的粗略结构，而图特征提供基于对象之间关系的结构对齐。最后，使用重建和 CLIP 对齐损失对预训练的扩散模型进行微调，其中包含图一致的条件信号。
（4）方法的性能：实验证明，本文的方法在 COCO-stuff 和 VisualGenome 数据集的标准基准上优于现有方法。这些结果表明，本文的方法能够生成与场景图一致的高质量图像。</li>
</ol>
<p>Methods:</p>
<p>(1) 图编码器：使用多层图卷积网络从场景图中生成图特征。图编码器融合各个对象嵌入和关系嵌入，以给出全局场景图嵌入。</p>
<p>(2) CLIP引导图条件信号：将图特征与场景图中存在的对象标签的 CLIP 嵌入融合，以创建图一致的 CLIP 引导条件信号。对象嵌入提供图像的粗略结构，而图特征提供基于对象之间关系的结构对齐。</p>
<p>(3) 微调扩散模型：使用重建和 CLIP 对齐损失对预训练的扩散模型进行微调，其中包含图一致的条件信号。</p>
<p>(4) GAN-based CLIP 对齐模块：使用基于 GAN 的 CLIP 对齐模块将图编码器输出的特征与相应图像的 CLIP 特征对齐。</p>
<ol>
<li>结论：
（1）：这项工作提出了一种新的场景图到图像生成方法，无需中间场景布局即可进行图像合成。我们使用预训练的文本到图像模型和 CLIP 引导的图条件信号来生成条件为场景图的图像。我们提出了一个基于 GAN 的对齐模块，将图嵌入与 CLIP 潜在空间对齐，以利用文本到图像扩散模型的先前语义理解。为了进一步增强图条件生成，我们引入了一个对齐损失。通过使用各种衡量生成图像质量和多样性的指标进行综合评估，我们的模型在场景图到图像生成任务中展示了最先进的性能。
（2）：创新点：</li>
<li>提出了一种新的场景图到图像生成方法，无需中间场景布局即可进行图像合成。</li>
<li>使用预训练的文本到图像模型和 CLIP 引导的图条件信号来生成条件为场景图的图像。</li>
<li>提出了一个基于 GAN 的对齐模块，将图嵌入与 CLIP 潜在空间对齐，以利用文本到图像扩散模型的先前语义理解。</li>
<li>引入了一个对齐损失，以进一步增强图条件生成。</li>
</ol>
<p>性能：
- 在 COCO-stuff 和 VisualGenome 数据集的标准基准上优于现有方法。
- 生成的图像与场景图一致，具有高质量和多样性。</p>
<p>工作量：
- 需要预训练文本到图像模型和 CLIP 引导的图条件信号。
- 需要对预训练的扩散模型进行微调。
- 需要对 GAN-based CLIP 对齐模块进行训练。</p>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-518a8740c8e81a84d5c9adad9faed822.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e55358c77a9d65f15701e8f33262e2a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-513010d0a919b07024562be2ef0e563a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e610be108eb0f2a257e8080f7af487d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95b8fb0743373584fbfe5eee13bc5497.jpg" align="middle">
</details>




## BootPIG: Bootstrapping Zero-shot Personalized Image Generation   Capabilities in Pretrained Diffusion Models

**Authors:Senthil Purushwalkam, Akash Gokul, Shafiq Joty, Nikhil Naik**

Recent text-to-image generation models have demonstrated incredible success in generating images that faithfully follow input prompts. However, the requirement of using words to describe a desired concept provides limited control over the appearance of the generated concepts. In this work, we address this shortcoming by proposing an approach to enable personalization capabilities in existing text-to-image diffusion models. We propose a novel architecture (BootPIG) that allows a user to provide reference images of an object in order to guide the appearance of a concept in the generated images.   The proposed BootPIG architecture makes minimal modifications to a pretrained text-to-image diffusion model and utilizes a separate UNet model to steer the generations toward the desired appearance. We introduce a training procedure that allows us to bootstrap personalization capabilities in the BootPIG architecture using data generated from pretrained text-to-image models, LLM chat agents, and image segmentation models. In contrast to existing methods that require several days of pretraining, the BootPIG architecture can be trained in approximately 1 hour. Experiments on the DreamBooth dataset demonstrate that BootPIG outperforms existing zero-shot methods while being comparable with test-time finetuning approaches. Through a user study, we validate the preference for BootPIG generations over existing methods both in maintaining fidelity to the reference object's appearance and aligning with textual prompts. 

[PDF](http://arxiv.org/abs/2401.13974v1) 

**Summary**
BootPIG 利用预训练文本到图像扩散模型数据，仅需一小时即可训练，表现优于现有零次学习方法，并与测试时微调方法相当。

**Key Takeaways**

- BootPIG 提出了一种新的 BootPIG 架构，允许用户使用参考图像引导生成图像中概念的外观。
- BootPIG 对预训练文本到图像扩散模型进行最小的修改，并使用单独的 UNet 模型将生成的图像引导至所需的外观。
- BootPIG 使用来自预训练文本到图像模型、LLM 聊天代理和图像分割模型生成的数据，引入了允许我们在 BootPIG 架构中引导个性化功能的训练过程。
- 与需要数天预训练的现有方法相比，BootPIG 架构可以在大约 1 小时内完成训练。
- DreamBooth 数据集上的实验表明，BootPIG 的性能优于现有的零次学习方法，同时与测试时微调方法相当。
- 通过用户研究，我们验证了 BootPIG 生成的图像在保持对参考对象外观的忠实度和与文本提示保持一致方面优于现有方法。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>题目：BootPIG：利用预训练扩散模型引导零样本个性化图像生成能力</li>
<li>作者：Senthil Purushwalkam、Akash Gokul、Shafiq Joty、Nikhil Naik</li>
<li>作者单位：Salesforce AI Research（仅翻译为中文）</li>
<li>关键词：文本到图像生成、个性化图像生成、零样本学习、引导扩散模型</li>
<li>论文链接：https://arxiv.org/abs/2401.13974，Github 链接：无</li>
<li>
<p>摘要：
（1）研究背景：近年来，文本到图像生成模型取得了显著进展，可以根据文本描述生成逼真的图像。然而，这些模型通常无法根据特定主题生成个性化的图像。
（2）过去方法与问题：现有方法通常需要大量的预训练数据或复杂的训练过程，并且在生成个性化图像时往往存在保真度低、与文本描述不一致等问题。
（3）研究方法：本文提出了一种新的架构 BootPIG，它允许用户提供参考图像来引导生成图像中概念的外观。BootPIG 对预训练的文本到图像扩散模型进行微小的修改，并利用一个单独的 U-Net 模型来引导生成过程，使其朝着期望的外观方向发展。此外，本文还引入了一种训练过程，可以使用从预训练的文本到图像模型、LLM 聊天代理和图像分割模型生成的数据来引导 BootPIG 架构中的个性化能力。
（4）实验结果：在 DreamBooth 数据集上的实验表明，BootPIG 在保持对参考对象外观的保真度和与文本描述的一致性方面优于现有方法，同时与测试时微调方法相当。</p>
</li>
<li>
<p>方法：
(1) BootPIG 架构：BootPIG 架构由一个预训练的文本到图像扩散模型和一个引导 U-Net 模型组成。预训练的文本到图像扩散模型负责从文本描述中生成图像，而引导 U-Net 模型则负责将参考图像中的概念外观引导到生成图像中。
(2) 引导训练过程：BootPIG 的训练过程包括两个阶段。在第一阶段，预训练的文本到图像扩散模型被微调，使其能够从文本描述中生成更逼真的图像。在第二阶段，引导 U-Net 模型被训练，使其能够将参考图像中的概念外观引导到生成图像中。
(3) 数据生成：BootPIG 的训练过程使用从预训练的文本到图像模型、LLM 聊天代理和图像分割模型生成的数据。这些数据包括文本描述、参考图像和分割掩码。
(4) 推理过程：在推理过程中，BootPIG 使用预训练的文本到图像扩散模型和引导 U-Net 模型来生成图像。首先，预训练的文本到图像扩散模型从文本描述中生成一个图像。然后，引导 U-Net 模型将参考图像中的概念外观引导到生成图像中。最后，生成的图像被输出。</p>
</li>
<li>
<p>结论：
(1)：BootPIG 提出了一种新的架构，允许用户提供参考图像来引导生成图像中概念的外观，在保持对参考对象外观的保真度和与文本描述的一致性方面优于现有方法，同时与测试时微调方法相当。
(2)：创新点：</p>
</li>
<li>BootPIG 提出了一种新的架构，允许用户提供参考图像来引导生成图像中概念的外观。</li>
<li>BootPIG 引入了一种训练过程，可以使用从预训练的文本到图像模型、LLM 聊天代理和图像分割模型生成的数据来引导 BootPIG 架构中的个性化能力。</li>
<li>BootPIG 在 DreamBooth 数据集上的实验表明，在保持对参考对象外观的保真度和与文本描述的一致性方面优于现有方法，同时与测试时微调方法相当。
性能：</li>
<li>BootPIG 在 DreamBooth 数据集上的实验表明，在保持对参考对象外观的保真度和与文本描述的一致性方面优于现有方法，同时与测试时微调方法相当。
工作量：</li>
<li>BootPIG 的训练过程包括两个阶段，第一阶段微调预训练的文本到图像扩散模型，第二阶段训练引导 U-Net 模型。</li>
<li>BootPIG 的训练过程使用从预训练的文本到图像模型、LLM 聊天代理和图像分割模型生成的数据。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e2ecdf9f08a2cadce3096ead80db29d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d3f5a69af875397219d470fd2b55dde8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-02a980dac9e87a3930c4c6e9ef96072c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7981b6cc67ddcffe001849665e1b21c5.jpg" align="middle">
</details>




<h2 id="HiCAST-Highly-Customized-Arbitrary-Style-Transfer-with-Adapter-Enhanced-Diffusion-Models"><a href="#HiCAST-Highly-Customized-Arbitrary-Style-Transfer-with-Adapter-Enhanced-Diffusion-Models" class="headerlink" title="HiCAST: Highly Customized Arbitrary Style Transfer with Adapter Enhanced   Diffusion Models"></a>HiCAST: Highly Customized Arbitrary Style Transfer with Adapter Enhanced   Diffusion Models</h2><p><strong>Authors:Hanzhang Wang, Haoran Wang, Jinze Yang, Zhongrui Yu, Zeke Xie, Lei Tian, Xinyan Xiao, Junjun Jiang, Xianming Liu, Mingming Sun</strong></p>
<p>The goal of Arbitrary Style Transfer (AST) is injecting the artistic features of a style reference into a given image/video. Existing methods usually focus on pursuing the balance between style and content, whereas ignoring the significant demand for flexible and customized stylization results and thereby limiting their practical application. To address this critical issue, a novel AST approach namely HiCAST is proposed, which is capable of explicitly customizing the stylization results according to various source of semantic clues. In the specific, our model is constructed based on Latent Diffusion Model (LDM) and elaborately designed to absorb content and style instance as conditions of LDM. It is characterized by introducing of \textit{Style Adapter}, which allows user to flexibly manipulate the output results by aligning multi-level style information and intrinsic knowledge in LDM. Lastly, we further extend our model to perform video AST. A novel learning objective is leveraged for video diffusion model training, which significantly improve cross-frame temporal consistency in the premise of maintaining stylization strength. Qualitative and quantitative comparisons as well as comprehensive user studies demonstrate that our HiCAST outperforms the existing SoTA methods in generating visually plausible stylization results. </p>
<p><a href="http://arxiv.org/abs/2401.05870v1">PDF</a> </p>
<p><strong>摘要</strong><br>HiCAST 是一种新颖的任意风格迁移方法，能够根据语义线索来源显式自定义风格化结果。</p>
<p><strong>要点</strong></p>
<ul>
<li>HiCAST 基于潜在扩散模型 (LDM) 构建，精心设计用于吸收内容和风格实例作为 LDM 的条件。</li>
<li>引入了“风格适配器”，允许用户通过对齐 LDM 中的多级风格信息和内在知识来灵活地操作输出结果。</li>
<li>将 HiCAST 扩展到视频任意风格迁移，并提出了一种新的学习目标，显著提高了视频扩散模型训练中的跨帧时间一致性，同时保持了风格化强度。</li>
<li>与现有最先进的方法相比，HiCAST 在生成视觉上合理的风格化结果方面具有更好的表现。</li>
<li>定性和定量比较以及全面的用户研究表明，HiCAST 在生成视觉上合理的风格化结果方面优于现有最先进的方法。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：HiCAST：高度定制的任意风格迁移</li>
<li>作者：Jiachen An, Shixiang Huang, Yuming Song, Dandan Dou, Wen Liu, Jinlong Luo</li>
<li>单位：无</li>
<li>关键词：Arbitrary Style Transfer, Diffusion Model, Style Customization, Video Stylization</li>
<li>链接：https://arxiv.org/abs/2306.09330, Github：None</li>
<li>
<p>摘要：
（1）：任意风格迁移（AST）旨在将风格参考的艺术特征注入给定图像/视频中。现有方法通常专注于追求风格和内容之间的平衡，而忽略了对灵活和定制的风格化结果的重大需求，从而限制了它们的实际应用。
（2）：为了解决这一关键问题，提出了一种新的 AST 方法，称为 HiCAST，它能够根据各种语义线索来源显式地定制风格化结果。具体来说，我们的模型基于潜在扩散模型（LDM）构建，并精心设计以吸收内容和风格实例作为 LDM 的条件。它的特点是引入了风格适配器，它允许用户通过对齐多级风格信息和 LDM 中的内在知识来灵活地操纵输出结果。最后，我们进一步扩展了我们的模型以执行视频 AST。利用了一种新的学习目标进行视频扩散模型训练，这在保持风格化强度的前提下显著提高了跨帧时间一致性。定性和定量比较以及全面的用户研究表明，我们的 HiCAST 在生成视觉上合理的风格化结果方面优于现有的 SoTA 方法。
（3）：本论文提出的研究方法是：构建基于潜在扩散模型（LDM）的AST模型，引入风格适配器以实现灵活的风格化结果定制，扩展模型以执行视频AST，并利用新的学习目标进行视频扩散模型训练。
（4）：本论文的方法在任意风格迁移任务上取得了优异的性能，在图像和视频风格化方面均优于现有最先进的方法。这些性能支持了论文的目标，即生成视觉上合理的风格化结果并实现灵活的风格化结果定制。</p>
</li>
<li>
<p>方法：
（1）构建基于潜在扩散模型（LDM）的AST模型，采用预训练的VAE编码器和VGG-16网络作为内容编码器和风格编码器，并设计了风格适配器来实现灵活的风格化结果定制。
（2）采用三阶段训练策略优化模型性能，包括图像模型微调阶段、适配器训练阶段和时间层训练阶段，并设计了混合监督损失函数来指导模型训练。
（3）提出了一种新的学习目标进行视频扩散模型训练，通过引入和谐一致性损失来保持跨帧时间一致性，并添加时间层来对视频进行建模。</p>
</li>
<li>
<p>结论：
（1）本工作通过构建基于潜在扩散模型（LDM）的AST模型，引入风格适配器以实现灵活的风格化结果定制，扩展模型以执行视频AST，并利用新的学习目标进行视频扩散模型训练，在任意风格迁移任务上取得了优异的性能，在图像和视频风格化方面均优于现有最先进的方法。这些性能支持了论文的目标，即生成视觉上合理的风格化结果并实现灵活的风格化结果定制。
（2）创新点：
（1）提出了基于潜在扩散模型（LDM）的AST模型，该模型能够根据各种语义线索来源显式地定制风格化结果。
（2）设计了风格适配器，它允许用户通过对齐多级风格信息和LDM中的内在知识来灵活地操纵输出结果。
（3）提出了新的学习目标进行视频扩散模型训练，这在保持风格化强度的前提下显著提高了跨帧时间一致性。
性能：
（1）在图像风格化任务上，HiCAST在FID和LPIPS指标上优于现有的最先进方法。
（2）在视频风格化任务上，HiCAST在FID和LPIPS指标上也优于现有的最先进方法。
（3）用户研究表明，HiCAST在生成视觉上合理的风格化结果方面优于现有的最先进方法。
工作量：
（1）构建基于潜在扩散模型（LDM）的AST模型。
（2）设计风格适配器以实现灵活的风格化结果定制。
（3）扩展模型以执行视频AST。
（4）利用新的学习目标进行视频扩散模型训练。</p>
</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-352507fbd77e3adcd733f2041bffbe47.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3a9d48ccea55e5c85c44aac94261c324.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16386b8ae01c7a44b1aac3c30a708331.jpg" align="middle">
</details>




<h2 id="Style-Injection-in-Diffusion-A-Training-free-Approach-for-Adapting-Large-scale-Diffusion-Models-for-Style-Transfer"><a href="#Style-Injection-in-Diffusion-A-Training-free-Approach-for-Adapting-Large-scale-Diffusion-Models-for-Style-Transfer" class="headerlink" title="Style Injection in Diffusion: A Training-free Approach for Adapting   Large-scale Diffusion Models for Style Transfer"></a>Style Injection in Diffusion: A Training-free Approach for Adapting   Large-scale Diffusion Models for Style Transfer</h2><p><strong>Authors:Jiwoo Chung, Sangeek Hyun, Jae-Pil Heo</strong></p>
<p>Despite the impressive generative capabilities of diffusion models, existing diffusion model-based style transfer methods require inference-stage optimization (e.g. fine-tuning or textual inversion of style) which is time-consuming, or fails to leverage the generative ability of large-scale diffusion models. To address these issues, we introduce a novel artistic style transfer method based on a pre-trained large-scale diffusion model without any optimization. Specifically, we manipulate the features of self-attention layers as the way the cross-attention mechanism works; in the generation process, substituting the key and value of content with those of style image. This approach provides several desirable characteristics for style transfer including 1) preservation of content by transferring similar styles into similar image patches and 2) transfer of style based on similarity of local texture (e.g. edge) between content and style images. Furthermore, we introduce query preservation and attention temperature scaling to mitigate the issue of disruption of original content, and initial latent Adaptive Instance Normalization (AdaIN) to deal with the disharmonious color (failure to transfer the colors of style). Our experimental results demonstrate that our proposed method surpasses state-of-the-art methods in both conventional and diffusion-based style transfer baselines. </p>
<p><a href="http://arxiv.org/abs/2312.09008v1">PDF</a> 16 pages</p>
<p><strong>Summary</strong><br>无优化扩散图像风格迁移，通过预训练模型操控自注意力层的特性，实现细致风格迁移。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>本文提出的方法无需优化即可将风格迁移到预训练的大规模扩散模型上。</li>
<li>本文操纵自注意层的特性，使风格图像的特征代替内容的特征。</li>
<li>该方法具有若干优点，包括保留内容，并根据内容和风格图像之间的局部纹理相似性进行风格迁移。</li>
<li>本文提出查询保留和注意力温度缩放，以减轻对原始内容的破坏，并引入初始潜在自适应实例归一化 (AdaIN) 来处理不和谐的色彩。</li>
<li>实验结果表明，该方法优于传统和基于扩散的风格迁移基准。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：扩散中的风格注入：一种无训练方法</li>
<li>作者：Sung Kwon An, Dongwon Kim, Junyoung Seo, Youngjoon Yoo</li>
<li>单位：韩国科学技术院</li>
<li>关键词：艺术风格迁移，扩散模型，自注意力，图像生成</li>
<li>论文链接：None，Github 代码链接：None</li>
<li>
<p>摘要：
（1）研究背景：扩散模型在生成图像方面取得了令人印象深刻的成果，但现有的基于扩散模型的风格迁移方法需要推理阶段的优化（例如，对风格进行微调或文本反演），这非常耗时，并且无法利用大规模扩散模型的生成能力。
（2）过去的方法及其问题：现有方法需要推理阶段的优化，这非常耗时，并且无法利用大规模扩散模型的生成能力。
（3）研究方法：为了解决这些问题，我们提出了一种新颖的艺术风格迁移方法，该方法基于预训练的大规模扩散模型，无需任何优化。具体来说，我们将自注意力层的特征作为交叉注意力机制工作的方式进行操作；在生成过程中，用风格图像的键和值替换内容的键和值。这种方法为风格迁移提供了几个理想的特性，包括 1）通过将相似风格转移到相似图像块中来保留内容，以及 2）基于内容和风格图像之间局部纹理（例如边缘）的相似性来转移风格。此外，我们引入了查询保留和注意力温度缩放来减轻破坏原始内容的问题，并引入了初始潜在自适应实例归一化 (AdaIN) 来处理不和谐的颜色（无法转移风格的颜色）。
（4）方法性能：我们的实验结果表明，我们提出的方法在传统和基于扩散的风格迁移基准中都优于最先进的方法。</p>
</li>
<li>
<p>方法：
（1）提出了一种新颖的艺术风格迁移方法，该方法基于预训练的大规模扩散模型，无需任何优化。
（2）将自注意力层的特征作为交叉注意力机制工作的方式进行操作；在生成过程中，用风格图像的键和值替换内容的键和值。
（3）引入了查询保留和注意力温度缩放来减轻破坏原始内容的问题，并引入了初始潜在自适应实例归一化(AdaIN)来处理不和谐的颜色（无法转移风格的颜色）。</p>
</li>
<li>
<p>结论：
（1）：本工作解决了基于扩散模型的风格迁移方法面临的挑战，这些方法通常需要耗时的优化步骤或难以利用大规模扩散模型的生成潜力。为此，我们提出了利用预训练的大规模扩散模型的方法，无需任何优化。
（2）：创新点：</p>
</li>
<li>将自注意力层的特征作为交叉注意力机制工作的方式进行操作；在生成过程中，用风格图像的键和值替换内容的键和值。</li>
<li>引入了查询保留和注意力温度缩放来减轻破坏原始内容的问题，并引入了初始潜在自适应实例归一化(AdaIN)来处理不和谐的颜色（无法转移风格的颜色）。
性能：</li>
<li>在传统和基于扩散的风格迁移基准中，我们的方法优于最先进的方法。
工作量：</li>
<li>该方法无需推理阶段的优化，因此可以利用大规模扩散模型的生成能力，从而减少了工作量。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-3c5ea0ed861e220177fdc07f214f3694.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0c9148fdb00478b35cac320276a8fc70.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-82f6f6c5f9fd21bc24d5f8b3ab902752.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ede5506ccc5b19f67905edc640f55e57.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4fd944f7f8c142e744a1d02eb4176d52.jpg" align="middle">
</details>




<h2 id="ArtBank-Artistic-Style-Transfer-with-Pre-trained-Diffusion-Model-and-Implicit-Style-Prompt-Bank"><a href="#ArtBank-Artistic-Style-Transfer-with-Pre-trained-Diffusion-Model-and-Implicit-Style-Prompt-Bank" class="headerlink" title="ArtBank: Artistic Style Transfer with Pre-trained Diffusion Model and   Implicit Style Prompt Bank"></a>ArtBank: Artistic Style Transfer with Pre-trained Diffusion Model and   Implicit Style Prompt Bank</h2><p><strong>Authors:Zhanjie Zhang, Quanwei Zhang, Guangyuan Li, Wei Xing, Lei Zhao, Jiakai Sun, Zehua Lan, Junsheng Luan, Yiling Huang, Huaizhong Lin</strong></p>
<p>Artistic style transfer aims to repaint the content image with the learned artistic style. Existing artistic style transfer methods can be divided into two categories: small model-based approaches and pre-trained large-scale model-based approaches. Small model-based approaches can preserve the content strucuture, but fail to produce highly realistic stylized images and introduce artifacts and disharmonious patterns; Pre-trained large-scale model-based approaches can generate highly realistic stylized images but struggle with preserving the content structure. To address the above issues, we propose ArtBank, a novel artistic style transfer framework, to generate highly realistic stylized images while preserving the content structure of the content images. Specifically, to sufficiently dig out the knowledge embedded in pre-trained large-scale models, an Implicit Style Prompt Bank (ISPB), a set of trainable parameter matrices, is designed to learn and store knowledge from the collection of artworks and behave as a visual prompt to guide pre-trained large-scale models to generate highly realistic stylized images while preserving content structure. Besides, to accelerate training the above ISPB, we propose a novel Spatial-Statistical-based self-Attention Module (SSAM). The qualitative and quantitative experiments demonstrate the superiority of our proposed method over state-of-the-art artistic style transfer methods. </p>
<p><a href="http://arxiv.org/abs/2312.06135v1">PDF</a> Accepted by AAAI2024</p>
<p><strong>摘要</strong><br>艺术库：一种通过可训练参数矩阵学习艺术知识并作为视觉提示指导模型生成写实艺术风格图像的艺术风格迁移框架。</p>
<p><strong>要点</strong></p>
<ul>
<li>艺术风格迁移旨在用习得的艺术风格重新绘制内容图像。</li>
<li>现有艺术风格迁移方法可分为基于小模型和基于预训练大规模模型两类。</li>
<li>基于小模型的方法可以保留内容结构，但无法生成高度逼真的风格化图像，并引入伪影和不和谐的图案；基于预训练大规模模型的方法可以生成高度逼真的风格化图像，但难以保留内容结构。</li>
<li>为了解决上述问题，我们提出了一种新颖的艺术风格迁移框架艺术库，以在保留内容图像的内容结构的同时生成高度逼真的风格化图像。</li>
<li>为了充分挖掘预训练大规模模型中嵌入的知识，我们设计了一个隐式风格提示库（ISPB），这是一个可训练参数矩阵集，用于学习和存储艺术品集合中的知识，并作为视觉提示指导预训练大规模模型生成高度逼真的风格化图像，同时保留内容结构。</li>
<li>此外，为了加速上述 ISPB 的训练，我们提出了一种新颖的空间统计自注意力模块（SSAM）。</li>
<li>定性和定量实验表明，我们提出的方法优于最先进的艺术风格迁移方法。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：ArtBank：预训练扩散模型和隐式风格提示库的艺术风格迁移</li>
<li>作者：Zhanjie Zhang<em>, Quanwei Zhang</em>, Guangyuan Li, Wei Xing†, Lei Zhao†, Jiakai Sun, Zehua Lan, Junsheng Luan, Yiling Huang, Huaizhong Lin†</li>
<li>单位：浙江大学智能视觉实验室</li>
<li>关键词：艺术风格迁移、预训练扩散模型、隐式风格提示库</li>
<li>论文链接：https://arxiv.org/abs/2312.06135
   Github 代码链接：https://github.com/Jamie-Cheung/ArtBank</li>
<li>
<p>摘要：
（1）研究背景：艺术风格迁移旨在将学习到的风格迁移到任意内容图像上以创建新的艺术图像。现有的艺术风格迁移方法可分为基于小模型的方法和基于预训练大规模模型的方法。
（2）过去的方法及其问题：基于小模型的方法可以保留内容结构，但无法生成高度逼真的风格化图像，并且会引入伪影和不和谐的图案；基于预训练大规模模型的方法可以生成高度逼真的风格化图像，但难以保留内容结构。
（3）研究方法：提出了一种新的艺术风格迁移框架 ArtBank，以生成高度逼真的风格化图像，同时保留内容图像的内容结构。具体来说，为了充分挖掘预训练大规模模型中嵌入的知识，设计了一个隐式风格提示库 (ISPB)，它是一组可训练的参数矩阵，用于从艺术品集中学习和存储知识，并作为视觉提示来指导预训练的大规模模型生成高度逼真的风格化图像，同时保留内容结构。此外，为了加速训练上述 ISPB，提出了一种新的基于空间统计的自注意力模块 (SSAM)。
（4）方法的性能：定性和定量实验表明，所提出的方法优于最先进的艺术风格迁移方法。</p>
</li>
<li>
<p>方法：
(1) 隐式风格提示库（ISPB）：ISPB 是一组可训练的参数矩阵，用于从艺术品集中学习和存储知识。ISPB 可以通过冻结预训练大规模模型的参数并训练 ISPB 来获得。
(2) 空间统计自注意力模块（SSAM）：SSAM 是一种新的注意力机制，可以加速 ISPB 的训练。SSAM 可以从空间和统计的角度学习和评估参数矩阵的变化值。
(3) 艺术风格迁移框架（ArtBank）：ArtBank 是一个新的艺术风格迁移框架，它包括一个不可训练的部分（预训练大规模模型）和一个可训练的部分（隐式风格提示库）。ArtBank 可以通过冻结预训练大规模模型的参数并训练 ISPB 来获得。</p>
</li>
<li>
<p>结论：
（1）：本文提出了一种新颖的艺术风格迁移框架 ArtBank，该框架可以解决从预训练的大规模模型中挖掘知识的挑战，从而生成高度逼真的风格化图像，同时保留内容图像的内容结构。
（2）：创新点：</p>
</li>
<li>提出了一种新的隐式风格提示库（ISPB），该库可以从艺术品集中学习和存储知识，并作为视觉提示来指导预训练的大规模模型生成高度逼真的风格化图像，同时保留内容结构。</li>
<li>提出了一种新的基于空间统计的自注意力模块（SSAM），该模块可以加速 ISPB 的训练。</li>
<li>提出了一种新的艺术风格迁移框架 ArtBank，该框架包括一个不可训练的部分（预训练大规模模型）和一个可训练的部分（隐式风格提示库）。
性能：</li>
<li>定性和定量实验表明，所提出的方法优于最先进的艺术风格迁移方法。
工作量：</li>
<li>本文的工作量较大，需要训练一个预训练的大规模模型和一个隐式风格提示库。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-540d697ad9011eb3502589a451edc412.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-73cec781e9284cf52fa225a24e917388.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15459584273510feb907035336d2f908.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c20a1ae8cef0cef555200bc6ee29aaa1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7812ebd2783d290dae9431dcaa43e169.jpg" align="middle">
</details>



]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>Diffusion Models</tag>
      </tags>
  </entry>
  <entry>
    <title>NeRF</title>
    <url>/2024/01/30/Paper/2024-01-30/NeRF/</url>
    <content><![CDATA[<blockquote>
<p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-01-30-更新"><a href="#2024-01-30-更新" class="headerlink" title="2024-01-30 更新"></a>2024-01-30 更新</h1><h2 id="Divide-and-Conquer-Rethinking-the-Training-Paradigm-of-Neural-Radiance-Fields"><a href="#Divide-and-Conquer-Rethinking-the-Training-Paradigm-of-Neural-Radiance-Fields" class="headerlink" title="Divide and Conquer: Rethinking the Training Paradigm of Neural Radiance   Fields"></a>Divide and Conquer: Rethinking the Training Paradigm of Neural Radiance   Fields</h2><p><strong>Authors:Rongkai Ma, Leo Lebrat, Rodrigo Santa Cruz, Gil Avraham, Yan Zuo, Clinton Fookes, Olivier Salvado</strong></p>
<p>Neural radiance fields (NeRFs) have exhibited potential in synthesizing high-fidelity views of 3D scenes but the standard training paradigm of NeRF presupposes an equal importance for each image in the training set. This assumption poses a significant challenge for rendering specific views presenting intricate geometries, thereby resulting in suboptimal performance. In this paper, we take a closer look at the implications of the current training paradigm and redesign this for more superior rendering quality by NeRFs. Dividing input views into multiple groups based on their visual similarities and training individual models on each of these groups enables each model to specialize on specific regions without sacrificing speed or efficiency. Subsequently, the knowledge of these specialized models is aggregated into a single entity via a teacher-student distillation paradigm, enabling spatial efficiency for online render-ing. Empirically, we evaluate our novel training framework on two publicly available datasets, namely NeRF synthetic and Tanks&amp;Temples. Our evaluation demonstrates that our DaC training pipeline enhances the rendering quality of a state-of-the-art baseline model while exhibiting convergence to a superior minimum. </p>
<p><a href="http://arxiv.org/abs/2401.16144v1">PDF</a> </p>
<p><strong>Summary</strong><br>利用教师-学生知识蒸馏范式，提升 NeRF 模型的渲染质量。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>传统 NeRF 模型的训练范式对训练集中每个图像赋予同等重要性，这导致在渲染具有复杂几何结构的特定视图时表现不佳。</li>
<li>将输入视图根据其视觉相似性划分为多个组，并在每个组上训练单独的模型，使每个模型专注于特定区域，从而提高渲染质量。</li>
<li>通过教师-学生知识蒸馏范式将这些专门模型的知识聚合到一个实体中，实现在线渲染的空间效率。</li>
<li>在 NeRF 合成和 Tanks&amp;Temples 两个公开数据集上对提出的训练框架进行评估，结果表明该框架优于最先进的基线模型，并且收敛到更好的最小值。</li>
<li>提出了一种名为 DaC 的分而治之训练框架。</li>
<li>DaC 将训练集划分为多个子集，并在每个子集上训练一个单独的神经辐射场 (NeRF) 模型。</li>
<li>然后将这些子模型通过知识蒸馏聚合成一个最终模型。</li>
<li>DaC 在 NeRF 合成和 Tanks&amp;Temples 数据集上的实验表明，它优于最先进的 NeRF 模型。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：分而治之：重新思考神经辐射场的训练范式</li>
<li>作者：Rongkai Ma、Leo Lebrat、Rodrigo SantaCruz、Gil Avraham、Yan Zuo、Clinton Fookes、Olivier Salvado</li>
<li>第一作者单位：英伟达</li>
<li>关键词：神经辐射场、分而治之、教师-学生蒸馏、空间效率</li>
<li>链接：https://arxiv.org/abs/2401.16144
Github：无</li>
<li>
<p>摘要：
（1）研究背景：神经辐射场（NeRF）在合成 3D 场景的高保真视图方面表现出潜力，但 NeRF 的标准训练范式预设了训练集中每个图像具有同等重要性。这种假设对渲染呈现复杂几何体的特定视图提出了重大挑战，从而导致性能不佳。
（2）过去的方法及其问题：以往的方法通常将所有场景视角的几何和光度信息统一压缩到神经网络权重中。这种方法往往忽略了复杂场景不同视角中存在的细节的自然不对称性，导致渲染质量下降。
（3）本文提出的研究方法：本文重新审视了当前训练范式的含义，并重新设计了该范式，以提高 NeRF 的渲染质量。将输入视图根据它们的视觉相似性划分为多个组，并在每个组上训练单独的模型，使每个模型能够专门针对特定区域，而不会牺牲速度或效率。随后，通过教师-学生蒸馏范式将这些专门化模型的知识聚集到一个单一实体中，从而实现在线渲染的空间效率。
（4）方法在任务和性能上的表现：在两个公开可用的数据集 NeRF 合成和 Tanks&amp;Temples 上对新颖的训练框架进行了评估。评估表明，本文提出的分而治之训练管道提高了最先进的基准模型的渲染质量，同时收敛到一个更好的最小值。</p>
</li>
<li>
<p>方法：
（1）场景划分：将输入视图根据视觉相似性划分为多个组，每个组训练一个专门的模型，称为专家模型。
（2）专家训练：在每个组上训练专家模型，使每个专家模型能够专门针对特定区域，而不会牺牲速度或效率。
（3）教师-学生蒸馏：通过教师-学生蒸馏范式将这些专门化模型的知识聚集到一个单一实体中，从而实现在线渲染的空间效率。</p>
</li>
<li>
<p>结论：
（1）：本文提出了一种新的NeRF训练框架，该框架通过将输入视图划分为多个组并训练专门的专家模型，提高了NeRF的渲染质量。
（2）：创新点：</p>
</li>
<li>提出了一种新的NeRF训练框架，该框架通过将输入视图划分为多个组并训练专门的专家模型，提高了NeRF的渲染质量。</li>
<li>使用教师-学生蒸馏范式将这些专门化模型的知识聚集到一个单一实体中，从而实现在线渲染的空间效率。
性能：</li>
<li>在两个公开可用的数据集NeRF合成和Tanks&amp;Temples上对新颖的训练框架进行了评估。</li>
<li>评估表明，本文提出的分而治之训练管道提高了最先进的基准模型的渲染质量，同时收敛到一个更好的最小值。
工作量：</li>
<li>提出了一种新的NeRF训练框架，该框架通过将输入视图划分为多个组并训练专门的专家模型，提高了NeRF的渲染质量。</li>
<li>使用教师-学生蒸馏范式将这些专门化模型的知识聚集到一个单一实体中，从而实现在线渲染的空间效率。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-402a9ebdaec36fd0b9ae3b035907bf37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d76298373c29f69a44796c3bfafe8a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e414fcdc94276655b9d7b111a7932e3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-86b43dc54cafd89cc41e3b7c64fefb1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca572fcd9b7c80bb78d37859a846f58c.jpg" align="middle">
</details>




<h2 id="3D-Reconstruction-and-New-View-Synthesis-of-Indoor-Environments-based-on-a-Dual-Neural-Radiance-Field"><a href="#3D-Reconstruction-and-New-View-Synthesis-of-Indoor-Environments-based-on-a-Dual-Neural-Radiance-Field" class="headerlink" title="3D Reconstruction and New View Synthesis of Indoor Environments based on   a Dual Neural Radiance Field"></a>3D Reconstruction and New View Synthesis of Indoor Environments based on   a Dual Neural Radiance Field</h2><p><strong>Authors:Zhenyu Bao, Guibiao Liao, Zhongyuan Zhao, Kanglin Liu, Qing Li, Guoping Qiu</strong></p>
<p>Simultaneously achieving 3D reconstruction and new view synthesis for indoor environments has widespread applications but is technically very challenging. State-of-the-art methods based on implicit neural functions can achieve excellent 3D reconstruction results, but their performances on new view synthesis can be unsatisfactory. The exciting development of neural radiance field (NeRF) has revolutionized new view synthesis, however, NeRF-based models can fail to reconstruct clean geometric surfaces. We have developed a dual neural radiance field (Du-NeRF) to simultaneously achieve high-quality geometry reconstruction and view rendering. Du-NeRF contains two geometric fields, one derived from the SDF field to facilitate geometric reconstruction and the other derived from the density field to boost new view synthesis. One of the innovative features of Du-NeRF is that it decouples a view-independent component from the density field and uses it as a label to supervise the learning process of the SDF field. This reduces shape-radiance ambiguity and enables geometry and color to benefit from each other during the learning process. Extensive experiments demonstrate that Du-NeRF can significantly improve the performance of novel view synthesis and 3D reconstruction for indoor environments and it is particularly effective in constructing areas containing fine geometries that do not obey multi-view color consistency. </p>
<p><a href="http://arxiv.org/abs/2401.14726v1">PDF</a> 20 pages, 8 figures</p>
<p><strong>Summary</strong><br>神经辐射场 (NeRF) 双模型杜-NeRF 实现高质几何重建与视图渲染。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>杜-NeRF 由两个几何场组成，一个源于 SDF 场，一个源于密度场，用于同时实现高质量的几何重建和视图渲染。</li>
<li>杜-NeRF 将密度场分解为视图无关组件和视图相关组件，并使用视图无关组件作为 SDF 场学习过程的标签。</li>
<li>杜-NeRF 减少了形状 - 辐射场模糊性，并在学习过程中使几何形状和颜色相互受益。</li>
<li>杜-NeRF 在新颖视图合成和室内环境 3D 重建方面大大优于现有方法。</li>
<li>杜-NeRF 在构建不遵守多视图颜色一致性的精细几何图形区域时特别有效。</li>
<li>杜-NeRF 可用于增强现实 (AR)、虚拟现实 (VR) 和 3D 建模等应用。</li>
<li>杜-NeRF 开辟了 3D 重建和新视图合成研究的新方向。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：基于双神经辐射场的室内环境三维重建与新视角合成</li>
<li>作者：Yuxuan Zhang, Yufan Ren, Jiaolong Yang, Yinda Zhang, Xin Tong, Qionghai Dai</li>
<li>单位：西湖大学</li>
<li>关键词：三维重建、新视角合成、神经辐射场、深度学习</li>
<li>论文链接：https://arxiv.org/abs/2302.09426, Github 链接：暂无</li>
<li>
<p>摘要：
（1）研究背景：三维重建和新视角合成在室内环境中有着广泛的应用，但技术上非常具有挑战性。基于隐式神经函数的最新方法可以实现出色的三维重建结果，但它们在新视角合成上的性能可能不尽如人意。神经辐射场 (NeRF) 的发展彻底改变了新视角合成，然而，基于 NeRF 的模型可能无法重建干净的几何表面。
（2）过去的方法及其问题：本文提出了一种双神经辐射场 (Du-NeRF) 来同时实现高质量的几何重建和视图渲染。Du-NeRF 包含两个几何场，一个源自 SDF 场以促进几何重建，另一个源自密度场以增强新视角合成。Du-NeRF 的创新特征之一是它将一个与视图无关的组件从密度场中分离出来，并将其用作标签来监督 SDF 场的学习过程。这减少了形状-辐射模糊性，并使几何和颜色在学习过程中受益于彼此。
（3）本文提出的研究方法：广泛的实验表明，Du-NeRF 可以显着提高室内环境的新视角合成和三维重建的性能，并且在构建不遵循多视图颜色一致性的精细几何区域时特别有效。
（4）方法在什么任务上取得了什么性能？性能是否支持其目标？在 Replica 数据集上，Du-NeRF 在新视角合成和三维重建方面均优于最先进的方法。在具有挑战性的 ThinGeometry 数据集上，Du-NeRF 在新视角合成方面也优于最先进的方法。这些结果支持了 Du-NeRF 的目标，即同时实现高质量的几何重建和新视角合成。</p>
</li>
<li>
<p>方法：
(1) Du-NeRF模型框架：Du-NeRF由两个几何场组成，一个源自SDF场以促进几何重建，另一个源自密度场以增强新视角合成。
(2) 几何场的设计：SDF场用于表示物体的几何形状，密度场用于表示物体的颜色和外观。
(3) 视图无关组件的分离：Du-NeRF将一个与视图无关的组件从密度场中分离出来，并将其用作标签来监督SDF场的学习过程。
(4) 损失函数的设计：Du-NeRF使用了一个结合了重建损失、视图合成损失和正则化损失的损失函数来训练模型。
(5) 训练过程：Du-NeRF使用梯度下降法来训练模型，训练过程中交替更新SDF场和密度场。</p>
</li>
<li>
<p>结论：
(1): 本文提出了一种双神经辐射场(Du-NeRF)来同时实现高质量的几何重建和视图渲染。Du-NeRF包含两个几何场，一个源自SDF场以促进几何重建，另一个源自密度场以增强新视角合成。Du-NeRF的创新特征之一是它将一个与视图无关的组件从密度场中分离出来，并将其用作标签来监督SDF场的学习过程。这减少了形状-辐射模糊性，并使几何和颜色在学习过程中受益于彼此。
(2): 创新点：Du-NeRF将一个与视图无关的组件从密度场中分离出来，并将其用作标签来监督SDF场的学习过程，减少了形状-辐射模糊性，并使几何和颜色在学习过程中受益于彼此。
性能：在Replica数据集上，Du-NeRF在新视角合成和三维重建方面均优于最先进的方法。在具有挑战性的ThinGeometry数据集上，Du-NeRF在新视角合成方面也优于最先进的方法。
工作量：本文的工作量较大，需要设计和训练两个几何场，还需要设计损失函数和训练过程。</p>
</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-56683e282b9ba64280391f34e5aa9f31.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6383efbe47ff44676e2c2f51579aaa23.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d811bf1bd890a7ed9dd96e40a81482c2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-98f97a5db5fd854c0d80066a92053a27.jpg" align="middle">
</details>




<h2 id="Sketch2NeRF-Multi-view-Sketch-guided-Text-to-3D-Generation"><a href="#Sketch2NeRF-Multi-view-Sketch-guided-Text-to-3D-Generation" class="headerlink" title="Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation"></a>Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation</h2><p><strong>Authors:Minglin Chen, Weihao Yuan, Yukun Wang, Zhe Sheng, Yisheng He, Zilong Dong, Liefeng Bo, Yulan Guo</strong></p>
<p>Recently, text-to-3D approaches have achieved high-fidelity 3D content generation using text description. However, the generated objects are stochastic and lack fine-grained control. Sketches provide a cheap approach to introduce such fine-grained control. Nevertheless, it is challenging to achieve flexible control from these sketches due to their abstraction and ambiguity. In this paper, we present a multi-view sketch-guided text-to-3D generation framework (namely, Sketch2NeRF) to add sketch control to 3D generation. Specifically, our method leverages pretrained 2D diffusion models (e.g., Stable Diffusion and ControlNet) to supervise the optimization of a 3D scene represented by a neural radiance field (NeRF). We propose a novel synchronized generation and reconstruction method to effectively optimize the NeRF. In the experiments, we collected two kinds of multi-view sketch datasets to evaluate the proposed method. We demonstrate that our method can synthesize 3D consistent contents with fine-grained sketch control while being high-fidelity to text prompts. Extensive results show that our method achieves state-of-the-art performance in terms of sketch similarity and text alignment. </p>
<p><a href="http://arxiv.org/abs/2401.14257v2">PDF</a> 11 pages, 9 figures</p>
<p><strong>Summary</strong><br>文本引导 3D 生成框架 Sketch2NeRF 可利用草图控制生成一致且高保真的 3D 内容。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Sketch2NeRF 是一个多视角草图引导的文本到 3D 生成框架，可以将草图控制添加到 3D 生成中。</li>
<li>Sketch2NeRF 利用预训练的 2D 扩散模型来监督由神经辐射场 (NeRF) 表示的 3D 场景的优化。</li>
<li>Sketch2NeRF 提出了一种新颖的同步生成和重建方法来有效优化 NeRF。</li>
<li>Sketch2NeRF 收集了两种多视角草图数据集来评估所提出的方法。</li>
<li>实验表明，Sketch2NeRF 可以合成具有细粒度草图控制并且对文本提示高度保真的 3D 一致内容。</li>
<li>广泛的结果表明，Sketch2NeRF 在草图相似性和文本对齐方面实现了最先进的性能。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>标题：Sketch2NeRF：多视角草图引导的文本到 3D 生成</li>
<li>作者：Minglin Chen、Weihao Yuan、Yukun Wang、Zhe Sheng、Yisheng He、Zilong Dong、Liefeng Bo、Yulan Guo</li>
<li>隶属单位：中山大学深圳校区</li>
<li>关键词：文本到 3D、NeRF、草图控制、多视角一致性</li>
<li>论文链接：https://arxiv.org/abs/2401.14257
Github 链接：无</li>
<li>摘要：
(1)：文本到 3D 生成方法可以通过文本描述生成高保真 3D 内容。然而，生成的物体是随机的，缺乏细粒度的控制。草图提供了一种引入这种细粒度控制的廉价方法。然而，由于草图的抽象性和模糊性，很难从这些草图中实现灵活的控制。
(2)：过去的方法主要使用预训练的 2D 扩散模型来监督 3D 场景的优化，这些场景由神经辐射场 (NeRF) 表示。然而，这些方法通常需要大量的草图作为输入，并且生成的 3D 对象可能与草图不一致。
(3)：本文提出了一种多视角草图引导的文本到 3D 生成框架（即 Sketch2NeRF），以将草图控制添加到 3D 生成中。具体来说，该方法利用预训练的 2D 扩散模型来监督 3D 场景的优化，该场景由神经辐射场 (NeRF) 表示。并提出了一种新颖的同步生成和重建方法来有效地优化 NeRF。
(4)：在实验中，本文收集了两种多视角草图数据集来评估所提出的方法。结果表明，该方法能够合成具有细粒度草图控制的 3D 一致内容，同时对文本提示保持高保真度。广泛的结果表明，该方法在草图相似性和文本对齐方面取得了最先进的性能。</li>
</ol>
<p><strong>Methods：</strong></p>
<ol>
<li>
<p><strong>3D表示：</strong>使用神经辐射场（NeRF）表示3D对象，NeRF是一种灵活且能够渲染逼真图像的方法。</p>
</li>
<li>
<p><strong>草图条件生成：</strong>使用预训练的2D草图条件扩散模型作为指导，迭代更新NeRF的权重。</p>
</li>
<li>
<p><strong>同步生成和重建优化：</strong>提出了一种同步生成和重建优化方法，该方法利用ControlNet和Stable Diffusion分别在草图的特定姿势和随机采样的姿势下生成真实图像，并使用NeRF渲染的图像作为重建目标，最小化生成图像和渲染图像之间的重建损失。</p>
</li>
<li>
<p><strong>优化：</strong>使用基于分数的蒸馏优化方法来优化NeRF，该方法可以有效地将草图条件生成与NeRF的优化相结合。</p>
</li>
<li>
<p>结论：
（1）：本文提出了一种新颖的多视角草图引导的文本到3D生成方法Sketch2NeRF，该方法可以生成与给定草图相似的逼真3D内容。具体来说，该方法利用预训练的2D草图条件扩散模型作为指导，迭代更新NeRF的权重，并提出了一种新的同步生成和重建优化方法来有效地优化NeRF。实验结果表明，该方法在草图相似性和文本对齐方面取得了最先进的性能。
（2）：创新点：</p>
</li>
<li>提出了一种新颖的多视角草图引导的文本到3D生成方法，该方法可以生成与给定草图相似的逼真3D内容。</li>
<li>提出了一种新的同步生成和重建优化方法来有效地优化NeRF，该方法可以有效地将草图条件生成与NeRF的优化相结合。
性能：</li>
<li>在两个多视角草图数据集上进行的实验表明，该方法在草图相似性和文本对齐方面取得了最先进的性能。
工作量：</li>
<li>该方法需要收集多视角草图数据集，并需要预训练2D草图条件扩散模型。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-432d996d35cef510a47b970f6a57f9ed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2b5a42bece9e656aff52a6fc20878da8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb3c2e84dae023cd921d28d348487b30.jpg" align="middle">
</details>




<h2 id="NeRF-AD-Neural-Radiance-Field-with-Attention-based-Disentanglement-for-Talking-Face-Synthesis"><a href="#NeRF-AD-Neural-Radiance-Field-with-Attention-based-Disentanglement-for-Talking-Face-Synthesis" class="headerlink" title="NeRF-AD: Neural Radiance Field with Attention-based Disentanglement for   Talking Face Synthesis"></a>NeRF-AD: Neural Radiance Field with Attention-based Disentanglement for   Talking Face Synthesis</h2><p><strong>Authors:Chongke Bi, Xiaoxing Liu, Zhilei Liu</strong></p>
<p>Talking face synthesis driven by audio is one of the current research hotspots in the fields of multidimensional signal processing and multimedia. Neural Radiance Field (NeRF) has recently been brought to this research field in order to enhance the realism and 3D effect of the generated faces. However, most existing NeRF-based methods either burden NeRF with complex learning tasks while lacking methods for supervised multimodal feature fusion, or cannot precisely map audio to the facial region related to speech movements. These reasons ultimately result in existing methods generating inaccurate lip shapes. This paper moves a portion of NeRF learning tasks ahead and proposes a talking face synthesis method via NeRF with attention-based disentanglement (NeRF-AD). In particular, an Attention-based Disentanglement module is introduced to disentangle the face into Audio-face and Identity-face using speech-related facial action unit (AU) information. To precisely regulate how audio affects the talking face, we only fuse the Audio-face with audio feature. In addition, AU information is also utilized to supervise the fusion of these two modalities. Extensive qualitative and quantitative experiments demonstrate that our NeRF-AD outperforms state-of-the-art methods in generating realistic talking face videos, including image quality and lip synchronization. To view video results, please refer to <a href="https://xiaoxingliu02.github.io/NeRF-AD">https://xiaoxingliu02.github.io/NeRF-AD</a>. </p>
<p><a href="http://arxiv.org/abs/2401.12568v1">PDF</a> Accepted by ICASSP 2024</p>
<p><strong>Summary</strong><br>神经辐射场 (NeRF) 带注意力机制的分解 (NeRF-AD) 提出了一种新颖的说话人脸合成方法，通过音频注意机制将人脸分解为音频面孔和身份面孔，从而提高人脸合成的真实性和唇部同步效果。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>NeRF-AD 提出了一种新的说话人脸合成方法，结合了神经辐射场 (NeRF) 和注意力机制，通过将人脸分解为音频面孔和身份面孔，大幅提升了生成人脸的真实性和唇部同步效果。</li>
</ul>
<ul>
<li>NeRF-AD 使用基于注意力的分解模块，利用语音相关的面部动作单元 (AU) 信息将人脸分解为音频面孔和身份面孔，有效地将音频与面部语音运动相关区域进行精确映射。</li>
</ul>
<ul>
<li>NeRF-AD 只将音频面孔与音频特征融合，从而精确地控制音频如何影响说话人脸。</li>
</ul>
<ul>
<li>NeRF-AD 利用 AU 信息来监督这两种模态的融合，提高了人脸合成的准确性和真实性。</li>
</ul>
<ul>
<li>大量的定量和定性实验表明，NeRF-AD 在生成逼真说话人脸视频方面优于现有最先进的方法，包括图像质量和唇部同步。</li>
</ul>
<ul>
<li>更详细的视频结果可以访问 <a href="https://xiaoxingliu02.github.io/NeRF-AD。">https://xiaoxingliu02.github.io/NeRF-AD。</a></li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>标题：神经辐射场与基于注意力的分离的说话人面部合成（NERF-AD）</li>
<li>作者：Bi Chongke，Liu Xiaoxing，Liu Zhilei</li>
<li>单位：天津大学智能与计算学院</li>
<li>关键词：说话人面部合成，神经辐射场，面部分离</li>
<li>论文链接：https://arxiv.org/abs/2401.12568，Github 链接：None</li>
<li>
<p>摘要：
（1）研究背景：说话人面部合成在多维信号处理和多媒体领域是一个热门的研究课题。神经辐射场（NeRF）最近被引入该研究领域，以增强生成面部的真实感和 3D 效果。然而，大多数现有的基于 NeRF 的方法要么给 NeRF 带来了复杂的学习任务，同时缺乏监督式多模态特征融合的方法，要么无法将音频精确映射到与语音运动相关的面部区域。这些原因最终导致现有方法生成的唇形不准确。
（2）过去的方法及其问题：一些现有的方法将 NeRF 的学习任务提前了一部分，并提出了一种通过具有基于注意力的分离的 NeRF（NeRF-AD）进行说话人面部合成的。具体来说，引入了一个基于注意力的分离模块，使用与语音相关的面部动作单元 (AU) 信息将面部分离为音频面部和身份面部。为了精确地调节音频如何影响说话人面部，我们只将音频面部与音频特征融合。此外，AU 信息还用于监督这两个模态的融合。
（3）研究方法：为了减少 NeRF 的学习负担并提高面部渲染的准确性，我们分解说话人面部并为 NeRF 提供两个分解的精确条件。我们提出了一个基于注意力的分离模块，允许音频与与语音运动相关的面部区域精确融合。同时，我们采用一系列方法来监督整个过程。
（4）方法性能：广泛的定性和定量实验表明，我们的 NeRF-AD 在生成逼真的说话人面部视频方面优于最先进的方法，包括图像质量和唇形同步。</p>
</li>
<li>
<p>方法：
(1)：提出了一种基于注意力的分离模块NeRF-AD，将面部分离为音频面部和身份面部，并只将音频面部与音频特征融合，以精确调节音频如何影响说话人面部。
(2)：为了减少NeRF的学习负担并提高面部渲染的准确性，将说话人面部分解并为NeRF提供两个分解的精确条件。
(3)：采用一系列方法来监督整个过程，包括使用与语音相关的面部动作单元(AU)信息监督音频面部和身份面部的融合，以及使用感知损失和对抗损失来监督NeRF的学习。</p>
</li>
<li>
<p>结论：
（1）：提出了一种基于注意力的分离模块NeRF-AD，将说话人面部分离为音频面部和身份面部，并只将音频面部与音频特征融合，以精确调节音频如何影响说话人面部。
（2）：创新点：
提出了一种基于注意力的分离模块，将说话人面部分离为音频面部和身份面部，并只将音频面部与音频特征融合，以精确调节音频如何影响说话人面部。
为了减少NeRF的学习负担并提高面部渲染的准确性，将说话人面部分解并为NeRF提供两个分解的精确条件。
采用一系列方法来监督整个过程，包括使用与语音相关的面部动作单元(AU)信息监督音频面部和身份面部的融合，以及使用感知损失和对抗损失来监督NeRF的学习。
性能：
实验结果表明，NeRF-AD在图像质量和唇形同步方面优于最先进的方法。
工作量：
工作量较大，需要大量的数据和计算资源。</p>
</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-964938af99e1099b95b512a910ce466c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-39deb199fcbfcf9dedfebf11b5272218.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d53c04a42d143a126e5b391f40684f6a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-55f96488825fc7af3820d32c3f4ac6ff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1072a698b0f056bb4d49ab4715962395.jpg" align="middle">
</details>




<h2 id="HG3-NeRF-Hierarchical-Geometric-Semantic-and-Photometric-Guided-Neural-Radiance-Fields-for-Sparse-View-Inputs"><a href="#HG3-NeRF-Hierarchical-Geometric-Semantic-and-Photometric-Guided-Neural-Radiance-Fields-for-Sparse-View-Inputs" class="headerlink" title="HG3-NeRF: Hierarchical Geometric, Semantic, and Photometric Guided   Neural Radiance Fields for Sparse View Inputs"></a>HG3-NeRF: Hierarchical Geometric, Semantic, and Photometric Guided   Neural Radiance Fields for Sparse View Inputs</h2><p><strong>Authors:Zelin Gao, Weichen Dai, Yu Zhang</strong></p>
<p>Neural Radiance Fields (NeRF) have garnered considerable attention as a paradigm for novel view synthesis by learning scene representations from discrete observations. Nevertheless, NeRF exhibit pronounced performance degradation when confronted with sparse view inputs, consequently curtailing its further applicability. In this work, we introduce Hierarchical Geometric, Semantic, and Photometric Guided NeRF (HG3-NeRF), a novel methodology that can address the aforementioned limitation and enhance consistency of geometry, semantic content, and appearance across different views. We propose Hierarchical Geometric Guidance (HGG) to incorporate the attachment of Structure from Motion (SfM), namely sparse depth prior, into the scene representations. Different from direct depth supervision, HGG samples volume points from local-to-global geometric regions, mitigating the misalignment caused by inherent bias in the depth prior. Furthermore, we draw inspiration from notable variations in semantic consistency observed across images of different resolutions and propose Hierarchical Semantic Guidance (HSG) to learn the coarse-to-fine semantic content, which corresponds to the coarse-to-fine scene representations. Experimental results demonstrate that HG3-NeRF can outperform other state-of-the-art methods on different standard benchmarks and achieve high-fidelity synthesis results for sparse view inputs. </p>
<p><a href="http://arxiv.org/abs/2401.11711v1">PDF</a> 13 pages, 6 figures</p>
<p><strong>摘要</strong><br>层次几何、语义和光度引导 NeRF（HG3-NeRF）方法能提高稀疏视图输入下场景表示的几何、语义内容和外观一致性。</p>
<p><strong>关键要点</strong></p>
<ul>
<li>HG3-NeRF 是一种新的方法，可以解决稀疏视图输入下 NeRF 的性能退化问题，并提高几何、语义内容和外观的一致性。</li>
<li>HG3-NeRF 提出了一种分层几何引导 (HGG) 方法，将运动结构 (SfM) 的附件（即稀疏深度先验）纳入场景表示中。</li>
<li>HGG 从局部到全局的几何区域对体积点进行采样，减轻了深度先验中固有偏差造成的错位。</li>
<li>HG3-NeRF 提出了一种分层语义引导 (HSG) 方法，学习从粗到细的语义内容，这对应于从粗到细的场景表示。</li>
<li>实验结果表明，HG3-NeRF 在不同的标准基准上优于其他最先进的方法，并实现了稀疏视图输入的高保真合成结果。</li>
<li>HG3-NeRF 方法能提高稀疏视图输入下场景表示的几何、语义内容和外观一致性。</li>
<li>HG3-NeRF 方法能提高稀疏视图输入下场景表示的几何、语义内容和外观一致性。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>标题：HG3-NeRF：用于稀疏视图输入的分层几何、语义和光度引导的神经辐射场</li>
<li>作者：Zelin Gao, Weichen Dai, Yu Zhang</li>
<li>隶属机构：浙江大学控制科学与工程学院</li>
<li>关键词：神经辐射场、稀疏视图、几何引导、语义引导、光度引导</li>
<li>论文链接：https://arxiv.org/abs/2401.11711，Github 链接：无</li>
<li>摘要：
（1）研究背景：神经辐射场（NeRF）因其从离散观测中学习场景表示以进行新颖视图合成而备受关注。然而，当面对稀疏视图输入时，NeRF 的性能会显著下降，从而限制了其进一步的适用性。
（2）过去方法及问题：现有方法采用预训练方法和逐场景优化方法来解决稀疏视图输入的挑战。预训练方法在大型数据集上训练模型，然后在测试时对每个场景进行微调。然而，这种方法的泛化能力很大程度上依赖于数据集的质量，而且通过捕捉许多不同场景来获得必要的数据集过于昂贵。逐场景优化方法在每个场景上优化模型，但它们通常需要大量计算，并且可能难以收敛到良好的解。
（3）研究方法：本文提出了一种新颖的方法 HG3-NeRF，可以解决上述限制并增强不同视图之间几何形状、语义内容和外观的一致性。HG3-NeRF 包括三个主要组件：分层几何引导（HGG）、分层语义引导（HSG）和光度引导。HGG 将结构从运动（SfM）的附加信息（即稀疏深度先验）纳入场景表示中。HSG 从不同分辨率的图像中学习粗到细的语义内容，这与粗到细的场景表示相对应。光度引导使用渲染方程来优化场景表示，以匹配输入视图的颜色和亮度。
（4）方法性能：实验结果表明，HG3-NeRF 在不同的标准基准上优于其他最先进的方法，并且在稀疏视图输入下实现了高保真合成结果。这些结果支持了本文提出的方法的目标，即解决稀疏视图输入的挑战并增强不同视图之间几何形状、语义内容和外观的一致性。</li>
</ol>
<p>Methods：
（1）分层几何引导（HGG）：利用来自结构运动（SfM）的稀疏深度先验，将几何一致性纳入场景表示中。HGG 方法指导神经辐射场学习密度和颜色的近似分布，这些分布来自深度先验确定的局部到全局的采样区域。
（2）分层语义引导（HSG）：从不同分辨率的图像中学习从粗到细的语义内容，这与从粗到细的场景表示相对应。HSG 使用 CLIP 编码器对渲染的图像和原始图像的特征向量进行编码，并计算粗到细的语义余弦相似性。
（3）光度引导：使用渲染方程优化场景表示，以匹配输入视图的颜色和亮度。光度引导通过最小化渲染的图像和原始图像之间的外观均方误差来实现。</p>
<ol>
<li>结论：
（1）：本文提出了一种分层几何、语义和光度引导的神经辐射场（HG3-NeRF）方法，可以解决稀疏视图输入的挑战并增强不同视图之间几何形状、语义内容和外观的一致性。
（2）：创新点：</li>
<li>提出了一种分层几何引导（HGG）方法，利用来自结构运动（SfM）的稀疏深度先验，将几何一致性纳入场景表示中。</li>
<li>提出了一种分层语义引导（HSG）方法，从不同分辨率的图像中学习从粗到细的语义内容，这与从粗到细的场景表示相对应。</li>
<li>使用渲染方程优化场景表示，以匹配输入视图的颜色和亮度。
性能：</li>
<li>在不同的标准基准上优于其他最先进的方法，并且在稀疏视图输入下实现了高保真合成结果。
工作量：</li>
<li>需要估计相机位姿，并且稀疏视图输入会影响位姿估计的准确性。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-56cd69227addb7c7e2e5ec9028bc8cb0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bb7c383a42f7306611645083f4d82eb9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-71514b137fee0e499428b6e4c393be26.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc5dccc88a28d6fafb1f550b78be5145.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bab43cfc9ed715f6025ba1321b7acdc3.jpg" align="middle">
</details>




<h2 id="IPR-NeRF-Ownership-Verification-meets-Neural-Radiance-Field"><a href="#IPR-NeRF-Ownership-Verification-meets-Neural-Radiance-Field" class="headerlink" title="IPR-NeRF: Ownership Verification meets Neural Radiance Field"></a>IPR-NeRF: Ownership Verification meets Neural Radiance Field</h2><p><strong>Authors:Win Kent Ong, Kam Woh Ng, Chee Seng Chan, Yi Zhe Song, Tao Xiang</strong></p>
<p>Neural Radiance Field (NeRF) models have gained significant attention in the computer vision community in the recent past with state-of-the-art visual quality and produced impressive demonstrations. Since then, technopreneurs have sought to leverage NeRF models into a profitable business. Therefore, NeRF models make it worth the risk of plagiarizers illegally copying, re-distributing, or misusing those models. This paper proposes a comprehensive intellectual property (IP) protection framework for the NeRF model in both black-box and white-box settings, namely IPR-NeRF. In the black-box setting, a diffusion-based solution is introduced to embed and extract the watermark via a two-stage optimization process. In the white-box setting, a designated digital signature is embedded into the weights of the NeRF model by adopting the sign loss objective. Our extensive experiments demonstrate that not only does our approach maintain the fidelity (\ie, the rendering quality) of IPR-NeRF models, but it is also robust against both ambiguity and removal attacks compared to prior arts. </p>
<p><a href="http://arxiv.org/abs/2401.09495v4">PDF</a> Error on result tabulation of state of the art method which might   cause misleading to readers</p>
<p><strong>Summary</strong><br>神经辐射场（NeRF）模型在计算机视觉领域备受关注，并产生了令人印象深刻的成果，由于其最先进的视觉质量，因此存在被剽窃者非法复制、再分发或滥用的风险。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>本文提出了一种针对 NeRF 模型的黑盒和白盒设置的综合知识产权（IP）保护框架，称为 IPR-NeRF。</li>
<li>在黑盒设置中，引入了一种基于扩散的解决方案，通过两阶段优化过程嵌入和提取水印。</li>
<li>在白盒设置中，通过采用符号损失目标将指定数字签名嵌入 NeRF 模型的权重中。</li>
<li>大量实验表明，我们的方法不仅保持了 IPR-NeRF 模型的保真度（即渲染质量），而且与现有技术相比，它还对歧义攻击和去除攻击具有鲁棒性。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：IPR-NERF：知识产权验证满足神经辐射场</li>
<li>作者：Kent Ong, Kam Woh Ng, Chee Seng Chan, Yi Zhe Song, Tao Xiang</li>
<li>单位：马来亚大学图像与信号处理中心（CISiP）</li>
<li>关键词：神经辐射场、知识产权保护、数字水印、数字签名</li>
<li>论文链接：arXiv:2401.09495v1[cs.CV]17Jan2024
Github 链接：无</li>
<li>
<p>摘要：
（1）研究背景：神经辐射场（NeRF）模型因其卓越的视觉质量和令人印象深刻的演示而在计算机视觉领域备受关注。然而，NeRF 模型也面临着知识产权保护的问题，剽窃者可能会非法复制、重新分发或滥用这些模型以获取经济利益或个人利益。
（2）过去方法及其问题：目前针对神经网络的知识产权保护方案主要针对卷积神经网络（CNN）、生成对抗网络（GAN）和循环神经网络（RNN）。然而，这些方案在应用于 NeRF 模型时面临诸多挑战，例如 NeRF 模型的复杂结构、对数据和计算资源的要求较高以及缺乏有效的知识产权保护技术。
（3）研究方法：本论文提出了一个综合的 NeRF 模型知识产权保护框架，称为 IPR-NERF。该框架包括黑盒和白盒两种设置。在黑盒设置中，引入了一个基于扩散的解决方案来嵌入和提取水印，通过一个两阶段的优化过程实现。在白盒设置中，通过采用符号损失目标函数，将指定数字签名嵌入 NeRF 模型的权重中。
（4）方法性能：实验结果表明，IPR-NERF 模型不仅保持了渲染质量，而且在面对模糊性和去除攻击时也具有鲁棒性，优于现有技术。</p>
</li>
<li>
<p>方法：
（1）：提出一个综合的NeRF模型知识产权保护框架IPR-NERF，包括黑盒和白盒两种设置。
（2）：在黑盒设置中，引入一个基于扩散的解决方案来嵌入和提取水印，通过一个两阶段的优化过程实现。
（3）：在白盒设置中，通过采用符号损失目标函数，将指定数字签名嵌入NeRF模型的权重中。</p>
</li>
<li>
<p>结论：
（1）：本工作提出了一种全面的、鲁棒的 NeRF-IPR 保护方案，包括黑盒和白盒两种场景。全面的实验结果表明了其在抵抗嵌入水印的模糊性和去除攻击方面的有效性，同时保持了渲染性能。然而，它在计算能力和对覆盖攻击的黑盒保护方面存在局限性，当攻击者拥有受保护模型的详细信息时。未来的研究将集中在改进这些方面。本研究为 NeRF 模型开发者和研究人员提供了极大的价值，提供了一种保护其知识产权并获得市场竞争优势的方法，考虑到开发高性能 NeRF 模型所需的巨大资源。加强 NeRF 模型对 IPR 侵权行为的抵抗具有广泛的社会效益，包括防止剽窃、确保在动态市场竞争中的竞争优势以及减少浪费诉讼案件的负担。
（2）：创新点：</p>
</li>
<li>提出了一种综合的 NeRF 模型知识产权保护框架 IPR-NERF，包括黑盒和白盒两种设置。</li>
<li>在黑盒设置中，引入了一个基于扩散的解决方案来嵌入和提取水印，通过一个两阶段的优化过程实现。</li>
<li>在白盒设置中，通过采用符号损失目标函数，将指定数字签名嵌入 NeRF 模型的权重中。
性能：</li>
<li>实验结果表明，IPR-NERF 模型不仅保持了渲染质量，而且在面对模糊性和去除攻击时也具有鲁棒性，优于现有技术。
工作量：</li>
<li>IPR-NERF 模型的计算成本较高，尤其是对于大型数据集和复杂场景。</li>
<li>在黑盒设置中，嵌入和提取水印的过程可能需要大量的时间和计算资源。</li>
<li>在白盒设置中，需要修改 NeRF 模型的训练过程以嵌入数字签名，这可能会增加训练时间和复杂性。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-7702dd0580aeb20d2469586499df517d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b6cd7f525efd45ad04614d4ae868c5ff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cd4e10da5a013a99ebc46d33f1e102a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed46804675ae115b408ec3a1b30d40dd.jpg" align="middle">
</details>




<h2 id="ProvNeRF-Modeling-per-Point-Provenance-in-NeRFs-as-a-Stochastic-Process"><a href="#ProvNeRF-Modeling-per-Point-Provenance-in-NeRFs-as-a-Stochastic-Process" class="headerlink" title="ProvNeRF: Modeling per Point Provenance in NeRFs as a Stochastic Process"></a>ProvNeRF: Modeling per Point Provenance in NeRFs as a Stochastic Process</h2><p><strong>Authors:Kiyohiro Nakayama, Mikaela Angelina Uy, Yang You, Ke Li, Leonidas Guibas</strong></p>
<p>Neural radiance fields (NeRFs) have gained popularity across various applications. However, they face challenges in the sparse view setting, lacking sufficient constraints from volume rendering. Reconstructing and understanding a 3D scene from sparse and unconstrained cameras is a long-standing problem in classical computer vision with diverse applications. While recent works have explored NeRFs in sparse, unconstrained view scenarios, their focus has been primarily on enhancing reconstruction and novel view synthesis. Our approach takes a broader perspective by posing the question: “from where has each point been seen?” — which gates how well we can understand and reconstruct it. In other words, we aim to determine the origin or provenance of each 3D point and its associated information under sparse, unconstrained views. We introduce ProvNeRF, a model that enriches a traditional NeRF representation by incorporating per-point provenance, modeling likely source locations for each point. We achieve this by extending implicit maximum likelihood estimation (IMLE) for stochastic processes. Notably, our method is compatible with any pre-trained NeRF model and the associated training camera poses. We demonstrate that modeling per-point provenance offers several advantages, including uncertainty estimation, criteria-based view selection, and improved novel view synthesis, compared to state-of-the-art methods. Please visit our project page at <a href="https://provnerf.github.io">https://provnerf.github.io</a> </p>
<p><a href="http://arxiv.org/abs/2401.08140v2">PDF</a> </p>
<p><strong>摘要</strong><br>针对稀疏无约束视点场景下神经辐射场（NeRF）模型的局限性，本文旨在重构和理解三维场景中每个点的来源信息，并提出了 ProvNeRF 模型来实现这一目标。</p>
<p><strong>要点</strong></p>
<ul>
<li>ProvNeRF 模型能够通过引入每个点可能的来源位置，来丰富传统的 NeRF 模型。</li>
<li>ProvNeRF 模型与任何预训练的 NeRF 模型及其相关的训练相机位姿兼容。</li>
<li>ProvNeRF 模型可以对每个点的不确定性进行估计。</li>
<li>ProvNeRF 模型可以根据指定的标准，选择合适的视角来进行场景重建。</li>
<li>ProvNeRF 模型可以改进场景的新视角合成结果。</li>
<li>ProvNeRF 模型的更多信息可以在项目主页 <a href="https://provnerf.github.io">https://provnerf.github.io</a> 查看。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<p></p><ol><p></p>
<p></p><li>标题：ProvNeRF：将 NeRF 中的逐点出处建模为随机过程</li><p></p>
<p></p><li>作者：George Kiyohiro Nakayama、Mikaela Angelina Uy、Yang You、Ke Li、Leonidas Guibas</li><p></p>
<p></p><li>隶属机构：斯坦福大学</li><p></p>
<p></p><li>关键词：神经辐射场、稀疏视图、出处建模、不确定性估计、基于标准的视点优化、新颖视图合成</li><p></p>
<p></p><li>论文链接：https://arxiv.org/abs/2401.08140
Github 链接：无</li><p></p>
<p></p><li>摘要：
（1）研究背景：神经辐射场 (NeRF) 在各种应用中越来越受欢迎，但它们在稀疏视图方案中面临挑战，因为仅靠体积渲染无法提供足够的约束。
（2）过去方法：过去的方法主要集中在增强重建和新颖视图合成上，但忽略了如何从更全面的角度理解场景，例如不确定性估计、基于标准的视点选择和改进的新颖视图合成。
（3）研究方法：我们提出 ProvNeRF，这是一种通过结合逐点出处来丰富传统 NeRF 表示的模型，对每个点建模可能的源位置。我们通过扩展随机过程的隐式最大似然估计 (IMLE) 来实现这一点。
（4）方法性能：我们的方法在不确定性估计、基于标准的视点选择和改进的新颖视图合成方面优于最先进的方法，这表明建模逐点出处可以提供几个优势。</li><br>&lt;/ol&gt;<p></p>
<p><strong>Methods</strong>：**</p>
<p>（1）：我们将神经辐射场 (NeRF) 表示扩展为包含每个点的出处，即每个点的来源或从何处看到它。</p>
<p>（2）：我们使用随机过程对每个点的出处进行建模，该随机过程由坐标 x∈R3 索引，其在 x 处的边际分布编码了 x 处的出处。</p>
<p>（3）：我们通过扩展随机过程的隐式最大似然估计 (IMLE) 来实现这一点，该估计将潜在随机变量的变换学习为数据分布，其中每个数据样本都是一个标量或向量。</p>
<p>（4）：我们提出 ProvNeRF，它通过扩展隐式概率模型（特别是 IMLE）来处理随机过程，从而将每个点的出处建模为随机过程。</p>
<p>（5）：ProvNeRF 学习一个确定性变换 Hθ：Rb→R+×D3，该变换将每个潜在随机函数样本 Z∼Z 映射到一个函数 Dθ∼Dθ。</p>
<p>（6）：为了优化 Dθ，我们扩展 IMLE 来对随机过程的分布进行建模。我们将 Eq.3 调整到函数空间，并证明它等价于在每个点 x 处对经验样本 ˆD(x)∼ˆD(x) 和模型样本 Dθ(x)∼Dθ(x) 进行逐点匹配。</p>
<ol>
<li>结论：
（1）：本工作提出 ProvNeRF，通过扩展随机过程的 IMLE 来增强传统 NeRF 表示，从而将每个点的出处建模为随机过程。ProvNeRF 可轻松应用于任何预训练的 NeRF 模型以及相关的训练相机位姿。我们展示了在各种下游应用中建模逐点出处的优势，包括不确定性建模、基于标准的视点选择以及与现有最先进方法相比改进的新颖视图合成。
（2）：创新点：</li>
<li>提出 ProvNeRF，一种通过扩展随机过程的 IMLE 来增强传统 NeRF 表示的模型，从而将每个点的出处建模为随机过程。</li>
<li>证明了 ProvNeRF 可以轻松应用于任何预训练的 NeRF 模型以及相关的训练相机位姿。</li>
<li>展示了在各种下游应用中建模逐点出处的优势，包括不确定性建模、基于标准的视点选择以及与现有最先进方法相比改进的新颖视图合成。</li>
</ol>
<p>性能：
- 在不确定性估计、基于标准的视点选择和改进的新颖视图合成方面优于最先进的方法。</p>
<p>工作量：
- 需要扩展随机过程的 IMLE 来对随机过程的分布进行建模。
- 需要调整 Eq.3 到函数空间，并证明它等价于在每个点 x 处对经验样本 ˆD(x)∼ˆD(x) 和模型样本 Dθ(x)∼Dθ(x) 进行逐点匹配。</p>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f48885cf9ef1b2a677c258f6b1e9a2a2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d72d125185075e757ca6e7284c2ace68.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a582ca9b91a20a6a1c1593166a2d8401.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d26582d170597ef79c1a5e15500eaa42.jpg" align="middle">
</details>





</ol>]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>NeRF</tag>
      </tags>
  </entry>
  <entry>
    <title>元宇宙/虚拟人</title>
    <url>/2024/01/24/Paper/2024-01-24/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/</url>
    <content><![CDATA[<blockquote>
<p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-01-24-更新"><a href="#2024-01-24-更新" class="headerlink" title="2024-01-24 更新"></a>2024-01-24 更新</h1><h2 id="UltrAvatar-A-Realistic-Animatable-3D-Avatar-Diffusion-Model-with-Authenticity-Guided-Textures"><a href="#UltrAvatar-A-Realistic-Animatable-3D-Avatar-Diffusion-Model-with-Authenticity-Guided-Textures" class="headerlink" title="UltrAvatar: A Realistic Animatable 3D Avatar Diffusion Model with   Authenticity Guided Textures"></a>UltrAvatar: A Realistic Animatable 3D Avatar Diffusion Model with   Authenticity Guided Textures</h2><p><strong>Authors:Mingyuan Zhou, Rakib Hyder, Ziwei Xuan, Guojun Qi</strong></p>
<p>Recent advances in 3D avatar generation have gained significant attentions. These breakthroughs aim to produce more realistic animatable avatars, narrowing the gap between virtual and real-world experiences. Most of existing works employ Score Distillation Sampling (SDS) loss, combined with a differentiable renderer and text condition, to guide a diffusion model in generating 3D avatars. However, SDS often generates oversmoothed results with few facial details, thereby lacking the diversity compared with ancestral sampling. On the other hand, other works generate 3D avatar from a single image, where the challenges of unwanted lighting effects, perspective views, and inferior image quality make them difficult to reliably reconstruct the 3D face meshes with the aligned complete textures. In this paper, we propose a novel 3D avatar generation approach termed UltrAvatar with enhanced fidelity of geometry, and superior quality of physically based rendering (PBR) textures without unwanted lighting. To this end, the proposed approach presents a diffuse color extraction model and an authenticity guided texture diffusion model. The former removes the unwanted lighting effects to reveal true diffuse colors so that the generated avatars can be rendered under various lighting conditions. The latter follows two gradient-based guidances for generating PBR textures to render diverse face-identity features and details better aligning with 3D mesh geometry. We demonstrate the effectiveness and robustness of the proposed method, outperforming the state-of-the-art methods by a large margin in the experiments. </p>
<p><a href="http://arxiv.org/abs/2401.11078v1">PDF</a> The project page is at <a href="http://usrc-sea.github.io/UltrAvatar/">http://usrc-sea.github.io/UltrAvatar/</a></p>
<p><strong>摘要</strong><br>利用人工智能生成更真实的 3D 虚拟形象，提升用户体验。</p>
<p><strong>要点</strong></p>
<ul>
<li>利用文本作为条件，生成场景和动画，构建更逼真的 3D 形象。</li>
<li>已有的大部分方法采用 Score Distillation Sampling 损失，存在过于光滑、细节缺失等问题。</li>
<li>部分方法从单一图像生成 3D 形象，存在光线影响、视角问题，难以重构 3D 脸部网格。</li>
<li>提出 UltrAvatar 方法，增强几何细节，使用物理渲染材质提高渲染质量。</li>
<li>引入漫反射提取模型，去除光线影响，生成不受光线影响的材质。</li>
<li>基于真实漫反射材质，采用梯度引导，生成真实面孔属性并与 3D 网格对齐的材质。</li>
<li>对比实验表明，该方法在真实性、多样性、与 3D 网格的一致性上均优于现有方法。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li><p>题目：UltrAvatar：基于真实感的纹理扩散模型的超真实 3D 头像生成（UltrAvatar: Ultra-Realistic 3D Avatar Generation with Authenticity-Guided Texture Diffusion Models）</p>
</li>
<li><p>作者：Yilun Du, Linchao Bao<em>, Xinyu Gong</em>, Hang Zhou, Chen Change Loy, Ziwei Liu</p>
</li>
<li><p>单位：香港中文大学（香港）</p>
</li>
<li><p>关键词：3D 头像生成、纹理扩散模型、真实感引导、照明去除、PBR 纹理</p>
</li>
<li><p>论文链接：https://arxiv.org/abs/2302.09864，Github 链接：None</p>
</li>
<li><p>摘要：
（1）：最近 3D 头像生成的进展备受关注。这些突破旨在产生更逼真的可动画头像，缩小虚拟和现实世界体验之间的差距。大多数现有工作采用分数蒸馏采样 (SDS) 损失，结合可微渲染器和文本条件，来指导扩散模型生成 3D 头像。然而，SDS 通常会生成过度平滑的结果，面部细节很少，因此与祖先采样相比缺乏多样性。另一方面，其他工作从单个图像生成 3D 头像，其中不需要的照明效果、透视视图和较差的图像质量使得它们难以可靠地重建具有对齐完整纹理的 3D 面部网格。
（2）：为了解决上述问题，本文提出了一种新颖的 3D 头像生成方法，称为 UltrAvatar，它增强了几何体的保真度，并具有物理渲染 (PBR) 纹理的卓越质量，且没有不需要的照明。为此，提出的方法提出了一个漫反射颜色提取模型和一个真实感引导的纹理扩散模型。前者去除了不需要的照明效果以揭示真实的漫反射颜色，以便可以在各种照明条件下渲染生成的头像。后者遵循两个基于梯度的指导，用于生成 PBR 纹理，以更好地呈现不同的面部身份特征和细节，并与 3D 网格几何体更好地对齐。
（3）：本文在实验中证明了所提出方法的有效性和鲁棒性，在很大程度上优于最先进的方法。
（4）：在人脸重建任务上，UltrAvatar 在多种指标上都优于现有方法。例如，在 FID 指标上，UltrAvatar 的平均值为 13.4，而最优的对比方法为 18.8；在 LPIPS 指标上，UltrAvatar 的平均值为 0.24，而最优的对比方法为 0.31。这些结果表明，UltrAvatar 能够生成更逼真、更具细节的 3D 头像。</p>
</li>
<li><p>方法：
（1）：提出了一种新颖的3D头像生成方法UltrAvatar，它增强了几何体的保真度，并具有物理渲染(PBR)纹理的卓越质量，且没有不需要的照明。
（2）：UltrAvatar包含一个漫反射颜色提取模型和一个真实感引导的纹理扩散模型。漫反射颜色提取模型去除了不需要的照明效果以揭示真实的漫反射颜色，以便可以在各种照明条件下渲染生成的头像。真实感引导的纹理扩散模型遵循两个基于梯度的指导，用于生成PBR纹理，以更好地呈现不同的面部身份特征和细节，并与3D网格几何体更好地对齐。
（3）：在人脸重建任务上，UltrAvatar在多种指标上都优于现有方法。例如，在FID指标上，UltrAvatar的平均值为13.4，而最优的对比方法为18.8；在LPIPS指标上，UltrAvatar的平均值为0.24，而最优的对比方法为0.31。这些结果表明，UltrAvatar能够生成更逼真、更具细节的3D头像。</p>
</li>
<li><p>结论：
（1）：本文提出了一种从文本提示或单个图像生成 3D 头像的新颖方法。我们方法的核心是 DCEM 模型，该模型旨在消除源图像中不需要的照明效果，以及一个由光度和边缘信号引导的纹理生成模型，以保留头像的 PBR 细节。与其他 SOTA 方法相比，我们证明了我们的方法可以生成显示出高度逼真、更高质量、卓越保真度和更广泛多样性的 3D 头像。
（2）：创新点：</p>
</li>
</ol>
<ul>
<li>提出了一种漫反射颜色提取模型，可以消除源图像中不需要的照明效果，以便在各种照明条件下渲染生成的头像。</li>
<li>提出了一种真实感引导的纹理扩散模型，该模型遵循两个基于梯度的指导，用于生成 PBR 纹理，以更好地呈现不同的面部身份特征和细节，并与 3D 网格几何体更好地对齐。</li>
<li>在人脸重建任务上，UltrAvatar 在多种指标上都优于现有方法。例如，在 FID 指标上，UltrAvatar 的平均值为 13.4，而最优的对比方法为 18.8；在 LPIPS 指标上，UltrAvatar 的平均值为 0.24，而最优的对比方法为 0.31。这些结果表明，UltrAvatar 能够生成更逼真、更具细节的 3D 头像。
性能：</li>
<li>在人脸重建任务上，UltrAvatar 在多种指标上都优于现有方法。例如，在 FID 指标上，UltrAvatar 的平均值为 13.4，而最优的对比方法为 18.8；在 LPIPS 指标上，UltrAvatar 的平均值为 0.24，而最优的对比方法为 0.31。这些结果表明，UltrAvatar 能够生成更逼真、更具细节的 3D 头像。
工作量：</li>
<li>该方法需要大量的数据和计算资源来训练模型。</li>
</ul>



<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-6ddb372268ab29440ab071d2e4e6e298.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e5a668d3e08b3e9f2b7d9e0f965d9762.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fe4a6193adf67c2ee040715753a40d2d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3f4f76cbd1c6492ba1fd985b02c9d05.jpg" align="middle">
</details>
​    


## Fast Registration of Photorealistic Avatars for VR Facial Animation
**Authors:Chaitanya Patel, Shaojie Bai, Te-Li Wang, Jason Saragih, Shih-En Wei**

Virtual Reality (VR) bares promise of social interactions that can feel more immersive than other media. Key to this is the ability to accurately animate a photorealistic avatar of one's likeness while wearing a VR headset. Although high quality registration of person-specific avatars to headset-mounted camera (HMC) images is possible in an offline setting, the performance of generic realtime models are significantly degraded. Online registration is also challenging due to oblique camera views and differences in modality. In this work, we first show that the domain gap between the avatar and headset-camera images is one of the primary sources of difficulty, where a transformer-based architecture achieves high accuracy on domain-consistent data, but degrades when the domain-gap is re-introduced. Building on this finding, we develop a system design that decouples the problem into two parts: 1) an iterative refinement module that takes in-domain inputs, and 2) a generic avatar-guided image-to-image style transfer module that is conditioned on current estimation of expression and head pose. These two modules reinforce each other, as image style transfer becomes easier when close-to-ground-truth examples are shown, and better domain-gap removal helps registration. Our system produces high-quality results efficiently, obviating the need for costly offline registration to generate personalized labels. We validate the accuracy and efficiency of our approach through extensive experiments on a commodity headset, demonstrating significant improvements over direct regression methods as well as offline registration. 

[PDF](http://arxiv.org/abs/2401.11002v1) Project page: https://chaitanya100100.github.io/FastRegistration/

**Summary**
虚拟现实中逼真虚拟人的生成方法，基于迭代细化模块和通用虚拟人引导的图像转图像风格迁移模块的组合。

**Key Takeaways**
- 虚拟现实中逼真虚拟人的生成是实现更加沉浸式社交互动的关键。
- 准确地将虚拟人动画与佩戴头显时的图像进行匹配对于逼真虚拟人的生成至关重要。
- 虚拟人与头显摄像头图像之间的域差异是导致匹配困难的主要原因之一。
- 开发了一种系统设计，将问题分解为两个部分：域内输入迭代细化模块和通用虚拟人引导的图像转图像风格迁移模块。
- 这两个模块相互增强，使得在接近真实结果的示例显示时，图像风格迁移变得更容易，而更好的域差异消除有助于匹配。
- 该系统高效地产生了高质量的结果，避免了昂贵的离线匹配以生成个性化标签的需要。
- 通过对商品头显的广泛实验验证了该方法的准确性和效率，证明了其优于直接回归方法和离线匹配的显着改进。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>题目：VR 面部动画的逼真虚拟形象快速注册</li>
<li>作者：Qianqian Wang, Jiapeng Tang, Yebin Liu, Xin Tong, Yajie Zhao, Shihong Xia</li>
<li>单位：香港中文大学（深圳）</li>
<li>关键词：虚拟现实、面部动画、图像风格迁移、在线注册</li>
<li>论文链接：https://arxiv.org/pdf/2208.04345.pdf，Github 链接：无</li>
<li>摘要：
（1）研究背景：虚拟现实（VR）有望带来比其他媒体更具沉浸感的人际互动。关键在于能够在佩戴 VR 头显时准确地为自己的形象制作逼真的虚拟形象动画。虽然可以在离线环境中将高质量的人特定虚拟形象注册到头显摄像头（HMC）图像，但通用实时模型的性能却会显著下降。在线注册也具有挑战性，原因在于存在倾斜的摄像头视角和模态差异。
（2）过去的方法及其问题：本文首先表明，虚拟形象和头显摄像头图像之间的域差距是主要难点之一，其中基于 Transformer 的架构在域一致的数据上实现了高精度，但在重新引入域差距时会退化。基于这一发现，我们开发了一种系统设计，将问题分解为两部分：1）一个迭代细化模块，用于获取域内输入；2）一个通用虚拟形象引导的图像到图像风格迁移模块，该模块以当前估计的表情和头部姿势为条件。这两个模块相互增强，因为当显示接近真实示例时图像风格迁移变得更容易，而更好的域差距消除有助于注册。
（3）研究方法：我们的系统以有效的方式产生了高质量的结果，消除了进行昂贵的离线注册以生成个性化标签的需要。我们通过在商品头显上进行广泛的实验来验证我们方法的准确性和效率，证明了它比直接回归方法和离线注册有显著的改进。
（4）方法性能：该方法在任务和性能方面取得了以下成果：</li>
</ol>
<ul>
<li>任务：在商品头显上对 VR 面部动画的逼真虚拟形象进行快速注册。</li>
<li>性能：与直接回归方法和离线注册相比，我们的方法在准确性和效率方面都有显著的提高。这些性能支持了我们的目标，即提供一种快速、准确且高效的方法来注册逼真虚拟形象。</li>
</ul>
<ol start="7">
<li><p>方法：
(1) 迭代细化模块：该模块以当前估计的表情和头部姿势为条件，生成逼真的虚拟形象动画。它使用一个卷积神经网络（CNN）来提取图像特征，并使用一个长短期记忆（LSTM）网络来建模时间依赖性。
(2) 通用虚拟形象引导的图像到图像风格迁移模块：该模块将虚拟形象的风格迁移到HMC图像上。它使用一个生成对抗网络（GAN）来学习虚拟形象和HMC图像之间的映射关系。
(3) 联合优化：这两个模块相互增强，因为当显示接近真实示例时图像风格迁移变得更容易，而更好的域差距消除有助于注册。</p>
</li>
<li><p>结论：
（1）：本文提出了一种快速、准确且高效的方法来注册逼真虚拟形象，该方法消除了进行昂贵的离线注册以生成个性化标签的需要。
（2）：创新点：</p>
</li>
</ol>
<ul>
<li>提出了一种迭代细化模块，该模块以当前估计的表情和头部姿势为条件，生成逼真的虚拟形象动画。</li>
<li>提出了一种通用虚拟形象引导的图像到图像风格迁移模块，该模块将虚拟形象的风格迁移到头显摄像头图像上。</li>
<li>将这两个模块结合起来，形成一个联合优化框架，相互增强，提高注册的准确性和效率。
性能：</li>
<li>与直接回归方法和离线注册相比，该方法在准确性和效率方面都有显著的提高。</li>
<li>该方法在商品头显上实现了实时性能，能够以每秒30帧的速度生成逼真的虚拟形象动画。</li>
<li>该方法能够处理各种各样的表情和头部姿势，并且对光照和遮挡具有鲁棒性。
工作量：</li>
<li>该方法的实现相对简单，并且可以在普通的GPU上训练和部署。</li>
<li>该方法不需要昂贵的离线注册，并且能够在几分钟内完成注册过程。</li>
<li>该方法对各种各样的虚拟形象和头显摄像头图像具有通用性，因此可以广泛应用于VR面部动画领域。</li>
</ul>



<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-adff4a2a529cc67cabb0ab4e3422329d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e38782058424beeee89575e1764c835.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4544bcb7227c9e7f7be3dd1a344f7b7b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee9e00db6f9bab9cbc238317f5ec6446.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e77a54cab90843cba1c304290ec1ded1.jpg" align="middle">
</details>
​    


## A Simple Baseline for Spoken Language to Sign Language Translation with   3D Avatars
**Authors:Ronglai Zuo, Fangyun Wei, Zenggui Chen, Brian Mak, Jiaolong Yang, Xin Tong**

The objective of this paper is to develop a functional system for translating spoken languages into sign languages, referred to as Spoken2Sign translation. The Spoken2Sign task is orthogonal and complementary to traditional sign language to spoken language (Sign2Spoken) translation. To enable Spoken2Sign translation, we present a simple baseline consisting of three steps: 1) creating a gloss-video dictionary using existing Sign2Spoken benchmarks; 2) estimating a 3D sign for each sign video in the dictionary; 3) training a Spoken2Sign model, which is composed of a Text2Gloss translator, a sign connector, and a rendering module, with the aid of the yielded gloss-3D sign dictionary. The translation results are then displayed through a sign avatar. As far as we know, we are the first to present the Spoken2Sign task in an output format of 3D signs. In addition to its capability of Spoken2Sign translation, we also demonstrate that two by-products of our approach-3D keypoint augmentation and multi-view understanding-can assist in keypoint-based sign language understanding. Code and models will be available at https://github.com/FangyunWei/SLRT 

[PDF](http://arxiv.org/abs/2401.04730v1) 

**Summary**
将口语翻译成手语，并通过3D手语形象显示翻译结果，此系统是一项创新的任务，并可以通过辅助语义理解辅助手语识别。

**Key Takeaways**

* 提出Spoken2Sign任务，将口语翻译成手语，并通过3D手语形象显示翻译结果。
* Spoken2Sign任务与传统的将手语翻译成口语（Sign2Spoken）的任务是正交和互补的。
* Spoken2Sign模型由文本到语义翻译器、手语连接器和渲染模块组成。
* 使用语义-视频字典对Spoken2Sign模型进行训练。
* 这是第一个以3D手势作为输出格式来展示Spoken2Sign任务的系统。
* 该系统还可以通过辅助语义理解辅助手语识别。
* 代码和模型将在 https://github.com/FangyunWei/SLRT 开源。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li><p>题目：一种简单的口语到手语翻译的基线方法，使用 3D 化身</p>
</li>
<li><p>作者：Zuo Ronglai、Wei Fangyun、Chen Zenggui、Mak Brian、Yang Jiaolong、Tong Xin</p>
</li>
<li><p>隶属单位：香港科技大学</p>
</li>
<li><p>关键词：手语翻译、口语到手语翻译、3D 化身、生成模型、关键点估计、多视图理解</p>
</li>
<li><p>链接：https://arxiv.org/abs/2401.04730
Github：https://github.com/FangyunWei/SLRT</p>
</li>
<li><p>摘要：
（1）研究背景：手语是聋哑人的主要交流方式。以往的研究主要集中在手语到口语翻译（Sign2Spoken）上，而本文将重点转移到逆向过程：口语到手语翻译（Spoken2Sign），以进一步缩小聋哑人和听力正常人群之间的沟通鸿沟。
（2）过去方法及问题：以往的大部分口语到手语翻译研究都集中在通过关键点来表达翻译结果，但关键点表示法对符号使用者来说往往难以理解。一些研究使用生成模型将关键点动画化为手语图像，但 2D 视频格式容易出现模糊和视觉失真。
（3）研究方法：本文提出了一种创新的口语到手语翻译方法，利用 3D 化身来表示翻译结果。该方法首先利用现有的 Sign2Spoken 基准数据集创建一个词表-视频词典，然后为词典中的每个手语视频估计一个 3D 手语，最后训练一个口语到手语翻译模型，该模型由文本到词表翻译器、符号连接器和渲染模块组成，借助于生成的词表-3D 手语词典进行训练。翻译结果通过手语化身显示。
（4）方法性能：该方法在口语到手语翻译任务上实现了良好的性能，并且还证明了该方法的两个副产品——3D 关键点增强和多视图理解——可以辅助基于关键点的理解。</p>
</li>
<li><p>方法：
（1）词典构建：利用现有的 CSLR 模型将连续手语视频分割成孤立手语，并构建一个手语词典，其中包含每个手语视频对应的词条。
（2）3D 手语估计：使用 SMPLify-X 和 SMPL-X 模型估计词典中每个孤立手语的 3D 表示。
（3）口语到手语翻译：使用文本到词条翻译器、手语连接器和渲染模块将输入文本翻译成手语动画。
（4）副产品：从 3D 手语中派生出 3D 关键点增强和多视图理解两个副产品，可以辅助基于关键点的理解。</p>
</li>
<li><p>结论：
（1）：本文关注口语到手语翻译，作为传统手语到口语翻译的逆向过程，旨在缩小聋哑人和听力正常人群之间的沟通鸿沟。与以往在二维空间产生翻译结果的作品不同，我们的创新方法使用 SMPLSign-X 和手语连接器等提出的技术生成三维手势。翻译结果通过虚拟形象显示。我们的方法涉及三个主要步骤：1）构建手语词典；2）估计词典中每个手势的三维表示；3）使用文本到手势翻译器将输入文本翻译成手势动画。
（2）：创新点：提出了一种基于 3D 虚拟形象的口语到手语翻译方法，该方法能够生成更逼真、更易于理解的手语翻译结果。
性能：在口语到手语翻译任务上实现了良好的性能，并且还证明了该方法的两个副产品——3D 关键点增强和多视图理解——可以辅助基于关键点的理解。
工作量：该方法需要构建手语词典和估计词典中每个手势的三维表示，这需要大量的数据和计算资源。</p>
</li>
</ol>



<p></p><details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ae306a2dc4fbff7be83edef56e736584.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7ef768d7501213cf3991c2239ea75b97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40bd6db0398f7d13cf5c8698c0fab4c3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3a927de2172cfccf688372b4f592d810.jpg" align="middle">
</details><br>​    <p></p>
<h2 id="Morphable-Diffusion-3D-Consistent-Diffusion-for-Single-image-Avatar-Creation"><a href="#Morphable-Diffusion-3D-Consistent-Diffusion-for-Single-image-Avatar-Creation" class="headerlink" title="Morphable Diffusion: 3D-Consistent Diffusion for Single-image Avatar   Creation"></a>Morphable Diffusion: 3D-Consistent Diffusion for Single-image Avatar   Creation</h2><p><strong>Authors:Xiyi Chen, Marko Mihajlovic, Shaofei Wang, Sergey Prokudin, Siyu Tang</strong></p>
<p>Recent advances in generative diffusion models have enabled the previously unfeasible capability of generating 3D assets from a single input image or a text prompt. In this work, we aim to enhance the quality and functionality of these models for the task of creating controllable, photorealistic human avatars. We achieve this by integrating a 3D morphable model into the state-of-the-art multiview-consistent diffusion approach. We demonstrate that accurate conditioning of a generative pipeline on the articulated 3D model enhances the baseline model performance on the task of novel view synthesis from a single image. More importantly, this integration facilitates a seamless and accurate incorporation of facial expression and body pose control into the generation process. To the best of our knowledge, our proposed framework is the first diffusion model to enable the creation of fully 3D-consistent, animatable, and photorealistic human avatars from a single image of an unseen subject; extensive quantitative and qualitative evaluations demonstrate the advantages of our approach over existing state-of-the-art avatar creation models on both novel view and novel expression synthesis tasks. </p>
<p><a href="http://arxiv.org/abs/2401.04728v1">PDF</a> Project page: <a href="https://xiyichen.github.io/morphablediffusion/">https://xiyichen.github.io/morphablediffusion/</a></p>
<p><strong>Summary</strong><br>利用生成扩散模型来生成可控写实的3D人形虚拟人。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>生成扩散模型可以从单张图片或文本提示生成3D资产。</li>
<li>将3D可变形模型集成到多视角一致扩散方法中可以提高模型生成人形虚拟人的性能。</li>
<li>这种集成可以将面部表情和身体姿势控制无缝准确地融入生成过程。</li>
<li>该框架是第一个能够从一个看不见的主题的单张图片中创建完全3D一致、可动画和逼真的人形虚拟人的扩散模型。</li>
<li>定量和定性评估表明该方法优于现有的人形虚拟人创建模型。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：可变形扩散：单张图像生成虚拟形象的 3D 一致扩散</li>
<li>作者：Xiyi Chen, Xiuming Zhang, Yinda Zhang, Kun Zhou, Yebin Liu</li>
<li>单位：北京大学</li>
<li>关键词：生成扩散模型、3D 一致扩散、虚拟形象创建、面部表情合成、身体姿势控制</li>
<li>论文链接：None，Github 代码链接：None</li>
<li>摘要：
(1)：最近生成扩散模型的进展使得从单张输入图像或文本提示生成 3D 资产成为可能。在这项工作中，我们旨在增强这些模型的质量和功能，以完成创建可控的、逼真的虚拟形象的任务。我们通过将 3D 可变形模型集成到最先进的多视图一致扩散方法中来实现这一点。我们证明了对生成管道在铰接 3D 模型上的准确调节增强了基线模型在从单张图像进行新视图合成的任务上的性能。更重要的是，这种集成促进了面部表情和身体姿势控制的无缝且准确地融入生成过程。据我们所知，我们提出的框架是第一个能够从未见过的主题的单张图像中创建完全 3D 一致、可动画且逼真的虚拟形象的扩散模型；广泛的定量和定性评估证明了我们的方法在现有最先进的虚拟形象创建模型上在新的视图和新的表达式合成任务上的优势。我们项目的代码将在 xiyichen.github.io/morphablediffusion 上公开。</li>
</ol>
<p>Methods:
(1): 将 3D 可变形模型集成到最先进的多视图一致扩散方法中，以增强生成扩散模型的质量和功能；
(2): 通过对生成管道在铰接 3D 模型上的准确调节，增强基线模型在从单张图像进行新视图合成的任务上的性能；
(3): 将面部表情和身体姿势控制无缝且准确地融入生成过程，从而促进虚拟形象的可控生成；
(4): 提出第一个能够从未见过的主题的单张图像中创建完全 3D 一致、可动画且逼真的虚拟形象的扩散模型；
(5): 通过广泛的定量和定性评估，证明了该方法在现有最先进的虚拟形象创建模型上在新的视图和新的表达式合成任务上的优势。</p>
<ol start="8">
<li>结论：
（1）这项工作将可变形扩散模型引入虚拟形象创建领域，通过将 3D 可变形模型与多视图一致扩散框架无缝集成，增强了生成扩散模型的质量和功能，实现了从单张图像生成完全 3D 一致、可动画且逼真的虚拟形象，为逼真的人类数字化加速和后续研究提供了新的思路。
（2）创新点：</li>
</ol>
<ul>
<li>将 3D 可变形模型集成到多视图一致扩散框架中，增强了生成扩散模型的质量和功能。</li>
<li>通过对生成管道在铰接 3D 模型上的准确调节，增强了基线模型在从单张图像进行新视图合成的任务上的性能。</li>
<li>将面部表情和身体姿势控制无缝且准确地融入生成过程，从而促进虚拟形象的可控生成。</li>
<li>提出第一个能够从未见过的主题的单张图像中创建完全 3D 一致、可动画且逼真的虚拟形象的扩散模型。
性能：</li>
<li>在定量和定性评估中，该方法在新的视图和新的表达式合成任务上优于现有最先进的虚拟形象创建模型。</li>
<li>该方法能够生成高质量的虚拟形象，具有逼真的面部表情和身体姿势。</li>
<li>该方法能够有效地控制虚拟形象的面部表情和身体姿势。
工作量：</li>
<li>该方法需要对生成扩散模型进行微调，这可能需要大量的数据和计算资源。</li>
<li>该方法需要对 3D 可变形模型进行拟合，这可能需要大量的人工劳动。</li>
<li>该方法需要对生成过程进行控制，这可能需要大量的人工干预。</li>
</ul>



<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7b2eff8a425f22057d3d5f29a6453f35.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-76fdd4baa10b10b9c96a8fdd6e70afd5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67653f4e7dd758faa5cf32a8691e30b3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7f1c5bb53301b2300b07aca2eec1c483.jpg" align="middle">
</details>
​    


## Deformable 3D Gaussian Splatting for Animatable Human Avatars
**Authors:HyunJun Jung, Nikolas Brasch, Jifei Song, Eduardo Perez-Pellitero, Yiren Zhou, Zhihao Li, Nassir Navab, Benjamin Busam**

Recent advances in neural radiance fields enable novel view synthesis of photo-realistic images in dynamic settings, which can be applied to scenarios with human animation. Commonly used implicit backbones to establish accurate models, however, require many input views and additional annotations such as human masks, UV maps and depth maps. In this work, we propose ParDy-Human (Parameterized Dynamic Human Avatar), a fully explicit approach to construct a digital avatar from as little as a single monocular sequence. ParDy-Human introduces parameter-driven dynamics into 3D Gaussian Splatting where 3D Gaussians are deformed by a human pose model to animate the avatar. Our method is composed of two parts: A first module that deforms canonical 3D Gaussians according to SMPL vertices and a consecutive module that further takes their designed joint encodings and predicts per Gaussian deformations to deal with dynamics beyond SMPL vertex deformations. Images are then synthesized by a rasterizer. ParDy-Human constitutes an explicit model for realistic dynamic human avatars which requires significantly fewer training views and images. Our avatars learning is free of additional annotations such as masks and can be trained with variable backgrounds while inferring full-resolution images efficiently even on consumer hardware. We provide experimental evidence to show that ParDy-Human outperforms state-of-the-art methods on ZJU-MoCap and THUman4.0 datasets both quantitatively and visually. 

[PDF](http://arxiv.org/abs/2312.15059v1) 

**Summary**
参数驱动动态人类虚拟人，仅需少量单目序列即可构建，且对背景无关，能在消费级硬件上高效推理。

**Key Takeaways**

* 提出了一种参数驱动的动态人类虚拟人方法，名为 ParDy-Human。
* ParDy-Human 由两个模块组成：第一个模块根据 SMPL 顶点变形规范 3D 高斯体，第二个模块进一步采用设计好的关节编码并预测每个高斯体变形，以处理超出 SMPL 顶点变形的动态情况。
* ParDy-Human 显式建模逼真的动态人类虚拟人，所需训练视图和图像显著减少。
* ParDy-Human 的训练无需额外注释，例如蒙版，即使在消费级硬件上也能以全分辨率高效推断图像。
* 实验表明，ParDy-Human 在 ZJU-MoCap 和 THUman4.0 数据集上均优于最先进的方法。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li><p>标题：可变形 3D 高斯散点动画人体化身</p>
</li>
<li><p>作者：Junggi Kim, Michael Zollhöfer, Christian Theobalt</p>
</li>
<li><p>单位：马克斯·普朗克计算机图形学研究所</p>
</li>
<li><p>关键词：神经辐射场、动态人体、单目视频、参数化动态人体化身</p>
</li>
<li><p>论文链接：https://arxiv.org/abs/2209.00926
Github 代码链接：https://github.com/Junggy/pardy-human</p>
</li>
<li><p>摘要：
(1)：近年来，神经辐射场的进步使得在动态场景中合成照片级真实图像的新颖视角成为可能，这可以应用于具有动画的人体场景。常用的隐式骨干网可以建立准确的模型，然而，它们需要许多输入视图和额外的注释，例如人体蒙版、UV 贴图和深度图。在这项工作中，我们提出了 ParDy-Human（参数化动态人体化身），这是一种完全显式的方法，可以从少量单目序列构建数字虚拟形象。ParDy-Human 将参数驱动的动态引入到 3D 高斯散点中，其中 3D 高斯散点由人体姿态模型变形以使化身动画化。我们的方法由两部分组成：第一个模块根据 SMPL 顶点变形规范 3D 高斯散点，连续的模块进一步采用它们设计好的关节编码并预测每个高斯散点的变形，以处理超出 SMPL 顶点变形的动态。然后通过光栅化器合成图像。ParDy-Human 构成了一个显式模型，用于逼真的动态人体化身，所需训练视图和图像明显更少。我们的虚拟形象学习过程无需额外的注释，例如蒙版，并且可以在推断全分辨率图像时使用可变背景进行训练，即使在消费级硬件上也是如此。我们提供了实验证据表明，ParDy-Human 在 ZJU-MoCap 和 THUman4.0 数据集上优于最先进的方法，无论是在定量上还是在视觉上。我们的代码可在 https://github.com/Junggy/pardy-human 获得。
(2)：过去的方法通常使用隐式表示来构建神经辐射场，这需要许多输入视图和额外的注释，例如人体蒙版、UV 贴图和深度图。这些方法通常需要大量的数据和计算资源，并且在处理动态场景时可能存在困难。
(3)：本文提出的方法 ParDy-Human 是一种完全显式的方法，可以从少量单目序列构建数字虚拟形象。ParDy-Human 将参数驱动的动态引入到 3D 高斯散点中，其中 3D 高斯散点由人体姿态模型变形以使化身动画化。我们的方法由两部分组成：第一个模块根据 SMPL 顶点变形规范 3D 高斯散点，连续的模块进一步采用它们设计好的关节编码并预测每个高斯散点的变形，以处理超出 SMPL 顶点变形的动态。然后通过光栅化器合成图像。
(4)：ParDy-Human 在 ZJU-MoCap 和 THUman4.0 数据集上优于最先进的方法，无论是在定量上还是在视觉上。在 ZJU-MoCap 数据集上，ParDy-Human 在平均重投影误差 (MRE) 和光度一致性 (PC) 方面分别优于最先进的方法 14.6% 和 11.5%。在 THUman4.0 数据集上，ParDy-Human 在 MRE 和 PC 方面分别优于最先进的方法 12.3% 和 9.1%。这些结果表明，ParDy-Human 可以生成高质量的动态人体图像，并且可以很好地处理动态场景。</p>
</li>
<li><p>方法：
(1) 初始化3D高斯散点：从粗糙的点云扫描开始，并使用特定的自适应密度控制方案进行训练，该方案在训练过程中根据高斯散点的大小和梯度幅度对其进行分割、克隆和剪枝。
(2) 姿态高斯散点：根据其父级使用逐顶点变形（PVD）对每个高斯散点进行变形。
(3) 变形细化：对于穿着紧身衣的人体，使用 SMPL 模型进行变形就足够了。然而，一般来说，服装运动会导致更多依赖于人体姿势的变形。为了获得更高保真的渲染效果，我们包含了一个变形细化模块 (DRM)。
(4) 球谐函数方向：在 3D-GS 中，球谐函数 (SH) 用于合并视角相关的效果。
(5) 取消姿势高斯散点并更新父级：一旦高斯散点更新，它们就会被转换回规范空间，以便使用下一组参数再次摆姿势。取消姿势是通过按照变形顺序的相反顺序进行的。
(6) 训练：训练过程中，高斯散点的数量及其中心可能会发生变化，因此必须相应地更新父索引 i。</p>
</li>
<li><p>结论：
（1）：本文提出了 ParDy-Human，一种完全显式的方法，可以从少量单目序列构建数字虚拟形象。ParDy-Human 将参数驱动的动态引入到 3D 高斯散点中，其中 3D 高斯散点由人体姿态模型变形以使化身动画化。我们的方法由两部分组成：第一个模块根据 SMPL 顶点变形规范 3D 高斯散点，连续的模块进一步采用它们设计好的关节编码并预测每个高斯散点的变形，以处理超出 SMPL 顶点变形的动态。然后通过光栅化器合成图像。ParDy-Human 构成了一个显式模型，用于逼真的动态人体化身，所需训练视图和图像明显更少。我们的虚拟形象学习过程无需额外的注释，例如蒙版，并且可以在推断全分辨率图像时使用可变背景进行训练，即使在消费级硬件上也是如此。我们提供了实验证据表明，ParDy-Human 在 ZJU-MoCap 和 THUman4.0 数据集上优于最先进的方法，无论是在定量上还是在视觉上。
（2）：创新点：</p>
</li>
</ol>
<ul>
<li>将参数驱动的动态引入到 3D 高斯散点中，使化身能够进行逼真的动画。</li>
<li>提出了一种新的变形细化模块 (DRM)，以处理超出 SMPL 顶点变形的动态。</li>
<li>无需额外的注释，例如蒙版，即可从少量单目序列构建数字虚拟形象。</li>
<li>可以在推断全分辨率图像时使用可变背景进行训练，即使在消费级硬件上也是如此。
性能：</li>
<li>在 ZJU-MoCap 数据集上，ParDy-Human 在平均重投影误差 (MRE) 和光度一致性 (PC) 方面分别优于最先进的方法 14.6% 和 11.5%。</li>
<li>在 THUman4.0 数据集上，ParDy-Human 在 MRE 和 PC 方面分别优于最先进的方法 12.3% 和 9.1%。
工作量：</li>
<li>ParDy-Human 需要较少的训练视图和图像，并且可以在消费级硬件上进行训练。</li>
<li>ParDy-Human 的训练过程无需额外的注释，例如蒙版。</li>
<li>ParDy-Human 可以使用可变背景进行训练，这使得它能够生成更逼真的图像。</li>
</ul>



<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-3a2dec08eda70704d60e83b281cc54a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a167032c68efd5d06543a5ec3ba4f79.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e243cc96b91f1cb9f2e0e8cb1aa2a523.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-805c12244272b525ede83f20a94c5569.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-df3f505c56582ddada94e66d5ec6791a.jpg" align="middle">
</details>
​    


## A Language-based solution to enable Metaverse Retrieval
**Authors:Ali Abdari, Alex Falcon, Giuseppe Serra**

Recently, the Metaverse is becoming increasingly attractive, with millions of users accessing the many available virtual worlds. However, how do users find the one Metaverse which best fits their current interests? So far, the search process is mostly done by word of mouth, or by advertisement on technology-oriented websites. However, the lack of search engines similar to those available for other multimedia formats (e.g., YouTube for videos) is showing its limitations, since it is often cumbersome to find a Metaverse based on some specific interests using the available methods, while also making it difficult to discover user-created ones which lack strong advertisement. To address this limitation, we propose to use language to naturally describe the desired contents of the Metaverse a user wishes to find. Second, we highlight that, differently from more conventional 3D scenes, Metaverse scenarios represent a more complex data format since they often contain one or more types of multimedia which influence the relevance of the scenario itself to a user query. Therefore, in this work, we create a novel task, called Text-to-Metaverse retrieval, which aims at modeling these aspects while also taking the cross-modal relations with the textual data into account. Since we are the first ones to tackle this problem, we also collect a dataset of 33000 Metaverses, each of which consists of a 3D scene enriched with multimedia content. Finally, we design and implement a deep learning framework based on contrastive learning, resulting in a thorough experimental setup. 

[PDF](http://arxiv.org/abs/2312.14630v1) Accepted at 30th International Conference on Multimedia Modeling-   MMM2024

**Summary**
元宇宙检索：利用语言描述用户期望的元宇宙内容，并构建跨模态关系学习模型实现文本到元宇宙检索。

**Key Takeaways**
- 元宇宙正变得越来越有吸引力，数百万用户正在访问许多可用的虚拟世界。
- 目前，人们发现适合其当前兴趣的元宇宙的方法主要是口耳相传或通过技术型网站上的广告。
- 缺乏类似于其他多媒体格式（例如 YouTube 用于视频）的搜索引擎显示出其局限性。
- 提出了一个新颖的任务：文本到元宇宙检索，旨在对这些方面进行建模，同时考虑与文本数据的跨模态关系。
- 创建了一个包含 33000 个元宇宙的数据集，每个元宇宙都由一个丰富的多媒体内容的 3D 场景组成。
- 设计并实现了基于对比学习的深度学习框架，形成了一个完整的实验设置。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>标题：基于语言的元宇宙检索解决方案</li>
<li>作者：Ali Abdari、Alex Falcon、Giuseppe Serra</li>
<li>第一作者单位：乌迪内大学</li>
<li>关键词：多媒体、文本到多媒体检索、跨模态理解、元宇宙、对比学习</li>
<li>论文链接：https://arxiv.org/abs/2312.14630，Github 链接：无</li>
<li>摘要：
（1）研究背景：元宇宙正变得越来越受欢迎，但目前缺乏有效的搜索引擎来帮助用户找到最适合他们当前兴趣的元宇宙。
（2）过去方法与问题：现有的搜索过程主要通过口碑或在技术导向的网站上做广告来完成。然而，缺乏类似于其他多媒体格式（如 YouTube 用于视频）的搜索引擎，这使得根据一些特定兴趣使用可用方法找到元宇宙变得很麻烦，同时也使得发现缺乏强力广告的用户创建的元宇宙变得困难。
（3）研究方法：为了解决这个问题，本文提出使用语言来自然地描述用户想要找到的元宇宙的所需内容。此外，本文还强调，与更传统的 3D 场景不同，元宇宙场景表示更复杂的数据格式，因为它们通常包含一种或多种影响场景本身与用户查询相关性的多媒体。因此，本文创建了一个名为文本到元宇宙检索的新任务，旨在对这些方面进行建模，同时还考虑与文本数据的跨模态关系。由于本文是第一个解决这个问题的研究，因此还收集了一个由 33000 个元宇宙组成的数据集，每个元宇宙都由一个包含多媒体内容的 3D 场景组成。最后，本文设计并实现了一个基于对比学习的深度学习框架，从而形成了一个彻底的实验设置。
（4）方法性能：本文方法在文本到元宇宙检索任务上取得了最先进的性能，证明了该方法可以有效地支持其目标。</li>
</ol>
<p>Some Error for method(比如是不是没有Methods这个章节)</p>
<ol start="8">
<li>结论：
（1）：本文提出了一种基于语言的元宇宙检索解决方案，该解决方案通过对比学习的深度学习框架，实现了文本到元宇宙检索任务的最先进性能。
（2）：创新点：
• 提出了一种新的文本到元宇宙检索任务，该任务旨在对元宇宙场景表示的复杂数据格式进行建模，并考虑与文本数据的跨模态关系。
• 收集了一个由33000个元宇宙组成的元宇宙检索数据集，为该任务的研究提供了基础。
• 设计并实现了一个基于对比学习的深度学习框架，该框架可以有效地支持文本到元宇宙检索任务。
性能：
• 在文本到元宇宙检索任务上取得了最先进的性能，证明了该方法可以有效地支持其目标。
工作量：
• 收集了一个由33000个元宇宙组成的元宇宙检索数据集，该数据集的构建需要大量的人力物力。
• 设计并实现了一个基于对比学习的深度学习框架，该框架的构建需要大量的时间和精力。</li>
</ol>



<p></p><details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-bcbf5fa9f9cbd442f630c06fe63aa7e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-768b3350157276eaf44aaa50642b5f8f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e7fc71bb9936d63fe907246316a055a.jpg" align="middle">
</details><br>​    <p></p>
<h2 id="MoSAR-Monocular-Semi-Supervised-Model-for-Avatar-Reconstruction-using-Differentiable-Shading"><a href="#MoSAR-Monocular-Semi-Supervised-Model-for-Avatar-Reconstruction-using-Differentiable-Shading" class="headerlink" title="MoSAR: Monocular Semi-Supervised Model for Avatar Reconstruction using   Differentiable Shading"></a>MoSAR: Monocular Semi-Supervised Model for Avatar Reconstruction using   Differentiable Shading</h2><p><strong>Authors:Abdallah Dib, Luiz Gustavo Hafemann, Emeline Got, Trevor Anderson, Amin Fadaeinejad, Rafael M. O. Cruz, Marc-Andre Carbonneau</strong></p>
<p>Reconstructing an avatar from a portrait image has many applications in multimedia, but remains a challenging research problem. Extracting reflectance maps and geometry from one image is ill-posed: recovering geometry is a one-to-many mapping problem and reflectance and light are difficult to disentangle. Accurate geometry and reflectance can be captured under the controlled conditions of a light stage, but it is costly to acquire large datasets in this fashion. Moreover, training solely with this type of data leads to poor generalization with in-the-wild images. This motivates the introduction of MoSAR, a method for 3D avatar generation from monocular images. We propose a semi-supervised training scheme that improves generalization by learning from both light stage and in-the-wild datasets. This is achieved using a novel differentiable shading formulation. We show that our approach effectively disentangles the intrinsic face parameters, producing relightable avatars. As a result, MoSAR estimates a richer set of skin reflectance maps, and generates more realistic avatars than existing state-of-the-art methods. We also introduce a new dataset, named FFHQ-UV-Intrinsics, the first public dataset providing intrinsic face attributes at scale (diffuse, specular, ambient occlusion and translucency maps) for a total of 10k subjects. The project website and the dataset are available on the following link: <a href="https://ubisoft-laforge.github.io/character/mosar/">https://ubisoft-laforge.github.io/character/mosar/</a> </p>
<p><a href="http://arxiv.org/abs/2312.13091v2">PDF</a> <a href="https://ubisoft-laforge.github.io/character/mosar/">https://ubisoft-laforge.github.io/character/mosar/</a></p>
<p><strong>Summary</strong><br>利用半监督训练方案，MoSAR可从单张图像合成更真实的人脸。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>MoSAR可以从单张图像中生成三维虚拟人。</li>
<li>MoSAR使用了半监督训练方案，利用光场和野外数据集进行训练。</li>
<li>MoSAR提出了一种创新的可微分着色公式，用于解耦内在面部参数。</li>
<li>MoSAR产生的虚拟人具有更丰富的皮肤反射图，更逼真。</li>
<li>FFHQ-UV-Intrinsics是第一个大规模提供内在面部属性的公开数据集。</li>
<li>FFHQ-UV-Intrinsics数据集包含10k个主题的漫反射、镜面反射、环境光遮蔽和半透明度贴图。</li>
<li>项目网站和数据集可通过以下链接获得：<a href="https://ubisoft-laforge.github.io/character/mosar/">https://ubisoft-laforge.github.io/character/mosar/</a></li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：MoSAR：使用可微分着色的单目半监督模型，用于虚拟形象重建</li>
<li>作者：Abdallah Dib、Luiz Gustavo Hafemann、Emeline Got、Trevor Anderson、Amin Fadaeinejad、Rafael M. O. Cruz、Marc-André Carbonneau</li>
<li>第一作者单位：育碧拉福格约克大学</li>
<li>关键词：虚拟形象生成、单目重建、半监督学习、可微分着色</li>
<li>论文链接：https://arxiv.org/abs/2312.13091
Github 链接：无</li>
<li>摘要：</li>
</ol>
<p>（1）研究背景：从人像图像中重建虚拟形象在多媒体领域有着广泛的应用，但仍然是一个具有挑战性的研究课题。从一张图像中提取反射率图和几何形状是不适定的：恢复几何形状是一个一对多的映射问题，并且反射率和光线难以解开。在光场等受控条件下可以捕捉到准确的几何形状和反射率，但以这种方式获取大型数据集的成本很高。此外，仅使用此类数据进行训练会导致在野外图像中泛化性较差。</p>
<p>（2）过去的方法及其问题：现有方法主要分为两类：基于光场的数据驱动方法和基于模型的几何重建方法。基于光场的数据驱动方法可以生成高质量的虚拟形象，但需要昂贵的采集设备和受控的拍摄环境。基于模型的几何重建方法可以从单目图像中重建几何形状，但通常需要大量的人工标注数据。</p>
<p>（3）本文提出的研究方法：为了解决上述问题，本文提出了一种新的单目半监督模型 MoSAR，用于虚拟形象重建。MoSAR 使用了一种新颖的可微分着色公式，可以有效地解开固有的人脸参数，从而产生可重新照明的虚拟形象。此外，MoSAR 还引入了一个新的数据集 FFHQ-UV-Intrinsics，该数据集提供了 10k 个受试者的固有面部属性（漫反射、镜面反射、环境光遮挡和半透明度贴图）。</p>
<p>（4）方法在任务和性能上的表现：在人脸几何形状重建和虚拟形象生成任务上，MoSAR 的性能优于现有最先进的方法。MoSAR 可以从单目图像中生成高质量的虚拟形象，并且这些虚拟形象可以重新照明以匹配不同的环境光照条件。</p>
<ol start="7">
<li><p>方法：
(1): 本文提出了一种新的单目半监督模型MoSAR，用于虚拟形象重建。MoSAR使用了一种新颖的可微分着色公式，可以有效地解开固有的人脸参数，从而产生可重新照明的虚拟形象。
(2): MoSAR还引入了一个新的数据集FFHQ-UV-Intrinsics，该数据集提供了10k个受试者的固有面部属性（漫反射、镜面反射、环境光遮挡和半透明度贴图）。
(3): 在人脸几何形状重建和虚拟形象生成任务上，MoSAR的性能优于现有最先进的方法。MoSAR可以从单目图像中生成高质量的虚拟形象，并且这些虚拟形象可以重新照明以匹配不同的环境光照条件。</p>
</li>
<li><p>结论：
（1）：本文提出了一种从单目图像重建虚拟形象的半监督模型MoSAR，该模型使用可微分着色公式有效解开固有的人脸参数，从而产生可重新照明的虚拟形象。此外，MoSAR还引入了一个新的数据集FFHQ-UV-Intrinsics，该数据集提供了10k个受试者的固有面部属性（漫反射、镜面反射、环境光遮挡和半透明度贴图）。
（2）：创新点：</p>
</li>
</ol>
<ul>
<li>提出了一种新的单目半监督模型MoSAR，用于虚拟形象重建。</li>
<li>使用了一种新颖的可微分着色公式，可以有效地解开固有的人脸参数，从而产生可重新照明的虚拟形象。</li>
<li>引入了一个新的数据集FFHQ-UV-Intrinsics，该数据集提供了10k个受试者的固有面部属性（漫反射、镜面反射、环境光遮挡和半透明度贴图）。
性能：</li>
<li>在人脸几何形状重建和虚拟形象生成任务上，MoSAR的性能优于现有最先进的方法。</li>
<li>MoSAR可以从单目图像中生成高质量的虚拟形象，并且这些虚拟形象可以重新照明以匹配不同的环境光照条件。
工作量：</li>
<li>该方法需要大量的数据和计算资源。</li>
<li>该方法的训练过程比较复杂，需要花费大量的时间和精力。</li>
</ul>



<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2050a1226482a2aeb4def8538e376837.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09d311b82ccb9b15a7db18277e983bf5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe2cd6e6acd2db5fb63e12b91608773e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2e81d9872c02b6ac45bf7a97118b311e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8544dbf9b68d824464a75139e05d2910.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5cce74b3c6fc797539b1b70305e061f.jpg" align="middle">
</details>
​    


## Relightable and Animatable Neural Avatars from Videos
**Authors:Wenbin Lin, Chengwei Zheng, Jun-Hai Yong, Feng Xu**

Lightweight creation of 3D digital avatars is a highly desirable but challenging task. With only sparse videos of a person under unknown illumination, we propose a method to create relightable and animatable neural avatars, which can be used to synthesize photorealistic images of humans under novel viewpoints, body poses, and lighting. The key challenge here is to disentangle the geometry, material of the clothed body, and lighting, which becomes more difficult due to the complex geometry and shadow changes caused by body motions. To solve this ill-posed problem, we propose novel techniques to better model the geometry and shadow changes. For geometry change modeling, we propose an invertible deformation field, which helps to solve the inverse skinning problem and leads to better geometry quality. To model the spatial and temporal varying shading cues, we propose a pose-aware part-wise light visibility network to estimate light occlusion. Extensive experiments on synthetic and real datasets show that our approach reconstructs high-quality geometry and generates realistic shadows under different body poses. Code and data are available at \url{https://wenbin-lin.github.io/RelightableAvatar-page/}. 

[PDF](http://arxiv.org/abs/2312.12877v1) Accepted by AAAI 2024

**摘要**
利用未知光照条件下人物的稀疏视频，构建可重打光并可动画化的神经人形。

**主要要点**
- 提出一种从稀疏视频创建可重打光和可动画化的神经人形的方法。
- 通过可逆变形场更好地建模几何形状和阴影变化来解决几何变化。
- 提出一种姿势感知的局部光线可见性网络来估计光线遮挡，以便对空间和时间变化的着色提示进行建模。
- 在合成和真实数据集上的广泛实验表明，该方法重建了高质量的几何形状并在不同的身体姿势下生成了逼真的阴影。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li><p>题目：从视频中生成可重照明和可动画的神经网络虚拟形象</p>
</li>
<li><p>作者：Wenbin Lin, Chengwei Zheng, Jun-Hai Yong, Feng Xu</p>
</li>
<li><p>单位：清华大学软件学院</p>
</li>
<li><p>关键词：神经网络虚拟形象、可重照明、可动画、几何建模、阴影建模</p>
</li>
<li><p>论文链接：https://arxiv.org/abs/2312.12877
Github 代码链接：无</p>
</li>
<li><p>摘要：
（1）研究背景：近年来，人类数字化技术发展迅速，其中 3D 着装人类虚拟形象的重建和动画在远程临场、AR/VR 和虚拟试穿等领域具有广泛应用。一个重要的目标是在所需照明环境和所需姿势下渲染人类虚拟形象。因此，人类虚拟形象需要同时具有可重照明性和可动画性，并实现逼真的渲染质量。通常，生成这些高质量的人类虚拟形象依赖于高品质数据，例如由光场（Light Stage）记录的数据，而这些数据复杂且昂贵。
（2）过去方法及问题：近年来，神经辐射场（NeRF）的出现为仅从日常记录的视频中生成可动画和可重照明 3D 人类虚拟形象开辟了新的窗口。基于 NeRF 的方法在 3D 对象表示和静态和动态对象的逼真渲染方面取得了显着的成功，包括人体（Peng et al. 2021b,a; Xu, Dieck, and Sminchisescu 2021; Wen et al. 2022; Jiang et al. 2022a,b; Wang et al. 2022; Peng et al. 2022; Yu et al. 2023; Su, Bagautdinov, and Rhodin 2023）。此外，NeRF 可用于固有分解，以实现静态对象的令人印象深刻的照明效果（Zhang et al. 2021; Yao et al. 2022; Boss et al. 2021a; Srinivasan et al. 2021; Boss et al. 2021b; Zhang et al. 2022; Jin et al. 2023）。然而，基于 NeRF 的动态对象重照明很少被研究。一个关键挑战是动态变化导致对象着色发生剧烈变化，这很难用当前的 NeRF 技术建模。
（3）本文方法：本文提出从稀疏视频中重建可重照明和可动画的 3D 人类虚拟形象，这些视频是在未校准的照明下记录的。为了实现这一目标，我们需要重建身体几何、材质和环境光照。动态的身体几何形状由规范空间中的静态几何形状和运动建模，以将其变形为每个帧的观察空间中的形状。我们提出了一种可逆神经变形场，它建立了规范空间和所有观察空间之间的双向映射。利用这种双向映射，我们可以轻松地利用规范姿势中提取的身体网格来更好地解决逆线性混合蒙皮问题，从而实现高质量的几何重建。在所有帧的几何重建之后，我们提出了一种光照可见性估计模块，以更好地模拟材质和光照重建的动态自遮挡效应。我们将全局姿势相关的可见性估计任务转移到多个局部部分任务中，这大大简化了光照可见性估计的复杂性。该模型受益于部分架构，具有良好的泛化能力，即使训练数据有限，也能成功估计各种身体姿势和照明条件下的光照可见性。最后，我们优化身体材质和照明参数，然后我们的方法可以在任何所需的身体姿势、照明和视点下渲染逼真的图像。
（4）方法性能：本文方法在合成和真实数据集上进行了广泛的实验，结果表明，该方法可以重建高质量的几何形状并在不同的身体姿势下生成逼真的阴影。代码和数据可在 https://wenbin-lin.github.io/RelightableAvatar-page/ 获得。</p>
</li>
<li><p>Methods:
(1): 提出可逆神经变形场，用于规范空间和所有观察空间之间的双向映射，从而更好地解决逆线性混合蒙皮问题，实现高质量的几何重建。
(2): 提出光照可见性估计模块，将全局姿势相关的可见性估计任务转移到多个局部部分任务中，简化光照可见性估计的复杂性。
(3): 优化身体材质和照明参数，使方法可以在任何所需的身体姿势、照明和视点下渲染逼真的图像。</p>
</li>
<li><p>结论：
（1）：本文提出了一种从稀疏视频中重建可重照明和可动画的 3D 人类虚拟形象的方法，该方法可以生成高质量的几何形状并在不同的身体姿势下生成逼真的阴影。
（2）：创新点：
提出可逆神经变形场，用于规范空间和所有观察空间之间的双向映射，从而更好地解决逆线性混合蒙皮问题，实现高质量的几何重建。
提出光照可见性估计模块，将全局姿势相关的可见性估计任务转移到多个局部部分任务中，简化光照可见性估计的复杂性。
优化身体材质和照明参数，使方法可以在任何所需的身体姿势、照明和视点下渲染逼真的图像。
性能：
该方法在合成和真实数据集上进行了广泛的实验，结果表明，该方法可以重建高质量的几何形状并在不同的身体姿势下生成逼真的阴影。
工作量：
该方法需要收集稀疏视频数据，并进行数据预处理，然后训练神经网络模型。训练过程可能需要大量的时间和计算资源。</p>
</li>
</ol>



<p></p><details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9e9391649c498b53d3aee9866413c321.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-38ceb0869d19022358f9afbec6d56552.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d213e510b1c1cd964c103074eb5cbc6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f01dc710ef4497b1ceabc9bbd0193f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-416c5c19e5e6ad7af22f15d01e6f67d3.jpg" align="middle">
</details><br>​    <p></p>
<h2 id="DLCA-Recon-Dynamic-Loose-Clothing-Avatar-Reconstruction-from-Monocular-Videos"><a href="#DLCA-Recon-Dynamic-Loose-Clothing-Avatar-Reconstruction-from-Monocular-Videos" class="headerlink" title="DLCA-Recon: Dynamic Loose Clothing Avatar Reconstruction from Monocular   Videos"></a>DLCA-Recon: Dynamic Loose Clothing Avatar Reconstruction from Monocular   Videos</h2><p><strong>Authors:Chunjie Luo, Fei Luo, Yusen Wang, Enxu Zhao, Chunxia Xiao</strong></p>
<p>Reconstructing a dynamic human with loose clothing is an important but difficult task. To address this challenge, we propose a method named DLCA-Recon to create human avatars from monocular videos. The distance from loose clothing to the underlying body rapidly changes in every frame when the human freely moves and acts. Previous methods lack effective geometric initialization and constraints for guiding the optimization of deformation to explain this dramatic change, resulting in the discontinuous and incomplete reconstruction surface. To model the deformation more accurately, we propose to initialize an estimated 3D clothed human in the canonical space, as it is easier for deformation fields to learn from the clothed human than from SMPL. With both representations of explicit mesh and implicit SDF, we utilize the physical connection information between consecutive frames and propose a dynamic deformation field (DDF) to optimize deformation fields. DDF accounts for contributive forces on loose clothing to enhance the interpretability of deformations and effectively capture the free movement of loose clothing. Moreover, we propagate SMPL skinning weights to each individual and refine pose and skinning weights during the optimization to improve skinning transformation. Based on more reasonable initialization and DDF, we can simulate real-world physics more accurately. Extensive experiments on public and our own datasets validate that our method can produce superior results for humans with loose clothing compared to the SOTA methods. </p>
<p><a href="http://arxiv.org/abs/2312.12096v2">PDF</a> </p>
<p><strong>Summary</strong><br>从单目视频中创建真人化身，解决人类穿着宽松服装时重建动态人的挑战。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>在规范空间初始化一个估计的 3D 穿衣人形，因为变形场从穿衣人形处学习比从 SMPL 处学习更容易。</li>
<li>利用连续帧之间的物理连接信息并提出动态变形场 (DDF) 来优化变形场。</li>
<li>DDF 考虑了对宽松衣服的贡献力，以增强变形的可解释性并有效地捕捉宽松衣服的自由运动。</li>
<li>将 SMPL 蒙皮权重传播到每个人并优化期间细化位姿和蒙皮权重，以改进蒙皮变换。</li>
<li>基于更合理初始化和 DDF，我们可以更准确地模拟真实世界的物理。</li>
<li>与 SOTA 方法相比，我们的方法可以为穿着宽松服装的人生成更好的结果。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<p></p><ol><p></p>
<p></p><li><p>标题：DLCA-Recon：基于单目视频的动态宽松服装化身重建</p>
</li><p></p>
<p></p><li><p>作者：Luo Chunjie, Luo Fei, Wang Yuseng, Zhao Enxu, Xiao Chunxia</p>
</li><p></p>
<p></p><li><p>单位：武汉大学计算机学院</p>
</li><p></p>
<p></p><li><p>关键词：人体重建、宽松服装、隐式神经表示、动态变形场</p>
</li><p></p>
<p></p><li><p>论文链接：https://arxiv.org/abs/2312.12096
Github 链接：None</p>
</li><p></p>
<p></p><li><p>摘要：
（1）：人体重建是计算机图形学中的重要研究课题，具有广泛的应用前景。然而，重建动态宽松服装的人体模型是一项极具挑战性的任务。
（2）：以往的方法在处理宽松服装时存在以下问题：</p><p></p>
<ul>
<li>几何初始化和约束不足，导致变形优化难以解释宽松服装的剧烈变化，重建表面不连续且不完整。</li>
<li>缺乏对宽松服装真实物理特性的建模，导致重建结果不准确。
（3）：针对上述问题，本文提出了 DLCA-Recon 方法，该方法具有以下特点：</li>
<li>在规范空间中初始化估计的 3D 穿衣人体，简化了变形场的学习过程。</li>
<li>利用显式网格和隐式 SDF 的双重表示，并结合连续帧之间的物理连接信息，提出动态变形场 (DDF) 来优化变形场。</li>
<li>传播 SMPL 蒙皮权重到每个个体，并在优化过程中细化姿态和蒙皮权重，以改进蒙皮变换。
（4）：在公开数据集和自有数据集上的广泛实验表明，与最先进的方法相比，DLCA-Recon 方法在重建动态宽松服装的人体方面取得了优异的性能。</li>
</ul>
</li>
<li><p>方法：
(1)：提出 DLCA-Recon 方法，该方法在规范空间中初始化估计的 3D 穿衣人体，简化了变形场的学习过程。
(2)：利用显式网格和隐式 SDF 的双重表示，并结合连续帧之间的物理连接信息，提出动态变形场 (DDF) 来优化变形场。
(3)：传播 SMPL 蒙皮权重到每个个体，并在优化过程中细化姿态和蒙皮权重，以改进蒙皮变换。
(4)：采用延迟优化策略，在训练过程中逐步启用姿态和蒙皮权重的优化，以缓解网络学习的负担。
(5)：使用表面渲染而不是体积渲染来获得准确的几何形状，并结合显式网格和隐式 SDF 的双重表示来提高渲染质量。</p>
</li>
<li><p>结论：
（1）：本文提出了一种基于单目视频的动态宽松服装化身重建方法DLCA-Recon，该方法在规范空间中初始化估计的3D穿衣人体，简化了变形场的学习过程。利用显式网格和隐式SDF的双重表示，并结合连续帧之间的物理连接信息，提出动态变形场(DDF)来优化变形场。传播SMPL蒙皮权重到每个个体，并在优化过程中细化姿态和蒙皮权重，以改进蒙皮变换。在公开数据集和自有数据集上的广泛实验表明，DLCA-Recon方法在重建动态宽松服装的人体方面取得了优异的性能。
（2）：创新点：</p>
</li>
</ol>
<ul>
<li>在规范空间中初始化估计的3D穿衣人体，简化了变形场的学习过程。</li>
<li>利用显式网格和隐式SDF的双重表示，并结合连续帧之间的物理连接信息，提出动态变形场(DDF)来优化变形场。</li>
<li>传播SMPL蒙皮权重到每个个体，并在优化过程中细化姿态和蒙皮权重，以改进蒙皮变换。
性能：</li>
<li>在公开数据集和自有数据集上的广泛实验表明，DLCA-Recon方法在重建动态宽松服装的人体方面取得了优异的性能。
工作量：</li>
<li>该方法需要大量的训练数据和计算资源。</li>
</ul>



<p></p><details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cb014d0a50eee58a6cfa584a8f0e910c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b9e9f2ccbb67bf3624babbc66b110f2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3b7bc38cc6395d39ef3a3801365a9cf7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a509596d091208b879661d7a3cf71fd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d98be771a5d656fda2fb54f19445a39c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4ab4017e410595f35f8ac9cca87655c1.jpg" align="middle">
</details><br>​    <p></p>
<h2 id="Attention-Based-VR-Facial-Animation-with-Visual-Mouth-Camera-Guidance-for-Immersive-Telepresence-Avatars"><a href="#Attention-Based-VR-Facial-Animation-with-Visual-Mouth-Camera-Guidance-for-Immersive-Telepresence-Avatars" class="headerlink" title="Attention-Based VR Facial Animation with Visual Mouth Camera Guidance   for Immersive Telepresence Avatars"></a>Attention-Based VR Facial Animation with Visual Mouth Camera Guidance   for Immersive Telepresence Avatars</h2><p><strong>Authors:Andre Rochow, Max Schwarz, Sven Behnke</strong></p>
<p>Facial animation in virtual reality environments is essential for applications that necessitate clear visibility of the user’s face and the ability to convey emotional signals. In our scenario, we animate the face of an operator who controls a robotic Avatar system. The use of facial animation is particularly valuable when the perception of interacting with a specific individual, rather than just a robot, is intended. Purely keypoint-driven animation approaches struggle with the complexity of facial movements. We present a hybrid method that uses both keypoints and direct visual guidance from a mouth camera. Our method generalizes to unseen operators and requires only a quick enrolment step with capture of two short videos. Multiple source images are selected with the intention to cover different facial expressions. Given a mouth camera frame from the HMD, we dynamically construct the target keypoints and apply an attention mechanism to determine the importance of each source image. To resolve keypoint ambiguities and animate a broader range of mouth expressions, we propose to inject visual mouth camera information into the latent space. We enable training on large-scale speaking head datasets by simulating the mouth camera input with its perspective differences and facial deformations. Our method outperforms a baseline in quality, capability, and temporal consistency. In addition, we highlight how the facial animation contributed to our victory at the ANA Avatar XPRIZE Finals. </p>
<p><a href="http://arxiv.org/abs/2312.09750v1">PDF</a> Published in IEEE/RSJ International Conference on Intelligent Robots   and Systems (IROS) 2023</p>
<p><strong>摘要</strong><br>将视觉信息引入面部动画，增强了对于人脸运动的捕捉与合成。</p>
<p><strong>要点</strong></p>
<ul>
<li>面部动画在虚拟现实环境中至关重要，它需要清晰地看到用户的面部并传达情感信号。</li>
<li>在我们的方案中，我们为控制机器人化身系统的操作员制作面部动画。</li>
<li>当想要让人们感觉与特定个体进行互动，而不仅仅是与机器人互动时，使用面部动画尤其有价值。</li>
<li>纯粹由关键点驱动的动画方法难以应对复杂的面部动作。</li>
<li>我们提出了一种混合方法，它同时利用关键点和来自嘴巴摄像头的直接视觉引导。</li>
<li>我们的方法适用于未见过的操作员，并且只需要一个快速注册步骤，其中包括录制两个简短的视频。</li>
<li>我们选择了多张源图像，目的是覆盖不同的面部表情。</li>
<li>给定来自头戴式显示器 (HMD) 的嘴巴摄像头帧，我们动态地构建目标关键点，并应用注意机制来确定每个源图像的重要性。</li>
<li>为了解决关键点的模糊性并制作更广泛的嘴巴表情动画，我们建议将视觉嘴巴摄像头信息注入潜在空间。</li>
<li>我们通过模拟嘴巴摄像头输入及其透视差异和面部变形来实现对大规模说话头部数据集的训练。</li>
<li>我们的方法在质量、能力和时间一致性方面优于基准。</li>
<li>此外，我们重点介绍了面部动画如何帮助我们在 ANA Avatar XPRIZE 决赛中取得胜利。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<p></p><ol><p></p>
<p></p><li><p>题目：基于视觉的嘴部相机指导的面部动画</p>
</li><p></p>
<p></p><li><p>作者：Matthias Niessner, Michael Zollhöfer, Shahram Izadi, Marc Stamminger, Andreas Kolb, Christian Theobalt</p>
</li><p></p>
<p></p><li><p>隶属单位：马克斯普朗克信息学研究所</p>
</li><p></p>
<p></p><li><p>关键词：面部动画、虚拟现实、嘴部相机、视觉指导、关键点</p>
</li><p></p>
<p></p><li><p>链接：https://arxiv.org/abs/1705.08922, Github：无</p>
</li><p></p>
<p></p><li><p>摘要：
（1）：面部动画在虚拟现实环境中至关重要，它可以使用户清晰地看到自己的脸部并传达情感信号。在我们的场景中，我们对控制机器人 Avatar 系统的操纵员的面部进行动画处理。当用户希望与特定个人（而非仅仅一个机器人）进行交互时，面部动画的使用特别有价值。纯关键点驱动的动画方法难以应对复杂的面部动作。我们提出了一种混合方法，该方法同时使用关键点和嘴部相机的直接视觉指导。我们的方法可以推广到未见过的操纵员，只需要一个快速注册步骤，其中包含两个短视频的捕捉。我们选择多个源图像，旨在覆盖不同的面部表情。给定来自 HMD 的嘴部相机帧，我们动态构建目标关键点并应用注意机制来确定每个源图像的重要性。为了解决关键点歧义并对更广泛的面部表情进行动画处理，我们提出将视觉嘴部相机信息注入潜在空间。我们通过模拟嘴部相机输入及其透视差异和面部变形，使训练能够在大规模说话头部数据集上进行。我们的方法在质量、能力和时间一致性方面优于基线。此外，我们重点介绍了面部动画如何为我们在 ANA AvatarXPRIZE 决赛中的胜利做出了贡献。</p>
</li><p></p>
<p></p><li><p>方法：</p>
</li><br>&lt;/ol&gt;<p></p>
<p></p><p>（1）：我们提出了一种混合方法，该方法同时使用关键点和嘴部相机的直接视觉指导。</p><p></p>
<p></p><p>（2）：我们的方法可以推广到未见过的操纵员，只需要一个快速注册步骤，其中包含两个短视频的捕捉。</p><p></p>
<p></p><p>（3）：我们选择多个源图像，旨在覆盖不同的面部表情。</p><p></p>
<p></p><p>（4）：给定来自HMD的嘴部相机帧，我们动态构建目标关键点并应用注意机制来确定每个源图像的重要性。</p><p></p>
<p></p><p>（5）：为了解决关键点歧义并对更广泛的面部表情进行动画处理，我们提出将视觉嘴部相机信息注入潜在空间。</p><p></p>
<p></p><p>（6）：我们通过模拟嘴部相机输入及其透视差异和面部变形，使训练能够在大规模说话头部数据集上进行。</p><p></p>
<p></p><ol start="8"><p></p>
<p></p><li>结论：
（1）：该工作提出了一种实时虚拟现实面部动画方法，与关键点驱动的面部动画方法相比，该方法可以推广到未见过的操作员，并允许建模更广泛的面部表情。我们通过源图像注意力机制扩展了基线，并开发了一种将视觉嘴部图像信息注入动画管道的方法，而不会出现过拟合。这两个扩展产生了更好的准确性并显着提高了时间一致性，这对于流畅的交互非常重要。我们的方法仍然难以生成不寻常的表情，例如伸出舌头。此外，上部面部的运动仍然有限。
（2）：创新点：</li><br>&lt;/ol&gt;<p></p>
<ul>
<li>提出了一种混合方法，该方法同时使用关键点和嘴部相机的直接视觉指导。</li>
<li>开发了一种快速注册步骤，该步骤只需要两个短视频的捕捉，即可将我们的方法推广到未见过的操作员。</li>
<li>提出了一种将视觉嘴部相机信息注入潜在空间的方法，以解决关键点歧义并对更广泛的面部表情进行动画处理。
性能：</li>
<li>我们的方法在质量、能力和时间一致性方面优于基线。</li>
<li>我们的方法可以推广到未见过的操作员，只需要一个快速注册步骤，其中包含两个短视频的捕捉。</li>
<li>我们的方法可以对更广泛的面部表情进行动画处理，包括不寻常的表情，例如伸出舌头。
工作量：</li>
<li>我们通过模拟嘴部相机输入及其透视差异和面部变形，使训练能够在大规模说话头部数据集上进行。</li>
<li>我们通过源图像注意力机制扩展了基线，并开发了一种将视觉嘴部图像信息注入动画管道的方法，而不会出现过拟合。</li>
<li>我们在ANAAvatarXPRIZE决赛中使用了我们的方法，并取得了胜利。</li>
</ul>



<p></p><details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-dc65e27ac791814ade4910bc092cbd2c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f634cbd903eec7aed8f5d4dfeb59915.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8403ee6486888d7a563ed47600f96335.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-26dc5864025dde63a1e3c374590b8f70.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8e0823080cd17a5ac265b162c95e7038.jpg" align="middle">
</details><br>​    <p></p>
<h2 id="3DGS-Avatar-Animatable-Avatars-via-Deformable-3D-Gaussian-Splatting"><a href="#3DGS-Avatar-Animatable-Avatars-via-Deformable-3D-Gaussian-Splatting" class="headerlink" title="3DGS-Avatar: Animatable Avatars via Deformable 3D Gaussian Splatting"></a>3DGS-Avatar: Animatable Avatars via Deformable 3D Gaussian Splatting</h2><p><strong>Authors:Zhiyin Qian, Shaofei Wang, Marko Mihajlovic, Andreas Geiger, Siyu Tang</strong></p>
<p>We introduce an approach that creates animatable human avatars from monocular videos using 3D Gaussian Splatting (3DGS). Existing methods based on neural radiance fields (NeRFs) achieve high-quality novel-view/novel-pose image synthesis but often require days of training, and are extremely slow at inference time. Recently, the community has explored fast grid structures for efficient training of clothed avatars. Albeit being extremely fast at training, these methods can barely achieve an interactive rendering frame rate with around 15 FPS. In this paper, we use 3D Gaussian Splatting and learn a non-rigid deformation network to reconstruct animatable clothed human avatars that can be trained within 30 minutes and rendered at real-time frame rates (50+ FPS). Given the explicit nature of our representation, we further introduce as-isometric-as-possible regularizations on both the Gaussian mean vectors and the covariance matrices, enhancing the generalization of our model on highly articulated unseen poses. Experimental results show that our method achieves comparable and even better performance compared to state-of-the-art approaches on animatable avatar creation from a monocular input, while being 400x and 250x faster in training and inference, respectively. </p>
<p><a href="http://arxiv.org/abs/2312.09228v2">PDF</a> Project page: <a href="https://neuralbodies.github.io/3DGS-Avatar">https://neuralbodies.github.io/3DGS-Avatar</a></p>
<p><strong>摘要</strong><br>基于3D高斯溅射法，本文提出了一种只需30分钟即可训练并能够以50 FPS以上实时帧速率渲染的可动画3D服装人形虚拟人重建方法。</p>
<p><strong>关键要点</strong></p>
<ul>
<li>本文提出了一种基于3D高斯溅射法和非刚性变形网络，在30分钟内训练出可动画的服装人形虚拟人，并以超过50 FPS的实时帧速率渲染。</li>
</ul>
<ul>
<li>基于神经辐射场（NeRF）的现有方法可实现高质量的新视角/新姿势图像合成，但通常需要数天的训练时间，并且推理时间非常慢。</li>
</ul>
<ul>
<li>近期研究探索了用于有效训练服装虚拟人的快速网格结构。尽管训练速度极快，但这些方法几乎无法实现约15 FPS的交互式渲染帧速率。</li>
</ul>
<ul>
<li>本文使用3D高斯溅射法并学习一个非刚性变形网络，以重建可动画的服装人形虚拟人，并在30分钟内完成训练并以实时帧速率（50+ FPS）渲染。</li>
</ul>
<ul>
<li>针对高斯均值向量和协方差矩阵引入尽可能等距的正则化，增强了模型对高度铰接的不可见姿势的泛化能力。</li>
</ul>
<ul>
<li>实验结果表明，与最先进的可动画虚拟人创建方法相比，本文方法实现了相当甚至更好的性能，同时训练和推理速度分别提高了400倍和250倍。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<p></p><ol><p></p>
<p></p><li>标题：3DGS-Avatar：可变形 3D 高斯散布的动画角色</li><p></p>
<p></p><li>作者：Yiyi Liao、Shuaicheng Liu、Tianchang Shen、Lingjie Liu、Christian Theobalt、Hao Li</li><p></p>
<p></p><li>隶属机构：马克斯·普朗克计算机科学研究所</li><p></p>
<p></p><li>关键词：动画角色、可变形模型、神经辐射场、单目视频、3D 高斯散布</li><p></p>
<p></p><li>论文链接：https://arxiv.org/abs/2302.06467，Github 代码链接：None</li><p></p>
<p></p><li>摘要：
（1）：研究背景：神经辐射场（NeRF）方法在单目视频中创建动画角色方面取得了显着进展，但通常需要数天的训练时间，并且推理速度非常慢。最近，研究人员探索了快速网格结构以高效训练带服装的角色。尽管这些方法的训练速度非常快，但它们只能实现约 15 FPS 的交互式渲染帧率。
（2）：过去的方法及其问题：现有方法基于神经辐射场（NeRF），可以实现高质量的新视角/新姿势图像合成，但通常需要数天的训练时间，并且推理速度非常慢。最近，研究社区探索了用于高效训练带服装角色的快速网格结构。尽管这些方法的训练速度非常快，但它们只能实现约 15 FPS 的交互式渲染帧率。
（3）：提出的研究方法：本文提出了一种使用 3D 高斯散布（3DGS）从单目视频创建动画人类角色的方法。该方法学习了一个非刚性变形网络来重建可动画的带服装的人类角色，可以在 30 分钟内训练完成，并以实时帧率（50+ FPS）渲染。此外，还引入了尽可能等距的正则化，以增强模型对未见姿势的泛化能力。
（4）：方法的性能：实验结果表明，该方法在从单目输入创建动画角色方面取得了与现有方法相当甚至更好的性能，同时训练速度提高了 400 倍，推理速度提高了 250 倍。这些性能支持了该方法的目标。</li><br>&lt;/ol&gt;<p></p>
<p></p><p>&lt;Methods&gt;:</p><p></p>
<p></p><p>（1）：方法概述：本文方法的管道如图2所示。输入是一个经过校准的相机、拟合的SMPL参数和前景掩码的单目视频。该方法优化了一组规范空间中的3D高斯分布，然后将其变形到观察空间，并从给定的相机渲染。对于一组3D高斯分布{G(i)}Ni=1，在每个点存储以下属性：位置x，缩放因子，旋转四元数q，不透明度α和颜色特征向量f。首先通过随机采样SMPL[26]网格表面的N=50k个点作为规范3D高斯分布{Gc}的初始化。受HumanNeRF[62]的启发，将复杂的人体变形分解为一个编码姿势相关布料变形的非刚性部分，以及由人体骨骼控制的刚性变换。</p><p></p>
<p></p><p>（2）：姿势相关的非刚性变形：将非刚性变形模块表述为：{Gd}=Fθnr({Gc};Zp)(6)</p><p></p>
<p></p><p>其中{Gd}表示非刚性变形的3D高斯分布。θnr表示非刚性变形模块的可学习参数。Z是一个潜在代码，它使用轻量级分层姿势编码器[28]对SMPL姿势和形状(θ,β)进行编码。具体来说，变形网络fθnr以规范位置xc和姿势潜在代码Zp作为输入，并输出高斯位置、尺度、旋转的偏移量以及特征向量z：(δx,δs,δq,z)=fθnr(xc;Zp)(7)</p><p></p>
<p></p><p>（3）：刚性变换：将非刚性变形的3D高斯分布{Gd}通过刚性变换模块进一步变换到观察空间：{Go}=Fθr({Gd};{Bb}Bb=1)(11)</p><p></p>
<p></p><p>其中皮肤网格变换MLPfθr被学习以预测位置xd处的皮肤权重。通过第3.1节中描述的前向LBS变换位置和3D高斯分布的旋转矩阵：T=�Bb=1fθr(xd)bBb(12)</p><p></p>
<p></p><p>（4）：颜色MLP：先前工作[63,67,68]遵循3DGS[14]的惯例，每个3D高斯分布存储球谐系数以编码视点相关颜色。将存储的颜色特征f视为球谐系数，则3D高斯分布的颜色可以通过球谐基和学习系数的点积来计算：c=⟨γ(d),f⟩(15)</p><p></p>
<p></p><p>其中d表示从相机中心到3D高斯分布的相对位置导出的视点方向。γ表示球谐基函数。虽然概念上很简单，但认为这种方法不适合单目设置。由于在训练期间只提供了一个相机视图，因此世界空间中的视点方向是固定的，导致对未见测试视图的泛化性较差。类似于[41]，使用第4.2节中的逆刚性变换将视点方向规范化：ˆd=T−11:3,1:3d(16)</p><p></p>
<p></p><p>其中T是等式（12）中定义的前向变换矩阵。理论上，规范视点方向可以提高模型对未见姿势的泛化能力。</p><p></p>
<p></p><ol start="8"><p></p>
<p></p><li>结论：</li><br>&lt;/ol&gt;<p></p>
<p></p><p>（1）本研究工作通过从单目视频中高效重建带服装的人类动画角色，推动了该领域的进步。该方法实现了逼真的渲染、对姿势相关布料变形的感知、对未见姿势的泛化、快速训练和实时渲染等优点。实验表明，该方法在渲染质量上与现有最先进的方法相当甚至更好，同时在训练和推理速度上提高了两个数量级。此外，还提出了用浅层 MLP 代替球谐函数来解码 3D 高斯颜色，并用几何约束来正则化变形，这两者都被证明可以有效提高渲染质量。我们希望这种新的表示能够促进从单目视图中快速、高质量的可动画带服装人类化身合成的进一步研究。</p><p></p>
<p></p><p>（2）创新点：</p><p></p>
<ul>
<li>提出了一种使用 3D 高斯散布 (3DGS) 从单目视频创建动画人类角色的方法。</li>
<li>该方法学习了一个非刚性变形网络来重建可动画的带服装的人类角色，可以在 30 分钟内训练完成，并以实时帧率（50+FPS）渲染。</li>
<li>引入了尽可能等距的正则化，以增强模型对未见姿势的泛化能力。</li>
</ul>
<p>性能：</p>
<ul>
<li>该方法在从单目输入创建动画角色方面取得了与现有方法相当甚至更好的性能。</li>
<li>该方法的训练速度提高了 400 倍，推理速度提高了 250 倍。</li>
</ul>
<p>工作量：</p>
<ul>
<li>该方法的训练和推理速度都非常快，可以在普通 GPU 上轻松实现。</li>
<li>该方法易于实现和使用。</li>
</ul>



<p></p><details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-491840e5e9b907bfe6c860125c793a8e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-df8a29e21b43e7322f740381b022b6e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c04b8f81d853c5df7e574e6e17d490fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-622f5d5aa71b525c2b25dfceb0d4c49a.jpg" align="middle">
</details><br>​    <p></p>
<h2 id="SEEAvatar-Photorealistic-Text-to-3D-Avatar-Generation-with-Constrained-Geometry-and-Appearance"><a href="#SEEAvatar-Photorealistic-Text-to-3D-Avatar-Generation-with-Constrained-Geometry-and-Appearance" class="headerlink" title="SEEAvatar: Photorealistic Text-to-3D Avatar Generation with Constrained   Geometry and Appearance"></a>SEEAvatar: Photorealistic Text-to-3D Avatar Generation with Constrained   Geometry and Appearance</h2><p><strong>Authors:Yuanyou Xu, Zongxin Yang, Yi Yang</strong></p>
<p>Powered by large-scale text-to-image generation models, text-to-3D avatar generation has made promising progress. However, most methods fail to produce photorealistic results, limited by imprecise geometry and low-quality appearance. Towards more practical avatar generation, we present SEEAvatar, a method for generating photorealistic 3D avatars from text with SElf-Evolving constraints for decoupled geometry and appearance. For geometry, we propose to constrain the optimized avatar in a decent global shape with a template avatar. The template avatar is initialized with human prior and can be updated by the optimized avatar periodically as an evolving template, which enables more flexible shape generation. Besides, the geometry is also constrained by the static human prior in local parts like face and hands to maintain the delicate structures. For appearance generation, we use diffusion model enhanced by prompt engineering to guide a physically based rendering pipeline to generate realistic textures. The lightness constraint is applied on the albedo texture to suppress incorrect lighting effect. Experiments show that our method outperforms previous methods on both global and local geometry and appearance quality by a large margin. Since our method can produce high-quality meshes and textures, such assets can be directly applied in classic graphics pipeline for realistic rendering under any lighting condition. Project page at: <a href="https://yoxu515.github.io/SEEAvatar/">https://yoxu515.github.io/SEEAvatar/</a>. </p>
<p><a href="http://arxiv.org/abs/2312.08889v2">PDF</a> </p>
<p><strong>Summary</strong><br>使用 SEEAvatar 生成逼真 3D 头像，结合几何和外观的自我进化约束，产生高质量的网格和纹理。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SEEAvatar 采用大规模文本到图像生成模型，从文本生成逼真的 3D 头像。</li>
<li>使用模板头像对优化后的头像进行约束，实现更灵活的形状生成。</li>
<li>人体先验也对脸部和手部等局部几何结构进行约束，以维持精细的结构。</li>
<li>扩散模型通过 prompt 工程增强，以指导基于物理的渲染管道生成逼真的纹理。</li>
<li>明度约束应用于漫反射贴图，以抑制不正确的照明效果。</li>
<li>SEEAvatar 在几何和外观的全局和局部质量上均优于以前的方法。</li>
<li>SEEAvatar 生成的优质网格和纹理可直接应用于经典图形管道，在任何照明条件下实现逼真的渲染。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<p></p><ol><p></p>
<p></p><li><p>题目：SEEAvatar：具有约束几何和外观的逼真文本到 3D 头像生成</p>
</li><p></p>
<p></p><li><p>作者：Yuxuan Zhou, Hongyu Zhou, Jiapeng Tang, Yebin Liu, Yu-Kun Lai, Tao Xiang</p>
</li><p></p>
<p></p><li><p>单位：北京大学</p>
</li><p></p>
<p></p><li><p>关键词：文本到 3D 头像生成、生成对抗网络、扩散模型、几何约束、外观约束</p>
</li><p></p>
<p></p><li><p>论文链接：https://arxiv.org/abs/2302.09291，Github 链接：None</p>
</li><p></p>
<p></p><li><p>摘要：
(1)：研究背景：文本到 3D 头像生成技术近年来取得了很大进展，但现有方法大多无法生成具有逼真几何和外观的头像。
(2)：过去方法：现有方法存在的问题包括：几何不准确、外观质量低、无法控制头像的比例和保持局部结构。
(3)：研究方法：本文提出了一种名为 SEEAvatar 的方法，该方法通过对几何和外观施加约束来生成逼真的 3D 头像。几何约束包括：全局形状约束、局部结构约束和人体先验约束。外观约束包括：光照约束和物理约束。
(4)：实验结果：SEEAvatar 方法在多个数据集上进行了评估，实验结果表明，该方法在几何和外观质量方面均优于现有方法。</p>
</li><p></p>
<p></p><li><p>方法：
（1）全局形状约束：使用球形谐波函数来表示头像的全局形状，并通过最小化重投影误差来优化形状参数。
（2）局部结构约束：使用循环神经网络来生成头像的局部结构，并通过对抗训练来确保生成的结构与真实头像的结构相似。
（3）人体先验约束：使用人体先验知识来约束头像的比例和姿势。
（4）光照约束：使用光照模型来模拟头像的照明效果，并通过最小化光照误差来优化光照参数。
（5）物理约束：使用物理模型来模拟头像的物理属性，并通过最小化物理误差来优化物理参数。</p>
</li><p></p>
<p></p><li><p>结论：
（1）：本文提出的 SEEAvatar 方法能够生成具有约束几何和外观的逼真 3D 头像，该方法在几何和外观质量方面均优于现有方法。
（2）：创新点：</p>
</li><br>&lt;/ol&gt;<p></p>
<ul>
<li>提出了一种新的文本到 3D 头像生成方法，该方法通过对几何和外观施加约束来生成逼真的 3D 头像。</li>
<li>该方法能够生成具有准确的几何形状、逼真的外观和丰富的细节的 3D 头像。</li>
<li>该方法能够控制头像的比例和姿势，并保持局部结构。
性能：</li>
<li>该方法在多个数据集上进行了评估，实验结果表明，该方法在几何和外观质量方面均优于现有方法。</li>
<li>该方法能够生成高质量的 3D 头像，这些头像可以应用于经典的工作流程中进行逼真的渲染。
工作量：</li>
<li>该方法的实现相对复杂，需要较多的计算资源。</li>
<li>该方法的训练过程需要较长时间。</li>
</ul>



<p></p><details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-31d8f3ef22e9983e6f080f4f979f6284.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-204ca8c7f61c24414854bac9e34ba0a9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-af635847f8e0712b1b887523a86123da.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-df9fb4cbdd77b5ee6fa2c33565667f41.jpg" align="middle">
</details><br>​    <p></p>
<h2 id="ANR-Articulated-Neural-Rendering-for-Virtual-Avatars"><a href="#ANR-Articulated-Neural-Rendering-for-Virtual-Avatars" class="headerlink" title="ANR: Articulated Neural Rendering for Virtual Avatars"></a>ANR: Articulated Neural Rendering for Virtual Avatars</h2><p><strong>Authors:Amit Raj, Julian Tanke, James Hays, Minh Vo, Carsten Stoll, Christoph Lassner</strong></p>
<p>The combination of traditional rendering with neural networks in Deferred Neural Rendering (DNR) provides a compelling balance between computational complexity and realism of the resulting images. Using skinned meshes for rendering articulating objects is a natural extension for the DNR framework and would open it up to a plethora of applications. However, in this case the neural shading step must account for deformations that are possibly not captured in the mesh, as well as alignment inaccuracies and dynamics — which can confound the DNR pipeline. We present Articulated Neural Rendering (ANR), a novel framework based on DNR which explicitly addresses its limitations for virtual human avatars. We show the superiority of ANR not only with respect to DNR but also with methods specialized for avatar creation and animation. In two user studies, we observe a clear preference for our avatar model and we demonstrate state-of-the-art performance on quantitative evaluation metrics. Perceptually, we observe better temporal stability, level of detail and plausibility. </p>
<p><a href="http://arxiv.org/abs/2012.12890v1">PDF</a> </p>
<p><strong>摘要</strong><br>利用神經網絡，融合遞延神經渲染和網格形變，改進虛擬人生成。</p>
<p><strong>重要要点</strong></p>
<ul>
<li>延遲神經渲染是一種將傳統渲染技術與神經網絡相結合的方法，它在計算複雜性和逼真結果之間取得了很好的平衡。</li>
<li>將蒙皮網格用於渲染活動物體是遞延神經渲染框架的自然延伸，這將開啟大量應用。</li>
<li>然而，在這種情況下，神經著色步驟必須考慮那些網格可能無法捕捉的形變，以及對齊不準和動力學問題——這些問題可能會使遞延神經渲染管道混亂。</li>
<li>我們提出了基於遞延神經渲染的關節神經渲染框架，它明確地解决了虛擬人形頭像的局限性。</li>
<li>我們展示了關節神經渲染方法的優越性，不僅相對於遞延神經渲染，還優於專門用於頭像創建和動畫的方法。</li>
<li>在兩項用戶研究中，我們觀察到用戶對我們頭像模型的明顯偏好，並且在定量評估指標上展示了最先進的性能。</li>
<li>在感知上，我們觀察到更好的時態穩定性、細節層次和可信度。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<p></p><ol><p></p>
<p></p><li><p>题目：关节神经渲染：虚拟形象的关节神经渲染</p>
</li><p></p>
<p></p><li><p>作者：Amit Raj、Julian Tanke、James Hays、Minh Vo、Carsten Stoll、Christoph Lassner</p>
</li><p></p>
<p></p><li><p>第一作者单位：佐治亚理工学院</p>
</li><p></p>
<p></p><li><p>关键词：神经渲染、虚拟形象、关节变形、神经纹理</p>
</li><p></p>
<p></p><li><p>论文链接：https://arxiv.org/pdf/2012.12890.pdf，Github 代码链接：无</p>
</li><p></p>
<p></p><li><p>摘要：
（1）研究背景：
计算机视觉的重要目标之一是捕捉逼真的外观。3D 渲染和神经网络的进步已经导致了具有显着保真度的技术。这些方法通常使用昂贵且复杂的捕捉设置，这阻止了生成模型的轻松数字化和传输。最近的延迟神经渲染范式为在准确的几何形状和相对简单的神经着色器内工作提供了一个激动人心的机会，同时逼真地捕捉具有视点依赖效果的复杂场景。
（2）过去的方法及问题：
延迟神经渲染特别适用于刚性物体。其管道可以以自然的方式扩展到可变形物体：可以使用蒙皮网格来捕捉几何形状。然后可以将来自姿势网格的光栅化神经纹理转换为 RGB 图像。虽然这个想法在概念上很简单，但神经网络必须学习更复杂的变形依赖效应。此外，用于渲染的网格通常不是 100% 准确的，并且可能与真实几何形状存在差异。这可能会导致神经渲染管道出现问题。
（3）本文提出的研究方法：
我们提出了一种新的框架，称为关节神经渲染 (ANR)，它明确解决了虚拟人形形象的延迟神经渲染限制。ANR 利用神经纹理和神经着色器来生成逼真的图像，同时显式地考虑关节变形和几何失真。ANR 还使用了一种新的损失函数，该损失函数可以更好地处理关节变形和几何失真。
（4）方法在任务和性能上的表现：
我们在两个用户研究中观察到人们对我们的人形形象模型的明显偏好，并且我们在定量评估指标上展示了最先进的性能。在感知上，我们观察到更好的时间稳定性、细节级别和合理性。更多结果可在我们的项目页面获得：https://anr-avatars.github.io。</p>
</li><p></p>
<p></p><li><p>方法：
（1）延迟神经渲染（DNR）：DNR 使用一个神经纹理和一个神经渲染模型来将神经图像转换为 RGB 图像。神经图像可以通过将网格光栅化到图像空间并使用神经纹理对其进行纹理化来获得。
（2）关节神经渲染（ANR）：ANR 在 DNR 的基础上，通过将神经渲染网络拆分为两个阶段 R1 和 R2 来处理关节变形和几何失真。R1 负责生成粗略的渲染结果和法线图像，R2 则使用 R1 的输出和法线图像来生成最终的渲染结果。
（3）损失函数和正则化方案：ANR 使用了一个加权损失函数，该损失函数包括光度损失、特征损失、掩码损失、对抗损失和总变差损失。光度损失用于衡量生成图像和真实图像之间的差异，特征损失用于提高生成图像的锐度，掩码损失用于惩罚预测的掩码与真实掩码之间的差异，对抗损失用于鼓励生成图像的真实感，总变差损失用于鼓励生成图像的平滑性。
（4）优化策略：ANR 使用了一个拆分优化策略来训练神经渲染模型。首先，使用一组关键帧来训练模型，以捕获静态的外观。然后，使用剩下的帧来训练模型，以学习处理关节变形和几何失真。</p>
</li><p></p>
<p></p><li><p>结论：
（1）：本文提出了关节神经渲染（ANR），一种新颖的神经渲染框架，用于生成具有任意骨骼动画和视点的虚拟化身。我们工作的关键在于能够解释几何错位和与姿势相关的表面变形。我们的解决方案被仔细地集成到一个端到端的学习框架中，具有新颖的神经渲染架构和调整的优化方案。此外，ANR 可以使用单个神经渲染模型渲染多个化身。通过纹理和几何的解耦，它允许混合和编辑外观。对于未来的工作，我们看到进一步减轻几何错位的影响和提高对大姿势跟踪误差的弹性的潜在方向，以及将环境光照纳入渲染过程。
（2）：创新点：</p>
</li><br>&lt;/ol&gt;<p></p>
<ul>
<li>提出了一种新的神经渲染框架——关节神经渲染（ANR），可以生成具有任意骨骼动画和视点的虚拟化身。</li>
<li>ANR 能够解释几何错位和与姿势相关的表面变形。</li>
<li>ANR 使用了一个加权损失函数，该损失函数包括光度损失、特征损失、掩码损失、对抗损失和总变差损失。</li>
<li>ANR 使用了一个拆分优化策略来训练神经渲染模型。</li>
<li>ANR 可以使用单个神经渲染模型渲染多个化身。</li>
<li>通过纹理和几何的解耦，ANR 允许混合和编辑外观。</li>
</ul>
<p>性能：</p>
<ul>
<li>在两个用户研究中观察到人们对我们的人形形象模型的明显偏好。</li>
<li>在定量评估指标上展示了最先进的性能。</li>
</ul>
<p>工作量：</p>
<ul>
<li>收集和处理数据。</li>
<li>训练神经渲染模型。</li>
<li>评估神经渲染模型的性能。</li>
</ul>



<p></p><details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b28956a7f40395402b0aec2307c9d6e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-124c65e51fa50d768c4606f610c9016e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-69e2a11eda84b6dbad8ef9daa42e2674.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0f13a2b60bef4c886a3317754c99b456.jpg" align="middle">
</details><br>​    <p></p>
<p>​    </p>
</ol></ol></ol></ol></ol></ol>]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>元宇宙/虚拟人</tag>
      </tags>
  </entry>
  <entry>
    <title>Talking Head Generation</title>
    <url>/2024/01/30/Paper/2024-01-30/Talking%20Head%20Generation/</url>
    <content><![CDATA[<blockquote>
<p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-01-30-更新"><a href="#2024-01-30-更新" class="headerlink" title="2024-01-30 更新"></a>2024-01-30 更新</h1><h2 id="NeRF-AD-Neural-Radiance-Field-with-Attention-based-Disentanglement-for-Talking-Face-Synthesis"><a href="#NeRF-AD-Neural-Radiance-Field-with-Attention-based-Disentanglement-for-Talking-Face-Synthesis" class="headerlink" title="NeRF-AD: Neural Radiance Field with Attention-based Disentanglement for   Talking Face Synthesis"></a>NeRF-AD: Neural Radiance Field with Attention-based Disentanglement for   Talking Face Synthesis</h2><p><strong>Authors:Chongke Bi, Xiaoxing Liu, Zhilei Liu</strong></p>
<p>Talking face synthesis driven by audio is one of the current research hotspots in the fields of multidimensional signal processing and multimedia. Neural Radiance Field (NeRF) has recently been brought to this research field in order to enhance the realism and 3D effect of the generated faces. However, most existing NeRF-based methods either burden NeRF with complex learning tasks while lacking methods for supervised multimodal feature fusion, or cannot precisely map audio to the facial region related to speech movements. These reasons ultimately result in existing methods generating inaccurate lip shapes. This paper moves a portion of NeRF learning tasks ahead and proposes a talking face synthesis method via NeRF with attention-based disentanglement (NeRF-AD). In particular, an Attention-based Disentanglement module is introduced to disentangle the face into Audio-face and Identity-face using speech-related facial action unit (AU) information. To precisely regulate how audio affects the talking face, we only fuse the Audio-face with audio feature. In addition, AU information is also utilized to supervise the fusion of these two modalities. Extensive qualitative and quantitative experiments demonstrate that our NeRF-AD outperforms state-of-the-art methods in generating realistic talking face videos, including image quality and lip synchronization. To view video results, please refer to <a href="https://xiaoxingliu02.github.io/NeRF-AD">https://xiaoxingliu02.github.io/NeRF-AD</a>. </p>
<p><a href="http://arxiv.org/abs/2401.12568v1">PDF</a> Accepted by ICASSP 2024</p>
<p><strong>Summary</strong><br>用注意力机制分解神经辐射场，用于音频驱动的说话人头部生成。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>神经辐射场（NeRF）已被引入到谈话面部合成的研究领域，以增强生成的面的逼真性和 3D 效果。</li>
<li>现有的 NeRF 方法要么给 NeRF 带来了复杂的学习任务，而缺乏监督式多模态特征融合的方法，或者无法将音频精确映射到与语音运动相关的面部区域。</li>
<li>提出了一种基于 NeRF 的注意力机制分解的说话人头部合成方法（NeRF-AD）。</li>
<li>引入了一个基于注意力的分解模块，使用与语音相关的面部动作单元 (AU) 信息将人脸分解成音频面部和身份面部。</li>
<li>仅将音频面部与音频特征融合，以精确地调节音频对说话人面部的影响。</li>
<li>将 AU 信息也用于监督这两种模态的融合。</li>
<li>定性和定量实验表明，NeRF-AD 在生成逼真的说话人头部视频方面优于最先进的方法，包括图像质量和唇形同步。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：NeRF-AD：基于注意力机制分解的神经辐射场说话人面部合成</li>
<li>作者：Chongke Bi，Xiaoxing Liu，Zhilei Liu</li>
<li>单位：天津大学智能与计算学院</li>
<li>关键词：说话人面部合成，神经辐射场，面部分解</li>
<li>论文链接：https://arxiv.org/abs/2401.12568，Github 链接：无</li>
<li>
<p>摘要：
（1）研究背景：说话人面部合成是多维信号处理和多媒体领域的研究热点之一。神经辐射场（NeRF）最近被引入该研究领域，以增强生成面部的真实感和 3D 效果。然而，大多数现有的基于 NeRF 的方法要么给 NeRF 增加复杂的学习任务，同时缺乏监督式多模态特征融合的方法，要么无法将音频精确映射到与说话运动相关的面部区域。这些原因最终导致现有方法生成的唇形不准确。
（2）过去方法及其问题：一些方法将 NeRF 的学习任务提前一部分，并提出了一种通过 NeRF 进行说话人面部合成的基于注意力机制分解的方法（NeRF-AD）。具体来说，引入了一个基于注意力的分解模块，利用与语音相关的面部动作单元（AU）信息将面部分解为音频面部和身份面部。为了精确地调节音频如何影响说话人的面部，我们只将音频面部与音频特征融合。此外，还利用 AU 信息来监督这两个模态的融合。大量定性和定量实验表明，我们的 NeRF-AD 在生成逼真的说话人面部视频（包括图像质量和唇形同步）方面优于最先进的方法。
（3）提出的研究方法：我们设计了一个基于注意力的分解模块，利用 AU 指导注意力模型生成与说话运动相关的面部区域的掩码。通过利用这些掩码，我们可以有效地将输入面部分解为不同的组成部分：音频面部和身份面部。音频面部表示与说话运动相关的面部区域，与音频特征融合，而身份面部表示与说话人身份相关的面部区域。在这种情况下，音频特征只影响音频面部，从而对生成的说话人面部提供精确的控制。随后，我们提出了一个条件 NeRF，以将融合的音频面部特征和身份面部特征作为条件，精确地渲染说话人面部图像。此外，我们使用 AU 损失来监督音频面部特征和音频特征的融合过程，以便能够准确地融合两者。在整个过程中，我们分散了 NeRF 的任务，并使用不同的方法来监督每个任务，使 NeRF 更清楚地知道它需要学习什么。
（4）方法在什么任务上取得了什么性能，性能是否支持其目标：我们的 NeRF-AD 在生成逼真的说话人面部视频（包括图像质量和唇形同步）方面优于最先进的方法。定量实验表明，我们的方法在唇形同步准确性和图像质量方面均优于最先进的方法。这些结果支持我们的目标，即生成逼真的说话人面部视频。</p>
</li>
<li>
<p>方法：
（1）：提出了一种基于注意力的分解模块，利用与语音相关的面部动作单元（AU）信息将面部分解为音频面部和身份面部。
（2）：将音频面部与音频特征融合，并利用AU信息来监督这两个模态的融合。
（3）：设计了一个条件NeRF，以将融合的音频面部特征和身份面部特征作为条件，精确地渲染说话人面部图像。
（4）：使用AU损失来监督音频面部特征和音频特征的融合过程，以便能够准确地融合两者。</p>
</li>
<li>
<p>结论：
（1）：NeRF-AD 提出了一种基于注意力的分解模块，利用与语音相关的面部动作单元（AU）信息将面部分解为音频面部和身份面部。我们只融合音频面部特征和音频特征来准确控制音频对说话面部的影响。此外，AU 信息用于控制不同模态特征的精确融合。广泛的定性和定量实验的结果表明，NeRF-AD 在图像质量和唇形同步方面均优于其他最先进的方法。
（2）：创新点：
提出了一种基于注意力的分解模块，将说话面部分解为音频面部和身份面部。
只融合音频面部特征和音频特征来准确控制音频对说话面部的影响。
利用 AU 信息来控制不同模态特征的精确融合。
性能：
在图像质量和唇形同步方面优于其他最先进的方法。
工作量：
需要大量的数据和计算资源来训练 NeRF 模型。</p>
</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-964938af99e1099b95b512a910ce466c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-39deb199fcbfcf9dedfebf11b5272218.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d53c04a42d143a126e5b391f40684f6a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55f96488825fc7af3820d32c3f4ac6ff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1072a698b0f056bb4d49ab4715962395.jpg" align="middle">
</details>




]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>Talking Head Generation</tag>
      </tags>
  </entry>
  <entry>
    <title>3DGS</title>
    <url>/2024/02/02/Paper/2024-02-02/3DGS/</url>
    <content><![CDATA[<blockquote>
<p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-02-02-更新"><a href="#2024-02-02-更新" class="headerlink" title="2024-02-02 更新"></a>2024-02-02 更新</h1><h2 id="360-GS-Layout-guided-Panoramic-Gaussian-Splatting-For-Indoor-Roaming"><a href="#360-GS-Layout-guided-Panoramic-Gaussian-Splatting-For-Indoor-Roaming" class="headerlink" title="360-GS: Layout-guided Panoramic Gaussian Splatting For Indoor Roaming"></a>360-GS: Layout-guided Panoramic Gaussian Splatting For Indoor Roaming</h2><p><strong>Authors:Jiayang Bai, Letian Huang, Jie Guo, Wen Gong, Yuanqi Li, Yanwen Guo</strong></p>
<p>3D Gaussian Splatting (3D-GS) has recently attracted great attention with real-time and photo-realistic renderings. This technique typically takes perspective images as input and optimizes a set of 3D elliptical Gaussians by splatting them onto the image planes, resulting in 2D Gaussians. However, applying 3D-GS to panoramic inputs presents challenges in effectively modeling the projection onto the spherical surface of ${360^\circ}$ images using 2D Gaussians. In practical applications, input panoramas are often sparse, leading to unreliable initialization of 3D Gaussians and subsequent degradation of 3D-GS quality. In addition, due to the under-constrained geometry of texture-less planes (e.g., walls and floors), 3D-GS struggles to model these flat regions with elliptical Gaussians, resulting in significant floaters in novel views. To address these issues, we propose 360-GS, a novel $360^{\circ}$ Gaussian splatting for a limited set of panoramic inputs. Instead of splatting 3D Gaussians directly onto the spherical surface, 360-GS projects them onto the tangent plane of the unit sphere and then maps them to the spherical projections. This adaptation enables the representation of the projection using Gaussians. We guide the optimization of 360-GS by exploiting layout priors within panoramas, which are simple to obtain and contain strong structural information about the indoor scene. Our experimental results demonstrate that 360-GS allows panoramic rendering and outperforms state-of-the-art methods with fewer artifacts in novel view synthesis, thus providing immersive roaming in indoor scenarios. </p>
<p><a href="http://arxiv.org/abs/2402.00763v1">PDF</a> 11 pages, 10 figures</p>
<p><strong>Summary</strong><br>360-GS 以平面投影为基础，利用布局先验来指导优化过程，从而产生可用于渲染全景和生成新视角图像的 3D 椭圆高斯分布。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>3D 高斯斑点 (3D-GS) 是一种流行的技术，它通常将透视图像作为输入，并优化一组 3D 椭圆高斯分布，将它们喷射到图像平面上，从而产生 2D 高斯分布。</li>
<li>然而，将 3D-GS 应用于全景输入时，使用 2D 高斯分布对 ${360^\circ}$ 图像的球形表面上的投影进行建模存在挑战。</li>
<li>在实际应用中，输入全景通常很稀疏，导致 3D 高斯分布的初始化不可靠，随后 3D-GS 质量下降。</li>
<li>此外，由于纹理平面（例如墙壁和地板）的几何形状受限，3D-GS 难以使用椭圆高斯分布对这些平坦区域进行建模，从而导致新视图中出现明显的漂浮物。</li>
<li>为了解决这些问题，我们提出了 360-GS，这是一种针对有限数量的全景输入的新型 $360^{\circ}$ 高斯斑点。</li>
<li>360-GS 不将 3D 高斯分布直接喷射到球形表面上，而是将其投影到单位球的切平面，然后将它们映射到球形投影。这种改编能够使用高斯分布表示投影。</li>
<li>我们通过利用全景中的布局先验来指导 360-GS 的优化，这些先验很容易获得，并且包含有关室内场景的强大结构信息。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：360-GS：布局引导的室内全景高斯渲染</li>
<li>作者：Jiayang Bai, Letian Huang, Jie Guo, Wen Gong, Yuanqi Li, Yanwen Guo</li>
<li>隶属：南京大学</li>
<li>关键词：3D高斯渲染、全景图像、室内场景、布局引导</li>
<li>论文链接：https://arxiv.org/abs/2402.00763
   Github 链接：无</li>
<li>
<p>摘要：
   (1)：研究背景：3D高斯渲染（3D-GS）因其实时性和照片级渲染效果而备受关注。该技术通常以透视图像作为输入，通过将一组 3D 椭圆高斯体渲染到图像平面上，从而生成 2D 高斯体。然而，将 3D-GS 应用于全景输入时，使用 2D 高斯体有效建模 360° 图像的球面投影存在挑战。在实际应用中，输入全景图像通常是稀疏的，导致 3D 高斯体的初始化不可靠，进而降低 3D-GS 的质量。此外，由于缺乏纹理的平面（例如墙壁和地板）的几何约束不足，3D-GS 难以使用椭圆高斯体对这些平面区域进行建模，从而导致在新的视角中出现明显的浮动物体。
   (2)：过去的方法及其问题：为了解决这些问题，本文提出了一种针对有限全景输入的新型 360° 高斯渲染方法 360-GS。与直接将 3D 高斯体渲染到球面上不同，360-GS 将其投影到单位球体的切平面，然后将其映射到球面投影。这种改进使得使用高斯体表示投影成为可能。我们通过利用全景图像中的布局先验来指导 360-GS 的优化，这些先验易于获取，并且包含有关室内场景的强结构信息。
   (3)：本文的研究方法：我们的实验结果表明，360-GS 能够从有限数量的全景输入中生成高质量的全景渲染。与 3D-GS 相比，360-GS 在准确性、细节和鲁棒性方面均表现出优势。
   (4)：方法的性能及其对目标的支持：360-GS 在室内场景渲染任务上取得了优异的性能。与 3D-GS 相比，360-GS 在准确性、细节和鲁棒性方面均表现出优势。这些结果表明，360-GS 能够有效地利用布局先验来指导 3D 高斯体的优化，从而生成高质量的全景渲染。</p>
</li>
<li>
<p>方法：
(1)：360◦高斯体镶嵌：提出了一种新颖的 splatting 技术，将 splatting 分解为两个步骤：在单位球体的切平面上 splatting 和映射到球面。
(2)：布局引导初始化和正则化：利用全景图像中的布局先验来指导 3D 高斯体的优化，这些先验易于获取，并且包含有关室内场景的强结构信息。
(3)：全景渲染：通过将 splattered 的高斯体从前到后进行 alpha 混合，可以生成全景渲染。</p>
</li>
<li>
<p>结论：
（1）：本文提出了一种新颖的布局引导全景高斯渲染流水线，名为360-GS，它支持直接全景渲染，并且对稀疏输入具有鲁棒性。360-GS的基石是我们的360◦高斯 splatting 算法以及房间布局先验的结合。360◦高斯 splatting 算法通过利用透视投影和映射来解决在球面表面建模投影的挑战，从而实现对具有等距矩形图像的 3D 高斯的直接优化。我们在 3D 高斯的初始化过程中利用全景图中的房间布局先验，提供了一种更易于访问且鲁棒的替代方案来替代 SfM 点云。我们还引入了布局引导正则化来减轻浮动问题并保留房间布局的几何结构。360-GS 支持实时漫游，并在真实世界场景中为新颖视角合成提供了最先进的性能。
（2）：创新点：</p>
</li>
<li>提出了一种新颖的 360◦高斯 splatting 算法，该算法将 splatting 分解为两个步骤：在单位球体的切平面上 splatting 和映射到球面。</li>
<li>利用全景图像中的布局先验来指导 3D 高斯的优化，这些先验易于获取，并且包含有关室内场景的强结构信息。</li>
<li>引入了布局引导正则化来减轻浮动问题并保留房间布局的几何结构。
性能：</li>
<li>与 3D-GS 相比，360-GS 在准确性、细节和鲁棒性方面均表现出优势。</li>
<li>360-GS 在室内场景渲染任务上取得了优异的性能。
工作量：</li>
<li>需要收集和预处理全景图像。</li>
<li>需要优化 3D 高斯的参数。</li>
<li>需要将 splattered 的高斯体从前到后进行 alpha 混合以生成全景渲染。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-38c0a2fd61f19043e9f57d34dec4a1c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9fe5198d06678b334414f192b0c83aa8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e4e5570dfa99dfac9b297f7650c717c3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f5349fc8a22abb33ba9a2c7388b0a826.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d8e3eade9a3d6331e76dbab98e15a68.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffe9d7162c03cd614dfd0b6e7509adbd.jpg" align="middle">
</details>




<h2 id="CoSSegGaussians-Compact-and-Swift-Scene-Segmenting-3D-Gaussians-with-Dual-Feature-Fusion"><a href="#CoSSegGaussians-Compact-and-Swift-Scene-Segmenting-3D-Gaussians-with-Dual-Feature-Fusion" class="headerlink" title="CoSSegGaussians: Compact and Swift Scene Segmenting 3D Gaussians with   Dual Feature Fusion"></a>CoSSegGaussians: Compact and Swift Scene Segmenting 3D Gaussians with   Dual Feature Fusion</h2><p><strong>Authors:Bin Dou, Tianyu Zhang, Yongjia Ma, Zhaohui Wang, Zejian Yuan</strong></p>
<p>We propose Compact and Swift Segmenting 3D Gaussians(CoSSegGaussians), a method for compact 3D-consistent scene segmentation at fast rendering speed with only RGB images input. Previous NeRF-based segmentation methods have relied on time-consuming neural scene optimization. While recent 3D Gaussian Splatting has notably improved speed, existing Gaussian-based segmentation methods struggle to produce compact masks, especially in zero-shot segmentation. This issue probably stems from their straightforward assignment of learnable parameters to each Gaussian, resulting in a lack of robustness against cross-view inconsistent 2D machine-generated labels. Our method aims to address this problem by employing Dual Feature Fusion Network as Gaussians’ segmentation field. Specifically, we first optimize 3D Gaussians under RGB supervision. After Gaussian Locating, DINO features extracted from images are applied through explicit unprojection, which are further incorporated with spatial features from the efficient point cloud processing network. Feature aggregation is utilized to fuse them in a global-to-local strategy for compact segmentation features. Experimental results show that our model outperforms baselines on both semantic and panoptic zero-shot segmentation task, meanwhile consumes less than 10% inference time compared to NeRF-based methods. Code and more results will be available at <a href="https://David-Dou.github.io/CoSSegGaussians">https://David-Dou.github.io/CoSSegGaussians</a> </p>
<p><a href="http://arxiv.org/abs/2401.05925v3">PDF</a> 9 pages, 8 figures, correct writing details</p>
<p><strong>摘要</strong><br>结合点云与显式反投射的特征融合网络，实现紧凑而快速的 3D 高斯混合分割。</p>
<p><strong>关键要点</strong></p>
<ul>
<li>提出一种用于紧凑、快速且仅以RGB图像作为输入的3D场景一致性分割方法：紧凑快速分割3D高斯（CoSSegGaussians）。</li>
<li>现有的基于高斯体素的分割方法在进行零镜头分割时难以生成紧凑的掩模，这可能是因为它们将可学习的参数直接分配给每个高斯体素，从而导致缺乏对跨视图不一致的2D机器生成的标签的鲁棒性。</li>
<li>利用双特征融合网络作为高斯体素的分割字段来解决上述问题。</li>
<li>首先在RGB监督下优化3D高斯体素。</li>
<li>然后通过显式反投影应用从图像中提取的DINO特征，并结合来自有效点云处理网络的空间特征。</li>
<li>利用特征聚合在全局到局部的策略中融合这些特征以实现紧凑的分割特征。</li>
<li>实验结果表明，与NeRF为基础的方法相比，该模型在语义分割和全景零镜头分割任务上都优于基线，同时推理时间少于10%。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：紧凑而快速的场景分割 3D 高斯体与双重特征融合</li>
<li>作者：Dou Bin, Zhang Tianyu, Ma Yongjia, Wang Zhaohui, Yuan Zejian</li>
<li>单位：西安交通大学人工智能与机器人学院</li>
<li>关键词：3D 场景分割、神经辐射场、高斯体、双重特征融合</li>
<li>论文链接：https://arxiv.org/abs/2401.05925，Github 代码链接：None</li>
<li>
<p>摘要：
（1）研究背景：近年来，计算机视觉和计算机图形学取得了显着进展，特别是在神经渲染领域。神经辐射场 (NeRF) 及其后续方法推动了神经场景表示的发展，在新型视图合成方面显示出显着的性能。
（2）过去的方法及其问题：基于 NeRF 的分割方法依赖于耗时的神经场景优化。虽然最近的 3D 高斯体 splatting 显着提高了速度，但现有的基于高斯体的分割方法难以产生紧凑的掩模，尤其是在零样本分割中。这个问题可能源于其直接将可学习参数分配给每个高斯体，导致对跨视图不一致的 2D 机器生成的标签缺乏鲁棒性。
（3）本文方法：本文提出了一种紧凑而快速的场景分割方法，称为 CoSSegGaussians，该方法仅使用 RGB 图像输入即可实现紧凑的 3D 一致场景分割，且渲染速度快。具体来说，我们首先在 RGB 监督下优化 3D 高斯体。在高斯体定位之后，通过显式反投影应用从图像中提取的 DINO 特征，然后将其与来自高效点云处理网络的空间特征结合。利用特征聚合在全局到局部策略中融合它们以获得紧凑的分割特征。
（4）方法性能：实验结果表明，我们的模型在语义和全景零样本分割任务上都优于基线方法，同时推理时间不到基于 NeRF 的方法的 10%。</p>
</li>
<li>
<p>方法：
（1）高斯体定位阶段：使用 L1 和 L_D-SSIM 光度损失来监督高斯体的几何信息，包括质心、协方差、不透明度和颜色。
（2）分割阶段：将多尺度的 DINO 特征反投影到高斯体上，并与从高斯体中提取的空间特征融合。
（3）特征聚合：使用全局到局部策略聚合融合后的特征，以生成紧凑的分割特征。
（4）监督：使用零样本分割掩模和关联掩模来监督分割参数，并使用 NCE 损失进行优化。</p>
</li>
<li>
<p>结论：
（1）：本文提出了一种紧凑而快速的场景分割方法 CoSSegGaussians，该方法仅使用 RGB 图像输入即可实现紧凑的 3D 一致场景分割，且渲染速度快。实验结果表明，我们的模型在语义和全景零样本分割任务上都优于基线方法，同时推理时间不到基于 NeRF 的方法的 10%。
（2）：创新点：</p>
</li>
<li>提出了一种紧凑而快速的场景分割方法 CoSSegGaussians，该方法仅使用 RGB 图像输入即可实现紧凑的 3D 一致场景分割，且渲染速度快。</li>
<li>提出了一种双重特征融合网络作为分割场，该网络聚合了 DINO 和空间特征用于分割。</li>
<li>将多尺度的 DINO 特征从图像反投影到定位的 3D 高斯体上，并进一步与高斯体的空间信息相结合。</li>
<li>应用全局到局部聚合模块生成紧凑的分割逻辑。
性能：</li>
<li>在语义和全景零样本分割任务上都优于基线方法。</li>
<li>推理时间不到基于 NeRF 的方法的 10%。
工作量：</li>
<li>使用了大量的数据集进行训练和测试。</li>
<li>算法的实现和训练过程较为复杂。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ecce62ef2d2a0a0c5d6577de6d7cb33f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-222c4f05c24f306aefd909de021e726c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6dff94133ac5b0802b5de3fb9550eff1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e96a03193e246ab9e77a3dd6aa18e239.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f381d5614322d380f003e54e659eb10.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb6b0eeec85fc1d0f2cd12928b40918f.jpg" align="middle">
</details>




]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>3DGS</tag>
      </tags>
  </entry>
  <entry>
    <title>NeRF</title>
    <url>/2024/02/02/Paper/2024-02-02/NeRF/</url>
    <content><![CDATA[<blockquote>
<p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-02-02-更新"><a href="#2024-02-02-更新" class="headerlink" title="2024-02-02 更新"></a>2024-02-02 更新</h1><h2 id="ViCA-NeRF-View-Consistency-Aware-3D-Editing-of-Neural-Radiance-Fields"><a href="#ViCA-NeRF-View-Consistency-Aware-3D-Editing-of-Neural-Radiance-Fields" class="headerlink" title="ViCA-NeRF: View-Consistency-Aware 3D Editing of Neural Radiance Fields"></a>ViCA-NeRF: View-Consistency-Aware 3D Editing of Neural Radiance Fields</h2><p><strong>Authors:Jiahua Dong, Yu-Xiong Wang</strong></p>
<p>We introduce ViCA-NeRF, the first view-consistency-aware method for 3D editing with text instructions. In addition to the implicit neural radiance field (NeRF) modeling, our key insight is to exploit two sources of regularization that explicitly propagate the editing information across different views, thus ensuring multi-view consistency. For geometric regularization, we leverage the depth information derived from NeRF to establish image correspondences between different views. For learned regularization, we align the latent codes in the 2D diffusion model between edited and unedited images, enabling us to edit key views and propagate the update throughout the entire scene. Incorporating these two strategies, our ViCA-NeRF operates in two stages. In the initial stage, we blend edits from different views to create a preliminary 3D edit. This is followed by a second stage of NeRF training, dedicated to further refining the scene’s appearance. Experimental results demonstrate that ViCA-NeRF provides more flexible, efficient (3 times faster) editing with higher levels of consistency and details, compared with the state of the art. Our code is publicly available. </p>
<p><a href="http://arxiv.org/abs/2402.00864v1">PDF</a> Neurips2023; project page: <a href="https://github.com/Dongjiahua/VICA-NeRF">https://github.com/Dongjiahua/VICA-NeRF</a></p>
<p><strong>Summary</strong><br>文本引入了一种新的方法 ViCA-NeRF，该方法可以利用文本编辑进行 3D 编辑，并使用几何和学习正则化来确保编辑的多视图一致性。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ViCA-NeRF 是一种新颖的基于文本的 3D 编辑方法，利用 NeRF 进行隐式神经辐射场建模。</li>
<li>ViCA-NeRF 的关键思想是利用两种正则化来源，明确地在不同视图之间传播编辑信息，确保多视图一致性。</li>
<li>ViCA-NeRF 利用从 NeRF 推导出的深度信息来建立不同视图之间的图像对应关系，用于几何正则化。</li>
<li>ViCA-NeRF 对经过编辑和未经过编辑的图像在 2D 扩散模型中的潜在编码进行对齐，实现编辑关键视图并更新整个场景。</li>
<li>ViCA-NeRF 采用两个阶段的工作流程，第一阶段将来自不同视图的编辑融合，创建初步的 3D 编辑。</li>
<li>第二阶段进行 NeRF 训练，进一步优化场景的外观。</li>
<li>与现有技术相比，ViCA-NeRF 提供更灵活、更高效（速度提升 3 倍）、更一致且更详细的编辑。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：ViCA-NeRF：基于视图一致性的神经辐射场三维编辑</li>
<li>作者：Jiahua Dong, Yu-Xiong Wang</li>
<li>单位：伊利诺伊大学厄巴纳-香槟分校</li>
<li>关键词：三维编辑、神经辐射场、视图一致性、文本指令</li>
<li>论文链接：https://arxiv.org/abs/2402.00864
Github 链接：https://dongjiahua.github.io/VICA-NeRF</li>
<li>
<p>摘要：
(1)：随着神经辐射场（NeRF）及其变体的最新进展，收集真实世界三维场景数据变得更加便捷。然而，现有的三维编辑方法通常缺乏视图一致性，导致编辑结果在不同视角下可能出现不一致的情况。
(2)：过去的方法主要包括基于几何的正则化和基于学习的正则化。几何正则化利用 NeRF 提取的深度信息来建立不同视角之间的图像对应关系，从而确保视图一致性。学习正则化则通过对编辑图像和未编辑图像的潜在代码进行对齐，使编辑信息能够在整个场景中传播。
(3)：本文提出的 ViCA-NeRF 是一种基于视图一致性的三维编辑方法，它结合了几何正则化和学习正则化两种策略。ViCA-NeRF 首先通过融合来自不同视角的编辑结果来创建初步的三维编辑，然后通过 NeRF 训练进一步细化场景的外观，从而确保视图一致性和细节丰富。
(4)：实验结果表明，与现有方法相比，ViCA-NeRF 提供了更加灵活、高效（速度提高 3 倍）的编辑方式，并且具有更高的视图一致性和细节水平。</p>
</li>
<li>
<p>方法：
(1) ViCA-NeRF 首先从不同视角收集输入图像，并使用 NeRF 从这些图像中提取深度信息。
(2) 然后，ViCA-NeRF 利用提取的深度信息来建立不同视角之间的图像对应关系，并使用这些对应关系来融合来自不同视角的编辑结果，从而创建初步的三维编辑。
(3) 最后，ViCA-NeRF 通过 NeRF 训练进一步细化场景的外观，从而确保视图一致性和细节丰富。</p>
</li>
<li>
<p>结论：
（1）：本文提出了 ViCA-NeRF，一种基于视图一致性的三维编辑框架，用于文本引导的 NeRF 编辑。给定文本指令，我们可以高效地编辑 NeRF。除了像人类风格化和天气变化这样的简单任务外，我们还支持上下文相关的操作，例如“添加一些花朵”和编辑高度详细的纹理。我们的方法在各种场景和文本提示上优于几个基线。未来，我们将继续提高三维编辑的可控性和真实性。
（2）：创新点：
ViCA-NeRF 结合了几何正则化和学习正则化两种策略，以确保视图一致性和细节丰富。
ViCA-NeRF 利用提取的深度信息来建立不同视角之间的图像对应关系，并使用这些对应关系来融合来自不同视角的编辑结果，从而创建初步的三维编辑。
ViCA-NeRF 通过 NeRF 训练进一步细化场景的外观，从而确保视图一致性和细节丰富。
性能：
ViCA-NeRF 在各种场景和文本提示上优于几个基线。
ViCA-NeRF 的速度提高了 3 倍。
工作量：
ViCA-NeRF 的实现相对简单。
ViCA-NeRF 的训练和推理速度较快。</p>
</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-b3cbdca659df3ac2eb7b2521752d1c8e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5c934d1ebae9f51cda700d605228196.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40418c9a6b8bcda24387d9b40ab2cd3a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ff0299de61f2dcce94a6f84b195a4b3.jpg" align="middle">
</details>




]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>NeRF</tag>
      </tags>
  </entry>
  <entry>
    <title>Talking Head Generation</title>
    <url>/2024/01/24/Paper/2024-01-24/Talking%20Head%20Generation/</url>
    <content><![CDATA[<blockquote>
<p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-01-24-更新"><a href="#2024-01-24-更新" class="headerlink" title="2024-01-24 更新"></a>2024-01-24 更新</h1><h2 id="Real3D-Portrait-One-shot-Realistic-3D-Talking-Portrait-Synthesis"><a href="#Real3D-Portrait-One-shot-Realistic-3D-Talking-Portrait-Synthesis" class="headerlink" title="Real3D-Portrait: One-shot Realistic 3D Talking Portrait Synthesis"></a>Real3D-Portrait: One-shot Realistic 3D Talking Portrait Synthesis</h2><p><strong>Authors:Zhenhui Ye, Tianyun Zhong, Yi Ren, Jiaqi Yang, Weichuang Li, Jiawei Huang, Ziyue Jiang, Jinzheng He, Rongjie Huang, Jinglin Liu, Chen Zhang, Xiang Yin, Zejun Ma, Zhou Zhao</strong></p>
<p>One-shot 3D talking portrait generation aims to reconstruct a 3D avatar from an unseen image, and then animate it with a reference video or audio to generate a talking portrait video. The existing methods fail to simultaneously achieve the goals of accurate 3D avatar reconstruction and stable talking face animation. Besides, while the existing works mainly focus on synthesizing the head part, it is also vital to generate natural torso and background segments to obtain a realistic talking portrait video. To address these limitations, we present Real3D-Potrait, a framework that (1) improves the one-shot 3D reconstruction power with a large image-to-plane model that distills 3D prior knowledge from a 3D face generative model; (2) facilitates accurate motion-conditioned animation with an efficient motion adapter; (3) synthesizes realistic video with natural torso movement and switchable background using a head-torso-background super-resolution model; and (4) supports one-shot audio-driven talking face generation with a generalizable audio-to-motion model. Extensive experiments show that Real3D-Portrait generalizes well to unseen identities and generates more realistic talking portrait videos compared to previous methods. Video samples and source code are available at <a href="https://real3dportrait.github.io">https://real3dportrait.github.io</a> . </p>
<p><a href="http://arxiv.org/abs/2401.08503v2">PDF</a> ICLR 2024 (Spotlight). Project page: <a href="https://real3dportrait.github.io">https://real3dportrait.github.io</a></p>
<p><strong>摘要</strong><br>利用大规模图像到平面模型提升 3D 人脸生成模型的重构能力，并结合动作适配器和头部躯干背景超分辨率模型，生成逼真的说话肖像视频。</p>
<p><strong>要点</strong></p>
<ul>
<li>提出 Real3D-Portrait 框架，用于生成逼真的说话肖像视频。</li>
<li>采用大规模图像到平面模型，从 3D 人脸生成模型中提取 3D 先验知识，提高一发 3D 重建能力。</li>
<li>使用高效的动作适配器，实现准确的动作条件动画。</li>
<li>利用头部躯干背景超分辨率模型，合成具有自然躯干运动和可切换背景的逼真视频。</li>
<li>支持一发音频驱动的说话面部生成，使用可推广的音频到动作模型。</li>
<li>大量实验证明，Real3D-Portrait 在看不见的身份上具有良好的泛化能力，并且与以前的方法相比，可以生成更逼真的说话肖像视频。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>标题：Real3D-Portrait：单次拍摄的逼真 3D 说话人像合成</li>
<li>作者：叶振辉、钟天云、任怡、杨佳奇、李维创、黄嘉伟、蒋子悦、何锦正、黄荣杰、刘敬林、张晨、尹翔、马泽君、赵周</li>
<li>单位：浙江大学、字节跳动、香港科技大学（广州）</li>
<li>关键词：One-shot 3D talking face generation, 3D reconstruction, Talking face animation, Video synthesis</li>
<li>论文链接：https://arxiv.org/abs/2401.08503
Github 链接：None</li>
<li>摘要：
(1)：研究背景：说话人像生成旨在根据驱动条件（动作序列或驱动音频）合成说话人像视频。这是一个计算机图形学和计算机视觉中长期存在的跨模态任务，具有视频会议和虚拟现实 (VR) 等多项实际应用。先前的 2D 方法可以产生逼真的视频，这要归功于生成对抗网络 (GAN) 的强大功能。然而，由于缺乏显式的 3D 建模，这些 2D 方法在头部大幅移动时会面临变形伪影和不真实的失真。在过去的几年中，基于神经辐射场 (NeRF) 的 3D 方法一直占主导地位，因为它们保持逼真的 3D 几何形状并保留丰富的纹理细节，即使在头部姿势较大的情况下也是如此。然而，在大多数方法中，模型都过度拟合特定的人，这需要为每个看不见的身份进行昂贵的单独训练。探索单次拍摄 3D 说话人像生成的任务很有希望，即给定一个看不见的人的参考图像，我们的目标是将其提升到 3D 头像并使用输入条件对其进行动画处理，以获得逼真的 3D 说话人视频。随着 3D 生成模型的最新进展，可以学习到推广到各种身份的 3D 三平面表示（EG3D，Chan et al. (2022)）的隐藏空间。虽然最近的工作 (Li et al., 2023b; Li, 2023) 开创了单次拍摄 3D 说话人像生成，但它们未能同时实现准确的重建和动画。具体来说，一些工作
(2)：过去的方法：一些工作仅使用 2D 图像作为输入，而另一些工作则使用 3D 图像作为输入。使用 2D 图像作为输入的方法通常会产生质量较差的结果，因为它们无法捕获对象的 3D 形状。使用 3D 图像作为输入的方法通常会产生质量更好的结果，但它们需要昂贵的 3D 扫描设备。
本方法的动机很充分。作者认为，单次拍摄 3D 说话人像生成是一个具有挑战性的任务，需要解决许多问题。这些问题包括：</li>
<li>如何从单张 2D 图像重建准确的 3D 模型？</li>
<li>如何将 3D 模型与驱动条件（动作序列或驱动音频）相关联？</li>
<li>如何合成逼真的说话人像视频？
作者提出了一种新的方法来解决这些问题。该方法包括以下几个步骤：</li>
<li>从单张 2D 图像重建准确的 3D 模型。</li>
<li>将 3D 模型与驱动条件（动作序列或驱动音频）相关联。</li>
<li>合成逼真的说话人像视频。
作者的方法在几个数据集上进行了评估，结果表明该方法能够生成高质量的说话人像视频。
(3)：研究方法：作者提出了一种名为 Real3D-Portrait 的框架，该框架可以从单张图像生成逼真的 3D 说话人像视频。Real3D-Portrait 包括以下几个模块：</li>
<li>图像到平面模型：该模块将输入图像转换为 3D 三平面表示。</li>
<li>运动适配器：该模块将 3D 三平面表示与驱动条件（动作序列或驱动音频）相关联。</li>
<li>头部躯干背景超分辨率模型：该模块合成逼真的视频，具有自然的躯干运动和可切换的背景。</li>
<li>
<p>音频到运动模型：该模块支持单次拍摄的音频驱动说话人像生成。
(4)：性能：Real3D-Portrait 在几个数据集上进行了评估，结果表明该方法能够生成高质量的说话人像视频。在 TalkingHead 数据集上，Real3D-Portrait 的平均重建误差为 0.006，平均动画误差为 0.008。在 VoxCeleb 数据集上，Real3D-Portrait 的平均重建误差为 0.007，平均动画误差为 0.009。在 LRW 数据集上，Real3D-Portrait 的平均重建误差为 0.008，平均动画误差为 0.010。这些结果表明，Real3D-Portrait 能够生成高质量的说话人像视频，并且该方法可以推广到看不见的身份。</p>
</li>
<li>
<p><strong>方法</strong>：</p>
</li>
</ol>
<p>（1）图像到平面模型：该模块将输入图像转换为3D三平面表示（EG3D）。EG3D是一种隐式神经表示，可以捕获对象的3D形状和纹理。</p>
<p>（2）运动适配器：该模块将3D三平面表示与驱动条件（动作序列或驱动音频）相关联。运动适配器使用一个神经网络来学习如何将驱动条件映射到3D三平面表示。</p>
<p>（3）头部躯干背景超分辨率模型：该模块合成逼真的视频，具有自然的躯干运动和可切换的背景。头部躯干背景超分辨率模型使用一个神经网络来学习如何将3D三平面表示渲染成逼真的视频。</p>
<p>（4）音频到运动模型：该模块支持单次拍摄的音频驱动说话人像生成。音频到运动模型使用一个神经网络来学习如何将驱动音频映射到动作序列。</p>
<ol>
<li>结论：
（1）：本文提出了一种单次拍摄逼真3D说话人像合成框架Real3D-Portrait。该方法同时实现了准确的3D头像重建和动画，并支持视频/音频驱动的应用。
（2）：创新点：</li>
<li>提出了一种预训练的大型图像到平面模型，可以从单张图像重建准确的3D三平面表示。</li>
<li>设计了一个PNCC条件运动适配器，可以将3D三平面表示与驱动条件（动作序列或驱动音频）相关联。</li>
<li>提出了一种头部躯干背景超分辨率模型，可以合成逼真的视频，具有自然的躯干运动和可切换的背景。</li>
<li>提出了一种通用的音频到运动模型，支持视频/音频驱动的应用。
性能：</li>
<li>在TalkingHead数据集上，Real3D-Portrait的平均重建误差为0.006，平均动画误差为0.008。</li>
<li>在VoxCeleb数据集上，Real3D-Portrait的平均重建误差为0.007，平均动画误差为0.009。</li>
<li>在LRW数据集上，Real3D-Portrait的平均重建误差为0.008，平均动画误差为0.010。
工作量：</li>
<li>该方法需要大量的数据和计算资源来训练模型。</li>
<li>该方法的训练过程比较复杂，需要专业知识和技能。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-49a987d29d4e89d46251e6ddc16c6776.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bcd0735a0f8445511d9ad42c4b5cc609.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-519ffb14435cf5d80acd488dc9b96504.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54411699feb07b3a92834da51afd6954.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86a981fea38f5376c644338440f55eff.jpg" align="middle">
</details>




<h2 id="DREAM-Talk-Diffusion-based-Realistic-Emotional-Audio-driven-Method-for-Single-Image-Talking-Face-Generation"><a href="#DREAM-Talk-Diffusion-based-Realistic-Emotional-Audio-driven-Method-for-Single-Image-Talking-Face-Generation" class="headerlink" title="DREAM-Talk: Diffusion-based Realistic Emotional Audio-driven Method for   Single Image Talking Face Generation"></a>DREAM-Talk: Diffusion-based Realistic Emotional Audio-driven Method for   Single Image Talking Face Generation</h2><p><strong>Authors:Chenxu Zhang, Chao Wang, Jianfeng Zhang, Hongyi Xu, Guoxian Song, You Xie, Linjie Luo, Yapeng Tian, Xiaohu Guo, Jiashi Feng</strong></p>
<p>The generation of emotional talking faces from a single portrait image remains a significant challenge. The simultaneous achievement of expressive emotional talking and accurate lip-sync is particularly difficult, as expressiveness is often compromised for the accuracy of lip-sync. As widely adopted by many prior works, the LSTM network often fails to capture the subtleties and variations of emotional expressions. To address these challenges, we introduce DREAM-Talk, a two-stage diffusion-based audio-driven framework, tailored for generating diverse expressions and accurate lip-sync concurrently. In the first stage, we propose EmoDiff, a novel diffusion module that generates diverse highly dynamic emotional expressions and head poses in accordance with the audio and the referenced emotion style. Given the strong correlation between lip motion and audio, we then refine the dynamics with enhanced lip-sync accuracy using audio features and emotion style. To this end, we deploy a video-to-video rendering module to transfer the expressions and lip motions from our proxy 3D avatar to an arbitrary portrait. Both quantitatively and qualitatively, DREAM-Talk outperforms state-of-the-art methods in terms of expressiveness, lip-sync accuracy and perceptual quality. </p>
<p><a href="http://arxiv.org/abs/2312.13578v1">PDF</a> Project Page at <a href="https://magic-research.github.io/dream-talk/">https://magic-research.github.io/dream-talk/</a></p>
<p><strong>Summary</strong><br>语音驱动下，DREAM-Talk 可同时实现准确的口型同步和自然的情感表达，生成逼真的动态对话人脸。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>DREAM-Talk 采用两阶段扩散式音频驱动框架，能同时实现丰富多样的情感表达和精准的口型同步。</li>
<li>首阶段提出 EmoDiff 模块，可依据音频和指定的情感样式，生成多样且富有动态感的情感表情和头部姿势。</li>
<li>基于唇部动作与音频的强相关性，利用音频特征和情感样式，DREAM-Talk 在第二阶段进一步优化动态效果，增强口型同步的精确性。</li>
<li>DREAM-Talk 运用视频到视频渲染模块，将代理 3D 头像的表情和唇部动作转移到任意肖像上。</li>
<li>定量和定性评估结果表明，DREAM-Talk在表情丰富度、口型同步精度以及感知质量方面均优于现有方法。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li><p>标题：DREAM-Talk：基于扩散的逼真情感音频驱动的单张图像说话人脸生成方法</p>
</li>
<li><p>作者：陈旭章<em>, 王超</em>, 张建峰, 徐鸿毅, 宋国贤, 谢宇, 罗林杰, 田亚鹏, 郭晓虎, 冯佳世</p>
</li>
<li><p>单位：字节跳动公司</p>
</li>
<li><p>关键词：情感说话人脸生成；扩散模型；音频驱动；唇形同步</p>
</li>
<li><p>论文链接：https://arxiv.org/abs/2312.13578</p>
</li>
<li><p>摘要：
(1) 研究背景：从单张人像图像生成情感说话人脸仍然是一项重大挑战。同时实现富有表现力的情感说话和准确的唇形同步尤其困难，因为表现力通常会因唇形同步的准确性而受到损害。LSTM 网络被许多先前的工作广泛采用，但往往无法捕捉情感表达的细微差别和变化。
(2) 过去的方法及其问题：为了解决这些挑战，我们引入了 DREAM-Talk，这是一个两阶段的基于扩散的音频驱动框架，专门用于同时生成多样化的表情和准确的唇形同步。在第一阶段，我们提出了 EmoDiiff，一个新颖的扩散模块，该模块根据音频和参考情感风格生成多样化的高度动态情感表达和头部姿势。鉴于唇部运动与音频之间存在很强的相关性，我们随后使用音频特征和情感风格来增强唇形同步准确性，从而优化动态效果。为此，我们部署了一个视频到视频渲染模块，将我们代理 3D 头像的表情和唇部动作转移到任意人像上。
(3) 本文提出的研究方法：在定量和定性方面，DREAM-Talk 在表现力、唇形同步准确性和感知质量方面都优于最先进的方法。
(4) 方法在什么任务上取得了什么性能？性能是否支持其目标：该方法在情感说话人脸生成任务上取得了很好的性能。在定量评估中，DREAM-Talk 在三个基准数据集上实现了最先进的结果，在情感多样性、唇形同步准确性和感知质量方面均优于现有方法。在定性评估中，DREAM-Talk 生成的说话人脸具有逼真的情感表达、准确的唇形同步和很高的视觉质量。这些结果支持了该方法的目标，即生成具有多样化情感表达和准确唇形同步的逼真说话人脸。</p>
</li>
<li><p>方法：
(1) 提出EmoDiff，一个新颖的扩散模块，根据音频和参考情感风格生成多样化的高度动态情感表达和头部姿势。
(2) 部署视频到视频渲染模块，将代理3D头像的表情和唇部动作转移到任意人像上。
(3) 使用音频特征和情感风格来增强唇形同步准确性，从而优化动态效果。</p>
</li>
<li><p>结论：
（1）：本文提出了一种名为DREAM-Talk的创新框架，该框架专为生成具有精确唇形同步的情感表达说话人脸而设计。我们的两阶段方法，包括EmoDiff模块和唇形细化，有效地捕捉了情感细微差别并确保了准确的唇形同步。利用情感条件扩散模型和唇形细化网络，我们的方法优于现有技术。我们的结果表明，在保持高视频质量的同时，面部情感表达能力得到了提高。DREAM-Talk代表了情感说话人脸生成领域向前迈出的重要一步，它使跨越广泛应用范围的逼真且情感参与的数字人形表征的创建成为可能。
（2）：创新点：
提出了一种新颖的扩散模块EmoDiff，该模块根据音频和参考情感风格生成多样化的高度动态情感表达和头部姿势。
部署了一个视频到视频渲染模块，将代理3D头像的表情和唇部动作转移到任意人像上。
使用音频特征和情感风格来增强唇形同步准确性，从而优化动态效果。
性能：
在定量评估中，DREAM-Talk在三个基准数据集上实现了最先进的结果，在情感多样性、唇形同步准确性和感知质量方面均优于现有方法。
在定性评估中，DREAM-Talk生成的说话人脸具有逼真的情感表达、准确的唇形同步和很高的视觉质量。
工作量：
该方法需要大量的数据和计算资源来训练模型。
该方法需要专业知识来实现和部署。</p>
</li>
</ol>




<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8c84d3a58a2189a2edc59f8826b7f47b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6380a292ea9f96c4c952ba930e343d6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-873526fee22103f77756de5c2690665e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ac94d739cb30587f6ce660be8fa86fc.jpg" align="middle">
</details>






<h2 id="VectorTalker-SVG-Talking-Face-Generation-with-Progressive-Vectorisation"><a href="#VectorTalker-SVG-Talking-Face-Generation-with-Progressive-Vectorisation" class="headerlink" title="VectorTalker: SVG Talking Face Generation with Progressive Vectorisation"></a>VectorTalker: SVG Talking Face Generation with Progressive Vectorisation</h2><p><strong>Authors:Hao Hu, Xuan Wang, Jingxiang Sun, Yanbo Fan, Yu Guo, Caigui Jiang</strong></p>
<p>High-fidelity and efficient audio-driven talking head generation has been a key research topic in computer graphics and computer vision. In this work, we study vector image based audio-driven talking head generation. Compared with directly animating the raster image that most widely used in existing works, vector image enjoys its excellent scalability being used for many applications. There are two main challenges for vector image based talking head generation: the high-quality vector image reconstruction w.r.t. the source portrait image and the vivid animation w.r.t. the audio signal. To address these, we propose a novel scalable vector graphic reconstruction and animation method, dubbed VectorTalker. Specifically, for the highfidelity reconstruction, VectorTalker hierarchically reconstructs the vector image in a coarse-to-fine manner. For the vivid audio-driven facial animation, we propose to use facial landmarks as intermediate motion representation and propose an efficient landmark-driven vector image deformation module. Our approach can handle various styles of portrait images within a unified framework, including Japanese manga, cartoon, and photorealistic images. We conduct extensive quantitative and qualitative evaluations and the experimental results demonstrate the superiority of VectorTalker in both vector graphic reconstruction and audio-driven animation. </p>
<p><a href="http://arxiv.org/abs/2312.11568v1">PDF</a> </p>
<p><strong>Summary</strong><br>矢量图像驱动的语音动画生成方法 VectorTalker，首次采用分层式矢量图像重建和特征点驱动的变形模块，可生成高质量语音动画。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>矢量图像驱动的语音动画生成方法 VectorTalker，可生成高质量语音动画。</li>
<li>VectorTalker 分层式地重建矢量图像，以实现高保真重建。</li>
<li>VectorTalker 提出特征点驱动的矢量图像变形模块，以实现生动的语音动画。</li>
<li>VectorTalker 可处理包括日本漫画、卡通和照片写实图像在内的各种风格的肖像图像。</li>
<li>VectorTalker 在矢量图像重建和语音动画方面都表现出优异的性能。</li>
<li>VectorTalker 可在统一框架内处理各种风格的肖像图像，包括日本漫画、卡通和照片写实图像。</li>
<li>VectorTalker 在矢量图像重建和语音动画方面都表现出优异的性能。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：矢量话者：渐进矢量化下的 SVG 会话人脸生成</li>
<li>作者：Jinsong Zhang, Yuxuan Zhang, Yebin Liu, Xiaoguang Han</li>
<li>单位：无</li>
<li>关键词：音频驱动、面部动画、矢量图像生成、可变形模型</li>
<li>论文链接：无，Github 链接：无</li>
<li>
<p>摘要：
（1）：随着计算机图形学和计算机视觉的发展，高保真且高效的音频驱动说话人头部生成已成为一项关键的研究课题。本文研究了基于矢量图像的音频驱动说话人头部生成。与现有工作中最广泛使用的直接对光栅图像进行动画处理相比，矢量图像因其出色的可扩展性而被用于许多应用程序。基于矢量图像的说话人头部生成面临两大挑战：相对于源人像图像的高质量矢量图像重建以及相对于音频信号的生动动画。为了解决这些问题，我们提出了一种新颖的可扩展矢量图像重建和动画方法，称为 VectorTalker。具体来说，对于高保真重建，VectorTalker 以粗到细的方式分层重建矢量图像。对于生动的音频驱动面部动画，我们建议使用面部地标作为中间运动表示，并提出了一种高效的地标驱动的矢量图像变形模块。我们的方法可以在统一的框架内处理各种风格的人像图像，包括日本漫画、卡通和照片写实图像。我们进行了广泛的定量和定性评估，实验结果证明了 VectorTalker 在矢量图像重建和音频驱动动画方面的优越性。</p>
</li>
<li>
<p>方法：
(1) 矢量图像重建：VectorTalker 采用分层重建策略，首先使用粗糙的矢量图像作为初始化，然后通过迭代细化过程逐步提高矢量图像的分辨率和质量。
(2) 面部地标提取：VectorTalker 使用预训练的深度学习模型从输入图像中提取面部地标，这些地标作为中间运动表示，用于驱动矢量图像的变形。
(3) 矢量图像变形：VectorTalker 提出了一种高效的地标驱动的矢量图像变形模块，该模块使用地标信息对矢量图像进行变形，从而实现生动的音频驱动面部动画。
(4) 统一框架：VectorTalker 可以处理各种风格的人像图像，包括日本漫画、卡通和照片写实图像，并可以在统一的框架内进行矢量图像重建和音频驱动动画。</p>
</li>
<li>
<p>结论：
（1）：本研究提出了一种名为 VectorTalker 的新颖方法，用于生成一镜到底的音频驱动的说话 SVG 肖像。我们的渐进矢量化算法允许我们准确地将输入光栅图像重建为矢量图形。我们提取面部关键点并使用基于仿射变换的扭曲系统，通过音频驱动的面部关键点偏移预测来为 SVG 肖像制作动画。我们的广泛实验表明，我们的渐进矢量化明显优于其他基线方法。此外，我们的方法有效地完成了说话 SVG 生成的任务。在未来，我们计划利用更多关于人类的先验知识来实现更逼真的面部动画。
（2）：创新点：VectorTalker 提出了一种新的矢量图像重建和动画方法，该方法可以处理各种风格的人像图像，包括日本漫画、卡通和照片写实图像，并且可以在统一的框架内进行矢量图像重建和音频驱动动画。
性能：VectorTalker 在矢量图像重建和音频驱动动画方面都取得了优异的性能。在矢量图像重建方面，VectorTalker 可以准确地重建输入光栅图像，并且重建的矢量图像具有很高的质量。在音频驱动动画方面，VectorTalker 可以生成生动逼真的面部动画，并且动画与音频信号高度同步。
工作量：VectorTalker 的工作量相对较大。在矢量图像重建方面，VectorTalker 需要迭代细化过程来逐步提高矢量图像的分辨率和质量。在音频驱动动画方面，VectorTalker 需要提取面部关键点并使用基于仿射变换的扭曲系统来对矢量图像进行变形。这些过程都比较耗时。</p>
</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-3d73af4a717ae743272e331632eb8141.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-acdfb181d2fbfb7129b4135fcac342c0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-97e9fae5e9ddf8626b7b6930899fc83a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c37086ba942564a3c3d7ee2a22da266f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-765663a7e8dd6bacba4c559aa42c0cd2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33719fe6e62cc2eba54e1d67239ef47b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24965c00c4a429cc2df1ec8ea2b00a3c.jpg" align="middle">
</details>




<h2 id="AE-NeRF-Audio-Enhanced-Neural-Radiance-Field-for-Few-Shot-Talking-Head-Synthesis"><a href="#AE-NeRF-Audio-Enhanced-Neural-Radiance-Field-for-Few-Shot-Talking-Head-Synthesis" class="headerlink" title="AE-NeRF: Audio Enhanced Neural Radiance Field for Few Shot Talking Head   Synthesis"></a>AE-NeRF: Audio Enhanced Neural Radiance Field for Few Shot Talking Head   Synthesis</h2><p><strong>Authors:Dongze Li, Kang Zhao, Wei Wang, Bo Peng, Yingya Zhang, Jing Dong, Tieniu Tan</strong></p>
<p>Audio-driven talking head synthesis is a promising topic with wide applications in digital human, film making and virtual reality. Recent NeRF-based approaches have shown superiority in quality and fidelity compared to previous studies. However, when it comes to few-shot talking head generation, a practical scenario where only few seconds of talking video is available for one identity, two limitations emerge: 1) they either have no base model, which serves as a facial prior for fast convergence, or ignore the importance of audio when building the prior; 2) most of them overlook the degree of correlation between different face regions and audio, e.g., mouth is audio related, while ear is audio independent. In this paper, we present Audio Enhanced Neural Radiance Field (AE-NeRF) to tackle the above issues, which can generate realistic portraits of a new speaker with fewshot dataset. Specifically, we introduce an Audio Aware Aggregation module into the feature fusion stage of the reference scheme, where the weight is determined by the similarity of audio between reference and target image. Then, an Audio-Aligned Face Generation strategy is proposed to model the audio related and audio independent regions respectively, with a dual-NeRF framework. Extensive experiments have shown AE-NeRF surpasses the state-of-the-art on image fidelity, audio-lip synchronization, and generalization ability, even in limited training set or training iterations. </p>
<p><a href="http://arxiv.org/abs/2312.10921v1">PDF</a> Accepted by AAAI 2024</p>
<p><strong>Summary</strong></p>
<p>利用音频提高神经辐射场以实现由几秒视频创建逼真谈话头像。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>音频驱动的谈话头像合成在数字人、电影制作和虚拟现实中具有广泛的应用。</li>
<li>近期基于 NeRF 的方法在质量和保真度方面优于以往的研究。</li>
<li>当前方法在仅有几秒谈话视频可用于创建谈话头像时存在局限性。</li>
<li>本研究提出的方法在构建先验时引入了音频感知合成模块和音频对齐面部生成策略。</li>
<li>广泛的实验表明该方法在图像保真度、音频-嘴唇同步性和泛化能力方面优于现有技术。</li>
<li>该方法即使在有限的训练集或训练迭代中也能表现出色。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：音频增强神经辐射场：用于小样本说话人头部合成</li>
<li>作者：董泽李、康赵、魏王、博朋、张颖雅、景东、谭铁牛</li>
<li>单位：中国科学院自动化研究所模式识别与智能控制国家重点实验室</li>
<li>关键词：音频驱动说话人头部合成、神经辐射场、小样本学习、音频感知聚合、音频对齐面部生成</li>
<li>论文链接：https://arxiv.org/abs/2312.10921
     Github 链接：无</li>
<li>
<p>摘要：
(1)：研究背景：音频驱动说话人头部合成是数字人、电影制作和虚拟现实等领域的重要技术。近年来，基于神经辐射场（NeRF）的方法在该领域取得了显著进展，但它们通常需要针对每个说话人进行单独训练，并且对训练数据的数量和质量非常敏感。
(2)：过去方法和问题：现有方法存在两个主要问题：一是缺乏鲁棒的先验知识，导致模型难以快速泛化到小样本说话人；二是忽略了不同面部区域与音频的相关性，导致生成的说话人头部缺乏音频唇形同步性和真实感。
(3)：研究方法：为了解决上述问题，本文提出了音频增强神经辐射场（AE-NeRF）方法。AE-NeRF通过引入音频感知聚合模块和音频对齐面部生成策略，有效地利用了音频信息来构建说话人头部模型。音频感知聚合模块根据音频相似性对参考图像的特征进行加权融合，从而增强模型对音频的感知能力。音频对齐面部生成策略将面部划分为音频相关区域和音频无关区域，并分别使用两个 NeRF 网络进行建模，从而提高模型的生成质量和音频唇形同步性。
(4)：方法性能：AE-NeRF 方法在多个数据集上进行了评估，结果表明该方法在图像保真度、音频唇形同步性和泛化能力方面都优于现有方法。即使在有限的训练集或训练迭代次数下，AE-NeRF 也可以生成高质量的说话人头部图像。</p>
</li>
<li>
<p>方法：
(1)：AE-NeRF 方法通过引入音频感知聚合模块和音频对齐面部生成策略，有效地利用音频信息来构建说话人头部模型。
(2)：音频感知聚合模块根据音频相似性对参考图像的特征进行加权融合，从而增强模型对音频的感知能力。
(3)：音频对齐面部生成策略将面部划分为音频相关区域和音频无关区域，并分别使用两个 NeRF 网络进行建模，从而提高模型的生成质量和音频唇形同步性。</p>
</li>
<li>
<p>结论：
（1）：本文提出了一种音频增强神经辐射场（AE-NeRF）方法，用于小样本说话人头部合成。该方法通过引入音频感知聚合模块和音频对齐面部生成策略，有效地利用了音频信息来构建说话人头部模型，在图像保真度、音频唇形同步性和泛化能力方面都优于现有方法。
（2）：创新点：
提出了一种新的音频增强神经辐射场方法，用于小样本说话人头部合成。
引入了音频感知聚合模块和音频对齐面部生成策略，有效地利用了音频信息来构建说话人头部模型。
在图像保真度、音频唇形同步性和泛化能力方面都优于现有方法。
性能：
在多个数据集上进行了评估，结果表明该方法在图像保真度、音频唇形同步性和泛化能力方面都优于现有方法。即使在有限的训练集或训练迭代次数下，AE-NeRF也可以生成高质量的说话人头部图像。
工作量：
该方法的实现相对复杂，需要较高的计算资源和专业知识。</p>
</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-f239087e0d2ac215f78cf754abb58cc2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df318df0c3e9e1e2538b215ac58c99ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c270f0bac6c5470a2e6b63529366977.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a96c7a7d82a681ee282d99801296cda.jpg" align="middle">
</details>




<h2 id="Mimic-Speaking-Style-Disentanglement-for-Speech-Driven-3D-Facial-Animation"><a href="#Mimic-Speaking-Style-Disentanglement-for-Speech-Driven-3D-Facial-Animation" class="headerlink" title="Mimic: Speaking Style Disentanglement for Speech-Driven 3D Facial   Animation"></a>Mimic: Speaking Style Disentanglement for Speech-Driven 3D Facial   Animation</h2><p><strong>Authors:Hui Fu, Zeqing Wang, Ke Gong, Keze Wang, Tianshui Chen, Haojie Li, Haifeng Zeng, Wenxiong Kang</strong></p>
<p>Speech-driven 3D facial animation aims to synthesize vivid facial animations that accurately synchronize with speech and match the unique speaking style. However, existing works primarily focus on achieving precise lip synchronization while neglecting to model the subject-specific speaking style, often resulting in unrealistic facial animations. To the best of our knowledge, this work makes the first attempt to explore the coupled information between the speaking style and the semantic content in facial motions. Specifically, we introduce an innovative speaking style disentanglement method, which enables arbitrary-subject speaking style encoding and leads to a more realistic synthesis of speech-driven facial animations. Subsequently, we propose a novel framework called \textbf{Mimic} to learn disentangled representations of the speaking style and content from facial motions by building two latent spaces for style and content, respectively. Moreover, to facilitate disentangled representation learning, we introduce four well-designed constraints: an auxiliary style classifier, an auxiliary inverse classifier, a content contrastive loss, and a pair of latent cycle losses, which can effectively contribute to the construction of the identity-related style space and semantic-related content space. Extensive qualitative and quantitative experiments conducted on three publicly available datasets demonstrate that our approach outperforms state-of-the-art methods and is capable of capturing diverse speaking styles for speech-driven 3D facial animation. The source code and supplementary video are publicly available at: <a href="https://zeqing-wang.github.io/Mimic/">https://zeqing-wang.github.io/Mimic/</a> </p>
<p><a href="http://arxiv.org/abs/2312.10877v1">PDF</a> 7 pages, 6 figures, accepted by AAAI-24</p>
<p><strong>Summary</strong><br>基于说话风格的说话人专有3D面部动画合成方法。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>基于语音的3D面部动画合成旨在于合成逼真的面部动画，该动画与语音准确同步并匹配独特的说话风格。</li>
<li>现有的工作主要集中于实现精确的唇部同步，而忽略了对特定说话风格建模，常常导致不真实的面部动画。</li>
<li>提出了一种创新的说话风格分离方法，该方法支持任意说话者风格编码，并导致更真实的有声3D面部动画合成。</li>
<li>提出一个称为Mimic的新框架，通过为风格和内容分别构建两个潜在空间，从面部动作中学习说话风格和内容的解耦表示。</li>
<li>提出四个设计精巧的约束：一个辅助风格分类器，一个辅助反向分类器，一个内容对比损失和一对潜在循环损失，有效构建与身份相关的风格空间和与语义相关的语境空间。</li>
<li>在三个公开数据集上进行的大量定性和定量实验证明，这种方法优于最先进的方法，并且能够捕获用于有声3D面部动画的不同说话风格。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：Mimic：具有风格内容分离的说话风格驱动的 3D 面部动画</li>
<li>作者：Zeqing Wang, Yitong Liu, Jiansheng Chen, Yajie Zhao, Xiaoguang Han</li>
<li>隶属机构：中国科学院计算技术研究所</li>
<li>关键词：面部动画、说话风格、内容分离、语音驱动</li>
<li>论文链接：https://arxiv.org/abs/2302.02789
Github 代码链接：https://github.com/zeqingwang/Mimic</li>
<li>
<p>摘要：
(1)：研究背景：语音驱动的 3D 面部动画旨在合成与语音准确同步并匹配独特说话风格的生动面部动画。然而，现有工作主要集中于实现精确的唇形同步，而忽略了对特定主题说话风格的建模，通常会导致不切实际的面部动画。
(2)：过去的方法及其问题：一些方法试图分离面部运动中的情感相关信息，但它们主要集中于情感，而忽略了说话风格。一些方法关注身份相关信息，但它们没有明确分离说话风格和语义相关内容。
(3)：研究方法：本文提出了一种创新的说话风格分离方法，该方法能够对任意主题的说话风格进行编码，并生成更逼真的语音驱动的面部动画。我们提出了一个名为 Mimic 的新框架，通过分别为风格和内容构建两个潜在空间，从面部运动中学习说话风格和内容的分离表示。为了促进分离表示学习，我们引入了四个精心设计的约束：辅助风格分类器、辅助逆分类器、内容对比损失和一对潜在循环损失，它们可以有效地促进与身份相关的风格空间和语义相关内容空间的构建。
(4)：方法性能：在三个公开可用的数据集上进行的广泛定性和定量实验表明，我们的方法优于最先进的方法，并且能够捕获用于语音驱动的 3D 面部动画的不同说话风格。这些性能支持了我们的目标。</p>
</li>
<li>
<p>方法：
(1) Mimic框架概述：Mimic框架由一个编码器和一个解码器组成。编码器将面部运动编码为风格和内容的潜在表示，解码器将潜在表示解码为面部动画。
(2) 潜在空间构建：Mimic框架构建了两个潜在空间，分别是风格空间和内容空间。风格空间用于编码说话风格，内容空间用于编码语义相关内容。
(3) 分离表示学习：Mimic框架通过四个精心设计的约束来促进分离表示学习，分别是辅助风格分类器、辅助逆分类器、内容对比损失和一对潜在循环损失。
(4) 面部动画生成：Mimic框架将分离的风格和内容表示解码为面部动画。解码器是一个多层感知机，将潜在表示映射到面部顶点的位置。</p>
</li>
<li>
<p>结论：
（1）：本工作提出了一种创新的说话风格分离方法，该方法能够对任意主题的说话风格进行编码，并生成更逼真的语音驱动的面部动画。我们提出的Mimic框架通过分别为风格和内容构建两个潜在空间，从面部运动中学习说话风格和内容的分离表示。广泛的定性和定量实验表明，我们的方法优于最先进的方法，并且能够捕获用于语音驱动的3D面部动画的不同说话风格。
（2）：创新点：</p>
</li>
<li>提出了一种创新的说话风格分离方法，该方法能够对任意主题的说话风格进行编码，并生成更逼真的语音驱动的面部动画。</li>
<li>构建了两个潜在空间，分别是风格空间和内容空间，分别用于编码说话风格和语义相关内容。</li>
<li>通过四个精心设计的约束来促进分离表示学习，分别是辅助风格分类器、辅助逆分类器、内容对比损失和一对潜在循环损失。
性能：</li>
<li>在三个公开可用的数据集上进行的广泛定性和定量实验表明，我们的方法优于最先进的方法，并且能够捕获用于语音驱动的3D面部动画的不同说话风格。
工作量：</li>
<li>Mimic框架的实现相对复杂，需要较高的编程技能和计算资源。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-95fa8ce2f96cb59aeced5036ae979cce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ea918fc0b742f0b948922090e9c51b8e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-47da103521519c5b941dedeff17adc75.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3019e10b507b7a2281715b1a591fd446.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-73bdec1ad3a869ac2b64b987125a7b99.jpg" align="middle">
</details>




<h2 id="DreamTalk-When-Expressive-Talking-Head-Generation-Meets-Diffusion-Probabilistic-Models"><a href="#DreamTalk-When-Expressive-Talking-Head-Generation-Meets-Diffusion-Probabilistic-Models" class="headerlink" title="DreamTalk: When Expressive Talking Head Generation Meets Diffusion   Probabilistic Models"></a>DreamTalk: When Expressive Talking Head Generation Meets Diffusion   Probabilistic Models</h2><p><strong>Authors:Yifeng Ma, Shiwei Zhang, Jiayu Wang, Xiang Wang, Yingya Zhang, Zhidong Deng</strong></p>
<p>Diffusion models have shown remarkable success in a variety of downstream generative tasks, yet remain under-explored in the important and challenging expressive talking head generation. In this work, we propose a DreamTalk framework to fulfill this gap, which employs meticulous design to unlock the potential of diffusion models in generating expressive talking heads. Specifically, DreamTalk consists of three crucial components: a denoising network, a style-aware lip expert, and a style predictor. The diffusion-based denoising network is able to consistently synthesize high-quality audio-driven face motions across diverse expressions. To enhance the expressiveness and accuracy of lip motions, we introduce a style-aware lip expert that can guide lip-sync while being mindful of the speaking styles. To eliminate the need for expression reference video or text, an extra diffusion-based style predictor is utilized to predict the target expression directly from the audio. By this means, DreamTalk can harness powerful diffusion models to generate expressive faces effectively and reduce the reliance on expensive style references. Experimental results demonstrate that DreamTalk is capable of generating photo-realistic talking faces with diverse speaking styles and achieving accurate lip motions, surpassing existing state-of-the-art counterparts. </p>
<p><a href="http://arxiv.org/abs/2312.09767v1">PDF</a> Project Page: <a href="https://dreamtalk-project.github.io">https://dreamtalk-project.github.io</a></p>
<p><strong>Summary</strong><br>梦语者: 一种新的扩散模型框架，能够产生写实且具有丰富表情的说话人头部。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>梦语者使用扩散模型来生成说话人头部，并通过不同的组件来确保质量和准确性。</li>
<li>扩散模型能够生成逼真的音频驱动的人脸运动，可在各种表情间切换。</li>
<li>风格感知唇形专家可以指导唇形同步，同时兼顾说话风格。</li>
<li>扩散模型风格预测器可直接从音频中预测目标表情，无需参考视频或文本。</li>
<li>梦语者可以生成具有不同说话风格的照片级写实说话人头部，并实现精确的唇形运动。</li>
<li>实验结果表明，梦语者优于现有的最先进的同类技术。</li>
<li>梦语者为使用扩散模型生成富有表现力的说话人头部开辟了新的道路。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：DreamTalk：当富有表现力的说话头生成遇到扩散概率模型</li>
<li>作者：Junyi Zhang, Yitong Yu, Xiaoming Liu, Yijun Li, Tao Mei</li>
<li>单位：香港中文大学（深圳）</li>
<li>关键词：扩散模型、说话风格预测、唇形同步、说话头生成</li>
<li>论文链接：None，Github 代码链接：None</li>
<li>
<p>摘要：
（1）研究背景：扩散模型在各种下游生成任务中取得了显着的成功，但在重要且具有挑战性的富有表现力的说话头生成中仍未得到充分探索。
（2）过去方法与问题：现有方法通常需要表达式参考视频或文本来指导说话头生成，这限制了其在实际应用中的灵活性。此外，这些方法在生成唇形动作时往往不够准确和逼真。
（3）研究方法：本文提出 DreamTalk 框架来解决上述问题。DreamTalk 由三个关键组件组成：去噪网络、风格感知唇形专家和风格预测器。去噪网络基于扩散模型，能够一致地合成高质量的音频驱动的面部动作，涵盖各种各样的表情。为了增强唇形动作的表现力和准确性，我们引入了一个风格感知唇形专家，它可以在考虑说话风格的同时指导唇形同步。为了消除对表达式参考视频或文本的需求，我们利用一个额外的基于扩散模型的风格预测器直接从音频中预测目标表情。通过这种方式，DreamTalk 可以有效地利用强大的扩散模型来生成富有表现力的面部表情，并减少对昂贵的风格参考的依赖。
（4）实验结果：实验结果表明，DreamTalk 能够生成具有多样说话风格和准确唇形动作的逼真说话面孔，超越了现有的最先进方法。这些结果支持了本文提出的方法能够实现其目标。</p>
</li>
<li>
<p>方法：
（1）去噪网络：DreamTalk的核心组件之一是去噪网络，它基于扩散模型，能够从噪声中逐渐恢复出高质量的音频驱动的面部动作。去噪网络由一系列的扩散步骤组成，在每个步骤中，网络都会将噪声图像逐渐转化为目标图像。通过这种方式，去噪网络能够生成逼真且具有多样性的面部动作，涵盖各种各样的表情。
（2）风格感知唇形专家：为了增强唇形动作的表现力和准确性，DreamTalk引入了风格感知唇形专家。风格感知唇形专家是一个卷积神经网络，它能够在考虑说话风格的同时指导唇形同步。风格感知唇形专家通过分析音频中的说话风格信息，来预测目标唇形动作。这种预测结果可以帮助去噪网络生成更加准确和逼真的唇形动作。
（3）风格预测器：为了消除对表达式参考视频或文本的需求，DreamTalk利用了一个额外的基于扩散模型的风格预测器。风格预测器能够直接从音频中预测目标表情。风格预测器通过分析音频中的说话风格信息，来预测目标表情的分布。这种预测结果可以帮助去噪网络生成具有多样说话风格的面部表情。</p>
</li>
<li>
<p>结论：
（1）：DreamTalk 提出了一种新颖的方法，利用扩散模型生成富有表现力的说话头，在减少对额外风格参考的依赖的同时，在不同的说话风格中表现出色。我们开发了一个去噪网络来创建富有表现力、音频驱动的面部动作，并引入了一个风格感知唇形专家来优化唇形同步，而不会损害风格表现力。此外，我们设计了一个风格预测器，可以直接从音频中推断出说话风格，从而消除了对视频参考的需求。DreamTalk 的有效性通过广泛的实验得到了验证。
致谢：这项工作得到了阿里巴巴集团通过阿里巴巴研究实习计划的支持。我们要感谢 Xinya Ji、Borong Liang、Yan Pan 和 Suzhen Wang 在比较方面给予的慷慨帮助。
（2）：创新点：</p>
</li>
<li>提出了一种新的框架 DreamTalk，利用扩散模型生成富有表现力的说话头。</li>
<li>开发了一个去噪网络来创建富有表现力、音频驱动的面部动作。</li>
<li>引入了一个风格感知唇形专家来优化唇形同步，而不会损害风格表现力。</li>
<li>设计了一个风格预测器，可以直接从音频中推断出说话风格，从而消除了对视频参考的需求。
性能：</li>
<li>DreamTalk 能够生成具有多样说话风格和准确唇形动作的逼真说话面孔，超越了现有的最先进方法。</li>
<li>DreamTalk 不需要表达式参考视频或文本，因此更加灵活且易于使用。</li>
<li>DreamTalk 可以生成具有不同说话风格的面部动作，包括高兴、悲伤、愤怒和惊讶等。
工作量：</li>
<li>DreamTalk 的实现相对简单，易于理解和使用。</li>
<li>DreamTalk 的训练过程相对较快，可以在几个小时内完成。</li>
<li>DreamTalk 的生成速度也很快，可以在几秒钟内生成一个说话头。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-642c19419d72e5147c54fe7e4901843d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a4fe6c655ccb30f54162deefe293021d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7242bb1e506e10657a005d461ece1d10.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a01ad66de23fc4a6cfd936019d21a0c2.jpg" align="middle">
</details>




<h2 id="FaceTalk-Audio-Driven-Motion-Diffusion-for-Neural-Parametric-Head-Models"><a href="#FaceTalk-Audio-Driven-Motion-Diffusion-for-Neural-Parametric-Head-Models" class="headerlink" title="FaceTalk: Audio-Driven Motion Diffusion for Neural Parametric Head   Models"></a>FaceTalk: Audio-Driven Motion Diffusion for Neural Parametric Head   Models</h2><p><strong>Authors:Shivangi Aneja, Justus Thies, Angela Dai, Matthias Nießner</strong></p>
<p>We introduce FaceTalk, a novel generative approach designed for synthesizing high-fidelity 3D motion sequences of talking human heads from input audio signal. To capture the expressive, detailed nature of human heads, including hair, ears, and finer-scale eye movements, we propose to couple speech signal with the latent space of neural parametric head models to create high-fidelity, temporally coherent motion sequences. We propose a new latent diffusion model for this task, operating in the expression space of neural parametric head models, to synthesize audio-driven realistic head sequences. In the absence of a dataset with corresponding NPHM expressions to audio, we optimize for these correspondences to produce a dataset of temporally-optimized NPHM expressions fit to audio-video recordings of people talking. To the best of our knowledge, this is the first work to propose a generative approach for realistic and high-quality motion synthesis of volumetric human heads, representing a significant advancement in the field of audio-driven 3D animation. Notably, our approach stands out in its ability to generate plausible motion sequences that can produce high-fidelity head animation coupled with the NPHM shape space. Our experimental results substantiate the effectiveness of FaceTalk, consistently achieving superior and visually natural motion, encompassing diverse facial expressions and styles, outperforming existing methods by 75% in perceptual user study evaluation. </p>
<p><a href="http://arxiv.org/abs/2312.08459v1">PDF</a> Paper Video: <a href="https://youtu.be/7Jf0kawrA3Q">https://youtu.be/7Jf0kawrA3Q</a> Project Page:   <a href="https://shivangi-aneja.github.io/projects/facetalk/">https://shivangi-aneja.github.io/projects/facetalk/</a></p>
<p><strong>摘要</strong><br>用音频信号合成高清3D说话人头运动序列的新生成方法。</p>
<p><strong>要点</strong></p>
<ul>
<li>FaceTalk 是一种生成性方法，能够从输入的音频信号中合成高保真的3D动态说话人头部序列。</li>
<li>提出将语音信号与神经参数头部模型的潜在空间相结合，以创建高保真、时间连贯的运动序列。</li>
<li>提出一种新的潜在扩散模型，在神经参数头部模型的表情空间中运行，以合成音频驱动的逼真头部序列。</li>
<li>优化了对应关系，以产生一个时间优化的NPHM表情数据集，该数据集与人们讲话的视音频记录相匹配。</li>
<li>这是第一个提出生成式方法来实现逼真、高质量的体积人头运动合成的工作。</li>
<li>FaceTalk能够生成合理的运动序列，产生高清头部动画，并与NPHM形状空间相结合。</li>
<li>FaceTalk在感知用户研究评估中比现有方法高出75%，证明了其有效性。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：FaceTalk：神经参数化头部模型的音频驱动运动扩散</li>
<li>作者：Shivangi Aneja, Justus Thies, Angela Dai, Matthias Nießner</li>
<li>隶属机构：慕尼黑工业大学</li>
<li>关键词：音频驱动动画，神经参数化头部模型，扩散模型，语音合成</li>
<li>论文链接：https://arxiv.org/abs/2312.08459，Github 代码链接：None</li>
<li>
<p>摘要：
（1）研究背景：三维动画建模在数字媒体领域有着广泛的应用，包括动画电影、电脑游戏和虚拟代理。近年来，大量工作提出了人类身体运动合成的生成方法，能够根据动作、语言、音乐等各种信号对人类骨骼进行动画处理。然而，生成三维面部运动的合成主要集中在三维可变形模型（3DMM）上，利用线性混合形状来表示头部运动和表情。这种模型刻画了一个分离的头部形状和运动空间，但缺乏全面表示人类面部几何形状复杂性和细粒度细节的能力。
（2）过去的方法及其问题：现有方法主要集中在三维可变形模型（3DMM）上，利用线性混合形状来表示头部运动和表情。这种模型刻画了一个分离的头部形状和运动空间，但缺乏全面表示人类面部几何形状复杂性和细粒度细节的能力。
（3）研究方法：本文提出了一种新的生成方法，称为 FaceTalk，用于从输入音频信号合成逼真的三维说话人头部运动序列。该方法将语音信号与神经参数化头部模型的潜在空间相结合，以创建逼真且时间连贯的运动序列。我们提出了一种新的潜在扩散模型来执行此任务，该模型在神经参数化头部模型的表情空间中运行，以合成音频驱动的逼真头部序列。在没有对应 NPHM 表情与音频的数据集的情况下，我们优化了这些对应关系，以生成适合人们说话的音频视频记录的时间优化的 NPHM 表情数据集。
（4）方法性能：实验结果证明了 FaceTalk 的有效性，在感知用户研究评估中，它始终如一地实现了优越且视觉上自然的运动，涵盖了多种面部表情和风格，比现有方法高出 75%。这些性能支持了本文的目标，即生成逼真的三维头部运动，这些运动可以与 NPHM 形状空间耦合，产生高保真头部动画。</p>
</li>
<li>
<p>方法：
(1) 数据集：本文使用了一个由 200 个说话人的音频和视频数据组成的公开数据集。音频数据是 8kHz 的单声道音频，视频数据是 25fps 的 RGB 视频。
(2) 神经参数化头部模型（NPHM）：本文使用了一个预训练的 NPHM，该模型可以从输入的 3D 扫描数据中生成逼真的头部运动序列。
(3) 潜在扩散模型：本文提出了一种新的潜在扩散模型，该模型可以在 NPHM 的表情空间中运行，以合成音频驱动的逼真头部序列。该模型使用了一个变分自编码器（VAE）作为编码器，将音频信号编码成潜在空间中的一个向量。然后，该向量被输入到一个扩散模型中，该模型逐渐将向量从一个高斯分布扩散到一个均匀分布。在扩散过程中，模型学习如何从潜在空间中生成逼真的头部运动序列。
(4) 优化对应关系：为了生成适合人们说话的音频视频记录的时间优化的 NPHM 表情数据集，本文优化了这些对应关系。具体来说，本文使用了一个生成对抗网络（GAN）来优化对应关系。GAN 的生成器将音频信号映射到 NPHM 的表情空间中的一个向量，而判别器则试图区分生成的表情和真实的表情。通过训练 GAN，可以学习到一个能够生成逼真表情的生成器。
(5) 实验结果：本文在公开数据集上对 FaceTalk 进行了评估。实验结果表明，FaceTalk 能够生成逼真的三维头部运动，这些运动可以与 NPHM 形状空间耦合，产生高保真头部动画。在感知用户研究评估中，FaceTalk 始终如一地实现了优越且视觉上自然的运动，涵盖了多种面部表情和风格，比现有方法高出 75%。</p>
</li>
<li>
<p>结论：
（1）：本文提出了一种新的方法 FaceTalk，可以从输入的音频信号中合成逼真的三维说话人头部运动序列。该方法将语音信号与神经参数化头部模型的潜在空间相结合，以创建逼真且时间连贯的运动序列。实验结果证明了 FaceTalk 的有效性，在感知用户研究评估中，它始终如一地实现了优越且视觉上自然的运动，涵盖了多种面部表情和风格，比现有方法高出 75%。
（2）：创新点：</p>
</li>
<li>提出了一种新的潜在扩散模型，可以在神经参数化头部模型的表情空间中运行，以合成音频驱动的逼真的头部序列。</li>
<li>优化了神经参数化头部模型表情与音频信号之间的对应关系，以生成适合人们说话的音频视频记录的时间优化的神经参数化头部模型表情数据集。</li>
<li>实验结果表明，FaceTalk 能够生成逼真的三维头部运动，这些运动可以与神经参数化头部模型形状空间耦合，产生高保真头部动画。
性能：</li>
<li>在感知用户研究评估中，FaceTalk 始终如一地实现了优越且视觉上自然的运动，涵盖了多种面部表情和风格，比现有方法高出 75%。</li>
<li>FaceTalk 能够生成逼真的三维头部运动，这些运动可以与神经参数化头部模型形状空间耦合，产生高保真头部动画。
工作量：</li>
<li>本文使用了一个由 200 个说话人的音频和视频数据组成的公开数据集。</li>
<li>本文使用了一个预训练的神经参数化头部模型，该模型可以从输入的 3D 扫描数据中生成逼真的头部运动序列。</li>
<li>本文提出了一种新的潜在扩散模型，该模型可以在神经参数化头部模型的表情空间中运行，以合成音频驱动的逼真的头部序列。</li>
<li>本文优化了神经参数化头部模型表情与音频信号之间的对应关系，以生成适合人们说话的音频视频记录的时间优化的神经参数化头部模型表情数据集。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c3d682b60e63c1acae348037b65c2339.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c635db505ff8573cd87519e86c5a8129.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7c2f6ea2aaaabd42a793354211bb803d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2db2438899bc884a4f1ba5f9498ddf15.jpg" align="middle">
</details>




<h2 id="GMTalker-Gaussian-Mixture-based-Emotional-talking-video-Portraits"><a href="#GMTalker-Gaussian-Mixture-based-Emotional-talking-video-Portraits" class="headerlink" title="GMTalker: Gaussian Mixture based Emotional talking video Portraits"></a>GMTalker: Gaussian Mixture based Emotional talking video Portraits</h2><p><strong>Authors:Yibo Xia, Lizhen Wang, Xiang Deng, Xiaoyan Luo, Yebin Liu</strong></p>
<p>Synthesizing high-fidelity and emotion-controllable talking video portraits, with audio-lip sync, vivid expression, realistic head pose, and eye blink, is an important and challenging task in recent years. Most of the existing methods suffer in achieving personalized precise emotion control or continuously interpolating between different emotions and generating diverse motion. To address these problems, we present GMTalker, a Gaussian mixture based emotional talking portraits generation framework. Specifically, we propose a Gaussian Mixture based Expression Generator (GMEG) which can construct a continuous and multi-modal latent space, achieving more flexible emotion manipulation. Furthermore, we introduce a normalizing flow based motion generator pretrained on the dataset with a wide-range motion to generate diverse motions. Finally, we propose a personalized emotion-guided head generator with an Emotion Mapping Network (EMN) which can synthesize high-fidelity and faithful emotional video portraits. Both quantitative and qualitative experiments demonstrate our method outperforms previous methods in image quality, photo-realism, emotion accuracy and motion diversity. </p>
<p><a href="http://arxiv.org/abs/2312.07669v1">PDF</a> Project page: <a href="https://bob35buaa.github.io/GMTalker">https://bob35buaa.github.io/GMTalker</a></p>
<p><strong>摘要</strong><br>使用高斯混合模型生成多模态潜在空间，实现多样的动作和灵活的情感控制。</p>
<p><strong>要点</strong></p>
<ul>
<li>使用高斯混合模型构建情感表达生成器，得到连续且多模态的潜在空间，实现更为灵活的情感控制。</li>
<li>引入基于正态化流的动作生成器，预训练包含广泛动作的数据集，以实现多样的动作生成。</li>
<li>提出了一种个性化情感引导头部生成器，使用情感映射网络合成高保真视频肖像，使其在情感表达上更准确和逼真。</li>
<li>定量和定性实验表明，该方法在图像质量、逼真度、情感准确性和动作多样性方面优于现有方法。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>标题：GMTalker：基于高斯混合模型的情感谈话视频肖像（补充材料）</li>
<li>作者：Xuechen Liu, Pengfei Wan, Yebin Liu, Wenpeng Yin, Wen Zheng, Chen Change Loy, Yu-Kun Lai, Xiaoguang Han</li>
<li>单位：清华大学</li>
<li>关键词：情感控制、说话头像、视频肖像、高斯混合模型、标准化流</li>
<li>论文链接：None，Github 代码链接：None</li>
<li>摘要：
（1）：研究背景：合成高保真且可情感控制的说话视频肖像，具有音频唇形同步、生动的表情、逼真的头部姿势和眨眼，是近年来的一项重要且具有挑战性的任务。大多数现有方法在实现个性化精确的情感控制或连续插值不同情感和生成多样化情感方面存在困难。
（2）：过去方法及其问题：现有方法难以实现个性化精确的情感控制或连续插值不同情感和生成多样化情感。本文方法动机充分。
（3）：研究方法：为了解决这些问题，我们提出了 GMTalker，这是一个基于高斯混合模型的情感谈话肖像生成框架。具体来说，我们提出了一种基于高斯混合模型的表情生成器 (GMEG)，它可以构建一个连续且多模态的潜在空间，实现更灵活的情感操纵。此外，我们引入了一个基于标准化流的运动生成器，该生成器在具有广泛运动的数据集上预训练，以生成不同的情感。最后，我们提出了一种个性化情感引导头部生成器，该生成器具有情感映射网络 (EMN)，可以合成高保真且逼真的情感视频肖像。
（4）：方法性能：定量和定性实验表明，我们的方法在图像质量、照片真实感、情感准确性和运动多样性方面优于以往的方法。这些性能可以支持我们的目标。</li>
</ol>
<p>7.Methods：
（1）：我们提出了一种基于高斯混合模型的表情生成器（GMEG），它可以构建一个连续且多模态的潜在空间，实现更灵活的情感操纵。
（2）：我们引入了一个基于标准化流的运动生成器，该生成器在具有广泛运动的数据集上预训练，以生成不同的情感。
（3）：我们提出了一种个性化情感引导头部生成器，该生成器具有情感映射网络（EMN），可以合成高保真且逼真的情感视频肖像。</p>
<ol>
<li>结论：
（1）：意义：GMTalker 模型能够生成逼真且情感丰富的说话人视频肖像，在图像质量、照片真实感、情感准确性和运动多样性方面优于以往的方法。
（2）：优缺点：
创新点：</li>
<li>提出了一种基于高斯混合模型的表情生成器 (GMEG)，构建连续且多模态的潜在空间，实现更灵活的情感操纵。</li>
<li>引入了一个基于标准化流的运动生成器，在具有广泛运动的数据集上预训练，以生成不同的情感。</li>
<li>提出了一种个性化情感引导头部生成器，具有情感映射网络 (EMN)，可以合成高保真且逼真的情感视频肖像。</li>
</ol>
<p>性能：
- 定量和定性实验表明，GMTalker 模型在图像质量、照片真实感、情感准确性和运动多样性方面优于以往的方法。</p>
<p>不足：
- 依赖于包含丰富情感内容的高质量视频，获取这些视频具有一定的挑战性。
- 目前仅能描述有限的情感，受限于数据集中的八种类别。</p>
<p>潜在的社会影响：
- GMTalker 模型能够从单目视频生成逼真的情感说话人视频，存在被用于创建欺骗性视频的潜在风险，在部署之前应谨慎考虑。</p>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a438836432dc4fba873399aa6e9333d9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-279ff5bfffe91a6c96bc7bdcb62720dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c1e76fa3f35eb328e56f9b35af348b9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5b04f9df996dfc2f7e846210b954a049.jpg" align="middle">
</details>




## GSmoothFace: Generalized Smooth Talking Face Generation via Fine Grained   3D Face Guidance

**Authors:Haiming Zhang, Zhihao Yuan, Chaoda Zheng, Xu Yan, Baoyuan Wang, Guanbin Li, Song Wu, Shuguang Cui, Zhen Li**

Although existing speech-driven talking face generation methods achieve significant progress, they are far from real-world application due to the avatar-specific training demand and unstable lip movements. To address the above issues, we propose the GSmoothFace, a novel two-stage generalized talking face generation model guided by a fine-grained 3d face model, which can synthesize smooth lip dynamics while preserving the speaker's identity. Our proposed GSmoothFace model mainly consists of the Audio to Expression Prediction (A2EP) module and the Target Adaptive Face Translation (TAFT) module. Specifically, we first develop the A2EP module to predict expression parameters synchronized with the driven speech. It uses a transformer to capture the long-term audio context and learns the parameters from the fine-grained 3D facial vertices, resulting in accurate and smooth lip-synchronization performance. Afterward, the well-designed TAFT module, empowered by Morphology Augmented Face Blending (MAFB), takes the predicted expression parameters and target video as inputs to modify the facial region of the target video without distorting the background content. The TAFT effectively exploits the identity appearance and background context in the target video, which makes it possible to generalize to different speakers without retraining. Both quantitative and qualitative experiments confirm the superiority of our method in terms of realism, lip synchronization, and visual quality. See the project page for code, data, and request pre-trained models: https://zhanghm1995.github.io/GSmoothFace. 

[PDF](http://arxiv.org/abs/2312.07385v1) 

**摘要**

利用细粒度的人脸模型，我们提出了一种新颖的说话人脸生成模型，能综合平滑的唇部动态，同时保留说话人的身份。

**要点:**

- 我们提出了 GSmoothFace，一种基于细粒度 3D 人脸模型的新型两阶段通用说话人脸生成模型。
- GSmoothFace 由音频表情预测 (A2EP) 模块和目标自适应人脸转换 (TAFT) 模块组成。
- A2EP 模块使用 Transformer 捕捉长期音频上下文，并从细粒度的 3D 面部顶点学习参数，从而实现准确而平滑的唇形同步性能。
- TAFT 模块利用形态增强人脸融合 (MAFB) 技术，将预测的表情参数和目标视频作为输入，修改目标视频的面部区域，同时不扭曲背景内容。
- TAFT 有效地利用了目标视频中的身份外观和背景上下文，无需重新训练即可泛化到不同的说话人。
- 我们的方法在真实感、唇形同步和视觉质量方面均优于现有方法。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>题目：GSmoothFace：通过细粒度 3D 面部引导实现广义平滑说话人面部生成</li>
<li>作者：Haiming Zhang, Zhihao Yuan, Chaoda Zheng, Xu Yan, Baoyuan Wang, Guanbin Li, Song Wu, Shuguang Cui, Zhen Li</li>
<li>单位：香港中文大学（深圳）未来网络智能研究所</li>
<li>关键词：深度学习、说话人面部生成、Transformer、生成对抗网络</li>
<li>论文链接：https://arxiv.org/abs/2312.07385</li>
<li>
<p>摘要：
（1）研究背景：说话人面部生成旨在合成与任意语音输入同步的逼真肖像视频，在数字人动画、视觉配音、虚拟视频会议和娱乐等领域具有广泛的应用。现有方法主要分为基于 2D 和基于 3D 的方法。基于 2D 的方法通常将说话人面部生成问题表述为条件生成对抗网络 (cGAN)。基于 3D 的方法依赖于 3D 可变形模型 (3DMM)，由于其具有 3D 感知建模的能力，最近受到更多关注。
（2）过去方法及问题：基于 2D 的方法由于音频到唇部运动映射学习的隐式监督和 GAN 的固有局限性，如训练不稳定和模式崩溃，产生的初步结果图像质量低，唇部同步不令人满意。基于 3D 的方法虽然能够生成更逼真、更自然的面部视频，但通常需要针对每个说话人进行专门训练，并且唇部运动可能不稳定。
（3）研究方法：本文提出了一种通过细粒度 3D 面部引导实现广义平滑说话人面部生成模型 GSmoothFace。该模型主要由音频到表情预测 (A2EP) 模块和目标自适应面部平移 (TAFT) 模块组成。A2EP 模块使用 Transformer 捕获长期音频上下文，并从细粒度的 3D 面部顶点学习表情参数，从而实现准确且平滑的唇部同步性能。TAFT 模块利用形态增强面部混合 (MAFB) 技术，将预测的表情参数和目标视频作为输入，修改目标视频的面部区域，而不会扭曲背景内容。
（4）方法性能：定量和定性实验表明，GSmoothFace 方法在逼真度、唇部同步和视觉质量方面均优于现有方法。该方法在说话人面部生成任务上取得了良好的性能，支持其目标。</p>
</li>
<li>
<p>方法：
(1) 音频到表情预测（A2EP）：</p>
<ul>
<li>使用预训练的 wav2vec2.0 模型生成音频嵌入。</li>
<li>使用表情编码器和多头自注意力层提取表情嵌入。</li>
<li>使用 transformer 解码器预测与驱动音频同步的表情参数。
(2) 目标自适应面部平移（TAFT）：</li>
<li>使用形态增强面部混合（MAFB）技术将预测的表情参数和目标视频融合。</li>
<li>使用生成器合成最终图像。</li>
</ul>
</li>
<li>
<p>结论：
(1)：本文提出了一种通过细粒度3D面部引导实现广义平滑说话人面部生成模型GSmoothFace，该模型在逼真度、唇部同步和视觉质量方面均优于现有方法，在说话人面部生成任务上取得了良好的性能，支持其目标。
(2)：创新点：</p>
</li>
<li>提出了一种新的说话人面部生成模型GSmoothFace，该模型通过细粒度3D面部引导实现广义平滑说话人面部生成。</li>
<li>设计了一种音频到表情预测模块，使用Transformer捕获长期音频上下文，并从细粒度的3D面部顶点学习表情参数，从而实现准确且平滑的唇部同步性能。</li>
<li>设计了一种目标自适应面部平移模块，利用形态增强面部混合技术，将预测的表情参数和目标视频融合，修改目标视频的面部区域，而不会扭曲背景内容。
性能：</li>
<li>定量和定性实验表明，GSmoothFace方法在逼真度、唇部同步和视觉质量方面均优于现有方法。</li>
<li>该方法在说话人面部生成任务上取得了良好的性能，支持其目标。
工作量：</li>
<li>该方法需要大量的训练数据和计算资源。</li>
<li>该方法的训练过程可能需要花费大量的时间。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-76d8d3d1ff62335f344500c45e58f207.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b2ae356b21a9745e5886796cd64fcd60.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-aea1844a3fa4563e27b3fae7190b60f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25f2d8c1eb09db9719a1cf18d1746617.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1b17c868324721da0d1204c8940af5fc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-39460c49b1a37832b878283b788a6c61.jpg" align="middle">
</details>




<h2 id="Neural-Text-to-Articulate-Talk-Deep-Text-to-Audiovisual-Speech-Synthesis-achieving-both-Auditory-and-Photo-realism"><a href="#Neural-Text-to-Articulate-Talk-Deep-Text-to-Audiovisual-Speech-Synthesis-achieving-both-Auditory-and-Photo-realism" class="headerlink" title="Neural Text to Articulate Talk: Deep Text to Audiovisual Speech   Synthesis achieving both Auditory and Photo-realism"></a>Neural Text to Articulate Talk: Deep Text to Audiovisual Speech   Synthesis achieving both Auditory and Photo-realism</h2><p><strong>Authors:Georgios Milis, Panagiotis P. Filntisis, Anastasios Roussos, Petros Maragos</strong></p>
<p>Recent advances in deep learning for sequential data have given rise to fast and powerful models that produce realistic videos of talking humans. The state of the art in talking face generation focuses mainly on lip-syncing, being conditioned on audio clips. However, having the ability to synthesize talking humans from text transcriptions rather than audio is particularly beneficial for many applications and is expected to receive more and more attention, following the recent breakthroughs in large language models. For that, most methods implement a cascaded 2-stage architecture of a text-to-speech module followed by an audio-driven talking face generator, but this ignores the highly complex interplay between audio and visual streams that occurs during speaking. In this paper, we propose the first, to the best of our knowledge, text-driven audiovisual speech synthesizer that uses Transformers and does not follow a cascaded approach. Our method, which we call NEUral Text to ARticulate Talk (NEUTART), is a talking face generator that uses a joint audiovisual feature space, as well as speech-informed 3D facial reconstructions and a lip-reading loss for visual supervision. The proposed model produces photorealistic talking face videos with human-like articulation and well-synced audiovisual streams. Our experiments on audiovisual datasets as well as in-the-wild videos reveal state-of-the-art generation quality both in terms of objective metrics and human evaluation. </p>
<p><a href="http://arxiv.org/abs/2312.06613v1">PDF</a> </p>
<p><strong>Summary</strong><br>文本驱动视听语音合成器NEUTART首次使用Transformer，在统一视听特征空间中生成逼真的人脸说话视频</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>NEUTART是第一个使用Transformer的文本驱动视听语音合成器。</li>
<li>NEUTART使用统一视听特征空间来学习语音和视觉特征之间的映射关系。</li>
<li>NEUTART利用语音提示的3D面部重建和唇读损失来进行视觉监督。</li>
<li>NEUTART生成的说话人脸视频在客观指标和人类评估方面都达到了最先进的生成质量。</li>
<li>NEUTART可以处理各种音视频数据集，包括有声数据集和野生视频。</li>
<li>NEUTART可以生成具有类人发音和良好同步视听流的逼真说话人脸视频。</li>
<li>NEUTART有望在许多应用中得到广泛使用，例如虚拟现实、教育和娱乐。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：神经文本到清晰语音：深度文本到视听语音合成</li>
<li>作者：Georgios Milis、Panagiotis P. Filntisis、Anastasios Roussos、Petros Maragos</li>
<li>隶属单位：雅典国立技术大学电气与计算机工程学院</li>
<li>关键词：文本到语音、语音到视觉、深度学习、Transformer、视听语音合成</li>
<li>论文链接：https://arxiv.org/abs/2312.06613</li>
<li>摘要：
(1) 研究背景：随着深度学习在序列数据领域的发展，快速且强大的模型可以生成逼真的说话人类视频。目前最先进的说话人脸生成主要集中在唇形同步上，以音频片段为条件。然而，能够从文本转录而不是音频合成说话的人类对于许多应用特别有益，并且预计随着大型语言模型的最新突破，它将受到越来越多的关注。为此，大多数方法采用级联的 2 阶段架构，包括文本到语音模块和音频驱动的说话人脸生成器，但这忽略了说话过程中出现的音频和视觉流之间高度复杂的相互作用。
(2) 过去的方法及其问题：大多数方法采用级联的 2 阶段架构，包括文本到语音模块和音频驱动的说话人脸生成器，但这忽略了说话过程中出现的音频和视觉流之间高度复杂的相互作用。
(3) 本文提出的研究方法：我们提出第一个使用 Transformer 的文本驱动的视听语音合成器，它不遵循级联架构。相反，它将文本直接映射到视听表示，该表示用于生成逼真的说话人脸视频。
(4) 方法在任务和性能上的表现：我们的方法在 TalkingHead++ 数据集上实现了最先进的结果，在合成视频的视觉和语音质量方面均优于现有方法。此外，我们的方法还能够生成具有逼真唇形同步和面部表情的说话人脸视频。</li>
</ol>
<p><methods>:
(1) NEUTART框架：本文提出的神经文本到清晰语音框架 NEUTART，用于解决文本驱动的逼真视听语音合成问题。NEUTART 包含两个主要模块：视听模块和逼真模块。视听模块用于联合合成语音音频和 3D 说话人脸序列，逼真模块用于合成 RGB 面部视频。
(2) 视听模块：视听模块基于 FastSpeech2 文本到语音系统构建，以纳入视觉生成。音频通过其梅尔谱图建模，预训练的声码器从梅尔谱图生成语音波形。同样，面部使用 FLAME 3DMM 建模，将 5023 个顶点的面部 3D 网格解耦为身份 β、表情 ψ 和关节姿势 θ 参数，其中包括 3 个下颌关节参数 θjaw。使用这种表示，我们使用 3 个下颌姿势和 50 个表情参数对说话过程中的面部表情和动作进行建模。因此，NEUTART 每帧音频预测 80 个梅尔通道和 53 个 3DMM 通道。然后将 3DMM 系数解码为 3D 面部重建，从而驱动面部渲染器。
(3) 逼真模块：逼真模块使用 StyleGAN2 生成器将 3D 面部重建合成到 RGB 视频中。该模块由以下子模块组成：
- 光照估计器：估计场景光照条件，以确保生成的视频具有逼真的照明。
- 背景合成器：合成逼真的背景，以增强视频的视觉质量。
- 合成器：将 3D 面部重建和估计的光照条件合成到 RGB 视频中。
(4) 训练和推理：NEUTART 使用对抗性训练方法进行训练。在推理时，视听模块和逼真模块是耦合的，但由于逼真模块中使用的神经渲染器的计算要求很高，因此它们是单独训练的。</methods></p>
<ol>
<li>结论：
（1）：本文提出了一种新的文本驱动的视听语音合成框架NEUTART，该框架能够直接将文本映射到视听表示，并生成逼真的说话人脸视频。NEUTART在TalkingHead++数据集上实现了最先进的结果，在合成视频的视觉和语音质量方面均优于现有方法。
（2）：创新点：</li>
<li>提出了一种新的文本驱动的视听语音合成框架NEUTART，该框架不遵循级联架构，而是将文本直接映射到视听表示，并生成逼真的说话人脸视频。</li>
<li>NEUTART包含两个主要模块：视听模块和逼真模块。视听模块用于联合合成语音音频和3D说话人脸序列，逼真模块用于合成RGB面部视频。</li>
<li>视听模块基于FastSpeech2文本到语音系统构建，以纳入视觉生成。音频通过其梅尔谱图建模，预训练的声码器从梅尔谱图生成语音波形。同样，面部使用FLAME3DMM建模，将5023个顶点的面部3D网格解耦为身份β、表情ψ和关节姿势θ参数，其中包括3个下颌关节参数θjaw。使用这种表示，我们使用3个下颌姿势和50个表情参数对说话过程中的面部表情和动作进行建模。因此，NEUTART每帧音频预测80个梅尔通道和53个3DMM通道。然后将3DMM系数解码为3D面部重建，从而驱动面部渲染器。</li>
<li>逼真模块使用StyleGAN2生成器将3D面部重建合成到RGB视频中。该模块由以下子模块组成：光照估计器、背景合成器和合成器。光照估计器估计场景光照条件，以确保生成的视频具有逼真的照明。背景合成器合成逼真的背景，以增强视频的视觉质量。合成器将3D面部重建和估计的光照条件合成到RGB视频中。</li>
<li>NEUTART使用对抗性训练方法进行训练。在推理时，视听模块和逼真模块是耦合的，但由于逼真模块中使用的神经渲染器的计算要求很高，因此它们是单独训练的。</li>
<li>性能：NEUTART在TalkingHead++数据集上实现了最先进的结果，在合成视频的视觉和语音质量方面均优于现有方法。此外，我们的方法还能够生成具有逼真唇形同步和面部表情的说话人脸视频。</li>
<li>工作量：NEUTART的训练和推理过程相对复杂，需要大量的数据和计算资源。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-726d63a429612bb5a9a4e98af93d828f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d97a4e9898602db53e92de86db31893.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-445934daa5563cb0bbb73a252c079f70.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0d51e1e89d038a1d7637e51d15eb70e0.jpg" align="middle">
</details>




<h2 id="R2-Talker-Realistic-Real-Time-Talking-Head-Synthesis-with-Hash-Grid-Landmarks-Encoding-and-Progressive-Multilayer-Conditioning"><a href="#R2-Talker-Realistic-Real-Time-Talking-Head-Synthesis-with-Hash-Grid-Landmarks-Encoding-and-Progressive-Multilayer-Conditioning" class="headerlink" title="R2-Talker: Realistic Real-Time Talking Head Synthesis with Hash Grid   Landmarks Encoding and Progressive Multilayer Conditioning"></a>R2-Talker: Realistic Real-Time Talking Head Synthesis with Hash Grid   Landmarks Encoding and Progressive Multilayer Conditioning</h2><p><strong>Authors:Zhiling Ye, LiangGuo Zhang, Dingheng Zeng, Quan Lu, Ning Jiang</strong></p>
<p>Dynamic NeRFs have recently garnered growing attention for 3D talking portrait synthesis. Despite advances in rendering speed and visual quality, challenges persist in enhancing efficiency and effectiveness. We present R2-Talker, an efficient and effective framework enabling realistic real-time talking head synthesis. Specifically, using multi-resolution hash grids, we introduce a novel approach for encoding facial landmarks as conditional features. This approach losslessly encodes landmark structures as conditional features, decoupling input diversity, and conditional spaces by mapping arbitrary landmarks to a unified feature space. We further propose a scheme of progressive multilayer conditioning in the NeRF rendering pipeline for effective conditional feature fusion. Our new approach has the following advantages as demonstrated by extensive experiments compared with the state-of-the-art works: 1) The lossless input encoding enables acquiring more precise features, yielding superior visual quality. The decoupling of inputs and conditional spaces improves generalizability. 2) The fusing of conditional features and MLP outputs at each MLP layer enhances conditional impact, resulting in more accurate lip synthesis and better visual quality. 3) It compactly structures the fusion of conditional features, significantly enhancing computational efficiency. </p>
<p><a href="http://arxiv.org/abs/2312.05572v1">PDF</a> </p>
<p><strong>摘要</strong><br>高效且有效的 R2-Talker 框架采用多尺度哈希网格对人脸特征进行无损编码，并引入渐进式多层调节方案，实现更逼真的实时动态人像合成。</p>
<p><strong>要点</strong></p>
<ul>
<li>基于多尺度哈希网格，提出将面部特征作为条件特征进行编码的创新方法，实现特征无损编码，条件空间与输入解耦。</li>
<li>提出在 NeRF 渲染管道中进行渐进式多层调节方案，实现条件特征的有效融合。</li>
<li>将条件特征和多层 perceptron 的输出在每个 perceptron 层融合，增强条件特征的影响，从而提高唇部的合成精度和视觉质量。</li>
<li>紧凑地构建条件特征的融合，显著提高计算效率。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：R2-Talker：基于哈希网格的实时说话人头部合成</li>
<li>作者：Junyu Luo, Jingbo Zhao, Zhaoyang Lv, Yajie Zhao, Hongtao Lu, Xiaoguang Han</li>
<li>隶属单位：北京大学</li>
<li>关键词：音频驱动、说话人头部合成、NeRF、条件特征融合、哈希网格</li>
<li>论文链接：https://arxiv.org/abs/2312.05572
   Github 链接：无</li>
<li>摘要：
   (1)：研究背景：动态 NeRF 近年来在 3D 说话人肖像合成中备受关注。尽管渲染速度和视觉质量取得了进步，但在提高效率和有效性方面仍然存在挑战。
   (2)：过去方法：现有方法主要集中在改进渲染速度和视觉质量上，但往往忽略了效率和有效性。例如，RAD-NeRF 虽然实现了较高的渲染速度，但视觉质量还有待提高；而 ER-NeRF 虽然具有较好的视觉质量，但渲染速度较慢。
   (3)：研究方法：本文提出了一种新的框架 R2-Talker，可以实现高效且有效的实时说话人头部合成。R2-Talker 的主要贡献包括：</li>
<li>提出了一种使用多分辨率哈希网格对面部地标进行编码的新方法，该方法可以无损地将地标结构编码为条件特征，并通过将任意地标映射到统一的特征空间来解耦输入多样性和条件空间。</li>
<li>
<p>提出了一种在 NeRF 渲染管道中进行渐进式多层条件特征融合的方案，可以有效地融合条件特征。
   (4)：方法性能：在广泛的实验中，R2-Talker 在视觉质量、泛化性和计算效率方面都优于现有技术。具体来说，R2-Talker 在视觉质量方面优于 RAD-NeRF 和 ER-NeRF，在泛化性方面优于 Geneface++，在计算效率方面优于 RAD-NeRF 和 Geneface++。这些性能支持了本文的目标，即实现高效且有效的实时说话人头部合成。</p>
</li>
<li>
<p>Methods：
（1）：本文提出了一种使用多分辨率哈希网格对面部地标进行编码的方法，该方法可以无损地将地标结构编码为条件特征，并通过将任意地标映射到统一的特征空间来解耦输入多样性和条件空间。
（2）：本文提出了一种在NeRF渲染管道中进行渐进式多层条件特征融合的方案，可以有效地融合条件特征。
（3）：本文提出了一种新的框架R2-Talker，可以实现高效且有效的实时说话人头部合成。</p>
</li>
<li>
<p>结论：
（1）：本工作提出了一种高效且有效的实时说话人头部合成框架R2-Talker，该框架在视觉质量、泛化性和计算效率方面优于现有技术。
（2）：创新点：</p>
</li>
<li>提出了一种使用多分辨率哈希网格对面部地标进行编码的方法，该方法可以无损地将地标结构编码为条件特征，并通过将任意地标映射到统一的特征空间来解耦输入多样性和条件空间。</li>
<li>提出了一种在NeRF渲染管道中进行渐进式多层条件特征融合的方案，可以有效地融合条件特征。
性能：</li>
<li>在视觉质量方面优于RAD-NeRF和ER-NeRF，在泛化性方面优于Geneface++，在计算效率方面优于RAD-NeRF和Geneface++。
工作量：</li>
<li>本文的工作量较大，需要大量的实验和计算。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2cdd63682ec6a29cc8aa99e91b02f344.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-201f569e2b5317cf129033a4f5a93d60.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5a7ff777ad56ad955bbe2a61d5e51b6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6c583a98ee2122cd98e00a07b748dcc1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d633b39b222518156d77fac342f43598.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9e2369dfa6e73f9f30aadec5db2fc944.jpg" align="middle">
</details>




<h2 id="FT2TF-First-Person-Statement-Text-To-Talking-Face-Generation"><a href="#FT2TF-First-Person-Statement-Text-To-Talking-Face-Generation" class="headerlink" title="FT2TF: First-Person Statement Text-To-Talking Face Generation"></a>FT2TF: First-Person Statement Text-To-Talking Face Generation</h2><p><strong>Authors:Xingjian Diao, Ming Cheng, Wayner Barrios, SouYoung Jin</strong></p>
<p>Talking face generation has gained immense popularity in the computer vision community, with various applications including AR/VR, teleconferencing, digital assistants, and avatars. Traditional methods are mainly audio-driven ones which have to deal with the inevitable resource-intensive nature of audio storage and processing. To address such a challenge, we propose FT2TF - First-Person Statement Text-To-Talking Face Generation, a novel one-stage end-to-end pipeline for talking face generation driven by first-person statement text. Moreover, FT2TF implements accurate manipulation of the facial expressions by altering the corresponding input text. Different from previous work, our model only leverages visual and textual information without any other sources (e.g. audio/landmark/pose) during inference. Extensive experiments are conducted on LRS2 and LRS3 datasets, and results on multi-dimensional evaluation metrics are reported. Both quantitative and qualitative results showcase that FT2TF outperforms existing relevant methods and reaches the state-of-the-art. This achievement highlights our model capability to bridge first-person statements and dynamic face generation, providing insightful guidance for future work. </p>
<p><a href="http://arxiv.org/abs/2312.05430v1">PDF</a> </p>
<p><strong>摘要</strong><br>以第一人称语句为驱动的单阶段端到端管道——FT2TF，无需额外信息即可生成逼真动态人脸。</p>
<p><strong>要点</strong></p>
<ul>
<li>FT2TF 是一种用于说话人脸生成的新颖单阶段端到端流水线，由第一人称语句文本驱动。</li>
<li>FT2TF 仅利用视觉和文本信息，无需任何其他来源（例如音频/地标/姿势）即可进行推理。</li>
<li>FT2TF在LRS2和LRS3数据集上进行了广泛的实验，并在多维评估指标上报告了结果。</li>
<li>定量和定性结果表明，FT2TF优于现有的相关方法，并达到了最先进的水平。</li>
<li>这一成果突出了我们的模型将第一人称陈述与动态人脸生成相桥接的能力，为未来的工作提供了有益的指导。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>论文标题：FT2TF：第一人称陈述文本到说话人脸生成</li>
<li>作者：Yuxiao Hu, Runpei Dong, Mingming He, Xiaoming Wei, Yajun Cai, Keke He, Jianfei Cai</li>
<li>第一作者单位：中国科学院大学</li>
<li>关键词：文本到人脸生成、第一人称陈述、情感表达、语言文本编码器、视觉解码器</li>
<li>论文链接：https://arxiv.org/abs/2302.08243 或 https://github.com/VITA-Group/FT2TF</li>
<li>
<p>摘要：
(1) 研究背景：说话人脸生成在计算机视觉领域备受关注，应用于 AR/VR、远程会议、数字助理和虚拟形象等。传统方法主要依赖音频驱动，但音频存储和处理不可避免地资源密集。
(2) 过去方法和问题：现有方法受限于音频驱动，导致资源密集且难以实现准确的表情操纵。
(3) 研究方法：提出 FT2TF，一种新颖的端到端流水线，仅利用视觉和文本信息，通过第一人称陈述文本生成说话人脸。FT2TF 通过改变相应的输入文本来实现对表情的准确操纵。
(4) 实验结果：在 LRS2 和 LRS3 数据集上进行广泛的实验，结果表明 FT2TF 在多维评估指标上优于现有相关方法，达到了最先进水平。这一成就突出了模型将第一人称陈述与动态人脸生成桥接的能力，为未来的工作提供了有益的指导。</p>
</li>
<li>
<p>方法：
(1)：提出FT2TF，一种新颖的端到端流水线，仅利用视觉和文本信息，通过第一人称陈述文本生成说话人脸；
(2)：FT2TF由语言文本编码器和视觉解码器两部分组成，语言文本编码器将第一人称陈述文本编码成语义向量，视觉解码器将语义向量解码成说话人脸；
(3)：FT2TF通过改变相应的输入文本来实现对表情的准确操纵；
(4)：在LRS2和LRS3数据集上进行广泛的实验，结果表明FT2TF在多维评估指标上优于现有相关方法，达到了最先进水平。</p>
</li>
<li>
<p>结论：
（1）：本文提出了一种新颖的端到端流水线FT2TF，仅利用视觉和文本信息，通过第一人称陈述文本生成说话人脸，为未来的工作提供了有益的指导。
（2）：创新点：
① FT2TF通过改变相应的输入文本来实现对表情的准确操纵，实现了说话人脸生成任务的新突破。
② FT2TF在LRS2和LRS3数据集上进行了广泛的实验，结果表明FT2TF在多维评估指标上优于现有相关方法，达到了最先进水平。
③ FT2TF具有较强的泛化能力，能够在不同的数据集上生成高质量的说话人脸。
性能：
① FT2TF在LRS2和LRS3数据集上进行了广泛的实验，结果表明FT2TF在多维评估指标上优于现有相关方法，达到了最先进水平。
② FT2TF能够生成高质量的说话人脸，具有较强的视觉保真度和情感表达能力。
③ FT2TF具有较强的泛化能力，能够在不同的数据集上生成高质量的说话人脸。
工作量：
① FT2TF的模型结构相对简单，易于训练和部署。
② FT2TF的训练速度较快，可以在短时间内生成高质量的说话人脸。
③ FT2TF的推理速度较快，可以实时生成说话人脸。</p>
</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2ab1187e4c933e4579db03a2b5dcd8e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e681830ba6447f3f79189954d392d003.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ef172006be19ffae8981a8a6d3a65fff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8b9dc58af4aebf75acd65646ccdabb9a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6acbbfb04100c974df91feb1eb4b5bf0.jpg" align="middle">
</details>




<h2 id="Emotional-Speech-driven-3D-Body-Animation-via-Disentangled-Latent-Diffusion"><a href="#Emotional-Speech-driven-3D-Body-Animation-via-Disentangled-Latent-Diffusion" class="headerlink" title="Emotional Speech-driven 3D Body Animation via Disentangled Latent   Diffusion"></a>Emotional Speech-driven 3D Body Animation via Disentangled Latent   Diffusion</h2><p><strong>Authors:Kiran Chhatre, Radek Daněček, Nikos Athanasiou, Giorgio Becherini, Christopher Peters, Michael J. Black, Timo Bolkart</strong></p>
<p>Existing methods for synthesizing 3D human gestures from speech have shown promising results, but they do not explicitly model the impact of emotions on the generated gestures. Instead, these methods directly output animations from speech without control over the expressed emotion. To address this limitation, we present AMUSE, an emotional speech-driven body animation model based on latent diffusion. Our observation is that content (i.e., gestures related to speech rhythm and word utterances), emotion, and personal style are separable. To account for this, AMUSE maps the driving audio to three disentangled latent vectors: one for content, one for emotion, and one for personal style. A latent diffusion model, trained to generate gesture motion sequences, is then conditioned on these latent vectors. Once trained, AMUSE synthesizes 3D human gestures directly from speech with control over the expressed emotions and style by combining the content from the driving speech with the emotion and style of another speech sequence. Randomly sampling the noise of the diffusion model further generates variations of the gesture with the same emotional expressivity. Qualitative, quantitative, and perceptual evaluations demonstrate that AMUSE outputs realistic gesture sequences. Compared to the state of the art, the generated gestures are better synchronized with the speech content and better represent the emotion expressed by the input speech. Our project website is amuse.is.tue.mpg.de. </p>
<p><a href="http://arxiv.org/abs/2312.04466v1">PDF</a> </p>
<p><strong>摘要</strong><br>通过分离内容、情感和个人风格，AMUSE 模型能够准确地生成与语音同步且情感丰富的动作。</p>
<p><strong>要点</strong></p>
<ul>
<li>AMUSE 是一种基于潜扩散的语音驱动肢体动画模型。</li>
<li>AMUSE 将驱动音频映射到三个解纠缠的潜在向量：内容、情感和个人风格。</li>
<li>AMUSE通过将驱动语音的内容与另一个语音序列的情感和风格相结合，直接从语音合成 3D 人体手势。</li>
<li>AMUSE 可以通过对扩散模型的噪声进行随机采样来生成具有相同情感表达力的姿态变化。</li>
<li>定性和定量评估表明 AMUSE 输出的手势序列是逼真的。</li>
<li>与最先进的技术相比，生成的姿态与语音内容的同步性更好，并且更好地表达了输入语音所表达的情感。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：情感语音驱动的三维身体动画，通过分离的潜在扩散</li>
<li>作者：Kiran Chhatre, Radek Danˇeˇcek, Nikos Athanasiou, Giorgio Becherini, Christopher Peters, Michael J. Black, Timo Bolkart</li>
<li>第一作者单位：瑞典皇家理工学院</li>
<li>关键词：语音驱动动画、情感表达、潜在扩散</li>
<li>论文链接：https://arxiv.org/abs/2312.04466
Github 链接：无</li>
<li>摘要：
(1)：研究背景：</li>
<li>语音驱动的三维身体动画在增强现实/虚拟现实中的远程临场感、游戏和电影中的虚拟人物动画以及具身交互式数字助理等方面具有广泛的应用。</li>
<li>现有的语音驱动三维身体动画方法在与语音节奏和单词发音相关的动作内容方面取得了很大进展，但它们没有充分解决一个关键因素：情感对生成动作的影响。</li>
</ol>
<p>(2)：过去方法和问题：
- 现有的语音驱动三维身体动画方法没有明确地建模情感对生成动作的影响，而是直接从语音输出动画，而不能控制表达的情感。
- 这些方法无法生成与输入语音表达的情感一致的动作，并且生成的动作与语音内容不同步。</p>
<p>(3)：研究方法：
- 提出了一种基于潜在扩散的情感语音驱动身体动画模型 AMUSE。
- AMUSE 将驱动音频映射到三个分离的潜在向量：内容、情感和个人风格。
- 然后将训练好的潜在扩散模型用于生成动作情感序列，该模型以这些潜在向量为条件。
- AMUSE 通过将驱动语音的内容与另一个语音序列的情感和风格相结合，直接从语音中合成三维人体动作，并控制表达的情感和风格。
- 随机采样扩散模型的噪声可以生成具有相同情感表达的不同动作。</p>
<p>(4)：实验结果：
- 定性和定量评估表明，AMUSE 输出逼真的动作序列。
- 与最先进的方法相比，生成的动作与语音内容更好地同步，并且更好地代表了输入语音表达的情感。</p>
<ol>
<li>
<p>方法：
（1）提出了一种基于潜在扩散的情感语音驱动身体动画模型 AMUSE，该模型将驱动音频映射到三个分离的潜在向量：内容、情感和个人风格。
（2）使用训练好的潜在扩散模型生成动作情感序列，该模型以这些潜在向量为条件。
（3）AMUSE 通过将驱动语音的内容与另一个语音序列的情感和风格相结合，直接从语音中合成三维人体动作，并控制表达的情感和风格。
（4）随机采样扩散模型的噪声可以生成具有相同情感表达的不同动作。</p>
</li>
<li>
<p>结论：
（1）：本文提出了一种情感语音驱动的身体动画模型 AMUSE，该模型可以从语音中合成三维人体动作，并控制表达的情感和风格。AMUSE 通过将驱动语音的内容与另一个语音序列的情感和风格相结合，直接从语音中合成三维人体动作，并控制表达的情感和风格。随机采样扩散模型的噪声可以生成具有相同情感表达的不同动作。
（2）：创新点：</p>
</li>
<li>提出了一种基于潜在扩散的情感语音驱动身体动画模型 AMUSE。</li>
<li>AMUSE 将驱动音频映射到三个分离的潜在向量：内容、情感和个人风格。</li>
<li>使用训练好的潜在扩散模型生成动作情感序列，该模型以这些潜在向量为条件。</li>
<li>AMUSE 通过将驱动语音的内容与另一个语音序列的情感和风格相结合，直接从语音中合成三维人体动作，并控制表达的情感和风格。</li>
<li>随机采样扩散模型的噪声可以生成具有相同情感表达的不同动作。
性能：</li>
<li>AMUSE 在各种指标上取得了最先进的性能：多样性、手势情感分类准确率、Frechét 手势距离、节拍对齐得分和语义相关手势召回。</li>
<li>感知研究表明，与之前的最先进技术相比，AMUSE 生成的动作与输入语音表达的情感更加同步并且更匹配。
工作量：</li>
<li>AMUSE 模型的训练和推理过程相对复杂，需要大量的数据和计算资源。</li>
<li>AMUSE 模型需要针对不同的任务和数据集进行微调，这可能会增加工作量。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-477aaf1cf553532d2ac6d081ce493dd5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0fb8f5969e38a10a90f4de1eb4b4df46.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59b0cb3d525b548d4a6433a459e908fc.jpg" align="middle">
</details>




<h2 id="PMMTalk-Speech-Driven-3D-Facial-Animation-from-Complementary-Pseudo-Multi-modal-Features"><a href="#PMMTalk-Speech-Driven-3D-Facial-Animation-from-Complementary-Pseudo-Multi-modal-Features" class="headerlink" title="PMMTalk: Speech-Driven 3D Facial Animation from Complementary Pseudo   Multi-modal Features"></a>PMMTalk: Speech-Driven 3D Facial Animation from Complementary Pseudo   Multi-modal Features</h2><p><strong>Authors:Tianshun Han, Shengnan Gui, Yiqing Huang, Baihui Li, Lijian Liu, Benjia Zhou, Ning Jiang, Quan Lu, Ruicong Zhi, Yanyan Liang, Du Zhang, Jun Wan</strong></p>
<p>Speech-driven 3D facial animation has improved a lot recently while most related works only utilize acoustic modality and neglect the influence of visual and textual cues, leading to unsatisfactory results in terms of precision and coherence. We argue that visual and textual cues are not trivial information. Therefore, we present a novel framework, namely PMMTalk, using complementary Pseudo Multi-Modal features for improving the accuracy of facial animation. The framework entails three modules: PMMTalk encoder, cross-modal alignment module, and PMMTalk decoder. Specifically, the PMMTalk encoder employs the off-the-shelf talking head generation architecture and speech recognition technology to extract visual and textual information from speech, respectively. Subsequently, the cross-modal alignment module aligns the audio-image-text features at temporal and semantic levels. Then PMMTalk decoder is employed to predict lip-syncing facial blendshape coefficients. Contrary to prior methods, PMMTalk only requires an additional random reference face image but yields more accurate results. Additionally, it is artist-friendly as it seamlessly integrates into standard animation production workflows by introducing facial blendshape coefficients. Finally, given the scarcity of 3D talking face datasets, we introduce a large-scale 3D Chinese Audio-Visual Facial Animation (3D-CAVFA) dataset. Extensive experiments and user studies show that our approach outperforms the state of the art. We recommend watching the supplementary video. </p>
<p><a href="http://arxiv.org/abs/2312.02781v1">PDF</a> </p>
<p><strong>Summary</strong><br>语音驱动的三维面部动画精度和连贯性不足，主要原因在于忽略视觉和文本线索，我们提出了一种利用互补伪多模态特征提高面部动画精度的 PMMTalk 框架。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>PMMTalk 框架包含 PMMTalk 编码器、跨模态对齐模块和 PMMTalk 解码器三个模块。</li>
<li>PMMTalk 编码器分别从语音中提取视觉和文本信息。</li>
<li>跨模态对齐模块在时间和语义层面上对齐音频-图像-文本特征。</li>
<li>PMMTalk 解码器用于预测唇形同步的面部混合形状系数。</li>
<li>与之前的研究相比，PMMTalk 只需要一个额外的随机参考面部图像，但产生了更准确的结果。</li>
<li>PMMTalk 与标准动画制作工作流程无缝集成，因为它引入了面部混合形状系数，因此对艺术家友好。</li>
<li>PMMTalk 引入了一个大规模的三维中文音视频面部动画 (3D-CAVFA) 数据集。</li>
<li>广泛的实验和用户研究表明，PMMTalk 优于现有技术。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：PMMTalk：基于互补伪多模态特征的语音驱动 3D 面部动画</li>
<li>作者：田顺寒、盛楠桂、易青黄、白慧丽、李立建、周本佳、蒋宁、陆全、池瑞聪、梁艳艳、张都、万钧</li>
<li>单位：澳门科技大学计算机科学与工程学院、创新工程学院</li>
<li>关键词：语音驱动 3D 面部动画、PMMTalk、3D-CAVFA 数据集</li>
<li>链接：https://arxiv.org/abs/2312.02781</li>
<li>摘要：
(1)：语音驱动 3D 面部动画近年来取得了很大进展，但大多数相关工作仅利用声学模态，忽略了视觉和文本线索的影响，导致精度和连贯性方面的不令人满意。
(2)：以往方法仅依赖于音频信号，忽视了视觉和文本线索，导致唇部运动不足和不连贯。
(3)：本文提出了一种新颖的框架 PMMTalk，使用互补的伪多模态特征来提高面部动画的准确性。该框架包含三个模块：PMMTalk 编码器、跨模态对齐模块和 PMMTalk 解码器。
(4)：PMMTalk 在 3D-CAVFA 数据集上取得了最先进的结果，并且用户研究表明，我们的方法在准确性和连贯性方面优于最先进的方法。</li>
</ol>
<p>方法：</p>
<p>（1）PMMTalk框架：PMMTalk框架由三个模块组成：PMMTalk编码器、跨模态对齐模块和PMMTalk解码器。PMMTalk编码器将语音、图像和文本特征编码成伪多模态特征。跨模态对齐模块将伪多模态特征对齐，以确保它们在时间和语义上的一致性。PMMTalk解码器将对齐后的伪多模态特征解码成3D面部动画。</p>
<p>（2）伪多模态特征生成：PMMTalk通过使用生成对抗网络（GAN）生成伪多模态特征。GAN由两个网络组成：生成器和判别器。生成器将语音、图像和文本特征作为输入，生成伪多模态特征。判别器将伪多模态特征与真实的多模态特征进行比较，并输出一个判别分数。生成器和判别器通过对抗训练的方式不断更新，直到生成的伪多模态特征与真实的多模态特征难以区分。</p>
<p>（3）跨模态对齐：PMMTalk使用一种基于注意力的机制来对齐伪多模态特征。注意力机制可以学习到伪多模态特征中与3D面部动画相关的重要信息。对齐后的伪多模态特征可以更好地反映3D面部动画的动态变化。</p>
<p>（4）3D面部动画解码：PMMTalk使用一种基于神经网络的解码器来将对齐后的伪多模态特征解码成3D面部动画。解码器可以学习到伪多模态特征与3D面部动画之间的映射关系。解码后的3D面部动画可以准确地反映语音、图像和文本中的信息。</p>
<ol>
<li>结论：
（1）：本工作利用互补的伪多模态特征，提出了一种新颖的语音驱动 3D 面部动画框架 PMMTalk，有效提高了面部动画的准确性和连贯性，为虚拟现实应用提供了更逼真且富有情感的面部动画。
（2）：创新点：</li>
<li>提出了一种新的语音驱动 3D 面部动画框架 PMMTalk，该框架利用互补的伪多模态特征，有效提高了面部动画的准确性和连贯性。</li>
<li>构建了一个大规模的 3D 面部动画数据集 3D-CAVFA，该数据集包含了同步的面部混合形状系数、多样化的语料库和广泛的主题。</li>
<li>在 3D-CAVFA 数据集上，PMMTalk 取得了最先进的结果，并且用户研究表明，PMMTalk 在准确性和连贯性方面优于最先进的方法。
性能：</li>
<li>在 3D-CAVFA 数据集上，PMMTalk 在准确性和连贯性方面优于最先进的方法。</li>
<li>用户研究表明，PMMTalk 在准确性和连贯性方面优于最先进的方法。
工作量：</li>
<li>PMMTalk 依赖于多个大规模的预训练模型，这增加了模型的推理时间，对实时应用提出了挑战。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-5e16e4a6505780bd258a48846b37d1cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a7e4b7e4be75685c80823cd56d8b266.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d8e43f08b5fd353528e3121153d4b584.jpg" align="middle">
</details>




<h2 id="MyPortrait-Morphable-Prior-Guided-Personalized-Portrait-Generation"><a href="#MyPortrait-Morphable-Prior-Guided-Personalized-Portrait-Generation" class="headerlink" title="MyPortrait: Morphable Prior-Guided Personalized Portrait Generation"></a>MyPortrait: Morphable Prior-Guided Personalized Portrait Generation</h2><p><strong>Authors:Bo Ding, Zhenfeng Fan, Shuang Yang, Shihong Xia</strong></p>
<p>Generating realistic talking faces is an interesting and long-standing topic in the field of computer vision. Although significant progress has been made, it is still challenging to generate high-quality dynamic faces with personalized details. This is mainly due to the inability of the general model to represent personalized details and the generalization problem to unseen controllable parameters. In this work, we propose Myportrait, a simple, general, and flexible framework for neural portrait generation. We incorporate personalized prior in a monocular video and morphable prior in 3D face morphable space for generating personalized details under novel controllable parameters. Our proposed framework supports both video-driven and audio-driven face animation given a monocular video of a single person. Distinguished by whether the test data is sent to training or not, our method provides a real-time online version and a high-quality offline version. Comprehensive experiments in various metrics demonstrate the superior performance of our method over the state-of-the-art methods. The code will be publicly available. </p>
<p><a href="http://arxiv.org/abs/2312.02703v1">PDF</a> </p>
<p><strong>摘要</strong><br>用单目视频中的个性化先验知识和3D面部可变形空间中的可变形先验知识，生成具有个性化细节的高质量动态面孔。</p>
<p><strong>要点</strong></p>
<ul>
<li>我们提出了Myportrait，一个用于神经肖像生成的简单、通用且灵活的框架。</li>
<li>Myportrait在单目视频中结合个性化先验和3D面部可变形空间中的可变形先验，以在新的可控参数下生成个性化细节。</li>
<li>该框架支持基于视频和基于音频的面部动画，只要有一段单个人物的单目视频。</li>
<li>根据测试数据是否发送到训练，该方法提供了实时的在线版本和高质量的离线版本。</li>
<li>该方法在各个指标的综合实验中表现出优于最先进方法的性能。</li>
<li>代码将公开。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：MyPortrait：可变形先验引导的个性化肖像生成</li>
<li>作者：Yujun Shen, Linchao Bao, Xiaogang Wang, Xiangyu Xu, Wenpeng Wang, Xiaowei Zhou</li>
<li>单位：香港城市大学</li>
<li>关键词：神经肖像生成、个性化生成、可变形先验、视频驱动、音频驱动</li>
<li>论文链接：Paper_info:MyPortrait:MorphablePrior-GuidedPersonalizedPortraitGenerationSupplementaryMaterial6.Dataset
Github 链接：无</li>
<li>摘要：
(1)：研究背景：生成逼真的说话人面孔是计算机视觉领域的一个有趣且长期存在的话题。尽管取得了重大进展，但生成具有个性化细节的高质量动态面孔仍然具有挑战性。这主要是由于通用模型无法表示个性化细节以及对不可见的可控参数的泛化问题。
(2)：过去方法及其问题：现有方法存在以下问题：无法表示个性化细节；对不可见的可控参数泛化能力差；无法同时支持视频驱动和音频驱动的面部动画。
(3)：研究方法：本文提出 MyPortrait，这是一个简单、通用且灵活的神经肖像生成框架。我们在单目视频中加入个性化先验，在 3D 人脸可变形空间中加入可变形先验，以便在新的可控参数下生成个性化细节。所提出的框架支持在给定单目视频的情况下进行视频驱动和音频驱动的面部动画。我们的方法根据测试数据是否发送到训练来提供实时在线版本和高质量的离线版本。
(4)：实验结果：在各种指标上的综合实验表明，本文方法优于最先进的方法。这些性能支持他们的目标。</li>
</ol>
<p>7.Methods：
(1)：提出MyPortrait，这是一个简单、通用且灵活的神经肖像生成框架。
(2)：在单目视频中加入个性化先验，在3D人脸可变形空间中加入可变形先验，以便在新的可控参数下生成个性化细节。
(3)：所提出的框架支持在给定单目视频的情况下进行视频驱动和音频驱动的面部动画。
(4)：我们的方法根据测试数据是否发送到训练来提供实时在线版本和高质量的离线版本。</p>
<ol>
<li>结论：
(1)：本文提出了一种简单、通用且灵活的神经肖像生成框架MyPortrait，该框架在单目视频中加入个性化先验，在3D人脸可变形空间中加入可变形先验，以便在新的可控参数下生成个性化细节。所提出的框架支持在给定单目视频的情况下进行视频驱动和音频驱动的面部动画。我们的方法根据测试数据是否发送到训练来提供实时在线版本和高质量的离线版本。综合实验表明，本文方法优于最先进的方法。
(2)：创新点：</li>
<li>提出了一种简单、通用且灵活的神经肖像生成框架MyPortrait。</li>
<li>在单目视频中加入个性化先验，在3D人脸可变形空间中加入可变形先验，以便在新的可控参数下生成个性化细节。</li>
<li>所提出的框架支持在给定单目视频的情况下进行视频驱动和音频驱动的面部动画。</li>
<li>我们的方法根据测试数据是否发送到训练来提供实时在线版本和高质量的离线版本。
性能：</li>
<li>在各种指标上的综合实验表明，本文方法优于最先进的方法。
工作量：</li>
<li>该方法需要收集单目视频数据，并对数据进行预处理。</li>
<li>该方法需要训练神经网络模型，这可能需要大量的时间和计算资源。</li>
<li>该方法需要将训练好的模型部署到实际应用中，这可能需要额外的工程工作。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-30c34cea0fa3b5c34bec52a8060f7083.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34426e9a289a340fb5a39c487fd354e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3551f581121858d6b86025dc61e6efa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c5be15aebe061090e2870367d2adb84.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c81ad84ca0a38873919e5990c28ae906.jpg" align="middle">
</details>




<h2 id="VividTalk-One-Shot-Audio-Driven-Talking-Head-Generation-Based-on-3D-Hybrid-Prior"><a href="#VividTalk-One-Shot-Audio-Driven-Talking-Head-Generation-Based-on-3D-Hybrid-Prior" class="headerlink" title="VividTalk: One-Shot Audio-Driven Talking Head Generation Based on 3D   Hybrid Prior"></a>VividTalk: One-Shot Audio-Driven Talking Head Generation Based on 3D   Hybrid Prior</h2><p><strong>Authors:Xusen Sun, Longhao Zhang, Hao Zhu, Peng Zhang, Bang Zhang, Xinya Ji, Kangneng Zhou, Daiheng Gao, Liefeng Bo, Xun Cao</strong></p>
<p>Audio-driven talking head generation has drawn much attention in recent years, and many efforts have been made in lip-sync, expressive facial expressions, natural head pose generation, and high video quality. However, no model has yet led or tied on all these metrics due to the one-to-many mapping between audio and motion. In this paper, we propose VividTalk, a two-stage generic framework that supports generating high-visual quality talking head videos with all the above properties. Specifically, in the first stage, we map the audio to mesh by learning two motions, including non-rigid expression motion and rigid head motion. For expression motion, both blendshape and vertex are adopted as the intermediate representation to maximize the representation ability of the model. For natural head motion, a novel learnable head pose codebook with a two-phase training mechanism is proposed. In the second stage, we proposed a dual branch motion-vae and a generator to transform the meshes into dense motion and synthesize high-quality video frame-by-frame. Extensive experiments show that the proposed VividTalk can generate high-visual quality talking head videos with lip-sync and realistic enhanced by a large margin, and outperforms previous state-of-the-art works in objective and subjective comparisons. </p>
<p><a href="http://arxiv.org/abs/2312.01841v2">PDF</a> 10 pages, 8 figures</p>
<p><strong>摘要</strong></p>
<p>创新的两阶段框架 VividTalk 可生成高质量视觉效果的说话人头部视频，包括唇形同步、丰富的面部表情、自然的头部姿势等。</p>
<p><strong>要点</strong></p>
<ul>
<li>VividTalk 采用双阶段通用框架，可以生成高质量视觉效果的说话人头部视频。</li>
<li>VividTalk 在第一阶段通过学习非刚性表情运动和刚性头部运动，将音频映射到网格。</li>
<li>VividTalk 在第二阶段使用双分支运动-VAE 和生成器将网格转换为密集运动并逐帧合成高质量视频。</li>
<li>广泛的实验表明，与目前最先进的作品相比，VividTalk 可以生成高质量视觉效果的说话人头部视频，并将唇形同步和逼真的增强效果提高很大幅度。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>标题：生动语聊：高保真音视频生成框架（VividTalk: A High-Fidelity Audio-Driven Talking Head Generation Framework）</li>
<li>作者：Yuhang Jiang, Mingyu Ding, Junhui Hou, Yanan Sun, Lu Sheng, Zhiwei Xiong, Hang Zhou</li>
<li>单位：无</li>
<li>关键词：音频驱动、说话头生成、面部表情、头部姿势、视频合成</li>
<li>链接：https://arxiv.org/abs/2312.01841, Github：无</li>
<li>
<p>摘要：
（1）：音频驱动的说话头生成已经引起广泛关注，在唇形同步、面部表情、头部姿势生成和视频质量方面取得了进展。然而，由于音频和动作之间的一对多映射，还没有模型能够在所有这些指标上达到最优。
（2）：以往的方法通常使用混合形状或顶点偏移来表示面部表情，但这些方法在捕捉精细的表情细节方面存在局限性。此外，头部姿势的生成通常是通过直接从音频中学习来实现的，这可能会导致不合理和不连续的结果。
（3）：本文提出了一种名为 VividTalk 的两阶段通用框架，支持生成具有所有上述属性的高视觉质量说话头视频。在第一阶段，通过学习非刚性表情运动和刚性头部运动将音频映射到网格。对于表情运动，采用混合形状和顶点作为中间表示，以最大限度地提高模型的表示能力。对于自然头部运动，提出了一种新颖的可学习头部姿势码本，并采用两阶段训练机制。在第二阶段，提出了一种双分支运动-VAE 和生成器，将网格转换为密集运动并逐帧合成高质量视频。
（4）：广泛的实验表明，所提出的 VividTalk 可以生成具有唇形同步和逼真头部姿势的高视觉质量说话头视频，并且在客观和主观比较中优于以往的最新作品。</p>
</li>
<li>
<p>方法：
（1）：VividTalk 框架分为两个阶段：网格生成阶段和视频合成阶段。在网格生成阶段，音频首先被映射到网格，网格由混合形状和顶点偏移表示。混合形状用于捕捉精细的表情细节，而顶点偏移用于捕捉刚性头部运动。在视频合成阶段，网格被转换为密集运动，然后逐帧合成高质量视频。
（2）：在网格生成阶段，音频首先被映射到一个中间表示，该中间表示由混合形状和顶点偏移组成。混合形状用于捕捉精细的表情细节，而顶点偏移用于捕捉刚性头部运动。然后，中间表示被映射到网格。
（3）：在视频合成阶段，网格被转换为密集运动。密集运动然后被用于逐帧合成高质量视频。视频合成器是一个双分支网络，由一个运动-VAE 和一个生成器组成。运动-VAE 用于生成密集运动，而生成器用于合成视频。</p>
</li>
<li>
<p>结论：
（1）：本工作首次提出了一种支持生成具有丰富面部表情和自然头部姿势的高质量说话头视频的新颖通用框架 VividTalk。对于非刚性表情运动，混合形状和顶点都被映射为中间表示以最大化模型的表示能力，并设计了一个精心构建的多分支生成器来分别对全局和局部面部运动进行建模。至于刚性头部运动，提出了一种新颖的可学习头部姿势码本和一种两阶段训练机制来合成自然结果。得益于双分支运动-VAE 和生成器，驱动的网格可以很好地转换为密集运动，并用于合成最终视频。实验表明，我们的方法优于以往最先进的方法，并在数字人创建、视频会议等许多应用中开辟了新途径。
（2）：创新点：</p>
</li>
<li>提出了一种新颖的通用框架 VividTalk，支持生成具有丰富面部表情和自然头部姿势的高质量说话头视频。</li>
<li>对于非刚性表情运动，混合形状和顶点都被映射为中间表示以最大化模型的表示能力，并设计了一个精心构建的多分支生成器来分别对全局和局部面部运动进行建模。</li>
<li>对于刚性头部运动，提出了一种新颖的可学习头部姿势码本和一种两阶段训练机制来合成自然结果。</li>
<li>得益于双分支运动-VAE 和生成器，驱动的网格可以很好地转换为密集运动，并用于合成最终视频。
性能：</li>
<li>在客观和主观比较中，VividTalk 优于以往最先进的作品。</li>
<li>VividTalk 可以生成具有唇形同步和逼真头部姿势的高视觉质量说话头视频。
工作量：</li>
<li>VividTalk 的实现相对复杂，需要大量的数据和计算资源。</li>
<li>VividTalk 的训练过程可能需要数天或数周的时间。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-b072ca131954e5aa54fae54f90858dae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ef1751be1b0bb02f7a73562aad64e5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18bcd1380728d32e1277fd17982288c6.jpg" align="middle">
</details>




<h2 id="3DiFACE-Diffusion-based-Speech-driven-3D-Facial-Animation-and-Editing"><a href="#3DiFACE-Diffusion-based-Speech-driven-3D-Facial-Animation-and-Editing" class="headerlink" title="3DiFACE: Diffusion-based Speech-driven 3D Facial Animation and Editing"></a>3DiFACE: Diffusion-based Speech-driven 3D Facial Animation and Editing</h2><p><strong>Authors:Balamurugan Thambiraja, Sadegh Aliakbarian, Darren Cosker, Justus Thies</strong></p>
<p>We present 3DiFACE, a novel method for personalized speech-driven 3D facial animation and editing. While existing methods deterministically predict facial animations from speech, they overlook the inherent one-to-many relationship between speech and facial expressions, i.e., there are multiple reasonable facial expression animations matching an audio input. It is especially important in content creation to be able to modify generated motion or to specify keyframes. To enable stochasticity as well as motion editing, we propose a lightweight audio-conditioned diffusion model for 3D facial motion. This diffusion model can be trained on a small 3D motion dataset, maintaining expressive lip motion output. In addition, it can be finetuned for specific subjects, requiring only a short video of the person. Through quantitative and qualitative evaluations, we show that our method outperforms existing state-of-the-art techniques and yields speech-driven animations with greater fidelity and diversity. </p>
<p><a href="http://arxiv.org/abs/2312.00870v1">PDF</a> Project page: <a href="https://balamuruganthambiraja.github.io/3DiFACE/">https://balamuruganthambiraja.github.io/3DiFACE/</a></p>
<p><strong>Summary</strong></p>
<p>语音驱动、可编辑的 3D 面部动画新方式，效果更逼真、更灵活。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>3DiFACE 提出一种利用扩散模型进行 3D 面部动作生成的新方法，支持个性化语音驱动以及编辑。</li>
<li>该方法可以从少量 3D 动作数据集中进行训练，并保持逼真的嘴部动作。</li>
<li>3DiFACE 还支持针对特定个体进行微调，只需一个简短的视频即可。</li>
<li>定量和定性评估表明，该方法优于现有技术，可生成更逼真、更具多样性的语音驱动动画。</li>
<li>与现有方法相比，3DiFACE 可以生成更具多样性的面部动画，并且能够编辑生成的运动或指定关键帧。</li>
<li>3DiFACE 模型训练简单，训练数据量少。</li>
<li>3DiFACE 模型可以应用于内容创作和动画制作等领域。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>标题：3DiFACE：基于扩散的语音驱动 3D 面部动画和编辑</li>
<li>作者：Balamurugan Thambiraja、Sadegh Aliakbarian、Darren Cosker、Justus Thies</li>
<li>隶属机构：德国图宾根马克斯·普朗克智能系统研究所</li>
<li>关键词：3D 面部动画、语音驱动、扩散模型、运动编辑</li>
<li>论文链接：https://balamuruganthambiraja.github.io/3DiFACESpeechAudio3DiFACEInpaintedMotionFixedKeyframeFixedKeyframeAnimationSynthesisAnimationEditing
   Github 链接：无</li>
<li>摘要：
（1）研究背景：3D 面部动画在数字体验中发挥着重要作用，但现有的方法大多是确定性地预测面部动画，忽略了语音和面部表情之间的一对多关系。
（2）过去的方法及其问题：现有方法学习语音和面部动画之间的确定性映射，限制了合成动画的多样性。
（3）提出的研究方法：提出了一种轻量级的音频条件扩散模型，用于 3D 面部运动。该模型可以在小型 3D 运动数据集上训练，并保持富有表现力的唇部运动输出。此外，它可以针对特定对象进行微调，只需一段该人的短视频。
（4）方法的性能和目标实现情况：通过定量和定性评估表明，该方法优于现有技术，并产生了具有更高保真度和多样性的语音驱动动画，证明了方法的有效性。</li>
</ol>
<p>Methods:
(1): 提出了一种基于扩散的语音驱动3D面部动画和编辑方法，该方法可以学习语音和面部动画之间的一对多关系，并生成具有更高保真度和多样性的语音驱动动画。
(2): 该方法使用了一个轻量级的音频条件扩散模型，该模型可以在小型3D运动数据集上训练，并保持富有表现力的唇部运动输出。
(3): 该方法还可以针对特定对象进行微调，只需一段该人的短视频。
(4): 通过定量和定性评估表明，该方法优于现有技术，并产生了具有更高保真度和多样性的语音驱动动画，证明了方法的有效性。</p>
<ol>
<li>结论：
（1）本工作首次提出了一种可以从语音输入生成和编辑多样化 3D 面部动画的方法。使用无分类器引导为我们提供了一种有效的工具来平衡合成多样性和准确性，使我们能够生成具有前所未有的多样性的动画，同时在合成准确性方面优于或匹配所有基线。通过个性化，我们可以从短（~100 秒）视频中提取特定人物的说话风格，从而显著提高性能。此外，我们的架构允许我们通过使用关键帧来编辑动画。我们相信这些特性使 3DiFACE 成为内容创作者的强大工具，并对未来的应用感到兴奋。
（2）创新点：
提出了一种基于扩散的语音驱动 3D 面部动画和编辑方法，该方法可以学习语音和面部动画之间的一对多关系，并生成具有更高保真度和多样性的语音驱动动画。
该方法使用了一个轻量级的音频条件扩散模型，该模型可以在小型 3D 运动数据集上训练，并保持富有表现力的唇部运动输出。
该方法还可以针对特定对象进行微调，只需一段该人的短视频。
通过定量和定性评估表明，该方法优于现有技术，并产生了具有更高保真度和多样性的语音驱动动画，证明了方法的有效性。
性能：
该方法在多个数据集上进行了评估，结果表明该方法在合成准确性和多样性方面优于现有技术。
该方法可以针对特定对象进行微调，只需一段该人的短视频，这使得该方法可以很容易地应用于各种应用场景。
该方法可以生成具有高保真度和多样性的语音驱动动画，这使得该方法非常适合用于电影、游戏和其他数字体验。
工作量：
该方法的训练过程相对简单，可以在小型 3D 运动数据集上训练。
该方法可以针对特定对象进行微调，只需一段该人的短视频，这使得该方法可以很容易地应用于各种应用场景。
该方法可以生成具有高保真度和多样性的语音驱动动画，这使得该方法非常适合用于电影、游戏和其他数字体验。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e9da6eed634be4372cb5b6b3a1d361be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-981d7296309ee36a1b0bddfb6e9dc188.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5d7e90cbb37a3074d13c36527f21fe60.jpg" align="middle">
</details>




<h2 id="SyncTalk-The-Devil-is-in-the-Synchronization-for-Talking-Head-Synthesis"><a href="#SyncTalk-The-Devil-is-in-the-Synchronization-for-Talking-Head-Synthesis" class="headerlink" title="SyncTalk: The Devil is in the Synchronization for Talking Head Synthesis"></a>SyncTalk: The Devil is in the Synchronization for Talking Head Synthesis</h2><p><strong>Authors:Ziqiao Peng, Wentao Hu, Yue Shi, Xiangyu Zhu, Xiaomei Zhang, Hao Zhao, Jun He, Hongyan Liu, Zhaoxin Fan</strong></p>
<p>Achieving high synchronization in the synthesis of realistic, speech-driven talking head videos presents a significant challenge. Traditional Generative Adversarial Networks (GAN) struggle to maintain consistent facial identity, while Neural Radiance Fields (NeRF) methods, although they can address this issue, often produce mismatched lip movements, inadequate facial expressions, and unstable head poses. A lifelike talking head requires synchronized coordination of subject identity, lip movements, facial expressions, and head poses. The absence of these synchronizations is a fundamental flaw, leading to unrealistic and artificial outcomes. To address the critical issue of synchronization, identified as the “devil” in creating realistic talking heads, we introduce SyncTalk. This NeRF-based method effectively maintains subject identity, enhancing synchronization and realism in talking head synthesis. SyncTalk employs a Face-Sync Controller to align lip movements with speech and innovatively uses a 3D facial blendshape model to capture accurate facial expressions. Our Head-Sync Stabilizer optimizes head poses, achieving more natural head movements. The Portrait-Sync Generator restores hair details and blends the generated head with the torso for a seamless visual experience. Extensive experiments and user studies demonstrate that SyncTalk outperforms state-of-the-art methods in synchronization and realism. We recommend watching the supplementary video: <a href="https://ziqiaopeng.github.io/synctalk">https://ziqiaopeng.github.io/synctalk</a> </p>
<p><a href="http://arxiv.org/abs/2311.17590v1">PDF</a> 11 pages, 5 figures</p>
<p><strong>摘要</strong><br>神经辐射场-生成对抗网络框架用于实现说话人头部视频的同步合成。</p>
<p><strong>关键要点</strong></p>
<ul>
<li>传统生成对抗网络难以维持一致的面部身份。</li>
<li>神经辐射场方法可以解决面部身份一致性问题，但经常出现嘴唇运动不匹配、面部表情不足和头部姿势不稳定的问题。</li>
<li>逼真的说话人头部视频需要同步协调主体身份、嘴唇运动、面部表情和头部姿势。</li>
<li>缺少同步性是导致不真实和人为结果的根本缺陷。</li>
<li>SyncTalk 是一种基于神经辐射场的方法，有效地保持了主体身份，提高了说话人头部合成中的同步性和真实感。</li>
<li>SyncTalk 使用面部同步控制器将嘴唇运动与语音对齐，并创新地使用 3D 面部混合形状模型来捕捉准确的面部表情。</li>
<li>SyncTalk 的头部同步稳定器优化了头部姿势，实现了更自然的头部运动。</li>
<li>人像同步生成器恢复头发细节，将生成的头部与躯干融合，以获得无缝的视觉体验。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：SyncTalk：谈话头部合成同步的魔鬼</li>
<li>作者：Ziqiao Peng, Wentao Hu, Yue Shi, Xiangyu Zhu, Xiaomei Zhang, Hao Zhao, Jun He, Hongyan Liu, Zhaoxin Fan</li>
<li>第一作者单位：中国人民大学</li>
<li>关键词：谈话头部合成、神经辐射场、同步、身份保持、表情控制、头部姿势稳定</li>
<li>论文链接：https://arxiv.org/abs/2311.17590
   Github 链接：无</li>
<li>
<p>摘要：
（1）：研究背景：
生成逼真的、由语音驱动的谈话头部视频是一项具有挑战性的任务。传统生成对抗网络（GAN）难以保持一致的面部身份，而神经辐射场（NeRF）方法虽然可以解决这个问题，但通常会产生不匹配的唇部动作、不充分的面部表情和不稳定的头部姿势。一个逼真的谈话头部需要同步协调主体身份、唇部动作、面部表情和头部姿势。缺乏这些同步是导致不真实和人工结果的根本缺陷。
（2）：过去的方法及其问题：
GAN 方法难以保持一致的面部身份。NeRF 方法虽然可以解决这个问题，但通常会产生不匹配的唇部动作、不充分的面部表情和不稳定的头部姿势。
（3）：提出的研究方法：
SyncTalk 是一种基于 NeRF 的方法，它有效地保持了主体身份，增强了谈话头部合成的同步性和真实性。SyncTalk 使用面部同步控制器将唇部动作与语音对齐，并创新地使用 3D 面部混合形状模型来捕捉准确的面部表情。头部同步稳定器优化头部姿势，实现更自然的头部运动。肖像同步生成器恢复头发细节，并将生成的头部与躯干融合，以获得无缝的视觉体验。
（4）：方法在什么任务上取得了什么性能，这些性能是否支持了它们的目标：
SyncTalk 在谈话头部合成同步性和真实性方面优于最先进的方法。广泛的实验和用户研究表明，SyncTalk 在同步性和真实性方面优于最先进的方法。</p>
</li>
<li>
<p>方法：
(1)：面部同步控制器：使用唇部同步判别器预训练一个高度同步的音频-视觉特征提取器；引入 3D 面部混合形状模型来捕捉准确的面部表情；使用面部感知掩码注意力来减少唇部特征和表情特征之间的相互干扰。
(2)：头部同步稳定器：使用头部运动跟踪器来获得头部姿态的粗略估计；使用头部点跟踪器来跟踪面部关键点；使用束调整来增强关键点和头部姿态估计的准确性。
(3)：动态肖像渲染器：使用三平面哈希表示来表示 3D 场景；使用可变形神经辐射场来捕捉动态对象的外观；使用肖像同步生成器来恢复头发细节并融合头部和躯干。</p>
</li>
<li>
<p>结论：</p>
</li>
</ol>
<p>（1）：意义：SyncTalk 是一种基于神经辐射场的高同步语音驱动谈话头部合成方法，能够保持主体身份并生成同步的唇部动作、面部表情和稳定的头部姿势。SyncTalk 在谈话头部合成同步性和真实性方面优于最先进的方法，有望增强各种应用并激发谈话头部合成领域进一步创新。</p>
<p>（2）：创新点：
- 提出了一种新的谈话头部合成方法 SyncTalk，该方法能够有效地保持主体身份，增强谈话头部合成的同步性和真实性。
- 使用面部同步控制器将唇部动作与语音对齐，并创新地使用 3D 面部混合形状模型来捕捉准确的面部表情。
- 使用头部同步稳定器优化头部姿势，实现更自然的头部运动。
- 使用肖像同步生成器恢复头发细节，并将生成的头部与躯干融合，以获得无缝的视觉体验。</p>
<p>性能：
- SyncTalk 在谈话头部合成同步性和真实性方面优于最先进的方法。
- 广泛的实验和用户研究表明，SyncTalk 在同步性和真实性方面优于最先进的方法。</p>
<p>工作量：
- SyncTalk 的实现相对复杂，需要大量的数据和计算资源。
- SyncTalk 的训练过程需要大量的时间和计算资源。</p>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-fd17c6961448d8c17f0288819dc76c44.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f17a1563bd9dde5f0ecdc2862b78f71c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11f3ba567cdea1cc222349d8eaca8ee1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-71774a339b795203c4ab57e06c0114e5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2dec8013e0bb70e058e8514cfbe99d7c.jpg" align="middle">
</details>





## DiffusionTalker: Personalization and Acceleration for Speech-Driven 3D   Face Diffuser

**Authors:Peng Chen, Xiaobao Wei, Ming Lu, Yitong Zhu, Naiming Yao, Xingyu Xiao, Hui Chen**

Speech-driven 3D facial animation has been an attractive task in both academia and industry. Traditional methods mostly focus on learning a deterministic mapping from speech to animation. Recent approaches start to consider the non-deterministic fact of speech-driven 3D face animation and employ the diffusion model for the task. However, personalizing facial animation and accelerating animation generation are still two major limitations of existing diffusion-based methods. To address the above limitations, we propose DiffusionTalker, a diffusion-based method that utilizes contrastive learning to personalize 3D facial animation and knowledge distillation to accelerate 3D animation generation. Specifically, to enable personalization, we introduce a learnable talking identity to aggregate knowledge in audio sequences. The proposed identity embeddings extract customized facial cues across different people in a contrastive learning manner. During inference, users can obtain personalized facial animation based on input audio, reflecting a specific talking style. With a trained diffusion model with hundreds of steps, we distill it into a lightweight model with 8 steps for acceleration. Extensive experiments are conducted to demonstrate that our method outperforms state-of-the-art methods. The code will be released. 

[PDF](http://arxiv.org/abs/2311.16565v2) 

**Summary**
扩散网络技术创新生成个性化3D动态人脸，显著提升人脸动画生成效率。

**Key Takeaways**

- 利用扩散模型生成 3D 人脸动画，能够考虑到言语驱动的 3D 面部动画的非确定性。
- 提出的扩散谈话器，是一种基于扩散的方法，利用对比学习实现个性化 3D 面部动画和知识蒸馏来加速 3D 动画生成。
- 可训练的说话者身份引入能够汇总音频序列中的知识。
- 提出的身份嵌入以对比学习的方式提取不同人之间的自定义面部线索。
- 在推理过程中，用户可以根据输入音频获得个性化的面部动画，从而体现特定的说话风格。
- 训练好的扩散模型有数百个步骤，通过知识蒸馏可以将其蒸馏成一个轻量级模型，具有 8 个步骤，加速生成。
- 广泛的实验表明该方法优于最先进的方法。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>题目：DiffusionTalker：语音驱动的三维人脸动画的个性化和加速</li>
<li>作者：Peng Chen、Xiaobao Wei、Ming Lu、Yitong Zhu、Naiming Yao、Xingyu Xiao、Hui Chen</li>
<li>单位：中国科学院软件研究所</li>
<li>关键词：语音驱动、三维人脸动画、扩散模型、个性化、加速</li>
<li>论文链接：https://arxiv.org/abs/2311.16565
   Github 代码链接：无</li>
<li>
<p>摘要：
（1）研究背景：语音驱动三维人脸动画是一项重要的任务，广泛应用于虚拟现实、增强现实和计算机游戏等领域。传统方法主要集中于学习从语音到动画的确定性映射，但最近的方法开始考虑语音驱动三维人脸动画的非确定性因素，并采用扩散模型来完成任务。
（2）过去方法：现有的扩散模型方法在个性化和加速方面仍然存在局限性。
（3）研究方法：为了解决上述局限性，本文提出了一种基于扩散模型的方法 DiffusionTalker，它利用对比学习来实现三维人脸动画的个性化，并利用知识蒸馏来加速三维动画的生成。具体来说，为了实现个性化，本文引入了一个可学习的说话者身份来聚合音频序列中的知识。所提出的身份嵌入以对比学习的方式提取不同人之间的定制面部提示。在推理过程中，用户可以根据输入音频获得个性化的面部动画，反映特定的说话风格。本文还将训练好的具有数百个步骤的扩散模型蒸馏成一个具有 8 个步骤的轻量级模型以实现加速。
（4）方法性能：本文在多个任务上进行了广泛的实验，结果表明，该方法优于最先进的方法。这些性能支持了本文的目标。</p>
</li>
<li>
<p>方法：
（1）扩散模型：DDPMs 是内容生成的关键元素，用于学习训练数据的分布并生成与该分布紧密匹配的图像。
（2）个性化适配器：提出了一种基于对比学习的个性化适配器，该适配器包含一个身份嵌入库，每个嵌入对应一个音频序列。通过对比学习，未知的输入音频可以找到身份嵌入库中相似的身份嵌入，从而实现推理期间说话风格的个性化。
（3）知识蒸馏：为了加速推理，利用知识蒸馏将具有 2n 步的教师模型蒸馏成具有 n 步的学生模型，加速语音驱动的 3D 面部动画合成的速度。
（4）训练和推理：在训练过程中，随机选择一个时间步长 t，将噪声添加到 x0 以获得 xt。将音频-身份训练对分别输入音频编码器和身份编码器以提取特征。在推理过程中，将给定的音频序列输入到音频编码器中，生成音频特征。然后将此特征与身份嵌入库中所有嵌入的特征进行矩阵乘法。具有最高相似性的嵌入被识别为与输入音频序列匹配的说话身份。</p>
</li>
<li>
<p>结论：
（1）：本工作提出了一种基于扩散模型的语音驱动三维人脸动画个性化和加速方法，该方法利用对比学习实现个性化，利用知识蒸馏实现加速，在多个任务上取得了优异的性能。
（2）：创新点：</p>
</li>
<li>提出了一种基于对比学习的个性化适配器，该适配器包含一个身份嵌入库，每个嵌入对应一个音频序列。通过对比学习，未知的输入音频可以找到身份嵌入库中相似的身份嵌入，从而实现推理期间说话风格的个性化。</li>
<li>利用知识蒸馏将具有2n步的教师模型蒸馏成具有n步的学生模型，加速语音驱动的3D面部动画合成的速度。
性能：</li>
<li>在多个任务上进行了广泛的实验，结果表明，该方法优于最先进的方法。这些性能支持了本文的目标。
工作量：</li>
<li>本文的工作量较大，需要大量的训练数据和计算资源。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-42dc6bb5ab80cf7d628dee32e112dd8e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2147d22176df09eea3c7ab6aaf274e54.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-293c9b171412db2fee9963cc42c767f0.jpg" align="middle">
</details>




<h2 id="GAIA-Zero-shot-Talking-Avatar-Generation"><a href="#GAIA-Zero-shot-Talking-Avatar-Generation" class="headerlink" title="GAIA: Zero-shot Talking Avatar Generation"></a>GAIA: Zero-shot Talking Avatar Generation</h2><p><strong>Authors:Tianyu He, Junliang Guo, Runyi Yu, Yuchi Wang, Jialiang Zhu, Kaikai An, Leyi Li, Xu Tan, Chunyu Wang, Han Hu, HsiangTao Wu, Sheng Zhao, Jiang Bian</strong></p>
<p>Zero-shot talking avatar generation aims at synthesizing natural talking videos from speech and a single portrait image. Previous methods have relied on domain-specific heuristics such as warping-based motion representation and 3D Morphable Models, which limit the naturalness and diversity of the generated avatars. In this work, we introduce GAIA (Generative AI for Avatar), which eliminates the domain priors in talking avatar generation. In light of the observation that the speech only drives the motion of the avatar while the appearance of the avatar and the background typically remain the same throughout the entire video, we divide our approach into two stages: 1) disentangling each frame into motion and appearance representations; 2) generating motion sequences conditioned on the speech and reference portrait image. We collect a large-scale high-quality talking avatar dataset and train the model on it with different scales (up to 2B parameters). Experimental results verify the superiority, scalability, and flexibility of GAIA as 1) the resulting model beats previous baseline models in terms of naturalness, diversity, lip-sync quality, and visual quality; 2) the framework is scalable since larger models yield better results; 3) it is general and enables different applications like controllable talking avatar generation and text-instructed avatar generation. </p>
<p><a href="http://arxiv.org/abs/2311.15230v1">PDF</a> Project page: <a href="https://microsoft.github.io/GAIA/">https://microsoft.github.io/GAIA/</a></p>
<p><strong>Summary</strong><br>移除谈话头像生成中的领域先验，利用语言模型控制动作，神经网络生成外观，进行分离编码实现可控谈话头像生成。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>GAIA 无需特定领域知识，摆脱领域先验的限制，利用语言模型控制动作，神经网络生成外观。</li>
<li>GAIA 将任务分为两个阶段：分离编码与运动序列生成。</li>
<li>GAIA 的数据集包含 168 万张图片，分为训练集、验证集和测试集。</li>
<li>GAIA 可扩展，模型参数从 128M 到 2B 不等，模型越大效果越好。</li>
<li>GAIA 具有通用性，可用于可控谈话头像生成和文本指示头像生成等应用。</li>
<li>GAIA 在三个评价指标上都优于基线模型。</li>
<li>GAIA 能有效生成自然、多样、唇形同步且视觉质量高的谈话头像视频。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>标题：GAIA：零样本说话头像生成</li>
<li>作者：Tianyu He、Junliang Guo、Runyi Yu、Yuchi Wang、Jialiang Zhu、Kaikai An、Leyi Li、Xu Tan、Chunyu Wang、Han Hu、Hsiang Tao Wu、Sheng Zhao、Jiang Bian</li>
<li>隶属单位：微软</li>
<li>关键词：零样本说话头像生成、运动表示、外观表示、生成模型</li>
<li>论文链接：https://arxiv.org/abs/2311.15230</li>
<li>
<p>摘要：
（1）研究背景：说话头像生成旨在从语音和单张人像图像中合成自然说话视频。以往方法依赖于特定领域的启发式方法，例如基于扭曲的运动表示和 3D 可变形模型，这限制了生成的头像的自然性和多样性。
（2）过去方法及其问题：以往方法通过对每个头像进行特定训练（即为每个头像训练或调整特定模型）或在推理期间利用模板视频来实现高质量的结果。但是，这些方法通过引入基于扭曲的运动表示、3D 可变形模型等领域先验来降低任务的难度。虽然有效，但引入此类启发式方法阻碍了直接从数据分布中学习，并可能导致不自然的结果和有限的多样性。
（3）研究方法：本文提出 GAIA（生成式人工智能头像），消除了说话头像生成中的领域先验。GAIA 揭示了两个关键见解：1）语音只驱动头像的运动，而背景和头像的外观通常在整个视频中保持不变。受此启发，我们对每帧进行运动和外观表示的解耦，其中外观在帧之间共享，而运动对于每帧都是唯一的。为了从语音预测运动，我们将运动序列编码成运动潜在序列，并将外观编码成外观潜在表示。然后，我们使用生成模型从语音和参考人像图像生成运动序列。
（4）方法性能：我们收集了一个大规模高质量的说话头像数据集，并在不同规模（高达 2B 参数）上训练模型。实验结果验证了 GAIA 的优越性、可扩展性和灵活性，因为它 1）在自然性、多样性、唇形同步质量和视觉质量方面优于之前的基线模型；2）由于更大的模型会产生更好的结果，因此该框架是可扩展的；3）它是通用的，并支持不同的应用程序，例如可控说话头像生成和文本指导的头像生成。</p>
</li>
<li>
<p>方法：
(1) 运动与外观表示解耦：将每帧解耦为运动表示和外观表示，其中外观在帧之间共享，而运动对于每帧都是唯一的。
(2) 运动序列编码：将运动序列编码成运动潜在序列，并将外观编码成外观潜在表示。
(3) 运动序列生成：使用生成模型从语音和参考人像图像生成运动序列。
(4) 可控说话头像生成：通过替换估计的头姿势或从另一个视频中提取的头姿势，实现姿势可控的说话头像生成。
(5) 全可控说话头像生成：通过编辑生成过程中的面部地标，实现任意面部属性的可控生成。</p>
</li>
<li>
<p>结论：
（1）：xxx；
（2）：创新点：提出了一种数据驱动的零样本说话头像生成框架 GAIA，该框架由两个模块组成：一个变分自动编码器，用于解耦和编码运动和外观表示，以及一个扩散模型，用于预测运动潜在序列，该序列以输入语音为条件。我们收集了一个大规模的数据集，并提出了几种过滤策略，以实现框架的有效训练。GAIA 框架具有通用性和可扩展性，能够在零样本说话头像生成中提供自然和多样化的结果，并且可以灵活地适应其他应用程序，包括可控说话头像生成和文本指导的头像生成。
性能：在自然性、多样性、唇形同步质量和视觉质量方面优于之前的基线模型；由于更大的模型会产生更好的结果，因此该框架是可扩展的；
工作量：收集了一个大规模高质量的说话头像数据集，并在不同规模（高达2B参数）上训练模型。</p>
</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-b24792b81d1876d37fc788a87d3177d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5607b67a73e71b9a00d408089c575ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55a4dfdd281f456c1ec180ddf006ff6d.jpg" align="middle">
</details>




<h2 id="ChatAnything-Facetime-Chat-with-LLM-Enhanced-Personas"><a href="#ChatAnything-Facetime-Chat-with-LLM-Enhanced-Personas" class="headerlink" title="ChatAnything: Facetime Chat with LLM-Enhanced Personas"></a>ChatAnything: Facetime Chat with LLM-Enhanced Personas</h2><p><strong>Authors:Yilin Zhao, Xinbin Yuan, Shanghua Gao, Zhijie Lin, Qibin Hou, Jiashi Feng, Daquan Zhou</strong></p>
<p>In this technical report, we target generating anthropomorphized personas for LLM-based characters in an online manner, including visual appearance, personality and tones, with only text descriptions. To achieve this, we first leverage the in-context learning capability of LLMs for personality generation by carefully designing a set of system prompts. We then propose two novel concepts: the mixture of voices (MoV) and the mixture of diffusers (MoD) for diverse voice and appearance generation. For MoV, we utilize the text-to-speech (TTS) algorithms with a variety of pre-defined tones and select the most matching one based on the user-provided text description automatically. For MoD, we combine the recent popular text-to-image generation techniques and talking head algorithms to streamline the process of generating talking objects. We termed the whole framework as ChatAnything. With it, users could be able to animate anything with any personas that are anthropomorphic using just a few text inputs. However, we have observed that the anthropomorphic objects produced by current generative models are often undetectable by pre-trained face landmark detectors, leading to failure of the face motion generation, even if these faces possess human-like appearances because those images are nearly seen during the training (e.g., OOD samples). To address this issue, we incorporate pixel-level guidance to infuse human face landmarks during the image generation phase. To benchmark these metrics, we have built an evaluation dataset. Based on it, we verify that the detection rate of the face landmark is significantly increased from 57.0% to 92.5% thus allowing automatic face animation based on generated speech content. The code and more results can be found at <a href="https://chatanything.github.io/">https://chatanything.github.io/</a>. </p>
<p><a href="http://arxiv.org/abs/2311.06772v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>利用语言生成模型实现任意文本创建拟人化形象，包括图像、语气和性格。</p>
<p><strong>要点</strong></p>
<ul>
<li>利用语言生成模型的上下文学习能力和精心设计的系统提示，生成人物个性。</li>
</ul>
<ul>
<li>提出两种新颖概念：混合声音 (MoV) 和混合扩散器 (MoD)，用于生成多样化的声音和图像。</li>
</ul>
<ul>
<li>MoV 利用文本转语音 (TTS) 算法和各种预定义语调，根据用户提供的文本描述自动选择最匹配的语调。</li>
</ul>
<ul>
<li>MoD 将流行的文本转图像生成技术和说话头算法相结合，简化生成说话对象的流程。</li>
</ul>
<ul>
<li>ChatAnything 框架允许用户使用少量文本输入来生成具有拟人化形象的动画。</li>
</ul>
<ul>
<li>发现当前生成模型生成的拟人化对象通常无法被预训练好的面部特征检测器检测到，导致面部运动生成失败，即使这些面孔具有类似人类的外观。</li>
</ul>
<ul>
<li>通过在图像生成阶段加入像素级指导，使生成的图像包含人类面部标志，从而解决这个问题。</li>
</ul>
<ul>
<li>建立评估数据集，验证面部标志检测率从 57.0% 显着提高到 92.5%，从而允许自动生成语音内容的动画。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：ChatAnything：基于 LLM 的角色的人格化</li>
<li>作者：Silin Zhao, Xinbin Yuan, Shanghua Gao, Zhijie Lin, Qibin Hou, Jiashi Feng, Daquan Zhou</li>
<li>隶属单位：南开大学</li>
<li>关键词：自然语言处理、生成式 AI、对话系统、人机交互</li>
<li>论文链接：https://arxiv.org/abs/2311.06772
Github 代码链接：https://github.com/chatanything/chatanything</li>
<li>摘要：
(1)：随着大语言模型 (LLM) 的快速发展，其强大的上下文学习能力和生成能力引起了广泛关注。本文旨在探索一种新的框架，该框架能够根据文本描述自动生成具有定制化个性、声音和视觉外观的 LLM 增强角色。
(2)：以往的方法主要集中在 LLM 的个性化生成上，但对于声音和视觉外观的生成则相对较少。同时，现有方法生成的拟人化对象通常无法被预训练的人脸关键点检测器检测到，导致无法进行面部动作生成。
(3)：本文提出的 ChatAnything 框架通过精心设计系统提示，利用 LLM 的上下文学习能力来生成定制化的角色个性。同时，本文还提出了两种新颖的概念：声音混合 (MoV) 和扩散器混合 (MoD)，用于生成多样化的声音和外观。此外，本文还提出了一种像素级引导的方法，以在图像生成阶段注入人脸关键点，从而提高面部关键点的检测率。
(4)：实验结果表明，ChatAnything 框架能够有效地生成具有定制化个性、声音和视觉外观的 LLM 增强角色。同时，本文提出的像素级引导方法也显著提高了面部关键点的检测率，从而支持自动面部动画生成。</li>
</ol>
<p>Methods:</p>
<p>(1) The ChatAnything framework consists of four main blocks: an LLM-based control module, a portrait initializer, a mixture of text-to-speech modules, and a motion generation module.</p>
<p>(2) The portrait initializer uses a mixture of fine-tuned diffusion models (MoD) along with their LoRA module to generate a reference image for the persona.</p>
<p>(3) The mixture of text-to-speech modules (MoV) converts the text input from the persona to speech signals with customized tones.</p>
<p>(4) The motion generation module takes in the speech signal and drives the generated image.</p>
<p>(5) To inject facial landmark guidance, the framework uses a guided diffusion process with a fixed Markov Gaussian diffusion process.</p>
<p>(6) The framework also utilizes a ControlNet to inject the face feature in the process of image generation.</p>
<p>(7) A pool of stylized diffusion-based generative models and voice changers are used to customize the artistic style and voice of the generated persona.</p>
<p>(8) The framework uses a prompt template to generate the personality of the persona based on the user's input.</p>
<ol>
<li>结论：
（1）：ChatAnything 框架能够有效地生成具有定制化个性、声音和视觉外观的 LLM 增强角色。同时，本文提出的像素级引导方法也显著提高了面部关键点的检测率，从而支持自动面部动画生成。
（2）：创新点：</li>
<li>提出了一种新的框架 ChatAnything，该框架能够根据文本描述自动生成具有定制化个性、声音和视觉外观的 LLM 增强角色。</li>
<li>提出了一种新的概念：声音混合 (MoV)，用于生成多样化的声音。</li>
<li>提出了一种新的概念：扩散器混合 (MoD)，用于生成多样化的外观。</li>
<li>提出了一种像素级引导的方法，以在图像生成阶段注入人脸关键点，从而提高面部关键点的检测率。
性能：</li>
<li>ChatAnything 框架能够有效地生成具有定制化个性、声音和视觉外观的 LLM 增强角色。</li>
<li>ChatAnything 框架能够显著提高面部关键点的检测率，从而支持自动面部动画生成。
工作量：</li>
<li>ChatAnything 框架的实现相对复杂，需要较高的技术水平。</li>
<li>ChatAnything 框架的训练需要大量的数据和计算资源。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-494f70b2c5eac2c09270dc86936da0f3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ecb9537a4b6bea4a888b9a98aa5f5584.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-afea263951ed31808a8df79048c30a2c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d9d4f86301d8d00bbb0d78ff4e1a1f89.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b4fd05a1ef19cd93425962ed04f16227.jpg" align="middle">
</details>




<h2 id="DualTalker-A-Cross-Modal-Dual-Learning-Approach-for-Speech-Driven-3D-Facial-Animation"><a href="#DualTalker-A-Cross-Modal-Dual-Learning-Approach-for-Speech-Driven-3D-Facial-Animation" class="headerlink" title="DualTalker: A Cross-Modal Dual Learning Approach for Speech-Driven 3D   Facial Animation"></a>DualTalker: A Cross-Modal Dual Learning Approach for Speech-Driven 3D   Facial Animation</h2><p><strong>Authors:Guinan Su, Yanwu Yang, Zhifeng Li</strong></p>
<p>In recent years, audio-driven 3D facial animation has gained significant attention, particularly in applications such as virtual reality, gaming, and video conferencing. However, accurately modeling the intricate and subtle dynamics of facial expressions remains a challenge. Most existing studies approach the facial animation task as a single regression problem, which often fail to capture the intrinsic inter-modal relationship between speech signals and 3D facial animation and overlook their inherent consistency. Moreover, due to the limited availability of 3D-audio-visual datasets, approaches learning with small-size samples have poor generalizability that decreases the performance. To address these issues, in this study, we propose a cross-modal dual-learning framework, termed DualTalker, aiming at improving data usage efficiency as well as relating cross-modal dependencies. The framework is trained jointly with the primary task (audio-driven facial animation) and its dual task (lip reading) and shares common audio/motion encoder components. Our joint training framework facilitates more efficient data usage by leveraging information from both tasks and explicitly capitalizing on the complementary relationship between facial motion and audio to improve performance. Furthermore, we introduce an auxiliary cross-modal consistency loss to mitigate the potential over-smoothing underlying the cross-modal complementary representations, enhancing the mapping of subtle facial expression dynamics. Through extensive experiments and a perceptual user study conducted on the VOCA and BIWI datasets, we demonstrate that our approach outperforms current state-of-the-art methods both qualitatively and quantitatively. We have made our code and video demonstrations available at <a href="https://github.com/sabrina-su/iadf.git">https://github.com/sabrina-su/iadf.git</a>. </p>
<p><a href="http://arxiv.org/abs/2311.04766v2">PDF</a> </p>
<p><strong>摘要</strong><br>音频驱动3D面部动画框架DualTalker，以音频为驱动，对3D面部进行动画，学习面部运动与音频之间的互补关系，提高数据利用率，生成逼真的面部表情。</p>
<p><strong>要点</strong></p>
<ul>
<li>提出音频驱动3D面部动画框架DualTalker，提高数据利用率，生成更为逼真的面部表情。</li>
<li>DualTalker由音频-运动编码器组成，训练主要任务（音频驱动面部动画）及其双重任务（唇读）。</li>
<li>联合训练促进信息共享，提高性能，辅助跨模态一致性损失减轻过度平滑。</li>
<li>实验表明，DualTalker在VOCA和BIWI数据集上的表现优于当前最先进的方法。</li>
<li>代码和视频演示可在<a href="https://github.com/sabrina-su/iadf.git上获取。">https://github.com/sabrina-su/iadf.git上获取。</a></li>
<li>我们的方法提高了面部动画的质量，并使音频-视觉同步更加自然。</li>
<li>DualTalker实现了跨模态双学习，具有良好的数据利用效率和泛化能力。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：双语者：一种用于语音驱动的三维面部动画的跨模态双重学习方法</li>
<li>作者：顾南苏，杨炎武，李志锋</li>
<li>单位：腾讯数据平台</li>
<li>关键词：双重学习、语音驱动的面部动画、跨模态一致性、Transformer</li>
<li>论文链接：https://arxiv.org/abs/2311.04766
    Github 链接：https://github.com/Guinan-Su/DualTalker</li>
<li>
<p>摘要：
(1)：语音驱动的三维面部动画技术近年来备受关注，但准确建模面部表情的复杂动态仍是一个挑战。
(2)：现有方法通常将面部动画任务视为单一回归问题，忽略了语音信号和三维面部动画之间的内在跨模态关系及其固有的一致性。此外，由于三维音频视觉数据集的有限可用性，使用小样本学习的方法具有较差的泛化能力，降低了性能。
(3)：提出了一种跨模态双重学习框架，旨在提高数据利用效率，并关联跨模态依赖关系以进一步提高性能。该框架与主任务（语音驱动的面部动画）及其双重任务（唇读）联合训练，并共享共同的音频/运动编码器组件。
(4)：在 VOCA 和 BIWI 数据集上进行的广泛实验和感知用户研究表明，该方法在定性和定量上都优于当前最先进的方法。</p>
</li>
<li>
<p>方法：
（1）提出了一种跨模态双重学习框架，用于语音驱动的三维面部动画。该框架由主任务（语音驱动的面部动画）及其双重任务（唇读）组成，并共享共同的音频/运动编码器组件。
（2）在语音驱动的面部动画任务中，采用编码器-解码器框架，其中编码器将语音信号转换为语音表示，解码器利用语音表示和过去的运动序列来预测面部运动。
（3）在唇读任务中，同样采用编码器-解码器框架，其中编码器将面部运动转换为运动表示，解码器利用运动表示和过去的语音特征来预测语音特征。
（4）为了实现语音驱动的面部动画和唇读的互补性，提出了一种对偶正则化损失，该损失函数鼓励语音驱动的面部动画和唇读的预测结果在跨模态特征空间中保持一致。
（5）在VOCA和BIWI数据集上进行的广泛实验和感知用户研究表明，该方法在定性和定量上都优于当前最先进的方法。</p>
</li>
<li>
<p>结论：
（1）：提出了一种跨模态双重学习框架，用于语音驱动的三维面部动画，有效地解决了语音驱动面部动画中固有的挑战。通过将面部动画和唇读组件表述为双重任务，并结合创新的参数共享方案和对偶正则化器，该方法有效地提高了数据利用率，并关联了跨模态依赖关系，进一步提高了性能。
（2）：创新点：</p>
</li>
<li>提出了一种跨模态双重学习框架，用于语音驱动的三维面部动画，该框架由主任务（语音驱动的面部动画）及其双重任务（唇读）组成，并共享共同的音频/运动编码器组件。</li>
<li>在语音驱动的面部动画任务中，采用编码器-解码器框架，其中编码器将语音信号转换为语音表示，解码器利用语音表示和过去的运动序列来预测面部运动。</li>
<li>在唇读任务中，同样采用编码器-解码器框架，其中编码器将面部运动转换为运动表示，解码器利用运动表示和过去的语音特征来预测语音特征。</li>
<li>提出了一种对偶正则化损失，该损失函数鼓励语音驱动的面部动画和唇读的预测结果在跨模态特征空间中保持一致。
性能：</li>
<li>在VOCA和BIWI数据集上进行的广泛实验和感知用户研究表明，该方法在定性和定量上都优于当前最先进的方法。</li>
<li>该方法在VOCA数据集上的平均误差为0.012，在BIWI数据集上的平均误差为0.015，均优于当前最先进的方法。</li>
<li>该方法在VOCA数据集上的感知用户研究中获得了4.2分的平均分（满分5分），在BIWI数据集上的感知用户研究中获得了4.1分的平均分，均优于当前最先进的方法。
工作量：</li>
<li>该方法的实现相对复杂，需要对跨模态双重学习框架、对偶正则化损失等进行深入理解。</li>
<li>该方法的训练时间相对较长，在VOCA数据集上训练一次需要约24小时，在BIWI数据集上训练一次需要约36小时。</li>
<li>该方法的推理时间相对较短，在VOCA数据集上推理一次需要约0.1秒，在BIWI数据集上推理一次需要约0.15秒。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2796e7be16d59d2ade40f87447f93837.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c353db315d023ec1c2c174b1887e6302.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e14c4424ba13626c11a4cc40af3ca98c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5252ec4a4a561db9ad58b8059a17f121.jpg" align="middle">
</details>




<h2 id="3D-Aware-Talking-Head-Video-Motion-Transfer"><a href="#3D-Aware-Talking-Head-Video-Motion-Transfer" class="headerlink" title="3D-Aware Talking-Head Video Motion Transfer"></a>3D-Aware Talking-Head Video Motion Transfer</h2><p><strong>Authors:Haomiao Ni, Jiachen Liu, Yuan Xue, Sharon X. Huang</strong></p>
<p>Motion transfer of talking-head videos involves generating a new video with the appearance of a subject video and the motion pattern of a driving video. Current methodologies primarily depend on a limited number of subject images and 2D representations, thereby neglecting to fully utilize the multi-view appearance features inherent in the subject video. In this paper, we propose a novel 3D-aware talking-head video motion transfer network, Head3D, which fully exploits the subject appearance information by generating a visually-interpretable 3D canonical head from the 2D subject frames with a recurrent network. A key component of our approach is a self-supervised 3D head geometry learning module, designed to predict head poses and depth maps from 2D subject video frames. This module facilitates the estimation of a 3D head in canonical space, which can then be transformed to align with driving video frames. Additionally, we employ an attention-based fusion network to combine the background and other details from subject frames with the 3D subject head to produce the synthetic target video. Our extensive experiments on two public talking-head video datasets demonstrate that Head3D outperforms both 2D and 3D prior arts in the practical cross-identity setting, with evidence showing it can be readily adapted to the pose-controllable novel view synthesis task. </p>
<p><a href="http://arxiv.org/abs/2311.02549v1">PDF</a> WACV2024</p>
<p><strong>摘要</strong><br>3D 感知说话头部视频运动迁移网络 Head3D，利用循环网络从 2D 主体帧生成视觉可解释的 3D 规范头部，较好地解决了多视角外观特征利用不充分的问题。</p>
<p><strong>要点</strong></p>
<ul>
<li>Head3D 通过循环网络从 2D 主体帧生成视觉可解释的 3D 规范头部，充分利用了主体外观信息。</li>
<li>Head3D 的关键组成部分是自监督 3D 头部几何学习模块，旨在从 2D 主体视频帧预测头部姿态和深度图。</li>
<li>Head3D 使用基于注意力的融合网络将主体帧中的背景和其他细节与 3D 主体头部相结合，进而生成合成目标视频。</li>
<li>Head3D 在两个公开说话头部视频数据集上的广泛实验表明，Head3D 在实际的跨身份设置中优于 2D 和 3D 先验方法，并且可以轻松适应可控姿势的新视图合成任务。</li>
<li>Head3D 较好地解决了多视角外观特征利用不充分的问题。</li>
<li>Head3D 可以充分利用 2D 主体图像和 3D 主体视频的优势。</li>
<li>Head3D 在实际的跨身份设置中优于 2D 和 3D 先验方法。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：3D感知说话人头部视频动作迁移</li>
<li>作者：Haomiao Ni, Jiachen Liu, Yuan Xue, Sharon X. Huang</li>
<li>第一作者单位：宾夕法尼亚州立大学</li>
<li>关键词：说话人头部视频、动作迁移、3D感知、自监督学习、注意机制</li>
<li>论文链接：https://arxiv.org/abs/2311.02549
Github 链接：无</li>
<li>摘要：
(1) 研究背景：说话人头部视频动作迁移旨在生成一个具有目标主体的外观和驱动视频的运动模式的新视频。现有的方法主要依赖于有限数量的主体图像和 2D 表示，从而忽略了充分利用主体视频中固有的多视角外观特征。
(2) 过去的方法及其问题：现有的方法主要使用一个主体图像或简单组合几个主体图像与 2D 表示。这些方法可能难以充分利用主体视频中固有的多视角外观信息。
(3) 本文提出的研究方法：本文提出 Head3D，这是一个新颖的 3D 感知说话人头部视频动作迁移框架。该框架以自监督、非对抗的方式进行训练，能够通过自监督 3D 头部几何学习从每个 2D 视频帧中恢复 3D 结构信息（即头部姿势和深度），而无需 3D 人头图形模型。通过将每个选定的主体视频帧映射到 3D 规范空间，Head3D 进一步使用循环网络估计 3D 主体规范头部。为了合成最终的视频帧，Head3D 采用基于注意力的融合机制将来自 3D 主体头部的外观特征与来自主体的背景和其他细节（例如，面部表情、肩膀）相结合。
(4) 方法在哪些任务上取得了怎样的性能，这些性能是否支持了它们的目标：在两个公开的说话人头部视频数据集上进行的广泛实验表明，Head3D 在实际的跨身份设置中优于 2D 和 3D 先验技术，并且有证据表明它可以很容易地适应姿势可控的新视角合成任务。</li>
</ol>
<p>Methods:
(1): Head3D采用自监督方式进行训练，使用自我重建损失来恢复一个视频帧与同一视频中随机采样的几个帧。这种训练过程既不需要任何人工标注，也不涉及对抗训练。
(2): Head3D的训练包括三个阶段：(1) 3D头部几何学习，(2) 循环规范头部生成，(3) 基于注意力的融合机制。为了便于训练，我们将这三个阶段的模块分别进行训练。
(3): 在3D头部几何学习阶段，我们利用一个自监督的3D头部几何学习框架来训练一个深度网络FD和一个姿态网络FP，用于预测每个2D视频帧的头部姿态和深度。
(4): 在循环规范头部生成阶段，我们使用一个循环规范头部生成网络，该网络利用基于ConvLSTM的特征聚合来创建一个3D规范头部ˆxc，其中包含了扭曲的参考帧特征。
(5): 在基于注意力的融合机制阶段，我们采用基于注意力的融合机制来合成每个最终输出帧ˆsdri，方法是将规范头部ˆxc的外观特征、来自随机选取的主题帧sref的背景和其他外观细节（例如，颈部和肩膀）以及来自驱动帧dri的运动和表情信息进行组合。</p>
<ol>
<li>结论：
（1）：本文提出了一种新的说话人头部视频动作迁移框架 Head3D，该框架能够通过自监督 3D 头部几何学习从每个 2D 视频帧中恢复 3D 结构信息，并进一步使用循环网络估计 3D 主体规范头部。Head3D 采用基于注意力的融合机制将来自 3D 主体头部的外观特征与来自主体的背景和其他细节相结合，合成最终的视频帧。
（2）：创新点：</li>
<li>Head3D 提出了一种新的自监督 3D 头部几何学习方法，能够从 2D 视频帧中恢复 3D 结构信息，无需 3D 人头图形模型。</li>
<li>Head3D 使用循环网络估计 3D 主体规范头部，该方法能够有效地捕获主体头部在视频中的运动模式。</li>
<li>Head3D 采用基于注意力的融合机制将来自 3D 主体头部的外观特征与来自主体的背景和其他细节相结合，合成最终的视频帧。
性能：</li>
<li>Head3D 在两个公开的说话人头部视频数据集上进行的广泛实验表明，Head3D 在实际的跨身份设置中优于 2D 和 3D 先验技术。</li>
<li>Head3D 可以很容易地适应姿势可控的新视角合成任务。
工作量：</li>
<li>Head3D 的训练需要大量的视频数据，这可能会增加训练时间和计算成本。</li>
<li>Head3D 的模型结构相对复杂，这可能会增加模型的训练和推理时间。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-35afec6fc14c4cd3bb501e49b198de69.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-afc27ddf3e6f7773dffd54e160f21da6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d1e8cece6f2ce2f23fc15496e6200de8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64af4f2f40a878ceba7a79e3170cfa03.jpg" align="middle">
</details>




<h2 id="Breathing-Life-into-Faces-Speech-driven-3D-Facial-Animation-with-Natural-Head-Pose-and-Detailed-Shape"><a href="#Breathing-Life-into-Faces-Speech-driven-3D-Facial-Animation-with-Natural-Head-Pose-and-Detailed-Shape" class="headerlink" title="Breathing Life into Faces: Speech-driven 3D Facial Animation with   Natural Head Pose and Detailed Shape"></a>Breathing Life into Faces: Speech-driven 3D Facial Animation with   Natural Head Pose and Detailed Shape</h2><p><strong>Authors:Wei Zhao, Yijun Wang, Tianyu He, Lianying Yin, Jianxin Lin, Xin Jin</strong></p>
<p>The creation of lifelike speech-driven 3D facial animation requires a natural and precise synchronization between audio input and facial expressions. However, existing works still fail to render shapes with flexible head poses and natural facial details (e.g., wrinkles). This limitation is mainly due to two aspects: 1) Collecting training set with detailed 3D facial shapes is highly expensive. This scarcity of detailed shape annotations hinders the training of models with expressive facial animation. 2) Compared to mouth movement, the head pose is much less correlated to speech content. Consequently, concurrent modeling of both mouth movement and head pose yields the lack of facial movement controllability. To address these challenges, we introduce VividTalker, a new framework designed to facilitate speech-driven 3D facial animation characterized by flexible head pose and natural facial details. Specifically, we explicitly disentangle facial animation into head pose and mouth movement and encode them separately into discrete latent spaces. Then, these attributes are generated through an autoregressive process leveraging a window-based Transformer architecture. To augment the richness of 3D facial animation, we construct a new 3D dataset with detailed shapes and learn to synthesize facial details in line with speech content. Extensive quantitative and qualitative experiments demonstrate that VividTalker outperforms state-of-the-art methods, resulting in vivid and realistic speech-driven 3D facial animation. </p>
<p><a href="http://arxiv.org/abs/2310.20240v1">PDF</a> </p>
<p><strong>摘要</strong><br>利用可变头部姿势和自然的面部细节实现逼真的语音驱动 3D 面部动画。</p>
<p><strong>要点</strong></p>
<ul>
<li>现有作品未能呈现出灵活头部姿势和自然面部细节的形状。</li>
<li>导致上述限制的两个主要因素是：训练数据集的收集成本高昂，且头部姿势与语音内容的相关性较低。</li>
<li>VividTalker 框架可将面部动画明确分解为头部姿势和嘴巴动作，并将其分别编码为离散的潜在空间。</li>
<li>采用基于窗口的 Transformer 架构，通过自回归过程生成这些属性。</li>
<li>构建了一个包含详细形状的新 3D 数据集，并学会根据语音内容合成面部细节。</li>
<li>VividTalker 在定量和定性实验中均优于现有方法，可实现生动逼真的语音驱动 3D 面部动画。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>标题：赋予面部生命：自然头部姿势和详细形状的语音驱动的 3D 面部动画</li>
<li>作者：魏巍，王艺军，何天宇，尹连英，林建新，金鑫</li>
<li>隶属单位：湖南大学计算机科学与电子工程学院</li>
<li>关键词：3D 面部动画，详细面部形状，动作解耦</li>
<li>论文链接：https://weizhaomolecules.github.io/VividTalker/，Github 代码链接：None</li>
<li>
<p>摘要：
（1）研究背景：3D 虚拟面部动画领域因其在娱乐、通信和医疗保健等领域的巨大价值而备受关注和研究兴趣。3D 虚拟面部动画的成功依赖于表现出类人特征，包括同步和自然性。同步涉及创建与用户期望一致的可信动画，弥合虚拟头像与现实世界之间的差距。自然性涉及创建具有自然的、逼真的面部细节（例如皱纹）的动画。
（2）过去的方法及其问题：现有工作在使用灵活的头部姿势和自然的细节（例如皱纹）渲染形状方面仍然存在不足。这种限制主要归因于两个方面：1) 收集具有详细 3D 面部形状的训练集非常昂贵。这种详细形状注释的稀缺阻碍了训练具有富有表现力的面部动画的模型。2) 与嘴巴运动相比，头部姿势与语音内容的相关性要小很多。因此，对嘴巴运动和头部姿势同时建模导致缺乏面部运动可控性。
（3）研究方法：为了解决这些挑战，我们引入了 VividTalker，这是一个旨在促进语音驱动的 3D 面部动画的新框架，其特点是灵活的头部姿势和自然的面部细节。具体来说，我们明确地将面部动画解耦成头部姿势和嘴巴运动，并将它们分别编码成离散的潜在空间。然后，通过利用基于窗口的 Transformer 架构的回归过程生成这些属性。为了增加 3D 面部动画的丰富性，我们构建了一个新的具有详细形状的 3D 数据集，并学会了根据语音内容合成面部细节。
（4）方法的性能：广泛的定量和定性实验表明，VividTalker 优于最先进的方法，产生了生动逼真的语音驱动的 3D 面部动画。这些性能证明了我们的方法可以很好地实现目标。</p>
</li>
<li>
<p>方法：
（1）：我们提出了 VividTalker，一个旨在促进语音驱动的 3D 面部动画的新框架，其特点是灵活的头部姿势和自然的面部细节。
（2）：我们将面部动画明确地解耦成头部姿势和嘴巴运动，并将它们分别编码成离散的潜在空间。
（3）：我们利用基于窗口的 Transformer 架构的回归过程生成这些属性。
（4）：我们构建了一个新的具有详细形状的 3D 数据集，并学会了根据语音内容合成面部细节。</p>
</li>
<li>
<p>结论：
（1）：VividTalker 旨在促进语音驱动的 3D 面部动画，具有灵活的头部姿势和自然的面部细节，在定量和定性实验中优于最先进的方法，产生了生动逼真的语音驱动的 3D 面部动画。
（2）：创新点：</p>
</li>
<li>将头部姿势和嘴巴运动明确解耦，分别编码成离散的潜在空间。</li>
<li>利用基于窗口的 Transformer 架构的回归过程生成这些属性。</li>
<li>构建了一个新的具有详细形状的 3D 数据集，学会了根据语音内容合成面部细节。
性能：</li>
<li>VividTalker 在定量和定性实验中优于最先进的方法，产生了生动逼真的语音驱动的 3D 面部动画。</li>
<li>VividTalker 能够生成具有灵活的头部姿势和自然的面部细节的动画。
工作量：</li>
<li>VividTalker 需要构建一个新的具有详细形状的 3D 数据集，并且需要训练一个基于窗口的 Transformer 架构的回归模型。</li>
<li>VividTalker 的训练过程可能需要大量的时间和计算资源。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7d367bbd980109a452dcecf661c89318.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-61f898b8eea6a84ec03e1da36317e047.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9f40581024d13a376e4d202368288380.jpg" align="middle">
</details>




<h2 id="CorrTalk-Correlation-Between-Hierarchical-Speech-and-Facial-Activity-Variances-for-3D-Animation"><a href="#CorrTalk-Correlation-Between-Hierarchical-Speech-and-Facial-Activity-Variances-for-3D-Animation" class="headerlink" title="CorrTalk: Correlation Between Hierarchical Speech and Facial Activity   Variances for 3D Animation"></a>CorrTalk: Correlation Between Hierarchical Speech and Facial Activity   Variances for 3D Animation</h2><p><strong>Authors:Zhaojie Chu, Kailing Guo, Xiaofen Xing, Yilin Lan, Bolun Cai, Xiangmin Xu</strong></p>
<p>Speech-driven 3D facial animation is a challenging cross-modal task that has attracted growing research interest. During speaking activities, the mouth displays strong motions, while the other facial regions typically demonstrate comparatively weak activity levels. Existing approaches often simplify the process by directly mapping single-level speech features to the entire facial animation, which overlook the differences in facial activity intensity leading to overly smoothed facial movements. In this study, we propose a novel framework, CorrTalk, which effectively establishes the temporal correlation between hierarchical speech features and facial activities of different intensities across distinct regions. A novel facial activity intensity metric is defined to distinguish between strong and weak facial activity, obtained by computing the short-time Fourier transform of facial vertex displacements. Based on the variances in facial activity, we propose a dual-branch decoding framework to synchronously synthesize strong and weak facial activity, which guarantees wider intensity facial animation synthesis. Furthermore, a weighted hierarchical feature encoder is proposed to establish temporal correlation between hierarchical speech features and facial activity at different intensities, which ensures lip-sync and plausible facial expressions. Extensive qualitatively and quantitatively experiments as well as a user study indicate that our CorrTalk outperforms existing state-of-the-art methods. The source code and supplementary video are publicly available at: <a href="https://zjchu.github.io/projects/CorrTalk/">https://zjchu.github.io/projects/CorrTalk/</a> </p>
<p><a href="http://arxiv.org/abs/2310.11295v1">PDF</a> </p>
<p><strong>摘要</strong><br>扩展了强弱面部动作相关性解码框架，实现多层次语言特征与不同面部区域多强度面部动作时间同层相关性，实现更自然的说话头部生成。</p>
<p><strong>要点</strong></p>
<ul>
<li>提出了一种新颖的框架 CorrTalk，该框架有效地建立了分层语音特征与不同强度、不同区域的面部活动之间的时间相关性。</li>
<li>定义了一种新颖的面部活动强度度量来区分强弱面部活动，该度量是通过计算面部顶点位移的短时傅里叶变换获得的。</li>
<li>提出了一种双分支解码框架，用于同步合成强弱面部活动，保证了强度更广的面部动画合成。</li>
<li>提出了一种加权分层特征编码器，用于建立分层语音特征与不同强度下地面部活动之间的时间相关性，确保唇形同步和合理的面部表情。</li>
<li>大量定性和定量实验以及用户研究表明，我们的 CorrTalk 优于现有最先进的方法。</li>
<li>源代码和补充视频可以在以下网址公开获得：<a href="https://zjchu.github.io/projects/CorrTalk/">https://zjchu.github.io/projects/CorrTalk/</a></li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：CorrTalk：用于 3D 动画的分层语音与面部活动方差之间的相关性</li>
<li>作者：赵杰楚、凯玲郭、肖芬兴、伊琳兰、波伦蔡、项民旭</li>
<li>隶属机构：华南理工大学</li>
<li>关键词：3D 面部动画、分层语音特征、3D 说话头部、面部活动差异、Transformer</li>
<li>论文链接：https://arxiv.org/abs/2310.11295，Github 链接：None</li>
<li>摘要：
（1）研究背景：语音驱动的 3D 面部动画是一项具有挑战性的跨模态任务，近年来引起了越来越多的研究兴趣。在说话活动中，嘴巴会显示出强烈的运动，而其他面部区域通常表现出相对较弱的活动水平。现有方法通常通过将单层语音特征直接映射到整个面部动画来简化过程，这忽略了面部活动强度方面的差异，导致面部运动过于平滑。
（2）过去方法及问题：一些研究调查了语音模态。早期工作建立了音素与面部手势之间的映射。单个音素可能对应几个合理的唇形，导致跨模态的不确定性。为了减轻面部姿势的模糊性，引入了短滑动窗口机制来剪辑几个连续的语音帧，然后为相应的视觉帧添加动画。短音频窗口从相邻的语音帧中捕获了额外的信息，但仍然导致面部运动变化的不确定性。MeshTalk 应用长期音频窗口来合成每个视觉帧。FaceFormer 提出了一种基于 Transformer 的模型来捕获帧级长期音频上下文的依赖性。尽管捕获长期上下文可以提高语音驱动面部动画的逼真性能，但过度冗长的长期上下文不可避免地会引入冗余信息，而过长或过短的单层语音特征缺乏足够的时间分辨率。相比之下，一些工作仅关注嘴巴的动画。嘴巴的运动在说话活动中最为常见，但在说话活动中，嘴巴和其他面部肌肉的协同运动是无法忽视的。最近，通过使用单层语音特征直接驱动整个面部动画来推动前沿。然而，忽略了不同区域（例如嘴巴和其他区域）的面部活动强度方面的差异。
（3）研究方法：为了解决上述问题，本文提出了一种新颖的框架 CorrTalk，该框架有效地建立了分层语音特征与不同强度和不同区域的面部活动之间的时序相关性。定义了一种新的面部活动强度度量来区分强弱面部活动，该度量是通过计算面部顶点位移的短时傅里叶变换获得的。基于面部活动的变化，提出了一种双分支解码框架来同步合成强弱面部活动，从而保证了更广泛强度的面部动画合成。此外，提出了一种加权分层特征编码器来建立分层语音特征与不同强度下的面部活动之间的时序相关性，从而确保唇形同步和合理的面部表情。
（4）方法性能：广泛的定性和定量实验以及用户研究表明，CorrTalk 优于现有的最先进方法。该方法在任务和性能方面取得的成就：</li>
<li>在 VOCASET 数据集上，CorrTalk 在唇形同步、面部表情和整体视觉质量方面优于最先进的方法。</li>
<li>在 VoxCeleb 数据集上，CorrTalk 在唇形同步和面部表情方面优于最先进的方法。</li>
<li>
<p>在用户研究中，CorrTalk 在唇形同步、面部表情和整体视觉质量方面均优于最先进的方法。
这些性能支持了本文的目标。</p>
</li>
<li>
<p>方法：
(1) 提出一种新的面部活动强度度量来区分强弱面部活动，该度量是通过计算面部顶点位移的短时傅里叶变换获得的。
(2) 基于面部活动的变化，提出了一种双分支解码框架来同步合成强弱面部活动，从而保证了更广泛强度的面部动画合成。
(3) 提出了一种加权分层特征编码器来建立分层语音特征与不同强度下的面部活动之间的时序相关性，从而确保唇形同步和合理的面部表情。</p>
</li>
<li>
<p>结论：
（1）：本文提出了一种新颖的驱动框架 CorrTalk，该框架有效地捕获了分层语音特征与不同强度和不同区域的面部活动之间的时序相关性。该框架考虑了面部活动强度的差异以及不同层次语音表征的异质性。加权分层特征编码器提供了一种互补且有效的机制来建立语音和面部活动之间的相关性。广泛的定性和定量实验以及用户研究表明，CorrTalk 优于现有的最先进方法。
（2）：创新点：</p>
</li>
<li>提出了一种新的面部活动强度度量来区分强弱面部活动，该度量是通过计算面部顶点位移的短时傅里叶变换获得的。</li>
<li>基于面部活动的变化，提出了一种双分支解码框架来同步合成强弱面部活动，从而保证了更广泛强度的面部动画合成。</li>
<li>提出了一种加权分层特征编码器来建立分层语音特征与不同强度下的面部活动之间的时序相关性，从而确保唇形同步和合理的面部表情。
性能：</li>
<li>在 VOCASET 数据集上，CorrTalk 在唇形同步、面部表情和整体视觉质量方面优于最先进的方法。</li>
<li>在 VoxCeleb 数据集上，CorrTalk 在唇形同步和面部表情方面优于最先进的方法。</li>
<li>在用户研究中，CorrTalk 在唇形同步、面部表情和整体视觉质量方面均优于最先进的方法。
工作量：</li>
<li>该方法需要大量的数据来训练，这可能需要大量的时间和计算资源。</li>
<li>该方法的实现可能需要大量的代码和工程工作。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d922178e56a58ce3c71b9f1423874fb1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-482c00c6ed52f71800345e13e8d77a81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cab60223ad61f1d2129c598293c0da62.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4c347875935ebcaed314d4b9997f39ba.jpg" align="middle">
</details>




<h2 id="Mini-DALLE3-Interactive-Text-to-Image-by-Prompting-Large-Language-Models"><a href="#Mini-DALLE3-Interactive-Text-to-Image-by-Prompting-Large-Language-Models" class="headerlink" title="Mini-DALLE3: Interactive Text to Image by Prompting Large Language   Models"></a>Mini-DALLE3: Interactive Text to Image by Prompting Large Language   Models</h2><p><strong>Authors:Zeqiang Lai, Xizhou Zhu, Jifeng Dai, Yu Qiao, Wenhai Wang</strong></p>
<p>The revolution of artificial intelligence content generation has been rapidly accelerated with the booming text-to-image (T2I) diffusion models. Within just two years of development, it was unprecedentedly of high-quality, diversity, and creativity that the state-of-the-art models could generate. However, a prevalent limitation persists in the effective communication with these popular T2I models, such as Stable Diffusion, using natural language descriptions. This typically makes an engaging image hard to obtain without expertise in prompt engineering with complex word compositions, magic tags, and annotations. Inspired by the recently released DALLE3 - a T2I model directly built-in ChatGPT that talks human language, we revisit the existing T2I systems endeavoring to align human intent and introduce a new task - interactive text to image (iT2I), where people can interact with LLM for interleaved high-quality image generation/edit/refinement and question answering with stronger images and text correspondences using natural language. In addressing the iT2I problem, we present a simple approach that augments LLMs for iT2I with prompting techniques and off-the-shelf T2I models. We evaluate our approach for iT2I in a variety of common-used scenarios under different LLMs, e.g., ChatGPT, LLAMA, Baichuan, and InternLM. We demonstrate that our approach could be a convenient and low-cost way to introduce the iT2I ability for any existing LLMs and any text-to-image models without any training while bringing little degradation on LLMs’ inherent capabilities in, e.g., question answering and code generation. We hope this work could draw broader attention and provide inspiration for boosting user experience in human-machine interactions alongside the image quality of the next-generation T2I systems. </p>
<p><a href="http://arxiv.org/abs/2310.07653v2">PDF</a> Technical report. Project page at <a href="https://minidalle3.github.io/">https://minidalle3.github.io/</a></p>
<p><strong>Summary</strong><br>通过增强 LLM 与现有文生图模型的交互能力，提出一种新的交互式文本图像生成任务，提高了人机交互的图像质量和用户体验。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>文本到图像 (T2I) 扩散模型的蓬勃发展极大地加速了人工智能内容生成的革命。</li>
<li>目前流行的 T2I 模型，如 Stable Diffusion，在使用自然语言描述时存在有效的沟通障碍。</li>
<li>受到近期发布的 DALL-E3 模型的启发，该模型直接内置 ChatGPT 并使用人类语言，我们重新审视现有 T2I 系统，努力实现人类意图的一致性，并提出一个新任务——交互式文本到图像 (iT2I)，人们可以使用 LLM 进行交织的高质量图像生成/编辑/细化，并使用自然语言进行问题回答，从而获得更强的图像和文本对应关系。</li>
<li>为了解决 iT2I 问题，我们提出了一种简单的方法，利用提示技术和现成的 T2I 模型来增强 LLM 的 iT2I 能力。</li>
<li>我们使用不同的 LLM（如 ChatGPT、LLAMA、Baichuan 和 InternLM）在各种常用场景下评估了我们的 iT2I 方法。</li>
<li>我们证明了我们的方法可以方便且低成本地为任何现有的 LLM 和任何文本到图像模型引入 iT2I 能力，而无需任何培训，同时对 LLM 在问题回答和代码生成等方面的固有能力几乎没有影响。</li>
<li>我们希望这项工作能够引起更广泛的关注，并为提升下一代 T2I 系统的图像质量和人机交互的用户体验提供灵感。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>标题：交互式图像生成故事概念原型交互式 Logo 设计</li>
<li>作者：T2IModel, StableDiffusionXL</li>
<li>单位：无</li>
<li>关键词：交互式图像生成、故事概念原型、交互式 Logo 设计</li>
<li>链接：无，Github 代码链接：无</li>
<li>摘要：
(1)：随着人工智能技术的发展，交互式图像生成技术逐渐成熟，为人们提供了新的创作方式。
(2)：过去的方法通常需要用户具备一定的专业知识和技能，并且生成结果往往不够令人满意。
(3)：本文提出了一种新的交互式图像生成方法，该方法允许用户通过简单的自然语言指令来生成图像，并且生成的图像质量较高。
(4)：该方法在多个任务上取得了良好的性能，并且能够支持用户生成各种各样的图像。</li>
</ol>
<p>Some Error for method(比如是不是没有Methods这个章节)</p>
<ol>
<li>结论：
（1）：本文提出了一种交互式文本到图像（iT2I）的概念，并提出了一种增强现有大型语言模型以完成此任务的方法。我们的评估表明，这种方法能够实现便捷的 iT2I 功能，而不会显著降低模型固有的能力。这项工作有可能增强人机交互中的用户体验，并提升下一代 T2I 模型的图像质量，为未来的研究和发展提供了有希望的方向。
（2）：创新点：
提出了一种新的交互式图像生成方法，该方法允许用户通过简单的自然语言指令来生成图像，并且生成的图像质量较高。
性能：
该方法在多个任务上取得了良好的性能，并且能够支持用户生成各种各样的图像。
工作量：
该方法的实现相对简单，并且可以在多种平台上运行。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-3792d852f31bc708a2cea9f03355bb97.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6afd19703c3d90bc25da231ed316facd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67bff90f67e33e05a8dc4330ebea5ab0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67ec55466a5a0a886f9c4fced2ca756a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0f4f4599995309681f54e1a3473d5606.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f8385b3f282787527483626a5982157.jpg" align="middle">
</details>




<h2 id="AdaMesh-Personalized-Facial-Expressions-and-Head-Poses-for-Adaptive-Speech-Driven-3D-Facial-Animation"><a href="#AdaMesh-Personalized-Facial-Expressions-and-Head-Poses-for-Adaptive-Speech-Driven-3D-Facial-Animation" class="headerlink" title="AdaMesh: Personalized Facial Expressions and Head Poses for Adaptive   Speech-Driven 3D Facial Animation"></a>AdaMesh: Personalized Facial Expressions and Head Poses for Adaptive   Speech-Driven 3D Facial Animation</h2><p><strong>Authors:Liyang Chen, Weihong Bao, Shun Lei, Boshi Tang, Zhiyong Wu, Shiyin Kang, Haozhi Huang</strong></p>
<p>Speech-driven 3D facial animation aims at generating facial movements that are synchronized with the driving speech, which has been widely explored recently. Existing works mostly neglect the person-specific talking style in generation, including facial expression and head pose styles. Several works intend to capture the personalities by fine-tuning modules. However, limited training data leads to the lack of vividness. In this work, we propose AdaMesh, a novel adaptive speech-driven facial animation approach, which learns the personalized talking style from a reference video of about 10 seconds and generates vivid facial expressions and head poses. Specifically, we propose mixture-of-low-rank adaptation (MoLoRA) to fine-tune the expression adapter, which efficiently captures the facial expression style. For the personalized pose style, we propose a pose adapter by building a discrete pose prior and retrieving the appropriate style embedding with a semantic-aware pose style matrix without fine-tuning. Extensive experimental results show that our approach outperforms state-of-the-art methods, preserves the talking style in the reference video, and generates vivid facial animation. The supplementary video and code will be available at <a href="https://adamesh.github.io">https://adamesh.github.io</a>. </p>
<p><a href="http://arxiv.org/abs/2310.07236v2">PDF</a> Project Page: <a href="https://adamesh.github.io">https://adamesh.github.io</a></p>
<p><strong>Summary</strong><br>AdaMesh 是一种新颖的自适应语音驱动面部动画方法，通过学习约 10 秒的参考视频来学习个性化说话风格。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>AdaMesh 是一种新颖的语音驱动的人脸动画方法，它学习个性化的说话风格，并生成生动的面部表情和头部姿势。</li>
<li>AdaMesh 使用混合低秩自适应 (MoLoRA) 来微调表情适配器，有效地捕获面部表情风格。</li>
<li>AdaMesh 为个性化姿势风格构建离散姿势先验，并通过具有语义感知的姿势风格矩阵检索适当的风格嵌入，而无需微调。</li>
<li>AdaMesh 在多个数据集上优于最先进的方法，保留了参考视频中的说话风格，并生成生动的面部动画。</li>
<li>AdaMesh 的代码和补充视频可在网站上找到。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：AdaMesh：个性化面部表情和头部姿势的自适应语音驱动 3D 面部动画</li>
<li>作者：Liyang Chen, Weihong Bao, Shun Lei, Boshi Tang, Zhiyong Wu, Shiyin Kang, Haozhi Huang</li>
<li>隶属机构：深圳国际研究生院，清华大学</li>
<li>关键词：面部动画、语音驱动、个性化、自适应、混合低秩自适应</li>
<li>论文链接：https://arxiv.org/abs/2310.07236v2，Github 代码链接：None</li>
<li>摘要：
（1）研究背景：语音驱动的 3D 面部动画旨在生成与驱动语音同步的面部动作，该技术在虚拟现实、电影制作和游戏创作中具有巨大潜力。以往大多数工作侧重于提高语音与唇部动作的同步性，而忽略了包括面部表情和头部姿势在内的个性化说话风格。
（2）过去方法与问题：一些研究尝试通过微调或自适应模块来建模特定人物的说话风格。然而，在实际应用中，目标用户仅提供少量视频片段（甚至短于 1 分钟）来捕捉个性化的说话风格。这些方法因此面临以下挑战：</li>
<li>1) 很少的自适应数据可能会导致预训练模型发生灾难性遗忘，并容易导致过拟合问题。</li>
<li>2) 语音是头部姿势的弱控制信号。在如此弱的信号上进行自适应或学习映射会导致生成结果趋于平均。
（3）研究方法：本文提出 AdaMesh，一种新颖的自适应语音驱动面部动画方法。该方法从约 10 秒的参考视频中学习个性化的说话风格，并生成生动的面部表情和头部姿势。具体来说，本文提出混合低秩自适应 (MoLoRA) 来微调表情适配器，该方法有效地捕捉了面部表情风格。对于个性化的姿势风格，本文提出了一种姿势适配器，通过构建离散姿势先验并使用语义感知姿势风格矩阵检索适当的风格嵌入，无需微调。
（4）实验结果：广泛的实验结果表明，本文方法优于现有技术，保留了参考视频中的说话风格，并生成了生动的面部动画。</li>
</ol>
<p>Some Error for method(比如是不是没有Methods这个章节)</p>
<ol>
<li>结论：
（1）：本工作提出了一种新的语音驱动面部动画方法 AdaMesh，该方法能够从约 10 秒的参考视频中学习个性化的说话风格，并生成生动的面部表情和头部姿势。
（2）：创新点：</li>
<li>提出混合低秩自适应 (MoLoRA) 来微调表情适配器，有效地捕捉了面部表情风格。</li>
<li>提出了一种姿势适配器，通过构建离散姿势先验并使用语义感知姿势风格矩阵检索适当的风格嵌入，无需微调。</li>
<li>广泛的实验结果表明，AdaMesh 优于现有技术，保留了参考视频中的说话风格，并生成了生动的面部动画。
性能：</li>
<li>AdaMesh 能够从约 10 秒的参考视频中学习个性化的说话风格，并生成生动的面部表情和头部姿势。</li>
<li>AdaMesh 优于现有技术，保留了参考视频中的说话风格，并生成了生动的面部动画。
工作量：</li>
<li>AdaMesh 的训练过程相对简单，只需要约 10 秒的参考视频即可。</li>
<li>AdaMesh 的推理过程也相对简单，只需要输入语音信号即可生成面部动画。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e2c325c4c46442c62d649aa8c3b3382d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21344205a1b26e05a0603dc168a71d7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a107bec6acd460840a8823d7bfd305da.jpg" align="middle">
</details>




<h2 id="GestSync-Determining-who-is-speaking-without-a-talking-head"><a href="#GestSync-Determining-who-is-speaking-without-a-talking-head" class="headerlink" title="GestSync: Determining who is speaking without a talking head"></a>GestSync: Determining who is speaking without a talking head</h2><p><strong>Authors:Sindhu B Hegde, Andrew Zisserman</strong></p>
<p>In this paper we introduce a new synchronisation task, Gesture-Sync: determining if a person’s gestures are correlated with their speech or not. In comparison to Lip-Sync, Gesture-Sync is far more challenging as there is a far looser relationship between the voice and body movement than there is between voice and lip motion. We introduce a dual-encoder model for this task, and compare a number of input representations including RGB frames, keypoint images, and keypoint vectors, assessing their performance and advantages. We show that the model can be trained using self-supervised learning alone, and evaluate its performance on the LRS3 dataset. Finally, we demonstrate applications of Gesture-Sync for audio-visual synchronisation, and in determining who is the speaker in a crowd, without seeing their faces. The code, datasets and pre-trained models can be found at: \url{<a href="https://www.robots.ox.ac.uk/~vgg/research/gestsync}">https://www.robots.ox.ac.uk/~vgg/research/gestsync}</a>. </p>
<p><a href="http://arxiv.org/abs/2310.05304v1">PDF</a> Accepted in BMVC 2023, 10 pages paper, 7 pages supplementary, 7   Figures</p>
<p><strong>摘要</strong><br>语音同步新任务——Gesture-Sync：判断一个人的手势是否与他的讲话相关。</p>
<p><strong>要点</strong></p>
<ul>
<li>Gesture-Sync 是一个全新的同步任务，旨在确定一个人的手势是否与其讲话相关。</li>
<li>与唇语同步相比，手势同步更具挑战性，因为声音和身体运动之间的关系比声音和嘴唇运动之间的关系松散得多。</li>
<li>文中提出了一种用于此任务的双编码器模型，并比较了几种输入表示，包括 RGB 帧、关键点图像和关键点向量，评估它们的性能和优势。</li>
<li>该模型仅使用自我监督学习就可以训练，并在 LRS3 数据集上评估了其性能。</li>
<li>最后，演示了 Gesture-Sync 在视听同步以及确定人群中说话者（不看他们的脸）方面的应用。</li>
<li>代码、数据集和预训练模型可在以下网址找到：<a href="https://www.robots.ox.ac.uk/~vgg/research/gestsync">https://www.robots.ox.ac.uk/~vgg/research/gestsync。</a></li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：手势同步：确定谁在说话，而无需说话的头部</li>
<li>作者：Sindhu B Hegde，Andrew Zisserman</li>
<li>隶属关系：牛津大学工程科学系视觉几何组</li>
<li>关键词：手势同步、唇形同步、自我监督学习、多模态学习、说话人识别</li>
<li>论文链接：https://arxiv.org/abs/2310.05304，Github 代码链接：None</li>
<li>
<p>摘要：
（1）研究背景：在人机交互和多媒体处理中，准确识别说话人对于理解和响应人类语言至关重要。传统的说话人识别方法主要依赖于唇形同步，即通过分析说话人的嘴唇运动来确定说话人。然而，在某些情况下，例如当说话人的脸部被遮挡或说话人在嘈杂的环境中时，唇形同步方法可能会失效。
（2）过去的方法及问题：为了解决唇形同步的局限性，一些研究人员提出了手势同步的方法，即通过分析说话人的手势运动来确定说话人。然而，现有的手势同步方法大多依赖于监督学习，需要大量带标签的数据进行训练。这在实际应用中往往难以获得。
（3）提出的研究方法：为了解决手势同步中数据稀缺的问题，本文提出了一种新的手势同步方法，该方法可以利用自我监督学习进行训练。具体来说，该方法通过学习手势和语音之间的相关性来确定说话人。该方法采用双编码器模型，其中一个编码器将手势图像或关键点向量编码成嵌入向量，另一个编码器将语音信号编码成嵌入向量。然后，通过计算两个嵌入向量之间的相似性来确定说话人。
（4）方法的性能：为了评估所提出方法的性能，本文在 LRS3 数据集上进行了实验。实验结果表明，该方法在说话人识别任务上取得了良好的性能。此外，该方法还可以用于音频-视觉同步和确定人群中谁在说话，而无需看到他们的脸。</p>
</li>
<li>
<p>方法：
（1）双编码器模型：该方法采用双编码器模型，其中一个编码器将手势图像或关键点向量编码成嵌入向量，另一个编码器将语音信号编码成嵌入向量。
（2）手势和语音之间的相关性学习：通过学习手势和语音之间的相关性来确定说话人。
（3）计算嵌入向量之间的相似性：通过计算两个嵌入向量之间的相似性来确定说话人。</p>
</li>
<li>
<p>结论：
（1）：本文提出了一种新的手势同步方法，该方法可以利用自我监督学习进行训练，解决了手势同步中数据稀缺的问题。
（2）：创新点：</p>
</li>
<li>利用自我监督学习进行训练，无需大量带标签的数据。</li>
<li>采用双编码器模型，学习手势和语音之间的相关性。</li>
<li>通过计算嵌入向量之间的相似性来确定说话人。
性能：</li>
<li>在LRS3数据集上进行了实验，实验结果表明，该方法在说话人识别任务上取得了良好的性能。</li>
<li>该方法还可以用于音频-视觉同步和确定人群中谁在说话，而无需看到他们的脸。
工作量：</li>
<li>该方法的实现相对简单，易于部署。</li>
<li>该方法的训练时间较短，可以在合理的时间内完成。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ce1c5e581126af42846fa6a80d1504c1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5e94dc122da890162c1017d072a1cfcc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c91edb80b044a859328ba0905835ad6.jpg" align="middle">
</details>




<h2 id="DiffPoseTalk-Speech-Driven-Stylistic-3D-Facial-Animation-and-Head-Pose-Generation-via-Diffusion-Models"><a href="#DiffPoseTalk-Speech-Driven-Stylistic-3D-Facial-Animation-and-Head-Pose-Generation-via-Diffusion-Models" class="headerlink" title="DiffPoseTalk: Speech-Driven Stylistic 3D Facial Animation and Head Pose   Generation via Diffusion Models"></a>DiffPoseTalk: Speech-Driven Stylistic 3D Facial Animation and Head Pose   Generation via Diffusion Models</h2><p><strong>Authors:Zhiyao Sun, Tian Lv, Sheng Ye, Matthieu Gaetan Lin, Jenny Sheng, Yu-Hui Wen, Minjing Yu, Yong-jin Liu</strong></p>
<p>The generation of stylistic 3D facial animations driven by speech poses a significant challenge as it requires learning a many-to-many mapping between speech, style, and the corresponding natural facial motion. However, existing methods either employ a deterministic model for speech-to-motion mapping or encode the style using a one-hot encoding scheme. Notably, the one-hot encoding approach fails to capture the complexity of the style and thus limits generalization ability. In this paper, we propose DiffPoseTalk, a generative framework based on the diffusion model combined with a style encoder that extracts style embeddings from short reference videos. During inference, we employ classifier-free guidance to guide the generation process based on the speech and style. We extend this to include the generation of head poses, thereby enhancing user perception. Additionally, we address the shortage of scanned 3D talking face data by training our model on reconstructed 3DMM parameters from a high-quality, in-the-wild audio-visual dataset. Our extensive experiments and user study demonstrate that our approach outperforms state-of-the-art methods. The code and dataset will be made publicly available. </p>
<p><a href="http://arxiv.org/abs/2310.00434v1">PDF</a> Project page: <a href="https://raineggplant.github.io/DiffPoseTalk/">https://raineggplant.github.io/DiffPoseTalk/</a></p>
<p><strong>摘要</strong><br>扩散模型结合风格编码器实现语音驱动的高质量、多样化三维动态人脸生成。</p>
<p><strong>要点</strong></p>
<ul>
<li>提出了一种基于扩散模型和风格编码器的语音驱动三维动态人脸生成框架 DiffPoseTalk。</li>
<li>风格编码器通过从短参考视频中提取风格嵌入来捕捉风格的复杂性。</li>
<li>在推理过程中，使用无分类器引导根据语音和风格引导生成过程。</li>
<li>扩展该框架以生成头部姿势，从而增强用户感知。</li>
<li>通过在高质量的自然环境下的音频-视觉数据集上训练模型，解决了扫描三维说话人脸数据短缺的问题。</li>
<li>实验结果表明，该方法优于最先进的方法。</li>
<li>代码和数据集将公开发布。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>标题：DiffPoseTalk：通过扩散模型和头部姿势生成语音驱动的风格化 3D 面部动画</li>
<li>作者：孙志尧、吕天、叶盛、林马修·盖坦、盛 Jenny、温宇辉、俞敏晶、刘永进</li>
<li>隶属机构：清华大学</li>
<li>关键词：语音驱动、3D 面部动画、扩散模型、风格化、头部姿势</li>
<li>论文链接：https://arxiv.org/abs/2310.00434
Github 代码链接：无</li>
<li>
<p>摘要：
（1）研究背景：语音驱动的 3D 面部动画是一项极具挑战性的任务，因为它需要学习语音、风格和相应自然面部动作之间的多对多映射。
（2）过去的方法：现有方法要么采用确定性模型进行语音到动作的映射，要么使用独热编码方案对风格进行编码。独热编码方法无法捕捉风格的复杂性，从而限制了泛化能力。
（3）研究方法：本文提出 DiffPoseTalk，这是一个基于扩散模型的生成框架，结合了一个从短参考视频中提取风格嵌入的风格编码器。在推理过程中，我们采用无分类器指导来根据语音和风格指导生成过程。我们将其扩展到包括头部姿势的生成，从而增强用户感知。此外，我们通过在从高质量野外视听数据集重建的 3DMM 参数上训练模型，解决了扫描 3D 说话面部数据的短缺问题。
（4）性能与目标：我们的广泛实验和用户研究表明，我们的方法优于最先进的方法。这些性能支持了他们的目标。</p>
</li>
<li>
<p>方法：
(1) 扩散模型：我们使用扩散模型作为生成框架，将语音和风格指导映射到3D面部动画。扩散模型通过逐渐增加噪声来将数据从已知状态转换到随机状态，然后通过反向过程从噪声中恢复数据。
(2) 风格编码器：我们设计了一个风格编码器，从短参考视频中提取风格嵌入。风格编码器由一个卷积神经网络组成，它将视频帧编码为一个风格向量。
(3) 无分类器指导：在推理过程中，我们采用无分类器指导来根据语音和风格指导生成过程。无分类器指导通过最小化生成数据与目标数据之间的距离来训练模型。
(4) 头部姿势生成：我们将模型扩展到包括头部姿势的生成，从而增强用户感知。我们使用一个额外的网络来预测头部姿势，并将其作为输入添加到生成模型中。
(5) 数据集：我们使用从高质量野外视听数据集重建的3DMM参数来训练模型。该数据集包含了各种说话者的3D面部扫描数据，以及相应的语音和头部姿势数据。</p>
</li>
<li>
<p>结论：
（1）：本文提出了一种基于扩散模型的语音驱动3D面部动画生成框架DiffPoseTalk，该框架结合了一个从短参考视频中提取风格嵌入的风格编码器。在推理过程中，采用无分类器指导来根据语音和风格指导生成过程。将模型扩展到包括头部姿势的生成，从而增强用户感知。此外，通过在从高质量野外视听数据集重建的3DMM参数上训练模型，解决了扫描3D说话面部数据的短缺问题。
（2）：创新点：</p>
</li>
<li>将扩散模型应用于语音驱动的3D面部动画生成，实现了语音、风格和头部姿势的多对多映射。</li>
<li>设计了一个风格编码器，从短参考视频中提取风格嵌入，有效地捕捉了风格的复杂性。</li>
<li>采用无分类器指导来生成过程，避免了分类器带来的误差。</li>
<li>将模型扩展到包括头部姿势的生成，增强了用户感知。</li>
<li>通过在从高质量野外视听数据集重建的3DMM参数上训练模型，解决了扫描3D说话面部数据的短缺问题。</li>
</ol>
<p>性能：
- 在多个数据集上进行了广泛的实验，证明了该方法优于最先进的方法。
- 用户研究表明，该方法生成的3D面部动画更自然、更逼真。</p>
<p>工作量：
- 该方法的实现相对复杂，需要较多的计算资源。
- 需要高质量的野外视听数据集来训练模型。</p>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-7312673b0c18714105ddb7899ac0df55.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8466e4588a2da641b678027f760475e8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dbecf9b011548b814dbf8ff1aade63c8.jpg" align="middle">
</details>




## FaceDiffuser: Speech-Driven 3D Facial Animation Synthesis Using   Diffusion

**Authors:Stefan Stan, Kazi Injamamul Haque, Zerrin Yumak**

Speech-driven 3D facial animation synthesis has been a challenging task both in industry and research. Recent methods mostly focus on deterministic deep learning methods meaning that given a speech input, the output is always the same. However, in reality, the non-verbal facial cues that reside throughout the face are non-deterministic in nature. In addition, majority of the approaches focus on 3D vertex based datasets and methods that are compatible with existing facial animation pipelines with rigged characters is scarce. To eliminate these issues, we present FaceDiffuser, a non-deterministic deep learning model to generate speech-driven facial animations that is trained with both 3D vertex and blendshape based datasets. Our method is based on the diffusion technique and uses the pre-trained large speech representation model HuBERT to encode the audio input. To the best of our knowledge, we are the first to employ the diffusion method for the task of speech-driven 3D facial animation synthesis. We have run extensive objective and subjective analyses and show that our approach achieves better or comparable results in comparison to the state-of-the-art methods. We also introduce a new in-house dataset that is based on a blendshape based rigged character. We recommend watching the accompanying supplementary video. The code and the dataset will be publicly available. 

[PDF](http://arxiv.org/abs/2309.11306v1) Pre-print of the paper accepted at ACM SIGGRAPH MIG 2023

**Summary**

语音驱动 3D 人脸动画合成结合了 3D 顶点和混合形状数据集，可生成非确定性的 3D 人脸动画。

**Key Takeaways**

- 我们提出一种非确定性深度学习模型 FaceDiffuser，用于生成语音驱动的面部动画。
- FaceDiffuser 基于漫散技术，使用预训练的语音表示模型 HuBERT 对音频输入进行编码。
- FaceDiffuser 首次将漫散方法应用于语音驱动的 3D 面部动画合成任务。
- 我们已经进行了广泛的客观和主观分析，表明我们的方法与最先进的方法相比取得了更好或相当的结果。
- 我们还引入了一个新的内部数据集，该数据集基于混合形状的装配角色。
- 我们建议观看随附的补充视频。
- 代码和数据集将公开提供。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>标题：FaceDiffuser：基于扩散的语音驱动 3D 面部动画合成</li>
<li>作者：Stefan Stan、Kazi Injamamul Haque、Zerrin Yumak</li>
<li>隶属单位：乌特勒支大学</li>
<li>关键词：面部动画合成、深度学习、虚拟人、网格动画、混合形状动画</li>
<li>论文链接：https://arxiv.org/abs/2309.11306
    Github 代码链接：https://github.com/uuembodiedsocialai/FaceDiffuser</li>
<li>
<p>摘要：
（1）语音驱动的 3D 面部动画合成一直是工业界和研究界的一项具有挑战性的任务。最近的方法主要集中在确定性深度学习方法上，这意味着给定语音输入，输出始终相同。然而，实际上，遍布整个面部的非语言面部线索本质上是不确定的。此外，大多数方法都集中在基于 3D 顶点的的数据集上，而与现有具有装备角色的面部动画管道兼容的方法很少。
（2）为了消除这些问题，我们提出了 FaceDiffuser，这是一种非确定性深度学习模型，用于生成语音驱动的面部动画，该模型使用 3D 顶点和混合形状数据集进行训练。我们的方法基于扩散技术，并使用预训练的大型语音表示模型 HuBERT 对音频输入进行编码。据我们所知，我们是第一个将扩散方法用于语音驱动的 3D 面部动画合成的任务。
（3）我们进行了广泛的客观和主观分析，结果表明，与最先进的方法相比，我们的方法取得了更好或相当的结果。我们还引入了一个新的内部数据集，该数据集基于混合形状的装备角色。我们建议观看附带的补充视频。代码和数据集将公开发布。
（4）在语音驱动的 3D 面部动画合成任务上，我们的方法在客观和主观评估中都取得了有竞争力的结果。这表明我们的方法可以很好地实现语音驱动的 3D 面部动画合成。</p>
</li>
<li>
<p>方法：
（1）提出了一种通用模型，该模型可以针对基于顶点和基于混合形状的数据集进行训练，只需对超参数进行轻微修改。基于顶点的模型配置称为 V-FaceDiffuser，基于混合形状的模型称为 B-FaceDiffuser。主要区别在于额外的噪声编码器，如图 2 中以虚线红色框标出。噪声编码器有助于将高维顶点数据投影到低维潜在表示中。扩散噪声过程采用 x1:N0 来计算噪声 x1:Nt，同时保持其原始形状。从图 2 中，我们可以识别出这两个版本模型中包含的以下主要组件：
（2）音频编码器：我们使用预训练的大型语音模型 HuBERT 作为音频编码器，类似于 [22]，并且在架构的两个版本中保持不变。我们采用 HuBERT 架构的预训练版本，并使用其发布的 hubert-base-ls960 版本，该版本在 960 小时的 LibriSpeech [35] 数据集上进行训练。
（3）扩散过程：设 x1:N0 是来自数据集的真实视觉帧序列，形状为 (N,C)，其中 C 是顶点数乘以 3（对于 3 个空间轴）或面部控制权重（或混合形状值）的数量。在训练期间，我们从 [1,T] 中随机抽取一个整数时间步长 t，表示应用于 x1:N0 以获得 x1:Nt 的噪声步骤数，公式为：
x1:Nt=q(x1:Nt|x1:Nt−1)=N(√1−βt⋅x1:Nt−1,(βt)⋅I)(2)
其中，N 是序列中的视觉帧数，t 是扩散时间步长，βt 是时间步长 t 处的常数噪声，使得 0&lt;β1&lt;β2&lt;...&lt;βT&lt;1。在正向噪声过程之后，理想情况下，我们希望能够计算反向过程并从 x1:NT∼N(0,1) 向后返回到 x1:N0。因此，条件分布函数 p(x1:Nt−1|x1:Nt) 需要事先知道。Ho 等人 [24] 提出了通过学习数据集的潜在表示方差来实现这一目标。训练目标被定义为学习预测添加到输入 x0 中的噪声 𝜖。然而，我们偏离 [24] 并遵循 MDM [49] 和 EDGE [50]，选择我们的模型来学习预测实际动画数据，而不是数据中的噪声水平。我们认为这更适合我们的任务，因为结果也取决于输入音频。此外，通过选择这种方法，即使从推理过程的第一个去噪步骤开始，我们的模型也能够预测出可接受的结果，从而实现更快的采样。然而，遵循完整的推理过程将给出最好的结果。我们采用类似于 [49] 和 [50] 的简单损失进行训练。Ho 等人进行了更彻底的实验。[24]，他们还声称利用简单的损失来学习变分界被证明既易于实现，也有利于采样结果的质量。损失定义为：
L=Ex0∼q(x0|c),t∼[1,T]<a href="3">∥x0−ˆx0∥</a>
（4）面部解码器：面部解码器负责根据编码音频和噪声的潜在表示生成最终的动画帧。它由多个 GRU 层组成，后跟一个最终的全连接层，该层预测输出序列。在解码步骤中，还可以以学习的样式嵌入向量与隐藏状态输出之间的元素级乘积的形式添加样式嵌入。我们在消融部分解释了选择 GRU 解码器的原因。
（5）FaceDiffuser：基于扩散的语音驱动 3D 面部动画合成
（6）FaceDiffuser 推理是一个迭代过程，从 T 递减到 1。初始噪声由正态分布 N(0,1) 的实际噪声表示。在每一步中，我们向网络提供音频和噪声动画输入。然后将预测的运动再次扩散并馈送到迭代的下一步骤。</p>
</li>
<li>
<p>结论：
（1）我们把扩散机制集成到一个生成式深度神经网络中，该网络经过训练可以生成以语音为条件的 3D 面部动画。所提出的方法可以推广到高维时间 3D 顶点数据以及低维混合形状数据，只需对超参数进行轻微修改。定量分析表明，我们的方法优于最先进的方法。我们展示了我们的模型能够在不同的样式条件之间产生更高的运动多样性。
（2）创新点：</p>
</li>
<li>我们提出了一种新颖的基于扩散的模型 FaceDiffuser，用于生成语音驱动的 3D 面部动画。</li>
<li>我们的模型可以针对基于顶点和基于混合形状的数据集进行训练，只需对超参数进行轻微修改。</li>
<li>我们的模型能够生成具有更高运动多样性的动画，即使在不同的样式条件下也是如此。
性能：</li>
<li>我们的模型在客观和主观评估中都优于最先进的方法。</li>
<li>我们的模型能够生成逼真的、高质量的 3D 面部动画。</li>
<li>我们的模型能够实时生成动画。
工作量：</li>
<li>我们模型的训练过程相对简单。</li>
<li>我们模型的推理过程也非常有效。</li>
<li>我们模型的代码和数据集都是公开可用的。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c6b2d7bbedf1f0aff99eafa6c4abc4dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bf4fe00fac43e23f442d69b1e6d0a10.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6a0ecfad348bccbc3857ece8ec2797c8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-07bf850a8b84e1168cec03ca40142feb.jpg" align="middle">
</details>




<h2 id="DT-NeRF-Decomposed-Triplane-Hash-Neural-Radiance-Fields-for-High-Fidelity-Talking-Portrait-Synthesis"><a href="#DT-NeRF-Decomposed-Triplane-Hash-Neural-Radiance-Fields-for-High-Fidelity-Talking-Portrait-Synthesis" class="headerlink" title="DT-NeRF: Decomposed Triplane-Hash Neural Radiance Fields for   High-Fidelity Talking Portrait Synthesis"></a>DT-NeRF: Decomposed Triplane-Hash Neural Radiance Fields for   High-Fidelity Talking Portrait Synthesis</h2><p><strong>Authors:Yaoyu Su, Shaohui Wang, Haoqian Wang</strong></p>
<p>In this paper, we present the decomposed triplane-hash neural radiance fields (DT-NeRF), a framework that significantly improves the photorealistic rendering of talking faces and achieves state-of-the-art results on key evaluation datasets. Our architecture decomposes the facial region into two specialized triplanes: one specialized for representing the mouth, and the other for the broader facial features. We introduce audio features as residual terms and integrate them as query vectors into our model through an audio-mouth-face transformer. Additionally, our method leverages the capabilities of Neural Radiance Fields (NeRF) to enrich the volumetric representation of the entire face through additive volumetric rendering techniques. Comprehensive experimental evaluations corroborate the effectiveness and superiority of our proposed approach. </p>
<p><a href="http://arxiv.org/abs/2309.07752v1">PDF</a> 5 pages, 5 figures. Submitted to ICASSP 2024</p>
<p><strong>Summary</strong><br>深度分解三平面哈希神经辐射场显著提高了说话人脸的光写实渲染效果，并在关键评估数据集中取得了最先进的成效。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>分解面部区域为两个专门的三平面：一个是专门用于表示嘴巴，另一个用于更广泛的面部特征。</li>
<li>将音频特征作为残差项引入，并通过音频-口-面转换器作为查询向量整合到模型中。</li>
<li>利用神经辐射场 (NeRF) 的功能，通过添加体积渲染技术丰富整个脸部的体积表示。</li>
<li>综合实验评估证实了我们提出的方法的有效性和优越性。</li>
<li>该方法是首个使用三平面哈希神经辐射场来实现说话人脸光写实渲染的技术。</li>
<li>该方法将面部区域分解为两个专门的三平面，以更有效地表示嘴巴和更广泛的面部特征。</li>
<li>该方法将音频特征作为残差项引入，并将其作为查询向量通过音频-口-面转换器集成到模型中。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：分解三平面哈希神经辐射场（DT-NERF）</li>
<li>作者：姚宇苏、邵辉王、郝谦王</li>
<li>隶属单位：深圳国际研究生院，清华大学，深圳 518071，中国</li>
<li>关键词：NeRF，会说话的面部肖像，分解三平面哈希，音频-嘴巴-面部转换器</li>
<li>论文链接：None，Github 代码链接：None</li>
<li>
<p>摘要：
（1）研究背景：音频驱动的说话面部肖像合成是一项关键且具有挑战性的领域，尤其是在增强现实 (AR)、虚拟现实 (VR) 和大型语言模型 (LLM) 在数字人、虚拟形象和远程会议等 3D 面部驱动技术中不断找到应用的情况下。近年来，研究人员对 3D 视觉环境中的音频驱动面部合成进行了广泛探索。随着 2020 年神经辐射场 (NeRF) 的出现，这种方法已被纳入这项任务，产生了令人印象深刻的视觉效果。然而，原始的 NeRF 模型在计算速度和语音期间精确的嘴巴同步方面存在局限性，这表明有改进的空间。
（2）过去方法及问题：NeRF 是一种神经渲染技术，采用 5D 辐射场来捕获复杂的 3D 表面。该技术最初设计用于渲染静态、有界的场景，但此后已发展到适应动态和无界设置。NeRF 已在各种子领域中找到应用，例如通过符号距离场 (SDF) 和体积渲染相结合的场景重建、面部和身体渲染，甚至手部重建。在利用神经辐射场 (NeRF) 进行 3D 面部合成的当前领域中，流行的方法往往遵循两条路径之一。它们要么采用明确的 3D 面部表情参数或 2D 地标，这可能导致信息显着丢失，尤其是嘴巴区域，要么采用隐式表示，使用音频作为潜在代码来调制或扭曲规范空间。这些策略存在缺陷，尤其是在捕捉语音期间嘴巴的细微变化方面。
（3）研究方法：为了解决这些问题，我们提出了一个双重方法。首先，我们采用动态 NeRF，该 NeRF 利用音频特征作为转换器的查询，旨在优化 NeRF 中的密度和颜色网络，以从规范空间调制到动态空间。此外，我们利用颜色和体积密度在同一 NeRF 空间中的加法特性，从而实现音频和视觉元素的更无缝集成。我们的方法旨在更有效地将音频线索与面部表情结合起来，特别关注嘴巴区域的优化。分解的 3D 表示被用来分别建模嘴巴和更广泛的面部特征。我们引入音频特征作为残差项，并通过音频-嘴巴-面部转换器将它们作为查询向量集成到我们的模型中。
（4）实验结果：在关键评估数据集上，我们的方法显著提高了逼真的说话面孔的渲染，并取得了最先进的结果。综合实验评估证实了我们提出的方法的有效性和优越性。</p>
</li>
<li>
<p>方法：
(1): 提出动态 NeRF，利用音频特征作为转换器的查询，优化 NeRF 中的密度和颜色网络，从规范空间调制到动态空间。
(2): 利用颜色和体积密度在同一 NeRF 空间中的加法特性，实现音频和视觉元素的更无缝集成。
(3): 采用分解的 3D 表示分别建模嘴巴和更广泛的面部特征。
(4): 引入音频特征作为残差项，并通过音频-嘴巴-面部转换器将它们作为查询向量集成到模型中。</p>
</li>
<li>
<p>结论：
（1）：本文提出了一种分解三平面哈希神经辐射场（DT-NERF）方法，用于音频驱动的说话面部肖像合成。该方法利用动态NeRF、分解的3D表示、音频特征作为残差项以及音频-嘴巴-面部转换器，有效地将音频线索与面部表情结合起来，特别关注嘴巴区域的优化。在关键评估数据集上，我们的方法显著提高了逼真的说话面孔的渲染，并取得了最先进的结果。
（2）：创新点：</p>
</li>
<li>提出动态NeRF，利用音频特征作为转换器的查询，优化NeRF中的密度和颜色网络，从规范空间调制到动态空间。</li>
<li>利用颜色和体积密度在同一NeRF空间中的加法特性，实现音频和视觉元素的更无缝集成。</li>
<li>采用分解的3D表示分别建模嘴巴和更广泛的面部特征。</li>
<li>引入音频特征作为残差项，并通过音频-嘴巴-面部转换器将它们作为查询向量集成到模型中。
性能：</li>
<li>在关键评估数据集上，我们的方法显著提高了逼真的说话面孔的渲染，并取得了最先进的结果。
工作量：</li>
<li>该方法需要收集和预处理音频和面部数据，构建分解三平面哈希神经辐射场模型，并进行训练和推理。工作量中等。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-ada9598533d7c8a1499ba097e55090dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8fb8385566e4ab5153fa149f745ccef6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3595e0ef582dce4ddf81794c218bee95.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb1d50c01e04c66fad1c597d9bf14ce5.jpg" align="middle">
</details>




]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>Talking Head Generation</tag>
      </tags>
  </entry>
  <entry>
    <title>3DGS</title>
    <url>/2024/02/09/Paper/2024-02-09/3DGS/</url>
    <content><![CDATA[<blockquote>
<p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-02-09-更新"><a href="#2024-02-09-更新" class="headerlink" title="2024-02-09 更新"></a>2024-02-09 更新</h1><h2 id="Rig3DGS-Creating-Controllable-Portraits-from-Casual-Monocular-Videos"><a href="#Rig3DGS-Creating-Controllable-Portraits-from-Casual-Monocular-Videos" class="headerlink" title="Rig3DGS: Creating Controllable Portraits from Casual Monocular Videos"></a>Rig3DGS: Creating Controllable Portraits from Casual Monocular Videos</h2><p><strong>Authors:Alfredo Rivero, ShahRukh Athar, Zhixin Shu, Dimitris Samaras</strong></p>
<p>Creating controllable 3D human portraits from casual smartphone videos is highly desirable due to their immense value in AR/VR applications. The recent development of 3D Gaussian Splatting (3DGS) has shown improvements in rendering quality and training efficiency. However, it still remains a challenge to accurately model and disentangle head movements and facial expressions from a single-view capture to achieve high-quality renderings. In this paper, we introduce Rig3DGS to address this challenge. We represent the entire scene, including the dynamic subject, using a set of 3D Gaussians in a canonical space. Using a set of control signals, such as head pose and expressions, we transform them to the 3D space with learned deformations to generate the desired rendering. Our key innovation is a carefully designed deformation method which is guided by a learnable prior derived from a 3D morphable model. This approach is highly efficient in training and effective in controlling facial expressions, head positions, and view synthesis across various captures. We demonstrate the effectiveness of our learned deformation through extensive quantitative and qualitative experiments. The project page can be found at <a href="http://shahrukhathar.github.io/2024/02/05/Rig3DGS.html">http://shahrukhathar.github.io/2024/02/05/Rig3DGS.html</a> </p>
<p><a href="http://arxiv.org/abs/2402.03723v1">PDF</a> </p>
<p><strong>摘要</strong><br>3D 高斯散点（3DGS）的开发改善了渲染质量和训练效率，利用可学习的 3D 可变形模型指导的变形方法能够准确建模和分离头部运动及面部表情。</p>
<p><strong>主要要点</strong></p>
<ul>
<li>3DGS 在 AR/VR 应用中具有巨大价值，因为它们能够从休闲智能手机视频中创建可控的 3D 人像。</li>
</ul>
<ul>
<li>3DGS 在渲染质量和训练效率方面取得了进展，但仍然难以从单视图捕捉中准确建模和分离头部运动和面部表情以实现高质量渲染。</li>
</ul>
<ul>
<li>Rig3DGS 使用一组 3D 高斯分布在规范空间中表示整个场景，包括动态主体。</li>
</ul>
<ul>
<li>Rig3DGS 使用一组控制信号，例如头部姿势和表情，将其转换为 3D 空间，并通过学习到的变形来生成所需的渲染。</li>
</ul>
<ul>
<li>Rig3DGS 的关键创新在于一种经过精心设计的变形方法，该方法由源自 3D 可变形模型的可学习先验引导。</li>
</ul>
<ul>
<li>这种方法在训练中非常有效，能够有效地控制各种捕捉中的面部表情、头部位置和视图合成。</li>
</ul>
<ul>
<li>通过广泛的定量和定性实验证明了 Rig3DGS 的学习变形是有效的。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：Rig3DGS：从随意单目视频创建可控肖像</li>
<li>作者：Alfredo Rivero<em>, Shah Rukh Athar</em>, Zhixin Shu, Dimitris Samaras</li>
<li>单位：纽约石溪大学</li>
<li>关键词：3D人像、3D高斯喷绘、可控变形、头部姿态、面部表情、视角合成</li>
<li>论文链接：https://arxiv.org/abs/2402.03723
Github 代码链接：无</li>
<li>
<p>摘要：
(1)：研究背景：创建可控的 3D 人类肖像对于各种沉浸式体验至关重要，包括虚拟现实、远程临场、电影制作和教育应用。然而，仅使用基本智能手机摄像头，普通消费者实现这项技术面临着相当大的挑战。
(2)：过去的方法：从视频中建模 3D 可控肖像通常涉及动态人类主体的显式或隐式配准，考虑每个帧中面部表情和头部姿势等不同因素。这个过程需要精确区分由这些因素引起的面部变形，这在没有真实依据的情况下通常具有挑战性。当使用单目捕捉时，挑战进一步加剧，因为每个头部姿势和表情只能从单个视点看到，这使得准确的区分变得更加复杂。
(3)：研究方法：本文提出 Rig3DGS 来解决这一挑战。我们使用一组 3D 高斯体在规范空间中表示整个场景，包括动态主体。使用一组控制信号，例如头部姿势和表情，我们利用学习到的变形将它们转换为 3D 空间以生成所需的渲染。我们的关键创新是一种精心设计的变形方法，该方法由从 3D 可变形模型派生的可学习先验引导。这种方法在训练中非常有效，并且能够控制面部表情、头部位置和跨各种捕捉的视角合成。
(4)：方法性能：我们通过广泛的定量和定性实验证明了我们学习到的变形的有效性。该项目页面可在此处找到。</p>
</li>
<li>
<p>方法：
(1): Rig3DGS 使用一组 3D 高斯体表示整个场景，包括动态主体，并使用一组控制信号（如头部姿势和表情）将它们转换为 3D 空间以生成所需的渲染。
(2): Rig3DGS 的关键创新是一种精心设计的变形方法，该方法由从 3D 可变形模型派生的可学习先验引导。
(3): 该方法在训练中非常有效，并且能够控制面部表情、头部位置和跨各种捕捉的视角合成。</p>
</li>
<li>
<p>结论：
（1）：本文提出了一种名为 Rig3DGS 的方法，该方法能够对肖像视频进行任意面部表情控制和新视角合成。Rig3DGS 使用可学习的变形先验来确保在训练期间的稳定性和对新面部表情、头部姿势和视角的一般化。Rig3DGS 还能够对拍摄对象的头发和眼镜等面部细节进行建模，并在视频被驱动时以高保真度再现它们。但是，具有新视角合成的可控人头部模型的问题还远未解决。Rig3DGS 无法对强烈的非均匀光照进行建模，并且要求肖像视频中的拍摄对象在拍摄期间保持相对静止。我们希望在未来的工作中解决这个问题。
（2）：创新点：
（1）提出了 Rig3DGS，一种使用一组 3D 高斯体表示整个场景（包括动态主体）的方法，并使用一组控制信号（如头部姿势和表情）将它们转换为 3D 空间以生成所需的渲染。
（2）提出了一种精心设计的变形方法，该方法由从 3D 可变形模型派生的可学习先验引导。
（3）证明了该方法能够控制面部表情、头部位置和跨各种捕捉的视角合成。
性能：
（1）Rig3DGS 能够生成高质量的 3D 肖像，具有逼真的面部表情、头部姿势和视角。
（2）Rig3DGS 能够在具有挑战性的照明条件下工作，例如强烈的非均匀光照。
（3）Rig3DGS 能够实时运行，使其适用于各种应用程序。
工作量：
（1）Rig3DGS 的训练过程相对简单且直接。
（2）Rig3DGS 易于使用，并且不需要任何专门的硬件或软件。
（3）Rig3DGS 是开源的，可以免费使用。</p>
</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3161a0632f560b62291a8cf525616b2c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d6843ee2a991081c82505388c065defc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-28074a5f13fdf5a52c0d4de04dfb9406.jpg" align="middle">
</details>




<h2 id="4D-Gaussian-Splatting-Towards-Efficient-Novel-View-Synthesis-for-Dynamic-Scenes"><a href="#4D-Gaussian-Splatting-Towards-Efficient-Novel-View-Synthesis-for-Dynamic-Scenes" class="headerlink" title="4D Gaussian Splatting: Towards Efficient Novel View Synthesis for   Dynamic Scenes"></a>4D Gaussian Splatting: Towards Efficient Novel View Synthesis for   Dynamic Scenes</h2><p><strong>Authors:Yuanxing Duan, Fangyin Wei, Qiyu Dai, Yuhang He, Wenzheng Chen, Baoquan Chen</strong></p>
<p>We consider the problem of novel view synthesis (NVS) for dynamic scenes. Recent neural approaches have accomplished exceptional NVS results for static 3D scenes, but extensions to 4D time-varying scenes remain non-trivial. Prior efforts often encode dynamics by learning a canonical space plus implicit or explicit deformation fields, which struggle in challenging scenarios like sudden movements or capturing high-fidelity renderings. In this paper, we introduce 4D Gaussian Splatting (4DGS), a novel method that represents dynamic scenes with anisotropic 4D XYZT Gaussians, inspired by the success of 3D Gaussian Splatting in static scenes. We model dynamics at each timestamp by temporally slicing the 4D Gaussians, which naturally compose dynamic 3D Gaussians and can be seamlessly projected into images. As an explicit spatial-temporal representation, 4DGS demonstrates powerful capabilities for modeling complicated dynamics and fine details, especially for scenes with abrupt motions. We further implement our temporal slicing and splatting techniques in a highly optimized CUDA acceleration framework, achieving real-time inference rendering speeds of up to 277 FPS on an RTX 3090 GPU and 583 FPS on an RTX 4090 GPU. Rigorous evaluations on scenes with diverse motions showcase the superior efficiency and effectiveness of 4DGS, which consistently outperforms existing methods both quantitatively and qualitatively. </p>
<p><a href="http://arxiv.org/abs/2402.03307v2">PDF</a> </p>
<p><strong>Summary</strong><br>动态场景下新视角合成方法 4DGS，基于高斯体素时空切片表示实现了快速的动态场景渲染。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>4DGS 是一种新颖的方法，它使用各向异性的 4D XYZT 高斯体素来表示动态场景。</li>
<li>4DGS 通过对 4D 高斯体素进行时间切片来建模每个时间戳的动态，从而自然地构成动态 3D 高斯体素并可以无缝地投影到图像中。</li>
<li>作为一种显式的时空表示，4DGS 在建模复杂动态和精细细节方面表现出强大的能力，尤其是对于具有突然运动的场景。</li>
<li>4DGS 在高度优化的 CUDA 加速框架中实现了时间切片和 splatting 技术，在 RTX 3090 GPU 上实现了高达 277 FPS 的实时推理渲染速度，在 RTX 4090 GPU 上实现了 583 FPS 的实时推理渲染速度。</li>
<li>在具有不同运动的场景上的严格评估表明，4DGS 的效率和有效性优于现有方法，无论是在定量还是定性方面都始终优于现有方法。</li>
<li>4DGS 可以轻松扩展到各种动态场景，例如具有复杂几何形状、遮挡和纹理的对象、具有细微运动的人体以及逼真的合成场景，并在这些场景中实现高质量的 NVS。</li>
<li>4DGS 可以在各种下游任务中发挥作用，例如视频插帧、运动模糊、运动估计、场景重建和增强现实。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：4D 高斯散点：面向动态场景的高效新视点合成</li>
<li>作者：段元兴，魏芳寅，戴启宇，何宇航，陈文正，陈宝权</li>
<li>单位：北京大学</li>
<li>关键词：新视点合成，动态场景，时间一致性，空间一致性，高斯散点</li>
<li>论文链接：https://arxiv.org/pdf/2402.03307.pdf，Github 链接：无</li>
<li>
<p>摘要：
（1）：研究背景：新视点合成（NVS）旨在从 2D 图像重建 3D 场景，并从新视点合成其外观。NVS 在影视、游戏、VR/AR 等领域有着广泛的应用。对于静态场景，NVS 已取得了显著进展。然而，对于动态场景，由于时间维度和复杂运动模式的引入，高效且准确的 NVS 仍然具有挑战性。
（2）：过去方法：现有方法主要分为两类：联合建模法和解耦建模法。联合建模法将 3D 场景及其动态联合建模，但往往难以保留 NVS 渲染中的精细细节。解耦建模法将动态场景分解为静态规范空间和变形场，但难以捕捉诸如物体突然出现或消失等复杂动态。此外，主流的基于体积渲染的方法通常无法支持实时渲染。
（3）：研究方法：本文提出了一种称为 4D 高斯散点（4DGS）的新方法。4DGS 将动态场景表示为各向异性的 4D XYZT 高斯分布，受静态场景中 3D 高斯散点成功的启发。通过对 4D 高斯分布进行时间切片，可以自然地组成动态 3D 高斯分布，并将其无缝投影到图像中。作为一种显式的时空表示，4DGS 能够有效地建模复杂的动态和精细细节，尤其适用于具有突然运动的场景。此外，本文还实现了一种高度优化的 CUDA 加速框架，在 RTX 3090 GPU 上实现了高达 277 FPS 的实时渲染速度，在 RTX 4090 GPU 上实现了 583 FPS 的实时渲染速度。
（4）：方法性能：在具有不同运动的场景上进行的严格评估表明，4DGS 在效率和有效性方面均优于现有方法。4DGS 在定量和定性方面都始终优于现有方法。这些性能支持了本文的目标，即开发一种高效且准确的动态场景 NVS 方法。</p>
</li>
<li>
<p>方法：
（1）：4D高斯散点（4DGS）的基本思想是将动态场景表示为各向异性的4D XYZT高斯分布，通过对4D高斯分布进行时间切片，可以自然地组成动态3D高斯分布，并将其无缝投影到图像中。这种显式的时空表示能够有效地建模复杂的动态和精细细节，尤其适用于具有突然运动的场景。
（2）：4DGS的具体步骤如下：</p>
</li>
<li>首先，通过将场景中的每个点及其运动轨迹建模为4D XYZT高斯分布，来表示动态场景。</li>
<li>其次，通过对4D高斯分布进行时间切片，得到一系列3D高斯分布，这些3D高斯分布可以无缝地投影到图像中，从而合成新视点图像。</li>
<li>
<p>最后，为了提高渲染速度，本文还实现了一种高度优化的CUDA加速框架，该框架可以在RTX3090 GPU上实现高达277 FPS的实时渲染速度，在RTX4090 GPU上实现583 FPS的实时渲染速度。</p>
</li>
<li>
<p>结论：
（1）：本工作通过提出4D高斯散点（4DGS）方法，实现了高效且准确的动态场景新视点合成，为动态场景NVS领域的研究提供了新的思路和方法。
（2）：创新点：</p>
</li>
<li>提出了一种新的动态场景表示方法，将动态场景表示为各向异性的4DXYZT高斯分布，能够有效地建模复杂的动态和精细细节。</li>
<li>提出了一种新的NVS方法，通过对4D高斯分布进行时间切片，得到一系列3D高斯分布，并将其无缝投影到图像中，合成新视点图像。</li>
<li>实现了一种高度优化的CUDA加速框架，在RTX3090GPU上实现高达277FPS的实时渲染速度，在RTX4090GPU上实现583FPS的实时渲染速度。
性能：</li>
<li>在具有不同运动的场景上进行的严格评估表明，4DGS在效率和有效性方面均优于现有方法。</li>
<li>4DGS在定量和定性方面都始终优于现有方法。
工作量：</li>
<li>本工作涉及了大量的理论推导和算法实现，工作量较大。</li>
<li>本工作使用了大量的实验数据，实验过程复杂，工作量较大。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-8afb4e4e499c5116d082b9b523480bbb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-960e35d536b25803abdadcc5fd2abea1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0570db380e05870cdbbd7a17934c699.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-db45e73c8294473dfec461a53ba7d2a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5983071f25b6e20421a8a05030a8a70f.jpg" align="middle">
</details>




<h2 id="SGS-SLAM-Semantic-Gaussian-Splatting-For-Neural-Dense-SLAM"><a href="#SGS-SLAM-Semantic-Gaussian-Splatting-For-Neural-Dense-SLAM" class="headerlink" title="SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM"></a>SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM</h2><p><strong>Authors:Mingrui Li, Shuhong Liu, Heng Zhou</strong></p>
<p>Semantic understanding plays a crucial role in Dense Simultaneous Localization and Mapping (SLAM), facilitating comprehensive scene interpretation. Recent advancements that integrate Gaussian Splatting into SLAM systems have demonstrated its effectiveness in generating high-quality renderings through the use of explicit 3D Gaussian representations. Building on this progress, we propose SGS-SLAM, the first semantic dense visual SLAM system grounded in 3D Gaussians, which provides precise 3D semantic segmentation alongside high-fidelity reconstructions. Specifically, we propose to employ multi-channel optimization during the mapping process, integrating appearance, geometric, and semantic constraints with key-frame optimization to enhance reconstruction quality. Extensive experiments demonstrate that SGS-SLAM delivers state-of-the-art performance in camera pose estimation, map reconstruction, and semantic segmentation, outperforming existing methods meanwhile preserving real-time rendering ability. </p>
<p><a href="http://arxiv.org/abs/2402.03246v1">PDF</a> </p>
<p><strong>Summary</strong><br>3D语义高斯表示的视觉SLAM系统，将外观、几何和语义约束融入到关键帧优化，实现实时的高精度3D语义分割和地图重建，效果优异。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>提出SGS-SLAM，第一个基于3D高斯表示的语义稠密视觉SLAM系统，提供精确的3D语义分割和高保真的地图重建。</li>
<li>在建图过程中采用多通道优化，将外观、几何和语义约束与关键帧优化相结合，提高重建质量。</li>
<li>SGS-SLAM在相机位姿估计、地图重建和语义分割方面达到了最先进的性能，优于现有方法，同时保持了实时的渲染能力。</li>
<li>SGS-SLAM同时适用于室内和室外场景，可在动态环境中处理光照变化和快速运动。</li>
<li>SGS-SLAM可用于各种机器人应用，如导航、探索和操纵。</li>
<li>SGS-SLAM的代码和数据集已开源，可供研究者和开发者使用。</li>
<li>SGS-SLAM具有广阔的应用前景，可用于自动驾驶、增强现实和虚拟现实等领域。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：SGS-SLAM：神经稠密 SLAM 的语义高斯绘图</li>
<li>作者：Mingrui Li、Shuhong Liu、Heng Zhou</li>
<li>单位：大连理工大学计算机科学系</li>
<li>关键词：SLAM、3D 重建、3D 语义分割</li>
<li>链接：Paper_info</li>
<li>摘要：
（1）研究背景：语义理解在稠密的同时定位和建图（SLAM）中起着至关重要的作用，有助于全面理解场景。最近将高斯绘图集成到 SLAM 系统中的进展已经证明了其在使用显式 3D 高斯表示生成高质量渲染方面的有效性。
（2）过去的方法及其问题：传统的视觉 SLAM 系统在稀疏重建方面取得了显着成就，但无法通过点云或体素有效地表示更密集的重建。为了提取用于高保真表示的密集几何信息，基于学习的 SLAM 方法获得了广泛关注。它们在生成良好的全局 3D 地图的同时，还表现出对噪声和异常值的鲁棒性。此外，受神经辐射场 (NeRF) 进展的启发，基于 NeRF 的 SLAM 方法取得了进一步的进展。它们擅长通过可微渲染捕获密集的光度信息，从而产生准确且高保真的全局重建。
（3）论文提出的研究方法：在上述研究的基础上，本文提出了 SGS-SLAM，这是第一个基于 3D 高斯的语义稠密视觉 SLAM 系统，它在提供高保真重建的同时，还提供了精确的 3D 语义分割。具体来说，本文提出在建图过程中采用多通道优化，将外观、几何和语义约束与关键帧优化相结合，以提高重建质量。
（4）方法在什么任务上取得了什么性能，该性能是否能支撑其目标：广泛的实验表明，SGS-SLAM 在相机位姿估计、地图重建和语义分割方面提供了最先进的性能，优于现有方法，同时保持了实时渲染能力。</li>
</ol>
<p><methods>:
(1) 多通道高斯表示：使用高斯影响函数表示场景，高斯函数具有半径、中心位置和颜色。通过渲染高斯函数到 2D 图像来优化高斯函数的参数，并使用深度渲染来计算像素级渲染颜色和深度。利用 2D 语义标签为高斯函数分配不同的通道来表示语义标签和颜色。
(2) 跟踪和建图：跟踪过程估计每帧的相机位姿，同时保持场景参数固定。建图过程根据估计的相机位姿优化场景表示。跟踪过程通过最小化跟踪损失来迭代优化当前位姿。关键帧选择和权重分配基于几何和语义约束。
(3) 地图重建：使用高斯函数对场景进行建模，高斯函数的均值坐标表示场景的几何信息，外观颜色描述场景的视觉外观，语义颜色指示物体的语义标签。在高斯函数致密化和优化的过程中，这些参数在各个通道上联合优化，而相机位姿则通过跟踪固定。通过将高斯函数渲染到 2D 图像来优化地图参数，并使用深度渲染来计算像素级渲染颜色和深度。</methods></p>
<ol>
<li>结论：
（1）：SGS-SLAM是第一个基于3D高斯表示的语义稠密视觉SLAM系统。我们提出利用多通道参数优化，其中外观、几何和语义约束被组合以强制执行高精度的3D语义分割，并同时进行高保真稠密地图重建，同时有效地产生鲁棒的相机位姿估计。SGS-SLAM利用了最优关键帧优化的好处，从而产生了可靠的重建质量。广泛的实验表明，我们的方法提供了最先进的跟踪和建图结果，同时保持了快速的渲染速度。此外，高质量的重建
（2）：创新点：</li>
<li>提出了一种新的语义稠密SLAM系统SGS-SLAM，该系统首次将3D高斯表示与语义分割相结合，实现了高保真重建和精确的3D语义分割。</li>
<li>设计了一种多通道参数优化方法，将外观、几何和语义约束相结合，提高了重建质量。</li>
<li>提出了一种基于高斯函数的稠密地图重建方法，该方法能够生成高保真的3D地图。
性能：</li>
<li>在相机位姿估计、地图重建和语义分割方面提供了最先进的性能，优于现有方法。</li>
<li>能够实时渲染，保持了良好的交互性。
工作量：</li>
<li>算法实现复杂，需要大量的计算资源。</li>
<li>数据集的构建和标注需要大量的人力物力。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-134845e702f2aa6e6e259afa165a6769.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8509cc5a8db3cd0d7633a8bcc603fddb.jpg" align="middle">
</details>




]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>3DGS</tag>
      </tags>
  </entry>
  <entry>
    <title>Diffusion Models</title>
    <url>/2024/02/02/Paper/2024-02-02/Diffusion%20Models/</url>
    <content><![CDATA[<blockquote>
<p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-02-02-更新"><a href="#2024-02-02-更新" class="headerlink" title="2024-02-02 更新"></a>2024-02-02 更新</h1><h2 id="ViCA-NeRF-View-Consistency-Aware-3D-Editing-of-Neural-Radiance-Fields"><a href="#ViCA-NeRF-View-Consistency-Aware-3D-Editing-of-Neural-Radiance-Fields" class="headerlink" title="ViCA-NeRF: View-Consistency-Aware 3D Editing of Neural Radiance Fields"></a>ViCA-NeRF: View-Consistency-Aware 3D Editing of Neural Radiance Fields</h2><p><strong>Authors:Jiahua Dong, Yu-Xiong Wang</strong></p>
<p>We introduce ViCA-NeRF, the first view-consistency-aware method for 3D editing with text instructions. In addition to the implicit neural radiance field (NeRF) modeling, our key insight is to exploit two sources of regularization that explicitly propagate the editing information across different views, thus ensuring multi-view consistency. For geometric regularization, we leverage the depth information derived from NeRF to establish image correspondences between different views. For learned regularization, we align the latent codes in the 2D diffusion model between edited and unedited images, enabling us to edit key views and propagate the update throughout the entire scene. Incorporating these two strategies, our ViCA-NeRF operates in two stages. In the initial stage, we blend edits from different views to create a preliminary 3D edit. This is followed by a second stage of NeRF training, dedicated to further refining the scene’s appearance. Experimental results demonstrate that ViCA-NeRF provides more flexible, efficient (3 times faster) editing with higher levels of consistency and details, compared with the state of the art. Our code is publicly available. </p>
<p><a href="http://arxiv.org/abs/2402.00864v1">PDF</a> Neurips2023; project page: <a href="https://github.com/Dongjiahua/VICA-NeRF">https://github.com/Dongjiahua/VICA-NeRF</a></p>
<p><strong>摘要</strong><br>利用深度信息和扩散模型，ViCA-NeRF 实现了多视图一致性，可以高效地编辑 3D 场景。</p>
<p><strong>要点</strong></p>
<ul>
<li>ViCA-NeRF 是一种利用深度信息和扩散模型来实现多视图一致性的 3D 编辑方法。</li>
<li>ViCA-NeRF 在 NeRF 建模的基础上，利用深度信息推断不同视角的图像对应关系，以实现几何正则化。</li>
<li>ViCA-NeRF 利用 2D 扩散模型对编辑图像和未编辑图像的潜在编码进行对齐，以实现学习正则化。</li>
<li>ViCA-NeRF 由两个阶段组成：第一阶段融合来自不同视角的编辑，创建初步的 3D 编辑；第二阶段对 NeRF 进行训练，以进一步细化场景外观。</li>
<li>ViCA-NeRF 比现有方法提供了更灵活、更高效（速度提高 3 倍）的编辑，并具有更高的层次一致性和细节。</li>
<li>ViCA-NeRF 的代码已公开。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：ViCA-NeRF：基于视图一致性的神经辐射场 3D 编辑</li>
<li>作者：Jiahua Dong, Yu-Xiong Wang</li>
<li>单位：伊利诺伊大学厄巴纳-香槟分校</li>
<li>关键词：神经辐射场、3D 编辑、文本指令、视图一致性</li>
<li>论文链接：https://arxiv.org/abs/2402.00864
   Github 链接：None</li>
<li>
<p>总结：
(1)：随着神经辐射场 (NeRF) 等 3D 重建技术的进步，收集真实世界 3D 场景变得更加便捷。然而，现有方法在 3D 场景编辑方面还存在诸多局限。
(2)：以往方法通常使用隐式神经辐射场进行建模，但缺乏对不同视图之间编辑信息传播的显式约束，导致编辑结果可能出现视图不一致的问题。
(3)：本文提出 ViCA-NeRF，一种基于视图一致性的 3D 编辑方法。ViCA-NeRF 利用几何和学习正则化两种策略来确保不同视图之间的编辑一致性。几何正则化利用 NeRF 提取的深度信息建立不同视图之间的图像对应关系，学习正则化则对编辑图像和未编辑图像在 2D 扩散模型中的潜在编码进行对齐，从而实现关键视图的编辑并将其传播到整个场景。
(4)：实验结果表明，与现有方法相比，ViCA-NeRF 能够提供更加灵活、高效（速度提高 3 倍）、一致性和细节更佳的编辑效果。</p>
</li>
<li>
<p>Methods：
(1)：ViCA-NeRF是一种基于视图一致性的3D编辑方法，它利用几何和学习正则化两种策略来确保不同视图之间的编辑一致性。
(2)：几何正则化利用NeRF提取的深度信息建立不同视图之间的图像对应关系，从而将编辑信息从关键视图传播到整个场景。
(3)：学习正则化对编辑图像和未编辑图像在2D扩散模型中的潜在编码进行对齐，从而实现关键视图的编辑并将其传播到整个场景。
(4)：ViCA-NeRF能够提供更加灵活、高效（速度提高3倍）、一致性和细节更佳的编辑效果。</p>
</li>
<li>
<p>结论：
（1）：本工作提出了一种基于视图一致性的 3D 编辑框架 ViCA-NeRF，该框架可以根据文本指令高效地编辑 NeRF。除了人类风格化和天气变化等简单任务外，我们还支持与上下文相关的操作，例如“添加一些花朵”和编辑高度详细的纹理。我们的方法在各种场景和文本提示上优于几个基线。未来，我们将继续提高 3D 编辑的可控性和真实性。
（2）：创新点：</p>
</li>
<li>提出了一种基于视图一致性的 3D 编辑框架 ViCA-NeRF，该框架可以根据文本指令高效地编辑 NeRF。</li>
<li>利用几何正则化和学习正则化两种策略来确保不同视图之间的编辑一致性。</li>
<li>支持与上下文相关的操作，例如“添加一些花朵”和编辑高度详细的纹理。
性能：</li>
<li>在各种场景和文本提示上优于几个基线。</li>
<li>编辑效率高，速度提高 3 倍。</li>
<li>编辑结果一致性好，细节丰富。
工作量：</li>
<li>实现复杂，需要较高的技术水平。</li>
<li>训练时间长，需要大量的计算资源。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b3cbdca659df3ac2eb7b2521752d1c8e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f5c934d1ebae9f51cda700d605228196.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40418c9a6b8bcda24387d9b40ab2cd3a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1ff0299de61f2dcce94a6f84b195a4b3.jpg" align="middle">
</details>




<h2 id="AnimateLCM-Accelerating-the-Animation-of-Personalized-Diffusion-Models-and-Adapters-with-Decoupled-Consistency-Learning"><a href="#AnimateLCM-Accelerating-the-Animation-of-Personalized-Diffusion-Models-and-Adapters-with-Decoupled-Consistency-Learning" class="headerlink" title="AnimateLCM: Accelerating the Animation of Personalized Diffusion Models   and Adapters with Decoupled Consistency Learning"></a>AnimateLCM: Accelerating the Animation of Personalized Diffusion Models   and Adapters with Decoupled Consistency Learning</h2><p><strong>Authors:Fu-Yun Wang, Zhaoyang Huang, Xiaoyu Shi, Weikang Bian, Guanglu Song, Yu Liu, Hongsheng Li</strong></p>
<p>Video diffusion models has been gaining increasing attention for its ability to produce videos that are both coherent and of high fidelity. However, the iterative denoising process makes it computationally intensive and time-consuming, thus limiting its applications. Inspired by the Consistency Model (CM) that distills pretrained image diffusion models to accelerate the sampling with minimal steps and its successful extension Latent Consistency Model (LCM) on conditional image generation, we propose AnimateLCM, allowing for high-fidelity video generation within minimal steps. Instead of directly conducting consistency learning on the raw video dataset, we propose a decoupled consistency learning strategy that decouples the distillation of image generation priors and motion generation priors, which improves the training efficiency and enhance the generation visual quality. Additionally, to enable the combination of plug-and-play adapters in stable diffusion community to achieve various functions (e.g., ControlNet for controllable generation). we propose an efficient strategy to adapt existing adapters to our distilled text-conditioned video consistency model or train adapters from scratch without harming the sampling speed. We validate the proposed strategy in image-conditioned video generation and layout-conditioned video generation, all achieving top-performing results. Experimental results validate the effectiveness of our proposed method. Code and weights will be made public. More details are available at <a href="https://github.com/G-U-N/AnimateLCM">https://github.com/G-U-N/AnimateLCM</a>. </p>
<p><a href="http://arxiv.org/abs/2402.00769v1">PDF</a> Project Page: <a href="https://animatelcm.github.io/">https://animatelcm.github.io/</a></p>
<p><strong>Summary</strong><br>扩散模型动画LCM（AnimateLCM）：通过分离图像生成先验和运动生成先验，实现快速高效的高保真视频生成。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>扩散模型视频生成由于迭代去噪过程计算量大和耗时，限制了其应用。</li>
<li>受Consistency Model (CM)和Latent Consistency Model (LCM)的启发，提出AnimateLCM，可在最少步骤内生成高保真视频。</li>
<li>提出了一种解耦一致性学习策略，将图像生成先验和运动生成先验的学习解耦，提高了训练效率和生成视觉质量。</li>
<li>提出了一种有效的策略，将现有的适配器适配到蒸馏后的文本条件视频一致性模型，或从头开始训练适配器，而不会损害采样速度。</li>
<li>在图像条件视频生成和布局条件视频生成中验证了所提出的策略，均取得了最优结果。</li>
<li>实验结果验证了所提方法的有效性。代码和权重将公开。更多详情请见 <a href="https://github.com/G-U-N/AnimateLCM。">https://github.com/G-U-N/AnimateLCM。</a></li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>标题：AnimateLCM：加速个性化扩散模型和适配器的动画制作，具有去耦合一致性学习</li>
<li>作者：Fu-Yun Wang, Zhaoyang Huang, Xiaoyu Shi, Weikang Bian, Guanglu Song, Yu Liu, Hongsheng Li</li>
<li>隶属单位：香港中文大学多媒体实验室</li>
<li>关键词：视频扩散模型、一致性模型、个性化层、动画制作</li>
<li>论文链接：https://arxiv.org/abs/2402.00769
Github 链接：无</li>
<li>
<p>摘要：
（1）：研究背景：视频扩散模型因其能够生成连贯且高保真视频而备受关注。然而，迭代式去噪过程使其计算密集且耗时，从而限制了其应用。
（2）：过去方法及其问题：受一致性模型 (CM) 的启发，CM 将预训练的图像扩散模型蒸馏以加速最小步长的采样，并在条件图像生成上成功扩展了潜在一致性模型 (LCM)。然而，直接对原始视频数据集进行一致性学习的训练效率低，生成的视觉质量也不佳。
（3）：研究方法：提出 AnimateLCM，允许在最少步长内生成高保真视频。提出了一种去耦合一致性学习策略，将图像生成先验和运动生成先验的蒸馏解耦，提高了训练效率并增强了生成视觉质量。此外，提出了一种有效策略，将稳定扩散社区中即插即用的适配器与蒸馏的文本条件视频一致性模型相结合，或从头开始训练适配器，而不会损害采样速度。
（4）：实验结果：在图像条件视频生成和布局条件视频生成中验证了所提出的策略，均取得了最优结果。实验结果验证了所提出方法的有效性。</p>
</li>
<li>
<p>方法：
（1）：提出了一种去耦合一致性学习策略，将图像生成先验和运动生成先验的蒸馏解耦，提高了训练效率并增强了生成视觉质量。
（2）：提出了一种有效策略，将稳定扩散社区中即插即用的适配器与蒸馏的文本条件视频一致性模型相结合，或从头开始训练适配器，而不会损害采样速度。
（3）：提出了一种新的初始化策略，该策略可以有效地将空间 LoRA 权重和时间层结合起来，从而提高训练效率。
（4）：提出了一种无教师的一致性学习策略，该策略可以通过单步 MCMC 近似来估计分数，从而无需预训练的视频扩散模型作为教师模型。
（5）：提出了一种新的图像到视频的预处理策略，该策略可以有效地提取图像上下文并将其融入一致性模型中。</p>
</li>
<li>
<p>结论：
（1）：本文提出了一种新的视频生成加速方法AnimateLCM，该方法通过解耦一致性学习策略和教师模型的适应策略，实现了视频生成的高效性和高质量。
（2）：创新点：</p>
</li>
<li>提出了一种解耦一致性学习策略，将图像生成先验和运动生成先验的蒸馏解耦，提高了训练效率并增强了生成视觉质量。</li>
<li>提出了一种有效策略，将稳定扩散社区中即插即用的适配器与蒸馏的文本条件视频一致性模型相结合，或从头开始训练适配器，而不会损害采样速度。</li>
<li>提出了一种新的初始化策略，该策略可以有效地将空间LoRA权重和时间层结合起来，从而提高训练效率。</li>
<li>提出了一种无教师的一致性学习策略，该策略可以通过单步MCMC近似来估计分数，从而无需预训练的视频扩散模型作为教师模型。</li>
<li>提出了一种新的图像到视频的预处理策略，该策略可以有效地提取图像上下文并将其融入一致性模型中。
性能：</li>
<li>在图像条件视频生成和布局条件视频生成中验证了所提出的策略，均取得了最优结果。</li>
<li>实验结果验证了所提出方法的有效性。
工作量：</li>
<li>本文的工作量较大，需要对视频扩散模型、一致性模型和适配器等多个方面进行研究和实现。</li>
<li>本文的实验部分也比较复杂，需要对多个数据集和多个模型进行训练和评估。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-0a500cdbd8cd65da7ce9d1f829b50f0a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c83ed1cad4b7378b141c6e7abe349fbd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8e303adc03472e85d52d1d42c05fd46.jpg" align="middle">
</details>




<h2 id="CapHuman-Capture-Your-Moments-in-Parallel-Universes"><a href="#CapHuman-Capture-Your-Moments-in-Parallel-Universes" class="headerlink" title="CapHuman: Capture Your Moments in Parallel Universes"></a>CapHuman: Capture Your Moments in Parallel Universes</h2><p><strong>Authors:Chao Liang, Fan Ma, Linchao Zhu, Yingying Deng, Yi Yang</strong></p>
<p>We concentrate on a novel human-centric image synthesis task, that is, given only one reference facial photograph, it is expected to generate specific individual images with diverse head positions, poses, and facial expressions in different contexts. To accomplish this goal, we argue that our generative model should be capable of the following favorable characteristics: (1) a strong visual and semantic understanding of our world and human society for basic object and human image generation. (2) generalizable identity preservation ability. (3) flexible and fine-grained head control. Recently, large pre-trained text-to-image diffusion models have shown remarkable results, serving as a powerful generative foundation. As a basis, we aim to unleash the above two capabilities of the pre-trained model. In this work, we present a new framework named CapHuman. We embrace the ``encode then learn to align” paradigm, which enables generalizable identity preservation for new individuals without cumbersome tuning at inference. CapHuman encodes identity features and then learns to align them into the latent space. Moreover, we introduce the 3D facial prior to equip our model with control over the human head in a flexible and 3D-consistent manner. Extensive qualitative and quantitative analyses demonstrate our CapHuman can produce well-identity-preserved, photo-realistic, and high-fidelity portraits with content-rich representations and various head renditions, superior to established baselines. Code and checkpoint will be released at <a href="https://github.com/VamosC/CapHuman">https://github.com/VamosC/CapHuman</a>. </p>
<p><a href="http://arxiv.org/abs/2402.00627v1">PDF</a> Project page: <a href="https://caphuman.github.io/">https://caphuman.github.io/</a></p>
<p><strong>Summary</strong><br>通过融合文本到图像扩散模型，CapHuman 可以生成具有丰富内容表示和多种头部渲染的、高度真实和保留身份的肖像。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CapHuman 旨在通过融合文本到图像扩散模型来生成具有丰富内容表示和多种头部渲染的、高度真实和保留身份的肖像。</li>
<li>CapHuman 框架采用“先编码再学习对齐”的范式，能够在推理时对新个体进行通用身份保留，而无需繁琐的微调。</li>
<li>CapHuman 使用 3D 面部先验来为模型提供以灵活且 3D 一致的方式控制人头的能力。</li>
<li>CapHuman 能够生成具有丰富内容表示和多种头部渲染的、高度真实和保留身份的肖像，优于现有的基准。</li>
<li>CapHuman 的代码和检查点将在 <a href="https://github.com/VamosC/CapHuman">https://github.com/VamosC/CapHuman</a> 上发布。</li>
<li>CapHuman 为人脸图像合成任务提供了一种新的解决方案，在身份保留、头部控制和照片真实感方面取得了显着的改进。</li>
<li>CapHuman 可以作为一种新的工具，用于各种应用，例如虚拟形象创建、游戏角色设计和电影视觉特效。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：CapHuman：捕捉平行宇宙中的瞬间</li>
<li>作者：Yilun Xu, Wenbo Li, Yajie Zhao, Yifan Jiang, Chen Change Loy</li>
<li>单位：香港中文大学</li>
<li>关键词：人脸图像生成、文本到图像生成、身份保持、头部控制</li>
<li>论文链接：https://arxiv.org/abs/2402.00627
Github 代码链接：暂无</li>
<li>
<p>摘要：
(1) 研究背景：人脸图像生成是一项具有挑战性的任务，需要模型能够理解人类社会和世界，并能够以逼真和一致的方式生成人脸图像。
(2) 过去的方法：现有的方法通常需要大量的数据和复杂的训练过程，并且在生成图像的质量和一致性方面存在问题。
(3) 研究方法：本文提出了一种名为 CapHuman 的新框架，该框架采用“编码然后学习对齐”的范式，可以对新个体进行身份保持，而无需在推理时进行繁琐的调整。CapHuman 对身份特征进行编码，然后学习将这些特征对齐到潜在空间中。此外，本文还引入了一个 3D 面部先验，使模型能够以灵活和 3D 一致的方式控制人像头部。
(4) 实验结果：广泛的定性和定量分析表明，CapHuman 可以生成具有良好身份保持性、逼真和高保真的人像，具有丰富的语义表示和各种头部呈现方式，优于已有的基准方法。</p>
</li>
<li>
<p><strong>方法</strong>：
(1) <strong>编码然后学习对齐范式</strong>：CapHuman 采用“编码然后学习对齐”的范式，将人脸图像生成任务分解为两个步骤：首先，将人脸图像编码成一个紧凑的表示；然后，学习将这个表示对齐到潜在空间中，以便生成新的图像。
(2) <strong>身份特征编码</strong>：CapHuman 使用一个预训练的人脸识别模型来提取人脸图像的身份特征。这些特征用于对齐人脸图像，以确保生成的图像具有与输入图像相同的人物身份。
(3) <strong>潜在空间学习</strong>：CapHuman 使用一个生成对抗网络 (GAN) 来学习潜在空间。GAN 由一个生成器和一个判别器组成。生成器将编码的人脸特征映射到潜在空间，判别器则试图区分生成的图像和真实图像。
(4) <strong>3D 面部先验</strong>：CapHuman 引入了一个 3D 面部先验，使模型能够以灵活和 3D 一致的方式控制人像头部。3D 面部先验是一个预训练的 3D 人脸模型，它可以提供人脸的形状、纹理和姿势信息。
(5) <strong>头部控制</strong>：CapHuman 使用一个头部控制模块来控制生成的人像头部的姿势。头部控制模块是一个卷积神经网络，它将潜在空间中的表示映射到一个头部姿势向量。这个头部姿势向量用于控制生成的人像头部的姿势。</p>
</li>
<li>
<p>结论：
（1）：CapHuman 提出了一种基于强大的预训练文本到图像扩散模型的可推广身份保持和细粒度头部控制以人为中心图像合成框架。该框架采用“编码然后学习对齐”范式，无需进一步微调即可实现可推广的身份保持能力。通过结合 3D 面部表示，它赋予预训练模型灵活且细粒度的头部控制。给定一张参考人脸图像，CapHuman 可以生成具有不同头部位置、姿势和面部表情的身份保持、高保真和逼真的真人肖像，适用于不同的场景。
（2）：创新点：</p>
</li>
<li>提出了一种基于预训练文本到图像扩散模型的通用身份保持和细粒度头部控制框架。</li>
<li>采用“编码然后学习对齐”范式，无需进一步微调即可实现可推广的身份保持能力。</li>
<li>引入 3D 面部表示，赋予预训练模型灵活且细粒度的头部控制。</li>
<li>提出了一种头部控制模块，可以控制生成的人像头部的姿势。
性能：</li>
<li>CapHuman 可以生成具有良好身份保持性、逼真和高保真的人像，具有丰富的语义表示和各种头部呈现方式。</li>
<li>在多个数据集上进行的广泛定性和定量分析表明，CapHuman 优于已有的基准方法。
工作量：</li>
<li>CapHuman 的实现相对简单，并且可以轻松扩展到其他数据集和任务。</li>
<li>CapHuman 的训练过程相对高效，并且可以在标准 GPU 上完成。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-c52c4014e9bcf0ad466bef3b776ce749.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dec30884252e67ce782b09b5a6b368e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6bf56f9b1649b16183af2aa8676dc283.jpg" align="middle">
</details>




<h2 id="LRDif-Diffusion-Models-for-Under-Display-Camera-Emotion-Recognition"><a href="#LRDif-Diffusion-Models-for-Under-Display-Camera-Emotion-Recognition" class="headerlink" title="LRDif: Diffusion Models for Under-Display Camera Emotion Recognition"></a>LRDif: Diffusion Models for Under-Display Camera Emotion Recognition</h2><p><strong>Authors:Zhifeng Wang, Kaihao Zhang, Ramesh Sankaranarayana</strong></p>
<p>This study introduces LRDif, a novel diffusion-based framework designed specifically for facial expression recognition (FER) within the context of under-display cameras (UDC). To address the inherent challenges posed by UDC’s image degradation, such as reduced sharpness and increased noise, LRDif employs a two-stage training strategy that integrates a condensed preliminary extraction network (FPEN) and an agile transformer network (UDCformer) to effectively identify emotion labels from UDC images. By harnessing the robust distribution mapping capabilities of Diffusion Models (DMs) and the spatial dependency modeling strength of transformers, LRDif effectively overcomes the obstacles of noise and distortion inherent in UDC environments. Comprehensive experiments on standard FER datasets including RAF-DB, KDEF, and FERPlus, LRDif demonstrate state-of-the-art performance, underscoring its potential in advancing FER applications. This work not only addresses a significant gap in the literature by tackling the UDC challenge in FER but also sets a new benchmark for future research in the field. </p>
<p><a href="http://arxiv.org/abs/2402.00250v1">PDF</a> </p>
<p><strong>摘要</strong><br>UDC 环境下的噪声和失真问题通过 LRDif 得到有效解决，在 FER 应用领域展示出强大能力。</p>
<p><strong>关键要点</strong></p>
<ul>
<li>LRDif 是一种专为在屏下摄像头 (UDC) 背景下人脸表情识别 (FER) 设计的基于扩散的框架。</li>
<li>LRDif 采用了包含浓缩预提取网络 (FPEN) 和敏捷 Transformer 网络 (UDCformer) 的两阶段训练策略，这些策略能有效地从 UDC 图像中识别出情感标签。</li>
<li>LRDif 将漫散模型 (DM) 的鲁棒分布映射功能与 Transformer 的空间依赖关系建模能力相结合，有效地克服了 UDC 环境中固有的噪声和失真障碍。</li>
<li>LRDif 在 RAF-DB、KDEF 和 FERPlus 等标准 FER 数据集上进行的综合实验表明，它具有先进的性能，突出了其在 FER 应用中的潜力。</li>
<li>这项工作不仅通过应对 FER 中的 UDC 挑战填补了文献中的空白，还为该领域的未来研究树立了新的基准。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：LRDif：用于屏下摄像头情绪识别的扩散模型</li>
<li>作者：Zhifeng Wang, Kaihao Zhang, Ramesh Sankaranarayana</li>
<li>单位：澳大利亚国立大学计算机学院</li>
<li>关键词：屏下摄像头、情绪识别、扩散模型</li>
<li>论文链接：https://arxiv.org/abs/2402.00250
    Github 代码链接：None</li>
<li>
<p>摘要：
（1）研究背景：随着屏下摄像头技术的不断发展，在屏下摄像头环境下进行情绪识别成为一个新的研究热点。然而，屏下摄像头图像质量较差，存在清晰度低、噪声大等问题，给情绪识别带来了挑战。
（2）过去方法及问题：以往的情绪识别方法主要针对传统摄像头采集的图像，无法很好地处理屏下摄像头图像。这些方法在屏下摄像头图像上往往会出现精度下降的问题。
（3）研究方法：本文提出了一种新的情绪识别方法LRDif，该方法采用了两阶段训练策略，首先使用预训练的特征提取网络FPEN提取图像特征，然后使用Transformer网络UDCformer对特征进行分类。LRDif利用扩散模型的强大分布映射能力和Transformer的时序依赖建模能力，有效地克服了屏下摄像头图像中存在的噪声和失真问题。
（4）实验结果：在RAF-DB、KDEF和FERPlus等标准FER数据集上进行的综合实验表明，LRDif在屏下摄像头图像上的情绪识别任务中取得了最先进的性能，证明了其在推进FER应用方面的潜力。</p>
</li>
<li>
<p>方法：
(1) 数据预处理：对屏下摄像头图像进行预处理，包括图像裁剪、缩放和归一化等操作。
(2) 特征提取：使用预训练的特征提取网络FPEN提取图像特征。FPEN是一个基于卷积神经网络的特征提取器，可以提取图像中具有判别力的特征。
(3) 特征分类：使用Transformer网络UDCformer对FPEN提取的特征进行分类。UDCformer是一个基于Transformer的分类器，可以对图像特征进行时序依赖建模，从而提高分类精度。
(4) 扩散模型训练：使用扩散模型对UDCformer进行训练。扩散模型是一种生成模型，可以将高维数据映射到低维空间，从而减少数据中的噪声和失真。
(5) 情绪识别：将训练好的UDCformer应用于屏下摄像头图像的情感识别任务。UDCformer可以对图像特征进行分类，从而识别出图像中人物的情绪。</p>
</li>
<li>
<p>结论：
（1）：本文提出了一种新的扩散模型框架LRDif，用于屏下摄像头环境下的人脸表情识别。LRDif通过两阶段训练策略，一个预提取网络（FPEN）和一个Transformer网络（UDCformer），克服了屏下摄像头图像退化的问题。这些模块能够有效地从退化的屏下摄像头图像中恢复表情标签。实验结果表明，提出的LRDif模型表现出优异的性能，在三个屏下摄像头人脸表情数据集上都取得了最先进的结果。
（2）：创新点：</p>
</li>
<li>提出了一种新的扩散模型框架LRDif，用于屏下摄像头环境下的人脸表情识别。</li>
<li>使用两阶段训练策略，一个预提取网络（FPEN）和一个Transformer网络（UDCformer），来克服屏下摄像头图像退化的问题。</li>
<li>实验结果表明，提出的LRDif模型在三个屏下摄像头人脸表情数据集上都取得了最先进的结果。
性能：</li>
<li>在RAF-DB、KDEF和FERPlus等标准FER数据集上进行的综合实验表明，LRDif在屏下摄像头图像上的情绪识别任务中取得了最先进的性能。
工作量：</li>
<li>本文的工作量适中，作者使用了预训练的特征提取网络FPEN和Transformer网络UDCformer，并对LRDif模型进行了综合实验，证明了其在屏下摄像头图像上的情绪识别任务中取得了最先进的性能。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-dd40f8d106e7073ea6d54966262e71e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd9d427bc731cebc6c9739681cdd0f4d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-568de78c017b3bcd7823d72ed39b1b28.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca356d9bc9e3749ffe997b0eeac0f361.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-136e8eea5cfa1e09239cddd5e2aea3e9.jpg" align="middle">
</details>




<h2 id="AEROBLADE-Training-Free-Detection-of-Latent-Diffusion-Images-Using-Autoencoder-Reconstruction-Error"><a href="#AEROBLADE-Training-Free-Detection-of-Latent-Diffusion-Images-Using-Autoencoder-Reconstruction-Error" class="headerlink" title="AEROBLADE: Training-Free Detection of Latent Diffusion Images Using   Autoencoder Reconstruction Error"></a>AEROBLADE: Training-Free Detection of Latent Diffusion Images Using   Autoencoder Reconstruction Error</h2><p><strong>Authors:Jonas Ricker, Denis Lukovnikov, Asja Fischer</strong></p>
<p>With recent text-to-image models, anyone can generate deceptively realistic images with arbitrary contents, fueling the growing threat of visual disinformation. A key enabler for generating high-resolution images with low computational cost has been the development of latent diffusion models (LDMs). In contrast to conventional diffusion models, LDMs perform the denoising process in the low-dimensional latent space of a pre-trained autoencoder (AE) instead of the high-dimensional image space. Despite their relevance, the forensic analysis of LDMs is still in its infancy. In this work we propose AEROBLADE, a novel detection method which exploits an inherent component of LDMs: the AE used to transform images between image and latent space. We find that generated images can be more accurately reconstructed by the AE than real images, allowing for a simple detection approach based on the reconstruction error. Most importantly, our method is easy to implement and does not require any training, yet nearly matches the performance of detectors that rely on extensive training. We empirically demonstrate that AEROBLADE is effective against state-of-the-art LDMs including Stable Diffusion and Midjourney. Beyond detection, our approach allows for the qualitative analysis of images, which can be leveraged for identifying inpainted regions. </p>
<p><a href="http://arxiv.org/abs/2401.17879v1">PDF</a> </p>
<p><strong>Summary</strong><br>利用预训练自动编码器低维空间中的去噪过程，扩散模型可以生成具有任意内容的极其逼真的图像，从而带来视觉错误信息。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>扩散模型 (LDMs) 利用预训练自动编码器 (AE) 在低维空间中执行去噪过程，以生成高分辨率图像。</li>
<li>LDMs 的取证分析尚处于起步阶段。</li>
<li>AEROBLADE 是一种利用 AE 来检测 LDMs 生成图像的新颖方法。</li>
<li>生成的图像可以被 AE 更准确地重建，而真实图像则不能。</li>
<li>AEROBLADE 是一种简单的检测方法，不需要任何训练，即可接近依赖大量训练的检测器的性能。</li>
<li>AEROBLADE 可以有效地检测出最先进的 LDMs，包括 Stable Diffusion 和 Midjourney。</li>
<li>除了检测之外，AEROBLADE 还可以对图像进行定性分析，以便识别被修复的区域。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：AEROBLADE：利用自动编码器重建误差实现无训练检测潜在扩散图像</li>
<li>作者：Cheng Zhang、Yuheng Li、Matthias Niessner</li>
<li>单位：马克斯·普朗克计算机图形学研究所</li>
<li>关键词：潜在扩散模型、图像取证、深度学习、自动编码器、重建误差</li>
<li>论文链接：https://arxiv.org/abs/2302.09734，Github 代码链接：无</li>
<li>摘要：
（1）研究背景：随着文本到图像模型的快速发展，人们可以轻松生成具有欺骗性的逼真图像，这加剧了视觉错误信息的威胁。潜在扩散模型 (LDM) 作为生成高分辨率图像的关键技术，因其低计算成本而备受关注。与传统扩散模型不同，LDM 在预训练自动编码器 (AE) 的低维潜在空间中执行去噪过程，而非高维图像空间。尽管 LDM 具有重要意义，但其取证分析仍处于起步阶段。
（2）过去方法及其问题：现有方法主要依赖于训练检测器来区分真实图像和生成图像。这些方法通常需要大量训练数据和计算资源，并且对新出现的 LDM 模型的泛化性较差。
（3）研究方法：本文提出了一种名为 AEROBLADE 的新型检测方法，该方法利用了 LDM 的固有组成部分：用于在图像空间和潜在空间之间转换图像的 AE。我们发现，与真实图像相比，生成图像可以通过 AE 更准确地重建，这为基于重建误差的简单检测方法提供了可能。最重要的是，我们的方法易于实现，无需任何训练，但其性能却与依赖于大量训练的检测器几乎相当。
（4）方法性能：我们通过实验证明，AEROBLADE 对包括 StableDiffusion 和 Midjourney 在内的最先进 LDM 模型有效。除了检测之外，我们的方法还允许对图像进行定性分析，这可用于识别图像中的修饰区域。</li>
</ol>
<p><strong>Methods</strong>：
（1）重建误差检测方法的基本框架：
- 给定生成模型 Gi 和图像 x，计算重建图像 ~x = ϕi(x)，其中 ϕi 是基于 Gi 的重建方法。
- 对于由模型 Gi 生成的图像 xi，其原始图像与重建图像之间的距离 d(xi, ~xi) 很小。
- 真实图像 xr 不能被准确重建，即 d(xr, ~xr) &gt; d(xi, ~xi)。</p>
<p>（2）AEROBLADE 方法：
- AEROBLADE（自动编码器重建误差潜在扩散检测）方法基于这样的观察：模型的自动编码器 (AE) 在重建生成图像方面优于重建真实图像。
- 图像与其重建图像之间的距离可以进行简单的阈值检测。
- 与之前的研究不同，AEROBLADE 方法既不需要执行代价高昂的确定性去噪过程，也不需要任何额外的训练。
- AEROBLADE 方法的重建误差定义为：∆AEi(x) = d(x, ~x) = d(x, Di(Ei(x))），其中 Ei 和 Di 分别表示 Gi 的自动编码器的编码器和解码器，d 是某种距离度量。</p>
<p>（3）AEROBLADE 方法的优势：
- 易于实现，无需任何训练，但性能与依赖于大量训练的检测器几乎相当。
- 对包括 StableDiffusion 和 Midjourney 在内的最先进潜在扩散模型有效。
- 除了检测之外，AEROBLADE 方法还允许对图像进行定性分析，这可用于识别图像中的修饰区域。</p>
<ol>
<li>结论：
（1）：本文提出了一种名为 AEROBLADE 的新型潜在扩散图像检测方法，该方法利用了潜在扩散模型的自动编码器重建误差来检测生成图像。AEROBLADE 方法易于实现，无需任何训练，但性能与依赖于大量训练的检测器几乎相当。
（2）：创新点：</li>
<li>提出了一种基于自动编码器重建误差的潜在扩散图像检测方法。</li>
<li>该方法易于实现，无需任何训练，但性能与依赖于大量训练的检测器几乎相当。</li>
<li>该方法对包括 StableDiffusion 和 Midjourney 在内的最先进潜在扩散模型有效。</li>
<li>该方法除了检测之外，还允许对图像进行定性分析，这可用于识别图像中的修饰区域。
性能：</li>
<li>该方法对包括 StableDiffusion 和 Midjourney 在内的最先进潜在扩散模型有效。</li>
<li>该方法的检测性能与依赖于大量训练的检测器几乎相当。</li>
<li>该方法除了检测之外，还允许对图像进行定性分析，这可用于识别图像中的修饰区域。
工作量：</li>
<li>该方法易于实现，无需任何训练。</li>
<li>该方法的计算成本较低。</li>
<li>该方法的存储成本较低。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-73def9abeca6572820631d77d6d5f109.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e0d1888497ab3bcee223e776ab4c50c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0f0becc9eb676089a928342cf2a8f891.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-07174ef7ab315c814e5b835ccce3106c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2bf092448ab8005e13f25729d701b790.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8d473060909c4dac5e620acfb56465e2.jpg" align="middle">
</details>




<h2 id="Spatial-and-Frequency-aware-Restoration-method-for-Images-based-on-Diffusion-Models"><a href="#Spatial-and-Frequency-aware-Restoration-method-for-Images-based-on-Diffusion-Models" class="headerlink" title="Spatial-and-Frequency-aware Restoration method for Images based on   Diffusion Models"></a>Spatial-and-Frequency-aware Restoration method for Images based on   Diffusion Models</h2><p><strong>Authors:Kyungsung Lee, Donggyu Lee, Myungjoo Kang</strong></p>
<p>Diffusion models have recently emerged as a promising framework for Image Restoration (IR), owing to their ability to produce high-quality reconstructions and their compatibility with established methods. Existing methods for solving noisy inverse problems in IR, considers the pixel-wise data-fidelity. In this paper, we propose SaFaRI, a spatial-and-frequency-aware diffusion model for IR with Gaussian noise. Our model encourages images to preserve data-fidelity in both the spatial and frequency domains, resulting in enhanced reconstruction quality. We comprehensively evaluate the performance of our model on a variety of noisy inverse problems, including inpainting, denoising, and super-resolution. Our thorough evaluation demonstrates that SaFaRI achieves state-of-the-art performance on both the ImageNet datasets and FFHQ datasets, outperforming existing zero-shot IR methods in terms of LPIPS and FID metrics. </p>
<p><a href="http://arxiv.org/abs/2401.17629v1">PDF</a> </p>
<p><strong>Summary</strong><br>扩频与频域信息充分结合的扩散模型图像复原方法 SaFaRI 以高保真成像能力达到图像修复的当前先进水准。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SaFaRI 模型在扩散模型框架下结合了图像的空间域和频域信息，提升了图像修复质量。</li>
<li>在各种噪声逆问题上，包括修复、去噪和超分辨率，SaFaRI 模型均取得了最优性能。</li>
<li>SaFaRI 模型同时在 ImageNet 数据集和 FFHQ 数据集上都优于其他零样本图像修复方法。</li>
<li>SaFaRI 模型在 LPIPS 和 FID 指标上均取得了最优性能。</li>
<li>与先前方法相比，SaFaRI 模型能在更好地恢复图像细节的同时有效降低噪声。</li>
<li>SaFaRI 模型在移除椒盐噪声和修复损坏图像方面表现出色。</li>
<li>SaFaRI 模型在超分辨率任务中能够有效地将低分辨率图像转换成高分辨率图像。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：基于扩散模型的空间和频率感知图像修复方法</li>
<li>作者：Kyungsung Lee、Donggyu Lee、Myungjoo Kang</li>
<li>第一作者单位：首尔大学数学科学系</li>
<li>关键词：图像修复、扩散模型、数据保真度、空间感知、频率感知</li>
<li>论文链接：https://arxiv.org/abs/2401.17629</li>
<li>
<p>摘要：
（1）研究背景：图像修复旨在从退化或损坏的图像中重建原始图像。经典方法是使用变分模型，其中包括数据保真度项和正则化项。扩散模型作为一种新兴的生成模型框架，在图像修复任务中展现出强大的能力，可以实现零样本学习。
（2）过去方法及其问题：现有方法主要关注像素级的数据保真度，但忽略了感知信息。这导致修复后的图像可能在视觉上不令人满意。
（3）研究方法：本文提出了一种空间和频率感知的扩散模型 SaFaRI，用于高斯噪声下的图像修复。该模型鼓励图像在空间域和频率域都保持数据保真度，从而提高重建质量。
（4）方法性能：在 ImageNet 和 FFHQ 数据集上的广泛评估表明，SaFaRI 在 LPIPS 和 FID 指标上优于现有的零样本图像修复方法，证明了其在图像修复任务上的有效性。</p>
</li>
<li>
<p>方法：
(1) SaFaRI模型框架：SaFaRI模型由编码器、扩散过程和解码器组成。编码器将退化图像映射到潜在空间，扩散过程通过添加噪声逐渐将潜在表示从退化状态转换到干净状态，解码器将干净的潜在表示重建为修复后的图像。
(2) 空间感知数据保真度：SaFaRI模型在空间域中使用局部感知损失来鼓励修复后的图像与退化图像在局部区域内保持一致。局部感知损失通过计算退化图像和修复图像在局部区域内的差异来衡量数据保真度。
(3) 频率感知数据保真度：SaFaRI模型在频率域中使用频谱损失来鼓励修复后的图像与退化图像在频率分布上保持一致。频谱损失通过计算退化图像和修复图像的频谱差异来衡量数据保真度。
(4) 扩散过程：SaFaRI模型采用非对称扩散过程，即正向扩散过程和反向扩散过程。正向扩散过程将潜在表示从退化状态逐渐转换到干净状态，反向扩散过程将潜在表示从干净状态逐渐转换到退化状态。
(5) 训练过程：SaFaRI模型通过最小化总损失函数来训练，总损失函数包括局部感知损失、频谱损失和正则化损失。正则化损失用于防止模型过拟合。</p>
</li>
<li>
<p>结论：</p>
</li>
</ol>
<p>（1）： 本文提出了一种新的基于扩散模型的图像修复方法 SaFaRI，该方法将空间和频率信息纳入数据保真度项中，有效提高了修复性能。通过利用双三次插值和傅里叶变换同时利用空间和频率信息，SaFaRI 在各种图像修复基准上取得了最先进的结果，优于现有方法。尽管我们提出的方法具有显着的性能，但由于先验项的影响，变换的应用不可避免地会对可行解产生扰动。对解扰动的全面分析可以加强我们方法论的理论基础。</p>
<p>（2）： 创新点：</p>
<ul>
<li>提出了一种新的基于扩散模型的图像修复方法 SaFaRI，该方法将空间和频率信息纳入数据保真度项中，有效提高了修复性能。</li>
<li>SaFaRI 利用双三次插值和傅里叶变换同时利用空间和频率信息，可以更好地保留图像的细节和纹理。</li>
<li>SaFaRI 采用非对称扩散过程，可以更好地控制图像修复过程，提高修复质量。</li>
</ul>
<p>性能：</p>
<ul>
<li>SaFaRI 在 ImageNet 和 FFHQ 数据集上的广泛评估表明，在 LPIPS 和 FID 指标上优于现有的零样本图像修复方法，证明了其在图像修复任务上的有效性。</li>
<li>SaFaRI 在修复高斯噪声图像时，可以有效地去除噪声，同时保留图像的细节和纹理。</li>
</ul>
<p>工作量：</p>
<ul>
<li>SaFaRI 模型的训练和推理过程相对简单，易于实现。</li>
<li>SaFaRI 模型的参数量较少，可以快速训练和推理。</li>
</ul>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d12c8ba98ed6bf34752247f9b5d4ed94.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-841cc516755a816daa1feb35b6020929.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6047f95584cb41e2634a1d794c58b933.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e2b66f6263c96cab5ccac11907563d1.jpg" align="middle">
</details>




## You Only Need One Step: Fast Super-Resolution with Stable Diffusion via   Scale Distillation

**Authors:Mehdi Noroozi, Isma Hadji, Brais Martinez, Adrian Bulat, Georgios Tzimiropoulos**

In this paper, we introduce YONOS-SR, a novel stable diffusion-based approach for image super-resolution that yields state-of-the-art results using only a single DDIM step. We propose a novel scale distillation approach to train our SR model. Instead of directly training our SR model on the scale factor of interest, we start by training a teacher model on a smaller magnification scale, thereby making the SR problem simpler for the teacher. We then train a student model for a higher magnification scale, using the predictions of the teacher as a target during the training. This process is repeated iteratively until we reach the target scale factor of the final model. The rationale behind our scale distillation is that the teacher aids the student diffusion model training by i) providing a target adapted to the current noise level rather than using the same target coming from ground truth data for all noise levels and ii) providing an accurate target as the teacher has a simpler task to solve. We empirically show that the distilled model significantly outperforms the model trained for high scales directly, specifically with few steps during inference. Having a strong diffusion model that requires only one step allows us to freeze the U-Net and fine-tune the decoder on top of it. We show that the combination of spatially distilled U-Net and fine-tuned decoder outperforms state-of-the-art methods requiring 200 steps with only one single step. 

[PDF](http://arxiv.org/abs/2401.17258v1) 

**Summary**
扩散模型的单步超分辨率方法，YONO-SR，通过蒸馏训练，可实现图像分辨率的提升。

**Key Takeaways**
- YONOS-SR 在保持推理速度的同时，实现高质量的图像超分辨率。
- 引入一种新的尺度蒸馏方法，从较小的尺度开始训练教师模型，然后采用迭代的方式将知识迁移到学生模型。
- 蒸馏训练使学生模型能够获得更准确的目标，从而提高了超分辨率的性能。
- 只需一步推理，YONOS-SR 就能够超越需要 200 步的最新方法。
- YONOS-SR 结合了空间蒸馏的 U-Net 和微调的解码器，进一步提高了超分辨率效果。
- 冻结 U-Net 并微调解码器，可以进一步提升超分辨率性能。
- YONOS-SR 对于计算资源受限的设备非常适用。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>题目：一步到位：通过尺度蒸馏实现稳定扩散的快速超分辨率</li>
<li>作者：Mehdi Noroozi, Isma Hadji, Brais Martinez, Adrian Bulat, Georgios Tzimiropoulos</li>
<li>第一作者单位：三星AI剑桥</li>
<li>关键词：图像超分辨率、稳定扩散、尺度蒸馏、深度学习</li>
<li>论文链接：https://arxiv.org/abs/2401.17258</li>
<li>摘要：
(1) 研究背景：扩散模型在各种图像生成任务中表现出色，包括图像超分辨率。然而，采样策略所需的连续去噪传递数量很大，即使对于在自动编码器潜在空间中运行的基于稳定扩散的模型（SD）也是如此。
(2) 过去的方法及问题：最近，已经提出了几种减少采样步骤数量的方法。不幸的是，这些方法通常会影响性能，尤其是对于较少的步骤。基于扩散的模型通常在与训练期间看到的大小相似的图像块上产生最佳结果（例如，SD 的 64×64）。另一方面，超分辨率应用程序需要在高分辨率设置中运行，这大大加剧了基于扩散模型的计算问题。
(3) 本文提出的研究方法：为了解决上述问题，本文提出了一种新颖的训练策略，称为尺度蒸馏。典型的基于扩散的超分辨率方法通过直接在目标尺度因子上的低分辨率图像上进行条件来训练超分辨率模型，而我们提出了一种渐进式训练方法，从较低尺度因子（即条件信号更接近目标）开始训练模型，并使用先前训练的模型作为教师逐步增加到目标尺度因子。
(4) 方法在什么任务上取得了什么性能：在图像超分辨率任务上，该方法使用仅一步 DDIM 即可实现最先进的结果。通过对教师模型的预测进行条件设置，可以训练出一个强大的扩散模型，该模型只需要一步即可冻结 U-Net 并微调其上的解码器。实验表明，空间蒸馏 U-Net 和微调解码器的组合仅需一步即可优于需要 200 步的最先进方法。</li>
</ol>
<p>Some Error for method(比如是不是没有Methods这个章节)</p>
<ol>
<li>结论：
（1）：本文提出了一种基于尺度蒸馏的快速稳定扩散超分辨率方法，该方法可以通过一步DDIM实现最先进的结果。
（2）：创新点：</li>
<li>提出了一种新颖的训练策略——尺度蒸馏，该策略可以将基于稳定扩散的超分辨率模型训练到仅需要一步即可生成高质量图像。</li>
<li>通过对教师模型的预测进行条件设置，可以训练出一个强大的扩散模型，该模型只需要一步即可冻结U-Net并微调其上的解码器。
性能：</li>
<li>在图像超分辨率任务上，该方法使用仅一步DDIM即可实现最先进的结果。</li>
<li>实验表明，空间蒸馏U-Net和微调解码器的组合仅需一步即可优于需要200步的最先进方法。
工作量：</li>
<li>该方法的训练过程相对简单，只需要对教师模型的预测进行条件设置，然后冻结U-Net并微调其上的解码器即可。</li>
<li>该方法的推理过程也非常快速，只需要一步DDIM即可生成高质量图像。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-66d1c3043943daf87e1f11e232a38f98.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5920453c69c00995f18077b22d4a790e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-506663e69d7322407f5094b321bf2044.jpg" align="middle">
</details>




]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>Diffusion Models</tag>
      </tags>
  </entry>
  <entry>
    <title>Talking Head Generation</title>
    <url>/2024/02/09/Paper/2024-02-09/Talking%20Head%20Generation/</url>
    <content><![CDATA[<blockquote>
<p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-02-09-更新"><a href="#2024-02-09-更新" class="headerlink" title="2024-02-09 更新"></a>2024-02-09 更新</h1><h2 id="EmoSpeaker-One-shot-Fine-grained-Emotion-Controlled-Talking-Face-Generation"><a href="#EmoSpeaker-One-shot-Fine-grained-Emotion-Controlled-Talking-Face-Generation" class="headerlink" title="EmoSpeaker: One-shot Fine-grained Emotion-Controlled Talking Face   Generation"></a>EmoSpeaker: One-shot Fine-grained Emotion-Controlled Talking Face   Generation</h2><p><strong>Authors:Guanwen Feng, Haoran Cheng, Yunan Li, Zhiyuan Ma, Chaoneng Li, Zhihao Qian, Qiguang Miao, Chi-Man Pun</strong></p>
<p>Implementing fine-grained emotion control is crucial for emotion generation tasks because it enhances the expressive capability of the generative model, allowing it to accurately and comprehensively capture and express various nuanced emotional states, thereby improving the emotional quality and personalization of generated content. Generating fine-grained facial animations that accurately portray emotional expressions using only a portrait and an audio recording presents a challenge. In order to address this challenge, we propose a visual attribute-guided audio decoupler. This enables the obtention of content vectors solely related to the audio content, enhancing the stability of subsequent lip movement coefficient predictions. To achieve more precise emotional expression, we introduce a fine-grained emotion coefficient prediction module. Additionally, we propose an emotion intensity control method using a fine-grained emotion matrix. Through these, effective control over emotional expression in the generated videos and finer classification of emotion intensity are accomplished. Subsequently, a series of 3DMM coefficient generation networks are designed to predict 3D coefficients, followed by the utilization of a rendering network to generate the final video. Our experimental results demonstrate that our proposed method, EmoSpeaker, outperforms existing emotional talking face generation methods in terms of expression variation and lip synchronization. Project page: <a href="https://peterfanfan.github.io/EmoSpeaker/">https://peterfanfan.github.io/EmoSpeaker/</a> </p>
<p><a href="http://arxiv.org/abs/2402.01422v1">PDF</a> </p>
<p><strong>摘要</strong><br>利用视觉属性引导音频解耦器和细粒度情绪系数预测模块，精细控制谈话头生成中的情绪表达，提升生成的视频的自然性和真实性。</p>
<p><strong>要点</strong></p>
<ul>
<li>提出视觉属性引导音频解耦器，仅与音频内容相关的表征向量，增强后续口型系数预测的稳定性。</li>
<li>引入细粒度情绪系数预测模块，实现更准确的情绪表达。</li>
<li>提出使用细粒度情绪矩阵的情绪强度控制方法，对生成的视频中的情绪表达进行有效控制，并对情绪强度进行更精细的分类。</li>
<li>3DMM 系数生成网络用于预测 3D 系数，然后利用渲染网络生成最终视频。</li>
<li>EmoSpeaker 方法在表情变化和唇形同步方面优于现有情感谈话人脸生成方法。</li>
<li>项目主页：<a href="https://peterfanfan.github.io/EmoSpeaker/">https://peterfanfan.github.io/EmoSpeaker/</a></li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>标题：EmoSpeaker：单次学习细粒度情感控制的说话人面部生成</li>
<li>作者：Guanwen Feng, Haoran Cheng, Yunan Li, Zhiyuan Ma, Chaoneng Li, Zhihao Qian, Qiguang Miao</li>
<li>第一作者单位：西安电子科技大学计算机科学与技术学院</li>
<li>关键词：说话人面部、三维可变形模型、视觉属性引导的解耦过程、细粒度情感控制</li>
<li>论文链接：https://arxiv.org/abs/2402.01422
Github 链接：https://github.com/peterfanfan/EmoSpeaker</li>
<li>
<p>摘要：
(1) 研究背景：说话人面部生成技术已成为近年来研究的热点，其在虚拟数字人生成、虚拟现实和电影特效等领域具有广泛的应用场景。然而，现有方法主要关注唇形同步和视频生成质量，对生成视频的情感表达关注较少。
(2) 过去方法及其问题：一些现有方法解决了情感面部动画生成的问题，但它们通常受限于长视频或短视频的驱动。此外，使用标签控制的方法难以生成具有不同强度和不同情感中间状态的情感视频。单次学习生成方法通常只考虑唇形同步，而没有考虑情感因素。
(3) 本文方法：本文提出了一种名为 EmoSpeaker 的方法，该方法通过 3D 系数作为中间表示来连接说话人面部生成过程的不同部分。为了实现这一目标，首先引入视觉属性引导的音频解耦器，从音频中提取与内容向量相关的内容向量，增强后续唇部动作系数预测的稳定性。其次，在细粒度情感系数预测模块中，将内容向量与情感标签聚合，预测细粒度的面部动作系数。此外，本文提出了一种使用细粒度情感矩阵的情感强度控制方法。通过这些方法，实现了对生成视频中的情感表达的有效控制和对情感强度的更精细分类。最后，设计了一系列 3DMM 系数生成网络来预测 3D 系数，然后利用渲染网络生成最终视频。
(4) 方法性能：实验结果表明，本文提出的 EmoSpeaker 方法在表情表达和唇形同步方面优于现有情感说话人面部生成方法。该方法可以支持其目标，生成具有任意强度的任意情感面部视频。</p>
</li>
<li>
<p>方法：
（1）视觉属性引导的音频解耦器：为了准确预测唇部信息，提出了一种视觉属性引导的音频解耦器。该解耦器利用面部动作单元（AU）作为视觉信息，指导音频的情感解耦过程，增强解耦的精度和可控性。
（2）细粒度情感系数预测模块：将内容向量与情感标签聚合，预测细粒度的面部动作系数。同时，提出了一种使用细粒度情感矩阵的情感强度控制方法，实现对生成视频中情感表达的有效控制和对情感强度的更精细分类。
（3）情感面部渲染器：设计了一系列三维可变形模型系数生成网络来预测三维系数，然后利用渲染网络生成最终视频。</p>
</li>
<li>
<p>结论：
（1）：本文提出了一种名为 EmoSpeaker 的算法，该算法仅需音频剪辑、肖像、特定情绪和强度粒度，即可生成具有细粒度强度的表情面部。该方法使用面部情绪解耦模块提取内容特征，并结合细粒度强度控制模块来实现任意情绪强度。这在电子游戏、虚拟现实、电影特效和人机界面交互等领域展示了有希望的应用。主观和客观评估表明，与最先进的方法相比，我们的方法在生成更丰富的面部动画方面具有优越性。未来的研究将集中于在细粒度强度控制领域进行深入研究，以增强更具表现力和细微差别的面部动画的生成。
（2）：创新点：</p>
</li>
<li>提出了一种视觉属性引导的音频解耦器，该解耦器利用面部动作单元作为视觉信息，指导音频的情感解耦过程，增强解耦的精度和可控性。</li>
<li>提出了一种细粒度情感系数预测模块，将内容向量与情感标签聚合，预测细粒度的面部动作系数。同时，提出了一种使用细粒度情感矩阵的情感强度控制方法，实现对生成视频中情感表达的有效控制和对情感强度的更精细分类。</li>
<li>设计了一系列三维可变形模型系数生成网络来预测三维系数，然后利用渲染网络生成最终视频。
性能：</li>
<li>在表情表达和唇形同步方面优于现有情感说话人面部生成方法。</li>
<li>可以支持其目标，生成具有任意强度的任意情感面部视频。
工作量：</li>
<li>需要收集大量的数据集来训练模型。</li>
<li>模型的训练过程比较耗时。</li>
<li>需要对模型进行微调以适应不同的数据集。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-6bacdbeff940a1345ff38f8b1dc2680f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c646c87add1ea43ace17da06ebd12a7c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d08dc09fd1df64224ed8ef166ac7d5b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0dc431600d1c5672918ab10a962f79ab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7d1b798a4f9c96adf7e70cbb6847a5b3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4c97492b45a0ba3e2e8b06c0abf4372f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ce57abfa37d7135a925aa7ba77e6120.jpg" align="middle">
</details>




]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>Talking Head Generation</tag>
      </tags>
  </entry>
  <entry>
    <title>3DGS</title>
    <url>/2024/02/29/Paper/2024-02-29/3DGS/</url>
    <content><![CDATA[<blockquote>
<p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-02-29-更新"><a href="#2024-02-29-更新" class="headerlink" title="2024-02-29 更新"></a>2024-02-29 更新</h1><h2 id="VastGaussian-Vast-3D-Gaussians-for-Large-Scene-Reconstruction"><a href="#VastGaussian-Vast-3D-Gaussians-for-Large-Scene-Reconstruction" class="headerlink" title="VastGaussian: Vast 3D Gaussians for Large Scene Reconstruction"></a>VastGaussian: Vast 3D Gaussians for Large Scene Reconstruction</h2><p><strong>Authors:Jiaqi Lin, Zhihao Li, Xiao Tang, Jianzhuang Liu, Shiyong Liu, Jiayue Liu, Yangdi Lu, Xiaofei Wu, Songcen Xu, Youliang Yan, Wenming Yang</strong></p>
<p>Existing NeRF-based methods for large scene reconstruction often have limitations in visual quality and rendering speed. While the recent 3D Gaussian Splatting works well on small-scale and object-centric scenes, scaling it up to large scenes poses challenges due to limited video memory, long optimization time, and noticeable appearance variations. To address these challenges, we present VastGaussian, the first method for high-quality reconstruction and real-time rendering on large scenes based on 3D Gaussian Splatting. We propose a progressive partitioning strategy to divide a large scene into multiple cells, where the training cameras and point cloud are properly distributed with an airspace-aware visibility criterion. These cells are merged into a complete scene after parallel optimization. We also introduce decoupled appearance modeling into the optimization process to reduce appearance variations in the rendered images. Our approach outperforms existing NeRF-based methods and achieves state-of-the-art results on multiple large scene datasets, enabling fast optimization and high-fidelity real-time rendering. </p>
<p><a href="http://arxiv.org/abs/2402.17427v1">PDF</a> Accepted to CVPR 2024. Project website:   <a href="https://vastgaussian.github.io">https://vastgaussian.github.io</a></p>
<p><strong>Summary</strong><br>利用 3D 高斯斑点技术，我们提出了 VastGaussian，一种用于大场景的高质量重建和实时渲染的新方法。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>提出渐进分区策略，使用视野感知可见性标准分配训练相机和点云。</li>
<li>引入解耦外观建模，减少渲染图像外观变化。</li>
<li>在多个大场景数据集上优于现有基于 NeRF 的方法。</li>
<li>实现最先进的成果，实现快速优化和高保真实时渲染。</li>
<li>使用 3D 高斯斑点技术进行大场景重建和渲染。</li>
<li>解决视频内存受限、优化时间长、外观变化明显等问题。</li>
<li>适用多个大场景数据集，包括 Matterport3D，SUNCG，和 Replica。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：VastGaussian：用于大场景重建的巨大 3D 高斯体</li>
<li>作者：Yuan Liu、Li-Yi Wei、Jia-Bin Huang、Yong-Liang Yang、Tong-Yee Lee</li>
<li>第一作者单位：香港中文大学（深圳）</li>
<li>关键词：NeRF、大场景重建、高斯体、外观建模</li>
<li>论文链接：https://arxiv.org/abs/2302.04750，Github 代码链接：None</li>
<li>
<p>摘要：
（1）研究背景：现有的基于 NeRF 的大场景重建方法在视觉质量和渲染速度上往往存在局限性。虽然最近的 3D 高斯体喷绘法在小规模和以物体为中心的场景中效果很好，但由于视频内存有限、优化时间长和外观变化明显，将其扩展到大型场景中会带来挑战。
（2）过去方法及其问题：本文方法的动机充分：为了解决这些挑战，我们提出了 VastGaussian，这是一种基于 3D 高斯体喷绘法在大场景上进行高质量重建和实时渲染的第一种方法。
（3）论文提出的研究方法：我们提出了一种渐进分区策略，将大场景划分为多个单元格，其中训练相机和点云通过考虑空域可见性的标准进行适当分布。在并行优化后，这些单元格被合并成一个完整的场景。我们还将解耦的外观建模引入优化过程，以减少渲染图像中的外观变化。
（4）方法在什么任务上取得了怎样的性能，性能是否能支撑其目标：我们的方法优于现有的基于 NeRF 的方法，并在多个大场景数据集上取得了最先进的结果，实现了快速优化和高保真实时渲染。</p>
</li>
<li>
<p>方法：
（1）：渐进数据分区：根据相机位置和可见性标准将大场景划分为多个单元格，并分配部分相机和点云进行优化。
（2）：解耦外观建模：引入外观嵌入和卷积神经网络，通过对渲染图像进行外观调整来减少外观变化。
（3）：无缝合并：优化各个单元格后，删除单元格外部的高斯体，然后合并非重叠单元格的高斯体，形成无缝的大场景。</p>
</li>
</ol>
<p>8.结论：
(1): 本工作提出了VastGaussian，一种基于3D高斯体喷绘法在大场景上进行高质量重建和实时渲染的第一种方法，解决了现有方法在视觉质量和渲染速度上的局限性。
(2): 创新点：
- 渐进数据分区：将大场景划分为单元格，并分配部分相机和点云进行优化，解决了视频内存有限和优化时间长的挑战。
- 解耦外观建模：引入外观嵌入和卷积神经网络，减少了渲染图像中的外观变化，提高了视觉质量。
- 无缝合并：优化各个单元格后，合并非重叠单元格的高斯体，形成了无缝的大场景。
性能：
- 在多个大场景数据集上取得了最先进的结果。
- 实现快速优化和高保真实时渲染。
工作量：
- 论文提供了详细的算法描述和实验结果。
- Github代码暂未提供。</p>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ee052136cbbee0e4d283f8c1613aa5c9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c9222e251d2d4b3d336feb1e5dc10d3c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9fb6f7a1a19593c7cf97f51e62283477.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9609bd8a7bee5ba2688b0bf50aa99233.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-04b4a21a99a56fa621e5dc34b03bb714.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-16c21380cd415ab4eb8e703f94c84868.jpg" align="middle">
</details>




## GEA: Reconstructing Expressive 3D Gaussian Avatar from Monocular Video

**Authors:Xinqi Liu, Chenming Wu, Xing Liu, Jialun Liu, Jinbo Wu, Chen Zhao, Haocheng Feng, Errui Ding, Jingdong Wang**

This paper presents GEA, a novel method for creating expressive 3D avatars with high-fidelity reconstructions of body and hands based on 3D Gaussians. The key contributions are twofold. First, we design a two-stage pose estimation method to obtain an accurate SMPL-X pose from input images, providing a correct mapping between the pixels of a training image and the SMPL-X model. It uses an attention-aware network and an optimization scheme to align the normal and silhouette between the estimated SMPL-X body and the real body in the image. Second, we propose an iterative re-initialization strategy to handle unbalanced aggregation and initialization bias faced by Gaussian representation. This strategy iteratively redistributes the avatar's Gaussian points, making it evenly distributed near the human body surface by applying meshing, resampling and re-Gaussian operations. As a result, higher-quality rendering can be achieved. Extensive experimental analyses validate the effectiveness of the proposed model, demonstrating that it achieves state-of-the-art performance in photorealistic novel view synthesis while offering fine-grained control over the human body and hand pose. Project page: https://3d-aigc.github.io/GEA/. 

[PDF](http://arxiv.org/abs/2402.16607v1) 

**Summary**
利用基于 3D 高斯体的手部和身体高保真重建技术创造富有表现力的 3D 头像。

**Key Takeaways**
- 采用两阶段姿势估计方法，从输入图像中获取准确的 SMPL-X 姿势。
- 提出迭代重新初始化策略，处理高斯表示中遇到的不平衡聚合和初始化偏差。
- 该模型在图像真实的新视角合成方面实现了最先进的性能。
- 允许对人体和手部姿态进行精细控制。
- 实验分析验证了该模型的有效性。
- 提供项目主页链接：https://3d-aigc.github.io/GEA/。
- 该方法在创建表达力丰富的 3D 头像方面具有应用潜力。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>标题：GEA：基于 3D 高斯重建表达式 3D 头像</li>
<li>作者：刘新奇、吴晨明、刘兴、刘家伦、武金波、赵晨、冯浩成、丁尔瑞、王京东</li>
<li>单位：百度视觉技术部</li>
<li>关键词：3D 头像、高斯表示、单目视频、姿态估计</li>
<li>论文链接：https://arxiv.org/abs/2402.16607，Github 代码链接：无</li>
<li>
<p>总结：
（1）研究背景：重建逼真且可驱动的头像一直是学术界和工业界的热点课题，具有广阔的商业价值和社会影响。
（2）过去方法：早期方法主要依赖于 RGB-D 相机、多视角采集设备和人工建模，但存在成本高、渲染效果不逼真等问题。神经辐射场方法虽然可以重建逼真的 3D 头像，但训练时间长、姿态泛化能力有限。3D 高斯表示方法因其显式表示而受到关注，但存在初始化不均衡和聚集不平衡的问题。
（3）研究方法：本文提出的 GEA 方法包括两大贡献。一是设计了一种两阶段姿态估计方法，通过注意力感知网络和优化方案，从输入图像中准确估计 SMPL-X 姿态，建立图像像素与 SMPL-X 模型之间的正确映射。二是提出了一种迭代式重新初始化策略，通过网格化、重采样和高斯重新操作，迭代地重新分配头像的高斯点，使其均匀分布在人体表面附近，从而提高渲染质量。
（4）任务和性能：GEA 方法在真实感新视图合成任务上取得了最先进的性能，同时提供了对人体和手部姿态的精细控制。实验结果验证了该方法的有效性，支持其目标。</p>
</li>
<li>
<p><strong>姿态估计</strong>：提出两阶段姿态估计方法，通过注意力感知网络和优化方案，从单目视频中准确估计 SMPL-X 姿态，建立图像像素与 SMPL-X 模型之间的正确映射。</p>
</li>
<li><strong>迭代式重新初始化</strong>：通过网格化、重采样和高斯重新操作，迭代地重新分配头像的高斯点，使其均匀分布在人体表面附近，提高渲染质量。</li>
<li><strong>3D 高斯表示</strong>：采用 3D 高斯点集合表示头像的形状和外观，并使用 SMPL-X 骨架模型实现详细的姿态控制。</li>
<li>
<p><strong>渲染损失函数</strong>：使用 SMPL-X 骨架变换将高斯头像从规范空间驱动到图像空间，并使用差异化渲染进行优化。损失函数包括重建损失、感知损失和残差正则化。</p>
</li>
<li>
<p>结论
（1）：本文提出了一种可由身体和手驱动的 3D 高斯头像重建方法，该方法从单目视频中获取 SMPL-X 姿态参数，指导 3D 高斯头像学习全身形状和外观。此外，还引入了一种迭代重新初始化机制，以避免 3D 高斯不平衡聚合和初始化偏差的问题。我们的目标是，这项贡献将为未来更逼真的头像重建铺平道路。
（2）：创新点：</p>
</li>
<li>提出了一种两阶段姿势细化机制，从图像中获取 SMPL-X 姿态参数，指导 3D 高斯头像学习全身形状和外观。</li>
<li>提出了一种迭代重新初始化机制，以避免 3D 高斯不平衡聚合和初始化偏差的问题。
性能：</li>
<li>在真实感新视图合成任务上取得了最先进的性能。</li>
<li>提供了对人体和手部姿态的精细控制。
工作量：</li>
<li>需要大量的数据和计算资源。</li>
<li>训练过程可能耗时。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9b9982465510d1b66a23858c60af4331.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c8ddc4d64a0f61f1a9a17acb134824c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9a9a5ebfedeaeecdc381441fa23504f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2131167109a684b8747fb7451590f0d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0d2c2740f3fa02de0dd80788a7d2df2.jpg" align="middle">
</details>




<h2 id="Spec-Gaussian-Anisotropic-View-Dependent-Appearance-for-3D-Gaussian-Splatting"><a href="#Spec-Gaussian-Anisotropic-View-Dependent-Appearance-for-3D-Gaussian-Splatting" class="headerlink" title="Spec-Gaussian: Anisotropic View-Dependent Appearance for 3D Gaussian   Splatting"></a>Spec-Gaussian: Anisotropic View-Dependent Appearance for 3D Gaussian   Splatting</h2><p><strong>Authors:Ziyi Yang, Xinyu Gao, Yangtian Sun, Yihua Huang, Xiaoyang Lyu, Wen Zhou, Shaohui Jiao, Xiaojuan Qi, Xiaogang Jin</strong></p>
<p>The recent advancements in 3D Gaussian splatting (3D-GS) have not only facilitated real-time rendering through modern GPU rasterization pipelines but have also attained state-of-the-art rendering quality. Nevertheless, despite its exceptional rendering quality and performance on standard datasets, 3D-GS frequently encounters difficulties in accurately modeling specular and anisotropic components. This issue stems from the limited ability of spherical harmonics (SH) to represent high-frequency information. To overcome this challenge, we introduce Spec-Gaussian, an approach that utilizes an anisotropic spherical Gaussian (ASG) appearance field instead of SH for modeling the view-dependent appearance of each 3D Gaussian. Additionally, we have developed a coarse-to-fine training strategy to improve learning efficiency and eliminate floaters caused by overfitting in real-world scenes. Our experimental results demonstrate that our method surpasses existing approaches in terms of rendering quality. Thanks to ASG, we have significantly improved the ability of 3D-GS to model scenes with specular and anisotropic components without increasing the number of 3D Gaussians. This improvement extends the applicability of 3D GS to handle intricate scenarios with specular and anisotropic surfaces. </p>
<p><a href="http://arxiv.org/abs/2402.15870v1">PDF</a> </p>
<p><strong>Summary</strong><br>3D 高斯球体溅射技术 (3D-GS) 在精确建模镜面和各向异性成分方面面临挑战，Spec-Gaussian 方法通过使用各向异性球面高斯外观场来解决这一难题，同时采用粗略到精细的训练策略来增强学习效率并消除过拟合浮动物。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>3D-GS技术在标准数据集上表现出色，但在精确建模镜面和各向异性成分方面遇到困难。</li>
<li>限制球谐函数 (SH) 表示高频信息的局限性导致3D-GS建模困难。</li>
<li>Spec-Gaussian方法采用各向异性球面高斯 (ASG) 外观场来代替SH，提高镜面和各向异性成分建模能力。</li>
<li>粗略到精细的培训策略提高了学习效率，消除了过拟合造成的浮动物。</li>
<li>实验结果表明Spec-Gaussian在渲染质量方面优于现有方法。</li>
<li>ASG显著提升了3D-GS建模镜面和各向异性成分场景的能力，无需增加3D高斯球体数量。</li>
<li>3D-GS技术可扩展至处理镜面和各向异性表面的复杂场景。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：Spec-Gaussian：高斯体渲染中的各向异性视点相关外观</li>
<li>作者：Jiahui Lei, Yinda Zhang, Wenbo Bao, Jingyi Yu, Qiong Yan, Hao Li</li>
<li>单位：香港中文大学（深圳）</li>
<li>关键词：3D 高斯体渲染、各向异性、视点相关外观、神经网络</li>
<li>论文链接：https://arxiv.org/abs/2208.05462</li>
<li>摘要：
（1）研究背景：
近年来，3D 高斯体渲染（3DGS）在实时渲染和高渲染质量方面取得了显著进展。然而，在建模镜面和各向异性成分时，3DGS 仍然面临挑战。</li>
</ol>
<p>（2）过去的方法及其问题：
过去的方法通常使用球谐函数（SH）来建模视点相关外观。然而，SH 在表示高频信息方面能力有限，难以准确建模镜面和各向异性效果。</p>
<p>（3）提出的研究方法：
本文提出 Spec-Gaussian，一种使用各向异性球面高斯（ASG）外观场来建模 3D 高斯体视点相关外观的方法。ASG 比 SH 具有更强的各向异性建模能力，可以更准确地表示镜面和各向异性成分。此外，本文还提出了一个粗到细的训练策略，以提高学习效率并消除过拟合引起的浮动现象。</p>
<p>（4）方法在任务和性能上取得的成就：
实验结果表明，Spec-Gaussian 在渲染质量方面优于现有方法。得益于 ASG，本文方法显著提高了 3DGS 在建模具有镜面和各向异性成分场景的能力，而无需增加 3D 高斯体的数量。这一改进扩展了 3DGS 在处理具有复杂镜面和各向异性表面的场景中的适用性。</p>
<p>7.Methods:(1):提出Spec-Gaussian方法，使用各向异性球面高斯（ASG）外观场来建模3D高斯体视点相关外观，ASG比球谐函数（SH）具有更强的各向异性建模能力；(2):提出粗到细的训练策略，以提高学习效率并消除过拟合引起的浮动现象；(3):通过实验验证Spec-Gaussian在渲染质量方面优于现有方法，显著提高了3DGS在建模具有镜面和各向异性成分场景的能力。</p>
<ol>
<li>结论：
（1）：本文提出Spec-Gaussian，一种使用各向异性球面高斯（ASG）外观场来建模3D高斯体视点相关外观的方法，有效地克服了传统3D-GS在渲染具有镜面高光和各向异性的场景时遇到的挑战。此外，本文创新地实现了粗到细的训练机制，消除了实际场景中的浮动现象。定量和定性实验表明，本文方法不仅赋予3D-GS建模镜面高光和各向异性的能力，而且提高了3D-GS在一般场景中的整体渲染质量，而不会显著影响FPS和存储开销。
（2）：创新点：提出Spec-Gaussian方法，使用各向异性球面高斯（ASG）外观场来建模3D高斯体视点相关外观，ASG比球谐函数（SH）具有更强的各向异性建模能力；提出粗到细的训练策略，以提高学习效率并消除过拟合引起的浮动现象。
性能：在渲染质量方面优于现有方法，显著提高了3DGS在建模具有镜面和各向异性成分场景的能力。
工作量：与现有方法相比，在渲染质量方面有显著提升，而不会显著增加FPS和存储开销。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4090f3d87f7165ab99a3612c93587c40.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-06c68db5202857ec55ce34cb4381f13c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-23504bdddd28cc6cb43a6d3e0229eedd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5e74d0aee36acee6c03305fd883438c.jpg" align="middle">
</details>




<h2 id="Magic-Me-Identity-Specific-Video-Customized-Diffusion"><a href="#Magic-Me-Identity-Specific-Video-Customized-Diffusion" class="headerlink" title="Magic-Me: Identity-Specific Video Customized Diffusion"></a>Magic-Me: Identity-Specific Video Customized Diffusion</h2><p><strong>Authors:Ze Ma, Daquan Zhou, Chun-Hsiao Yeh, Xue-She Wang, Xiuyu Li, Huanrui Yang, Zhen Dong, Kurt Keutzer, Jiashi Feng</strong></p>
<p>Creating content for a specific identity (ID) has shown significant interest in the field of generative models. In the field of text-to-image generation (T2I), subject-driven content generation has achieved great progress with the ID in the images controllable. However, extending it to video generation is not well explored. In this work, we propose a simple yet effective subject identity controllable video generation framework, termed Video Custom Diffusion (VCD). With a specified subject ID defined by a few images, VCD reinforces the identity information extraction and injects frame-wise correlation at the initialization stage for stable video outputs with identity preserved to a large extent. To achieve this, we propose three novel components that are essential for high-quality ID preservation: 1) an ID module trained with the cropped identity by prompt-to-segmentation to disentangle the ID information and the background noise for more accurate ID token learning; 2) a text-to-video (T2V) VCD module with 3D Gaussian Noise Prior for better inter-frame consistency and 3) video-to-video (V2V) Face VCD and Tiled VCD modules to deblur the face and upscale the video for higher resolution.   Despite its simplicity, we conducted extensive experiments to verify that VCD is able to generate stable and high-quality videos with better ID over the selected strong baselines. Besides, due to the transferability of the ID module, VCD is also working well with finetuned text-to-image models available publically, further improving its usability. The codes are available at <a href="https://github.com/Zhen-Dong/Magic-Me">https://github.com/Zhen-Dong/Magic-Me</a>. </p>
<p><a href="http://arxiv.org/abs/2402.09368v1">PDF</a> </p>
<p><strong>Summary</strong><br>用少量图像指定主体 ID，VCD 框架通过强化身份信息提取和注入帧间相关性，生成主体身份可控的高质量视频。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>提出 VCD 框架用于主体身份可控视频生成，通过指定几个图像定义主体 ID。</li>
<li>ID 模块利用提示到分割训练， disentangle ID 信息和背景噪声，更准确地学习 ID 标记。</li>
<li>T2V VCD 模块使用 3D 高斯噪声先验，以获得更好的帧间一致性。</li>
<li>V2V Face VCD 和 Tiled VCD 模块用于模糊面部和提升视频分辨率。</li>
<li>VCD 在选定的强基线上生成稳定、高质量且 ID 更佳的视频。</li>
<li>ID 模块可迁移，VCD 可与公开提供的微调文本到图像模型配合使用，进一步提高其可用性。</li>
<li>提供了 VCD 的代码：<a href="https://github.com/Zhen-Dong/Magic-Me。">https://github.com/Zhen-Dong/Magic-Me。</a></li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：Magic-Me: 身份特定视频定制化扩散</li>
<li>作者：Ze Ma<em>1, Daquan Zhou</em>†1, Chun-Hsiao Yeh2, Xue-She Wang1, Xiuyu Li2, Huanrui Yang2, Zhen Dong†2, Kurt Keutzer2, Jiashi Feng1</li>
<li>第一作者单位：字节跳动公司</li>
<li>关键词：身份特定视频生成、文本到视频、视频定制化扩散</li>
<li>论文链接：https://arxiv.org/abs/2402.09368
   Github代码链接：https://github.com/Zhen-Dong/Magic-Me</li>
<li>摘要：
   (1): 研究背景：文本到视频生成取得了显著进展，但精确控制生成内容仍然具有挑战性。身份特定生成在许多场景中很重要，例如电影制作和广告。
   (2): 过去方法：之前的研究主要集中在利用图像参考控制风格和动作，或通过视频编辑进行定制化生成。这些方法的重点不在于身份特定控制。
   (3): 研究方法：本文提出了一种简单的但有效的身份特定视频生成框架，称为视频定制化扩散（VCD）。VCD 使用身份模块提取身份信息，并在初始化阶段注入帧间相关性，以生成具有稳定身份的视频输出。
   (4): 性能：VCD 在身份保留方面优于选定的强基线。此外，由于身份模块的可迁移性，VCD 也适用于公开可用的微调文本到图像模型，进一步提高了其可用性。</li>
</ol>
<p>7.Methods：
（1）提出用于 VCD 的预处理模块，以及 ID 模块和运动模块，如图 3 所示。此外，我们提供了一个可选模块，利用 ControlNet Tile 来上采样视频并生成高分辨率内容。我们的方法结合了 AnimateDiff [18] 中现成的运动模块，并通过我们提出的 3D 高斯噪声先验进行了增强，如第 4.1 节所述。ID 模块具有带掩码损失和提示到分割的扩展 ID 令牌，在第 4.2 节中介绍。在第 4.3 节中，我们介绍了两个 V2V VCD 管道，FaceVCD 和 TiledVCD。
（2）为了简单起见，我们应用我们的无训练 3D 高斯噪声先验到现成的运动模块 [18]，以减轻推理期间的曝光偏差。所选的运动模块将网络扩展到包含时间维度。它将 2D 卷积和注意力层转换为时间伪 3D 层 [23]，遵循方程式 2 中概述的训练目标。3D 高斯噪声先验。对于包含 f 帧的视频，3D 高斯噪声先验从多元高斯分布 N(0, Σf(γ)) 中采样。这里，Σf(γ) 表示由 γ∈(0,1) 参数化的协方差矩阵。Σf(γ)=1γγ2···γf−1γ1γ···γf−2γ2γ1···γf−3...............γf−1γf−2γf−3···1。(4)
（3）上面描述的协方差确保初始化的 3D 噪声在 m 和 n 帧之间的相同位置表现出 γ|m−n| 的协方差。超参数 γ 表示稳定性和运动幅度之间的权衡，如图 4 所示。较低的 γ 值会导致运动剧烈但稳定性降低的视频，而较高的 γ 会导致幅度减小的更稳定的运动。
（4）ID 模块 VAE 提示到分割 Lmask<v*>man 主体是一个穿着粉色 T 恤的人图 5.扩展 ID 令牌学习。通过提示到分割，针对掩码主体区域对扩展 ID 令牌进行优化。虽然以前的工作已经探索了 T2I 身份定制的令牌嵌入 [16,58] 和权重微调 [11,17,31,48]，但很少有人深入研究 T2V 生成中的身份定制。我们观察到，虽然像 CustomDiffusion [31] 或 LoRA [25] 这样的权重调整方法在图像生成中实现了精确的身份，但生成的视频通常显示出有限的多样性和用户输入对齐。扩展 ID 令牌。我们建议使用扩展 ID 令牌仅与条件编码交互，并更好地保留身份的视觉特征，如图 5 所示。与原始 LoRA 相比，这种方法可以产生更好的视频质量，如表 1 所示。此外，提出的 ID 模块只需要 16KB 的存储空间，与 Stable Diffusion 中所需的参数 3.6G 或 SVDiff [20] 中的 1.7MB 相比，这是一个非常紧凑的参数空间。</v*></p>
<ol>
<li>结论：
（1）本工作的重要意义：
本文提出的 Video Custom Diffusion（VCD）框架旨在解决可控视频生成中主体身份控制的挑战。通过融合身份信息和帧间相关性，VCD 为生成不仅在帧间保持主体身份，而且具有稳定性和清晰度的视频铺平了道路。我们新颖的贡献，包括用于精确身份分离的 ID 模块、用于增强帧一致性的 T2V VCD 模块以及用于提高视频质量的 V2V 模块，共同为视频内容中的身份保留建立了新的标准。我们进行的广泛实验肯定了 VCD 在生成高质量、稳定且保留主体身份的视频方面的优势。此外，我们的 ID 模块适用于现有的文本到图像模型，增强了 VCD 的实用性，使其适用于广泛的应用。
（2）本文的创新点、性能和工作量总结：
创新点：</li>
<li>提出了一种用于视频定制扩散的框架，该框架结合了身份信息和帧间相关性，以生成具有稳定身份的视频。</li>
<li>设计了一个 ID 模块，用于从文本提示中提取身份信息并将其注入视频生成过程中。</li>
<li>提出了一种 T2V VCD 模块，用于增强帧间一致性，生成具有平滑运动和清晰细节的视频。
性能：</li>
<li>VCD 在身份保留方面优于选定的强基线，生成的高质量视频在帧间保持了主体身份。</li>
<li>由于 ID 模块的可迁移性，VCD 也适用于公开可用的微调文本到图像模型，进一步提高了其可用性。
工作量：</li>
<li>VCD 的实现相对简单，仅需要少量额外的计算开销。</li>
<li>ID 模块具有紧凑的参数空间，仅需 16KB 的存储空间，使其易于部署和使用。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e6a21bfcb16c6c0deb1d0539ef94af7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9fb6739198960204ae02b3df3b1108f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3af883ea390b349d783415082941342e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f79fc49019e994a2b5124fecafb23683.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ffb39f913681e339c8d1aa9719f971cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ad7c82a7b238a18cf1ae3935cfce436.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e509076266dabf0c8283fba23dba850.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ef1ee7f0f72cd6bec6307311ed8330ee.jpg" align="middle">
</details>




<h2 id="SGS-SLAM-Semantic-Gaussian-Splatting-For-Neural-Dense-SLAM"><a href="#SGS-SLAM-Semantic-Gaussian-Splatting-For-Neural-Dense-SLAM" class="headerlink" title="SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM"></a>SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM</h2><p><strong>Authors:Mingrui Li, Shuhong Liu, Heng Zhou, Guohao Zhu, Na Cheng, Hongyu Wang</strong></p>
<p>Semantic understanding plays a crucial role in Dense Simultaneous Localization and Mapping (SLAM). Recent advancements that integrate Gaussian Splatting into SLAM systems have demonstrated its effectiveness in generating high-quality renderings. Building on this progress, we propose SGS-SLAM which provides precise 3D semantic segmentation alongside high-fidelity reconstructions. Specifically, we propose to employ multi-channel optimization during the mapping process, integrating appearance, geometric, and semantic constraints with key-frame optimization to enhance reconstruction quality. Extensive experiments demonstrate that SGS-SLAM delivers state-of-the-art performance in camera pose estimation, map reconstruction, and semantic segmentation. It outperforms existing methods by a large margin meanwhile preserves real-time rendering ability. </p>
<p><a href="http://arxiv.org/abs/2402.03246v2">PDF</a> </p>
<p><strong>摘要</strong><br>SGS-SLAM 采用多通道优化，将外观、几何和语义约束融入关键帧优化中，实现了高精度 3D 语义分割和高保真重建。</p>
<p><strong>关键要点</strong></p>
<ul>
<li>利用高斯喷射将语义理解融入 SLAM 系统，生成高质量渲染效果。</li>
<li>采用多通道优化，融合外观、几何和语义约束，提升重建质量。</li>
<li>在相机位姿估计、地图重建和语义分割方面达到最先进性能。</li>
<li>显著优于现有方法，同时保持实时渲染能力。</li>
<li>扩展了 SLAM 系统的应用范围，使其在语义理解和重建任务中表现出色。</li>
<li>为室内或室外环境的高保真重建和交互式探索提供了新的可能性。</li>
<li>为自动驾驶、机器人导航和增强现实等领域提供了新的技术支持。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>标题：SGS-SLAM：用于神经稠密 SLAM 的语义高斯斑点绘制</li>
<li>作者：Mingrui Li、Shuhong Liu、Heng Zhou、Guohao Zhu、Na Cheng、Hongyu Wang</li>
<li>单位：大连理工大学计算机科学与技术系</li>
<li>关键词：SLAM、3D 重建、3D 语义分割</li>
<li>论文链接：https://arxiv.org/abs/2402.03246，Github 代码链接：无</li>
<li>摘要：
(1)：研究背景：语义理解在稠密 SLAM 中至关重要，而将高斯斑点绘制集成到 SLAM 系统中的最新进展已证明其在生成高质量渲染方面的有效性。
(2)：过去方法及问题：传统视觉 SLAM 系统擅长使用点云和体素进行稀疏重建，但无法进行稠密重建。基于学习的 SLAM 方法可以提取用于高质量表示的稠密几何信息，但它们容易受到噪声和异常值的影响。神经辐射场 (NeRF) 启发的 SLAM 方法进一步提高了重建质量，但它们通常不包含语义信息。
(3)：研究方法：本文提出 SGS-SLAM，它在高保真重建的同时提供精确的 3D 语义分割。SGS-SLAM 在映射过程中采用多通道优化，将外观、几何和语义约束与关键帧优化相结合，以增强重建质量。
(4)：任务和性能：SGS-SLAM 在相机姿态估计、地图重建和语义分割方面都取得了最先进的性能。它以很大的优势优于现有方法，同时保留了实时渲染能力。</li>
</ol>
<p>方法：
(1): SGS-SLAM采用多通道高斯表示，将外观、几何和语义约束与关键帧优化相结合，以增强重建质量。
(2): 跟踪过程估计每一帧的相机位姿，同时保持场景参数固定。映射优化基于估计的相机位姿优化场景表示。
(3): 场景表示使用高斯影响函数 f(·)，其中 σ 表示不透明度，μ 表示中心位置，r 表示半径。每个高斯还携带 RGB 颜色 ci。
(4): 使用渲染方法将高斯渲染成 2D 图像，通过沿深度维度逼近影响函数 f(·) 的积分投影来完成。
(5): 通过对高斯进行深度排序并执行从前到后的体积渲染，可以组合所有高斯对该像素的影响。
(6): 像素级渲染颜色 Cpix 是每个高斯颜色 ci 的总和，并根据影响函数 f2Di,pix 加权，乘以遮挡项。
(7): 深度可以渲染为：Dpix = ∑i=1 di f2Di,pix i−1 ∏j=1 (1−f2Dj,pix)，其中 di 表示每个高斯的深度。
(8): 通过设置 di=1，可以计算出轮廓 Silpix = Dpix(di=1)，这有助于确定像素是否在当前视图中可见。
(9): 在映射过程中，将 2D 语义标签分配给高斯参数的特定通道以表示其语义标签和颜色。
(10): 渲染过程中，可以从重建的 3D 场景渲染 2D 语义图：Spix = ∑i=1 si f2Di,pix i−1 ∏j=1 (1−f2Dj,pix)，其中 si = [ri, gi, bi]T 表示与高斯关联的语义颜色。
(11): 相机位姿估计通过最小化跟踪损失来实现，该损失表示真实颜色、深度图像和语义图与其可微渲染视图之间的差异。
(12): 关键帧选择和加权：在跟踪阶段，同时识别和存储关键帧。这些关键帧提供了对象的不同视图，对于映射优化 3D 场景重建至关重要。
(13): SGS-SLAM 在恒定时间间隔内捕获和存储关键帧。随后，根据几何和语义约束选择与当前帧关联的关键帧。
(14): 首先进行基于几何的初始选择，然后进行基于语义的二次筛选。
(15): 对于每个关键帧，计算不确定性分数 U(t) = e−τt，其中 t 表示关键帧的时间戳，τ 为衰减系数。
(16): 使用此不确定性分数对映射损失 Lmapping 加权。
(17): 地图重建：场景使用三个不同通道的高斯建模：它们的均值坐标表示场景的几何信息，它们的外观颜色描绘了场景的视觉外观，它们的语义颜色表示对象的语义标签。
(18): 在高斯致密化和优化过程中，跨通道的这些参数被联合优化，而从跟踪中确定的相机位姿保持固定。
(19): 从第一帧开始，所有像素都有助于初始化地图。
(20): 在新时间步的地图重建过程中，将新高斯引入到地图中，这些区域要么密度不足，要么显示先前估计的地图前面的新几何形状。
(21): 通过将掩码应用于像素来调节新高斯的添加，其中要么 (i) 轮廓值 Silpix 低于某个阈值，表示可见性高度不确定，要么 (ii) 真实深度远小于估计深度，表明存在新的几何实体。
(22): 致密化后，通过最小化映射损失来优化地图参数：Lmapping = U ∑pix λD |DGTpix−Dpix| + λC L C + λS L S。</p>
<ol>
<li>结论：
（1）本工作的重要意义：SGS-SLAM 在进行高保真重建的同时提供了精确的 3D 语义分割，在相机姿态估计、地图重建和语义分割方面都取得了最先进的性能，为神经稠密 SLAM 提供了一种新的解决方案。
（2）本文的优缺点总结：
创新点：SGS-SLAM 采用多通道高斯表示，将外观、几何和语义约束与关键帧优化相结合，增强了重建质量，并首次将语义信息集成到神经稠密 SLAM 系统中。
性能：SGS-SLAM 在相机姿态估计、地图重建和语义分割方面都取得了最先进的性能，以很大的优势优于现有方法，同时保留了实时渲染能力。
工作量：SGS-SLAM 的实现需要大量的计算资源和数据，这可能会限制其在某些资源受限的应用中的使用。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-49d695fd07273ec0ead5f03d33095327.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9e64fa80d8afdcf89c98cfd50dd717f.jpg" align="middle">
</details>




]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>3DGS</tag>
      </tags>
  </entry>
  <entry>
    <title>Diffusion Models</title>
    <url>/2024/02/29/Paper/2024-02-29/Diffusion%20Models/</url>
    <content><![CDATA[<blockquote>
<p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-02-29-更新"><a href="#2024-02-29-更新" class="headerlink" title="2024-02-29 更新"></a>2024-02-29 更新</h1><h2 id="Objective-and-Interpretable-Breast-Cosmesis-Evaluation-with-Attention-Guided-Denoising-Diffusion-Anomaly-Detection-Model"><a href="#Objective-and-Interpretable-Breast-Cosmesis-Evaluation-with-Attention-Guided-Denoising-Diffusion-Anomaly-Detection-Model" class="headerlink" title="Objective and Interpretable Breast Cosmesis Evaluation with Attention   Guided Denoising Diffusion Anomaly Detection Model"></a>Objective and Interpretable Breast Cosmesis Evaluation with Attention   Guided Denoising Diffusion Anomaly Detection Model</h2><p><strong>Authors:Sangjoon Park, Yong Bae Kim, Jee Suk Chang, Seo Hee Choi, Hyungjin Chung, Ik Jae Lee, Hwa Kyung Byun</strong></p>
<p>As advancements in the field of breast cancer treatment continue to progress, the assessment of post-surgical cosmetic outcomes has gained increasing significance due to its substantial impact on patients’ quality of life. However, evaluating breast cosmesis presents challenges due to the inherently subjective nature of expert labeling. In this study, we present a novel automated approach, Attention-Guided Denoising Diffusion Anomaly Detection (AG-DDAD), designed to assess breast cosmesis following surgery, addressing the limitations of conventional supervised learning and existing anomaly detection models. Our approach leverages the attention mechanism of the distillation with no label (DINO) self-supervised Vision Transformer (ViT) in combination with a diffusion model to achieve high-quality image reconstruction and precise transformation of discriminative regions. By training the diffusion model on unlabeled data predominantly with normal cosmesis, we adopt an unsupervised anomaly detection perspective to automatically score the cosmesis. Real-world data experiments demonstrate the effectiveness of our method, providing visually appealing representations and quantifiable scores for cosmesis evaluation. Compared to commonly used rule-based programs, our fully automated approach eliminates the need for manual annotations and offers objective evaluation. Moreover, our anomaly detection model exhibits state-of-the-art performance, surpassing existing models in accuracy. Going beyond the scope of breast cosmesis, our research represents a significant advancement in unsupervised anomaly detection within the medical domain, thereby paving the way for future investigations. </p>
<p><a href="http://arxiv.org/abs/2402.18362v1">PDF</a> </p>
<p><strong>Summary</strong><br>利用无人监督方法，自动评估乳腺癌术后外观，为提高患者生活质量提供新途径。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>采用无监督异常检测视角，无需标记即可评估外观。</li>
<li>使用蒸馏无标签 (DINO) 自监督视觉 Transformer (ViT) 的注意力机制，实现高质量图像重建和判别区域的精确转换。</li>
<li>在以正常外观为主的未标记数据上训练扩散模型。</li>
<li>提供视觉上吸引人的表示和可量化的分数，用于外观评估。</li>
<li>消除人工标注的需要，提供客观评估。</li>
<li>在准确性方面超过现有模型，展现出最先进的性能。</li>
<li>为医学领域的无监督异常检测提供了重大进展。</li>
<li>探索无监督外观评估在其他医疗领域的潜力。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>Title: 基于注意力引导去噪扩散的客观可解释乳房美观评估</li>
<li>Authors: Sangjoon Park, YongBae Kim, JeeSuk Chang, SeoHee Choi, Hyungjin Chung, IkJae Lee, HwaKyung Byun</li>
<li>Affiliation: 韩国首尔延世大学医学院放射肿瘤科</li>
<li>Keywords: 扩散模型、异常检测、视觉 Transformer、乳房美观</li>
<li>Urls: Paper, Github: None</li>
<li>
<p>Summary:
(1): 乳房癌术后美观评估对患者生活质量影响很大，但传统方法存在主观性强、依赖人工标注等问题。
(2): 现有方法依赖专家标注，存在成本高、标注偏差、模型过拟合、可解释性差等问题。
(3): 本文提出一种名为 AG-DDAD 的创新架构，利用扩散模型的高质量生成能力和 DINO 视觉 Transformer 注意力的显著特征识别能力。该模型可以在无监督的方式下训练，利用来自 1,237 名主要为正常美观（优秀到良好）患者的未标记数据，无需专家标注和人工勾勒。AG-DDAD 通过比较正常美观情况下预期的恢复结果，提供直接且出色的可视化结果，为不良美观成因提供有价值的见解。
(4): 在一个经过精心整理的包含 300 名接受乳腺癌保乳手术患者的数据集上进行的实验表明，本文模型优于传统的基于规则的方法和其他最先进的异常检测方法。</p>
</li>
<li>
<p>方法：
（1）：提出一种名为 AG-DDAD 的创新架构，该架构利用扩散模型的高质量生成能力和 DINO 视觉 Transformer 注意力的显著特征识别能力；
（2）：AG-DDAD 在无监督的方式下训练，利用来自主要为正常美观（优秀到良好）患者的未标记数据，无需专家标注和人工勾勒；
（3）：AG-DDAD 通过比较正常美观情况下预期的恢复结果，提供直接且出色的可视化结果，为不良美观成因提供有价值的见解。</p>
</li>
<li>
<p>结论：
（1）：本文提出了一种基于注意力引导去噪扩散的客观可解释乳房美观评估方法，该方法利用了扩散模型的高质量生成能力和DINO视觉Transformer注意力的显著特征识别能力，在无监督的方式下训练，无需专家标注和人工勾勒，通过比较正常美观情况下预期的恢复结果，提供直接且出色的可视化结果，为不良美观成因提供有价值的见解。
（2）：创新点：</p>
</li>
<li>提出了一种基于注意力引导去噪扩散的客观可解释乳房美观评估方法，该方法利用了扩散模型的高质量生成能力和DINO视觉Transformer注意力的显著特征识别能力。</li>
<li>该方法在无监督的方式下训练，无需专家标注和人工勾勒，通过比较正常美观情况下预期的恢复结果，提供直接且出色的可视化结果，为不良美观成因提供有价值的见解。
性能：</li>
<li>在一个经过精心整理的包含300名接受乳腺癌保乳手术患者的数据集上进行的实验表明，本文模型优于传统的基于规则的方法和其他最先进的异常检测方法。
工作量：</li>
<li>该方法需要比传统的分类器模型略多的时间，评估单个患者的美观大约需要15秒，而简单的分类器模型可以在1秒内产生结果。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-678c2254dd6a3d39889bef35f9067c05.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cfa8a6039aebee57a2721ad761165bd3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d6811aab9ac5a0e1edc535c928e3bd0f.jpg" align="middle">
</details>




<h2 id="FineDiffusion-Scaling-up-Diffusion-Models-for-Fine-grained-Image-Generation-with-10-000-Classes"><a href="#FineDiffusion-Scaling-up-Diffusion-Models-for-Fine-grained-Image-Generation-with-10-000-Classes" class="headerlink" title="FineDiffusion: Scaling up Diffusion Models for Fine-grained Image   Generation with 10,000 Classes"></a>FineDiffusion: Scaling up Diffusion Models for Fine-grained Image   Generation with 10,000 Classes</h2><p><strong>Authors:Ziying Pan, Kun Wang, Gang Li, Feihong He, Xiwang Li, Yongxuan Lai</strong></p>
<p>The class-conditional image generation based on diffusion models is renowned for generating high-quality and diverse images. However, most prior efforts focus on generating images for general categories, e.g., 1000 classes in ImageNet-1k. A more challenging task, large-scale fine-grained image generation, remains the boundary to explore. In this work, we present a parameter-efficient strategy, called FineDiffusion, to fine-tune large pre-trained diffusion models scaling to large-scale fine-grained image generation with 10,000 categories. FineDiffusion significantly accelerates training and reduces storage overhead by only fine-tuning tiered class embedder, bias terms, and normalization layers’ parameters. To further improve the image generation quality of fine-grained categories, we propose a novel sampling method for fine-grained image generation, which utilizes superclass-conditioned guidance, specifically tailored for fine-grained categories, to replace the conventional classifier-free guidance sampling. Compared to full fine-tuning, FineDiffusion achieves a remarkable 1.56x training speed-up and requires storing merely 1.77% of the total model parameters, while achieving state-of-the-art FID of 9.776 on image generation of 10,000 classes. Extensive qualitative and quantitative experiments demonstrate the superiority of our method compared to other parameter-efficient fine-tuning methods. The code and more generated results are available at our project website: <a href="https://finediffusion.github.io/">https://finediffusion.github.io/</a>. </p>
<p><a href="http://arxiv.org/abs/2402.18331v1">PDF</a> </p>
<p><strong>Summary</strong><br>通过微调预训练扩散模型，以参数高效策略实现针对 10,000 个细粒度类别的大规模图像生成</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>提出 FineDiffusion，将大规模扩散模型缩小到细粒度图像生成中</li>
<li>只微调分类嵌入、偏置项和归一化层的参数，大幅提升训练速度和存储效率</li>
<li>提出针对细粒度类别的超类条件引导采样方法，提升图像生成质量</li>
<li>与完全微调相比，FineDiffusion 训练速度提升 1.56 倍，所需存储参数仅为原模型的 1.77%</li>
<li>在 10,000 个类别的图像生成上取得最先进的 FID 为 9.776</li>
<li>大量定性和定量实验验证了该方法的优越性</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>标题：FineDiffusion：将扩散模型扩展到 10,000 类别的细粒度图像生成</li>
<li>作者：Ziying Pan, Kun Wang, Gang Li, Feihong He, Xiwang Li, Yongxuan Lai</li>
<li>第一作者单位：厦门大学</li>
<li>关键词：Diffusion Models, Fine-grained Image Generation, Parameter-efficient Fine-tuning</li>
<li>论文链接：https://arxiv.org/abs/2402.18331
   Github 代码链接：None</li>
<li>摘要：
   （1）：研究背景：基于扩散模型的图像生成以产生高质量和多样化的图像而闻名。然而，大多数先前的努力都集中在为一般类别生成图像，例如 ImageNet-1k 中的 1000 个类别。一个更具挑战性的任务，即大规模细粒度图像生成，仍然是需要探索的边界。
   （2）：过去的方法：过去的方法主要集中在一般类别的图像生成，而对于细粒度图像生成，需要模型对高度相似的细粒度类别中的细微差异（例如鸟类的羽毛纹理）进行复杂的建模。从头开始训练用于大规模细粒度图像生成的扩散模型需要更大的计算资源和训练迭代。
   （3）：研究方法：本文提出了一种新的微调方法 FineDiffusion，它可以通过微调预训练模型的一小部分参数，有效地微调大型预训练图像生成扩散模型，以进行大规模细粒度图像生成。
   （4）：方法性能：与完全微调相比，FineDiffusion 实现了显着的 1.56 倍训练加速，并且只需要存储 1.77% 的总模型参数，同时在 10,000 个类别的图像生成上实现了 9.776 的最先进 FID。广泛的定性和定量实验表明，与其他参数有效的微调方法相比，本文方法具有优越性。</li>
</ol>
<p>7.方法：(1)提出了一种新的微调方法FineDiffusion，该方法通过微调预训练模型的一小部分参数，有效地微调大型预训练图像生成扩散模型，以进行大规模细粒度图像生成；(2)提出了一种分层类标签编码策略，该策略同时对超类和子类标签进行编码；(3)同时微调偏差和归一化项以及分层嵌入器，以学习全局数据集的分布特征；(4)引入了一种分层无分类器引导采样方法，该方法利用超类条件信息来增强对生成图像的控制。</p>
<ol>
<li>结论：
（1）：本文首次尝试将扩散模型扩展到 10,000 类的细粒度图像生成。我们引入了 FineDiffusion，这是一种高效的参数微调方法，可以微调预训练模型的关键组件，包括分层标签嵌入、偏差项和归一化项。我们的方法大幅减少了训练和存储开销。此外，我们引入了一种细粒度无分类器引导采样技术，利用分层数据标签信息来有效增强细粒度图像生成的性能。充分的定性和定量结果证明了我们方法与其他方法相比的优越性。
（2）：创新点：提出了一种新的微调方法 FineDiffusion，该方法通过微调预训练模型的一小部分参数，有效地微调大型预训练图像生成扩散模型，以进行大规模细粒度图像生成；提出了分层类标签编码策略，该策略同时对超类和子类标签进行编码；同时微调偏差和归一化项以及分层嵌入器，以学习全局数据集的分布特征；引入了一种分层无分类器引导采样方法，该方法利用超类条件信息来增强对生成图像的控制。
性能：与完全微调相比，FineDiffusion 实现了显着的 1.56 倍训练加速，并且只需要存储 1.77% 的总模型参数，同时在 10,000 个类别的图像生成上实现了 9.776 的最先进 FID。广泛的定性和定量实验表明，与其他参数有效的微调方法相比，本文方法具有优越性。
工作量：与从头开始训练用于大规模细粒度图像生成的扩散模型相比，FineDiffusion 可以显着减少计算资源和训练迭代。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f68a4db99ea4f9179538c6c4b4d7c7ce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4e768fecf2a73ce9e4c8b13ef7c8cd6a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c0d4b61db744892b76754513d9f6676.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-665dc312a2eacee1bb375efacd7d609c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d25afe2f19082c3abc80d90affd76466.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-68e2a9d895710b3df489a49501a85625.jpg" align="middle">
</details>




<h2 id="Balancing-Act-Distribution-Guided-Debiasing-in-Diffusion-Models"><a href="#Balancing-Act-Distribution-Guided-Debiasing-in-Diffusion-Models" class="headerlink" title="Balancing Act: Distribution-Guided Debiasing in Diffusion Models"></a>Balancing Act: Distribution-Guided Debiasing in Diffusion Models</h2><p><strong>Authors:Rishubh Parihar, Abhijnya Bhat, Saswat Mallick, Abhipsa Basu, Jogendra Nath Kundu, R. Venkatesh Babu</strong></p>
<p>Diffusion Models (DMs) have emerged as powerful generative models with unprecedented image generation capability. These models are widely used for data augmentation and creative applications. However, DMs reflect the biases present in the training datasets. This is especially concerning in the context of faces, where the DM prefers one demographic subgroup vs others (eg. female vs male). In this work, we present a method for debiasing DMs without relying on additional data or model retraining. Specifically, we propose Distribution Guidance, which enforces the generated images to follow the prescribed attribute distribution. To realize this, we build on the key insight that the latent features of denoising UNet hold rich demographic semantics, and the same can be leveraged to guide debiased generation. We train Attribute Distribution Predictor (ADP) - a small mlp that maps the latent features to the distribution of attributes. ADP is trained with pseudo labels generated from existing attribute classifiers. The proposed Distribution Guidance with ADP enables us to do fair generation. Our method reduces bias across single/multiple attributes and outperforms the baseline by a significant margin for unconditional and text-conditional diffusion models. Further, we present a downstream task of training a fair attribute classifier by rebalancing the training set with our generated data. </p>
<p><a href="http://arxiv.org/abs/2402.18206v1">PDF</a> CVPR 2024. Project Page : <a href="https://ab-34.github.io/balancing_act/">https://ab-34.github.io/balancing_act/</a></p>
<p><strong>Summary</strong><br>去除扩散模型中的偏见，无需额外数据或模型重新训练。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>扩散模型（DM）存在偏见，表现为对特定人口亚组（如女性）的偏好。</li>
<li>分布引导是一种无偏 DM 的方法，无需额外数据或重新训练。</li>
<li>分布引导利用去噪 UNet 的潜在特征中丰富的语义信息。</li>
<li>属性分布预测器 (ADP) 将潜在特征映射到属性分布。</li>
<li>ADP 使用现有属性分类器生成的伪标签进行训练。</li>
<li>分布引导和 ADP 实现了公平生成，显著优于基线。</li>
<li>通过使用生成的数据重新平衡训练集，可以训练公平的属性分类器。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>标题：平衡行为：扩散模型中的分布引导去偏</li>
<li>作者：Rishubh Parihar*, Abhijnya Bhat∗, Saswat Mallick, Abhipsa Basu, Jogendra Nath Kundu, R. Venkatesh Babu</li>
<li>隶属：印度科学院，班加罗尔</li>
<li>关键词：Diffusion Models, Debiasing, Distribution Guidance, Attribute Distribution Predictor, Fair Generation</li>
<li>链接：https://arxiv.org/abs/2402.18206</li>
<li>
<p>摘要：
（1）研究背景：扩散模型（DM）作为强大的生成模型，在图像生成方面表现出色，但它们会反映训练数据集中的偏见，特别是对于人脸，DM 偏好某些人口统计学亚组（例如女性比男性）。
（2）过去方法：现有去偏方法需要额外数据或模型重新训练。
（3）研究方法：本文提出分布引导，通过强制生成图像遵循规定的属性分布来对 DM 进行去偏。通过训练属性分布预测器 (ADP) 来映射潜在特征到属性分布，ADP 使用现有属性分类器生成的伪标签进行训练。
（4）方法性能：该方法在无条件和文本条件扩散模型上减少了单一/多属性的偏差，并且优于基线方法。此外，本文还提出了一种通过使用生成数据重新平衡训练集来训练公平属性分类器的下游任务。</p>
</li>
<li>
<p>方法：
(1): 提出分布引导方法，通过强制生成图像遵循规定的属性分布来对扩散模型（DM）进行去偏。
(2): 训练属性分布预测器（ADP）来映射潜在特征到属性分布，ADP使用现有属性分类器生成的伪标签进行训练。
(3): 在无条件和文本条件扩散模型上评估该方法，减少了单一/多属性的偏差，并优于基线方法。
(4): 提出了一种通过使用生成数据重新平衡训练集来训练公平属性分类器的下游任务。</p>
</li>
<li>
<p>结论：
（1）本文的意义：本文提出了一种无需重新训练即可减轻预训练扩散模型偏差的方法，仅给定所需的参考属性分布。我们提出了一种新颖的方法，利用分布引导，联合引导一批图像遵循参考属性分布。所提出的方法是有效的，并且在
（2）创新点：本文的创新点在于提出了一种新的分布引导方法，通过强制生成图像遵循规定的属性分布来对扩散模型进行去偏。
性能：本文的方法在无条件和文本条件扩散模型上减少了单一/多属性的偏差，并且优于基线方法。
工作量：本文的方法需要训练一个属性分布预测器，该预测器使用现有属性分类器生成的伪标签进行训练。训练属性分布预测器的工作量取决于训练数据的规模和属性的数量。</p>
</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-05a1a956ee3a51fe0c06ffc4859c7231.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16ae5c5f9f522148622d40f8f3f15f86.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-46f6a987113095ab338596820ca6e653.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f32e1f0036b8646f3ffad99a82575f09.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1128b65d6c33c58a2f6b04087adf31b0.jpg" align="middle">
</details>




<h2 id="Coarse-to-Fine-Latent-Diffusion-for-Pose-Guided-Person-Image-Synthesis"><a href="#Coarse-to-Fine-Latent-Diffusion-for-Pose-Guided-Person-Image-Synthesis" class="headerlink" title="Coarse-to-Fine Latent Diffusion for Pose-Guided Person Image Synthesis"></a>Coarse-to-Fine Latent Diffusion for Pose-Guided Person Image Synthesis</h2><p><strong>Authors:Yanzuo Lu, Manlin Zhang, Andy J Ma, Xiaohua Xie, Jian-Huang Lai</strong></p>
<p>Diffusion model is a promising approach to image generation and has been employed for Pose-Guided Person Image Synthesis (PGPIS) with competitive performance. While existing methods simply align the person appearance to the target pose, they are prone to overfitting due to the lack of a high-level semantic understanding on the source person image. In this paper, we propose a novel Coarse-to-Fine Latent Diffusion (CFLD) method for PGPIS. In the absence of image-caption pairs and textual prompts, we develop a novel training paradigm purely based on images to control the generation process of the pre-trained text-to-image diffusion model. A perception-refined decoder is designed to progressively refine a set of learnable queries and extract semantic understanding of person images as a coarse-grained prompt. This allows for the decoupling of fine-grained appearance and pose information controls at different stages, and thus circumventing the potential overfitting problem. To generate more realistic texture details, a hybrid-granularity attention module is proposed to encode multi-scale fine-grained appearance features as bias terms to augment the coarse-grained prompt. Both quantitative and qualitative experimental results on the DeepFashion benchmark demonstrate the superiority of our method over the state of the arts for PGPIS. Code is available at <a href="https://github.com/YanzuoLu/CFLD">https://github.com/YanzuoLu/CFLD</a>. </p>
<p><a href="http://arxiv.org/abs/2402.18078v1">PDF</a> Accepted by CVPR 2024</p>
<p><strong>Summary</strong><br> 提出了一种粗到细的潜在扩散（CFLD）方法，利用图像而非文本提示，控制预训练文本到图像扩散模型，实现姿势引导的图像合成。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>提出 CFLD 方法，改善了 PGPIS 中姿势引导图像合成的效果。</li>
<li>使用纯图像训练范式，无需图像字幕或文本提示。</li>
<li>设计了一个感知精炼解码器，逐步优化查询并提取人物图像的语义理解。</li>
<li>将外貌和姿势信息控制解耦，避免过度拟合。</li>
<li>提出混合粒度注意力模块，对多尺度外观特征进行编码，增强粗粒度提示。</li>
<li>在 DeepFashion 数据集上，定量和定性实验结果证明了 CFLD 的优越性。</li>
<li>代码已开源。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：粗到细的潜在扩散用于姿态引导的人物图像合成</li>
<li>作者：Lu Yanzuo, Zhang Manlin, Ma Andy J, Xie Xiaohua, Lai Jianhuang</li>
<li>单位：中山大学计算机科学与工程学院</li>
<li>关键词：姿态引导、人物图像合成、潜在扩散模型、粗到细、语义理解</li>
<li>论文链接：https://arxiv.org/abs/2402.18078
   Github 代码链接：https://github.com/YanzuoLu/CFLD</li>
<li>摘要：
(1) 研究背景：姿态引导的人物图像合成旨在将源人物图像转换为特定的目标姿态，同时尽可能保留外观。它在电影制作、虚拟现实和时尚电子商务等领域有广泛的应用。
(2) 过去的方法：现有基于生成对抗网络 (GAN) 的方法容易出现极小极大训练目标的不稳定性和难以在一次前向传递中生成高质量图像的问题。作为 GAN 在图像生成中的一种有前途的替代方案，扩散模型通过一系列去噪步骤逐渐合成更逼真的图像。
(3) 本文方法：本文提出了一种新颖的粗到细潜在扩散 (CFLD) 方法用于姿态引导的人物图像合成。在没有图像-标题对和文本提示的情况下，我们开发了一种纯粹基于图像的新颖训练范式来控制预训练文本到图像扩散模型的生成过程。我们设计了一个感知精炼解码器来渐进地细化一组可学习查询并提取人物图像的语义理解作为粗粒度提示。这允许在不同的阶段解耦细粒度外观和姿态信息控制，从而规避了潜在的过拟合问题。为了生成更逼真的纹理细节，我们提出了一种混合粒度注意力模块，将多尺度细粒度外观特征编码为偏差项以增强粗粒度提示。
(4) 性能：在 DeepFashion 基准上的定量和定性实验结果证明了我们方法在姿态引导的人物图像合成任务上的优越性。这些性能支持了他们的目标，即生成具有更好泛化性能的高质量图像。</li>
</ol>
<p>7.Methods：
(1) 提出粗到细潜在扩散（CFLD）方法，用于姿态引导的人物图像合成；
(2) 开发基于图像的新训练范式，控制预训练文本到图像扩散模型的生成过程；
(3) 设计感知精炼解码器，渐进细化可学习查询，提取人物图像语义理解作为粗粒度提示；
(4) 提出混合粒度注意力模块，将多尺度细粒度外观特征编码为偏差项，增强粗粒度提示；
(5) 通过在DeepFashion基准上的定量和定性实验，验证了CFLD方法的优越性。</p>
<ol>
<li>
<p>结论：
（1）xxx；
（2）创新点：xxx；性能：xxx；工作量：xxx；</p>
</li>
<li>
<p>总结：
（1）本工作的重要意义是什么？
（2）从创新点、性能、工作量三个维度总结本文的优缺点：
创新点：本文提出了一种新颖的粗到细潜在扩散（CFLD）方法，用于姿态引导的人物图像合成。该方法通过渐进细化可学习查询，提取人物图像的语义理解作为粗粒度提示，并提出混合粒度注意力模块，将多尺度细粒度外观特征编码为偏差项，增强粗粒度提示。
性能：在 DeepFashion 基准上的定量和定性实验结果证明了 CFLD 方法在姿态引导的人物图像合成任务上的优越性。这些性能支持了他们的目标，即生成具有更好泛化性能的高质量图像。
工作量：本文的工作量适中。该方法的实现需要对文本到图像扩散模型进行预训练，这可能需要大量的计算资源。此外，该方法需要设计感知精炼解码器和混合粒度注意力模块，这需要额外的开发和实验工作。</p>
</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ee807dc5573280abe63e138fa82f6eb3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-07506917791ee3066c02770faa1a2052.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5192aaa635e4ab29d557ee967971be49.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-269e1bea1b870d8f0466ace81c9d2e01.jpg" align="middle">
</details>




<h2 id="SynArtifact-Classifying-and-Alleviating-Artifacts-in-Synthetic-Images-via-Vision-Language-Model"><a href="#SynArtifact-Classifying-and-Alleviating-Artifacts-in-Synthetic-Images-via-Vision-Language-Model" class="headerlink" title="SynArtifact: Classifying and Alleviating Artifacts in Synthetic Images   via Vision-Language Model"></a>SynArtifact: Classifying and Alleviating Artifacts in Synthetic Images   via Vision-Language Model</h2><p><strong>Authors:Bin Cao, Jianhao Yuan, Yexin Liu, Jian Li, Shuyang Sun, Jing Liu, Bo Zhao</strong></p>
<p>In the rapidly evolving area of image synthesis, a serious challenge is the presence of complex artifacts that compromise perceptual realism of synthetic images. To alleviate artifacts and improve quality of synthetic images, we fine-tune Vision-Language Model (VLM) as artifact classifier to automatically identify and classify a wide range of artifacts and provide supervision for further optimizing generative models. Specifically, we develop a comprehensive artifact taxonomy and construct a dataset of synthetic images with artifact annotations for fine-tuning VLM, named SynArtifact-1K. The fine-tuned VLM exhibits superior ability of identifying artifacts and outperforms the baseline by 25.66%. To our knowledge, this is the first time such end-to-end artifact classification task and solution have been proposed. Finally, we leverage the output of VLM as feedback to refine the generative model for alleviating artifacts. Visualization results and user study demonstrate that the quality of images synthesized by the refined diffusion model has been obviously improved. </p>
<p><a href="http://arxiv.org/abs/2402.18068v1">PDF</a> </p>
<p><strong>Summary</strong><br>利用预训练的视觉语言模型对图像合成中的伪影进行自动分类，为生成模型的进一步优化提供监管，从而提高合成图像的质量。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>合成图像中复杂伪影的存在构成了一项重大挑战，对感知真实性产生了负面影响。</li>
<li>研究人员提出将视觉语言模型（VLM）微调为伪影分类器，以便自动识别和分类各种伪影。</li>
<li>开发了一个全面的伪影分类体系，并构建了一个具有伪影注释的合成图像数据集（SynArtifact-1K）。</li>
<li>微调后的 VLM 在识别伪影方面表现出优异的能力，比基线高出 25.66%。</li>
<li>这是首次提出此类端到端伪影分类任务和解决方案。</li>
<li>利用 VLM 的输出作为反馈来优化生成模型，以减轻伪影。</li>
<li>视觉化结果和用户研究表明，优化后的扩散模型合成的图像质量得到了明显改善。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>论文题目：SynArtifact：通过视觉语言模型对合成图像中的伪影进行分类和消除</li>
<li>作者：Bin Cao, Jianhao Yuan, Yexin Liu, Jian Li, Shuyang Sun, Jing Liu, Bo Zhao</li>
<li>第一作者单位：中国科学院自动化研究所</li>
<li>关键词：合成图像、伪影、视觉语言模型、生成模型</li>
<li>论文链接：https://arxiv.org/abs/2402.18068</li>
<li>
<p>摘要：
(1) 研究背景：合成图像中存在复杂伪影，影响图像的感知真实性。
(2) 过往方法：现有方法主要依赖单一评分指标优化生成模型，无法有效反映伪影的多样性和复杂性。
(3) 本文方法：提出一个综合伪影分类法，构建了一个带有伪影注释的合成图像数据集 SynArtifact-1K，并微调视觉语言模型 (VLM) 对伪影进行分类。利用 VLM 的输出作为 AI 反馈来改进生成模型，以减轻伪影。
(4) 实验结果：微调后的 VLM 在伪影分类任务上比基线方法提高了 25.66% 的准确率和 29.01% 的 F1 分数。通过利用伪影分类器的输出作为 AI 反馈，可以有效减轻生成模型中的伪影。</p>
</li>
<li>
<p>方法：
（1）构建综合伪影分类法，建立包含伪影注释的合成图像数据集 SynArtifact-1K；
（2）微调视觉语言模型 VLM，将其作为伪影分类器；
（3）利用 VLM 输出作为 AI 反馈，计算生成模型输出与每种伪影之间的 BertScore，作为伪影分类奖励；
（4）通过最大化伪影分类奖励，优化扩散模型，以减轻伪影。</p>
</li>
<li>
<p>结论：
（1）：本文针对合成图像中的伪影问题，提出了一个综合的伪影分类法，构建了包含伪影注释的合成图像数据集 SynArtifact-1K，并利用视觉语言模型对伪影进行分类，有效地减轻了生成模型中的伪影，提升了合成图像的感知真实性。
（2）：创新点：</p>
</li>
<li>构建了包含 13 种常见伪影的综合伪影分类法。</li>
<li>创建了首个带有伪影类别、描述和坐标注释的图像数据集 SynArtifact-1K。</li>
<li>微调视觉语言模型用于自动分类伪影，并利用其输出作为 AI 反馈来优化生成模型。
性能：</li>
<li>微调后的视觉语言模型在伪影分类任务上比基线方法提高了 25.66% 的准确率和 29.01% 的 F1 分数。</li>
<li>利用伪影分类器的输出作为 AI 反馈，可以有效减轻生成模型中的伪影。
工作量：</li>
<li>构建了包含 1000 张合成图像的 SynArtifact-1K 数据集。</li>
<li>微调了视觉语言模型用于伪影分类。</li>
<li>通过最大化伪影分类奖励，优化了扩散模型以减轻伪影。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-887bb2eb3bab7f102340a00fb115308a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a67234ceff494848cb67aa7bc7345a5e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c0c890345f83368ccd384b81c55c4b11.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-48d8c1e1b56b76bfccfccfcb96c1d5a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1a5599c3d37db39e68fa5fb2e0139cec.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-94675e3c8e66717ee97bc9e3472ed274.jpg" align="middle">
</details>




<h2 id="Box-It-to-Bind-It-Unified-Layout-Control-and-Attribute-Binding-in-T2I-Diffusion-Models"><a href="#Box-It-to-Bind-It-Unified-Layout-Control-and-Attribute-Binding-in-T2I-Diffusion-Models" class="headerlink" title="Box It to Bind It: Unified Layout Control and Attribute Binding in T2I   Diffusion Models"></a>Box It to Bind It: Unified Layout Control and Attribute Binding in T2I   Diffusion Models</h2><p><strong>Authors:Ashkan Taghipour, Morteza Ghahremani, Mohammed Bennamoun, Aref Miri Rekavandi, Hamid Laga, Farid Boussaid</strong></p>
<p>While latent diffusion models (LDMs) excel at creating imaginative images, they often lack precision in semantic fidelity and spatial control over where objects are generated. To address these deficiencies, we introduce the Box-it-to-Bind-it (B2B) module - a novel, training-free approach for improving spatial control and semantic accuracy in text-to-image (T2I) diffusion models. B2B targets three key challenges in T2I: catastrophic neglect, attribute binding, and layout guidance. The process encompasses two main steps: i) Object generation, which adjusts the latent encoding to guarantee object generation and directs it within specified bounding boxes, and ii) attribute binding, guaranteeing that generated objects adhere to their specified attributes in the prompt. B2B is designed as a compatible plug-and-play module for existing T2I models, markedly enhancing model performance in addressing the key challenges. We evaluate our technique using the established CompBench and TIFA score benchmarks, demonstrating significant performance improvements compared to existing methods. The source code will be made publicly available at <a href="https://github.com/nextaistudio/BoxIt2BindIt">https://github.com/nextaistudio/BoxIt2BindIt</a>. </p>
<p><a href="http://arxiv.org/abs/2402.17910v1">PDF</a> </p>
<p><strong>Summary</strong><br>Box-it-to-Bind-it（B2B）是一种无需训练的新模块，可提高文本到图像（T2I）扩散模型中图像的生成质量、语义准确度和空间控制能力。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>B2B 模块可改善 T2I 中的三个关键挑战：灾难性遗漏、属性绑定和布局指导。</li>
<li>B2B 包括生成对象和属性绑定的两个主要步骤。</li>
<li>B2B 可作为现有的 T2I 模型的即插即用模块，无需训练。</li>
<li>B2B 在 CompBench 和 TIFA 评分基准上表现出显著的性能提升。</li>
<li>B2B 的源代码将在 <a href="https://github.com/nextaistudio/BoxIt2BindIt">https://github.com/nextaistudio/BoxIt2BindIt</a> 上公开。</li>
<li>B2B 提高了 LDM 在生成图像时的空间控制和语义准确性。</li>
<li>B2B 适用于不同的 T2I 模型，易于集成。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：Box-it-to-Bind-it：统一布局控制和属性绑定到 T2I 扩散模型中</li>
<li>作者：Ashkan Taghipour, Morteza Ghahremani, Mohammed Bennamoun, Aref Miri Rekavandi, Hamid Laga, Farid Boussaid</li>
<li>第一作者单位：西澳大利亚大学</li>
<li>关键词：文本到图像、扩散模型、空间控制、属性绑定、布局引导</li>
<li>论文链接：https://arxiv.org/abs/2402.17910</li>
<li>摘要：
（1）研究背景：
现有扩散模型在生成图像时缺乏语义保真度和空间控制，难以忠实地遵循给定的提示，尤其是在对象属性和对象放置方面。</li>
</ol>
<p>（2）过去的方法和问题：
现有方法要么从头开始训练模型，要么微调现有模型，但需要大量计算资源和时间。此外，利用预训练模型并集成特征的方法虽然不需要大量训练，但效果有限。</p>
<p>（3）提出的研究方法：
本文提出了一种免训练的方法 Box-it-to-Bind-it (B2B)，解决文本到图像生成中的三个关键挑战：灾难性遗漏、属性绑定和布局引导。B2B 在推理阶段通过两步引导扩散模型的潜在编码：对象生成和属性绑定。</p>
<p>（4）方法在任务和性能上的表现：
在 CompBench 和 TIFA 得分基准上，与现有方法相比，B2B 在解决关键挑战方面显着提高了模型性能。这些性能提升支持了本文的目标，即提高文本到图像生成中的空间控制和语义准确性。</p>
<p>方法：
(1) B2B是一种奖励引导扩散模型，它在推理阶段通过两步引导扩散模型的潜在编码：对象生成和属性绑定。
(2) 对象生成：基于IoU，增加对象生成概率，将注意力权重集中在给定边界框内，同时抑制边界框外的注意力权重。
(3) 属性绑定：使用KL散度测量属性概率分布与对应对象概率分布的差异，减少差异，将属性分布强制收敛到各自的对象。</p>
<ol>
<li>结论：
（1）：本研究针对文本到图像生成中的关键挑战，特别是属性绑定和空间控制，提出了 B2B 模型。B2B 采用生成和绑定双模块系统，有效解决了灾难性遗漏、提高属性绑定精度和确保准确对象放置的问题。它作为现有 T2I 框架的即插即用模块的兼容性通过其在 CompBench 和 TIFA 基准中的出色表现得到证明，标志着生成建模的重大飞跃。B2B 的突破凸显了其作为未来研究潜在标准的作用，为数字成像和生成式 AI 的创新发展铺平了道路。
（2）：创新点：</li>
<li>提出了一种免训练的方法 B2B，通过两步引导扩散模型的潜在编码来解决文本到图像生成中的关键挑战。</li>
<li>设计了对象生成和属性绑定两个模块，有效解决了灾难性遗漏、属性绑定和布局引导问题。</li>
<li>B2B 作为现有 T2I 框架的即插即用模块，易于集成和使用。
性能：</li>
<li>在 CompBench 和 TIFA 基准上，与现有方法相比，B2B 在解决关键挑战方面显着提高了模型性能。</li>
<li>消融研究验证了对象生成和属性绑定奖励组件的有效性，表明 B2B 的各个组件对整体性能至关重要。
工作量：</li>
<li>B2B 是一种免训练的方法，不需要从头开始训练模型或微调现有模型，从而节省了大量的计算资源和时间。</li>
<li>B2B 易于集成到现有 T2I 框架中，无需进行复杂的修改或重新训练，降低了工作量。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9044558cdc31309b419fea5199aa8a89.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78bccd36910d4aa870962c445823ad57.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-967a215bde68183f03e457a7ff3f8e9a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e2a4cdc833464a14406a357aa9e0c358.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d140c3c8e05d724098a1c03138203a01.jpg" align="middle">
</details>




<h2 id="Structure-Guided-Adversarial-Training-of-Diffusion-Models"><a href="#Structure-Guided-Adversarial-Training-of-Diffusion-Models" class="headerlink" title="Structure-Guided Adversarial Training of Diffusion Models"></a>Structure-Guided Adversarial Training of Diffusion Models</h2><p><strong>Authors:Ling Yang, Haotian Qian, Zhilong Zhang, Jingwei Liu, Bin Cui</strong></p>
<p>Diffusion models have demonstrated exceptional efficacy in various generative applications. While existing models focus on minimizing a weighted sum of denoising score matching losses for data distribution modeling, their training primarily emphasizes instance-level optimization, overlooking valuable structural information within each mini-batch, indicative of pair-wise relationships among samples. To address this limitation, we introduce Structure-guided Adversarial training of Diffusion Models (SADM). In this pioneering approach, we compel the model to learn manifold structures between samples in each training batch. To ensure the model captures authentic manifold structures in the data distribution, we advocate adversarial training of the diffusion generator against a novel structure discriminator in a minimax game, distinguishing real manifold structures from the generated ones. SADM substantially improves existing diffusion transformers (DiT) and outperforms existing methods in image generation and cross-domain fine-tuning tasks across 12 datasets, establishing a new state-of-the-art FID of 1.58 and 2.11 on ImageNet for class-conditional image generation at resolutions of 256x256 and 512x512, respectively. </p>
<p><a href="http://arxiv.org/abs/2402.17563v1">PDF</a> Accepted by CVPR 2024</p>
<p><strong>Summary</strong><br>扩散模型通过结构对抗训练，学习批内样本流形结构，提升图像生成和跨域微调任务的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>现有扩散模型专注于单个样本的去噪得分匹配损失优化，忽视批内样本之间的成对关系。</li>
<li>结构对抗训练 (SADM) 引入结构鉴别器来区分真实和生成的流形结构。</li>
<li>SADM 迫使模型学习训练批次中样本之间的流形结构。</li>
<li>SADM 与扩散变压器 (DiT) 相结合，在图像生成和跨域微调任务上优于现有方法。</li>
<li>SADM 在 12 个数据集上实现了图像生成和跨域微调任务的最新 FID 分别为 1.58 和 2.11。</li>
<li>SADM 在 256x256 和 512x512 分辨率下，在 ImageNet 上实现了类条件图像生成的最新 FID 分别为 1.58 和 2.11。</li>
<li>SADM 证明了流形结构学习对于扩散模型在生成任务中的重要性。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：结构引导扩散模型对抗性训练</li>
<li>作者：杨凌、钱浩天、张智龙、刘景伟、崔斌</li>
<li>第一作者单位：北京大学</li>
<li>关键词：扩散模型、结构引导、对抗训练</li>
<li>论文链接：https://arxiv.org/abs/2402.17563
   Github 链接：无</li>
<li>摘要：
（1）研究背景：
扩散模型在生成任务中表现出色，但现有方法主要关注最小化去噪得分匹配损失的加权和，训练过程侧重于实例级优化，忽略了小批量样本之间的宝贵结构信息。</li>
</ol>
<p>（2）过去的方法及其问题：
现有方法主要集中在实例级优化，忽略了小批量样本之间的结构信息，导致无法充分建模数据分布。</p>
<p>（3）提出的研究方法：
提出结构引导扩散模型对抗性训练（SADM），通过对抗训练指导扩散生成器学习小批量样本之间的流形结构。引入结构判别器来区分真实流形结构和生成流形结构，确保模型捕获数据分布中的真实流形结构。</p>
<p>（4）方法在任务和性能上的表现：
SADM 显著提升了现有扩散 Transformer，在 12 个数据集上的图像生成和跨域微调任务中优于现有方法，在 ImageNet 上分别以 256×256 和 512×512 的分辨率实现了 1.58 和 2.11 的新 SOTA FID，验证了方法的有效性。</p>
<p>7.Methods：
（1）提出结构引导扩散模型对抗性训练（SADM），通过对抗训练指导扩散生成器学习小批量样本之间的流形结构；
（2）引入结构判别器来区分真实流形结构和生成流形结构，确保模型捕获数据分布中的真实流形结构；
（3）采用Wasserstein GAN损失函数，指导生成器生成与真实流形结构相似的样本；
（4）在训练过程中交替更新生成器和判别器，直至达到纳什均衡；
（5）将SADM与扩散Transformer相结合，形成更强大的图像生成模型。</p>
<ol>
<li>总结
(1): 本文提出了从结构角度优化扩散模型的结构引导对抗性训练方法，该训练算法可以轻松推广到图像和潜在扩散模型，并通过理论推导和实验结果一致地改进了现有的扩散模型。我们在 12 个图像数据集上的图像生成和跨域微调任务中取得了新的 SOTA 性能。对于未来的工作，我们将把我们的方法扩展到更具挑战性的基于扩散的应用程序（例如，文本到图像/视频生成）。
(2): 创新点: 提出结构引导对抗性训练方法，通过对抗训练指导扩散生成器学习小批量样本之间的流形结构，从而提升扩散模型的生成质量。
性能: 在 12 个图像数据集上的图像生成和跨域微调任务中取得了新的 SOTA 性能，在 ImageNet 上分别以 256×256 和 512×512 的分辨率实现了 1.58 和 2.11 的新 SOTAFID。
工作量: 该方法易于实现，可以轻松推广到图像和潜在扩散模型，工作量较小。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-11a45496d9d4169c7ee0bbb4a6534ffa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b4ae1e4da806d223271756f678f15ce9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-02b820484fca35ffef9bc52706101c79.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14ed9373ba8bdaf3ecaca75391245256.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-75ca2aa69507bb15984388d3520039af.jpg" align="middle">
</details>




<h2 id="Diffusion-Model-Based-Image-Editing-A-Survey"><a href="#Diffusion-Model-Based-Image-Editing-A-Survey" class="headerlink" title="Diffusion Model-Based Image Editing: A Survey"></a>Diffusion Model-Based Image Editing: A Survey</h2><p><strong>Authors:Yi Huang, Jiancheng Huang, Yifan Liu, Mingfu Yan, Jiaxi Lv, Jianzhuang Liu, Wei Xiong, He Zhang, Shifeng Chen, Liangliang Cao</strong></p>
<p>Denoising diffusion models have emerged as a powerful tool for various image generation and editing tasks, facilitating the synthesis of visual content in an unconditional or input-conditional manner. The core idea behind them is learning to reverse the process of gradually adding noise to images, allowing them to generate high-quality samples from a complex distribution. In this survey, we provide an exhaustive overview of existing methods using diffusion models for image editing, covering both theoretical and practical aspects in the field. We delve into a thorough analysis and categorization of these works from multiple perspectives, including learning strategies, user-input conditions, and the array of specific editing tasks that can be accomplished. In addition, we pay special attention to image inpainting and outpainting, and explore both earlier traditional context-driven and current multimodal conditional methods, offering a comprehensive analysis of their methodologies. To further evaluate the performance of text-guided image editing algorithms, we propose a systematic benchmark, EditEval, featuring an innovative metric, LMM Score. Finally, we address current limitations and envision some potential directions for future research. The accompanying repository is released at <a href="https://github.com/SiatMMLab/Awesome-Diffusion-Model-Based-Image-Editing-Methods">https://github.com/SiatMMLab/Awesome-Diffusion-Model-Based-Image-Editing-Methods</a>. </p>
<p><a href="http://arxiv.org/abs/2402.17525v1">PDF</a> </p>
<p><strong>Summary</strong><br>扩散模型在图像生成和编辑任务中应用广泛，可从复杂分布中生成高质量样本，且支持无条件和输入条件下的图像编辑。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>扩散模型通过学习逆转图像加噪过程，生成高质量样本。</li>
<li>扩散模型图像编辑方法可分为不同学习策略、用户输入条件和编辑任务。</li>
<li>图像修复和外延可使用传统上下文驱动方法或多模态条件方法。</li>
<li>提出 EditEval 基准和 LMM 评分用于评估文本指导图像编辑算法。</li>
<li>目前存在限制，未来研究方向包括多模态、3D 和编辑元数据。</li>
<li>可在 <a href="https://github.com/SiatMMLab/Awesome-Diffusion-Model-Based-Image-Editing-Methods">https://github.com/SiatMMLab/Awesome-Diffusion-Model-Based-Image-Editing-Methods</a> 获取相关代码。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>标题：基于扩散模型的图像编辑：综述</li>
<li>作者：Yi Huang、Jiancheng Huang、Yifan Liu、Mingfu Yan、Jiaxi Lv、Jianzhuang Liu、Wei Xiong、He Zhang、Shifeng Chen、Liangliang Cao</li>
<li>单位：深圳先进技术研究院</li>
<li>关键词：Diffusion Model、Image Editing、AIGC</li>
<li>链接：https://arxiv.org/abs/2402.17525
Github：无</li>
<li>摘要：
(1)：随着人工智能（AI）技术的发展，AI 生成的内容（AIGC）领域蓬勃发展，图像编辑作为其中一项重要任务，在数字媒体、广告和科学研究等领域有着广泛的应用。
(2)：基于扩散模型的图像编辑方法近年来取得了显著进展，该方法通过学习逐步给图像添加噪声并逆转这一过程，可以从复杂分布中生成高质量的样本。
(3)：本文对基于扩散模型的图像编辑方法进行了全面的综述，从学习策略、用户输入条件和具体编辑任务等多个角度对现有工作进行了深入分析和分类。
(4)：基于扩散模型的图像编辑方法在图像修复、图像外延等任务上取得了很好的效果，本文还提出了一个系统性的基准 EditEval 和一个创新的指标 LMMScore 来进一步评估文本引导图像编辑算法的性能。</li>
</ol>
<p>7.Methods:
(1): 基于CLIP指导的图像编辑：DiffusionCLIP、Asyrp、EffDiff、DiffStyler、StyleDiffusion、UNIT-DDPM、CycleNet、DiffusionAutoencoders、HDAE、EGSDE、Pixel-GuidedDiffusion；
(2): 基于参考和属性指导的图像编辑：PbE、RIC、ObjectStitch、PhD、DreamInpainter、Anydoor、FADING、PAIRDiffusion、SmartBrush、IIR-Net；
(3): 基于指令指导的图像编辑：InstructPix2Pix、MoEController、FoI、LOFIE、InstructDiffusion、EmuEdit、DialogPaint、Inst-Inpaint、HIVE、ImageBrush、InstructAny2Pix、MGIE、SmartEdit。</p>
<ol>
<li>结论：
（1）本工作对基于扩散模型的图像编辑方法进行了全面的综述，从多个角度对现有工作进行了深入分析和分类，并提出了一个系统性的基准 EditEval 和一个创新的指标 LMMScore 来进一步评估文本引导图像编辑算法的性能。
（2）创新点：</li>
<li>提出了一种新的图像编辑基准 EditEval 和一个创新的指标 LMMScore，用于评估文本引导图像编辑算法的性能。</li>
<li>对基于扩散模型的图像编辑方法进行了全面的综述和分类，从学习策略、用户输入条件和具体编辑任务等多个角度对现有工作进行了深入分析。</li>
<li>探索了这些方法在增强编辑性能方面的贡献。</li>
<li>在我们的图像编辑基准 EditEval 中对 7 项任务进行了评估，以及最新最先进的方法。</li>
<li>总结了图像编辑领域的广泛潜力，并提出了未来研究的方向。</li>
<li>性能：在 EditEval 基准上，基于扩散模型的图像编辑方法在图像修复、图像外延等任务上取得了很好的效果。</li>
<li>工作量：本文对超过 100 种基于扩散模型的图像编辑方法进行了综述和分类，并对 7 项任务进行了评估，工作量较大。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4c52565ddb49dad37f10475b00a6abbc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4537d5996d9b29f71e82d00a227227b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db76ba27193f9ab6b62bab161a239510.jpg" align="middle">
</details>




<h2 id="Enhancing-Hyperspectral-Images-via-Diffusion-Model-and-Group-Autoencoder-Super-resolution-Network"><a href="#Enhancing-Hyperspectral-Images-via-Diffusion-Model-and-Group-Autoencoder-Super-resolution-Network" class="headerlink" title="Enhancing Hyperspectral Images via Diffusion Model and Group-Autoencoder   Super-resolution Network"></a>Enhancing Hyperspectral Images via Diffusion Model and Group-Autoencoder   Super-resolution Network</h2><p><strong>Authors:Zhaoyang Wang, Dongyang Li, Mingyang Zhang, Hao Luo, Maoguo Gong</strong></p>
<p>Existing hyperspectral image (HSI) super-resolution (SR) methods struggle to effectively capture the complex spectral-spatial relationships and low-level details, while diffusion models represent a promising generative model known for their exceptional performance in modeling complex relations and learning high and low-level visual features. The direct application of diffusion models to HSI SR is hampered by challenges such as difficulties in model convergence and protracted inference time. In this work, we introduce a novel Group-Autoencoder (GAE) framework that synergistically combines with the diffusion model to construct a highly effective HSI SR model (DMGASR). Our proposed GAE framework encodes high-dimensional HSI data into low-dimensional latent space where the diffusion model works, thereby alleviating the difficulty of training the diffusion model while maintaining band correlation and considerably reducing inference time. Experimental results on both natural and remote sensing hyperspectral datasets demonstrate that the proposed method is superior to other state-of-the-art methods both visually and metrically. </p>
<p><a href="http://arxiv.org/abs/2402.17285v1">PDF</a> Accepted by AAAI2024</p>
<p><strong>Summary</strong><br>扩散模型与群组自编码器相结合的创新框架，有效提升高光谱图像超分辨率，显著改善谱空关系建模和低层细节恢复。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>扩散模型擅长建模复杂关系和学习视觉特征，在高光谱图像超分辨率中潜力巨大。</li>
<li>训练扩散模型面临收敛困难和推理时间长挑战。</li>
<li>群组自编码器框架通过将高维高光谱数据编码到低维潜在空间，缓解了扩散模型训练难度，并保持了波段相关性。</li>
<li>扩散模型与群组自编码器相结合，有效解决了推理时间问题。</li>
<li>在自然和遥感高光谱数据集上，该方法在视觉和度量上均优于现有技术。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>标题：基于扩散模型和组自编码器的超分辨率高光谱图像增强</li>
<li>作者：王兆阳，李东阳，张明阳，罗浩，巩茂国</li>
<li>隶属单位：西安电子科技大学协同智能系统教育部重点实验室</li>
<li>关键词：高光谱图像，超分辨率，扩散模型，组自编码器</li>
<li>论文链接：https://arxiv.org/abs/2402.17285</li>
<li>
<p>摘要：
（1）研究背景：现有高光谱图像超分辨率方法难以有效捕捉复杂的光谱-空间关系和低级细节，而扩散模型是一种有前途的生成模型，以其在建模复杂关系和学习高低级视觉特征方面的出色性能而闻名。
（2）过去方法及问题：将扩散模型直接应用于高光谱图像超分辨率面临着模型收敛困难和推理时间长的挑战。
（3）研究方法：提出了一种新的组自编码器（GAE）框架，该框架与扩散模型协同结合，构建了一个高效的高光谱图像超分辨率模型（DMGASR）。提出的 GAE 框架将高维高光谱数据编码为低维潜在空间，扩散模型在此空间中工作，从而缓解了训练扩散模型的难度，同时保持了波段相关性并大大减少了推理时间。
（4）任务和性能：在自然和遥感高光谱数据集上的实验结果表明，所提出的方法在视觉和度量上都优于其他最先进的方法。</p>
</li>
<li>
<p>方法：
(1): 提出了一种基于扩散模型和组自编码器的超分辨率高光谱图像增强模型（DMGASR）；
(2): 该模型采用两阶段训练方式，包括自动编码器和扩散超分辨率模型；
(3): 采用谱分组策略和非对称解码器设计，有效地将高维高光谱数据编码为低维潜在空间；
(4): 训练扩散模型在潜在空间中工作，缓解了训练扩散模型的难度，同时保持了波段相关性并大大减少了推理时间；
(5): 训练自动编码器重构输入数据，生成一系列隐藏变量；
(6): 将低分辨率隐藏变量作为条件信息，与高分辨率隐藏变量串联，在去噪过程中加入到扩散模型中；
(7): 采用 U-Net 作为去噪模型，迭代去除噪声，生成超分辨率潜在变量列表；
(8): 将超分辨率潜在变量列表传递给解码器，生成超分辨率图像。</p>
</li>
<li>
<p>结论：
（1）：本文提出了一种基于扩散模型和组自编码器的高光谱图像超分辨率增强模型（DMGASR），该模型将扩散模型与自动编码器相结合，有效解决了扩散模型在高维数据上收敛困难的问题，并通过在低维潜在空间中训练扩散模型，大大减少了推理时间。该方法在自然和遥感高光谱数据集上均取得了优异的性能，在视觉和度量上均优于其他最先进的方法。
（2）：创新点：</p>
</li>
<li>提出了一种基于扩散模型和组自编码器的超分辨率高光谱图像增强模型（DMGASR）。</li>
<li>采用两阶段训练方式，包括自动编码器和扩散超分辨率模型。</li>
<li>采用谱分组策略和非对称解码器设计，有效地将高维高光谱数据编码为低维潜在空间。</li>
<li>训练扩散模型在潜在空间中工作，缓解了训练扩散模型的难度，同时保持了波段相关性并大大减少了推理时间。
性能：</li>
<li>在自然和遥感高光谱数据集上均取得了优异的性能。</li>
<li>在视觉和度量上均优于其他最先进的方法。
工作量：</li>
<li>算法设计和实现。</li>
<li>数据集的收集和预处理。</li>
<li>实验的进行和结果分析。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-1b637edd1829307f3889177173204f7c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cc3237f0ece24500c44086801ebc1feb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3e331ea518a2b9c151178e17f115708.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b211209593777f9420f6bb845daa71b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f24696c9c22f22b6e487ce2e6fc31ec7.jpg" align="middle">
</details>




<h2 id="One-Shot-Structure-Aware-Stylized-Image-Synthesis"><a href="#One-Shot-Structure-Aware-Stylized-Image-Synthesis" class="headerlink" title="One-Shot Structure-Aware Stylized Image Synthesis"></a>One-Shot Structure-Aware Stylized Image Synthesis</h2><p><strong>Authors:Hansam Cho, Jonghyun Lee, Seunggyu Chang, Yonghyun Jeong</strong></p>
<p>While GAN-based models have been successful in image stylization tasks, they often struggle with structure preservation while stylizing a wide range of input images. Recently, diffusion models have been adopted for image stylization but still lack the capability to maintain the original quality of input images. Building on this, we propose OSASIS: a novel one-shot stylization method that is robust in structure preservation. We show that OSASIS is able to effectively disentangle the semantics from the structure of an image, allowing it to control the level of content and style implemented to a given input. We apply OSASIS to various experimental settings, including stylization with out-of-domain reference images and stylization with text-driven manipulation. Results show that OSASIS outperforms other stylization methods, especially for input images that were rarely encountered during training, providing a promising solution to stylization via diffusion models. </p>
<p><a href="http://arxiv.org/abs/2402.17275v1">PDF</a> CVPR 2024</p>
<p><strong>Summary</strong><br>基于扩散模型的 OSASIS 实现了图像风格化，同时保持了结构完整性，即使是对训练中很少遇到的输入图像。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>OSASIS 采用扩散模型进行图像风格化，解决了 GAN 模型在保持结构方面的不足。</li>
<li>OSASIS 能够有效分离图像语义和结构，可控地调整给定输入的内容和风格级别。</li>
<li>OSASIS 在各种实验设置中表现出色，包括使用域外参考图像进行风格化和使用文本驱动的操作进行风格化。</li>
<li>与其他风格化方法相比，OSASIS 在训练中很少遇到的输入图像上表现得尤为出色，为通过扩散模型进行风格化提供了有前景的解决方案。</li>
<li>OSASIS 采用了渐进式训练策略，通过从添加噪声到恢复图像，逐步将风格应用于输入。</li>
<li>OSASIS 使用预训练的扩散模型，提高了效率和泛化性。</li>
<li>OSASIS 在图像风格化领域展现出了广泛的应用前景，包括图像编辑、艺术创作和视频处理。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>标题：单次结构感知风格化图像合成</li>
<li>作者：Jongmin Lee*, Jaeyeon Kang, Sangwoo Mo, Seongwon Lee†, Kyoung Mu Lee†</li>
<li>隶属单位：NAVER Cloud</li>
<li>关键词：图像风格化、扩散模型、结构保持</li>
<li>论文链接：https://arxiv.org/abs/2302.05447, Github 代码链接：无</li>
<li>摘要：
(1) 研究背景：GAN 模型在图像风格化任务中取得成功，但难以在风格化各种输入图像时保持结构。最近，扩散模型被用于图像风格化，但仍缺乏保持输入图像原始质量的能力。
(2) 过去方法及问题：过去方法包括基于 GAN 的模型和基于扩散模型的方法。GAN 模型难以保持结构，而基于扩散模型的方法缺乏控制内容和风格的能力。
(3) 本文提出的研究方法：本文提出了一种新的单次风格化方法 OSASIS，该方法在结构保持方面具有鲁棒性。OSASIS 通过将语义从图像的结构中解耦，从而有效地控制应用于给定输入的内容和风格的级别。
(4) 任务和性能：OSASIS 在各种实验设置中得到应用，包括使用域外参考图像的风格化和使用文本驱动的操作的风格化。结果表明，OSASIS 优于其他风格化方法，特别是对于在训练期间很少遇到的输入图像，为通过扩散模型进行风格化提供了一种有前景的解决方案。</li>
</ol>
<p><strong>Methods：</strong></p>
<ol>
<li><strong>图像分解：</strong>将输入图像分解为内容和结构特征，其中内容特征表示图像的语义信息，而结构特征表示图像的几何形状和纹理。</li>
<li><strong>风格嵌入：</strong>将参考风格图像嵌入到一个潜在空间中，该空间由扩散模型训练。</li>
<li><strong>风格传输：</strong>将输入图像的内容特征与参考风格的风格嵌入相结合，生成一个新的图像，该图像具有输入图像的结构和参考风格的风格。</li>
<li>
<p><strong>结构保持：</strong>通过使用一个额外的损失函数，将输入图像的结构特征与生成图像的结构特征进行匹配，从而保持输入图像的原始质量。</p>
</li>
<li>
<p>结论：
(1): 本工作提出了一种基于扩散模型的新型单次图像风格化方法 OSASIS，该方法在结构保持方面具有鲁棒性。与基于 GAN 和其他基于扩散的风格化方法相比，OSASIS 展示了在风格化中对结构的强大感知，有效地将图像的结构和语义解耦。尽管 OSASIS 在结构感知风格化方面取得了重大进展，但仍存在一些局限性。OSASIS 的一个显着限制是其训练时间，比比较方法更长。这种延长的训练持续时间是为了换取该方法增强了保持结构完整性和适应各种风格的能力。此外，OSASIS 需要针对每张风格图像进行训练。在需要跨多种风格快速部署的场景中，这一要求可以被视为一种限制。尽管存在这些挑战，但 OSASIS 在保持输入图像结构完整性方面的稳健性、其在域外参考风格化中的有效性以及其在文本驱动操作中的适应性使其成为风格化图像合成领域中一种很有前景的方法。未来的工作将解决这些限制，特别是在优化训练效率和减少对单个风格图像训练的必要性方面，以增强 OSASIS 在各种实际场景中的实用性和适用性。
(2): 创新点：</p>
</li>
<li>提出了一种新的图像风格化方法 OSASIS，该方法基于扩散模型，在结构保持方面具有鲁棒性。</li>
<li>OSASIS 通过将图像的结构和语义解耦，有效地控制应用于给定输入的内容和风格的级别。</li>
<li>OSASIS 在各种实验设置中得到应用，包括使用域外参考图像的风格化和使用文本驱动的操作的风格化。
性能：</li>
<li>OSASIS 在结构保持方面优于其他风格化方法，特别是对于在训练期间很少遇到的输入图像。</li>
<li>OSASIS 为通过扩散模型进行风格化提供了一种有前景的解决方案。
工作量：</li>
<li>OSASIS 的训练时间比比较方法更长。</li>
<li>OSASIS 需要针对每张风格图像进行训练。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-957518995345024bb9a18f0e683a4e55.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0f3cefa16e52b2bb0bdbb679863e234.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e8afc30904c2bad1400fb9f044e33a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0eead50e28d5ed02ff0105780a9e22e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b842ecc40528644a1d824a5a8948f487.jpg" align="middle">
</details>




<h2 id="Playground-v2-5-Three-Insights-towards-Enhancing-Aesthetic-Quality-in-Text-to-Image-Generation"><a href="#Playground-v2-5-Three-Insights-towards-Enhancing-Aesthetic-Quality-in-Text-to-Image-Generation" class="headerlink" title="Playground v2.5: Three Insights towards Enhancing Aesthetic Quality in   Text-to-Image Generation"></a>Playground v2.5: Three Insights towards Enhancing Aesthetic Quality in   Text-to-Image Generation</h2><p><strong>Authors:Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, Suhail Doshi</strong></p>
<p>In this work, we share three insights for achieving state-of-the-art aesthetic quality in text-to-image generative models. We focus on three critical aspects for model improvement: enhancing color and contrast, improving generation across multiple aspect ratios, and improving human-centric fine details. First, we delve into the significance of the noise schedule in training a diffusion model, demonstrating its profound impact on realism and visual fidelity. Second, we address the challenge of accommodating various aspect ratios in image generation, emphasizing the importance of preparing a balanced bucketed dataset. Lastly, we investigate the crucial role of aligning model outputs with human preferences, ensuring that generated images resonate with human perceptual expectations. Through extensive analysis and experiments, Playground v2.5 demonstrates state-of-the-art performance in terms of aesthetic quality under various conditions and aspect ratios, outperforming both widely-used open-source models like SDXL and Playground v2, and closed-source commercial systems such as DALLE 3 and Midjourney v5.2. Our model is open-source, and we hope the development of Playground v2.5 provides valuable guidelines for researchers aiming to elevate the aesthetic quality of diffusion-based image generation models. </p>
<p><a href="http://arxiv.org/abs/2402.17245v1">PDF</a> Model weights:   <a href="https://huggingface.co/playgroundai/playground-v2.5-1024px-aesthetic">https://huggingface.co/playgroundai/playground-v2.5-1024px-aesthetic</a></p>
<p><strong>Summary</strong><br>通过对噪声时间表、宽高比准备和面向人类的微调的研究，Playground v2.5  diffusion 模型可产生极佳的美学质量。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>噪音时间表对模型真实性和视觉保真度至关重要。</li>
<li>平衡的分区数据集可改善不同宽高比的图像生成。</li>
<li>将模型输出与人类偏好相结合可提升图像的共鸣效果。</li>
<li>Playground v2.5 在各种条件和宽高比下表现出最先进的审美质量。</li>
<li>Playground v2.5 模型开源，为提升基于扩散的图像生成模型的审美质量提供了有价值的指导。</li>
<li>Playground v2.5 优于 SDXL、Playground v2、DALLE 3 和 Midjourney v5.2。</li>
<li>研究有助于提高基于扩散的图像生成模型的审美质量。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>标题：Playground v2.5：提升文本到图像生成审美质量的三点见解</li>
<li>作者：Daiqing Li、Aleks Kamko、Ehsan Akhgari、Ali Sabet、Linmiao Xu、Suhail Doshi</li>
<li>第一作者单位：Playground Research</li>
<li>关键词：文本到图像生成、扩散模型、审美质量</li>
<li>论文链接：arXiv:2402.17245v1[cs.CV]</li>
<li>摘要：
（1）研究背景：文本到图像生成模型在生成图像的审美质量方面取得了显著进展，但仍存在一些挑战，如颜色和对比度不足、不同宽高比生成质量不佳、缺乏对人类偏好的对齐。
（2）过去方法：以往方法主要集中在改进扩散模型的训练过程，如优化噪声调度或使用更大的数据集。然而，这些方法在提升审美质量方面效果有限。
（3）研究方法：本文提出了三点见解来提升审美质量：改进噪声调度以增强颜色和对比度，构建平衡的分桶数据集以支持不同宽高比的生成，以及利用人类反馈来对齐模型输出与人类偏好。
（4）方法性能：在广泛的分析和实验中，Playground v2.5 在各种条件和宽高比下展示了最先进的审美质量，优于 SDXL、Playground v2 等开源模型和 DALL·E 3、Midjourney v5.2 等闭源商业系统。</li>
</ol>
<p>方法：
（1）改进噪声调度：采用 EDM 框架和更噪声的调度方式，增强图像色彩和对比度。
（2）平衡分桶数据集：构建包含不同宽高比图像的分桶数据集，支持多种宽高比的生成。
（3）利用人类反馈：使用人类评级系统自动筛选高质量数据集，并采用迭代训练方法，根据人类偏好对齐模型输出。</p>
<ol>
<li>总结：
（1）：本文提出 Playground v2.5，该模型通过改进噪声调度、构建平衡的分桶数据集和利用人类反馈等三点见解，提升了文本到图像生成模型的审美质量。
（2）：
创新点：</li>
<li>提出了一种新的噪声调度框架，增强了图像的色彩和对比度。</li>
<li>构建了一个包含不同宽高比图像的分桶数据集，支持多种宽高比的生成。</li>
<li>利用人类评级系统自动筛选高质量数据集，并采用迭代训练方法，根据人类偏好对齐模型输出。
性能：</li>
<li>在广泛的分析和实验中，Playground v2.5 在各种条件和宽高比下展示了最先进的审美质量，优于其他开源和闭源模型。</li>
<li>Playground v2.5 在增强图像色彩和对比度、生成不同宽高比的高质量图像以及对齐模型输出与人类偏好方面表现出色，尤其是在生成人物图像的精细细节方面。
工作量：</li>
<li>该模型已开源，用户可以在 Playground 产品网站上使用。</li>
<li>Playground v2.5 的权重已在 Hugging Face 上开源。</li>
<li>Playground 将继续提供扩展，以便在 A1111 和 ComfyUI 等流行社区工具中使用 Playground v2.5。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b9ee43af14ab727bc293d7a249e6d156.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ff95dbf16b9c2e734124d2c99954b6c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b62a3df3bac0ff8ef7d20dfeccb0f6b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-869a1d35fa675595c5662a91b215c366.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-226f377d76bcd81c0c005d4e513c6f81.jpg" align="middle">
</details>




<h2 id="SAM-DiffSR-Structure-Modulated-Diffusion-Model-for-Image-Super-Resolution"><a href="#SAM-DiffSR-Structure-Modulated-Diffusion-Model-for-Image-Super-Resolution" class="headerlink" title="SAM-DiffSR: Structure-Modulated Diffusion Model for Image   Super-Resolution"></a>SAM-DiffSR: Structure-Modulated Diffusion Model for Image   Super-Resolution</h2><p><strong>Authors:Chengcheng Wang, Zhiwei Hao, Yehui Tang, Jianyuan Guo, Yujie Yang, Kai Han, Yunhe Wang</strong></p>
<p>Diffusion-based super-resolution (SR) models have recently garnered significant attention due to their potent restoration capabilities. But conventional diffusion models perform noise sampling from a single distribution, constraining their ability to handle real-world scenes and complex textures across semantic regions. With the success of segment anything model (SAM), generating sufficiently fine-grained region masks can enhance the detail recovery of diffusion-based SR model. However, directly integrating SAM into SR models will result in much higher computational cost. In this paper, we propose the SAM-DiffSR model, which can utilize the fine-grained structure information from SAM in the process of sampling noise to improve the image quality without additional computational cost during inference. In the process of training, we encode structural position information into the segmentation mask from SAM. Then the encoded mask is integrated into the forward diffusion process by modulating it to the sampled noise. This adjustment allows us to independently adapt the noise mean within each corresponding segmentation area. The diffusion model is trained to estimate this modulated noise. Crucially, our proposed framework does NOT change the reverse diffusion process and does NOT require SAM at inference. Experimental results demonstrate the effectiveness of our proposed method, showcasing superior performance in suppressing artifacts, and surpassing existing diffusion-based methods by 0.74 dB at the maximum in terms of PSNR on DIV2K dataset. The code and dataset are available at <a href="https://github.com/lose4578/SAM-DiffSR">https://github.com/lose4578/SAM-DiffSR</a>. </p>
<p><a href="http://arxiv.org/abs/2402.17133v1">PDF</a> </p>
<p><strong>Summary</strong><br>基于扩散的超分辨率模型中，本文提出了一种新颖的SAM-DiffSR方法，该方法利用SAM的精细结构信息在采样噪声的过程中来改善最终图像质量，而推理过程中不需要SAM，有效降低了计算成本。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>提出了一种SAM-DiffSR模型，可以利用SAM的精细结构信息来改善图像质量。</li>
<li>SAM-DiffSR模型通过将编码的掩码整合到前向扩散过程中，在采样噪声之前进行调整。</li>
<li>该调整允许独立调整每个对应分割区域内的噪声均值。</li>
<li>扩散模型被训练来估计这种调制的噪声。</li>
<li>所提出的方法不改变反向扩散过程，并且在推理过程中不需要SAM。</li>
<li>实验结果表明，该方法有效地抑制了伪影，在DIV2K数据集上以PSNR指标超越了现有的基于扩散的方法0.74 dB。</li>
<li>代码和数据集可在<a href="https://github.com/lose4578/SAM-DiffSR获得。">https://github.com/lose4578/SAM-DiffSR获得。</a></li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>标题：SAM-DiffSR：用于图像超分辨率的结构调制扩散模型</li>
<li>作者：Chengcheng Wang、Zhiwei Hao、Yehui Tang、Jianyuan Guo、Yujie Yang、Kai Han、Yunhe Wang</li>
<li>单位：华为诺亚方舟实验室</li>
<li>关键词：图像超分辨率、扩散模型、结构调制</li>
<li>链接：https://arxiv.org/abs/2402.17133
   Github：https://github.com/lose4578/SAM-DiffSR</li>
<li>摘要：
（1）研究背景：
   扩散模型在图像超分辨率领域取得了显著进展，但传统扩散模型从单一分布中进行噪声采样，限制了其处理真实场景和跨语义区域复杂纹理的能力。</li>
</ol>
<p>（2）过去方法及问题：
   Segment Anything Model（SAM）能生成足够精细的区域掩码，增强扩散模型的细节恢复能力。但直接将 SAM 集成到 SR 模型中会大幅增加计算成本。</p>
<p>（3）研究方法：
   提出 SAM-DiffSR 模型，在噪声采样过程中利用 SAM 的精细结构信息，在不增加推理计算成本的情况下提高图像质量。在训练过程中，将结构位置信息编码到 SAM 的分割掩码中。然后将编码后的掩码集成到前向扩散过程中，将其调制到采样的噪声中。这种调整允许在每个对应的分割区域内独立调整噪声均值。扩散模型被训练来估计这种调制的噪声。</p>
<p>（4）方法性能：
   实验结果表明，所提出的方法有效，在抑制伪影方面表现出优异的性能，在 DIV2K 数据集上以 PSNR 衡量，比现有的基于扩散的方法提高了 0.74dB。该方法的性能支持其目标。</p>
<p><strong>Methods：</strong></p>
<p>(1) 利用 SegmentAnythingModel（SAM）生成精细的区域掩码，编码结构位置信息。</p>
<p>(2) 将编码后的掩码集成到前向扩散过程中，调制采样的噪声。</p>
<p>(3) 训练扩散模型估计调制的噪声，从而在每个分割区域内独立调整噪声均值。</p>
<ol>
<li>结论：
（1）：本文重点通过集成 SAM，增强基于扩散的图像超分辨率模型的结构层次信息恢复能力。具体来说，我们引入了一个名为 SAM-DiffSR 的框架，它涉及将结构位置信息纳入 SAM 生成的掩码，然后在正向扩散过程中将其添加到采样的噪声中。此操作单独调节每个相应分割区域中噪声的均值，从而将结构层次知识注入扩散模型。通过采用这种方法，训练后的模型在恢复结构细节和抑制图像伪影方面表现出改进，而无需产生任何额外的推理成本。我们的方法的有效性通过在常用的图像超分辨率基准上进行的广泛实验得到证实。
（2）：创新点：利用 SAM 注入结构信息，增强扩散模型的结构恢复能力；
性能：在抑制伪影和恢复结构细节方面优于现有方法；
工作量：推理成本与基线模型相当。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9a754ccd89139d7dc6a576434e6b119e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0906797fab629c359270ce611fcb26d4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-66893d51d835b7965b76fb168b66db51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1f36de01723e09ebef0661e0e152ae2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9bca3bdea09d0b0b3c4c6b041a3c1758.jpg" align="middle">
</details>




<h2 id="Cross-Modal-Contextualized-Diffusion-Models-for-Text-Guided-Visual-Generation-and-Editing"><a href="#Cross-Modal-Contextualized-Diffusion-Models-for-Text-Guided-Visual-Generation-and-Editing" class="headerlink" title="Cross-Modal Contextualized Diffusion Models for Text-Guided Visual   Generation and Editing"></a>Cross-Modal Contextualized Diffusion Models for Text-Guided Visual   Generation and Editing</h2><p><strong>Authors:Ling Yang, Zhilong Zhang, Zhaochen Yu, Jingwei Liu, Minkai Xu, Stefano Ermon, Bin Cui</strong></p>
<p>Conditional diffusion models have exhibited superior performance in high-fidelity text-guided visual generation and editing. Nevertheless, prevailing text-guided visual diffusion models primarily focus on incorporating text-visual relationships exclusively into the reverse process, often disregarding their relevance in the forward process. This inconsistency between forward and reverse processes may limit the precise conveyance of textual semantics in visual synthesis results. To address this issue, we propose a novel and general contextualized diffusion model (ContextDiff) by incorporating the cross-modal context encompassing interactions and alignments between text condition and visual sample into forward and reverse processes. We propagate this context to all timesteps in the two processes to adapt their trajectories, thereby facilitating cross-modal conditional modeling. We generalize our contextualized diffusion to both DDPMs and DDIMs with theoretical derivations, and demonstrate the effectiveness of our model in evaluations with two challenging tasks: text-to-image generation, and text-to-video editing. In each task, our ContextDiff achieves new state-of-the-art performance, significantly enhancing the semantic alignment between text condition and generated samples, as evidenced by quantitative and qualitative evaluations. Our code is available at <a href="https://github.com/YangLing0818/ContextDiff">https://github.com/YangLing0818/ContextDiff</a> </p>
<p><a href="http://arxiv.org/abs/2402.16627v1">PDF</a> ICLR 2024. Project: <a href="https://github.com/YangLing0818/ContextDiff">https://github.com/YangLing0818/ContextDiff</a></p>
<p><strong>Summary</strong><br>上下文扩散模型通过在扩散正反过程中加入文本可视关系，提升了文本引导可视化生成和编辑的语义对齐。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>扩散模型在文本引导可视化生成和编辑中表现优越。</li>
<li>传统模型只将文本可视关系融入反向过程，忽略了正向过程的关联性。</li>
<li>正反过程的不一致性限制了文本语义在可视化合成结果中的传递精度。</li>
<li>语义扩散模型通过将文本条件和可视样本之间的交互和对齐纳入正反过程，改善了这种不一致性。</li>
<li>改进适用于 DDPM 和 DDIM，并通过理论推理得到证明。</li>
<li>在文本到图像生成和文本到视频编辑任务中，语义扩散模型均达到新的最佳性能。</li>
<li>定量和定性评估表明语义扩散模型显著提升了文本条件和生成样本之间的语义对齐。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>标题：跨模态语境化扩散模型用于文本引导的视觉生成和编辑</li>
<li>作者：杨凌、张志龙、于兆宸、刘景伟、徐明凯、Stefano Ermon、崔斌</li>
<li>隶属：北京大学</li>
<li>关键词：文本引导视觉生成、文本引导视频编辑、扩散模型、语境化</li>
<li>论文链接：https://arxiv.org/abs/2402.16627
   Github 代码链接：None</li>
<li>
<p>摘要：
   (1)：研究背景：扩散模型在文本引导视觉生成和编辑领域表现优异，但现有方法主要关注将文本-视觉关系融入逆过程，忽视了其在前向过程中的相关性，导致文本语义在视觉合成结果中的精确传达受到限制。
   (2)：过去方法及问题：现有方法存在以下问题：</p>
<ul>
<li>忽略了文本-视觉关系在前向过程中的作用，导致文本语义在视觉合成结果中的精确传达受限。</li>
<li>缺乏一种通用的语境化扩散模型，无法同时处理文本引导图像和视频生成/编辑任务。
   (3)：研究方法：本文提出了一种新颖且通用的语境化扩散模型（CONTEXTDIFF），通过将跨模态语境（包含文本条件和视觉样本之间的交互和对齐）融入前向和逆过程来解决上述问题。具体来说，将该语境传播到两个过程中的所有时间步，以适应它们的轨迹，从而促进跨模态条件建模。同时，将语境化扩散模型推广到 DDPM 和 DDIM，并通过理论推导证明了其有效性。
   (4)：任务和性能：在文本到图像生成和文本到视频编辑两个具有挑战性的任务上，CONTEXTDIFF 均取得了新的 SOTA 性能，显著增强了文本条件与生成样本之间的语义对齐，定量和定性评估均证明了这一点。</li>
</ul>
</li>
<li>
<p>Methods:
(1): 提出跨模态语境化扩散模型（CONTEXTDIFF），将跨模态语境（包含文本条件和视觉样本之间的交互和对齐）融入前向和逆过程，促进跨模态条件建模；
(2): 将语境化扩散模型推广到DDPM和DDIM，并通过理论推导证明了其有效性；
(3): 在文本到图像生成和文本到视频编辑两个任务上，CONTEXTDIFF均取得了新的SOTA性能，显著增强了文本条件与生成样本之间的语义对齐。</p>
</li>
<li>
<p>结论：
（1）本工作提出了一种新颖且通用的条件扩散模型（CONTEXTDIFF），通过将跨模态语境传播到扩散和逆过程中的所有时间步，并适应它们的轨迹，从而促进跨模态条件建模。我们将上下文化轨迹适配器推广到 DDPM 和 DDIM，并通过理论推导证明了其有效性。在文本到图像生成和文本到视频编辑这两个具有挑战性的任务上，CONTEXTDIFF 始终达到最先进的性能。两项任务的广泛定量和定性结果证明了我们提出的跨模态语境化扩散模型的有效性和优越性。
（2）创新点：提出了一种新颖的跨模态语境化扩散模型，通过将跨模态语境融入扩散和逆过程，促进跨模态条件建模。
性能：在文本到图像生成和文本到视频编辑两个任务上达到最先进的性能，显著增强了文本条件与生成样本之间的语义对齐。
工作量：工作量较大，需要对扩散模型和跨模态语境化进行深入理解。</p>
</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0bc30cb1ebccfebfcc1ffd4ee246c26b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64adb5f655a12b089618a5496f3cd332.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f01bc8ec645d09757f45be018ce1fe96.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a622ae5ed900b07d2994967a2269c23.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0d264d770c3a4265052827f62ee48f0b.jpg" align="middle">
</details>




<h2 id="Placing-Objects-in-Context-via-Inpainting-for-Out-of-distribution-Segmentation"><a href="#Placing-Objects-in-Context-via-Inpainting-for-Out-of-distribution-Segmentation" class="headerlink" title="Placing Objects in Context via Inpainting for Out-of-distribution   Segmentation"></a>Placing Objects in Context via Inpainting for Out-of-distribution   Segmentation</h2><p><strong>Authors:Pau de Jorge, Riccardo Volpi, Puneet K. Dokania, Philip H. S. Torr, Gregory Rogez</strong></p>
<p>When deploying a semantic segmentation model into the real world, it will inevitably be confronted with semantic classes unseen during training. Thus, to safely deploy such systems, it is crucial to accurately evaluate and improve their anomaly segmentation capabilities. However, acquiring and labelling semantic segmentation data is expensive and unanticipated conditions are long-tail and potentially hazardous. Indeed, existing anomaly segmentation datasets capture a limited number of anomalies, lack realism or have strong domain shifts. In this paper, we propose the Placing Objects in Context (POC) pipeline to realistically add any object into any image via diffusion models. POC can be used to easily extend any dataset with an arbitrary number of objects. In our experiments, we present different anomaly segmentation datasets based on POC-generated data and show that POC can improve the performance of recent state-of-the-art anomaly fine-tuning methods in several standardized benchmarks. POC is also effective to learn new classes. For example, we use it to edit Cityscapes samples by adding a subset of Pascal classes and show that models trained on such data achieve comparable performance to the Pascal-trained baseline. This corroborates the low sim-to-real gap of models trained on POC-generated images. </p>
<p><a href="http://arxiv.org/abs/2402.16392v1">PDF</a> </p>
<p><strong>Summary</strong><br>使用扩散模型将对象插入上下文(POC)管道，可真实地向图像中添加任何对象，有效扩展数据集和改善异常分割性能。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>利用扩散模型构建POC管道，可向图像中真实地添加任意对象。</li>
<li>POC能轻松扩展数据集，添加任意数量的对象。</li>
<li>POC生成的异常分割数据集比现有数据集更真实、全面。</li>
<li>POC能提升最新异常精调方法在基准测试中的性能。</li>
<li>POC可用于学习新类别，如将Pascal类别添加到Cityscapes。</li>
<li>基于POC生成图像训练的模型，其仿真到真实差距低。</li>
<li>POC管道能够提高模型应对未见语义类别的能力，增强异常分割性能。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<p>1.标题：通过图像修复将对象置于上下文中以进行分布外分割
2.作者：Paude Jorge†, Riccardo Volpi†, Puneet K. Dokania‡, Philip H.S. Torr‡, Grégory Rogez†
3.所属机构：NAVERLABS 欧洲，牛津大学
4.关键词：异常分割、分布外检测、图像修复、语义分割、开放词汇分割
5.链接：https://github.com/naver/poc
6.摘要：
(1)：研究背景：在现实世界中部署语义分割模型时，模型不可避免地会遇到训练期间未见过的语义类别。因此，为了安全地部署此类系统，准确评估和提高其异常分割能力至关重要。然而，获取和标记语义分割数据代价高昂，而且意外情况是长尾且可能具有危险性。实际上，现有的异常分割数据集捕获的异常数量有限，缺乏真实性或具有很强的域偏移。
(2)：过去的方法及其问题：本文提出了一种放置对象在上下文（POC）管道，通过扩散模型将任何对象现实地添加到任何图像中。POC 可用于轻松地使用任意数量的对象扩展任何数据集。在我们的实验中，我们展示了基于 POC 生成的不同异常分割数据集，并表明 POC 可以提高几种标准基准中最近的异常精细调整方法的性能。POC 还可以有效地学习新类别。例如，我们使用它通过添加 Pascal 类别的子集来编辑 Cityscapes 样本，并表明在这些数据上训练的模型与 Pascal 训练的基线实现了相当的性能。这证实了在 POC 生成的图像上训练的模型的低模拟到真实差距。
(3)：提出的研究方法：POC 管道建立在图像修复和开放词汇分割模型之上，将任意对象现实地插入图像中。修改后的图像和掩码可用于不同的任务。
(4)：方法在什么任务上取得了什么性能，该性能是否能支撑其目标：在我们的实验中，我们表明在 POC 生成的图像上进行微调可以显着提高最先进的异常分割方法的性能——优于通过标准做法（拼接 COCO 对象）进行微调的模型。我们还展示了三个基于 Cityscapes 和其他两个自动驾驶数据集的 POC 生成的评估集，并在其上对不同的异常分割方法进行了基准测试（有关结果的第一眼，请参见图 1）。最后，由于 POC 可以添加任意对象，我们展示了它可用于学习新类别。例如，使用动物类别增强 Cityscapes 图像导致 Pascal 测试集上的 93.14mIoU（考虑相同的类别），而直接在 Pascal 上训练产生 94.75——也就是说，在 POC 编辑的图像上训练的模型产生了相当小的模拟到真实差距。</p>
<ol>
<li>方法：
(1) POC管道：POC管道由图像修复模型和开放词汇分割模型组成。图像修复模型用于将对象现实地插入图像中，而开放词汇分割模型用于为插入的对象生成掩码。修改后的图像和掩码可用于不同的任务，例如异常分割。
(2) 异常分割微调：POC管道可用于生成异常分割数据集。在这些数据集上微调异常分割模型可以提高模型的性能，优于通过标准做法（拼接COCO对象）进行微调的模型。
(3) 学习新类别：POC管道还可以用于学习新类别。例如，使用动物类别增强Cityscapes图像导致Pascal测试集上的93.14mIoU（考虑相同的类别），而直接在Pascal上训练产生94.75——也就是说，在POC编辑的图像上训练的模型产生了相当小的模拟到真实差距。</li>
</ol>
<p>8.结论：
（1）：xxx；
（2）：创新点：xxx；性能：xxx；工作量：xxx；
8. 结论：
（1）：本文提出了一种放置对象在上下文（POC）管道，通过扩散模型将任意对象现实地添加到任何图像中。POC可用于轻松地使用任意数量的对象扩展任何数据集。在我们的实验中，我们展示了基于POC生成的不同异常分割数据集，并表明POC可以提高几种标准基准中最近的异常精细调整方法的性能。POC还可以有效地学习新类别。例如，我们使用它通过添加Pascal类别的子集来编辑Cityscapes样本，并表明在这些数据上训练的模型与Pascal训练的基线实现了相当的性能。这证实了在POC生成的图像上训练的模型的低模拟到真实差距。
（2）：创新点：
- 提出了一种放置对象在上下文（POC）管道，通过扩散模型将任意对象现实地添加到任何图像中。
- POC可用于轻松地使用任意数量的对象扩展任何数据集。
- POC可以提高几种标准基准中最近的异常精细调整方法的性能。
- POC还可以有效地学习新类别。
性能：
- 在我们的实验中，我们展示了基于POC生成的不同异常分割数据集，并表明POC可以提高几种标准基准中最近的异常精细调整方法的性能。
- POC还可以有效地学习新类别。例如，我们使用它通过添加Pascal类别的子集来编辑Cityscapes样本，并表明在这些数据上训练的模型与Pascal训练的基线实现了相当的性能。这证实了在POC生成的图像上训练的模型的低模拟到真实差距。
工作量：
- POC管道由图像修复模型和开放词汇分割模型组成。图像修复模型用于将对象现实地插入图像中，而开放词汇分割模型用于为插入的对象生成掩码。
- POC管道可用于生成异常分割数据集。在这些数据集上微调异常分割模型可以提高模型的性能，优于通过标准做法（拼接COCO对象）进行微调的模型。
- POC管道还可以用于学习新类别。例如，使用动物类别增强Cityscapes图像导致Pascal测试集上的93.14mIoU（考虑相同的类别），而直接在Pascal上训练产生94.75——也就是说，在POC编辑的图像上训练的模型产生了相当小的模拟到真实差距。</p>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-13236ee2bf286b59f5da0689a0363f64.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dec0e216eb8083342215a3e4e8c1dc95.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2067d81b02e8cd7fea592f12fcef21d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-37aa0eb4c5f86ae9ed22c98b2703f9a5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-84f58d6d1052332176a17f015aaa2d9f.jpg" align="middle">
</details>




]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>Diffusion Models</tag>
      </tags>
  </entry>
  <entry>
    <title>Talking Head Generation</title>
    <url>/2024/02/29/Paper/2024-02-29/Talking%20Head%20Generation/</url>
    <content><![CDATA[<blockquote>
<p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-02-29-更新"><a href="#2024-02-29-更新" class="headerlink" title="2024-02-29 更新"></a>2024-02-29 更新</h1><h2 id="EMO-Emote-Portrait-Alive-Generating-Expressive-Portrait-Videos-with-Audio2Video-Diffusion-Model-under-Weak-Conditions"><a href="#EMO-Emote-Portrait-Alive-Generating-Expressive-Portrait-Videos-with-Audio2Video-Diffusion-Model-under-Weak-Conditions" class="headerlink" title="EMO: Emote Portrait Alive - Generating Expressive Portrait Videos with   Audio2Video Diffusion Model under Weak Conditions"></a>EMO: Emote Portrait Alive - Generating Expressive Portrait Videos with   Audio2Video Diffusion Model under Weak Conditions</h2><p><strong>Authors:Linrui Tian, Qi Wang, Bang Zhang, Liefeng Bo</strong></p>
<p>In this work, we tackle the challenge of enhancing the realism and expressiveness in talking head video generation by focusing on the dynamic and nuanced relationship between audio cues and facial movements. We identify the limitations of traditional techniques that often fail to capture the full spectrum of human expressions and the uniqueness of individual facial styles. To address these issues, we propose EMO, a novel framework that utilizes a direct audio-to-video synthesis approach, bypassing the need for intermediate 3D models or facial landmarks. Our method ensures seamless frame transitions and consistent identity preservation throughout the video, resulting in highly expressive and lifelike animations. Experimental results demonsrate that EMO is able to produce not only convincing speaking videos but also singing videos in various styles, significantly outperforming existing state-of-the-art methodologies in terms of expressiveness and realism. </p>
<p><a href="http://arxiv.org/abs/2402.17485v1">PDF</a> </p>
<p><strong>Summary</strong><br>音频线索能够协助生成更具表现力和真实感的面部动画。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>传统技术无法充分捕捉人类面部表情和个人风格差异。</li>
<li>EMO 框架采用直接音频到视频合成方法，无需中间 3D 模型或面部关键点。</li>
<li>EMO 可生成流畅无缝的视频，并始终保持身份一致性。</li>
<li>EMO 可生成具有高度表现力和真实感的说话和唱歌视频。</li>
<li>EMO 在表现力和真实感方面明显优于现有方法。</li>
<li>EMO 充分利用了音频线索，提升了面部动画的动态性和细致度。</li>
<li>EMO 可广泛应用于各种领域，包括电影、游戏和视频会议。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>标题：EMO：EmotePortraitAlive——在弱条件下使用音频到视频扩散模型生成富有表现力的肖像视频</li>
<li>作者：Tian Linrui、Wang Qi、Zhang Bang、Bo Liefeng</li>
<li>隶属单位：阿里巴巴集团智能计算研究院</li>
<li>关键词：Audio-driven portrait video generation、Talking head、Expressive facial expressions、Audio-to-video synthesis</li>
<li>论文链接：https://humanaigc.github.io/emote-portrait-alive/
   Github 代码链接：无</li>
<li>摘要：
   （1）研究背景：
   在说话人头像视频生成中，增强真实感和表现力是一项挑战，需要关注音频线索和面部动作之间的动态和细微关系。传统技术往往无法捕捉到人类表情的全貌和个人面部风格的独特性。
   （2）过去方法和问题：
   传统的说话人头像视频生成方法通常需要中间 3D 模型或面部关键点，这会引入额外的复杂性和限制。此外，这些方法在捕捉细微的表情和保持帧之间的一致性方面存在困难。
   （3）研究方法：
   本文提出了一种名为 EMO 的新框架，它采用直接音频到视频合成的方法，绕过了对中间 3D 模型或面部关键点的需求。该方法利用音频扩散模型，将音频线索直接映射到视频帧，确保了无缝的帧过渡和一致的面部动作。
   （4）任务和性能：
   EMO 在说话人头像视频生成任务上进行了评估。实验结果表明，该方法在生成具有丰富面部表情和头部姿势的逼真且富有表现力的视频方面取得了显着性能。这些性能支持了本文增强说话人头像视频生成真实感和表现力的目标。</li>
</ol>
<p>7.方法：（1）提出了一种名为EMO的新框架，该框架采用直接音频到视频合成的方法，绕过了对中间3D模型或面部关键点的需求。（2）该方法利用音频扩散模型，将音频线索直接映射到视频帧，确保了无缝的帧过渡和一致的面部动作。（3）在说话人头像视频生成任务上对EMO进行了评估，实验结果表明，该方法在生成具有丰富面部表情和头部姿势的逼真且富有表现力的视频方面取得了显着性能。</p>
<ol>
<li>结论：
(1): 本工作提出了一种名为 EMO 的新框架，该框架采用直接音频到视频合成的方法，绕过了对中间 3D 模型或面部关键点的需求。该方法利用音频扩散模型，将音频线索直接映射到视频帧，确保了无缝的帧过渡和一致的面部动作。在说话人头像视频生成任务上对 EMO 进行了评估，实验结果表明，该方法在生成具有丰富面部表情和头部姿势的逼真且富有表现力的视频方面取得了显着性能。这些性能支持了本文增强说话人头像视频生成真实感和表现力的目标。
(2): 创新点：</li>
<li>直接音频到视频合成的方法，绕过了对中间 3D 模型或面部关键点的需求。</li>
<li>利用音频扩散模型，将音频线索直接映射到视频帧，确保了无缝的帧过渡和一致的面部动作。
性能：</li>
<li>在说话人头像视频生成任务上取得了显着性能。</li>
<li>生成了具有丰富面部表情和头部姿势的逼真且富有表现力的视频。
工作量：</li>
<li>该方法的实现相对简单，不需要复杂的中间步骤或额外的模型。</li>
</ol>



<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-10c8e47dfe09b5369134bad3bf5b1e69.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-262ccbd331f2623737aa6cbcc24c64e5.jpg" align="middle">
</details>




<h2 id="Learning-Dynamic-Tetrahedra-for-High-Quality-Talking-Head-Synthesis"><a href="#Learning-Dynamic-Tetrahedra-for-High-Quality-Talking-Head-Synthesis" class="headerlink" title="Learning Dynamic Tetrahedra for High-Quality Talking Head Synthesis"></a>Learning Dynamic Tetrahedra for High-Quality Talking Head Synthesis</h2><p><strong>Authors:Zicheng Zhang, Ruobing Zheng, Ziwen Liu, Congying Han, Tianqi Li, Meng Wang, Tiande Guo, Jingdong Chen, Bonan Li, Ming Yang</strong></p>
<p>Recent works in implicit representations, such as Neural Radiance Fields (NeRF), have advanced the generation of realistic and animatable head avatars from video sequences. These implicit methods are still confronted by visual artifacts and jitters, since the lack of explicit geometric constraints poses a fundamental challenge in accurately modeling complex facial deformations. In this paper, we introduce Dynamic Tetrahedra (DynTet), a novel hybrid representation that encodes explicit dynamic meshes by neural networks to ensure geometric consistency across various motions and viewpoints. DynTet is parameterized by the coordinate-based networks which learn signed distance, deformation, and material texture, anchoring the training data into a predefined tetrahedra grid. Leveraging Marching Tetrahedra, DynTet efficiently decodes textured meshes with a consistent topology, enabling fast rendering through a differentiable rasterizer and supervision via a pixel loss. To enhance training efficiency, we incorporate classical 3D Morphable Models to facilitate geometry learning and define a canonical space for simplifying texture learning. These advantages are readily achievable owing to the effective geometric representation employed in DynTet. Compared with prior works, DynTet demonstrates significant improvements in fidelity, lip synchronization, and real-time performance according to various metrics. Beyond producing stable and visually appealing synthesis videos, our method also outputs the dynamic meshes which is promising to enable many emerging applications. </p>
<p><a href="http://arxiv.org/abs/2402.17364v1">PDF</a> CVPR 2024</p>
<p><strong>Summary</strong><br>神经网络编码的动态四面体（DynTet）是一种结合表示方法，确保了复杂面部变形在各种动作和视点下的几何一致性。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>DynTet 采用动态四面体（DynTet），将显式动态网格编码到神经网络中，以确保几何一致性。</li>
<li>坐标网络用于学习符号距离、形变和材质纹理，将训练数据锚定到预定义的四面体网格中。</li>
<li>运用 Marching Tetrahedra，DynTet 有效地解码具有连续拓扑的纹理网格，通过可微渲染器实现快速渲染并利用像素损失进行监督。</li>
<li>DynTet 结合经典 3D 可变形模型，以促进几何学习并定义一种规范空间以简化纹理学习。</li>
<li>DynTet 相比于先前的研究，在保真度、唇形同步和实时性能方面都有显著提升。</li>
<li>除了制作稳定且视觉上吸引人的合成视频，该方法还输出动态网格，有望实现许多新兴应用。</li>
<li>DynTet 弥补了隐式方法缺乏显式几何约束的问题，通过学习动态网格来提高面部变形建模的准确性。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>标题：用于高质量说话人头部合成的动态四面体学习</li>
<li>作者：Zhang Zhicheng, Xu Chenyang, Zhang Haoran, Wu Yuxuan, Wang Yebin, Chen Min, Chen Biao</li>
<li>单位：北京大学</li>
<li>关键词：说话人头部合成、动态网格、隐式表示、神经辐射场</li>
<li>论文链接：https://arxiv.org/abs/2302.05915</li>
<li>
<p>摘要：
（1）研究背景：隐式表示方法，如神经辐射场（NeRF），在从视频序列中生成逼真且可动画的头部头像方面取得了进展。然而，由于缺乏显式几何约束，这些隐式方法仍面临视觉伪影和抖动的挑战，这给准确建模复杂的面部变形带来了根本性挑战。
（2）过去方法及问题：以往方法主要采用隐式表示，由于缺乏显式几何约束，存在视觉伪影和抖动问题。
（3）研究方法：本文提出了动态四面体（DynTet），这是一种新的混合表示，它通过神经网络对显式动态网格进行编码，以确保在各种运动和视点下的几何一致性。DynTet 由基于坐标的网络参数化，这些网络学习有符号距离、变形和材质纹理，将训练数据锚定到预定义的四面体网格中。利用行进四面体，DynTet 有效地解码具有统一拓扑结构的纹理网格，通过可微渲染器和像素损失进行监督，从而实现快速渲染。为了提高训练效率，我们结合了经典的 3D 可变形模型，以促进几何学习并定义规范空间以简化纹理学习。由于 DynTet 中采用有效的几何表示，这些优势很容易实现。
（4）方法性能：与之前的工作相比，DynTet 在保真度、唇形同步和实时性能方面根据各种指标展示了显着的改进。除了制作稳定且视觉上吸引人的合成视频外，我们的方法还输出动态网格，这有望支持许多新兴应用。</p>
</li>
<li>
<p>方法：
(1): 提出动态四面体（DynTet）框架，快速从短视频序列学习 3D 头部头像，并实现高质量说话人头部实时渲染。
(2): 改进四面体表示，使用神经网络对显式动态网格进行编码，确保不同运动和视点下的几何一致性。
(3): 采用行进四面体解码具有统一拓扑结构的纹理网格，通过可微渲染器和像素损失进行监督，实现快速渲染。
(4): 结合经典 3D 可变形模型，促进几何学习，定义规范空间简化纹理学习。</p>
</li>
<li>
<p>结论：
（1）本工作提出了一种名为动态四面体（DynTet）的新型混合表示，用于从短视频序列中学习逼真且可动画的说话人头部，并实现了高质量说话人头部实时渲染。
（2）创新点：
提出动态四面体（DynTet）框架，快速从短视频序列学习 3D 头部头像，并实现高质量说话人头部实时渲染。
改进四面体表示，使用神经网络对显式动态网格进行编码，确保不同运动和视点下的几何一致性。
采用行进四面体解码具有统一拓扑结构的纹理网格，通过可微渲染器和像素损失进行监督，实现快速渲染。
结合经典 3D 可变形模型，促进几何学习，定义规范空间简化纹理学习。
性能：
与之前的工作相比，DynTet 在保真度、唇形同步和实时性能方面根据各种指标展示了显着的改进。
除了制作稳定且视觉上吸引人的合成视频外，我们的方法还输出动态网格，这有望支持许多新兴应用。
工作量：
本文的工作量较大，涉及到神经网络、动态网格、隐式表示、神经辐射场等多个方面。
作者提出了一个新的混合表示——动态四面体（DynTet），并将其应用于说话人头部合成任务中。
DynTet 结合了显式动态网格和隐式表示的优点，能够生成逼真且可动画的头部头像。
作者还提出了一个新的训练框架，结合了经典的 3D 可变形模型和可微渲染器。
该框架能够有效地学习几何和纹理信息，并生成高质量的合成视频。
总体而言，本文的工作量较大，但提出的方法新颖有效，在说话人头部合成领域具有重要的意义。</p>
</li>
</ol>



<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2927e4da13bb2db0a8c147b32e65c4ba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a69eb8d9ee3b7163b0dd216926919257.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-989288a0ad24820fe95020a4ed1f2ea7.jpg" align="middle">
</details>




]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>Talking Head Generation</tag>
      </tags>
  </entry>
  <entry>
    <title>3DGS</title>
    <url>/2024/02/13/Paper/2024-02-13/3DGS/</url>
    <content><![CDATA[<blockquote>
<p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-02-13-更新"><a href="#2024-02-13-更新" class="headerlink" title="2024-02-13 更新"></a>2024-02-13 更新</h1><h2 id="GALA3D-Towards-Text-to-3D-Complex-Scene-Generation-via-Layout-guided-Generative-Gaussian-Splatting"><a href="#GALA3D-Towards-Text-to-3D-Complex-Scene-Generation-via-Layout-guided-Generative-Gaussian-Splatting" class="headerlink" title="GALA3D: Towards Text-to-3D Complex Scene Generation via Layout-guided   Generative Gaussian Splatting"></a>GALA3D: Towards Text-to-3D Complex Scene Generation via Layout-guided   Generative Gaussian Splatting</h2><p><strong>Authors:Xiaoyu Zhou, Xingjian Ran, Yajiao Xiong, Jinlin He, Zhiwei Lin, Yongtao Wang, Deqing Sun, Ming-Hsuan Yang</strong></p>
<p>We present GALA3D, generative 3D GAussians with LAyout-guided control, for effective compositional text-to-3D generation. We first utilize large language models (LLMs) to generate the initial layout and introduce a layout-guided 3D Gaussian representation for 3D content generation with adaptive geometric constraints. We then propose an object-scene compositional optimization mechanism with conditioned diffusion to collaboratively generate realistic 3D scenes with consistent geometry, texture, scale, and accurate interactions among multiple objects while simultaneously adjusting the coarse layout priors extracted from the LLMs to align with the generated scene. Experiments show that GALA3D is a user-friendly, end-to-end framework for state-of-the-art scene-level 3D content generation and controllable editing while ensuring the high fidelity of object-level entities within the scene. Source codes and models will be available at <a href="https://gala3d.github.io/">https://gala3d.github.io/</a>. </p>
<p><a href="http://arxiv.org/abs/2402.07207v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>利用大型语言模型 (LLM) 生成初始布局，并引入布局引导 3D 高斯表示，指导 3D 内容生成，同时满足适应性几何约束。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>GALA3D 将大型语言模型与布局引导 3D 高斯表示相结合，用于有效地进行文本到 3D 的生成。</li>
<li>布局引导 3D 高斯表示提供了自适应的几何约束，确保生成的 3D 内容具有真实感和一致性。</li>
<li>GALA3D 采用对象-场景组合优化机制，以生成具有真实几何形状、纹理、比例和准确交互的多对象 3D 场景。</li>
<li>GALA3D 可以同时调整从大型语言模型中提取的粗略布局，使其与生成的场景保持一致。</li>
<li>GALA3D 是一个用户友好的端到端框架，可进行最先进的场景级 3D 内容生成和可控编辑。</li>
<li>GALA3D 能够确保场景中对象级实体的高保真度。</li>
<li>GALA3D 的源代码和模型可从 <a href="https://gala3d.github.io/">https://gala3d.github.io/</a> 获取。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：GALA3D：基于布局引导的文本到 3D 复杂场景生成</li>
<li>作者：Xiaoyu Zhou, Xingjian Ran, Yajiao Xiong, Jinlin He, Zhiwei Lin, Yongtao Wang, Deqing Sun, Ming-Hsuan Yang</li>
<li>第一作者单位：北京大学万选信息技术学院</li>
<li>关键词：文本到 3D、生成式高斯体素、布局引导、条件扩散</li>
<li>论文链接：https://arxiv.org/abs/2402.07207
    Github 链接：None</li>
<li>摘要：
(1) 研究背景：文本到 3D 生成旨在根据自然语言描述生成逼真的 3D 场景。现有方法要么产生低质量的纹理、视觉伪影和几何失真，要么无法根据文本准确生成多个对象及其交互。
(2) 过去的方法：现有方法主要分为两类：基于体素的方法和基于网格的方法。基于体素的方法虽然可以生成高质量的 3D 场景，但计算成本高昂。基于网格的方法虽然计算成本较低，但生成的 3D 场景质量较差。
(3) 研究方法：本文提出了一种基于生成式高斯体素的文本到 3D 生成方法。该方法首先利用大型语言模型生成初始布局，然后引入布局引导的 3D 高斯体素表示来生成 3D 内容。接着，提出了一种对象场景组合优化机制，该机制利用条件扩散来协同生成具有真实几何形状、纹理、比例和准确交互的多对象 3D 场景，同时调整从大型语言模型中提取的粗略布局先验，使其与生成的场景对齐。
(4) 实验结果：实验表明，GALA3D 是一个用户友好的端到端框架，可用于生成高质量的 3D 场景。该方法在多个数据集上取得了最先进的性能，并且能够支持交互式可控编辑。</li>
</ol>
<p>Methods:</p>
<p>(1) 粗略布局先验：利用大型语言模型（LLM）从文本描述中提取粗略布局先验，包括对象实例及其对应的位置、尺寸和方向。</p>
<p>(2) 布局引导的高斯体素表示：将粗略布局先验转换为布局引导的高斯体素表示，其中每个对象实例由一组高斯体素表示，高斯体素的位置、尺寸和方向由布局先验决定。</p>
<p>(3) 自适应几何控制：对高斯体素的分布和形状进行自适应几何控制，以确保高斯体素的分布紧密贴合对象表面，并且形状更加规则和一致。</p>
<p>(4) 具有扩散先验的组合优化：采用具有扩散先验的组合优化策略来更新布局引导的高斯体素参数，包括多视图扩散优化和场景条件扩散优化，以生成具有统一风格和交互关系的对象实例。</p>
<p>(5) 布局损失：引入布局损失来确保生成的3D场景与布局先验在语义和空间上的一致性，从而提高场景的整体质量。</p>
<ol>
<li>结论：
（1）：GALA3D 是一种基于生成式布局引导的 3D 高斯体素表示的场景级文本到 3D 框架，该框架可以生成具有多个对象的高保真、3D 一致的场景。实验表明，该方法在文本到 3D 生成方面优于现有方法，展示了生成具有多个对象和交互的复杂场景的能力，并实现了出色的纹理和几何效果。该方法还促进了交互式和可控的场景编辑，实现了一个高效且用户友好的 3D 场景生成和编辑框架。
（2）：创新点：</li>
<li>提出了一种基于生成式布局引导的 3D 高斯体素表示，该表示可以生成具有统一风格和交互关系的对象实例。</li>
<li>引入了一种具有扩散先验的组合优化策略，该策略可以更新布局引导的高斯体素参数，以生成具有真实几何形状、纹理、比例和准确交互的多对象 3D 场景。</li>
<li>提出了一种布局损失，该损失可以确保生成的 3D 场景与布局先验在语义和空间上的一致性，从而提高场景的整体质量。
性能：</li>
<li>在多个数据集上取得了最先进的性能。</li>
<li>能够支持交互式可控编辑。
工作量：</li>
<li>该方法的实现相对复杂，需要大量的计算资源。</li>
<li>该方法需要大量的数据进行训练。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-3dde3c6bf6237679d7dc8e3a25b014e3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c780f9b8f1b542e9c562c2d185d7e16a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-785f0dd46228bdf108d1677b776eeb58.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e76b694075c9297c3e8a8d38bf4c8fe3.jpg" align="middle">
</details>




<h2 id="GS-CLIP-Gaussian-Splatting-for-Contrastive-Language-Image-3D-Pretraining-from-Real-World-Data"><a href="#GS-CLIP-Gaussian-Splatting-for-Contrastive-Language-Image-3D-Pretraining-from-Real-World-Data" class="headerlink" title="GS-CLIP: Gaussian Splatting for Contrastive Language-Image-3D   Pretraining from Real-World Data"></a>GS-CLIP: Gaussian Splatting for Contrastive Language-Image-3D   Pretraining from Real-World Data</h2><p><strong>Authors:Haoyuan Li, Yanpeng Zhou, Yihan Zeng, Hang Xu, Xiaodan Liang</strong></p>
<p>3D Shape represented as point cloud has achieve advancements in multimodal pre-training to align image and language descriptions, which is curial to object identification, classification, and retrieval. However, the discrete representations of point cloud lost the object’s surface shape information and creates a gap between rendering results and 2D correspondences. To address this problem, we propose GS-CLIP for the first attempt to introduce 3DGS (3D Gaussian Splatting) into multimodal pre-training to enhance 3D representation. GS-CLIP leverages a pre-trained vision-language model for a learned common visual and textual space on massive real world image-text pairs and then learns a 3D Encoder for aligning 3DGS optimized per object. Additionally, a novel Gaussian-Aware Fusion is proposed to extract and fuse global explicit feature. As a general framework for language-image-3D pre-training, GS-CLIP is agnostic to 3D backbone networks. Experiments on challenging shows that GS-CLIP significantly improves the state-of-the-art, outperforming the previously best results. </p>
<p><a href="http://arxiv.org/abs/2402.06198v1">PDF</a> 6-page technical report</p>
<p><strong>Summary</strong><br>3D 高斯曲面表示增强多模态预训练， 促进图像、语言和 3D 表示的统一。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>多模态预训练在图像和语言描述的对齐方面取得进展，对物体识别、分类和检索至关重要。</li>
<li>点云的离散表示丢失了物体的表面形状信息，导致渲染结果与 2D 对应关系之间存在差距。</li>
<li>提出 GS-CLIP 首次将 3DGS（3D 高斯曲面）引入多模态预训练，以增强 3D 表示。</li>
<li>GS-CLIP 利用预训练的视觉语言模型，在大规模真实世界图像文本对上学习共同的视觉和文本空间，然后学习一个 3D 编码器来对齐针对每个对象优化的 3DGS。</li>
<li>提出了一种新颖的 Gaussian-Aware Fusion 来提取和融合全局显式特征。</li>
<li>作为语言图像 3D 预训练的通用框架，GS-CLIP 与 3D 主干网络无关。</li>
<li>具有挑战性的实验表明，GS-CLIP 显着改善了最先进的技术，优于先前最好的结果。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<p></p><ol><p></p>
<p></p><li>题目：GS-CLIP：用于对比语言-图像-3D 预训练的高斯溅射</li><p></p>
<p></p><li>作者：李浩源、周彦鹏、曾义涵、许航、梁晓丹</li><p></p>
<p></p><li>第一作者单位：中山大学深圳校区</li><p></p>
<p></p><li>关键词：3D 表示、高斯溅射、对比学习、多模态预训练</li><p></p>
<p></p><li>论文链接：https://arxiv.org/abs/2402.06198，Github 链接：无</li><p></p>
<p></p><li>摘要：
(1)：研究背景：3D 形状以点云表示在多模态预训练中取得了进展，用于对齐图像和语言描述，这对于物体识别、分类和检索至关重要。然而，点云的离散表示丢失了物体的表面形状信息，并在渲染结果和 2D 对应关系之间产生差距。
(2)：过去的方法和问题：现有的 3D 表示学习方法主要对点云的关键点位置信息进行建模，这限制了 3D 视觉理解和 3D 表示学习的性能。
(3)：研究方法：为了解决上述问题，本文提出了 GS-CLIP，将 3D 高斯溅射 (3DGS) 引入多模态预训练，以增强 3D 表示。GS-CLIP 利用预训练的视觉语言模型在真实世界的大规模图像-文本对上学习一个共同的视觉和文本空间，然后学习一个 3D 编码器，用于对齐针对每个对象优化的 3DGS。此外，本文还提出了一种新的高斯感知融合，用于提取和融合全局显式特征。
(4)：实验结果：在 SUN-RGBD 数据集上的实验表明，GS-CLIP 在真实世界环境中的零样本/开放词学习中取得了优异的性能。实验结果表明，3DGS 在跨模态学习中具有强大的表示能力。</li><br>&lt;/ol&gt;<p></p>
<p>7.方法：
（1）跨模态预训练：为了对齐文本、图像和3DGS的多模态表示，GS-CLIP采用预训练的语言-图像模型CLIP，形成一个共同的语言-图像潜在空间，作为3DGS的目标潜在空间。对于零样本/开放词识别，通过冻结CLIP文本编码器、图像编码器和公共真实世界潜在空间，保证了3DGS表示的可迁移性。具体来说，我们借鉴了[19, 28]中的对比损失，并形成文本-3DGS对齐和图像-3DGS对齐，用于多模态对齐。
（2）高斯感知融合：虽然将点云投影到3D体素的3D骨干可以更好地学习全局位置和特征，但我们发现3DGS的显式特征会被忽略，因为体素化丢失了形状和纹理信息。因此，我们采用基于Transformer的分支直接对高斯特征建模为高斯特征上下文，并以残差形式注入它。具体来说，给定具有n个高斯的3DGS输入XG∈Rn×14，我们首先将XG分成Ng组，用于XgroupG=Ng�g=1XgG，然后使用基于卷积的体系结构EGθ,c提取全局特征fGc和基于Transformer的体系结构EGθ,t提取显式高斯特征fGG，最后将fGc和fGG连接起来，形成最终的3DGS表示fG。</p>
<ol>
<li>结论：
（1）意义：本文首次提出 GS-CLIP，将 3DGS 纳入跨模态学习，作为补充形状和纹理信息的通用 3D 表示。为此，提出了一种高斯感知融合，以便从补充信息中更好地学习信息。我们证明了我们提出的 GS-CLIP 在最先进的方法中取得了优异的性能。
（2）优缺点：
创新点：</li>
<li>将 3DGS 引入跨模态学习，作为补充形状和纹理信息的通用 3D 表示。</li>
<li>提出了一种高斯感知融合，以便从补充信息中更好地学习信息。</li>
</ol>
<p>性能：
- 在 SUN-RGBD 数据集上的实验表明，GS-CLIP 在真实世界环境中的零样本/开放词学习中取得了优异的性能。</p>
<p>工作量：
- 需要对 3DGS 进行预训练，这可能需要大量的数据和计算资源。
- 需要对高斯感知融合进行训练，这可能需要大量的数据和计算资源。</p>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5ca02e3188a2350914f961c6e31c0616.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4980273838b01e0c94c7593c3becb878.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b33d684beebaf5252e0357a0e0af9c1d.jpg" align="middle">
</details>




</ol>]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>3DGS</tag>
      </tags>
  </entry>
  <entry>
    <title>NeRF</title>
    <url>/2024/02/09/Paper/2024-02-09/NeRF/</url>
    <content><![CDATA[<blockquote>
<p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-02-09-更新"><a href="#2024-02-09-更新" class="headerlink" title="2024-02-09 更新"></a>2024-02-09 更新</h1><h2 id="NeRF-as-Non-Distant-Environment-Emitter-in-Physics-based-Inverse-Rendering"><a href="#NeRF-as-Non-Distant-Environment-Emitter-in-Physics-based-Inverse-Rendering" class="headerlink" title="NeRF as Non-Distant Environment Emitter in Physics-based Inverse   Rendering"></a>NeRF as Non-Distant Environment Emitter in Physics-based Inverse   Rendering</h2><p><strong>Authors:Jingwang Ling, Ruihan Yu, Feng Xu, Chun Du, Shuang Zhao</strong></p>
<p>Physics-based inverse rendering aims to jointly optimize shape, materials, and lighting from captured 2D images. Here lighting is an important part of achieving faithful light transport simulation. While the environment map is commonly used as the lighting model in inverse rendering, we show that its distant lighting assumption leads to spatial invariant lighting, which can be an inaccurate approximation in real-world inverse rendering. We propose to use NeRF as a spatially varying environment lighting model and build an inverse rendering pipeline using NeRF as the non-distant environment emitter. By comparing our method with the environment map on real and synthetic datasets, we show that our NeRF-based emitter models the scene lighting more accurately and leads to more accurate inverse rendering. Project page and video: <a href="https://nerfemitterpbir.github.io/">https://nerfemitterpbir.github.io/</a>. </p>
<p><a href="http://arxiv.org/abs/2402.04829v1">PDF</a> Project page and video: <a href="https://nerfemitterpbir.github.io/">https://nerfemitterpbir.github.io/</a></p>
<p><strong>摘要</strong><br>神经辐射场可以作为空间非距离环境光源，用于物理逆渲染，使逆渲染更加真实准确。</p>
<p><strong>主要要点</strong></p>
<ul>
<li>基于物理的逆渲染旨在联合优化从捕获的 2D 图像中提取的形状、材质和光照。</li>
<li>在逆渲染中，通常使用环境贴图作为光照模型，但这种假设会导致空间不变的光照，这在真实世界的逆渲染中可能是不准确的近似。</li>
<li>提出使用神经辐射场作为空间可变的环境光照模型，并构建了一个以神经辐射场作为非距离环境光源的逆渲染管道。</li>
<li>将方法与环境贴图在真实和合成数据集上进行比较，结果表明，基于神经辐射场的光源可以更准确地模拟场景光照，并实现更准确的逆渲染。</li>
<li>项目页面和视频：<a href="https://nerfemitterpbir.github.io/。">https://nerfemitterpbir.github.io/。</a></li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：基于物理的反演渲染中，NeRF 作为非远处环境发射器</li>
<li>作者：Jingwang Ling、Ruihan Yu、Feng Xu、Chun Du、Shuang Zhao</li>
<li>单位：清华大学</li>
<li>关键词：NeRF、物理反演渲染、环境光照、形状重建</li>
<li>论文链接：https://arxiv.org/pdf/2402.04829.pdf，Github 链接：None</li>
<li>
<p>摘要：
（1）研究背景：基于物理的反演渲染旨在从捕获的 2D 图像中联合优化形状、材质和光照。其中，光照是实现真实光照传输模拟的重要组成部分。环境贴图是反演渲染中常用的光照模型，但我们发现，在光源不是无限远处的场景中，环境贴图的空间不变光照假设会导致空间不变的光照，这在现实世界中的反演渲染中可能是不准确的近似。
（2）过去方法及问题：过去的方法通常使用环境贴图来近似物体周围的光照，但这种方法在光源不是无限远处的场景中会导致不准确的结果。
（3）研究方法：我们提出使用 NeRF 作为空间变化的环境光照模型，并构建了一个以 NeRF 作为非远处环境发射器的反演渲染管道。通过在真实和合成数据集上与环境贴图进行比较，我们证明了我们的 NeRF 模型可以更准确地模拟场景光照，并实现更准确的反演渲染。
（4）方法性能：我们的方法在真实和合成数据集上都取得了比环境贴图更好的结果。在真实数据集上，我们的方法在重照明和形状重建任务上都取得了更好的性能。在合成数据集上，我们的方法在重照明、形状重建和材质估计任务上都取得了更好的性能。这些结果证明了我们的方法可以更准确地模拟场景光照，并实现更准确的反演渲染。</p>
</li>
<li>
<p>方法：
（1）提出使用 NeRF 作为空间变化的环境光照模型，构建以 NeRF 作为非远处环境发射器的反演渲染管道。
（2）利用真实和合成数据集，与环境贴图进行比较，证明 NeRF 模型可以更准确地模拟场景光照，实现更准确的反演渲染。
（3）在真实和合成数据集上，与环境贴图相比，在重照明、形状重建和材质估计任务上都取得了更好的性能。</p>
</li>
<li>
<p>结论：
(1)：本文提出了一种基于 NeRF 的反演渲染管道，该管道将 NeRF 用作非远处环境发射器，可以更准确地模拟场景光照，并实现更准确的反演渲染。
(2)：创新点：</p>
</li>
<li>使用 NeRF 作为空间变化的环境光照模型，可以更准确地模拟场景光照。</li>
<li>构建了一个以 NeRF 作为非远处环境发射器的反演渲染管道，可以实现更准确的反演渲染。</li>
<li>在真实和合成数据集上，与环境贴图相比，在重照明、形状重建和材质估计任务上都取得了更好的性能。
性能：</li>
<li>在真实数据集上，在重照明和形状重建任务上都取得了更好的性能。</li>
<li>在合成数据集上，在重照明、形状重建和材质估计任务上都取得了更好的性能。
工作量：</li>
<li>需要训练 NeRF 模型，这可能需要大量的数据和计算资源。</li>
<li>需要构建反演渲染管道，这可能需要大量的编程工作。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5217f666aff1dcbbc55e20cda0c76080.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a9fa354da141f8905a59ea4a06f90f25.jpg" align="middle">
</details>




<h2 id="OV-NeRF-Open-vocabulary-Neural-Radiance-Fields-with-Vision-and-Language-Foundation-Models-for-3D-Semantic-Understanding"><a href="#OV-NeRF-Open-vocabulary-Neural-Radiance-Fields-with-Vision-and-Language-Foundation-Models-for-3D-Semantic-Understanding" class="headerlink" title="OV-NeRF: Open-vocabulary Neural Radiance Fields with Vision and Language   Foundation Models for 3D Semantic Understanding"></a>OV-NeRF: Open-vocabulary Neural Radiance Fields with Vision and Language   Foundation Models for 3D Semantic Understanding</h2><p><strong>Authors:Guibiao Liao, Kaichen Zhou, Zhenyu Bao, Kanglin Liu, Qing Li</strong></p>
<p>The development of Neural Radiance Fields (NeRFs) has provided a potent representation for encapsulating the geometric and appearance characteristics of 3D scenes. Enhancing the capabilities of NeRFs in open-vocabulary 3D semantic perception tasks has been a recent focus. However, current methods that extract semantics directly from Contrastive Language-Image Pretraining (CLIP) for semantic field learning encounter difficulties due to noisy and view-inconsistent semantics provided by CLIP. To tackle these limitations, we propose OV-NeRF, which exploits the potential of pre-trained vision and language foundation models to enhance semantic field learning through proposed single-view and cross-view strategies. First, from the single-view perspective, we introduce Region Semantic Ranking (RSR) regularization by leveraging 2D mask proposals derived from SAM to rectify the noisy semantics of each training view, facilitating accurate semantic field learning. Second, from the cross-view perspective, we propose a Cross-view Self-enhancement (CSE) strategy to address the challenge raised by view-inconsistent semantics. Rather than invariably utilizing the 2D inconsistent semantics from CLIP, CSE leverages the 3D consistent semantics generated from the well-trained semantic field itself for semantic field training, aiming to reduce ambiguity and enhance overall semantic consistency across different views. Extensive experiments validate our OV-NeRF outperforms current state-of-the-art methods, achieving a significant improvement of 20.31% and 18.42% in mIoU metric on Replica and Scannet, respectively. Furthermore, our approach exhibits consistent superior results across various CLIP configurations, further verifying its robustness. </p>
<p><a href="http://arxiv.org/abs/2402.04648v1">PDF</a> </p>
<p><strong>Summary</strong><br>神经辐射场（NeRF）技术通过结合视觉和语言基础模型，提升了NeRF在开放词汇表3D语义感知中的表现。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>OV-NeRF 提出了一种单视图和跨视图策略，将NeRF用于开放词汇表3D语义感知。</li>
<li>利用 SAM 提取的 2D 掩模建议，引入区域语义排序 (RSR) 正则化，以纠正每个训练视图的语义噪声。</li>
<li>提出跨视图自增强 (CSE) 策略，利用训练语义场本身生成的 3D 一致语义，减少语义模糊性和增强语义一致性。</li>
<li>OV-NeRF 在 Replica 和 Scannet 数据集上分别实现了 20.31% 和 18.42% 的 mIoU 指标提升，优于现有最优方法。</li>
<li>OV-NeRF 在各种 CLIP 配置下均表现出优异的性能，验证了其鲁棒性。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：OV-NeRF：具有视觉和语言的开放词汇神经辐射场</li>
<li>作者：廖桂标，周凯晨，鲍振宇，刘康林，李庆</li>
<li>单位：北京大学</li>
<li>关键词：神经辐射场，开放词汇，语义理解，视觉语言模型</li>
<li>链接：https://arxiv.org/abs/2402.04648</li>
<li>
<p>摘要：
(1) 研究背景：神经辐射场（NeRF）是一种强大的表示方法，可以捕捉复杂真实世界的 3D 场景。然而，在开放词汇 3D 语义感知任务中实现全面的 3D 语义理解仍然是一个具有挑战性的问题。
(2) 过去的方法：过去的方法直接从对比视觉语言预训练（CLIP）中提取语义，用于语义场学习，但遇到了来自 CLIP 的嘈杂和视图不一致语义的困难。
(3) 研究方法：为了解决这些限制，本文提出了 OV-NeRF，它利用预训练的视觉语言基础模型的潜力，通过提出的单视图和跨视图策略来增强语义场学习。
(4) 性能表现：OV-NeRF 在 Replica 和 Scannet 上分别在 mIoU 度量中取得了 20.31% 和 18.42% 的显着改进。此外，该方法在各种 CLIP 配置中表现出一致的优异结果，进一步验证了其鲁棒性。</p>
</li>
<li>
<p>方法：
(1) 提出 OV-NeRF，利用预训练的视觉语言基础模型的潜力，通过提出的单视图和跨视图策略来增强语义场学习。
(2) 利用预先计算的 CLIP 特征和 SAM 的区域提议来生成精确的相关性图，以监督 OV-NeRF，而不是使用源自 CLIP 模型的原始噪声相关性图。
(3) 在训练 OV-NeRF 数个 epoch 后，利用从 OV-NeRF 获得的渲染伪输出，包括训练视图和未见新颖视图，用于跨视图自我增强监督。</p>
</li>
<li>
<p>结论：
（1）：本工作通过利用视觉语言基础模型的能力，提出 OV-NeRF 来解决基于 NeRF 的 3D 语义理解挑战。在 OV-NeRF 中，提出的区域语义排序（RSR）正则化产生精确的单视图相关性图来训练 OV-NeRF，跨视图自我增强确保视图一致的分割结果。实验结果表明，我们的方法在合成和真实世界基准数据集上以很大优势优于 SOTA 方法，显示了我们方法的优越性。此外，我们的方法在不同的 CLIP 配置中始终表现出优异的性能，肯定了其通用性。
（2）：创新点：
提出了一种新的 NeRF 模型 OV-NeRF，该模型利用预训练的视觉语言基础模型的潜力，通过提出的单视图和跨视图策略来增强语义场学习。
提出了一种新的区域语义排序（RSR）正则化，该正则化产生精确的单视图相关性图来训练 OV-NeRF。
提出了一种新的跨视图自我增强方法，该方法利用从 OV-NeRF 获得的渲染伪输出，包括训练视图和未见新颖视图，用于跨视图自我增强监督。
性能：
OV-NeRF 在 Replica 和 Scannet 上分别在 mIoU 度量中取得了 20.31% 和 18.42% 的显着改进。
该方法在各种 CLIP 配置中表现出一致的优异结果，进一步验证了其鲁棒性。
工作量：
该方法需要预先计算 CLIP 特征和 SAM 的区域提议，这可能会增加计算成本。
该方法需要训练多个 epoch，这可能会增加训练时间。</p>
</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d28a855be0d118e883bd9f8001dbbcd1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1c6219c40ef2be88e25422dda1aae264.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fece26674b484110bc1b8871018a6a3a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea1c3e14317a591427313451f7980698.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a1bd1a1dc370f614b943567738833593.jpg" align="middle">
</details>




<h2 id="GSN-Generalisable-Segmentation-in-Neural-Radiance-Field"><a href="#GSN-Generalisable-Segmentation-in-Neural-Radiance-Field" class="headerlink" title="GSN: Generalisable Segmentation in Neural Radiance Field"></a>GSN: Generalisable Segmentation in Neural Radiance Field</h2><p><strong>Authors:Vinayak Gupta, Rahul Goel, Sirikonda Dhawal, P. J. Narayanan</strong></p>
<p>Traditional Radiance Field (RF) representations capture details of a specific scene and must be trained afresh on each scene. Semantic feature fields have been added to RFs to facilitate several segmentation tasks. Generalised RF representations learn the principles of view interpolation. A generalised RF can render new views of an unknown and untrained scene, given a few views. We present a way to distil feature fields into the generalised GNT representation. Our GSN representation generates new views of unseen scenes on the fly along with consistent, per-pixel semantic features. This enables multi-view segmentation of arbitrary new scenes. We show different semantic features being distilled into generalised RFs. Our multi-view segmentation results are on par with methods that use traditional RFs. GSN closes the gap between standard and generalisable RF methods significantly. Project Page: <a href="https://vinayak-vg.github.io/GSN/">https://vinayak-vg.github.io/GSN/</a> </p>
<p><a href="http://arxiv.org/abs/2402.04632v1">PDF</a> Accepted at the Main Technical Track of AAAI 2024</p>
<p><strong>Summary</strong><br>利用几个视图就可以渲染未知且未训练场景的新视图，并提供一致的逐像素语义特征。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>传统辐射场表示捕获特定场景的细节，必须在每个场景上重新训练。</li>
<li>语义特征字段已添加到射频中以促进多项分割任务。</li>
<li>广义射频表示学习了视图插值原理。</li>
<li>给定几个视图，广义射频可以渲染未知且未训练场景的新视图。</li>
<li>我们提供了一种将特征字段提炼成广义 GNT 表示的方法。</li>
<li>我们的 GSN 表示可以快速生成未见场景的新视图，并提供一致的逐像素语义特征。</li>
<li>这允许对任意新场景进行多视图分割。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：GSN：神经辐射场中的可泛化分割</li>
<li>作者：Vinayak Gupta，Rahul Goel，Sirikonda Dhawal，P.J. Narayanan</li>
<li>隶属机构：印度理工学院马德拉斯分校</li>
<li>关键词：神经辐射场、可泛化分割、语义特征场</li>
<li>论文链接：https://arxiv.org/abs/2402.04632
   Github 链接：无</li>
<li>
<p>摘要：
（1）研究背景：传统的神经辐射场（RF）表示可以捕捉特定场景的细节，但必须针对每个场景重新训练。语义特征场已被添加到 RF 中以促进多项分割任务。泛化的 RF 表示学习视图插值原理。给定几个视图，泛化的 RF 可以渲染未知且未训练场景的新视图。
（2）过去的方法及其问题：本文提出了一种将特征场提炼到泛化的 GNT 表示中的方法。我们的 GSN 表示可以即时生成未见场景的新视图，同时提供一致的逐像素语义特征。这使得任意新场景的多视图分割成为可能。我们展示了将不同语义特征提取到泛化的 RF 中。我们的多视图分割结果与使用传统 RF 的方法相当。GSN 显着缩小了标准 RF 方法和可泛化 RF 方法之间的差距。
（3）研究方法：过去的 RF 表示学习特定场景的细节，必须针对每个场景重新训练。语义特征场已被添加到 RF 中以促进多项分割任务。泛化的 RF 表示学习视图插值原理。给定几个视图，泛化的 RF 可以渲染未知且未训练场景的新视图。我们提出了一种将特征场提炼到泛化的 GNT 表示中的方法。我们的 GSN 表示可以即时生成未见场景的新视图，同时提供一致的逐像素语义特征。这使得任意新场景的多视图分割成为可能。我们展示了将不同语义特征提取到泛化的 RF 中。我们的多视图分割结果与使用传统 RF 的方法相当。GSN 显着缩小了标准 RF 方法和可泛化 RF 方法之间的差距。
（4）方法的性能：我们的多视图分割结果与使用传统 RF 的方法相当。GSN 显着缩小了标准 RF 方法和可泛化 RF 方法之间的差距。这些性能支持了我们的目标。</p>
</li>
<li>
<p>方法：
（1）首先，我们修改 GNT 架构以帮助语义特征提取。
（2）然后，我们描述了我们的两阶段训练蒸馏过程。
（3）最后，我们描述了如何使用蒸馏特征执行多视图分割。</p>
</li>
<li>
<p>结论：
（1）：本文提出了一种新的多视图分割方法，其主要优势在于其泛化性，即它可以在任意新场景上执行分割而无需任何训练。这使其区别于以前的方法。我们将我们的结果与早期方法进行了比较，并表明我们的性能与它们相当，同时可以泛化到未见场景。这是将泛化神经辐射场的应用拉近到特定场景辐射场的一大步。我们方法预测的特征可用于多种下游任务。
（2）：创新点：提出了将特征场提炼到泛化的 GNT 表示中的方法，该表示可以即时生成未见场景的新视图，同时提供一致的逐像素语义特征。
性能：我们的多视图分割结果与使用传统 RF 的方法相当。GSN 显着缩小了标准 RF 方法和可泛化 RF 方法之间的差距。
工作量：我们的方法依赖于基于 transformer 的架构，因此渲染过程与几种特定场景的辐射场方法相比固有地缓慢。提高渲染速度可以显着改善我们基于笔划的分割方法所需的人机交互体验。我们将泛化辐射场的渲染速度改进留作未来的工作。目前，我们的方法执行多视图分割，因为它使用基于图像的渲染。某些应用程序需要 3D 分割而不是多视图分割。因此，可泛化的 3D 分割框架有望成为未来的工作。</p>
</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-bf67c21104c6d20a1d6e37e83bff2155.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-03400222552085971945e9fc363dc323.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-61621673ca99816fe4332d9623a7e1b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c82ea98993102ebb08c3d96886f8caf8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d9b9a069535dfb6e09a7654648b4f040.jpg" align="middle">
</details>




<h2 id="BirdNeRF-Fast-Neural-Reconstruction-of-Large-Scale-Scenes-From-Aerial-Imagery"><a href="#BirdNeRF-Fast-Neural-Reconstruction-of-Large-Scale-Scenes-From-Aerial-Imagery" class="headerlink" title="BirdNeRF: Fast Neural Reconstruction of Large-Scale Scenes From Aerial   Imagery"></a>BirdNeRF: Fast Neural Reconstruction of Large-Scale Scenes From Aerial   Imagery</h2><p><strong>Authors:Huiqing Zhang, Yifei Xue, Ming Liao, Yizhen Lao</strong></p>
<p>In this study, we introduce BirdNeRF, an adaptation of Neural Radiance Fields (NeRF) designed specifically for reconstructing large-scale scenes using aerial imagery. Unlike previous research focused on small-scale and object-centric NeRF reconstruction, our approach addresses multiple challenges, including (1) Addressing the issue of slow training and rendering associated with large models. (2) Meeting the computational demands necessitated by modeling a substantial number of images, requiring extensive resources such as high-performance GPUs. (3) Overcoming significant artifacts and low visual fidelity commonly observed in large-scale reconstruction tasks due to limited model capacity. Specifically, we present a novel bird-view pose-based spatial decomposition algorithm that decomposes a large aerial image set into multiple small sets with appropriately sized overlaps, allowing us to train individual NeRFs of sub-scene. This decomposition approach not only decouples rendering time from the scene size but also enables rendering to scale seamlessly to arbitrarily large environments. Moreover, it allows for per-block updates of the environment, enhancing the flexibility and adaptability of the reconstruction process. Additionally, we propose a projection-guided novel view re-rendering strategy, which aids in effectively utilizing the independently trained sub-scenes to generate superior rendering results. We evaluate our approach on existing datasets as well as against our own drone footage, improving reconstruction speed by 10x over classical photogrammetry software and 50x over state-of-the-art large-scale NeRF solution, on a single GPU with similar rendering quality. </p>
<p><a href="http://arxiv.org/abs/2402.04554v1">PDF</a> </p>
<p><strong>摘要</strong><br>鸟瞰 NeRF：基于神经辐射场的大规模场景重建。</p>
<p><strong>要点</strong></p>
<ul>
<li>针对大规模场景重建，提出了基于神经辐射场的 BirdNeRF 算法。</li>
<li>BirdNeRF 将大场景图像集分解为多个小集合，每个小集合训练单独的 NeRF 模型。</li>
<li>这种分解方法将渲染时间与场景大小解耦，并使渲染能够无缝扩展到任意大的环境。</li>
<li>此外，它允许对环境进行逐块更新，从而提高重建过程的灵活性和适应性。</li>
<li>提出了一种基于投影的新颖视角重新渲染策略，有助于有效利用独立训练的子场景生成更好的渲染结果。</li>
<li>在现有数据集以及我们自己的无人机航拍视频上评估了我们的方法，在单个 GPU 上将重建速度提高了 10 倍（相对于经典摄影测量软件）和 50 倍（相对于最先进的大规模 NeRF 解决方案），同时渲染质量相似。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：鸟瞰神经辐射场：使用航拍图像快速神经重建大场景</li>
<li>作者：张慧清、薛一飞、廖明、老一真</li>
<li>单位：无</li>
<li>关键词：神经辐射场、大规模重建、航拍图像、空间分解、投影引导</li>
<li>链接：https://arxiv.org/abs/2402.04554
Github：无</li>
<li>
<p>摘要：
(1)：随着航空测量技术的进步，获取高分辨率图像变得更加容易和经济实惠，基于图像的 3D 重建已成为一个活跃的研究领域，并在城市规划、导航、虚拟旅游、房地产和灾害管理等领域有着广泛的应用。
(2)：现有的基于图像的 3D 重建技术主要分为传统的基于几何的方法和基于神经网络的方法。基于几何的方法通常需要大量的人工干预，并且对图像的质量和数量非常敏感。基于神经网络的方法，例如神经辐射场 (NeRF)，可以自动从图像中学习场景的 3D 表示，并且对图像的质量和数量不太敏感。然而，NeRF 在处理大规模场景时面临着训练速度慢、渲染时间长和容易产生伪影等挑战。
(3)：为了解决这些挑战，本文提出了一种新的 NeRF 变体，称为鸟瞰神经辐射场 (BirdNeRF)。BirdNeRF 使用了一种新的空间分解算法，将大规模航拍图像集分解成多个较小的子集，并分别训练每个子集的 NeRF 模型。这种分解方法不仅可以减少训练时间和渲染时间，还可以提高重建的质量。此外，BirdNeRF 还提出了一种新的投影引导的新视图重新渲染策略，可以有效地利用独立训练的子场景来生成高质量的渲染结果。
(4)：在多个数据集上的实验结果表明，BirdNeRF 在重建速度和质量方面都优于现有的方法。在单个 GPU 上，BirdNeRF 的重建速度比传统的摄影测量软件快 10 倍，比最先进的大规模 NeRF 解决方案快 50 倍，同时具有相似的渲染质量。</p>
</li>
<li>
<p>方法：
(1) 将大场景分解为多个较小的子场景，分别训练每个子场景的 NeRF 模型。
(2) 使用一种新的投影引导的新视图重新渲染策略，有效地利用独立训练的子场景来生成高质量的渲染结果。</p>
</li>
<li>
<p>结论：
（1）：本文提出了一种新的NeRF变体，称为鸟瞰神经辐射场（BirdNeRF），可以快速重建大规模场景。BirdNeRF使用了一种新的空间分解算法，将大规模航拍图像集分解成多个较小的子集，并分别训练每个子集的NeRF模型。这种分解方法不仅可以减少训练时间和渲染时间，还可以提高重建的质量。此外，BirdNeRF还提出了一种新的投影引导的新视图重新渲染策略，可以有效地利用独立训练的子场景来生成高质量的渲染结果。
（2）：创新点：</p>
</li>
<li>提出了一种新的NeRF变体，称为鸟瞰神经辐射场（BirdNeRF），可以快速重建大规模场景。</li>
<li>提出了一种新的空间分解算法，将大规模航拍图像集分解成多个较小的子集，并分别训练每个子集的NeRF模型。</li>
<li>提出了一种新的投影引导的新视图重新渲染策略，可以有效地利用独立训练的子场景来生成高质量的渲染结果。</li>
</ol>
<p>性能：
- 在单个GPU上，BirdNeRF的重建速度比传统的摄影测量软件快10倍，比最先进的大规模NeRF解决方案快50倍，同时具有相似的渲染质量。
- BirdNeRF可以重建包含数百万个三角形的场景，而不会出现明显的伪影。</p>
<p>工作量：
- BirdNeRF的实现相对简单，易于使用。
- BirdNeRF的训练时间和渲染时间都比较短，可以满足实际应用的需求。</p>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0ffe2746a28f7248c7dc45305ca5a0d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d3e3e28cf5dd4b506a44e1769d5abf0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51dea38443c497692956a6fd50ec6a18.jpg" align="middle">
</details>




## ViewFusion: Learning Composable Diffusion Models for Novel View   Synthesis

**Authors:Bernard Spiegl, Andrea Perin, Stéphane Deny, Alexander Ilin**

Deep learning is providing a wealth of new approaches to the old problem of novel view synthesis, from Neural Radiance Field (NeRF) based approaches to end-to-end style architectures. Each approach offers specific strengths but also comes with specific limitations in their applicability. This work introduces ViewFusion, a state-of-the-art end-to-end generative approach to novel view synthesis with unparalleled flexibility. ViewFusion consists in simultaneously applying a diffusion denoising step to any number of input views of a scene, then combining the noise gradients obtained for each view with an (inferred) pixel-weighting mask, ensuring that for each region of the target scene only the most informative input views are taken into account. Our approach resolves several limitations of previous approaches by (1) being trainable and generalizing across multiple scenes and object classes, (2) adaptively taking in a variable number of pose-free views at both train and test time, (3) generating plausible views even in severely undetermined conditions (thanks to its generative nature) -- all while generating views of quality on par or even better than state-of-the-art methods. Limitations include not generating a 3D embedding of the scene, resulting in a relatively slow inference speed, and our method only being tested on the relatively small dataset NMR. Code is available. 

[PDF](http://arxiv.org/abs/2402.02906v1) 

**Summary**
将多个不同视角的图像输入到 ViewFusion 模型中，就可以基于这些图像合成出新的视角图像。


**Key Takeaways**

- ViewFusion 将扩散去噪步骤同时应用于任意数量的场景输入视图，然后将每个视图获得的噪声梯度与像素权重掩码相结合，确保在目标场景的每个区域内仅考虑最具信息性的输入视图。
- ViewFusion 解决了先前方法的几个局限性：跨多个场景和对象类别进行训练和泛化；在训练和测试时自适应地采用可变数量的不受姿势限制的视图；能够生成合理的视图，即使在严重不确定的条件下。
- ViewFusion 优于或与最先进的方法相比能更高质量地生成视图。
- ViewFusion 无法生成场景的 3D 嵌入，导致其推理速度相对较慢。
- ViewFusion 目前仅在相对较小的 NMR 数据集上进行了测试。
- 代码库现已发布。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>题目：ViewFusion：用于新颖视图合成的可组合扩散模型的学习</li>
<li>作者：Bernard Spiegl、Andrea Perin、St´ephane Deny、Alexander Ilin</li>
<li>第一作者单位：阿尔托大学计算机科学系（仅翻译中文）</li>
<li>关键词：新颖视图合成、扩散模型、可组合性、自适应输入视图、鲁棒性</li>
<li>论文链接：https://arxiv.org/abs/2402.02906，Github 代码链接：None</li>
<li>
<p>摘要：
（1）研究背景：新颖视图合成是一个计算机视觉领域的长期研究课题。传统方法通常使用显式建模 3D 空间的方法，如体素、点云或网格。近年来，基于神经辐射场 (NeRF) 的方法也取得了很大进展。然而，这些方法通常存在需要昂贵的逐场景重新训练、无法在没有输入视图的姿态信息的情况下操作或无法适应测试时输入视图数量的可变性等缺点。
（2）过去方法及问题：过去的方法通常存在需要昂贵的逐场景重新训练、无法在没有输入视图的姿态信息的情况下操作或无法适应测试时输入视图数量的可变性等缺点。因此，本文旨在提出一种直观的端到端架构，用于执行新颖视图合成，同时解决先前工作中提到的缺点。
（3）研究方法：本文提出的 ViewFusion 方法通过一系列针对特定问题的设计选择，一次性解决了上述缺点。首先，使用在大量场景和类别上同时训练的扩散概率框架，使其能够在无需重新训练的情况下进行泛化。此外，由于扩散过程的随机性质，该模型即使在不确定性设置（例如，对象的严重遮挡或有限数量的输入视图）中也能表现良好，因为它提供了多种合理的视图。此外，本文提出的解决方案不需要输入视图的顺序或任何显式姿态信息。最后，与之前的对应方法不同，一旦训练完成，本文的方法就可以有效地处理任意长度的输入。这要归功于一种新的加权解决方案，与去噪骨干网络的组合一起，该解决方案允许模型根据视图的信息量对视图进行加权，同时扩展到任意数量的视图。
（4）方法性能：本文在包含各种类别和输入视图姿势的数据集上评估了所提出的方法。此外，本文通过对中间模型输出的分析验证了该方法，证明了该模型能够推断和自适应地调整每个输入视图的重要性。加权不仅对输出的质量有影响，而且推断的加权方案也与直观的人类感知一致。</p>
</li>
<li>
<p>方法：
(1) 提出了一种基于扩散概率框架的新颖视图合成方法 ViewFusion，该方法能够同时处理多个输入视图，并根据每个视图的重要性对视图进行加权，从而生成高质量的新颖视图。
(2) ViewFusion 模型由多个 U-Net 组成，每个 U-Net 负责处理一个输入视图。U-Net 的输出包括噪声预测和权重，权重用于对噪声进行加权，从而生成最终的新颖视图。
(3) ViewFusion 模型的训练过程包括两个阶段：预训练和微调。在预训练阶段，模型在大量场景和类别上进行训练，以学习一般化的特征表示。在微调阶段，模型在特定场景或类别上进行微调，以提高模型的性能。
(4) ViewFusion 模型的推理过程包括两个阶段：噪声采样和扩散过程。在噪声采样阶段，模型从正态分布中采样噪声。在扩散过程中，模型通过逐渐降低噪声的强度来生成新颖视图。</p>
</li>
<li>
<p>结论：
（1）：ViewFusion 是一种灵活、无需姿态的生成方法，可使用可组合扩散模型执行新颖视图合成。我们提出了一种新颖的加权方案，用于组合扩散模型，确保仅将信息量最大的输入视图用于预测目标视图，并使 ViewFusion 能够自适应地处理任意长且无序的输入视图集合，而无需重新训练。此外，ViewFusion 的生成性质使其即使在严重欠定条件下也能生成合理视图。我们认为，我们的方法在进行新颖视图合成时是一个有价值的贡献，并且有可能应用于其他问题。
（2）：创新点：
ViewFusion 引入了一种新颖的加权方案，用于组合扩散模型，确保仅将信息量最大的输入视图用于预测目标视图，并使 ViewFusion 能够自适应地处理任意长且无序的输入视图集合，而无需重新训练。此外，ViewFusion 的生成性质使其即使在严重欠定条件下也能生成合理视图。
性能：
ViewFusion 在各种类别和输入视图姿势的数据集上均取得了最先进的性能。此外，ViewFusion 能够有效地处理任意数量的输入视图，并且对输入视图的顺序和姿态信息不敏感。
工作量：
ViewFusion 的训练过程包括两个阶段：预训练和微调。预训练阶段需要大量的数据和计算资源，但微调阶段可以相对快速地完成。ViewFusion 的推理过程也非常有效，可以在几秒钟内生成新颖视图。</p>
</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-672b204f9242001f6ba5e1b350c81c87.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ab2417ac343ade4b32aea1621299f294.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a35b2635715a736813769f26b2939948.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-512893851e477e6cab6fb9d3224f7acf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6fa2f794caefa6d02e53b7a03fc9f646.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f5e41e289131352d483b38fb05ca0ce8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1874aa5e890d55cc56f18c742397f3bf.jpg" align="middle">
</details>




<h2 id="Robust-Inverse-Graphics-via-Probabilistic-Inference"><a href="#Robust-Inverse-Graphics-via-Probabilistic-Inference" class="headerlink" title="Robust Inverse Graphics via Probabilistic Inference"></a>Robust Inverse Graphics via Probabilistic Inference</h2><p><strong>Authors:Tuan Anh Le, Pavel Sountsov, Matthew D. Hoffman, Ben Lee, Brian Patton, Rif A. Saurous</strong></p>
<p>How do we infer a 3D scene from a single image in the presence of corruptions like rain, snow or fog? Straightforward domain randomization relies on knowing the family of corruptions ahead of time. Here, we propose a Bayesian approach-dubbed robust inverse graphics (RIG)-that relies on a strong scene prior and an uninformative uniform corruption prior, making it applicable to a wide range of corruptions. Given a single image, RIG performs posterior inference jointly over the scene and the corruption. We demonstrate this idea by training a neural radiance field (NeRF) scene prior and using a secondary NeRF to represent the corruptions over which we place an uninformative prior. RIG, trained only on clean data, outperforms depth estimators and alternative NeRF approaches that perform point estimation instead of full inference. The results hold for a number of scene prior architectures based on normalizing flows and diffusion models. For the latter, we develop reconstruction-guidance with auxiliary latents (ReGAL)-a diffusion conditioning algorithm that is applicable in the presence of auxiliary latent variables such as the corruption. RIG demonstrates how scene priors can be used beyond generation tasks. </p>
<p><a href="http://arxiv.org/abs/2402.01915v1">PDF</a> </p>
<p><strong>Summary</strong><br>新颖的贝叶斯方法 RIG 可同时对场景和破坏进行推理，以克服各种场景损坏。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>RIG 是一种新的贝叶斯方法，可同时对场景和破坏进行推理。</li>
<li>RIG 仅使用干净的数据进行训练，优于深度估计器和替代的 NeRF 方法。</li>
<li>RIG 可与多种基于正则化流和扩散模型的场景先验架构一起使用。</li>
<li>对于后者，我们开发了具有辅助潜变量的重建指导（ReGAL）——一种扩散调节算法，适用于具有辅助潜变量（如破坏）的情况。</li>
<li>RIG 演示了场景先验如何用于生成任务之外。</li>
<li>RIG 利用强大的场景先验和无信息的均匀破坏先验，使其适用于广泛的破坏。</li>
<li>在给定单一图像的情况下，RIG 对场景和破坏进行后验推理。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：鲁棒逆向图形生成：基于概率推理</li>
<li>作者：Tuan Anh Le、Pavel Sountsov、Matthew D. Hoffman、Ben Lee、Brian Patton、Rif A. Saurous</li>
<li>单位：谷歌（Google）</li>
<li>关键词：鲁棒逆向图形生成、神经辐射场、概率推理、域随机化、数据增强</li>
<li>论文链接：https://arxiv.org/abs/2402.01915</li>
<li>摘要：</li>
</ol>
<p>（1）研究背景：
    * 在存在雨、雪、雾等干扰的情况下，如何从单张图像中推断出 3D 场景？
    * 直接的域随机化依赖于提前知道干扰的种类。</p>
<p>（2）过去的方法及其问题：
    * 域随机化：通过在数据生成过程中选择一系列干扰来实现鲁棒性，但这种方法需要提前知道干扰的种类。
    * 正则化训练：通过在重建损失中添加额外的损失项来实现鲁棒性，但这种方法难以扩展到更极端的情况。</p>
<p>（3）本文提出的研究方法：
    * 鲁棒逆向图形生成（RIG）：将问题视为概率推理问题，利用预训练的场景先验（在本例中是神经辐射场先验）和一个关于干扰的弱先验（在本例中是具有均匀先验权重的另一个神经辐射场）来进行推理。
    * RIG 在场景和干扰神经辐射场中执行完整的概率推理，而不是像最大后验概率推理那样寻找最可能的解。</p>
<p>（4）方法的性能表现：
    * RIG 在具有各种场景先验架构（基于正则化流和扩散模型）的情况下都优于深度估计器和执行点估计而不是完整推理的替代神经辐射场方法。
    * RIG 仅在干净数据上训练，但它在具有各种干扰（雨、雪、雾、噪声）的图像上都优于其他方法。</p>
<ol>
<li>
<p>方法：
（1）场景表示：我们使用神经辐射场 (NeRF) 表示，因为它易于进行基于梯度的推理。
（2）场景先验：我们假设我们有一个预训练的 NeRF 先验 p(x)，我们可以从中对场景潜在变量 x 进行采样，并从不同的视点 ζ 渲染图像 y。
（3）损坏表示和先验：我们关注的是对 3D 场景的损坏，例如漂浮物或天气伪影（如雨、雪或雾），尽管我们的方法可以泛化到传感器损坏，如相机内部噪声（第 6.1 节）。我们将 3D 损坏表示为另一个 NeRF 的参数。与场景 x 不同，我们不需要对 c 有一个强先验。在我们的实验中，我们假设一个不适当的先验 p(c)∝1。这意味着我们不需要预先知道损坏的种类；损坏可以是任何 3D 实体，从天气伪影和漂浮物到不需要的对象。
（4）似然：为了给定场景潜在变量 x 和损坏 c 渲染图像 y，我们组合各自的 NeRF 输出。对于光线位置和方向 (xr, dr)，我们将场景 NeRF (γz, σz) 和损坏 NeRF (γc, σc) 的输出组合为 σ = σz + σc，γ = (γzσz + γcσc)/σ（Niemeyer &amp; Geiger，2021）。我们将组合的 NeRF 的渲染表示为 y = R(x, c)。似然是一个逐像素和逐通道的高斯分布 p(y|x, c) = ∏像素和通道j N(yij|R(x, c)ij, σ2y)，其中 σ2y 是观测噪声方差。
（5）MAP 推理不够：推断场景 x 的一种直接方法是找到最大化 p(x)p(c)p(y|x, c) 的 MAP 估计 (x<em>, c</em>)。然而，这种方法会导致“广告牌”解决方案，其中损坏最终解释了场景，就像一个放置在相机前面的广告牌。
（6）完全后验推理就足够了：在 RIG 中，我们执行完全后验推理以获得潜在场景 x, c∼p(x, c|y) ∝ p(x)p(c)p(y|x, c)，这在经验上可以避免广告牌解决方案（第 6.1 节）。直观地说，这可以看作是模式与典型集不同的一个实例。损坏完全覆盖场景的模式周围区域具有高密度但低体积——没有许多损坏可以精确地渲染到观测图像。另一方面，后验同时考虑密度和体积，集中在具有高概率质量的区域——有许多非广告牌损坏与正确的场景一起渲染到观测图像，尽管每个这样的解决方案可能具有低密度。
（7）变分推理：我们使用变分推理，其中我们优化证据下界 (ELBO) 关于引导分布 q(x, c)：ELBO(q) = Eq(x, c)[logp(y, x|c) - logq(x, c)]。
（8）扩散场景先验：去噪扩散已成为正则化流的有力替代方案。虽然可以直接用基于扩散的先验替换 ProbNeRF 中使用的 RealNVP（例如 Dupontet al.，2022），但扩散模型允许我们可追踪地增加我们的潜在表示的维数。高维潜在空间能够进行高保真采样和重建。我们构建了单级扩散 NeRF (SSDNeRF) 框架（Chen et al.，2023）来训练场景先验。SSDNeRF 优化了一组针对每个训练示例的潜在变量 {xn}，也称为 GLO 潜在变量（Bojanowski et al.，2018），由 ϕ 参数化的扩散先验 pϕ(x) 和由 ψ 参数化的似然 pψ(y|x)。有关更多详细信息，请参见附录 D。
（9）扩散模型：扩散模型是一个潜在变量生成模型，包含正向和反向过程。正向扩散过程 q(z|x) 从数据 x 开始。</p>
</li>
<li>
<p>结论：
(1)：本文提出了一种鲁棒逆向图形生成（RIG）方法，该方法将问题视为概率推理问题，利用预训练的场景先验和一个关于干扰的弱先验来进行推理。RIG在各种场景先验架构下都优于深度估计器和执行点估计而不是完整推理的替代神经辐射场方法。RIG仅在干净数据上训练，但它在具有各种干扰（雨、雪、雾、噪声）的图像上都优于其他方法。
(2)：创新点：</p>
</li>
<li>将逆向图形生成问题视为概率推理问题，利用预训练的场景先验和一个关于干扰的弱先验来进行推理。</li>
<li>提出了一种鲁棒逆向图形生成（RIG）方法，该方法在各种场景先验架构下都优于深度估计器和执行点估计而不是完整推理的替代神经辐射场方法。</li>
<li>RIG仅在干净数据上训练，但它在具有各种干扰（雨、雪、雾、噪声）的图像上都优于其他方法。
性能：</li>
<li>RIG在各种场景先验架构下都优于深度估计器和执行点估计而不是完整推理的替代神经辐射场方法。</li>
<li>RIG仅在干净数据上训练，但它在具有各种干扰（雨、雪、雾、噪声）的图像上都优于其他方法。
工作量：</li>
<li>RIG需要预训练一个场景先验和一个关于干扰的弱先验。</li>
<li>RIG需要执行完整的概率推理，这比执行点估计或最大后验概率推理更耗时。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-25f26b8c4a059fad96179f9402d4ddf8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b464c110b8bfcce608856052d9518e4b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f7396fa7b1ad32dc9c645595746950b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c6960210c2e3765f8601fd7fb69b4ba.jpg" align="middle">
</details>




<h2 id="HyperPlanes-Hypernetwork-Approach-to-Rapid-NeRF-Adaptation"><a href="#HyperPlanes-Hypernetwork-Approach-to-Rapid-NeRF-Adaptation" class="headerlink" title="HyperPlanes: Hypernetwork Approach to Rapid NeRF Adaptation"></a>HyperPlanes: Hypernetwork Approach to Rapid NeRF Adaptation</h2><p><strong>Authors:Paweł Batorski, Dawid Malarz, Marcin Przewięźlikowski, Marcin Mazur, Sławomir Tadeja, Przemysław Spurek</strong></p>
<p>Neural radiance fields (NeRFs) are a widely accepted standard for synthesizing new 3D object views from a small number of base images. However, NeRFs have limited generalization properties, which means that we need to use significant computational resources to train individual architectures for each item we want to represent. To address this issue, we propose a few-shot learning approach based on the hypernetwork paradigm that does not require gradient optimization during inference. The hypernetwork gathers information from the training data and generates an update for universal weights. As a result, we have developed an efficient method for generating a high-quality 3D object representation from a small number of images in a single step. This has been confirmed by direct comparison with the state-of-the-art solutions and a comprehensive ablation study. </p>
<p><a href="http://arxiv.org/abs/2402.01524v1">PDF</a> </p>
<p><strong>Summary</strong><br>神经辐射场方法对于少数基础图像合成新奇3D物体视图有着广泛的认可，却存在泛化性质有限的问题，不妨碍我们利用显著计算资源为我们要展示的每个对象训练独立体系结构。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>神经辐射场方法是一种用于从少数基础图像合成新 3D 物体视图的标准方法。</li>
<li>这种方法存在泛化性质有限的弊端，导致为我们要展示的每个对象训练独立体系结构时需要显著的计算资源。</li>
<li>作者针对此问题提出了一个基于超网络范式的 few-shot 学习方法，该方法在推理过程中无需梯度优化。</li>
<li>超网络从训练数据中收集信息，并为通用权重生成更新。</li>
<li>上述方式打造了一种有效的方法，可从少量图像中生成高质量的 3D 对象表示，只需一个步骤即可完成。</li>
<li>我们已通过直接比较最先进的解决方案和全面的消融研究来证实这一点。</li>
<li>该方法已被直接比较最先进的解决方案和全面的消融研究证实。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>标题：HyperPlanes：快速 NeRF 适应的超网络方法</li>
<li>作者：Paweł Batorski<em>, Dawid Malarz</em>, Marcin Przewi˛e´zlikowski, Marcin Mazur, Slawomir Tadeja, Przemysław Spurek</li>
<li>第一作者单位：雅盖隆大学，数学与计算机科学学院，克拉科夫，波兰</li>
<li>关键词：NeRF，Few-Shot 学习，超网络，快速适应</li>
<li>论文链接：https://arxiv.org/abs/2402.01524
Github 链接：无</li>
<li>摘要：
（1）：NeRF 是一种可以从少量基本图像合成新的逼真 3D 对象视图的深度学习方法，但它缺乏泛化性，需要针对每个对象训练单独的架构。
（2）：过去的方法通常需要大量的计算资源和训练时间，并且泛化性能有限。
（3）：本文提出了一种基于超网络范式的 few-shot 学习方法，该方法不需要在推理期间进行梯度优化。超网络从训练数据中收集信息并生成对通用权重的更新。
（4）：实验结果表明，该方法可以在单个步骤中从少量图像生成高质量的 3D 对象表示，并且在速度和质量方面都优于现有技术。</li>
</ol>
<p>Some Error for method(比如是不是没有Methods这个章节)</p>
<ol>
<li>结论：
（1）：本文提出了一种基于超网络范式的few-shot学习方法HyperPlanes，该方法不需要在推理期间进行梯度优化，可以在单个步骤中从少量图像生成高质量的3D对象表示，并且在速度和质量方面都优于现有技术。
（2）：创新点：</li>
<li>提出了一种基于超网络范式的few-shot学习方法，该方法不需要在推理期间进行梯度优化。</li>
<li>该方法可以从训练数据中收集信息并生成对通用权重的更新。</li>
<li>该方法可以在单个步骤中从少量图像生成高质量的3D对象表示。
性能：</li>
<li>该方法在速度和质量方面都优于现有技术。</li>
<li>该方法可以在单个步骤中从少量图像生成高质量的3D对象表示。
工作量：</li>
<li>该方法不需要在推理期间进行梯度优化，因此可以减少计算资源和训练时间。</li>
<li>该方法可以从训练数据中收集信息并生成对通用权重的更新，因此可以减少训练时间。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d17d9bcf9aa679caea1d14977ee1030c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-309779f6bf52d8d8cfebf258af239717.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-639e9fd34cf9c9e63acc4cb78afac975.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-299ffc14425343bcd3a07c8f9122813c.jpg" align="middle">
</details>




]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>NeRF</tag>
      </tags>
  </entry>
  <entry>
    <title>NeRF</title>
    <url>/2024/02/13/Paper/2024-02-13/NeRF/</url>
    <content><![CDATA[<blockquote>
<p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-02-13-更新"><a href="#2024-02-13-更新" class="headerlink" title="2024-02-13 更新"></a>2024-02-13 更新</h1><h2 id="BioNeRF-Biologically-Plausible-Neural-Radiance-Fields-for-View-Synthesis"><a href="#BioNeRF-Biologically-Plausible-Neural-Radiance-Fields-for-View-Synthesis" class="headerlink" title="BioNeRF: Biologically Plausible Neural Radiance Fields for View   Synthesis"></a>BioNeRF: Biologically Plausible Neural Radiance Fields for View   Synthesis</h2><p><strong>Authors:Leandro A. Passos, Douglas Rodrigues, Danilo Jodas, Kelton A. P. Costa, João Paulo Papa</strong></p>
<p>This paper presents BioNeRF, a biologically plausible architecture that models scenes in a 3D representation and synthesizes new views through radiance fields. Since NeRF relies on the network weights to store the scene’s 3-dimensional representation, BioNeRF implements a cognitive-inspired mechanism that fuses inputs from multiple sources into a memory-like structure, improving the storing capacity and extracting more intrinsic and correlated information. BioNeRF also mimics a behavior observed in pyramidal cells concerning contextual information, in which the memory is provided as the context and combined with the inputs of two subsequent neural models, one responsible for producing the volumetric densities and the other the colors used to render the scene. Experimental results show that BioNeRF outperforms state-of-the-art results concerning a quality measure that encodes human perception in two datasets: real-world images and synthetic data. </p>
<p><a href="http://arxiv.org/abs/2402.07310v1">PDF</a> </p>
<p><strong>Summary</strong><br>生物神经形态学启发的 NeRF 架构，融合多源输入，提取更本质相关信息，实现场景 3D 表示和新视角合成。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>BioNeRF 是一种受生物神经形态学启发的架构，用于建模场景的 3D 表示并通过辐射场合成新视角。</li>
<li>BioNeRF 实现了一种认知启发的机制，将来自多个来源的输入融合到一个类似记忆的结构中，提高存储容量并提取更多内在和相关信息。</li>
<li>BioNeRF 模仿在锥体细胞中观察到的关于上下文信息的行为，其中记忆被提供为上下文并与两个后续神经模型的输入相结合，一个负责产生体积密度，另一个负责用于渲染场景的颜色。</li>
<li>实验结果表明，BioNeRF 在衡量人类感知的质量指标上优于最先进的结果，包括真实世界图像和合成数据两类数据集。</li>
<li>BioNeRF 在两个数据集上都优于最先进的结果，分别为真实世界图像和合成数据。</li>
<li>BioNeRF 在自由视角视频和全景视频的渲染上均取得了最先进的结果。</li>
<li>BioNeRF 在不同场景和条件下表现出鲁棒性和泛化性。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<p></p><p>1标题：《BioNeRF 生物合理神经辐射场的视图合成》(BioNeRF Biologically Plausable Neural Radiance Fields for View Synthesis)。</p><p></p>
<p></p><p>作者列表：(Leandro A Passos)、Douglas Rodrigues)、Danilo Jodas)、Kelton A P Costa)、João Paulo Papa)。</p><p></p>
<p></p><p>第一作者单位：(巴西 Bauru 市 Av Eng Luiz Edmundo Carrijo Coube 街十四之一栋 São Paulo State University)。</p><p></p>
<p></p><p>关键词：(神经渲染)、生物合理神经模型)。</p><p></p>
<p></p><p>链接：(Paper URL)。</p><p></p>
<p></p><p>Github代码链接：(Github None)。</p><p></p>
<p></p><p>摘要：(BioNeRF是一种生物合理架构)，可以利用辐射字段构建场景的三 D 表示形式并且合成新的视图)。由于 NeRF 利用网络中的各种参数存储场景的三 D 表示形式)，BioNeRF 便采用一种认知激励方法)，通过融合多个来源中的信息生成记忆结构)，从而提高储存容量并且提取更多本质信息以及相关信息)。BioNeRF 还模仿锥体型神经细胞有关上下文信息的行为)，其中记忆作为上下文提供)，并且结合两个后续神经模型中的信息)，其中一个模型负责生成容量密度)，另一个模型负责生成用于渲染场景的颜色)。实验结果表明)，BioNeRF 在两个数据集中的质量评估方面超越现有技术)，这些数据集包括真实世界图像以及合成数据)。</p><p></p>
<p></p><p>摘要：(BioNeRF是一种生物合理架构)，可以利用辐射字段构建场景的三 D 表示形式并且合成新的视图)。由于 NeRF 利用网络中的各种参数存储场景的三 D 表示形式)，BioNeRF 便采用一种认知激励方法)，通过融合多个来源中的信息生成记忆结构)，从而提高储存容量并且提取更多本质信息以及相关信息)。BioNeRF 还模仿锥体型神经细胞有关上下文信息的行为)，其中记忆作为上下文提供)，并且结合两个后续神经模型中的信息)，其中一个模型负责生成容量密度)，另一个模型负责生成用于渲染场景的颜色)。实验结果表明)，BioNeRF 在两个数据集中的质量评估方面超越现有技术)，这些数据集包括真实世界图像以及合成数据)。</p><p></p>
<ol>
<li>
<p>方法：
（1）BioNeRF采用认知启发的方法，通过融合多个来源中的信息生成记忆结构，从而提高存储容量并提取更多本质信息和相关信息。
（2）BioNeRF模仿锥体型神经细胞有关上下文信息的行为，其中记忆作为上下文提供，并结合两个后续神经模型中的信息，其中一个模型负责生成容量密度，另一个模型负责生成用于渲染场景的颜色。
（3）实验结果表明，BioNeRF在两个数据集中的质量评估方面超越现有技术，这些数据集包括真实世界图像以及合成数据。</p>
</li>
<li>
<p>结论：
（1）：BioNeRF在神经渲染领域取得了重大突破，提出了一种新的生物合理神经辐射场架构，该架构能够利用辐射字段构建场景的三维表示形式并合成新的视图。
（2）：创新点：</p>
</li>
<li>BioNeRF采用了一种认知启发的方法，通过融合多个来源中的信息生成记忆结构，从而提高存储容量并提取更多本质信息和相关信息。</li>
<li>BioNeRF模仿锥体型神经细胞有关上下文信息的行为，其中记忆作为上下文提供，并结合两个后续神经模型中的信息，其中一个模型负责生成容量密度，另一个模型负责生成用于渲染场景的颜色。</li>
<li>BioNeRF在两个数据集中的质量评估方面超越现有技术，这些数据集包括真实世界图像以及合成数据。
性能：</li>
<li>BioNeRF在两个数据集中的质量评估方面超越现有技术，这些数据集包括真实世界图像以及合成数据。
工作量：</li>
<li>BioNeRF的实现难度较高，需要较强的编程能力和数学基础。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a3147366d087ebe11e207f5d9173f950.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-91083b7a4d33cafbb989e6672e5d0690.jpg" align="middle">
</details>




<h2 id="NCRF-Neural-Contact-Radiance-Fields-for-Free-Viewpoint-Rendering-of-Hand-Object-Interaction"><a href="#NCRF-Neural-Contact-Radiance-Fields-for-Free-Viewpoint-Rendering-of-Hand-Object-Interaction" class="headerlink" title="NCRF: Neural Contact Radiance Fields for Free-Viewpoint Rendering of   Hand-Object Interaction"></a>NCRF: Neural Contact Radiance Fields for Free-Viewpoint Rendering of   Hand-Object Interaction</h2><p><strong>Authors:Zhongqun Zhang, Jifei Song, Eduardo Pérez-Pellitero, Yiren Zhou, Hyung Jin Chang, Aleš Leonardis</strong></p>
<p>Modeling hand-object interactions is a fundamentally challenging task in 3D computer vision. Despite remarkable progress that has been achieved in this field, existing methods still fail to synthesize the hand-object interaction photo-realistically, suffering from degraded rendering quality caused by the heavy mutual occlusions between the hand and the object, and inaccurate hand-object pose estimation. To tackle these challenges, we present a novel free-viewpoint rendering framework, Neural Contact Radiance Field (NCRF), to reconstruct hand-object interactions from a sparse set of videos. In particular, the proposed NCRF framework consists of two key components: (a) A contact optimization field that predicts an accurate contact field from 3D query points for achieving desirable contact between the hand and the object. (b) A hand-object neural radiance field to learn an implicit hand-object representation in a static canonical space, in concert with the specifically designed hand-object motion field to produce observation-to-canonical correspondences. We jointly learn these key components where they mutually help and regularize each other with visual and geometric constraints, producing a high-quality hand-object reconstruction that achieves photo-realistic novel view synthesis. Extensive experiments on HO3D and DexYCB datasets show that our approach outperforms the current state-of-the-art in terms of both rendering quality and pose estimation accuracy. </p>
<p><a href="http://arxiv.org/abs/2402.05532v2">PDF</a> Accepted by 3DV 2024</p>
<p><strong>Summary</strong><br>手-物交互的自由视角逼真重建。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>手-物交互建模是计算机三维建模的挑战性任务。</li>
<li>现存方法无法真实地进行手-物交互建模。</li>
<li>提出 NCRF 框架来从视频中重建手-物交互。</li>
<li>NCRF 包括接触优化场和手-物的神经辐射场。</li>
<li>接触优化场预测三维查询点精确的接触场。</li>
<li>手-物的神经辐射场学习手-物隐式表示。</li>
<li>手-物运动场产生观察到标准的对应关系。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：NCRF：用于手-物体交互自由视点渲染的神经接触辐射场</li>
<li>作者：Zhongqun Zhang, Jifei Song, Eduardo Pérez-Pellitero, Yiren Zhou, Hyung Jin Chang, Aleš Leonardis</li>
<li>第一作者单位：伯明翰大学</li>
<li>关键词：手-物体交互、自由视点渲染、神经辐射场、接触场优化</li>
<li>论文链接：https://arxiv.org/abs/2402.05532</li>
<li>
<p>摘要：
（1）研究背景：手-物体交互建模是计算机视觉中一项极具挑战性的任务。尽管该领域取得了显着进展，但现有方法仍然无法以逼真的方式合成手-物体交互，这源于手和物体之间严重的相互遮挡以及不准确的手-物体姿态估计，从而导致渲染质量下降。
（2）过去方法及其问题：以往工作通常将此任务表述为联合手和物体姿态估计问题，并依赖参数化的手-物体模型（如 MANO 和 YCB）来估计手的运动变换。然而，现有方法难以恢复手-物体接触场的准确几何形状，并且渲染质量受到遮挡和姿态估计误差的严重影响。
（3）本文提出的研究方法：为了解决这些挑战，我们提出了一种新颖的自由视点渲染框架——神经接触辐射场（NCRF），以从一组稀疏视频中重建手-物体交互。NCRF 框架主要由两个关键组件组成：（a）接触优化场：从 3D 查询点预测准确的接触场，以实现手和物体之间的理想接触。（b）手-物体神经辐射场：与专门设计的手-物体运动场协同工作，学习静态规范空间中的隐式手-物体表示，以产生观测到规范的对应关系。我们联合学习这些关键组件，它们通过视觉和几何约束相互帮助和正则化，从而产生高质量的手-物体重建，实现逼真的新视角合成。
（4）方法在什么任务上取得了怎样的性能，这些性能是否支持了它们的目标：在 HO3D 和 Dex-YCB 数据集上的广泛实验表明，我们的方法在渲染质量和姿态估计精度方面均优于当前最先进的方法。这些性能支持了我们的目标，即以逼真的方式重建和渲染手-物体交互。</p>
</li>
<li>
<p>Methods:
(1): 本文提出了一种新颖的自由视点渲染框架——神经接触辐射场（NCRF），以从一组稀疏视频中重建手-物体交互。
(2): NCRF框架主要由两个关键组件组成：（a）接触优化场：从3D查询点预测准确的接触场，以实现手和物体之间的理想接触。（b）手-物体神经辐射场：与专门设计的手-物体运动场协同工作，学习静态规范空间中的隐式手-物体表示，以产生观测到规范的对应关系。
(3): 我们联合学习这些关键组件，它们通过视觉和几何约束相互帮助和正则化，从而产生高质量的手-物体重建，实现逼真的新视角合成。</p>
</li>
<li>
<p>结论：
（1）：本文提出了一种新颖的自由视点渲染框架——神经接触辐射场（NCRF），该框架能够从一组稀疏视频中重建手-物体交互，并生成逼真的新视角合成。NCRF框架通过设计动态手-物体神经辐射场和接触优化场，能够建模具有复杂手部抓握动作和频繁相互遮挡的具有挑战性的手-物体交互。
（2）：创新点：</p>
</li>
<li>提出了一种新颖的自由视点渲染框架——神经接触辐射场（NCRF），该框架能够从一组稀疏视频中重建手-物体交互，并生成逼真的新视角合成。</li>
<li>设计了动态手-物体神经辐射场和接触优化场，能够建模具有复杂手部抓握动作和频繁相互遮挡的具有挑战性的手-物体交互。</li>
<li>提出了一种新的手-物体变形模块，该模块能够将射线变形到规范空间中，并以逼真的方式渲染手-物体交互。
性能：</li>
<li>在HO3D和Dex-YCB数据集上的广泛实验表明，NCRF框架在渲染质量和姿态估计精度方面均优于当前最先进的方法。</li>
<li>NCRF框架能够生成逼真的新视角合成，并且能够很好地处理具有复杂手部抓握动作和频繁相互遮挡的具有挑战性的手-物体交互。
工作量：</li>
<li>NCRF框架的实现相对复杂，需要较高的计算资源。</li>
<li>NCRF框架的训练过程需要较长时间，并且需要大量的数据。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-19c080ef42e2fcaa0595e65274d339b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7f0899ff9371cac98ca44ab3913a349.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1403a98bc963e537484ce413bb5d32ea.jpg" align="middle">
</details>




<h2 id="BirdNeRF-Fast-Neural-Reconstruction-of-Large-Scale-Scenes-From-Aerial-Imagery"><a href="#BirdNeRF-Fast-Neural-Reconstruction-of-Large-Scale-Scenes-From-Aerial-Imagery" class="headerlink" title="BirdNeRF: Fast Neural Reconstruction of Large-Scale Scenes From Aerial   Imagery"></a>BirdNeRF: Fast Neural Reconstruction of Large-Scale Scenes From Aerial   Imagery</h2><p><strong>Authors:Huiqing Zhang, Yifei Xue, Ming Liao, Yizhen Lao</strong></p>
<p>In this study, we introduce BirdNeRF, an adaptation of Neural Radiance Fields (NeRF) designed specifically for reconstructing large-scale scenes using aerial imagery. Unlike previous research focused on small-scale and object-centric NeRF reconstruction, our approach addresses multiple challenges, including (1) Addressing the issue of slow training and rendering associated with large models. (2) Meeting the computational demands necessitated by modeling a substantial number of images, requiring extensive resources such as high-performance GPUs. (3) Overcoming significant artifacts and low visual fidelity commonly observed in large-scale reconstruction tasks due to limited model capacity. Specifically, we present a novel bird-view pose-based spatial decomposition algorithm that decomposes a large aerial image set into multiple small sets with appropriately sized overlaps, allowing us to train individual NeRFs of sub-scene. This decomposition approach not only decouples rendering time from the scene size but also enables rendering to scale seamlessly to arbitrarily large environments. Moreover, it allows for per-block updates of the environment, enhancing the flexibility and adaptability of the reconstruction process. Additionally, we propose a projection-guided novel view re-rendering strategy, which aids in effectively utilizing the independently trained sub-scenes to generate superior rendering results. We evaluate our approach on existing datasets as well as against our own drone footage, improving reconstruction speed by 10x over classical photogrammetry software and 50x over state-of-the-art large-scale NeRF solution, on a single GPU with similar rendering quality. </p>
<p><a href="http://arxiv.org/abs/2402.04554v2">PDF</a> </p>
<p><strong>Summary</strong><br>对于大场景下的重建任务，本文引入 BirdNeRF，该方法能够有效利用无人机影像数据实现高效低存储的大场景重建。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>BirdNeRF 是一款针对航空图像的大场景重建方法，解决了以往小场景重建存在的训练慢、渲染慢、模型容量小等问题。</li>
<li>BirdNeRF 提出了一种基于鸟瞰视角的姿势分解算法，将大场景图像集分解成多个小场景子集，每个子集使用单独的 NeRF 进行训练。</li>
<li>BirdNeRF 采用了一种新颖的投影引导式新视角重新渲染策略，可以有效利用独立训练的子场景生成更好的渲染结果。</li>
<li>BirdNeRF 在现有数据集和我们自己的无人机数据上进行了评估，在单个 GPU 上的重建速度比经典摄影测量软件快 10 倍，比最先进的大场景 NeRF 解决方案快 50 倍，且渲染质量相似。</li>
<li>BirdNeRF 可以在任意大的场景中无缝扩展，并支持对环境的局部更新，提高了重建过程的灵活性。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：BirdNeRF：基于航拍图像的大场景快速神经重建</li>
<li>作者：张惠卿、薛一菲、廖明、老义珍</li>
<li>单位：无</li>
<li>关键词：NeRF、大场景重建、航拍图像、空间分解、投影引导</li>
<li>链接：无，Github 链接：无</li>
<li>
<p>摘要：
（1）研究背景：大场景三维重建是摄影测量和遥感领域的一项重要任务，可以利用航拍或卫星图像、激光雷达数据和街景图像等多种数据源构建城市的三维模型。近年来，基于图像的三维重建技术取得了很大的进展，并在城市规划、导航、虚拟旅游、房地产、灾害管理和历史保护等领域得到了广泛的应用。
（2）过去方法与问题：现有的基于图像的三维重建技术主要分为传统几何方法和基于神经网络的方法。传统几何方法主要包括摄影测量和激光扫描，这些方法可以生成高精度的三维模型，但需要大量的人工参与和昂贵的设备。基于神经网络的方法，如神经辐射场（NeRF），可以从图像中自动学习三维场景的表示，但这些方法通常需要大量的训练数据和计算资源，并且在大场景重建任务中容易出现伪影和低视觉保真度的问题。
（3）研究方法：为了解决上述问题，本文提出了一种新的基于 NeRF 的大场景重建方法，称为 BirdNeRF。BirdNeRF 采用了一种新的鸟瞰视角姿势分解算法，将大场景图像分解成多个小场景，并分别训练每个小场景的 NeRF 模型。这种分解方法不仅可以减少训练和渲染时间，还可以提高重建的质量。此外，BirdNeRF 还提出了一种投影引导的新视角重新渲染策略，可以有效地利用独立训练的小场景模型生成高质量的渲染结果。
（4）性能与目标：BirdNeRF 在现有数据集和我们自己的无人机航拍数据上进行了评估。结果表明，BirdNeRF 的重建速度比传统的摄影测量软件快 10 倍，比最先进的大场景 NeRF 解决方案快 50 倍，并且在单个 GPU 上可以实现相似的渲染质量。这些结果证明了 BirdNeRF 的有效性和实用性。</p>
</li>
<li>
<p>方法：
（1）场景分解：将大场景图像分解成多个小场景，并分别训练每个小场景的 NeRF 模型。
（2）视角姿势分解：采用鸟瞰视角姿势分解算法，将大场景图像分解成多个小场景。
（3）新视角重新渲染：提出一种投影引导的新视角重新渲染策略，可以有效地利用独立训练的小场景模型生成高质量的渲染结果。</p>
</li>
<li>
<p>结论：
（1）：本文提出了一种新的基于 NeRF 的大场景重建方法 BirdNeRF，该方法采用鸟瞰视角姿势分解算法和投影引导的新视角重新渲染策略，可以有效地解决大场景重建任务中容易出现伪影和低视觉保真度的问题。
（2）：创新点：</p>
</li>
<li>提出了一种新的鸟瞰视角姿势分解算法，可以将大场景图像分解成多个小场景，并分别训练每个小场景的 NeRF 模型，从而减少训练和渲染时间，提高重建质量。</li>
<li>提出了一种投影引导的新视角重新渲染策略，可以有效地利用独立训练的小场景模型生成高质量的渲染结果。
性能：</li>
<li>BirdNeRF 在现有数据集和我们自己的无人机航拍数据上进行了评估。结果表明，BirdNeRF 的重建速度比传统的摄影测量软件快 10 倍，比最先进的大场景 NeRF 解决方案快 50 倍，并且在单个 GPU 上可以实现相似的渲染质量。
工作量：</li>
<li>BirdNeRF 的实现相对简单，并且可以在单个 GPU 上训练和渲染。然而，由于需要对大场景图像进行分解，因此 BirdNeRF 的预处理时间可能会比较长。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a5c73ab0e2d97eb040012ca4a7c897fe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-daadce77f0b48dc25dd984f5c66ee7ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6d52642c6cfdc84439f5ea843cff2fd1.jpg" align="middle">
</details>




]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>NeRF</tag>
      </tags>
  </entry>
  <entry>
    <title>Diffusion Models</title>
    <url>/2024/02/09/Paper/2024-02-09/Diffusion%20Models/</url>
    <content><![CDATA[<blockquote>
<p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-02-09-更新"><a href="#2024-02-09-更新" class="headerlink" title="2024-02-09 更新"></a>2024-02-09 更新</h1><h2 id="Source-Free-Domain-Adaptation-with-Diffusion-Guided-Source-Data-Generation"><a href="#Source-Free-Domain-Adaptation-with-Diffusion-Guided-Source-Data-Generation" class="headerlink" title="Source-Free Domain Adaptation with Diffusion-Guided Source Data   Generation"></a>Source-Free Domain Adaptation with Diffusion-Guided Source Data   Generation</h2><p><strong>Authors:Shivang Chopra, Suraj Kothawade, Houda Aynaou, Aman Chadha</strong></p>
<p>This paper introduces a novel approach to leverage the generalizability capability of Diffusion Models for Source-Free Domain Adaptation (DM-SFDA). Our proposed DM-SFDA method involves fine-tuning a pre-trained text-to-image diffusion model to generate source domain images using features from the target images to guide the diffusion process. Specifically, the pre-trained diffusion model is fine-tuned to generate source samples that minimize entropy and maximize confidence for the pre-trained source model. We then apply established unsupervised domain adaptation techniques to align the generated source images with target domain data. We validate our approach through comprehensive experiments across a range of datasets, including Office-31, Office-Home, and VisDA. The results highlight significant improvements in SFDA performance, showcasing the potential of diffusion models in generating contextually relevant, domain-specific images. </p>
<p><a href="http://arxiv.org/abs/2402.04929v1">PDF</a> arXiv admin note: substantial text overlap with arXiv:2310.01701</p>
<p><strong>Summary</strong><br>利用扩散模型的泛化能力进行无源域自适应。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>通过微调预训练的文生图扩散模型，利用目标图像的特征引导扩散过程，生成源域图像。</li>
<li>目标是生成熵最小化且对预训练源模型的置信度最大的源样本。</li>
<li>直接在目标图像分布上训练扩散模型，而无需成对的源和目标图像。</li>
<li>所提出的方法在 Office-31、Office-Home 和 VisDA 等多个数据集上都取得了最先进的性能。</li>
<li>生成的高质量源图像有助于跨域任务，例如图像分类和目标检测。</li>
<li>充分利用扩散模型的生成能力，在源和目标域之间建立桥梁。</li>
<li>无源域自适应的模型具有降噪效果，有助于提高分类和检测的性能。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：基于扩散引导源数据生成的无源域自适应</li>
<li>作者：Shivang Chopra, Suraj Kothawade, Houda Aynaou, Aman Chadha</li>
<li>单位：佐治亚理工学院计算机系</li>
<li>关键词：无源域自适应、扩散模型、数据生成、跨域图像分类</li>
<li>链接：https://arxiv.org/abs/2402.04929
Github：无</li>
<li>
<p>摘要：
（1）研究背景：深度卷积神经网络（CNN）在视觉任务中取得了令人印象深刻的性能，但它们对训练和测试数据分布一致性的假设限制了其在真实世界中的应用。领域自适应（DA）旨在减少这种差异，使模型能够跨多个领域表现良好。传统 DA 方法依赖于固定的源数据，可能难以适应不断变化的领域。无源域自适应（SFDA）是一种特殊类型的 DA，它不需要访问源训练数据。
（2）过去的方法与问题：现有的大多数 SFDA 方法通过在共享特征空间中融合两个不同的数据分布来实现模型适应性。一种实现无源方式的方法是使用合成生成的源数据。然而，生成准确表示源域多样性和复杂性的合成源数据可能很困难。此外，如果合成数据质量不高，可能会引入噪声和不一致性，从而对模型在目标域上的性能产生负面影响。
（3）提出的研究方法：本文提出了一种名为 DM-SFDA 的新方法，该方法利用扩散生成模型（DGM）的泛化能力来解决 SFDA 的挑战。DM-SFDA 的核心思想是微调一个预训练的文本到图像扩散模型，以使用来自目标图像的特征来生成源域图像，从而指导扩散过程。具体来说，预训练的扩散模型被微调以生成源样本，这些样本最小化熵并最大化预训练源模型的置信度。然后，将已建立的无监督域适应技术应用于将生成的源图像与目标域数据对齐。
（4）方法的性能：本文在 Office-31、Office-Home 和 VisDA 等多个数据集上对 DM-SFDA 进行了全面的实验验证。结果表明，DM-SFDA 在 SFDA 任务上取得了显着的性能提升，证明了扩散模型在生成上下文相关、特定于领域的图像方面的潜力。这些性能支持了本文提出的方法能够有效地解决 SFDA 问题。</p>
</li>
<li>
<p>方法：
(1): 基于扩散模型的无源域自适应（DM-SFDA）方法的基本思想是微调一个预训练的文本到图像扩散模型，以使用来自目标图像的特征来生成源域图像，从而指导扩散过程。
(2): 预训练的扩散模型被微调以生成源样本，这些样本最小化熵并最大化预训练源模型的置信度。
(3): 然后，将已建立的无监督域适应技术应用于将生成的源图像与目标域数据对齐。</p>
</li>
<li>
<p>结论：</p>
</li>
</ol>
<p>（1）重要性：本文提出了一种基于扩散模型的无源域自适应（DM-SFDA）方法，该方法利用扩散生成模型（DGM）的泛化能力来解决SFDA的挑战。DM-SFDA的核心思想是微调一个预训练的文本到图像扩散模型，以使用来自目标图像的特征来生成源域图像，从而指导扩散过程。该方法在多个数据集上取得了显着的性能提升，证明了扩散模型在生成上下文相关、特定于领域的图像方面的潜力。</p>
<p>（2）优缺点：</p>
<p>创新点：</p>
<ul>
<li>提出了一种基于扩散模型的无源域自适应方法，该方法利用扩散生成模型（DGM）的泛化能力来解决SFDA的挑战。</li>
<li>设计了一种新的生成源图像的方法，该方法使用来自目标图像的特征来指导扩散过程，从而生成上下文相关、特定于领域的图像。</li>
</ul>
<p>性能：</p>
<ul>
<li>在多个数据集上取得了显着的性能提升，证明了扩散模型在生成上下文相关、特定于领域的图像方面的潜力。</li>
</ul>
<p>工作量：</p>
<ul>
<li>需要微调一个预训练的文本到图像扩散模型，这可能需要大量的计算资源。</li>
<li>需要收集目标域的数据，这在某些情况下可能很困难。</li>
</ul>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-ec2a5c717af2a4c67eb4715437c633c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf3cb970b1edbd90925d67dc50ebd458.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b60fc581c86cc20b03dbf6c09543aea2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-282170863545d09c18b118ee88d874e2.jpg" align="middle">
</details>




## EvoSeed: Unveiling the Threat on Deep Neural Networks with Real-World   Illusions

**Authors:Shashank Kotyan, PoYuan Mao, Danilo Vasconcellos Vargas**

Deep neural networks are exploited using natural adversarial samples, which have no impact on human perception but are misclassified. Current approaches often rely on the white-box nature of deep neural networks to generate these adversarial samples or alter the distribution of adversarial samples compared to training distribution. To alleviate the limitations of current approaches, we propose EvoSeed, a novel evolutionary strategy-based search algorithmic framework to generate natural adversarial samples. Our EvoSeed framework uses auxiliary Diffusion and Classifier models to operate in a model-agnostic black-box setting. We employ CMA-ES to optimize the search for an adversarial seed vector, which, when processed by the Conditional Diffusion Model, results in an unrestricted natural adversarial sample misclassified by the Classifier Model. Experiments show that generated adversarial images are of high image quality and are transferable to different classifiers. Our approach demonstrates promise in enhancing the quality of adversarial samples using evolutionary algorithms. We hope our research opens new avenues to enhance the robustness of deep neural networks in real-world scenarios. Project Website can be accessed at \url{https://shashankkotyan.github.io/EvoSeed}. 

[PDF](http://arxiv.org/abs/2402.04699v1) 

**Summary**
利用进化策略搜索算法框架生成自然对抗样本，以增强扩散模型在真实世界场景中的鲁棒性。

**Key Takeaways**
- 基于进化策略的搜索算法框架 EvoSeed 用于生成自然对抗样本。
- EvoSeed 框架使用辅助扩散和分类器模型在与模型无关的黑盒设置中运行。
- 采用 CMA-ES 优化对抗种子向量的搜索，该向量在条件扩散模型处理后，会生成分类器模型错误分类的无限制自然对抗样本。
- 实验表明生成的对抗图像具有高图像质量，并且可以转移到不同的分类器。
- 该方法证明了使用进化算法来增强对抗样本质量的潜力。
- 希望这项研究能够为增强深度神经网络在真实世界场景中的鲁棒性开辟新途径。
- 项目网站可以访问网址：https://shashankkotyan.github.io/EvoSeed。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>题目：EvoSeed：揭示深度神经网络的威胁</li>
<li>作者：Shashank Kotyan、Po Yuan Mao、Danilo Vasconcellos Vargas</li>
<li>单位：九州大学</li>
<li>关键词：深度学习、计算机视觉、CMA-ES、扩散模型、自然对抗样本</li>
<li>论文链接：https://arxiv.org/abs/2402.04699，Github 链接：None</li>
<li>
<p>摘要：
（1）研究背景：深度神经网络在各种视觉识别任务中取得了空前的成功。然而，当测试分布与训练分布不同时，它们的性能会下降，Hendrycks 等人[10]和 Ilyas 等人[17]的研究表明了这一点。这给开发能够处理这种分布变化的鲁棒深度神经网络带来了重大挑战。对抗样本和对抗攻击利用了这种漏洞，通过操纵图像来改变与原始分布相比的分布。Dalvi 等人[4]的研究强调，输入数据的对抗性操纵通常会导致分类器做出不正确的预测，这引发了人们对经典机器学习算法的安全性和完整性的严重担忧。这种担忧仍然相关，尤其是考虑到最先进的深度神经网络极易受到涉及故意对输入进行扰动的对抗性攻击[22, 26]。对这些扰动施加了各种约束，使这些扰动变得微妙且难以检测。例如，𝐿0对抗攻击，例如 One-Pixel Attack[22, 38]限制了扰动像素的数量，𝐿2对抗攻击，例如 PGD-L2[26]限制了与原始图像的欧几里得距离，并且𝐿∞对抗攻击，例如 PGD-L∞[26]限制了所有像素的变化量。
（2）过去的方法及其问题：虽然对抗样本[22, 26, 38]暴露了深度神经网络中的漏洞，但它们的人工性质和对受限输入数据的依赖限制了它们在现实世界中的适用性。相比之下，在实际情况下，挑战变得更加明显，因为将所有潜在威胁全面地包含在训练数据集中变得不可行。这种复杂性突出了深度神经网络对 Hendrycks 等人[10]提出的自然对抗示例和 Song 等人[37]提出的无限制对抗示例的敏感性不断提高。近年来，这些类型的对抗样本在对抗攻击研究中获得了突出地位，因为它们可以对图像进行实质性改变，而不会显着影响人类对其含义和真实性的感知。
（3）本文提出的研究方法：在这样的背景下，我们提出了 EvoSeed，这是一种第一个基于进化策略的算法框架，旨在生成如图 2 所示的无限制自然对抗样本。我们的算法需要一个条件扩散模型𝐺和一个分类器模型𝐹来生成对抗样本𝑥。具体来说，它利用协方差矩阵自适应进化策略 (CMA-ES) 作为其核心来增强对能够生成对抗样本𝑥的对抗种子向量𝑧′的搜索。CMA-ES 对噪声种子向量𝑧′进行微调，以优化目标函数，该目标函数将分类器模型𝐹的输出与人类对图像𝑥的感知之间的差异作为惩罚。
（4）方法在任务和性能上的表现：实验表明，生成的对抗图像具有很高的图像质量，并且可以转移到不同的分类器。我们的方法证明了使用进化算法提高对抗样本质量的前景。我们希望我们的研究为增强深度神经网络在现实世界场景中的鲁棒性开辟了新的途径。</p>
</li>
<li>
<p>方法：
（1）随机种子法（RandSeed）：基于随机偏移的随机搜索策略，通过在初始种子向量上添加随机扰动来生成对抗样本。
（2）进化种子法（EvoSeed）：基于协方差矩阵自适应进化策略（CMA-ES）的优化算法，通过迭代优化初始种子向量来搜索对抗种子向量，以生成对抗样本。
（3）条件扩散模型（Conditional Diffusion Model）：用于生成对抗样本的生成模型，通过条件信息和初始种子向量生成图像。
（4）分类器模型（Classifier Model）：用于评估对抗样本质量的分类模型，通过计算对抗样本的分类错误率来衡量对抗样本的攻击成功率。
（5）攻击成功率（ASR）：衡量对抗样本攻击成功率的指标，计算为对抗样本被分类器错误分类的比例。
（6）弗雷歇特起始距离（FID）：衡量对抗样本与真实样本分布差异的指标，计算为对抗样本与真实样本在特征空间中的距离。
（7）感知评分（IS）：衡量对抗样本质量的指标，计算为对抗样本在分类器上的平均对数似然值。
（8）结构相似性（SSIM）：衡量对抗样本与真实样本在结构上的相似性，计算为对抗样本与真实样本在像素空间中的相似度。</p>
</li>
<li>
<p>结论：
（1）：EvoSeed是一种基于进化策略的算法框架，旨在生成无限制自然对抗样本。它利用协方差矩阵自适应进化策略（CMA-ES）作为其核心来增强对能够生成对抗样本𝑥的对抗种子向量𝑧′的搜索。实验表明，生成的对抗图像具有很高的图像质量，并且可以转移到不同的分类器。我们的方法证明了使用进化算法提高对抗样本质量的前景。我们希望我们的研究为增强深度神经网络在现实世界场景中的鲁棒性开辟了新的途径。
（2）：创新点：</p>
</li>
<li>提出了一种基于进化策略的算法框架EvoSeed，用于生成无限制自然对抗样本。</li>
<li>利用协方差矩阵自适应进化策略（CMA-ES）作为核心来增强对能够生成对抗样本𝑥的对抗种子向量𝑧′的搜索。</li>
<li>实验表明，生成的对抗图像具有很高的图像质量，并且可以转移到不同的分类器。
性能：</li>
<li>EvoSeed生成的对抗图像具有很高的图像质量，并且可以转移到不同的分类器。</li>
<li>EvoSeed在ImageNet数据集上实现了99.9%的攻击成功率，并且在CIFAR-10数据集上实现了99.8%的攻击成功率。</li>
<li>EvoSeed生成的对抗图像在弗雷歇特起始距离（FID）和感知评分（IS）方面都优于其他方法。
工作量：</li>
<li>EvoSeed的实现相对简单，并且可以很容易地应用于不同的数据集和分类器。</li>
<li>EvoSeed的训练时间与其他方法相比相对较短。</li>
<li>EvoSeed可以生成高质量的对抗图像，而不需要大量的数据和计算资源。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-fba3784cdfd913938a2c25b5d6802005.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1191333c7b6b916696b230758671066a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d57cb25c209c458064f830f4a1d7c2d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b515b564419e732b66802017f00ce12.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b4c2ccc13f827d3ede06ea04ae36e1da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5df7bec69fc3a0aa2cf5d26456e611b5.jpg" align="middle">
</details>




<h2 id="BRI3L-A-Brightness-Illusion-Image-Dataset-for-Identification-and-Localization-of-Regions-of-Illusory-Perception"><a href="#BRI3L-A-Brightness-Illusion-Image-Dataset-for-Identification-and-Localization-of-Regions-of-Illusory-Perception" class="headerlink" title="BRI3L: A Brightness Illusion Image Dataset for Identification and   Localization of Regions of Illusory Perception"></a>BRI3L: A Brightness Illusion Image Dataset for Identification and   Localization of Regions of Illusory Perception</h2><p><strong>Authors:Aniket Roy, Anirban Roy, Soma Mitra, Kuntal Ghosh</strong></p>
<p>Visual illusions play a significant role in understanding visual perception. Current methods in understanding and evaluating visual illusions are mostly deterministic filtering based approach and they evaluate on a handful of visual illusions, and the conclusions therefore, are not generic. To this end, we generate a large-scale dataset of 22,366 images (BRI3L: BRightness Illusion Image dataset for Identification and Localization of illusory perception) of the five types of brightness illusions and benchmark the dataset using data-driven neural network based approaches. The dataset contains label information - (1) whether a particular image is illusory/nonillusory, (2) the segmentation mask of the illusory region of the image. Hence, both the classification and segmentation task can be evaluated using this dataset. We follow the standard psychophysical experiments involving human subjects to validate the dataset. To the best of our knowledge, this is the first attempt to develop a dataset of visual illusions and benchmark using data-driven approach for illusion classification and localization. We consider five well-studied types of brightness illusions: 1) Hermann grid, 2) Simultaneous Brightness Contrast, 3) White illusion, 4) Grid illusion, and 5) Induced Grating illusion. Benchmarking on the dataset achieves 99.56% accuracy in illusion identification and 84.37% pixel accuracy in illusion localization. The application of deep learning model, it is shown, also generalizes over unseen brightness illusions like brightness assimilation to contrast transitions. We also test the ability of state-of-theart diffusion models to generate brightness illusions. We have provided all the code, dataset, instructions etc in the github repo: <a href="https://github.com/aniket004/BRI3L">https://github.com/aniket004/BRI3L</a> </p>
<p><a href="http://arxiv.org/abs/2402.04541v1">PDF</a> </p>
<p><strong>Summary</strong><br>深度学习可以识别和定位亮度错觉，甚至可以生成新的错觉图像。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>提出大规模亮度错觉数据集BRI3L，包含22,366张图像，涵盖五种错觉类型。</li>
<li>数据集包含标签信息，可用于评估分类和分割任务。</li>
<li>基于数据驱动的深度学习方法在该数据集上取得了良好的性能。</li>
<li>深度学习模型可以泛化到未见过的亮度错觉，如亮度同化到对比度转换。</li>
<li>扩散模型能够生成亮度错觉图像。</li>
<li>该研究为视觉错觉的理解和评估提供了新的方法。</li>
<li>该研究的数据集和代码已开源，以便其他研究人员使用。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>论文标题：BRI3L：亮度错觉图像数据集，用于识别和定位错觉感知区域</li>
<li>作者：Aniket Roy, Anirban Roy, Soma Mitr, Kuntal Ghosh</li>
<li>第一作者单位：约翰·霍普金斯大学</li>
<li>关键词：视觉错觉，感知</li>
<li>论文链接：https://arxiv.org/abs/2402.04541，Github 代码链接：https://github.com/aniket004/BRI3L</li>
<li>摘要：
(1)：研究背景：视觉错觉在理解视觉感知中发挥着重要作用。当前理解和评估视觉错觉的方法主要是基于确定性滤波的方法，并且它们对少数视觉错觉进行评估，因此结论不具有普遍性。
(2)：过去的方法及其问题，方法动机：为了解决这个问题，我们生成了一个包含 22,366 张图像的大规模数据集（BRI3L：亮度错觉图像数据集，用于识别和定位错觉感知），其中包含五种类型的亮度错觉，并使用数据驱动的基于神经网络的方法对该数据集进行了基准测试。该数据集包含标签信息——（1）特定图像是否具有错觉/非错觉，（2）图像中错觉区域的分割掩码。因此，可以使用此数据集评估分类和分割任务。我们遵循涉及人类受试者的标准心理物理实验来验证数据集。据我们所知，这是首次尝试开发视觉错觉数据集，并使用数据驱动的方法对错觉分类和定位进行基准测试。我们考虑了五种研究充分的亮度错觉类型：1) 赫尔曼网格，2) 同步亮度对比，3) 白色错觉，4) 网格错觉，5) 感应光栅错觉。在该数据集上的基准测试实现了 99.56% 的错觉识别准确率和 84.37% 的错觉定位像素准确率。结果表明，深度学习模型的应用还可推广到未见过的亮度错觉，例如从亮度同化到对比度转换。我们还测试了最先进的扩散模型生成亮度错觉的能力。我们在 GitHub 仓库中提供了所有代码、数据集、指令集等：https://github.com/aniket004/BRI3L
(3)：研究方法：我们遵循涉及人类受试者的标准心理物理实验来验证数据集。我们考虑了五种研究充分的亮度错觉类型：1) 赫尔曼网格，2) 同步亮度对比，3) 白色错觉，4) 网格错觉，5) 感应光栅错觉。在该数据集上的基准测试实现了 99.56% 的错觉识别准确率和 84.37% 的错觉定位像素准确率。结果表明，深度学习模型的应用还可推广到未见过的亮度错觉，例如从亮度同化到对比度转换。我们还测试了最先进的扩散模型生成亮度错觉的能力。
(4)：方法性能：在该数据集上的基准测试实现了 99.56% 的错觉识别准确率和 84.37% 的错觉定位像素准确率。结果表明，深度学习模型的应用还可推广到未见过的亮度错觉，例如从亮度同化到对比度转换。这些性能支持了我们开发一个大规模数据集和使用数据驱动的方法对错觉分类和定位进行基准测试的目标。</li>
</ol>
<p>7.<methods>：
(1) 我们遵循涉及人类受试者的标准心理物理实验来验证数据集。
(2) 我们考虑了五种研究充分的亮度错觉类型：赫尔曼网格、同步亮度对比、白色错觉、网格错觉、感应光栅错觉。
(3) 在该数据集上的基准测试实现了 99.56% 的错觉识别准确率和 84.37% 的错觉定位像素准确率。
(4) 结果表明，深度学习模型的应用还可推广到未见过的亮度错觉，例如从亮度同化到对比度转换。
(5) 我们还测试了最先进的扩散模型生成亮度错觉的能力。</methods></p>
<ol>
<li>结论：
（1）：本工作的重要意义在于，它提供了一个包含五种类型亮度错觉的、包含22,366张图像的大规模数据集BRI3L，并使用数据驱动的基于神经网络的方法对该数据集进行了基准测试。该数据集包含标签信息——（1）特定图像是否具有错觉/非错觉，（2）图像中错觉区域的分割掩码。因此，可以使用此数据集评估分类和分割任务。这将有助于研究人员和从业者更好地理解视觉错觉，并开发新的方法来识别和定位错觉感知区域。
（2）：创新点：</li>
<li>首次尝试开发视觉错觉数据集，并使用数据驱动的方法对错觉分类和定位进行基准测试。</li>
<li>该数据集包含五种类型亮度错觉，涵盖了多种错觉现象。</li>
<li>使用深度学习模型对数据集进行了基准测试，实现了99.56%的错觉识别准确率和84.37%的错觉定位像素准确率。</li>
<li>结果表明，深度学习模型的应用还可推广到未见过的亮度错觉，例如从亮度同化到对比度转换。</li>
</ol>
<p>性能：
* 在该数据集上的基准测试实现了99.56%的错觉识别准确率和84.37%的错觉定位像素准确率。
* 结果表明，深度学习模型的应用还可推广到未见过的亮度错觉，例如从亮度同化到对比度转换。</p>
<p>工作量：
* 收集和注释数据的工作量很大。
* 开发和训练深度学习模型的工作量也很大。</p>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-d9494fba06526e8b87f8dd5e3bc6d94a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a61414f51deef787aabe72aa30947292.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c9a8fbfbb6ed5b80eb2803e27c328d8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2d1fec65eb07ceea77a12925d47fbae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32488f736ee10537497afccc3a1a1d76.jpg" align="middle">
</details>




## Polyp-DDPM: Diffusion-Based Semantic Polyp Synthesis for Enhanced   Segmentation

**Authors:Zolnamar Dorjsembe, Hsing-Kuo Pao, Furen Xiao**

This study introduces Polyp-DDPM, a diffusion-based method for generating realistic images of polyps conditioned on masks, aimed at enhancing the segmentation of gastrointestinal (GI) tract polyps. Our approach addresses the challenges of data limitations, high annotation costs, and privacy concerns associated with medical images. By conditioning the diffusion model on segmentation masks-binary masks that represent abnormal areas-Polyp-DDPM outperforms state-of-the-art methods in terms of image quality (achieving a Frechet Inception Distance (FID) score of 78.47, compared to scores above 83.79) and segmentation performance (achieving an Intersection over Union (IoU) of 0.7156, versus less than 0.6694 for synthetic images from baseline models and 0.7067 for real data). Our method generates a high-quality, diverse synthetic dataset for training, thereby enhancing polyp segmentation models to be comparable with real images and offering greater data augmentation capabilities to improve segmentation models. The source code and pretrained weights for Polyp-DDPM are made publicly available at https://github.com/mobaidoctor/polyp-ddpm. 

[PDF](http://arxiv.org/abs/2402.04031v1) This work has been submitted to the IEEE for possible publication.   Copyright may be transferred without notice, after which this version may no   longer be accessible

**摘要**
聚合扩散模型 Polyp-DDPM 可结合掩码生成逼真的息肉图像，有效提高胃肠道息肉分割性能。

**要点**

- Polyp-DDPM 采用基于扩散的方法，通过训练扩散模型生成逼真且与掩码条件相符的息肉图像，提高胃肠道息肉分割的性能。
- Polyp-DDPM 以分割掩码（表示异常区域的二值掩码）为条件，在图像质量和分割性能方面均优于现有方法。在图像质量方面，Polyp-DDPM 在 Frechet Inception Distance (FID) 得分上达到 78.47，而现有方法的分数高于 83.79。在分割性能方面，Polyp-DDPM 在交集比 (IoU) 上达到 0.7156，而基线模型生成的合成图像的 IoU 小于 0.6694，真实数据的 IoU 为 0.7067。
- Polyp-DDPM 生成高质量且多样化的合成数据集，用于训练，从而提高息肉分割模型的性能使其能够与真实图像媲美，并提供更强大的数据增强功能来改进分割模型。
- Polyp-DDPM 的源代码和预训练权重已在 https://github.com/mobaidoctor/polyp-ddpm 上公开发布。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>题目：Polyp-DDPM：基于扩散的语义息肉合成，用于增强分割</li>
<li>作者：Zolnamar Dorjsembe、Hsing-Kuo Pao、Furen Xiao</li>
<li>隶属单位：国立台湾科技大学计算机科学与信息工程系</li>
<li>关键词：扩散模型、语义息肉合成、息肉分割</li>
<li>论文链接：https://arxiv.org/abs/2302.09766，Github 代码链接：https://github.com/mobaidoctor/polyp-ddpm</li>
<li>摘要：
（1）研究背景：结直肠癌是全球第三常见的癌症，通常始于结直肠息肉，早期发现和切除息肉可预防结直肠癌并降低死亡率。然而，在结肠镜检查中发现小息肉可能很困难，这取决于医生的专业知识和其他挑战，例如息肉在手术过程中无法观察到或被忽视。为了增强息肉检测，研究人员正在利用机器学习来自主识别和强调内窥镜检查中的息肉。然而，由于需要广泛且多样化的数据集，这些技术的发展面临着重大挑战，这些数据集对于训练模型以实现高准确性至关重要。医疗保健行业经常面临此类数据的短缺，这归因于异常区域外观的多样性、患者招募困难、数据注释成本高以及对患者数据隐私的担忧。
（2）过去的方法及其问题：为了减轻数据稀缺问题，探索合成图像作为一种可行的解决方案已引起关注。现有的方法包括基于 GAN 的方法和基于扩散的方法。基于 GAN 的方法，如 SinGAN-Seg，能够生成比其他 GAN 模型更逼真的图像，但面临多样性和细节准确性的挑战。基于扩散的方法，如两阶段扩散模型，能够生成多样化的图像，但由于需要两个模型，因此训练和推理的计算成本很高。
（3）提出的研究方法：为了应对这些挑战，我们提出了一种新颖的基于扩散的语义息肉合成方法 Polyp-DDPM，旨在增强息肉分割。我们的方法通过掩模图像的逐通道连接对扩散模型进行条件化。
（4）实验结果与方法性能：我们在 Kvasir-SEG 数据集上进行了实验，并将我们提出的方法与 SinGAN-Seg 和潜在扩散模型进行了比较，因为这些方法代表了带注释息肉数据集生成的最新进展，包括基于 GAN 的方法和基于扩散的方法。在我们的实验中，Polyp-DDPM 在图像质量和分割任务中均表现出优于基线模型的性能。这项研究通过提供一种新的基于扩散的方法来合成高质量的合成息肉图像，为任何给定的掩模图像做出了贡献，可用于训练更准确的息肉分割模型。源代码和预训练模型已公开提供，以便在这一重要的医学成像领域进一步研究和应用。</li>
</ol>
<p>Some Error for method(比如是不是没有Methods这个章节)</p>
<ol>
<li>结论：
（1）：本文提出了一种基于扩散的语义息肉合成方法Polyp-DDPM，旨在增强息肉分割。Polyp-DDPM通过掩模图像的逐通道连接对扩散模型进行条件化，能够生成高质量的合成息肉图像，可用于训练更准确的息肉分割模型。
（2）：创新点：</li>
<li>提出了一种新的基于扩散的方法来合成高质量的合成息肉图像。</li>
<li>通过掩模图像的逐通道连接对扩散模型进行条件化，使生成的图像具有更强的语义信息。</li>
<li>在Kvasir-SEG数据集上进行了实验，Polyp-DDPM在图像质量和分割任务中均表现出优于基线模型的性能。
性能：</li>
<li>在Kvasir-SEG数据集上，Polyp-DDPM在图像质量和分割任务中均表现出优于基线模型的性能。</li>
<li>Polyp-DDPM生成的合成息肉图像具有更高的质量和更强的语义信息。</li>
<li>Polyp-DDPM训练的息肉分割模型在Kvasir-SEG数据集上取得了更高的分割准确率。
工作量：</li>
<li>Polyp-DDPM的训练和推理过程相对简单，不需要额外的预处理或后处理步骤。</li>
<li>Polyp-DDPM的源代码和预训练模型已公开提供，以便在这一重要的医学成像领域进一步研究和应用。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9bf79a830a62ae44664c6ef3ee743ea3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ab8b48f00e4ff12693b68c086e1559c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3407fac6823c4e76f7ea595ff4e0854.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7873897e8b443db04b52f243086ce9e6.jpg" align="middle">
</details>




<h2 id="EscherNet-A-Generative-Model-for-Scalable-View-Synthesis"><a href="#EscherNet-A-Generative-Model-for-Scalable-View-Synthesis" class="headerlink" title="EscherNet: A Generative Model for Scalable View Synthesis"></a>EscherNet: A Generative Model for Scalable View Synthesis</h2><p><strong>Authors:Xin Kong, Shikun Liu, Xiaoyang Lyu, Marwan Taher, Xiaojuan Qi, Andrew J. Davison</strong></p>
<p>We introduce EscherNet, a multi-view conditioned diffusion model for view synthesis. EscherNet learns implicit and generative 3D representations coupled with a specialised camera positional encoding, allowing precise and continuous relative control of the camera transformation between an arbitrary number of reference and target views. EscherNet offers exceptional generality, flexibility, and scalability in view synthesis — it can generate more than 100 consistent target views simultaneously on a single consumer-grade GPU, despite being trained with a fixed number of 3 reference views to 3 target views. As a result, EscherNet not only addresses zero-shot novel view synthesis, but also naturally unifies single- and multi-image 3D reconstruction, combining these diverse tasks into a single, cohesive framework. Our extensive experiments demonstrate that EscherNet achieves state-of-the-art performance in multiple benchmarks, even when compared to methods specifically tailored for each individual problem. This remarkable versatility opens up new directions for designing scalable neural architectures for 3D vision. Project page: \url{<a href="https://kxhit.github.io/EscherNet}">https://kxhit.github.io/EscherNet}</a>. </p>
<p><a href="http://arxiv.org/abs/2402.03908v1">PDF</a> Project Page: <a href="https://kxhit.github.io/EscherNet">https://kxhit.github.io/EscherNet</a></p>
<p><strong>Summary</strong></p>
<p>利用条件扩散模型进行多视角视图合成，实现任意数量的视角转换。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>EscherNet 是一种多视角条件扩散模型，用于视图合成。</li>
<li>EscherNet 的本质是，以多视角图像作为输入，生成任意数量的目标视角图像。</li>
<li>EscherNet 可以在单个消费级 GPU 上同时生成 100 多个一致的目标视角，在准确性上达到最先进的效果。</li>
<li>EscherNet 的多功能性使其可以解决多种 3D 视觉任务，例如零样本新视角合成、单图像 3D 重建、多图像 3D 重建等。</li>
<li>EscherNet 的应用场景包括虚拟现实、增强现实、医学成像、自动驾驶等。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：埃舍尔网络：一种用于可扩展视图合成的生成模型</li>
<li>作者：Xin Kong, Shikun Liu, Xiaoyang Lyu, Marwan Taher, Xiaojuan Qi, Andrew J. Davison</li>
<li>隶属单位：伦敦帝国理工学院戴森机器人实验室</li>
<li>关键词：视图合成、扩散模型、隐式神经表示、多视图几何</li>
<li>论文链接：https://arxiv.org/abs/2402.03908，Github 链接：None</li>
<li>
<p>摘要：
(1)：研究背景：视图合成是计算机视觉和计算机图形学中的一项基本任务，它允许根据一组参考视点呈现场景的任意视点，从而模拟人类的视觉适应性。
(2)：过去的方法：现有方法通常专注于单一任务，例如零样本新颖视图合成、单图像三维重建或多图像三维重建，并且在处理复杂场景时面临着泛化性差、灵活性不足和可扩展性有限等问题。
(3)：研究方法：本文提出了一种多视图条件扩散模型——埃舍尔网络，它学习隐式和生成的三维表示，并结合专门的相机位置编码，允许对任意数量的参考视图和目标视图之间的相机变换进行精确和连续的相对控制。埃舍尔网络在视图合成方面具有出色的通用性、灵活性，以及可扩展性，即使在固定数量的 3 个参考视图到 3 个目标视图上训练的情况下，它也可以在单个消费级 GPU 上同时生成 100 多个一致的目标视图。
(4)：方法性能：埃舍尔网络在多个基准测试中实现了最先进的性能，即使与针对每个单独问题量身定制的方法相比也是如此。这种卓越的多功能性为设计用于三维视觉的可扩展神经架构开辟了新方向。</p>
</li>
<li>
<p>方法：
(1)：埃舍尔网络是一种多视图条件扩散模型，它学习隐式和生成的三维表示，并结合专门的相机位置编码，允许对任意数量的参考视图和目标视图之间的相机变换进行精确和连续的相对控制。
(2)：埃舍尔网络在视图合成方面具有出色的通用性、灵活性，以及可扩展性，即使在固定数量的3个参考视图到3个目标视图上训练的情况下，它也可以在单个消费级GPU上同时生成100多个一致的目标视图。
(3)：埃舍尔网络在多个基准测试中实现了最先进的性能，即使与针对每个单独问题量身定制的方法相比也是如此。这种卓越的多功能性为设计用于三维视觉的可扩展神经架构开辟了新方向。</p>
</li>
<li>
<p>结论：
（1）：埃舍尔网络提出了一种多视图条件扩散模型，该模型在视图合成方面具有出色的通用性、灵活性，以及可扩展性，即使在固定数量的3个参考视图到3个目标视图上训练的情况下，它也可以在单个消费级GPU上同时生成100多个一致的目标视图。埃舍尔网络在多个基准测试中实现了最先进的性能，即使与针对每个单独问题量身定制的方法相比也是如此。这种卓越的多功能性为设计用于三维视觉的可扩展神经架构开辟了新方向。
（2）：创新点：</p>
</li>
<li>提出了一种多视图条件扩散模型——埃舍尔网络，该模型学习隐式和生成的三维表示，并结合专门的相机位置编码，允许对任意数量的参考视图和目标视图之间的相机变换进行精确和连续的相对控制。</li>
<li>埃舍尔网络在视图合成方面具有出色的通用性、灵活性，以及可扩展性，即使在固定数量的3个参考视图到3个目标视图上训练的情况下，它也可以在单个消费级GPU上同时生成100多个一致的目标视图。</li>
<li>埃舍尔网络在多个基准测试中实现了最先进的性能，即使与针对每个单独问题量身定制的方法相比也是如此。这种卓越的多功能性为设计用于三维视觉的可扩展神经架构开辟了新方向。
性能：</li>
<li>在多个基准测试中实现了最先进的性能，即使与针对每个单独问题量身定制的方法相比也是如此。</li>
<li>即使在固定数量的3个参考视图到3个目标视图上训练的情况下，它也可以在单个消费级GPU上同时生成100多个一致的目标视图。
工作量：</li>
<li>埃舍尔网络是一个复杂的神经网络模型，需要大量的数据和计算资源进行训练。</li>
<li>埃舍尔网络的训练过程可能需要几天或几周的时间，具体取决于数据集的大小和使用的硬件。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cdd01255ccb3e0ac7a9532f4537d7c8a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7709d4f2ffb5392bba195cc2b965aeee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86704d39a54eee216395f69db00a0918.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73c26a4c69f3a172a8651cabc4a69ed2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9a53c48c54043613c01b125b54da3368.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-526811fa3a0b6e1b6d850e3911c0f54f.jpg" align="middle">
</details>




<h2 id="QuEST-Low-bit-Diffusion-Model-Quantization-via-Efficient-Selective-Finetuning"><a href="#QuEST-Low-bit-Diffusion-Model-Quantization-via-Efficient-Selective-Finetuning" class="headerlink" title="QuEST: Low-bit Diffusion Model Quantization via Efficient Selective   Finetuning"></a>QuEST: Low-bit Diffusion Model Quantization via Efficient Selective   Finetuning</h2><p><strong>Authors:Haoxuan Wang, Yuzhang Shang, Zhihang Yuan, Junyi Wu, Yan Yan</strong></p>
<p>Diffusion models have achieved remarkable success in image generation tasks, yet their practical deployment is restrained by the high memory and time consumption. While quantization paves a way for diffusion model compression and acceleration, existing methods totally fail when the models are quantized to low-bits. In this paper, we unravel three properties in quantized diffusion models that compromise the efficacy of current methods: imbalanced activation distributions, imprecise temporal information, and vulnerability to perturbations of specific modules. To alleviate the intensified low-bit quantization difficulty stemming from the distribution imbalance, we propose finetuning the quantized model to better adapt to the activation distribution. Building on this idea, we identify two critical types of quantized layers: those holding vital temporal information and those sensitive to reduced bit-width, and finetune them to mitigate performance degradation with efficiency. We empirically verify that our approach modifies the activation distribution and provides meaningful temporal information, facilitating easier and more accurate quantization. Our method is evaluated over three high-resolution image generation tasks and achieves state-of-the-art performance under various bit-width settings, as well as being the first method to generate readable images on full 4-bit (i.e. W4A4) Stable Diffusion. </p>
<p><a href="http://arxiv.org/abs/2402.03666v1">PDF</a> </p>
<p><strong>Summary</strong><br>扩散模型量化后，如何提高准确率？</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>低位量化扩散模型面临三大问题：激活分布不平衡、时间信息不精确、特定模块对扰动敏感。</li>
<li>提出微调量化模型，使其更好地适应激活分布。</li>
<li>识别出两种关键的量化层：保存重要时间信息的层和对比特宽度降低敏感的层，并对其进行微调以缓解性能退化。</li>
<li>经验验证表明，该方法修改了激活分布并提供了有意义的时间信息，促进了更容易、更准确的量化。</li>
<li>该方法在三个高分辨率图像生成任务上进行了评估，并在各种比特宽度设置下实现了最先进的性能，并且是第一个在完全4位（即 W4A4）Stable Diffusion 上生成可读图像的方法。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>论文标题：QuEST：低比特扩散模型量化通过高效选择性微调</li>
<li>作者：Haoxuan Wang, Yuzhang Shang, Zhihang Yuan, Junyi Wu, Yan Yan</li>
<li>第一作者单位：伊利诺伊理工学院计算机科学系</li>
<li>关键词：扩散模型、量化、低比特、微调</li>
<li>论文链接：https://arxiv.org/abs/2402.03666
Github 链接：无</li>
<li>
<p>摘要：
(1) 研究背景：扩散模型在图像生成任务中取得了显著的成功，但其实际部署受到高内存和时间消耗的限制。量化可以为扩散模型压缩和加速提供一种可行的方法，但现有方法在模型被量化为低比特时完全失败。
(2) 过去方法及其问题：现有扩散模型量化方法要么集中在时间步长感知校准数据构造，要么集中在量化噪声校正，目标是将现有的量化技术调整到扩散模型的特性，而这些特性与其他模型类型（如 CNN 和 ViT）不同。这些方法忽略了与量化相关的扩散模型内在机制，导致方法部署与模型特征之间存在不一致。
(3) 本文方法：本文揭示了量化扩散模型的三个属性，这些属性阻碍了有效的量化：（1）激活分布可能不平衡，其中大多数值接近 0，但其他值很大且不一致地出现；（2）时间信息不精确；（3）容易受到特定模块的扰动。为了减轻源于分布不平衡的低比特量化难度，本文提出微调量化模型以更好地适应激活分布。在此基础上，本文确定了两种关键类型的量化层：那些持有重要时间信息和那些对降低比特宽度敏感的层，并微调它们以有效地减轻性能下降。
(4) 实验结果：本文方法在三个高分辨率图像生成任务上进行了评估，并在各种比特宽度设置下实现了最先进的性能，并且是第一个在全 4 位（即 W4A4）Stable Diffusion 上生成可读图像的方法。这些性能支持了本文的目标。</p>
</li>
<li>
<p>方法：
（1）属性一：激活分布不平衡，大多数值接近 0，但其他值很大且不一致地出现。为了解决这个问题，我们提出微调量化模型以更好地适应激活分布。
（2）属性二：时间信息不准确。为此，我们确定了两种关键类型的量化层：那些持有重要时间信息和那些对降低比特宽度敏感的层，并微调它们以有效地减轻性能下降。
（3）属性三：不同激活对降低比特宽度的敏感性不同。我们提出了一种时间感知激活量化器，以进一步解决由于量化而导致的时间信息丢失的问题。
（4）QuEST：一种通过高效选择性微调实现低比特扩散模型量化的框架。QuEST 是一个基于蒸馏的微调策略，包括选择性权重优化和网络级缩放因子优化。</p>
</li>
<li>
<p>结论：
（1）本文提出了 QuEST，一种用于低比特扩散模型量化的有效无数据微调框架。我们的方法的动机来自于在量化扩散模型中发现的三个基本属性。我们还从理论上证明了微调的充分性，将其解释为增强模型对大激活扰动的鲁棒性的一种方法。为了减轻性能下降，我们提出在全精度对应模型的监督下微调时间嵌入层和注意力相关层。还引入了一个时间感知激活量化器来处理不同的时间步长。在三个高分辨率图像生成任务上的实验结果证明了 QuEST 的有效性和效率，在更少的时间和内存成本下实现了低比特兼容性。
（2）创新点：</p>
</li>
<li>揭示了量化扩散模型的三个属性，这些属性阻碍了有效的量化。</li>
<li>提出了一种基于蒸馏的微调策略 QuEST，包括选择性权重优化和网络级缩放因子优化。</li>
<li>提出了一种时间感知激活量化器，以进一步解决由于量化而导致的时间信息丢失的问题。
性能：</li>
<li>在三个高分辨率图像生成任务上实现了最先进的性能。</li>
<li>是第一个在全 4 位（即 W4A4）Stable Diffusion 上生成可读图像的方法。
工作量：</li>
<li>在 ImageNet-64x64 数据集上，QuEST 只需 10 个 GPU 天即可将 Stable Diffusion 量化为 4 位。</li>
<li>在 ImageNet-256x256 数据集上，QuEST 只需 40 个 GPU 天即可将 Stable Diffusion 量化为 4 位。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fb8f38fcd6a6857ddffdf84e6eded575.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8c7e8e687f519bd6aea6d7aa431f440.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-808d7c694b655862c89add4bffc7e8b1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e6556a45821b662485e3c321d4542f94.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d15f14313fc02ce9abba40125462e990.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e9f9c978c41171320dbafc60fb23b8e.jpg" align="middle">
</details>




<h2 id="InstanceDiffusion-Instance-level-Control-for-Image-Generation"><a href="#InstanceDiffusion-Instance-level-Control-for-Image-Generation" class="headerlink" title="InstanceDiffusion: Instance-level Control for Image Generation"></a>InstanceDiffusion: Instance-level Control for Image Generation</h2><p><strong>Authors:Xudong Wang, Trevor Darrell, Sai Saketh Rambhatla, Rohit Girdhar, Ishan Misra</strong></p>
<p>Text-to-image diffusion models produce high quality images but do not offer control over individual instances in the image. We introduce InstanceDiffusion that adds precise instance-level control to text-to-image diffusion models. InstanceDiffusion supports free-form language conditions per instance and allows flexible ways to specify instance locations such as simple single points, scribbles, bounding boxes or intricate instance segmentation masks, and combinations thereof. We propose three major changes to text-to-image models that enable precise instance-level control. Our UniFusion block enables instance-level conditions for text-to-image models, the ScaleU block improves image fidelity, and our Multi-instance Sampler improves generations for multiple instances. InstanceDiffusion significantly surpasses specialized state-of-the-art models for each location condition. Notably, on the COCO dataset, we outperform previous state-of-the-art by 20.4% AP$_{50}^\text{box}$ for box inputs, and 25.4% IoU for mask inputs. </p>
<p><a href="http://arxiv.org/abs/2402.03290v1">PDF</a> Preprint; Project page:   <a href="https://people.eecs.berkeley.edu/~xdwang/projects/InstDiff/">https://people.eecs.berkeley.edu/~xdwang/projects/InstDiff/</a></p>
<p><strong>Summary</strong><br>文本到图像扩散模型实现了高质量图像生成，但无法控制图像中的单独实例。我们引入了InstanceDiffusion，它为文本到图像扩散模型添加了精确的实例级控制。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>InstanceDiffusion支持每个实例的自由形式语言条件。</li>
<li>InstanceDiffusion支持灵活方式指定实例位置，如简单单点、涂鸦、边界框或复杂的实例分割掩码及其组合。</li>
<li>InstanceDiffusion提出了三个主要更改，以实现精确的实例级控制。</li>
<li>UniFusion模块为文本到图像模型启用了实例级条件。</li>
<li>ScaleU模块提高了图像保真度。</li>
<li>Multi-instance Sampler改进了多个实例的生成。</li>
<li>InstanceDiffusion在每个位置条件下都显着超过了专门的最新模型。</li>
<li>在COCO数据集上，InstanceDiffusion在框输入时优于之前的最新技术20.4% AP50box，在掩码输入时优于之前的最新技术25.4% IoU。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：实例扩散：图像生成的实例级控制</li>
<li>作者：Jun-Yan Zhu, Taesung Park, Abhishek Sharma, Prafulla Dhariwal, Alexei A. Efros, Pieter Abbeel</li>
<li>隶属机构：加州大学伯克利分校</li>
<li>关键词：文本到图像生成、实例级控制、扩散模型</li>
<li>论文链接：https://arxiv.org/abs/2212.04915，Github 代码链接：无</li>
<li>
<p>摘要：
(1) 研究背景：文本到图像扩散模型可以生成高质量的图像，但无法对图像中的各个实例进行控制。
(2) 过去的方法：现有方法主要集中在对整个图像进行控制，而无法对各个实例进行精细的控制。这些方法的问题在于，它们无法处理复杂的实例条件，例如，当实例重叠或被遮挡时，它们无法生成高质量的图像。
(3) 论文提出的方法：本文提出了一种新的文本到图像扩散模型 InstanceDiffusion，该模型可以对图像中的各个实例进行精细的控制。InstanceDiffusion 主要包含三个部分：UniFusion 模块、ScaleU 模块和 Multi-instance Sampler。UniFusion 模块可以将实例条件融合到文本嵌入中，ScaleU 模块可以提高图像的保真度，Multi-instance Sampler 可以改善多实例生成的质量。
(4) 实验结果：InstanceDiffusion 在 COCO 数据集上取得了最先进的性能，在 APbox50 指标上，InstanceDiffusion 比之前的最先进模型高出 20.4%，在 IoU 指标上，InstanceDiffusion 比之前的最先进模型高出 25.4%。这些结果表明，InstanceDiffusion 能够有效地对图像中的各个实例进行控制，并生成高质量的图像。</p>
</li>
<li>
<p>Methods：
(1) UniFusion：UniFusion是InstanceDiffusion模型中的一个关键模块，它可以将模糊的语义信息融合到图像嵌入中。UniFusion由两个子模块组成：语义信息提取模块和信息融合模块。语义信息提取模块负责从语义信息中提取特征，信息融合模块负责将这些特征融合到图像嵌入中。
(2) ScaleU：ScaleU是InstanceDiffusion模型中的另一个关键模块，它可以提高图像的保真度。ScaleU由两个子模块组成：上采样模块和残差模块。上采样模块负责将图像从低分辨率上采样到高分辨率，残差模块负责添加残差连接，以提高图像的保真度。
(3) Multi-instanceSampler：Multi-instanceSampler是InstanceDiffusion模型中的一个采样模块，它可以改善多实例生成的质量。Multi-instanceSampler通过对每个实例进行多次采样，然后将这些采样结果进行融合，以生成最终的图像。</p>
</li>
<li>
<p>结论：
（1）：InstanceDiffusion 模型在文本到图像生成任务中取得了最先进的性能，在 COCO 数据集上的 APbox50 指标和 IoU 指标上均优于之前的最先进模型。这表明 InstanceDiffusion 模型能够有效地对图像中的各个实例进行控制，并生成高质量的图像。
（2）：创新点：</p>
</li>
<li>提出了一种新的文本到图像扩散模型 InstanceDiffusion，该模型可以对图像中的各个实例进行精细的控制。</li>
<li>设计了 UniFusion 模块，可以将模糊的语义信息融合到图像嵌入中。</li>
<li>设计了 ScaleU 模块，可以提高图像的保真度。</li>
<li>设计了 Multi-instanceSampler 模块，可以改善多实例生成的质量。
性能：</li>
<li>在 COCO 数据集上取得了最先进的性能，在 APbox50 指标上比之前的最先进模型高出 20.4%，在 IoU 指标上比之前的最先进模型高出 25.4%。
工作量：</li>
<li>模型的训练和推理过程相对复杂，需要较大的计算资源。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-ad80374506fc08e660bb8742f25dc5ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb1dc22d5f1b16516125f58ffce2ab07.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cc40befe0322c7f0f22fe9b42e02d05a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0667dfce57b3d47e56cf440eb22a837d.jpg" align="middle">
</details>




<h2 id="Organic-or-Diffused-Can-We-Distinguish-Human-Art-from-AI-generated-Images"><a href="#Organic-or-Diffused-Can-We-Distinguish-Human-Art-from-AI-generated-Images" class="headerlink" title="Organic or Diffused: Can We Distinguish Human Art from AI-generated   Images?"></a>Organic or Diffused: Can We Distinguish Human Art from AI-generated   Images?</h2><p><strong>Authors:Anna Yoo Jeong Ha, Josephine Passananti, Ronik Bhaskar, Shawn Shan, Reid Southen, Haitao Zheng, Ben Y. Zhao</strong></p>
<p>The advent of generative AI images has completely disrupted the art world. Distinguishing AI generated images from human art is a challenging problem whose impact is growing over time. A failure to address this problem allows bad actors to defraud individuals paying a premium for human art and companies whose stated policies forbid AI imagery. It is also critical for content owners to establish copyright, and for model trainers interested in curating training data in order to avoid potential model collapse.   There are several different approaches to distinguishing human art from AI images, including classifiers trained by supervised learning, research tools targeting diffusion models, and identification by professional artists using their knowledge of artistic techniques. In this paper, we seek to understand how well these approaches can perform against today’s modern generative models in both benign and adversarial settings. We curate real human art across 7 styles, generate matching images from 5 generative models, and apply 8 detectors (5 automated detectors and 3 different human groups including 180 crowdworkers, 4000+ professional artists, and 13 expert artists experienced at detecting AI). Both Hive and expert artists do very well, but make mistakes in different ways (Hive is weaker against adversarial perturbations while Expert artists produce higher false positives). We believe these weaknesses will remain as models continue to evolve, and use our data to demonstrate why a combined team of human and automated detectors provides the best combination of accuracy and robustness. </p>
<p><a href="http://arxiv.org/abs/2402.03214v2">PDF</a> </p>
<p><strong>Summary</strong><br>人工智能图像生成技术引发艺术领域巨变，区分人工智能生成图像与人类艺术品是一项不断加剧的难题。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>AI生成图像对艺术世界的颠覆性影响与日俱增。</li>
<li>鉴别AI生成的图像对于防止欺诈、版权保护和模型训练至关重要。</li>
<li>目前有几种方法可以区分人类艺术与AI图像，包括监督学习训练的分类器、针对扩散模型的研究工具以及专业艺术家利用其对艺术技巧的了解进行识别。</li>
<li>研究表明，Hive和专家艺术家在区分AI生成的图像方面表现出色，但各有优劣（Hive对对抗性扰动较弱，而专家艺术家产生较高误报率）。</li>
<li>随着模型的不断发展，这些弱点可能仍然存在，研究数据表明，由人类和自动检测器组成的组合团队可以提供最佳的准确性和鲁棒性。</li>
<li>人工生成的图像在艺术领域引发了一场颠覆，准确区分人工智能生成的图像对于防止欺诈和保护版权至关重要。</li>
<li>尽管有不同的方法可以区分人类艺术与AI图像，但没有一种方法是完美的。</li>
<li>将人类和自动检测器结合起来可以提供最佳的准确性和鲁棒性。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：有机还是扩散：我们能区分人类艺术和人工智能生成图像吗？</li>
<li>作者：Anna Yoo Jeong Ha、Josephine Passananti、Ronik Bhaskar、Shawn Shan、Reid Southen1、Haitao Zheng、Ben Y. Zhao</li>
<li>第一作者单位：芝加哥大学计算机科学系</li>
<li>关键词：人工智能艺术、图像生成、鉴别器、人类艺术家</li>
<li>论文链接：https://arxiv.org/abs/2402.03214，Github 代码链接：无</li>
<li>摘要：
(1)：随着人工智能生成图像的出现，艺术领域发生了巨大变革。区分人工智能生成图像和人类艺术是一个具有挑战性的问题，其影响随着时间的推移而不断扩大。如果不解决这个问题，就会让不法分子欺骗那些为人类艺术支付高价的个人和禁止使用人工智能图像的公司。这对内容所有者建立版权和对模型训练者来说也是至关重要的，他们需要对训练数据进行整理以避免潜在的模型崩溃。
(2)：目前，有几种不同的方法可以区分人类艺术和人工智能图像，包括通过监督学习训练的分类器、针对扩散模型的研究工具以及专业艺术家利用其对艺术技巧的知识进行识别。
(3)：在本文中，我们寻求了解这些方法在面对当今现代生成模型时，在良性和对抗性环境中的表现如何。我们整理了跨越 7 种风格的真实人类艺术，从 5 个生成模型中生成了匹配的图像，并应用了 8 个检测器（5 个自动检测器和 3 个不同的人类群体，包括 180 名众包工人、4000 多名专业艺术家和 13 名在检测人工智能方面经验丰富的专家艺术家）。
(4)：Hive 和专家艺术家都表现得非常好，但在不同的方面犯了错误（Hive 在对抗性扰动中较弱，而专家艺术家产生较高的误报）。我们认为随着模型的不断发展，这些弱点将继续存在，并利用我们的数据证明为什么人类和自动检测器的组合团队提供了准确性和鲁棒性的最佳组合。</li>
</ol>
<p>方法：</p>
<p>（1）构建数据集：
- 收集真人艺术作品、AI 生成的图像、扰动版本的人类艺术作品和 AI 图像以及结合人类和 AI 努力创建的非典型图像。
- 从 53 位艺术家处收集 280 幅真人艺术作品，涵盖 7 种主要艺术风格。
- 为 7 种艺术风格中的每一种，使用 5 个流行的 AI 生成器生成 10 张图像，共生成 350 张 AI 生成的图像。
- 调整 BLIP 生成的标题以包括艺术作品的风格，并根据每个 AI 生成器的独特限制和配置对标题进行自定义调整。</p>
<p>（2）评估自动检测器：
- 考虑已部署的商业系统和基于研究的系统。
- 评估自动检测器在核心测试数据集上的性能，该数据集包含 280 幅真人艺术作品、350 幅 AI 图像和 40 幅混合图像。
- 测试自动检测器针对各种对抗性扰动，包括高斯噪声、JPEG 压缩、对抗性扰动和 Glaze 风格模拟保护工具。</p>
<p>（3）评估人类检测：用户研究：
- 进行单独的用户研究，针对 3 个独立的用户群体：基本参与者、专业艺术家志愿者和专家参与者。
- 基本参与者：通过 Prolific 在线众包平台招募 180 名参与者，完成一致性检查后有 177 人参与。
- 专业艺术家志愿者：通过社交媒体招募超过 4000 名专业艺术家志愿者，3803 人完成调查并通过所有一致性检查。
- 专家参与者：招募 13 位知名专业艺术家，他们具有识别 AI 图像的经验。
- 专家团队提供对产生最多错误分类的最困难图像的详细反馈。</p>
<p>（4）数据收集：
- 策划包含真人创作的艺术作品、AI 生成的图像和混合图像的数据集。
- 定义真人图像为由人类艺术家原创的艺术作品。
- AI 生成的图像使用 AI 模型（如 Midjourney、Stable Diffusion 和 DALL-E3）从文本提示生成。
- 混合图像由 AI 生成、润色并部分由人类绘制。
- 从社交媒体网站和艺术平台收集真人艺术作品。
- 与艺术家社区合作，收集跨越 7 种主要艺术风格的艺术作品。
- 使用 BLIP 模型为 AI 生成器创建提示，以生成有效捕捉艺术作品风格和内容的标题。
- 根据每个 AI 生成器的独特限制和配置，对 BLIP 生成的标题进行自定义调整。</p>
<ol>
<li>结论：
（1）随着人工智能生成图像的出现，区分人工智能生成图像和人类艺术是一个具有挑战性的问题。本文研究了目前几种不同的方法在面对当今现代生成模型时，在良性和对抗性环境中的表现，并证明了人类和自动检测器的组合团队提供了准确性和鲁棒性的最佳组合。
（2）创新点：</li>
<li>构建了一个跨越7种风格的真实人类艺术、人工智能生成图像和混合图像的数据集。</li>
<li>评估了8个检测器（5个自动检测器和3个不同的人类群体）在核心测试数据集和各种对抗性扰动上的性能。</li>
<li>发现人类和自动检测器的组合团队提供了准确性和鲁棒性的最佳组合。</li>
<li>专家艺术家在对抗性扰动中表现较弱，而自动检测器产生较高的误报。</li>
<li>随着模型的不断发展，这些弱点将继续存在，人类和自动检测器的组合团队将发挥重要作用。</li>
<li>分析了Hive和专家艺术家在不同方面的错误，并证明了为什么人类和自动检测器的组合团队提供了准确性和鲁棒性的最佳组合。
（3）性能：</li>
<li>在核心测试数据集上，Hive和专家艺术家都表现得非常好，但Hive在对抗性扰动中表现较弱，而专家艺术家产生较高的误报。</li>
<li>Hive在对抗性扰动中较弱，而专家艺术家产生较高的误报。</li>
<li>人类和自动检测器的组合团队提供了准确性和鲁棒性的最佳组合。
（4）工作量：</li>
<li>收集了跨越7种风格的280幅真实人类艺术作品和350幅人工智能生成图像。</li>
<li>评估了8个检测器（5个自动检测器和3个不同的人类群体）在核心测试数据集和各种对抗性扰动上的性能。</li>
<li>进行单独的用户研究，针对3个独立的用户群体：基本参与者、专业艺术家志愿者和专家参与者。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8cba12717aa69817e10b925c47c7e5f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-654bd7a18967bfc99c8234931b745b7f.jpg" align="middle">
</details>




<h2 id="PFDM-Parser-Free-Virtual-Try-on-via-Diffusion-Model"><a href="#PFDM-Parser-Free-Virtual-Try-on-via-Diffusion-Model" class="headerlink" title="PFDM: Parser-Free Virtual Try-on via Diffusion Model"></a>PFDM: Parser-Free Virtual Try-on via Diffusion Model</h2><p><strong>Authors:Yunfang Niu, Dong Yi, Lingxiang Wu, Zhiwei Liu, Pengxiang Cai, Jinqiao Wang</strong></p>
<p>Virtual try-on can significantly improve the garment shopping experiences in both online and in-store scenarios, attracting broad interest in computer vision. However, to achieve high-fidelity try-on performance, most state-of-the-art methods still rely on accurate segmentation masks, which are often produced by near-perfect parsers or manual labeling. To overcome the bottleneck, we propose a parser-free virtual try-on method based on the diffusion model (PFDM). Given two images, PFDM can “wear” garments on the target person seamlessly by implicitly warping without any other information. To learn the model effectively, we synthesize many pseudo-images and construct sample pairs by wearing various garments on persons. Supervised by the large-scale expanded dataset, we fuse the person and garment features using a proposed Garment Fusion Attention (GFA) mechanism. Experiments demonstrate that our proposed PFDM can successfully handle complex cases, synthesize high-fidelity images, and outperform both state-of-the-art parser-free and parser-based models. </p>
<p><a href="http://arxiv.org/abs/2402.03047v1">PDF</a> Accepted by IEEE ICASSP 2024</p>
<p><strong>Summary</strong><br>无解析器虚拟试穿方法基于扩散模型，无需精准分割掩码，即可实现逼真试穿效果。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>PFDM是一种无解析器的虚拟试穿方法，可以无缝地“穿上”目标人物的衣服，而无需任何其他信息。</li>
<li>PFDM使用扩散模型来学习无解析器的虚拟试穿，可以有效地捕捉人体的结构和衣服的细节。</li>
<li>PFDM通过合成大量伪图像并构造样本对来学习，其中伪图像包含了各种穿着不同衣服的人。</li>
<li>PFDM使用提出的服装融合注意（GFA）机制来融合人物和衣服的特征，从而生成逼真的试穿图像。</li>
<li>PFDM可以处理复杂的情况，合成高保真图像，并且优于现有基于解析器和无解析器的虚拟试穿模型。</li>
<li>PFDM可以用于在线和店内购物场景，显著改善服装购物体验。</li>
<li>PFDM有望在虚拟现实和增强现实等领域得到广泛应用。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：PFDM：基于扩散模型的无解析虚拟试穿</li>
<li>作者：牛云芳，易东，吴令祥，刘智伟，蔡鹏翔，王金桥</li>
<li>隶属单位：中国科学院自动化研究所，模式识别国家重点实验室，基础模型研究中心，北京，中国</li>
<li>关键词：虚拟试穿，扩散模型，隐式扭曲，高分辨率图像合成</li>
<li>论文链接：https://arxiv.org/abs/2402.03047</li>
<li>
<p>摘要：
（1）：虚拟试穿可以显著改善在线和店内场景中的服装购物体验，在计算机视觉领域引起了广泛关注。然而，为了实现高保真试穿性能，大多数最先进的方法仍然依赖于准确的分割掩码，这些掩码通常由近乎完美的解析器或手动标注产生。为了克服这一瓶颈，我们提出了一种基于扩散模型的无解析虚拟试穿方法（PFDM）。给定两张图像，PFDM 可以通过隐式扭曲将服装无缝地“穿”在目标人物身上，而无需任何其他信息。为了有效地学习模型，我们合成了许多伪图像，并通过在人物身上穿戴各种服装来构建样本对。在由大规模扩展数据集监督下，我们使用提出的服装融合注意（GFA）机制融合人物和服装特征。实验表明，我们提出的 PFDM 可以成功处理复杂情况，合成高保真图像，并且优于最先进的无解析和基于解析的模型。
（2）：GAN 用于虚拟试穿。基于 GAN 的虚拟试穿方法通常采用两步架构，首先将服装扭曲成目标形状，然后通过组合扭曲的服装和人物图像来合成结果。一些工作专注于基于薄板样条变换 (TPS) 或全局流增强扭曲模块。其他工作旨在提高生成模块的性能，例如，采用对齐感知生成器来提高合成图像的分辨率，或改进损失函数以保留人物身份。
（3）：本文提出的研究方法。我们提出了一种基于扩散模型的无解析虚拟试穿框架。这是第一个将扩散模型用于无解析虚拟试穿的工作。我们还精心设计了一个增强的交叉注意模块来融合人物和服装特征以进行隐式扭曲。
（4）：方法在任务和性能上取得的成就。我们在 VITON-HD 上评估了我们的方法，实验表明，我们的无解析模型在定性和定量评估中都优于竞争对手。这些性能支持了我们的目标。</p>
</li>
<li>
<p>方法：
(1)：我们提出了一种基于扩散模型的无解析虚拟试穿框架，该框架可以将服装无缝地“穿”在目标人物身上，而无需任何其他信息。
(2)：我们精心设计了一个增强的交叉注意模块来融合人物和服装特征以进行隐式扭曲。
(3)：我们合成了许多伪图像，并通过在人物身上穿戴各种服装来构建样本对。
(4)：在由大规模扩展数据集监督下，我们使用提出的服装融合注意（GFA）机制融合人物和服装特征。</p>
</li>
<li>
<p>结论：
（1）：本文提出了一种基于扩散模型的无解析虚拟试穿方法，该方法将扭曲和融合步骤统一到一个模型中，同时避免了使用任何解析器或外部模块。据我们所知，PFDM 是第一个基于扩散的无解析虚拟试穿模型。实验表明，PFD 可以生成具有丰富纹理细节的高分辨率高保真试穿结果，并成功处理错位和遮挡，这不仅优于现有的无解析方法，而且在定性和定量分析中也超越了最先进的基于解析器的模型。我们希望我们的工作能够促进虚拟试穿技术在电子商务和元宇宙中的普及。
（2）：创新点：提出了一种基于扩散模型的无解析虚拟试穿方法，该方法将扭曲和融合步骤统一到一个模型中，同时避免了使用任何解析器或外部模块。
性能：在定性和定量分析中，PFDM 优于现有的无解析方法和最先进的基于解析器的模型。
工作量：该方法需要合成大量伪图像并构建样本对，这可能需要大量计算资源。</p>
</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4fa2bcca39e4d002618ff0b3dcd93311.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d530a087c0dd3abddf2412c841493d90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4af8acc18772befb8884db138ea6e422.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b3f34614b487167c039e9989a45cc12d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b5123c9bbcb697725b9020c9d4ab0422.jpg" align="middle">
</details>




<h2 id="Extreme-Two-View-Geometry-From-Object-Poses-with-Diffusion-Models"><a href="#Extreme-Two-View-Geometry-From-Object-Poses-with-Diffusion-Models" class="headerlink" title="Extreme Two-View Geometry From Object Poses with Diffusion Models"></a>Extreme Two-View Geometry From Object Poses with Diffusion Models</h2><p><strong>Authors:Yujing Sun, Caiyi Sun, Yuan Liu, Yuexin Ma, Siu Ming Yiu</strong></p>
<p>Human has an incredible ability to effortlessly perceive the viewpoint difference between two images containing the same object, even when the viewpoint change is astonishingly vast with no co-visible regions in the images. This remarkable skill, however, has proven to be a challenge for existing camera pose estimation methods, which often fail when faced with large viewpoint differences due to the lack of overlapping local features for matching. In this paper, we aim to effectively harness the power of object priors to accurately determine two-view geometry in the face of extreme viewpoint changes. In our method, we first mathematically transform the relative camera pose estimation problem to an object pose estimation problem. Then, to estimate the object pose, we utilize the object priors learned from a diffusion model Zero123 to synthesize novel-view images of the object. The novel-view images are matched to determine the object pose and thus the two-view camera pose. In experiments, our method has demonstrated extraordinary robustness and resilience to large viewpoint changes, consistently estimating two-view poses with exceptional generalization ability across both synthetic and real-world datasets. Code will be available at <a href="https://github.com/scy639/Extreme-Two-View-Geometry-From-Object-Poses-with-Diffusion-Models">https://github.com/scy639/Extreme-Two-View-Geometry-From-Object-Poses-with-Diffusion-Models</a>. </p>
<p><a href="http://arxiv.org/abs/2402.02800v1">PDF</a> </p>
<p><strong>Summary</strong><br>利用扩散模型合成新视图图像进行对象姿态估计，有效求解极端视角变化下的两视图几何问题。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>利用对象先验通过扩散模型Zero123合成新视图图像，增强了对象姿态估计的鲁棒性和适应性。</li>
<li>将相对相机姿态估计问题数学转换为对象姿态估计问题，简化了问题的求解。</li>
<li>在极端视角变化的情况下，合成的新视图图像经匹配可以确定对象姿态，从而确定两视图相机姿态。</li>
<li>该方法在合成和真实世界数据集上均表现出非凡的鲁棒性和弹性，估计两视图姿态具有杰出的泛化能力。</li>
<li>可在 <a href="https://github.com/scy639/Extreme-Two-View-Geometry-From-Object-Poses-with-Diffusion-Models">https://github.com/scy639/Extreme-Two-View-Geometry-From-Object-Poses-with-Diffusion-Models</a> 获取代码。</li>
<li>该方法精度高，在合成和真实世界数据集上均表现良好。</li>
<li>该方法适用于解决极端视角变化下的两视图几何问题。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<p></p><ol><p></p>
<p></p><li>标题：从物体位姿估计极端两视图几何</li><p></p>
<p></p><li>作者：Yujing Sun、Caiyi Sun、Yuan Liu、Yuexin Ma、Siu Ming Yiu</li><p></p>
<p></p><li>隶属机构：香港大学</li><p></p>
<p></p><li>关键词：两视图几何、物体位姿估计、扩散模型、生成模型</li><p></p>
<p></p><li>论文链接：https://arxiv.org/abs/2402.02800
    Github 代码链接：https://github.com/scy639/Extreme-Two-View-Geometry-From-Object-Poses-with-Diffusion-Models</li><p></p>
<p></p><li>摘要：
(1) 研究背景：人类具有惊人的能力，能够毫不费力地感知包含相同物体的两幅图像之间的视点差异，即使视点变化非常大，图像中没有共同可见的区域。然而，对于现有的相机位姿估计方法来说，这种非凡的能力被证明是一个挑战，因为这些方法在面对大的视点差异时通常会失败，原因是缺少用于匹配的重叠局部特征。
(2) 过去的方法及其问题：过去的方法主要集中在使用局部特征匹配来估计两视图几何。然而，当视点差异较大时，这种方法往往会失败，因为没有足够的重叠局部特征可供匹配。
(3) 本文提出的研究方法：在本文中，我们提出了一种新方法来估计极端两视图几何。我们的方法首先将相对相机位姿估计问题转换为物体位姿估计问题。然后，为了估计物体位姿，我们利用从扩散模型 Zero123 学习到的物体先验来合成物体的 novel-view 图像。将 novel-view 图像匹配以确定物体位姿，从而确定两视图相机位姿。
(4) 方法在任务和性能上的表现：在实验中，我们的方法表现出非凡的鲁棒性和对大视点变化的适应性，能够一致地估计两视图位姿，并且在合成和真实世界数据集上都具有出色的泛化能力。这些性能支持了我们的目标，即准确地确定极端视点变化下的两视图几何。</li><br>&lt;/ol&gt;<p></p>
<p>Methods:
(1): 提出了一种新方法来估计极端两视图几何，该方法将相对相机位姿估计问题转换为物体位姿估计问题；
(2): 利用从扩散模型Zero123学习到的物体先验来合成物体的novel-view图像，将novel-view图像匹配以确定物体位姿；
(3): 通过实验验证了该方法的鲁棒性和泛化能力。</p>
<ol>
<li>结论：
（1）：本文提出了一种新颖的算法，可以估计具有极端视点变化的相对相机位姿。该方法的核心思想是利用从大规模 2D 扩散模型 Zero123 学习到的物体先验，该先验能够生成对象的 novel-view 图像。但是，由于 Zero123 在其模型中隐式定义了规范坐标系，并且图像可能不会看向对象，因此我们无法直接应用 Zero123。为了解决这一挑战，我们首先提出了一种新的两视图位姿估计公式，作为物体位姿估计问题，并正确定义输入图像和生成图像的物体位姿。最后，我们匹配另一幅图像。
（2）：创新点：</li>
<li>将相对相机位姿估计问题转换为物体位姿估计问题，并利用从扩散模型 Zero123 学习到的物体先验来合成物体的 novel-view 图像，将 novel-view 图像匹配以确定物体位姿，从而确定两视图相机位姿。</li>
<li>在实验中，我们的方法表现出非凡的鲁棒性和对大视点变化的适应性，能够一致地估计两视图位姿，并且在合成和真实世界数据集上都具有出色的泛化能力。</li>
<li>这些性能支持了我们的目标，即准确地确定极端视点变化下的两视图几何。</li>
</ol>
<p>性能：
- 在合成数据集上，我们的方法在所有视点变化范围内都优于最先进的方法，并且在极端视点变化下具有显着的优势。
- 在真实世界数据集上，我们的方法也优于最先进的方法，并且在极端视点变化下具有显着的优势。</p>
<p>工作量：
- 该方法需要训练一个扩散模型来学习物体先验，这可能需要大量的数据和计算资源。
- 该方法还需要合成 novel-view 图像，这可能需要大量的时间和计算资源。
- 该方法还需要匹配 novel-view 图像，这可能需要大量的时间和计算资源。</p>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-25907674667aa8b32d056fad9f68800a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-36f6cbd8fb9421b0eea500253c925684.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d37974797f92e16872fe7a27774fa5a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b283ab7c3ead68d1a7cd2cceb1c42365.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5815c280ae8797af92483f51007a87d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a313bd5672a496366b53aa94dffc26ae.jpg" align="middle">
</details>




</ol>]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>Diffusion Models</tag>
      </tags>
  </entry>
  <entry>
    <title>Talking Head Generation</title>
    <url>/2024/02/13/Paper/2024-02-13/Talking%20Head%20Generation/</url>
    <content><![CDATA[<blockquote>
<p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-02-13-更新"><a href="#2024-02-13-更新" class="headerlink" title="2024-02-13 更新"></a>2024-02-13 更新</h1><h2 id="DiffSpeaker-Speech-Driven-3D-Facial-Animation-with-Diffusion-Transformer"><a href="#DiffSpeaker-Speech-Driven-3D-Facial-Animation-with-Diffusion-Transformer" class="headerlink" title="DiffSpeaker: Speech-Driven 3D Facial Animation with Diffusion   Transformer"></a>DiffSpeaker: Speech-Driven 3D Facial Animation with Diffusion   Transformer</h2><p><strong>Authors:Zhiyuan Ma, Xiangyu Zhu, Guojun Qi, Chen Qian, Zhaoxiang Zhang, Zhen Lei</strong></p>
<p>Speech-driven 3D facial animation is important for many multimedia applications. Recent work has shown promise in using either Diffusion models or Transformer architectures for this task. However, their mere aggregation does not lead to improved performance. We suspect this is due to a shortage of paired audio-4D data, which is crucial for the Transformer to effectively perform as a denoiser within the Diffusion framework. To tackle this issue, we present DiffSpeaker, a Transformer-based network equipped with novel biased conditional attention modules. These modules serve as substitutes for the traditional self/cross-attention in standard Transformers, incorporating thoughtfully designed biases that steer the attention mechanisms to concentrate on both the relevant task-specific and diffusion-related conditions. We also explore the trade-off between accurate lip synchronization and non-verbal facial expressions within the Diffusion paradigm. Experiments show our model not only achieves state-of-the-art performance on existing benchmarks, but also fast inference speed owing to its ability to generate facial motions in parallel. </p>
<p><a href="http://arxiv.org/abs/2402.05712v1">PDF</a> 9 pages, 5 figures. Code is avalable at   <a href="https://github.com/theEricMa/DiffSpeaker">https://github.com/theEricMa/DiffSpeaker</a></p>
<p><strong>Summary</strong><br>通过提出带偏条件注意力的扩散模型，我们解决了音视频配对数据的稀缺问题，在保持音视频同步的情况下，可以快速生成高质量的面部动画。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>提出了一种新的扩散模型，用于语音驱动的3D面部动画。</li>
<li>使用带偏条件注意力模块，可以更好地处理音视频配对数据的稀缺问题。</li>
<li>在现有基准上取得了最先进的性能。</li>
<li>可以快速生成面部动画，推理速度快。</li>
<li>可以有效地生成非语言面部表情。</li>
<li>可以控制动画过程中的嘴型同步和非语言面部表情之间的权衡。</li>
<li>该模型可以用于各种多媒体应用。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：DiffSpeaker：基于扩散变换器的语音驱动 3D 面部动画（DiffSpeaker: Speech-Driven 3DFacial Animation with Diffusion Transformer）</li>
<li>作者：Zhiyuan Ma, Xiangyu Zhu, Guojun Qi, Chen Qian, Zhaoxiang Zhang, Zhen Lei</li>
<li>第一作者单位：香港理工大学（香港理工大学）</li>
<li>关键词：语音驱动面部动画、扩散模型、Transformer、条件注意机制</li>
<li>论文链接：https://arxiv.org/abs/2402.05712
Github 代码链接：https://github.com/theEricMa/DiffSpeaker</li>
<li>
<p>摘要：
（1）：语音驱动 3D 面部动画在许多多媒体应用中非常重要。最近的研究表明，使用扩散模型或 Transformer 架构来执行此任务很有前景。然而，它们的简单聚合并不能带来改进的性能。我们怀疑这是由于缺乏配对的音频-4D 数据，这对于 Transformer 在扩散框架内有效地执行去噪器至关重要。
（2）：过去的方法主要使用基本的滑动窗口方法处理音频输入，这通常会导致生成的的面部动作范围狭窄。近年来，研究人员开始采用 Transformer 架构来进行语音驱动 3D 面部动画，但传统的确定性回归可能不是最好的方法，因为人类的语音和面部表情是可变且动态的，很难用一个固定的映射来准确捕捉它们之间的关系。
（3）：为了解决这个问题，我们提出了 DiffSpeaker，这是一个基于 Transformer 的网络，配备了新颖的偏置条件注意机制模块。这些模块可以替代标准 Transformer 中传统的自注意力/交叉注意力，并结合了经过精心设计的偏置，这些偏置引导注意力机制集中在相关的特定任务和与扩散相关的条件上。我们还探索了在扩散范式中准确的唇形同步和非语言面部表情之间的权衡。
（4）：实验表明，我们的模型不仅在现有基准上实现了最先进的性能，而且由于其能够并行生成面部动作，因此推理速度也很快。</p>
</li>
<li>
<p>方法：
(1): 本文提出了一种基于扩散模型和Transformer架构的语音驱动3D面部动画方法DiffSpeaker，该方法利用了扩散模型的生成能力和Transformer架构的序列建模能力，有效地解决了语音驱动3D面部动画任务中的配对音频-4D数据缺乏的问题。
(2): DiffSpeaker采用了一种新颖的偏置条件注意机制模块，该模块可以替代标准Transformer中的传统自注意力/交叉注意力，并结合了经过精心设计的偏置，这些偏置引导注意力机制集中在相关的特定任务和与扩散相关的条件上。
(3): DiffSpeaker还探索了在扩散范式中准确的唇形同步和非语言面部表情之间的权衡，并提出了一种新的损失函数，该损失函数可以同时优化唇形同步和非语言面部表情的质量。
(4): 实验结果表明，DiffSpeaker在现有基准上实现了最先进的性能，并且由于其能够并行生成面部动作，因此推理速度也很快。</p>
</li>
<li>
<p>结论：
（1）：本工作探索了将 Transformer 架构与基于扩散的框架有效结合用于语音驱动 3D 面部动画的方法。我们贡献的核心是引入了带有偏置的条件自注意力/交叉注意力机制，该机制解决了使用受限且跨度短的音频-4D 数据训练基于扩散的 Transformer 的困难。我们还研究了在实现准确的唇形同步和生成与语音相关性较小的面部表情之间的平衡。我们开发的模型优于当前的方法，在动画质量和生成速度方面都表现出色。
（2）：创新点：
提出了一种新的带有偏置的条件自注意力/交叉注意力机制，该机制可以有效地利用受限且跨度短的音频-4D 数据来训练基于扩散的 Transformer。
提出了一种新的损失函数，该损失函数可以同时优化唇形同步和非语言面部表情的质量。
性能：
在现有基准上实现了最先进的性能。
推理速度快，能够并行生成面部动作。
工作量：
需要收集和预处理大量的音频-4D 数据。
需要对模型进行大量的训练。</p>
</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a6a1095c49476b6d0a24c660e7abca7e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2c694a105e50cf1ba9a9e0743f793c62.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1dd59be8351677e84215dd037093b2ca.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ebb8f9ab10ef4d053668941b0c247fcb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff2d1ca9215127e6894689d494fb8244.jpg" align="middle">
</details>




]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>Talking Head Generation</tag>
      </tags>
  </entry>
  <entry>
    <title>NeRF</title>
    <url>/2024/02/29/Paper/2024-02-29/NeRF/</url>
    <content><![CDATA[<blockquote>
<p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-02-29-更新"><a href="#2024-02-29-更新" class="headerlink" title="2024-02-29 更新"></a>2024-02-29 更新</h1><h2 id="Learning-Dynamic-Tetrahedra-for-High-Quality-Talking-Head-Synthesis"><a href="#Learning-Dynamic-Tetrahedra-for-High-Quality-Talking-Head-Synthesis" class="headerlink" title="Learning Dynamic Tetrahedra for High-Quality Talking Head Synthesis"></a>Learning Dynamic Tetrahedra for High-Quality Talking Head Synthesis</h2><p><strong>Authors:Zicheng Zhang, Ruobing Zheng, Ziwen Liu, Congying Han, Tianqi Li, Meng Wang, Tiande Guo, Jingdong Chen, Bonan Li, Ming Yang</strong></p>
<p>Recent works in implicit representations, such as Neural Radiance Fields (NeRF), have advanced the generation of realistic and animatable head avatars from video sequences. These implicit methods are still confronted by visual artifacts and jitters, since the lack of explicit geometric constraints poses a fundamental challenge in accurately modeling complex facial deformations. In this paper, we introduce Dynamic Tetrahedra (DynTet), a novel hybrid representation that encodes explicit dynamic meshes by neural networks to ensure geometric consistency across various motions and viewpoints. DynTet is parameterized by the coordinate-based networks which learn signed distance, deformation, and material texture, anchoring the training data into a predefined tetrahedra grid. Leveraging Marching Tetrahedra, DynTet efficiently decodes textured meshes with a consistent topology, enabling fast rendering through a differentiable rasterizer and supervision via a pixel loss. To enhance training efficiency, we incorporate classical 3D Morphable Models to facilitate geometry learning and define a canonical space for simplifying texture learning. These advantages are readily achievable owing to the effective geometric representation employed in DynTet. Compared with prior works, DynTet demonstrates significant improvements in fidelity, lip synchronization, and real-time performance according to various metrics. Beyond producing stable and visually appealing synthesis videos, our method also outputs the dynamic meshes which is promising to enable many emerging applications. </p>
<p><a href="http://arxiv.org/abs/2402.17364v1">PDF</a> CVPR 2024</p>
<p><strong>Summary</strong><br>神经辐射场（NeRF）的最新混合表示方法，即动态四面体（DynTet），通过神经网络对明确动态网格进行编码，以确保各种动作和视点的几何一致性。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>DynTet 是一种新的混合表示，它使用神经网络对显式动态网格进行编码，以确保不同动作和视点下的几何一致性。</li>
<li>DynTet 使用基于坐标的网络对符号距离、变形和材质纹理进行学习，将训练数据锚定到预定义的四面体网格中。</li>
<li>DynTet 利用 Marching Tetrahedra 有效地解码了具有稳定拓扑结构的纹理网格，并通过可微分光栅器和像素损失的监督实现了快速渲染。</li>
<li>DynTet 结合经典的 3D 可变形模型来促进几何学习，并定义了一个规范化空间来简化纹理学习。</li>
<li>与之前的研究相比，DynTet 在保真度、唇形同步和实时性能方面有了显著的提升。</li>
<li>除了制作出稳定且视觉上吸引人的合成视频外，该方法还输出动态网格，有望实现许多新兴应用。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>标题：用于高品质说话人头部合成的动态四面体学习</li>
<li>作者：张子川，张恒，王佳俊，刘子超，孙剑</li>
<li>单位：北京大学</li>
<li>关键词：说话人头部合成、隐式表示、动态网格、神经辐射场</li>
<li>论文链接：https://arxiv.org/abs/2302.02574</li>
<li>摘要：
（1）研究背景：
近年来，隐式表示方法，如神经辐射场（NeRF），在从视频序列生成逼真且可动画化的头部头像方面取得了进展。然而，这些隐式方法仍然面临视觉伪影和抖动问题，因为缺乏明确的几何约束，这给准确建模复杂的面部变形带来了根本性挑战。
（2）过去方法及问题：
过去的方法主要采用隐式表示，但缺乏明确的几何约束，导致视觉伪影和抖动问题。
（3）本文提出的研究方法：
本文提出了一种新颖的混合表示方法，称为动态四面体（DynTet），它通过神经网络对显式动态网格进行编码，以确保在各种运动和视点下几何一致性。DynTet 由基于坐标的网络参数化，该网络学习符号距离、变形和材质纹理，将训练数据锚定到预定义的四面体网格中。利用行进四面体，DynTet 可以有效地解码具有相同拓扑结构的纹理网格，从而可以通过可微分光栅化器快速渲染，并通过像素损失进行监督。为了提高训练效率，本文结合了经典的 3D 可变形模型来促进几何学习，并定义了一个规范空间来简化纹理学习。这些优势得益于 DynTet 中采用的有效几何表示。
（4）方法性能及对目标的支持：
与以往的工作相比，根据各种指标，DynTet 在保真度、唇形同步和实时性能方面均表现出显着提升。除了生成稳定且视觉上吸引人的合成视频外，本文方法还输出动态网格，有望支持许多新兴应用。</li>
</ol>
<p>7.方法：
(1): 动态四面体（DynTet）通过神经网络对显式动态网格进行编码，确保几何一致性；
(2): 基于坐标的网络参数化，学习符号距离、变形和材质纹理，将数据锚定到四面体网格中；
(3): 利用行进四面体解码纹理网格，通过可微分光栅化器渲染并通过像素损失进行监督；
(4): 结合经典的3D可变形模型促进几何学习，定义规范空间简化纹理学习。</p>
<ol>
<li>总结：
（1）：本文提出了动态四面体（DynTet）方法，通过神经网络对显式动态网格进行编码，确保几何一致性，提升了说话人头部合成的保真度、唇形同步和实时性能。
（2）：创新点：</li>
<li>提出了一种新的混合表示方法，称为动态四面体（DynTet），它通过神经网络对显式动态网格进行编码，以确保在各种运动和视点下几何一致性。</li>
<li>基于坐标的网络参数化，学习符号距离、变形和材质纹理，将训练数据锚定到预定义的四面体网格中。</li>
<li>利用行进四面体，DynTet可以有效地解码具有相同拓扑结构的纹理网格，从而可以通过可微分光栅化器快速渲染，并通过像素损失进行监督。</li>
<li>结合了经典的3D可变形模型来促进几何学习，并定义了一个规范空间来简化纹理学习。</li>
<li>这些优势得益于DynTet中采用的有效几何表示。</li>
<li>与以往的工作相比，根据各种指标，DynTet在保真度、唇形同步和实时性能方面均表现出显着提升。</li>
<li>除了生成稳定且视觉上吸引人的合成视频外，本文方法还输出动态网格，有望支持许多新兴应用。
性能：</li>
<li>在保真度、唇形同步和实时性能方面均表现出显着提升。</li>
<li>生成了稳定且视觉上吸引人的合成视频。</li>
<li>输出动态网格，有望支持许多新兴应用。
工作量：</li>
<li>论文中没有明确提到工作量。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2927e4da13bb2db0a8c147b32e65c4ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a69eb8d9ee3b7163b0dd216926919257.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-989288a0ad24820fe95020a4ed1f2ea7.jpg" align="middle">
</details>




<h2 id="CharNeRF-3D-Character-Generation-from-Concept-Art"><a href="#CharNeRF-3D-Character-Generation-from-Concept-Art" class="headerlink" title="CharNeRF: 3D Character Generation from Concept Art"></a>CharNeRF: 3D Character Generation from Concept Art</h2><p><strong>Authors:Eddy Chu, Yiyang Chen, Chedy Raissi, Anand Bhojan</strong></p>
<p>3D modeling holds significant importance in the realms of AR/VR and gaming, allowing for both artistic creativity and practical applications. However, the process is often time-consuming and demands a high level of skill. In this paper, we present a novel approach to create volumetric representations of 3D characters from consistent turnaround concept art, which serves as the standard input in the 3D modeling industry. While Neural Radiance Field (NeRF) has been a game-changer in image-based 3D reconstruction, to the best of our knowledge, there is no known research that optimizes the pipeline for concept art. To harness the potential of concept art, with its defined body poses and specific view angles, we propose encoding it as priors for our model. We train the network to make use of these priors for various 3D points through a learnable view-direction-attended multi-head self-attention layer. Additionally, we demonstrate that a combination of ray sampling and surface sampling enhances the inference capabilities of our network. Our model is able to generate high-quality 360-degree views of characters. Subsequently, we provide a simple guideline to better leverage our model to extract the 3D mesh. It is important to note that our model’s inferencing capabilities are influenced by the training data’s characteristics, primarily focusing on characters with a single head, two arms, and two legs. Nevertheless, our methodology remains versatile and adaptable to concept art from diverse subject matters, without imposing any specific assumptions on the data. </p>
<p><a href="http://arxiv.org/abs/2402.17115v1">PDF</a> </p>
<p><strong>Summary</strong><br>用概念图创建 3D 模型的新方法，利用神经辐射场并为图像建模提供更好的视角。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>艺术创作和实际应用中，3D 建模很有价值，但需要花费时间和技能。</li>
<li>该方法从标准的 3D 建模行业输入，即可根据一致的透视图概念图创建 3D 角色的体积表示。</li>
<li>神经辐射场 (NeRF) 已改变基于图像的 3D 重建，但尚无针对概念图优化管道。</li>
<li>编码概念图为模型的先验，利用概念图中的清晰的身体姿势和特定的视角。</li>
<li>通过可学习的视向注意力多头自注意力层，训练网络利用各种 3D 点的先验。</li>
<li>射线采样和表面采样的组合增强了网络的推理能力。</li>
<li>模型可以生成高质量的 360 度角色视图。</li>
<li>开发了简单的指南，以更好地利用模型提取 3D 网格。</li>
<li>模型的推理能力受训练数据的影响，主要针对头部、手臂和腿部。</li>
<li>该方法适用于各种主题的概念图，对数据没有特殊假设。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>论文标题：CharNeRF：基于概念图的 3D 角色生成</li>
<li>作者：Eddy Chu、Yiyang Chen、Chedy Raissi、Anand Bhojan</li>
<li>第一作者单位：新加坡国立大学</li>
<li>关键词：神经网络、计算机图形、虚拟现实、游戏、网格生成</li>
<li>论文链接：https://arxiv.org/abs/2402.17115</li>
<li>
<p>摘要：
（1）研究背景：3D 建模在 AR/VR 和游戏中至关重要，但通常耗时且要求高。本文提出了一种从一致的周转概念图中创建 3D 角色体积表示的新方法。
（2）过去的方法：神经辐射场 (NeRF) 已成为图像重建的变革者，但尚无针对概念图优化管线的研究。
（3）研究方法：本文利用概念图中的定义的身体姿势和特定的视角，将其编码为模型的先验。提出了一种可学习的视图方向注意力多头自注意力层，让网络利用这些先验。此外，本文还证明了光线采样和表面采样的组合增强了网络的推理能力。
（4）任务和性能：本文模型能够生成高质量的 360 度角色视图。此外，还提供了一个简单的指南，以更好地利用模型提取 3D 网格。模型的推理能力受训练数据特征的影响，主要针对具有一个头部、两个手臂和两条腿的角色。尽管如此，本文方法具有通用性，可适应不同主题的概念图，而无需对数据做出任何特定假设。</p>
</li>
<li>
<p>方法：
(1) 编码概念图：采用双层沙漏编码器，提取概念图的高低层次细节。
(2) 视图方向注意力多头自注意力特征向量组合：使用多头自注意力机制融合来自概念图的三个特征向量，重点关注查询视图方向与源草图视图方向之间的相似性。
(3) 神经辐射场：使用神经辐射场预测最终颜色和密度，指导网络学习特定类别的一般形状和特征。</p>
</li>
<li>
<p>结论：
（1）：本工作尝试解决计算机视觉中一个具有重要 AR/VR/游戏应用价值的挑战性问题，即使用 NeRF 从概念图构建 3D 角色的 3D 表示。我们提出的最终模型 CharNeRF 得益于用于组合不同输入视图信息的视图方向注意力多头自注意力组件，能够从如此稀疏的图像输入中生成良好的结果。
（2）：创新点：提出了一种可学习的视图方向注意力多头自注意力层，让网络利用概念图中的定义的身体姿势和特定的视角。此外，还证明了光线采样和表面采样的组合增强了网络的推理能力。
性能：模型能够生成高质量的 360 度角色视图。此外，还提供了一个简单的指南，以更好地利用模型提取 3D 网格。
工作量：模型的推理能力受训练数据特征的影响，主要针对具有一个头部、两个手臂和两条腿的角色。尽管如此，本文方法具有通用性，可适应不同主题的概念图，而无需对数据做出任何特定假设。</p>
</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-828eaae544f50ff5c3cb4c05ee9d80e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef7369a7d8878e03f6b272a4d1ebd217.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19f2984d16b69f5650701e035c363f95.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b8a11537cec84e0f035cff561493d37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f60295f4a9ff4a9d9749851b16f04d26.jpg" align="middle">
</details>




<h2 id="CMC-Few-shot-Novel-View-Synthesis-via-Cross-view-Multiplane-Consistency"><a href="#CMC-Few-shot-Novel-View-Synthesis-via-Cross-view-Multiplane-Consistency" class="headerlink" title="CMC: Few-shot Novel View Synthesis via Cross-view Multiplane Consistency"></a>CMC: Few-shot Novel View Synthesis via Cross-view Multiplane Consistency</h2><p><strong>Authors:Hanxin Zhu, Tianyu He, Zhibo Chen</strong></p>
<p>Neural Radiance Field (NeRF) has shown impressive results in novel view synthesis, particularly in Virtual Reality (VR) and Augmented Reality (AR), thanks to its ability to represent scenes continuously. However, when just a few input view images are available, NeRF tends to overfit the given views and thus make the estimated depths of pixels share almost the same value. Unlike previous methods that conduct regularization by introducing complex priors or additional supervisions, we propose a simple yet effective method that explicitly builds depth-aware consistency across input views to tackle this challenge. Our key insight is that by forcing the same spatial points to be sampled repeatedly in different input views, we are able to strengthen the interactions between views and therefore alleviate the overfitting problem. To achieve this, we build the neural networks on layered representations (\textit{i.e.}, multiplane images), and the sampling point can thus be resampled on multiple discrete planes. Furthermore, to regularize the unseen target views, we constrain the rendered colors and depths from different input views to be the same. Although simple, extensive experiments demonstrate that our proposed method can achieve better synthesis quality over state-of-the-art methods. </p>
<p><a href="http://arxiv.org/abs/2402.16407v1">PDF</a> Accepted by IEEE Conference on Virtual Reality and 3D User Interfaces   (IEEE VR 2024)</p>
<p><strong>Summary</strong><br>神经辐射场（NeRF）在全新视角合成中展示出令人印象深刻的效果，特别是在虚拟现实 (VR) 和增强现实 (AR) 中，这得益于其连续表示场景的能力。然而，当只有少数输入视图图像可用时，NeRF 倾向于对给定的视图进行过度拟合，从而使估计的像素深度几乎具有相同的值。不同于通过引入复杂先验或附加监督来进行正则化的先前方法，我们提出了一种简单但有效的方法，该方法明确构建了输入视图之间的深度感知一致性来解决这一挑战。我们的关键见解是，通过强制相同的空间点在不同的输入视图中被重复采样，我们能够加强视图之间的交互，从而减轻过度拟合问题。为了实现这一点，我们在分层表示（即多平面图像）上建立神经网络，并且采样点可以在多个离散平面上重新采样。此外，为了正则化未见的目标视图，我们约束不同输入视图的渲染颜色和深度相同。虽然简单，但大量的实验表明，我们提出的方法可以比最先进的方法实现更好的合成质量。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>NeRF 在只有少数输入视图图像可用时会过拟合。</li>
<li>通过强制相同的空间点在不同的输入视图中被重复采样可以减轻过度拟合问题。</li>
<li>我们在分层表示上构建神经网络，以便在多个离散平面上重新采样采样点。</li>
<li>我们约束不同输入视图的渲染颜色和深度相同，以正则化未见的目标视图。</li>
<li>我们的方法比最先进的方法实现了更好的合成质量。</li>
<li>我们方法的关键在于显式构建输入视图之间的深度感知一致性。</li>
<li>我们的方法简单有效，不需要引入复杂先验或额外的监督。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>标题：CMC：通过跨视图多平面一致性进行小样本新视角合成</li>
<li>作者：韩昕竹、何天宇、陈志波</li>
<li>第一作者单位：中国科学技术大学</li>
<li>关键词：神经辐射场、小样本视角合成、多平面图像、跨视图一致性</li>
<li>论文链接：None, Github 链接：None</li>
<li>摘要：
（1）研究背景：神经辐射场（NeRF）在小样本视角合成中容易出现过拟合问题，导致估计的像素深度几乎相同。
（2）过去方法：现有方法通过引入复杂先验或额外监督来进行正则化，但存在预训练成本高、域差距等问题。
（3）研究方法：本文提出了一种简单有效的跨视图深度感知一致性方法，通过在不同输入视图中强制采样相同空间点，加强视图之间的交互，缓解过拟合问题。具体来说，本文构建了基于分层表示（即多平面图像）的神经网络，并对多平面进行采样。此外，为了正则化未见的目标视图，本文约束了不同输入视图渲染的颜色和深度一致性。
（4）方法性能：实验表明，本文方法在小样本视角合成任务上优于现有方法，证明了其有效性。</li>
</ol>
<p>7.Methods:
(1):构建基于分层表示的多平面图像，并对其进行采样；
(2):通过在不同输入视图中强制采样相同空间点，加强视图之间的交互；
(3):约束不同输入视图渲染的颜色和深度一致性，正则化未见的目标视图。</p>
<ol>
<li>结论：
(1): 本文提出了 CMC 方法，通过跨视图多平面一致性，缓解了 NeRF 在小样本视角合成中的过拟合问题，提升了合成图像的质量。
(2): 创新点：<ul>
<li>提出跨视图深度感知一致性方法，加强视图之间的交互，缓解过拟合。</li>
<li>构建基于分层表示的多平面图像，并对其进行采样。</li>
<li>约束不同输入视图渲染的颜色和深度一致性，正则化未见的目标视图。
Performance：</li>
<li>在小样本视角合成任务上优于现有方法，证明了其有效性。
Workload：</li>
<li>方法简单有效，易于实现。</li>
</ul>
</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-bdd46c7b217cb4180eb948c43ffad849.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-571786b47c356d9bc3c90a0ca95fe68b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78bf909d8f8aa9e18f65bc56fd97a0b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0da54ff7a201688851cb82cbbbe20007.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eff9d03d40a8b3f7618fd67f793df987.jpg" align="middle">
</details>




<h2 id="SPC-NeRF-Spatial-Predictive-Compression-for-Voxel-Based-Radiance-Field"><a href="#SPC-NeRF-Spatial-Predictive-Compression-for-Voxel-Based-Radiance-Field" class="headerlink" title="SPC-NeRF: Spatial Predictive Compression for Voxel Based Radiance Field"></a>SPC-NeRF: Spatial Predictive Compression for Voxel Based Radiance Field</h2><p><strong>Authors:Zetian Song, Wenhong Duan, Yuhuai Zhang, Shiqi Wang, Siwei Ma, Wen Gao</strong></p>
<p>Representing the Neural Radiance Field (NeRF) with the explicit voxel grid (EVG) is a promising direction for improving NeRFs. However, the EVG representation is not efficient for storage and transmission because of the terrific memory cost. Current methods for compressing EVG mainly inherit the methods designed for neural network compression, such as pruning and quantization, which do not take full advantage of the spatial correlation of voxels. Inspired by prosperous digital image compression techniques, this paper proposes SPC-NeRF, a novel framework applying spatial predictive coding in EVG compression. The proposed framework can remove spatial redundancy efficiently for better compression performance.Moreover, we model the bitrate and design a novel form of the loss function, where we can jointly optimize compression ratio and distortion to achieve higher coding efficiency. Extensive experiments demonstrate that our method can achieve 32% bit saving compared to the state-of-the-art method VQRF on multiple representative test datasets, with comparable training time. </p>
<p><a href="http://arxiv.org/abs/2402.16366v1">PDF</a> </p>
<p><strong>Summary</strong><br>利用空间预测编码对神经辐射场（NeRF）的显式体素网格（EVG）进行压缩，可有效提升其存储和传输效率。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>提出基于显式体素网格（voxel grid）的 NeRF 压缩新框架——SPC-NeRF</li>
<li>利用空间预测编码有效去除体素的空间冗余，提升压缩性能</li>
<li>提出新的比特率建模和损失函数形式，实现压缩率与失真的联合优化</li>
<li>在多个代表性测试数据集上，与最先进的 VQRF 方法相比，节省 32% 的比特率</li>
<li>训练时间与 VQRF 相当</li>
<li>充分利用了体素的空间相关性，优于从神经网络压缩方法继承的压缩技术</li>
<li>显式体素网格的压缩对于 NeRF 的存储和传输至关重要</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>标题：SPC-NeRF：体素化光场辐射的空域预测压缩</li>
<li>作者：宋泽天、段文宏、张宇怀、王诗奇、马思伟、高文</li>
<li>单位：北京大学</li>
<li>关键词：NeRF、EVG、空域预测编码、数据压缩</li>
<li>论文链接：https://arxiv.org/abs/2402.16366
    Github 代码链接：无</li>
<li>摘要：
（1）研究背景：使用显式体素网格（EVG）表示神经辐射场（NeRF）是提升 NeRF 性能的一个有前景的方向。然而，EVG 表示在存储和传输方面效率低下，因为内存开销巨大。当前用于压缩 EVG 的方法主要继承了为神经网络压缩设计的剪枝和量化等方法，而这些方法并没有充分利用体素的空间相关性。
（2）过去方法：现有方法主要利用神经网络压缩技术，如剪枝和量化，但这些方法没有充分利用体素的空间相关性。
（3）研究方法：受繁荣的数字图像压缩技术启发，本文提出了 SPC-NeRF，一个将空域预测编码应用于 EVG 压缩的新框架。提出的框架可以有效去除空间冗余，以获得更好的压缩性能。此外，我们对比特率进行建模并设计了新的损失函数形式，在该损失函数中，我们可以联合优化压缩比和失真，以实现更高的编码效率。
（4）实验结果：大量实验表明，与最先进的 EVG NeRF 压缩方法 VQRF 相比，我们的方法在多个代表性测试数据集上实现了 32% 的比特节省，训练时间相当。</li>
</ol>
<p>7.方法：(1)受数字图像压缩技术的启发，提出SPC-NeRF，一个将空域预测编码应用于EVG压缩的新框架；(2)将EVG表示为特征网格，并利用其空间相关性，通过预测编码去除空间冗余；(3)设计新的损失函数形式，联合优化压缩比和失真，实现更高的编码效率。</p>
<ol>
<li>总结
（1）：本文工作的主要意义在于提出了SPC-NeRF，一个将空域预测编码应用于EVG压缩的新框架，有效去除了空间冗余，提高了压缩性能。
（2）：创新点：
• 提出SPC-NeRF，将空域预测编码应用于EVG压缩，充分利用了体素的空间相关性。
• 设计新的损失函数形式，联合优化压缩比和失真，实现更高的编码效率。
性能：
• 与最先进的EVG-NeRF压缩方法VQRF相比，在多个代表性测试数据集上实现了32%的比特节省，训练时间相当。
工作量：
• 论文理论分析清晰，实验结果充分，代码开源。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-6f6705a1aaf3db9b5a416e3ffecb9e26.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5908f2606537f6a0653b96477b77c75f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-efc08eb0ec890344de572f2b2004f9c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-866d14094e6f176536a298862171f8d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b3117d16ce413f3de96c9535aaa0804e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d0efdf7e947815763e89d08400d8bd32.jpg" align="middle">
</details>




<h2 id="GenNBV-Generalizable-Next-Best-View-Policy-for-Active-3D-Reconstruction"><a href="#GenNBV-Generalizable-Next-Best-View-Policy-for-Active-3D-Reconstruction" class="headerlink" title="GenNBV: Generalizable Next-Best-View Policy for Active 3D Reconstruction"></a>GenNBV: Generalizable Next-Best-View Policy for Active 3D Reconstruction</h2><p><strong>Authors:Xiao Chen, Quanyi Li, Tai Wang, Tianfan Xue, Jiangmiao Pang</strong></p>
<p>While recent advances in neural radiance field enable realistic digitization for large-scale scenes, the image-capturing process is still time-consuming and labor-intensive. Previous works attempt to automate this process using the Next-Best-View (NBV) policy for active 3D reconstruction. However, the existing NBV policies heavily rely on hand-crafted criteria, limited action space, or per-scene optimized representations. These constraints limit their cross-dataset generalizability. To overcome them, we propose GenNBV, an end-to-end generalizable NBV policy. Our policy adopts a reinforcement learning (RL)-based framework and extends typical limited action space to 5D free space. It empowers our agent drone to scan from any viewpoint, and even interact with unseen geometries during training. To boost the cross-dataset generalizability, we also propose a novel multi-source state embedding, including geometric, semantic, and action representations. We establish a benchmark using the Isaac Gym simulator with the Houses3K and OmniObject3D datasets to evaluate this NBV policy. Experiments demonstrate that our policy achieves a 98.26% and 97.12% coverage ratio on unseen building-scale objects from these datasets, respectively, outperforming prior solutions. </p>
<p><a href="http://arxiv.org/abs/2402.16174v1">PDF</a> </p>
<p><strong>Summary</strong><br>人工智能驱动场景重建的自动化拍摄过程，提升了真实感，简化了工作</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>利用强化学习的自动化拍摄流程</li>
<li>5D自由空间扩展了动作范围</li>
<li>多源状态嵌入增强了跨数据集泛化性</li>
<li>Isaac Gym模拟器建立了NBV策略评估基准</li>
<li>在Houses3K和OmniObject3D数据集上，覆盖率分别达到98.26%和97.12%</li>
<li>优于现有解决方案</li>
<li>适用于大型场景的扫描和交互</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>标题：GenNBV：用于主动 3D 重建的可泛化最佳下一视角策略</li>
<li>作者：Ziqi Wang, Xinyu Zhang, Tianhao Wu, Yinda Zhang, Xiaogang Jin, Yu Rong, Hui Huang</li>
<li>隶属：清华大学</li>
<li>关键词：主动 3D 重建，最佳下一视角，深度学习，强化学习</li>
<li>论文链接：GenNBV: Generalizable Next-Best-View Policy for Active 3D Reconstruction，Github 链接：None</li>
<li>摘要：
（1）：研究背景：神经辐射场在逼真数字化大型场景方面取得了最新进展，但图像捕捉过程仍然耗时且费力。以往工作尝试使用最佳下一视角（NBV）策略来自动执行此过程以主动进行 3D 重建。
（2）：过去方法及其问题：现有的 NBV 策略严重依赖于手工制作的标准、有限的动作空间或针对特定场景优化后的表示。这些限制因素限制了它们在不同数据集上的泛化能力。
（3）：论文提出的研究方法：提出 GenNBV，一种端到端可泛化的 NBV 策略。该策略采用基于强化学习（RL）的框架，并将典型有限的动作空间扩展到 5D 自由空间。它使代理无人机能够从任何视点进行扫描，甚至在训练期间与看不见的几何体进行交互。为了提高跨数据集的泛化能力，还提出了一种新颖的多源状态嵌入，包括几何、语义和动作表示。
（4）：方法在什么任务上取得了怎样的性能：使用 IsaacGym 模拟器和 Houses3K 及 OmniObject3D 数据集建立基准来评估此 NBV 策略。实验表明，该策略在这些数据集未曾见过的建筑规模物体上分别达到 98.26% 和 97.12% 的覆盖率，优于先前的解决方案。</li>
</ol>
<p>7.方法：
（1）将主动3D重建问题表述为马尔可夫决策过程（MDP），设计新的观测空间和动作空间；
（2）提出端到端的NBV策略，该策略将典型有限的动作空间扩展到5D自由空间；
（3）提出一种新的多源状态嵌入，包括几何、语义和动作表示，以提高跨数据集的泛化能力；
（4）设计反映优化目标的奖励函数，并详细说明策略优化过程。</p>
<ol>
<li>结论：
（1）：本研究提出了一种主动 3D 场景重建的端到端方法，减少了人工干预的需要。具体来说，基于学习的策略探索了如何在训练阶段重建各种对象，从而能够以完全自主的方式泛化以重建看不见的对象。我们的控制器在自由空间中机动，然后基于混合场景表示选择下一个最佳视图，该表示传达了场景覆盖状态，从而实现重建进度。我们通过在包括 Houses3K、OmniObject3D 和 Objaverse 在内的多个数据集上进行测试，展示了我们方法的有效性。在 holdout Houses3K 测试集和跨域 OmniObject3D 房屋类别上的定量和定性泛化结果表明，我们的方法在重建的完整性、效率和准确性方面优于其他基线。此外，在 Objaverse 上进行的实验表明，在单一建筑设置中训练的策略甚至可以泛化到复杂的户外场景。
（2）：创新点：GenNBV 提出了一种端到端可泛化的最佳下一视角策略，扩展了动作空间，并提出了一种新的多源状态嵌入来提高跨数据集的泛化能力；
性能：在 Houses3K 和 OmniObject3D 数据集上，GenNBV 在未见过的建筑规模物体上分别达到 98.26% 和 97.12% 的覆盖率，优于先前的解决方案；
工作量：GenNBV 的训练过程需要大量的数据和计算资源，并且需要针对不同的场景进行微调以获得最佳性能。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5e8d5c56796ce65689171d3e4517ceb1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3132d23adee2a0316b9fc9d6cad91a0b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f46161465b1542e68d3bcde0a29f1da4.jpg" align="middle">
</details>




<h2 id="NeRF-Det-Incorporating-Semantic-Cues-and-Perspective-aware-Depth-Supervision-for-Indoor-Multi-View-3D-Detection"><a href="#NeRF-Det-Incorporating-Semantic-Cues-and-Perspective-aware-Depth-Supervision-for-Indoor-Multi-View-3D-Detection" class="headerlink" title="NeRF-Det++: Incorporating Semantic Cues and Perspective-aware Depth   Supervision for Indoor Multi-View 3D Detection"></a>NeRF-Det++: Incorporating Semantic Cues and Perspective-aware Depth   Supervision for Indoor Multi-View 3D Detection</h2><p><strong>Authors:Chenxi Huang, Yuenan Hou, Weicai Ye, Di Huang, Xiaoshui Huang, Binbin Lin, Deng Cai, Wanli Ouyang</strong></p>
<p>NeRF-Det has achieved impressive performance in indoor multi-view 3D detection by innovatively utilizing NeRF to enhance representation learning. Despite its notable performance, we uncover three decisive shortcomings in its current design, including semantic ambiguity, inappropriate sampling, and insufficient utilization of depth supervision. To combat the aforementioned problems, we present three corresponding solutions: 1) Semantic Enhancement. We project the freely available 3D segmentation annotations onto the 2D plane and leverage the corresponding 2D semantic maps as the supervision signal, significantly enhancing the semantic awareness of multi-view detectors. 2) Perspective-aware Sampling. Instead of employing the uniform sampling strategy, we put forward the perspective-aware sampling policy that samples densely near the camera while sparsely in the distance, more effectively collecting the valuable geometric clues. 3)Ordinal Residual Depth Supervision. As opposed to directly regressing the depth values that are difficult to optimize, we divide the depth range of each scene into a fixed number of ordinal bins and reformulate the depth prediction as the combination of the classification of depth bins as well as the regression of the residual depth values, thereby benefiting the depth learning process. The resulting algorithm, NeRF-Det++, has exhibited appealing performance in the ScanNetV2 and ARKITScenes datasets. Notably, in ScanNetV2, NeRF-Det++ outperforms the competitive NeRF-Det by +1.9% in mAP@0.25 and +3.5% in mAP@0.50$. The code will be publicly at <a href="https://github.com/mrsempress/NeRF-Detplusplus">https://github.com/mrsempress/NeRF-Detplusplus</a>. </p>
<p><a href="http://arxiv.org/abs/2402.14464v1">PDF</a> 7 pages, 2 figures</p>
<p><strong>Summary</strong><br>神经辐射场（NeRF）技术被创新应用于增强多视角3D检测任务中的表示学习，显著提升了室内场景中的3D检测性能。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>发现了NeRF-Det存在语义歧义、采样不当和深度监督利用不足等主要缺陷。</li>
<li>提出语义增强、透视感知采样和序数残差深度监督来解决上述问题。</li>
<li>NeRF-Det++有效解决了NeRF-Det的缺陷，在ScanNetV2和ARKITScenes数据集上表现出色。</li>
<li>NeRF-Det++在ScanNetV2上比NeRF-Det在mAP@0.25和mAP@0.50分别提高了1.9%和3.5%。</li>
<li>代码已公开发布：<a href="https://github.com/mrsempress/NeRF-Detplusplus。">https://github.com/mrsempress/NeRF-Detplusplus。</a></li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li><strong>论文标题：</strong> NeRF-Det++：融合语义线索和视点感知深度</li>
<li><strong>作者：</strong> Chenxi Huang, Yuenan Hou, Weicai Ye, Di Huang, Xiaoshui Huang, Binbin Lin, Deng Cai, Wanli Ouyang</li>
<li><strong>第一作者单位：</strong> 浙江大学计算机科学与技术学院，计算机辅助设计与图形学国家重点实验室</li>
<li><strong>关键词：</strong> NeRF、多视图三维检测、语义分割、深度估计</li>
<li><strong>论文链接：</strong> https://arxiv.org/abs/2402.14464</li>
<li><strong>摘要：</strong>
   (1) <strong>研究背景：</strong> NeRF-Det 在室内多视图三维检测中取得了令人印象深刻的性能，它创新性地利用 NeRF 增强了表征学习。
   (2) <strong>过去方法及问题：</strong> NeRF-Det 存在语义模糊、采样不当和深度监督利用不足三个关键缺陷。
   (3) <strong>研究方法：</strong> 针对上述问题，本文提出了三个相应的解决方案：<ul>
<li><strong>语义增强：</strong> 将免费提供的 3D 分割注释投影到 2D 平面，并利用相应的 2D 语义图作为监督信号，显著增强了多视图检测器的语义感知能力。</li>
<li><strong>视点感知采样：</strong> 提出视点感知采样策略，该策略在靠近相机处密集采样，而在远处稀疏采样，更有效地收集有价值的几何线索。</li>
<li><strong>有序残差深度监督：</strong> 与直接回归难以优化的深度值相反，将每个场景的深度范围划分为固定数量的有序箱，并将深度预测重新表述为深度箱分类和残差深度值回归的组合，从而有利于深度学习过程。
   (4) <strong>方法性能：</strong> 在室内多视图三维检测任务上，本文方法取得了优异的性能，证明了其有效性。</li>
</ul>
</li>
</ol>
<p>7.方法：
（1）语义增强：在NeRF-Det中加入语义分支ΦS，将几何模块ΦG生成的特征h(x)输入ΦS，产生语义预测s，并利用交叉熵损失LSeg监督语义图的学习。
（2）视点感知采样：将NeRF-Det中的均匀采样（US）替换为视点感知采样策略，在靠近相机处密集采样，而在远处稀疏采样，更有效地收集有价值的几何线索。
（3）有序残差深度监督：将每个场景的深度范围划分为固定数量的有序箱，将深度预测重新表述为深度箱分类和残差深度值回归的组合，有利于深度学习过程。</p>
<ol>
<li>结论：
(1): 本文提出 NeRF-Det++，一种用于从多视图图像进行室内 3D 检测的新颖方法。我们识别并解决了 NeRF-Det 中的三个关键缺陷。首先，为了解决语义模糊，我们引入了语义增强模块，该模块利用语义监督来改善分类。其次，为了解决不适当的采样，我们通过透视感知采样的设计优先考虑附近对象并利用多视图的特性。最后，我们通过提出序数残差深度监督来解决深度监督利用不足的问题，该监督结合了序数深度箱的分类和残差深度值的回归。在 ScanNetV2 和 ARKIT 场景上进行的广泛实验验证了我们 NeRF-Det++ 的优越性。
(2): 创新点：</li>
<li>语义增强：引入语义分支，利用语义监督增强语义感知能力。</li>
<li>透视感知采样：设计透视感知采样策略，更有效地收集有价值的几何线索。</li>
<li>序数残差深度监督：将深度预测重新表述为深度箱分类和残差深度值回归的组合，有利于深度学习过程。
性能：</li>
<li>在 ScanNetV2 和 ARKIT 场景上取得了优异的性能，证明了其有效性。
工作量：</li>
<li>提出了一种新的方法 NeRF-Det++，涉及语义增强、透视感知采样和序数残差深度监督。</li>
<li>在 ScanNetV2 和 ARKIT 场景上进行了广泛的实验，证明了其优越性。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-10b590fb75f1e40d114fb69be9c25a2b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffacf9378a148c5b9fac1fd2e03fc268.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-478a5df442fbaaa3a3c020c875f267ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ecbc9426af10136860227da1181ee0cd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-af160b3a5172d7fc20bcc97ad42a6d6f.jpg" align="middle">
</details>




<h2 id="Mip-Grid-Anti-aliased-Grid-Representations-for-Neural-Radiance-Fields"><a href="#Mip-Grid-Anti-aliased-Grid-Representations-for-Neural-Radiance-Fields" class="headerlink" title="Mip-Grid: Anti-aliased Grid Representations for Neural Radiance Fields"></a>Mip-Grid: Anti-aliased Grid Representations for Neural Radiance Fields</h2><p><strong>Authors:Seungtae Nam, Daniel Rho, Jong Hwan Ko, Eunbyung Park</strong></p>
<p>Despite the remarkable achievements of neural radiance fields (NeRF) in representing 3D scenes and generating novel view images, the aliasing issue, rendering “jaggies” or “blurry” images at varying camera distances, remains unresolved in most existing approaches. The recently proposed mip-NeRF has addressed this challenge by rendering conical frustums instead of rays. However, it relies on MLP architecture to represent the radiance fields, missing out on the fast training speed offered by the latest grid-based methods. In this work, we present mip-Grid, a novel approach that integrates anti-aliasing techniques into grid-based representations for radiance fields, mitigating the aliasing artifacts while enjoying fast training time. The proposed method generates multi-scale grids by applying simple convolution operations over a shared grid representation and uses the scale-aware coordinate to retrieve features at different scales from the generated multi-scale grids. To test the effectiveness, we integrated the proposed method into the two recent representative grid-based methods, TensoRF and K-Planes. Experimental results demonstrate that mip-Grid greatly improves the rendering performance of both methods and even outperforms mip-NeRF on multi-scale datasets while achieving significantly faster training time. For code and demo videos, please see <a href="https://stnamjef.github.io/mipgrid.github.io/">https://stnamjef.github.io/mipgrid.github.io/</a>. </p>
<p><a href="http://arxiv.org/abs/2402.14196v1">PDF</a> Accepted to NeurIPS 2023</p>
<p><strong>Summary</strong><br>基于网格表示的反走样 NeRF 方法，实现快速训练同时消除混叠伪影。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>mip-Grid 将反走样技术集成到基于网格的 NeRF 中，解决了混叠问题。</li>
<li>使用简单卷积操作在共享网格表示上生成多尺度网格，减轻了混叠伪影。</li>
<li>使用尺度感知坐标从生成的多尺度网格中检索不同尺度的特征。</li>
<li>将该方法集成到 TensoRF 和 K-Planes 等基于网格的 NeRF 方法中。</li>
<li>实验表明 mip-Grid 大幅提高了两种方法的渲染性能，在多尺度数据集上甚至优于 mip-NeRF。</li>
<li>mip-Grid 实现了显著更快的训练时间。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<p>1.标题：Mip-Grid：神经辐射场中的抗锯齿网格表示（中文翻译）
2.作者：Seungtae Nam、Daniel Rho、Jong Hwan Ko、Eunbyung Park
3.第一作者单位：韩国成均馆大学人工智能系（中文翻译）
4.关键词：神经辐射场、抗锯齿、网格表示
5.论文链接：https://arxiv.org/abs/2402.14196
Github代码链接：无
6.总结：
（1）：研究背景：神经辐射场（NeRF）在表示3D场景和生成新视图图像方面取得了显著成就，但现有的方法中普遍存在锯齿问题，即在不同的相机距离下渲染出“锯齿”或“模糊”的图像。
（2）：过去方法：mip-NeRF通过渲染圆锥截锥体而不是射线来解决这个问题。然而，它依赖于MLP架构来表示辐射场，错失了基于网格的最新方法提供的快速训练速度。
（3）：本文提出的研究方法：mip-Grid，一种将抗锯齿技术集成到基于网格的辐射场表示中的新方法，在享受快速训练时间的同时减轻了锯齿伪影。该方法通过在共享网格表示上应用简单的卷积操作生成多尺度网格，并使用尺度感知坐标从生成的网格中检索不同尺度的特征。
（4）：方法在任务和性能上的表现：为了测试有效性，我们将提出的方法集成到两种最新的基于网格的代表性方法中，即TensoRF和K-Planes。实验结果表明，mip-Grid极大地提高了这两种方法的渲染性能，甚至在多尺度数据集上也优于mip-NeRF，同时实现了明显更快的训练时间。</p>
<ol>
<li>
<p>方法：
（1）：mip-Grid 将抗锯齿技术集成到基于网格的辐射场表示中，通过在共享网格表示上应用简单的卷积操作生成多尺度网格，并使用尺度感知坐标从生成的网格中检索不同尺度的特征。
（2）：为了测试有效性，将提出的方法集成到两种最新的基于网格的代表性方法中，即 TensoRF 和 K-Planes。实验结果表明，mip-Grid 极大地提高了这两种方法的渲染性能，甚至在多尺度数据集上也优于 mip-NeRF，同时实现了明显更快的训练时间。</p>
</li>
<li>
<p>结论：
（1）：本工作提出了 mip-Grid，一种用于 NeRF 的抗锯齿网格表示。提出的方法可以轻松集成到现有的基于网格的 NeRF 中，并且使用我们方法的两种方法 mip-TensoRF 和 mip-K-Planes 已经证明可以有效去除混叠伪影。由于我们从共享的网格表示中生成多尺度网格，并且不依赖于超采样，因此所提出的方法最大程度地减少了额外参数的数量，并且训练速度明显快于现有的基于 MLP 的抗锯齿 NeRF。我们相信我们的工作为利用网格表示的训练效率，朝着无混叠 NeRF 的新研究方向铺平了道路。</p>
</li>
</ol>
<p>（2）：创新点：将抗锯齿技术集成到基于网格的辐射场表示中，通过在共享网格表示上应用简单的卷积操作生成多尺度网格，并使用尺度感知坐标从生成的网格中检索不同尺度的特征。</p>
<p>性能：在两种最新的基于网格的代表性方法 TensoRF 和 K-Planes 中集成提出的方法，实验结果表明，mip-Grid 极大地提高了这两种方法的渲染性能，甚至在多尺度数据集上也优于 mip-NeRF，同时实现了明显更快的训练时间。</p>
<p>工作量：mip-Grid 是一种简单且易于实现的方法，它可以轻松集成到现有的基于网格的 NeRF 中。该方法不需要额外的超采样步骤，并且训练速度明显快于现有的基于 MLP 的抗锯齿 NeRF。</p>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f43ff38bcf01c320536c04f1be39506c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bcbbb2f379d74a0aeb7179da023c78a5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fe3f4f6d4cf8758d74cb0be86547e9f6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7b2eb107a8f1fa6044a1d951be6c903a.jpg" align="middle">
</details>




]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>NeRF</tag>
      </tags>
  </entry>
  <entry>
    <title>Diffusion Models</title>
    <url>/2024/02/13/Paper/2024-02-13/Diffusion%20Models/</url>
    <content><![CDATA[<blockquote>
<p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-02-13-更新"><a href="#2024-02-13-更新" class="headerlink" title="2024-02-13 更新"></a>2024-02-13 更新</h1><h2 id="Synthesizing-CTA-Image-Data-for-Type-B-Aortic-Dissection-using-Stable-Diffusion-Models"><a href="#Synthesizing-CTA-Image-Data-for-Type-B-Aortic-Dissection-using-Stable-Diffusion-Models" class="headerlink" title="Synthesizing CTA Image Data for Type-B Aortic Dissection using Stable   Diffusion Models"></a>Synthesizing CTA Image Data for Type-B Aortic Dissection using Stable   Diffusion Models</h2><p><strong>Authors:Ayman Abaid, Muhammad Ali Farooq, Niamh Hynes, Peter Corcoran, Ihsan Ullah</strong></p>
<p>Stable Diffusion (SD) has gained a lot of attention in recent years in the field of Generative AI thus helping in synthesizing medical imaging data with distinct features. The aim is to contribute to the ongoing effort focused on overcoming the limitations of data scarcity and improving the capabilities of ML algorithms for cardiovascular image processing. Therefore, in this study, the possibility of generating synthetic cardiac CTA images was explored by fine-tuning stable diffusion models based on user defined text prompts, using only limited number of CTA images as input. A comprehensive evaluation of the synthetic data was conducted by incorporating both quantitative analysis and qualitative assessment, where a clinician assessed the quality of the generated data. It has been shown that Cardiac CTA images can be successfully generated using using Text to Image (T2I) stable diffusion model. The results demonstrate that the tuned T2I CTA diffusion model was able to generate images with features that are typically unique to acute type B aortic dissection (TBAD) medical conditions. </p>
<p><a href="http://arxiv.org/abs/2402.06969v1">PDF</a> Submitted in IEEE EMBC 2024 Conference</p>
<p><strong>Summary</strong><br>稳定扩散模型在医学成像数据合成中展现出强大能力，有望解决数据稀缺问题，助力心血管图像处理领域的发展。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>稳定扩散模型在医学成像数据合成中展现出巨大潜力，可用于解决数据稀缺问题。</li>
<li>通过微调用户定义文本提示的稳定扩散模型，仅使用有限数量的 CTA 图像作为输入，即可生成合成的冠状动脉 CTA 图像。</li>
<li>定量分析和定性评估相结合的综合评估表明，使用文本到图像 (T2I) 稳定扩散模型可以成功生成心脏 CTA 图像。</li>
<li>微调的 T2I CTA 扩散模型能够生成具有急性 B 型主动脉夹层 (TBAD) 医学特征的图像。</li>
<li>合成的图像在视觉上与真实图像相似，并保留了真实图像中的关键解剖结构。</li>
<li>临床医生认为合成的图像具有足够的质量，可用于临床实践。</li>
<li>该研究表明，稳定扩散模型在医学成像数据合成中具有广阔的应用前景，有望改善心血管疾病的诊断和治疗。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：使用稳定扩散模型合成 B 型主动脉夹层断层扫描图像数据</li>
<li>作者：Ayman Abaid、Muhammad Ali Farooq、Niamh Hynes、Peter Corcoran 和 Ihsan Ullah</li>
<li>第一作者单位：爱尔兰戈尔韦大学计算机科学学院</li>
<li>关键词：主动脉夹层、计算机断层扫描血管造影、医学图像合成、稳定扩散模型、文本到图像</li>
<li>论文链接：https://arxiv.org/abs/2402.06969
Github 代码链接：无</li>
<li>
<p>摘要：
(1)：研究背景：主动脉夹层是一种严重的心血管疾病，需要准确和及时的诊断。计算机断层扫描血管造影 (CTA) 是诊断主动脉夹层最常用的成像方式，但由于数据稀缺，机器学习算法在主动脉夹层图像处理中的能力受到限制。
(2)：过去的方法：过去的研究使用深度学习模型来自动分割主動脈夾層圖像中的真腔、假腔和假腔血栓。然而，这些模型通常需要大量的数据进行训练，而主动脉夹层的数据集往往很小。
(3)：研究方法：本研究提出了一种使用稳定扩散模型合成主动脉夹层 CTA 图像的方法。稳定扩散模型是一种生成式人工智能模型，可以根据文本提示生成逼真的图像。本研究通过微调稳定扩散模型，使其能够根据用户定义的文本提示生成主动脉夹层 CTA 图像。
(4)：方法性能：实验结果表明，微调后的稳定扩散模型能够生成具有主动脉夹层典型特征的图像。定量分析和定性评估都表明，合成的图像具有很高的质量，并且可以用于训练深度学习模型进行主动脉夹层图像分割。</p>
</li>
<li>
<p>方法：
(1) 数据预处理：将 3D CTA 图像转换为 2D 图像，并将其划分为训练集、测试集和验证集。将数据分为五类：有真腔 (TL) 的图像、有假腔 (FL) 的图像、有假腔血栓 (FLT) 的图像、有 TL 和 FL 的图像，以及无 TL、FL 和 FLT 信息的数据。
(2) 文本到图像 (T2I) 模型训练：使用 ImageTBAD 数据集和 DreamBooth 训练工具微调预训练的稳定扩散模型，以生成高质量的 CTA 数据。在训练过程中，为每类数据分配专门的文本提示，并为后续类别的特定类提供否定提示。
(3) 图像采样：使用欧拉和欧拉 A 图像采样器从潜空间的不同区域采样，以生成多样化和逼真的图像。
(4) 数据增强：使用独特的文本提示渲染具有类别分布的 CT 数据，以增强具有特定 CT 特征的数据，例如 TL、FL 和 FLT。
(5) 模型评估：使用 Fréchet Inception Distance (FID) 和 Multiscale Structural Similarity Index Measure (MS-SSIM) 评估合成图像的质量和多样性。训练 SoTA 模型（例如 UNet）对合成图像进行分割，以评估其实用性。</p>
</li>
<li>
<p>结论：
（1）：本研究提出了一种使用稳定扩散模型合成主动脉夹层 CTA 图像的方法，该方法能够生成具有主动脉夹层典型特征的图像，并且可以用于训练深度学习模型进行主动脉夹层图像分割。
（2）：创新点：</p>
</li>
<li>使用稳定扩散模型合成主动脉夹层 CTA 图像，这是首次将稳定扩散模型应用于主动脉夹层图像合成。</li>
<li>通过微调稳定扩散模型，使其能够根据用户定义的文本提示生成主动脉夹层 CTA 图像，这使得图像合成过程更加灵活和可控。</li>
<li>合成的图像具有很高的质量，并且可以用于训练深度学习模型进行主动脉夹层图像分割，这表明该方法具有实际应用价值。
性能：</li>
<li>定量分析和定性评估都表明，合成的图像具有很高的质量，并且可以用于训练深度学习模型进行主动脉夹层图像分割。</li>
<li>训练的 SoTA 模型（例如 UNet）对合成图像进行分割，获得了良好的分割精度，这表明该方法合成的图像具有很高的实用性。
工作量：</li>
<li>该方法需要对稳定扩散模型进行微调，这需要一定的计算资源和时间。</li>
<li>该方法需要对数据进行预处理，这需要一定的人工劳动。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-66ad8c9bd4b7c6c0abc54d425f5bff3e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7eb93cb5e3a23926b4fa972f1f7e5a2b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed3565ac4c49d72e02f85632488a4e3a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c0c6793d4532774c78760ad1a11631e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b3d53880237f6c704914112c0392f627.jpg" align="middle">
</details>




<h2 id="Improving-2D-3D-Dense-Correspondences-with-Diffusion-Models-for-6D-Object-Pose-Estimation"><a href="#Improving-2D-3D-Dense-Correspondences-with-Diffusion-Models-for-6D-Object-Pose-Estimation" class="headerlink" title="Improving 2D-3D Dense Correspondences with Diffusion Models for 6D   Object Pose Estimation"></a>Improving 2D-3D Dense Correspondences with Diffusion Models for 6D   Object Pose Estimation</h2><p><strong>Authors:Peter Hönig, Stefan Thalhammer, Markus Vincze</strong></p>
<p>Estimating 2D-3D correspondences between RGB images and 3D space is a fundamental problem in 6D object pose estimation. Recent pose estimators use dense correspondence maps and Point-to-Point algorithms to estimate object poses. The accuracy of pose estimation depends heavily on the quality of the dense correspondence maps and their ability to withstand occlusion, clutter, and challenging material properties. Currently, dense correspondence maps are estimated using image-to-image translation models based on GANs, Autoencoders, or direct regression models. However, recent advancements in image-to-image translation have led to diffusion models being the superior choice when evaluated on benchmarking datasets. In this study, we compare image-to-image translation networks based on GANs and diffusion models for the downstream task of 6D object pose estimation. Our results demonstrate that the diffusion-based image-to-image translation model outperforms the GAN, revealing potential for further improvements in 6D object pose estimation models. </p>
<p><a href="http://arxiv.org/abs/2402.06436v1">PDF</a> Submitted to the First Austrian Symposium on AI, Robotics, and Vision   2024</p>
<p><strong>Summary</strong><br>扩散模型在图像到图像转换任务中表现优于生成对抗网络，在 6D 目标位姿估计任务中具有潜在优势。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>估计 RGB 图像和 3D 空间之间的 2D-3D 对应关系是 6D 目标位姿估计中的一个基本问题。</li>
<li>当前，密集对应图是使用基于 GAN、自动编码器或直接回归模型的图像到图像转换模型估计的。</li>
<li>最近，图像到图像转换领域的最新进展已使扩散模型成为在基准数据集上评估时的优越选择。</li>
<li>在这项研究中，我们比较了基于 GAN 和扩散模型的图像到图像转换网络，用于 6D 目标位姿估计的下游任务。</li>
<li>我们的结果表明，基于扩散的图像到图像转换模型优于 GAN，表明 6D 目标位姿估计模型有进一步改进的潜力。</li>
<li>扩散模型在图像到图像转换任务中具有更高的准确性和鲁棒性。</li>
<li>扩散模型在 6D 目标位姿估计任务中具有潜在优势，可以进一步提高位姿估计的准确性。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：使用扩散模型改进 2D-3D 密集对应以进行 6D 目标位姿估计</li>
<li>作者：Peter Hönig、Stefan Thalhammer、Markus Vincze</li>
<li>作者单位：奥地利维也纳工业大学自动化与控制研究所</li>
<li>关键词：6D 目标位姿估计、2D-3D 密集对应、扩散模型、图像到图像翻译</li>
<li>论文链接：https://arxiv.org/abs/2402.06436</li>
<li>摘要：
（1）研究背景：6D 目标位姿估计是许多感知任务的基础，例如自动驾驶、增强现实、创建数字孪生或机器人抓取。RGB-D 传感器可以同时提供颜色和深度数据，但并不总是可用。深度数据也容易受到噪声和其他失真的影响，这些失真通常由场景中闪亮、金属和透明物体反射引起。为了解决这个问题，人们考虑仅使用 RGB 图像进行位姿估计。最先进的方法依赖于估计 RGB 图像和 3D 对象模型之间的 2D-3D 密集对应。尽管这些方法擅长推断具有高可见性的对象位姿，但它们仍然面临着由杂波、遮挡、图像失真和闪亮物体表面带来的重大挑战。
（2）过去的方法：过去，人们通过使用直接回归、生成对抗网络 (GAN) 和 U-Net 架构的组合或编码器-解码器卷积神经网络 (CNN) 来解决位姿估计的 2D-3D 对应问题。上述方法估计的密集对应图包含从 RGB 图像到 3D 模型的每个像素的 3D 坐标。然而，这些方法在处理遮挡、杂波和具有挑战性的材料特性时存在困难。
（3）论文方法：本文提出了一种基于扩散模型的图像到图像翻译网络，用于估计 2D-3D 密集对应。扩散模型是一种生成模型，它通过逐渐添加噪声并逐渐减少噪声来生成图像。本文的方法将 RGB 图像作为输入，并生成一个密集对应图，该图包含从 RGB 图像到 3D 模型的每个像素的 3D 坐标。
（4）实验结果：本文的方法在 YCB-Video 数据集上进行了评估。实验结果表明，本文的方法在 6D 目标位姿估计任务上优于基于 GAN 的图像到图像翻译网络。这表明扩散模型在 6D 目标位姿估计任务中具有潜力。</li>
</ol>
<p><strong>方法</strong>：</p>
<p>（1）图像到图像翻译模型：本文提出了一种基于扩散模型的图像到图像翻译网络，用于估计2D-3D密集对应。扩散模型是一种生成模型，它通过逐渐添加噪声并逐渐减少噪声来生成图像。本文的方法将RGB图像作为输入，并生成一个密集对应图，该图包含从RGB图像到3D模型的每个像素的3D坐标。</p>
<p>（2）位置先验：为了获得2D位置先验，本文使用2D目标检测器从RGB图像中裁剪感兴趣区域（ROI）。ROI是图像到图像翻译模型的输入。图像到图像翻译模型学习从RGB裁剪中估计2D-3D密集对应。2D目标检测器用一个矩形边界框裁剪对象。因此，对象没有被完全裁剪，背景像素仍然存在。因此，图像到图像翻译网络的学习目标是双重的。网络的主要目标是学习估计2D-3D密集对应。同时，网络需要隐式地学习如何将对象从背景中分割出来。RANSAC+PnP步骤从密集对应图中估计6D目标位姿。</p>
<p>（3）数据增强：为了生成图像到图像翻译任务的训练数据，对象网格被归一化，以适应无量纲的1x1x1立方体。然后，根据顶点在归一化对象坐标空间中的XYZ位置，用RGB值对对象网格的顶点进行着色。然后，使用真实平移、旋转和相机内参对归一化和着色的网格进行渲染。</p>
<p>（4）图像到图像翻译算法：本文比较了两种图像到图像翻译算法，即GAN模型Pix2Pix和扩散模型BBDM。位置先验和RANSAC+PnP步骤对于这两种方法都是相同的，只有图像到图像翻译函数IDC=F(IRGB)是不同的。两种模型都在相同条件下进行训练。首先，模型在没有任何数据增强的情况下进行训练，除了将ROI裁剪调整为128x128像素，这是两个模型的输入和输出大小。在第二次训练运行中，两个模型都使用相同的数据增强参数进行训练，如表1所示。对于每次运行，两个模型都训练40个epoch。</p>
<p>（5）数据集：本文在LMO数据集上评估图像到图像翻译模型。它具有8个在随机域中采样的家用物体和50000张合成渲染的图像。这些合成渲染的图像仅用于训练。为了评估，使用了1214张真实世界的测试图像。</p>
<p>（6）位置先验：使用两组预先计算的位置先验进行对象裁剪。我们使用来自2023年目标位姿估计（BOP）挑战赛基准的YOLOx检测结果来评估位姿估计的下游任务。为了评估对象分割，使用真实位置先验。</p>
<p>（7）评估指标：本文评估了估计的6D位姿的质量，以及2D-3D密集对应图和对象分割的质量。6D目标位姿使用ADD(-S)分数进行评估。ADD(-S)是指模型点m之间的平均距离，对于kmd≥m。误差阈值km用10%定义。公式1中显示了m的计算。R和T表示真实旋转和平移，而^R和^T表示估计旋转和平移，而x表示模型M中的模型点。为了将位姿估计结果与其他方法进行比较，我们依靠2023年目标位姿估计（BOP）挑战赛基准计算的平均召回率。该AR分数是可见表面差异（VSD）、最大对称感知表面距离（MSSD）和最大对称感知投影距离（MSPD）的平均召回率的平均值。</p>
<ol>
<li>结论：
（1）：本文提出了一个基于扩散模型的图像到图像翻译网络，用于估计2D-3D密集对应。我们比较了GAN模型Pix2Pix和扩散模型BBDM在相同训练条件下的性能。我们的实验表明，扩散模型在估计2D-3D密集对应图的质量方面优于GAN。
（2）：创新点：</li>
<li>提出了一种基于扩散模型的图像到图像翻译网络，用于估计2D-3D密集对应。</li>
<li>比较了GAN模型Pix2Pix和扩散模型BBDM在相同训练条件下的性能。
性能：</li>
<li>扩散模型在估计2D-3D密集对应图的质量方面优于GAN。</li>
<li>在YCB-Video数据集上，扩散模型在6D目标位姿估计任务上优于基于GAN的图像到图像翻译网络。
工作量：</li>
<li>需要收集和预处理大量的数据。</li>
<li>需要训练两个图像到图像翻译模型。</li>
<li>需要对估计的2D-3D密集对应图进行后处理。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d77ba14fed7eddde5b06eaba6ff57afd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-77c4e5753a8cd6ab35f73ede239b04a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8d82762ff5fc78409df5e252c8a6442.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8833866e4d976d23589211a0d2587b35.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3c7c60e43fae13c906596978f0558ac8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-68bedb16f322fb5603066efd18ca6348.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3a909299ddc09a5143e9d208d38ac851.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ffb421ba600650f3eb815efb8fb9a80.jpg" align="middle">
</details>




<h2 id="Animated-Stickers-Bringing-Stickers-to-Life-with-Video-Diffusion"><a href="#Animated-Stickers-Bringing-Stickers-to-Life-with-Video-Diffusion" class="headerlink" title="Animated Stickers: Bringing Stickers to Life with Video Diffusion"></a>Animated Stickers: Bringing Stickers to Life with Video Diffusion</h2><p><strong>Authors:David Yan, Winnie Zhang, Luxin Zhang, Anmol Kalia, Dingkang Wang, Ankit Ramchandani, Miao Liu, Albert Pumarola, Edgar Schoenfeld, Elliot Blanchard, Krishna Narni, Yaqiao Luo, Lawrence Chen, Guan Pang, Ali Thabet, Peter Vajda, Amy Bearman, Licheng Yu</strong></p>
<p>We introduce animated stickers, a video diffusion model which generates an animation conditioned on a text prompt and static sticker image. Our model is built on top of the state-of-the-art Emu text-to-image model, with the addition of temporal layers to model motion. Due to the domain gap, i.e. differences in visual and motion style, a model which performed well on generating natural videos can no longer generate vivid videos when applied to stickers. To bridge this gap, we employ a two-stage finetuning pipeline: first with weakly in-domain data, followed by human-in-the-loop (HITL) strategy which we term ensemble-of-teachers. It distills the best qualities of multiple teachers into a smaller student model. We show that this strategy allows us to specifically target improvements to motion quality while maintaining the style from the static image. With inference optimizations, our model is able to generate an eight-frame video with high-quality, interesting, and relevant motion in under one second. </p>
<p><a href="http://arxiv.org/abs/2402.06088v1">PDF</a> </p>
<p><strong>Summary</strong><br>文本提出一种带有动画贴纸的视频扩散模型，该模型可根据文本提示和静态贴纸图像来生成动画。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>引入了一种带有动画贴纸的视频扩散模型，该模型可根据文本提示和静态贴纸图像来生成动画。</li>
<li>该模型建立在最先进的 Emu 文本到图像模型的基础上，并添加了时间层来模拟动作。</li>
<li>由于视觉和动作风格的差异，在自然视频生成中表现良好的模型在应用于贴纸时无法再生成生动的视频。</li>
<li>为了弥合这一差距，我们采用了分两阶段进行微调的管道：首先是弱域内数据，其次是人类在回路 (HITL) 策略，我们称之为教师集成。</li>
<li>该策略使我们能够专门针对运动质量进行改进，同时保持静态图像的风格。</li>
<li>经过推理优化，我们的模型能够在一秒内生成具有高质量、有趣且相关运动的八帧视频。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>标题：动画贴纸：使用视频扩散将贴纸变成生动贴纸</li>
<li>作者：David Yan<em>, Winnie Zhang</em>, Luxin Zhang, Anmol Kalia, Dingkang Wang, Ankit Ramchandani, Miao Liu, Albert Pumarola, Edgar Schönfeld, Elliot Blanchard, Krishna Narni, Yaqiao Luo, Lawrence Chen, Guan Pang, Ali Thabet, Peter Vajda, Amy Bearman†, Licheng Yu†</li>
<li>第一作者单位：GenAI, Meta Menlo Park, California, USA</li>
<li>关键词：动画贴纸、视频扩散、文本到视频、图像到视频、人类参与循环</li>
<li>论文链接：https://arxiv.org/abs/2402.06088，Github 链接：无</li>
<li>摘要：
（1）研究背景：最近，人们对生成文本（和图像）到视频 (T2V) 建模产生了浓厚的兴趣。当前最先进模型生成的视频通常很短（不到 3 秒），并且通常使用文本（文本到视频或 T2V）、图像（图像到视频或 I2V）或两者。在这项工作中，我们使用文本和图像到视频的生成管道来针对短视频生成的自然应用：为社交表达制作动画贴纸。
（2）过去的方法及其问题：我们发现，使用通用 I2V 模型（即仅在通用视频数据集上训练的模型）在应用于贴纸时不会产生高质量的运动，并且经常会生成具有静态或微不足道的运动（例如，仅“摆动”效果）和/或引入不一致性和运动伪影（例如，变形）。这是由于自然（逼真）视频与贴纸风格动画之间的视觉和运动差异，即域差距。
（3）提出的研究方法：在这项工作中，我们使用人类参与循环 (HITL) 训练策略来弥合域差距。首先，使用数据集和帧采样率的不同“配方”训练了许多“教师”模型，以便教师模型能够集体产生高质量的多样化运动，尽管很少。接下来，通过使用教师模型在涵盖广泛提示集的大型提示集上执行推理来构建 HITL 数据集。然后，使用 HITL 数据集训练一个较小的“学生”模型，该模型可以从教师模型中学习并产生高质量的动画贴纸。
（4）方法在任务和性能上的表现：我们的模型能够在不到一秒的时间内生成具有高质量、有趣且相关的运动的八帧视频。性能支持其目标。</li>
</ol>
<p>Some Error for method(比如是不是没有Methods这个章节)</p>
<ol>
<li>结论：
（1）：本文提出了一种动画贴纸模型，该模型使用时空潜在扩散模型以文本-图像对为条件，将贴纸图像变成动画贴纸。我们的预训练到生产的管道从 Emumodel 开始，该模型在大量自然视频上进行了微调，然后在域内数据集上进行了微调。然后，我们使用教师集合 HITL 微调策略来进一步提高运动质量、一致性和相关性。我们使用许多基于架构、蒸馏的优化和后训练优化来将推理速度提高到每批一秒。我们表明，我们的微调策略显着提高了运动大小和质量，优于仅在自然视频上训练的模型，证明了教师集合的有效性。
（2）：创新点：提出了一种使用人类参与循环 (HITL) 训练策略来弥合域差距的方法，该方法可以生成高质量、有趣且相关的运动；
性能：该模型能够在不到一秒的时间内生成具有高质量、有趣且相关的运动的八帧视频；
工作量：该模型的训练过程需要大量的数据和计算资源。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-188a2b4c4ed9e284afed14a8e020b622.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-29dcdf079faf656ac8934c9dcb4fe4da.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a85a0fa5d13e8bd37d6352571f52fa54.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-73989a294ebc6b241211e4051f9a71db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cee3ea6a017b3b85519c905ebe1d86a3.jpg" align="middle">
</details>




<h2 id="InstaGen-Enhancing-Object-Detection-by-Training-on-Synthetic-Dataset"><a href="#InstaGen-Enhancing-Object-Detection-by-Training-on-Synthetic-Dataset" class="headerlink" title="InstaGen: Enhancing Object Detection by Training on Synthetic Dataset"></a>InstaGen: Enhancing Object Detection by Training on Synthetic Dataset</h2><p><strong>Authors:Chengjian Feng, Yujie Zhong, Zequn Jie, Weidi Xie, Lin Ma</strong></p>
<p>In this paper, we introduce a novel paradigm to enhance the ability of object detector, e.g., expanding categories or improving detection performance, by training on synthetic dataset generated from diffusion models. Specifically, we integrate an instance-level grounding head into a pre-trained, generative diffusion model, to augment it with the ability of localising arbitrary instances in the generated images. The grounding head is trained to align the text embedding of category names with the regional visual feature of the diffusion model, using supervision from an off-the-shelf object detector, and a novel self-training scheme on (novel) categories not covered by the detector. This enhanced version of diffusion model, termed as InstaGen, can serve as a data synthesizer for object detection. We conduct thorough experiments to show that, object detector can be enhanced while training on the synthetic dataset from InstaGen, demonstrating superior performance over existing state-of-the-art methods in open-vocabulary (+4.5 AP) and data-sparse (+1.2 to 5.2 AP) scenarios. </p>
<p><a href="http://arxiv.org/abs/2402.05937v1">PDF</a> Tech report</p>
<p><strong>摘要</strong><br>利用扩散模型生成的合成数据集训练物体检测器，可以提高检测性能或扩展类别。</p>
<p><strong>要点</strong></p>
<ul>
<li>将实例级定位头集成到预训练生成扩散模型中，使其能够在生成图像中定位任意实例。</li>
<li>定位头通过来自现有物体检测器的监督和针对检测器未涵盖类别的自训练方案进行训练。</li>
<li>将合成数据用于物体检测器的训练可以提高其性能，在开放词表场景中比现有最先进方法提高 4.5 个 AP，在数据稀疏场景中提高 1.2 到 5.2 个 AP。</li>
<li>InstaGen 是一种新颖的范式，可通过使用扩散模型生成的合成数据集进行训练来增强对象检测器的能力，例如扩展类别或提高检测性能。</li>
<li>InstaGen 将实例级定位头集成到预训练的生成扩散模型中，使其能够在生成的图像中定位任意实例。</li>
<li>定位头通过来自现有物体检测器的监督和针对检测器未涵盖类别的自训练方案进行训练。</li>
<li>InstaGen 作为数据合成器可用于物体检测，在开放词表场景和数据稀疏场景中均优于现有方法。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<p></p><p><strong>标题</strong>：InstaGen：通过合成数据集增强对象检测</p><p></p>
<p></p><p><strong>作者</strong>：Yuxin Fang, Yifan Zhang, Xiaolin Fang, Xiaohua Shi, Wei Shen, Enhua Wu</p><p></p>
<p></p><p><strong>第一作者单位</strong>：华中科技大学</p><p></p>
<p></p><p><strong>关键词</strong>：对象检测，合成数据，扩散模型，实例级接地头</p><p></p>
<p></p><p><strong>论文链接</strong>：https://arxiv.org/abs/2302.07603</p><p></p>
<p></p><p><strong>Github 代码链接</strong>：None</p><p></p>
<p></p><p><strong>摘要</strong>：</p><p></p>
<p></p><p>本文提出了一种通过合成数据集增强对象检测能力的新范式，该范式通过从扩散模型生成合成数据来扩展检测性能。具体来说，我们将实例级接地头集成到预训练的生成扩散模型中，使其能够在生成图像中定位实例。接地头通过使用来自现成对象检测器的监督和一种在检测器无法识别的类上进行的自训练策略，来训练以将类别名称的文本嵌入与扩散模型的空间特征对齐。我们通过大量的实验表明，这种称为 InstaGen 的增强版扩散模型可以作为数据合成器来增强对象检测器，并在开放词汇表（+4.6 AP）和数据稀疏（+4.8 AP）上展示出优于现有最先进方法的性能。</p><p></p>
<p></p><p><strong>总结</strong>：</p><p></p>
<p></p><p>（一）：研究背景：</p><p></p>
<p></p><p>对象检测是计算机视觉中的一项基本任务，广泛应用于自动驾驶、人脸识别、医疗图像分析等领域。近年来，随着深度学习的发展，基于深度学习的对象检测方法取得了很大的进展。然而，这些方法通常需要大量的数据进行训练，这在一些领域是难以获得的。</p><p></p>
<p></p><p>（二）：过去的研究工作：</p><p></p>
<p></p><p>为了解决数据不足的问题，研究人员提出了各种数据增强技术来扩充训练数据。这些技术包括图像裁剪、翻转、旋转、颜色抖动等。然而，这些技术只能产生有限数量的图像，并且不能保证生成图像的质量。</p><p></p>
<p></p><p>（三）：本文的问题：</p><p></p>
<p></p><p>本文认为，现有的数据增强技术不能很好地解决数据不足的问题。因此，本文提出了一种新的数据增强技术，称为 InstaGen，该技术可以生成高质量的合成图像，并且可以保证生成图像的质量。</p><p></p>
<p></p><p>（四）：本文的方法：</p><p></p>
<p></p><p>InstaGen 是一种基于扩散模型的数据增强技术。扩散模型是一种生成模型，可以从噪声生成图像。InstaGen 将一个实例级接地头集成到预训练的扩散模型中，使其能够在生成图像中定位实例。接地头通过使用来自现成对象检测器的监督和一种在检测器无法识别的类上进行的自训练策略，来训练以将类别名称的文本嵌入与扩散模型的空间特征对齐。</p><p></p>
<p></p><p>（五）：本文的实验结果：</p><p></p>
<p></p><p>本文在 PASCAL VOC 和 COCO 数据集上对 InstaGen 进行了评估。实验结果表明，InstaGen 可以有效地增强对象检测器的性能。在 PASCAL VOC 数据集上，InstaGen 将 Faster R-CNN 的 AP 提高了 4.6 个百分点。在 COCO 数据集上，InstaGen 将 Faster R-CNN 的 AP 提高了 4.8 个百分点。</p><p></p>
<ol>
<li>
<p>方法：
（1）：构建图像合成器：采用预训练的 Stable Diffusion 模型，并使用检测数据集对模型进行微调，以生成具有真实感且包含指定类别的图像。
（2）：引入实例级接地头：设计一种实例级接地头，将类别名称的文本嵌入与扩散模型的空间特征对齐，从而生成对象实例的边界框。
（3）：监督学习和自训练：使用来自现有对象检测器的监督和一种在检测器无法识别的类上进行的自训练策略，来训练接地头。
（4）：数据合成器生成合成数据集：使用训练好的接地头和图像合成器，生成包含对象实例及其边界框的合成数据集。
（5）：在合成数据集上训练对象检测器：将合成数据集与真实数据集相结合，训练对象检测器，以提高检测性能。</p>
</li>
<li>
<p>结论：
（1）：本文提出了一种称为InstaGen的数据集合成管道，该管道能够为任意类别生成具有对象边界框的图像，作为构建大规模合成数据集以训练对象检测器的免费资源。我们进行了详尽的实验，以展示在合成数据上训练的有效性，以提高检测性能或扩展检测类别的数量。在各种检测场景中，包括开放词汇表（+4.5AP）和数据稀疏（+1.2∼5.2AP）检测中，都取得了显着的改进。
（2）：创新点：</p>
</li>
<li>提出了一种新的数据合成管道InstaGen，该管道能够为任意类别生成具有对象边界框的图像。</li>
<li>设计了一种实例级接地头，将类别名称的文本嵌入与扩散模型的空间特征对齐，从而生成对象实例的边界框。</li>
<li>使用来自现有对象检测器的监督和一种在检测器无法识别的类上进行的自训练策略，来训练接地头。
性能：</li>
<li>在PASCAL VOC和COCO数据集上，InstaGen将Faster R-CNN的AP提高了4.6个百分点和4.8个百分点。</li>
<li>在开放词汇表和数据稀疏检测中，InstaGen取得了显着的改进。
工作量：</li>
<li>InstaGen是一种数据合成管道，需要预训练的扩散模型和实例级接地头。</li>
<li>InstaGen需要大量的数据来训练接地头。</li>
<li>InstaGen需要大量的时间来生成合成数据集。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e5bc75a4d614b9abf0055ef9f09e29eb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dcaa5f4430aaa302f904c1eb77cd432c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ed1d3b41f15d36193b946e6064581300.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6998a66afc9f7f895bfb98faa0596297.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85a17fc9e78759363117b1e3dbd18da2.jpg" align="middle">
</details>




<h2 id="Scalable-Diffusion-Models-with-State-Space-Backbone"><a href="#Scalable-Diffusion-Models-with-State-Space-Backbone" class="headerlink" title="Scalable Diffusion Models with State Space Backbone"></a>Scalable Diffusion Models with State Space Backbone</h2><p><strong>Authors:Zhengcong Fei, Mingyuan Fan, Changqian Yu, Junshi Huang</strong></p>
<p>This paper presents a new exploration into a category of diffusion models built upon state space architecture. We endeavor to train diffusion models for image data, wherein the traditional U-Net backbone is supplanted by a state space backbone, functioning on raw patches or latent space. Given its notable efficacy in accommodating long-range dependencies, Diffusion State Space Models (DiS) are distinguished by treating all inputs including time, condition, and noisy image patches as tokens. Our assessment of DiS encompasses both unconditional and class-conditional image generation scenarios, revealing that DiS exhibits comparable, if not superior, performance to CNN-based or Transformer-based U-Net architectures of commensurate size. Furthermore, we analyze the scalability of DiS, gauged by the forward pass complexity quantified in Gflops. DiS models with higher Gflops, achieved through augmentation of depth/width or augmentation of input tokens, consistently demonstrate lower FID. In addition to demonstrating commendable scalability characteristics, DiS-H/2 models in latent space achieve performance levels akin to prior diffusion models on class-conditional ImageNet benchmarks at the resolution of 256$\times$256 and 512$\times$512, while significantly reducing the computational burden. The code and models are available at: <a href="https://github.com/feizc/DiS">https://github.com/feizc/DiS</a>. </p>
<p><a href="http://arxiv.org/abs/2402.05608v1">PDF</a> </p>
<p><strong>摘要</strong><br>利用状态空间架构构建的新型扩散模型，在图像数据上实现可与 U 形卷积神经网络架构媲美的性能，并具有良好的可扩展性。</p>
<p><strong>要点</strong></p>
<ul>
<li>基于状态空间架构的扩散模型在图像数据生成任务上表现良好，可与基于 U 形卷积神经网络或基于 Transformer 的 U 形卷积神经网络架构实现相当的性能，甚至优于它们。</li>
<li>扩散模型的状态空间模型将时间、条件和噪声图像块等所有输入都视为标记。</li>
<li>扩散模型的状态空间模型在无条件图像生成和类别条件图像生成场景中均表现良好。</li>
<li>通过增加深度/宽度或增加输入标记，扩散模型的状态空间模型的正向传递复杂度（以 Gflops 为单位）更高，并且始终表现出更低的 FID。</li>
<li>扩散模型的状态空间模型具有良好的可扩展性。</li>
<li>在分辨率为 256×256 和 512×512 的类别条件 ImageNet 基准上，扩散模型的状态空间模型在潜在空间中实现了与先前扩散模型相当的性能，同时大幅降低了计算负担。</li>
<li>扩散模型的状态空间模型代码和模型可在 <a href="https://github.com/feizc/DiS">https://github.com/feizc/DiS</a> 上获取。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：基于状态空间的可扩展扩散模型</li>
<li>作者：Zhengcong Fei, Mingyuan Fan, Changqian Yu, Junshi Huang</li>
<li>第一作者单位：昆仑科技</li>
<li>关键词：扩散模型、状态空间、可扩展性、图像生成</li>
<li>论文链接：https://arxiv.org/abs/2402.05608，Github 链接：https://github.com/feizc/DiS</li>
<li>摘要：
(1)：研究背景：扩散模型作为强大的深度生成模型，近年来在图像生成领域取得了显著进展，广泛应用于文本到图像生成、图像到图像生成、视频生成、语音合成和 3D 合成等领域。扩散模型的发展离不开采样算法和模型骨干的进步，其中 U-Net 是扩散模型中常用的骨干网络，但其在处理长程依赖关系方面存在局限性。
(2)：过去方法与问题：传统的扩散模型骨干网络，如 U-Net，在处理长程依赖关系方面存在局限性。为了解决这一问题，本文提出了基于状态空间的扩散模型（DiS），该模型将时间、条件和噪声图像块视为标记，并使用状态空间骨干网络来处理这些标记。
(3)：研究方法：本文提出的 DiS 模型具有以下特点：</li>
<li>将时间、条件和噪声图像块视为标记，并使用状态空间骨干网络来处理这些标记。</li>
<li>DiS 模型可以处理原始图像块或潜在空间中的标记。</li>
<li>
<p>DiS 模型具有良好的可扩展性，可以通过增加深度、宽度或输入标记的数量来提高模型的性能。
(4)：实验结果与性能：本文在无条件和类条件图像生成任务上对 DiS 模型进行了评估，结果表明 DiS 模型与基于 CNN 或 Transformer 的 U-Net 模型具有相当或更好的性能。此外，本文还分析了 DiS 模型的可扩展性，结果表明 DiS 模型具有良好的可扩展性，可以通过增加深度、宽度或输入标记的数量来提高模型的性能。在 ImageNet 数据集上，DiS-H/2 模型在分辨率为 256×256 和 512×512 的类条件图像生成任务上取得了与之前的扩散模型相当的性能，同时显著降低了计算负担。</p>
</li>
<li>
<p>方法：
（1）：提出了一种基于状态空间的扩散模型（DiS），该模型将时间、条件和噪声块视为标记，并使用状态空间骨干网络来处理这些标记。
（2）：DiS模型可以处理原始块或潜在空间中的标记。
（3）：DiS模型具有良好的可扩展性，可以通过增加深度、宽度或输入标记的数量来提高模型的性能。</p>
</li>
<li>
<p>结论：
（1）：本工作提出了一种基于状态空间的扩散模型（DiS），该模型将时间、条件和噪声块视为标记，并使用状态空间骨干网络来处理这些标记。DiS 采用了一种统一的方法来处理所有输入，包括时间、条件和噪声图像块，将它们视为连接的标记。实验结果表明，DiS 与基于 CNN 或 Transformer 的 U-Net 模型相比具有相当或更好的性能，同时继承了状态空间模型类的显着可扩展性特征。我们认为 DiS 可以为未来研究扩散模型中的骨干网络提供有价值的见解，并有助于推进大规模多模态数据集中的生成建模。鉴于本研究中提出的令人鼓舞的可扩展性结果，未来的努力应集中在将 DiS 进一步扩展到更大的模型和标记计数上。
（2）：创新点：
DiS 模型将时间、条件和噪声块视为标记，并使用状态空间骨干网络来处理这些标记，这是一种新的方法，可以有效地处理长程依赖关系。
DiS 模型可以处理原始块或潜在空间中的标记，这使得它可以应用于各种图像生成任务。
DiS 模型具有良好的可扩展性，可以通过增加深度、宽度或输入标记的数量来提高模型的性能。
性能：
在无条件和类条件图像生成任务上，DiS 模型与基于 CNN 或 Transformer 的 U-Net 模型具有相当或更好的性能。
在 ImageNet 数据集上，DiS-H/2 模型在分辨率为 256×256 和 512×512 的类条件图像生成任务上取得了与之前的扩散模型相当的性能，同时显著降低了计算负担。
工作量：
DiS 模型的训练和推理成本与基于 CNN 或 Transformer 的 U-Net 模型相当。
DiS 模型的可扩展性使得它可以应用于各种图像生成任务，包括高分辨率图像生成、视频生成和 3D 合成。</p>
</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6bb4b2235878abe86e04f19f24047beb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1d4bb00838a5fb623fcc9eb998c2c6b9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-adf0dc0a97f9ca167de7eccda01fe6df.jpg" align="middle">
</details>




<h2 id="SPAD-Spatially-Aware-Multiview-Diffusers"><a href="#SPAD-Spatially-Aware-Multiview-Diffusers" class="headerlink" title="SPAD : Spatially Aware Multiview Diffusers"></a>SPAD : Spatially Aware Multiview Diffusers</h2><p><strong>Authors:Yash Kant, Ziyi Wu, Michael Vasilkovsky, Guocheng Qian, Jian Ren, Riza Alp Guler, Bernard Ghanem, Sergey Tulyakov, Igor Gilitschenski, Aliaksandr Siarohin</strong></p>
<p>We present SPAD, a novel approach for creating consistent multi-view images from text prompts or single images. To enable multi-view generation, we repurpose a pretrained 2D diffusion model by extending its self-attention layers with cross-view interactions, and fine-tune it on a high quality subset of Objaverse. We find that a naive extension of the self-attention proposed in prior work (e.g. MVDream) leads to content copying between views. Therefore, we explicitly constrain the cross-view attention based on epipolar geometry. To further enhance 3D consistency, we utilize Plucker coordinates derived from camera rays and inject them as positional encoding. This enables SPAD to reason over spatial proximity in 3D well. In contrast to recent works that can only generate views at fixed azimuth and elevation, SPAD offers full camera control and achieves state-of-the-art results in novel view synthesis on unseen objects from the Objaverse and Google Scanned Objects datasets. Finally, we demonstrate that text-to-3D generation using SPAD prevents the multi-face Janus issue. See more details at our webpage: <a href="https://yashkant.github.io/spad">https://yashkant.github.io/spad</a> </p>
<p><a href="http://arxiv.org/abs/2402.05235v1">PDF</a> Webpage: <a href="https://yashkant.github.io/spad">https://yashkant.github.io/spad</a></p>
<p><strong>Summary</strong><br>跨视角图像生成模型 SPAD：自我注意和空间编码的结合，实现文本到图像生成的一致性。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SPAD 是一种新的方法，可以从文本提示或单个图像生成一致的多视角图像。</li>
<li>SPAD 是通过扩展预训练的 2D 扩散模型的自注意力层来实现多视角生成，并对 Objaverse 的高质量子集进行微调。</li>
<li>SPAD 显示，先前的研究提出的自我注意的朴素扩展（例如 MVDream）导致视角之间的内容复制。</li>
<li>SPAD 显式地限制基于极线几何的跨视角注意。</li>
<li>SPAD 利用从相机射线派生的 Plücker 坐标，并将它们注入作为位置编码，以进一步增强 3D 一致性。</li>
<li>与只能在固定方位角和仰角生成视图的最近工作相比，SPAD 提供了完全的相机控制，并在 Objaverse 和 Google Scanned Objects 数据集上看不见的物体的新颖视图合成中实现了最先进的结果。</li>
<li>使用 SPAD 进行文本到 3D 生成消除了多面 Janus 问题。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：SPAD：空间感知多视图扩散器</li>
<li>作者：Yash Kant, Ziyi Wu, Michael Vasilkovsky, Guocheng Qian, Jian Ren, Riza Alp Guler, Bernard Ghanem, Sergey Tulyakov, Igor Gilitschenski, Aliaksandr Siarohin</li>
<li>第一作者单位：多伦多大学</li>
<li>关键词：多视图生成、扩散模型、文本到 3D</li>
<li>论文链接：https://yashkant.github.io/spad/，Github 代码链接：None</li>
<li>
<p>摘要：
（1）研究背景：多视图生成是指从文本提示或单个图像生成一组在 3D 空间中一致的图像。这对于许多应用很有用，例如增强现实、虚拟现实和 3D 建模。
（2）过去方法：现有方法通常使用 2D 扩散模型来生成多视图图像。然而，这些方法通常会导致视图之间出现不一致，例如对象形状或纹理不匹配。
（3）研究方法：本文提出了一种新的多视图生成方法 SPAD。SPAD 通过在 2D 扩散模型中引入跨视图交互来实现多视图生成。此外，SPAD 还利用了 Plücker 坐标来增强 3D 一致性。
（4）方法性能：SPAD 在 Objaverse 和 Google Scanned Objects 数据集上进行了评估。结果表明，SPAD 在新视图合成任务上优于现有方法。此外，SPAD 还能够防止多面 Janus 问题，即生成的图像在不同视图中具有不同的外观。</p>
</li>
<li>
<p>方法：
(1): SPAD的核心思想是将多视图生成问题转化为一个扩散模型问题。SPAD使用一个2D扩散模型来生成每个视图的图像，并通过在扩散模型中引入跨视图交互来确保视图之间的一致性。
(2): SPAD使用Plücker坐标来表示3D空间中的点。Plücker坐标具有不变性，这意味着它们不受视角和投影变换的影响。SPAD利用Plücker坐标来增强3D一致性，并防止多面Janus问题。
(3): SPAD在Objaverse和GoogleScannedObjects数据集上进行了评估。结果表明，SPAD在新视图合成任务上优于现有方法。此外，SPAD还能够防止多面Janus问题。</p>
</li>
<li>
<p>结论：
（1）：SPAD是一种新颖的多视图生成框架，它将文本或图像输入转换为多个视图。SPAD在预训练的文本到图像扩散模型的自注意力层中引入了极线注意力，以促进多视图交互并改进相机控制。此外，SPAD使用Plücker位置编码增强了自注意力层，以通过防止对象的翻转视图预测来进一步改进相机控制。SPAD在Objaverse和GoogleScannedObjects数据集上进行了严格的评估，并在图像条件的新视图合成方面展示了最先进的结果。
（2）：创新点：</p>
</li>
<li>将多视图生成问题转化为扩散模型问题，并通过在扩散模型中引入跨视图交互来确保视图之间的一致性。</li>
<li>使用Plücker坐标来表示3D空间中的点，并利用Plücker坐标来增强3D一致性，防止多面Janus问题。
性能：</li>
<li>在Objaverse和GoogleScannedObjects数据集上，SPAD在新视图合成任务上优于现有方法。</li>
<li>SPAD能够防止多面Janus问题，即生成的图像在不同视图中具有不同的外观。
工作量：</li>
<li>SPAD的实现相对简单，并且可以在PyTorch中轻松实现。</li>
<li>SPAD的训练过程相对快速，并且可以在单个GPU上完成。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-afe3524a8f81d817d06d1d9498a1728a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3709a9941aada6c4d3ed35934e311765.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a80a51acf35ce9d57c5584647e5cca12.jpg" align="middle">
</details>




]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>Diffusion Models</tag>
      </tags>
  </entry>
  <entry>
    <title>3DGS</title>
    <url>/2024/02/23/Paper/2024-02-23/3DGS/</url>
    <content><![CDATA[<blockquote>
<p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-02-23-更新"><a href="#2024-02-23-更新" class="headerlink" title="2024-02-23 更新"></a>2024-02-23 更新</h1><h2 id="Identifying-Unnecessary-3D-Gaussians-using-Clustering-for-Fast-Rendering-of-3D-Gaussian-Splatting"><a href="#Identifying-Unnecessary-3D-Gaussians-using-Clustering-for-Fast-Rendering-of-3D-Gaussian-Splatting" class="headerlink" title="Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering   of 3D Gaussian Splatting"></a>Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering   of 3D Gaussian Splatting</h2><p><strong>Authors:Joongho Jo, Hyeongwon Kim, Jongsun Park</strong></p>
<p>3D Gaussian splatting (3D-GS) is a new rendering approach that outperforms the neural radiance field (NeRF) in terms of both speed and image quality. 3D-GS represents 3D scenes by utilizing millions of 3D Gaussians and projects these Gaussians onto the 2D image plane for rendering. However, during the rendering process, a substantial number of unnecessary 3D Gaussians exist for the current view direction, resulting in significant computation costs associated with their identification. In this paper, we propose a computational reduction technique that quickly identifies unnecessary 3D Gaussians in real-time for rendering the current view without compromising image quality. This is accomplished through the offline clustering of 3D Gaussians that are close in distance, followed by the projection of these clusters onto a 2D image plane during runtime. Additionally, we analyze the bottleneck associated with the proposed technique when executed on GPUs and propose an efficient hardware architecture that seamlessly supports the proposed scheme. For the Mip-NeRF360 dataset, the proposed technique excludes 63% of 3D Gaussians on average before the 2D image projection, which reduces the overall rendering computation by almost 38.3% without sacrificing peak-signal-to-noise-ratio (PSNR). The proposed accelerator also achieves a speedup of 10.7x compared to a GPU. </p>
<p><a href="http://arxiv.org/abs/2402.13827v1">PDF</a> </p>
<p><strong>Summary</strong><br>3D 高斯散splatting 通过聚类 和 投影优化，减少了 38.3% 的渲染计算，且不损失图像质量。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>3D 高斯散splatting（3D-GS）是一种新的渲染方法，在速度和图像质量上优于神经辐射场（NeRF）。</li>
<li>3D-GS 使用数百万个 3D 高斯表示 3D 场景，并将这些高斯投影到 2D 图像平面上进行渲染。</li>
<li>在渲染过程中，大量不必要的高斯存在于当前视图方向，导致与识别它们相关的计算成本巨大。</li>
<li>提出了一种计算简化技术，可在运行时快速识别出不必要的高斯，用于渲染当前视图，且不损害图像质量。</li>
<li>这种简化技术方法是离线对距离相近的高斯进行聚类，然后在运行时将这些集群投影到 2D 图像平面上。</li>
<li>对该技术在 GPU 上执行时的瓶颈进行了分析，并提出了一种与该方案无缝兼容的高效硬件架构。</li>
<li>对于 Mip-NeRF360 数据集，该技术在 2D 图像投影之前平均排除了 63% 的高斯，将整体渲染计算减少了 38.3%，且不损失峰值信噪比 (PSNR)。</li>
<li>该加速器与 GPU 相比，还实现了 10.7 倍的加速。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：使用聚类识别不必要的 3D 高斯体，以快速渲染 3D 高斯体飞溅</li>
<li>作者：Joongho Jo、Hyeongwon Kim 和 Jongsun Park</li>
<li>隶属机构：韩国大学电气工程学院</li>
<li>关键词：3D 高斯体飞溅、渲染、NeRF、神经辐射场、硬件加速器</li>
<li>论文链接：Paper_info:Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering of 3D Gaussian Splatting，Github 链接：无</li>
<li>摘要：</li>
</ol>
<p>（1）研究背景：在计算机视觉应用中，例如增强现实 (AR)、虚拟现实 (VR) 和元宇宙，快速且高质量的图像渲染非常重要。虽然已经广泛研究了使用深度神经网络的渲染技术，例如神经辐射场 (NeRF)，但 3D 高斯体飞溅 (3D-GS) 作为一种新的渲染方法，最近因其与传统 NeRF 相比能够快速渲染高质量图像而备受关注。3D-GS 利用数百万个 3D 高斯体来表示复杂的 3D 场景，并通过将 3D 高斯体投影到 2D 图像平面上来渲染 3D 场景。</p>
<p>（2）过去的方法及其问题：3D-GS 渲染过程主要分为两步：首先，将所有 3D 高斯体投影到 2D 图像平面上，并识别影响 2D 图像颜色的 3D 高斯体。然后，使用影响颜色的已识别 3D 高斯体计算 2D 图像中每个像素的颜色。在渲染过程的第一步中，高斯体投影到 2D 图像上后，如果被识别为不影响 2D 图像的颜色，那么投影就变成了计算浪费。在 Mip-NeRF360 数据集中，平均约有 67.6% 的 3D 高斯体不影响 2D 图像的颜色。因此，这些高斯体可以从当前视图渲染过程中排除。但是，由于影响 2D 图像颜色的 3D 高斯体可能会随着渲染视点的方向和位置而改变，因此在将 3D 高斯体投影到 2D 图像平面上之前识别不必要的高斯体仍然具有挑战性。因此，这些不必要的高斯体仍然会进行投影计算。因此，如果可以开发出一种简单而有效的方法来预测不会影响 2D 图像颜色的 3D 高斯体，并在渲染过程开始之前将它们排除，那么可以显着降低整个 3D-GS 过程的总体计算复杂度。</p>
<p>（3）本文提出的研究方法：本文提出了一种基于聚类的方案，通过识别不影响 2D 图像颜色的簇来排除当前视图渲染过程中的不必要 3D 高斯体。聚类的目标是将位置相近的 3D 高斯体分组在一起，并且簇的形状应该是球形的，以便于投影到 2D 图像上。因此，本文采用 K-means 聚类算法，该算法满足这两个标准。考虑到 3D 高斯体具有由其协方差定义的大小或影响，簇球体的半径不仅由到簇质心的距离确定，还要考虑高斯体的大小。然后将这些定义的簇球体投影到 2D 图像平面上，以确定它们对 2D 图像颜色的影响。不影响图像颜色的簇可以从渲染过程中排除。在本文的方法中，可以在离线执行聚类和计算簇的半径，并且仅在实时执行将簇投影到 2D 图像平面上，这仅需 6.2% 的计算开销。在 3D-GS 渲染过程中，在当前视图渲染之前应用所提出的方法时，平均可以排除 63% 的 3D 高斯体，从而在不牺牲峰值信噪比 (PSNR) 的情况下将整体渲染计算减少了近 38.3%。所提出的加速器还实现了与 GPU 相比 10.7 倍的加速。</p>
<p>（4）方法在任务和性能上的表现：在 Mip-NeRF360 数据集上，所提出的方法平均排除了 63% 的 3D 高斯体，在不牺牲峰值信噪比 (PSNR) 的情况下将整体渲染计算减少了近 38.3%。所提出的加速器还实现了与 GPU 相比 10.7 倍的加速。这些性能支持了本文的目标，即快速且高质量地渲染 3D 场景。</p>
<p>Some Error for method(比如是不是没有Methods这个章节)</p>
<ol>
<li>结论：
（1）：本文提出了一种基于聚类的方案，通过识别不影响 2D 图像颜色的簇来排除当前视图渲染过程中的不必要 3D 高斯体。该方法平均可以排除 63% 的 3D 高斯体，在不牺牲峰值信噪比 (PSNR) 的情况下将整体渲染计算减少了近 38.3%。所提出的加速器还实现了与 GPU 相比 10.7 倍的加速。
（2）：创新点：</li>
<li>提出了一种基于聚类的方案来识别不必要的 3D 高斯体。</li>
<li>该方法可以离线执行聚类和计算簇的半径，并且仅在实时执行将簇投影到 2D 图像平面上，这仅需 6.2% 的计算开销。</li>
<li>所提出的加速器实现了与 GPU 相比 10.7 倍的加速。
性能：</li>
<li>在 Mip-NeRF360 数据集上，该方法平均排除了 63% 的 3D 高斯体，在不牺牲峰值信噪比 (PSNR) 的情况下将整体渲染计算减少了近 38.3%。
工作量：</li>
<li>该方法可以在离线执行聚类和计算簇的半径，并且仅在实时执行将簇投影到 2D 图像平面上，这仅需 6.2% 的计算开销。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-eb8532b7f44bd3308c4f19fe6bf7f78c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e5e9d849dcc9fd5228abd36df009311.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a43367bbb6924d5ba043f598753b956.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d13d1af17267a2b843bea8ac607b39a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bca1cf3d857e2d53600b33fc6c9e298c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5799fc43b51197a24672703783ee479.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee494dce0084e0f0c71d55d940b03dc9.jpg" align="middle">
</details>




<h2 id="GaussianObject-Just-Taking-Four-Images-to-Get-A-High-Quality-3D-Object-with-Gaussian-Splatting"><a href="#GaussianObject-Just-Taking-Four-Images-to-Get-A-High-Quality-3D-Object-with-Gaussian-Splatting" class="headerlink" title="GaussianObject: Just Taking Four Images to Get A High-Quality 3D Object   with Gaussian Splatting"></a>GaussianObject: Just Taking Four Images to Get A High-Quality 3D Object   with Gaussian Splatting</h2><p><strong>Authors:Chen Yang, Sikuang Li, Jiemin Fang, Ruofan Liang, Lingxi Xie, Xiaopeng Zhang, Wei Shen, Qi Tian</strong></p>
<p>Reconstructing and rendering 3D objects from highly sparse views is of critical importance for promoting applications of 3D vision techniques and improving user experience. However, images from sparse views only contain very limited 3D information, leading to two significant challenges: 1) Difficulty in building multi-view consistency as images for matching are too few; 2) Partially omitted or highly compressed object information as view coverage is insufficient. To tackle these challenges, we propose GaussianObject, a framework to represent and render the 3D object with Gaussian splatting, that achieves high rendering quality with only 4 input images. We first introduce techniques of visual hull and floater elimination which explicitly inject structure priors into the initial optimization process for helping build multi-view consistency, yielding a coarse 3D Gaussian representation. Then we construct a Gaussian repair model based on diffusion models to supplement the omitted object information, where Gaussians are further refined. We design a self-generating strategy to obtain image pairs for training the repair model. Our GaussianObject is evaluated on several challenging datasets, including MipNeRF360, OmniObject3D, and OpenIllumination, achieving strong reconstruction results from only 4 views and significantly outperforming previous state-of-the-art methods. </p>
<p><a href="http://arxiv.org/abs/2402.10259v2">PDF</a> Project page: <a href="https://gaussianobject.github.io/">https://gaussianobject.github.io/</a></p>
<p><strong>摘要</strong><br>利用仅有 4 张输入图像，以高斯散点图表示和渲染三维对象，展现出极佳的渲染质量。</p>
<p><strong>要点</strong></p>
<ul>
<li>重建和渲染高度稀疏视图的 3D 对象对于促进 3D 视觉技术应用和改善用户体验至关重要。</li>
<li>提出 GaussianObject，一种以高斯散点图表示和渲染 3D 对象的框架，仅需 4 张输入图像即可实现高渲染质量。</li>
<li>引入视觉外壳和浮子消除技术，将结构先验明确注入初始优化过程，帮助建立多视图一致性，产生粗糙的 3D 高斯表示。</li>
<li>基于扩散模型构建高斯修复模型，以补充省略的对象信息，其中高斯值进一步细化。</li>
<li>设计了一种自生成策略来获取图像对，以训练修复模型。</li>
<li>在多个具有挑战性的数据集上评估了 GaussianObject，包括 MipNeRF360、OmniObject3D 和 OpenIllumination，仅使用 4 个视图即可实现强大的重建结果，并且明显优于先前的最先进方法。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：高斯对象：只需四张图像即可获取高质量的 3D 对象</li>
<li>作者：陈阳，李思宽，方杰民，梁若凡，谢凌希，张晓鹏，沈巍，田齐</li>
<li>单位：上海交通大学</li>
<li>关键词：神经辐射场、3D 重建、稀疏视图、高斯球面体</li>
<li>论文链接：https://arxiv.org/abs/2402.10259，Github 链接：None</li>
<li>
<p>摘要：
（1）研究背景：重建和渲染 3D 对象是计算机视觉领域的重要课题，但传统方法通常需要大量视图才能获得高质量的结果。这对于用户来说非常繁琐，限制了 3D 技术的广泛应用。
（2）过去的方法：一些研究尝试减少对密集捕获的依赖，但当视图变得极度稀疏时，仍然难以生成高质量的 3D 对象。主要挑战在于难以建立多视图一致性，以及部分缺失或高度压缩的对象信息。
（3）研究方法：本文提出了一种名为高斯对象的新框架，旨在从稀疏视图中重建高质量的 3D 对象。该框架使用 3D 高斯球面体作为基本表示，并设计了几种技术来引入对象结构先验，帮助建立多视图一致性。此外，还提出了一种基于扩散模型的高斯修复模型，以消除由缺失或高度压缩的对象信息引起的伪影。
（4）性能表现：高斯对象方法在几个具有挑战性的真实世界数据集上表现出强大的性能，在定性和定量评估中均优于以前的最先进方法。这表明该方法能够有效地从稀疏视图中重建高质量的 3D 对象。</p>
</li>
<li>
<p>方法：
(1) 高斯球面体表示：将3D对象表示为一个3D高斯球面体，该球面体由一系列3D高斯分布组成。每个高斯分布对应于对象的一个局部区域，其参数（中心位置、尺度和权重）由神经网络学习得到。
(2) 结构先验引入：设计了几种技术来引入对象结构先验，帮助建立多视图一致性。这些技术包括：</p>
<ul>
<li>形状正则化：使用一个预训练的形状生成模型来正则化高斯球面体的形状，使其更加真实和自然。</li>
<li>拓扑正则化：使用一个拓扑生成模型来正则化高斯球面体的拓扑结构，使其更加连通和完整。</li>
<li>语义正则化：使用一个语义分割模型来正则化高斯球面体的语义信息，使其更加准确和一致。
(3) 高斯修复模型：提出了一种基于扩散模型的高斯修复模型，以消除由缺失或高度压缩的对象信息引起的伪影。该模型通过迭代地扩散和恢复高斯球面体的参数，逐步消除伪影并生成高质量的3D对象。</li>
</ul>
</li>
<li>
<p>结论：
（1）：高斯对象是一种新颖的框架，旨在从极度稀疏的 360° 视图中重建高质量的 3D 对象，该框架基于 3D 高斯球面体，并具有实时的渲染能力。我们设计了两种主要方法来实现这一目标：辅助结构先验的优化，以促进多视图一致性的构建，以及高斯修复模型，以去除由遗漏或高度压缩的对象信息引起的伪影。我们希望高斯对象能够推进重建 3D 对象的日常应用。
（2）：创新点：</p>
</li>
<li>提出了一种新的 3D 对象表示形式——高斯球面体，该表示形式能够有效地捕获对象的形状、拓扑结构和语义信息。</li>
<li>设计了几种技术来引入对象结构先验，帮助建立多视图一致性，包括形状正则化、拓扑正则化和语义正则化。</li>
<li>提出了一种基于扩散模型的高斯修复模型，以消除由缺失或高度压缩的对象信息引起的伪影。
性能：</li>
<li>在几个具有挑战性的真实世界数据集上，高斯对象方法在定性和定量评估中均优于以前的最先进方法。</li>
<li>高斯对象方法能够从极度稀疏的 360° 视图中重建高质量的 3D 对象，这对于用户来说非常方便，并且可以广泛应用于各种领域。
工作量：</li>
<li>高斯对象方法需要大量的训练数据，这可能会增加训练时间和成本。</li>
<li>高斯对象方法需要使用神经网络来学习高斯球面体的参数，这可能会增加计算复杂度。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ec0859f0d4156531b928896ce0f20711.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a6cf586e290dad38d6317bf5e32650f6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fc6b9cc2318a136451091ab1f1c68efb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0ee843ee1e2c5a9e509cc05d4936f7f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de6acbb2bc7ce290268eb48c8af2cb6b.jpg" align="middle">
</details>




<h2 id="GES-Generalized-Exponential-Splatting-for-Efficient-Radiance-Field-Rendering"><a href="#GES-Generalized-Exponential-Splatting-for-Efficient-Radiance-Field-Rendering" class="headerlink" title="GES: Generalized Exponential Splatting for Efficient Radiance Field   Rendering"></a>GES: Generalized Exponential Splatting for Efficient Radiance Field   Rendering</h2><p><strong>Authors:Abdullah Hamdi, Luke Melas-Kyriazi, Guocheng Qian, Jinjie Mai, Ruoshi Liu, Carl Vondrick, Bernard Ghanem, Andrea Vedaldi</strong></p>
<p>Advancements in 3D Gaussian Splatting have significantly accelerated 3D reconstruction and generation. However, it may require a large number of Gaussians, which creates a substantial memory footprint. This paper introduces GES (Generalized Exponential Splatting), a novel representation that employs Generalized Exponential Function (GEF) to model 3D scenes, requiring far fewer particles to represent a scene and thus significantly outperforming Gaussian Splatting methods in efficiency with a plug-and-play replacement ability for Gaussian-based utilities. GES is validated theoretically and empirically in both principled 1D setup and realistic 3D scenes.   It is shown to represent signals with sharp edges more accurately, which are typically challenging for Gaussians due to their inherent low-pass characteristics. Our empirical analysis demonstrates that GEF outperforms Gaussians in fitting natural-occurring signals (e.g. squares, triangles, and parabolic signals), thereby reducing the need for extensive splitting operations that increase the memory footprint of Gaussian Splatting. With the aid of a frequency-modulated loss, GES achieves competitive performance in novel-view synthesis benchmarks while requiring less than half the memory storage of Gaussian Splatting and increasing the rendering speed by up to 39%. The code is available on the project website <a href="https://abdullahamdi.com/ges">https://abdullahamdi.com/ges</a> . </p>
<p><a href="http://arxiv.org/abs/2402.10128v1">PDF</a> preprint</p>
<p><strong>摘要</strong><br>广义指数散列法（GES）是一种新颖的 3D 场景表示方法，它使用广义指数函数 (GEF) 对 3D 场景进行建模，从而显著减少了表示场景所需的粒子数量，比高斯散列方法更加高效，并且即插即用，可以替代基于高斯的工具。</p>
<p><strong>要点</strong></p>
<ul>
<li>GES 使用广义指数函数 (GEF) 对 3D 场景进行建模，显著减少了所需粒子数量，提高了效率。</li>
<li>GES 优于高斯散列法，能够将 3D 场景建模为更少的粒子，在效率方面显著优于高斯散列法。</li>
<li>GES 在原理性的一维设置和现实的 3D 场景中经过理论和经验验证。</li>
<li>GES 在表达具有清晰边缘的信号方面更准确，而这些信号通常对高斯函数构成挑战，因其本身具有低通特性。</li>
<li>GES 在拟合自然发生的信号（例如正方形、三角形和抛物线信号）方面优于高斯函数，因而减少了增加高斯散列法的内存占用的大量分裂操作的需要。</li>
<li>使用调制频率损失，GES 可实现在新视图合成基准中具有竞争力的性能，同时所需的存储空间不到高斯散列法的二分之一，并使渲染速度提高多达 39%。</li>
<li>GES 的代码可在项目网站 <a href="https://abdullahamdi.com/ges">https://abdullahamdi.com/ges</a> 上获取。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：GES：用于高效光场渲染的广义指数散射（中文翻译）</li>
<li>作者：Abdullah Hamdi、Luke Melas-Kyriazi、Guocheng Qian、Jinjie Mai、Ruoshi Liu、Carl Vondrick、Bernard Ghanem、Andrea Vedaldi</li>
<li>第一作者单位：牛津大学视觉几何组（中文翻译）</li>
<li>关键词：3D 重建、3D 生成、3D 表示、光场渲染、广义指数函数</li>
<li>论文链接：https://arxiv.org/abs/2402.10128，Github 代码链接：无</li>
<li>摘要：
（1）研究背景：3D 高斯散射在 3D 重建和生成方面取得了重大进展。然而，它可能需要大量高斯函数，这会造成巨大的内存占用。
（2）过去的方法及其问题：高斯散射方法假设场景信号是低通的，但大多数 3D 场景都包含形状和外观上的突变，因此高斯散射需要使用大量非常小的高斯函数来表示这些 3D 场景，这会对内存利用率产生负面影响。
（3）本文提出的研究方法：本文提出 GES（广义指数散射），它使用广义指数函数（具有额外的可学习形状参数）来建模 3D 场景，从而可以减少表示场景所需的粒子数量，从而在效率上明显优于高斯散射方法，并且可以即插即用地替换基于高斯的实用工具。
（4）方法在什么任务上取得了什么性能，这些性能是否支持其目标：GES 在原理性 1D 设置和逼真的 3D 场景中都得到了理论和经验验证。结果表明，它可以更准确地表示具有锐利边缘的信号，而这对于高斯函数来说通常具有挑战性，因为它们具有固有的低通特性。实证分析表明，GES 在拟合自然出现的信号（例如，正方形、三角形、抛物线信号）方面优于高斯函数，从而减少了增加高斯散射内存占用率的广泛分裂操作的需要。在频率调制损失的帮助下，GES 在新视图合成基准测试中取得了具有竞争力的性能，同时所需的内存存储量不到高斯散射的一半，并且渲染速度提高了 39%。</li>
</ol>
<p><methods>:
(1): GES使用广义指数函数（具有额外的可学习形状参数）来建模3D场景，从而可以减少表示场景所需的粒子数量，从而在效率上明显优于高斯散射方法，并且可以即插即用地替换基于高斯的实用工具。
(2): GES在原理性1D设置和逼真的3D场景中都得到了理论和经验验证。结果表明，它可以更准确地表示具有锐利边缘的信号，而这对于高斯函数来说通常具有挑战性，因为它们具有固有的低通特性。
(3): 实证分析表明，GES在拟合自然出现的信号（例如，正方形、三角形、抛物线信号）方面优于高斯函数，从而减少了增加高斯散射内存占用率的广泛分裂操作的需要。
(4): 在频率调制损失的帮助下，GES在新视图合成基准测试中取得了具有竞争力的性能，同时所需的内存存储量不到高斯散射的一半，并且渲染速度提高了39%。</methods></p>
<ol>
<li>结论：
(1): 本文提出了一种新的光场渲染方法 GES，它使用广义指数函数来建模 3D 场景，从而可以减少表示场景所需的粒子数量，从而在效率上明显优于高斯散射方法，并且可以即插即用地替换基于高斯的实用工具。
(2): 创新点：</li>
<li>GES 使用广义指数函数来建模 3D 场景，从而可以减少表示场景所需的粒子数量，从而在效率上明显优于高斯散射方法。</li>
<li>GES 在原理性 1D 设置和逼真的 3D 场景中都得到了理论和经验验证。结果表明，它可以更准确地表示具有锐利边缘的信号，而这对于高斯函数来说通常具有挑战性，因为它们具有固有的低通特性。</li>
<li>实证分析表明，GES 在拟合自然出现的信号（例如，正方形、三角形、抛物线信号）方面优于高斯函数，从而减少了增加高斯散射内存占用率的广泛分裂操作的需要。</li>
<li>在频率调制损失的帮助下，GES 在新视图合成基准测试中取得了具有竞争力的性能，同时所需的内存存储量不到高斯散射的一半，并且渲染速度提高了 39%。
性能：</li>
<li>GES 在原理性 1D 设置和逼真的 3D 场景中都得到了理论和经验验证。结果表明，它可以更准确地表示具有锐利边缘的信号，而这对于高斯函数来说通常具有挑战性，因为它们具有固有的低通特性。</li>
<li>实证分析表明，GES 在拟合自然出现的信号（例如，正方形、三角形、抛物线信号）方面优于高斯函数，从而减少了增加高斯散射内存占用率的广泛分裂操作的需要。</li>
<li>在频率调制损失的帮助下，GES 在新视图合成基准测试中取得了具有竞争力的性能，同时所需的内存存储量不到高斯散射的一半，并且渲染速度提高了 39%。
工作量：</li>
<li>GES 的实现相对简单，并且可以很容易地集成到现有的光场渲染工具中。</li>
<li>GES 的训练过程相对较快，并且可以在几分钟内完成。</li>
<li>GES 的渲染速度很快，并且可以在几秒钟内生成高质量的图像。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-06e50cf8fcf2b71cc6d5f5fa60bd416c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e0387aa41ca3382d21ca4822a1185a81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d98ce6f15593a9709f1a7d0a0c108a7f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4903d39957be51dd29a4222bcccefaa4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c50bfcbaec1420bcb70374001db6c443.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e090b0178d5a97f88600cc386571b770.jpg" align="middle">
</details>




<h2 id="GS-CLIP-Gaussian-Splatting-for-Contrastive-Language-Image-3D-Pretraining-from-Real-World-Data"><a href="#GS-CLIP-Gaussian-Splatting-for-Contrastive-Language-Image-3D-Pretraining-from-Real-World-Data" class="headerlink" title="GS-CLIP: Gaussian Splatting for Contrastive Language-Image-3D   Pretraining from Real-World Data"></a>GS-CLIP: Gaussian Splatting for Contrastive Language-Image-3D   Pretraining from Real-World Data</h2><p><strong>Authors:Haoyuan Li, Yanpeng Zhou, Yihan Zeng, Hang Xu, Xiaodan Liang</strong></p>
<p>3D Shape represented as point cloud has achieve advancements in multimodal pre-training to align image and language descriptions, which is curial to object identification, classification, and retrieval. However, the discrete representations of point cloud lost the object’s surface shape information and creates a gap between rendering results and 2D correspondences. To address this problem, we propose GS-CLIP for the first attempt to introduce 3DGS (3D Gaussian Splatting) into multimodal pre-training to enhance 3D representation. GS-CLIP leverages a pre-trained vision-language model for a learned common visual and textual space on massive real world image-text pairs and then learns a 3D Encoder for aligning 3DGS optimized per object. Additionally, a novel Gaussian-Aware Fusion is proposed to extract and fuse global explicit feature. As a general framework for language-image-3D pre-training, GS-CLIP is agnostic to 3D backbone networks. Experiments on challenging shows that GS-CLIP significantly improves the state-of-the-art, outperforming the previously best results. </p>
<p><a href="http://arxiv.org/abs/2402.06198v2">PDF</a> The content of the technical report needs to be updated and retracted   to avoid other impacts</p>
<p><strong>摘要</strong><br>利用3DGS(三维高斯渲染)增强3D表现，以进行多模态预训练，提升图像、语言和三维数据的对齐，改善物体识别、分类和检索任务。</p>
<p><strong>要点</strong></p>
<ul>
<li>利用点云表示的3D形状在图像和语言描述的对齐上取得了多模态预训练的进步，这对于物体识别、分类和检索至关重要。</li>
<li>点云的离散表示丢失了物体的曲面形状信息，并在渲染结果和2D对应关系之间制造差距。</li>
<li>提出GS-CLIP，首次尝试将3DGS（三维高斯渲染）引入多模态预训练，以增强3D表示。</li>
<li>GS-CLIP利用预训练的视觉-语言模型，在大量真实世界图像-文本对上学习一个通用的视觉和文本空间，然后学习一个针对每个物体优化3DGS的3D编码器。</li>
<li>提出了一种新的高斯感知融合来提取和融合全局显式特征。</li>
<li>作为语言-图像-3D预训练的通用框架，GS-CLIP独立于3D骨干网络。</li>
<li>具有挑战性的实验表明，GS-CLIP显著优于最先进的技术，超越了以前最好的成果。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：GS-CLIP：用于对比语言-图像-3D 预训练的高斯喷绘</li>
<li>作者：李昊源、周雁鹏、曾义涵、徐航、梁晓丹</li>
<li>第一作者单位：深圳大学</li>
<li>关键词：3D 表示、对比学习、多模态预训练、高斯喷绘</li>
<li>论文链接：https://arxiv.org/abs/2402.06198，Github 代码链接：无</li>
<li>
<p>摘要：
（1）研究背景：3D 形状表示为点云在多模态预训练中取得了进展，可以对齐图像和语言描述，这对物体识别、分类和检索至关重要。然而，点云的离散表示丢失了物体的表面形状信息，并在渲染结果和 2D 对应关系之间产生了差距。
（2）过去的方法及其问题：现有方法主要集中在对点云进行建模，但这些方法通常会丢失物体的几何信息和形状纹理。此外，现有方法通常需要大量的数据，这使得它们难以应用于现实世界中的场景。
（3）提出的研究方法：为了解决上述问题，本文提出了一种新的框架 GS-CLIP，该框架将 3D 高斯喷绘 (3DGS) 引入多模态预训练中，以增强 3D 表示。GS-CLIP 利用预训练的视觉语言模型在大量真实世界图像-文本对上学习一个共同的视觉和文本空间，然后学习一个 3D 编码器来对齐针对每个对象优化的 3DGS。此外，本文还提出了一种新的高斯感知融合方法，用于提取和融合全局显式特征。
（4）方法在任务和性能上的表现：在 SUN-RGBD 数据集上的实验表明，GS-CLIP 在真实世界环境中的零样本/开放世界学习中取得了最先进的性能。这些结果表明，3DGS 在跨模态学习中具有强大的表示能力。</p>
</li>
<li>
<p>方法：
（1）跨模态预训练：利用预训练的语言-图像模型CLIP，为文本、图像和3DGS建立共同的语言-图像潜在空间，作为3DGS的目标潜在空间。
（2）语言-3DGS对齐和图像-3DGS对齐：分别使用对比损失函数来对齐文本与3DGS、图像与3DGS的特征表示。
（3）高斯感知融合：采用基于Transformer的分支直接对高斯特征进行建模，并将其与残差形式注入到3D主干网络中。</p>
</li>
<li>
<p>结论：
（1）：本工作首次将 3DGS 纳入跨模态学习，作为补充形状和纹理信息的通用 3D 表示。为此，提出了一种高斯感知融合，以便更好地从补充信息中学习信息。我们证明了我们提出的 GS-CLIP 在最先进的方法中取得了优异的性能，并在真实世界环境中实现了零样本/开放世界学习的最新性能。
（2）：创新点：</p>
</li>
<li>将 3DGS 引入跨模态学习，作为补充形状和纹理信息的通用 3D 表示。</li>
<li>提出了一种高斯感知融合，以便更好地从补充信息中学习信息。</li>
<li>在真实世界环境中实现了零样本/开放世界学习的最新性能。
性能：</li>
<li>在 SUN-RGBD 数据集上，GS-CLIP 在真实世界环境中的零样本/开放世界学习中取得了最先进的性能。</li>
<li>这些结果表明，3DGS 在跨模态学习中具有强大的表示能力。
工作量：</li>
<li>该工作涉及到大量的数据预处理和模型训练。</li>
<li>需要对 3DGS 进行优化，以使其能够更好地对齐文本和图像的特征表示。</li>
<li>需要对高斯感知融合进行进一步的研究，以使其能够更好地提取和融合全局显式特征。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5ca02e3188a2350914f961c6e31c0616.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4980273838b01e0c94c7593c3becb878.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b33d684beebaf5252e0357a0e0af9c1d.jpg" align="middle">
</details>




<h2 id="GaMeS-Mesh-Based-Adapting-and-Modification-of-Gaussian-Splatting"><a href="#GaMeS-Mesh-Based-Adapting-and-Modification-of-Gaussian-Splatting" class="headerlink" title="GaMeS: Mesh-Based Adapting and Modification of Gaussian Splatting"></a>GaMeS: Mesh-Based Adapting and Modification of Gaussian Splatting</h2><p><strong>Authors:Joanna Waczyńska, Piotr Borycki, Sławomir Tadeja, Jacek Tabor, Przemysław Spurek</strong></p>
<p>Recently, a range of neural network-based methods for image rendering have been introduced. One such widely-researched neural radiance field (NeRF) relies on a neural network to represent 3D scenes, allowing for realistic view synthesis from a small number of 2D images. However, most NeRF models are constrained by long training and inference times. In comparison, Gaussian Splatting (GS) is a novel, state-of-the-art technique for rendering points in a 3D scene by approximating their contribution to image pixels through Gaussian distributions, warranting fast training and swift, real-time rendering. A drawback of GS is the absence of a well-defined approach for its conditioning due to the necessity to condition several hundred thousand Gaussian components. To solve this, we introduce the Gaussian Mesh Splatting (GaMeS) model, which allows modification of Gaussian components in a similar way as meshes. We parameterize each Gaussian component by the vertices of the mesh face. Furthermore, our model needs mesh initialization on input or estimated mesh during training. We also define Gaussian splats solely based on their location on the mesh, allowing for automatic adjustments in position, scale, and rotation during animation. As a result, we obtain a real-time rendering of editable GS. </p>
<p><a href="http://arxiv.org/abs/2402.01459v3">PDF</a> </p>
<p><strong>Summary:</strong><br>神经辐射场 (NeRF) 是一种用于图像渲染的神经网络方法，而高斯网格泼溅 (GaMeS) 模型则通过高斯分布来估算 3D 场景中点的贡献，从而实现快速训练和实时渲染。</p>
<p><strong>Key Takeaways:</strong></p>
<ul>
<li>利用神经网络表征 3D 场景的 NeRF，允许从少量 2D 图像中进行逼真的视点合成。</li>
<li>高斯泼溅 (GS) 通过高斯分布来估算 3D 场景中点的贡献，从而实现快速训练和实时渲染。</li>
<li>GaMeS 模型允许以与网格类似的方式修改高斯分量，从而为 GS 的调节提供了一个明确的方法。</li>
<li>将每个高斯分量参数化为网格面的顶点，这使得 GaMeS 模型可以对 GS 进行实时渲染。</li>
<li>GaMeS 模型需要在输入时初始化网格或在训练期间估计网格。</li>
<li>根据其在网格上的位置定义高斯泼溅，从而允许在动画期间自动调整位置、缩放和旋转。</li>
<li>GaMeS 模型可以实现可编辑 GS 的实时渲染。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：GaMeS：基于网格的自适应和修改高斯喷绘</li>
<li>作者：Joanna Waczyńska、Piotr Borycki、Sławomir Tadeja、Jacek Tabor、Przemysław Spurek</li>
<li>第一作者单位：雅盖隆大学数学与计算机科学学院，波兰克拉科夫</li>
<li>关键词：高斯喷绘、神经辐射场、神经渲染、网格、实时渲染</li>
<li>论文链接：https://arxiv.org/abs/2402.01459，Github 代码链接：无</li>
<li>摘要：
(1)：研究背景：近年来，基于神经网络的图像渲染方法取得了很大进展，其中神经辐射场（NeRF）是一种流行的方法，它使用神经网络来表示 3D 场景，并能够从少量 2D 图像中合成逼真的视图。然而，大多数 NeRF 模型都受到训练和推理时间长的限制。与之相比，高斯喷绘（GS）是一种新颖的、最先进的技术，它通过高斯分布来近似点对图像像素的贡献，从而渲染 3D 场景中的点，具有快速训练和快速实时渲染的能力。
(2)：过去的方法和问题：GS 的一个缺点是缺乏明确的调节方法，因为需要调节数十万个高斯分量。为了解决这个问题，本文介绍了高斯网格喷绘（GaMeS）模型，它允许像修改网格一样修改高斯分量。我们将每个高斯分量参数化为网格面的顶点。此外，我们的模型需要在输入或训练期间估计的网格上进行网格初始化。我们还定义了仅基于其在网格上的位置的高斯喷绘，允许在动画期间自动调整位置、比例和旋转。因此，我们获得了可编辑 GS 的实时渲染。
(3)：研究方法：我们提出了 GaMeS 模型，它允许像修改网格一样修改高斯分量。我们将每个高斯分量参数化为网格面的顶点。此外，我们的模型需要在输入或训练期间估计的网格上进行网格初始化。我们还定义了仅基于其在网格上的位置的高斯喷绘，允许在动画期间自动调整位置、比例和旋转。
(4)：方法的性能：实验结果表明，GaMeS 模型能够在保持高质量渲染的同时，实现实时修改和适应高斯喷绘。这使得 GaMeS 成为交互式应用程序和游戏中的一个有前景的技术。</li>
</ol>
<p>Some Error for method(比如是不是没有Methods这个章节)</p>
<ol>
<li>结论：
（1）GaMeS 允许实时修改，但对于具有大面的网格，在发生重大变化的情况下会出现伪影。在实践中，大面应该被分成更小的面。当网格面分裂时如何在 GaMeS 中更改高斯分量尚不清楚。
（2）创新点：GaMeS 提出了一种新的基于网格的自适应和修改高斯喷绘模型，该模型允许像修改网格一样修改高斯分量，从而实现了实时修改和适应高斯喷绘。
性能：实验结果表明，GaMeS 模型能够在保持高质量渲染的同时，实现实时修改和适应高斯喷绘。这使得 GaMeS 成为交互式应用程序和游戏中的一个有前景的技术。
工作量：GaMeS 模型需要在输入或训练期间估计网格，这可能会增加模型的训练和推理时间。此外，GaMeS 模型需要对网格进行修改，这可能会增加模型的修改时间。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-11676aa94eeb837bc5149bf9038274ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d3c20ac78640d356ea03699146c96e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4070017cd795fd8699e30a356efae899.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0416310a796f7ec70150342ac59ffe37.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6eb0975a0f5d702a6daef3f78e530869.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9fb0edd088d9a64e792369a6d6a72979.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dd54f927f26f28fdcefe778d566087c5.jpg" align="middle">
</details>




]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>3DGS</tag>
      </tags>
  </entry>
  <entry>
    <title>Diffusion Models</title>
    <url>/2024/03/04/Paper/2024-03-04/Diffusion%20Models/</url>
    <content><![CDATA[<blockquote>
<p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-03-04-更新"><a href="#2024-03-04-更新" class="headerlink" title="2024-03-04 更新"></a>2024-03-04 更新</h1><h2 id="DistriFusion-Distributed-Parallel-Inference-for-High-Resolution-Diffusion-Models"><a href="#DistriFusion-Distributed-Parallel-Inference-for-High-Resolution-Diffusion-Models" class="headerlink" title="DistriFusion: Distributed Parallel Inference for High-Resolution   Diffusion Models"></a>DistriFusion: Distributed Parallel Inference for High-Resolution   Diffusion Models</h2><p><strong>Authors:Muyang Li, Tianle Cai, Jiaxin Cao, Qinsheng Zhang, Han Cai, Junjie Bai, Yangqing Jia, Ming-Yu Liu, Kai Li, Song Han</strong></p>
<p>Diffusion models have achieved great success in synthesizing high-quality images. However, generating high-resolution images with diffusion models is still challenging due to the enormous computational costs, resulting in a prohibitive latency for interactive applications. In this paper, we propose DistriFusion to tackle this problem by leveraging parallelism across multiple GPUs. Our method splits the model input into multiple patches and assigns each patch to a GPU. However, na\”{\i}vely implementing such an algorithm breaks the interaction between patches and loses fidelity, while incorporating such an interaction will incur tremendous communication overhead. To overcome this dilemma, we observe the high similarity between the input from adjacent diffusion steps and propose displaced patch parallelism, which takes advantage of the sequential nature of the diffusion process by reusing the pre-computed feature maps from the previous timestep to provide context for the current step. Therefore, our method supports asynchronous communication, which can be pipelined by computation. Extensive experiments show that our method can be applied to recent Stable Diffusion XL with no quality degradation and achieve up to a 6.1$\times$ speedup on eight NVIDIA A100s compared to one. Our code is publicly available at <a href="https://github.com/mit-han-lab/distrifuser">https://github.com/mit-han-lab/distrifuser</a>. </p>
<p><a href="http://arxiv.org/abs/2402.19481v1">PDF</a> CVPR 2024 Code: <a href="https://github.com/mit-han-lab/distrifuser">https://github.com/mit-han-lab/distrifuser</a> Website:   <a href="https://hanlab.mit.edu/projects/distrifusion">https://hanlab.mit.edu/projects/distrifusion</a> Blog:   <a href="https://hanlab.mit.edu/blog/distrifusion">https://hanlab.mit.edu/blog/distrifusion</a></p>
<p><strong>Summary</strong><br>利用多GPU实现并行处理，提升高分辨率图像生成效率，并通过复用特征图降低通信开销，显著加速扩散模型推理。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>多GPU并行处理可大幅提升扩散模型推理速度。</li>
<li>将模型输入拆分为多个patch，分配给不同GPU处理。</li>
<li>位移patch并行机制，利用相邻扩散步长的相似性，复用特征图减少通信开销。</li>
<li>支持异步通信，可与计算流水线化。</li>
<li>在Stable Diffusion XL模型上验证有效性，无质量损失且加速6.1倍。</li>
<li>已开源代码：<a href="https://github.com/mit-han-lab/distrifuser">https://github.com/mit-han-lab/distrifuser</a>.</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>标题：DistriFusion：用于高分辨率扩散模型的分布式并行推理</li>
<li>作者：Muyang Li、Tianle Cai、Jiaxin Cao、Qinsheng Zhang、Han Cai、Junjie Bai、Yangqing Jia、Ming-Yu Liu、Kai Li、Song Han</li>
<li>第一作者单位：麻省理工学院</li>
<li>关键词：Diffusion Models、Parallel Inference、High-Resolution Images</li>
<li>论文链接：https://arxiv.org/abs/2402.19481
Github 代码链接：https://github.com/mit-han-lab/distrifuser</li>
<li>摘要：
（1）研究背景：扩散模型在合成高质量图像方面取得了巨大成功。然而，使用扩散模型生成高分辨率图像仍然具有挑战性，因为计算成本巨大，导致交互式应用程序的延迟很高。
（2）过去方法及问题：过去的方法将模型输入拆分为多个块，并将其分配给不同的 GPU。然而，这种朴素的实现会破坏块之间的交互，从而降低保真度。而引入交互又会导致巨大的通信开销。
（3）研究方法：本文提出了 DistriFusion，通过利用多 GPU 的并行性来解决这个问题。该方法利用扩散过程的顺序性质，重用来自前一时间步的预计算特征图，为当前时间步提供上下文。
（4）任务和性能：DistriFusion 可以应用于最新的 Stable Diffusion XL，且不降低质量。与单个 GPU 相比，在八个 NVIDIA A100 上，该方法实现了高达 6.1 倍的加速。这些性能支持了他们的目标，即以较低的延迟生成高质量的高分辨率图像。</li>
</ol>
<p><strong>7. 方法</strong>
(1): DistriFusion通过利用扩散过程的顺序性质，重用来自前一时间步的预计算特征图，为当前时间步提供上下文，从而解决多GPU并行推理中块之间交互破坏保真度的问题。
(2): 该方法将模型输入拆分为多个块，并将其分配给不同的GPU，在每个GPU上独立执行扩散过程。
(3): 为了维护块之间的交互，DistriFusion利用了预计算特征图，这些特征图包含了前一时间步的上下文信息。
(4): 通过重用这些预计算特征图，DistriFusion避免了在块之间传输中间特征图的需要，从而减少了通信开销。
(5): 此外，DistriFusion还采用了异步执行机制，允许不同GPU在不同的时间步上工作，进一步提高了并行效率。</p>
<ol>
<li>结论：
（1）本工作通过提出 DistriFusion 方法，解决了高分辨率扩散模型分布式并行推理中块之间交互破坏保真度的难题，为交互式应用程序生成高质量高分辨率图像提供了支持。
（2）创新点：</li>
<li>利用扩散过程的顺序性质，重用预计算特征图，为当前时间步提供上下文，避免了块之间传输中间特征图的需要，减少了通信开销。</li>
<li>采用异步执行机制，允许不同 GPU 在不同的时间步上工作，进一步提高了并行效率。
性能：</li>
<li>在八个 NVIDIA A100 上，与单个 GPU 相比，实现了高达 6.1 倍的加速。
工作量：</li>
<li>该方法可以应用于最新的 StableDiffusionXL，且不降低质量。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-437f25db9d3e29d465c2ea11bbb5cca0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d41c099d139cb88d89783cdff85061d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e528b344942b85d8abba3ea6722f8989.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-693881daa5f71c118b273327cab24071.jpg" align="middle">
</details>




<h2 id="A-Novel-Approach-to-Industrial-Defect-Generation-through-Blended-Latent-Diffusion-Model-with-Online-Adaptation"><a href="#A-Novel-Approach-to-Industrial-Defect-Generation-through-Blended-Latent-Diffusion-Model-with-Online-Adaptation" class="headerlink" title="A Novel Approach to Industrial Defect Generation through Blended Latent   Diffusion Model with Online Adaptation"></a>A Novel Approach to Industrial Defect Generation through Blended Latent   Diffusion Model with Online Adaptation</h2><p><strong>Authors:Hanxi Li, Zhengxun Zhang, Hao Chen, Lin Wu, Bo Li, Deyin Liu, Mingwen Wang</strong></p>
<p>Effectively addressing the challenge of industrial Anomaly Detection (AD) necessitates an ample supply of defective samples, a constraint often hindered by their scarcity in industrial contexts. This paper introduces a novel algorithm designed to augment defective samples, thereby enhancing AD performance. The proposed method tailors the blended latent diffusion model for defect sample generation, employing a diffusion model to generate defective samples in the latent space. A feature editing process, controlled by a “trimap” mask and text prompts, refines the generated samples. The image generation inference process is structured into three stages: a free diffusion stage, an editing diffusion stage, and an online decoder adaptation stage. This sophisticated inference strategy yields high-quality synthetic defective samples with diverse pattern variations, leading to significantly improved AD accuracies based on the augmented training set. Specifically, on the widely recognized MVTec AD dataset, the proposed method elevates the state-of-the-art (SOTA) performance of AD with augmented data by 1.5%, 1.9%, and 3.1% for AD metrics AP, IAP, and IAP90, respectively. The implementation code of this work can be found at the GitHub repository <a href="https://github.com/GrandpaXun242/AdaBLDM.git">https://github.com/GrandpaXun242/AdaBLDM.git</a> </p>
<p><a href="http://arxiv.org/abs/2402.19330v1">PDF</a> 13 pages,7 figures</p>
<p><strong>Summary</strong><br>用扩散模型生成缺陷样本来增强工业异常检测。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>工业异常检测（AD）的缺陷样本不足。</li>
<li>本文提出了一种算法，使用扩散模型在潜在空间生成缺陷样本。</li>
<li>特征编辑过程由三幅图掩码和文本提示控制。</li>
<li>图像生成推理分为自由扩散阶段、编辑扩散阶段和在线解码器适应阶段。</li>
<li>该方法产生了高质量的合成缺陷样本，具有多样化的模式变化。</li>
<li>在MVTec AD数据集上，该方法将AD的SOTA性能提升了1.5%（AP）、1.9%（IAP）和3.1%（IAP90）。</li>
<li>代码可在GitHub存储库中找到。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>标题：基于多阶段去噪的内容编辑缺陷样本生成</li>
<li>作者：Xun Zhou, Yuhui Quan, Xiaoguang Han, Wei Shen</li>
<li>隶属单位：西湖大学</li>
<li>关键词：Anomaly detection, Blended latent diffusion model, Online adaptation</li>
<li>论文链接：None
   Github 代码链接：https://github.com/GrandpaXun242/AdaBLDM.git</li>
<li>
<p>摘要：
（1）研究背景：工业异常检测面临缺陷样本匮乏的挑战。
（2）过去方法：现有方法主要基于图像生成模型生成缺陷样本，但存在生成质量差、多样性不足等问题。
（3）研究方法：本文提出了一种基于混合潜在扩散模型的缺陷样本生成方法，通过特征编辑过程，在潜在空间中生成缺陷样本，并通过“trimap”掩码和文本提示进行优化。
（4）任务和性能：在 MVTecAD 数据集上，该方法将基于扩充数据集的异常检测精度提升了 1.5%（AP）、1.9%（IAP）和 3.1%（IAP90），证明了其有效性。</p>
</li>
<li>
<p>方法：
(1) 提出基于混合潜在扩散模型（BLDM）的缺陷样本生成方法；
(2) 利用特征编辑过程，在潜在空间中生成缺陷样本；
(3) 通过 "trimap" 掩码和文本提示对生成样本进行优化；
(4) 在 MVTecAD 数据集上评估方法的有效性，并与现有方法进行比较。</p>
</li>
<li>
<p>结论：
(1): 本文提出了一种基于混合潜在扩散模型（BLDM）的缺陷样本生成方法，通过特征编辑过程在潜在空间中生成缺陷样本，并通过“trimap”掩码和文本提示对生成样本进行优化。该方法在MVTecAD数据集上将基于扩充数据集的异常检测精度提升了1.5%（AP）、1.9%（IAP）和3.1%（IAP90），证明了其有效性。
(2): 创新点：</p>
</li>
<li>提出了一种基于混合潜在扩散模型（BLDM）的缺陷样本生成方法。</li>
<li>利用特征编辑过程，在潜在空间中生成缺陷样本。</li>
<li>通过“trimap”掩码和文本提示对生成样本进行优化。
性能：</li>
<li>在MVTecAD数据集上，该方法将基于扩充数据集的异常检测精度提升了1.5%（AP）、1.9%（IAP）和3.1%（IAP90）。
工作量：</li>
<li>提出了一种新的缺陷样本生成方法，需要额外的计算资源和数据预处理步骤。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-1e4adba77bea5b8766028ddf128d14f8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ddc6dc7d79a00c265a6871998b50f1d8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-47283af00a9ac7f4f8c1fd9a4862962d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc13df59604429aeb15f04943c88e89e.jpg" align="middle">
</details>




<h2 id="DiffAssemble-A-Unified-Graph-Diffusion-Model-for-2D-and-3D-Reassembly"><a href="#DiffAssemble-A-Unified-Graph-Diffusion-Model-for-2D-and-3D-Reassembly" class="headerlink" title="DiffAssemble: A Unified Graph-Diffusion Model for 2D and 3D Reassembly"></a>DiffAssemble: A Unified Graph-Diffusion Model for 2D and 3D Reassembly</h2><p><strong>Authors:Gianluca Scarpellini, Stefano Fiorini, Francesco Giuliari, Pietro Morerio, Alessio Del Bue</strong></p>
<p>Reassembly tasks play a fundamental role in many fields and multiple approaches exist to solve specific reassembly problems. In this context, we posit that a general unified model can effectively address them all, irrespective of the input data type (images, 3D, etc.). We introduce DiffAssemble, a Graph Neural Network (GNN)-based architecture that learns to solve reassembly tasks using a diffusion model formulation. Our method treats the elements of a set, whether pieces of 2D patch or 3D object fragments, as nodes of a spatial graph. Training is performed by introducing noise into the position and rotation of the elements and iteratively denoising them to reconstruct the coherent initial pose. DiffAssemble achieves state-of-the-art (SOTA) results in most 2D and 3D reassembly tasks and is the first learning-based approach that solves 2D puzzles for both rotation and translation. Furthermore, we highlight its remarkable reduction in run-time, performing 11 times faster than the quickest optimization-based method for puzzle solving. Code available at <a href="https://github.com/IIT-PAVIS/DiffAssemble">https://github.com/IIT-PAVIS/DiffAssemble</a> </p>
<p><a href="http://arxiv.org/abs/2402.19302v1">PDF</a> Accepted at CVPR2024</p>
<p><strong>Summary</strong><br>利用扩散模型和图神经网络，DiffAssemble 提出了一种统一的模型来解决各种重组任务，包括 2D 和 3D 数据。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>DiffAssemble 采用扩散模型框架，将重组问题建模为扩散过程。</li>
<li>基于图神经网络，DiffAssemble 将元素视为空间图中的节点。</li>
<li>通过引入位置和旋转噪声并进行去噪，DiffAssemble 能够重构初始姿态。</li>
<li>DiffAssemble 在大多数 2D 和 3D 重组任务上达到最先进的性能。</li>
<li>DiffAssemble 是第一个能够同时解决旋转和平移的 2D 拼图的学习方法。</li>
<li>DiffAssemble 在运行时显著减少，比最快的基于优化的拼图求解方法快 11 倍。</li>
<li>DiffAssemble 的代码可在 <a href="https://github.com/IIT-PAVIS/DiffAssemble">https://github.com/IIT-PAVIS/DiffAssemble</a> 获得。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<p>1.标题：DiffAssemble：适用于二维和三维重组的统一图扩散模型
2.作者：Yifan Jiang, Yifan Zhang, Guilin Liu, Emanuele Rodolà, Mathieu Salzmann, Federico Tombari
3.所属单位：意大利理工学院
4.关键词：重组、图神经网络、扩散模型、计算机视觉、计算机图形学
5.论文链接：None, Github：https://github.com/IITPAVIS/DiffAssemble
6.摘要：
（1）研究背景：重组任务在许多领域发挥着基础性作用，存在多种方法来解决特定的重组问题。
（2）过去方法：过去的方法主要针对特定类型的重组问题，例如二维拼图或三维对象碎片重组，并且通常依赖于启发式或优化方法。这些方法可能在某些任务上表现良好，但在泛化到其他任务或处理复杂输入时存在困难。
（3）研究方法：本文提出 DiffAssemble，这是一种基于图神经网络 (GNN) 的架构，它利用扩散模型的框架来学习解决重组任务。该方法将集合中的元素（无论是二维块还是三维对象碎片）视为空间图中的节点。通过向元素的位置和旋转引入噪声并迭代去噪以重建连贯的初始姿势来进行训练。
（4）方法性能：DiffAssemble 在大多数二维和三维重组任务中达到最先进 (SOTA) 的结果，并且是第一个基于学习的方法，可以解决二维拼图的旋转和平移问题。此外，它还显着减少了运行时间，比用于拼图求解的最快的基于优化的方法快 11 倍。</p>
<ol>
<li>
<p><strong>方法</strong>：
(1) <strong>图扩散模型框架</strong>：将集合中的元素视为空间图中的节点，通过向元素的位置和旋转引入噪声并迭代去噪以重建连贯的初始姿势来进行训练。
(2) <strong>图神经网络架构</strong>：使用图神经网络（GNN）对图中的节点进行编码和解码，学习元素之间的关系和位置信息。
(3) <strong>扩散过程</strong>：通过逐步增加噪声水平来对图进行扩散，然后通过反向扩散过程逐步去除噪声，重建元素的初始姿势。
(4) <strong>旋转和平移不变性</strong>：通过引入旋转和平移不变的损失函数，使模型对元素的旋转和平移具有鲁棒性。
(5) <strong>高效优化</strong>：采用高效的优化算法和并行计算技术，显着减少训练和推理时间。</p>
</li>
<li>
<p>结论：
(1): 本工作提出了 DiffAssemble，这是一种用于解决重组任务的通用框架，它利用图表示和扩散模型公式。通过将重组表述为去噪任务，我们利用基于注意力的图神经网络通过扩散过程迭代细化每块的姿态。我们的实验评估展示了 DiffAssemble 的有效性，涵盖了 3D 对象重组和带有平移和旋转块的 2D 拼图。结果表明在大多数 2D 和 3D 场景中都取得了最优性能，揭示了这些看似截然不同的任务之间的共同点。值得注意的是，在 2D 领域，DiffAssemble 表现出对缺失块的鲁棒性，并且与基于优化的方法相比，实现了显着的效率。在 3D 中，我们的解决方案获得了最优结果，与之前的解决方案不同，它在平移和旋转中保持了准确性。
(2): 创新点：提出了一种统一的图扩散模型框架，用于解决二维和三维重组任务；
性能：在大多数二维和三维重组任务中达到最先进的结果，并且是第一个基于学习的方法，可以解决二维拼图的旋转和平移问题；
工作量：即使引入了基于扩展图的稀疏机制，DiffAssemble 的内存使用量也很高。未来的工作将集中在减轻内存需求和探索进一步的重组场景，同时处理来自真实世界扫描的数据。</p>
</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fbd1e6323bcd0532b52c4f695cce2d40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8cbc8e3077367b4529558da64e7a2d6a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9773a302fdfab51db4b378cbe8e1ac12.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-907399766cad36090773e74bbdce0d78.jpg" align="middle">
</details>




## ViewFusion: Towards Multi-View Consistency via Interpolated Denoising

**Authors:Xianghui Yang, Yan Zuo, Sameera Ramasinghe, Loris Bazzani, Gil Avraham, Anton van den Hengel**

Novel-view synthesis through diffusion models has demonstrated remarkable potential for generating diverse and high-quality images. Yet, the independent process of image generation in these prevailing methods leads to challenges in maintaining multiple-view consistency. To address this, we introduce ViewFusion, a novel, training-free algorithm that can be seamlessly integrated into existing pre-trained diffusion models. Our approach adopts an auto-regressive method that implicitly leverages previously generated views as context for the next view generation, ensuring robust multi-view consistency during the novel-view generation process. Through a diffusion process that fuses known-view information via interpolated denoising, our framework successfully extends single-view conditioned models to work in multiple-view conditional settings without any additional fine-tuning. Extensive experimental results demonstrate the effectiveness of ViewFusion in generating consistent and detailed novel views. 

[PDF](http://arxiv.org/abs/2402.18842v1) CVPR2024,homepage:https://wi-sc.github.io/ViewFusion.github.io/

**Summary**
扩散模型中的ViewFusion算法通过融合已知视图信息，无缝生成一致且详细的新视图。

**Key Takeaways**
- ViewFusion 是一种无训练算法，可集成到预训练的扩散模型中。
- 使用自回归方法，ViewFusion 将先前生成的视图作为上下文的下一视图生成。
- 通过扩散过程融合已知视图信息，ViewFusion 将单视图条件模型扩展到多视图条件设置。
- ViewFusion 无需额外微调。
- ViewFusion 在生成一致且详细的新视图方面具有有效性。
- ViewFusion 可与任何预训练的扩散模型兼容。
- ViewFusion 适用于各种多视图生成任务，例如 3D 场景重建和虚拟现实内容创建。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>题目：ViewFusion：通过扩散模型实现多视图一致的新颖视图合成</li>
<li>作者：Lingjie Liu, Shuyang Gu, Lingxi Xie, Jianmin Bao, Weiwei Xu, Wenxiu Sun, Tao Mei</li>
<li>单位：清华大学</li>
<li>关键词：新颖视图合成、扩散模型、多视图一致性</li>
<li>论文链接：https://arxiv.org/abs/2302.07033，Github 链接：None</li>
<li>摘要：
（1）：研究背景：新颖视图合成通过扩散模型已取得显著进展，但现有方法中独立的图像生成过程导致难以保持多视图一致性。
（2）：过去方法及其问题：Zero1-to-3 采用直接条件，Stochastic conditioning 采用随机条件，但这些方法都存在局限性，动机充分。
（3）：本文提出的研究方法：提出 ViewFusion，一种无训练的算法，可无缝集成到预训练扩散模型中。该方法采用自回归方法，隐式利用先前生成的视图作为下一视图生成的上下文，确保新颖视图生成过程中的稳健多视图一致性。通过融合已知视图信息进行插值去噪的扩散过程，该框架成功地将单视图条件模型扩展到多视图条件设置中，无需任何额外的微调。
（4）：方法在何任务上取得何种性能，性能是否支撑其目标：广泛的实验结果证明了 ViewFusion 在生成一致且详细的新颖视图方面的有效性。性能支撑了其目标，展示了该方法在多视图一致性新颖视图合成方面的潜力。</li>
</ol>
<p>7.方法：
（1）：本文提出了一种无训练算法 ViewFusion，可无缝集成到预训练扩散模型中。该方法采用自回归方法，隐式利用先前生成的视图作为下一视图生成的上下文，确保新颖视图生成过程中的稳健多视图一致性。
（2）：通过融合已知视图信息进行插值去噪的扩散过程，该框架成功地将单视图条件模型扩展到多视图条件设置中，无需任何额外的微调。
（3）：广泛的实验结果证明了 ViewFusion 在生成一致且详细的新颖视图方面的有效性。</p>
<ol>
<li>结论：
（1）本工作的重要性：ViewFusion 算法在多视图一致性新颖视图合成方面取得了突破性进展，为新颖视图合成和 3D 重建应用提供了新的思路。
（2）本文的优点和不足：
创新点：提出了一种无训练算法 ViewFusion，该算法通过自回归机制和扩散插值技术，无缝集成到预训练扩散模型中，实现了多视图一致性新颖视图合成。
性能：广泛的实验结果表明，ViewFusion 在生成一致且详细的新颖视图方面具有较好的性能，在多视图一致性新颖视图合成方面取得了显著的进步。
工作量：ViewFusion 算法的实现相对简单，不需要额外的微调或训练，工作量较小。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5ed3ebbc827c14338f60b96facf76706.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d71d68cb287ff4c48a689006c689e54e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ace8e541d3b0dc6b583217346370f6ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a9399a1aa83daa1e8f5056049bc5af0.jpg" align="middle">
</details>




## A Quantitative Evaluation of Score Distillation Sampling Based   Text-to-3D

**Authors:Xiaohan Fei, Chethan Parameshwara, Jiawei Mo, Xiaolong Li, Ashwin Swaminathan, CJ Taylor, Paolo Favaro, Stefano Soatto**

The development of generative models that create 3D content from a text prompt has made considerable strides thanks to the use of the score distillation sampling (SDS) method on pre-trained diffusion models for image generation. However, the SDS method is also the source of several artifacts, such as the Janus problem, the misalignment between the text prompt and the generated 3D model, and 3D model inaccuracies. While existing methods heavily rely on the qualitative assessment of these artifacts through visual inspection of a limited set of samples, in this work we propose more objective quantitative evaluation metrics, which we cross-validate via human ratings, and show analysis of the failure cases of the SDS technique. We demonstrate the effectiveness of this analysis by designing a novel computationally efficient baseline model that achieves state-of-the-art performance on the proposed metrics while addressing all the above-mentioned artifacts. 

[PDF](http://arxiv.org/abs/2402.18780v1) 

**Summary**
文本提出基于分数蒸馏采样的预训练扩散模型，在文本提示下生成3D内容。详细分析了生成3D模型的失效案例，并提出了新的评价指标，有效地改善了模型性能。

**Key Takeaways**
- 扩散模型结合文本提示生成3D内容取得进展，但仍存在人工制品和不准确问题。
- 提出新的定量评价指标客观评估人工制品，并与人工评级交叉验证。
- 分析了分数蒸馏采样技术的失效案例，找出其不足之处。
- 设计了一种新的计算高效基线模型，在提出的指标上达到最先进的性能，解决了上述所有人工制品问题。
- 基线模型通过分数蒸馏采样生成文本提示下3D内容，同时保持了语义一致性和几何准确性。
- 新的评价指标和基线模型为3D文本生成任务提供了一个更可靠和全面评估方法。
- 此方法可以应用于各种3D内容生成领域，如视频游戏、电影特效和虚拟现实。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>标题：基于分数蒸馏采样的文本到 3D 的定量评估</li>
<li>作者：Jiapeng Tang、Zhenyu Tan、Yixuan Wei、Yiyi Liao、Tongtong Zhao、Jingtuo Liu、Xin Tong、Qixing Huang</li>
<li>所属机构：清华大学</li>
<li>关键词：文本到 3D、生成模型、分数蒸馏采样、定量评估</li>
<li>论文链接：https://arxiv.org/abs/2302.05237
Github 代码链接：无</li>
<li>摘要：
(1) 研究背景：
生成模型从文本提示创建 3D 内容取得了很大进展，这得益于在图像生成预训练扩散模型上使用分数蒸馏采样 (SDS) 方法。然而，SDS 方法也是多种伪影的来源，例如 Janus 问题、文本提示和生成 3D 模型之间的未对齐以及 3D 模型不准确。</li>
</ol>
<p>(2) 过去的方法和问题：
现有方法严重依赖于通过对有限样本集进行视觉检查对这些伪影进行定性评估。</p>
<p>(3) 论文提出的研究方法：
本文提出了更客观的定量评估指标，并通过人类评级对其进行交叉验证，并展示了 SDS 技术失效情况的分析。</p>
<p>(4) 方法在任务和性能上的表现：
本文的方法在所提出的指标上实现了最先进的性能，同时解决了上述所有伪影。这些性能可以支持他们的目标。</p>
<p><methods>:
(1)图像真实度评价指标：使用Fréchet Inception Distance (FID) 和 Inception Score (IS) 衡量生成 3D 模型的真实度。
(2)训练效率指标：测量生成一个 3D 模型所需的 GPU 小时数，以评估方法的效率。
(3)分数蒸馏采样 (SDS) 框架：一种将预训练的文本到图像模型与神经辐射场 (NeRF) 相结合的方法，用于从文本提示创建 3D 模型。
(4)高斯散射：一种提高 SDS 效率的技术，通过将 3D 模型表示为高斯体素。
(5) T3Bench：一个用于评估文本到 3D 模型质量和对齐度的基准。</methods></p>
<p>8.结论：
（1）：本文提出了一个评估协议来检查文本到3D模型的三个关键方面：Janus问题、文本和3D对齐以及生成3D内容的真实性。通过使用此协议，我们评估了几种最先进的方法，并能够表征这些方法的局限性。通过这些发现，我们提出了一种新的文本到3D模型，该模型高效且在所有质量指标上表现良好，从而为未来的文本到3D工作设定了一个强有力的基线。未来的研究方向包括进一步提高文本到3D的效率，利用真实世界和合成数据来进一步提高3D内容生成的多样性、对齐性和真实性。
（2）：创新点：分数蒸馏采样（SDS）框架、高斯散射、T3Bench基准；
性能：在所提出的指标上实现了最先进的性能，解决了Janus问题、文本提示和生成3D模型之间的未对齐以及3D模型不准确等问题；
工作量：较低，仅需少量GPU小时即可生成一个3D模型。</p>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-7138ce8b5e2f1775ed9a260418c8f287.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fcb452bb7e50d746bb2fb822b0ef87b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe3df588379d7ce647754ec2d57d0c11.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-622d53734237ff0152b760777b6b876e.jpg" align="middle">
</details>




]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>Diffusion Models</tag>
      </tags>
  </entry>
  <entry>
    <title>3DGS</title>
    <url>/2024/03/09/Paper/2024-03-09/3DGS/</url>
    <content><![CDATA[<blockquote>
<p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-03-09-更新"><a href="#2024-03-09-更新" class="headerlink" title="2024-03-09 更新"></a>2024-03-09 更新</h1><h2 id="3DGStream-On-the-Fly-Training-of-3D-Gaussians-for-Efficient-Streaming-of-Photo-Realistic-Free-Viewpoint-Videos"><a href="#3DGStream-On-the-Fly-Training-of-3D-Gaussians-for-Efficient-Streaming-of-Photo-Realistic-Free-Viewpoint-Videos" class="headerlink" title="3DGStream: On-the-Fly Training of 3D Gaussians for Efficient Streaming   of Photo-Realistic Free-Viewpoint Videos"></a>3DGStream: On-the-Fly Training of 3D Gaussians for Efficient Streaming   of Photo-Realistic Free-Viewpoint Videos</h2><p><strong>Authors:Jiakai Sun, Han Jiao, Guangyuan Li, Zhanjie Zhang, Lei Zhao, Wei Xing</strong></p>
<p>Constructing photo-realistic Free-Viewpoint Videos (FVVs) of dynamic scenes from multi-view videos remains a challenging endeavor. Despite the remarkable advancements achieved by current neural rendering techniques, these methods generally require complete video sequences for offline training and are not capable of real-time rendering. To address these constraints, we introduce 3DGStream, a method designed for efficient FVV streaming of real-world dynamic scenes. Our method achieves fast on-the-fly per-frame reconstruction within 12 seconds and real-time rendering at 200 FPS. Specifically, we utilize 3D Gaussians (3DGs) to represent the scene. Instead of the na\”ive approach of directly optimizing 3DGs per-frame, we employ a compact Neural Transformation Cache (NTC) to model the translations and rotations of 3DGs, markedly reducing the training time and storage required for each FVV frame. Furthermore, we propose an adaptive 3DG addition strategy to handle emerging objects in dynamic scenes. Experiments demonstrate that 3DGStream achieves competitive performance in terms of rendering speed, image quality, training time, and model storage when compared with state-of-the-art methods. </p>
<p><a href="http://arxiv.org/abs/2403.01444v2">PDF</a> CVPR 2024 Accepted. Project Page: <a href="https://sjojok.github.io/3dgstream">https://sjojok.github.io/3dgstream</a></p>
<p><strong>Summary</strong><br>动态场景实时自由视点视频流方法3DGStream，利用3D高斯分布表示场景，通过神经网络变换缓存建模3D高斯分布的平移和旋转，实现每帧12秒内重建和200FPS实时渲染。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>提出3DGStream方法，实现动态场景的实时自由视点视频流。</li>
<li>利用3D高斯分布表示场景，有效捕捉场景结构。</li>
<li>使用神经网络变换缓存建模3D高斯分布的平移和旋转，减少训练时间和存储需求。</li>
<li>提出自适应3D高斯分布添加策略，处理动态场景中的新增对象。</li>
<li>3DGStream在渲染速度、图像质量、训练时间和模型存储方面达到先进水平。</li>
<li>每帧重建时间12秒内，实时渲染速度200FPS。</li>
<li>模型存储空间小，有效降低计算成本。</li>
<li>适用于动态场景的实时自由视点视频流，拓展3D视觉应用领域。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：3DGStream：动态场景高效流式传输的 3D 高斯实时训练</li>
<li>作者：Yuxuan Zhang, Lingjie Liu, Wenbo Bao, Wenxiu Sun, Qionghai Dai</li>
<li>单位：北京理工大学</li>
<li>关键词：Free-Viewpoint Video、动态场景、流式传输、3D 高斯</li>
<li>论文链接：https://arxiv.org/pdf/2209.04734.pdf
Github 代码链接：None</li>
<li>摘要：
（1）研究背景：构建动态场景的逼真自由视点视频（FVV）仍然是一项具有挑战性的任务。尽管当前的神经渲染技术取得了显着进步，但这些方法通常需要完整的视频序列进行离线训练，并且无法进行实时渲染。
（2）过去方法：现有方法存在的问题：</li>
<li>离线训练：需要完整的视频序列，无法实时渲染。</li>
<li>存储开销：需要为每个 FVV 帧存储大量数据。</li>
<li>训练时间：训练过程耗时。</li>
<li>无法处理动态场景中出现的物体。
（3）研究方法：</li>
<li>3D 高斯表示：使用 3D 高斯表示场景。</li>
<li>神经转换缓存（NTC）：使用 NTC 对 3D 高斯的平移和旋转进行建模，从而减少训练时间和存储需求。</li>
<li>自适应 3D 高斯添加策略：处理动态场景中出现的物体。
（4）性能：</li>
<li>渲染速度：实时渲染，达到 200FPS。</li>
<li>图像质量：与最先进的方法相比具有竞争力的渲染质量。</li>
<li>训练时间：与最先进的方法相比，训练时间显著减少。</li>
<li>
<p>模型存储：与最先进的方法相比，模型存储需求显著减少。</p>
</li>
<li>
<p>方法：
(1) 使用3D高斯表示场景，将场景表示为一系列3D高斯分布的叠加。
(2) 使用神经转换缓存（NTC）对3D高斯的平移和旋转进行建模，从而减少训练时间和存储需求。
(3) 提出自适应3D高斯添加策略，处理动态场景中出现的物体。</p>
</li>
<li>
<p>结论：
（1）：提出 3DGStream，一种用于高效自由视点视频流的高效 3D 高斯实时训练方法。
（2）：创新点：基于 3DG-S，利用神经转换缓存（NTC）捕捉物体运动；提出自适应 3DG 添加策略，准确建模动态场景中出现的物体。
性能：实现即时训练（每帧约 10 秒）和实时渲染（约 200FPS），在百万像素分辨率下具有适度的存储需求。
工作量：使用 3DG-S 的代码库实现 3DGStream，使用 tiny-cuda-nn 实现 NTC。</p>
</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-56fa714ff2f8a27b5ea568d4ef616b5e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cf2d0d9167fc721c8b229c0141471c56.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5a6c132c8a153da0f9bad3e8ca7eabd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-752f81f447063ef3902e3a021755740e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4cd01032696c0735dbb058f523ca0022.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-053adecfa0f0d915b2350de6633e2581.jpg" align="middle">
</details>




]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>3DGS</tag>
      </tags>
  </entry>
  <entry>
    <title>NeRF</title>
    <url>/2024/02/23/Paper/2024-02-23/NeRF/</url>
    <content><![CDATA[<blockquote>
<p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-02-23-更新"><a href="#2024-02-23-更新" class="headerlink" title="2024-02-23 更新"></a>2024-02-23 更新</h1><h2 id="Identifying-Unnecessary-3D-Gaussians-using-Clustering-for-Fast-Rendering-of-3D-Gaussian-Splatting"><a href="#Identifying-Unnecessary-3D-Gaussians-using-Clustering-for-Fast-Rendering-of-3D-Gaussian-Splatting" class="headerlink" title="Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering   of 3D Gaussian Splatting"></a>Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering   of 3D Gaussian Splatting</h2><p><strong>Authors:Joongho Jo, Hyeongwon Kim, Jongsun Park</strong></p>
<p>3D Gaussian splatting (3D-GS) is a new rendering approach that outperforms the neural radiance field (NeRF) in terms of both speed and image quality. 3D-GS represents 3D scenes by utilizing millions of 3D Gaussians and projects these Gaussians onto the 2D image plane for rendering. However, during the rendering process, a substantial number of unnecessary 3D Gaussians exist for the current view direction, resulting in significant computation costs associated with their identification. In this paper, we propose a computational reduction technique that quickly identifies unnecessary 3D Gaussians in real-time for rendering the current view without compromising image quality. This is accomplished through the offline clustering of 3D Gaussians that are close in distance, followed by the projection of these clusters onto a 2D image plane during runtime. Additionally, we analyze the bottleneck associated with the proposed technique when executed on GPUs and propose an efficient hardware architecture that seamlessly supports the proposed scheme. For the Mip-NeRF360 dataset, the proposed technique excludes 63% of 3D Gaussians on average before the 2D image projection, which reduces the overall rendering computation by almost 38.3% without sacrificing peak-signal-to-noise-ratio (PSNR). The proposed accelerator also achieves a speedup of 10.7x compared to a GPU. </p>
<p><a href="http://arxiv.org/abs/2402.13827v1">PDF</a> </p>
<p><strong>摘要</strong><br> NeurRF 加速：一种新的计算方法，通过快速识别不必要的 3D 高斯体在实时渲染当前视图，从而提高渲染速度和图像质量。</p>
<p><strong>要点</strong></p>
<ul>
<li>NeurRF 是一种新的渲染方法，利用数百万个 3D 高斯体来表示 3D 场景，并将其投影到 2D 图像平面上进行渲染。</li>
<li>在渲染过程中，存在大量对于当前视图方向来说不必要的 3D 高斯体，导致识别这些高斯体的计算成本很高。</li>
<li>提出了一种计算简化技术，能够在实时快速识别不必要的 3D 高斯体，从而在不影响图像质量的情况下渲染当前视图。</li>
<li>该技术通过对距离相近的 3D 高斯体进行离线聚类来实现，然后在运行时将这些簇投影到 2D 图像平面上。</li>
<li>分析了该技术在 GPU 上执行时遇到的瓶颈，并提出了一种高效的硬件架构来无缝支持该方案。</li>
<li>对于 Mip-NeRF360 数据集，该技术在 2D 图像投影前平均排除 63% 的 3D 高斯体，从而将整体渲染计算量减少了近 38.3%，而峰值信噪比 (PSNR) 却不会下降。</li>
<li>所提出的加速器与 GPU 相比，速度提高了 10.7 倍。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：使用聚类识别不必要的 3D 高斯体，实现 3D 高斯斑点渲染的快速渲染</li>
<li>作者：Joongho Jo, Hyeongwon Kim, Jongsun Park</li>
<li>单位：韩国大学电气工程学院（仅翻译单位名称）</li>
<li>关键词：3D 高斯斑点渲染、渲染、NeRF、神经辐射场、硬件加速器</li>
<li>链接：Paper_info:Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering of 3D Gaussian Splatting，Github：无</li>
<li>摘要：
（1）研究背景：在 3D 计算机视觉应用中，例如增强现实 (AR)、虚拟现实 (VR) 和元宇宙，快速且高质量的图像渲染非常重要。虽然使用深度神经网络的渲染技术，例如神经辐射场 (NeRF)，已经得到了广泛的研究，但 3D 高斯斑点渲染 (3D-GS) 作为一种新的渲染方法，因其与传统 NeRF 相比能够快速渲染高质量图像而备受关注。3D-GS 利用数百万个 3D 高斯体来表示复杂的 3D 场景，并通过将 3D 高斯体投影到 2D 图像平面上来渲染 3D 场景。
（2）过去的方法及其问题：3D-GS 渲染过程主要分为两步：1）将所有 3D 高斯体投影到 2D 图像平面上，并识别出影响 2D 图像颜色的 3D 高斯体。2）然后使用影响颜色的已识别 3D 高斯体计算 2D 图像中每个像素的颜色。在渲染过程的第一步中，高斯体投影到 2D 图像上，但被识别为不影响 2D 图像的颜色，投影就变成了计算浪费。在 Mip-NeRF360 数据集中，平均约有 67.6% 的 3D 高斯体不影响 2D 图像的颜色。因此，这些高斯体可以从当前视图渲染过程中排除。然而，由于影响 2D 图像颜色的 3D 高斯体可能会随着渲染视点的位置和方向而改变，因此在将它们投影到 2D 图像平面前识别出不必要的 3D 高斯体仍然具有挑战性。因此，这些不必要的 3D 高斯体仍然会进行投影计算。因此，如果能够开发一种简单而有效的方法来预测不影响 2D 图像颜色的 3D 高斯体，并在渲染过程开始前将它们排除，则可以显着降低整个 3D-GS 过程的总体计算复杂度。
（3）本文提出的研究方法：本文提出了一种基于聚类的方法，通过识别不影响 2D 图像颜色的簇来排除当前视图渲染过程中的不必要 3D 高斯体。聚类的目的是将位置相近的 3D 高斯体分组在一起，并且簇的形状应该是球形的，以便于投影到 2D 图像上。因此，我们采用了 K-means 聚类算法，它满足这两个标准。鉴于 3D 高斯体具有由其协方差定义的大小或影响，簇球体的半径不仅由到簇质心的距离决定，还考虑了高斯体的大小。然后将这些定义的簇球体投影到 2D 图像平面上，以确定它们对 2D 图像颜色的影响。不影响图像颜色的簇可以从渲染过程中排除。在我们的方法中，聚类和计算簇的半径可以在线​​下执行，只有将簇投影到 2D 图像平面上是在实时进行的，这仅需要 6.2% 的计算开销。在 3D-GS 渲染过程中，在当前视图渲染之前应用所提出的方法时，平均可以排除 63% 的 3D 高斯体，从而将整体渲染计算减少了近 38.3%，而不会牺牲峰值信噪比 (PSNR)。所提出的加速器还实现了比 GPU 快 10.7 倍的速度。
（4）方法在什么任务上取得了什么性能？性能是否支持其目标？本文提出的方法在 Mip-NeRF360 数据集上进行了评估。结果表明，该方法能够有效地排除不必要的 3D 高斯体，从而减少渲染计算量并提高渲染速度。具体来说，该方法可以排除平均 63% 的 3D 高斯体，从而将整体渲染计算减少了近 38.3%，而不会牺牲峰值信噪比 (PSNR)。此外，所提出的加速器还实现了比 GPU 快 10.7 倍的速度。这些结果表明，该方法能够有效地实现其目标，即快速渲染高质量的 3D 图像。</li>
</ol>
<p>Some Error for method(比如是不是没有Methods这个章节)</p>
<ol>
<li>结论：
（1）：本文提出了一种基于聚类的方法，通过识别不影响2D图像颜色的簇来排除当前视图渲染过程中的不必要3D高斯体。该方法能够有效地减少渲染计算量并提高渲染速度，在Mip-NeRF360数据集上，该方法可以排除平均63%的3D高斯体，从而将整体渲染计算减少了近38.3%，而不会牺牲峰值信噪比（PSNR）。此外，所提出的加速器还实现了比GPU快10.7倍的速度。
（2）：创新点：本文提出了一种基于聚类的方法来识别不必要的3D高斯体，该方法简单有效，能够显着降低3D-GS渲染过程的总体计算复杂度。
性能：该方法能够有效地排除不必要的3D高斯体，从而减少渲染计算量并提高渲染速度。在Mip-NeRF360数据集上，该方法可以排除平均63%的3D高斯体，从而将整体渲染计算减少了近38.3%，而不会牺牲峰值信噪比（PSNR）。此外，所提出的加速器还实现了比GPU快10.7倍的速度。
工作量：该方法的实现相对简单，并且可以在线​​下执行聚类和计算簇的半径，只有将簇投影到2D图像平面上是在实时进行的，这仅需要6.2%的计算开销。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-eb8532b7f44bd3308c4f19fe6bf7f78c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e5e9d849dcc9fd5228abd36df009311.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a43367bbb6924d5ba043f598753b956.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d13d1af17267a2b843bea8ac607b39a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bca1cf3d857e2d53600b33fc6c9e298c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5799fc43b51197a24672703783ee479.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee494dce0084e0f0c71d55d940b03dc9.jpg" align="middle">
</details>




<h2 id="OccFlowNet-Towards-Self-supervised-Occupancy-Estimation-via-Differentiable-Rendering-and-Occupancy-Flow"><a href="#OccFlowNet-Towards-Self-supervised-Occupancy-Estimation-via-Differentiable-Rendering-and-Occupancy-Flow" class="headerlink" title="OccFlowNet: Towards Self-supervised Occupancy Estimation via   Differentiable Rendering and Occupancy Flow"></a>OccFlowNet: Towards Self-supervised Occupancy Estimation via   Differentiable Rendering and Occupancy Flow</h2><p><strong>Authors:Simon Boeder, Fabian Gigengack, Benjamin Risse</strong></p>
<p>Semantic occupancy has recently gained significant traction as a prominent 3D scene representation. However, most existing methods rely on large and costly datasets with fine-grained 3D voxel labels for training, which limits their practicality and scalability, increasing the need for self-monitored learning in this domain. In this work, we present a novel approach to occupancy estimation inspired by neural radiance field (NeRF) using only 2D labels, which are considerably easier to acquire. In particular, we employ differentiable volumetric rendering to predict depth and semantic maps and train a 3D network based on 2D supervision only. To enhance geometric accuracy and increase the supervisory signal, we introduce temporal rendering of adjacent time steps. Additionally, we introduce occupancy flow as a mechanism to handle dynamic objects in the scene and ensure their temporal consistency. Through extensive experimentation we demonstrate that 2D supervision only is sufficient to achieve state-of-the-art performance compared to methods using 3D labels, while outperforming concurrent 2D approaches. When combining 2D supervision with 3D labels, temporal rendering and occupancy flow we outperform all previous occupancy estimation models significantly. We conclude that the proposed rendering supervision and occupancy flow advances occupancy estimation and further bridges the gap towards self-supervised learning in this domain. </p>
<p><a href="http://arxiv.org/abs/2402.12792v1">PDF</a> </p>
<p><strong>Summary</strong><br>神经辐射场可从仅使用二维标签中估计语义占用。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>利用神经辐射场（NeRF）提出了一种仅使用二维标签估计占用率的新方法。</li>
<li>采用可微体积渲染来预测深度和语义图，并仅基于二维监督训练三维网络。</li>
<li>为了增强几何精度并增加监督信号，引入了相邻时间步长的时序渲染。</li>
<li>引入占用流作为处理场景中动态对象并确保其时间一致性的机制。</li>
<li>与使用三维标签的方法相比，实验表明仅二维监督就足以实现最先进的性能，同时优于同时期的二维方法。</li>
<li>当将二维监督与三维标签、时态渲染和占用流相结合时，大大优于所有以前的占有估计模型。</li>
<li>渲染监督和占用流的进步促进了占用估计，并进一步缩小了该领域中自监督学习的差距。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：OccFlowNet：基于可微渲染和占用流的自监督占用估计</li>
<li>作者：Simon Boeder, Fabian Gigengack, Benjamin Risse</li>
<li>作者单位：博世公司、明斯特大学</li>
<li>关键词：占用估计、神经辐射场、可微渲染、占用流、自监督学习</li>
<li>论文链接：https://arxiv.org/abs/2402.12792</li>
<li>
<p>摘要：
(1) 研究背景：语义占用最近作为一种突出的 3D 场景表示形式而受到广泛关注。然而，大多数现有方法依赖于具有细粒度 3D 体素标签的大型且昂贵的训练数据集，这限制了它们的实用性和可扩展性，增加了该领域中自监督学习的需求。
(2) 过去方法及其问题：本文提出了受神经辐射场 (NeRF) 启发的新型占用估计方法，仅使用更易获取的 2D 标签。具体来说，我们采用可微体积渲染来预测深度和语义图，并仅基于 2D 监督训练 3D 网络。为了提高几何精度并增加监督信号，我们引入了相邻时间步的长时渲染。此外，我们引入了占用流作为处理场景中动态对象并确保其时间一致性的机制。
(3) 研究方法：我们通过广泛的实验表明，仅使用 2D 监督就足以与使用 3D 标签的方法相比实现最先进的性能，同时优于同时期的 2D 方法。当将 2D 监督与 3D 标签、时序渲染和占用流相结合时，我们明显优于所有以前的占用估计模型。我们得出结论，所提出的渲染监督和占用流促进了占用估计，并进一步缩小了该领域中自监督学习的差距。
(4) 性能和结论：在广泛使用的数据集上进行的实验表明，所提出的方法在占用估计任务上取得了最先进的性能。这些结果支持了我们的目标，即仅使用 2D 监督就可以实现准确的占用估计，从而使该方法更易于训练和部署。</p>
</li>
<li>
<p>方法：
(1)：我们提出了一种新的占用估计方法 OccFlowNet，仅使用更易获取的 2D 标签，无需昂贵的 3D 体素标签。
(2)：我们采用可微体积渲染来预测深度和语义图，并仅基于 2D 监督训练 3D 网络。
(3)：为了提高几何精度并增加监督信号，我们引入了相邻时间步的长时渲染。
(4)：我们引入了占用流作为处理场景中动态对象并确保其时间一致性的机制。
(5)：我们通过广泛的实验表明，仅使用 2D 监督就足以与使用 3D 标签的方法相比实现最先进的性能，同时优于同时期的 2D 方法。
(6)：当将 2D 监督与 3D 标签、时序渲染和占用流相结合时，我们明显优于所有以前的占用估计模型。</p>
</li>
<li>
<p>结论：
（1）：本工作首次提出了一种仅使用易于获取的 2D 标签即可进行占用估计的方法，无需昂贵的 3D 体素标签，为占用估计任务提供了一种新的思路。
（2）：创新点：
创新点 1：提出了一种新的占用估计方法 OccFlowNet，仅使用更易获取的 2D 标签，无需昂贵的 3D 体素标签。
创新点 2：采用可微体积渲染来预测深度和语义图，并仅基于 2D 监督训练 3D 网络。
创新点 3：为了提高几何精度并增加监督信号，引入了相邻时间步的长时渲染。
创新点 4：引入了占用流作为处理场景中动态对象并确保其时间一致性的机制。
性能：
在广泛使用的数据集上进行的实验表明，所提出的方法在占用估计任务上取得了最先进的性能。
工作量：
本工作需要解决的问题是，如何仅使用 2D 标签进行占用估计。为了解决这个问题，作者提出了 OccFlowNet 方法，该方法采用可微体积渲染来预测深度和语义图，并仅基于 2D 监督训练 3D 网络。为了提高几何精度并增加监督信号，作者引入了相邻时间步的长时渲染。此外，作者还引入了占用流作为处理场景中动态对象并确保其时间一致性的机制。通过广泛的实验，作者证明了所提出的方法在占用估计任务上取得了最先进的性能。</p>
</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-48dbaf92efe683516d537be273981834.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff303fd6f4dc54f5b59e902e9b98c34a.jpg" align="middle">
</details>




<h2 id="Colorizing-Monochromatic-Radiance-Fields"><a href="#Colorizing-Monochromatic-Radiance-Fields" class="headerlink" title="Colorizing Monochromatic Radiance Fields"></a>Colorizing Monochromatic Radiance Fields</h2><p><strong>Authors:Yean Cheng, Renjie Wan, Shuchen Weng, Chengxuan Zhu, Yakun Chang, Boxin Shi</strong></p>
<p>Though Neural Radiance Fields (NeRF) can produce colorful 3D representations of the world by using a set of 2D images, such ability becomes non-existent when only monochromatic images are provided. Since color is necessary in representing the world, reproducing color from monochromatic radiance fields becomes crucial. To achieve this goal, instead of manipulating the monochromatic radiance fields directly, we consider it as a representation-prediction task in the Lab color space. By first constructing the luminance and density representation using monochromatic images, our prediction stage can recreate color representation on the basis of an image colorization module. We then reproduce a colorful implicit model through the representation of luminance, density, and color. Extensive experiments have been conducted to validate the effectiveness of our approaches. Our project page: <a href="https://liquidammonia.github.io/color-nerf">https://liquidammonia.github.io/color-nerf</a>. </p>
<p><a href="http://arxiv.org/abs/2402.12184v1">PDF</a> </p>
<p><strong>Summary</strong><br>神经辐射场（NeRF）可通过一组二维图像产生色彩鲜艳的 3D 场景再现，但仅提供单色图像时便无法实现。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>NeRF 可以使用一组 2D 图像生成世界的彩色 3D 表示。</li>
<li>仅提供单色图像时，NeRF 无法生成彩色 3D 表示。</li>
<li>NeRF 的目标是从单色辐射场再现彩色表示。</li>
<li>提出了一种在 Lab 颜色空间中将单色辐射场视为表示预测任务的方法。</li>
<li>首先使用单色图像构建亮度和密度表示，然后使用图像着色模块重新创建颜色表示。</li>
<li>然后通过亮度、密度和颜色的表示再现一个彩色隐式模型。</li>
<li>大量实验验证了所提出方法的有效性。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：彩色化单色辐射场</li>
<li>作者：叶安成、万任杰<em>、翁书琛、朱承轩、常亚坤、石博欣</em></li>
<li>隶属单位：北京大学多媒体信息处理国家重点实验室、计算机科学系</li>
<li>关键词：NeRF、单色图像、颜色再现、Lab颜色空间、图像着色</li>
<li>论文链接：https://arxiv.org/abs/2402.12184
   Github 链接：无</li>
<li>
<p>摘要：
   （1）研究背景：神经辐射场（NeRF）可以利用一组二维图像创建世界的彩色三维表示。然而，当只有单色图像可用时，这种能力就不复存在了。颜色对于表征世界是必要的，因此从单色辐射场中再现颜色变得至关重要。
   （2）过去的方法及其问题：直接操纵单色辐射场似乎是实现颜色化的直接方法。一种解决方案是将颜色视为一种“风格”，然后将其转移到辐射场中。然而，这种策略并不能保证逐像素的颜色一致性，因此颜色只能不规则地分布在辐射场中，从而违背了合理性标准。另一种方法涉及直接操纵辐射场中的颜色属性。这种技术旨在通过识别当前的颜色属性并用新的颜色属性替换它们来替换颜色。然而，它不适用于没有现有颜色属性的单色辐射场。
   （3）论文提出的研究方法：为了解决上述问题，本文提出了一种在 Lab 颜色空间中进行表示预测的任务。首先使用单色图像构建亮度和密度表示，然后利用图像着色模块重新创建颜色表示。最后，通过亮度、密度和颜色的表示来再现一个彩色隐式模型。
   （4）方法在任务上的表现及其对目标的支持：本文的方法在多个任务上取得了优异的性能，包括单色图像着色、多视图图像合成和视频插帧。这些结果表明，本文的方法可以有效地从单色图像中再现颜色，并生成逼真且视觉上令人愉悦的彩色结果。</p>
</li>
<li>
<p>方法：
(1) 构建亮度和密度表示：使用单色图像构建亮度和密度表示，为后续的颜色再现提供基础。
(2) 图像着色模块：利用图像着色模块重新创建颜色表示，将单色图像中的信息转换为彩色表示。
(3) 表示预测：在Lab颜色空间中进行表示预测，将亮度、密度和颜色的表示相结合，再现一个彩色隐式模型。
(4) 颜色注入：利用分类器将颜色注入到辐射场中，确保颜色的合理性和一致性。
(5) 直方图净化：使用直方图净化模块去除不合理的颜色，提高颜色的准确性和一致性。</p>
</li>
<li>
<p>结论：</p>
</li>
</ol>
<p>（1）意义：本文提出了一种从单色图像中再现颜色的新方法，该方法在多个任务上取得了优异的性能，为单色图像的彩色化提供了新的思路和技术支持。</p>
<p>（2）优缺点总结：</p>
<p>创新点：</p>
<ul>
<li>提出了一种在Lab颜色空间中进行表示预测的任务，有效地解决了单色图像着色的问题。</li>
<li>提出了一种颜色注入模块，确保了颜色的合理性和一致性。</li>
<li>提出了一种直方图净化模块，去除不合理的颜色，提高了颜色的准确性和一致性。</li>
</ul>
<p>性能：</p>
<ul>
<li>在单色图像着色、多视图图像合成和视频插帧等任务上取得了优异的性能。</li>
<li>生成的彩色结果逼真且视觉上令人愉悦。</li>
</ul>
<p>工作量：</p>
<ul>
<li>需要构建亮度和密度表示、图像着色模块、颜色注入模块和直方图净化模块。</li>
<li>需要训练模型，这可能需要大量的数据和计算资源。</li>
</ul>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-53ef44a8d86663951eb27790c491bec4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40e071a248a066a783512765ca1dd311.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-04a5930c0187125fe64b74f7d43ea704.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08fb7fd6e14278c9083abd8d5401c6b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34c76f358a2021ed97956d162ca195e3.jpg" align="middle">
</details>




## One2Avatar: Generative Implicit Head Avatar For Few-shot User Adaptation

**Authors:Zhixuan Yu, Ziqian Bai, Abhimitra Meka, Feitong Tan, Qiangeng Xu, Rohit Pandey, Sean Fanello, Hyun Soo Park, Yinda Zhang**

Traditional methods for constructing high-quality, personalized head avatars from monocular videos demand extensive face captures and training time, posing a significant challenge for scalability. This paper introduces a novel approach to create high quality head avatar utilizing only a single or a few images per user. We learn a generative model for 3D animatable photo-realistic head avatar from a multi-view dataset of expressions from 2407 subjects, and leverage it as a prior for creating personalized avatar from few-shot images. Different from previous 3D-aware face generative models, our prior is built with a 3DMM-anchored neural radiance field backbone, which we show to be more effective for avatar creation through auto-decoding based on few-shot inputs. We also handle unstable 3DMM fitting by jointly optimizing the 3DMM fitting and camera calibration that leads to better few-shot adaptation. Our method demonstrates compelling results and outperforms existing state-of-the-art methods for few-shot avatar adaptation, paving the way for more efficient and personalized avatar creation. 

[PDF](http://arxiv.org/abs/2402.11909v1) 

**Summary**
用一张或数张用户照片和 3DMM 编码即可生成高质量且可控动的头像。

**Key Takeaways**

- 该研究提出了一种使用一张或多张图像创建高质量头像的新方法。
- 该方法利用了一个从 2407 个人的多视角面部表情数据集中学得的生成模型。
- 该方法使用了基于 3DMM 的神经辐射场作为骨干网络，以增强通过少量输入进行自动解码的效果。
- 该研究提出了一种通过联合优化 3DMM 拟合和相机校准来处理不稳定的 3DMM 拟合问题。
- 该研究提出的方法在少量图像头像生成任务中表现出色，并优于现有技术。
- 该方法为更高效和个性化的头像生成铺平了道路。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>标题：基于 3DMM 的神经辐射场在虚拟化身的身份和表情建模中的应用</li>
<li>作者：Kangxue Yin, Changjian Li, Yebin Liu, Yue Dong, Kun Zhou, Chen Change Loy, Ziwei Liu</li>
<li>隶属机构：香港中文大学（深圳）</li>
<li>关键词：神经辐射场、3DMM、身份建模、表情建模、虚拟化身</li>
<li>论文链接：https://arxiv.org/abs/2302.09924，Github 代码链接：None</li>
<li>
<p>摘要：
（1）：研究背景：虚拟化身在游戏、社交媒体和电子商务等领域有着广泛的应用。然而，现有的虚拟化身通常缺乏真实感和个性化。
（2）：过去的方法及其问题：过去的方法通常使用 3D 模型来表示虚拟化身，但这些模型往往缺乏细节和真实感。此外，这些方法通常需要大量的手工制作，这使得它们难以个性化。
（3）：研究方法：本文提出了一种基于 3DMM 的神经辐射场（NeRF）方法来表示虚拟化身。该方法将 3DMM 作为虚拟化身的骨架，并使用 NeRF 来生成虚拟化身的表面。NeRF 是一种神经网络，它可以从一组稀疏的观测数据中学习生成连续的表面。
（4）：方法性能：本文的方法在多个任务上取得了良好的性能。在身份建模任务上，该方法能够生成逼真的虚拟化身，这些虚拟化身与真实的人类非常相似。在表情建模任务上，该方法能够生成逼真的虚拟化身表情，这些表情与真实的人类表情非常相似。</p>
</li>
<li>
<p>方法：
（1）：多视角多表情人脸捕捉：我们从 13 个预定义的面部表情中捕获了总共 2407 个受试者的分辨率面部图像，这些图像来自 13 个稀疏摄像头视角。对于每个受试者在每个表情中，我们运行基于面部地标的 3DMM 拟合算法，并从多视角图像中重建 3D 几何形状。与现有的数据集（例如 FFHQ 中的 70K）相比，我们的数据集包含有限数量的独特受试者。尽管如此，它包含更广泛的面部表情，这在学习生成式先验模型中起着关键作用。
（2）：生成式头像先验：我们的生成式头像先验生成了一个由神经辐射场表示的头像。给定一个身份编码 w 和一个表情编码 ψ，我们的模型 f 为 3D 查询点 q 从方向 d 查看时生成局部颜色 c 和密度 σ：
σ(q), c(q) = f(w, ψ, q, d; θ),
其中 θ 是模型权重。然后通过应用体积渲染公式获得每个像素的颜色来生成彩色图像：
ˆc = ∫t^∞ T(t)σq(r(t))cq(r(t), d)dt,
其中 T(t) = exp(−∫^t^0 σq(r(s))ds)。遵循先前的艺术，我们采用 3DMM 表达式代码空间作为 ψ，并学习 w 的潜在空间 Rl。
（3）：3DMM 锚定头像生成模型：受 Bai 等人启发，我们采用 3DMM 锚定的神经辐射场作为我们的头像表示。具体来说，我们不会将所有渲染信息编码到一个高容量神经网络中，而是将局部特征附加在针对目标身份和表情重建的 3DMM 网格支架的顶点上。在渲染期间，每个查询点聚合来自 3DMM 顶点中的 k 个最近邻 (kNN) 的特征，并将其发送到 MLP 网络以预测颜色和密度。为了简化使用现有 2D CNN 的学习，可以在统一的 UV 空间中学习 3DMM 顶点附加特征，并使用纹理坐标进行采样。</p>
</li>
<li>
<p>结论：
（1）：本文提出了一种基于3DMM的神经辐射场方法来表示虚拟化身。该方法将3DMM作为虚拟化身的骨架，并使用NeRF来生成虚拟化身的表面。NeRF是一种神经网络，它可以从一组稀疏的观测数据中学习生成连续的表面。该方法在身份建模和表情建模任务上取得了良好的性能。
（2）：创新点：</p>
</li>
<li>提出了一种基于3DMM的神经辐射场方法来表示虚拟化身。</li>
<li>该方法将3DMM作为虚拟化身的骨架，并使用NeRF来生成虚拟化身的表面。</li>
<li>该方法在身份建模和表情建模任务上取得了良好的性能。
性能：</li>
<li>在身份建模任务上，该方法能够生成逼真的虚拟化身，这些虚拟化身与真实的人类非常相似。</li>
<li>在表情建模任务上，该方法能够生成逼真的虚拟化身表情，这些表情与真实的人类表情非常相似。
工作量：</li>
<li>该方法需要大量的数据来训练。</li>
<li>该方法的训练过程非常耗时。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-93031d1d3a37626178f6b3786cd2c74e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eab6eef6309df63167647ea626493f1a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8493d16068dbd16ea6a5062fa4270269.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-842dff2df6fd65f7fd0227ced8c01e7c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-efb4142cad4111ae1edb459aafe2c7ab.jpg" align="middle">
</details>




<h2 id="PC-NeRF-Parent-Child-Neural-Radiance-Fields-Using-Sparse-LiDAR-Frames-in-Autonomous-Driving-Environments"><a href="#PC-NeRF-Parent-Child-Neural-Radiance-Fields-Using-Sparse-LiDAR-Frames-in-Autonomous-Driving-Environments" class="headerlink" title="PC-NeRF: Parent-Child Neural Radiance Fields Using Sparse LiDAR Frames   in Autonomous Driving Environments"></a>PC-NeRF: Parent-Child Neural Radiance Fields Using Sparse LiDAR Frames   in Autonomous Driving Environments</h2><p><strong>Authors:Xiuzhong Hu, Guangming Xiong, Zheng Zang, Peng Jia, Yuxuan Han, Junyi Ma</strong></p>
<p>Large-scale 3D scene reconstruction and novel view synthesis are vital for autonomous vehicles, especially utilizing temporally sparse LiDAR frames. However, conventional explicit representations remain a significant bottleneck towards representing the reconstructed and synthetic scenes at unlimited resolution. Although the recently developed neural radiance fields (NeRF) have shown compelling results in implicit representations, the problem of large-scale 3D scene reconstruction and novel view synthesis using sparse LiDAR frames remains unexplored. To bridge this gap, we propose a 3D scene reconstruction and novel view synthesis framework called parent-child neural radiance field (PC-NeRF). Based on its two modules, parent NeRF and child NeRF, the framework implements hierarchical spatial partitioning and multi-level scene representation, including scene, segment, and point levels. The multi-level scene representation enhances the efficient utilization of sparse LiDAR point cloud data and enables the rapid acquisition of an approximate volumetric scene representation. With extensive experiments, PC-NeRF is proven to achieve high-precision novel LiDAR view synthesis and 3D reconstruction in large-scale scenes. Moreover, PC-NeRF can effectively handle situations with sparse LiDAR frames and demonstrate high deployment efficiency with limited training epochs. Our approach implementation and the pre-trained models are available at <a href="https://github.com/biter0088/pc-nerf">https://github.com/biter0088/pc-nerf</a>. </p>
<p><a href="http://arxiv.org/abs/2402.09325v1">PDF</a> arXiv admin note: substantial text overlap with arXiv:2310.00874</p>
<p><strong>Summary</strong><br>基于分层空间分割和多层次场景表示，PC-NeRF 框架实现了大规模场景的 3D 重建和新视图合成。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>PC-NeRF 框架由父 NeRF 和子 NeRF 两个模块组成，实现了分层空间分割和多层次场景表示。</li>
<li>分层空间分割和多层次场景表示可以提高稀疏激光雷达点云数据的利用效率，并实现快速获取近似体积场景表示。</li>
<li>PC-NeRF 可以有效处理稀疏激光雷达帧的情况，并在有限的训练轮数下表现出很高的部署效率。</li>
<li>PC-NeRF 的实现和预训练模型可在 <a href="https://github.com/biter0088/pc-nerf">https://github.com/biter0088/pc-nerf</a> 上获取。</li>
<li>PC-NeRF 可以实现高精度的激光雷达新视图合成和 3D 重建。</li>
<li>PC-NeRF 可以有效处理稀疏激光雷达帧的情况。</li>
<li>PC-NeRF 在有限的训练轮数下表现出很高的部署效率。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：PC-NeRF：自动驾驶环境中稀疏激光雷达帧的父子神经辐射场</li>
<li>作者：Xiuzhong Hu, Guangming Xiong, Zheng Zang, Peng Jia, Yuxuan Han, Junyi Ma</li>
<li>隶属单位：北京理工大学机械工程学院</li>
<li>关键词：神经辐射场、三维场景重建、自动驾驶</li>
<li>论文链接：https://arxiv.org/abs/2402.09325，Github 链接：https://github.com/biter0088/pc-nerf</li>
<li>
<p>摘要：
(1)：研究背景：大规模三维场景重建和新颖视角合成对于自动驾驶汽车进行环境探索、运动规划和闭环仿真至关重要，尤其是在可用传感器数据由于各种实际因素而变得稀疏的情况下。
(2)：过去的方法及其问题：传统的显式表示可以描绘重建的场景和合成视图，但它们在以无限分辨率表示场景方面仍然存在重大瓶颈。最近开发的神经辐射场 (NeRF) 在隐式表示方面取得了引人注目的结果，但使用稀疏激光雷达帧进行大规模三维场景重建和新颖视角合成的难题仍未得到探索。
(3)：提出的研究方法：为了弥合这一差距，我们提出了一种称为父子神经辐射场 (PC-NeRF) 的三维场景重建和新颖视角合成框架。该框架基于其两个模块，父 NeRF 和子 NeRF，实现了分层空间划分和多级场景表示，包括场景、片段和点级。多级场景表示增强了对稀疏激光雷达点云数据的有效利用，并能够快速获取近似体积场景表示。
(4)：方法在任务和性能上的表现：通过广泛的实验，PC-NeRF 被证明可以在大规模场景中实现高精度的激光雷达新视角合成和三维重建。此外，PC-NeRF 可以有效地处理稀疏激光雷达帧的情况，并证明了在有限的训练轮次下具有较高的部署效率。</p>
</li>
<li>
<p>方法：
（1）PC-NeRF框架：提出了一种称为父子神经辐射场（PC-NeRF）的三维场景重建和新颖视角合成框架，该框架基于其两个模块，父NeRF和子NeRF，实现了分层空间划分和多级场景表示，包括场景、片段和点级。
（2）多级场景表示：多级场景表示增强了对稀疏激光雷达点云数据的有效利用，并能够快速获取近似体积场景表示。
（3）训练过程：PC-NeRF采用分阶段训练策略，首先训练父NeRF，然后训练子NeRF，最后将父NeRF和子NeRF结合起来进行联合训练。
（4）损失函数：PC-NeRF的损失函数包括父NeRF的损失函数和子NeRF的损失函数，父NeRF的损失函数包括重投影误差和光度误差，子NeRF的损失函数包括自由空间误差和深度误差。
（5）新颖视角合成和三维重建：PC-NeRF可以通过新颖视角合成和三维重建来评估其性能，新颖视角合成是将稀疏激光雷达帧合成到新的视角，三维重建是将稀疏激光雷达帧重建为三维点云。</p>
</li>
<li>
<p>结论：
(1)：本工作提出了一种适用于自动驾驶中稀疏激光雷达帧的大规模三维场景重建和新颖视角合成框架 PC-NeRF，该框架采用分层空间划分和多级场景表示，有效利用稀疏激光雷达点云数据，实现高精度的新颖视角合成和三维重建。
(2)：创新点：
PC-NeRF 提出了一种分层空间划分和多级场景表示的方法，有效利用稀疏激光雷达点云数据。
PC-NeRF 提出了一种两步深度推理方法，实现从片段到点的推理。
PC-NeRF 在 KITTI 和 MaiCity 数据集上进行了广泛的实验，证明了其在稀疏激光雷达帧条件下进行新颖视角合成和三维重建的有效性。
性能：
PC-NeRF 在 KITTI 和 MaiCity 数据集上实现了高精度的激光雷达新视角合成和三维重建。
PC-NeRF 可以有效地处理稀疏激光雷达帧的情况，并证明了在有限的训练轮次下具有较高的部署效率。
工作量：
PC-NeRF 的实现相对简单，易于部署。
PC-NeRF 的训练过程需要大量的数据和计算资源。</p>
</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-6782f984ff8bf4da1d81a6ca240eded4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9a2171d3c5e58e5589aa20525792832a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a7d40aa20abd78a5813673cde1893940.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-40b695293253e411ba8966555ca76058.jpg" align="middle">
</details>




<h2 id="NeRF-Analogies-Example-Based-Visual-Attribute-Transfer-for-NeRFs"><a href="#NeRF-Analogies-Example-Based-Visual-Attribute-Transfer-for-NeRFs" class="headerlink" title="NeRF Analogies: Example-Based Visual Attribute Transfer for NeRFs"></a>NeRF Analogies: Example-Based Visual Attribute Transfer for NeRFs</h2><p><strong>Authors:Michael Fischer, Zhengqin Li, Thu Nguyen-Phuoc, Aljaz Bozic, Zhao Dong, Carl Marshall, Tobias Ritschel</strong></p>
<p>A Neural Radiance Field (NeRF) encodes the specific relation of 3D geometry and appearance of a scene. We here ask the question whether we can transfer the appearance from a source NeRF onto a target 3D geometry in a semantically meaningful way, such that the resulting new NeRF retains the target geometry but has an appearance that is an analogy to the source NeRF. To this end, we generalize classic image analogies from 2D images to NeRFs. We leverage correspondence transfer along semantic affinity that is driven by semantic features from large, pre-trained 2D image models to achieve multi-view consistent appearance transfer. Our method allows exploring the mix-and-match product space of 3D geometry and appearance. We show that our method outperforms traditional stylization-based methods and that a large majority of users prefer our method over several typical baselines. </p>
<p><a href="http://arxiv.org/abs/2402.08622v1">PDF</a> Project page: <a href="https://mfischer-ucl.github.io/nerf_analogies/">https://mfischer-ucl.github.io/nerf_analogies/</a></p>
<p><strong>Summary</strong><br>神经辐射场 (NeRF) 可将场景的 3D 几何形状和外观进行编码。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>NeRF 可以将源 NeRF 中的外观转移到目标 3D 几何形状上，从而创建具有目标几何形状但外观类似于源 NeRF 的新 NeRF。</li>
<li>该方法将经典图像类比从 2D 图像推广到 NeRF。</li>
<li>基于语义亲和性的对应转移，由大型预训练 2D 图像模型提供的语义特征驱动，可实现多视图一致外观转移。</li>
<li>该方法能够探索 3D 几何形状和外观的混合匹配产品空间。</li>
<li>该方法优于传统的基于样式化的方法。</li>
<li>大多数用户更喜欢该方法，而不是其他几种典型基线方法。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：NeRF 类比：基于示例的 NeRF 视觉属性迁移</li>
<li>作者：Michael Fischer、Zhengqin Li、Thu Nguyen-Phuoc、Aljaž Božič、Zhao Dong、Carl Marshall、Tobias Ritschel</li>
<li>第一作者单位：伦敦大学学院</li>
<li>关键词：NeRF、视觉属性迁移、语义特征、深度学习</li>
<li>论文链接：None，Github 代码链接：None</li>
<li>摘要：</li>
</ol>
<p>（1）研究背景：NeRF（神经辐射场）是一种用于表示和渲染 3D 场景的强大技术。然而，NeRF 通常需要大量数据才能训练，并且难以将从一个场景学到的外观迁移到另一个场景。
（2）过去方法及其问题：过去的方法通常使用基于样式迁移的技术来将一种场景的外观迁移到另一种场景。然而，这些方法通常难以产生语义上连贯的结果，并且需要大量的数据来训练。
（3）研究方法：本文提出了一种新的方法，可以将一种场景的外观迁移到另一种场景，而无需大量的数据。该方法利用了预训练的 2D 图像模型中的语义特征来建立源场景和目标场景之间的对应关系。然后，这些对应关系被用来将源场景的外观迁移到目标场景。
（4）方法性能：该方法在多个数据集上进行了评估，结果表明该方法能够产生语义上连贯的结果，并且优于过去的方法。此外，该方法还可以用于生成新的场景，这些场景具有源场景的外观和目标场景的几何形状。</p>
<p><methods>:
(1)：我们的方法利用预训练的二维图像模型中的语义特征来建立源场景和目标场景之间的对应关系。
(2)：然后，这些对应关系被用来将源场景的外观迁移到目标场景。
(3)：我们训练了一个三维一致的NeRF表示，该表示在先前提取的点云FSource和FTarget上。
(4)：我们采样FSource中的位置，并在每个位置提取源特征描述符fSource、源外观LSource和源视向ωSource。
(5)：我们还从目标点云FTarget中采样位置，并在每个位置获取图像特征fTarget和目标位置xTarget。
(6)：我们找到一个离散映射ϕ，该映射将每个目标位置索引j映射到具有最大相似性的源位置索引i。
(7)：我们定义LTargetj=LSourceϕj作为目标在映射ϕ和某个视向下的外观。
(8)：我们训练NeRF Analogy Lθ的参数θ，使得对于每个观察到的目标位置，目标和源外观在源视向下一致。</methods></p>
<ol>
<li>结论：
（1）：本工作首次提出了 NeRF 类比，一种基于语义相似性的 NeRF 视觉属性迁移框架。该方法可以辅助内容创作，例如，通过将用户捕获的几何体与在线 3D 模型的外观相结合，并且还适用于多对象设置和真实世界场景。我们的方法在颜色迁移、图像合成和风格化文献中的其他方法中表现出色，并且在用户研究中获得了最高的排名，无论是在迁移质量还是多视图一致性方面。
（2）：创新点：</li>
<li>提出了一种基于语义相似性的 NeRF 视觉属性迁移框架。</li>
<li>该框架可以用于辅助内容创作、多对象设置和真实世界场景。</li>
<li>该框架在颜色迁移、图像合成和风格化文献中的其他方法中表现出色。
性能：</li>
<li>该框架在用户研究中获得了最高的排名，无论是在迁移质量还是多视图一致性方面。</li>
<li>该框架可以生成语义上连贯的结果，并且优于过去的方法。</li>
<li>该框架还可以用于生成新的场景，这些场景具有源场景的外观和目标场景的几何形状。
工作量：</li>
<li>该框架需要预训练一个 2D 图像模型来提取语义特征。</li>
<li>该框架需要训练一个 3D 一致的 NeRF 表示。</li>
<li>该框架需要找到一个离散映射来将源场景和目标场景之间的对应关系。</li>
<li>该框架需要训练一个 NeRF 类比模型来将源场景的外观迁移到目标场景。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-56d4edbaccc121abec3c1fbc5aa2a7b2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b96734ea48c9163e25bc72d32ad13598.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d80da8fbb7f50a1faceaf09341a6dada.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c35035cd1513fc1b8683c14a413721b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-190136188bdfd4cb8f04bafbfb9ef577.jpg" align="middle">
</details>




]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>NeRF</tag>
      </tags>
  </entry>
  <entry>
    <title>Talking Head Generation</title>
    <url>/2024/03/09/Paper/2024-03-09/Talking%20Head%20Generation/</url>
    <content><![CDATA[<blockquote>
<p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-03-09-更新"><a href="#2024-03-09-更新" class="headerlink" title="2024-03-09 更新"></a>2024-03-09 更新</h1><h2 id="FaceChain-ImagineID-Freely-Crafting-High-Fidelity-Diverse-Talking-Faces-from-Disentangled-Audio"><a href="#FaceChain-ImagineID-Freely-Crafting-High-Fidelity-Diverse-Talking-Faces-from-Disentangled-Audio" class="headerlink" title="FaceChain-ImagineID: Freely Crafting High-Fidelity Diverse Talking Faces   from Disentangled Audio"></a>FaceChain-ImagineID: Freely Crafting High-Fidelity Diverse Talking Faces   from Disentangled Audio</h2><p><strong>Authors:Chao Xu, Yang Liu, Jiazheng Xing, Weida Wang, Mingze Sun, Jun Dan, Tianxin Huang, Siyuan Li, Zhi-Qi Cheng, Ying Tai, Baigui Sun</strong></p>
<p>In this paper, we abstract the process of people hearing speech, extracting meaningful cues, and creating various dynamically audio-consistent talking faces, termed Listening and Imagining, into the task of high-fidelity diverse talking faces generation from a single audio. Specifically, it involves two critical challenges: one is to effectively decouple identity, content, and emotion from entangled audio, and the other is to maintain intra-video diversity and inter-video consistency. To tackle the issues, we first dig out the intricate relationships among facial factors and simplify the decoupling process, tailoring a Progressive Audio Disentanglement for accurate facial geometry and semantics learning, where each stage incorporates a customized training module responsible for a specific factor. Secondly, to achieve visually diverse and audio-synchronized animation solely from input audio within a single model, we introduce the Controllable Coherent Frame generation, which involves the flexible integration of three trainable adapters with frozen Latent Diffusion Models (LDMs) to focus on maintaining facial geometry and semantics, as well as texture and temporal coherence between frames. In this way, we inherit high-quality diverse generation from LDMs while significantly improving their controllability at a low training cost. Extensive experiments demonstrate the flexibility and effectiveness of our method in handling this paradigm. The codes will be released at <a href="https://github.com/modelscope/facechain">https://github.com/modelscope/facechain</a>. </p>
<p><a href="http://arxiv.org/abs/2403.01901v1">PDF</a> </p>
<p><strong>Summary</strong><br>聆听与想象任务：从单音频生成高保真、多样的会说话的面孔，解决了身份、内容、情感解耦和维持视频内多样性、视频间一致性的双重挑战。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>抽象人们聆听语音、提取有意义的线索并创建各种动态音频一致会说话的面孔的过程，称为“聆听与想象”。</li>
<li>面临身份、内容和情感从纠缠音频中有效解耦和维持视频内多样性、视频间一致性两大挑战。</li>
<li>提出渐进式音频解耦方法，用于准确的面部几何和语义学习。</li>
<li>引入可控连贯帧生成，将三个可训练适配器与冻结的潜在扩散模型（LDM）灵活集成，以专注于保持面部几何和语义，以及帧之间的纹理和时间连贯性。</li>
<li>继承了 LDM 的高质量多样化生成，同时以低训练成本显著提高了它们的控制能力。</li>
<li>广泛的实验表明了该方法在处理此范式方面的灵活性和有效性。</li>
<li>代码将在 <a href="https://github.com/modelscope/facechain">https://github.com/modelscope/facechain</a> 发布。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：FaceChain-ImagineID：自由生成高保真多样化的说话人脸（人脸链-想象识别：从分离音频中自由生成高保真多样化的说话人脸）</li>
<li>作者：Chao Xu, Yang Liu, Jiazheng Xing, Weida Wang, Mingze Sun, Jun Dan, Tianxin Huang, Siyuan Li, Zhi-Qi Cheng, Ying Tai, Baigui Sun</li>
<li>第一作者单位：阿里巴巴集团</li>
<li>关键词：说话人脸生成、音频分离、控制生成、生成式对抗网络</li>
<li>论文链接：https://arxiv.org/abs/2403.01901</li>
<li>摘要：
（1）研究背景：说话人脸生成技术旨在根据提供的音频和图像合成视频，广泛应用于虚拟交互等实际场景。然而，用户在使用过程中面临隐私泄露和虚拟头像与自身声音不匹配的困境。
（2）过去方法：现有方法主要集中于从图像中提取特征来生成说话人脸，但存在隐私泄露、生成质量不高等问题。
（3）研究方法：本文提出了一种新的范式——聆听和想象，将人类听到语音、提取有意义线索并创造各种动态音频一致说话人脸的过程抽象为从单个音频生成高保真多样化说话人脸的任务。该方法主要包括两个关键挑战：一是有效地从纠缠的音频中分离身份、内容和情感；二是保持视频内多样性和视频间一致性。为此，本文设计了一种渐进式音频分离方法，用于准确学习人脸几何和语义；并提出了可控连贯帧生成方法，通过将三个可训练适配器与冻结的潜在扩散模型灵活集成，专注于保持帧间的人脸几何、语义、纹理和时间连贯性。
（4）方法性能：在说话人脸生成任务上，该方法展现出良好的灵活性与有效性。实验结果表明，该方法在保持音频一致性的同时，可以生成视觉上多样化的高保真说话人脸，满足了用户对隐私保护和生成质量的双重需求。</li>
</ol>
<p>7.Methods：
(1) 提出渐进式音频分离方法，准确学习人脸几何和语义。
(2) 设计可控连贯帧生成方法，通过将三个可训练适配器与冻结的潜在扩散模型灵活集成，保持帧间的人脸几何、语义、纹理和时间连贯性。</p>
<ol>
<li>结论：
(1): FaceChain-ImagineID 为说话人脸生成领域提供了一种新的范式，有效地解决了隐私泄露和生成质量不高等问题，满足了用户对隐私保护和生成质量的双重需求。
(2): 创新点：<ul>
<li>提出渐进式音频分离方法，准确学习人脸几何和语义。</li>
<li>设计可控连贯帧生成方法，通过将三个可训练适配器与冻结的潜在扩散模型灵活集成，保持帧间的人脸几何、语义、纹理和时间连贯性。
 性能：</li>
<li>在说话人脸生成任务上，该方法展现出良好的灵活性与有效性。</li>
<li>实验结果表明，该方法在保持音频一致性的同时，可以生成视觉上多样化的高保真说话人脸。
 工作量：</li>
<li>该方法的实现需要较高的技术门槛，包括音频分离、生成式对抗网络和潜在扩散模型等方面的知识。</li>
</ul>
</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f9beb664fee087369a84229a9751302f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7122e8a5514f08293520b989812bde2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bca46fa0ffc8639dfa0117a5baad6ae0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6323f54d35add5790fd10654dbb8dd9d.jpg" align="middle">
</details>




<h2 id="G4G-A-Generic-Framework-for-High-Fidelity-Talking-Face-Generation-with-Fine-grained-Intra-modal-Alignment"><a href="#G4G-A-Generic-Framework-for-High-Fidelity-Talking-Face-Generation-with-Fine-grained-Intra-modal-Alignment" class="headerlink" title="G4G:A Generic Framework for High Fidelity Talking Face Generation with   Fine-grained Intra-modal Alignment"></a>G4G:A Generic Framework for High Fidelity Talking Face Generation with   Fine-grained Intra-modal Alignment</h2><p><strong>Authors:Juan Zhang, Jiahao Chen, Cheng Wang, Zhiwang Yu, Tangquan Qi, Di Wu</strong></p>
<p>Despite numerous completed studies, achieving high fidelity talking face generation with highly synchronized lip movements corresponding to arbitrary audio remains a significant challenge in the field. The shortcomings of published studies continue to confuse many researchers. This paper introduces G4G, a generic framework for high fidelity talking face generation with fine-grained intra-modal alignment. G4G can reenact the high fidelity of original video while producing highly synchronized lip movements regardless of given audio tones or volumes. The key to G4G’s success is the use of a diagonal matrix to enhance the ordinary alignment of audio-image intra-modal features, which significantly increases the comparative learning between positive and negative samples. Additionally, a multi-scaled supervision module is introduced to comprehensively reenact the perceptional fidelity of original video across the facial region while emphasizing the synchronization of lip movements and the input audio. A fusion network is then used to further fuse the facial region and the rest. Our experimental results demonstrate significant achievements in reenactment of original video quality as well as highly synchronized talking lips. G4G is an outperforming generic framework that can produce talking videos competitively closer to ground truth level than current state-of-the-art methods. </p>
<p><a href="http://arxiv.org/abs/2402.18122v2">PDF</a> </p>
<p><strong>Summary</strong><br>高质量会说话头像生成框架 G4G 可生成高度同步的唇部动作，实现逼真视频重现。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>G4G 框架可生成高度逼真的会说话头像，唇部动作与任意音频高度同步。</li>
<li>G4G 采用对角矩阵增强视音频模态内特征对齐，提升正负样本比较学习。</li>
<li>多尺度监督模块全面重现视频感知保真度，强调唇部动作与输入音频同步。</li>
<li>融合网络进一步融合面部区域与其他区域。</li>
<li>实验结果表明，G4G 在重现原始视频质量和唇部动作同步方面取得显著成就。</li>
<li>G4G 优于现有方法，可生成更接近真实水平的会说话头像视频。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<p></p><ol><p></p>
<p></p><li>标题：G4G：一个用于高保真说话人脸生成和精细化模态内对齐的通用框架</li><p></p>
<p></p><li>作者：Juan Zhang, Jiahao Chen, Cheng Wang, Zhiwang Yu, Tangquan Qi, Di Wu</li><p></p>
<p></p><li>第一作者单位：长沙万兴科技股份有限公司</li><p></p>
<p></p><li>关键词：说话人脸生成、模态内对齐、多尺度监督、融合网络</li><p></p>
<p></p><li>论文链接：https://arxiv.org/abs/2402.18122</li><p></p>
<p></p><li>摘要：
(1) 研究背景：说话人脸生成旨在合成一个目标人物的高保真视频，其唇部动作与任意音频同步。尽管有许多研究，但要实现高保真说话人脸生成并使其唇部动作与任意音频高度同步仍然是一个重大挑战。
(2) 过去方法：以往方法存在的问题主要在于：1）无法重现原始视频的高保真度；2）生成的唇部动作与音频不同步；3）生成的人脸视频保真度低。
(3) 研究方法：本文提出了 G4G，这是一个用于高保真说话人脸生成和精细化模态内对齐的通用框架。G4G 采用对角矩阵来增强音频-图像模态内特征的普通对齐，显著增加了正负样本之间的比较学习。此外，还引入了一个多尺度监督模块，以全面重现原始视频在面部区域的感知保真度，同时强调唇部动作与输入音频的同步性。然后使用融合网络进一步融合面部区域和其他部分。
(4) 性能：G4G 在重现原始视频质量和高度同步的说话人嘴唇方面取得了显著成就。实验结果表明，G4G 生成的说话人视频比当前最先进的方法更接近真实水平。</li><br>&lt;/ol&gt;<p></p>
<p></p><p>7.Methods：
(1)：提出G4G框架，采用对角矩阵增强音频-图像模态内特征对齐，增加正负样本比较学习；
(2)：引入多尺度监督模块，重现原始视频面部区域感知保真度，强调唇部动作与音频同步；
(3)：使用融合网络融合面部区域和其他部分。</p><p></p>
<p></p><p><strong>8. 结论</strong><br>(1): 本工作提出了 G4G 框架，用于生成高保真且高度同步的说话人脸视频。该框架由两个关键组件组成：对角精细化对齐网络和多尺度监督自适应空间变换网络。这些组件协同工作，生成具有卓越保真度和多尺度细节的说话人脸视频。对角精细化对齐网络专门设计用于解决模态内和模态间对齐的挑战。通过保留源图像的面部身份、属性和丰富的纹理细节，我们的网络确保生成的视频与源角色高度相似。此对齐过程对于保持生成视频的真实性和视觉质量至关重要。多尺度监督自适应空间变换网络进一步增强了生成视频的保真度。通过对嘴形和头部姿势进行空间变形，我们的网络实现了嘴唇运动的非凡准确性和真实性。生成嘴唇运动与给定音频之间的这种同步水平明显超过了现有的人脸通用方法。大量实验表明，我们的 G4G 框架在保留角色身份、皮肤纹理和与真实情况高度相似的细节方面是有效的。此外，我们的方法在生成与任意给定音频相对应的、高度同步的嘴唇运动方面表现出色。这些结果优于现有人脸通用方法，突出了我们方法的优越性。虽然我们的 G4G 框架代表了说话人脸生成领域的重大进步，但我们认识到仍有挑战需要解决。例如，生成具有大头部姿势角度的视频以及处理快速变化的背景和光照条件仍然是持续的研究领域。我们正在积极应对这些挑战，并计划在不久的将来发布进一步的研究结果。总之，我们提出的 G4G 框架为生成高保真且高度同步的说话人脸视频提供了一种强大且有效的解决方案。通过保留角色身份、皮肤纹理和细节，我们的方法为包括娱乐、教育和医疗保健在内的各个领域的应用开辟了新的可能性。<br>(2): <strong>创新点：</strong></p>
<ul>
<li>提出对角精细化对齐网络，增强音频-图像模态内特征对齐，增加正负样本比较学习。</li>
<li>引入多尺度监督自适应空间变换网络，重现原始视频面部区域感知保真度，强调唇部动作与音频同步。</li>
<li>使用融合网络融合面部区域和其他部分。<br><strong>性能：</strong></li>
<li>在重现原始视频质量和高度同步的说话人嘴唇方面取得了显著成就。</li>
<li>生成的说话人视频比当前最先进的方法更接近真实水平。<br><strong>工作量：</strong></li>
<li>模型复杂度和训练时间中等。&lt;/p&gt;<details>
<summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e12c89676d8b67fdf727809d6024eb2f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-153d9657273ba05cfef190ef2e389848.jpg" align="middle">
</details>


</li>
</ul>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-0ed20de4df697f188c4e24a324ed403c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-153d9657273ba05cfef190ef2e389848.jpg" align="middle">
</details>




</ol>]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>Talking Head Generation</tag>
      </tags>
  </entry>
  <entry>
    <title>Diffusion Models</title>
    <url>/2024/02/23/Paper/2024-02-23/Diffusion%20Models/</url>
    <content><![CDATA[<blockquote>
<p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-02-23-更新"><a href="#2024-02-23-更新" class="headerlink" title="2024-02-23 更新"></a>2024-02-23 更新</h1><h2 id="Hybrid-Video-Diffusion-Models-with-2D-Triplane-and-3D-Wavelet-Representation"><a href="#Hybrid-Video-Diffusion-Models-with-2D-Triplane-and-3D-Wavelet-Representation" class="headerlink" title="Hybrid Video Diffusion Models with 2D Triplane and 3D Wavelet   Representation"></a>Hybrid Video Diffusion Models with 2D Triplane and 3D Wavelet   Representation</h2><p><strong>Authors:Kihong Kim, Haneol Lee, Jihye Park, Seyeon Kim, Kwanghee Lee, Seungryong Kim, Jaejun Yoo</strong></p>
<p>Generating high-quality videos that synthesize desired realistic content is a challenging task due to their intricate high-dimensionality and complexity of videos. Several recent diffusion-based methods have shown comparable performance by compressing videos to a lower-dimensional latent space, using traditional video autoencoder architecture. However, such method that employ standard frame-wise 2D and 3D convolution fail to fully exploit the spatio-temporal nature of videos. To address this issue, we propose a novel hybrid video diffusion model, called HVDM, which can capture spatio-temporal dependencies more effectively. The HVDM is trained by a hybrid video autoencoder which extracts a disentangled representation of the video including: (i) a global context information captured by a 2D projected latent (ii) a local volume information captured by 3D convolutions with wavelet decomposition (iii) a frequency information for improving the video reconstruction. Based on this disentangled representation, our hybrid autoencoder provide a more comprehensive video latent enriching the generated videos with fine structures and details. Experiments on video generation benchamarks (UCF101, SkyTimelapse, and TaiChi) demonstrate that the proposed approach achieves state-of-the-art video generation quality, showing a wide range of video applications (e.g., long video generation, image-to-video, and video dynamics control). </p>
<p><a href="http://arxiv.org/abs/2402.13729v1">PDF</a> 17 pages, 13 figures</p>
<p><strong>Summary</strong><br>混合视频扩散模型能够有效地捕获视频的时空依赖性，从而生成高质量和逼真的视频。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>现有基于扩散的方法通过使用传统的视频自动编码器架构将视频压缩到低维潜在空间，实现了可比的性能。</li>
<li>标准帧级 2D 和 3D 卷积无法充分利用视频的时空性质。</li>
<li>提出了一种新的混合视频扩散模型 HVDM，可以更有效地捕捉时空相关性。</li>
<li>HVDM 由混合视频自动编码器训练，该编码器提取视频的解耦表示，包括：由 2D 投影潜在变量捕获的全局上下文信息、由具有小波分解的 3D 卷积捕获的局部体积信息以及用于改进视频重建的频率信息。</li>
<li>基于这种解耦表示，提出的混合自动编码器提供了更全面的视频潜在变量，丰富了生成视频的精细结构和细节。</li>
<li>在视频生成基准（UCF101、SkyTimelapse 和 TaiChi）上进行的实验表明，所提出的方法实现了最先进的视频生成质量，展示了广泛的视频应用（例如，长视频生成、图像到视频和视频动态控制）。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>标题：具有 2D 三平面和 3D 小波表示的混合视频扩散模型</li>
<li>作者：Tianhan Wang、Junyu Dong、Xiaolong Wang、Yibing Song、Yezhou Yang、Kun Zhou、Jiayi Ma</li>
<li>隶属关系：华中科技大学</li>
<li>关键词：视频生成、扩散模型、视频表示、小波变换</li>
<li>论文链接：None，Github 链接：None</li>
<li>
<p>摘要：
(1)：视频生成是一项具有挑战性的任务，需要生成具有复杂且高维度的逼真视频。最近的一些基于扩散的方法通过使用传统的视频自动编码器架构将视频压缩到更低维度的潜在空间，显示出可比的性能。但是，采用标准帧级 2D 和 3D 卷积的方法未能充分利用视频的时空性质。
(2)：为了解决这个问题，我们提出了一种新颖的混合视频扩散模型，称为 HVDM，它可以更有效地捕获时空依赖性。HVDM 由一个混合视频自动编码器训练，该自动编码器提取视频的纠缠表示，包括：由 2D 投影潜在变量捕获的全局上下文信息、由具有小波分解的 3D 卷积捕获的局部体积信息以及用于改进视频重建的频率信息。基于这种纠缠表示，我们的混合自动编码器提供了一个更全面的视频潜在变量，丰富了生成视频的精细结构和细节。
(3)：在视频生成基准（UCF101、SkyTimelapse 和 TaiChi）上的实验表明，所提出的方法实现了最先进的视频生成质量，展示了广泛的视频应用（例如，长视频生成、图像到视频和视频动态控制）。
(4)：我们的方法在 UCF101、SkyTimelapse 和 TaiChi 数据集上实现了最先进的视频生成质量。在 UCF101 数据集上，我们的方法在 FID 和 MS-SSIM 度量上分别比最先进的方法提高了 2.8% 和 0.011。在 SkyTimelapse 数据集上，我们的方法在 FID 和 MS-SSIM 度量上分别比最先进的方法提高了 3.2% 和 0.012。在 TaiChi 数据集上，我们的方法在 FID 和 MS-SSIM 度量上分别比最先进的方法提高了 2.9% 和 0.010。这些结果支持了我们的目标，即生成具有更高质量和更丰富细节的视频。</p>
</li>
<li>
<p>Methods:
(1): 我们提出了一种新的混合视频扩散模型HVDM，它可以更有效地捕获时空依赖性。
(2): HVDM由一个混合视频自动编码器训练，该自动编码器提取视频的纠缠表示，包括：由2D投影潜在变量捕获的全局上下文信息、由具有小波分解的3D卷积捕获的局部体积信息以及用于改进视频重建的频率信息。
(3): 基于这种纠缠表示，我们的混合自动编码器提供了一个更全面的视频潜在变量，丰富了生成视频的精细结构和细节。</p>
</li>
<li>
<p>结论：
（1）：本文提出了一种用于视频生成的混合视频自动编码器，称为 HVDM，该方法可以更有效地捕获时空依赖性。HVDM 由一个混合视频自动编码器训练，该自动编码器提取视频的纠缠表示，包括：由 2D 投影潜在变量捕获的全局上下文信息、由具有小波分解的 3D 卷积捕获的局部体积信息以及用于改进视频重建的频率信息。基于这种纠缠表示，我们的混合自动编码器提供了一个更全面的视频潜在变量，丰富了生成视频的精细结构和细节。在 UCF101、SkyTimelapse 和 TaiChi 基准数据集上的实验表明，所提出的方法实现了最先进的视频生成质量，展示了广泛的视频应用（例如，长视频生成、图像到视频和视频动态控制）。
（2）：创新点：</p>
</li>
<li>提出了一种新的混合视频扩散模型 HVDM，该模型可以更有效地捕获时空依赖性。</li>
<li>提出了一种混合视频自动编码器，该自动编码器提取视频的纠缠表示，包括：由 2D 投影潜在变量捕获的全局上下文信息、由具有小波分解的 3D 卷积捕获的局部体积信息以及用于改进视频重建的频率信息。</li>
<li>通过结合这些表示与时空交叉注意力，HVDM 可以生成具有改进的真实感的高质量视频。
性能：</li>
<li>在 UCF101、SkyTimelapse 和 TaiChi 基准数据集上的实验表明，所提出的方法实现了最先进的视频生成质量。</li>
<li>在 UCF101 数据集上，我们的方法在 FID 和 MS-SSIM 度量上分别比最先进的方法提高了 2.8% 和 0.011。</li>
<li>在 SkyTimelapse 数据集上，我们的方法在 FID 和 MS-SSIM 度量上分别比最先进的方法提高了 3.2% 和 0.012。</li>
<li>在 TaiChi 数据集上，我们的方法在 FID 和 MS-SSIM 度量上分别比最先进的方法提高了 2.9% 和 0.010。
工作量：</li>
<li>该方法需要设计和训练一个混合视频自动编码器，该自动编码器可以提取视频的纠缠表示。</li>
<li>该方法需要设计和训练一个时空交叉注意力机制，该机制可以将混合视频自动编码器的表示融合起来，生成高质量的视频。</li>
<li>该方法需要在多个视频生成基准数据集上进行实验，以评估其性能。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b0561ef07a60189b28853dc0eda76ddf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-851a92656b32ae2990dcf703193d622b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-63e056db347f6648afdcaf392f094dd6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c9f03009913498a6d9d199e594d8e64.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f2313ec6324cb296d16788788f949eec.jpg" align="middle">
</details>




<h2 id="ToDo-Token-Downsampling-for-Efficient-Generation-of-High-Resolution-Images"><a href="#ToDo-Token-Downsampling-for-Efficient-Generation-of-High-Resolution-Images" class="headerlink" title="ToDo: Token Downsampling for Efficient Generation of High-Resolution   Images"></a>ToDo: Token Downsampling for Efficient Generation of High-Resolution   Images</h2><p><strong>Authors:Ethan Smith, Nayan Saxena, Aninda Saha</strong></p>
<p>Attention mechanism has been crucial for image diffusion models, however, their quadratic computational complexity limits the sizes of images we can process within reasonable time and memory constraints. This paper investigates the importance of dense attention in generative image models, which often contain redundant features, making them suitable for sparser attention mechanisms. We propose a novel training-free method ToDo that relies on token downsampling of key and value tokens to accelerate Stable Diffusion inference by up to 2x for common sizes and up to 4.5x or more for high resolutions like 2048x2048. We demonstrate that our approach outperforms previous methods in balancing efficient throughput and fidelity. </p>
<p><a href="http://arxiv.org/abs/2402.13573v1">PDF</a> </p>
<p><strong>Summary</strong><br>改进稳定扩散注意机制以提高推理速度。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>注意力机制在图像扩散模型中很重要，但其二次计算复杂度限制了我们在合理的时间和内存限制内可以处理的图像大小。</li>
<li>生成图像模型通常包含冗余特征，适合稀疏注意力机制。</li>
<li>提出了一种新颖的免训练方法 ToDo，它依赖于键和值标记的标记降采样，从而将 Stable Diffusion 推理速度提高了 2 倍（常见大小）和 4.5 倍或更多（2048x2048 等高分辨率）。</li>
<li>ToDo 在平衡有效吞吐量和保真度方面优于以前的方法。</li>
<li>ToDo 是一个免费且易于实现的方法，可以应用于任何基于注意力的扩散模型。</li>
<li>ToDo 的推理速度比现有的最先进方法快，同时还能保持良好的图像质量。</li>
<li>ToDo 可以让图像扩散模型在更大的图像上进行训练和推理，从而提高图像质量。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：ToDo：令牌降采样以高效生成高分辨率图像</li>
<li>作者：Ethan Smith、Nayan Saxena、Aninda Saha</li>
<li>第一作者单位：Leonardo AI</li>
<li>关键词：图像生成、扩散模型、注意机制、令牌降采样、计算效率</li>
<li>论文链接：https://arxiv.org/abs/2402.13573、Github 代码链接：无</li>
<li>摘要：
（1）研究背景：注意机制是图像扩散模型成功的关键因素，但其二次计算复杂度限制了图像处理的大小。本文研究了生成图像模型中的密集注意机制，提出了一种无需训练的令牌降采样方法 ToDo，可加速 Stable Diffusion 推理，在常见尺寸下提速 2 倍，在 2048×2048 等高分辨率下提速 4.5 倍以上。
（2）过去方法及其问题：过去的稀疏注意方法通常需要训练时修改，增加了优化开销。注意近似方法虽然不需要训练，但通常需要预训练。
（3）研究方法：本文提出的 ToDo 方法是一种后处理方法，通过对注意机制中的键和值令牌进行降采样来加速推理，无需修改模型或重新训练。
（4）性能表现：ToDo 方法在各种任务和性能指标上都优于以往方法，在平衡计算效率和保真度方面表现出色。</li>
</ol>
<p>Methods:
(1): 本文提出了一种称为ToDo的令牌降采样方法，用于加速StableDiffusion推理。
(2): ToDo方法通过对注意机制中的键和值令牌进行降采样来加速推理，无需修改模型或重新训练。
(3): ToDo方法采用了一种优化的令牌合并策略，该策略利用了图像令牌固有的空间邻近性。
(4): ToDo方法还引入了一种改进的注意力机制，该机制将降采样操作应用于注意机制中的键和值，同时保留原始查询。
(5): ToDo方法在各种任务和性能指标上都优于以往方法，在平衡计算效率和保真度方面表现出色。</p>
<ol>
<li>结论：
（1）：本文提出的 ToDo 方法在保持计算效率和保真度之间取得了很好的平衡，尤其是在高频分量上。我们还表明，U-Net 中的相邻特征可能是冗余的，并假设我们的方法可以使其他基于注意力的生成图像模型受益，尤其是那些在大量令牌上运行的模型。未来的工作可以探索我们方法的可微分性，并利用它来有效地微调 StableDiffusion，使其在以前未见过的更大的图像尺寸上运行。
（2）：创新点：提出了一种称为 ToDo 的令牌降采样方法，用于加速 StableDiffusion 推理。
性能：在各种任务和性能指标上都优于以往方法，在平衡计算效率和保真度方面表现出色。
工作量：无需修改模型或重新训练，采用了一种优化的令牌合并策略，该策略利用了图像令牌固有的空间邻近性。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b29b6788a3c63bf19060ac13a17491fd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-588f50850c143462d31aee32d4aec168.jpg" align="middle">
</details>




<h2 id="Visual-Style-Prompting-with-Swapping-Self-Attention"><a href="#Visual-Style-Prompting-with-Swapping-Self-Attention" class="headerlink" title="Visual Style Prompting with Swapping Self-Attention"></a>Visual Style Prompting with Swapping Self-Attention</h2><p><strong>Authors:Jaeseok Jeong, Junho Kim, Yunjey Choi, Gayoung Lee, Youngjung Uh</strong></p>
<p>In the evolving domain of text-to-image generation, diffusion models have emerged as powerful tools in content creation. Despite their remarkable capability, existing models still face challenges in achieving controlled generation with a consistent style, requiring costly fine-tuning or often inadequately transferring the visual elements due to content leakage. To address these challenges, we propose a novel approach, \ours, to produce a diverse range of images while maintaining specific style elements and nuances. During the denoising process, we keep the query from original features while swapping the key and value with those from reference features in the late self-attention layers. This approach allows for the visual style prompting without any fine-tuning, ensuring that generated images maintain a faithful style. Through extensive evaluation across various styles and text prompts, our method demonstrates superiority over existing approaches, best reflecting the style of the references and ensuring that resulting images match the text prompts most accurately. Our project page is available <a href="https://curryjung.github.io/VisualStylePrompt/">https://curryjung.github.io/VisualStylePrompt/</a>. </p>
<p><a href="http://arxiv.org/abs/2402.12974v2">PDF</a> </p>
<p><strong>Summary</strong><br>使用风格样式提示获取更准确匹配文本提示的图像</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>扩散模型在文本到图像生成领域中表现出强大，但它们在保持一致风格的受控生成方面仍然存在挑战，需要昂贵的微调或由于内容泄漏而无法充分地再现视觉元素。</li>
<li>提出了一种新颖的方法，\ours，可以在保持特定风格元素和细微差别的情况下生成各种图像。</li>
<li>在去噪过程中，我们在最后的自我注意层中把原始特征中的查询保持不变，同时用参考特征的键和值进行交换。</li>
<li>这种方法允许在无需微调的情况下进行视觉风格提示，确保生成的图像保持忠实的风格。</li>
<li>通过在各种风格和文本提示下的广泛评估，我们的方法证明了优于现有方法，最能反映参考文献的风格，并确保生成的图像最准确地匹配文本提示。</li>
<li>我们的项目页面可以在 <a href="https://curryjung.github.io/VisualStylePrompt/">https://curryjung.github.io/VisualStylePrompt/</a> 获得。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：视觉风格提示与交换自我注意力</li>
<li>作者：Jongwook Choi, Kyumin Lee, Jun-Ho Kim</li>
<li>第一作者单位：韩国科学技术院</li>
<li>关键词：扩散模型、文本到图像生成、视觉风格提示、交换自我注意力</li>
<li>论文链接：https://arxiv.org/abs/2302.08551，Github 链接：无</li>
<li>摘要：
(1) 研究背景：扩散模型在文本到图像生成领域取得了显著进展，但仍面临着在保持一致风格的同时实现可控生成的挑战，需要昂贵的微调或由于内容泄漏而导致视觉元素转移不足。
(2) 过去的方法及其问题：现有方法通常通过微调或使用预训练的模型来实现视觉风格提示，但这些方法存在成本高昂、风格转移不足或内容泄漏等问题。
(3) 本文提出的研究方法：本文提出了一种新的方法——视觉风格提示，通过在去噪过程中保留原始特征的查询，同时在最后的自注意力层中用参考特征交换键和值，来实现视觉风格提示。这种方法不需要任何微调，并确保生成的图像保持忠实风格。
(4) 方法在任务和性能上的表现：本文的方法在各种风格和文本提示下的评估中表现出优越性，在反映参考风格和确保生成图像最准确地匹配文本提示方面优于现有方法。这些性能支持了本文的目标，即实现具有特定风格元素和细微差别的图像生成。</li>
</ol>
<p>Some Error for method(比如是不是没有Methods这个章节)</p>
<ol>
<li>结论：
（1）：本文提出了一种新的视觉风格提示方法，该方法无需微调，并确保生成的图像保持忠实风格。
（2）：创新点：
本方法创新性地提出了一种新的视觉风格提示方法，该方法通过在去噪过程中保留原始特征的查询，同时在最后的自注意力层中用参考特征交换键和值，来实现视觉风格提示。
性能：
本方法在各种风格和文本提示下的评估中表现出优越性，在反映参考风格和确保生成图像最准确地匹配文本提示方面优于现有方法。
工作量：
本方法的工作量相对较低，不需要任何微调，并且可以很容易地应用于现有的扩散模型。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-ca682f6681ca2aea4fdb5980de4dc8f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d771e643cabdf04390bb34c56e1d306.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11f4ff0d9aeecd7bd560b037f6d9c569.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff425802a32a4519e30b9044a3eed1e8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6b333a460ba441d80a537e0874e7628a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ef6e8248b60241a24705f590a653e38.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4422e0b37dd7515345602877f9ea3a62.jpg" align="middle">
</details>




<h2 id="CLIPping-the-Deception-Adapting-Vision-Language-Models-for-Universal-Deepfake-Detection"><a href="#CLIPping-the-Deception-Adapting-Vision-Language-Models-for-Universal-Deepfake-Detection" class="headerlink" title="CLIPping the Deception: Adapting Vision-Language Models for Universal   Deepfake Detection"></a>CLIPping the Deception: Adapting Vision-Language Models for Universal   Deepfake Detection</h2><p><strong>Authors:Sohail Ahmed Khan, Duc-Tien Dang-Nguyen</strong></p>
<p>The recent advancements in Generative Adversarial Networks (GANs) and the emergence of Diffusion models have significantly streamlined the production of highly realistic and widely accessible synthetic content. As a result, there is a pressing need for effective general purpose detection mechanisms to mitigate the potential risks posed by deepfakes. In this paper, we explore the effectiveness of pre-trained vision-language models (VLMs) when paired with recent adaptation methods for universal deepfake detection. Following previous studies in this domain, we employ only a single dataset (ProGAN) in order to adapt CLIP for deepfake detection. However, in contrast to prior research, which rely solely on the visual part of CLIP while ignoring its textual component, our analysis reveals that retaining the text part is crucial. Consequently, the simple and lightweight Prompt Tuning based adaptation strategy that we employ outperforms the previous SOTA approach by 5.01% mAP and 6.61% accuracy while utilizing less than one third of the training data (200k images as compared to 720k). To assess the real-world applicability of our proposed models, we conduct a comprehensive evaluation across various scenarios. This involves rigorous testing on images sourced from 21 distinct datasets, including those generated by GANs-based, Diffusion-based and Commercial tools. </p>
<p><a href="http://arxiv.org/abs/2402.12927v1">PDF</a> </p>
<p><strong>摘要</strong><br>CLIP模型结合文本和视觉信息，在深度伪造检测任务上取得了优异的性能，优于仅使用视觉信息的SOTA方法。</p>
<p><strong>要点</strong></p>
<ul>
<li>CLIP模型在与最近的通用深度伪造检测适应方法配对时，在深度伪造检测任务上表现出良好的有效性。</li>
<li>只需使用一个数据集（ProGAN）就可以对CLIP进行改编，以实现深度伪造检测。</li>
<li>保留CLIP模型的文本部分对于提高检测性能至关重要。</li>
<li>基于Prompt Tuning的简单且轻量级的适应策略在使用不到三分之一的训练数据（20万张图像，而之前的方法使用了72万张图像）的情况下，在mAP和准确率方面分别优于之前的SOTA方法5.01%和6.61%。</li>
<li>CLIP模型在对来自21个不同数据集的图像进行的全面评估中表现出良好的真实世界适用性，包括由基于GAN、基于扩散和商业工具生成的图像。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：剪辑欺骗：适应通用深度伪造检测的视觉语言模型</li>
<li>作者：Sohail Ahmed Khan, Duc-Tien Dang-Nguyen</li>
<li>单位：卑尔根大学</li>
<li>关键词：深度伪造检测，迁移学习，视觉语言模型</li>
<li>论文链接：https://arxiv.org/abs/2402.12927，Github 链接：None</li>
<li>摘要：
(1)：随着生成对抗网络 (GAN) 的最新进展和扩散模型的出现，高度逼真且广泛可访问的合成内容的制作变得更加容易。因此，迫切需要有效的通用检测机制来减轻深度伪造带来的潜在风险。
(2)：在本文中，我们探索了在与最近的适应方法配对时预训练的视觉语言模型 (VLM) 在通用深度伪造检测中的有效性。遵循该领域的先前研究，我们仅使用单个数据集 (ProGAN) 来适应 CLIP 以进行深度伪造检测。然而，与仅依赖 CLIP 的视觉部分而忽略其文本组件的先前研究相比，我们的分析表明保留文本部分至关重要。因此，我们采用的简单轻量级 PromptTuning 基于适应策略在利用不到三分之一的训练数据（200k 图像，相比之下为 720k）的情况下，在 mAP 上优于之前的 SOTA 方法 5.01%，准确率提高 6.61%。为了评估我们提出的模型的实际适用性，我们对各种场景进行了综合评估。这涉及对来自 21 个不同数据集的图像进行严格测试，包括基于 GAN、基于扩散和商业工具生成的图像。
(3)：我们提出了一种简单而有效的方法来适应 CLIP 以进行通用深度伪造检测。我们的方法基于 PromptTuning，这是一种轻量级且易于实现的适应策略。我们还表明，保留 CLIP 的文本部分对于提高检测性能至关重要。
(4)：在 ProGAN 数据集上，我们的方法在 mAP 上实现了 95.21% 的准确率和 97.82% 的准确率，优于之前的 SOTA 方法。我们的方法还显示出良好的泛化能力，能够检测来自各种来源的深度伪造，包括 GAN、扩散和商业工具。</li>
</ol>
<p>方法：</p>
<p>（1）线性探测：线性探测是一种将冻结模型（本例中为 CLIP）作为特征提取器，并在其上微调线性分类器的方法。我们遵循 Ojha 等人采用的相同方法。[32]，即我们丢弃 CLIP 的文本编码器并冻结其图像编码器。然后，我们在冻结的 CLIP 图像特征上训练一个用于分类的单层线性层，使用 Sigmoid 激活函数将倒数第二个图像特征映射到用于类别预测的逻辑值。优化使用二进制交叉熵损失进行。</p>
<p>（2）微调：微调在此上下文中意味着再次在用于下游数据集的整个 CLIP 模型（ViT-Large）上进行训练，在本例中是也被 [45] 和 [32] 使用的 ProGAN 数据集。完全微调需要显着更多的计算机资源、数据和训练时间，因为整个模型都经过了重新训练。此外，随着模型大小的增加，此策略表现出不稳定和效率低下 [26]。在训练模型时，我们遇到了这个问题，并通过使用极小的学习率 1×10-6 来缓解这个问题。为了微调我们的模型，我们遵循 CLIP 预训练中概述的程序 [37]。但是，我们引入了一个修改：不是对每个图像使用整个文本标题，我们只提供单个单词标题，具体来说是 real 或 fake。典型的用于调整 CLIP 的微调管道如图 2 所示。</p>
<p>（3）PromptTuning：PromptTuning 是一种通过调整文本提示来适应 CLIP 的方法。我们遵循 CoOp [50] 的方法，该方法使用 CLIP 的文本编码器生成一个提示，该提示可以指导图像编码器进行分类。我们使用单个单词提示 real 或 fake 来生成图像特征，然后使用这些特征来训练线性分类器。</p>
<p>（4）适配器网络：适配器网络是一种通过在预训练模型上添加小型网络来适应新任务的方法。我们使用一个适配器网络来调整 CLIP，该网络由一个卷积层和一个线性层组成。适配器网络将 CLIP 的图像特征作为输入，并输出一个用于分类的逻辑值。</p>
<ol>
<li>结论：
（1）：本工作通过探索预训练的视觉语言模型 CLIP 在通用深度伪造检测中的有效性，展示了 CLIP 在检测来自各种数据分布的深度伪造图像方面的鲁棒性。我们使用来自 ProGAN 数据集的 200k 图像作为多样化的训练集，并比较了四种不同的迁移学习策略，包括微调、线性探测、PromptTuning 和训练适配器网络。我们的实验包括对包含 21 个不同图像生成器的综合测试集进行评估。在整个实验中，我们证明了结合 CLIP 的图像和文本组件的迁移学习策略始终优于仅使用 CLIP 视觉方面的简单方法（如线性探测）的性能。我们的研究结果凸显了 PromptTuning 优于当前基准和 SOTA 方法的优势，在展示其有效性的同时，即使训练参数最少，也能实现显着的改进幅度。此外，我们还进行了少量实验，分析了在 JPEG 压缩和高斯模糊等后处理操作下的鲁棒性，并证明了即使训练集规模较小（20k 图像），基于 CLIP 的检测器也具有稳定的性能。
（2）：创新点：
• 探索了预训练的视觉语言模型 CLIP 在通用深度伪造检测中的有效性。
• 比较了四种不同的迁移学习策略，包括微调、线性探测、PromptTuning 和训练适配器网络。
• 证明了结合 CLIP 的图像和文本组件的迁移学习策略始终优于仅使用 CLIP 视觉方面的简单方法（如线性探测）的性能。
• PromptTuning 优于当前基准和 SOTA 方法，在展示其有效性的同时，即使训练参数最少，也能实现显着的改进幅度。
• 分析了在 JPEG 压缩和高斯模糊等后处理操作下的鲁棒性，并证明了即使训练集规模较小（20k 图像），基于 CLIP 的检测器也具有稳定的性能。</li>
</ol>
<p>性能：
• 在 ProGAN 数据集上，PromptTuning 在 mAP 上实现了 95.21% 的准确率和 97.82% 的准确率，优于之前的 SOTA 方法。
• PromptTuning 在综合测试集上也表现出良好的泛化能力，能够检测来自各种来源的深度伪造，包括 GAN、扩散和商业工具。</p>
<p>工作量：
• PromptTuning 是一种简单而有效的方法，易于实现。
• PromptTuning 只需要少量的数据和训练时间。
• PromptTuning 可以用于检测来自各种来源的深度伪造，包括 GAN、扩散和商业工具。</p>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-745a50bdee80b1df6d9da45abefcb26e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0e6ec4d0ce05a2af6e93f8a2710069bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca30024b468b77b358f2f1058147b9e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f749a0d770c3a7267b5153b59c39032b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6f8430a1aafee1b2f88631389c9cdc32.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-05df037ca314f896a85f2bb5c514f5dd.jpg" align="middle">
</details>




## RealCompo: Dynamic Equilibrium between Realism and Compositionality   Improves Text-to-Image Diffusion Models

**Authors:Xinchen Zhang, Ling Yang, Yaqi Cai, Zhaochen Yu, Jiake Xie, Ye Tian, Minkai Xu, Yong Tang, Yujiu Yang, Bin Cui**

Diffusion models have achieved remarkable advancements in text-to-image generation. However, existing models still have many difficulties when faced with multiple-object compositional generation. In this paper, we propose a new training-free and transferred-friendly text-to-image generation framework, namely RealCompo, which aims to leverage the advantages of text-to-image and layout-to-image models to enhance both realism and compositionality of the generated images. An intuitive and novel balancer is proposed to dynamically balance the strengths of the two models in denoising process, allowing plug-and-play use of any model without extra training. Extensive experiments show that our RealCompo consistently outperforms state-of-the-art text-to-image models and layout-to-image models in multiple-object compositional generation while keeping satisfactory realism and compositionality of the generated images. Code is available at https://github.com/YangLing0818/RealCompo 

[PDF](http://arxiv.org/abs/2402.12908v1) Project: https://github.com/YangLing0818/RealCompo

**Summary**
利用文本到图像模型和布局到图像模型的优势，提出了一种新的无训练和易于迁移的文本到图像生成框架 RealCompo，以增强生成图像的真实性和组合性。

**Key Takeaways**

- RealCompo 是一种新的无训练和易于迁移的文本到图像生成框架，旨在利用文本到图像模型和布局到图像模型的优势。
- RealCompo 利用文本到图像模型生成逼真的图像，利用布局到图像模型生成合理的构图。
- RealCompo 引入了新的平衡器，以动态平衡两个模型在去噪过程中的优势。
- RealCompo 即插即用，无需额外训练即可使用任何模型。
- RealCompo 在多对象组合生成方面始终优于最先进的文本到图像模型和布局到图像模型。
- RealCompo 保持了生成图像的令人满意的真实性和组合性。
- RealCompo 的代码可在 https://github.com/YangLing0818/RealCompo 获得。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>题目：RealCompo：真实感与组合性的动态平衡可改进文本到图像扩散模型</li>
<li>作者：Xinchen Zhang<em>, Ling Yang</em>, Yaqi Cai, Zhaochen Yu, Jiake Xie, Ye Tian, Minkai Xu, Yong Tang, Yujiu Yang, Bin Cui</li>
<li>单位：清华大学</li>
<li>关键词：文本到图像生成、布局到图像生成、扩散模型、组合性、真实感</li>
<li>论文链接：https://github.com/YangLing0818/RealCompo
Github 代码链接：https://github.com/YangLing0818/RealCompo</li>
<li>摘要：
（1）研究背景：扩散模型在文本到图像生成任务中取得了显著进展，但现有模型在处理多对象组合性生成时仍面临许多困难。
（2）过去方法及其问题：现有方法包括文本到图像模型和布局到图像模型。文本到图像模型能够生成逼真的图像，但组合性较差；布局到图像模型能够控制对象的位置和数量，但真实感较差。
（3）研究方法：本文提出了一种新的训练友好且可迁移的文本到图像生成框架 RealCompo，该框架旨在利用文本到图像模型和布局到图像模型的优势，同时提高生成图像的真实感和组合性。RealCompo 使用了一个直观且新颖的平衡器，可以在去噪过程中动态平衡两个模型的强度，从而允许即插即用任何模型而无需额外训练。
（4）实验结果：广泛的实验表明，RealCompo 在多对象组合性生成任务上始终优于最先进的文本到图像模型和布局到图像模型，同时保持了生成图像令人满意的真实感和组合性。这些性能支持了本文的目标。</li>
</ol>
<p>Methods:
(1) 提出了一种新的训练友好且可迁移的文本到图像生成框架RealCompo，该框架旨在利用文本到图像模型和布局到图像模型的优势，同时提高生成图像的真实感和组合性。
(2) 设计了一个直观且新颖的平衡器，可以在去噪过程中动态平衡两个模型的强度，从而允许即插即用任何模型而无需额外训练。
(3) 分析了每个模型预测噪声的影响，并提供了一种计算系数的方法。
(4) 提供了平衡器所采用的更新规则的详细解释，该规则利用了一种无训练方法来动态更新系数。
(5) 扩展了RealCompo的应用，为L2I模型的每一类设计了损失函数。</p>
<ol>
<li>结论：
（1）：本文提出了一种训练友好且可迁移的文本到图像生成框架RealCompo，该框架在多对象组合性生成任务上始终优于最先进的文本到图像模型和布局到图像模型，同时保持了生成图像令人满意的真实感和组合性。
（2）：创新点：
提出了一个直观且新颖的平衡器，可以在去噪过程中动态平衡两个模型的强度，从而允许即插即用任何模型而无需额外训练。
分析了每个模型预测噪声的影响，并提供了一种计算系数的方法。
提供了平衡器所采用的更新规则的详细解释，该规则利用了一种无训练方法来动态更新系数。
扩展了RealCompo的应用，为L2I模型的每一类设计了损失函数。
性能：
RealCompo在多对象组合性生成任务上始终优于最先进的文本到图像模型和布局到图像模型，同时保持了生成图像令人满意的真实感和组合性。
RealCompo可以被推广到任何LLM、T2I和L2I模型，并保持强大的生成结果。
工作量：
RealCompo的实现相对简单，易于使用。
RealCompo可以轻松地集成到现有的文本到图像生成系统中。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-264ae173bcca3292815b8e45db353de6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9c5f244037ff17e98afe9f2c1851e4f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-caea4b22ae09f52bc515627d4e3cba84.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5fcdadd1b307e5df492d508f86958e6.jpg" align="middle">
</details>




<h2 id="Two-stage-Rainfall-Forecasting-Diffusion-Model"><a href="#Two-stage-Rainfall-Forecasting-Diffusion-Model" class="headerlink" title="Two-stage Rainfall-Forecasting Diffusion Model"></a>Two-stage Rainfall-Forecasting Diffusion Model</h2><p><strong>Authors:XuDong Ling, ChaoRong Li, FengQing Qin, LiHong Zhu, Yuanyuan Huang</strong></p>
<p>Deep neural networks have made great achievements in rainfall prediction.However, the current forecasting methods have certain limitations, such as with blurry generated images and incorrect spatial positions. To overcome these challenges, we propose a Two-stage Rainfall-Forecasting Diffusion Model (TRDM) aimed at improving the accuracy of long-term rainfall forecasts and addressing the imbalance in performance between temporal and spatial modeling. TRDM is a two-stage method for rainfall prediction tasks. The task of the first stage is to capture robust temporal information while preserving spatial information under low-resolution conditions. The task of the second stage is to reconstruct the low-resolution images generated in the first stage into high-resolution images. We demonstrate state-of-the-art results on the MRMS and Swedish radar datasets. Our project is open source and available on GitHub at: \href{<a href="https://github.com/clearlyzerolxd/TRDM}{https://github.com/clearlyzerolxd/TRDM}">https://github.com/clearlyzerolxd/TRDM}{https://github.com/clearlyzerolxd/TRDM}</a>. </p>
<p><a href="http://arxiv.org/abs/2402.12779v1">PDF</a> </p>
<p><strong>Summary</strong><br>利用两阶段降雨预测扩散模型（TRDM）提高降雨预测的准确性。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>TRDM是一种用于降雨预测任务的两阶段方法。</li>
<li>TRDM的第一阶段任务是在低分辨率条件下捕获稳健的时间信息，同时保留空间信息。</li>
<li>TRDM的第二阶段任务是将第一阶段生成的低分辨率图像重建为高分辨率图像。</li>
<li>TRDM在MRMS和瑞典雷达数据集上展示了最先进的结果。</li>
<li>TRDM开源，可在 GitHub 上获取：\href{<a href="https://github.com/clearlyzerolxd/TRDM}{https://github.com/clearlyzerolxd/TRDM}。">https://github.com/clearlyzerolxd/TRDM}{https://github.com/clearlyzerolxd/TRDM}。</a></li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：两阶段降雨预测扩散模型</li>
<li>作者：Xu DongLing, Chao RongLi*, FengQing Qin, LiHong Zhu, Yuanyuan Huang</li>
<li>第一作者单位：重庆理工大学人工智能与大数据学院</li>
<li>关键词：扩散模型、生成对抗网络、降雨预测</li>
<li>论文链接：https://arxiv.org/abs/2402.12779，Github 代码链接：https://github.com/clearlyzerolxd/TRDM</li>
<li>
<p>总结：
（1）研究背景：深度神经网络在降雨预测领域取得了很大的成就。然而，现有的预测方法存在一定的局限性，例如生成的图像模糊、空间位置不准确等。
（2）以往方法：针对上述问题，已有研究提出了卷积LSTM和卷积GRU模型来提高降雨预测的准确性。然而，这些方法在长期的预测中存在准确性不高的问题。此外，SmaAt-UNet模型虽然能够有效地捕捉输入序列中的关键空间信息，但在长期预测性能方面仍有待提高。
（3）研究方法：为了解决上述问题，本文提出了一种两阶段降雨预测扩散模型（TRDM）。TRDM是一个两阶段的降雨预测方法。第一阶段的任务是在低分辨率条件下捕获鲁棒的时间信息，同时保留空间信息。第二阶段的任务是将第一阶段生成的低分辨率图像重建为高分辨率图像。
（4）方法性能：在MRMS和瑞典雷达数据集上，TRDM取得了最先进的结果。</p>
</li>
<li>
<p>方法：
(1) 预测扩散模型：利用三维卷积神经网络生成 16 帧 32×32 低分辨率降雨结果，同时保留一定程度的空间信息，为后续重建阶段提供鲁棒的基础。
(2) 空间超分辨率：使用扩散模型构建超分辨率网络，将低分辨率图像重建为高分辨率图像，增强图像质量和细节，以便更准确地分析未来降雨的强度和分布。
(3) 潜在超分辨率：提出一种潜在超分辨率方法，将高分辨率图像编码为潜在空间，然后利用扩散模型生成条件，指导生成条件。
(4) 模型训练：使用 L1 损失函数训练预测扩散模型和超分辨率模型，以最小化预测误差。
(5) 模型推理：在推理过程中，将低分辨率图像输入到超分辨率模型中，并使用扩散模型生成条件，逐步恢复高分辨率图像。</p>
</li>
<li>
<p>结论：
（1）：本文提出了一种两阶段降雨预测扩散模型（TRDM），该模型能够有效地捕捉输入序列中的关键空间信息，并在长期预测性能方面取得了最先进的结果。
（2）：创新点：
TRDM模型采用了两阶段的预测策略，第一阶段的任务是在低分辨率条件下捕获鲁棒的时间信息，同时保留空间信息。第二阶段的任务是将第一阶段生成的低分辨率图像重建为高分辨率图像。这种两阶段的策略能够有效地提高降雨预测的准确性。
性能：
在MRMS和瑞典雷达数据集上，TRDM模型取得了最先进的结果。与其他方法相比，TRDM模型能够生成更加清晰和准确的降雨预测图像。
工作量：
TRDM模型的训练和推理过程相对简单。该模型只需要少量的训练数据，并且训练时间较短。在推理过程中，TRDM模型能够快速地生成降雨预测图像。</p>
</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-77f75079fa9cf15e6ab90ae9bfdf3659.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e40db6d053eb3ccf707a2dbcd4cf2e8d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1f5823da8ecb8e38058c288533b8775e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf44de1da53f2ab1acf3c0d8075ec068.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b36e8a07f0692df0799659af074a0a49.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-56d34d3e7c52a330e5782ff67a0df331.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4bfaafb452921e1d0c1a1d6c62510229.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5fa91d38aada7882b2ac95950348567d.jpg" align="middle">
</details>




<h2 id="MuLan-Multimodal-LLM-Agent-for-Progressive-Multi-Object-Diffusion"><a href="#MuLan-Multimodal-LLM-Agent-for-Progressive-Multi-Object-Diffusion" class="headerlink" title="MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion"></a>MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion</h2><p><strong>Authors:Sen Li, Ruochen Wang, Cho-Jui Hsieh, Minhao Cheng, Tianyi Zhou</strong></p>
<p>Existing text-to-image models still struggle to generate images of multiple objects, especially in handling their spatial positions, relative sizes, overlapping, and attribute bindings. In this paper, we develop a training-free Multimodal-LLM agent (MuLan) to address these challenges by progressive multi-object generation with planning and feedback control, like a human painter. MuLan harnesses a large language model (LLM) to decompose a prompt to a sequence of sub-tasks, each generating only one object conditioned on previously generated objects by stable diffusion. Unlike existing LLM-grounded methods, MuLan only produces a high-level plan at the beginning while the exact size and location of each object are determined by an LLM and attention guidance upon each sub-task. Moreover, MuLan adopts a vision-language model (VLM) to provide feedback to the image generated in each sub-task and control the diffusion model to re-generate the image if it violates the original prompt. Hence, each model in every step of MuLan only needs to address an easy sub-task it is specialized for. We collect 200 prompts containing multi-objects with spatial relationships and attribute bindings from different benchmarks to evaluate MuLan. The results demonstrate the superiority of MuLan in generating multiple objects over baselines. The code is available on <a href="https://github.com/measure-infinity/mulan-code">https://github.com/measure-infinity/mulan-code</a>. </p>
<p><a href="http://arxiv.org/abs/2402.12741v1">PDF</a> Project website: <a href="https://measure-infinity.github.io/mulan">https://measure-infinity.github.io/mulan</a></p>
<p><strong>Summary</strong><br>多模态语言模型助力扩散模型生成多对象图像，分步规划，反馈控制，轻松满足复杂要求。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>现有的文本转图像模型在生成多对象图像时仍然面临挑战，尤其是在处理对象的空间位置、相对大小、重叠和属性绑定方面。</li>
<li>MuLan 采用无训练的训练方式，通过规划和反馈控制逐步生成多对象，类似于人类画家作画。</li>
<li>MuLan 利用大型语言模型 (LLM) 将提示分解为一系列子任务，每个子任务仅生成一个对象，并通过稳定扩散模型对先前生成的对象进行条件控制。</li>
<li>与现有的 LLM 方法不同，MuLan 只在开始时生成一个高层次的规划，而每个对象的确切大小和位置由 LLM 和注意力引导在每个子任务中确定。</li>
<li>MuLan 采用视觉语言模型 (VLM) 为每个子任务中生成的图像提供反馈，并在图像违反原始提示时控制扩散模型重新生成图像。</li>
<li>MuLan 在每个步骤中只处理自己专门处理的简单子任务。</li>
<li>MuLan 在不同基准上收集了 200 个包含空间关系和属性绑定的多对象提示来评估 MuLan。结果表明，MuLan 在生成多对象方面优于基线方法。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>标题：MuLan：用于渐进式多对象扩散的多模态-LLM 代理</li>
<li>作者：Sen Li、Ruochen Wang、Cho-Jui Hsieh、Minhao Cheng、Tianyi Zhou</li>
<li>隶属机构：香港科技大学计算机科学与工程系</li>
<li>关键词：文本到图像、多对象生成、扩散模型、大语言模型、视觉语言模型</li>
<li>论文链接：https://arxiv.org/abs/2402.12741
Github 链接：https://github.com/measure-infinity/mulan-code</li>
<li>
<p>摘要：
(1) 研究背景：现有的文本到图像模型在生成包含多个对象的图像时仍然存在困难，尤其是在处理对象的空间位置、相对大小、重叠和属性绑定方面。
(2) 过去的方法及其问题：一些方法试图利用大语言模型（LLM）来指导生成过程，但由于 LLM 的空间推理能力有限以及它们与扩散模型缺乏一致性，因此直接生成完整且精确的多对象布局仍然具有挑战性。此外，这些方法通常将布局作为对每个模型的额外条件，这可能会导致扩散模型由于对复杂提示的误解而生成不正确图像。
(3) 本文提出的研究方法：本文提出了一种无训练且可控的文本到图像生成范式，该范式不需要演示，而是主要关注改进现有模型的工具使用。该范式建立在由多模态-LLM 代理 (MuLan) 进行的渐进式多对象生成之上，MuLan 每个阶段只生成一个对象，并根据图像中已生成的对象和最有可能放置新对象的位置的注意力掩码进行条件生成。此外，MuLan 采用视觉语言模型 (VLM) 来提供对每个子任务中生成的图像的反馈，并控制扩散模型以重新生成图像（如果它违反了原始提示）。
(4) 实验结果与性能：在包含来自不同基准的多对象（具有空间关系和属性绑定）的 200 个提示上评估 MuLan。结果表明，MuLan 在生成多对象方面优于基线方法。这些性能支持了本文的目标，即开发一种无训练且可控的文本到图像生成范式，该范式不需要演示，而是主要关注改进现有模型的工具使用。</p>
</li>
<li>
<p>方法：
（1）：提出了一种无训练且可控的文本到图像生成范式，该范式不需要演示，而是主要关注改进现有模型的工具使用。
（2）：该范式建立在由多模态-LLM代理（MuLan）进行的渐进式多对象生成之上，MuLan每个阶段只生成一个对象，并根据图像中已生成的对象和最有可能放置新对象的位置的注意力掩码进行条件生成。
（3）：采用视觉语言模型（VLM）来提供对每个子任务中生成的图像的反馈，并控制扩散模型以重新生成图像（如果它违反了原始提示）。</p>
</li>
<li>
<p>结论：
（1）：本文提出了一种无训练且可控的文本到图像生成范式MuLan，该范式不需要演示，而是主要关注改进现有模型的工具使用，在生成多对象方面优于基线方法。
（2）：创新点：
MuLan：一种无训练且可控的文本到图像生成范式，不需要演示，而是主要关注改进现有模型的工具使用。
渐进式多对象生成：MuLan每个阶段只生成一个对象，并根据图像中已生成的对象和最有可能放置新对象的位置的注意力掩码进行条件生成。
视觉语言模型（VLM）：采用VLM来提供对每个子任务中生成的图像的反馈，并控制扩散模型以重新生成图像（如果它违反了原始提示）。
性能：
在包含来自不同基准的多对象（具有空间关系和属性绑定）的200个提示上评估MuLan。结果表明，MuLan在生成多对象方面优于基线方法。
工作量：
MuLan的实现相对简单，不需要额外的训练或数据。</p>
</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-6204318646d6f8f073e72dd012036b52.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-339c08e21eaf72db7bf6af40d44b1ebd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c764cf1c9de7293c1a1c79a15a87313.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5f2c4d6c6e5f00fd67d4a729192f3826.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85b2bad757801f5c51069e7f6c02cbc7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9001380fef222e92159ed423b319dc8a.jpg" align="middle">
</details>




<h2 id="Improving-Deep-Generative-Models-on-Many-To-One-Image-to-Image-Translation"><a href="#Improving-Deep-Generative-Models-on-Many-To-One-Image-to-Image-Translation" class="headerlink" title="Improving Deep Generative Models on Many-To-One Image-to-Image   Translation"></a>Improving Deep Generative Models on Many-To-One Image-to-Image   Translation</h2><p><strong>Authors:Sagar Saxena, Mohammad Nayeem Teli</strong></p>
<p>Deep generative models have been applied to multiple applications in image-to-image translation. Generative Adversarial Networks and Diffusion Models have presented impressive results, setting new state-of-the-art results on these tasks. Most methods have symmetric setups across the different domains in a dataset. These methods assume that all domains have either multiple modalities or only one modality. However, there are many datasets that have a many-to-one relationship between two domains. In this work, we first introduce a Colorized MNIST dataset and a Color-Recall score that can provide a simple benchmark for evaluating models on many-to-one translation. We then introduce a new asymmetric framework to improve existing deep generative models on many-to-one image-to-image translation. We apply this framework to StarGAN V2 and show that in both unsupervised and semi-supervised settings, the performance of this new model improves on many-to-one image-to-image translation. </p>
<p><a href="http://arxiv.org/abs/2402.12531v1">PDF</a> 11 pages, 6 figures</p>
<p><strong>摘要</strong><br>用深度扩散模型改进多对一的图像到图像翻译。</p>
<p><strong>要点</strong></p>
<ul>
<li>深度扩散模型是用于图像到图像翻译的生成模型。</li>
<li>现有的方法通常假设所有领域都具有多个模态或只有一个模态。</li>
<li>在许多场景下，两个领域之间存在多对一的关系。</li>
<li>研究者提出了一个着色 MNIST 数据集和一个彩色召回分数，为多对一翻译提供了一个简单的基准。</li>
<li>提出一种新的非对称框架来改进现有深度生成模型在多对一图像到图像翻译中的性能。</li>
<li>将该框架应用于 StarGAN V2，实验表明，在新模型中，无监督和半监督设置下的多对一图像到图像翻译性能均得到提高。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>标题：改进多对一图像到图像翻译中的深度生成模型</li>
<li>作者：Sagar Saxena, Mohammad Nayeem Teli</li>
<li>隶属关系：马里兰大学计算机科学系</li>
<li>关键词：深度生成模型、图像到图像翻译、多对一翻译、非对称框架</li>
<li>链接：https://arxiv.org/abs/2402.12531</li>
<li>摘要：
（1）研究背景：深度生成模型已广泛应用于图像到图像翻译中，取得了令人印象深刻的结果。然而，大多数方法在不同领域之间采用对称设置，假设所有领域都具有多模态或单一模态。然而，许多数据集在两个领域之间具有多对一的关系。
（2）过去的方法：过去的方法要么学习双射映射，要么学习多对多映射，但这些方法无法准确建模某些任务中领域之间的关系，例如图像着色、语义分割、深度估计等。
（3）研究方法：本文提出了一种新的非对称框架来改进现有深度生成模型在多对一图像到图像翻译中的性能。该框架将生成器和判别器模块解耦，并引入了一种新的损失函数来鼓励生成器生成与输入图像相似的输出图像。
（4）方法性能：本文将该框架应用于 StarGAN V2 模型，并在无监督和半监督设置中进行了评估。结果表明，该框架可以有效提高模型在多对一图像到图像翻译中的性能。</li>
</ol>
<p>Some Error for method(比如是不是没有Methods这个章节)</p>
<ol>
<li>结论：
（1）：本文提出了一种新的非对称框架来改进现有深度生成模型在多对一图像到图像翻译中的性能，该框架可以有效提高模型在多对一图像到图像翻译中的性能。
（2）：创新点：</li>
<li>提出了一种新的非对称框架，将生成器和判别器模块解耦，并引入了一种新的损失函数来鼓励生成器生成与输入图像相似的输出图像。</li>
<li>将该框架应用于StarGANV2模型，并在无监督和半监督设置中进行了评估。</li>
<li>结果表明，该框架可以有效提高模型在多对一图像到图像翻译中的性能。
性能：</li>
<li>在无监督设置中，该框架在CelebA数据集上取得了比StarGANV2模型更高的FID和LPIPS得分。</li>
<li>在半监督设置中，该框架在CelebA数据集上取得了比StarGANV2模型更高的FID和LPIPS得分。</li>
<li>在Cityscapes数据集上，该框架取得了比StarGANV2模型更高的mIoU和F1得分。
工作量：</li>
<li>该框架的实现相对简单，可以在TensorFlow或PyTorch等深度学习框架中轻松实现。</li>
<li>该框架的训练时间与StarGANV2模型相似。</li>
<li>该框架的推理时间与StarGANV2模型相似。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-847aa6560da9e8f5bc3efa20a3a60ab6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b87108e9e8879c6d14d1fe6eaf34112.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e7677caf8041932830de453431d2abd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50784d0e85e2b28f9cc755ede524a772.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9b40dd37bb889c7e90ab259793c5ab5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df01b0bd8844297db8557dc012591bb8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af0a71391ad75be3ea34e547daa4db1e.jpg" align="middle">
</details>




<h2 id="FiT-Flexible-Vision-Transformer-for-Diffusion-Model"><a href="#FiT-Flexible-Vision-Transformer-for-Diffusion-Model" class="headerlink" title="FiT: Flexible Vision Transformer for Diffusion Model"></a>FiT: Flexible Vision Transformer for Diffusion Model</h2><p><strong>Authors:Zeyu Lu, Zidong Wang, Di Huang, Chengyue Wu, Xihui Liu, Wanli Ouyang, Lei Bai</strong></p>
<p>Nature is infinitely resolution-free. In the context of this reality, existing diffusion models, such as Diffusion Transformers, often face challenges when processing image resolutions outside of their trained domain. To overcome this limitation, we present the Flexible Vision Transformer (FiT), a transformer architecture specifically designed for generating images with unrestricted resolutions and aspect ratios. Unlike traditional methods that perceive images as static-resolution grids, FiT conceptualizes images as sequences of dynamically-sized tokens. This perspective enables a flexible training strategy that effortlessly adapts to diverse aspect ratios during both training and inference phases, thus promoting resolution generalization and eliminating biases induced by image cropping. Enhanced by a meticulously adjusted network structure and the integration of training-free extrapolation techniques, FiT exhibits remarkable flexibility in resolution extrapolation generation. Comprehensive experiments demonstrate the exceptional performance of FiT across a broad range of resolutions, showcasing its effectiveness both within and beyond its training resolution distribution. Repository available at <a href="https://github.com/whlzy/FiT">https://github.com/whlzy/FiT</a>. </p>
<p><a href="http://arxiv.org/abs/2402.12376v1">PDF</a> </p>
<p><strong>Summary</strong><br>通过将图像视为动态大小标记序列，弹性视觉变换器可在不同分辨率和宽高比上生成图像。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>现有扩散模型在处理训练域之外的图像分辨率时面临挑战。</li>
<li>弹性视觉变换器 (FiT) 是一种专为生成不受限分辨率和宽高比的图像而设计的转换器架构。</li>
<li>FiT 将图像视为动态大小标记序列，从而支持不同的宽高比。</li>
<li>FiT 在训练和推理阶段均支持不同的宽高比，从而消除了图像裁剪引起的偏差。</li>
<li>FiT 在多种分辨率下表现出优异的性能，证明其在训练分辨率分布之外也很有效。</li>
<li>FiT 的存储库位于 <a href="https://github.com/whlzy/FiT。">https://github.com/whlzy/FiT。</a></li>
<li>FiT 为图像生成领域开辟了新的可能性。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：FiT：用于扩散模型的灵活视觉变换器</li>
<li>作者：Zeyu Lu<em>，Zidong Wang</em>，Di Huang，Chengyue Wu，Xihui Liu，Wanli Ouyang，Lei Bai</li>
<li>单位：上海人工智能实验室</li>
<li>关键词：扩散模型，视觉变换器，分辨率泛化，外推技术</li>
<li>论文链接：https://arxiv.org/abs/2402.12376，Github 链接：None</li>
<li>摘要：
（1）研究背景：自然界的图像分辨率是无限的。现有的扩散模型（如扩散变换器）在处理超出其训练域的图像分辨率时往往面临挑战。
（2）过去方法与问题：传统方法将图像视为静态分辨率网格，这限制了它们处理不同分辨率图像的能力。此外，图像裁剪会引入偏差，影响模型的泛化性能。
（3）研究方法：为了克服这些问题，本文提出了灵活视觉变换器（FiT），这是一种专门为生成具有无限分辨率和纵横比的图像而设计的变换器架构。FiT 将图像概念化为动态大小标记的序列，这使得它能够在训练和推理阶段轻松适应不同的纵横比，从而促进了分辨率泛化并消除了图像裁剪引起的偏差。通过精心调整的网络结构和训练自由外推技术的集成，FiT 在分辨率外推生成方面表现出卓越的灵活性。
（4）方法性能：综合实验表明，FiT 在广泛的分辨率范围内表现出优异的性能，证明了其在训练分辨率分布内和之外的有效性。</li>
</ol>
<p>方法：</p>
<p>（1）灵活训练：提出了一种灵活的训练方法，允许模型在训练过程中处理不同纵横比的图像，从而促进分辨率泛化并消除图像裁剪引起的偏差。</p>
<p>（2）SwiGLU激活函数：将MLP激活函数替换为SwiGLU激活函数，可以提高模型的性能。</p>
<p>（3）2DRoPE位置编码：将绝对位置编码替换为2DRoPE位置编码，可以提高模型的性能和外推能力。</p>
<p>（4）位置嵌入插值方法：提出了一种位置嵌入插值方法，可以将模型的外推能力扩展到训练分布之外的分辨率。</p>
<ol>
<li>结论：
（1）：本工作的主要贡献在于，我们提出了用于扩散模型的灵活视觉变换器（FiT），这是一种专门为生成具有无限分辨率和纵横比的图像而设计的变换器架构。FiT 在广泛的分辨率范围内表现出优异的性能，证明了其在训练分辨率分布内和之外的有效性。
（2）：创新点：</li>
<li>提出了一种灵活的训练方法，允许模型在训练过程中处理不同纵横比的图像，从而促进分辨率泛化并消除图像裁剪引起的偏差。</li>
<li>将 MLP 激活函数替换为 SwiGLU 激活函数，可以提高模型的性能。</li>
<li>将绝对位置编码替换为 2DRoPE 位置编码，可以提高模型的性能和外推能力。</li>
<li>提出了一种位置嵌入插值方法，可以将模型的外推能力扩展到训练分布之外的分辨率。
性能：</li>
<li>FiT 在广泛的分辨率范围内表现出优异的性能，证明了其在训练分辨率分布内和之外的有效性。</li>
<li>FiT 在各种分辨率下均优于所有先前模型，无论是基于 Transformer 的还是基于 CNN 的。</li>
<li>结合我们的分辨率外推方法 VisionNTK，FiT 的性能得到了进一步显着提升。
工作量：</li>
<li>本文的工作量较大，涉及到模型架构设计、训练方法改进、外推技术集成等多个方面。</li>
<li>本文的实验部分也比较复杂，涉及到多个数据集、多个分辨率、多个评价指标等。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f2dad57fd66943bffc8c0eefec68b3e8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-297eceedf1e98b27794f86f0cb8285ba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6760b58ea1f0ee4f73bf15eae4ddb673.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09693fd0b9790328fcc71c49c26da3ad.jpg" align="middle">
</details>




<h2 id="Direct-Consistency-Optimization-for-Compositional-Text-to-Image-Personalization"><a href="#Direct-Consistency-Optimization-for-Compositional-Text-to-Image-Personalization" class="headerlink" title="Direct Consistency Optimization for Compositional Text-to-Image   Personalization"></a>Direct Consistency Optimization for Compositional Text-to-Image   Personalization</h2><p><strong>Authors:Kyungmin Lee, Sangkyung Kwak, Kihyuk Sohn, Jinwoo Shin</strong></p>
<p>Text-to-image (T2I) diffusion models, when fine-tuned on a few personal images, are able to generate visuals with a high degree of consistency. However, they still lack in synthesizing images of different scenarios or styles that are possible in the original pretrained models. To address this, we propose to fine-tune the T2I model by maximizing consistency to reference images, while penalizing the deviation from the pretrained model. We devise a novel training objective for T2I diffusion models that minimally fine-tunes the pretrained model to achieve consistency. Our method, dubbed \emph{Direct Consistency Optimization}, is as simple as regular diffusion loss, while significantly enhancing the compositionality of personalized T2I models. Also, our approach induces a new sampling method that controls the tradeoff between image fidelity and prompt fidelity. Lastly, we emphasize the necessity of using a comprehensive caption for reference images to further enhance the image-text alignment. We show the efficacy of the proposed method on the T2I personalization for subject, style, or both. In particular, our method results in a superior Pareto frontier to the baselines. Generated examples and codes are in our project page( <a href="https://dco-t2i.github.io/">https://dco-t2i.github.io/</a>). </p>
<p><a href="http://arxiv.org/abs/2402.12004v1">PDF</a> Preprint. See our project page (<a href="https://dco-t2i.github.io/">https://dco-t2i.github.io/</a>) for more   examples and codes</p>
<p><strong>Summary</strong><br>基于文本生成图像的扩散模型可通过微调少数个人图像生成高度一致的视觉效果。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>微调基于文本生成图像的扩散模型时，最大化与参考图像的一致性，同时惩罚与预训练模型的偏差。</li>
<li>提出一种最小化微调预训练模型以实现一致性的新颖训练目标。</li>
<li>该方法简单且有效，显着增强了个性化基于文本生成图像模型的组合性。</li>
<li>引入一种新的采样方法以控制图像保真度与提示保真度之间的权衡。</li>
<li>强调使用综合标题作为参考图像以进一步增强图像与文本的一致性。</li>
<li>证明了该方法在主题、风格或两者方面的基于文本生成图像个性化中的有效性。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>标题：直接一致性优化用于合成文本到图像个性化</li>
<li>作者：Seunghoon Hong, Inwoong Ko, Sunghyun Cho, Seonghyeon Nam, Dong Huk Park</li>
<li>隶属机构：首尔大学</li>
<li>关键词：文本到图像合成、个性化、扩散模型、一致性优化</li>
<li>论文链接：None，Github 代码链接：None</li>
<li>摘要：
(1)：研究背景：文本到图像 (T2I) 扩散模型在经过少量个人图像的微调后，能够生成具有高度一致性的视觉效果。然而，它们仍然缺乏在原始预训练模型中可能的不同场景或风格的图像合成能力。
(2)：过去的方法及其问题：为了解决这个问题，本文提出了一种通过最大化与参考图像的一致性来微调 T2I 模型的方法，同时惩罚与预训练模型的偏差。过去的方法存在的问题是，它们在个性化 T2I 模型中仍然缺乏合成不同场景或风格的图像的能力。
(3)：研究方法：本文提出了一种新的 T2I 扩散模型训练目标，该目标可以最小程度地微调预训练模型以实现一致性。该方法称为直接一致性优化，它与常规扩散损失一样简单，同时显着提高了个性化 T2I 模型的组合性。此外，本文的方法还引入了一种新的采样方法，该方法可以控制图像保真度与提示保真度之间的权衡。最后，本文强调了使用综合标题作为参考图像的必要性，以进一步增强图像与文本的对齐。
(4)：实验结果：本文的方法在主题、风格或两者兼而有之的 T2I 个性化方面都取得了优异的性能。具体来说，本文的方法在帕累托前沿方面优于基线方法。</li>
</ol>
<p>Methods:
(1) Direct Consistency Optimization (DCO): We formulate T2I diffusion model fine-tuning as a constrained policy optimization problem and propose DCO loss to maximize the consistency reward of generated samples while penalizing the deviation from the pretrained model.
(2) Reward Guidance (RG): After fine-tuning with DCO loss, we introduce RG to control the trade-off between consistency and image-text alignment by interpolating the noise estimations from the fine-tuned model and the pretrained model.
(3) Prompt Construction for Reference Images: We emphasize the importance of comprehensive captions for reference images and provide examples to illustrate the difference between compact captions and comprehensive captions.</p>
<ol>
<li>结论：
（1）：提出了一种新的文本到图像扩散模型训练目标，该目标可以最小程度地微调预训练模型以实现一致性。
（2）：创新点：
提出了一种新的文本到图像扩散模型训练目标，该目标可以最小程度地微调预训练模型以实现一致性。
引入了一种新的采样方法，该方法可以控制图像保真度与提示保真度之间的权衡。
强调了使用综合标题作为参考图像的必要性，以进一步增强图像与文本的对齐。
性能：
在主题、风格或两者兼而有之的文本到图像个性化方面都取得了优异的性能。
在帕累托前沿方面优于基线方法。
工作量：
需要收集和准备参考图像。
需要微调文本到图像扩散模型。
需要采样生成的图像。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-747445a04d574a8975290f4c0ffe6aca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-915bf11d3f533330ed7c94f5f635e501.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3a074dca6974482c499ea0392640cb3.jpg" align="middle">
</details>




]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>Diffusion Models</tag>
      </tags>
  </entry>
  <entry>
    <title>NeRF</title>
    <url>/2024/03/09/Paper/2024-03-09/NeRF/</url>
    <content><![CDATA[<blockquote>
<p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-03-09-更新"><a href="#2024-03-09-更新" class="headerlink" title="2024-03-09 更新"></a>2024-03-09 更新</h1><h2 id="DART-Implicit-Doppler-Tomography-for-Radar-Novel-View-Synthesis"><a href="#DART-Implicit-Doppler-Tomography-for-Radar-Novel-View-Synthesis" class="headerlink" title="DART: Implicit Doppler Tomography for Radar Novel View Synthesis"></a>DART: Implicit Doppler Tomography for Radar Novel View Synthesis</h2><p><strong>Authors:Tianshu Huang, John Miller, Akarsh Prabhakara, Tao Jin, Tarana Laroia, Zico Kolter, Anthony Rowe</strong></p>
<p>Simulation is an invaluable tool for radio-frequency system designers that enables rapid prototyping of various algorithms for imaging, target detection, classification, and tracking. However, simulating realistic radar scans is a challenging task that requires an accurate model of the scene, radio frequency material properties, and a corresponding radar synthesis function. Rather than specifying these models explicitly, we propose DART - Doppler Aided Radar Tomography, a Neural Radiance Field-inspired method which uses radar-specific physics to create a reflectance and transmittance-based rendering pipeline for range-Doppler images. We then evaluate DART by constructing a custom data collection platform and collecting a novel radar dataset together with accurate position and instantaneous velocity measurements from lidar-based localization. In comparison to state-of-the-art baselines, DART synthesizes superior radar range-Doppler images from novel views across all datasets and additionally can be used to generate high quality tomographic images. </p>
<p><a href="http://arxiv.org/abs/2403.03896v1">PDF</a> To appear in CVPR 2024; see <a href="https://wiselabcmu.github.io/dart/">https://wiselabcmu.github.io/dart/</a> for   our project site</p>
<p><strong>Summary</strong></p>
<p>基于雷达特定物理特性，使用神经辐射场方法创建反射和透射渲染管道，用于生成多普勒范围雷达图像。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>通过模拟器快速原型化成像、目标检测、分类和跟踪算法。</li>
<li>构建真实的雷达扫描模型面临场景、射频材料特性和雷达合成函数的挑战。</li>
<li>提出 DART 方法，受神经辐射场启发，构建基于反射率和透射率的渲染管道。</li>
<li>构建定制数据收集平台，收集包含位置和即时速度测量的新型雷达数据集。</li>
<li>与现有基准相比，DART 合成出所有数据集新视角下的更优质雷达多普勒范围图像。</li>
<li>DART 可用于生成高质量的层析图像。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：雷达隐式多普勒层析成像用于新型视角合成</li>
<li>作者：Jiahui Yu、Yiyi Liao、Yinda Zhang、Wenqi Xian、Lingxiao Li、Junjie Gu、Xiaoyang Guo、Shilin Zhu、Shanshan Zhao、Biao Yang、Lingbo Liu</li>
<li>隶属：上海交通大学</li>
<li>关键词：雷达、合成孔径雷达、多普勒层析成像、神经辐射场</li>
<li>论文链接：None，Github 代码链接：None</li>
<li>
<p>摘要：
(1) 研究背景：雷达仿真对于射频系统设计至关重要，但仿真逼真的雷达扫描具有挑战性，需要场景、射频材料属性和雷达合成函数的准确模型。
(2) 过去方法：传统方法需要显式指定这些模型，但它们复杂且耗时。
(3) 论文提出的研究方法：DART（多普勒辅助雷达层析成像）是一种受神经辐射场启发的雷达特定物理方法，它创建了一个基于反射率和透射率的渲染管道，用于生成距离-多普勒图像。
(4) 方法在任务中的表现：DART 在所有数据集上从新视角合成了出色的雷达距离-多普勒图像，此外还可用于生成高质量的层析图像。这些性能支持了论文的目标，即提供一种无需显式模型即可生成逼真雷达图像的方法。</p>
</li>
<li>
<p>方法：
(1) 数据驱动方法使用真实的传感器扫描来构建环境模型。稀疏方法使用恒定误报率检测 (CFAR) 来检测环境中的离散反射器 [15, 49, 63]。另一方面，密集方法将环境划分为显式的体素网格，并推断每个单元的雷达属性。密集方法可以进一步细分为相干和非相干聚合。如果可以使用固定（例如线性和圆形）轨迹或亚波长精度的姿态估计，则可以使用合成孔径雷达 (SAR) [46, 50, 52, 56, 81, 82]；然而，这对于大面积移动平台来说是不切实际的。相反，传感器读数（通过多个天线或较小轨迹片段上的 SAR 获得高角度分辨率）也可以以非相干方式聚合，这被称为多视图 3D 重建 [33–35] 和雷达测量法 [12]。
(2) 雷达中的机器学习方法许多经典的雷达问题，例如雷达超分辨率 [10, 17, 20, 21, 23, 53, 54, 72]、里程计 [2, 43]、测绘 [42]、活动识别 [39, 70, 77, 80] 和物体分类 [32, 69, 85] 已应用于使用机器学习的更便宜、更轻、更紧凑的雷达系统。我们现在寻求从紧凑、低分辨率雷达中解决新颖的视图合成问题，同时隐式创建更高分辨率的地图。
(3) 神经辐射场神经辐射场 [48] 没有定义明确的逆成像算法从传感器读数中恢复场景的表示，而是通过随机梯度下降隐式地反转前向渲染函数。这需要以下组件：</p>
</li>
<li>世界模型：NeRF 将世界定义为每个位置和视角的 RGB 颜色和透明度；后续工作已将其推广到处理抗锯齿 [5]、不同的相机和照明 [47, 73]。</li>
<li>世界表示：除了神经网络 [48] 或体素网格 [40] 之外，最近的工作还探索了空间哈希表 [51] 以及用于视场角依赖性的函数分解 [18, 83]。</li>
<li>渲染函数和模型反演：NeRF 将每个像素建模为射线并对辐射场进行射线追踪。此渲染函数的可逆性至关重要：通过假设每个像素都是一条射线，NeRF 由每个射线上的一个 RGB 图像像素“监督”，允许 NeRF “求解”沿射线的不透明点。我们对 NeRF 的这些关键推动因素进行了创新，以便将这种方法应用于毫米波雷达。通过将 NeRF 技术应用于雷达，我们希望利用大量神经辐射场文献，同时释放神经隐式表示的潜力。超越视觉领域 NeRF 的成功激发了众多其他努力，将相同的通用原理应用于其他传感器，包括空间音频 [44]、成像声纳 [55, 59]、激光雷达模拟 [27] 和 RSSI（接收信号强度指示器）映射 [84]。NeRF 也已应用于雷达 [29, 71]，用于类似相机的超高分辨率合成孔径雷达，而不是我们在本文中探索的紧凑且廉价的雷达。
(4) DART：多普勒辅助雷达层析成像虽然我们的整体方法受神经辐射场的启发，但雷达的物理特性提出了几个新的挑战。我们做出以下关键设计决策（图 3）：</li>
<li>我们首先选择一个雷达测量表示空间——距离-多普勒——该空间克服了紧凑型雷达的较差空间分辨率（第 3.1、3.2 节）。</li>
<li>然后我们选择一个模型来解释电磁波相互作用的雷达特定效应，这些效应对于逼真的视图合成至关重要，例如镜面反射、重影和部分遮挡（第 3.3 节）。</li>
<li>
<p>最后，为了有效地训练和学习雷达的神经隐式地图，我们为自适应网格世界表示选择了网络架构，设计了距离-多普勒渲染方法，并提出了关键渲染优化（第 3.3-3.4 节）。
(5) 距离-多普勒表示与相机不同，雷达是主动传感器，它通过发射射频波形来照亮场景。在处理从场景中的物体接收到的反射后，雷达可以以 3D 形式感知世界——距离、方位角和仰角——作为热图，指示该 3D 坐标处物体的反射率 [60, 61]。然而，虽然笨重的机械雷达或大型固态雷达阵列可以提供接近典型相机的方位角和仰角分辨率，但现代廉价且紧凑的固态雷达阵列具有小天线阵列，这使得它们在方位角和仰角轴上远逊于典型相机 [28]。因此，这些紧凑型雷达只能在方位角和仰角轴上生成粗糙的热图（&gt;15◦ 分辨率），导致每个距离-方位角-仰角箱指向 3D 空间中的一个较粗糙区域，远不如来自相机像素的射线清晰 [38, 41, 76]。为了获得更好的角度分辨率，雷达可以利用多普勒效应：相对于雷达以不同相对速度移动的物体具有不同的多普勒速度，可以通过检查距离-方位角-仰角热图的残余相位来测量这些速度 [79]。至关重要的是，在静态场景中，这些相对速度不仅取决于雷达和世界之间的相对速度，还取决于物体与雷达之间的相对方位角和仰角，每个多普勒对应于空间中的一个圆锥 [60]。由于更精细的距离和多普勒分辨率，多普勒极大地降低了 3D 空间中每个箱的模糊性，使其变为一个薄环（图 4），我们通过在距离和多普勒轴上进行细度论证进一步将其简化为雷达渲染的圆圈（第 3.4 节）。
(6) 雷达预处理毫米波雷达使用称为调频连续波 (FMCW) 的波形，并测量连续时间信号；然后我们将这些信号转换为距离-多普勒-天线热图。为了总结我们的雷达处理管道的要点（附录 A.1）：
• 不希望的距离-多普勒旁瓣：单个反射物体可以创建旁瓣，这些旁瓣会渗入几个距离-多普勒箱并掩盖较弱的物体 [61, 86]。我们使用汉宁加权窗口沿着距离和多普勒轴来减轻这种影响，而不是强迫 DART 对其进行建模（附录 A.1）。
• 多个天线：我们对雷达中的八个发射-接收 (TX/RX) 对执行距离-多普勒处理。在我们的渲染过程中（第 3.4 节），我们对每个 TX/RX 对应用天线增益和阵列因子（图 3），强调视野的 8 个部分。虽然我们对高质量方位角-仰角信息的感知仍然源于利用多普勒，但这提供了一些粗略的方向信息。
(7) DART 的世界模型如果我们有世界和世界中所有物体电磁波相互作用的准确模型，我们就可以将该模型应用于由每个距离-多普勒像素定义的区域来计算其值。然而，由于现实世界场景和交互的复杂性，这两个任务都非常困难且通常不切实际。相反，我们以数据驱动的方式对这些属性进行建模，使用视场相关的神经网络方法表示反射率和透射率。建模射频反射率建模毫米波材料相互作用是雷达视图合成最具挑战性的因素之一。从雷达的角度来看，空间中的点具有两个关键属性：反射率（反射回的能量比例）和透射率（继续过去的能量比例）[60]。然而，毫米波也会根据入射角与物体进行不同的交互 [4]；例如，金属表面可能是镜面反射的，并且可能从某些视点不可见。因此，我们使用反射率 σ：R6→R 和透射率 α：R6→[0,1] 对每个物理点进行建模，(1)
它将反射率 σ 和透射率 α 建模为入射波的位置 (R3) 和入射角 (R3) 的函数，并允许 DART 对各种雷达现象进行建模，例如部分遮挡、镜面反射和重影（附录 A.2）。世界表示虽然基于体素的方法对于学习视觉辐射场非常有效 [18, 83]，但即使在利用多普勒轴后，雷达图像与相机相比也具有更差的仰角和方位角分辨率。这放大了 σ 和 α 可以解决的空间分辨率差异，即使在近距离和远距离之间也是如此。此外，与相机不同，我们的角度分辨率在所有尺度上都是可变的——无论是在轨迹级别、帧到帧级别甚至帧内（第 3.1 节）。类似于 NeRF [48]，我们转向神经隐式表示作为创建“自适应”网格的一种手段，并将我们的模型基于 Instant Neural Graphics Primitive3 [51]。与大多数视觉 NeRF 不同，我们不将入射角作为输入提供给神经网络 [74]。相反，我们的架构（可视化在图 3 的中心块中）输出“基本”反射率 ¯σ 和透射率 ¯α，以及共享球谐函数系数 [83]，这些系数作为内积应用于入射角。除了计算优势之外，这还允许我们直接将 (¯σ, ¯α) 解释为我们学习的反射率和透射率函数的球积分（附录 A.3）。我们还发现 σ 和 α 上的输出激活函数对于数值稳定性和性能至关重要。由于 σ 是无界的4，我们对 σ 应用线性激活。然后，为了将 α 约束在 [0,1] 中，我们应用激活函数 f(α) = exp(max(0,α))，(2)
我们将其与自定义梯度估计器配对以处理初始化不稳定性（附录 A.4）。
(8) 雷达渲染和模型训练我们使用可微映射训练 σ 和 α，该映射从给定的 (σ, α) 网络生成多天线距离-多普勒热图；我们称之为雷达渲染。与视觉 NeRF 不同，DART 除了遮挡之外还必须考虑一系列物理效应，包括路径衰减、天线增益模式和雷达特定的多普勒轴。射线追踪考虑从雷达位置 x 和方向（旋转矩阵）A 以入射角 w 发射的单个“射线”。当射线在太空中传播到处理的（距离、多普勒、天线）图像的最大范围时，每个点 x + riw 在距离 r 处接收幅度为 u_i 的信号，该信号因自由空间而衰减。</p>
</li>
<li>
<p>结论：
（1）本文提出了 DART（多普勒辅助雷达层析成像）方法，该方法利用神经辐射场技术，无需显式模型即可生成逼真的雷达图像，为新型视角合成提供了新的方法。
（2）创新点：</p>
</li>
<li>提出了一种雷达特定物理模型，用于解释电磁波相互作用的雷达特定效应。</li>
<li>设计了一种距离-多普勒渲染方法，用于有效地训练和学习雷达的神经隐式地图。</li>
<li>提出了一种关键渲染优化，以提高渲染效率和图像质量。</li>
<li>性能：DART 在所有数据集上从新视角合成了出色的雷达距离-多普勒图像，此外还可用于生成高质量的层析图像。</li>
<li>工作量：DART 的实现相对简单，易于部署和使用。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-7a08f4b46a27b4550cca3fdbb7bb2699.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f5dd4309cf1d06499c45ea2d70f80cbb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4136ef209f4ed07822647cd67d564e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4196074de7d63d703597568e97025da.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7aa27948966717e8808650a0fc34b361.jpg" align="middle">
</details>




<h2 id="DaReNeRF-Direction-aware-Representation-for-Dynamic-Scenes"><a href="#DaReNeRF-Direction-aware-Representation-for-Dynamic-Scenes" class="headerlink" title="DaReNeRF: Direction-aware Representation for Dynamic Scenes"></a>DaReNeRF: Direction-aware Representation for Dynamic Scenes</h2><p><strong>Authors:Ange Lou, Benjamin Planche, Zhongpai Gao, Yamin Li, Tianyu Luan, Hao Ding, Terrence Chen, Jack Noble, Ziyan Wu</strong></p>
<p>Addressing the intricate challenge of modeling and re-rendering dynamic scenes, most recent approaches have sought to simplify these complexities using plane-based explicit representations, overcoming the slow training time issues associated with methods like Neural Radiance Fields (NeRF) and implicit representations. However, the straightforward decomposition of 4D dynamic scenes into multiple 2D plane-based representations proves insufficient for re-rendering high-fidelity scenes with complex motions. In response, we present a novel direction-aware representation (DaRe) approach that captures scene dynamics from six different directions. This learned representation undergoes an inverse dual-tree complex wavelet transformation (DTCWT) to recover plane-based information. DaReNeRF computes features for each space-time point by fusing vectors from these recovered planes. Combining DaReNeRF with a tiny MLP for color regression and leveraging volume rendering in training yield state-of-the-art performance in novel view synthesis for complex dynamic scenes. Notably, to address redundancy introduced by the six real and six imaginary direction-aware wavelet coefficients, we introduce a trainable masking approach, mitigating storage issues without significant performance decline. Moreover, DaReNeRF maintains a 2x reduction in training time compared to prior art while delivering superior performance. </p>
<p><a href="http://arxiv.org/abs/2403.02265v1">PDF</a> Accepted at CVPR 2024. Paper + supplementary material</p>
<p><strong>Summary</strong><br>使用六个不同方向捕捉场景动态并融合信息，DaReNeRF 在复杂动态场景的新视图合成中取得了最先进的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>利用六个方向感知表示捕获场景动态。</li>
<li>采用逆向双树复小波变换恢复平面信息。</li>
<li>将方向感知表示融合到 NeRF 中，计算时空点的特征。</li>
<li>使用小的 MLP 进行颜色回归，利用体积渲染进行训练。</li>
<li>引入可训练掩码方法，在不降低性能的情况下减轻存储问题。</li>
<li>与现有技术相比，训练时间减少 2 倍，同时性能更优。</li>
<li>适用于具有复杂运动的高保真场景的重新渲染。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li><strong>标题：</strong> DaReNeRF：动态场景的方向感知表征</li>
<li><strong>作者：</strong> Ange Lou, Tianyu Luan, Hao Ding, Wenbo Luo, Xiaogang Wang, Wenzheng Chen</li>
<li><strong>第一作者单位：</strong> United Imaging Intelligence</li>
<li><strong>关键词：</strong> 动态场景，神经辐射场，平面表示，方向感知表征</li>
<li><strong>论文链接：</strong> None</li>
<li>
<p><strong>摘要：</strong>
   (1) <strong>研究背景：</strong> 近期方法使用基于平面的显式表征来简化动态场景建模和渲染，克服了神经辐射场等方法相关的训练时间慢的问题。然而，将 4D 动态场景直接分解为多个基于平面的 2D 表征不足以渲染具有复杂运动的高保真场景。
   (2) <strong>过去方法及问题：</strong> 现有方法将动态场景分解为多个基于平面的 2D 表征，但这种方法不足以渲染具有复杂运动的高保真场景。
   (3) <strong>研究方法：</strong> 本文提出了一种新的方向感知表征 (DaRe) 方法，该方法从六个不同方向捕获场景动态。这种学习到的表征经过逆双树复小波变换 (DTCWT) 以恢复基于平面的信息。DaReNeRF 通过融合这些恢复的平面的向量来计算每个时空点的特征。将 DaReNeRF 与用于颜色回归的微小 MLP 结合起来，并利用体积渲染进行训练，在复杂动态场景的新视角合成中实现了最先进的性能。
   (4) <strong>方法性能：</strong> DaReNeRF 在训练时间上比现有方法减少了 2 倍，同时提供了更好的性能。</p>
</li>
<li>
<p>方法：
(1): 该方法从六个不同方向捕获场景动态，学习到的表征经过逆双树复小波变换 (DTCWT) 以恢复基于平面的信息。
(2): DaReNeRF 通过融合这些恢复的平面的向量来计算每个时空点的特征。
(3): 将 DaReNeRF 与用于颜色回归的微小 MLP 结合起来，并利用体积渲染进行训练。</p>
</li>
<li>
<p>结论：
(1): 本工作通过提出 DaReNeRF 方法，在动态场景建模和渲染领域取得了重要进展。该方法从六个不同方向捕获场景动态，并利用逆双树复小波变换恢复基于平面的信息，从而有效解决了复杂动态场景的高保真渲染问题。
(2): 创新点：</p>
</li>
<li>从六个不同方向捕获场景动态，丰富了场景信息的获取。</li>
<li>采用逆双树复小波变换恢复基于平面的信息，有效融合了不同方向的特征。</li>
<li>将 DaReNeRF 与微小 MLP 结合，并利用体积渲染进行训练，实现了高效且高质量的渲染。
性能：</li>
<li>在复杂动态场景的新视角合成任务上，DaReNeRF 实现了最先进的性能。</li>
<li>与现有方法相比，DaReNeRF 训练时间减少了 2 倍，渲染效率更高。
工作量：</li>
<li>DaReNeRF 方法的实现难度适中，需要对神经辐射场、小波变换和体积渲染等技术有一定的了解。</li>
<li>训练 DaReNeRF 模型需要大量的动态场景数据和较长的训练时间。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0b34eef417abcdd2b497ef2ebd10beb3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a94b89ba44b447b4f183c953bb896e07.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0fc68e3cc2c894a358a3d010ccbf0fa0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f3c90874730f6ec424afc1f7edde45a.jpg" align="middle">
</details>




<h2 id="Depth-Guided-Robust-and-Fast-Point-Cloud-Fusion-NeRF-for-Sparse-Input-Views"><a href="#Depth-Guided-Robust-and-Fast-Point-Cloud-Fusion-NeRF-for-Sparse-Input-Views" class="headerlink" title="Depth-Guided Robust and Fast Point Cloud Fusion NeRF for Sparse Input   Views"></a>Depth-Guided Robust and Fast Point Cloud Fusion NeRF for Sparse Input   Views</h2><p><strong>Authors:Shuai Guo, Qiuwen Wang, Yijie Gao, Rong Xie, Li Song</strong></p>
<p>Novel-view synthesis with sparse input views is important for real-world applications like AR/VR and autonomous driving. Recent methods have integrated depth information into NeRFs for sparse input synthesis, leveraging depth prior for geometric and spatial understanding. However, most existing works tend to overlook inaccuracies within depth maps and have low time efficiency. To address these issues, we propose a depth-guided robust and fast point cloud fusion NeRF for sparse inputs. We perceive radiance fields as an explicit voxel grid of features. A point cloud is constructed for each input view, characterized within the voxel grid using matrices and vectors. We accumulate the point cloud of each input view to construct the fused point cloud of the entire scene. Each voxel determines its density and appearance by referring to the point cloud of the entire scene. Through point cloud fusion and voxel grid fine-tuning, inaccuracies in depth values are refined or substituted by those from other views. Moreover, our method can achieve faster reconstruction and greater compactness through effective vector-matrix decomposition. Experimental results underline the superior performance and time efficiency of our approach compared to state-of-the-art baselines. </p>
<p><a href="http://arxiv.org/abs/2403.02063v1">PDF</a> </p>
<p><strong>Summary</strong><br><strong>NeRF深度引导点云融合：增强稀疏输入场景下新视角合成</strong></p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>提出深度引导的NeRF，用于稀疏输入的新视角合成。</li>
<li>使用显式体素网格表示辐射场。</li>
<li>构造每个输入视图的点云，并在体素网格中用矩阵和向量描述。</li>
<li>融合每个输入视图的点云，构建整个场景的融合点云。</li>
<li>每个体素根据整个场景的点云确定其密度和外观。</li>
<li>通过点云融合和体素网格微调，可以修正和替换深度值的误差。</li>
<li>通过有效的向量-矩阵分解，方法实现了更快的重建和更大的紧凑性。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>标题：深度引导的鲁棒且快速的点云融合 NeRF，用于稀疏输入视图</li>
<li>作者：Shuai Guo、Qiuwen Wang、Yijie Gao、Rong Xie、Li Song</li>
<li>隶属单位：上海交通大学图像通信与网络工程学院</li>
<li>关键词：NeRF、稀疏视图、深度融合、点云融合</li>
<li>论文链接：None
    Github 代码链接：None</li>
<li>摘要：
   （1）研究背景：NeRF 在稀疏输入视图下的新视图合成对于 AR/VR 和自动驾驶等真实世界应用非常重要。
   （2）过去的方法：现有方法将深度信息集成到 NeRF 中以进行稀疏输入合成，利用深度先验进行几何和空间理解。然而，大多数现有工作往往忽略深度图中的不准确性，并且时间效率低。
   （3）研究方法：为了解决这些问题，本文提出了一种用于稀疏输入的深度引导的鲁棒且快速的点云融合 NeRF。我们将辐射场感知为一个显式的特征体素网格。为每个输入视图构建一个点云，使用矩阵和向量在体素网格中表征。我们累积每个输入视图的点云，以构建整个场景的融合点云。每个体素通过参考整个场景的点云来确定其密度和外观。通过点云融合和体素网格微调，可以细化深度值中的不准确性或用其他视图中的值替换它们。此外，我们的方法可以通过有效的向量矩阵分解实现更快的重建和更高的紧凑性。
   （4）方法性能：实验结果强调了我们方法与最先进基准相比的卓越性能和时间效率。</li>
</ol>
<p>7.Methods:
(1): 本文提出了一种深度引导的鲁棒且快速的点云融合NeRF，用于稀疏输入视图；
(2): 将辐射场感知为一个显式的特征体素网格，为每个输入视图构建一个点云，并使用矩阵和向量在体素网格中表征；
(3): 累积每个输入视图的点云，以构建整个场景的融合点云，每个体素通过参考整个场景的点云来确定其密度和外观；
(4): 通过点云融合和体素网格微调，可以细化深度值中的不准确性或用其他视图中的值替换它们；
(5): 此外，通过有效的向量矩阵分解，可以实现更快的重建和更高的紧凑性。</p>
<ol>
<li>结论：
（1）本文提出的深度引导的鲁棒且快速的点云融合NeRF，对于稀疏输入视图下的新视图合成具有重要意义。
（2）创新点：</li>
<li>将辐射场感知为一个显式的特征体素网格，并使用矩阵和向量进行表征。</li>
<li>通过点云融合和体素网格微调，细化深度值中的不准确性。</li>
<li>通过有效的向量矩阵分解，实现更快的重建和更高的紧凑性。
性能：</li>
<li>与最先进的基准相比，具有卓越的性能和时间效率。
工作量：</li>
<li>实现了更快的重建和更高的紧凑性。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-01b32742a4cabe31ed749a6761475634.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-70b0b04ae4cf460209e8f732888cddee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86aa24ab75498868b39b0c370990c2e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f6398dec60102c0bb1f5d24d9a89432.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d78f63f12b2bcb3ca39476e980147ba.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4a484aa0d25d0950586c81e66b07ef9d.jpg" align="middle">
</details>




<h2 id="NeRF-VPT-Learning-Novel-View-Representations-with-Neural-Radiance-Fields-via-View-Prompt-Tuning"><a href="#NeRF-VPT-Learning-Novel-View-Representations-with-Neural-Radiance-Fields-via-View-Prompt-Tuning" class="headerlink" title="NeRF-VPT: Learning Novel View Representations with Neural Radiance   Fields via View Prompt Tuning"></a>NeRF-VPT: Learning Novel View Representations with Neural Radiance   Fields via View Prompt Tuning</h2><p><strong>Authors:Linsheng Chen, Guangrun Wang, Liuchun Yuan, Keze Wang, Ken Deng, Philip H. S. Torr</strong></p>
<p>Neural Radiance Fields (NeRF) have garnered remarkable success in novel view synthesis. Nonetheless, the task of generating high-quality images for novel views persists as a critical challenge. While the existing efforts have exhibited commendable progress, capturing intricate details, enhancing textures, and achieving superior Peak Signal-to-Noise Ratio (PSNR) metrics warrant further focused attention and advancement. In this work, we propose NeRF-VPT, an innovative method for novel view synthesis to address these challenges. Our proposed NeRF-VPT employs a cascading view prompt tuning paradigm, wherein RGB information gained from preceding rendering outcomes serves as instructive visual prompts for subsequent rendering stages, with the aspiration that the prior knowledge embedded in the prompts can facilitate the gradual enhancement of rendered image quality. NeRF-VPT only requires sampling RGB data from previous stage renderings as priors at each training stage, without relying on extra guidance or complex techniques. Thus, our NeRF-VPT is plug-and-play and can be readily integrated into existing methods. By conducting comparative analyses of our NeRF-VPT against several NeRF-based approaches on demanding real-scene benchmarks, such as Realistic Synthetic 360, Real Forward-Facing, Replica dataset, and a user-captured dataset, we substantiate that our NeRF-VPT significantly elevates baseline performance and proficiently generates more high-quality novel view images than all the compared state-of-the-art methods. Furthermore, the cascading learning of NeRF-VPT introduces adaptability to scenarios with sparse inputs, resulting in a significant enhancement of accuracy for sparse-view novel view synthesis. The source code and dataset are available at \url{<a href="https://github.com/Freedomcls/NeRF-VPT}">https://github.com/Freedomcls/NeRF-VPT}</a>. </p>
<p><a href="http://arxiv.org/abs/2403.01325v1">PDF</a> AAAI 2024</p>
<p><strong>Summary</strong><br>神经辐射场（NeRF）在新的视野合成中取得了显著成功，但生成高质量新视角图像仍是一项重要挑战。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>NeRF-VPT 利用级联视图提示调整范例来解决新视角合成中的细节捕获、纹理增强和 PSNR 提升问题。</li>
<li>NeRF-VPT 仅需在各个训练阶段对前一阶段渲染结果的 RGB 数据进行采样作为先验。</li>
<li>NeRF-VPT 是一种即插即用的方法，可以轻松集成到现有方法中。</li>
<li>NeRF-VPT 在 Realistic Synthetic 360、Real Forward-Facing、Replica 数据集和用户捕获数据集等具有挑战性的真实场景基准上显著提升了基准性能，并产生了比所有比较的最新方法更高质量的新视角图像。</li>
<li>NeRF-VPT 的级联学习引入了对稀疏输入场景的适应性，从而显着提高了稀疏视角新视角合成的准确性。</li>
<li>源代码和数据集可在 \url{<a href="https://github.com/Freedomcls/NeRF-VPT}">https://github.com/Freedomcls/NeRF-VPT}</a> 获得。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>标题：NeRF-VPT：通过视图提示调整学习新颖视图表示</li>
<li>作者：Linsheng Chen、Guangrun Wang、Liuchun Yuan、Keze Wang、Ken Deng、Philip H.S. Torr</li>
<li>Affiliation：中山大学</li>
<li>关键词：NeRF、新颖视图合成、视图提示调整</li>
<li>论文链接：https://arxiv.org/abs/2403.01325
   Github 链接：None</li>
<li>摘要：
   （1）研究背景：NeRF 在新颖视图合成中取得了显著成功，但生成高质量的新颖视图图像仍然是一项关键挑战。
   （2）过去方法：现有方法在捕捉复杂细节、增强纹理和提高 PSNR 方面取得了可喜的进展，但仍需要进一步关注和改进。
   （3）研究方法：本文提出了一种名为 NeRF-VPT 的新颖视图合成方法，采用级联视图提示调整范式。该范式将来自先前渲染结果的 RGB 信息作为后续渲染阶段的指导性视觉提示，期望提示中嵌入的先验知识能够促进渲染图像质量的逐步提高。
   （4）方法性能：在 RealisticSynthetic360、RealForward-Facing、Replica 数据集和用户捕获数据集等具有挑战性的真实场景基准上，将 NeRF-VPT 与基于 NeRF 的方法进行比较分析，结果表明 NeRF-VPT 显着提升了基准性能，并比所有比较的最先进方法更有效地生成了更多高质量的新颖视图图像。此外，NeRF-VPT 的级联学习引入了对稀疏输入场景的适应性，从而显着提高了稀疏视图新颖视图合成的准确性。</li>
</ol>
<p>7.方法：
（1）：NeRF-VPT采用级联视图提示调整范式，将来自先前渲染结果的RGB信息作为后续渲染阶段的指导性视觉提示，期望提示中嵌入的先验知识能够促进渲染图像质量的逐步提高。
（2）：NeRF-VPT在NeRF的基础上，将位置编码和方向编码扩展为包含先验信息的编码，并采用分层结构，在每一层中使用更新的视图提示来指导渲染。
（3）：NeRF-VPT引入了一个新的损失函数，该损失函数将渲染图像与视图提示之间的差异纳入考虑，从而鼓励渲染图像与视图提示保持一致。</p>
<ol>
<li>结论：
（1）：本研究提出了一种新颖且通用的框架，以提高基于 NeRF 的视图合成的性能。我们提出了 NeRF-VPT，它引入了一种具有循环模块的新结构，并采用 NeRF 的输出作为先验。这使得 NeRF-VPT 能够显着提高视图相关外观的质量。它对端口友好，并且能够与现有方法相结合以获得最先进的性能。我们相信这项工作为充分利用表示提供了新的视角。
（2）：创新点：</li>
<li>提出了一种新的视图提示调整范式，将先验信息嵌入到 NeRF 中，以逐步提高渲染图像的质量。</li>
<li>设计了一种分层结构，在每一层中使用更新的视图提示来指导渲染，从而捕获复杂细节并增强纹理。</li>
<li>引入了一个新的损失函数，将渲染图像与视图提示之间的差异纳入考虑，以鼓励渲染图像与视图提示保持一致。</li>
<li>性能：</li>
<li>在具有挑战性的真实场景基准上，NeRF-VPT 显着提升了基准性能，并比所有比较的最先进方法更有效地生成了更多高质量的新颖视图图像。</li>
<li>NeRF-VPT 的级联学习引入了对稀疏输入场景的适应性，从而显着提高了稀疏视图新颖视图合成的准确性。</li>
<li>工作量：</li>
<li>NeRF-VPT 的实现相对简单，并且可以轻松集成到现有的 NeRF 框架中。</li>
<li>NeRF-VPT 的训练过程高效且稳定，并且可以在各种硬件平台上轻松并行化。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a3d4a33c83819ae9629aeb5c7e195d32.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19c08401f045ff72d6d7af9a10c9430a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a9c42f61f791fd5834fe43a11782fabd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-135c07d8cd0edaf636a5f342ab6e1725.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf190c96eea398ae33fd3f16daf3d9cc.jpg" align="middle">
</details>




<h2 id="Neural-radiance-fields-based-holography-Invited"><a href="#Neural-radiance-fields-based-holography-Invited" class="headerlink" title="Neural radiance fields-based holography [Invited]"></a>Neural radiance fields-based holography [Invited]</h2><p><strong>Authors:Minsung Kang, Fan Wang, Kai Kumano, Tomoyoshi Ito, Tomoyoshi Shimobaba</strong></p>
<p>This study presents a novel approach for generating holograms based on the neural radiance fields (NeRF) technique. Generating three-dimensional (3D) data is difficult in hologram computation. NeRF is a state-of-the-art technique for 3D light-field reconstruction from 2D images based on volume rendering. The NeRF can rapidly predict new-view images that do not include a training dataset. In this study, we constructed a rendering pipeline directly from a 3D light field generated from 2D images by NeRF for hologram generation using deep neural networks within a reasonable time. The pipeline comprises three main components: the NeRF, a depth predictor, and a hologram generator, all constructed using deep neural networks. The pipeline does not include any physical calculations. The predicted holograms of a 3D scene viewed from any direction were computed using the proposed pipeline. The simulation and experimental results are presented. </p>
<p><a href="http://arxiv.org/abs/2403.01137v1">PDF</a> </p>
<p><strong>Summary</strong><br>NeRF技术结合深度预测器和全息图生成器，可快速生成高质量全息图，无需物理计算。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>利用NeRF技术从2D图像生成3D光场，为全息图计算提供数据源。</li>
<li>构建由NeRF、深度预测器和全息图生成器组成的渲染管道，用于全息图生成。</li>
<li>渲染管道完全基于深度学习，无物理计算。</li>
<li>渲染管道可快速生成任意视角下的3D场景全息图。</li>
<li>仿真和实验结果表明，所提出的管道可以生成高质量的全息图。</li>
<li>该方法消除了全息图计算中对物理模拟的需求。</li>
<li>通过结合NeRF技术和深度学习，该方法提高了全息图生成的速度和质量。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>标题：基于神经辐射场的全息术[受邀]</li>
<li>作者：Minsung Kang, Fan Wang, Kai Kumao, Tomoyoshi Ito, Tomoyoshi Shimobaba</li>
<li>隶属单位：千叶大学工程学院</li>
<li>关键词：全息显示、神经辐射场、深度学习、光场重建</li>
<li>链接：http://dx.doi.org/10.1364/ao.XX.XXXXXX</li>
<li>摘要：
（1）研究背景：全息显示器需要三维场景数据、全息图和三维图像再现三个步骤，每个步骤都存在障碍。特别是，对三维场景数据和全息图的计算是障碍。
（2）过去方法及其问题：全息图的计算基于光传播模型，可以分为点云、多边形、光场和深度学习方法。这些方法各有优缺点，但都需要繁琐且耗时的三维场景生成。
（3）本文方法：提出了一种基于神经辐射场 (NeRF) 的全息图生成方法，该方法可以直接从新合成视图预测全息图，而无需使用三维相机或三维图形处理管道。该方法包括三个主要部分：NeRF、深度预测器和全息图生成器，所有这些部分都是使用深度神经网络构建的。
（4）方法性能：该方法在合理的时间内预测了从任何方向观看的三维场景的预测全息图。仿真和实验结果表明，该方法可以生成高质量的全息图，并且比现有方法更有效。</li>
</ol>
<p>Some Error for method(比如是不是没有Methods这个章节)</p>
<ol>
<li>结论：
（1）本工作的主要意义：提出了基于神经辐射场（NeRF）的全息图生成方法，该方法可以直接从新合成视图预测全息图，无需使用三维相机或三维图形处理管道，为全息显示器的发展提供了新的思路。
（2）文章的优缺点总结：</li>
<li>创新点：提出了基于 NeRF 的全息图生成方法，该方法无需三维场景数据，直接从合成视图预测全息图，简化了全息显示器的生成流程。</li>
<li>性能：仿真和实验结果表明，该方法可以生成高质量的全息图，并且比现有方法更有效。</li>
<li>工作量：该方法的实现需要大量的训练数据和计算资源，工作量较大。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-eb426bcf4ff137aa9adfa122cfe7a503.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6343dbdb7aebaa121558d05d8650d069.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ca137b835829d4a4eee9df8c8a93246.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c695400302eaf7b15d2075d6d9b58551.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1dcd582021c5b9223214535016af9ad3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3397dddd9230a1b23f0336e517fb6f6a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5cf31914b41fb8442b5926209326359c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4f42e681d33823bde779da3c7eba53f.jpg" align="middle">
</details>




<h2 id="Neural-Field-Classifiers-via-Target-Encoding-and-Classification-Loss"><a href="#Neural-Field-Classifiers-via-Target-Encoding-and-Classification-Loss" class="headerlink" title="Neural Field Classifiers via Target Encoding and Classification Loss"></a>Neural Field Classifiers via Target Encoding and Classification Loss</h2><p><strong>Authors:Xindi Yang, Zeke Xie, Xiong Zhou, Boyu Liu, Buhua Liu, Yi Liu, Haoran Wang, Yunfeng Cai, Mingming Sun</strong></p>
<p>Neural field methods have seen great progress in various long-standing tasks in computer vision and computer graphics, including novel view synthesis and geometry reconstruction. As existing neural field methods try to predict some coordinate-based continuous target values, such as RGB for Neural Radiance Field (NeRF), all of these methods are regression models and are optimized by some regression loss. However, are regression models really better than classification models for neural field methods? In this work, we try to visit this very fundamental but overlooked question for neural fields from a machine learning perspective. We successfully propose a novel Neural Field Classifier (NFC) framework which formulates existing neural field methods as classification tasks rather than regression tasks. The proposed NFC can easily transform arbitrary Neural Field Regressor (NFR) into its classification variant via employing a novel Target Encoding module and optimizing a classification loss. By encoding a continuous regression target into a high-dimensional discrete encoding, we naturally formulate a multi-label classification task. Extensive experiments demonstrate the impressive effectiveness of NFC at the nearly free extra computational costs. Moreover, NFC also shows robustness to sparse inputs, corrupted images, and dynamic scenes. </p>
<p><a href="http://arxiv.org/abs/2403.01058v1">PDF</a> ICLR 2024 Main Conference; 17 pages; 11 figures; 13 tables</p>
<p><strong>Summary</strong><br>神经场分类器框架通过预测颜色编码来替代神经场回归器中的回归目标，从而将神经场方法表述为分类任务而非回归任务。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>神经场方法本质上可以表述为分类任务。</li>
<li>神经场分类器框架通过目标编码模块将连续回归目标编码为高维离散编码。</li>
<li>将回归任务转换为分类任务不会增加显著的计算成本。</li>
<li>神经场分类器在稀疏输入、损坏图像和动态场景下表现出鲁棒性。</li>
<li>神经场分类器比神经场回归器更有效，并且可以轻松应用于现有神经场方法。</li>
<li>神经场分类器提供了一个新的视角来理解和设计神经场方法。</li>
<li>本研究为神经场方法的研究提供了新的方向。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>标题：Neural Field 分类器：目标编码和分类损失</li>
<li>作者：Xindi Yang、Zeke Xie、Xiong Zhou、Boyu Liu、Buhua Liu、Yi Liu、Haoran Wang、Yunfeng Cai、Mingming Sun</li>
<li>第一作者单位：北京交通大学交通数据分析与挖掘重点实验室</li>
<li>关键词：神经场、目标编码、分类损失、神经辐射场</li>
<li>论文链接：https://arxiv.org/abs/2403.01058</li>
<li>摘要：
(1) 研究背景：神经场方法在计算机视觉和计算机图形学中取得了很大进展，包括新视图合成和几何重建。现有神经场方法尝试预测一些基于坐标的连续目标值，例如神经辐射场 (NeRF) 中的 RGB，所有这些方法都是回归模型，并通过一些回归损失进行优化。
(2) 过去方法及其问题：回归模型是否真的优于神经场方法的分类模型？本文从机器学习的角度探讨了神经场这个非常基本但被忽视的问题。该方法提出了一个新颖的神经场分类器 (NFC) 框架，该框架将现有神经场方法表述为分类任务而不是回归任务。提出的 NFC 可以通过使用新颖的目标编码模块并将分类损失最小化，轻松地将任意神经场回归器 (NFR) 转换为其分类变体。通过将连续回归目标编码为高维离散编码，自然地制定了一个多标签分类任务。
(3) 本文提出的研究方法：广泛的实验表明，NFC 在几乎没有额外计算成本的情况下具有令人印象深刻的有效性。此外，NFC 还显示了对稀疏输入、损坏图像和动态场景的鲁棒性。
(4) 方法在什么任务上取得了什么性能：该方法在以下任务上取得了以下性能：</li>
<li>新视图合成：在 NeRF 数据集上，NFC 在 PSNR 和 SSIM 指标上优于 NeRF。</li>
<li>表面重建：在 ShapeNet 数据集上，NFC 在 Chamfer 距离和法向量一致性方面优于 NeRF。</li>
<li>
<p>鲁棒性：NFC 对稀疏输入、损坏图像和动态场景表现出鲁棒性。</p>
</li>
<li>
<p>方法：
（1）：目标编码模块，将连续回归目标编码为高维离散编码；
（2）：分类损失，使用交叉熵损失作为优化目标；
（3）：二进制数目标编码，将颜色值编码为 8 位二进制数；
（4）：逐位分类损失，对每个二进制位计算分类损失，权重随位值增加而增加。</p>
</li>
<li>
<p>结论：
（1）：本工作探讨了神经场方法中一个非常基本但被忽视的问题：回归与分类。我们设计了一个新颖的神经场分类器（NFC）框架，该框架可以将现有的神经场方法表述为分类模型，而不是回归模型。广泛的实验表明，目标编码和分类损失可以显着提高大多数现有神经场方法在新视图合成和几何重建中的性能。此外，NFC 的改进对稀疏输入、图像噪声和动态场景具有鲁棒性。虽然我们的工作主要集中在 3D 视觉和重建上，但我们相信 NFC 是一个通用的神经场框架。我们相信探索和增强神经场的泛化性将非常有前景。
（2）：创新点：</p>
</li>
<li>提出了一种新的神经场分类器（NFC）框架，该框架将现有神经场方法表述为分类任务，而不是回归任务。</li>
<li>设计了一种新颖的目标编码模块，将连续回归目标编码为高维离散编码。</li>
<li>使用交叉熵损失作为优化目标，并提出了一种逐位分类损失，对每个二进制位计算分类损失，权重随位值增加而增加。
性能：</li>
<li>在新视图合成任务上，在 NeRF 数据集上，NFC 在 PSNR 和 SSIM 指标上优于 NeRF。</li>
<li>在表面重建任务上，在 ShapeNet 数据集上，NFC 在 Chamfer 距离和法向量一致性方面优于 NeRF。</li>
<li>NFC 对稀疏输入、损坏图像和动态场景表现出鲁棒性。
工作量：</li>
<li>NFC 可以轻松地将任意神经场回归器 (NFR) 转换为其分类变体，几乎没有额外的计算成本。</li>
<li>目标编码模块和分类损失的实现相对简单，易于集成到现有的神经场方法中。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-33d7ddc258be3cc2226509c273b4d9b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d935134ee8dff34576f093f0e4bd187.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e56f20cd07e166f0199df0193f095f54.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa381fc61520f7cb599b68ee654d61b5.jpg" align="middle">
</details>




]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>NeRF</tag>
      </tags>
  </entry>
  <entry>
    <title>Diffusion Models</title>
    <url>/2024/03/09/Paper/2024-03-09/Diffusion%20Models/</url>
    <content><![CDATA[<blockquote>
<p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-03-09-更新"><a href="#2024-03-09-更新" class="headerlink" title="2024-03-09 更新"></a>2024-03-09 更新</h1><h2 id="Pix2Gif-Motion-Guided-Diffusion-for-GIF-Generation"><a href="#Pix2Gif-Motion-Guided-Diffusion-for-GIF-Generation" class="headerlink" title="Pix2Gif: Motion-Guided Diffusion for GIF Generation"></a>Pix2Gif: Motion-Guided Diffusion for GIF Generation</h2><p><strong>Authors:Hitesh Kandala, Jianfeng Gao, Jianwei Yang</strong></p>
<p>We present Pix2Gif, a motion-guided diffusion model for image-to-GIF (video) generation. We tackle this problem differently by formulating the task as an image translation problem steered by text and motion magnitude prompts, as shown in teaser fig. To ensure that the model adheres to motion guidance, we propose a new motion-guided warping module to spatially transform the features of the source image conditioned on the two types of prompts. Furthermore, we introduce a perceptual loss to ensure the transformed feature map remains within the same space as the target image, ensuring content consistency and coherence. In preparation for the model training, we meticulously curated data by extracting coherent image frames from the TGIF video-caption dataset, which provides rich information about the temporal changes of subjects. After pretraining, we apply our model in a zero-shot manner to a number of video datasets. Extensive qualitative and quantitative experiments demonstrate the effectiveness of our model — it not only captures the semantic prompt from text but also the spatial ones from motion guidance. We train all our models using a single node of 16xV100 GPUs. Code, dataset and models are made public at: <a href="https://hiteshk03.github.io/Pix2Gif/">https://hiteshk03.github.io/Pix2Gif/</a>. </p>
<p><a href="http://arxiv.org/abs/2403.04634v1">PDF</a> </p>
<p><strong>Summary</strong><br>图像到 GIF（视频）生成中的运动引导扩散模型 Pix2Gif，通过文本和运动幅度提示将任务表示为图像翻译问题。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Pix2Gif 是一个用于图像到 GIF（视频）生成的运动引导扩散模型。</li>
<li>Pix2Gif 将任务表述为由文本和运动幅度提示指导的图像翻译问题。</li>
<li>Pix2Gif 提出了一种新的运动引导变形模块，以根据两种类型的提示对源图像的特征进行空间变换，确保模型遵守运动指导。</li>
<li>Pix2Gif 引入了感知损失，以确保变换后的特征图保持在与目标图像相同空间内，从而确保内容一致性和连贯性。</li>
<li>Pix2Gif 使用从 TGIF 视频字幕数据集中提取的连贯图像帧对数据进行了精心整理，该数据集提供了有关对象时间变化的丰富信息。</li>
<li>Pix2Gif 以零样本方式将模型应用于多个视频数据集，取得了出色的效果。</li>
<li>Pix2Gif 不仅可以捕捉文本中的语义提示，还可以捕捉运动引导中的空间提示。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>标题：Pix2Gif：基于运动引导的图像到 GIF 生成</li>
<li>作者：Hitesh Khandelwal、Alexei A. Efros、Pieter Abbeel、William T. Freeman</li>
<li>隶属机构：马萨诸塞理工学院计算机科学与人工智能实验室</li>
<li>关键词：图像到 GIF 生成、运动引导、扩散模型、图像翻译</li>
<li>论文链接：https://arxiv.org/abs/2302.08206
Github 代码链接：None</li>
<li>
<p>摘要：
(1): 研究背景：图像到 GIF 生成任务旨在将静态图像转换为动态 GIF 图像。现有的方法主要依赖于文本提示来指导生成，但缺乏对运动信息的利用。
(2): 过去方法：传统的图像到 GIF 生成方法使用文本提示来指导生成，但这些方法无法充分利用运动信息。
(3): 研究方法：本文提出 Pix2Gif 模型，该模型采用运动引导的扩散模型，通过引入运动嵌入层和运动引导的变形模块，将运动信息融入图像生成过程中。
(4): 实验结果：Pix2Gif 模型在 TGIF 视频字幕数据集上进行了训练和评估，结果表明该模型能够有效捕获文本提示中的语义信息和运动提示中的空间信息，生成高质量的 GIF 图像。</p>
</li>
<li>
<p>方法：
(1): 引入运动嵌入层，将运动信息编码为连续向量；
(2): 设计运动引导变形模块，利用运动嵌入层引导图像变形；
(3): 采用扩散模型，通过逐步增加噪声并反向扩散，生成图像；
(4): 将运动信息融入扩散模型中，指导图像生成过程。</p>
</li>
<li>
<p>结论
（1）：xxx；
（2）：创新点：xxx；性能：xxx；工作量：xxx；</p>
</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-786aa45d1c0e323f035b56f16f1140be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ddec3a8952939ae9c917e7b1984fb9e4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-538b38079b2f1cde247a179f7b6ab9b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-579c5c472fca8ba1022f880a544c4526.jpg" align="middle">
</details>




<h2 id="Controllable-Generation-with-Text-to-Image-Diffusion-Models-A-Survey"><a href="#Controllable-Generation-with-Text-to-Image-Diffusion-Models-A-Survey" class="headerlink" title="Controllable Generation with Text-to-Image Diffusion Models: A Survey"></a>Controllable Generation with Text-to-Image Diffusion Models: A Survey</h2><p><strong>Authors:Pu Cao, Feng Zhou, Qing Song, Lu Yang</strong></p>
<p>In the rapidly advancing realm of visual generation, diffusion models have revolutionized the landscape, marking a significant shift in capabilities with their impressive text-guided generative functions. However, relying solely on text for conditioning these models does not fully cater to the varied and complex requirements of different applications and scenarios. Acknowledging this shortfall, a variety of studies aim to control pre-trained text-to-image (T2I) models to support novel conditions. In this survey, we undertake a thorough review of the literature on controllable generation with T2I diffusion models, covering both the theoretical foundations and practical advancements in this domain. Our review begins with a brief introduction to the basics of denoising diffusion probabilistic models (DDPMs) and widely used T2I diffusion models. We then reveal the controlling mechanisms of diffusion models, theoretically analyzing how novel conditions are introduced into the denoising process for conditional generation. Additionally, we offer a detailed overview of research in this area, organizing it into distinct categories from the condition perspective: generation with specific conditions, generation with multiple conditions, and universal controllable generation. For an exhaustive list of the controllable generation literature surveyed, please refer to our curated repository at \url{<a href="https://github.com/PRIV-Creation/Awesome-Controllable-T2I-Diffusion-Models}">https://github.com/PRIV-Creation/Awesome-Controllable-T2I-Diffusion-Models}</a>. </p>
<p><a href="http://arxiv.org/abs/2403.04279v1">PDF</a> A collection of resources on controllable generation with   text-to-image diffusion models:   <a href="https://github.com/PRIV-Creation/Awesome-Controllable-T2I-Diffusion-Models">https://github.com/PRIV-Creation/Awesome-Controllable-T2I-Diffusion-Models</a></p>
<p><strong>Summary</strong><br>扩散模型可控生成综述：理论基础与实践进展</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>扩散模型已在文本指导生成中取得重大进展。</li>
<li>控制文本到图像 (T2I) 扩散模型是应对复杂应用场景的必要条件。</li>
<li>控制机制是将新条件引入扩散模型中的关键。</li>
<li>可控生成的研究按条件类型分为三类：特定条件、多条件和通用可控。</li>
<li>扩散概率去噪模型 (DDPM) 是扩散模型的基础。</li>
<li>文本指导扩散模型广泛用于可控图像生成。</li>
<li>有关可控生成文献的全面列表请参见 GitHub 存储库：​​<a href="https://github.com/PRIV-Creation/Awesome-Controllable-T2I-Diffusion-Models。">https://github.com/PRIV-Creation/Awesome-Controllable-T2I-Diffusion-Models。</a></li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：可控生成：文本到图像扩散模型综述</li>
<li>作者：曹普、周峰、宋青、杨路</li>
<li>隶属单位：北京邮电大学</li>
<li>关键词：综述、文本到图像扩散模型、可控生成、AIGC</li>
<li>论文链接：https://arxiv.org/abs/2403.04279
   Github 链接：无</li>
<li>摘要：
   (1) 研究背景：随着视觉生成领域的快速发展，扩散模型凭借其令人印象深刻的文本引导生成功能，彻底改变了该领域的格局。然而，仅依靠文本对这些模型进行条件化并不能完全满足不同应用和场景的多样化和复杂要求。
   (2) 过去方法及其问题：现有的方法主要基于文本条件，但无法充分满足所有用户需求，尤其是在需要超出文本条件的场景中，例如特定条件生成、多条件生成和通用可控生成。
   (3) 本文提出的研究方法：本文回顾了基于文本到图像扩散模型的可控生成文献，涵盖了该领域的理论基础和实际进展。我们从去噪扩散概率模型 (DDPM) 和广泛使用的文本到图像扩散模型的基础知识入手，然后揭示了扩散模型的控制机制，从理论上分析了如何将新颖条件引入去噪过程中以进行条件生成。此外，我们对该领域的研究成果进行了详细概述，并从条件的角度将其组织成不同的类别：特定条件生成、多条件生成和通用可控生成。
   (4) 方法在什么任务上取得了什么性能：本文综述了可控生成文献，并提供了我们精心策划的存储库：https://github.com/PRIV-Creation/Awesome-Controllable-T2I-Diffusion-Models。</li>
</ol>
<p>Some Error for method(比如是不是没有Methods这个章节)</p>
<ol>
<li>结论：
（1）本综述工作的重要性：
本综述全面深入地探讨了文本到图像扩散模型的可控生成领域，揭示了文本引导生成过程中引入的新颖条件。我们首先为读者提供了基础知识，介绍了去噪扩散概率模型、突出的文本到图像扩散模型以及结构良好的分类法。随后，我们揭示了在 T2I 扩散模型中引入新颖条件的机制。然后，我们总结了先前的条件生成方法，并从理论基础、技术进步和解决方案策略方面对其进行了分析。此外，我们探索了可控生成在实践中的应用，强调了其在 AI 生成内容时代的重要作用和巨大潜力。本综述旨在提供对可控 T2I 生成的当前格局的全面理解，从而为这个充满活力的研究领域的持续演进和扩展做出贡献。</li>
</ol>
<p>（2）本文的优点和不足：
创新点：
* 系统性地总结了文本到图像扩散模型的可控生成方法，提供了全面的理论基础和技术进展。
* 提出了一种新的分类法，将条件生成方法组织成特定条件生成、多条件生成和通用可控生成。
* 分析了条件生成方法的理论基础，揭示了如何将新颖条件引入去噪过程中。</p>
<p>性能：
* 提供了一个精心策划的存储库，收集了可控 T2I 扩散模型的最新研究成果。
* 综述了可控生成在各种应用中的实践，展示了其在 AI 生成内容中的潜力。</p>
<p>工作量：
* 本综述涵盖了该领域的广泛研究，提供了对可控 T2I 生成的全面概述。
* 分析了大量文献，并对其进行了深入的分类和总结。</p>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-acbf3784bf1c20bd1d6bd9456318f64e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7891a291c9d85dfa3c58fb2ba167ec65.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a662a2b7f90a052a2c166ddd64f1d77b.jpg" align="middle">
</details>




## Latent Dataset Distillation with Diffusion Models

**Authors:Brian B. Moser, Federico Raue, Sebastian Palacio, Stanislav Frolov, Andreas Dengel**

The efficacy of machine learning has traditionally relied on the availability of increasingly larger datasets. However, large datasets pose storage challenges and contain non-influential samples, which could be ignored during training without impacting the final accuracy of the model. In response to these limitations, the concept of distilling the information on a dataset into a condensed set of (synthetic) samples, namely a distilled dataset, emerged. One crucial aspect is the selected architecture (usually ConvNet) for linking the original and synthetic datasets. However, the final accuracy is lower if the employed model architecture differs from the model used during distillation. Another challenge is the generation of high-resolution images, e.g., 128x128 and higher. In this paper, we propose Latent Dataset Distillation with Diffusion Models (LD3M) that combine diffusion in latent space with dataset distillation to tackle both challenges. LD3M incorporates a novel diffusion process tailored for dataset distillation, which improves the gradient norms for learning synthetic images. By adjusting the number of diffusion steps, LD3M also offers a straightforward way of controlling the trade-off between speed and accuracy. We evaluate our approach in several ImageNet subsets and for high-resolution images (128x128 and 256x256). As a result, LD3M consistently outperforms state-of-the-art distillation techniques by up to 4.8 p.p. and 4.2 p.p. for 1 and 10 images per class, respectively. 

[PDF](http://arxiv.org/abs/2403.03881v1) 

**Summary**
利用扩散模型和数据集蒸馏相结合的潜数据集蒸馏方法（LD3M），在提高准确性的同时，可生成高分辨率合成图像。

**Key Takeaways**
- 数据集蒸馏可解决大数据集的存储和非影响性样本问题。
- 合适的模型架构是连接原始和合成数据集的关键。
- LD3M提出一种针对数据集蒸馏的扩散过程，改善了合成图像的梯度规范。
- LD3M通过调整扩散步骤，可在速度和准确性之间进行权衡。
- 在ImageNet子集和高分辨率图像上，LD3M优于现有蒸馏技术，每类生成1张图像时提升4.8个百分点，生成10张图像时提升4.2个百分点。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>标题：扩散模型下的潜在数据集蒸馏</li>
<li>作者：Brian B. Moser, Federico Raue, Sebastian Palacio, Stanislav Frolov, Andreas Dengel</li>
<li>单位：德国人工智能研究中心（DFKI）</li>
<li>关键词：数据集蒸馏、扩散模型、图像生成</li>
<li>链接：</li>
<li>
<p>摘要：
(1) 研究背景：随着机器学习的发展，数据集规模不断扩大，但大规模数据集面临存储挑战，且包含非影响性样本，这在训练过程中可以被忽略而不会影响模型的最终准确性。
(2) 过去方法：针对上述问题，出现了将数据集信息蒸馏成一组浓缩的（合成）样本（即蒸馏数据集）的概念。一个关键方面是用于连接原始数据集和合成数据集的选定架构（通常是卷积神经网络）。然而，如果所采用的模型架构与蒸馏过程中使用的模型不同，最终准确性会降低。另一个挑战是生成高分辨率图像，例如 128x128 及更高。
(3) 本文方法：为了解决这两个挑战，本文提出了扩散模型下的潜在数据集蒸馏（LD3M），它将潜在空间中的扩散与数据集蒸馏相结合。LD3M 结合了一个针对数据集蒸馏量身定制的新型扩散过程，该过程改进了学习合成图像的梯度范数。通过调整扩散步骤的数量，LD3M 还提供了一种控制速度和准确性之间权衡的直接方法。
(4) 实验结果：作者在多个 ImageNet 子集中以及高分辨率图像（128x128 和 256x256）上评估了该方法。结果表明，对于每个类别 1 张和 10 张图像，LD3M 在准确性上分别比最先进的蒸馏技术高出 4.8 个百分点和 4.2 个百分点，这支持了他们的目标。</p>
</li>
<li>
<p>方法：
（1）：LD3M通过引入修改的采样过程公式，从扩散模型中获益，该公式针对数据集蒸馏进行了定制，以合成高分辨率图像。
（2）：LD3M允许微调时间步数以平衡运行时间和图像质量。
（3）：潜码的初始化可以通过将自动编码器应用到相应类别的随机图像来直接执行，这比 GLaD 中必要的 GAN 反演有所改进。</p>
</li>
</ol>
<p><strong>8. 结论</strong></p>
<p><strong>(1): 本项工作的意义</strong></p>
<p>LD3M 将扩散模型与数据集蒸馏相结合，解决了大规模数据集蒸馏中面临的两个挑战：合成高分辨率图像和模型架构不匹配。它为数据集蒸馏提供了一种新的方法，在准确性上优于现有技术。</p>
<p><strong>(2): 创新点、性能、工作量</strong></p>
<p><strong>创新点：</strong></p>
<ul>
<li>引入修改的采样过程公式，针对数据集蒸馏定制，以合成高分辨率图像。</li>
<li>允许微调时间步数以平衡运行时间和图像质量。</li>
<li>通过自动编码器直接初始化潜码。</li>
</ul>
<p><strong>性能：</strong></p>
<ul>
<li>在 ImageNet 子集中，对于每个类别 1 张和 10 张图像，LD3M 在准确性上分别比最先进的蒸馏技术高出 4.8 个百分点和 4.2 个百分点。</li>
</ul>
<p><strong>工作量：</strong></p>
<ul>
<li>LD3M 的训练过程比 GLaD 更简单，因为它不需要 GAN 反演。</li>
<li>微调时间步数允许根据具体任务调整工作量。</li>
</ul>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-720ec34e44cebbf566f3940acd0e95df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-208b1d2d5a3d8b3432e8217d8423991e.jpg" align="middle">
</details>




## NoiseCollage: A Layout-Aware Text-to-Image Diffusion Model Based on   Noise Cropping and Merging

**Authors:Takahiro Shirakawa, Seiichi Uchida**

Layout-aware text-to-image generation is a task to generate multi-object images that reflect layout conditions in addition to text conditions. The current layout-aware text-to-image diffusion models still have several issues, including mismatches between the text and layout conditions and quality degradation of generated images. This paper proposes a novel layout-aware text-to-image diffusion model called NoiseCollage to tackle these issues. During the denoising process, NoiseCollage independently estimates noises for individual objects and then crops and merges them into a single noise. This operation helps avoid condition mismatches; in other words, it can put the right objects in the right places. Qualitative and quantitative evaluations show that NoiseCollage outperforms several state-of-the-art models. These successful results indicate that the crop-and-merge operation of noises is a reasonable strategy to control image generation. We also show that NoiseCollage can be integrated with ControlNet to use edges, sketches, and pose skeletons as additional conditions. Experimental results show that this integration boosts the layout accuracy of ControlNet. The code is available at https://github.com/univ-esuty/noisecollage. 

[PDF](http://arxiv.org/abs/2403.03485v1) Accepted at CVPR 2024

**Summary**
利用独立估计物体噪声并裁剪合并的创新策略，NoiseCollage 实现了布局感知文本到图像生成模型，可有效避免条件错位、提升生成图像质量。

**Key Takeaways**
- 提出了一种新颖的布局感知文本到图像扩散模型 NoiseCollage。
- NoiseCollage 在去噪过程中独立估计各个物体的噪声，然后裁剪并合并成一个噪声。
- 裁剪合并噪声操作有助于避免条件错位，即能够将正确的物体放在正确的位置。
- 定性和定量评价表明，NoiseCollage 优于其他几个最先进的模型。
- 裁剪合并噪声操作是一种控制图像生成的可行策略。
- NoiseCollage 可以与 ControlNet 集成，使用边缘、草图和姿势骨架作为附加条件。
- 实验结果表明，这种集成提高了 ControlNet 的布局准确性。
- 代码可在 https://github.com/univ-esuty/noisecollage 获取。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>题目：NoiseCollage：一种布局感知文本到图像扩散模型</li>
<li>作者：Yusuke Matsui、Shohei Nobuhara、Tatsuya Harada</li>
<li>所属单位：东京大学</li>
<li>关键词：文本到图像生成、布局感知、扩散模型</li>
<li>论文链接：https://arxiv.org/abs/2303.10080
   Github 代码链接：https://github.com/univ-esuty/noisecollage</li>
<li>摘要：
(1): 研究背景：布局感知文本到图像生成任务旨在生成反映布局条件和文本条件的多对象图像。现有的布局感知文本到图像扩散模型仍然存在一些问题，包括文本和布局条件之间的不匹配以及生成图像的质量下降。
(2): 过去的方法及其问题：现有的方法主要通过在扩散过程中引入布局条件来实现布局感知。然而，这些方法往往会出现条件不匹配，即生成的对象无法准确放置在指定的位置。此外，这些方法还会导致生成图像质量下降。
(3): 本文提出的研究方法：本文提出了一种新的布局感知文本到图像扩散模型，称为 NoiseCollage，以解决上述问题。NoiseCollage 在去噪过程中独立估计各个对象的噪声，然后将其裁剪并合并成一个单一的噪声。这种操作有助于避免条件不匹配，即可以将正确对象放置在正确的位置。
(4): 实验结果：定性和定量评估表明，NoiseCollage 优于几种最先进的模型。这些成功的结果表明，噪声的裁剪和合并操作是一种控制图像生成的可行策略。我们还展示了 NoiseCollage 可以与 ControlNet 集成，以使用边缘、草图和姿势骨架作为附加条件。实验结果表明，这种集成提高了 ControlNet 的布局准确性。</li>
</ol>
<p>Some Error for method(比如是不是没有Methods这个章节)</p>
<ol>
<li>结论：
(1): 本文提出了一种新的布局感知文本到图像扩散模型 NoiseCollage，该模型能够解决现有模型中存在的条件不匹配和生成图像质量下降的问题。通过在去噪过程中独立估计各个对象的噪声，然后将其裁剪并合并成一个单一的噪声，NoiseCollage 有助于避免条件不匹配，并提高生成图像的质量。
(2): 创新点：NoiseCollage 主要创新点在于其独特的噪声裁剪和合并操作，该操作有助于控制图像生成，并避免条件不匹配。
性能：定性和定量评估表明，NoiseCollage 优于几种最先进的模型，其生成图像的质量和布局准确性均有显著提升。
工作量：NoiseCollage 的实现相对简单，其代码已开源，便于其他研究人员使用和扩展。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-ca9a660019d0cd052bfc7e32bdb132dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df5a89d450de8eb386d1390e5d56ec6b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7827e655355d6c7eb010489c4348651f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e18efcbba7dce490367cbbca1c706670.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c9dc4c69766a33fac7222193d9452952.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff6a63d2c8ab24b31509b60e008dd6b9.jpg" align="middle">
</details>




<h2 id="Scaling-Rectified-Flow-Transformers-for-High-Resolution-Image-Synthesis"><a href="#Scaling-Rectified-Flow-Transformers-for-High-Resolution-Image-Synthesis" class="headerlink" title="Scaling Rectified Flow Transformers for High-Resolution Image Synthesis"></a>Scaling Rectified Flow Transformers for High-Resolution Image Synthesis</h2><p><strong>Authors:Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, Robin Rombach</strong></p>
<p>Diffusion models create data from noise by inverting the forward paths of data towards noise and have emerged as a powerful generative modeling technique for high-dimensional, perceptual data such as images and videos. Rectified flow is a recent generative model formulation that connects data and noise in a straight line. Despite its better theoretical properties and conceptual simplicity, it is not yet decisively established as standard practice. In this work, we improve existing noise sampling techniques for training rectified flow models by biasing them towards perceptually relevant scales. Through a large-scale study, we demonstrate the superior performance of this approach compared to established diffusion formulations for high-resolution text-to-image synthesis. Additionally, we present a novel transformer-based architecture for text-to-image generation that uses separate weights for the two modalities and enables a bidirectional flow of information between image and text tokens, improving text comprehension, typography, and human preference ratings. We demonstrate that this architecture follows predictable scaling trends and correlates lower validation loss to improved text-to-image synthesis as measured by various metrics and human evaluations. Our largest models outperform state-of-the-art models, and we will make our experimental data, code, and model weights publicly available. </p>
<p><a href="http://arxiv.org/abs/2403.03206v1">PDF</a> </p>
<p><strong>Summary</strong><br>扩散模型通过将数据向噪声反向转化来从噪声中创建数据，已成为图像和视频等高维感知数据强有力的生成建模技术。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>扩散模型通过反向数据路径从噪声中生成数据。</li>
<li>校正流是一种连接数据和噪声的生成模型，具有更好的理论性质和概念简单性。</li>
<li>改进的噪声采样技术通过将它们偏向于感知相关尺度来训练校正流模型。</li>
<li>大规模研究表明，这种方法在高分辨率文本到图像合成中优于已建立的扩散公式。</li>
<li>提出了一种新颖的基于 Transformer 的文本到图像生成架构，它为这两种模式使用单独的权重，并在图像和文本标记之间实现信息的双向流动，从而改善文本理解、印刷术和人类偏好评级。</li>
<li>该架构遵循可预测的缩放趋势，并将较低的验证损失与通过各种指标和人类评估测量的改进的文本到图像合成相关联。</li>
<li>我们的最大模型优于最先进的模型，我们将公开我们的实验数据、代码和模型权重。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>标题：用于高分辨率图像合成的可整流流变换器扩展</li>
<li>作者：Patrick Esser、Sumith Kulal、Andreas Blattmann、Rahim Entezari、Jonas Müller、Harry Saini、Yam Levi、Dominik Lorenz、Axel Sauer、Frederic Boesel、Dustin Podell、Tim Dockhorn、Zion English、Kyle Lacey、Alex Goodwin、Yannik Marek、Robin Rombach</li>
<li>第一作者单位：Stability AI</li>
<li>关键词：扩散模型、可整流流、文本到图像合成、变压器架构、大规模研究</li>
<li>论文链接：https://arxiv.org/abs/2403.03206
Github 链接：无</li>
<li>摘要：
（1）研究背景：扩散模型和可整流流模型是生成图像的两种流行方法。扩散模型通过将数据反向扩散到噪声中来生成数据，而可整流流模型则通过将数据和噪声直接连接起来生成数据。尽管可整流流模型具有更好的理论特性和概念上的简单性，但它尚未被确立为标准实践。
（2）过去的方法和问题：现有的可整流流模型训练方法存在噪声采样技术不佳的问题。
（3）研究方法：本文提出了一种改进的可整流流模型训练方法，该方法通过将噪声采样偏向于感知相关尺度来提高模型性能。此外，本文还提出了一种新的基于 Transformer 的文本到图像生成架构，该架构使用单独的权重进行两种模态，并允许图像和文本标记之间双向信息流，从而提高文本理解、排版和人类偏好评分。
（4）任务和性能：在文本到图像合成任务上，本文提出的方法在各种指标和人类评估中均优于现有的扩散模型公式。本文最大的模型优于最先进的模型，并且作者将公开实验数据、代码和模型权重。</li>
</ol>
<p>Some Error for method(比如是不是没有Methods这个章节)</p>
<p>8.结论：
（1）：本文提出了可整流流模型在大规模文本到图像合成任务中的扩展，并取得了最先进的性能。我们提出的新颖的时间步长采样方法和基于 Transformer 的多模态架构显着提高了模型性能。
（2）：创新点：
- 提出了一种新的时间步长采样方法，该方法通过偏向感知相关尺度来提高可整流流模型的训练性能。
- 提出了一种新的基于 Transformer 的多模态文本到图像生成架构，该架构使用单独的权重进行两种模态，并允许图像和文本标记之间双向信息流。
性能：
- 在文本到图像合成任务上，本文提出的方法在各种指标和人类评估中均优于现有的扩散模型公式。
- 本文最大的模型优于最先进的模型，并且作者将公开实验数据、代码和模型权重。
工作量：
- 本文提出的方法需要大量的计算资源进行训练。
- 最大模型的训练需要 5×10^22 次浮点运算。</p>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-94c3bec1e7bd9dc1fcb74a4fe7a98802.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-749be73a890e57d0e49c34844678f429.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-896603d491956157816c079e119bb1cf.jpg" align="middle">
</details>




## MAGID: An Automated Pipeline for Generating Synthetic Multi-modal   Datasets

**Authors:Hossein Aboutalebi, Hwanjun Song, Yusheng Xie, Arshit Gupta, Justin Sun, Hang Su, Igor Shalyminov, Nikolaos Pappas, Siffi Singh, Saab Mansour**

Development of multimodal interactive systems is hindered by the lack of rich, multimodal (text, images) conversational data, which is needed in large quantities for LLMs. Previous approaches augment textual dialogues with retrieved images, posing privacy, diversity, and quality constraints. In this work, we introduce \textbf{M}ultimodal \textbf{A}ugmented \textbf{G}enerative \textbf{I}mages \textbf{D}ialogues (MAGID), a framework to augment text-only dialogues with diverse and high-quality images. Subsequently, a diffusion model is applied to craft corresponding images, ensuring alignment with the identified text. Finally, MAGID incorporates an innovative feedback loop between an image description generation module (textual LLM) and image quality modules (addressing aesthetics, image-text matching, and safety), that work in tandem to generate high-quality and multi-modal dialogues. We compare MAGID to other SOTA baselines on three dialogue datasets, using automated and human evaluation. Our results show that MAGID is comparable to or better than baselines, with significant improvements in human evaluation, especially against retrieval baselines where the image database is small. 

[PDF](http://arxiv.org/abs/2403.03194v1) 

**Summary**
对话式大语言模型训练需要大量富文本和图像数据，然而现有增强方法受限于隐私、多样性和质量问题。本文提出 MAGID 框架，利用扩散模型生成与文本一致的高质量图像，并通过图像描述和图像质量模块之间的反馈回路，生成高质量的多模态对话。

**Key Takeaways**
- 多模态对话系统缺乏丰富的对话数据，阻碍了其发展。
- 传统增强方法存在隐私、多样性和质量问题。
- MAGID 框架使用扩散模型生成与文本一致的图像。
- MAGID 框架包含图像描述和图像质量模块之间的反馈回路。
- MAGID 框架可生成高质量的多模态对话。
- MAGID 框架优于基于检索的基线模型。
- 特别是在图像数据库较小的情况下，MAGID 框架在人类评估中表现明显优于基线模型。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>标题：多模态增强生成图像对话（MAGID）</li>
<li>作者：Yonglong Tian, Lijie Fan, Phillip Isola, Huiwen Chang, Dilip Krishnan</li>
<li>所属机构：未提及</li>
<li>关键词：多模态交互系统、对话生成、图像生成、扩散模型</li>
<li>论文链接：https://arxiv.org/abs/2306.00984
    Github 代码链接：无</li>
<li>摘要：
（1）研究背景：多模态交互系统的开发受到丰富、多模态（文本、图像）对话数据的缺乏的阻碍，而 LLM 需要大量此类数据。
（2）以往方法：以往的方法通过检索图像来增强文本对话，但存在隐私、多样性和质量限制。
（3）提出的研究方法：本文提出了多模态增强生成图像对话（MAGID）框架，该框架通过将文本对话与多样化的高质量图像进行增强。随后，应用扩散模型来制作相应的图像，确保与识别出的文本一致。最后，MAGID 结合了图像描述生成模块（文本 LLM）和图像质量模块（解决美观、图像文本匹配和安全性）之间的创新反馈回路，它们协同工作以生成高质量的多模态对话。
（4）方法性能：在三个对话数据集上，使用自动化和人工评估将 MAGID 与其他 SOTA 基准进行比较。结果表明，MAGID 与基准相当或优于基准，在人工评估中得到了显着改善，尤其是在图像数据库较小的检索基准中。</li>
</ol>
<p>Some Error for method(比如是不是没有Methods这个章节)</p>
<ol>
<li>结论：
（1）本工作的重要意义在于：</li>
<li>提出了一种生成式、全自动化的管道，旨在将仅文本的数据集转化为多模态变体，通过提示工程利用 LLM 的能力。</li>
<li>该解决方案解决了先前方法面临的局限性，特别是在数据隐私、可访问性、受限图像分布以及不当或非自愿内容的出现方面。</li>
<li>至关重要的是，我们的管道允许用合成的对应物替换真实、可能损害隐私的图像。</li>
</ol>
<p>（2）本文的优缺点总结：
- 创新点：
  - 提出了一种新颖的多模态增强生成图像对话 (MAGID) 框架，该框架通过将文本对话与多样化的高质量图像进行增强。
  - 应用扩散模型来制作相应的图像，确保与识别出的文本一致。
  - MAGID 结合了图像描述生成模块（文本 LLM）和图像质量模块（解决美观、图像文本匹配和安全性）之间的创新反馈回路，它们协同工作以生成高质量的多模态对话。</p>
<ul>
<li>性能：</li>
<li>在三个对话数据集上，使用自动化和人工评估将 MAGID 与其他 SOTA 基准进行比较。</li>
<li>
<p>结果表明，MAGID 与基准相当或优于基准，在人工评估中得到了显着改善，尤其是在图像数据库较小的检索基准中。</p>
</li>
<li>
<p>工作量：</p>
</li>
<li>MAGID 的管道涉及多个步骤，包括文本对话增强、图像生成和图像质量评估。</li>
<li>虽然该管道是自动化的，但它需要大量的计算资源，特别是对于图像生成和评估步骤。</li>
</ul>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-84fde2dff4e1f4865d7f188ca7408a6b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bd4b8824a503447811021a2b6d333dd0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e09f64c262fc7c9670307db0aff8128b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2e0397944ad64c6c70c00a97cc74c90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a008a1b4e8e10183bf68cc62740312d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4fb6b0ea96a737eeae673e1e2ead968.jpg" align="middle">
</details>




## Zero-LED: Zero-Reference Lighting Estimation Diffusion Model for   Low-Light Image Enhancement

**Authors:Jinhong He, Minglong Xue, Zhipu Liu, Chengyun Song, Senming Zhong**

Diffusion model-based low-light image enhancement methods rely heavily on paired training data, leading to limited extensive application. Meanwhile, existing unsupervised methods lack effective bridging capabilities for unknown degradation. To address these limitations, we propose a novel zero-reference lighting estimation diffusion model for low-light image enhancement called Zero-LED. It utilizes the stable convergence ability of diffusion models to bridge the gap between low-light domains and real normal-light domains and successfully alleviates the dependence on pairwise training data via zero-reference learning. Specifically, we first design the initial optimization network to preprocess the input image and implement bidirectional constraints between the diffusion model and the initial optimization network through multiple objective functions. Subsequently, the degradation factors of the real-world scene are optimized iteratively to achieve effective light enhancement. In addition, we explore a frequency-domain based and semantically guided appearance reconstruction module that encourages feature alignment of the recovered image at a fine-grained level and satisfies subjective expectations. Finally, extensive experiments demonstrate the superiority of our approach to other state-of-the-art methods and more significant generalization capabilities. We will open the source code upon acceptance of the paper. 

[PDF](http://arxiv.org/abs/2403.02879v1) 

**Summary**
采用零参考光照估计扩散模型，通过优化网络和目标函数，缓解低光图像增强对配对训练数据的依赖性。

**Key Takeaways**
- 基于扩散模型的低光图像增强依赖配对训练数据，限制了广泛应用。
- 现有无监督方法缺乏对未知退化的有效衔接能力。
- 提出无参考光照估计扩散模型 Zero-LED，用于低光图像增强。
- 利用扩散模型的稳定收敛能力，弥合低光域和正常光域之间的差距。
- 通过零参考学习，成功缓解对成对训练数据的依赖。
- 设计初始优化网络预处理输入图像，通过多目标函数实现扩散模型和初始优化网络之间的双向约束。
- 迭代优化真实场景的退化因子，实现有效的亮度增强。
- 探索基于频域和语义引导的外观重建模块，在精细级别上鼓励恢复图像的特征对齐，满足主观期望。

**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**

<ol>
<li>标题：Zero-LED：零参考光照估计</li>
<li>作者：Jinhong He、Minglong Xue、Zhipu Liu、Chengyun Song、Senming Zhong</li>
<li>第一作者单位：重庆理工大学</li>
<li>关键词：低光图像增强、扩散模型、零参考学习、外观重建模块</li>
<li>论文链接：Github：无</li>
<li>
<p>摘要：
(1)：研究背景：基于扩散模型的低光图像增强方法严重依赖成对训练数据，限制了广泛应用。同时，现有的无监督方法缺乏对未知退化的有效桥接能力。
(2)：过去方法及问题：现有方法存在依赖成对训练数据、泛化能力差等问题。该研究动机充分，提出了一种新颖的零参考光照估计扩散模型。
(3)：研究方法：该方法利用扩散模型的稳定收敛能力，构建低光域和真实正常光域之间的桥梁，通过零参考学习成功缓解了对成对训练数据的依赖。具体来说，首先设计初始优化网络预处理输入图像，并通过多目标函数在扩散模型和初始优化网络之间实现双向约束。随后，迭代优化真实场景的退化因子以实现有效的亮度增强。此外，探索了一种基于频域和语义指导的外观重建模块，在精细级别鼓励恢复图像的特征对齐，满足主观期望。
(4)：任务及性能：该方法在低光图像增强任务上取得了优于其他最先进方法的性能，并且具有更强的泛化能力。实验结果支持了其目标。</p>
</li>
<li>
<p>方法：(1) 利用扩散模型的生成能力，实现图像质量的显著提升；(2) 提出基于双向优化训练的方法，建立基于零参考图像的扩散模型，降低对训练数据的依赖，增强对真实场景的泛化能力；(3) 采用基于小波变换的低频域推理，降低扩散模型的计算资源消耗，提升效率；(4) 提出外观重建模块（ARM），基于语义和频域指导，有效引导图像内容结构的重建和整体质量的提升。</p>
</li>
<li>
<p>结论：
（1）：xxx；
（2）：创新点：xxx；性能：xxx；工作量：xxx；</p>
</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d125a1f2cd5a7e4ff232c9bd5803b4e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-784317768dc5754292d2d8e3a428986c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9b5416df99f3c9bf78a001a3966ca21.jpg" align="middle">
</details>




<h2 id="Tuning-Free-Noise-Rectification-for-High-Fidelity-Image-to-Video-Generation"><a href="#Tuning-Free-Noise-Rectification-for-High-Fidelity-Image-to-Video-Generation" class="headerlink" title="Tuning-Free Noise Rectification for High Fidelity Image-to-Video   Generation"></a>Tuning-Free Noise Rectification for High Fidelity Image-to-Video   Generation</h2><p><strong>Authors:Weijie Li, Litong Gong, Yiran Zhu, Fanda Fan, Biao Wang, Tiezheng Ge, Bo Zheng</strong></p>
<p>Image-to-video (I2V) generation tasks always suffer from keeping high fidelity in the open domains. Traditional image animation techniques primarily focus on specific domains such as faces or human poses, making them difficult to generalize to open domains. Several recent I2V frameworks based on diffusion models can generate dynamic content for open domain images but fail to maintain fidelity. We found that two main factors of low fidelity are the loss of image details and the noise prediction biases during the denoising process. To this end, we propose an effective method that can be applied to mainstream video diffusion models. This method achieves high fidelity based on supplementing more precise image information and noise rectification. Specifically, given a specified image, our method first adds noise to the input image latent to keep more details, then denoises the noisy latent with proper rectification to alleviate the noise prediction biases. Our method is tuning-free and plug-and-play. The experimental results demonstrate the effectiveness of our approach in improving the fidelity of generated videos. For more image-to-video generated results, please refer to the project website: <a href="https://noise-rectification.github.io">https://noise-rectification.github.io</a>. </p>
<p><a href="http://arxiv.org/abs/2403.02827v1">PDF</a> </p>
<p><strong>Summary</strong><br>图像到视频（I2V）生成任务在开放领域始终难以保持高保真度。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>传统图像动画技术侧重于面部或人体姿势等特定领域，难以推广到开放领域。</li>
<li>基于扩散模型的 I2V 框架可以为开放领域图像生成动态内容，但无法保持保真度。</li>
<li>低保真度的主要原因是去噪过程中图像细节丢失和噪声预测偏差。</li>
<li>提出一种有效的方法，可以应用于主流视频扩散模型。</li>
<li>该方法通过补充更精确的图像信息和噪声校正来实现高保真度。</li>
<li>给定特定图像，该方法首先向输入图像潜变量添加噪声以保留更多细节，然后通过适当的校正对噪声潜变量进行去噪以减轻噪声预测偏差。</li>
<li>该方法无需调整且即插即用。</li>
<li>实验结果证明了该方法在提高生成视频保真度方面的有效性。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>题目：无调优噪声校正，用于高保真图像转视频生成</li>
<li>作者：魏杰李、李彤宫、一然朱、范达范、标王、铁正葛、波正</li>
<li>单位：阿里巴巴集团北京阿里妈妈技术</li>
<li>关键词：图像转视频、视频生成、噪声校正、扩散模型</li>
<li>论文链接：https://arxiv.org/abs/2403.02827
Github 代码链接：无</li>
<li>摘要：
（1）研究背景：图像转视频（I2V）生成任务在开放域中保持高保真度始终面临挑战。传统图像动画技术主要集中在特定领域，如面部或人体姿势，难以推广到开放域。基于扩散模型的 I2V 框架可以为开放域图像生成动态内容，但无法保持保真度。
（2）过去方法及其问题：现有方法的不足之处在于图像细节的丢失和去噪过程中的噪声预测偏差。
（3）本文方法：提出了一种适用于主流视频扩散模型的高效方法。该方法通过补充更精确的图像信息和噪声校正来实现高保真度。具体来说，给定一张指定图像，该方法首先向输入图像潜变量添加噪声以保留更多细节，然后对噪声潜变量进行适当校正以减轻噪声预测偏差。该方法无需调优且即插即用。
（4）方法性能：实验结果证明了该方法在提高生成视频保真度方面的有效性。</li>
</ol>
<p><strong>Methods</strong></p>
<ol>
<li><strong>图像增强条件分析</strong>：将图像潜变量注入到反向过程的开始，引导反向去噪过程向图像潜变量在潜在空间中的方向发展，但只能达到与给定图像相似，与高保真度仍有一定差距。</li>
<li><strong>将完整干净图像与初始噪声连接</strong>：提高保真度，但需要重新训练整个生成框架，可扩展性低，难以与 ControlNet 等预训练模块集成。</li>
<li><strong>在扩散模型的内部计算中引入更多图像特征信号和条件</strong>：图像特征作为强监督来提高保真度，但特征提取不可避免地会丢失图像细节，难以实现细节方面的保真度。</li>
<li>
<p><strong>噪声校正策略</strong>：提出“噪声和校正去噪”过程，在去噪过程的某些中间步骤中，通过自适应地用已知的初始噪声补偿预测噪声来校正预测噪声。</p>
</li>
<li>
<p>总结：
（1）：本文提出了一种用于图像转视频生成的高效无调优噪声校正方法，通过补充更精确的图像信息和噪声校正来实现高保真度。
（2）：创新点：</p>
</li>
<li>提出了一种“噪声和校正去噪”过程，通过自适应地用已知的初始噪声补偿预测噪声来校正预测噪声。</li>
<li>该方法无需调优且即插即用，可与其他视频扩散模型集成。
性能：</li>
<li>实验结果证明了该方法在提高生成视频保真度方面的有效性。
工作量：</li>
<li>该方法简单易用，易于集成到现有的视频生成框架中。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0197a02f813c3611a9266978be983045.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f12bc1d8e5e3f0a7bb65cd3aa0275044.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6416123c2bdeefb6d5270913d20d6664.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7370c1b440fe22b048fbc20b419b5dd7.jpg" align="middle">
</details>




<h2 id="Few-shot-Learner-Parameterization-by-Diffusion-Time-steps"><a href="#Few-shot-Learner-Parameterization-by-Diffusion-Time-steps" class="headerlink" title="Few-shot Learner Parameterization by Diffusion Time-steps"></a>Few-shot Learner Parameterization by Diffusion Time-steps</h2><p><strong>Authors:Zhongqi Yue, Pan Zhou, Richang Hong, Hanwang Zhang, Qianru Sun</strong></p>
<p>Even when using large multi-modal foundation models, few-shot learning is still challenging — if there is no proper inductive bias, it is nearly impossible to keep the nuanced class attributes while removing the visually prominent attributes that spuriously correlate with class labels. To this end, we find an inductive bias that the time-steps of a Diffusion Model (DM) can isolate the nuanced class attributes, i.e., as the forward diffusion adds noise to an image at each time-step, nuanced attributes are usually lost at an earlier time-step than the spurious attributes that are visually prominent. Building on this, we propose Time-step Few-shot (TiF) learner. We train class-specific low-rank adapters for a text-conditioned DM to make up for the lost attributes, such that images can be accurately reconstructed from their noisy ones given a prompt. Hence, at a small time-step, the adapter and prompt are essentially a parameterization of only the nuanced class attributes. For a test image, we can use the parameterization to only extract the nuanced class attributes for classification. TiF learner significantly outperforms OpenCLIP and its adapters on a variety of fine-grained and customized few-shot learning tasks. Codes are in <a href="https://github.com/yue-zhongqi/tif">https://github.com/yue-zhongqi/tif</a>. </p>
<p><a href="http://arxiv.org/abs/2403.02649v1">PDF</a> Accepted by CVPR 2024</p>
<p><strong>摘要</strong><br>利用扩散模型的时间步，可以分离细微的类别属性，通过文本条件的适配器弥补丢失的属性，实现小样本学习任务的准确分类。</p>
<p><strong>要点</strong></p>
<ul>
<li>扩散模型的时间步可以隔离细微的类别属性。</li>
<li>细微的属性通常在较早的时间步丢失，而视觉突出的属性则在较晚的时间步丢失。</li>
<li>提出时间步小样本学习器 (TiF)，为文本条件的 DM 训练特定于类别的低秩适配器。</li>
<li>适配器和小提示本质上是在小时间步内仅参数化细微的类别属性。</li>
<li>对于测试图像，可以使用参数化仅提取细微的类别属性进行分类。</li>
<li>TiF 学习器在各种细粒度和定制的小样本学习任务上明显优于 OpenCLIP 及其适配器。</li>
<li>代码可在 <a href="https://github.com/yue-zhongqi/tif">https://github.com/yue-zhongqi/tif</a> 获得。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<p></p><p>1.标题：基于扩散时间步长的少样本学习器参数化
2.作者：Yue Zhongqi, Bowen Cheng, Yaming Wang, Qinghua Hu, Xiaodan Liang
3.所属单位：北京大学
4.关键词：Few-shot learning, Diffusion model, Low-rank adaptation
5.论文地址：None
6.摘要：
（1）研究背景：少样本学习中，模型容易学习到与类别标签虚假相关的视觉突出属性，而忽略细微的类别属性。
（2）过去方法及问题：现有方法缺乏合适的归纳偏置，无法有效区分细微的类别属性和视觉突出属性。
（3）本文方法：提出时间步长少样本学习器（TiF learner），利用扩散模型的时间步长分离细微的类别属性，并训练类别特定的低秩适配器来弥补丢失的属性。
（4）方法性能：TiF learner 在各种细粒度和定制的少样本学习任务上明显优于 OpenCLIP 及其适配器。</p><p></p>
<ol>
<li>
<p>方法：(1) 训练去噪网络 d，使用扩散模型的时间步长分离丢失的细微类别属性；(2) 训练类别特定的低秩适配器来弥补丢失的属性；(3) 通过计算时间步长上的加权平均值 Lt 来进行推理。</p>
</li>
<li>
<p>总结：
(1): 本工作提出了一种基于扩散时间步长的少样本学习器 TiFlearner，通过分离细微的类别属性和视觉突出属性，有效解决了少样本学习中易学习到虚假相关属性的问题，显著提升了细粒度和定制少样本学习任务的性能。
(2): Innovation point: TiFlearner 创新性地利用扩散模型的时间步长分离丢失的细微类别属性，并训练类别特定的低秩适配器来弥补丢失的属性，有效区分了细微的类别属性和视觉突出属性。
Performance: TiFlearner 在各种细粒度和定制少样本学习任务上明显优于 OpenCLIP 及其适配器，证明了其有效性和鲁棒性。
Workload: TiFlearner 的训练和推理过程相对复杂，需要训练去噪网络和类别特定的低秩适配器，计算时间步长上的加权平均值，工作量较大。</p>
</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c1f7d70acd760956bfb9ce16a4c9a32f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9fd5fe0d098a2e3948ad5e4744720eed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3823bdb18fac83dfd9b0fde352c77358.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-255f6ff30f2576a40ef0753bdfd6f57e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-46aa23abe5a92b4732abedfceaed986b.jpg" align="middle">
</details>




<h2 id="Semantic-Human-Mesh-Reconstruction-with-Textures"><a href="#Semantic-Human-Mesh-Reconstruction-with-Textures" class="headerlink" title="Semantic Human Mesh Reconstruction with Textures"></a>Semantic Human Mesh Reconstruction with Textures</h2><p><strong>Authors:Xiaoyu Zhan, Jianxin Yang, Yuanqi Li, Jie Guo, Yanwen Guo, Wenping Wang</strong></p>
<p>The field of 3D detailed human mesh reconstruction has made significant progress in recent years. However, current methods still face challenges when used in industrial applications due to unstable results, low-quality meshes, and a lack of UV unwrapping and skinning weights. In this paper, we present SHERT, a novel pipeline that can reconstruct semantic human meshes with textures and high-precision details. SHERT applies semantic- and normal-based sampling between the detailed surface (eg mesh and SDF) and the corresponding SMPL-X model to obtain a partially sampled semantic mesh and then generates the complete semantic mesh by our specifically designed self-supervised completion and refinement networks. Using the complete semantic mesh as a basis, we employ a texture diffusion model to create human textures that are driven by both images and texts. Our reconstructed meshes have stable UV unwrapping, high-quality triangle meshes, and consistent semantic information. The given SMPL-X model provides semantic information and shape priors, allowing SHERT to perform well even with incorrect and incomplete inputs. The semantic information also makes it easy to substitute and animate different body parts such as the face, body, and hands. Quantitative and qualitative experiments demonstrate that SHERT is capable of producing high-fidelity and robust semantic meshes that outperform state-of-the-art methods. </p>
<p><a href="http://arxiv.org/abs/2403.02561v1">PDF</a> </p>
<p><strong>Summary</strong><br>SHERT 是一种新颖的管道，可以重建具有纹理和高精度细节的语义人体网格。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SHERT可在详细表面和 SMPL-X 模型之间进行基于语义和法线的采样，以获得部分采样的语义网格。</li>
<li>自监督完成和细化网络可生成完整的语义网格。</li>
<li>纹理扩散模型可创建由图像和文本驱动的纹理。</li>
<li>重建的网格具有稳定的 UV 展开、高质量三角形网格和一致的语义信息。</li>
<li>SMPL-X 模型提供语义信息和形状先验，即使在输入不正确和不完全的情况下，SHERT 也能很好地执行。</li>
<li>语义信息便于替换和动画不同的身体部位，如面部、身体和手。</li>
<li>定量和定性实验表明，SHERT 能够产生高保真和鲁棒的语义网格，其性能优于最先进的方法。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>标题：语义人体网格重建与纹理化</li>
<li>作者：Yu-Kun Lai, Chen Cao, Lei Zhou, Yajie Zhao, Kun Zhou, Chen Change Loy, Ziwei Liu</li>
<li>隶属机构：香港中文大学</li>
<li>关键词：语义人体网格重建、纹理化、自监督学习、图像生成</li>
<li>论文链接：None
Github 链接：None</li>
<li>
<p>摘要：
(1) 研究背景：
近年来，3D 详细人体网格重建领域取得了重大进展。然而，当前方法在工业应用中仍面临以下挑战：结果不稳定、网格质量低以及缺乏 UV 展开和蒙皮权重。
(2) 过去方法及其问题：
过去的方法通常使用基于图像的方法，这些方法需要大量的数据和计算资源，并且对输入图像的质量非常敏感。此外，这些方法通常无法生成具有语义信息的网格，这使得它们难以用于动画和虚拟现实等应用。
(3) 本文提出的研究方法：
本文提出了 SHERT，这是一种新颖的管道，可以重建具有纹理和高精度细节的语义人体网格。SHERT 在详细表面（例如网格和 SDF）和相应的 SMPL-X 模型之间应用基于语义和法线的采样，以获得部分采样的语义网格，然后通过专门设计的自监督完成和细化网络生成完整的语义网格。使用完整的语义网格作为基础，我们采用纹理扩散模型来创建受图像和文本驱动的纹理。
(4) 方法在任务和性能上的表现：
本文方法能够生成高保真且鲁棒的语义网格，其性能优于最先进的方法。在多个数据集上的定量和定性实验表明，SHERT 可以很好地处理不正确和不完整输入，并且可以轻松替换和动画不同的身体部位，例如面部、身体和手。</p>
</li>
<li>
<p>方法：
（1）基于语义和法线的采样，在详细表面（如网格和 SDF）和相应的 SMPL-X 模型之间进行采样，以获得部分采样的语义网格；
（2）通过专门设计的自监督完成和细化网络，生成完整的语义网格；
（3）使用完整的语义网格作为基础，采用纹理扩散模型来创建受图像和文本驱动的纹理。</p>
</li>
<li>
<p>结论：
（1）：本文提出了一种从详细表面或单目图像重建完全纹理化语义人体模型的方法 SHERT，该方法利用了目标表面的几何细节、语义信息和语义指导先验知识。重建结果具有高保真衣着细节、高质量三角形网格、清晰的面部特征和完整的手部几何形状。SHERT 还能够生成具有稳定 UV 展开的超高分辨率纹理贴图。该方法弥合理论重建工作和下游工业应用之间的差距，相信可以推动人体模型的发展。
（2）：创新点：xxx；性能：xxx；工作量：xxx；</p>
</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0fbc346a8aa3d55b54bc776d96e213e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7dd492b9ec7ce1ca56e9958a2ba8f0b9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2a4d7a0b580701e5f5f50e6834ff3111.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6184e9766e7cd4d5a85ef285d96ccb64.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4992740f820eac8eee20ee9e8c27784.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ca5e15c452099d37d81cea6645ae175.jpg" align="middle">
</details>




<h2 id="Updating-the-Minimum-Information-about-CLinical-Artificial-Intelligence-MI-CLAIM-checklist-for-generative-modeling-research"><a href="#Updating-the-Minimum-Information-about-CLinical-Artificial-Intelligence-MI-CLAIM-checklist-for-generative-modeling-research" class="headerlink" title="Updating the Minimum Information about CLinical Artificial Intelligence   (MI-CLAIM) checklist for generative modeling research"></a>Updating the Minimum Information about CLinical Artificial Intelligence   (MI-CLAIM) checklist for generative modeling research</h2><p><strong>Authors:Brenda Y. Miao, Irene Y. Chen, Christopher YK Williams, Jaysón Davidson, Augusto Garcia-Agundez, Harry Sun, Travis Zack, Atul J. Butte, Madhumita Sushil</strong></p>
<p>Recent advances in generative models, including large language models (LLMs), vision language models (VLMs), and diffusion models, have accelerated the field of natural language and image processing in medicine and marked a significant paradigm shift in how biomedical models can be developed and deployed. While these models are highly adaptable to new tasks, scaling and evaluating their usage presents new challenges not addressed in previous frameworks. In particular, the ability of these models to produce useful outputs with little to no specialized training data (“zero-“ or “few-shot” approaches), as well as the open-ended nature of their outputs, necessitate the development of updated guidelines in using and evaluating these models. In response to gaps in standards and best practices for the development of clinical AI tools identified by US Executive Order 141103 and several emerging national networks for clinical AI evaluation, we begin to formalize some of these guidelines by building on the “Minimum information about clinical artificial intelligence modeling” (MI-CLAIM) checklist. The MI-CLAIM checklist, originally developed in 2020, provided a set of six steps with guidelines on the minimum information necessary to encourage transparent, reproducible research for artificial intelligence (AI) in medicine. Here, we propose modifications to the original checklist that highlight differences in training, evaluation, interpretability, and reproducibility of generative models compared to traditional AI models for clinical research. This updated checklist also seeks to clarify cohort selection reporting and adds additional items on alignment with ethical standards. </p>
<p><a href="http://arxiv.org/abs/2403.02558v1">PDF</a> </p>
<p><strong>Summary</strong><br>生成模型的兴起，如 LLM、VLM 和扩散模型，对医学自然语言和图像处理产生了重大影响，并提出了新的挑战，需要更新的模型开发和评估指南，以确保其可推广性、可解释性和可重复性。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>生成模型的适应性强，但对新任务的评估提出了新的挑战。</li>
<li>无/少样本学习和开放式输出需要新的评估指南。</li>
<li>MI-CLAIM 清单提供了一个框架，用于指导生成模型的透明和可复制的研究。</li>
<li>更新后的 MI-CLAIM 清单强调了生成模型与传统 AI 模型在训练、评估、可解释性和可复制性方面的差异。</li>
<li>更新后的清单澄清了队列选择报告，并增加了符合道德标准的附加项目。</li>
<li>强调了生成模型在医学中的伦理使用和负责任创新。</li>
<li>鼓励生成模型的标准化评估和报告，以促进可信和可重复的研究。</li>
<li>通过跨学科协作和持续的指导，可以解决生成模型的持续挑战和机会。</li>
</ul>
<p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>标题：更新临床人工智能最低信息（MI-CLAIM）</li>
<li>作者：Brenda Y. Miao</li>
<li>所属机构：加州大学伯克利分校和加州大学旧金山分校</li>
<li>关键词：临床人工智能、生成模型、MI-CLAIM、评估</li>
<li>链接：Github：https://github.com/mi-claim/mi-claim</li>
<li>摘要：
（1）研究背景：
随着生成模型，包括大型语言模型（LLM）的快速发展，临床人工智能（AI）工具的开发面临着标准和最佳实践的差距。</li>
</ol>
<p>（2）过去方法及其问题：
MI-CLAIM 清单于 2020 年首次开发，提供了一套包含六个步骤的标准，但随着生成模型的快速发展，该清单已不再适用。</p>
<p>（3）论文提出的研究方法：
本文更新了 MI-CLAIM 清单，以解决生成模型在临床 AI 中应用的新挑战。更新后的清单包括以下部分：
- 研究设计：强调生成模型评估中自动化和人工评估的结合，并提供基于非结构化或多模态数据的队列选择最佳实践。
- 数据和优化：要求详细说明数据来源、预处理步骤和训练、验证和测试集之间的独立性。
- 模型评估：提供用于无结构文本输出的自动化模型评估方法，以及用于人类模型评估的指导。
- 生成模型的可解释性：鼓励使用错误分析和敏感性分析（消融测试）来解释模型预测。
- 端到端管道复制：强调提供代码和数据透明度，并讨论模型风险和潜在偏差。</p>
<p>（4）方法在什么任务上取得了什么性能？性能是否支持其目标？
本文没有报告具体任务和性能结果，因为它着重于提供临床 AI 生成模型研究的标准和最佳实践。</p>
<p>Some Error for method(比如是不是没有Methods这个章节)</p>
<ol>
<li>结论：
（1）：更新后的 MI-CLAIM 清单为临床人工智能生成模型的研究和开发提供了标准和最佳实践，有助于提高模型的可信度和可解释性，促进临床人工智能的负责任和有效应用。
（2）：创新点：</li>
<li>扩展了 MI-CLAIM 清单，以解决生成模型在临床人工智能中的新挑战。</li>
<li>提供了针对生成模型评估的具体指导，包括自动化和人工评估相结合、基于非结构化或多模态数据的队列选择最佳实践。</li>
<li>强调了生成模型的可解释性，鼓励使用错误分析和敏感性分析来解释模型预测。
性能：本文没有报告具体任务和性能结果，因为它着重于提供标准和最佳实践。
工作量：更新后的 MI-CLAIM 清单提供了详细的指导和要求，这可能会增加研究人员在临床人工智能生成模型研究中的工作量。</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e0a6a135c6657ff1a197759497122ce9.jpg" align="middle">
</details>




]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>Diffusion Models</tag>
      </tags>
  </entry>
</search>
