<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>CUDA编程实践：LLTM加速优化实践 | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="接下来再进行一个实践，根据pytorch官网的一个[应用扩展文档](https:&#x2F;&#x2F;pytorch.org&#x2F;tutorials&#x2F;advanced&#x2F;cpp_extension.html#)来一起来实现这个LLTM的神经网络的实现，这是官方对应的代码 https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;extension-cpp。 首先介绍一下，假设有一种新的循环单元，这个循环单元类似于 LSTM，"><meta property="og:type" content="article"><meta property="og:title" content="CUDA编程实践：LLTM加速优化实践"><meta property="og:url" content="https://kedreamix.github.io/2023/12/12/CUDA/CUDA%E7%BC%96%E7%A8%8B%E5%AE%9E%E8%B7%B5%EF%BC%9ALLTM%E5%8A%A0%E9%80%9F%E4%BC%98%E5%8C%96%E5%AE%9E%E8%B7%B5/index.html"><meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World"><meta property="og:description" content="接下来再进行一个实践，根据pytorch官网的一个[应用扩展文档](https:&#x2F;&#x2F;pytorch.org&#x2F;tutorials&#x2F;advanced&#x2F;cpp_extension.html#)来一起来实现这个LLTM的神经网络的实现，这是官方对应的代码 https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;extension-cpp。 首先介绍一下，假设有一种新的循环单元，这个循环单元类似于 LSTM，"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://pic1.zhimg.com/80/v2-5328b2df743543e0ca21d09eeaf45c28_1440w.jpeg"><meta property="article:published_time" content="2023-12-12T07:30:00.000Z"><meta property="article:modified_time" content="2024-01-30T05:04:13.226Z"><meta property="article:author" content="Kedreamix"><meta property="article:tag" content="CUDA"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://pic1.zhimg.com/80/v2-5328b2df743543e0ca21d09eeaf45c28_1440w.jpeg"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/2023/12/12/CUDA/CUDA%E7%BC%96%E7%A8%8B%E5%AE%9E%E8%B7%B5%EF%BC%9ALLTM%E5%8A%A0%E9%80%9F%E4%BC%98%E5%8C%96%E5%AE%9E%E8%B7%B5/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-74LZ5BEQQ1")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!0,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"简"},noticeOutdate:{limitDay:500,position:"top",messagePrev:"It has been",messageNext:"days since the last update, the content of the article may be outdated."},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:200},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!0,post:!0},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!0,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"CUDA编程实践：LLTM加速优化实践",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-01-30 13:04:13"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach((e=>{n.setAttribute(e,t[e])})),document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)})),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=24?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 7.0.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml"></head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{const d=document.getElementById("loading-box"),e=document.body,o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l(),window.addEventListener("load",(()=>{o()}))})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">137</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url('https://pic1.zhimg.com/80/v2-5328b2df743543e0ca21d09eeaf45c28_1440w.jpeg')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">CUDA编程实践：LLTM加速优化实践</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-12-12T07:30:00.000Z" title="发表于 2023-12-12 15:30:00">2023-12-12</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-01-30T05:04:13.226Z" title="更新于 2024-01-30 13:04:13">2024-01-30</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/CUDA/">CUDA</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">7.3k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>30分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="CUDA编程实践：LLTM加速优化实践"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>Github：<a target="_blank" rel="noopener" href="https://github.com/Kedreamix/pytorch-cppcuda-tutorial">https://github.com/Kedreamix/pytorch-cppcuda-tutorial</a></p><p>前馈知识：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/671704557">Kedreamix：CUDA编程学习：自定义Pytorch+cpp/cuda extension</a></p><p>接下来再进行一个实践，根据pytorch官网的一个<a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/advanced/cpp_extension.html#">应用扩展文档</a>来一起来实现这个LLTM的神经网络的实现，这是官方对应的代码 <a target="_blank" rel="noopener" href="https://github.com/pytorch/extension-cpp。">https://github.com/pytorch/extension-cpp。</a></p><p>首先介绍一下，假设有一种新的循环单元，这个循环单元类似于 LSTM，但不同之处在于它没有遗忘门，并使用指数线性单元 (<a target="_blank" rel="noopener" href="https://so.csdn.net/so/search?q=ELU&amp;spm=1001.2101.3001.7020">ELU</a>) 作为其内部激活函数。因为这个单元能够记忆很久，我们称之为 LLTM，或 Long-Long-Term-Memory 单元。</p><p>LLTM 与普通 LSTM 的两种不同之处非常重要，以至于我们无法为我们的目的配置 PyTorch 的 LSTM 算子，因此我们必须创建一个自定义算子。第一个也是最简单的方法——可能在所有情况下都是很好的第一步——是用 Python 在普通的 PyTorch 中实现我们想要的功能。为此，我们需要继承 torch.nn.Module 并实现 LLTM 的 forward。</p><h2 id="Pytorch普通实现"><a href="#Pytorch普通实现" class="headerlink" title="Pytorch普通实现"></a>Pytorch普通实现</h2><p>接下来我们就用普通的pytorch对其进行实现LLTM，具体代码如下</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LLTM</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_features, state_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(LLTM, self).__init__()</span><br><span class="line">        self.input_features = input_features</span><br><span class="line">        self.state_size = state_size</span><br><span class="line">        <span class="comment"># 3 * state_size for input gate, output gate and candidate cell gate.</span></span><br><span class="line">        <span class="comment"># input_features + state_size because we will multiply with [input, h].</span></span><br><span class="line">        self.weights = torch.nn.Parameter(</span><br><span class="line">            torch.empty(<span class="number">3</span> * state_size, input_features + state_size))</span><br><span class="line">        self.bias = torch.nn.Parameter(torch.empty(<span class="number">3</span> * state_size))</span><br><span class="line">        self.reset_parameters()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reset_parameters</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        重置模型参数的方法。</span></span><br><span class="line"><span class="string">        初始化权重和偏差的值，使用均匀分布和标准差进行初始化。</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        stdv = <span class="number">1.0</span> / math.sqrt(self.state_size)</span><br><span class="line">        <span class="keyword">for</span> weight <span class="keyword">in</span> self.parameters():</span><br><span class="line">            weight.data.uniform_(-stdv, +stdv)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span>, state</span>):</span><br><span class="line">        old_h, old_cell = state</span><br><span class="line">        X = torch.cat([old_h, <span class="built_in">input</span>], dim=<span class="number">1</span>)</span><br><span class="line">        gate_weights = F.linear(X, self.weights, self.bias)</span><br><span class="line">        gates = gate_weights.chunk(<span class="number">3</span>, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        input_gate = torch.sigmoid(gates[<span class="number">0</span>])</span><br><span class="line">        output_gate = torch.sigmoid(gates[<span class="number">1</span>])</span><br><span class="line">        candidate_cell = F.elu(gates[<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">        new_cell = old_cell + candidate_cell * input_gate</span><br><span class="line">        new_h = torch.tanh(new_cell) * output_gate</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> new_h, new_cell</span><br></pre></td></tr></tbody></table></figure><h2 id="Python扩展实现"><a href="#Python扩展实现" class="headerlink" title="Python扩展实现"></a>Python扩展实现</h2><p>我们在大多数的时候，都是用上述的方法进行对Pytorch进行扩展，因为 PyTorch 对 CPU 和 GPU 的操作实现了高度优化，并由 <a target="_blank" rel="noopener" href="https://developer.nvidia.com/cudnn">NVIDIA cuDNN</a>、<a target="_blank" rel="noopener" href="https://software.intel.com/en-us/mkl">Intel MKL</a> 或 <a target="_blank" rel="noopener" href="https://github.com/Maratyszcza/NNPACK">NNPACK</a> 等库提供支持。因此，通常情况下，上述的PyTorch代码已经足够快速。</p><p>然而，在某些情况下，还有进一步提升性能的空间。这是因为PyTorch可能不了解我们实现的特定算法，它只知道我们使用的各种操作。因此，它可能会逐个执行这些操作的内核（可能涉及启动CUDA内核），这会导致累积的开销变得很大。此外，Python本身也存在一些速度限制。</p><p>因此，一种明显的加速方法是使用C++（或CUDA）对部分代码进行重写，并将特定的操作组合起来。组合意味着将许多函数的实现合并为一个函数，这样可以启动较少的内核，并且可以通过提高全局数据流的可见性来执行其他优化。</p><p>接下来，我们将分别使用<strong>Python，C++，Pytorch</strong>扩展来实现LLTM（Long Long-Term Memory）的组合版本。我们将从使用纯C++编写它开始，使用<a target="_blank" rel="noopener" href="https://github.com/zdevito/ATen">ATen</a>库，该库为PyTorch的大部分后端提供支持。然后，我们将通过将模型的部分移动到CUDA内核中，以利用GPU提供的大规模并行性，进一步提高速度。</p><p>在上述的代码中，为了后续对写C++扩展有一个更好的理解，所以再加上一部分首先实现反向传播的代码。所以我们需要手写我们对应函数出现的微分，这里面我们就需要先计算基础函数的微分函数，分别是的<code>d_sigmoid</code>，<code>d_tanh</code>和<code>d_elu</code>，后续可以对其进行复用。</p><hr><p>在这一部分的代码中，结果实际上的方式和上面代码是一样的，主要是就是独立实现了一下对应的反向传播部分，方便后续进行训练的反向传播的CUDA编写，并且让大家也更加理解一下对应底层的C++和CUDA反向传播的代码的编写原理。</p><p>在前面我们有介绍过<code>ctx</code>可以保存我们反向传播需要的参数，所以这里使用了<code>ctx.save_for_backward(X, weights, input_gate, output_gate, old_cell,new_cell, candidate_cell, gate_weights)</code>，最后返回了两个值，分别是<code>new_h, new_cell</code>，并且这两部分我们也需要在反向传播中传入两个参数进行求微分，在<code>forward</code>函数中，我们有四个参数<code>input, weights, bias, old_h, old_cell</code>，所以反向传播的最后返回值也分别是<code>d_input, d_weights, d_bias, d_old_h, d_old_cell</code>进行一一对应。</p><h3 id="基础函数"><a href="#基础函数" class="headerlink" title="基础函数"></a>基础函数</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">d_sigmoid</span>(<span class="params">z</span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    计算sigmoid函数的导数</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    z (tensor): 输入张量</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回值:</span></span><br><span class="line"><span class="string">    d_sigmoid (tensor): sigmoid函数的导数张量</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    s = torch.sigmoid(z)</span><br><span class="line">    <span class="keyword">return</span> (<span class="number">1</span> - s) * s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">d_tanh</span>(<span class="params">z</span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    计算tanh函数的导数</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    z (tensor): 输入张量</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回值:</span></span><br><span class="line"><span class="string">    d_tanh (tensor): tanh函数的导数张量</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    t = torch.tanh(z)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> - (t * t)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">d_elu</span>(<span class="params">z, alpha=<span class="number">1.0</span></span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    计算ELU函数的导数</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    z (tensor): 输入张量</span></span><br><span class="line"><span class="string">    alpha (float): ELU函数的alpha参数，默认为1.0</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回值:</span></span><br><span class="line"><span class="string">    d_elu (tensor): ELU函数的导数张量</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    e = z.exp()</span><br><span class="line">    mask = (alpha * (e - <span class="number">1</span>)) &lt; <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> (z &gt; <span class="number">0</span>).type_as(z) + mask.type_as(z) * (alpha * e)</span><br></pre></td></tr></tbody></table></figure><h3 id="前向和反向传播"><a href="#前向和反向传播" class="headerlink" title="前向和反向传播"></a>前向和反向传播</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义一个自定义的LLTM函数，继承自torch.autograd.Function类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LLTMPythonFunction</span>(torch.autograd.Function):</span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">ctx, <span class="built_in">input</span>, weights, bias, old_h, old_cell</span>):</span><br><span class="line">        X = torch.cat([old_h, <span class="built_in">input</span>], dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        gate_weights = F.linear(X, weights, bias)</span><br><span class="line">        gates = gate_weights.chunk(<span class="number">3</span>, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        input_gate = torch.sigmoid(gates[<span class="number">0</span>])</span><br><span class="line">        output_gate = torch.sigmoid(gates[<span class="number">1</span>])</span><br><span class="line">        candidate_cell = F.elu(gates[<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">        new_cell = old_cell + candidate_cell * input_gate</span><br><span class="line">        new_h = torch.tanh(new_cell) * output_gate</span><br><span class="line"></span><br><span class="line">        ctx.save_for_backward(X, weights, input_gate, output_gate, old_cell,</span><br><span class="line">                            new_cell, candidate_cell, gate_weights)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> new_h, new_cell</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">ctx, grad_h, grad_cell</span>):</span><br><span class="line">        <span class="comment"># 从上下文中获取保存的变量</span></span><br><span class="line">        X, weights, input_gate, output_gate, old_cell = ctx.saved_variables[:<span class="number">5</span>]</span><br><span class="line">        new_cell, candidate_cell, gate_weights = ctx.saved_variables[<span class="number">5</span>:]</span><br><span class="line"></span><br><span class="line">        d_input = d_weights = d_bias = d_old_h = d_old_cell = <span class="literal">None</span></span><br><span class="line">		</span><br><span class="line">        <span class="comment"># 计算关于输出门和 tanh(new_cell) 的梯度</span></span><br><span class="line">        d_output_gate = torch.tanh(new_cell) * grad_h</span><br><span class="line">        d_tanh_new_cell = output_gate * grad_h</span><br><span class="line">        d_new_cell = d_tanh(new_cell) * d_tanh_new_cell + grad_cell</span><br><span class="line"></span><br><span class="line">        d_old_cell = d_new_cell</span><br><span class="line">        d_candidate_cell = input_gate * d_new_cell</span><br><span class="line">        d_input_gate = candidate_cell * d_new_cell</span><br><span class="line">		</span><br><span class="line">        <span class="comment"># 将门控权重分割成输入门、输出门和候选细胞状态的梯度</span></span><br><span class="line">        gates = gate_weights.chunk(<span class="number">3</span>, dim=<span class="number">1</span>)</span><br><span class="line">        d_input_gate *= d_sigmoid(gates[<span class="number">0</span>])</span><br><span class="line">        d_output_gate *= d_sigmoid(gates[<span class="number">1</span>])</span><br><span class="line">        d_candidate_cell *= d_elu(gates[<span class="number">2</span>])</span><br><span class="line">		</span><br><span class="line">        <span class="comment"># 拼接三个门的梯度</span></span><br><span class="line">        d_gates = torch.cat(</span><br><span class="line">        [d_input_gate, d_output_gate, d_candidate_cell], dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果需要计算对权重的梯度</span></span><br><span class="line">        <span class="keyword">if</span> ctx.needs_input_grad[<span class="number">1</span>]:</span><br><span class="line">            d_weights = d_gates.t().mm(X)</span><br><span class="line">        <span class="comment"># 如果需要计算对偏置的梯度</span></span><br><span class="line">        <span class="keyword">if</span> ctx.needs_input_grad[<span class="number">2</span>]:</span><br><span class="line">            d_bias = d_gates.<span class="built_in">sum</span>(dim=<span class="number">0</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 如果需要计算对上一个时间步的隐藏状态和输入的梯度</span></span><br><span class="line">        <span class="keyword">if</span> ctx.needs_input_grad[<span class="number">3</span>] <span class="keyword">or</span> ctx.needs_input_grad[<span class="number">4</span>]:</span><br><span class="line">            d_X = d_gates.mm(weights)</span><br><span class="line">            state_size = grad_h.shape[<span class="number">1</span>]</span><br><span class="line">            d_old_h, d_input = d_X[:, :state_size], d_X[:, state_size:]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> d_input, d_weights, d_bias, d_old_h, d_old_cell</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LLTM</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_features, state_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(LLTMPython, self).__init__()</span><br><span class="line">        self.input_features = input_features</span><br><span class="line">        self.state_size = state_size</span><br><span class="line">        self.weights = nn.Parameter(</span><br><span class="line">            torch.Tensor(<span class="number">3</span> * state_size, input_features + state_size))</span><br><span class="line">        self.bias = nn.Parameter(torch.Tensor(<span class="number">1</span>, <span class="number">3</span> * state_size))</span><br><span class="line">        self.reset_parameters()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reset_parameters</span>(<span class="params">self</span>):</span><br><span class="line">        stdv = <span class="number">1.0</span> / math.sqrt(self.state_size)</span><br><span class="line">        <span class="keyword">for</span> weight <span class="keyword">in</span> self.parameters():</span><br><span class="line">            weight.data.uniform_(-stdv, +stdv)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span>, state</span>):</span><br><span class="line">        <span class="keyword">return</span> LLTMPythonFunction.apply(<span class="built_in">input</span>, self.weights, self.bias, *state)</span><br></pre></td></tr></tbody></table></figure><h3 id="时间效率比较"><a href="#时间效率比较" class="headerlink" title="时间效率比较"></a>时间效率比较</h3><p>实现完成以后可以简单进行一下测试两者<code>forward</code>和<code>backward</code>的时间，比较一下使用Pytorch的自动求导比较快，还是我们自己独立编写一个前向反向传播的函数比较快。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">cuda_device = torch.device(<span class="string">"cuda"</span>)  <span class="comment"># device object representing GPU</span></span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">16</span></span><br><span class="line">input_features = <span class="number">32</span></span><br><span class="line">state_size = <span class="number">128</span></span><br><span class="line"></span><br><span class="line">X = torch.randn(batch_size, input_features, device=cuda_device, dtype=torch.float32)</span><br><span class="line">h = torch.randn(batch_size, state_size, device=cuda_device, dtype=torch.float32)</span><br><span class="line">C = torch.randn(batch_size, state_size, device=cuda_device, dtype=torch.float32)</span><br><span class="line"></span><br><span class="line">rnn = LLTMPython(input_features, state_size).to(cuda_device)</span><br><span class="line"></span><br><span class="line">forward = <span class="number">0</span></span><br><span class="line">backward = <span class="number">0</span></span><br><span class="line">n = <span class="number">100</span></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">    start = time.time()</span><br><span class="line">    new_h, new_C = rnn(X, (h, C))</span><br><span class="line">    torch.cuda.synchronize()</span><br><span class="line">    forward += time.time() - start</span><br><span class="line"></span><br><span class="line">    start = time.time()</span><br><span class="line">    (new_h.<span class="built_in">sum</span>() + new_C.<span class="built_in">sum</span>()).backward()</span><br><span class="line">    torch.cuda.synchronize()</span><br><span class="line">    backward += time.time() - start</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'Forward: {:.10f} s | Backward {:.10f} s'</span>.<span class="built_in">format</span>(forward / n, backward  / n))</span><br><span class="line"></span><br><span class="line">rnn = LLTM(input_features, state_size).to(cuda_device)</span><br><span class="line"></span><br><span class="line">forward = <span class="number">0</span></span><br><span class="line">backward = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">    start = time.time()</span><br><span class="line">    new_h, new_C = rnn(X, (h, C))</span><br><span class="line">    torch.cuda.synchronize()</span><br><span class="line">    forward += time.time() - start</span><br><span class="line"></span><br><span class="line">    start = time.time()</span><br><span class="line">    (new_h.<span class="built_in">sum</span>() + new_C.<span class="built_in">sum</span>()).backward()</span><br><span class="line">    torch.cuda.synchronize()</span><br><span class="line">    backward += time.time() - start</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'Forward: {:.10f} s | Backward {:.10f} s'</span>.<span class="built_in">format</span>(forward / n, backward  / n))</span><br></pre></td></tr></tbody></table></figure><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Forward: 0.0012168598 s | Backward 0.0008872311 s</span><br><span class="line">Forward: 0.0002657304 s | Backward 0.0004498205 s</span><br></pre></td></tr></tbody></table></figure><p>从前向和反向的时间比较来看，我们的自实现方法在性能上表现较差，相较于PyTorch的默认自动求导机制。尽管前向传播速度的提升并不明显，这可能表明PyTorch的自动求导机制在某些方面确实具有强大的优势。我们的手动实现可能存在一些效率方面的不足，导致性能不如PyTorch默认方式高效。虽然重新编写前向传播和反向传播是提升训练速度的一种尝试，但我们需要进一步优化以确保其性能与PyTorch默认机制相媲美。在反向传播方面，可能需要考虑使用C++扩展和CUDA扩展来提高效率。</p><h2 id="C-扩展实现"><a href="#C-扩展实现" class="headerlink" title="C++扩展实现"></a>C++扩展实现</h2><p>接下来，我们将使用C++扩展来实现LLTM（Long Long-Term Memory）的组合版本。我们将从使用纯C++编写它开始，使用<a target="_blank" rel="noopener" href="https://github.com/zdevito/ATen">ATen</a>库，在代码中表现为<code>torch/extension.h</code>，该库为PyTorch的大部分后端提供支持。</p><blockquote><p><strong><torch extension.h=""></torch></strong>是一站式头文件，包含写入C++扩展所需的所有PyTorch操作，包括：</p><ul><li>ATen库是用于张量计算的主要API，</li><li>pybind11，是为C++代码创建Python绑定的方式</li><li>管理ATen和pybind11之间交互细节的头文件</li></ul><p>PyTorch的张量和变量接口是从ATen库自动生成的，因此几乎可以将Python实现1:1转换为C++。所有计算的主要数据类型将是torch::Tensor。</p></blockquote><h3 id="基础函数-1"><a href="#基础函数-1" class="headerlink" title="基础函数"></a>基础函数</h3><p>对于C++的部分来说，我们首先需要对里面的函数进行重写一遍，利用C++的方式对来进行重写，这样进行计算的时候，程序就会通过C++来计算而不是Python来计算，这样就提高了效率和速度，首先主要是计算基础函数的微分函数，分别是的<code>d_sigmoid</code>，<code>d_tanh</code>和<code>d_elu</code>，主要是通过C++来进行编写。PyTorch的张量和变量接口是从ATen库自动生成的，因此几乎可以将Python实现1:1转换为C++。所有计算的主要数据类型将是<code>torch::Tensor</code>。</p><figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;torch/extension.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">torch::Tensor <span class="title">d_sigmoid</span><span class="params">(torch::Tensor z)</span> </span>{</span><br><span class="line">  <span class="keyword">auto</span> s = torch::<span class="built_in">sigmoid</span>(z);</span><br><span class="line">  <span class="keyword">return</span> (<span class="number">1</span> - s) * s;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// tanh'(z) = 1 - tanh^2(z)</span></span><br><span class="line"><span class="function">torch::Tensor <span class="title">d_tanh</span><span class="params">(torch::Tensor z)</span> </span>{</span><br><span class="line">  <span class="keyword">return</span> <span class="number">1</span> - z.<span class="built_in">tanh</span>().<span class="built_in">pow</span>(<span class="number">2</span>);</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// elu'(z) = relu'(z) + { alpha * exp(z) if (alpha * (exp(z) - 1)) &lt; 0, else 0}</span></span><br><span class="line"><span class="function">torch::Tensor <span class="title">d_elu</span><span class="params">(torch::Tensor z, torch::Scalar alpha = <span class="number">1.0</span>)</span> </span>{</span><br><span class="line">  <span class="keyword">auto</span> e = z.<span class="built_in">exp</span>();</span><br><span class="line">  <span class="keyword">auto</span> mask = (alpha * (e - <span class="number">1</span>)) &lt; <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">return</span> (z &gt; <span class="number">0</span>).<span class="built_in">type_as</span>(z) + mask.<span class="built_in">type_as</span>(z) * (alpha * e);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h3 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h3><p>接下来比较重要的就是对应前向传播和反向传播的编写了，这里面其实依旧是将我们前面使用的Python代码来转化为C++的代码的编写，逻辑都是一样的，只不过语言是有些不同的，这里<code>forward</code>不需要传入<code>ctx</code>参数，所以一共有五个参数，实际上跟Python实现的一一对应，只是转化为C++的扩展而已。</p><p>以前向传播为例子，这里面有一些语法和用法是不一样的，比如Pytorch里面的<code>torch.cat</code>对应着C++里面的<code>torch::cat</code>，还有比较不一样的就是<code>F.linear</code>对应着<code>torch.addmm</code>，除此之外，大部分的方法我们都可以在前面加一个前缀<code>torch::</code>即可，这也算写C++扩展的一个规律。</p><blockquote><p><code>torch::addmm</code> 是 PyTorch 中的一个函数，用于执行矩阵的乘法和加法操作。具体来说，它执行以下操作：</p><figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">result = beta * mat + alpha * (mat1 @ mat2)</span><br></pre></td></tr></tbody></table></figure><p>其中，<code>mat</code> 是输出矩阵，<code>mat1</code> 和 <code>mat2</code> 是输入矩阵，<code>alpha</code> 和 <code>beta</code> 是标量系数。这个函数通常用于线性代数运算，特别是在神经网络的前向传播过程中经常会用到。</p></blockquote><figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 前向传播</span></span><br><span class="line"><span class="function">std::vector&lt;at::Tensor&gt; <span class="title">lltm_forward</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Tensor input,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Tensor weights,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Tensor bias,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Tensor old_h,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Tensor old_cell)</span> </span>{</span><br><span class="line">  <span class="keyword">auto</span> X = torch::<span class="built_in">cat</span>({old_h, input}, <span class="comment">/*dim=*/</span><span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">auto</span> gate_weights = torch::<span class="built_in">addmm</span>(bias, X, weights.<span class="built_in">transpose</span>(<span class="number">0</span>, <span class="number">1</span>));</span><br><span class="line">  <span class="keyword">auto</span> gates = gate_weights.<span class="built_in">chunk</span>(<span class="number">3</span>, <span class="comment">/*dim=*/</span><span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">auto</span> input_gate = torch::<span class="built_in">sigmoid</span>(gates[<span class="number">0</span>]);</span><br><span class="line">  <span class="keyword">auto</span> output_gate = torch::<span class="built_in">sigmoid</span>(gates[<span class="number">1</span>]);</span><br><span class="line">  <span class="keyword">auto</span> candidate_cell = torch::<span class="built_in">elu</span>(gates[<span class="number">2</span>], <span class="comment">/*alpha=*/</span><span class="number">1.0</span>);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">auto</span> new_cell = old_cell + candidate_cell * input_gate;</span><br><span class="line">  <span class="keyword">auto</span> new_h = torch::<span class="built_in">tanh</span>(new_cell) * output_gate;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> {new_h,</span><br><span class="line">          new_cell,</span><br><span class="line">          input_gate,</span><br><span class="line">          output_gate,</span><br><span class="line">          candidate_cell,</span><br><span class="line">          X,</span><br><span class="line">          gate_weights};</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><p>反向传播与前向传播是类似的，有一点不同的是，由于没有<code>ctx</code>存储信息，所以我们的传入参数还包括<code>ctx</code>里面的参数，都是<code>torch::Tensor</code>格式，根据Python编写的反向传播的代码，对应着C++实现即可，其实也是一模一样，一一对应的。</p><figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 反向传播</span></span><br><span class="line"><span class="function">std::vector&lt;torch::Tensor&gt; <span class="title">lltm_backward</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Tensor grad_h,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Tensor grad_cell,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Tensor new_cell,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Tensor input_gate,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Tensor output_gate,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Tensor candidate_cell,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Tensor X,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Tensor gate_weights,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Tensor weights)</span> </span>{</span><br><span class="line">  <span class="keyword">auto</span> d_output_gate = torch::<span class="built_in">tanh</span>(new_cell) * grad_h;</span><br><span class="line">  <span class="keyword">auto</span> d_tanh_new_cell = output_gate * grad_h;</span><br><span class="line">  <span class="keyword">auto</span> d_new_cell = <span class="built_in">d_tanh</span>(new_cell) * d_tanh_new_cell + grad_cell;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">auto</span> d_old_cell = d_new_cell;</span><br><span class="line">  <span class="keyword">auto</span> d_candidate_cell = input_gate * d_new_cell;</span><br><span class="line">  <span class="keyword">auto</span> d_input_gate = candidate_cell * d_new_cell;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">auto</span> gates = gate_weights.<span class="built_in">chunk</span>(<span class="number">3</span>, <span class="comment">/*dim=*/</span><span class="number">1</span>);</span><br><span class="line">  d_input_gate *= <span class="built_in">d_sigmoid</span>(gates[<span class="number">0</span>]);</span><br><span class="line">  d_output_gate *= <span class="built_in">d_sigmoid</span>(gates[<span class="number">1</span>]);</span><br><span class="line">  d_candidate_cell *= <span class="built_in">d_elu</span>(gates[<span class="number">2</span>]);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">auto</span> d_gates =</span><br><span class="line">      torch::<span class="built_in">cat</span>({d_input_gate, d_output_gate, d_candidate_cell}, <span class="comment">/*dim=*/</span><span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">auto</span> d_weights = d_gates.<span class="built_in">t</span>().<span class="built_in">mm</span>(X);</span><br><span class="line">  <span class="keyword">auto</span> d_bias = d_gates.<span class="built_in">sum</span>(<span class="comment">/*dim=*/</span><span class="number">0</span>, <span class="comment">/*keepdim=*/</span><span class="literal">true</span>);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">auto</span> d_X = d_gates.<span class="built_in">mm</span>(weights);</span><br><span class="line">  <span class="type">const</span> <span class="keyword">auto</span> state_size = grad_h.<span class="built_in">size</span>(<span class="number">1</span>);</span><br><span class="line">  <span class="keyword">auto</span> d_old_h = d_X.<span class="built_in">slice</span>(<span class="comment">/*dim=*/</span><span class="number">1</span>, <span class="number">0</span>, state_size);</span><br><span class="line">  <span class="keyword">auto</span> d_input = d_X.<span class="built_in">slice</span>(<span class="comment">/*dim=*/</span><span class="number">1</span>, state_size);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> {d_old_h, d_input, d_weights, d_bias, d_old_cell};</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>完成上述代码的编写之后，我们就只需要<code>PYBIND11_MODULE</code>来进行绑定，再简单介绍一下他的用处，他的作用其实就是一个命名的绑定，也就是Python里面的forward为C++里面的<code>lltm_forward</code>，这样调用模块的时候，Python中的函数就会找到对应的C++函数来进行运行。</p><blockquote><p><code>PYBIND11_MODULE</code>这是 Python 调用 C++ 函数的关键部分。这个函数会在Python执行<code>import</code>语句时被调用，其接受两个参数，</p><p>第一个参数为模块名称，这里我们直接将<code>lltm_cpp</code>填入，稍候可以在Python中使用<code>import lltm_cpp</code>导入该模块；第二个参数<code>m</code>是创建Python关联代码的主接口，其类型为<code>py::module_</code>。<code>module_::def()</code>用于生成能够将<code>lltm_cpp</code>函数暴露给Python的代码，其第一个参数为<strong>字符串</strong>，将会成为Python中调用的函数名；</p><p>第二个参数是<strong>C++函数</strong>的引用；</p><p>第三个参数是<strong>说明字符串</strong>，在Python中可以使用<code>help(lltm_cpp)</code>查看。比如下面的例子中，C++ 中的函数 <code>lltm_forward</code> 对应 Python 中的 <code>forward</code>。</p></blockquote><figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">PYBIND11_MODULE</span>(TORCH_EXTENSION_NAME, m) {</span><br><span class="line">  m.<span class="built_in">def</span>(<span class="string">"forward"</span>, &amp;lltm_forward, <span class="string">"LLTM forward"</span>);</span><br><span class="line">  m.<span class="built_in">def</span>(<span class="string">"backward"</span>, &amp;lltm_backward, <span class="string">"LLTM backward"</span>);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h3 id="C-扩展调用"><a href="#C-扩展调用" class="headerlink" title="C++扩展调用"></a>C++扩展调用</h3><p>C++扩展一般有两种方式</p><ul><li>通过<code>setuptools</code>“提前”构建</li><li>通过<code>torch.utils.cpp_extension.load()</code>“实时”构建</li></ul><p>对于这两种方法，如果你的代码只运行一次，可以利用<code>jit</code>实时构建，这样不用去<code>python setup.py</code>来安装，但是如果你会多次复用这个C++扩展，那么还是需要去用第一种方法，这样后续运行的时候不需要一直构建，这样等待的时间就回比较长。</p><h4 id="Building-with-setuptools"><a href="#Building-with-setuptools" class="headerlink" title="Building with setuptools"></a>Building with <code>setuptools</code></h4><p>接下来先试用<code>setuptools</code>进行构建，编写一个 <code>setup.py</code> 文件，主要用于定义和说明一些重要的信息。其中关键的参数包括：</p><ul><li><code>name</code>：Python 调用的包的名称。</li><li><code>ext_modules</code> 的 <code>sources</code>：需要编译的 C++ 源文件，如果有多个 C++ 文件，需要列举所有。</li><li><code>cmdclass</code>：用BuildExtension执行许多必需的配置步骤和检查，并在混合C++/CUDA扩展的情况下处理混合编译。</li></ul><figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from setuptools <span class="keyword">import</span> setup, Extension</span><br><span class="line">from torch.<span class="function">utils <span class="keyword">import</span> cpp_extension</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"><span class="title">setup</span><span class="params">(name=<span class="string">'lltm_cpp'</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">      ext_modules=[cpp_extension.CppExtension(<span class="string">'lltm_cpp'</span>, [<span class="string">'lltm.cpp'</span>])],</span></span></span><br><span class="line"><span class="params"><span class="function">      cmdclass={<span class="string">'build_ext'</span>: cpp_extension.BuildExtension})</span></span></span><br></pre></td></tr></tbody></table></figure><p>完成这一步后，如果使用<code>setuptools</code>进行构建，我们可以使用 <code>pip</code> 进行安装。如果在当前文件夹下，直接运行 <code>pip install .</code> 即可完成安装或者我们也可以使用<code>python set.py install</code>，安装成功后应该会显示以下结果：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Installed /path/python3.10/site-packages/lltm_cpp-0.0.0-py3.10-linux-x86_64.egg</span><br><span class="line">Processing dependencies <span class="keyword">for</span> lltm-cpp==0.0.0</span><br><span class="line">Finished processing dependencies <span class="keyword">for</span> lltm-cpp==0.0.0</span><br></pre></td></tr></tbody></table></figure><p>这样后续我们就可以在对应的环境来进行<code>import lltm_cpp</code>来使用对应C++函数了。</p><h4 id="JIT-Compiling-Extensions"><a href="#JIT-Compiling-Extensions" class="headerlink" title="JIT Compiling Extensions"></a>JIT Compiling Extensions</h4><p>除了上述的<code>setuptools</code>的方法，接下来介绍即时编译（JIT）机制构建C++扩展。JIT编译机制通过调用PyTorch API中的一个简单函数<code>torch.utils.cpp_extension.load()</code>，为你提供了一种即时编译和加载扩展的方式。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.cpp_extension <span class="keyword">import</span> load</span><br><span class="line"></span><br><span class="line">cppcuda_tutorial = load(name=<span class="string">"cppcuda_tutorial"</span>,</span><br><span class="line">                        <span class="comment"># extra_include_paths=include_dirs,</span></span><br><span class="line">                        sources=[<span class="string">'interpolation.cpp'</span>],)</span><br></pre></td></tr></tbody></table></figure><p>在这里，实际提供的是域setuptools相同的信息。在后台，这将执行以下操作：</p><ol><li>创建一个临时目录<code>/tmp/torch_extensions/cppcuda_tutorial</code>，</li><li>向该临时目录发出Ninja构建文件，</li><li>将你的源文件编译成一个共享库，</li><li>将这个共享库导入为Python模块。</li></ol><p>实际上，如果将<code>verbose=True</code>传递给<code>cpp_extension.load()</code>，你将得到有关该过程的信息：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Using /path/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...</span><br><span class="line">Creating extension directory /path/.cache/torch_extensions/py310_cu113/lltm_cpp...</span><br><span class="line">Emitting ninja build file /path/.cache/torch_extensions/py310_cu113/lltm_cpp/build.ninja...</span><br><span class="line">Building extension module lltm_cpp...</span><br><span class="line">Allowing ninja to <span class="built_in">set</span> a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)</span><br><span class="line">[1/2] c++ -MMD -MF lltm.o.d -DTORCH_EXTENSION_NAME=lltm_cpp -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -isystem /path//path/python3.10/site-packages/torch/include -isystem /path//path/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /path//path/python3.10/site-packages/torch/include/TH -isystem /path//path/python3.10/site-packages/torch/include/THC -isystem /path/anaconda3/envs/ernerf/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -c /path/workdirs/pytorch-cppcuda-tutorial/lltm/lltm.cpp -o lltm.o </span><br><span class="line">[2/2] c++ lltm.o -shared -L/path//path/python3.10/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o lltm_cpp.so</span><br><span class="line">Loading extension module lltm_cpp...</span><br></pre></td></tr></tbody></table></figure><h3 id="加速前向反向传播"><a href="#加速前向反向传播" class="headerlink" title="加速前向反向传播"></a>加速前向反向传播</h3><p>实现了加速的扩展之后，我们就可以改写我们对应的<code>LLTMFuntcion</code>利用C++扩展来进行加速，方法很简单，就是将函数传入到<code>LLTMFuntcion</code>传入即可</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LLTMFunction</span>(torch.autograd.Function):</span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">ctx, <span class="built_in">input</span>, weights, bias, old_h, old_cell</span>):</span><br><span class="line">        outputs = lltm_cpp.forward(<span class="built_in">input</span>, weights, bias, old_h, old_cell)</span><br><span class="line">        new_h, new_cell = outputs[:<span class="number">2</span>]</span><br><span class="line">        variables = outputs[<span class="number">1</span>:] + [weights]</span><br><span class="line">        ctx.save_for_backward(*variables)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> new_h, new_cell</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">ctx, grad_h, grad_cell</span>):</span><br><span class="line">        outputs = lltm_cpp.backward(</span><br><span class="line">            grad_h.contiguous(), grad_cell.contiguous(), *ctx.saved_tensors)</span><br><span class="line">        d_old_h, d_input, d_weights, d_bias, d_old_cell = outputs</span><br><span class="line">        <span class="keyword">return</span> d_input, d_weights, d_bias, d_old_h, d_old_cell</span><br></pre></td></tr></tbody></table></figure><h3 id="时间效率比较-1"><a href="#时间效率比较-1" class="headerlink" title="时间效率比较"></a>时间效率比较</h3><p>与Python扩展实现类似，我们进行了与C++扩展实现类似的时间效率比较，结果显示前向传播在<code>PyTorch</code>自动求导机制下相对较慢，与我们手动实现相比可能有四五倍的性能差距。然而，值得注意的是，尽管前向传播的速度较慢，但所获得的结果却更为优越，相较于默认机制大约提高了2倍左右。</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Forward: 0.0011254544 s | Backward 0.0005326970 s</span><br><span class="line">Forward: 0.0002915986 s | Backward 0.0008708093 s</span><br></pre></td></tr></tbody></table></figure><h2 id="CUDA扩展实现"><a href="#CUDA扩展实现" class="headerlink" title="CUDA扩展实现"></a>CUDA扩展实现</h2><p>接下来就到了重头戏，也就是CUDA扩展的实现，首先我们先看看，怎么使用CUDA去进行编程，首先CUDA的代码是<code>cu</code>结尾的，我们通过编写CUDA来进行一个计算加速。我们还是先按之前的方法，看看如何使用CUDA进行编程先，这里面有几个注意的点：</p><ul><li>需要编写CUDA函数</li><li>需要在头文件<code>.h</code>中去定义需要使用的函数，包括一些常用的测试函数。也可以写到函数中</li><li>修改CUDA的<code>setup.py</code></li></ul><p>接下来一步一步来，首先我们写对应调用CUDA的C++文件，这里面做的主要是声明和一些定义<code>lltm_forward</code>,<code>lltm_backward</code>,<code>lltm_cuda_forward</code>,<code>lltm_cuda_forward</code>函数，具体的函数操作会在CUDA里面进行实现，这样编写以后，我们的cpp的代码就可以调用CUDA对函数来进行调用，但是由于我们使用CUDA的函数，所以这里面我们还要用到<code>CHECK_INPUT</code>函数来判断是否在GPU上，也就是一个检测，并且内存是否连续，因为后续要进行一个并行的计算。</p><figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;torch/extension.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// C++ interface</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK_CUDA(x) TORCH_CHECK(x.device().is_cuda(), #x <span class="string">" must be a CUDA tensor"</span>)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x <span class="string">" must be contiguous"</span>)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// CUDA forward declarations</span></span><br><span class="line"></span><br><span class="line"><span class="function">std::vector&lt;torch::Tensor&gt; <span class="title">lltm_cuda_forward</span><span class="params">( </span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor input,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor weights,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor bias,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor old_h,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor old_cell)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function">std::vector&lt;torch::Tensor&gt; <span class="title">lltm_cuda_backward</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor grad_h,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor grad_cell,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor new_cell,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor input_gate,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor output_gate,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor candidate_cell,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor X,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor gate_weights,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor weights)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function">std::vector&lt;torch::Tensor&gt; <span class="title">lltm_forward</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor input,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor weights,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor bias,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor old_h,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor old_cell)</span> </span>{</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(input);</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(weights);</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(bias);</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(old_h);</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(old_cell);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">lltm_cuda_forward</span>(input, weights, bias, old_h, old_cell);</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function">std::vector&lt;torch::Tensor&gt; <span class="title">lltm_backward</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor grad_h,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor grad_cell,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor new_cell,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor input_gate,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor output_gate,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor candidate_cell,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor X,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor gate_weights,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor weights)</span> </span>{</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(grad_h);</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(grad_cell);</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(input_gate);</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(output_gate);</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(candidate_cell);</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(X);</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(gate_weights);</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(weights);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">lltm_cuda_backward</span>(</span><br><span class="line">            grad_h,</span><br><span class="line">            grad_cell,</span><br><span class="line">            new_cell,</span><br><span class="line">            input_gate,</span><br><span class="line">            output_gate,</span><br><span class="line">            candidate_cell,</span><br><span class="line">            X,</span><br><span class="line">            gate_weights,</span><br><span class="line">            weights);</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="built_in">PYBIND11_MODULE</span>(TORCH_EXTENSION_NAME, m) {</span><br><span class="line">    m.<span class="built_in">def</span>(<span class="string">"forward"</span>, &amp;lltm_forward, <span class="string">"LLTM forward (CUDA)"</span>);</span><br><span class="line">    m.<span class="built_in">def</span>(<span class="string">"backward"</span>, &amp;lltm_backward, <span class="string">"LLTM backward (CUDA)"</span>);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h3 id="基础函数-2"><a href="#基础函数-2" class="headerlink" title="基础函数"></a>基础函数</h3><p>接下来就是重中之重个，也就是我们需要写一个<code>lltm_cuda_kernel.cu</code>函数，也就是一个LLTM对应的CUDA函数，后面我们可以用C++调用CUDA，最基础的还是对应的基础函数CUDA函数编写，和前面类似，这里面是将前面使用的C++，Python转化为CUDA编程的格式，实际上也是对应的C++代码。</p><p>对于CUDA函数来说，首先比较的不同的就是，我们都需要使用到模板参数<code>template&lt;typename scalar_t&gt;</code>，这样就代表着<code>scalar_t</code>能够代表任何的类型，方便后续去调用。</p><p><code>__device__ __forceinline__</code> 这两个标记告诉编译器将函数内联到调用处，以提高执行效率。<code>__device__</code> 表示这个函数在设备（GPU）上执行，而 <code>__forceinline__</code> 提示编译器尽可能地进行内联优化。这都是CUDA来提高效率的一些方式，实际上的计算方式就和C++的实现是一样的。</p><figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;torch/extension.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> <span class="type">scalar_t</span>&gt;</span></span><br><span class="line"><span class="function">__device__ __forceinline__ <span class="type">scalar_t</span> <span class="title">sigmoid</span><span class="params">(<span class="type">scalar_t</span> z)</span> </span>{</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span> / (<span class="number">1.0</span> + <span class="built_in">exp</span>(-z));</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> <span class="type">scalar_t</span>&gt;</span></span><br><span class="line"><span class="function">__device__ __forceinline__ <span class="type">scalar_t</span> <span class="title">d_sigmoid</span><span class="params">(<span class="type">scalar_t</span> z)</span> </span>{</span><br><span class="line">    <span class="type">const</span> <span class="keyword">auto</span> s = <span class="built_in">sigmoid</span>(z);</span><br><span class="line">    <span class="keyword">return</span> (<span class="number">1.0</span> - s) * s;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> <span class="type">scalar_t</span>&gt;</span></span><br><span class="line"><span class="function">__device__ __forceinline__ <span class="type">scalar_t</span> <span class="title">d_tanh</span><span class="params">(<span class="type">scalar_t</span> z)</span> </span>{</span><br><span class="line">    <span class="type">const</span> <span class="keyword">auto</span> t = <span class="built_in">tanh</span>(z);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> - (t * t);</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> <span class="type">scalar_t</span>&gt;</span></span><br><span class="line"><span class="function">__device__ __forceinline__ <span class="type">scalar_t</span> <span class="title">elu</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">scalar_t</span> z,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">scalar_t</span> alpha = <span class="number">1.0</span></span></span></span><br><span class="line"><span class="params"><span class="function">)</span> </span>{</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">fmax</span>(<span class="number">0.0</span>,</span><br><span class="line">                z) + <span class="built_in">fmin</span>(<span class="number">0.0</span>,</span><br><span class="line">                          alpha * (<span class="built_in">exp</span>(z) - <span class="number">1.0</span>));</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> <span class="type">scalar_t</span>&gt;</span></span><br><span class="line"><span class="function">__device__ __forceinline__ <span class="type">scalar_t</span> <span class="title">d_elu</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">scalar_t</span> z,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">scalar_t</span> alpha = <span class="number">1.0</span></span></span></span><br><span class="line"><span class="params"><span class="function">)</span> </span>{</span><br><span class="line">    <span class="type">const</span> <span class="keyword">auto</span> e = <span class="built_in">exp</span>(z);</span><br><span class="line">    <span class="type">const</span> <span class="keyword">auto</span> d_relu = z &lt; <span class="number">0.0</span> ? <span class="number">0.0</span> : <span class="number">1.0</span>;</span><br><span class="line">    <span class="keyword">return</span> d_relu + (((alpha * (e - <span class="number">1.0</span>)) &lt; <span class="number">0.0</span>) ? (alpha * e) : <span class="number">0.0</span>);</span><br><span class="line">}</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><h3 id="CUDA加速原理"><a href="#CUDA加速原理" class="headerlink" title="CUDA加速原理"></a>CUDA加速原理</h3><p>首先介绍一个CUDA程序实现的流程</p><ol><li>把数据从CPU内存拷贝到GPU内存</li><li>调用核函数对存储在GPU内存中的数据进行操作</li><li>将数据从GPU内存传送回CPU内存</li></ol><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic2.zhimg.com/v2-2959e07a36a8dc8f59280f53b43eb9d1_b.jpg" alt="CUDA编程入门极简教程- 知乎"></p><p>如下图所示，在利用CUDA加速的时候，图的左边是CPU，右边是GPU，我们需要把数据从CPU传到GPU中。在GPU中，就会生成对应的Grid来进行计算，每个Grid里面又有很多的block，从block中看又有很多的线程thread进行运算。我们也可以想象一下，如果计算一个矩阵的加法，我们可以让每个thread做对应的元素的相加，这样就可以大大加快计算速度，达到并行的效果。所以这之中内核（kernel）是CUDA编程中一个重要的部分，其代码在GPU上运行，比如矩阵乘法，我们就可以写一个加法的核函数，然后串行执行核函数，这样我们就快速能完成CUDA代码的编写，而不用在创建和管理大量的GPU线程时拘泥于细节。</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://nyu-cds.github.io/python-gpu/fig/02-threadmapping.png" alt="Thread Mapping"></p><p>所以我们可以发现，实际上CUDA的计算是<code>Grid</code>——》<code>Block</code>——》<code>Thread</code>，然后用多个<code>Thread</code>进行计算，这里面可能会疑惑，为什么不是直接<code>Grid</code>——》<code>Thread</code>，实际上是因为硬件的限制是<code>Block</code>上限是$(2^{31}-1)<em>2^{16}</em>2^{16}$，<code>Thread</code>的上限是1024，所以这样的组合设计能够利用好更多的<code>Thread</code>，这也是为什么GPU速度那么快的原因。</p><h3 id="前向传播核函数"><a href="#前向传播核函数" class="headerlink" title="前向传播核函数"></a>前向传播核函数</h3><p>对于CUDA编程来说，往往首先我们需要编写一个<code>kernel</code>模板函数，这个模板函数实际上就是说明，在一个核中的对应的计算方式和流程。这样GPU实现的就是在多个<code>thread</code>做这样的事情，这样就实现了并行运算，极大的提高了效率，这也是是为什么CUDA加速比较快的原因。</p><p>接下里分析函数的主体部分，主要做两件事情：</p><ol><li>为每个<code>threads</code>进行编号</li><li>去除不必要的<code>threads</code></li></ol><p>在使用<code>threads</code>计算的时候，实际上每一个<code>threads</code>都有一个对应的编号，计算方式如第12,13行所示，实际上就是block的x*block的个数+block的y就能得到最后的结果，后续就是对应着每一个<code>threads</code>的计算，对应<code>threads</code>的编号<code>index</code>。</p><figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> <span class="type">scalar_t</span>&gt;</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">lltm_cuda_forward_kernel</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">const</span> <span class="type">scalar_t</span> *__restrict__ gates,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">const</span> <span class="type">scalar_t</span> *__restrict__ old_cell,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">scalar_t</span> *__restrict__ new_h,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">scalar_t</span> *__restrict__ new_cell,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">scalar_t</span> *__restrict__ input_gate,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">scalar_t</span> *__restrict__ output_gate,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">scalar_t</span> *__restrict__ candidate_cell,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">size_t</span> state_size</span></span></span><br><span class="line"><span class="params"><span class="function">)</span> </span>{</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> column = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> index = blockIdx.y * state_size + column;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> gates_row = blockIdx.y * (state_size * <span class="number">3</span>);</span><br><span class="line">    <span class="keyword">if</span> (column &lt; state_size) {</span><br><span class="line">        input_gate[index] = <span class="built_in">sigmoid</span>(gates[gates_row + column]);</span><br><span class="line">        output_gate[index] = <span class="built_in">sigmoid</span>(gates[gates_row + state_size +</span><br><span class="line">                                           column]);</span><br><span class="line">        candidate_cell[index] = <span class="built_in">elu</span>(gates[gates_row + <span class="number">2</span> * state_size +</span><br><span class="line">                                          column]);</span><br><span class="line">        new_cell[index] =</span><br><span class="line">                old_cell[index] +</span><br><span class="line">                candidate_cell[index] * input_gate[index];</span><br><span class="line">        new_h[index] = <span class="built_in">tanh</span>(new_cell[index]) * output_gate[index];</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h3 id="前向传播-1"><a href="#前向传播-1" class="headerlink" title="前向传播"></a>前向传播</h3><p>接下来我们就使用实现的<code>lltm_cuda_forward_kernel</code>来实现<code>lltm_cuda_forward</code>，前面和C++扩展的方式是一样的，以及初始化空的变量以便后续进行前向传播，对于<code>const int threads = 256; const dim3 blocks((state_size + threads - 1) / threads, batch_size);</code>这两行，实际上就是在定义上述提过的<code>threads</code>和<code>blocks</code>了，<code>dim3</code>是NVIDIA的CUDA编程中一种自定义的整型向量类型，基于用于指定维度的<code>uint3</code>，<code>dim3</code>类型最终设置的是一个三维向量，三维参数分别为x,y,z。在并行中，通常只支持三个并行，比如这里的N和F刚刚好就是两个并行，这里设置为16x16的线程，一般可以是128,256,512，不一定使用越多越好，这里面只是给了一个例子。</p><p><code>const dim3 blocks((state_size + threads - 1) / threads, batch_size);</code>部分还定义了<code>blocks</code>的个数计算，也就是使用多大的<code>blocks</code>去覆盖我们计算矩阵，这里面的<code>state_size</code>是每一个计算的状态大小，<code>batch_size</code>是每次的数目，这样我们就可以计算出对应的<code>blocks</code>大小来进行计算。（详细解读可以看看上一篇我的博客，里面可以画图帮助理解）<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/671704557">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a></p><p>最后还有一个就是<code>AT_DISPATCH_FLOATING_TYPES</code>，这里面就是一个启动核函数的部分，这里也可以认为是一个框架函数，<code>AT_DISPATCH_FLOATING_TYPES</code> 是处理核函数的启动（使用 <code>&lt;&lt;&lt;...&gt;&gt;&gt;</code> 表示），它一般有三个参数</p><ul><li>一个类型<code>gates.type()</code></li><li>一个名称 <code>"lltm_forward_cuda"</code>，用于错误消息</li><li>一个 lambda 函数，是一个模版函数<code>template</code>，类型别名为 <code>scalar_t</code>，这里面就是核函数</li></ul><figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">std::vector&lt;torch::Tensor&gt; <span class="title">lltm_cuda_forward</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor input,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor weights,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor bias,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor old_h,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor old_cell</span></span></span><br><span class="line"><span class="params"><span class="function">)</span> </span>{</span><br><span class="line">    <span class="keyword">auto</span> X = torch::<span class="built_in">cat</span>({old_h, input}, <span class="comment">/*dim=*/</span></span><br><span class="line">                        <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">auto</span> gate_weights = torch::<span class="built_in">addmm</span>(bias,</span><br><span class="line">                                     X,</span><br><span class="line">                                     weights.<span class="built_in">transpose</span>(<span class="number">0</span>,</span><br><span class="line">                                                       <span class="number">1</span>));</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="keyword">auto</span> batch_size = old_cell.<span class="built_in">size</span>(<span class="number">0</span>);</span><br><span class="line">    <span class="type">const</span> <span class="keyword">auto</span> state_size = old_cell.<span class="built_in">size</span>(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> gates = gate_weights.<span class="built_in">reshape</span>({batch_size, <span class="number">3</span>, state_size});</span><br><span class="line">    <span class="keyword">auto</span> new_h = torch::<span class="built_in">zeros_like</span>(old_cell);</span><br><span class="line">    <span class="keyword">auto</span> new_cell = torch::<span class="built_in">zeros_like</span>(old_cell);</span><br><span class="line">    <span class="keyword">auto</span> input_gate = torch::<span class="built_in">zeros_like</span>(old_cell);</span><br><span class="line">    <span class="keyword">auto</span> output_gate = torch::<span class="built_in">zeros_like</span>(old_cell);</span><br><span class="line">    <span class="keyword">auto</span> candidate_cell = torch::<span class="built_in">zeros_like</span>(old_cell);</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> threads = <span class="number">256</span>;</span><br><span class="line">    <span class="function"><span class="type">const</span> dim3 <span class="title">blocks</span><span class="params">((state_size + threads - <span class="number">1</span>) / threads,</span></span></span><br><span class="line"><span class="params"><span class="function">                      batch_size)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">AT_DISPATCH_FLOATING_TYPES</span>(gates.<span class="built_in">type</span>(),</span><br><span class="line">                               <span class="string">"lltm_forward_cuda"</span>,</span><br><span class="line">                               ([&amp;] {</span><br><span class="line">                                   lltm_cuda_forward_kernel&lt;<span class="type">scalar_t</span>&gt;&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(</span><br><span class="line">                                           gates.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">                                           old_cell.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">                                           new_h.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">                                           new_cell.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">                                           input_gate.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">                                           output_gate.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">                                           candidate_cell.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">                                           state_size);</span><br><span class="line">                               }));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> {new_h, new_cell, input_gate, output_gate, candidate_cell, X,</span><br><span class="line">            gates};</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h3 id="反向传播-1"><a href="#反向传播-1" class="headerlink" title="反向传播"></a>反向传播</h3><p>与前向传播类似，反向传播也是类似的方法进行了实现，这里具体就不讲述，几乎和前向传播的步骤是一样的。</p><figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> <span class="type">scalar_t</span>&gt;</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">lltm_cuda_backward_kernel</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">scalar_t</span> *d_old_cell,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">scalar_t</span> *d_gates,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">const</span> <span class="type">scalar_t</span> *grad_h,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">const</span> <span class="type">scalar_t</span> *grad_cell,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">const</span> <span class="type">scalar_t</span> *new_cell,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">const</span> <span class="type">scalar_t</span> *input_gate,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">const</span> <span class="type">scalar_t</span> *output_gate,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">const</span> <span class="type">scalar_t</span> *candidate_cell,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">const</span> <span class="type">scalar_t</span> *gate_weights,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">int</span> state_size</span></span></span><br><span class="line"><span class="params"><span class="function">)</span> </span>{</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> column = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> index = blockIdx.y * state_size + column;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> gates_row = blockIdx.y * (state_size * <span class="number">3</span>);</span><br><span class="line">    <span class="keyword">if</span> (column &lt; state_size) {</span><br><span class="line">        <span class="type">const</span> <span class="keyword">auto</span> d_output_gate = <span class="built_in">tanh</span>(new_cell[index]) *</span><br><span class="line">                                   grad_h[index];</span><br><span class="line">        <span class="type">const</span> <span class="keyword">auto</span> d_tanh_new_cell = output_gate[index] * grad_h[index];</span><br><span class="line">        <span class="type">const</span> <span class="keyword">auto</span> d_new_cell =</span><br><span class="line">                <span class="built_in">d_tanh</span>(new_cell[index]) * d_tanh_new_cell +</span><br><span class="line">                grad_cell[index];</span><br><span class="line">        d_old_cell[index] = d_new_cell;</span><br><span class="line">        <span class="type">const</span> <span class="keyword">auto</span> d_candidate_cell = input_gate[index] * d_new_cell;</span><br><span class="line">        <span class="type">const</span> <span class="keyword">auto</span> d_input_gate = candidate_cell[index] * d_new_cell;</span><br><span class="line">        d_gates[gates_row + column] =</span><br><span class="line">                d_input_gate * <span class="built_in">d_sigmoid</span>(gate_weights[gates_row +</span><br><span class="line">                                                      column]);</span><br><span class="line">        d_gates[gates_row + state_size +</span><br><span class="line">                column] =</span><br><span class="line">                d_output_gate *</span><br><span class="line">                <span class="built_in">d_sigmoid</span>(gate_weights[gates_row + state_size +</span><br><span class="line">                                       column]);</span><br><span class="line">        d_gates[gates_row + <span class="number">2</span> * state_size +</span><br><span class="line">                column] =</span><br><span class="line">                d_candidate_cell *</span><br><span class="line">                <span class="built_in">d_elu</span>(gate_weights[gates_row + <span class="number">2</span> * state_size +</span><br><span class="line">                                   column]);</span><br><span class="line">    }</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function">std::vector&lt;torch::Tensor&gt; <span class="title">lltm_cuda_backward</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor grad_h,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor grad_cell,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor new_cell,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor input_gate,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor output_gate,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor candidate_cell,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor X,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor gates,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::Tensor weights</span></span></span><br><span class="line"><span class="params"><span class="function">)</span> </span>{</span><br><span class="line">    <span class="keyword">auto</span> d_old_cell = torch::<span class="built_in">zeros_like</span>(new_cell);</span><br><span class="line">    <span class="keyword">auto</span> d_gates = torch::<span class="built_in">zeros_like</span>(gates);</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="keyword">auto</span> batch_size = new_cell.<span class="built_in">size</span>(<span class="number">0</span>);</span><br><span class="line">    <span class="type">const</span> <span class="keyword">auto</span> state_size = new_cell.<span class="built_in">size</span>(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> threads = <span class="number">256</span>;</span><br><span class="line">    <span class="function"><span class="type">const</span> dim3 <span class="title">blocks</span><span class="params">((state_size + threads - <span class="number">1</span>) / threads,</span></span></span><br><span class="line"><span class="params"><span class="function">                      batch_size)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">AT_DISPATCH_FLOATING_TYPES</span>(X.<span class="built_in">type</span>(),</span><br><span class="line">                               <span class="string">"lltm_backward_cuda"</span>,</span><br><span class="line">                               ([&amp;] {</span><br><span class="line">                                   lltm_cuda_backward_kernel&lt;<span class="type">scalar_t</span>&gt;&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(</span><br><span class="line">                                           d_old_cell.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">                                           d_gates.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">                                           grad_h.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">                                           grad_cell.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">                                           new_cell.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">                                           input_gate.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">                                           output_gate.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">                                           candidate_cell.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">                                           gates.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">                                           state_size</span><br><span class="line">                                           );</span><br><span class="line"></span><br><span class="line">                               }));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> d_gate_weights = d_gates.<span class="built_in">reshape</span>({batch_size, <span class="number">3</span> * state_size});</span><br><span class="line">    <span class="keyword">auto</span> d_weights = d_gate_weights.<span class="built_in">t</span>().<span class="built_in">mm</span>(X);</span><br><span class="line">    <span class="keyword">auto</span> d_bias = d_gate_weights.<span class="built_in">sum</span>(<span class="comment">/*dim=*/</span><span class="number">0</span>, <span class="comment">/*keepdim=*/</span></span><br><span class="line">                                             <span class="literal">true</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> d_X = d_gate_weights.<span class="built_in">mm</span>(weights);</span><br><span class="line">    <span class="keyword">auto</span> d_old_h = d_X.<span class="built_in">slice</span>(<span class="comment">/*dim=*/</span><span class="number">1</span>,</span><br><span class="line">                                     <span class="number">0</span>,</span><br><span class="line">                                     state_size);</span><br><span class="line">    <span class="keyword">auto</span> d_input = d_X.<span class="built_in">slice</span>(<span class="comment">/*dim=*/</span><span class="number">1</span>,</span><br><span class="line">                                     state_size);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> {d_old_h, d_input, d_weights, d_bias, d_old_cell};</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h3 id="时间效率对比"><a href="#时间效率对比" class="headerlink" title="时间效率对比"></a>时间效率对比</h3><p>接下来，我们就可以调用扩展，方法是一样的，所以这里不多说，我使用的Jit的方式进行实时构建</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.cpp_extension <span class="keyword">import</span> load</span><br><span class="line">lltm_cuda = load(name=<span class="string">'lltm_cuda'</span>, </span><br><span class="line">                 sources=[<span class="string">'lltm/lltm_cuda.cpp'</span>, <span class="string">'lltm/lltm_cuda_kernel.cu'</span>], verbose=<span class="literal">True</span></span><br><span class="line">                 )</span><br></pre></td></tr></tbody></table></figure><p>同样也是加速对应的<code>LLTMFunction</code>，调用对应CUDA写的<code>forward</code>和<code>backward</code>即可。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LLTMFunction</span>(torch.autograd.Function):</span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">ctx, <span class="built_in">input</span>, weights, bias, old_h, old_cell</span>):</span><br><span class="line">        outputs = lltm_cuda.forward(<span class="built_in">input</span>, weights, bias, old_h, old_cell)</span><br><span class="line">        new_h, new_cell = outputs[:<span class="number">2</span>]</span><br><span class="line">        variables = outputs[<span class="number">1</span>:] + [weights]</span><br><span class="line">        ctx.save_for_backward(*variables)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> new_h, new_cell</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">ctx, grad_h, grad_cell</span>):</span><br><span class="line">        outputs = lltm_cuda.backward(</span><br><span class="line">            grad_h.contiguous(), grad_cell.contiguous(), *ctx.saved_tensors)</span><br><span class="line">        d_old_h, d_input, d_weights, d_bias, d_old_cell = outputs</span><br><span class="line">        <span class="keyword">return</span> d_input, d_weights, d_bias, d_old_h, d_old_cell</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://github.com/pytorch/extension-cpp">https://github.com/pytorch/extension-cpp</a></p><p><a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/advanced/cpp_extension.html#">https://pytorch.org/tutorials/advanced/cpp_extension.html#</a></p><p><a target="_blank" rel="noopener" href="https://github.com/kevinghst/lltm_experiment">https://github.com/kevinghst/lltm_experiment</a></p></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io/2023/12/12/CUDA/CUDA%E7%BC%96%E7%A8%8B%E5%AE%9E%E8%B7%B5%EF%BC%9ALLTM%E5%8A%A0%E9%80%9F%E4%BC%98%E5%8C%96%E5%AE%9E%E8%B7%B5/">https://kedreamix.github.io/2023/12/12/CUDA/CUDA编程实践：LLTM加速优化实践/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/CUDA/">CUDA</a></div><div class="post_share"><div class="social-share" data-image="https://pic1.zhimg.com/80/v2-5328b2df743543e0ca21d09eeaf45c28_1440w.jpeg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/12/13/Note/Vscode/" title="VS Code Server 离线安装（解决超时，XHR Failed等问题）"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/70/v2-fb3a7f46e7256be3802ea6bd887543d2_1440w.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">VS Code Server 离线安装（解决超时，XHR Failed等问题）</div></div></a></div><div class="next-post pull-right"><a href="/2023/12/12/CUDA/Pytorch+cppcuda%20extension%20%E5%AD%A6%E4%B9%A0/" title="CUDA编程学习：自定义Pytorch+cpp/cuda extension"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">CUDA编程学习：自定义Pytorch+cpp/cuda extension</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2023/12/12/CUDA/Pytorch+cppcuda%20extension%20%E5%AD%A6%E4%B9%A0/" title="CUDA编程学习：自定义Pytorch+cpp&#x2F;cuda extension"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-12-12</div><div class="title">CUDA编程学习：自定义Pytorch+cpp&#x2F;cuda extension</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Pytorch%E6%99%AE%E9%80%9A%E5%AE%9E%E7%8E%B0"><span class="toc-number">1.</span> <span class="toc-text">Pytorch普通实现</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Python%E6%89%A9%E5%B1%95%E5%AE%9E%E7%8E%B0"><span class="toc-number">2.</span> <span class="toc-text">Python扩展实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80%E5%87%BD%E6%95%B0"><span class="toc-number">2.1.</span> <span class="toc-text">基础函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%89%8D%E5%90%91%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">2.2.</span> <span class="toc-text">前向和反向传播</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%97%B6%E9%97%B4%E6%95%88%E7%8E%87%E6%AF%94%E8%BE%83"><span class="toc-number">2.3.</span> <span class="toc-text">时间效率比较</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#C-%E6%89%A9%E5%B1%95%E5%AE%9E%E7%8E%B0"><span class="toc-number">3.</span> <span class="toc-text">C++扩展实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80%E5%87%BD%E6%95%B0-1"><span class="toc-number">3.1.</span> <span class="toc-text">基础函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">3.2.</span> <span class="toc-text">前向传播</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">3.3.</span> <span class="toc-text">反向传播</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#C-%E6%89%A9%E5%B1%95%E8%B0%83%E7%94%A8"><span class="toc-number">3.4.</span> <span class="toc-text">C++扩展调用</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Building-with-setuptools"><span class="toc-number">3.4.1.</span> <span class="toc-text">Building with setuptools</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#JIT-Compiling-Extensions"><span class="toc-number">3.4.2.</span> <span class="toc-text">JIT Compiling Extensions</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A0%E9%80%9F%E5%89%8D%E5%90%91%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">3.5.</span> <span class="toc-text">加速前向反向传播</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%97%B6%E9%97%B4%E6%95%88%E7%8E%87%E6%AF%94%E8%BE%83-1"><span class="toc-number">3.6.</span> <span class="toc-text">时间效率比较</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CUDA%E6%89%A9%E5%B1%95%E5%AE%9E%E7%8E%B0"><span class="toc-number">4.</span> <span class="toc-text">CUDA扩展实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80%E5%87%BD%E6%95%B0-2"><span class="toc-number">4.1.</span> <span class="toc-text">基础函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CUDA%E5%8A%A0%E9%80%9F%E5%8E%9F%E7%90%86"><span class="toc-number">4.2.</span> <span class="toc-text">CUDA加速原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%A0%B8%E5%87%BD%E6%95%B0"><span class="toc-number">4.3.</span> <span class="toc-text">前向传播核函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD-1"><span class="toc-number">4.4.</span> <span class="toc-text">前向传播</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD-1"><span class="toc-number">4.5.</span> <span class="toc-text">反向传播</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%97%B6%E9%97%B4%E6%95%88%E7%8E%87%E5%AF%B9%E6%AF%94"><span class="toc-number">4.6.</span> <span class="toc-text">时间效率对比</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83"><span class="toc-number">5.</span> <span class="toc-text">参考</span></a></li></ol></div></div></div></div></main><footer id="footer" style="background-image:url('https://pic1.zhimg.com/80/v2-5328b2df743543e0ca21d09eeaf45c28_1440w.jpeg')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js").then((()=>{pangu.autoSpacingPage()}))}function panguInit(){panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"></div><script id="canvas_nest" defer color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><script data-pjax>function butterfly_swiper_injector_config(){var a=document.getElementById("recent-posts");console.log("已挂载butterfly_swiper"),a.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/25/Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="2024/01/25/Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="2024/01/25/Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/17/Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023/12/17/Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="2023/12/17/Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script></body></html>