<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>CUDA编程学习：自定义Pytorch+cpp/cuda extension | Adventures in Kedreamix' Digital World</title><meta name="author" content="Kedreamix"><meta name="copyright" content="Kedreamix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，"><meta property="og:type" content="article"><meta property="og:title" content="CUDA编程学习：自定义Pytorch+cpp&#x2F;cuda extension"><meta property="og:url" content="https://kedreamix.github.io/2023/12/12/CUDA/Pytorch+cppcuda%20extension%20%E5%AD%A6%E4%B9%A0/index.html"><meta property="og:site_name" content="Adventures in Kedreamix&#39; Digital World"><meta property="og:description" content="虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png"><meta property="article:published_time" content="2023-12-12T03:26:00.000Z"><meta property="article:modified_time" content="2024-01-30T05:04:13.227Z"><meta property="article:author" content="Kedreamix"><meta property="article:tag" content="CUDA"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png"><link rel="shortcut icon" href="/img/pikachu.png"><link rel="canonical" href="https://kedreamix.github.io/2023/12/12/CUDA/Pytorch+cppcuda%20extension%20%E5%AD%A6%E4%B9%A0/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b55fdb2ccecfe92347e7ef01fc095ff8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-74LZ5BEQQ1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-74LZ5BEQQ1")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!0,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"简"},noticeOutdate:{limitDay:500,position:"top",messagePrev:"It has been",messageNext:"days since the last update, the content of the article may be outdated."},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:200},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!0,post:!0},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!0,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"CUDA编程学习：自定义Pytorch+cpp/cuda extension",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-01-30 13:04:13"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach((e=>{n.setAttribute(e,t[e])})),document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)})),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=24?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 7.0.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="Adventures in Kedreamix' Digital World" type="application/atom+xml"></head><body><div id="loading-box"><div class="pokeball-back"></div><div class="pokeball-loading"><div class="pokeball" id="pokeball-normal"></div><div class="pokeball" id="pokeball-great"></div><div class="pokeball" id="pokeball-ultra"></div><div class="pokeball" id="pokeball-master"></div><div class="pokeball" id="pokeball-safari"></div></div></div><script>(()=>{const d=document.getElementById("loading-box"),e=document.body,o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l(),window.addEventListener("load",(()=>{o()}))})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">146</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url('https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png')"><nav id="nav"><span id="blog-info"><a href="/" title="Adventures in Kedreamix' Digital World"><img class="site-icon" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pikachu.png"><span class="site-name">Adventures in Kedreamix' Digital World</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">CUDA编程学习：自定义Pytorch+cpp/cuda extension</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-12-12T03:26:00.000Z" title="发表于 2023-12-12 11:26:00">2023-12-12</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-01-30T05:04:13.227Z" title="更新于 2024-01-30 13:04:13">2024-01-30</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/CUDA/">CUDA</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">8.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>32分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="CUDA编程学习：自定义Pytorch+cpp/cuda extension"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>视频资料：</p><p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=l_Rpk6CRJYI&amp;list=PLDV2CyUo4q-LKuiNltBqCKdO9GH4SS_ec&amp;ab_channel=AI%E8%91%B5">Pytorch+cpp/cuda extension 教學</a></p><p>Github：</p><p><a target="_blank" rel="noopener" href="https://github.com/Kedreamix/pytorch-cppcuda-tutorial">https://github.com/Kedreamix/pytorch-cppcuda-tutorial</a></p><p>Pytorch官方资料：</p><p><a target="_blank" rel="noopener" href="https://pytorch.org/cppdocs/">PyTorch C++ API - PyTorch main documentation</a></p><p><a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/advanced/cpp_extension.html">pytorch.org/tutorials/advanced/cpp_extension.html</a></p><p>CUDA doc：</p><p><a target="_blank" rel="noopener" href="https://nyu-cds.github.io/python-gpu/02-cuda/">Introduction to GPU</a></p><h2 id="学习背景"><a href="#学习背景" class="headerlink" title="学习背景"></a>学习背景</h2><p>虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要<strong>更定制化的操作</strong>，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，这使我们可以充分利用自动微分<code>autograd</code>的功能，<strong>然而有时候代码在模型中被频繁调用或者调用代价比较大，我们就可能需要在C++中进行实现。另一个可能的原因可能需要依赖于其他的C++库，为了解决这种情况，PyTorch提供了一种非常简单的方式来编写自定义C++扩展。</strong></p><p>简单介绍一下Pytorch C++的API部分，主要有以下五部分</p><ol><li><strong>ATen：</strong> 作为基础张量和数学操作库，所有其他接口都构建在其上。</li><li><strong>Autograd：</strong> 通过自动微分增强了ATen，记录张量上的操作以形成自动微分图。</li><li><strong>C++ Frontend：</strong> 提供了用于神经网络和机器学习模型的高级纯C++建模接口。</li><li><strong>TorchScript：</strong> 是一个可以由TorchScript编译器理解、编译和序列化的PyTorch模型表示。</li><li><strong>C++ Extensions：</strong> 用于扩展Python API的自定义C++和CUDA例程。</li></ol><p>这些块组合形成了一个C++库，可用于张量计算和具有高效的GPU加速以及快速CPU性能的动态神经网络。</p><blockquote><p>在这部分中，ATen是一个基础张量库，几乎所有PyTorch的Python和C++接口都构建在其上。Autograd是C++ API的一部分，用于为ATen张量类添加自动微分功能。我们编写C++的扩展的时候，我们实际上是基于ATen进行操作和书写的。</p></blockquote><h2 id="适用对象与场景"><a href="#适用对象与场景" class="headerlink" title="适用对象与场景"></a>适用对象与场景</h2><p>实际上pytorch+cuda是为了加速pytorch的计算，如果pytorch的计算已经可以满足了，就可以跳过这一部分，因为本身pytorch也已经蕴含了很多的函数</p><ul><li><p><strong>非平行运算 non parallel computation</strong>：在这样的场景下，比如现在一个batch里面，都是平行运算，所以这时候可以直接用pytorch进行实现，但是在NeRF的体渲染volume rendering中，我们就可以知道，每一条射线可能采样的点都是不一样的，如果我们去用for循环就可能需要花比较多的时间，这时候就需要cuda的存在。</p></li><li><p><strong>大量的串列计算 lots of sequential computation</strong>：比如神经网络的卷积层的计算的，比如在forward中，经常会出现以下这样的情况</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = f1(x)</span><br><span class="line">x = f2(x)</span><br><span class="line">...</span><br><span class="line">x = fn(x)</span><br></pre></td></tr></tbody></table></figure><p>如果在层数比较小的时候，这样是可以得到不错的结果的，但是层数比较大的时候，不断的内存访问其实会减慢速度，这时候就需要CUDA来进行加速，比如我们可以将所有的f变成一个函数F，融合了所有的函数后，我们就可以进行一次运算得到最后的结果，在层数大的时候能得到很大的提升。</p></li></ul><p>在这一部分的学习中，主要还是在第一个场景，非平行运算，特别是NeRF的体渲染部分，这一部分的学习和加速还是非常重要的，值得学习。</p><h2 id="Pytorch和CUDA的关系"><a href="#Pytorch和CUDA的关系" class="headerlink" title="Pytorch和CUDA的关系"></a>Pytorch和CUDA的关系</h2><p>一般来说，是pytorch -&gt; C++ &gt; cuda，也就是pytorch调用C++，然后C++再调用cuda，所以C++作为的是一个桥梁，所以比较重要的cuda，而不是C++，利用cuda进行平行的计算。</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img-blog.csdnimg.cn/direct/64f3116d776247ae975479b252554c0a.png" alt="Pytorch和CUDA的关系"></p><h2 id="Python调用C-函数（桥梁）"><a href="#Python调用C-函数（桥梁）" class="headerlink" title="Python调用C++函数（桥梁）"></a>Python调用C++函数（桥梁）</h2><p>首先声明一下，我的文件夹格式如下：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">pytorch-cppcuda-tutorial/</span><br><span class="line">  test.py</span><br><span class="line">  setup.py</span><br><span class="line">  interpolation.cpp</span><br><span class="line">  interpolation_kernel.cu</span><br><span class="line">  include/</span><br><span class="line">    utils.h</span><br></pre></td></tr></tbody></table></figure><p><strong>有一个问题可能我们会疑惑很久，就是python是怎么调用C++和CUDA的，这里面根据课程简单来讲一下，以三线性插值为例子</strong></p><p>首先，我们定义一个简单的函数。这个函数接受两个参数，分别是特征和点，然后直接返回特征。在这里，我们将看到一个核心的东西，即 <code>PYBIND11_MODULE</code>。这是 Python 调用 C++ 函数的关键部分。这个函数会在Python执行<code>import</code>语句时被调用，其接受两个参数，第一个参数为模块名称，这里我们直接将<code>trilinear_interpolation</code>填入，稍候可以在Python中使用<code>import cppcuda_tutorial</code>导入该模块；第二个参数<code>m</code>是创建Python关联代码的主接口，其类型为<code>py::module_</code>。<code>module_::def()</code>用于生成能够将<code>trilinear_interpolation</code>函数暴露给Python的代码，其第一个参数为<strong>字符串</strong>，将会成为Python中调用的函数名；第二个参数是<strong>C++函数</strong>的引用；第三个参数是<strong>说明字符串</strong>，在Python中可以使用<code>help(trilinear_interpolation)</code>查看。比如下面的例子中，C++ 中的函数 <code>trilinear_interpolation</code> 对应 Python 中的 <code>trilinear_interpolation</code>。</p><blockquote><p><strong><torch extension.h=""></torch></strong>是一站式头文件，包含写入C++扩展所需的所有PyTorch操作，包括：</p><ul><li>ATen库是用于张量计算的主要API，</li><li>pybind11，是为C++代码创建Python绑定的方式</li><li>管理ATen和pybind11之间交互细节的头文件</li></ul><p>PyTorch的张量和变量接口是从ATen库自动生成的，因此几乎可以将Python实现1:1转换为C++。所有计算的主要数据类型将是torch::Tensor。</p></blockquote><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;torch/extension.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">torch::Tensor <span class="title">trilinear_interpolation</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Tensor feats,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Tensor point</span></span></span><br><span class="line"><span class="params"><span class="function">)</span></span>{</span><br><span class="line">    <span class="keyword">return</span> feats;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="built_in">PYBIND11_MODULE</span>(TORCH_EXTENSION_NAME, m){</span><br><span class="line">    m.<span class="built_in">def</span>(<span class="string">"trilinear_interpolation"</span>, &amp;trilinear_interpolation);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><strong>注意：TORCH_EXTENSION_NAME，torch扩展构建会将其定义为在setup.py脚本中为扩展指定的名称。比如这里为“TORCH_EXTENSION_NAME“，两者之间的不匹配可能会导致严重且难以跟踪的问题。</strong></p><p>C++扩展一般有两种方式</p><ul><li>通过<code>setuptools</code>“提前”构建</li><li>通过<code>torch.utils.cpp_extension.load()</code>“实时”构建</li></ul><h3 id="Building-with-setuptools"><a href="#Building-with-setuptools" class="headerlink" title="Building with setuptools"></a>Building with <code>setuptools</code></h3><p>接下来先试用<code>setuptools</code>进行构建，编写一个 <code>setup.py</code> 文件，主要用于定义和说明一些重要的信息。其中关键的参数包括：</p><ul><li><code>name</code>：Python 调用的包的名称。</li><li><code>ext_modules</code> 的 <code>sources</code>：需要编译的 C++ 源文件，如果有多个 C++ 文件，需要列举所有。</li><li><code>cmdclass</code>：用BuildExtension执行许多必需的配置步骤和检查，并在混合C++/CUDA扩展的情况下处理混合编译。</li></ul><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">from setuptools <span class="keyword">import</span> setup</span><br><span class="line">from torch.utils.cpp_extension <span class="keyword">import</span> CppExtension, <span class="function">BuildExtension</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"><span class="title">setup</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    name=<span class="string">'cppcuda_tutorial'</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    version=<span class="string">'1.0'</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    author=<span class="string">'xxx'</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    author_email=<span class="string">'xxx@gmail.com'</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    description=<span class="string">'cppcuda example'</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    long_description=<span class="string">'cppcuda example'</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    ext_modules=[</span></span></span><br><span class="line"><span class="params"><span class="function">        CppExtension(</span></span></span><br><span class="line"><span class="params"><span class="function">            name=<span class="string">'cppcuda_tutorial'</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">            sources=[<span class="string">'interpolation.cpp'</span>])</span></span></span><br><span class="line"><span class="params"><span class="function">    ],</span></span></span><br><span class="line"><span class="params"><span class="function">    cmdclass={</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="string">'build_ext'</span>: BuildExtension</span></span></span><br><span class="line"><span class="params"><span class="function">    }</span></span></span><br><span class="line"><span class="params"><span class="function">)</span></span></span><br></pre></td></tr></tbody></table></figure><h3 id="JIT-Compiling-Extensions"><a href="#JIT-Compiling-Extensions" class="headerlink" title="JIT Compiling Extensions"></a>JIT Compiling Extensions</h3><p>除了上述的<code>setuptools</code>的方法，接下来介绍即时编译（JIT）机制构建C++扩展。JIT编译机制通过调用PyTorch API中的一个简单函数<code>torch.utils.cpp_extension.load()</code>，为你提供了一种即时编译和加载扩展的方式。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.cpp_extension <span class="keyword">import</span> load</span><br><span class="line"></span><br><span class="line">cppcuda_tutorial = load(name=<span class="string">"cppcuda_tutorial"</span>,</span><br><span class="line">                        <span class="comment"># extra_include_paths=include_dirs,</span></span><br><span class="line">                        sources=[<span class="string">'interpolation.cpp'</span>],)</span><br></pre></td></tr></tbody></table></figure><p>在这里，实际提供的是域setuptools相同的信息。在后台，这将执行以下操作：</p><ol><li>创建一个临时目录<code>/tmp/torch_extensions/cppcuda_tutorial</code>，</li><li>向该临时目录发出Ninja构建文件，</li><li>将你的源文件编译成一个共享库，</li><li>将这个共享库导入为Python模块。</li></ol><p>实际上，如果将<code>verbose=True</code>传递给<code>cpp_extension.load()</code>，你将得到有关该过程的信息：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Using /path/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...</span><br><span class="line">Detected CUDA files, patching ldflags</span><br><span class="line">Emitting ninja build file /path/.cache/torch_extensions/py310_cu113/cppcuda_tutorial/build.ninja...</span><br><span class="line">Building extension module cppcuda_tutorial...</span><br><span class="line">Allowing ninja to <span class="built_in">set</span> a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)</span><br><span class="line">[1/2] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=cppcuda_tutorial -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/path/workdirs/pytorch-cppcuda-tutorial/include -isystem /path/anaconda3/envs/cppcuda/lib/python3.10/site-packages/torch/include -isystem /path/anaconda3/envs/cppcuda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /path/anaconda3/envs/cppcuda/lib/python3.10/site-packages/torch/include/TH -isystem /path/anaconda3/envs/cppcuda/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /path/anaconda3/envs/cppcuda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=<span class="built_in">arch</span>=compute_86,code=compute_86 -gencode=<span class="built_in">arch</span>=compute_86,code=sm_86 --compiler-options <span class="string">'-fPIC'</span> -std=c++14 -c /path/workdirs/pytorch-cppcuda-tutorial/interpolation_kernel.cu -o interpolation_kernel.cuda.o </span><br><span class="line">[2/2] c++ interpolation.o interpolation_kernel.cuda.o -shared -L/path/anaconda3/envs/cppcuda/lib/python3.10/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda_cu -ltorch_cuda_cpp -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o cppcuda_tutorial.so</span><br></pre></td></tr></tbody></table></figure><p>完成这一步后，如果使用<code>setuptools</code>进行构建，我们可以使用 <code>pip</code> 进行安装。如果在当前文件夹下，直接运行 <code>pip install .</code> 即可完成安装或者我们也可以使用<code>python set.py install</code>，安装成功后应该会显示以下结果：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Processing path/pytorch-cppcuda-tutorial</span><br><span class="line">  Preparing metadata (setup.py) ... <span class="keyword">done</span></span><br><span class="line">Building wheels <span class="keyword">for</span> collected packages: cppcuda-tutorial</span><br><span class="line">  Building wheel <span class="keyword">for</span> cppcuda-tutorial (setup.py) ... <span class="keyword">done</span></span><br><span class="line">  Created wheel <span class="keyword">for</span> cppcuda-tutorial: filename=cppcuda_tutorial-1.0-cp310-cp310-linux_x86_64.whl size=74123 sha256=3029b98bd3b49bed65f42640e60932c38379f15db48a5187fe40610b525307c9</span><br><span class="line">  Stored <span class="keyword">in</span> directory: /path/.cache/pip/wheels/65/53/4a/5e2c10d11e3a657b9efae376ccce3277e5535d691dd4659883</span><br><span class="line">Successfully built cppcuda-tutorial</span><br><span class="line">Installing collected packages: cppcuda-tutorial</span><br><span class="line">Successfully installed cppcuda-tutorial-1.0</span><br></pre></td></tr></tbody></table></figure><p>完成以上步骤后，我们可以编写一个 <code>test.py</code> 文件来测试代码的正确性。只要能够成功运行，就代表一切正常。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> cppcuda_tutorial <span class="comment"># 位置需要在import torch后面</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">feats = torch.ones(<span class="number">2</span>)</span><br><span class="line">point = torch.zeros(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用函数</span></span><br><span class="line">out = cppcuda_tutorial.trilinear_interpolation(feats, point)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(out)</span><br></pre></td></tr></tbody></table></figure><p>这里面要注意的就是，首席爱你要导入torch，解析动态链接器必须看到的一些符号</p><h2 id="CUDA加速的原理"><a href="#CUDA加速的原理" class="headerlink" title="CUDA加速的原理"></a>CUDA加速的原理</h2><p>首先介绍一个CUDA程序实现的流程</p><ol><li>把数据从CPU内存拷贝到GPU内存</li><li>调用核函数对存储在GPU内存中的数据进行操作</li><li>将数据从GPU内存传送回CPU内存</li></ol><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic2.zhimg.com/v2-2959e07a36a8dc8f59280f53b43eb9d1_b.jpg" alt="CUDA编程入门极简教程- 知乎"></p><p>如下图所示，在利用CUDA加速的时候，图的左边是CPU，右边是GPU，我们需要把数据从CPU传到GPU中。在GPU中，就会生成对应的Grid来进行计算，每个Grid里面又有很多的block，从block中看又有很多的线程thread进行运算。我们也可以想象一下，如果计算一个矩阵的加法，我们可以让每个thread做对应的元素的相加，这样就可以大大加快计算速度，达到并行的效果。所以这之中内核（kernel）是CUDA编程中一个重要的部分，其代码在GPU上运行，比如矩阵乘法，我们就可以写一个加法的核函数，然后串行执行核函数，这样我们就快速能完成CUDA代码的编写，而不用在创建和管理大量的GPU线程时拘泥于细节。</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://nyu-cds.github.io/python-gpu/fig/02-threadmapping.png" alt="Thread Mapping"></p><p>所以我们可以发现，实际上CUDA的计算是<code>Grid</code>——》<code>Block</code>——》<code>Thread</code>，然后用多个<code>Thread</code>进行计算，这里面可能会疑惑，为什么不是直接<code>Grid</code>——》<code>Thread</code>，实际上是因为硬件的限制是<code>Block</code>上限是$(2^{31}-1)<em>2^{16}</em>2^{16}$，<code>Thread</code>的上限是1024，所以这样的组合设计能够利用好更多的<code>Thread</code>，这也是为什么GPU速度那么快的原因。</p><h2 id="三线性插值问题定义"><a href="#三线性插值问题定义" class="headerlink" title="三线性插值问题定义"></a>三线性插值问题定义</h2><p>有关于线性插值和三线性插值的介绍，可以从这部分资料进行了解，<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/77496615">https://zhuanlan.zhihu.com/p/77496615</a>，<a target="_blank" rel="noopener" href="https://blog.csdn.net/webzhuce/article/details/86585489">https://blog.csdn.net/webzhuce/article/details/86585489</a>，这样我们就知道三线性插值的概念，和大概的思路，这样我们就可以进行一个CUDA的实现了。</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img-blog.csdnimg.cn/20190121221016700.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlYnpodWNl,size_16,color_FFFFFF,t_70" alt="三线性插值"></p><p>从三线性插值的概念我们可以知道，我们需要传入两个参数</p><ul><li>feats(N, 8, F)：N个立方体，每个立方体有8个点，每个点有F个特征</li><li>Points(N,3)：N个点，每个点的坐标分别是xyz，一共有三个维度</li></ul><p>我们也可以知道输出的参数为<code>feat_interp(N, F)</code>，也就是插值后的结果</p><p>从上述定义我们就可以知道，我们有两个部分可以进行平行运算，分别是N和F，因为它们是独立的，不会相互影响计算。</p><h2 id="C-调用CUDA函数"><a href="#C-调用CUDA函数" class="headerlink" title="C++调用CUDA函数"></a>C++调用CUDA函数</h2><p>首先我们先看看，怎么使用CUDA去进行编程，首先CUDA的代码是<code>cu</code>结尾的，我们通过编写CUDA来进行一个计算加速。我们还是先按之前的方法，看看如何使用CUDA进行编程先，这里面有几个注意的点：</p><ul><li>需要编写CUDA函数</li><li>需要在头文件<code>.h</code>中去定义需要使用的函数，包括一些常用的测试函数。</li><li>修改CUDA的<code>setup.py</code></li></ul><p>接下来一步一步来，首先写一个<code>interpolation_kernel.cu</code>函数，也就是一个CUDA函数，后面我们可以用C++调用CUDA，这里面还是直接返回feats</p><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;torch/extension.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">torch::Tensor <span class="title">trilinear_fw_cu</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Tensor feats,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Tensor points</span></span></span><br><span class="line"><span class="params"><span class="function">)</span></span>{</span><br><span class="line">    <span class="keyword">return</span> feats;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>接下来，我们就需要在头文件<code>utils.h</code>中去定义我们在文件中需要使用的函数，类似于原本C++的一个声明和定义函数</p><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;torch/extension.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK_CUDA(x) TORCH_CHECK(x.is_cuda(), #x <span class="string">" must be a CUDA tensor"</span>)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x <span class="string">" must be contiguous"</span>)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 声明和定义函数</span></span><br><span class="line"><span class="function">torch::Tensor <span class="title">trilinear_fw_cu</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Tensor feats,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Tensor points</span></span></span><br><span class="line"><span class="params"><span class="function">)</span></span>;</span><br></pre></td></tr></tbody></table></figure><p>这样编写以后，我们的cpp的代码就可以调用CUDA对函数来进行调用，但是由于我们使用CUDA的函数，所以这里面我们还要用到<code>CHECK_INPUT</code>函数来判断是否在GPU上，也就是一个检测，并且内存是否连续，因为后续要进行一个并行的计算。</p><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">"utils.h"</span></span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function">torch::Tensor <span class="title">trilinear_interpolation</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Tensor feats,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Tensor points</span></span></span><br><span class="line"><span class="params"><span class="function">)</span></span>{</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(feats);</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(points);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">trilinear_fw_cu</span>(feats, points);</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">PYBIND11_MODULE</span>(TORCH_EXTENSION_NAME, m){</span><br><span class="line">    m.<span class="built_in">def</span>(<span class="string">"trilinear_interpolation"</span>, &amp;trilinear_interpolation);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>完成上述编写之后，我们最后就只剩下<code>setup.py</code>函数需要修改，其实需要修改的东西非常有限，只需要将上述的<code>CPPExtension</code>改为<code>CUDAExtension</code>，也就是改成CUDA的编译，这里面还有比较好的方法就是，之前我们source需要自己写，但是当我们有很多个文件的时候，我们就可以自动检索文件夹下的cpp和cu文件，进行build即可得到最后的结果。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">import</span> os.path <span class="keyword">as</span> osp</span><br><span class="line"><span class="keyword">from</span> setuptools <span class="keyword">import</span> setup</span><br><span class="line"><span class="keyword">from</span> torch.utils.cpp_extension <span class="keyword">import</span> CUDAExtension, BuildExtension</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ROOT_DIR = osp.dirname(osp.abspath(__file__))</span><br><span class="line">include_dirs = [osp.join(ROOT_DIR, <span class="string">"include"</span>)] <span class="comment"># 得到include文件夹下所有的头文件.h</span></span><br><span class="line"></span><br><span class="line">sources = glob.glob(<span class="string">'*.cpp'</span>)+glob.glob(<span class="string">'*.cu'</span>) <span class="comment"># 得到当前文件夹下所有cu文件和cpp文件</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">setup(</span><br><span class="line">    name=<span class="string">'cppcuda_tutorial'</span>,</span><br><span class="line">    version=<span class="string">'1.0'</span>,</span><br><span class="line">    author=<span class="string">'xxx'</span>,</span><br><span class="line">    author_email=<span class="string">'xxx@gmail.com'</span>,</span><br><span class="line">    description=<span class="string">'cppcuda_tutorial'</span>,</span><br><span class="line">    long_description=<span class="string">'cppcuda_tutorial'</span>,</span><br><span class="line">    ext_modules=[</span><br><span class="line">        CUDAExtension(</span><br><span class="line">            name=<span class="string">'cppcuda_tutorial'</span>,</span><br><span class="line">            sources=sources,</span><br><span class="line">            include_dirs=include_dirs,</span><br><span class="line">            extra_compile_args={<span class="string">'cxx'</span>: [<span class="string">'-O2'</span>],</span><br><span class="line">                                <span class="string">'nvcc'</span>: [<span class="string">'-O2'</span>]}</span><br><span class="line">        )</span><br><span class="line">    ],</span><br><span class="line">    cmdclass={</span><br><span class="line">        <span class="string">'build_ext'</span>: BuildExtension</span><br><span class="line">    }</span><br><span class="line">)</span><br></pre></td></tr></tbody></table></figure><p>安装过后，我们就可以测试是否使用CUDA进行计算，唯一不同的是，由于我们是使用CUDA进行计算，所以我们要把数据转到CUDA中即可</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> cppcuda_tutorial</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line"></span><br><span class="line">    feats = torch.ones(<span class="number">2</span>, device=<span class="string">'cuda'</span>)</span><br><span class="line">    points = torch.zeros(<span class="number">2</span>, device=<span class="string">'cuda'</span>)</span><br><span class="line"></span><br><span class="line">    out = cppcuda_tutorial.trilinear_interpolation(feats, points)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(out)</span><br></pre></td></tr></tbody></table></figure><blockquote><p>在这里，可能第一次学习会觉得比较麻烦，但是实际上有一些函数，比如CHECK的函数和setup.py的函数，只要写了一次以后，之后都是可以参考复用的，不用重复写</p></blockquote><h2 id="三线性插值CUDA实现"><a href="#三线性插值CUDA实现" class="headerlink" title="三线性插值CUDA实现"></a>三线性插值CUDA实现</h2><p>接下来就是主要的三线性插值的CUDA实现了，在前面的CUDA加速中有说到，实际上我们是希望在每一个thread都执行一个单元的计算，在三线性插值中，我们可以知道，我们两个部分需要并行，分别是<code>N</code>和<code>F</code>两个部分，也就是立方体的个数和特征的个数。</p><p>我们先看看需要进行编写的函数，然后一步一步的来解释和探索，以下是更新后的<code>forward</code>函数</p><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">torch::Tensor <span class="title">trilinear_fw_cu</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Tensor feats,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Tensor points</span></span></span><br><span class="line"><span class="params"><span class="function">)</span></span>{</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> N = feats.<span class="built_in">size</span>(<span class="number">0</span>), F = feats.<span class="built_in">size</span>(<span class="number">2</span>);</span><br><span class="line">    <span class="comment">// 等价于 feat_interp = torch.zeros(N, F, dtype = torch.float32, device = "cuda:0")</span></span><br><span class="line">    torch::Tensor feat_interp = torch::<span class="built_in">zeros</span>({N, F}, feats.<span class="built_in">options</span>());</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">const</span> dim3 <span class="title">threads</span><span class="params">(<span class="number">16</span>, <span class="number">16</span>)</span></span>; <span class="comment">// 128,256,512</span></span><br><span class="line">    <span class="function"><span class="type">const</span> dim3 <span class="title">blocks</span><span class="params">((N+threads.x<span class="number">-1</span>)/threads.x, (F+threads.y<span class="number">-1</span>)/threads.y)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">AT_DISPATCH_FLOATING_TYPES</span>(feats.<span class="built_in">type</span>(), <span class="string">"trilinear_fw_cu"</span>, </span><br><span class="line">    ([&amp;] {</span><br><span class="line">        trilinear_fw_kernel&lt;<span class="type">scalar_t</span>&gt;&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(</span><br><span class="line">            feats.<span class="built_in">packed_accessor</span>&lt;<span class="type">scalar_t</span>, <span class="number">3</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt;(),</span><br><span class="line">            points.<span class="built_in">packed_accessor</span>&lt;<span class="type">scalar_t</span>, <span class="number">2</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt;(),</span><br><span class="line">            feat_interp.<span class="built_in">packed_accessor</span>&lt;<span class="type">scalar_t</span>, <span class="number">2</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt;()</span><br><span class="line">        );</span><br><span class="line">    }));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> feat_interp;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>第5行我们得到了对应的维度，分别是N和F，这也是我们最后需要返回值<code>feat_interp</code>的维度。</p><p>第7行我们初始化了变量<code>feat_interp</code>，这里面是初始化为zero，在里面还有一个参数是<code>feats.options()</code>，在CUDA编程中，<code>feats.options()</code>表示获取<code>feats</code>张量的选项（options）。选项包括张量的数据类型、设备（设备指定为CUDA或CPU）以及其他相关的配置信息。通过使用<code>feats.options()</code>，可以确保新创建的<code>feat_interp</code>张量与<code>feats</code>张量具有相同的选项，以便在相同的设备上进行操作，并保持一致性。</p><p>简单来说，就是保持一致的设备等等，这样就方便后续在同一个设备进行计算，和pytorch需要放在cpu和cuda上是一样的，除此之外，还有一些另外的写法，比如是创建一个整型的，可以写成如下，一样的意思。</p><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch::<span class="built_in">zeros</span>({N，F}，torch::<span class="built_in">dtype</span>(torch::kInt32).<span class="built_in">device</span>(feats,device));</span><br></pre></td></tr></tbody></table></figure><p>第9行和第10行就是定义上述提过的<code>threads</code>和<code>blocks</code>了，dim3是NVIDIA的CUDA编程中一种自定义的整型向量类型，基于用于指定维度的uint3，dim3类型最终设置的是一个三维向量，三维参数分别为x,y,z。在并行中，通常只支持三个并行，比如这里的N和F刚刚好就是两个并行，这里设置为16x16的线程，一般可以是128,256,512，不一定使用越多越好，这里面只是给了一个例子。</p><p>第10行中是定义了<code>blocks</code>的计算，<code>blocks</code>的个数实际上是计算的得到的，如下图所示，如果N=10，F=20，我们最后的输出就是10x20，这样我们就会用一个16x16的block去覆盖这整个矩阵，我们会发现大概需要2个矩阵，所以我们的<code>blocks</code>就是(2,1)，从下图也可以看出来，所以上述公式就是计算<code>block</code>的个数，这样我们就可以用每一个<code>thread</code>去计算，这样就能大大加快速度。</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img-blog.csdnimg.cn/direct/3dfa0f2ed94b4d9d9297a90e55a59662.png" alt="在这里插入图片描述"></p><p>第12~19行就是CUDA的核心函数，这里面就是一个启动核函数的部分，后面会提到核函数的编写，这里也是一个框架的部分，<code>AT_DISPATCH_FLOATING_TYPES</code> 是处理核函数的启动（使用 <code>&lt;&lt;&lt;...&gt;&gt;&gt;</code> 表示），它一般有三个参数</p><ul><li>一个类型 feats.type()</li><li>一个名称 “trilinear_fw_cu”，用于错误消息</li><li>一个 lambda 函数，是一个模版函数template，类型别名为 <code>scalar_t</code></li></ul><p>在这里面可以看出处理的是float类型的数据，如果想对所有类型进行操作而不仅仅是浮点类型（Float 和 Double），可以使用 <code>AT_DISPATCH_ALL_TYPES</code>。</p><h3 id="scalar-t类型"><a href="#scalar-t类型" class="headerlink" title="scalar_t类型"></a>scalar_t类型</h3><p>在函数之中，我们可以看到我们有三个input，其中两个是三线性插值的input，一个是output，为什么呢，是因为其实这个函数是没有返回值的，所以说实际上是在函数里面计算后复制在output之中，最后进行返回。</p><p>我们来仔细了解了一下具体函数的编写是什么意思，首先是<code>scalar_t</code>其实是一种类型，他可以表示任何类型，包括整型，浮点型等等，如果我们确定数据是float，我们也可以直接将<code>scalar_t</code>写为<code>float</code>，那么他可能就只能处理浮点型的数据了，从下面也可以看到<code>scalar_t</code>的一个简单的实现。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">switch (tensor.<span class="built_in">type</span>().scalarType()) {</span><br><span class="line">  <span class="keyword">case</span> torch::ScalarType::Double:</span><br><span class="line">    <span class="keyword">return</span> function&lt;double&gt;(tensor.data&lt;double&gt;());</span><br><span class="line">  <span class="keyword">case</span> torch::ScalarType::Float:</span><br><span class="line">    <span class="keyword">return</span> function&lt;<span class="built_in">float</span>&gt;(tensor.data&lt;<span class="built_in">float</span>&gt;());</span><br><span class="line">  ...</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="accessors"><a href="#accessors" class="headerlink" title="accessors"></a>accessors</h3><p>在CUDA计算的时候，还有一个问题，在 CUDA 核函数内部，虽然我们能正确处理数据，直接使用高级类型<code>scalar_t</code>不可知的张量将非常低效，因为这是以易用性和可读性为代价的，特别是对于高维数据。</p><p>比如说，在数据中，我们这里有(N, F)个数据，那我们有没有快速的方法去读取到<code>feat_interp[i][j]</code>的数据呢，特别是有些一般是三个维度的，比如(bs,row,index)这样的，并且有时候我们还需要知道stride才能快速索引到位置，比如<code>gates.data&lt;scalar_t&gt;()[n*3*state_size + row*state_size + column]</code></p><p>在这里面，我们可能就需要用到一个<code>ATen</code>提供的<code>accessors</code>，他可以动态检查确保张量具有指定的类型和维度数量，器提供了一个 API，用于高效地访问张量元素，而无需转换为单个指针，就可以高效访问 cpu 张量上的数据，cuda我们就可以用<code>packed_accessor</code>。</p><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">torch::Tensor foo = torch::<span class="built_in">rand</span>({<span class="number">12</span>, <span class="number">12</span>});</span><br><span class="line"></span><br><span class="line"><span class="comment">// 确定 foo 是二维的并且包含浮点数。</span></span><br><span class="line"><span class="keyword">auto</span> foo_a = foo.<span class="built_in">accessor</span>&lt;<span class="type">float</span>,<span class="number">2</span>&gt;();</span><br><span class="line"><span class="type">float</span> trace = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; foo_a.<span class="built_in">size</span>(<span class="number">0</span>); i++) {</span><br><span class="line">  <span class="comment">// 使用访问器 foo_a 来获取张量数据。</span></span><br><span class="line">  trace += foo_a[i][i];</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>所以我们在核函数内部看到了<code>packed_accessor</code>，这一部分就是做这样一件事情，<strong>不过值得注意的是，只有对torch的向量我们需要这样的操作，如果是bool等，我们是不需要处理的。</strong></p><hr><h3 id="模板函数"><a href="#模板函数" class="headerlink" title="模板函数"></a>模板函数</h3><p>上述有提到，实际上我们的<code>trilinear_fw_kernel</code>是一个模板函数，我们接下来看一下具体的实现，我们利用<code>scalar_t</code>对其进行实例化，我们在这里再解释一下这个模板函数的参数部分：</p><ul><li><p>首先是 <code>scalar_t</code>，它是一个模板参数，代表张量的数据类型。在这个上下文中，通常会使用 <code>float</code> 或 <code>double</code> 作为 <code>scalar_t</code>，具体取决于张量的数据类型。</p></li><li><p>接下来是 <code>3</code>，它表示张量的维度数量。在这个例子中，我们的feats的维度是3，所以维度数量为 3。</p></li><li><p>然后是 <code>torch::RestrictPtrTraits</code>，它是一个模板参数，用于指定指针的限定符。<code>__restrict__</code> 关键字在 CUDA 中用于指示指针是唯一的，并且没有别名。这有助于编译器进行优化，提高代码的性能。</p></li><li><p>最后是 <code>PackedTensorAccessor</code>，它是一个访问器（accessor）的变体，用于存储大小和步幅信息，可以使得在访问器对象传递给 CUDA 核函数时，内存传输的数据量也更小。</p></li></ul><p>接下来我们仔细分析里面代码的细节，首先介绍一下这个<code>__global__</code>，他实际意义如下，如下图所示</p><ul><li><p><code>__global__</code>表示CPU上定义，GPU上执行，是CUDA的关键字</p></li><li><p><code>__device__</code> GPU定 义，GPU执行</p></li><li><p><code>__host__</code> CPU定义，CPU执行</p></li></ul><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img2018.cnblogs.com/blog/1093303/201809/1093303-20180919123125957-1702896390.png" alt="CUDA编程之快速入门- 最难不过二叉树- 博客园"></p><p>接下里分析函数的主体部分，主要做两件事情：</p><ol><li>为每个<code>threads</code>进行编号</li><li>去除不必要的<code>threads</code></li></ol><p>在使用<code>threads</code>计算的时候，实际上每一个<code>threads</code>都有一个对应的编号，计算方式如第7,8行所示，实际上就是block的x*block的个数+block的y就能得到最后的结果</p><p>除了编号之外，还有去除不必要的<code>threads</code>，因为有一部分是没有覆盖到的，比如如下图的黄色部分就是不必要的<code>threads</code>，所以在第10行进行判断，如果超过范围，直接return不计算</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img-blog.csdnimg.cn/direct/6daed7e700bb4fb6927b4a8a89b8a1ea.png" alt="在这里插入图片描述"></p><p>最后就是上述说明的三线性插值的做法了，先进行一个标准化，然后代入公式进行计算，最后将值写入feat_interp中，就完成了整个模板函数的编写，大功告成！！！</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img-blog.csdnimg.cn/20190121221044883.png" alt="在这里插入图片描述"></p><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> <span class="type">scalar_t</span>&gt;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">trilinear_fw_kernel</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::PackedTensorAccessor&lt;<span class="type">scalar_t</span>, <span class="number">3</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt; feats,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::PackedTensorAccessor&lt;<span class="type">scalar_t</span>, <span class="number">2</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt; points,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::PackedTensorAccessor&lt;<span class="type">scalar_t</span>, <span class="number">2</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt; feat_interp</span></span></span><br><span class="line"><span class="params"><span class="function">)</span></span>{</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> n = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> f = blockIdx.y * blockDim.y + threadIdx.y;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (n&gt;=feats.<span class="built_in">size</span>(<span class="number">0</span>) || f&gt;=feats.<span class="built_in">size</span>(<span class="number">2</span>)) <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// point -1~1</span></span><br><span class="line">    <span class="type">const</span> <span class="type">scalar_t</span> u = (points[n][<span class="number">0</span>]+<span class="number">1</span>)/<span class="number">2</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">scalar_t</span> v = (points[n][<span class="number">1</span>]+<span class="number">1</span>)/<span class="number">2</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">scalar_t</span> w = (points[n][<span class="number">2</span>]+<span class="number">1</span>)/<span class="number">2</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="type">const</span> <span class="type">scalar_t</span> a = (<span class="number">1</span>-v)*(<span class="number">1</span>-w);</span><br><span class="line">    <span class="type">const</span> <span class="type">scalar_t</span> b = (<span class="number">1</span>-v)*w;</span><br><span class="line">    <span class="type">const</span> <span class="type">scalar_t</span> c = v*(<span class="number">1</span>-w);</span><br><span class="line">    <span class="type">const</span> <span class="type">scalar_t</span> d = <span class="number">1</span>-a-b-c;</span><br><span class="line">    feat_interp[n][f] = (<span class="number">1</span>-u)*(a*feats[n][<span class="number">0</span>][f] +</span><br><span class="line">                               b*feats[n][<span class="number">1</span>][f] +</span><br><span class="line">                               c*feats[n][<span class="number">2</span>][f] +</span><br><span class="line">                               d*feats[n][<span class="number">3</span>][f]) + </span><br><span class="line">                            u*(a*feats[n][<span class="number">4</span>][f] +</span><br><span class="line">                               b*feats[n][<span class="number">5</span>][f] +</span><br><span class="line">                               c*feats[n][<span class="number">6</span>][f] +</span><br><span class="line">                               d*feats[n][<span class="number">7</span>][f]);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h3 id="foward验证与比较"><a href="#foward验证与比较" class="headerlink" title="foward验证与比较"></a>foward验证与比较</h3><p>经过<code>python setup.py install</code>以后（每次修改后都要重新运行<code>setup.py</code>），我们就可以进行运行了，在这里面为了验证结果的正确性和与python进行比较，用python实现三线性插值的算法，比较两者的结果和时间效率，<code>test.py</code>如下：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> cppcuda_tutorial</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">trilinear_interpolation_py</span>(<span class="params">feats, points</span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">        feats: (N, 8, F)</span></span><br><span class="line"><span class="string">        points: (N, 3) local coordinates in [-1, 1]</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Outputs:</span></span><br><span class="line"><span class="string">        feats_interp: (N, F)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    u = (points[:, <span class="number">0</span>:<span class="number">1</span>]+<span class="number">1</span>)/<span class="number">2</span></span><br><span class="line">    v = (points[:, <span class="number">1</span>:<span class="number">2</span>]+<span class="number">1</span>)/<span class="number">2</span></span><br><span class="line">    w = (points[:, <span class="number">2</span>:<span class="number">3</span>]+<span class="number">1</span>)/<span class="number">2</span></span><br><span class="line">    a = (<span class="number">1</span>-v)*(<span class="number">1</span>-w)</span><br><span class="line">    b = (<span class="number">1</span>-v)*w</span><br><span class="line">    c = v*(<span class="number">1</span>-w)</span><br><span class="line">    d = <span class="number">1</span>-a-b-c</span><br><span class="line"></span><br><span class="line">    feats_interp = (<span class="number">1</span>-u)*(a*feats[:, <span class="number">0</span>] +</span><br><span class="line">                          b*feats[:, <span class="number">1</span>] +</span><br><span class="line">                          c*feats[:, <span class="number">2</span>] +</span><br><span class="line">                          d*feats[:, <span class="number">3</span>]) + \</span><br><span class="line">                       u*(a*feats[:, <span class="number">4</span>] +</span><br><span class="line">                          b*feats[:, <span class="number">5</span>] +</span><br><span class="line">                          c*feats[:, <span class="number">6</span>] +</span><br><span class="line">                          d*feats[:, <span class="number">7</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> feats_interp</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    N = <span class="number">65536</span>; F = <span class="number">256</span></span><br><span class="line">    feats = torch.rand(N, <span class="number">8</span>, F, device=<span class="string">'cuda'</span>).requires_grad_()</span><br><span class="line">    points = torch.rand(N, <span class="number">3</span>, device=<span class="string">'cuda'</span>)*<span class="number">2</span>-<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    t = time.time()</span><br><span class="line">    out_cuda = cppcuda_tutorial.trilinear_interpolation(feats, points)</span><br><span class="line">    torch.cuda.synchronize()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'   cuda time'</span>, time.time()-t, <span class="string">'s'</span>)</span><br><span class="line"></span><br><span class="line">    t = time.time()</span><br><span class="line">    out_py = trilinear_interpolation_py(feats, points)</span><br><span class="line">    torch.cuda.synchronize()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'pytorch time'</span>, time.time()-t, <span class="string">'s'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(torch.allclose(out_py, out_cuda)) <span class="comment"># 判断两者的差异</span></span><br></pre></td></tr></tbody></table></figure><p>经过运行和计算后，我们会得到以下结果，我们可以看到，CUDA是明显比Pytorch更快的，并且两者的计算结果也是一样的，如果需要更好的计算两者的速度的话，可能需要进行循环运行计算取平均更有可信度。</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">   cuda time 0.02436351776123047 s</span><br><span class="line">pytorch time 0.04364943504333496 s</span><br><span class="line">True</span><br></pre></td></tr></tbody></table></figure><h2 id="CUDA反向传播"><a href="#CUDA反向传播" class="headerlink" title="CUDA反向传播"></a>CUDA反向传播</h2><p>在上述实验中，当我们尝试添加自动求导的梯度计算时，使用<code>requires_grad_</code>，我们会发现通过CUDA返回的值实际上不会自动进行梯度计算（autograd）。然而，如果我们在Python中进行计算，它会自动进行梯度计算。</p><p>然而，在实际应用中，神经网络经常需要计算损失函数，并使用梯度下降等优化算法来不断优化参数。但是，在CUDA编程中，C++扩展API并没有提供自动求导（autograd）的方法。因此，我们必须自己实现反向传播的代码，计算每个输入的导数，并将其封装在<code>torch.autograd.Function</code>中。</p><p>在CUDA编程中，实现反向传播的代码通常包括以下步骤：</p><ol><li>在C++扩展中，创建一个新类，继承自<code>torch::autograd::Function</code>，用于定义前向传播和反向传播操作。</li><li>在新类中，重写<code>forward()</code>方法，定义前向传播的操作。这些操作将使用CUDA执行计算，并返回结果，其实就是上述的cuda的部分。</li><li>在新类中，重写<code>backward()</code>方法，定义反向传播的操作。这些操作将计算输入张量的梯度，并传递给上一层。</li><li>在CUDA和C++中，编写对应的<code>forward</code>和<code>backward</code>函数，计算前向传播和微分。</li><li>在Python代码中，使用这个自定义函数执行前向传播，并通过调用<code>backward()</code>方法执行反向传播。</li><li>在反向传播过程中，梯度将通过CUDA计算，并在每个层之间传递，从而计算出每个输入的导数。</li></ol><p>通过这种方式，我们可以在CUDA编程中手动实现反向传播，并获得每个输入的梯度，以便进行优化算法的参数更新。尽管需要手动编写反向传播代码，但这使我们能够在CUDA扩展中自定义梯度计算，并与PyTorch的自动求导机制无缝配合。</p><p>我们还是把三线性插值作为我们的一个例子，编写对应的反向传播，根据问题设定，我们可以知道我们的Points是固定的，所以我们需要对我们的Feats进行求微分。我们以简单的双线性插值来学习一下，怎么求微分，这里面其实涉及高数的知识，当然我们也可以把函数交给一些数学网站帮我们求得结果。<strong>从下图我们可以看到，<code>f</code>是双线性插值的结果，我们可以得到对应的四个导数，我们会发现实际上，他们的微分是对应的系数，推导在三线性插值也是一样的，所以他们对应的微分也就是对应的前缀。</strong></p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img-blog.csdnimg.cn/direct/c35a354c6fcb49678872728cf04e6306.png" alt="在这里插入图片描述"></p><p>在计算反向传播前，我们往往会有一个对应的一个损失<code>Loss</code>，然后再进行求微分，这里面其实就用到了高数里面的链式法则，使用链式法则我们就可以得到<code>L</code>对每一个<code>feat</code>的微分，明白了双线性插值的计算，我们也可以推导在三线性插值中。</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img-blog.csdnimg.cn/direct/777b69824dcf4ca0ba257613067fa9be.png" alt="在这里插入图片描述"></p><h3 id="定义CUDA函数"><a href="#定义CUDA函数" class="headerlink" title="定义CUDA函数"></a>定义CUDA函数</h3><p>明白了理论的计算，我们就可以进行对应的实现，首先我们可以编写对应的反向传播的CUDA函数，这一部分实际上和前向传播是一样的，首先我们还是定义反向传播函数，在这里和前向传播函数的不同就是对名字进行了修改，除此之外加入了两个参数，分别是<code>dL_dfeats</code>参数和<code>dL_dfeats</code>。简单解释一下这些参数，在问题的设定中，我们的<code>feats</code>的维度是的(N, 8, F)，所以我们的微分<code>dL_dfeats</code>的维度是和<code>feats</code>是一样的，然后再加入反向传播的核函数中即可；<code>dL_dfeat_interp</code>则是根据函数已知的，所以不用计算，直接传参数。</p><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">torch::Tensor <span class="title">trilinear_bw_cu</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::Tensor dL_dfeat_interp,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::Tensor feats,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::Tensor points</span></span></span><br><span class="line"><span class="params"><span class="function">)</span></span>{</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> N = feats.<span class="built_in">size</span>(<span class="number">0</span>), F = feats.<span class="built_in">size</span>(<span class="number">2</span>);</span><br><span class="line">    </span><br><span class="line">    torch::Tensor dL_dfeats = torch::<span class="built_in">empty</span>({N, <span class="number">8</span>, F}, feats.<span class="built_in">options</span>());</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">const</span> dim3 <span class="title">threads</span><span class="params">(<span class="number">16</span>, <span class="number">16</span>)</span></span>;</span><br><span class="line">    <span class="function"><span class="type">const</span> dim3 <span class="title">blocks</span><span class="params">((N+threads.x<span class="number">-1</span>)/threads.x, (F+threads.y<span class="number">-1</span>)/threads.y)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">AT_DISPATCH_FLOATING_TYPES</span>(feats.<span class="built_in">type</span>(), <span class="string">"trilinear_bw_cu"</span>, </span><br><span class="line">    ([&amp;] {</span><br><span class="line">        trilinear_bw_kernel&lt;<span class="type">scalar_t</span>&gt;&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(</span><br><span class="line">            dL_dfeat_interp.<span class="built_in">packed_accessor</span>&lt;<span class="type">scalar_t</span>, <span class="number">2</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt;(),</span><br><span class="line">            feats.<span class="built_in">packed_accessor</span>&lt;<span class="type">scalar_t</span>, <span class="number">3</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt;(),</span><br><span class="line">            points.<span class="built_in">packed_accessor</span>&lt;<span class="type">scalar_t</span>, <span class="number">2</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt;(),</span><br><span class="line">            dL_dfeats.<span class="built_in">packed_accessor</span>&lt;<span class="type">scalar_t</span>, <span class="number">3</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt;()</span><br><span class="line">        );</span><br><span class="line">    }));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dL_dfeats;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h3 id="核函数实现微分计算"><a href="#核函数实现微分计算" class="headerlink" title="核函数实现微分计算"></a>核函数实现微分计算</h3><p>接下来就是主要的核函数的实现，在这一部分我们就需要实现微分的计算，在前面已经介绍了双线性插值的微分的计算，推导在三线性插值是一样的，我们可以根据前向传播的代码，这一部分只需要保留前面的系数✖️对应位置的<code>dL_dfeat_interp</code>就可以得到最后的微分值，这一部分跟上述的推导是一模一样的。</p><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> <span class="type">scalar_t</span>&gt;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">trilinear_bw_kernel</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::PackedTensorAccessor&lt;<span class="type">scalar_t</span>, <span class="number">2</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt; dL_dfeat_interp,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::PackedTensorAccessor&lt;<span class="type">scalar_t</span>, <span class="number">3</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt; feats,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::PackedTensorAccessor&lt;<span class="type">scalar_t</span>, <span class="number">2</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt; points,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::PackedTensorAccessor&lt;<span class="type">scalar_t</span>, <span class="number">3</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt; dL_dfeats</span></span></span><br><span class="line"><span class="params"><span class="function">)</span></span>{</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> n = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> f = blockIdx.y * blockDim.y + threadIdx.y;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (n&gt;=feats.<span class="built_in">size</span>(<span class="number">0</span>) || f&gt;=feats.<span class="built_in">size</span>(<span class="number">2</span>)) <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// point -1~1</span></span><br><span class="line">    <span class="type">const</span> <span class="type">scalar_t</span> u = (points[n][<span class="number">0</span>]+<span class="number">1</span>)/<span class="number">2</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">scalar_t</span> v = (points[n][<span class="number">1</span>]+<span class="number">1</span>)/<span class="number">2</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">scalar_t</span> w = (points[n][<span class="number">2</span>]+<span class="number">1</span>)/<span class="number">2</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="type">const</span> <span class="type">scalar_t</span> a = (<span class="number">1</span>-v)*(<span class="number">1</span>-w);</span><br><span class="line">    <span class="type">const</span> <span class="type">scalar_t</span> b = (<span class="number">1</span>-v)*w;</span><br><span class="line">    <span class="type">const</span> <span class="type">scalar_t</span> c = v*(<span class="number">1</span>-w);</span><br><span class="line">    <span class="type">const</span> <span class="type">scalar_t</span> d = <span class="number">1</span>-a-b-c;</span><br><span class="line"></span><br><span class="line">    dL_dfeats[n][<span class="number">0</span>][f] = (<span class="number">1</span>-u)*a*dL_dfeat_interp[n][f];</span><br><span class="line">    dL_dfeats[n][<span class="number">1</span>][f] = (<span class="number">1</span>-u)*b*dL_dfeat_interp[n][f];</span><br><span class="line">    dL_dfeats[n][<span class="number">2</span>][f] = (<span class="number">1</span>-u)*c*dL_dfeat_interp[n][f];</span><br><span class="line">    dL_dfeats[n][<span class="number">3</span>][f] = (<span class="number">1</span>-u)*d*dL_dfeat_interp[n][f];</span><br><span class="line">    dL_dfeats[n][<span class="number">4</span>][f] = u*a*dL_dfeat_interp[n][f];</span><br><span class="line">    dL_dfeats[n][<span class="number">5</span>][f] = u*b*dL_dfeat_interp[n][f];</span><br><span class="line">    dL_dfeats[n][<span class="number">6</span>][f] = u*c*dL_dfeat_interp[n][f];</span><br><span class="line">    dL_dfeats[n][<span class="number">7</span>][f] = u*d*dL_dfeat_interp[n][f];</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h3 id="PYBIND11绑定函数"><a href="#PYBIND11绑定函数" class="headerlink" title="PYBIND11绑定函数"></a>PYBIND11绑定函数</h3><p>写好了反向传播函数之后，不要忘记绑定函数，这样我们才能在最后的python中调用对应的函数。</p><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">torch::Tensor <span class="title">trilinear_interpolation_fw</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::Tensor feats,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::Tensor points</span></span></span><br><span class="line"><span class="params"><span class="function">)</span></span>{</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(feats);</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(points);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">trilinear_fw_cu</span>(feats, points);</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function">torch::Tensor <span class="title">trilinear_interpolation_bw</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::Tensor dL_dfeat_interp,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::Tensor feats,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::Tensor points</span></span></span><br><span class="line"><span class="params"><span class="function">)</span></span>{</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(dL_dfeat_interp);</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(feats);</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(points);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">trilinear_bw_cu</span>(dL_dfeat_interp, feats, points);</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">PYBIND11_MODULE</span>(TORCH_EXTENSION_NAME, m){</span><br><span class="line">    m.<span class="built_in">def</span>(<span class="string">"trilinear_interpolation_fw"</span>, &amp;trilinear_interpolation_fw);</span><br><span class="line">    m.<span class="built_in">def</span>(<span class="string">"trilinear_interpolation_bw"</span>, &amp;trilinear_interpolation_bw);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h3 id="torch-autograd-Function封装"><a href="#torch-autograd-Function封装" class="headerlink" title="torch.autograd.Function封装"></a>torch.autograd.Function封装</h3><p>为了使用pytorch的<code>autograd</code>我们还差最后一步，就是使用<code>torch.autograd.Function</code>进行封装，不然的话不能进行反向传播，会出现一些奇奇怪怪的bug。</p><p>在下面的代码中，首先，我们需要定义<code>forward</code>和<code>backward</code>函数，<strong>记得我们都需要定义<code>@staticmethod</code>装饰器，这个是一定要的。</strong>接下来我们就可以开始完善<code>forward</code>和<code>backward</code>函数。在两个函数中，实际上我们就是调用C++扩展写好的函数，这里面唯一一个需要注意的就是<code>ctx</code>，实际上这里是<code>context</code>的缩写，这里就是表示有什么数据需要进行保存在反向传播中使用到，因为在<code>backward</code>我们还要传入对应的<code>feats</code>和<code>points</code>，所以在这里这两个参数都需要<code>save_for_backward</code>。</p><p>最后的<code>backward</code>就更简单了，传入的参数与<code>forward</code>返回的参数进行对应，接着我们从<code>ctx</code>取出需要用到的参数，从<code>ctx.saved_tensors</code>中取出，后续只需要调用对应的C++函数即可，在这里面我们返回了两个参数，分别是<code>dL_dfeats, None</code>，这一部分是因为实际上是因为，我们有两个参数，分别<code>feats, points</code>，而我们并没有对<code>points</code>进行计算微分，所以这里就返回None。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Trilinear_interpolation_cuda</span>(torch.autograd.Function):</span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">ctx, feats, points</span>):</span><br><span class="line">        feat_interp = cppcuda_tutorial.trilinear_interpolation_fw(feats, points)</span><br><span class="line"></span><br><span class="line">        ctx.save_for_backward(feats, points)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> feat_interp</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">ctx, dL_dfeat_interp</span>):</span><br><span class="line">        feats, points = ctx.saved_tensors</span><br><span class="line"></span><br><span class="line">        dL_dfeats = cppcuda_tutorial.trilinear_interpolation_bw(dL_dfeat_interp.contiguous(), feats, points)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dL_dfeats, <span class="literal">None</span></span><br></pre></td></tr></tbody></table></figure><h3 id="backward验证与比较"><a href="#backward验证与比较" class="headerlink" title="backward验证与比较"></a>backward验证与比较</h3><p>和上述一样，经过<code>python setup.py install</code>以后（每次修改后都要重新运行<code>setup.py</code>），我们就可以进行运行了，在这里面为了验证结果的正确性和与pytorch本身的反向传播进行比较，比较两者的结果和时间效率，<code>test.py</code>的主函数如下：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Trilinear_interpolation_cuda</span>(torch.autograd.Function):</span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">ctx, feats, points</span>):</span><br><span class="line">        feat_interp = cppcuda_tutorial.trilinear_interpolation_fw(feats, points)</span><br><span class="line"></span><br><span class="line">        ctx.save_for_backward(feats, points)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> feat_interp</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">ctx, dL_dfeat_interp</span>):</span><br><span class="line">        feats, points = ctx.saved_tensors</span><br><span class="line"></span><br><span class="line">        dL_dfeats = cppcuda_tutorial.trilinear_interpolation_bw(dL_dfeat_interp.contiguous(), feats, points)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dL_dfeats, <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    N = <span class="number">65536</span>; F = <span class="number">256</span></span><br><span class="line">    rand = torch.rand(N, <span class="number">8</span>, F, device=<span class="string">'cuda'</span>)</span><br><span class="line">    feats = rand.clone().requires_grad_()</span><br><span class="line">    feats2 = rand.clone().requires_grad_()</span><br><span class="line">    points = torch.rand(N, <span class="number">3</span>, device=<span class="string">'cuda'</span>)*<span class="number">2</span>-<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    t = time.time()</span><br><span class="line">    <span class="comment"># 调用CUDA计算</span></span><br><span class="line">    out_cuda = Trilinear_interpolation_cuda.apply(feats2, points)</span><br><span class="line">    torch.cuda.synchronize()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'   cuda fw time'</span>, time.time()-t, <span class="string">'s'</span>)</span><br><span class="line"></span><br><span class="line">    t = time.time()</span><br><span class="line">    out_py = trilinear_interpolation_py(feats, points)</span><br><span class="line">    torch.cuda.synchronize()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'pytorch fw time'</span>, time.time()-t, <span class="string">'s'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'fw all close'</span>, torch.allclose(out_py, out_cuda))</span><br><span class="line"></span><br><span class="line">    t = time.time()</span><br><span class="line">    <span class="comment"># CUDA反向传播</span></span><br><span class="line">    loss2 = out_cuda.<span class="built_in">sum</span>()</span><br><span class="line">    loss2.backward()</span><br><span class="line">    torch.cuda.synchronize()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'   cuda bw time'</span>, time.time()-t, <span class="string">'s'</span>)</span><br><span class="line"></span><br><span class="line">    t = time.time()</span><br><span class="line">    loss = out_py.<span class="built_in">sum</span>()</span><br><span class="line">    loss.backward()</span><br><span class="line">    torch.cuda.synchronize()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'pytorch bw time'</span>, time.time()-t, <span class="string">'s'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'bw all close'</span>, torch.allclose(feats.grad, feats2.grad))</span><br></pre></td></tr></tbody></table></figure><p>经过运行和计算后，我们会得到以下结果，我们可以看到，CUDA和Pytorch前向传播相差不大，但是对于反向传播的效率可以看得出来，结果大概差了10倍所有，CUDA的反向传播还是有一个较为明显的效率提升的。</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">   cuda fw time 0.0033109188079833984 s</span><br><span class="line">pytorch fw time 0.004142045974731445 s</span><br><span class="line">fw all close True</span><br><span class="line">   cuda bw time 0.004648447036743164 s</span><br><span class="line">pytorch bw time 0.04614758491516113 s</span><br><span class="line">bw all close True</span><br></pre></td></tr></tbody></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>视频资料： <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=l_Rpk6CRJYI&amp;list=PLDV2CyUo4q-LKuiNltBqCKdO9GH4SS_ec&amp;ab_channel=AI葵">https://www.youtube.com/watch?v=l_Rpk6CRJYI&amp;list=PLDV2CyUo4q-LKuiNltBqCKdO9GH4SS_ec&amp;ab_channel=AI%E8%91%B5</a></p><p>Github：<a target="_blank" rel="noopener" href="https://github.com/kwea123/pytorch-cppcuda-tutorial">https://github.com/kwea123/pytorch-cppcuda-tutorial</a></p><p>Pytorch官方资料：<a target="_blank" rel="noopener" href="https://pytorch.org/cppdocs/">https://pytorch.org/cppdocs/</a>，<a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/advanced/cpp_extension.html">https://pytorch.org/tutorials/advanced/cpp_extension.html</a></p><p>CUDA doc：<a target="_blank" rel="noopener" href="https://nyu-cds.github.io/python-gpu/02-cuda/">https://nyu-cds.github.io/python-gpu/02-cuda/</a></p></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io">Kedreamix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://kedreamix.github.io/2023/12/12/CUDA/Pytorch+cppcuda%20extension%20%E5%AD%A6%E4%B9%A0/">https://kedreamix.github.io/2023/12/12/CUDA/Pytorch+cppcuda extension 学习/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kedreamix.github.io" target="_blank">Adventures in Kedreamix' Digital World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/CUDA/">CUDA</a></div><div class="post_share"><div class="social-share" data-image="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-9a9b3c8658acbcad50b3234f818a6f6e.png" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-285d9459d746def9a847dd41da474e4c.png" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-full"><a href="/2023/12/12/CUDA/CUDA%E7%BC%96%E7%A8%8B%E5%AE%9E%E8%B7%B5%EF%BC%9ALLTM%E5%8A%A0%E9%80%9F%E4%BC%98%E5%8C%96%E5%AE%9E%E8%B7%B5/" title="CUDA编程实践：LLTM加速优化实践"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-5328b2df743543e0ca21d09eeaf45c28_1440w.jpeg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">CUDA编程实践：LLTM加速优化实践</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2023/12/12/CUDA/CUDA%E7%BC%96%E7%A8%8B%E5%AE%9E%E8%B7%B5%EF%BC%9ALLTM%E5%8A%A0%E9%80%9F%E4%BC%98%E5%8C%96%E5%AE%9E%E8%B7%B5/" title="CUDA编程实践：LLTM加速优化实践"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-5328b2df743543e0ca21d09eeaf45c28_1440w.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-12-12</div><div class="title">CUDA编程实践：LLTM加速优化实践</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E8%83%8C%E6%99%AF"><span class="toc-number">1.</span> <span class="toc-text">学习背景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%80%82%E7%94%A8%E5%AF%B9%E8%B1%A1%E4%B8%8E%E5%9C%BA%E6%99%AF"><span class="toc-number">2.</span> <span class="toc-text">适用对象与场景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Pytorch%E5%92%8CCUDA%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="toc-number">3.</span> <span class="toc-text">Pytorch和CUDA的关系</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Python%E8%B0%83%E7%94%A8C-%E5%87%BD%E6%95%B0%EF%BC%88%E6%A1%A5%E6%A2%81%EF%BC%89"><span class="toc-number">4.</span> <span class="toc-text">Python调用C++函数（桥梁）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Building-with-setuptools"><span class="toc-number">4.1.</span> <span class="toc-text">Building with setuptools</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#JIT-Compiling-Extensions"><span class="toc-number">4.2.</span> <span class="toc-text">JIT Compiling Extensions</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CUDA%E5%8A%A0%E9%80%9F%E7%9A%84%E5%8E%9F%E7%90%86"><span class="toc-number">5.</span> <span class="toc-text">CUDA加速的原理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E7%BA%BF%E6%80%A7%E6%8F%92%E5%80%BC%E9%97%AE%E9%A2%98%E5%AE%9A%E4%B9%89"><span class="toc-number">6.</span> <span class="toc-text">三线性插值问题定义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#C-%E8%B0%83%E7%94%A8CUDA%E5%87%BD%E6%95%B0"><span class="toc-number">7.</span> <span class="toc-text">C++调用CUDA函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E7%BA%BF%E6%80%A7%E6%8F%92%E5%80%BCCUDA%E5%AE%9E%E7%8E%B0"><span class="toc-number">8.</span> <span class="toc-text">三线性插值CUDA实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#scalar-t%E7%B1%BB%E5%9E%8B"><span class="toc-number">8.1.</span> <span class="toc-text">scalar_t类型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#accessors"><span class="toc-number">8.2.</span> <span class="toc-text">accessors</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E6%9D%BF%E5%87%BD%E6%95%B0"><span class="toc-number">8.3.</span> <span class="toc-text">模板函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#foward%E9%AA%8C%E8%AF%81%E4%B8%8E%E6%AF%94%E8%BE%83"><span class="toc-number">8.4.</span> <span class="toc-text">foward验证与比较</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CUDA%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">9.</span> <span class="toc-text">CUDA反向传播</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89CUDA%E5%87%BD%E6%95%B0"><span class="toc-number">9.1.</span> <span class="toc-text">定义CUDA函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%B8%E5%87%BD%E6%95%B0%E5%AE%9E%E7%8E%B0%E5%BE%AE%E5%88%86%E8%AE%A1%E7%AE%97"><span class="toc-number">9.2.</span> <span class="toc-text">核函数实现微分计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#PYBIND11%E7%BB%91%E5%AE%9A%E5%87%BD%E6%95%B0"><span class="toc-number">9.3.</span> <span class="toc-text">PYBIND11绑定函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#torch-autograd-Function%E5%B0%81%E8%A3%85"><span class="toc-number">9.4.</span> <span class="toc-text">torch.autograd.Function封装</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#backward%E9%AA%8C%E8%AF%81%E4%B8%8E%E6%AF%94%E8%BE%83"><span class="toc-number">9.5.</span> <span class="toc-text">backward验证与比较</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83"><span class="toc-number">10.</span> <span class="toc-text">参考</span></a></li></ol></div></div></div></div></main><footer id="footer" style="background-image:url('https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Kedreamix</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://kedreamix.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js").then((()=>{pangu.autoSpacingPage()}))}function panguInit(){panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"></div><script id="canvas_nest" defer color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div><script data-pjax>function butterfly_swiper_injector_config(){var a=document.getElementById("recent-posts");console.log("已挂载butterfly_swiper"),a.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/25/Paper/3DGS Survey/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-25</span><a class="blog-slider__title" href="2024/01/25/Paper/3DGS Survey/" alt="">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</a><div class="blog-slider__text">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</div><a class="blog-slider__button" href="2024/01/25/Paper/3DGS Survey/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/80/v2-dd72e374ab099a8115894f5247afb51f_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-12</span><a class="blog-slider__title" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">CUDA编程学习：自定义Pytorch+cpp/cuda extension</a><div class="blog-slider__text">虽然说PyTorch提供了丰富的与神经网络、张量代数、数据处理等相关的操作，但是有时候你可能需要**更定制化的操作**，比如使用论文中的新型激活函数，或者实现作为研究一部分开发的操作。在PyTorch中，最简单的集成自定义操作的方式是在Python中编写，通过扩展Function和Module来实现，</div><a class="blog-slider__button" href="2023/12/12/CUDA/Pytorch+cppcuda extension 学习/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker - GPT-SoVITS/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/20/Project/Linly-Talker/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-2ae8c11c1aae13ac400d5589124377b9_720w.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-20</span><a class="blog-slider__title" href="2024/01/20/Project/Linly-Talker/" alt="">数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</a><div class="blog-slider__text">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</div><a class="blog-slider__button" href="2024/01/20/Project/Linly-Talker/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/70/v2-de8deb9ec70dc554000936e71b7e907a.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-01-01</span><a class="blog-slider__title" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</a><div class="blog-slider__text">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</div><a class="blog-slider__button" href="2024/01/01/Paper/Awesome-Talking-Head-Synthesis/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/12/17/Project/ChatPaperFree/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-e127b4c88f3c1dae17604827851af750_720w.png?source=d16d100b" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023/12/17/Project/ChatPaperFree/" alt="">ChatPaperFree GeminiPro（一分钟读论文）</a><div class="blog-slider__text">ChatPaperFree是一个基于ChatGPT的自动论文摘要生成器，在ChatPaper的基础上进行的更新，采用了最近由Google开源的Gemini Pro大模型。目前,我们能够对用户输入的论文进行自动总结。未来,我还计划加入对论文图片/表格/公式的识别 extraction,从而生成更全面而易读的总结。</div><a class="blog-slider__button" href="2023/12/17/Project/ChatPaperFree/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script></body></html>