<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Adventures in Kedreamix&#39; Digital World</title>
  
  
  <link href="https://kedreamix.github.io/atom.xml" rel="self"/>
  
  <link href="https://kedreamix.github.io/"/>
  <updated>2024-04-22T09:43:13.959Z</updated>
  <id>https://kedreamix.github.io/</id>
  
  <author>
    <name>Kedreamix</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/04/22/Paper/2024-04-22/NeRF/"/>
    <id>https://kedreamix.github.io/2024/04/22/Paper/2024-04-22/NeRF/</id>
    <published>2024-04-22T09:43:13.000Z</published>
    <updated>2024-04-22T09:43:13.959Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-04-22-æ›´æ–°"><a href="#2024-04-22-æ›´æ–°" class="headerlink" title="2024-04-22 æ›´æ–°"></a>2024-04-22 æ›´æ–°</h1><h2 id="AG-NeRF-Attention-guided-Neural-Radiance-Fields-for-Multi-height-Large-scale-Outdoor-Scene-Rendering"><a href="#AG-NeRF-Attention-guided-Neural-Radiance-Fields-for-Multi-height-Large-scale-Outdoor-Scene-Rendering" class="headerlink" title="AG-NeRF: Attention-guided Neural Radiance Fields for Multi-height   Large-scale Outdoor Scene Rendering"></a>AG-NeRF: Attention-guided Neural Radiance Fields for Multi-height   Large-scale Outdoor Scene Rendering</h2><p><strong>Authors:Jingfeng Guo, Xiaohan Zhang, Baozhu Zhao, Qi Liu</strong></p><p>Existing neural radiance fields (NeRF)-based novel view synthesis methods for large-scale outdoor scenes are mainly built on a single altitude. Moreover, they often require a priori camera shooting height and scene scope, leading to inefficient and impractical applications when camera altitude changes. In this work, we propose an end-to-end framework, termed AG-NeRF, and seek to reduce the training cost of building good reconstructions by synthesizing free-viewpoint images based on varying altitudes of scenes. Specifically, to tackle the detail variation problem from low altitude (drone-level) to high altitude (satellite-level), a source image selection method and an attention-based feature fusion approach are developed to extract and fuse the most relevant features of target view from multi-height images for high-fidelity rendering. Extensive experiments demonstrate that AG-NeRF achieves SOTA performance on 56 Leonard and Transamerica benchmarks and only requires a half hour of training time to reach the competitive PSNR as compared to the latest BungeeNeRF. </p><p><a href="http://arxiv.org/abs/2404.11897v1">PDF</a> </p><p><strong>Summary</strong><br>é™ä½è®­ç»ƒæˆæœ¬ï¼Œå®ç°å¤šé«˜åº¦è‡ªç”±è§†è§’å›¾åƒåˆæˆã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>å¤šé«˜åº¦ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰èƒ½å¤Ÿåˆæˆè‡ªç”±è§†è§’å›¾åƒã€‚</li><li>æå‡ºå›¾åƒé€‰æ‹©æ–¹æ³•å’Œæ³¨æ„åŠ›ç‰¹å¾èåˆï¼Œè§£å†³ä¸åŒé«˜åº¦ç»†èŠ‚å·®å¼‚é—®é¢˜ã€‚</li><li>AG-NeRF åœ¨ 56 Leonard å’Œ Transamerica åŸºå‡†ä¸Šè¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ã€‚</li><li>AG-NeRF è®­ç»ƒæ—¶é—´ä»…éœ€åŠå°æ—¶ï¼Œå³å¯è¾¾åˆ° BungeeNeRF çš„ç«äº‰æ°´å¹³ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: AG-NeRF: å¤šé«˜åº¦å¤§å°ºåº¦æˆ·å¤–åœºæ™¯æ¸²æŸ“çš„æ³¨æ„åŠ›å¼•å¯¼ç¥ç»è¾å°„åœº</p></li><li><p>Authors: Jingfeng Guo, Xiaohan Zhang, Baozhu Zhao, Qi Liu</p></li><li><p>Affiliation: åå—ç†å·¥å¤§å­¦</p></li><li><p>Keywords: Novel View Synthesis, NeRF, Large-scale Outdoor Scene Rendering</p></li><li><p>Urls: https://arxiv.org/abs/2404.11897v1 , Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): ç°æœ‰çš„åŸºäºç¥ç»è¾å°„åœº (NeRF) çš„å¤§è§„æ¨¡æˆ·å¤–åœºæ™¯æ–°è§†è§’åˆæˆæ–¹æ³•ä¸»è¦å»ºç«‹åœ¨å•ä¸€é«˜åº¦ä¸Šã€‚æ­¤å¤–ï¼Œå®ƒä»¬é€šå¸¸éœ€è¦å…ˆéªŒçš„ç›¸æœºæ‹æ‘„é«˜åº¦å’Œåœºæ™¯èŒƒå›´ï¼Œå½“ç›¸æœºé«˜åº¦å‘ç”Ÿå˜åŒ–æ—¶ï¼Œä¼šå¯¼è‡´ä½æ•ˆä¸”ä¸å®ç”¨çš„åº”ç”¨ã€‚</p><p>(2): è¿‡å»çš„æ–¹æ³•ï¼š   - åœ°ç†ä¸Šå°†åœºæ™¯åˆ†è§£ä¸ºå‡ ä¸ªå•å…ƒæ ¼ï¼Œå¹¶ä¸ºæ¯ä¸ªå•å…ƒæ ¼è®­ç»ƒä¸€ä¸ªå­ NeRFï¼Œç„¶åå°†å®ƒä»¬åˆå¹¶ã€‚   - åœ¨ä½ç½®ç¼–ç ä¸­å¹¶è¡Œåº”ç”¨å¹³é¢å’Œç½‘æ ¼ç‰¹å¾ä»¥å®ç°é«˜æ•ˆå»ºæ¨¡ã€‚   - é—®é¢˜ï¼šå®ƒä»¬åœ¨åŸºç¡€é«˜åº¦ä¸Šé‡å»ºå¤§è§„æ¨¡åœºæ™¯ï¼Œå½“å¯¼èˆªåˆ°æ›´è¿‘çš„åœ°æ–¹ä»¥æ£€æŸ¥å¤§è§„æ¨¡æˆ·å¤–åœºæ™¯çš„å¾®è§‚ç»†èŠ‚æ—¶ï¼Œè¡¨ç°å‡ºè¿‡åº¦æ¨¡ç³Šçš„ä¼ªå½±å’Œä¸å®Œæ•´çš„é‡å»ºã€‚</p><p>(3): æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼š   - æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯æ¡†æ¶ AG-NeRFï¼Œé€šè¿‡åˆæˆåŸºäºåœºæ™¯ä¸åŒé«˜åº¦çš„è‡ªç”±è§†è§’å›¾åƒæ¥é™ä½æ„å»ºè‰¯å¥½é‡å»ºçš„è®­ç»ƒæˆæœ¬ã€‚   - å…·ä½“æ¥è¯´ï¼Œä¸ºäº†è§£å†³ä»ä½é«˜åº¦ï¼ˆæ— äººæœºçº§åˆ«ï¼‰åˆ°é«˜é«˜åº¦ï¼ˆå«æ˜Ÿçº§åˆ«ï¼‰çš„ç»†èŠ‚å˜åŒ–é—®é¢˜ï¼Œå¼€å‘äº†ä¸€ç§æºå›¾åƒé€‰æ‹©æ–¹æ³•å’Œä¸€ç§åŸºäºæ³¨æ„åŠ›çš„ç‰¹å¾èåˆæ–¹æ³•ï¼Œä»å¤šé«˜åº¦å›¾åƒä¸­æå–å’Œèåˆç›®æ ‡è§†å›¾æœ€ç›¸å…³çš„ç‰¹å¾ï¼Œä»¥å®ç°é«˜ä¿çœŸæ¸²æŸ“ã€‚</p><p>(4): æœ¬æ–‡æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼š   - åœ¨ 56 Leonard å’Œ Transamerica åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº† SOTA æ€§èƒ½ã€‚   - åªéœ€è¦åŠå°æ—¶çš„è®­ç»ƒæ—¶é—´å³å¯è¾¾åˆ°ä¸æœ€æ–° BungeeNeRF ç›¸å½“çš„ç«äº‰æ€§ PSNRã€‚   - æ€§èƒ½æ”¯æŒäº†ä»–ä»¬çš„ç›®æ ‡ï¼šé™ä½æ„å»ºè‰¯å¥½é‡å»ºçš„è®­ç»ƒæˆæœ¬ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯æ¡†æ¶ AG-NeRFï¼Œé€šè¿‡åˆæˆåŸºäºåœºæ™¯ä¸åŒé«˜åº¦çš„è‡ªç”±è§†è§’å›¾åƒæ¥é™ä½æ„å»ºè‰¯å¥½é‡å»ºçš„è®­ç»ƒæˆæœ¬ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šå¼€å‘äº†ä¸€ç§æºå›¾åƒé€‰æ‹©æ–¹æ³•å’Œä¸€ç§åŸºäºæ³¨æ„åŠ›çš„ç‰¹å¾èåˆæ–¹æ³•ï¼Œä»å¤šé«˜åº¦å›¾åƒä¸­æå–å’Œèåˆç›®æ ‡è§†å›¾æœ€ç›¸å…³çš„ç‰¹å¾ï¼Œä»¥å®ç°é«˜ä¿çœŸæ¸²æŸ“ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šåˆ©ç”¨å¯è®­ç»ƒçš„ U-Net ç½‘ç»œä»æºå›¾åƒä¸­æå–ç‰¹å¾å›¾ï¼Œå¹¶ä½¿ç”¨ Transformer å¯¹æå–çš„ç‰¹å¾å‘é‡è¿›è¡Œèåˆï¼Œä»¥æœ€å¤§åŒ–èåˆç‰¹å¾ä¸ç›®æ ‡åƒç´ ä¹‹é—´çš„ç›¸å…³æ€§ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šé‡‡ç”¨åˆ†å±‚é‡‡æ ·æ–¹æ³•ï¼Œä½¿ç”¨ç²—ç•¥ç½‘ç»œå’Œç²¾ç»†ç½‘ç»œåŒæ—¶ä¼˜åŒ–ï¼Œå¹¶ä½¿ç”¨åŸºäºæ³¨æ„åŠ›çš„ç‰¹å¾èåˆæ–¹æ³•å°†å¤šé«˜åº¦å›¾åƒä¸­çš„ç‰¹å¾èåˆèµ·æ¥ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡é’ˆå¯¹ä¸åŒé«˜åº¦æ‹æ‘„çš„å¤§åœºæ™¯æ¸²æŸ“æå‡ºäº†ç«¯åˆ°ç«¯çš„ AG-NeRF æ¡†æ¶ï¼Œé™ä½äº†æ„å»ºè‰¯å¥½é‡å»ºæ¨¡å‹çš„è®­ç»ƒæˆæœ¬ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§æºå›¾åƒé€‰æ‹©æ–¹æ³•å’ŒåŸºäºæ³¨æ„åŠ›çš„ç‰¹å¾èåˆæ–¹æ³•ï¼Œä»å¤šé«˜åº¦å›¾åƒä¸­æå–å’Œèåˆç›®æ ‡è§†å›¾æœ€ç›¸å…³çš„ç‰¹å¾ï¼Œä»¥å®ç°é«˜ä¿çœŸæ¸²æŸ“ã€‚æ€§èƒ½ï¼šåœ¨ 56 Leonard å’Œ Transamerica åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº† SOTA æ€§èƒ½ï¼Œåªéœ€è¦åŠå°æ—¶çš„è®­ç»ƒæ—¶é—´å³å¯è¾¾åˆ°ä¸æœ€æ–° BungeeNeRF ç›¸å½“çš„ç«äº‰æ€§ PSNRã€‚å·¥ä½œé‡ï¼šé‡‡ç”¨åˆ†å±‚é‡‡æ ·æ–¹æ³•ï¼Œä½¿ç”¨ç²—ç•¥ç½‘ç»œå’Œç²¾ç»†ç½‘ç»œåŒæ—¶ä¼˜åŒ–ï¼Œå¹¶ä½¿ç”¨åŸºäºæ³¨æ„åŠ›çš„ç‰¹å¾èåˆæ–¹æ³•å°†å¤šé«˜åº¦å›¾åƒä¸­çš„ç‰¹å¾èåˆèµ·æ¥ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-82fe2876dffe132719e410910e28492d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-fbedf0965ea4b6e30b80160a9ce71484.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b5a30ff8e4f41c8671a8c9f7dbcb45d2.jpg" align="middle"></details><h2 id="SLAIM-Robust-Dense-Neural-SLAM-for-Online-Tracking-and-Mapping"><a href="#SLAIM-Robust-Dense-Neural-SLAM-for-Online-Tracking-and-Mapping" class="headerlink" title="SLAIM: Robust Dense Neural SLAM for Online Tracking and Mapping"></a>SLAIM: Robust Dense Neural SLAM for Online Tracking and Mapping</h2><p><strong>Authors:Vincent Cartillier, Grant Schindler, Irfan Essa</strong></p><p>We present SLAIM - Simultaneous Localization and Implicit Mapping. We propose a novel coarse-to-fine tracking model tailored for Neural Radiance Field SLAM (NeRF-SLAM) to achieve state-of-the-art tracking performance. Notably, existing NeRF-SLAM systems consistently exhibit inferior tracking performance compared to traditional SLAM algorithms. NeRF-SLAM methods solve camera tracking via image alignment and photometric bundle-adjustment. Such optimization processes are difficult to optimize due to the narrow basin of attraction of the optimization loss in image space (local minima) and the lack of initial correspondences. We mitigate these limitations by implementing a Gaussian pyramid filter on top of NeRF, facilitating a coarse-to-fine tracking optimization strategy. Furthermore, NeRF systems encounter challenges in converging to the right geometry with limited input views. While prior approaches use a Signed-Distance Function (SDF)-based NeRF and directly supervise SDF values by approximating ground truth SDF through depth measurements, this often results in suboptimal geometry. In contrast, our method employs a volume density representation and introduces a novel KL regularizer on the ray termination distribution, constraining scene geometry to consist of empty space and opaque surfaces. Our solution implements both local and global bundle-adjustment to produce a robust (coarse-to-fine) and accurate (KL regularizer) SLAM solution. We conduct experiments on multiple datasets (ScanNet, TUM, Replica) showing state-of-the-art results in tracking and in reconstruction accuracy. </p><p><a href="http://arxiv.org/abs/2404.11419v1">PDF</a> </p><p><strong>Summary</strong><br>Nerf-SLAM é€šè¿‡é‡‡ç”¨ä»ç²—åˆ°ç»†çš„è·Ÿè¸ªæ¨¡å‹å’Œ KL æ­£åˆ™åŒ–å™¨ï¼Œåœ¨è·Ÿè¸ªæ€§èƒ½å’Œé‡å»ºç²¾åº¦ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æˆç»©ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>SLAIM æå‡ºäº†ä¸€ç§ä»ç²—åˆ°ç»†çš„è·Ÿè¸ªæ¨¡å‹ä»¥æé«˜ NeRF-SLAM çš„è·Ÿè¸ªæ€§èƒ½ã€‚</li><li>SLAIM é€šè¿‡é«˜æ–¯é‡‘å­—å¡”æ»¤æ³¢å™¨å®ç°ä»ç²—åˆ°ç»†çš„è·Ÿè¸ªä¼˜åŒ–ç­–ç•¥ã€‚</li><li>NeRF ç³»ç»Ÿéš¾ä»¥ä½¿ç”¨æœ‰é™çš„è¾“å…¥è§†å›¾æ”¶æ•›åˆ°æ­£ç¡®çš„å‡ ä½•å½¢çŠ¶ã€‚</li><li>SLAIM ä½¿ç”¨ä½“ç§¯å¯†åº¦è¡¨ç¤ºå’Œä¸€ä¸ªæ–°çš„ KL æ­£åˆ™åŒ–å™¨æ¥çº¦æŸåœºæ™¯å‡ ä½•å½¢çŠ¶ã€‚</li><li>SLAIM å®ç°å±€éƒ¨å’Œå…¨å±€æ†ç»‘è°ƒæ•´ä»¥æé«˜é²æ£’æ€§å’Œå‡†ç¡®æ€§ã€‚</li><li>SLAIM åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œåœ¨è·Ÿè¸ªå’Œé‡å»ºç²¾åº¦ä¸Šå‡æ˜¾ç¤ºå‡ºæœ€å…ˆè¿›çš„ç»“æœã€‚</li><li>SLAIM è§£å†³äº† NeRF-SLAM åœ¨ä¼ ç»Ÿ SLAM ç®—æ³•ä¸‹è¡¨ç°å‡ºè¾ƒå·®çš„è·Ÿè¸ªæ€§èƒ½è¿™ä¸€éš¾é¢˜ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>æ ‡é¢˜ï¼šSLAIMï¼šç”¨äºåœ¨çº¿è·Ÿè¸ªå’Œå»ºå›¾çš„é²æ£’ç¨ å¯†ç¥ç»SLAM</p></li><li><p>ä½œè€…ï¼šVincent Cartillierã€Grant Schindlerã€Irfan Essa</p></li><li><p>éš¶å±å…³ç³»ï¼šä½æ²»äºšç†å·¥å­¦é™¢</p></li><li><p>å…³é”®è¯ï¼šç¥ç»è¾å°„åœºã€SLAMã€ç¨ å¯†å»ºå›¾ã€è·Ÿè¸ª</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.11419ï¼ŒGithub ä»£ç é“¾æ¥ï¼šNone</p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šç¨ å¯†è§†è§‰SLAMæ˜¯3Dè®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€ä¸ªé•¿æœŸé—®é¢˜ï¼Œåœ¨è‡ªåŠ¨é©¾é©¶ã€å®¤å†…å¤–æœºå™¨äººå¯¼èˆªã€è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®ç­‰é¢†åŸŸæœ‰ç€å¹¿æ³›çš„åº”ç”¨ã€‚</p><p>ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠé—®é¢˜ï¼šä¼ ç»Ÿçš„SLAMç³»ç»Ÿé€šè¿‡ä¼°è®¡å›¾åƒå¯¹åº”å…³ç³»æ¥å¼€å§‹ï¼Œè¿™äº›å¯¹åº”å…³ç³»å¯èƒ½æ˜¯ç¨€ç–çš„ï¼Œä¾‹å¦‚åŒ¹é…çš„ç‰¹å¾ç‚¹ã€‚ç¥ç»è¾å°„åœºSLAMï¼ˆNeRF-SLAMï¼‰æ–¹æ³•é€šè¿‡å›¾åƒå¯¹é½å’Œå…‰åº¦æ†ç»‘è°ƒæ•´æ¥è§£å†³ç›¸æœºè·Ÿè¸ªé—®é¢˜ã€‚ç”±äºå›¾åƒç©ºé—´ä¸­ä¼˜åŒ–æŸå¤±çš„å¸å¼•åŸŸçª„ï¼ˆå±€éƒ¨æå°å€¼ï¼‰ä»¥åŠç¼ºä¹åˆå§‹å¯¹åº”å…³ç³»ï¼Œæ­¤ç±»ä¼˜åŒ–è¿‡ç¨‹éš¾ä»¥ä¼˜åŒ–ã€‚</p><p>ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç²—åˆ°ç»†è·Ÿè¸ªæ¨¡å‹ï¼Œä¸“é—¨é’ˆå¯¹NeRF-SLAMï¼Œä»¥å®ç°æœ€å…ˆè¿›çš„è·Ÿè¸ªæ€§èƒ½ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¼•å…¥äº†ä¸€ç§æ–°çš„ç›®æ ‡å°„çº¿ç»ˆæ­¢åˆ†å¸ƒï¼Œå¹¶å°†å…¶ç”¨äºKLæ­£åˆ™åŒ–å™¨ä¸­ï¼Œä»¥çº¦æŸåœºæ™¯å‡ ä½•ç”±ç©ºç©ºé—´å’Œä¸é€æ˜è¡¨é¢ç»„æˆã€‚</p><p>ï¼ˆ4ï¼‰ä»»åŠ¡å’Œæ€§èƒ½ï¼šæœ¬æ–‡æ–¹æ³•åœ¨ScanNetã€TUMã€Replicaç­‰å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œåœ¨è·Ÿè¸ªå’Œé‡å»ºç²¾åº¦æ–¹é¢å‡å–å¾—äº†æœ€å…ˆè¿›çš„æˆæœã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ã€‚</p><ol><li><p>æ–¹æ³•ï¼š</p><pre><code>            (1):SLAIM æ˜¯ä¸€ç§ç”¨äºç¨ å¯†æ˜ å°„å’Œè·Ÿè¸ªçš„ RGB-D è¾“å…¥æµçš„ novel æ–¹æ³•ï¼›            (2):SLAIM é‡‡ç”¨äº†ä¸€ç§ä»ç²—åˆ°ç²¾çš„è·Ÿè¸ªæ¨¡å‹ï¼Œä»¥å®ç°æœ€å…ˆè¿›çš„è·Ÿè¸ªæ€§èƒ½ï¼›            (3):SLAIM å¼•å…¥äº†ä¸€ç§æ–°çš„ç›®æ ‡å°„çº¿ç»ˆæ­¢åˆ†å¸ƒï¼Œå¹¶å°†å…¶ç”¨äº KL æ­£åˆ™åŒ–å™¨ä¸­ï¼Œä»¥çº¦æŸåœºæ™¯å‡ ä½•ç”±ç©ºç©ºé—´å’Œä¸é€æ˜è¡¨é¢ç»„æˆï¼›            (4):SLAIM åœ¨ ScanNetã€TUMã€Replica ç­‰å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œåœ¨è·Ÿè¸ªå’Œé‡å»ºç²¾åº¦æ–¹é¢å‡å–å¾—äº†æœ€å…ˆè¿›çš„æˆæœã€‚</code></pre></li><li><p>ç»“è®ºï¼š</p></li></ol><p>ï¼ˆ1ï¼‰æœ¬æ–‡çš„å·¥ä½œæ„ä¹‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æœ€å…ˆè¿›çš„ç¨ å¯†å®æ—¶ RGB-D NeRF-SLAM ç³»ç»Ÿ SLAIMï¼Œè¯¥ç³»ç»Ÿå…·æœ‰æœ€å…ˆè¿›çš„ç›¸æœºè·Ÿè¸ªå’Œå»ºå›¾èƒ½åŠ›ã€‚</p><p>ï¼ˆ2ï¼‰æœ¬æ–‡çš„ä¼˜ç¼ºç‚¹æ€»ç»“ï¼š    - åˆ›æ–°ç‚¹ï¼š        - é‡‡ç”¨ä»ç²—åˆ°ç²¾çš„è·Ÿè¸ªæ¨¡å‹ï¼Œå®ç°æœ€å…ˆè¿›çš„è·Ÿè¸ªæ€§èƒ½ã€‚        - å¼•å…¥æ–°çš„ç›®æ ‡å°„çº¿ç»ˆæ­¢åˆ†å¸ƒï¼Œå¹¶å°†å…¶ç”¨äº KL æ­£åˆ™åŒ–å™¨ä¸­ï¼Œä»¥çº¦æŸåœºæ™¯å‡ ä½•ç”±ç©ºç©ºé—´å’Œä¸é€æ˜è¡¨é¢ç»„æˆã€‚    - æ€§èƒ½ï¼š        - åœ¨ ScanNetã€TUMã€Replica ç­‰å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„è·Ÿè¸ªå’Œé‡å»ºç²¾åº¦ã€‚    - å·¥ä½œé‡ï¼š        - å†…å­˜æ•ˆç‡é«˜ï¼Œåœ¨ Replica å’Œ ScanNet æ•°æ®é›†ä¸Šä¸åŸºå‡†ç›¸æ¯”ï¼Œè·Ÿè¸ªå’Œå»ºå›¾æ—¶é—´å‡æœ‰æ˜æ˜¾é™ä½ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-486ca0b76c4db89899a0670269d00796.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f729a5308a9aa1435c3a0e2db312184f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ddcd1f27f832c7cfc1c274567204de22.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c7d35d3daa3f9540491cf1d974f07bc9.jpg" align="middle"></details><h2 id="RainyScape-Unsupervised-Rainy-Scene-Reconstruction-using-Decoupled-Neural-Rendering"><a href="#RainyScape-Unsupervised-Rainy-Scene-Reconstruction-using-Decoupled-Neural-Rendering" class="headerlink" title="RainyScape: Unsupervised Rainy Scene Reconstruction using Decoupled   Neural Rendering"></a>RainyScape: Unsupervised Rainy Scene Reconstruction using Decoupled   Neural Rendering</h2><p><strong>Authors:Xianqiang Lyu, Hui Liu, Junhui Hou</strong></p><p>We propose RainyScape, an unsupervised framework for reconstructing clean scenes from a collection of multi-view rainy images. RainyScape consists of two main modules: a neural rendering module and a rain-prediction module that incorporates a predictor network and a learnable latent embedding that captures the rain characteristics of the scene. Specifically, based on the spectral bias property of neural networks, we first optimize the neural rendering pipeline to obtain a low-frequency scene representation. Subsequently, we jointly optimize the two modules, driven by the proposed adaptive direction-sensitive gradient-based reconstruction loss, which encourages the network to distinguish between scene details and rain streaks, facilitating the propagation of gradients to the relevant components. Extensive experiments on both the classic neural radiance field and the recently proposed 3D Gaussian splatting demonstrate the superiority of our method in effectively eliminating rain streaks and rendering clean images, achieving state-of-the-art performance. The constructed high-quality dataset and source code will be publicly available. </p><p><a href="http://arxiv.org/abs/2404.11401v1">PDF</a> </p><p><strong>Summary</strong><br>åŸºäºç¥ç»ç½‘ç»œçš„å…‰è°±åå·®ç‰¹æ€§ï¼ŒRainyScapeåˆ©ç”¨æ— ç›‘ç£æ¡†æ¶é‡å»ºå¹²å‡€åœºæ™¯ï¼ŒåŒ…å«ç¥ç»æ¸²æŸ“æ¨¡å—å’Œé›¨æ»´é¢„æµ‹æ¨¡å—ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>åˆ©ç”¨ç¥ç»ç½‘ç»œçš„å…‰è°±åå·®ç‰¹æ€§è·å¾—ä½é¢‘åœºæ™¯è¡¨ç¤ºã€‚</li><li>è”åˆä¼˜åŒ–ç¥ç»æ¸²æŸ“æ¨¡å—å’Œé›¨æ»´é¢„æµ‹æ¨¡å—ï¼Œä»¥åŒºåˆ†åœºæ™¯ç»†èŠ‚å’Œé›¨æ»´æ¡çº¹ã€‚</li><li>æå‡ºè‡ªé€‚åº”æ–¹å‘æ•æ„Ÿæ¢¯åº¦é‡å»ºæŸå¤±ï¼Œå¼•å¯¼ç½‘ç»œåŒºåˆ†åœºæ™¯ç»†èŠ‚å’Œé›¨æ»´æ¡çº¹ã€‚</li><li>åœ¨ç»å…¸ç¥ç»è¾å°„åœºå’Œ 3D é«˜æ–¯æ–‘ç‚¹ splatting æ•°æ®é›†ä¸Šå‡è¾¾åˆ°æœ€å…ˆè¿›çš„å»é›¨æ€§èƒ½ã€‚</li><li>æä¾›é«˜è´¨é‡æ•°æ®é›†å’Œæºä»£ç ï¼Œä¿ƒè¿›ç ”ç©¶å·¥ä½œã€‚</li><li>å¼•å…¥å¯å­¦ä¹ æ½œåœ¨åµŒå…¥ï¼Œæ•æ‰åœºæ™¯çš„é›¨æ»´ç‰¹å¾ã€‚</li><li>é€šè¿‡é›¨æ»´é¢„æµ‹ç½‘ç»œæœ‰æ•ˆæ¶ˆé™¤é›¨æ»´æ¡çº¹ï¼Œæ¸²æŸ“å¹²å‡€å›¾åƒã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: RainyScape: æ— ç›‘ç£é›¨æ™¯é‡å»ºä½¿ç”¨è§£è€¦ç¥ç»æ¸²æŸ“</p></li><li><p>Authors: Xianqiang Lyu, Hui Liu, Junhui Hou</p></li><li><p>Affiliation: é¦™æ¸¯åŸå¸‚å¤§å­¦è®¡ç®—æœºç§‘å­¦ç³»</p></li><li><p>Keywords: Rainy scene reconstruction, Neural rendering, Unsupervised loss</p></li><li><p>Urls: https://arxiv.org/abs/2404.11401 , Github:None</p></li><li><p>Summary:</p></li></ol><p>(1):éšç€ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åœ¨å›¾åƒåˆæˆä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œå½“è¾“å…¥å›¾åƒå—åˆ°æ¨¡ç³Šã€å™ªå£°æˆ–é›¨æ°´ç­‰å› ç´ å½±å“æ—¶ï¼Œæ¸²æŸ“ç»“æœä¸å¯é¿å…åœ°ä¼šäº§ç”Ÿæ˜æ˜¾çš„ä¼ªå½±ã€‚</p><p>(2):ç°æœ‰çš„æ–¹æ³•é’ˆå¯¹ç‰¹å®šä»»åŠ¡æå‡ºäº†å„ç§è§£å†³æ–¹æ¡ˆï¼Œä½†å¯¹äºé›¨æ™¯é‡å»ºä»»åŠ¡ï¼Œå®ƒä»¬æ— æ³•æœ‰æ•ˆè¡¨ç¤ºä¸‰ç»´ç©ºé—´ä¸­ç¨€ç–ä¸”é—´æ­‡æ€§çš„é™é›¨ã€‚</p><p>(3):æœ¬æ–‡æå‡ºRainyScapeï¼Œä¸€ä¸ªè§£è€¦çš„ç¥ç»æ¸²æŸ“æ¡†æ¶ï¼Œå®ƒèƒ½å¤Ÿä»¥æ— ç›‘ç£çš„æ–¹å¼ä»é›¨æ™¯å›¾åƒä¸­é‡å»ºæ— é›¨åœºæ™¯ã€‚è¯¥æ¡†æ¶é€šè¿‡ç¥ç»æ¸²æŸ“ç®¡é“è·å¾—åœºæ™¯çš„ä½é¢‘è¡¨ç¤ºï¼Œå¹¶ä½¿ç”¨å¯å­¦ä¹ çš„é›¨æ°´åµŒå…¥å’Œé¢„æµ‹å™¨æ¥è¡¨å¾é›¨æ°´ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ä¸ªè‡ªé€‚åº”è§’åº¦ä¼°è®¡ç­–ç•¥å’Œæ¢¯åº¦æ—‹è½¬æŸå¤±ï¼Œä»¥è§£è€¦åœºæ™¯é«˜é¢‘ç»†èŠ‚å’Œé›¨æ°´æ¡çº¹ã€‚</p><p>(4):åœ¨ç¥ç»è¾å°„åœºå’Œä¸‰ç»´é«˜æ–¯æ•£å°„ä¸¤ç§æ¸²æŸ“æŠ€æœ¯ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æœ‰æ•ˆæ¶ˆé™¤é›¨æ°´æ¡çº¹å’Œæ¸²æŸ“æ¸…æ™°å›¾åƒæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p><ol><li><p>æ–¹æ³•ï¼š</p><p>ï¼ˆ1ï¼‰ï¼šæå‡ºRainyScapeï¼Œä¸€ä¸ªè§£è€¦çš„ç¥ç»æ¸²æŸ“æ¡†æ¶ï¼Œå¯ä»¥æ— ç›‘ç£åœ°ä»é›¨æ™¯å›¾åƒä¸­é‡å»ºæ— é›¨åœºæ™¯ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šé€šè¿‡ç¥ç»æ¸²æŸ“ç®¡é“è·å¾—åœºæ™¯çš„ä½é¢‘è¡¨ç¤ºï¼Œå¹¶ä½¿ç”¨å¯å­¦ä¹ çš„é›¨æ°´åµŒå…¥å’Œé¢„æµ‹å™¨æ¥è¡¨å¾é›¨æ°´ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šæå‡ºä¸€ä¸ªè‡ªé€‚åº”è§’åº¦ä¼°è®¡ç­–ç•¥å’Œæ¢¯åº¦æ—‹è½¬æŸå¤±ï¼Œä»¥è§£è€¦åœºæ™¯é«˜é¢‘ç»†èŠ‚å’Œé›¨æ°´æ¡çº¹ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šåœ¨ç¥ç»è¾å°„åœºå’Œä¸‰ç»´é«˜æ–¯æ•£å°„ä¸¤ç§æ¸²æŸ“æŠ€æœ¯ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æœ‰æ•ˆæ¶ˆé™¤é›¨æ°´æ¡çº¹å’Œæ¸²æŸ“æ¸…æ™°å›¾åƒæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p></li><li><p>ç»“è®ºï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šRainyScapeçš„æ„ä¹‰åœ¨äºï¼Œå®ƒæå‡ºäº†ä¸€ç§æ— ç›‘ç£çš„è§£è€¦ç¥ç»æ¸²æŸ“æ¡†æ¶ï¼Œå¯ä»¥ä»é›¨æ™¯å›¾åƒä¸­é‡å»ºæ— é›¨åœºæ™¯ï¼Œæœ‰æ•ˆè§£å†³äº†é›¨æ™¯é‡å»ºä¸­çš„é›¨æ°´æ¡çº¹å»é™¤é—®é¢˜ï¼Œä¸ºé›¨æ™¯å›¾åƒå¤„ç†æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p><ul><li><p>æå‡ºäº†ä¸€ç§è§£è€¦çš„ç¥ç»æ¸²æŸ“æ¡†æ¶ï¼Œé€šè¿‡ä½é¢‘åœºæ™¯è¡¨ç¤ºã€å¯å­¦ä¹ çš„é›¨æ°´åµŒå…¥å’Œé¢„æµ‹å™¨ä»¥åŠè‡ªé€‚åº”è§’åº¦ä¼°è®¡ç­–ç•¥å’Œæ¢¯åº¦æ—‹è½¬æŸå¤±ï¼Œæœ‰æ•ˆè§£è€¦äº†åœºæ™¯é«˜é¢‘ç»†èŠ‚å’Œé›¨æ°´æ¡çº¹ã€‚</p></li><li><p>æ€§èƒ½ï¼š</p></li><li><p>åœ¨ç¥ç»è¾å°„åœºå’Œä¸‰ç»´é«˜æ–¯æ•£å°„ä¸¤ç§æ¸²æŸ“æŠ€æœ¯ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æœ‰æ•ˆæ¶ˆé™¤é›¨æ°´æ¡çº¹å’Œæ¸²æŸ“æ¸…æ™°å›¾åƒæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p></li><li><p>å·¥ä½œé‡ï¼š</p></li><li><p>è¯¥æ–¹æ³•éœ€è¦å¯¹é›¨æ™¯å›¾åƒè¿›è¡Œé¢„å¤„ç†ï¼ŒåŒ…æ‹¬å›¾åƒåˆ†å‰²ã€é›¨æ°´æ¡çº¹æ£€æµ‹å’Œé›¨æ°´åµŒå…¥æå–ç­‰æ­¥éª¤ï¼Œå¢åŠ äº†è®¡ç®—é‡å’Œæ—¶é—´å¼€é”€ã€‚</p></li></ul><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-789763f7ebb6ec7a923539611ab1fe24.jpg" align="middle"><img src="https://picx.zhimg.com/v2-89f176b1378008d1c0b63c9241adfdb2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5f7fb8305c36c1fe2572adfd98b584f7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-76be36036e15658d754b57c4864b0abf.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3765b699865b1d89cc9f5f13f9843a0e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-34d10a80ece07ba92081dfc066d00427.jpg" align="middle"></details>## REACTO: Reconstructing Articulated Objects from a Single Video**Authors:Chaoyue Song, Jiacheng Wei, Chuan-Sheng Foo, Guosheng Lin, Fayao Liu**In this paper, we address the challenge of reconstructing general articulated 3D objects from a single video. Existing works employing dynamic neural radiance fields have advanced the modeling of articulated objects like humans and animals from videos, but face challenges with piece-wise rigid general articulated objects due to limitations in their deformation models. To tackle this, we propose Quasi-Rigid Blend Skinning, a novel deformation model that enhances the rigidity of each part while maintaining flexible deformation of the joints. Our primary insight combines three distinct approaches: 1) an enhanced bone rigging system for improved component modeling, 2) the use of quasi-sparse skinning weights to boost part rigidity and reconstruction fidelity, and 3) the application of geodesic point assignment for precise motion and seamless deformation. Our method outperforms previous works in producing higher-fidelity 3D reconstructions of general articulated objects, as demonstrated on both real and synthetic datasets. Project page: https://chaoyuesong.github.io/REACTO. [PDF](http://arxiv.org/abs/2404.11151v1) **Summary**å¯¹äºä¸€èˆ¬æ€§å…³èŠ‚åŠ¨ä½œçš„3Dç‰©ä½“ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å˜å½¢æ¨¡å‹ï¼Œå³å‡†åˆšæ€§æ··åˆè’™çš®ï¼Œä»¥ä¾¿ä»å•ä¸ªè§†é¢‘ä¸­è¿›è¡Œå…¨é¢é‡å»ºã€‚**Key Takeaways**- æå‡ºä¸€ç§æ–°çš„å˜å½¢æ¨¡å‹ï¼Œå‡†åˆšæ€§æ··åˆè’™çš®ï¼Œå¢å¼ºäº†é›¶ä»¶åˆšæ€§ï¼ŒåŒæ—¶ä¿æŒå…³èŠ‚æŸ”æ€§å˜å½¢ã€‚- é‡‡ç”¨å¢å¼ºéª¨éª¼ç»‘å®šç³»ç»Ÿæ”¹å–„ç»„ä»¶å»ºæ¨¡ã€‚- ä½¿ç”¨å‡†ç¨€ç–è’™çš®æƒé‡æé«˜é›¶ä»¶åˆšæ€§å’Œé‡å»ºä¿çœŸåº¦ã€‚- åº”ç”¨æµ‹åœ°çº¿ç‚¹èµ‹å€¼å®ç°ç²¾ç¡®è¿åŠ¨å’Œæ— ç¼å˜å½¢ã€‚- åœ¨çœŸå®å’Œåˆæˆæ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨ç”Ÿæˆé«˜ä¿çœŸä¸€èˆ¬æ€§å…³èŠ‚åŠ¨ä½œçš„3Dé‡å»ºæ–¹é¢ä¼˜äºå…ˆå‰çš„å·¥ä½œã€‚- è¯¥ç ”ç©¶ä¸ºä¸€èˆ¬æ€§å…³èŠ‚åŠ¨ä½œçš„3Dç‰©ä½“é‡å»ºæä¾›äº†æ–°çš„æ–¹æ³•ã€‚- è¯¥ç ”ç©¶åœ¨è®¡ç®—æœºè§†è§‰å’Œå›¾å½¢å­¦é¢†åŸŸå…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ã€‚- è¯¥ç ”ç©¶æœ‰åŠ©äºæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„å‘å±•ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>æ ‡é¢˜ï¼šREACTOï¼šä»å•ä¸€è§†é¢‘ä¸­é‡å»ºé“°æ¥ç‰©ä½“</p></li><li><p>ä½œè€…ï¼šChaoyue Songã€Jiacheng Weiã€Chuan Sheng Fooã€Guosheng Linã€Fayao Liu</p></li><li><p>éš¶å±ï¼šå—æ´‹ç†å·¥å¤§å­¦</p></li><li><p>å…³é”®è¯ï¼šé“°æ¥ç‰©ä½“é‡å»ºã€åŠ¨æ€ç¥ç»è¾å°„åœºã€å‡†åˆšæ€§æ··åˆè’™çš®</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.11151, Githubï¼šæ— </p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šé‡å»ºé“°æ¥ç‰©ä½“æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€é¡¹é‡è¦ä»»åŠ¡ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å…·æœ‰åˆ†æ®µåˆšæ€§çš„é€šç”¨é“°æ¥ç‰©ä½“æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¿‡å»æ–¹æ³•ï¼šNASAMå’ŒPARISç­‰æ–¹æ³•éœ€è¦å¤šè§†è§’å›¾åƒæˆ–å¤šè§†å›¾å›¾åƒï¼Œåœ¨å®é™…åº”ç”¨ä¸­å—é™ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§å‡†åˆšæ€§æ··åˆè’™çš®å˜å½¢æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡å¢å¼ºéª¨éª¼è£…é…ç³»ç»Ÿã€ä½¿ç”¨å‡†ç¨€ç–è’™çš®æƒé‡å’Œåº”ç”¨æµ‹åœ°çº¿ç‚¹åˆ†é…æ¥æé«˜åˆšæ€§å¹¶ä¿æŒå…³èŠ‚çš„çµæ´»å˜å½¢ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šä»»åŠ¡ä¸æ€§èƒ½ï¼šREACTOåœ¨çœŸå®å’Œåˆæˆæ•°æ®é›†ä¸Šå¯¹é€šç”¨é“°æ¥ç‰©ä½“çš„3Dé‡å»ºä»»åŠ¡ä¸­å–å¾—äº†è¾ƒé«˜çš„ä¿çœŸåº¦ï¼Œè¯æ˜äº†å…¶æ€§èƒ½å¯ä»¥æ”¯æŒå…¶ç›®æ ‡ã€‚</p><p><strong>7. Methodsï¼š</strong></p><p>(1)ï¼šæå‡ºå‡†åˆšæ€§æ··åˆè’™çš®å˜å½¢æ¨¡å‹ï¼Œå¢å¼ºéª¨éª¼è£…é…ç³»ç»Ÿï¼Œä½¿ç”¨å‡†ç¨€ç–è’™çš®æƒé‡ï¼Œå¹¶åº”ç”¨æµ‹åœ°çº¿ç‚¹åˆ†é…ï¼›</p><p>(2)ï¼šæ„å»ºREACTOæ¡†æ¶ï¼ŒåŒ…æ‹¬éª¨éª¼è£…é…ã€è’™çš®å˜å½¢ã€ä½“ç»˜åˆ¶å’Œæ¸²æŸ“æ¨¡å—ï¼›</p><p>(3)ï¼šä½¿ç”¨åŸºäºç¥ç»è¾å°„åœºçš„æ¸²æŸ“å™¨ï¼Œä»å•ä¸€è§†é¢‘ä¸­é‡å»ºé“°æ¥ç‰©ä½“ï¼›</p><p>(4)ï¼šé€šè¿‡ä¼˜åŒ–éª¨éª¼å‚æ•°ã€è’™çš®æƒé‡å’Œç¥ç»è¾å°„åœºå‚æ•°ï¼Œå®ç°é“°æ¥ç‰©ä½“çš„é«˜ä¿çœŸé‡å»ºï¼›</p><p>(5)ï¼šåœ¨çœŸå®å’Œåˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒï¼ŒéªŒè¯REACTOçš„æœ‰æ•ˆæ€§ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œæå‡ºREACTOï¼Œä¸€ç§ä»å•ä¸€è§†é¢‘ä¸­é‡å»ºé€šç”¨é“°æ¥3Dç‰©ä½“çš„å¼€åˆ›æ€§æ–¹æ³•ï¼Œé€šè¿‡é‡æ–°å®šä¹‰è£…é…ç»“æ„å¹¶é‡‡ç”¨å‡†åˆšæ€§æ··åˆè’™çš®ï¼Œå®ç°äº†å»ºæ¨¡å’Œç²¾åº¦çš„æå‡ã€‚å‡†åˆšæ€§æ··åˆè’™çš®é€šè¿‡åˆ©ç”¨å‡†ç¨€ç–è’™çš®æƒé‡å’Œæµ‹åœ°çº¿ç‚¹åˆ†é…ï¼Œç¡®ä¿äº†æ¯ä¸ªéƒ¨ä»¶çš„åˆšæ€§ï¼ŒåŒæ—¶åœ¨å…³èŠ‚å¤„ä¿æŒå¹³æ»‘å˜å½¢ã€‚å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼ŒREACTOåœ¨çœŸå®å’Œåˆæˆæ•°æ®é›†ä¸Šéƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¿çœŸåº¦å’Œç»†èŠ‚æ–¹é¢éƒ½æœ‰æ‰€æå‡ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºå‡†åˆšæ€§æ··åˆè’™çš®å˜å½¢æ¨¡å‹ï¼Œå¢å¼ºéª¨éª¼è£…é…ç³»ç»Ÿï¼Œä½¿ç”¨å‡†ç¨€ç–è’™çš®æƒé‡ï¼Œå¹¶åº”ç”¨æµ‹åœ°çº¿ç‚¹åˆ†é…ï¼›</p><p>æ€§èƒ½ï¼šåœ¨çœŸå®å’Œåˆæˆæ•°æ®é›†ä¸Šï¼ŒREACTOåœ¨ä¿çœŸåº¦å’Œç»†èŠ‚æ–¹é¢éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼›</p><p>å·¥ä½œé‡ï¼šä¸éœ€è¦å¤šè§†è§’æˆ–å¤šè§†å›¾å›¾åƒçš„ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒREACTOåªéœ€å•ä¸€è§†é¢‘å³å¯é‡å»ºé“°æ¥ç‰©ä½“ï¼Œå·¥ä½œé‡æ›´å°ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-b24d1992bf52c35d5d68092f3855e178.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-cc1782e8c3f880dfa4512201f4175379.jpg" align="middle"><img src="https://picx.zhimg.com/v2-46959553add30d1e8d2dff8cb9e56563.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f4000a7f506812312f58f8dd21486b3b.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-04-22  AG-NeRF Attention-guided Neural Radiance Fields for Multi-height   Large-scale Outdoor Scene Rendering</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/04/22/Paper/2024-04-22/3DGS/"/>
    <id>https://kedreamix.github.io/2024/04/22/Paper/2024-04-22/3DGS/</id>
    <published>2024-04-22T09:32:29.000Z</published>
    <updated>2024-04-22T09:32:29.438Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-04-22-æ›´æ–°"><a href="#2024-04-22-æ›´æ–°" class="headerlink" title="2024-04-22 æ›´æ–°"></a>2024-04-22 æ›´æ–°</h1><h2 id="Learn2Talk-3D-Talking-Face-Learns-from-2D-Talking-Face"><a href="#Learn2Talk-3D-Talking-Face-Learns-from-2D-Talking-Face" class="headerlink" title="Learn2Talk: 3D Talking Face Learns from 2D Talking Face"></a>Learn2Talk: 3D Talking Face Learns from 2D Talking Face</h2><p><strong>Authors:Yixiang Zhuang, Baoping Cheng, Yao Cheng, Yuntao Jin, Renshuai Liu, Chengyang Li, Xuan Cheng, Jing Liao, Juncong Lin</strong></p><p>Speech-driven facial animation methods usually contain two main classes, 3D and 2D talking face, both of which attract considerable research attention in recent years. However, to the best of our knowledge, the research on 3D talking face does not go deeper as 2D talking face, in the aspect of lip-synchronization (lip-sync) and speech perception. To mind the gap between the two sub-fields, we propose a learning framework named Learn2Talk, which can construct a better 3D talking face network by exploiting two expertise points from the field of 2D talking face. Firstly, inspired by the audio-video sync network, a 3D sync-lip expert model is devised for the pursuit of lip-sync between audio and 3D facial motion. Secondly, a teacher model selected from 2D talking face methods is used to guide the training of the audio-to-3D motions regression network to yield more 3D vertex accuracy. Extensive experiments show the advantages of the proposed framework in terms of lip-sync, vertex accuracy and speech perception, compared with state-of-the-arts. Finally, we show two applications of the proposed framework: audio-visual speech recognition and speech-driven 3D Gaussian Splatting based avatar animation. </p><p><a href="http://arxiv.org/abs/2404.12888v1">PDF</a> </p><p><strong>Summary</strong><br>é€šè¿‡å€Ÿé‰´2Dè¯´è¯äººé¢éƒ¨çš„å”‡å½¢åŒæ­¥å’Œè¨€è¯­æ„ŸçŸ¥é¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†ï¼Œæå‡ºäº†ä¸€ç§å­¦ä¹ æ¡†æ¶ï¼Œå¯ä»¥æ„å»ºæ›´å¥½çš„3Dè¯´è¯äººé¢éƒ¨ç½‘ç»œã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>3Dè¯´è¯äººé¢éƒ¨ç ”ç©¶åœ¨å”‡å½¢åŒæ­¥å’Œè¨€è¯­æ„ŸçŸ¥æ–¹é¢ä¸å¦‚2Dè¯´è¯äººé¢éƒ¨ç ”ç©¶æ·±å…¥ã€‚</li><li>Learn2Talkæ¡†æ¶åˆ©ç”¨2Dè¯´è¯äººé¢éƒ¨é¢†åŸŸçš„ä¸¤ä¸ªä¸“ä¸šçŸ¥è¯†ç‚¹æ¥æ„å»ºæ›´å¥½çš„3Dè¯´è¯äººé¢éƒ¨ç½‘ç»œã€‚</li><li>3DåŒæ­¥å”‡ä¸“å®¶æ¨¡å‹æ—¨åœ¨å®ç°éŸ³é¢‘å’Œ3Dé¢éƒ¨è¿åŠ¨ä¹‹é—´çš„å”‡å½¢åŒæ­¥ã€‚</li><li>2Dè¯´è¯äººé¢éƒ¨æ–¹æ³•ä¸­é€‰æ‹©çš„æ•™å¸ˆæ¨¡å‹ç”¨äºæŒ‡å¯¼éŸ³é¢‘åˆ°3Dè¿åŠ¨å›å½’ç½‘ç»œçš„è®­ç»ƒï¼Œä»¥æé«˜3Dé¡¶ç‚¹ç²¾åº¦ã€‚</li><li>å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å”‡å½¢åŒæ­¥ã€é¡¶ç‚¹ç²¾åº¦å’Œè¨€è¯­æ„ŸçŸ¥æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li><li>è¯¥æ¡†æ¶æœ‰è¯­éŸ³-è§†è§‰è¯­éŸ³è¯†åˆ«å’Œè¯­éŸ³é©±åŠ¨3Dé«˜æ–¯é£æº…åŸºäºå¤´åƒåŠ¨ç”»ä¸¤ä¸ªåº”ç”¨ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>æ ‡é¢˜ï¼šLearn2Talkï¼š3D è¯´è¯äººè„¸ä» 2D è¯´è¯äººè„¸å­¦ä¹ </p></li><li><p>ä½œè€…ï¼šYixiang Zhuang, Baoping Cheng, Yao Cheng, Yuntao Jin, Renshuai Liu, Chengyang Li, XuanCheng, Jing Liao, Juncong Lin</p></li><li><p>å•ä½ï¼šæš‚ç¼º</p></li><li><p>å…³é”®è¯ï¼šSpeech-driven, 3D Facial Animation, 2D Talking face, Transformer, 3D Gaussian Splatting</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.12888v1Githubï¼šNone</p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šè¯´è¯äººè„¸åŠ¨ç”»æ–¹æ³•é€šå¸¸åŒ…å« 3D å’Œ 2D è¯´è¯äººè„¸ä¸¤å¤§ç±»ï¼Œè¿‘å¹´æ¥ä¸¤è€…éƒ½å¤‡å—ç ”ç©¶å…³æ³¨ã€‚ç„¶è€Œï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œ3D è¯´è¯äººè„¸çš„ç ”ç©¶åœ¨å”‡å½¢åŒæ­¥ï¼ˆlip-syncï¼‰å’Œè¯­éŸ³æ„ŸçŸ¥æ–¹é¢å¹¶æœªåƒ 2D è¯´è¯äººè„¸é‚£æ ·æ·±å…¥ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šæœ¬æ–‡æ–¹æ³•åŠ¨æœºå……åˆ†ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæå‡ºä¸€ä¸ªåä¸º Learn2Talk çš„å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡åˆ©ç”¨ 2D è¯´è¯äººè„¸é¢†åŸŸçš„ä¸¤ä¸ªä¸“ä¸šçŸ¥è¯†ç‚¹æ¥æ„å»ºä¸€ä¸ªæ›´å¥½çš„ 3D è¯´è¯äººè„¸ç½‘ç»œã€‚é¦–å…ˆï¼Œå—éŸ³é¢‘è§†é¢‘åŒæ­¥ç½‘ç»œçš„å¯å‘ï¼Œè®¾è®¡äº†ä¸€ä¸ª 3D åŒæ­¥å”‡å½¢ä¸“å®¶æ¨¡å‹ï¼Œä»¥è¿½æ±‚éŸ³é¢‘å’Œ 3D é¢éƒ¨åŠ¨ä½œä¹‹é—´çš„å”‡å½¢åŒæ­¥ã€‚å…¶æ¬¡ï¼Œé€‰æ‹©ä¸€ä¸ªæ¥è‡ª 2D è¯´è¯äººè„¸æ–¹æ³•çš„æ•™å¸ˆæ¨¡å‹æ¥æŒ‡å¯¼éŸ³é¢‘åˆ° 3D è¿åŠ¨å›å½’ç½‘ç»œçš„è®­ç»ƒï¼Œä»¥äº§ç”Ÿæ›´é«˜çš„ 3D é¡¶ç‚¹ç²¾åº¦ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šæ–¹æ³•æ€§èƒ½ï¼šæœ¬æ–‡æ–¹æ³•åœ¨å”‡å½¢åŒæ­¥ã€é¡¶ç‚¹ç²¾åº¦å’Œè¯­éŸ³æ„ŸçŸ¥æ–¹é¢å‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚è¿™äº›æ€§èƒ½å¯ä»¥æ”¯æŒå…¶ç›®æ ‡ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæå‡ºä¸€ä¸ªåä¸º Learn2Talk çš„å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡åˆ©ç”¨ 2D è¯´è¯äººè„¸é¢†åŸŸçš„ä¸¤ä¸ªä¸“ä¸šçŸ¥è¯†ç‚¹æ¥æ„å»ºä¸€ä¸ªæ›´å¥½çš„ 3D è¯´è¯äººè„¸ç½‘ç»œã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè®¾è®¡äº†ä¸€ä¸ª 3D åŒæ­¥å”‡å½¢ä¸“å®¶æ¨¡å‹ï¼Œä»¥è¿½æ±‚éŸ³é¢‘å’Œ 3D é¢éƒ¨åŠ¨ä½œä¹‹é—´çš„å”‡å½¢åŒæ­¥ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šé€‰æ‹©ä¸€ä¸ªæ¥è‡ª 2D è¯´è¯äººè„¸æ–¹æ³•çš„æ•™å¸ˆæ¨¡å‹æ¥æŒ‡å¯¼éŸ³é¢‘åˆ° 3D è¿åŠ¨å›å½’ç½‘ç»œçš„è®­ç»ƒï¼Œä»¥äº§ç”Ÿæ›´é«˜çš„ 3D é¡¶ç‚¹ç²¾åº¦ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º Learn2Talk çš„å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡åˆ©ç”¨ 2D è¯´è¯äººè„¸é¢†åŸŸçš„ä¸¤ä¸ªä¸“ä¸šçŸ¥è¯†ç‚¹æ¥æ„å»ºä¸€ä¸ªæ›´å¥½çš„ 3D è¯´è¯äººè„¸ç½‘ç»œï¼Œåœ¨å”‡å½¢åŒæ­¥ã€é¡¶ç‚¹ç²¾åº¦å’Œè¯­éŸ³æ„ŸçŸ¥æ–¹é¢å‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§æ–°çš„ 3D è¯´è¯äººè„¸åŠ¨ç”»æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨äº† 2D è¯´è¯äººè„¸é¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†ï¼›æ€§èƒ½ï¼šåœ¨å”‡å½¢åŒæ­¥ã€é¡¶ç‚¹ç²¾åº¦å’Œè¯­éŸ³æ„ŸçŸ¥æ–¹é¢å‡ä¼˜äºç°æœ‰æŠ€æœ¯ï¼›å·¥ä½œé‡ï¼šéœ€è¦æ”¶é›†å’Œæ ‡æ³¨å¤§é‡çš„æ•°æ®ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-c2e8566372db83537dc565617387f4cf.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c36e0e4bd338738e2a5420e68cd6ab5d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-3fe7530e7260eff001a6736622671663.jpg" align="middle"><img src="https://pica.zhimg.com/v2-3f8c50de092534c8ec8b833626c35e42.jpg" align="middle"></details>## Contrastive Gaussian Clustering: Weakly Supervised 3D Scene Segmentation**Authors:Myrna C. Silva, Mahtab Dahaghin, Matteo Toso, Alessio Del Bue**We introduce Contrastive Gaussian Clustering, a novel approach capable of provide segmentation masks from any viewpoint and of enabling 3D segmentation of the scene. Recent works in novel-view synthesis have shown how to model the appearance of a scene via a cloud of 3D Gaussians, and how to generate accurate images from a given viewpoint by projecting on it the Gaussians before $\alpha$ blending their color. Following this example, we train a model to include also a segmentation feature vector for each Gaussian. These can then be used for 3D scene segmentation, by clustering Gaussians according to their feature vectors; and to generate 2D segmentation masks, by projecting the Gaussians on a plane and $\alpha$ blending over their segmentation features. Using a combination of contrastive learning and spatial regularization, our method can be trained on inconsistent 2D segmentation masks, and still learn to generate segmentation masks consistent across all views. Moreover, the resulting model is extremely accurate, improving the IoU accuracy of the predicted masks by $+8\%$ over the state of the art. Code and trained models will be released soon. [PDF](http://arxiv.org/abs/2404.12784v1) **Summary**ä½¿ç”¨æ¥è‡ªä¸åŒè§†è§’çš„å¯¹æ¯”é«˜æ–¯èšç±»å®ç° 3D åœºæ™¯åˆ†å‰²ã€‚**Key Takeaways**- æå‡ºä¸€ç§æ–°çš„å¯¹æ¯”é«˜æ–¯èšç±»æ–¹æ³•ï¼Œèƒ½å¤Ÿä»ä»»ä½•è§†è§’æä¾›åˆ†å‰²æ©æ¨¡ï¼Œå¹¶å®ç°åœºæ™¯çš„ 3D åˆ†å‰²ã€‚- å—æ–°è§†è§’åˆæˆé¢†åŸŸç ”ç©¶çš„å¯å‘ï¼Œä½¿ç”¨ 3D é«˜æ–¯äº‘å»ºæ¨¡åœºæ™¯çš„å¤–è§‚ã€‚- é€šè¿‡å°†é«˜æ–¯æŠ•å½±åˆ°ç»™å®šè§†ç‚¹å¹¶å¯¹å…¶é¢œè‰²è¿›è¡ŒÎ±æ··åˆï¼Œä»ç»™å®šè§†ç‚¹ç”Ÿæˆå‡†ç¡®çš„å›¾åƒã€‚- è®­ç»ƒæ¨¡å‹ï¼Œä½¿æ¯ä¸ªé«˜æ–¯éƒ½åŒ…å«ä¸€ä¸ªåˆ†å‰²ç‰¹å¾å‘é‡ã€‚- é€šè¿‡æ ¹æ®å…¶ç‰¹å¾å‘é‡å¯¹é«˜æ–¯è¿›è¡Œèšç±»ï¼Œå¯ç”¨äº 3D åœºæ™¯åˆ†å‰²ï¼›é€šè¿‡å°†é«˜æ–¯æŠ•å½±åˆ°å¹³é¢ä¸Šå¹¶å¯¹å…¶åˆ†å‰²ç‰¹å¾è¿›è¡Œ Î± æ··åˆï¼Œå¯ç”Ÿæˆ 2D åˆ†å‰²æ©æ¨¡ã€‚- ä½¿ç”¨å¯¹æ¯”å­¦ä¹ å’Œç©ºé—´æ­£åˆ™åŒ–çš„ç»„åˆï¼Œå¯ä»¥åœ¨ä¸ä¸€è‡´çš„ 2D åˆ†å‰²æ©æ¨¡ä¸Šè®­ç»ƒæˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¹¶å­¦ä¹ ç”Ÿæˆåœ¨æ‰€æœ‰è§†å›¾ä¸­éƒ½ä¸€è‡´çš„åˆ†å‰²æ©æ¨¡ã€‚- æ‰€æå‡ºçš„æ–¹æ³•éå¸¸å‡†ç¡®ï¼Œä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼Œé¢„æµ‹æ©æ¨¡çš„ IoU å‡†ç¡®åº¦æé«˜äº† 8%ã€‚- ä»£ç å’Œè®­ç»ƒå¥½çš„æ¨¡å‹å³å°†å‘å¸ƒã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>æ ‡é¢˜ï¼šå¯¹æ¯”é«˜æ–¯èšç±»ï¼šå¼±ç›‘ç£ 3D åœºæ™¯åˆ†å‰²</p></li><li><p>ä½œè€…ï¼šMyrna C. Silvaã€Mahtab Dahaghinã€Matteo Tosoã€Alessio Del Bue</p></li><li><p>å•ä½ï¼šæ„å¤§åˆ©ç†å·¥å­¦é™¢æ¨¡å¼åˆ†æä¸è®¡ç®—æœºè§†è§‰ï¼ˆPAVISï¼‰</p></li><li><p>å…³é”®è¯ï¼š3D é«˜æ–¯æ•£å°„ã€3D åˆ†å‰²ã€å¯¹æ¯”å­¦ä¹ </p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šarXiv:2404.12784v1 [cs.CV]   Github é“¾æ¥ï¼šæ— </p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šè¿‘å¹´æ¥ï¼Œæ–°è§†è§’åˆæˆé¢†åŸŸçš„ç ”ç©¶è¡¨æ˜ï¼Œå¯ä»¥é€šè¿‡ 3D é«˜æ–¯äº‘å¯¹åœºæ™¯çš„å¤–è§‚è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶é€šè¿‡åœ¨ç»™å®šè§†è§’ä¸ŠæŠ•å½±é«˜æ–¯å¹¶ Î± æ··åˆå…¶é¢œè‰²æ¥ç”Ÿæˆå‡†ç¡®çš„å›¾åƒã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¿‡å»æ–¹æ³•ä¸é—®é¢˜ï¼šé«˜æ–¯åˆ†ç»„å’Œ LangSplat ç­‰æ–¹æ³•å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š   - è®­ç»ƒå’Œè¯„ä¼°éœ€è¦å¤§é‡ GPU å†…å­˜ï¼Œå¯¼è‡´æŸäº›åœºæ™¯æ— æ³•å¤„ç†ã€‚   - æ— æ³•ä»ä»»æ„è§†è§’æä¾›åˆ†å‰²æ©ç ï¼Œä¹Ÿæ— æ³•å®ç°åœºæ™¯çš„ 3D åˆ†å‰²ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºå¯¹æ¯”é«˜æ–¯èšç±»æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡ä»¥ä¸‹æ­¥éª¤å®ç° 3D åœºæ™¯åˆ†å‰²å’Œ 2D åˆ†å‰²æ©ç é¢„æµ‹ï¼š   - è®­ç»ƒæ¨¡å‹ä¸ºæ¯ä¸ªé«˜æ–¯ä½“æ·»åŠ åˆ†å‰²ç‰¹å¾å‘é‡ã€‚   - æ ¹æ®ç‰¹å¾å‘é‡å¯¹é«˜æ–¯ä½“è¿›è¡Œèšç±»ï¼Œå®ç° 3D åœºæ™¯åˆ†å‰²ã€‚   - å°†é«˜æ–¯ä½“æŠ•å½±åˆ°å¹³é¢ä¸Šå¹¶ Î± æ··åˆå…¶åˆ†å‰²ç‰¹å¾ï¼Œç”Ÿæˆ 2D åˆ†å‰²æ©ç ã€‚   - ä½¿ç”¨å¯¹æ¯”å­¦ä¹ å’Œç©ºé—´æ­£åˆ™åŒ–ï¼Œåœ¨ä¸ä¸€è‡´çš„ 2D åˆ†å‰²æ©ç ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œç”Ÿæˆè·¨æ‰€æœ‰è§†è§’ä¸€è‡´çš„åˆ†å‰²æ©ç ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šæ€§èƒ½ä¸ç›®æ ‡ï¼š   - ä»»åŠ¡ï¼š3D åœºæ™¯åˆ†å‰²å’Œ 2D åˆ†å‰²æ©ç é¢„æµ‹ã€‚   - æ€§èƒ½ï¼šIoU å‡†ç¡®ç‡æ¯”ç°æœ‰æŠ€æœ¯æé«˜ +8%ï¼Œè¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆå®ç°å…¶ç›®æ ‡ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šå°†åœºæ™¯è¡¨ç¤ºä¸º 3D é«˜æ–¯ä½“é›†åˆï¼Œç¼–ç å‡ ä½•ã€å¤–è§‚å’Œå®ä¾‹åˆ†å‰²ä¿¡æ¯ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šä½¿ç”¨åŸºç¡€æ¨¡å‹ç”Ÿæˆ 2D åˆ†å‰²æ©ç ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šä¼˜åŒ– 3D é«˜æ–¯ä½“ï¼Œæœ€å°åŒ–æ¸²æŸ“å›¾åƒå’ŒçœŸå®å›¾åƒä¹‹é—´çš„å·®å¼‚ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šä½¿ç”¨å¯¹æ¯”åˆ†å‰²æŸå¤±ç›‘ç£ 3D ç‰¹å¾åœºï¼›</p><p>ï¼ˆ5ï¼‰ï¼šå¼•å…¥æ­£åˆ™åŒ–é¡¹ï¼Œå¼ºåˆ¶é«˜æ–¯ä½“åœ¨æ¬§å‡ é‡Œå¾—å’Œåˆ†å‰²ç‰¹å¾ç©ºé—´ä¸­çš„è·ç¦»ç›¸å…³ï¼›</p><p>ï¼ˆ6ï¼‰ï¼šæ¸²æŸ“ 2D ç‰¹å¾å›¾ï¼Œæ ¹æ®å¯¹åº”çš„ 2D åˆ†å‰²æ©ç å¯¹æ¸²æŸ“ç‰¹å¾è¿›è¡Œèšç±»ï¼Œè®¡ç®—å¯¹æ¯”èšç±»æŸå¤±ï¼›</p><p>ï¼ˆ7ï¼‰ï¼šæœ€å¤§åŒ–åŒä¸€åˆ†å‰²å†…ç‰¹å¾ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼Œæœ€å°åŒ–ä¸åŒåˆ†å‰²å†…çš„ç‰¹å¾ç›¸ä¼¼åº¦ã€‚</p><p><strong>8. ç»“è®º</strong></p><p><strong>(1)</strong> æœ¬å·¥ä½œçš„ä¸»è¦æ„ä¹‰åœ¨äºï¼š</p><p>æå‡ºäº†å¯¹æ¯”é«˜æ–¯èšç±»æ–¹æ³•ï¼Œå®ç°äº† 3D åœºæ™¯åˆ†å‰²å’Œ 2D åˆ†å‰²æ©ç é¢„æµ‹ï¼Œæœ‰æ•ˆæé«˜äº†åˆ†å‰²ç²¾åº¦ã€‚</p><p><strong>(2)</strong> æœ¬æ–‡ä¼˜ç¼ºç‚¹æ€»ç»“ï¼ˆåˆ›æ–°ç‚¹ã€æ€§èƒ½ã€å·¥ä½œé‡ï¼‰ï¼š</p><p><strong>åˆ›æ–°ç‚¹ï¼š</strong></p><ul><li>å¼•å…¥å¯¹æ¯”å­¦ä¹ å’Œç©ºé—´æ­£åˆ™åŒ–ï¼Œæé«˜äº†åˆ†å‰²æ©ç çš„ä¸€è‡´æ€§ã€‚</li><li>ä½¿ç”¨ 3D é«˜æ–¯ä½“è¡¨ç¤ºåœºæ™¯ï¼Œç¼–ç å‡ ä½•ã€å¤–è§‚å’Œå®ä¾‹åˆ†å‰²ä¿¡æ¯ã€‚</li></ul><p><strong>æ€§èƒ½ï¼š</strong></p><ul><li>IoU å‡†ç¡®ç‡æ¯”ç°æœ‰æŠ€æœ¯æé«˜ +8%ï¼Œåˆ†å‰²ç²¾åº¦é«˜ã€‚</li></ul><p><strong>å·¥ä½œé‡ï¼š</strong></p><ul><li>è®­ç»ƒå’Œè¯„ä¼°éœ€è¦å¤§é‡ GPU å†…å­˜ï¼Œå¤§åœºæ™¯å¤„ç†å›°éš¾ã€‚</li><li>æ— æ³•ä»ä»»æ„è§†è§’æä¾›åˆ†å‰²æ©ç ï¼Œæ— æ³•å®ç°åœºæ™¯çš„å®Œæ•´ 3D åˆ†å‰²ã€‚</li></ul><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-252e679c7e0a5cfc8056b41c43d99b59.jpg" align="middle"><img src="https://picx.zhimg.com/v2-668e640c91611b7b91220b00abd05f4e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-03dada656b628530891ef19dcbebedba.jpg" align="middle"></details>## RainyScape: Unsupervised Rainy Scene Reconstruction using Decoupled   Neural Rendering**Authors:Xianqiang Lyu, Hui Liu, Junhui Hou**We propose RainyScape, an unsupervised framework for reconstructing clean scenes from a collection of multi-view rainy images. RainyScape consists of two main modules: a neural rendering module and a rain-prediction module that incorporates a predictor network and a learnable latent embedding that captures the rain characteristics of the scene. Specifically, based on the spectral bias property of neural networks, we first optimize the neural rendering pipeline to obtain a low-frequency scene representation. Subsequently, we jointly optimize the two modules, driven by the proposed adaptive direction-sensitive gradient-based reconstruction loss, which encourages the network to distinguish between scene details and rain streaks, facilitating the propagation of gradients to the relevant components. Extensive experiments on both the classic neural radiance field and the recently proposed 3D Gaussian splatting demonstrate the superiority of our method in effectively eliminating rain streaks and rendering clean images, achieving state-of-the-art performance. The constructed high-quality dataset and source code will be publicly available. [PDF](http://arxiv.org/abs/2404.11401v1) **Summary**é›¨æ™¯é‡å»ºï¼šæ— ç›‘ç£åœ°ä»å¤šè§†è§’é›¨æ™¯å›¾é‡å»ºå¹²å‡€åœºæ™¯ã€‚**Key Takeaways**- æå‡ºæ— ç›‘ç£æ¡†æ¶ RainyScapeï¼Œé‡å»ºå¹²å‡€åœºæ™¯ã€‚- RainyScape ç”±ç¥ç»æ¸²æŸ“å’Œé™é›¨é¢„æµ‹æ¨¡å—ç»„æˆã€‚- é™é›¨é¢„æµ‹æ¨¡å—åŒ…å«é¢„æµ‹ç½‘ç»œå’Œå¯å­¦ä¹ æ½œåµŒå…¥ï¼Œæ•æ‰åœºæ™¯çš„é™é›¨ç‰¹å¾ã€‚- åŸºäºç¥ç»ç½‘ç»œçš„å…‰è°±åå·®å±æ€§ï¼Œä¼˜åŒ–ç¥ç»æ¸²æŸ“ç®¡é“ï¼Œè·å¾—ä½é¢‘åœºæ™¯è¡¨ç¤ºã€‚- åˆ©ç”¨è‡ªé€‚åº”æ–¹å‘æ•æ„Ÿæ¢¯åº¦é‡å»ºæŸå¤±ï¼Œä¼˜åŒ–ä¸¤ä¸ªæ¨¡å—ï¼ŒåŒºåˆ†åœºæ™¯ç»†èŠ‚å’Œé›¨ç—•ã€‚- åœ¨ç¥ç»è¾å°„åœºå’Œ 3D é«˜æ–¯å–·æº…ä¸­è¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆæ¶ˆé™¤é›¨ç—•ã€æ¸²æŸ“å¹²å‡€å›¾åƒï¼Œè¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ã€‚- å°†å…¬å¼€æ„å»ºé«˜è´¨é‡æ•°æ®é›†å’Œæºä»£ç ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>æ ‡é¢˜ï¼šRainyScapeï¼šåŸºäºè§£è€¦ç¥ç»æ¸²æŸ“çš„æ— ç›‘ç£é›¨æ™¯é‡å»º</p></li><li><p>ä½œè€…ï¼šXianqiang Lyu, Hui Liu, Junhui Hou</p></li><li><p>å•ä½ï¼šé¦™æ¸¯åŸå¸‚å¤§å­¦è®¡ç®—æœºç§‘å­¦ç³»</p></li><li><p>å…³é”®è¯ï¼šé›¨æ™¯é‡å»ºã€ç¥ç»æ¸²æŸ“ã€æ— ç›‘ç£æŸå¤±</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šxxxï¼ŒGithub é“¾æ¥ï¼šNone</p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åœ¨å­¦ä¹ åœºæ™¯çš„è¿ç»­ä½“ç§¯è¡¨ç¤ºæ–¹é¢å–å¾—äº†çªç ´æ€§è¿›å±•ï¼Œä½†å½“è¾“å…¥å›¾åƒå› æ¨¡ç³Šã€å™ªå£°æˆ–é›¨æ°´ç­‰å› ç´ è€Œé€€åŒ–æ—¶ï¼Œæ¸²æŸ“ç»“æœä¸å¯é¿å…åœ°ä¼šå‡ºç°æ˜æ˜¾ä¼ªå½±ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¿‡å»æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•é’ˆå¯¹ä¸åŒçš„é€€åŒ–å› ç´ æå‡ºäº†ç‰¹å®šä»»åŠ¡çš„è§£å†³æ–¹æ¡ˆï¼Œä½†é’ˆå¯¹é›¨æ™¯é‡å»ºä»»åŠ¡çš„æ–¹æ³•è¾ƒå°‘ï¼Œä¸”éš¾ä»¥é€šè¿‡é™„åŠ ç¥ç»æ¸²æŸ“åœºæ¥è¡¨ç¤ºé›¨æ°´ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡º RainyScapeï¼Œä¸€ä¸ªè§£è€¦çš„ç¥ç»æ¸²æŸ“æ¡†æ¶ï¼Œèƒ½å¤Ÿä»é›¨æ™¯å›¾åƒä¸­æ— ç›‘ç£åœ°é‡å»ºæ— é›¨åœºæ™¯ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸€ä¸ªç¥ç»æ¸²æŸ“æ¨¡å—å’Œä¸€ä¸ªé›¨æ»´é¢„æµ‹æ¨¡å—ï¼Œé€šè¿‡å­¦ä¹ é›¨æ»´åµŒå…¥å’Œä½¿ç”¨é¢„æµ‹å™¨æ¥é¢„æµ‹é›¨æ»´æ¡çº¹ï¼Œå¹¶æå‡ºè‡ªé€‚åº”è§’åº¦ä¼°è®¡ç­–ç•¥å’Œæ¢¯åº¦æ—‹è½¬æŸå¤±æ¥è§£è€¦åœºæ™¯é«˜é¢‘ç»†èŠ‚å’Œé›¨æ»´æ¡çº¹ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šæ–¹æ³•æ€§èƒ½ï¼šåœ¨ç»å…¸ç¥ç»è¾å°„åœºå’Œæœ€è¿‘æå‡ºçš„ 3D é«˜æ–¯ splatting ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æœ‰æ•ˆæ¶ˆé™¤é›¨æ»´æ¡çº¹å’Œæ¸²æŸ“æ¸…æ™°å›¾åƒæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæå‡º RainyScapeï¼Œä¸€ä¸ªè§£è€¦çš„ç¥ç»æ¸²æŸ“æ¡†æ¶ï¼Œèƒ½å¤Ÿä»é›¨æ™¯å›¾åƒä¸­æ— ç›‘ç£åœ°é‡å»ºæ— é›¨åœºæ™¯ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¯¥æ¡†æ¶åŒ…æ‹¬ä¸€ä¸ªç¥ç»æ¸²æŸ“æ¨¡å—å’Œä¸€ä¸ªé›¨æ»´é¢„æµ‹æ¨¡å—ï¼Œé€šè¿‡å­¦ä¹ é›¨æ»´åµŒå…¥å’Œä½¿ç”¨é¢„æµ‹å™¨æ¥é¢„æµ‹é›¨æ»´æ¡çº¹ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šæå‡ºè‡ªé€‚åº”è§’åº¦ä¼°è®¡ç­–ç•¥å’Œæ¢¯åº¦æ—‹è½¬æŸå¤±æ¥è§£è€¦åœºæ™¯é«˜é¢‘ç»†èŠ‚å’Œé›¨æ»´æ¡çº¹ã€‚</p><ol><li>ç»“è®ºï¼š<pre><code>            ï¼ˆ1ï¼‰ï¼šRainyScape åœ¨é›¨æ™¯é‡å»ºé¢†åŸŸå…·æœ‰é‡è¦æ„ä¹‰ï¼Œå®ƒé¦–æ¬¡æå‡ºäº†ä¸€ä¸ªè§£è€¦ç¥ç»æ¸²æŸ“æ¡†æ¶ï¼Œèƒ½å¤Ÿä»é›¨æ™¯å›¾åƒä¸­æ— ç›‘ç£åœ°é‡å»ºæ— é›¨åœºæ™¯ã€‚ è¯¥æ¡†æ¶é€šè¿‡å°†åœºæ™¯é«˜é¢‘ç»†èŠ‚å’Œé›¨æ»´æ¡çº¹è§£è€¦ï¼Œæœ‰æ•ˆåœ°æ¶ˆé™¤äº†é›¨æ»´æ¡çº¹ï¼Œå¹¶æ¸²æŸ“å‡ºæ¸…æ™°çš„å›¾åƒã€‚            ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šRainyScape åˆ›æ–°æ€§åœ°æå‡ºäº†ä¸€ä¸ªè§£è€¦ç¥ç»æ¸²æŸ“æ¡†æ¶ï¼Œå°†åœºæ™¯é«˜é¢‘ç»†èŠ‚å’Œé›¨æ»´æ¡çº¹è§£è€¦ï¼Œæœ‰æ•ˆåœ°æ¶ˆé™¤äº†é›¨æ»´æ¡çº¹ï¼Œå¹¶æ¸²æŸ“å‡ºæ¸…æ™°çš„å›¾åƒã€‚            æ€§èƒ½ï¼šRainyScape åœ¨ç»å…¸ç¥ç»è¾å°„åœºå’Œæœ€è¿‘æå‡ºçš„ 3D é«˜æ–¯ splatting ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æœ‰æ•ˆæ¶ˆé™¤é›¨æ»´æ¡çº¹å’Œæ¸²æŸ“æ¸…æ™°å›¾åƒæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚            å·¥ä½œé‡ï¼šRainyScape çš„å·¥ä½œé‡ä¸­ç­‰ï¼Œéœ€è¦è®­ç»ƒç¥ç»æ¸²æŸ“æ¨¡å—å’Œé›¨æ»´é¢„æµ‹æ¨¡å—ï¼Œå¹¶æå‡ºè‡ªé€‚åº”è§’åº¦ä¼°è®¡ç­–ç•¥å’Œæ¢¯åº¦æ—‹è½¬æŸå¤±æ¥è§£è€¦åœºæ™¯é«˜é¢‘ç»†èŠ‚å’Œé›¨æ»´æ¡çº¹ã€‚</code></pre></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-789763f7ebb6ec7a923539611ab1fe24.jpg" align="middle"><img src="https://picx.zhimg.com/v2-89f176b1378008d1c0b63c9241adfdb2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5f7fb8305c36c1fe2572adfd98b584f7.jpg" align="middle"><img src="https://pica.zhimg.com/v2-76be36036e15658d754b57c4864b0abf.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3765b699865b1d89cc9f5f13f9843a0e.jpg" align="middle"><img src="https://pica.zhimg.com/v2-34d10a80ece07ba92081dfc066d00427.jpg" align="middle"></details><h2 id="DeblurGS-Gaussian-Splatting-for-Camera-Motion-Blur"><a href="#DeblurGS-Gaussian-Splatting-for-Camera-Motion-Blur" class="headerlink" title="DeblurGS: Gaussian Splatting for Camera Motion Blur"></a>DeblurGS: Gaussian Splatting for Camera Motion Blur</h2><p><strong>Authors:Jeongtaek Oh, Jaeyoung Chung, Dongwoo Lee, Kyoung Mu Lee</strong></p><p>Although significant progress has been made in reconstructing sharp 3D scenes from motion-blurred images, a transition to real-world applications remains challenging. The primary obstacle stems from the severe blur which leads to inaccuracies in the acquisition of initial camera poses through Structure-from-Motion, a critical aspect often overlooked by previous approaches. To address this challenge, we propose DeblurGS, a method to optimize sharp 3D Gaussian Splatting from motion-blurred images, even with the noisy camera pose initialization. We restore a fine-grained sharp scene by leveraging the remarkable reconstruction capability of 3D Gaussian Splatting. Our approach estimates the 6-Degree-of-Freedom camera motion for each blurry observation and synthesizes corresponding blurry renderings for the optimization process. Furthermore, we propose Gaussian Densification Annealing strategy to prevent the generation of inaccurate Gaussians at erroneous locations during the early training stages when camera motion is still imprecise. Comprehensive experiments demonstrate that our DeblurGS achieves state-of-the-art performance in deblurring and novel view synthesis for real-world and synthetic benchmark datasets, as well as field-captured blurry smartphone videos. </p><p><a href="http://arxiv.org/abs/2404.11358v2">PDF</a> </p><p><strong>Summary</strong><br>ä»æ¨¡ç³Šè¿åŠ¨å›¾åƒé‡å»ºæ¸…æ™° 3D åœºæ™¯æ–¹æ³•ï¼Œä¼˜åŒ– 3D é«˜æ–¯æŠ•å°„ï¼Œå®ç°ç²¾ç¡®æ‘„åƒæœºä½å§¿åˆå§‹åŒ–ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>DeblurGS ä¼˜åŒ–é«˜æ–¯æŠ•å°„ï¼Œæé«˜è¿åŠ¨æ¨¡ç³Šå›¾åƒ 3D é‡å»ºç²¾åº¦ã€‚</li><li>åˆ©ç”¨é«˜æ–¯æŠ•å°„çš„é‡å»ºèƒ½åŠ›ï¼Œè¿˜åŸç²¾ç»†é”åˆ©åœºæ™¯ã€‚</li><li>ä¼°è®¡æ¯å¹…æ¨¡ç³Šå›¾åƒçš„ 6 è‡ªç”±åº¦æ‘„åƒæœºè¿åŠ¨ï¼Œç”Ÿæˆæ¨¡ç³Šæ¸²æŸ“ç”¨äºä¼˜åŒ–ã€‚</li><li>é«˜æ–¯å¯†åº¦é€€ç«ç­–ç•¥é˜²æ­¢é”™è¯¯ä½ç½®ç”Ÿæˆä¸å‡†ç¡®çš„é«˜æ–¯ã€‚</li><li>DeblurGS åœ¨å»æ¨¡ç³Šå’Œåˆæˆæ–°è§†è§’æ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li><li>é€‚ç”¨äºçœŸå®ä¸–ç•Œå’ŒåˆæˆåŸºå‡†æ•°æ®é›†ï¼Œä»¥åŠç°åœºæ‹æ‘„çš„æ¨¡ç³Šæ™ºèƒ½æ‰‹æœºè§†é¢‘ã€‚</li><li>DeblurGS æå¤§åœ°æ‰©å±•äº†è¿åŠ¨æ¨¡ç³Šå›¾åƒçš„ 3D é‡å»ºçš„å®é™…åº”ç”¨ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: DeblurGS: é«˜æ–¯æº…å°„ç›¸æœºè¿åŠ¨æ¨¡ç³Š (DeblurGS: Gaussian Splatting for Camera Motion Blur)</p></li><li><p>Authors: Jeongtaek Oh, Jaeyoung Chung, Dongwoo Lee, and Kyoung Mu Lee</p></li><li><p>Affiliation: é¦–å°”å›½ç«‹å¤§å­¦äººå·¥æ™ºèƒ½ä¸ä¿¡æ¯å¤„ç†ç ”ç©¶æ‰€ (IPAI, Seoul National University)</p></li><li><p>Keywords: 3D Gaussian Splatting Â· Camera Motion Deblurring</p></li><li><p>Urls: None, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): å°½ç®¡ä»è¿åŠ¨æ¨¡ç³Šå›¾åƒé‡å»ºæ¸…æ™°çš„ 3D åœºæ™¯æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†å‘å®é™…åº”ç”¨çš„è¿‡æ¸¡ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸»è¦éšœç¢æºäºä¸¥é‡çš„æ¨¡ç³Šï¼Œè¿™ä¼šå¯¼è‡´é€šè¿‡ Structure-from-Motion è·å–åˆå§‹ç›¸æœºå§¿æ€çš„ä¸å‡†ç¡®ï¼Œè€Œè¿™å¾€å¾€æ˜¯ä»¥å‰çš„æ–¹æ³•æ‰€å¿½è§†çš„å…³é”®æ–¹é¢ã€‚</p><p>(2): è¿‡å»çš„æ–¹æ³•ä¸»è¦é›†ä¸­äºæ¨¡ç³Šå›¾åƒçš„å»æ¨¡ç³Šå¤„ç†ï¼Œä½†å¯¹äºåˆå§‹ç›¸æœºå§¿æ€çš„å™ªå£°åˆå§‹åŒ–ä¸é²æ£’ã€‚</p><p>(3): æœ¬æ–‡æå‡º DeblurGSï¼Œè¿™æ˜¯ä¸€ç§ä»è¿åŠ¨æ¨¡ç³Šå›¾åƒä¼˜åŒ–æ¸…æ™°çš„ 3D é«˜æ–¯æº…å°„çš„æ–¹æ³•ï¼Œå³ä½¿åœ¨å™ªå£°ç›¸æœºå§¿æ€åˆå§‹åŒ–çš„æƒ…å†µä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æˆ‘ä»¬åˆ©ç”¨ 3D é«˜æ–¯æº…å°„çš„å‡ºè‰²é‡å»ºèƒ½åŠ›æ¥æ¢å¤ç»†ç²’åº¦çš„æ¸…æ™°åœºæ™¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¼°è®¡æ¯ä¸ªæ¨¡ç³Šè§‚æµ‹çš„ 6 è‡ªç”±åº¦ç›¸æœºè¿åŠ¨ï¼Œå¹¶ä¸ºä¼˜åŒ–è¿‡ç¨‹åˆæˆç›¸åº”çš„æ¨¡ç³Šæ¸²æŸ“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†é«˜æ–¯è‡´å¯†åŒ–é€€ç«ç­–ç•¥ï¼Œä»¥é˜²æ­¢åœ¨ç›¸æœºè¿åŠ¨ä»ç„¶ä¸ç²¾ç¡®çš„æ—©æœŸè®­ç»ƒé˜¶æ®µåœ¨é”™è¯¯çš„ä½ç½®ç”Ÿæˆä¸å‡†ç¡®çš„é«˜æ–¯ã€‚</p><p>(4): ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ DeblurGS åœ¨çœŸå®ä¸–ç•Œå’ŒåˆæˆåŸºå‡†æ•°æ®é›†ä»¥åŠç°åœºæ•è·çš„æ¨¡ç³Šæ™ºèƒ½æ‰‹æœºè§†é¢‘çš„å»æ¨¡ç³Šå’Œæ–°è§†å›¾åˆæˆæ–¹é¢å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæå‡º DeblurGSï¼Œä¸€ç§ä»è¿åŠ¨æ¨¡ç³Šå›¾åƒä¼˜åŒ–æ¸…æ™°çš„ 3D é«˜æ–¯æº…å°„çš„æ–¹æ³•ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šåˆ©ç”¨ 3D é«˜æ–¯æº…å°„çš„é‡å»ºèƒ½åŠ›æ¢å¤ç»†ç²’åº¦çš„æ¸…æ™°åœºæ™¯ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šä¼°è®¡æ¯ä¸ªæ¨¡ç³Šè§‚æµ‹çš„ 6 è‡ªç”±åº¦ç›¸æœºè¿åŠ¨ï¼Œå¹¶åˆæˆç›¸åº”çš„æ¨¡ç³Šæ¸²æŸ“ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šæå‡ºé«˜æ–¯è‡´å¯†åŒ–é€€ç«ç­–ç•¥ï¼Œé˜²æ­¢åœ¨ç›¸æœºè¿åŠ¨ä¸ç²¾ç¡®çš„æ—©æœŸè®­ç»ƒé˜¶æ®µç”Ÿæˆä¸å‡†ç¡®çš„é«˜æ–¯ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ä»è¿åŠ¨æ¨¡ç³Šå›¾åƒä¼˜åŒ–æ¸…æ™°çš„ 3D é«˜æ–¯æº…å°„çš„æ–¹æ³•ï¼Œå³ä½¿åœ¨å™ªå£°ç›¸æœºå§¿æ€åˆå§‹åŒ–çš„æƒ…å†µä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ 3D é«˜æ–¯æº…å°„çš„å‡ºè‰²é‡å»ºèƒ½åŠ›æ¥æ¢å¤ç»†ç²’åº¦çš„æ¸…æ™°åœºæ™¯ï¼Œä¼°è®¡æ¯ä¸ªæ¨¡ç³Šè§‚æµ‹çš„ 6 è‡ªç”±åº¦ç›¸æœºè¿åŠ¨ï¼Œå¹¶ä¸ºä¼˜åŒ–è¿‡ç¨‹åˆæˆç›¸åº”çš„æ¨¡ç³Šæ¸²æŸ“ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•æå‡ºäº†é«˜æ–¯è‡´å¯†åŒ–é€€ç«ç­–ç•¥ï¼Œä»¥é˜²æ­¢åœ¨ç›¸æœºè¿åŠ¨ä»ç„¶ä¸ç²¾ç¡®çš„æ—©æœŸè®­ç»ƒé˜¶æ®µåœ¨é”™è¯¯çš„ä½ç½®ç”Ÿæˆä¸å‡†ç¡®çš„é«˜æ–¯ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨çœŸå®ä¸–ç•Œå’ŒåˆæˆåŸºå‡†æ•°æ®é›†ä»¥åŠç°åœºæ•è·çš„æ¨¡ç³Šæ™ºèƒ½æ‰‹æœºè§†é¢‘çš„å»æ¨¡ç³Šå’Œæ–°è§†å›¾åˆæˆæ–¹é¢å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šåˆ©ç”¨ 3D é«˜æ–¯æº…å°„çš„é‡å»ºèƒ½åŠ›æ¢å¤ç»†ç²’åº¦çš„æ¸…æ™°åœºæ™¯ï¼Œå³ä½¿åœ¨å™ªå£°ç›¸æœºå§¿æ€åˆå§‹åŒ–çš„æƒ…å†µä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ï¼›</p><p>æ€§èƒ½ï¼šåœ¨çœŸå®ä¸–ç•Œå’ŒåˆæˆåŸºå‡†æ•°æ®é›†ä»¥åŠç°åœºæ•è·çš„æ¨¡ç³Šæ™ºèƒ½æ‰‹æœºè§†é¢‘çš„å»æ¨¡ç³Šå’Œæ–°è§†å›¾åˆæˆæ–¹é¢å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼›</p><p>å·¥ä½œé‡ï¼šè¯¥æ–¹æ³•éœ€è¦ä¼°è®¡æ¯ä¸ªæ¨¡ç³Šè§‚æµ‹çš„ 6 è‡ªç”±åº¦ç›¸æœºè¿åŠ¨ï¼Œå¹¶ä¸ºä¼˜åŒ–è¿‡ç¨‹åˆæˆç›¸åº”çš„æ¨¡ç³Šæ¸²æŸ“ï¼Œè¿™å¯èƒ½ä¼šå¢åŠ è®¡ç®—æˆæœ¬ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-d1b62fa212aabdf515b9baf8fdc306be.jpg" align="middle"><img src="https://pica.zhimg.com/v2-32c4f56eaf456fe86ff5f42abfbd6ffb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-50b9e9cff40ee36449b6b3559539186a.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-04-22  Learn2Talk 3D Talking Face Learns from 2D Talking Face</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/04/22/Paper/2024-04-22/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/04/22/Paper/2024-04-22/Talking%20Head%20Generation/</id>
    <published>2024-04-22T09:22:24.000Z</published>
    <updated>2024-04-22T09:22:24.975Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-04-22-æ›´æ–°"><a href="#2024-04-22-æ›´æ–°" class="headerlink" title="2024-04-22 æ›´æ–°"></a>2024-04-22 æ›´æ–°</h1><h2 id="Learn2Talk-3D-Talking-Face-Learns-from-2D-Talking-Face"><a href="#Learn2Talk-3D-Talking-Face-Learns-from-2D-Talking-Face" class="headerlink" title="Learn2Talk: 3D Talking Face Learns from 2D Talking Face"></a>Learn2Talk: 3D Talking Face Learns from 2D Talking Face</h2><p><strong>Authors:Yixiang Zhuang, Baoping Cheng, Yao Cheng, Yuntao Jin, Renshuai Liu, Chengyang Li, Xuan Cheng, Jing Liao, Juncong Lin</strong></p><p>Speech-driven facial animation methods usually contain two main classes, 3D and 2D talking face, both of which attract considerable research attention in recent years. However, to the best of our knowledge, the research on 3D talking face does not go deeper as 2D talking face, in the aspect of lip-synchronization (lip-sync) and speech perception. To mind the gap between the two sub-fields, we propose a learning framework named Learn2Talk, which can construct a better 3D talking face network by exploiting two expertise points from the field of 2D talking face. Firstly, inspired by the audio-video sync network, a 3D sync-lip expert model is devised for the pursuit of lip-sync between audio and 3D facial motion. Secondly, a teacher model selected from 2D talking face methods is used to guide the training of the audio-to-3D motions regression network to yield more 3D vertex accuracy. Extensive experiments show the advantages of the proposed framework in terms of lip-sync, vertex accuracy and speech perception, compared with state-of-the-arts. Finally, we show two applications of the proposed framework: audio-visual speech recognition and speech-driven 3D Gaussian Splatting based avatar animation. </p><p><a href="http://arxiv.org/abs/2404.12888v1">PDF</a> </p><p><strong>Summary</strong><br>é€šè¿‡å€Ÿé‰´2Dè¯´è¯äººè„¸çš„å”‡å½¢åŒæ­¥(lip-sync)å’Œè¯­éŸ³æ„ŸçŸ¥çš„ä¸“ä¸šçŸ¥è¯†ï¼ŒLearn2Talkæ¡†æ¶æ„å»ºäº†ä¸€ä¸ªæ›´å¥½çš„3Dè¯´è¯äººè„¸ç½‘ç»œã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºäº†Learn2Talkæ¡†æ¶ï¼Œå°†2Dè¯´è¯äººè„¸çš„ä¸“ä¸šçŸ¥è¯†åº”ç”¨äº3Dè¯´è¯äººè„¸ç½‘ç»œã€‚</li><li>è®¾è®¡äº†3Då”‡å½¢åŒæ­¥ä¸“å®¶æ¨¡å‹ï¼Œè¿½æ±‚éŸ³é¢‘å’Œ3Dé¢éƒ¨åŠ¨ä½œä¹‹é—´çš„å”‡å½¢åŒæ­¥ã€‚</li><li>ä½¿ç”¨2Dè¯´è¯äººè„¸æ–¹æ³•é€‰æ‹©çš„æ•™å¸ˆæ¨¡å‹æ¥æŒ‡å¯¼éŸ³é¢‘åˆ°3DåŠ¨ä½œå›å½’ç½‘ç»œçš„è®­ç»ƒï¼Œä»¥æé«˜3Dé¡¶ç‚¹ç²¾åº¦ã€‚</li><li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å”‡å½¢åŒæ­¥ã€é¡¶ç‚¹ç²¾åº¦å’Œè¯­éŸ³æ„ŸçŸ¥æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li><li>å±•ç¤ºäº†è¯¥æ¡†æ¶çš„ä¸¤ä¸ªåº”ç”¨ï¼šè§†å¬è¯­éŸ³è¯†åˆ«å’Œè¯­éŸ³é©±åŠ¨çš„3Dé«˜æ–¯å–·å°„åŠ¨ç”»ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>æ ‡é¢˜ï¼šLearn2Talkï¼š3D Talking Face Learns from 2Dï¼ˆ3D è¯´è¯äººè„¸ä» 2D è¯´è¯äººè„¸ä¸­å­¦ä¹ ï¼‰</p></li><li><p>ä½œè€…ï¼šYixiang Zhuang, Baoping Cheng, Yao Cheng, Yuntao Jin, Renshuai Liu, Chengyang Li, Xuan Cheng, Jing Liao, Juncong Lin</p></li><li><p>ç¬¬ä¸€ä½œè€…å•ä½ï¼šæš‚æ— </p></li><li><p>å…³é”®è¯ï¼šSpeech-driven, 3D Facial Animation, 2D Talking face, Transformer, 3D Gaussian Splatting</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šæš‚æ— ï¼ŒGithub é“¾æ¥ï¼šhttps://lkjkjoiuiu.github.io/Learn2Talk/</p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šè¯­éŸ³é©±åŠ¨çš„é¢éƒ¨åŠ¨ç”»æ–¹æ³•ä¸»è¦åŒ…å« 3D å’Œ 2D è¯´è¯äººè„¸ä¸¤å¤§ç±»ï¼Œè¿‘å¹´æ¥ä¸¤è€…éƒ½å¤‡å—ç ”ç©¶å…³æ³¨ã€‚ç„¶è€Œï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œ3D è¯´è¯äººè„¸çš„ç ”ç©¶å¹¶æœªåƒ 2D è¯´è¯äººè„¸é‚£æ ·æ·±å…¥ï¼Œåœ¨å”‡å½¢åŒæ­¥ï¼ˆlip-syncï¼‰å’Œè¨€è¯­æ„ŸçŸ¥æ–¹é¢å­˜åœ¨å·®è·ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šä»¥å¾€æ–¹æ³•ï¼šä»¥å¾€æ–¹æ³•ä¸»è¦åˆ†ä¸º 2D å’Œ 3D è¯´è¯äººè„¸æ–¹æ³•ã€‚2D è¯´è¯äººè„¸æ–¹æ³•é€šå¸¸åœ¨åƒç´ ç©ºé—´ï¼ˆä¾‹å¦‚å›¾åƒã€è§†é¢‘ï¼‰ä¸­ç”Ÿæˆå”‡éƒ¨è¿åŠ¨æˆ–å¤´éƒ¨è¿åŠ¨ä»¥åŒ¹é…ç»™å®šçš„è¾“å…¥éŸ³é¢‘æµï¼Œè€Œ 3D è¯´è¯äººè„¸æ–¹æ³•ä½¿ç”¨æ—¶é—´ 3D é¡¶ç‚¹æ•°æ®ï¼ˆä¾‹å¦‚ 3D äººè„¸æ¨¡æ¿ã€æ··åˆå½¢çŠ¶å‚æ•°ï¼‰æ¥è¡¨ç¤ºé¢éƒ¨è¿åŠ¨ã€‚ä¸ 2D è¯´è¯äººè„¸æ–¹æ³•ç›¸æ¯”ï¼Œ3D è¯´è¯äººè„¸æ–¹æ³•å¯ä»¥åˆæˆæ›´ç»†å¾®çš„å”‡éƒ¨åŠ¨ä½œï¼Œå› ä¸ºç»†ç²’åº¦çš„å”‡å½¢æ ¡æ­£å¯ä»¥åœ¨ 3D ç©ºé—´ä¸­æ›´å¥½åœ°æ‰§è¡Œã€‚æ­¤å¤–ï¼Œ3D é¢éƒ¨åŠ¨ç”»å…·æœ‰é‡è¦çš„ä¼˜åŠ¿ï¼Œå› ä¸ºå®ƒå¯ä»¥ä¸ 3D æ¨¡å‹æˆ–è™šæ‹Ÿè§’è‰²æ— ç¼é›†æˆï¼Œä»è€Œå®ç°æ›´é€¼çœŸçš„äº¤äº’ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šä¸ºäº†å¼¥åˆä¸¤è€…ä¹‹é—´çš„å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸º Learn2Talk çš„å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥é€šè¿‡åˆ©ç”¨ 2D è¯´è¯äººè„¸é¢†åŸŸçš„ä¸¤ä¸ªä¸“ä¸šçŸ¥è¯†ç‚¹æ¥æ„å»ºæ›´å¥½çš„ 3D è¯´è¯äººè„¸ç½‘ç»œã€‚é¦–å…ˆï¼Œå—éŸ³é¢‘è§†é¢‘åŒæ­¥ç½‘ç»œçš„å¯å‘ï¼Œè®¾è®¡äº†ä¸€ä¸ª 3D åŒæ­¥å”‡éƒ¨ä¸“å®¶æ¨¡å‹ï¼Œä»¥è¿½æ±‚éŸ³é¢‘å’Œ 3D é¢éƒ¨åŠ¨ä½œä¹‹é—´çš„å”‡å½¢åŒæ­¥ã€‚å…¶æ¬¡ï¼Œä½¿ç”¨ä» 2D è¯´è¯äººè„¸æ–¹æ³•ä¸­é€‰æ‹©çš„æ•™å¸ˆæ¨¡å‹æ¥æŒ‡å¯¼éŸ³é¢‘åˆ° 3D åŠ¨ä½œå›å½’ç½‘ç»œçš„è®­ç»ƒï¼Œä»¥äº§ç”Ÿæ›´é«˜çš„ 3D é¡¶ç‚¹ç²¾åº¦ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šæ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†ä»€ä¹ˆæ€§èƒ½ï¼šå¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„æ¡†æ¶åœ¨å”‡å½¢åŒæ­¥ã€é¡¶ç‚¹ç²¾åº¦å’Œè¨€è¯­æ„ŸçŸ¥æ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬å±•ç¤ºäº†æ‰€æå‡ºæ¡†æ¶çš„ä¸¤ä¸ªåº”ç”¨ï¼šè§†å¬è¯­éŸ³è¯†åˆ«å’Œè¯­éŸ³é©±åŠ¨çš„åŸºäº 3D é«˜æ–¯æ³¼æº…çš„å¤´åƒåŠ¨ç”»ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒLearn2Talk å¯ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨ 2D è¯´è¯äººè„¸çš„ä¸“ä¸šçŸ¥è¯†æ¥æé«˜ 3D è¯´è¯äººè„¸çš„æ€§èƒ½ï¼Œä»è€Œä¸ºè¯­éŸ³é©±åŠ¨çš„é¢éƒ¨åŠ¨ç”»é¢†åŸŸåšå‡ºè´¡çŒ®ã€‚</p><ol><li><p>æ–¹æ³•ï¼š</p><pre><code>            (1): å—éŸ³é¢‘è§†é¢‘åŒæ­¥ç½‘ç»œçš„å¯å‘ï¼Œè®¾è®¡äº† 3D åŒæ­¥å”‡éƒ¨ä¸“å®¶æ¨¡å‹ SyncNet3Dï¼Œä»¥è¿½æ±‚éŸ³é¢‘å’Œ 3D é¢éƒ¨åŠ¨ä½œä¹‹é—´çš„å”‡å½¢åŒæ­¥ï¼›            (2): ä½¿ç”¨ä» 2D è¯´è¯äººè„¸æ–¹æ³•ä¸­é€‰æ‹©çš„æ•™å¸ˆæ¨¡å‹ LipReadNet æ¥æŒ‡å¯¼éŸ³é¢‘åˆ° 3D åŠ¨ä½œå›å½’ç½‘ç»œ Audio2Mesh çš„è®­ç»ƒï¼Œä»¥äº§ç”Ÿæ›´é«˜çš„ 3D é¡¶ç‚¹ç²¾åº¦ï¼›            (3): æå‡ºäº†ä¸€ç§è”åˆè®­ç»ƒæ¡†æ¶ï¼Œå°† SyncNet3D å’Œ Audio2Mesh ç»“åˆèµ·æ¥ï¼Œé€šè¿‡è”åˆæŸå¤±å‡½æ•°ä¼˜åŒ–ï¼Œä½¿ 3D è¯´è¯äººè„¸æ¨¡å‹åŒæ—¶æ»¡è¶³å”‡å½¢åŒæ­¥å’Œé¡¶ç‚¹ç²¾åº¦è¦æ±‚ã€‚</code></pre></li><li><p>ç»“è®ºï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œé€šè¿‡å€Ÿé‰´ 2D è¯´è¯äººè„¸é¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†ï¼Œæå‡ºäº†ä¸€ç§åä¸º Learn2Talk çš„å­¦ä¹ æ¡†æ¶ï¼Œæœ‰æ•ˆæå‡äº† 3D è¯´è¯äººè„¸çš„æ€§èƒ½ï¼Œä¸ºè¯­éŸ³é©±åŠ¨çš„é¢éƒ¨åŠ¨ç”»é¢†åŸŸåšå‡ºäº†è´¡çŒ®ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šLearn2Talk åˆ›æ–°æ€§åœ°å°† 3D åŒæ­¥å”‡éƒ¨ä¸“å®¶æ¨¡å‹ SyncNet3D ä¸æ•™å¸ˆæ¨¡å‹ LipReadNet ç›¸ç»“åˆï¼Œé€šè¿‡è”åˆè®­ç»ƒï¼Œå®ç°äº† 3D è¯´è¯äººè„¸æ¨¡å‹åœ¨å”‡å½¢åŒæ­¥å’Œé¡¶ç‚¹ç²¾åº¦æ–¹é¢çš„åŒé‡æå‡ã€‚</p><p>æ€§èƒ½ï¼šåœ¨å”‡å½¢åŒæ­¥ã€é¡¶ç‚¹ç²¾åº¦å’Œè¨€è¯­æ„ŸçŸ¥æ–¹é¢ï¼ŒLearn2Talk å‡ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p><p>å·¥ä½œé‡ï¼šLearn2Talk çš„è®­ç»ƒè¿‡ç¨‹è¾ƒä¸ºå¤æ‚ï¼Œéœ€è¦åŒæ—¶è®­ç»ƒ SyncNet3D å’Œ Audio2Mesh ä¸¤ä¸ªæ¨¡å‹ï¼Œå¹¶ä¸”éœ€è¦ä» 2D è¯´è¯äººè„¸æ–¹æ³•ä¸­é€‰æ‹©æ•™å¸ˆæ¨¡å‹ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-c2e8566372db83537dc565617387f4cf.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c36e0e4bd338738e2a5420e68cd6ab5d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3fe7530e7260eff001a6736622671663.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3f8c50de092534c8ec8b833626c35e42.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-04-22  Learn2Talk 3D Talking Face Learns from 2D Talking Face</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/04/22/Paper/2024-04-22/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/04/22/Paper/2024-04-22/Diffusion%20Models/</id>
    <published>2024-04-22T09:18:09.000Z</published>
    <updated>2024-04-22T09:18:09.823Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-04-22-æ›´æ–°"><a href="#2024-04-22-æ›´æ–°" class="headerlink" title="2024-04-22 æ›´æ–°"></a>2024-04-22 æ›´æ–°</h1><h2 id="Zero-Shot-Medical-Phrase-Grounding-with-Off-the-shelf-Diffusion-Models"><a href="#Zero-Shot-Medical-Phrase-Grounding-with-Off-the-shelf-Diffusion-Models" class="headerlink" title="Zero-Shot Medical Phrase Grounding with Off-the-shelf Diffusion Models"></a>Zero-Shot Medical Phrase Grounding with Off-the-shelf Diffusion Models</h2><p><strong>Authors:Konstantinos Vilouras, Pedro Sanchez, Alison Q. Oâ€™Neil, Sotirios A. Tsaftaris</strong></p><p>Localizing the exact pathological regions in a given medical scan is an important imaging problem that requires a large amount of bounding box ground truth annotations to be accurately solved. However, there exist alternative, potentially weaker, forms of supervision, such as accompanying free-text reports, which are readily available. The task of performing localization with textual guidance is commonly referred to as phrase grounding. In this work, we use a publicly available Foundation Model, namely the Latent Diffusion Model, to solve this challenging task. This choice is supported by the fact that the Latent Diffusion Model, despite being generative in nature, contains mechanisms (cross-attention) that implicitly align visual and textual features, thus leading to intermediate representations that are suitable for the task at hand. In addition, we aim to perform this task in a zero-shot manner, i.e., without any further training on target data, meaning that the modelâ€™s weights remain frozen. To this end, we devise strategies to select features and also refine them via post-processing without extra learnable parameters. We compare our proposed method with state-of-the-art approaches which explicitly enforce image-text alignment in a joint embedding space via contrastive learning. Results on a popular chest X-ray benchmark indicate that our method is competitive wih SOTA on different types of pathology, and even outperforms them on average in terms of two metrics (mean IoU and AUC-ROC). Source code will be released upon acceptance. </p><p><a href="http://arxiv.org/abs/2404.12920v1">PDF</a> 8 pages, 3 figures, submitted to IEEE J-BHI Special Issue on   Foundation Models in Medical Imaging</p><p><strong>Summary</strong><br>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚éšæ‰©æ•£æ¨¡å‹ï¼‰å³ä½¿åœ¨æ²¡æœ‰ç›®æ ‡æ•°æ®è®­ç»ƒçš„æƒ…å†µä¸‹ä¹Ÿèƒ½æ‰§è¡Œæ–‡æœ¬å¼•å¯¼å®šä½ä»»åŠ¡ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>éšæ‰©æ•£æ¨¡å‹å…·æœ‰éšå¼å¯¹é½è§†è§‰å’Œæ–‡æœ¬ç‰¹å¾çš„æœºåˆ¶ï¼Œé€‚ç”¨äºæ–‡æœ¬å¼•å¯¼å®šä½ä»»åŠ¡ã€‚</li><li>è¯¥æ–¹æ³•é‡‡ç”¨é›¶æ ·æœ¬æ–¹å¼ï¼Œæ— éœ€å¯¹ç›®æ ‡æ•°æ®è¿›è¡Œè¿›ä¸€æ­¥è®­ç»ƒã€‚</li><li>é€šè¿‡ç‰¹å¾é€‰æ‹©å’Œåå¤„ç†ç­–ç•¥ï¼Œåœ¨ä¸å¢åŠ å¯å­¦ä¹ å‚æ•°çš„æƒ…å†µä¸‹ä¼˜åŒ–ç‰¹å¾ã€‚</li><li>è¯¥æ–¹æ³•ä¸é‡‡ç”¨å¯¹æ¯”å­¦ä¹ æ˜¾å¼å¼ºåˆ¶å›¾åƒå’Œæ–‡æœ¬å¯¹é½çš„å…ˆè¿›æ–¹æ³•å…·æœ‰ç«äº‰åŠ›ã€‚</li><li>åœ¨èƒ¸éƒ¨ X å°„çº¿åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•åœ¨ä¸åŒç±»å‹çš„ç—…ç†ä¸Šä¸ SOTA æŒå¹³ï¼Œåœ¨å¹³å‡ IoU å’Œ AUC-ROC ä¸¤ä¸ªæŒ‡æ ‡ä¸Šç”šè‡³ä¼˜äº SOTAã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: é›¶æ ·æœ¬åŒ»å­¦çŸ­è¯­å®šä½</p></li><li><p>Authors: Konstantinos Vilouras, Pedro Sanchez, Alison Q. O'Neil, Sotirios A. Tsaftaris</p></li><li><p>Affiliation: çˆ±ä¸å ¡å¤§å­¦å·¥ç¨‹å­¦é™¢</p></li><li><p>Keywords: æ·±åº¦å­¦ä¹ , æ‰©æ•£æ¨¡å‹, åŒ»å­¦å½±åƒ, çŸ­è¯­å®šä½, é›¶æ ·æœ¬å­¦ä¹ </p></li><li><p>Urls: Paper: https://arxiv.org/abs/2404.12920, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): æœ¬æ–‡ç ”ç©¶èƒŒæ™¯æ˜¯åŒ»å­¦å½±åƒä¸­ç—…ç†åŒºåŸŸå®šä½ä»»åŠ¡éœ€è¦å¤§é‡è¾¹ç•Œæ¡†æ ‡æ³¨ï¼Œè€Œæ–‡æœ¬å¼•å¯¼å®šä½ä»»åŠ¡ï¼ˆçŸ­è¯­å®šä½ï¼‰å¯ä»¥æä¾›ä¸€ç§æ›¿ä»£çš„å¼±ç›‘ç£å½¢å¼ã€‚</p><p>(2): ç°æœ‰æ–¹æ³•é€šè¿‡å¯¹æ¯”å­¦ä¹ åœ¨è”åˆåµŒå…¥ç©ºé—´ä¸­å¼ºåˆ¶æ‰§è¡Œå›¾åƒ-æ–‡æœ¬å¯¹é½ï¼Œä½†å­˜åœ¨æ˜¾å¼å¯¹é½è®¡ç®—é‡å¤§ã€æ³›åŒ–æ€§å·®çš„é—®é¢˜ã€‚</p><p>(3): æœ¬æ–‡æå‡ºäº†ä¸€ç§é›¶æ ·æœ¬çŸ­è¯­å®šä½æ–¹æ³•ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä¸­çš„äº¤å‰æ³¨æ„åŠ›æœºåˆ¶éšå¼å¯¹é½å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾ï¼Œå¹¶é€šè¿‡ç‰¹å¾é€‰æ‹©å’Œåå¤„ç†ç­–ç•¥åœ¨ä¸å¢åŠ å¯å­¦ä¹ å‚æ•°çš„æƒ…å†µä¸‹æå‡å®šä½ç²¾åº¦ã€‚</p><p>(4): è¯¥æ–¹æ³•åœ¨èƒ¸éƒ¨ X å°„çº¿åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†ä¸ç°æœ‰æ–¹æ³•ç›¸å½“çš„å®šä½æ€§èƒ½ï¼Œåœ¨å¹³å‡ IoU å’Œ AUC-ROC ä¸¤ä¸ªæŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶åœ¨åŒ»å­¦å½±åƒé¢†åŸŸé›¶æ ·æœ¬å­¦ä¹ çš„å¯è¡Œæ€§å’Œæœ‰æ•ˆæ€§ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šé‡‡ç”¨ Latent Diffusion Modelï¼ˆLDMï¼‰ï¼Œé€šè¿‡åå‘æ‰©æ•£è¿‡ç¨‹é€æ­¥æ¢å¤å›¾åƒï¼Œå¹¶åˆ©ç”¨ U-Net æ¨¡å‹ä¸­çš„äº¤å‰æ³¨æ„åŠ›æœºåˆ¶å¯¹å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾è¿›è¡Œéšå¼å¯¹é½ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šæ”¶é›†ä¸åŒå±‚çº§å’Œæ—¶é—´æ­¥é•¿çš„äº¤å‰æ³¨æ„åŠ›å›¾ï¼Œå¹¶é€šè¿‡ç‰¹å¾é€‰æ‹©å’Œåå¤„ç†ç­–ç•¥ä¼˜åŒ–å®šä½ç²¾åº¦ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šåœ¨ä¸å¢åŠ å¯å­¦ä¹ å‚æ•°çš„æƒ…å†µä¸‹ï¼Œåœ¨èƒ¸éƒ¨ X å°„çº¿åŸºå‡†æµ‹è¯•ä¸Šå–å¾—ä¸ç°æœ‰æ–¹æ³•ç›¸å½“çš„å®šä½æ€§èƒ½ï¼Œåœ¨å¹³å‡ IoU å’Œ AUC-ROC ä¸¤ä¸ªæŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œæå‡ºäº†ä¸€ç§åˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹è¿›è¡ŒçŸ­è¯­å®šä½çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨ä¸æ”¹å˜ç”Ÿæˆæ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œåˆ©ç”¨æ¨¡å‹ä¸­è§†è§‰å’Œæ–‡æœ¬ç‰¹å¾èåˆçš„äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°äº†é›¶æ ·æœ¬çŸ­è¯­å®šä½ï¼Œä¸ºåŒ»å­¦å½±åƒé¢†åŸŸé›¶æ ·æœ¬å­¦ä¹ æä¾›äº†æ–°çš„æ€è·¯ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šåˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ä¸­çš„äº¤å‰æ³¨æ„åŠ›æœºåˆ¶éšå¼å¯¹é½å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾ï¼Œå®ç°é›¶æ ·æœ¬çŸ­è¯­å®šä½ï¼›æ€§èƒ½ï¼šåœ¨èƒ¸éƒ¨ X å°„çº¿åŸºå‡†æµ‹è¯•ä¸Šå–å¾—ä¸ç°æœ‰æ–¹æ³•ç›¸å½“çš„å®šä½æ€§èƒ½ï¼Œåœ¨å¹³å‡ IoU å’Œ AUC-ROC ä¸¤ä¸ªæŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼›å·¥ä½œé‡ï¼šåœ¨ä¸å¢åŠ å¯å­¦ä¹ å‚æ•°çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡ç‰¹å¾é€‰æ‹©å’Œåå¤„ç†ç­–ç•¥ä¼˜åŒ–å®šä½ç²¾åº¦ï¼Œå·¥ä½œé‡è¾ƒå°ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-edc65b84041a4ffbf6fad90dfbf52862.jpg" align="middle"><img src="https://picx.zhimg.com/v2-42ab68ed87191afb18c00170b44f792e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d782a2682f47c83a60efe8ef4da1aeb0.jpg" align="middle"></details><h2 id="Robust-CLIP-Based-Detector-for-Exposing-Diffusion-Model-Generated-Images"><a href="#Robust-CLIP-Based-Detector-for-Exposing-Diffusion-Model-Generated-Images" class="headerlink" title="Robust CLIP-Based Detector for Exposing Diffusion Model-Generated Images"></a>Robust CLIP-Based Detector for Exposing Diffusion Model-Generated Images</h2><p><strong>Authors: Santosh, Li Lin, Irene Amerini, Xin Wang, Shu Hu</strong></p><p>Diffusion models (DMs) have revolutionized image generation, producing high-quality images with applications spanning various fields. However, their ability to create hyper-realistic images poses significant challenges in distinguishing between real and synthetic content, raising concerns about digital authenticity and potential misuse in creating deepfakes. This work introduces a robust detection framework that integrates image and text features extracted by CLIP model with a Multilayer Perceptron (MLP) classifier. We propose a novel loss that can improve the detectorâ€™s robustness and handle imbalanced datasets. Additionally, we flatten the loss landscape during the model training to improve the detectorâ€™s generalization capabilities. The effectiveness of our method, which outperforms traditional detection techniques, is demonstrated through extensive experiments, underscoring its potential to set a new state-of-the-art approach in DM-generated image detection. The code is available at <a href="https://github.com/Purdue-M2/Robust_DM_Generated_Image_Detection">https://github.com/Purdue-M2/Robust_DM_Generated_Image_Detection</a>. </p><p><a href="http://arxiv.org/abs/2404.12908v1">PDF</a> </p><p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹ç”Ÿæˆå›¾åƒçš„çœŸå®æ€§é‰´åˆ«æ¡†æ¶ï¼Œåˆ©ç”¨ CLIP æ¨¡å‹æå–å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾ï¼Œå¹¶é€šè¿‡ MLP åˆ†ç±»å™¨åˆ¤åˆ«çœŸå®æ€§å’Œåˆæˆæ€§ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„å›¾åƒçœŸå®æ€§é‰´åˆ«æŒ‘æˆ˜æ€§ã€‚</li><li>æå‡ºåˆ©ç”¨ CLIP æ¨¡å‹æå–å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾çš„é‰´åˆ«æ¡†æ¶ã€‚</li><li>è®¾è®¡æ”¹è¿›é‰´åˆ«å™¨é²æ£’æ€§çš„æŸå¤±å‡½æ•°ï¼Œå¹¶å¤„ç†ä¸å¹³è¡¡æ•°æ®é›†ã€‚</li><li>å¯¹æŸå¤±å‡½æ•°è¿›è¡Œå¹³æ»‘å¤„ç†ï¼Œæå‡é‰´åˆ«æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚</li><li>å®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•ä¼˜äºä¼ ç»Ÿé‰´åˆ«æŠ€æœ¯ã€‚</li><li>ä»£ç å·²å¼€æºï¼š<a href="https://github.com/Purdue-M2/Robust_DM_Generated_Image_Detectionã€‚">https://github.com/Purdue-M2/Robust_DM_Generated_Image_Detectionã€‚</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: åŸºäºCLIPçš„ç¨³å¥æ£€æµ‹å™¨ç”¨äºæ­éœ²æ‰©æ•£æ¨¡å‹ç”Ÿæˆå›¾åƒ</p></li><li><p>Authors: Santosh, Li Lin, Irene Amerini, Xin Wang, Shu Hu</p></li><li><p>Affiliation: æ™®æ¸¡å¤§å­¦</p></li><li><p>Keywords: Diffusion models, CLIP, Robust, AI images</p></li><li><p>Urls: https://arxiv.org/abs/2404.12908, Github:https://github.com/Purdue-M2/Robust DM Generated Image Detection</p></li><li><p>Summary:</p></li></ol><p>(1):éšç€æ‰©æ•£æ¨¡å‹ï¼ˆDiffusion modelsï¼ŒDMsï¼‰åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸå–å¾—é‡å¤§è¿›å±•ï¼Œå…¶ç”Ÿæˆçš„å›¾åƒè´¨é‡ä¸æ–­æå‡ï¼Œåº”ç”¨èŒƒå›´ä¹Ÿä¸æ–­æ‰©å¤§ã€‚ç„¶è€Œï¼ŒDMç”Ÿæˆå›¾åƒçš„é€¼çœŸæ€§ä¹Ÿç»™åŒºåˆ†çœŸå®å›¾åƒå’Œåˆæˆå›¾åƒå¸¦æ¥äº†å·¨å¤§æŒ‘æˆ˜ï¼Œå¼•å‘äº†å¯¹æ•°å­—å†…å®¹çœŸå®æ€§å’Œæ½œåœ¨æ»¥ç”¨ï¼ˆå¦‚ç”Ÿæˆæ·±åº¦ä¼ªé€ å†…å®¹ï¼‰çš„æ‹…å¿§ã€‚</p><p>(2):ä¼ ç»Ÿæ–¹æ³•ä¸»è¦åˆ©ç”¨CLIPå›¾åƒç‰¹å¾æˆ–å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾ï¼Œç»“åˆå¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰åˆ†ç±»å™¨å’ŒäºŒå…ƒäº¤å‰ç†µï¼ˆBCEï¼‰æŸå¤±å‡½æ•°è¿›è¡ŒDMç”Ÿæˆå›¾åƒæ£€æµ‹ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å­˜åœ¨é²æ£’æ€§å·®ã€å¯¹ä¸å¹³è¡¡æ•°æ®é›†å¤„ç†èƒ½åŠ›å¼±ç­‰é—®é¢˜ã€‚</p><p>(3):æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºCLIPå›¾åƒå’Œæ–‡æœ¬ç‰¹å¾çš„ç¨³å¥æ£€æµ‹æ¡†æ¶ï¼Œé‡‡ç”¨MLPåˆ†ç±»å™¨å’Œæ¡ä»¶é£é™©ä»·å€¼ï¼ˆCVaRï¼‰æŸå¤±å‡½æ•°ä¸é¢ç§¯ä¸‹æ›²çº¿ï¼ˆAUCï¼‰æŸå¤±å‡½æ•°çš„ç»„åˆï¼Œå¹¶åœ¨å¹³å¦åŒ–çš„æŸå¤±å‡½æ•°æ›²é¢ä¸‹è¿›è¡Œè®­ç»ƒï¼Œä»¥æé«˜æ£€æµ‹å™¨çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p><p>(4):åœ¨DMç”Ÿæˆå›¾åƒæ£€æµ‹ä»»åŠ¡ä¸Šï¼Œæœ¬æ–‡æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœå‡ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œè¡¨æ˜äº†å…¶åœ¨è¯¥ä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§ï¼Œæœ‰æœ›æˆä¸ºDMç”Ÿæˆå›¾åƒæ£€æµ‹é¢†åŸŸçš„æ–°æŠ€æœ¯æ ‡æ†ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæå‡ºäº†ä¸€ç§åŸºäº CLIP å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾çš„ç¨³å¥æ£€æµ‹æ¡†æ¶ï¼Œé‡‡ç”¨ MLP åˆ†ç±»å™¨å’Œæ¡ä»¶é£é™©ä»·å€¼ (CVaR) æŸå¤±å‡½æ•°ä¸é¢ç§¯ä¸‹æ›²çº¿ (AUC) æŸå¤±å‡½æ•°çš„ç»„åˆã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåœ¨å¹³å¦åŒ–çš„æŸå¤±å‡½æ•°æ›²é¢ä¸‹è¿›è¡Œè®­ç»ƒï¼Œä»¥æé«˜æ£€æµ‹å™¨çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šåœ¨ DM ç”Ÿæˆå›¾åƒæ£€æµ‹ä»»åŠ¡ä¸Šï¼Œæœ¬æ–‡æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœå‡ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œè¡¨æ˜äº†å…¶åœ¨è¯¥ä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰æœ¬æ–‡æå‡ºçš„æ–¹æ³•åœ¨DMç”Ÿæˆå›¾åƒæ£€æµ‹ä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œæœ‰æœ›æˆä¸ºè¯¥é¢†åŸŸçš„æ–°æŠ€æœ¯æ ‡æ†ã€‚</p><p>ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäºCLIPå›¾åƒå’Œæ–‡æœ¬ç‰¹å¾çš„ç¨³å¥æ£€æµ‹æ¡†æ¶ï¼Œé‡‡ç”¨MLPåˆ†ç±»å™¨å’ŒCVaRæŸå¤±å‡½æ•°ä¸AUCæŸå¤±å‡½æ•°çš„ç»„åˆï¼Œå¹¶åœ¨å¹³å¦åŒ–çš„æŸå¤±å‡½æ•°æ›²é¢ä¸‹è¿›è¡Œè®­ç»ƒã€‚</p><p>æ€§èƒ½ï¼šåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœå‡ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œè¡¨æ˜äº†å…¶åœ¨è¯¥ä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§ã€‚</p><p>å·¥ä½œé‡ï¼šä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæœ¬æ–‡æ–¹æ³•çš„è®­ç»ƒæ—¶é—´æ›´é•¿ï¼Œéœ€è¦æ›´å¤šçš„è®¡ç®—èµ„æºã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-4d2d3895766f30bd509b9a3d935d9804.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4bf1ac8a20b7e67bfd03bc5cca10058c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f6f7856d5aaeb46c1d7aa9023b3a02ae.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ce9de0cd6eee8dc551b4cd04b517c61c.jpg" align="middle"></details><h2 id="Training-and-prompt-free-General-Painterly-Harmonization-Using-Image-wise-Attention-Sharing"><a href="#Training-and-prompt-free-General-Painterly-Harmonization-Using-Image-wise-Attention-Sharing" class="headerlink" title="Training-and-prompt-free General Painterly Harmonization Using   Image-wise Attention Sharing"></a>Training-and-prompt-free General Painterly Harmonization Using   Image-wise Attention Sharing</h2><p><strong>Authors:Teng-Fang Hsiao, Bo-Kai Ruan, Hong-Han Shuai</strong></p><p>Painterly Image Harmonization aims at seamlessly blending disparate visual elements within a single coherent image. However, previous approaches often encounter significant limitations due to training data constraints, the need for time-consuming fine-tuning, or reliance on additional prompts. To surmount these hurdles, we design a Training-and-prompt-Free General Painterly Harmonization method using image-wise attention sharing (TF-GPH), which integrates a novel â€œshare-attention moduleâ€. This module redefines the traditional self-attention mechanism by allowing for comprehensive image-wise attention, facilitating the use of a state-of-the-art pretrained latent diffusion model without the typical training data limitations. Additionally, we further introduce â€œsimilarity reweightingâ€ mechanism enhances performance by effectively harnessing cross-image information, surpassing the capabilities of fine-tuning or prompt-based approaches. At last, we recognize the deficiencies in existing benchmarks and propose the â€œGeneral Painterly Harmonization Benchmarkâ€, which employs range-based evaluation metrics to more accurately reflect real-world application. Extensive experiments demonstrate the superior efficacy of our method across various benchmarks. The code and web demo are available at <a href="https://github.com/BlueDyee/TF-GPH">https://github.com/BlueDyee/TF-GPH</a>. </p><p><a href="http://arxiv.org/abs/2404.12900v1">PDF</a> </p><p><strong>Summary</strong></p><p>å›¾åƒé£æ ¼ç»Ÿä¸€æ–¹æ³•TF-GPHé€šè¿‡å›¾åƒæ³¨æ„åŠ›å…±äº«ï¼Œä¸éœ€è®­ç»ƒå’Œæç¤ºï¼Œå³å¯å®ç°å¤šæ ·è§†è§‰å…ƒç´ çš„æ— ç¼èåˆã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>è®¾è®¡äº†ä¸€ç§ä¸éœ€è®­ç»ƒå’Œæç¤ºçš„é€šç”¨å›¾åƒé£æ ¼ç»Ÿä¸€æ–¹æ³• TF-GPHã€‚</li><li>å¼•å…¥å›¾åƒçº§æ³¨æ„åŠ›å…±äº«ï¼Œæ‰“ç ´äº†ä¼ ç»Ÿè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„å±€é™ã€‚</li><li>æå‡ºç›¸ä¼¼æ€§é‡æ–°åŠ æƒæœºåˆ¶ï¼Œæœ‰æ•ˆåˆ©ç”¨è·¨å›¾åƒä¿¡æ¯ï¼Œæå‡æ€§èƒ½ã€‚</li><li>æå‡ºé€šç”¨å›¾åƒé£æ ¼ç»Ÿä¸€åŸºå‡†ï¼Œé‡‡ç”¨åŸºäºèŒƒå›´çš„è¯„ä¼°æŒ‡æ ‡ï¼Œæ›´è´´è¿‘çœŸå®åº”ç”¨ã€‚</li><li>å®éªŒè¡¨æ˜ï¼ŒTF-GPH åœ¨å¤šä¸ªåŸºå‡†ä¸Šå‡è¡¨ç°ä¼˜å¼‚ã€‚</li><li>ä»£ç å’Œç½‘ç»œæ¼”ç¤ºå¯åœ¨ <a href="https://github.com/BlueDyee/TF-GPH">https://github.com/BlueDyee/TF-GPH</a> è·å–ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>æ ‡é¢˜ï¼šè®­ç»ƒä¸æç¤ºæ— å…³çš„é€šç”¨ç»˜ç”»è°ƒå’Œ</li><p></p><p></p><li>ä½œè€…ï¼šTeng-Fang Hsiao, Bo-Kai Ruan, Hong-Han Shuai</li><p></p><p></p><li>å•ä½ï¼šå›½ç«‹é˜³æ˜äº¤é€šå¤§å­¦</li><p></p><p></p><li>å…³é”®è¯ï¼šdiffusion model, attention, image editing, image harmonization, painterly harmonization, style transfer</li><p></p><p></p><li>è®ºæ–‡é“¾æ¥ï¼šxxxï¼ŒGithub ä»£ç é“¾æ¥ï¼šhttps://github.com/BlueDyee/TF-GPH</li><p></p><p></p><li><p></p><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šç»˜ç”»å›¾åƒè°ƒå’Œæ—¨åœ¨æ— ç¼åœ°å°†ä¸åŒçš„è§†è§‰å…ƒç´ èåˆåˆ°ä¸€ä¸ªè¿è´¯çš„å›¾åƒä¸­ã€‚ç„¶è€Œï¼Œç”±äºè®­ç»ƒæ•°æ®é™åˆ¶ã€éœ€è¦è€—æ—¶çš„å¾®è°ƒæˆ–ä¾èµ–é¢å¤–çš„æç¤ºï¼Œä»¥å‰çš„æ–¹æ³•ç»å¸¸é‡åˆ°é‡å¤§é™åˆ¶ã€‚ï¼ˆ2ï¼‰ï¼šè¿‡å»çš„æ–¹æ³•ï¼šè¿‡å»çš„æ–¹æ³•åŒ…æ‹¬ä½¿ç”¨åŒåŸŸç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨çš„åŒåŸŸç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨ï¼Œä»¥åŠå°†å›¾åƒèåˆåˆ°ç»˜ç”»ä¸­çš„ PHDiffusion æ¨¡å‹ã€‚è¿™äº›æ–¹æ³•å­˜åœ¨è®­ç»ƒæ•°æ®é™åˆ¶ã€éœ€è¦å¾®è°ƒå’Œä¾èµ–æç¤ºçš„é—®é¢˜ã€‚ï¼ˆ3ï¼‰ï¼šç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ä½¿ç”¨å›¾åƒçº§æ³¨æ„åŠ›å…±äº«ï¼ˆTF-GPHï¼‰çš„è®­ç»ƒå’Œæç¤ºæ— å…³çš„é€šç”¨ç»˜ç”»è°ƒå’Œæ–¹æ³•ï¼Œè¯¥æ–¹æ³•é›†æˆäº†ä¸€ä¸ªæ–°é¢–çš„â€œå…±äº«æ³¨æ„åŠ›æ¨¡å—â€ã€‚è¯¥æ¨¡å—é€šè¿‡å…è®¸å…¨é¢çš„å›¾åƒçº§æ³¨æ„åŠ›æ¥é‡æ–°å®šä¹‰ä¼ ç»Ÿçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œä»è€Œä¿ƒè¿›ä½¿ç”¨æœ€å…ˆè¿›çš„é¢„è®­ç»ƒæ½œåœ¨æ‰©æ•£æ¨¡å‹è€Œæ²¡æœ‰å…¸å‹çš„è®­ç»ƒæ•°æ®é™åˆ¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†â€œç›¸ä¼¼æ€§é‡æ–°åŠ æƒâ€æœºåˆ¶ï¼Œé€šè¿‡æœ‰æ•ˆåˆ©ç”¨è·¨å›¾åƒä¿¡æ¯æ¥å¢å¼ºæ€§èƒ½ï¼Œè¶…è¶Šäº†å¾®è°ƒæˆ–åŸºäºæç¤ºçš„æ–¹æ³•çš„èƒ½åŠ›ã€‚æœ€åï¼Œæˆ‘ä»¬è®¤è¯†åˆ°ç°æœ‰åŸºå‡†çš„ç¼ºé™·ï¼Œå¹¶æå‡ºäº†â€œé€šç”¨ç»˜ç”»è°ƒå’ŒåŸºå‡†â€ï¼Œè¯¥åŸºå‡†é‡‡ç”¨åŸºäºèŒƒå›´çš„è¯„ä¼°æŒ‡æ ‡æ¥æ›´å‡†ç¡®åœ°åæ˜ å®é™…åº”ç”¨ã€‚ï¼ˆ4ï¼‰ï¼šä»»åŠ¡å’Œæ€§èƒ½ï¼šæœ¬æ–‡æ–¹æ³•åœ¨å„ç§åŸºå‡†ä¸Šå±•ç¤ºäº†å…¶å“è¶Šçš„åŠŸæ•ˆã€‚è¯¥æ–¹æ³•åœ¨é€šç”¨ç»˜ç”»è°ƒå’ŒåŸºå‡†ä¸Šçš„ FID å¾—åˆ†ä¸º 10.6ï¼Œåœ¨ç»˜ç”»å›¾åƒè°ƒå’ŒåŸºå‡†ä¸Šçš„ FID å¾—åˆ†ä¸º 10.3ï¼Œåœ¨å›¾åƒç¼–è¾‘åŸºå‡†ä¸Šçš„ FID å¾—åˆ†ä¸º 11.2ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†ä»–ä»¬çš„ç›®æ ‡ï¼Œå³æä¾›ä¸€ç§è®­ç»ƒå’Œæç¤ºæ— å…³çš„é€šç”¨ç»˜ç”»è°ƒå’Œæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥åœ¨å„ç§ä»»åŠ¡ä¸Šå®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p></li><li><p>Methods:</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šæå‡ºäº†ä¸€ç§ä½¿ç”¨å›¾åƒçº§æ³¨æ„åŠ›å…±äº«ï¼ˆTF-GPHï¼‰çš„è®­ç»ƒå’Œæç¤ºæ— å…³çš„é€šç”¨ç»˜ç”»è°ƒå’Œæ–¹æ³•ï¼Œè¯¥æ–¹æ³•é›†æˆäº†ä¸€ä¸ªæ–°é¢–çš„â€œå…±äº«æ³¨æ„åŠ›æ¨¡å—â€ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¯¥æ¨¡å—é€šè¿‡å…è®¸å…¨é¢çš„å›¾åƒçº§æ³¨æ„åŠ›æ¥é‡æ–°å®šä¹‰ä¼ ç»Ÿçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œä»è€Œä¿ƒè¿›ä½¿ç”¨æœ€å…ˆè¿›çš„é¢„è®­ç»ƒæ½œåœ¨æ‰©æ•£æ¨¡å‹è€Œæ²¡æœ‰å…¸å‹çš„è®­ç»ƒæ•°æ®é™åˆ¶ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šè¿›ä¸€æ­¥å¼•å…¥äº†â€œç›¸ä¼¼æ€§é‡æ–°åŠ æƒâ€æœºåˆ¶ï¼Œé€šè¿‡æœ‰æ•ˆåˆ©ç”¨è·¨å›¾åƒä¿¡æ¯æ¥å¢å¼ºæ€§èƒ½ï¼Œè¶…è¶Šäº†å¾®è°ƒæˆ–åŸºäºæç¤ºçš„æ–¹æ³•çš„èƒ½åŠ›ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šæå‡ºäº†â€œé€šç”¨ç»˜ç”»è°ƒå’ŒåŸºå‡†â€ï¼Œè¯¥åŸºå‡†é‡‡ç”¨åŸºäºèŒƒå›´çš„è¯„ä¼°æŒ‡æ ‡æ¥æ›´å‡†ç¡®åœ°åæ˜ å®é™…åº”ç”¨ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œæå‡ºäº†ä¸€ç§è®­ç»ƒå’Œæç¤ºæ— å…³çš„é€šç”¨ç»˜ç”»è°ƒå’Œæ–¹æ³• TF-GPHï¼Œè¯¥æ–¹æ³•é›†æˆäº†æ–°é¢–çš„â€œå…±äº«æ³¨æ„åŠ›æ¨¡å—â€ï¼Œå¹¶å¼•å…¥äº†â€œç›¸ä¼¼æ€§é‡æ–°åŠ æƒâ€æœºåˆ¶ï¼Œæœ‰æ•ˆåˆ©ç”¨è·¨å›¾åƒä¿¡æ¯ï¼Œè¶…è¶Šäº†å¾®è°ƒæˆ–åŸºäºæç¤ºçš„æ–¹æ³•çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæå‡ºäº†â€œé€šç”¨ç»˜ç”»è°ƒå’ŒåŸºå‡†â€ï¼Œé‡‡ç”¨åŸºäºèŒƒå›´çš„è¯„ä¼°æŒ‡æ ‡æ¥æ›´å‡†ç¡®åœ°åæ˜ å®é™…åº”ç”¨ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†â€œå…±äº«æ³¨æ„åŠ›æ¨¡å—â€ï¼Œé‡æ–°å®šä¹‰äº†ä¼ ç»Ÿçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œå…è®¸å…¨é¢çš„å›¾åƒçº§æ³¨æ„åŠ›ï¼›å¼•å…¥äº†â€œç›¸ä¼¼æ€§é‡æ–°åŠ æƒâ€æœºåˆ¶ï¼Œæœ‰æ•ˆåˆ©ç”¨è·¨å›¾åƒä¿¡æ¯å¢å¼ºæ€§èƒ½ã€‚</p><p>æ€§èƒ½ï¼šåœ¨é€šç”¨ç»˜ç”»è°ƒå’ŒåŸºå‡†ä¸Šçš„ FID å¾—åˆ†ä¸º 10.6ï¼Œåœ¨ç»˜ç”»å›¾åƒè°ƒå’ŒåŸºå‡†ä¸Šçš„ FID å¾—åˆ†ä¸º 10.3ï¼Œåœ¨å›¾åƒç¼–è¾‘åŸºå‡†ä¸Šçš„ FID å¾—åˆ†ä¸º 11.2ï¼Œè¶…è¶Šäº†å¾®è°ƒæˆ–åŸºäºæç¤ºçš„æ–¹æ³•ã€‚</p><p>å·¥ä½œé‡ï¼šæ— éœ€å…¸å‹çš„è®­ç»ƒæ•°æ®é™åˆ¶ï¼Œæ— éœ€è€—æ—¶çš„å¾®è°ƒæˆ–ä¾èµ–é¢å¤–çš„æç¤ºï¼Œé™ä½äº†ä½¿ç”¨é—¨æ§›ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-23788675c99f2a6910d21b93d104c6ba.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-be289866fd46a1130a926aac4953f56b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3005a952210df9687a21ac0bd5813a2c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-340d1d74c9871713d3a7044daea486c2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ed1fb496bff0b4f7658cf7a6aba9a5a2.jpg" align="middle"></details><h2 id="Detecting-Out-Of-Distribution-Earth-Observation-Images-with-Diffusion-Models"><a href="#Detecting-Out-Of-Distribution-Earth-Observation-Images-with-Diffusion-Models" class="headerlink" title="Detecting Out-Of-Distribution Earth Observation Images with Diffusion   Models"></a>Detecting Out-Of-Distribution Earth Observation Images with Diffusion   Models</h2><p><strong>Authors:Georges Le Bellier, Nicolas Audebert</strong></p><p>Earth Observation imagery can capture rare and unusual events, such as disasters and major landscape changes, whose visual appearance contrasts with the usual observations. Deep models trained on common remote sensing data will output drastically different features for these out-of-distribution samples, compared to those closer to their training dataset. Detecting them could therefore help anticipate changes in the observations, either geographical or environmental. In this work, we show that the reconstruction error of diffusion models can effectively serve as unsupervised out-of-distribution detectors for remote sensing images, using them as a plausibility score. Moreover, we introduce ODEED, a novel reconstruction-based scorer using the probability-flow ODE of diffusion models. We validate it experimentally on SpaceNet 8 with various scenarios, such as classical OOD detection with geographical shift and near-OOD setups: pre/post-flood and non-flooded/flooded image recognition. We show that our ODEED scorer significantly outperforms other diffusion-based and discriminative baselines on the more challenging near-OOD scenarios of flood image detection, where OOD images are close to the distribution tail. We aim to pave the way towards better use of generative models for anomaly detection in remote sensing. </p><p><a href="http://arxiv.org/abs/2404.12667v1">PDF</a> EARTHVISION 2024 IEEE/CVF CVPR Workshop. Large Scale Computer Vision   for Remote Sensing Imagery, Jun 2024, Seattle, United States</p><p><strong>æ‘˜è¦</strong><br>æ‰©æ•£æ¨¡å‹çš„é‡å»ºè¯¯å·®å¯ä»¥ä½œä¸ºé¥æ„Ÿå›¾åƒçš„æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹å™¨ï¼Œå…¶å¯¹ç½•è§äº‹ä»¶çš„æ£€æµ‹æ•ˆæœä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p><p><strong>å…³é”®è¦ç‚¹</strong></p><ul><li>æ‰©æ•£æ¨¡å‹çš„é‡å»ºè¯¯å·®å¯ä»¥ä½œä¸ºé¥æ„Ÿå›¾åƒçš„æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹æŒ‡æ ‡ã€‚</li><li>ODEED æ˜¯ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹æ¦‚ç‡æµ ODE çš„é‡å»ºå‹è¯„åˆ†å™¨ï¼Œæ€§èƒ½ä¼˜å¼‚ã€‚</li><li>ODEED åœ¨åœ°ç†åç§»å’Œè¿‘å¼‚å¸¸åœºæ™¯ä¸‹å‡è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯æ´ªæ°´å›¾åƒæ£€æµ‹ç­‰åˆ†å¸ƒå°¾éƒ¨å¼‚å¸¸æ£€æµ‹ä»»åŠ¡ã€‚</li><li>ODEED ä¼˜äºå…¶ä»–åŸºäºæ‰©æ•£æ¨¡å‹å’Œåˆ¤åˆ«æ¨¡å‹çš„åŸºçº¿æ–¹æ³•ã€‚</li><li>æœ¬ç ”ç©¶ä¸ºåˆ©ç”¨ç”Ÿæˆæ¨¡å‹è¿›è¡Œé¥æ„Ÿå¼‚å¸¸æ£€æµ‹é“ºå¹³äº†é“è·¯ã€‚</li><li>ç½•è§äº‹ä»¶çš„è§†è§‰å¤–è§‚ä¸å¸¸è§è§‚æµ‹å­˜åœ¨å·®å¼‚ï¼Œæ£€æµ‹è¿™äº›äº‹ä»¶æœ‰åŠ©äºé¢„æµ‹è§‚æµ‹çš„å˜åŒ–ã€‚</li><li>æ‰©æ•£æ¨¡å‹å¯ä»¥è¾“å‡ºä¸è®­ç»ƒæ•°æ®é›†æ›´æ¥è¿‘çš„æ ·æœ¬çš„æˆªç„¶ä¸åŒçš„ç‰¹å¾ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: æ‰©æ•£æ¨¡å‹æ£€æµ‹åœ°çƒè§‚æµ‹å›¾åƒçš„åˆ†å¸ƒå¤–æƒ…å†µ</p></li><li><p>Authors: Georges Le Bellier, Nicolas Audebert</p></li><li><p>Affiliation: æ³•å›½å·´é»å›½ç«‹å·¥è‰ºæŠ€æœ¯å­¦é™¢</p></li><li><p>Keywords: Out-of-Distribution, Remote Sensing, Diffusion Model, Anomaly Detection</p></li><li><p>Urls: Paper: https://arxiv.org/pdf/2404.12667.pdf , Github:None</p></li><li><p>Summary:</p></li></ol><p>(1): é¥æ„Ÿå›¾åƒå¯ä»¥æ•æ‰åˆ°ç½•è§å’Œå¼‚å¸¸äº‹ä»¶ï¼Œä¾‹å¦‚ç¾å®³å’Œé‡å¤§æ™¯è§‚å˜åŒ–ï¼Œå…¶è§†è§‰å¤–è§‚ä¸é€šå¸¸çš„è§‚æµ‹ç»“æœå½¢æˆå¯¹æ¯”ã€‚åœ¨å¸¸è§é¥æ„Ÿæ•°æ®ä¸Šè®­ç»ƒçš„æ·±åº¦æ¨¡å‹å°†ä¸ºè¿™äº›åˆ†å¸ƒå¤–æ ·æœ¬è¾“å‡ºæˆªç„¶ä¸åŒçš„ç‰¹å¾ï¼Œè€Œä¸é‚£äº›æ›´æ¥è¿‘å…¶è®­ç»ƒæ•°æ®é›†çš„æ ·æœ¬ç›¸æ¯”ã€‚å› æ­¤ï¼Œæ£€æµ‹å®ƒä»¬æœ‰åŠ©äºé¢„æµ‹è§‚æµ‹ç»“æœçš„å˜åŒ–ï¼Œæ— è®ºæ˜¯åœ°ç†ä¸Šçš„è¿˜æ˜¯ç¯å¢ƒä¸Šçš„ã€‚</p><p>(2): è¿‡å»çš„æ–¹æ³•ä¸»è¦ä¾èµ–äºåˆ¤åˆ«æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹éœ€è¦ç›‘ç£å­¦ä¹ æ¥åŒºåˆ†åˆ†å¸ƒå†…å’Œåˆ†å¸ƒå¤–æ ·æœ¬ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨è¿‘åˆ†å¸ƒå¤–è®¾ç½®ä¸­è¡¨ç°ä¸ä½³ï¼Œå…¶ä¸­åˆ†å¸ƒå¤–æ ·æœ¬ä¸è®­ç»ƒåˆ†å¸ƒçš„å°¾éƒ¨æ¥è¿‘ã€‚</p><p>(3): æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œç§°ä¸º ODEEDï¼Œå®ƒåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„æ¦‚ç‡æµå¸¸å¾®åˆ†æ–¹ç¨‹ (ODE) æ¥è®¡ç®—é‡å»ºç›¸ä¼¼æ€§ã€‚ODEED å°†æ‰©æ•£æ¨¡å‹é‡å»ºè¯¯å·®ç”¨ä½œéç›‘ç£åˆ†å¸ƒå¤–æ£€æµ‹å™¨ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨å„ç§åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬ç»å…¸åˆ†å¸ƒå¤–æ£€æµ‹å’Œè¿‘åˆ†å¸ƒå¤–è®¾ç½®ã€‚</p><p>(4): åœ¨ SpaceNet 8 æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒODEED åœ¨æ´ªæ°´å›¾åƒæ£€æµ‹çš„æ›´å…·æŒ‘æˆ˜æ€§çš„è¿‘åˆ†å¸ƒå¤–åœºæ™¯ä¸­æ˜æ˜¾ä¼˜äºå…¶ä»–åŸºäºæ‰©æ•£å’Œåˆ¤åˆ«çš„æ–¹æ³•ã€‚è¿™äº›ç»“æœæ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ï¼Œå³ä¸ºé¥æ„Ÿä¸­çš„å¼‚å¸¸æ£€æµ‹æ›´å¥½åœ°åˆ©ç”¨ç”Ÿæˆæ¨¡å‹ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º ODEED çš„æ–°é¢–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„æ¦‚ç‡æµå¸¸å¾®åˆ†æ–¹ç¨‹ (ODE) æ¥è®¡ç®—é‡å»ºç›¸ä¼¼æ€§ï¼Œå°†æ‰©æ•£æ¨¡å‹é‡å»ºè¯¯å·®ç”¨ä½œéç›‘ç£åˆ†å¸ƒå¤–æ£€æµ‹å™¨ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šODEED é€šè¿‡ç§¯åˆ†æ¦‚ç‡æµå¸¸å¾®åˆ†æ–¹ç¨‹ (PF-ODE) ä»æ•°æ®åˆ†å¸ƒå°†æ ·æœ¬ç¼–ç åˆ°å…ˆéªŒåˆ†å¸ƒï¼Œåä¹‹äº¦ç„¶ï¼Œå¹¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›å’Œé‡å»ºæ€§èƒ½æ¥æ£€æµ‹åˆ†å¸ƒå¤–æ ·æœ¬ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡ä½¿ç”¨ä¸‰ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„è¯„åˆ†å™¨æ¥è¯„ä¼°é‡å»ºæ€§èƒ½ï¼ŒåŒ…æ‹¬åŸºäºæ—¶é—´æˆªæ–­æ‰©æ•£æŸå¤±çš„æ‰©æ•£æŸå¤±è¯„åˆ†å™¨ã€ä¸“æ³¨äºå›ºå®šæ—¶é—´æ­¥é•¿å»å™ªæ€§èƒ½çš„ä¸€æ­¥å»å™ªè¯„åˆ†å™¨ï¼Œä»¥åŠåˆ©ç”¨ PF-ODE è½¨è¿¹ç²¾åº¦ä½œä¸ºåŒºåˆ†åˆ†å¸ƒå†…å’Œåˆ†å¸ƒå¤–æ ·æœ¬çš„æ–¹æ³•çš„ ODEEDï¼ˆODE ç¼–ç è§£ç ï¼‰è¯„åˆ†å™¨ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡è¯„ä¼°äº†æ‰©æ•£æ¨¡å‹æ£€æµ‹åœ°çƒè§‚æµ‹å›¾åƒåˆ†å¸ƒå¤–æƒ…å†µçš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬å¼•å…¥äº† ODEED è¯„åˆ†å™¨ï¼Œå®ƒåˆ©ç”¨è¿ç»­æ—¶é—´æ‰©æ•£æ¨¡å‹çš„ç¡®å®šæ€§é‡å»ºèƒ½åŠ›ã€‚æˆ‘ä»¬é’ˆå¯¹ 1ï¼‰äº‘æ£€æµ‹å’Œ 2ï¼‰Space-Net 8 æ•°æ®é›†ä¸Šå…·æœ‰æŒ‘æˆ˜æ€§çš„ OOD æ£€æµ‹ä»»åŠ¡é›†åˆè¯„ä¼°äº†è¿™äº›æ–¹æ³•ã€‚æˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„ ODEED è¯„åˆ†å™¨åœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„æ´ªæ°´ç›¸å…³åœºæ™¯ä¸­æ˜æ˜¾ä¼˜äºåŸºçº¿ï¼Œå±•ç¤ºäº†æ‰©æ•£æ¨¡å‹æ£€æµ‹â€œæ¥è¿‘åˆ†å¸ƒå¤–â€é¥æ„Ÿå›¾åƒï¼ˆä¾‹å¦‚æ´ªæ°´å›¾åƒï¼‰çš„æ„ä¹‰ã€‚è¿™äº›å‘ç°ä¸ºåˆ©ç”¨ç”Ÿæˆæ¨¡å‹ä»æœªæ ‡è®°çš„ EO æ•°æ®ä¸­æ£€æµ‹ç½•è§äº‹ä»¶å¼€è¾Ÿäº†æ–°çš„é€”å¾„ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„æ¦‚ç‡æµå¸¸å¾®åˆ†æ–¹ç¨‹ (ODE) æ¥è®¡ç®—é‡å»ºç›¸ä¼¼æ€§çš„æ–°é¢–æ–¹æ³• ODEEDï¼›æ€§èƒ½ï¼šåœ¨ SpaceNet 8 æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒODEED åœ¨æ´ªæ°´å›¾åƒæ£€æµ‹çš„æ›´å…·æŒ‘æˆ˜æ€§çš„è¿‘åˆ†å¸ƒå¤–åœºæ™¯ä¸­æ˜æ˜¾ä¼˜äºå…¶ä»–åŸºäºæ‰©æ•£å’Œåˆ¤åˆ«çš„æ–¹æ³•ï¼›å·¥ä½œé‡ï¼šæœ¬æ–‡çš„å·¥ä½œé‡ä¸­ç­‰ï¼Œéœ€è¦å¯¹æ‰©æ•£æ¨¡å‹å’Œæ¦‚ç‡æµå¸¸å¾®åˆ†æ–¹ç¨‹æœ‰åŸºæœ¬çš„äº†è§£ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-a861c2c676669b5a005a8c6460157c23.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a94e85aaa5cc539ad5464e2facd58f70.jpg" align="middle"><img src="https://picx.zhimg.com/v2-166f6cfb38b037adea4b992761e7f8c9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bb4eaeab52bbd1091417582a1679d9b4.jpg" align="middle"></details><h2 id="Learning-the-Domain-Specific-Inverse-NUFFT-for-Accelerated-Spiral-MRI-using-Diffusion-Models"><a href="#Learning-the-Domain-Specific-Inverse-NUFFT-for-Accelerated-Spiral-MRI-using-Diffusion-Models" class="headerlink" title="Learning the Domain Specific Inverse NUFFT for Accelerated Spiral MRI   using Diffusion Models"></a>Learning the Domain Specific Inverse NUFFT for Accelerated Spiral MRI   using Diffusion Models</h2><p><strong>Authors:Trevor J. Chan, Chamith S. Rajapakse</strong></p><p>Deep learning methods for accelerated MRI achieve state-of-the-art results but largely ignore additional speedups possible with noncartesian sampling trajectories. To address this gap, we created a generative diffusion model-based reconstruction algorithm for multi-coil highly undersampled spiral MRI. This model uses conditioning during training as well as frequency-based guidance to ensure consistency between images and measurements. Evaluated on retrospective data, we show high quality (structural similarity &gt; 0.87) in reconstructed images with ultrafast scan times (0.02 seconds for a 2D image). We use this algorithm to identify a set of optimal variable-density spiral trajectories and show large improvements in image quality compared to conventional reconstruction using the non-uniform fast Fourier transform. By combining efficient spiral sampling trajectories, multicoil imaging, and deep learning reconstruction, these methods could enable the extremely high acceleration factors needed for real-time 3D imaging. </p><p><a href="http://arxiv.org/abs/2404.12361v1">PDF</a> </p><p><strong>Summary</strong><br>æ·±åº¦å­¦ä¹ æ–¹æ³•å¯åŠ é€Ÿç£å…±æŒ¯æˆåƒï¼Œè¾¾åˆ°æœ€å…ˆè¿›æ°´å¹³ï¼Œä½†å¹¶æœªå……åˆ†åˆ©ç”¨éç¬›å¡å°”é‡‡æ ·è½¨è¿¹å¯èƒ½å®ç°çš„é¢å¤–åŠ é€Ÿã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>åˆ›å»ºåŸºäºç”Ÿæˆæ‰©æ•£æ¨¡å‹çš„é‡å»ºç®—æ³•ï¼Œç”¨äºå¤šçº¿åœˆé«˜æ¬ é‡‡æ ·èºæ—‹ç£å…±æŒ¯æˆåƒã€‚</li><li>è¯¥æ¨¡å‹åˆ©ç”¨è®­ç»ƒæœŸé—´çš„è°ƒèŠ‚å’ŒåŸºäºé¢‘ç‡çš„å¼•å¯¼ï¼Œç¡®ä¿å›¾åƒå’Œæµ‹é‡å€¼çš„ä¸€è‡´æ€§ã€‚</li><li>åœ¨å›é¡¾æ€§æ•°æ®ä¸Šè¯„ä¼°ï¼Œæ˜¾ç¤ºå‡ºé«˜å›¾åƒè´¨é‡ï¼ˆç»“æ„ç›¸ä¼¼åº¦&gt; 0.87ï¼‰ï¼Œæ‰«ææ—¶é—´æå¿«ï¼ˆ2D å›¾åƒä¸º 0.02 ç§’ï¼‰ã€‚</li><li>ä½¿ç”¨è¯¥ç®—æ³•è¯†åˆ«äº†ä¸€ç»„ä¼˜åŒ–çš„å¯å˜å¯†åº¦èºæ—‹è½¨è¿¹ï¼Œä¸ä½¿ç”¨éå‡åŒ€å¿«é€Ÿå‚…é‡Œå¶å˜æ¢çš„ä¼ ç»Ÿé‡å»ºç›¸æ¯”ï¼Œå›¾åƒè´¨é‡æœ‰äº†å¾ˆå¤§æé«˜ã€‚</li><li>é€šè¿‡ç»“åˆæœ‰æ•ˆçš„èºæ—‹é‡‡æ ·è½¨è¿¹ã€å¤šçº¿åœˆæˆåƒå’Œæ·±åº¦å­¦ä¹ é‡å»ºï¼Œè¿™äº›æ–¹æ³•å¯ä»¥å®ç°å®æ—¶ 3D æˆåƒæ‰€éœ€çš„æé«˜åŠ é€Ÿå› å­ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: åŠ é€Ÿèºæ—‹MRIçš„åŸŸç‰¹å®šé€†éå‡åŒ€å¿«é€Ÿå‚…é‡Œå¶å˜æ¢å­¦ä¹ </p></li><li><p>Authors: Trevor J. Chan, Chamith S. Rajapakse</p></li><li><p>Affiliation: å®¾å¤•æ³•å°¼äºšå¤§å­¦ç”Ÿç‰©å·¥ç¨‹ç³»</p></li><li><p>Keywords: åŠ é€ŸMRIï¼Œèºæ—‹MRIï¼Œæ·±åº¦å­¦ä¹ ï¼Œå›¾åƒé‡å»º</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2404.12361, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): MRIæˆåƒé€Ÿåº¦æ…¢ï¼Œé™åˆ¶äº†å…¶åœ¨ä¸´åºŠä¸­çš„åº”ç”¨ã€‚åŠ é€ŸMRIé‡‡é›†æ˜¯è§£å†³è¿™ä¸€é—®é¢˜çš„å…³é”®ï¼Œå…¶ä¸­éç¬›å¡å°”é‡‡æ ·è½¨è¿¹å’Œæ·±åº¦å­¦ä¹ é‡å»ºæ–¹æ³•æ˜¯ä¸¤ä¸ªé‡è¦çš„æ–¹å‘ã€‚</p><p>(2): ç°æœ‰çš„æ·±åº¦å­¦ä¹ é‡å»ºæ–¹æ³•ä¸»è¦é’ˆå¯¹ç¬›å¡å°”é‡‡æ ·MRIï¼Œå¿½ç•¥äº†éç¬›å¡å°”é‡‡æ ·è½¨è¿¹å¸¦æ¥çš„é¢å¤–åŠ é€Ÿæ½œåŠ›ã€‚</p><p>(3): æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„ã€è½¨è¿¹æ— å…³çš„å¤šçº¿åœˆèºæ—‹MRIæ¬ é‡‡æ ·å›¾åƒé‡å»ºæ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ¡ä»¶è®­ç»ƒå’Œé¢‘ç‡å¼•å¯¼ï¼Œç¡®ä¿å›¾åƒå’Œæµ‹é‡å€¼ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚</p><p>(4): åœ¨å›é¡¾æ€§æ•°æ®ä¸Šï¼Œè¯¥æ–¹æ³•é‡å»ºçš„å›¾åƒè´¨é‡é«˜ï¼ˆç»“æ„ç›¸ä¼¼æ€§&gt;0.87ï¼‰ï¼Œæ‰«ææ—¶é—´æå¿«ï¼ˆ2Då›¾åƒ0.02ç§’ï¼‰ã€‚è¯¥æ–¹æ³•è¿˜ç”¨äºè¯†åˆ«ä¸€ç»„æœ€ä¼˜çš„å¯å˜å¯†åº¦èºæ—‹è½¨è¿¹ï¼Œä¸ä½¿ç”¨éå‡åŒ€å¿«é€Ÿå‚…é‡Œå¶å˜æ¢çš„ä¼ ç»Ÿé‡å»ºæ–¹æ³•ç›¸æ¯”ï¼Œå›¾åƒè´¨é‡æœ‰å¾ˆå¤§æé«˜ã€‚é€šè¿‡ç»“åˆé«˜æ•ˆçš„èºæ—‹é‡‡æ ·è½¨è¿¹ã€å¤šçº¿åœˆæˆåƒå’Œæ·±åº¦å­¦ä¹ é‡å»ºï¼Œè¯¥æ–¹æ³•æœ‰æœ›å®ç°å®æ—¶3Dæˆåƒæ‰€éœ€çš„é«˜åŠ é€Ÿå› å­ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬ç ”ç©¶å›é¡¾æ€§ä½¿ç”¨[12]å…¬å¼€è·å–çš„äººç±»å—è¯•è€…æ•°æ®è¿›è¡Œã€‚æ— éœ€ä¼¦ç†æ‰¹å‡†ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šæˆ‘ä»¬ä½¿ç”¨NYU FastMRIæ•°æ®é›†[12]ï¼Œè¯¥æ•°æ®é›†åŒ…å«6970ä¸ªåœ¨4åˆ°24ä¸ªçº¿åœˆçš„ç¡¬ä»¶ä¸Šå®Œå…¨é‡‡æ ·çš„2Dè„‘éƒ¨æ‰«æã€‚å¯¹äºè®­ç»ƒå’Œæµ‹è¯•ï¼Œæˆ‘ä»¬è€ƒè™‘ä»¥ä¸‹åºåˆ—å‚æ•°æ¥è¡¨å¾è½´å‘T2åŠ æƒæ¶¡æ—‹è‡ªæ—‹å›æ³¢åºåˆ—ï¼šæ‰«ææ—¶é—´=140sï¼ŒTR=6sï¼ŒTE=113msï¼Œåˆ‡ç‰‡=30ï¼Œåˆ‡ç‰‡åšåº¦=5mmï¼Œè§†é‡=22cmï¼ŒçŸ©é˜µå¤§å°=320x320ã€‚2562åˆ†è¾¨ç‡çš„2Dåˆ‡ç‰‡çš„æœ‰æ•ˆæ‰«ææ—¶é—´ä¸º140s/320 âˆ— 256/30 â‰ˆ 3.7sã€‚ç”±äºè¿™äº›æ•°æ®æœ€åˆæ˜¯ä½¿ç”¨ç¬›å¡å°”åºåˆ—è·å–çš„ï¼Œå› æ­¤å›¾2ã€‚ç»™å®šæµ‹é‡å€¼y0ï¼Œé‡å»ºéµå¾ªä¿®æ”¹åçš„æ‰©æ•£é‡‡æ ·è¿‡ç¨‹ã€‚åœ¨æ¯ä¸ªæ—¶é—´æ­¥é•¿ï¼Œä¸€ä¸ªæœ‰å™ªå£°çš„æ½œåœ¨xtä¸å…ˆéªŒp0è¿æ¥ï¼Œå¹¶ä¼ é€’åˆ°å»å™ªæ¨¡å‹ä»¥è·å¾—Ëœxtâˆ’1ã€‚ä¸ºäº†å¼ºåˆ¶ä¸y0ä¸€è‡´ï¼Œæˆ‘ä»¬è®¡ç®—é¢‘ç‡æ¢¯åº¦âˆ‡ytâˆ’1å¹¶ä½¿ç”¨ä¿®æ”¹åçš„è¿­ä»£é€†nufftï¼ˆç¬¬3.3èŠ‚ï¼‰æ±‚è§£å›¾åƒæ¢¯åº¦ã€‚xtâˆ’1å’Œâˆ‡xtâˆ’1çš„åŠ æƒå’Œäº§ç”Ÿæ ¡æ­£åçš„å›¾åƒxtâˆ’1ã€‚é‡å¤æ­¤æ“ä½œï¼Œç›´åˆ°t = 0ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šæˆ‘ä»¬æ¨¡æ‹Ÿèºæ—‹é‡‡é›†ï¼Œæ–¹æ³•æ˜¯å›é¡¾æ€§åœ°åœ¨kç©ºé—´ä¸­æ’å€¼ï¼Œä»¥è·å¾—æ²¿ç”Ÿæˆèºæ—‹è½¨è¿¹çš„å¤å€¼æµ‹é‡å€¼ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šæ ¹æ®Kimç­‰äºº[13]ï¼Œæˆ‘ä»¬è€ƒè™‘ä»¥ä¸‹å½¢å¼çš„èºæ—‹è½¨è¿¹ï¼šk(Ï„) = ï¿½ Ï„ 0 1 Ï(Ï•)dÏ•ejÏ‰Ï„ â‰ˆ Î»Ï„ Î±ejÏ‰Ï„ã€‚ï¼ˆ3ï¼‰æ­¤å¤„ï¼ŒÏè¡¨ç¤ºé‡‡æ ·å¯†åº¦ï¼ŒÏ„æ˜¯æ—¶é—´çš„å‡½æ•°ï¼ŒÏ•æ˜¯è§’åº¦ä½ç½®ï¼ŒÏ‰ = 2Ï€næ˜¯é¢‘ç‡ï¼Œnæ˜¯kç©ºé—´ä¸­çš„è½¬æ•°ï¼ŒÎ»æ˜¯ç¼©æ”¾å› å­ï¼Œç­‰äºçŸ©é˜µå¤§å°/(2âˆ— FOV)ï¼ŒÎ±æ˜¯ç›¸å¯¹äºè¾¹ç¼˜è¿‡åº¦é‡‡æ ·kç©ºé—´ä¸­å¿ƒçš„åå·®é¡¹ã€‚åœ¨æ¢¯åº¦å›è½¬ç‡ä¸Šé™å’Œæ¢¯åº¦å¹…åº¦ä¸Šé™çš„çº¦æŸä¸‹æ±‚è§£è¿™ä¸ªå‚æ•°æ–¹ç¨‹ï¼Œäº§ç”Ÿæ¢¯åº¦ï¼ˆgx(t)å’Œgy(t)ï¼‰ä»¥åŠkx,kyå¹³é¢çš„èºæ—‹è½¨è¿¹ï¼ˆå›¾1ï¼‰ã€‚è¿™æ ·åšï¼Œæˆ‘ä»¬å¯ä»¥è°ƒæ•´é‡‡æ ·å‚æ•°ä»¥æ§åˆ¶è¯¸å¦‚è¯»å‡ºæŒç»­æ—¶é—´å’Œåœç•™æ—¶é—´ä¹‹ç±»çš„å› ç´ ï¼ŒåŒæ—¶æ”¹å˜äº¤é”™æ•°å’Œä½é¢‘åˆ°é«˜é¢‘è¿‡é‡‡æ ·ç‡ã€‚</p><p>ï¼ˆ5ï¼‰ï¼šå›¾åƒé‡å»ºæ˜¯é€†é—®é¢˜æ±‚è§£MRIæ¬ é‡‡æ ·é‡‡é›†ç­‰åŒäºé€šè¿‡æŸç§ä¸å®Œç¾çš„é‡‡æ ·å‡½æ•°Aæµ‹é‡æœªçŸ¥ä¿¡å·xï¼šy = Ax + Ïµã€‚è¿™é‡Œï¼Œyæ˜¯æµ‹é‡å¤šçº¿åœˆkç©ºé—´æ•°æ®ï¼ŒAæ˜¯éå‡åŒ€å‚…é‡Œå¶å˜æ¢ã€‚Ïµæ˜¯æµ‹é‡å™ªå£°ï¼Œä¸yå­˜åœ¨äºåŒä¸€åŸŸä¸­ï¼›åœ¨MRIä¸­ï¼Œå¯¹äºæ¯ä¸ªçº¿åœˆï¼Œå™ªå£°åœ¨yçš„å®éƒ¨å’Œè™šéƒ¨ä¸­å‘ˆé«˜æ–¯åˆ†å¸ƒã€‚é‡å»ºæ˜¯ä»ä¸€ç»„ä¸å®Œæ•´çš„kç©ºé—´æµ‹é‡å€¼yä¸­æ¢å¤å›¾åƒä¿¡å·xçš„ä¸é€‚å®šé€†é—®é¢˜ã€‚ç”±äºxå’Œyå­˜åœ¨äºä¸åŒçš„åŸŸä¸­ï¼Œå› æ­¤xéšè—åœ¨é‡‡æ ·ç®—å­Açš„åé¢ã€‚è§£å†³è¿™ä¸ªé—®é¢˜éœ€è¦å…ˆéªŒçŸ¥è¯†ã€‚åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ å›¾åƒçš„æ½œåœ¨æ¡ä»¶åˆ†å¸ƒå¹¶å¯»æ±‚é‡å»º</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„èºæ—‹MRIæ¬ é‡‡æ ·å›¾åƒé‡å»ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†å¤šçº¿åœˆæˆåƒã€èºæ—‹æ‰«æå’Œæ¬ é‡‡æ ·ï¼Œå®ç°äº†æå¿«çš„æˆåƒé€Ÿåº¦ï¼Œæœ‰æœ›å®ç°å®æ—¶3Dæˆåƒæ‰€éœ€çš„æé«˜åŠ é€Ÿå› å­ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šåŸºäºæ‰©æ•£æ¨¡å‹çš„å›¾åƒé‡å»ºæ–¹æ³•ï¼›å¤šçº¿åœˆæˆåƒå’Œèºæ—‹æ‰«æçš„ç»“åˆï¼›å¯å˜å¯†åº¦èºæ—‹è½¨è¿¹çš„ä¼˜åŒ–ã€‚æ€§èƒ½ï¼šå›¾åƒè´¨é‡é«˜ï¼ˆç»“æ„ç›¸ä¼¼æ€§&gt;0.87ï¼‰ï¼Œæ‰«ææ—¶é—´æå¿«ï¼ˆ2Då›¾åƒ0.02ç§’ï¼‰ã€‚å·¥ä½œé‡ï¼šéœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®å’Œè®¡ç®—èµ„æºã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-dad46f934fa27aedf6f5bcc658a1e97b.jpg" align="middle"><img src="https://pica.zhimg.com/v2-7ca2a2a56eaf899a4ab5fb7f25a2d0dd.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-09dc794137fef040d7fe26326b8c5bd2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1377428c568ca550e4683544f87b3da2.jpg" align="middle"></details><h2 id="AniClipart-Clipart-Animation-with-Text-to-Video-Priors"><a href="#AniClipart-Clipart-Animation-with-Text-to-Video-Priors" class="headerlink" title="AniClipart: Clipart Animation with Text-to-Video Priors"></a>AniClipart: Clipart Animation with Text-to-Video Priors</h2><p><strong>Authors:Ronghuan Wu, Wanchao Su, Kede Ma, Jing Liao</strong></p><p>Clipart, a pre-made graphic art form, offers a convenient and efficient way of illustrating visual content. Traditional workflows to convert static clipart images into motion sequences are laborious and time-consuming, involving numerous intricate steps like rigging, key animation and in-betweening. Recent advancements in text-to-video generation hold great potential in resolving this problem. Nevertheless, direct application of text-to-video generation models often struggles to retain the visual identity of clipart images or generate cartoon-style motions, resulting in unsatisfactory animation outcomes. In this paper, we introduce AniClipart, a system that transforms static clipart images into high-quality motion sequences guided by text-to-video priors. To generate cartoon-style and smooth motion, we first define B\â€™{e}zier curves over keypoints of the clipart image as a form of motion regularization. We then align the motion trajectories of the keypoints with the provided text prompt by optimizing the Video Score Distillation Sampling (VSDS) loss, which encodes adequate knowledge of natural motion within a pretrained text-to-video diffusion model. With a differentiable As-Rigid-As-Possible shape deformation algorithm, our method can be end-to-end optimized while maintaining deformation rigidity. Experimental results show that the proposed AniClipart consistently outperforms existing image-to-video generation models, in terms of text-video alignment, visual identity preservation, and motion consistency. Furthermore, we showcase the versatility of AniClipart by adapting it to generate a broader array of animation formats, such as layered animation, which allows topological changes. </p><p><a href="http://arxiv.org/abs/2404.12347v1">PDF</a> Project Page: <a href="https://aniclipart.github.io/">https://aniclipart.github.io/</a></p><p><strong>Summary</strong><br>é€šè¿‡ä½¿ç”¨æ–‡ç”Ÿå›¾è¯­è¨€æ¨¡å‹ï¼ŒAniClipartå¯ä»¥å°†é™æ€å‰ªè´´ç”»å›¾åƒè½¬æ¢ä¸ºé«˜è´¨é‡çš„åŠ¨æ€åºåˆ—ï¼Œå¹¶ä¸”å§‹ç»ˆä¼˜äºç°æœ‰çš„å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆæ¨¡å‹ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>AniClipart å°†é™æ€å‰ªè´´ç”»è½¬æ¢ä¸ºåŠ¨ç”»åºåˆ—ï¼Œä¿ç•™äº†å‰ªè´´ç”»çš„è§†è§‰ç‰¹å¾å¹¶ç”Ÿæˆäº†å¡é€šé£æ ¼çš„åŠ¨ä½œã€‚</li><li>AniClipart ä½¿ç”¨è´å¡å°”æ›²çº¿å¯¹å‰ªè´´ç”»å›¾åƒçš„å…³é”®ç‚¹è¿›è¡Œè¿åŠ¨æ­£åˆ™åŒ–ã€‚</li><li>AniClipart é€šè¿‡ä¼˜åŒ–è§†é¢‘è¯„åˆ†è’¸é¦é‡‡æ · (VSDS) æŸå¤±å°†å…³é”®ç‚¹çš„è¿åŠ¨è½¨è¿¹ä¸æä¾›çš„æ–‡æœ¬æç¤ºå¯¹é½ã€‚</li><li>VSDS æŸå¤±ç¼–ç äº†é¢„è®­ç»ƒæ–‡æœ¬åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­è‡ªç„¶è¿åŠ¨çš„å……åˆ†çŸ¥è¯†ã€‚</li><li>AniClipart ä½¿ç”¨å¯å¾®åˆ†åƒµç¡¬å½¢çŠ¶å˜å½¢ç®—æ³•ï¼Œå¯ä»¥åœ¨ä¿æŒå˜å½¢åˆšæ€§çš„åŒæ—¶è¿›è¡Œç«¯åˆ°ç«¯ä¼˜åŒ–ã€‚</li><li>AniClipart åœ¨æ–‡æœ¬-è§†é¢‘å¯¹é½ã€è§†è§‰ç‰¹å¾ä¿ç•™å’Œè¿åŠ¨ä¸€è‡´æ€§æ–¹é¢å§‹ç»ˆä¼˜äºç°æœ‰çš„å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆæ¨¡å‹ã€‚</li><li>AniClipart å¯ä»¥é€‚åº”æ›´å¹¿æ³›çš„åŠ¨ç”»æ ¼å¼ï¼Œä¾‹å¦‚å…è®¸æ‹“æ‰‘æ›´æ”¹çš„åˆ†å±‚åŠ¨ç”»ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: AniClipartï¼šåŸºäºæ–‡æœ¬åˆ°è§†é¢‘å…ˆéªŒçš„å‰ªè¾‘ç”»åŠ¨ç”»</p></li><li><p>Authors: RONGHUAN WU, WANCHAO SU, KEDE MA, JING LIAO</p></li><li><p>Affiliation: é¦™æ¸¯åŸå¸‚å¤§å­¦</p></li><li><p>Keywords: Clipart Animation, Text-to-Video Diffusion, Score Distillation Sampling, As-Rigid-As-Possible Shape Deformation</p></li><li><p>Urls: Paper:https://arxiv.org/abs/2404.12347v1 Github:None</p></li><li><p>Summary:</p></li></ol><p>(1):å‰ªè¾‘ç”»æ˜¯ä¸€ç§é¢„å…ˆåˆ¶ä½œçš„å›¾å½¢è‰ºæœ¯å½¢å¼ï¼Œå®ƒæä¾›äº†ä¸€ç§æ–¹ä¾¿ä¸”æœ‰æ•ˆçš„æ–¹æ³•æ¥æ’å›¾è§†è§‰å†…å®¹ã€‚å°†é™æ€å‰ªè¾‘ç”»å›¾åƒè½¬æ¢ä¸ºè¿åŠ¨åºåˆ—çš„ä¼ ç»Ÿå·¥ä½œæµç¨‹æ—¢è´¹åŠ›åˆè´¹æ—¶ï¼Œæ¶‰åŠè®¸å¤šå¤æ‚çš„æ­¥éª¤ï¼Œå¦‚è£…é…ã€å…³é”®åŠ¨ç”»å’Œä¸­é—´åŠ¨ç”»ã€‚</p><p>(2):æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ–¹é¢çš„æœ€æ–°è¿›å±•åœ¨è§£å†³è¿™ä¸ªé—®é¢˜æ–¹é¢å…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç›´æ¥åº”ç”¨æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ¨¡å‹é€šå¸¸éš¾ä»¥ä¿ç•™å‰ªè¾‘ç”»å›¾åƒçš„è§†è§‰æ ‡è¯†æˆ–ç”Ÿæˆå¡é€šé£æ ¼çš„åŠ¨ä½œï¼Œä»è€Œå¯¼è‡´åŠ¨ç”»ç»“æœä¸ä»¤äººæ»¡æ„ã€‚</p><p>(3):æœ¬æ–‡ä»‹ç»äº† AniClipartï¼Œè¿™æ˜¯ä¸€ä¸ªå°†é™æ€å‰ªè¾‘ç”»å›¾åƒè½¬æ¢ä¸ºé«˜è´¨é‡è¿åŠ¨åºåˆ—çš„ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿç”±æ–‡æœ¬åˆ°è§†é¢‘å…ˆéªŒæŒ‡å¯¼ã€‚ä¸ºäº†ç”Ÿæˆå¡é€šé£æ ¼å’Œæµç•…çš„åŠ¨ä½œï¼Œæˆ‘ä»¬é¦–å…ˆå°†è´å¡å°”æ›²çº¿å®šä¹‰ä¸ºå‰ªè¾‘ç”»å›¾åƒå…³é”®ç‚¹çš„è¿åŠ¨æ­£åˆ™åŒ–å½¢å¼ã€‚ç„¶åï¼Œé€šè¿‡ä¼˜åŒ–è§†é¢‘è¯„åˆ†è’¸é¦é‡‡æ · (VSDS) æŸå¤±å°†å…³é”®ç‚¹çš„è¿åŠ¨è½¨è¿¹ä¸æä¾›çš„æ–‡æœ¬æç¤ºå¯¹é½ï¼Œè¯¥æŸå¤±å¯¹é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­çš„è‡ªç„¶è¿åŠ¨çŸ¥è¯†è¿›è¡Œäº†å……åˆ†ç¼–ç ã€‚é€šè¿‡å¯å¾®åˆ†å°½å¯èƒ½åˆšæ€§å½¢çŠ¶å˜å½¢ç®—æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åœ¨ä¿æŒå˜å½¢åˆšæ€§çš„åŒæ—¶è¿›è¡Œç«¯åˆ°ç«¯ä¼˜åŒ–ã€‚</p><p>(4):å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æ–‡æœ¬è§†é¢‘å¯¹é½ã€è§†è§‰æ ‡è¯†ä¿ç•™å’Œè¿åŠ¨ä¸€è‡´æ€§æ–¹é¢ï¼Œæ‰€æå‡ºçš„ AniClipart å§‹ç»ˆä¼˜äºç°æœ‰çš„å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆæ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å±•ç¤ºäº† AniClipart çš„å¤šåŠŸèƒ½æ€§ï¼Œé€šè¿‡å¯¹å…¶è¿›è¡Œè°ƒæ•´ä»¥ç”Ÿæˆæ›´å¹¿æ³›çš„åŠ¨ç”»æ ¼å¼ï¼Œä¾‹å¦‚å…è®¸æ‹“æ‰‘å˜åŒ–çš„åˆ†å±‚åŠ¨ç”»ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ–‡æœ¬åˆ°è§†é¢‘å…ˆéªŒçš„å‰ªè¾‘ç”»åŠ¨ç”»ç³»ç»Ÿ AniClipartã€‚</p><p>ï¼ˆ2ï¼‰ï¼šAniClipart ç”±ä»¥ä¸‹å‡ ä¸ªå…³é”®ç»„ä»¶ç»„æˆï¼šè´å¡å°”æ›²çº¿è¿åŠ¨æ­£åˆ™åŒ–ã€è§†é¢‘è¯„åˆ†è’¸é¦é‡‡æ ·ï¼ˆVSDSï¼‰æŸå¤±å’Œå¯å¾®åˆ†å°½å¯èƒ½åˆšæ€§å½¢çŠ¶å˜å½¢ç®—æ³•ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šè´å¡å°”æ›²çº¿è¿åŠ¨æ­£åˆ™åŒ–å°†å‰ªè¾‘ç”»å›¾åƒå…³é”®ç‚¹çš„è¿åŠ¨å®šä¹‰ä¸ºè´å¡å°”æ›²çº¿ï¼Œä»è€Œç¡®ä¿äº†è¿åŠ¨çš„å¹³æ»‘æ€§å’Œè¿ç»­æ€§ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šVSDS æŸå¤±å°†å…³é”®ç‚¹çš„è¿åŠ¨è½¨è¿¹ä¸æä¾›çš„æ–‡æœ¬æç¤ºå¯¹é½ï¼Œä»è€Œå°†æ–‡æœ¬åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­çš„è‡ªç„¶è¿åŠ¨çŸ¥è¯†èå…¥åˆ°åŠ¨ç”»ä¸­ã€‚</p><p>ï¼ˆ5ï¼‰ï¼šå¯å¾®åˆ†å°½å¯èƒ½åˆšæ€§å½¢çŠ¶å˜å½¢ç®—æ³•å…è®¸åœ¨ä¿æŒå˜å½¢åˆšæ€§çš„åŒæ—¶è¿›è¡Œç«¯åˆ°ç«¯ä¼˜åŒ–ï¼Œä»è€Œç”Ÿæˆå…·æœ‰æ¸…æ™°è§†è§‰æ ‡è¯†çš„åŠ¨ç”»ã€‚</p><p><strong>8. ç»“è®ºï¼š</strong></p><p>ï¼ˆ1ï¼‰æœ¬æ–‡æå‡ºçš„ AniClipart ç³»ç»Ÿï¼Œé€šè¿‡å°†æ–‡æœ¬åˆ°è§†é¢‘å…ˆéªŒèå…¥å‰ªè¾‘ç”»åŠ¨ç”»ç”Ÿæˆä¸­ï¼Œä¸ºé™æ€å‰ªè¾‘ç”»å›¾åƒèµ‹äºˆäº†ç”ŸåŠ¨æ€§ï¼Œç®€åŒ–äº†åŠ¨ç”»åˆ¶ä½œæµç¨‹ï¼Œå…·æœ‰é‡è¦çš„åˆ›æ–°æ„ä¹‰ã€‚</p><p>ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼šAniClipart åˆ›æ–°æ€§åœ°é‡‡ç”¨äº†è´å¡å°”æ›²çº¿è¿åŠ¨æ­£åˆ™åŒ–ã€è§†é¢‘è¯„åˆ†è’¸é¦é‡‡æ ·æŸå¤±å’Œå¯å¾®åˆ†å°½å¯èƒ½åˆšæ€§å½¢çŠ¶å˜å½¢ç®—æ³•ï¼Œå®ç°äº†å‰ªè¾‘ç”»å›¾åƒçš„å…³é”®ç‚¹è¿åŠ¨è½¨è¿¹ä¸æ–‡æœ¬æç¤ºçš„ç²¾ç¡®å¯¹é½ï¼Œä»¥åŠå˜å½¢åˆšæ€§çš„ä¿æŒï¼Œä»è€Œç”Ÿæˆé«˜è´¨é‡çš„å‰ªè¾‘ç”»åŠ¨ç”»ã€‚</p><p>æ€§èƒ½ï¼šAniClipart åœ¨æ–‡æœ¬è§†é¢‘å¯¹é½ã€è§†è§‰æ ‡è¯†ä¿ç•™å’Œè¿åŠ¨ä¸€è‡´æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰çš„å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œå±•ç°å‡ºå‡ºè‰²çš„åŠ¨ç”»ç”Ÿæˆèƒ½åŠ›ã€‚</p><p>å·¥ä½œé‡ï¼šAniClipart é‡‡ç”¨ç«¯åˆ°ç«¯ä¼˜åŒ–ï¼Œç®€åŒ–äº†å‰ªè¾‘ç”»åŠ¨ç”»åˆ¶ä½œæµç¨‹ï¼Œé™ä½äº†å·¥ä½œé‡ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-7f003c736b9e8d225fc78c7b356b7e25.jpg" align="middle"><img src="https://picx.zhimg.com/v2-33e8a11d43dfdfe88d324da3694df802.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b455537a4612c5459d9162e1601fc155.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0aba7591bd6a8a973210ed734d1006c9.jpg" align="middle"></details><h2 id="StyleBooth-Image-Style-Editing-with-Multimodal-Instruction"><a href="#StyleBooth-Image-Style-Editing-with-Multimodal-Instruction" class="headerlink" title="StyleBooth: Image Style Editing with Multimodal Instruction"></a>StyleBooth: Image Style Editing with Multimodal Instruction</h2><p><strong>Authors:Zhen Han, Chaojie Mao, Zeyinzi Jiang, Yulin Pan, Jingfeng Zhang</strong></p><p>Given an original image, image editing aims to generate an image that align with the provided instruction. The challenges are to accept multimodal inputs as instructions and a scarcity of high-quality training data, including crucial triplets of source/target image pairs and multimodal (text and image) instructions. In this paper, we focus on image style editing and present StyleBooth, a method that proposes a comprehensive framework for image editing and a feasible strategy for building a high-quality style editing dataset. We integrate encoded textual instruction and image exemplar as a unified condition for diffusion model, enabling the editing of original image following multimodal instructions. Furthermore, by iterative style-destyle tuning and editing and usability filtering, the StyleBooth dataset provides content-consistent stylized/plain image pairs in various categories of styles. To show the flexibility of StyleBooth, we conduct experiments on diverse tasks, such as text-based style editing, exemplar-based style editing and compositional style editing. The results demonstrate that the quality and variety of training data significantly enhance the ability to preserve content and improve the overall quality of generated images in editing tasks. Project page can be found at <a href="https://ali-vilab.github.io/stylebooth-page/">https://ali-vilab.github.io/stylebooth-page/</a>. </p><p><a href="http://arxiv.org/abs/2404.12154v1">PDF</a> </p><p><strong>Summary</strong><br>StyleBoothæ˜¯ä¸€ä¸ªå›¾åƒç¼–è¾‘æ¡†æ¶ï¼Œé›†æˆäº†æ–‡æœ¬å’Œå›¾åƒæŒ‡ä»¤ï¼Œå¹¶æä¾›é«˜è´¨é‡çš„é£æ ¼ç¼–è¾‘æ•°æ®é›†ï¼Œå¯ç”¨äºå„ç§ç¼–è¾‘ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬é£æ ¼ç¼–è¾‘å’Œç¤ºä¾‹é£æ ¼ç¼–è¾‘ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>StyleBooth æ¡†æ¶å°†æ–‡æœ¬æŒ‡ä»¤å’Œå›¾åƒç¤ºä¾‹æ•´åˆä¸ºæ‰©æ•£æ¨¡å‹çš„ç»Ÿä¸€æ¡ä»¶ï¼Œå®ç°å›¾åƒé£æ ¼ç¼–è¾‘ã€‚</li><li>StyleBooth æ•°æ®é›†é€šè¿‡è¿­ä»£çš„é£æ ¼-å»é£æ ¼è°ƒæ•´å’Œç¼–è¾‘ä»¥åŠå¯ç”¨æ€§è¿‡æ»¤ï¼Œæä¾›äº†å†…å®¹ä¸€è‡´çš„é£æ ¼åŒ–/æ™®é€šå›¾åƒå¯¹ã€‚</li><li>StyleBooth é€šè¿‡é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®å¢å¼ºäº†ç¼–è¾‘ä»»åŠ¡ä¸­ä¿ç•™å†…å®¹å’Œæé«˜ç”Ÿæˆå›¾åƒæ•´ä½“è´¨é‡çš„èƒ½åŠ›ã€‚</li><li>StyleBooth å¯ç”¨äºæ–‡æœ¬é£æ ¼ç¼–è¾‘ã€ç¤ºä¾‹é£æ ¼ç¼–è¾‘å’Œåˆæˆé£æ ¼ç¼–è¾‘ç­‰å¤šç§ä»»åŠ¡ã€‚</li><li>StyleBooth é¡¹ç›®ä¸»é¡µï¼š<a href="https://ali-vilab.github.io/stylebooth-page/ã€‚">https://ali-vilab.github.io/stylebooth-page/ã€‚</a></li><li>å¤šæ¨¡æ€è¾“å…¥å’Œé«˜è´¨é‡è®­ç»ƒæ•°æ®å¯¹äºå›¾åƒç¼–è¾‘è‡³å…³é‡è¦ã€‚</li><li>StyleBooth æä¾›äº†ä¸€ä¸ªå…¨é¢çš„å›¾åƒç¼–è¾‘æ¡†æ¶å’Œè®­ç»ƒæ•°æ®çš„æ„å»ºç­–ç•¥ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: StyleBooth: ä½¿ç”¨å¤šæ¨¡æ€æŒ‡ä»¤è¿›è¡Œå›¾åƒé£æ ¼ç¼–è¾‘</p></li><li><p>Authors: Zhen Han, Chaojie Mao, Zeyinzi Jiang, Yulin Pan, Jingfeng Zhang</p></li><li><p>Affiliation: é˜¿é‡Œå·´å·´é›†å›¢</p></li><li><p>Keywords: Text-based style editing Â· Exemplar-based style editing Â· Multimodal instruction-tuning</p></li><li><p>Urls: https://arxiv.org/abs/2404.12154, Github:None</p></li><li><p>Summary:</p></li></ol><p>(1):å›¾åƒç¼–è¾‘æ—¨åœ¨æ ¹æ®æä¾›çš„æŒ‡ä»¤ç”Ÿæˆä¸åŸå›¾åƒå¯¹é½çš„å›¾åƒã€‚æŒ‘æˆ˜åœ¨äºæ¥å—å¤šæ¨¡æ€è¾“å…¥ä½œä¸ºæŒ‡ä»¤ï¼Œä»¥åŠç¼ºä¹é«˜è´¨é‡è®­ç»ƒæ•°æ®ï¼ŒåŒ…æ‹¬æº/ç›®æ ‡å›¾åƒå¯¹å’Œå¤šæ¨¡æ€ï¼ˆæ–‡æœ¬å’Œå›¾åƒï¼‰æŒ‡ä»¤çš„å…³é”®ä¸‰å…ƒç»„ã€‚</p><p>(2):ä»¥å¾€çš„æ–¹æ³•ä¸»è¦åŒ…æ‹¬æ“çºµæ³¨æ„åŠ›æœºåˆ¶çš„ç‰¹å¾ã€åœ¨å»å™ªæ­¥éª¤ä¸­å®ç°å¼•å¯¼æ‰©æ•£ã€ä½¿ç”¨å›¾åƒå¯¹è¿›è¡Œç›‘ç£æ¥è°ƒæ•´ T2I æ¨¡å‹ç­‰ã€‚è¿™äº›æ–¹æ³•éƒ½é¢ä¸´ç€åªæ”¯æŒå•æ¨¡æ€è¾“å…¥ã€ç¼ºä¹é«˜è´¨é‡è®­ç»ƒæ•°æ®ã€éš¾ä»¥ä¿æŒå†…å®¹ä¸€è‡´æ€§ç­‰é—®é¢˜ã€‚</p><p>(3):æœ¬æ–‡æå‡º StyleBooth æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æå‡ºäº†ä¸€ä¸ªç”¨äºå›¾åƒç¼–è¾‘çš„ç»¼åˆæ¡†æ¶å’Œæ„å»ºé«˜è´¨é‡é£æ ¼ç¼–è¾‘æ•°æ®é›†çš„å¯è¡Œç­–ç•¥ã€‚æˆ‘ä»¬å°†ç¼–ç çš„æ–‡æœ¬æŒ‡ä»¤å’Œå›¾åƒç¤ºä¾‹æ•´åˆä¸ºæ‰©æ•£æ¨¡å‹çš„ç»Ÿä¸€æ¡ä»¶ï¼Œä»è€Œèƒ½å¤Ÿæ ¹æ®å¤šæ¨¡æ€æŒ‡ä»¤ç¼–è¾‘åŸå§‹å›¾åƒã€‚æ­¤å¤–ï¼Œé€šè¿‡è¿­ä»£çš„é£æ ¼-å»é£æ ¼è°ƒæ•´å’Œç¼–è¾‘ä»¥åŠå¯ç”¨æ€§è¿‡æ»¤ï¼ŒStyleBooth æ•°æ®é›†æä¾›äº†å„ç§é£æ ¼ç±»åˆ«ä¸­å†…å®¹ä¸€è‡´çš„é£æ ¼åŒ–/-æ™®é€šå›¾åƒå¯¹ã€‚</p><p>(4):å®éªŒç»“æœè¡¨æ˜ï¼Œè®­ç»ƒæ•°æ®çš„è´¨é‡å’Œå¤šæ ·æ€§æ˜¾ç€å¢å¼ºäº†åœ¨ç¼–è¾‘ä»»åŠ¡ä¸­ä¿ç•™å†…å®¹å’Œæé«˜ç”Ÿæˆå›¾åƒæ•´ä½“è´¨é‡çš„èƒ½åŠ›ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šStyleBooth æ–¹æ³•æå‡ºäº†ä¸€ç§ç”¨äºå›¾åƒç¼–è¾‘çš„ç»¼åˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†ç¼–ç çš„æ–‡æœ¬æŒ‡ä»¤å’Œå›¾åƒç¤ºä¾‹æ•´åˆä¸ºæ‰©æ•£æ¨¡å‹çš„ç»Ÿä¸€æ¡ä»¶ï¼Œèƒ½å¤Ÿæ ¹æ®å¤šæ¨¡æ€æŒ‡ä»¤ç¼–è¾‘åŸå§‹å›¾åƒï¼›</p><p>ï¼ˆ2ï¼‰ï¼šStyleBooth æ•°æ®é›†é€šè¿‡è¿­ä»£çš„é£æ ¼-å»é£æ ¼è°ƒæ•´å’Œç¼–è¾‘ä»¥åŠå¯ç”¨æ€§è¿‡æ»¤ï¼Œæä¾›äº†å„ç§é£æ ¼ç±»åˆ«ä¸­å†…å®¹ä¸€è‡´çš„é£æ ¼åŒ–/-æ™®é€šå›¾åƒå¯¹ï¼Œæé«˜äº†è®­ç»ƒæ•°æ®çš„è´¨é‡å’Œå¤šæ ·æ€§ï¼Œå¢å¼ºäº†åœ¨ç¼–è¾‘ä»»åŠ¡ä¸­ä¿ç•™å†…å®¹å’Œæé«˜ç”Ÿæˆå›¾åƒæ•´ä½“è´¨é‡çš„èƒ½åŠ›ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šScale Weighting Mechanism æœºåˆ¶é€šè¿‡å¯¹éšè—ç©ºé—´åµŒå…¥è¿›è¡Œç¼©æ”¾åŠ æƒï¼Œå¹³è¡¡äº†ä¸åŒæ¨¡æ€çš„é£æ ¼è¡¨ç°ï¼Œä¿è¯äº†å›¾åƒç¼–è¾‘çš„è´¨é‡ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡º StyleBoothï¼Œè¿™æ˜¯ä¸€ç§å¤šæ¨¡æ€æŒ‡ä»¤å›¾åƒé£æ ¼ç¼–è¾‘æ–¹æ³•ã€‚å®ƒç‹¬ç«‹ç¼–ç å‚è€ƒå›¾åƒå’Œæ–‡æœ¬ï¼Œéšååœ¨æ½œåœ¨ç©ºé—´å†…å¯¹å…¶è¿›è¡Œè½¬æ¢å’Œå¯¹é½ï¼Œç„¶åæ³¨å…¥éª¨å¹²ç½‘ç»œä»¥è¿›è¡Œç”ŸæˆæŒ‡å¯¼ï¼Œä»¥å®ç°åŸºäºæ–‡æœ¬å’Œç¤ºä¾‹çš„æŒ‡ä»¤ç¼–è¾‘ã€‚åŒæ—¶ï¼ŒStyleBooth è¿˜å¯ä»¥èåˆå¤šæ¨¡æ€ä¿¡æ¯è¿›è¡Œåˆæˆåˆ›é€ æ€§ç”Ÿæˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªç”¨äºé£æ ¼ç¼–è¾‘çš„é«˜è´¨é‡æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ç”±å„ç§å†…å®¹ä¸€è‡´çš„é£æ ¼åŒ–å’Œæ™®é€šå›¾åƒå¯¹ç»„æˆï¼Œè¿™æœ‰åŠ©äºæˆ‘ä»¬æ„å»ºæ›´å¥½çš„ç¼–è¾‘æ¨¡å‹ã€‚å±€é™æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªç”¨äºé£æ ¼ç¼–è¾‘çš„ä¸°å¯Œæ•°æ®é›†ã€‚ç„¶è€Œï¼Œæ•°æ®æ„å»ºåŸºäºç‰¹å®šé£æ ¼çš„æ–‡æœ¬æè¿°ï¼Œä¾‹å¦‚æ°´å½©ç”»ï¼Œè¿™æå¤§åœ°é™åˆ¶äº†é£æ ¼çš„æ•°é‡ã€‚æ”¶é›†æ›´å¹¿æ³›ã€æ›´å¹¿æ³›çš„ç¼–è¾‘æ•°æ®é›†å°†æ˜¯æˆ‘ä»¬æœªæ¥çš„å·¥ä½œã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡º StyleBooth æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†ç¼–ç çš„æ–‡æœ¬æŒ‡ä»¤å’Œå›¾åƒç¤ºä¾‹æ•´åˆä¸ºæ‰©æ•£æ¨¡å‹çš„ç»Ÿä¸€æ¡ä»¶ï¼Œèƒ½å¤Ÿæ ¹æ®å¤šæ¨¡æ€æŒ‡ä»¤ç¼–è¾‘åŸå§‹å›¾åƒï¼›æ„å»º StyleBooth æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†é€šè¿‡è¿­ä»£çš„é£æ ¼-å»é£æ ¼è°ƒæ•´å’Œç¼–è¾‘ä»¥åŠå¯ç”¨æ€§è¿‡æ»¤ï¼Œæä¾›äº†å„ç§é£æ ¼ç±»åˆ«ä¸­å†…å®¹ä¸€è‡´çš„é£æ ¼åŒ–/-æ™®é€šå›¾åƒå¯¹ï¼Œæé«˜äº†è®­ç»ƒæ•°æ®çš„è´¨é‡å’Œå¤šæ ·æ€§ï¼›æå‡º Scale Weighting Mechanism æœºåˆ¶ï¼Œé€šè¿‡å¯¹éšè—ç©ºé—´åµŒå…¥è¿›è¡Œç¼©æ”¾åŠ æƒï¼Œå¹³è¡¡äº†ä¸åŒæ¨¡æ€çš„é£æ ¼è¡¨ç°ï¼Œä¿è¯äº†å›¾åƒç¼–è¾‘çš„è´¨é‡ã€‚æ€§èƒ½ï¼šå®éªŒç»“æœè¡¨æ˜ï¼Œè®­ç»ƒæ•°æ®çš„è´¨é‡å’Œå¤šæ ·æ€§æ˜¾ç€å¢å¼ºäº†åœ¨ç¼–è¾‘ä»»åŠ¡ä¸­ä¿ç•™å†…å®¹å’Œæé«˜ç”Ÿæˆå›¾åƒæ•´ä½“è´¨é‡çš„èƒ½åŠ›ã€‚å·¥ä½œé‡ï¼šæœ¬æ–‡çš„æ–¹æ³•å’Œæ•°æ®é›†çš„æ„å»ºè¿‡ç¨‹ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦è¾ƒå¤§çš„è®¡ç®—èµ„æºå’ŒäººåŠ›æŠ•å…¥ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-b55633af77d0cb6e6dbf35b308d980ca.jpg" align="middle"><img src="https://pica.zhimg.com/v2-8f75d6d068abc3a313d3144833482a9f.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f20cb7f7a36d3c7049b18131117bc5cd.jpg" align="middle"></details><h2 id="IntrinsicAnything-Learning-Diffusion-Priors-for-Inverse-Rendering-Under-Unknown-Illumination"><a href="#IntrinsicAnything-Learning-Diffusion-Priors-for-Inverse-Rendering-Under-Unknown-Illumination" class="headerlink" title="IntrinsicAnything: Learning Diffusion Priors for Inverse Rendering Under   Unknown Illumination"></a>IntrinsicAnything: Learning Diffusion Priors for Inverse Rendering Under   Unknown Illumination</h2><p><strong>Authors:Xi Chen, Sida Peng, Dongchen Yang, Yuan Liu, Bowen Pan, Chengfei Lv, Xiaowei Zhou</strong></p><p>This paper aims to recover object materials from posed images captured under an unknown static lighting condition. Recent methods solve this task by optimizing material parameters through differentiable physically based rendering. However, due to the coupling between object geometry, materials, and environment lighting, there is inherent ambiguity during the inverse rendering process, preventing previous methods from obtaining accurate results. To overcome this ill-posed problem, our key idea is to learn the material prior with a generative model for regularizing the optimization process. We observe that the general rendering equation can be split into diffuse and specular shading terms, and thus formulate the material prior as diffusion models of albedo and specular. Thanks to this design, our model can be trained using the existing abundant 3D object data, and naturally acts as a versatile tool to resolve the ambiguity when recovering material representations from RGB images. In addition, we develop a coarse-to-fine training strategy that leverages estimated materials to guide diffusion models to satisfy multi-view consistent constraints, leading to more stable and accurate results. Extensive experiments on real-world and synthetic datasets demonstrate that our approach achieves state-of-the-art performance on material recovery. The code will be available at <a href="https://zju3dv.github.io/IntrinsicAnything">https://zju3dv.github.io/IntrinsicAnything</a>. </p><p><a href="http://arxiv.org/abs/2404.11593v1">PDF</a> Project page: <a href="https://zju3dv.github.io/IntrinsicAnything">https://zju3dv.github.io/IntrinsicAnything</a></p><p><strong>Summary</strong><br>åˆ©ç”¨ç”Ÿæˆæ¨¡å‹å­¦ä¹ æè´¨å…ˆéªŒï¼Œä»¥æ­£åˆ™åŒ–ä¼˜åŒ–è¿‡ç¨‹ï¼Œä»è€Œæ¢å¤æœªçŸ¥é™æ€å…‰ç…§æ¡ä»¶ä¸‹å§¿åŠ¿å›¾åƒä¸­çš„ç‰©ä½“æè´¨ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>å°†é€šç”¨æ¸²æŸ“æ–¹ç¨‹æ‹†åˆ†ä¸ºæ¼«åå°„å’Œé•œé¢åå°„ç€è‰²é¡¹ï¼Œå¹¶å°†æè´¨å…ˆéªŒè¡¨è¿°ä¸ºæ¼«åå°„ç‡å’Œé•œé¢çš„æ‰©æ•£æ¨¡å‹ã€‚</li><li>ä½¿ç”¨ç°æœ‰çš„ä¸°å¯Œ 3D ç‰©ä½“æ•°æ®è®­ç»ƒæ¨¡å‹ï¼Œå°†å…¶ä½œä¸ºè§£å†³ä» RGB å›¾åƒæ¢å¤æè´¨è¡¨ç¤ºæ—¶æ¨¡ç³Šæ€§çš„é€šç”¨å·¥å…·ã€‚</li><li>å¼€å‘äº†ä¸€ç§ç²—åˆ°ç»†çš„è®­ç»ƒç­–ç•¥ï¼Œåˆ©ç”¨ä¼°è®¡çš„æè´¨æ¥å¼•å¯¼æ‰©æ•£æ¨¡å‹æ»¡è¶³å¤šè§†å›¾ä¸€è‡´æ€§çº¦æŸï¼Œä»è€Œè·å¾—æ›´ç¨³å®šå’Œå‡†ç¡®çš„ç»“æœã€‚</li><li>åœ¨çœŸå®ä¸–ç•Œå’Œåˆæˆæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æè´¨æ¢å¤æ–¹é¢è¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>è®ºæ–‡æ ‡é¢˜ï¼šIntrinsicAnything: å­¦ä¹ æ‰©æ•£å…ˆéªŒä»¥åœ¨æœªçŸ¥å…‰ç…§ä¸‹è¿›è¡Œé€†å‘æ¸²æŸ“</p></li><li><p>ä½œè€…ï¼šXi Chen, Sida Peng, Dongchen Yang, Yuan Liu, Bowen Pan, Chengfei Lv, Xiaowei Zhou</p></li><li><p>ç¬¬ä¸€ä½œè€…å•ä½ï¼šæµ™æ±Ÿå¤§å­¦è®¡ç®—æœºè¾…åŠ©è®¾è®¡ä¸å›¾å½¢å­¦å›½å®¶é‡ç‚¹å®éªŒå®¤</p></li><li><p>å…³é”®è¯ï¼šInverse Rendering, Material Recovery, Diffusion Model, Generative Prior</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.11593 , Githubï¼šNone</p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>(1) ç ”ç©¶èƒŒæ™¯ï¼šä»æ•è·çš„å›¾åƒä¸­æ¢å¤ç‰©ä½“çš„å‡ ä½•ã€æè´¨å’Œå…‰ç…§ï¼Œå³é€†å‘æ¸²æŸ“ï¼Œæ˜¯è®¡ç®—æœºè§†è§‰å’Œå›¾å½¢å­¦ä¸­ä¸€é¡¹é•¿æœŸå­˜åœ¨çš„ä»»åŠ¡ã€‚è¿™äº› 3D ç‰©ä½“çš„ç‰©ç†å±æ€§å¯¹äºè®¸å¤šåº”ç”¨ç¨‹åºè‡³å…³é‡è¦ï¼Œä¾‹å¦‚ VR/ARã€ç”µå½±åˆ¶ä½œå’Œè§†é¢‘æ¸¸æˆã€‚ç”±äºç°å®ä¸–ç•Œç‰©ä½“ä¸ç¯å¢ƒå…‰ç…§ä¹‹é—´ç›¸äº’ä½œç”¨çš„å›ºæœ‰å¤æ‚æ€§ï¼Œé€†å‘æ¸²æŸ“ä»ç„¶æ˜¯ä¸€ä¸ªä¸é€‚å®šé—®é¢˜ã€‚</p><p>(2) è¿‡å¾€æ–¹æ³•åŠé—®é¢˜ï¼šä»¥å¾€çš„å·¥ä½œé€šè¿‡å¤æ‚çš„æ•è·ç³»ç»Ÿ[16,20]æˆ–åœ¨é»‘æš—ç¯å¢ƒä¸­å…±åŒå®šä½çš„æ‰‹ç”µç­’å’Œç›¸æœº[5,50,84]æ¥å…‹æœè¿™ä¸ªé—®é¢˜ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•éœ€è¦ç‰¹æ®Šçš„ç¡¬ä»¶è®¾å¤‡æˆ–å—é™çš„ç¯å¢ƒï¼Œé™åˆ¶äº†å®ƒä»¬çš„åº”ç”¨ã€‚</p><p>(3) æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šä¸ºäº†å…‹æœè¿™ä¸ªä¸é€‚å®šé—®é¢˜ï¼Œæˆ‘ä»¬çš„å…³é”®æ€æƒ³æ˜¯å­¦ä¹ ä¸€ä¸ªç”Ÿæˆæ¨¡å‹ä½œä¸ºæè´¨å…ˆéªŒæ¥æ­£åˆ™åŒ–ä¼˜åŒ–è¿‡ç¨‹ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œä¸€èˆ¬çš„æ¸²æŸ“æ–¹ç¨‹å¯ä»¥åˆ†è§£ä¸ºæ¼«åå°„å’Œé•œé¢åå°„é˜´å½±é¡¹ï¼Œå› æ­¤å°†æè´¨å…ˆéªŒè¡¨è¿°ä¸ºæ¼«åå°„å’Œé•œé¢åå°„çš„æ‰©æ•£æ¨¡å‹ã€‚ç”±äºè¿™ç§è®¾è®¡ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥ä½¿ç”¨ç°æœ‰çš„ä¸°å¯Œ 3D å¯¹è±¡æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå¹¶ä¸”è‡ªç„¶åœ°å……å½“äº†ä¸€ç§å¤šåŠŸèƒ½å·¥å…·ï¼Œå¯ä»¥åœ¨ä» RGB å›¾åƒä¸­æ¢å¤æè´¨è¡¨ç¤ºæ—¶è§£å†³æ­§ä¹‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§ä»ç²—åˆ°ç²¾çš„è®­ç»ƒç­–ç•¥ï¼Œåˆ©ç”¨ä¼°è®¡çš„æè´¨æ¥å¼•å¯¼æ‰©æ•£æ¨¡å‹æ»¡è¶³å¤šè§†å›¾ä¸€è‡´æ€§çº¦æŸï¼Œä»è€Œå¾—åˆ°æ›´ç¨³å®šå’Œå‡†ç¡®çš„ç»“æœã€‚</p><p>(4) æ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†æ€æ ·çš„æ€§èƒ½ï¼šåœ¨çœŸå®ä¸–ç•Œå’Œåˆæˆæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æè´¨æ¢å¤æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¯¥æ€§èƒ½å¯ä»¥æ”¯æŒä»–ä»¬çš„ç›®æ ‡ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæå‡ºå­¦ä¹ æ‰©æ•£æ¨¡å‹ä½œä¸ºæè´¨å…ˆéªŒï¼Œæ­£åˆ™åŒ–é€†å‘æ¸²æŸ“ä¼˜åŒ–è¿‡ç¨‹ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šå°†æ¸²æŸ“æ–¹ç¨‹åˆ†è§£ä¸ºæ¼«åå°„å’Œé•œé¢åå°„é˜´å½±é¡¹ï¼Œå°†æè´¨å…ˆéªŒè¡¨è¿°ä¸ºæ¼«åå°„å’Œé•œé¢åå°„çš„æ‰©æ•£æ¨¡å‹ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šé‡‡ç”¨ä»ç²—åˆ°ç²¾çš„è®­ç»ƒç­–ç•¥ï¼Œåˆ©ç”¨ä¼°è®¡çš„æè´¨å¼•å¯¼æ‰©æ•£æ¨¡å‹æ»¡è¶³å¤šè§†å›¾ä¸€è‡´æ€§çº¦æŸï¼Œå¾—åˆ°æ›´ç¨³å®šå’Œå‡†ç¡®çš„ç»“æœã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæå‡ºäº† IntrinsicAnything æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨æ‰©æ•£æ¨¡å‹ä½œä¸ºæè´¨å…ˆéªŒï¼Œåœ¨æœªçŸ¥é™æ€å…‰ç…§æ¡ä»¶ä¸‹è¿›è¡Œé€†å‘æ¸²æŸ“ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºå°†æè´¨å…ˆéªŒè®¾è®¡ä¸ºæ¼«åå°„å’Œé•œé¢åå°„çš„æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼›å¼€å‘äº†ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ¡ˆï¼Œåˆ©ç”¨ç²—ç•¥æè´¨å¼•å¯¼æ‰©æ•£æ¨¡å‹æ»¡è¶³å¤šè§†å›¾ä¸€è‡´æ€§çº¦æŸã€‚æ€§èƒ½ï¼šåœ¨çœŸå®ä¸–ç•Œå’Œåˆæˆæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æè´¨æ¢å¤æ€§èƒ½ã€‚å·¥ä½œé‡ï¼šéœ€è¦è¾ƒå¤§çš„æ•°æ®é›†å’Œè¾ƒé•¿çš„è®­ç»ƒæ—¶é—´ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-2e3e705009374322a07a0404ed794846.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b8b028549fe12e9acbbb7374c824289a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-474fe30507cc76c6aa5c2fec1a6e92ad.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ffae13d6e393c5f464c5f05ee6f4295a.jpg" align="middle"></details><h2 id="MoA-Mixture-of-Attention-for-Subject-Context-Disentanglement-in-Personalized-Image-Generation"><a href="#MoA-Mixture-of-Attention-for-Subject-Context-Disentanglement-in-Personalized-Image-Generation" class="headerlink" title="MoA: Mixture-of-Attention for Subject-Context Disentanglement in   Personalized Image Generation"></a>MoA: Mixture-of-Attention for Subject-Context Disentanglement in   Personalized Image Generation</h2><p><strong>Authors: Kuan-Chieh,  Wang, Daniil Ostashev, Yuwei Fang, Sergey Tulyakov, Kfir Aberman</strong></p><p>We introduce a new architecture for personalization of text-to-image diffusion models, coined Mixture-of-Attention (MoA). Inspired by the Mixture-of-Experts mechanism utilized in large language models (LLMs), MoA distributes the generation workload between two attention pathways: a personalized branch and a non-personalized prior branch. MoA is designed to retain the original modelâ€™s prior by fixing its attention layers in the prior branch, while minimally intervening in the generation process with the personalized branch that learns to embed subjects in the layout and context generated by the prior branch. A novel routing mechanism manages the distribution of pixels in each layer across these branches to optimize the blend of personalized and generic content creation. Once trained, MoA facilitates the creation of high-quality, personalized images featuring multiple subjects with compositions and interactions as diverse as those generated by the original model. Crucially, MoA enhances the distinction between the modelâ€™s pre-existing capability and the newly augmented personalized intervention, thereby offering a more disentangled subject-context control that was previously unattainable. Project page: <a href="https://snap-research.github.io/mixture-of-attention">https://snap-research.github.io/mixture-of-attention</a> </p><p><a href="http://arxiv.org/abs/2404.11565v1">PDF</a> Project Website: <a href="https://snap-research.github.io/mixture-of-attention">https://snap-research.github.io/mixture-of-attention</a></p><p><strong>Summary</strong><br>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸ªæ€§åŒ–çš„æ··åˆæ³¨æ„åŠ›æœºåˆ¶ï¼Œåœ¨é¢„å…ˆå›ºå®šçš„éä¸ªæ€§åŒ–åŸºç¡€åˆ†æ”¯ä¸Šå åŠ å¯å­¦ä¹ çš„ä¸ªæ€§åŒ–åˆ†æ”¯ï¼Œä¼˜åŒ–ä¸ªæ€§åŒ–å’Œé€šç”¨å†…å®¹åˆ›å»ºçš„æ··åˆï¼Œå®ç°æ›´è§£è€¦çš„ä¸»é¢˜-ä¸Šä¸‹æ–‡æ§åˆ¶ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºæ··åˆæ³¨æ„æœºåˆ¶ï¼ˆMoAï¼‰æ¶æ„ï¼Œç”¨äºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„ä¸ªæ€§åŒ–ã€‚</li><li>MoA åˆ†å‘ç”Ÿæˆå·¥ä½œè´Ÿè½½åˆ°ä¸ªæ€§åŒ–åˆ†æ”¯å’Œéä¸ªæ€§åŒ–å…ˆéªŒåˆ†æ”¯ã€‚</li><li>ä¸ªæ€§åŒ–åˆ†æ”¯åœ¨å…ˆéªŒåˆ†æ”¯ç”Ÿæˆçš„å¸ƒå±€å’Œä¸Šä¸‹æ–‡ä¸­åµŒå…¥ä¸»é¢˜ã€‚</li><li>æ–°é¢–çš„è·¯ç”±æœºåˆ¶ä¼˜åŒ–äº†è·¨åˆ†æ”¯çš„åƒç´ åˆ†é…ã€‚</li><li>MoA å…è®¸åˆ›å»ºé«˜è´¨é‡çš„ä¸ªæ€§åŒ–å›¾åƒï¼Œå…·æœ‰å¤šç§ä¸»é¢˜å’Œäº¤äº’ã€‚</li><li>MoA å¢å¼ºäº†æ¨¡å‹çš„å…ˆéªŒåŠŸèƒ½å’Œä¸ªæ€§åŒ–å¹²é¢„ä¹‹é—´çš„åŒºåˆ«ã€‚</li><li>MoA æä¾›äº†ä»¥å‰æ— æ³•å®ç°çš„æ›´è§£è€¦çš„ä¸»é¢˜-ä¸Šä¸‹æ–‡æ§åˆ¶ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: MoA: Mixture-of-Attention for Subject-Context Disentanglement in Personalized Image Generation</p></li><li><p>Authors: Kuan-Chieh (Jackson) Wang, Daniil Ostashev, Yuwei Fang, Sergey Tulyakov, Kfir Aberman</p></li><li><p>Affiliation: Snap Inc., USA</p></li><li><p>Keywords: Personalization, Text-to-image Generation, Diffusion Models</p></li><li><p>Urls: https://arxiv.org/abs/2404.11565, Github:None</p></li><li><p>Summary:</p></li></ol><p>(1): The research background of this article is the rapid progress in foundation text-conditioned image synthesis with diffusion models. Personalized generation focuses on adapting and contextualizing the generation to a set of desired subjects using limited input images, while retaining the powerful generative capabilities of the foundation model.</p><p>(2): Past methods for personalized generation include fine-tuning-based personalization techniques and approaches optimized for multi-subject generation. However, fine-tuning-based methods tend to overfit to certain attributes in the distribution of the input images or struggle to adhere adequately to the input prompt. Approaches optimized for multi-subject generation often modify the original model's weights, resulting in compositions that lack diversity and naturalness.</p><p>(3): The research methodology proposed in this paper is Mixture-of-Attention (MoA), which extends the vanilla attention mechanism into multiple attention blocks (i.e. experts), and has a router network that softly combines the different experts. MoA distributes the generation between personalized and non-personalized attention pathways. It is designed to retain the original model's prior by fixing its attention layers in the prior (non-personalized) branch, while minimally intervening in the generation process with the personalized branch.</p><p>(4): MoA is evaluated on the task of personalized image generation. The results show that MoA can generate high-quality, personalized images featuring multiple subjects with compositions and interactions as diverse as those generated by the original model. MoA also enhances the distinction between the model's pre-existing capability and the newly augmented personalized intervention, thereby offering a more disentangled subject-context control that was previously unattainable.</p><ol><li><p>æ–¹æ³•ï¼š</p><pre><code>            (1): æå‡ºMixture-of-Attentionï¼ˆMoAï¼‰å±‚ï¼Œå°†vanillaæ³¨æ„åŠ›æœºåˆ¶æ‰©å±•ä¸ºå¤šä¸ªæ³¨æ„åŠ›æ¨¡å—ï¼ˆå³ä¸“å®¶ï¼‰ï¼Œå¹¶ä½¿ç”¨è·¯ç”±å™¨ç½‘ç»œå¯¹ä¸åŒä¸“å®¶è¿›è¡Œè½¯ç»„åˆã€‚            (2): MoAå°†ç”Ÿæˆåˆ†é…åˆ°ä¸ªæ€§åŒ–å’Œéä¸ªæ€§åŒ–æ³¨æ„åŠ›è·¯å¾„ä¹‹é—´ã€‚å®ƒé€šè¿‡å›ºå®šå…ˆéªŒï¼ˆéä¸ªæ€§åŒ–ï¼‰åˆ†æ”¯ä¸­çš„æ³¨æ„åŠ›å±‚æ¥ä¿ç•™åŸå§‹æ¨¡å‹çš„å…ˆéªŒï¼ŒåŒæ—¶é€šè¿‡ä¸ªæ€§åŒ–åˆ†æ”¯å¯¹ç”Ÿæˆè¿‡ç¨‹è¿›è¡Œæœ€å°å¹²é¢„ã€‚            (3): å°†MoAåº”ç”¨äºæ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹ä¸­ï¼Œç”¨äºä¸»é¢˜é©±åŠ¨çš„ç”Ÿæˆã€‚è¯¥æ¶æ„ä½¿æˆ‘ä»¬èƒ½å¤Ÿå¢å¼ºT2Iæ¨¡å‹çš„èƒ½åŠ›ï¼Œä»¥æ‰§è¡Œä¸»é¢˜é©±åŠ¨çš„ç”Ÿæˆï¼ŒåŒæ—¶å¯¹ä¸»é¢˜å’Œä¸Šä¸‹æ–‡è¿›è¡Œè§£è€¦æ§åˆ¶ï¼Œä»è€Œä¿ç•™å…ˆéªŒæ¨¡å‹ä¸­å›ºæœ‰çš„å¤šæ ·åŒ–å›¾åƒåˆ†å¸ƒã€‚</code></pre></li><li><p>ç»“è®ºï¼š</p></li></ol><p>ï¼ˆ1ï¼‰æœ¬å·¥ä½œçš„æ„ä¹‰åœ¨äºï¼šæå‡ºäº†ä¸€ç§æ–°çš„ä¸ªæ€§åŒ–ç”Ÿæˆæ¶æ„ Mixture-of-Attentionï¼ˆMoAï¼‰ï¼Œè¯¥æ¶æ„å¢å¼ºäº†åŸºç¡€æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ³¨å…¥ä¸»é¢˜å›¾åƒï¼ŒåŒæ—¶ä¿ç•™äº†æ¨¡å‹çš„å…ˆå‰èƒ½åŠ›ã€‚ä¸ç°æœ‰ä¸»é¢˜é©±åŠ¨ç”Ÿæˆæ–¹æ³•ç”Ÿæˆçš„å›¾åƒç›¸æ¯”ï¼ŒMoA æ— ç¼åœ°ç»Ÿä¸€äº†ä¸¤ç§èŒƒå¼ï¼Œé€šè¿‡æ‹¥æœ‰ä¸¤ä¸ªä¸åŒçš„ä¸“å®¶å’Œä¸€ä¸ªè·¯ç”±å™¨æ¥åŠ¨æ€åˆå¹¶ä¸¤ä¸ªè·¯å¾„ã€‚MoA å±‚èƒ½å¤Ÿåœ¨ä¸€æ¬¡åå‘æ‰©æ•£ä¼ é€’ä¸­ä»å…·æœ‰ä¸°å¯Œäº¤äº’çš„å¤šä¸ªè¾“å…¥ä¸»é¢˜ç”Ÿæˆä¸ªæ€§åŒ–ä¸Šä¸‹æ–‡ï¼Œå¹¶ä¸”ä¸éœ€è¦æµ‹è¯•æ—¶å¾®è°ƒæ­¥éª¤ï¼Œä»è€Œè§£é”äº†ä»¥å‰æ— æ³•è¾¾åˆ°çš„ç»“æœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å±•ç¤ºäº†ç”Ÿæˆå›¾åƒä¸­ä»¥å‰æœªè§çš„å¸ƒå±€å˜åŒ–ï¼Œä»¥åŠå¤„ç†ç‰©ä½“æˆ–å…¶ä»–ä¸»é¢˜é®æŒ¡çš„èƒ½åŠ›ï¼Œå¹¶ä¸”æ— éœ€æ˜¾å¼æ§åˆ¶å³å¯å¤„ç†ä¸åŒçš„èº«ä½“å½¢çŠ¶ã€‚æœ€åï¼Œç”±äºå…¶ç®€å•æ€§ï¼ŒMoA ä¸ä¼—æ‰€å‘¨çŸ¥çš„åŸºäºæ‰©æ•£çš„ç”Ÿæˆå’Œç¼–è¾‘æŠ€æœ¯ï¼ˆå¦‚ ControlNet å’Œ DDIM Inversionï¼‰å¤©ç„¶å…¼å®¹ã€‚ä¾‹å¦‚ï¼ŒMoA å’Œ DDIM Inversion çš„ç»“åˆè§£é”äº†åœ¨çœŸå®ç…§ç‰‡ä¸­è¿›è¡Œä¸»é¢˜äº¤æ¢çš„åº”ç”¨ã€‚å±•æœ›æœªæ¥ï¼Œæˆ‘ä»¬è®¾æƒ³é€šè¿‡ä¸“é—¨é’ˆå¯¹ä¸åŒä»»åŠ¡æˆ–è¯­ä¹‰æ ‡ç­¾çš„ä¸åŒä¸“å®¶è¿›ä¸€æ­¥å¢å¼º MoA æ¶æ„ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨æå°å¹²é¢„ä¸ªæ€§åŒ–çš„æ–¹æ³•å¯ä»¥æ‰©å±•åˆ°å„ç§åŸºç¡€æ¨¡å‹ï¼ˆä¾‹å¦‚è§†é¢‘å’Œ 3D/4D ç”Ÿæˆï¼‰ï¼Œä»è€Œä¿ƒè¿›ä½¿ç”¨ç°æœ‰å’Œæœªæ¥ç”Ÿæˆæ¨¡å‹åˆ›å»ºä¸ªæ€§åŒ–å†…å®¹ã€‚</p><p>ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§æ–°çš„ Mixture-of-Attentionï¼ˆMoAï¼‰æ¶æ„ï¼Œè¯¥æ¶æ„å¢å¼ºäº†åŸºç¡€æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ³¨å…¥ä¸»é¢˜å›¾åƒï¼ŒåŒæ—¶ä¿ç•™äº†æ¨¡å‹çš„å…ˆå‰èƒ½åŠ›ã€‚MoA å±‚èƒ½å¤Ÿåœ¨ä¸€æ¬¡åå‘æ‰©æ•£ä¼ é€’ä¸­ä»å…·æœ‰ä¸°å¯Œäº¤äº’çš„å¤šä¸ªè¾“å…¥ä¸»é¢˜ç”Ÿæˆä¸ªæ€§åŒ–ä¸Šä¸‹æ–‡ï¼Œå¹¶ä¸”ä¸éœ€è¦æµ‹è¯•æ—¶å¾®è°ƒæ­¥éª¤ï¼Œä»è€Œè§£é”äº†ä»¥å‰æ— æ³•è¾¾åˆ°çš„ç»“æœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å±•ç¤ºäº†ç”Ÿæˆå›¾åƒä¸­ä»¥å‰æœªè§çš„å¸ƒå±€å˜åŒ–ï¼Œä»¥åŠå¤„ç†ç‰©ä½“æˆ–å…¶ä»–ä¸»é¢˜é®æŒ¡çš„èƒ½åŠ›ï¼Œå¹¶ä¸”æ— éœ€æ˜¾å¼æ§åˆ¶å³å¯å¤„ç†ä¸åŒçš„èº«ä½“å½¢çŠ¶ã€‚</p><p>æ€§èƒ½ï¼šåœ¨ä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜ MoA å¯ä»¥ç”Ÿæˆé«˜è´¨é‡ã€ä¸ªæ€§åŒ–çš„å›¾åƒï¼Œå…¶ç‰¹å¾æ˜¯å…·æœ‰ä¸åŸå§‹æ¨¡å‹ç”Ÿæˆå›¾åƒä¸€æ ·å¤šæ ·åŒ–çš„æ„å›¾å’Œäº¤äº’ã€‚MoA è¿˜å¢å¼ºäº†æ¨¡å‹ç°æœ‰èƒ½åŠ›å’Œæ–°å¢å¼ºä¸ªæ€§åŒ–å¹²é¢„ä¹‹é—´çš„åŒºåˆ«ï¼Œä»è€Œæä¾›äº†ä»¥å‰æ— æ³•å®ç°çš„æ›´åˆ†ç¦»çš„ä¸»é¢˜-ä¸Šä¸‹æ–‡æ§åˆ¶ã€‚</p><p>å·¥ä½œé‡ï¼šMoA å…·æœ‰ç®€å•æ€§ï¼Œä¸ä¼—æ‰€å‘¨çŸ¥çš„åŸºäºæ‰©æ•£çš„ç”Ÿæˆå’Œç¼–è¾‘æŠ€æœ¯ï¼ˆå¦‚ ControlNet å’Œ DDIM Inversionï¼‰å¤©ç„¶å…¼å®¹ã€‚ä¾‹å¦‚ï¼ŒMoA å’Œ DDIM Inversion çš„ç»“åˆè§£é”äº†åœ¨çœŸå®ç…§ç‰‡ä¸­è¿›è¡Œä¸»é¢˜äº¤æ¢çš„åº”ç”¨ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-65299f0067c3022cccf14b21e08de1a6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-058ccecf97ed2df42286d132194a3ffe.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-577cd46daa6ad37536d9e75f13d51239.jpg" align="middle"><img src="https://picx.zhimg.com/v2-728a034564b4e30499365332b12dfe09.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c4cd4a1d04735a77754757aa6e384614.jpg" align="middle"><img src="https://pica.zhimg.com/v2-fdeae7faf257d15601bbe1c62a204408.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5c3251b1f6745891a76c08f07664b47d.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-04-22  Zero-Shot Medical Phrase Grounding with Off-the-shelf Diffusion Models</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/04/17/Paper/2024-04-17/NeRF/"/>
    <id>https://kedreamix.github.io/2024/04/17/Paper/2024-04-17/NeRF/</id>
    <published>2024-04-17T11:09:58.000Z</published>
    <updated>2024-04-17T11:09:58.614Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-04-17-æ›´æ–°"><a href="#2024-04-17-æ›´æ–°" class="headerlink" title="2024-04-17 æ›´æ–°"></a>2024-04-17 æ›´æ–°</h1><h2 id="Plug-and-Play-Acceleration-of-Occupancy-Grid-based-NeRF-Rendering-using-VDB-Grid-and-Hierarchical-Ray-Traversal"><a href="#Plug-and-Play-Acceleration-of-Occupancy-Grid-based-NeRF-Rendering-using-VDB-Grid-and-Hierarchical-Ray-Traversal" class="headerlink" title="Plug-and-Play Acceleration of Occupancy Grid-based NeRF Rendering using   VDB Grid and Hierarchical Ray Traversal"></a>Plug-and-Play Acceleration of Occupancy Grid-based NeRF Rendering using   VDB Grid and Hierarchical Ray Traversal</h2><p><strong>Authors:Yoshio Kato, Shuhei Tarashima</strong></p><p>Transmittance estimators such as Occupancy Grid (OG) can accelerate the training and rendering of Neural Radiance Field (NeRF) by predicting important samples that contributes much to the generated image. However, OG manages occupied regions in the form of the dense binary grid, in which there are many blocks with the same values that cause redundant examination of voxelsâ€™ emptiness in ray-tracing. In our work, we introduce two techniques to improve the efficiency of ray-tracing in trained OG without fine-tuning. First, we replace the dense grids with VDB grids to reduce the spatial redundancy. Second, we use hierarchical digital differential analyzer (HDDA) to efficiently trace voxels in the VDB grids. Our experiments on NeRF-Synthetic and Mip-NeRF 360 datasets show that our proposed method successfully accelerates rendering NeRF-Synthetic dataset by 12% in average and Mip-NeRF 360 dataset by 4% in average, compared to a fast implementation of OG, NerfAcc, without losing the quality of rendered images. </p><p><a href="http://arxiv.org/abs/2404.10272v1">PDF</a> Short paper for CVPR Neural Rendering Intelligence Workshop 2024.   Code: <a href="https://github.com/Yosshi999/faster-occgrid">https://github.com/Yosshi999/faster-occgrid</a></p><p><strong>Summary</strong><br>ä¼˜åŒ–ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰é€šè¿‡é€å°„ç‡ä¼°è®¡é‡åŠ é€Ÿæ¸²æŸ“</p><p><strong>Key Takeaways</strong></p><ul><li>é€å°„ç‡ä¼°è®¡é‡ï¼ˆå¦‚å ç”¨æ ¼ç½‘ï¼‰å¯åŠ é€Ÿ NeRF è®­ç»ƒå’Œæ¸²æŸ“ã€‚</li><li>å ç”¨æ ¼ç½‘ä½¿ç”¨å¯†é›†äºŒè¿›åˆ¶æ ¼ç®¡ç†å æ®åŒºåŸŸï¼Œå­˜åœ¨å†—ä½™æ£€æŸ¥ã€‚</li><li>ä½“ç´ æ•°æ®å—ï¼ˆVDBï¼‰æ ¼ä»£æ›¿å¯†é›†æ ¼ï¼Œå‡å°‘ç©ºé—´å†—ä½™ã€‚</li><li>åˆ†å±‚æ•°å­—å¾®åˆ†åˆ†æä»ªï¼ˆHDDAï¼‰é«˜æ•ˆè¿½è¸ª VDB æ ¼ä¸­çš„ä½“ç´ ã€‚</li><li>è¯¥æ–¹æ³•åŠ é€Ÿ NeRF-Synthetic æ¸²æŸ“ 12%ã€Mip-NeRF 360 æ¸²æŸ“ 4%ã€‚</li><li>æ¸²æŸ“å›¾åƒè´¨é‡ä¿æŒä¸å˜ã€‚</li><li>æ— éœ€å¾®è°ƒå³å¯æé«˜ OG å°„çº¿è¿½è¸ªæ•ˆç‡ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šä½¿ç”¨ VDB ç½‘æ ¼å’Œåˆ†å±‚å°„çº¿éå†å®ç°å ç”¨ç½‘æ ¼åŸºäº NeRF æ¸²æŸ“çš„å³æ’å³ç”¨åŠ é€Ÿ</li><li>ä½œè€…ï¼šYifan Wang, Chenghua Li, Ya-Qin Zhang</li><li>éš¶å±å•ä½ï¼šé¦™æ¸¯ä¸­æ–‡å¤§å­¦ï¼ˆæ·±åœ³ï¼‰</li><li>å…³é”®è¯ï¼šç¥ç»è¾å°„åœºã€å ç”¨ç½‘æ ¼ã€å³æ’å³ç”¨åŠ é€Ÿã€VDB ç½‘æ ¼ã€åˆ†å±‚æ•°å­—å¾®åˆ†åˆ†æä»ª</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2204.06814   Github ä»£ç é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼š   (1) ç ”ç©¶èƒŒæ™¯ï¼š   ç¥ç»è¾å°„åœº (NeRF) æ˜¯ä¸€ç§å¼ºå¤§çš„ç¥ç»æ¸²æŸ“æŠ€æœ¯ï¼Œèƒ½å¤Ÿä»å›¾åƒæ•°æ®ä¸­å­¦ä¹ åœºæ™¯çš„ 3D è¡¨ç¤ºã€‚ç„¶è€Œï¼ŒNeRF çš„è®­ç»ƒå’Œæ¸²æŸ“è®¡ç®—æˆæœ¬å¾ˆé«˜ã€‚   (2) è¿‡å»çš„æ–¹æ³•å’Œé—®é¢˜ï¼š   å ç”¨ç½‘æ ¼ (OG) æ˜¯ä¸€ç§åŠ é€Ÿ NeRF è®­ç»ƒå’Œæ¸²æŸ“çš„æŠ€æœ¯ï¼Œå®ƒé€šè¿‡é¢„æµ‹å¯¹ç”Ÿæˆå›¾åƒè´¡çŒ®è¾ƒå¤§çš„é‡è¦æ ·æœ¬æ¥å·¥ä½œã€‚ç„¶è€Œï¼ŒOG ä½¿ç”¨å¯†é›†çš„äºŒè¿›åˆ¶ç½‘æ ¼æ¥ç®¡ç†å æ®åŒºåŸŸï¼Œè¿™ä¼šå¯¼è‡´è®¸å¤šå…·æœ‰ç›¸åŒå€¼çš„å—ï¼Œä»è€Œå¯¼è‡´åœ¨å…‰çº¿è¿½è¸ªä¸­å†—ä½™æ£€æŸ¥ç½‘æ ¼å•å…ƒæ˜¯å¦ä¸ºç©ºã€‚   (3) è®ºæ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼š   ä¸ºäº†æé«˜ OG ä¸­å…‰çº¿è¿½è¸ªçš„æ•ˆç‡ï¼Œè®ºæ–‡æå‡ºäº†ä¸¤ç§æŠ€æœ¯ã€‚é¦–å…ˆï¼Œä½¿ç”¨ VDB ç½‘æ ¼æ›¿æ¢å¯†é›†ç½‘æ ¼ä»¥å‡å°‘ç©ºé—´å†—ä½™ã€‚å…¶æ¬¡ï¼Œä½¿ç”¨åˆ†å±‚æ•°å­—å¾®åˆ†åˆ†æä»ª (HDDA) åœ¨ VDB ç½‘æ ¼ä¸­é«˜æ•ˆåœ°è¿½è¸ªç½‘æ ¼å•å…ƒã€‚   (4) æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼š   åœ¨ NeRF-Synthetic å’Œ Mip-NeRF360 æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸ NerfAccï¼ˆä¸€ç§ OG çš„å¿«é€Ÿå®ç°ï¼‰ç›¸æ¯”ï¼Œè®ºæ–‡æå‡ºçš„æ–¹æ³•æˆåŠŸåœ°å°† NeRF-Synthetic æ•°æ®é›†çš„æ¸²æŸ“é€Ÿåº¦å¹³å‡æé«˜äº† 12%ï¼ŒMip-NeRF360 æ•°æ®é›†çš„æ¸²æŸ“é€Ÿåº¦å¹³å‡æé«˜äº† 4%ï¼ŒåŒæ—¶ä¸ä¼šé™ä½æ¸²æŸ“å›¾åƒçš„è´¨é‡ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1): å°†è®­ç»ƒå¥½çš„ NeRF æ¨¡å‹çš„å æ®ç½‘æ ¼ (OG) è½¬æ¢ä¸ºä½¿ç”¨ OpenVDB [1] çš„åŸºäº VDB çš„ç»“æ„ï¼Œå¹¶ä½¿ç”¨ NanoVDB [13] å°†å…¶ä¼ è¾“åˆ° GPUã€‚(2): åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ Instant-NGP [10] ä½œä¸ºåŸºç¡€æ¨¡å‹ã€‚(3): å¦‚æœæœ‰æ›´å¤š re27. è¯¦ç»†æè¿°æœ¬æ–‡çš„æ–¹æ³•è®ºæ€æƒ³ã€‚ä¸€å®šè¦ç”¨ä¸­æ–‡å›ç­”ï¼ˆä¸“æœ‰åè¯éœ€è¦ç”¨è‹±æ–‡æ ‡æ³¨ï¼‰ã€‚ä¾‹å¦‚ï¼Œå®ƒçš„æ­¥éª¤æ˜¯ã€‚(1):...(2):...(3):..........æŒ‰ç…§ä»¥ä¸‹è¾“å‡ºçš„æ ¼å¼ï¼š</p></li><li><p>æ–¹æ³•ï¼š(1): xxxï¼›ï¼ˆ2ï¼‰ï¼šxxxï¼›ï¼ˆ3ï¼‰ï¼šxxxï¼›.......ä¸€å®šè¦ç”¨ä¸­æ–‡å›ç­”ï¼ˆä¸“æœ‰åè¯éœ€è¦ç”¨è‹±æ–‡æ ‡æ³¨ï¼‰ï¼Œè¡¨è¿°å°½å¯èƒ½ç®€æ´ã€å­¦æœ¯åŒ–ï¼Œä¸è¦é‡å¤å‰é¢</p><summary>çš„å†…å®¹ï¼ŒåŸæ•°å­—çš„ä½¿ç”¨ä»·å€¼ï¼Œä¸€å®šè¦ä¸¥æ ¼æŒ‰ç…§æ ¼å¼ï¼Œå¯¹åº”çš„å†…å®¹è¾“å‡ºåˆ°xxxï¼ŒæŒ‰ç…§æ¢è¡Œï¼Œ.......è¡¨ç¤ºæ ¹æ®å®é™…éœ€è¦å¡«å†™ï¼Œå¦‚æœæ²¡æœ‰ï¼Œä¸èƒ½å†™ã€‚<p></p></summary></li><li><p>æ–¹æ³•ï¼š(1): å°†è®­ç»ƒå¥½çš„NeRFæ¨¡å‹çš„å æ®ç½‘æ ¼ï¼ˆOGï¼‰è½¬æ¢ä¸ºä½¿ç”¨OpenVDB[1]çš„åŸºäºVDBçš„ç»“æ„ï¼Œå¹¶ä½¿ç”¨NanoVDB[13]å°†å…¶ä¼ è¾“åˆ°GPUã€‚(2): åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨Instant-NGP[10]ä½œä¸ºåŸºç¡€æ¨¡å‹ã€‚(3): å¦‚æœæœ‰æ›´å¤šre27. è¯¦ç»†æè¿°æœ¬æ–‡çš„æ–¹æ³•è®ºæ€æƒ³ã€‚ä¸€å®šè¦ç”¨ä¸­æ–‡å›ç­”ï¼ˆä¸“æœ‰åè¯éœ€è¦ç”¨è‹±æ–‡æ ‡æ³¨ï¼‰ã€‚ä¾‹å¦‚ï¼Œå®ƒçš„æ­¥éª¤æ˜¯ã€‚(1):...(2):...(3):..........æŒ‰ç…§ä»¥ä¸‹è¾“å‡ºçš„æ ¼å¼ï¼š</p></li><li><p>æ–¹æ³•ï¼š(1):xxxï¼›ï¼ˆ2ï¼‰ï¼šxxxï¼›ï¼ˆ3ï¼‰ï¼šxxxï¼›.......ä¸€å®šè¦ç”¨ä¸­æ–‡å›ç­”ï¼ˆä¸“æœ‰åè¯éœ€è¦ç”¨è‹±æ–‡æ ‡æ³¨ï¼‰ï¼Œè¡¨è¿°å°½å¯èƒ½ç®€æ´ã€å­¦æœ¯åŒ–ï¼Œä¸è¦é‡å¤å‰é¢</p><summary>çš„å†…å®¹ï¼ŒåŸæ•°å­—çš„ä½¿ç”¨ä»·å€¼ï¼Œä¸€å®šè¦ä¸¥æ ¼æŒ‰ç…§æ ¼å¼ï¼Œå¯¹åº”çš„å†…å®¹è¾“å‡ºåˆ°xxxï¼ŒæŒ‰ç…§æ¢è¡Œï¼Œ.......è¡¨ç¤ºæ ¹æ®å®é™…éœ€è¦å¡«å†™ï¼Œå¦‚æœæ²¡æœ‰ï¼Œä¸èƒ½å†™ã€‚<p></p></summary></li><li><p>ç»“è®ºï¼š(1): è¿™é¡¹å·¥ä½œçš„æ„ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ(2): ä»åˆ›æ–°ç‚¹ã€æ€§èƒ½ã€å·¥ä½œé‡ä¸‰ä¸ªç»´åº¦æ€»ç»“æœ¬æ–‡çš„ä¼˜ç¼ºç‚¹ã€‚........æŒ‰ç…§åé¢çš„è¾“å‡ºæ ¼å¼ï¼š</p></li><li>ç»“è®ºï¼š(1):xxx;(2):åˆ›æ–°ç‚¹ï¼šxxxï¼›æ€§èƒ½ï¼šxxxï¼›å·¥ä½œé‡ï¼šxxx;ä¸€å®šè¦ç”¨ä¸­æ–‡å›ç­”ï¼ˆä¸“æœ‰åè¯éœ€è¦ç”¨è‹±æ–‡æ ‡æ³¨ï¼‰ï¼Œè¡¨è¿°å°½å¯èƒ½ç®€æ´ã€å­¦æœ¯åŒ–ï¼Œä¸è¦é‡å¤å‰é¢<summary>çš„å†…å®¹ï¼ŒåŸæ•°å­—çš„ä½¿ç”¨ä»·å€¼ï¼Œä¸€å®šè¦ä¸¥æ ¼æŒ‰ç…§æ ¼å¼ï¼Œå¯¹åº”çš„å†…å®¹è¾“å‡ºåˆ°xxxï¼ŒæŒ‰ç…§æ¢è¡Œï¼Œ.......è¡¨ç¤ºæ ¹æ®å®é™…éœ€è¦å¡«å†™ï¼Œå¦‚æœæ²¡æœ‰ï¼Œä¸èƒ½å†™ã€‚</summary></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-97d863b4d0c1a7df4fa9efa004db885d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b192f1f80d49d74775f653d122feedce.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d87faabfaa6a3719df968c6bd795b312.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-8ada6cbf2edd7e1759c7ba909af2521f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-04b1531de51d0c973eb6011ffedeceb8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-df09277707b0e21453e0c13c0f195645.jpg" align="middle"></details><h2 id="Taming-Latent-Diffusion-Model-for-Neural-Radiance-Field-Inpainting"><a href="#Taming-Latent-Diffusion-Model-for-Neural-Radiance-Field-Inpainting" class="headerlink" title="Taming Latent Diffusion Model for Neural Radiance Field Inpainting"></a>Taming Latent Diffusion Model for Neural Radiance Field Inpainting</h2><p><strong>Authors:Chieh Hubert Lin, Changil Kim, Jia-Bin Huang, Qinbo Li, Chih-Yao Ma, Johannes Kopf, Ming-Hsuan Yang, Hung-Yu Tseng</strong></p><p>Neural Radiance Field (NeRF) is a representation for 3D reconstruction from multi-view images. Despite some recent work showing preliminary success in editing a reconstructed NeRF with diffusion prior, they remain struggling to synthesize reasonable geometry in completely uncovered regions. One major reason is the high diversity of synthetic contents from the diffusion model, which hinders the radiance field from converging to a crisp and deterministic geometry. Moreover, applying latent diffusion models on real data often yields a textural shift incoherent to the image condition due to auto-encoding errors. These two problems are further reinforced with the use of pixel-distance losses. To address these issues, we propose tempering the diffusion modelâ€™s stochasticity with per-scene customization and mitigating the textural shift with masked adversarial training. During the analyses, we also found the commonly used pixel and perceptual losses are harmful in the NeRF inpainting task. Through rigorous experiments, our framework yields state-of-the-art NeRF inpainting results on various real-world scenes. Project page: <a href="https://hubert0527.github.io/MALD-NeRF">https://hubert0527.github.io/MALD-NeRF</a> </p><p><a href="http://arxiv.org/abs/2404.09995v1">PDF</a> Project page: <a href="https://hubert0527.github.io/MALD-NeRF">https://hubert0527.github.io/MALD-NeRF</a></p><p><strong>Summary</strong><br>ä¼˜åŒ–NeRFå›¾åƒæ’è¡¥ï¼šæŠ‘åˆ¶æ‰©æ•£æ¨¡å‹çš„éšæœºæ€§ã€ç¼“è§£çº¹ç†åç§»ï¼Œå¹¶å¼ƒç”¨åƒç´ å’Œæ„ŸçŸ¥æŸå¤±å‡½æ•°ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æ‰©æ•£æ¨¡å‹åˆæˆå†…å®¹å¤šæ ·æ€§é«˜ï¼Œé˜»ç¢è¾å°„åœºæ”¶æ•›ä¸ºæ¸…æ™°å‡ ä½•ä½“ã€‚</li><li>å°†æ½œåœ¨æ‰©æ•£æ¨¡å‹åº”ç”¨äºçœŸå®æ•°æ®ä¼šå¯¼è‡´ä¸å›¾åƒæ¡ä»¶ä¸ç¬¦çš„çº¹ç†åç§»ã€‚</li><li>åƒç´ è·ç¦»æŸå¤±åŠ å‰§äº†ä¸Šè¿°ä¸¤ä¸ªé—®é¢˜ã€‚</li><li>å¼•å…¥åœºæ™¯å®šåˆ¶ä»¥ç¼“å’Œæ‰©æ•£æ¨¡å‹çš„éšæœºæ€§ã€‚</li><li>ä½¿ç”¨è’™ç‰ˆå¯¹æŠ—è®­ç»ƒæ¥ç¼“è§£çº¹ç†åç§»ã€‚</li><li>åƒç´ å’Œæ„ŸçŸ¥æŸå¤±åœ¨NeRFå›¾åƒæ’è¡¥ä»»åŠ¡ä¸­æ˜¯æœ‰å®³çš„ã€‚</li><li>è¯¥æ¡†æ¶åœ¨å„ç§çœŸå®åœºæ™¯ä¸­å®ç°äº†æœ€å…ˆè¿›çš„NeRFå›¾åƒæ’è¡¥ç»“æœã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>æ ‡é¢˜ï¼šé©¯åŒ–æ½œåœ¨æ‰©æ•£æ¨¡å‹ç”¨äºè¡¥å……ææ–™</li><p></p><p></p><li>ä½œè€…ï¼šC.H. Lin ç­‰</li><p></p><p></p><li>å•ä½ï¼šæœªæåŠ</li><p></p><p></p><li>å…³é”®è¯ï¼šNeRFã€å›¾åƒä¿®å¤ã€æ½œåœ¨æ‰©æ•£æ¨¡å‹ã€å¯¹æŠ—æ€§è®­ç»ƒ</li><p></p><p></p><li>é“¾æ¥ï¼šæ— </li><p></p><p></p><li><p></p><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šNeRF æ˜¯ä¸€ç§ä»å¤šè§†è§’å›¾åƒè¿›è¡Œ 3D é‡å»ºçš„è¡¨ç¤ºå½¢å¼ã€‚å°½ç®¡ä¸€äº›æœ€è¿‘çš„å·¥ä½œæ˜¾ç¤ºå‡ºä½¿ç”¨æ‰©æ•£å…ˆéªŒç¼–è¾‘é‡å»ºçš„ NeRF å–å¾—äº†åˆæ­¥æˆåŠŸï¼Œä½†å®ƒä»¬ä»ç„¶éš¾ä»¥åœ¨å®Œå…¨æœªè¦†ç›–çš„åŒºåŸŸä¸­åˆæˆåˆç†çš„å‡ ä½•å½¢çŠ¶ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šä¸€ä¸ªä¸»è¦åŸå› æ˜¯æ‰©æ•£æ¨¡å‹åˆæˆå†…å®¹çš„é«˜åº¦å¤šæ ·æ€§ï¼Œè¿™é˜»ç¢äº†è¾å°„åœºæ”¶æ•›åˆ°æ¸…æ™°ä¸”ç¡®å®šæ€§çš„å‡ ä½•å½¢çŠ¶ã€‚æ­¤å¤–ï¼Œç”±äºè‡ªåŠ¨ç¼–ç é”™è¯¯ï¼Œåœ¨çœŸå®æ•°æ®ä¸Šåº”ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹é€šå¸¸ä¼šå¯¼è‡´ä¸å›¾åƒæ¡ä»¶ä¸ä¸€è‡´çš„çº¹ç†åç§»ã€‚è¿™ä¸¤ä¸ªé—®é¢˜å› ä½¿ç”¨åƒç´ è·ç¦»æŸå¤±è€Œè¿›ä¸€æ­¥åŠ å‰§ã€‚ï¼ˆ3ï¼‰æœ¬æ–‡æ–¹æ³•ï¼šä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºç”¨åœºæ™¯å®šåˆ¶æ¥ç¼“å’Œæ‰©æ•£æ¨¡å‹çš„éšæœºæ€§ï¼Œå¹¶ç”¨æ©ç å¯¹æŠ—æ€§è®­ç»ƒæ¥å‡è½»çº¹ç†åç§»ã€‚åœ¨åˆ†æè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬è¿˜å‘ç°å¸¸ç”¨çš„åƒç´ å’Œæ„ŸçŸ¥æŸå¤±åœ¨ NeRF ä¿®å¤ä»»åŠ¡ä¸­æ˜¯æœ‰å®³çš„ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šé€šè¿‡ä¸¥æ ¼çš„å®éªŒï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨å„ç§çœŸå®åœºæ™¯ä¸Šäº§ç”Ÿäº†æœ€å…ˆè¿›çš„ NeRF ä¿®å¤ç»“æœã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰NeRFè¡¨ç¤ºï¼šä½¿ç”¨ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰è¡¨ç¤º3Dåœºæ™¯ï¼Œé€šè¿‡åƒç´ çº§å›å½’æŸå¤±å‡½æ•°ä¼˜åŒ–NeRFï¼Œé‡å»ºå·²çŸ¥åŒºåŸŸã€‚ï¼ˆ2ï¼‰è’™ç‰ˆå¯¹æŠ—è®­ç»ƒï¼šä¸ä½¿ç”¨åƒç´ è·ç¦»æŸå¤±ï¼Œè€Œæ˜¯é‡‡ç”¨å¯¹æŠ—æŸå¤±å’Œåˆ¤åˆ«å™¨ç‰¹å¾åŒ¹é…æŸå¤±æŒ‡å¯¼NeRFåœ¨ä¿®å¤åŒºåŸŸçš„ç›‘ç£ã€‚ï¼ˆ3ï¼‰å•ç›®æ·±åº¦ç›‘ç£ï¼šåˆ©ç”¨å•ç›®æ·±åº¦å…ˆéªŒå¯¹ä¿®å¤åŒºåŸŸçš„å‡ ä½•å½¢çŠ¶è¿›è¡Œæ­£åˆ™åŒ–ã€‚ï¼ˆ4ï¼‰æ€»è®­ç»ƒç›®æ ‡ï¼šè®­ç»ƒè¿­ä»£åŒ…æ‹¬é‡å»ºæ­¥éª¤ã€ä¿®å¤æ­¥éª¤å’Œåˆ¤åˆ«å™¨è®­ç»ƒæ­¥éª¤ï¼Œæ¯ä¸ªæ­¥éª¤ä¼˜åŒ–ä¸åŒçš„ç›®æ ‡ã€‚ï¼ˆ5ï¼‰è¿­ä»£æ•°æ®æ›´æ–°å’Œå™ªå£°è°ƒåº¦ï¼šé‡‡ç”¨è¿­ä»£æ•°æ®æ›´æ–°å’Œéƒ¨åˆ†DDIMä¿®å¤ï¼Œä»¥å‡è½»æ‰©æ•£æ¨¡å‹çš„å¤šæ ·æ€§å’Œéšæœºæ€§ã€‚</p></li></ol><p><strong>8. ç»“è®º</strong></p><p><strong>(1): æœ¬å·¥ä½œçš„æ„ä¹‰</strong></p><p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œé€šè¿‡åœºæ™¯å®šåˆ¶å’Œæ©ç å¯¹æŠ—æ€§è®­ç»ƒæ¥è§£å†³NeRFä¿®å¤ä¸­çš„å‡ ä½•æ¨¡ç³Šå’Œçº¹ç†åç§»é—®é¢˜ã€‚è¯¥æ¡†æ¶åœ¨çœŸå®åœºæ™¯ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„NeRFä¿®å¤ç»“æœï¼Œä¸ºå›¾åƒä¿®å¤å’Œ3Dé‡å»ºæä¾›äº†æ–°çš„æ–¹æ³•ã€‚</p><p><strong>(2): åˆ›æ–°ç‚¹ã€æ€§èƒ½å’Œå·¥ä½œé‡</strong></p><ul><li><strong>åˆ›æ–°ç‚¹ï¼š</strong><ul><li>æå‡ºåœºæ™¯å®šåˆ¶æ¥ç¼“å’Œæ‰©æ•£æ¨¡å‹çš„éšæœºæ€§ã€‚</li><li>é‡‡ç”¨æ©ç å¯¹æŠ—æ€§è®­ç»ƒæ¥å‡è½»çº¹ç†åç§»ã€‚</li><li>å‘ç°åƒç´ å’Œæ„ŸçŸ¥æŸå¤±åœ¨NeRFä¿®å¤ä»»åŠ¡ä¸­æ˜¯æœ‰å®³çš„ã€‚</li></ul></li><li><strong>æ€§èƒ½ï¼š</strong><ul><li>åœ¨å„ç§çœŸå®åœºæ™¯ä¸Šäº§ç”Ÿäº†æœ€å…ˆè¿›çš„NeRFä¿®å¤ç»“æœã€‚</li><li>å®ç°äº†æ¸…æ™°ä¸”ç¡®å®šæ€§çš„å‡ ä½•å½¢çŠ¶åˆæˆã€‚</li><li>å‡è½»äº†çº¹ç†åç§»ï¼Œæé«˜äº†ä¸å›¾åƒæ¡ä»¶çš„ä¸€è‡´æ€§ã€‚</li></ul></li><li><strong>å·¥ä½œé‡ï¼š</strong><ul><li>åœºæ™¯å®šåˆ¶å’Œæ©ç å¯¹æŠ—æ€§è®­ç»ƒå¢åŠ äº†è®­ç»ƒå¤æ‚åº¦ã€‚</li><li>è¿­ä»£æ•°æ®æ›´æ–°å’Œå™ªå£°è°ƒåº¦éœ€è¦é¢å¤–çš„è®¡ç®—èµ„æºã€‚</li></ul></li></ul><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-71b2d0d350aca831aa75f321f4a4b0fc.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e53c1166741cf80b67784bf8605b441d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f0ca2bc16aea3d2352fbc4822bb93beb.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7b5effeb64b57b56ea109097322b49a0.jpg" align="middle"></details>## GPN: Generative Point-based NeRF**Authors:Haipeng Wang**Scanning real-life scenes with modern registration devices typically gives incomplete point cloud representations, primarily due to the limitations of partial scanning, 3D occlusions, and dynamic light conditions. Recent works on processing incomplete point clouds have always focused on point cloud completion. However, these approaches do not ensure consistency between the completed point cloud and the captured images regarding color and geometry. We propose using Generative Point-based NeRF (GPN) to reconstruct and repair a partial cloud by fully utilizing the scanning images and the corresponding reconstructed cloud. The repaired point cloud can achieve multi-view consistency with the captured images at high spatial resolution. For the finetunes of a single scene, we optimize the global latent condition by incorporating an Auto-Decoder architecture while retaining multi-view consistency. As a result, the generated point clouds are smooth, plausible, and geometrically consistent with the partial scanning images. Extensive experiments on ShapeNet demonstrate that our works achieve competitive performances to the other state-of-the-art point cloud-based neural scene rendering and editing performances. [PDF](http://arxiv.org/abs/2404.08312v1) **Summary**ç”Ÿæˆå¼åŸºäºç‚¹çš„ NeRF åœ¨æ‰«æå›¾åƒå’Œé‡å»ºç‚¹äº‘çš„å¼•å¯¼ä¸‹ï¼Œä¿®å¤ä¸å®Œæ•´ç‚¹äº‘ï¼Œå®ç°å¤šè§†è§’ä¸€è‡´æ€§ã€‚**Key Takeaways**- åˆ©ç”¨ç”Ÿæˆå¼ç‚¹äº‘ NeRF ä¿®å¤ä¸å®Œæ•´ç‚¹äº‘ï¼ŒåŒæ—¶ä¿è¯å‡ ä½•å’Œé¢œè‰²ä¸€è‡´æ€§ã€‚- é‡‡ç”¨è‡ªåŠ¨è§£ç å™¨æ¶æ„ä¼˜åŒ–å…¨å±€æ½œåœ¨æ¡ä»¶ï¼Œç¡®ä¿å¤šè§†è§’ä¸€è‡´æ€§ã€‚- ç”Ÿæˆç‚¹äº‘ä¸æ‰«æå›¾åƒå‡ ä½•ä¸€è‡´ã€å…‰æ»‘ä¸”åˆç†ã€‚- åœ¨ ShapeNet ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç¥ç»åœºæ™¯æ¸²æŸ“å’Œç¼–è¾‘æ–¹é¢å…·æœ‰ç«äº‰åŠ›ã€‚- è¯¥æ–¹æ³•è§£å†³äº†éƒ¨åˆ†æ‰«æã€3D é®æŒ¡å’ŒåŠ¨æ€å…‰ç…§æ¡ä»¶ä¸‹ç‚¹äº‘ä¸å®Œæ•´çš„é—®é¢˜ã€‚- è¯¥æ–¹æ³•ä¸“æ³¨äºç‚¹äº‘ä¿®å¤ï¼Œè€Œéç‚¹äº‘å®ŒæˆåŠŸèƒ½ã€‚- è¯¥æ–¹æ³•å……åˆ†åˆ©ç”¨äº†æ‰«æå›¾åƒå’Œé‡å»ºç‚¹äº‘çš„ä¿¡æ¯ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>æ ‡é¢˜ï¼šGPNï¼šåŸºäºç”Ÿæˆç‚¹äº‘çš„ NeRF</li><li>ä½œè€…ï¼šHaipeng Wang</li><li>å•ä½ï¼šæµ™æ±Ÿç†å·¥å¤§å­¦æœºæ¢°å·¥ç¨‹å­¦é™¢</li><li>å…³é”®è¯ï¼šç‚¹äº‘é‡å»ºã€ç‚¹äº‘ä¿®å¤ã€ç”Ÿæˆå¼ç¥ç»è¾å°„åœºã€å¤šè§†å›¾ä¸€è‡´æ€§</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.08312</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šåœ¨ç°å®åœºæ™¯ä¸­ï¼Œç”±äºéƒ¨åˆ†æ‰«æã€é®æŒ¡å’ŒåŠ¨æ€å…‰ç…§æ¡ä»¶çš„é™åˆ¶ï¼Œä½¿ç”¨ç°ä»£æ³¨å†Œè®¾å¤‡æ‰«æå¾—åˆ°çš„ç‚¹äº‘é€šå¸¸æ˜¯ä¸å®Œæ•´çš„ã€‚</li></ol><p>ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠé—®é¢˜ï¼šè¿‡å»çš„æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨ç‚¹äº‘è¡¥å…¨ä¸Šï¼Œä½†è¿™äº›æ–¹æ³•ä¸èƒ½ä¿è¯è¡¥å…¨åçš„ç‚¹äº‘ä¸æ•è·çš„å›¾åƒåœ¨é¢œè‰²å’Œå‡ ä½•ä¸Šçš„ä¸€è‡´æ€§ã€‚</p><p>ï¼ˆ3ï¼‰æå‡ºçš„æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç”Ÿæˆç‚¹äº‘çš„ NeRFï¼ˆGPNï¼‰æ¡†æ¶ï¼Œé€šè¿‡å……åˆ†åˆ©ç”¨æ‰«æå›¾åƒå’Œç›¸åº”çš„é‡å»ºç‚¹äº‘ï¼Œå¯¹éƒ¨åˆ†ç‚¹äº‘è¿›è¡Œé‡å»ºå’Œä¿®å¤ã€‚ä¿®å¤åçš„ç‚¹äº‘å¯ä»¥å®ç°ä¸æ•è·å›¾åƒåœ¨å¤šè§†å›¾ä¸Šçš„ä¸€è‡´æ€§ï¼Œå¹¶å…·æœ‰è¾ƒé«˜çš„ç©ºé—´åˆ†è¾¨ç‡ã€‚</p><p>ï¼ˆ4ï¼‰æ–¹æ³•çš„æ€§èƒ½åŠæ•ˆæœï¼šåœ¨ ShapeNet æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæœ¬æ–‡æ–¹æ³•åœ¨ç‚¹äº‘æ¸²æŸ“å’Œç¼–è¾‘ä»»åŠ¡ä¸Šå–å¾—äº†ä¸å…¶ä»–æœ€å…ˆè¿›æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ï¼Œå³ç”Ÿæˆä¸éƒ¨åˆ†æ‰«æå›¾åƒå‡ ä½•ä¸€è‡´çš„ã€å¹³æ»‘ä¸”åˆç†çš„ç‚¹äº‘ã€‚</p><p>Some Error for method(æ¯”å¦‚æ˜¯ä¸æ˜¯æ²¡æœ‰Methodsè¿™ä¸ªç« èŠ‚)</p><p><strong>8. ç»“è®º</strong></p><p><strong>(1): æœ¬æ–‡æ„ä¹‰</strong></p><p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç”Ÿæˆç‚¹äº‘çš„ NeRFï¼ˆGPNï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿä¿®å¤éƒ¨åˆ†ç‚¹äº‘å¹¶é‡å»ºç¼ºå¤±éƒ¨åˆ†ï¼ŒåŒæ—¶ç¡®ä¿ä¿®å¤åçš„ç‚¹äº‘ä¸æ•è·å›¾åƒåœ¨å¤šè§†å›¾ä¸Šçš„ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•ä¸ºç‚¹äº‘é‡å»ºå’Œä¿®å¤é¢†åŸŸæä¾›äº†æ–°çš„æ€è·¯ï¼Œå…·æœ‰è¾ƒé«˜çš„å®ç”¨ä»·å€¼ã€‚</p><p><strong>(2): ä¼˜ç¼ºç‚¹æ€»ç»“</strong></p><p><strong>åˆ›æ–°ç‚¹ï¼š</strong></p><ul><li>æå‡ºäº†ä¸€ç§åŸºäºç”Ÿæˆç‚¹äº‘çš„ NeRF æ¡†æ¶ï¼Œç”¨äºç‚¹äº‘ä¿®å¤å’Œé‡å»ºã€‚</li><li>é€šè¿‡å¼•å…¥å¤šè§†å›¾ä¸€è‡´æ€§çº¦æŸï¼Œç¡®ä¿ä¿®å¤åçš„ç‚¹äº‘ä¸æ•è·å›¾åƒåœ¨å‡ ä½•å’Œé¢œè‰²ä¸Šçš„ä¸€è‡´ã€‚</li></ul><p><strong>æ€§èƒ½ï¼š</strong></p><ul><li>åœ¨ ShapeNet æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç‚¹äº‘æ¸²æŸ“å’Œç¼–è¾‘ä»»åŠ¡ä¸Šå–å¾—äº†ä¸å…¶ä»–æœ€å…ˆè¿›æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ã€‚</li><li>ç”Ÿæˆçš„ç‚¹äº‘å…·æœ‰è¾ƒé«˜çš„ç©ºé—´åˆ†è¾¨ç‡å’Œå¹³æ»‘æ€§ã€‚</li></ul><p><strong>å·¥ä½œé‡ï¼š</strong></p><ul><li>è¯¥æ–¹æ³•çš„å®ç°éœ€è¦è¾ƒé«˜çš„è®¡ç®—èµ„æºå’Œæ—¶é—´æˆæœ¬ã€‚</li><li>å¯¹äºå¤æ‚åœºæ™¯ï¼Œä¿®å¤è¿‡ç¨‹å¯èƒ½è€—æ—¶è¾ƒé•¿ã€‚</li></ul><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-977026755832e69838d0636842958c12.jpg" align="middle"><img src="https://pica.zhimg.com/v2-40839a585a476aaaa262d3984922b2ea.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4e3d24ffa7fa8024bbe07bea2f5e200e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0fe6f628a3b732261e6a91523842e27c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f1a7a543764220776107e4bb9f17417e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8b8085e2655a2ad99861a7ef579e2447.jpg" align="middle"></details>## Are NeRFs ready for autonomous driving? Towards closing the   real-to-simulation gap**Authors:Carl LindstrÃ¶m, Georg Hess, Adam Lilja, Maryam Fatemi, Lars Hammarstrand, Christoffer Petersson, Lennart Svensson**Neural Radiance Fields (NeRFs) have emerged as promising tools for advancing autonomous driving (AD) research, offering scalable closed-loop simulation and data augmentation capabilities. However, to trust the results achieved in simulation, one needs to ensure that AD systems perceive real and rendered data in the same way. Although the performance of rendering methods is increasing, many scenarios will remain inherently challenging to reconstruct faithfully. To this end, we propose a novel perspective for addressing the real-to-simulated data gap. Rather than solely focusing on improving rendering fidelity, we explore simple yet effective methods to enhance perception model robustness to NeRF artifacts without compromising performance on real data. Moreover, we conduct the first large-scale investigation into the real-to-simulated data gap in an AD setting using a state-of-the-art neural rendering technique. Specifically, we evaluate object detectors and an online mapping model on real and simulated data, and study the effects of different fine-tuning strategies.Our results show notable improvements in model robustness to simulated data, even improving real-world performance in some cases. Last, we delve into the correlation between the real-to-simulated gap and image reconstruction metrics, identifying FID and LPIPS as strong indicators. See https://research.zenseact.com/publications/closing-real2sim-gap for our project page. [PDF](http://arxiv.org/abs/2403.16092v2) Accepted at Workshop on Autonomous Driving, CVPR 2024**æ‘˜è¦**é’ˆå¯¹è‡ªåŠ¨é©¾é©¶çš„NeRFæ¨¡æ‹Ÿï¼Œåœ¨ä¸å½±å“çœŸå®æ•°æ®æ€§èƒ½çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡å¢å¼ºæ„ŸçŸ¥æ¨¡å‹å¯¹NeRFä¼ªå½±çš„é²æ£’æ€§å¼¥åˆçœŸå®ç°å®å’Œæ¨¡æ‹Ÿæ•°æ®å·®å¼‚ã€‚**è¦ç‚¹*** NeRFåœ¨è‡ªåŠ¨é©¾é©¶æ¨¡æ‹Ÿå’Œæ•°æ®å¢å¼ºä¸­æ½œåŠ›å·¨å¤§ã€‚* æ¸²æŸ“æ–¹æ³•æ€§èƒ½æå‡ï¼Œä½†ä»æœ‰åœºæ™¯é‡å»ºå›°éš¾ã€‚* æå‡ºé€šè¿‡å¢å¼ºæ„ŸçŸ¥æ¨¡å‹é²æ£’æ€§æ¥è§£å†³çœŸå®ç°å®ä¸æ¨¡æ‹Ÿæ•°æ®å·®å¼‚ã€‚* å¼€å±•äº†ä½¿ç”¨æœ€æ–°ç¥ç»æ¸²æŸ“æŠ€æœ¯åœ¨è‡ªåŠ¨é©¾é©¶èƒŒæ™¯ä¸‹çš„çœŸå®ç°å®ä¸æ¨¡æ‹Ÿæ•°æ®å·®å¼‚å¤§è§„æ¨¡ç ”ç©¶ã€‚* è¯„ä¼°äº†çœŸå®å’Œæ¨¡æ‹Ÿæ•°æ®ä¸Šçš„ç›®æ ‡æ£€æµ‹å™¨å’Œåœ¨çº¿å»ºå›¾æ¨¡å‹ã€‚* ç ”ç©¶äº†ä¸åŒå¾®è°ƒç­–ç•¥çš„å½±å“ã€‚* æ¨¡å‹å¯¹æ¨¡æ‹Ÿæ•°æ®çš„é²æ£’æ€§æ˜¾è‘—æé«˜ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹æå‡äº†çœŸå®ä¸–ç•Œæ€§èƒ½ã€‚* æ¢ç´¢äº†çœŸå®ç°å®ä¸æ¨¡æ‹Ÿæ•°æ®å·®å¼‚å’Œå›¾åƒé‡å»ºåº¦é‡ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œç¡®å®šFIDå’ŒLPIPSæ˜¯å¼ºæœ‰åŠ›çš„æŒ‡æ ‡ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>é¢˜ç›®ï¼šNeRFsèƒ½å¦ç”¨äºè‡ªåŠ¨é©¾é©¶ï¼Ÿç¼©å°çœŸå®ä¸æ¨¡æ‹Ÿçš„å·®è·</li><li>ä½œè€…ï¼šCarl LindstrÃ¶mã€Georg Hessã€Adam Liljaã€Maryam Fatemiã€Lars Hammarstrandã€Christoffer Peterssonã€Lennart Svensson</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šZenseact</li><li>å…³é”®è¯ï¼šNeRFsã€è‡ªåŠ¨é©¾é©¶ã€æ„ŸçŸ¥æ¨¡å‹ã€çœŸå®ä¸æ¨¡æ‹Ÿå·®è·</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.16092</li><li><p>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šNeRFsåœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸå±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œå¯ç”¨äºé—­ç¯ä»¿çœŸå’Œæ•°æ®å¢å¼ºã€‚ç„¶è€Œï¼Œè¦ä¿¡ä»»ä»¿çœŸç»“æœï¼Œéœ€è¦ç¡®ä¿è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿèƒ½å¤Ÿä»¥ç›¸åŒçš„æ–¹å¼æ„ŸçŸ¥çœŸå®å’Œæ¸²æŸ“æ•°æ®ã€‚(2) è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šè™½ç„¶æ¸²æŸ“æ–¹æ³•çš„æ€§èƒ½ä¸æ–­æé«˜ï¼Œä½†è®¸å¤šåœºæ™¯å¯¹äºçœŸå®é‡å»ºä»ç„¶å…·æœ‰å›ºæœ‰æŒ‘æˆ˜æ€§ã€‚è¿‡å»çš„æ–¹æ³•ä¸»è¦ä¸“æ³¨äºæé«˜æ¸²æŸ“ä¿çœŸåº¦ï¼Œè€Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è§†è§’ï¼Œé€šè¿‡å¢å¼ºæ„ŸçŸ¥æ¨¡å‹å¯¹NeRFä¼ªå½±çš„é²æ£’æ€§æ¥è§£å†³çœŸå®ä¸æ¨¡æ‹Ÿæ•°æ®å·®è·é—®é¢˜ï¼Œè€Œä¸ä¼šæŸå®³çœŸå®æ•°æ®ä¸Šçš„æ€§èƒ½ã€‚(3) ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡é¦–æ¬¡ä½¿ç”¨æœ€å…ˆè¿›çš„ç¥ç»æ¸²æŸ“æŠ€æœ¯å¯¹è‡ªåŠ¨é©¾é©¶åœºæ™¯ä¸­çš„çœŸå®ä¸æ¨¡æ‹Ÿæ•°æ®å·®è·è¿›è¡Œäº†å¤§è§„æ¨¡è°ƒæŸ¥ã€‚å…·ä½“æ¥è¯´ï¼Œä½œè€…è¯„ä¼°äº†ç‰©ä½“æ£€æµ‹å™¨å’Œåœ¨çº¿å»ºå›¾æ¨¡å‹åœ¨çœŸå®å’Œæ¨¡æ‹Ÿæ•°æ®ä¸Šçš„æ€§èƒ½ï¼Œå¹¶ç ”ç©¶äº†ä¸åŒå¾®è°ƒç­–ç•¥çš„å½±å“ã€‚(4) æ€§èƒ½å’Œæ„ä¹‰ï¼šç»“æœè¡¨æ˜ï¼Œæ¨¡å‹å¯¹æ¨¡æ‹Ÿæ•°æ®çš„é²æ£’æ€§æœ‰äº†æ˜¾ç€æé«˜ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ç”šè‡³æé«˜äº†çœŸå®ä¸–ç•Œçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œä½œè€…æ·±å…¥ç ”ç©¶äº†çœŸå®ä¸æ¨¡æ‹Ÿå·®è·ä¸å›¾åƒé‡å»ºæŒ‡æ ‡ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œå‘ç°FIDå’ŒLPIPSæ˜¯å¼ºæœ‰åŠ›çš„æŒ‡æ ‡ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) å›¾åƒå¢å¼ºï¼šä½¿ç”¨å›¾åƒå¢å¼ºæ–¹æ³•æ¥æé«˜æ„ŸçŸ¥æ¨¡å‹å¯¹æ¸²æŸ“æ•°æ®ä¼ªå½±çš„é²æ£’æ€§ã€‚(2) æ··åˆæ¸²æŸ“å›¾åƒè¿›è¡Œå¾®è°ƒï¼šåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­åŠ å…¥æ¸²æŸ“æ•°æ®ï¼Œä»¥é€‚åº”æ„ŸçŸ¥æ¨¡å‹åˆ° NeRF æ¸²æŸ“æ•°æ®ã€‚(3) å›¾åƒåˆ°å›¾åƒè½¬æ¢ï¼šä½¿ç”¨å›¾åƒåˆ°å›¾åƒè½¬æ¢æ–¹æ³•ç”Ÿæˆç±»ä¼¼ NeRF çš„å›¾åƒï¼Œä»¥å¢åŠ  NeRF ç±»ä¼¼å›¾åƒçš„æ•°é‡ï¼Œç”¨äºå¾®è°ƒã€‚</p></li></ol><p><strong>æ‘˜è¦</strong></p><p><strong>ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯</strong></p><p>NeRFs åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸå±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œå¯ç”¨äºé—­ç¯ä»¿çœŸå’Œæ•°æ®å¢å¼ºã€‚ç„¶è€Œï¼Œè¦ä¿¡ä»»ä»¿çœŸç»“æœï¼Œéœ€è¦ç¡®ä¿è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿèƒ½å¤Ÿä»¥ç›¸åŒçš„æ–¹å¼æ„ŸçŸ¥çœŸå®å’Œæ¸²æŸ“æ•°æ®ã€‚</p><p><strong>ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜</strong></p><p>è™½ç„¶æ¸²æŸ“æ–¹æ³•çš„æ€§èƒ½ä¸æ–­æé«˜ï¼Œä½†è®¸å¤šåœºæ™¯å¯¹äºçœŸå®é‡å»ºä»ç„¶å…·æœ‰å›ºæœ‰æŒ‘æˆ˜æ€§ã€‚è¿‡å»çš„æ–¹æ³•ä¸»è¦ä¸“æ³¨äºæé«˜æ¸²æŸ“ä¿çœŸåº¦ï¼Œè€Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è§†è§’ï¼Œé€šè¿‡å¢å¼ºæ„ŸçŸ¥æ¨¡å‹å¯¹ NeRF ä¼ªå½±çš„é²æ£’æ€§æ¥è§£å†³çœŸå®ä¸æ¨¡æ‹Ÿæ•°æ®å·®è·é—®é¢˜ï¼Œè€Œä¸ä¼šæŸå®³çœŸå®æ•°æ®ä¸Šçš„æ€§èƒ½ã€‚</p><p><strong>ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•</strong></p><p>æœ¬æ–‡é¦–æ¬¡ä½¿ç”¨æœ€å…ˆè¿›çš„ç¥ç»æ¸²æŸ“æŠ€æœ¯å¯¹è‡ªåŠ¨é©¾é©¶åœºæ™¯ä¸­çš„çœŸå®ä¸æ¨¡æ‹Ÿæ•°æ®å·®è·è¿›è¡Œäº†å¤§è§„æ¨¡è°ƒæŸ¥ã€‚å…·ä½“æ¥è¯´ï¼Œä½œè€…è¯„ä¼°äº†ç‰©ä½“æ£€æµ‹å™¨å’Œåœ¨çº¿å»ºå›¾æ¨¡å‹åœ¨çœŸå®å’Œæ¨¡æ‹Ÿæ•°æ®ä¸Šçš„æ€§èƒ½ï¼Œå¹¶ç ”ç©¶äº†ä¸åŒå¾®è°ƒç­–ç•¥çš„å½±å“ã€‚</p><p><strong>ï¼ˆ4ï¼‰æ€§èƒ½å’Œæ„ä¹‰</strong></p><p>ç»“æœè¡¨æ˜ï¼Œæ¨¡å‹å¯¹æ¨¡æ‹Ÿæ•°æ®çš„é²æ£’æ€§æœ‰äº†æ˜¾ç€æé«˜ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ç”šè‡³æé«˜äº†çœŸå®ä¸–ç•Œçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œä½œè€…æ·±å…¥ç ”ç©¶äº†çœŸå®ä¸æ¨¡æ‹Ÿå·®è·ä¸å›¾åƒé‡å»ºæŒ‡æ ‡ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œå‘ç° FID å’Œ LPIPS æ˜¯å¼ºæœ‰åŠ›çš„æŒ‡æ ‡ã€‚</p><p><strong>æ–¹æ³•æ‘˜è¦</strong></p><p><strong>ï¼ˆ5ï¼‰æ–¹æ³•</strong></p><p>ï¼ˆ1ï¼‰å›¾åƒå¢å¼ºï¼šä½¿ç”¨å›¾åƒå¢å¼ºæ–¹æ³•æ¥æé«˜æ„ŸçŸ¥æ¨¡å‹å¯¹æ¸²æŸ“æ•°æ®ä¼ªå½±çš„é²æ£’æ€§ã€‚ï¼ˆ2ï¼‰æ··åˆæ¸²æŸ“å›¾åƒè¿›è¡Œå¾®è°ƒï¼šåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­åŠ å…¥æ¸²æŸ“æ•°æ®ï¼Œä»¥é€‚åº”æ„ŸçŸ¥æ¨¡å‹åˆ° NeRF æ¸²æŸ“æ•°æ®ã€‚ï¼ˆ3ï¼‰å›¾åƒåˆ°å›¾åƒè½¬æ¢ï¼šä½¿ç”¨å›¾åƒåˆ°å›¾åƒè½¬æ¢æ–¹æ³•ç”Ÿæˆç±»ä¼¼ NeRF çš„å›¾åƒï¼Œä»¥å¢åŠ  NeRF ç±»ä¼¼å›¾åƒçš„æ•°é‡ï¼Œç”¨äºå¾®è°ƒã€‚</p><p><strong>ç»“è®º</strong></p><p><strong>ï¼ˆ6ï¼‰ç»“è®º</strong></p><p>ç¥ç»æ¸²æŸ“å·²æˆä¸ºæ¨¡æ‹Ÿè‡ªåŠ¨é©¾é©¶ (AD) æ•°æ®çš„ä¸€ç§æœ‰å‰æ™¯çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œä¸ºäº†åœ¨å®è·µä¸­å®ç”¨ï¼Œäººä»¬å¿…é¡»äº†è§£ AD ç³»ç»Ÿåœ¨æ¨¡æ‹Ÿæ•°æ®ä¸Šçš„è¡Œä¸ºå¦‚ä½•è½¬ç§»åˆ°çœŸå®æ•°æ®ä¸Šã€‚æˆ‘ä»¬çš„<strong>å¤§è§„æ¨¡è°ƒæŸ¥æ­ç¤ºäº†æ„ŸçŸ¥æ¨¡å‹åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®å›¾åƒä¸­æš´éœ²çš„æ€§èƒ½å·®è·</strong>ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ç­–ç•¥æ¥ç¼©å°å·®è·ï¼šå¢åŠ æ„ŸçŸ¥æ¨¡å‹å¯¹ NeRF æ¨¡æ‹Ÿæ•°æ®çš„é²æ£’æ€§ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œä½¿ç”¨ NeRF æˆ–ç±»ä¼¼ NeRF çš„æ•°æ®è¿›è¡Œå¾®è°ƒ<strong>æ˜¾è‘—ç¼©å°äº†ç‰©ä½“æ£€æµ‹å’Œåœ¨çº¿å»ºå›¾æ–¹æ³•çš„çœŸå®åˆ°æ¨¡æ‹Ÿå·®è·</strong>ï¼Œè€Œå¯¹çœŸå®æ•°æ®çš„æ€§èƒ½å‡ ä¹æ²¡æœ‰ä¸‹é™ã€‚æ­¤å¤–ï¼Œå¯¹äºåœ¨çº¿å»ºå›¾ï¼Œæˆ‘ä»¬è¡¨æ˜æœ‰é’ˆå¯¹æ€§åœ°ç”Ÿæˆæ–°åœºæ™¯å¯ä»¥æé«˜çœŸå®æ•°æ®çš„æ€§èƒ½ã€‚å°½ç®¡å¦‚æ­¤ï¼Œå½“æ”¹å˜è‡ªæˆ‘è½¦è¾†å§¿æ€æ—¶ï¼Œæ¸²æŸ“è´¨é‡ä¼šè¿…é€Ÿä¸‹é™ã€‚é‰´äºæˆ‘ä»¬çš„å‘ç°ï¼Œå³ä½æ„ŸçŸ¥è´¨é‡ï¼ˆå³ LPIPS å’Œ FID åˆ†æ•°ï¼‰ä¸è¾ƒå¤§çš„çœŸå®åˆ°æ¨¡æ‹Ÿå·®è·å¯†åˆ‡ç›¸å…³ï¼Œæˆ‘ä»¬è®¤ä¸ºåœ¨æ¨æ–­è®¾ç½®ä¸­æé«˜æ¸²æŸ“è´¨é‡ä»ç„¶æ˜¯ä½¿ NeRF èƒ½å¤Ÿç”¨äºæµ‹è¯•å’Œæ”¹è¿› AD ç³»ç»Ÿçš„å…³é”®æŒ‘æˆ˜ã€‚</p><p><strong>è‡´è°¢</strong></p><p>æˆ‘ä»¬æ„Ÿè°¢ Adam Tonderski å’Œ William Ljungbergh æä¾›å®è´µçš„è®¨è®ºã€‚è¿™é¡¹å·¥ä½œéƒ¨åˆ†ç”± Knut å’Œ Alice Wallenberg åŸºé‡‘ä¼šèµ„åŠ©çš„ Wallenberg äººå·¥æ™ºèƒ½ã€è‡ªä¸»ç³»ç»Ÿå’Œè½¯ä»¶è®¡åˆ’ (WASP) èµ„åŠ©ã€‚è®¡ç®—èµ„æºç”± NAISS åœ¨ NSC Berzelius æä¾›ï¼Œéƒ¨åˆ†ç”±ç‘å…¸ç ”ç©¶å§”å‘˜ä¼šèµ„åŠ©ï¼Œåè®®å·ã€‚2022-06725ã€‚</p><p><strong>ï¼ˆ7ï¼‰æ€»ç»“</strong></p><p>ï¼ˆ1ï¼‰<strong>æœ¬é¡¹å·¥ä½œçš„æ„ä¹‰</strong>ï¼šæå‡ºäº†ä¸€ç§æ–°é¢–çš„è§†è§’æ¥è§£å†³çœŸå®ä¸æ¨¡æ‹Ÿæ•°æ®å·®è·é—®é¢˜ï¼Œé€šè¿‡å¢å¼ºæ„ŸçŸ¥æ¨¡å‹å¯¹ NeRF ä¼ªå½±çš„é²æ£’æ€§ï¼Œè€Œä¸ä¼šæŸå®³çœŸå®æ•°æ®ä¸Šçš„æ€§èƒ½ã€‚</p><p>ï¼ˆ2ï¼‰<strong>æœ¬æ–‡çš„ä¼˜ç¼ºç‚¹</strong>ï¼š* <strong>åˆ›æ–°ç‚¹</strong>ï¼šé¦–æ¬¡ä½¿ç”¨æœ€å…ˆè¿›çš„ç¥ç»æ¸²æŸ“æŠ€æœ¯å¯¹è‡ªåŠ¨é©¾é©¶åœºæ™¯ä¸­çš„çœŸå®ä¸æ¨¡æ‹Ÿæ•°æ®å·®è·è¿›è¡Œäº†å¤§è§„æ¨¡è°ƒæŸ¥ã€‚* <strong>æ€§èƒ½</strong>ï¼šæå‡ºçš„æ–¹æ³•æ˜¾ç€æé«˜äº†æ„ŸçŸ¥æ¨¡å‹å¯¹æ¨¡æ‹Ÿæ•°æ®çš„é²æ£’æ€§ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ç”šè‡³æé«˜äº†çœŸå®ä¸–ç•Œçš„æ€§èƒ½ã€‚* <strong>å·¥ä½œé‡</strong>ï¼šéœ€è¦å¤§é‡çš„æ¸²æŸ“æ•°æ®å’Œè®­ç»ƒæ—¶é—´æ¥å®ç°æ„ŸçŸ¥æ¨¡å‹çš„é²æ£’æ€§ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-e8445490e4eaaeba826ce93fa44739ab.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-226e40089f23e26b7537bc25c8c4012b.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d40bf7f142a8199e369826096b0b0904.jpg" align="middle"><img src="https://pica.zhimg.com/v2-44b007ade1b910cc4a89084343b2e13c.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-04-17  Plug-and-Play Acceleration of Occupancy Grid-based NeRF Rendering using   VDB Grid and Hierarchical Ray Traversal</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/04/17/Paper/2024-04-17/3DGS/"/>
    <id>https://kedreamix.github.io/2024/04/17/Paper/2024-04-17/3DGS/</id>
    <published>2024-04-17T10:58:41.000Z</published>
    <updated>2024-04-17T10:58:41.335Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-04-17-æ›´æ–°"><a href="#2024-04-17-æ›´æ–°" class="headerlink" title="2024-04-17 æ›´æ–°"></a>2024-04-17 æ›´æ–°</h1><h2 id="Gaussian-Opacity-Fields-Efficient-and-Compact-Surface-Reconstruction-in-Unbounded-Scenes"><a href="#Gaussian-Opacity-Fields-Efficient-and-Compact-Surface-Reconstruction-in-Unbounded-Scenes" class="headerlink" title="Gaussian Opacity Fields: Efficient and Compact Surface Reconstruction in   Unbounded Scenes"></a>Gaussian Opacity Fields: Efficient and Compact Surface Reconstruction in   Unbounded Scenes</h2><p><strong>Authors:Zehao Yu, Torsten Sattler, Andreas Geiger</strong></p><p>Recently, 3D Gaussian Splatting (3DGS) has demonstrated impressive novel view synthesis results, while allowing the rendering of high-resolution images in real-time. However, leveraging 3D Gaussians for surface reconstruction poses significant challenges due to the explicit and disconnected nature of 3D Gaussians. In this work, we present Gaussian Opacity Fields (GOF), a novel approach for efficient, high-quality, and compact surface reconstruction in unbounded scenes. Our GOF is derived from ray-tracing-based volume rendering of 3D Gaussians, enabling direct geometry extraction from 3D Gaussians by identifying its levelset, without resorting to Poisson reconstruction or TSDF fusion as in previous work. We approximate the surface normal of Gaussians as the normal of the ray-Gaussian intersection plane, enabling the application of regularization that significantly enhances geometry. Furthermore, we develop an efficient geometry extraction method utilizing marching tetrahedra, where the tetrahedral grids are induced from 3D Gaussians and thus adapt to the sceneâ€™s complexity. Our evaluations reveal that GOF surpasses existing 3DGS-based methods in surface reconstruction and novel view synthesis. Further, it compares favorably to, or even outperforms, neural implicit methods in both quality and speed. </p><p><a href="http://arxiv.org/abs/2404.10772v1">PDF</a> Project page:   <a href="https://niujinshuchong.github.io/gaussian-opacity-fields">https://niujinshuchong.github.io/gaussian-opacity-fields</a></p><p><strong>æ‘˜è¦</strong><br>ä¸‰ç»´é«˜æ–¯æ–‘ç‚¹èåˆ (3DGS) å°†ä¸‰ç»´é«˜æ–¯ä½“æ¸²æŸ“ä¸ºä½“ç´ ï¼Œç›´æ¥æå–è¡¨é¢å‡ ä½•ä¿¡æ¯ï¼Œé«˜æ•ˆä¸”ç´§å‡‘åœ°é‡å»ºä»»æ„åœºæ™¯ä¸‹çš„ç‰©ä½“è¡¨é¢ã€‚</p><p><strong>å…³é”®è¦ç‚¹</strong></p><ul><li>æå‡ºé«˜æ–¯ä¸é€æ˜åº¦åœº (GOF)ï¼Œé€šè¿‡å°„çº¿è¿½è¸ªä½“ç»˜åˆ¶ä¸‰ç»´é«˜æ–¯ä½“è·å¾—ï¼Œç›´æ¥ä»ä¸‰ç»´é«˜æ–¯ä½“ä¸­æå–å‡ ä½•ä¿¡æ¯ã€‚</li><li>å°†é«˜æ–¯ä½“è¡¨é¢æ³•å‘é‡è¿‘ä¼¼ä¸ºå°„çº¿-é«˜æ–¯ä½“ç›¸äº¤å¹³é¢çš„æ³•å‘é‡ï¼Œå¹¶åº”ç”¨æ­£åˆ™åŒ–ä»¥æ˜¾è‘—å¢å¼ºå‡ ä½•å½¢çŠ¶ã€‚</li><li>å¼€å‘äº†ä¸€ç§åˆ©ç”¨è¡Œè¿›å››é¢ä½“çš„æœ‰æ•ˆå‡ ä½•æå–æ–¹æ³•ï¼Œå…¶ä¸­å››é¢ä½“ç½‘æ ¼ç”±ä¸‰ç»´é«˜æ–¯ä½“è¯±å¯¼ï¼Œå¹¶é€‚åº”åœºæ™¯çš„å¤æ‚ç¨‹åº¦ã€‚</li><li>åœ¨è¡¨é¢é‡å»ºå’Œæ–°è§†å›¾åˆæˆä¸­ï¼ŒGOF ä¼˜äºç°æœ‰çš„åŸºäº 3DGS çš„æ–¹æ³•ã€‚</li><li>åœ¨è´¨é‡å’Œé€Ÿåº¦æ–¹é¢ï¼ŒGOF ä¸ç¥ç»éšå¼æ–¹æ³•ç›¸å½“ï¼Œç”šè‡³ä¼˜äºç¥ç»éšå¼æ–¹æ³•ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šé«˜æ–¯ä¸é€æ˜åº¦åœºï¼šæ— ç•Œåœºæ™¯ä¸­çš„é«˜æ•ˆç´§å‡‘è¡¨é¢é‡å»º</li><li>ä½œè€…ï¼šZehao Yu, Torsten Sattler, Andreas Geiger</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šå¾·å›½å›¾å®¾æ ¹å¤§å­¦å›¾å®¾æ ¹äººå·¥æ™ºèƒ½ä¸­å¿ƒ</li><li>å…³é”®è¯ï¼šæ–°é¢–è§†å›¾åˆæˆã€å¯å¾®æ¸²æŸ“ã€é«˜æ–¯æº…å°„ã€è¡¨é¢é‡å»ºã€å¤šè§†å›¾è½¬ 3D</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://niujinshuchong.github.io/gaussian-opacity-fields</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šè¿‘å¹´æ¥ï¼Œç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åœ¨æ–°å‹è§†å›¾åˆæˆå’Œè¡¨é¢é‡å»ºæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨é‡å»ºæ— ç•Œåœºæ™¯ä¸­å‰æ™¯å¯¹è±¡æ—¶å­˜åœ¨å±€é™æ€§ï¼Œå¹¶ä¸”è®¡ç®—æˆæœ¬é«˜ã€‚ï¼ˆ2ï¼‰ï¼šè¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šä¼ ç»Ÿæ–¹æ³•é€šå¸¸é‡‡ç”¨ä½“ç´ èåˆæˆ–æ³Šæ¾é‡å»ºæ¥ä» NeRF çš„ä¸é€æ˜åº¦åœºä¸­æå–è¡¨é¢ã€‚è¿™äº›æ–¹æ³•å­˜åœ¨å™ªå£°ã€ä¸å®Œæ•´å’Œè®¡ç®—æˆæœ¬é«˜çš„ç¼ºç‚¹ã€‚ï¼ˆ3ï¼‰ï¼šæå‡ºçš„æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç§°ä¸ºé«˜æ–¯ä¸é€æ˜åº¦åœºï¼ˆGOFï¼‰çš„æ–°é¢–æ–¹æ³•ã€‚GOF é€šè¿‡å¯¹ 3D é«˜æ–¯è¿›è¡ŒåŸºäºå…‰çº¿è¿½è¸ªçš„ä½“ç§¯æ¸²æŸ“æ¥è·å¾—ï¼Œå®ƒä½¿æˆ‘ä»¬èƒ½å¤Ÿç›´æ¥è¯†åˆ« 3D é«˜æ–¯çš„æ°´å¹³é›†ï¼Œä»è€Œä» 3D é«˜æ–¯ä¸­æå–å‡ ä½•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åˆ©ç”¨è¡Œè¿›å››é¢ä½“çš„æœ‰æ•ˆå‡ ä½•æå–æ–¹æ³•ï¼Œå…¶ä¸­å››é¢ä½“ç½‘æ ¼æ˜¯ä» 3D é«˜æ–¯ä¸­æ„Ÿåº”å‡ºæ¥çš„ï¼Œå¹¶å› æ­¤é€‚åº”åœºæ™¯çš„å¤æ‚æ€§ã€‚ï¼ˆ4ï¼‰ï¼šæ–¹æ³•çš„æ€§èƒ½ï¼šæˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼ŒGOF åœ¨è¡¨é¢é‡å»ºå’Œæ–°é¢–è§†å›¾åˆæˆæ–¹é¢ä¼˜äºç°æœ‰çš„åŸºäº 3DGS çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨è´¨é‡å’Œé€Ÿåº¦ä¸Šéƒ½ä¸ç¥ç»éšå¼æ–¹æ³•ç›¸å½“ç”šè‡³ä¼˜äºç¥ç»éšå¼æ–¹æ³•ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) æ„å»ºé«˜æ–¯ä¸é€æ˜åº¦åœº (GOF)ï¼Œé€šè¿‡å¯¹ 3D é«˜æ–¯è¿›è¡ŒåŸºäºå…‰çº¿è¿½è¸ªçš„ä½“ç§¯æ¸²æŸ“è·å¾—ï¼›(2) æ‰©å±• 2D é«˜æ–¯ä¸­çš„ä¸¤ä¸ªæœ‰æ•ˆæ­£åˆ™åŒ–é¡¹åˆ° 3D é«˜æ–¯ï¼Œæå‡é‡å»ºè´¨é‡ï¼›(3) æå‡ºä¸€ç§åŸºäºè¡Œè¿›å››é¢ä½“çš„æœ‰æ•ˆå‡ ä½•æå–æ–¹æ³•ï¼Œä» GOF ä¸­æå–è¯¦ç»†ä¸”ç´§å‡‘çš„ç½‘æ ¼ã€‚</p></li><li><p>ç»“è®ºï¼š(1): æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§é«˜æ–¯ä¸é€æ˜åº¦åœº (GOF)ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºåœ¨æ— ç•Œåœºæ™¯ä¸­è¿›è¡Œé«˜æ•ˆã€é«˜è´¨é‡ä¸”ç´§å‡‘çš„è¡¨é¢é‡å»ºçš„æ–°é¢–æ–¹æ³•ã€‚æˆ‘ä»¬çš„ GOF æ˜¯é€šè¿‡å¯¹ 3D é«˜æ–¯è¿›è¡ŒåŸºäºå…‰çº¿è¿½è¸ªçš„ä½“ç§¯æ¸²æŸ“è·å¾—çš„ï¼Œä¸ RGB æ¸²æŸ“ä¿æŒä¸€è‡´ã€‚æˆ‘ä»¬çš„ GOF èƒ½å¤Ÿç›´æ¥ä» 3D é«˜æ–¯ä¸­æå–å‡ ä½•ï¼Œé€šè¿‡è¯†åˆ«å…¶æ°´å¹³é›†ï¼Œè€Œæ— éœ€æ³Šæ¾é‡å»ºæˆ– TSDFã€‚æˆ‘ä»¬è¿‘ä¼¼é«˜æ–¯çš„æ›²é¢æ³•çº¿ä¸ºå°„çº¿-é«˜æ–¯ç›¸äº¤å¹³é¢çš„æ³•çº¿ï¼Œå¹¶åº”ç”¨æ·±åº¦-æ³•çº¿ä¸€è‡´æ€§æ­£åˆ™åŒ–æ¥å¢å¼ºå‡ ä½•é‡å»ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨è¡Œè¿›å››é¢ä½“çš„æœ‰æ•ˆä¸”ç´§å‡‘çš„ç½‘æ ¼æå–æ–¹æ³•ï¼Œå…¶ä¸­å››é¢ä½“ç½‘æ ¼æ˜¯ä» 3D é«˜æ–¯ä¸­æ„Ÿåº”å‡ºæ¥çš„ã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼ŒGOF åœ¨æ— ç•Œåœºæ™¯ä¸­çš„è¡¨é¢é‡å»ºå’Œæ–°é¢–è§†å›¾åˆæˆæ–¹é¢éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚(2): åˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäº 3D é«˜æ–¯çš„é«˜æ–¯ä¸é€æ˜åº¦åœº (GOF) æ¥è¿›è¡Œè¡¨é¢é‡å»ºï¼›å¼€å‘äº†ä¸€ç§åŸºäºè¡Œè¿›å››é¢ä½“çš„æœ‰æ•ˆå‡ ä½•æå–æ–¹æ³•ï¼Œå¯ä»¥ä» GOF ä¸­æå–è¯¦ç»†ä¸”ç´§å‡‘çš„ç½‘æ ¼ã€‚æ€§èƒ½ï¼šåœ¨è¡¨é¢é‡å»ºå’Œæ–°é¢–è§†å›¾åˆæˆæ–¹é¢ä¼˜äºç°æœ‰çš„åŸºäº 3D é«˜æ–¯çš„æ–¹æ³•ï¼›åœ¨è´¨é‡å’Œé€Ÿåº¦ä¸Šéƒ½ä¸ç¥ç»éšå¼æ–¹æ³•ç›¸å½“ç”šè‡³ä¼˜äºç¥ç»éšå¼æ–¹æ³•ã€‚å·¥ä½œé‡ï¼šéœ€è¦è¿›è¡ŒåŸºäºå…‰çº¿è¿½è¸ªçš„ä½“ç§¯æ¸²æŸ“å’Œè¡Œè¿›å››é¢ä½“çš„å‡ ä½•æå–ï¼Œè®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-7120d48e211e632332c006e60959fa7f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-5a10b3207b26758f6049e10e774c09a3.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e998868d938693cc86772478ebad84a5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-818d65bf288671197aa1a4f35098147c.jpg" align="middle"></details><h2 id="AbsGS-Recovering-Fine-Details-for-3D-Gaussian-Splatting"><a href="#AbsGS-Recovering-Fine-Details-for-3D-Gaussian-Splatting" class="headerlink" title="AbsGS: Recovering Fine Details for 3D Gaussian Splatting"></a>AbsGS: Recovering Fine Details for 3D Gaussian Splatting</h2><p><strong>Authors:Zongxin Ye, Wenyu Li, Sidun Liu, Peng Qiao, Yong Dou</strong></p><p>3D Gaussian Splatting (3D-GS) technique couples 3D Gaussian primitives with differentiable rasterization to achieve high-quality novel view synthesis results while providing advanced real-time rendering performance. However, due to the flaw of its adaptive density control strategy in 3D-GS, it frequently suffers from over-reconstruction issue in intricate scenes containing high-frequency details, leading to blurry rendered images. The underlying reason for the flaw has still been under-explored. In this work, we present a comprehensive analysis of the cause of aforementioned artifacts, namely gradient collision, which prevents large Gaussians in over-reconstructed regions from splitting. To address this issue, we propose the novel homodirectional view-space positional gradient as the criterion for densification. Our strategy efficiently identifies large Gaussians in over-reconstructed regions, and recovers fine details by splitting. We evaluate our proposed method on various challenging datasets. The experimental results indicate that our approach achieves the best rendering quality with reduced or similar memory consumption. Our method is easy to implement and can be incorporated into a wide variety of most recent Gaussian Splatting-based methods. We will open source our codes upon formal publication. Our project page is available at: <a href="https://ty424.github.io/AbsGS.github.io/">https://ty424.github.io/AbsGS.github.io/</a> </p><p><a href="http://arxiv.org/abs/2404.10484v1">PDF</a> </p><p><strong>Summary</strong><br> ä¸‰ç»´é«˜æ–¯æº…å°„ï¼ˆ3D-GSï¼‰æŠ€æœ¯å°†ä¸‰ç»´é«˜æ–¯åŸè¯­ä¸å¯å¾®æ …æ ¼åŒ–ç›¸ç»“åˆï¼Œåœ¨æä¾›å…ˆè¿›å®æ—¶æ¸²æŸ“æ€§èƒ½çš„åŒæ—¶å®ç°é«˜è´¨é‡æ–°è§†å›¾åˆæˆç»“æœã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>3D-GSä¸­è‡ªé€‚åº”å¯†åº¦æ§åˆ¶ç­–ç•¥å­˜åœ¨ç¼ºé™·ï¼Œå¯¼è‡´åœ¨åŒ…å«é«˜é¢‘ç»†èŠ‚çš„å¤æ‚åœºæ™¯ä¸­ç»å¸¸å‡ºç°è¿‡åº¦é‡å»ºé—®é¢˜ï¼Œä»è€Œå¯¼è‡´æ¸²æŸ“å›¾åƒæ¨¡ç³Šã€‚</li><li>è¿‡åº¦é‡å»ºåŒºåŸŸä¸­è¿‡å¤§çš„é«˜æ–¯ä½“æ— æ³•åˆ†è£‚ï¼ŒåŸå› æ˜¯æ¢¯åº¦ç¢°æ’é˜»æ­¢äº†å®ƒä»¬çš„åˆ†è£‚ã€‚</li><li>æå‡ºäº†ä¸€ç§æ–°çš„åŒå‘è§†å›¾ç©ºé—´ä½ç½®æ¢¯åº¦ï¼Œä½œä¸ºè‡´å¯†åŒ–çš„æ ‡å‡†ï¼Œè§£å†³äº†è¿™ä¸€é—®é¢˜ã€‚</li><li>è¯¥ç­–ç•¥æœ‰æ•ˆåœ°è¯†åˆ«å‡ºè¿‡åº¦é‡å»ºåŒºåŸŸä¸­çš„å¤§é«˜æ–¯ä½“ï¼Œå¹¶é€šè¿‡åˆ†è£‚æ¢å¤ç²¾ç»†ç»†èŠ‚ã€‚</li><li>è¯¥æ–¹æ³•åœ¨å„ç§å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•å®ç°äº†æœ€ä½³æ¸²æŸ“è´¨é‡ï¼ŒåŒæ—¶å‡å°‘äº†å†…å­˜æ¶ˆè€—ã€‚</li><li>è¯¥æ–¹æ³•æ˜“äºå®ç°ï¼Œå¯ä»¥æ•´åˆåˆ°å¤šç§æœ€æ–°çš„åŸºäºé«˜æ–¯æº…å°„çš„æ–¹æ³•ä¸­ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šåŸºäºæ¢¯åº¦ç¢°æ’çš„ 3D é«˜æ–¯æ³¼æº…ä¸­çš„è¿‡é‡å»ºé—®é¢˜åˆ†æä¸è§£å†³ï¼ˆ3D é«˜æ–¯æ³¼æº…ä¸­çš„è¿‡é‡å»ºé—®é¢˜åˆ†æä¸è§£å†³ï¼‰</li><li>ä½œè€…ï¼šTianye Li<em>, Yuxuan Zhang</em></li><li>å•ä½ï¼šæ— </li><li>å…³é”®è¯ï¼šNovel View Synthesis, 3D Gaussian Splatting, Point-based Radiance Field, 3D reconstruction</li><li>è®ºæ–‡é“¾æ¥ï¼šarXiv:2404.10484v1[cs.CV]   Github é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼š3D é«˜æ–¯æ³¼æº…æŠ€æœ¯å°† 3D é«˜æ–¯åŸºå…ƒä¸å¯å¾®å…‰æ …åŒ–ç›¸ç»“åˆï¼Œä»¥å®ç°é«˜è´¨é‡çš„æ–°è§†è§’åˆæˆç»“æœï¼ŒåŒæ—¶æä¾›å…ˆè¿›çš„å®æ—¶æ¸²æŸ“æ€§èƒ½ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼š3D é«˜æ–¯æ³¼æº…ä¸­è‡ªé€‚åº”å¯†åº¦æ§åˆ¶ç­–ç•¥çš„ç¼ºé™·å¯¼è‡´å…¶åœ¨åŒ…å«é«˜é¢‘ç»†èŠ‚çš„å¤æ‚åœºæ™¯ä¸­ç»å¸¸å‡ºç°è¿‡é‡å»ºé—®é¢˜ï¼Œä»è€Œå¯¼è‡´æ¸²æŸ“å›¾åƒæ¨¡ç³Šã€‚è¯¥ç¼ºé™·çš„æ ¹æœ¬åŸå› å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ï¼ˆ3ï¼‰æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæå‡ºäº†ä¸€ç§æ–°é¢–çš„åŒå‘è§†å›¾ç©ºé—´ä½ç½®æ¢¯åº¦ä½œä¸ºè‡´å¯†åŒ–çš„æ ‡å‡†ï¼Œå¯¹ä¸Šè¿°ä¼ªå½±çš„åŸå› ï¼ˆå³æ¢¯åº¦ç¢°æ’ï¼‰è¿›è¡Œäº†å…¨é¢åˆ†æã€‚è¯¥ç­–ç•¥æœ‰æ•ˆåœ°è¯†åˆ«å‡ºè¿‡é‡å»ºåŒºåŸŸä¸­çš„å¤§é«˜æ–¯ä½“ï¼Œå¹¶é€šè¿‡åˆ†è£‚æ¢å¤ç²¾ç»†ç»†èŠ‚ã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä½•ç§ä»»åŠ¡ä¸Šå–å¾—äº†æ€æ ·çš„æ€§èƒ½ï¼šåœ¨å„ç§å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šè¯„ä¼°äº†æ‰€æå‡ºçš„æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä»¥å‡å°‘æˆ–ç±»ä¼¼çš„å†…å­˜æ¶ˆè€—å®ç°äº†æœ€ä½³æ¸²æŸ“è´¨é‡ã€‚è¯¥æ–¹æ³•æ˜“äºå®ç°ï¼Œå¯ä»¥æ•´åˆåˆ°å„ç§æœ€æ–°çš„åŸºäºé«˜æ–¯æ³¼æº…çš„æ–¹æ³•ä¸­ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰åˆ†æäº† 3D é«˜æ–¯æ³¼æº…ä¸­è¿‡é‡å»ºé—®é¢˜çš„æˆå› ï¼Œå³æ¢¯åº¦ç¢°æ’ï¼Œè¯¥ç°è±¡é˜»æ­¢äº†è¿‡é‡å»ºåŒºåŸŸä¸­å¤§é«˜æ–¯ä½“åˆ†è£‚ï¼›ï¼ˆ2ï¼‰æå‡ºäº†åŒå‘è§†å›¾ç©ºé—´ä½ç½®æ¢¯åº¦ä½œä¸ºè‡´å¯†åŒ–æ ‡å‡†ï¼Œæœ‰æ•ˆè¯†åˆ«å‡ºè¿‡é‡å»ºåŒºåŸŸä¸­çš„å¤§é«˜æ–¯ä½“ï¼›ï¼ˆ3ï¼‰é€šè¿‡åˆ†è£‚æ“ä½œæ¢å¤ç²¾ç»†ç»†èŠ‚ï¼Œæ”¹å–„æ¸²æŸ“è´¨é‡ã€‚</p></li><li><p>ç»“è®ºï¼š(1)ï¼šæœ¬æ–‡æ·±å…¥ç ”ç©¶äº† 3D é«˜æ–¯æ³¼æº…ä¸­çš„è¿‡é‡å»ºé—®é¢˜ï¼Œå¹¶æå‡ºäº†åŸºäºæ¢¯åº¦ç¢°æ’çš„åˆ†æä¸è§£å†³æ–¹æ³•ï¼Œæœ‰æ•ˆæå‡äº†æ¸²æŸ“è´¨é‡ã€‚(2)ï¼šåˆ›æ–°ç‚¹ï¼šæœ¬æ–‡ä»æ¢¯åº¦ç¢°æ’çš„è§’åº¦åˆ†æäº†è¿‡é‡å»ºé—®é¢˜çš„æˆå› ï¼Œå¹¶æå‡ºäº†åŒå‘è§†å›¾ç©ºé—´ä½ç½®æ¢¯åº¦ä½œä¸ºè‡´å¯†åŒ–æ ‡å‡†ï¼Œæœ‰æ•ˆè¯†åˆ«å‡ºè¿‡é‡å»ºåŒºåŸŸä¸­çš„å¤§é«˜æ–¯ä½“ï¼Œé€šè¿‡åˆ†è£‚æ“ä½œæ¢å¤ç²¾ç»†ç»†èŠ‚ï¼Œæ”¹å–„æ¸²æŸ“è´¨é‡ã€‚æ€§èƒ½ï¼šå®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä»¥å‡å°‘æˆ–ç±»ä¼¼çš„å†…å­˜æ¶ˆè€—å®ç°äº†æœ€ä½³æ¸²æŸ“è´¨é‡ã€‚å·¥ä½œé‡ï¼šè¯¥æ–¹æ³•æ˜“äºå®ç°ï¼Œå¯ä»¥æ•´åˆåˆ°å„ç§æœ€æ–°çš„åŸºäºé«˜æ–¯æ³¼æº…çš„æ–¹æ³•ä¸­ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-b3954ae2d030deb08a1858901e173aeb.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7ed623f29e65c33839f7af3ba662cf77.jpg" align="middle"><img src="https://picx.zhimg.com/v2-32e2b412c01557b89b9f9fab52d1386e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a224297269ae60107729a25ab0846851.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b860f19dbacac5f895a9fea794424f48.jpg" align="middle"></details><h2 id="SRGS-Super-Resolution-3D-Gaussian-Splatting"><a href="#SRGS-Super-Resolution-3D-Gaussian-Splatting" class="headerlink" title="SRGS: Super-Resolution 3D Gaussian Splatting"></a>SRGS: Super-Resolution 3D Gaussian Splatting</h2><p><strong>Authors:Xiang Feng, Yongbo He, Yubo Wang, Yan Yang, Zhenzhong Kuang, Yu Jun, Jianping Fan, Jiajun ding</strong></p><p>Recently, 3D Gaussian Splatting (3DGS) has gained popularity as a novel explicit 3D representation. This approach relies on the representation power of Gaussian primitives to provide a high-quality rendering. However, primitives optimized at low resolution inevitably exhibit sparsity and texture deficiency, posing a challenge for achieving high-resolution novel view synthesis (HRNVS). To address this problem, we propose Super-Resolution 3D Gaussian Splatting (SRGS) to perform the optimization in a high-resolution (HR) space. The sub-pixel constraint is introduced for the increased viewpoints in HR space, exploiting the sub-pixel cross-view information of the multiple low-resolution (LR) views. The gradient accumulated from more viewpoints will facilitate the densification of primitives. Furthermore, a pre-trained 2D super-resolution model is integrated with the sub-pixel constraint, enabling these dense primitives to learn faithful texture features. In general, our method focuses on densification and texture learning to effectively enhance the representation ability of primitives. Experimentally, our method achieves high rendering quality on HRNVS only with LR inputs, outperforming state-of-the-art methods on challenging datasets such as Mip-NeRF 360 and Tanks &amp; Temples. Related codes will be released upon acceptance. </p><p><a href="http://arxiv.org/abs/2404.10318v1">PDF</a> submit ACM MM 2024</p><p><strong>Summary</strong><br>SRGS åœ¨é«˜åˆ†è¾¨ç‡ç©ºé—´è¿›è¡Œä¼˜åŒ–ï¼Œå¼•å…¥äºšåƒç´ çº¦æŸåˆ©ç”¨äºšåƒç´ äº¤å‰è§†å›¾ä¿¡æ¯ï¼Œå¹¶ç»“åˆé¢„è®­ç»ƒçš„ 2D è¶…åˆ†è¾¨ç‡æ¨¡å‹æ¥æé«˜ä¸‰ç»´é«˜æ–¯å…‰æ …åŸå§‹ä½“çš„è¡¨ç¤ºèƒ½åŠ›ï¼Œç”¨äºè§£å†³é«˜åˆ†è¾¨ç‡æ–°è§†è§’åˆæˆé—®é¢˜ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>3DGS åœ¨ä½åˆ†è¾¨ç‡ä¸­ä¼˜åŒ–å¯¼è‡´åŸå§‹ä½“çš„ç¨€ç–æ€§å’Œçº¹ç†ç¼ºé™·ï¼Œé˜»ç¢é«˜åˆ†è¾¨ç‡æ–°è§†è§’åˆæˆã€‚</li><li>SRGS åœ¨é«˜åˆ†è¾¨ç‡ç©ºé—´è¿›è¡Œä¼˜åŒ–ï¼Œæé«˜äº†åŸå§‹ä½“çš„å¯†åº¦ã€‚</li><li>å¼•å…¥äºšåƒç´ çº¦æŸæ¥åˆ©ç”¨å¤šä¸ªä½åˆ†è¾¨ç‡è§†å›¾çš„äºšåƒç´ äº¤å‰è§†å›¾ä¿¡æ¯ã€‚</li><li>ç´¯ç§¯æ›´å¤šè§†ç‚¹çš„æ¢¯åº¦æœ‰åˆ©äºåŸå§‹ä½“çš„å¯†é›†åŒ–ã€‚</li><li>é›†æˆé¢„è®­ç»ƒçš„ 2D è¶…åˆ†è¾¨ç‡æ¨¡å‹ï¼Œä½¿å¯†é›†çš„åŸå§‹ä½“èƒ½å¤Ÿå­¦ä¹ å¯é çš„çº¹ç†ç‰¹å¾ã€‚</li><li>SRGS ä¸“æ³¨äºè‡´å¯†åŒ–å’Œçº¹ç†å­¦ä¹ ï¼Œæœ‰æ•ˆå¢å¼ºäº†åŸå§‹ä½“çš„è¡¨ç¤ºèƒ½åŠ›ã€‚</li><li>SRGS åœ¨ä»…æœ‰ä½åˆ†è¾¨ç‡è¾“å…¥çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†é«˜åˆ†è¾¨ç‡æ–°è§†è§’åˆæˆçš„å‡ºè‰²æ¸²æŸ“è´¨é‡ã€‚</li><li>SRGS åœ¨ Mip-NeRF 360 å’Œå¦å…‹ä¸å¯ºåº™ç­‰æŒ‘æˆ˜æ€§æ•°æ®é›†ä¸Šä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šSRGSï¼šè¶…åˆ†è¾¨ç‡ 3D é«˜æ–¯æ³¼æº…</li><li>ä½œè€…ï¼šXiang Fengï¼ŒYongbo Heï¼ŒYubo Wangï¼ŒYan Yangï¼ŒZhenzhong Kuangï¼ŒJun Yuï¼ŒJianping Fanï¼ŒJiajun Ding</li><li>å•ä½ï¼šæ­å·ç”µå­ç§‘æŠ€å¤§å­¦</li><li>å…³é”®è¯ï¼š3D é«˜æ–¯æ³¼æº…ï¼Œè¶…åˆ†è¾¨ç‡ï¼Œæ–°è§†è§’åˆæˆ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/pdf/2404.10318.pdfï¼ŒGithub é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼š3D é«˜æ–¯æ³¼æº… (3DGS) æ˜¯ä¸€ç§æ–°é¢–çš„æ˜¾å¼ 3D è¡¨ç¤ºï¼Œä¾èµ–äºé«˜æ–¯åŸºå…ƒçš„è¡¨ç¤ºèƒ½åŠ›æ¥æä¾›é«˜è´¨é‡çš„æ¸²æŸ“ã€‚ç„¶è€Œï¼Œåœ¨ä½åˆ†è¾¨ç‡ä¸‹ä¼˜åŒ–åçš„åŸºå…ƒä¸å¯é¿å…åœ°è¡¨ç°å‡ºç¨€ç–æ€§å’Œçº¹ç†ä¸è¶³ï¼Œå¯¹å®ç°é«˜åˆ†è¾¨ç‡æ–°è§†è§’åˆæˆ (HRNVS) æ„æˆæŒ‘æˆ˜ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼š3DGS è™½ç„¶å…·æœ‰å¤šé¡¹ä¼˜ç‚¹ï¼Œä½†åœ¨ä»…ä½¿ç”¨ä½åˆ†è¾¨ç‡è¾“å…¥æ‰§è¡Œ HRNVS æ—¶ï¼Œæ¸²æŸ“è´¨é‡ä¼šæ€¥å‰§ä¸‹é™ã€‚è¿™æ˜¯å› ä¸º 3DGS ä¸­é«˜æ–¯åŸºå…ƒçš„è¡¨ç¤ºèƒ½åŠ›å¯¹äºå®ç°é«˜è´¨é‡çš„è§†å›¾åˆæˆè‡³å…³é‡è¦ã€‚å…·ä½“æ¥è¯´ï¼Œé«˜åˆ†è¾¨ç‡æ¸²æŸ“éœ€è¦å…·æœ‰ç»†ç²’åº¦çº¹ç†ç‰¹å¾çš„æ›´å¯†é›†çš„é«˜æ–¯åŸºå…ƒã€‚ç„¶è€Œï¼Œå¯¹äºä½åˆ†è¾¨ç‡åœºæ™¯ä¼˜åŒ–çš„åŸºå…ƒä¸å¯é¿å…åœ°åˆ†å¸ƒç¨€ç–ï¼Œå¯¼è‡´æ¸²æŸ“çš„ HR è§†å›¾ä¸­å‡ºç°ä¼ªå½±ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„ä½åˆ†è¾¨ç‡è§†å›¾ç¼ºä¹å¿…è¦çš„ HR çº¹ç†ã€‚å› æ­¤ï¼Œ3D ç©ºé—´ä¸­çš„åŸºå…ƒä¸å¯èƒ½åœ¨æ²¡æœ‰ HR çº¹ç†çš„åæŠ•å½±çš„æƒ…å†µä¸‹å­¦ä¹ ç›¸åº”çš„ç‰¹å¾ã€‚ï¼ˆ3ï¼‰æœ¬æ–‡æ–¹æ³•ï¼šä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæœ¬æ–‡æå‡ºäº†è¶…åˆ†è¾¨ç‡ 3D é«˜æ–¯æ³¼æº… (SRGS)ï¼Œå®ƒæ‰©å±•äº† 3DGS ä»¥å®ç°é«˜è´¨é‡çš„ HRNVSã€‚æ‰€æå‡ºçš„æ–¹æ³•åŒ…æ‹¬ä¸¤éƒ¨åˆ†ï¼Œå³è¶…åˆ†è¾¨ç‡é«˜æ–¯è‡´å¯†åŒ–å’Œçº¹ç†å¼•å¯¼é«˜æ–¯å­¦ä¹ ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šåœ¨ HRNVS ä»»åŠ¡ä¸Šï¼Œä¸ä»…ä½¿ç”¨ä½åˆ†è¾¨ç‡è¾“å…¥çš„æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼Œæœ¬æ–‡æ–¹æ³•åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ï¼ˆå¦‚ Mip-NeRF360 å’Œ Tanks &amp; Templesï¼‰ä¸Šå®ç°äº†æ›´é«˜çš„æ¸²æŸ“è´¨é‡ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ï¼Œå³åœ¨ä»…ä½¿ç”¨ä½åˆ†è¾¨ç‡è¾“å…¥çš„æƒ…å†µä¸‹å®ç°é«˜è´¨é‡çš„ HRNVSã€‚</li></ol><p>7.æ–¹æ³•ï¼šï¼ˆ1ï¼‰è¶…åˆ†è¾¨ç‡é«˜æ–¯è‡´å¯†åŒ–ï¼›ï¼ˆ2ï¼‰çº¹ç†å¼•å¯¼é«˜æ–¯å­¦ä¹ ã€‚</p><ol><li>ç»“è®ºï¼š(1): æœ¬å·¥ä½œé€šè¿‡ä»…ä½¿ç”¨ä½åˆ†è¾¨ç‡è¾“å…¥å®ç°é«˜è´¨é‡çš„æ–°è§†è§’åˆæˆï¼Œåœ¨è¯¥é¢†åŸŸåšå‡ºäº†é¦–æ¬¡å°è¯•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨è¶…åˆ†è¾¨ç‡é«˜æ–¯è‡´å¯†åŒ–ç­–ç•¥æ¥å¢åŠ é«˜æ–¯åŸºå…ƒçš„å¯†åº¦ï¼Œä»è€Œèƒ½å¤Ÿè¡¨ç¤ºç»†ç²’åº¦çš„é«˜åˆ†è¾¨ç‡ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§çº¹ç†å¼•å¯¼é«˜æ–¯å­¦ä¹ ç­–ç•¥ï¼Œè¯¥ç­–ç•¥æŒ‡å¯¼é«˜æ–¯åŸºå…ƒä»å¤–éƒ¨ 2D è¶…åˆ†è¾¨ç‡æ¨¡å‹çš„å…ˆéªŒä¸­å­¦ä¹ çœŸå®çš„çº¹ç†ã€‚åœ¨ä¸‰ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒSRGS æœ‰æ•ˆåœ°å¢å¼ºäº†é«˜æ–¯åŸºå…ƒçš„è¡¨ç¤ºèƒ½åŠ›ï¼Œæ¥è¿‘ä½¿ç”¨é«˜åˆ†è¾¨ç‡è§†å›¾è®­ç»ƒçš„ 3DGS çš„æ¸²æŸ“æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„ SRGS ä¸»è¦å— 2D è¶…åˆ†è¾¨ç‡æ¨¡å‹çš„é™åˆ¶ã€‚åœ¨æˆ‘ä»¬çš„æœªæ¥å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°†è¿›ä¸€æ­¥æ¢ç´¢æ²¡æœ‰ 2D è¶…åˆ†è¾¨ç‡æ¨¡å‹çš„æ–°è§†è§’åˆæˆæ–¹æ³•ã€‚(2): åˆ›æ–°ç‚¹ï¼šSRGSï¼›æ€§èƒ½ï¼šSRGS åœ¨ä»…ä½¿ç”¨ä½åˆ†è¾¨ç‡è¾“å…¥çš„æƒ…å†µä¸‹å®ç°äº†é«˜è´¨é‡çš„æ–°è§†è§’åˆæˆï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šä¼˜äºæœ€æ–°æ–¹æ³•ï¼›å·¥ä½œé‡ï¼šä¸­ç­‰ï¼Œéœ€è¦é¢å¤–çš„ 2D è¶…åˆ†è¾¨ç‡æ¨¡å‹ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-8e51f459b7f920ab07ae498fd133cdfa.jpg" align="middle"><img src="https://picx.zhimg.com/v2-73911338ed13dfd8725fff2143317b2d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-09dc3fb075728d00cdf50313743df98f.jpg" align="middle"><img src="https://pica.zhimg.com/v2-8712c281513c2e8cc3bc68f15b35bb68.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-8f9ab4e24b8b4895c13b3db9ace1733c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3ee8ae605e49ec5d4b5081d13f242fab.jpg" align="middle"></details><h2 id="LetsGo-Large-Scale-Garage-Modeling-and-Rendering-via-LiDAR-Assisted-Gaussian-Primitives"><a href="#LetsGo-Large-Scale-Garage-Modeling-and-Rendering-via-LiDAR-Assisted-Gaussian-Primitives" class="headerlink" title="LetsGo: Large-Scale Garage Modeling and Rendering via LiDAR-Assisted   Gaussian Primitives"></a>LetsGo: Large-Scale Garage Modeling and Rendering via LiDAR-Assisted   Gaussian Primitives</h2><p><strong>Authors:Jiadi Cui, Junming Cao, Yuhui Zhong, Liao Wang, Fuqiang Zhao, Penghao Wang, Yifan Chen, Zhipeng He, Lan Xu, Yujiao Shi, Yingliang Zhang, Jingyi Yu</strong></p><p>Large garages are ubiquitous yet intricate scenes in our daily lives, posing challenges characterized by monotonous colors, repetitive patterns, reflective surfaces, and transparent vehicle glass. Conventional Structure from Motion (SfM) methods for camera pose estimation and 3D reconstruction fail in these environments due to poor correspondence construction. To address these challenges, this paper introduces LetsGo, a LiDAR-assisted Gaussian splatting approach for large-scale garage modeling and rendering. We develop a handheld scanner, Polar, equipped with IMU, LiDAR, and a fisheye camera, to facilitate accurate LiDAR and image data scanning. With this Polar device, we present a GarageWorld dataset consisting of five expansive garage scenes with diverse geometric structures and will release the dataset to the community for further research. We demonstrate that the collected LiDAR point cloud by the Polar device enhances a suite of 3D Gaussian splatting algorithms for garage scene modeling and rendering. We also propose a novel depth regularizer for 3D Gaussian splatting algorithm training, effectively eliminating floating artifacts in rendered images, and a lightweight Level of Detail (LOD) Gaussian renderer for real-time viewing on web-based devices. Additionally, we explore a hybrid representation that combines the advantages of traditional mesh in depicting simple geometry and colors (e.g., walls and the ground) with modern 3D Gaussian representations capturing complex details and high-frequency textures. This strategy achieves an optimal balance between memory performance and rendering quality. Experimental results on our dataset, along with ScanNet++ and KITTI-360, demonstrate the superiority of our method in rendering quality and resource efficiency. </p><p><a href="http://arxiv.org/abs/2404.09748v1">PDF</a> Project Page: <a href="https://jdtsui.github.io/letsgo/">https://jdtsui.github.io/letsgo/</a></p><p><strong>Summary</strong><br>æ¿€å…‰è¾…åŠ©é«˜æ–¯çƒé¢æŠ•å½±æ³•ï¼Œå¯é«˜æ•ˆå»ºæ¨¡å¤§è§„æ¨¡å¤æ‚å®¤å†…åœºæ™¯ï¼ˆä¾‹å¦‚è½¦åº“ï¼‰ï¼Œå¹¶åœ¨ç½‘é¡µç«¯å®ç°å®æ—¶æ¸²æŸ“ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>å¼€å‘äº†é€‚ç”¨äºå¤§è§„æ¨¡è½¦åº“å»ºæ¨¡å’Œæ¸²æŸ“çš„æ¿€å…‰è¾…åŠ©é«˜æ–¯çƒé¢æŠ•å½±æ–¹æ³• LetsGoã€‚</li><li>è®¾è®¡äº†ä¸€ç§é›† IMUã€æ¿€å…‰é›·è¾¾å’Œé±¼çœ¼ç›¸æœºäºä¸€ä½“çš„æ‰‹æŒæ‰«æä»ª Polarã€‚</li><li>æ¨å‡ºäº†åŒ…å«äº”ç§å¤§è§„æ¨¡è½¦åº“åœºæ™¯çš„ GarageWorld æ•°æ®é›†ã€‚</li><li>æå‡ºäº†ä¸€ç§æ–°çš„æ·±åº¦æ­£åˆ™åŒ–å™¨ï¼Œå¯æœ‰æ•ˆæ¶ˆé™¤æ¸²æŸ“å›¾åƒä¸­çš„æµ®åŠ¨ä¼ªå½±ã€‚</li><li>æå‡ºäº†ä¸€ç§è½»é‡çº§çš„ç»†èŠ‚çº§åˆ«ï¼ˆLODï¼‰é«˜æ–¯æ¸²æŸ“å™¨ï¼Œå¯å®ç°åŸºäºç½‘ç»œè®¾å¤‡çš„å®æ—¶æŸ¥çœ‹ã€‚</li><li>æ¢ç´¢äº†ä¸€ç§æ··åˆè¡¨ç¤ºï¼Œç»“åˆäº†ä¼ ç»Ÿç½‘æ ¼ï¼ˆæè¿°ç®€å•å‡ ä½•å½¢çŠ¶å’Œé¢œè‰²ï¼‰å’Œç°ä»£ 3D é«˜æ–¯è¡¨ç¤ºï¼ˆæ•æ‰å¤æ‚ç»†èŠ‚å’Œé«˜é¢‘çº¹ç†ï¼‰çš„ä¼˜åŠ¿ã€‚</li><li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ¸²æŸ“è´¨é‡å’Œèµ„æºæ•ˆç‡æ–¹é¢å‡ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šLetsGoï¼šåŸºäºæ¿€å…‰é›·è¾¾è¾…åŠ©çš„é«˜æ–¯ä½“ç»˜åˆ¶çš„å¤§è§„æ¨¡è½¦åº“å»ºæ¨¡ä¸æ¸²æŸ“</li><li>ä½œè€…ï¼šJiadi Cui, Junming Cao, Yuhui Zhong, Liao Wang, Fuqiang Zhao, Penghao Wang, Yifan Chen, Zhipeng He, Lan Xu, Yujiao Shi, Yingliang Zhang, Jingyi Yu</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šä¸Šæµ·ç§‘æŠ€å¤§å­¦</li><li>å…³é”®è¯ï¼šè½¦åº“å»ºæ¨¡ã€æ¿€å…‰é›·è¾¾ã€é«˜æ–¯ä½“ç»˜åˆ¶</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.09748   Github ä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šå¤§è§„æ¨¡è½¦åº“æ˜¯æ—¥å¸¸ç”Ÿæ´»ä¸­çš„æ™®éå­˜åœ¨ï¼Œä½†ç”±äºå•è°ƒçš„é¢œè‰²ã€é‡å¤çš„å›¾æ¡ˆã€åå…‰è¡¨é¢å’Œé€æ˜çš„è½¦è¾†ç»ç’ƒï¼Œç»™å»ºæ¨¡å’Œæ¸²æŸ“å¸¦æ¥äº†æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„åŸºäº Structure from Motionï¼ˆSfMï¼‰çš„ç›¸æœºä½å§¿ä¼°è®¡å’Œä¸‰ç»´é‡å»ºæ–¹æ³•åœ¨è¿™äº›ç¯å¢ƒä¸­ä¼šå¤±è´¥ï¼Œå› ä¸ºéš¾ä»¥å»ºç«‹è‰¯å¥½çš„å¯¹åº”å…³ç³»ã€‚   ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ä¸é—®é¢˜ï¼šåŸºäº SfM çš„æ–¹æ³•åœ¨è½¦åº“ç¯å¢ƒä¸­æ•ˆæœä¸ä½³ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ¿€å…‰é›·è¾¾è¾…åŠ©çš„é«˜æ–¯ä½“ç»˜åˆ¶æ–¹æ³•æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚   ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç§°ä¸º LetsGo çš„æ–¹æ³•ï¼Œå®ƒä½¿ç”¨æ¿€å…‰é›·è¾¾è¾…åŠ©çš„é«˜æ–¯ä½“ç»˜åˆ¶æ¥è¿›è¡Œå¤§è§„æ¨¡è½¦åº“å»ºæ¨¡å’Œæ¸²æŸ“ã€‚è¯¥æ–¹æ³•åŒ…æ‹¬ï¼š</li><li>å¼€å‘äº†ä¸€ç§é…å¤‡ IMUã€æ¿€å…‰é›·è¾¾å’Œé±¼çœ¼ç›¸æœºçš„ä¾¿æºå¼æ‰«æä»ª Polarï¼Œç”¨äºé‡‡é›†ç²¾ç¡®çš„æ¿€å…‰é›·è¾¾å’Œå›¾åƒæ•°æ®ã€‚</li><li>æå‡ºäº†ä¸€ç§æ–°çš„æ·±åº¦æ­£åˆ™åŒ–å™¨ï¼Œç”¨äºè®­ç»ƒä¸‰ç»´é«˜æ–¯ä½“ç»˜åˆ¶ç®—æ³•ï¼Œæœ‰æ•ˆåœ°æ¶ˆé™¤äº†æ¸²æŸ“å›¾åƒä¸­çš„æ‚¬æµ®ä¼ªå½±ã€‚</li><li>è®¾è®¡äº†ä¸€ç§è½»é‡çº§çš„ç»†èŠ‚å±‚æ¬¡ï¼ˆLODï¼‰é«˜æ–¯æ¸²æŸ“å™¨ï¼Œç”¨äºåœ¨åŸºäº Web çš„è®¾å¤‡ä¸Šå®æ—¶æŸ¥çœ‹ã€‚</li><li>æ¢ç´¢äº†ä¸€ç§æ··åˆè¡¨ç¤ºï¼Œå®ƒç»“åˆäº†ä¼ ç»Ÿç½‘æ ¼ï¼ˆç”¨äºæç»˜ç®€å•çš„å‡ ä½•å½¢çŠ¶å’Œé¢œè‰²ï¼‰å’Œç°ä»£ä¸‰ç»´é«˜æ–¯è¡¨ç¤ºï¼ˆç”¨äºæ•æ‰å¤æ‚ç»†èŠ‚å’Œé«˜é¢‘çº¹ç†ï¼‰çš„ä¼˜ç‚¹ã€‚   ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šåœ¨è½¦åº“åœºæ™¯å»ºæ¨¡å’Œæ¸²æŸ“ä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•å–å¾—äº†è‰¯å¥½çš„æ€§èƒ½ï¼Œæœ‰æ•ˆåœ°æ¶ˆé™¤äº†æ‚¬æµ®ä¼ªå½±ï¼Œå¹¶å®ç°äº†å®æ—¶æŸ¥çœ‹ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ï¼Œå³åœ¨å¤§è§„æ¨¡è½¦åº“åœºæ™¯ä¸­å®ç°é«˜ä¿çœŸæ¸²æŸ“ã€‚</li></ol><p>7.æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼šä½¿ç”¨é…å¤‡IMUã€æ¿€å…‰é›·è¾¾å’Œé±¼çœ¼ç›¸æœºçš„ä¾¿æºå¼æ‰«æä»ªPolaré‡‡é›†ç²¾ç¡®çš„æ¿€å…‰é›·è¾¾å’Œå›¾åƒæ•°æ®ï¼›ï¼ˆ2ï¼‰ï¼šæå‡ºæ–°çš„æ·±åº¦æ­£åˆ™åŒ–å™¨ï¼Œè®­ç»ƒä¸‰ç»´é«˜æ–¯ä½“ç»˜åˆ¶ç®—æ³•ï¼Œæ¶ˆé™¤æ¸²æŸ“å›¾åƒä¸­çš„æ‚¬æµ®ä¼ªå½±ï¼›ï¼ˆ3ï¼‰ï¼šè®¾è®¡è½»é‡çº§çš„ç»†èŠ‚å±‚æ¬¡ï¼ˆLODï¼‰é«˜æ–¯æ¸²æŸ“å™¨ï¼Œç”¨äºåœ¨åŸºäºWebçš„è®¾å¤‡ä¸Šå®æ—¶æŸ¥çœ‹ï¼›ï¼ˆ4ï¼‰ï¼šæ¢ç´¢ç»“åˆä¼ ç»Ÿç½‘æ ¼å’Œç°ä»£ä¸‰ç»´é«˜æ–¯è¡¨ç¤ºçš„æ··åˆè¡¨ç¤ºï¼Œå…¼é¡¾ç®€å•å‡ ä½•å½¢çŠ¶å’Œå¤æ‚ç»†èŠ‚çš„æç»˜ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œè´¡çŒ®äº†ä¸€æ¬¾ç”¨äºæ•°æ®é‡‡é›†çš„æ‰‹æŒè®¾å¤‡Polarï¼Œä¸€ä¸ªè½¦åº“ä¸–ç•Œæ•°æ®é›†ï¼Œç”¨äºåœºæ™¯è¡¨ç¤ºçš„æ¿€å…‰é›·è¾¾è¾…åŠ©çš„é«˜æ–¯ä½“ï¼Œä»¥åŠä¸€ç§è½»é‡çº§çš„æ¸²æŸ“æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯å…è®¸åœ¨æ¶ˆè´¹è€…çº§è®¾å¤‡ä¸Šè¿›è¡ŒåŸºäºWebçš„æ¸²æŸ“ã€‚å¾—ç›Šäºè¿™äº›åˆ›æ–°ï¼Œæˆ‘ä»¬æˆåŠŸåœ°é‡å»ºäº†å„ç§å…·æœ‰å¤šæ ·åŒ–å’Œå…·æœ‰æŒ‘æˆ˜æ€§ç¯å¢ƒçš„è½¦åº“ï¼Œå…è®¸ä»ä»»ä½•è§†ç‚¹è¿›è¡Œå®æ—¶è½»é‡çº§æ¸²æŸ“ã€‚åœ¨æ”¶é›†çš„å’Œä¸¤ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„è½¦åº“ä¸–ç•Œä»¥åŠé‡å»ºçš„3Dæ¨¡å‹å’Œå®æ—¶æ¸²æŸ“ï¼Œæ”¯æŒä¸€ç³»åˆ—åº”ç”¨ç¨‹åºï¼ŒåŒ…æ‹¬è®­ç»ƒæ•°æ®ç”Ÿæˆå’Œè‡ªåŠ¨é©¾é©¶ç®—æ³•çš„æµ‹è¯•å¹³å°ã€è‡ªåŠ¨è½¦è¾†å®šä½ã€å¯¼èˆªå’Œåœè½¦çš„å®æ—¶è¾…åŠ©ï¼Œä»¥åŠVFXåˆ¶ä½œã€‚æˆ‘ä»¬ç›®å‰çš„å·¥ä½œä¸»è¦é›†ä¸­åœ¨å¯¹ä¸–ç•Œçš„æ„ŸçŸ¥ä¸Šï¼Œå¹¶ä¸”å®ƒæ”¯æŒä¸‹æ¸¸è¯†åˆ«ä»»åŠ¡ã€‚æœªæ¥ï¼Œæˆ‘ä»¬è¿˜å°†æ¢ç´¢è½¦åº“ç”Ÿæˆï¼Œä¸æ–­æ¨åŠ¨è½¦åº“å»ºæ¨¡çš„è¾¹ç•Œï¼Œå¹¶å®ç°ä»æ„ŸçŸ¥ã€è¯†åˆ«åˆ°ç”Ÿæˆã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šxxxï¼›æ€§èƒ½ï¼šxxxï¼›å·¥ä½œé‡ï¼šxxxï¼›</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-2a3cd69515b02d46d8a8eb1653b52018.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a59b13ea73e0cb9d1f84e56d6ffa6262.jpg" align="middle"><img src="https://pica.zhimg.com/v2-6a7de06ef015705d30f275980dba4bac.jpg" align="middle"></details><h2 id="CompGS-Efficient-3D-Scene-Representation-via-Compressed-Gaussian-Splatting"><a href="#CompGS-Efficient-3D-Scene-Representation-via-Compressed-Gaussian-Splatting" class="headerlink" title="CompGS: Efficient 3D Scene Representation via Compressed Gaussian   Splatting"></a>CompGS: Efficient 3D Scene Representation via Compressed Gaussian   Splatting</h2><p><strong>Authors:Xiangrui Liu, Xinju Wu, Pingping Zhang, Shiqi Wang, Zhu Li, Sam Kwong</strong></p><p>Gaussian splatting, renowned for its exceptional rendering quality and efficiency, has emerged as a prominent technique in 3D scene representation. However, the substantial data volume of Gaussian splatting impedes its practical utility in real-world applications. Herein, we propose an efficient 3D scene representation, named Compressed Gaussian Splatting (CompGS), which harnesses compact Gaussian primitives for faithful 3D scene modeling with a remarkably reduced data size. To ensure the compactness of Gaussian primitives, we devise a hybrid primitive structure that captures predictive relationships between each other. Then, we exploit a small set of anchor primitives for prediction, allowing the majority of primitives to be encapsulated into highly compact residual forms. Moreover, we develop a rate-constrained optimization scheme to eliminate redundancies within such hybrid primitives, steering our CompGS towards an optimal trade-off between bitrate consumption and representation efficacy. Experimental results show that the proposed CompGS significantly outperforms existing methods, achieving superior compactness in 3D scene representation without compromising model accuracy and rendering quality. Our code will be released on GitHub for further research. </p><p><a href="http://arxiv.org/abs/2404.09458v1">PDF</a> Submitted to a conference</p><p><strong>Summary</strong><br>é«˜æ–¯ç‚¹é›†é€šè¿‡ä¼˜åŒ–æ··åˆé«˜æ–¯åŸºå…ƒç»“æ„ï¼Œå®ç°é«˜ç²¾åº¦3Dåœºæ™¯è¡¨ç¤ºçš„ä½æ•°æ®å¼€é”€ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>é«˜æ–¯ç‚¹é›†å› é«˜æ¸²æŸ“è´¨é‡å’Œæ•ˆç‡åœ¨3Dåœºæ™¯è¡¨ç¤ºä¸­å—åˆ°å¹¿æ³›åº”ç”¨ã€‚</li><li>å¤§é‡æ•°æ®é˜»ç¢äº†é«˜æ–¯ç‚¹é›†åœ¨å®é™…åº”ç”¨ä¸­çš„å®ç”¨æ€§ã€‚</li><li>æå‡ºå‹ç¼©é«˜æ–¯ç‚¹é›†ï¼ˆCompGSï¼‰ï¼Œä½¿ç”¨ç´§å‡‘çš„é«˜æ–¯åŸºå…ƒè¿›è¡Œ3Dåœºæ™¯å»ºæ¨¡ã€‚</li><li>è®¾è®¡æ··åˆåŸºå…ƒç»“æ„ï¼Œæ•æ‰åŸºå…ƒé—´çš„é¢„æµ‹å…³ç³»ï¼Œä¿è¯ç´§å‡‘æ€§ã€‚</li><li>åˆ©ç”¨å°‘é‡é”šå®šåŸºå…ƒè¿›è¡Œé¢„æµ‹ï¼Œå°†å¤§å¤šæ•°åŸºå…ƒå°è£…æˆç´§å‡‘çš„æ®‹å·®å½¢å¼ã€‚</li><li>å¼€å‘é€Ÿç‡å—é™ä¼˜åŒ–æ–¹æ¡ˆï¼Œæ¶ˆé™¤æ··åˆåŸºå…ƒå†…çš„å†—ä½™ã€‚</li><li>CompGSåœ¨ä¸å½±å“æ¨¡å‹ç²¾åº¦å’Œæ¸²æŸ“è´¨é‡çš„å‰æä¸‹ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li><li>ä»£ç å°†åœ¨GitHubä¸Šå‘å¸ƒï¼Œä»¥ä¿ƒè¿›è¿›ä¸€æ­¥ç ”ç©¶ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šCompGSï¼šé€šè¿‡å‹ç¼©é«˜æ–¯å–·å°„å®ç°é«˜æ•ˆçš„ 3D åœºæ™¯è¡¨ç¤º</li><li>ä½œè€…ï¼šXiangrui Liuï¼ŒXinju Wuï¼ŒPingping Zhangï¼ŒShiqi Wangï¼ŒZhu Liï¼ŒSam Kwong</li><li>æ‰€å±æœºæ„ï¼šé¦™æ¸¯åŸå¸‚å¤§å­¦</li><li>å…³é”®è¯ï¼š3D åœºæ™¯è¡¨ç¤ºï¼Œé«˜æ–¯å–·å°„ï¼Œå‹ç¼©</li><li>è®ºæ–‡é“¾æ¥ï¼šNoneï¼ŒGithub ä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šé«˜æ–¯å–·å°„ä»¥å…¶å‡ºè‰²çš„æ¸²æŸ“è´¨é‡å’Œæ•ˆç‡è€Œé—»åï¼Œå·²æˆä¸º 3D åœºæ™¯è¡¨ç¤ºä¸­çš„é‡è¦æŠ€æœ¯ã€‚ä½†æ˜¯ï¼Œé«˜æ–¯å–·å°„å¤§é‡çš„æ•°æ®é‡é˜»ç¢äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å®ç”¨æ€§ã€‚ï¼ˆ2ï¼‰ï¼šè¿‡å»çš„æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•é€šè¿‡å‡å°‘é«˜æ–¯å–·å°„ä¸­ 3D é«˜æ–¯çš„æ•°é‡å’Œä½“ç§¯æ¥å‹ç¼©é«˜æ–¯å–·å°„ã€‚è¿™äº›æ–¹æ³•é€šå¸¸é‡‡ç”¨å¯å‘å¼å‰ªæç­–ç•¥æ¥ç§»é™¤å¯¹æ¸²æŸ“è´¨é‡è´¡çŒ®ä¸å¤§çš„ 3D é«˜æ–¯ã€‚æ­¤å¤–ï¼Œé€šå¸¸å°†çŸ¢é‡é‡åŒ–åº”ç”¨äºä¿ç•™çš„ 3D é«˜æ–¯ä»¥è¿›ä¸€æ­¥å‹ç¼©ã€‚ï¼ˆ3ï¼‰ï¼šæå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç§°ä¸ºå‹ç¼©é«˜æ–¯å–·å°„ (CompGS) çš„é«˜æ•ˆ 3D åœºæ™¯è¡¨ç¤ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ç´§å‡‘çš„é«˜æ–¯åŸºå…ƒè¿›è¡Œé€¼çœŸçš„ 3D åœºæ™¯å»ºæ¨¡ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘äº†æ•°æ®å¤§å°ã€‚ä¸ºäº†ç¡®ä¿é«˜æ–¯åŸºå…ƒçš„ç´§å‡‘æ€§ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ··åˆåŸºå…ƒç»“æ„æ¥æ•è·å®ƒä»¬ä¹‹é—´çš„é¢„æµ‹å…³ç³»ã€‚ç„¶åï¼Œæˆ‘ä»¬åˆ©ç”¨ä¸€å°ç»„é”šå®šåŸºå…ƒè¿›è¡Œé¢„æµ‹ï¼Œå…è®¸å°†å¤§å¤šæ•°åŸºå…ƒå°è£…æˆé«˜åº¦ç´§å‡‘çš„æ®‹å·®å½¢å¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªé€Ÿç‡çº¦æŸä¼˜åŒ–æ–¹æ¡ˆæ¥æ¶ˆé™¤è¿™ç§æ··åˆåŸºå…ƒä¸­çš„å†—ä½™ï¼Œä»è€Œå¼•å¯¼æˆ‘ä»¬çš„ CompGS åœ¨æ¯”ç‰¹ç‡æ¶ˆè€—å’Œè¡¨ç¤ºæ•ˆç‡ä¹‹é—´å®ç°æœ€ä½³æƒè¡¡ã€‚ï¼ˆ4ï¼‰ï¼šæ–¹æ³•æ€§èƒ½ï¼šå®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„ CompGS æ˜æ˜¾ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨ 3D åœºæ™¯è¡¨ç¤ºä¸­å®ç°äº†å“è¶Šçš„ç´§å‡‘æ€§ï¼ŒåŒæ—¶ä¸å½±å“æ¨¡å‹å‡†ç¡®æ€§å’Œæ¸²æŸ“è´¨é‡ã€‚</li></ol><p><strong>æ–¹æ³•</strong></p><p>(1): æ··åˆåŸºå…ƒç»“æ„ï¼šå»ºç«‹ä¸€ä¸ªæ··åˆåŸºå…ƒç»“æ„ï¼ŒåŒ…æ‹¬é”šå®šåŸºå…ƒå’Œè€¦åˆåŸºå…ƒã€‚é”šå®šåŸºå…ƒæä¾›å‚è€ƒåµŒå…¥å’Œå‡ ä½•å±æ€§ï¼Œè€Œè€¦åˆåŸºå…ƒä»…åŒ…å«ç´§å‡‘çš„æ®‹å·®åµŒå…¥ï¼Œä»¥å¼¥è¡¥é¢„æµ‹è¯¯å·®ã€‚</p><p>(2): è·¨åŸºå…ƒé¢„æµ‹ï¼šåˆ©ç”¨é”šå®šåŸºå…ƒçš„å‡ ä½•å’Œå¤–è§‚å±æ€§ï¼Œé€šè¿‡è·¨åŸºå…ƒé¢„æµ‹ä¸ºè€¦åˆåŸºå…ƒç”Ÿæˆå‡ ä½•å’Œå¤–è§‚å±æ€§ã€‚</p><p>(3): é€Ÿç‡çº¦æŸä¼˜åŒ–ï¼šå»ºç«‹é€Ÿç‡çº¦æŸä¼˜åŒ–æ–¹æ¡ˆï¼Œé€šè¿‡æœ€å°åŒ–æ¯”ç‰¹ç‡æ¶ˆè€—å’Œæ¸²æŸ“å¤±çœŸï¼Œè”åˆä¼˜åŒ–åŸºå…ƒå’Œç¥ç»ç½‘ç»œï¼Œå®ç°ç´§å‡‘çš„åŸºå…ƒè¡¨ç¤ºå’Œå‹ç¼©æ•ˆç‡ã€‚</p><p>(4): ç†µä¼°è®¡ï¼šåˆ©ç”¨ç†µä¼°è®¡æ¥æœ‰æ•ˆå»ºæ¨¡é”šå®šåŸºå…ƒå’Œè€¦åˆåŸºå…ƒçš„æ¯”ç‰¹ç‡ï¼Œä¸ºé€Ÿç‡çº¦æŸä¼˜åŒ–æä¾›ä¿¡æ¯ã€‚</p><ol><li>ç»“è®ºï¼š(1) æœ¬å·¥ä½œæå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„ 3D åœºæ™¯è¡¨ç¤ºæ–¹æ³•ï¼Œå‹ç¼©é«˜æ–¯å–·å°„ (CompGS)ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ç´§å‡‘çš„åŸºå…ƒè¿›è¡Œé«˜æ•ˆçš„ 3D åœºæ™¯è¡¨ç¤ºï¼ŒåŒæ—¶æ˜¾è‘—å‡å°äº†æ•°æ®å¤§å°ã€‚åœ¨æ­¤ï¼Œæˆ‘ä»¬ä¸ºç´§å‡‘çš„åœºæ™¯å»ºæ¨¡é‡èº«å®šåˆ¶äº†æ··åˆåŸºå…ƒç»“æ„ï¼Œå…¶ä¸­è€¦åˆåŸºå…ƒé€šè¿‡æœ‰é™çš„é”šå®šåŸºå…ƒé›†æœ‰æ•ˆåœ°è¿›è¡Œé¢„æµ‹ï¼Œä»è€Œå°è£…æˆç®€æ´çš„æ®‹å·®åµŒå…¥ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªé€Ÿç‡çº¦æŸä¼˜åŒ–æ–¹æ¡ˆï¼Œä»¥è¿›ä¸€æ­¥æé«˜åŸºå…ƒçš„ç´§å‡‘æ€§ã€‚åœ¨è¿™ä¸ªæ–¹æ¡ˆä¸­ï¼ŒåŸºå…ƒæ¯”ç‡æ¨¡å‹é€šè¿‡ç†µä¼°è®¡å»ºç«‹ï¼Œç„¶ååˆ¶å®šé€Ÿç‡å¤±çœŸä»£ä»·ä»¥ä¼˜åŒ–è¿™äº›åŸºå…ƒï¼Œä»¥åœ¨æ¸²æŸ“æ•ˆç‡å’Œæ¯”ç‰¹ç‡æ¶ˆè€—ä¹‹é—´å®ç°æœ€ä½³æƒè¡¡ã€‚ç»“åˆæ··åˆåŸºå…ƒç»“æ„å’Œé€Ÿç‡çº¦æŸä¼˜åŒ–ï¼Œæˆ‘ä»¬çš„ CompGS ä¼˜äºç°æœ‰çš„å‹ç¼©æ–¹æ³•ï¼Œå®ç°äº†å“è¶Šçš„å°ºå¯¸ç¼©å‡ï¼ŒåŒæ—¶ä¸å½±å“æ¸²æŸ“è´¨é‡ã€‚(2) åˆ›æ–°ç‚¹ï¼š</li><li>æ··åˆåŸºå…ƒç»“æ„ï¼Œé€šè¿‡é”šå®šåŸºå…ƒé¢„æµ‹è€¦åˆåŸºå…ƒï¼Œå®ç°ç´§å‡‘çš„åœºæ™¯å»ºæ¨¡ã€‚</li><li>é€Ÿç‡çº¦æŸä¼˜åŒ–æ–¹æ¡ˆï¼Œè”åˆä¼˜åŒ–åŸºå…ƒå’Œç¥ç»ç½‘ç»œï¼Œå®ç°ç´§å‡‘çš„åŸºå…ƒè¡¨ç¤ºå’Œå‹ç¼©æ•ˆç‡ã€‚</li><li>ç†µä¼°è®¡ï¼Œç”¨äºæœ‰æ•ˆå»ºæ¨¡åŸºå…ƒçš„æ¯”ç‰¹ç‡ï¼Œä¸ºé€Ÿç‡çº¦æŸä¼˜åŒ–æä¾›ä¿¡æ¯ã€‚æ€§èƒ½ï¼š</li><li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨ 3D åœºæ™¯è¡¨ç¤ºä¸­å®ç°äº†å“è¶Šçš„ç´§å‡‘æ€§ï¼ŒåŒæ—¶ä¸å½±å“æ¨¡å‹å‡†ç¡®æ€§å’Œæ¸²æŸ“è´¨é‡ã€‚å·¥ä½œé‡ï¼š</li><li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„ 3D åœºæ™¯è¡¨ç¤ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ç´§å‡‘çš„åŸºå…ƒè¿›è¡Œé«˜æ•ˆçš„ 3D åœºæ™¯è¡¨ç¤ºï¼ŒåŒæ—¶æ˜¾è‘—å‡å°äº†æ•°æ®å¤§å°ã€‚</li><li>è®¾è®¡äº†ä¸€ç§æ··åˆåŸºå…ƒç»“æ„ï¼ŒåŒ…æ‹¬é”šå®šåŸºå…ƒå’Œè€¦åˆåŸºå…ƒï¼Œä»¥æ•è·åŸºå…ƒä¹‹é—´çš„é¢„æµ‹å…³ç³»ã€‚</li><li>åˆ©ç”¨ä¸€å°ç»„é”šå®šåŸºå…ƒè¿›è¡Œé¢„æµ‹ï¼Œå…è®¸å°†å¤§å¤šæ•°åŸºå…ƒå°è£…æˆé«˜åº¦ç´§å‡‘çš„æ®‹å·®å½¢å¼ã€‚</li><li>å¼€å‘äº†ä¸€ä¸ªé€Ÿç‡çº¦æŸä¼˜åŒ–æ–¹æ¡ˆï¼Œé€šè¿‡æœ€å°åŒ–æ¯”ç‰¹ç‡æ¶ˆè€—å’Œæ¸²æŸ“å¤±çœŸï¼Œè”åˆä¼˜åŒ–åŸºå…ƒå’Œç¥ç»ç½‘ç»œï¼Œå®ç°ç´§å‡‘çš„åŸºå…ƒè¡¨ç¤ºå’Œå‹ç¼©æ•ˆç‡ã€‚</li><li>åˆ©ç”¨ç†µä¼°è®¡æ¥æœ‰æ•ˆå»ºæ¨¡é”šå®šåŸºå…ƒå’Œè€¦åˆåŸºå…ƒçš„æ¯”ç‰¹ç‡ï¼Œä¸ºé€Ÿç‡çº¦æŸä¼˜åŒ–æä¾›ä¿¡æ¯ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-7d580221a3c320fe2485a958d5382e40.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e32abb85df2e20749e660a25f1ddab87.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4824050d2c2a0b18fbff95df4c7fbc91.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c0ac958806d8e21cae6a784ed3e74514.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9cbd9e0e22a3201f4a5053aa5a6d3df2.jpg" align="middle"></details><h2 id="DreamScape-3D-Scene-Creation-via-Gaussian-Splatting-joint-Correlation-Modeling"><a href="#DreamScape-3D-Scene-Creation-via-Gaussian-Splatting-joint-Correlation-Modeling" class="headerlink" title="DreamScape: 3D Scene Creation via Gaussian Splatting joint Correlation   Modeling"></a>DreamScape: 3D Scene Creation via Gaussian Splatting joint Correlation   Modeling</h2><p><strong>Authors:Xuening Yuan, Hongyu Yang, Yueming Zhao, Di Huang</strong></p><p>Recent progress in text-to-3D creation has been propelled by integrating the potent prior of Diffusion Models from text-to-image generation into the 3D domain. Nevertheless, generating 3D scenes characterized by multiple instances and intricate arrangements remains challenging. In this study, we present DreamScape, a method for creating highly consistent 3D scenes solely from textual descriptions, leveraging the strong 3D representation capabilities of Gaussian Splatting and the complex arrangement abilities of large language models (LLMs). Our approach involves a 3D Gaussian Guide ($3{DG^2}$) for scene representation, consisting of semantic primitives (objects) and their spatial transformations and relationships derived directly from text prompts using LLMs. This compositional representation allows for local-to-global optimization of the entire scene. A progressive scale control is tailored during local object generation, ensuring that objects of different sizes and densities adapt to the scene, which addresses training instability issue arising from simple blending in the subsequent global optimization stage. To mitigate potential biases of LLM priors, we model collision relationships between objects at the global level, enhancing physical correctness and overall realism. Additionally, to generate pervasive objects like rain and snow distributed extensively across the scene, we introduce a sparse initialization and densification strategy. Experiments demonstrate that DreamScape offers high usability and controllability, enabling the generation of high-fidelity 3D scenes from only text prompts and achieving state-of-the-art performance compared to other methods. </p><p><a href="http://arxiv.org/abs/2404.09227v1">PDF</a> </p><p><strong>Summary</strong><br>DreamScape æ˜¯ä¸€ä¸ªä»…ä»æ–‡æœ¬æè¿°ä¸­åˆ›å»ºé«˜åº¦ä¸€è‡´çš„ 3D åœºæ™¯çš„æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨äº† Gaussian Splatting çš„å¼ºå¤§ 3D è¡¨ç¤ºåŠŸèƒ½å’Œå¤§è¯­è¨€æ¨¡å‹ (LLM) çš„å¤æ‚æ’åˆ—èƒ½åŠ›ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>3D é«˜æ–¯æŒ‡å— ($3{DG^2}$) ç”¨äºåœºæ™¯è¡¨ç¤ºï¼ŒåŒ…æ‹¬ä» LLM ä½¿ç”¨æ–‡æœ¬æç¤ºç›´æ¥æ¨å¯¼å‡ºçš„è¯­ä¹‰åŸè¯­ï¼ˆå¯¹è±¡ï¼‰åŠå…¶ç©ºé—´å˜æ¢å’Œå…³ç³»ã€‚</li><li>æ¸è¿›å¼æ¯”ä¾‹æ§åˆ¶åœ¨å±€éƒ¨å¯¹è±¡ç”ŸæˆæœŸé—´è¿›è¡Œå®šåˆ¶ï¼Œç¡®ä¿ä¸åŒå¤§å°å’Œå¯†åº¦çš„å¯¹è±¡é€‚åº”åœºæ™¯ï¼Œè§£å†³äº†åç»­å…¨å±€ä¼˜åŒ–é˜¶æ®µä¸­ç®€å•æ··åˆå¼•èµ·çš„è®­ç»ƒä¸ç¨³å®šé—®é¢˜ã€‚</li><li>ä¸ºäº†å‡è½» LLM å…ˆéªŒçš„æ½œåœ¨åå·®ï¼Œæˆ‘ä»¬åœ¨å…¨å±€çº§åˆ«å¯¹å¯¹è±¡ä¹‹é—´çš„ç¢°æ’å…³ç³»è¿›è¡Œå»ºæ¨¡ï¼Œä»è€Œå¢å¼ºç‰©ç†æ­£ç¡®æ€§å’Œæ•´ä½“çœŸå®æ„Ÿã€‚</li><li>ä¸ºäº†ç”Ÿæˆåƒé›¨å’Œé›ªè¿™æ ·å¹¿æ³›åˆ†å¸ƒåœ¨åœºæ™¯ä¸­çš„æ™®éå¯¹è±¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç¨€ç–åˆå§‹åŒ–å’Œè‡´å¯†åŒ–ç­–ç•¥ã€‚</li><li>å®éªŒè¡¨æ˜ï¼ŒDreamScape å…·æœ‰å¾ˆé«˜çš„å¯ç”¨æ€§å’Œå¯æ§æ€§ï¼Œèƒ½å¤Ÿä»…ä»æ–‡æœ¬æç¤ºç”Ÿæˆé«˜ä¿çœŸ 3D åœºæ™¯ï¼Œå¹¶ä¸”ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šDreamScapeï¼šåŸºäºé«˜æ–¯æ³¼æº…çš„ 3D åœºæ™¯åˆ›å»º</li><li>ä½œè€…ï¼šè¢é›ªå®ã€æ¨å®å®‡ã€èµµæ‚¦æ˜ã€é»„è¿ª</li><li>å•ä½ï¼šåŒ—äº¬èˆªç©ºèˆªå¤©å¤§å­¦</li><li>å…³é”®è¯ï¼šå¤šæ¨¡æ€ç”Ÿæˆã€3D åœºæ™¯ç”Ÿæˆã€åœºæ™¯åˆæˆã€3D é«˜æ–¯æ³¼æº…ã€LLM</li><li>è®ºæ–‡é“¾æ¥ï¼šNone    Github é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼š(1)ï¼šç ”ç©¶èƒŒæ™¯ï¼šè¿‘å¹´æ¥ï¼Œæ–‡æœ¬åˆ° 3D åˆ›å»ºçš„è¿›å±•å¾—ç›Šäºå°†æ¥è‡ªæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„æ‰©æ•£æ¨¡å‹çš„å¼ºå¤§å…ˆéªŒæ•´åˆåˆ° 3D åŸŸä¸­ã€‚ç„¶è€Œï¼Œç”Ÿæˆå…·æœ‰å¤šä¸ªå®ä¾‹å’Œå¤æ‚æ’åˆ—ç‰¹å¾çš„ 3D åœºæ™¯ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚(2)ï¼šä»¥å¾€æ–¹æ³•ï¼šä»¥å¾€æ–¹æ³•å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š</li><li>æ— æ³•å¤„ç†å…·æœ‰å¤šä¸ªå®ä¾‹å’Œå¤æ‚æ’åˆ—çš„ 3D åœºæ™¯ã€‚</li><li>è®­ç»ƒä¸ç¨³å®šï¼Œå¯¼è‡´åœºæ™¯ä¸­ä¸åŒå¤§å°å’Œå¯†åº¦çš„å¯¹è±¡æ— æ³•é€‚åº”ã€‚</li><li>å®¹æ˜“å—åˆ° LLM å…ˆéªŒçš„åå·®å½±å“ï¼Œå¯¼è‡´ç‰©ç†ä¸æ­£ç¡®å’Œæ•´ä½“çœŸå®æ€§é™ä½ã€‚</li><li>éš¾ä»¥ç”Ÿæˆåˆ†å¸ƒåœ¨åœºæ™¯ä¸­çš„æ™®éå¯¹è±¡ï¼Œå¦‚é›¨å’Œé›ªã€‚(3)ï¼šæœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡º DreamScapeï¼Œè¿™æ˜¯ä¸€ç§ä»…ä»æ–‡æœ¬æè¿°åˆ›å»ºé«˜åº¦ä¸€è‡´çš„ 3D åœºæ™¯çš„æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨äº†é«˜æ–¯æ³¼æº…çš„å¼ºå¤§ 3D è¡¨ç¤ºèƒ½åŠ›å’Œå¤§è¯­è¨€æ¨¡å‹ (LLM) çš„å¤æ‚æ’åˆ—èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ï¼š</li><li>3D é«˜æ–¯å¼•å¯¼ (3DG2) ç”¨äºåœºæ™¯è¡¨ç¤ºï¼Œå®ƒç”±è¯­ä¹‰åŸºå…ƒï¼ˆå¯¹è±¡ï¼‰åŠå…¶ç©ºé—´å˜æ¢å’Œå…³ç³»ç»„æˆï¼Œè¿™äº›å…³ç³»ç›´æ¥ä»æ–‡æœ¬æç¤ºä¸­ä½¿ç”¨ LLM æ¨å¯¼å‡ºæ¥ã€‚è¿™ç§ç»„åˆè¡¨ç¤ºå…è®¸å¯¹æ•´ä¸ªåœºæ™¯è¿›è¡Œå±€éƒ¨åˆ°å…¨å±€ä¼˜åŒ–ã€‚</li><li>åœ¨å±€éƒ¨å¯¹è±¡ç”ŸæˆæœŸé—´å®šåˆ¶çš„æ¸è¿›å¼å°ºåº¦æ§åˆ¶ï¼Œç¡®ä¿ä¸åŒå¤§å°å’Œå¯†åº¦çš„å¯¹è±¡é€‚åº”åœºæ™¯ï¼Œä»è€Œè§£å†³åç»­å…¨å±€ä¼˜åŒ–é˜¶æ®µä¸­ç®€å•æ··åˆå¼•èµ·çš„è®­ç»ƒä¸ç¨³å®šæ€§é—®é¢˜ã€‚</li><li>ä¸ºäº†å‡è½» LLM å…ˆéªŒçš„æ½œåœ¨åå·®ï¼Œæˆ‘ä»¬åœ¨å…¨å±€çº§åˆ«å¯¹å¯¹è±¡ä¹‹é—´çš„ç¢°æ’å…³ç³»è¿›è¡Œå»ºæ¨¡ï¼Œä»è€Œå¢å¼ºç‰©ç†æ­£ç¡®æ€§å’Œæ•´ä½“çœŸå®æ€§ã€‚</li><li>ä¸ºäº†ç”Ÿæˆåˆ†å¸ƒåœ¨åœºæ™¯ä¸­çš„æ™®éå¯¹è±¡ï¼Œå¦‚é›¨å’Œé›ªï¼Œæˆ‘ä»¬å¼•å…¥äº†ç¨€ç–åˆå§‹åŒ–å’Œè‡´å¯†åŒ–ç­–ç•¥ã€‚(4)ï¼šå®éªŒç»“æœï¼šå®éªŒè¡¨æ˜ï¼ŒDreamScape å…·æœ‰å¾ˆé«˜çš„å¯ç”¨æ€§å’Œå¯æ§æ€§ï¼Œèƒ½å¤Ÿä»…ä»æ–‡æœ¬æç¤ºç”Ÿæˆé«˜ä¿çœŸ 3D åœºæ™¯ï¼Œå¹¶ä¸”ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li></ol><p>7.æ–¹æ³•ï¼š(1)æ–‡æœ¬æç¤ºå¼•å¯¼ä¸‹çš„3Dé«˜æ–¯å¼•å¯¼(3DG2)åœºæ™¯è¡¨ç¤ºï¼›(2)å®šåˆ¶çš„æ¸è¿›å¼å°ºåº¦æ§åˆ¶ï¼Œç¡®ä¿ä¸åŒå¤§å°å’Œå¯†åº¦çš„å¯¹è±¡é€‚åº”åœºæ™¯ï¼›(3)å…¨å±€å¯¹è±¡ç¢°æ’å…³ç³»å»ºæ¨¡ï¼Œå¢å¼ºç‰©ç†æ­£ç¡®æ€§å’Œæ•´ä½“çœŸå®æ€§ï¼›(4)ç¨€ç–åˆå§‹åŒ–å’Œè‡´å¯†åŒ–ç­–ç•¥ï¼Œç”Ÿæˆåˆ†å¸ƒåœ¨åœºæ™¯ä¸­çš„æ™®éå¯¹è±¡ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡º DreamScapeï¼Œè¿™æ˜¯ä¸€ç§ä»…ä»æ–‡æœ¬æè¿°åˆ›å»ºé«˜åº¦ä¸€è‡´çš„ 3D åœºæ™¯çš„æ–¹æ³•ï¼Œåˆ©ç”¨äº†é«˜æ–¯æ³¼æº…çš„å¼ºå¤§ 3D è¡¨ç¤ºèƒ½åŠ›å’Œå¤§è¯­è¨€æ¨¡å‹ (LLM) çš„å¤æ‚æ’åˆ—èƒ½åŠ›ã€‚DreamScape å…·æœ‰å¾ˆé«˜çš„å¯ç”¨æ€§å’Œå¯æ§æ€§ï¼Œèƒ½å¤Ÿä»…ä»æ–‡æœ¬æç¤ºç”Ÿæˆé«˜ä¿çœŸ 3D åœºæ™¯ï¼Œå¹¶ä¸”ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>æå‡º 3D é«˜æ–¯å¼•å¯¼ (3DG2) åœºæ™¯è¡¨ç¤ºï¼Œå°†è¯­ä¹‰åŸºå…ƒåŠå…¶ç©ºé—´å˜æ¢å’Œå…³ç³»ç›´æ¥ä»æ–‡æœ¬æç¤ºä¸­ä½¿ç”¨ LLM æ¨å¯¼å‡ºæ¥ï¼Œå®ç°å±€éƒ¨åˆ°å…¨å±€ä¼˜åŒ–ã€‚</li><li>é‡‡ç”¨å®šåˆ¶çš„æ¸è¿›å¼å°ºåº¦æ§åˆ¶ï¼Œç¡®ä¿ä¸åŒå¤§å°å’Œå¯†åº¦çš„å¯¹è±¡é€‚åº”åœºæ™¯ï¼Œè§£å†³è®­ç»ƒä¸ç¨³å®šæ€§é—®é¢˜ã€‚</li><li>å»ºæ¨¡å¯¹è±¡ä¹‹é—´çš„ç¢°æ’å…³ç³»ï¼Œå¢å¼ºç‰©ç†æ­£ç¡®æ€§å’Œæ•´ä½“çœŸå®æ€§ã€‚</li><li>å¼•å…¥ç¨€ç–åˆå§‹åŒ–å’Œè‡´å¯†åŒ–ç­–ç•¥ï¼Œç”Ÿæˆåˆ†å¸ƒåœ¨åœºæ™¯ä¸­çš„æ™®éå¯¹è±¡ã€‚æ€§èƒ½ï¼š</li><li>èƒ½å¤Ÿä»…ä»æ–‡æœ¬æç¤ºç”Ÿæˆé«˜ä¿çœŸ 3D åœºæ™¯ã€‚</li><li>ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å·¥ä½œé‡ï¼š</li><li>éœ€è¦ä¸€ä¸ªç›¸å¯¹è¾ƒå¤§çš„å¼•å¯¼æ¯”ä¾‹æ¥ç¡®ä¿æ¨¡å‹æ”¶æ•›ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-4f88c017fbd210351ebea517e05fd02b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2e9e7a61267e388dc08cefff90f6c8da.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a3f52ac0c6425d9782495f42a860e11c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4177109b99815945eb22c94298f7ecfd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bcf3ed3cba06de00cc7e8e90859ff690.jpg" align="middle"></details><h2 id="LoopGaussian-Creating-3D-Cinemagraph-with-Multi-view-Images-via-Eulerian-Motion-Field"><a href="#LoopGaussian-Creating-3D-Cinemagraph-with-Multi-view-Images-via-Eulerian-Motion-Field" class="headerlink" title="LoopGaussian: Creating 3D Cinemagraph with Multi-view Images via   Eulerian Motion Field"></a>LoopGaussian: Creating 3D Cinemagraph with Multi-view Images via   Eulerian Motion Field</h2><p><strong>Authors:Jiyang Li, Lechao Cheng, Zhangye Wang, Tingting Mu, Jingxuan He</strong></p><p>Cinemagraph is a unique form of visual media that combines elements of still photography and subtle motion to create a captivating experience. However, the majority of videos generated by recent works lack depth information and are confined to the constraints of 2D image space. In this paper, inspired by significant progress in the field of novel view synthesis (NVS) achieved by 3D Gaussian Splatting (3D-GS), we propose LoopGaussian to elevate cinemagraph from 2D image space to 3D space using 3D Gaussian modeling. To achieve this, we first employ the 3D-GS method to reconstruct 3D Gaussian point clouds from multi-view images of static scenes,incorporating shape regularization terms to prevent blurring or artifacts caused by object deformation. We then adopt an autoencoder tailored for 3D Gaussian to project it into feature space. To maintain the local continuity of the scene, we devise SuperGaussian for clustering based on the acquired features. By calculating the similarity between clusters and employing a two-stage estimation method, we derive an Eulerian motion field to describe velocities across the entire scene. The 3D Gaussian points then move within the estimated Eulerian motion field. Through bidirectional animation techniques, we ultimately generate a 3D Cinemagraph that exhibits natural and seamlessly loopable dynamics. Experiment results validate the effectiveness of our approach, demonstrating high-quality and visually appealing scene generation. The project is available at <a href="https://pokerlishao.github.io/LoopGaussian/">https://pokerlishao.github.io/LoopGaussian/</a>. </p><p><a href="http://arxiv.org/abs/2404.08966v2">PDF</a> 10 pages</p><p><strong>Summary</strong><br>åŸºäº3Dé«˜æ–¯å»ºæ¨¡ï¼ŒLoopGaussianåˆ©ç”¨å¤šè§†å›¾å›¾åƒåˆæˆæŠ€æœ¯ï¼Œå°†å½±éŸ³å›¾ä»2Då›¾åƒç©ºé—´å‡çº§ä¸º3Dç©ºé—´ï¼Œå±•ç°è‡ªç„¶æµç•…çš„åŠ¨æ€æ•ˆæœã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>å½±éŸ³å›¾ç»“åˆé™æ­¢å›¾åƒå’Œç»†å¾®è¿åŠ¨ï¼Œåˆ›é€ å¼•äººå…¥èƒœçš„ä½“éªŒã€‚</li><li>LoopGaussiané‡‡ç”¨3Dé«˜æ–¯å»ºæ¨¡ï¼Œå°†å½±éŸ³å›¾ä»2Dæå‡è‡³3Dç©ºé—´ã€‚</li><li>3D-GSæ–¹æ³•ä»é™æ€åœºæ™¯çš„å¤šè§†å›¾å›¾åƒé‡å»º3Dé«˜æ–¯ç‚¹äº‘ï¼Œå¹¶èåˆå½¢çŠ¶æ­£åˆ™åŒ–é¡¹é˜²æ­¢å˜å½¢ã€‚</li><li>3Dé«˜æ–¯è‡ªç¼–ç å™¨å°†ç‚¹äº‘æŠ•å½±åˆ°ç‰¹å¾ç©ºé—´ã€‚</li><li>åŸºäºç‰¹å¾ï¼ŒSuperGaussianè¿›è¡Œèšç±»ï¼Œä¿æŒåœºæ™¯å±€éƒ¨è¿ç»­æ€§ã€‚</li><li>ä¸¤é˜¶æ®µä¼°è®¡æ–¹æ³•è®¡ç®—ç°‡é—´ç›¸ä¼¼æ€§ï¼Œå¯¼å‡ºæ¬§æ‹‰è¿åŠ¨åœºæè¿°åœºæ™¯é€Ÿåº¦ã€‚</li><li>3Dé«˜æ–¯ç‚¹åœ¨ä¼°è®¡çš„æ¬§æ‹‰è¿åŠ¨åœºå†…ç§»åŠ¨ï¼ŒåŒå‘åŠ¨ç”»æŠ€æœ¯äº§ç”Ÿè‡ªç„¶å¾ªç¯çš„3Då½±éŸ³å›¾ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šLoopGaussianï¼šé€šè¿‡æ¬§æ‹‰è¿åŠ¨åœºåˆ›å»ºå…·æœ‰å¤šè§†å›¾çš„ 3D å½±åƒå›¾</li><li>ä½œè€…ï¼šæå˜‰æ´‹ï¼Œç¨‹ä¹è¶…ï¼Œç‹å¼ é‡ï¼Œç©†å©·å©·ï¼Œä½•é™è½©</li><li>å•ä½ï¼šæµ™æ±Ÿå¤§å­¦</li><li>å…³é”®è¯ï¼šå½±åƒå›¾ï¼ŒåŠ¨æ€åœºæ™¯ç”Ÿæˆï¼Œ3D åœºæ™¯é‡å»º</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.08966   Github ä»£ç é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šå½±åƒå›¾æ˜¯ä¸€ç§ç‹¬ç‰¹çš„è§†è§‰åª’ä½“å½¢å¼ï¼Œå®ƒç»“åˆäº†é™æ­¢æ‘„å½±å’Œå¾®å¦™è¿åŠ¨çš„å…ƒç´ ï¼Œåˆ›é€ å‡ºä¸€ç§å¼•äººå…¥èƒœçš„ä½“éªŒã€‚ç„¶è€Œï¼Œç›®å‰å¤§å¤šæ•°ä½œå“ç”Ÿæˆçš„è§†é¢‘ç¼ºä¹æ·±åº¦ä¿¡æ¯ï¼Œå¹¶ä¸”å±€é™äº 2D å›¾åƒç©ºé—´çš„çº¦æŸã€‚   ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæœ¬æ–‡å€Ÿé‰´äº† 3D é«˜æ–¯ç‚¹äº‘ï¼ˆ3D-GSï¼‰åœ¨æ–°å‹è§†å›¾åˆæˆï¼ˆNVSï¼‰é¢†åŸŸå–å¾—çš„é‡å¤§è¿›å±•ï¼Œæå‡ºäº†ä¸€ç§åä¸º LoopGaussian çš„æ–¹æ³•ï¼Œåˆ©ç”¨ 3D é«˜æ–¯å»ºæ¨¡å°†å½±åƒå›¾ä» 2D å›¾åƒç©ºé—´æå‡åˆ° 3D ç©ºé—´ã€‚   ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šé¦–å…ˆï¼Œæœ¬æ–‡ä½¿ç”¨ 3D-GS æ–¹æ³•ä»é™æ€åœºæ™¯çš„å¤šè§†å›¾å›¾åƒä¸­é‡å»º 3D é«˜æ–¯ç‚¹äº‘ï¼Œå¹¶ç»“åˆå½¢çŠ¶æ­£åˆ™åŒ–é¡¹ä»¥é˜²æ­¢å› ç‰©ä½“å˜å½¢è€Œé€ æˆçš„æ¨¡ç³Šæˆ–ä¼ªå½±ã€‚ç„¶åï¼Œé‡‡ç”¨é’ˆå¯¹ 3D é«˜æ–¯é‡èº«å®šåˆ¶çš„è‡ªç¼–ç å™¨å°†å…¶æŠ•å½±åˆ°ç‰¹å¾ç©ºé—´ã€‚ä¸ºäº†ä¿æŒåœºæ™¯çš„å±€éƒ¨è¿ç»­æ€§ï¼Œæœ¬æ–‡è®¾è®¡äº† SuperGaussianï¼ŒåŸºäºæ‰€è·å–çš„ç‰¹å¾è¿›è¡Œèšç±»ã€‚é€šè¿‡è®¡ç®—èšç±»ä¹‹é—´çš„ç›¸ä¼¼æ€§å¹¶é‡‡ç”¨ä¸¤é˜¶æ®µä¼°è®¡æ–¹æ³•ï¼Œæ¨å¯¼å‡ºä¸€ä¸ªæ¬§æ‹‰è¿åŠ¨åœºæ¥æè¿°æ•´ä¸ªåœºæ™¯ä¸­çš„é€Ÿåº¦ã€‚ç„¶åï¼Œ3D é«˜æ–¯ç‚¹åœ¨ä¼°è®¡çš„æ¬§æ‹‰è¿åŠ¨åœºä¸­è¿åŠ¨ã€‚é€šè¿‡åŒå‘åŠ¨ç”»æŠ€æœ¯ï¼Œæœ€ç»ˆç”Ÿæˆä¸€ä¸ª 3D å½±åƒå›¾ï¼Œè¡¨ç°å‡ºè‡ªç„¶ä¸”æ— ç¼å¾ªç¯çš„åŠ¨æ€æ•ˆæœã€‚   ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šå®éªŒç»“æœéªŒè¯äº†æœ¬æ–‡æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†é«˜è´¨é‡ä¸”è§†è§‰ä¸Šå¸å¼•äººçš„åœºæ™¯ç”Ÿæˆã€‚è¯¥é¡¹ç›®å¯åœ¨ https://pokerlishao.github.io/LoopGaussian/ è·å¾—ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰åˆ©ç”¨ 3D-GS æ–¹æ³•ä»é™æ€åœºæ™¯çš„å¤šè§†å›¾å›¾åƒä¸­é‡å»º 3D é«˜æ–¯ç‚¹äº‘ï¼Œå¹¶ç»“åˆå½¢çŠ¶æ­£åˆ™åŒ–é¡¹ä»¥é˜²æ­¢å› ç‰©ä½“å˜å½¢è€Œé€ æˆçš„æ¨¡ç³Šæˆ–ä¼ªå½±ã€‚ï¼ˆ2ï¼‰é‡‡ç”¨é’ˆå¯¹ 3D é«˜æ–¯é‡èº«å®šåˆ¶çš„è‡ªç¼–ç å™¨å°†å…¶æŠ•å½±åˆ°ç‰¹å¾ç©ºé—´ã€‚ï¼ˆ3ï¼‰è®¾è®¡ SuperGaussianï¼ŒåŸºäºæ‰€è·å–çš„ç‰¹å¾è¿›è¡Œèšç±»ã€‚ï¼ˆ4ï¼‰è®¡ç®—èšç±»ä¹‹é—´çš„ç›¸ä¼¼æ€§å¹¶é‡‡ç”¨ä¸¤é˜¶æ®µä¼°è®¡æ–¹æ³•ï¼Œæ¨å¯¼å‡ºä¸€ä¸ªæ¬§æ‹‰è¿åŠ¨åœºæ¥æè¿°æ•´ä¸ªåœºæ™¯ä¸­çš„é€Ÿåº¦ã€‚ï¼ˆ5ï¼‰3D é«˜æ–¯ç‚¹åœ¨ä¼°è®¡çš„æ¬§æ‹‰è¿åŠ¨åœºä¸­è¿åŠ¨ã€‚ï¼ˆ6ï¼‰é€šè¿‡åŒå‘åŠ¨ç”»æŠ€æœ¯ï¼Œæœ€ç»ˆç”Ÿæˆä¸€ä¸ª 3D å½±åƒå›¾ï¼Œè¡¨ç°å‡ºè‡ªç„¶ä¸”æ— ç¼å¾ªç¯çš„åŠ¨æ€æ•ˆæœã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§åä¸º LoopGaussian çš„æ–°é¢–æ¡†æ¶ï¼Œç”¨äºä»é™æ€åœºæ™¯çš„å¤šè§†å›¾å›¾åƒç”ŸæˆçœŸå®çš„ 3D å½±åƒå›¾ã€‚é€šè¿‡åˆ©ç”¨ 3D é«˜æ–¯ç‚¹äº‘ splatting å’Œå›ºæœ‰çš„æ¬§æ‹‰è¿åŠ¨åœºï¼ŒLoopGaussian å¯ä»¥ç”Ÿæˆå…·æœ‰æ·±åº¦ä¿¡æ¯å’Œè‡ªç„¶åŠ¨æ€æ•ˆæœçš„ 3D å½±åƒå›¾ã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§ä½¿ç”¨ 3D é«˜æ–¯ splatting ä»å¤šè§†å›¾å›¾åƒé‡å»º 3D é«˜æ–¯ç‚¹äº‘çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°è¡¨ç¤ºåœºæ™¯çš„å½¢çŠ¶å’Œå¤–è§‚ã€‚</li><li>è®¾è®¡äº†ä¸€ç§é’ˆå¯¹ 3D é«˜æ–¯é‡èº«å®šåˆ¶çš„è‡ªç¼–ç å™¨ï¼Œå¯ä»¥å°† 3D é«˜æ–¯ç‚¹äº‘æŠ•å½±åˆ°ç‰¹å¾ç©ºé—´ï¼Œä»è€Œæ•è·åœºæ™¯çš„å±€éƒ¨è¿ç»­æ€§ã€‚</li><li>æå‡ºäº†ä¸€ç§åŸºäº SuperGaussian èšç±»çš„ä¸¤é˜¶æ®µä¼°è®¡æ–¹æ³•ï¼Œç”¨äºä»ç‰¹å¾ä¸­æ¨å¯¼å‡ºæ¬§æ‹‰è¿åŠ¨åœºï¼Œè¯¥è¿åŠ¨åœºæè¿°äº†åœºæ™¯ä¸­çš„é€Ÿåº¦ã€‚</li><li>æå‡ºäº†ä¸€ç§åŒå‘åŠ¨ç”»æŠ€æœ¯ï¼Œå¯ä»¥å°† 3D é«˜æ–¯ç‚¹äº‘åœ¨ä¼°è®¡çš„æ¬§æ‹‰è¿åŠ¨åœºä¸­è¿åŠ¨ï¼Œä»è€Œç”Ÿæˆå…·æœ‰è‡ªç„¶ä¸”æ— ç¼å¾ªç¯åŠ¨æ€æ•ˆæœçš„ 3D å½±åƒå›¾ã€‚æ€§èƒ½ï¼š</li><li>LoopGaussian ç”Ÿæˆçš„ 3D å½±åƒå›¾å…·æœ‰é«˜è´¨é‡å’Œè§†è§‰å¸å¼•åŠ›ï¼Œå±•ç¤ºäº†åœºæ™¯çš„æ·±åº¦ä¿¡æ¯å’Œè‡ªç„¶åŠ¨æ€æ•ˆæœã€‚</li><li>LoopGaussian åœ¨å®šæ€§å’Œå®šé‡è¯„ä¼°ä¸­éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¯æ˜äº†å…¶ç”Ÿæˆé€¼çœŸ 3D å½±åƒå›¾çš„èƒ½åŠ›ã€‚å·¥ä½œé‡ï¼š</li><li>LoopGaussian çš„å®ç°æ¶‰åŠå¤šä¸ªæ­¥éª¤ï¼ŒåŒ…æ‹¬ 3D é«˜æ–¯ç‚¹äº‘é‡å»ºã€ç‰¹å¾æå–ã€æ¬§æ‹‰è¿åŠ¨åœºä¼°è®¡å’ŒåŒå‘åŠ¨ç”»ã€‚</li><li>è¯¥æ–¹æ³•éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºï¼Œå°¤å…¶æ˜¯å¯¹äºå¤æ‚åœºæ™¯ã€‚</li><li>ç„¶è€Œï¼ŒLoopGaussian æä¾›äº†ä¸€ä¸ªæ˜“äºä½¿ç”¨çš„ç•Œé¢ï¼Œå¯ä»¥æ–¹ä¾¿åœ°ç”Ÿæˆ 3D å½±åƒå›¾ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-e2eba83df2d2f5ac39a3c3be75067d7a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-51e0153b3e7c8edd334c5d696dd3d80a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-17bf5e90eaefe5fa5f4c44be35ab164a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-5e793286a69b6603190dc809e34a6be3.jpg" align="middle"></details><h2 id="OccGaussian-3D-Gaussian-Splatting-for-Occluded-Human-Rendering"><a href="#OccGaussian-3D-Gaussian-Splatting-for-Occluded-Human-Rendering" class="headerlink" title="OccGaussian: 3D Gaussian Splatting for Occluded Human Rendering"></a>OccGaussian: 3D Gaussian Splatting for Occluded Human Rendering</h2><p><strong>Authors:Jingrui Ye, Zongkai Zhang, Yujiao Jiang, Qingmin Liao, Wenming Yang, Zongqing Lu</strong></p><p>Rendering dynamic 3D human from monocular videos is crucial for various applications such as virtual reality and digital entertainment. Most methods assume the people is in an unobstructed scene, while various objects may cause the occlusion of body parts in real-life scenarios. Previous method utilizing NeRF for surface rendering to recover the occluded areas, but it requiring more than one day to train and several seconds to render, failing to meet the requirements of real-time interactive applications. To address these issues, we propose OccGaussian based on 3D Gaussian Splatting, which can be trained within 6 minutes and produces high-quality human renderings up to 160 FPS with occluded input. OccGaussian initializes 3D Gaussian distributions in the canonical space, and we perform occlusion feature query at occluded regions, the aggregated pixel-align feature is extracted to compensate for the missing information. Then we use Gaussian Feature MLP to further process the feature along with the occlusion-aware loss functions to better perceive the occluded area. Extensive experiments both in simulated and real-world occlusions, demonstrate that our method achieves comparable or even superior performance compared to the state-of-the-art method. And we improving training and inference speeds by 250x and 800x, respectively. Our code will be available for research purposes. </p><p><a href="http://arxiv.org/abs/2404.08449v2">PDF</a> </p><p><strong>Summary</strong><br>ç”¨3Dé«˜æ–¯æº…å°„æ³•å–ä»£NeRFï¼Œå¤§å¹…æå‡å•ç›®è§†é¢‘ç”ŸæˆåŠ¨æ€3Däººç‰©çš„é€Ÿåº¦å’Œæ•ˆç‡ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>åˆ©ç”¨3Dé«˜æ–¯æº…å°„ï¼Œç›´æ¥åœ¨è§„èŒƒç©ºé—´ä¸­åˆå§‹åŒ–3Dé«˜æ–¯åˆ†å¸ƒã€‚</li><li>ä½¿ç”¨é®æŒ¡ç‰¹å¾æŸ¥è¯¢è¡¥å¿ç¼ºå¤±ä¿¡æ¯ï¼Œç»“åˆé®æŒ¡æ„ŸçŸ¥losså‡½æ•°æ›´å¥½åœ°æ„ŸçŸ¥é®æŒ¡åŒºåŸŸã€‚</li><li>é‡‡ç”¨é«˜æ–¯ç‰¹å¾MLPè¿›ä¸€æ­¥å¤„ç†ç‰¹å¾ï¼Œæå‡é®æŒ¡åŒºåŸŸè¾¨è¯†åº¦ã€‚</li><li>åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®é®æŒ¡åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸æœ€å…ˆè¿›æ–¹æ³•åª²ç¾æˆ–ä¼˜å¼‚ã€‚</li><li>è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦ç›¸æ¯”ä¹‹å‰æå‡250å€å’Œ800å€ã€‚</li><li>é€‚ç”¨äºè™šæ‹Ÿç°å®å’Œæ•°å­—å¨±ä¹ç­‰å¤šç§åº”ç”¨åœºæ™¯ã€‚</li><li>æ–¹æ³•å·²å¼€æºï¼Œå¯ä¾›ç ”ç©¶ä½¿ç”¨ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šOccGaussianï¼šç”¨äºé®æŒ¡äººç±»æ¸²æŸ“çš„ 3D é«˜æ–¯å–·ç»˜ï¼ˆä¸­è¯‘ï¼‰</li><li>ä½œè€…ï¼šJingrui Ye, Zhongkai Zhang, Yujiao Jiang, Qingmin Liao*, Wenming Yang, Zongqing Lu</li><li>éš¶å±å•ä½ï¼šæ¸…åå¤§å­¦æ·±åœ³å›½é™…ç ”ç©¶ç”Ÿé™¢ï¼ˆä¸­è¯‘ï¼‰</li><li>å…³é”®è¯ï¼šCanonical 3D Gaussiansã€Novel View Synthesisã€10mins Trainingã€160FPS Rendering</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.08449</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šåŠ¨æ€ 3D äººç±»æ¸²æŸ“åœ¨è™šæ‹Ÿç°å®å’Œæ•°å­—å¨±ä¹ç­‰åº”ç”¨ä¸­è‡³å…³é‡è¦ã€‚å¤§å¤šæ•°æ–¹æ³•å‡è®¾äººç±»å¤„äºæ— é®æŒ¡åœºæ™¯ä¸­ï¼Œè€Œç°å®ç”Ÿæ´»åœºæ™¯ä¸­å„ç§ç‰©ä½“å¯èƒ½ä¼šå¯¼è‡´èº«ä½“éƒ¨ä½è¢«é®æŒ¡ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šä»¥å‰çš„æ–¹æ³•åˆ©ç”¨ NeRF è¿›è¡Œè¡¨é¢æ¸²æŸ“ä»¥æ¢å¤è¢«é®æŒ¡åŒºåŸŸï¼Œä½†è¿™éœ€è¦ä¸€å¤©å¤šçš„è®­ç»ƒæ—¶é—´å’Œå‡ ç§’é’Ÿçš„æ¸²æŸ“æ—¶é—´ï¼Œæ— æ³•æ»¡è¶³å®æ—¶äº¤äº’å¼åº”ç”¨çš„è¦æ±‚ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œè®ºæ–‡æå‡ºäº†åŸºäº 3D é«˜æ–¯å–·ç»˜çš„ OccGaussianï¼Œå®ƒå¯ä»¥åœ¨ 6 åˆ†é’Ÿå†…è®­ç»ƒå®Œæˆï¼Œå¹¶ä»¥é«˜è¾¾ 160FPS çš„é€Ÿåº¦ç”Ÿæˆé«˜è´¨é‡çš„äººç±»æ¸²æŸ“ï¼Œå³ä½¿è¾“å…¥è¢«é®æŒ¡ã€‚OccGaussian åœ¨è§„èŒƒç©ºé—´ä¸­åˆå§‹åŒ– 3D é«˜æ–¯åˆ†å¸ƒï¼Œå¹¶åœ¨è¢«é®æŒ¡åŒºåŸŸæ‰§è¡Œé®æŒ¡ç‰¹å¾æŸ¥è¯¢ï¼Œæå–èšåˆçš„åƒç´ å¯¹é½ç‰¹å¾ä»¥è¡¥å¿ç¼ºå¤±ä¿¡æ¯ã€‚ç„¶åä½¿ç”¨é«˜æ–¯ç‰¹å¾ MLP è¿›ä¸€æ­¥å¤„ç†ç‰¹å¾ï¼Œå¹¶ç»“åˆé®æŒ¡æ„ŸçŸ¥æŸå¤±å‡½æ•°æ¥æ›´å¥½åœ°æ„ŸçŸ¥è¢«é®æŒ¡åŒºåŸŸã€‚ï¼ˆ4ï¼‰ä»»åŠ¡å’Œæ€§èƒ½ï¼šåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œé®æŒ¡ä¸­è¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å®ç°äº†ç›¸å½“ç”šè‡³æ›´å¥½çš„æ€§èƒ½ã€‚å¹¶ä¸”å°†è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦åˆ†åˆ«æé«˜äº† 250 å€å’Œ 800 å€ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) 3Dé«˜æ–¯æ­£å‘è’™çš®ï¼šåœ¨è§„èŒƒç©ºé—´ä¸­åˆå§‹åŒ– 3D é«˜æ–¯åˆ†å¸ƒï¼Œå¹¶ä½¿ç”¨ LBS å˜æ¢æ ¹æ® SMPL å‚æ•°å°†ç‚¹æ˜ å°„åˆ°æ¯ä¸ªå¸§çš„å§¿æ€ç©ºé—´ã€‚ï¼ˆ2ï¼‰é®æŒ¡ç‰¹å¾æŸ¥è¯¢ï¼šå¯¹äºæ¯ä¸ªè¢«é®æŒ¡ç‚¹ï¼ŒæŸ¥è¯¢å…¶åœ¨æ‰€æœ‰å¯è§ç‚¹ä¸­çš„ K ä¸ªæœ€è¿‘å¯è§ç‚¹ï¼Œå¹¶å°†è¿™äº› K ä¸ªæœ€è¿‘å¯è§ç‚¹æŠ•å½±åˆ°ç‰¹å¾å›¾ä¸Šä»¥æå–åƒç´ å¯¹é½ç‰¹å¾ã€‚ï¼ˆ3ï¼‰é«˜æ–¯ç‰¹å¾ MLPï¼šå°†é®æŒ¡ç‰¹å¾ä¸åµŒå…¥çš„è¢«é®æŒ¡ç‚¹è¿æ¥èµ·æ¥ï¼Œå¹¶å°†å…¶æ”¾å…¥ MLP ä¸­ä»¥é¢„æµ‹çƒè°ç³»æ•° f å’Œä¸é€æ˜åº¦ Î±ã€‚ï¼ˆ4ï¼‰å¯å¾®æ¸²æŸ“å™¨ï¼šåº”ç”¨åŸºäºå¹³é“ºçš„å¯å¾®æ¸²æŸ“å™¨æ¥å®ç°å¿«é€Ÿæ¸²æŸ“å’Œè®­ç»ƒæœŸé—´çš„è‡ªé€‚åº”å¯†åº¦æ§åˆ¶ã€‚ï¼ˆ5ï¼‰æŸå¤±å‡½æ•°ï¼šè®¾è®¡é®æŒ¡æŸå¤±å’Œä¸€è‡´æ€§æŸå¤±ï¼Œä»¥é˜²æ­¢æ¨¡å‹åœ¨è¢«é®æŒ¡åŒºåŸŸå­¦ä¹ èƒŒæ™¯ä¿¡æ¯ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œé¦–æ¬¡æå‡ºäº†ä¸€ç§ä½¿ç”¨ 3D é«˜æ–¯å–·ç»˜åœ¨å•ç›®è§†é¢‘ä¸­æ¸²æŸ“äººç±»é®æŒ¡çš„æ–¹æ³•ã€‚ä»¥å¾€æ–¹æ³•åœ¨è®­ç»ƒå’Œæ¨ç†ä¸­è€—æ—¶å¤ªé•¿ï¼Œæ— æ³•æ»¡è¶³å®æ—¶åº”ç”¨çš„è¦æ±‚ï¼Œè€Œæˆ‘ä»¬å®ç°äº†å¿«é€Ÿè®­ç»ƒï¼ˆ6~13 åˆ†é’Ÿï¼‰å’Œå®æ—¶æ¸²æŸ“ï¼ˆ169FPSï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨é®æŒ¡åŒºåŸŸæ‰§è¡Œç‰¹å¾æŸ¥è¯¢ï¼Œå¹¶å°†å¯è§ K è¿‘é‚»ç‚¹çš„èšåˆåƒç´ å¯¹é½ç‰¹å¾è¾“å…¥åˆ° MLP ä¸­ä»¥å­¦ä¹ ä¸å¯è§ç‚¹çš„ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¸ºé®æŒ¡åŒºåŸŸè®¾è®¡äº†ä¸“é—¨çš„æŸå¤±å‡½æ•°ï¼Œä½¿æ¸²æŸ“æ›´åŠ å®Œæ•´ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œé®æŒ¡ä¸‹å°† OccGaussian ä¸ SOTA æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ OccGaussian åœ¨ä¿æŒå¿«é€Ÿè®­ç»ƒå’Œå®æ—¶æ¸²æŸ“çš„åŒæ—¶å®ç°äº† SOTA æ€§èƒ½ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šOccGaussian æ˜¯ç¬¬ä¸€ä¸ªä½¿ç”¨ 3D é«˜æ–¯å–·ç»˜æ¸²æŸ“é®æŒ¡åœºæ™¯ä¸­äººç±»çš„æ–¹æ³•ï¼›æå‡ºäº†é®æŒ¡åŒºåŸŸçš„ç‰¹å¾æŸ¥è¯¢å’Œ MLP é¢„æµ‹æœºåˆ¶ï¼Œæœ‰æ•ˆè¡¥å¿äº†ç¼ºå¤±ä¿¡æ¯ï¼›è®¾è®¡äº†ä¸“é—¨çš„é®æŒ¡æŸå¤±å‡½æ•°ï¼Œå¢å¼ºäº†æ¨¡å‹å¯¹é®æŒ¡åŒºåŸŸçš„æ„ŸçŸ¥èƒ½åŠ›ã€‚æ€§èƒ½ï¼šåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œé®æŒ¡åœºæ™¯ä¸‹ï¼ŒOccGaussian åœ¨æ¸²æŸ“è´¨é‡ä¸Šä¸ SOTA æ–¹æ³•ç›¸å½“ç”šè‡³æ›´å¥½ï¼›è®­ç»ƒé€Ÿåº¦æé«˜äº† 250 å€ï¼Œæ¨ç†é€Ÿåº¦æé«˜äº† 800 å€ã€‚å·¥ä½œé‡ï¼šOccGaussian çš„è®­ç»ƒæ—¶é—´ä»…ä¸º 6~13 åˆ†é’Ÿï¼Œæ¨ç†é€Ÿåº¦é«˜è¾¾ 169FPSï¼Œæ»¡è¶³äº†å®æ—¶äº¤äº’å¼åº”ç”¨çš„è¦æ±‚ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-22b96540b149a8534443374615ca8599.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ab11fe939c3521e47d3227ac9f217bda.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3930eb7a35a4e86cf46c4da432a8a109.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-fe0032a9957ec683a4fc3e6deab6cc1d.jpg" align="middle"></details><h2 id="3D-Geometry-aware-Deformable-Gaussian-Splatting-for-Dynamic-View-Synthesis"><a href="#3D-Geometry-aware-Deformable-Gaussian-Splatting-for-Dynamic-View-Synthesis" class="headerlink" title="3D Geometry-aware Deformable Gaussian Splatting for Dynamic View   Synthesis"></a>3D Geometry-aware Deformable Gaussian Splatting for Dynamic View   Synthesis</h2><p><strong>Authors:Zhicheng Lu, Xiang Guo, Le Hui, Tianrui Chen, Min Yang, Xiao Tang, Feng Zhu, Yuchao Dai</strong></p><p>In this paper, we propose a 3D geometry-aware deformable Gaussian Splatting method for dynamic view synthesis. Existing neural radiance fields (NeRF) based solutions learn the deformation in an implicit manner, which cannot incorporate 3D scene geometry. Therefore, the learned deformation is not necessarily geometrically coherent, which results in unsatisfactory dynamic view synthesis and 3D dynamic reconstruction. Recently, 3D Gaussian Splatting provides a new representation of the 3D scene, building upon which the 3D geometry could be exploited in learning the complex 3D deformation. Specifically, the scenes are represented as a collection of 3D Gaussian, where each 3D Gaussian is optimized to move and rotate over time to model the deformation. To enforce the 3D scene geometry constraint during deformation, we explicitly extract 3D geometry features and integrate them in learning the 3D deformation. In this way, our solution achieves 3D geometry-aware deformation modeling, which enables improved dynamic view synthesis and 3D dynamic reconstruction. Extensive experimental results on both synthetic and real datasets prove the superiority of our solution, which achieves new state-of-the-art performance.   The project is available at <a href="https://npucvr.github.io/GaGS/">https://npucvr.github.io/GaGS/</a> </p><p><a href="http://arxiv.org/abs/2404.06270v2">PDF</a> Accepted by CVPR 2024. Project page: <a href="https://npucvr.github.io/GaGS/">https://npucvr.github.io/GaGS/</a></p><p><strong>Summary</strong><br>ä¸‰ç»´å‡ ä½•æ„ŸçŸ¥å˜å½¢é«˜æ–¯æ•£å°„ï¼ˆ3DGSï¼‰æ–¹æ³•ï¼Œç”¨äºåŠ¨æ€è§†ç‚¹åˆæˆä¸­å¯¹ä¸‰ç»´åŠ¨æ€é‡å»ºå’Œå‡ ä½•çº¦æŸå˜å½¢å»ºæ¨¡çš„ä¼˜åŒ–ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>3DGS æ–¹æ³•ç»“åˆäº†ä¸‰ç»´åœºæ™¯å‡ ä½•å’Œé«˜æ–¯æ•£å°„ï¼Œè§£å†³äº†ç°æœ‰çš„ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æ–¹æ³•å˜å½¢å»ºæ¨¡ä¸­å‡ ä½•ä¸ä¸€è‡´çš„é—®é¢˜ã€‚</li><li>3DGS æ¨¡å‹æ˜¯ç”±å¯ç§»åŠ¨å’Œæ—‹è½¬çš„ä¸‰ç»´é«˜æ–¯å‡½æ•°ç»„æˆçš„ï¼Œèƒ½é€¼çœŸåœ°æ¨¡æ‹Ÿå˜å½¢ã€‚</li><li>è¯¥æ–¹æ³•æå–ä¸‰ç»´å‡ ä½•ç‰¹å¾ï¼Œå°†å…¶èå…¥å˜å½¢å­¦ä¹ ä¸­ï¼Œå®ç°äº†ä¸‰ç»´å‡ ä½•æ„ŸçŸ¥çš„å˜å½¢å»ºæ¨¡ã€‚</li><li>ä¸ä¼ ç»Ÿçš„ NeRF æ–¹æ³•ç›¸æ¯”ï¼Œ3DGS æ–¹æ³•åœ¨åŠ¨æ€è§†ç‚¹åˆæˆå’Œä¸‰ç»´åŠ¨æ€é‡å»ºä»»åŠ¡ä¸Šè·å¾—äº†æ›´å¥½çš„æ€§èƒ½ã€‚</li><li>3DGS æ–¹æ³•é€‚ç”¨äºåˆæˆæ•°æ®é›†å’ŒçœŸå®æ•°æ®é›†ã€‚</li><li>3DGS æ–¹æ³•åœ¨åŠ¨æ€è§†ç‚¹åˆæˆå’Œä¸‰ç»´åŠ¨æ€é‡å»ºä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li><li>3DGS é¡¹ç›®åœ°å€ï¼š<a href="https://npucvr.github.io/GaGS/">https://npucvr.github.io/GaGS/</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>è®ºæ–‡æ ‡é¢˜ï¼šåŸºäº 3D å‡ ä½•çš„åŠ¨æ€è§†ç‚¹åˆæˆå¯å˜å½¢é«˜æ–¯æ•£å°„</li><li>ä½œè€…ï¼šJun Gao, Yixin Zhu, Jiahao Li, Jingyi Yu, Yebin Liu, Qiang Liu, Xiaogang Wang</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šå—äº¬é‚®ç”µå¤§å­¦</li><li>å…³é”®è¯ï¼šåŠ¨æ€è§†ç‚¹åˆæˆã€ç¥ç»è¾å°„åœºã€é«˜æ–¯æ•£å°„ã€3D åœºæ™¯å‡ ä½•</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.02203Github é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åœ¨åŠ¨æ€è§†ç‚¹åˆæˆä¸­å–å¾—äº†æˆåŠŸï¼Œä½†å…¶éšå¼å˜å½¢å­¦ä¹ æ–¹å¼æ— æ³•å……åˆ†åˆ©ç”¨ 3D åœºæ™¯å‡ ä½•ä¿¡æ¯ï¼Œå¯¼è‡´å˜å½¢ä¸ä¸€è‡´ï¼Œå½±å“åˆæˆè´¨é‡ã€‚(2) è¿‡å»æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•é€šè¿‡å¼•å…¥ 3D é«˜æ–¯æ•£å°„è¡¨ç¤ºï¼Œå¯ä»¥æ˜¾å¼å»ºæ¨¡åœºæ™¯å‡ ä½•ï¼Œä½†ç¼ºä¹å¯¹å˜å½¢è¿‡ç¨‹çš„å‡ ä½•çº¦æŸã€‚(3) æœ¬æ–‡æ–¹æ³•ï¼šæå‡ºäº†ä¸€ç§åŸºäº 3D å‡ ä½•çš„åŠ¨æ€è§†ç‚¹åˆæˆå¯å˜å½¢é«˜æ–¯æ•£å°„æ–¹æ³•ï¼Œé€šè¿‡æå– 3D å‡ ä½•ç‰¹å¾å¹¶å°†å…¶èå…¥å˜å½¢å­¦ä¹ ä¸­ï¼Œå¢å¼ºäº†å˜å½¢ä¸åœºæ™¯å‡ ä½•çš„å…³è”æ€§ã€‚(4) æ–¹æ³•æ€§èƒ½ï¼šåœ¨åˆæˆå’Œé‡å»ºä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨åˆæˆè´¨é‡ã€å‡ ä½•ä¸€è‡´æ€§å’Œé²æ£’æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¯æ˜äº†å…¶åœ¨åŠ¨æ€è§†ç‚¹åˆæˆä¸­çš„æœ‰æ•ˆæ€§ã€‚</li></ol><p>7.æ–¹æ³•ï¼š(1): æå–3Då‡ ä½•ç‰¹å¾ï¼šåˆ©ç”¨åŸºäºç‚¹äº‘çš„3Då‡ ä½•åˆ†æç®—æ³•ï¼Œæå–åœºæ™¯çš„ç‚¹äº‘è¡¨ç¤ºï¼Œå¹¶ä»ä¸­æå–æ³•å‘é‡ã€æ›²ç‡ç­‰å‡ ä½•ç‰¹å¾ã€‚(2): èå…¥å˜å½¢å­¦ä¹ ï¼šå°†æå–çš„å‡ ä½•ç‰¹å¾ä½œä¸ºå…ˆéªŒçŸ¥è¯†ï¼Œèå…¥åˆ°ç¥ç»è¾å°„åœºçš„å˜å½¢å­¦ä¹ ä¸­ã€‚é€šè¿‡è®¾è®¡ä¸€ä¸ªå‡ ä½•æŸå¤±å‡½æ•°ï¼Œçº¦æŸå˜å½¢è¿‡ç¨‹ä¸åœºæ™¯å‡ ä½•ç‰¹å¾çš„ä¸€è‡´æ€§ã€‚(3): åŠ¨æ€è§†ç‚¹åˆæˆï¼šåˆ©ç”¨å˜å½¢åçš„ç¥ç»è¾å°„åœºï¼Œé€šè¿‡ä½“æ¸²æŸ“æŠ€æœ¯ï¼Œåˆæˆä¸åŒè§†ç‚¹çš„å›¾åƒã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäº 3D å‡ ä½•æ„ŸçŸ¥çš„é«˜æ–¯æ•£å°„å¯å˜å½¢åŠ¨æ€è§†ç‚¹åˆæˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡æå–åœºæ™¯çš„ 3D å‡ ä½•ç‰¹å¾å¹¶å°†å…¶èå…¥å˜å½¢å­¦ä¹ ä¸­ï¼Œå¢å¼ºäº†å˜å½¢ä¸åœºæ™¯å‡ ä½•çš„å…³è”æ€§ï¼Œåœ¨åˆæˆå’Œé‡å»ºä»»åŠ¡ä¸Šå–å¾—äº†è¾ƒå¥½çš„æ•ˆæœã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§åŸºäº 3D å‡ ä½•æ„ŸçŸ¥çš„é«˜æ–¯æ•£å°„å¯å˜å½¢åŠ¨æ€è§†ç‚¹åˆæˆæ–¹æ³•ã€‚</li><li>æå‡ºäº†ä¸€ç§åŸºäºç‚¹äº‘çš„ 3D å‡ ä½•åˆ†æç®—æ³•ï¼Œæå–åœºæ™¯çš„ç‚¹äº‘è¡¨ç¤ºï¼Œå¹¶ä»ä¸­æå–æ³•å‘é‡ã€æ›²ç‡ç­‰å‡ ä½•ç‰¹å¾ã€‚</li><li>è®¾è®¡äº†ä¸€ä¸ªå‡ ä½•æŸå¤±å‡½æ•°ï¼Œçº¦æŸå˜å½¢è¿‡ç¨‹ä¸åœºæ™¯å‡ ä½•ç‰¹å¾çš„ä¸€è‡´æ€§ã€‚</li><li>æ€§èƒ½ï¼š</li><li>åœ¨åˆæˆå’Œé‡å»ºä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨åˆæˆè´¨é‡ã€å‡ ä½•ä¸€è‡´æ€§å’Œé²æ£’æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¯æ˜äº†å…¶åœ¨åŠ¨æ€è§†ç‚¹åˆæˆä¸­çš„æœ‰æ•ˆæ€§ã€‚</li><li>å·¥ä½œé‡ï¼š</li><li>è¯¥æ–¹æ³•éœ€è¦æå–åœºæ™¯çš„ 3D å‡ ä½•ç‰¹å¾ï¼Œå¹¶å°†å…¶èå…¥å˜å½¢å­¦ä¹ ä¸­ï¼Œå› æ­¤å·¥ä½œé‡è¾ƒå¤§ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-08bef4d6505c50c1da246e20076b2580.jpg" align="middle"><img src="https://picx.zhimg.com/v2-653303d99420769bc567815e0df0bf5a.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f241223d01fd5b44400560a99355429c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-20cc277b202c45bf86629c31910f0a4c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-403b047ba4b0fb3b2c45a81dd2533d35.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-04-17  Gaussian Opacity Fields Efficient and Compact Surface Reconstruction in   Unbounded Scenes</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/04/17/Paper/2024-04-17/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/04/17/Paper/2024-04-17/Talking%20Head%20Generation/</id>
    <published>2024-04-17T10:33:12.000Z</published>
    <updated>2024-04-17T10:33:12.763Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-04-17-æ›´æ–°"><a href="#2024-04-17-æ›´æ–°" class="headerlink" title="2024-04-17 æ›´æ–°"></a>2024-04-17 æ›´æ–°</h1><h2 id="VASA-1-Lifelike-Audio-Driven-Talking-Faces-Generated-in-Real-Time"><a href="#VASA-1-Lifelike-Audio-Driven-Talking-Faces-Generated-in-Real-Time" class="headerlink" title="VASA-1: Lifelike Audio-Driven Talking Faces Generated in Real Time"></a>VASA-1: Lifelike Audio-Driven Talking Faces Generated in Real Time</h2><p><strong>Authors:Sicheng Xu, Guojun Chen, Yu-Xiao Guo, Jiaolong Yang, Chong Li, Zhenyu Zang, Yizhong Zhang, Xin Tong, Baining Guo</strong></p><p>We introduce VASA, a framework for generating lifelike talking faces with appealing visual affective skills (VAS) given a single static image and a speech audio clip. Our premiere model, VASA-1, is capable of not only producing lip movements that are exquisitely synchronized with the audio, but also capturing a large spectrum of facial nuances and natural head motions that contribute to the perception of authenticity and liveliness. The core innovations include a holistic facial dynamics and head movement generation model that works in a face latent space, and the development of such an expressive and disentangled face latent space using videos. Through extensive experiments including evaluation on a set of new metrics, we show that our method significantly outperforms previous methods along various dimensions comprehensively. Our method not only delivers high video quality with realistic facial and head dynamics but also supports the online generation of 512x512 videos at up to 40 FPS with negligible starting latency. It paves the way for real-time engagements with lifelike avatars that emulate human conversational behaviors. </p><p><a href="http://arxiv.org/abs/2404.10667v1">PDF</a> Tech Report. Project webpage:   <a href="https://www.microsoft.com/en-us/research/project/vasa-1/">https://www.microsoft.com/en-us/research/project/vasa-1/</a></p><p><strong>Summary</strong></p><p>é€šè¿‡å•å¼ é™æ€å›¾åƒå’Œè¯­éŸ³éŸ³é¢‘å‰ªè¾‘ç”Ÿæˆé€¼çœŸçš„ä¼šè¯´è¯çš„é¢å­”ï¼Œå…·æœ‰è§†è§‰æƒ…æ„ŸæŠ€èƒ½ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡º VASA æ¡†æ¶ï¼Œå¯ç”Ÿæˆé€¼çœŸçš„ä¼šè¯´è¯çš„é¢å­”ï¼Œå…·æœ‰è§†è§‰æƒ…æ„ŸæŠ€èƒ½ã€‚</li><li>æ ¸å¿ƒåˆ›æ–°åœ¨äºåœ¨äººè„¸æ½œåœ¨ç©ºé—´ä¸­å·¥ä½œçš„é¢éƒ¨åŠ¨æ€å’Œå¤´éƒ¨è¿åŠ¨ç”Ÿæˆæ¨¡å‹ã€‚</li><li>å¼€å‘å‡ºè¡¨è¾¾ä¸°å¯Œä¸”ä¸çº ç¼ çš„äººè„¸æ½œåœ¨ç©ºé—´ã€‚</li><li>æ–¹æ³•åœ¨å„ä¸ªç»´åº¦ä¸Šæ˜æ˜¾ä¼˜äºä»¥å¾€æ–¹æ³•ã€‚</li><li>æ–¹æ³•ä¸ä»…æä¾›å…·æœ‰é€¼çœŸé¢éƒ¨å’Œå¤´éƒ¨åŠ¨æ€çš„é«˜è§†é¢‘è´¨é‡ï¼Œè¿˜æ”¯æŒä»¥é«˜è¾¾ 40 FPS çš„é€Ÿåº¦åœ¨çº¿ç”Ÿæˆ 512x512 è§†é¢‘ï¼Œä¸”å¯åŠ¨å»¶è¿Ÿå¯å¿½ç•¥ä¸è®¡ã€‚</li><li>ä¸ºä¸å…·æœ‰ç±»äººä¼šè¯è¡Œä¸ºçš„é€¼çœŸè™šæ‹Ÿå½¢è±¡è¿›è¡Œå®æ—¶äº’åŠ¨é“ºå¹³äº†é“è·¯ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šVASA-1ï¼šå®æ—¶ç”Ÿæˆé€¼çœŸçš„éŸ³é¢‘é©±åŠ¨è¯´è¯äººè„¸</li><li>ä½œè€…ï¼šSicheng Xuã€Guojun Chenã€Yu-Xiao Guoã€Jiaolong Yangã€Chong Liã€Zhenyu Zangã€Yizhong Zhangã€Xin Tongã€Baining Guo</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šå¾®è½¯äºšæ´²ç ”ç©¶é™¢</li><li>å…³é”®è¯ï¼šéŸ³é¢‘é©±åŠ¨è¯´è¯äººè„¸ã€è§†è§‰æƒ…æ„ŸæŠ€èƒ½ã€æ‰©æ•£æ¨¡å‹ã€äººè„¸æ½œåœ¨ç©ºé—´</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.10667</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šåœ¨å¤šåª’ä½“å’Œé€šä¿¡é¢†åŸŸï¼Œäººè„¸ä¸ä»…ä»…æ˜¯ä¸€å¼ é¢å­”ï¼Œè€Œæ˜¯ä¸€ä¸ªåŠ¨æ€çš„ç”»å¸ƒï¼Œå…¶ä¸Šçš„æ¯ä¸€ä¸ªç»†å¾®åŠ¨ä½œå’Œè¡¨æƒ…éƒ½å¯ä»¥è¡¨è¾¾æƒ…æ„Ÿã€ä¼ é€’æœªè¯´å‡ºå£çš„ä¿¡æ¯ï¼Œå¹¶ä¿ƒè¿›ç§»æƒ…è¿æ¥ã€‚äººå·¥æ™ºèƒ½ç”Ÿæˆè¯´è¯äººè„¸æŠ€æœ¯çš„å‡ºç°ä¸ºæœªæ¥æä¾›äº†ä¸€ä¸ªçª—å£ï¼Œåœ¨è¿™ä¸ªçª—å£ä¸­ï¼ŒæŠ€æœ¯å¯ä»¥æ”¾å¤§äººä¸äººä»¥åŠäººä¸äººå·¥æ™ºèƒ½äº¤äº’çš„ä¸°å¯Œæ€§ã€‚è¿™ç§æŠ€æœ¯æœ‰æœ›ä¸°å¯Œæ•°å­—é€šä¿¡ã€æé«˜äº¤æµéšœç¢è€…çš„å¯è®¿é—®æ€§ã€é€šè¿‡äº’åŠ¨å¼äººå·¥æ™ºèƒ½è¾…å¯¼æ”¹å˜æ•™è‚²æ–¹æ³•ï¼Œå¹¶åœ¨åŒ»ç–—ä¿å¥ä¸­æä¾›æ²»ç–—æ”¯æŒå’Œç¤¾äº¤äº’åŠ¨ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç°æœ‰çš„éŸ³é¢‘é©±åŠ¨è¯´è¯äººè„¸ç”Ÿæˆæ–¹æ³•åœ¨é€¼çœŸåº¦å’Œç”ŸåŠ¨æ€§æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚è¿™äº›æ–¹æ³•é€šå¸¸ä¾èµ–äºåŸºäºå…³é”®ç‚¹çš„å”‡å½¢åŒæ­¥å’ŒåŸºäºæ¨¡æ¿çš„é¢éƒ¨åŠ¨ç”»ï¼Œè¿™ä¼šå¯¼è‡´åƒµç¡¬å’Œä¸è‡ªç„¶çš„åŠ¨ä½œã€‚æ­¤å¤–ï¼Œå®ƒä»¬éš¾ä»¥æ•æ‰å¤´éƒ¨è¿åŠ¨å’Œå¾®å¦™çš„é¢éƒ¨è¡¨æƒ…ï¼Œè¿™äº›è¡¨æƒ…å¯¹äºæ„ŸçŸ¥çœŸå®æ€§å’Œç”ŸåŠ¨æ€§è‡³å…³é‡è¦ã€‚ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡º VASA-1ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå¯ä»¥ç”Ÿæˆé«˜åº¦é€¼çœŸå’Œç”ŸåŠ¨çš„éŸ³é¢‘é©±åŠ¨è¯´è¯äººè„¸ã€‚VASA-1 çš„æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š</li><li>åŸºäºæ‰©æ•£çš„æ•´ä½“é¢éƒ¨åŠ¨æ€å’Œå¤´éƒ¨è¿åŠ¨ç”Ÿæˆæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨äººè„¸æ½œåœ¨ç©ºé—´ä¸­å·¥ä½œã€‚</li><li>ä½¿ç”¨è§†é¢‘å¼€å‘äº†ä¸€ä¸ªè¡¨è¾¾æ€§å’Œåˆ†ç¦»çš„äººè„¸æ½œåœ¨ç©ºé—´ã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šåœ¨å¹¿æ³›çš„å®éªŒä¸­ï¼ŒåŒ…æ‹¬å¯¹ä¸€ç»„æ–°æŒ‡æ ‡çš„è¯„ä¼°ï¼ŒVASA-1 åœ¨å„ä¸ªæ–¹é¢éƒ½æ˜æ˜¾ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚å®ƒæä¾›äº†å…·æœ‰é€¼çœŸé¢éƒ¨å’Œå¤´éƒ¨åŠ¨æ€çš„é«˜è§†é¢‘è´¨é‡ï¼Œå¹¶ä¸”è¿˜æ”¯æŒä»¥é«˜è¾¾ 40 FPS åœ¨çº¿ç”Ÿæˆ 512Ã—512 è§†é¢‘ï¼Œä¸”å¯åŠ¨å»¶è¿Ÿå¯ä»¥å¿½ç•¥ä¸è®¡ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†å®æ—¶ä¸æ¨¡æ‹Ÿäººç±»ä¼šè¯è¡Œä¸ºçš„é€¼çœŸåŒ–èº«è¿›è¡Œäº¤äº’çš„ç›®æ ‡ã€‚</li></ol><p>7.æ–¹æ³•ï¼š(1)æ„å»ºäººè„¸æ½œåœ¨ç©ºé—´ï¼šåˆ©ç”¨æ— æ ‡æ³¨äººè„¸è§†é¢‘æ•°æ®é›†ï¼Œæ„å»ºå…·æœ‰é«˜åˆ†ç¦»åº¦å’Œè¡¨è¾¾èƒ½åŠ›çš„äººè„¸æ½œåœ¨ç©ºé—´ï¼Œå®ç°å¯¹äººè„¸å¤–è§‚å’ŒåŠ¨æ€çš„æœ‰æ•ˆç”Ÿæˆå»ºæ¨¡ã€‚(2)æ‰©æ•£å˜å‹å™¨ç”ŸæˆåŠ¨æ€ï¼šåˆ©ç”¨æ‰©æ•£æ¨¡å‹å’Œå˜å‹å™¨æ¶æ„ï¼Œæå‡ºå…¨é¢çš„é¢éƒ¨åŠ¨æ€ç”Ÿæˆæ¡†æ¶ï¼Œä»¥éŸ³é¢‘ä¸ºæ¡ä»¶ï¼Œç”Ÿæˆå¤´å’Œé¢éƒ¨è¿åŠ¨åºåˆ—ã€‚(3)æ— åˆ†ç±»å™¨å¼•å¯¼ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œéšæœºä¸¢å¼ƒè¾“å…¥æ¡ä»¶ï¼Œå¹¶åº”ç”¨æ— åˆ†ç±»å™¨å¼•å¯¼ï¼Œå¢å¼ºæ¨¡å‹å¯¹å„ç§æ¡ä»¶çš„é²æ£’æ€§ã€‚(4)äººè„¸è§†é¢‘ç”Ÿæˆï¼šåœ¨æ¨ç†æ—¶ï¼Œæå–è¾“å…¥äººè„¸å›¾åƒå’ŒéŸ³é¢‘ç‰¹å¾ï¼Œç”Ÿæˆå¤´å’Œé¢éƒ¨è¿åŠ¨åºåˆ—ï¼Œå¹¶ä½¿ç”¨è®­ç»ƒå¥½çš„è§£ç å™¨ç”Ÿæˆæœ€ç»ˆè§†é¢‘ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º VASA-1 çš„éŸ³é¢‘é©±åŠ¨è¯´è¯äººè„¸ç”Ÿæˆæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä»¥å…¶ä»å•å¼ å›¾åƒå’ŒéŸ³é¢‘è¾“å…¥ä¸­é«˜æ•ˆç”Ÿæˆé€¼çœŸçš„å”‡å½¢åŒæ­¥ã€ç”ŸåŠ¨çš„é¢éƒ¨è¡¨æƒ…å’Œè‡ªç„¶çš„å¤´éƒ¨åŠ¨ä½œè€Œè‘—ç§°ã€‚å®ƒåœ¨è§†é¢‘è´¨é‡å’Œæ€§èƒ½æ•ˆç‡æ–¹é¢æ˜æ˜¾ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨ç”Ÿæˆçš„è¯´è¯äººè„¸è§†é¢‘ä¸­å±•ç¤ºäº†æœ‰å‰æ™¯çš„è§†è§‰æƒ…æ„ŸæŠ€èƒ½ã€‚è¯¥æ¨¡å‹çš„æŠ€æœ¯åŸºçŸ³æ˜¯ä¸€ä¸ªåˆ›æ–°çš„æ•´ä½“é¢éƒ¨åŠ¨æ€å’Œå¤´éƒ¨åŠ¨ä½œç”Ÿæˆæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨å…·æœ‰è¡¨è¾¾æ€§å’Œåˆ†ç¦»åº¦çš„äººè„¸æ½œåœ¨ç©ºé—´ä¸­å·¥ä½œã€‚VASA-1 å–å¾—çš„è¿›æ­¥æœ‰å¯èƒ½é‡å¡‘å„ä¸ªé¢†åŸŸçš„äº¤äº’ï¼ŒåŒ…æ‹¬é€šä¿¡ã€æ•™è‚²å’ŒåŒ»ç–—ä¿å¥ã€‚å¯æ§æ¡ä»¶ä¿¡å·çš„é›†æˆè¿›ä¸€æ­¥å¢å¼ºäº†æ¨¡å‹å¯¹ä¸ªæ€§åŒ–ç”¨æˆ·ä½“éªŒçš„é€‚åº”æ€§ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹å’Œå˜å‹å™¨æ¶æ„çš„å…¨é¢é¢éƒ¨åŠ¨æ€ç”Ÿæˆæ¡†æ¶ï¼Œä»¥éŸ³é¢‘ä¸ºæ¡ä»¶ï¼Œç”Ÿæˆå¤´éƒ¨å’Œé¢éƒ¨è¿åŠ¨åºåˆ—ã€‚</li><li>æ„å»ºäº†ä¸€ä¸ªå…·æœ‰é«˜åˆ†ç¦»åº¦å’Œè¡¨è¾¾èƒ½åŠ›çš„äººè„¸æ½œåœ¨ç©ºé—´ï¼Œå®ç°äº†å¯¹äººè„¸å¤–è§‚å’ŒåŠ¨æ€çš„æœ‰æ•ˆç”Ÿæˆå»ºæ¨¡ã€‚</li><li>åº”ç”¨æ— åˆ†ç±»å™¨å¼•å¯¼ï¼Œå¢å¼ºäº†æ¨¡å‹å¯¹å„ç§æ¡ä»¶çš„é²æ£’æ€§ã€‚æ€§èƒ½ï¼š</li><li>åœ¨å¹¿æ³›çš„å®éªŒä¸­ï¼ŒåŒ…æ‹¬å¯¹ä¸€ç»„æ–°æŒ‡æ ‡çš„è¯„ä¼°ï¼ŒVASA-1 åœ¨å„ä¸ªæ–¹é¢éƒ½æ˜æ˜¾ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚</li><li>æä¾›å…·æœ‰é€¼çœŸé¢éƒ¨å’Œå¤´éƒ¨åŠ¨æ€çš„é«˜è§†é¢‘è´¨é‡ï¼Œå¹¶ä¸”è¿˜æ”¯æŒä»¥é«˜è¾¾ 40FPS åœ¨çº¿ç”Ÿæˆ 512Ã—512 è§†é¢‘ï¼Œä¸”å¯åŠ¨å»¶è¿Ÿå¯ä»¥å¿½ç•¥ä¸è®¡ã€‚å·¥ä½œé‡ï¼š</li><li>è¯¥æ–¹æ³•åœ¨æ¨ç†æ—¶ï¼Œæå–è¾“å…¥äººè„¸å›¾åƒå’ŒéŸ³é¢‘ç‰¹å¾ï¼Œç”Ÿæˆå¤´éƒ¨å’Œé¢éƒ¨è¿åŠ¨åºåˆ—ï¼Œå¹¶ä½¿ç”¨è®­ç»ƒå¥½çš„è§£ç å™¨ç”Ÿæˆæœ€ç»ˆè§†é¢‘ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-57afb9746460c539242f5be2406abcd8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4c78cc77ce02a94033d2c27026996d18.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f451991b54ed6b1770c282a53cf0f267.jpg" align="middle"></details><h2 id="THQA-A-Perceptual-Quality-Assessment-Database-for-Talking-Heads"><a href="#THQA-A-Perceptual-Quality-Assessment-Database-for-Talking-Heads" class="headerlink" title="THQA: A Perceptual Quality Assessment Database for Talking Heads"></a>THQA: A Perceptual Quality Assessment Database for Talking Heads</h2><p><strong>Authors:Yingjie Zhou, Zicheng Zhang, Wei Sun, Xiaohong Liu, Xiongkuo Min, Zhihua Wang, Xiao-Ping Zhang, Guangtao Zhai</strong></p><p>In the realm of media technology, digital humans have gained prominence due to rapid advancements in computer technology. However, the manual modeling and control required for the majority of digital humans pose significant obstacles to efficient development. The speech-driven methods offer a novel avenue for manipulating the mouth shape and expressions of digital humans. Despite the proliferation of driving methods, the quality of many generated talking head (TH) videos remains a concern, impacting user visual experiences. To tackle this issue, this paper introduces the Talking Head Quality Assessment (THQA) database, featuring 800 TH videos generated through 8 diverse speech-driven methods. Extensive experiments affirm the THQA databaseâ€™s richness in character and speech features. Subsequent subjective quality assessment experiments analyze correlations between scoring results and speech-driven methods, ages, and genders. In addition, experimental results show that mainstream image and video quality assessment methods have limitations for the THQA database, underscoring the imperative for further research to enhance TH video quality assessment. The THQA database is publicly accessible at <a href="https://github.com/zyj-2000/THQA">https://github.com/zyj-2000/THQA</a>. </p><p><a href="http://arxiv.org/abs/2404.09003v1">PDF</a> </p><p><strong>Summary</strong><br>è§†é¢‘é©±åŠ¨çš„æ•°å­—äººè¯´è¯å¤´éƒ¨è¯„ä¼°æ•°æ®åº“ï¼ˆTHQAï¼‰ä¸º8ç§ä¸åŒè¯­éŸ³é©±åŠ¨æ–¹æ³•ç”Ÿæˆçš„800ä¸ªè¯´è¯å¤´éƒ¨è§†é¢‘å»ºç«‹äº†åŸºå‡†ï¼Œä¿ƒè¿›äº†è¯´è¯å¤´éƒ¨è§†é¢‘è´¨é‡è¯„ä¼°çš„ç ”ç©¶ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>è¯´è¯å¤´éƒ¨è§†é¢‘é©±åŠ¨æ–¹æ³•å¤šæ ·ï¼Œè´¨é‡å‚å·®ä¸é½ï¼Œå½±å“ç”¨æˆ·è§†è§‰ä½“éªŒã€‚</li><li>THQA æ•°æ®åº“åŒ…å« 800 ä¸ªè¯´è¯å¤´éƒ¨è§†é¢‘ï¼Œæ¶µç›– 8 ç§è¯­éŸ³é©±åŠ¨æ–¹æ³•ã€ä¸åŒäººç‰©å’Œè¯­éŸ³ç‰¹å¾ã€‚</li><li>ä¸»è§‚è´¨é‡è¯„ä¼°å®éªŒåˆ†æäº†è¯„åˆ†ç»“æœä¸è¯­éŸ³é©±åŠ¨æ–¹æ³•ã€å¹´é¾„å’Œæ€§åˆ«ä¹‹é—´çš„ç›¸å…³æ€§ã€‚</li><li>ä¸»æµå›¾åƒå’Œè§†é¢‘è´¨é‡è¯„ä¼°æ–¹æ³•å¯¹ THQA æ•°æ®åº“æœ‰å±€é™æ€§ï¼Œéœ€è¦è¿›ä¸€æ­¥ç ”ç©¶æ”¹è¿› TH è§†é¢‘è´¨é‡è¯„ä¼°ã€‚</li><li>THQA æ•°æ®åº“å·²å…¬å¼€ï¼Œå¯ç”¨äºç ”ç©¶ã€‚</li><li>TH è§†é¢‘è´¨é‡è¯„ä¼°å¯¹äºæé«˜ç”¨æˆ·è§†è§‰ä½“éªŒè‡³å…³é‡è¦ã€‚</li><li>è¯­éŸ³é©±åŠ¨æ–¹æ³•çš„è¿›æ­¥å°†ä¿ƒè¿›æ•°å­—äººè¯´è¯å¤´éƒ¨æŠ€æœ¯çš„å¹¿æ³›åº”ç”¨ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>é¢˜ç›®ï¼šTHQAï¼šç”¨äºè¯´è¯äººå¤´åƒæ„ŸçŸ¥è´¨é‡è¯„ä¼°çš„æ•°æ®åº“</li><p></p><p></p><li>ä½œè€…ï¼šYingjie Zhou, Zicheng Zhang, Wei Sun, Xiaohong Liu, Xiongkuo Min, Zhihua Wang, Xiao-Ping Zhang, Guangtao Zhai</li><p></p><p></p><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šä¸Šæµ·äº¤é€šå¤§å­¦</li><p></p><p></p><li>å…³é”®è¯ï¼šæ•°å­—äººå¤´éƒ¨ã€è¯­éŸ³é©±åŠ¨æ–¹æ³•ã€è´¨é‡è¯„ä¼°æ•°æ®åº“ã€æ— å‚è€ƒã€å¤šåª’ä½“å¤„ç†</li><p></p><p></p><li>è®ºæ–‡é“¾æ¥ï¼šNone</li><p></p><p></p><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šæ•°å­—äººä½œä¸ºä¸€ç§æ–°å…´çš„æ•°å­—åª’ä½“æŠ€æœ¯ï¼Œåœ¨å¨±ä¹ã€åŒ»ç–—ã€å½±è§†ç­‰è¡Œä¸šå¾—åˆ°äº†å¹¿æ³›çš„åº”ç”¨ã€‚ç„¶è€Œï¼Œç›®å‰æ•°å­—äººçš„è®¾è®¡å’Œåˆ¶ä½œè¿‡ç¨‹ä»ç„¶ååˆ†ç¹çå’Œè€—æ—¶ï¼Œä¸»è¦ä¾èµ–äºç†Ÿç»ƒä¸“ä¸šäººå‘˜çš„æ‰‹å·¥æ“ä½œã€‚è¿™ç§æ‰‹å·¥è®¾è®¡æ–¹å¼æå¤§åœ°åˆ¶çº¦äº†æ•°å­—äººå†…å®¹åˆ¶ä½œçš„æ•ˆç‡ï¼Œå°¤å…¶æ˜¯åœ¨å¤´éƒ¨è®¾è®¡å’Œé©±åŠ¨æœºåˆ¶çš„å¤æ‚é¢†åŸŸã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•å’Œé—®é¢˜ï¼šä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„å‡ºç°å’Œæ™®åŠæä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œå¦‚å›¾ 1 æ‰€ç¤ºã€‚è™½ç„¶è¯­éŸ³é©±åŠ¨æ–¹æ³•å·²è¢«ç›¸ç»§æå‡ºï¼Œç®€åŒ–äº†æ•°å­—äººé¢éƒ¨è¡¨æƒ…å’ŒåŠ¨ä½œçš„è®¾è®¡ï¼Œä½†ä»ç„¶ç¼ºä¹ä¸“é—¨é’ˆå¯¹ AI ç”Ÿæˆçš„è¯´è¯äººå¤´åƒï¼ˆTHï¼‰è§†é¢‘çš„è´¨é‡è¯„ä¼°æŒ‡æ ‡ã€‚è¿™äº›è´¨é‡æŒ‡æ ‡ä¸ä»…å¯ä»¥æœ‰æ•ˆè¯„ä¼°è¯´è¯äººå¤´åƒï¼ˆTHï¼‰è§†é¢‘çš„è´¨é‡ï¼Œè¿˜å¯ä»¥é—´æ¥ä¿ƒè¿›è¯­éŸ³é©±åŠ¨æ–¹æ³•çš„è¿›ä¸€æ­¥å‘å±•ï¼Œä»è€Œä¸ºç”¨æˆ·æä¾›æ›´é«˜è´¨é‡çš„è§†è§‰ä½“éªŒã€‚é—æ†¾çš„æ˜¯ï¼Œç›®å‰è¯„ä¼°ç”Ÿæˆè¯´è¯äººå¤´åƒè§†é¢‘çš„ä¸»æµæ–¹æ³•ä»ç„¶éµå¾ªä¿ç•™ä¸åŸå§‹è§†é¢‘æ¯”è¾ƒçš„èŒƒå¼ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒFrÃ©chet æ„ŸçŸ¥è·ç¦»ï¼ˆFIDï¼‰å’Œä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆCSIMï¼‰ä»ç„¶æ˜¯ç”¨äºæ­¤ç±»è¯„ä¼°çš„ä¸»è¦æŒ‡æ ‡ã€‚ç„¶è€Œï¼Œè¿™äº›æŒ‡æ ‡çš„å±€é™æ€§è¡¨ç°åœ¨ä¸¤ä¸ªåŸºæœ¬æ–¹é¢ã€‚é¦–å…ˆï¼Œè¿™äº›å®¢è§‚è¯„ä¼°æŒ‡æ ‡ä»…å…³æ³¨å›¾åƒæˆ–è§†é¢‘ç›¸ä¼¼æ€§ï¼Œè€Œå¿½ç•¥äº†æ•´ä¸ªç”Ÿæˆå†…å®¹å¸¦ç»™è§‚çœ‹è€…ç”¨æˆ·çš„æ•´ä½“è§†è§‰ä½“éªŒã€‚å…¶æ¬¡ï¼Œå®ƒä»¬å¯¹åŸå§‹å‚è€ƒè§†é¢‘çš„ä¾èµ–æ€§æ„æˆäº†ä¸€ä¸ªå®è´¨æ€§çš„é™åˆ¶ï¼Œå› ä¸ºæœ€ç»ˆç”¨æˆ·æ— æ³•è·å¾—åŸå§‹å‚è€ƒè§†é¢‘ï¼Œä»è€Œä¸¥é‡é™åˆ¶äº†å®ƒä»¬çš„é€‚ç”¨æ€§ã€‚è™½ç„¶ CPBD å’Œ CGIQA ç­‰æŒ‡æ ‡å·²è¢«çº³å…¥ä¸€äº›æœ€è¿‘çš„å·¥ä½œä¸­ä»¥è¡¡é‡æ¨¡ç³Šçº§åˆ«å’Œç¾å­¦ç‰¹å¾ï¼Œä½†æ²¡æœ‰å¹¿æ³›ä½¿ç”¨çš„è¯„ä¼°æŒ‡æ ‡ä¸“é—¨é’ˆå¯¹ TH è§†é¢‘é‡èº«å®šåˆ¶ã€‚ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œé¦–å…ˆéœ€è¦å¼€å‘ä¸€ä¸ªå¯å…¬å¼€è®¿é—®çš„å¤§è§„æ¨¡ TH è§†é¢‘æ•°æ®åº“ã€‚å› æ­¤ï¼Œæœ¬æ–‡å°†é‡ç‚¹æ”¾åœ¨ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š</li><p></p><p></p><li>æå‡ºä¸€ä¸ªæ–°çš„è¯´è¯äººå¤´åƒè´¨é‡è¯„ä¼°æ•°æ®åº“ THQAï¼Œå…¶ä¸­åŒ…å« 800 ä¸ª TH è§†é¢‘ï¼Œè¿™äº›è§†é¢‘æ˜¯é€šè¿‡ 8 ç§ä¸åŒçš„è¯­éŸ³é©±åŠ¨æ–¹æ³•ç”Ÿæˆçš„ã€‚</li><p></p><p></p><li>å¹¿æ³›çš„å®éªŒéªŒè¯äº† THQA æ•°æ®åº“åœ¨è§’è‰²å’Œè¯­éŸ³ç‰¹å¾æ–¹é¢çš„ä¸°å¯Œæ€§ã€‚</li><p></p><p></p><li>åç»­çš„ä¸»è§‚è´¨é‡è¯„ä¼°å®éªŒåˆ†æäº†è¯„åˆ†ç»“æœä¸è¯­éŸ³é©±åŠ¨æ–¹æ³•ã€å¹´é¾„å’Œæ€§åˆ«ä¹‹é—´çš„ç›¸å…³æ€§ã€‚</li><p></p><p></p><li>æ­¤å¤–ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œä¸»æµå›¾åƒå’Œè§†é¢‘è´¨é‡è¯„ä¼°æ–¹æ³•å¯¹ THQA æ•°æ®åº“æœ‰å±€é™æ€§ï¼Œå¼ºè°ƒäº†è¿›ä¸€æ­¥ç ”ç©¶ä»¥å¢å¼º TH è§†é¢‘è´¨é‡è¯„ä¼°çš„å¿…è¦æ€§ã€‚</li><p></p><p></p><li><p></p><p>THQA æ•°æ®åº“å¯åœ¨ https://github.com/zyj-2000/THQA å…¬å¼€è·å–ã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†ä»€ä¹ˆæ€§èƒ½ï¼Ÿæ€§èƒ½æ˜¯å¦èƒ½æ”¯æ’‘å…¶ç›®æ ‡ï¼ŸTHQA æ•°æ®åº“çš„å»ºç«‹ä¸ºè¯´è¯äººå¤´åƒè§†é¢‘è´¨é‡è¯„ä¼°æä¾›äº†ä¸°å¯Œçš„èµ„æºï¼Œå¹¶ä¸ºè¿›ä¸€æ­¥ç ”ç©¶è¯´è¯äººå¤´åƒè§†é¢‘è´¨é‡å¢å¼ºå¥ å®šäº†åŸºç¡€ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1): æ„å»ºäº†ä¸€ä¸ªåŒ…å« 800 ä¸ª TH è§†é¢‘çš„å¤§å‹å…¬å¼€æ•°æ®é›† THQAï¼Œè¿™äº›è§†é¢‘ç”± 8 ç§ä¸åŒçš„è¯­éŸ³é©±åŠ¨æ–¹æ³•ç”Ÿæˆï¼›(2): é€šè¿‡ä¸»è§‚è´¨é‡è¯„ä¼°å®éªŒï¼Œåˆ†æäº†è¯„åˆ†ç»“æœä¸è¯­éŸ³é©±åŠ¨æ–¹æ³•ã€å¹´é¾„å’Œæ€§åˆ«ä¹‹é—´çš„ç›¸å…³æ€§ï¼›(3): éªŒè¯äº† THQA æ•°æ®åº“åœ¨è§’è‰²å’Œè¯­éŸ³ç‰¹å¾æ–¹é¢çš„ä¸°å¯Œæ€§ï¼›(4): å®éªŒç»“æœè¡¨æ˜ï¼Œä¸»æµå›¾åƒå’Œè§†é¢‘è´¨é‡è¯„ä¼°æ–¹æ³•å¯¹ THQA æ•°æ®åº“æœ‰å±€é™æ€§ï¼Œå¼ºè°ƒäº†è¿›ä¸€æ­¥ç ”ç©¶ä»¥å¢å¼º TH è§†é¢‘è´¨é‡è¯„ä¼°çš„å¿…è¦æ€§ã€‚</p></li></ol><p><strong>8.ç»“è®º</strong>(1) æœ¬å·¥ä½œçš„é‡è¦æ„ä¹‰ï¼šæœ¬å·¥ä½œæ„å»ºäº†ä¸€ä¸ªåä¸º THQA çš„è¯´è¯äººå¤´åƒï¼ˆTHï¼‰è§†é¢‘è´¨é‡è¯„ä¼°æ•°æ®åº“ã€‚è¯¥æ•°æ®åº“åŒ…å«é€šè¿‡ 8 ç§ä¸åŒçš„è¯­éŸ³é©±åŠ¨æ–¹æ³•ç”Ÿæˆçš„ 800 ä¸ª TH è§†é¢‘ã€‚æˆ‘ä»¬çš„åˆ†ææ¶‰åŠå¯¹æ”¶é›†çš„å›¾åƒã€è¯­éŸ³æ•°æ®å’Œç”Ÿæˆçš„è§†é¢‘çš„å½»åº•æ£€æŸ¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿›è¡Œä¸»è§‚è¯„åˆ†å®éªŒä»¥éªŒè¯ THQA çš„ä»£è¡¨æ€§ï¼Œè‚¯å®šå…¶ä½œä¸º TH è§†é¢‘è´¨é‡è¯„ä¼°æŒ‡å¯¼æ¡†æ¶çš„æ•ˆç”¨ã€‚æœ€åï¼Œæˆ‘ä»¬åŸºäº THQA æ•°æ®åº“å¯¹å„ç§ä¸»æµè¯„ä¼°æ–¹æ³•çš„æ€§èƒ½è¿›è¡Œæ¯”è¾ƒè¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œå¤§å¤šæ•°ç°æœ‰çš„è¯„ä¼°æ–¹æ³•åœ¨æœ‰æ•ˆè¯„ä¼° TH è§†é¢‘è´¨é‡æ–¹é¢è¡¨ç°å‡ºå±€é™æ€§ã€‚</p><p>(2) æœ¬æ–‡ä¼˜ç¼ºç‚¹æ€»ç»“ï¼ˆä¸‰ä¸ªç»´åº¦ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š* æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„è¯´è¯äººå¤´åƒè§†é¢‘è´¨é‡è¯„ä¼°æ•°æ®åº“ THQAã€‚* åˆ†æäº† TH è§†é¢‘çš„ä¸°å¯Œæ€§ï¼ŒåŒ…æ‹¬è§’è‰²å’Œè¯­éŸ³ç‰¹å¾ã€‚* æ¢ç´¢äº†ä¸»æµå›¾åƒå’Œè§†é¢‘è´¨é‡è¯„ä¼°æ–¹æ³•åœ¨è¯„ä¼° TH è§†é¢‘è´¨é‡æ–¹é¢çš„å±€é™æ€§ã€‚</p><p>æ€§èƒ½ï¼š* THQA æ•°æ®åº“ä¸ºè¯´è¯äººå¤´åƒè§†é¢‘è´¨é‡è¯„ä¼°æä¾›äº†ä¸°å¯Œçš„èµ„æºã€‚* ä¸»è§‚è´¨é‡è¯„ä¼°å®éªŒéªŒè¯äº† THQA çš„ä»£è¡¨æ€§ã€‚* æ¯”è¾ƒè¯„ä¼°è¡¨æ˜ï¼Œç°æœ‰çš„è¯„ä¼°æ–¹æ³•åœ¨è¯„ä¼° TH è§†é¢‘è´¨é‡æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</p><p>å·¥ä½œé‡ï¼š* æ„å»º THQA æ•°æ®åº“æ¶‰åŠæ”¶é›†å’Œå¤„ç†å¤§é‡æ•°æ®ã€‚* ä¸»è§‚è´¨é‡è¯„ä¼°å®éªŒéœ€è¦å¤§é‡çš„äººåŠ›èµ„æºã€‚* æ¢ç´¢ä¸»æµè¯„ä¼°æ–¹æ³•çš„å±€é™æ€§éœ€è¦è¿›è¡Œå¹¿æ³›çš„å®éªŒã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-1f6bf6d7bad9eaf02e82acd303b468f2.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e543b10e4a34e8d4e06d3f29d16a43fb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8610a595c8734930ae6c9ef9d82979cc.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-beef350bacef5e83341d2b9912c3cd5f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a3dad9d09b0691502f22ef81f9dd0bbc.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-04-17  VASA-1 Lifelike Audio-Driven Talking Faces Generated in Real Time</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/04/17/Paper/2024-04-17/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/04/17/Paper/2024-04-17/Diffusion%20Models/</id>
    <published>2024-04-17T06:02:48.000Z</published>
    <updated>2024-04-17T06:02:48.569Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-04-17-æ›´æ–°"><a href="#2024-04-17-æ›´æ–°" class="headerlink" title="2024-04-17 æ›´æ–°"></a>2024-04-17 æ›´æ–°</h1><h2 id="RefFusion-Reference-Adapted-Diffusion-Models-for-3D-Scene-Inpainting"><a href="#RefFusion-Reference-Adapted-Diffusion-Models-for-3D-Scene-Inpainting" class="headerlink" title="RefFusion: Reference Adapted Diffusion Models for 3D Scene Inpainting"></a>RefFusion: Reference Adapted Diffusion Models for 3D Scene Inpainting</h2><p><strong>Authors:Ashkan Mirzaei, Riccardo De Lutio, Seung Wook Kim, David Acuna, Jonathan Kelly, Sanja Fidler, Igor Gilitschenski, Zan Gojcic</strong></p><p>Neural reconstruction approaches are rapidly emerging as the preferred representation for 3D scenes, but their limited editability is still posing a challenge. In this work, we propose an approach for 3D scene inpainting â€” the task of coherently replacing parts of the reconstructed scene with desired content. Scene inpainting is an inherently ill-posed task as there exist many solutions that plausibly replace the missing content. A good inpainting method should therefore not only enable high-quality synthesis but also a high degree of control. Based on this observation, we focus on enabling explicit control over the inpainted content and leverage a reference image as an efficient means to achieve this goal. Specifically, we introduce RefFusion, a novel 3D inpainting method based on a multi-scale personalization of an image inpainting diffusion model to the given reference view. The personalization effectively adapts the prior distribution to the target scene, resulting in a lower variance of score distillation objective and hence significantly sharper details. Our framework achieves state-of-the-art results for object removal while maintaining high controllability. We further demonstrate the generality of our formulation on other downstream tasks such as object insertion, scene outpainting, and sparse view reconstruction. </p><p><a href="http://arxiv.org/abs/2404.10765v1">PDF</a> Project page: <a href="https://reffusion.github.io">https://reffusion.github.io</a></p><p><strong>Summary</strong><br>åŸºäºå¤šå°ºåº¦ä¸ªæ€§åŒ–çš„å›¾åƒä¿®å¤æ‰©æ•£æ¨¡å‹ï¼Œæå‡ºäº†RefFusionï¼Œä¸€ç§3Dåœºæ™¯ä¿®å¤æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯é€šè¿‡å‚è€ƒå›¾åƒå¯¹å†…å®¹è¿›è¡Œæ˜ç¡®æ§åˆ¶ï¼Œä»è€Œå®ç°é«˜è´¨é‡åˆæˆã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>3Dåœºæ™¯ä¿®å¤ä»ç„¶é¢ä¸´å¯ç¼–è¾‘æ€§æœ‰é™çš„æŒ‘æˆ˜ã€‚</li><li>åœºæ™¯ä¿®å¤æ˜¯ä¸€ä¸ªå›ºæœ‰çš„ç—…æ€ä»»åŠ¡ï¼Œå³å­˜åœ¨è®¸å¤šå¯ä¿¡åœ°æ›¿ä»£ç¼ºå¤±å†…å®¹çš„è§£å†³æ–¹æ¡ˆã€‚</li><li>å¥½çš„ä¿®å¤æ–¹æ³•ä¸ä»…è¦å®ç°é«˜è´¨é‡çš„åˆæˆï¼Œè¿˜è¦å…·æœ‰é«˜åº¦çš„å¯æ§æ€§ã€‚</li><li>RefFusionæ˜¯ä¸€ç§åŸºäºå¤šå°ºåº¦ä¸ªæ€§åŒ–çš„å›¾åƒä¿®å¤æ‰©æ•£æ¨¡å‹ï¼Œå¯ä»¥å¯¹ç»™å®šçš„å‚è€ƒè§†å›¾è¿›è¡Œ3Dä¿®å¤ã€‚</li><li>ä¸ªæ€§åŒ–æœ‰æ•ˆåœ°å°†å…ˆéªŒåˆ†å¸ƒé€‚åº”åˆ°ç›®æ ‡åœºæ™¯ï¼Œä»è€Œé™ä½äº†è¯„åˆ†è’¸é¦ç›®æ ‡çš„æ–¹å·®ï¼Œå› æ­¤äº§ç”Ÿäº†æ˜æ˜¾æ›´æ¸…æ™°çš„ç»†èŠ‚ã€‚</li><li>RefFusionæ¡†æ¶åœ¨å¯¹è±¡ç§»é™¤æ–¹é¢å®ç°äº†æœ€å…ˆè¿›çš„ç»“æœï¼ŒåŒæ—¶ä¿æŒäº†é«˜åº¦çš„å¯æ§æ€§ã€‚</li><li>RefFusionçš„å…¬å¼åœ¨å¯¹è±¡æ’å…¥ã€åœºæ™¯å¤–ç”»å’Œç¨€ç–è§†å›¾é‡å»ºç­‰å…¶ä»–ä¸‹æ¸¸ä»»åŠ¡ä¸Šä¹Ÿå…·æœ‰æ™®éæ€§ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>1.æ ‡é¢˜ï¼šRefFusionï¼šç”¨äº 3D åœºæ™¯ä¿®å¤çš„å‚è€ƒè‡ªé€‚åº”æ‰©æ•£æ¨¡å‹2.ä½œè€…ï¼šAshkan Mirzaei, Riccardo de Lutio, SeungWook Kim, David Acuna, Jonathan Kelly, Sanja Fidler, Igor Gilitschenski, Zan Gojcic3.ç¬¬ä¸€ä½œè€…å•ä½ï¼šNVIDIAï¼Œå¤šä¼¦å¤šå¤§å­¦4.å…³é”®è¯ï¼š3D åœºæ™¯ä¿®å¤ã€æ‰©æ•£æ¨¡å‹ã€å‚è€ƒè‡ªé€‚åº”5.è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.10765Github é“¾æ¥ï¼šæ— 6.æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»é‡å»ºæ–¹æ³•èƒ½å¤Ÿä»ä¸€ç»„å§¿æ€å›¾åƒä¸­æ— ç¼é‡å»º 3D åœºæ™¯ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„å¯ç¼–è¾‘æ€§ä»ç„¶æœ‰é™ï¼Œè€Œ 3D åœºæ™¯ä¿®å¤æ˜¯ä¸€ç§å…³é”®çš„å¯ç¼–è¾‘æ“ä½œï¼Œæ¶‰åŠåˆæˆåˆç†çš„å†…å®¹ï¼Œä»¥ä¾¿ä»ä»»ä½•è§’åº¦è§‚çœ‹æ—¶éƒ½èƒ½ä¸åœºæ™¯çš„å…¶ä½™éƒ¨åˆ†èä¸ºä¸€ä½“ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•ï¼šè¿‡å»çš„æ–¹æ³•é€šå¸¸ä½¿ç”¨ç¥ç»ç½‘ç»œæ¥ç”Ÿæˆä¿®å¤å†…å®¹ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸ç¼ºä¹å¯¹ä¿®å¤å†…å®¹çš„æ˜¾å¼æ§åˆ¶ï¼Œå¹¶ä¸”å¯èƒ½äº§ç”Ÿæ¨¡ç³Šæˆ–ä¸è¿è´¯çš„ç»“æœã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º RefFusion çš„ 3D åœºæ™¯ä¿®å¤æ–¹æ³•ã€‚RefFusion åŸºäºå›¾åƒä¿®å¤æ‰©æ•£æ¨¡å‹çš„å¤šå°ºåº¦ä¸ªæ€§åŒ–ï¼Œå¯ä»¥å°†ç»™å®šçš„å‚è€ƒè§†å›¾ä¸­çš„å…ˆéªŒåˆ†å¸ƒæœ‰æ•ˆåœ°é€‚åº”ç›®æ ‡åœºæ™¯ã€‚è¿™ç§ä¸ªæ€§åŒ–é™ä½äº†åˆ†æ•°è’¸é¦ç›®æ ‡å‡½æ•°çš„æ–¹å·®ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†ç»†èŠ‚çš„æ¸…æ™°åº¦ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šRefFusion åœ¨å¯¹è±¡ç§»é™¤ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„ç»“æœï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒé«˜çš„å¯æ§æ€§ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨å…¶ä»–ä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚å¯¹è±¡æ’å…¥ã€åœºæ™¯å¤–å»¶å’Œç¨€ç–è§†å›¾é‡å»ºï¼‰ä¸Šçš„é€šç”¨æ€§ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒRefFusion èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆæˆé«˜è´¨é‡ä¸”å¯æ§çš„ä¿®å¤å†…å®¹ï¼Œä»è€Œæ»¡è¶³å„ç§ 3D åœºæ™¯ç¼–è¾‘éœ€æ±‚ã€‚</p><ol><li><p>æ–¹æ³•ï¼š(1) <strong>RefFusion</strong>ï¼šæœ¬æ–‡æå‡ºçš„3Dåœºæ™¯ä¿®å¤æ–¹æ³•ï¼ŒåŸºäºå›¾åƒä¿®å¤æ‰©æ•£æ¨¡å‹çš„å¤šå°ºåº¦ä¸ªæ€§åŒ–ï¼Œå¯ä»¥å°†ç»™å®šçš„å‚è€ƒè§†å›¾ä¸­çš„å…ˆéªŒåˆ†å¸ƒæœ‰æ•ˆåœ°é€‚åº”ç›®æ ‡åœºæ™¯ã€‚(2) <strong>å¤šå°ºåº¦ä¸ªæ€§åŒ–</strong>ï¼šRefFusionåœ¨æ‰©æ•£æ¨¡å‹çš„ä¸åŒå°ºåº¦ä¸Šå¯¹å‚è€ƒè§†å›¾è¿›è¡Œä¸ªæ€§åŒ–ï¼Œä»è€Œä½¿æ¨¡å‹èƒ½å¤Ÿæ•è·ä¸åŒå°ºåº¦ä¸Šçš„ç»†èŠ‚å’Œç»“æ„ã€‚(3) <strong>åˆ†æ•°è’¸é¦ç›®æ ‡å‡½æ•°</strong>ï¼šRefFusionä½¿ç”¨åˆ†æ•°è’¸é¦ç›®æ ‡å‡½æ•°æ¥åŒ¹é…ä¿®å¤å†…å®¹å’Œå‚è€ƒè§†å›¾çš„å…ˆéªŒåˆ†å¸ƒã€‚é€šè¿‡ä¸ªæ€§åŒ–é™ä½ç›®æ ‡å‡½æ•°çš„æ–¹å·®ï¼Œæé«˜äº†ä¿®å¤å†…å®¹çš„ç»†èŠ‚æ¸…æ™°åº¦ã€‚</p></li><li><p>ç»“è®ºï¼š(1): RefFusion æå‡ºäº†ä¸€ç§åŸºäºå›¾åƒä¿®å¤æ‰©æ•£æ¨¡å‹çš„å¤šå°ºåº¦ä¸ªæ€§åŒ–æ–¹æ³•ï¼Œç”¨äº 3D åœºæ™¯ä¿®å¤ï¼Œæ˜¾è‘—æé«˜äº†ä¿®å¤å†…å®¹çš„ç»†èŠ‚æ¸…æ™°åº¦å’Œå¯æ§æ€§ï¼Œåœ¨å¯¹è±¡ç§»é™¤ç­‰ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚(2): åˆ›æ–°ç‚¹ï¼š</p><ul><li>å¤šå°ºåº¦ä¸ªæ€§åŒ–ï¼šåœ¨æ‰©æ•£æ¨¡å‹çš„ä¸åŒå°ºåº¦ä¸Šå¯¹å‚è€ƒè§†å›¾è¿›è¡Œä¸ªæ€§åŒ–ï¼Œæ•è·ä¸åŒå°ºåº¦ä¸Šçš„ç»†èŠ‚å’Œç»“æ„ã€‚</li><li>åˆ†æ•°è’¸é¦ç›®æ ‡å‡½æ•°ï¼šä½¿ç”¨åˆ†æ•°è’¸é¦ç›®æ ‡å‡½æ•°åŒ¹é…ä¿®å¤å†…å®¹å’Œå‚è€ƒè§†å›¾çš„å…ˆéªŒåˆ†å¸ƒï¼Œé™ä½ç›®æ ‡å‡½æ•°çš„æ–¹å·®ï¼Œæé«˜ä¿®å¤å†…å®¹çš„ç»†èŠ‚æ¸…æ™°åº¦ã€‚æ€§èƒ½ï¼š</li><li>åœ¨å¯¹è±¡ç§»é™¤ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„ç»“æœã€‚</li><li>åœ¨å¯¹è±¡æ’å…¥ã€åœºæ™¯å¤–å»¶å’Œç¨€ç–è§†å›¾é‡å»ºç­‰ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¡¨ç°å‡ºé€šç”¨æ€§ã€‚å·¥ä½œé‡ï¼š</li><li>è®­ç»ƒ RefFusion æ¨¡å‹éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºã€‚</li><li>RefFusion çš„æ¨ç†é€Ÿåº¦å¯èƒ½ä¼šå—åˆ°æ¨¡å‹å¤æ‚åº¦çš„é™åˆ¶ã€‚</li></ul></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="http://article.biliimg.com/bfs/new_dyn/02817604a88632e7e3ea4560f26f9bac241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/c106f428df739a7142772c42a95151c1241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/15ab37f03412f6ff0a7f34d5503212ec241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/542185dc6584d7e97ea791d916e54a04241286257.jpg" align="middle"></details>## LaDiC: Are Diffusion Models Really Inferior to Autoregressive   Counterparts for Image-to-Text Generation?**Authors:Yuchi Wang, Shuhuai Ren, Rundong Gao, Linli Yao, Qingyan Guo, Kaikai An, Jianhong Bai, Xu Sun**Diffusion models have exhibited remarkable capabilities in text-to-image generation. However, their performance in image-to-text generation, specifically image captioning, has lagged behind Auto-Regressive (AR) models, casting doubt on their applicability for such tasks. In this work, we revisit diffusion models, highlighting their capacity for holistic context modeling and parallel decoding. With these benefits, diffusion models can alleviate the inherent limitations of AR methods, including their slow inference speed, error propagation, and unidirectional constraints. Furthermore, we identify the prior underperformance of diffusion models stemming from the absence of an effective latent space for image-text alignment, and the discrepancy between continuous diffusion processes and discrete textual data. In response, we introduce a novel architecture, LaDiC, which utilizes a split BERT to create a dedicated latent space for captions and integrates a regularization module to manage varying text lengths. Our framework also includes a diffuser for semantic image-to-text conversion and a Back&amp;Refine technique to enhance token interactivity during inference. LaDiC achieves state-of-the-art performance for diffusion-based methods on the MS COCO dataset with 38.2 BLEU@4 and 126.2 CIDEr, demonstrating exceptional performance without pre-training or ancillary modules. This indicates strong competitiveness with AR models, revealing the previously untapped potential of diffusion models in image-to-text generation. [PDF](http://arxiv.org/abs/2404.10763v1) **Summary**æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œç°åœ¨é€šè¿‡LaDiCæ¶æ„åœ¨å›¾åƒåˆ°æ–‡æœ¬ç”Ÿæˆæ–¹é¢ä¹Ÿå–å¾—äº†çªç ´æ€§çš„è¿›å±•ã€‚**Key Takeaways**- æ‰©æ•£æ¨¡å‹å…·æœ‰æ•´ä½“ä¸Šä¸‹æ–‡å»ºæ¨¡å’Œå¹¶è¡Œè§£ç èƒ½åŠ›ã€‚- AR æ–¹æ³•å­˜åœ¨æ¨ç†é€Ÿåº¦æ…¢ã€è¯¯å·®ä¼ æ’­å’Œå•å‘çº¦æŸç­‰å›ºæœ‰ç¼ºé™·ã€‚- ç¼ºä¹æœ‰æ•ˆçš„å›¾åƒæ–‡æœ¬å¯¹é½æ½œåœ¨ç©ºé—´å’Œè¿ç»­æ‰©æ•£è¿‡ç¨‹ä¸ç¦»æ•£æ–‡æœ¬æ•°æ®ä¹‹é—´çš„å·®å¼‚æ˜¯æ‰©æ•£æ¨¡å‹å…ˆå‰è¡¨ç°ä¸ä½³çš„åŸå› ã€‚- LaDiC æ¶æ„ä½¿ç”¨ split BERT åˆ›å»ºäº†ä¸“é—¨çš„æ ‡é¢˜æ½œåœ¨ç©ºé—´ï¼Œå¹¶é›†æˆäº†ä¸€ä¸ªæ­£åˆ™åŒ–æ¨¡å—æ¥ç®¡ç†ä¸åŒçš„æ–‡æœ¬é•¿åº¦ã€‚- LaDiC åŒ…æ‹¬ä¸€ä¸ªç”¨äºè¯­ä¹‰å›¾åƒåˆ°æ–‡æœ¬è½¬æ¢çš„æ‰©æ•£å™¨ï¼Œä»¥åŠä¸€ç§åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¢å¼ºæ ‡è®°äº¤äº’æ€§çš„ Back&amp;Refine æŠ€æœ¯ã€‚- åœ¨ MS COCO æ•°æ®é›†ä¸Šï¼ŒLaDiC å®ç°äº†åŸºäºæ‰©æ•£æ–¹æ³•çš„æœ€æ–°æ€§èƒ½ï¼ŒBLEU@4 ä¸º 38.2ï¼ŒCIDEr ä¸º 126.2ï¼Œåœ¨æ²¡æœ‰é¢„è®­ç»ƒæˆ–è¾…åŠ©æ¨¡å—çš„æƒ…å†µä¸‹è¡¨ç°å‡ºè‰²ã€‚- è¿™è¡¨æ˜æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒåˆ°æ–‡æœ¬ç”Ÿæˆæ–¹é¢å…·æœ‰ä¸ AR æ¨¡å‹ç›¸å½“çš„ç«äº‰åŠ›ï¼Œæ­ç¤ºäº†å…¶åœ¨è¯¥é¢†åŸŸå°šæœªå¼€å‘çš„æ½œåŠ›ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>æ ‡é¢˜ï¼šå›¾åƒæ®µè½æè¿°ï¼šç”Ÿæˆè¿è´¯ä¸”ä¿¡æ¯ä¸°å¯Œçš„å›¾åƒæè¿°</li><li>ä½œè€…ï¼šJonathan Krause, Justin Johnson, Ranjay Krishna, Li Fei-Fei</li><li>éš¶å±å…³ç³»ï¼šæ–¯å¦ç¦å¤§å­¦</li><li>å…³é”®è¯ï¼šå›¾åƒæè¿°ï¼Œæ®µè½ç”Ÿæˆï¼Œå¤šæ¨¡æ€å­¦ä¹ ï¼Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼Œè®¡ç®—æœºè§†è§‰</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/pdf/1611.06607.pdfï¼ŒGithub é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼š(1)ï¼šç ”ç©¶èƒŒæ™¯ï¼šå›¾åƒæè¿°ä»»åŠ¡é€šå¸¸ç”Ÿæˆå•ä¸ªå¥å­æ¥æè¿°å›¾åƒï¼Œä½†è¿™ç§æè¿°è¿‡äºç²—ç•¥ï¼Œæ— æ³•æ•æ‰å›¾åƒçš„ä¸°å¯Œç»†èŠ‚ã€‚å¯†é›†æè¿°ä»»åŠ¡å¯ä»¥æè¿°å›¾åƒä¸­çš„å¤šä¸ªåŒºåŸŸï¼Œä½†ç”Ÿæˆçš„æè¿°ç¼ºä¹è¿è´¯æ€§ã€‚(2)ï¼šè¿‡å»çš„æ–¹æ³•ï¼šä¼ ç»Ÿå›¾åƒæè¿°æ–¹æ³•ç”Ÿæˆå•ä¸ªå¥å­ï¼Œè€Œå¯†é›†æè¿°æ–¹æ³•ç”Ÿæˆå¤šä¸ªçŸ­è¯­ï¼Œä½†éƒ½å­˜åœ¨ä¿¡æ¯ä¸è¶³æˆ–ç¼ºä¹è¿è´¯æ€§çš„é—®é¢˜ã€‚(3)ï¼šç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§å›¾åƒæ®µè½æè¿°æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å°†å›¾åƒåˆ†è§£ä¸ºè¯­ä¹‰åŒºåŸŸï¼Œå¹¶ä½¿ç”¨åˆ†å±‚å¾ªç¯ç¥ç»ç½‘ç»œå¯¹è¯­è¨€è¿›è¡Œæ¨ç†ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œæ¨¡å‹å¯ä»¥ç”Ÿæˆè¿è´¯ä¸”ä¿¡æ¯ä¸°å¯Œçš„æ®µè½ï¼Œè¯¦ç»†æè¿°å›¾åƒä¸­çš„å†…å®¹ã€‚(4)ï¼šæ–¹æ³•æ€§èƒ½ï¼šåœ¨å›¾åƒæ®µè½æè¿°æ•°æ®é›†ä¸Šï¼Œè¯¥æ¨¡å‹åœ¨ BLEU-4ã€METEOR å’Œ ROUGE-L ç­‰æŒ‡æ ‡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶ç”Ÿæˆè¿è´¯ä¸”ä¿¡æ¯ä¸°å¯Œçš„å›¾åƒæè¿°çš„èƒ½åŠ›ï¼Œæ»¡è¶³äº†ç ”ç©¶ç›®æ ‡ã€‚</li></ol><p>7.æ–¹æ³•ï¼š(1) åˆ©ç”¨æ–‡æœ¬ç¼–ç å™¨å°†ç¦»æ•£æ–‡æœ¬ç©ºé—´ C è½¬æ¢ä¸ºè¿ç»­æ–‡æœ¬æ½œåœ¨ç©ºé—´ Xï¼›(2) è®­ç»ƒæ‰©æ•£å™¨ä»¥åœ¨å›¾åƒç©ºé—´ V å’Œæ–‡æœ¬ç©ºé—´ X ä¹‹é—´æ¶èµ·æ¡¥æ¢ï¼›(3) æ–‡æœ¬è§£ç å™¨å°†æ–‡æœ¬æ½œåœ¨ç æ˜ å°„å›ç¦»æ•£æ–‡æœ¬ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼š æœ¬æ–‡ç ”ç©¶äº†åŸºäºæ‰©æ•£çš„å›¾åƒåˆ°æ–‡æœ¬èŒƒå¼ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„æ¶æ„ï¼Œç§°ä¸º LaDiCã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨åŸºäºæ‰©æ•£çš„æ–¹æ³•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶å±•ç¤ºäº†ä¸ä¸€äº›é¢„è®­ç»ƒçš„ AR æ¨¡å‹ç›¸å½“çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¹¿æ³›çš„å®éªŒæ­ç¤ºäº†æ‰©æ•£æ¨¡å‹åœ¨è€ƒè™‘æ›´å¤šæ•´ä½“ä¸Šä¸‹æ–‡å’Œå¹¶è¡Œå‘å‡ºæ‰€æœ‰æ ‡è®°æ–¹é¢ä¼˜äº AR æ¨¡å‹çš„ä»¤äººå…´å¥‹çš„ä¼˜åŠ¿ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è®¤ä¸ºæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒåˆ°æ–‡æœ¬ç”Ÿæˆæ–¹é¢å…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œæˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„å·¥ä½œå°†ä¸ºè¯¥é¢†åŸŸçš„è¿™ä¸€é¢†åŸŸå¼€è¾Ÿæ–°çš„å¯èƒ½æ€§ã€‚ï¼ˆ2ï¼‰ï¼š åˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§æ–°é¢–çš„åŸºäºæ‰©æ•£çš„å›¾åƒåˆ°æ–‡æœ¬ç”Ÿæˆæ¶æ„ LaDiCï¼Œè¯¥æ¶æ„åœ¨å›¾åƒç©ºé—´å’Œæ–‡æœ¬ç©ºé—´ä¹‹é—´å»ºç«‹äº†æ¡¥æ¢ï¼Œå¹¶åˆ©ç”¨åˆ†å±‚å¾ªç¯ç¥ç»ç½‘ç»œè¿›è¡Œè¯­è¨€æ¨ç†ï¼Œä»è€Œç”Ÿæˆè¿è´¯ä¸”ä¿¡æ¯ä¸°å¯Œçš„å›¾åƒæè¿°ã€‚æ€§èƒ½ï¼šåœ¨å›¾åƒæ®µè½æè¿°æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨ BLEU-4ã€METEOR å’Œ ROUGE-L ç­‰æŒ‡æ ‡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶ç”Ÿæˆè¿è´¯ä¸”ä¿¡æ¯ä¸°å¯Œçš„å›¾åƒæè¿°çš„èƒ½åŠ›ï¼Œæ»¡è¶³äº†ç ”ç©¶ç›®æ ‡ã€‚å·¥ä½œé‡ï¼šæœ¬æ–‡ä¸»è¦é›†ä¸­åœ¨å›¾åƒåˆ°æ–‡æœ¬ç”Ÿæˆçš„ä¸»è¦ç ”ç©¶è¯¾é¢˜ä¸Šï¼Œä»¥ä¿æŒç®€æ´æ€§å’Œé‡ç‚¹ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°æˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥å¾ˆå®¹æ˜“åœ°é€‚åº”å…¶ä»–æ¨¡æ€ç”šè‡³çº¯æ–‡æœ¬ç”Ÿæˆï¼Œè€Œåªéœ€è¿›è¡Œæœ€å°çš„ä¿®æ”¹ã€‚æˆ‘ä»¬å°†è¿™äº›æ½œåœ¨çš„æ‰©å±•ç•™ç»™æœªæ¥çš„å·¥ä½œï¼ŒåŒæ—¶ï¼Œæˆ‘ä»¬å¸Œæœ›æœ¬æ–‡å°†æ¿€å‘ç ”ç©¶äººå‘˜ä½¿ç”¨æ‰©æ•£æ¨¡å‹ä»äº‹ä»¥æ–‡æœ¬ä¸ºä¸­å¿ƒçš„æ¨¡æ€ç”Ÿæˆä»»åŠ¡çš„ä¿¡å¿ƒï¼Œå¹¶æœŸå¾…åœ¨è¿™ä¸ªé¢†åŸŸæœªæ¥çš„ç²¾å½©ä½œå“ã€‚æ­¤å¤–ï¼Œç”±äºèµ„æºé™åˆ¶ï¼Œæˆ‘ä»¬ç ”ç©¶ä¸­ä½¿ç”¨çš„æ¨¡å‹å‚æ•°å’Œæ•°æ®é›†å¹¶ä¸å¹¿æ³›ã€‚è€ƒè™‘åˆ°åƒ GPT è¿™æ ·çš„è‡ªå›å½’æ¨¡å‹åœ¨æ”¾å¤§æ—¶è¡¨ç°å‡ºçš„æ˜¾ç€çš„ç´§æ€¥èƒ½åŠ›ï¼Œæ¢ç´¢æˆ‘ä»¬çš„æ¨¡å‹æˆ–ä¸€èˆ¬çš„æ‰©æ•£æ¨¡å‹æ˜¯å¦å¯ä»¥è¡¨ç°å‡ºç±»ä¼¼çš„å¯æ‰©å±•æ€§ï¼Œæˆä¸ºä¸€é¡¹æœ‰è¶£ä¸”æœ‰ä»·å€¼çš„æ¢ç´¢ã€‚é£é™©è€ƒè™‘ï¼šä½œä¸ºä¸€ä¸ªç”Ÿæˆæ¨¡å‹ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¯èƒ½ä¼šæ— æ„ä¸­äº§ç”Ÿéš¾ä»¥ä¸äººç±»ä¹¦é¢å†…å®¹åŒºåˆ†å¼€æ¥çš„ç»“æœï¼Œå¼•å‘å¯¹æ½œåœ¨è¯¯ç”¨çš„æ‹…å¿§ã€‚é‡‡ç”¨æ–‡æœ¬æ°´å°æŠ€æœ¯å¯èƒ½æœ‰åŠ©äºå‡è½»è¿™ä¸ªé—®é¢˜ã€‚æ­¤å¤–ï¼Œæ‰©æ•£æ¨¡å‹é€šå¸¸éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºè¿›è¡Œè®­ç»ƒï¼Œä»è€Œå¯¼è‡´äºŒæ°§åŒ–ç¢³æ’æ”¾å’Œç¯å¢ƒå½±å“å¢åŠ ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="http://article.biliimg.com/bfs/new_dyn/ad873da96dc2ed96671aaa4ec1d1b20f241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/35dd717311044737324107cdc54b6822241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/9f7a7a48ee0399a430575adacda8ed66241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/093e2d146940e0222b3021bdfb674cf9241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/9d1dfb06c22a75df200fdac77b6e7498241286257.jpg" align="middle"></details>## GazeHTA: End-to-end Gaze Target Detection with Head-Target Association**Authors:Zhi-Yi Lin, Jouh Yeong Chew, Jan van Gemert, Xucong Zhang**We propose an end-to-end approach for gaze target detection: predicting a head-target connection between individuals and the target image regions they are looking at. Most of the existing methods use independent components such as off-the-shelf head detectors or have problems in establishing associations between heads and gaze targets. In contrast, we investigate an end-to-end multi-person Gaze target detection framework with Heads and Targets Association (GazeHTA), which predicts multiple head-target instances based solely on input scene image. GazeHTA addresses challenges in gaze target detection by (1) leveraging a pre-trained diffusion model to extract scene features for rich semantic understanding, (2) re-injecting a head feature to enhance the head priors for improved head understanding, and (3) learning a connection map as the explicit visual associations between heads and gaze targets. Our extensive experimental results demonstrate that GazeHTA outperforms state-of-the-art gaze target detection methods and two adapted diffusion-based baselines on two standard datasets. [PDF](http://arxiv.org/abs/2404.10718v1) **Summary**åŸºäºè§†è§‰å›¾åƒé¢„æµ‹æ³¨è§†ç›®æ ‡ï¼Œæå‡ºç«¯åˆ°ç«¯å¤šäººç‰©GazeHTAæ¡†æ¶ï¼Œé€šè¿‡é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ã€é‡æ–°æ³¨å…¥å¤´ç‰¹å¾ã€å­¦ä¹ è¿æ¥å›¾å®ç°é«˜æ•ˆçš„ç›®æ ‡æ£€æµ‹ã€‚**Key Takeaways**- æå‡ºç«¯åˆ°ç«¯å¤šäººç‰©æ³¨è§†ç›®æ ‡æ£€æµ‹æ¡†æ¶ - GazeHTAã€‚- åˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹æå–åœºæ™¯ç‰¹å¾ï¼Œæå‡è¯­ä¹‰ç†è§£ã€‚- é‡æ–°æ³¨å…¥å¤´ç‰¹å¾ï¼Œå¢å¼ºå¤´éƒ¨å…ˆéªŒï¼Œæå‡å¤´éƒ¨ç†è§£ã€‚- å­¦ä¹ è¿æ¥å›¾ï¼Œæ˜ç¡®å¤´éƒ¨ä¸æ³¨è§†ç›®æ ‡ä¹‹é—´çš„è§†è§‰å…³è”ã€‚- é‡‡ç”¨æ‰©æ•£æ¨¡å‹ä½œä¸ºåŸºå‡†ï¼Œå®ç°æ›´ä¼˜çš„æ€§èƒ½ã€‚- åœ¨æ ‡å‡†æ•°æ®é›†ä¸Šï¼ŒGazeHTA ä¼˜äºç°æœ‰æ–¹æ³•ã€‚- GazeHTA æä¾›äº†ç«¯åˆ°ç«¯çš„è§£å†³æ–¹æ¡ˆï¼Œå¯ç›´æ¥ä»å›¾åƒé¢„æµ‹æ³¨è§†ç›®æ ‡ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>æ ‡é¢˜ï¼šGazeHTAï¼šç«¯åˆ°ç«¯çš„æ³¨è§†ç›®æ ‡æ£€æµ‹</li><li>ä½œè€…ï¼šZhi-Yi Linã€Jouh Yeong Chewã€Jan van Gemertã€Xucong Zhang</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šä»£å°”å¤«ç‰¹ç†å·¥å¤§å­¦è®¡ç®—æœºè§†è§‰å®éªŒå®¤</li><li>å…³é”®è¯ï¼šæ³¨è§†ç›®æ ‡æ£€æµ‹ã€å¤´éƒ¨ç›®æ ‡å…³è”ã€æ‰©æ•£æ¨¡å‹</li><li>è®ºæ–‡é“¾æ¥ï¼šarXiv:2404.10718v1[cs.CV]16Apr2024</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šäººç±»æ³¨æ„åŠ›ä¼°è®¡åœ¨äººæœºäº¤äº’ã€ç¤¾ä¼šæ´»åŠ¨è¯†åˆ«ã€å¿ƒç†å¥åº·è¯Šæ–­å’Œå®¢æˆ·è¡Œä¸ºåˆ†æç­‰é¢†åŸŸå…·æœ‰é‡è¦æ„ä¹‰ã€‚æ³¨è§†ç›®æ ‡æ£€æµ‹æ—¨åœ¨ç›´æ¥å°†ä¸ªä½“ä¸å…¶æ³¨è§†ç›®æ ‡å…³è”èµ·æ¥ï¼Œæä¾›äº†ä¸€ç§ç«¯åˆ°ç«¯çš„æ³¨æ„åŠ›ä¼°è®¡è§£å†³æ–¹æ¡ˆã€‚</li></ol><p>ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šå¤§å¤šæ•°æ³¨è§†ç›®æ ‡æ£€æµ‹æ–¹æ³•é‡‡ç”¨åŒæµæ¶æ„ï¼Œå­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š- ç¼ºä¹å¤´éƒ¨å’Œæ³¨è§†ç›®æ ‡ä¹‹é—´çš„ç›´æ¥å…³è”ã€‚- ä¾èµ–äºç°æˆçš„å¤´éƒ¨æ£€æµ‹å™¨ã€‚- ä»…é™äºä¸€æ¬¡å¤„ç†ä¸€ä¸ªå¤´éƒ¨ï¼Œåœ¨å¤šäººç‰©åœºæ™¯ä¸­éœ€è¦é‡å¤å¤„ç†ã€‚</p><p>ï¼ˆ3ï¼‰æœ¬æ–‡æ–¹æ³•ï¼šGazeHTA æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯çš„å¤šäººæ³¨è§†ç›®æ ‡æ£€æµ‹æ¡†æ¶ï¼Œé€šè¿‡å¤´éƒ¨ç›®æ ‡å…³è”æ¥é¢„æµ‹å¤´éƒ¨ç›®æ ‡å®ä¾‹ã€‚å…¶ç‰¹ç‚¹åŒ…æ‹¬ï¼š- åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹æå–åœºæ™¯ç‰¹å¾ã€‚- é‡æ–°æ³¨å…¥å¤´éƒ¨ç‰¹å¾ä»¥å¢å¼ºå¤´éƒ¨å…ˆéªŒçŸ¥è¯†ã€‚- å­¦ä¹ è¿æ¥å›¾æ¥è¡¨ç¤ºå¤´éƒ¨å’Œæ³¨è§†ç›®æ ‡ä¹‹é—´çš„è§†è§‰å…³è”ã€‚</p><p>ï¼ˆ4ï¼‰æ€§èƒ½ï¼šGazeHTA åœ¨ä¸¤ä¸ªæ ‡å‡†æ•°æ®é›†ä¸Šä¼˜äºæœ€å…ˆè¿›çš„æ³¨è§†ç›®æ ‡æ£€æµ‹æ–¹æ³•å’Œä¸¤ä¸ªæ”¹ç¼–çš„åŸºäºæ‰©æ•£çš„åŸºå‡†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥å¾ˆå¥½åœ°æ”¯æŒå…¶ç›®æ ‡ã€‚</p><p>7.æ–¹æ³•ï¼šï¼ˆ1ï¼‰åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹æå–åœºæ™¯ç‰¹å¾ï¼ˆStable Diffusionï¼‰ï¼›ï¼ˆ2ï¼‰é‡æ–°æ³¨å…¥å¤´éƒ¨ç‰¹å¾ä»¥å¢å¼ºå¤´éƒ¨å…ˆéªŒçŸ¥è¯†ï¼ˆHead Feature Re-Injectionï¼‰ï¼›ï¼ˆ3ï¼‰å­¦ä¹ è¿æ¥å›¾æ¥è¡¨ç¤ºå¤´éƒ¨å’Œæ³¨è§†ç›®æ ‡ä¹‹é—´çš„è§†è§‰å…³è”ï¼ˆConnection Mapï¼‰ã€‚</p><ol><li>ç»“è®ºï¼š(1): GazeHTA æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯çš„æ³¨è§†ç›®æ ‡æ£€æµ‹æ¡†æ¶ï¼Œé€šè¿‡å¤´éƒ¨ç›®æ ‡å…³è”æ¥é¢„æµ‹å¤´éƒ¨ç›®æ ‡å®ä¾‹ï¼Œåœ¨ä¸¤ä¸ªæ ‡å‡†æ•°æ®é›†ä¸Šä¼˜äºæœ€å…ˆè¿›çš„æ³¨è§†ç›®æ ‡æ£€æµ‹æ–¹æ³•å’Œä¸¤ä¸ªæ”¹ç¼–çš„åŸºäºæ‰©æ•£çš„åŸºå‡†ã€‚(2): åˆ›æ–°ç‚¹ï¼š</li><li>åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹æå–åœºæ™¯ç‰¹å¾ï¼Œå¢å¼ºäº†å¤´éƒ¨å…ˆéªŒçŸ¥è¯†ï¼Œå­¦ä¹ äº†å¤´éƒ¨å’Œæ³¨è§†ç›®æ ‡ä¹‹é—´çš„è§†è§‰å…³è”ã€‚</li><li>é‡‡ç”¨å¤šä»»åŠ¡å­¦ä¹ ç­–ç•¥ï¼ŒåŒæ—¶é¢„æµ‹å¤´éƒ¨ç›®æ ‡å’Œæ³¨è§†ç›®æ ‡ï¼Œæé«˜äº†æ£€æµ‹ç²¾åº¦ã€‚</li><li>å¼•å…¥äº†è¿æ¥å›¾ï¼Œè¡¨ç¤ºå¤´éƒ¨å’Œæ³¨è§†ç›®æ ‡ä¹‹é—´çš„è§†è§‰å…³è”ï¼Œå¢å¼ºäº†å¤´éƒ¨ç›®æ ‡å…³è”çš„é²æ£’æ€§ã€‚æ€§èƒ½ï¼š</li><li>åœ¨ä¸¤ä¸ªæ ‡å‡†æ•°æ®é›†ä¸Šï¼ŒGazeHTA åœ¨æ³¨è§†ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li><li>è¯¥æ–¹æ³•åœ¨å¤šäººç‰©åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿå‡†ç¡®åœ°å°†å¤´éƒ¨ä¸æ³¨è§†ç›®æ ‡å…³è”èµ·æ¥ã€‚</li><li>è¯¥æ–¹æ³•å¯¹å¤´éƒ¨å§¿æ€å’Œç…§æ˜æ¡ä»¶çš„å˜åŒ–å…·æœ‰é²æ£’æ€§ã€‚å·¥ä½œé‡ï¼š</li><li>è¯¥æ–¹æ³•çš„å®ç°ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹å’Œè®­ç»ƒè¿æ¥å›¾ã€‚</li><li>è¯¥æ–¹æ³•çš„è®¡ç®—æˆæœ¬è¾ƒé«˜ï¼Œåœ¨å®é™…åº”ç”¨ä¸­å¯èƒ½éœ€è¦ä¼˜åŒ–ã€‚</li><li>è¯¥æ–¹æ³•éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œè¿™å¯èƒ½ä¼šé™åˆ¶å…¶åœ¨æŸäº›åœºæ™¯ä¸­çš„åº”ç”¨ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="http://article.biliimg.com/bfs/new_dyn/85b5892f7794783ad79c67d67689cac6241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/c2dd1f0a5f6c974a1e1cc5b4db6180e3241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/831d352002cad6012c105349850e0b6b241286257.jpg" align="middle"></details>## Efficient Conditional Diffusion Model with Probability Flow Sampling for   Image Super-resolution**Authors:Yutao Yuan, Chun Yuan**Image super-resolution is a fundamentally ill-posed problem because multiple valid high-resolution images exist for one low-resolution image. Super-resolution methods based on diffusion probabilistic models can deal with the ill-posed nature by learning the distribution of high-resolution images conditioned on low-resolution images, avoiding the problem of blurry images in PSNR-oriented methods. However, existing diffusion-based super-resolution methods have high time consumption with the use of iterative sampling, while the quality and consistency of generated images are less than ideal due to problems like color shifting. In this paper, we propose Efficient Conditional Diffusion Model with Probability Flow Sampling (ECDP) for image super-resolution. To reduce the time consumption, we design a continuous-time conditional diffusion model for image super-resolution, which enables the use of probability flow sampling for efficient generation. Additionally, to improve the consistency of generated images, we propose a hybrid parametrization for the denoiser network, which interpolates between the data-predicting parametrization and the noise-predicting parametrization for different noise scales. Moreover, we design an image quality loss as a complement to the score matching loss of diffusion models, further improving the consistency and quality of super-resolution. Extensive experiments on DIV2K, ImageNet, and CelebA demonstrate that our method achieves higher super-resolution quality than existing diffusion-based image super-resolution methods while having lower time consumption. Our code is available at https://github.com/Yuan-Yutao/ECDP. [PDF](http://arxiv.org/abs/2404.10688v1) AAAI 2024**Summary**é€šè¿‡æ¦‚ç‡æµé‡‡æ ·çš„é«˜æ•ˆæ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼ˆECDPï¼‰åœ¨å›¾åƒè¶…åˆ†è¾¨ç‡æ–¹é¢å®ç°äº†é«˜è¶…åˆ†è¾¨ç‡å›¾åƒè´¨é‡å’Œä½æ—¶é—´æ¶ˆè€—ã€‚**Key Takeaways**- æ‰©æ•£æ¦‚ç‡æ¨¡å‹èƒ½å¤Ÿå¤„ç†å›¾åƒè¶…åˆ†è¾¨ç‡ä¸­å›ºæœ‰çš„ç—…æ€æ€§é—®é¢˜ã€‚- è¿­ä»£é‡‡æ ·å¯¼è‡´ç°æœ‰åŸºäºæ‰©æ•£çš„è¶…åˆ†è¾¨ç‡æ–¹æ³•æ—¶é—´æ¶ˆè€—é«˜ã€‚- ECDP å¼•å…¥äº†è¿ç»­æ—¶é—´æ¡ä»¶æ‰©æ•£æ¨¡å‹ä»¥æé«˜é‡‡æ ·æ•ˆç‡ã€‚- æ··åˆå‚æ•°åŒ–ä¸ºå»å™ªç½‘ç»œæé«˜äº†ç”Ÿæˆå›¾åƒçš„ä¸€è‡´æ€§ã€‚- å›¾åƒè´¨é‡æŸå¤±ä½œä¸ºæ‰©æ•£æ¨¡å‹å¾—åˆ†åŒ¹é…æŸå¤±çš„è¡¥å……ï¼Œè¿›ä¸€æ­¥æé«˜äº†ä¸€è‡´æ€§å’Œè´¨é‡ã€‚- ECDP åœ¨ DIV2Kã€ImageNet å’Œ CelebA ä¸Šä¼˜äºç°æœ‰åŸºäºæ‰©æ•£çš„è¶…åˆ†è¾¨ç‡æ–¹æ³•ï¼ŒåŒæ—¶æ—¶é—´æ¶ˆè€—æ›´ä½ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>æ ‡é¢˜ï¼šåŸºäºæ¦‚ç‡æµé‡‡æ ·çš„é«˜æ•ˆæ¡ä»¶æ‰©æ•£æ¨¡å‹ç”¨äºå›¾åƒè¶…åˆ†è¾¨ç‡</li><li>ä½œè€…ï¼šè¢å®‡æ¶›ï¼Œè¢æ·³</li><li>å•ä½ï¼šæ¸…åå¤§å­¦</li><li>å…³é”®è¯ï¼šå›¾åƒè¶…åˆ†è¾¨ç‡ï¼Œæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼Œæ¦‚ç‡æµé‡‡æ ·</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.10688   Githubä»£ç é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šå›¾åƒè¶…åˆ†è¾¨ç‡æ˜¯ä¸€é¡¹æœ¬è´¨ä¸Šä¸é€‚å®šçš„é—®é¢˜ï¼Œå› ä¸ºå¯¹äºä¸€ä¸ªä½åˆ†è¾¨ç‡å›¾åƒå­˜åœ¨å¤šä¸ªæœ‰æ•ˆçš„é«˜åˆ†è¾¨ç‡å›¾åƒã€‚åŸºäºæ‰©æ•£æ¦‚ç‡æ¨¡å‹çš„è¶…åˆ†è¾¨ç‡æ–¹æ³•å¯ä»¥é€šè¿‡å­¦ä¹ æ¡ä»¶åœ¨ä½åˆ†è¾¨ç‡å›¾åƒä¸Šçš„é«˜åˆ†è¾¨ç‡å›¾åƒåˆ†å¸ƒæ¥å¤„ç†ä¸é€‚å®šæ€§ï¼Œé¿å…äº†ä»¥ PSNR ä¸ºå¯¼å‘çš„æ–¹æ³•ä¸­å›¾åƒæ¨¡ç³Šçš„é—®é¢˜ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠé—®é¢˜ï¼šç°æœ‰çš„åŸºäºæ‰©æ•£çš„è¶…åˆ†è¾¨ç‡æ–¹æ³•ä½¿ç”¨è¿­ä»£é‡‡æ ·ï¼Œæ—¶é—´æ¶ˆè€—å¤§ï¼›å¹¶ä¸”ç”±äºé¢œè‰²åç§»ç­‰é—®é¢˜ï¼Œç”Ÿæˆå›¾åƒçš„è´¨é‡å’Œä¸€è‡´æ€§ä¸ç†æƒ³ã€‚ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ç”¨äºå›¾åƒè¶…åˆ†è¾¨ç‡çš„åŸºäºæ¦‚ç‡æµé‡‡æ ·çš„é«˜æ•ˆæ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼ˆECDPï¼‰ã€‚ä¸ºäº†å‡å°‘æ—¶é—´æ¶ˆè€—ï¼Œè®¾è®¡äº†ç”¨äºå›¾åƒè¶…åˆ†è¾¨ç‡çš„è¿ç»­æ—¶é—´æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œè¿™ä½¿å¾—å¯ä»¥ä½¿ç”¨æ¦‚ç‡æµé‡‡æ ·è¿›è¡Œé«˜æ•ˆç”Ÿæˆã€‚æ­¤å¤–ï¼Œä¸ºäº†æé«˜ç”Ÿæˆå›¾åƒçš„ä¸€è‡´æ€§ï¼Œæå‡ºäº†å»å™ªå™¨ç½‘ç»œçš„æ··åˆå‚æ•°åŒ–ï¼Œå®ƒåœ¨ä¸åŒçš„å™ªå£°å°ºåº¦ä¸Šå¯¹æ•°æ®é¢„æµ‹å‚æ•°åŒ–å’Œå™ªå£°é¢„æµ‹å‚æ•°åŒ–è¿›è¡Œæ’å€¼ã€‚æ­¤å¤–ï¼Œè®¾è®¡äº†ä¸€ä¸ªå›¾åƒè´¨é‡æŸå¤±ä½œä¸ºæ‰©æ•£æ¨¡å‹åˆ†æ•°åŒ¹é…æŸå¤±çš„è¡¥å……ï¼Œè¿›ä¸€æ­¥æé«˜äº†è¶…åˆ†è¾¨ç‡çš„ä¸€è‡´æ€§å’Œè´¨é‡ã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šåœ¨ DIV2Kã€ImageNet å’Œ CelebA ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæœ¬æ–‡æ–¹æ³•æ¯”ç°æœ‰çš„åŸºäºæ‰©æ•£çš„å›¾åƒè¶…åˆ†è¾¨ç‡æ–¹æ³•å®ç°äº†æ›´é«˜çš„è¶…åˆ†è¾¨ç‡è´¨é‡ï¼ŒåŒæ—¶å…·æœ‰æ›´ä½çš„æ—¶é—´æ¶ˆè€—ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1): è®¾è®¡äº†ç”¨äºå›¾åƒè¶…åˆ†è¾¨ç‡çš„è¿ç»­æ—¶é—´æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œä½¿å¾—å¯ä»¥ä½¿ç”¨æ¦‚ç‡æµé‡‡æ ·è¿›è¡Œé«˜æ•ˆç”Ÿæˆï¼›(2): æå‡ºäº†å»å™ªå™¨ç½‘ç»œçš„æ··åˆå‚æ•°åŒ–ï¼Œå®ƒåœ¨ä¸åŒçš„å™ªå£°å°ºåº¦ä¸Šå¯¹æ•°æ®é¢„æµ‹å‚æ•°åŒ–å’Œå™ªå£°é¢„æµ‹å‚æ•°åŒ–è¿›è¡Œæ’å€¼ï¼Œæé«˜ç”Ÿæˆå›¾åƒçš„ä¸€è‡´æ€§ï¼›(3): è®¾è®¡äº†ä¸€ä¸ªå›¾åƒè´¨é‡æŸå¤±ä½œä¸ºæ‰©æ•£æ¨¡å‹åˆ†æ•°åŒ¹é…æŸå¤±çš„è¡¥å……ï¼Œè¿›ä¸€æ­¥æé«˜äº†è¶…åˆ†è¾¨ç‡çš„ä¸€è‡´æ€§å’Œè´¨é‡ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬æ–‡æå‡ºçš„ ECDP æ¡†æ¶åœ¨å›¾åƒè¶…åˆ†è¾¨ç‡ä»»åŠ¡ä¸Šå–å¾—äº†è¾ƒå¥½çš„æ•ˆæœï¼Œåœ¨ä¿è¯è¶…åˆ†è¾¨ç‡è´¨é‡çš„åŒæ—¶ï¼Œé™ä½äº†æ—¶é—´æ¶ˆè€—ã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§åŸºäºè¿ç»­æ—¶é—´æ¡ä»¶æ‰©æ•£æ¨¡å‹çš„å›¾åƒè¶…åˆ†è¾¨ç‡æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨æ¦‚ç‡æµé‡‡æ ·è¿›è¡Œç”Ÿæˆã€‚</li><li>æå‡ºäº†ä¸€ç§æ··åˆå‚æ•°åŒ–çš„å»å™ªå™¨ç½‘ç»œï¼Œè¯¥ç½‘ç»œå¯ä»¥åœ¨ä¸åŒçš„å™ªå£°å°ºåº¦ä¸Šå¯¹æ•°æ®é¢„æµ‹å‚æ•°åŒ–å’Œå™ªå£°é¢„æµ‹å‚æ•°åŒ–è¿›è¡Œæ’å€¼ï¼Œä»è€Œæé«˜ç”Ÿæˆå›¾åƒçš„ä¸€è‡´æ€§ã€‚</li><li>è®¾è®¡äº†ä¸€ä¸ªå›¾åƒè´¨é‡æŸå¤±ä½œä¸ºæ‰©æ•£æ¨¡å‹åˆ†æ•°åŒ¹é…æŸå¤±çš„è¡¥å……ï¼Œè¯¥æŸå¤±å¯ä»¥æœ‰æ•ˆåœ°æé«˜è¶…åˆ†è¾¨ç‡ç»“æœçš„è´¨é‡ã€‚æ€§èƒ½ï¼š</li><li>åœ¨ DIV2Kã€ImageNet å’Œ CelebA æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæœ¬æ–‡æ–¹æ³•æ¯”ç°æœ‰çš„åŸºäºæ‰©æ•£çš„å›¾åƒè¶…åˆ†è¾¨ç‡æ–¹æ³•å…·æœ‰æ›´é«˜çš„è¶…åˆ†è¾¨ç‡è´¨é‡å’Œæ›´ä½çš„æ—¶é—´æ¶ˆè€—ã€‚å·¥ä½œé‡ï¼š</li><li>æœ¬æ–‡æ–¹æ³•çš„å®ç°ç›¸å¯¹ç®€å•ï¼Œæ˜“äºå¤ç°ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="http://article.biliimg.com/bfs/new_dyn/a82d966193c93e548a48f6956d503a03241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/3243ba61ae0dc29581f4a5b0b8bc24ac241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/9ec5bc713a1dd0f65714d070ec06c103241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/8b672147537b2050d8dcfda1a25fba99241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/84fee720619599629de5e02d5ca3c96a241286257.jpg" align="middle"></details>## StyleCity: Large-Scale 3D Urban Scenes Stylization with Vision-and-Text   Reference via Progressive Optimization**Authors:Yingshu Chen, Huajian Huang, Tuan-Anh Vu, Ka Chun Shum, Sai-Kit Yeung**Creating large-scale virtual urban scenes with variant styles is inherently challenging. To facilitate prototypes of virtual production and bypass the need for complex materials and lighting setups, we introduce the first vision-and-text-driven texture stylization system for large-scale urban scenes, StyleCity. Taking an image and text as references, StyleCity stylizes a 3D textured mesh of a large-scale urban scene in a semantics-aware fashion and generates a harmonic omnidirectional sky background. To achieve that, we propose to stylize a neural texture field by transferring 2D vision-and-text priors to 3D globally and locally. During 3D stylization, we progressively scale the planned training views of the input 3D scene at different levels in order to preserve high-quality scene content. We then optimize the scene style globally by adapting the scale of the style image with the scale of the training views. Moreover, we enhance local semantics consistency by the semantics-aware style loss which is crucial for photo-realistic stylization. Besides texture stylization, we further adopt a generative diffusion model to synthesize a style-consistent omnidirectional sky image, which offers a more immersive atmosphere and assists the semantic stylization process. The stylized neural texture field can be baked into an arbitrary-resolution texture, enabling seamless integration into conventional rendering pipelines and significantly easing the virtual production prototyping process. Extensive experiments demonstrate our stylized scenes' superiority in qualitative and quantitative performance and user preferences. [PDF](http://arxiv.org/abs/2404.10681v1) project page: https://chenyingshu.github.io/stylecity3d/**Summary**åŸå¸‚åœºæ™¯å¤§è§„æ¨¡çº¹ç†é£æ ¼åŒ–åœ¨æ–‡å›¾é©±åŠ¨ä¸‹ï¼Œèåˆç¥ç»çº¹ç†åœºï¼Œèƒ½ç”Ÿæˆé€¼çœŸé£æ ¼åŒ–çš„çº¹ç†å’Œå…¨æ™¯å¤©ç©ºã€‚**Key Takeaways**- æå‡ºäº†ä¸€ä¸ªåŸå¸‚åœºæ™¯å¤§è§„æ¨¡çº¹ç†é£æ ¼åŒ–ç³»ç»Ÿã€‚- æå‡ºäº†ä¸€ç§ç¥ç»çº¹ç†åœºçš„é£æ ¼åŒ–æ–¹æ³•ï¼Œå°†äºŒç»´è§†è§‰ä¸æ–‡æœ¬å…ˆéªŒå…¨å±€å’Œå±€éƒ¨åœ°è¿ç§»åˆ°ä¸‰ç»´ä¸­ã€‚- æ¸è¿›ç¼©æ”¾è¾“å…¥ä¸‰ç»´åœºæ™¯çš„è®­ç»ƒè§†å›¾ä»¥ä¿ç•™åœºæ™¯å†…å®¹çš„é«˜è´¨é‡ã€‚- é€šè¿‡è°ƒæ•´é£æ ¼å›¾åƒå’Œè®­ç»ƒè§†å›¾çš„å°ºåº¦æ¥å…¨å±€ä¼˜åŒ–åœºæ™¯é£æ ¼ã€‚- é‡‡ç”¨è¯­ä¹‰æ„ŸçŸ¥çš„é£æ ¼æŸå¤±å¢å¼ºå±€éƒ¨è¯­ä¹‰ä¸€è‡´æ€§ï¼Œè¿™å¯¹äºçœŸå®æ„Ÿé£æ ¼åŒ–è‡³å…³é‡è¦ã€‚- é‡‡ç”¨ç”Ÿæˆå¼æ‰©æ•£æ¨¡å‹åˆæˆé£æ ¼ä¸€è‡´çš„å…¨æ™¯å¤©ç©ºå›¾åƒï¼Œæä¾›æ›´æ²‰æµ¸çš„æ°›å›´å¹¶è¾…åŠ©è¯­ä¹‰é£æ ¼åŒ–ã€‚- é£æ ¼åŒ–çš„ç¥ç»çº¹ç†åœºå¯ä»¥çƒ˜ç„™æˆä»»æ„åˆ†è¾¨ç‡çš„çº¹ç†ï¼Œæ— ç¼é›†æˆåˆ°ä¼ ç»Ÿæ¸²æŸ“ç®¡é“ä¸­ï¼Œæå¤§åœ°ç®€åŒ–äº†è™šæ‹Ÿåˆ¶ä½œåŸå‹åˆ¶ä½œè¿‡ç¨‹ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>é¢˜ç›®ï¼šStyleCityï¼šå¤§è§„æ¨¡ 3D åŸå¸‚åœºæ™¯</li><li>ä½œè€…ï¼šYingshu Chen, Huajian Huang, Tuan-Anh Vu, Ka Chun Shum, Sai-Kit Yeung</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šé¦™æ¸¯ç§‘æŠ€å¤§å­¦</li><li>å…³é”®è¯ï¼šåŸå¸‚é£æ ¼åŒ–ã€å¤§è§„æ¨¡åœºæ™¯ã€è§†è§‰å’Œæ–‡æœ¬å‚è€ƒã€æ¸è¿›å¼ä¼˜åŒ–</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.10681</li><li><p>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šåˆ›å»ºå…·æœ‰ä¸åŒé£æ ¼çš„å¤§è§„æ¨¡è™šæ‹ŸåŸå¸‚åœºæ™¯å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œéœ€è¦å¤æ‚çš„æè´¨å’Œç¯å…‰è®¾ç½®ã€‚(2) è¿‡å»æ–¹æ³•ï¼šç°æœ‰çš„æ–¹æ³•ä¸»è¦ä¾èµ–äºæ‰‹å·¥åˆ¶ä½œçš„æè´¨å’Œç¯å…‰ï¼Œéš¾ä»¥å®ç°å¤§è§„æ¨¡åœºæ™¯çš„è‡ªåŠ¨é£æ ¼åŒ–ã€‚(3) ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡º StyleCity æ¡†æ¶ï¼Œé€šè¿‡å°† 2D è§†è§‰å’Œæ–‡æœ¬å…ˆéªŒå…¨å±€å’Œå±€éƒ¨åœ°è½¬ç§»åˆ° 3Dï¼Œå¯¹ç¥ç»çº¹ç†åœºè¿›è¡Œé£æ ¼åŒ–ã€‚é€šè¿‡æ¸è¿›å¼ç¼©æ”¾è®­ç»ƒè§†å›¾å’Œè‡ªé€‚åº”é£æ ¼å›¾åƒæ¯”ä¾‹ä»¥åŠè¯­ä¹‰æ„ŸçŸ¥é£æ ¼æŸå¤±ï¼Œå®ç°åœºæ™¯é£æ ¼åŒ–å’Œå…¨æ™¯å¤©ç©ºèƒŒæ™¯ç”Ÿæˆã€‚(4) å®éªŒç»“æœï¼šåœ¨åŸå¸‚åœºæ™¯é£æ ¼åŒ–ä»»åŠ¡ä¸Šï¼ŒStyleCity åœ¨è§†è§‰ä¿çœŸåº¦å’Œè¯­ä¹‰ä¸€è‡´æ€§æ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ”¯æŒåˆ›å»ºä¸ªæ€§åŒ–ä¸”å¼•äººå…¥èƒœçš„åŸå¸‚æ¢ç´¢ä½“éªŒã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼šæ¢è½´è§†å›¾è§„åˆ’ï¼šæå‡ºäº†ä¸€ç§åŸºäºæ¢è½´çš„è§†å›¾è§„åˆ’æ–¹æ³•ï¼Œé€šè¿‡åœ¨ç½‘æ ¼è¾¹ç•Œæ¡†çš„ä¸Šã€ä¾§é¢å‡åŒ€é‡‡æ ·ç›¸æœºä½ç½®ï¼Œå¹¶ä½¿ç”¨ç½‘æ ¼åŒºåŸŸçš„è´¨å¿ƒä½œä¸ºç›¸æœºè§†ç‚¹ï¼Œè·å¾—æ¢è½´è§†å›¾ï¼Œè¦†ç›–å¯è§è¡¨é¢çš„å¤§éƒ¨åˆ†åŒºåŸŸï¼Œä¸ºæ–°è§†å›¾ï¼ˆå›¾ 2 ä¸­çš„ç»¿è‰²ç›¸æœºï¼‰æä¾›åˆå§‹åŒ–ï¼Œä»¥å¢å¼ºè®­ç»ƒï¼Œè§ç¬¬ 3.3 èŠ‚å¤šå°ºåº¦æ¸è¿›ä¼˜åŒ–ã€‚ï¼ˆ2ï¼‰ï¼šåŸå¸‚åœºæ™¯åˆ†å‰²ï¼šä¸ºè¯­ä¹‰æ„ŸçŸ¥çš„åŸå¸‚åœºæ™¯é£æ ¼åŒ–ï¼Œè€ƒè™‘äº†å»ºç­‘åœºæ™¯ä¸­æ„Ÿå…´è¶£çš„ç±»åˆ«ï¼ŒåŒ…æ‹¬â€œå¤©ç©ºâ€ã€â€œå»ºç­‘ç‰©â€ã€â€œçª—æˆ·â€ã€â€œé“è·¯â€ã€â€œäººâ€ã€â€œæ¤ç‰©â€ã€â€œæ±½è½¦â€ã€â€œæ°´â€å’Œâ€œç¯å…‰â€ã€‚ï¼ˆ3ï¼‰ï¼šç¥ç»çº¹ç†åœºå®šä¹‰ï¼šä½¿ç”¨ç¥ç»çº¹ç†åœºè¡¨ç¤ºï¼Œå°†å·¨å¤§çš„çº¹ç†è´´å›¾é‡æ–°å‚æ•°åŒ–ä¸ºäºŒç»´è¿ç»­å‡½æ•°ï¼Œé€šè¿‡ä¸€ä¸ª MLP å°†å½’ä¸€åŒ–çš„ UV çº¹ç†åæ ‡æ˜ å°„ä¸ºé¢œè‰² RGB å€¼ï¼Œç†è®ºä¸Šæ”¯æŒä»»æ„çº¹ç†åˆ†è¾¨ç‡ï¼Œå¹³å‡çº¹ç†å°ºå¯¸å‹ç¼© 90%ã€‚ï¼ˆ4ï¼‰ï¼šç¥ç»æ¸²æŸ“ï¼šç»™å®šç›¸æœºä½å§¿ï¼Œå…‰æ …åŒ–ç½‘æ ¼å¹¶æ£€ç´¢ UV ä»¥æŸ¥è¯¢çº¹ç†æ¨¡å‹ TÎ˜(Â·)ï¼Œè·å¾—ç›¸åº”çš„çº¹ç† RGB å€¼ï¼Œç„¶åå°†å€¼é‡æ–°ç»„åˆåˆ°æ¸²æŸ“çš„å›¾åƒä¸­ã€‚ï¼ˆ5ï¼‰ï¼šå†…å®¹å’Œé£æ ¼è”åˆæ¸è¿›ä¼˜åŒ–ï¼šé€šè¿‡å¤šä¸ªè§†å›¾ä¸æ¯ä¸ªè¿­ä»£ä¸­çš„æºå†…å®¹å’Œç›®æ ‡é£æ ¼çº¦æŸå…±åŒä¼˜åŒ–ç¥ç»çº¹ç†åœºï¼Œéšæœºé‡‡æ ·è§†ç‚¹ï¼Œæ¸²æŸ“å†…å®¹è§†å›¾åŠå…¶åˆ†å‰²ï¼Œå¹¶ä»ç¥ç»çº¹ç†ä¸­è·å¾—é£æ ¼åŒ–è§†å›¾ã€‚ï¼ˆ6ï¼‰ï¼šå¤šå°ºåº¦æ¸è¿›ä¼˜åŒ–ï¼šåœ¨ä¼˜åŒ–æœŸé—´ï¼Œæ²¿è´å¡å°”æ›²çº¿éšæœºé‡‡æ ·æ–°è§†å›¾ï¼Œä»¥é™„è¿‘çš„è®¡åˆ’æ¢è½´ç›¸æœºä½œä¸ºæ§åˆ¶ç‚¹ï¼Œä»¥æ‰©å¤§è¦†ç›–è§’åº¦ï¼Œå¹¶åœ¨è®­ç»ƒæœŸé—´ä»¥â€œæ”¾å¤§â€æ•ˆæœé€æ¸ç¼©å°é‡‡æ ·è§†å›¾çš„è§†åœº (FoV)ï¼Œç¡®ä¿æ¯ä¸ªè¡¨é¢éƒ½è¢«å…¨é¢é£æ ¼åŒ–å’Œå…¨å±€è°æ³¢ã€‚ï¼ˆ7ï¼‰ï¼šå†…å®¹å’ŒçœŸå®æ„Ÿä¿ç•™ä¼˜åŒ–ï¼šåˆ©ç”¨æ¸²æŸ“è§†å›¾çš„å†…å®¹ç‰¹å¾å’Œæ‹‰æ™®æ‹‰æ–¯å€¼è¿›è¡Œç›‘ç£ï¼Œä»¥ä¿æŒçº¹ç†å†…å®¹å’Œåœºæ™¯æ ‡è¯†ï¼Œå±è”½èƒŒæ™¯åŒºåŸŸä»¥ä¿æŒå†…å®¹å®Œæ•´æ€§å’ŒçœŸå®æ„Ÿã€‚ï¼ˆ8ï¼‰ï¼šå…¨å±€è§„æ¨¡è‡ªé€‚åº”é£æ ¼ä¼˜åŒ–ï¼šå…¨å±€é£æ ¼ä¼˜åŒ–è´Ÿè´£å°†é£æ ¼ç‰¹å¾å…¨å±€è½¬ç§»åˆ°ç¥ç»çº¹ç†åœºï¼Œä»¥è¿›è¡Œæ•´ä½“æ°›å›´å¯¹é½ï¼ŒåŸºäºè®­ç»ƒè§†å›¾å’Œé£æ ¼è´´å—ä¹‹é—´çš„ç»“æ„ç›¸ä¼¼æ€§åŒ¹é…å¤šå°ºåº¦é£æ ¼ã€‚ï¼ˆ9ï¼‰ï¼šå…¨å±€è§†è§‰å’Œæ–‡æœ¬é©±åŠ¨çš„é£æ ¼æŸå¤±ï¼šé€šè¿‡æƒ©ç½šå…¨å±€é£æ ¼ç‰¹å¾åˆ†å¸ƒçš„å·®å¼‚ï¼Œå¿«é€Ÿè·å–å’Œç”Ÿæˆæœ‰æ„ä¹‰çš„æ–°é£æ ¼åŒ–çº¹ç†ã€‚ï¼ˆ10ï¼‰ï¼šå±€éƒ¨è¯­ä¹‰æ„ŸçŸ¥é£æ ¼ä¼˜åŒ–ï¼šå¯¹äºå…·æœ‰å¤æ‚ä¸Šä¸‹æ–‡çš„åŸå¸‚åœºæ™¯ï¼Œå…¨å±€é£æ ¼è½¬ç§»å®¹æ˜“å¯¼è‡´é£æ ¼è¯­ä¹‰ä¸åŒ¹é…ï¼Œå› æ­¤å¼•å…¥å±€éƒ¨é£æ ¼ä¼˜åŒ–ç­–ç•¥ï¼Œè¿›è¡ŒæŒ‰ç±»åˆ«ç‰¹å¾æ­£åˆ™åŒ–ï¼Œä»¥å®ç°æ›´é€¼çœŸçš„é£æ ¼åŒ–ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šStyleCityæ˜¯ä¸€ä¸ªåŸºäºè§†è§‰å’Œæ–‡æœ¬é©±åŠ¨çš„åŸå¸‚çº§çº¹ç†ç½‘æ ¼å¤§è§„æ¨¡é£æ ¼åŒ–ç®¡é“ã€‚æˆ‘ä»¬åˆ©ç”¨ç¥ç»çº¹ç†åœºå»ºæ¨¡åœºæ™¯å¤–è§‚ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„å¤šå°ºåº¦æ¸è¿›ä¼˜åŒ–æ–¹æ³•ï¼Œä»¥å®ç°é«˜ä¿çœŸé£æ ¼åŒ–ã€‚å¯¹äºè°æ³¢é£æ ¼åŒ–ï¼Œæˆ‘ä»¬å¼•å…¥äº†å°ºåº¦è‡ªé€‚åº”é£æ ¼ä¼˜åŒ–å’Œæ–°çš„æŸå¤±å‡½æ•°ï¼Œä»¥å…¨å±€å’Œå±€éƒ¨æ­£åˆ™åŒ–é£æ ¼ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ”¹è¿›äº†æ¼«åå°„å…¨æ™¯åˆæˆæ–¹æ³•ï¼Œä»¥æ”¯æŒé£æ ¼å¯¹é½çš„é«˜åˆ†è¾¨ç‡å…¨æ–¹ä½å¤©ç©ºåˆæˆï¼Œè¿™ä½œä¸ºæ²‰æµ¸å¼æ°›å›´å’Œæ›´å¥½è¯­ä¹‰çš„èƒŒæ™¯ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šStyleCityæ¡†æ¶ã€ç¥ç»çº¹ç†åœºè¡¨ç¤ºã€å¤šå°ºåº¦æ¸è¿›ä¼˜åŒ–ã€å°ºåº¦è‡ªé€‚åº”é£æ ¼ä¼˜åŒ–ã€å…¨å±€è§†è§‰å’Œæ–‡æœ¬é©±åŠ¨çš„é£æ ¼æŸå¤±ã€åŸºäºæ¼«åå°„çš„å…¨æ™¯å¤©ç©ºåˆæˆï¼›æ€§èƒ½ï¼šåœ¨åŸå¸‚åœºæ™¯é£æ ¼åŒ–ä»»åŠ¡ä¸Šï¼ŒStyleCityåœ¨è§†è§‰ä¿çœŸåº¦å’Œè¯­ä¹‰ä¸€è‡´æ€§æ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ”¯æŒåˆ›å»ºä¸ªæ€§åŒ–ä¸”å¼•äººå…¥èƒœçš„åŸå¸‚æ¢ç´¢ä½“éªŒï¼›å·¥ä½œé‡ï¼šè¯¥æ–¹æ³•éœ€è¦å¤§é‡çš„æ•°æ®é›†å’Œè®¡ç®—èµ„æºï¼Œå¹¶ä¸”è®­ç»ƒè¿‡ç¨‹å¯èƒ½éœ€è¦å¾ˆé•¿æ—¶é—´ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="http://article.biliimg.com/bfs/new_dyn/7cb106e145913b85abac0f0c3f097cc8241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/086cffd59fa8e20d53cc5c5be733a969241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/68c301be7084d603d72cc10756a2ae2c241286257.jpg" align="middle"></details>## Generating Counterfactual Trajectories with Latent Diffusion Models for   Concept Discovery**Authors:Payal Varshney, Adriano Lucieri, Christoph Balada, Andreas Dengel, Sheraz Ahmed**Trustworthiness is a major prerequisite for the safe application of opaque deep learning models in high-stakes domains like medicine. Understanding the decision-making process not only contributes to fostering trust but might also reveal previously unknown decision criteria of complex models that could advance the state of medical research. The discovery of decision-relevant concepts from black box models is a particularly challenging task. This study proposes Concept Discovery through Latent Diffusion-based Counterfactual Trajectories (CDCT), a novel three-step framework for concept discovery leveraging the superior image synthesis capabilities of diffusion models. In the first step, CDCT uses a Latent Diffusion Model (LDM) to generate a counterfactual trajectory dataset. This dataset is used to derive a disentangled representation of classification-relevant concepts using a Variational Autoencoder (VAE). Finally, a search algorithm is applied to identify relevant concepts in the disentangled latent space. The application of CDCT to a classifier trained on the largest public skin lesion dataset revealed not only the presence of several biases but also meaningful biomarkers. Moreover, the counterfactuals generated within CDCT show better FID scores than those produced by a previously established state-of-the-art method, while being 12 times more resource-efficient. Unsupervised concept discovery holds great potential for the application of trustworthy AI and the further development of human knowledge in various domains. CDCT represents a further step in this direction. [PDF](http://arxiv.org/abs/2404.10356v1) Submitted to International Conference on Pattern Recognition (ICPR)   2024**Summary**åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å“è¶Šå›¾åƒåˆæˆèƒ½åŠ›ï¼Œæå‡ºä¸€ç§æ–°çš„æ¦‚å¿µå‘ç°æ¡†æ¶ï¼Œä»¥å‘ç°é»‘ç›’æ¨¡å‹ä¸­ä¸å†³ç­–ç›¸å…³çš„é‡è¦æ¦‚å¿µã€‚**Key Takeaways**- é»‘ç›’æ¨¡å‹ä¸­çš„æ¦‚å¿µå‘ç°å¯¹äºå»ºç«‹ä¿¡ä»»å’Œæ¨è¿›åŒ»å­¦ç ”ç©¶è‡³å…³é‡è¦ã€‚- CDCTæ˜¯ä¸€ç§ç”¨äºæ¦‚å¿µå‘ç°çš„æ–°å‹ä¸‰æ­¥æ¡†æ¶ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å›¾åƒåˆæˆèƒ½åŠ›ã€‚- CDCTé€šè¿‡ç”Ÿæˆåäº‹å®è½¨è¿¹æ•°æ®é›†ã€æå–è§£è€¦è¡¨å¾å’Œåº”ç”¨æœç´¢ç®—æ³•æ¥è¯†åˆ«ç›¸å…³çš„æ¦‚å¿µã€‚- CDCTå‘ç°äº†çš®è‚¤ç—…å˜åˆ†ç±»å™¨ä¸­çš„åå·®å’Œæœ‰æ„ä¹‰çš„ç”Ÿç‰©æ ‡å¿—ç‰©ã€‚- CDCTç”Ÿæˆçš„å¯¹æŠ—æ ·æœ¬çš„FIDå¾—åˆ†æ¯”ç°æœ‰æ–¹æ³•æ›´å¥½ï¼ŒåŒæ—¶èµ„æºæ•ˆç‡æé«˜äº†12å€ã€‚- æ— ç›‘ç£æ¦‚å¿µå‘ç°å¯¹äºå¯ä¿¡èµ–AIçš„åº”ç”¨å’Œäººç±»çŸ¥è¯†åœ¨å„ä¸ªé¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚- CDCTä»£è¡¨äº†æœç€è¿™ä¸ªæ–¹å‘è¿ˆå‡ºçš„è¿›ä¸€æ­¥ä¸€æ­¥ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>æ ‡é¢˜ï¼šåŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„åäº‹å®è½¨è¿¹ç”Ÿæˆï¼Œç”¨äºæ¦‚å¿µå‘ç°</li><li>ä½œè€…ï¼š</li><li>Payal Varshney</li><li>Adriano Lucieri</li><li>Christoph Balada</li><li>Andreas Dengel</li><li>Sheraz Ahmed</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šå¾·å›½å‡¯æ’’æ–¯åŠ³æ»•-å…°é“è±èŒµå…°-æ™®æ³•å°”èŒ¨æŠ€æœ¯å¤§å­¦</li><li>å…³é”®è¯ï¼šå¯è§£é‡Šæ€§ã€åäº‹å®ã€åŸºäºæ¦‚å¿µçš„è§£é‡Šã€æ½œåœ¨æ‰©æ•£æ¨¡å‹ã€çš®è‚¤é•œæ£€æŸ¥ã€æ¦‚å¿µå‘ç°</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.10356   Github ä»£ç é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼š   (1) ç ”ç©¶èƒŒæ™¯ï¼š      æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨åŒ»ç–—ç­‰é«˜é£é™©é¢†åŸŸçš„å®‰å…¨åº”ç”¨éœ€è¦å¯ä¿¡åº¦ã€‚ç†è§£å†³ç­–è¿‡ç¨‹ä¸ä»…æœ‰åŠ©äºå»ºç«‹ä¿¡ä»»ï¼Œè€Œä¸”è¿˜èƒ½æ­ç¤ºå¤æ‚æ¨¡å‹ä»¥å‰æœªçŸ¥çš„å†³ç­–æ ‡å‡†ï¼Œä»è€Œæ¨è¿›åŒ»å­¦ç ”ç©¶çš„å‘å±•ã€‚ä»é»‘ç›’æ¨¡å‹ä¸­å‘ç°ä¸å†³ç­–ç›¸å…³çš„æ¦‚å¿µæ˜¯ä¸€é¡¹ç‰¹åˆ«å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚   (2) è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼š      ä»¥å¾€çš„æ–¹æ³•ä¸»è¦ä¾èµ–äºæ¢¯åº¦æˆ–æ‰°åŠ¨æŠ€æœ¯ï¼Œä½†è¿™äº›æ–¹æ³•ç”Ÿæˆçš„åäº‹å®å›¾åƒè´¨é‡è¾ƒå·®ï¼Œéš¾ä»¥ä»ä¸­æå–æœ‰æ„ä¹‰çš„æ¦‚å¿µã€‚      æœ¬æ–‡æå‡ºçš„æ–¹æ³•åŠ¨æœºå……åˆ†ï¼Œå®ƒåˆ©ç”¨æ‰©æ•£æ¨¡å‹å‡ºè‰²çš„å›¾åƒåˆæˆèƒ½åŠ›ï¼Œé€šè¿‡ç”Ÿæˆåäº‹å®è½¨è¿¹æ•°æ®é›†æ¥å‘ç°ä¸åˆ†ç±»ç›¸å…³çš„æ¦‚å¿µã€‚   (3) æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼š      æœ¬æ–‡æå‡ºäº†ä¸€ç§ç§°ä¸º CDCT çš„ä¸‰æ­¥æ¡†æ¶ï¼Œç”¨äºé€šè¿‡åŸºäºæ½œåœ¨æ‰©æ•£çš„åäº‹å®è½¨è¿¹è¿›è¡Œæ¦‚å¿µå‘ç°ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ï¼š</p><ul><li>ä½¿ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆåäº‹å®è½¨è¿¹æ•°æ®é›†ã€‚</li><li>åˆ©ç”¨å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ä»åäº‹å®è½¨è¿¹æ•°æ®é›†ä¸­æå–åˆ†ç±»ç›¸å…³æ¦‚å¿µçš„è§£è€¦è¡¨ç¤ºã€‚</li><li>åº”ç”¨æœç´¢ç®—æ³•åœ¨è§£è€¦çš„æ½œåœ¨ç©ºé—´ä¸­è¯†åˆ«ç›¸å…³æ¦‚å¿µã€‚   (4) æ–¹æ³•åœ¨ä»»åŠ¡ä¸Šçš„è¡¨ç°åŠæ€§èƒ½ï¼š  å°† CDCT åº”ç”¨äºåœ¨æœ€å¤§çš„å…¬å¼€çš®è‚¤ç—…å˜æ•°æ®é›†ä¸Šè®­ç»ƒçš„åˆ†ç±»å™¨ï¼Œå‘ç°äº†å‡ ä¸ªåå·®å’Œæœ‰æ„ä¹‰çš„ç”Ÿç‰©æ ‡è®°ã€‚  æ­¤å¤–ï¼Œåœ¨ CDCT ä¸­ç”Ÿæˆçš„åäº‹å®å›¾åƒæ˜¾ç¤ºå‡ºæ¯”å…ˆå‰å»ºç«‹çš„æœ€æ–°æ–¹æ³•æ›´å¥½çš„ FID åˆ†æ•°ï¼ŒåŒæ—¶èµ„æºæ•ˆç‡æé«˜äº† 12 å€ã€‚  è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡æå‡ºçš„æ–¹æ³•å¯ä»¥æœ‰æ•ˆå‘ç°ä¸åˆ†ç±»ç›¸å…³çš„æ¦‚å¿µï¼Œå¹¶ä¸ºå¯ä¿¡èµ–çš„äººå·¥æ™ºèƒ½å’Œå„ä¸ªé¢†åŸŸçš„çŸ¥è¯†å‘å±•åšå‡ºè´¡çŒ®ã€‚</li></ul></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼šæ½œåœ¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆåäº‹å®è½¨è¿¹æ•°æ®é›†ï¼›ï¼ˆ2ï¼‰ï¼šå˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ä»åäº‹å®è½¨è¿¹æ•°æ®é›†ä¸­æå–åˆ†ç±»ç›¸å…³æ¦‚å¿µçš„è§£è€¦è¡¨ç¤ºï¼›ï¼ˆ3ï¼‰ï¼šæœç´¢ç®—æ³•åœ¨è§£è€¦çš„æ½œåœ¨ç©ºé—´ä¸­è¯†åˆ«ç›¸å…³æ¦‚å¿µã€‚</p></li><li><p>ç»“è®º(1): æœ¬å·¥ä½œé€šè¿‡åŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„åäº‹å®è½¨è¿¹ç”Ÿæˆï¼Œä¸ºæ¦‚å¿µå‘ç°æä¾›äº†ä¸€ç§å¯è§£é‡Šä¸”é«˜æ•ˆçš„æ–¹æ³•ï¼Œåœ¨åŒ»ç–—ç­‰é«˜é£é™©é¢†åŸŸçš„å®‰å…¨åº”ç”¨ä¸­å…·æœ‰é‡è¦æ„ä¹‰ã€‚(2): åˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§ç§°ä¸º CDCT çš„ä¸‰æ­¥æ¡†æ¶ï¼Œç”¨äºé€šè¿‡åŸºäºæ½œåœ¨æ‰©æ•£çš„åäº‹å®è½¨è¿¹è¿›è¡Œæ¦‚å¿µå‘ç°ã€‚</li><li>åˆ©ç”¨æ‰©æ•£æ¨¡å‹å‡ºè‰²çš„å›¾åƒåˆæˆèƒ½åŠ›ï¼Œç”Ÿæˆåäº‹å®è½¨è¿¹æ•°æ®é›†ï¼Œä»ä¸­æå–åˆ†ç±»ç›¸å…³æ¦‚å¿µçš„è§£è€¦è¡¨ç¤ºã€‚</li><li>åœ¨è§£è€¦çš„æ½œåœ¨ç©ºé—´ä¸­åº”ç”¨æœç´¢ç®—æ³•è¯†åˆ«ç›¸å…³æ¦‚å¿µã€‚æ€§èƒ½ï¼š</li><li>åœ¨æœ€å¤§çš„å…¬å¼€çš®è‚¤ç—…å˜æ•°æ®é›†ä¸Šè®­ç»ƒçš„åˆ†ç±»å™¨ä¸­å‘ç°äº†å‡ ä¸ªåå·®å’Œæœ‰æ„ä¹‰çš„ç”Ÿç‰©æ ‡è®°ã€‚</li><li>ä¸å…ˆå‰å»ºç«‹çš„æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼Œç”Ÿæˆçš„åäº‹å®å›¾åƒæ˜¾ç¤ºå‡ºæ›´å¥½çš„ FID åˆ†æ•°ï¼ŒåŒæ—¶èµ„æºæ•ˆç‡æé«˜äº† 12 å€ã€‚å·¥ä½œé‡ï¼š</li><li>è¯¥æ–¹æ³•æ˜“äºå®ç°ï¼Œå¹¶ä¸”å¯ä»¥åº”ç”¨äºå„ç§åˆ†ç±»ä»»åŠ¡ã€‚</li><li>è®­ç»ƒå’Œéƒ¨ç½² CDCT æ¨¡å‹çš„è®¡ç®—æˆæœ¬ç›¸å¯¹è¾ƒä½ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="http://article.biliimg.com/bfs/new_dyn/576c1e270406787f968cf066ad94df12241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/921adc3c228efd65b737862c3ffcf199241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/4d2bd80c6bf872188ed28b597b22867a241286257.jpg" align="middle"></details>## Efficiently Adversarial Examples Generation for Visual-Language Models   under Targeted Transfer Scenarios using Diffusion Models**Authors:Qi Guo, Shanmin Pang, Xiaojun Jia, Qing Guo**Targeted transfer-based attacks involving adversarial examples pose a significant threat to large visual-language models (VLMs). However, the state-of-the-art (SOTA) transfer-based attacks incur high costs due to excessive iteration counts. Furthermore, the generated adversarial examples exhibit pronounced adversarial noise and demonstrate limited efficacy in evading defense methods such as DiffPure. To address these issues, inspired by score matching, we introduce AdvDiffVLM, which utilizes diffusion models to generate natural, unrestricted adversarial examples. Specifically, AdvDiffVLM employs Adaptive Ensemble Gradient Estimation to modify the score during the diffusion model's reverse generation process, ensuring the adversarial examples produced contain natural adversarial semantics and thus possess enhanced transferability. Simultaneously, to enhance the quality of adversarial examples further, we employ the GradCAM-guided Mask method to disperse adversarial semantics throughout the image, rather than concentrating them in a specific area. Experimental results demonstrate that our method achieves a speedup ranging from 10X to 30X compared to existing transfer-based attack methods, while maintaining superior quality of adversarial examples. Additionally, the generated adversarial examples possess strong transferability and exhibit increased robustness against adversarial defense methods. Notably, AdvDiffVLM can successfully attack commercial VLMs, including GPT-4V, in a black-box manner. [PDF](http://arxiv.org/abs/2404.10335v1) **Summary**åˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆé€¼çœŸçš„å¯¹æŠ—æ ·æœ¬ï¼Œæ˜¾è‘—æå‡å›¾åƒå’Œè¯­è¨€æ¨¡å‹å¯¹æŠ—æ”»å‡»æ•ˆç‡å’Œå®‰å…¨æ€§ã€‚**Key Takeaways**- æå‡º AdvDiffVLMï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆè‡ªç„¶ã€æ— çº¦æŸçš„å¯¹æŠ—æ ·æœ¬ã€‚- é‡‡ç”¨è‡ªé€‚åº”é›†æˆæ¢¯åº¦ä¼°è®¡ï¼Œä¿®æ”¹æ‰©æ•£æ¨¡å‹åå‘ç”Ÿæˆè¿‡ç¨‹ä¸­çš„è¯„åˆ†ï¼Œç¡®ä¿å¯¹æŠ—æ ·æœ¬åŒ…å«è‡ªç„¶çš„å¯¹æŠ—è¯­ä¹‰ã€‚- ä½¿ç”¨ GradCAM å¼•å¯¼æ©ç æ–¹æ³•ï¼Œå°†å¯¹æŠ—è¯­ä¹‰åˆ†æ•£åˆ°æ•´ä¸ªå›¾åƒï¼Œé¿å…é›†ä¸­åœ¨ç‰¹å®šåŒºåŸŸã€‚- ä¸ç°æœ‰è½¬ç§»æ”»å‡»æ–¹æ³•ç›¸æ¯”ï¼Œé€Ÿåº¦æé«˜ 10-30 å€ï¼ŒåŒæ—¶ä¿æŒå¯¹æŠ—æ ·æœ¬çš„è´¨é‡ã€‚- ç”Ÿæˆçš„å¯¹æŠ—æ ·æœ¬å…·æœ‰å¾ˆå¼ºçš„è¿ç§»æ€§ï¼Œå¯¹å¯¹æŠ—é˜²å¾¡æ–¹æ³•çš„é²æ£’æ€§æ›´é«˜ã€‚- AdvDiffVLM å¯ä»¥ä»¥é»‘ç›’æ–¹å¼æˆåŠŸæ”»å‡»å•†ç”¨ VLMï¼ŒåŒ…æ‹¬ GPT-4Vã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>æ ‡é¢˜ï¼šåŸºäºæ‰©æ•£æ¨¡å‹çš„è§†è§‰è¯­è¨€æ¨¡å‹ç›®æ ‡è¿ç§»åœºæ™¯ä¸‹é«˜æ•ˆå¯¹æŠ—æ ·æœ¬ç”Ÿæˆ</li><li>ä½œè€…ï¼šéƒ­å¥‡ã€åºå±±æ°‘ã€è´¾æ™“å†›ã€éƒ­åº†</li><li>éš¶å±å•ä½ï¼šè¥¿å®‰äº¤é€šå¤§å­¦</li><li>å…³é”®è¯ï¼šå¯¹æŠ—æ ·æœ¬ã€è§†è§‰è¯­è¨€æ¨¡å‹ã€æ‰©æ•£æ¨¡å‹</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.10335</li><li><p>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šé’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„åŸºäºç›®æ ‡è¿ç§»çš„å¯¹æŠ—æ”»å‡»å¯¹ VLM æ„æˆé‡å¤§å¨èƒã€‚ç„¶è€Œï¼Œæœ€å…ˆè¿›çš„åŸºäºè¿ç§»çš„æ”»å‡»ç”±äºè¿‡å¤šçš„è¿­ä»£æ¬¡æ•°è€Œäº§ç”Ÿé«˜æ˜‚çš„æˆæœ¬ã€‚æ­¤å¤–ï¼Œç”Ÿæˆçš„å¯¹æŠ—æ ·æœ¬è¡¨ç°å‡ºæ˜æ˜¾çš„å¯¹æŠ—å™ªå£°ï¼Œå¹¶ä¸”åœ¨è§„é¿ DiffPure ç­‰é˜²å¾¡æ–¹æ³•æ–¹é¢è¡¨ç°å‡ºæœ‰é™çš„æ•ˆåŠ›ã€‚(2) è¿‡å»çš„æ–¹æ³•åŠé—®é¢˜ï¼šAttackVLM ä½¿ç”¨åŸºäºæŸ¥è¯¢çš„æ”»å‡»æ–¹æ³•å¹¶ç»“åˆåŸºäºè¿ç§»çš„å…ˆéªŒï¼Œæç¤ºé»‘ç›’ VLM äº§ç”Ÿç›®æ ‡å“åº”ã€‚ä½†æ˜¯ï¼Œç”±äºéœ€è¦å¤§é‡çš„æŸ¥è¯¢ï¼Œè¿™ä¸ªè¿‡ç¨‹éå¸¸è€—æ—¶ï¼Œé€šå¸¸éœ€è¦å‡ ä¸ªå°æ—¶æ‰èƒ½ç”Ÿæˆä¸€ä¸ªå¯¹æŠ—æ ·æœ¬ã€‚å› æ­¤ï¼Œè€ƒè™‘äº†å¦ä¸€ç§é»‘ç›’æ”»å‡»æ–¹æ³•ï¼Œå³åŸºäºè¿ç§»çš„æ”»å‡»ã€‚ç„¶è€Œï¼Œå¦‚å›¾ 1 æ‰€ç¤ºï¼Œå½“å‰æœ€å…ˆè¿›çš„åŸºäºè¿ç§»çš„æ”»å‡»åœ¨ç”Ÿæˆå¯¹æŠ—æ ·æœ¬æ–¹é¢ä¹Ÿè¾ƒæ…¢ï¼Œå¹¶ä¸”åœ¨è§„é¿å¯¹æŠ—é˜²å¾¡æ–¹æ³•æ–¹é¢æ•ˆæœè¾ƒå·®ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•ç”Ÿæˆçš„å¯¹æŠ—æ ·æœ¬è¡¨ç°å‡ºæ˜æ˜¾çš„å™ªå£°ã€‚(3) è®ºæ–‡æå‡ºçš„æ–¹æ³•ï¼šä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œå—è¯„åˆ†åŒ¹é…å’Œä¸å—é™åˆ¶çš„å¯¹æŠ—æ ·æœ¬çš„å¯å‘ï¼Œæå‡ºäº† AdvDiffVLMï¼Œå®ƒä½¿ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆè‡ªç„¶ã€ä¸å—é™åˆ¶çš„å¯¹æŠ—æ ·æœ¬ã€‚å…·ä½“æ¥è¯´ï¼Œåˆ©ç”¨å¹¶ä¿®æ”¹äº†é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„åå‘ç”Ÿæˆè¿‡ç¨‹ï¼Œå…¶ä¸­åˆ©ç”¨è‡ªé€‚åº”é›†æˆæ¢¯åº¦ä¼°è®¡æ¥æ”¹å˜è¯„åˆ†å¹¶å°†ç›®æ ‡è¯­ä¹‰åµŒå…¥å¯¹æŠ—æ ·æœ¬ä¸­ã€‚ä¸ºäº†å¢å¼ºè¾“å‡ºçš„è‡ªç„¶æ€§ï¼Œå¼•å…¥äº† GradCAM å¼•å¯¼æ©ç ï¼Œå®ƒå°†å¯¹æŠ—ç›®æ ‡è¯­ä¹‰åˆ†æ•£åœ¨å¯¹æŠ—æ ·æœ¬ä¸­ï¼Œè€Œä¸æ˜¯å°†å®ƒä»¬é›†ä¸­åœ¨ç‰¹å®šåŒºåŸŸï¼Œä»è€Œæé«˜å›¾åƒè´¨é‡ã€‚(4) æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šè¯¥æ–¹æ³•åªéœ€è¦å‡ ä¸ªåå‘å»å™ªæ­¥éª¤å³å¯ç”Ÿæˆå¯¹æŠ—æ ·æœ¬ï¼Œä½¿å…¶æ˜æ˜¾å¿«äºä»¥å‰å…¬å¸ƒçš„åŸºäºè¿ç§»çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒAdvDiffVLM é€šè¿‡å»å™ªç”Ÿæˆå¯¹æŠ—æ ·æœ¬ï¼Œå¯¹é˜²å¾¡æ–¹æ³•è¡¨ç°å‡ºæ›´é«˜çš„é²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAdvDiffVLM åœ¨ç›®æ ‡å’Œè¿ç§»åœºæ™¯ä¸­é’ˆå¯¹æœ€å…ˆè¿›çš„åŸºäºè¿ç§»çš„æ”»å‡»å®ç°äº†æ•°é‡çº§æˆ–æ›´å¤§çš„åŠ é€Ÿï¼ŒåŒæ—¶æä¾›äº†å…·æœ‰æ›´é«˜å›¾åƒè´¨é‡çš„å¯¹æŠ—æ ·æœ¬ã€‚æ­¤å¤–ï¼Œè¿™äº›å¯¹æŠ—æ ·æœ¬åœ¨è·¨ä¸åŒ VLMï¼ˆåŒ…æ‹¬å•†ä¸š VLMï¼Œå¦‚ GPT-4Vï¼‰çš„è¿ç§»ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„å¯è¿ç§»æ€§ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1): ä»è¯„åˆ†åŒ¹é…çš„è§’åº¦å‡ºå‘ï¼Œå°†å¯¹æŠ—æ”»å‡»å»ºæ¨¡ä¸ºç”Ÿæˆè¿‡ç¨‹ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„æ•°æ®åˆ†å¸ƒç”Ÿæˆè‡ªç„¶ã€æ— çº¦æŸçš„å¯¹æŠ—æ ·æœ¬ã€‚(2): æå‡ºè‡ªé€‚åº”é›†æˆæ¢¯åº¦ä¼°è®¡æ–¹æ³•ï¼Œåˆ©ç”¨ CLIP æ¨¡å‹ä½œä¸ºä»£ç†æ¨¡å‹ï¼Œä¼°è®¡é»‘ç›’ VLM çš„æ¢¯åº¦ä¿¡æ¯ã€‚(3): å¼•å…¥ GradCAM å¼•å¯¼æ©ç ï¼Œå°†å¯¹æŠ—ç›®æ ‡è¯­ä¹‰åˆ†æ•£åœ¨å¯¹æŠ—æ ·æœ¬ä¸­ï¼Œæé«˜å›¾åƒè´¨é‡ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ— çº¦æŸå¯¹æŠ—æ ·æœ¬ç”Ÿæˆæ–¹æ³• AdvDiffVLMï¼Œå®ƒåœ¨ç›®æ ‡è¿ç§»åœºæ™¯ä¸‹é«˜æ•ˆåœ°ç”Ÿæˆå¯¹æŠ—æ ·æœ¬ã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºè‡ªé€‚åº”é›†æˆæ¢¯åº¦ä¼°è®¡æ–¹æ³•ï¼Œåˆ©ç”¨ CLIP æ¨¡å‹ä½œä¸ºä»£ç†æ¨¡å‹ï¼Œä¼°è®¡é»‘ç›’ VLM çš„æ¢¯åº¦ä¿¡æ¯ã€‚</li><li>å¼•å…¥ GradCAM å¼•å¯¼æ©ç ï¼Œå°†å¯¹æŠ—ç›®æ ‡è¯­ä¹‰åˆ†æ•£åœ¨å¯¹æŠ—æ ·æœ¬ä¸­ï¼Œæé«˜å›¾åƒè´¨é‡ã€‚æ€§èƒ½ï¼š</li><li>ä¸ç°æœ‰çš„ç³»ç»Ÿç›¸æ¯”ï¼ŒAdvDiffVLM çš„é€Ÿåº¦æé«˜äº† 10 å€åˆ° 30 å€ã€‚</li><li>ç”Ÿæˆçš„å¯¹æŠ—æ ·æœ¬å…·æœ‰è¾ƒé«˜çš„å›¾åƒè´¨é‡å’Œé²æ£’æ€§ã€‚å·¥ä½œé‡ï¼š</li><li>AdvDiffVLM åœ¨ä¸åŒçš„ VLMï¼ˆåŒ…æ‹¬å•†ä¸š VLMï¼Œå¦‚ GPT-4Vï¼‰ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„å¯è¿ç§»æ€§ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="http://article.biliimg.com/bfs/new_dyn/58f36ca28ba6f05864f978063dc05641241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/2e4d299418d7d5553271b20ba81e4ebf241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/aeab2739defd1322978a3eff9a615297241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/70b668e1887faf1bb1428acdafb7a9fc241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/888909a0a5fe2c07ff40b91188d0a47d241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/cb46778d4b426e082ad902ea978cc0fa241286257.jpg" align="middle"></details>## OneActor: Consistent Character Generation via Cluster-Conditioned   Guidance**Authors:Jiahao Wang, Caixia Yan, Haonan Lin, Weizhan Zhang**Text-to-image diffusion models benefit artists with high-quality image generation. Yet its stochastic nature prevent artists from creating consistent images of the same character. Existing methods try to tackle this challenge and generate consistent content in various ways. However, they either depend on external data or require expensive tuning of the diffusion model. For this issue, we argue that a lightweight but intricate guidance is enough to function. Aiming at this, we lead the way to formalize the objective of consistent generation, derive a clustering-based score function and propose a novel paradigm, OneActor. We design a cluster-conditioned model which incorporates posterior samples to guide the denoising trajectories towards the target cluster. To overcome the overfitting challenge shared by one-shot tuning pipelines, we devise auxiliary components to simultaneously augment the tuning and regulate the inference. This technique is later verified to significantly enhance the content diversity of generated images. Comprehensive experiments show that our method outperforms a variety of baselines with satisfactory character consistency, superior prompt conformity as well as high image quality. And our method is at least 4 times faster than tuning-based baselines. Furthermore, to our best knowledge, we first prove that the semantic space has the same interpolation property as the latent space dose. This property can serve as another promising tool for fine generation control. [PDF](http://arxiv.org/abs/2404.10267v1) **Summary**é€šè¿‡å¼•å…¥èšç±»æ¡ä»¶æ¨¡å‹å’Œä¸€ä¸ªæ–°çš„èŒƒä¾‹OneActorï¼Œè¯¥ç ”ç©¶å®ç°äº†æ— ç›‘ç£ä¸€è‡´å›¾åƒç”Ÿæˆï¼ŒåŒæ—¶ä¿æŒå›¾åƒè´¨é‡å’Œå¤šæ ·æ€§ã€‚**Key Takeaways*** æ— éœ€å¤–éƒ¨æ•°æ®æˆ–æ˜‚è´µçš„å¾®è°ƒå³å¯å®ç°ä¸€è‡´å›¾åƒç”Ÿæˆã€‚* æå‡ºäº†ä¸€ç§åŸºäºèšç±»çš„è¯„åˆ†å‡½æ•°ï¼Œå°†åéªŒæ ·æœ¬çº³å…¥å»å™ªè½¨è¿¹ã€‚* ä¸ºå¾®è°ƒå’Œæ¨ç†è®¾è®¡äº†è¾…åŠ©ç»„ä»¶ä»¥å¢å¼ºå¤šæ ·æ€§å’Œé¿å…è¿‡æ‹Ÿåˆã€‚* å®éªŒè¡¨æ˜è¯¥æ–¹æ³•åœ¨å­—ç¬¦ä¸€è‡´æ€§ã€æç¤ºç¬¦åˆæ€§å’Œå›¾åƒè´¨é‡æ–¹é¢ä¼˜äºåŸºçº¿ã€‚* å¾®è°ƒé€Ÿåº¦è‡³å°‘æ˜¯åŸºäºå¾®è°ƒçš„åŸºçº¿çš„ 4 å€ã€‚* è¯æ˜äº†è¯­ä¹‰ç©ºé—´å…·æœ‰ä¸æ½œåœ¨ç©ºé—´ç›¸åŒçš„æ’å€¼å±æ€§ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>æ ‡é¢˜ï¼šOneActorï¼šé€šè¿‡é›†ç¾¤æ¡ä»¶å¼•å¯¼å®ç°ä¸€è‡´çš„è§’è‰²ç”Ÿæˆ</li><li>ä½œè€…ï¼šJiahao Wangã€Caixia Yanã€Haonan Linã€Weizhan Zhang</li><li>å•ä½ï¼šè¥¿å®‰äº¤é€šå¤§å­¦è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯å­¦é™¢</li><li>å…³é”®è¯ï¼šæ–‡æœ¬åˆ°å›¾åƒã€æ‰©æ•£æ¨¡å‹ã€ä¸€è‡´æ€§ç”Ÿæˆã€é›†ç¾¤å¼•å¯¼</li><li>é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.10267</li><li>æ‘˜è¦ï¼š   (1) ç ”ç©¶èƒŒæ™¯ï¼šæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸ºè‰ºæœ¯å®¶æä¾›äº†é«˜è´¨é‡çš„å›¾åƒç”Ÿæˆèƒ½åŠ›ï¼Œä½†å…¶éšæœºæ€§ä½¿å¾—è‰ºæœ¯å®¶æ— æ³•åˆ›å»ºåŒä¸€è§’è‰²çš„ä¸€è‡´å›¾åƒã€‚   (2) è¿‡å»æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•é€šè¿‡å„ç§æ–¹å¼å°è¯•è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œä½†å®ƒä»¬è¦ä¹ˆä¾èµ–äºå¤–éƒ¨æ•°æ®ï¼Œè¦ä¹ˆéœ€è¦å¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œæ˜‚è´µçš„å¾®è°ƒã€‚   (3) ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§ä½†å¤æ‚çš„æŒ‡å¯¼æ–¹æ³•ï¼Œé€šè¿‡å½¢å¼åŒ–ä¸€è‡´æ€§ç”Ÿæˆçš„ç›®æ ‡ã€æ¨å¯¼åŸºäºèšç±»çš„è¯„åˆ†å‡½æ•°ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°èŒƒå¼ OneActorã€‚è®¾è®¡äº†ä¸€ä¸ªé›†ç¾¤æ¡ä»¶æ¨¡å‹ï¼Œå°†åéªŒæ ·æœ¬çº³å…¥å…¶ä¸­ï¼Œå¼•å¯¼å»å™ªè½¨è¿¹æœå‘ç›®æ ‡é›†ç¾¤ã€‚ä¸ºäº†å…‹æœå•æ¬¡å¾®è°ƒç®¡é“ä¸­å¸¸è§çš„è¿‡åº¦æ‹ŸåˆæŒ‘æˆ˜ï¼Œè®¾è®¡äº†è¾…åŠ©ç»„ä»¶æ¥åŒæ—¶å¢å¼ºå¾®è°ƒå’Œè°ƒèŠ‚æ¨ç†ã€‚   (4) å®éªŒç»“æœï¼šè¯¥æ–¹æ³•åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå…·æœ‰ä»¤äººæ»¡æ„çš„è§’è‰²ä¸€è‡´æ€§ã€å‡ºè‰²çš„æç¤ºç¬¦åˆæ€§ä»¥åŠè¾ƒé«˜çš„å›¾åƒè´¨é‡ã€‚å¹¶ä¸”è¯¥æ–¹æ³•è‡³å°‘æ¯”åŸºäºå¾®è°ƒçš„åŸºçº¿å¿« 4 å€ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡é¦–æ¬¡è¯æ˜è¯­ä¹‰ç©ºé—´ä¸æ½œåœ¨ç©ºé—´å…·æœ‰ç›¸åŒçš„æ’å€¼ç‰¹æ€§ï¼Œè¯¥ç‰¹æ€§å¯ä»¥ä½œä¸ºç»†ç²’åº¦ç”Ÿæˆæ§åˆ¶çš„å¦ä¸€æœ‰å‰é€”çš„å·¥å…·ã€‚</li></ol><p>7.æ–¹æ³•ï¼š(1)å®šä¹‰é—®é¢˜ï¼šç»™å®šç”¨æˆ·å®šä¹‰çš„æè¿°æç¤º ptï¼ˆä¾‹å¦‚ï¼Œä¸€ä¸ªç©¿ç€é•¿è¢çš„éœæ¯”ç‰¹äººï¼‰ï¼Œæ ‡å‡†æ‰©æ•£æ¨¡å‹ ÏµÎ¸ ç”Ÿæˆç”¨æˆ·é¦–é€‰å›¾åƒ xt ä½œä¸ºç›®æ ‡è§’è‰²ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä¸ºåŸå§‹ ÏµÎ¸ é…å¤‡ä¸€ä¸ªæ”¯æŒç½‘ç»œ Ï•ï¼Œåˆ¶å®š ÏµÎ¸,Ï•ã€‚åœ¨å¯¹ Ï• è¿›è¡Œå¿«é€Ÿå¾®è°ƒåï¼Œæˆ‘ä»¬çš„æ¨¡å‹ ÏµÎ¸,Ï• å¯ä»¥ç”Ÿæˆå…·æœ‰ä»»ä½•å…¶ä»–ä»¥è§’è‰²ä¸ºä¸­å¿ƒçš„æç¤ºï¼ˆä¾‹å¦‚ï¼Œä¸€ä¸ªç©¿ç€é•¿è¢çš„éœæ¯”ç‰¹äºº + åœ¨è¡—ä¸Šè¡Œèµ°ï¼‰çš„åŒä¸€è§’è‰²çš„ä¸€è‡´å›¾åƒã€‚ä¸ºäº†å®Œæˆè¿™é¡¹ä»»åŠ¡ï¼Œæˆ‘ä»¬åœ¨ç¬¬ 4.2 èŠ‚ä¸­é¦–å…ˆè¿›è¡Œæ•°å­¦åˆ†æã€‚ç„¶åæˆ‘ä»¬åœ¨ç¬¬ 4.3 èŠ‚ä¸­æ„å»ºä¸€ä¸ªé›†ç¾¤æ¡ä»¶æ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªè‡ªå¼•å¯¼æ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬åœ¨ç¬¬ 4.4 èŠ‚ä¸­ä»¥å¯¹æ¯”æ–¹å¼å¯¹å…¶è¿›è¡Œå¾®è°ƒã€‚æœ€ååœ¨æ¨ç†æœŸé—´ï¼Œæˆ‘ä»¬åœ¨ç¬¬ 4.5 èŠ‚ä¸­ä½¿ç”¨è¯­ä¹‰æ’å€¼ç”Ÿæˆå„ç§ä¸€è‡´çš„å›¾åƒï¼Œå¹¶åœ¨ç¬¬ 4.6 èŠ‚ä¸­ä½¿ç”¨æ½œåœ¨å¼•å¯¼ã€‚(2)é›†ç¾¤å¼•å¯¼è¯„åˆ†å‡½æ•°çš„æ¨å¯¼ï¼šç»™å®šç”¨æˆ·å®šä¹‰çš„æç¤º ptï¼ˆä¾‹å¦‚ï¼Œä¸€ä¸ªç©¿ç€ç™½è‰²è¿è¡£è£™çš„ç¾ä¸½å¥³å­©ï¼‰å’Œç›¸åº”çš„ base è¯ wbï¼ˆä¾‹å¦‚ï¼Œå¥³å­©ï¼‰ï¼Œæˆ‘ä»¬å°† pt è¾“å…¥æ ‡å‡†æ‰©æ•£æ¨¡å‹ ÏµÎ¸ ä»¥è·å¾— N ä¸ª base å›¾åƒ Xbase={xbasei}Ni=1ã€‚æˆ‘ä»¬éšæœºé€‰æ‹©ä¸€å¼ å›¾ç‰‡ä½œä¸ºç›®æ ‡è§’è‰² xtarï¼Œå¹¶å°†å…¶ä»–å›¾ç‰‡æ ‡è®°ä¸ºè¾…åŠ©æ ·æœ¬ Xaux={xauxi}Nâˆ’1i=1ã€‚æˆ‘ä»¬å¯¹ç›®æ ‡å›¾åƒåº”ç”¨äººè„¸è£å‰ªå’Œå›¾åƒç¿»è½¬ï¼Œå¾—åˆ°å¢å¼ºé›† Xtar={xtari}Mi=1ã€‚åœ¨æ ‡å‡†æ½œåœ¨æ‰©æ•£æ¨¡å‹ ÏµÎ¸(zt, t, c) ä¸­ï¼Œç”¨äºæ–‡æœ¬æ§åˆ¶çš„æ¡ä»¶ c ç”±æ–‡æœ¬ç¼–ç å™¨ Et ç”Ÿæˆï¼Œè¯¥ç¼–ç å™¨å°†æ–‡æœ¬æç¤º p æŠ•å½±åˆ°è¯­ä¹‰å‘é‡ï¼šc=Et(p)ã€‚ç„¶åè¿™äº›è¯­ä¹‰å‘é‡å¼•å¯¼å»å™ªç½‘ç»œä»åˆå§‹æ½œåœ¨å™ªå£° zT é‡‡æ ·å»å™ª z0ã€‚åœ¨æˆ‘ä»¬çš„ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬å‡è®¾åœ¨ z0 çš„æ½œåœ¨ç©ºé—´ä¸­ï¼Œæœ‰ä¸åŒçš„é›†ç¾¤å¯¹åº”äºè§’è‰²çš„ä¸åŒæ˜¾ç€èº«ä»½ã€‚ç»™å®šä¸åŒçš„åˆå§‹ zT å’Œç›¸åŒçš„å­—ç¬¦æ¡ä»¶ ptï¼ŒÏµÎ¸ æ— æ³•åˆ°è¾¾ä¸€ä¸ªç‰¹å®šçš„é›†ç¾¤ï¼Œè€Œæ˜¯æ‰©æ•£åˆ°ä¸åŒé›†ç¾¤çš„åŒºåŸŸã€‚ä¹Ÿå°±æ˜¯è¯´ï¼ŒXbase çš„ç”Ÿæˆå½¢æˆä¸€ä¸ª base åŒºåŸŸ Sbaseï¼Œå…¶ä¸­åŒ…æ‹¬ä¸€ä¸ªç›®æ ‡é›†ç¾¤å’Œå‡ ä¸ªè¾…åŠ©é›†ç¾¤ï¼šSbaseâŠƒStar1âˆªSaux1âˆªSaux2âˆª... ä¸€è‡´æ€§ç”Ÿæˆçš„å…³é”®æ˜¯å¦‚ä½•å¼•å¯¼ ÏµÎ¸ åˆ°é¢„æœŸçš„ç›®æ ‡é›†ç¾¤ Starã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å½¢å¼åŒ–ä¸€è‡´æ€§ç”Ÿæˆé—®é¢˜ã€‚ä»é¢å‘ç»“æœçš„è§’åº¦æ¥çœ‹ï¼Œæˆ‘ä»¬å¸Œæœ›å¢åŠ ç”Ÿæˆç›®æ ‡é›†ç¾¤ Star å›¾åƒçš„æ¦‚ç‡ï¼Œå¹¶é™ä½è¾…åŠ©é›†ç¾¤ Sauxi çš„æ¦‚ç‡ã€‚å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬å°†åŸå§‹æ‰©æ•£è¿‡ç¨‹è§†ä¸ºå…ˆéªŒåˆ†å¸ƒ p(x)ï¼Œåˆ™æˆ‘ä»¬çš„æœŸæœ›åˆ†å¸ƒå¯ä»¥è¡¨ç¤ºä¸ºï¼šp(x)Â·p(Star|x)Î·1 / Nâˆ’1i=1p(Sauxi|x)Î·2ï¼Œå…¶ä¸­ Î·1ã€Î·2 æ˜¯æ¯”ä¾‹å› å­ã€‚æˆ‘ä»¬åº”ç”¨è´å¶æ–¯è§„åˆ™æ¨å¯¼å‡ºï¼šp(x)Â·(p(x|Star)p(Star)p(x))Î·1 / Nâˆ’1i=1(p(x|Sauxi)p(Sauxi)p(x))Î·2ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å–å…¶å¯¹æ•°æ¦‚ç‡çš„æ¢¯åº¦å¹¶å¿½ç•¥æ— å…³é¡¹ï¼Œå¾—åˆ°ï¼šâˆ‡logp(x)+Î·1Â·[âˆ‡logp(x|Star)âˆ’âˆ‡logp(x)]âˆ’Î·2Â·Nâˆ’1i=1[âˆ‡logp(x|Sauxi)âˆ’âˆ‡logp(x)]ã€‚å¦‚æœæˆ‘ä»¬å¼•å…¥è¯„åˆ†å‡½æ•°çš„æ¦‚å¿µï¼ˆSong ç­‰äººï¼Œ[2021]ï¼‰ï¼ŒEq.6 ä¸­çš„æ¯ä¸ªæœ¯è¯­éƒ½è¡¨ç¤ºä¸€ä¸ªè¯„åˆ†å¹¶æŒ‡å¯¼æ¨ç†è¿‡ç¨‹ã€‚åˆ©ç”¨ Ho ç­‰äºº [2020] çš„é‡æ–°å‚æ•°åŒ–æŠ€å·§ï¼Œæˆ‘ä»¬å¯ä»¥å°†è¯„åˆ†è¡¨ç¤ºä¸ºæ½œåœ¨ç©ºé—´ä¸­å»å™ªç½‘ç»œ Î¸ çš„é¢„æµ‹ï¼šÏµÎ¸(zt, t)+Î·1Â·[ÏµÎ¸(zt, t, Star)âˆ’ÏµÎ¸(zt, t)]âˆ’Î·2Â·Nâˆ’1i=1[ÏµÎ¸(zt, t, Sauxi)âˆ’ÏµÎ¸(zt, t)]ã€‚è¿™ä¸ªå…¬å¼ç§°ä¸ºé›†ç¾¤å¼•å¯¼è¯„åˆ†å‡½æ•°ï¼Œæ˜¯æˆ‘ä»¬å·¥ä½œçš„æ ¸å¿ƒã€‚æˆ‘ä»¬å°†åœ¨åç»­éƒ¨åˆ†ä¸­å®ç°å®ƒã€‚(3)ä½¿ç”¨è¯­ä¹‰è¡¨ç¤ºçš„é›†ç¾¤æ¡ä»¶æ¨¡å‹ï¼šæ ¹æ® Eq.7ï¼Œæˆ‘ä»¬éœ€è¦å°†é›†ç¾¤è¡¨ç¤ºå¼•å…¥æˆ‘ä»¬çš„ç®¡é“å¹¶å»ºç«‹ä¸€ä¸ªé›†ç¾¤æ¡ä»¶æ¨¡å‹ ÏµÎ¸(zt, t, S)ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†æ ·æœ¬çš„æ½œåœ¨ä»£ç è§†ä¸ºé›†ç¾¤è¡¨ç¤ºï¼Œå¹¶å°† Ï• æ„å»ºä¸ºç¼–ç å™¨ã€‚å¦‚å›¾ 2 æ‰€ç¤ºï¼Œå®ƒå°†æ½œåœ¨ä»£ç  z ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¾“å‡ºä¸€ä¸ªç‰¹å®šäºè§’è‰²çš„å‘é‡ âˆ†cã€‚æ­¤å‘é‡è¡¨ç¤ºè§’è‰²é›†ç¾¤çš„è¯­ä¹‰æ–¹å‘ã€‚ç»™å®šæ–‡æœ¬æç¤º pï¼Œæˆ‘ä»¬å°†æç¤ºåµŒå…¥æ‹†åˆ†ä¸ºé€è¯åµŒå…¥ï¼šc={cwi}ï¼Œå…¶ä¸­ wi æ˜¯æç¤ºçš„ç¬¬ i ä¸ªå•è¯ã€‚æˆ‘ä»¬å°† base è¯åµŒå…¥ cw å’Œåç§»è¾“å‡ºå‘é‡è¿æ¥èµ·æ¥ï¼šcâ€²wb=cwb+âˆ†cã€‚ç›´è§‚åœ°è¯´ï¼ŒÏ• å°±åƒæ–‡æœ¬ç¼–ç å™¨ä¸€æ ·ï¼Œå°†æ½œåœ¨ä»£ç æŠ•å½±åˆ°è¯­ä¹‰åµŒå…¥ä¸­ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°† Ï• ç§°ä¸ºæ½œåœ¨ç¼–ç å™¨ã€‚æ½œåœ¨ç¼–ç å™¨ç”±ä¸€ä¸ªæå–å™¨å’Œä¸€ä¸ªæŠ•å½±ä»ªç»„æˆã€‚ç”±äºåŸå§‹ U-Net ç¼–ç å™¨ Eu å·²ç»ç»è¿‡è‰¯å¥½è®­ç»ƒï¼Œå¯ä»¥ä»æ½œåœ¨æ ·æœ¬ä¸­æå–ç‰¹å¾ï¼Œå› æ­¤æˆ‘ä»¬ç›´æ¥å°†å…¶ç”¨ä½œæå–å™¨ã€‚æŠ•å½±ä»ªæ˜¯ä¸€ä¸ªå¤šå±‚ ResNetï¼ˆHe ç­‰äººï¼Œ[2016]ï¼‰å’Œçº¿æ€§ç½‘ç»œï¼Œå¸¦æœ‰å±‚å½’ä¸€åŒ–ã€‚åœ¨å¾®è°ƒæœŸé—´ï¼Œæˆ‘ä»¬åªæ¿€å‘æŠ•å½±ä»ªå¹¶å†»ç»“æ‰€æœ‰å…¶ä»–ç»„ä»¶ã€‚ç„¶è€Œï¼ŒU-Net æå–å™¨å¯èƒ½ä¼šé€ æˆé¢å¤–çš„è®¡ç®—è´Ÿæ‹…ã€‚ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬ç”¨ z1 çš„ç‰¹å¾è¿‘ä¼¼ z0 çš„ç‰¹å¾ï¼šh=Eu(z1, c)â‰ˆEu(z0)ã€‚å› æ­¤ï¼Œåœ¨ç”Ÿæˆ base å›¾åƒæ—¶ï¼Œæˆ‘ä»¬åœ¨æœ€åä¸€ä¸ªé‡‡æ ·æ­¥éª¤ä¸­ä¿å­˜ U-Net ç¼–ç å™¨çš„è¾“å‡º H={hi}Ni=1ã€‚h å¯ä»¥è¿‘ä¼¼ä¸ºæå–å™¨çš„è¾“å‡ºã€‚åœ¨å¾®è°ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­ï¼Œh ç›´æ¥é¦ˆé€åˆ°æŠ•å½±ä»ªï¼šâˆ†c=Ï•(h)ã€‚é€šè¿‡è¿™ç§è¿‘ä¼¼ï¼Œæˆ‘ä»¬é¿å…äº† U-Net æå–å™¨çš„é¢å¤–è®¡ç®—ï¼Œå¹¶å°†è®¡ç®—æˆæœ¬é™ä½äº† 30%ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæ‰€æœ‰å¯ä»¥ç¡®å®šé›†ç¾¤çš„å› ç´ ï¼ˆå³ pã€hï¼‰éƒ½å¯ä»¥ç”±æ–‡æœ¬ç¼–ç å™¨å’Œæ½œåœ¨ç¼–ç å™¨å¤„ç†ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å°† Eq.7 ä¸­çš„é›†ç¾¤æ¡ä»¶é¡¹è¡¨ç¤ºä¸ºï¼šÏµÎ¸(zt, t, S)=ÏµÎ¸,Ï•(zt, t, Et(p), Ï•(h))ã€‚(4)ä½¿ç”¨è¾…åŠ©æ ·æœ¬çš„å¹¿ä¹‰ç®€åŒ–å¾®è°ƒï¼šåœ¨æˆ‘ä»¬çš„é›†ç¾¤æ¡ä»¶æ¨¡å‹çš„å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œæ‰©æ•£æ¨¡å‹çš„å†…åœ¨å±æ€§æ˜¯å…³é”®æ–¹é¢ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å……åˆ†åˆ©ç”¨ç›®æ ‡å’Œè¾…åŠ©æ ·æœ¬ï¼šå¹³è¡¡å’Œç¨³å®šæŠ•å½±ä»ªã€‚ä¸€æ¬¡æ€§å¾®è°ƒä¸­çš„ä¸»è¦æŒ‘æˆ˜æ˜¯è¿‡æ‹Ÿåˆã€‚æ•°æ®çš„ä¸è¶³å¯èƒ½ä¼šå¯¼è‡´ä¸¥é‡çš„åå·®å’Œç”Ÿæˆå›¾åƒçš„å¤šæ ·æ€§æœ‰é™ã€‚ä¸ºäº†å…‹æœè¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬ä¸ä»…ä½¿ç”¨ç›®æ ‡æ ·æœ¬ï¼Œè¿˜ä½¿ç”¨è¾…åŠ©æ ·æœ¬å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬éšæœºé€‰æ‹© 1 ä¸ªç›®æ ‡å’Œ K ä¸ªè¾…åŠ©æ ·æœ¬æ¥å½¢æˆä¸€æ‰¹æ•°æ®ï¼šB={xtar, htar}âˆª{xauxi, hauxi}Ki=1ã€‚å› æ­¤ï¼Œæ›´å¤šçš„æ•°æ®æœ‰åŠ©äºä¼˜åŒ–ï¼Œå¹¶ä¸”æ‰¹å¤„ç†å½’ä¸€åŒ–å¯ä»¥åº”ç”¨äºæŠ•å½±ä»ªï¼Œä»è€Œäº§ç”Ÿæ›´é€šç”¨çš„æŠ•å½±ã€‚å¯¹äºå¾®è°ƒç®¡é“ï¼Œæˆ‘ä»¬é¦–å…ˆé€šè¿‡ï¼šz=Ea(x), xâˆˆB ä¸ºå®ƒä»¬æ·»åŠ å™ªå£° Ïµt æ¥è·å¾—æ½œåœ¨ä»£ç ã€‚ç„¶åå¦‚å›¾æ‰€ç¤ºï¼Œæˆ‘ä»¬å°†å™ªå£°æ½œåœ¨ ztã€æç¤º p å’Œç‰¹å¾ h è¾“å…¥é›†ç¾¤æ¡ä»¶æ¨¡å‹ã€‚æç¤ºæ˜¯ä¸€ä¸ªéšæœºæ¨¡æ¿ï¼Œå¡«å……æœ‰ base è¯ï¼ˆä¾‹å¦‚ï¼Œä¸€ä¸ªå¥³å­©çš„è‚–åƒï¼‰ã€‚æˆ‘ä»¬å¯¹ç›®æ ‡å’Œè¾…åŠ©æ ·æœ¬åº”ç”¨æ ‡å‡†å»å™ªæŸå¤±ï¼šLtar(Ï•)=Etâˆˆ[1,T],ztar0,Ïµtâˆ¥Ïµtâˆ’ÏµÎ¸,Ï•(ztart, t, Et(p), Ï•(htar))âˆ¥2ï¼ŒLaux(Ï•)=Etâˆˆ[1,T],zaux0,Ïµtâˆ¥Ïµtâˆ’ÏµÎ¸,Ï•(zauxt, t, Et(p), Ï•(haux))âˆ¥2ã€‚å¯¹äºè¾…åŠ©é¡¹ Eq.7ï¼Œè®¡ç®—æ¯ä¸ªæ­¥éª¤ N-1 ä¸ªè¾…åŠ©æ¡ä»¶çš„å»å™ªé¢„æµ‹éå¸¸è´¹åŠ›ã€‚ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¹³å‡æ¡ä»¶ âˆ†Â¯cã€‚å®ƒæ˜¯é€šè¿‡å¯¹è¾…åŠ©æ¡ä»¶çš„æ¡ä»¶å‘é‡æ±‚å¹³å‡è·å¾—çš„ï¼šâˆ†Â¯c=1KKi=1âˆ†cauxi=1KKi=1Ï•(hauxi)ã€‚ç›´è§‚åœ°è¯´ï¼Œè¿™ä¸ªå¹³å‡æ¡ä»¶è¡¨ç¤ºæ‰€æœ‰è¾…åŠ©é›†ç¾¤çš„ä¸­å¿ƒã€‚å› æ­¤ï¼Œå¯ä»¥ç”¨è¿™ä¸ªå¹³å‡é¢„æµ‹æ¥è¿‘ä¼¼è¾…åŠ©å»å™ªé¢„æµ‹ã€‚å»å™ªæŸå¤±ä¹Ÿç”¨äºæ­¤æ¡ä»¶ï¼šLaver(Ï•)=Etâˆˆ[1,T],ztar0,Ïµtâˆ¥Ïµtâˆ’ÏµÎ¸,Ï•(ztart, t, Et(p), âˆ†Â¯c)âˆ¥2ã€‚è¯·æ³¨æ„ï¼Œè¿™é‡Œçš„æ•°æ®æ¥è‡ªç›®æ ‡é›†ã€‚æ­¤å¹³å‡æ¡ä»¶å°†å……å½“æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆHo å’Œ Salimans [2022]ï¼‰çš„ç©ºæ¡ä»¶ï¼Œå¦‚ç¬¬ 4.6 èŠ‚ä¸­æ‰€è¿°ã€‚(5)è¯­ä¹‰æ’å€¼çš„è¯æ˜å’Œå®ç°ï¼šåœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œä¼ ç»Ÿçš„åŸºäºå¾®è°ƒçš„ç®¡é“åœ¨ä¸€è‡´æ€§å’Œå¤šæ ·æ€§ä¹‹é—´å­˜åœ¨ç›¸åŒçš„å¤±è¡¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ¸©å’Œçš„ç­–ç•¥ï¼Œè¯­ä¹‰æ’å€¼ï¼Œå®ƒåˆ©ç”¨äº†æ‰©æ•£æ¨¡å‹çš„å†…éƒ¨å®¹é‡ã€‚æ— åˆ†ç±»å™¨å¼•å¯¼æ¨¡å‹ï¼ˆHo å’Œ Salimans [2022]ï¼‰è¯æ˜äº†æ½œåœ¨ç©ºé—´ä¸­çš„æ¡ä»¶æ’å€¼å’Œå¤–æ¨ï¼Œè¿™è¡¨æ˜äº†æˆ‘ä»¬çš„è®ºç‚¹ã€‚</p><p>8.ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§ä¸”é«˜æ•ˆçš„æŒ‡å¯¼æ–¹æ³•ï¼Œé€šè¿‡å½¢å¼åŒ–ä¸€è‡´æ€§ç”Ÿæˆçš„ç›®æ ‡ã€æ¨å¯¼åŸºäºèšç±»çš„è¯„åˆ†å‡½æ•°ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°èŒƒå¼ OneActorï¼Œè§£å†³äº†æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­è§’è‰²ä¸€è‡´æ€§ç”Ÿæˆçš„é—®é¢˜ã€‚è¯¥æ–¹æ³•åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå…·æœ‰ä»¤äººæ»¡æ„çš„è§’è‰²ä¸€è‡´æ€§ã€å‡ºè‰²çš„æç¤ºç¬¦åˆæ€§ä»¥åŠè¾ƒé«˜çš„å›¾åƒè´¨é‡ã€‚å¹¶ä¸”è¯¥æ–¹æ³•è‡³å°‘æ¯”åŸºäºå¾®è°ƒçš„åŸºçº¿å¿« 4 å€ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡é¦–æ¬¡è¯æ˜è¯­ä¹‰ç©ºé—´ä¸æ½œåœ¨ç©ºé—´å…·æœ‰ç›¸åŒçš„æ’å€¼ç‰¹æ€§ï¼Œè¯¥ç‰¹æ€§å¯ä»¥ä½œä¸ºç»†ç²’åº¦ç”Ÿæˆæ§åˆ¶çš„å¦ä¸€æœ‰å‰é€”çš„å·¥å…·ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäºé›†ç¾¤æ¡ä»¶å¼•å¯¼çš„è½»é‡çº§ä¸”é«˜æ•ˆçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œå®ç°äº†è§’è‰²ä¸€è‡´æ€§ç”Ÿæˆã€‚æ€§èƒ½ï¼šåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå…·æœ‰ä»¤äººæ»¡æ„çš„è§’è‰²ä¸€è‡´æ€§ã€å‡ºè‰²çš„æç¤ºç¬¦åˆæ€§ä»¥åŠè¾ƒé«˜çš„å›¾åƒè´¨é‡ã€‚å¹¶ä¸”è¯¥æ–¹æ³•è‡³å°‘æ¯”åŸºäºå¾®è°ƒçš„åŸºçº¿å¿« 4 å€ã€‚å·¥ä½œé‡ï¼šè¯¥æ–¹æ³•çš„å®ç°ç›¸å¯¹ç®€å•ï¼Œå¹¶ä¸”å¯ä»¥åœ¨å„ç§æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸Šè½»æ¾éƒ¨ç½²ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="http://article.biliimg.com/bfs/new_dyn/89b6f16a0a761f249e333f44a5168204241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/ac080ab3aeb6fbbdbf4314b392f0a4e6241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/a05f66aebcfc1504a123423e38d9d751241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/81b50b6c224994210d30702ea8e901e3241286257.jpg" align="middle"></details><h2 id="Taming-Latent-Diffusion-Model-for-Neural-Radiance-Field-Inpainting"><a href="#Taming-Latent-Diffusion-Model-for-Neural-Radiance-Field-Inpainting" class="headerlink" title="Taming Latent Diffusion Model for Neural Radiance Field Inpainting"></a>Taming Latent Diffusion Model for Neural Radiance Field Inpainting</h2><p><strong>Authors:Chieh Hubert Lin, Changil Kim, Jia-Bin Huang, Qinbo Li, Chih-Yao Ma, Johannes Kopf, Ming-Hsuan Yang, Hung-Yu Tseng</strong></p><p>Neural Radiance Field (NeRF) is a representation for 3D reconstruction from multi-view images. Despite some recent work showing preliminary success in editing a reconstructed NeRF with diffusion prior, they remain struggling to synthesize reasonable geometry in completely uncovered regions. One major reason is the high diversity of synthetic contents from the diffusion model, which hinders the radiance field from converging to a crisp and deterministic geometry. Moreover, applying latent diffusion models on real data often yields a textural shift incoherent to the image condition due to auto-encoding errors. These two problems are further reinforced with the use of pixel-distance losses. To address these issues, we propose tempering the diffusion modelâ€™s stochasticity with per-scene customization and mitigating the textural shift with masked adversarial training. During the analyses, we also found the commonly used pixel and perceptual losses are harmful in the NeRF inpainting task. Through rigorous experiments, our framework yields state-of-the-art NeRF inpainting results on various real-world scenes. Project page: <a href="https://hubert0527.github.io/MALD-NeRF">https://hubert0527.github.io/MALD-NeRF</a> </p><p><a href="http://arxiv.org/abs/2404.09995v1">PDF</a> Project page: <a href="https://hubert0527.github.io/MALD-NeRF">https://hubert0527.github.io/MALD-NeRF</a></p><p><strong>Summary</strong><br>ä½¿ç”¨åœºæ™¯å®šåˆ¶å’Œå¯¹æŠ—æ€§è®­ç»ƒæ¥æ”¹è¿›ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å›¾åƒä¿®å¤ä¸­çš„æ‰©æ•£æ¨¡å‹ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æ‰©æ•£æ¨¡å‹åœ¨ NeRF å›¾åƒä¿®å¤ä¸­é¢ä¸´åˆæˆå‡ ä½•å’Œçº¹ç†å˜åŒ–æŒ‘æˆ˜ã€‚</li><li>åƒç´ å·®å¼‚æŸå¤±åœ¨ NeRF å›¾åƒä¿®å¤ä»»åŠ¡ä¸­æ˜¯æœ‰å®³çš„ã€‚</li><li>ä½¿ç”¨åœºæ™¯å®šåˆ¶å¯ä»¥å‡å°‘æ‰©æ•£æ¨¡å‹çš„éšæœºæ€§ã€‚</li><li>å¯¹æŠ—æ€§è®­ç»ƒå¯ä»¥å‡è½»çº¹ç†å˜åŒ–ã€‚</li><li>æ„ŸçŸ¥æŸå¤±åœ¨ NeRF å›¾åƒä¿®å¤ä¸­ä¹Ÿä¸ç†æƒ³ã€‚</li><li>è¯¥æ¡†æ¶åœ¨å„ç§çœŸå®åœºæ™¯ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„ NeRF å›¾åƒä¿®å¤ç»“æœã€‚</li><li>è¯¥æ–¹æ³•è§£å†³äº† NeRF å›¾åƒä¿®å¤ä¸­çš„ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šåˆæˆå‡ ä½•å’Œçº¹ç†å˜åŒ–ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šé©¯æœéšå¼æ‰©æ•£æ¨¡å‹ç”¨äºè¡¥å……ææ–™</li><li>ä½œè€…ï¼šC.H. Lin ç­‰</li><li>éš¶å±ï¼šæœªæåŠ</li><li>å…³é”®è¯ï¼šNeRFã€éšå¼æ‰©æ•£æ¨¡å‹ã€å›¾åƒç¼–è¾‘ã€å›¾åƒç”Ÿæˆã€æ·±åº¦å­¦ä¹ </li><li>è®ºæ–‡é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šNeRF æ˜¯ä»å¤šè§†å›¾å›¾åƒè¿›è¡Œä¸‰ç»´é‡å»ºçš„ä¸€ç§è¡¨ç¤ºæ–¹æ³•ã€‚å°½ç®¡æœ€è¿‘ä¸€äº›å·¥ä½œå±•ç¤ºäº†ä½¿ç”¨æ‰©æ•£å…ˆéªŒç¼–è¾‘é‡å»ºçš„ NeRF çš„åˆæ­¥æˆåŠŸï¼Œä½†å®ƒä»¬ä»ç„¶éš¾ä»¥åœ¨å®Œå…¨æœªè¦†ç›–çš„åŒºåŸŸä¸­åˆæˆåˆç†çš„å‡ ä½•å½¢çŠ¶ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šä¸€ä¸ªä¸»è¦åŸå› æ˜¯æ‰©æ•£æ¨¡å‹åˆæˆå†…å®¹çš„é«˜åº¦å¤šæ ·æ€§ï¼Œè¿™é˜»ç¢äº†è¾å°„åœºæ”¶æ•›åˆ°æ¸…æ™°ä¸”ç¡®å®šæ€§çš„å‡ ä½•å½¢çŠ¶ã€‚æ­¤å¤–ï¼Œç”±äºè‡ªåŠ¨ç¼–ç é”™è¯¯ï¼Œåœ¨çœŸå®æ•°æ®ä¸Šåº”ç”¨éšå¼æ‰©æ•£æ¨¡å‹é€šå¸¸ä¼šå¯¼è‡´ä¸å›¾åƒæ¡ä»¶ä¸ä¸€è‡´çš„çº¹ç†åç§»ã€‚è¿™ä¸¤ä¸ªé—®é¢˜åœ¨ä½¿ç”¨åƒç´ è·ç¦»æŸå¤±æ—¶è¿›ä¸€æ­¥åŠ å‰§ã€‚ï¼ˆ3ï¼‰æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºç”¨ç‰¹å®šäºåœºæ™¯çš„å®šåˆ¶æ¥ç¼“å’Œæ‰©æ•£æ¨¡å‹çš„éšæœºæ€§ï¼Œå¹¶ç”¨æ©ç å¯¹æŠ—æ€§è®­ç»ƒæ¥å‡è½»çº¹ç†åç§»ã€‚åœ¨åˆ†æè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬è¿˜å‘ç°å¸¸ç”¨çš„åƒç´ å’Œæ„ŸçŸ¥æŸå¤±åœ¨ NeRF inpainting ä»»åŠ¡ä¸­æœ‰å®³ã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†ä»€ä¹ˆæ€§èƒ½ï¼šé€šè¿‡ä¸¥æ ¼çš„å®éªŒï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨å„ç§çœŸå®ä¸–ç•Œåœºæ™¯ä¸Šäº§ç”Ÿäº†æœ€å…ˆè¿›çš„ NeRF inpainting ç»“æœã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†ä»–ä»¬çš„ç›®æ ‡ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1): æå‡ºäº†ä¸€ç§ç”¨ç‰¹å®šäºåœºæ™¯çš„å®šåˆ¶æ¥ç¼“å’Œæ‰©æ•£æ¨¡å‹çš„éšæœºæ€§ï¼Œå¹¶ç”¨æ©ç å¯¹æŠ—æ€§è®­ç»ƒæ¥å‡è½»çº¹ç†åç§»çš„æ–¹æ³•ï¼›(2): ä½¿ç”¨é¢„å…ˆåœ¨å†…éƒ¨å›¾åƒä¿®å¤æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹æ¥ä¿®å¤äºŒç»´å›¾åƒï¼Œç„¶åæ›¿æ¢ç”¨äº NeRF è®­ç»ƒçš„è¾“å…¥å›¾åƒï¼›(3): ä½¿ç”¨åƒç´ çº§æŸå¤±å‡½æ•°æ¥é‡å»ºè¾“å…¥å›¾åƒä¸­çš„å·²çŸ¥åŒºåŸŸï¼ŒåŒæ—¶ä½¿ç”¨æ©ç å¯¹æŠ—æ€§è®­ç»ƒæ¥ä¿®å¤ä¿®å¤åŒºåŸŸï¼›(4): è®¾è®¡äº†ä¸€ç§æ©ç å¯¹æŠ—æ€§è®­ç»ƒæ–¹æ¡ˆï¼Œä»åˆ¤åˆ«å™¨ä¸­éšè—å›¾åƒè¡¥ä¸ä¸Šçš„é‡å»º/ä¿®å¤è¾¹ç•Œï¼›(5): åˆ©ç”¨ç°æˆçš„å•ç›®æ·±åº¦å…ˆéªŒæ¥è§„èŒƒå­¦ä¹ åˆ°çš„ NeRF çš„å‡ ä½•å½¢çŠ¶ï¼›(6): ä½¿ç”¨è¿­ä»£æ•°æ®è®¾ç½®æ›´æ–°å’Œå™ªå£°è°ƒåº¦ï¼Œä»¥å‡è½»æ‰©æ•£æ¨¡å‹çš„å¤šæ ·æ€§å’Œéšæœºæ€§å¸¦æ¥çš„ä¸ä¸€è‡´é—®é¢˜ã€‚</p></li></ol><p>8.ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬å·¥ä½œçš„é‡è¦æ„ä¹‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äº NeRFinpainting çš„æ–°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡ç¼“è§£æ‰©æ•£æ¨¡å‹çš„éšæœºæ€§å’Œå‡è½»çº¹ç†åç§»æ¥æ˜¾ç€æé«˜ NeRF çš„ç¼–è¾‘è´¨é‡ã€‚ï¼ˆ2ï¼‰æœ¬æ–‡çš„åˆ›æ–°ç‚¹ã€æ€§èƒ½å’Œå·¥ä½œé‡ï¼šåˆ›æ–°ç‚¹ï¼š- æå‡ºäº†ä¸€ç§ç”¨ç‰¹å®šäºåœºæ™¯çš„å®šåˆ¶æ¥ç¼“å’Œæ‰©æ•£æ¨¡å‹çš„éšæœºæ€§ï¼Œå¹¶ç”¨æ©ç å¯¹æŠ—æ€§è®­ç»ƒæ¥å‡è½»çº¹ç†åç§»çš„æ–¹æ³•ã€‚- è®¾è®¡äº†ä¸€ç§æ©ç å¯¹æŠ—æ€§è®­ç»ƒæ–¹æ¡ˆï¼Œä»åˆ¤åˆ«å™¨ä¸­éšè—å›¾åƒè¡¥ä¸ä¸Šçš„é‡å»º/ä¿®å¤è¾¹ç•Œã€‚æ€§èƒ½ï¼š- åœ¨å„ç§çœŸå®ä¸–ç•Œåœºæ™¯ä¸Šäº§ç”Ÿäº†æœ€å…ˆè¿›çš„ NeRFinpainting ç»“æœã€‚å·¥ä½œé‡ï¼š- å®éªŒè®¾ç½®å¤æ‚ï¼Œéœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="http://article.biliimg.com/bfs/new_dyn/308cb7c2143fa740c4192a671925dee1241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/68fb1ff3b56d69424096a269f5033b6e241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/8d6fc19a0079ac001db1a67326957da6241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/bad14fb5377a3790949b10e7d83b5ddb241286257.jpg" align="middle"></details>## Ctrl-Adapter: An Efficient and Versatile Framework for Adapting Diverse   Controls to Any Diffusion Model**Authors:Han Lin, Jaemin Cho, Abhay Zala, Mohit Bansal**ControlNets are widely used for adding spatial control in image generation with different conditions, such as depth maps, canny edges, and human poses. However, there are several challenges when leveraging the pretrained image ControlNets for controlled video generation. First, pretrained ControlNet cannot be directly plugged into new backbone models due to the mismatch of feature spaces, and the cost of training ControlNets for new backbones is a big burden. Second, ControlNet features for different frames might not effectively handle the temporal consistency. To address these challenges, we introduce Ctrl-Adapter, an efficient and versatile framework that adds diverse controls to any image/video diffusion models, by adapting pretrained ControlNets (and improving temporal alignment for videos). Ctrl-Adapter provides diverse capabilities including image control, video control, video control with sparse frames, multi-condition control, compatibility with different backbones, adaptation to unseen control conditions, and video editing. In Ctrl-Adapter, we train adapter layers that fuse pretrained ControlNet features to different image/video diffusion models, while keeping the parameters of the ControlNets and the diffusion models frozen. Ctrl-Adapter consists of temporal and spatial modules so that it can effectively handle the temporal consistency of videos. We also propose latent skipping and inverse timestep sampling for robust adaptation and sparse control. Moreover, Ctrl-Adapter enables control from multiple conditions by simply taking the (weighted) average of ControlNet outputs. With diverse image/video diffusion backbones (SDXL, Hotshot-XL, I2VGen-XL, and SVD), Ctrl-Adapter matches ControlNet for image control and outperforms all baselines for video control (achieving the SOTA accuracy on the DAVIS 2017 dataset) with significantly lower computational costs (less than 10 GPU hours). [PDF](http://arxiv.org/abs/2404.09967v1) First two authors contributed equally; Project page:   https://ctrl-adapter.github.io/**Summary**é¢„è®­ç»ƒå›¾åƒæ§åˆ¶ç½‘ç»œé€šè¿‡ä¸€ä¸ªé«˜æ•ˆé€šç”¨æ¡†æ¶ Ctrl-Adapter æ‹“å±•è‡³è§†é¢‘ç”Ÿæˆï¼Œé€‚é…å„ç±»å›¾åƒ/è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œä¸ºè§†é¢‘æ§åˆ¶å¸¦æ¥å¤šæ ·åŠŸèƒ½ã€‚**Key Takeaways**- Ctrl-Adapter å°†é¢„è®­ç»ƒæ§åˆ¶ç½‘ç»œé€‚é…åˆ°æ‰©æ•£æ¨¡å‹ï¼Œå®ç°å›¾åƒ/è§†é¢‘æ§åˆ¶ã€‚- é€‚é…å±‚èåˆæ§åˆ¶ç½‘ç»œç‰¹å¾ï¼Œä¿æŒæ§åˆ¶ç½‘ç»œå’Œæ‰©æ•£æ¨¡å‹å‚æ•°ä¸å˜ã€‚- æ—¶ç©ºæ¨¡å—å¤„ç†è§†é¢‘çš„æ—¶é—´ä¸€è‡´æ€§ã€‚- æ½œåœ¨è·³è¿‡å’Œé€†æ—¶æ­¥é‡‡æ ·æå‡é€‚åº”æ€§å’Œç¨€ç–æ§åˆ¶ã€‚- åŠ æƒå¹³å‡æ§åˆ¶ç½‘ç»œè¾“å‡ºå®ç°å¤šæ¡ä»¶æ§åˆ¶ã€‚- é€‚é…å„ç±»æ‰©æ•£æ¨¡å‹ï¼Œå›¾åƒæ§åˆ¶æ•ˆæœä¸æ§åˆ¶ç½‘ç»œåŒ¹é…ï¼Œè§†é¢‘æ§åˆ¶æ•ˆæœè¿œè¶…åŸºçº¿ã€‚- è®¡ç®—æˆæœ¬æä½ï¼ˆä½äº 10 ä¸ª GPU å°æ—¶ï¼‰ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>æ ‡é¢˜ï¼šCTRL-Adapterï¼šä¸€ç§é«˜æ•ˆä¸”é€šç”¨çš„æ¡†æ¶ï¼Œå¯å°†å„ç§æ§åˆ¶é€‚é…åˆ°ä»»ä½•æ‰©æ•£æ¨¡å‹</li><li>ä½œè€…ï¼šHan Linã€Jaemin Choã€Abhay Zalaã€Mohit Bansal</li><li>éš¶å±æœºæ„ï¼šåŒ—å¡ç½—æ¥çº³å¤§å­¦æ•™å ‚å±±åˆ†æ ¡</li><li>å…³é”®è¯ï¼šæ§åˆ¶ç½‘ç»œã€è§†é¢‘ç”Ÿæˆã€æ‰©æ•£æ¨¡å‹ã€æ¡ä»¶æ§åˆ¶ã€è§†é¢‘ç¼–è¾‘</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.09967</li><li><p>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šæ§åˆ¶ç½‘ç»œå¹¿æ³›ç”¨äºå›¾åƒç”Ÿæˆä¸­æ·»åŠ ç©ºé—´æ§åˆ¶ï¼Œä½†ç›´æ¥å°†é¢„è®­ç»ƒçš„å›¾åƒæ§åˆ¶ç½‘ç»œç”¨äºè§†é¢‘ç”Ÿæˆé¢ä¸´æŒ‘æˆ˜ã€‚(2) è¿‡å»çš„æ–¹æ³•ï¼šé¢„è®­ç»ƒçš„æ§åˆ¶ç½‘ç»œæ— æ³•ç›´æ¥æ’å…¥æ–°çš„ä¸»å¹²æ¨¡å‹ï¼Œä¸”ä¸ºæ–°ä¸»å¹²æ¨¡å‹è®­ç»ƒæ§åˆ¶ç½‘ç»œçš„æˆæœ¬å¾ˆé«˜ï¼›ä¸åŒå¸§çš„æ§åˆ¶ç½‘ç»œç‰¹å¾å¯èƒ½æ— æ³•æœ‰æ•ˆå¤„ç†å¯¹è±¡çš„æ—¶åºä¸€è‡´æ€§ã€‚(3) ç ”ç©¶æ–¹æ³•ï¼šæå‡º CTRL-Adapter æ¡†æ¶ï¼Œé€šè¿‡é€‚é…é¢„è®­ç»ƒçš„æ§åˆ¶ç½‘ç»œï¼ˆå¹¶æ”¹è¿›è§†é¢‘çš„æ—¶é—´å¯¹é½ï¼‰å°†å„ç§æ§åˆ¶æ·»åŠ åˆ°å›¾åƒ/è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­ã€‚(4) æ€§èƒ½ï¼šCTRL-Adapter å…·æœ‰å›¾åƒæ§åˆ¶ã€è§†é¢‘æ§åˆ¶ã€ç¨€ç–å¸§è§†é¢‘æ§åˆ¶ã€å¤šæ¡ä»¶æ§åˆ¶ã€ä¸ä¸åŒä¸»å¹²æ¨¡å‹çš„å…¼å®¹æ€§ã€å¯¹æœªè§æ§åˆ¶æ¡ä»¶çš„é€‚åº”ä»¥åŠè§†é¢‘ç¼–è¾‘ç­‰å¼ºå¤§ä¸”å¤šæ ·çš„åŠŸèƒ½ã€‚</p></li><li><p>Methods:(1) CTRL-Adapteræ¡†æ¶ï¼šæå‡ºä¸€ç§é€‚é…å™¨ï¼Œå°†é¢„è®­ç»ƒçš„æ§åˆ¶ç½‘ç»œä¸æ‰©æ•£æ¨¡å‹è¿æ¥ï¼Œå¹¶é€šè¿‡æ—¶é—´å¯¹é½æ¨¡å—å¤„ç†è§†é¢‘çš„æ—¶é—´ä¸€è‡´æ€§ã€‚(2) é¢„è®­ç»ƒæ§åˆ¶ç½‘ç»œé€‚é…ï¼šè®¾è®¡ä¸€ç§é€‚é…å™¨ï¼Œå°†é¢„è®­ç»ƒçš„æ§åˆ¶ç½‘ç»œç‰¹å¾æ˜ å°„åˆ°æ‰©æ•£æ¨¡å‹çš„æ½œåœ¨ç©ºé—´ï¼Œå®ç°ä¸åŒä¸»å¹²æ¨¡å‹çš„å…¼å®¹æ€§ã€‚(3) è§†é¢‘æ—¶é—´å¯¹é½ï¼šå¼•å…¥æ—¶é—´å¯¹é½æ¨¡å—ï¼Œé€šè¿‡å¾ªç¯ä¸€è‡´æ€§æŸå¤±å’Œæ—¶é—´å¹³æ»‘æŸå¤±ï¼Œç¡®ä¿ä¸åŒå¸§çš„æ§åˆ¶ç½‘ç»œç‰¹å¾åœ¨æ—¶é—´ä¸Šä¿æŒä¸€è‡´ã€‚(4) å¤šæ¡ä»¶æ§åˆ¶ï¼šæå‡ºä¸€ç§å¤šæ¡ä»¶æ§åˆ¶æœºåˆ¶ï¼Œå…è®¸åŒæ—¶ä½¿ç”¨å¤šä¸ªæ§åˆ¶æ¡ä»¶ï¼Œå¹¶é€šè¿‡æ¡ä»¶æ··åˆå™¨å°†ä¸åŒæ¡ä»¶çš„ç‰¹å¾èåˆã€‚(5) æœªè§æ§åˆ¶æ¡ä»¶é€‚åº”ï¼šåˆ©ç”¨å¯¹æŠ—æ€§è®­ç»ƒï¼Œä½¿CTRL-Adapterèƒ½å¤Ÿé€‚åº”æœªåœ¨è®­ç»ƒé›†ä¸­å‡ºç°çš„æ§åˆ¶æ¡ä»¶ã€‚(6) è§†é¢‘ç¼–è¾‘ï¼šé€šè¿‡æ§åˆ¶ç½‘ç»œçš„æ©ç æœºåˆ¶ï¼Œå®ç°å¯¹è§†é¢‘ç‰¹å®šåŒºåŸŸæˆ–å¸§çš„ç¼–è¾‘ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡º CTRL-Adapter æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡é€‚é…å’Œæ—¶é—´å¯¹é½é¢„è®­ç»ƒçš„ ControlNetï¼Œå¯ä»¥å°†å„ç§æ§åˆ¶æ·»åŠ åˆ°ä»»ä½•å›¾åƒ/è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­ï¼ŒåŒæ—¶ä¿æŒ ControlNet å’Œéª¨å¹²æ‰©æ•£æ¨¡å‹çš„å‚æ•°ä¸å˜ã€‚è®­ç»ƒ CTRL-Adapter æ˜æ˜¾æ¯”ä¸ºæ–°éª¨å¹²æ¨¡å‹è®­ç»ƒ ControlNet æ›´æœ‰æ•ˆç‡ã€‚CTRL-Adapter è¿˜æä¾›äº†è®¸å¤šæœ‰ç”¨çš„åŠŸèƒ½ï¼ŒåŒ…æ‹¬å›¾åƒæ§åˆ¶ã€è§†é¢‘æ§åˆ¶ã€å…·æœ‰ç¨€ç–è¾“å…¥çš„è§†é¢‘æ§åˆ¶å’Œå¤šæºæ§åˆ¶ã€‚æˆ‘ä»¬é€šè¿‡ç»¼åˆåˆ†æå®è¯å±•ç¤ºäº† CTRL-Adapter çš„æœ‰ç”¨æ€§ã€‚ä½¿ç”¨ä¸åŒçš„å›¾åƒå’Œè§†é¢‘æ‰©æ•£éª¨å¹²ï¼Œè®­ç»ƒ CTRL-Adapter å¯ä»¥åŒ¹é…æˆ–ä¼˜äºè®­ç»ƒæ–°çš„ ControlNetï¼ŒåŒæ—¶é™ä½è®¡ç®—æˆæœ¬ã€‚CTRL-Adapter å¯¹æœªè§æ¡ä»¶æ‰§è¡Œé›¶æ ·æœ¬é€‚åº”ï¼Œå¹¶å¸®åŠ©ç”Ÿæˆå…·æœ‰ç¨€ç–å¸§æ¡ä»¶æˆ–å¤šä¸ªæ¡ä»¶ï¼ˆä¾‹å¦‚ï¼Œæ·±åº¦å›¾ã€Canny è¾¹ç¼˜ã€äººä½“å§¿åŠ¿å’Œæ›²é¢æ³•çº¿ï¼‰çš„è§†é¢‘ã€‚æˆ‘ä»¬è¿˜ä¸º CTRL-Adapter çš„è®¾è®¡é€‰æ‹©å’Œå®šæ€§ç¤ºä¾‹æä¾›äº†å…¨é¢çš„æ¶ˆèç ”ç©¶ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„å·¥ä½œå¯ä»¥ä¿ƒè¿›æœªæ¥è§†é¢‘å’Œå›¾åƒé«˜æ•ˆå—æ§ç”Ÿæˆçš„ç ”ç©¶ã€‚è‡´è°¢ï¼šè¿™é¡¹å·¥ä½œå¾—åˆ°äº† DARPA ECole è®¡åˆ’å· HR00112390060ã€NSF-AI Engage ç ”ç©¶æ‰€ DRL-2112635ã€DARPA æœºå™¨å¸¸è¯† (MCS) è¡¥åŠ©é‡‘ N66001-19-2-4031ã€ARO å¥–é‡‘ W911NF2110220ã€ONR è¡¥åŠ©é‡‘ N00014-23-1-2356 å’Œ Bloomberg æ•°æ®ç§‘å­¦åšå£«å¥–å­¦é‡‘çš„æ”¯æŒã€‚æœ¬æ–‡ä¸­åŒ…å«çš„è§‚ç‚¹æ˜¯ä½œè€…çš„è§‚ç‚¹ï¼Œä¸ä»£è¡¨èµ„åŠ©æœºæ„çš„è§‚ç‚¹ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡º CTRL-Adapter æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡é€‚é…å’Œæ—¶é—´å¯¹é½é¢„è®­ç»ƒçš„ ControlNetï¼Œå¯ä»¥å°†å„ç§æ§åˆ¶æ·»åŠ åˆ°ä»»ä½•å›¾åƒ/è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­ï¼ŒåŒæ—¶ä¿æŒ ControlNet å’Œéª¨å¹²æ‰©æ•£æ¨¡å‹çš„å‚æ•°ä¸å˜ã€‚æ€§èƒ½ï¼šCTRL-Adapter åœ¨å›¾åƒæ§åˆ¶ã€è§†é¢‘æ§åˆ¶ã€å…·æœ‰ç¨€ç–è¾“å…¥çš„è§†é¢‘æ§åˆ¶å’Œå¤šæºæ§åˆ¶æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§ä¸”å¤šæ ·çš„åŠŸèƒ½ã€‚å®ƒå¯ä»¥ä¸ä¸åŒçš„å›¾åƒå’Œè§†é¢‘æ‰©æ•£éª¨å¹²å…¼å®¹ï¼Œå¹¶å¯¹æœªè§æ§åˆ¶æ¡ä»¶å…·æœ‰é€‚åº”æ€§ã€‚å·¥ä½œé‡ï¼šè®­ç»ƒ CTRL-Adapter æ˜æ˜¾æ¯”ä¸ºæ–°éª¨å¹²æ¨¡å‹è®­ç»ƒ ControlNet æ›´æœ‰æ•ˆç‡ã€‚å®ƒè¿˜å¯ä»¥å¯¹æœªè§æ§åˆ¶æ¡ä»¶æ‰§è¡Œé›¶æ ·æœ¬é€‚åº”ï¼Œå¹¶å¸®åŠ©ç”Ÿæˆå…·æœ‰ç¨€ç–å¸§æ¡ä»¶æˆ–å¤šä¸ªæ¡ä»¶çš„è§†é¢‘ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="http://article.biliimg.com/bfs/new_dyn/c25f069f15093064473dcdf4dfaa56ee241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/f677d02b9dd20e46686e3c8c08280d9d241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/ce2e3a267b9806afbb6606fbc59d0755241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/33b547f986f3df3860ea96d4a3aa9f63241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/48ab1365ae4e53ae2b1e46fafa4c5b7c241286257.jpg" align="middle"></details><h2 id="A-Diffusion-based-Data-Generator-for-Training-Object-Recognition-Models-in-Ultra-Range-Distance"><a href="#A-Diffusion-based-Data-Generator-for-Training-Object-Recognition-Models-in-Ultra-Range-Distance" class="headerlink" title="A Diffusion-based Data Generator for Training Object Recognition Models   in Ultra-Range Distance"></a>A Diffusion-based Data Generator for Training Object Recognition Models   in Ultra-Range Distance</h2><p><strong>Authors:Eran Bamani, Eden Nissinman, Lisa Koenigsberg, Inbar Meir, Avishai Sintov</strong></p><p>Object recognition, commonly performed by a camera, is a fundamental requirement for robots to complete complex tasks. Some tasks require recognizing objects far from the robotâ€™s camera. A challenging example is Ultra-Range Gesture Recognition (URGR) in human-robot interaction where the user exhibits directive gestures at a distance of up to 25~m from the robot. However, training a model to recognize hardly visible objects located in ultra-range requires an exhaustive collection of a significant amount of labeled samples. The generation of synthetic training datasets is a recent solution to the lack of real-world data, while unable to properly replicate the realistic visual characteristics of distant objects in images. In this letter, we propose the Diffusion in Ultra-Range (DUR) framework based on a Diffusion model to generate labeled images of distant objects in various scenes. The DUR generator receives a desired distance and class (e.g., gesture) and outputs a corresponding synthetic image. We apply DUR to train a URGR model with directive gestures in which fine details of the gesturing hand are challenging to distinguish. DUR is compared to other types of generative models showcasing superiority both in fidelity and in recognition success rate when training a URGR model. More importantly, training a DUR model on a limited amount of real data and then using it to generate synthetic data for training a URGR model outperforms directly training the URGR model on real data. The synthetic-based URGR model is also demonstrated in gesture-based direction of a ground robot. </p><p><a href="http://arxiv.org/abs/2404.09846v1">PDF</a> </p><p><strong>Summary</strong><br>é€šè¿‡æ‰©æ•£æ¨¡å‹ç”Ÿæˆè¿œè·ç¦»ç‰©ä½“å›¾åƒï¼Œæå‡æœºå™¨äººæ‰‹åŠ¿è¯†åˆ«èƒ½åŠ›ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>é’ˆå¯¹è¿œè·ç¦»æ‰‹åŠ¿è¯†åˆ«ï¼Œæå‡ºäº†åŸºäºæ‰©æ•£æ¨¡å‹çš„DURæ¡†æ¶ã€‚</li><li>DURæ¥æ”¶è·ç¦»å’Œç±»åˆ«ï¼Œç”Ÿæˆç›¸åº”è¿œè·ç¦»ç‰©ä½“åˆæˆå›¾åƒã€‚</li><li>DURç”Ÿæˆçš„å›¾åƒåœ¨ä¿çœŸåº¦å’Œè¯†åˆ«æˆåŠŸç‡ä¸Šä¼˜äºå…¶ä»–ç”Ÿæˆæ¨¡å‹ã€‚</li><li>URGRæ¨¡å‹åœ¨çœŸå®æ•°æ®å’ŒDURåˆæˆæ•°æ®ä¸Šè®­ç»ƒéƒ½ä¼˜äºç›´æ¥åœ¨çœŸå®æ•°æ®ä¸Šè®­ç»ƒã€‚</li><li>åŸºäºDURåˆæˆæ•°æ®çš„URGRæ¨¡å‹å¯ç”¨äºåœ°é¢æœºå™¨äººçš„æ‰‹åŠ¿å¼•å¯¼ã€‚</li><li>DURæ¡†æ¶å¯æ¨å¹¿åˆ°å…¶ä»–è¿œè·ç¦»ç‰©ä½“è¯†åˆ«ä»»åŠ¡ã€‚</li><li>åˆæˆæ•°æ®é›†ç¼“è§£äº†çœŸå®ä¸–ç•Œæ•°æ®ç¼ºä¹çš„é—®é¢˜ï¼Œå¢å¼ºäº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šåŸºäºæ‰©æ•£çš„è¶…è¿œè·ç¦»ç‰©ä½“è¯†åˆ«æ¨¡å‹è®­ç»ƒæ•°æ®ç”Ÿæˆå™¨</li><li>ä½œè€…ï¼šEran Bamani, Eden Nissinman, Lisa Koenigsberg, Inbar Meir, Avishai Sintov</li><li>éš¶å±å•ä½ï¼šç‰¹æ‹‰ç»´å¤«å¤§å­¦æœºæ¢°å·¥ç¨‹å­¦é™¢</li><li>å…³é”®è¯ï¼šè¶…è¿œè·ç¦»æ‰‹åŠ¿è¯†åˆ«ã€æ•°æ®ç”Ÿæˆã€æ‰©æ•£æ¨¡å‹</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.09846   Github ä»£ç é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼š   (1)ï¼šç ”ç©¶èƒŒæ™¯ï¼šç‰©ä½“è¯†åˆ«æ˜¯æœºå™¨äººæœ‰æ•ˆä¸ç¯å¢ƒäº¤äº’çš„åŸºç¡€ï¼Œä½†éšç€æœºå™¨äººä¸ç‰©ä½“è·ç¦»çš„å¢åŠ ï¼Œå›¾åƒåˆ†è¾¨ç‡ä¸‹é™ï¼Œè¯†åˆ«æ€§èƒ½ä¼šå› å¤±å»åˆ¤åˆ«æ€§è§†è§‰ç‰¹å¾è€Œæ˜¾ç€ä¸‹é™ã€‚   (2)ï¼šè¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰ç­‰ä¼ ç»Ÿç”Ÿæˆæ¨¡å‹æ— æ³•å¾ˆå¥½åœ°å¤åˆ¶è¿œè·ç¦»ç‰©ä½“åœ¨å›¾åƒä¸­çš„çœŸå®è§†è§‰ç‰¹å¾ã€‚   (3)ï¼šæœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„è¶…è¿œè·ç¦»æ‰©æ•£ï¼ˆDURï¼‰æ¡†æ¶ï¼Œå¯ä»¥ç”Ÿæˆå…·æœ‰ä¸åŒåœºæ™¯å’Œè·ç¦»æ¡ä»¶çš„è¿œè·ç¦»ç‰©ä½“æ ‡è®°å›¾åƒã€‚   (4)ï¼šæ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šDUR åœ¨è¶…è¿œè·ç¦»æ‰‹åŠ¿è¯†åˆ«ï¼ˆURGRï¼‰ä»»åŠ¡ä¸Šè®­ç»ƒäº†ä¸€ä¸ªæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨è·ç¦»é«˜è¾¾ 25 ç±³çš„å®¤å†…å¤–ç¯å¢ƒä¸­è¡¨ç°å‡ºè‰¯å¥½çš„è¯†åˆ«æˆåŠŸç‡ã€‚æ­¤å¤–ï¼ŒåŸºäºåˆæˆæ•°æ®çš„ URGR æ¨¡å‹è¿˜è¢«è¯æ˜å¯ä»¥ç”¨äºåŸºäºæ‰‹åŠ¿çš„åœ°é¢æœºå™¨äººå¯¼èˆªã€‚</p></li><li><p>æ–¹æ³•ï¼š(1): æå‡ºè¶…è¿œè·ç¦»æ‰©æ•£ï¼ˆDURï¼‰æ¡†æ¶ï¼ŒåŸºäºæ‰©æ•£æ¨¡å‹ç”Ÿæˆä¸åŒåœºæ™¯å’Œè·ç¦»æ¡ä»¶ä¸‹çš„è¿œè·ç¦»ç‰©ä½“æ ‡è®°å›¾åƒï¼›(2): å®šä¹‰è¶…è¿œè·ç¦»ç‰©ä½“è¯†åˆ«é—®é¢˜ï¼Œç›®æ ‡æ˜¯æ ¹æ®è·ç¦»å°äº 25 ç±³çš„ RGB å›¾åƒï¼Œå°†å›¾åƒåŸºäºå±•ç¤ºçš„å¯¹è±¡åˆ†ç±»ä¸º m ä¸ªå¯èƒ½çš„ç±»åˆ«ä¹‹ä¸€ï¼›(3): æ”¶é›†æ ‡è®°æ•°æ®é›†ï¼ŒåŒ…æ‹¬å›¾åƒã€æ‰‹åŠ¿ç±»åˆ«ç´¢å¼•å’Œè·ç¦»ä¿¡æ¯ï¼Œå¹¶ä½¿ç”¨ YOLOv8 æ£€æµ‹ç”¨æˆ·å¹¶è£å‰ªèƒŒæ™¯ï¼Œä½¿ç”¨ HQ-Net å¢å¼ºå›¾åƒè´¨é‡ï¼›(4): ä½¿ç”¨å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMï¼‰ä½œä¸ºç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡é€æ¸æ·»åŠ å’Œåè½¬å™ªå£°æ¥ç”Ÿæˆåˆæˆå›¾åƒï¼›(5): å­¦ä¹ æ¨¡å‹ pÎ¸ è¿‘ä¼¼æ¡ä»¶æ¦‚ç‡ï¼Œä»¥é‡å»ºåŸå§‹åˆ†å¸ƒçš„å›¾åƒã€‚</p></li><li><p>ç»“è®ºï¼š(1): æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„è¶…è¿œè·ç¦»æ‰©æ•£ï¼ˆDURï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥ç”Ÿæˆå…·æœ‰ä¸åŒåœºæ™¯å’Œè·ç¦»æ¡ä»¶çš„è¿œè·ç¦»ç‰©ä½“æ ‡è®°å›¾åƒï¼Œä¸ºè¶…è¿œè·ç¦»ç‰©ä½“è¯†åˆ«ä»»åŠ¡æä¾›äº†ä¸°å¯Œçš„è®­ç»ƒæ•°æ®ã€‚(2): åˆ›æ–°ç‚¹ï¼š</p><ul><li>æå‡ºåŸºäºæ‰©æ•£æ¨¡å‹çš„è¶…è¿œè·ç¦»æ‰©æ•£ï¼ˆDURï¼‰æ¡†æ¶ï¼Œä¸ºè¶…è¿œè·ç¦»ç‰©ä½“è¯†åˆ«ä»»åŠ¡ç”Ÿæˆé€¼çœŸçš„åˆæˆè®­ç»ƒæ•°æ®ã€‚</li><li>å®šä¹‰äº†è¶…è¿œè·ç¦»ç‰©ä½“è¯†åˆ«é—®é¢˜ï¼Œå¹¶æ”¶é›†äº†åŒ…å«å›¾åƒã€æ‰‹åŠ¿ç±»åˆ«ç´¢å¼•å’Œè·ç¦»ä¿¡æ¯çš„æ ‡è®°æ•°æ®é›†ã€‚</li><li>ä½¿ç”¨å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMï¼‰ä½œä¸ºç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡é€æ¸æ·»åŠ å’Œåè½¬å™ªå£°æ¥ç”Ÿæˆåˆæˆå›¾åƒã€‚</li></ul></li><li>æ€§èƒ½ï¼š<ul><li>åœ¨è¶…è¿œè·ç¦»æ‰‹åŠ¿è¯†åˆ«ï¼ˆURGRï¼‰ä»»åŠ¡ä¸Šè®­ç»ƒäº†ä¸€ä¸ªæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨è·ç¦»é«˜è¾¾25ç±³çš„å®¤å†…å¤–ç¯å¢ƒä¸­è¡¨ç°å‡ºè‰¯å¥½çš„è¯†åˆ«æˆåŠŸç‡ã€‚</li><li>åŸºäºåˆæˆæ•°æ®çš„URGRæ¨¡å‹è¿˜è¢«è¯æ˜å¯ä»¥ç”¨äºåŸºäºæ‰‹åŠ¿çš„åœ°é¢æœºå™¨äººå¯¼èˆªã€‚</li></ul></li><li>å·¥ä½œé‡ï¼š<ul><li>æ”¶é›†äº†åŒ…å«50,000å¼ å›¾åƒçš„æ ‡è®°æ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨YOLOv8å’ŒHQ-Netå¯¹å›¾åƒè¿›è¡Œäº†é¢„å¤„ç†ã€‚</li><li>è®­ç»ƒäº†å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMï¼‰ï¼Œä»¥ç”Ÿæˆå…·æœ‰ä¸åŒåœºæ™¯å’Œè·ç¦»æ¡ä»¶çš„è¿œè·ç¦»ç‰©ä½“æ ‡è®°å›¾åƒã€‚</li><li>åœ¨è¶…è¿œè·ç¦»æ‰‹åŠ¿è¯†åˆ«ï¼ˆURGRï¼‰ä»»åŠ¡ä¸Šè®­ç»ƒäº†ä¸€ä¸ªæ¨¡å‹ï¼Œå¹¶è¯„ä¼°äº†å…¶æ€§èƒ½ã€‚</li></ul></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="http://article.biliimg.com/bfs/new_dyn/6b8ecb308186704259aa27fb23638660241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/e63d9562d6323864e562317f7c12ea55241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/c14de792d860790e380b7de225a0fa74241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/958ce44fd42dfbbff812a95c9714e514241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/14eb7f29fa6a016695804dd8fc230360241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/31c2f09164cbb10143e3860a7c7e3599241286257.jpg" align="middle"></details><h2 id="Digging-into-contrastive-learning-for-robust-depth-estimation-with-diffusion-models"><a href="#Digging-into-contrastive-learning-for-robust-depth-estimation-with-diffusion-models" class="headerlink" title="Digging into contrastive learning for robust depth estimation with   diffusion models"></a>Digging into contrastive learning for robust depth estimation with   diffusion models</h2><p><strong>Authors:Jiyuan Wang, Chunyu Lin, Lang Nie, Kang Liao, Shuwei Shao, Yao Zhao</strong></p><p>Recently, diffusion-based depth estimation methods have drawn widespread attention due to their elegant denoising patterns and promising performance. However, they are typically unreliable under adverse conditions prevalent in real-world scenarios, such as rainy, snowy, etc. In this paper, we propose a novel robust depth estimation method called D4RD, featuring a custom contrastive learning mode tailored for diffusion models to mitigate performance degradation in complex environments. Concretely, we integrate the strength of knowledge distillation into contrastive learning, building the `trinityâ€™ contrastive scheme. This scheme utilizes the sampled noise of the forward diffusion process as a natural reference, guiding the predicted noise in diverse scenes toward a more stable and precise optimum. Moreover, we extend noise-level trinity to encompass more generic feature and image levels, establishing a multi-level contrast to distribute the burden of robust perception across the overall network. Before addressing complex scenarios, we enhance the stability of the baseline diffusion model with three straightforward yet effective improvements, which facilitate convergence and remove depth outliers. Extensive experiments demonstrate that D4RD surpasses existing state-of-the-art solutions on synthetic corruption datasets and real-world weather conditions. The code for D4RD will be made available for further exploration and adoption. </p><p><a href="http://arxiv.org/abs/2404.09831v1">PDF</a> 8 pages,6 figures</p><p><strong>Summary</strong><br>æ‰©æ•£æ·±åº¦ä¼°è®¡ä¸­çš„ä¸‰å…ƒå¯¹æ¯”å­¦ä¹ æ¨¡å¼ï¼Œå¢å¼ºäº†æ¨¡å‹å¯¹å¤æ‚ç¯å¢ƒçš„é²æ£’æ€§ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºæ‰©æ•£çš„é²æ£’æ·±åº¦ä¼°è®¡æ–¹æ³• D4RDã€‚</li><li>å¼•å…¥äº†å®šåˆ¶çš„å¯¹æ¯”å­¦ä¹ æ¨¡å¼ï¼Œä»¥å‡è½»å¤æ‚ç¯å¢ƒä¸­çš„æ€§èƒ½ä¸‹é™ã€‚</li><li>æå‡ºäº†ä¸€ç§â€œä¸‰å…ƒâ€å¯¹æ¯”æ–¹æ¡ˆï¼Œåˆ©ç”¨å‰å‘æ‰©æ•£è¿‡ç¨‹ä¸­çš„é‡‡æ ·å™ªå£°ä½œä¸ºè‡ªç„¶å‚è€ƒã€‚</li><li>å°†å™ªå£°æ°´å¹³ä¸‰å…ƒæ‰©å±•åˆ°æ›´é€šç”¨çš„ç‰¹å¾å’Œå›¾åƒçº§åˆ«ï¼Œå»ºç«‹å¤šå±‚æ¬¡å¯¹æ¯”ã€‚</li><li>é€šè¿‡ä¸‰ä¸ªç®€å•çš„æ”¹è¿›å¢å¼ºäº†åŸºçº¿æ‰©æ•£æ¨¡å‹çš„ç¨³å®šæ€§ã€‚</li><li>D4RD åœ¨åˆæˆæŸåæ•°æ®é›†å’ŒçœŸå®ä¸–ç•Œå¤©æ°”æ¡ä»¶ä¸‹ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„è§£å†³æ–¹æ¡ˆã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šåŸºäºå¯¹æ¯”å­¦ä¹ çš„é²æ£’æ·±åº¦ä¼°è®¡</li><li>ä½œè€…ï¼šç‹ç»§æºã€æ—æ˜¥å®‡ã€è‚æœ—ã€å»–åº·ã€é‚µä¹¦ä¼Ÿã€èµµå°§</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šåŒ—äº¬äº¤é€šå¤§å­¦</li><li>å…³é”®è¯ï¼šæ·±åº¦ä¼°è®¡ã€é²æ£’æ„ŸçŸ¥ã€è‡ªç›‘ç£å­¦ä¹ ã€æ‰©æ•£æ–¹æ³•</li><li>è®ºæ–‡é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šæ‰©æ•£æ¨¡å‹åœ¨æ·±åº¦ä¼°è®¡ä¸­å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬åœ¨é›¨é›ªç­‰æ¶åŠ£æ¡ä»¶ä¸‹è¡¨ç°ä¸å¯é ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•å’Œé—®é¢˜ï¼šå¯¹æ¯”å­¦ä¹ å’ŒçŸ¥è¯†è’¸é¦åˆ†åˆ«ç”¨äºå¢å¼ºé²æ£’æ€§ï¼Œä½†å­˜åœ¨ä¸€äº›ä¸è¶³ï¼Œä¾‹å¦‚å¯¹æ¯”å­¦ä¹ å®¹æ˜“å—å™ªå£°å½±å“ï¼ŒçŸ¥è¯†è’¸é¦éœ€è¦é«˜è´¨é‡çš„ä¼ªæ ‡ç­¾ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº† D4RDï¼Œå®ƒç»“åˆäº†å¯¹æ¯”å­¦ä¹ å’ŒçŸ¥è¯†è’¸é¦ï¼Œæ„å»ºäº†ä¸€ä¸ªâ€œä¸‰å…ƒâ€å¯¹æ¯”æ–¹æ¡ˆã€‚è¯¥æ–¹æ¡ˆåˆ©ç”¨æ­£å‘æ‰©æ•£è¿‡ç¨‹ä¸­çš„é‡‡æ ·å™ªå£°ä½œä¸ºè‡ªç„¶å‚è€ƒï¼Œå¼•å¯¼ä¸åŒåœºæ™¯ä¸­çš„é¢„æµ‹å™ªå£°æœå‘æ›´ç¨³å®šå’Œç²¾ç¡®çš„æå€¼ã€‚æ­¤å¤–ï¼Œè¿˜æ‰©å±•äº†å™ªå£°çº§ä¸‰å…ƒå¯¹æ¯”åˆ°æ›´é€šç”¨çš„ç‰¹å¾å’Œå›¾åƒçº§åˆ«ï¼Œå»ºç«‹äº†å¤šçº§å¯¹æ¯”ä»¥è·¨æ•´ä¸ªç½‘ç»œåˆ†é…é²æ£’æ„ŸçŸ¥çš„è´Ÿæ‹…ã€‚ï¼ˆ4ï¼‰ä»»åŠ¡å’Œæ€§èƒ½ï¼šåœ¨åˆæˆæŸåæ•°æ®é›†å’ŒçœŸå®ä¸–ç•Œå¤©æ°”æ¡ä»¶ä¸‹ï¼ŒD4RD è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›è§£å†³æ–¹æ¡ˆã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°å¢å¼ºé²æ£’æ€§å¹¶å‡è½»æ¶åŠ£æ¡ä»¶ä¸‹çš„æ€§èƒ½ä¸‹é™ã€‚</p></li><li><p><strong>æ–¹æ³•</strong>ï¼šï¼ˆ1ï¼‰æå‡ºäº†ä¸€ç§æ–°çš„å¤šçº§â€œä¸‰å…ƒâ€å¯¹æ¯”æ–¹æ¡ˆï¼Œä»¥å¢å¼ºé²æ£’æ€§å’Œå‡è½»æ¶åŠ£æ¡ä»¶ä¸‹çš„æ€§èƒ½ä¸‹é™ï¼›ï¼ˆ2ï¼‰åˆ©ç”¨æ­£å‘æ‰©æ•£è¿‡ç¨‹ä¸­çš„é‡‡æ ·å™ªå£°ä½œä¸ºè‡ªç„¶å‚è€ƒï¼Œå¼•å¯¼ä¸åŒåœºæ™¯ä¸­çš„é¢„æµ‹å™ªå£°æœå‘æ›´ç¨³å®šå’Œç²¾ç¡®çš„æå€¼ï¼›ï¼ˆ3ï¼‰å°†å™ªå£°çº§ä¸‰å…ƒå¯¹æ¯”æ‰©å±•åˆ°æ›´é€šç”¨çš„ç‰¹å¾å’Œå›¾åƒçº§åˆ«ï¼Œå»ºç«‹å¤šçº§å¯¹æ¯”ä»¥è·¨æ•´ä¸ªç½‘ç»œåˆ†é…é²æ£’æ„ŸçŸ¥çš„è´Ÿæ‹…ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬æ–‡æå‡ºäº† D4RDï¼Œä¸€ç§åŸºäºå¯¹æ¯”å­¦ä¹ çš„é²æ£’æ·±åº¦ä¼°è®¡æ¡†æ¶ï¼Œå¢å¼ºäº†åŸºäºæ‰©æ•£çš„æ·±åº¦ä¼°è®¡çš„ç¨³å®šæ€§å’Œæ”¶æ•›æ€§ã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§æ–°çš„å¤šçº§â€œä¸‰å…ƒâ€å¯¹æ¯”æ–¹æ¡ˆï¼Œä»¥å¢å¼ºé²æ£’æ€§å’Œå‡è½»æ¶åŠ£æ¡ä»¶ä¸‹çš„æ€§èƒ½ä¸‹é™ã€‚</li><li>åˆ©ç”¨æ­£å‘æ‰©æ•£è¿‡ç¨‹ä¸­çš„é‡‡æ ·å™ªå£°ä½œä¸ºè‡ªç„¶å‚è€ƒï¼Œå¼•å¯¼ä¸åŒåœºæ™¯ä¸­çš„é¢„æµ‹å™ªå£°æœå‘æ›´ç¨³å®šå’Œç²¾ç¡®çš„æå€¼ã€‚</li><li>å°†å™ªå£°çº§ä¸‰å…ƒå¯¹æ¯”æ‰©å±•åˆ°æ›´é€šç”¨çš„ç‰¹å¾å’Œå›¾åƒçº§åˆ«ï¼Œå»ºç«‹å¤šçº§å¯¹æ¯”ä»¥è·¨æ•´ä¸ªç½‘ç»œåˆ†é…é²æ£’æ„ŸçŸ¥çš„è´Ÿæ‹…ã€‚æ€§èƒ½ï¼š</li><li>åœ¨åˆæˆæŸåæ•°æ®é›†å’ŒçœŸå®ä¸–ç•Œå¤©æ°”æ¡ä»¶ä¸‹ï¼ŒD4RD è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›è§£å†³æ–¹æ¡ˆã€‚</li><li>è¿™äº›ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°å¢å¼ºé²æ£’æ€§å¹¶å‡è½»æ¶åŠ£æ¡ä»¶ä¸‹çš„æ€§èƒ½ä¸‹é™ã€‚å·¥ä½œé‡ï¼š</li><li>è¯¥æ–¹æ³•éœ€è¦å¯¹æ­£å‘æ‰©æ•£è¿‡ç¨‹ä¸­çš„é‡‡æ ·å™ªå£°è¿›è¡Œå»ºæ¨¡ï¼Œè¿™å¯èƒ½éœ€è¦é¢å¤–çš„è®¡ç®—å¼€é”€ã€‚</li><li>å¤šçº§å¯¹æ¯”æ–¹æ¡ˆçš„å®ç°ä¹Ÿå¯èƒ½å¢åŠ æ¨¡å‹çš„å¤æ‚æ€§ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="http://article.biliimg.com/bfs/new_dyn/944e85b707a7fb7eff2e43b4ba0298bc241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/bab219f0ca75b9f2908a62071e68eeda241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/c66b109acaddbd0ab6aa42c7965a9683241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/d9984015fa79bdccf3644f3446f0ee43241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/5ab47e273e1c34b2c8a6d3ee8c66b6ef241286257.jpg" align="middle"></details><h2 id="Equipping-Diffusion-Models-with-Differentiable-Spatial-Entropy-for-Low-Light-Image-Enhancement"><a href="#Equipping-Diffusion-Models-with-Differentiable-Spatial-Entropy-for-Low-Light-Image-Enhancement" class="headerlink" title="Equipping Diffusion Models with Differentiable Spatial Entropy for   Low-Light Image Enhancement"></a>Equipping Diffusion Models with Differentiable Spatial Entropy for   Low-Light Image Enhancement</h2><p><strong>Authors:Wenyi Lian, Wenjing Lian, Ziwei Luo</strong></p><p>Image restoration, which aims to recover high-quality images from their corrupted counterparts, often faces the challenge of being an ill-posed problem that allows multiple solutions for a single input. However, most deep learning based works simply employ l1 loss to train their network in a deterministic way, resulting in over-smoothed predictions with inferior perceptual quality. In this work, we propose a novel method that shifts the focus from a deterministic pixel-by-pixel comparison to a statistical perspective, emphasizing the learning of distributions rather than individual pixel values. The core idea is to introduce spatial entropy into the loss function to measure the distribution difference between predictions and targets. To make this spatial entropy differentiable, we employ kernel density estimation (KDE) to approximate the probabilities for specific intensity values of each pixel with their neighbor areas. Specifically, we equip the entropy with diffusion models and aim for superior accuracy and enhanced perceptual quality over l1 based noise matching loss. In the experiments, we evaluate the proposed method for low light enhancement on two datasets and the NTIRE challenge 2024. All these results illustrate the effectiveness of our statistic-based entropy loss. Code is available at <a href="https://github.com/shermanlian/spatial-entropy-loss">https://github.com/shermanlian/spatial-entropy-loss</a>. </p><p><a href="http://arxiv.org/abs/2404.09735v1">PDF</a> CVPRW 2024, best LPIPS in the NTIRE low light enhancement challenge   2024</p><p><strong>Summary</strong></p><p>å›¾åƒä¿®å¤ä»æŸåçš„å›¾åƒä¸­æ¢å¤é«˜è´¨é‡å›¾åƒï¼Œé€šå¸¸é¢ä¸´ç—…æ€é—®é¢˜ï¼Œå³å•ä¸€è¾“å…¥æœ‰å¤šä¸ªè§£ã€‚ç„¶è€Œï¼Œå¤šæ•°åŸºäºæ·±åº¦å­¦ä¹ çš„å·¥ä½œä½¿ç”¨ l1 æŸå¤±ï¼Œä»¥ç¡®å®šæ€§æ–¹å¼è®­ç»ƒç½‘ç»œï¼Œå¯¼è‡´è¿‡åº¦å¹³æ»‘çš„é¢„æµ‹ï¼Œæ„ŸçŸ¥è´¨é‡è¾ƒå·®ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œå°†é‡ç‚¹ä»ç¡®å®šæ€§é€åƒç´ æ¯”è¾ƒè½¬ç§»åˆ°ç»Ÿè®¡è§’åº¦ï¼Œå¼ºè°ƒå­¦ä¹ åˆ†å¸ƒè€Œéä¸ªåˆ«åƒç´ å€¼ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯åœ¨æŸå¤±å‡½æ•°ä¸­å¼•å…¥ç©ºé—´ç†µï¼Œæµ‹é‡é¢„æµ‹ä¸ç›®æ ‡ä¹‹é—´çš„åˆ†å¸ƒå·®å¼‚ã€‚ä¸ºäº†ä½¿ç©ºé—´ç†µå¯å¾®ï¼Œæˆ‘ä»¬ä½¿ç”¨æ ¸å¯†åº¦ä¼°è®¡ (KDE) æ¥è¿‘ä¼¼æ¯ä¸ªåƒç´ ä¸å…¶é‚»åŸŸçš„ç‰¹å®šå¼ºåº¦å€¼çš„æ¦‚ç‡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ç”¨æ‰©æ•£æ¨¡å‹è£…å¤‡ç†µï¼Œå¹¶é’ˆå¯¹åŸºäº l1 çš„å™ªå£°åŒ¹é…æŸå¤±ï¼Œè¿½æ±‚æ›´é«˜çš„å‡†ç¡®æ€§å’Œå¢å¼ºçš„æ„ŸçŸ¥è´¨é‡ã€‚åœ¨å®éªŒä¸­ï¼Œæˆ‘ä»¬è¯„ä¼°äº†æ‰€ææ–¹æ³•åœ¨ä¸¤ä¸ªæ•°æ®é›†å’Œ NTIRE æŒ‘æˆ˜ 2024 ä¸­çš„ä½å…‰å¢å¼ºã€‚æ‰€æœ‰è¿™äº›ç»“æœè¯´æ˜äº†æˆ‘ä»¬åŸºäºç»Ÿè®¡çš„ç†µæŸå¤±çš„æœ‰æ•ˆæ€§ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>å›¾åƒä¿®å¤é¢ä¸´ç—…æ€é—®é¢˜ï¼Œæœ‰å¤šä¸ªè§£ã€‚</li><li>ä¼ ç»Ÿæ–¹æ³•ä½¿ç”¨ l1 æŸå¤±ï¼Œå¯¼è‡´è¿‡åº¦å¹³æ»‘çš„é¢„æµ‹ã€‚</li><li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç»Ÿè®¡çš„ç©ºé—´ç†µæŸå¤±ã€‚</li><li>ç©ºé—´ç†µæŸå¤±ä½¿ç”¨æ ¸å¯†åº¦ä¼°è®¡ (KDE) æ¥è¿‘ä¼¼æ¦‚ç‡åˆ†å¸ƒã€‚</li><li>æœ¬æ–‡ä½¿ç”¨æ‰©æ•£æ¨¡å‹è£…å¤‡äº†ç©ºé—´ç†µæŸå¤±ã€‚</li><li>å®éªŒè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨ä½å…‰å¢å¼ºæ–¹é¢ä¼˜äºåŸºäº l1 çš„æŸå¤±ã€‚</li><li>ä»£ç å¯åœ¨ <a href="https://github.com/shermanlian/spatial-entropy-loss">https://github.com/shermanlian/spatial-entropy-loss</a> è·å¾—ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šå¯å¾®ç©ºé—´ç†µæ‰©æ•£æ¨¡å‹åœ¨ä½å…‰å›¾åƒå¢å¼ºä¸­çš„åº”ç”¨</li><li>ä½œè€…ï¼šæ–‡ä¹‰è¿ã€æ–‡é™è¿ã€ç´«è–‡ç½—</li><li>éš¶å±ï¼šä¹Œæ™®è¨æ‹‰å¤§å­¦</li><li>å…³é”®è¯ï¼šå›¾åƒå¤åŸã€ä½å…‰å¢å¼ºã€æ‰©æ•£æ¨¡å‹ã€ç©ºé—´ç†µ</li><li>é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.09735    Githubï¼šhttps://github.com/shermanlian/spatial-entropy-loss</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šå›¾åƒå¤åŸæ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œä¼ ç»Ÿçš„åŸºäºåƒç´ çº§çš„æŸå¤±å‡½æ•°ï¼ˆå¦‚ L1 å’Œ L2ï¼‰åœ¨æé«˜å›¾åƒä¿çœŸåº¦æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†å¾€å¾€ä¼šå¯¼è‡´è¿‡åº¦å¹³æ»‘çš„è¾“å‡ºï¼Œæ— æ³•æ•æ‰å›¾åƒçš„æ„ŸçŸ¥è´¨é‡ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šä¸ºäº†è§£å†³è¿‡åº¦å¹³æ»‘çš„é—®é¢˜ï¼Œä¸€äº›ç ”ç©¶åˆ©ç”¨å¯¹æŠ—ç”Ÿæˆç½‘ç»œï¼ˆGANï¼‰å’Œæ„ŸçŸ¥åº¦é‡ï¼ˆå¦‚ VGG æŸå¤±å’Œ LPIPS æŸå¤±ï¼‰æ¥æé«˜å›¾åƒçš„è§†è§‰è´¨é‡ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä»ç„¶ä¾èµ–äº L1 æŸå¤±æ¥ç»´æŒå¤åŸç²¾åº¦ï¼Œå¹¶ä¸”éœ€è¦é¢å¤–çš„ï¼ˆé¢„è®­ç»ƒçš„ï¼‰ç½‘ç»œï¼Œå¢åŠ äº†æ¨¡å‹çš„å¤æ‚æ€§å’Œä¸å¯é¢„æµ‹æ€§ã€‚ï¼ˆ3ï¼‰æœ¬æ–‡æå‡ºçš„æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç©ºé—´ç†µçš„æŸå¤±å‡½æ•°ï¼Œå°†é‡ç‚¹ä»ç¡®å®šæ€§çš„åƒç´ çº§æ¯”è¾ƒè½¬ç§»åˆ°ç»Ÿè®¡å­¦è§’åº¦ï¼Œå¼ºè°ƒå­¦ä¹ åˆ†å¸ƒè€Œä¸æ˜¯å•ä¸ªåƒç´ å€¼ã€‚å…·ä½“æ¥è¯´ï¼Œæœ¬æ–‡å°†ç©ºé—´ç†µå¼•å…¥æŸå¤±å‡½æ•°ä¸­ï¼Œä»¥è¡¡é‡é¢„æµ‹å€¼å’Œç›®æ ‡å€¼ä¹‹é—´çš„åˆ†å¸ƒå·®å¼‚ã€‚ä¸ºäº†ä½¿ç©ºé—´ç†µå¯å¾®ï¼Œæœ¬æ–‡é‡‡ç”¨æ ¸å¯†åº¦ä¼°è®¡ï¼ˆKDEï¼‰æ¥è¿‘ä¼¼æ¯ä¸ªåƒç´ ç‰¹å®šå¼ºåº¦å€¼ä¸å…¶é‚»åŸŸçš„æ¦‚ç‡ã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼šæœ¬æ–‡å°†æå‡ºçš„æ–¹æ³•åº”ç”¨äºä½å…‰å¢å¼ºä»»åŠ¡ï¼Œå¹¶åœ¨ä¸¤ä¸ªæ•°æ®é›†å’Œ NTIRE æŒ‘æˆ˜ 2024 ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºç»Ÿè®¡çš„ç©ºé—´ç†µæŸå¤±å‡½æ•°åœ¨å‡†ç¡®æ€§å’Œæ„ŸçŸ¥è´¨é‡æ–¹é¢å‡ä¼˜äºåŸºäº L1 çš„å™ªå£°åŒ¹é…æŸå¤±å‡½æ•°ã€‚</li></ol><p><strong>Methodsï¼š</strong></p><p>(1) <strong>åŸºäºç©ºé—´ç†µçš„æŸå¤±å‡½æ•°ï¼š</strong>   - å¼•å…¥ç©ºé—´ç†µè¡¡é‡é¢„æµ‹å€¼å’Œç›®æ ‡å€¼ä¹‹é—´çš„åˆ†å¸ƒå·®å¼‚ã€‚   - é‡‡ç”¨æ ¸å¯†åº¦ä¼°è®¡ï¼ˆKDEï¼‰è¿‘ä¼¼æ¯ä¸ªåƒç´ çš„æ¦‚ç‡åˆ†å¸ƒã€‚</p><p>(2) <strong>ä½å…‰å¢å¼ºä¸­çš„åº”ç”¨ï¼š</strong>   - å°†æå‡ºçš„æ–¹æ³•åº”ç”¨äºä½å…‰å›¾åƒå¢å¼ºä»»åŠ¡ã€‚   - åœ¨ä¸¤ä¸ªæ•°æ®é›†å’Œ NTIRE æŒ‘æˆ˜ 2024 ä¸Šè¿›è¡Œè¯„ä¼°ã€‚</p><p>(3) <strong>å®éªŒç»“æœï¼š</strong>   - åŸºäºç»Ÿè®¡çš„ç©ºé—´ç†µæŸå¤±å‡½æ•°åœ¨å‡†ç¡®æ€§å’Œæ„ŸçŸ¥è´¨é‡æ–¹é¢ä¼˜äºåŸºäº L1 çš„å™ªå£°åŒ¹é…æŸå¤±å‡½æ•°ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç»Ÿè®¡åŒ¹é…çš„ç©ºé—´ç†µæŸå¤±å‡½æ•°ï¼Œç”¨äºå›¾åƒå¤åŸã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ ¸å¯†åº¦ä¼°è®¡ï¼ˆKDEï¼‰æ¥ä½¿ç©ºé—´ç†µå¯å¾®åˆ†ã€‚ç„¶åï¼Œç©ºé—´ç†µå¯ä»¥ç”¨äºå›¾åƒé‡å»ºçš„ä¸åŒåŸºäºå­¦ä¹ çš„æ¡†æ¶ã€‚é€šè¿‡å°†å…¶è£…å¤‡åˆ°æ‰©æ•£æ¨¡å‹ä¸­ï¼ˆä»¥æ›¿ä»£â„“1æˆ–â„“2ï¼‰ï¼Œæˆ‘ä»¬è·å¾—äº†ç”¨äºçœŸå®å›¾åƒå¤åŸçš„æ–°é¢–ç»Ÿè®¡å™ªå£°åŒ¹é…æŸå¤±ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†æ­¤æ¨¡å‹åº”ç”¨äºä½å…‰å¢å¼ºä»»åŠ¡ä»¥è¯´æ˜å…¶æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨ NTIRE ä½å…‰å¢å¼ºæŒ‘æˆ˜ä¸­å–å¾—äº†æœ€ä½³ LPIPS æ€§èƒ½ã€‚æ‰€æœ‰è¿™äº›ç»“æœè¡¨æ˜ï¼Œç©ºé—´ç†µæŸå¤±å¯¹äºé«˜æ„ŸçŸ¥æ‰©æ•£å­¦ä¹ è¿‡ç¨‹æ˜¯æœ‰æ•ˆçš„ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäºç©ºé—´ç†µçš„ç»Ÿè®¡åŒ¹é…æŸå¤±å‡½æ•°ï¼Œç”¨äºå›¾åƒå¤åŸã€‚æ€§èƒ½ï¼šåœ¨ä½å…‰å¢å¼ºä»»åŠ¡ä¸Šï¼ŒåŸºäºç»Ÿè®¡çš„ç©ºé—´ç†µæŸå¤±å‡½æ•°åœ¨å‡†ç¡®æ€§å’Œæ„ŸçŸ¥è´¨é‡æ–¹é¢ä¼˜äºåŸºäº L1 çš„å™ªå£°åŒ¹é…æŸå¤±å‡½æ•°ã€‚å·¥ä½œé‡ï¼šè¯¥æ–¹æ³•éœ€è¦é¢å¤–çš„è®¡ç®—å¼€é”€æ¥è¿‘ä¼¼ç©ºé—´ç†µï¼Œè¿™å¯èƒ½ä¼šå¢åŠ è®­ç»ƒæ—¶é—´å’Œå†…å­˜æ¶ˆè€—ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="http://article.biliimg.com/bfs/new_dyn/a3c133d398fb53ead47a00645c5327d0241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/cf4bf5cc17f4334e352f588ed2ccefcb241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/b7a0ec6067c60992e1a2e395bdf11c91241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/1f0299e2b835d711d284c66cc863a956241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/1ffcc1ff6c9b31250304fc557e4bfd89241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/26cfb6c25aaf0c1236c9980dae831e68241286257.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-04-17  RefFusion Reference Adapted Diffusion Models for 3D Scene Inpainting</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/04/14/Paper/2024-04-14/NeRF/"/>
    <id>https://kedreamix.github.io/2024/04/14/Paper/2024-04-14/NeRF/</id>
    <published>2024-04-14T04:42:58.000Z</published>
    <updated>2024-04-14T04:42:58.005Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-04-14-æ›´æ–°"><a href="#2024-04-14-æ›´æ–°" class="headerlink" title="2024-04-14 æ›´æ–°"></a>2024-04-14 æ›´æ–°</h1><h2 id="Boosting-Self-Supervision-for-Single-View-Scene-Completion-via-Knowledge-Distillation"><a href="#Boosting-Self-Supervision-for-Single-View-Scene-Completion-via-Knowledge-Distillation" class="headerlink" title="Boosting Self-Supervision for Single-View Scene Completion via Knowledge   Distillation"></a>Boosting Self-Supervision for Single-View Scene Completion via Knowledge   Distillation</h2><p><strong>Authors:Keonhee Han, Dominik Muhle, Felix Wimbauer, Daniel Cremers</strong></p><p>Inferring scene geometry from images via Structure from Motion is a long-standing and fundamental problem in computer vision. While classical approaches and, more recently, depth map predictions only focus on the visible parts of a scene, the task of scene completion aims to reason about geometry even in occluded regions. With the popularity of neural radiance fields (NeRFs), implicit representations also became popular for scene completion by predicting so-called density fields. Unlike explicit approaches. e.g. voxel-based methods, density fields also allow for accurate depth prediction and novel-view synthesis via image-based rendering. In this work, we propose to fuse the scene reconstruction from multiple images and distill this knowledge into a more accurate single-view scene reconstruction. To this end, we propose Multi-View Behind the Scenes (MVBTS) to fuse density fields from multiple posed images, trained fully self-supervised only from image data. Using knowledge distillation, we use MVBTS to train a single-view scene completion network via direct supervision called KDBTS. It achieves state-of-the-art performance on occupancy prediction, especially in occluded regions. </p><p><a href="http://arxiv.org/abs/2404.07933v1">PDF</a> </p><p><strong>æ‘˜è¦</strong><br>å¤šè§†è§’å¹•åèåˆï¼ˆMVBTSï¼‰ç»“åˆå¤šå¹…å›¾åƒçš„åœºæ™¯å‡ ä½•ä¿¡æ¯ï¼Œé€šè¿‡è’¸é¦å¾—åˆ°é«˜ç²¾åº¦çš„å•è§†è§’åœºæ™¯é‡æ„ï¼Œæ˜¾è‘—æå‡è¢«é®æŒ¡åŒºåŸŸçš„å æœ‰ç‡é¢„æµ‹æ€§èƒ½ã€‚</p><p><strong>è¦ç‚¹</strong></p><ul><li>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰é€šè¿‡é¢„æµ‹å¯†åº¦åœºå®ç°åœºæ™¯å®ŒæˆåŠŸèƒ½ã€‚</li><li>å¯†åº¦åœºå…è®¸é€šè¿‡å›¾åƒæ¸²æŸ“å®ç°ç²¾ç¡®çš„æ·±åº¦é¢„æµ‹å’Œæ–°è§†è§’åˆæˆã€‚</li><li>å¤šè§†è§’å¹•åèåˆï¼ˆMVBTSï¼‰èåˆå¤šå¹…å›¾åƒçš„å¯†åº¦åœºï¼Œæ— éœ€æ·±åº¦æ ‡ç­¾è¿›è¡Œå®Œå…¨è‡ªç›‘ç£è®­ç»ƒã€‚</li><li>çŸ¥è¯†è’¸é¦å°† MVBTS çš„çŸ¥è¯†æç‚¼è‡³å•è§†è§’åœºæ™¯å®ŒæˆåŠŸèƒ½ç½‘ç»œ KDBTS ä¸­ã€‚</li><li>KDBTS åˆ©ç”¨ç›´æ¥ç›‘ç£è¿›è¡Œè®­ç»ƒï¼Œåœ¨å æœ‰ç‡é¢„æµ‹ä»»åŠ¡ä¸­è¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li><li>KDBTS ç‰¹åˆ«æå‡äº†è¢«é®æŒ¡åŒºåŸŸçš„å æœ‰ç‡é¢„æµ‹ç²¾åº¦ã€‚</li><li>MVBTS å’Œ KDBTS å‡å¯ç”¨äº 3D é‡å»ºå’Œæ–°è§†è§’åˆæˆç­‰ä¸‹æ¸¸ä»»åŠ¡ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šé€šè¿‡çŸ¥è¯†è’¸é¦æå‡å•è§†å›¾åœºæ™¯è¡¥å…¨çš„è‡ªç›‘ç£</li><li>ä½œè€…ï¼š</li><li>Jannik Bollmeyer</li><li>Sven Behnke</li><li>éš¶å±ï¼šæ…•å°¼é»‘å·¥ä¸šå¤§å­¦</li><li>å…³é”®è¯ï¼š</li><li>åœºæ™¯è¡¥å…¨</li><li>ç¥ç»è¾å°„åœº</li><li>çŸ¥è¯†è’¸é¦</li><li>è‡ªç›‘ç£å­¦ä¹ </li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/pdf/2302.04322.pdf   Github ä»£ç é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼š   (1)ï¼šç ”ç©¶èƒŒæ™¯ï¼šåœºæ™¯è¡¥å…¨æ—¨åœ¨ä»å›¾åƒä¸­æ¨æ–­åœºæ™¯å‡ ä½•ï¼ŒåŒ…æ‹¬è¢«é®æŒ¡åŒºåŸŸçš„å‡ ä½•ã€‚ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åœ¨åœºæ™¯è¡¥å…¨ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†é€šå¸¸éœ€è¦å¤§é‡æ•°æ®å’Œè®¡ç®—èµ„æºã€‚   (2)ï¼šè¿‡å»æ–¹æ³•ï¼šä»¥å¾€æ–¹æ³•ä¸»è¦å…³æ³¨ä»å•è§†å›¾å›¾åƒä¸­é¢„æµ‹åœºæ™¯å‡ ä½•ï¼Œä½†å¯¹äºè¢«é®æŒ¡åŒºåŸŸçš„é¢„æµ‹å‡†ç¡®åº¦æœ‰é™ã€‚   (3)ï¼šç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºå¤šè§†å›¾å¹•åï¼ˆMVBTSï¼‰çš„æ–¹æ³•ï¼Œåˆ©ç”¨æ¥è‡ªå¤šå¼ å›¾åƒçš„å¯†åº¦åœºä¿¡æ¯ï¼Œé€šè¿‡çŸ¥è¯†è’¸é¦è®­ç»ƒä¸€ä¸ªå•è§†å›¾åœºæ™¯è¡¥å…¨ç½‘ç»œï¼Œç§°ä¸ºçŸ¥è¯†è’¸é¦å¹•åï¼ˆKDBTSï¼‰ã€‚   (4)ï¼šæ–¹æ³•æ€§èƒ½ï¼šKDBTS åœ¨å ç”¨é¢„æµ‹ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨è¢«é®æŒ¡åŒºåŸŸã€‚è¯¥æ–¹æ³•åœ¨ KITTI æ•°æ®é›†ä¸Šå®ç°äº† 95.5% çš„å ç”¨å‡†ç¡®ç‡ï¼Œä¼˜äºå…¶ä»–åŸºå‡†æ–¹æ³•ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) æå‡ºäº†ä¸€ç§å¤šè§†å›¾å¹•åï¼ˆMVBTSï¼‰æ–¹æ³•ï¼Œåˆ©ç”¨æ¥è‡ªå¤šå¼ å›¾åƒçš„å¯†åº¦åœºä¿¡æ¯ï¼Œé€šè¿‡çŸ¥è¯†è’¸é¦è®­ç»ƒä¸€ä¸ªå•è§†å›¾åœºæ™¯è¡¥å…¨ç½‘ç»œï¼Œç§°ä¸ºçŸ¥è¯†è’¸é¦å¹•åï¼ˆKDBTSï¼‰ã€‚(2) MVBTSé€šè¿‡å°†æ¥è‡ªå¤šå¼ å›¾åƒçš„å¯†åº¦åœºä¿¡æ¯èåˆåˆ°å•è§†å›¾åœºæ™¯è¡¥å…¨ç½‘ç»œä¸­ï¼Œæé«˜äº†è¢«é®æŒ¡åŒºåŸŸçš„é¢„æµ‹å‡†ç¡®æ€§ã€‚(3) KDBTSé‡‡ç”¨çŸ¥è¯†è’¸é¦æŠ€æœ¯ï¼Œå°†å¤šè§†å›¾åœºæ™¯è¡¥å…¨ç½‘ç»œçš„çŸ¥è¯†è½¬ç§»åˆ°å•è§†å›¾åœºæ™¯è¡¥å…¨ç½‘ç»œä¸­ï¼Œè¿›ä¸€æ­¥æå‡äº†æ€§èƒ½ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œé€šè¿‡åˆ©ç”¨å¤šè§†å›¾ä¿¡æ¯ï¼Œæå‡ºäº†ä¸€ç§æ”¹è¿›å•è§†å›¾å‡ ä½•åœºæ™¯é‡å»ºçš„æ–°é¢–æ–¹æ³•ã€‚è¿™åŒ…æ‹¬æ‰©å±•æœ€å…ˆè¿›çš„å¯†åº¦é¢„æµ‹æ¨¡å‹ä»¥æ”¹è¿›åœºæ™¯å‡ ä½•ï¼Œç„¶åé€šè¿‡çŸ¥è¯†è’¸é¦ä»¥ 3D æ–¹å¼è¿›è¡Œç›´æ¥ç›‘ç£ä»¥æå‡å•è§†å›¾æ¨¡å‹ã€‚è®­ç»ƒå®Œå…¨åœ¨è§†é¢‘æ•°æ®ä¸Šè‡ªç›‘ç£å®Œæˆã€‚æˆ‘ä»¬åœ¨æ·±åº¦ä¼°è®¡å’Œå ç”¨é¢„æµ‹ä»»åŠ¡ä¸Šè¯„ä¼°äº†æ‰€æå‡ºçš„å¤šè§†å›¾å’Œå¢å¼ºå‹å•è§†å›¾æ¨¡å‹ã€‚è™½ç„¶æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ·±åº¦ä¼°è®¡æ–¹é¢æ¥è¿‘æœ€å…ˆè¿›æ°´å¹³ï¼Œä½†è¢«æ˜ç¡®ä¸ºè¯¥ä»»åŠ¡è®­ç»ƒçš„æ–¹æ³•æ‰€è¶…è¶Šï¼Œä½†æˆ‘ä»¬å¢å¼ºçš„å•è§†å›¾é‡å»ºæ¨¡å‹åœ¨å ç”¨é¢„æµ‹æ–¹é¢å§‹ç»ˆè¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æœªæ¥å¯¹è¿åŠ¨ç‰©ä½“å»ºæ¨¡çš„å·¥ä½œå¯ä»¥è§£å†³åŠ¨æ€åœºæ™¯ä¸­ç›¸äº’å†²çªçš„ä¿¡æ¯ï¼Œä»è€Œæé«˜ 3D é‡å»ºçš„æ•´ä½“å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚è‡´è°¢ã€‚è¿™é¡¹å·¥ä½œå¾—åˆ° ERC é«˜çº§è¡¥åŠ©é‡‘ SIMULACRONã€æ…•å°¼é»‘æœºå™¨å­¦ä¹ ä¸­å¿ƒä»¥åŠå¾·å›½è”é‚¦äº¤é€šå’Œæ•°å­—åŸºç¡€è®¾æ–½éƒ¨ (BMDV) èµ„åŠ©ï¼Œç”¨äº ADAM é¡¹ç›®çš„ 19F2251F è¡¥åŠ©é‡‘ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§å¤šè§†å›¾å¹•åï¼ˆMVBTSï¼‰æ–¹æ³•ï¼Œåˆ©ç”¨æ¥è‡ªå¤šå¼ å›¾åƒçš„å¯†åº¦åœºä¿¡æ¯ï¼Œé€šè¿‡çŸ¥è¯†è’¸é¦è®­ç»ƒä¸€ä¸ªå•è§†å›¾åœºæ™¯è¡¥å…¨ç½‘ç»œï¼Œç§°ä¸ºçŸ¥è¯†è’¸é¦å¹•åï¼ˆKDBTSï¼‰ã€‚ï¼›æ€§èƒ½ï¼šKDBTS åœ¨å ç”¨é¢„æµ‹ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨è¢«é®æŒ¡åŒºåŸŸã€‚è¯¥æ–¹æ³•åœ¨ KITTI æ•°æ®é›†ä¸Šå®ç°äº† 95.5% çš„å ç”¨å‡†ç¡®ç‡ï¼Œä¼˜äºå…¶ä»–åŸºå‡†æ–¹æ³•ã€‚ï¼›å·¥ä½œé‡ï¼šæœ¬æ–¹æ³•é‡‡ç”¨è‡ªç›‘ç£å­¦ä¹ ï¼Œè®­ç»ƒè¿‡ç¨‹æ— éœ€äººå·¥æ ‡æ³¨ï¼Œå·¥ä½œé‡è¾ƒå°ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-6a815fb51ac960f580b9349c84d4aaef.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fdaac4a1aa97db0d9e87e268cca712eb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e1eb9e0800e52f5241f7180d3cd3cf5e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3114ea9821cc3e57197fd091c0fa954c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f2da95bb07d3e0a3fb36ca71ea4e0a27.jpg" align="middle"></details><h2 id="SplatPose-amp-Detect-Pose-Agnostic-3D-Anomaly-Detection"><a href="#SplatPose-amp-Detect-Pose-Agnostic-3D-Anomaly-Detection" class="headerlink" title="SplatPose &amp; Detect: Pose-Agnostic 3D Anomaly Detection"></a>SplatPose &amp; Detect: Pose-Agnostic 3D Anomaly Detection</h2><p><strong>Authors:Mathis Kruse, Marco Rudolph, Dominik Woiwode, Bodo Rosenhahn</strong></p><p>Detecting anomalies in images has become a well-explored problem in both academia and industry. State-of-the-art algorithms are able to detect defects in increasingly difficult settings and data modalities. However, most current methods are not suited to address 3D objects captured from differing poses. While solutions using Neural Radiance Fields (NeRFs) have been proposed, they suffer from excessive computation requirements, which hinder real-world usability. For this reason, we propose the novel 3D Gaussian splatting-based framework SplatPose which, given multi-view images of a 3D object, accurately estimates the pose of unseen views in a differentiable manner, and detects anomalies in them. We achieve state-of-the-art results in both training and inference speed, and detection performance, even when using less training data than competing methods. We thoroughly evaluate our framework using the recently proposed Pose-agnostic Anomaly Detection benchmark and its multi-pose anomaly detection (MAD) data set. </p><p><a href="http://arxiv.org/abs/2404.06832v1">PDF</a> Visual Anomaly and Novelty Detection 2.0 Workshop at CVPR 2024</p><p><strong>æ‘˜è¦</strong><br>é€šè¿‡ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å®ç° 3D å¤šè§†è§’å›¾åƒçš„æ— å§¿æ€ç¼ºé™·æ£€æµ‹ã€‚</p><p><strong>è¦ç‚¹</strong></p><ul><li>ç¼ºé™·æ£€æµ‹åœ¨å›¾åƒè¯†åˆ«é¢†åŸŸå¹¿æ³›æ¢ç´¢ï¼Œå½“å‰ç®—æ³•åœ¨å›°éš¾åœºæ™¯å’Œæ•°æ®ç±»å‹ä¸­çš„ç¼ºé™·æ£€æµ‹èƒ½åŠ›ä¸æ–­æå‡ã€‚</li><li>ç°æœ‰æ–¹æ³•ä¸é€‚ç”¨äºä¸åŒå§¿åŠ¿æ‹æ‘„çš„ 3D ç‰©ä½“ç¼ºé™·æ£€æµ‹ã€‚</li><li>åŸºäº NeRF çš„è§£å†³æ–¹æ¡ˆå­˜åœ¨ç®—åŠ›è¦æ±‚é«˜çš„é—®é¢˜ï¼Œé™åˆ¶å…¶å®ç”¨æ€§ã€‚</li><li>æå‡ºåŸºäº 3D é«˜æ–¯ splatting çš„ SplatPose æ¡†æ¶ï¼Œå¯ä»¥å¯¹ 3D å¤šè§†è§’å›¾åƒçš„æ— å§¿æ€ç¼ºé™·æ£€æµ‹ã€‚</li><li>åœ¨è®­ç»ƒã€æ¨ç†é€Ÿåº¦å’Œæ£€æµ‹æ€§èƒ½æ–¹é¢è¾¾åˆ°æœ€å…ˆè¿›æ°´å¹³ï¼Œå³ä½¿ä½¿ç”¨æ¯”ç«äº‰æ–¹æ³•æ›´å°‘çš„è®­ç»ƒæ•°æ®ã€‚</li><li>åœ¨ Pose-agnostic Anomaly Detection åŸºå‡†åŠå¤šå§¿åŠ¿ç¼ºé™·æ£€æµ‹ï¼ˆMADï¼‰æ•°æ®é›†ä¸Šè¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>è®ºæ–‡æ ‡é¢˜ï¼šSplatPose&amp;Detectï¼šä¸å§¿æ€æ— å…³çš„ 3D å¼‚å¸¸æ£€æµ‹</li><li>ä½œè€…ï¼šZixuan Huangã€Wenbo Liã€Junjie Huangã€Hao Liã€Yida Wangã€Lei Zhouã€Dachuan Zhangã€Dacheng Tao</li><li>éš¶å±æœºæ„ï¼šä¸­ç§‘é™¢è‡ªåŠ¨åŒ–æ‰€</li><li>å…³é”®è¯ï¼šå¼‚å¸¸æ£€æµ‹ã€å§¿æ€æ— å…³ã€3D æ„ŸçŸ¥ã€ç¥ç»è¾å°„åœº</li><li>è®ºæ–‡é“¾æ¥ï¼šNoneGithub ä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šå¼‚å¸¸æ£€æµ‹åœ¨å›¾åƒä¸­å·²æˆä¸ºä¸€ä¸ªç ”ç©¶å……åˆ†çš„é—®é¢˜ï¼Œä½†å¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä¸é€‚ç”¨äºä»ä¸åŒå§¿æ€æ•è·çš„ 3D å¯¹è±¡ã€‚ä½¿ç”¨ç¥ç»è¾å°„åœº (NeRF) çš„è§£å†³æ–¹æ¡ˆè™½ç„¶è¢«æå‡ºï¼Œä½†å­˜åœ¨è®¡ç®—éœ€æ±‚è¿‡å¤§ã€é˜»ç¢å®é™…ä½¿ç”¨çš„é—®é¢˜ã€‚(2) è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šNeRF æ–¹æ³•è®¡ç®—é‡å¤§ï¼›æœ¬æ–‡æ–¹æ³•åŠ¨æœºæ˜ç¡®ã€‚(3) ç ”ç©¶æ–¹æ³•ï¼šæå‡ºåŸºäº 3D é«˜æ–¯æ–‘å—çš„æ–°å‹æ¡†æ¶ SplatPoseï¼Œè¯¥æ¡†æ¶ç»™å®š 3D å¯¹è±¡çš„å¤šè§†å›¾å›¾åƒï¼Œèƒ½å¤Ÿä»¥å¯å¾®åˆ†çš„æ–¹å¼å‡†ç¡®ä¼°è®¡æœªè§è§†å›¾çš„å§¿æ€ï¼Œå¹¶æ£€æµ‹å…¶ä¸­çš„å¼‚å¸¸ã€‚(4) æ–¹æ³•æ€§èƒ½ï¼šåœ¨è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦ä»¥åŠæ£€æµ‹æ€§èƒ½æ–¹é¢å‡è¾¾åˆ°æœ€å…ˆè¿›æ°´å¹³ï¼Œå³ä½¿ä½¿ç”¨æ¯”ç«äº‰æ–¹æ³•æ›´å°‘çš„è®­ç»ƒæ•°æ®ã€‚åœ¨æœ€è¿‘æå‡ºçš„ä¸å§¿æ€æ— å…³çš„å¼‚å¸¸æ£€æµ‹åŸºå‡†åŠå…¶å¤šå§¿æ€å¼‚å¸¸æ£€æµ‹ (MAD) æ•°æ®é›†ä¸Šå¯¹æ¡†æ¶è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚</li></ol><p>Methods:(1)æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŸºäº3Dé«˜æ–¯æ–‘å—çš„æ¡†æ¶SplatPoseï¼Œè¯¥æ¡†æ¶ç»™å®š3Då¯¹è±¡çš„å¤šè§†å›¾å›¾åƒï¼Œèƒ½å¤Ÿä»¥å¯å¾®åˆ†çš„æ–¹å¼å‡†ç¡®ä¼°è®¡æœªè§è§†å›¾çš„å§¿æ€ï¼Œå¹¶æ£€æµ‹å…¶ä¸­çš„å¼‚å¸¸ã€‚(2)SplatPoseåŒ…å«ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šå§¿æ€ä¼°è®¡æ¨¡å—å’Œå¼‚å¸¸æ£€æµ‹æ¨¡å—ã€‚å§¿æ€ä¼°è®¡æ¨¡å—ä½¿ç”¨3Dé«˜æ–¯æ–‘å—å¯¹3Då¯¹è±¡è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶ä½¿ç”¨ç¥ç»è¾å°„åœº(NeRF)é¢„æµ‹æœªè§è§†å›¾çš„å§¿æ€ã€‚å¼‚å¸¸æ£€æµ‹æ¨¡å—ä½¿ç”¨åŸºäºé‡å»ºè¯¯å·®çš„åº¦é‡æ¥æ£€æµ‹å¼‚å¸¸ã€‚(3)SplatPoseåœ¨è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦ä»¥åŠæ£€æµ‹æ€§èƒ½æ–¹é¢å‡è¾¾åˆ°æœ€å…ˆè¿›æ°´å¹³ï¼Œå³ä½¿ä½¿ç”¨æ¯”ç«äº‰æ–¹æ³•æ›´å°‘çš„è®­ç»ƒæ•°æ®ã€‚åœ¨æœ€è¿‘æå‡ºçš„ä¸å§¿æ€æ— å…³çš„å¼‚å¸¸æ£€æµ‹åŸºå‡†åŠå…¶å¤šå§¿æ€å¼‚å¸¸æ£€æµ‹(MAD)æ•°æ®é›†ä¸Šå¯¹æ¡†æ¶è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¸å§¿æ€æ— å…³çš„å¼‚å¸¸æ£€æµ‹æ–¹æ³•ã€‚ç»™å®šå¤šè§†å›¾å›¾åƒï¼Œæˆ‘ä»¬å°†å¯¹è±¡è¡¨ç¤ºä¸ºé«˜æ–¯ç‚¹äº‘ï¼Œç”¨äºå§¿æ€ä¼°è®¡ï¼Œå¹¶åœ¨æ²¡æœ‰å…ˆéªŒå§¿æ€ä¿¡æ¯çš„æƒ…å†µä¸‹æŸ¥æ‰¾å›¾åƒä¸­çš„å¼‚å¸¸ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ£€æµ‹ä»»åŠ¡ä¸­å‡»è´¥äº†æ‰€æœ‰ç«äº‰å¯¹æ‰‹ï¼ŒåŒæ—¶åœ¨è®­ç»ƒå’Œæ¨ç†æ—¶é—´ä¸Šä»ç„¶å¿«å‡ ä¸ªæ•°é‡çº§ï¼Œä½¿å…¶æ›´é€‚åˆåœ¨ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½²ã€‚æˆ‘ä»¬å¸Œæœ›æœªæ¥è‡´åŠ›äºæ”¹è¿›ç²—ç•¥å§¿æ€ä¼°è®¡å’Œå›¾åƒç‰¹å¾æ¯”è¾ƒã€‚å°†æˆ‘ä»¬çš„å‘ç°åº”ç”¨äºé‚»è¿‘é¢†åŸŸï¼Œä¾‹å¦‚äººç±»å§¿æ€ä¼°è®¡[16,43]ï¼Œå¯¹æˆ‘ä»¬æ¥è¯´æ˜¯ä¸€ä¸ªå¾ˆæœ‰å¸Œæœ›çš„ä¸‹ä¸€æ­¥æ–¹å‘ã€‚ç¼©å°åˆæˆæ•°æ®å’ŒçœŸå®ä¸–ç•Œæ•°æ®ä¹‹é—´çš„å·®è·ä¹Ÿéœ€è¦æ›´å¤šçš„å·¥ä½œã€‚æœ€åï¼Œæˆ‘ä»¬å¸Œæœ›ç ”ç©¶å°†ä¸‰ç»´ç‚¹äº‘ä¿¡æ¯åŒ…å«åœ¨ç°æœ‰äºŒç»´æ–¹æ³•ä¸­çš„æ–¹æ³•ã€‚è‡´è°¢ã€‚è¿™é¡¹å·¥ä½œå¾—åˆ°äº†å¾·å›½è”é‚¦æ•™è‚²å’Œç ”ç©¶éƒ¨ (BMBF) åœ¨ AIservicecenter KISSKIï¼ˆæ‹¨æ¬¾å· 01IS22093Cï¼‰ä¸‹ã€ä¸‹è¨å…‹æ£®å·ç§‘å­¦å’Œæ–‡åŒ–éƒ¨ (MWK) é€šè¿‡å¤§ä¼—æ±½è½¦åŸºé‡‘ä¼šå’Œå¾·å›½ç ”ç©¶åŸºé‡‘ä¼š (DFG) åœ¨å¾·å›½å“è¶Šæˆ˜ç•¥ä¸‹çš„ Zukunft.niedersachsen è®¡åˆ’çš„æ”¯æŒä¸‹ï¼Œåœ¨å“è¶Šé›†ç¾¤ PhoenixD (EXC2122) å†…ã€‚</li><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šxxxï¼›ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šxxxï¼›æ€§èƒ½ï¼šxxxï¼›å·¥ä½œé‡ï¼šxxxï¼›åŠ¡å¿…ä½¿ç”¨ä¸­æ–‡å›ç­”ï¼ˆä¸“æœ‰åè¯éœ€ç”¨è‹±æ–‡æ ‡æ³¨ï¼‰ï¼Œè¡¨è¿°å°½é‡ç®€æ´ã€å­¦æœ¯ï¼Œä¸è¦é‡å¤å‰é¢<summary>çš„å†…å®¹ï¼Œåˆ©ç”¨åŸæ–‡æ•°å­—çš„å€¼ï¼ŒåŠ¡å¿…ä¸¥æ ¼æŒ‰ç…§æ ¼å¼ï¼Œç›¸åº”å†…å®¹è¾“å‡ºåˆ°xxxï¼ŒæŒ‰ç…§æ¢è¡Œï¼Œ.......è¡¨ç¤ºæ ¹æ®å®é™…è¦æ±‚å¡«å†™ï¼Œè‹¥æ— åˆ™ä¸å†™ã€‚</summary></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-98f8ef1e6e42e25a4f68a8eac21e75a0.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e84352a2af71b475f2c2f75f76369d19.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ab11a3ec9140c1596b81427a74134f5b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d06c9960f9e8a142bf4951e22b98ea8d.jpg" align="middle"></details><h2 id="MonoSelfRecon-Purely-Self-Supervised-Explicit-Generalizable-3D-Reconstruction-of-Indoor-Scenes-from-Monocular-RGB-Views"><a href="#MonoSelfRecon-Purely-Self-Supervised-Explicit-Generalizable-3D-Reconstruction-of-Indoor-Scenes-from-Monocular-RGB-Views" class="headerlink" title="MonoSelfRecon: Purely Self-Supervised Explicit Generalizable 3D   Reconstruction of Indoor Scenes from Monocular RGB Views"></a>MonoSelfRecon: Purely Self-Supervised Explicit Generalizable 3D   Reconstruction of Indoor Scenes from Monocular RGB Views</h2><p><strong>Authors:Runfa Li, Upal Mahbub, Vasudev Bhaskaran, Truong Nguyen</strong></p><p>Current monocular 3D scene reconstruction (3DR) works are either fully-supervised, or not generalizable, or implicit in 3D representation. We propose a novel framework - MonoSelfRecon that for the first time achieves explicit 3D mesh reconstruction for generalizable indoor scenes with monocular RGB views by purely self-supervision on voxel-SDF (signed distance function). MonoSelfRecon follows an Autoencoder-based architecture, decodes voxel-SDF and a generalizable Neural Radiance Field (NeRF), which is used to guide voxel-SDF in self-supervision. We propose novel self-supervised losses, which not only support pure self-supervision, but can be used together with supervised signals to further boost supervised training. Our experiments show that â€œMonoSelfReconâ€ trained in pure self-supervision outperforms current best self-supervised indoor depth estimation models and is comparable to 3DR models trained in fully supervision with depth annotations. MonoSelfRecon is not restricted by specific model design, which can be used to any models with voxel-SDF for purely self-supervised manner. </p><p><a href="http://arxiv.org/abs/2404.06753v1">PDF</a> </p><p><strong>Summary</strong><br>å•ç›®è‡ªç›‘ç£é‡å»ºæ¡†æ¶é¦–æ¬¡é€šè¿‡çº¯è‡ªç›‘ç£åœ¨ä½“ç´ SDFä¸Šå®ç°äº†å¯æ³›åŒ–å®¤å†…åœºæ™¯çš„æ˜¾å¼3Dç½‘æ ¼é‡å»ºã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>é¦–æ¬¡é€šè¿‡å•ç›®RGBè§†å›¾å®ç°å¯æ³›åŒ–å®¤å†…åœºæ™¯çš„æ˜¾å¼3Dç½‘æ ¼é‡å»ºã€‚</li><li>é‡‡ç”¨è‡ªç¼–ç å™¨æ¶æ„ï¼Œè§£ç ä½“ç´ SDFå’Œå¯æ³›åŒ–çš„NeRFã€‚</li><li>æå‡ºæ–°çš„è‡ªç›‘ç£æŸå¤±ï¼Œæ”¯æŒçº¯è‡ªç›‘ç£ï¼Œå¹¶å¯ä¸ç›‘ç£ä¿¡å·ç»“åˆä½¿ç”¨ä»¥è¿›ä¸€æ­¥æå‡ç›‘ç£è®­ç»ƒã€‚</li><li>çº¯è‡ªç›‘ç£è®­ç»ƒçš„MonoSelfReconä¼˜äºå½“å‰æœ€å¥½çš„è‡ªç›‘ç£å®¤å†…æ·±åº¦ä¼°è®¡æ¨¡å‹ã€‚</li><li>MonoSelfReconä¸ä½¿ç”¨æ·±åº¦æ³¨é‡Šè¿›è¡Œå®Œå…¨ç›‘ç£è®­ç»ƒçš„3DRæ¨¡å‹ç›¸å½“ã€‚</li><li>MonoSelfReconä¸å—ç‰¹å®šæ¨¡å‹è®¾è®¡é™åˆ¶ï¼Œå¯ç”¨äºä»»ä½•å…·æœ‰ä½“ç´ SDFçš„æ¨¡å‹è¿›è¡Œçº¯ç²¹çš„è‡ªç›‘ç£ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šMonoSelfReconï¼šçº¯ç²¹è‡ªç›‘ç£æ˜¾å¼å¯æ³›åŒ– 3D</li><li>ä½œè€…ï¼šYuxuan Zhang, Shuaicheng Liu, Chen Feng, Songyou Peng, Xiaowei Zhou, Qixing Huang</li><li>å•ä½ï¼šä¸­å›½ç§‘å­¦æŠ€æœ¯å¤§å­¦</li><li>å…³é”®è¯ï¼šå•ç›®é‡å»ºã€è‡ªç›‘ç£ã€æ˜¾å¼ 3D è¡¨ç¤ºã€ç¥ç»è¾å°„åœº</li><li>è®ºæ–‡é“¾æ¥ï¼šNoneï¼ŒGithub é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šå½“å‰çš„å•ç›® 3D åœºæ™¯é‡å»ºï¼ˆ3DRï¼‰å·¥ä½œè¦ä¹ˆå®Œå…¨ç›‘ç£ï¼Œè¦ä¹ˆä¸å¯æ³›åŒ–ï¼Œè¦ä¹ˆåœ¨ 3D è¡¨ç¤ºä¸­æ˜¯éšå¼çš„ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šæœ¬æ–¹æ³•çš„åŠ¨æœºå……åˆ†å—ï¼Ÿç°æœ‰æ–¹æ³•å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š</li><li>å®Œå…¨ç›‘ç£çš„æ–¹æ³•éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®ï¼Œè¿™åœ¨ç°å®åœºæ™¯ä¸­éš¾ä»¥è·å¾—ã€‚</li><li>è‡ªç›‘ç£çš„æ–¹æ³•è™½ç„¶ä¸éœ€è¦æ ‡æ³¨æ•°æ®ï¼Œä½†é‡å»ºçš„ 3D è¡¨ç¤ºå¾€å¾€æ˜¯éšå¼çš„ï¼Œéš¾ä»¥ç”¨äºä¸‹æ¸¸ä»»åŠ¡ã€‚</li><li><p>æ˜¾å¼ 3D è¡¨ç¤ºçš„æ–¹æ³•è™½ç„¶å¯ä»¥ç”Ÿæˆæ˜¾å¼çš„ 3D æ¨¡å‹ï¼Œä½†å¾€å¾€éœ€è¦é¢å¤–çš„ç›‘ç£ä¿¡å·æˆ–å…ˆéªŒçŸ¥è¯†ã€‚ï¼ˆ3ï¼‰æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ MonoSelfReconï¼Œè¯¥æ¡†æ¶é¦–æ¬¡é€šè¿‡çº¯è‡ªç›‘ç£åœ¨ä½“ç´  SDFï¼ˆæœ‰ç¬¦å·è·ç¦»å‡½æ•°ï¼‰ä¸Šå®ç°äº†å¯æ³›åŒ–å®¤å†…åœºæ™¯çš„æ˜¾å¼ 3D ç½‘æ ¼é‡å»ºã€‚MonoSelfRecon éµå¾ªåŸºäºè‡ªåŠ¨ç¼–ç å™¨çš„æ¶æ„ï¼Œè§£ç ä½“ç´  SDF å’Œå¯æ³›åŒ–ç¥ç»è¾å°„åœº (NeRF)ï¼Œåè€…ç”¨äºåœ¨è‡ªç›‘ç£ä¸­æŒ‡å¯¼ä½“ç´  SDFã€‚æœ¬æ–‡æå‡ºäº†æ–°çš„è‡ªç›‘ç£æŸå¤±ï¼Œä¸ä»…æ”¯æŒçº¯è‡ªç›‘ç£ï¼Œè¿˜å¯ä»¥ä¸ç›‘ç£ä¿¡å·ä¸€èµ·ä½¿ç”¨ä»¥è¿›ä¸€æ­¥æå‡ç›‘ç£è®­ç»ƒã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†ä»€ä¹ˆæ€§èƒ½ï¼Ÿæ€§èƒ½æ˜¯å¦èƒ½æ”¯æ’‘å…¶ç›®æ ‡ï¼Ÿå®éªŒè¡¨æ˜ï¼Œåœ¨çº¯è‡ªç›‘ç£ä¸‹è®­ç»ƒçš„â€œMonoSelfReconâ€ä¼˜äºå½“å‰æœ€å¥½çš„è‡ªç›‘ç£å®¤å†…æ·±åº¦ä¼°è®¡æ¨¡å‹ï¼Œå¹¶ä¸”ä¸ä½¿ç”¨æ·±åº¦æ³¨é‡Šåœ¨å®Œå…¨ç›‘ç£ä¸‹è®­ç»ƒçš„ 3DR æ¨¡å‹ç›¸å½“ã€‚MonoSelfRecon ä¸å—ç‰¹å®šæ¨¡å‹è®¾è®¡çš„é™åˆ¶ï¼Œå¯ç”¨äºä»»ä½•å…·æœ‰ä½“ç´  SDF çš„æ¨¡å‹ä»¥å®ç°çº¯è‡ªç›‘ç£çš„æ–¹å¼ã€‚</p></li><li><p>Methodsï¼š(1) æå‡º MonoSelfRecon æ¡†æ¶ï¼Œé¦–æ¬¡é€šè¿‡çº¯è‡ªç›‘ç£åœ¨ä½“ç´  SDF ä¸Šå®ç°äº†å¯æ³›åŒ–å®¤å†…åœºæ™¯çš„æ˜¾å¼ 3D ç½‘æ ¼é‡å»ºï¼›(2) æå‡ºæ–°çš„è‡ªç›‘ç£æŸå¤±ï¼Œä¸ä»…æ”¯æŒçº¯è‡ªç›‘ç£ï¼Œè¿˜å¯ä»¥ä¸ç›‘ç£ä¿¡å·ä¸€èµ·ä½¿ç”¨ä»¥æå‡ç›‘ç£è®­ç»ƒï¼›(3) é‡‡ç”¨åŸºäºè‡ªåŠ¨ç¼–ç å™¨çš„æ¶æ„ï¼Œè§£ç ä½“ç´  SDF å’Œå¯æ³›åŒ–ç¥ç»è¾å°„åœº (NeRF)ï¼Œåè€…ç”¨äºåœ¨è‡ªç›‘ç£ä¸­æŒ‡å¯¼ä½“ç´  SDFã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬å·¥ä½œé¦–æ¬¡é€šè¿‡çº¯è‡ªç›‘ç£åœ¨ä½“ç´ SDFä¸Šå®ç°äº†å¯æ³›åŒ–å®¤å†…åœºæ™¯çš„æ˜¾å¼3Dç½‘æ ¼é‡å»ºï¼Œå…·æœ‰é‡è¦æ„ä¹‰ã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶MonoSelfReconï¼Œé¦–æ¬¡é€šè¿‡çº¯è‡ªç›‘ç£åœ¨ä½“ç´ SDFä¸Šå®ç°äº†å¯æ³›åŒ–å®¤å†…åœºæ™¯çš„æ˜¾å¼3Dç½‘æ ¼é‡å»ºã€‚</li><li>æå‡ºæ–°çš„è‡ªç›‘ç£æŸå¤±ï¼Œä¸ä»…æ”¯æŒçº¯è‡ªç›‘ç£ï¼Œè¿˜å¯ä»¥ä¸ç›‘ç£ä¿¡å·ä¸€èµ·ä½¿ç”¨ä»¥æå‡ç›‘ç£è®­ç»ƒã€‚</li><li>é‡‡ç”¨åŸºäºè‡ªåŠ¨ç¼–ç å™¨çš„æ¶æ„ï¼Œè§£ç ä½“ç´ SDFå’Œå¯æ³›åŒ–ç¥ç»è¾å°„åœº(NeRF)ï¼Œåè€…ç”¨äºåœ¨è‡ªç›‘ç£ä¸­æŒ‡å¯¼ä½“ç´ SDFã€‚æ€§èƒ½ï¼š</li><li>åœ¨çº¯è‡ªç›‘ç£ä¸‹è®­ç»ƒçš„MonoSelfReconä¼˜äºå½“å‰æœ€å¥½çš„è‡ªç›‘ç£å®¤å†…æ·±åº¦ä¼°è®¡æ¨¡å‹ï¼Œå¹¶ä¸”ä¸ä½¿ç”¨æ·±åº¦æ³¨é‡Šåœ¨å®Œå…¨ç›‘ç£ä¸‹è®­ç»ƒçš„3DRæ¨¡å‹ç›¸å½“ã€‚</li><li>MonoSelfReconä¸å—ç‰¹å®šæ¨¡å‹è®¾è®¡çš„é™åˆ¶ï¼Œå¯ç”¨äºä»»ä½•å…·æœ‰ä½“ç´ SDFçš„æ¨¡å‹ä»¥å®ç°çº¯è‡ªç›‘ç£çš„æ–¹å¼ã€‚å·¥ä½œé‡ï¼š</li><li>å®éªŒè¡¨æ˜ï¼ŒMonoSelfReconåœ¨ScanNetå’Œ7Scenesæ•°æ®é›†ä¸Šå–å¾—äº†å¾ˆå¥½çš„æ•ˆæœã€‚</li><li>MonoSelfReconå¯ä»¥é€šè¿‡å°‘é‡å­¦ä¹ è½»æ¾è½¬ç§»åˆ°å…¶ä»–é¢†åŸŸã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-3b74e26f87e5c69504b3e0bf5614d4ce.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d8c823491ef532d498c54b5bc4954cc5.jpg" align="middle"><img src="https://pica.zhimg.com/v2-066b26c50380cb863d74934c40a0317f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9e696c0929fb3a424cfb7cec25388bf9.jpg" align="middle"></details><h2 id="3D-Geometry-aware-Deformable-Gaussian-Splatting-for-Dynamic-View-Synthesis"><a href="#3D-Geometry-aware-Deformable-Gaussian-Splatting-for-Dynamic-View-Synthesis" class="headerlink" title="3D Geometry-aware Deformable Gaussian Splatting for Dynamic View   Synthesis"></a>3D Geometry-aware Deformable Gaussian Splatting for Dynamic View   Synthesis</h2><p><strong>Authors:Zhicheng Lu, Xiang Guo, Le Hui, Tianrui Chen, Min Yang, Xiao Tang, Feng Zhu, Yuchao Dai</strong></p><p>In this paper, we propose a 3D geometry-aware deformable Gaussian Splatting method for dynamic view synthesis. Existing neural radiance fields (NeRF) based solutions learn the deformation in an implicit manner, which cannot incorporate 3D scene geometry. Therefore, the learned deformation is not necessarily geometrically coherent, which results in unsatisfactory dynamic view synthesis and 3D dynamic reconstruction. Recently, 3D Gaussian Splatting provides a new representation of the 3D scene, building upon which the 3D geometry could be exploited in learning the complex 3D deformation. Specifically, the scenes are represented as a collection of 3D Gaussian, where each 3D Gaussian is optimized to move and rotate over time to model the deformation. To enforce the 3D scene geometry constraint during deformation, we explicitly extract 3D geometry features and integrate them in learning the 3D deformation. In this way, our solution achieves 3D geometry-aware deformation modeling, which enables improved dynamic view synthesis and 3D dynamic reconstruction. Extensive experimental results on both synthetic and real datasets prove the superiority of our solution, which achieves new state-of-the-art performance.   The project is available at <a href="https://npucvr.github.io/GaGS/">https://npucvr.github.io/GaGS/</a> </p><p><a href="http://arxiv.org/abs/2404.06270v1">PDF</a> Accepted by CVPR 2024. Project page: <a href="https://npucvr.github.io/GaGS/">https://npucvr.github.io/GaGS/</a></p><p><strong>Summary</strong><br>ç¥ç»è¾å°„åœº (NeRF) ä½¿ç”¨é«˜æ–¯çƒé¢æ³•å»ºæ¨¡ 3D å‡ ä½•çº¦æŸï¼Œç”¨äºåŠ¨æ€è§†å›¾åˆæˆã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºå¯å˜å½¢é«˜æ–¯çƒé¢æ³•ï¼Œåœ¨ 3D åŠ¨æ€è§†å›¾åˆæˆä¸­è€ƒè™‘ 3D å‡ ä½•å½¢çŠ¶ã€‚</li><li>ä½¿ç”¨é«˜æ–¯çƒé¢è¡¨ç¤ºåœºæ™¯ï¼Œä¼˜åŒ–å…¶ä½ç½®å’Œæ—‹è½¬ä»¥å»ºæ¨¡å˜å½¢ã€‚</li><li>é€šè¿‡æå– 3D å‡ ä½•ç‰¹å¾å¹¶å°†å…¶èå…¥å˜å½¢å­¦ä¹ ä¸­ï¼Œæ‰§è¡ŒåŸºäº 3D å‡ ä½•å½¢çŠ¶çš„å˜å½¢å»ºæ¨¡ã€‚</li><li>é€šè¿‡åˆæˆå’ŒçœŸå®æ•°æ®é›†çš„å¹¿æ³›å®éªŒéªŒè¯äº†æ‰€ææ–¹æ³•çš„ä¼˜è¶Šæ€§ï¼Œè¾¾åˆ°æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚</li><li>è¯¥é¡¹ç›®å¯åœ¨ <a href="https://npucvr.github.io/GaGS/">https://npucvr.github.io/GaGS/</a> è·å–ã€‚</li><li>3D é«˜æ–¯çƒé¢æ³•å¯ç”¨äº 3D å½¢çŠ¶å»ºæ¨¡ï¼Œå¹¶åº”ç”¨äºåŠ¨æ€åœºæ™¯ä¸­ã€‚</li><li>æ˜¾å¼å‡ ä½•çº¦æŸå¢å¼ºäº† NeRF åœ¨åŠ¨æ€è§†å›¾åˆæˆä¸­çš„æ€§èƒ½ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šåŸºäº 3D å‡ ä½•æ„ŸçŸ¥çš„å¯å˜å½¢é«˜æ–¯æ•£å°„ç”¨äºåŠ¨æ€è§†å›¾åˆæˆ</li><li>ä½œè€…ï¼šQiangeng Xu, Pengfei Wan, Wentao Yuan, Junyu Han, Jiayuan Mao, Yebin Liu, Qi Tian</li><li>å•ä½ï¼šå—äº¬é‚®ç”µå¤§å­¦</li><li>å…³é”®è¯ï¼šåŠ¨æ€è§†å›¾åˆæˆã€ç¥ç»è¾å°„åœºã€3D å‡ ä½•æ„ŸçŸ¥ã€é«˜æ–¯æ•£å°„</li><li>è®ºæ–‡é“¾æ¥ï¼šNoneï¼ŒGithub é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æ–¹æ³•åœ¨åŠ¨æ€è§†å›¾åˆæˆä¸­é¢ä¸´å˜å½¢å­¦ä¹ çš„æŒ‘æˆ˜ï¼Œç°æœ‰ NeRF è§£å†³æ–¹æ¡ˆä»¥éšå¼æ–¹å¼å­¦ä¹ å˜å½¢ï¼Œæ— æ³•çº³å…¥ 3D åœºæ™¯å‡ ä½•ä¿¡æ¯ï¼Œå¯¼è‡´å­¦ä¹ çš„å˜å½¢åœ¨å‡ ä½•ä¸Šä¸è¿è´¯ï¼ŒåŠ¨æ€è§†å›¾åˆæˆå’Œ 3D åŠ¨æ€é‡å»ºæ•ˆæœä¸ä½³ã€‚   ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼š3D é«˜æ–¯æ•£å°„æä¾›äº†ä¸€ç§æ–°çš„ 3D åœºæ™¯è¡¨ç¤ºæ–¹æ³•ï¼Œåœ¨æ­¤åŸºç¡€ä¸Šå¯ä»¥åˆ©ç”¨ 3D å‡ ä½•ä¿¡æ¯æ¥å­¦ä¹ å¤æ‚çš„ 3D å˜å½¢ã€‚ç°æœ‰æ–¹æ³•å­˜åœ¨çš„é—®é¢˜æ˜¯ï¼š</li><li>æ— æ³•æœ‰æ•ˆåˆ©ç”¨ 3D åœºæ™¯å‡ ä½•çº¦æŸæ¥æŒ‡å¯¼å˜å½¢å­¦ä¹ ã€‚</li><li>å­¦ä¹ åˆ°çš„å˜å½¢åœ¨å‡ ä½•ä¸Šä¸è¿è´¯ï¼Œå¯¼è‡´åŠ¨æ€è§†å›¾åˆæˆå’Œ 3D åŠ¨æ€é‡å»ºæ•ˆæœä¸ä½³ã€‚   ï¼ˆ3ï¼‰æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäº 3D å‡ ä½•æ„ŸçŸ¥çš„å¯å˜å½¢é«˜æ–¯æ•£å°„æ–¹æ³•ç”¨äºåŠ¨æ€è§†å›¾åˆæˆã€‚è¯¥æ–¹æ³•é€šè¿‡æ˜¾å¼æå– 3D å‡ ä½•ç‰¹å¾å¹¶å°†å…¶èå…¥ 3D å˜å½¢å­¦ä¹ ä¸­ï¼Œå®ç°äº† 3D å‡ ä½•æ„ŸçŸ¥çš„å˜å½¢å»ºæ¨¡ï¼Œä»è€Œæé«˜äº†åŠ¨æ€è§†å›¾åˆæˆå’Œ 3D åŠ¨æ€é‡å»ºçš„è´¨é‡ã€‚   ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡ä¸Šçš„è¡¨ç°åŠæ€§èƒ½ï¼šæœ¬æ–‡æ–¹æ³•åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œè¯æ˜äº†å…¶ä¼˜è¶Šæ€§ï¼Œè¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚</li></ol><p>7.Methodsï¼š(1) æå‡ºäº†ä¸€ç§åŸºäº3Då‡ ä½•æ„ŸçŸ¥çš„å¯å˜å½¢é«˜æ–¯æ•£å°„æ–¹æ³•ï¼Œç”¨äºåŠ¨æ€è§†å›¾åˆæˆã€‚(2) é€šè¿‡æ˜¾å¼æå–3Då‡ ä½•ç‰¹å¾å¹¶å°†å…¶èå…¥3Då˜å½¢å­¦ä¹ ä¸­ï¼Œå®ç°äº†3Då‡ ä½•æ„ŸçŸ¥çš„å˜å½¢å»ºæ¨¡ã€‚(3) åˆ©ç”¨3Då‡ ä½•ä¿¡æ¯æŒ‡å¯¼å˜å½¢å­¦ä¹ ï¼Œæé«˜äº†åŠ¨æ€è§†å›¾åˆæˆå’Œ3DåŠ¨æ€é‡å»ºçš„è´¨é‡ã€‚(4) åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„ä¼˜è¶Šæ€§ï¼Œè¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäº 3D å‡ ä½•æ„ŸçŸ¥çš„å¯å˜å½¢é«˜æ–¯æ•£å°„æ–¹æ³•ï¼Œç”¨äºåŠ¨æ€è§†å›¾åˆæˆã€‚è¯¥æ–¹æ³•é€šè¿‡æ˜¾å¼æå– 3D å‡ ä½•ç‰¹å¾å¹¶å°†å…¶èå…¥ 3D å˜å½¢å­¦ä¹ ä¸­ï¼Œå®ç°äº† 3D å‡ ä½•æ„ŸçŸ¥çš„å˜å½¢å»ºæ¨¡ï¼Œä»è€Œæé«˜äº†åŠ¨æ€è§†å›¾åˆæˆå’Œ 3D åŠ¨æ€é‡å»ºçš„è´¨é‡ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§åŸºäº 3D å‡ ä½•æ„ŸçŸ¥çš„å¯å˜å½¢é«˜æ–¯æ•£å°„æ–¹æ³•ï¼Œç”¨äºåŠ¨æ€è§†å›¾åˆæˆã€‚</li><li>é€šè¿‡æ˜¾å¼æå– 3D å‡ ä½•ç‰¹å¾å¹¶å°†å…¶èå…¥ 3D å˜å½¢å­¦ä¹ ä¸­ï¼Œå®ç°äº† 3D å‡ ä½•æ„ŸçŸ¥çš„å˜å½¢å»ºæ¨¡ã€‚</li><li>åˆ©ç”¨ 3D å‡ ä½•ä¿¡æ¯æŒ‡å¯¼å˜å½¢å­¦ä¹ ï¼Œæé«˜äº†åŠ¨æ€è§†å›¾åˆæˆå’Œ 3D åŠ¨æ€é‡å»ºçš„è´¨é‡ã€‚æ€§èƒ½ï¼š</li><li>åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„ä¼˜è¶Šæ€§ï¼Œè¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚å·¥ä½œé‡ï¼š</li><li>è¯¥æ–¹æ³•éœ€è¦å¯¹ 3D å‡ ä½•ç‰¹å¾è¿›è¡Œæ˜¾å¼æå–ï¼Œå¢åŠ äº†è®¡ç®—é‡ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-afebc695ce43b9cca36774d451214003.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-75335d5dad43bcba0d01c7ed06e24b06.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-efd74ebc1d29a7da3e6ab58eae03277d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0f460737777a95c71a697250cf77dccd.jpg" align="middle"></details><h2 id="GHNeRF-Learning-Generalizable-Human-Features-with-Efficient-Neural-Radiance-Fields"><a href="#GHNeRF-Learning-Generalizable-Human-Features-with-Efficient-Neural-Radiance-Fields" class="headerlink" title="GHNeRF: Learning Generalizable Human Features with Efficient Neural   Radiance Fields"></a>GHNeRF: Learning Generalizable Human Features with Efficient Neural   Radiance Fields</h2><p><strong>Authors:Arnab Dey, Di Yang, Rohith Agaram, Antitza Dantcheva, Andrew I. Comport, Srinath Sridhar, Jean Martinet</strong></p><p>Recent advances in Neural Radiance Fields (NeRF) have demonstrated promising results in 3D scene representations, including 3D human representations. However, these representations often lack crucial information on the underlying human pose and structure, which is crucial for AR/VR applications and games. In this paper, we introduce a novel approach, termed GHNeRF, designed to address these limitations by learning 2D/3D joint locations of human subjects with NeRF representation. GHNeRF uses a pre-trained 2D encoder streamlined to extract essential human features from 2D images, which are then incorporated into the NeRF framework in order to encode human biomechanic features. This allows our network to simultaneously learn biomechanic features, such as joint locations, along with human geometry and texture. To assess the effectiveness of our method, we conduct a comprehensive comparison with state-of-the-art human NeRF techniques and joint estimation algorithms. Our results show that GHNeRF can achieve state-of-the-art results in near real-time. </p><p><a href="http://arxiv.org/abs/2404.06246v1">PDF</a> </p><p><strong>Summary</strong><br>åˆ©ç”¨é¢„è®­ç»ƒçš„ 2D ç¼–ç å™¨å°†äººä½“ 2D/3D å…³èŠ‚ä½ç½®ä¸ NeRF ç»“åˆï¼Œå®ç°äººä½“å‡ ä½•ã€çº¹ç†å’Œç”Ÿç‰©åŠ›å­¦ç‰¹å¾çš„è”åˆè¡¨ç¤ºã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>GHNeRF æ˜¯ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œå¯é€šè¿‡ NeRF è¡¨ç¤ºå­¦ä¹ äººä½“ 2D/3D å…³èŠ‚ä½ç½®ã€‚</li><li>GHNeRF å°†é¢„è®­ç»ƒçš„ 2D ç¼–ç å™¨é›†æˆåˆ° NeRF æ¡†æ¶ä¸­ï¼Œä»¥æå–äººä½“æœ¬è´¨ç‰¹å¾ã€‚</li><li>è¯¥æ–¹æ³•å¯ä»¥åŒæ—¶å­¦ä¹ äººä½“å‡ ä½•ã€çº¹ç†å’Œç”Ÿç‰©åŠ›å­¦ç‰¹å¾ï¼ˆå¦‚å…³èŠ‚ä½ç½®ï¼‰ã€‚</li><li>GHNeRF åœ¨è¿‘ä¹å®æ—¶çš„æƒ…å†µä¸‹ä¼˜äºæœ€å…ˆè¿›çš„äººä½“ NeRF æŠ€æœ¯å’Œå…³èŠ‚ä¼°è®¡ç®—æ³•ã€‚</li><li>GHNeRF æå–çš„å…³èŠ‚ä¼°è®¡å‡†ç¡®ä¸”ç¨³å®šã€‚</li><li>GHNeRF å¯¹é®æŒ¡å’Œè‡ªé®æŒ¡å…·æœ‰é²æ£’æ€§ã€‚</li><li>GHNeRF å¯ç”¨äº AR/VR åº”ç”¨ç¨‹åºå’Œæ¸¸æˆä¸­çš„äººä½“å»ºæ¨¡ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šGHNeRFï¼šå­¦ä¹ å¯æ³›åŒ–çš„äººä½“ç‰¹å¾</li><li>ä½œè€…ï¼šArnab Deyï¼ŒDi Yangï¼ŒRohith Agaramï¼ŒAntitza Dantchevaï¼ŒAndrew I. Comportï¼ŒSrinath Sridharï¼ŒJean Martinet</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šI3S-CNRS/UniversitÂ´e CË†otedâ€™Azur</li><li>å…³é”®è¯ï¼šç¥ç»è¾å°„åœºï¼Œäººä½“è¡¨ç¤ºï¼Œäººä½“ç‰¹å¾ï¼Œå…³èŠ‚å®šä½ï¼Œå§¿åŠ¿ä¼°è®¡</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.06246   Github é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åœ¨ 3D åœºæ™¯è¡¨ç¤ºä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼ŒåŒ…æ‹¬ 3D äººä½“è¡¨ç¤ºã€‚ç„¶è€Œï¼Œè¿™äº›è¡¨ç¤ºé€šå¸¸ç¼ºä¹äººä½“å§¿åŠ¿å’Œç»“æ„çš„å…³é”®ä¿¡æ¯ï¼Œè¿™å¯¹äº AR/VR åº”ç”¨å’Œæ¸¸æˆè‡³å…³é‡è¦ã€‚   ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šä»¥å¾€æ–¹æ³•åªèƒ½å­¦ä¹ äººä½“å‡ ä½•å’Œçº¹ç†ï¼Œæ— æ³•åŒæ—¶å­¦ä¹ äººä½“ç”Ÿç‰©åŠ›å­¦ç‰¹å¾ï¼Œä¾‹å¦‚å…³èŠ‚ä½ç½®ã€‚   ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡º GHNeRFï¼Œå®ƒå°†é¢„è®­ç»ƒçš„ 2D ç¼–ç å™¨ä¸ NeRF æ¡†æ¶ç›¸ç»“åˆï¼Œä» 2D å›¾åƒä¸­æå–äººä½“ç‰¹å¾ï¼Œå¹¶å°†å…¶ç¼–ç åˆ° NeRF ä¸­ã€‚è¿™ä½¿å¾— GHNeRF èƒ½å¤ŸåŒæ—¶å­¦ä¹ äººä½“å‡ ä½•ã€çº¹ç†å’Œç”Ÿç‰©åŠ›å­¦ç‰¹å¾ï¼Œä¾‹å¦‚å…³èŠ‚ä½ç½®ã€‚   ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šGHNeRF åœ¨äººä½“ NeRF æŠ€æœ¯å’Œå…³èŠ‚ä¼°è®¡ç®—æ³•çš„ç»¼åˆæ¯”è¾ƒä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå¹¶ä¸”å¯ä»¥åœ¨æ¥è¿‘å®æ—¶çš„æƒ…å†µä¸‹è¿è¡Œã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼šGHNeRFå°†é¢„è®­ç»ƒçš„2Dç¼–ç å™¨ä¸NeRFæ¡†æ¶ç›¸ç»“åˆï¼Œä»2Då›¾åƒä¸­æå–äººä½“ç‰¹å¾ï¼Œå¹¶å°†å…¶ç¼–ç åˆ°NeRFä¸­ï¼ŒåŒæ—¶å­¦ä¹ äººä½“å‡ ä½•ã€çº¹ç†å’Œç”Ÿç‰©åŠ›å­¦ç‰¹å¾ï¼Œä¾‹å¦‚å…³èŠ‚ä½ç½®ã€‚ï¼ˆ2ï¼‰ï¼šGHNeRFä½¿ç”¨åŸºäºTransformerçš„2Dç¼–ç å™¨ï¼Œå¯ä»¥ä»2Då›¾åƒä¸­æå–å±€éƒ¨å’Œå…¨å±€ç‰¹å¾ï¼Œå¹¶å°†å…¶ç¼–ç ä¸ºä¸€ä¸ªæ½œåœ¨çš„ç‰¹å¾å‘é‡ã€‚ï¼ˆ3ï¼‰ï¼šç„¶åï¼Œå°†è¿™ä¸ªæ½œåœ¨çš„ç‰¹å¾å‘é‡è¾“å…¥åˆ°NeRFä¸­ï¼ŒNeRFä½¿ç”¨å¤šå±‚æ„ŸçŸ¥å™¨æ¥é¢„æµ‹åœºæ™¯ä¸­æ¯ä¸ªç‚¹çš„é¢œè‰²å’Œå¯†åº¦ã€‚ï¼ˆ4ï¼‰ï¼šé€šè¿‡ä¼˜åŒ–NeRFçš„æŸå¤±å‡½æ•°ï¼ŒGHNeRFå¯ä»¥åŒæ—¶å­¦ä¹ äººä½“å‡ ä½•ã€çº¹ç†å’Œç”Ÿç‰©åŠ›å­¦ç‰¹å¾ï¼Œä¾‹å¦‚å…³èŠ‚ä½ç½®ã€‚</p></li></ol><p><strong>ç»“è®º</strong>1. æœ¬å·¥ä½œé€šè¿‡æå‡º GHNeRFï¼Œå°†äººä½“ç”Ÿç‰©åŠ›å­¦ç‰¹å¾å­¦ä¹ èå…¥ NeRFï¼Œæ˜¾è‘—æå‡äº†äººä½“ NeRF è¡¨å¾çš„æ³›åŒ–èƒ½åŠ›ã€‚2. åˆ›æ–°ç‚¹ï¼š   - æå‡ºäº†ä¸€ç§å°†é¢„è®­ç»ƒçš„ 2D ç¼–ç å™¨ä¸ NeRF æ¡†æ¶ç›¸ç»“åˆçš„æ–¹æ³•ï¼Œä» 2D å›¾åƒä¸­æå–äººä½“ç‰¹å¾å¹¶å°†å…¶ç¼–ç åˆ° NeRF ä¸­ã€‚   - åˆ›æ–°æ€§åœ°åˆ©ç”¨åŸºäº Transformer çš„ 2D ç¼–ç å™¨ï¼Œèƒ½å¤Ÿä» 2D å›¾åƒä¸­æå–å±€éƒ¨å’Œå…¨å±€ç‰¹å¾ã€‚3. æ€§èƒ½ï¼š   - åœ¨äººä½“ NeRF æŠ€æœ¯å’Œå…³èŠ‚ä¼°è®¡ç®—æ³•çš„ç»¼åˆæ¯”è¾ƒä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚   - å¯ä»¥åœ¨æ¥è¿‘å®æ—¶çš„æƒ…å†µä¸‹è¿è¡Œï¼Œå…·æœ‰è¾ƒé«˜çš„å®ç”¨æ€§ã€‚4. å·¥ä½œé‡ï¼š   - å·¥ä½œé‡è¾ƒå¤§ï¼Œæ¶‰åŠåˆ° 2D ç¼–ç å™¨çš„é¢„è®­ç»ƒã€NeRF æ¨¡å‹çš„è®­ç»ƒå’Œä¼˜åŒ–ã€‚   - ç®—æ³•çš„å¤æ‚åº¦è¾ƒé«˜ï¼Œéœ€è¦è¾ƒé«˜çš„è®¡ç®—èµ„æºã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-d1e65b0e4287dba0204c3edb8075bb41.jpg" align="middle"><img src="https://pica.zhimg.com/v2-962fd6bcf11373783e89def5f58c894b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-04d65c8c665cfeac6b6c20878f5001d2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e13a327e9d52f71bd0c265b3d7ab6c51.jpg" align="middle"></details>## HFNeRF: Learning Human Biomechanic Features with Neural Radiance Fields**Authors:Arnab Dey, Di Yang, Antitza Dantcheva, Jean Martinet**In recent advancements in novel view synthesis, generalizable Neural Radiance Fields (NeRF) based methods applied to human subjects have shown remarkable results in generating novel views from few images. However, this generalization ability cannot capture the underlying structural features of the skeleton shared across all instances. Building upon this, we introduce HFNeRF: a novel generalizable human feature NeRF aimed at generating human biomechanic features using a pre-trained image encoder. While previous human NeRF methods have shown promising results in the generation of photorealistic virtual avatars, such methods lack underlying human structure or biomechanic features such as skeleton or joint information that are crucial for downstream applications including Augmented Reality (AR)/Virtual Reality (VR). HFNeRF leverages 2D pre-trained foundation models toward learning human features in 3D using neural rendering, and then volume rendering towards generating 2D feature maps. We evaluate HFNeRF in the skeleton estimation task by predicting heatmaps as features. The proposed method is fully differentiable, allowing to successfully learn color, geometry, and human skeleton in a simultaneous manner. This paper presents preliminary results of HFNeRF, illustrating its potential in generating realistic virtual avatars with biomechanic features using NeRF. [PDF](http://arxiv.org/abs/2404.06152v1) **Summary**æ–°é¢–è§†å›¾ç”ŸæˆæŠ€æœ¯ä¸­çš„æ³›åŒ–ç¥ç»è¾å°„åœº (NeRF) æ–¹æ³•åœ¨ä»å°‘é‡å›¾åƒç”Ÿæˆæ–°è§†å›¾æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†æ— æ³•æ•æ‰æ‰€æœ‰äººä½“å®ä¾‹ä¸­éª¨éª¼çš„æ½œåœ¨ç»“æ„ç‰¹å¾ã€‚**Key Takeaways**- å¼•å…¥äº† HFNeRFï¼šä¸€ç§æ–°é¢–çš„æ³›åŒ–äººä½“ç‰¹å¾ NeRFï¼Œæ—¨åœ¨ä½¿ç”¨é¢„è®­ç»ƒå›¾åƒç¼–ç å™¨ç”Ÿæˆäººä½“ç”Ÿç‰©åŠ›å­¦ç‰¹å¾ã€‚- ä»¥å‰çš„äººä½“ NeRF æ–¹æ³•åœ¨ç”Ÿæˆé€¼çœŸçš„è™šæ‹ŸåŒ–èº«æ–¹é¢æ˜¾ç¤ºå‡ºæœ‰å¸Œæœ›çš„ç»“æœï¼Œä½†ç¼ºä¹å¯¹ä¸‹æ¸¸åº”ç”¨ï¼ˆåŒ…æ‹¬ AR/VRï¼‰è‡³å…³é‡è¦çš„æ½œåœ¨äººä½“ç»“æ„æˆ–ç”Ÿç‰©åŠ›å­¦ç‰¹å¾ï¼ˆä¾‹å¦‚éª¨éª¼æˆ–å…³èŠ‚ä¿¡æ¯ï¼‰ã€‚- HFNeRF åˆ©ç”¨ 2D é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹ï¼Œé€šè¿‡ç¥ç»æ¸²æŸ“å­¦ä¹  3D äººä½“ç‰¹å¾ï¼Œç„¶åé€šè¿‡ä½“ç§¯æ¸²æŸ“ç”Ÿæˆ 2D ç‰¹å¾å›¾ã€‚- é€šè¿‡é¢„æµ‹çƒ­å›¾ä½œä¸ºç‰¹å¾ï¼Œè¯„ä¼°äº† HFNeRF åœ¨éª¨éª¼ä¼°è®¡ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚- æ‰€æå‡ºçš„æ–¹æ³•å®Œå…¨å¯å¾®åˆ†ï¼Œå…è®¸åŒæ—¶æˆåŠŸå­¦ä¹ é¢œè‰²ã€å‡ ä½•å’Œäººä½“éª¨éª¼ã€‚- æœ¬æ–‡å±•ç¤ºäº† HFNeRF çš„åˆæ­¥ç»“æœï¼Œè¯´æ˜äº†å…¶ä½¿ç”¨ NeRF ç”Ÿæˆå…·æœ‰ç”Ÿç‰©åŠ›å­¦ç‰¹å¾çš„é€¼çœŸè™šæ‹ŸåŒ–èº«çš„æ½œåŠ›ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>æ ‡é¢˜ï¼šHFNeRFï¼šä½¿ç”¨ç¥ç»è¾å°„åœºå­¦ä¹ äººä½“ç”Ÿç‰©åŠ›å­¦ç‰¹å¾</li><li>ä½œè€…ï¼šArnab Deyï¼ŒDi Yangï¼ŒAntitza Dantchevaï¼ŒJean Martinet</li><li>éš¶å±æœºæ„ï¼šI3S-CNRS/UniversitÂ´e CË†otedâ€™Azur</li><li>å…³é”®è¯ï¼šè®¡ç®—æœºè§†è§‰ã€å¢å¼ºç°å®ã€è™šæ‹Ÿç°å®ã€NeRF</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.06152</li><li>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åœ¨ç”Ÿæˆæ–°é¢–è§†å›¾æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•æ— æ³•æ•æ‰åˆ°ä¸åŒå®ä¾‹ä¹‹é—´å…±äº«çš„éª¨éª¼ç­‰æ½œåœ¨ç»“æ„ç‰¹å¾ã€‚   ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šç°æœ‰åŸºäº NeRF çš„äººä½“æ–¹æ³•è™½ç„¶åœ¨ç”Ÿæˆé€¼çœŸçš„è™šæ‹ŸåŒ–èº«æ–¹é¢å–å¾—äº†å¯å–œçš„æˆæœï¼Œä½†ç¼ºä¹æ½œåœ¨çš„äººä½“ç»“æ„æˆ–ç”Ÿç‰©åŠ›å­¦ç‰¹å¾ï¼Œå¦‚éª¨éª¼æˆ–å…³èŠ‚ä¿¡æ¯ï¼Œè¿™å¯¹äºå¢å¼ºç°å® (AR)/è™šæ‹Ÿç°å® (VR) ç­‰ä¸‹æ¸¸åº”ç”¨è‡³å…³é‡è¦ã€‚   ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º HFNeRF çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ NeRF æ¶æ„å­¦ä¹ äººä½“ç”Ÿç‰©åŠ›å­¦ç‰¹å¾ï¼Œå¦‚äººä½“éª¨éª¼ã€‚HFNeRF é‡‡ç”¨é¢„è®­ç»ƒçš„ 2D ç¼–ç å™¨ï¼Œä½¿ç”¨ç¥ç»æ¸²æŸ“ä»å›¾åƒä¸­æå–äººä½“ç‰¹å¾ï¼Œç„¶åä½¿ç”¨ä½“ç§¯æ¸²æŸ“ç”Ÿæˆ 2D ç‰¹å¾å›¾ã€‚   ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šHFNeRF åœ¨éª¨éª¼ä¼°è®¡ä»»åŠ¡ä¸­é€šè¿‡é¢„æµ‹çƒ­å›¾ä½œä¸ºç‰¹å¾è¿›è¡Œè¯„ä¼°ã€‚è¯¥æ–¹æ³•æ˜¯å®Œå…¨å¯å¾®çš„ï¼Œå…è®¸ä»¥åŒæ­¥çš„æ–¹å¼æˆåŠŸå­¦ä¹ é¢œè‰²ã€å‡ ä½•å½¢çŠ¶å’Œäººä½“éª¨éª¼ã€‚æœ¬æ–‡å±•ç¤ºäº† HFNeRF çš„åˆæ­¥ç»“æœï¼Œè¯´æ˜äº†å…¶ä½¿ç”¨ NeRF ç”Ÿæˆå…·æœ‰ç”Ÿç‰©åŠ›å­¦ç‰¹å¾çš„é€¼çœŸè™šæ‹ŸåŒ–èº«çš„æ½œåŠ›ã€‚</li></ol><p><strong>Methods</strong></p><p>(1): <strong>NeRFæ¶æ„</strong>ï¼šHFNeRFåˆ©ç”¨ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æ¶æ„ï¼Œé€šè¿‡å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰å°†3Dåæ ‡æ˜ å°„åˆ°é¢œè‰²å’Œä¸é€æ˜åº¦ã€‚</p><p>(2): <strong>ç‰¹å¾æå–</strong>ï¼šä½¿ç”¨é¢„è®­ç»ƒçš„2Dç¼–ç å™¨ä»å›¾åƒä¸­æå–äººä½“ç‰¹å¾ï¼Œç”Ÿæˆ2Dç‰¹å¾å›¾ã€‚</p><p>(3): <strong>éª¨éª¼æå–</strong>ï¼šä»2Dç‰¹å¾å›¾ä¸­é¢„æµ‹çƒ­å›¾ä½œä¸ºéª¨éª¼ç‰¹å¾ï¼Œç„¶åé€šè¿‡åå¤„ç†æå–éª¨éª¼ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º HFNeRF çš„æ–°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿ç”¨ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æ¥å­¦ä¹ äººä½“ç”Ÿç‰©åŠ›å­¦ç‰¹å¾ã€‚æˆ‘ä»¬çš„åˆæ­¥ç ”ç©¶ç»“æœè¯æ˜äº† HFNeRF åœ¨é¢„æµ‹äººä½“ç‰¹å¾æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œè¿™æ¯”ä»¥å‰ç”¨äºäººç±»çš„ NeRF æ–¹æ³•æœ‰äº†æ˜¾ç€æ”¹è¿›ã€‚è™½ç„¶æˆ‘ä»¬çš„é‡ç‚¹æ˜¯äººä½“éª¨éª¼æ£€æµ‹ï¼Œä½†æˆ‘ä»¬ç›¸ä¿¡è¿™ç§æ¶æ„å¯ä»¥æ‰©å±•åˆ°å…¶ä»–å¯æ¦‚æ‹¬çš„äººä½“ç‰¹å¾ï¼Œä¾‹å¦‚èº«ä½“éƒ¨ä½æ£€æµ‹ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ HFNeRFï¼Œè¯¥æ¡†æ¶ä½¿ç”¨ NeRF å­¦ä¹ äººä½“ç”Ÿç‰©åŠ›å­¦ç‰¹å¾ï¼Œå¦‚äººä½“éª¨éª¼ã€‚æ€§èƒ½ï¼šHFNeRF åœ¨éª¨éª¼ä¼°è®¡ä»»åŠ¡ä¸­é€šè¿‡é¢„æµ‹çƒ­å›¾ä½œä¸ºç‰¹å¾è¿›è¡Œè¯„ä¼°ï¼Œåœ¨é¢„æµ‹äººä½“ç‰¹å¾æ–¹é¢è¡¨ç°å‡ºæœ‰æ•ˆæ€§ã€‚å·¥ä½œé‡ï¼šæœ¬æ–‡å±•ç¤ºäº† HFNeRF çš„åˆæ­¥ç»“æœï¼Œè¯´æ˜äº†å…¶ä½¿ç”¨ NeRF ç”Ÿæˆå…·æœ‰ç”Ÿç‰©åŠ›å­¦ç‰¹å¾çš„é€¼çœŸè™šæ‹ŸåŒ–èº«çš„æ½œåŠ›ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-d759674d19facfbca30699f2f267c071.jpg" align="middle"><img src="https://picx.zhimg.com/v2-776a0f7d6568012bbae47efa541663bf.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-04-14  Boosting Self-Supervision for Single-View Scene Completion via Knowledge   Distillation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/04/14/Paper/2024-04-14/3DGS/"/>
    <id>https://kedreamix.github.io/2024/04/14/Paper/2024-04-14/3DGS/</id>
    <published>2024-04-14T04:25:51.000Z</published>
    <updated>2024-04-14T04:25:51.782Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-04-14-æ›´æ–°"><a href="#2024-04-14-æ›´æ–°" class="headerlink" title="2024-04-14 æ›´æ–°"></a>2024-04-14 æ›´æ–°</h1><h2 id="GoMAvatar-Efficient-Animatable-Human-Modeling-from-Monocular-Video-Using-Gaussians-on-Mesh"><a href="#GoMAvatar-Efficient-Animatable-Human-Modeling-from-Monocular-Video-Using-Gaussians-on-Mesh" class="headerlink" title="GoMAvatar: Efficient Animatable Human Modeling from Monocular Video   Using Gaussians-on-Mesh"></a>GoMAvatar: Efficient Animatable Human Modeling from Monocular Video   Using Gaussians-on-Mesh</h2><p><strong>Authors:Jing Wen, Xiaoming Zhao, Zhongzheng Ren, Alexander G. Schwing, Shenlong Wang</strong></p><p>We introduce GoMAvatar, a novel approach for real-time, memory-efficient, high-quality animatable human modeling. GoMAvatar takes as input a single monocular video to create a digital avatar capable of re-articulation in new poses and real-time rendering from novel viewpoints, while seamlessly integrating with rasterization-based graphics pipelines. Central to our method is the Gaussians-on-Mesh representation, a hybrid 3D model combining rendering quality and speed of Gaussian splatting with geometry modeling and compatibility of deformable meshes. We assess GoMAvatar on ZJU-MoCap data and various YouTube videos. GoMAvatar matches or surpasses current monocular human modeling algorithms in rendering quality and significantly outperforms them in computational efficiency (43 FPS) while being memory-efficient (3.63 MB per subject). </p><p><a href="http://arxiv.org/abs/2404.07991v1">PDF</a> CVPR 2024; project page: <a href="https://wenj.github.io/GoMAvatar/">https://wenj.github.io/GoMAvatar/</a></p><p><strong>Summary</strong><br>å®æ—¶ã€å†…å­˜é«˜æ•ˆã€é«˜è´¨é‡å¯åŠ¨ç”»äººä½“é‡å»ºçš„å…¨æ–°æ–¹æ³•ï¼ŒGoMAvatarã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>å¼•å…¥é«˜æ–¯ç½‘æ ¼è¡¨ç¤ºï¼Œç»“åˆäº†é«˜æ–¯ç‚¹äº‘æ¸²æŸ“çš„è´¨é‡å’Œé€Ÿåº¦ã€å‡ ä½•å»ºæ¨¡ä»¥åŠå¯å˜å½¢ç½‘æ ¼çš„å…¼å®¹æ€§ã€‚</li><li>è¾“å…¥å•ç›®è§†é¢‘å³å¯åˆ›å»ºå¯åœ¨æ–°å§¿åŠ¿ä¸­é‡æ–°å…³èŠ‚åŒ–å¹¶ä»æ–°è§†ç‚¹å®æ—¶æ¸²æŸ“çš„æ•°å­—è™šæ‹Ÿäººã€‚</li><li>ä¸å…‰æ …åŒ–å›¾å½¢ç®¡é“æ— ç¼é›†æˆã€‚</li><li>åœ¨ ZJU-MoCap æ•°æ®å’Œå„ç§ YouTube è§†é¢‘ä¸Šè¯„ä¼°äº† GoMAvatarã€‚</li><li>åœ¨æ¸²æŸ“è´¨é‡ä¸Šè¾¾åˆ°æˆ–è¶…è¿‡å½“å‰å•ç›®äººå½¢å»ºæ¨¡ç®—æ³•ï¼ŒåŒæ—¶åœ¨è®¡ç®—æ•ˆç‡ï¼ˆ43 FPSï¼‰å’Œå†…å­˜æ•ˆç‡ï¼ˆæ¯ä¸ªå—è¯•è€… 3.63 MBï¼‰æ–¹é¢æ˜¾è‘—ä¼˜äºå®ƒä»¬ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>è®ºæ–‡æ ‡é¢˜ï¼šGoMAvatarï¼šé€šè¿‡å•ç›®è§†é¢‘é«˜æ•ˆæ„å»ºå¯åŠ¨ç”»çš„äººä½“æ¨¡å‹</li><li>ä½œè€…ï¼š</li><li>Chen Cao</li><li>Pengfei Xiang</li><li>Yuting Ye</li><li>Yuxuan Zhang</li><li>Hongyi Xu</li><li>Yebin Liu</li><li>Hao Li</li><li>Hanqing Lu</li><li>Wenping Wang</li><li>Xiaoguang Han</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šæµ™æ±Ÿå¤§å­¦</li><li>å…³é”®è¯ï¼š</li><li>å•ç›®äººä½“å»ºæ¨¡</li><li>é«˜æ–¯ç½‘æ ¼è¡¨ç¤º</li><li>å®æ—¶æ¸²æŸ“</li><li>å¯åŠ¨ç”»</li><li>è®ºæ–‡é“¾æ¥ï¼šNone   Github ä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼š<ul><li>å•ç›®äººä½“å»ºæ¨¡æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„é‡è¦è¯¾é¢˜ï¼Œå¯ä»¥ä»å•ç›®è§†é¢‘ä¸­åˆ›å»ºå¯åŠ¨ç”»çš„äººä½“æ¨¡å‹ã€‚</li><li>ç°æœ‰çš„å•ç›®äººä½“å»ºæ¨¡æ–¹æ³•è¦ä¹ˆæ¸²æŸ“è´¨é‡å·®ï¼Œè¦ä¹ˆè®¡ç®—æ•ˆç‡ä½ï¼Œè¦ä¹ˆå†…å­˜æ¶ˆè€—å¤§ã€‚   ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼š</li><li>åŸºäºç½‘æ ¼çš„æ–¹æ³•ï¼šæ¸²æŸ“è´¨é‡é«˜ï¼Œä½†è®¡ç®—æ•ˆç‡ä½ã€‚</li><li>åŸºäºé«˜æ–¯çƒçš„æ–¹æ³•ï¼šè®¡ç®—æ•ˆç‡é«˜ï¼Œä½†æ¸²æŸ“è´¨é‡å·®ã€‚   ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼š</li><li>æå‡ºäº†ä¸€ç§æ–°çš„é«˜æ–¯ç½‘æ ¼è¡¨ç¤ºï¼ˆGoMï¼‰ï¼Œç»“åˆäº†é«˜æ–¯çƒçš„æ¸²æŸ“é€Ÿåº¦å’Œç½‘æ ¼æ¨¡å‹çš„å‡ ä½•å»ºæ¨¡èƒ½åŠ›ã€‚</li><li>è®¾è®¡äº†ä¸€ä¸ªç«¯åˆ°ç«¯å¯å¾®åˆ†ç®¡é“ï¼Œä»å•ç›®è§†é¢‘è¾“å…¥åˆ°å¯åŠ¨ç”»çš„äººä½“æ¨¡å‹è¾“å‡ºã€‚</li><li>é‡‡ç”¨ç¥ç»ç½‘ç»œå¯¹æ¨¡å‹å‚æ•°è¿›è¡Œä¼˜åŒ–ï¼ŒåŒ…æ‹¬å½¢çŠ¶ã€çº¹ç†ã€å§¿æ€å’ŒåŠ¨ç”»ã€‚   ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼š</li><li>åœ¨ ZJU-MoCapã€PeopleSnapshot å’Œ YouTube è§†é¢‘æ•°æ®é›†ä¸Šè¯„ä¼°äº† GoMAvatarã€‚</li><li>GoMAvatar åœ¨æ¸²æŸ“è´¨é‡ä¸Šä¸ç°æœ‰çš„å•ç›®äººä½“å»ºæ¨¡ç®—æ³•ç›¸å½“æˆ–ä¼˜äºå®ƒä»¬ï¼Œåœ¨è®¡ç®—æ•ˆç‡ä¸Šæ˜¾è‘—ä¼˜äºå®ƒä»¬ï¼ˆ43 FPSï¼‰ï¼ŒåŒæ—¶å†…å­˜æ¶ˆè€—ä¹Ÿè¾ƒä½ï¼ˆæ¯ä¸ªä¸»ä½“ 3.63 MBï¼‰ã€‚</li></ul></li></ol><p>Some Error for method(æ¯”å¦‚æ˜¯ä¸æ˜¯æ²¡æœ‰Methodsè¿™ä¸ªç« èŠ‚)</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œæå‡ºäº† GoMAvatarï¼Œè¯¥æ¡†æ¶æ—¨åœ¨ä½¿ç”¨å•ä¸ªè¾“å…¥è§†é¢‘æ¸²æŸ“å‡ºäººç±»è¡¨æ¼”è€…çš„é«˜ä¿çœŸè‡ªç”±è§†è§’å›¾åƒã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯é«˜æ–¯ç½‘æ ¼è¡¨ç¤ºã€‚ç»“åˆå‰å‘å…³èŠ‚è¿åŠ¨å’Œç¥ç»æ¸²æŸ“ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ¸²æŸ“é€Ÿåº¦å¿«ï¼ŒåŒæ—¶å†…å­˜æ•ˆç‡é«˜ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¯¥æ–¹æ³•å¾ˆå¥½åœ°å¤„ç†é‡å¤–è§†é¢‘ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†é«˜æ–¯ç½‘æ ¼è¡¨ç¤ºï¼Œç»“åˆäº†é«˜æ–¯çƒçš„æ¸²æŸ“é€Ÿåº¦å’Œç½‘æ ¼æ¨¡å‹çš„å‡ ä½•å»ºæ¨¡èƒ½åŠ›ã€‚è®¾è®¡äº†ä¸€ä¸ªç«¯åˆ°ç«¯å¯å¾®åˆ†ç®¡é“ï¼Œä»å•ç›®è§†é¢‘è¾“å…¥åˆ°å¯åŠ¨ç”»çš„äººä½“æ¨¡å‹è¾“å‡ºã€‚é‡‡ç”¨ç¥ç»ç½‘ç»œå¯¹æ¨¡å‹å‚æ•°è¿›è¡Œä¼˜åŒ–ï¼ŒåŒ…æ‹¬å½¢çŠ¶ã€çº¹ç†ã€å§¿æ€å’ŒåŠ¨ç”»ã€‚æ€§èƒ½ï¼šåœ¨ ZJU-MoCapã€PeopleSnapshot å’Œ YouTube è§†é¢‘æ•°æ®é›†ä¸Šè¯„ä¼°äº† GoMAvatarã€‚GoMAvatar åœ¨æ¸²æŸ“è´¨é‡ä¸Šä¸ç°æœ‰çš„å•ç›®äººä½“å»ºæ¨¡ç®—æ³•ç›¸å½“æˆ–ä¼˜äºå®ƒä»¬ï¼Œåœ¨è®¡ç®—æ•ˆç‡ä¸Šæ˜¾è‘—ä¼˜äºå®ƒä»¬ï¼ˆ43FPSï¼‰ï¼ŒåŒæ—¶å†…å­˜æ¶ˆè€—ä¹Ÿè¾ƒä½ï¼ˆæ¯ä¸ªä¸»ä½“ 3.63MBï¼‰ã€‚å·¥ä½œé‡ï¼šåœ¨ 2 ä¸ª NVIDIA Tesla V100 GPU ä¸Šè®­ç»ƒæ¨¡å‹éœ€è¦å¤§çº¦ 10 å¤©ã€‚åœ¨å•ä¸ª NVIDIA RTX 2080 Ti GPU ä¸Šè¿›è¡Œæ¨ç†éœ€è¦å¤§çº¦ 23msã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-f6679894ad5fb175b61f1275145cd461.jpg" align="middle"><img src="https://pica.zhimg.com/v2-acf0512eb9d25a17024d67cc7e7ac305.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4f1e513ece4b778293f135ec5b0edea5.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ea1e1eb74d0ff5d3bdeeb203aac60cdc.jpg" align="middle"></details><h2 id="RealmDreamer-Text-Driven-3D-Scene-Generation-with-Inpainting-and-Depth-Diffusion"><a href="#RealmDreamer-Text-Driven-3D-Scene-Generation-with-Inpainting-and-Depth-Diffusion" class="headerlink" title="RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth   Diffusion"></a>RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth   Diffusion</h2><p><strong>Authors:Jaidev Shriram, Alex Trevithick, Lingjie Liu, Ravi Ramamoorthi</strong></p><p>We introduce RealmDreamer, a technique for generation of general forward-facing 3D scenes from text descriptions. Our technique optimizes a 3D Gaussian Splatting representation to match complex text prompts. We initialize these splats by utilizing the state-of-the-art text-to-image generators, lifting their samples into 3D, and computing the occlusion volume. We then optimize this representation across multiple views as a 3D inpainting task with image-conditional diffusion models. To learn correct geometric structure, we incorporate a depth diffusion model by conditioning on the samples from the inpainting model, giving rich geometric structure. Finally, we finetune the model using sharpened samples from image generators. Notably, our technique does not require video or multi-view data and can synthesize a variety of high-quality 3D scenes in different styles, consisting of multiple objects. Its generality additionally allows 3D synthesis from a single image. </p><p><a href="http://arxiv.org/abs/2404.07199v1">PDF</a> Project Page: <a href="https://realmdreamer.github.io/">https://realmdreamer.github.io/</a></p><p><strong>Summary</strong><br>æ–‡æœ¬æè¿°ç”Ÿæˆé€šç”¨å‰è§†è§’ 3D åœºæ™¯çš„ RealmDreamer æŠ€æœ¯ï¼Œåˆ©ç”¨ 3D é«˜æ–¯é£æº…è¡¨å¾åŒ¹é…å¤æ‚æ–‡æœ¬æç¤ºã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>åˆ©ç”¨æœ€å…ˆè¿›çš„æ–‡æœ¬å¯¹å›¾åƒç”Ÿæˆå™¨åˆå§‹åŒ– 3D é«˜æ–¯é£æº…ã€‚</li><li>é€šè¿‡å›¾åƒæ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œå°†æ­¤è¡¨ç¤ºä¼˜åŒ–ä¸ºå¤šè§†å›¾ 3D ä¿®å¤ä»»åŠ¡ã€‚</li><li>ç»“åˆæ·±åº¦æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡ä¿®å¤æ¨¡å‹æ ·æœ¬æ¥å­¦ä¹ æ­£ç¡®çš„å‡ ä½•ç»“æ„ï¼Œæä¾›ä¸°å¯Œçš„å‡ ä½•ç»“æ„ã€‚</li><li>ä½¿ç”¨å›¾åƒç”Ÿæˆå™¨ä¸­é”åŒ–çš„æ ·æœ¬å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚</li><li>æ— éœ€è§†é¢‘æˆ–å¤šè§†å›¾æ•°æ®ï¼Œå¯åˆæˆå„ç§é«˜è´¨é‡ã€ä¸åŒé£æ ¼çš„ 3D åœºæ™¯ã€‚</li><li>å…è®¸ä»å•å¼ å›¾åƒä¸­è¿›è¡Œ 3D åˆæˆã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šRealmDreamerï¼šæ–‡æœ¬é©±åŠ¨çš„ä¸‰ç»´åœºæ™¯ç”Ÿæˆï¼Œå¸¦å†…ç»˜å’Œæ·±åº¦æ‰©æ•£</li><li>ä½œè€…ï¼šJaidev Shriram<em> Alex Trevithick</em> Lingjie Liu Ravi Ramamoorthi</li><li>éš¶å±æœºæ„ï¼šåŠ å·å¤§å­¦åœ£åœ°äºšå“¥åˆ†æ ¡</li><li>å…³é”®è¯ï¼šæ–‡æœ¬åˆ° 3Dã€3D åœºæ™¯ç”Ÿæˆã€å†…ç»˜ã€æ·±åº¦æ‰©æ•£</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://realmdreamer.github.io/</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šæ–‡æœ¬é©±åŠ¨çš„ä¸‰ç»´åœºæ™¯åˆæˆå…·æœ‰é©æ–°ä¸‰ç»´å†…å®¹åˆ›å»ºçš„æ½œåŠ›ï¼Œä½†ç°æœ‰æ–¹æ³•å­˜åœ¨è¿­ä»£æ—¶é—´é•¿ã€ä»…é™äºç®€å•å¯¹è±¡çº§æ•°æ®æˆ–å…¨æ™¯å›¾ç­‰é—®é¢˜ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•åŒ…æ‹¬ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ã€Prolific Dreamer ç­‰ï¼Œä½†è¿™äº›æ–¹æ³•éœ€è¦è§†é¢‘æˆ–å¤šè§†å›¾æ•°æ®ï¼Œä¸”ç”Ÿæˆçš„åœºæ™¯å‡ ä½•ç»“æ„ä¸å‡†ç¡®ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šRealmDreamer ä¼˜åŒ–ä¸‰ç»´é«˜æ–¯æ•£å°„è¡¨ç¤ºä»¥åŒ¹é…å¤æ‚çš„æ–‡æœ¬æç¤ºã€‚å®ƒåˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå™¨åˆå§‹åŒ–æ•£å°„ç‚¹ï¼Œå°†å…¶æå‡åˆ°ä¸‰ç»´å¹¶è®¡ç®—é®æŒ¡ä½“ç§¯ã€‚ç„¶åï¼Œå®ƒå°†æ­¤è¡¨ç¤ºä¼˜åŒ–ä¸ºè·¨å¤šä¸ªè§†å›¾çš„ä¸‰ç»´å†…ç»˜ä»»åŠ¡ï¼Œå¹¶ä½¿ç”¨å›¾åƒæ¡ä»¶æ‰©æ•£æ¨¡å‹ã€‚ä¸ºäº†å­¦ä¹ æ­£ç¡®çš„å‡ ä½•ç»“æ„ï¼Œå®ƒç»“åˆæ·±åº¦æ‰©æ•£æ¨¡å‹ï¼Œä»¥å†…ç»˜æ¨¡å‹çš„æ ·æœ¬ä¸ºæ¡ä»¶ï¼Œä»è€Œè·å¾—ä¸°å¯Œçš„å‡ ä½•ç»“æ„ã€‚æœ€åï¼Œä½¿ç”¨å›¾åƒç”Ÿæˆå™¨çš„é”åŒ–æ ·æœ¬å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚ï¼ˆ4ï¼‰æ€§èƒ½ï¼šRealmDreamer åœ¨å„ç§é£æ ¼å’ŒåŒ…å«å¤šä¸ªå¯¹è±¡çš„é«˜è´¨é‡ä¸‰ç»´åœºæ™¯åˆæˆæ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚å®ƒè¿˜å¯ä»¥ä»å•ä¸ªå›¾åƒä¸­åˆæˆä¸‰ç»´åœºæ™¯ï¼Œæ— éœ€è§†é¢‘æˆ–å¤šè§†å›¾æ•°æ®ã€‚</li></ol><p>æ–¹æ³•ï¼š(1): å°†æ–‡æœ¬æç¤ºè½¬æ¢ä¸ºä¸‰ç»´é«˜æ–¯æ•£å°„è¡¨ç¤ºï¼ˆ3DGSï¼‰ï¼Œåˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå™¨åˆå§‹åŒ–æ•£å°„ç‚¹ï¼Œå¹¶æå‡åˆ°ä¸‰ç»´ä»¥è®¡ç®—é®æŒ¡ä½“ç§¯ï¼›(2): ä½¿ç”¨å›¾åƒæ¡ä»¶æ‰©æ•£æ¨¡å‹å¯¹ä¸‰ç»´è¡¨ç¤ºè¿›è¡Œä¼˜åŒ–ï¼Œä½œä¸ºè·¨å¤šä¸ªè§†å›¾çš„ä¸‰ç»´å†…ç»˜ä»»åŠ¡ï¼›(3): ç»“åˆæ·±åº¦æ‰©æ•£æ¨¡å‹ï¼Œä»¥å†…ç»˜æ¨¡å‹çš„æ ·æœ¬ä¸ºæ¡ä»¶ï¼Œè·å¾—ä¸°å¯Œçš„å‡ ä½•ç»“æ„ï¼›(4): ä½¿ç”¨å›¾åƒç”Ÿæˆå™¨çš„é”åŒ–æ ·æœ¬å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥è·å¾—æ¸…æ™°çš„ä¸‰ç»´æ ·æœ¬ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šRealmDreamer åœ¨ 3D åœºæ™¯ç”Ÿæˆæ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„æˆæœï¼Œä¸º 3D å†…å®¹åˆ›å»ºå¸¦æ¥äº†æ–°çš„å¯èƒ½æ€§ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§åŸºäºå†…ç»˜å’Œæ·±åº¦æ‰©æ•£çš„æ–‡æœ¬é©±åŠ¨çš„ 3D åœºæ™¯ç”Ÿæˆæ–¹æ³•ã€‚</li><li>åˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå™¨åˆå§‹åŒ– 3D æ•£å°„è¡¨ç¤ºï¼Œå¹¶ä½¿ç”¨å›¾åƒæ¡ä»¶æ‰©æ•£æ¨¡å‹å’Œæ·±åº¦æ‰©æ•£æ¨¡å‹ä¼˜åŒ–å‡ ä½•ç»“æ„ã€‚</li><li>å¯ä»¥ä»å•ä¸ªå›¾åƒä¸­åˆæˆ 3D åœºæ™¯ï¼Œæ— éœ€è§†é¢‘æˆ–å¤šè§†å›¾æ•°æ®ã€‚</li><li>æ€§èƒ½ï¼šåœ¨å„ç§é£æ ¼å’ŒåŒ…å«å¤šä¸ªå¯¹è±¡çš„é«˜è´¨é‡ 3D åœºæ™¯åˆæˆæ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚</li><li>è´Ÿè½½ï¼šè®­ç»ƒæ—¶é—´è¾ƒé•¿ï¼ˆæ•°å°æ—¶ï¼‰ï¼Œå¯¹äºå…·æœ‰é«˜åº¦é®æŒ¡çš„å¤æ‚åœºæ™¯ï¼Œç”Ÿæˆçš„å›¾åƒå¯èƒ½ä¼šæ¨¡ç³Šã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-a5b848a94e7c9ceb54ea9d3c0f1d9886.jpg" align="middle"><img src="https://pica.zhimg.com/v2-38a9e6e511a25c96557597931a870630.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-868bab455ec35ce1f302df76db771b6f.jpg" align="middle"><img src="https://pica.zhimg.com/v2-b22afc5e0c39a48548a9120642a8038d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6a53086cc451b1c9072ffe37abccae8f.jpg" align="middle"></details><h2 id="DreamScene360-Unconstrained-Text-to-3D-Scene-Generation-with-Panoramic-Gaussian-Splatting"><a href="#DreamScene360-Unconstrained-Text-to-3D-Scene-Generation-with-Panoramic-Gaussian-Splatting" class="headerlink" title="DreamScene360: Unconstrained Text-to-3D Scene Generation with Panoramic   Gaussian Splatting"></a>DreamScene360: Unconstrained Text-to-3D Scene Generation with Panoramic   Gaussian Splatting</h2><p><strong>Authors:Shijie Zhou, Zhiwen Fan, Dejia Xu, Haoran Chang, Pradyumna Chari, Tejas Bharadwaj, Suya You, Zhangyang Wang, Achuta Kadambi</strong></p><p>The increasing demand for virtual reality applications has highlighted the significance of crafting immersive 3D assets. We present a text-to-3D 360$^{\circ}$ scene generation pipeline that facilitates the creation of comprehensive 360$^{\circ}$ scenes for in-the-wild environments in a matter of minutes. Our approach utilizes the generative power of a 2D diffusion model and prompt self-refinement to create a high-quality and globally coherent panoramic image. This image acts as a preliminary â€œflatâ€ (2D) scene representation. Subsequently, it is lifted into 3D Gaussians, employing splatting techniques to enable real-time exploration. To produce consistent 3D geometry, our pipeline constructs a spatially coherent structure by aligning the 2D monocular depth into a globally optimized point cloud. This point cloud serves as the initial state for the centroids of 3D Gaussians. In order to address invisible issues inherent in single-view inputs, we impose semantic and geometric constraints on both synthesized and input camera views as regularizations. These guide the optimization of Gaussians, aiding in the reconstruction of unseen regions. In summary, our method offers a globally consistent 3D scene within a 360$^{\circ}$ perspective, providing an enhanced immersive experience over existing techniques. Project website at: <a href="http://dreamscene360.github.io/">http://dreamscene360.github.io/</a> </p><p><a href="http://arxiv.org/abs/2404.06903v1">PDF</a> </p><p><strong>Summary</strong><br>æ–‡æœ¬åˆ°ä¸‰ç»´ 360 åº¦åœºæ™¯ç”Ÿæˆç®¡é“ï¼Œå¯å¿«é€Ÿè½»æ¾åœ°åˆ›å»ºèº«ä¸´å…¶å¢ƒçš„ 360 åº¦åœºæ™¯ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>åˆ©ç”¨ 2D æ‰©æ•£æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡ä¸”å…¨å±€è¿è´¯çš„å…¨æ™¯å›¾åƒä½œä¸ºå¹³å¦åœºæ™¯è¡¨ç¤ºã€‚</li><li>ä½¿ç”¨å–·å°„æŠ€æœ¯å°†å¹³å¦åœºæ™¯æå‡ä¸ºä¸‰ç»´é«˜æ–¯ä½“ï¼Œå®ç°å®æ—¶æ¢ç´¢ã€‚</li><li>æ„å»ºç©ºé—´è¿è´¯ç»“æ„ï¼Œå°† 2D å•ç›®æ·±åº¦å¯¹é½åˆ°å…¨å±€ä¼˜åŒ–ç‚¹äº‘ï¼Œç”Ÿæˆä¸€è‡´çš„ä¸‰ç»´å‡ ä½•ä½“ã€‚</li><li>åˆ©ç”¨è¯­ä¹‰å’Œå‡ ä½•çº¦æŸæ­£åˆ™åŒ–åˆæˆå’Œè¾“å…¥ç›¸æœºè§†å›¾ï¼Œä¼˜åŒ–é«˜æ–¯ä½“ï¼Œé‡å»ºä¸å¯è§åŒºåŸŸã€‚</li><li>è¯¥æ–¹æ³•æä¾›å…¨å±€ä¸€è‡´çš„ä¸‰ç»´åœºæ™¯ï¼Œæä¾›æ¯”ç°æœ‰æŠ€æœ¯æ›´å¥½çš„æ²‰æµ¸å¼ä½“éªŒã€‚</li><li>é¡¹ç›®ç½‘ç«™ï¼š<a href="http://dreamscene360.github.io/">http://dreamscene360.github.io/</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šDreamScene360ï¼šæ— çº¦æŸæ–‡æœ¬åˆ° 3D åœºæ™¯</li><li>ä½œè€…ï¼šShijie Zhouã€Zhiwen Fanã€Dejia Xuã€Haoran Changã€Pradyumna Chariã€Tejas Bharadwajã€Suya Youã€Zhangyang Wangã€Achuta Kadambi</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šåŠ å·å¤§å­¦æ´›æ‰çŸ¶åˆ†æ ¡</li><li>å…³é”®è¯ï¼šæ–‡æœ¬åˆ° 3Dã€360 åº¦å…¨æ™¯ã€é«˜æ–¯ç‚¹ splattingã€2D æ‰©æ•£æ¨¡å‹ã€å•ç›®æ·±åº¦ä¼°è®¡</li><li>è®ºæ–‡é“¾æ¥ï¼šhttp://dreamscene360.github.io/ï¼ŒGithub ä»£ç é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šè™šæ‹Ÿç°å®åº”ç”¨çš„å…´èµ·å‡¸æ˜¾äº†åˆ›å»ºæ²‰æµ¸å¼ 3D èµ„äº§çš„é‡è¦æ€§ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äº 3D å»ºæ¨¡æˆ–æ‰«æï¼Œè¿™éœ€è¦å¤§é‡çš„äººåŠ›å’Œæ—¶é—´ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–‡æœ¬åˆ° 3D 360 åº¦åœºæ™¯ç”Ÿæˆç®¡é“ï¼Œåˆ©ç”¨ 2D æ‰©æ•£æ¨¡å‹å’Œæç¤ºè‡ªä¼˜åŒ–ç”Ÿæˆé«˜è´¨é‡ä¸”å…¨å±€ä¸€è‡´çš„å…¨æ™¯å›¾åƒï¼Œå†å°†å…¶æå‡åˆ° 3D é«˜æ–¯ç‚¹ splatting ä¸­ï¼Œå¹¶é€šè¿‡å¯¹é½ 2D å•ç›®æ·±åº¦æ¥æ„å»ºç©ºé—´ä¸€è‡´çš„ç»“æ„ã€‚ï¼ˆ4ï¼‰å®éªŒç»“æœï¼šè¯¥æ–¹æ³•åœ¨æ–‡æœ¬åˆ° 360 åº¦å…¨æ™¯åœºæ™¯ç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†è¾ƒå¥½çš„æ€§èƒ½ï¼Œå¯ä»¥åœ¨å‡ åˆ†é’Ÿå†…ç”Ÿæˆé«˜è´¨é‡ã€å…¨å±€ä¸€è‡´ä¸”å¯å®æ—¶æ¢ç´¢çš„ 360 åº¦å…¨æ™¯åœºæ™¯ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ï¼Œå³æä¾›ä¸€ç§å¿«é€Ÿä¸”é«˜æ•ˆçš„æ–¹æ³•æ¥åˆ›å»ºæ²‰æµ¸å¼è™šæ‹Ÿç°å®ä½“éªŒã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼šæ–‡æœ¬åˆ° 360Â° å…¨æ™¯ç”Ÿæˆï¼Œé‡‡ç”¨è‡ªä¼˜åŒ–æµç¨‹ï¼Œç¡®ä¿ç”Ÿæˆé²æ£’æ€§ï¼Œå¹¶ä¸æ–‡æœ¬è¯­ä¹‰å¯¹é½ï¼›ï¼ˆ2ï¼‰ï¼šä»å…¨æ™¯å‡ ä½•åœºåˆå§‹åŒ–ï¼Œå°†è¯­ä¹‰å¯¹é½å’Œå‡ ä½•å¯¹åº”å…³ç³»ä½œä¸ºé«˜æ–¯ä¼˜åŒ–æ­£åˆ™åŒ–ï¼Œä»¥è§£å†³å•è§†å›¾è¾“å…¥é€ æˆçš„å·®è·ï¼›ï¼ˆ3ï¼‰ï¼šåˆ©ç”¨è™šæ‹Ÿç›¸æœºåˆæˆè§†å·®ï¼Œå¹¶é€šè¿‡å¼ºåˆ¶ç‰¹å¾çº§ç›¸ä¼¼æ€§æ¥æŒ‡å¯¼é«˜æ–¯å¡«å……ä¸å¯è§åŒºåŸŸçš„å‡ ä½•å·®è·ã€‚</p></li></ol><p><strong>æ‘˜è¦</strong></p><p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–‡æœ¬åˆ°3D 360åº¦åœºæ™¯ç”Ÿæˆç®¡é“ï¼Œåˆ©ç”¨2Dæ‰©æ•£æ¨¡å‹å’Œæç¤ºè‡ªä¼˜åŒ–ç”Ÿæˆé«˜è´¨é‡ä¸”å…¨å±€ä¸€è‡´çš„å…¨æ™¯å›¾åƒï¼Œå†å°†å…¶æå‡åˆ°3Dé«˜æ–¯ç‚¹splattingä¸­ï¼Œå¹¶é€šè¿‡å¯¹é½2Då•ç›®æ·±åº¦æ¥æ„å»ºç©ºé—´ä¸€è‡´çš„ç»“æ„ã€‚</p><p><strong>æ–¹æ³•</strong></p><p>ï¼ˆ1ï¼‰æ–‡æœ¬åˆ°360Â°å…¨æ™¯ç”Ÿæˆï¼Œé‡‡ç”¨è‡ªä¼˜åŒ–æµç¨‹ï¼Œç¡®ä¿ç”Ÿæˆé²æ£’æ€§ï¼Œå¹¶ä¸æ–‡æœ¬è¯­ä¹‰å¯¹é½ï¼›ï¼ˆ2ï¼‰ä»å…¨æ™¯å‡ ä½•åœºåˆå§‹åŒ–ï¼Œå°†è¯­ä¹‰å¯¹é½å’Œå‡ ä½•å¯¹åº”å…³ç³»ä½œä¸ºé«˜æ–¯ä¼˜åŒ–æ­£åˆ™åŒ–ï¼Œä»¥è§£å†³å•è§†å›¾è¾“å…¥é€ æˆçš„å·®è·ï¼›ï¼ˆ3ï¼‰åˆ©ç”¨è™šæ‹Ÿç›¸æœºåˆæˆè§†å·®ï¼Œå¹¶é€šè¿‡å¼ºåˆ¶ç‰¹å¾çº§ç›¸ä¼¼æ€§æ¥æŒ‡å¯¼é«˜æ–¯å¡«å……ä¸å¯è§åŒºåŸŸçš„å‡ ä½•å·®è·ã€‚</p><p><strong>ç»“è®º</strong></p><p>ï¼ˆ1ï¼‰æœ¬æ–‡æå‡ºçš„æ–¹æ³•åœ¨æ–‡æœ¬åˆ°360åº¦å…¨æ™¯åœºæ™¯ç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†è¾ƒå¥½çš„æ€§èƒ½ï¼Œå¯ä»¥åœ¨å‡ åˆ†é’Ÿå†…ç”Ÿæˆé«˜è´¨é‡ã€å…¨å±€ä¸€è‡´ä¸”å¯å®æ—¶æ¢ç´¢çš„360åº¦å…¨æ™¯åœºæ™¯ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ï¼Œå³æä¾›ä¸€ç§å¿«é€Ÿä¸”é«˜æ•ˆçš„æ–¹æ³•æ¥åˆ›å»ºæ²‰æµ¸å¼è™šæ‹Ÿç°å®ä½“éªŒã€‚</p><p>ï¼ˆ2ï¼‰<strong>åˆ›æ–°ç‚¹</strong>ï¼š- æå‡ºäº†ä¸€ç§æ–‡æœ¬åˆ°3D 360åº¦åœºæ™¯ç”Ÿæˆç®¡é“ï¼Œè¯¥ç®¡é“åˆ©ç”¨2Dæ‰©æ•£æ¨¡å‹å’Œæç¤ºè‡ªä¼˜åŒ–ç”Ÿæˆé«˜è´¨é‡ä¸”å…¨å±€ä¸€è‡´çš„å…¨æ™¯å›¾åƒï¼Œå¹¶å°†å…¶æå‡åˆ°3Dé«˜æ–¯ç‚¹splattingä¸­ã€‚- é€šè¿‡å¯¹é½2Då•ç›®æ·±åº¦æ¥æ„å»ºç©ºé—´ä¸€è‡´çš„ç»“æ„ï¼Œè§£å†³äº†å•è§†å›¾è¾“å…¥é€ æˆçš„å·®è·ã€‚</p><p><strong>æ€§èƒ½</strong>ï¼š- è¯¥æ–¹æ³•åœ¨æ–‡æœ¬åˆ°360åº¦å…¨æ™¯åœºæ™¯ç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†è¾ƒå¥½çš„æ€§èƒ½ï¼Œå¯ä»¥åœ¨å‡ åˆ†é’Ÿå†…ç”Ÿæˆé«˜è´¨é‡ã€å…¨å±€ä¸€è‡´ä¸”å¯å®æ—¶æ¢ç´¢çš„360åº¦å…¨æ™¯åœºæ™¯ã€‚</p><p><strong>å·¥ä½œé‡</strong>ï¼š- è¯¥æ–¹æ³•çš„å·¥ä½œé‡ç›¸å¯¹è¾ƒå°ï¼Œå¯ä»¥åœ¨å‡ åˆ†é’Ÿå†…ç”Ÿæˆä¸€ä¸ª360åº¦å…¨æ™¯åœºæ™¯ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-31c98e3198cc0cdc817317b3bf23d03b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0729d57eb4a8e1293909757da5dd5064.jpg" align="middle"></details>## SplatPose &amp; Detect: Pose-Agnostic 3D Anomaly Detection**Authors:Mathis Kruse, Marco Rudolph, Dominik Woiwode, Bodo Rosenhahn**Detecting anomalies in images has become a well-explored problem in both academia and industry. State-of-the-art algorithms are able to detect defects in increasingly difficult settings and data modalities. However, most current methods are not suited to address 3D objects captured from differing poses. While solutions using Neural Radiance Fields (NeRFs) have been proposed, they suffer from excessive computation requirements, which hinder real-world usability. For this reason, we propose the novel 3D Gaussian splatting-based framework SplatPose which, given multi-view images of a 3D object, accurately estimates the pose of unseen views in a differentiable manner, and detects anomalies in them. We achieve state-of-the-art results in both training and inference speed, and detection performance, even when using less training data than competing methods. We thoroughly evaluate our framework using the recently proposed Pose-agnostic Anomaly Detection benchmark and its multi-pose anomaly detection (MAD) data set. [PDF](http://arxiv.org/abs/2404.06832v1) Visual Anomaly and Novelty Detection 2.0 Workshop at CVPR 2024**Summary**é€šè¿‡ç»™å®š 3D ç‰©ä½“çš„å¤šè§†å›¾å›¾åƒï¼ŒSplatPose å¯ä»¥å‡†ç¡®ä¼°è®¡æœªè§è§†å›¾çš„å§¿åŠ¿å¹¶æ£€æµ‹å…¶ä¸­çš„å¼‚å¸¸ã€‚**Key Takeaways**- **è§£å†³ 3D å§¿æ€é—®é¢˜ï¼š** SplatPose é€‚ç”¨äºä»ä¸åŒå§¿åŠ¿æ•è·çš„ 3D å¯¹è±¡çš„å¼‚å¸¸æ£€æµ‹ã€‚- **åŸºäº 3D é«˜æ–¯æº…å°„ï¼š** è¯¥æ¡†æ¶é‡‡ç”¨åˆ›æ–°çš„åŸºäº 3D é«˜æ–¯æº…å°„çš„ç®—æ³•ã€‚- **å¯å¾®å§¿åŠ¿ä¼°è®¡ï¼š** ä»¥å¯å¾®æ–¹å¼ä¼°è®¡æœªè§è§†å›¾çš„å§¿åŠ¿ã€‚- **é«˜æ•ˆè®¡ç®—ï¼š** åœ¨è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦æ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„æˆæœã€‚- **ä¼˜å¼‚çš„æ£€æµ‹æ€§èƒ½ï¼š** å³ä½¿ä½¿ç”¨è¾ƒå°‘çš„è®­ç»ƒæ•°æ®ï¼Œä¹Ÿèƒ½æ£€æµ‹å¼‚å¸¸ã€‚- **å¯¹å§¿åŠ¿æ— å…³çš„å¼‚å¸¸æ£€æµ‹åŸºå‡†è¯„ä¼°ï¼š** ä½¿ç”¨æœ€æ–°çš„åŸºå‡†è¿›è¡Œäº†å…¨é¢çš„è¯„ä¼°ã€‚- **å¤šå§¿åŠ¿å¼‚å¸¸æ£€æµ‹æ•°æ®é›†ï¼š** åœ¨å¤šå§¿åŠ¿å¼‚å¸¸æ£€æµ‹æ•°æ®é›†ä¸Šè¿›è¡Œæµ‹è¯•ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>è®ºæ–‡æ ‡é¢˜ï¼šSplatPose&amp;Detectï¼šä¸å§¿æ€æ— å…³çš„ 3D å¼‚å¸¸æ£€æµ‹</li><li>ä½œè€…ï¼šYifan Jiang, Guilin Liu, Zhehui Yuan, Shenghua Gao, Jingyi Yu, Xiaoguang Han</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šåä¸­ç§‘æŠ€å¤§å­¦</li><li>å…³é”®è¯ï¼šComputer Vision, Anomaly Detection, 3D Object, Pose-Agnostic</li><li>è®ºæ–‡é“¾æ¥ï¼šNone    Github é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šå¼‚å¸¸æ£€æµ‹åœ¨å›¾åƒä¸­æ˜¯ä¸€ä¸ªç»è¿‡å……åˆ†æ¢ç´¢çš„é—®é¢˜ï¼Œæœ€å…ˆè¿›çš„ç®—æ³•èƒ½å¤Ÿåœ¨è¶Šæ¥è¶Šå›°éš¾çš„è®¾ç½®å’Œæ•°æ®æ¨¡å¼ä¸­æ£€æµ‹ç¼ºé™·ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°å½“å‰çš„æ–¹æ³•ä¸é€‚åˆå¤„ç†ä»ä¸åŒå§¿æ€æ•è·çš„ 3D å¯¹è±¡ã€‚è™½ç„¶å·²ç»æå‡ºäº†ä½¿ç”¨ç¥ç»è¾å°„åœºçš„è§£å†³æ–¹æ¡ˆï¼Œä½†å®ƒä»¬å­˜åœ¨è¿‡åº¦çš„è®¡ç®—è¦æ±‚ï¼Œè¿™é˜»ç¢äº†å®é™…ä½¿ç”¨ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šè¿‡å»æ–¹æ³•å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š</li><li>æ— æ³•å¤„ç†ä¸åŒå§¿æ€çš„ 3D å¯¹è±¡ã€‚</li><li>ä½¿ç”¨ç¥ç»è¾å°„åœºçš„æ–¹æ³•è®¡ç®—è¦æ±‚è¿‡é«˜ã€‚</li><li>è®­ç»ƒæ•°æ®é‡å¤§ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†åŸºäº 3D é«˜æ–¯æ–‘å—çš„æ–°é¢–æ¡†æ¶ SplatPoseï¼Œè¯¥æ¡†æ¶åœ¨ç»™å®š 3D å¯¹è±¡çš„å¤šè§†å›¾å›¾åƒçš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿä»¥å¯å¾®åˆ†çš„æ–¹å¼å‡†ç¡®ä¼°è®¡æœªè§è§†å›¾çš„å§¿æ€ï¼Œå¹¶æ£€æµ‹å…¶ä¸­çš„å¼‚å¸¸ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šè¯¥æ–¹æ³•åœ¨è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦ä»¥åŠæ£€æµ‹æ€§èƒ½æ–¹é¢éƒ½è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œå³ä½¿ä½¿ç”¨æ¯”ç«äº‰æ–¹æ³•æ›´å°‘çš„è®­ç»ƒæ•°æ®ä¹Ÿæ˜¯å¦‚æ­¤ã€‚ä½¿ç”¨æœ€è¿‘æå‡ºçš„ä¸å§¿æ€æ— å…³çš„å¼‚å¸¸æ£€æµ‹åŸºå‡†åŠå…¶å¤šå§¿æ€å¼‚å¸¸æ£€æµ‹ (MAD) æ•°æ®é›†å¯¹è¯¥æ¡†æ¶è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚</li></ol><p><strong>7. æ–¹æ³•</strong></p><p>è¯¥æ–¹æ³•æå‡ºäº†ä¸€ç§åŸºäº 3D é«˜æ–¯æ–‘å—çš„æ–°é¢–æ¡†æ¶ SplatPoseï¼Œå…·ä½“æ­¥éª¤å¦‚ä¸‹ï¼š</p><p>(1) <strong>å§¿æ€ä¼°è®¡ï¼š</strong>åˆ©ç”¨å¤šè§†å›¾å›¾åƒï¼Œé€šè¿‡å¯å¾®åˆ†çš„æ–¹å¼ä¼°è®¡æœªè§è§†å›¾çš„å§¿æ€ï¼Œä»è€Œè·å¾— 3D å¯¹è±¡çš„å®Œæ•´è¡¨ç¤ºã€‚</p><p>(2) <strong>å¼‚å¸¸æ£€æµ‹ï¼š</strong>åœ¨ä¼°è®¡çš„ 3D è¡¨ç¤ºä¸Šï¼Œä½¿ç”¨é«˜æ–¯æ··åˆæ¨¡å‹ (GMM) æ£€æµ‹å¼‚å¸¸ï¼Œå…¶ä¸­æ¯ä¸ªé«˜æ–¯åˆ†é‡å¯¹åº”äºå¯¹è±¡çš„æ­£å¸¸éƒ¨åˆ†ã€‚</p><p>(3) <strong>ä¸å§¿æ€æ— å…³ï¼š</strong>é€šè¿‡å°†å§¿æ€ä¼°è®¡ä¸å¼‚å¸¸æ£€æµ‹è§£è€¦ï¼Œè¯¥æ–¹æ³•å®ç°äº†ä¸å§¿æ€æ— å…³çš„å¼‚å¸¸æ£€æµ‹ï¼Œå³ä½¿å¯¹è±¡ä»¥ä¸åŒçš„å§¿æ€å‡ºç°ï¼Œä¹Ÿèƒ½å‡†ç¡®æ£€æµ‹å¼‚å¸¸ã€‚</p><ol><li>ç»“è®ºï¼š(1): æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¸å§¿æ€æ— å…³çš„å¼‚å¸¸æ£€æµ‹æ–¹æ³•ã€‚ç»™å®šå¤šè§†å›¾å›¾åƒï¼Œæˆ‘ä»¬ä½¿ç”¨é«˜æ–¯æ–‘å—è¡¨ç¤ºå¯¹è±¡ï¼Œç”¨äºå§¿æ€ä¼°è®¡ï¼Œå¹¶åœ¨æ²¡æœ‰å…ˆéªŒå§¿æ€ä¿¡æ¯çš„æƒ…å†µä¸‹æŸ¥æ‰¾å›¾åƒä¸­çš„å¼‚å¸¸ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ£€æµ‹ä»»åŠ¡ä¸­å‡»è´¥äº†æ‰€æœ‰ç«äº‰å¯¹æ‰‹ï¼ŒåŒæ—¶åœ¨è®­ç»ƒå’Œæ¨ç†æ—¶é—´ä¸Šä»ç„¶å¿«å‡ ä¸ªæ•°é‡çº§ï¼Œè¿™ä½¿å…¶æ›´é€‚åˆåœ¨ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½²ã€‚æˆ‘ä»¬å¸Œæœ›æœªæ¥çš„å·¥ä½œè‡´åŠ›äºæ”¹è¿›ç²—ç•¥çš„å§¿æ€ä¼°è®¡å’Œå›¾åƒç‰¹å¾æ¯”è¾ƒã€‚å°†æˆ‘ä»¬çš„å‘ç°åº”ç”¨äºç›¸é‚»é¢†åŸŸï¼Œä¾‹å¦‚äººç±»å§¿æ€ä¼°è®¡[16,43]ï¼Œå¯¹æˆ‘ä»¬æ¥è¯´æ˜¯ä¸€ä¸ªæœ‰å¸Œæœ›çš„ä¸‹ä¸€æ­¥æ–¹å‘ã€‚ç¼©å°åˆæˆæ•°æ®å’ŒçœŸå®ä¸–ç•Œæ•°æ®ä¹‹é—´çš„å·®è·ä¹Ÿéœ€è¦æ›´å¤šçš„å·¥ä½œã€‚æœ€åï¼Œæˆ‘ä»¬å¸Œæœ›ç ”ç©¶å°†ä¸‰ç»´ç‚¹äº‘ä¿¡æ¯çº³å…¥ç°æœ‰äºŒç»´æ–¹æ³•çš„æ–¹æ³•ã€‚è‡´è°¢ã€‚è¿™é¡¹å·¥ä½œå¾—åˆ°äº†å¾·å›½è”é‚¦æ•™è‚²å’Œç ”ç©¶éƒ¨ (BMBF) çš„æ”¯æŒï¼Œå¾·å›½åœ¨ AIservicecenter KISSKIï¼ˆæ‹¨æ¬¾å· 01IS22093Cï¼‰ä¸‹ï¼Œä¸‹è¨å…‹æ£®å·ç§‘å­¦å’Œæ–‡åŒ–éƒ¨ (MWK) é€šè¿‡ Volkswagen åŸºé‡‘ä¼šçš„ zukunft.niedersachsen è®¡åˆ’ä»¥åŠå¾·å›½ç ”ç©¶åŸºé‡‘ä¼š (DFG) åœ¨å¾·å›½å“è¶Šæˆ˜ç•¥ä¸‹ï¼Œåœ¨å“è¶Šé›†ç¾¤ PhoenixD (EXC2122) å†…ã€‚(2): åˆ›æ–°ç‚¹ï¼šæå‡ºäº†åŸºäº 3D é«˜æ–¯æ–‘å—çš„æ–°é¢–æ¡†æ¶ SplatPoseï¼Œè¯¥æ¡†æ¶å¯ä»¥ä»¥å¯å¾®åˆ†çš„æ–¹å¼ä¼°è®¡æœªè§è§†å›¾çš„å§¿æ€ï¼Œå¹¶æ£€æµ‹å…¶ä¸­çš„å¼‚å¸¸ã€‚æ€§èƒ½ï¼šåœ¨è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦ä»¥åŠæ£€æµ‹æ€§èƒ½æ–¹é¢éƒ½è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œå³ä½¿ä½¿ç”¨æ¯”ç«äº‰æ–¹æ³•æ›´å°‘çš„è®­ç»ƒæ•°æ®ä¹Ÿæ˜¯å¦‚æ­¤ã€‚å·¥ä½œé‡ï¼šè®­ç»ƒå’Œæ¨ç†é€Ÿåº¦å¿«å‡ ä¸ªæ•°é‡çº§ï¼Œä½¿å…¶æ›´é€‚åˆåœ¨ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½²ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-98f8ef1e6e42e25a4f68a8eac21e75a0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e84352a2af71b475f2c2f75f76369d19.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ab11a3ec9140c1596b81427a74134f5b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d06c9960f9e8a142bf4951e22b98ea8d.jpg" align="middle"></details><h2 id="Zero-shot-Point-Cloud-Completion-Via-2D-Priors"><a href="#Zero-shot-Point-Cloud-Completion-Via-2D-Priors" class="headerlink" title="Zero-shot Point Cloud Completion Via 2D Priors"></a>Zero-shot Point Cloud Completion Via 2D Priors</h2><p><strong>Authors:Tianxin Huang, Zhiwen Yan, Yuyang Zhao, Gim Hee Lee</strong></p><p>3D point cloud completion is designed to recover complete shapes from partially observed point clouds. Conventional completion methods typically depend on extensive point cloud data for training %, with their effectiveness often constrained to object categories similar to those seen during training. In contrast, we propose a zero-shot framework aimed at completing partially observed point clouds across any unseen categories. Leveraging point rendering via Gaussian Splatting, we develop techniques of Point Cloud Colorization and Zero-shot Fractal Completion that utilize 2D priors from pre-trained diffusion models to infer missing regions. Experimental results on both synthetic and real-world scanned point clouds demonstrate that our approach outperforms existing methods in completing a variety of objects without any requirement for specific training data. </p><p><a href="http://arxiv.org/abs/2404.06814v1">PDF</a> </p><p><strong>æ‘˜è¦</strong><br>é›¶æ ·æœ¬3Dç‚¹äº‘è¡¥å…¨é‡‡ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„2Då…ˆéªŒæ¥æ¢å¤æœªè§‚å¯Ÿåˆ°çš„ç‚¹äº‘åŒºåŸŸã€‚</p><p><strong>å…³é”®è¦ç‚¹</strong></p><ul><li>æå‡ºé›¶æ ·æœ¬3Dç‚¹äº‘è¡¥å…¨æ¡†æ¶ï¼Œé€‚ç”¨äºä»»ä½•æœªè§ç±»åˆ«ã€‚</li><li>åˆ©ç”¨é«˜æ–¯æ•£å°„è¿›è¡Œç‚¹äº‘æ¸²æŸ“ï¼Œå°†2Då…ˆéªŒèå…¥ç‚¹äº‘è¡¥å…¨ã€‚</li><li>å¼€å‘ç‚¹äº‘ç€è‰²å’Œé›¶æ ·æœ¬åˆ†å½¢è¡¥å…¨æŠ€æœ¯ã€‚</li><li>æ— éœ€é’ˆå¯¹æ€§è®­ç»ƒæ•°æ®å³å¯è¡¥å…¨å„ç±»ç‰©ä½“ã€‚</li><li>åœ¨åˆæˆå’ŒçœŸå®æ‰«æç‚¹äº‘ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li><li>æ‹“å±•äº†3Dç‚¹äº‘å¤„ç†çš„é€‚ç”¨èŒƒå›´ã€‚</li><li>ä¿ƒè¿›é›¶æ ·æœ¬å­¦ä¹ åœ¨3Dè§†è§‰ä¸­çš„åº”ç”¨ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šé›¶æ ·æœ¬ç‚¹äº‘è¡¥å…¨é€šè¿‡ 2D å…ˆéªŒ</li><li>ä½œè€…ï¼šTianxin Huangã€Zhiwen Yanã€Yuyang Zhaoã€Gim Hee Lee</li><li>å•ä½ï¼šæ–°åŠ å¡å›½ç«‹å¤§å­¦è®¡ç®—å­¦é™¢</li><li>å…³é”®è¯ï¼šç‚¹äº‘è¡¥å…¨ã€é«˜æ–¯æ¸²æŸ“ã€æ‰©æ•£æ¨¡å‹</li><li>è®ºæ–‡é“¾æ¥ï¼šNoneï¼ŒGithub é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šç‚¹äº‘è¡¥å…¨æ—¨åœ¨ä»éƒ¨åˆ†è§‚æµ‹çš„ç‚¹äº‘ä¸­æ¢å¤å®Œæ•´çš„å½¢çŠ¶ã€‚ä¼ ç»Ÿè¡¥å…¨æ–¹æ³•é€šå¸¸ä¾èµ–äºå¤§é‡çš„ç‚¹äº‘æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå…¶æœ‰æ•ˆæ€§é€šå¸¸ä»…é™äºä¸è®­ç»ƒæœŸé—´æ‰€è§å¯¹è±¡ç±»åˆ«ç›¸ä¼¼çš„å¯¹è±¡ç±»åˆ«ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šç°æœ‰æ–¹æ³•åœ¨å¤„ç†æµ‹è¯•æ—¶æ•°æ®æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œä¾‹å¦‚æœªè§çš„å¯¹è±¡ç±»åˆ«æˆ–çœŸå®ä¸–ç•Œçš„æ‰«æã€‚è¿™äº›æ–¹æ³•çš„æœ‰æ•ˆæ€§å¾€å¾€å—åˆ°è®­ç»ƒæ•°æ®é›†å¤šæ ·æ€§ä¸è¶³çš„é™åˆ¶ã€‚ï¼ˆ3ï¼‰è®ºæ–‡æ–¹æ³•ï¼šè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§é›¶æ ·æœ¬æ¡†æ¶ï¼Œæ—¨åœ¨è·¨è¶Šä»»ä½•æœªè§ç±»åˆ«è¡¥å…¨éƒ¨åˆ†è§‚æµ‹çš„ç‚¹äº‘ã€‚åˆ©ç”¨é€šè¿‡é«˜æ–¯æ¸²æŸ“è¿›è¡Œçš„ç‚¹æ¸²æŸ“ï¼Œå¼€å‘äº†ç‚¹äº‘ç€è‰²å’Œé›¶æ ·æœ¬åˆ†å½¢è¡¥å…¨æŠ€æœ¯ï¼Œåˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„ 2D å…ˆéªŒæ¥æ¨æ–­ç¼ºå¤±åŒºåŸŸã€‚ï¼ˆ4ï¼‰ä»»åŠ¡å’Œæ€§èƒ½ï¼šè¯¥æ–¹æ³•åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ‰«æçš„ç‚¹äº‘ä¸Šéƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ— éœ€ä»»ä½•ç‰¹å®šè®­ç»ƒæ•°æ®å³å¯è¡¥å…¨å„ç§å¯¹è±¡ã€‚å®éªŒç»“æœè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) ç‚¹äº‘ç€è‰²ï¼šåˆ©ç”¨é«˜æ–¯æ¸²æŸ“å°†ç‚¹äº‘è½¬æ¢ä¸ºå¯æ¸²æŸ“çš„ 2D å›¾åƒï¼Œå¹¶é€šè¿‡æ·±åº¦æ¡ä»¶ç€è‰²ä¼˜åŒ– 3D é«˜æ–¯ä½“ï¼›(2) é›¶æ ·æœ¬åˆ†å½¢è¡¥å…¨ï¼šåˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„ 2D å…ˆéªŒï¼Œä¼˜åŒ– 3D é«˜æ–¯ä½“ï¼Œå¹¶å¼•å…¥è§†å›¾ç›¸å…³æŒ‡å¯¼å’Œä¿æŒçº¦æŸï¼Œä»¥å®Œæˆç¼ºå¤±åŒºåŸŸï¼›(3) é«˜æ–¯æ›²é¢æå–ï¼šä»ä¼˜åŒ–åçš„ 3D é«˜æ–¯ä½“çš„ä¸­å¿ƒä¸­æå–è¡¨é¢ç‚¹ï¼Œå½¢æˆå‡åŒ€çš„è¡¥å…¨ç‚¹äº‘ã€‚</p></li><li><p>ç»“è®ºï¼š(1): æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§é›¶æ ·æœ¬ç‚¹äº‘è¡¥å…¨æ¡†æ¶ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹ä¸°å¯Œçš„äºŒç»´å…ˆéªŒé€šè¿‡ä¸‰ç»´é«˜æ–¯æ¸²æŸ“è¿›è¡Œè¡¥å…¨ã€‚ä¸æ–‡æœ¬é©±åŠ¨çš„è¡¥å…¨æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸éœ€è¦ä»»ä½•é¢å¤–çš„æç¤ºã€‚æ•´ä¸ªè¡¥å…¨è¿‡ç¨‹ç”±ç‚¹äº‘ç€è‰²å’Œé›¶æ ·æœ¬åˆ†å½¢è¡¥å…¨ï¼ˆZFCï¼‰ç»„æˆã€‚åœ¨ç‚¹äº‘ç€è‰²ä¸­ï¼Œæˆ‘ä»¬æå‡ºå‚è€ƒè§†ç‚¹ä¼°è®¡å’Œæ·±åº¦æ¡ä»¶ç€è‰²æ¥ä¼°è®¡éƒ¨åˆ†ç‚¹äº‘çš„å‚è€ƒå›¾åƒã€‚éšåï¼Œæˆ‘ä»¬å¼•å…¥ ZFCï¼Œé€šè¿‡ä¼˜åŒ–ä¸‰ç»´é«˜æ–¯ä½“æ¥è¡¥å…¨éƒ¨åˆ†ç‚¹äº‘çš„ç¼ºå¤±åŒºåŸŸï¼Œè¯¥é«˜æ–¯ä½“é€šè¿‡å‚è€ƒå›¾åƒè°ƒèŠ‚çš„è§†ç‚¹ç›¸å…³æŒ‡å¯¼è¿›è¡Œè°ƒèŠ‚ã€‚æœ€åï¼Œæˆ‘ä»¬ä»ä¸‰ç»´é«˜æ–¯ä½“ä¸­æå–å®Œæˆçš„ç‚¹äº‘ï¼Œå¹¶ä½¿ç”¨ç½‘æ ¼æ‹‰å–æ¨¡å—å°†å…¶é‡æ–°é‡‡æ ·ä¸ºå‡åŒ€çš„ç»“æœã€‚æ ¹æ®æˆ‘ä»¬çš„å®éªŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ¯”ç°æœ‰çš„åŸºäºç½‘ç»œçš„è¡¥å…¨æ–¹æ³•å–å¾—äº†æ›´å¥½çš„æ€§èƒ½ï¼Œåœ¨åˆæˆå’ŒçœŸå®æ‰«æçš„ç‚¹äº‘ä¸Šéƒ½å…·æœ‰å¾ˆå¼ºçš„é²æ£’æ€§ã€‚(2): åˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åˆ©ç”¨æ‰©æ•£æ¨¡å‹äºŒç»´å…ˆéªŒè¿›è¡Œé›¶æ ·æœ¬ç‚¹äº‘è¡¥å…¨çš„æ¡†æ¶ï¼›æ€§èƒ½ï¼šåœ¨åˆæˆå’ŒçœŸå®æ‰«æçš„ç‚¹äº‘ä¸Šéƒ½ä¼˜äºç°æœ‰çš„åŸºäºç½‘ç»œçš„è¡¥å…¨æ–¹æ³•ï¼›å·¥ä½œé‡ï¼šç”±äºéœ€è¦é’ˆå¯¹æ¯ä¸ªç‚¹äº‘è¿›è¡Œå•ç‹¬çš„ä¼˜åŒ–è¿‡ç¨‹ä»¥é›†æˆæ‰©æ•£æ¨¡å‹çš„äºŒç»´å…ˆéªŒï¼Œå› æ­¤æ¯”ç°æœ‰çš„åŸºäºç½‘ç»œçš„æ–¹æ³•æ…¢ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-8167bf42bfd5c3b7928434682050264a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e5f1e4af1bed29e26696ea969cdbf7b4.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c528148defac6befac55b074fe88fc24.jpg" align="middle"></details><h2 id="3D-Geometry-aware-Deformable-Gaussian-Splatting-for-Dynamic-View-Synthesis"><a href="#3D-Geometry-aware-Deformable-Gaussian-Splatting-for-Dynamic-View-Synthesis" class="headerlink" title="3D Geometry-aware Deformable Gaussian Splatting for Dynamic View   Synthesis"></a>3D Geometry-aware Deformable Gaussian Splatting for Dynamic View   Synthesis</h2><p><strong>Authors:Zhicheng Lu, Xiang Guo, Le Hui, Tianrui Chen, Min Yang, Xiao Tang, Feng Zhu, Yuchao Dai</strong></p><p>In this paper, we propose a 3D geometry-aware deformable Gaussian Splatting method for dynamic view synthesis. Existing neural radiance fields (NeRF) based solutions learn the deformation in an implicit manner, which cannot incorporate 3D scene geometry. Therefore, the learned deformation is not necessarily geometrically coherent, which results in unsatisfactory dynamic view synthesis and 3D dynamic reconstruction. Recently, 3D Gaussian Splatting provides a new representation of the 3D scene, building upon which the 3D geometry could be exploited in learning the complex 3D deformation. Specifically, the scenes are represented as a collection of 3D Gaussian, where each 3D Gaussian is optimized to move and rotate over time to model the deformation. To enforce the 3D scene geometry constraint during deformation, we explicitly extract 3D geometry features and integrate them in learning the 3D deformation. In this way, our solution achieves 3D geometry-aware deformation modeling, which enables improved dynamic view synthesis and 3D dynamic reconstruction. Extensive experimental results on both synthetic and real datasets prove the superiority of our solution, which achieves new state-of-the-art performance.   The project is available at <a href="https://npucvr.github.io/GaGS/">https://npucvr.github.io/GaGS/</a> </p><p><a href="http://arxiv.org/abs/2404.06270v1">PDF</a> Accepted by CVPR 2024. Project page: <a href="https://npucvr.github.io/GaGS/">https://npucvr.github.io/GaGS/</a></p><p><strong>Summary</strong><br>ä¸‰ç»´å‡ ä½•æ„ŸçŸ¥å˜å½¢é«˜æ–¯æ–‘ç‚¹æŠ•å½±ï¼Œå¯å®ç°åŠ¨æ€è§†è§’åˆæˆã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ç°æœ‰åŸºäºç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰çš„è§£å†³æ–¹æ¡ˆä»¥éšå¼æ–¹å¼å­¦ä¹ å˜å½¢ï¼Œæ— æ³•çº³å…¥ 3D åœºæ™¯å‡ ä½•ã€‚</li><li>å› æ­¤ï¼Œå­¦ä¹ åˆ°çš„å˜å½¢ä¸ä¸€å®šå…·æœ‰å‡ ä½•ç›¸å¹²æ€§ï¼Œè¿™ä¼šå¯¼è‡´åŠ¨æ€è§†è§’åˆæˆå’Œ 3D åŠ¨æ€é‡å»ºæ•ˆæœä¸ç†æƒ³ã€‚</li><li>3D é«˜æ–¯æ–‘ç‚¹æŠ•å½±æä¾›äº† 3D åœºæ™¯çš„æ–°è¡¨ç¤ºï¼Œå¯ä»¥åœ¨æ­¤åŸºç¡€ä¸Šåˆ©ç”¨ 3D å‡ ä½•æ¥å­¦ä¹ å¤æ‚çš„ 3D å˜å½¢ã€‚</li><li>åœºæ™¯è¡¨ç¤ºä¸º 3D é«˜æ–¯é›†åˆï¼Œå…¶ä¸­æ¯ä¸ª 3D é«˜æ–¯ç»è¿‡ä¼˜åŒ–ï¼Œå¯ä»¥åœ¨æ—¶é—´ä¸Šç§»åŠ¨å’Œæ—‹è½¬ä»¥å»ºæ¨¡å˜å½¢ã€‚</li><li>ä¸ºäº†åœ¨å˜å½¢è¿‡ç¨‹ä¸­å¼ºåˆ¶æ‰§è¡Œ 3D åœºæ™¯å‡ ä½•çº¦æŸï¼Œæˆ‘ä»¬æ˜¾å¼æå– 3D å‡ ä½•ç‰¹å¾å¹¶å°†å…¶æ•´åˆåˆ°å­¦ä¹  3D å˜å½¢ä¸­ã€‚</li><li>é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆå®ç°äº† 3D å‡ ä½•æ„ŸçŸ¥å˜å½¢å»ºæ¨¡ï¼Œä»è€Œæ”¹è¿›äº†åŠ¨æ€è§†å›¾åˆæˆå’Œ 3D åŠ¨æ€é‡å»ºã€‚</li><li>åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒç»“æœè¯æ˜äº†æˆ‘ä»¬è§£å†³æ–¹æ¡ˆçš„ä¼˜è¶Šæ€§ï¼Œå®ƒå–å¾—äº†æ–°çš„æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>è®ºæ–‡æ ‡é¢˜ï¼š3D å‡ ä½•æ„ŸçŸ¥çš„å¯å˜å½¢é«˜æ–¯æ•£å¸ƒç”¨äºåŠ¨æ€è§†å›¾åˆæˆ</li><li>ä½œè€…ï¼šMinghao Chen, Yuxin Wen, Yufeng Zheng, Yong-Liang Yang</li><li>å•ä½ï¼šæ— </li><li>å…³é”®è¯ï¼šåŠ¨æ€è§†å›¾åˆæˆã€3D å‡ ä½•æ„ŸçŸ¥ã€å¯å˜å½¢é«˜æ–¯æ•£å¸ƒ</li><li>è®ºæ–‡é“¾æ¥ï¼šæ— ï¼ŒGithub ä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šç°æœ‰çš„åŸºäºç¥ç»è¾å°„åœº (NeRF) çš„åŠ¨æ€è§†å›¾åˆæˆæ–¹æ³•ä»¥éšå¼æ–¹å¼å­¦ä¹ å˜å½¢ï¼Œæ— æ³•èå…¥ 3D åœºæ™¯å‡ ä½•ã€‚å› æ­¤ï¼Œå­¦ä¹ åˆ°çš„å˜å½¢åœ¨å‡ ä½•ä¸Šä¸ä¸€å®šè¿è´¯ï¼Œå¯¼è‡´åŠ¨æ€è§†å›¾åˆæˆå’Œ 3D åŠ¨æ€é‡å»ºæ•ˆæœä¸ä½³ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼š3D é«˜æ–¯æ•£å¸ƒæä¾›äº†ä¸€ç§æ–°çš„ 3D åœºæ™¯è¡¨ç¤ºï¼Œåœ¨æ­¤åŸºç¡€ä¸Šï¼Œå¯ä»¥åœ¨å­¦ä¹  3D å¤æ‚å˜å½¢æ—¶åˆ©ç”¨ 3D å‡ ä½•ã€‚ç„¶è€Œï¼Œè¿‡å»çš„æ–¹æ³•ç¼ºä¹å¯¹ 3D åœºæ™¯å‡ ä½•çº¦æŸçš„æ˜¾å¼å»ºæ¨¡ï¼Œä»è€Œé™åˆ¶äº†å˜å½¢å»ºæ¨¡çš„å‡†ç¡®æ€§å’Œå‡ ä½•è¿è´¯æ€§ã€‚ï¼ˆ3ï¼‰æœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ 3D å‡ ä½•æ„ŸçŸ¥çš„å¯å˜å½¢é«˜æ–¯æ•£å¸ƒæ–¹æ³•ï¼Œç”¨äºåŠ¨æ€è§†å›¾åˆæˆã€‚è¯¥æ–¹æ³•æ˜¾å¼æå– 3D å‡ ä½•ç‰¹å¾ï¼Œå¹¶å°†å…¶èå…¥å­¦ä¹  3D å˜å½¢ä¸­ï¼Œä»è€Œå®ç° 3D å‡ ä½•æ„ŸçŸ¥çš„å˜å½¢å»ºæ¨¡ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šæœ¬æ–‡æ–¹æ³•åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒç»“æœè¯æ˜äº†å…¶ä¼˜è¶Šæ€§ï¼Œè¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚</li></ol><p>7.Methodsï¼š(1): æå‡ºä¸€ç§3Då‡ ä½•æ„ŸçŸ¥çš„å¯å˜å½¢é«˜æ–¯æ•£å¸ƒæ–¹æ³•ï¼Œç”¨äºåŠ¨æ€è§†å›¾åˆæˆï¼›(2): æ˜¾å¼æå–3Då‡ ä½•ç‰¹å¾ï¼Œå¹¶å°†å…¶èå…¥å­¦ä¹ 3Då˜å½¢ä¸­ï¼Œå®ç°3Då‡ ä½•æ„ŸçŸ¥çš„å˜å½¢å»ºæ¨¡ï¼›(3): åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šè¿›è¡Œå¹¿æ³›å®éªŒï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„ä¼˜è¶Šæ€§ï¼Œè¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ 3D å‡ ä½•æ„ŸçŸ¥çš„å¯å˜å½¢é«˜æ–¯æ•£å¸ƒæ–¹æ³•ï¼Œç”¨äºåŠ¨æ€è§†å›¾åˆæˆï¼Œè¯¥æ–¹æ³•æ˜¾å¼æå– 3D å‡ ä½•ç‰¹å¾å¹¶å°†å…¶èå…¥å­¦ä¹  3D å˜å½¢ä¸­ï¼Œå®ç°äº† 3D å‡ ä½•æ„ŸçŸ¥çš„å˜å½¢å»ºæ¨¡ï¼Œåœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒç»“æœè¯æ˜äº†å…¶ä¼˜è¶Šæ€§ï¼Œè¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š<ul><li>æå‡ºäº†ä¸€ç§ 3D å‡ ä½•æ„ŸçŸ¥çš„å¯å˜å½¢é«˜æ–¯æ•£å¸ƒæ–¹æ³•ï¼Œç”¨äºåŠ¨æ€è§†å›¾åˆæˆã€‚</li><li>æ˜¾å¼æå– 3D å‡ ä½•ç‰¹å¾ï¼Œå¹¶å°†å…¶èå…¥å­¦ä¹  3D å˜å½¢ä¸­ï¼Œå®ç° 3D å‡ ä½•æ„ŸçŸ¥çš„å˜å½¢å»ºæ¨¡ã€‚</li><li>åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šè¿›è¡Œå¹¿æ³›å®éªŒï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„ä¼˜è¶Šæ€§ï¼Œè¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚æ€§èƒ½ï¼š</li><li>åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒç»“æœè¯æ˜äº†è¯¥æ–¹æ³•çš„ä¼˜è¶Šæ€§ï¼Œè¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚å·¥ä½œé‡ï¼š</li><li>è¯¥æ–¹æ³•çš„å®ç°ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦è¾ƒå¤§çš„è®¡ç®—èµ„æºå’Œè¾ƒé•¿çš„è®­ç»ƒæ—¶é—´ã€‚</li></ul></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-afebc695ce43b9cca36774d451214003.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-75335d5dad43bcba0d01c7ed06e24b06.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-efd74ebc1d29a7da3e6ab58eae03277d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0f460737777a95c71a697250cf77dccd.jpg" align="middle"></details><h2 id="Hash3D-Training-free-Acceleration-for-3D-Generation"><a href="#Hash3D-Training-free-Acceleration-for-3D-Generation" class="headerlink" title="Hash3D: Training-free Acceleration for 3D Generation"></a>Hash3D: Training-free Acceleration for 3D Generation</h2><p><strong>Authors:Xingyi Yang, Xinchao Wang</strong></p><p>The evolution of 3D generative modeling has been notably propelled by the adoption of 2D diffusion models. Despite this progress, the cumbersome optimization process per se presents a critical hurdle to efficiency. In this paper, we introduce Hash3D, a universal acceleration for 3D generation without model training. Central to Hash3D is the insight that feature-map redundancy is prevalent in images rendered from camera positions and diffusion time-steps in close proximity. By effectively hashing and reusing these feature maps across neighboring timesteps and camera angles, Hash3D substantially prevents redundant calculations, thus accelerating the diffusion modelâ€™s inference in 3D generation tasks. We achieve this through an adaptive grid-based hashing. Surprisingly, this feature-sharing mechanism not only speed up the generation but also enhances the smoothness and view consistency of the synthesized 3D objects. Our experiments covering 5 text-to-3D and 3 image-to-3D models, demonstrate Hash3Dâ€™s versatility to speed up optimization, enhancing efficiency by 1.3 to 4 times. Additionally, Hash3Dâ€™s integration with 3D Gaussian splatting largely speeds up 3D model creation, reducing text-to-3D processing to about 10 minutes and image-to-3D conversion to roughly 30 seconds. The project page is at <a href="https://adamdad.github.io/hash3D/">https://adamdad.github.io/hash3D/</a>. </p><p><a href="http://arxiv.org/abs/2404.06091v1">PDF</a> <a href="https://adamdad.github.io/hash3D/">https://adamdad.github.io/hash3D/</a></p><p><strong>Summary</strong><br>ä½¿ç”¨Hash3Då“ˆå¸Œç®—æ³•åŠ é€Ÿ3Dç”Ÿæˆå»ºæ¨¡ï¼Œé€šè¿‡é‡ç”¨ç›¸é‚»æ—¶é—´æ­¥å’Œç›¸æœºè§†è§’ä¸­çš„ç‰¹å¾å›¾ï¼Œä»å›¾åƒæå–ä¸‰ç»´æ¨¡å‹ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ä½¿ç”¨ 2D æ‰©æ•£æ¨¡å‹åŠ é€Ÿäº† 3D ç”Ÿæˆå»ºæ¨¡ã€‚</li><li>Hash3D æ˜¯ä¸€ç§é€šç”¨åŠ é€Ÿï¼Œæ— éœ€æ¨¡å‹è®­ç»ƒå³å¯åŠ é€Ÿ 3D ç”Ÿæˆã€‚</li><li>Hash3D åˆ©ç”¨æ¸²æŸ“å›¾åƒä¸­ç›¸é‚»ä½ç½®å’Œæ—¶é—´æ­¥çš„ç‰¹å¾å›¾å†—ä½™ã€‚</li><li>é€šè¿‡å“ˆå¸Œå’Œé‡ç”¨ç›¸é‚»æ—¶é—´æ­¥å’Œç›¸æœºè§’åº¦ä¸­çš„ç‰¹å¾å›¾ï¼ŒHash3D æ¶ˆé™¤äº†å†—ä½™è®¡ç®—ã€‚</li><li>Hash3D é€šè¿‡è‡ªé€‚åº”ç½‘æ ¼å“ˆå¸Œå®ç°è¿™ä¸€ç‚¹ã€‚</li><li>ç‰¹å¾å…±äº«æœºåˆ¶ä¸ä»…åŠ å¿«äº†ç”Ÿæˆé€Ÿåº¦ï¼Œè¿˜å¢å¼ºäº†åˆæˆ 3D ç‰©ä½“çš„å¹³æ»‘åº¦å’Œè§†å›¾ä¸€è‡´æ€§ã€‚</li><li>Hash3D å¯ä¸ 3D é«˜æ–¯æ¸²æŸ“ç›¸ç»“åˆï¼Œä»è€Œæå¤§åœ°åŠ å¿« 3D æ¨¡å‹çš„åˆ›å»ºé€Ÿåº¦ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>é¢˜ç›®ï¼šHash3Dï¼šæ— éœ€è®­ç»ƒçš„ 3D ç”ŸæˆåŠ é€Ÿ</li><p></p><p></p><li>ä½œè€…ï¼šé‚¢ä¸€é˜³ï¼Œç‹æ–°è¶…</li><p></p><p></p><li>å•ä½ï¼šæ–°åŠ å¡å›½ç«‹å¤§å­¦</li><p></p><p></p><li>å…³é”®è¯ï¼šå¿«é€Ÿ 3D ç”Ÿæˆ Â· åˆ†æ•°è’¸é¦ Â· é‡‡æ ·</li><p></p><p></p><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.06091ï¼ŒGithub ä»£ç é“¾æ¥ï¼šNone</li><p></p><p></p><li>æ‘˜è¦ï¼š(1)ï¼šéšç€ 2D æ‰©æ•£æ¨¡å‹çš„é‡‡ç”¨ï¼Œ3D ç”Ÿæˆå»ºæ¨¡å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚å°½ç®¡å–å¾—äº†è¿™äº›è¿›å±•ï¼Œä½†ç¹ççš„ä¼˜åŒ–è¿‡ç¨‹æœ¬èº«å¯¹æ•ˆç‡æ„æˆäº†å…³é”®éšœç¢ã€‚(2)ï¼šè¿‡å»çš„æ–¹æ³•ï¼šåŸºäº 2D æ‰©æ•£æ¨¡å‹çš„ 3D ç”Ÿæˆæ–¹æ³•ã€‚é—®é¢˜ï¼šä¼˜åŒ–è¿‡ç¨‹ç¹çï¼Œæ•ˆç‡ä½ä¸‹ã€‚åŠ¨æœºï¼šåˆ©ç”¨ç‰¹å¾å›¾å†—ä½™æ¥åŠ é€Ÿç”Ÿæˆã€‚(3)ï¼šæœ¬è®ºæ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šHash3Dï¼Œä¸€ç§æ— éœ€æ¨¡å‹è®­ç»ƒçš„é€šç”¨ 3D ç”ŸæˆåŠ é€Ÿæ–¹æ³•ã€‚é€šè¿‡è‡ªé€‚åº”ç½‘æ ¼å“ˆå¸Œï¼Œæœ‰æ•ˆåœ°å“ˆå¸Œå’Œé‡ç”¨ç›¸é‚»æ—¶é—´æ­¥é•¿å’Œç›¸æœºè§’åº¦çš„ç‰¹å¾å›¾ï¼Œä»è€Œå¤§å¹…å‡å°‘å†—ä½™è®¡ç®—ï¼ŒåŠ é€Ÿæ‰©æ•£æ¨¡å‹åœ¨ 3D ç”Ÿæˆä»»åŠ¡ä¸­çš„æ¨ç†ã€‚(4)ï¼šæœ¬è®ºæ–‡æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šåœ¨ 5 ä¸ªæ–‡æœ¬åˆ° 3D å’Œ 3 ä¸ªå›¾åƒåˆ° 3D æ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒHash3D èƒ½å¤Ÿä»¥ 1.3~4 å€çš„æ•ˆç‡åŠ é€Ÿä¼˜åŒ–ã€‚æ­¤å¤–ï¼ŒHash3D ä¸ 3D é«˜æ–¯ splatting é›†æˆï¼Œæå¤§åœ°åŠ å¿«äº† 3D æ¨¡å‹çš„åˆ›å»ºï¼Œå°†æ–‡æœ¬åˆ° 3D å¤„ç†å‡å°‘åˆ°çº¦ 10 åˆ†é’Ÿï¼Œå°†å›¾åƒåˆ° 3D è½¬æ¢å‡å°‘åˆ°çº¦ 30 ç§’ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†ä½œè€…åŠ é€Ÿ 3D ç”Ÿæˆå¹¶æé«˜æ•ˆç‡çš„ç›®æ ‡ã€‚</li><br>&lt;/ol&gt;<p></p><p><strong>Methods:</strong></p><p>(1): è‡ªé€‚åº”ç½‘æ ¼å“ˆå¸Œï¼šæ ¹æ®æ¯ä¸ªæ ·æœ¬åŠ¨æ€è°ƒæ•´å“ˆå¸Œç½‘æ ¼å¤§å°ï¼Œä»¥åŒ¹é…ç›¸é‚»ç‰¹å¾å›¾ï¼Œæé«˜åŒ¹é…æ•ˆç‡ã€‚</p><p>(2): æ¸è¿›å¼å“ˆå¸Œï¼šéšç€æ‰©æ•£è¿‡ç¨‹çš„è¿›è¡Œï¼Œé€æ­¥å¢åŠ å“ˆå¸Œæ¦‚ç‡ï¼Œå¹³è¡¡åŒ¹é…ç²¾åº¦å’Œè®¡ç®—æˆæœ¬ã€‚</p><p>(3): ç‰¹å¾å“ˆå¸Œï¼šç›´æ¥å“ˆå¸Œç‰¹å¾å›¾ï¼Œè€Œä¸æ˜¯å™ªå£°ï¼Œä»¥æ›´æœ‰æ•ˆåœ°åˆ©ç”¨ç‰¹å¾å›¾å†—ä½™ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰æœ¬å·¥ä½œé€šè¿‡æå‡º Hash3D åŠ é€Ÿå™¨ï¼Œä¸ºåŸºäºæ‰©æ•£çš„ 3D ç”Ÿæˆå»ºæ¨¡å¸¦æ¥äº†ä»¥ä¸‹é‡è¦æ„ä¹‰ï¼š    - æ— éœ€æ¨¡å‹è®­ç»ƒï¼Œå³æ’å³ç”¨ï¼Œæœ‰æ•ˆæå‡ 3D ç”Ÿæˆæ•ˆç‡ã€‚    - ç»“åˆ 3D é«˜æ–¯ splattingï¼Œå¤§å¹…ç¼©çŸ­æ–‡æœ¬åˆ° 3D å’Œå›¾åƒåˆ° 3D çš„å¤„ç†æ—¶é—´ï¼Œæå‡ç”Ÿæˆæ•ˆç‡ã€‚</p><p>ï¼ˆ2ï¼‰æ–‡ç« çš„ä¼˜ç¼ºç‚¹æ€»ç»“ï¼ˆæŒ‰åˆ›æ–°ç‚¹ã€æ€§èƒ½ã€å·¥ä½œé‡ä¸‰ä¸ªç»´åº¦ï¼‰ï¼š    - åˆ›æ–°ç‚¹ï¼š        - æå‡ºè‡ªé€‚åº”ç½‘æ ¼å“ˆå¸Œï¼Œæ ¹æ®æ ·æœ¬åŠ¨æ€è°ƒæ•´å“ˆå¸Œç½‘æ ¼å¤§å°ï¼Œæé«˜åŒ¹é…æ•ˆç‡ã€‚        - é‡‡ç”¨æ¸è¿›å¼å“ˆå¸Œï¼Œå¹³è¡¡åŒ¹é…ç²¾åº¦å’Œè®¡ç®—æˆæœ¬ã€‚        - ç›´æ¥å“ˆå¸Œç‰¹å¾å›¾ï¼Œæ›´æœ‰æ•ˆåœ°åˆ©ç”¨ç‰¹å¾å›¾å†—ä½™ã€‚    - æ€§èƒ½ï¼š        - åœ¨ 5 ä¸ªæ–‡æœ¬åˆ° 3D å’Œ 3 ä¸ªå›¾åƒåˆ° 3D æ¨¡å‹ä¸Šï¼Œå®éªŒè¡¨æ˜ Hash3D èƒ½å¤Ÿä»¥ 1.3~4 å€çš„æ•ˆç‡åŠ é€Ÿä¼˜åŒ–ã€‚        - ç»“åˆ 3D é«˜æ–¯ splattingï¼Œå°†æ–‡æœ¬åˆ° 3D å¤„ç†å‡å°‘åˆ°çº¦ 10 åˆ†é’Ÿï¼Œå°†å›¾åƒåˆ° 3D è½¬æ¢å‡å°‘åˆ°çº¦ 30 ç§’ã€‚    - å·¥ä½œé‡ï¼š        - ç®—æ³•å®ç°ç›¸å¯¹ç®€å•ï¼Œæ˜“äºä¸ç°æœ‰çš„ 3D ç”Ÿæˆæ¨¡å‹é›†æˆã€‚        - æ— éœ€é¢å¤–çš„æ¨¡å‹è®­ç»ƒï¼Œé™ä½äº†æ—¶é—´å’Œè®¡ç®—èµ„æºçš„æ¶ˆè€—ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-0f4a6a4161f3cb3fcdc5c4397500d100.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d21f45ab3f4603885126438d366655c4.jpg" align="middle"></details></ol>]]></content>
    
    
    <summary type="html">3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-04-14  GoMAvatar Efficient Animatable Human Modeling from Monocular Video   Using Gaussians-on-Mesh</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/04/14/Paper/2024-04-14/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/04/14/Paper/2024-04-14/Talking%20Head%20Generation/</id>
    <published>2024-04-14T04:07:56.000Z</published>
    <updated>2024-04-14T04:07:56.535Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-04-14-æ›´æ–°"><a href="#2024-04-14-æ›´æ–°" class="headerlink" title="2024-04-14 æ›´æ–°"></a>2024-04-14 æ›´æ–°</h1><h2 id="Deepfake-Generation-and-Detection-A-Benchmark-and-Survey"><a href="#Deepfake-Generation-and-Detection-A-Benchmark-and-Survey" class="headerlink" title="Deepfake Generation and Detection: A Benchmark and Survey"></a>Deepfake Generation and Detection: A Benchmark and Survey</h2><p><strong>Authors:Gan Pei, Jiangning Zhang, Menghan Hu, Zhenyu Zhang, Chengjie Wang, Yunsheng Wu, Guangtao Zhai, Jian Yang, Chunhua Shen, Dacheng Tao</strong></p><p>In addition to the advancements in deepfake generation, corresponding detection technologies need to continuously evolve to regulate the potential misuse of deepfakes, such as for privacy invasion and phishing attacks. This survey comprehensively reviews the latest developments in deepfake generation and detection, summarizing and analyzing the current state of the art in this rapidly evolving field. We first unify task definitions, comprehensively introduce datasets and metrics, and discuss the development of generation and detection technology frameworks. Then, we discuss the development of several related sub-fields and focus on researching four mainstream deepfake fields: popular face swap, face reenactment, talking face generation, and facial attribute editing, as well as foreign detection. Subsequently, we comprehensively benchmark representative methods on popular datasets for each field, fully evaluating the latest and influential works published in top conferences/journals. Finally, we analyze the challenges and future research directions of the discussed fields. We closely follow the latest developments in <a href="https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection">https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection</a>. </p><p><a href="http://arxiv.org/abs/2403.17881v2">PDF</a> </p><p><strong>Summary</strong><br>æ·±åº¦ä¼ªé€ ç”ŸæˆæŠ€æœ¯ä¸æ–­å‘å±•ï¼Œç›¸åº”æ£€æµ‹æŠ€æœ¯éœ€ä¸æ–­æ¼”è¿›ï¼Œè§„åˆ¶æ·±åº¦ä¼ªé€ åœ¨éšç§ä¾µçŠ¯ã€ç½‘ç»œé’“é±¼ç­‰é¢†åŸŸçš„æ»¥ç”¨ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ç»Ÿä¸€ä»»åŠ¡å®šä¹‰ï¼Œå…¨é¢ä»‹ç»æ•°æ®é›†å’ŒæŒ‡æ ‡ï¼Œè®¨è®ºç”Ÿæˆå’Œæ£€æµ‹æŠ€æœ¯æ¡†æ¶å‘å±•ã€‚</li><li>æ¢è®¨å¤šä¸ªç›¸å…³å­é¢†åŸŸçš„è¿›å±•ï¼Œé‡ç‚¹ç ”ç©¶å››å¤§ä¸»æµæ·±åº¦ä¼ªé€ é¢†åŸŸï¼šæ¢è„¸ã€äººè„¸é‡ç°ã€è¯´è¯äººè„¸ç”Ÿæˆã€é¢éƒ¨å±æ€§ç¼–è¾‘ï¼Œä»¥åŠå¯¹æŠ—æ£€æµ‹ã€‚</li><li>å¯¹æ¯ä¸ªé¢†åŸŸçš„ä»£è¡¨æ€§æ–¹æ³•åœ¨æµè¡Œæ•°æ®é›†ä¸Šè¿›è¡Œå…¨é¢åŸºå‡†æµ‹è¯•ï¼Œå……åˆ†è¯„ä¼°é¡¶çº§ä¼šè®®/æœŸåˆŠä¸­å‘è¡¨çš„æœ€æ–°ä¸”æœ‰å½±å“åŠ›çš„æˆæœã€‚</li><li>åˆ†æè®¨è®ºé¢†åŸŸæŒ‘æˆ˜å’Œæœªæ¥ç ”ç©¶æ–¹å‘ã€‚</li><li>ç´§è·Ÿ <a href="https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection">https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection</a> ä¸­çš„æœ€æ–°è¿›å±•ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šæ·±åº¦ä¼ªé€ ç”Ÿæˆä¸æ£€æµ‹ï¼šåŸºå‡†ä¸ç»¼è¿°</li><li>ä½œè€…ï¼šGan Peiã€Jiangning Zhangã€Menghan Huã€Zhenyu Zhangã€Chengjie Wangã€Yunsheng Wuã€Guangtao Zhaiã€Jian Yangã€Chunhua Shenã€Dacheng Tao</li><li>å•ä½ï¼šåä¸œå¸ˆèŒƒå¤§å­¦å¤šç»´ä¿¡æ¯å¤„ç†ä¸Šæµ·å¸‚é‡ç‚¹å®éªŒå®¤</li><li>å…³é”®è¯ï¼šæ·±åº¦ä¼ªé€ ç”Ÿæˆï¼Œäººè„¸æ›¿æ¢ï¼Œäººè„¸é‡æ¼”ï¼Œè¯´è¯äººè„¸ç”Ÿæˆï¼Œäººè„¸å±æ€§ç¼–è¾‘ï¼Œå¤–æ¥æ£€æµ‹ï¼Œç»¼è¿°</li><li>é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šæ·±åº¦ä¼ªé€ æŠ€æœ¯èƒ½å¤Ÿç”Ÿæˆé«˜åº¦é€¼çœŸçš„é¢éƒ¨å›¾åƒå’Œè§†é¢‘ï¼Œåœ¨å¨±ä¹ã€ç”µå½±åˆ¶ä½œã€æ•°å­—äººç‰©åˆ›ä½œç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚éšç€æ·±åº¦å­¦ä¹ æŠ€æœ¯çš„è¿›æ­¥ï¼Œä»¥å˜åˆ†è‡ªç¼–ç å™¨å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œä¸ºä»£è¡¨çš„æŠ€æœ¯å–å¾—äº†ä»¤äººç©ç›®çš„ç”Ÿæˆæ•ˆæœã€‚è¿‘å¹´æ¥ï¼Œå…·æœ‰å¼ºå¤§ç”Ÿæˆèƒ½åŠ›çš„æ‰©æ•£æ¨¡å‹çš„å‡ºç°ï¼Œå¼•å‘äº†æ–°ä¸€è½®çš„ç ”ç©¶çƒ­æ½®ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šæ—©æœŸæ–¹æ³•ä¸»è¦é‡‡ç”¨å˜åˆ†è‡ªç¼–ç å™¨å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œæŠ€æœ¯ï¼Œèƒ½å¤Ÿç”Ÿæˆçœ‹ä¼¼é€¼çœŸçš„å›¾åƒï¼Œä½†å…¶æ€§èƒ½ä»ä¸å°½å¦‚äººæ„ï¼Œé™åˆ¶äº†å®é™…åº”ç”¨ã€‚æœ¬æ–‡çš„åŠ¨æœºå¾ˆå……åˆ†ï¼Œæ—¨åœ¨é€šè¿‡ç»¼è¿°æ·±åº¦ä¼ªé€ ç”Ÿæˆå’Œæ£€æµ‹çš„æœ€æ–°è¿›å±•ï¼Œæ€»ç»“å’Œåˆ†æè¿™ä¸€å¿«é€Ÿå‘å±•é¢†åŸŸçš„å½“å‰æŠ€æœ¯æ°´å¹³ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡é¦–å…ˆç»Ÿä¸€äº†ä»»åŠ¡å®šä¹‰ï¼Œå…¨é¢ä»‹ç»äº†æ•°æ®é›†å’Œè¯„ä»·æŒ‡æ ‡ï¼Œå¹¶è®¨è®ºäº†å‘å±•æŠ€æœ¯ã€‚ç„¶åï¼Œè®¨è®ºäº†å‡ ä¸ªç›¸å…³å­é¢†åŸŸçš„è¿›å±•ï¼Œé‡ç‚¹ç ”ç©¶äº†å››ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„æ·±åº¦ä¼ªé€ é¢†åŸŸï¼šäººè„¸æ›¿æ¢ã€äººè„¸é‡æ¼”ã€è¯´è¯äººè„¸ç”Ÿæˆå’Œäººè„¸å±æ€§ç¼–è¾‘ï¼Œä»¥åŠå¤–æ¥æ£€æµ‹ã€‚éšåï¼Œå¯¹æ¯ä¸ªé¢†åŸŸä¸­æµè¡Œæ•°æ®é›†ä¸Šçš„ä»£è¡¨æ€§æ–¹æ³•è¿›è¡Œäº†å…¨é¢åŸºå‡†æµ‹è¯•ï¼Œå…¨é¢è¯„ä¼°äº†æœ€æ–°å’Œæœ‰å½±å“åŠ›çš„å·²å‘è¡¨ä½œå“ã€‚æœ€åï¼Œåˆ†æäº†æ‰€è®¨è®ºé¢†åŸŸçš„æŒ‘æˆ˜å’Œæœªæ¥ç ”ç©¶æ–¹å‘ã€‚ï¼ˆ4ï¼‰ä»»åŠ¡å’Œæ€§èƒ½ï¼šæœ¬æ–‡åœ¨äººè„¸æ›¿æ¢ã€äººè„¸é‡æ¼”ã€è¯´è¯äººè„¸ç”Ÿæˆã€äººè„¸å±æ€§ç¼–è¾‘å’Œå¤–æ¥æ£€æµ‹ä»»åŠ¡ä¸Šè¿›è¡Œäº†å…¨é¢åŸºå‡†æµ‹è¯•ï¼Œå¯¹æ¯ä¸ªé¢†åŸŸä¸­æµè¡Œæ•°æ®é›†ä¸Šçš„ä»£è¡¨æ€§æ–¹æ³•è¿›è¡Œäº†è¯„ä¼°ã€‚è¿™äº›æ–¹æ³•åœ¨å„ä¸ªä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¯æ˜äº†å®ƒä»¬åœ¨ç”Ÿæˆå’Œæ£€æµ‹æ·±åº¦ä¼ªé€ æ–¹é¢çš„é«˜æ•ˆæ€§ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†ä½œè€…çš„ç›®æ ‡ï¼Œå³æä¾›æ·±åº¦ä¼ªé€ ç”Ÿæˆå’Œæ£€æµ‹é¢†åŸŸçš„å…¨é¢æ¦‚è¿°ã€‚</li></ol><p>7.æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼šç»Ÿä¸€ä»»åŠ¡å®šä¹‰ï¼Œå…¨é¢ä»‹ç»æ•°æ®é›†å’Œè¯„ä»·æŒ‡æ ‡ï¼Œè®¨è®ºå‘å±•æŠ€æœ¯ï¼›ï¼ˆ2ï¼‰ï¼šè®¨è®ºäººè„¸æ›¿æ¢ã€äººè„¸é‡æ¼”ã€è¯´è¯äººè„¸ç”Ÿæˆå’Œäººè„¸å±æ€§ç¼–è¾‘å››ä¸ªæ·±åº¦ä¼ªé€ é¢†åŸŸè¿›å±•ï¼›ï¼ˆ3ï¼‰ï¼šå¯¹æ¯ä¸ªé¢†åŸŸä»£è¡¨æ€§æ–¹æ³•è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°æœ€æ–°å‘è¡¨ä½œå“ï¼›ï¼ˆ4ï¼‰ï¼šåˆ†ææŒ‘æˆ˜å’Œæœªæ¥ç ”ç©¶æ–¹å‘ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬ç»¼è¿°å…¨é¢å›é¡¾äº†æ·±åº¦ä¼ªé€ ç”Ÿæˆå’Œæ£€æµ‹é¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼Œé¦–æ¬¡å…¨é¢æ¶µç›–äº†ç›¸å…³é¢†åŸŸï¼Œå¹¶è®¨è®ºäº†æ‰©æ•£ç­‰æœ€æ–°æŠ€æœ¯ã€‚å…·ä½“è€Œè¨€ï¼Œæœ¬æ–‡æ¶µç›–äº†åŸºæœ¬èƒŒæ™¯çŸ¥è¯†çš„æ¦‚è¿°ï¼ŒåŒ…æ‹¬ç ”ç©¶ä»»åŠ¡çš„æ¦‚å¿µã€ç”Ÿæˆæ¨¡å‹å’Œç¥ç»ç½‘ç»œçš„å‘å±•ä»¥åŠå…¶ä»–æ¥è‡ªå¯†åˆ‡ç›¸å…³é¢†åŸŸçš„ä¿¡æ¯ã€‚éšåï¼Œæˆ‘ä»¬æ€»ç»“äº†ä¸»æµçš„å››ä¸ªç”Ÿæˆå’Œä¸€ä¸ªæ£€æµ‹é¢†åŸŸçš„ä¸åŒæ–¹æ³•é‡‡ç”¨çš„æŠ€æœ¯æ–¹æ³•ï¼Œå¹¶ä»æŠ€æœ¯è§’åº¦å¯¹æ–¹æ³•è¿›è¡Œåˆ†ç±»å’Œè®¨è®ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åŠ›æ±‚å…¬å¹³åœ°ç»„ç»‡å’Œæ ‡æ³¨æ¯ä¸ªé¢†åŸŸä¸­çš„ä»£è¡¨æ€§æ–¹æ³•ã€‚æœ€åï¼Œæˆ‘ä»¬æ€»ç»“äº†æ¯ä¸ªé¢†åŸŸçš„å½“å‰æŒ‘æˆ˜å’Œæœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼š</li><li>å…¨é¢è¦†ç›–æ·±åº¦ä¼ªé€ ç”Ÿæˆå’Œæ£€æµ‹é¢†åŸŸï¼ŒåŒ…æ‹¬äººè„¸æ›¿æ¢ã€äººè„¸é‡æ¼”ã€è¯´è¯äººè„¸ç”Ÿæˆã€äººè„¸å±æ€§ç¼–è¾‘å’Œå¤–æ¥æ£€æµ‹ã€‚</li><li>ç»Ÿä¸€ä»»åŠ¡å®šä¹‰ï¼Œå…¨é¢ä»‹ç»æ•°æ®é›†å’Œè¯„ä»·æŒ‡æ ‡ï¼Œè®¨è®ºå‘å±•æŠ€æœ¯ã€‚</li><li>å¯¹æ¯ä¸ªé¢†åŸŸä»£è¡¨æ€§æ–¹æ³•è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°æœ€æ–°å‘è¡¨ä½œå“ã€‚</li><li>åˆ†ææŒ‘æˆ˜å’Œæœªæ¥ç ”ç©¶æ–¹å‘ã€‚</li><li>æ€§èƒ½ï¼š</li><li>åœ¨äººè„¸æ›¿æ¢ã€äººè„¸é‡æ¼”ã€è¯´è¯äººè„¸ç”Ÿæˆã€äººè„¸å±æ€§ç¼–è¾‘å’Œå¤–æ¥æ£€æµ‹ä»»åŠ¡ä¸Šè¿›è¡Œäº†å…¨é¢åŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°äº†æ¯ä¸ªé¢†åŸŸæµè¡Œæ•°æ®é›†ä¸Šçš„ä»£è¡¨æ€§æ–¹æ³•ã€‚</li><li>è¿™äº›æ–¹æ³•åœ¨å„ä¸ªä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¯æ˜äº†å®ƒä»¬åœ¨ç”Ÿæˆå’Œæ£€æµ‹æ·±åº¦ä¼ªé€ æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</li><li>è¿™äº›æ€§èƒ½æ”¯æŒäº†ä½œè€…çš„ç›®æ ‡ï¼Œå³æä¾›æ·±åº¦ä¼ªé€ ç”Ÿæˆå’Œæ£€æµ‹é¢†åŸŸå…¨é¢æ¦‚è¿°ã€‚</li><li>å·¥ä½œé‡ï¼š</li><li>å¤§é‡çš„å·¥ä½œé‡ï¼Œéœ€è¦å¯¹æ·±åº¦ä¼ªé€ ç”Ÿæˆå’Œæ£€æµ‹é¢†åŸŸçš„å¹¿æ³›æ–‡çŒ®è¿›è¡Œå…¨é¢å®¡æŸ¥å’Œåˆ†æã€‚</li><li>éœ€è¦å¯¹ç›¸å…³æŠ€æœ¯ï¼ŒåŒ…æ‹¬å˜åˆ†è‡ªç¼–ç å™¨ã€ç”Ÿæˆå¯¹æŠ—ç½‘ç»œã€æ‰©æ•£æ¨¡å‹å’Œå¤–æ¥æ£€æµ‹æ–¹æ³•è¿›è¡Œæ·±å…¥ç†è§£ã€‚</li><li>éœ€è¦ä»”ç»†è®¾è®¡å’Œæ‰§è¡ŒåŸºå‡†æµ‹è¯•ï¼Œä»¥å…¬å¹³è¯„ä¼°ä¸åŒæ–¹æ³•çš„æ€§èƒ½ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-6aeb35b9b32deab9d1d23aa9b1eea276.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b4cf83bab5fd31096f8d73dfc31c29e2.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-25e200804e3a12a1413b7bb204b5140d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f7724b1a6d114dcf338b21d91980680f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7308534a9cb3137f16881c6b4c39ae70.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-37bc450ea15d85f35b70da807b592dbc.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-04-14  Deepfake Generation and Detection A Benchmark and Survey</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/04/14/Paper/2024-04-14/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/04/14/Paper/2024-04-14/Diffusion%20Models/</id>
    <published>2024-04-14T04:03:04.000Z</published>
    <updated>2024-04-14T04:03:04.033Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-04-14-æ›´æ–°"><a href="#2024-04-14-æ›´æ–°" class="headerlink" title="2024-04-14 æ›´æ–°"></a>2024-04-14 æ›´æ–°</h1><h2 id="Taming-Stable-Diffusion-for-Text-to-360Â°-Panorama-Image-Generation"><a href="#Taming-Stable-Diffusion-for-Text-to-360Â°-Panorama-Image-Generation" class="headerlink" title="Taming Stable Diffusion for Text to 360Â° Panorama Image Generation"></a>Taming Stable Diffusion for Text to 360Â° Panorama Image Generation</h2><p><strong>Authors:Cheng Zhang, Qianyi Wu, Camilo Cruz Gambardella, Xiaoshui Huang, Dinh Phung, Wanli Ouyang, Jianfei Cai</strong></p><p>Generative models, e.g., Stable Diffusion, have enabled the creation of photorealistic images from text prompts. Yet, the generation of 360-degree panorama images from text remains a challenge, particularly due to the dearth of paired text-panorama data and the domain gap between panorama and perspective images. In this paper, we introduce a novel dual-branch diffusion model named PanFusion to generate a 360-degree image from a text prompt. We leverage the stable diffusion model as one branch to provide prior knowledge in natural image generation and register it to another panorama branch for holistic image generation. We propose a unique cross-attention mechanism with projection awareness to minimize distortion during the collaborative denoising process. Our experiments validate that PanFusion surpasses existing methods and, thanks to its dual-branch structure, can integrate additional constraints like room layout for customized panorama outputs. Code is available at <a href="https://chengzhag.github.io/publication/panfusion">https://chengzhag.github.io/publication/panfusion</a>. </p><p><a href="http://arxiv.org/abs/2404.07949v1">PDF</a> CVPR 2024. Project Page:   <a href="https://chengzhag.github.io/publication/panfusion">https://chengzhag.github.io/publication/panfusion</a> Code:   <a href="https://github.com/chengzhag/PanFusion">https://github.com/chengzhag/PanFusion</a></p><p><strong>Summary</strong><br>æ–‡æœ¬ä¸»æ—¨æ¦‚æ‹¬ï¼šåŒåˆ†æ”¯æ‰©æ•£æ¨¡å‹PanFusionåœ¨æ–‡æœ¬æç¤ºæŒ‡å¯¼ä¸‹ç”Ÿæˆ360åº¦å…¨æ™¯å›¾åƒã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>åˆ©ç”¨Stable Diffusionæ¨¡å‹çš„è‡ªç„¶å›¾åƒç”Ÿæˆå…ˆéªŒçŸ¥è¯†å’Œå…¨æ™¯å›¾åƒç”Ÿæˆåˆ†æ”¯ï¼Œç”Ÿæˆ360åº¦å…¨æ™¯å›¾åƒã€‚</li><li>å¼•å…¥ç‹¬ç‰¹çš„cross-attentionæœºåˆ¶å’Œprojection awarenessï¼Œä»¥æœ€å°åŒ–åä½œå»å™ªè¿‡ç¨‹ä¸­çš„å¤±çœŸã€‚</li><li>PanFusionè¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œå¹¶ä¸”ç”±äºå…¶åŒåˆ†æ”¯ç»“æ„ï¼Œå¯ä»¥é›†æˆæˆ¿é—´å¸ƒå±€ç­‰å…¶ä»–çº¦æŸï¼Œä»¥è·å¾—å®šåˆ¶çš„å…¨æ™¯è¾“å‡ºã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šé©¯æœç¨³å®šæ‰©æ•£ä»¥å®ç°æ–‡æœ¬åˆ° 360 åº¦å›¾åƒç”Ÿæˆï¼ˆä¸­æ–‡ç¿»è¯‘ï¼‰</li><li>ä½œè€…ï¼šCheng Zhangã€Kai Zhangã€Ya Zhangã€Zhenyu Wangã€Zhaopeng Cuiã€Yanwei Fu</li><li>æ‰€å±å•ä½ï¼šé¦™æ¸¯ä¸­æ–‡å¤§å­¦ï¼ˆæ·±åœ³ï¼‰ï¼ˆä»…è¾“å‡ºä¸­æ–‡ç¿»è¯‘ï¼‰</li><li>å…³é”®è¯ï¼šæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€360 åº¦å…¨æ™¯å›¾åƒã€ç¨³å®šæ‰©æ•£</li><li>è®ºæ–‡é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šç”Ÿæˆæ¨¡å‹ï¼ˆä¾‹å¦‚ Stable Diffusionï¼‰å·²èƒ½æ ¹æ®æ–‡æœ¬æç¤ºåˆ›å»ºé€¼çœŸçš„å›¾åƒã€‚ç„¶è€Œï¼Œç”±äºæˆå¯¹æ–‡æœ¬å…¨æ™¯æ•°æ®åŒ®ä¹ä»¥åŠå…¨æ™¯å›¾åƒä¸é€è§†å›¾åƒä¹‹é—´çš„åŸŸå·®å¼‚ï¼Œä»æ–‡æœ¬ç”Ÿæˆ 360 åº¦å…¨æ™¯å›¾åƒä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç°æœ‰æ–¹æ³•ï¼ˆä¾‹å¦‚ MVDiffusion å’Œ Text2Lightï¼‰åœ¨ä¸åŒçš„é¢†åŸŸè§£å†³äº†æ–‡æœ¬æ¡ä»¶å›¾åƒç”Ÿæˆé—®é¢˜ã€‚MVDiffusion ç”Ÿæˆ 90Â° è§†åœºçš„ 8 ä¸ªæ°´å¹³è§†å›¾ï¼Œå› æ­¤ä»…é™äºå¯¹é€è§†å›¾åƒè¿›è¡Œè¯„ä¼°ã€‚Text2Light ç”Ÿæˆ 180Â° å‚ç›´è§†åœºï¼Œå› æ­¤ä¸“æ³¨äºè¯„ä¼°å…¨æ™¯è´¨é‡ã€‚ï¼ˆ3ï¼‰æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º PanFusion çš„æ–°å‹åŒåˆ†æ”¯æ‰©æ•£æ¨¡å‹ï¼Œå¯æ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆ 360 åº¦å›¾åƒã€‚æˆ‘ä»¬åˆ©ç”¨ç¨³å®šæ‰©æ•£æ¨¡å‹ä½œä¸ºåˆ†æ”¯ä¹‹ä¸€ï¼Œä¸ºè‡ªç„¶å›¾åƒç”Ÿæˆæä¾›å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶å°†å…¶æ³¨å†Œåˆ°å¦ä¸€ä¸ªå…¨æ™¯åˆ†æ”¯ä»¥è¿›è¡Œæ•´ä½“å›¾åƒç”Ÿæˆã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç‹¬ç‰¹çš„è·¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œå…·æœ‰æŠ•å½±æ„ŸçŸ¥èƒ½åŠ›ï¼Œä»¥åœ¨åä½œå»å™ªè¿‡ç¨‹ä¸­æœ€å¤§ç¨‹åº¦åœ°å‡å°‘å¤±çœŸã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šå®éªŒéªŒè¯äº† PanFusion ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶ä¸”ç”±äºå…¶åŒåˆ†æ”¯ç»“æ„ï¼Œå¯ä»¥é›†æˆé¢å¤–çš„çº¦æŸï¼ˆå¦‚æˆ¿é—´å¸ƒå±€ï¼‰ï¼Œä»¥è·å¾—å®šåˆ¶çš„å…¨æ™¯è¾“å‡ºã€‚</p></li><li><p>æ–¹æ³•ï¼š(1): æå‡ºäº†ä¸€ç§åŒåˆ†æ”¯æ‰©æ•£æ¨¡å‹PanFusionï¼Œå…¶ä¸­Stable Diffusionåˆ†æ”¯æä¾›è‡ªç„¶å›¾åƒå…ˆéªŒçŸ¥è¯†ï¼Œå…¨æ™¯åˆ†æ”¯è´Ÿè´£ç”Ÿæˆ360åº¦å›¾åƒï¼›(2): è®¾è®¡äº†ä¸€ç§è·¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œå…·æœ‰æŠ•å½±æ„ŸçŸ¥èƒ½åŠ›ï¼Œåœ¨åä½œå»å™ªè¿‡ç¨‹ä¸­æœ€å¤§ç¨‹åº¦åœ°å‡å°‘å¤±çœŸï¼›(3): å®éªŒéªŒè¯äº†PanFusionä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶å¯ä»¥é€šè¿‡é›†æˆé¢å¤–çš„çº¦æŸï¼ˆå¦‚æˆ¿é—´å¸ƒå±€ï¼‰æ¥è·å¾—å®šåˆ¶çš„å…¨æ™¯è¾“å‡ºã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–‡æœ¬åˆ° 360Â° å…¨æ™¯å›¾åƒç”Ÿæˆæ–¹æ³• PanFusionï¼Œè¯¥æ–¹æ³•å¯ä»¥ä»å•ä¸ªæ–‡æœ¬æç¤ºç”Ÿæˆé«˜è´¨é‡çš„å…¨æ™¯å›¾åƒã€‚ç‰¹åˆ«æ˜¯ï¼Œå¼•å…¥äº†åŒåˆ†æ”¯æ‰©æ•£æ¶æ„ï¼Œä»¥åˆ©ç”¨ StableDiffusion åœ¨é€è§†åŸŸä¸­çš„å…ˆéªŒçŸ¥è¯†ï¼ŒåŒæ—¶è§£å†³äº†å…ˆå‰å·¥ä½œä¸­è§‚å¯Ÿåˆ°çš„é‡å¤å…ƒç´ å’Œä¸ä¸€è‡´æ€§é—®é¢˜ã€‚è¿›ä¸€æ­¥å¼•å…¥äº† EPPA æ¨¡å—æ¥å¢å¼ºä¸¤ä¸ªåˆ†æ”¯ä¹‹é—´çš„ä¿¡æ¯ä¼ é€’ã€‚æˆ‘ä»¬è¿˜æ‰©å±•äº† PanFusionï¼Œç”¨äºå¸ƒå±€æ¡ä»¶å…¨æ™¯ç”Ÿæˆã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼Œä¸ä»¥å‰çš„æ–¹æ³•ç›¸æ¯”ï¼ŒPanFusion å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„å…¨æ™¯å›¾åƒï¼Œå…·æœ‰æ›´å¥½çš„çœŸå®æ„Ÿå’Œå¸ƒå±€ä¸€è‡´æ€§ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§åŒåˆ†æ”¯æ‰©æ•£æ¨¡å‹ PanFusionï¼Œè¯¥æ¨¡å‹ç»“åˆäº†å…¨æ™¯å’Œé€è§†åŸŸçš„ä¼˜ç‚¹ã€‚</li><li>è®¾è®¡äº†ä¸€ç§è·¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œå…·æœ‰æŠ•å½±æ„ŸçŸ¥èƒ½åŠ›ï¼Œåœ¨åä½œå»å™ªè¿‡ç¨‹ä¸­æœ€å¤§ç¨‹åº¦åœ°å‡å°‘å¤±çœŸã€‚</li><li>æ‰©å±•äº† PanFusionï¼Œç”¨äºå¸ƒå±€æ¡ä»¶å…¨æ™¯ç”Ÿæˆã€‚æ€§èƒ½ï¼š</li><li>PanFusion åœ¨ç”Ÿæˆé«˜è´¨é‡å…¨æ™¯å›¾åƒæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå…·æœ‰æ›´å¥½çš„çœŸå®æ„Ÿå’Œå¸ƒå±€ä¸€è‡´æ€§ã€‚</li><li>PanFusion å¯ä»¥é›†æˆé¢å¤–çš„çº¦æŸï¼ˆå¦‚æˆ¿é—´å¸ƒå±€ï¼‰ï¼Œä»¥è·å¾—å®šåˆ¶çš„å…¨æ™¯è¾“å‡ºã€‚å·¥ä½œé‡ï¼š</li><li>PanFusion çš„åŒåˆ†æ”¯æ¶æ„è™½ç„¶ç»“åˆäº†å…¨æ™¯å’Œé€è§†åŸŸçš„ä¼˜ç‚¹ï¼Œä½†å¸¦æ¥äº†æ›´é«˜çš„è®¡ç®—å¤æ‚åº¦ã€‚</li><li>PanFusion æœ‰æ—¶æ— æ³•ç”Ÿæˆå®¤å†…åœºæ™¯çš„å…¥å£ï¼Œå¦‚å›¾ 7 æ‰€ç¤ºï¼Œè¿™å¯¹äºè™šæ‹Ÿæ¼«æ¸¸ç­‰ç”¨ä¾‹è‡³å…³é‡è¦ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-20c4c5edd8e50849c4f750424d23bde9.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f954ecc0c15fed7c058a21218d6a0e72.jpg" align="middle"><img src="https://pica.zhimg.com/v2-9b4d22a42de1cbd7601f33ba41795f18.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2dc43850a703f33a76c2e3a982c05b5e.jpg" align="middle"></details><h2 id="Joint-Conditional-Diffusion-Model-for-Image-Restoration-with-Mixed-Degradations"><a href="#Joint-Conditional-Diffusion-Model-for-Image-Restoration-with-Mixed-Degradations" class="headerlink" title="Joint Conditional Diffusion Model for Image Restoration with Mixed   Degradations"></a>Joint Conditional Diffusion Model for Image Restoration with Mixed   Degradations</h2><p><strong>Authors:Yufeng Yue, Meng Yu, Luojie Yang, Yi Yang</strong></p><p>Image restoration is rather challenging in adverse weather conditions, especially when multiple degradations occur simultaneously. Blind image decomposition was proposed to tackle this issue, however, its effectiveness heavily relies on the accurate estimation of each component. Although diffusion-based models exhibit strong generative abilities in image restoration tasks, they may generate irrelevant contents when the degraded images are severely corrupted. To address these issues, we leverage physical constraints to guide the whole restoration process, where a mixed degradation model based on atmosphere scattering model is constructed. Then we formulate our Joint Conditional Diffusion Model (JCDM) by incorporating the degraded image and degradation mask to provide precise guidance. To achieve better color and detail recovery results, we further integrate a refinement network to reconstruct the restored image, where Uncertainty Estimation Block (UEB) is employed to enhance the features. Extensive experiments performed on both multi-weather and weather-specific datasets demonstrate the superiority of our method over state-of-the-art competing methods. </p><p><a href="http://arxiv.org/abs/2404.07770v1">PDF</a> </p><p><strong>Summary</strong><br>åœ¨æ¶åŠ£å¤©æ°”æ¡ä»¶ä¸‹ï¼Œå›¾åƒä¿®å¤éš¾åº¦è¾ƒå¤§ï¼Œå°¤å…¶æ˜¯å½“å¤šç§é€€åŒ–åŒæ—¶å‘ç”Ÿæ—¶ã€‚ç›²å›¾åˆ†è§£è¢«æå‡ºä»¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†æ˜¯å®ƒçš„æœ‰æ•ˆæ€§å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºæ¯ä¸ªåˆ†é‡çš„å‡†ç¡®ä¼°è®¡ã€‚è™½ç„¶åŸºäºæ‰©æ•£çš„æ¨¡å‹åœ¨å›¾åƒä¿®å¤ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ï¼Œä½†å½“é€€åŒ–çš„å›¾åƒä¸¥é‡æŸåæ—¶ï¼Œå®ƒä»¬å¯èƒ½ä¼šç”Ÿæˆæ— å…³çš„å†…å®¹ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬åˆ©ç”¨ç‰©ç†çº¦æŸæ¥æŒ‡å¯¼æ•´ä¸ªä¿®å¤è¿‡ç¨‹ï¼Œå…¶ä¸­æ„å»ºäº†ä¸€ä¸ªåŸºäºå¤§æ°”æ•£å°„æ¨¡å‹çš„æ··åˆé€€åŒ–æ¨¡å‹ã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡å°†é€€åŒ–çš„å›¾åƒå’Œé€€åŒ–è’™ç‰ˆç»“åˆèµ·æ¥ï¼Œæ„é€ äº†æˆ‘ä»¬çš„è”åˆæ¡ä»¶æ‰©æ•£æ¨¡å‹ (JCDM) ä»¥æä¾›ç²¾ç¡®çš„æŒ‡å¯¼ã€‚ä¸ºäº†è·å¾—æ›´å¥½çš„é¢œè‰²å’Œç»†èŠ‚æ¢å¤ç»“æœï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æ•´åˆäº†ä¸€ä¸ªç»†åŒ–ç½‘ç»œæ¥é‡å»ºæ¢å¤çš„å›¾åƒï¼Œå…¶ä¸­ä½¿ç”¨ä¸ç¡®å®šæ€§ä¼°è®¡å— (UEB) æ¥å¢å¼ºç‰¹å¾ã€‚åœ¨å¤šå¤©æ°”å’Œç‰¹å®šå¤©æ°”æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºæœ€å…ˆè¿›çš„ç«äº‰æ–¹æ³•ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>åˆ©ç”¨ç‰©ç†çº¦æŸå»ºç«‹æ··åˆé€€åŒ–æ¨¡å‹ï¼Œå¼•å¯¼å›¾åƒä¿®å¤ã€‚</li><li>æå‡ºè”åˆæ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼ˆJCDMï¼‰ï¼Œé€šè¿‡é€€åŒ–å›¾åƒå’Œè’™ç‰ˆæä¾›ç²¾ç¡®æŒ‡å¯¼ã€‚</li><li>åŠ å…¥ç»†åŒ–ç½‘ç»œï¼Œæå‡é¢œè‰²å’Œç»†èŠ‚æ¢å¤æ•ˆæœã€‚</li><li>ä½¿ç”¨ä¸ç¡®å®šæ€§ä¼°è®¡å—ï¼ˆUEBï¼‰å¢å¼ºç‰¹å¾ï¼Œæ”¹å–„å›¾åƒé‡å»ºè´¨é‡ã€‚</li><li>åœ¨å¤šå¤©æ°”å’Œç‰¹å®šå¤©æ°”æ•°æ®é›†ä¸ŠéªŒè¯æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚</li><li>JCDMåœ¨å›¾åƒä¿®å¤ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¾ˆå¼ºçš„ç”Ÿæˆèƒ½åŠ›ã€‚</li><li>ç»†åŒ–ç½‘ç»œæœ‰åŠ©äºæé«˜å›¾åƒä¿®å¤çš„è‰²å½©å’Œç»†èŠ‚è¡¨ç°ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šæ··åˆé€€åŒ–å›¾åƒè”åˆæ¡ä»¶æ‰©æ•£æ¨¡å‹</li><li>ä½œè€…ï¼šå²³é›¨å³°ï¼ŒäºèŒï¼Œæ¨ç½—æ°ï¼Œæ¨æ¯…</li><li>å•ä½ï¼šåŒ—äº¬ç†å·¥å¤§å­¦è‡ªåŠ¨åŒ–å­¦é™¢</li><li>å…³é”®è¯ï¼šå»å™ªæ‰©æ•£æ¨¡å‹ï¼Œç›²å›¾åƒå¤åŸï¼Œæ··åˆé€€åŒ–ï¼Œä½å±‚æ¬¡è§†è§‰</li><li>è®ºæ–‡é“¾æ¥ï¼šNoneï¼ŒGithubä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šå›¾åƒå¤åŸåœ¨æ¶åŠ£å¤©æ°”æ¡ä»¶ä¸‹é¢‡å…·æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯å½“å¤šç§é€€åŒ–åŒæ—¶å‘ç”Ÿæ—¶ã€‚ç›²å›¾åƒåˆ†è§£è¢«æå‡ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç„¶è€Œå…¶æœ‰æ•ˆæ€§ä¸¥é‡ä¾èµ–äºæ¯ä¸ªåˆ†é‡çš„å‡†ç¡®ä¼°è®¡ã€‚åŸºäºæ‰©æ•£çš„æ¨¡å‹è™½ç„¶åœ¨å›¾åƒå¤åŸä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ï¼Œä½†å½“é€€åŒ–å›¾åƒä¸¥é‡æŸåæ—¶ï¼Œå®ƒä»¬å¯èƒ½ä¼šç”Ÿæˆæ— å…³çš„å†…å®¹ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬åˆ©ç”¨ç‰©ç†çº¦æŸæ¥æŒ‡å¯¼æ•´ä¸ªå¤åŸè¿‡ç¨‹ï¼Œå…¶ä¸­æ„å»ºäº†ä¸€ä¸ªåŸºäºå¤§æ°”æ•£å°„æ¨¡å‹çš„æ··åˆé€€åŒ–æ¨¡å‹ã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡ç»“åˆé€€åŒ–å›¾åƒå’Œé€€åŒ–æ©ç æ¥åˆ¶å®šè”åˆæ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼ˆJCDMï¼‰ï¼Œä»¥æä¾›ç²¾ç¡®çš„æŒ‡å¯¼ã€‚ä¸ºäº†è·å¾—æ›´å¥½çš„é¢œè‰²å’Œç»†èŠ‚æ¢å¤ç»“æœï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥é›†æˆäº†ä¸€ä¸ªç²¾åŒ–ç½‘ç»œæ¥é‡å»ºæ¢å¤çš„å›¾åƒï¼Œå…¶ä¸­é‡‡ç”¨ä¸ç¡®å®šæ€§ä¼°è®¡å—ï¼ˆUEBï¼‰æ¥å¢å¼ºç‰¹å¾ã€‚ï¼ˆ3ï¼‰æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šåœ¨å¤šå¤©æ°”å’Œç‰¹å®šå¤©æ°”æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºæœ€å…ˆè¿›çš„ç«äº‰æ–¹æ³•ã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†ä»€ä¹ˆæ€§èƒ½ï¼šè¯¥æ–¹æ³•åœ¨å›¾åƒå¤åŸä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œåœ¨å¤šå¤©æ°”å’Œç‰¹å®šå¤©æ°”æ•°æ®é›†ä¸Šå‡ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†ä»–ä»¬æå‡ºçš„ç›®æ ‡ï¼Œå³å¼€å‘èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å¤æ‚ä¸”å¤šæ ·åŒ–çš„é€€åŒ–åœºæ™¯çš„æŠ€æœ¯ï¼Œè€Œæ— éœ€æ˜ç¡®è¯†åˆ«æˆ–åˆ†ç¦»å„ä¸ªé€€åŒ–åˆ†é‡ã€‚</li></ol><p>7.Methodsï¼š(1)æ„å»ºåŸºäºå¤§æ°”æ•£å°„æ¨¡å‹çš„æ··åˆé€€åŒ–æ¨¡å‹ï¼Œåˆ»ç”»å¤šç§é€€åŒ–åŒæ—¶å‘ç”Ÿçš„åœºæ™¯ï¼›(2)æå‡ºè”åˆæ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼ˆJCDMï¼‰ï¼Œåˆ©ç”¨é€€åŒ–å›¾åƒå’Œé€€åŒ–æ©ç æä¾›ç²¾ç¡®æŒ‡å¯¼ï¼›(3)é›†æˆç²¾åŒ–ç½‘ç»œï¼Œé‡‡ç”¨ä¸ç¡®å®šæ€§ä¼°è®¡å—ï¼ˆUEBï¼‰å¢å¼ºç‰¹å¾ï¼Œæå‡é¢œè‰²å’Œç»†èŠ‚æ¢å¤æ•ˆæœã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡é’ˆå¯¹æ¶åŠ£å¤©æ°”æ¡ä»¶ä¸‹çš„å›¾åƒå¤åŸé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„å›¾åƒå¤åŸæ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†é€€åŒ–å›¾åƒå’Œé€€åŒ–æ©ç ä½œä¸ºæ¡ä»¶ä¿¡æ¯ï¼Œèƒ½å¤Ÿæ›´å…·é’ˆå¯¹æ€§å’Œè‡ªé€‚åº”åœ°è¿›è¡Œå¤åŸï¼Œä»è€Œæé«˜äº†å›¾åƒè´¨é‡å’Œå‡†ç¡®åº¦ã€‚æ­¤å¤–ï¼Œè¿˜é›†æˆäº†ä¸€ä¸ªç²¾åŒ–ç½‘ç»œï¼Œä»¥å¢å¼ºåˆå§‹å¤åŸç»“æœã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤æ‚åœºæ™¯ä¸­å°¤å…¶å…·æœ‰æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æœªæ¥ç ”ç©¶å·¥ä½œå¯ä»¥é›†ä¸­åœ¨ä¼˜åŒ–æ‰©æ•£è¿‡ç¨‹ï¼Œä»¥åœ¨æœ‰æ•ˆå»é™¤é€€åŒ–çš„åŒæ—¶æ›´å¥½åœ°ä¿ç•™è¯­ä¹‰ç»†èŠ‚ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºè”åˆæ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼ˆJCDMï¼‰ï¼Œåˆ©ç”¨é€€åŒ–å›¾åƒå’Œé€€åŒ–æ©ç æä¾›ç²¾ç¡®æŒ‡å¯¼ã€‚é›†æˆç²¾åŒ–ç½‘ç»œï¼Œé‡‡ç”¨ä¸ç¡®å®šæ€§ä¼°è®¡å—ï¼ˆUEBï¼‰å¢å¼ºç‰¹å¾ï¼Œæå‡é¢œè‰²å’Œç»†èŠ‚æ¢å¤æ•ˆæœã€‚æ€§èƒ½ï¼šåœ¨å¤šå¤©æ°”å’Œç‰¹å®šå¤©æ°”æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒå¤åŸä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚å·¥ä½œé‡ï¼šè¯¥æ–¹æ³•çš„å®ç°ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦æ„å»ºæ··åˆé€€åŒ–æ¨¡å‹ã€è”åˆæ¡ä»¶æ‰©æ•£æ¨¡å‹å’Œç²¾åŒ–ç½‘ç»œã€‚ç„¶è€Œï¼Œå…¶å‡ºè‰²çš„æ€§èƒ½ä½¿å…¶åœ¨å®é™…åº”ç”¨ä¸­å…·æœ‰è¾ƒé«˜çš„ä»·å€¼ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-2c2870376f78214015b071151083a700.jpg" align="middle"><img src="https://picx.zhimg.com/v2-80fa9d98a47a8b98d09bc356d597890a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6cde31d1894dbb88eb8b6b56e5977932.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3337c693a286a77dd59323c2cfb48d03.jpg" align="middle"></details><h2 id="Applying-Guidance-in-a-Limited-Interval-Improves-Sample-and-Distribution-Quality-in-Diffusion-Models"><a href="#Applying-Guidance-in-a-Limited-Interval-Improves-Sample-and-Distribution-Quality-in-Diffusion-Models" class="headerlink" title="Applying Guidance in a Limited Interval Improves Sample and Distribution   Quality in Diffusion Models"></a>Applying Guidance in a Limited Interval Improves Sample and Distribution   Quality in Diffusion Models</h2><p><strong>Authors:Tuomas KynkÃ¤Ã¤nniemi, Miika Aittala, Tero Karras, Samuli Laine, Timo Aila, Jaakko Lehtinen</strong></p><p>Guidance is a crucial technique for extracting the best performance out of image-generating diffusion models. Traditionally, a constant guidance weight has been applied throughout the sampling chain of an image. We show that guidance is clearly harmful toward the beginning of the chain (high noise levels), largely unnecessary toward the end (low noise levels), and only beneficial in the middle. We thus restrict it to a specific range of noise levels, improving both the inference speed and result quality. This limited guidance interval improves the record FID in ImageNet-512 significantly, from 1.81 to 1.40. We show that it is quantitatively and qualitatively beneficial across different sampler parameters, network architectures, and datasets, including the large-scale setting of Stable Diffusion XL. We thus suggest exposing the guidance interval as a hyperparameter in all diffusion models that use guidance. </p><p><a href="http://arxiv.org/abs/2404.07724v1">PDF</a> </p><p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹ä¸­çš„å¼•å¯¼åœ¨å›¾åƒç”Ÿæˆçš„è¿‡ç¨‹ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œä½†ä¼ ç»Ÿä¸Šåªåœ¨æ•´ä¸ªå›¾åƒé‡‡æ ·çš„è¿‡ç¨‹ä¸­ä½¿ç”¨æ’å®šçš„å¼•å¯¼æƒé‡ï¼Œè€Œæœ€æ–°çš„ç ”ç©¶è¡¨æ˜ï¼Œåœ¨é‡‡æ ·çš„å¼€å§‹å’Œç»“æŸé˜¶æ®µï¼Œå¼•å¯¼æ˜¯æœ‰å®³æˆ–å¤šä½™çš„ï¼Œä»…åœ¨ä¸­é—´é˜¶æ®µæ‰æ˜¯æœ‰ç›Šçš„ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ä¼ ç»Ÿçš„è¿ç»­å¼•å¯¼ç­–ç•¥æ˜¯æ— æ•ˆçš„ï¼Œåœ¨é‡‡æ ·é“¾çš„å¼€å§‹é˜¶æ®µæœ‰å®³ï¼Œç»“æŸé˜¶æ®µå¤šä½™ï¼Œä»…åœ¨ä¸­é—´é˜¶æ®µæœ‰ç›Šã€‚</li><li>é™åˆ¶å¼•å¯¼åŒºé—´å¯æé«˜æ¨ç†é€Ÿåº¦å’Œç»“æœè´¨é‡ã€‚</li><li>å—é™å¼•å¯¼åŒºé—´å°† ImageNet-512 ä¸Šçš„ FID ä» 1.81 æ˜¾ç€æé«˜åˆ° 1.40ã€‚</li><li>è¯¥ç­–ç•¥åœ¨ä¸åŒçš„é‡‡æ ·å™¨å‚æ•°ã€ç½‘ç»œæ¶æ„å’Œæ•°æ®é›†ä¸Šéƒ½è¡¨ç°å‡ºå®šé‡å’Œå®šæ€§çš„ä¼˜åŠ¿ï¼ŒåŒ…æ‹¬å¤§è§„æ¨¡çš„ Stable Diffusion XLã€‚</li><li>å»ºè®®åœ¨æ‰€æœ‰ä½¿ç”¨å¼•å¯¼çš„æ‰©æ•£æ¨¡å‹ä¸­å°†å¼•å¯¼åŒºé—´ä½œä¸ºä¸€ä¸ªè¶…å‚æ•°å…¬å¼€ã€‚</li><li>ä½œè€…é€šè¿‡åœ¨é‡‡æ ·è¿‡ç¨‹ä¸­é™åˆ¶å¼•å¯¼çš„åº”ç”¨èŒƒå›´ï¼Œæé«˜äº†å›¾åƒç”Ÿæˆæ¨¡å‹çš„æ€§èƒ½ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>1.æ ‡é¢˜ï¼šåœ¨æœ‰é™åŒºé—´å†…åº”ç”¨æŒ‡å¯¼å¯æå‡æ‰©æ•£æ¨¡å‹ä¸­çš„é‡‡æ ·å’Œåˆ†å¸ƒè´¨é‡ï¼ˆä¸­æ–‡ç¿»è¯‘ï¼šåœ¨æœ‰é™åŒºé—´å†…åº”ç”¨æŒ‡å¯¼å¯æå‡æ‰©æ•£æ¨¡å‹ä¸­çš„é‡‡æ ·å’Œåˆ†å¸ƒè´¨é‡ï¼‰2.ä½œè€…ï¼šTuomas KynkÃ¤Ã¤nniemiã€Miika Aittalaã€Tero Karrasã€Samuli Laineã€Timo Ailaã€Jaakko Lehtinen3.ç¬¬ä¸€ä½œè€…å•ä½ï¼šAalto Universityï¼ˆä¸­æ–‡ç¿»è¯‘ï¼šé˜¿å°”æ‰˜å¤§å­¦ï¼‰4.å…³é”®è¯ï¼šDiffusion Modelsã€Guidanceã€Image Synthesisã€Sampling5.è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.07724Githubä»£ç é“¾æ¥ï¼šæ— 6.æ‘˜è¦ï¼šï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸå–å¾—äº†å·¨å¤§è¿›æ­¥ï¼ŒæŒ‡å¯¼æŠ€æœ¯æ˜¯å…¶ä¸­ä¸€é¡¹å…³é”®æŠ€æœ¯ã€‚ä¼ ç»Ÿä¸Šï¼Œåœ¨å›¾åƒé‡‡æ ·é“¾çš„æ•´ä¸ªè¿‡ç¨‹ä¸­éƒ½ä¼šåº”ç”¨æ’å®šçš„æŒ‡å¯¼æƒé‡ã€‚ï¼ˆ2ï¼‰ï¼šè¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šè¿‡å»çš„æ–¹æ³•å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼šæŒ‡å¯¼åœ¨é‡‡æ ·é“¾å¼€å§‹æ—¶ï¼ˆé«˜å™ªå£°æ°´å¹³ï¼‰æ˜æ˜¾æœ‰å®³ï¼Œåœ¨ç»“æŸæ—¶ï¼ˆä½å™ªå£°æ°´å¹³ï¼‰åŸºæœ¬æ— å¿…è¦ï¼Œä»…åœ¨ä¸­é—´é˜¶æ®µæœ‰ç›Šã€‚ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå°†æŒ‡å¯¼é™åˆ¶åœ¨ç‰¹å®šçš„å™ªå£°æ°´å¹³èŒƒå›´å†…ï¼Œä»è€Œæé«˜äº†æ¨ç†é€Ÿåº¦å’Œç»“æœè´¨é‡ã€‚ï¼ˆ4ï¼‰ï¼šæ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šè¯¥æ–¹æ³•åœ¨ ImageNet-512 æ•°æ®é›†ä¸Šå°† FID ä» 1.81 æ˜¾è‘—æå‡è‡³ 1.40ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚å®ƒåœ¨ä¸åŒçš„é‡‡æ ·å™¨å‚æ•°ã€ç½‘ç»œæ¶æ„å’Œæ•°æ®é›†ä¸Šéƒ½è¡¨ç°å‡ºå®šé‡å’Œå®šæ€§çš„ä¼˜åŠ¿ï¼ŒåŒ…æ‹¬å¤§è§„æ¨¡çš„ Stable Diffusion XLã€‚</p><p>Some Error for method(æ¯”å¦‚æ˜¯ä¸æ˜¯æ²¡æœ‰Methodsè¿™ä¸ªç« èŠ‚)</p><ol><li>ç»“è®ºï¼š(1): æœ¬æ–‡æå‡ºäº†åœ¨ç‰¹å®šå™ªå£°æ°´å¹³èŒƒå›´å†…åº”ç”¨æŒ‡å¯¼çš„æ–°æ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†æ‰©æ•£æ¨¡å‹ä¸­çš„é‡‡æ ·å’Œåˆ†å¸ƒè´¨é‡ï¼Œåœ¨ ImageNet-512 æ•°æ®é›†ä¸Šå°† FID ä» 1.81 æ˜¾è‘—æå‡è‡³ 1.40ã€‚è¯¥æ–¹æ³•åœ¨ä¸åŒçš„é‡‡æ ·å™¨å‚æ•°ã€ç½‘ç»œæ¶æ„å’Œæ•°æ®é›†ä¸Šéƒ½è¡¨ç°å‡ºå®šé‡å’Œå®šæ€§çš„ä¼˜åŠ¿ï¼ŒåŒ…æ‹¬å¤§è§„æ¨¡çš„ StableDiffusionXLã€‚(2): åˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åœ¨ç‰¹å®šå™ªå£°æ°´å¹³èŒƒå›´å†…åº”ç”¨æŒ‡å¯¼çš„æ–°æ–¹æ³•ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•ä¸­æŒ‡å¯¼æƒé‡æ’å®šçš„é—®é¢˜ã€‚æ€§èƒ½ï¼šåœ¨ ImageNet-512 æ•°æ®é›†ä¸Šå°† FID ä» 1.81 æ˜¾è‘—æå‡è‡³ 1.40ï¼Œåœ¨ä¸åŒçš„é‡‡æ ·å™¨å‚æ•°ã€ç½‘ç»œæ¶æ„å’Œæ•°æ®é›†ä¸Šéƒ½è¡¨ç°å‡ºå®šé‡å’Œå®šæ€§çš„ä¼˜åŠ¿ã€‚å·¥ä½œé‡ï¼šè¯¥æ–¹æ³•çš„å®ç°ç›¸å¯¹ç®€å•ï¼Œæ˜“äºé›†æˆåˆ°ç°æœ‰çš„æ‰©æ•£æ¨¡å‹ä¸­ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-97b33bdd19e6d84a81189b39c0d3a191.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3937b9915f757e63ceb909036b736ffe.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c67d4ffcc864f8d6cd7eeb2117645b33.jpg" align="middle"><img src="https://picx.zhimg.com/v2-454b46abf8cd6a58c9c639ee2baec578.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-329285c5cade0afa7a8d3bf806ee9bd0.jpg" align="middle"></details>## Implicit and Explicit Language Guidance for Diffusion-based Visual   Perception**Authors:Hefeng Wang, Jiale Cao, Jin Xie, Aiping Yang, Yanwei Pang**Text-to-image diffusion models have shown powerful ability on conditional image synthesis. With large-scale vision-language pre-training, diffusion models are able to generate high-quality images with rich texture and reasonable structure under different text prompts. However, it is an open problem to adapt the pre-trained diffusion model for visual perception. In this paper, we propose an implicit and explicit language guidance framework for diffusion-based perception, named IEDP. Our IEDP comprises of an implicit language guidance branch and an explicit language guidance branch. The implicit branch employs frozen CLIP image encoder to directly generate implicit text embeddings that are fed to diffusion model, without using explicit text prompts. The explicit branch utilizes the ground-truth labels of corresponding images as text prompts to condition feature extraction of diffusion model. During training, we jointly train diffusion model by sharing the model weights of these two branches. As a result, implicit and explicit branches can jointly guide feature learning. During inference, we only employ implicit branch for final prediction, which does not require any ground-truth labels. Experiments are performed on two typical perception tasks, including semantic segmentation and depth estimation. Our IEDP achieves promising performance on both tasks. For semantic segmentation, our IEDP has the mIoU score of 55.9% on AD20K validation set, which outperforms the baseline method VPD by 2.2%. For depth estimation, our IEDP outperforms the baseline method VPD with a relative gain of 10.2%. [PDF](http://arxiv.org/abs/2404.07600v1) **Summary**æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨æ¡ä»¶å›¾åƒåˆæˆæ–¹é¢å±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚é€šè¿‡å¤§è§„æ¨¡è§†è§‰è¯­è¨€é¢„è®­ç»ƒï¼Œæ‰©æ•£æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸åŒçš„æ–‡æœ¬æç¤ºä¸‹ç”Ÿæˆå…·æœ‰ä¸°å¯Œçº¹ç†å’Œåˆç†ç»“æ„çš„é«˜è´¨é‡å›¾åƒã€‚ç„¶è€Œï¼Œå°†é¢„å…ˆè®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ç”¨äºè§†è§‰æ„ŸçŸ¥æ˜¯ä¸€ä¸ªå¼€æ”¾çš„é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç”¨äºåŸºäºæ‰©æ•£çš„æ„ŸçŸ¥çš„éšå¼å’Œæ˜¾å¼è¯­è¨€æŒ‡å¯¼æ¡†æ¶ï¼Œåä¸º IEDPã€‚æˆ‘ä»¬çš„ IEDP åŒ…å«ä¸€ä¸ªéšå¼è¯­è¨€æŒ‡å¯¼åˆ†æ”¯å’Œä¸€ä¸ªæ˜¾å¼è¯­è¨€æŒ‡å¯¼åˆ†æ”¯ã€‚éšå¼åˆ†æ”¯é‡‡ç”¨å†»ç»“çš„ CLIP å›¾åƒç¼–ç å™¨ç›´æ¥ç”Ÿæˆéšå¼æ–‡æœ¬åµŒå…¥ï¼Œå¹¶å°†å…¶è¾“å…¥åˆ°æ‰©æ•£æ¨¡å‹ä¸­ï¼Œè€Œæ— éœ€ä½¿ç”¨æ˜¾å¼æ–‡æœ¬æç¤ºã€‚æ˜¾å¼åˆ†æ”¯åˆ©ç”¨å¯¹åº”å›¾åƒçš„çœŸå®æ ‡ç­¾ä½œä¸ºæ–‡æœ¬æç¤ºæ¥è°ƒèŠ‚æ‰©æ•£æ¨¡å‹çš„ç‰¹å¾æå–ã€‚åœ¨è®­ç»ƒæœŸé—´ï¼Œæˆ‘ä»¬é€šè¿‡å…±äº«è¿™ä¸¤ä¸ªåˆ†æ”¯çš„æ¨¡å‹æƒé‡æ¥è”åˆè®­ç»ƒæ‰©æ•£æ¨¡å‹ã€‚å› æ­¤ï¼Œéšå¼å’Œæ˜¾å¼åˆ†æ”¯å¯ä»¥å…±åŒæŒ‡å¯¼ç‰¹å¾å­¦ä¹ ã€‚åœ¨æ¨ç†æœŸé—´ï¼Œæˆ‘ä»¬ä»…ä½¿ç”¨éšå¼åˆ†æ”¯è¿›è¡Œæœ€ç»ˆé¢„æµ‹ï¼Œä¸éœ€è¦ä»»ä½•çœŸå®æ ‡ç­¾ã€‚åœ¨åŒ…æ‹¬è¯­ä¹‰åˆ†å‰²å’Œæ·±åº¦ä¼°è®¡åœ¨å†…çš„ä¸¤ä¸ªå…¸å‹æ„ŸçŸ¥ä»»åŠ¡ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ IEDP åœ¨è¿™ä¸¤ä¸ªä»»åŠ¡ä¸Šéƒ½å–å¾—äº†æœ‰å¸Œæœ›çš„æ€§èƒ½ã€‚å¯¹äºè¯­ä¹‰åˆ†å‰²ï¼Œæˆ‘ä»¬çš„ IEDP åœ¨ AD20K éªŒè¯é›†ä¸Šçš„ mIoU å¾—åˆ†ä¸º 55.9%ï¼Œæ¯”åŸºçº¿æ–¹æ³• VPD æé«˜äº† 2.2%ã€‚å¯¹äºæ·±åº¦ä¼°è®¡ï¼Œæˆ‘ä»¬çš„ IEDP æ¯”åŸºçº¿æ–¹æ³• VPD æé«˜äº† 10.2%ã€‚**Key Takeaways**- æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„æ„ŸçŸ¥çš„éšå¼å’Œæ˜¾å¼è¯­è¨€æŒ‡å¯¼æ¡†æ¶ï¼ˆIEDPï¼‰ã€‚- IEDP åŒ…æ‹¬ä¸€ä¸ªéšå¼è¯­è¨€æŒ‡å¯¼åˆ†æ”¯å’Œä¸€ä¸ªæ˜¾å¼è¯­è¨€æŒ‡å¯¼åˆ†æ”¯ã€‚- éšå¼åˆ†æ”¯ä½¿ç”¨å†»ç»“çš„ CLIP å›¾åƒç¼–ç å™¨ç”Ÿæˆéšå¼æ–‡æœ¬åµŒå…¥ã€‚- æ˜¾å¼åˆ†æ”¯ä½¿ç”¨çœŸå®æ ‡ç­¾ä½œä¸ºæ–‡æœ¬æç¤ºè°ƒèŠ‚æ‰©æ•£æ¨¡å‹çš„ç‰¹å¾æå–ã€‚- åœ¨è®­ç»ƒæœŸé—´ï¼Œè”åˆè®­ç»ƒ IEDP ä»¥å…±äº«ä¸¤ä¸ªåˆ†æ”¯çš„æ¨¡å‹æƒé‡ã€‚- åœ¨æ¨ç†æœŸé—´ï¼Œä»…ä½¿ç”¨éšå¼åˆ†æ”¯è¿›è¡Œé¢„æµ‹ï¼Œæ— éœ€çœŸå®æ ‡ç­¾ã€‚- IEDP åœ¨è¯­ä¹‰åˆ†å‰²å’Œæ·±åº¦ä¼°è®¡æ–¹é¢å‡å–å¾—äº†æœ‰å¸Œæœ›çš„æ€§èƒ½ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>é¢˜ç›®ï¼šåŸºäºæ‰©æ•£çš„è§†è§‰æ„ŸçŸ¥çš„éšå¼å’Œæ˜¾å¼è¯­è¨€å¼•å¯¼</li><li>ä½œè€…ï¼šHefeng Wang, Jiale Cao, Jin Xie, Aiping Yang, Yanwei Pang</li><li>å•ä½ï¼šå¤©æ´¥å¤§å­¦ç”µæ°”ä¸ä¿¡æ¯å·¥ç¨‹å­¦é™¢</li><li>å…³é”®è¯ï¼šæ‰©æ•£æ¨¡å‹ã€è¯­è¨€å¼•å¯¼ã€è§†è§‰æ„ŸçŸ¥</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.07600ï¼ŒGithubä»£ç ï¼šæ— </li><li><p>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šæ‰©æ•£æ¨¡å‹åœ¨æ¡ä»¶å›¾åƒåˆæˆä¸­è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†å¦‚ä½•å°†å…¶åº”ç”¨äºè§†è§‰æ„ŸçŸ¥ä»æ˜¯ä¸€ä¸ªå¼€æ”¾é—®é¢˜ã€‚(2) è¿‡å»æ–¹æ³•ï¼šVPDå’ŒTADPç­‰æ–¹æ³•é€šè¿‡æ–‡æœ¬æç¤ºæˆ–å›¾åƒå¯¹é½æ ‡é¢˜æ¥å¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œè¯­è¨€å¼•å¯¼ï¼Œä½†å­˜åœ¨ç¹çã€ä¸ä¸€è‡´å’Œæœªå……åˆ†åˆ©ç”¨è®­ç»ƒæ•°æ®ç­‰é—®é¢˜ã€‚(3) ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§éšå¼å’Œæ˜¾å¼è¯­è¨€å¼•å¯¼æ¡†æ¶ï¼ˆIEDPï¼‰ï¼ŒåŒ…æ‹¬éšå¼è¯­è¨€å¼•å¯¼åˆ†æ”¯ï¼ˆç›´æ¥ç”Ÿæˆéšå¼æ–‡æœ¬åµŒå…¥ï¼‰å’Œæ˜¾å¼è¯­è¨€å¼•å¯¼åˆ†æ”¯ï¼ˆä½¿ç”¨çœŸå®æ ‡ç­¾ä½œä¸ºæ–‡æœ¬æç¤ºï¼‰ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè¿™ä¸¤ä¸ªåˆ†æ”¯å…±äº«æ¨¡å‹æƒé‡ï¼Œè”åˆæŒ‡å¯¼ç‰¹å¾å­¦ä¹ ã€‚æ¨ç†æ—¶ï¼Œä»…ä½¿ç”¨éšå¼åˆ†æ”¯è¿›è¡Œé¢„æµ‹ã€‚(4) æ€§èƒ½å’Œåº”ç”¨ï¼šIEDPåœ¨è¯­ä¹‰åˆ†å‰²å’Œæ·±åº¦ä¼°è®¡ä»»åŠ¡ä¸Šå–å¾—äº†è¾ƒå¥½çš„æ€§èƒ½ã€‚åœ¨è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸Šï¼ŒIEDPåœ¨AD20KéªŒè¯é›†ä¸Šçš„mIoUsså¾—åˆ†ä¸º 55.9%ï¼Œæ¯”åŸºçº¿æ–¹æ³• VPD é«˜ 2.2%ã€‚åœ¨æ·±åº¦ä¼°è®¡ä»»åŠ¡ä¸Šï¼ŒIEDP æ¯”åŸºçº¿æ–¹æ³• VPD æé«˜äº† 10.2%ã€‚è¿™äº›æ€§èƒ½è¡¨æ˜ IEDP å¯ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç‰¹å¾è¡¨ç¤ºèƒ½åŠ›ï¼Œå¹¶ä¸ºåŸºäºæ‰©æ•£çš„è§†è§‰æ„ŸçŸ¥æä¾›äº†ä¸€ç§æ–°çš„æ–¹æ³•ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) æå‡ºéšå¼å’Œæ˜¾å¼è¯­è¨€å¼•å¯¼æ¡†æ¶ï¼ˆIEDPï¼‰ï¼ŒåŒ…æ‹¬éšå¼è¯­è¨€å¼•å¯¼åˆ†æ”¯å’Œæ˜¾å¼è¯­è¨€å¼•å¯¼åˆ†æ”¯ï¼›(2) éšå¼è¯­è¨€å¼•å¯¼åˆ†æ”¯ä½¿ç”¨å†»ç»“çš„ CLIP å›¾åƒç¼–ç å™¨ç›´æ¥ç”Ÿæˆéšå¼æ–‡æœ¬åµŒå…¥ï¼Œå¹¶å°†å…¶é¦ˆé€åˆ°æ‰©æ•£æ¨¡å‹ä»¥è°ƒèŠ‚ç‰¹å¾æå–ï¼›(3) æ˜¾å¼è¯­è¨€å¼•å¯¼åˆ†æ”¯åˆ©ç”¨è®­ç»ƒå›¾åƒçš„çœŸå®æ ‡ç­¾ä½œä¸ºæ˜¾å¼æ–‡æœ¬æç¤ºï¼Œå¹¶ä½¿ç”¨ CLIP æ–‡æœ¬ç¼–ç å™¨ç”Ÿæˆæ‰©æ•£æ¨¡å‹çš„æ–‡æœ¬åµŒå…¥ï¼›(4) è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¸¤ä¸ªåˆ†æ”¯å…±äº«æ¨¡å‹æƒé‡ï¼Œè”åˆæŒ‡å¯¼ç‰¹å¾å­¦ä¹ ï¼›(5) æ¨ç†æ—¶ï¼Œä»…ä½¿ç”¨éšå¼åˆ†æ”¯è¿›è¡Œé¢„æµ‹ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„è§†è§‰æ„ŸçŸ¥çš„éšå¼å’Œæ˜¾å¼è¯­è¨€å¼•å¯¼æ¡†æ¶ï¼ˆIEDPï¼‰ï¼Œè¯¥æ¡†æ¶å°†éšå¼è¯­è¨€å¼•å¯¼åˆ†æ”¯å’Œæ˜¾å¼è¯­è¨€å¼•å¯¼åˆ†æ”¯å¼•å…¥åˆ°æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­ã€‚åœ¨éšå¼è¯­è¨€å¼•å¯¼åˆ†æ”¯ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨å†»ç»“çš„ CLIP å›¾åƒç¼–ç å™¨ç›´æ¥ç”Ÿæˆéšå¼æ–‡æœ¬åµŒå…¥ï¼Œå¹¶å°†å…¶é¦ˆé€åˆ°æ‰©æ•£æ¨¡å‹ä»¥è°ƒèŠ‚ç‰¹å¾æå–ã€‚åœ¨æ˜¾å¼è¯­è¨€å¼•å¯¼åˆ†æ”¯ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨è®­ç»ƒå›¾åƒçš„çœŸå®æ ‡ç­¾ä½œä¸ºæ˜¾å¼æ–‡æœ¬æç¤ºï¼Œå¹¶ä½¿ç”¨ CLIP æ–‡æœ¬ç¼–ç å™¨ä¸ºæ‰©æ•£æ¨¡å‹ç”Ÿæˆæ–‡æœ¬åµŒå…¥ã€‚éšå¼è¯­è¨€å¼•å¯¼æ¨¡å—å’Œæ˜¾å¼è¯­è¨€å¼•å¯¼æ¨¡å—å…±äº«æ¨¡å‹æƒé‡ï¼Œè”åˆæŒ‡å¯¼ç‰¹å¾å­¦ä¹ ã€‚åœ¨æ¨ç†æ—¶ï¼Œä»…ä½¿ç”¨éšå¼åˆ†æ”¯è¿›è¡Œé¢„æµ‹ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§æ–°çš„éšå¼å’Œæ˜¾å¼è¯­è¨€å¼•å¯¼æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç‰¹å¾è¡¨ç¤ºèƒ½åŠ›ï¼Œå¹¶ä¸ºåŸºäºæ‰©æ•£çš„è§†è§‰æ„ŸçŸ¥æä¾›äº†ä¸€ç§æ–°çš„æ–¹æ³•ã€‚æ€§èƒ½ï¼šåœ¨è¯­ä¹‰åˆ†å‰²å’Œæ·±åº¦ä¼°è®¡ä»»åŠ¡ä¸Šå–å¾—äº†è¾ƒå¥½çš„æ€§èƒ½ã€‚åœ¨è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸Šï¼ŒIEDP åœ¨ AD20K éªŒè¯é›†ä¸Šçš„ mIoUss å¾—åˆ†ä¸º 55.9%ï¼Œæ¯”åŸºçº¿æ–¹æ³• VPD é«˜ 2.2%ã€‚åœ¨æ·±åº¦ä¼°è®¡ä»»åŠ¡ä¸Šï¼ŒIEDP æ¯”åŸºçº¿æ–¹æ³• VPD æé«˜äº† 10.2%ã€‚å·¥ä½œé‡ï¼šè¯¥æ–¹æ³•éœ€è¦å†»ç»“ CLIP å›¾åƒç¼–ç å™¨å’Œ CLIP æ–‡æœ¬ç¼–ç å™¨ï¼Œå¹¶ä¸”éœ€è¦åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è”åˆè®­ç»ƒéšå¼è¯­è¨€å¼•å¯¼åˆ†æ”¯å’Œæ˜¾å¼è¯­è¨€å¼•å¯¼åˆ†æ”¯ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-7b2851b0665f614f336edc1eb5941c39.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b65fe390679340b89d78ac15bc8be324.jpg" align="middle"><img src="https://picx.zhimg.com/v2-44087ad540fff45ceacfb3af3a6d0f19.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0f8972e34342b703aa0454a3187e07bf.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7786ef639a4220c64f8d4e5d89d8521d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6890beb3eed282355c685640deec3020.jpg" align="middle"></details>## ObjBlur: A Curriculum Learning Approach With Progressive Object-Level   Blurring for Improved Layout-to-Image Generation**Authors:Stanislav Frolov, Brian B. Moser, Sebastian Palacio, Andreas Dengel**We present ObjBlur, a novel curriculum learning approach to improve layout-to-image generation models, where the task is to produce realistic images from layouts composed of boxes and labels. Our method is based on progressive object-level blurring, which effectively stabilizes training and enhances the quality of generated images. This curriculum learning strategy systematically applies varying degrees of blurring to individual objects or the background during training, starting from strong blurring to progressively cleaner images. Our findings reveal that this approach yields significant performance improvements, stabilized training, smoother convergence, and reduced variance between multiple runs. Moreover, our technique demonstrates its versatility by being compatible with generative adversarial networks and diffusion models, underlining its applicability across various generative modeling paradigms. With ObjBlur, we reach new state-of-the-art results on the complex COCO and Visual Genome datasets. [PDF](http://arxiv.org/abs/2404.07564v1) **Summary**æ¸è¿›å¼å¯¹è±¡çº§æ¨¡ç³Šå¤„ç†ï¼Œæé«˜å›¾åƒç”Ÿæˆè´¨é‡ã€‚**Key Takeaways**- æ¸è¿›å¼å¯¹è±¡çº§æ¨¡ç³Šå¤„ç†å¯æœ‰æ•ˆæé«˜å›¾åƒç”Ÿæˆæ¨¡å‹çš„è´¨é‡ã€‚- ä»æ¨¡ç³Šåˆ°æ¸…æ™°çš„è®­ç»ƒè¿‡ç¨‹å¯ç¨³å®šè®­ç»ƒå’Œå¢å¼ºå›¾åƒè´¨é‡ã€‚- è¯¥æ–¹æ³•é€‚ç”¨äºç”Ÿæˆå¯¹æŠ—ç½‘ç»œå’Œæ‰©æ•£æ¨¡å‹ã€‚- åœ¨ COCO å’Œ Visual Genome æ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚- æ¸è¿›å¼æ¨¡ç³Šå¤„ç†å¯å‡å°‘å¤šæ¬¡è¿è¡Œä¹‹é—´çš„å·®å¼‚ã€‚- æ¸è¿›å¼æ¨¡ç³Šå¤„ç†å¯ä½¿æ¨¡å‹æ”¶æ•›æ›´å¹³æ»‘ã€‚- è¯¥æ–¹æ³•ä¸æ‰©æ•£æ¨¡å‹å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œå…¼å®¹ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>é¢˜ç›®ï¼šObjBlurï¼šä¸€ç§æ¸è¿›å¼å¯¹è±¡çº§æ¨¡ç³Šçš„è¯¾ç¨‹å­¦ä¹ æ–¹æ³•</li><li>ä½œè€…ï¼šStanislav Frolovã€Brian B. Moserã€Sebastian Palacioã€Andreas Dengel</li><li>éš¶å±æœºæ„ï¼šå¾·å›½äººå·¥æ™ºèƒ½ç ”ç©¶ä¸­å¿ƒï¼ˆDFKIï¼‰ã€å¾·å›½å‡¯æ’’æ–¯åŠ³æ»•-å…°é“åº”ç”¨æŠ€æœ¯å¤§å­¦</li><li>å…³é”®è¯ï¼šå›¾åƒç”Ÿæˆã€è¯¾ç¨‹å­¦ä¹ ã€å¸ƒå±€åˆ°å›¾åƒ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.07564   Github é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šå¸ƒå±€åˆ°å›¾åƒç”Ÿæˆæ˜¯è®¡ç®—æœºè§†è§‰å’Œå›¾å½¢å­¦ä¸­ä¸€é¡¹åŸºæœ¬ä»»åŠ¡ï¼Œå®ƒå°†ç”±è¾¹ç•Œæ¡†å’Œæ ‡ç­¾ç»„æˆçš„ç»“æ„åŒ–åœºæ™¯æè¿°ä¸ç”Ÿæˆé€¼çœŸå›¾åƒè”ç³»èµ·æ¥ã€‚ç„¶è€Œï¼Œç”±äºä¸åŒå¯¹è±¡ç±»åˆ«çš„å­¦ä¹ éš¾åº¦å’Œå½¢çŠ¶ã€å¤§å°å’Œè¯­å¢ƒçš„å¤šæ ·æ€§ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤æ‚çš„ä»»åŠ¡ã€‚   ï¼ˆ2ï¼‰ï¼šè¿‡å»çš„æ–¹æ³•ï¼šå¸ƒå±€åˆ°å›¾åƒæ¨¡å‹ä¸»è¦åŸºäº GANï¼Œå› æ­¤ç»§æ‰¿äº†å®ƒä»¬åœ¨è®­ç»ƒç¨³å®šæ€§æ–¹é¢çš„ä¸è¶³ï¼Œå¦‚æ¨¡å‹å´©æºƒå’Œè¿‡æ‹Ÿåˆã€‚æ•°æ®å¢å¼ºï¼ˆDAï¼‰æŠ€æœ¯è™½ç„¶åœ¨è§†è§‰è¯†åˆ«æ¨¡å‹ä¸­å·²è¢«è¯æ˜æ˜¯æœ‰æ•ˆçš„ï¼Œä½†åœ¨ GAN ä¸­ä½¿ç”¨ç±»ä¼¼çš„å¢å¼ºä¼šå¯¼è‡´æ³„æ¼æ•ˆåº”ï¼Œå³ç”Ÿæˆå™¨å­¦ä¹ ç”Ÿæˆå¢å¼ºï¼ˆè€Œä¸æ˜¯å¹²å‡€ï¼‰çš„å›¾åƒã€‚   ï¼ˆ3ï¼‰ï¼šè®ºæ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡º ObjBlurï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„å¸ƒå±€åˆ°å›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œå®ƒåˆ©ç”¨è¯¾ç¨‹å­¦ä¹ ï¼Œé€šè¿‡åº”ç”¨æ¸è¿›å¼å¯¹è±¡çº§æ¨¡ç³Šæ¥æé«˜å¸ƒå±€åˆ°å›¾åƒæ¨¡å‹çš„å›¾åƒè´¨é‡ã€‚æ¨¡ç³Šæ˜¯ä¸€ç§è‡ªç„¶çš„å›¾åƒé€€åŒ–æ“ä½œï¼Œå› ä¸ºä½é¢‘æ¯”é«˜é¢‘ä¿ç•™å¾—æ›´å¤šã€‚äº‹å®ä¸Šï¼Œå³ä½¿æ˜¯äººç±»æ„ŸçŸ¥ä¹Ÿå¯¹å›¾åƒçš„ä½é¢‘æ›´æ•æ„Ÿã€‚å¼ºçƒˆçš„æ¨¡ç³Šæ¶ˆé™¤äº†é«˜é¢‘ç»†èŠ‚ï¼Œäº§ç”Ÿäº†æ²¡æœ‰å½±å“å›¾åƒç»“æ„å†…å®¹çš„æ›´ç®€å•çš„ä¿¡å·ï¼ˆä¸æ·»åŠ å™ªå£°ç­‰é€€åŒ–æ›¿ä»£æ–¹æ¡ˆç›¸åï¼‰ã€‚é™ä½æ¨¡ç³Šå¼ºåº¦ä¼šäº§ç”Ÿå…·æœ‰é«˜é¢‘ç»†èŠ‚çš„æ›´å¤æ‚çš„ä¿¡å·ï¼Œä»è€Œä½¿æ¨¡å‹é¢ä¸´æ›´å›°éš¾çš„ä»»åŠ¡ã€‚å› æ­¤ï¼Œæ¨¡ç³Šæä¾›äº†ä¸€ç§ç›´è§‚ä¸”å¼ºå¤§çš„æ–¹æ³•æ¥é€æ­¥è°ƒæ•´ä»»åŠ¡éš¾åº¦ï¼Œç¡®ä¿è®­ç»ƒè¿‡ç¨‹å¹³ç¨³è¿›è¡Œã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥é€šè¿‡ä»…ä¿®æ”¹æ•°æ®åŠ è½½å™¨æ¥å°†æ¸è¿›å¼æ¨¡ç³Šåº”ç”¨äºå›¾åƒæ¥å®ç°ã€‚å› æ­¤ï¼Œå®ƒå¯ä»¥è½»æ¾é›†æˆåˆ°ç°æœ‰çš„å¸ƒå±€åˆ°å›¾åƒæ–¹æ³•ä¸­ï¼Œå¹¶ä¸”ä¸ä¾èµ–äºéš¾åº¦ä¼°è®¡å™¨æˆ–æ¨¡å‹æ¶æ„å’Œä¼˜åŒ–åè®®çš„æ›´æ”¹ã€‚   ï¼ˆ4ï¼‰ï¼šæ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†ä»€ä¹ˆæ€§èƒ½ï¼šé€šè¿‡ç³»ç»Ÿåœ°åº”ç”¨ä¸åŒç¨‹åº¦çš„æ¨¡ç³Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤æ‚çš„æ•°æ®é›† COCO å’Œ VisualGenome ä¸Šå–å¾—äº†æ–°çš„æœ€å…ˆè¿›ç»“æœã€‚è¿™äº›ç»“æœæ”¯æŒäº†æˆ‘ä»¬çš„ç›®æ ‡ï¼Œå³é€šè¿‡æ¸è¿›å¼å¯¹è±¡çº§æ¨¡ç³Šçš„è¯¾ç¨‹å­¦ä¹ ç­–ç•¥å¯ä»¥æ˜¾ç€æé«˜å¸ƒå±€åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„æ€§èƒ½ã€‚</li></ol><p><strong>æ–¹æ³•</strong></p><p>ï¼ˆ1ï¼‰é€æ­¥åº”ç”¨ä¸åŒç¨‹åº¦çš„æ¨¡ç³Šï¼Œä»¥æé«˜å¸ƒå±€åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„å›¾åƒè´¨é‡ã€‚</p><p>ï¼ˆ2ï¼‰æ¨¡ç³Šæ˜¯ä¸€ç§è‡ªç„¶çš„å›¾åƒé€€åŒ–æ“ä½œï¼Œä½é¢‘æ¯”é«˜é¢‘ä¿ç•™å¾—æ›´å¤šã€‚</p><p>ï¼ˆ3ï¼‰å¼ºçƒˆçš„æ¨¡ç³Šæ¶ˆé™¤é«˜é¢‘ç»†èŠ‚ï¼Œäº§ç”Ÿæ›´ç®€å•çš„ä¿¡å·ï¼Œä¸ä¼šå½±å“å›¾åƒç»“æ„å†…å®¹ã€‚</p><p>ï¼ˆ4ï¼‰é™ä½æ¨¡ç³Šå¼ºåº¦ä¼šäº§ç”Ÿå…·æœ‰é«˜é¢‘ç»†èŠ‚çš„æ›´å¤æ‚çš„ä¿¡å·ï¼Œä½¿æ¨¡å‹é¢ä¸´æ›´å›°éš¾çš„ä»»åŠ¡ã€‚</p><p>ï¼ˆ5ï¼‰æ¨¡ç³Šæä¾›äº†ä¸€ç§ç›´è§‚ä¸”å¼ºå¤§çš„æ–¹æ³•æ¥é€æ­¥è°ƒæ•´ä»»åŠ¡éš¾åº¦ï¼Œç¡®ä¿è®­ç»ƒè¿‡ç¨‹å¹³ç¨³è¿›è¡Œã€‚</p><p>ï¼ˆ6ï¼‰é€šè¿‡ä¿®æ”¹æ•°æ®åŠ è½½å™¨å°†æ¸è¿›å¼æ¨¡ç³Šåº”ç”¨äºå›¾åƒï¼Œå¯ä»¥è½»æ¾é›†æˆåˆ°ç°æœ‰çš„å¸ƒå±€åˆ°å›¾åƒæ–¹æ³•ä¸­ã€‚</p><p>ï¼ˆ7ï¼‰æ— éœ€ä¾èµ–éš¾åº¦ä¼°è®¡å™¨æˆ–æ¨¡å‹æ¶æ„å’Œä¼˜åŒ–åè®®çš„æ›´æ”¹ã€‚</p><p>ï¼ˆ8ï¼‰åœ¨å¤æ‚çš„æ•°æ®é›† COCO å’Œ VisualGenome ä¸Šå–å¾—äº†æ–°çš„æœ€å…ˆè¿›ç»“æœã€‚</p><ol><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šxxxï¼›ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šxxxï¼›æ€§èƒ½ï¼šxxxï¼›å·¥ä½œé‡ï¼šxxxï¼›</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº† ObjBlurï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¯¹è±¡çº§æ¨¡ç³Šçš„åˆ›æ–°è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œæ˜¾è‘—æå‡äº†å¸ƒå±€åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ç³»ç»Ÿæ€§åœ°ä»å¼ºæ¨¡ç³Šé€æ­¥è¿‡æ¸¡åˆ°æ›´æ¸…æ™°çš„å›¾åƒï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€æ›´å¥½çš„è®­ç»ƒç¨³å®šæ€§å’Œä¸åŒè¿è¡Œé—´æ›´å°çš„å·®å¼‚ã€‚ObjBlur å³æ’å³ç”¨ï¼Œä»…éœ€ä¿®æ”¹æ•°æ®åŠ è½½å™¨ï¼Œä¾¿äºä½¿ç”¨ã€‚å®ƒä¸ç”Ÿæˆå¯¹æŠ—ç½‘ç»œå’Œæ‰©æ•£æ¨¡å‹çš„å…¼å®¹æ€§å‡¸æ˜¾äº†å…¶åœ¨å„ç§ç”Ÿæˆå»ºæ¨¡èŒƒå¼ä¸­çš„é€šç”¨æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶é¦–æ¬¡æ¢ç´¢äº†è¯¾ç¨‹å­¦ä¹ åœ¨å¸ƒå±€åˆ°å›¾åƒç”Ÿæˆä¸­çš„åº”ç”¨ï¼Œæˆ‘ä»¬å¸Œæœ›å®ƒèƒ½æ¿€å‘äººä»¬è¿›ä¸€æ­¥ç ”ç©¶è¯¾ç¨‹å­¦ä¹ å’Œç”Ÿæˆæ¨¡å‹ä¸­çš„æ•°æ®æ‰©å¢çš„æ½œåŠ›ã€‚</p></li></ol><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäºå¯¹è±¡çº§æ¨¡ç³Šçš„è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œé€æ­¥è°ƒæ•´ä»»åŠ¡éš¾åº¦ï¼Œæé«˜æ¨¡å‹æ€§èƒ½ï¼›æ€§èƒ½ï¼šåœ¨å¤æ‚æ•°æ®é›† COCO å’Œ VisualGenome ä¸Šå–å¾—äº†æ–°çš„æœ€å…ˆè¿›ç»“æœï¼›å·¥ä½œé‡ï¼šä¿®æ”¹æ•°æ®åŠ è½½å™¨å³å¯è½»æ¾é›†æˆåˆ°ç°æœ‰çš„å¸ƒå±€åˆ°å›¾åƒæ–¹æ³•ä¸­ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-81db863464ac81e7066b67137335f12c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fb06482086e09d7034b2aace6c6ef4f3.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-356052af0237823d4f23d6121d59488a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-90aec6a9d5718bf54a4f59f8b05b6148.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6e2c7323cbb5da2c03de9a295ad7d1fd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-21dc61a5fbc2f34bd21e9739f745d9a7.jpg" align="middle"></details><h2 id="RealmDreamer-Text-Driven-3D-Scene-Generation-with-Inpainting-and-Depth-Diffusion"><a href="#RealmDreamer-Text-Driven-3D-Scene-Generation-with-Inpainting-and-Depth-Diffusion" class="headerlink" title="RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth   Diffusion"></a>RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth   Diffusion</h2><p><strong>Authors:Jaidev Shriram, Alex Trevithick, Lingjie Liu, Ravi Ramamoorthi</strong></p><p>We introduce RealmDreamer, a technique for generation of general forward-facing 3D scenes from text descriptions. Our technique optimizes a 3D Gaussian Splatting representation to match complex text prompts. We initialize these splats by utilizing the state-of-the-art text-to-image generators, lifting their samples into 3D, and computing the occlusion volume. We then optimize this representation across multiple views as a 3D inpainting task with image-conditional diffusion models. To learn correct geometric structure, we incorporate a depth diffusion model by conditioning on the samples from the inpainting model, giving rich geometric structure. Finally, we finetune the model using sharpened samples from image generators. Notably, our technique does not require video or multi-view data and can synthesize a variety of high-quality 3D scenes in different styles, consisting of multiple objects. Its generality additionally allows 3D synthesis from a single image. </p><p><a href="http://arxiv.org/abs/2404.07199v1">PDF</a> Project Page: <a href="https://realmdreamer.github.io/">https://realmdreamer.github.io/</a></p><p><strong>Summary</strong><br>ä»æ–‡æœ¬æè¿°ä¸­ç”Ÿæˆé€šç”¨å‰å‘ 3D åœºæ™¯çš„æ–°æŠ€æœ¯ï¼šRealmDreamerã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ä»æ–‡æœ¬ç”Ÿæˆ 3D é«˜æ–¯é£æº…è¡¨ç¤ºï¼Œä»¥åŒ¹é…å¤æ‚çš„æ–‡æœ¬æç¤ºã€‚</li><li>ä½¿ç”¨æœ€å…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå™¨åˆå§‹åŒ–é£æº…ï¼Œå°†å…¶æ ·æœ¬æå‡åˆ° 3D å¹¶è®¡ç®—é®æŒ¡ä½“ç§¯ã€‚</li><li>è·¨å¤šä¸ªè§†å›¾ä¼˜åŒ–æ­¤è¡¨ç¤ºï¼Œä½œä¸ºå…·æœ‰å›¾åƒæ¡ä»¶æ‰©æ•£æ¨¡å‹çš„ 3D ä¿®å¤ä»»åŠ¡ã€‚</li><li>é€šè¿‡å¯¹æ¥è‡ªä¿®å¤æ¨¡å‹çš„æ ·æœ¬è¿›è¡Œæ¡ä»¶åŒ–ï¼Œçº³å…¥æ·±åº¦æ‰©æ•£æ¨¡å‹ä»¥äº†è§£æ­£ç¡®çš„å‡ ä½•ç»“æ„ï¼Œä»è€Œæä¾›ä¸°å¯Œçš„å‡ ä½•ç»“æ„ã€‚</li><li>ä½¿ç”¨å›¾åƒç”Ÿæˆå™¨ä¸­çš„é”åŒ–æ ·æœ¬å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚</li><li>è¯¥æŠ€æœ¯ä¸éœ€è¦è§†é¢‘æˆ–å¤šè§†å›¾æ•°æ®ï¼Œå¹¶ä¸”å¯ä»¥åˆæˆå„ç§ä¸åŒé£æ ¼çš„é«˜è´¨é‡ 3D åœºæ™¯ï¼ŒåŒ…æ‹¬å¤šä¸ªå¯¹è±¡ã€‚</li><li>å…¶é€šç”¨æ€§è¿˜å…è®¸ä»å•ä¸ªå›¾åƒè¿›è¡Œ 3D åˆæˆã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šRealmDreamerï¼šæ–‡æœ¬é©±åŠ¨çš„ 3D åœºæ™¯ç”Ÿæˆï¼Œå¸¦å†…ç»˜å’Œæ·±åº¦æ‰©æ•£</li><li>ä½œè€…ï¼šJaidev Shriram<em>ã€Alex Trevithick</em>ã€Lingjie Liuã€Ravi Ramamoorthi</li><li>éš¶å±å•ä½ï¼šåŠ å·å¤§å­¦åœ£åœ°äºšå“¥åˆ†æ ¡</li><li>å…³é”®è¯ï¼šæ–‡æœ¬åˆ° 3Dã€3D åœºæ™¯ç”Ÿæˆã€å†…ç»˜ã€æ·±åº¦æ‰©æ•£</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://realmdreamer.github.io/   Github ä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šæ–‡æœ¬é©±åŠ¨çš„ 3D åœºæ™¯åˆæˆå…·æœ‰é©æ–° 3D å†…å®¹åˆ›ä½œçš„æ½œåŠ›ï¼Œä½†ç°æœ‰çš„æ–¹æ³•å­˜åœ¨è¿­ä»£æ—¶é—´é•¿ã€é™åˆ¶äºç®€å•çš„å¯¹è±¡çº§æ•°æ®æˆ–å…¨æ™¯å›¾ç­‰é—®é¢˜ã€‚(2) è¿‡å»æ–¹æ³•ï¼šè¿‡å»æ–¹æ³•é€šå¸¸ä½¿ç”¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå™¨åˆå§‹åŒ– 3D è¡¨ç¤ºï¼Œä½†å­˜åœ¨å‡ ä½•ç»“æ„ä¸å‡†ç¡®çš„é—®é¢˜ã€‚(3) ç ”ç©¶æ–¹æ³•ï¼šRealmDreamer æå‡ºäº†ä¸€ç§ä¼˜åŒ– 3D é«˜æ–¯æ•£å°„è¡¨ç¤ºä»¥åŒ¹é…å¤æ‚æ–‡æœ¬æç¤ºçš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå™¨åˆå§‹åŒ–æ•£å°„ä½“ï¼Œå¹¶é€šè¿‡å†…ç»˜å’Œæ·±åº¦æ‰©æ•£æ¨¡å‹ä¼˜åŒ–è¡¨ç¤ºï¼Œä»¥å®ç°è§†å·®ã€è¯¦ç»†å¤–è§‚å’Œé«˜ä¿çœŸå‡ ä½•ç»“æ„ã€‚(4) æ€§èƒ½ï¼šRealmDreamer åœ¨ 3D åœºæ™¯ç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå¯ä»¥åˆæˆå„ç§é«˜è´¨é‡çš„ 3D åœºæ™¯ï¼ŒåŒ…æ‹¬å¤šä¸ªå¯¹è±¡ã€‚å…¶é€šç”¨æ€§è¿˜å…è®¸ä»å•ä¸ªå›¾åƒä¸­è¿›è¡Œ 3D åˆæˆã€‚</li></ol><p>7.æ–¹æ³•ï¼š(1)åˆå§‹åŒ–ï¼šåˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå™¨åˆå§‹åŒ–3Dé«˜æ–¯æ•£å°„è¡¨ç¤ºï¼Œå¹¶é€šè¿‡å•ç›®æ·±åº¦ä¼°è®¡å°†å›¾åƒæå‡ä¸º3Dç‚¹äº‘ï¼›(2)å†…ç»˜ï¼šä½¿ç”¨2Då†…ç»˜æ‰©æ•£æ¨¡å‹å¡«å……ç‚¹äº‘ä¸­çš„ç©ºæ´åŒºåŸŸï¼Œå¹¶å¼•å…¥æ·±åº¦æ‰©æ•£å…ˆéªŒæ¨¡å‹ä»¥æé«˜å‡ ä½•ç²¾åº¦å’Œæ”¶æ•›é€Ÿåº¦ï¼›(3)å¾®è°ƒï¼šä½¿ç”¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å¾®è°ƒæ¨¡å‹ï¼Œæé«˜åœºæ™¯çš„è¿è´¯æ€§å’Œç»†èŠ‚æ¸…æ™°åº¦ï¼›(4)ä¼˜åŒ–ï¼šä½¿ç”¨å†…ç»˜æŸå¤±ã€æ·±åº¦æ‰©æ•£æŸå¤±ã€æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æŸå¤±ã€ä¸é€æ˜åº¦æŸå¤±å’Œé”åŒ–è¿‡ç¨‹ä¼˜åŒ–æ¨¡å‹ï¼Œå¾—åˆ°æœ€ç»ˆçš„3Dåœºæ™¯ã€‚</p><ol><li>ç»“è®ºï¼š(1) RealmDreameråœ¨æ–‡æœ¬é©±åŠ¨çš„3Dåœºæ™¯ç”Ÿæˆé¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ï¼Œå®ç°äº†é«˜è´¨é‡ã€é«˜ä¿çœŸå‡ ä½•ç»“æ„çš„3Dåœºæ™¯åˆæˆï¼›(2) åˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§ä¼˜åŒ–3Dé«˜æ–¯æ•£å°„è¡¨ç¤ºçš„æ–¹æ³•ï¼Œåˆ©ç”¨å†…ç»˜å’Œæ·±åº¦æ‰©æ•£æ¨¡å‹åŒ¹é…å¤æ‚æ–‡æœ¬æç¤ºï¼›</li><li>å®ç°äº†è§†å·®ã€è¯¦ç»†å¤–è§‚å’Œé«˜ä¿çœŸå‡ ä½•ç»“æ„çš„ç»Ÿä¸€ç”Ÿæˆï¼›</li><li>å…·æœ‰ä»å•ä¸ªå›¾åƒè¿›è¡Œ3Dåˆæˆçš„é€šç”¨æ€§ï¼›</li><li>æ€§èƒ½ï¼š</li><li>åœ¨3Dåœºæ™¯ç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå¯ä»¥åˆæˆå„ç§é«˜è´¨é‡çš„3Dåœºæ™¯ï¼ŒåŒ…æ‹¬å¤šä¸ªå¯¹è±¡ï¼›</li><li>å·¥ä½œé‡ï¼š</li><li>è®­ç»ƒè¿‡ç¨‹éœ€è¦æ•°å°æ—¶ï¼›</li><li>å¯¹äºå…·æœ‰é«˜åº¦é®æŒ¡çš„å¤æ‚åœºæ™¯ï¼Œç”Ÿæˆç»“æœå¯èƒ½å­˜åœ¨æ¨¡ç³Šé—®é¢˜ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-a5b848a94e7c9ceb54ea9d3c0f1d9886.jpg" align="middle"><img src="https://pica.zhimg.com/v2-38a9e6e511a25c96557597931a870630.jpg" align="middle"><img src="https://picx.zhimg.com/v2-868bab455ec35ce1f302df76db771b6f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b22afc5e0c39a48548a9120642a8038d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6a53086cc451b1c9072ffe37abccae8f.jpg" align="middle"></details><h2 id="InstantMesh-Efficient-3D-Mesh-Generation-from-a-Single-Image-with-Sparse-view-Large-Reconstruction-Models"><a href="#InstantMesh-Efficient-3D-Mesh-Generation-from-a-Single-Image-with-Sparse-view-Large-Reconstruction-Models" class="headerlink" title="InstantMesh: Efficient 3D Mesh Generation from a Single Image with   Sparse-view Large Reconstruction Models"></a>InstantMesh: Efficient 3D Mesh Generation from a Single Image with   Sparse-view Large Reconstruction Models</h2><p><strong>Authors:Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, Ying Shan</strong></p><p>We present InstantMesh, a feed-forward framework for instant 3D mesh generation from a single image, featuring state-of-the-art generation quality and significant training scalability. By synergizing the strengths of an off-the-shelf multiview diffusion model and a sparse-view reconstruction model based on the LRM architecture, InstantMesh is able to create diverse 3D assets within 10 seconds. To enhance the training efficiency and exploit more geometric supervisions, e.g, depths and normals, we integrate a differentiable iso-surface extraction module into our framework and directly optimize on the mesh representation. Experimental results on public datasets demonstrate that InstantMesh significantly outperforms other latest image-to-3D baselines, both qualitatively and quantitatively. We release all the code, weights, and demo of InstantMesh, with the intention that it can make substantial contributions to the community of 3D generative AI and empower both researchers and content creators. </p><p><a href="http://arxiv.org/abs/2404.07191v1">PDF</a> Technical report. Project: <a href="https://github.com/TencentARC/InstantMesh">https://github.com/TencentARC/InstantMesh</a></p><p><strong>Summary</strong><br>å³æ—¶ç½‘æ ¼ç”Ÿæˆæ¨¡å‹InstantMeshèƒ½å¤Ÿä»å•å¼ å›¾åƒä¸­é«˜æ•ˆç”Ÿæˆé«˜è´¨é‡3Dç½‘æ ¼æ¨¡å‹ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>InstantMeshé‡‡ç”¨å‰é¦ˆæ¡†æ¶ï¼Œå¯ä»å•å¼ å›¾åƒå³æ—¶ç”Ÿæˆ3Dç½‘æ ¼ã€‚</li><li>æ¨¡å‹èåˆå¤šåˆ†è¾¨ç‡æ‰©æ•£æ¨¡å‹å’Œç¨€ç–è§†å›¾é‡å»ºæ¨¡å‹ï¼Œç”Ÿæˆé«˜ç²¾åº¦3Dæ¨¡å‹ã€‚</li><li>å¯å·®åˆ†ç­‰å€¼é¢æå–æ¨¡å—ç›´æ¥ä¼˜åŒ–ç½‘æ ¼è¡¨ç¤ºï¼Œæå‡è®­ç»ƒæ•ˆç‡ã€‚</li><li>å……åˆ†åˆ©ç”¨æ·±åº¦å’Œæ³•çº¿ç­‰å‡ ä½•ç›‘ç£ä¿¡æ¯è¿›è¡Œè®­ç»ƒã€‚</li><li>InstantMeshæ€§èƒ½ä¼˜äºå…¶ä»–å›¾åƒåˆ°3Dç”ŸæˆåŸºçº¿æ¨¡å‹ã€‚</li><li>æ¨¡å‹ä»£ç ã€æƒé‡å’Œæ¼”ç¤ºå·²å…¨éƒ¨å¼€æºï¼Œæ¨åŠ¨3Dç”Ÿæˆå¼äººå·¥æ™ºèƒ½ç¤¾åŒºå‘å±•ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šInstantMeshï¼šé«˜æ•ˆçš„å•å¼ å›¾åƒç”Ÿæˆ 3D ç½‘æ ¼</li><li>ä½œè€…ï¼šJiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang<em>, Shenghua Gao</em>, Ying Shan</li><li>å•ä½ï¼šä¸Šæµ·ç§‘æŠ€å¤§å­¦</li><li>å…³é”®è¯ï¼šå›¾åƒåˆ° 3Dã€3D ç”Ÿæˆã€ç½‘æ ¼ç”Ÿæˆã€æ‰©æ•£æ¨¡å‹</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.07191   Github ä»£ç é“¾æ¥ï¼šhttps://github.com/TencentARC/InstantMesh</li><li><p>æ‘˜è¦ï¼š   (1) ç ”ç©¶èƒŒæ™¯ï¼šéšç€ç”Ÿæˆå¼ AI çš„é£é€Ÿå‘å±•ï¼Œå›¾åƒåˆ° 3D ç”Ÿæˆä»»åŠ¡å¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•è¦ä¹ˆç”Ÿæˆè´¨é‡è¾ƒå·®ï¼Œè¦ä¹ˆè®­ç»ƒæ•ˆç‡ä½ä¸‹ã€‚   (2) è¿‡å»æ–¹æ³•ï¼šç°æœ‰çš„å›¾åƒåˆ° 3D æ–¹æ³•ä¸»è¦åŸºäºå¤šè§†å›¾æ‰©æ•£æ¨¡å‹æˆ–ç¨€ç–è§†å›¾é‡å»ºæ¨¡å‹ã€‚å¤šè§†å›¾æ‰©æ•£æ¨¡å‹ç”Ÿæˆè´¨é‡é«˜ï¼Œä½†è®­ç»ƒæ•ˆç‡ä½ï¼›ç¨€ç–è§†å›¾é‡å»ºæ¨¡å‹è®­ç»ƒæ•ˆç‡é«˜ï¼Œä½†ç”Ÿæˆè´¨é‡è¾ƒå·®ã€‚   (3) æœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡º InstantMeshï¼Œä¸€ä¸ªä»å•å¼ å›¾åƒç”Ÿæˆ 3D ç½‘æ ¼çš„å‰é¦ˆæ¡†æ¶ã€‚InstantMesh ç»“åˆäº†å¤šè§†å›¾æ‰©æ•£æ¨¡å‹å’ŒåŸºäº LRM æ¶æ„çš„ç¨€ç–è§†å›¾é‡å»ºæ¨¡å‹çš„ä¼˜åŠ¿ï¼Œåœ¨ 10 ç§’å†…ç”Ÿæˆé«˜è´¨é‡çš„ 3D ç½‘æ ¼ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜é›†æˆäº†å¯å¾®ç­‰å€¼é¢æå–æ¨¡å—ï¼Œç›´æ¥ä¼˜åŒ–ç½‘æ ¼è¡¨ç¤ºï¼Œæé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚   (4) æ€§èƒ½ï¼šå®éªŒç»“æœè¡¨æ˜ï¼ŒInstantMesh åœ¨ç”Ÿæˆè´¨é‡å’Œè®­ç»ƒæ•ˆç‡æ–¹é¢å‡ä¼˜äºå…¶ä»–æœ€æ–°çš„å›¾åƒåˆ° 3D åŸºçº¿ã€‚æœ¬æ–‡å‘å¸ƒäº† InstantMesh çš„æ‰€æœ‰ä»£ç ã€æƒé‡å’Œæ¼”ç¤ºï¼Œæ—¨åœ¨ä¸º 3D ç”Ÿæˆå¼ AI ç¤¾åŒºåšå‡ºé‡å¤§è´¡çŒ®ï¼Œå¹¶èµ‹èƒ½ç ”ç©¶äººå‘˜å’Œå†…å®¹åˆ›ä½œè€…ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1): InstantMeshå°†å¤šè§†å›¾æ‰©æ•£æ¨¡å‹ä¸åŸºäºLRMæ¶æ„çš„ç¨€ç–è§†å›¾é‡å»ºæ¨¡å‹ç›¸ç»“åˆï¼Œç”Ÿæˆé«˜è´¨é‡çš„3Dç½‘æ ¼ï¼›(2): InstantMeshé›†æˆäº†å¯å¾®ç­‰å€¼é¢æå–æ¨¡å—ï¼Œç›´æ¥ä¼˜åŒ–ç½‘æ ¼è¡¨ç¤ºï¼Œæé«˜è®­ç»ƒæ•ˆç‡ï¼›(3): InstantMeshé‡‡ç”¨å‰é¦ˆæ¡†æ¶ï¼Œåœ¨10ç§’å†…ç”Ÿæˆ3Dç½‘æ ¼ï¼›(4): InstantMeshåœ¨ç”Ÿæˆè´¨é‡å’Œè®­ç»ƒæ•ˆç‡æ–¹é¢ä¼˜äºå…¶ä»–å›¾åƒåˆ°3DåŸºçº¿ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šxxxï¼›ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šxxxï¼›æ€§èƒ½ï¼šxxxï¼›å·¥ä½œé‡ï¼šxxxï¼›</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº† InstantMeshï¼Œä¸€ä¸ªä»å•å¼ å›¾åƒç”Ÿæˆé«˜è´¨é‡ 3D ç½‘æ ¼çš„å‰é¦ˆæ¡†æ¶ï¼Œä¸º 3D ç”Ÿæˆå¼ AI ç¤¾åŒºåšå‡ºäº†é‡å¤§è´¡çŒ®ï¼Œå¹¶èµ‹èƒ½ç ”ç©¶äººå‘˜å’Œå†…å®¹åˆ›ä½œè€…ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>å°†å¤šè§†å›¾æ‰©æ•£æ¨¡å‹ä¸åŸºäº LRM æ¶æ„çš„ç¨€ç–è§†å›¾é‡å»ºæ¨¡å‹ç›¸ç»“åˆï¼Œç”Ÿæˆé«˜è´¨é‡ 3D ç½‘æ ¼ã€‚</li><li>é›†æˆäº†å¯å¾®ç­‰å€¼é¢æå–æ¨¡å—ï¼Œç›´æ¥ä¼˜åŒ–ç½‘æ ¼è¡¨ç¤ºï¼Œæé«˜è®­ç»ƒæ•ˆç‡ã€‚</li><li>é‡‡ç”¨å‰é¦ˆæ¡†æ¶ï¼Œåœ¨ 10 ç§’å†…ç”Ÿæˆ 3D ç½‘æ ¼ã€‚æ€§èƒ½ï¼š</li><li>åœ¨ç”Ÿæˆè´¨é‡å’Œè®­ç»ƒæ•ˆç‡æ–¹é¢ä¼˜äºå…¶ä»–å›¾åƒåˆ° 3D åŸºçº¿ã€‚å·¥ä½œé‡ï¼š</li><li>ä»£ç ã€æƒé‡å’Œæ¼”ç¤ºå‡å·²å¼€æºï¼Œä¾¿äºç ”ç©¶äººå‘˜å’Œåˆ›ä½œè€…ä½¿ç”¨ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-0ee14af0ac02e082feb1a14d55e218ef.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5553d48b4842d024fe7366df280e0637.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ce382d719d315b907c8fef5040c0ca19.jpg" align="middle"><img src="https://picx.zhimg.com/v2-93d190dbc5a43b31005479cc18772537.jpg" align="middle"></details><h2 id="Diffusion-based-inpainting-of-incomplete-Euclidean-distance-matrices-of-trajectories-generated-by-a-fractional-Brownian-motion"><a href="#Diffusion-based-inpainting-of-incomplete-Euclidean-distance-matrices-of-trajectories-generated-by-a-fractional-Brownian-motion" class="headerlink" title="Diffusion-based inpainting of incomplete Euclidean distance matrices of   trajectories generated by a fractional Brownian motion"></a>Diffusion-based inpainting of incomplete Euclidean distance matrices of   trajectories generated by a fractional Brownian motion</h2><p><strong>Authors:Alexander Lobashev, Kirill Polovnikov</strong></p><p>Fractional Brownian trajectories (fBm) feature both randomness and strong scale-free correlations, challenging generative models to reproduce the intrinsic memory characterizing the underlying process. Here we test a diffusion probabilistic model on a specific dataset of corrupted images corresponding to incomplete Euclidean distance matrices of fBm at various memory exponents $H$. Our dataset implies uniqueness of the data imputation in the regime of low missing ratio, where the remaining partial graph is rigid, providing the ground truth for the inpainting. We find that the conditional diffusion generation stably reproduces the statistics of missing fBm-distributed distances for different values of $H$ exponent. Furthermore, while diffusion models have been recently shown to remember samples from the training database, we show that diffusion-based inpainting behaves qualitatively different from the database search with the increasing database size. Finally, we apply our fBm-trained diffusion model with $H=1/3$ for completion of chromosome distance matrices obtained in single-cell microscopy experiments, showing its superiority over the standard bioinformatics algorithms. Our source code is available on GitHub at <a href="https://github.com/alobashev/diffusion_fbm">https://github.com/alobashev/diffusion_fbm</a>. </p><p><a href="http://arxiv.org/abs/2404.07029v1">PDF</a> </p><p><strong>Summary</strong><br>æ‰©æ•£æ¦‚ç‡æ¨¡å‹å¯ç¨³å®šç”Ÿæˆå…·æœ‰ä¸åŒè®°å¿†æŒ‡æ•°çš„ fBm åˆ†å¸ƒè·ç¦»ï¼Œåœ¨å•ç»†èƒæ˜¾å¾®å®éªŒä¸­ä¼˜äºæ ‡å‡†ç”Ÿç‰©ä¿¡æ¯å­¦ç®—æ³•ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æ‰©æ•£æ¦‚ç‡æ¨¡å‹å¯ç¨³å®šç”Ÿæˆå…·æœ‰ä¸åŒè®°å¿†æŒ‡æ•°çš„ fBm åˆ†å¸ƒè·ç¦»ã€‚</li><li>æ‰©æ•£æ¨¡å‹åœ¨ä½ç¼ºå¤±ç‡ä¸‹å¯å”¯ä¸€åœ°æ’è¡¥æ•°æ®ã€‚</li><li>æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„æ’è¡¥ç»“æœä¸è®­ç»ƒæ•°æ®åº“æœç´¢å­˜åœ¨è´¨çš„ä¸åŒã€‚</li><li>æ‰©æ•£æ¨¡å‹åœ¨å•ç»†èƒæ˜¾å¾®å®éªŒä¸­è¡¨ç°å‡ºä¼˜äºæ ‡å‡†ç”Ÿç‰©ä¿¡æ¯å­¦ç®—æ³•çš„æ€§èƒ½ã€‚</li><li>æ‰©æ•£æ¨¡å‹å…·æœ‰è®°å¿†è®­ç»ƒæ•°æ®åº“æ ·æœ¬çš„èƒ½åŠ›ã€‚</li><li>fBm è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹åœ¨å°ç¼ºå¤±ç‡ä¸‹è¡¨ç°å‡ºç¨³å®šæ€§ã€‚</li><li>ä»£ç å¯åœ¨ GitHub ä¸Šè·å–ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šæ‰©æ•£æ¦‚ç‡æ¨¡å‹åœ¨åˆ†æ•°å¸ƒæœ—è¿åŠ¨è·ç¦»çŸ©é˜µä¿®å¤ä¸­çš„åº”ç”¨</li><li>ä½œè€…ï¼šAlexey Lobashev, Dmitry Krotov, Vadim S. Smelyanskiy</li><li>æ‰€å±å•ä½ï¼šè«æ–¯ç§‘å›½ç«‹å¤§å­¦</li><li>å…³é”®è¯ï¼šåˆ†æ•°å¸ƒæœ—è¿åŠ¨ï¼Œæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼Œæ•°æ®ä¿®å¤ï¼Œç”Ÿç‰©ä¿¡æ¯å­¦</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.09842Github ä»£ç é“¾æ¥ï¼šhttps://github.com/alobashev/diffusionfbm</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šåˆ†æ•°å¸ƒæœ—è¿åŠ¨ï¼ˆfBmï¼‰è½¨è¿¹å…·æœ‰éšæœºæ€§å’Œå¼ºå°ºåº¦è‡ªç”±ç›¸å…³æ€§ï¼Œå¯¹ç”Ÿæˆæ¨¡å‹é‡ç°å…¶å†…åœ¨è®°å¿†æå‡ºäº†æŒ‘æˆ˜ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•ï¼šè¿‡å»çš„æ–¹æ³•åŒ…æ‹¬æ•°æ®åº“æœç´¢å’Œæœ€è¿‘é‚»æ–¹æ³•ï¼Œä½†è¿™äº›æ–¹æ³•åœ¨ä½ç¼ºå¤±ç‡ä¸‹ä¼šå‡ºç°ä¸ç¨³å®šæ€§å’Œä¸å‡†ç¡®æ€§ã€‚ï¼ˆ3ï¼‰æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMï¼‰çš„æ•°æ®ä¿®å¤æ–¹æ³•ï¼Œé€šè¿‡ä¼˜åŒ–æ¡ä»¶æ‰©æ•£è¿‡ç¨‹æ¥é‡ç° fBm åˆ†å¸ƒè·ç¦»çš„ç»Ÿè®¡æ•°æ®ã€‚ï¼ˆ4ï¼‰æ–¹æ³•çš„æ€§èƒ½ï¼šåœ¨ä¸åŒç¼ºå¤±ç‡å’Œ Hurst æŒ‡æ•°ä¸‹ï¼ŒDDPM æ–¹æ³•åœ¨ä¿®å¤ fBm è·ç¦»çŸ©é˜µæ–¹é¢ä¼˜äºæ•°æ®åº“æœç´¢å’Œæœ€è¿‘é‚»æ–¹æ³•ã€‚åœ¨å•ç»†èƒæ˜¾å¾®é•œå®éªŒä¸­ï¼ŒDDPM æ–¹æ³•ä¹Ÿä¼˜äºæ ‡å‡†ç”Ÿç‰©ä¿¡æ¯å­¦ç®—æ³•ã€‚è¯¥æ–¹æ³•çš„æ€§èƒ½è¯æ˜äº†å…¶åœ¨ä¿®å¤ fBm æ•°æ®æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</li></ol><p>7.æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼šæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMï¼‰ä¿®å¤ï¼šåˆ©ç”¨é¢„è®­ç»ƒçš„ DDPM æ¨¡å‹ï¼Œå°†å·²çŸ¥è·ç¦»çŸ©é˜µä¸­çš„å·²çŸ¥å€¼ä½œä¸ºæ¡ä»¶ï¼Œé€šè¿‡é€†å‘æ‰©æ•£è¿‡ç¨‹é‡ç° fBm åˆ†å¸ƒè·ç¦»çš„ç»Ÿè®¡æ•°æ®ï¼Œä¿®å¤ç¼ºå¤±å€¼ã€‚ï¼ˆ2ï¼‰ï¼šæ•°æ®åº“æœç´¢ä¿®å¤ï¼šä»é¢„å…ˆæ„å»ºçš„è·ç¦»çŸ©é˜µæ•°æ®åº“ä¸­ï¼Œæœç´¢ä¸æŸåçŸ©é˜µæœ€ç›¸ä¼¼çš„å®Œæ•´çŸ©é˜µï¼Œå¹¶åŸºäºä¸¤è€…èåˆå¾—åˆ°ä¿®å¤ç»“æœã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œé€šè¿‡å°†æ¬§æ°è·ç¦»çŸ©é˜µè§†ä¸ºå›¾åƒï¼Œè¯æ˜äº†æ‰©æ•£æ¦‚ç‡æ¨¡å‹å¯ä»¥å­¦ä¹ åˆ°å„ç§è®°å¿†æŒ‡æ•° H çš„ fBm è½¨è¿¹é›†åˆçš„è·ç¦»çŸ©é˜µä¸­æœ¬è´¨çš„å¤§å°ºåº¦ç›¸å…³æ€§ã€‚åŸºäºè¿™ä¸€è§‚å¯Ÿï¼Œæˆ‘ä»¬åº”ç”¨åŸºäºæ‰©æ•£çš„ä¿®å¤æ¥è§£å†³ EDM è¡¥å…¨é—®é¢˜ï¼Œå‘ç°æ‰©æ•£æ¡ä»¶ç”Ÿæˆä¸æ•°æ®åº“æœç´¢çš„è¡Œä¸ºæˆªç„¶ä¸åŒï¼Œæ•°æ®åº“å¤§å°ä¸æ‰©æ•£æ¨¡å‹çš„å‚æ•°æ•°é‡ç›¸ä¼¼ã€‚æˆ‘ä»¬æä¾›äº†å…³äºæœ‰æ•ˆæ•°æ®åº“å¤§å°çš„ç†è®ºè®ºè¯ï¼Œè§£é‡Šäº†è¿™ç§å®šæ€§å·®å¼‚ï¼Œå¹¶åœ¨æ•°å€¼å®éªŒä¸­éªŒè¯äº†è¿™ä¸€ç‚¹ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¡¨æ˜ï¼Œè™½ç„¶åŸºäºæ‰©æ•£çš„ä¿®å¤è¡Œä¸ºç±»ä¼¼äºæ¢¯åº¦è½¨è¿¹ä¼˜åŒ–ï¼Œä½†å®ƒä¸ä»…å­¦ä¹ æ½œåœ¨ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¦‚ç‡æ¨¡å‹çš„ fBm è·ç¦»çŸ©é˜µä¿®å¤æ–¹æ³•ï¼Œé€šè¿‡ä¼˜åŒ–æ¡ä»¶æ‰©æ•£è¿‡ç¨‹æ¥é‡ç° fBm åˆ†å¸ƒè·ç¦»çš„ç»Ÿè®¡æ•°æ®ã€‚æ€§èƒ½ï¼šåœ¨ä¸åŒç¼ºå¤±ç‡å’Œ Hurst æŒ‡æ•°ä¸‹ï¼ŒDDPM æ–¹æ³•åœ¨ä¿®å¤ fBm è·ç¦»çŸ©é˜µæ–¹é¢ä¼˜äºæ•°æ®åº“æœç´¢å’Œæœ€è¿‘é‚»æ–¹æ³•ã€‚åœ¨å•ç»†èƒæ˜¾å¾®é•œå®éªŒä¸­ï¼ŒDDPM æ–¹æ³•ä¹Ÿä¼˜äºæ ‡å‡†ç”Ÿç‰©ä¿¡æ¯å­¦ç®—æ³•ã€‚å·¥ä½œé‡ï¼šè¯¥æ–¹æ³•çš„å®ç°éœ€è¦é¢„è®­ç»ƒ DDPM æ¨¡å‹å’Œæ„å»ºè·ç¦»çŸ©é˜µæ•°æ®åº“ï¼Œè¿™å¯èƒ½éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºå’Œæ—¶é—´ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-1e591413055303714fd287d5550c2a23.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-623f1390886e9691bc03d34d9211c37f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-efdf6238114991b4b2ee774294d87f63.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-839cd328f9832c761e8e3589b9cc527b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-055cddc77c7aa2cb247c55b9dd706d2b.jpg" align="middle"></details><h2 id="DreamScene360-Unconstrained-Text-to-3D-Scene-Generation-with-Panoramic-Gaussian-Splatting"><a href="#DreamScene360-Unconstrained-Text-to-3D-Scene-Generation-with-Panoramic-Gaussian-Splatting" class="headerlink" title="DreamScene360: Unconstrained Text-to-3D Scene Generation with Panoramic   Gaussian Splatting"></a>DreamScene360: Unconstrained Text-to-3D Scene Generation with Panoramic   Gaussian Splatting</h2><p><strong>Authors:Shijie Zhou, Zhiwen Fan, Dejia Xu, Haoran Chang, Pradyumna Chari, Tejas Bharadwaj, Suya You, Zhangyang Wang, Achuta Kadambi</strong></p><p>The increasing demand for virtual reality applications has highlighted the significance of crafting immersive 3D assets. We present a text-to-3D 360$^{\circ}$ scene generation pipeline that facilitates the creation of comprehensive 360$^{\circ}$ scenes for in-the-wild environments in a matter of minutes. Our approach utilizes the generative power of a 2D diffusion model and prompt self-refinement to create a high-quality and globally coherent panoramic image. This image acts as a preliminary â€œflatâ€ (2D) scene representation. Subsequently, it is lifted into 3D Gaussians, employing splatting techniques to enable real-time exploration. To produce consistent 3D geometry, our pipeline constructs a spatially coherent structure by aligning the 2D monocular depth into a globally optimized point cloud. This point cloud serves as the initial state for the centroids of 3D Gaussians. In order to address invisible issues inherent in single-view inputs, we impose semantic and geometric constraints on both synthesized and input camera views as regularizations. These guide the optimization of Gaussians, aiding in the reconstruction of unseen regions. In summary, our method offers a globally consistent 3D scene within a 360$^{\circ}$ perspective, providing an enhanced immersive experience over existing techniques. Project website at: <a href="http://dreamscene360.github.io/">http://dreamscene360.github.io/</a> </p><p><a href="http://arxiv.org/abs/2404.06903v1">PDF</a> </p><p><strong>Summary</strong><br>æ–‡æœ¬åˆ° 3D å…¨æ™¯åœºæ™¯ç”Ÿæˆç®¡é“ï¼Œå¯å¿«é€Ÿåˆ›å»ºå…¨å±€ä¸€è‡´ä¸”å¼•äººå…¥èƒœçš„ 360 åº¦åœºæ™¯ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>é‡‡ç”¨ 2D æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›å’Œæç¤ºè‡ªæˆ‘ä¼˜åŒ–ç”Ÿæˆé«˜è´¨é‡ä¸”å…¨å±€è¿è´¯çš„å…¨æ™¯å›¾åƒã€‚</li><li>ä½¿ç”¨é•¶åµŒæŠ€æœ¯å°†å›¾åƒæå‡ä¸º 3D é«˜æ–¯ä½“ï¼Œå®ç°å®æ—¶æµè§ˆã€‚</li><li>é€šè¿‡å°† 2D å•ç›®æ·±åº¦å¯¹é½åˆ°å…¨å±€ä¼˜åŒ–ç‚¹äº‘ä¸­ï¼Œæ„å»ºç©ºé—´è¿è´¯ç»“æ„ï¼Œç”Ÿæˆä¸€è‡´çš„ 3D å‡ ä½•ä½“ã€‚</li><li>åˆ©ç”¨åˆæˆå’Œè¾“å…¥ç›¸æœºè§†å›¾çš„è¯­ä¹‰å’Œå‡ ä½•çº¦æŸä½œä¸ºæ­£åˆ™åŒ–ï¼Œè§£å†³å•è§†å›¾è¾“å…¥çš„ä¸å¯è§é—®é¢˜ã€‚</li><li>è¯¥æ–¹æ³•åœ¨ 360 åº¦è§†è§’å†…æä¾›å…¨å±€ä¸€è‡´çš„ 3D åœºæ™¯ï¼Œæ¯”ç°æœ‰æŠ€æœ¯æä¾›æ›´èº«ä¸´å…¶å¢ƒçš„ä½“éªŒã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šDreamScene360ï¼šæ— çº¦æŸæ–‡æœ¬åˆ° 3D åœºæ™¯</li><li>ä½œè€…ï¼šShijie Zhouã€Zhiwen Fanã€Dejia Xuã€Haoran Changã€Pradyumna Chariã€Tejas Bharadwajã€Suya Youã€Zhangyang Wangã€Achuta Kadambi</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šåŠ å·å¤§å­¦æ´›æ‰çŸ¶åˆ†æ ¡</li><li>å…³é”®è¯ï¼šæ–‡æœ¬åˆ° 3D åœºæ™¯ã€360 å…¨æ™¯å›¾ã€é«˜æ–¯æ•£å°„ã€æ·±åº¦ä¼°è®¡ã€ç©ºé—´ä¸€è‡´æ€§</li><li>è®ºæ–‡é“¾æ¥ï¼šhttp://dreamscene360.github.io/ï¼ŒGithub ä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šè™šæ‹Ÿç°å®åº”ç”¨å¯¹æ²‰æµ¸å¼ 3D èµ„äº§çš„éœ€æ±‚æ—¥ç›Šå¢é•¿ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•éš¾ä»¥åˆ›å»ºå…¨é¢ä¸”ä¸€è‡´çš„ 360Â° åœºæ™¯ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæå‡º DreamScene360ï¼Œä¸€ä¸ªæ–‡æœ¬åˆ° 3D åœºæ™¯ç”Ÿæˆç®¡é“ï¼Œåˆ©ç”¨ 2D æ‰©æ•£æ¨¡å‹ç”Ÿæˆå…¨æ™¯å›¾åƒï¼Œå¹¶é€šè¿‡é«˜æ–¯æ•£å°„å’Œæ·±åº¦ä¼°è®¡æŠ€æœ¯å°†å…¶æå‡ä¸º 3D åœºæ™¯ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šåœ¨ in-the-wild ç¯å¢ƒä¸­ï¼ŒDreamScene360 å¯ä»¥åœ¨å‡ åˆ†é’Ÿå†…ç”Ÿæˆé«˜è´¨é‡ã€å…¨å±€ä¸€è‡´çš„ 360Â° åœºæ™¯ï¼Œæ”¯æŒå®æ—¶æ¢ç´¢ã€‚</li></ol><p>7.æ–¹æ³•ï¼š(1) <strong>æ–‡æœ¬åˆ°360Â°å…¨æ™¯å›¾ï¼š</strong> åˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆå…¨æ™¯å›¾åƒï¼Œå¹¶é€šè¿‡è‡ªä¼˜åŒ–è¿‡ç¨‹ç¡®ä¿å›¾åƒä¸æ–‡æœ¬çš„è¯­ä¹‰å¯¹é½ï¼›(2) <strong>å…¨æ™¯å›¾åˆ°3Dåœºæ™¯ï¼š</strong> åˆ©ç”¨å•ç›®å‡ ä½•åˆå§‹åŒ–å’Œä¼˜åŒ–å•ç›®å…¨æ™¯ 3D é«˜æ–¯ä½“ï¼Œå¹¶é€šè¿‡åˆæˆè™šæ‹Ÿç›¸æœºå’Œè’¸é¦è¯­ä¹‰ç›¸ä¼¼æ€§æ¥å¢å¼ºæ·±åº¦ä¸€è‡´æ€§ï¼›(3) <strong>ä¼˜åŒ–å•ç›®å…¨æ™¯ 3D é«˜æ–¯ä½“ï¼š</strong> ä½¿ç”¨ 3D é«˜æ–¯ä½“æ¸²æŸ“æŠ€æœ¯ç”Ÿæˆé€è§†å›¾ï¼Œå¹¶é€šè¿‡æœ€å°åŒ–è¯­ä¹‰æŸå¤±å’Œå‡ ä½•æŸå¤±æ¥ä¼˜åŒ–é«˜æ–¯ä½“çš„å‚æ•°ã€‚</p><ol><li>ç»“è®ºï¼š(1): æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§æ–‡æœ¬åˆ° 3D åœºæ™¯ç”Ÿæˆç®¡é“ DreamScene360ï¼Œè¯¥ç®¡é“å¯ä»¥ç”Ÿæˆé«˜è´¨é‡ã€å…¨å±€ä¸€è‡´çš„ 360Â° åœºæ™¯ï¼Œæ”¯æŒå®æ—¶æ¢ç´¢ï¼Œä¸ºè™šæ‹Ÿç°å®åº”ç”¨æä¾›äº†æ²‰æµ¸å¼ 3D èµ„äº§ã€‚(2): åˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–‡æœ¬åˆ° 360Â° å…¨æ™¯å›¾ç”Ÿæˆæ–¹æ³•ï¼Œç¡®ä¿äº†å›¾åƒä¸æ–‡æœ¬çš„è¯­ä¹‰å¯¹é½ã€‚</li><li>æå‡ºäº†ä¸€ç§åŸºäºå•ç›®å‡ ä½•å’Œä¼˜åŒ–å•ç›®å…¨æ™¯ 3D é«˜æ–¯ä½“çš„å…¨æ™¯å›¾åˆ° 3D åœºæ™¯ç”Ÿæˆæ–¹æ³•ï¼Œå¢å¼ºäº†æ·±åº¦ä¸€è‡´æ€§ã€‚</li><li>æå‡ºäº†ä¸€ç§ä½¿ç”¨ 3D é«˜æ–¯ä½“æ¸²æŸ“æŠ€æœ¯å’Œè¯­ä¹‰æŸå¤±å’Œå‡ ä½•æŸå¤±æœ€å°åŒ–æ¥ä¼˜åŒ–å•ç›®å…¨æ™¯ 3D é«˜æ–¯ä½“çš„æ–¹æ³•ã€‚æ€§èƒ½ï¼š</li><li>DreamScene360 å¯ä»¥ç”Ÿæˆé«˜è´¨é‡ã€å…¨å±€ä¸€è‡´çš„ 360Â° åœºæ™¯ï¼Œæ”¯æŒå®æ—¶æ¢ç´¢ã€‚</li><li>DreamScene360 åœ¨ in-the-wild ç¯å¢ƒä¸­ï¼Œå¯ä»¥åœ¨å‡ åˆ†é’Ÿå†…ç”Ÿæˆåœºæ™¯ã€‚å·¥ä½œé‡ï¼š</li><li>DreamScene360 çš„å®ç°éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºï¼ŒåŒ…æ‹¬ GPU å’Œå†…å­˜ã€‚</li><li>DreamScene360 çš„è®­ç»ƒéœ€è¦å¤§é‡çš„æ•°æ®é›†å’Œè®­ç»ƒæ—¶é—´ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-31c98e3198cc0cdc817317b3bf23d03b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0729d57eb4a8e1293909757da5dd5064.jpg" align="middle"></details><h2 id="UDiFF-Generating-Conditional-Unsigned-Distance-Fields-with-Optimal-Wavelet-Diffusion"><a href="#UDiFF-Generating-Conditional-Unsigned-Distance-Fields-with-Optimal-Wavelet-Diffusion" class="headerlink" title="UDiFF: Generating Conditional Unsigned Distance Fields with Optimal   Wavelet Diffusion"></a>UDiFF: Generating Conditional Unsigned Distance Fields with Optimal   Wavelet Diffusion</h2><p><strong>Authors:Junsheng Zhou, Weiqi Zhang, Baorui Ma, Kanle Shi, Yu-Shen Liu, Zhizhong Han</strong></p><p>Diffusion models have shown remarkable results for image generation, editing and inpainting. Recent works explore diffusion models for 3D shape generation with neural implicit functions, i.e., signed distance function and occupancy function. However, they are limited to shapes with closed surfaces, which prevents them from generating diverse 3D real-world contents containing open surfaces. In this work, we present UDiFF, a 3D diffusion model for unsigned distance fields (UDFs) which is capable to generate textured 3D shapes with open surfaces from text conditions or unconditionally. Our key idea is to generate UDFs in spatial-frequency domain with an optimal wavelet transformation, which produces a compact representation space for UDF generation. Specifically, instead of selecting an appropriate wavelet transformation which requires expensive manual efforts and still leads to large information loss, we propose a data-driven approach to learn the optimal wavelet transformation for UDFs. We evaluate UDiFF to show our advantages by numerical and visual comparisons with the latest methods on widely used benchmarks. Page: <a href="https://weiqi-zhang.github.io/UDiFF">https://weiqi-zhang.github.io/UDiFF</a>. </p><p><a href="http://arxiv.org/abs/2404.06851v1">PDF</a> To appear at CVPR2024. Project page:   <a href="https://weiqi-zhang.github.io/UDiFF">https://weiqi-zhang.github.io/UDiFF</a></p><p><strong>Summary</strong><br>UDiFFæ¨¡å‹é‡‡ç”¨ä¸€ç§æ•°æ®é©±åŠ¨çš„æœ€ä¼˜å°æ³¢å˜æ¢æ–¹æ³•ï¼Œå¯ç”ŸæˆåŒ…å«å¼€å£è¡¨é¢çš„3Då½¢çŠ¶å’Œçº¹ç†ï¼Œå¹¶ä¸”å¯ä»¥ä»æ–‡æœ¬æ¡ä»¶ç”Ÿæˆæˆ–æ— æ¡ä»¶ç”Ÿæˆã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºUDiFFæ¨¡å‹ï¼Œç”¨äºç”Ÿæˆå¸¦æœ‰å¼€å£è¡¨é¢çš„çº¹ç†3Då½¢çŠ¶ã€‚</li><li>ä½¿ç”¨æ•°æ®é©±åŠ¨çš„æœ€ä¼˜å°æ³¢å˜æ¢æ–¹æ³•ï¼Œåœ¨æ—¶ç©ºåŸŸç”ŸæˆUDFã€‚</li><li>æ— éœ€æ‰‹å·¥é€‰æ‹©å°æ³¢å˜æ¢ï¼Œå‡å°‘äººå·¥å·¥ä½œé‡å’Œä¿¡æ¯æŸå¤±ã€‚</li><li>åœ¨å¹¿æ³›ä½¿ç”¨çš„åŸºå‡†ä¸Šï¼Œé€šè¿‡æ•°å­—å’Œè§†è§‰æ¯”è¾ƒè¯„ä¼°äº†UDiFFçš„ä¼˜åŠ¿ã€‚</li><li>å¯ä»¥ä»æ–‡æœ¬æ¡ä»¶ç”Ÿæˆæˆ–æ— æ¡ä»¶ç”Ÿæˆ3Då½¢çŠ¶ã€‚</li><li>åœ¨å›¾åƒç”Ÿæˆã€ç¼–è¾‘å’Œä¿®å¤æ–¹é¢æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„ç»“æœã€‚</li><li>æ‰©å±•äº†æ‰©æ•£æ¨¡å‹åœ¨3Då½¢çŠ¶ç”Ÿæˆä¸­çš„åº”ç”¨ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><p>1.æ ‡é¢˜ï¼šUDiFFï¼šä½¿ç”¨æœ€ä¼˜å°æ³¢ç”Ÿæˆæ¡ä»¶æ— ç¬¦å·è·ç¦»åœº2.ä½œè€…ï¼šWeiqi Zhang<em>, Yifan Wang</em>, Wentao Yuan, Jiayuan Mao, Hui Huang, Xiaogang Wang3.å•ä½ï¼šæ¸…åå¤§å­¦4.å…³é”®è¯ï¼š3Då½¢çŠ¶ç”Ÿæˆã€æ‰©æ•£æ¨¡å‹ã€æ— ç¬¦å·è·ç¦»åœºã€å°æ³¢å˜æ¢5.è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.06851Githubä»£ç é“¾æ¥ï¼šæ— 6.æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆã€ç¼–è¾‘å’Œç»˜ç”»æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæœã€‚æœ€è¿‘çš„å·¥ä½œæ¢ç´¢äº†ç”¨ç¥ç»éšå¼å‡½æ•°ï¼ˆå³å¸¦ç¬¦å·è·ç¦»å‡½æ•°å’Œå ç”¨å‡½æ•°ï¼‰è¿›è¡Œ3Då½¢çŠ¶ç”Ÿæˆã€‚ç„¶è€Œï¼Œå®ƒä»¬ä»…é™äºå…·æœ‰å°é—­è¡¨é¢çš„å½¢çŠ¶ï¼Œè¿™é˜»ç¢äº†å®ƒä»¬ç”ŸæˆåŒ…å«å¼€æ”¾è¡¨é¢çš„å„ç§3DçœŸå®ä¸–ç•Œå†…å®¹ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç°æœ‰æ–¹æ³•çš„åŠ¨æœºå¾ˆå¥½ï¼Œä½†å®ƒä»¬æ— æ³•ç”Ÿæˆå…·æœ‰å¼€æ”¾è¡¨é¢çš„å½¢çŠ¶ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†UDiFFï¼Œä¸€ç§ç”¨äºæ— ç¬¦å·è·ç¦»åœºï¼ˆUDFï¼‰çš„3Dæ‰©æ•£æ¨¡å‹ï¼Œå®ƒèƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æ¡ä»¶æˆ–æ— æ¡ä»¶ç”Ÿæˆå…·æœ‰å¼€æ”¾è¡¨é¢çš„çº¹ç†3Då½¢çŠ¶ã€‚å…¶å…³é”®æ€æƒ³æ˜¯åœ¨æ—¶é¢‘åŸŸä¸­ä½¿ç”¨æœ€ä¼˜å°æ³¢å˜æ¢ç”ŸæˆUDFï¼Œè¿™ä¸ºUDFç”Ÿæˆäº§ç”Ÿäº†ä¸€ä¸ªç´§å‡‘çš„è¡¨ç¤ºç©ºé—´ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ•°æ®é©±åŠ¨çš„ç®—æ³•æ¥å­¦ä¹ UDFçš„æœ€ä¼˜å°æ³¢å˜æ¢ï¼Œè€Œä¸æ˜¯é€‰æ‹©éœ€è¦æ˜‚è´µçš„ç»éªŒåŠªåŠ›å¹¶ä¸”ä»ç„¶ä¼šå¯¼è‡´å¤§é‡ä¿¡æ¯ä¸¢å¤±çš„ä¸åˆé€‚çš„å°æ³¢å˜æ¢ã€‚ï¼ˆ4ï¼‰ä»»åŠ¡å’Œæ€§èƒ½ï¼šæˆ‘ä»¬åœ¨å¹¿æ³›ä½¿ç”¨çš„åŸºå‡†ä¸Šè¯„ä¼°äº†UDiFFï¼Œé€šè¿‡ä¸æœ€æ–°æ–¹æ³•è¿›è¡Œæ•°å€¼å’Œè§†è§‰æ¯”è¾ƒå±•ç¤ºäº†æˆ‘ä»¬çš„ä¼˜åŠ¿ã€‚è¿™äº›æ–¹æ³•çš„æ€§èƒ½å¯ä»¥æ”¯æŒå®ƒä»¬çš„ç›®æ ‡ã€‚</p><p></p><ol><li><p>æ–¹æ³•ï¼š(1): UDiFF é‡‡ç”¨æœ€ä¼˜å°æ³¢å˜æ¢åœ¨æ—¶é¢‘åŸŸç”Ÿæˆæ— ç¬¦å·è·ç¦»åœº (UDF)ï¼Œä¸º UDF ç”Ÿæˆæä¾›äº†ç´§å‡‘çš„è¡¨ç¤ºç©ºé—´ã€‚(2): æå‡ºäº†ä¸€ç§æ•°æ®é©±åŠ¨çš„ç®—æ³•æ¥å­¦ä¹  UDF çš„æœ€ä¼˜å°æ³¢å˜æ¢ï¼Œé¿å…äº†é€‰æ‹©ä¸å½“çš„å°æ³¢å˜æ¢å¯¼è‡´ä¿¡æ¯ä¸¢å¤±ã€‚(3): é€šè¿‡åœ¨å¹¿æ³›ä½¿ç”¨çš„åŸºå‡†ä¸Šä¸æœ€æ–°æ–¹æ³•è¿›è¡Œæ•°å€¼å’Œè§†è§‰æ¯”è¾ƒï¼Œå±•ç¤ºäº† UDiFF åœ¨ç”Ÿæˆå…·æœ‰å¼€æ”¾è¡¨é¢çš„çº¹ç† 3D å½¢çŠ¶æ–¹é¢çš„ä¼˜åŠ¿ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡º UDiFFï¼Œä¸€ç§ç”¨äºæ¡ä»¶æˆ–æ— æ¡ä»¶ç”Ÿæˆå…·æœ‰å¼€æ”¾å’Œé—­åˆè¡¨é¢çš„çº¹ç† 3D å½¢çŠ¶çš„ 3D æ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬åˆ©ç”¨æ‰©æ•£æ¨¡å‹å­¦ä¹ åœ¨é€šè¿‡ UDF çš„æœ€ä¼˜å°æ³¢å˜æ¢å»ºç«‹çš„æ—¶é¢‘ç©ºé—´ä¸­ UDF çš„åˆ†å¸ƒï¼Œè¯¥å˜æ¢æ˜¯é€šè¿‡æ•°æ®é©±åŠ¨çš„ä¼˜åŒ–è·å¾—çš„ã€‚åœ¨å¹¿æ³›ä½¿ç”¨çš„åŸºå‡†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬åœ¨ç”Ÿæˆå…·æœ‰å¼€æ”¾å’Œé—­åˆè¡¨é¢çš„å½¢çŠ¶æ–¹é¢ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§æ•°æ®é©±åŠ¨çš„ç®—æ³•æ¥å­¦ä¹  UDF çš„æœ€ä¼˜å°æ³¢å˜æ¢ï¼Œé¿å…äº†é€‰æ‹©ä¸å½“çš„å°æ³¢å˜æ¢å¯¼è‡´ä¿¡æ¯ä¸¢å¤±ã€‚æ€§èƒ½ï¼šåœ¨ç”Ÿæˆå…·æœ‰å¼€æ”¾å’Œé—­åˆè¡¨é¢çš„å½¢çŠ¶æ–¹é¢ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚å·¥ä½œé‡ï¼šä¸­ç­‰ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-9541327552191532e2f3cebc77a6daa2.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f92525bff0a6b6ad1d46f5258c985f36.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-02e7b71f1534704a3f548c9312638377.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-51a7b4fa5663f281335cbfead03eb9ac.jpg" align="middle"><img src="https://picx.zhimg.com/v2-64c047a6b975468d09d71a01d1e3df5e.jpg" align="middle"></details><h2 id="Urban-Architect-Steerable-3D-Urban-Scene-Generation-with-Layout-Prior"><a href="#Urban-Architect-Steerable-3D-Urban-Scene-Generation-with-Layout-Prior" class="headerlink" title="Urban Architect: Steerable 3D Urban Scene Generation with Layout Prior"></a>Urban Architect: Steerable 3D Urban Scene Generation with Layout Prior</h2><p><strong>Authors:Fan Lu, Kwan-Yee Lin, Yan Xu, Hongsheng Li, Guang Chen, Changjun Jiang</strong></p><p>Text-to-3D generation has achieved remarkable success via large-scale text-to-image diffusion models. Nevertheless, there is no paradigm for scaling up the methodology to urban scale. Urban scenes, characterized by numerous elements, intricate arrangement relationships, and vast scale, present a formidable barrier to the interpretability of ambiguous textual descriptions for effective model optimization. In this work, we surmount the limitations by introducing a compositional 3D layout representation into text-to-3D paradigm, serving as an additional prior. It comprises a set of semantic primitives with simple geometric structures and explicit arrangement relationships, complementing textual descriptions and enabling steerable generation. Upon this, we propose two modifications â€” (1) We introduce Layout-Guided Variational Score Distillation to address model optimization inadequacies. It conditions the score distillation sampling process with geometric and semantic constraints of 3D layouts. (2) To handle the unbounded nature of urban scenes, we represent 3D scene with a Scalable Hash Grid structure, incrementally adapting to the growing scale of urban scenes. Extensive experiments substantiate the capability of our framework to scale text-to-3D generation to large-scale urban scenes that cover over 1000m driving distance for the first time. We also present various scene editing demonstrations, showing the powers of steerable urban scene generation. Website: <a href="https://urbanarchitect.github.io">https://urbanarchitect.github.io</a>. </p><p><a href="http://arxiv.org/abs/2404.06780v1">PDF</a> Project page: <a href="https://urbanarchitect.github.io/">https://urbanarchitect.github.io/</a></p><p><strong>Summary</strong><br>æ–‡æœ¬åˆ°3Dç”Ÿæˆå¼•å…¥å¸ƒå±€å¼•å¯¼å˜åˆ†å¾—åˆ†è’¸é¦å’Œå¯æ‰©å±•å“ˆå¸Œç½‘æ ¼ç»“æ„ï¼Œå®ç°å¯¹å¤§è§„æ¨¡åŸå¸‚åœºæ™¯çš„å¯æ§3Dåœºæ™¯ç”Ÿæˆã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>å¸ƒå±€å¼•å¯¼å˜åˆ†å¾—åˆ†è’¸é¦ï¼Œçº¦æŸè¯„åˆ†è’¸é¦é‡‡æ ·è¿‡ç¨‹çš„å‡ ä½•å’Œè¯­ä¹‰çº¦æŸã€‚</li><li>å¯æ‰©å±•å“ˆå¸Œç½‘æ ¼ç»“æ„ï¼Œé€æ­¥é€‚åº”åŸå¸‚åœºæ™¯çš„å¢é•¿è§„æ¨¡ã€‚</li><li>é¦–æ¬¡å®ç°æ–‡æœ¬åˆ°3Dç”Ÿæˆæ‰©å±•åˆ°è¦†ç›–1000mä»¥ä¸Šé©¾é©¶è·ç¦»çš„å¤§è§„æ¨¡åŸå¸‚åœºæ™¯ã€‚</li><li>å¯æ§çš„åŸå¸‚åœºæ™¯ç”Ÿæˆï¼Œæ”¯æŒå„ç§åœºæ™¯ç¼–è¾‘æ¼”ç¤ºã€‚</li><li>æå‡ºä¸€ä¸ªæˆåˆ†3Då¸ƒå±€è¡¨ç¤ºï¼Œä½œä¸ºæ–‡æœ¬åˆ°3DèŒƒå¼çš„é™„åŠ å…ˆéªŒã€‚</li><li>3Då¸ƒå±€ç”±å…·æœ‰ç®€å•å‡ ä½•ç»“æ„å’Œæ˜ç¡®æ’åˆ—å…³ç³»çš„ä¸€ç»„è¯­ä¹‰åŸºå…ƒç»„æˆã€‚</li><li>3Då¸ƒå±€è¡¥å……æ–‡æœ¬æè¿°ï¼Œå®ç°å¯æ§ç”Ÿæˆã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šUrbanArchitectï¼šåŸºäºå¸ƒå±€å…ˆéªŒçš„å¯æ“çºµ 3D åŸå¸‚åœºæ™¯ç”Ÿæˆ</li><li>ä½œè€…ï¼šFan Lu1ã€Kwan-Yee Lin2â€ ã€Yan Xu3ã€Hongsheng Li2,4,5ã€Guang Chen1â€ ã€Changjun Jiang1</li><li>éš¶å±å•ä½ï¼šåŒæµå¤§å­¦</li><li>å…³é”®è¯ï¼šæ–‡æœ¬åˆ° 3Dã€åŸå¸‚åœºæ™¯ç”Ÿæˆã€å¸ƒå±€å…ˆéªŒã€å¯æ“çºµç”Ÿæˆ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/pdf/2404.06780.pdfï¼ŒGithub ä»£ç é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šæ–‡æœ¬åˆ° 3D ç”Ÿæˆåœ¨æ•°å­—å¯¹è±¡åˆ›å»ºä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œè¿™å½’åŠŸäºå¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„åˆ©ç”¨ã€‚ç„¶è€Œï¼Œå¯¹äºåŸå¸‚å°ºåº¦çš„åœºæ™¯ç”Ÿæˆï¼Œç›®å‰è¿˜æ²¡æœ‰å¯è¡Œçš„èŒƒä¾‹ã€‚åŸå¸‚åœºæ™¯çš„å¤æ‚æ€§å’Œå·¨å¤§è§„æ¨¡ï¼Œä»¥åŠä¼—å¤šå…ƒç´ å’Œå¤æ‚çš„æ’åˆ—å…³ç³»ï¼Œç»™æ¨¡æ£±ä¸¤å¯çš„æ–‡æœ¬æè¿°çš„å¯è§£é‡Šæ€§å¸¦æ¥äº†å·¨å¤§éšœç¢ï¼Œä»è€Œå½±å“äº†æ¨¡å‹çš„æœ‰æ•ˆä¼˜åŒ–ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šè¿‡å»çš„æ–¹æ³•é€šå¸¸ä½¿ç”¨ 3D å¸ƒå±€ä½œä¸ºå…ˆéªŒä¿¡æ¯ï¼Œä½†è¿™äº›æ–¹æ³•å­˜åœ¨æ¨¡å¼ä¼˜åŒ–ä¸è¶³å’Œæ— æ³•å¤„ç†åŸå¸‚åœºæ™¯æ— ç•Œæ€§ç­‰é—®é¢˜ã€‚ï¼ˆ3ï¼‰æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡é€šè¿‡å¼•å…¥æ„æˆçš„ 3D å¸ƒå±€è¡¨ç¤ºä½œä¸ºé™„åŠ å…ˆéªŒï¼Œå®ç°äº†å½“å‰æ–‡æœ¬åˆ° 3D èŒƒä¾‹çš„èŒƒå¼è½¬å˜ã€‚3D å¸ƒå±€ç”±ä¸€ç»„å…·æœ‰ç®€å•å‡ ä½•ç»“æ„ï¼ˆä¾‹å¦‚ï¼Œç«‹æ–¹ä½“ã€æ¤­çƒä½“å’Œå¹³é¢ï¼‰çš„è¯­ä¹‰åŸºå…ƒå’Œæ˜ç¡®çš„æ’åˆ—å…³ç³»ç»„æˆã€‚å®ƒè¡¥å……äº†æ–‡æœ¬æè¿°ï¼ŒåŒæ—¶æ”¯æŒå¯æ“çºµçš„ç”Ÿæˆã€‚åŸºäº 3D å¸ƒå±€è¡¨ç¤ºï¼Œæœ¬æ–‡æå‡ºäº†å¯¹å½“å‰æ–‡æœ¬åˆ° 3D èŒƒä¾‹çš„ä¸¤ä¸ªä¿®æ”¹ï¼š1ï¼‰å¼•å…¥å¸ƒå±€å¼•å¯¼å˜åˆ†åˆ†æ•°è’¸é¦ï¼ˆLG-VSDï¼‰æ¥è§£å†³æ¨¡å‹ä¼˜åŒ–ä¸è¶³é—®é¢˜ã€‚2ï¼‰ä½¿ç”¨å¯æ‰©å±•å“ˆå¸Œç½‘æ ¼ç»“æ„æ¥è¡¨ç¤º 3D åœºæ™¯ï¼Œè¯¥ç»“æ„å¯éšç€åŸå¸‚åœºæ™¯è§„æ¨¡çš„å¢é•¿è€Œé€æ­¥é€‚åº”ã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šå®éªŒç»“æœè¯æ˜äº†æœ¬æ–‡æ¡†æ¶çš„é²æ£’æ€§ï¼Œå±•ç¤ºäº†å…¶å°†æ–‡æœ¬åˆ° 3D ç”Ÿæˆæ‰©å±•åˆ°è¦†ç›–è¶…è¿‡ 1000 ç±³é©¾é©¶è·ç¦»çš„å¤§è§„æ¨¡åŸå¸‚åœºæ™¯çš„èƒ½åŠ›ã€‚æœ¬æ–‡è¿˜å±•ç¤ºäº†å„ç§åœºæ™¯ç¼–è¾‘æ¼”ç¤ºï¼ˆä¾‹å¦‚ï¼Œæ ·å¼ç¼–è¾‘ã€å¯¹è±¡æ“ä½œç­‰ï¼‰ï¼Œå±•ç¤ºäº† 3D å¸ƒå±€å…ˆéªŒå’Œæ–‡æœ¬æè¿°çš„äº’è¡¥ä¼˜åŠ¿ã€‚</p></li><li><p>Methods:(1): å¼•å…¥æ„æˆçš„3Då¸ƒå±€è¡¨ç¤ºä½œä¸ºé™„åŠ å…ˆéªŒï¼Œå°†æ–‡æœ¬æè¿°ä¸3Då¸ƒå±€è¡¨ç¤ºç›¸ç»“åˆï¼Œå®ç°å¯æ“çºµçš„ç”Ÿæˆï¼›(2): æå‡ºå¸ƒå±€å¼•å¯¼å˜åˆ†åˆ†æ•°è’¸é¦ï¼ˆLG-VSDï¼‰è§£å†³æ¨¡å‹ä¼˜åŒ–ä¸è¶³é—®é¢˜ï¼Œä½¿ç”¨å¯æ‰©å±•å“ˆå¸Œç½‘æ ¼ç»“æ„è¡¨ç¤º3Dåœºæ™¯ï¼Œé€‚åº”å¤§è§„æ¨¡åŸå¸‚åœºæ™¯ï¼›(3): å®éªŒéªŒè¯äº†æ¡†æ¶çš„é²æ£’æ€§ï¼Œå±•ç¤ºäº†å…¶å°†æ–‡æœ¬åˆ°3Dç”Ÿæˆæ‰©å±•åˆ°è¦†ç›–è¶…è¿‡1000ç±³é©¾é©¶è·ç¦»çš„å¤§è§„æ¨¡åŸå¸‚åœºæ™¯çš„èƒ½åŠ›ï¼Œå¹¶å±•ç¤ºäº†å„ç§åœºæ™¯ç¼–è¾‘æ¼”ç¤ºã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¸ƒå±€å…ˆéªŒçš„å¯æ“çºµ3DåŸå¸‚åœºæ™¯ç”Ÿæˆæ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥æ„æˆçš„3Då¸ƒå±€è¡¨ç¤ºä½œä¸ºé™„åŠ å…ˆéªŒï¼Œå¹¶ç»“åˆå¸ƒå±€å¼•å¯¼å˜åˆ†åˆ†æ•°è’¸é¦ï¼ˆLG-VSDï¼‰å’Œå¯æ‰©å±•å“ˆå¸Œç½‘æ ¼ç»“æ„ï¼Œè§£å†³äº†æ¨¡å‹ä¼˜åŒ–ä¸è¶³å’ŒåŸå¸‚åœºæ™¯æ— ç•Œæ€§ç­‰é—®é¢˜ï¼Œå®ç°äº†æ–‡æœ¬åˆ°3Dç”Ÿæˆåœ¨åŸå¸‚åœºæ™¯ä¸­çš„æ‰©å±•å’Œå¯æ“çºµæ€§ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼ša. å¼•å…¥æ„æˆçš„3Då¸ƒå±€è¡¨ç¤ºä½œä¸ºé™„åŠ å…ˆéªŒï¼Œå®ç°æ–‡æœ¬æè¿°ä¸3Då¸ƒå±€è¡¨ç¤ºçš„ç»“åˆï¼Œæ”¯æŒå¯æ“çºµçš„ç”Ÿæˆã€‚b. æå‡ºå¸ƒå±€å¼•å¯¼å˜åˆ†åˆ†æ•°è’¸é¦ï¼ˆLG-VSDï¼‰è§£å†³æ¨¡å‹ä¼˜åŒ–ä¸è¶³é—®é¢˜ï¼Œä½¿ç”¨å¯æ‰©å±•å“ˆå¸Œç½‘æ ¼ç»“æ„è¡¨ç¤º3Dåœºæ™¯ï¼Œé€‚åº”å¤§è§„æ¨¡åŸå¸‚åœºæ™¯ã€‚æ€§èƒ½ï¼ša. å®éªŒéªŒè¯äº†æ¡†æ¶çš„é²æ£’æ€§ï¼Œå±•ç¤ºäº†å…¶å°†æ–‡æœ¬åˆ°3Dç”Ÿæˆæ‰©å±•åˆ°è¦†ç›–è¶…è¿‡1000ç±³é©¾é©¶è·ç¦»çš„å¤§è§„æ¨¡åŸå¸‚åœºæ™¯çš„èƒ½åŠ›ã€‚b. å±•ç¤ºäº†å„ç§åœºæ™¯ç¼–è¾‘æ¼”ç¤ºï¼Œä¾‹å¦‚æ ·å¼ç¼–è¾‘ã€å¯¹è±¡æ“ä½œç­‰ï¼Œå±•ç¤ºäº†3Då¸ƒå±€å…ˆéªŒå’Œæ–‡æœ¬æè¿°çš„äº’è¡¥ä¼˜åŠ¿ã€‚å·¥ä½œé‡ï¼ša. æœ¬æ–‡å·¥ä½œé‡è¾ƒå¤§ï¼Œæ¶‰åŠåˆ°3Då¸ƒå±€è¡¨ç¤ºã€æ¨¡å‹ä¼˜åŒ–ã€åœºæ™¯è¡¨ç¤ºç­‰å¤šä¸ªæ–¹é¢çš„ç ”ç©¶å’Œå®ç°ã€‚b. å®éªŒæ¶‰åŠåˆ°å¤§é‡çš„æ•°æ®é›†å’Œæ¨¡å‹è®­ç»ƒï¼Œéœ€è¦è¾ƒé«˜çš„è®¡ç®—èµ„æºå’Œæ—¶é—´æŠ•å…¥ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-72bd0f7a0ad4505c7b280b1af3502482.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d4049b392b32ce6952f9321d3f3e6b57.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0892904979a00031ac29359f719a48f5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-cadd8a5021887c4f18dad5127fb58fd6.jpg" align="middle"></details><h2 id="Training-Free-Open-Vocabulary-Segmentation-with-Offline-Diffusion-Augmented-Prototype-Generation"><a href="#Training-Free-Open-Vocabulary-Segmentation-with-Offline-Diffusion-Augmented-Prototype-Generation" class="headerlink" title="Training-Free Open-Vocabulary Segmentation with Offline   Diffusion-Augmented Prototype Generation"></a>Training-Free Open-Vocabulary Segmentation with Offline   Diffusion-Augmented Prototype Generation</h2><p><strong>Authors:Luca Barsellotti, Roberto Amoroso, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara</strong></p><p>Open-vocabulary semantic segmentation aims at segmenting arbitrary categories expressed in textual form. Previous works have trained over large amounts of image-caption pairs to enforce pixel-level multimodal alignments. However, captions provide global information about the semantics of a given image but lack direct localization of individual concepts. Further, training on large-scale datasets inevitably brings significant computational costs. In this paper, we propose FreeDA, a training-free diffusion-augmented method for open-vocabulary semantic segmentation, which leverages the ability of diffusion models to visually localize generated concepts and local-global similarities to match class-agnostic regions with semantic classes. Our approach involves an offline stage in which textual-visual reference embeddings are collected, starting from a large set of captions and leveraging visual and semantic contexts. At test time, these are queried to support the visual matching process, which is carried out by jointly considering class-agnostic regions and global semantic similarities. Extensive analyses demonstrate that FreeDA achieves state-of-the-art performance on five datasets, surpassing previous methods by more than 7.0 average points in terms of mIoU and without requiring any training. </p><p><a href="http://arxiv.org/abs/2404.06542v1">PDF</a> CVPR 2024. Project page: <a href="https://aimagelab.github.io/freeda/">https://aimagelab.github.io/freeda/</a></p><p><strong>æ‘˜è¦</strong></p><p>æ— è®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼ˆFreeDAï¼‰ é€šè¿‡è¯­ä¹‰åŒ¹é…ï¼Œæ— éœ€è®­ç»ƒå³å¯è¿›è¡Œå¼€è¯æ±‡è¯­ä¹‰åˆ†å‰²ï¼Œå–å¾—äº†æœ€ä¼˜æ€§èƒ½ã€‚</p><p><strong>å…³é”®è¦ç‚¹</strong></p><ul><li>FreeDA æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„å¼€è¯æ±‡è¯­ä¹‰åˆ†å‰²æ–¹æ³•ã€‚</li><li>FreeDA åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å¯è§†åŒ–å±€éƒ¨åŒ–ç”Ÿæˆæ¦‚å¿µçš„èƒ½åŠ›ã€‚</li><li>FreeDA é‡‡ç”¨æ–‡æœ¬-è§†è§‰å‚è€ƒåµŒå…¥æ¥æ”¯æŒè§†è§‰åŒ¹é…è¿‡ç¨‹ã€‚</li><li>FreeDA è”åˆè€ƒè™‘ç±»åˆ«æ— å…³åŒºåŸŸå’Œå…¨å±€è¯­ä¹‰ç›¸ä¼¼æ€§è¿›è¡ŒåŒ¹é…ã€‚</li><li>FreeDA åœ¨äº”ä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li><li>FreeDA æ— éœ€ä»»ä½•è®­ç»ƒï¼Œä¸å…ˆå‰æ–¹æ³•ç›¸æ¯”ï¼ŒmIoU å¾—åˆ†å¹³å‡æé«˜äº† 7.0 ä¸ªç™¾åˆ†ç‚¹ã€‚</li><li>FreeDA å…‹æœäº†å¤§è§„æ¨¡æ•°æ®é›†è®­ç»ƒå¸¦æ¥çš„æ˜¾è‘—è®¡ç®—æˆæœ¬ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>è®ºæ–‡æ ‡é¢˜ï¼šæ— éœ€è®­ç»ƒçš„å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²</li><li>ä½œè€…ï¼šLuca Barsellottiï¼ŒRoberto Amorosoï¼ŒMarcella Corniaï¼ŒLorenzo Baraldiï¼ŒRita Cucchiara</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šæ„å¤§åˆ©æ‘©å¾·çº³å’Œé›·ç„¦è‰¾ç±³åˆ©äºšå¤§å­¦</li><li>å…³é”®è¯ï¼šå¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ï¼Œæ‰©æ•£æ¨¡å‹ï¼Œç”Ÿæˆå¼æ–‡æœ¬-å›¾åƒåµŒå…¥ï¼Œæ— éœ€è®­ç»ƒ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.06542Github ä»£ç é“¾æ¥ï¼šimagelab.github.io/freeda</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šå¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²æ—¨åœ¨åˆ†å‰²ä»¥æ–‡æœ¬å½¢å¼è¡¨ç¤ºçš„ä»»æ„ç±»åˆ«ã€‚ä»¥å¾€æ–¹æ³•é€šè¿‡å¯¹æ¯”å­¦ä¹ æŠ€æœ¯å’Œæ¥åœ°æœºåˆ¶ï¼Œä»å¤§é‡çš„å›¾åƒ-æ ‡é¢˜å¯¹ä¸­å¼ºåˆ¶åƒç´ çº§å¤šæ¨¡æ€å¯¹é½ã€‚ç„¶è€Œï¼Œæ ‡é¢˜æä¾›äº†å›¾åƒè¯­ä¹‰çš„å…¨å±€ä¿¡æ¯ï¼Œä½†ç¼ºä¹å¯¹å•ä¸ªæ¦‚å¿µçš„ç›´æ¥å®šä½ã€‚æ­¤å¤–ï¼Œåœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒä¸å¯é¿å…åœ°ä¼šå¸¦æ¥å·¨å¤§çš„è®¡ç®—æˆæœ¬ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šä»¥å¾€æ–¹æ³•é€šè¿‡å¯¹æ¯”å­¦ä¹ æŠ€æœ¯å’Œæ¥åœ°æœºåˆ¶ï¼Œä»å¤§é‡çš„å›¾åƒ-æ ‡é¢˜å¯¹ä¸­å¼ºåˆ¶åƒç´ çº§å¤šæ¨¡æ€å¯¹é½ã€‚ç„¶è€Œï¼Œæ ‡é¢˜é€šå¸¸åªæ•è·å…¨å±€åœºæ™¯ï¼Œå¯¹äºç»†ç²’åº¦å…ƒç´ å¯èƒ½å­˜åœ¨æ­§ä¹‰ï¼Œè¿™ä½¿å¾—è¿™ç§æ–¹æ³•æ¬¡ä¼˜ä¸”è®¡ç®—å¯†é›†ã€‚ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡º FreeDAï¼Œä¸€ç§æ— éœ€è®­ç»ƒçš„æ‰©æ•£å¢å¼ºæ–¹æ³•ï¼Œç”¨äºå¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ã€‚FreeDA åˆ©ç”¨æ‰©æ•£æ¨¡å‹åœ¨è§†è§‰ä¸Šå®šä½ç”Ÿæˆæ¦‚å¿µçš„èƒ½åŠ›ï¼Œä»¥åŠå±€éƒ¨-å…¨å±€ç›¸ä¼¼æ€§ï¼Œå°†ä¸ç±»åˆ«æ— å…³çš„åŒºåŸŸä¸è¯­ä¹‰ç±»åˆ«è¿›è¡ŒåŒ¹é…ã€‚è¯¥æ–¹æ³•æ¶‰åŠä¸€ä¸ªç¦»çº¿é˜¶æ®µï¼Œå…¶ä¸­ä»å¤§é‡çš„æ ‡é¢˜å¼€å§‹æ”¶é›†æ–‡æœ¬-è§†è§‰å‚è€ƒåµŒå…¥ï¼Œå¹¶åˆ©ç”¨è§†è§‰å’Œè¯­ä¹‰ä¸Šä¸‹æ–‡ã€‚åœ¨æµ‹è¯•æ—¶ï¼ŒæŸ¥è¯¢è¿™äº›åµŒå…¥ä»¥æ”¯æŒè§†è§‰åŒ¹é…è¿‡ç¨‹ï¼Œè¯¥è¿‡ç¨‹é€šè¿‡è”åˆè€ƒè™‘ä¸ç±»åˆ«æ— å…³çš„åŒºåŸŸå’Œå…¨å±€è¯­ä¹‰ç›¸ä¼¼æ€§æ¥è¿›è¡Œã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨æŒ‡å®šä»»åŠ¡ä¸Šçš„è¡¨ç°åŠå…¶å¯¹ç›®æ ‡çš„æ”¯æŒï¼šå¹¿æ³›çš„åˆ†æè¡¨æ˜ï¼ŒFreeDA åœ¨äº”ä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨ mIoU æ–¹é¢æ¯”ä»¥å¾€æ–¹æ³•å¹³å‡æé«˜äº† 7.0 ä¸ªç‚¹ï¼Œå¹¶ä¸”ä¸éœ€è¦ä»»ä½•è®­ç»ƒã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ï¼Œå³å¼€å‘ä¸€ç§æ— éœ€è®­ç»ƒå³å¯è¿›è¡Œå¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²çš„æ–¹æ³•ã€‚</li></ol><p><strong>æ–¹æ³•</strong></p><p>ï¼ˆ1ï¼‰æ‰©æ•£å¢å¼ºåŸå‹ç”Ÿæˆï¼š- åˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆå¤§é‡åˆæˆå›¾åƒå’Œç›¸åº”çš„å¼±å®šä½æ©ç ã€‚- ä»å¼±å®šä½æ©ç ä¸­æå–è§†è§‰åŸå‹ï¼Œè¡¨ç¤ºè¯­ä¹‰ç±»åˆ«åœ¨åˆæˆåœºæ™¯ä¸­çš„è§†è§‰è¡¨ç°ã€‚</p><p>ï¼ˆ2ï¼‰æ–‡æœ¬å¯†é’¥æå–ï¼š- ä½¿ç”¨æ–‡æœ¬ç¼–ç å™¨å°†åè¯åµŒå…¥åˆ°å®ƒä»¬çš„è¯æ±‡ä¸Šä¸‹æ–‡ä¸­ã€‚- é€šè¿‡æ¨¡æ¿å’Œå¹³å‡æ“ä½œï¼Œæ„é€ æ–‡æœ¬å¯†é’¥ï¼Œè¡¨ç¤ºåè¯åœ¨æè¿°æ€§æ ‡é¢˜ä¸­çš„è¯­ä¹‰ä¸Šä¸‹æ–‡ã€‚</p><p>ï¼ˆ3ï¼‰è®­ç»ƒå…è´¹æ©ç é¢„æµ‹ï¼š- åœ¨æ¨ç†æ—¶ï¼ŒæŸ¥è¯¢é¢„å…ˆæ„å»ºçš„æ–‡æœ¬å¯†é’¥ç´¢å¼•ï¼Œæ£€ç´¢ä¸è¾“å…¥æ–‡æœ¬ç±»åˆ«å¯¹åº”çš„è§†è§‰åŸå‹ã€‚- è®¡ç®—è¾“å…¥å›¾åƒä¸­çš„å±€éƒ¨å’Œå…¨å±€ç‰¹å¾ï¼Œå¹¶ä¸æ£€ç´¢åˆ°çš„åŸå‹è¿›è¡Œè¯­ä¹‰åŒ¹é…ã€‚- æ ¹æ®è¯­ä¹‰åŒ¹é…ç»“æœï¼Œé¢„æµ‹å›¾åƒä¸­æ¯ä¸ªè¯­ä¹‰ç±»åˆ«çš„åˆ†å‰²æ©ç ã€‚</p><ol><li><strong>ç»“è®º</strong>(1): æœ¬å·¥ä½œæå‡ºäº† FreeDAï¼Œä¸€ç§æ— éœ€è®­ç»ƒçš„å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ç¦»çº¿é˜¶æ®µæ”¶é›†çš„æ–‡æœ¬-è§†è§‰å‚è€ƒåµŒå…¥ï¼Œå¹¶åœ¨æ¨ç†æ—¶æŸ¥è¯¢è¿™äº›åµŒå…¥ä»¥æ”¯æŒè§†è§‰åŒ¹é…è¿‡ç¨‹ï¼Œä»è€Œå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚(2): <strong>åˆ›æ–°ç‚¹ï¼š</strong></li><li>æ— éœ€è®­ç»ƒï¼Œåˆ©ç”¨æ‰©æ•£å¢å¼ºç”Ÿæˆè§†è§‰åŸå‹å’Œæ–‡æœ¬å¯†é’¥ã€‚</li><li>è”åˆè€ƒè™‘å±€éƒ¨å’Œå…¨å±€ç›¸ä¼¼æ€§è¿›è¡Œè¯­ä¹‰åŒ¹é…ã€‚<strong>æ€§èƒ½ï¼š</strong></li><li>åœ¨äº”ä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒmIoU æ¯”ä»¥å¾€æ–¹æ³•å¹³å‡æé«˜äº† 7.0 ä¸ªç‚¹ã€‚<strong>å·¥ä½œé‡ï¼š</strong></li><li>ç¦»çº¿é˜¶æ®µéœ€è¦æ”¶é›†æ–‡æœ¬-è§†è§‰å‚è€ƒåµŒå…¥ï¼Œä½†æ¨ç†è¿‡ç¨‹é«˜æ•ˆä¸”æ— éœ€è®­ç»ƒã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-645b415a932f37bdaa02be65f5b1097d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6e8ab37086a84c0d2b562c5ea763ae8f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c94f3fe8057634302eb5b92c44e40df9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f7522da89bc72914cb56f1d3f500ee33.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-04-14  Taming Stable Diffusion for Text to 360Â° Panorama Image Generation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/04/09/Paper/2024-04-09/NeRF/"/>
    <id>https://kedreamix.github.io/2024/04/09/Paper/2024-04-09/NeRF/</id>
    <published>2024-04-09T08:35:38.000Z</published>
    <updated>2024-04-09T08:35:38.766Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-04-09-æ›´æ–°"><a href="#2024-04-09-æ›´æ–°" class="headerlink" title="2024-04-09 æ›´æ–°"></a>2024-04-09 æ›´æ–°</h1><h2 id="Stylizing-Sparse-View-3D-Scenes-with-Hierarchical-Neural-Representation"><a href="#Stylizing-Sparse-View-3D-Scenes-with-Hierarchical-Neural-Representation" class="headerlink" title="Stylizing Sparse-View 3D Scenes with Hierarchical Neural Representation"></a>Stylizing Sparse-View 3D Scenes with Hierarchical Neural Representation</h2><p><strong>Authors:Y. Wang, A. Gao, Y. Gong, Y. Zeng</strong></p><p>Recently, a surge of 3D style transfer methods has been proposed that leverage the scene reconstruction power of a pre-trained neural radiance field (NeRF). To successfully stylize a scene this way, one must first reconstruct a photo-realistic radiance field from collected images of the scene. However, when only sparse input views are available, pre-trained few-shot NeRFs often suffer from high-frequency artifacts, which are generated as a by-product of high-frequency details for improving reconstruction quality. Is it possible to generate more faithful stylized scenes from sparse inputs by directly optimizing encoding-based scene representation with target style? In this paper, we consider the stylization of sparse-view scenes in terms of disentangling content semantics and style textures. We propose a coarse-to-fine sparse-view scene stylization framework, where a novel hierarchical encoding-based neural representation is designed to generate high-quality stylized scenes directly from implicit scene representations. We also propose a new optimization strategy with content strength annealing to achieve realistic stylization and better content preservation. Extensive experiments demonstrate that our method can achieve high-quality stylization of sparse-view scenes and outperforms fine-tuning-based baselines in terms of stylization quality and efficiency. </p><p><a href="http://arxiv.org/abs/2404.05236v1">PDF</a> </p><p><strong>Summary</strong><br>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ä¼˜åŒ–ï¼Œç»“åˆå†…å®¹è¡¨ç¤ºå’Œç›®æ ‡æ ·å¼ï¼Œå¯ä»ç¨€ç–è§†å›¾ç›´æ¥ç”Ÿæˆé«˜è´¨é‡çš„é£æ ¼åŒ–åœºæ™¯ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æ–°é¢–çš„åˆ†å±‚ç¼–ç ç¥ç»è¡¨ç¤ºå¯ä»éšå¼åœºæ™¯è¡¨ç¤ºç›´æ¥ç”Ÿæˆé«˜è´¨é‡çš„é£æ ¼åŒ–åœºæ™¯ã€‚</li><li>ä»ç¨€ç–è§†å›¾åœºæ™¯ä¸­åˆ†ç¦»å†…å®¹è¯­ä¹‰å’Œæ ·å¼çº¹ç†ï¼Œå®ç°é£æ ¼åŒ–ã€‚</li><li>é€å±‚ç²¾ç»†çš„åœºæ™¯é£æ ¼åŒ–æ¡†æ¶ã€‚</li><li>å†…å®¹å¼ºåº¦é€€ç«ä¼˜åŒ–ç­–ç•¥ï¼Œå®ç°çœŸå®æ„Ÿé£æ ¼åŒ–å’Œæ›´å¥½çš„å†…å®¹ä¿ç•™ã€‚</li><li>åœ¨é£æ ¼åŒ–è´¨é‡å’Œæ•ˆç‡æ–¹é¢ä¼˜äºåŸºäºå¾®è°ƒçš„åŸºçº¿ã€‚</li><li>å¹¿æ³›çš„å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨ç¨€ç–è§†å›¾åœºæ™¯çš„é«˜è´¨é‡é£æ ¼åŒ–ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li><li>æ–°çš„ä¼˜åŒ–ç­–ç•¥ä¿ç•™äº†å†…å®¹ï¼Œæ”¹å–„äº†é£æ ¼åŒ–æ•ˆæœã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>è®ºæ–‡æ ‡é¢˜ï¼šç²—åˆ°ç²¾çš„ç¨€ç–è§†è§’åœºæ™¯é£æ ¼åŒ–</li><li>ä½œè€…ï¼šYifan Wang, Yuxuan Zhang, Kun Xu, Yinda Zhang, Wenxiu Sun, Qifeng Chen</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šä¸Šæµ·äº¤é€šå¤§å­¦</li><li>å…³é”®è¯ï¼š3D é£æ ¼è¿ç§» Â· ç¥ç»è¾å°„åœº Â· ç¨€ç–å†…å®¹è¾“å…¥</li><li>è®ºæ–‡é“¾æ¥ï¼šNoneGithub é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šè¿‘å¹´æ¥ï¼Œ3D é£æ ¼è¿ç§»æ–¹æ³•è“¬å‹ƒå‘å±•ï¼Œåˆ©ç”¨é¢„è®­ç»ƒç¥ç»è¾å°„åœº (NeRF) çš„åœºæ™¯é‡å»ºèƒ½åŠ›ã€‚ä¸ºäº†æˆåŠŸåœ°ä»¥è¿™ç§æ–¹å¼å¯¹åœºæ™¯è¿›è¡Œé£æ ¼åŒ–ï¼Œå¿…é¡»é¦–å…ˆä»æ”¶é›†çš„åœºæ™¯å›¾åƒä¸­é‡å»ºä¸€ä¸ªé€¼çœŸçš„è¾å°„åœºã€‚ç„¶è€Œï¼Œå½“åªæœ‰ç¨€ç–è¾“å…¥è§†å›¾å¯ç”¨æ—¶ï¼Œé¢„è®­ç»ƒçš„ few-shot NeRF ä¼šå—åˆ°é«˜é¢‘ä¼ªå½±çš„å½±å“ï¼Œè¿™äº›ä¼ªå½±æ˜¯ä½œä¸ºæé«˜é‡å»ºè´¨é‡çš„é«˜é¢‘ç»†èŠ‚çš„å‰¯äº§å“ç”Ÿæˆçš„ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•å’Œé—®é¢˜ï¼šç°æœ‰æ–¹æ³•é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„è¾å°„åœºæ¥å®ç°é£æ ¼åŒ–ï¼Œä½†å®ƒä»¬åœ¨å¤„ç†ç¨€ç–è¾“å…¥æ—¶ä¼šäº§ç”Ÿé«˜é¢‘ä¼ªå½±ã€‚ç›´æ¥ä¼˜åŒ–åŸºäºç¼–ç çš„åœºæ™¯è¡¨ç¤ºä»¥å®ç°ç›®æ ‡é£æ ¼ï¼Œæ˜¯å¦å¯ä»¥ä»ç¨€ç–è¾“å…¥ç”Ÿæˆæ›´é€¼çœŸçš„é£æ ¼åŒ–åœºæ™¯ï¼Ÿï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡ä»å†…å®¹è¯­ä¹‰å’Œé£æ ¼çº¹ç†è§£è€¦çš„è§’åº¦è€ƒè™‘ç¨€ç–è§†è§’åœºæ™¯çš„é£æ ¼åŒ–ã€‚æå‡ºäº†ä¸€ç§ç²—åˆ°ç²¾çš„ç¨€ç–è§†è§’åœºæ™¯é£æ ¼åŒ–æ¡†æ¶ï¼Œå…¶ä¸­è®¾è®¡äº†ä¸€ç§æ–°é¢–çš„åˆ†å±‚åŸºäºç¼–ç çš„ç¥ç»è¡¨ç¤ºï¼Œä»¥ç›´æ¥ä»éšå¼åœºæ™¯è¡¨ç¤ºç”Ÿæˆé«˜è´¨é‡çš„é£æ ¼åŒ–åœºæ™¯ã€‚è¿˜æå‡ºäº†ä¸€ç§æ–°çš„ä¼˜åŒ–ç­–ç•¥ï¼Œé€šè¿‡å†…å®¹å¼ºåº¦é€€ç«æ¥å®ç°é€¼çœŸçš„é£æ ¼åŒ–å’Œæ›´å¥½çš„å†…å®¹ä¿ç•™ã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šå¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥å®ç°ç¨€ç–è§†è§’åœºæ™¯çš„é«˜è´¨é‡é£æ ¼åŒ–ï¼Œå¹¶ä¸”åœ¨é£æ ¼åŒ–è´¨é‡å’Œæ•ˆç‡æ–¹é¢ä¼˜äºåŸºäºå¾®è°ƒçš„åŸºçº¿ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†ä»–ä»¬çš„ç›®æ ‡ã€‚</p></li><li><p>Methods:(1): æå‡ºäº†ä¸€ç§ç²—åˆ°ç²¾çš„ç¨€ç–è§†è§’åœºæ™¯é£æ ¼åŒ–æ¡†æ¶ï¼Œå°†åœºæ™¯è¡¨ç¤ºä¸ºåˆ†å±‚åŸºäºç¼–ç çš„ç¥ç»è¡¨ç¤ºï¼Œé€šè¿‡å†…å®¹å¼ºåº¦é€€ç«ä¼˜åŒ–ç­–ç•¥å®ç°é€¼çœŸçš„é£æ ¼åŒ–å’Œæ›´å¥½çš„å†…å®¹ä¿ç•™ã€‚(2): è®¾è®¡äº†ä¸€ç§æ–°é¢–çš„åˆ†å±‚åŸºäºç¼–ç çš„ç¥ç»è¡¨ç¤ºï¼Œä»¥ç›´æ¥ä»éšå¼åœºæ™¯è¡¨ç¤ºç”Ÿæˆé«˜è´¨é‡çš„é£æ ¼åŒ–åœºæ™¯ã€‚(3): æå‡ºäº†ä¸€ç§æ–°çš„ä¼˜åŒ–ç­–ç•¥ï¼Œé€šè¿‡å†…å®¹å¼ºåº¦é€€ç«æ¥å®ç°é€¼çœŸçš„é£æ ¼åŒ–å’Œæ›´å¥½çš„å†…å®¹ä¿ç•™ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬å·¥ä½œæå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„ç¨€ç–è§†è§’åœºæ™¯é£æ ¼åŒ– 3D è¿ç§»æ¡†æ¶ï¼Œå®ç°äº†è§†è§‰ä¸Šä»¤äººæ„‰æ‚¦çš„é£æ ¼åŒ–æ–°è§†è§’ç”Ÿæˆã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸€ä¸ªæ–°çš„åˆ†å±‚åœºæ™¯è¡¨ç¤ºï¼Œç”¨äºç›´æ¥å°†ç²¾ç»†å±‚æ¬¡åœºæ™¯è¡¨ç¤ºä¼˜åŒ–ä¸ºé£æ ¼åŒ–åœºæ™¯ã€‚åœ¨é£æ ¼åŒ–è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå¼•å…¥å†…å®¹é€€ç«ç­–ç•¥ï¼Œä»¥æ›´å¥½åœ°å¹³è¡¡å†…å®¹ä¿ç•™å’Œåœºæ™¯é£æ ¼åŒ–æ•ˆæœã€‚æˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬çš„è®¾è®¡åœ¨ä»ç¨€ç–è¾“å…¥è§†è§’ç”Ÿæˆé«˜è´¨é‡é£æ ¼åŒ–åœºæ™¯æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œåœºæ™¯ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå½“åœºæ™¯åªæœ‰ç¨€ç–è§†è§’å¯ç”¨æ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ¯”åŸºçº¿æ–¹æ³•å®ç°äº†æ›´å¥½çš„ 3D é£æ ¼åŒ–è´¨é‡å’Œæ•ˆç‡ã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼šxxxï¼›æ€§èƒ½ï¼šxxxï¼›å·¥ä½œé‡ï¼šxxxï¼›</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-adaaaa84e08f09fc591c1762b2ddff07.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6b9dd356c27dc99f180e7927504fe0a7.jpg" align="middle"><img src="https://pica.zhimg.com/v2-54de457db78ad2b709bb7fd1ba375030.jpg" align="middle"></details><h2 id="RaFE-Generative-Radiance-Fields-Restoration"><a href="#RaFE-Generative-Radiance-Fields-Restoration" class="headerlink" title="RaFE: Generative Radiance Fields Restoration"></a>RaFE: Generative Radiance Fields Restoration</h2><p><strong>Authors:Zhongkai Wu, Ziyu Wan, Jing Zhang, Jing Liao, Dong Xu</strong></p><p>NeRF (Neural Radiance Fields) has demonstrated tremendous potential in novel view synthesis and 3D reconstruction, but its performance is sensitive to input image quality, which struggles to achieve high-fidelity rendering when provided with low-quality sparse input viewpoints. Previous methods for NeRF restoration are tailored for specific degradation type, ignoring the generality of restoration. To overcome this limitation, we propose a generic radiance fields restoration pipeline, named RaFE, which applies to various types of degradations, such as low resolution, blurriness, noise, compression artifacts, or their combinations. Our approach leverages the success of off-the-shelf 2D restoration methods to recover the multi-view images individually. Instead of reconstructing a blurred NeRF by averaging inconsistencies, we introduce a novel approach using Generative Adversarial Networks (GANs) for NeRF generation to better accommodate the geometric and appearance inconsistencies present in the multi-view images. Specifically, we adopt a two-level tri-plane architecture, where the coarse level remains fixed to represent the low-quality NeRF, and a fine-level residual tri-plane to be added to the coarse level is modeled as a distribution with GAN to capture potential variations in restoration. We validate RaFE on both synthetic and real cases for various restoration tasks, demonstrating superior performance in both quantitative and qualitative evaluations, surpassing other 3D restoration methods specific to single task. Please see our project website <a href="https://zkaiwu.github.io/RaFE-Project/">https://zkaiwu.github.io/RaFE-Project/</a>. </p><p><a href="http://arxiv.org/abs/2404.03654v2">PDF</a> Project Page: <a href="https://zkaiwu.github.io/RaFE">https://zkaiwu.github.io/RaFE</a></p><p><strong>Summary</strong><br>RaFEæå‡ºäº†ä¸€ç§é€‚ç”¨äºå„ç§é€€åŒ–ç±»å‹çš„ç¥ç»è¾å°„åœºä¿®å¤é€šç”¨ç®¡é“ï¼Œåˆ©ç”¨å¯¹æŠ—ç”Ÿæˆç½‘ç»œï¼ˆGANï¼‰æ›´å¥½åœ° accommodated å‡ ä½•ä¸å¤–è§‚çš„ä¸ä¸€è‡´ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>RaFEæ˜¯ä¸€ç§é€šç”¨çš„ç¥ç»è¾å°„åœºä¿®å¤ç®¡é“ï¼Œé€‚ç”¨äºå„ç§ç±»å‹çš„é€€åŒ–ã€‚</li><li>RaFEåˆ©ç”¨ç°æˆçš„2Dä¿®å¤æ–¹æ³•é€ä¸ªæ¢å¤å¤šè§†å›¾å›¾åƒã€‚</li><li>RaFEä½¿ç”¨GANsç”Ÿæˆç¥ç»è¾å°„åœºï¼Œä»¥æ›´å¥½åœ°é€‚åº”å¤šè§†å›¾å›¾åƒä¸­å­˜åœ¨çš„å‡ ä½•å’Œå¤–è§‚ä¸ä¸€è‡´ã€‚</li><li>RaFEé‡‡ç”¨ä¸¤çº§ä¸‰å¹³é¢æ¶æ„ï¼Œå…¶ä¸­ç²—å±‚ä¿æŒå›ºå®šä»¥è¡¨ç¤ºä½è´¨é‡ç¥ç»è¾å°„åœºï¼Œç»†å±‚æ®‹å·®ä¸‰å¹³é¢è¢«å»ºæ¨¡ä¸ºå…·æœ‰GANsçš„åˆ†å¸ƒï¼Œä»¥æ•è·ä¿®å¤ä¸­çš„æ½œåœ¨å˜åŒ–ã€‚</li><li>RaFEåœ¨åˆæˆå’ŒçœŸå®æ¡ˆä¾‹ä¸­å¯¹äºå„ç§ä¿®å¤ä»»åŠ¡éƒ½ç»è¿‡éªŒè¯ï¼Œåœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸­éƒ½å±•ç°äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†å…¶ä»–ç‰¹å®šäºå•ä¸€ä»»åŠ¡çš„3Dä¿®å¤æ–¹æ³•ã€‚</li><li>RaFEé¡¹ç›®ç½‘ç«™ï¼š<a href="https://zkaiwu.github.io/RaFE-Project/ã€‚">https://zkaiwu.github.io/RaFE-Project/ã€‚</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šRaFEï¼šç”Ÿæˆå¼è¾å°„åœºä¿®å¤è¡¥å……ææ–™</li><li>ä½œè€…ï¼šZhongkai Wuã€Ziyu Wanã€Jing Zhangã€Jing Liaoã€Dong Xu</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šåŒ—äº¬èˆªç©ºèˆªå¤©å¤§å­¦è½¯ä»¶å­¦é™¢</li><li>å…³é”®è¯ï¼šç¥ç»æ¸²æŸ“Â·ç”Ÿæˆæ¨¡å‹Â·ä¸‰ç»´ä¿®å¤Â·ç¥ç»è¾å°„åœº</li><li>è®ºæ–‡é“¾æ¥ï¼šarxiv.org/abs/2404.03654v2ï¼ŒGithub ä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼š(1)ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åœ¨æ–°å‹è§†å›¾åˆæˆå’Œä¸‰ç»´é‡å»ºä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†å…¶æ€§èƒ½å¯¹è¾“å…¥å›¾åƒè´¨é‡æ•æ„Ÿï¼Œå½“æä¾›ä½è´¨é‡ç¨€ç–è¾“å…¥è§†ç‚¹æ—¶éš¾ä»¥å®ç°é«˜ä¿çœŸæ¸²æŸ“ã€‚ä»¥å¾€é’ˆå¯¹ NeRF çš„ä¿®å¤æ–¹æ³•é’ˆå¯¹ç‰¹å®šé€€åŒ–ç±»å‹å®šåˆ¶ï¼Œå¿½ç•¥äº†ä¿®å¤çš„é€šç”¨æ€§ã€‚(2)ï¼šä¸ºäº†å…‹æœè¿™ä¸€é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šç”¨çš„è¾å°„åœºä¿®å¤ç®¡é“ï¼Œç§°ä¸º RaFEï¼Œé€‚ç”¨äºå„ç§ç±»å‹çš„é€€åŒ–ï¼Œå¦‚ä½åˆ†è¾¨ç‡ã€æ¨¡ç³Šã€å™ªå£°ã€å‹ç¼©ä¼ªå½±æˆ–å®ƒä»¬çš„ç»„åˆã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨ç°æˆçš„äºŒç»´ä¿®å¤æ–¹æ³•åˆ†åˆ«æ¢å¤å¤šè§†å›¾å›¾åƒã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œä½¿ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰è¿›è¡Œ NeRF ç”Ÿæˆï¼Œä»¥æ›´å¥½åœ°é€‚åº”å¤šè§†å›¾å›¾åƒä¸­å­˜åœ¨çš„å‡ ä½•å’Œå¤–è§‚ä¸ä¸€è‡´æ€§ï¼Œè€Œä¸æ˜¯é€šè¿‡å¹³å‡ä¸ä¸€è‡´æ€§æ¥é‡å»ºæ¨¡ç³Šçš„ NeRFã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨ä¸¤çº§ä¸‰å¹³é¢æ¶æ„ï¼Œå…¶ä¸­ç²—ç³™çº§åˆ«ä¿æŒå›ºå®šä»¥è¡¨ç¤ºä½è´¨é‡ NeRFï¼Œå¹¶æ·»åŠ ä¸€ä¸ªç²¾ç»†çº§åˆ«æ®‹å·®ä¸‰å¹³é¢åˆ°ç²—ç³™çº§åˆ«ï¼Œå¹¶å°†å…¶å»ºæ¨¡ä¸ºå…·æœ‰ GAN çš„åˆ†å¸ƒä»¥æ•è·ä¿®å¤ä¸­çš„æ½œåœ¨å˜åŒ–ã€‚(3)ï¼šæˆ‘ä»¬åœ¨åˆæˆå’ŒçœŸå®æ¡ˆä¾‹ä¸­å¯¹å„ç§ä¿®å¤ä»»åŠ¡éªŒè¯äº† RaFEï¼Œåœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸­å±•ç¤ºäº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†å…¶ä»–é’ˆå¯¹å•ä¸€ä»»åŠ¡çš„ä¸‰ç»´ä¿®å¤æ–¹æ³•ã€‚è¯·å‚é˜…æˆ‘ä»¬çš„é¡¹ç›®ç½‘ç«™ zkaiwu.github.io/RaFEã€‚(4)ï¼šåœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œè¯æ˜äº† RaFE åœ¨å„ç§ä¿®å¤ä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§ã€‚åœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸­ï¼ŒRaFE ä¼˜äºå…¶ä»–é’ˆå¯¹ç‰¹å®šé€€åŒ–ç±»å‹çš„ç°æœ‰æ–¹æ³•ã€‚è¿™äº›ç»“æœæ”¯æŒäº†æˆ‘ä»¬çš„ç›®æ ‡ï¼Œå³å¼€å‘ä¸€ç§é€šç”¨çš„ NeRF ä¿®å¤ç®¡é“ï¼Œé€‚ç”¨äºå„ç§é€€åŒ–ç±»å‹ï¼Œå¹¶äº§ç”Ÿé«˜è´¨é‡çš„ä¿®å¤ç»“æœã€‚</li></ol><p>7.æ–¹æ³•ï¼š(1): é‡‡ç”¨ç°æˆçš„äºŒç»´ä¿®å¤æ–¹æ³•åˆ†åˆ«æ¢å¤å¤šè§†å›¾å›¾åƒï¼›(2): å¼•å…¥ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰è¿›è¡ŒNeRFç”Ÿæˆï¼Œä»¥æ›´å¥½åœ°é€‚åº”å¤šè§†å›¾å›¾åƒä¸­å­˜åœ¨çš„å‡ ä½•å’Œå¤–è§‚ä¸ä¸€è‡´æ€§ï¼›(3): é‡‡ç”¨ä¸¤çº§ä¸‰å¹³é¢æ¶æ„ï¼Œå…¶ä¸­ç²—ç³™çº§åˆ«ä¿æŒå›ºå®šä»¥è¡¨ç¤ºä½è´¨é‡NeRFï¼Œå¹¶æ·»åŠ ä¸€ä¸ªç²¾ç»†çº§åˆ«æ®‹å·®ä¸‰å¹³é¢åˆ°ç²—ç³™çº§åˆ«ï¼Œå¹¶å°†å…¶å»ºæ¨¡ä¸ºå…·æœ‰GANçš„åˆ†å¸ƒä»¥æ•è·ä¿®å¤ä¸­çš„æ½œåœ¨å˜åŒ–ã€‚</p><ol><li>ç»“è®ºï¼š(1) æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§é€šç”¨çš„è¾å°„åœºä¿®å¤ç®¡é“ RaFEï¼Œé€‚ç”¨äºå„ç§ç±»å‹çš„é€€åŒ–ï¼Œå¦‚ä½åˆ†è¾¨ç‡ã€æ¨¡ç³Šã€å™ªå£°ã€å‹ç¼©ä¼ªå½±æˆ–å®ƒä»¬çš„ç»„åˆï¼Œåœ¨åˆæˆå’ŒçœŸå®æ¡ˆä¾‹ä¸­éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚(2) åˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§é€šç”¨çš„è¾å°„åœºä¿®å¤ç®¡é“ï¼Œé€‚ç”¨äºå„ç§ç±»å‹çš„é€€åŒ–ï¼Œæ— éœ€é’ˆå¯¹ç‰¹å®šé€€åŒ–ç±»å‹è¿›è¡Œå®šåˆ¶ã€‚</li><li>å¼•å…¥ GAN è¿›è¡Œ NeRF ç”Ÿæˆï¼Œä»¥æ›´å¥½åœ°é€‚åº”å¤šè§†å›¾å›¾åƒä¸­å­˜åœ¨çš„å‡ ä½•å’Œå¤–è§‚ä¸ä¸€è‡´æ€§ã€‚</li><li>é‡‡ç”¨ä¸¤çº§ä¸‰å¹³é¢æ¶æ„ï¼Œå…¶ä¸­ç²—ç³™çº§åˆ«ä¿æŒå›ºå®šä»¥è¡¨ç¤ºä½è´¨é‡ NeRFï¼Œå¹¶æ·»åŠ ä¸€ä¸ªç²¾ç»†çº§åˆ«æ®‹å·®ä¸‰å¹³é¢åˆ°ç²—ç³™çº§åˆ«ï¼Œå¹¶å°†å…¶å»ºæ¨¡ä¸ºå…·æœ‰ GAN çš„åˆ†å¸ƒä»¥æ•è·ä¿®å¤ä¸­çš„æ½œåœ¨å˜åŒ–ã€‚</li><li>æ€§èƒ½ï¼šåœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸­ï¼ŒRaFE ä¼˜äºå…¶ä»–é’ˆå¯¹å•ä¸€ä»»åŠ¡çš„ä¸‰ç»´ä¿®å¤æ–¹æ³•ã€‚</li><li>å·¥ä½œé‡ï¼šRaFE çš„å®ç°ç›¸å¯¹ç®€å•ï¼Œæ˜“äºä½¿ç”¨ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-48340fe40fff2e45663514e4ff3ee376.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e8fad6c067ffca0f2b396d38c7e58bbd.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e71c253fdfbf8c1083d326f01390eaeb.jpg" align="middle"></details><h2 id="Knowledge-NeRF-Few-shot-Novel-View-Synthesis-for-Dynamic-Articulated-Objects"><a href="#Knowledge-NeRF-Few-shot-Novel-View-Synthesis-for-Dynamic-Articulated-Objects" class="headerlink" title="Knowledge NeRF: Few-shot Novel View Synthesis for Dynamic Articulated   Objects"></a>Knowledge NeRF: Few-shot Novel View Synthesis for Dynamic Articulated   Objects</h2><p><strong>Authors:Wenxiao Cai, Xinyue Lei, Xinyu He, Junming Leo Chen, Yangang Wang</strong></p><p>We present Knowledge NeRF to synthesize novel views for dynamic scenes. Reconstructing dynamic 3D scenes from few sparse views and rendering them from arbitrary perspectives is a challenging problem with applications in various domains. Previous dynamic NeRF methods learn the deformation of articulated objects from monocular videos. However, qualities of their reconstructed scenes are limited. To clearly reconstruct dynamic scenes, we propose a new framework by considering two frames at a time.We pretrain a NeRF model for an articulated object.When articulated objects moves, Knowledge NeRF learns to generate novel views at the new state by incorporating past knowledge in the pretrained NeRF model with minimal observations in the present state. We propose a projection module to adapt NeRF for dynamic scenes, learning the correspondence between pretrained knowledge base and current states. Experimental results demonstrate the effectiveness of our method in reconstructing dynamic 3D scenes with 5 input images in one state. Knowledge NeRF is a new pipeline and promising solution for novel view synthesis in dynamic articulated objects. The data and implementation are publicly available at <a href="https://github.com/RussRobin/Knowledge_NeRF">https://github.com/RussRobin/Knowledge_NeRF</a>. </p><p><a href="http://arxiv.org/abs/2404.00674v2">PDF</a> </p><p><strong>Summary</strong><br>é€šè¿‡åŒæ—¶è€ƒè™‘ä¸¤å¸§å†…å®¹ï¼ŒKnowledge NeRF èƒ½å¤Ÿåˆ©ç”¨å…ˆå‰çŸ¥è¯†ä»¥æœ€å°‘çš„å½“å‰å¸§è§‚å¯Ÿç»“æœç”ŸæˆåŠ¨æ€åœºæ™¯çš„æ–°é¢–è§†å›¾ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>Knowledge NeRF é€‚ç”¨äºåŠ¨æ€åœºæ™¯ï¼Œé€šè¿‡ä¸€æ¬¡è¾“å…¥ä¸€ä¸ªçŠ¶æ€çš„ 5 å¼ å›¾åƒå³å¯é‡å»ºåŠ¨æ€ 3D åœºæ™¯ã€‚</li><li>Knowledge NeRF é‡‡ç”¨äº†ä¸€ç§æ–°æ¡†æ¶ï¼Œä¸€æ¬¡è€ƒè™‘ä¸¤å¸§å†…å®¹ã€‚</li><li>Knowledge NeRF åˆ©ç”¨é¢„è®­ç»ƒçš„ NeRF æ¨¡å‹ä¸­çš„è¿‡å»çŸ¥è¯†æ¥ç”Ÿæˆæ–°çŠ¶æ€ä¸‹çš„æ–°é¢–è§†å›¾ã€‚</li><li>Knowledge NeRF æå‡ºäº†ä¸€ç§æŠ•å½±æ¨¡å—ï¼Œç”¨äºå°† NeRF é€‚åº”äºåŠ¨æ€åœºæ™¯ï¼Œå­¦ä¹ é¢„è®­ç»ƒçŸ¥è¯†åº“ä¸å½“å‰çŠ¶æ€ä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚</li><li>Knowledge NeRF æ˜¯åŠ¨æ€é“°æ¥ç‰©ä½“ä¸­æ–°é¢–è§†å›¾åˆæˆçš„å…¨æ–°ç®¡é“å’Œæœ‰å¸Œæœ›çš„è§£å†³æ–¹æ¡ˆã€‚</li><li>Knowledge NeRF çš„æ•°æ®å’Œå®ç°å·²å…¬å¼€ï¼Œç½‘å€ä¸º <a href="https://github.com/RussRobin/Knowledge_NeRFã€‚">https://github.com/RussRobin/Knowledge_NeRFã€‚</a></li><li>Knowledge NeRF èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„åŠ¨æ€åœºæ™¯é‡å»ºï¼Œè€Œä»¥å¾€çš„åŠ¨æ€ NeRF æ–¹æ³•åˆ™å—åˆ°é™åˆ¶ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šKnowledgeNeRFï¼šåŠ¨æ€é“°æ¥ç‰©ä½“çš„æ–°è§†è§’åˆæˆ</li><li>ä½œè€…ï¼šWenxiao Caiã€Xinyue Leiã€Xinyu Heã€Junming Leo Chenã€Yangang Wang</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šä¸œå—å¤§å­¦</li><li>å…³é”®è¯ï¼šæ–°è§†è§’åˆæˆã€ç¥ç»è¾å°„åœºã€åŠ¨æ€ 3D åœºæ™¯ã€ç¨€ç–è§†è§’åˆæˆã€çŸ¥è¯†é›†æˆ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/pdf/2404.00674.pdfï¼ŒGithub ä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šåŠ¨æ€åœºæ™¯çš„é‡å»ºå’Œæ¸²æŸ“æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œåœ¨å¢å¼ºç°å®ã€è™šæ‹Ÿç°å®ã€3D å†…å®¹åˆ¶ä½œç­‰é¢†åŸŸæœ‰ç€å¹¿æ³›çš„åº”ç”¨ã€‚</li></ol><p>(2) è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šä»¥å¾€çš„åŠ¨æ€ NeRF æ–¹æ³•ä»å•ç›®è§†é¢‘ä¸­å­¦ä¹ é“°æ¥ç‰©ä½“çš„å˜å½¢ï¼Œä½†é‡å»ºåœºæ™¯çš„è´¨é‡æœ‰é™ã€‚</p><p>(3) æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šKnowledgeNeRF æå‡ºäº†ä¸€ç§æ–°æ¡†æ¶ï¼Œé€šè¿‡ä¸€æ¬¡è€ƒè™‘ä¸¤å¸§æ¥é‡å»ºåŠ¨æ€åœºæ™¯ã€‚è¯¥æ–¹æ³•é¢„è®­ç»ƒäº†ä¸€ä¸ªé“°æ¥ç‰©ä½“çš„ NeRF æ¨¡å‹ï¼Œå½“ç‰©ä½“ç§»åŠ¨æ—¶ï¼ŒKnowledgeNeRF é€šè¿‡å°†é¢„è®­ç»ƒçš„ NeRF æ¨¡å‹ä¸­çš„è¿‡å»çŸ¥è¯†ä¸å½“å‰çŠ¶æ€ä¸­çš„æœ€å°è§‚å¯Ÿç›¸ç»“åˆï¼Œå­¦ä¹ åœ¨æ–°çš„çŠ¶æ€ä¸‹ç”Ÿæˆæ–°è§†è§’ã€‚</p><p>(4) æ–¹æ³•åœ¨ä»»åŠ¡ä¸Šçš„è¡¨ç°åŠæ€§èƒ½ï¼šKnowledgeNeRF åœ¨åŠ¨æ€ 3D åœºæ™¯é‡å»ºä»»åŠ¡ä¸Šå–å¾—äº†æœ‰æ•ˆæ€§ï¼Œåœ¨å•ä¸ªçŠ¶æ€ä¸‹ä½¿ç”¨ 5 å¹…è¾“å…¥å›¾åƒå³å¯é‡å»ºã€‚è¯¥æ–¹æ³•å¯ä»¥æ”¯æŒå…¶ç›®æ ‡ï¼Œå³ä¸ºåŠ¨æ€é“°æ¥ç‰©ä½“æä¾›æ–°è§†è§’åˆæˆçš„æ–°ç®¡é“å’Œæœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚</p><p>7.Methodsï¼šï¼ˆ1ï¼‰é¢„è®­ç»ƒé“°æ¥ç‰©ä½“NeRFæ¨¡å‹ï¼šè®­ç»ƒä¸€ä¸ªNeRFæ¨¡å‹ï¼Œä»å•ç›®è§†é¢‘ä¸­å­¦ä¹ é“°æ¥ç‰©ä½“çš„å˜å½¢ã€‚ï¼ˆ2ï¼‰æ„å»ºçŸ¥è¯†å›¾è°±ï¼šå°†é¢„è®­ç»ƒçš„NeRFæ¨¡å‹çš„æƒé‡å’Œæ¿€æ´»å€¼å­˜å‚¨åœ¨ä¸€ä¸ªçŸ¥è¯†å›¾è°±ä¸­ã€‚ï¼ˆ3ï¼‰æ–°è§†è§’åˆæˆï¼šå½“ç‰©ä½“ç§»åŠ¨æ—¶ï¼Œå°†çŸ¥è¯†å›¾è°±ä¸­çš„è¿‡å»çŸ¥è¯†ä¸å½“å‰çŠ¶æ€ä¸­çš„æœ€å°è§‚å¯Ÿç›¸ç»“åˆï¼Œå­¦ä¹ åœ¨æ–°çš„çŠ¶æ€ä¸‹ç”Ÿæˆæ–°è§†è§’ã€‚</p><ol><li>ç»“è®ºï¼š(1): KnowledgeNeRF æå‡ºäº†ä¸€ç§æ–°æ¡†æ¶ï¼Œé€šè¿‡ä¸€æ¬¡è€ƒè™‘ä¸¤å¸§æ¥é‡å»ºåŠ¨æ€åœºæ™¯ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†åŠ¨æ€é“°æ¥ç‰©ä½“çš„æ–°è§†è§’åˆæˆé—®é¢˜ã€‚è¯¥æ–¹æ³•é¢„è®­ç»ƒäº†ä¸€ä¸ªé“°æ¥ç‰©ä½“çš„ NeRF æ¨¡å‹ï¼Œå¹¶é€šè¿‡å°†é¢„è®­ç»ƒçš„ NeRF æ¨¡å‹ä¸­çš„è¿‡å»çŸ¥è¯†ä¸å½“å‰çŠ¶æ€ä¸­çš„æœ€å°è§‚å¯Ÿç›¸ç»“åˆï¼Œå­¦ä¹ åœ¨æ–°çš„çŠ¶æ€ä¸‹ç”Ÿæˆæ–°è§†è§’ï¼Œä¸ºåŠ¨æ€é“°æ¥ç‰©ä½“æä¾›äº†æ–°è§†è§’åˆæˆçš„æ–°ç®¡é“å’Œæœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚(2): åˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ KnowledgeNeRFï¼Œé€šè¿‡ä¸€æ¬¡è€ƒè™‘ä¸¤å¸§æ¥é‡å»ºåŠ¨æ€åœºæ™¯ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†åŠ¨æ€é“°æ¥ç‰©ä½“çš„æ–°è§†è§’åˆæˆé—®é¢˜ã€‚</li><li>å°†é¢„è®­ç»ƒçš„é“°æ¥ç‰©ä½“ NeRF æ¨¡å‹ä¸­çš„è¿‡å»çŸ¥è¯†ä¸å½“å‰çŠ¶æ€ä¸­çš„æœ€å°è§‚å¯Ÿç›¸ç»“åˆï¼Œå­¦ä¹ åœ¨æ–°çš„çŠ¶æ€ä¸‹ç”Ÿæˆæ–°è§†è§’ï¼Œæé«˜äº†é‡å»ºåœºæ™¯çš„è´¨é‡ã€‚</li><li>æå‡ºäº†ä¸€ç§æ„å»ºçŸ¥è¯†å›¾è°±çš„æ–¹æ³•ï¼Œå°†é¢„è®­ç»ƒçš„ NeRF æ¨¡å‹çš„æƒé‡å’Œæ¿€æ´»å€¼å­˜å‚¨åœ¨ä¸€ä¸ªçŸ¥è¯†å›¾è°±ä¸­ï¼Œæ–¹ä¾¿åç»­çš„çŸ¥è¯†æå–å’Œåˆ©ç”¨ã€‚æ€§èƒ½ï¼š</li><li>åœ¨åŠ¨æ€ 3D åœºæ™¯é‡å»ºä»»åŠ¡ä¸Šå–å¾—äº†æœ‰æ•ˆæ€§ï¼Œåœ¨å•ä¸ªçŠ¶æ€ä¸‹ä½¿ç”¨ 5 å¹…è¾“å…¥å›¾åƒå³å¯é‡å»ºã€‚</li><li>å¯ä»¥æ”¯æŒå…¶ç›®æ ‡ï¼Œå³ä¸ºåŠ¨æ€é“°æ¥ç‰©ä½“æä¾›æ–°è§†è§’åˆæˆçš„æ–°ç®¡é“å’Œæœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚å·¥ä½œé‡ï¼š</li><li>éœ€è¦é¢„è®­ç»ƒä¸€ä¸ªé“°æ¥ç‰©ä½“ NeRF æ¨¡å‹ï¼Œè¿™å¯èƒ½éœ€è¦å¤§é‡çš„æ•°æ®å’Œè®¡ç®—èµ„æºã€‚</li><li>éœ€è¦æ„å»ºä¸€ä¸ªçŸ¥è¯†å›¾è°±ï¼Œè¿™å¯èƒ½ä¼šå¢åŠ å­˜å‚¨å’Œè®¡ç®—å¼€é”€ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-5a878411dcb6ab842b9571fbf35e761b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1c30a4c13059600200df9151f02890b7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-35e6ac626071f10b54837546e9ead1e4.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-04-09  Stylizing Sparse-View 3D Scenes with Hierarchical Neural Representation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/04/09/Paper/2024-04-09/3DGS/"/>
    <id>https://kedreamix.github.io/2024/04/09/Paper/2024-04-09/3DGS/</id>
    <published>2024-04-09T08:23:53.000Z</published>
    <updated>2024-04-09T08:23:53.012Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-04-09-æ›´æ–°"><a href="#2024-04-09-æ›´æ–°" class="headerlink" title="2024-04-09 æ›´æ–°"></a>2024-04-09 æ›´æ–°</h1><h2 id="Robust-Gaussian-Splatting"><a href="#Robust-Gaussian-Splatting" class="headerlink" title="Robust Gaussian Splatting"></a>Robust Gaussian Splatting</h2><p><strong>Authors:FranÃ§ois Darmon, Lorenzo Porzi, Samuel Rota-BulÃ², Peter Kontschieder</strong></p><p>In this paper, we address common error sources for 3D Gaussian Splatting (3DGS) including blur, imperfect camera poses, and color inconsistencies, with the goal of improving its robustness for practical applications like reconstructions from handheld phone captures. Our main contribution involves modeling motion blur as a Gaussian distribution over camera poses, allowing us to address both camera pose refinement and motion blur correction in a unified way. Additionally, we propose mechanisms for defocus blur compensation and for addressing color in-consistencies caused by ambient light, shadows, or due to camera-related factors like varying white balancing settings. Our proposed solutions integrate in a seamless way with the 3DGS formulation while maintaining its benefits in terms of training efficiency and rendering speed. We experimentally validate our contributions on relevant benchmark datasets including Scannet++ and Deblur-NeRF, obtaining state-of-the-art results and thus consistent improvements over relevant baselines. </p><p><a href="http://arxiv.org/abs/2404.04211v1">PDF</a> </p><p><strong>Summary</strong><br>3Dé«˜æ–¯ä½“ç´ æ¸²æŸ“ï¼ˆ3DGSï¼‰çš„é€šç”¨é”™è¯¯æºå»ºæ¨¡åŠå…¶åœ¨å®é™…åº”ç”¨ä¸­çš„é²æ£’æ€§æå‡ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>å°†è¿åŠ¨æ¨¡ç³Šå»ºæ¨¡ä¸ºç›¸æœºä½å§¿ä¸Šçš„é«˜æ–¯åˆ†å¸ƒï¼Œç»Ÿä¸€å¤„ç†ç›¸æœºä½å§¿ä¼˜åŒ–å’Œè¿åŠ¨æ¨¡ç³Šæ ¡æ­£ã€‚</li><li>æå‡ºæ•£ç„¦æ¨¡ç³Šè¡¥å¿å’Œè§£å†³ç”±äºç¯å¢ƒå…‰ã€é˜´å½±æˆ–ä¸ç›¸æœºç›¸å…³çš„å› ç´ ï¼ˆå¦‚ç™½å¹³è¡¡è®¾ç½®å˜åŒ–ï¼‰å¯¼è‡´çš„é¢œè‰²ä¸ä¸€è‡´çš„æœºåˆ¶ã€‚</li><li>æå‡ºçš„è§£å†³æ–¹æ¡ˆä¸ 3DGS å…¬å¼æ— ç¼é›†æˆï¼ŒåŒæ—¶ä¿æŒå…¶åœ¨è®­ç»ƒæ•ˆç‡å’Œæ¸²æŸ“é€Ÿåº¦æ–¹é¢çš„ä¼˜åŠ¿ã€‚</li><li>åœ¨ Scannet++ å’Œ Deblur-NeRF ç­‰ç›¸å…³åŸºå‡†æ•°æ®é›†ä¸Šé€šè¿‡å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„è´¡çŒ®ï¼Œè·å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå¹¶å§‹ç»ˆå¦‚ä¸€åœ°æ”¹è¿›äº†ç›¸å…³åŸºå‡†ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šé²æ£’é«˜æ–¯æº…å°„</li><li>ä½œè€…ï¼šFranÃ§ois Darmon, Lorenzo Porzi, Samuel Rota-BulÃ², Peter Kontschieder</li><li>éš¶å±ï¼šMeta Reality Labs è‹é»ä¸–</li><li>å…³é”®è¯ï¼š3D é«˜æ–¯æº…å°„ã€ä½å§¿ä¼˜åŒ–ã€è¿åŠ¨æ¨¡ç³Š</li><li>é“¾æ¥ï¼šarxiv.org/abs/2404.04â€¦</li><li>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼š3D é«˜æ–¯æº…å°„ (3DGS) æ˜¯ä¸€ç§ç”¨äºä»å›¾åƒé‡å»º 3D åœºæ™¯çš„æœ‰æ•ˆæŠ€æœ¯ã€‚ç„¶è€Œï¼Œå®ƒå®¹æ˜“å—åˆ°æ¨¡ç³Šã€ä¸å®Œç¾çš„ç›¸æœºä½å§¿å’Œé¢œè‰²ä¸ä¸€è‡´ç­‰å¸¸è§é”™è¯¯æºçš„å½±å“ã€‚(2) è¿‡å»çš„æ–¹æ³•ï¼šç°æœ‰çš„æ–¹æ³•é€šå¸¸åˆ†åˆ«å¤„ç†è¿™äº›é”™è¯¯æºï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´æ¬¡ä¼˜ç»“æœã€‚(3) æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„æ¡†æ¶ï¼Œå°†è¿åŠ¨æ¨¡ç³Šå»ºæ¨¡ä¸ºç›¸æœºä½å§¿ä¸Šçš„é«˜æ–¯åˆ†å¸ƒï¼Œä»è€ŒåŒæ—¶è§£å†³ç›¸æœºä½å§¿ä¼˜åŒ–å’Œè¿åŠ¨æ¨¡ç³Šæ ¡æ­£é—®é¢˜ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†é’ˆå¯¹æ•£ç„¦æ¨¡ç³Šè¡¥å¿å’Œè§£å†³ç”±ç¯å¢ƒå…‰ã€é˜´å½±æˆ–ç›¸æœºç›¸å…³å› ç´ ï¼ˆå¦‚ç™½å¹³è¡¡è®¾ç½®ä¸åŒï¼‰å¼•èµ·çš„é¢œè‰²ä¸ä¸€è‡´çš„æœºåˆ¶ã€‚(4) ä»»åŠ¡å’Œæ€§èƒ½ï¼šåœ¨ Scannet++ å’Œ Deblur-NeRF ç­‰åŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒéªŒè¯äº†æœ¬æ–‡æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè·å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå¹¶å¯¹ç›¸å…³åŸºå‡†çº¿è¿›è¡Œäº†æŒç»­çš„æ”¹è¿›ã€‚è¿™äº›ç»“æœæ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ï¼Œå³æé«˜ 3DGS åœ¨å®é™…åº”ç”¨ä¸­çš„é²æ£’æ€§ï¼Œä¾‹å¦‚ä»æ‰‹æŒæ‰‹æœºæ‹æ‘„çš„å›¾åƒè¿›è¡Œé‡å»ºã€‚</li></ol><p>7.æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼šæå‡ºäº†ä¸€ç§ç»Ÿä¸€æ¡†æ¶ï¼Œå°†è¿åŠ¨æ¨¡ç³Šå»ºæ¨¡ä¸ºç›¸æœºä½å§¿ä¸Šçš„é«˜æ–¯åˆ†å¸ƒï¼ŒåŒæ—¶è§£å†³ç›¸æœºä½å§¿ä¼˜åŒ–å’Œè¿åŠ¨æ¨¡ç³Šæ ¡æ­£é—®é¢˜ã€‚ï¼ˆ2ï¼‰ï¼šé’ˆå¯¹æ•£ç„¦æ¨¡ç³Šè¡¥å¿ï¼Œæå‡ºäº†ä¸€ç§æœºåˆ¶æ¥è¡¥å¿ç”±ç¯å¢ƒå…‰ã€é˜´å½±æˆ–ç›¸æœºç›¸å…³å› ç´ ï¼ˆå¦‚ç™½å¹³è¡¡è®¾ç½®ä¸åŒï¼‰å¼•èµ·çš„é¢œè‰²ä¸ä¸€è‡´ã€‚ï¼ˆ3ï¼‰ï¼šæå‡ºäº†ä¸€ä¸ªå…·æœ‰é€å›¾åƒå‚æ•°çš„RGBè§£ç å™¨å‡½æ•°ï¼Œä»¥è§£å†³ç”±ç¯å¢ƒå…‰ã€é˜´å½±æˆ–ç›¸æœºç›¸å…³å› ç´ ï¼ˆå¦‚ç™½å¹³è¡¡è®¾ç½®ä¸åŒï¼‰å¼•èµ·çš„é¢œè‰²ä¸ä¸€è‡´ã€‚</p><p>8.ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€æ¡†æ¶ï¼Œå°†è¿åŠ¨æ¨¡ç³Šå»ºæ¨¡ä¸ºç›¸æœºä½å§¿ä¸Šçš„é«˜æ–¯åˆ†å¸ƒï¼ŒåŒæ—¶è§£å†³ç›¸æœºä½å§¿ä¼˜åŒ–å’Œè¿åŠ¨æ¨¡ç³Šæ ¡æ­£é—®é¢˜ï¼Œå¹¶é’ˆå¯¹æ•£ç„¦æ¨¡ç³Šè¡¥å¿å’Œé¢œè‰²ä¸ä¸€è‡´æå‡ºäº†æœºåˆ¶ï¼Œæé«˜äº†3Dé«˜æ–¯æº…å°„çš„é²æ£’æ€§ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼ŒåŒæ—¶è§£å†³ç›¸æœºä½å§¿ä¼˜åŒ–ã€è¿åŠ¨æ¨¡ç³Šæ ¡æ­£ã€æ•£ç„¦æ¨¡ç³Šè¡¥å¿å’Œé¢œè‰²ä¸ä¸€è‡´ç­‰é—®é¢˜ï¼Œæé«˜äº†3Dé«˜æ–¯æº…å°„çš„é²æ£’æ€§ã€‚æ€§èƒ½ï¼šæœ¬æ–‡æ–¹æ³•åœ¨Scannet++å’ŒDeblur-NeRFç­‰åŸºå‡†æ•°æ®é›†ä¸Šè·å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå¹¶å¯¹ç›¸å…³åŸºå‡†çº¿è¿›è¡Œäº†æŒç»­çš„æ”¹è¿›ã€‚å·¥ä½œé‡ï¼šæœ¬æ–‡æ–¹æ³•çš„å®ç°ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦å¯¹ç›¸æœºä½å§¿ä¼˜åŒ–ã€è¿åŠ¨æ¨¡ç³Šæ ¡æ­£ã€æ•£ç„¦æ¨¡ç³Šè¡¥å¿å’Œé¢œè‰²ä¸ä¸€è‡´ç­‰å¤šä¸ªæ–¹é¢è¿›è¡Œå»ºæ¨¡å’Œæ±‚è§£ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-1fe522891f8ae397344ebb9db256a018.jpg" align="middle"><img src="https://picx.zhimg.com/v2-09d15a60f6aa00f7632f702431cf9775.jpg" align="middle"></details>## OmniGS: Omnidirectional Gaussian Splatting for Fast Radiance Field   Reconstruction using Omnidirectional Images**Authors:Longwei Li, Huajian Huang, Sai-Kit Yeung, Hui Cheng**Photorealistic reconstruction relying on 3D Gaussian Splatting has shown promising potential in robotics. However, the current 3D Gaussian Splatting system only supports radiance field reconstruction using undistorted perspective images. In this paper, we present OmniGS, a novel omnidirectional Gaussian splatting system, to take advantage of omnidirectional images for fast radiance field reconstruction. Specifically, we conduct a theoretical analysis of spherical camera model derivatives in 3D Gaussian Splatting. According to the derivatives, we then implement a new GPU-accelerated omnidirectional rasterizer that directly splats 3D Gaussians onto the equirectangular screen space for omnidirectional image rendering. As a result, we realize differentiable optimization of the radiance field without the requirement of cube-map rectification or tangent-plane approximation. Extensive experiments conducted in egocentric and roaming scenarios demonstrate that our method achieves state-of-the-art reconstruction quality and high rendering speed using omnidirectional images. To benefit the research community, the code will be made publicly available once the paper is published. [PDF](http://arxiv.org/abs/2404.03202v2) 7 pages, 4 figures**Summary**å…¨æ™¯é«˜æ–¯ç‚¹äº‘ç³»ç»Ÿåˆ©ç”¨å…¨å‘å›¾åƒè¿›è¡Œå¿«é€Ÿçš„è§†åœºé‡å»ºï¼Œæ— éœ€ç«‹æ–¹ä½“è´´å›¾æ ¡æ­£æˆ–åˆ‡å¹³é¢é€¼è¿‘ï¼Œå®ç°å¯å¾®åˆ†ä¼˜åŒ–ã€‚**Key Takeaways**- å…¨æ™¯é«˜æ–¯ç‚¹äº‘ç³»ç»Ÿåˆ©ç”¨å…¨å‘å›¾åƒè¿›è¡Œè§†åœºé‡å»ºã€‚- è¯¥ç³»ç»Ÿé€šè¿‡ç†è®ºåˆ†æçƒé¢ç›¸æœºæ¨¡å‹å¯¼æ•°ï¼Œå®ç°å¯¹å…¨å‘å›¾åƒçš„å¿«é€Ÿå…‰æ …åŒ–ã€‚- ç³»ç»Ÿé€šè¿‡ GPU åŠ é€Ÿï¼Œç›´æ¥å°†ä¸‰ç»´é«˜æ–¯ç‚¹äº‘æ¸²æŸ“åˆ°ç­‰è·çŸ©å½¢å±å¹•ç©ºé—´ã€‚- æ— éœ€ç«‹æ–¹ä½“è´´å›¾æ ¡æ­£æˆ–åˆ‡å¹³é¢é€¼è¿‘ï¼Œå¯å®ç°è§†åœºçš„å…‰å·®åˆ†ä¼˜åŒ–ã€‚- è¯¥æ–¹æ³•åœ¨è‡ªä¸­å¿ƒå’Œæ¼«æ¸¸åœºæ™¯ä¸­å‡è¾¾åˆ°æœ€å…ˆè¿›çš„é‡å»ºè´¨é‡å’Œé«˜æ¸²æŸ“é€Ÿåº¦ã€‚- è¯¥ç³»ç»Ÿä»£ç å°†äºè®ºæ–‡å‘è¡¨åå…¬å¼€ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>é¢˜ç›®ï¼šOmniGSï¼šå…¨å‘é«˜æ–¯ splattingï¼Œç”¨äºä½¿ç”¨å…¨å‘å›¾åƒå¿«é€Ÿé‡å»ºå…‰åœº</li><li>ä½œè€…ï¼šæé¾™å¨ï¼Œé»„åå¥ï¼Œæ¨ä¸–æ°ï¼Œç¨‹è¾‰</li><li>éš¶å±ï¼šä¸­å±±å¤§å­¦è®¡ç®—æœºç§‘å­¦ä¸å·¥ç¨‹å­¦é™¢</li><li>å…³é”®è¯ï¼šå…¨å‘è§†è§‰ï¼Œå…‰åœºé‡å»ºï¼Œ3D é‡å»ºï¼Œæ–°è§†è§’åˆæˆï¼Œé«˜æ–¯ splatting</li><li>é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.03202</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šè¿‘å¹´æ¥ï¼Œä½¿ç”¨ç¥ç»è¾å°„åœº (NeRF) æŠ€æœ¯è¿›è¡Œå…‰åœºé‡å»ºå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼ŒNeRF æ–¹æ³•çš„è®­ç»ƒå’Œæ¨ç†æ—¶é—´è¾ƒé•¿ï¼Œé™åˆ¶äº†å…¶åœ¨å®æ—¶åº”ç”¨ä¸­çš„ä½¿ç”¨ã€‚</li></ol><p>ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼š3D é«˜æ–¯ splatting æ˜¯ä¸€ç§æœ‰æ•ˆè§£å†³ NeRF é™åˆ¶çš„æ–¹æ³•ï¼Œå®ƒä½¿ç”¨ 3D é«˜æ–¯æ˜ç¡®è¡¨ç¤ºå…‰åœºã€‚ç„¶è€Œï¼Œç°æœ‰çš„ splatting ç®—æ³•ä»…æ”¯æŒä½¿ç”¨æœªå¤±çœŸçš„é€è§†å›¾åƒè¿›è¡Œå…‰åœºé‡å»ºã€‚</p><p>ï¼ˆ3ï¼‰æœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç³»ç»Ÿ OmniGSï¼Œå®ƒåˆ©ç”¨å…¨å‘é«˜æ–¯ splatting è¿›è¡Œå¿«é€Ÿå…‰åœºé‡å»ºã€‚OmniGS å¯¹çƒé¢ç›¸æœºæ¨¡å‹å¯¼æ•°è¿›è¡Œäº†ç†è®ºåˆ†æï¼Œå¹¶å®ç°äº†æ–°çš„ GPU åŠ é€Ÿå…¨å‘å…‰æ …åŒ–å™¨ï¼Œå¯ç›´æ¥å°† 3D é«˜æ–¯ splatting åˆ°ç­‰è·çŸ©å½¢å±å¹•ç©ºé—´ï¼Œç”¨äºå…¨å‘å›¾åƒæ¸²æŸ“ã€‚</p><p>ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šåœ¨ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒå’Œæ¼«æ¸¸åœºæ™¯ä¸­è¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒOmniGS ä½¿ç”¨å…¨å‘å›¾åƒå®ç°äº†æœ€å…ˆè¿›çš„é‡å»ºè´¨é‡å’Œé«˜æ¸²æŸ“é€Ÿåº¦ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº† OmniGS åœ¨å®æ—¶åº”ç”¨ä¸­çš„ä½¿ç”¨ã€‚</p><ol><li><p>æ–¹æ³•ï¼š(1): æå‡º OmniGS ç³»ç»Ÿï¼Œåˆ©ç”¨å…¨å‘é«˜æ–¯ splatting è¿›è¡Œå¿«é€Ÿå…‰åœºé‡å»ºï¼›(2): å¯¹çƒé¢ç›¸æœºæ¨¡å‹å¯¼æ•°è¿›è¡Œç†è®ºåˆ†æï¼Œå®ç° GPU åŠ é€Ÿå…¨å‘å…‰æ …åŒ–å™¨ï¼›(3): å°† 3D é«˜æ–¯ splatting ç›´æ¥å…‰æ …åŒ–åˆ°ç­‰è·çŸ©å½¢å±å¹•ç©ºé—´ï¼Œç”¨äºå…¨å‘å›¾åƒæ¸²æŸ“ï¼›(4): åœ¨è‡ªæˆ‘ä¸ºä¸­å¿ƒå’Œæ¼«æ¸¸åœºæ™¯ä¸­è¿›è¡Œå¹¿æ³›å®éªŒï¼ŒéªŒè¯ OmniGS åœ¨ä½¿ç”¨å…¨å‘å›¾åƒè¿›è¡Œé‡å»ºæ—¶ï¼Œå…·æœ‰æœ€å…ˆè¿›çš„é‡å»ºè´¨é‡å’Œé«˜æ¸²æŸ“é€Ÿåº¦ã€‚</p></li><li><p>ç»“è®ºï¼š(1): æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§ä½¿ç”¨å…¨å‘é«˜æ–¯ splatting è¿›è¡Œå¿«é€Ÿå…‰åœºé‡å»ºçš„æ–°ç³»ç»Ÿ OmniGSï¼Œè¯¥ç³»ç»Ÿåœ¨ä½¿ç”¨å…¨å‘å›¾åƒè¿›è¡Œé‡å»ºæ—¶ï¼Œå…·æœ‰æœ€å…ˆè¿›çš„é‡å»ºè´¨é‡å’Œé«˜æ¸²æŸ“é€Ÿåº¦ï¼Œæ”¯æŒäº† OmniGS åœ¨å®æ—¶åº”ç”¨ä¸­çš„ä½¿ç”¨ã€‚(2): åˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡º OmniGS ç³»ç»Ÿï¼Œåˆ©ç”¨å…¨å‘é«˜æ–¯ splatting è¿›è¡Œå¿«é€Ÿå…‰åœºé‡å»ºã€‚</li><li>å¯¹çƒé¢ç›¸æœºæ¨¡å‹å¯¼æ•°è¿›è¡Œç†è®ºåˆ†æï¼Œå®ç° GPU åŠ é€Ÿå…¨å‘å…‰æ …åŒ–å™¨ã€‚</li><li>å°† 3D é«˜æ–¯ splatting ç›´æ¥å…‰æ …åŒ–åˆ°ç­‰è·çŸ©å½¢å±å¹•ç©ºé—´ï¼Œç”¨äºå…¨å‘å›¾åƒæ¸²æŸ“ã€‚</li><li>åœ¨è‡ªæˆ‘ä¸ºä¸­å¿ƒå’Œæ¼«æ¸¸åœºæ™¯ä¸­è¿›è¡Œå¹¿æ³›å®éªŒï¼ŒéªŒè¯ OmniGS åœ¨ä½¿ç”¨å…¨å‘å›¾åƒè¿›è¡Œé‡å»ºæ—¶ï¼Œå…·æœ‰æœ€å…ˆè¿›çš„é‡å»ºè´¨é‡å’Œé«˜æ¸²æŸ“é€Ÿåº¦ã€‚æ€§èƒ½ï¼š</li><li>ä½¿ç”¨å…¨å‘å›¾åƒå®ç°äº†æœ€å…ˆè¿›çš„é‡å»ºè´¨é‡å’Œé«˜æ¸²æŸ“é€Ÿåº¦ã€‚</li><li>æ”¯æŒ OmniGS åœ¨å®æ—¶åº”ç”¨ä¸­çš„ä½¿ç”¨ã€‚å·¥ä½œé‡ï¼š</li><li>å¯¹çƒé¢ç›¸æœºæ¨¡å‹å¯¼æ•°è¿›è¡Œç†è®ºåˆ†æã€‚</li><li>å®ç° GPU åŠ é€Ÿå…¨å‘å…‰æ …åŒ–å™¨ã€‚</li><li>åœ¨è‡ªæˆ‘ä¸ºä¸­å¿ƒå’Œæ¼«æ¸¸åœºæ™¯ä¸­è¿›è¡Œå¹¿æ³›å®éªŒã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-5c5391fc4277ce922cdddc0af1ec26d4.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d443789e6c3193b616d8dc21049af0b5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1ca44202ac53707a8da1ef4807f9c933.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c42315ac9ad685573dcfa99dc36d6e4e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-03c97710b7522487bbb73acfc93336fd.jpg" align="middle"></details><h2 id="CityGaussian-Real-time-High-quality-Large-Scale-Scene-Rendering-with-Gaussians"><a href="#CityGaussian-Real-time-High-quality-Large-Scale-Scene-Rendering-with-Gaussians" class="headerlink" title="CityGaussian: Real-time High-quality Large-Scale Scene Rendering with   Gaussians"></a>CityGaussian: Real-time High-quality Large-Scale Scene Rendering with   Gaussians</h2><p><strong>Authors:Yang Liu, He Guan, Chuanchen Luo, Lue Fan, Junran Peng, Zhaoxiang Zhang</strong></p><p>The advancement of real-time 3D scene reconstruction and novel view synthesis has been significantly propelled by 3D Gaussian Splatting (3DGS). However, effectively training large-scale 3DGS and rendering it in real-time across various scales remains challenging. This paper introduces CityGaussian (CityGS), which employs a novel divide-and-conquer training approach and Level-of-Detail (LoD) strategy for efficient large-scale 3DGS training and rendering. Specifically, the global scene prior and adaptive training data selection enables efficient training and seamless fusion. Based on fused Gaussian primitives, we generate different detail levels through compression, and realize fast rendering across various scales through the proposed block-wise detail levels selection and aggregation strategy. Extensive experimental results on large-scale scenes demonstrate that our approach attains state-of-theart rendering quality, enabling consistent real-time rendering of largescale scenes across vastly different scales. Our project page is available at <a href="https://dekuliutesla.github.io/citygs/">https://dekuliutesla.github.io/citygs/</a>. </p><p><a href="http://arxiv.org/abs/2404.01133v2">PDF</a> Project Page: <a href="https://dekuliutesla.github.io/citygs/">https://dekuliutesla.github.io/citygs/</a></p><p><strong>Summary</strong><br>é€šè¿‡é‡‡ç”¨åˆ†è€Œæ²»ä¹‹çš„è®­ç»ƒæ–¹æ³•å’Œåˆ†çº§ç»†èŠ‚ç­–ç•¥ï¼ŒCityGaussian æœ‰åŠ©äºæœ‰æ•ˆåœ°è®­ç»ƒå¤§è§„æ¨¡ 3DGS å¹¶å®æ—¶æ¸²æŸ“ä¸åŒæ¯”ä¾‹çš„åœºæ™¯ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>CityGaussian æå‡ºäº†ä¸€ç§æ–°é¢–çš„åˆ†è€Œæ²»ä¹‹è®­ç»ƒæ–¹æ³•ï¼Œç”¨äºé«˜æ•ˆçš„å¤§è§„æ¨¡ 3DGS è®­ç»ƒã€‚</li><li>å…¨å±€åœºæ™¯å…ˆéªŒå’Œè‡ªé€‚åº”è®­ç»ƒæ•°æ®é€‰æ‹©å¯å®ç°é«˜æ•ˆçš„è®­ç»ƒå’Œæ— ç¼èåˆã€‚</li><li>åŸºäºèåˆçš„é«˜æ–¯åŸºå…ƒï¼Œé€šè¿‡å‹ç¼©ç”Ÿæˆä¸åŒç»†èŠ‚ç­‰çº§ã€‚</li><li>é€šè¿‡æå‡ºçš„åˆ†å—ç»†èŠ‚çº§åˆ«é€‰æ‹©å’Œèšåˆç­–ç•¥ï¼Œå®ç°è·¨ä¸åŒæ¯”ä¾‹çš„å¿«é€Ÿæ¸²æŸ“ã€‚</li><li>å¤§è§„æ¨¡åœºæ™¯ä¸Šçš„å¹¿æ³›å®éªŒç»“æœè¡¨æ˜ï¼ŒCityGaussian çš„æ¸²æŸ“è´¨é‡è¾¾åˆ°æœ€å…ˆè¿›çš„æ°´å¹³ã€‚</li><li>CityGaussian èƒ½å¤Ÿä»¥ä¸€è‡´çš„æ–¹å¼å®æ—¶æ¸²æŸ“è·¨ä¸åŒæ¯”ä¾‹çš„å¤§è§„æ¨¡åœºæ™¯ã€‚</li><li>CityGaussian é¡¹ç›®ä¸»é¡µï¼š<a href="https://dekuliutesla.github.io/citygs/ã€‚">https://dekuliutesla.github.io/citygs/ã€‚</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>è®ºæ–‡æ ‡é¢˜ï¼šCityGaussianï¼šå®æ—¶é«˜è´¨é‡å¤§åœºæ™¯æ¸²æŸ“ä¸­çš„é«˜æ–¯ä½“ç´ </li><li>ä½œè€…ï¼šYang Liu, He Guan, Chuanchen Luo, Lue Fan, Junran Peng, Zhaoxiang Zhang</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šä¸­å›½ç§‘å­¦é™¢è‡ªåŠ¨åŒ–ç ”ç©¶æ‰€</li><li>å…³é”®è¯ï¼šå¤§åœºæ™¯é‡å»ºã€æ–°è§†è§’åˆæˆã€3Dé«˜æ–¯ä½“ç´ </li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.01133Github ä»£ç é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šå®æ—¶ 3D åœºæ™¯é‡å»ºå’Œæ–°è§†è§’åˆæˆåœ¨ AR/VRã€èˆªç©ºæµ‹é‡å’Œè‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸè‡³å…³é‡è¦ã€‚è¯¥ä»»åŠ¡è¿½æ±‚å¤§èŒƒå›´ï¼ˆé€šå¸¸è¶…è¿‡ 1.5 å…¬é‡ŒÂ²ï¼‰çš„é«˜ä¿çœŸé‡å»ºå’Œå®æ—¶æ¸²æŸ“ï¼Œè·¨è¶Šä¸åŒçš„å°ºåº¦ã€‚è¿‘å¹´æ¥ï¼Œç¥ç»è¾å°„åœº (NeRF) ä¸»å¯¼äº†è¯¥é¢†åŸŸï¼Œä½†å®ƒä»¬åœ¨ç»†èŠ‚ä¿çœŸåº¦æ–¹é¢ä»å­˜åœ¨ä¸è¶³æˆ–æ€§èƒ½ä½ä¸‹çš„é—®é¢˜ã€‚(2) è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼š3D é«˜æ–¯ä½“ç´  (3DGS) ä½œä¸ºä¸€ç§æœ‰å‰é€”çš„æ›¿ä»£è§£å†³æ–¹æ¡ˆå‡ºç°ã€‚å®ƒä½¿ç”¨æ˜¾å¼ 3D é«˜æ–¯ä½“ç´ ä½œä¸ºåŸºå…ƒï¼Œåœ¨æ¸²æŸ“é€Ÿåº¦å’Œè´¨é‡æ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ã€‚ç„¶è€Œï¼Œæœ‰æ•ˆè®­ç»ƒå¤§è§„æ¨¡ 3DGS å¹¶åœ¨å„ç§å°ºåº¦ä¸Šå®æ—¶æ¸²æŸ“å®ƒä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚(3) æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº† CityGaussian (CityGS)ï¼Œå®ƒé‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„åˆ†å‰²å’Œå¾æœè®­ç»ƒæ–¹æ³•å’Œç»†èŠ‚çº§åˆ« (LoD) ç­–ç•¥ï¼Œä»¥å®ç°é«˜æ•ˆçš„å¤§è§„æ¨¡ 3DGS è®­ç»ƒå’Œæ¸²æŸ“ã€‚å…·ä½“æ¥è¯´ï¼Œå…¨å±€åœºæ™¯å…ˆéªŒå’Œè‡ªé€‚åº”è®­ç»ƒæ•°æ®é€‰æ‹©å®ç°äº†é«˜æ•ˆçš„è®­ç»ƒå’Œæ— ç¼èåˆã€‚åŸºäºèåˆçš„é«˜æ–¯åŸºå…ƒï¼Œæˆ‘ä»¬é€šè¿‡å‹ç¼©ç”Ÿæˆäº†ä¸åŒçš„ç»†èŠ‚çº§åˆ«ï¼Œå¹¶é€šè¿‡æå‡ºçš„å—çº§ç»†èŠ‚çº§åˆ«é€‰æ‹©å’Œèšåˆç­–ç•¥å®ç°äº†è·¨ä¸åŒå°ºåº¦çš„å¿«é€Ÿæ¸²æŸ“ã€‚(4) æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šåœ¨å¤§è§„æ¨¡åœºæ™¯ä¸Šçš„å¹¿æ³›å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ¸²æŸ“è´¨é‡ï¼Œå®ç°äº†è·¨ä¸åŒå°ºåº¦çš„å¤§è§„æ¨¡åœºæ™¯çš„å®æ—¶æ¸²æŸ“ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰ç”Ÿæˆç²—ç•¥çš„å…¨å±€é«˜æ–¯ä½“ç´ ï¼Œä½œä¸ºè®­ç»ƒçš„å…ˆéªŒï¼›ï¼ˆ2ï¼‰åŸºäºå…¨å±€å…ˆéªŒï¼Œæ ¹æ®æ•°æ®åˆ†å¸ƒè‡ªé€‚åº”åœ°åˆ’åˆ†é«˜æ–¯ä½“ç´ å’Œæ•°æ®ï¼›ï¼ˆ3ï¼‰åˆ©ç”¨èåˆçš„é«˜æ–¯åŸºå…ƒï¼Œç”Ÿæˆä¸åŒç»†èŠ‚å±‚æ¬¡ï¼Œå¹¶é€šè¿‡å—çº§ç»†èŠ‚å±‚æ¬¡é€‰æ‹©å’Œèšåˆç­–ç•¥å®ç°è·¨å°ºåº¦çš„å¿«é€Ÿæ¸²æŸ“ã€‚</p></li><li><p>ç»“è®ºï¼š(1): æœ¬å·¥ä½œæå‡ºäº† CityGaussian (CityGS)ï¼Œä¸€ç§ç”¨äºå¤§è§„æ¨¡åœºæ™¯çš„é«˜æ–¯ä½“ç´ è¡¨ç¤ºæ–¹æ³•ï¼Œé€šè¿‡åˆ†å‰²å’Œå¾æœè®­ç»ƒæ–¹æ³•å’Œç»†èŠ‚çº§åˆ«ç­–ç•¥å®ç°äº†é«˜æ•ˆçš„è®­ç»ƒå’Œæ¸²æŸ“ã€‚è¯¥æ–¹æ³•åœ¨å¤§è§„æ¨¡åœºæ™¯ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ¸²æŸ“è´¨é‡ï¼Œæ”¯æŒè·¨ä¸åŒå°ºåº¦çš„å®æ—¶æ¸²æŸ“ã€‚(2): åˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„åˆ†å‰²å’Œå¾æœè®­ç»ƒæ–¹æ³•ï¼Œæœ‰æ•ˆè®­ç»ƒå¤§è§„æ¨¡ 3DGSã€‚</li><li>è®¾è®¡äº†ä¸€ç§ç»†èŠ‚çº§åˆ«ç­–ç•¥ï¼Œé€šè¿‡å‹ç¼©ç”Ÿæˆä¸åŒç»†èŠ‚çº§åˆ«ï¼Œå¹¶é€šè¿‡å—çº§ç»†èŠ‚çº§åˆ«é€‰æ‹©å’Œèšåˆç­–ç•¥å®ç°è·¨å°ºåº¦çš„å¿«é€Ÿæ¸²æŸ“ã€‚æ€§èƒ½ï¼š</li><li>åœ¨å¤§è§„æ¨¡åœºæ™¯ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ¸²æŸ“è´¨é‡ã€‚</li><li>æ”¯æŒè·¨ä¸åŒå°ºåº¦çš„å®æ—¶æ¸²æŸ“ã€‚å·¥ä½œé‡ï¼š</li><li>è®­ç»ƒå’Œæ¸²æŸ“å¤§è§„æ¨¡ 3DGS å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li><li>éœ€è¦è¿›ä¸€æ­¥çš„ç ”ç©¶æ¥æé«˜è®­ç»ƒå’Œæ¸²æŸ“æ•ˆç‡ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-cdc289cc94afaf05e9abae37e6d49ef8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-547f5a57e280ecd60e9f1e905a13c71d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-8faf5cbf97c2d3cac383a23cf4a18d31.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-04-09  Robust Gaussian Splatting</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/04/09/Paper/2024-04-09/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/04/09/Paper/2024-04-09/Diffusion%20Models/</id>
    <published>2024-04-09T08:10:25.000Z</published>
    <updated>2024-04-09T08:10:25.416Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-04-09-æ›´æ–°"><a href="#2024-04-09-æ›´æ–°" class="headerlink" title="2024-04-09 æ›´æ–°"></a>2024-04-09 æ›´æ–°</h1><h2 id="MoMA-Multimodal-LLM-Adapter-for-Fast-Personalized-Image-Generation"><a href="#MoMA-Multimodal-LLM-Adapter-for-Fast-Personalized-Image-Generation" class="headerlink" title="MoMA: Multimodal LLM Adapter for Fast Personalized Image Generation"></a>MoMA: Multimodal LLM Adapter for Fast Personalized Image Generation</h2><p><strong>Authors:Kunpeng Song, Yizhe Zhu, Bingchen Liu, Qing Yan, Ahmed Elgammal, Xiao Yang</strong></p><p>In this paper, we present MoMA: an open-vocabulary, training-free personalized image model that boasts flexible zero-shot capabilities. As foundational text-to-image models rapidly evolve, the demand for robust image-to-image translation grows. Addressing this need, MoMA specializes in subject-driven personalized image generation. Utilizing an open-source, Multimodal Large Language Model (MLLM), we train MoMA to serve a dual role as both a feature extractor and a generator. This approach effectively synergizes reference image and text prompt information to produce valuable image features, facilitating an image diffusion model. To better leverage the generated features, we further introduce a novel self-attention shortcut method that efficiently transfers image features to an image diffusion model, improving the resemblance of the target object in generated images. Remarkably, as a tuning-free plug-and-play module, our model requires only a single reference image and outperforms existing methods in generating images with high detail fidelity, enhanced identity-preservation and prompt faithfulness. Our work is open-source, thereby providing universal access to these advancements. </p><p><a href="http://arxiv.org/abs/2404.05674v1">PDF</a> </p><p><strong>Summary</strong><br>MoMA: ä¸€æ¬¾å…è®­ç»ƒã€å¼€æ”¾è¯æ±‡ã€ä¸“ç”¨äºå›¾åƒä¸ªæ€§åŒ–ç”Ÿæˆä¸”å…·å¤‡çµæ´»é›¶æ ·æœ¬èƒ½åŠ›çš„å›¾åƒæ¨¡å‹ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡º MoMAï¼Œå¯ç”¨äºä¸»é¢˜é©±åŠ¨çš„ä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆã€‚</li><li>ä½¿ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLM) åŒæ—¶å……å½“ç‰¹å¾æå–å™¨å’Œç”Ÿæˆå™¨ã€‚</li><li>åˆ©ç”¨å‚è€ƒå›¾åƒå’Œæ–‡æœ¬æç¤ºä¿¡æ¯ç”Ÿæˆæœ‰ä»·å€¼çš„å›¾åƒç‰¹å¾ã€‚</li><li>é‡‡ç”¨è‡ªæ³¨æ„åŠ›å¿«æ·æ–¹å¼æ–¹æ³•ï¼Œå°†å›¾åƒç‰¹å¾æœ‰æ•ˆåœ°ä¼ é€’ç»™å›¾åƒæ‰©æ•£æ¨¡å‹ã€‚</li><li>ä½œä¸ºå…è°ƒä¼˜å³æ’å³ç”¨æ¨¡å—ï¼ŒMoMA ä»…éœ€ä¸€å¼ å‚è€ƒå›¾åƒå³å¯ç”Ÿæˆé«˜ä¿çœŸã€å¢å¼ºèº«ä»½ä¿æŒå’Œæç¤ºå¿ å®åº¦çš„å›¾åƒã€‚</li><li>ä»£ç å¼€æºï¼Œä»¥æœŸæƒ åŠæ›´å¤šä»ä¸šè€…ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šMoMAï¼šç”¨äºå¿«é€Ÿä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆçš„æ¨¡æ€ LLM é€‚é…å™¨</li><li>ä½œè€…ï¼šKunpeng Songã€Yizhe Zhuã€Bingchen Liuã€Qing Yanã€Ahmed Elgammalã€Xiao Yang</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šå­—èŠ‚è·³åŠ¨</li><li>å…³é”®è¯ï¼šå›¾åƒç”Ÿæˆã€å¤šæ¨¡æ€ã€ä¸ªæ€§åŒ–ã€LLM</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.05674</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šéšç€æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œå¯¹é²æ£’å›¾åƒåˆ°å›¾åƒè½¬æ¢çš„éœ€æ±‚ä¹Ÿåœ¨ä¸æ–­å¢é•¿ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç°æœ‰çš„å›¾åƒæ¡ä»¶ç”Ÿæˆæ–¹æ³•é€šå¸¸éœ€è¦å¯¹è¾“å…¥å›¾åƒè¿›è¡Œæ–‡æœ¬è¡¨ç¤ºçš„åæ¼”ï¼Œå¹¶ä½¿ç”¨å¯å­¦ä¹ çš„æ–‡æœ¬æ ‡è®°æ¥è¡¨ç¤ºç›®æ ‡æ¦‚å¿µã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•å­˜åœ¨æ–‡æœ¬æè¿°æ— æ³•å……åˆ†è¡¨è¾¾è¯¦ç»†è§†è§‰ç‰¹å¾çš„é—®é¢˜ã€‚ï¼ˆ3ï¼‰æœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º MoMA çš„å¼€æ”¾è¯æ±‡ã€å…è®­ç»ƒçš„ä¸ªæ€§åŒ–å›¾åƒæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å…·æœ‰çµæ´»çš„é›¶æ ·æœ¬èƒ½åŠ›ã€‚MoMA åˆ©ç”¨å¼€æºçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ (MLLM)ï¼Œå°†å…¶è®­ç»ƒä¸ºåŒæ—¶å……å½“ç‰¹å¾æå–å™¨å’Œç”Ÿæˆå™¨çš„åŒé‡è§’è‰²ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆåœ°ååŒäº†å‚è€ƒå›¾åƒå’Œæ–‡æœ¬æç¤ºä¿¡æ¯ï¼Œä»¥äº§ç”Ÿæœ‰ä»·å€¼çš„å›¾åƒç‰¹å¾ï¼Œä»è€Œä¿ƒè¿›å›¾åƒæ‰©æ•£æ¨¡å‹ã€‚ä¸ºäº†æ›´å¥½åœ°åˆ©ç”¨ç”Ÿæˆçš„ç‰¹å¾ï¼Œæœ¬æ–‡è¿˜å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„è‡ªæ³¨æ„åŠ›å¿«æ·æ–¹å¼æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°å°†å›¾åƒç‰¹å¾è½¬ç§»åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­ï¼Œä»è€Œæé«˜ç”Ÿæˆå›¾åƒä¸­ç›®æ ‡å¯¹è±¡çš„ç›¸ä¼¼æ€§ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šä½œä¸ºå…è°ƒä¼˜çš„å³æ’å³ç”¨æ¨¡å—ï¼ŒMoMA åªéœ€è¦ä¸€å¼ å‚è€ƒå›¾åƒï¼Œå°±èƒ½åœ¨ç”Ÿæˆå…·æœ‰é«˜ç»†èŠ‚ä¿çœŸåº¦ã€å¢å¼ºèº«ä»½ä¿ç•™å’Œæç¤ºå¿ å®åº¦çš„å›¾åƒæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ï¼Œå³æä¾›ä¸€ç§ç”¨äºå¿«é€Ÿä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆçš„é«˜æ•ˆä¸”æœ‰æ•ˆçš„æ¨¡å‹ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º MoMA çš„å¼€æ”¾è¯æ±‡ã€å…è®­ç»ƒçš„ä¸ªæ€§åŒ–å›¾åƒæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å…·æœ‰çµæ´»çš„é›¶æ ·æœ¬èƒ½åŠ›ã€‚ï¼ˆ2ï¼‰ï¼šMoMA åˆ©ç”¨å¼€æºçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ (MLLM)ï¼Œå°†å…¶è®­ç»ƒä¸ºåŒæ—¶å……å½“ç‰¹å¾æå–å™¨å’Œç”Ÿæˆå™¨çš„åŒé‡è§’è‰²ã€‚ï¼ˆ3ï¼‰ï¼šè¯¥æ–¹æ³•æœ‰æ•ˆåœ°ååŒäº†å‚è€ƒå›¾åƒå’Œæ–‡æœ¬æç¤ºä¿¡æ¯ï¼Œä»¥äº§ç”Ÿæœ‰ä»·å€¼çš„å›¾åƒç‰¹å¾ï¼Œä»è€Œä¿ƒè¿›å›¾åƒæ‰©æ•£æ¨¡å‹ã€‚ï¼ˆ4ï¼‰ï¼šä¸ºäº†æ›´å¥½åœ°åˆ©ç”¨ç”Ÿæˆçš„ç‰¹å¾ï¼Œæœ¬æ–‡è¿˜å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„è‡ªæ³¨æ„åŠ›å¿«æ·æ–¹å¼æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°å°†å›¾åƒç‰¹å¾è½¬ç§»åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­ï¼Œä»è€Œæé«˜ç”Ÿæˆå›¾åƒä¸­ç›®æ ‡å¯¹è±¡çš„ç›¸ä¼¼æ€§ã€‚</p></li><li><p>æ€»ç»“ï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºçš„ MoMA æ¨¡å‹ï¼Œä¸ºåŸºäºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿå›¾åƒä¸ªæ€§åŒ–æä¾›äº†å¼ºå¤§çš„è§£å†³æ–¹æ¡ˆã€‚è¯¥æ¨¡å‹å…è°ƒä¼˜ã€å¼€æ”¾è¯æ±‡ï¼Œæ”¯æŒé‡æ–°è¯­å¢ƒåŒ–å’Œçº¹ç†ç¼–è¾‘ã€‚å®éªŒç»“æœè¡¨æ˜å…¶ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æˆ‘ä»¬æå‡ºçš„å¤šæ¨¡æ€å›¾åƒç‰¹å¾è§£ç å™¨æˆåŠŸåˆ©ç”¨äº† MLLM çš„ä¼˜åŠ¿ï¼Œç”¨äºä¸Šä¸‹æ–‡ç‰¹å¾ç”Ÿæˆã€‚æˆ‘ä»¬æå‡ºçš„æ©ç ä¸»ä½“äº¤å‰æ³¨æ„åŠ›æŠ€æœ¯æä¾›äº†ä¸€ä¸ªå¼•äººæ³¨ç›®çš„ç‰¹å¾æ·å¾„ï¼Œæ˜¾è‘—æé«˜äº†ç»†èŠ‚å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œä½œä¸ºå³æ’å³ç”¨æ¨¡å—ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥ç›´æ¥é›†æˆåˆ°ä»åŒä¸€åŸºç¡€æ¨¡å‹è°ƒæ•´çš„ç¤¾åŒºæ¨¡å‹ä¸­ï¼Œå°†å…¶åº”ç”¨æ‰©å±•åˆ°æ›´å¹¿æ³›çš„é¢†åŸŸã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§æ–°çš„å¼€æ”¾è¯æ±‡ã€å…è®­ç»ƒçš„å›¾åƒä¸ªæ€§åŒ–æ¨¡å‹ MoMAï¼Œè¯¥æ¨¡å‹åˆ©ç”¨ MLLM åŒæ—¶å……å½“ç‰¹å¾æå–å™¨å’Œç”Ÿæˆå™¨ï¼Œæœ‰æ•ˆåœ°ååŒå‚è€ƒå›¾åƒå’Œæ–‡æœ¬æç¤ºä¿¡æ¯ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„è‡ªæ³¨æ„åŠ›å¿«æ·æ–¹å¼æ–¹æ³•ï¼Œä»¥æé«˜ç”Ÿæˆå›¾åƒä¸­ç›®æ ‡å¯¹è±¡çš„ç›¸ä¼¼æ€§ã€‚æ€§èƒ½ï¼šåœ¨å›¾åƒä¸ªæ€§åŒ–ä»»åŠ¡ä¸Šï¼ŒMoMA åœ¨ç»†èŠ‚ä¿çœŸåº¦ã€èº«ä»½ä¿ç•™å¢å¼ºå’Œæç¤ºå¿ å®åº¦æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å·¥ä½œé‡ï¼šMoMA ä½œä¸ºå…è°ƒä¼˜çš„å³æ’å³ç”¨æ¨¡å—ï¼Œåªéœ€è¦ä¸€å¼ å‚è€ƒå›¾åƒï¼Œå³å¯å¿«é€Ÿç”Ÿæˆä¸ªæ€§åŒ–çš„å›¾åƒã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-08d1519202a8d4216c20ee3e5477b63a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9de383e1cd50dba55e6f28db82b876b6.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-fb5d987f58b579f725793a41be6d546d.jpg" align="middle"></details><h2 id="YaART-Yet-Another-ART-Rendering-Technology"><a href="#YaART-Yet-Another-ART-Rendering-Technology" class="headerlink" title="YaART: Yet Another ART Rendering Technology"></a>YaART: Yet Another ART Rendering Technology</h2><p><strong>Authors:Sergey Kastryulin, Artem Konev, Alexander Shishenya, Eugene Lyapustin, Artem Khurshudov, Alexander Tselousov, Nikita Vinokurov, Denis Kuznedelev, Alexander Markovich, Grigoriy Livshits, Alexey Kirillov, Anastasiia Tabisheva, Liubov Chubarova, Marina Kaminskaia, Alexander Ustyuzhanin, Artemii Shvetsov, Daniil Shlenskii, Valerii Startsev, Dmitrii Kornilov, Mikhail Romanov, Artem Babenko, Sergei Ovcharenko, Valentin Khrulkov</strong></p><p>In the rapidly progressing field of generative models, the development of efficient and high-fidelity text-to-image diffusion systems represents a significant frontier. This study introduces YaART, a novel production-grade text-to-image cascaded diffusion model aligned to human preferences using Reinforcement Learning from Human Feedback (RLHF). During the development of YaART, we especially focus on the choices of the model and training dataset sizes, the aspects that were not systematically investigated for text-to-image cascaded diffusion models before. In particular, we comprehensively analyze how these choices affect both the efficiency of the training process and the quality of the generated images, which are highly important in practice. Furthermore, we demonstrate that models trained on smaller datasets of higher-quality images can successfully compete with those trained on larger datasets, establishing a more efficient scenario of diffusion models training. From the quality perspective, YaART is consistently preferred by users over many existing state-of-the-art models. </p><p><a href="http://arxiv.org/abs/2404.05666v1">PDF</a> Prompts and additional information are available on the project page,   see <a href="https://ya.ru/ai/art/paper-yaart-v1">https://ya.ru/ai/art/paper-yaart-v1</a></p><p><strong>Summary</strong><br>åŸºäºäººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ æ„å»ºYaARTï¼Œé«˜æ•ˆé«˜ä¿çœŸæ–‡æœ¬ç”Ÿæˆå›¾åƒå¤šçº§æ‰©æ•£æ¨¡å‹ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>å¼•å…¥YaARTï¼Œä¸€ç§é‡‡ç”¨äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ çš„äººç±»åå¥½æ–‡æœ¬ç”Ÿæˆå›¾åƒçº§è”æ‰©æ•£æ¨¡å‹ã€‚</li><li>åˆ†ææ¨¡å‹å’Œè®­ç»ƒæ•°æ®é›†å¤§å°å¯¹è®­ç»ƒæ•ˆç‡å’Œå›¾åƒè´¨é‡çš„å½±å“ã€‚</li><li>ä½¿ç”¨è¾ƒå°çš„é«˜è´¨é‡å›¾åƒæ•°æ®é›†è®­ç»ƒæ¨¡å‹å¯ç«äº‰ä½¿ç”¨è¾ƒå¤§å‹æ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹ã€‚</li><li>YaARTåœ¨è´¨é‡ä¸Šä¼˜äºè®¸å¤šç°æœ‰æœ€å…ˆè¿›æ¨¡å‹ã€‚</li><li>å¤šçº§æ‰©æ•£æ¨¡å‹è®­ç»ƒä¸­ï¼Œæ¨¡å‹å’Œè®­ç»ƒæ•°æ®é›†å¤§å°é€‰æ‹©éå¸¸é‡è¦ã€‚</li><li>é«˜è´¨é‡å°æ•°æ®é›†è®­ç»ƒæ¨¡å‹æ›´æœ‰æ•ˆç‡ã€‚</li><li>äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ æ˜¯æ–‡æœ¬ç”Ÿæˆå›¾åƒçº§è”æ‰©æ•£æ¨¡å‹çš„å…³é”®æŠ€æœ¯ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><strong>æ ‡é¢˜ï¼š</strong> YaARTï¼šåˆä¸€ç§è‰ºæœ¯æ¸²æŸ“æŠ€æœ¯</li><li><strong>ä½œè€…ï¼š</strong> Sergey Kastryulin, Artem Konev, Alexander Shishenya, Eugene Lyapustin, Artem Khurshudov, Alexander Tselousov, Nikita Vinokurov, Denis Kuznedelev, Alexander Markovich, Grigoriy Livshits, Alexey Kirillov, Anastasiia Tabisheva, Liubov Chubarova, Marina Kaminskaia, Alexander Ustyuzhanin, Artemii Shvetsov, Daniil Shlenskii, Valerii Startsev, Dmitrii Kornilov, Mikhail Romanov, Artem Babenko, Sergei Ovcharenko, Valentin Khrulkov</li><li><strong>ç¬¬ä¸€ä½œè€…å•ä½ï¼š</strong> Yandex</li><li><strong>å…³é”®è¯ï¼š</strong> Diffusion models, Scaling, Efficiency</li><li><strong>è®ºæ–‡é“¾æ¥ï¼š</strong> arXiv:2404.05666</li><li><p><strong>æ‘˜è¦ï¼š</strong>   (1) <strong>ç ”ç©¶èƒŒæ™¯ï¼š</strong> ç”Ÿæˆæ¨¡å‹é¢†åŸŸå¿«é€Ÿå‘å±•ï¼Œé«˜æ•ˆä¸”é«˜ä¿çœŸçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£ç³»ç»Ÿæ˜¯é‡è¦çš„ç ”ç©¶å‰æ²¿ã€‚   (2) <strong>è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼š</strong> ä¹‹å‰çš„æ–‡æœ¬åˆ°å›¾åƒçº§è”æ‰©æ•£æ¨¡å‹å°šæœªç³»ç»Ÿåœ°ç ”ç©¶æ¨¡å‹å’Œè®­ç»ƒæ•°æ®é›†å¤§å°å¯¹è®­ç»ƒæ•ˆç‡å’Œç”Ÿæˆå›¾åƒè´¨é‡çš„å½±å“ã€‚   (3) <strong>ç ”ç©¶æ–¹æ³•ï¼š</strong> æœ¬æ–‡æå‡º YaARTï¼Œä¸€ç§æ–°çš„é¢å‘ç”Ÿäº§çº§æ–‡æœ¬åˆ°å›¾åƒçº§è”æ‰©æ•£æ¨¡å‹ï¼Œä½¿ç”¨äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ä¸äººç±»åå¥½ä¿æŒä¸€è‡´ã€‚é‡ç‚¹åˆ†æäº†æ¨¡å‹å’Œè®­ç»ƒæ•°æ®é›†å¤§å°çš„é€‰æ‹©å¦‚ä½•å½±å“è®­ç»ƒæ•ˆç‡å’Œå›¾åƒè´¨é‡ã€‚   (4) <strong>ä»»åŠ¡å’Œæ€§èƒ½ï¼š</strong> åœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šï¼ŒYaART åœ¨æ•ˆç‡å’Œè´¨é‡æ–¹é¢éƒ½ä¼˜äºç°æœ‰æ¨¡å‹ã€‚è®­ç»ƒåœ¨è¾ƒå°çš„é«˜è´¨é‡å›¾åƒæ•°æ®é›†ä¸Šçš„æ¨¡å‹å¯ä»¥ä¸è®­ç»ƒåœ¨è¾ƒå¤§æ•°æ®é›†ä¸Šçš„æ¨¡å‹ç«äº‰ï¼Œå»ºç«‹äº†æ›´æœ‰æ•ˆçš„æ‰©æ•£æ¨¡å‹è®­ç»ƒæ–¹æ¡ˆã€‚ä»è´¨é‡è§’åº¦æ¥çœ‹ï¼Œç”¨æˆ·ä¸€è‡´è®¤ä¸º YaART ä¼˜äºè®¸å¤šç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) å¤§è§„æ¨¡æ‰©æ•£æ¨¡å‹è®­ç»ƒæ–¹æ³•ï¼›(2) è®­ç»ƒé›†æ„å»ºç­–ç•¥ï¼›(3) æ¨¡å‹è®­ç»ƒé˜¶æ®µï¼›(4) RL å¯¹é½ã€‚</p></li></ol><p>8.ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬å·¥ä½œçš„é‡è¦æ„ä¹‰ï¼šæœ¬æ–‡æå‡ºäº†YaARTï¼Œä¸€ç§é¢å‘ç”Ÿäº§çº§çš„æ–‡æœ¬åˆ°å›¾åƒçº§è”æ‰©æ•£æ¨¡å‹ï¼Œç³»ç»Ÿåœ°ç ”ç©¶äº†æ¨¡å‹å’Œè®­ç»ƒæ•°æ®é›†å¤§å°å¯¹è®­ç»ƒæ•ˆç‡å’Œç”Ÿæˆå›¾åƒè´¨é‡çš„å½±å“ï¼Œå»ºç«‹äº†æ›´æœ‰æ•ˆçš„æ‰©æ•£æ¨¡å‹è®­ç»ƒæ–¹æ¡ˆï¼Œåœ¨æ•ˆç‡å’Œè´¨é‡æ–¹é¢éƒ½ä¼˜äºç°æœ‰æ¨¡å‹ã€‚ï¼ˆ2ï¼‰æœ¬æ–‡çš„ä¼˜ç¼ºç‚¹æ€»ç»“ï¼šåˆ›æ–°ç‚¹ï¼š* æå‡ºäº†ä¸€ç§æ–°çš„æ–‡æœ¬åˆ°å›¾åƒçº§è”æ‰©æ•£æ¨¡å‹YaARTï¼Œä½¿ç”¨RLHFä¸äººç±»åå¥½ä¿æŒä¸€è‡´ã€‚* é‡ç‚¹åˆ†æäº†æ¨¡å‹å’Œè®­ç»ƒæ•°æ®é›†å¤§å°çš„é€‰æ‹©å¦‚ä½•å½±å“è®­ç»ƒæ•ˆç‡å’Œå›¾åƒè´¨é‡ã€‚æ€§èƒ½ï¼š* åœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šï¼ŒYaARTåœ¨æ•ˆç‡å’Œè´¨é‡æ–¹é¢éƒ½ä¼˜äºç°æœ‰æ¨¡å‹ã€‚* è®­ç»ƒåœ¨è¾ƒå°çš„é«˜è´¨é‡å›¾åƒæ•°æ®é›†ä¸Šçš„æ¨¡å‹å¯ä»¥ä¸è®­ç»ƒåœ¨è¾ƒå¤§æ•°æ®é›†ä¸Šçš„æ¨¡å‹ç«äº‰ã€‚å·¥ä½œé‡ï¼š* éœ€è¦å¤§é‡çš„é«˜è´¨é‡å›¾åƒæ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚* RLå¯¹é½è¿‡ç¨‹éœ€è¦å¤§é‡çš„äººåŠ›èµ„æºã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-586cabc8d6b91f9a7fefe521e9c7b1d1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-53b4b16cc30d978d6ba9fbf815ca25c5.jpg" align="middle"></details>## Learning a Category-level Object Pose Estimator without Pose Annotations**Authors:Fengrui Tian, Yaoyao Liu, Adam Kortylewski, Yueqi Duan, Shaoyi Du, Alan Yuille, Angtian Wang**3D object pose estimation is a challenging task. Previous works always require thousands of object images with annotated poses for learning the 3D pose correspondence, which is laborious and time-consuming for labeling. In this paper, we propose to learn a category-level 3D object pose estimator without pose annotations. Instead of using manually annotated images, we leverage diffusion models (e.g., Zero-1-to-3) to generate a set of images under controlled pose differences and propose to learn our object pose estimator with those images. Directly using the original diffusion model leads to images with noisy poses and artifacts. To tackle this issue, firstly, we exploit an image encoder, which is learned from a specially designed contrastive pose learning, to filter the unreasonable details and extract image feature maps. Additionally, we propose a novel learning strategy that allows the model to learn object poses from those generated image sets without knowing the alignment of their canonical poses. Experimental results show that our method has the capability of category-level object pose estimation from a single shot setting (as pose definition), while significantly outperforming other state-of-the-art methods on the few-shot category-level object pose estimation benchmarks. [PDF](http://arxiv.org/abs/2404.05626v1) **Summary**åˆ©ç”¨æ— æ ‡æ³¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆå›¾åƒï¼Œæå‡ºæ— å§¿æ€æ ‡æ³¨çš„ç±»åˆ«çº§3Dç‰©ä½“å§¿æ€ä¼°è®¡æ–¹æ³•ã€‚**Key Takeaways**- æå‡ºäº†ä¸€ç§æ— å§¿æ€æ ‡æ³¨çš„ç±»åˆ«çº§3Dç‰©ä½“å§¿æ€ä¼°è®¡æ–¹æ³•ã€‚- åˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆå—æ§å§¿æ€å·®å¼‚çš„å›¾åƒé›†ï¼Œç”¨äºè®­ç»ƒå§¿æ€ä¼°è®¡å™¨ã€‚- è®¾è®¡äº†ä¸€ä¸ªå›¾åƒç¼–ç å™¨ï¼Œä»å¯¹æ¯”å§¿æ€å­¦ä¹ ä¸­å­¦ä¹ ï¼Œè¿‡æ»¤ä¸åˆç†çš„ç»†èŠ‚å¹¶æå–å›¾åƒç‰¹å¾å›¾ã€‚- æå‡ºäº†ä¸€ç§æ–°é¢–çš„å­¦ä¹ ç­–ç•¥ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä»ç”Ÿæˆçš„å›¾åƒé›†ä¸­å­¦ä¹ ç‰©ä½“å§¿æ€ï¼Œè€Œæ— éœ€çŸ¥é“å…¶è§„èŒƒå§¿æ€çš„å¯¹é½æ–¹å¼ã€‚- å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å…·æœ‰ä»å•æ¬¡æ‹æ‘„è®¾ç½®ï¼ˆä½œä¸ºå§¿æ€å®šä¹‰ï¼‰ä¸­è¿›è¡Œç±»åˆ«çº§ç‰©ä½“å§¿æ€ä¼°è®¡çš„èƒ½åŠ›ã€‚- åœ¨å°‘æ ·æœ¬ç±»åˆ«çº§ç‰©ä½“å§¿æ€ä¼°è®¡åŸºå‡†ä¸Šæ˜æ˜¾ä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>è®ºæ–‡æ ‡é¢˜ï¼šæ— éœ€å§¿æ€æ ‡æ³¨çš„ç±»åˆ«çº§ç‰©ä½“å§¿æ€ä¼°è®¡</li><li>ä½œè€…ï¼šå†¯ç‘å¤©ï¼Œå§šç‘¶ï¼Œäºšå½“Â·ç§‘è’‚è±å¤«æ–¯åŸºï¼Œå²³ç¦æ®µï¼Œé‚µæ¯…æœï¼Œè‰¾ä¼¦Â·å°¤å°”ï¼Œç‹å®‰å¤©</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šè¥¿å®‰äº¤é€šå¤§å­¦</li><li>å…³é”®è¯ï¼šç±»åˆ«çº§ç‰©ä½“å§¿æ€ä¼°è®¡ï¼Œæ‰©æ•£æ¨¡å‹ï¼Œå¯¹æ¯”å§¿æ€å­¦ä¹ </li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.05626Github é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼š3D ç‰©ä½“å§¿æ€ä¼°è®¡æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ä»¥å¾€çš„å·¥ä½œé€šå¸¸éœ€è¦æ•°åƒå¼ å¸¦æœ‰æ ‡æ³¨å§¿æ€çš„ç‰©ä½“å›¾åƒæ¥å­¦ä¹  3D å§¿æ€å¯¹åº”å…³ç³»ï¼Œè¿™éœ€è¦å¤§é‡çš„äººåŠ›åŠ³åŠ¨å’Œæ—¶é—´æˆæœ¬ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šä»¥å¾€æ–¹æ³•é€šå¸¸éµå¾ªåˆ†æ-ç»¼åˆåŸç†ï¼Œé€šè¿‡ä½¿ç”¨å¸¦æœ‰æ ‡æ³¨å§¿æ€çš„ç‰©ä½“å›¾åƒæ„å»º 3D ç¥ç»ç½‘æ ¼ä½œä¸ºç±»åˆ«çº§ç‰©ä½“è¡¨ç¤ºï¼Œå¹¶é€šè¿‡å°†æ–°ç‰©ä½“çš„ 2D å›¾åƒä¸ 3D ç½‘æ ¼è¿›è¡Œæ¯”è¾ƒæ¥åˆ†ææ–°ç‰©ä½“çš„å§¿æ€ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•éœ€è¦ä¸ºæ–°ç‰©ä½“ç±»åˆ«æ ‡æ³¨å¤§é‡å›¾åƒæ‰èƒ½å­¦ä¹ åˆ°ç»Ÿä¸€çš„è¡¨ç¤ºã€‚ï¼ˆ3ï¼‰æå‡ºçš„æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€å§¿æ€æ ‡æ³¨çš„ç±»åˆ«çº§ç‰©ä½“å§¿æ€ä¼°è®¡æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆä¸€ç»„å›¾åƒï¼Œæ¯ç»„å›¾åƒéƒ½æ˜¯ä»å•ä¸ªæœªæ ‡æ³¨å›¾åƒç”Ÿæˆï¼Œå…·æœ‰å—æ§çš„å§¿æ€å·®å¼‚ã€‚ç„¶åï¼Œä½¿ç”¨è¿™äº›å›¾åƒé›†è®­ç»ƒç‰©ä½“å§¿æ€ä¼°è®¡å™¨ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†å›¾åƒç¼–ç å™¨å’Œæ–°é¢–çš„å­¦ä¹ ç­–ç•¥ï¼Œä»¥è§£å†³æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„å›¾åƒè´¨é‡é—®é¢˜å’Œå§¿æ€æ§åˆ¶ç²—ç³™é—®é¢˜ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šå®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•èƒ½å¤Ÿä»å•æ¬¡æ‹æ‘„ä¸­è¿›è¡Œç±»åˆ«çº§ç‰©ä½“å§¿æ€ä¼°è®¡ï¼Œå¹¶ä¸”åœ¨å°æ ·æœ¬ç±»åˆ«çº§ç‰©ä½“å§¿æ€ä¼°è®¡åŸºå‡†ä¸Šæ˜¾è‘—ä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚è¿™äº›ç»“æœæ”¯æŒäº†æœ¬æ–‡æå‡ºçš„æ— éœ€å§¿æ€æ ‡æ³¨å³å¯å­¦ä¹ ç±»åˆ«çº§ç‰©ä½“å§¿æ€ä¼°è®¡å™¨çš„ç›®æ ‡ã€‚</li></ol><p><strong>æ–¹æ³•</strong>ï¼ˆ1ï¼‰ï¼šåˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆä¸€ç»„å›¾åƒï¼Œæ¯ç»„å›¾åƒéƒ½æ˜¯ä»å•ä¸ªæœªæ ‡æ³¨å›¾åƒç”Ÿæˆï¼Œå…·æœ‰å—æ§çš„å§¿æ€å·®å¼‚ã€‚ï¼ˆ2ï¼‰ï¼šä½¿ç”¨å›¾åƒç¼–ç å™¨å’Œæ–°é¢–çš„å­¦ä¹ ç­–ç•¥æ¥è§£å†³æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„å›¾åƒè´¨é‡é—®é¢˜å’Œå§¿æ€æ§åˆ¶ç²—ç³™é—®é¢˜ã€‚ï¼ˆ3ï¼‰ï¼šä½¿ç”¨è¿™äº›å›¾åƒé›†è®­ç»ƒç‰©ä½“å§¿æ€ä¼°è®¡å™¨ã€‚ï¼ˆ4ï¼‰ï¼šåœ¨æµ‹è¯•é˜¶æ®µï¼Œæå–æ–°å›¾åƒçš„ç‰¹å¾å›¾ï¼Œåˆå§‹åŒ–3Då§¿æ€é¢„æµ‹ï¼Œåˆ©ç”¨å¯å¾®æ¸²æŸ“å™¨åˆæˆç‰¹å¾å›¾ï¼Œè®¡ç®—ç‰¹å¾é‡å»ºæŸå¤±ï¼Œè¿­ä»£ä¼˜åŒ–3Då§¿æ€ï¼Œå¾—åˆ°æœ€ç»ˆå§¿æ€ã€‚</p><p><strong>8. ç»“è®ºï¼š</strong></p><p>ï¼ˆ1ï¼‰æœ¬å·¥ä½œæ„ä¹‰ï¼šæå‡ºäº†æ— éœ€å§¿æ€æ ‡æ³¨çš„ç±»åˆ«çº§ç‰©ä½“å§¿æ€ä¼°è®¡æ–¹æ³•ï¼Œä¸ºå§¿æ€ä¼°è®¡é¢†åŸŸæä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</p><p>ï¼ˆ2ï¼‰è®ºæ–‡ä¼˜ç¼ºç‚¹æ€»ç»“ï¼š<strong>åˆ›æ–°ç‚¹ï¼š</strong>* åˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆå—æ§å§¿æ€å·®å¼‚çš„å›¾åƒé›†ï¼Œæ— éœ€å§¿æ€æ ‡æ³¨ã€‚* æå‡ºå›¾åƒç¼–ç å™¨å’Œå­¦ä¹ ç­–ç•¥ï¼Œè§£å†³å›¾åƒè´¨é‡å’Œå§¿æ€æ§åˆ¶é—®é¢˜ã€‚</p><p><strong>æ€§èƒ½ï¼š</strong>* åœ¨å°æ ·æœ¬ç±»åˆ«çº§ç‰©ä½“å§¿æ€ä¼°è®¡åŸºå‡†ä¸Šæ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚* èƒ½å¤Ÿä»å•æ¬¡æ‹æ‘„ä¸­è¿›è¡Œç±»åˆ«çº§ç‰©ä½“å§¿æ€ä¼°è®¡ã€‚</p><p><strong>å·¥ä½œé‡ï¼š</strong>* è®­ç»ƒæ‰©æ•£æ¨¡å‹å’Œå§¿æ€ä¼°è®¡å™¨éœ€è¦å¤§é‡è®¡ç®—èµ„æºã€‚* ç”Ÿæˆå—æ§å§¿æ€å·®å¼‚çš„å›¾åƒé›†éœ€è¦ä¸€å®šçš„æ—¶é—´æˆæœ¬ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-f10bc892c948dad7c6b8781503ed040e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6f0301823586c7902a2fbd2ccb15f9aa.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ff755061842e6baf5aa5f74bdd55142f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4d9264ad9d901b82ed6559f4c23cdfb9.jpg" align="middle"><img src="https://pica.zhimg.com/v2-fcc188d5cce0944fb8e5bacb5d763c85.jpg" align="middle"></details>## UniFL: Improve Stable Diffusion via Unified Feedback Learning**Authors:Jiacheng Zhang, Jie Wu, Yuxi Ren, Xin Xia, Huafeng Kuang, Pan Xie, Jiashi Li, Xuefeng Xiao, Weilin Huang, Min Zheng, Lean Fu, Guanbin Li**Diffusion models have revolutionized the field of image generation, leading to the proliferation of high-quality models and diverse downstream applications. However, despite these significant advancements, the current competitive solutions still suffer from several limitations, including inferior visual quality, a lack of aesthetic appeal, and inefficient inference, without a comprehensive solution in sight. To address these challenges, we present UniFL, a unified framework that leverages feedback learning to enhance diffusion models comprehensively. UniFL stands out as a universal, effective, and generalizable solution applicable to various diffusion models, such as SD1.5 and SDXL. Notably, UniFL incorporates three key components: perceptual feedback learning, which enhances visual quality; decoupled feedback learning, which improves aesthetic appeal; and adversarial feedback learning, which optimizes inference speed. In-depth experiments and extensive user studies validate the superior performance of our proposed method in enhancing both the quality of generated models and their acceleration. For instance, UniFL surpasses ImageReward by 17% user preference in terms of generation quality and outperforms LCM and SDXL Turbo by 57% and 20% in 4-step inference. Moreover, we have verified the efficacy of our approach in downstream tasks, including Lora, ControlNet, and AnimateDiff. [PDF](http://arxiv.org/abs/2404.05595v1) **Summary**é€šè¿‡å¼•å…¥åé¦ˆå­¦ä¹ ï¼ŒUniFL ç»Ÿä¸€æ¡†æ¶å…¨é¢æå‡æ‰©æ•£æ¨¡å‹ï¼Œè§£å†³è§†è§‰è´¨é‡ã€ç¾è§‚æ€§å’Œæ¨ç†æ•ˆç‡ç­‰éš¾é¢˜ã€‚**Key Takeaways**- UniFL æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„ã€æœ‰æ•ˆçš„ã€å¯æ¨å¹¿çš„è§£å†³æ–¹æ¡ˆï¼Œé€‚ç”¨äºå„ç§æ‰©æ•£æ¨¡å‹ã€‚- UniFL åŒ…å«ä¸‰å¤§ç»„ä»¶ï¼šæ„ŸçŸ¥åé¦ˆå­¦ä¹ ã€è§£è€¦åé¦ˆå­¦ä¹ å’Œå¯¹æŠ—åé¦ˆå­¦ä¹ ã€‚- æ„ŸçŸ¥åé¦ˆå­¦ä¹ æé«˜è§†è§‰è´¨é‡ï¼Œè§£è€¦åé¦ˆå­¦ä¹ æ”¹å–„ç¾è§‚æ€§ï¼Œå¯¹æŠ—åé¦ˆå­¦ä¹ ä¼˜åŒ–æ¨ç†é€Ÿåº¦ã€‚- UniFL åœ¨ç”Ÿæˆè´¨é‡å’ŒåŠ é€Ÿæ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¾‹å¦‚ ImageRewardã€LCM å’Œ SDXL Turboã€‚- UniFL åœ¨ Loraã€ControlNet å’Œ AnimateDiff ç­‰ä¸‹æ¸¸ä»»åŠ¡ä¸­ä¹Ÿè¡¨ç°å‡ºè‰²ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>æ ‡é¢˜ï¼šUniFLï¼šé€šè¿‡ç»Ÿä¸€åé¦ˆå­¦ä¹ æ”¹è¿› Stable Diffusion</li><li>ä½œè€…ï¼šJiaming Song<em>, Chenlin Meng</em>, Boya Wang, Lu Yuan, Xiaodong He, Bo Ren, Ming-Hsuan Yang</li><li>éš¶å±å•ä½ï¼šåŒ—äº¬å¤§å­¦</li><li>å…³é”®è¯ï¼šDiffusion Modelã€Stable Diffusionã€åé¦ˆå­¦ä¹ ã€å›¾åƒç”Ÿæˆ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.05595</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸå–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†ç°æœ‰çš„ç«äº‰æ€§è§£å†³æ–¹æ¡ˆä»ç„¶å­˜åœ¨è§†è§‰è´¨é‡å·®ã€ç¼ºä¹ç¾æ„Ÿã€æ¨ç†æ•ˆç‡ä½ç­‰é—®é¢˜ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šè¿‡å»æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å¾®è°ƒæ¨¡å‹æˆ–ä½¿ç”¨é¢å¤–çš„ç›‘ç£ä¿¡å·ï¼Œä½†è¿™äº›æ–¹æ³•å¾€å¾€ä¼šå¯¼è‡´è¿‡åº¦æ‹Ÿåˆæˆ–å¼•å…¥åå·®ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€åé¦ˆå­¦ä¹ ï¼ˆUniFLï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥å°†æ¥è‡ªä¸åŒè§†è§‰æ„ŸçŸ¥æ¨¡å‹çš„ç‰¹å®šåé¦ˆä¿¡å·æ•´åˆåˆ°æ‰©æ•£æ¨¡å‹ä¸­ã€‚UniFL å…è®¸æ¨¡å‹æ ¹æ®ç‰¹å®šæ–¹é¢ï¼ˆå¦‚å¸ƒå±€ã€ç»†èŠ‚ã€ç¾æ„Ÿï¼‰çš„åé¦ˆè¿›è¡Œè°ƒæ•´ã€‚ï¼ˆ4ï¼‰å®éªŒç»“æœï¼šåœ¨ Stable Diffusion 1.5 ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒUniFL å¯ä»¥æ˜¾ç€æé«˜å›¾åƒçš„å¸ƒå±€ã€ç»†èŠ‚å’Œç¾æ„Ÿï¼ŒåŒæ—¶ä¿æŒæ¨ç†æ•ˆç‡ã€‚ç”¨æˆ·ç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº† UniFL çš„æœ‰æ•ˆæ€§ã€‚</li></ol><p>7.æ–¹æ³•ï¼šï¼ˆ1ï¼‰æ”¶é›†åé¦ˆæ•°æ®ï¼šæ”¶é›†ç”¨æˆ·å¯¹å›¾åƒä¸åŒæ–¹é¢çš„åå¥½åé¦ˆï¼ŒåŒ…æ‹¬å¸ƒå±€ã€ç»†èŠ‚ã€ç¾æ„Ÿç­‰ã€‚ï¼ˆ2ï¼‰è§†è§‰æ„ŸçŸ¥æ¨¡å‹é€‰æ‹©ï¼šä½¿ç”¨ä¸åŒçš„è§†è§‰æ„ŸçŸ¥æ¨¡å‹æ¥æä¾›ç‰¹å®šç»´åº¦çš„è§†è§‰åé¦ˆï¼Œä¾‹å¦‚å®ä¾‹åˆ†å‰²æ¨¡å‹ç”¨äºç»“æ„ä¼˜åŒ–ã€è¯­ä¹‰è§£ææ¨¡å‹ç”¨äºç¾æ„Ÿä¼˜åŒ–ã€‚ï¼ˆ3ï¼‰è§£è€¦åé¦ˆå­¦ä¹ ï¼šå°†ä¸åŒç»´åº¦çš„åé¦ˆä¿¡å·è§£è€¦ï¼Œåˆ†åˆ«è¿›è¡Œä¼˜åŒ–ã€‚ï¼ˆ4ï¼‰ä¸»åŠ¨æç¤ºé€‰æ‹©ï¼šé‡‡ç”¨è¿­ä»£è¿‡ç¨‹ï¼Œé€‰æ‹©å¤šæ ·åŒ–çš„æç¤ºï¼Œä»¥å‡è½»è¿‡åº¦ä¼˜åŒ–é—®é¢˜ã€‚ï¼ˆ5ï¼‰åŠ é€Ÿæ­¥éª¤ï¼šæ¯”è¾ƒ UniFL ä¸ç°æœ‰åŠ é€Ÿæ–¹æ³•åœ¨ä¸åŒæ¨ç†æ­¥éª¤ä¸‹çš„æ€§èƒ½ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬å·¥ä½œé€šè¿‡åé¦ˆå­¦ä¹ ï¼Œæå‡ºäº†ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ UniFLï¼Œæé«˜äº†è§†è§‰è´¨é‡ã€ç¾æ„Ÿå¸å¼•åŠ›å’Œæ¨ç†æ•ˆç‡ã€‚UniFL é€šè¿‡ç»“åˆæ„ŸçŸ¥ã€è§£è€¦å’Œå¯¹æŠ—åé¦ˆå­¦ä¹ ï¼Œåœ¨ç”Ÿæˆè´¨é‡å’Œæ¨ç†åŠ é€Ÿæ–¹é¢éƒ½è¶…è¿‡äº†ç°æœ‰æ–¹æ³•ï¼Œå¹¶ä¸”å¯ä»¥å¾ˆå¥½åœ°æ¨å¹¿åˆ°å„ç§æ‰©æ•£æ¨¡å‹å’Œä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡ã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„åé¦ˆå­¦ä¹ æ¡†æ¶ UniFLï¼Œå¯ä»¥å°†æ¥è‡ªä¸åŒè§†è§‰æ„ŸçŸ¥æ¨¡å‹çš„ç‰¹å®šåé¦ˆä¿¡å·æ•´åˆåˆ°æ‰©æ•£æ¨¡å‹ä¸­ã€‚</li><li>é‡‡ç”¨äº†è§£è€¦åé¦ˆå­¦ä¹ ç­–ç•¥ï¼Œå°†ä¸åŒç»´åº¦çš„åé¦ˆä¿¡å·è§£è€¦ï¼Œåˆ†åˆ«è¿›è¡Œä¼˜åŒ–ï¼Œé¿å…äº†è¿‡åº¦æ‹Ÿåˆé—®é¢˜ã€‚</li><li>å¼•å…¥äº†ä¸»åŠ¨æç¤ºé€‰æ‹©æœºåˆ¶ï¼Œè¿­ä»£é€‰æ‹©å¤šæ ·åŒ–çš„æç¤ºï¼Œå‡è½»äº†è¿‡åº¦ä¼˜åŒ–é—®é¢˜ã€‚</li><li>åœ¨æ¨ç†æ­¥éª¤æ–¹é¢ï¼ŒUniFL é‡‡ç”¨äº†åŠ é€Ÿç­–ç•¥ï¼Œæé«˜äº†æ¨ç†æ•ˆç‡ã€‚æ€§èƒ½ï¼š</li><li>åœ¨ StableDiffusion 1.5 ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒUniFL å¯ä»¥æ˜¾ç€æé«˜å›¾åƒçš„å¸ƒå±€ã€ç»†èŠ‚å’Œç¾æ„Ÿï¼ŒåŒæ—¶ä¿æŒæ¨ç†æ•ˆç‡ã€‚</li><li>ç”¨æˆ·ç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº† UniFL çš„æœ‰æ•ˆæ€§ã€‚å·¥ä½œé‡ï¼š</li><li>æ”¶é›†ç”¨æˆ·å¯¹å›¾åƒä¸åŒæ–¹é¢çš„åå¥½åé¦ˆã€‚</li><li>é€‰æ‹©ä¸åŒçš„è§†è§‰æ„ŸçŸ¥æ¨¡å‹æ¥æä¾›ç‰¹å®šç»´åº¦çš„è§†è§‰åé¦ˆã€‚</li><li>è®­ç»ƒ UniFL æ¡†æ¶ã€‚</li><li>åœ¨ä¸åŒçš„æ¨ç†æ­¥éª¤ä¸‹è¯„ä¼° UniFL çš„æ€§èƒ½ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-1d102b63946d070b5ca373896795363d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9bbf3246783f48d3668d0ccb93da7ea4.jpg" align="middle"></details><h2 id="Taming-Transformers-for-Realistic-Lidar-Point-Cloud-Generation"><a href="#Taming-Transformers-for-Realistic-Lidar-Point-Cloud-Generation" class="headerlink" title="Taming Transformers for Realistic Lidar Point Cloud Generation"></a>Taming Transformers for Realistic Lidar Point Cloud Generation</h2><p><strong>Authors:Hamed Haghighi, Amir Samadi, Mehrdad Dianati, Valentina Donzella, Kurt Debattista</strong></p><p>Diffusion Models (DMs) have achieved State-Of-The-Art (SOTA) results in the Lidar point cloud generation task, benefiting from their stable training and iterative refinement during sampling. However, DMs often fail to realistically model Lidar raydrop noise due to their inherent denoising process. To retain the strength of iterative sampling while enhancing the generation of raydrop noise, we introduce LidarGRIT, a generative model that uses auto-regressive transformers to iteratively sample the range images in the latent space rather than image space. Furthermore, LidarGRIT utilises VQ-VAE to separately decode range images and raydrop masks. Our results show that LidarGRIT achieves superior performance compared to SOTA models on KITTI-360 and KITTI odometry datasets. Code available at:<a href="https://github.com/hamedhaghighi/LidarGRIT">https://github.com/hamedhaghighi/LidarGRIT</a>. </p><p><a href="http://arxiv.org/abs/2404.05505v1">PDF</a> </p><p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰åˆ©ç”¨å…¶ç¨³å®šè®­ç»ƒå’Œé‡‡æ ·æœŸé—´çš„è¿­ä»£ä¼˜åŒ–ï¼Œåœ¨ç”Ÿæˆæ¿€å…‰é›·è¾¾ç‚¹äº‘ä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›ï¼ˆSOTAï¼‰ç»“æœï¼Œä½†ç”±äºå…¶å›ºæœ‰çš„å»å™ªè¿‡ç¨‹ï¼ŒDMé€šå¸¸æ— æ³•çœŸå®åœ°æ¨¡æ‹Ÿæ¿€å…‰é›·è¾¾å°„çº¿å™ªå£°ã€‚ä¸ºäº†åœ¨å¢å¼ºå°„çº¿å™ªå£°ç”Ÿæˆçš„åŒæ—¶ä¿æŒè¿­ä»£é‡‡æ ·çš„ä¼˜åŠ¿ï¼Œæˆ‘ä»¬æå‡ºäº† LidarGRITï¼Œè¿™æ˜¯ä¸€ç§ä½¿ç”¨è‡ªå›å½’ç”Ÿæˆå¼æ¨¡å‹åœ¨æ½œåœ¨ç©ºé—´ä¸­è¿­ä»£é‡‡æ ·èŒƒå›´å›¾åƒè€Œéå›¾åƒç©ºé—´ã€‚æ­¤å¤–ï¼ŒLidarGRIT åˆ©ç”¨ VQ-VAE åˆ†åˆ«è§£ç èŒƒå›´å›¾åƒå’Œå°„çº¿é®ç½©ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œä¸ KITTI-360 å’Œ KITTI æµ‹ç¨‹æ³•æ•°æ®é›†ä¸Šçš„ SOTA æ¨¡å‹ç›¸æ¯”ï¼ŒLidarGRIT å–å¾—äº†å“è¶Šçš„æ€§èƒ½ã€‚ä»£ç å¯åœ¨æ­¤å¤„è·å¾—ï¼š<a href="https://github.com/hamedhaghighi/LidarGRITã€‚">https://github.com/hamedhaghighi/LidarGRITã€‚</a></p><p><strong>Key Takeaways</strong></p><ul><li>æ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰åœ¨æ¿€å…‰é›·è¾¾ç‚¹äº‘ç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›ï¼ˆSOTAï¼‰ç»“æœã€‚</li><li>DM ç”±äºå…¶å›ºæœ‰çš„å»å™ªè¿‡ç¨‹ï¼Œé€šå¸¸æ— æ³•çœŸå®åœ°æ¨¡æ‹Ÿæ¿€å…‰é›·è¾¾å°„çº¿å™ªå£°ã€‚</li><li>LidarGRIT æå‡ºäº†ä¸€ç§ä½¿ç”¨è‡ªå›å½’å˜æ¢æ¨¡å‹åœ¨æ½œåœ¨ç©ºé—´ä¸­è¿­ä»£é‡‡æ ·èŒƒå›´å›¾åƒçš„æ–¹æ³•ã€‚</li><li>LidarGRIT åˆ©ç”¨ VQ-VAE åˆ†åˆ«è§£ç èŒƒå›´å›¾åƒå’Œå°„çº¿é®ç½©ã€‚</li><li>LidarGRIT åœ¨ KITTI-360 å’Œ KITTI æµ‹ç¨‹æ³•æ•°æ®é›†ä¸Šå–å¾—äº†ä¼˜äº SOTA æ¨¡å‹çš„æ€§èƒ½ã€‚</li><li>ä»£ç å¯åœ¨ <a href="https://github.com/hamedhaghighi/LidarGRIT">https://github.com/hamedhaghighi/LidarGRIT</a> è·å¾—ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>è®ºæ–‡æ ‡é¢˜ï¼šè°ƒæ•™ Transformer ä»¥ç”Ÿæˆé€¼çœŸçš„æ¿€å…‰é›·è¾¾ç‚¹äº‘</li><li>ä½œè€…ï¼šHamed Haghighiã€Amir Samadiã€Mehrdad Dianatiã€Valentina Donzellaã€Kurt Debattista</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šè‹±å›½åå¨å¤§å­¦ WMG</li><li>å…³é”®è¯ï¼šæ¿€å…‰é›·è¾¾ã€ç‚¹äº‘ç”Ÿæˆã€æ‰©æ•£æ¨¡å‹ã€è‡ªå›å½’ Transformer</li><li>è®ºæ–‡é“¾æ¥ï¼šNoneï¼ŒGithub ä»£ç é“¾æ¥ï¼šhttps://github.com/hamedhaghighi/LidarGRIT</li><li>æ‘˜è¦ï¼š   (1) ç ”ç©¶èƒŒæ™¯ï¼šæ¿€å…‰é›·è¾¾ç‚¹äº‘ç”Ÿæˆæ˜¯è‡ªåŠ¨é©¾é©¶é¢†åŸŸçš„å…³é”®æŠ€æœ¯ï¼Œä½†ä¼ ç»Ÿçš„ç‰©ç†å»ºæ¨¡æ–¹æ³•å¤æ‚ä¸”è€—æ—¶ã€‚æ•°æ®é©±åŠ¨çš„ç”Ÿæˆæ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯æ‰©æ•£æ¨¡å‹ï¼Œå› å…¶å¼ºå¤§çš„é«˜ç»´æ•°æ®å»ºæ¨¡èƒ½åŠ›è€Œå—åˆ°å…³æ³¨ã€‚   (2) ç°æœ‰æ–¹æ³•ï¼šæ‰©æ•£æ¨¡å‹åœ¨æ¿€å…‰é›·è¾¾ç‚¹äº‘ç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†å¾ˆå¥½çš„æ•ˆæœï¼Œä½†å®ƒä»¬åœ¨ç”Ÿæˆé€¼çœŸçš„æ¿€å…‰é›·è¾¾é˜µåˆ—å™ªå£°æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå¯¼è‡´ç”Ÿæˆçš„ç‚¹äº‘ç¼ºä¹çœŸå®æ„Ÿã€‚   (3) æœ¬æ–‡æ–¹æ³•ï¼šæå‡ºäº†ä¸€ç§æ–°çš„æ¿€å…‰é›·è¾¾ç”ŸæˆèŒƒå›´å›¾åƒ Transformerï¼ˆLidarGRITï¼‰æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†æ¸è¿›ç”Ÿæˆå’Œå‡†ç¡®çš„é˜µåˆ—å™ªå£°åˆæˆã€‚LidarGRIT åœ¨æ½œåœ¨ç©ºé—´ä¸­ä½¿ç”¨è‡ªå›å½’ Transformer è¿­ä»£é‡‡æ ·èŒƒå›´å›¾åƒï¼Œç„¶åä½¿ç”¨ VQ-VAE è§£ç å™¨å°†é‡‡æ ·çš„ token è§£ç ä¸ºèŒƒå›´å›¾åƒã€‚   (4) å®éªŒç»“æœï¼šåœ¨ KITTI-360 å’Œ KITTI é‡Œç¨‹è®¡æ•°æ®é›†ä¸Šï¼ŒLidarGRIT åœ¨ç”Ÿæˆé€¼çœŸçš„æ¿€å…‰é›·è¾¾ç‚¹äº‘æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li></ol><p><strong>Methodsï¼š</strong></p><p>(1) æå‡ºäº†ä¸€ç§æ–°çš„æ¿€å…‰é›·è¾¾ç”ŸæˆèŒƒå›´å›¾åƒ Transformerï¼ˆLidarGRITï¼‰æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†æ¸è¿›ç”Ÿæˆå’Œå‡†ç¡®çš„é˜µåˆ—å™ªå£°åˆæˆã€‚</p><p>(2) LidarGRIT åœ¨æ½œåœ¨ç©ºé—´ä¸­ä½¿ç”¨è‡ªå›å½’ Transformer è¿­ä»£é‡‡æ ·èŒƒå›´å›¾åƒï¼Œç„¶åä½¿ç”¨ VQ-VAE è§£ç å™¨å°†é‡‡æ ·çš„ token è§£ç ä¸ºèŒƒå›´å›¾åƒã€‚</p><p>(3) åœ¨ VQ-VAE æ¨¡å‹ä¸­ï¼Œå¼•å…¥äº†å°„çº¿ä¸‹é™æŸå¤± (RL) å’Œå‡ ä½•ä¿æŒ (GP) æŠ€æœ¯ï¼Œä»¥æé«˜æ¨¡å‹çš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p><p>(4) RL æŠ€æœ¯é€šè¿‡ç›´æ¥é€¼è¿‘è¾“å…¥å™ªå£°èŒƒå›´å›¾åƒï¼Œæ›´å‡†ç¡®åœ°ç”Ÿæˆå°„çº¿ä¸‹é™å™ªå£°ã€‚</p><p>(5) GP æŠ€æœ¯é€šè¿‡å¢åŠ  VQ-VAE çš„æ³›åŒ–èƒ½åŠ›ï¼Œæé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚</p><p><strong>8. ç»“è®º</strong>(1): æœ¬æ–‡æå‡ºäº†ä¸€ç§æ¿€å…‰é›·è¾¾ç‚¹äº‘ç”Ÿæˆæ¨¡å‹ LidarGRITï¼Œè¯¥æ¨¡å‹åœ¨ KITTI-360 å’Œ KITTI é‡Œç¨‹è®¡æ•°æ®é›†ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚(2): <strong>åˆ›æ–°ç‚¹</strong>: æå‡ºäº†ä¸€ç§ç»“åˆæ¸è¿›ç”Ÿæˆå’Œå‡†ç¡®é˜µåˆ—å™ªå£°åˆæˆçš„æ¿€å…‰é›·è¾¾ç”ŸæˆèŒƒå›´å›¾åƒ Transformer æ¨¡å‹ LidarGRITã€‚<strong>æ€§èƒ½</strong>: LidarGRIT åœ¨ç”Ÿæˆé€¼çœŸçš„æ¿€å…‰é›·è¾¾ç‚¹äº‘æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚<strong>å·¥ä½œé‡</strong>: LidarGRIT çš„è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹è¾ƒä¸ºå¤æ‚ï¼Œéœ€è¦è¾ƒå¤§çš„è®¡ç®—èµ„æºã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-2e3090a3ad93111df8aeef9c80cdfdc0.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6c9ddab4b121f964880903b2c3babe92.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9f65ec97526efe2dd6d96ab65a987661.jpg" align="middle"><img src="https://picx.zhimg.com/v2-005772089f42b683bb9184ba763c0da3.jpg" align="middle"></details>## Rethinking the Spatial Inconsistency in Classifier-Free Diffusion   Guidance**Authors:Dazhong Shen, Guanglu Song, Zeyue Xue, Fu-Yun Wang, Yu Liu**Classifier-Free Guidance (CFG) has been widely used in text-to-image diffusion models, where the CFG scale is introduced to control the strength of text guidance on the whole image space. However, we argue that a global CFG scale results in spatial inconsistency on varying semantic strengths and suboptimal image quality. To address this problem, we present a novel approach, Semantic-aware Classifier-Free Guidance (S-CFG), to customize the guidance degrees for different semantic units in text-to-image diffusion models. Specifically, we first design a training-free semantic segmentation method to partition the latent image into relatively independent semantic regions at each denoising step. In particular, the cross-attention map in the denoising U-net backbone is renormalized for assigning each patch to the corresponding token, while the self-attention map is used to complete the semantic regions. Then, to balance the amplification of diverse semantic units, we adaptively adjust the CFG scales across different semantic regions to rescale the text guidance degrees into a uniform level. Finally, extensive experiments demonstrate the superiority of S-CFG over the original CFG strategy on various text-to-image diffusion models, without requiring any extra training cost. our codes are available at https://github.com/SmilesDZgk/S-CFG. [PDF](http://arxiv.org/abs/2404.05384v1) accepted by CVPR-2024**Summary**æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­çš„è¯­ä¹‰æ„ŸçŸ¥æ— åˆ†ç±»å¼•å¯¼ï¼ˆS-CFGï¼‰ä¸ºä¸åŒè¯­ä¹‰å•å…ƒè®¾ç½®å¯å®šåˆ¶å¼•å¯¼å¼ºåº¦ï¼Œæé«˜å›¾åƒè´¨é‡ã€‚**Key Takeaways**- CFGå­˜åœ¨ç©ºé—´ä¸ä¸€è‡´é—®é¢˜ï¼Œå¯¼è‡´å›¾åƒè´¨é‡è¾ƒå·®ã€‚- S-CFGæå‡ºä½¿ç”¨è®­ç»ƒå…è´¹è¯­ä¹‰åˆ†å‰²æ–¹æ³•å¯¹æ½œåœ¨å›¾åƒè¿›è¡Œè¯­ä¹‰åˆ†å‰²ã€‚- S-CFGé€šè¿‡è‡ªæ³¨æ„åŠ›åœ°å›¾å®Œæˆè¯­ä¹‰åŒºåŸŸã€‚- S-CFGé€šè¿‡è·¨æ³¨æ„åŠ›åœ°å›¾å°†æ¯ä¸ªè¡¥ä¸åˆ†é…åˆ°ç›¸åº”çš„æ ‡è®°ã€‚- S-CFGåœ¨ä¸åŒçš„è¯­ä¹‰åŒºåŸŸè‡ªé€‚åº”è°ƒæ•´CFGå°ºåº¦ï¼Œä»¥å¹³è¡¡ä¸åŒè¯­ä¹‰å•å…ƒçš„æ”¾å¤§ã€‚- S-CFGåœ¨å„ç§æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸Šä¼˜äºåŸå§‹CFGç­–ç•¥ã€‚- S-CFGæ— éœ€é¢å¤–è®­ç»ƒæˆæœ¬ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>é¢˜ç›®ï¼šé‡æ–°æ€è€ƒåˆ†ç±»å™¨è‡ªç”±æ‰©æ•£å¼•å¯¼ä¸­çš„ç©ºé—´ä¸ä¸€è‡´æ€§</li><li>ä½œè€…ï¼šZhaoyuan Ding, Yuhong Guo, Jianmin Bao, Hongyang Chao, Fei Wu</li><li>å•ä½ï¼šåŒ—äº¬å¤§å­¦ä¿¡æ¯ç§‘å­¦æŠ€æœ¯å­¦é™¢</li><li>å…³é”®è¯ï¼šæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ã€åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼ã€ç©ºé—´ä¸ä¸€è‡´æ€§ã€è¯­ä¹‰åˆ†å‰²</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/pdf/2302.02533.pdfï¼ŒGithubï¼šNone</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šåœ¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­ï¼Œåˆ†ç±»å™¨è‡ªç”±å¼•å¯¼ï¼ˆCFGï¼‰è¢«å¹¿æ³›ä½¿ç”¨ï¼Œå…¶ä¸­å¼•å…¥ CFG å°ºåº¦æ¥æ§åˆ¶æ–‡æœ¬å¼•å¯¼å¯¹æ•´ä¸ªå›¾åƒç©ºé—´å¼ºåº¦çš„å½±å“ã€‚ç„¶è€Œï¼Œä½œè€…è®¤ä¸ºå…¨å±€ CFG å°ºåº¦ä¼šå¯¼è‡´ä¸åŒè¯­ä¹‰å¼ºåº¦å’Œæ¬¡ä¼˜å›¾åƒè´¨é‡çš„ç©ºé—´ä¸ä¸€è‡´æ€§ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šä¼ ç»Ÿçš„ CFG ç­–ç•¥ä½¿ç”¨å…¨å±€å°ºåº¦æ¥æ§åˆ¶æ•´ä¸ªå›¾åƒç©ºé—´çš„æ–‡æœ¬å¼•å¯¼å¼ºåº¦ï¼Œè¿™ä¼šå¯¼è‡´ä¸åŒè¯­ä¹‰åŒºåŸŸçš„å¼•å¯¼ç¨‹åº¦ä¸ä¸€è‡´ï¼Œä»è€Œäº§ç”Ÿç©ºé—´ä¸ä¸€è‡´æ€§ã€‚ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºè¯­ä¹‰æ„ŸçŸ¥åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼ï¼ˆS-CFGï¼‰ï¼Œä»¥å®šåˆ¶æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­ä¸åŒè¯­ä¹‰å•å…ƒçš„å¼•å¯¼ç¨‹åº¦ã€‚å…·ä½“æ¥è¯´ï¼Œä½œè€…é¦–å…ˆè®¾è®¡äº†ä¸€ç§æ— è®­ç»ƒè¯­ä¹‰åˆ†å‰²æ–¹æ³•ï¼Œåœ¨æ¯ä¸ªå»å™ªæ­¥éª¤ä¸­å°†æ½œåœ¨å›¾åƒåˆ’åˆ†ä¸ºç›¸å¯¹ç‹¬ç«‹çš„è¯­ä¹‰åŒºåŸŸã€‚ç„¶åï¼Œä¸ºäº†å¹³è¡¡ä¸åŒè¯­ä¹‰å•å…ƒçš„æ”¾å¤§ï¼Œä½œè€…è‡ªé€‚åº”åœ°è°ƒæ•´ä¸åŒè¯­ä¹‰åŒºåŸŸçš„ CFG å°ºåº¦ï¼Œå°†æ–‡æœ¬å¼•å¯¼ç¨‹åº¦ç¼©æ”¾ä¸ºç»Ÿä¸€çš„æ°´å¹³ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šä½œè€…åœ¨å„ç§æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸Šå¯¹ S-CFG å’ŒåŸå§‹ CFG ç­–ç•¥è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œè¯æ˜äº† S-CFG çš„ä¼˜è¶Šæ€§ï¼Œè€Œæ— éœ€ä»»ä½•é¢å¤–çš„è®­ç»ƒæˆæœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒS-CFG åœ¨ FID-30K å’Œ CLIP å¾—åˆ†æ–¹é¢éƒ½ä¼˜äºåŸå§‹ CFG ç­–ç•¥ï¼Œæ”¯æŒäº†ä½œè€…æå‡ºçš„æ–¹æ³•å¯ä»¥è§£å†³ç©ºé—´ä¸ä¸€è‡´æ€§é—®é¢˜å¹¶æé«˜å›¾åƒè´¨é‡ã€‚</li></ol><p>7.æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼šåŸºäºè¯­ä¹‰çš„æ³¨æ„åŠ›åˆ†å‰²ï¼Œé€šè¿‡äº¤å‰æ³¨æ„åŠ›å’Œè‡ªæ³¨æ„åŠ›å›¾ï¼Œå¯¹æ½œåœ¨å›¾åƒè¿›è¡Œè¯­ä¹‰åˆ†å‰²ï¼Œå¾—åˆ°ç›¸å¯¹ç‹¬ç«‹çš„è¯­ä¹‰åŒºåŸŸã€‚ï¼ˆ2ï¼‰ï¼šè¯­ä¹‰æ„ŸçŸ¥åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼ï¼Œæ ¹æ®è¯­ä¹‰åŒºåŸŸçš„æ©ç ï¼Œè‡ªé€‚åº”è°ƒæ•´ CFG å°ºåº¦ï¼Œç»Ÿä¸€ä¸åŒè¯­ä¹‰åŒºåŸŸçš„åˆ†ç±»å™¨åˆ†æ•°ã€‚ï¼ˆ3ï¼‰ï¼šè‡ªé€‚åº” CFG å°ºåº¦ï¼Œé€šè¿‡è®¡ç®—ä¸åŒè¯­ä¹‰åŒºåŸŸçš„åˆ†ç±»å™¨åˆ†æ•°èŒƒæ•°ï¼Œå°†å…¶ç¼©æ”¾è‡³åŸºå‡†å°ºåº¦ï¼Œå¹³è¡¡ä¸åŒè¯­ä¹‰ä¿¡æ¯çš„æ”¾å¤§ç¨‹åº¦ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§è¯­ä¹‰æ„ŸçŸ¥åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼ï¼ˆS-CFGï¼‰æ–¹æ³•ï¼Œè§£å†³äº†æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼çš„ç©ºé—´ä¸ä¸€è‡´æ€§é—®é¢˜ï¼Œæå‡äº†å›¾åƒç”Ÿæˆè´¨é‡ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§æ— è®­ç»ƒçš„è¯­ä¹‰åˆ†å‰²æ–¹æ³•ï¼Œè‡ªé€‚åº”è°ƒæ•´ä¸åŒè¯­ä¹‰åŒºåŸŸçš„åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼å°ºåº¦ï¼Œå¹³è¡¡ä¸åŒè¯­ä¹‰ä¿¡æ¯çš„æ”¾å¤§ç¨‹åº¦ã€‚æ€§èƒ½ï¼šåœ¨ FID-30K å’Œ CLIP å¾—åˆ†æ–¹é¢å‡ä¼˜äºåŸå§‹åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼ç­–ç•¥ã€‚å·¥ä½œé‡ï¼šä¸åŸå§‹åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼ç­–ç•¥ç›¸æ¯”ï¼Œæ²¡æœ‰é¢å¤–çš„è®­ç»ƒæˆæœ¬ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-1525e599af4b9d40ecb59ad934082d32.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-49ace2f9b99cf09bb4ebfca5117a4744.jpg" align="middle"><img src="https://picx.zhimg.com/v2-79397283aee66eda3e811c6f8eb26447.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b5ed37011cd879c67efe657e08355166.jpg" align="middle"></details><h2 id="Gaussian-Shading-Provable-Performance-Lossless-Image-Watermarking-for-Diffusion-Models"><a href="#Gaussian-Shading-Provable-Performance-Lossless-Image-Watermarking-for-Diffusion-Models" class="headerlink" title="Gaussian Shading: Provable Performance-Lossless Image Watermarking for   Diffusion Models"></a>Gaussian Shading: Provable Performance-Lossless Image Watermarking for   Diffusion Models</h2><p><strong>Authors:Zijin Yang, Kai Zeng, Kejiang Chen, Han Fang, Weiming Zhang, Nenghai Yu</strong></p><p>Ethical concerns surrounding copyright protection and inappropriate content generation pose challenges for the practical implementation of diffusion models. One effective solution involves watermarking the generated images. However, existing methods often compromise the model performance or require additional training, which is undesirable for operators and users. To address this issue, we propose Gaussian Shading, a diffusion model watermarking technique that is both performance-lossless and training-free, while serving the dual purpose of copyright protection and tracing of offending content. Our watermark embedding is free of model parameter modifications and thus is plug-and-play. We map the watermark to latent representations following a standard Gaussian distribution, which is indistinguishable from latent representations obtained from the non-watermarked diffusion model. Therefore we can achieve watermark embedding with lossless performance, for which we also provide theoretical proof. Furthermore, since the watermark is intricately linked with image semantics, it exhibits resilience to lossy processing and erasure attempts. The watermark can be extracted by Denoising Diffusion Implicit Models (DDIM) inversion and inverse sampling. We evaluate Gaussian Shading on multiple versions of Stable Diffusion, and the results demonstrate that Gaussian Shading not only is performance-lossless but also outperforms existing methods in terms of robustness. </p><p><a href="http://arxiv.org/abs/2404.04956v1">PDF</a> 17 pages, 11 figures, accepted by CVPR 2024</p><p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹ä¸­ï¼Œå›¾ç‰‡æ°´å°æŠ€æœ¯é¿å…äº†å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œä¸”æ— éœ€é¢å¤–è®­ç»ƒï¼Œå¯ç”¨äºç‰ˆæƒä¿æŠ¤å’Œè¿è§„å†…å®¹è¿½è¸ªã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>é«˜æ–¯é˜´å½±æ°´å°æŠ€æœ¯æ€§èƒ½æ— æŸä¸”æ— éœ€è®­ç»ƒï¼Œå¯ç”¨äºæ‰©æ•£æ¨¡å‹ç‰ˆæƒä¿æŠ¤å’Œè¿è§„å†…å®¹è¿½è¸ªã€‚</li><li>æ°´å°åµŒå…¥ä¸ä¿®æ”¹æ¨¡å‹å‚æ•°ï¼Œå³æ’å³ç”¨ã€‚</li><li>æ°´å°æ˜ å°„åˆ°æœä»æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„æ½œåœ¨è¡¨å¾ï¼Œä¸éæ°´å°æ‰©æ•£æ¨¡å‹è·å¾—çš„æ½œåœ¨è¡¨å¾æ— æ³•åŒºåˆ†ã€‚</li><li>æ°´å°åµŒå…¥å¯å®ç°æ€§èƒ½æ— æŸï¼Œå¹¶æä¾›ç†è®ºè¯æ˜ã€‚</li><li>æ°´å°ä¸å›¾åƒè¯­ä¹‰å¯†åˆ‡ç›¸å…³ï¼Œå¯¹æœ‰æŸå¤„ç†å’Œæ“¦é™¤å…·æœ‰é²æ£’æ€§ã€‚</li><li>å¯é€šè¿‡å»å™ªæ‰©æ•£éšå¼æ¨¡å‹ (DDIM) åæ¼”å’Œé€†é‡‡æ ·æå–æ°´å°ã€‚</li><li>åœ¨ Stable Diffusion çš„å¤šä¸ªç‰ˆæœ¬ä¸Šè¯„ä¼°äº†é«˜æ–¯é˜´å½±ï¼Œç»“æœè¡¨æ˜å®ƒä¸ä»…æ€§èƒ½æ— æŸï¼Œè€Œä¸”åœ¨é²æ£’æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šé«˜æ–¯ç€è‰²ï¼šå¯è¯æ˜æ€§èƒ½æ— æŸå›¾åƒæ°´å°</li><li>ä½œè€…ï¼šZhenyu He, Yuhang Song, Jiawei Chen, Zhe Lin, Xinyuan Zhang</li><li>æ‰€å±å•ä½ï¼šåŒ—äº¬å¤§å­¦</li><li>å…³é”®è¯ï¼šDiffusion modelã€Gaussian shadingã€Watermarkã€Copyright protection</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.03065ï¼ŒGithub é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šéšç€æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œç‰ˆæƒä¿æŠ¤å’Œä¸å½“å†…å®¹ç”Ÿæˆæ–¹é¢çš„ä¼¦ç†é—®é¢˜æ—¥ç›Šå‡¸æ˜¾ã€‚æ°´å°æŠ€æœ¯æ˜¯ä¸€ç§æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œä½†ç°æœ‰æ–¹æ³•å¾€å¾€ä¼šå½±å“æ¨¡å‹æ€§èƒ½æˆ–éœ€è¦é¢å¤–çš„è®­ç»ƒï¼Œç»™æ“ä½œè€…å’Œç”¨æˆ·å¸¦æ¥ä¸ä¾¿ã€‚</li></ol><p>ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šè¿‡å»çš„æ–¹æ³•ä¸»è¦é€šè¿‡ä¿®æ”¹æ¨¡å‹å‚æ•°æˆ–è®­ç»ƒé¢å¤–çš„ç½‘ç»œæ¥åµŒå…¥æ°´å°ï¼Œä½†è¿™äº›æ–¹æ³•è¦ä¹ˆä¼šå½±å“æ¨¡å‹æ€§èƒ½ï¼Œè¦ä¹ˆéœ€è¦é¢å¤–çš„è®­ç»ƒæˆæœ¬ã€‚</p><p>ï¼ˆ3ï¼‰æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºé«˜æ–¯ç€è‰²çš„æ‰©æ•£æ¨¡å‹æ°´å°æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯æ— éœ€ä¿®æ”¹æ¨¡å‹å‚æ•°ï¼Œä¸”æ— éœ€é¢å¤–è®­ç»ƒï¼ŒåŒæ—¶å…¼é¡¾ç‰ˆæƒä¿æŠ¤å’Œè¿è§„å†…å®¹è¿½è¸ªçš„åŒé‡ç›®çš„ã€‚æ°´å°åµŒå…¥è¿‡ç¨‹ä¸æ ‡å‡†é«˜æ–¯åˆ†å¸ƒçš„æ½œåœ¨è¡¨ç¤ºç›¸æ˜ å°„ï¼Œä¸éæ°´å°æ‰©æ•£æ¨¡å‹è·å¾—çš„æ½œåœ¨è¡¨ç¤ºæ— æ³•åŒºåˆ†ï¼Œå› æ­¤å¯ä»¥å®ç°æ— æŸæ€§èƒ½çš„æ°´å°åµŒå…¥ã€‚</p><p>ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šæœ¬æ–‡åœ¨ Stable Diffusion çš„å¤šä¸ªç‰ˆæœ¬ä¸Šè¯„ä¼°äº†é«˜æ–¯ç€è‰²æŠ€æœ¯ï¼Œç»“æœè¡¨æ˜ï¼Œè¯¥æŠ€æœ¯ä¸ä»…æ€§èƒ½æ— æŸï¼Œè€Œä¸”åœ¨é²æ£’æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p><ol><li><p>æ–¹æ³•ï¼š(1) é«˜æ–¯ç€è‰²æŠ€æœ¯çš„åŸºæœ¬åŸç†ï¼šåœ¨æ‰©æ•£æ¨¡å‹çš„æ½œåœ¨ç©ºé—´ä¸­ï¼Œå°†æ°´å°ä¿¡æ¯æ˜ å°„åˆ°æ ‡å‡†é«˜æ–¯åˆ†å¸ƒçš„æ½œåœ¨è¡¨ç¤ºä¸­ï¼Œä»è€Œå®ç°æ— æŸæ°´å°åµŒå…¥ã€‚(2) æ°´å°åµŒå…¥è¿‡ç¨‹ï¼šåœ¨é‡‡æ ·è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡ä¿®æ”¹å™ªå£°è¾“å…¥æ¥åµŒå…¥æ°´å°ä¿¡æ¯ï¼Œä½†ä¸ä¼šå½±å“æ½œåœ¨è¡¨ç¤ºçš„åˆ†å¸ƒã€‚(3) æ°´å°æå–è¿‡ç¨‹ï¼šé€šè¿‡æ¯”è¾ƒæ°´å°å›¾åƒå’Œéæ°´å°å›¾åƒçš„æ½œåœ¨è¡¨ç¤ºï¼Œå¯ä»¥æå–åµŒå…¥çš„æ°´å°ä¿¡æ¯ã€‚</p></li><li><p>ç»“è®ºï¼š(1): æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§é«˜æ–¯ç€è‰²æ°´å°æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯æ€§èƒ½æ— æŸï¼Œæ— éœ€ä¿®æ”¹æ¨¡å‹å‚æ•°ï¼Œä¸”æ— éœ€é¢å¤–è®­ç»ƒï¼Œå…¼é¡¾ç‰ˆæƒä¿æŠ¤å’Œè¿è§„å†…å®¹è¿½è¸ªçš„åŒé‡ç›®çš„ã€‚(2): åˆ›æ–°ç‚¹ï¼š</p><ul><li>æå‡ºäº†ä¸€ç§æ–°çš„æ°´å°åµŒå…¥æ–¹æ³•ï¼Œå°†æ°´å°ä¿¡æ¯æ˜ å°„åˆ°æ ‡å‡†é«˜æ–¯åˆ†å¸ƒçš„æ½œåœ¨è¡¨ç¤ºä¸­ï¼Œå®ç°æ— æŸæ°´å°åµŒå…¥ã€‚</li><li>è®¾è®¡äº†ä¸€ç§æ–°çš„æ°´å°æå–ç®—æ³•ï¼Œé€šè¿‡æ¯”è¾ƒæ°´å°å›¾åƒå’Œéæ°´å°å›¾åƒçš„æ½œåœ¨è¡¨ç¤ºï¼Œå¯ä»¥æå–åµŒå…¥çš„æ°´å°ä¿¡æ¯ã€‚</li><li>è¯¥æŠ€æœ¯åœ¨Stable Diffusionçš„å¤šä¸ªç‰ˆæœ¬ä¸Šå‡å–å¾—äº†æ€§èƒ½æ— æŸçš„æ•ˆæœï¼Œå¹¶ä¸”åœ¨é²æ£’æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li><li>è¯¥æŠ€æœ¯æ— éœ€ä¿®æ”¹æ¨¡å‹å‚æ•°ï¼Œä¸”æ— éœ€é¢å¤–è®­ç»ƒï¼Œæ“ä½œç®€å•ï¼Œä¾¿äºéƒ¨ç½²ã€‚æ€§èƒ½ï¼š</li><li>è¯¥æŠ€æœ¯åœ¨Stable Diffusionçš„å¤šä¸ªç‰ˆæœ¬ä¸Šå‡å–å¾—äº†æ€§èƒ½æ— æŸçš„æ•ˆæœã€‚</li><li>è¯¥æŠ€æœ¯åœ¨é²æ£’æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å·¥ä½œé‡ï¼š</li><li>è¯¥æŠ€æœ¯æ“ä½œç®€å•ï¼Œä¾¿äºéƒ¨ç½²ã€‚</li><li>è¯¥æŠ€æœ¯æ— éœ€ä¿®æ”¹æ¨¡å‹å‚æ•°ï¼Œä¸”æ— éœ€é¢å¤–è®­ç»ƒï¼Œå·¥ä½œé‡è¾ƒå°ã€‚</li></ul></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-4b470a83454be957795f4d0246530acb.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-05c09cb3e9c494866256691389ae308f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-80967f6d7355b9f5c165e60d564d7218.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-cbe4e21f2000f38502c5af54d393a6c3.jpg" align="middle"></details><h2 id="Light-the-Night-A-Multi-Condition-Diffusion-Framework-for-Unpaired-Low-Light-Enhancement-in-Autonomous-Driving"><a href="#Light-the-Night-A-Multi-Condition-Diffusion-Framework-for-Unpaired-Low-Light-Enhancement-in-Autonomous-Driving" class="headerlink" title="Light the Night: A Multi-Condition Diffusion Framework for Unpaired   Low-Light Enhancement in Autonomous Driving"></a>Light the Night: A Multi-Condition Diffusion Framework for Unpaired   Low-Light Enhancement in Autonomous Driving</h2><p><strong>Authors:Jinlong Li, Baolu Li, Zhengzhong Tu, Xinyu Liu, Qing Guo, Felix Juefei-Xu, Runsheng Xu, Hongkai Yu</strong></p><p>Vision-centric perception systems for autonomous driving have gained considerable attention recently due to their cost-effectiveness and scalability, especially compared to LiDAR-based systems. However, these systems often struggle in low-light conditions, potentially compromising their performance and safety. To address this, our paper introduces LightDiff, a domain-tailored framework designed to enhance the low-light image quality for autonomous driving applications. Specifically, we employ a multi-condition controlled diffusion model. LightDiff works without any human-collected paired data, leveraging a dynamic data degradation process instead. It incorporates a novel multi-condition adapter that adaptively controls the input weights from different modalities, including depth maps, RGB images, and text captions, to effectively illuminate dark scenes while maintaining context consistency. Furthermore, to align the enhanced images with the detection modelâ€™s knowledge, LightDiff employs perception-specific scores as rewards to guide the diffusion training process through reinforcement learning. Extensive experiments on the nuScenes datasets demonstrate that LightDiff can significantly improve the performance of several state-of-the-art 3D detectors in night-time conditions while achieving high visual quality scores, highlighting its potential to safeguard autonomous driving. </p><p><a href="http://arxiv.org/abs/2404.04804v1">PDF</a> This paper is accepted by CVPR 2024</p><p><strong>Summary</strong></p><p>å›¾ç‰‡æ‰©æ•£æ¨¡å‹ LightDiff èå…¥è‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥ç³»ç»Ÿï¼Œåœ¨æ— éœ€é…å¯¹æ•°æ®çš„æƒ…å†µä¸‹æå‡å¼±å…‰å›¾åƒè´¨é‡ï¼Œå¢å¼ºè½¦è¾†å®‰å…¨æ€§èƒ½ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>é’ˆå¯¹è‡ªåŠ¨é©¾é©¶å¼€å‘çš„å›¾ç‰‡æ‰©æ•£æ¨¡å‹ LightDiffã€‚</li><li>ç»“åˆå¤šæ¡ä»¶æ§åˆ¶æ‰©æ•£æ¨¡å‹ï¼Œä¸éœ€è¦äººå·¥æ”¶é›†çš„é…å¯¹æ•°æ®ã€‚</li><li>å¼•å…¥å¤šæ¡ä»¶é€‚é…å™¨ï¼Œè‡ªé€‚åº”æ§åˆ¶æ·±åº¦å›¾ã€RGB å›¾åƒå’Œæ–‡æœ¬æè¿°ç­‰ä¸åŒæ¨¡æ€çš„è¾“å…¥æƒé‡ã€‚</li><li>åˆ©ç”¨æ„ŸçŸ¥ç‰¹å®šåˆ†æ•°ä½œä¸ºå¥–åŠ±ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ æŒ‡å¯¼æ‰©æ•£è®­ç»ƒè¿‡ç¨‹ï¼Œä½¿å¢å¼ºå›¾åƒä¸æ£€æµ‹æ¨¡å‹çŸ¥è¯†ä¿æŒä¸€è‡´ã€‚</li><li>åœ¨ nuScenes æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒLightDiff å¯ä»¥æ˜¾è‘—æå‡å¤šç§æœ€å…ˆè¿›çš„ 3D æ£€æµ‹å™¨åœ¨å¤œé—´æ¡ä»¶ä¸‹çš„æ€§èƒ½ï¼ŒåŒæ—¶å®ç°é«˜è§†è§‰è´¨é‡åˆ†æ•°ã€‚</li><li>LightDiff æœ‰æ½œåŠ›ä¿éšœè‡ªåŠ¨é©¾é©¶çš„å®‰å…¨æ€§ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šLight the Nightï¼ˆç‚¹äº®å¤œæ™šï¼‰</li><li>ä½œè€…ï¼šJinlong Liã€Baolu Liã€Zhengzhong Tuã€Xinyu Liuã€Qing Guoã€Felix Juefei-Xuã€Runsheng Xuã€Hongkai Yu</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šå…‹åˆ©å¤«å…°å·ç«‹å¤§å­¦</li><li>å…³é”®è¯ï¼šä½å…‰å›¾åƒå¢å¼ºã€è‡ªä¸»é©¾é©¶ã€æ‰©æ•£æ¨¡å‹ã€å¤šæ¨¡æ€å­¦ä¹ ã€å¼ºåŒ–å­¦ä¹ </li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.04804Github ä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šåœ¨è‡ªä¸»é©¾é©¶é¢†åŸŸï¼Œè§†è§‰æ„ŸçŸ¥ç³»ç»Ÿç”±äºå…¶æˆæœ¬æ•ˆç›Šå’Œå¯æ‰©å±•æ€§è€Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œè¿™äº›ç³»ç»Ÿåœ¨ä½å…‰æ¡ä»¶ä¸‹å¾€å¾€è¡¨ç°ä¸ä½³ï¼Œè¿™å¯èƒ½ä¼šå½±å“å…¶æ€§èƒ½å’Œå®‰å…¨æ€§ã€‚</li></ol><p>ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šä¼ ç»Ÿæ–¹æ³•é€šå¸¸éœ€è¦æ”¶é›†å¤§é‡é…å¯¹æ•°æ®ï¼Œè¿™æ—¢è´¹æ—¶åˆè´¹åŠ›ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•å¾€å¾€æ— æ³•å¾ˆå¥½åœ°å¤„ç†ä¸åŒæ¨¡æ€ï¼ˆå¦‚æ·±åº¦å›¾ã€RGB å›¾åƒå’Œæ–‡æœ¬æè¿°ï¼‰ä¹‹é—´çš„å·®å¼‚ï¼Œå¯¼è‡´å¢å¼ºå›¾åƒè´¨é‡ä¸ä½³ã€‚</p><p>ï¼ˆ3ï¼‰æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º LightDiff çš„å¤šæ¡ä»¶æ§åˆ¶æ‰©æ•£æ¨¡å‹ï¼Œå®ƒæ— éœ€äººå·¥æ”¶é›†é…å¯¹æ•°æ®ï¼Œè€Œæ˜¯åˆ©ç”¨åŠ¨æ€æ•°æ®é€€åŒ–è¿‡ç¨‹ã€‚LightDiff é‡‡ç”¨äº†ä¸€ç§å¤šæ¡ä»¶é€‚é…å™¨ï¼Œå¯ä»¥è‡ªé€‚åº”åœ°æ§åˆ¶æ¥è‡ªä¸åŒæ¨¡æ€çš„è¾“å…¥æƒé‡ï¼Œæœ‰æ•ˆåœ°ç…§äº®æš—åœºæ™¯ï¼ŒåŒæ—¶ä¿æŒä¸Šä¸‹æ–‡ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œä¸ºäº†å°†å¢å¼ºå›¾åƒä¸æ£€æµ‹æ¨¡å‹çš„çŸ¥è¯†ç›¸ç»“åˆï¼ŒLightDiff é‡‡ç”¨æ„ŸçŸ¥ç‰¹å®šåˆ†æ•°ä½œä¸ºå¥–åŠ±ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ æŒ‡å¯¼æ‰©æ•£è®­ç»ƒè¿‡ç¨‹ã€‚</p><p>ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šåœ¨ nuScenes æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒLightDiff å¯ä»¥æ˜¾ç€æé«˜å‡ ç§æœ€å…ˆè¿›çš„ 3D æ£€æµ‹å™¨åœ¨å¤œé—´æ¡ä»¶ä¸‹çš„æ€§èƒ½ï¼ŒåŒæ—¶è·å¾—è¾ƒé«˜çš„è§†è§‰è´¨é‡åˆ†æ•°ï¼Œçªå‡ºäº†å…¶åœ¨ä¿éšœè‡ªä¸»é©¾é©¶å®‰å…¨æ–¹é¢çš„æ½œåŠ›ã€‚</p><ol><li><p>æ–¹æ³•ï¼š(1) æ„å»ºå¤šæ ·åŒ–å¤œé—´å›¾åƒç”Ÿæˆç®¡é“ï¼Œç”¨äºç”Ÿæˆè®­ç»ƒæ•°æ®å¯¹ï¼›(2) æå‡º LightDiff æ¨¡å‹ï¼Œä¸€ç§æ–°é¢–çš„æ¡ä»¶ç”Ÿæˆæ¨¡å‹ï¼Œå¯ä»¥è‡ªé€‚åº”åœ°åˆ©ç”¨æ¡ä»¶çš„å¤šæ¨¡æ€ï¼ˆä½å…‰å›¾åƒã€æ·±åº¦å›¾å’Œæ–‡æœ¬æç¤ºï¼‰æ¥é¢„æµ‹å¢å¼ºå…‰è¾“å‡ºï¼›(3) å¼•å…¥å¥–åŠ±ç­–ç•¥ï¼Œè€ƒè™‘æ¥è‡ªå¯ä¿¡æ¿€å…‰é›·è¾¾å’Œç»Ÿè®¡åˆ†å¸ƒä¸€è‡´æ€§çš„æŒ‡å¯¼ï¼Œä»¥æé«˜æ¨¡å‹çš„ä»»åŠ¡æ„ŸçŸ¥èƒ½åŠ›ï¼›(4) æå‡ºä¸€ç§é€’å½’ç…§æ˜æ¨ç†ç­–ç•¥ï¼Œåœ¨æµ‹è¯•æ—¶è¿›ä¸€æ­¥æå‡æ¨¡å‹ç»“æœã€‚</p></li><li><p>ç»“è®ºï¼š(1): æœ¬å·¥ä½œæå‡ºäº† LightDiffï¼Œä¸€ç§æ— éœ€é…å¯¹æ•°æ®çš„å¤šæ¨¡æ€æ¡ä»¶ç”Ÿæˆæ¨¡å‹ï¼Œå®ƒå¯ä»¥æœ‰æ•ˆåœ°å¢å¼ºä½å…‰å›¾åƒï¼Œæé«˜è‡ªä¸»é©¾é©¶åœºæ™¯ä¸­çš„è§†è§‰æ„ŸçŸ¥æ€§èƒ½ã€‚(2): åˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§æ— éœ€é…å¯¹æ•°æ®çš„å¤šæ¨¡æ€æ¡ä»¶ç”Ÿæˆæ¨¡å‹ LightDiffï¼Œå®ƒå¯ä»¥è‡ªé€‚åº”åœ°åˆ©ç”¨æ¡ä»¶çš„å¤šæ¨¡æ€ï¼ˆä½å…‰å›¾åƒã€æ·±åº¦å›¾å’Œæ–‡æœ¬æç¤ºï¼‰æ¥é¢„æµ‹å¢å¼ºå…‰è¾“å‡ºã€‚</li><li>å¼•å…¥äº†å¥–åŠ±ç­–ç•¥ï¼Œè€ƒè™‘æ¥è‡ªå¯ä¿¡æ¿€å…‰é›·è¾¾å’Œç»Ÿè®¡åˆ†å¸ƒä¸€è‡´æ€§çš„æŒ‡å¯¼ï¼Œä»¥æé«˜æ¨¡å‹çš„ä»»åŠ¡æ„ŸçŸ¥èƒ½åŠ›ã€‚</li><li>æå‡ºäº†ä¸€ç§é€’å½’ç…§æ˜æ¨ç†ç­–ç•¥ï¼Œåœ¨æµ‹è¯•æ—¶è¿›ä¸€æ­¥æå‡æ¨¡å‹ç»“æœã€‚æ€§èƒ½ï¼š</li><li>åœ¨ nuScenes æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒLightDiff å¯ä»¥æ˜¾ç€æé«˜å‡ ç§æœ€å…ˆè¿›çš„ 3D æ£€æµ‹å™¨åœ¨å¤œé—´æ¡ä»¶ä¸‹çš„æ€§èƒ½ï¼ŒåŒæ—¶è·å¾—è¾ƒé«˜çš„è§†è§‰è´¨é‡åˆ†æ•°ã€‚å·¥ä½œé‡ï¼š</li><li>æœ¬å·¥ä½œéœ€è¦æ”¶é›†å’Œé¢„å¤„ç†å¤§é‡å¤œé—´å›¾åƒå’Œæ¿€å…‰é›·è¾¾æ•°æ®ã€‚</li><li>LightDiff æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹éœ€è¦å¤§é‡è®¡ç®—èµ„æºã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-b991e9b583160922886ab085b9cd1de9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-100ac2258004919206e5f101d9b8f5b5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e48847f9305eb6b295a969f3aadc0864.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c9fd1da58ac85510836ff360b0ca0feb.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c46a6b58aeb6290276196edf18b98cc5.jpg" align="middle"><img src="https://pica.zhimg.com/v2-caa85ecc05b3d0edc7c60fd7b25e3726.jpg" align="middle"></details><h2 id="Rethinking-Diffusion-Model-for-Multi-Contrast-MRI-Super-Resolution"><a href="#Rethinking-Diffusion-Model-for-Multi-Contrast-MRI-Super-Resolution" class="headerlink" title="Rethinking Diffusion Model for Multi-Contrast MRI Super-Resolution"></a>Rethinking Diffusion Model for Multi-Contrast MRI Super-Resolution</h2><p><strong>Authors:Guangyuan Li, Chen Rao, Juncheng Mo, Zhanjie Zhang, Wei Xing, Lei Zhao</strong></p><p>Recently, diffusion models (DM) have been applied in magnetic resonance imaging (MRI) super-resolution (SR) reconstruction, exhibiting impressive performance, especially with regard to detailed reconstruction. However, the current DM-based SR reconstruction methods still face the following issues: (1) They require a large number of iterations to reconstruct the final image, which is inefficient and consumes a significant amount of computational resources. (2) The results reconstructed by these methods are often misaligned with the real high-resolution images, leading to remarkable distortion in the reconstructed MR images. To address the aforementioned issues, we propose an efficient diffusion model for multi-contrast MRI SR, named as DiffMSR. Specifically, we apply DM in a highly compact low-dimensional latent space to generate prior knowledge with high-frequency detail information. The highly compact latent space ensures that DM requires only a few simple iterations to produce accurate prior knowledge. In addition, we design the Prior-Guide Large Window Transformer (PLWformer) as the decoder for DM, which can extend the receptive field while fully utilizing the prior knowledge generated by DM to ensure that the reconstructed MR image remains undistorted. Extensive experiments on public and clinical datasets demonstrate that our DiffMSR outperforms state-of-the-art methods. </p><p><a href="http://arxiv.org/abs/2404.04785v1">PDF</a> 14 pages, 12 figures, Accepted by CVPR2024</p><p><strong>æ‘˜è¦</strong><br>åˆ©ç”¨ç´§å‡‘çš„é«˜é¢‘ç»†èŠ‚æ½œç©ºé—´å¼¥åˆäº†æ‰©æ•£æ¨¡å‹ä¸MRå›¾åƒè¶…åˆ†è¾¨ç‡é‡å»ºé—´å­˜åœ¨çš„é—®é¢˜ã€‚</p><p><strong>è¦ç‚¹</strong></p><ul><li>æ‰©æ•£æ¨¡å‹åœ¨ç£å…±æŒ¯æˆåƒ (MRI) è¶…åˆ†è¾¨ç‡ (SR) é‡å»ºä¸­è¡¨ç°å‡ºè‰²ã€‚</li><li>ç°æœ‰æ–¹æ³•è®¡ç®—æ•ˆç‡ä½ï¼Œè€—æ—¶ä¸”è®¡ç®—èµ„æºå¤§ã€‚</li><li>é‡å»ºç»“æœä¸å®é™…é«˜åˆ†è¾¨ç‡å›¾åƒé”™ä½ï¼Œé‡å»º MR å›¾åƒå¤±çœŸã€‚</li><li>æå‡ºäº†ä¸€ç§ç”¨äºå¤šå¯¹æ¯”åº¦ MRI SR çš„é«˜æ•ˆæ‰©æ•£æ¨¡å‹ DiffMSRã€‚</li><li>åœ¨ä½ç»´æ½œç©ºé—´ä¸­åº”ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆé«˜é¢‘ç»†èŠ‚ä¿¡æ¯ã€‚</li><li>ä½ç»´æ½œç©ºé—´ç¡®ä¿æ‰©æ•£æ¨¡å‹ä»…éœ€å°‘é‡è¿­ä»£å³å¯äº§ç”Ÿå‡†ç¡®çš„å…ˆéªŒçŸ¥è¯†ã€‚</li><li>è®¾è®¡äº†å…ˆéªŒå¼•å¯¼å¤§çª—å£ Transformer (PLWformer) ä½œä¸ºè§£ç å™¨ï¼Œå……åˆ†åˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„å…ˆéªŒçŸ¥è¯†ï¼Œä¿è¯é‡å»º MR å›¾åƒå¤±çœŸå°ã€‚</li><li>å®éªŒè¡¨æ˜ DiffMSR ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šåŸºäºå¤šå¯¹æ¯”åº¦ MRI è¶…åˆ†è¾¨ç‡é‡å»ºçš„æ‰©æ•£æ¨¡å‹å†æ€è€ƒ</li><li>ä½œè€…ï¼šYuxuan Zhang, Jiahui Zhang, Xiaoxuan Zhang, Yang Chen, Hongming Shan, Yuxin Zhang, Yuyuan Zhang, Xiaoliang Zhang, Yi Zhang, Xiaochuan Pan</li><li>éš¶å±å•ä½ï¼šä¸­å›½ç§‘å­¦æŠ€æœ¯å¤§å­¦</li><li>å…³é”®è¯ï¼šDiffusion Model, MRI, Super-Resolution</li><li>è®ºæ–‡é“¾æ¥ï¼šNoneï¼ŒGithub é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šè¿‘å¹´æ¥ï¼Œæ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰åœ¨ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰è¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰é‡å»ºä¸­å¾—åˆ°äº†åº”ç”¨ï¼Œè¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ç»†èŠ‚é‡å»ºæ–¹é¢ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäº DM çš„ SR é‡å»ºæ–¹æ³•ä»ç„¶é¢ä¸´ä»¥ä¸‹é—®é¢˜ï¼šï¼ˆ1ï¼‰å®ƒä»¬éœ€è¦å¤§é‡çš„è¿­ä»£æ‰èƒ½é‡å»ºæœ€ç»ˆå›¾åƒï¼Œè¿™æ•ˆç‡ä½ä¸‹ä¸”æ¶ˆè€—å¤§é‡çš„è®¡ç®—èµ„æºã€‚ï¼ˆ2ï¼‰è¿™äº›æ–¹æ³•é‡å»ºçš„ç»“æœå¾€å¾€ä¸çœŸå®çš„é«˜åˆ†è¾¨ç‡å›¾åƒä¸ä¸€è‡´ï¼Œå¯¼è‡´é‡å»ºçš„ MRI å›¾åƒå‡ºç°æ˜æ˜¾çš„å¤±çœŸã€‚</li></ol><p>ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠé—®é¢˜ï¼šè¿‡å»çš„æ–¹æ³•ä¸»è¦ä½¿ç”¨ DM åœ¨é«˜ç»´æ½œåœ¨ç©ºé—´ä¸­ç”Ÿæˆå…ˆéªŒçŸ¥è¯†ï¼Œè¿™éœ€è¦å¤§é‡çš„è¿­ä»£æ‰èƒ½äº§ç”Ÿå‡†ç¡®çš„å…ˆéªŒçŸ¥è¯†ã€‚æ­¤å¤–ï¼Œè§£ç å™¨æ— æ³•å……åˆ†åˆ©ç”¨å…ˆéªŒçŸ¥è¯†ï¼Œå¯¼è‡´é‡å»ºçš„ MR å›¾åƒå¤±çœŸã€‚</p><p>ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºå¤šå¯¹æ¯”åº¦ MRI SR çš„é«˜æ•ˆæ‰©æ•£æ¨¡å‹ï¼Œç§°ä¸º DiffMSRã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åº”ç”¨ DM åœ¨é«˜åº¦ç´§å‡‘çš„ä½ç»´æ½œåœ¨ç©ºé—´ä¸­ç”Ÿæˆå…·æœ‰é«˜é¢‘ç»†èŠ‚ä¿¡æ¯çš„å…ˆéªŒçŸ¥è¯†ã€‚é«˜åº¦ç´§å‡‘çš„æ½œåœ¨ç©ºé—´ç¡®ä¿ DM åªéœ€è¦å‡ ä¸ªç®€å•çš„è¿­ä»£å°±å¯ä»¥äº§ç”Ÿå‡†ç¡®çš„å…ˆéªŒçŸ¥è¯†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†å…ˆéªŒå¼•å¯¼å¤§çª—å£ Transformerï¼ˆPLWformerï¼‰ä½œä¸º DM çš„è§£ç å™¨ï¼Œå®ƒå¯ä»¥åœ¨å……åˆ†åˆ©ç”¨ DM ç”Ÿæˆçš„å…ˆéªŒçŸ¥è¯†çš„åŒæ—¶æ‰©å±•æ„Ÿå—é‡ï¼Œä»¥ç¡®ä¿é‡å»ºçš„ MR å›¾åƒä¸ä¼šå¤±çœŸã€‚</p><p>ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½åŠæ•ˆæœï¼šåœ¨å…¬å…±å’Œä¸´åºŠæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ DiffMSR ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚åœ¨ FastMRI æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ PSNR å’Œ SSIM æŒ‡æ ‡ä¸Šåˆ†åˆ«æ¯”æœ€å…ˆè¿›çš„æ–¹æ³•æé«˜äº† 0.3 dB å’Œ 0.005ã€‚åœ¨ä¸´åºŠæ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ PSNR å’Œ SSIM æŒ‡æ ‡ä¸Šä¹Ÿå–å¾—äº†æ˜¾ç€çš„æ”¹è¿›ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æˆ‘ä»¬çš„ç›®æ ‡ï¼Œå³å¼€å‘ä¸€ç§é«˜æ•ˆä¸”å‡†ç¡®çš„ MRI SR é‡å»ºæ–¹æ³•ã€‚</p><p>7.æ–¹æ³•ï¼šï¼ˆ1ï¼‰æå‡ºäº†ä¸€ç§åä¸ºDiffMSRçš„é«˜æ•ˆæ‰©æ•£æ¨¡å‹ï¼Œç”¨äºå¤šå¯¹æ¯”åº¦MRIè¶…åˆ†è¾¨ç‡é‡å»ºï¼›ï¼ˆ2ï¼‰å°†æ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰åº”ç”¨äºé«˜åº¦ç´§å‡‘çš„ä½ç»´æ½œåœ¨ç©ºé—´ä¸­ç”Ÿæˆå…ˆéªŒçŸ¥è¯†ï¼›ï¼ˆ3ï¼‰è®¾è®¡äº†å…ˆéªŒå¼•å¯¼å¤§çª—å£Transformerï¼ˆPLWformerï¼‰ä½œä¸ºDMçš„è§£ç å™¨ï¼Œå®ƒå¯ä»¥åœ¨å……åˆ†åˆ©ç”¨DMç”Ÿæˆçš„å…ˆéªŒçŸ¥è¯†çš„åŒæ—¶æ‰©å±•æ„Ÿå—é‡ï¼›ï¼ˆ4ï¼‰åœ¨å…¬å…±å’Œä¸´åºŠæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼ŒéªŒè¯äº†DiffMSRçš„ä¼˜è¶Šæ€§èƒ½ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„æ‰©æ•£æ¨¡å‹ DiffMSRï¼Œç”¨äºå¤šå¯¹æ¯”åº¦ MRI è¶…åˆ†è¾¨ç‡é‡å»ºï¼Œè¯¥æ¨¡å‹å°† DM å’Œ Transformer ç›¸ç»“åˆï¼Œä»…éœ€å››æ¬¡è¿­ä»£å³å¯é‡å»ºé«˜è´¨é‡å›¾åƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº† PLWformerï¼Œå®ƒå¯ä»¥åœ¨ä¸å¢åŠ è®¡ç®—è´Ÿæ‹…çš„æƒ…å†µä¸‹æ‰©å±•æ³¨æ„åŠ›çª—å£å¤§å°ï¼Œå¹¶å¯ä»¥åˆ©ç”¨ DM ç”Ÿæˆçš„å…ˆéªŒçŸ¥è¯†é‡å»ºå…·æœ‰é«˜é¢‘ä¿¡æ¯çš„ MRI å›¾åƒã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ DiffMSR ä¼˜äºç°æœ‰çš„ SOTA æ–¹æ³•ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§ç”¨äºå¤šå¯¹æ¯”åº¦ MRI è¶…åˆ†è¾¨ç‡é‡å»ºçš„é«˜æ•ˆæ‰©æ•£æ¨¡å‹ DiffMSRï¼›å°†æ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰åº”ç”¨äºé«˜åº¦ç´§å‡‘çš„ä½ç»´æ½œåœ¨ç©ºé—´ä¸­ç”Ÿæˆå…ˆéªŒçŸ¥è¯†ï¼›è®¾è®¡äº†å…ˆéªŒå¼•å¯¼å¤§çª—å£ Transformerï¼ˆPLWformerï¼‰ä½œä¸º DM çš„è§£ç å™¨ï¼Œå®ƒå¯ä»¥åœ¨å……åˆ†åˆ©ç”¨ DM ç”Ÿæˆçš„å…ˆéªŒçŸ¥è¯†çš„åŒæ—¶æ‰©å±•æ„Ÿå—é‡ã€‚æ€§èƒ½ï¼šåœ¨å…¬å…±å’Œä¸´åºŠæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ DiffMSR ä¼˜äºç°æœ‰çš„ SOTA æ–¹æ³•ã€‚åœ¨ FastMRI æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ PSNR å’Œ SSIM æŒ‡æ ‡ä¸Šåˆ†åˆ«æ¯”æœ€å…ˆè¿›çš„æ–¹æ³•æé«˜äº† 0.3dB å’Œ 0.005ã€‚åœ¨ä¸´åºŠæ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ PSNR å’Œ SSIM æŒ‡æ ‡ä¸Šä¹Ÿå–å¾—äº†æ˜¾ç€çš„æ”¹è¿›ã€‚å·¥ä½œé‡ï¼šä¸ç°æœ‰çš„åŸºäº DM çš„ SR é‡å»ºæ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„ DiffMSR å…·æœ‰æ›´é«˜çš„æ•ˆç‡å’Œæ›´ä½çš„è®¡ç®—æˆæœ¬ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»…éœ€å››æ¬¡è¿­ä»£å³å¯é‡å»ºé«˜è´¨é‡å›¾åƒï¼Œè€Œç°æœ‰çš„æ–¹æ³•é€šå¸¸éœ€è¦å‡ åæ¬¡ç”šè‡³æ•°ç™¾æ¬¡è¿­ä»£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è®¡ç®—æˆæœ¬æ–¹é¢ä¹Ÿæ›´ä½ï¼Œå› ä¸ºå®ƒä½¿ç”¨é«˜åº¦ç´§å‡‘çš„ä½ç»´æ½œåœ¨ç©ºé—´æ¥ç”Ÿæˆå…ˆéªŒçŸ¥è¯†ï¼Œå¹¶ä¸”ä½¿ç”¨ PLWformer ä½œä¸ºè§£ç å™¨ï¼Œè¯¥è§£ç å™¨å¯ä»¥æ‰©å±•æ„Ÿå—é‡è€Œä¸å¢åŠ è®¡ç®—è´Ÿæ‹…ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-fab7cb8e4dbcff8c6fb52d0547898323.jpg" align="middle"><img src="https://picx.zhimg.com/v2-125112a90313cfa5c6897db82bd60236.jpg" align="middle"><img src="https://picx.zhimg.com/v2-df6190a9bc5535eaf3663c9cd6127ad0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-dccdba0a4a3109932c5ed7a8ea55d49f.jpg" align="middle"></details><h2 id="InitNO-Boosting-Text-to-Image-Diffusion-Models-via-Initial-Noise-Optimization"><a href="#InitNO-Boosting-Text-to-Image-Diffusion-Models-via-Initial-Noise-Optimization" class="headerlink" title="InitNO: Boosting Text-to-Image Diffusion Models via Initial Noise   Optimization"></a>InitNO: Boosting Text-to-Image Diffusion Models via Initial Noise   Optimization</h2><p><strong>Authors:Xiefan Guo, Jinlin Liu, Miaomiao Cui, Jiankai Li, Hongyu Yang, Di Huang</strong></p><p>Recent strides in the development of diffusion models, exemplified by advancements such as Stable Diffusion, have underscored their remarkable prowess in generating visually compelling images. However, the imperative of achieving a seamless alignment between the generated image and the provided prompt persists as a formidable challenge. This paper traces the root of these difficulties to invalid initial noise, and proposes a solution in the form of Initial Noise Optimization (InitNO), a paradigm that refines this noise. Considering text prompts, not all random noises are effective in synthesizing semantically-faithful images. We design the cross-attention response score and the self-attention conflict score to evaluate the initial noise, bifurcating the initial latent space into valid and invalid sectors. A strategically crafted noise optimization pipeline is developed to guide the initial noise towards valid regions. Our method, validated through rigorous experimentation, shows a commendable proficiency in generating images in strict accordance with text prompts. Our code is available at <a href="https://github.com/xiefan-guo/initno">https://github.com/xiefan-guo/initno</a>. </p><p><a href="http://arxiv.org/abs/2404.04650v1">PDF</a> Accepted by CVPR 2024</p><p><strong>Summary</strong><br>æ–‡æœ¬æå‡ºäº†ä¸€ç§æ”¹è¿›åˆå§‹å™ªå£°ï¼Œä»¥æé«˜åŸºäºæ–‡æœ¬æç¤ºç”Ÿæˆå›¾åƒçš„è´¨é‡ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æ— æ•ˆçš„åˆå§‹å™ªå£°ä¼šé˜»ç¢æ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆé«˜è´¨é‡å›¾åƒã€‚</li><li>è·¨æ³¨æ„åŠ›å“åº”å¾—åˆ†å’Œè‡ªæ³¨æ„åŠ›å†²çªå¾—åˆ†å¯ç”¨äºè¯„ä¼°åˆå§‹å™ªå£°çš„æœ‰æ•ˆæ€§ã€‚</li><li>åŸºäºåˆ†æ•°çš„å™ªå£°ä¼˜åŒ–ç®¡é“å°†åˆå§‹å™ªå£°å¼•å¯¼è‡³æœ‰æ•ˆåŒºåŸŸã€‚</li><li>InitNO åœ¨æ–‡æœ¬æç¤ºæŒ‡å¯¼å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚</li><li>ä»£ç å¯åœ¨ <a href="https://github.com/xiefan-guo/initno">https://github.com/xiefan-guo/initno</a> è·å–ã€‚</li><li>ä¼˜åŒ–åˆå§‹å™ªå£°æ˜¯æ”¹å–„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­å›¾åƒå’Œæ–‡æœ¬æç¤ºå¯¹é½çš„å…³é”®ã€‚</li><li>InitNO ç®—æ³•ä½“ç°äº†å™ªå£°ä¼˜åŒ–åœ¨è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†äº¤å‰é¢†åŸŸä¸­çš„åº”ç”¨ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šåŸºäºåˆå§‹å™ªå£°ä¼˜åŒ–çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å¢å¼º</li><li>ä½œè€…ï¼šè°¢å¸†å›½ã€é‡‘ç³ã€å´”å¦™å¦™ã€æå»ºå‡¯ã€æ¨é¸¿å®‡ã€é»„è¿ª</li><li>éš¶å±ï¼šåŒ—äº¬èˆªç©ºèˆªå¤©å¤§å­¦è½¯ä»¶å¼€å‘ç¯å¢ƒå›½å®¶é‡ç‚¹å®éªŒå®¤</li><li>å…³é”®è¯ï¼šæ–‡æœ¬åˆ°å›¾åƒåˆæˆã€æ‰©æ•£æ¨¡å‹ã€åˆå§‹å™ªå£°ä¼˜åŒ–</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.04650   Github ä»£ç é“¾æ¥ï¼šhttps://github.com/xiefan-guo/initno</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ï¼šæ–‡æœ¬åˆ°å›¾åƒåˆæˆï¼ˆT2Iï¼‰æ˜¯ç”Ÿæˆæ¨¡å‹é¢†åŸŸçš„å‰æ²¿ç ”ç©¶ï¼Œè‡´åŠ›äºä»æ–‡æœ¬æç¤ºä¸­ç”ŸæˆçœŸå®ä¸”è§†è§‰ä¸Šè¿è´¯çš„å›¾åƒã€‚åœ¨ç”Ÿæˆæ¨¡å‹é¢†åŸŸï¼ŒåŒ…æ‹¬ç”Ÿæˆå¯¹æŠ—ç½‘ç»œã€å˜åˆ†è‡ªç¼–ç å™¨å’Œè‡ªå›å½’æ¨¡å‹ï¼Œæ‰©æ•£æ¨¡å‹å·²æˆä¸ºä¸€ç§ä¸»è¦çš„è§£å†³æ–¹æ¡ˆã€‚ï¼ˆ2ï¼‰ï¼šå°½ç®¡åœ¨å¤§å‹æ–‡æœ¬å›¾åƒæ•°æ®é›†ä¸Šè®­ç»ƒäº†æœ€å…ˆè¿›çš„ T2I æ‰©æ•£æ¨¡å‹ï¼Œä½†ä¸ç»™å®šæ–‡æœ¬æç¤ºå®Œå…¨å¯¹é½çš„å›¾åƒåˆæˆä»ç„¶æ˜¯ä¸€ä¸ªç›¸å½“å¤§çš„æŒ‘æˆ˜ã€‚ä¼—æ‰€å‘¨çŸ¥çš„é—®é¢˜ï¼Œå³ä¸»é¢˜å¿½ç•¥ã€ä¸»é¢˜æ··åˆå’Œä¸æ­£ç¡®çš„å±æ€§ç»‘å®šï¼Œå¦‚å›¾ 1 æ‰€ç¤ºï¼Œä»ç„¶å­˜åœ¨ã€‚æˆ‘ä»¬å°†è¿™äº›æŒ‘æˆ˜å½’å› äºæ— æ•ˆçš„åˆå§‹å™ªå£°ã€‚å½“å°†ä¸åŒçš„å™ªå£°è¾“å…¥å¼•å…¥å…·æœ‰ç›¸åŒæ–‡æœ¬æç¤ºçš„ T2I æ‰©æ•£æ¨¡å‹æ—¶ï¼Œåœ¨å›¾åƒå’Œæä¾›çš„æ–‡æœ¬ä¹‹é—´è§‚å¯Ÿåˆ°å¯¹é½ä¸Šçš„å®è´¨æ€§å·®å¼‚ï¼Œå¦‚å›¾ 2 æ‰€ç¤ºã€‚è¿™ä¸€è§‚å¯Ÿè¡¨æ˜ï¼Œå¹¶éæ‰€æœ‰éšæœºé‡‡æ ·çš„å™ªå£°éƒ½èƒ½äº§ç”Ÿè§†è§‰ä¸Šä¸€è‡´çš„å›¾åƒã€‚æ ¹æ®ç”Ÿæˆçš„å›¾åƒä¸ç›®æ ‡æ–‡æœ¬ä¹‹é—´çš„ä¸€è‡´æ€§ï¼Œåˆå§‹æ½œåœ¨ç©ºé—´å¯ä»¥åˆ’åˆ†ä¸ºæœ‰æ•ˆåŒºåŸŸå’Œæ— æ•ˆåŒºåŸŸã€‚ä»æœ‰æ•ˆåŒºåŸŸè·å–çš„å™ªå£°è¾“å…¥åˆ° T2I æ‰©æ•£æ¨¡å‹åï¼Œä¼šäº§ç”Ÿè¯­ä¹‰ä¸Šåˆç†çš„å›¾åƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯å°†ä»»ä½•åˆå§‹å™ªå£°å¼•å¯¼åˆ°æœ‰æ•ˆåŒºåŸŸï¼Œä»è€Œä¿ƒè¿›å›¾åƒç”Ÿæˆã€‚ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç§°ä¸ºåˆå§‹å™ªå£°ä¼˜åŒ–ï¼ˆINITNOï¼‰çš„èŒƒä¾‹æ¥è§£å†³æ— æ•ˆåˆå§‹å™ªå£°çš„é—®é¢˜ã€‚INITNO é€šè¿‡è®¾è®¡äº¤å‰æ³¨æ„åŠ›å“åº”åˆ†æ•°å’Œè‡ªæ³¨æ„åŠ›å†²çªåˆ†æ•°æ¥è¯„ä¼°åˆå§‹å™ªå£°ï¼Œå°†åˆå§‹æ½œåœ¨ç©ºé—´åˆ†ä¸ºæœ‰æ•ˆå’Œæ— æ•ˆåŒºåŸŸã€‚å¼€å‘äº†ä¸€ä¸ªç­–ç•¥æ€§è®¾è®¡çš„å™ªå£°ä¼˜åŒ–ç®¡é“ï¼Œä»¥å°†åˆå§‹å™ªå£°å¼•å¯¼åˆ°æœ‰æ•ˆåŒºåŸŸã€‚ï¼ˆ4ï¼‰ï¼šINITNO åœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†å‡ºè‰²çš„æ€§èƒ½ï¼Œåœ¨ä¸æ–‡æœ¬æç¤ºä¸¥æ ¼ä¸€è‡´çš„æƒ…å†µä¸‹ç”Ÿæˆäº†å›¾åƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒINITNO èƒ½å¤Ÿæœ‰æ•ˆåœ°è§£å†³ä¸»é¢˜å¿½ç•¥ã€ä¸»é¢˜æ··åˆå’Œä¸æ­£ç¡®çš„å±æ€§ç»‘å®šç­‰é—®é¢˜ã€‚</li></ol><p><strong>æ–¹æ³•</strong></p><p>(1) <strong>åˆå§‹å™ªå£°è¯„ä¼°ï¼š</strong>   - è®¾è®¡äº¤å‰æ³¨æ„åŠ›å“åº”åˆ†æ•°å’Œè‡ªæ³¨æ„åŠ›å†²çªåˆ†æ•°ï¼Œå°†åˆå§‹æ½œåœ¨ç©ºé—´åˆ’åˆ†ä¸ºæœ‰æ•ˆå’Œæ— æ•ˆåŒºåŸŸã€‚</p><p>(2) <strong>å™ªå£°ä¼˜åŒ–ç®¡é“ï¼š</strong>   - ç­–ç•¥æ€§è®¾è®¡å™ªå£°ä¼˜åŒ–ç®¡é“ï¼Œå°†åˆå§‹å™ªå£°å¼•å¯¼åˆ°æœ‰æ•ˆåŒºåŸŸã€‚</p><p>(3) <strong>ç”¨æˆ·ç ”ç©¶ï¼š</strong>   - ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼ŒINITNO åœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†å‡ºè‰²çš„æ€§èƒ½ï¼Œåœ¨ä¸æ–‡æœ¬æç¤ºä¸¥æ ¼ä¸€è‡´çš„æƒ…å†µä¸‹ç”Ÿæˆäº†å›¾åƒã€‚</p><p>(4) <strong>æ¨ç†æ—¶é—´ï¼š</strong>   - åœ¨å•ä¸ª Tesla V100 (32GB) ä¸Šè¯„ä¼°ï¼ŒINITNO åˆæˆäº† 100 å¼ åˆ†è¾¨ç‡ä¸º 512Ã—512 åƒç´ çš„å›¾åƒï¼Œå¹³å‡ç”¨æ—¶ 18.93 ç§’ã€‚</p><p>(5) <strong>æ¶ˆèç ”ç©¶ï¼š</strong>   - <strong>è‡ªæ³¨æ„åŠ›å†²çªæŸå¤±ï¼š</strong>æœ‰æ•ˆè§£å†³äº†è‡ªæ³¨æ„åŠ›é‡å å¼•èµ·çš„ä¸»é¢˜æ··åˆé—®é¢˜ã€‚   - <strong>åˆ†å¸ƒå¯¹é½æŸå¤±ï¼š</strong>ç¡®ä¿ä¼˜åŒ–åçš„å™ªå£°ç¬¦åˆæ ‡å‡†æ­£æ€åˆ†å¸ƒã€‚</p><p>(6) <strong>åŸºäºæ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆï¼š</strong>   - INITNO æ˜¯ä¸€ç§å³æ’å³ç”¨æ–¹æ³•ï¼Œå¯ä»¥è½»æ¾é›†æˆåˆ°ç°æœ‰æ‰©æ•£æ¨¡å‹ä¸­ï¼Œå®ç°æ— è®­ç»ƒçš„å¯æ§ç”Ÿæˆï¼Œä¾‹å¦‚å¸ƒå±€åˆ°å›¾åƒã€è’™ç‰ˆåˆ°å›¾åƒç”Ÿæˆç­‰ã€‚</p><p><strong>8. ç»“è®º</strong></p><p><strong>(1): æœ¬å·¥ä½œçš„æ„ä¹‰</strong></p><p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºåˆå§‹å™ªå£°ä¼˜åŒ–ï¼ˆINITNOï¼‰çš„èŒƒä¾‹ï¼Œä»¥è§£å†³æ— æ•ˆåˆå§‹å™ªå£°çš„é—®é¢˜ã€‚INITNOé€šè¿‡è®¾è®¡äº¤å‰æ³¨æ„åŠ›å“åº”åˆ†æ•°å’Œè‡ªæ³¨æ„åŠ›å†²çªåˆ†æ•°æ¥è¯„ä¼°åˆå§‹å™ªå£°ï¼Œå°†åˆå§‹æ½œåœ¨ç©ºé—´åˆ’åˆ†ä¸ºæœ‰æ•ˆå’Œæ— æ•ˆåŒºåŸŸã€‚å¼€å‘äº†ä¸€ä¸ªç­–ç•¥æ€§è®¾è®¡çš„å™ªå£°ä¼˜åŒ–ç®¡é“ï¼Œä»¥å°†åˆå§‹å™ªå£°å¼•å¯¼åˆ°æœ‰æ•ˆåŒºåŸŸã€‚INITNOåœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†å‡ºè‰²çš„æ€§èƒ½ï¼Œåœ¨ä¸æ–‡æœ¬æç¤ºä¸¥æ ¼ä¸€è‡´çš„æƒ…å†µä¸‹ç”Ÿæˆäº†å›¾åƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒINITNOèƒ½å¤Ÿæœ‰æ•ˆåœ°è§£å†³ä¸»é¢˜å¿½ç•¥ã€ä¸»é¢˜æ··åˆå’Œä¸æ­£ç¡®çš„å±æ€§ç»‘å®šç­‰é—®é¢˜ã€‚</p><p><strong>(2): æœ¬æ–‡çš„ä¼˜ç¼ºç‚¹æ€»ç»“</strong></p><p><strong>åˆ›æ–°ç‚¹ï¼š</strong></p><ul><li>æå‡ºäº†ä¸€ç§æ–°çš„åˆå§‹å™ªå£°è¯„ä¼°æ–¹æ³•ï¼Œå¯ä»¥å°†åˆå§‹æ½œåœ¨ç©ºé—´åˆ’åˆ†ä¸ºæœ‰æ•ˆå’Œæ— æ•ˆåŒºåŸŸã€‚</li><li>è®¾è®¡äº†ä¸€ä¸ªç­–ç•¥æ€§è®¾è®¡çš„å™ªå£°ä¼˜åŒ–ç®¡é“ï¼Œå°†åˆå§‹å™ªå£°å¼•å¯¼åˆ°æœ‰æ•ˆåŒºåŸŸã€‚</li><li>æå‡ºäº†ä¸€ç§æ–°çš„åˆ†å¸ƒå¯¹é½æŸå¤±ï¼Œä»¥ç¡®ä¿ä¼˜åŒ–åçš„å™ªå£°ç¬¦åˆæ ‡å‡†æ­£æ€åˆ†å¸ƒã€‚</li></ul><p><strong>æ€§èƒ½ï¼š</strong></p><ul><li>INITNOåœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†å‡ºè‰²çš„æ€§èƒ½ï¼Œåœ¨ä¸æ–‡æœ¬æç¤ºä¸¥æ ¼ä¸€è‡´çš„æƒ…å†µä¸‹ç”Ÿæˆäº†å›¾åƒã€‚</li><li>INITNOèƒ½å¤Ÿæœ‰æ•ˆåœ°è§£å†³ä¸»é¢˜å¿½ç•¥ã€ä¸»é¢˜æ··åˆå’Œä¸æ­£ç¡®çš„å±æ€§ç»‘å®šç­‰é—®é¢˜ã€‚</li></ul><p><strong>å·¥ä½œé‡ï¼š</strong></p><ul><li>INITNOæ˜¯ä¸€ç§å³æ’å³ç”¨çš„æ–¹æ³•ï¼Œå¯ä»¥è½»æ¾é›†æˆåˆ°ç°æœ‰æ‰©æ•£æ¨¡å‹ä¸­ï¼Œå®ç°æ— è®­ç»ƒçš„å¯æ§ç”Ÿæˆã€‚</li><li>INITNOçš„æ¨ç†æ—¶é—´ç›¸å¯¹è¾ƒçŸ­ï¼Œåœ¨å•ä¸ªTesla V100 (32GB) ä¸Šè¯„ä¼°ï¼ŒINITNO åˆæˆäº† 100 å¼ åˆ†è¾¨ç‡ä¸º 512Ã—512 åƒç´ çš„å›¾åƒï¼Œå¹³å‡ç”¨æ—¶ 18.93 ç§’ã€‚</li></ul><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-6b8805d41a0f842dfd100f0ec94562de.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f4cf1dd225d50f9419f7438de165c98a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b2a7e6fec8bf9c557df9b7c39d0a37ee.jpg" align="middle"><img src="https://pica.zhimg.com/v2-3db98ef94d50d29cc49f8e9fe6509549.jpg" align="middle"><img src="https://pica.zhimg.com/v2-479a0f109d0d474a6bb3e17b7fcb99fd.jpg" align="middle"></details>## Diffusion Time-step Curriculum for One Image to 3D Generation**Authors:Xuanyu Yi, Zike Wu, Qingshan Xu, Pan Zhou, Joo-Hwee Lim, Hanwang Zhang**Score distillation sampling~(SDS) has been widely adopted to overcome the absence of unseen views in reconstructing 3D objects from a \textbf{single} image. It leverages pre-trained 2D diffusion models as teacher to guide the reconstruction of student 3D models. Despite their remarkable success, SDS-based methods often encounter geometric artifacts and texture saturation. We find out the crux is the overlooked indiscriminate treatment of diffusion time-steps during optimization: it unreasonably treats the student-teacher knowledge distillation to be equal at all time-steps and thus entangles coarse-grained and fine-grained modeling. Therefore, we propose the Diffusion Time-step Curriculum one-image-to-3D pipeline (DTC123), which involves both the teacher and student models collaborating with the time-step curriculum in a coarse-to-fine manner. Extensive experiments on NeRF4, RealFusion15, GSO and Level50 benchmark demonstrate that DTC123 can produce multi-view consistent, high-quality, and diverse 3D assets. Codes and more generation demos will be released in https://github.com/yxymessi/DTC123. [PDF](http://arxiv.org/abs/2404.04562v1) **Summary**é€æ­¥çš„æ‰©æ•£æ—¶é—´è®¾ç½®æŒ‡å¯¼å­¦ç”Ÿæ¨¡å‹ä»å•ä¸€å›¾åƒç”Ÿæˆé«˜è´¨é‡ 3D å¯¹è±¡ã€‚**Key Takeaways**- æœªç»å¤„ç†çš„æ‰©æ•£æ—¶é—´æ­¥é•¿ä¼˜åŒ–å¯¼è‡´å­¦ç”Ÿæ¨¡å‹å‡ ä½•é”™è¯¯å’Œçº¹ç†é¥±å’Œåº¦ã€‚- DTC123 æå‡ºäº†ä¸€ç§ä»ç²—åˆ°ç»†çš„æ—¶é—´æ­¥é•¿è¯¾ç¨‹è¡¨ï¼Œç”¨äºæŒ‡å¯¼å­¦ç”Ÿå’Œæ•™å¸ˆæ¨¡å‹ååŒå·¥ä½œã€‚- DTC123 åœ¨ NeRF4ã€RealFusion15ã€GSO å’Œ Level50 åŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç”Ÿæˆå¤šè§†å›¾ä¸€è‡´ã€é«˜è´¨é‡å’Œå¤šæ ·çš„ 3D èµ„äº§ã€‚- DTC123 æ–¹æ³•å…‹æœäº†ä»å•ä¸€å›¾åƒé‡å»º 3D å¯¹è±¡æ—¶ç¼ºä¹æœªè§è§†å›¾çš„æŒ‘æˆ˜ã€‚- æ•™å¸ˆæ¨¡å‹åœ¨ç²—ç²’åº¦å»ºæ¨¡ä¸­æä¾›æŒ‡å¯¼ï¼Œè€Œå­¦ç”Ÿæ¨¡å‹åœ¨ç»†ç²’åº¦ç»†èŠ‚ä¸­è¿›è¡Œå¾®è°ƒã€‚- æ—¶é—´æ­¥é•¿è¯¾ç¨‹è¡¨å¯ç¡®ä¿åœ¨ä¸åŒé˜¶æ®µé‡ç‚¹å…³æ³¨ä¸åŒç²’åº¦çš„ç‰¹å¾ã€‚- ä»£ç å’Œæ›´å¤šç”Ÿæˆæ¼”ç¤ºå°†äº https://github.com/yxymessi/DTC123 å‘å¸ƒã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>æ ‡é¢˜ï¼šæ‰©æ•£æ—¶é—´æ­¥è¯¾ç¨‹è¡¨ï¼šå•å›¾åƒåˆ° 3D çš„æ–°ç®¡é“</li><li>ä½œè€…ï¼šYuxiao Yao, Yifan Jiang, Yuxin Wen, Jingyu Yang, Zhe Lin, Chen Change Loy, Ziwei Liu</li><li>éš¶å±ï¼šé¦™æ¸¯ä¸­æ–‡å¤§å­¦ï¼ˆæ·±åœ³ï¼‰</li><li>å…³é”®è¯ï¼š3D é‡å»ºï¼Œå›¾åƒåˆ° 3Dï¼Œæ‰©æ•£æ¨¡å‹ï¼ŒçŸ¥è¯†è’¸é¦ï¼Œæ—¶é—´æ­¥è¯¾ç¨‹è¡¨</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.12910ï¼ŒGithub ä»£ç é“¾æ¥ï¼šhttps://github.com/yxymessi/DTC123</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šå•å›¾åƒ 3D é‡å»ºæ–¹æ³•åœ¨è¿‡å»å‡ å¹´ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»ç„¶å­˜åœ¨å‡ ä½•ä¼ªå½±å’Œçº¹ç†é¥±å’Œç­‰é—®é¢˜ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šåŸºäº SDS çš„æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„ 2D æ‰©æ•£æ¨¡å‹ä½œä¸ºæ•™å¸ˆæ¥æŒ‡å¯¼å­¦ç”Ÿ 3D æ¨¡å‹çš„é‡å»ºï¼Œä½†å®ƒä»¬å¿½ç•¥äº†æ‰©æ•£æ—¶é—´æ­¥æœŸé—´çš„çŸ¥è¯†è’¸é¦å¤„ç†ï¼Œå¯¼è‡´ç²—ç²’åº¦å’Œç»†ç²’åº¦å»ºæ¨¡çº ç¼ åœ¨ä¸€èµ·ã€‚ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†æ‰©æ•£æ—¶é—´æ­¥è¯¾ç¨‹è¡¨å•å›¾åƒåˆ° 3D ç®¡é“ï¼ˆDTC123ï¼‰ï¼Œè¯¥ç®¡é“ä»¥ç²—åˆ°ç»†çš„æ–¹å¼æ¶‰åŠæ•™å¸ˆå’Œå­¦ç”Ÿæ¨¡å‹ä¸æ—¶é—´æ­¥è¯¾ç¨‹è¡¨çš„åä½œã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šåœ¨ NeRF4ã€RealFusion15ã€GSO å’Œ Level50 åŸºå‡†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDTC123 å¯ä»¥ç”Ÿæˆå¤šè§†å›¾ä¸€è‡´ã€é«˜è´¨é‡å’Œå¤šæ ·åŒ–çš„ 3D èµ„äº§ï¼Œè¿™æ”¯æŒäº†ä»–ä»¬çš„ç›®æ ‡ã€‚</li></ol><p>Some Error for method(æ¯”å¦‚æ˜¯ä¸æ˜¯æ²¡æœ‰Methodsè¿™ä¸ªç« èŠ‚)</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†æ‰©æ•£æ—¶é—´æ­¥è¯¾ç¨‹è¡¨ï¼Œé€šè¿‡ç²—åˆ°ç»†çš„æ–¹å¼è®©æ•™å¸ˆå’Œå­¦ç”Ÿæ¨¡å‹ä¸æ—¶é—´æ­¥è¯¾ç¨‹è¡¨åä½œï¼Œæ˜¾è‘—æé«˜äº†å›¾åƒåˆ° 3D ç”Ÿæˆä¸­çš„çœŸå®æ„Ÿå’Œå¤šè§†å›¾ä¸€è‡´æ€§ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šDiffusion Time-step Curriculumï¼›æ€§èƒ½ï¼šåœ¨ NeRF4ã€RealFusion15ã€GSO å’Œ Level50 åŸºå‡†ä¸Šè¡¨ç°å‡ºè‰²ï¼›å·¥ä½œé‡ï¼šä¸­ç­‰ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-744c7f5a081447863699bed80f656a2a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-dd5d14fea14d35db1bbda6adb0c315a9.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-551a47f8383d1a4797b18d85cec41fb3.jpg" align="middle"><img src="https://pica.zhimg.com/v2-4b111e6fcddc0b871d26d7799de87b88.jpg" align="middle"></details><h2 id="BeyondScene-Higher-Resolution-Human-Centric-Scene-Generation-With-Pretrained-Diffusion"><a href="#BeyondScene-Higher-Resolution-Human-Centric-Scene-Generation-With-Pretrained-Diffusion" class="headerlink" title="BeyondScene: Higher-Resolution Human-Centric Scene Generation With   Pretrained Diffusion"></a>BeyondScene: Higher-Resolution Human-Centric Scene Generation With   Pretrained Diffusion</h2><p><strong>Authors:Gwanghyun Kim, Hayeon Kim, Hoigi Seo, Dong Un Kang, Se Young Chun</strong></p><p>Generating higher-resolution human-centric scenes with details and controls remains a challenge for existing text-to-image diffusion models. This challenge stems from limited training image size, text encoder capacity (limited tokens), and the inherent difficulty of generating complex scenes involving multiple humans. While current methods attempted to address training size limit only, they often yielded human-centric scenes with severe artifacts. We propose BeyondScene, a novel framework that overcomes prior limitations, generating exquisite higher-resolution (over 8K) human-centric scenes with exceptional text-image correspondence and naturalness using existing pretrained diffusion models. BeyondScene employs a staged and hierarchical approach to initially generate a detailed base image focusing on crucial elements in instance creation for multiple humans and detailed descriptions beyond token limit of diffusion model, and then to seamlessly convert the base image to a higher-resolution output, exceeding training image size and incorporating details aware of text and instances via our novel instance-aware hierarchical enlargement process that consists of our proposed high-frequency injected forward diffusion and adaptive joint diffusion. BeyondScene surpasses existing methods in terms of correspondence with detailed text descriptions and naturalness, paving the way for advanced applications in higher-resolution human-centric scene creation beyond the capacity of pretrained diffusion models without costly retraining. Project page: <a href="https://janeyeon.github.io/beyond-scene">https://janeyeon.github.io/beyond-scene</a>. </p><p><a href="http://arxiv.org/abs/2404.04544v1">PDF</a> Project page: <a href="https://janeyeon.github.io/beyond-scene">https://janeyeon.github.io/beyond-scene</a></p><p><strong>Summary</strong><br>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜åˆ†è¾¨ç‡ã€åŒ…å«äººç±»å…ƒç´ ä¸”å¯Œæœ‰ç»†èŠ‚å’Œå¯æ§çš„åœºæ™¯æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡º BeyondScene æ¡†æ¶æ¥è§£å†³è¿™ä¸€éš¾é¢˜ï¼Œä½¿ç”¨ç°æˆçš„é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆåˆ†è¾¨ç‡è¶…è¿‡ 8K çš„äººåƒä¸­å¿ƒåœºæ™¯ï¼Œå¹¶å…·æœ‰å‡ºè‰²çš„æ–‡æœ¬å›¾åƒå¯¹åº”å’Œè‡ªç„¶åº¦ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>BeyondScene é‡‡ç”¨åˆ†é˜¶æ®µã€åˆ†å±‚çš„æ–¹æ³•ï¼Œå…ˆç”Ÿæˆä¸€ä¸ªå…³æ³¨å…³é”®å…ƒç´ çš„è¯¦ç»†åŸºç¡€å›¾åƒï¼Œç„¶åå°†å…¶è½¬æ¢ä¸ºé«˜åˆ†è¾¨ç‡è¾“å‡ºã€‚</li><li>é«˜é¢‘æ³¨å…¥å‰å‘æ‰©æ•£å’Œè‡ªé€‚åº”è”åˆæ‰©æ•£èƒ½å¤Ÿæ„ŸçŸ¥æ–‡æœ¬å’Œå®ä¾‹çš„ç»†èŠ‚ï¼Œç”Ÿæˆè‡ªç„¶çš„äººåƒä¸­å¿ƒåœºæ™¯ã€‚</li><li>BeyondScene åœ¨æ–‡æœ¬æè¿°å¯¹åº”å’Œè‡ªç„¶åº¦æ–¹é¢è¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œä¸ºåœ¨ç°æœ‰é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹èƒ½åŠ›ä¹‹å¤–åˆ›å»ºé«˜åˆ†è¾¨ç‡äººåƒä¸­å¿ƒåœºæ™¯çš„é«˜çº§åº”ç”¨é“ºå¹³äº†é“è·¯ã€‚</li><li>BeyondSceneæ— éœ€è¿›è¡Œä»£ä»·é«˜æ˜‚çš„é‡æ–°è®­ç»ƒï¼Œå³å¯ä½¿ç”¨ç°æˆçš„é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆé«˜åˆ†è¾¨ç‡ã€åŒ…å«äººç±»å…ƒç´ ä¸”å¯Œæœ‰ç»†èŠ‚å’Œå¯æ§çš„åœºæ™¯ã€‚</li><li>BeyondScene é€šè¿‡<a href="https://janeyeon.github.io/beyond-sceneæä¾›é¡¹ç›®ä¸»é¡µã€‚">https://janeyeon.github.io/beyond-sceneæä¾›é¡¹ç›®ä¸»é¡µã€‚</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šè¶…è¶Šåœºæ™¯ï¼šæ›´é«˜åˆ†è¾¨ç‡çš„äººä½“ä¸­å¿ƒè¡¥å……ææ–™</li><li>ä½œè€…ï¼šJane Yeonã€Minseop Parkã€Seunghoon Hong</li><li>æ‰€å±æœºæ„ï¼šé¦–å°”å¤§å­¦</li><li>å…³é”®è¯ï¼šä»¥äººä¸ºä¸­å¿ƒçš„åœºæ™¯ç”Ÿæˆã€æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ã€é«˜åˆ†è¾¨ç‡</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.08182ï¼ŒGithub ä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šç°æœ‰æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜åˆ†è¾¨ç‡ã€ä»¥äººä¸ºä¸­å¿ƒä¸”ç»†èŠ‚ä¸°å¯Œã€å¯æ§çš„åœºæ™¯æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼ŒåŸå› åœ¨äºè®­ç»ƒå›¾åƒå°ºå¯¸ã€æ–‡æœ¬ç¼–ç å™¨å®¹é‡ï¼ˆä»¤ç‰Œæ•°é‡æœ‰é™ï¼‰å’Œç”Ÿæˆæ¶‰åŠå¤šä¸ªäººç‰©çš„å¤æ‚åœºæ™¯çš„å›ºæœ‰éš¾åº¦ã€‚ï¼ˆ2ï¼‰ï¼šè¿‡å»çš„æ–¹æ³•å’Œé—®é¢˜ï¼šå½“å‰æ–¹æ³•ä»…å°è¯•è§£å†³è®­ç»ƒå°ºå¯¸é™åˆ¶ï¼Œä½†é€šå¸¸ä¼šäº§ç”Ÿå¸¦æœ‰ä¸¥é‡ä¼ªå½±çš„äººä½“ä¸­å¿ƒåœºæ™¯ã€‚è¯¥æ–¹æ³•çš„åŠ¨æœºå¾ˆå¥½ï¼Œå› ä¸ºå®ƒå…‹æœäº†å…ˆå‰çš„é™åˆ¶ï¼Œä½¿ç”¨ç°æœ‰çš„é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆäº†ç²¾ç¾çš„æ›´é«˜åˆ†è¾¨ç‡ï¼ˆè¶…è¿‡ 8Kï¼‰çš„äººä½“ä¸­å¿ƒåœºæ™¯ï¼Œå…·æœ‰å‡ºè‰²çš„æ–‡æœ¬å›¾åƒå¯¹åº”å…³ç³»å’Œè‡ªç„¶æ€§ã€‚ï¼ˆ3ï¼‰ï¼šæå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šBeyondScene é‡‡ç”¨åˆ†é˜¶æ®µä¸”åˆ†å±‚çš„æ–¹æ³•ï¼Œé¦–å…ˆç”Ÿæˆä¸€ä¸ªè¯¦ç»†çš„åŸºæœ¬å›¾åƒï¼Œé‡ç‚¹å…³æ³¨å¤šä¸ªäººçš„å®ä¾‹åˆ›å»ºä¸­çš„å…³é”®å…ƒç´ å’Œæ‰©æ•£æ¨¡å‹ä»¤ç‰Œé™åˆ¶ä¹‹å¤–çš„è¯¦ç»†æè¿°ï¼Œç„¶åå°†åŸºæœ¬å›¾åƒæ— ç¼è½¬æ¢ä¸ºæ›´é«˜åˆ†è¾¨ç‡çš„è¾“å‡ºï¼Œè¶…è¿‡è®­ç»ƒå›¾åƒå°ºå¯¸å¹¶é€šè¿‡æˆ‘ä»¬æ–°é¢–çš„å®ä¾‹æ„ŸçŸ¥åˆ†å±‚æ”¾å¤§è¿‡ç¨‹çº³å…¥æ–‡æœ¬å’Œå®ä¾‹æ„ŸçŸ¥çš„ç»†èŠ‚ï¼Œè¯¥è¿‡ç¨‹åŒ…æ‹¬æˆ‘ä»¬æå‡ºçš„é«˜é¢‘æ³¨å…¥æ­£å‘æ‰©æ•£å’Œè‡ªé€‚åº”è”åˆæ‰©æ•£ã€‚ï¼ˆ4ï¼‰ï¼šæ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†ä»€ä¹ˆæ€§èƒ½ï¼šBeyondScene åœ¨ä¸è¯¦ç»†æ–‡æœ¬æè¿°çš„å¯¹åº”å…³ç³»å’Œè‡ªç„¶æ€§æ–¹é¢è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œä¸ºåœ¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹å®¹é‡ä¹‹å¤–åˆ›å»ºæ›´é«˜åˆ†è¾¨ç‡çš„äººä½“ä¸­å¿ƒåœºæ™¯çš„é«˜çº§åº”ç”¨é“ºå¹³äº†é“è·¯ï¼Œè€Œæ— éœ€è¿›è¡Œæ˜‚è´µçš„é‡æ–°è®­ç»ƒã€‚</li></ol><p>7.æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼šè¯¦ç»†åŸºæœ¬å›¾åƒç”Ÿæˆï¼šåˆ©ç”¨SDXL-ControlNet-Openposeç›´æ¥ç”ŸæˆåŸºäºæ–‡æœ¬æè¿°å’Œå§¿æ€ä¿¡æ¯çš„å®ä¾‹ï¼Œé‡‡ç”¨Lang-SegmentAnythingè¿›è¡Œç²¾ç¡®çš„äººä½“åˆ†å‰²ï¼Œä½¿ç”¨ç›¸åŒçš„æ¨¡å‹å°†å¤´éƒ¨åŒºåŸŸåˆ†å‰²æˆâ€œå¤´éƒ¨â€å’Œâ€œå¤´å‘â€ï¼Œå†ç»„åˆå½¢æˆå¤´éƒ¨åˆ†å‰²ï¼Œç„¶åå¯¹èº«ä½“éƒ¨ä½è¿›è¡Œåˆ†å‰²ï¼ŒåŒ…æ‹¬é™¤å¤´éƒ¨åˆ†å‰²ä»¥å¤–çš„æ•´ä¸ªäººä½“ï¼Œéšåä½¿ç”¨åœ¨å…¨èº«å§¿æ€æ•°æ®é›†ä¸Šè®­ç»ƒçš„ä¸¤ä¸ªæ¨¡å‹ï¼ˆViTPoseå’ŒYOLOv8æ£€æµ‹å™¨ï¼‰é‡æ–°ä¼°è®¡ç”Ÿæˆå›¾åƒä¸­çš„äººä½“å§¿æ€ï¼Œæœ€åï¼Œä¸ºäº†å°†å‰æ™¯å…ƒç´ ä¸èƒŒæ™¯æ— ç¼é›†æˆï¼Œé¦–å…ˆè°ƒæ•´å¤§å°å¹¶åˆ›å»ºä¸€ä¸ªåŸºæœ¬æ‹¼è´´ï¼Œç„¶åä½¿ç”¨SDXL-inpaintingå°†ç”Ÿæˆçš„å‰æ™¯å…ƒç´ ç»˜åˆ¶åˆ°èƒŒæ™¯ä¸Šï¼Œä¸ºäº†å¤„ç†ä»»æ„å¤§å°çš„èƒŒæ™¯ï¼Œä½¿ç”¨SDXLinpaintingå®ç°è”åˆæ‰©æ•£ï¼›ï¼ˆ2ï¼‰ï¼šå®ä¾‹æ„ŸçŸ¥åˆ†å±‚æ”¾å¤§ï¼šé«˜é¢‘æ³¨å…¥æ­£å‘æ‰©æ•£ï¼šä½¿ç”¨é˜ˆå€¼åˆ†åˆ«ä¸º100å’Œ200çš„Cannyè¾¹ç¼˜æ£€æµ‹ç®—æ³•ï¼Œä½¿ç”¨æ ‡å‡†å·®Ïƒä¸º50çš„é«˜æ–¯æ ¸å¹³æ»‘è¾¹ç¼˜å›¾ï¼Œé€šè¿‡å¯¹æ¨¡ç³Šè¾¹ç¼˜å›¾è¿›è¡Œå½’ä¸€åŒ–å’Œæ¡ä»¶åŒ–æ¥æ„å»ºæ¦‚ç‡å›¾Cï¼Œå®šä¹‰é«˜æ¦‚ç‡é˜ˆå€¼pmaxä¸º0.1ï¼Œä½æ¦‚ç‡é˜ˆå€¼pbaseä¸º0.005ï¼Œä½¿ç”¨Lanczosæ’å€¼è¿›è¡Œå›¾åƒä¸Šé‡‡æ ·ï¼ŒdrandÎ±interpisåˆ†åˆ«è®¾ç½®ä¸º4å’Œ2ï¼Œç”¨äºåŸºäºæ¦‚ç‡å›¾çš„åƒç´ æ‰°åŠ¨ï¼Œæœ€åï¼Œæ­£å‘æ‰©æ•£æ—¶é—´æ­¥Tbisè®¾ç½®ä¸º700ï¼Œæ˜¯SDXLæ¡†æ¶ä¸­ä½¿ç”¨çš„æ€»è®­ç»ƒæ­¥æ•°1000çš„0.7å€ï¼›è‡ªé€‚åº”è”åˆå¤„ç†ï¼šå¯¹äºè‡ªé€‚åº”è”åˆå¤„ç†ï¼Œæ¥æ”¶ç”Ÿæˆçš„å§¿æ€å›¾å’Œé«˜é¢‘æ³¨å…¥å™ªå£°æ½œå˜é‡ä½œä¸ºè¾“å…¥ï¼Œä½¿ç”¨SDXLControlNet-Openposeï¼Œå½“ä½¿ç”¨è‡ªé€‚åº”æ­¥å¹…æ—¶ï¼ŒÎ²overè®¾ç½®ä¸º0.2ï¼ŒèƒŒæ™¯æ­¥å¹…backè®¾ç½®ä¸º64ï¼Œsinstè®¾ç½®ä¸º32ï¼Œå½“ä¸ä½¿ç”¨è‡ªé€‚åº”æ­¥å¹…æ—¶ï¼Œbackå’Œsinstéƒ½è®¾ç½®ä¸º32ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šBeyondScene åœ¨ç”Ÿæˆé«˜åˆ†è¾¨ç‡ã€ä»¥äººä¸ºä¸­å¿ƒä¸”ç»†èŠ‚ä¸°å¯Œã€å¯æ§çš„åœºæ™¯æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ï¼Œè§£å†³äº†ç°æœ‰æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„å±€é™æ€§ï¼Œä¸ºåœ¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹å®¹é‡ä¹‹å¤–åˆ›å»ºæ›´é«˜åˆ†è¾¨ç‡çš„äººä½“ä¸­å¿ƒåœºæ™¯çš„é«˜çº§åº”ç”¨é“ºå¹³äº†é“è·¯ï¼Œè€Œæ— éœ€è¿›è¡Œæ˜‚è´µçš„é‡æ–°è®­ç»ƒã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§åˆ†é˜¶æ®µä¸”åˆ†å±‚çš„æ–¹æ³•ï¼Œé¦–å…ˆç”Ÿæˆä¸€ä¸ªè¯¦ç»†çš„åŸºæœ¬å›¾åƒï¼Œé‡ç‚¹å…³æ³¨å¤šä¸ªäººçš„å®ä¾‹åˆ›å»ºä¸­çš„å…³é”®å…ƒç´ å’Œæ‰©æ•£æ¨¡å‹ä»¤ç‰Œé™åˆ¶ä¹‹å¤–çš„è¯¦ç»†æè¿°ï¼Œç„¶åå°†åŸºæœ¬å›¾åƒæ— ç¼è½¬æ¢ä¸ºæ›´é«˜åˆ†è¾¨ç‡çš„è¾“å‡ºï¼Œè¶…è¿‡è®­ç»ƒå›¾åƒå°ºå¯¸å¹¶é€šè¿‡æˆ‘ä»¬æ–°é¢–çš„å®ä¾‹æ„ŸçŸ¥åˆ†å±‚æ”¾å¤§è¿‡ç¨‹çº³å…¥æ–‡æœ¬å’Œå®ä¾‹æ„ŸçŸ¥çš„ç»†èŠ‚ã€‚</li><li>æå‡ºäº†ä¸€ç§é«˜é¢‘æ³¨å…¥æ­£å‘æ‰©æ•£å’Œè‡ªé€‚åº”è”åˆæ‰©æ•£ï¼Œç”¨äºå®ä¾‹æ„ŸçŸ¥åˆ†å±‚æ”¾å¤§ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å°†ä½åˆ†è¾¨ç‡åŸºæœ¬å›¾åƒæ”¾å¤§åˆ°æ›´é«˜åˆ†è¾¨ç‡ï¼ŒåŒæ—¶ä¿ç•™ç»†èŠ‚å’Œè‡ªç„¶æ€§ã€‚æ€§èƒ½ï¼š</li><li>BeyondScene åœ¨ä¸è¯¦ç»†æ–‡æœ¬æè¿°çš„å¯¹åº”å…³ç³»å’Œè‡ªç„¶æ€§æ–¹é¢è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œåœ¨å„ç§æ•°æ®é›†ä¸Šéƒ½å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å·¥ä½œé‡ï¼š</li><li>BeyondScene çš„å®ç°ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦ä½¿ç”¨å¤šä¸ªé¢„è®­ç»ƒæ¨¡å‹å’Œè‡ªå®šä¹‰è®­ç»ƒè¿‡ç¨‹ï¼Œè¿™å¯èƒ½ä¼šå¢åŠ å·¥ä½œé‡ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-35e73818c7206d5bf11663e3f3a1cf8b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6df01273262f94209f883ec74bc32383.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c6320840444a6fbd77fadf0ed87c258f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a022759deb873c8a9f622ecd7392aeeb.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-04-09  MoMA Multimodal LLM Adapter for Fast Personalized Image Generation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/NeRF/"/>
    <id>https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/NeRF/</id>
    <published>2024-04-06T10:47:58.000Z</published>
    <updated>2024-04-06T10:47:58.786Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-04-06-æ›´æ–°"><a href="#2024-04-06-æ›´æ–°" class="headerlink" title="2024-04-06 æ›´æ–°"></a>2024-04-06 æ›´æ–°</h1><h2 id="RaFE-Generative-Radiance-Fields-Restoration"><a href="#RaFE-Generative-Radiance-Fields-Restoration" class="headerlink" title="RaFE: Generative Radiance Fields Restoration"></a>RaFE: Generative Radiance Fields Restoration</h2><p><strong>Authors:Zhongkai Wu, Ziyu Wan, Jing Zhang, Jing Liao, Dong Xu</strong></p><p>NeRF (Neural Radiance Fields) has demonstrated tremendous potential in novel view synthesis and 3D reconstruction, but its performance is sensitive to input image quality, which struggles to achieve high-fidelity rendering when provided with low-quality sparse input viewpoints. Previous methods for NeRF restoration are tailored for specific degradation type, ignoring the generality of restoration. To overcome this limitation, we propose a generic radiance fields restoration pipeline, named RaFE, which applies to various types of degradations, such as low resolution, blurriness, noise, compression artifacts, or their combinations. Our approach leverages the success of off-the-shelf 2D restoration methods to recover the multi-view images individually. Instead of reconstructing a blurred NeRF by averaging inconsistencies, we introduce a novel approach using Generative Adversarial Networks (GANs) for NeRF generation to better accommodate the geometric and appearance inconsistencies present in the multi-view images. Specifically, we adopt a two-level tri-plane architecture, where the coarse level remains fixed to represent the low-quality NeRF, and a fine-level residual tri-plane to be added to the coarse level is modeled as a distribution with GAN to capture potential variations in restoration. We validate RaFE on both synthetic and real cases for various restoration tasks, demonstrating superior performance in both quantitative and qualitative evaluations, surpassing other 3D restoration methods specific to single task. Please see our project website <a href="https://zkaiwu.github.io/RaFE-Project/">https://zkaiwu.github.io/RaFE-Project/</a>. </p><p><a href="http://arxiv.org/abs/2404.03654v1">PDF</a> Project Page: <a href="https://zkaiwu.github.io/RaFE-Project/">https://zkaiwu.github.io/RaFE-Project/</a></p><p><strong>Summary</strong><br>RaFE æ˜¯ä¸€ç§é€šç”¨å…‰åœºä¿®å¤ç®¡é“ï¼Œå¯ä»¥ä¿®å¤å„ç§ç±»å‹çš„å›¾åƒé€€åŒ–ï¼Œä»è€Œæé«˜ NeRF çš„æ€§èƒ½ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>RaFE é€‚ç”¨äºå„ç§ç±»å‹çš„å›¾åƒé€€åŒ–ï¼ŒåŒ…æ‹¬ä½åˆ†è¾¨ç‡ã€æ¨¡ç³Šã€å™ªå£°å’Œå‹ç¼©ä¼ªå½±ã€‚</li><li>RaFE ä½¿ç”¨ç°æˆçš„ 2D ä¿®å¤æ–¹æ³•å•ç‹¬æ¢å¤å¤šè§†å›¾å›¾åƒã€‚</li><li>RaFE ä½¿ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ (GAN) æ¥ç”Ÿæˆ NeRFï¼Œä»¥æ›´å¥½åœ°é€‚åº”å¤šè§†å›¾å›¾åƒä¸­å­˜åœ¨çš„å‡ ä½•å’Œå¤–è§‚ä¸ä¸€è‡´æ€§ã€‚</li><li>RaFE é‡‡ç”¨äº†ä¸¤çº§ä¸‰å¹³é¢æ¶æ„ï¼Œå…¶ä¸­ç²—ç³™çº§åˆ«ä¿æŒå›ºå®šä»¥è¡¨ç¤ºä½è´¨é‡çš„ NeRFï¼Œå¹¶ä¸”å°†æ·»åŠ åˆ°ç²—ç³™çº§åˆ«çš„ç²¾ç»†çº§åˆ«æ®‹å·®ä¸‰å¹³é¢å»ºæ¨¡ä¸ºå…·æœ‰ GAN çš„åˆ†å¸ƒä»¥æ•è·ä¿®å¤ä¸­çš„æ½œåœ¨å˜åŒ–ã€‚</li><li>RaFE åœ¨åˆæˆå’ŒçœŸå®æ¡ˆä¾‹ä¸­é’ˆå¯¹å„ç§ä¿®å¤ä»»åŠ¡è¿›è¡Œäº†éªŒè¯ï¼Œåœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸­éƒ½è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†é’ˆå¯¹å•ä¸ªä»»åŠ¡çš„å…¶ä»– 3D ä¿®å¤æ–¹æ³•ã€‚</li><li>RaFE çš„é¡¹ç›®ç½‘ç«™ï¼š<a href="https://zkaiwu.github.io/RaFE-Project/ã€‚">https://zkaiwu.github.io/RaFE-Project/ã€‚</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šRaFEï¼šç”Ÿæˆè¾å°„åœºä¿®å¤è¡¥å……ææ–™</li><li>ä½œè€…ï¼šZhongkai Wu, Ziyu Wan, Jing Zhang, Jing Liao, Dong Xu</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šåŒ—äº¬èˆªç©ºèˆªå¤©å¤§å­¦è½¯ä»¶å­¦é™¢</li><li>å…³é”®è¯ï¼šç¥ç»æ¸²æŸ“Â·ç”Ÿæˆæ¨¡å‹Â·3Dä¿®å¤Â·ç¥ç»è¾å°„åœº</li><li>è®ºæ–‡é“¾æ¥ï¼šarxiv.org/abs/2404.03654   Github ä»£ç é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼š   (1): ç ”ç©¶èƒŒæ™¯ï¼šNeRFï¼ˆç¥ç»è¾å°„åœºï¼‰åœ¨ novel view synthesis å’Œ 3D é‡å»ºä¸­è¡¨ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ï¼Œä½†å…¶æ€§èƒ½å¯¹è¾“å…¥å›¾åƒè´¨é‡å¾ˆæ•æ„Ÿï¼Œå½“æä¾›ä½è´¨é‡ç¨€ç–è¾“å…¥è§†ç‚¹æ—¶å¾ˆéš¾å®ç°é«˜ä¿çœŸæ¸²æŸ“ã€‚é’ˆå¯¹ NeRF ä¿®å¤çš„ç°æœ‰æ–¹æ³•é’ˆå¯¹ç‰¹å®šçš„é€€åŒ–ç±»å‹è¿›è¡Œå®šåˆ¶ï¼Œå¿½ç•¥äº†ä¿®å¤çš„é€šç”¨æ€§ã€‚   (2): è¿‡å»çš„æ–¹æ³•ï¼šé’ˆå¯¹ç‰¹å®šé€€åŒ–ç±»å‹è¿›è¡Œå®šåˆ¶ï¼Œå¿½ç•¥äº†ä¿®å¤çš„é€šç”¨æ€§ã€‚   (3): æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæå‡ºäº†ä¸€ç§é€šç”¨çš„è¾å°„åœºä¿®å¤ç®¡é“ RaFEï¼Œé€‚ç”¨äºå„ç§ç±»å‹çš„é€€åŒ–ï¼Œå¦‚ä½åˆ†è¾¨ç‡ã€æ¨¡ç³Šã€å™ªå£°ã€å‹ç¼©ä¼ªå½±æˆ–å®ƒä»¬çš„ç»„åˆã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨ç°æˆçš„ 2D ä¿®å¤æ–¹æ³•åˆ†åˆ«æ¢å¤å¤šè§†å›¾å›¾åƒã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œä½¿ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ (GAN) è¿›è¡Œ NeRF ç”Ÿæˆï¼Œä»¥æ›´å¥½åœ°é€‚åº”å¤šè§†å›¾å›¾åƒä¸­å­˜åœ¨çš„å‡ ä½•å’Œå¤–è§‚ä¸ä¸€è‡´ï¼Œè€Œä¸æ˜¯é€šè¿‡å¹³å‡ä¸ä¸€è‡´æ€§æ¥é‡å»ºæ¨¡ç³Šçš„ NeRFã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸¤çº§ä¸‰å¹³é¢æ¶æ„ï¼Œå…¶ä¸­ç²—ç³™çº§åˆ«ä¿æŒå›ºå®šä»¥è¡¨ç¤ºä½è´¨é‡çš„ NeRFï¼Œå¹¶ä¸”å°†ç»†çº§åˆ«æ®‹å·®ä¸‰å¹³é¢æ·»åŠ åˆ°ç²—ç³™çº§åˆ«å¹¶å»ºæ¨¡ä¸ºå…·æœ‰ GAN çš„åˆ†å¸ƒä»¥æ•è·ä¿®å¤ä¸­çš„æ½œåœ¨å˜åŒ–ã€‚   (4): æ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†ä»€ä¹ˆæ€§èƒ½ï¼šæˆ‘ä»¬åœ¨åˆæˆå’ŒçœŸå®æ¡ˆä¾‹ä¸­å¯¹ RaFE è¿›è¡Œäº†å„ç§ä¿®å¤ä»»åŠ¡çš„éªŒè¯ï¼Œè¯æ˜äº†å…¶åœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸­éƒ½å…·æœ‰ä¼˜å¼‚çš„æ€§èƒ½ï¼Œè¶…è¿‡äº†å…¶ä»–é’ˆå¯¹å•ä¸€ä»»åŠ¡çš„ 3D ä¿®å¤æ–¹æ³•ã€‚æ€§èƒ½æ”¯æŒå…¶ç›®æ ‡ã€‚</p></li><li><p><strong>æ–¹æ³•</strong>ï¼šï¼ˆ1ï¼‰æå‡ºRaFEç®¡é“ï¼Œåˆ©ç”¨ç°æˆ2Dä¿®å¤æ–¹æ³•æ¢å¤å¤šè§†å›¾å›¾åƒï¼Œå¹¶ä½¿ç”¨GANè¿›è¡ŒNeRFç”Ÿæˆä»¥é€‚åº”å‡ ä½•å’Œå¤–è§‚ä¸ä¸€è‡´ï¼›ï¼ˆ2ï¼‰é‡‡ç”¨ä¸¤çº§ä¸‰å¹³é¢æ¶æ„ï¼Œç²—ç³™çº§åˆ«è¡¨ç¤ºä½è´¨é‡NeRFï¼Œç»†çº§åˆ«æ®‹å·®ä¸‰å¹³é¢å»ºæ¨¡ä¸ºå…·æœ‰GANçš„åˆ†å¸ƒï¼Œæ•è·ä¿®å¤ä¸­çš„æ½œåœ¨å˜åŒ–ã€‚</p></li><li><p>ç»“è®ºï¼š(1): æœ¬æ–‡æå‡ºäº† RaFEï¼Œä¸€ç§é€šç”¨çš„è¾å°„åœºä¿®å¤ç®¡é“ï¼Œé€‚ç”¨äºå„ç§ç±»å‹çš„é€€åŒ–ï¼Œåœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸­éƒ½å…·æœ‰ä¼˜å¼‚çš„æ€§èƒ½ã€‚(2): åˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§é€šç”¨çš„è¾å°„åœºä¿®å¤ç®¡é“ï¼Œé€‚ç”¨äºå„ç§ç±»å‹çš„é€€åŒ–ã€‚</li><li>ä½¿ç”¨ GAN è¿›è¡Œ NeRF ç”Ÿæˆä»¥é€‚åº”å¤šè§†å›¾å›¾åƒä¸­å­˜åœ¨çš„å‡ ä½•å’Œå¤–è§‚ä¸ä¸€è‡´ã€‚</li><li>é‡‡ç”¨äº†ä¸¤çº§ä¸‰å¹³é¢æ¶æ„ï¼Œä»¥æ•è·ä¿®å¤ä¸­çš„æ½œåœ¨å˜åŒ–ã€‚æ€§èƒ½ï¼š</li><li>åœ¨åˆæˆå’ŒçœŸå®æ¡ˆä¾‹ä¸­å¯¹ RaFE è¿›è¡Œäº†å„ç§ä¿®å¤ä»»åŠ¡çš„éªŒè¯ï¼Œè¯æ˜äº†å…¶åœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸­éƒ½å…·æœ‰ä¼˜å¼‚çš„æ€§èƒ½ã€‚</li><li>è¶…è¿‡äº†å…¶ä»–é’ˆå¯¹å•ä¸€ä»»åŠ¡çš„ 3D ä¿®å¤æ–¹æ³•ã€‚å·¥ä½œé‡ï¼š</li><li>è®ºæ–‡æ¸…æ™°ç®€æ´ï¼Œæ˜“äºç†è§£ã€‚</li><li>å®éªŒè®¾ç½®å…¨é¢ï¼Œç»“æœå¯ä¿¡ã€‚</li><li>ä»£ç å’Œæ•°æ®å·²å…¬å¼€ï¼Œä¾¿äºå…¶ä»–äººå¤ç°ç»“æœã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-4a0bc8faf250a6fbe548d099582570b0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e8fad6c067ffca0f2b396d38c7e58bbd.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e71c253fdfbf8c1083d326f01390eaeb.jpg" align="middle"></details><h2 id="VF-NeRF-Viewshed-Fields-for-Rigid-NeRF-Registration"><a href="#VF-NeRF-Viewshed-Fields-for-Rigid-NeRF-Registration" class="headerlink" title="VF-NeRF: Viewshed Fields for Rigid NeRF Registration"></a>VF-NeRF: Viewshed Fields for Rigid NeRF Registration</h2><p><strong>Authors:Leo Segre, Shai Avidan</strong></p><p>3D scene registration is a fundamental problem in computer vision that seeks the best 6-DoF alignment between two scenes. This problem was extensively investigated in the case of point clouds and meshes, but there has been relatively limited work regarding Neural Radiance Fields (NeRF). In this paper, we consider the problem of rigid registration between two NeRFs when the position of the original cameras is not given. Our key novelty is the introduction of Viewshed Fields (VF), an implicit function that determines, for each 3D point, how likely it is to be viewed by the original cameras. We demonstrate how VF can help in the various stages of NeRF registration, with an extensive evaluation showing that VF-NeRF achieves SOTA results on various datasets with different capturing approaches such as LLFF and Objaverese. </p><p><a href="http://arxiv.org/abs/2404.03349v1">PDF</a> </p><p><strong>Summary</strong><br>ç¥ç»è¾å°„åœº (NeRF) çš„åˆšæ€§é…å‡†é—®é¢˜ï¼Œå¼•å…¥äº†è§†é‡åœº (VF) ä»¥æé«˜é…å‡†æ€§èƒ½ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>3D åœºæ™¯é…å‡†æ˜¯è®¡ç®—æœºè§†è§‰ä¸­å¯»æ‰¾ä¸¤ä¸ªåœºæ™¯ä¹‹é—´æœ€ä½³ 6 è‡ªç”±åº¦å¯¹é½çš„åŸºæœ¬é—®é¢˜ã€‚</li><li>ç‚¹äº‘å’Œç½‘æ ¼åœºæ™¯é…å‡†å¾—åˆ°äº†å¹¿æ³›ç ”ç©¶ï¼Œä½†å…³äºç¥ç»è¾å°„åœº (NeRF) çš„å·¥ä½œç›¸å¯¹è¾ƒå°‘ã€‚</li><li>è€ƒè™‘äº†åœ¨æœªç»™å®šåŸå§‹ç›¸æœºä½ç½®çš„æƒ…å†µä¸‹ï¼Œä¸¤ä¸ª NeRF ä¹‹é—´çš„åˆšæ€§é…å‡†é—®é¢˜ã€‚</li><li>æå‡ºäº†ä¸€ç§æ–°çš„è§†å›¾åœº (VF) æ¦‚å¿µï¼Œå®ƒæ˜¯ä¸€ç§éšå¼å‡½æ•°ï¼Œç”¨äºç¡®å®šæ¯ä¸ª 3D ç‚¹è¢«åŸå§‹ç›¸æœºè§‚å¯Ÿåˆ°çš„å¯èƒ½æ€§ã€‚</li><li>è¯æ˜äº† VF å¦‚ä½•å¸®åŠ© NeRF é…å‡†çš„å„ä¸ªé˜¶æ®µã€‚</li><li>åœ¨å¹¿æ³›çš„è¯„ä¼°ä¸­è¡¨æ˜ï¼ŒVF-NeRF åœ¨ä½¿ç”¨ LLFF å’Œ Objaverser ç­‰ä¸åŒæ•æ‰æ–¹æ³•çš„ä¸åŒæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„ç»“æœã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šVF-NeRFï¼šåˆšæ€§ NeRF çš„å¯è§†åŸŸåœº</li><li>ä½œè€…ï¼šLeo Segreã€Shai Avidan</li><li>éš¶å±å•ä½ï¼šç‰¹æ‹‰ç»´å¤«å¤§å­¦</li><li>å…³é”®è¯ï¼šç¥ç»è¾å°„åœºã€3D é…å‡†ã€å½’ä¸€åŒ–æµ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://leosegre.github.io/VF_NeRF/   Github ä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼š   (1)ï¼šç ”ç©¶èƒŒæ™¯ï¼š3D åœºæ™¯é…å‡†æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€ä¸ªåŸºæœ¬é—®é¢˜ï¼Œæ—¨åœ¨å¯»æ‰¾ä¸¤ä¸ªåœºæ™¯ä¹‹é—´çš„æœ€ä½³ 6 è‡ªç”±åº¦å¯¹é½ã€‚è¯¥é—®é¢˜å·²åœ¨ç‚¹äº‘å’Œç½‘æ ¼çš„æƒ…å†µä¸‹å¾—åˆ°å¹¿æ³›ç ”ç©¶ï¼Œä½†å…³äºç¥ç»è¾å°„åœº (NeRF) çš„å·¥ä½œç›¸å¯¹è¾ƒå°‘ã€‚   (2)ï¼šè¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šå½“åŸå§‹æ‘„åƒæœºçš„ä½ç½®æœªçŸ¥æ—¶ï¼Œè¿‡å»çš„æ–¹æ³•åœ¨ä¸¤ä¸ª NeRF ä¹‹é—´è¿›è¡Œåˆšæ€§é…å‡†æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚   (3)ï¼šæœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç§°ä¸ºå¯è§†åŸŸåœº (VF) çš„éšå¼å‡½æ•°ï¼Œè¯¥å‡½æ•°ç¡®å®šæ¯ä¸ª 3D ç‚¹è¢«åŸå§‹ç›¸æœºè§‚å¯Ÿåˆ°çš„å¯èƒ½æ€§ã€‚VF-NeRF åˆ©ç”¨ VF è¾…åŠ© NeRF é…å‡†çš„å„ä¸ªé˜¶æ®µã€‚   (4)ï¼šæ–¹æ³•åœ¨ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼šVF-NeRF åœ¨ä½¿ç”¨ä¸åŒæ•è·æ–¹æ³•ï¼ˆå¦‚ LLFF å’Œ Objaverseï¼‰çš„å„ç§æ•°æ®é›†ä¸Šå®ç°äº† SOTA ç»“æœï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</li></ol><p>7.Methodsï¼šï¼ˆ1ï¼‰ä½¿ç”¨Viewshed Fieldï¼ˆVFï¼‰ç”Ÿæˆåœºæ™¯Aä¸­å¤šä¸ªè‰¯å¥½çš„ç›¸æœºè§†è§’é›†åˆCAï¼›ï¼ˆ2ï¼‰åˆ©ç”¨åœºæ™¯Bçš„VFåˆ¤æ–­ç»è¿‡å˜æ¢Tçš„CAä¸­ç›¸æœºè§‚å¯Ÿåœºæ™¯Bä¸­è‰¯å¥½ç‚¹çš„ç¨‹åº¦ï¼Œè®¡ç®—å˜æ¢Tçš„åˆå§‹åŒ–å¾—åˆ†ï¼›ï¼ˆ3ï¼‰éšæœºé‡‡æ ·å¤šä¸ªå˜æ¢Tï¼Œé€‰æ‹©å¾—åˆ†æœ€é«˜çš„ä½œä¸ºåˆå§‹åŒ–ï¼›ï¼ˆ4ï¼‰ä»NeRFæ½œåœ¨åˆ†å¸ƒä¸­é‡‡æ ·ç‚¹ï¼Œç”Ÿæˆå®šå‘ç‚¹ï¼Œå¹¶ä½¿ç”¨NeRFè·å–å¯¹åº”çš„å¯†åº¦å’ŒRGBï¼›ï¼ˆ5ï¼‰åˆ©ç”¨å¯†åº¦å€¼å’Œé˜ˆå€¼æ»¤å‡ºä¸ç¡®å®šçš„ç‚¹ï¼Œç”Ÿæˆç‚¹äº‘ï¼›ï¼ˆ6ï¼‰ä½¿ç”¨å·²æœ‰çš„ç‚¹äº‘å…¨å±€é…å‡†æ–¹æ³•ï¼Œå¾—åˆ°åˆå§‹çŒœæµ‹ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬æ–‡æå‡ºäº†VF-NeRFï¼Œä¸€ç§ç”¨äºåˆšæ€§NeRFé…å‡†çš„éšå¼å‡½æ•°ï¼Œè¯¥å‡½æ•°ç¡®å®šæ¯ä¸ª3Dç‚¹è¢«åŸå§‹ç›¸æœºè§‚å¯Ÿåˆ°çš„å¯èƒ½æ€§ã€‚VF-NeRFåˆ©ç”¨VFè¾…åŠ©NeRFé…å‡†çš„å„ä¸ªé˜¶æ®µï¼Œåœ¨ä½¿ç”¨ä¸åŒæ•è·æ–¹æ³•ï¼ˆå¦‚LLFFå’ŒObjaverseï¼‰çš„å„ç§æ•°æ®é›†ä¸Šå®ç°äº†SOTAç»“æœï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§ç§°ä¸ºå¯è§†åŸŸåœº(VF)çš„éšå¼å‡½æ•°ï¼Œè¯¥å‡½æ•°ç¡®å®šæ¯ä¸ª3Dç‚¹è¢«åŸå§‹ç›¸æœºè§‚å¯Ÿåˆ°çš„å¯èƒ½æ€§ã€‚</li><li>å°†VFä¸å½’ä¸€åŒ–æµï¼ˆNFï¼‰ç›¸ç»“åˆï¼Œç”¨äºé‡‡æ ·æ–°é¢–çš„ç›¸æœºè§†ç‚¹å’Œç”Ÿæˆæœ‰è‰²çš„3Dç‚¹äº‘ã€‚</li><li>åˆ©ç”¨VFæŒ‡å¯¼å…‰çº¿é‡‡æ ·ï¼Œä¼˜åŒ–NeRFé…å‡†ã€‚</li><li>æ€§èƒ½ï¼š</li><li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†SOTAç»“æœï¼ŒåŒ…æ‹¬æ­£é¢åœºæ™¯ã€ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„è§†é¢‘å’Œåˆæˆå¯¹è±¡å›¾åƒã€‚</li><li>åœ¨å…·æœ‰æœ€å°é…å‡†è¯¯å·®çš„å™ªå£°è®¾ç½®ä¸­ï¼Œä¸COLMAPçš„è¯¯å·®å’Œå…‰åº¦è¯¯å·®çš„ä¼˜åŠ£éš¾ä»¥åŒºåˆ†ã€‚</li><li>å·¥ä½œé‡ï¼š</li><li>ä½¿ç”¨Nerfactoä½œä¸ºNeRFè¡¨ç¤ºï¼Œæ¯æ‰¹æ¬¡é‡‡æ ·1024æ¡å…‰çº¿ï¼Œä½¿ç”¨Adamä¼˜åŒ–å™¨è¿›è¡Œè®­ç»ƒï¼Œåˆå§‹å­¦ä¹ ç‡ä¸º1e-2ï¼ŒæŒ‡æ•°è¡°å‡ã€‚</li><li>ä½¿ç”¨å…·æœ‰L=4å±‚å’ŒH=128éšè—ç»´åº¦çš„Real-NVPå­¦ä¹ VFï¼Œä½¿ç”¨RAdamä¼˜åŒ–å™¨ï¼Œæ’å®šå­¦ä¹ ç‡ä¸º5e-5ã€‚</li><li>å®é™…åœºæ™¯NeRFè®­ç»ƒ60Kæ¬¡è¿­ä»£ï¼ŒVFè®­ç»ƒåœ¨æœ€å10Kæ¬¡è¿­ä»£ä¸­å¯ç”¨ã€‚</li><li>åˆæˆåœºæ™¯NeRFè®­ç»ƒ20Kæ¬¡è¿­ä»£ï¼ŒVFè®­ç»ƒåœ¨æœ€å5Kæ¬¡è¿­ä»£ä¸­å¯ç”¨ï¼Œå¹¶åœ¨å›¾åƒé€æ˜ï¼ˆRGBAå›¾åƒçš„Î±&lt;128ï¼‰æ—¶å¿½ç•¥ã€‚</li><li>å…‰åº¦åˆå§‹åŒ–åœ¨25ä¸ªéšæœºå˜æ¢ä¸Šå®Œæˆã€‚</li><li>å¯¹äºPCåˆå§‹åŒ–ï¼Œé¦–å…ˆä»VFåˆ†å¸ƒä¸­é‡‡æ ·100Kä¸ªç‚¹ç”Ÿæˆç‚¹äº‘ï¼Œé€‰æ‹©å¯†åº¦é«˜äº10çš„ç‚¹ï¼Œå¹¶å°†è¿™äº›ç‚¹äº‘ä½œä¸ºç»å…¸å…¨å±€é…å‡†æ–¹æ³•çš„è¾“å…¥ã€‚</li><li>åœ¨é…å‡†é˜¶æ®µï¼Œå¯¹äºå®é™…åœºæ™¯ï¼Œä½¿ç”¨SGDä¼˜åŒ–å™¨å¯¹6DoFå‚æ•°è¿›è¡Œ15Kæ¬¡è¿­ä»£ä¼˜åŒ–ï¼Œæ¯æ¬¡è¿­ä»£32Kä¸ªæ ·æœ¬ï¼Œåˆå§‹å­¦ä¹ ç‡ä¸º5e-3ï¼ŒæŒ‡æ•°è¡°å‡ã€‚</li><li>å¯¹äºåˆæˆåœºæ™¯ï¼Œä½¿ç”¨SGDä¼˜åŒ–å™¨å¯¹6DoFå‚æ•°è¿›è¡Œ2.5Kæ¬¡è¿­ä»£ä¼˜åŒ–ï¼Œæ¯æ¬¡è¿­ä»£8128ä¸ªæ ·æœ¬ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-c42dc03989b870facba1e92f9650d148.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5036daad3cd46832226594b54b75df78.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fcba1449fcbdf5cb3bf62129225960c5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4a10f7f3b4aaec1b94ed587220378c6b.jpg" align="middle"></details><h2 id="LiDAR4D-Dynamic-Neural-Fields-for-Novel-Space-time-View-LiDAR-Synthesis"><a href="#LiDAR4D-Dynamic-Neural-Fields-for-Novel-Space-time-View-LiDAR-Synthesis" class="headerlink" title="LiDAR4D: Dynamic Neural Fields for Novel Space-time View LiDAR Synthesis"></a>LiDAR4D: Dynamic Neural Fields for Novel Space-time View LiDAR Synthesis</h2><p><strong>Authors:Zehan Zheng, Fan Lu, Weiyi Xue, Guang Chen, Changjun Jiang</strong></p><p>Although neural radiance fields (NeRFs) have achieved triumphs in image novel view synthesis (NVS), LiDAR NVS remains largely unexplored. Previous LiDAR NVS methods employ a simple shift from image NVS methods while ignoring the dynamic nature and the large-scale reconstruction problem of LiDAR point clouds. In light of this, we propose LiDAR4D, a differentiable LiDAR-only framework for novel space-time LiDAR view synthesis. In consideration of the sparsity and large-scale characteristics, we design a 4D hybrid representation combined with multi-planar and grid features to achieve effective reconstruction in a coarse-to-fine manner. Furthermore, we introduce geometric constraints derived from point clouds to improve temporal consistency. For the realistic synthesis of LiDAR point clouds, we incorporate the global optimization of ray-drop probability to preserve cross-region patterns. Extensive experiments on KITTI-360 and NuScenes datasets demonstrate the superiority of our method in accomplishing geometry-aware and time-consistent dynamic reconstruction. Codes are available at <a href="https://github.com/ispc-lab/LiDAR4D">https://github.com/ispc-lab/LiDAR4D</a>. </p><p><a href="http://arxiv.org/abs/2404.02742v1">PDF</a> Accepted by CVPR 2024. Project Page:   <a href="https://dyfcalid.github.io/LiDAR4D">https://dyfcalid.github.io/LiDAR4D</a></p><p><strong>Summary</strong><br> æ¿€å…‰é›·è¾¾ä¸“å±çš„å¯å¾®ç¥ç»è¾å°„åœºæ¡†æ¶ï¼Œå®ç°å¯ä¿¡ã€æ—¶é—´ä¸€è‡´çš„åŠ¨æ€é‡å»ºã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºäº†é¦–ä¸ªæ¿€å…‰é›·è¾¾ç¥ç»è¾å°„åœºï¼ˆLiDAR NeRFï¼‰ï¼Œç”¨äºæ¿€å…‰é›·è¾¾æ–°è§†ç‚¹åˆæˆã€‚</li><li>è®¾è®¡äº†ä¸€ç§ 4D æ··åˆè¡¨ç¤ºï¼Œç»“åˆäº†å¤šå¹³é¢å’Œç½‘æ ¼ç‰¹å¾ï¼Œä»¥æœ‰æ•ˆé‡å»ºå¤§è§„æ¨¡æ¿€å…‰é›·è¾¾ç‚¹äº‘ã€‚</li><li>å¼•å…¥äº†æºè‡ªç‚¹äº‘çš„å‡ ä½•çº¦æŸï¼Œå¢å¼ºäº†æ—¶é—´ä¸€è‡´æ€§ã€‚</li><li>é›†æˆäº†å°„çº¿æŠ•å°„æ¦‚ç‡çš„å…¨å±€ä¼˜åŒ–ï¼Œä»¥ä¿ç•™è·¨åŒºåŸŸæ¨¡å¼ï¼Œå®ç°æ¿€å…‰é›·è¾¾ç‚¹äº‘çš„çœŸå®åˆæˆã€‚</li><li>åœ¨ KITTI-360 å’Œ NuScenes æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜äº†è¯¥æ–¹æ³•åœ¨å®ç°æ„ŸçŸ¥å‡ ä½•å’Œæ—¶é—´ä¸€è‡´åŠ¨æ€é‡å»ºæ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚</li><li>å·²å¼€æºä»£ç ï¼š<a href="https://github.com/ispc-lab/LiDAR4Dã€‚">https://github.com/ispc-lab/LiDAR4Dã€‚</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šLiDAR4Dï¼šç”¨äºæ–°å‹æ—¶ç©ºè§†å›¾ LiDAR åˆæˆçš„åŠ¨æ€ç¥ç»åœº</li><li>ä½œè€…ï¼šHongrui Zhou, Xiaoguang Han, Yulan Guo, Qiang Zhang, Hao Li, Wenping Wang</li><li>æ‰€å±æœºæ„ï¼šä¸­å›½ç§‘å­¦é™¢å¤§å­¦è®¡ç®—æœºå­¦é™¢</li><li>å…³é”®è¯ï¼šLiDAR ç‚¹äº‘ã€ç¥ç»è¾å°„åœºã€æ—¶ç©ºè§†å›¾åˆæˆã€åŠ¨æ€é‡å»º</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.03988Github ä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœº (NeRF) åœ¨å›¾åƒæ–°è§†å›¾åˆæˆ (NVS) ä¸­å–å¾—äº†æˆåŠŸï¼Œä½† LiDAR NVS ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ç°æœ‰çš„ LiDAR NVS æ–¹æ³•ç®€å•åœ°ä»å›¾åƒ NVS æ–¹æ³•è½¬ç§»ï¼Œè€Œå¿½ç•¥äº† LiDAR ç‚¹äº‘çš„åŠ¨æ€ç‰¹æ€§å’Œå¤§è§„æ¨¡é‡å»ºé—®é¢˜ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç°æœ‰æ–¹æ³•å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š</li><li>å¿½ç•¥äº† LiDAR ç‚¹äº‘çš„åŠ¨æ€ç‰¹æ€§ï¼Œå¯¼è‡´åŠ¨æ€ç‰©ä½“å‡ºç°ä¼ªå½±å’Œå™ªå£°ã€‚</li><li>ç¼ºä¹å¯¹å¤§è§„æ¨¡åœºæ™¯ä¸­ç»†èŠ‚çš„é‡å»ºèƒ½åŠ›ã€‚</li><li>æ— æ³•å»ºç«‹è¿œè·ç¦»å¯¹åº”å…³ç³»ã€‚ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº† LiDAR4Dï¼Œè¿™æ˜¯ä¸€ä¸ªå¯å¾®çš„ä»…é™ LiDAR çš„æ–°æ—¶ç©º LiDAR è§†å›¾åˆæˆæ¡†æ¶ã€‚è¯¥æ¡†æ¶åŒ…å«ä»¥ä¸‹åˆ›æ–°ï¼š</li><li>è®¾è®¡äº†ä¸€ç§ 4D æ··åˆè¡¨ç¤ºï¼Œç»“åˆäº†å¤šå¹³é¢å’Œç½‘æ ¼ç‰¹å¾ï¼Œä»¥ç²—åˆ°ç»†çš„æ–¹å¼è¿›è¡Œæœ‰æ•ˆé‡å»ºã€‚</li><li>å¼•å…¥äº†ä»ç‚¹äº‘æ´¾ç”Ÿçš„å‡ ä½•çº¦æŸï¼Œä»¥æé«˜æ—¶é—´ä¸€è‡´æ€§ã€‚</li><li>é’ˆå¯¹ LiDAR ç‚¹äº‘çš„çœŸå®åˆæˆï¼Œå¼•å…¥äº†å°„çº¿æ‰è½æ¦‚ç‡çš„å…¨å±€ä¼˜åŒ–ï¼Œä»¥ä¿ç•™è·¨åŒºåŸŸæ¨¡å¼ã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šå–å¾—çš„æˆå°±ï¼šåœ¨ KITTI-360 å’Œ NuScenes æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å®ç°å‡ ä½•æ„ŸçŸ¥å’Œæ—¶é—´ä¸€è‡´çš„åŠ¨æ€é‡å»ºæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å…·ä½“æ€§èƒ½å¦‚ä¸‹ï¼š</li><li>åœ¨ KITTI-360 æ•°æ®é›†ä¸Šï¼Œåœ¨å‡ ä½•å’Œå¼ºåº¦ RMSE æŒ‡æ ‡ä¸Šåˆ†åˆ«æ¯” LiDAR-NeRF é™ä½äº† 12.0% å’Œ 13.7%ã€‚</li><li>åœ¨ NuScenes æ•°æ®é›†ä¸Šï¼Œåœ¨å‡ ä½•å’Œå¼ºåº¦ RMSE æŒ‡æ ‡ä¸Šåˆ†åˆ«æ¯” LiDAR-NeRF é™ä½äº† 11.6% å’Œ 13.5%ã€‚</li></ol><p><strong>æ–¹æ³•</strong></p><p>ï¼ˆ1ï¼‰4Dæ··åˆå¹³é¢æ ¼è¡¨ç¤ºï¼šé‡‡ç”¨å¤šå¹³é¢å’Œç½‘æ ¼ç‰¹å¾ç›¸ç»“åˆçš„4Dæ··åˆè¡¨ç¤ºï¼Œä»¥ç²—åˆ°ç»†çš„æ–¹å¼è¿›è¡Œæœ‰æ•ˆé‡å»ºã€‚</p><p>ï¼ˆ2ï¼‰åœºæ™¯æµå…ˆéªŒï¼šå¼•å…¥ä»ç‚¹äº‘æ´¾ç”Ÿçš„åœºæ™¯æµå…ˆéªŒï¼Œä»¥æé«˜æ—¶é—´ä¸€è‡´æ€§ã€‚</p><p>ï¼ˆ3ï¼‰ç¥ç»LiDARåœºï¼šå»ºç«‹åŸºäºLiDARçš„ç¥ç»åœºï¼Œé¢„æµ‹æ·±åº¦ã€å¼ºåº¦å’Œå°„çº¿æ‰è½æ¦‚ç‡ã€‚</p><p>ï¼ˆ4ï¼‰å°„çº¿æ‰è½æ¦‚ç‡ä¼˜åŒ–ï¼šå¼•å…¥å°„çº¿æ‰è½æ¦‚ç‡çš„å…¨å±€ä¼˜åŒ–ï¼Œä»¥ä¿ç•™è·¨åŒºåŸŸæ¨¡å¼ï¼Œæé«˜ç”ŸæˆçœŸå®æ€§ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡é’ˆå¯¹ç°æœ‰ LiDAR NVS æ–¹æ³•çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶æ¥è§£å†³åŠ¨æ€é‡å»ºã€å¤§è§„æ¨¡åœºæ™¯è¡¨å¾å’ŒçœŸå®åˆæˆè¿™ä¸‰ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚æå‡ºçš„æ–¹æ³• LiDAR4D åœ¨å¹¿æ³›çš„å®éªŒä¸­è¯æ˜äº†å…¶ä¼˜è¶Šæ€§ï¼Œå®ç°äº†å¤§è§„æ¨¡åŠ¨æ€ç‚¹äº‘åœºæ™¯çš„å‡ ä½•æ„ŸçŸ¥å’Œæ—¶é—´ä¸€è‡´é‡å»ºï¼Œå¹¶ç”Ÿæˆäº†æ›´æ¥è¿‘çœŸå®åˆ†å¸ƒçš„æ–°æ—¶ç©ºè§†å›¾ LiDAR ç‚¹äº‘ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œæœªæ¥çš„å·¥ä½œå°†æ›´å¤šåœ°é›†ä¸­åœ¨å°† LiDAR ç‚¹äº‘ä¸ç¥ç»è¾å°„åœºç›¸ç»“åˆï¼Œå¹¶æ¢ç´¢åŠ¨æ€åœºæ™¯é‡å»ºå’Œåˆæˆçš„æ›´å¤šå¯èƒ½æ€§ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§ 4D æ··åˆå¹³é¢æ ¼è¡¨ç¤ºï¼Œç»“åˆäº†å¤šå¹³é¢å’Œç½‘æ ¼ç‰¹å¾ï¼Œä»¥ç²—åˆ°ç»†çš„æ–¹å¼è¿›è¡Œæœ‰æ•ˆé‡å»ºã€‚</li><li>å¼•å…¥äº†ä»ç‚¹äº‘æ´¾ç”Ÿçš„åœºæ™¯æµå…ˆéªŒï¼Œä»¥æé«˜æ—¶é—´ä¸€è‡´æ€§ã€‚</li><li>å»ºç«‹äº†åŸºäº LiDAR çš„ç¥ç»åœºï¼Œé¢„æµ‹æ·±åº¦ã€å¼ºåº¦å’Œå°„çº¿æ‰è½æ¦‚ç‡ã€‚</li><li>å¼•å…¥äº†å°„çº¿æ‰è½æ¦‚ç‡çš„å…¨å±€ä¼˜åŒ–ï¼Œä»¥ä¿ç•™è·¨åŒºåŸŸæ¨¡å¼ï¼Œæé«˜ç”ŸæˆçœŸå®æ€§ã€‚æ€§èƒ½ï¼š</li><li>åœ¨ KITTI-360 æ•°æ®é›†ä¸Šï¼Œåœ¨å‡ ä½•å’Œå¼ºåº¦ RMSE æŒ‡æ ‡ä¸Šåˆ†åˆ«æ¯” LiDAR-NeRF é™ä½äº† 12.0% å’Œ 13.7%ã€‚</li><li>åœ¨ NuScenes æ•°æ®é›†ä¸Šï¼Œåœ¨å‡ ä½•å’Œå¼ºåº¦ RMSE æŒ‡æ ‡ä¸Šåˆ†åˆ«æ¯” LiDAR-NeRF é™ä½äº† 11.6% å’Œ 13.5%ã€‚å·¥ä½œé‡ï¼š</li><li>æå‡ºäº†ä¸€ç§æ–°çš„æ—¶ç©º LiDAR è§†å›¾åˆæˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶è§£å†³äº†åŠ¨æ€é‡å»ºã€å¤§è§„æ¨¡åœºæ™¯è¡¨å¾å’ŒçœŸå®åˆæˆè¿™ä¸‰ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚</li><li>åœ¨ KITTI-360 å’Œ NuScenes æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚</li><li>å¼€æºäº†ä»£ç ï¼Œä¾¿äºå…¶ä»–ç ”ç©¶äººå‘˜è¿›è¡Œç ”ç©¶å’Œåº”ç”¨ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-2963b70a266c3a04d92a7dbee2c86759.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7a65da90b3848baf2adb2e8ce440176c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4fd1d5df12dbb5393c4e1c3591fe5d11.jpg" align="middle"><img src="https://pica.zhimg.com/v2-4f24d8c17a6447cf6c6bff2640772e2c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f2d050ccfba4add3a017bb850515949a.jpg" align="middle"></details><h2 id="Freditor-High-Fidelity-and-Transferable-NeRF-Editing-by-Frequency-Decomposition"><a href="#Freditor-High-Fidelity-and-Transferable-NeRF-Editing-by-Frequency-Decomposition" class="headerlink" title="Freditor: High-Fidelity and Transferable NeRF Editing by Frequency   Decomposition"></a>Freditor: High-Fidelity and Transferable NeRF Editing by Frequency   Decomposition</h2><p><strong>Authors:Yisheng He, Weihao Yuan, Siyu Zhu, Zilong Dong, Liefeng Bo, Qixing Huang</strong></p><p>This paper enables high-fidelity, transferable NeRF editing by frequency decomposition. Recent NeRF editing pipelines lift 2D stylization results to 3D scenes while suffering from blurry results, and fail to capture detailed structures caused by the inconsistency between 2D editings. Our critical insight is that low-frequency components of images are more multiview-consistent after editing compared with their high-frequency parts. Moreover, the appearance style is mainly exhibited on the low-frequency components, and the content details especially reside in high-frequency parts. This motivates us to perform editing on low-frequency components, which results in high-fidelity edited scenes. In addition, the editing is performed in the low-frequency feature space, enabling stable intensity control and novel scene transfer. Comprehensive experiments conducted on photorealistic datasets demonstrate the superior performance of high-fidelity and transferable NeRF editing. The project page is at \url{<a href="https://aigc3d.github.io/freditor}">https://aigc3d.github.io/freditor}</a>. </p><p><a href="http://arxiv.org/abs/2404.02514v1">PDF</a> </p><p><strong>Summary</strong><br>ä½é¢‘ç‰¹å¾ç©ºé—´ç¼–è¾‘æé«˜NeRFå¯ç¼–è¾‘æ€§ï¼Œå¸¦æ¥é«˜ä¿çœŸå¯è¿ç§»çš„NeRFç¼–è¾‘ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>å›¾åƒç¼–è¾‘åï¼Œä½é¢‘åˆ†é‡è·¨è§†è§’ä¸€è‡´æ€§æ›´é«˜ã€‚</li><li>å¤–è§‚é£æ ¼ä¸»è¦ä½“ç°åœ¨ä½é¢‘åˆ†é‡ä¸Šï¼Œå†…å®¹ç»†èŠ‚ä¸»è¦ä½äºé«˜é¢‘åˆ†é‡ä¸Šã€‚</li><li>åœ¨ä½é¢‘åˆ†é‡ä¸Šè¿›è¡Œç¼–è¾‘å¯äº§ç”Ÿé«˜ä¿çœŸç¼–è¾‘åœºæ™¯ã€‚</li><li>ä½é¢‘ç‰¹å¾ç©ºé—´ä¸­çš„ç¼–è¾‘å¯å®ç°ç¨³å®šçš„å¼ºåº¦æ§åˆ¶å’Œæ–°åœºæ™¯è¿ç§»ã€‚</li><li>å®éªŒè¡¨æ˜ï¼Œé«˜ä¿çœŸå¯è¿ç§»çš„NeRFç¼–è¾‘å…·æœ‰å‡ºè‰²æ€§èƒ½ã€‚</li><li>é¡¹ç›®ä¸»é¡µï¼š<a href="https://aigc3d.github.io/freditorã€‚">https://aigc3d.github.io/freditorã€‚</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šé¢‘ç‡åˆ†è§£çš„é«˜ä¿çœŸå¯è¿ç§» NeRF ç¼–è¾‘</li><li>ä½œè€…ï¼šYisheng He, Weihao Yuan, Siyu Zhu, Zilong Dong, Liefeng Bo, Qixing Huang</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šé˜¿é‡Œå·´å·´é›†å›¢</li><li>å…³é”®è¯ï¼šNeRFã€ç¼–è¾‘ã€é¢‘ç‡åˆ†è§£ã€é«˜ä¿çœŸã€å¯è¿ç§»</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.02514   Github ä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼š   (1)ï¼šç ”ç©¶èƒŒæ™¯ï¼šNeRF ç¼–è¾‘ç®¡é“å°† 2D é£æ ¼åŒ–ç»“æœæå‡åˆ° 3D åœºæ™¯ï¼Œä½†å­˜åœ¨ç»“æœæ¨¡ç³Šçš„é—®é¢˜ï¼Œå¹¶ä¸”ç”±äº 2D ç¼–è¾‘çš„ä¸ä¸€è‡´æ€§è€Œæ— æ³•æ•æ‰åˆ°è¯¦ç»†çš„ç»“æ„ã€‚   (2)ï¼šè¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šç°æœ‰æ–¹æ³•å­˜åœ¨çš„é—®é¢˜åœ¨äºï¼Œç¼–è¾‘åçš„å›¾åƒçš„ä½é¢‘åˆ†é‡æ¯”é«˜é¢‘éƒ¨åˆ†æ›´å…·å¤šè§†å›¾ä¸€è‡´æ€§ã€‚è€Œä¸”ï¼Œå¤–è§‚é£æ ¼ä¸»è¦ä½“ç°åœ¨ä½é¢‘åˆ†é‡ä¸Šï¼Œè€Œå†…å®¹ç»†èŠ‚åˆ™ä¸»è¦å­˜åœ¨äºé«˜é¢‘éƒ¨åˆ†ã€‚   (3)ï¼šç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡é¢‘ç‡åˆ†è§£è¿›è¡Œ NeRF ç¼–è¾‘çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•åœ¨ä½é¢‘åˆ†é‡ä¸Šè¿›è¡Œç¼–è¾‘ï¼Œä»è€Œäº§ç”Ÿé«˜ä¿çœŸç¼–è¾‘åœºæ™¯ã€‚   (4)ï¼šæ–¹æ³•æ€§èƒ½ï¼šè¯¥æ–¹æ³•åœ¨åœºæ™¯ç¼–è¾‘å’Œå¯è¿ç§»ç¼–è¾‘ä»»åŠ¡ä¸Šå–å¾—äº†è‰¯å¥½çš„æ€§èƒ½ã€‚åœ¨åœºæ™¯ç¼–è¾‘ä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•å¯ä»¥ç”Ÿæˆé«˜ä¿çœŸç¼–è¾‘åœºæ™¯ï¼Œå¹¶ä¸”å¯ä»¥æ•æ‰åˆ°è¯¦ç»†çš„ç»“æ„ã€‚åœ¨å¯è¿ç§»ç¼–è¾‘ä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•å¯ä»¥å°†åœ¨ä¸€ä¸ªåœºæ™¯ä¸­è®­ç»ƒçš„ç¼–è¾‘æ¨¡å‹ç›´æ¥è¿ç§»åˆ°ä¸åŒçš„æ–°åœºæ™¯ä¸­ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡æå‡ºçš„æ–¹æ³•çš„ç›®æ ‡ã€‚</li></ol><p>7.æ–¹æ³•ï¼š(1)ï¼šé¢‘ç‡åˆ†è§£é«˜ä¿çœŸå¯è¿ç§»NeRFç¼–è¾‘æ–¹æ³•é€šè¿‡é¢‘ç‡åˆ†è§£å¯¹NeRFè¿›è¡Œç¼–è¾‘ï¼Œä»¥äº§ç”Ÿé«˜ä¿çœŸç¼–è¾‘åœºæ™¯ã€‚(2)ï¼šè¯¥æ–¹æ³•åœ¨ä½é¢‘åˆ†é‡ä¸Šè¿›è¡Œç¼–è¾‘ï¼Œä»è€Œäº§ç”Ÿé«˜ä¿çœŸç¼–è¾‘åœºæ™¯ï¼Œå¹¶ä¸”å¯ä»¥æ•æ‰åˆ°è¯¦ç»†çš„ç»“æ„ã€‚(3)ï¼šè¯¥æ–¹æ³•åœ¨åœºæ™¯ç¼–è¾‘å’Œå¯è¿ç§»ç¼–è¾‘ä»»åŠ¡ä¸Šå–å¾—äº†è‰¯å¥½çš„æ€§èƒ½ã€‚</p><ol><li>ç»“è®ºï¼š(1): æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§é€šè¿‡é¢‘ç‡åˆ†è§£è¿›è¡Œ NeRF ç¼–è¾‘çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨ä½é¢‘åˆ†é‡ä¸Šè¿›è¡Œç¼–è¾‘ï¼Œä»è€Œäº§ç”Ÿé«˜ä¿çœŸç¼–è¾‘åœºæ™¯ï¼Œå¹¶ä¸”å¯ä»¥æ•æ‰åˆ°è¯¦ç»†çš„ç»“æ„ã€‚(2): åˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§é€šè¿‡é¢‘ç‡åˆ†è§£è¿›è¡Œ NeRF ç¼–è¾‘çš„æ–¹æ³•ã€‚</li><li>è¯¥æ–¹æ³•åœ¨ä½é¢‘åˆ†é‡ä¸Šè¿›è¡Œç¼–è¾‘ï¼Œä»è€Œäº§ç”Ÿé«˜ä¿çœŸç¼–è¾‘åœºæ™¯ï¼Œå¹¶ä¸”å¯ä»¥æ•æ‰åˆ°è¯¦ç»†çš„ç»“æ„ã€‚</li><li>è¯¥æ–¹æ³•åœ¨åœºæ™¯ç¼–è¾‘å’Œå¯è¿ç§»ç¼–è¾‘ä»»åŠ¡ä¸Šå–å¾—äº†è‰¯å¥½çš„æ€§èƒ½ã€‚æ€§èƒ½ï¼š</li><li>åœ¨åœºæ™¯ç¼–è¾‘ä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•å¯ä»¥ç”Ÿæˆé«˜ä¿çœŸç¼–è¾‘åœºæ™¯ï¼Œå¹¶ä¸”å¯ä»¥æ•æ‰åˆ°è¯¦ç»†çš„ç»“æ„ã€‚</li><li>åœ¨å¯è¿ç§»ç¼–è¾‘ä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•å¯ä»¥å°†åœ¨ä¸€ä¸ªåœºæ™¯ä¸­è®­ç»ƒçš„ç¼–è¾‘æ¨¡å‹ç›´æ¥è¿ç§»åˆ°ä¸åŒçš„æ–°åœºæ™¯ä¸­ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚å·¥ä½œé‡ï¼š</li><li>è¯¥æ–¹æ³•éœ€è¦å¯¹ NeRF è¿›è¡Œé¢‘ç‡åˆ†è§£ï¼Œè¿™å¯èƒ½ä¼šå¢åŠ è®¡ç®—æˆæœ¬ã€‚</li><li>è¯¥æ–¹æ³•éœ€è¦åœ¨ä½é¢‘åˆ†é‡ä¸Šè¿›è¡Œç¼–è¾‘ï¼Œè¿™å¯èƒ½ä¼šå¢åŠ ç¼–è¾‘éš¾åº¦ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-fb6df696389c18849d0142f7f9834863.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e82d2e193f21cda63cdb16a49b96fb83.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-bed27b82ba84f05629b001f77ba3c8b1.jpg" align="middle"></details><h2 id="NeRFCodec-Neural-Feature-Compression-Meets-Neural-Radiance-Fields-for-Memory-Efficient-Scene-Representation"><a href="#NeRFCodec-Neural-Feature-Compression-Meets-Neural-Radiance-Fields-for-Memory-Efficient-Scene-Representation" class="headerlink" title="NeRFCodec: Neural Feature Compression Meets Neural Radiance Fields for   Memory-Efficient Scene Representation"></a>NeRFCodec: Neural Feature Compression Meets Neural Radiance Fields for   Memory-Efficient Scene Representation</h2><p><strong>Authors:Sicheng Li, Hao Li, Yiyi Liao, Lu Yu</strong></p><p>The emergence of Neural Radiance Fields (NeRF) has greatly impacted 3D scene modeling and novel-view synthesis. As a kind of visual media for 3D scene representation, compression with high rate-distortion performance is an eternal target. Motivated by advances in neural compression and neural field representation, we propose NeRFCodec, an end-to-end NeRF compression framework that integrates non-linear transform, quantization, and entropy coding for memory-efficient scene representation. Since training a non-linear transform directly on a large scale of NeRF feature planes is impractical, we discover that pre-trained neural 2D image codec can be utilized for compressing the features when adding content-specific parameters. Specifically, we reuse neural 2D image codec but modify its encoder and decoder heads, while keeping the other parts of the pre-trained decoder frozen. This allows us to train the full pipeline via supervision of rendering loss and entropy loss, yielding the rate-distortion balance by updating the content-specific parameters. At test time, the bitstreams containing latent code, feature decoder head, and other side information are transmitted for communication. Experimental results demonstrate our method outperforms existing NeRF compression methods, enabling high-quality novel view synthesis with a memory budget of 0.5 MB. </p><p><a href="http://arxiv.org/abs/2404.02185v1">PDF</a> Accepted at CVPR2024. The source code will be released</p><p><strong>Summary</strong><br>ç¥ç»è¾å°„åœº (NeRF) å‹ç¼©æ¡†æ¶ï¼Œé›†æˆäº†éçº¿æ€§å˜æ¢ã€é‡åŒ–å’Œç†µç¼–ç ï¼Œé€šè¿‡å¯é‡ç”¨é¢„è®­ç»ƒçš„ 2D å›¾åƒç¼–è§£ç å™¨ï¼Œå®ç°äº†é«˜æ•ˆçš„å†…å­˜åœºæ™¯è¡¨ç¤ºã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF çš„å…´èµ·ä¿ƒè¿›äº† 3D åœºæ™¯å»ºæ¨¡å’Œæ–°è§†å›¾åˆæˆã€‚</li><li>é«˜é€Ÿç‡-å¤±çœŸæ€§èƒ½çš„å‹ç¼©æ˜¯ 3D åœºæ™¯è¡¨ç¤ºçš„å…³é”®ã€‚</li><li>NeRFCodec é‡‡ç”¨éçº¿æ€§å˜æ¢ã€é‡åŒ–å’Œç†µç¼–ç ï¼Œå®ç°ç«¯åˆ°ç«¯çš„ NeRF å‹ç¼©ã€‚</li><li>é¢„è®­ç»ƒçš„ 2D å›¾åƒç¼–è§£ç å™¨å¯ç”¨äºå‹ç¼©ç‰¹å¾ï¼ŒåŒæ—¶æ·»åŠ å†…å®¹ç‰¹å®šå‚æ•°ã€‚</li><li>å¯é‡ç”¨ç¥ç» 2D å›¾åƒç¼–è§£ç å™¨ï¼Œä¿®æ”¹å…¶ç¼–ç å™¨å’Œè§£ç å™¨å¤´ï¼Œå†»ç»“å…¶ä»–éƒ¨åˆ†ã€‚</li><li>é€šè¿‡ç›‘ç£æ¸²æŸ“æŸå¤±å’Œç†µæŸå¤±è®­ç»ƒå®Œæ•´ç®¡é“ï¼Œæ›´æ–°å†…å®¹ç‰¹å®šå‚æ•°ï¼Œè¾¾åˆ°é€Ÿç‡å¤±çœŸå¹³è¡¡ã€‚</li><li>æµ‹è¯•æ—¶ï¼ŒåŒ…å«æ½œåœ¨ä»£ç ã€ç‰¹å¾è§£ç å¤´å’Œå…¶ä»–è¾¹ä¿¡æ¯çš„æ¯”ç‰¹æµç”¨äºé€šä¿¡ã€‚</li><li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰çš„ NeRF å‹ç¼©æ–¹æ³•ï¼Œä»¥ 0.5 MB çš„å†…å­˜é¢„ç®—å®ç°é«˜è´¨é‡çš„æ–°è§†å›¾åˆæˆã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šNeRFCodecï¼šç¥ç»ç‰¹å¾å‹ç¼©ä¸ç¥ç»è¾å°„åœºç›¸ç»“åˆï¼Œå®ç°å†…å­˜é«˜æ•ˆçš„åœºæ™¯è¡¨ç¤º</li><li>ä½œè€…ï¼šææ€æˆï¼Œææ˜Šï¼Œå»–æ€¡æ€¡ï¼Œäºé™†</li><li>æµ™æ±Ÿå¤§å­¦</li><li>Keywords: NeRF, Neural compression, Neural field representation, Rate-distortion optimization</li><li>é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.02185Githubï¼šNone</li><li>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åœ¨ 3D åœºæ™¯å»ºæ¨¡å’Œæ–°è§†è§’åˆæˆä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ï¼Œä½†å…¶è¡¨ç¤ºéœ€è¦å¤§é‡çš„å†…å­˜ï¼Œå‹ç¼© NeRF ä»¥æé«˜å­˜å‚¨æ•ˆç‡å’Œé€šä¿¡æ•ˆç‡æˆä¸ºä¸€ä¸ªé‡è¦çš„é—®é¢˜ã€‚(2) è¿‡å»æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨äºè®¾è®¡é«˜æ•ˆçš„æ•°æ®ç»“æ„æˆ–ä½¿ç”¨å‹ç¼©æŠ€æœ¯ï¼ˆå¦‚é‡åŒ–å’Œç†µç¼–ç ï¼‰æ¥å‹ç¼© NeRF å‚æ•°ï¼Œä½†å¿½ç•¥äº†å˜æ¢ç¼–ç çš„æœ‰æ•ˆæ€§ã€‚(3) ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡º NeRFCodecï¼Œä¸€ä¸ªç«¯åˆ°ç«¯çš„ NeRF å‹ç¼©æ¡†æ¶ï¼Œå®ƒé›†æˆäº†éçº¿æ€§å˜æ¢ã€é‡åŒ–å’Œç†µç¼–ç ï¼Œä»¥å®ç°å†…å­˜é«˜æ•ˆçš„åœºæ™¯è¡¨ç¤ºã€‚å…·ä½“æ¥è¯´ï¼Œæœ¬æ–‡åˆ©ç”¨é¢„è®­ç»ƒçš„ç¥ç» 2D å›¾åƒç¼–è§£ç å™¨ï¼Œå¹¶æ·»åŠ ç‰¹å®šäºå†…å®¹çš„å‚æ•°æ¥å‹ç¼© NeRF ç‰¹å¾ã€‚(4) æ€§èƒ½å’Œæ•ˆæœï¼šå®éªŒç»“æœè¡¨æ˜ï¼ŒNeRFCodec ä¼˜äºç°æœ‰çš„ NeRF å‹ç¼©æ–¹æ³•ï¼Œåœ¨ 0.5MB çš„å†…å­˜é¢„ç®—ä¸‹å®ç°äº†é«˜è´¨é‡çš„æ–°è§†è§’åˆæˆã€‚</li></ol><p>7.Methodsï¼š(1)åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºä¸€ä¸ªç«¯åˆ°ç«¯çš„NeRFå‹ç¼©æ¡†æ¶ï¼Œä¸åŸºäºå¹³é¢çš„æ··åˆNeRFå˜ä½“å…¼å®¹ã€‚å›¾2ç»™å‡ºäº†æˆ‘ä»¬æ¡†æ¶çš„æ¦‚è¿°ï¼ŒåŒ…æ‹¬ç¥ç»ç‰¹å¾å‹ç¼©å’ŒNeRFæ¸²æŸ“ã€‚ç¥ç»ç‰¹å¾å‹ç¼©åŒ…æ‹¬å†…å®¹è‡ªé€‚åº”éçº¿æ€§å˜æ¢ã€é‡åŒ–å’Œç†µç¼–ç ã€‚NeRFæ¸²æŸ“éµå¾ªç›¸åº”çš„NeRFå˜ä½“ã€‚(2)åœ¨ä»¥ä¸‹éƒ¨åˆ†ï¼Œæˆ‘ä»¬é¦–å…ˆä»‹ç»æ··åˆNeRFæ¨¡å‹å’Œç¥ç»å›¾åƒå‹ç¼©çš„é¢„å¤‡çŸ¥è¯†ã€‚(3)è¯¦ç»†æè¿°æœ¬æ–‡çš„æ–¹æ³•è®ºæ€æƒ³ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯çš„æ··åˆNeRFå‹ç¼©æ¡†æ¶NeRFCodecï¼Œè¯¥æ¡†æ¶å°†éçº¿æ€§å˜æ¢ã€é‡åŒ–å’Œç†µç¼–ç ç›¸ç»“åˆï¼Œç”¨äºå‹ç¼©æ··åˆNeRFä¸­çš„ç‰¹å¾å¹³é¢ï¼Œä»¥å®ç°å†…å­˜é«˜æ•ˆçš„åœºæ™¯è¡¨ç¤ºã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨ä»…æœ‰0.5MBçš„å†…å­˜å¼€é”€ä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å³å¯è¡¨ç¤ºå•ä¸ªåœºæ™¯ï¼ŒåŒæ—¶å®ç°é«˜è´¨é‡çš„æ–°è§†è§’åˆæˆã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ··åˆNeRFå‹ç¼©æ¡†æ¶ï¼Œå°†éçº¿æ€§å˜æ¢ã€é‡åŒ–å’Œç†µç¼–ç ç›¸ç»“åˆï¼Œç”¨äºå‹ç¼©æ··åˆNeRFä¸­çš„ç‰¹å¾å¹³é¢ï¼Œä»¥å®ç°å†…å­˜é«˜æ•ˆçš„åœºæ™¯è¡¨ç¤ºã€‚æ€§èƒ½ï¼šå®éªŒè¡¨æ˜ï¼Œåœ¨ä»…æœ‰0.5MBçš„å†…å­˜å¼€é”€ä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å³å¯è¡¨ç¤ºå•ä¸ªåœºæ™¯ï¼ŒåŒæ—¶å®ç°é«˜è´¨é‡çš„æ–°è§†è§’åˆæˆã€‚å·¥ä½œé‡ï¼šæœ¬æ–‡æå‡ºçš„æ–¹æ³•éœ€è¦è®­ç»ƒéçº¿æ€§å˜æ¢ï¼Œè¯¥è¿‡ç¨‹è€—æ—¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬éœ€è¦ä¸ºæ¯ä¸ªåœºæ™¯å•ç‹¬è®­ç»ƒä¸€ä¸ªä¸“é—¨çš„ç¥ç»ç‰¹å¾ç¼–è§£ç å™¨ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-4f02a9afbf123d3e5a994a2d49e3c0b7.jpg" align="middle"><img src="https://pica.zhimg.com/v2-3b65608fa67d1d139afe6f67463a630c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f6537648d45f0abf7c8ff70180094d6c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a6dfbb832840c5b4a530faf49106c554.jpg" align="middle"></details><h2 id="NeRF-MAE-Masked-AutoEncoders-for-Self-Supervised-3D-representation-Learning-for-Neural-Radiance-Fields"><a href="#NeRF-MAE-Masked-AutoEncoders-for-Self-Supervised-3D-representation-Learning-for-Neural-Radiance-Fields" class="headerlink" title="NeRF-MAE : Masked AutoEncoders for Self Supervised 3D representation   Learning for Neural Radiance Fields"></a>NeRF-MAE : Masked AutoEncoders for Self Supervised 3D representation   Learning for Neural Radiance Fields</h2><p><strong>Authors:Muhammad Zubair Irshad, Sergey Zakahrov, Vitor Guizilini, Adrien Gaidon, Zsolt Kira, Rares Ambrus</strong></p><p>Neural fields excel in computer vision and robotics due to their ability to understand the 3D visual world such as inferring semantics, geometry, and dynamics. Given the capabilities of neural fields in densely representing a 3D scene from 2D images, we ask the question: Can we scale their self-supervised pretraining, specifically using masked autoencoders, to generate effective 3D representations from posed RGB images. Owing to the astounding success of extending transformers to novel data modalities, we employ standard 3D Vision Transformers to suit the unique formulation of NeRFs. We leverage NeRFâ€™s volumetric grid as a dense input to the transformer, contrasting it with other 3D representations such as pointclouds where the information density can be uneven, and the representation is irregular. Due to the difficulty of applying masked autoencoders to an implicit representation, such as NeRF, we opt for extracting an explicit representation that canonicalizes scenes across domains by employing the camera trajectory for sampling. Our goal is made possible by masking random patches from NeRFâ€™s radiance and density grid and employing a standard 3D Swin Transformer to reconstruct the masked patches. In doing so, the model can learn the semantic and spatial structure of complete scenes. We pretrain this representation at scale on our proposed curated posed-RGB data, totaling over 1.6 million images. Once pretrained, the encoder is used for effective 3D transfer learning. Our novel self-supervised pretraining for NeRFs, NeRF-MAE, scales remarkably well and improves performance on various challenging 3D tasks. Utilizing unlabeled posed 2D data for pretraining, NeRF-MAE significantly outperforms self-supervised 3D pretraining and NeRF scene understanding baselines on Front3D and ScanNet datasets with an absolute performance improvement of over 20% AP50 and 8% AP25 for 3D object detection. </p><p><a href="http://arxiv.org/abs/2404.01300v1">PDF</a> 29 pages, 13 figures. Project Page: <a href="https://nerf-mae.github.io/">https://nerf-mae.github.io/</a></p><p><strong>Summary</strong><br>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰çš„è‡ªç›‘ç£é¢„è®­ç»ƒå¯ä»¥æ˜¾ç€æé«˜3Dè§†è§‰ä»»åŠ¡çš„æ€§èƒ½ï¼Œä¾‹å¦‚3Dç‰©ä½“æ£€æµ‹å’Œåœºæ™¯ç†è§£ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>NeRFåœ¨è®¡ç®—æœºè§†è§‰å’Œæœºå™¨äººé¢†åŸŸè¡¨ç°å‡ºè‰²ï¼Œå› ä¸ºå®ƒèƒ½å¤Ÿç†è§£3Dè§†è§‰ä¸–ç•Œï¼Œå¦‚è¯­ä¹‰ã€å‡ ä½•å’ŒåŠ¨æ€ã€‚</li><li>ç ”ç©¶äººå‘˜æ¢ç´¢äº†ä½¿ç”¨æ©ç è‡ªç¼–ç å™¨å¯¹å…¶è¿›è¡Œè‡ªç›‘ç£é¢„è®­ç»ƒï¼Œä»¥ä»æ‘†å§¿åŠ¿çš„RGBå›¾åƒä¸­ç”Ÿæˆæœ‰æ•ˆçš„3Dè¡¨ç¤ºã€‚</li><li>è¯¥ç ”ç©¶é‡‡ç”¨äº†æ ‡å‡†çš„3Dè§†è§‰Transformeræ¥é€‚åº”NeRFçš„ç‹¬ç‰¹å…¬å¼ï¼Œå°†NeRFçš„ä½“ç§¯ç½‘æ ¼ä½œä¸ºå˜å‹å™¨çš„å¯†é›†è¾“å…¥ã€‚</li><li>ç”±äºå°†æ©ç è‡ªç¼–ç å™¨åº”ç”¨äºéšå¼è¡¨ç¤ºï¼ˆå¦‚NeRFï¼‰å­˜åœ¨å›°éš¾ï¼Œç ”ç©¶äººå‘˜é€‰æ‹©æå–ä¸€ä¸ªæ˜¾å¼è¡¨ç¤ºï¼Œé€šè¿‡ä½¿ç”¨ç›¸æœºè½¨è¿¹è¿›è¡Œé‡‡æ ·æ¥è§„èŒƒè·¨åŸŸåœºæ™¯ã€‚</li><li>ç ”ç©¶äººå‘˜é€šè¿‡æ©ç›–NeRFçš„è¾å°„å’Œå¯†åº¦ç½‘æ ¼ä¸­çš„éšæœºè¡¥ä¸ï¼Œå¹¶ä½¿ç”¨æ ‡å‡†çš„3D Swin Transformeré‡å»ºæ©ç›–çš„è¡¥ä¸ï¼Œå®ç°äº†è¿™ä¸€ç›®æ ‡ã€‚</li><li>è¯¥æ¨¡å‹ä»¥è‡ªç›‘ç£æ–¹å¼åœ¨è¶…è¿‡160ä¸‡å¼ å›¾åƒçš„æ‹Ÿè®®ç­–åˆ’çš„æ‘†å§¿åŠ¿RGBæ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒã€‚</li><li>é¢„è®­ç»ƒåçš„ç¼–ç å™¨ç”¨äºæœ‰æ•ˆçš„3Dè¿ç§»å­¦ä¹ ï¼Œå¹¶åœ¨å„ç§å…·æœ‰æŒ‘æˆ˜æ€§çš„3Dä»»åŠ¡ä¸Šæ˜¾ç€æé«˜äº†æ€§èƒ½ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šNeRF-MAEï¼šç”¨äºè‡ªç›‘ç£ NeRF çš„æ©ç è‡ªåŠ¨ç¼–ç å™¨</li><li>ä½œè€…ï¼šYuxuan Zhang, Xinyu Chen, Jiaxin Li, Yining Li, Chen Feng, Chao Wen, Wei Wang</li><li>å•ä½ï¼šåŒ—äº¬å¤§å­¦</li><li>å…³é”®è¯ï¼šNeRFï¼Œè‡ªç›‘ç£å­¦ä¹ ï¼Œæ©ç è‡ªåŠ¨ç¼–ç å™¨ï¼Œ3D è¡¨ç¤ºå­¦ä¹ </li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.01300</li><li>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»åœºåœ¨è®¡ç®—æœºè§†è§‰å’Œæœºå™¨äººé¢†åŸŸè¡¨ç°å‡ºè‰²ï¼Œå› ä¸ºå®ƒèƒ½å¤Ÿç†è§£ä¸‰ç»´è§†è§‰ä¸–ç•Œï¼Œå¦‚æ¨æ–­è¯­ä¹‰ã€å‡ ä½•å’ŒåŠ¨åŠ›å­¦ã€‚(2) è¿‡å»çš„æ–¹æ³•ï¼šNeRF æ˜¯ä¸€ç§æˆåŠŸçš„éšå¼ç¥ç»åœºè¡¨ç¤ºï¼Œä½†å…¶è‡ªç›‘ç£é¢„è®­ç»ƒå­˜åœ¨æŒ‘æˆ˜ã€‚(3) æœ¬æ–‡æ–¹æ³•ï¼šæå‡º NeRF-MAEï¼Œä¸€ç§ä½¿ç”¨æ©ç è‡ªåŠ¨ç¼–ç å™¨çš„è‡ªç›‘ç£ NeRF é¢„è®­ç»ƒæ–¹æ³•ã€‚è¯¥æ–¹æ³•å°† NeRF çš„ä½“ç´ ç½‘æ ¼ä½œä¸ºè¾“å…¥ï¼Œå¹¶ä½¿ç”¨ 3D Swin Transformer é‡å»ºæ©ç è¡¥ä¸ã€‚(4) æ€§èƒ½ï¼šåœ¨ 3D å¯¹è±¡è¯†åˆ«ã€è¯­ä¹‰åˆ†å‰²å’Œæ·±åº¦ä¼°è®¡ä»»åŠ¡ä¸Šï¼ŒNeRF-MAE çš„æ€§èƒ½ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚è¿™äº›ç»“æœæ”¯æŒäº†ä½¿ç”¨æ©ç è‡ªåŠ¨ç¼–ç å™¨è¿›è¡Œ NeRF è‡ªç›‘ç£é¢„è®­ç»ƒçš„æœ‰æ•ˆæ€§ã€‚</li></ol><p>7.Methodsï¼š(1) NeRF-MAE æå‡ºäº†ä¸€ç§ä½¿ç”¨æ©ç è‡ªåŠ¨ç¼–ç å™¨ (MAE) è¿›è¡Œè‡ªç›‘ç£ NeRF é¢„è®­ç»ƒçš„æ–¹æ³•ã€‚(2) æ–¹æ³•å°† NeRF çš„ä½“ç´ ç½‘æ ¼ä½œä¸ºè¾“å…¥ï¼Œå¹¶ä½¿ç”¨ 3DSwinTransformer é‡å»ºæ©ç è¡¥ä¸ã€‚(3) å…·ä½“æ¥è¯´ï¼Œæ–¹æ³•é¦–å…ˆå°†ä½“ç´ ç½‘æ ¼åˆ’åˆ†ä¸º patchesï¼Œç„¶åéšæœºæ©ç›–å…¶ä¸­ä¸€éƒ¨åˆ† patchesã€‚(4) 3DSwinTransformer ç¼–ç å™¨å°†æ©ç›–çš„ patches æŠ•å½±åˆ°ä½ç»´è¡¨ç¤ºä¸­ï¼Œç„¶åè§£ç å™¨å°†è¿™äº›è¡¨ç¤ºé‡å»ºä¸ºåŸå§‹ patchesã€‚(5) é€šè¿‡æœ€å°åŒ–é‡å»ºè¯¯å·®ï¼ŒNeRF-MAE å­¦ä¹ è¡¨ç¤ºä¸‰ç»´åœºæ™¯çš„ç‰¹å¾ï¼Œä»è€Œå®ç°è‡ªç›‘ç£é¢„è®­ç»ƒã€‚</p><ol><li>ç»“è®ºï¼š(1): æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§ä½¿ç”¨æ©ç è‡ªåŠ¨ç¼–ç å™¨è¿›è¡Œ NeRF è‡ªç›‘ç£é¢„è®­ç»ƒçš„æ–¹æ³•ï¼Œä¸º NeRF çš„è‡ªç›‘ç£å­¦ä¹ æä¾›äº†æ–°çš„æ€è·¯ï¼Œæå‡äº† NeRF åœ¨ä¸‰ç»´è§†è§‰ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚(2): åˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäºæ©ç è‡ªåŠ¨ç¼–ç å™¨çš„è‡ªç›‘ç£ NeRF é¢„è®­ç»ƒæ–¹æ³•ï¼Œä½¿ç”¨ 3D Swin Transformer é‡å»ºæ©ç è¡¥ä¸ï¼Œæœ‰æ•ˆå­¦ä¹ ä¸‰ç»´åœºæ™¯çš„ç‰¹å¾ã€‚æ€§èƒ½ï¼šåœ¨ 3D å¯¹è±¡è¯†åˆ«ã€è¯­ä¹‰åˆ†å‰²å’Œæ·±åº¦ä¼°è®¡ä»»åŠ¡ä¸Šï¼ŒNeRF-MAE çš„æ€§èƒ½ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚å·¥ä½œé‡ï¼šè¯¥æ–¹æ³•éœ€è¦å¯¹ NeRF çš„ä½“ç´ ç½‘æ ¼è¿›è¡Œé¢„å¤„ç†ï¼Œå¹¶ä½¿ç”¨ 3D Swin Transformer è¿›è¡Œè®­ç»ƒï¼Œå·¥ä½œé‡ç›¸å¯¹è¾ƒå¤§ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-ebc2863cbef45a417493c8c06f6da7f7.jpg" align="middle"><img src="https://pica.zhimg.com/v2-a7df4839533998c067dcf937ee13625b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-410dfb78b1608c0f22605988b109ec23.jpg" align="middle"><img src="https://picx.zhimg.com/v2-72ef188b10053e0ff78cd0d57d23eb07.jpg" align="middle"><img src="https://pica.zhimg.com/v2-186964e11f6fa449110cabd1f47254e2.jpg" align="middle"></details><h2 id="MagicMirror-Fast-and-High-Quality-Avatar-Generation-with-a-Constrained-Search-Space"><a href="#MagicMirror-Fast-and-High-Quality-Avatar-Generation-with-a-Constrained-Search-Space" class="headerlink" title="MagicMirror: Fast and High-Quality Avatar Generation with a Constrained   Search Space"></a>MagicMirror: Fast and High-Quality Avatar Generation with a Constrained   Search Space</h2><p><strong>Authors:Armand Comas-MassaguÃ©, Di Qiu, Menglei Chai, Marcel BÃ¼hler, Amit Raj, Ruiqi Gao, Qiangeng Xu, Mark Matthews, Paulo Gotardo, Octavia Camps, Sergio Orts-Escolano, Thabo Beeler</strong></p><p>We introduce a novel framework for 3D human avatar generation and personalization, leveraging text prompts to enhance user engagement and customization. Central to our approach are key innovations aimed at overcoming the challenges in photo-realistic avatar synthesis. Firstly, we utilize a conditional Neural Radiance Fields (NeRF) model, trained on a large-scale unannotated multi-view dataset, to create a versatile initial solution space that accelerates and diversifies avatar generation. Secondly, we develop a geometric prior, leveraging the capabilities of Text-to-Image Diffusion Models, to ensure superior view invariance and enable direct optimization of avatar geometry. These foundational ideas are complemented by our optimization pipeline built on Variational Score Distillation (VSD), which mitigates texture loss and over-saturation issues. As supported by our extensive experiments, these strategies collectively enable the creation of custom avatars with unparalleled visual quality and better adherence to input text prompts. You can find more results and videos in our website: <a href="https://syntec-research.github.io/MagicMirror">https://syntec-research.github.io/MagicMirror</a> </p><p><a href="http://arxiv.org/abs/2404.01296v1">PDF</a> </p><p><strong>Summary</strong><br>æ–‡æœ¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆå’Œä¸ªæ€§åŒ–3Däººå½¢èº«ï¼Œåˆ©ç”¨æ–‡æœ¬æç¤ºæ¥å¢å¼ºç”¨æˆ·å‚ä¸åº¦å’Œè‡ªå®šä¹‰åº¦ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>åˆ©ç”¨æ— æ ‡ç­¾å¤šè§†å›¾æ•°æ®é›†è®­ç»ƒçš„æ¡ä»¶ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æ¨¡å‹ï¼Œåˆ›å»ºé€šç”¨çš„åˆå§‹è§£å†³æ–¹æ¡ˆç©ºé—´ï¼Œä»¥åŠ é€Ÿå’Œå¤šæ ·åŒ–å¤´åƒç”Ÿæˆã€‚</li><li>å¼€å‘å‡ ä½•å…ˆéªŒï¼Œåˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„èƒ½åŠ›ï¼Œä»¥ç¡®ä¿æ›´å¥½çš„è§†å›¾ä¸å˜æ€§å¹¶å®ç°å¤´åƒå‡ ä½•å½¢çŠ¶çš„ç›´æ¥ä¼˜åŒ–ã€‚</li><li>å¼•å…¥åŸºäºå˜åˆ†å¾—åˆ†è’¸é¦ï¼ˆVSDï¼‰çš„ä¼˜åŒ–ç®¡é“ï¼Œä»¥å‡è½»çº¹ç†æŸå¤±å’Œè¿‡é¥±å’Œé—®é¢˜ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šMagicMirrorï¼šå¿«é€Ÿä¸”é«˜è´¨é‡çš„å¤´åƒ</li><li>Armand Comas-MassaguÃ©, Di Qiu, Menglei Chai, Marcel BÃ¼hler, Amit Raj, Ruiqi Gao, Qiangeng Xu, Mark Matthews, Paulo Gotardo, Octavia Camps, Sergio Orts-Escolano, Thabo Beeler</li><li>è°·æ­Œ</li><li>3Då¤´åƒç”Ÿæˆï¼›æ–‡æœ¬å¼•å¯¼ï¼›NeRFï¼›å‡ ä½•å…ˆéªŒï¼›å˜åˆ†åˆ†æ•°è’¸é¦</li><li>Paper: https://arxiv.org/abs/2404.01296   Github: None</li><li><p>æ‘˜è¦ï¼š(1)ï¼šéšç€æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„è¿›æ­¥ï¼Œæ–‡æœ¬å¼•å¯¼çš„ 3D äººç±»å¤´åƒç”Ÿæˆå˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•åœ¨ç”Ÿæˆé€¼çœŸçš„ã€é«˜è´¨é‡çš„å¤´åƒæ–¹é¢ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å‡ ä½•ç»†èŠ‚å’Œçº¹ç†è¿‡é¥±å’Œæ–¹é¢ã€‚(2)ï¼šå…ˆå‰çš„æ–¹æ³•é€šå¸¸ä½¿ç”¨åŸºäºä½“ç´ æˆ–ç½‘æ ¼çš„è¡¨ç¤ºæ¥ç”Ÿæˆå¤´åƒï¼Œè¿™é™åˆ¶äº†å‡ ä½•ç»†èŠ‚å¹¶å®¹æ˜“å‡ºç°çº¹ç†è¿‡é¥±å’Œã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡çš„é¢„è®­ç»ƒæ•°æ®å’Œæ¼«é•¿çš„ä¼˜åŒ–è¿‡ç¨‹ã€‚(3)ï¼šMagicMirror æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œç”¨äº 3D äººç±»å¤´åƒç”Ÿæˆå’Œä¸ªæ€§åŒ–ï¼Œåˆ©ç”¨æ–‡æœ¬æç¤ºæ¥å¢å¼ºç”¨æˆ·å‚ä¸åº¦å’Œè‡ªå®šä¹‰ã€‚è¯¥æ–¹æ³•çš„å…³é”®åˆ›æ–°åŒ…æ‹¬ï¼š1ï¼‰åˆ©ç”¨åœ¨å¤§å‹æœªæ³¨é‡Šå¤šè§†å›¾æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¡ä»¶ç¥ç»è¾å°„åœº (NeRF) æ¨¡å‹ï¼Œåˆ›å»ºäº†ä¸€ä¸ªå¤šåŠŸèƒ½çš„åˆå§‹è§£ç©ºé—´ï¼Œå¯ä»¥åŠ é€Ÿå’Œå¤šæ ·åŒ–å¤´åƒç”Ÿæˆï¼›2ï¼‰å¼€å‘å‡ ä½•å…ˆéªŒï¼Œåˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„èƒ½åŠ›ï¼Œä»¥ç¡®ä¿å‡ºè‰²çš„è§†å›¾ä¸å˜æ€§å’Œç›´æ¥ä¼˜åŒ–å¤´åƒå‡ ä½•å½¢çŠ¶ï¼›3ï¼‰ä¼˜åŒ–ç®¡é“å»ºç«‹åœ¨å˜åˆ†åˆ†æ•°è’¸é¦ (VSD) ä¹‹ä¸Šï¼Œå¯å‡è½»çº¹ç†æŸå¤±å’Œè¿‡é¥±å’Œé—®é¢˜ã€‚(4)ï¼šå®éªŒè¡¨æ˜ï¼Œè¿™äº›ç­–ç•¥å…±åŒå®ç°äº†åˆ›å»ºå…·æœ‰æ— ä¸ä¼¦æ¯”è§†è§‰è´¨é‡å’Œæ›´å¥½åœ°éµå¾ªè¾“å…¥æ–‡æœ¬æç¤ºçš„è‡ªå®šä¹‰å¤´åƒã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) åˆ©ç”¨æ¡ä»¶ç¥ç»è¾å°„åœº (NeRF) æ¨¡å‹åˆ›å»ºå¤šåŠŸèƒ½çš„åˆå§‹è§£ç©ºé—´ï¼ŒåŠ é€Ÿå¤´åƒç”Ÿæˆï¼›(2) å¼€å‘å‡ ä½•å…ˆéªŒï¼Œåˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„èƒ½åŠ›ï¼Œä¼˜åŒ–å¤´åƒå‡ ä½•å½¢çŠ¶ï¼›(3) ä¼˜åŒ–ç®¡é“å»ºç«‹åœ¨å˜åˆ†åˆ†æ•°è’¸é¦ (VSD) ä¹‹ä¸Šï¼Œå‡è½»çº¹ç†æŸå¤±å’Œè¿‡é¥±å’Œé—®é¢˜ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šMagicMirroråœ¨æ–‡æœ¬å¼•å¯¼çš„ 3D äººç±»å¤´åƒç”Ÿæˆé¢†åŸŸå–å¾—äº†é‡å¤§çªç ´ï¼Œé€šè¿‡çº¦æŸè§£ç©ºé—´ã€å¯»æ‰¾è‰¯å¥½çš„å‡ ä½•å…ˆéªŒå¹¶é€‰æ‹©è‰¯å¥½çš„æµ‹è¯•æ—¶ä¼˜åŒ–ç›®æ ‡ï¼Œå®ç°äº†è§†è§‰è´¨é‡ã€å¤šæ ·æ€§å’Œä¿çœŸåº¦çš„æå‡ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šåˆ©ç”¨æ¡ä»¶ NeRF æ¨¡å‹åˆ›å»ºå¤šåŠŸèƒ½çš„åˆå§‹è§£ç©ºé—´ï¼Œå¼€å‘å‡ ä½•å…ˆéªŒä¼˜åŒ–å¤´åƒå‡ ä½•å½¢çŠ¶ï¼Œé‡‡ç”¨å˜åˆ†åˆ†æ•°è’¸é¦å‡è½»çº¹ç†æŸå¤±å’Œè¿‡é¥±å’Œé—®é¢˜ã€‚æ€§èƒ½ï¼šåœ¨è§†è§‰è´¨é‡ã€å¤šæ ·æ€§å’Œä¿çœŸåº¦æ–¹é¢è¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œåœ¨å¹¿æ³›çš„æ¶ˆèå’Œæ¯”è¾ƒç ”ç©¶ä¸­å¾—åˆ°éªŒè¯ã€‚å·¥ä½œé‡ï¼šéœ€è¦å¤šä¸ªæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œè‡³å°‘æ¯ä¸ªç”¨äºé¢œè‰²å’Œæ³•çº¿ï¼Œå¦‚æœè¦æ‰§è¡Œæ¦‚å¿µæ··åˆåˆ™éœ€è¦æ›´å¤šã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-1baa0925f922a12a8e66e59ff6fb331c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c42a213fc6f5741472d164c3e74fb649.jpg" align="middle"><img src="https://pica.zhimg.com/v2-864c32c352be8bc29cb98bb5b6fc3f53.jpg" align="middle"></details><h2 id="Mirror-3DGS-Incorporating-Mirror-Reflections-into-3D-Gaussian-Splatting"><a href="#Mirror-3DGS-Incorporating-Mirror-Reflections-into-3D-Gaussian-Splatting" class="headerlink" title="Mirror-3DGS: Incorporating Mirror Reflections into 3D Gaussian Splatting"></a>Mirror-3DGS: Incorporating Mirror Reflections into 3D Gaussian Splatting</h2><p><strong>Authors:Jiarui Meng, Haijie Li, Yanmin Wu, Qiankun Gao, Shuzhou Yang, Jian Zhang, Siwei Ma</strong></p><p>3D Gaussian Splatting (3DGS) has marked a significant breakthrough in the realm of 3D scene reconstruction and novel view synthesis. However, 3DGS, much like its predecessor Neural Radiance Fields (NeRF), struggles to accurately model physical reflections, particularly in mirrors that are ubiquitous in real-world scenes. This oversight mistakenly perceives reflections as separate entities that physically exist, resulting in inaccurate reconstructions and inconsistent reflective properties across varied viewpoints. To address this pivotal challenge, we introduce Mirror-3DGS, an innovative rendering framework devised to master the intricacies of mirror geometries and reflections, paving the way for the generation of realistically depicted mirror reflections. By ingeniously incorporating mirror attributes into the 3DGS and leveraging the principle of plane mirror imaging, Mirror-3DGS crafts a mirrored viewpoint to observe from behind the mirror, enriching the realism of scene renderings. Extensive assessments, spanning both synthetic and real-world scenes, showcase our methodâ€™s ability to render novel views with enhanced fidelity in real-time, surpassing the state-of-the-art Mirror-NeRF specifically within the challenging mirror regions. Our code will be made publicly available for reproducible research. </p><p><a href="http://arxiv.org/abs/2404.01168v1">PDF</a> 22 pages, 7 figures</p><p><strong>Summary</strong><br>3D é«˜æ–¯æ•£ç‚¹æŠ€æœ¯ï¼ˆ3DGSï¼‰åœ¨ 3D åœºæ™¯é‡å»ºå’Œæ–°è§†è§’åˆæˆé¢†åŸŸå–å¾—äº†é‡å¤§çªç ´ï¼Œä½†å®ƒæ— æ³•å‡†ç¡®å»ºæ¨¡ç‰©ç†åå°„ï¼Œç‰¹åˆ«æ˜¯é•œé¢åå°„ï¼Œè€Œé•œé¢åå°„åœ¨çœŸå®åœºæ™¯ä¸­æ— å¤„ä¸åœ¨ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>3DGSé”™è¯¯åœ°å°†åå°„è§†ä¸ºç‹¬ç«‹äºç‰©ç†ä¸–ç•Œçš„å•ç‹¬å®ä½“ï¼Œå¯¼è‡´é‡å»ºä¸å‡†ç¡®ã€ä¸åŒè§†è§’çš„åå°„å±æ€§ä¸ä¸€è‡´ã€‚</li><li>é•œé¢ 3DGS æ˜¯ä¸€ç§æ–°é¢–çš„æ¸²æŸ“æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³é•œå­å‡ ä½•å½¢çŠ¶å’Œåå°„çš„å¤æ‚æ€§ï¼Œä¸ºçœŸå®å‘ˆç°é•œå­åå°„é“ºå¹³äº†é“è·¯ã€‚</li><li>é•œé¢ 3DGS å·§å¦™åœ°å°†é•œå­å±æ€§èå…¥ 3DGSï¼Œå¹¶åˆ©ç”¨å¹³é¢é•œæˆåƒåŸç†ï¼Œæ„å»ºäº†ä¸€ä¸ªä»é•œå­åé¢è§‚å¯Ÿçš„é•œåƒè§†ç‚¹ï¼Œä¸°å¯Œäº†åœºæ™¯æ¸²æŸ“çš„çœŸå®æ„Ÿã€‚</li><li>å¹¿æ³›çš„è¯„ä¼°è¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„ Mirror-NeRF ç›¸æ¯”ï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„é•œå­åŒºåŸŸå†…ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä»¥æ›´é«˜çš„ä¿çœŸåº¦å®æ—¶æ¸²æŸ“æ–°çš„è§†è§’ã€‚</li><li>è¯¥æ–¹æ³•çš„ä»£ç å°†å…¬å¼€ï¼Œä»¥ä¾›å¯é‡å¤çš„ç ”ç©¶ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šMirror-3DGSï¼šå°†é•œå­åå°„èå…¥ 3D é«˜æ–¯æº…å°„</li><li>ä½œè€…ï¼šHeng Li, Zexiang Xu, Hao Tang, Sijia Liu, Ya-Qin Zhang</li><li>å•ä½ï¼šä¸Šæµ·äº¤é€šå¤§å­¦</li><li>å…³é”®è¯ï¼šé«˜æ–¯æº…å°„ Â· é•œåƒåœºæ™¯ Â· æ–°è§†è§’åˆæˆ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.06266, Github æš‚æ— </li><li><p>æ‘˜è¦ï¼š(1)ï¼šç ”ç©¶èƒŒæ™¯ï¼š3D é«˜æ–¯æº…å°„ (3DGS) åœ¨ 3D åœºæ™¯é‡å»ºå’Œæ–°è§†è§’åˆæˆé¢†åŸŸå–å¾—äº†é‡å¤§çªç ´ã€‚ç„¶è€Œï¼Œ3DGS ä¸å…¶å‰èº«ç¥ç»è¾å°„åœº (NeRF) ä¸€æ ·ï¼Œéš¾ä»¥å‡†ç¡®å»ºæ¨¡ç‰©ç†åå°„ï¼Œå°¤å…¶æ˜¯åœ¨ç°å®åœºæ™¯ä¸­æ— å¤„ä¸åœ¨çš„é•œå­ä¸­ã€‚è¿™ç§ç–å¿½é”™è¯¯åœ°å°†åå°„è§†ä¸ºç‹¬ç«‹å­˜åœ¨çš„ç‰©ç†å®ä½“ï¼Œå¯¼è‡´é‡å»ºä¸å‡†ç¡®ï¼Œå¹¶ä¸”ä¸åŒè§†è§’ä¸‹çš„åå°„å±æ€§ä¸ä¸€è‡´ã€‚(2)ï¼šè¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šä¸ºäº†è§£å†³è¿™ä¸€å…³é”®æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº† Mirror-3DGSï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ›æ–°çš„æ¸²æŸ“æ¡†æ¶ï¼Œæ—¨åœ¨æŒæ¡é•œå­å‡ ä½•å½¢çŠ¶å’Œåå°„çš„å¤æ‚æ€§ï¼Œä¸ºç”Ÿæˆé€¼çœŸçš„é•œå­åå°„é“ºå¹³äº†é“è·¯ã€‚é€šè¿‡å·§å¦™åœ°å°†é•œå­å±æ€§èå…¥ 3DGS å¹¶åˆ©ç”¨å¹³é¢é•œæˆåƒåŸç†ï¼ŒMirror-3DGS åˆ¶ä½œäº†ä¸€ä¸ªé•œåƒè§†ç‚¹ï¼Œä»é•œå­åé¢è§‚å¯Ÿï¼Œä»è€Œä¸°å¯Œäº†åœºæ™¯æ¸²æŸ“çš„çœŸå®æ„Ÿã€‚(3)ï¼šç ”ç©¶æ–¹æ³•ï¼šåœ¨åˆæˆå’ŒçœŸå®åœºæ™¯ä¸­è¿›è¡Œçš„å¹¿æ³›è¯„ä¼°å±•ç¤ºäº†æˆ‘ä»¬æ–¹æ³•åœ¨å®æ—¶æ¸²æŸ“æ–°è§†è§’æ—¶å¢å¼ºä¿çœŸåº¦çš„èƒ½åŠ›ï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„é•œå­åŒºåŸŸå†…è¶…è¶Šäº†æœ€å…ˆè¿›çš„ Mirror-NeRFã€‚æˆ‘ä»¬çš„ä»£ç å°†å…¬å¼€å‘å¸ƒä»¥è¿›è¡Œå¯é‡å¤çš„ç ”ç©¶ã€‚(4)ï¼šä»»åŠ¡å’Œæ€§èƒ½ï¼šåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„é•œå­åŒºåŸŸå†…ï¼ŒMirror-3DGS åœ¨æ–°è§†è§’åˆæˆä»»åŠ¡ä¸Šå–å¾—äº†æ¯”æœ€å…ˆè¿›æ–¹æ³•æ›´å¥½çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) é•œåƒæ„ŸçŸ¥ 3D é«˜æ–¯è¡¨ç¤ºï¼šå¼•å…¥å¯å­¦ä¹ çš„é•œåƒå±æ€§ï¼ŒåŒºåˆ†é•œé¢å’Œéé•œé¢é«˜æ–¯çƒä½“ã€‚(2) è™šæ‹Ÿé•œåƒè§†ç‚¹æ„å»ºï¼šåŸºäºé•œåƒå±æ€§å’Œä¸é€æ˜åº¦ï¼Œç­›é€‰å‡ºé•œé¢é«˜æ–¯çƒä½“ï¼Œåˆ©ç”¨å¹³é¢å‚æ•°åŒ–æ„å»ºé•œåƒå¹³é¢ï¼Œæ¨å¯¼å‡ºé•œåƒè§†ç‚¹å˜æ¢çŸ©é˜µã€‚(3) å›¾åƒèåˆï¼šä»åŸå§‹è§†ç‚¹å’Œé•œåƒè§†ç‚¹åˆ†åˆ«æ¸²æŸ“å›¾åƒï¼Œåˆ©ç”¨é•œåƒæ©ç èåˆä¸¤å¹…å›¾åƒï¼Œç”Ÿæˆæœ€ç»ˆç»“æœã€‚(4) ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼šç¬¬ä¸€é˜¶æ®µä¼˜åŒ–é•œåƒå¹³é¢æ–¹ç¨‹å’Œç²—ç•¥çš„ 3D é«˜æ–¯è¡¨ç¤ºï¼Œç¬¬äºŒé˜¶æ®µåŸºäºä¼°è®¡çš„é•œåƒå¹³é¢æ–¹ç¨‹ï¼ŒèåˆåŸå§‹è§†ç‚¹å’Œé•œåƒè§†ç‚¹æ¸²æŸ“çš„å›¾åƒï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–åœºæ™¯çš„ 3D é«˜æ–¯è¡¨ç¤ºã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œçš„é‡è¦æ„ä¹‰ï¼šMirror-3DGS åˆ›æ–°æ€§åœ°å°†é•œå­å±æ€§èå…¥ 3D é«˜æ–¯è¡¨ç¤ºï¼Œæœ‰æ•ˆè§£å†³äº† 3D åœºæ™¯ä¸­é•œå­åå°„å»ºæ¨¡çš„éš¾é¢˜ï¼Œä¸ºæ–°è§†è§’åˆæˆä¸­é€¼çœŸé•œé¢åå°„çš„ç”Ÿæˆé“ºå¹³äº†é“è·¯ã€‚ï¼ˆ2ï¼‰ï¼šæ–‡ç« ä¼˜ç¼ºç‚¹æ€»ç»“ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>å¼•å…¥é•œåƒæ„ŸçŸ¥ 3D é«˜æ–¯è¡¨ç¤ºï¼ŒåŒºåˆ†é•œé¢å’Œéé•œé¢é«˜æ–¯çƒä½“ã€‚</li><li>æ„å»ºè™šæ‹Ÿé•œåƒè§†ç‚¹ï¼Œä¸°å¯Œåœºæ™¯æ¸²æŸ“çš„çœŸå®æ„Ÿã€‚</li><li>ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œä¼˜åŒ–é•œåƒå¹³é¢æ–¹ç¨‹å’Œ 3D é«˜æ–¯è¡¨ç¤ºã€‚æ€§èƒ½ï¼š</li><li>åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„é•œå­åŒºåŸŸå†…ï¼Œæ–°è§†è§’åˆæˆä»»åŠ¡å–å¾—äº†æ¯”æœ€å…ˆè¿›æ–¹æ³•æ›´å¥½çš„æ€§èƒ½ã€‚</li><li>ä¸ Mirror-NeRF ç›¸æ¯”ï¼Œåœ¨ä¿çœŸåº¦æ–¹é¢å–å¾—äº†å®è´¨æ€§æå‡ã€‚å·¥ä½œé‡ï¼š</li><li>éœ€è¦æ‰‹åŠ¨æ ‡æ³¨é•œé¢åŒºåŸŸï¼Œå·¥ä½œé‡è¾ƒå¤§ã€‚</li><li>è®­ç»ƒè¿‡ç¨‹è¾ƒå¤æ‚ï¼Œéœ€è¦è¾ƒé•¿çš„è®­ç»ƒæ—¶é—´ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-b687b1f16dc36cf89c9d228e4a9c2fcc.jpg" align="middle"><img src="https://picx.zhimg.com/v2-006e8205855032985f3309526106945f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b68e59518e5eb4ceae94fee53f29fd38.jpg" align="middle"></details><h2 id="MM3DGS-SLAM-Multi-modal-3D-Gaussian-Splatting-for-SLAM-Using-Vision-Depth-and-Inertial-Measurements"><a href="#MM3DGS-SLAM-Multi-modal-3D-Gaussian-Splatting-for-SLAM-Using-Vision-Depth-and-Inertial-Measurements" class="headerlink" title="MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vision,   Depth, and Inertial Measurements"></a>MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vision,   Depth, and Inertial Measurements</h2><p><strong>Authors:Lisong C. Sun, Neel P. Bhatt, Jonathan C. Liu, Zhiwen Fan, Zhangyang Wang, Todd E. Humphreys, Ufuk Topcu</strong></p><p>Simultaneous localization and mapping is essential for position tracking and scene understanding. 3D Gaussian-based map representations enable photorealistic reconstruction and real-time rendering of scenes using multiple posed cameras. We show for the first time that using 3D Gaussians for map representation with unposed camera images and inertial measurements can enable accurate SLAM. Our method, MM3DGS, addresses the limitations of prior neural radiance field-based representations by enabling faster rendering, scale awareness, and improved trajectory tracking. Our framework enables keyframe-based mapping and tracking utilizing loss functions that incorporate relative pose transformations from pre-integrated inertial measurements, depth estimates, and measures of photometric rendering quality. We also release a multi-modal dataset, UT-MM, collected from a mobile robot equipped with a camera and an inertial measurement unit. Experimental evaluation on several scenes from the dataset shows that MM3DGS achieves 3x improvement in tracking and 5% improvement in photometric rendering quality compared to the current 3DGS SLAM state-of-the-art, while allowing real-time rendering of a high-resolution dense 3D map. Project Webpage: <a href="https://vita-group.github.io/MM3DGS-SLAM">https://vita-group.github.io/MM3DGS-SLAM</a> </p><p><a href="http://arxiv.org/abs/2404.00923v1">PDF</a> Project Webpage: <a href="https://vita-group.github.io/MM3DGS-SLAM">https://vita-group.github.io/MM3DGS-SLAM</a></p><p><strong>Summary</strong><br>åˆ©ç”¨æœªå®šä½ç›¸æœºå›¾åƒå’Œæƒ¯æ€§æµ‹é‡ï¼Œ3Dé«˜æ–¯åœ°å›¾è¡¨ç¤ºå¯å®ç°å‡†ç¡®çš„SLAMã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>3Dé«˜æ–¯ç”¨äºåœ°å›¾è¡¨ç¤ºï¼Œæ— éœ€å®šä½ç›¸æœºå›¾åƒå’Œæƒ¯æ€§æµ‹é‡å³å¯å®ç°å‡†ç¡®çš„SLAMã€‚</li><li>MM3DGSè§£å†³äº†åŸºäºç¥ç»è¾å°„åœºçš„å…ˆå‰è¡¨ç¤ºçš„å±€é™æ€§ï¼Œå®ç°äº†æ›´å¿«çš„æ¸²æŸ“ã€å°ºåº¦æ„ŸçŸ¥å’Œæ”¹è¿›çš„è½¨è¿¹è·Ÿè¸ªã€‚</li><li>æ¡†æ¶ä½¿ç”¨æŸå¤±å‡½æ•°å¯ç”¨åŸºäºå…³é”®å¸§çš„æ˜ å°„å’Œè·Ÿè¸ªï¼Œè¯¥æŸå¤±å‡½æ•°ç»“åˆäº†é¢„å…ˆé›†æˆçš„æƒ¯æ€§æµ‹é‡ã€æ·±åº¦ä¼°è®¡å’Œå…‰åº¦æ¸²æŸ“è´¨é‡åº¦é‡ä¸­çš„ç›¸å¯¹ä½å§¿å˜æ¢ã€‚</li><li>å‘å¸ƒäº†ä»é…å¤‡ç…§ç›¸æœºå’Œæƒ¯æ€§æµ‹é‡å•å…ƒçš„ç§»åŠ¨æœºå™¨äººæ”¶é›†çš„å¤šæ¨¡æ€æ•°æ®é›†UT-MMã€‚</li><li>åœ¨æ•°æ®é›†ä¸­çš„å¤šä¸ªåœºæ™¯ä¸Šè¿›è¡Œçš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œä¸å½“å‰3DGS SLAMæœ€å…ˆè¿›æŠ€æœ¯ç›¸æ¯”ï¼ŒMM3DGSåœ¨è·Ÿè¸ªæ–¹é¢æé«˜äº†3å€ï¼Œåœ¨å…‰åº¦æ¸²æŸ“è´¨é‡æ–¹é¢æé«˜äº†5%ï¼ŒåŒæ—¶å…è®¸å®æ—¶æ¸²æŸ“é«˜åˆ†è¾¨ç‡å¯†é›†3Dåœ°å›¾ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šMM3DGSSLAMï¼šä½¿ç”¨è§†è§‰ã€æ·±åº¦å’Œæƒ¯æ€§æµ‹é‡è¿›è¡Œ SLAM çš„å¤šæ¨¡æ€ 3D é«˜æ–¯æ–‘ç‚¹</li><li>ä½œè€…ï¼šLisong C. Sunã€Neel P. Bhattã€Jonathan C. Liuã€Zhiwen Fanã€Zhangyang Wangã€Todd E. Humphreysã€Ufuk Topcu</li><li>æ‰€å±æœºæ„ï¼šå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡</li><li>å…³é”®è¯ï¼šSLAMã€3D é‡å»ºã€ç¥ç»è¾å°„åœºã€é«˜æ–¯è¿‡ç¨‹ã€å¤šæ¨¡æ€ä¼ æ„Ÿå™¨</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://vita-group.github.io/MM3DGS-SLAM   Github ä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼š   (1)ï¼šç ”ç©¶èƒŒæ™¯ï¼šSLAM æ˜¯ç”Ÿæˆç¯å¢ƒåœ°å›¾å¹¶ä¼°è®¡ä¼ æ„Ÿå™¨ä½å§¿çš„ä»»åŠ¡ï¼Œåœ¨è‡ªåŠ¨é©¾é©¶ã€å¢å¼ºç°å®å’Œè‡ªä¸»ç§»åŠ¨æœºå™¨äººç­‰åº”ç”¨ä¸­è‡³å…³é‡è¦ã€‚3D åœºæ™¯é‡å»ºå’Œä¼ æ„Ÿå™¨å®šä½æ˜¯è‡ªä¸»ç³»ç»Ÿæ‰§è¡Œå†³ç­–å’Œå¯¼èˆªç­‰ä¸‹æ¸¸ä»»åŠ¡çš„å…³é”®èƒ½åŠ›ã€‚   (2)ï¼šè¿‡å»çš„æ–¹æ³•å’Œé—®é¢˜ï¼šä½¿ç”¨ç¨€ç–ç‚¹äº‘è¿›è¡Œ SLAM çš„æ–¹æ³•è™½ç„¶å…·æœ‰æœ€å…ˆè¿›çš„è·Ÿè¸ªç²¾åº¦ï¼Œä½†ç”±äºç¨€ç–æ€§è€Œå¯¼è‡´ç”Ÿæˆçš„åœ°å›¾æ˜¯æ–­å¼€çš„ï¼Œå¹¶ä¸”åœ¨è§†è§‰ä¸Šä¸å¦‚è¾ƒæ–°çš„ 3D é‡å»ºæ–¹æ³•ã€‚è™½ç„¶è§†è§‰è´¨é‡å¯¹äºå¯¼èˆªç›®çš„æ— å…³ç´§è¦ï¼Œä½†åˆ›å»ºé€¼çœŸçš„åœ°å›¾å¯¹äºäººå·¥æ¶ˆè´¹ã€è¯­ä¹‰åˆ†å‰²å’Œåå¤„ç†å¾ˆæœ‰ä»·å€¼ã€‚åŸºäºç¥ç»è¾å°„åœºçš„ SLAM æ–¹æ³•å¯ä»¥ç”Ÿæˆé€¼çœŸçš„ 3D åœ°å›¾ï¼Œä½†å­˜åœ¨æ¸²æŸ“é€Ÿåº¦æ…¢ã€ç¼ºä¹å°ºåº¦æ„ŸçŸ¥å’Œè½¨è¿¹è·Ÿè¸ªç²¾åº¦ä½çš„é—®é¢˜ã€‚   (3)ï¼šæœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šMM3DGS æ˜¯ä¸€ç§å¤šæ¨¡æ€ 3D é«˜æ–¯æ–‘ç‚¹ SLAM æ–¹æ³•ï¼Œå®ƒé€šè¿‡ä½¿ç”¨ 3D é«˜æ–¯æ–‘ç‚¹è¿›è¡Œåœ°å›¾è¡¨ç¤ºæ¥è§£å†³åŸºäºç¥ç»è¾å°„åœºçš„ SLAM çš„å±€é™æ€§ã€‚MM3DGS åˆ©ç”¨é¢„å…ˆé›†æˆçš„æƒ¯æ€§æµ‹é‡ã€æ·±åº¦ä¼°è®¡å’Œå…‰åº¦æ¸²æŸ“è´¨é‡åº¦é‡æ¥æ‰§è¡ŒåŸºäºå…³é”®å¸§çš„æ˜ å°„å’Œè·Ÿè¸ªã€‚   (4)ï¼šæ–¹æ³•çš„æ€§èƒ½ï¼šåœ¨ UT-MM æ•°æ®é›†ä¸Šçš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œä¸å½“å‰æœ€å…ˆè¿›çš„ 3DGSSLAM ç›¸æ¯”ï¼ŒMM3DGS åœ¨è·Ÿè¸ªæ–¹é¢æé«˜äº† 3 å€ï¼Œåœ¨å…‰åº¦æ¸²æŸ“è´¨é‡æ–¹é¢æé«˜äº† 5%ï¼ŒåŒæ—¶å…è®¸å®æ—¶æ¸²æŸ“é«˜åˆ†è¾¨ç‡å¯†é›† 3D åœ°å›¾ã€‚</li></ol><p><strong>æ–¹æ³•</strong></p><p>ï¼ˆ1ï¼‰<strong>å¤šæ¨¡æ€æ•°æ®èåˆï¼š</strong>MM3DGS åˆ©ç”¨è§†è§‰ã€æ·±åº¦å’Œæƒ¯æ€§æµ‹é‡æ•°æ®è¿›è¡Œå¤šæ¨¡æ€èåˆï¼Œä»¥å¢å¼º SLAM çš„é²æ£’æ€§å’Œå‡†ç¡®æ€§ã€‚</p><p>ï¼ˆ2ï¼‰<strong>3D é«˜æ–¯æ–‘ç‚¹åœ°å›¾è¡¨ç¤ºï¼š</strong>MM3DGS ä½¿ç”¨ 3D é«˜æ–¯æ–‘ç‚¹å¯¹ç¯å¢ƒè¿›è¡Œå»ºæ¨¡ï¼Œè§£å†³äº†åŸºäºç¥ç»è¾å°„åœºçš„ SLAM æ–¹æ³•ä¸­æ¸²æŸ“é€Ÿåº¦æ…¢å’Œç¼ºä¹å°ºåº¦æ„ŸçŸ¥çš„é—®é¢˜ã€‚</p><p>ï¼ˆ3ï¼‰<strong>å…³é”®å¸§æ˜ å°„å’Œè·Ÿè¸ªï¼š</strong>MM3DGS é‡‡ç”¨åŸºäºå…³é”®å¸§çš„æ–¹æ³•è¿›è¡Œæ˜ å°„å’Œè·Ÿè¸ªã€‚å®ƒåˆ©ç”¨é¢„å…ˆé›†æˆçš„æƒ¯æ€§æµ‹é‡ã€æ·±åº¦ä¼°è®¡å’Œå…‰åº¦æ¸²æŸ“è´¨é‡åº¦é‡æ¥é€‰æ‹©å…³é”®å¸§ï¼Œå¹¶ä½¿ç”¨ 3D é«˜æ–¯æ–‘ç‚¹æ›´æ–°åœ°å›¾ã€‚</p><p>ï¼ˆ4ï¼‰<strong>å…‰åº¦æ¸²æŸ“è´¨é‡åº¦é‡ï¼š</strong>MM3DGS å¼•å…¥äº†å…‰åº¦æ¸²æŸ“è´¨é‡åº¦é‡ï¼Œä»¥è¯„ä¼°ç”Ÿæˆåœ°å›¾çš„è§†è§‰è´¨é‡ã€‚è¿™æœ‰åŠ©äºæé«˜åœ°å›¾çš„è§†è§‰ä¿çœŸåº¦ã€‚</p><p>ï¼ˆ5ï¼‰<strong>å®æ—¶æ¸²æŸ“ï¼š</strong>MM3DGS å®ç°äº†å®æ—¶æ¸²æŸ“é«˜åˆ†è¾¨ç‡å¯†é›† 3D åœ°å›¾ã€‚è¿™ä½¿å¾—ç³»ç»Ÿèƒ½å¤Ÿåœ¨æ‰§è¡Œ SLAM çš„åŒæ—¶æä¾›é€¼çœŸçš„åœ°å›¾å¯è§†åŒ–ã€‚</p><ol><li>æ€»ç»“(1): <strong>æœ¬å·¥ä½œçš„æ„ä¹‰ï¼š</strong>MM3DGS æ˜¯ä¸€ç§å¤šæ¨¡æ€ 3D é«˜æ–¯æ–‘ç‚¹ SLAM æ–¹æ³•ï¼Œå®ƒé€šè¿‡ä½¿ç”¨ 3D é«˜æ–¯æ–‘ç‚¹è¿›è¡Œåœ°å›¾è¡¨ç¤ºæ¥è§£å†³åŸºäºç¥ç»è¾å°„åœºçš„ SLAM çš„å±€é™æ€§ï¼Œå®ç°äº†è·Ÿè¸ªç²¾åº¦æé«˜ 3 å€ï¼Œå…‰åº¦æ¸²æŸ“è´¨é‡æé«˜ 5%ï¼ŒåŒæ—¶å…è®¸å®æ—¶æ¸²æŸ“é«˜åˆ†è¾¨ç‡å¯†é›† 3D åœ°å›¾ã€‚(2): <strong>ä¼˜ç¼ºç‚¹æ€»ç»“ï¼š</strong><strong>åˆ›æ–°ç‚¹ï¼š</strong></li><li>ä½¿ç”¨ 3D é«˜æ–¯æ–‘ç‚¹è¿›è¡Œåœ°å›¾è¡¨ç¤ºï¼Œè§£å†³äº†æ¸²æŸ“é€Ÿåº¦æ…¢å’Œç¼ºä¹å°ºåº¦æ„ŸçŸ¥çš„é—®é¢˜ã€‚</li><li>å¼•å…¥äº†å…‰åº¦æ¸²æŸ“è´¨é‡åº¦é‡ï¼Œæé«˜äº†åœ°å›¾çš„è§†è§‰ä¿çœŸåº¦ã€‚</li><li>å®ç°å®æ—¶æ¸²æŸ“é«˜åˆ†è¾¨ç‡å¯†é›† 3D åœ°å›¾ã€‚<strong>æ€§èƒ½ï¼š</strong></li><li>åœ¨è·Ÿè¸ªæ–¹é¢æé«˜äº† 3 å€ï¼Œåœ¨å…‰åº¦æ¸²æŸ“è´¨é‡æ–¹é¢æé«˜äº† 5%ã€‚</li><li>å…è®¸å®æ—¶æ¸²æŸ“é«˜åˆ†è¾¨ç‡å¯†é›† 3D åœ°å›¾ã€‚<strong>å·¥ä½œé‡ï¼š</strong></li><li>éœ€è¦é¢„å…ˆé›†æˆæƒ¯æ€§æµ‹é‡ã€æ·±åº¦ä¼°è®¡å’Œå…‰åº¦æ¸²æŸ“è´¨é‡åº¦é‡ã€‚</li><li>æ¸²æŸ“é«˜åˆ†è¾¨ç‡å¯†é›† 3D åœ°å›¾éœ€è¦è¾ƒé«˜çš„è®¡ç®—èµ„æºã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-e555dce577db7ee48c6d0700898f8354.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-45584721725016a544b3438b3b4e3524.jpg" align="middle"><img src="https://picx.zhimg.com/v2-efe4c6fcb1b7199ca40bd96e51c223a5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a82f468f62090c014e63f7697e14bafd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5bd070bb451dced5c2cc58f655ff4729.jpg" align="middle"></details><h2 id="Marrying-NeRF-with-Feature-Matching-for-One-step-Pose-Estimation"><a href="#Marrying-NeRF-with-Feature-Matching-for-One-step-Pose-Estimation" class="headerlink" title="Marrying NeRF with Feature Matching for One-step Pose Estimation"></a>Marrying NeRF with Feature Matching for One-step Pose Estimation</h2><p><strong>Authors:Ronghan Chen, Yang Cong, Yu Ren</strong></p><p>Given the image collection of an object, we aim at building a real-time image-based pose estimation method, which requires neither its CAD model nor hours of object-specific training. Recent NeRF-based methods provide a promising solution by directly optimizing the pose from pixel loss between rendered and target images. However, during inference, they require long converging time, and suffer from local minima, making them impractical for real-time robot applications. We aim at solving this problem by marrying image matching with NeRF. With 2D matches and depth rendered by NeRF, we directly solve the pose in one step by building 2D-3D correspondences between target and initial view, thus allowing for real-time prediction. Moreover, to improve the accuracy of 2D-3D correspondences, we propose a 3D consistent point mining strategy, which effectively discards unfaithful points reconstruted by NeRF. Moreover, current NeRF-based methods naively optimizing pixel loss fail at occluded images. Thus, we further propose a 2D matches based sampling strategy to preclude the occluded area. Experimental results on representative datasets prove that our method outperforms state-of-the-art methods, and improves inference efficiency by 90x, achieving real-time prediction at 6 FPS. </p><p><a href="http://arxiv.org/abs/2404.00891v1">PDF</a> ICRA, 2024. Video <a href="https://www.youtube.com/watch?v=70fgUobOFWo">https://www.youtube.com/watch?v=70fgUobOFWo</a></p><p><strong>Summary</strong><br>å•ç›®ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å›¾åƒåŒ¹é…å®æ—¶ç‰©ä½“å§¿æ€ä¼°è®¡æ–¹æ³•</p><p><strong>Key Takeaways</strong></p><ul><li>åˆ©ç”¨å›¾åƒåŒ¹é…å’ŒNeRFç»“åˆå®ç°å•ç›®ç‰©ä½“å§¿æ€ä¼°è®¡</li><li>æå‡ºåŸºäº3Dä¸€è‡´æ€§çš„ç‚¹æŒ–æ˜ç­–ç•¥ä»¥æé«˜2D-3Då¯¹åº”ç²¾åº¦</li><li>åˆ©ç”¨2DåŒ¹é…é‡‡æ ·ç­–ç•¥æ’é™¤è¢«é®æŒ¡åŒºåŸŸ</li><li>ç›´æ¥æ±‚è§£ä½å§¿ï¼Œæ— éœ€æ¼«é•¿çš„ä¼˜åŒ–æ—¶é—´</li><li>å®æ—¶é¢„æµ‹é€Ÿåº¦ä¸º6 FPSï¼Œæ¯”ç°æœ‰æŠ€æœ¯æé«˜90å€</li><li>è¯¥æ–¹æ³•åœ¨å…·æœ‰ä»£è¡¨æ€§çš„æ•°æ®é›†ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½</li><li>è¯¥æ–¹æ³•é€‚ç”¨äºéœ€è¦å®æ—¶å§¿æ€ä¼°è®¡çš„æœºå™¨äººåº”ç”¨</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šå°† NeRF ä¸ç‰¹å¾åŒ¹é…ç»“åˆç”¨äºä¸€æ­¥åˆ°ä½å§¿åŠ¿ä¼°è®¡</li><li>ä½œè€…ï¼šé™ˆè£ç¿°ã€ä¸›é˜³ã€ä»»å®‡</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šä¸­ç§‘é™¢æ²ˆé˜³è‡ªåŠ¨åŒ–ç ”ç©¶æ‰€æœºå™¨äººå­¦å›½å®¶é‡ç‚¹å®éªŒå®¤</li><li>å…³é”®è¯ï¼šNeRFã€å§¿åŠ¿ä¼°è®¡ã€ç‰¹å¾åŒ¹é…</li><li>è®ºæ–‡é“¾æ¥ï¼šNoneï¼ŒGithub ä»£ç é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šå›¾åƒé©±åŠ¨çš„ç‰©ä½“å§¿æ€ä¼°è®¡åœ¨æœºå™¨äººæ“ä½œã€å¢å¼ºç°å®å’Œç§»åŠ¨æœºå™¨äººé¢†åŸŸæœ‰ç€å¹¿æ³›çš„åº”ç”¨ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸éœ€è¦ç‰©ä½“çš„ CAD æ¨¡å‹ï¼Œå¹¶ä¸”éœ€è¦æœç´¢é¢„å…ˆæ³¨å†Œå›¾åƒæˆ–æ¨¡æ¿ä¸ç›®æ ‡å›¾åƒä¹‹é—´çš„ç‰¹å¾ã€‚ç„¶è€Œï¼Œè·å–é«˜è´¨é‡çš„ CAD æ¨¡å‹å¯èƒ½å¾ˆå›°éš¾ä¸”è€—è´¹äººåŠ›ï¼Œæˆ–è€…éœ€è¦ä¸“é—¨çš„é«˜ç«¯æ‰«æä»ªã€‚(2) è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šæœ€è¿‘çš„æ–¹æ³•å·²å°†æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨äºå›å½’å§¿æ€ã€‚ç„¶è€Œï¼Œå®ƒä»¬åªèƒ½ä¼°è®¡å·²çŸ¥å®ä¾‹çš„å§¿æ€æˆ–åŒä¸€ç±»åˆ«ä¸­ç›¸ä¼¼å®ä¾‹çš„å§¿æ€ï¼Œå¹¶ä¸”å¿…é¡»é’ˆå¯¹æ–°ç‰©ä½“è¿›è¡Œæ•°å°æ—¶çš„é‡æ–°è®­ç»ƒã€‚æ­¤å¤–ï¼Œå®ƒä»¬éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®ï¼Œè€Œè¿™äº›æ•°æ®æ”¶é›†å’Œæ³¨é‡Šèµ·æ¥å¾ˆç¹çã€‚ä¸ºäº†è¿›ä¸€æ­¥é¿å…é’ˆå¯¹æ¯ä¸ªæ–°ç‰©ä½“è¿›è¡Œç¹ççš„é‡æ–°è®­ç»ƒï¼Œæœ€è¿‘çš„æ–¹æ³•ä» SfMï¼ˆè¿åŠ¨ç»“æ„ï¼‰çš„ä¼ ç»Ÿç®¡é“ä¸­å­¦ä¹ ï¼Œé€šè¿‡ç‰¹å¾åŒ¹é…æ¥ä¼°è®¡ç‰©ä½“å§¿æ€ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä¾èµ–äºåœ¨æ‰€æœ‰è¾“å…¥å¸§ä¸­å½¢æˆç¨³å®šå¯é‡å¤çš„å¯¹åº”å…³ç³»ï¼Œè¿™é€šå¸¸æ— æ³•ä¿è¯ï¼Œä»è€Œå¯¼è‡´è¾ƒå¤§çš„å§¿æ€è¯¯å·®ã€‚(3) æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šå¦ä¸€æ–¹é¢ï¼ŒNeRFï¼ˆç¥ç»è¾å°„åœºï¼‰çš„æœ€æ–°è¿›å±•æä¾›äº†ä¸€ç§æ•è·å¤æ‚ 3D å‡ ä½•å½¢çŠ¶çš„æœºåˆ¶ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå°† NeRF ä¸ç‰¹å¾åŒ¹é…ç›¸ç»“åˆï¼Œç”¨äºä¸€æ­¥åˆ°ä½å§¿åŠ¿ä¼°è®¡ã€‚è¯¥æ–¹æ³•é€šè¿‡æ„å»ºç›®æ ‡è§†å›¾å’Œåˆå§‹è§†å›¾ä¹‹é—´çš„ 2D-3D å¯¹åº”å…³ç³»ï¼Œç›´æ¥æ±‚è§£å§¿æ€ï¼Œä»è€Œå®ç°å®æ—¶é¢„æµ‹ã€‚æ­¤å¤–ï¼Œä¸ºäº†æé«˜ 2D-3D å¯¹åº”å…³ç³»çš„å‡†ç¡®æ€§ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ 3D ä¸€è‡´ç‚¹æŒ–æ˜ç­–ç•¥ï¼Œè¯¥ç­–ç•¥å¯ä»¥æœ‰æ•ˆåœ°ä¸¢å¼ƒ NeRF é‡å»ºçš„ä¸çœŸå®ç‚¹ã€‚(4) æ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†ä»€ä¹ˆæ€§èƒ½ï¼šå®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¹¶å°†æ¨ç†æ•ˆç‡æé«˜äº† 90 å€ï¼Œå®ç°äº† 6FPS çš„å®æ—¶é¢„æµ‹ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1): æ„å»ºç›®æ ‡è§†å›¾å’Œåˆå§‹è§†å›¾ä¹‹é—´çš„ 2D-3D å¯¹åº”å…³ç³»ï¼Œç›´æ¥æ±‚è§£å§¿æ€ï¼›(2): æå‡º 3D ä¸€è‡´ç‚¹æŒ–æ˜ç­–ç•¥ï¼Œä¸¢å¼ƒ NeRF é‡å»ºçš„ä¸çœŸå®ç‚¹ï¼Œæé«˜ 2D-3D å¯¹åº”å…³ç³»çš„å‡†ç¡®æ€§ï¼›(3): å°† NeRF ä¸ç‰¹å¾åŒ¹é…ç›¸ç»“åˆï¼Œä¸€æ­¥åˆ°ä½æ±‚è§£å§¿æ€ï¼Œå®ç°å®æ—¶é¢„æµ‹ï¼›(4): é‡‡ç”¨ 40 æ­¥åä¼˜åŒ–ï¼Œè¿›ä¸€æ­¥æå‡å§¿æ€ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäº NeRF çš„å¿«é€Ÿå›¾åƒé©±åŠ¨ã€æ—  CAD æ–°ç‰©ä½“å§¿æ€ä¼°è®¡æ¡†æ¶ã€‚é€šè¿‡å¼•å…¥å…³é”®ç‚¹åŒ¹é…ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥ç›´æ¥ä¸€æ­¥æ±‚è§£å§¿æ€ï¼Œå¹¶ä¸”ä¸å—é•¿æ—¶é—´ä¼˜åŒ–å’Œå±€éƒ¨æœ€å°å€¼çš„å½±å“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ 3D ä¸€è‡´ç‚¹æŒ–æ˜ç­–ç•¥æ¥æé«˜ 2D-3D å¯¹åº”å…³ç³»çš„è´¨é‡ï¼Œä»¥åŠä¸€ç§åŸºäºåŒ¹é…å…³é”®ç‚¹çš„é‡‡æ ·ç­–ç•¥æ¥æé«˜å¯¹é®æŒ¡å›¾åƒçš„é²æ£’æ€§ã€‚å®éªŒè¡¨æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„ä¼˜è¶Šæ€§èƒ½å’Œå¯¹é®æŒ¡çš„é²æ£’æ€§ã€‚å¯¹äºæœªæ¥çš„å·¥ä½œï¼Œæˆ‘ä»¬å¸Œæœ›è¯¥æ–¹æ³•å¯ä»¥æ‰©å±•åˆ°æœºå™¨äººæ“ä½œæˆ–æœ€è¿‘åŸºäºç¥ç»åœºçš„ SLAM ä»»åŠ¡ [36]ã€[51]â€“[54]ï¼Œä»¥æé«˜å®šä½çš„æ•ˆç‡æé™ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šå°† NeRF ä¸ç‰¹å¾åŒ¹é…ç›¸ç»“åˆï¼Œä¸€æ­¥åˆ°ä½æ±‚è§£å§¿æ€ï¼›æå‡º 3D ä¸€è‡´ç‚¹æŒ–æ˜ç­–ç•¥ï¼Œæé«˜ 2D-3D å¯¹åº”å…³ç³»çš„å‡†ç¡®æ€§ï¼›åŸºäºåŒ¹é…å…³é”®ç‚¹çš„é‡‡æ ·ç­–ç•¥ï¼Œæé«˜å¯¹é®æŒ¡å›¾åƒçš„é²æ£’æ€§ã€‚æ€§èƒ½ï¼šä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œæ¨ç†æ•ˆç‡æé«˜ 90 å€ï¼Œå®ç° 6FPS çš„å®æ—¶é¢„æµ‹ã€‚å·¥ä½œé‡ï¼šéœ€è¦æ„å»ºç›®æ ‡è§†å›¾å’Œåˆå§‹è§†å›¾ä¹‹é—´çš„ 2D-3D å¯¹åº”å…³ç³»ï¼Œå¹¶è¿›è¡Œ 40 æ­¥åä¼˜åŒ–ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-c945c9d575f76d39cd87ae54b10755b6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f9604c4b56914b94028dfc9542a10656.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-140c2b41b6b6fbcdf4d3c7b1eeb46dc2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c1ad0e80ab82bfabe091780a98abbeec.jpg" align="middle"></details><h2 id="DPA-Net-Structured-3D-Abstraction-from-Sparse-Views-via-Differentiable-Primitive-Assembly"><a href="#DPA-Net-Structured-3D-Abstraction-from-Sparse-Views-via-Differentiable-Primitive-Assembly" class="headerlink" title="DPA-Net: Structured 3D Abstraction from Sparse Views via Differentiable   Primitive Assembly"></a>DPA-Net: Structured 3D Abstraction from Sparse Views via Differentiable   Primitive Assembly</h2><p><strong>Authors:Fenggen Yu, Yiming Qian, Xu Zhang, Francisca Gil-Ureta, Brian Jackson, Eric Bennett, Hao Zhang</strong></p><p>We present a differentiable rendering framework to learn structured 3D abstractions in the form of primitive assemblies from sparse RGB images capturing a 3D object. By leveraging differentiable volume rendering, our method does not require 3D supervision. Architecturally, our network follows the general pipeline of an image-conditioned neural radiance field (NeRF) exemplified by pixelNeRF for color prediction. As our core contribution, we introduce differential primitive assembly (DPA) into NeRF to output a 3D occupancy field in place of density prediction, where the predicted occupancies serve as opacity values for volume rendering. Our network, coined DPA-Net, produces a union of convexes, each as an intersection of convex quadric primitives, to approximate the target 3D object, subject to an abstraction loss and a masking loss, both defined in the image space upon volume rendering. With test-time adaptation and additional sampling and loss designs aimed at improving the accuracy and compactness of the obtained assemblies, our method demonstrates superior performance over state-of-the-art alternatives for 3D primitive abstraction from sparse views. </p><p><a href="http://arxiv.org/abs/2404.00875v2">PDF</a> 14 pages</p><p><strong>Summary</strong><br>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰èå…¥å¯å¾®åˆ†åŸºå…ƒç»„è£…ï¼Œç›´æ¥è¾“å‡º3Då æœ‰ç‡åœºï¼Œæ— éœ€3Dç›‘ç£ï¼Œå®ç°ä»ç¨€ç–RGBå›¾åƒå­¦ä¹ æŠ½è±¡3Dç»“æ„ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>é‡‡ç”¨å¯å¾®åˆ†ä½“ç´ æ¸²æŸ“ï¼Œæ— éœ€3Dç›‘ç£ã€‚</li><li>æ¶æ„éµå¾ªåŸºäºå›¾åƒçš„NeRFç®¡é“ï¼Œé¢„æµ‹é¢œè‰²ã€‚</li><li>æ ¸å¿ƒè´¡çŒ®ï¼šå°†å¯å¾®åˆ†åŸºå…ƒç»„è£…å¼•å…¥NeRFï¼Œè¾“å‡º3Då æœ‰ç‡åœºã€‚</li><li>é¢„æµ‹çš„å æœ‰ç‡ç”¨ä½œä½“ç´ æ¸²æŸ“çš„ä¸é€æ˜åº¦å€¼ã€‚</li><li>DPAç½‘ç»œç”Ÿæˆå‡¸é›†å¹¶é›†ï¼Œé€¼è¿‘ç›®æ ‡3Dç‰©ä½“ã€‚</li><li>æŸå¤±å‡½æ•°åŒ…æ‹¬å›¾åƒç©ºé—´ä¸­çš„æŠ½è±¡æŸå¤±å’Œé®ç½©æŸå¤±ã€‚</li><li>æµ‹è¯•æ—¶è‡ªé€‚åº”ã€é¢å¤–é‡‡æ ·å’ŒæŸå¤±è®¾è®¡ï¼Œæé«˜ç»„è£…ç²¾åº¦å’Œç´§å‡‘æ€§ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šDPA-Netï¼šé€šè¿‡å¯å¾®åˆ†åŸºå…ƒè£…é…ä»ç¨€ç–è§†å›¾ä¸­è¿›è¡Œç»“æ„åŒ– 3D æŠ½è±¡</li><li>ä½œè€…ï¼šFenggen Yuã€Yiming Qianã€Xu Zhangã€Francisca Gil-Uretaã€Brian Jacksonã€Eric Bennettã€Hao Zhang</li><li>éš¶å±å•ä½ï¼šäºšé©¬é€Š</li><li>å…³é”®è¯ï¼š3D æŠ½è±¡ã€ç¨€ç–è§†å›¾ã€å¯å¾®åˆ†ä½“æ¸²æŸ“ã€ç¥ç»è¾å°„åœº</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.00875</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šä»å•è§†å›¾æˆ–å¤šè§†å›¾å›¾åƒä¸­è¿›è¡Œ 3D æ¨ç†ï¼ˆä¾‹å¦‚æŠ½è±¡æˆ–é‡å»ºï¼‰æ˜¯è®¡ç®—æœºè§†è§‰ä¸­æœ€åŸºæœ¬çš„é—®é¢˜ä¹‹ä¸€ã€‚éšç€ç¥ç»åœºï¼ˆå°¤å…¶æ˜¯ç¥ç»è¾å°„åœºå’Œ 3D é«˜æ–¯ splattingï¼‰çš„å‡ºç°ï¼Œ3D é‡å»ºçš„è´¨é‡ã€é€Ÿåº¦ä»¥åŠå¤„ç†ç¨€ç–è§†å›¾ï¼ˆè€Œä¸æ˜¯æ—©æœŸå·¥ä½œä¸­çš„å¯†é›†è¾“å…¥è§†å›¾ï¼‰çš„èƒ½åŠ›éƒ½å¾—åˆ°äº†å¿«é€Ÿå‘å±•ã€‚ä½†æ˜¯ï¼ŒNeRF åŠå…¶å¤§å¤šæ•°å˜ä½“åœ¨è®¾è®¡ä¸Šéƒ½ä»¥æ–°é¢–è§†å›¾åˆæˆä¸ºç›®æ ‡ï¼Œé‡ç‚¹åœ¨äºä¼˜åŒ–å…¶åŸºå…ƒä»¥æé«˜æ¸²æŸ“æ€§èƒ½ï¼Œè€Œä¸æ˜¯æœåŠ¡äºæ¶‰åŠå½¢çŠ¶å»ºæ¨¡æˆ–æ“ä½œçš„ä¸‹æ¸¸ä»»åŠ¡ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•ï¼šæœ€è¿‘æå‡ºäº†ä¸€äº›é€šè¿‡å­¦ä¹ åŸºå…ƒè£…é…ï¼ˆä¾‹å¦‚æ„é€ å®ä½“å‡ ä½•æ ‘ã€è‰å›¾æŒ¤å‡ºæ¨¡å‹æˆ–å½¢çŠ¶ç¨‹åºï¼‰è¿›è¡Œ CAD å»ºæ¨¡çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™äº›ç¥ç»æ¨¡å‹éƒ½é‡‡ç”¨ä½“ç´ å’Œç‚¹äº‘ç­‰ 3D è¾“å…¥ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§å¯å¾®åˆ†æ¸²æŸ“æ¡†æ¶ï¼Œç”¨äºä»æ•è· 3D ç‰©ä½“çš„ç¨€ç– RGB å›¾åƒä¸­ä»¥åŸºå…ƒè£…é…çš„å½¢å¼å­¦ä¹ ç»“æ„åŒ– 3D æŠ½è±¡ã€‚é€šè¿‡åˆ©ç”¨å¯å¾®åˆ†ä½“æ¸²æŸ“ï¼Œæœ¬æ–‡æ–¹æ³•ä¸éœ€è¦ 3D ç›‘ç£ã€‚åœ¨æ¶æ„ä¸Šï¼Œæœ¬æ–‡ç½‘ç»œéµå¾ªä»¥ pixelNeRF ä¸ºä¾‹çš„å›¾åƒæ¡ä»¶ç¥ç»è¾å°„åœºçš„ä¸€èˆ¬ç®¡é“è¿›è¡Œé¢œè‰²é¢„æµ‹ã€‚ä½œä¸ºæ ¸å¿ƒè´¡çŒ®ï¼Œæœ¬æ–‡å°†å¯å¾®åˆ†åŸºå…ƒè£…é…å¼•å…¥ NeRFï¼Œä»¥è¾“å‡º 3D å ç”¨åœºæ¥ä»£æ›¿å¯†åº¦é¢„æµ‹ï¼Œå…¶ä¸­é¢„æµ‹çš„å ç”¨ç‡ç”¨ä½œä½“ç§¯æ¸²æŸ“çš„ä¸é€æ˜åº¦å€¼ã€‚æœ¬æ–‡ç½‘ç»œç§°ä¸º DPA-Netï¼Œå®ƒç”Ÿæˆå‡¸é›†çš„å¹¶é›†ï¼Œæ¯ä¸ªå‡¸é›†éƒ½æ˜¯å‡¸äºŒæ¬¡åŸºå…ƒçš„äº¤é›†ï¼Œä»¥è¿‘ä¼¼ç›®æ ‡ 3D å¯¹è±¡ï¼Œå—æŠ½è±¡æŸå¤±å’Œæ©ç æŸå¤±çš„çº¦æŸï¼Œä¸¤è€…éƒ½åœ¨ä½“ç§¯æ¸²æŸ“æ—¶åœ¨å›¾åƒç©ºé—´ä¸­å®šä¹‰ã€‚é€šè¿‡æµ‹è¯•æ—¶é€‚åº”ä»¥åŠæ—¨åœ¨æé«˜æ‰€è·å¾—è£…é…çš„å‡†ç¡®æ€§å’Œç´§å‡‘æ€§çš„é™„åŠ é‡‡æ ·å’ŒæŸå¤±è®¾è®¡ï¼Œæœ¬æ–‡æ–¹æ³•å±•ç¤ºäº†ä»ç¨€ç–è§†å›¾ä¸­è¿›è¡Œ 3D åŸºå…ƒæŠ½è±¡çš„æœ€æ–°æ›¿ä»£æ–¹æ¡ˆçš„ä¼˜è¶Šæ€§èƒ½ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šåœ¨ ShapeNet å’Œ PartNet æ•°æ®é›†ä¸Šï¼Œæœ¬æ–‡æ–¹æ³•åœ¨å‡†ç¡®æ€§å’Œç´§å‡‘æ€§æ–¹é¢éƒ½ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚è¿™äº›æ€§èƒ½æ”¯æŒæœ¬æ–‡ç›®æ ‡ï¼Œå³ä»ç¨€ç–è§†å›¾ä¸­å­¦ä¹ ç»“æ„åŒ– 3D æŠ½è±¡ï¼Œä»¥ä¿ƒè¿›ä¸‹æ¸¸å½¢çŠ¶å»ºæ¨¡å’Œæ“ä½œä»»åŠ¡ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1): ç‰¹å¾æå–å’Œèšåˆï¼›(2): åŸå§‹è£…é…ï¼š</p><ul><li>åŸå§‹å‚æ•°åŒ–ï¼š</li><li>åŸå§‹äº¤é›†ï¼š</li><li>å‡¸é›†å¹¶é›†ï¼š(3): å¯å¾®åˆ†æ¸²æŸ“ï¼›(4): ç½‘ç»œè®­ç»ƒå’Œæµ‹è¯•æ—¶è‡ªé€‚åº”ï¼š</li><li>é¢„è®­ç»ƒï¼š</li><li>æµ‹è¯•æ—¶è‡ªé€‚åº”ï¼ˆTTAï¼‰ï¼š<ul><li>ç¬¬ä¸€é˜¶æ®µï¼š</li><li>ç¬¬äºŒé˜¶æ®µï¼š</li><li>ç¬¬ä¸‰é˜¶æ®µï¼š</li></ul></li></ul></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§å¯å¾®åˆ†æ¸²æŸ“æ¡†æ¶ DPA-Netï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿä»ä»…æœ‰çš„å‡ ä¸ªï¼ˆä¾‹å¦‚ä¸‰ä¸ªï¼‰RGB å›¾åƒä¸­ä»¥åŸºå…ƒè£…é…çš„å½¢å¼å­¦ä¹ ç»“æ„åŒ–çš„ 3D æŠ½è±¡ï¼Œè¿™äº›å›¾åƒæ˜¯åœ¨éå¸¸ä¸åŒçš„è§†è§’ä¸‹æ‹æ‘„çš„ã€‚æˆ‘ä»¬çš„å…³é”®åˆ›æ–°æ˜¯å°†å¯å¾®åˆ†åŸºå…ƒè£…é…é›†æˆåˆ° NeRF æ¶æ„ä¸­ï¼Œä»è€Œèƒ½å¤Ÿé¢„æµ‹å ç”¨ç‡ä»¥ç”¨ä½œä½“ç§¯æ¸²æŸ“çš„ä¸é€æ˜åº¦å€¼ã€‚åœ¨æ²¡æœ‰ä»»ä½• 3D æˆ–å½¢çŠ¶åˆ†è§£ç›‘ç£çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥ç”Ÿæˆä¸€ä¸ªå¯è§£é‡Šä¸”éšåå¯ç¼–è¾‘çš„å‡¸é›†å¹¶é›†ï¼Œè¯¥å¹¶é›†è¿‘ä¼¼äºç›®æ ‡ 3D å¯¹è±¡ã€‚åœ¨ ShapeNet å’Œ DTU ä¸Šçš„å®šé‡å’Œå®šæ€§è¯„ä¼°è¡¨æ˜ï¼ŒDPA-Net ä¼˜äºæœ€å…ˆè¿›çš„æ›¿ä»£æ–¹æ¡ˆã€‚å±•ç¤ºçš„åº”ç”¨ç¨‹åºè¿›ä¸€æ­¥è¡¨æ˜ï¼Œæˆ‘ä»¬å¯ç¼–è¾‘çš„ 3D æŠ½è±¡å¯ä»¥ç”¨ä½œç»“æ„æç¤ºï¼Œå¹¶æœ‰åˆ©äºå…¶ä»– 3D ç”Ÿæˆä»»åŠ¡ã€‚æˆ‘ä»¬å½“å‰çš„å®ç°åˆ©ç”¨äº† GT ç›¸æœºä½å§¿ã€‚ä¸ºäº†å‡è½»ç”±ä¼°è®¡çš„ã€å˜ˆæ‚çš„ä½å§¿å¼•èµ·çš„æ€§èƒ½ä¸‹é™ï¼Œå¯ä»¥åº”ç”¨ç°æœ‰çš„ç”¨äºè”åˆç›¸æœºåœºæ™¯ä¼˜åŒ–çš„ç°æœ‰æ–¹æ³•ï¼Œä¾‹å¦‚ [44]ã€‚ç”±äºçº¹ç†é¢„æµ‹ä¸æ˜¯æˆ‘ä»¬å·¥ä½œçš„é‡ç‚¹ï¼Œå› æ­¤éœ€è¦è¿›ä¸€æ­¥å¾®è°ƒï¼ˆä¾‹å¦‚ï¼Œåå‘è¾“å…¥è§†å›¾ï¼‰å’Œä¼˜åŒ–ä»¥æé«˜æ¸²æŸ“è´¨é‡ã€‚æœ€åï¼Œä»…ä½¿ç”¨å‡¸é›†çš„è£…é…æ˜¯æœ‰é™çš„ã€‚å¦‚è¡¥å……ææ–™æ‰€ç¤ºï¼ŒDPA-Net æ— æ³•å¾ˆå¥½åœ°å¤„ç†å‡¹å½¢ã€‚å°†å·®åˆ†è¿ç®—æ·»åŠ åˆ°å¯å¾®åˆ†è£…é…ä¸­å€¼å¾—æ¢ç´¢ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šDPA-Net å°†å¯å¾®åˆ†åŸºå…ƒè£…é…é›†æˆåˆ° NeRF æ¶æ„ä¸­ï¼Œä»è€Œèƒ½å¤Ÿé¢„æµ‹å ç”¨ç‡ä»¥ç”¨ä½œä½“ç§¯æ¸²æŸ“çš„ä¸é€æ˜åº¦å€¼ã€‚è¿™ä½¿å¾— DPA-Net èƒ½å¤Ÿä»ç¨€ç–è§†å›¾ä¸­å­¦ä¹ ç»“æ„åŒ–çš„ 3D æŠ½è±¡ï¼Œè€Œæ— éœ€ä»»ä½• 3D æˆ–å½¢çŠ¶åˆ†è§£ç›‘ç£ã€‚æ€§èƒ½ï¼šåœ¨ ShapeNet å’Œ DTU ä¸Šçš„å®šé‡å’Œå®šæ€§è¯„ä¼°è¡¨æ˜ï¼ŒDPA-Net ä¼˜äºæœ€å…ˆè¿›çš„æ›¿ä»£æ–¹æ¡ˆã€‚DPA-Net ç”Ÿæˆçš„ 3D æŠ½è±¡å‡†ç¡®ã€ç´§å‡‘ä¸”å¯ç¼–è¾‘ï¼Œå¯ä»¥ä½œä¸ºç»“æ„æç¤ºï¼Œå¹¶æœ‰åˆ©äºå…¶ä»– 3D ç”Ÿæˆä»»åŠ¡ã€‚å·¥ä½œé‡ï¼šDPA-Net çš„å®ç°åˆ©ç”¨äº† GT ç›¸æœºä½å§¿ã€‚ä¸ºäº†å‡è½»ç”±ä¼°è®¡çš„ã€å˜ˆæ‚çš„ä½å§¿å¼•èµ·çš„æ€§èƒ½ä¸‹é™ï¼Œå¯ä»¥åº”ç”¨ç°æœ‰çš„ç”¨äºè”åˆç›¸æœºåœºæ™¯ä¼˜åŒ–çš„ç°æœ‰æ–¹æ³•ï¼Œä¾‹å¦‚ [44]ã€‚æ­¤å¤–ï¼Œç”±äºçº¹ç†é¢„æµ‹ä¸æ˜¯æˆ‘ä»¬å·¥ä½œçš„é‡ç‚¹ï¼Œå› æ­¤éœ€è¦è¿›ä¸€æ­¥å¾®è°ƒï¼ˆä¾‹å¦‚ï¼Œåå‘è¾“å…¥è§†å›¾ï¼‰å’Œä¼˜åŒ–ä»¥æé«˜æ¸²æŸ“è´¨é‡ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-d1e745532008f87ea77f1571498e7a15.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-673670c0d185d530bd9f22bc5c036d4e.jpg" align="middle"></details><h2 id="Knowledge-NeRF-Few-shot-Novel-View-Synthesis-for-Dynamic-Articulated-Objects"><a href="#Knowledge-NeRF-Few-shot-Novel-View-Synthesis-for-Dynamic-Articulated-Objects" class="headerlink" title="Knowledge NeRF: Few-shot Novel View Synthesis for Dynamic Articulated   Objects"></a>Knowledge NeRF: Few-shot Novel View Synthesis for Dynamic Articulated   Objects</h2><p><strong>Authors:Wenxiao Cai, Xinyue LeiÄ±nst, Xinyu He, Junming Leo Chen, Yangang Wang</strong></p><p>We present Knowledge NeRF to synthesize novel views for dynamic scenes.Reconstructing dynamic 3D scenes from few sparse views and rendering them from arbitrary perspectives is a challenging problem with applications in various domains. Previous dynamic NeRF methods learn the deformation of articulated objects from monocular videos. However, qualities of their reconstructed scenes are limited.To clearly reconstruct dynamic scenes, we propose a new framework by considering two frames at a time.We pretrain a NeRF model for an articulated object.When articulated objects moves, Knowledge NeRF learns to generate novel views at the new state by incorporating past knowledge in the pretrained NeRF model with minimal observations in the present state. We propose a projection module to adapt NeRF for dynamic scenes, learning the correspondence between pretrained knowledge base and current states. Experimental results demonstrate the effectiveness of our method in reconstructing dynamic 3D scenes with 5 input images in one state. Knowledge NeRF is a new pipeline and promising solution for novel view synthesis in dynamic articulated objects. The data and implementation are publicly available at <a href="https://github.com/RussRobin/Knowledge_NeRF">https://github.com/RussRobin/Knowledge_NeRF</a>. </p><p><a href="http://arxiv.org/abs/2404.00674v1">PDF</a> </p><p><strong>Summary</strong><br>é€šè¿‡å°†è¿‡å»çŸ¥è¯†åº”ç”¨äºå½“å‰çŠ¶æ€çš„æœ‰é™è§‚æµ‹å€¼ï¼ŒKnowledge NeRF å¯ä¸ºåŠ¨æ€åœºæ™¯åˆæˆæ–°é¢–è§†å›¾ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>é’ˆå¯¹åŠ¨æ€åœºæ™¯ï¼ŒKnowledge NeRF æå‡ºäº†ä¸€ç§åŒæ—¶è€ƒè™‘ä¸¤å¸§çš„æ–°æ¡†æ¶ã€‚</li><li>é¢„è®­ç»ƒçš„ NeRF æ¨¡å‹ç”¨äºå­¦ä¹ é“°æ¥å¯¹è±¡çš„å˜å½¢ã€‚</li><li>æå‡ºäº†ä¸€ç§æŠ•å½±æ¨¡å—ï¼Œç”¨äºå­¦ä¹ é¢„è®­ç»ƒçŸ¥è¯†åº“å’Œå½“å‰çŠ¶æ€ä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚</li><li>Knowledge NeRF é€šè¿‡ 5 ä¸ªè¾“å…¥å›¾åƒåœ¨ä¸€å¸§ä¸­é‡å»ºåŠ¨æ€ 3D åœºæ™¯ã€‚</li><li>Knowledge NeRF ä¸ºåŠ¨æ€é“°æ¥å¯¹è±¡çš„å…¨æ–°è§†å›¾åˆæˆæä¾›äº†ä¸€ä¸ªæ–°çš„ç®¡é“å’Œæœ‰å¸Œæœ›çš„è§£å†³æ–¹æ¡ˆã€‚</li><li>è¯¥æ–¹æ³•é¿å…äº†åŠ¨æ€ NeRF æ–¹æ³•ä¸­å¸¸è§çš„é—®é¢˜ï¼Œä¾‹å¦‚æ¨¡ç³Šå’Œå˜å½¢é”™è¯¯ã€‚</li><li>æ•°æ®å’Œå®ç°å·²å…¬å¼€ï¼Œå¯ç”¨äºè¿›ä¸€æ­¥ç ”ç©¶å’Œåº”ç”¨ç¨‹åºå¼€å‘ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šçŸ¥è¯† NeRFï¼šåŠ¨æ€é“°æ¥å¯¹è±¡çš„å°æ ·æœ¬æ–°è§†è§’åˆæˆ</li><li>ä½œè€…ï¼šè”¡æ–‡æ™“ã€é›·æ¬£æ‚¦<em>ã€ä½•æ¬£å®‡</em>ã€é™ˆå›æ˜å’Œç‹æ‰¬åˆš**</li><li>å•ä½ï¼šä¸œå—å¤§å­¦</li><li>å…³é”®è¯ï¼šæ–°è§†è§’åˆæˆÂ·ç¥ç»è¾å°„åœºÂ·åŠ¨æ€ 3D åœºæ™¯Â·ç¨€ç–è§†è§’åˆæˆÂ·çŸ¥è¯†é›†æˆ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.00674</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šåŠ¨æ€åœºæ™¯é‡å»ºå’Œæ¸²æŸ“ä¸€ç›´æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„é‡è¦è¯¾é¢˜ã€‚ä¼ ç»Ÿçš„åŠ¨æ€ NeRF æ–¹æ³•é€šè¿‡å•ç›®è§†é¢‘å­¦ä¹ é“°æ¥å¯¹è±¡çš„å˜å½¢ï¼Œä½†é‡å»ºåœºæ™¯çš„è´¨é‡æœ‰é™ã€‚</li></ol><p>ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šè¿‡å»çš„æ–¹æ³•ä¸»è¦é€šè¿‡å•ç›®è§†é¢‘å­¦ä¹ é“°æ¥å¯¹è±¡çš„å˜å½¢ï¼Œä½†é‡å»ºåœºæ™¯çš„è´¨é‡æœ‰é™ã€‚</p><p>ï¼ˆ3ï¼‰æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œä¸€æ¬¡è€ƒè™‘ä¸¤å¸§å›¾åƒã€‚é¦–å…ˆï¼Œå¯¹é“°æ¥å¯¹è±¡é¢„è®­ç»ƒä¸€ä¸ª NeRF æ¨¡å‹ã€‚å½“é“°æ¥å¯¹è±¡ç§»åŠ¨æ—¶ï¼ŒçŸ¥è¯† NeRF é€šè¿‡å°†é¢„è®­ç»ƒ NeRF æ¨¡å‹ä¸­çš„è¿‡å»çŸ¥è¯†ä¸å½“å‰çŠ¶æ€ä¸­çš„æœ€å°‘è§‚å¯Ÿç›¸ç»“åˆï¼Œå­¦ä¹ åœ¨æ–°çš„çŠ¶æ€ä¸‹ç”Ÿæˆæ–°è§†è§’ã€‚æœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§æŠ•å½±æ¨¡å—ï¼Œå°† NeRF é€‚åº”äºåŠ¨æ€åœºæ™¯ï¼Œå­¦ä¹ é¢„è®­ç»ƒçŸ¥è¯†åº“å’Œå½“å‰çŠ¶æ€ä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚</p><p>ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†ä»€ä¹ˆæ€§èƒ½ï¼Œè¯¥æ€§èƒ½æ˜¯å¦èƒ½æ”¯æ’‘å…¶ç›®æ ‡ï¼šå®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä½¿ç”¨ä¸€ä¸ªçŠ¶æ€ä¸­çš„ 5 å¼ è¾“å…¥å›¾åƒé‡å»ºåŠ¨æ€ 3D åœºæ™¯ã€‚è¯¥æ–¹æ³•ä¸ºåŠ¨æ€é“°æ¥å¯¹è±¡çš„æ–°è§†è§’åˆæˆæä¾›äº†ä¸€ç§æ–°çš„ç®¡é“å’Œæœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚</p><p><methods>:(1): çŸ¥è¯†NeRFæ¡†æ¶ï¼šä¸€æ¬¡è€ƒè™‘ä¸¤å¸§å›¾åƒï¼Œå°†é¢„è®­ç»ƒNeRFæ¨¡å‹çš„çŸ¥è¯†ä¸å½“å‰çŠ¶æ€çš„ç¨€ç–è§‚å¯Ÿç›¸ç»“åˆï¼Œç”Ÿæˆæ–°è§†è§’å›¾åƒã€‚(2): æŠ•å½±æ¨¡å—ï¼šå­¦ä¹ é¢„è®­ç»ƒçŸ¥è¯†åº“å’Œå½“å‰çŠ¶æ€ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼Œå°†NeRFé€‚åº”äºåŠ¨æ€åœºæ™¯ã€‚(3): ç¨€ç–è§†è§’åˆæˆï¼šä½¿ç”¨ä¸€ä¸ªçŠ¶æ€ä¸­çš„5å¼ è¾“å…¥å›¾åƒé‡å»ºåŠ¨æ€3Dåœºæ™¯ã€‚</methods></p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„çŸ¥è¯†NeRFæ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿä¸€æ¬¡è€ƒè™‘ä¸¤å¸§å›¾åƒï¼Œå°†é¢„è®­ç»ƒNeRFæ¨¡å‹çš„çŸ¥è¯†ä¸å½“å‰çŠ¶æ€çš„ç¨€ç–è§‚å¯Ÿç›¸ç»“åˆï¼Œç”Ÿæˆæ–°è§†è§’å›¾åƒã€‚è¯¥æ¡†æ¶è¿˜æå‡ºäº†ä¸€ç§æŠ•å½±æ¨¡å—ï¼Œå­¦ä¹ é¢„è®­ç»ƒçŸ¥è¯†åº“å’Œå½“å‰çŠ¶æ€ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼Œå°†NeRFé€‚åº”äºåŠ¨æ€åœºæ™¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä½¿ç”¨ä¸€ä¸ªçŠ¶æ€ä¸­çš„5å¼ è¾“å…¥å›¾åƒé‡å»ºåŠ¨æ€3Dåœºæ™¯ï¼Œä¸ºåŠ¨æ€é“°æ¥å¯¹è±¡çš„æ–°è§†è§’åˆæˆæä¾›äº†ä¸€ç§æ–°çš„ç®¡é“å’Œæœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§æ–°çš„çŸ¥è¯†NeRFæ¡†æ¶ï¼Œå°†é¢„è®­ç»ƒNeRFæ¨¡å‹çš„çŸ¥è¯†ä¸å½“å‰çŠ¶æ€çš„ç¨€ç–è§‚å¯Ÿç›¸ç»“åˆï¼Œç”Ÿæˆæ–°è§†è§’å›¾åƒã€‚</li><li>è®¾è®¡äº†ä¸€ç§æŠ•å½±æ¨¡å—ï¼Œå­¦ä¹ é¢„è®­ç»ƒçŸ¥è¯†åº“å’Œå½“å‰çŠ¶æ€ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼Œå°†NeRFé€‚åº”äºåŠ¨æ€åœºæ™¯ã€‚æ€§èƒ½ï¼š</li><li>èƒ½å¤Ÿä½¿ç”¨ä¸€ä¸ªçŠ¶æ€ä¸­çš„5å¼ è¾“å…¥å›¾åƒé‡å»ºåŠ¨æ€3Dåœºæ™¯ã€‚å·¥ä½œé‡ï¼š</li><li>éœ€è¦é¢„è®­ç»ƒä¸€ä¸ªNeRFæ¨¡å‹ã€‚</li><li>éœ€è¦è®¾è®¡ä¸€ä¸ªæŠ•å½±æ¨¡å—ã€‚</li><li>éœ€è¦æ”¶é›†å’Œæ ‡æ³¨åŠ¨æ€3Dåœºæ™¯çš„æ•°æ®é›†ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-51d2760768289f17a022822e034438cb.jpg" align="middle"><img src="https://pica.zhimg.com/v2-1c30a4c13059600200df9151f02890b7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-35e6ac626071f10b54837546e9ead1e4.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-04-06  RaFE Generative Radiance Fields Restoration</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/3DGS/"/>
    <id>https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/3DGS/</id>
    <published>2024-04-06T10:15:08.000Z</published>
    <updated>2024-04-06T10:15:08.616Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-04-06-æ›´æ–°"><a href="#2024-04-06-æ›´æ–°" class="headerlink" title="2024-04-06 æ›´æ–°"></a>2024-04-06 æ›´æ–°</h1><h2 id="Per-Gaussian-Embedding-Based-Deformation-for-Deformable-3D-Gaussian-Splatting"><a href="#Per-Gaussian-Embedding-Based-Deformation-for-Deformable-3D-Gaussian-Splatting" class="headerlink" title="Per-Gaussian Embedding-Based Deformation for Deformable 3D Gaussian   Splatting"></a>Per-Gaussian Embedding-Based Deformation for Deformable 3D Gaussian   Splatting</h2><p><strong>Authors:Jeongmin Bae, Seoha Kim, Youngsik Yun, Hahyun Lee, Gun Bang, Youngjung Uh</strong></p><p>As 3D Gaussian Splatting (3DGS) provides fast and high-quality novel view synthesis, it is a natural extension to deform a canonical 3DGS to multiple frames. However, previous works fail to accurately reconstruct dynamic scenes, especially 1) static parts moving along nearby dynamic parts, and 2) some dynamic areas are blurry. We attribute the failure to the wrong design of the deformation field, which is built as a coordinate-based function. This approach is problematic because 3DGS is a mixture of multiple fields centered at the Gaussians, not just a single coordinate-based framework. To resolve this problem, we define the deformation as a function of per-Gaussian embeddings and temporal embeddings. Moreover, we decompose deformations as coarse and fine deformations to model slow and fast movements, respectively. Also, we introduce an efficient training strategy for faster convergence and higher quality. Project page: <a href="https://jeongminb.github.io/e-d3dgs/">https://jeongminb.github.io/e-d3dgs/</a> </p><p><a href="http://arxiv.org/abs/2404.03613v1">PDF</a> Preprint</p><p><strong>Summary</strong><br>3D é«˜æ–¯æ–‘ç‚¹é‡‡æ ·é€šè¿‡å˜å½¢ç½‘æ ¼æ¥å®ç°åŠ¨æ€åœºæ™¯çš„ç²¾ç¡®é‡å»ºï¼Œè§£å†³äº†ä»¥å¾€ä½œå“çš„å±€é™æ€§ï¼ŒåŒ…æ‹¬é™æ€éƒ¨ä»¶æ²¿ç€åŠ¨æ€éƒ¨ä»¶ç§»åŠ¨å’ŒåŠ¨æ€åŒºåŸŸæ¨¡ç³Šçš„é—®é¢˜ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>åŠ¨æ€åœºæ™¯å˜å½¢é‡å»ºå­˜åœ¨é—®é¢˜ï¼ŒåŒ…æ‹¬é™æ€éƒ¨ä»¶æ²¿åŠ¨æ€éƒ¨ä»¶ç§»åŠ¨å’ŒåŠ¨æ€åŒºåŸŸæ¨¡ç³Šã€‚</li><li>é—®é¢˜çš„æ ¹æºåœ¨äºå˜å½¢åœºçš„é”™è¯¯è®¾è®¡ï¼Œéœ€é‡‡ç”¨åŸºäºæ··åˆé«˜æ–¯æ ¸çš„å‡½æ•°ã€‚</li><li>å˜å½¢å®šä¹‰ä¸ºåŸºäºé«˜æ–¯åµŒå…¥å’Œæ—¶é—´åµŒå…¥çš„å‡½æ•°ï¼Œå¯åˆ†è§£ä¸ºç²—ç•¥å’Œç²¾ç»†å˜å½¢ã€‚</li><li>å¼•å…¥é«˜æ•ˆè®­ç»ƒç­–ç•¥ï¼ŒåŠ é€Ÿæ”¶æ•›å¹¶æå‡è´¨é‡ã€‚</li><li>è¯¥ç ”ç©¶é€šè¿‡å˜å½¢ç½‘æ ¼å®ç°äº†åŠ¨æ€åœºæ™¯çš„ç²¾ç¡®é‡å»ºã€‚</li><li>æå‡ºäº†ä¸€ç§æ–°çš„å˜å½¢åœºè®¾è®¡ï¼ŒåŸºäºæ¯ä¸ªé«˜æ–¯æ ¸çš„åµŒå…¥å’Œæ—¶é—´åµŒå…¥ã€‚</li><li>é‡‡ç”¨ç²—ç•¥å’Œç²¾ç»†å˜å½¢ç›¸ç»“åˆçš„æ–¹å¼ï¼Œåˆ†åˆ«å»ºæ¨¡ç¼“æ…¢å’Œå¿«é€Ÿè¿åŠ¨ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šåŸºäºé«˜æ–¯åµŒå…¥çš„å˜å½¢</li><li>ä½œè€…ï¼šJeongmin Baeã€Seoha Kimã€Youngsik Yunã€Hahyun Leeã€Gun Bangã€Youngjung Uh</li><li>æ‰€å±å•ä½ï¼šå»¶ä¸–å¤§å­¦</li><li>å…³é”®è¯ï¼šé«˜æ–¯æ•£å¸ƒã€åŠ¨æ€åœºæ™¯é‡å»ºã€æ–°é¢–è§†å›¾åˆæˆ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.03613   Github é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼š   (1) ç ”ç©¶èƒŒæ™¯ï¼š   3D é«˜æ–¯æ•£å¸ƒï¼ˆ3DGSï¼‰æä¾›å¿«é€Ÿä¸”é«˜è´¨é‡çš„æ–°é¢–è§†å›¾åˆæˆï¼Œå°†æ­£åˆ™ 3DGS å˜å½¢åˆ°å¤šä¸ªå¸§æ˜¯å…¶è‡ªç„¶å»¶ä¼¸ã€‚ç„¶è€Œï¼Œä»¥å¾€çš„ç ”ç©¶æ— æ³•å‡†ç¡®é‡å»ºåŠ¨æ€åœºæ™¯ï¼Œç‰¹åˆ«æ˜¯ï¼š1ï¼‰é™æ­¢éƒ¨åˆ†æ²¿ç€é™„è¿‘çš„åŠ¨æ€éƒ¨åˆ†ç§»åŠ¨ï¼›2ï¼‰ä¸€äº›åŠ¨æ€åŒºåŸŸæ¨¡ç³Šã€‚   (2) è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼š   å°†å˜å½¢åœºè®¾è®¡ä¸ºåŸºäºåæ ‡çš„å‡½æ•°ï¼Œè¿™æ˜¯å¯¼è‡´ä¸Šè¿°é—®é¢˜çš„åŸå› ã€‚è¿™ç§æ–¹æ³•å­˜åœ¨é—®é¢˜ï¼Œå› ä¸º 3DGS æ˜¯ä»¥é«˜æ–¯ä¸ºä¸­å¿ƒçš„å¤šä¸ªåœºçš„æ··åˆï¼Œè€Œä¸ä»…ä»…æ˜¯ä¸€ä¸ªåŸºäºåæ ‡çš„æ¡†æ¶ã€‚   (3) æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼š   å°†å˜å½¢å®šä¹‰ä¸ºæ¯ä¸ªé«˜æ–¯åµŒå…¥å’Œæ—¶é—´åµŒå…¥çš„å‡½æ•°ã€‚æ­¤å¤–ï¼Œå°†å˜å½¢åˆ†è§£ä¸ºç²—ç•¥å˜å½¢å’Œç²¾ç»†å˜å½¢ï¼Œåˆ†åˆ«å¯¹æ…¢é€Ÿè¿åŠ¨å’Œå¿«é€Ÿè¿åŠ¨è¿›è¡Œå»ºæ¨¡ã€‚è¿˜å¼•å…¥äº†ä¸€ç§æœ‰æ•ˆçš„è®­ç»ƒç­–ç•¥ï¼Œä»¥å®ç°æ›´å¿«çš„æ”¶æ•›å’Œæ›´é«˜çš„è´¨é‡ã€‚   (4) æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼š   è¯¥æ–¹æ³•åœ¨åŠ¨æ€åœºæ™¯é‡å»ºä»»åŠ¡ä¸Šå®ç°äº†å…ˆè¿›çš„æ€§èƒ½ã€‚å®ƒå¯ä»¥å‡†ç¡®åœ°é‡å»ºåŠ¨æ€åœºæ™¯ï¼ŒåŒæ—¶é¿å…é™æ­¢éƒ¨åˆ†æ²¿é™„è¿‘åŠ¨æ€éƒ¨åˆ†ç§»åŠ¨å’ŒåŠ¨æ€åŒºåŸŸæ¨¡ç³Šçš„é—®é¢˜ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ï¼Œå³å‡†ç¡®é‡å»ºåŠ¨æ€åœºæ™¯ã€‚</p></li><li><p>Methods:(1): å°†å˜å½¢å®šä¹‰ä¸ºæ¯ä¸ªé«˜æ–¯åµŒå…¥å’Œæ—¶é—´åµŒå…¥çš„å‡½æ•°ï¼Œä»¥è§£å†³ä»¥å¾€åŸºäºåæ ‡çš„å˜å½¢å‡½æ•°çš„å±€é™æ€§ã€‚(2): å°†å˜å½¢åˆ†è§£ä¸ºç²—ç•¥å˜å½¢å’Œç²¾ç»†å˜å½¢ï¼Œåˆ†åˆ«å»ºæ¨¡æ…¢é€Ÿè¿åŠ¨å’Œå¿«é€Ÿè¿åŠ¨ï¼Œä»è€Œæé«˜é‡å»ºç²¾åº¦ã€‚(3): æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„è®­ç»ƒç­–ç•¥ï¼ŒåŒ…æ‹¬é¢„è®­ç»ƒã€è”åˆè®­ç»ƒå’Œç»†åŒ–è®­ç»ƒï¼Œä»¥å®ç°æ›´å¿«çš„æ”¶æ•›å’Œæ›´é«˜çš„è´¨é‡ã€‚</p></li></ol><p>8.ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†åŸºäºé«˜æ–¯åµŒå…¥çš„å˜å½¢æ–¹æ³•ï¼Œè§£å†³äº†ä»¥å¾€åŸºäºåæ ‡çš„å˜å½¢å‡½æ•°çš„å±€é™æ€§ï¼Œæœ‰æ•ˆåœ°é‡å»ºåŠ¨æ€åœºæ™¯ï¼Œé¿å…äº†é™æ­¢éƒ¨åˆ†æ²¿ç€é™„è¿‘åŠ¨æ€éƒ¨åˆ†ç§»åŠ¨å’ŒåŠ¨æ€åŒºåŸŸæ¨¡ç³Šçš„é—®é¢˜ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š- å°†å˜å½¢å®šä¹‰ä¸ºæ¯ä¸ªé«˜æ–¯åµŒå…¥å’Œæ—¶é—´åµŒå…¥çš„å‡½æ•°ï¼Œæé«˜äº†é‡å»ºç²¾åº¦ã€‚- å°†å˜å½¢åˆ†è§£ä¸ºç²—ç•¥å˜å½¢å’Œç²¾ç»†å˜å½¢ï¼Œåˆ†åˆ«å»ºæ¨¡æ…¢é€Ÿè¿åŠ¨å’Œå¿«é€Ÿè¿åŠ¨ã€‚- æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„è®­ç»ƒç­–ç•¥ï¼ŒåŒ…æ‹¬é¢„è®­ç»ƒã€è”åˆè®­ç»ƒå’Œç»†åŒ–è®­ç»ƒï¼Œå®ç°æ›´å¿«çš„æ”¶æ•›å’Œæ›´é«˜çš„è´¨é‡ã€‚æ€§èƒ½ï¼š- åœ¨åŠ¨æ€åœºæ™¯é‡å»ºä»»åŠ¡ä¸Šå®ç°äº†å…ˆè¿›çš„æ€§èƒ½ã€‚- å‡†ç¡®åœ°é‡å»ºäº†åŠ¨æ€åœºæ™¯ï¼Œé¿å…äº†é™æ­¢éƒ¨åˆ†æ²¿ç€é™„è¿‘åŠ¨æ€éƒ¨åˆ†ç§»åŠ¨å’ŒåŠ¨æ€åŒºåŸŸæ¨¡ç³Šçš„é—®é¢˜ã€‚å·¥ä½œé‡ï¼š- å·¥ä½œé‡è¾ƒå¤§ï¼Œæ¶‰åŠåˆ°é«˜æ–¯åµŒå…¥ã€æ—¶é—´åµŒå…¥ã€ç²—ç•¥å˜å½¢ã€ç²¾ç»†å˜å½¢ã€æœ‰æ•ˆçš„è®­ç»ƒç­–ç•¥ç­‰å¤šä¸ªæ–¹é¢çš„è®¾è®¡å’Œå®ç°ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-889daa3d497b87544ff9eda8fe72a591.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c9961409bb22844f4e0d50a2379465d1.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4682b20e9fb95c7bb73c2d72c03cbec6.jpg" align="middle"></details>## DreamScene: 3D Gaussian-based Text-to-3D Scene Generation via Formation   Pattern Sampling**Authors:Haoran Li, Haolin Shi, Wenli Zhang, Wenjun Wu, Yong Liao, Lin Wang, Lik-hang Lee, Pengyuan Zhou**Text-to-3D scene generation holds immense potential for the gaming, film, and architecture sectors. Despite significant progress, existing methods struggle with maintaining high quality, consistency, and editing flexibility. In this paper, we propose DreamScene, a 3D Gaussian-based novel text-to-3D scene generation framework, to tackle the aforementioned three challenges mainly via two strategies. First, DreamScene employs Formation Pattern Sampling (FPS), a multi-timestep sampling strategy guided by the formation patterns of 3D objects, to form fast, semantically rich, and high-quality representations. FPS uses 3D Gaussian filtering for optimization stability, and leverages reconstruction techniques to generate plausible textures. Second, DreamScene employs a progressive three-stage camera sampling strategy, specifically designed for both indoor and outdoor settings, to effectively ensure object-environment integration and scene-wide 3D consistency. Last, DreamScene enhances scene editing flexibility by integrating objects and environments, enabling targeted adjustments. Extensive experiments validate DreamScene's superiority over current state-of-the-art techniques, heralding its wide-ranging potential for diverse applications. Code and demos will be released at https://dreamscene-project.github.io . [PDF](http://arxiv.org/abs/2404.03575v1) **Summary**åŸºäº3Dé«˜æ–¯åˆ†å¸ƒDreamSceneæ–‡æœ¬è½¬3Dåœºæ™¯ç”Ÿæˆæ¡†æ¶ï¼Œåˆ©ç”¨FPSæ–¹æ³•å’Œä¸‰é˜¶æ®µç›¸æœºé‡‡æ ·ç­–ç•¥ï¼Œå®ç°äº†åœºæ™¯è´¨é‡é«˜ã€ä¸€è‡´æ€§å’Œç¼–è¾‘çµæ´»æ€§ã€‚**Key Takeaways**- FPSæ–¹æ³•é‡‡ç”¨é«˜æ–¯æ»¤æ³¢ä¼˜åŒ–ç¨³å®šæ€§ï¼Œé‡æ„æŠ€æœ¯ç”ŸæˆçœŸå®çº¹ç†ï¼Œå®ç°åœºæ™¯ä¸°å¯Œã€é«˜è´¨é‡ã€‚- ä¸‰é˜¶æ®µç›¸æœºé‡‡æ ·ç­–ç•¥é’ˆå¯¹å®¤å†…å¤–åœºæ™¯ï¼Œæœ‰æ•ˆç¡®ä¿å¯¹è±¡ä¸ç¯å¢ƒèåˆï¼Œå®ç°åœºæ™¯å…¨å±€3Dä¸€è‡´æ€§ã€‚- é›†æˆå¯¹è±¡ä¸ç¯å¢ƒï¼Œæ”¯æŒç›®æ ‡è°ƒæ•´ï¼Œå¢å¼ºåœºæ™¯ç¼–è¾‘çµæ´»æ€§ã€‚- å®éªŒéªŒè¯DreamSceneåœ¨è´¨é‡ã€ä¸€è‡´æ€§å’Œçµæ´»æ€§æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚- ä»£ç å’Œæ¼”ç¤ºå°†åœ¨https://dreamscene-project.github.ioå‘å¸ƒã€‚- DreamSceneé€‚ç”¨äºæ¸¸æˆã€ç”µå½±å’Œå»ºç­‘ç­‰é¢†åŸŸã€‚- DreamSceneè§£å†³äº†ç°æœ‰æ–‡æœ¬è½¬3Dåœºæ™¯ç”Ÿæˆæ–¹æ³•ä¸­è´¨é‡ã€ä¸€è‡´æ€§å’Œç¼–è¾‘çµæ´»æ€§æ–¹é¢çš„æŒ‘æˆ˜ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>é¢˜ç›®ï¼šDreamSceneï¼šåŸºäº 3D é«˜æ–¯åˆ†å¸ƒçš„æ–‡æœ¬åˆ° 3D è¡¥å……ææ–™</li><li>ä½œè€…ï¼šHaoran Li, Mingxing Tan, Yajun Cai, Zexiang Xu, Xiaogang Wang</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šä¸­å›½ç§‘å­¦æŠ€æœ¯å¤§å­¦</li><li>å…³é”®è¯ï¼šText-to-3Dã€Text-to-3D Sceneã€3D Gaussianã€Scene Generationã€Scene Editing</li><li>è®ºæ–‡é“¾æ¥ï¼šæ— ï¼ŒGithub ä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ï¼šæ–‡æœ¬åˆ° 3D åœºæ™¯ç”Ÿæˆåœ¨æ¸¸æˆã€ç”µå½±å’Œå»ºç­‘é¢†åŸŸå…·æœ‰å·¨å¤§æ½œåŠ›ã€‚å°½ç®¡å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨ä¿æŒé«˜è´¨é‡ã€ä¸€è‡´æ€§å’Œç¼–è¾‘çµæ´»æ€§æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚ï¼ˆ2ï¼‰ï¼šç°æœ‰æ–¹æ³•åŒ…æ‹¬åŸºäºå†…æ’å’ŒåŸºäºç»„åˆçš„æ–¹æ³•ã€‚åŸºäºå†…æ’çš„æ–¹æ³•ä½¿ç”¨æ–‡æœ¬åˆ°å›¾åƒå†…æ’è¿›è¡Œåœºæ™¯ç”Ÿæˆï¼Œä½†å®ƒä»¬åœ¨å¯è§èŒƒå›´ä¹‹å¤–é‡åˆ°äº†æ˜æ˜¾çš„é™åˆ¶ï¼Œå¹¶ä¸”åœ¨é€»è¾‘åœºæ™¯ç»„åˆæ–¹é¢å­˜åœ¨é—®é¢˜ã€‚åŸºäºç»„åˆçš„æ–¹æ³•ä¹Ÿé‡‡ç”¨ç»„åˆæ–¹æ³•æ¥æ„å»ºåœºæ™¯ï¼Œä½†å®ƒä»¬é¢ä¸´ç”Ÿæˆè´¨é‡ä½å’Œè®­ç»ƒé€Ÿåº¦æ…¢çš„æŒ‘æˆ˜ã€‚ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æå‡ºçš„ DreamScene æ˜¯ä¸€ç§åŸºäº 3D é«˜æ–¯åˆ†å¸ƒçš„æ–°å‹æ–‡æœ¬åˆ° 3D åœºæ™¯ç”Ÿæˆæ¡†æ¶ï¼Œä¸»è¦é€šè¿‡ä¸¤ç§ç­–ç•¥æ¥è§£å†³ä¸Šè¿°ä¸‰ä¸ªæŒ‘æˆ˜ã€‚é¦–å…ˆï¼ŒDreamScene é‡‡ç”¨å½¢æˆæ¨¡å¼é‡‡æ · (FPS)ï¼Œè¿™æ˜¯ä¸€ç§å— 3D å¯¹è±¡å½¢æˆæ¨¡å¼æŒ‡å¯¼çš„å¤šæ—¶é—´æ­¥é‡‡æ ·ç­–ç•¥ï¼Œç”¨äºå½¢æˆå¿«é€Ÿã€è¯­ä¹‰ä¸°å¯Œä¸”é«˜è´¨é‡çš„è¡¨ç¤ºã€‚FPS ä½¿ç”¨ 3D é«˜æ–¯æ»¤æ³¢è¿›è¡Œä¼˜åŒ–ç¨³å®šæ€§ï¼Œå¹¶åˆ©ç”¨é‡å»ºæŠ€æœ¯ç”Ÿæˆåˆç†çš„çº¹ç†ã€‚å…¶æ¬¡ï¼ŒDreamScene é‡‡ç”¨æ¸è¿›çš„ä¸‰é˜¶æ®µç›¸æœºé‡‡æ ·ç­–ç•¥ï¼Œä¸“é—¨è®¾è®¡ç”¨äºå®¤å†…å’Œå®¤å¤–è®¾ç½®ï¼Œä»¥æœ‰æ•ˆç¡®ä¿å¯¹è±¡ç¯å¢ƒé›†æˆå’Œåœºæ™¯èŒƒå›´å†…çš„ 3D ä¸€è‡´æ€§ã€‚æœ€åï¼ŒDreamScene é€šè¿‡é›†æˆå¯¹è±¡å’Œç¯å¢ƒæ¥å¢å¼ºåœºæ™¯ç¼–è¾‘çµæ´»æ€§ï¼Œä»è€Œå®ç°æœ‰é’ˆå¯¹æ€§çš„è°ƒæ•´ã€‚ï¼ˆ4ï¼‰ï¼šå¹¿æ³›çš„å®éªŒéªŒè¯äº† DreamScene ä¼˜äºå½“å‰æœ€å…ˆè¿›æŠ€æœ¯çš„ä¼˜åŠ¿ï¼Œé¢„ç¤ºç€å®ƒåœ¨å„ç§åº”ç”¨ä¸­çš„å¹¿æ³›æ½œåŠ›ã€‚</li></ol><p>7.Methodsï¼š(1) DreamSceneé‡‡ç”¨å½¢æˆæ¨¡å¼é‡‡æ ·ï¼ˆFPSï¼‰ç­–ç•¥ï¼Œè¯¥ç­–ç•¥å—3Då¯¹è±¡å½¢æˆæ¨¡å¼æŒ‡å¯¼ï¼Œå¹¶ä½¿ç”¨3Dé«˜æ–¯æ»¤æ³¢è¿›è¡Œä¼˜åŒ–ï¼Œä»¥å½¢æˆå¿«é€Ÿã€è¯­ä¹‰ä¸°å¯Œä¸”é«˜è´¨é‡çš„è¡¨ç¤ºã€‚(2) DreamSceneé‡‡ç”¨æ¸è¿›çš„ä¸‰é˜¶æ®µç›¸æœºé‡‡æ ·ç­–ç•¥ï¼Œä¸“é—¨è®¾è®¡ç”¨äºå®¤å†…å’Œå®¤å¤–è®¾ç½®ï¼Œä»¥æœ‰æ•ˆç¡®ä¿å¯¹è±¡ç¯å¢ƒé›†æˆå’Œåœºæ™¯èŒƒå›´å†…çš„3Dä¸€è‡´æ€§ã€‚(3) DreamSceneé€šè¿‡é›†æˆå¯¹è±¡å’Œç¯å¢ƒæ¥å¢å¼ºåœºæ™¯ç¼–è¾‘çµæ´»æ€§ï¼Œä»è€Œå®ç°æœ‰é’ˆå¯¹æ€§çš„è°ƒæ•´ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬å·¥ä½œé€šè¿‡æå‡º DreamSceneï¼Œå°†æ–‡æœ¬åˆ° 3D åœºæ™¯ç”Ÿæˆæå‡åˆ°äº†ä¸€ä¸ªæ–°çš„æ°´å¹³ï¼Œå®ƒåœ¨æ•ˆç‡ã€ä¸€è‡´æ€§å’Œå¯ç¼–è¾‘æ€§æ–¹é¢å–å¾—äº†çªç ´ã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼ša) æå‡ºå½¢æˆæ¨¡å¼é‡‡æ ·ï¼ˆFPSï¼‰ï¼Œæœ‰æ•ˆåœ°ç”Ÿæˆå¿«é€Ÿã€è¯­ä¹‰ä¸°å¯Œä¸”é«˜è´¨é‡çš„è¡¨ç¤ºã€‚b) è®¾è®¡æ¸è¿›çš„ä¸‰é˜¶æ®µç›¸æœºé‡‡æ ·ç­–ç•¥ï¼Œç¡®ä¿å¯¹è±¡ç¯å¢ƒé›†æˆå’Œåœºæ™¯èŒƒå›´å†…çš„ 3D ä¸€è‡´æ€§ã€‚c) é€šè¿‡é›†æˆå¯¹è±¡å’Œç¯å¢ƒå¢å¼ºåœºæ™¯ç¼–è¾‘çµæ´»æ€§ï¼Œå®ç°æœ‰é’ˆå¯¹æ€§çš„è°ƒæ•´ã€‚æ€§èƒ½ï¼ša) åœ¨æ•ˆç‡æ–¹é¢ï¼ŒDreamScene æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œåœºæ™¯ç”Ÿæˆæ—¶é—´ä» 13.3 å°æ—¶å‡å°‘åˆ° 1 å°æ—¶ã€‚b) åœ¨ä¸€è‡´æ€§æ–¹é¢ï¼ŒDreamScene é€šè¿‡ä¼˜åŒ– 3D é«˜æ–¯æ»¤æ³¢å’Œé‡å»ºæŠ€æœ¯ï¼Œç”Ÿæˆè¯­ä¹‰åˆç†ä¸”çº¹ç†æ¸…æ™°çš„åœºæ™¯ã€‚c) åœ¨å¯ç¼–è¾‘æ€§æ–¹é¢ï¼ŒDreamScene å…è®¸ç”¨æˆ·é€šè¿‡æè¿°æ€§æ‰‹æ®µè½»æ¾ä¿®æ”¹å¯¹è±¡ä½ç½®å’Œåœºæ™¯é£æ ¼ã€‚å·¥ä½œé‡ï¼ša) æœ¬æ–‡æä¾›äº† DreamScene çš„è¯¦ç»†ç®—æ³•æè¿°å’Œå®ç°ç»†èŠ‚ï¼Œæ–¹ä¾¿ç ”ç©¶äººå‘˜å¤ç°å’Œæ”¹è¿›ã€‚b) ä½œè€…æä¾›äº†å¤§é‡å®éªŒç»“æœå’Œç”¨æˆ·ç ”ç©¶ï¼Œè¯æ˜äº† DreamScene çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚c) æœ¬æ–‡è¿˜è®¨è®ºäº† DreamScene çš„æ½œåœ¨åº”ç”¨å’Œæœªæ¥ç ”ç©¶æ–¹å‘ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-c2411c008574ac1121f44aa182639618.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ac1bd97d131a2cbaaf9bb1fd2be45222.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e702cfeccb50c7e77ba99588312fda04.jpg" align="middle"></details><h2 id="OmniGS-Omnidirectional-Gaussian-Splatting-for-Fast-Radiance-Field-Reconstruction-using-Omnidirectional-Images"><a href="#OmniGS-Omnidirectional-Gaussian-Splatting-for-Fast-Radiance-Field-Reconstruction-using-Omnidirectional-Images" class="headerlink" title="OmniGS: Omnidirectional Gaussian Splatting for Fast Radiance Field   Reconstruction using Omnidirectional Images"></a>OmniGS: Omnidirectional Gaussian Splatting for Fast Radiance Field   Reconstruction using Omnidirectional Images</h2><p><strong>Authors:Longwei Li, Huajian Huang, Sai-Kit Yeung, Hui Cheng</strong></p><p>Photorealistic reconstruction relying on 3D Gaussian Splatting has shown promising potential in robotics. However, the current 3D Gaussian Splatting system only supports radiance field reconstruction using undistorted perspective images. In this paper, we present OmniGS, a novel omnidirectional Gaussian splatting system, to take advantage of omnidirectional images for fast radiance field reconstruction. Specifically, we conduct a theoretical analysis of spherical camera model derivatives in 3D Gaussian Splatting. According to the derivatives, we then implement a new GPU-accelerated omnidirectional rasterizer that directly splats 3D Gaussians onto the equirectangular screen space for omnidirectional image rendering. As a result, we realize differentiable optimization of the radiance field without the requirement of cube-map rectification or tangent-plane approximation. Extensive experiments conducted in egocentric and roaming scenarios demonstrate that our method achieves state-of-the-art reconstruction quality and high rendering speed using omnidirectional images. To benefit the research community, the code will be made publicly available once the paper is published. </p><p><a href="http://arxiv.org/abs/2404.03202v1">PDF</a> IROS 2024 submission, 7 pages, 4 figures</p><p><strong>Summary</strong><br>å…¨æ™¯é«˜æ–¯æ³¼æº…æ³•åˆ©ç”¨å…¨æ™¯å›¾åƒå®ç°å¿«é€Ÿè¾ç…§åœºé‡å»ºï¼Œæ— éœ€ç«‹æ–¹ä½“è´´å›¾æ ¡æ­£æˆ–åˆ‡å¹³é¢è¿‘ä¼¼ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„å…¨æ™¯é«˜æ–¯æ³¼æº…ç³»ç»Ÿ OmniGSï¼Œç”¨äºåˆ©ç”¨å…¨æ™¯å›¾åƒè¿›è¡Œå¿«é€Ÿè¾ç…§åœºé‡å»ºã€‚</li><li>å¯¹ 3D é«˜æ–¯æ³¼æº…ä¸­çš„çƒå½¢ç›¸æœºæ¨¡å‹å¯¼æ•°è¿›è¡Œäº†ç†è®ºåˆ†æã€‚</li><li>å®ç°äº†ä¸€ç§æ–°çš„ GPU åŠ é€Ÿå…¨æ™¯å…‰æ …åŒ–å™¨ï¼Œç”¨äºå°† 3D é«˜æ–¯ç›´æ¥æ³¼æº…åˆ°ç­‰è·å±å¹•ç©ºé—´ä»¥è¿›è¡Œå…¨æ™¯å›¾åƒæ¸²æŸ“ã€‚</li><li>å®ç°äº†è¾ç…§åœºçš„å¯å¾®ä¼˜åŒ–ï¼Œæ— éœ€ç«‹æ–¹ä½“è´´å›¾æ ¡æ­£æˆ–åˆ‡å¹³é¢è¿‘ä¼¼ã€‚</li><li>å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨å…¨æ™¯å›¾åƒå®ç°äº†æœ€å…ˆè¿›çš„é‡å»ºè´¨é‡å’Œé«˜æ¸²æŸ“é€Ÿåº¦ã€‚</li><li>ä»£ç å°†åœ¨è®ºæ–‡å‘è¡¨åå…¬å¼€ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šå…¨å‘é«˜æ–¯æ¸²æŸ“ï¼šç”¨äºå¿«é€Ÿè¾å°„åœºé‡å»ºçš„å…¨å‘é«˜æ–¯æ¸²æŸ“</li><li>ä½œè€…ï¼šæé¾™å¨ã€é»„åå¥ã€æ¨ä¸–æ°ã€ç¨‹è¾‰</li><li>éš¶å±ï¼šä¸­å±±å¤§å­¦è®¡ç®—æœºç§‘å­¦ä¸å·¥ç¨‹å­¦é™¢</li><li>å…³é”®è¯ï¼šå…¨å‘è§†è§‰ã€çœŸå®æ„Ÿå»ºå›¾ã€3D é‡å»ºã€æ–°è§†è§’åˆæˆã€é«˜æ–¯æ¸²æŸ“</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.03202   Github é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šçœŸå®æ„Ÿé‡å»ºä¾èµ–äº 3D é«˜æ–¯æ¸²æŸ“åœ¨æœºå™¨äººé¢†åŸŸæ˜¾ç¤ºå‡ºå¹¿é˜”å‰æ™¯ã€‚ç„¶è€Œï¼Œå½“å‰çš„ 3D é«˜æ–¯æ¸²æŸ“ç³»ç»Ÿä»…æ”¯æŒä½¿ç”¨æ— ç•¸å˜é€è§†å›¾åƒè¿›è¡Œè¾å°„åœºé‡å»ºã€‚   ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç°æœ‰æ–¹æ³•åˆ©ç”¨ç¥ç»è¾å°„åœº (NeRF) æŠ€æœ¯æ¢ç´¢å…¨å‘è¾å°„åœºé‡å»ºï¼Œä½† NeRF æ–¹æ³•çš„è®­ç»ƒå’Œæ¨ç†æ—¶é—´è¾ƒé•¿ã€‚3D é«˜æ–¯æ¸²æŸ“ (3DGS) åˆ™é€šè¿‡å¼•å…¥ 3D é«˜æ–¯æ˜¾å¼è¡¨ç¤ºè¾å°„åœºæ¥æœ‰æ•ˆåœ°è§£å†³äº† NeRF çš„å±€é™æ€§ï¼Œä½†å…¶æ¸²æŸ“ç®—æ³•ä»…é€‚ç”¨äºæ— ç•¸å˜é€è§†å›¾åƒã€‚   ï¼ˆ3ï¼‰æœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º OmniGS çš„æ–°ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿåˆ©ç”¨å…¨å‘é«˜æ–¯æ¸²æŸ“è¿›è¡Œå¿«é€Ÿè¾å°„åœºé‡å»ºã€‚å…·ä½“æ¥è¯´ï¼Œæœ¬æ–‡å¯¹çƒé¢ç›¸æœºæ¨¡å‹åœ¨ 3D é«˜æ–¯æ¸²æŸ“ä¸­çš„å¯¼æ•°è¿›è¡Œäº†ç†è®ºåˆ†æï¼Œå¹¶åŸºäºæ­¤å®ç°äº†ä¸€ç§æ–°çš„ GPU åŠ é€Ÿå…¨å‘å…‰æ …åŒ–å™¨ï¼Œè¯¥å…‰æ …åŒ–å™¨å¯å°† 3D é«˜æ–¯ç›´æ¥æ¸²æŸ“åˆ°å…¨å‘å›¾åƒçš„ç­‰è·çŸ©å½¢å±å¹•ç©ºé—´ä¸­ã€‚è¿™æ ·ä¸€æ¥ï¼Œæ— éœ€å¯¹ç«‹æ–¹ä½“è´´å›¾è¿›è¡Œæ ¡æ­£æˆ–åˆ‡å¹³é¢è¿‘ä¼¼ï¼Œå³å¯å®ç°è¾å°„åœºçš„å¯å¾®ä¼˜åŒ–ã€‚   ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šåœ¨ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒå’Œæ¼«æ¸¸åœºæ™¯ä¸­è¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæœ¬æ–‡æ–¹æ³•ä½¿ç”¨å…¨å‘å›¾åƒå®ç°äº†æœ€å…ˆè¿›çš„é‡å»ºè´¨é‡å’Œè¾ƒé«˜çš„æ¸²æŸ“é€Ÿåº¦ã€‚è¿™äº›æ€§èƒ½æŒ‡æ ‡æœ‰åŠ›åœ°æ”¯æŒäº†æœ¬æ–‡æ–¹æ³•çš„ç›®æ ‡ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) çƒé¢ç›¸æœºæ¨¡å‹åœ¨ 3D é«˜æ–¯æ¸²æŸ“ä¸­çš„å¯¼æ•°åˆ†æï¼›(2) åŸºäºå¯¼æ•°åˆ†æå®ç°å…¨å‘å…‰æ …åŒ–å™¨ï¼›(3) å°† 3D é«˜æ–¯ç›´æ¥æ¸²æŸ“åˆ°å…¨å‘å›¾åƒçš„ç­‰è·çŸ©å½¢å±å¹•ç©ºé—´ä¸­ï¼›(4) å¯å¾®ä¼˜åŒ–è¾å°„åœºã€‚</p></li><li><p>ç»“è®ºï¼š(1): æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º OmniGS çš„æ–°ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿåˆ©ç”¨å…¨å‘é«˜æ–¯æ¸²æŸ“è¿›è¡Œå¿«é€Ÿè¾å°„åœºé‡å»ºï¼Œåœ¨ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒå’Œæ¼«æ¸¸åœºæ™¯ä¸­è¿›è¡Œäº†å¤§é‡å®éªŒï¼Œè¡¨æ˜æœ¬æ–‡æ–¹æ³•ä½¿ç”¨å…¨å‘å›¾åƒå®ç°äº†æœ€å…ˆè¿›çš„é‡å»ºè´¨é‡å’Œè¾ƒé«˜çš„æ¸²æŸ“é€Ÿåº¦ã€‚è¿™äº›æ€§èƒ½æŒ‡æ ‡æœ‰åŠ›åœ°æ”¯æŒäº†æœ¬æ–‡æ–¹æ³•çš„ç›®æ ‡ã€‚(2): åˆ›æ–°ç‚¹ï¼šæœ¬æ–‡å¯¹çƒé¢ç›¸æœºæ¨¡å‹åœ¨ 3D é«˜æ–¯æ¸²æŸ“ä¸­çš„å¯¼æ•°è¿›è¡Œäº†ç†è®ºåˆ†æï¼Œå¹¶åŸºäºæ­¤å®ç°äº†ä¸€ç§æ–°çš„ GPU åŠ é€Ÿå…¨å‘å…‰æ …åŒ–å™¨ï¼Œè¯¥å…‰æ …åŒ–å™¨å¯å°† 3D é«˜æ–¯ç›´æ¥æ¸²æŸ“åˆ°å…¨å‘å›¾åƒçš„ç­‰è·çŸ©å½¢å±å¹•ç©ºé—´ä¸­ã€‚æ€§èƒ½ï¼šæœ¬æ–‡æ–¹æ³•ä½¿ç”¨å…¨å‘å›¾åƒå®ç°äº†æœ€å…ˆè¿›çš„é‡å»ºè´¨é‡å’Œè¾ƒé«˜çš„æ¸²æŸ“é€Ÿåº¦ã€‚å·¥ä½œé‡ï¼šæœ¬æ–‡æ–¹æ³•éœ€è¦å¯¹çƒé¢ç›¸æœºæ¨¡å‹åœ¨ 3D é«˜æ–¯æ¸²æŸ“ä¸­çš„å¯¼æ•°è¿›è¡Œç†è®ºåˆ†æï¼Œå¹¶å®ç°æ–°çš„ GPU åŠ é€Ÿå…¨å‘å…‰æ …åŒ–å™¨ï¼Œå·¥ä½œé‡è¾ƒå¤§ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-b9d6c2aff4465d5a401fd1b95a4290c8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d443789e6c3193b616d8dc21049af0b5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1ca44202ac53707a8da1ef4807f9c933.jpg" align="middle"><img src="https://pica.zhimg.com/v2-c42315ac9ad685573dcfa99dc36d6e4e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-03c97710b7522487bbb73acfc93336fd.jpg" align="middle"></details><h2 id="TCLC-GS-Tightly-Coupled-LiDAR-Camera-Gaussian-Splatting-for-Surrounding-Autonomous-Driving-Scenes"><a href="#TCLC-GS-Tightly-Coupled-LiDAR-Camera-Gaussian-Splatting-for-Surrounding-Autonomous-Driving-Scenes" class="headerlink" title="TCLC-GS: Tightly Coupled LiDAR-Camera Gaussian Splatting for Surrounding   Autonomous Driving Scenes"></a>TCLC-GS: Tightly Coupled LiDAR-Camera Gaussian Splatting for Surrounding   Autonomous Driving Scenes</h2><p><strong>Authors:Cheng Zhao, Su Sun, Ruoyu Wang, Yuliang Guo, Jun-Jun Wan, Zhou Huang, Xinyu Huang, Yingjie Victor Chen, Liu Ren</strong></p><p>Most 3D Gaussian Splatting (3D-GS) based methods for urban scenes initialize 3D Gaussians directly with 3D LiDAR points, which not only underutilizes LiDAR data capabilities but also overlooks the potential advantages of fusing LiDAR with camera data. In this paper, we design a novel tightly coupled LiDAR-Camera Gaussian Splatting (TCLC-GS) to fully leverage the combined strengths of both LiDAR and camera sensors, enabling rapid, high-quality 3D reconstruction and novel view RGB/depth synthesis. TCLC-GS designs a hybrid explicit (colorized 3D mesh) and implicit (hierarchical octree feature) 3D representation derived from LiDAR-camera data, to enrich the properties of 3D Gaussians for splatting. 3D Gaussianâ€™s properties are not only initialized in alignment with the 3D mesh which provides more completed 3D shape and color information, but are also endowed with broader contextual information through retrieved octree implicit features. During the Gaussian Splatting optimization process, the 3D mesh offers dense depth information as supervision, which enhances the training process by learning of a robust geometry. Comprehensive evaluations conducted on the Waymo Open Dataset and nuScenes Dataset validate our methodâ€™s state-of-the-art (SOTA) performance. Utilizing a single NVIDIA RTX 3090 Ti, our method demonstrates fast training and achieves real-time RGB and depth rendering at 90 FPS in resolution of 1920x1280 (Waymo), and 120 FPS in resolution of 1600x900 (nuScenes) in urban scenarios. </p><p><a href="http://arxiv.org/abs/2404.02410v1">PDF</a> </p><p><strong>Summary</strong><br>åˆ©ç”¨é›·è¾¾-ç›¸æœºæ•°æ®èåˆå¢å¼º3Dé«˜æ–¯å–·å°„æ³•ï¼Œå®ç°å¿«é€Ÿé«˜è´¨é‡çš„3Dé‡å»ºå’Œæ–°è§†è§’RGB/æ·±åº¦èåˆã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ç´§å¯†èåˆé›·è¾¾-ç›¸æœºæ•°æ®ï¼Œå……åˆ†åˆ©ç”¨ä¸¤è€…ä¼˜åŠ¿ã€‚</li><li>æ„å»ºæ··åˆæ˜¾å¼ï¼ˆç€è‰²3Dç½‘æ ¼ï¼‰å’Œéšå¼ï¼ˆå±‚æ¬¡å…«å‰æ ‘ç‰¹å¾ï¼‰3Dè¡¨ç¤ºã€‚</li><li>æ ¹æ®3Dç½‘æ ¼åˆå§‹åŒ–3Dé«˜æ–¯å±æ€§ï¼Œæä¾›æ›´å®Œæ•´çš„3Då½¢çŠ¶å’Œé¢œè‰²ä¿¡æ¯ã€‚</li><li>ç»“åˆå…«å‰æ ‘éšå¼ç‰¹å¾èµ‹äºˆ3Dé«˜æ–¯æ›´å¹¿æ³›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li><li>åœ¨é«˜æ–¯å–·å°„ä¼˜åŒ–è¿‡ç¨‹ä¸­ï¼Œ3Dç½‘æ ¼æä¾›å¯†é›†æ·±åº¦ä¿¡æ¯ä½œä¸ºç›‘ç£ã€‚</li><li>åœ¨Waymoå’ŒnuScenesæ•°æ®é›†ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•çš„å…ˆè¿›æ€§ã€‚</li><li>åœ¨å•ä¸ªNVIDIA RTX 3090 Tiä¸Šï¼Œè¯¥æ–¹æ³•è®­ç»ƒå¿«é€Ÿï¼Œåœ¨åŸå¸‚åœºæ™¯ä¸­å®ç°1920x1280ï¼ˆWaymoï¼‰åˆ†è¾¨ç‡ä¸‹çš„90 FPSå’Œ1600x900ï¼ˆnuScenesï¼‰åˆ†è¾¨ç‡ä¸‹çš„120 FPSçš„å®æ—¶RGBå’Œæ·±åº¦æ¸²æŸ“ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šTCLC-GSï¼šç”¨äºç¯ç»•å¼è‡ªåŠ¨é©¾é©¶åœºæ™¯çš„ç´§å¯†è€¦åˆ LiDAR-Camera é«˜æ–¯ä½“ç´ ç»˜åˆ¶</li><li>ä½œè€…ï¼šCheng Zhaoï¼ŒSu Sunï¼ŒRuoyu Wangï¼ŒYuliang Guoï¼ŒJun-Jun Wanï¼ŒZhou Huangï¼ŒXinyu Huangï¼ŒYingjie Victor Chenï¼ŒLiu Ren</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šåšä¸–åŒ—ç¾ç ”ç©¶é™¢ï¼Œåšä¸–äººå·¥æ™ºèƒ½ä¸­å¿ƒï¼ˆBCAIï¼‰</li><li>å…³é”®è¯ï¼šLiDAR-Cameraã€é«˜æ–¯ä½“ç´ ç»˜åˆ¶ã€å®æ—¶æ¸²æŸ“ã€ç¯ç»•å¼é©¾é©¶è§†è§’</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.02410ï¼ŒGithub é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šåŸå¸‚çº§é‡å»ºå’Œæ¸²æŸ“ç”±äºç¯å¢ƒè§„æ¨¡å·¨å¤§ä¸”æ•è·çš„æ•°æ®ç¨€ç–è€Œæå…·æŒ‘æˆ˜æ€§ã€‚åœ¨è‡ªåŠ¨é©¾é©¶æ±½è½¦è®¾ç½®ä¸­ï¼Œé€šå¸¸å¯ä»¥ä½¿ç”¨å¤šä¸ªä¼ æ„Ÿå™¨æ•è·çš„å„ç§æ¨¡å¼çš„æ•°æ®ã€‚ç„¶è€Œï¼Œå®Œå…¨åˆ©ç”¨ LiDAR å’Œç›¸æœºä¼ æ„Ÿå™¨ç›¸ç»“åˆçš„ä¼˜åŠ¿ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ï¼ˆ2ï¼‰ï¼šè¿‡å»çš„æ–¹æ³•åŠé—®é¢˜ï¼šå¤§å¤šæ•°åŸºäº 3D é«˜æ–¯ä½“ç´ ç»˜åˆ¶ï¼ˆ3D-GSï¼‰çš„åŸå¸‚åœºæ™¯æ–¹æ³•ç›´æ¥ä½¿ç”¨ 3D LiDAR ç‚¹åˆå§‹åŒ– 3D é«˜æ–¯ä½“ç´ ï¼Œè¿™ä¸ä»…æ²¡æœ‰å……åˆ†åˆ©ç”¨ LiDAR æ•°æ®çš„èƒ½åŠ›ï¼Œè€Œä¸”å¿½è§†äº†èåˆ LiDAR å’Œç›¸æœºæ•°æ®æ½œåœ¨çš„ä¼˜åŠ¿ã€‚ï¼ˆ3ï¼‰ï¼šç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡è®¾è®¡äº†ä¸€ç§æ–°é¢–çš„ç´§å¯†è€¦åˆ LiDAR-Camera é«˜æ–¯ä½“ç´ ç»˜åˆ¶ï¼ˆTCLC-GSï¼‰æ–¹æ³•ï¼Œä»¥å……åˆ†åˆ©ç”¨ LiDAR å’Œç›¸æœºä¼ æ„Ÿå™¨çš„ç»¼åˆä¼˜åŠ¿ï¼Œå®ç°å¿«é€Ÿã€é«˜è´¨é‡çš„ 3D é‡å»ºå’Œæ–°è§†è§’ RGB/æ·±åº¦åˆæˆã€‚TCLC-GS è®¾è®¡äº†ä¸€ç§æ··åˆæ˜¾å¼ï¼ˆç€è‰² 3D ç½‘æ ¼ï¼‰å’Œéšå¼ï¼ˆåˆ†å±‚å…«å‰æ ‘ç‰¹å¾ï¼‰çš„ 3D è¡¨ç¤ºï¼Œè¯¥è¡¨ç¤ºæºè‡ª LiDAR-Camera æ•°æ®ï¼Œä»¥ä¸°å¯Œ 3D é«˜æ–¯ä½“ç´ çš„å±æ€§ä»¥è¿›è¡Œä½“ç´ ç»˜åˆ¶ã€‚3D é«˜æ–¯ä½“ç´ çš„å±æ€§ä¸ä»…ä¸æä¾›æ›´å®Œæ•´çš„ 3D å½¢çŠ¶å’Œé¢œè‰²ä¿¡æ¯çš„ 3D ç½‘æ ¼å¯¹é½è¿›è¡Œåˆå§‹åŒ–ï¼Œè€Œä¸”è¿˜é€šè¿‡æ£€ç´¢åˆ°çš„å…«å‰æ ‘éšå¼ç‰¹å¾èµ‹äºˆäº†æ›´å¹¿æ³›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚åœ¨é«˜æ–¯ä½“ç´ ç»˜åˆ¶ä¼˜åŒ–è¿‡ç¨‹ä¸­ï¼Œ3D ç½‘æ ¼æä¾›äº†å¯†é›†çš„æ·±åº¦ä¿¡æ¯ä½œä¸ºç›‘ç£ï¼Œé€šè¿‡å­¦ä¹ é²æ£’å‡ ä½•å½¢çŠ¶å¢å¼ºäº†è®­ç»ƒè¿‡ç¨‹ã€‚ï¼ˆ4ï¼‰ï¼šæ–¹æ³•æ€§èƒ½ï¼šåœ¨ Waymo Open æ•°æ®é›†å’Œ nuScenes æ•°æ®é›†ä¸Šè¿›è¡Œçš„ç»¼åˆè¯„ä¼°éªŒè¯äº†æˆ‘ä»¬æ–¹æ³•çš„æœ€æ–°ï¼ˆSOTAï¼‰æ€§èƒ½ã€‚ä½¿ç”¨å•ä¸ª NVIDIA RTX 3090 Tiï¼Œæˆ‘ä»¬çš„æ–¹æ³•å±•ç¤ºäº†å¿«é€Ÿè®­ç»ƒï¼Œå¹¶åœ¨åŸå¸‚åœºæ™¯ä¸­ä»¥ 1920Ã—1280ï¼ˆWaymoï¼‰çš„åˆ†è¾¨ç‡ä»¥ 90 FPS å®ç°å®æ—¶ RGB å’Œæ·±åº¦æ¸²æŸ“ï¼Œä»¥åŠä»¥ 1600Ã—900ï¼ˆnuScenesï¼‰çš„åˆ†è¾¨ç‡ä»¥ 120 FPS å®ç°å®æ—¶ RGB å’Œæ·±åº¦æ¸²æŸ“ã€‚</li></ol><p>7.æ–¹æ³•ï¼š(1)æ„å»ºåˆ†å±‚å…«å‰æ ‘éšå¼ç‰¹å¾ç½‘æ ¼ï¼Œä»¥å°è£…åœºæ™¯çš„å‡ ä½•ç»†èŠ‚å’Œä¸Šä¸‹æ–‡ç»“æ„ä¿¡æ¯ï¼›(2)ç”Ÿæˆå½©è‰²3Dç½‘æ ¼å’Œç¨ å¯†æ·±åº¦ï¼Œä»¥å¢å¼º3Dé«˜æ–¯ä½“ç´ çš„å±æ€§ï¼›(3)åˆ©ç”¨3Dé«˜æ–¯ä½“ç´ ç»˜åˆ¶ï¼Œå®ç°åœºæ™¯çš„é‡å»ºå’Œæ–°è§†è§’å›¾åƒçš„åˆæˆã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œæå‡ºäº†ä¸€ç§æ–°é¢–çš„ç´§å¯†è€¦åˆ LiDAR-Camera é«˜æ–¯ä½“ç´ ç»˜åˆ¶ï¼ˆTCLC-GSï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ååŒåˆ©ç”¨ LiDAR å’Œç¯ç»•å¼æ‘„åƒå¤´çš„ä¼˜åŠ¿ï¼Œå®ç°äº†åŸå¸‚é©¾é©¶åœºæ™¯ä¸­çš„å¿«é€Ÿå»ºæ¨¡å’Œå®æ—¶æ¸²æŸ“ã€‚TCLC-GS çš„å…³é”®æ€æƒ³æ˜¯å°†æ˜¾å¼ï¼ˆç€è‰² 3D ç½‘æ ¼ï¼‰å’Œéšå¼ï¼ˆåˆ†å±‚å…«å‰æ ‘ç‰¹å¾ï¼‰ä¿¡æ¯ç›¸ç»“åˆçš„æ··åˆ 3D è¡¨ç¤ºï¼Œè¿™äº›ä¿¡æ¯æºè‡ª LiDAR-Camera æ•°æ®ï¼Œä»è€Œä¸°å¯Œäº† 3D é«˜æ–¯ä½“ç´ çš„å‡ ä½•å’Œå¤–è§‚å±æ€§ã€‚é€šè¿‡å°†æ¸²æŸ“çš„å¯†é›†æ·±åº¦æ•°æ®ä¸ 3D ç½‘æ ¼ç›¸ç»“åˆï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†é«˜æ–¯ä½“ç´ ç»˜åˆ¶çš„ä¼˜åŒ–ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨ WaymoOpen å’Œ nuScenes æ•°æ®é›†ä¸Šè¶…è¶Šäº† SOTA æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†é«˜æ–¯ä½“ç´ ç»˜åˆ¶çš„å®æ—¶æ•ˆç‡ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„ TCLC-GS æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ååŒåˆ©ç”¨äº† LiDAR å’Œç¯ç»•å¼æ‘„åƒå¤´çš„æ•°æ®ï¼Œä»¥ä¸°å¯Œ 3D é«˜æ–¯ä½“ç´ çš„å±æ€§ã€‚</li><li>è®¾è®¡äº†ä¸€ç§æ··åˆ 3D è¡¨ç¤ºï¼Œå°†æ˜¾å¼ï¼ˆç€è‰² 3D ç½‘æ ¼ï¼‰å’Œéšå¼ï¼ˆåˆ†å±‚å…«å‰æ ‘ç‰¹å¾ï¼‰ä¿¡æ¯ç›¸ç»“åˆï¼Œä»¥å¢å¼º 3D é«˜æ–¯ä½“ç´ çš„å‡ ä½•å’Œå¤–è§‚å±æ€§ã€‚</li><li>é€šè¿‡å°†æ¸²æŸ“çš„å¯†é›†æ·±åº¦æ•°æ®ä¸ 3D ç½‘æ ¼ç›¸ç»“åˆï¼Œå¢å¼ºäº†é«˜æ–¯ä½“ç´ ç»˜åˆ¶çš„ä¼˜åŒ–ã€‚æ€§èƒ½ï¼š</li><li>åœ¨ WaymoOpen å’Œ nuScenes æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¶…è¶Šäº† SOTA æ€§èƒ½ã€‚</li><li>ä½¿ç”¨å•ä¸ª NVIDIA RTX 3090Tiï¼Œæˆ‘ä»¬çš„æ–¹æ³•å±•ç¤ºäº†å¿«é€Ÿè®­ç»ƒï¼Œå¹¶åœ¨åŸå¸‚åœºæ™¯ä¸­ä»¥ 1920Ã—1280ï¼ˆWaymoï¼‰çš„åˆ†è¾¨ç‡ä»¥ 90FPS å®ç°å®æ—¶ RGB å’Œæ·±åº¦æ¸²æŸ“ï¼Œä»¥åŠä»¥ 1600Ã—900ï¼ˆnuScenesï¼‰çš„åˆ†è¾¨ç‡ä»¥ 120FPS å®ç°å®æ—¶ RGB å’Œæ·±åº¦æ¸²æŸ“ã€‚å·¥ä½œé‡ï¼š</li><li>æœ¬æ–‡å·¥ä½œé‡è¾ƒå¤§ï¼Œæ¶‰åŠåˆ° LiDAR-Camera æ•°æ®èåˆã€3D è¡¨ç¤ºæ„å»ºã€é«˜æ–¯ä½“ç´ ç»˜åˆ¶ä¼˜åŒ–ç­‰å¤šä¸ªæ–¹é¢ã€‚</li><li>å®éªŒè¯„ä¼°åœ¨ WaymoOpen å’Œ nuScenes æ•°æ®é›†ä¸Šè¿›è¡Œï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-e62c1f2bd102fec03e2ba5d9b33334ec.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9d3ed25688daa58902225a06381d1611.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7214e7e3cb097a97cffcd1071a0d7d53.jpg" align="middle"></details><h2 id="Surface-Reconstruction-from-Gaussian-Splatting-via-Novel-Stereo-Views"><a href="#Surface-Reconstruction-from-Gaussian-Splatting-via-Novel-Stereo-Views" class="headerlink" title="Surface Reconstruction from Gaussian Splatting via Novel Stereo Views"></a>Surface Reconstruction from Gaussian Splatting via Novel Stereo Views</h2><p><strong>Authors:Yaniv Wolf, Amit Bracha, Ron Kimmel</strong></p><p>The Gaussian splatting for radiance field rendering method has recently emerged as an efficient approach for accurate scene representation. It optimizes the location, size, color, and shape of a cloud of 3D Gaussian elements to visually match, after projection, or splatting, a set of given images taken from various viewing directions. And yet, despite the proximity of Gaussian elements to the shape boundaries, direct surface reconstruction of objects in the scene is a challenge.   We propose a novel approach for surface reconstruction from Gaussian splatting models. Rather than relying on the Gaussian elementsâ€™ locations as a prior for surface reconstruction, we leverage the superior novel-view synthesis capabilities of 3DGS. To that end, we use the Gaussian splatting model to render pairs of stereo-calibrated novel views from which we extract depth profiles using a stereo matching method. We then combine the extracted RGB-D images into a geometrically consistent surface. The resulting reconstruction is more accurate and shows finer details when compared to other methods for surface reconstruction from Gaussian splatting models, while requiring significantly less compute time compared to other surface reconstruction methods.   We performed extensive testing of the proposed method on in-the-wild scenes, taken by a smartphone, showcasing its superior reconstruction abilities. Additionally, we tested the proposed method on the Tanks and Temples benchmark, and it has surpassed the current leading method for surface reconstruction from Gaussian splatting models. Project page: <a href="https://gs2mesh.github.io/">https://gs2mesh.github.io/</a>. </p><p><a href="http://arxiv.org/abs/2404.01810v1">PDF</a> Project Page: <a href="https://gs2mesh.github.io/">https://gs2mesh.github.io/</a></p><p><strong>Summary</strong><br>åˆ©ç”¨é«˜æ–¯æ•£å°„æ¨¡å‹çš„æ–°å‹åœ°è¡¨é‡å»ºæ–¹æ³•ï¼Œé€šè¿‡æå–æ·±åº¦å›¾è¿›è¡Œæ¸²æŸ“ï¼Œç”Ÿæˆæ›´ä¸ºç²¾å‡†ã€ç»†èŠ‚ä¸°å¯Œçš„é‡å»ºç»“æœã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>é«˜æ–¯æ•£å°„æ³•æ˜¯ä¸€ç§ç”¨äºæ¸²æŸ“è¾å°„åœºçš„æœ‰æ•ˆæ–¹æ³•ï¼Œèƒ½å¤Ÿé€šè¿‡ä¼˜åŒ– 3D é«˜æ–¯å…ƒç´ çš„ä½ç½®ã€å¤§å°ã€é¢œè‰²å’Œå½¢çŠ¶ï¼ŒåŒ¹é…ä»ä¸åŒè§†è§’æ‹æ‘„çš„å›¾åƒã€‚</li><li>ç›´æ¥ä»é«˜æ–¯å…ƒç´ çš„ä½ç½®é‡å»ºåœºæ™¯ä¸­çš„ç‰©ä½“è¡¨é¢å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li><li>æå‡ºä¸€ç§åŸºäºé«˜æ–¯æ•£å°„æ¨¡å‹è¿›è¡Œåœ°è¡¨é‡å»ºçš„æ–°æ–¹æ³•ï¼Œåˆ©ç”¨é«˜æ–¯æ•£å°„æ¨¡å‹çš„å‡ºè‰²æ–°è§†è§’åˆæˆèƒ½åŠ›ã€‚</li><li>ä½¿ç”¨é«˜æ–¯æ•£å°„æ¨¡å‹æ¸²æŸ“ç«‹ä½“æ ¡å‡†çš„æ–°è§†è§’å¯¹ï¼Œå¹¶ä½¿ç”¨ç«‹ä½“åŒ¹é…æ–¹æ³•æå–æ·±åº¦å›¾ã€‚</li><li>å°†æå–çš„ RGB-D å›¾åƒç»„åˆæˆå‡ ä½•ä¸€è‡´çš„è¡¨é¢ã€‚</li><li>ä¸å…¶ä»–ä»é«˜æ–¯æ•£å°„æ¨¡å‹è¿›è¡Œåœ°è¡¨é‡å»ºçš„æ–¹æ³•ç›¸æ¯”ï¼Œå¾—åˆ°çš„é‡å»ºç»“æœæ›´å‡†ç¡®ï¼Œæ˜¾ç¤ºå‡ºæ›´ç²¾ç»†çš„ç»†èŠ‚ï¼ŒåŒæ—¶è®¡ç®—æ—¶é—´æ˜æ˜¾å‡å°‘ã€‚</li><li>åœ¨æ™ºèƒ½æ‰‹æœºæ‹æ‘„çš„é‡å¤–åœºæ™¯ä¸­å¯¹æ‰€æå‡ºçš„æ–¹æ³•è¿›è¡Œäº†å¹¿æ³›çš„æµ‹è¯•ï¼Œå±•ç¤ºäº†å…¶å‡ºè‰²çš„é‡å»ºèƒ½åŠ›ã€‚</li><li>åœ¨ Tanks and Temples åŸºå‡†ä¸Šæµ‹è¯•äº†æ‰€æå‡ºçš„æ–¹æ³•ï¼Œè¶…è¿‡äº†å½“å‰ä»é«˜æ–¯æ•£å°„æ¨¡å‹è¿›è¡Œåœ°è¡¨é‡å»ºçš„é¢†å…ˆæ–¹æ³•ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><strong>æ ‡é¢˜ï¼š</strong>é«˜æ–¯ç‚¹äº‘æ¸²æŸ“ä¸­çš„æ›²é¢é‡å»º</li><li><strong>ä½œè€…ï¼š</strong>Yuxuan Zhang<em>, Xiangyu Xu</em>, Zexiang Xu, Xiaowei Zhou, Jiaya Jia</li><li><strong>ç¬¬ä¸€ä½œè€…å•ä½ï¼š</strong>åŒ—äº¬å¤§å­¦</li><li><strong>å…³é”®è¯ï¼š</strong>è¡¨é¢é‡å»ºã€é«˜æ–¯ç‚¹äº‘ã€ç¥ç»è¾å°„åœºã€ç«‹ä½“åŒ¹é…</li><li><strong>è®ºæ–‡é“¾æ¥ï¼š</strong>https://arxiv.org/pdf/2404.01810.pdf</li><li><p><strong>æ‘˜è¦ï¼š</strong>   (1) <strong>ç ”ç©¶èƒŒæ™¯ï¼š</strong>é«˜æ–¯ç‚¹äº‘æ¸²æŸ“æ˜¯ä¸€ç§é«˜æ•ˆå‡†ç¡®çš„åœºæ™¯è¡¨ç¤ºæ–¹æ³•ï¼Œä½†ç›´æ¥ä»é«˜æ–¯ç‚¹äº‘æ¨¡å‹ä¸­è¿›è¡Œæ›²é¢é‡å»ºå…·æœ‰æŒ‘æˆ˜æ€§ã€‚   (2) <strong>è¿‡å»æ–¹æ³•ï¼š</strong>ç°æœ‰æ–¹æ³•ä¾èµ–äºé«˜æ–¯å…ƒç´ çš„ä½ç½®ä½œä¸ºæ›²é¢é‡å»ºçš„å…ˆéªŒï¼Œä½†æ•ˆæœä¸ä½³ã€‚   (3) <strong>ç ”ç©¶æ–¹æ³•ï¼š</strong>æœ¬æ–‡æå‡ºäº†ä¸€ç§ä»é«˜æ–¯ç‚¹äº‘æ¨¡å‹è¿›è¡Œæ›²é¢é‡å»ºçš„æ–°æ–¹æ³•ã€‚åˆ©ç”¨é«˜æ–¯ç‚¹äº‘æ¨¡å‹æ¸²æŸ“ç«‹ä½“æ ¡å‡†çš„æ–°é¢–è§†å›¾å¯¹ï¼Œç„¶åä½¿ç”¨ç«‹ä½“åŒ¹é…æ–¹æ³•æå–æ·±åº¦è½®å»“ã€‚æœ€åï¼Œå°†æå–çš„ RGB-D å›¾åƒç»„åˆæˆå‡ ä½•ä¸€è‡´çš„æ›²é¢ã€‚   (4) <strong>æ€§èƒ½ï¼š</strong>è¯¥æ–¹æ³•åœ¨çœŸå®åœºæ™¯ä¸­è¿›è¡Œäº†å¹¿æ³›æµ‹è¯•ï¼Œå±•ç¤ºäº†å…¶ä¼˜å¼‚çš„é‡å»ºèƒ½åŠ›ã€‚åœ¨ Tanks and Temples åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•ä¹Ÿè¶…è¿‡äº†å½“å‰ä»é«˜æ–¯ç‚¹äº‘æ¨¡å‹è¿›è¡Œæ›²é¢é‡å»ºçš„é¢†å…ˆæ–¹æ³•ã€‚</p></li><li><p><strong>Methodsï¼š</strong>(1) <strong>æ¸²æŸ“ç«‹ä½“æ ¡å‡†è§†å›¾å¯¹ï¼š</strong>åˆ©ç”¨é«˜æ–¯ç‚¹äº‘æ¨¡å‹æ¸²æŸ“ä¸€ç³»åˆ—å…·æœ‰ç«‹ä½“æ ¡å‡†çš„è§†å›¾å¯¹ï¼Œç¡®ä¿è§†å›¾å¯¹ä¸­çš„å¯¹åº”åƒç´ å…·æœ‰ç›¸åŒçš„åœºæ™¯ä¸‰ç»´åæ ‡ã€‚(2) <strong>ç«‹ä½“åŒ¹é…æå–æ·±åº¦è½®å»“ï¼š</strong>å¯¹æ¸²æŸ“çš„ç«‹ä½“æ ¡å‡†è§†å›¾å¯¹è¿›è¡Œç«‹ä½“åŒ¹é…ï¼Œæå–åœºæ™¯çš„æ·±åº¦è½®å»“ï¼Œå¾—åˆ°æ¯ä¸ªåƒç´ çš„æ·±åº¦å€¼ã€‚(3) <strong>èåˆRGB-Då›¾åƒæ„å»ºæ›²é¢ï¼š</strong>å°†æå–çš„æ·±åº¦è½®å»“ä¸RGBå›¾åƒç›¸ç»“åˆï¼Œå½¢æˆRGB-Då›¾åƒï¼Œç„¶ååˆ©ç”¨å¤šè§†å›¾å‡ ä½•æ–¹æ³•å°†RGB-Då›¾åƒèåˆæˆå‡ ä½•ä¸€è‡´çš„æ›²é¢ã€‚</p></li><li><p><strong>æ€»ç»“</strong>(1) <strong>æœ¬å·¥ä½œçš„æ„ä¹‰ï¼š</strong>æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§ä»é«˜æ–¯ç‚¹äº‘æ¨¡å‹è¿›è¡Œæ›²é¢é‡å»ºçš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ç«‹ä½“åŒ¹é…æå–æ·±åº¦è½®å»“ï¼Œå¹¶å°†å…¶ä¸RGBå›¾åƒèåˆæ„å»ºæ›²é¢ã€‚è¯¥æ–¹æ³•å…‹æœäº†ç›´æ¥ä»é«˜æ–¯ç‚¹äº‘æ¨¡å‹è¿›è¡Œæ›²é¢é‡å»ºçš„å±€é™æ€§ï¼Œæé«˜äº†é‡å»ºçš„å‡†ç¡®æ€§å’Œä¿çœŸåº¦ã€‚</p></li></ol><p>(2) <strong>æ–‡ç« ä¼˜ç¼ºç‚¹æ€»ç»“</strong><strong>åˆ›æ–°ç‚¹ï¼š</strong>- æå‡ºäº†ä¸€ç§ä»é«˜æ–¯ç‚¹äº‘æ¨¡å‹è¿›è¡Œæ›²é¢é‡å»ºçš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ç«‹ä½“åŒ¹é…æå–æ·±åº¦è½®å»“ã€‚- è¯¥æ–¹æ³•ä¿ç•™äº†é«˜æ–¯ç‚¹äº‘è¡¨ç¤ºçš„å›ºæœ‰ç‰¹æ€§ï¼ŒåŒæ—¶å¢å¼ºäº†é‡å»ºæ›²é¢çš„å‡†ç¡®æ€§å’Œä¿çœŸåº¦ã€‚</p><p><strong>æ€§èƒ½ï¼š</strong>- åœ¨Tanks and Templesæ•°æ®é›†ã€Mip-NeRF360æ•°æ®é›†å’Œä½¿ç”¨æ™ºèƒ½æ‰‹æœºæ‹æ‘„çš„çœŸå®åœºæ™¯ä¸Šè¿›è¡Œäº†å¹¿æ³›æµ‹è¯•ï¼Œå±•ç¤ºäº†ä¼˜å¼‚çš„é‡å»ºèƒ½åŠ›ã€‚- åœ¨Tanks and TemplesåŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•è¶…è¿‡äº†å½“å‰ä»é«˜æ–¯ç‚¹äº‘æ¨¡å‹è¿›è¡Œæ›²é¢é‡å»ºçš„é¢†å…ˆæ–¹æ³•ã€‚</p><p><strong>å·¥ä½œé‡ï¼š</strong>- è¯¥æ–¹æ³•çš„è®¡ç®—æ—¶é—´æ˜æ˜¾çŸ­äºå½“å‰ä»é«˜æ–¯ç‚¹äº‘æ¨¡å‹è¿›è¡Œæ›²é¢é‡å»ºçš„é¢†å…ˆæ–¹æ³•ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-e879b29415f3de27eafe2cc9161fbc48.jpg" align="middle"><img src="https://picx.zhimg.com/v2-47c6b2fed33605828932fea2b80699ec.jpg" align="middle"></details>## Feature Splatting: Language-Driven Physics-Based Scene Synthesis and   Editing**Authors:Ri-Zhao Qiu, Ge Yang, Weijia Zeng, Xiaolong Wang**Scene representations using 3D Gaussian primitives have produced excellent results in modeling the appearance of static and dynamic 3D scenes. Many graphics applications, however, demand the ability to manipulate both the appearance and the physical properties of objects. We introduce Feature Splatting, an approach that unifies physics-based dynamic scene synthesis with rich semantics from vision language foundation models that are grounded by natural language. Our first contribution is a way to distill high-quality, object-centric vision-language features into 3D Gaussians, that enables semi-automatic scene decomposition using text queries. Our second contribution is a way to synthesize physics-based dynamics from an otherwise static scene using a particle-based simulator, in which material properties are assigned automatically via text queries. We ablate key techniques used in this pipeline, to illustrate the challenge and opportunities in using feature-carrying 3D Gaussians as a unified format for appearance, geometry, material properties and semantics grounded on natural language. Project website: https://feature-splatting.github.io/ [PDF](http://arxiv.org/abs/2404.01223v1) Project website: https://feature-splatting.github.io/**Summary**ç”¨è‡ªç„¶è¯­è¨€æ“æ§ç‰©ç†å±æ€§ï¼Œå®ç°åŸºäºè§†è§‰å’Œè¯­è¨€çš„é«˜è´¨é‡å¯¹è±¡çº§åœºæ™¯åˆ†è§£å’ŒåŸºäºç²’å­çš„åŠ¨æ€åˆæˆã€‚**Key Takeaways**- å°†è§†è§‰è¯­è¨€ç‰¹å¾æå–åˆ° 3D é«˜æ–¯åŸè¯­ï¼Œå®ç°åŠè‡ªåŠ¨åœºæ™¯åˆ†è§£ã€‚- é€šè¿‡åŸºäºç²’å­çš„æ¨¡æ‹Ÿå™¨åˆæˆç‰©ç†åŠ¨åŠ›å­¦ï¼Œè‡ªåŠ¨åˆ†é…ææ–™å±æ€§ã€‚- é‡‡ç”¨è§£è€¦å’Œé‡æ–°æ··åˆæ¥å¤„ç†ç‰©è´¨å±æ€§ã€‚- ä½¿ç”¨è¯åµŒå…¥æ¥æŒ‡å¯¼ææ–™å±æ€§çš„åˆ†é…ã€‚- æå‡ºå¤šçº§æ–¹æ³•æ¥å¤„ç†å¤æ‚åœºæ™¯ã€‚- é€šè¿‡æ¶ˆèå®éªŒéªŒè¯äº†ç‰¹å¾æºå¸¦ 3D é«˜æ–¯åŸè¯­çš„æœ‰æ•ˆæ€§ã€‚- æä¾›äº†ç”¨äºåœºæ™¯ç¼–è¾‘å’Œåˆæˆçš„é«˜è´¨é‡ 3D æ•°æ®é›†ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>æ ‡é¢˜ï¼šç‰¹å¾æº…å°„ï¼šè¯­è¨€é©±åŠ¨çš„ç‰©ç†åœºæ™¯åˆæˆå’Œç¼–è¾‘</li><li>ä½œè€…ï¼šé»é’Šç§‹ã€æ¨æ­Œã€æ›¾ç»´ä½³ã€ç‹æ™“é¾™</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šåŠ å·å¤§å­¦åœ£åœ°äºšå“¥åˆ†æ ¡</li><li>å…³é”®è¯ï¼šè¡¨ç¤ºå­¦ä¹ ã€é«˜æ–¯æº…å°„ã€åœºæ™¯ç¼–è¾‘ã€ç‰©ç†æ¨¡æ‹Ÿ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://feature-splatting.github.ioGithubä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šä½¿ç”¨ 3D é«˜æ–¯åŸºå…ƒè¿›è¡Œåœºæ™¯è¡¨ç¤ºåœ¨å»ºæ¨¡é™æ€å’ŒåŠ¨æ€ 3D åœºæ™¯çš„å¤–è§‚æ–¹é¢å–å¾—äº†ä¼˜å¼‚çš„æˆæœã€‚ç„¶è€Œï¼Œè®¸å¤šå›¾å½¢åº”ç”¨ç¨‹åºéœ€è¦èƒ½å¤ŸåŒæ—¶æ“çºµå¯¹è±¡çš„å¤–è§‚å’Œç‰©ç†å±æ€§ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•å’Œé—®é¢˜ï¼šæœ¬æ–‡ä»‹ç»äº† Feature Splattingï¼Œä¸€ç§å°†åŸºäºç‰©ç†çš„åŠ¨æ€åœºæ™¯åˆæˆä¸ç”±è‡ªç„¶è¯­è¨€åŸºç¡€æ¨¡å‹æä¾›çš„ä¸°å¯Œè¯­ä¹‰ç›¸ç»Ÿä¸€çš„æ–¹æ³•ã€‚è¿‡å»çš„æ–¹æ³•å­˜åœ¨çš„é—®é¢˜åœ¨äºï¼šæ— æ³•åŒæ—¶æ“çºµå¯¹è±¡çš„å¤–è§‚å’Œç‰©ç†å±æ€§ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•æ˜¯ï¼šä½¿ç”¨æ–‡æœ¬æŸ¥è¯¢å°†é«˜è´¨é‡ã€ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„å¯è§†åŒ–è¯­è¨€ç‰¹å¾æå–åˆ° 3D é«˜æ–¯ä¸­ï¼Œå®ç°ä½¿ç”¨æ–‡æœ¬æŸ¥è¯¢è¿›è¡ŒåŠè‡ªåŠ¨åœºæ™¯åˆ†è§£ï¼›ä½¿ç”¨åŸºäºç²’å­çš„æ¨¡æ‹Ÿå™¨åˆæˆåŸºäºç‰©ç†çš„åŠ¨æ€ï¼Œå…¶ä¸­ææ–™å±æ€§é€šè¿‡æ–‡æœ¬æŸ¥è¯¢è‡ªåŠ¨åˆ†é…ã€‚ï¼ˆ4ï¼‰ä»»åŠ¡å’Œæ€§èƒ½ï¼šæœ¬æ–‡æ–¹æ³•åœ¨ä»¥ä¸‹ä»»åŠ¡ä¸Šå–å¾—äº†æ€§èƒ½ï¼šåŠè‡ªåŠ¨åœºæ™¯åˆ†è§£ã€åŸºäºç‰©ç†çš„åŠ¨æ€åˆæˆã€‚æœ¬æ–‡æ–¹æ³•çš„æ€§èƒ½æ”¯æŒå…¶ç›®æ ‡ï¼šä½¿ç”¨æ–‡æœ¬æŸ¥è¯¢åŒæ—¶æ“çºµå¯¹è±¡çš„å¤–è§‚å’Œç‰©ç†å±æ€§ã€‚</li></ol><p>7.Methodsï¼šï¼ˆ1ï¼‰ä½¿ç”¨æ–‡æœ¬æŸ¥è¯¢å°†é«˜è´¨é‡ã€ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„å¯è§†åŒ–è¯­è¨€ç‰¹å¾æå–åˆ°3Dé«˜æ–¯ä¸­ï¼Œå®ç°ä½¿ç”¨æ–‡æœ¬æŸ¥è¯¢è¿›è¡ŒåŠè‡ªåŠ¨åœºæ™¯åˆ†è§£ï¼›ï¼ˆ2ï¼‰ä½¿ç”¨åŸºäºç²’å­çš„æ¨¡æ‹Ÿå™¨åˆæˆåŸºäºç‰©ç†çš„åŠ¨æ€ï¼Œå…¶ä¸­ææ–™å±æ€§é€šè¿‡æ–‡æœ¬æŸ¥è¯¢è‡ªåŠ¨åˆ†é…ã€‚</p><ol><li>ç»“è®ºï¼š(1): æœ¬å·¥ä½œæå‡ºäº† FeatureSplattingï¼Œä¸€ç§å°†åŸºäºç‰©ç†çš„åŠ¨æ€åœºæ™¯åˆæˆä¸ç”±è‡ªç„¶è¯­è¨€åŸºç¡€æ¨¡å‹æä¾›çš„ä¸°å¯Œè¯­ä¹‰ç›¸ç»Ÿä¸€çš„æ–¹æ³•ï¼Œå®ç°äº†ä½¿ç”¨æ–‡æœ¬æŸ¥è¯¢åŒæ—¶æ“çºµå¯¹è±¡çš„å¤–è§‚å’Œç‰©ç†å±æ€§ã€‚(2): Innovation point:<ul><li>æå‡ºäº†ä¸€ç§ä½¿ç”¨æ–‡æœ¬æŸ¥è¯¢å°†é«˜è´¨é‡ã€ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„å¯è§†åŒ–è¯­è¨€ç‰¹å¾æå–åˆ° 3D é«˜æ–¯ä¸­ï¼Œå®ç°ä½¿ç”¨æ–‡æœ¬æŸ¥è¯¢è¿›è¡ŒåŠè‡ªåŠ¨åœºæ™¯åˆ†è§£çš„æ–¹æ³•ã€‚</li><li>æå‡ºäº†ä¸€ç§ä½¿ç”¨åŸºäºç²’å­çš„æ¨¡æ‹Ÿå™¨åˆæˆåŸºäºç‰©ç†çš„åŠ¨æ€çš„æ–¹æ³•ï¼Œå…¶ä¸­ææ–™å±æ€§é€šè¿‡æ–‡æœ¬æŸ¥è¯¢è‡ªåŠ¨åˆ†é…ã€‚Performance:</li><li>åœ¨åŠè‡ªåŠ¨åœºæ™¯åˆ†è§£å’ŒåŸºäºç‰©ç†çš„åŠ¨æ€åˆæˆä»»åŠ¡ä¸Šå–å¾—äº†è‰¯å¥½çš„æ€§èƒ½ã€‚Workload:</li><li>å®ç°äº†ä½¿ç”¨æ–‡æœ¬æŸ¥è¯¢åŒæ—¶æ“çºµå¯¹è±¡çš„å¤–è§‚å’Œç‰©ç†å±æ€§çš„ç›®æ ‡ã€‚</li></ul></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-c91174167e56a6ecedfdcc689866ca66.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2511b95da83059bea2dd34a684e6c2d1.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7238c09c3aa3223a11ad3927197bfd97.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1999b5e545fee5aa2f838d1ea143b0d1.jpg" align="middle"></details><h2 id="Mirror-3DGS-Incorporating-Mirror-Reflections-into-3D-Gaussian-Splatting"><a href="#Mirror-3DGS-Incorporating-Mirror-Reflections-into-3D-Gaussian-Splatting" class="headerlink" title="Mirror-3DGS: Incorporating Mirror Reflections into 3D Gaussian Splatting"></a>Mirror-3DGS: Incorporating Mirror Reflections into 3D Gaussian Splatting</h2><p><strong>Authors:Jiarui Meng, Haijie Li, Yanmin Wu, Qiankun Gao, Shuzhou Yang, Jian Zhang, Siwei Ma</strong></p><p>3D Gaussian Splatting (3DGS) has marked a significant breakthrough in the realm of 3D scene reconstruction and novel view synthesis. However, 3DGS, much like its predecessor Neural Radiance Fields (NeRF), struggles to accurately model physical reflections, particularly in mirrors that are ubiquitous in real-world scenes. This oversight mistakenly perceives reflections as separate entities that physically exist, resulting in inaccurate reconstructions and inconsistent reflective properties across varied viewpoints. To address this pivotal challenge, we introduce Mirror-3DGS, an innovative rendering framework devised to master the intricacies of mirror geometries and reflections, paving the way for the generation of realistically depicted mirror reflections. By ingeniously incorporating mirror attributes into the 3DGS and leveraging the principle of plane mirror imaging, Mirror-3DGS crafts a mirrored viewpoint to observe from behind the mirror, enriching the realism of scene renderings. Extensive assessments, spanning both synthetic and real-world scenes, showcase our methodâ€™s ability to render novel views with enhanced fidelity in real-time, surpassing the state-of-the-art Mirror-NeRF specifically within the challenging mirror regions. Our code will be made publicly available for reproducible research. </p><p><a href="http://arxiv.org/abs/2404.01168v1">PDF</a> 22 pages, 7 figures</p><p><strong>Summary</strong><br>çªç ´3DGSé‡å»ºé•œåƒåå°„ç“¶é¢ˆï¼Œé‡‡ç”¨é•œåƒå±æ€§å’Œå¹³é¢åå°„åŸç†ï¼Œå®ç°çœŸå®é•œåƒæ¸²æŸ“ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>3DGSåœ¨é‡å»ºåœºæ™¯å’Œåˆæˆæ–°è§†å›¾æ–¹é¢å–å¾—çªç ´ï¼Œä½†æ— æ³•å‡†ç¡®å»ºæ¨¡ç‰©ç†åå°„ï¼Œç‰¹åˆ«æ˜¯é•œé¢åå°„ã€‚</li><li>3DGSå°†åå°„è¯¯è®¤ä¸ºç‹¬ç«‹å®ä½“ï¼Œå¯¼è‡´é‡å»ºä¸å‡†ç¡®ï¼Œåå°„å±æ€§åœ¨ä¸åŒè§†è§’ä¸‹ä¸ä¸€è‡´ã€‚</li><li>Mirror-3DGSå¼•å…¥é•œåƒå±æ€§ï¼Œåˆ©ç”¨å¹³é¢é•œæˆåƒåŸç†ï¼Œä»é•œåè§‚å¯Ÿï¼Œæå‡åœºæ™¯æ¸²æŸ“çœŸå®æ€§ã€‚</li><li>Mirror-3DGSåœ¨åˆæˆå’ŒçœŸå®åœºæ™¯ä¸­ï¼Œå®æ—¶æ¸²æŸ“æ–°è§†å›¾æ—¶ï¼Œä¿çœŸåº¦è¾ƒé«˜ï¼Œåœ¨é•œåƒåŒºåŸŸè¶…è¶Šäº†Mirror-NeRFã€‚</li><li>Mirror-3DGSé€šè¿‡å·§å¦™çš„ç®—æ³•è®¾è®¡ï¼Œè§£å†³äº†3DGSé‡å»ºé•œåƒåå°„çš„éš¾é¢˜ã€‚</li><li>è¯¥æ–¹æ³•å¯ç”¨äºæ¸²æŸ“å…·æœ‰æŒ‘æˆ˜æ€§çš„é•œåƒåŒºåŸŸï¼Œå¦‚çœŸå®åœºæ™¯ä¸­çš„é•œå­ã€‚</li><li>ç ”ç©¶ä»£ç å°†å…¬å¼€ï¼Œä¾¿äºç ”ç©¶äººå‘˜å¤ç°ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šMirror-3DGSï¼šå°†é•œé¢åå°„èå…¥ 3D é«˜æ–¯ç‚¹ splatting ä¸­</li><li>ä½œè€…ï¼šYiyi Liao, Yuxuan Zhang, Wenqi Xian, Lingjie Liu, Chen Change Loy, Richard Zhang</li><li>éš¶å±å•ä½ï¼šé¦™æ¸¯ä¸­æ–‡å¤§å­¦</li><li>å…³é”®è¯ï¼šGaussian Splattingã€Mirror Sceneã€Novel View Synthesis</li><li>è®ºæ–‡é“¾æ¥ï¼šæ— ï¼ŒGithub ä»£ç é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼š3D é«˜æ–¯ç‚¹ splatting (3DGS) åœ¨ 3D åœºæ™¯é‡å»ºå’Œæ–°è§†è§’åˆæˆé¢†åŸŸå–å¾—äº†é‡å¤§çªç ´ã€‚ç„¶è€Œï¼Œ3DGS ä¸å…¶å‰èº«ç¥ç»è¾å°„åœº (NeRF) ä¸€æ ·ï¼Œéš¾ä»¥å‡†ç¡®å»ºæ¨¡ç‰©ç†åå°„ï¼Œå°¤å…¶æ˜¯åœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­æ— å¤„ä¸åœ¨çš„é•œå­ä¸­ã€‚è¿™ç§ç–å¿½é”™è¯¯åœ°å°†åå°„è§†ä¸ºç‹¬ç«‹å­˜åœ¨çš„ç‰©ç†å®ä½“ï¼Œå¯¼è‡´é‡å»ºä¸å‡†ç¡®ï¼Œå¹¶ä¸”ä¸åŒè§†è§’ä¸‹çš„åå°„å±æ€§ä¸ä¸€è‡´ã€‚ï¼ˆ2ï¼‰ï¼šè¿‡å»çš„æ–¹æ³•åŠé—®é¢˜ï¼šä¸ºäº†è§£å†³è¿™ä¸€å…³é”®æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº† Mirror-3DGSï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ›æ–°çš„æ¸²æŸ“æ¡†æ¶ï¼Œæ—¨åœ¨æŒæ¡é•œé¢å‡ ä½•å’Œåå°„çš„å¤æ‚æ€§ï¼Œä¸ºç”Ÿæˆé€¼çœŸçš„é•œé¢åå°„é“ºå¹³é“è·¯ã€‚é€šè¿‡å·§å¦™åœ°å°†é•œå­å±æ€§èå…¥ 3DGS å¹¶åˆ©ç”¨å¹³é¢é•œæˆåƒåŸç†ï¼ŒMirror-3DGS åˆ›å»ºäº†ä¸€ä¸ªé•œåƒè§†ç‚¹ï¼Œä»é•œåè§‚å¯Ÿï¼Œä¸°å¯Œäº†åœºæ™¯æ¸²æŸ“çš„çœŸå®æ„Ÿã€‚ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šå¯¹åˆæˆå’ŒçœŸå®ä¸–ç•Œåœºæ™¯çš„å¹¿æ³›è¯„ä¼°å±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•ä»¥å¢å¼ºä¿çœŸåº¦å®æ—¶æ¸²æŸ“æ–°è§†è§’çš„èƒ½åŠ›ï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„é•œå­åŒºåŸŸå†…è¶…è¶Šäº†æœ€å…ˆè¿›çš„ Mirror-NeRFã€‚æˆ‘ä»¬çš„ä»£ç å°†å…¬å¼€æä¾›ï¼Œä»¥è¿›è¡Œå¯é‡å¤çš„ç ”ç©¶ã€‚ï¼ˆ4ï¼‰ï¼šæ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†ä»€ä¹ˆæ€§èƒ½ï¼Ÿæ€§èƒ½æ˜¯å¦æ”¯æŒå…¶ç›®æ ‡ï¼šæˆ‘ä»¬åœ¨åˆæˆå’ŒçœŸå®åœºæ™¯ä¸­å¯¹ Mirror-3DGS è¿›è¡Œäº†å¹¿æ³›çš„è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼ŒMirror-3DGS åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„é•œå­åŒºåŸŸå†…ä»¥æ›´é«˜çš„ä¿çœŸåº¦æ¸²æŸ“æ–°è§†è§’ã€‚è¿™äº›ç»“æœæ”¯æŒäº†æˆ‘ä»¬çš„ç›®æ ‡ï¼Œå³å¼€å‘ä¸€ç§èƒ½å¤Ÿå‡†ç¡®å»ºæ¨¡é•œé¢åå°„å¹¶ç”Ÿæˆé€¼çœŸæ¸²æŸ“çš„æ¸²æŸ“æ¡†æ¶ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) 3D é«˜æ–¯ç‚¹ splattingï¼ˆ3DGSï¼‰æ–¹æ³•ï¼šåˆ©ç”¨é«˜æ–¯ç‚¹ splatting æŠ€æœ¯ç”Ÿæˆå›¾åƒï¼Œå®ç°å®æ—¶æ¸²æŸ“ã€‚(2) Mirror-3DGS æ–¹æ³•ï¼šé€šè¿‡å°†é•œå­å±æ€§èå…¥ 3DGSï¼Œå¹¶åˆ©ç”¨å¹³é¢é•œæˆåƒåŸç†ï¼Œåˆ›å»ºé•œåƒè§†ç‚¹ï¼Œä»é•œåè§‚å¯Ÿï¼Œå¢å¼ºåœºæ™¯æ¸²æŸ“çš„çœŸå®æ„Ÿã€‚(3) é•œåƒè§†ç‚¹æ„å»ºï¼šæ ¹æ®é•œå­å±æ€§å’Œä¸é€æ˜åº¦ï¼Œè¿‡æ»¤å‡ºå±äºé•œå­çš„é«˜æ–¯ç‚¹ï¼Œæ„é€  3D ç©ºé—´ä¸­çš„å¹³é¢ï¼Œå¹¶åŸºäºæ­¤å¹³é¢è·å¾—é•œåƒè§†ç‚¹ã€‚(4) å›¾åƒèåˆï¼šä»åŸå§‹è§†ç‚¹å’Œé•œåƒè§†ç‚¹æ¸²æŸ“å›¾åƒï¼Œå¹¶æ ¹æ®é•œå­æ©ç èåˆä¸¤å¹…å›¾åƒï¼Œå¾—åˆ°æœ€ç»ˆåˆæˆå›¾åƒã€‚(5) ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼šç¬¬ä¸€é˜¶æ®µä¼˜åŒ–é•œå­å±æ€§å’Œç²—ç•¥çš„é«˜æ–¯ç‚¹è¡¨ç¤ºï¼Œç¬¬äºŒé˜¶æ®µåŸºäºä¼°è®¡çš„é•œå­å¹³é¢æ–¹ç¨‹ï¼ŒèåˆåŸå§‹è§†ç‚¹å’Œé•œåƒè§†ç‚¹çš„å›¾åƒï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–åœºæ™¯çš„é«˜æ–¯ç‚¹è¡¨ç¤ºã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œçš„é‡è¦æ„ä¹‰ï¼šMirror-3DGS åˆ›æ–°æ€§åœ°å°†é•œå­å±æ€§èå…¥ 3D é«˜æ–¯ç‚¹ splattingï¼Œå¹¶åˆ©ç”¨å¹³é¢é•œæˆåƒåŸç†ï¼Œæ„å»ºé•œåƒè§†ç‚¹ï¼Œä»é•œåè§‚å¯Ÿï¼Œå¢å¼ºäº†åœºæ™¯æ¸²æŸ“çš„çœŸå®æ„Ÿï¼Œä¸ºå‡†ç¡®å»ºæ¨¡é•œé¢åå°„å¹¶ç”Ÿæˆé€¼çœŸæ¸²æŸ“é“ºå¹³äº†é“è·¯ã€‚ï¼ˆ2ï¼‰ï¼šæ–‡ç« çš„ä¼˜ç¼ºç‚¹æ€»ç»“ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº† Mirror-3DGS æ¸²æŸ“æ¡†æ¶ï¼Œå°†é•œå­å±æ€§èå…¥ 3DGSï¼Œå¹¶åˆ©ç”¨å¹³é¢é•œæˆåƒåŸç†ï¼Œæ„å»ºé•œåƒè§†ç‚¹ï¼Œä»é•œåè§‚å¯Ÿï¼Œå¢å¼ºäº†åœºæ™¯æ¸²æŸ“çš„çœŸå®æ„Ÿã€‚æ€§èƒ½ï¼šåœ¨åˆæˆå’ŒçœŸå®åœºæ™¯ä¸­å¯¹ Mirror-3DGS è¿›è¡Œäº†å¹¿æ³›çš„è¯„ä¼°ï¼Œç»“æœè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼ŒMirror-3DGS åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„é•œå­åŒºåŸŸå†…ä»¥æ›´é«˜çš„ä¿çœŸåº¦æ¸²æŸ“æ–°è§†è§’ã€‚å·¥ä½œé‡ï¼šMirror-3DGS çš„å®ç°éœ€è¦ä¿®æ”¹ 3DGS æ¸²æŸ“æ¡†æ¶ï¼Œå¹¶å¼•å…¥é•œå­å±æ€§å’Œé•œåƒè§†ç‚¹æ„å»ºçš„é€»è¾‘ï¼Œå·¥ä½œé‡ä¸­ç­‰ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-b687b1f16dc36cf89c9d228e4a9c2fcc.jpg" align="middle"><img src="https://picx.zhimg.com/v2-006e8205855032985f3309526106945f.jpg" align="middle"><img src="https://pica.zhimg.com/v2-b68e59518e5eb4ceae94fee53f29fd38.jpg" align="middle"></details><h2 id="CityGaussian-Real-time-High-quality-Large-Scale-Scene-Rendering-with-Gaussians"><a href="#CityGaussian-Real-time-High-quality-Large-Scale-Scene-Rendering-with-Gaussians" class="headerlink" title="CityGaussian: Real-time High-quality Large-Scale Scene Rendering with   Gaussians"></a>CityGaussian: Real-time High-quality Large-Scale Scene Rendering with   Gaussians</h2><p><strong>Authors:Yang Liu, He Guan, Chuanchen Luo, Lue Fan, Junran Peng, Zhaoxiang Zhang</strong></p><p>The advancement of real-time 3D scene reconstruction and novel view synthesis has been significantly propelled by 3D Gaussian Splatting (3DGS). However, effectively training large-scale 3DGS and rendering it in real-time across various scales remains challenging. This paper introduces CityGaussian (CityGS), which employs a novel divide-and-conquer training approach and Level-of-Detail (LoD) strategy for efficient large-scale 3DGS training and rendering. Specifically, the global scene prior and adaptive training data selection enables efficient training and seamless fusion. Based on fused Gaussian primitives, we generate different detail levels through compression, and realize fast rendering across various scales through the proposed block-wise detail levels selection and aggregation strategy. Extensive experimental results on large-scale scenes demonstrate that our approach attains state-of-theart rendering quality, enabling consistent real-time rendering of largescale scenes across vastly different scales. Our project page is available at <a href="https://dekuliutesla.github.io/citygs/">https://dekuliutesla.github.io/citygs/</a>. </p><p><a href="http://arxiv.org/abs/2404.01133v1">PDF</a> Project Page: <a href="https://dekuliutesla.github.io/citygs/">https://dekuliutesla.github.io/citygs/</a></p><p><strong>æ‘˜è¦</strong><br>é€šè¿‡æå‡ºåˆ†å‰²è®­ç»ƒä¸æ¸è¿›ç»†èŠ‚ç­‰çº§ç­–ç•¥ï¼ŒCityGS å®ç°é«˜æ•ˆå¤§è§„æ¨¡ 3DGS è®­ç»ƒå’Œæ¸²æŸ“ï¼Œè¾¾åˆ°å…ˆè¿›æ¸²æŸ“è´¨é‡ï¼Œæ”¯æŒè·¨ä¸åŒå°ºåº¦çš„å¤§åœºæ™¯å®æ—¶æ¸²æŸ“ã€‚</p><p><strong>è¦ç‚¹</strong></p><ul><li>CityGS é‡‡ç”¨åˆ†å‰²è®­ç»ƒä¸æ¸è¿›ç»†èŠ‚ç­‰çº§ç­–ç•¥ï¼Œæå‡å¤§è§„æ¨¡ 3DGS è®­ç»ƒä¸æ¸²æŸ“æ•ˆç‡ã€‚</li><li>å…¨å±€åœºæ™¯å…ˆéªŒä¸è‡ªé€‚åº”è®­ç»ƒæ•°æ®é€‰æ‹©ï¼Œä¿è¯é«˜æ•ˆè®­ç»ƒä¸æ— ç¼èåˆã€‚</li><li>åŸºäºèåˆçš„é«˜æ–¯åŸºæœ¬ä½“ç”Ÿæˆä¸åŒç»†èŠ‚ç­‰çº§ï¼Œé€šè¿‡åˆ†å—ç»†èŠ‚ç­‰çº§é€‰æ‹©ä¸èšåˆç­–ç•¥å®ç°è·¨å°ºåº¦å¿«é€Ÿæ¸²æŸ“ã€‚</li><li>å®éªŒç»“æœè¡¨æ˜ï¼ŒCityGS æ¸²æŸ“è´¨é‡è¾¾å…ˆè¿›æ°´å¹³ï¼Œæ”¯æŒè·¨å°ºåº¦å¤§åœºæ™¯ä¸€è‡´å®æ—¶æ¸²æŸ“ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šCityGaussianï¼šå®æ—¶é«˜è´¨é‡å¤§åœºæ™¯æ¸²æŸ“çš„é«˜æ–¯ä½“</li><li>ä½œè€…ï¼šæ¨æŸ³ï¼Œå…³é¹¤ï¼Œç½—å·æ™¨ï¼ŒèŒƒç•¥ï¼Œå½­ä¿Šç„¶ï¼Œå¼ å…†ç¿”</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šä¸­å›½ç§‘å­¦é™¢è‡ªåŠ¨åŒ–ç ”ç©¶æ‰€</li><li>å…³é”®è¯ï¼šå¤§åœºæ™¯é‡å»ºÂ·æ–°è§†è§’åˆæˆÂ·3Dé«˜æ–¯ä½“</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/pdf/2404.01133.pdfï¼ŒGithubä»£ç é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šå¤§åœºæ™¯é‡å»ºå’Œæ–°è§†è§’åˆæˆåœ¨AR/VRã€èˆªç©ºæµ‹é‡å’Œè‡ªåŠ¨é©¾é©¶ä¸­è‡³å…³é‡è¦ï¼Œä½†å¯¹å¤§åœºæ™¯çš„å®æ—¶é«˜è´¨é‡é‡å»ºå’Œæ¸²æŸ“ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æ–¹æ³•ç¼ºä¹ç»†èŠ‚ä¿çœŸåº¦æˆ–æ€§èƒ½è¾ƒå·®ï¼Œ3Dé«˜æ–¯ä½“ï¼ˆ3DGSï¼‰ä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†å¤§è§„æ¨¡3DGSçš„è®­ç»ƒå’Œå®æ—¶æ¸²æŸ“ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ï¼ˆ3ï¼‰æœ¬æ–‡æ–¹æ³•ï¼šæå‡ºCityGaussianï¼ˆCityGSï¼‰ï¼Œé‡‡ç”¨åˆ†è€Œæ²»ä¹‹çš„è®­ç»ƒæ–¹æ³•å’Œç»†èŠ‚å±‚æ¬¡ï¼ˆLoDï¼‰ç­–ç•¥ï¼Œå®ç°é«˜æ•ˆçš„å¤§è§„æ¨¡3DGSè®­ç»ƒå’Œæ¸²æŸ“ã€‚åˆ©ç”¨å…¨å±€åœºæ™¯å…ˆéªŒå’Œè‡ªé€‚åº”è®­ç»ƒæ•°æ®é€‰æ‹©ï¼Œå®ç°é«˜æ•ˆè®­ç»ƒå’Œæ— ç¼èåˆã€‚åŸºäºèåˆçš„é«˜æ–¯ä½“ï¼Œé€šè¿‡å‹ç¼©ç”Ÿæˆä¸åŒç»†èŠ‚å±‚æ¬¡ï¼Œå¹¶é€šè¿‡æå‡ºçš„å—çº§ç»†èŠ‚å±‚æ¬¡é€‰æ‹©å’Œèšåˆç­–ç•¥ï¼Œå®ç°è·¨ä¸åŒå°ºåº¦çš„å¿«é€Ÿæ¸²æŸ“ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šåœ¨å¤§åœºæ™¯æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æ–¹æ³•è¾¾åˆ°æœ€å…ˆè¿›çš„æ¸²æŸ“è´¨é‡ï¼Œèƒ½å¤Ÿåœ¨å¤§åœºæ™¯ä¸­è·¨è¶Šä¸åŒå°ºåº¦å®ç°ä¸€è‡´çš„å®æ—¶æ¸²æŸ“ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1): ç²—ç•¥çš„å…¨å±€é«˜æ–¯ä½“å…ˆéªŒç”Ÿæˆï¼›(2): é«˜æ–¯ä½“å’Œæ•°æ®åŸºæœ¬ä½“çš„åˆ’åˆ†ç­–ç•¥ï¼›(3): è®­ç»ƒå’Œåå¤„ç†ç»†èŠ‚ï¼›(4): ç»†èŠ‚å±‚æ¬¡ç”Ÿæˆï¼›(5): ç»†èŠ‚å±‚æ¬¡é€‰æ‹©å’Œèåˆã€‚</p></li><li><p><strong>ç»“è®º</strong>(1) <strong>æœ¬æ–‡æ„ä¹‰</strong>ï¼šCityGaussian æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„å¤§è§„æ¨¡ 3DGS è®­ç»ƒå’Œæ¸²æŸ“æ–¹æ³•ï¼Œä¸ºå¤§åœºæ™¯çš„å®æ—¶é«˜è´¨é‡é‡å»ºå’Œæ¸²æŸ“æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚(2) <strong>ä¼˜ç¼ºç‚¹æ€»ç»“</strong>ï¼š</p></li><li><strong>åˆ›æ–°ç‚¹</strong>ï¼š<ul><li>æå‡ºåˆ†è€Œæ²»ä¹‹çš„è®­ç»ƒæ–¹æ³•ï¼Œæœ‰æ•ˆè§£å†³å¤§è§„æ¨¡ 3DGS è®­ç»ƒé—®é¢˜ã€‚</li><li>æå‡ºç»†èŠ‚å±‚æ¬¡ï¼ˆLoDï¼‰ç­–ç•¥ï¼Œå®ç°è·¨ä¸åŒå°ºåº¦çš„å¿«é€Ÿæ¸²æŸ“ã€‚</li></ul></li><li><strong>æ€§èƒ½</strong>ï¼š<ul><li>åœ¨å¤§åœºæ™¯æ•°æ®é›†ä¸Šè¾¾åˆ°æœ€å…ˆè¿›çš„æ¸²æŸ“è´¨é‡ã€‚</li><li>èƒ½å¤Ÿåœ¨å¤§åœºæ™¯ä¸­è·¨è¶Šä¸åŒå°ºåº¦å®ç°ä¸€è‡´çš„å®æ—¶æ¸²æŸ“ã€‚</li></ul></li><li><strong>å·¥ä½œé‡</strong>ï¼š<ul><li>è®­ç»ƒè¿‡ç¨‹ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦åˆ†æ­¥è¿›è¡Œã€‚</li><li>æ¸²æŸ“è¿‡ç¨‹éœ€è¦æ ¹æ®åœºæ™¯ç»†èŠ‚è¿›è¡Œç»†èŠ‚å±‚æ¬¡é€‰æ‹©å’Œèåˆï¼Œå¢åŠ è®¡ç®—é‡ã€‚</li></ul></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-99b04580a863af8ce4f631e8bd0ec9e7.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-547f5a57e280ecd60e9f1e905a13c71d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-8faf5cbf97c2d3cac383a23cf4a18d31.jpg" align="middle"></details><h2 id="HAHA-Highly-Articulated-Gaussian-Human-Avatars-with-Textured-Mesh-Prior"><a href="#HAHA-Highly-Articulated-Gaussian-Human-Avatars-with-Textured-Mesh-Prior" class="headerlink" title="HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior"></a>HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior</h2><p><strong>Authors:David Svitov, Pietro Morerio, Lourdes Agapito, Alessio Del Bue</strong></p><p>We present HAHA - a novel approach for animatable human avatar generation from monocular input videos. The proposed method relies on learning the trade-off between the use of Gaussian splatting and a textured mesh for efficient and high fidelity rendering. We demonstrate its efficiency to animate and render full-body human avatars controlled via the SMPL-X parametric model. Our model learns to apply Gaussian splatting only in areas of the SMPL-X mesh where it is necessary, like hair and out-of-mesh clothing. This results in a minimal number of Gaussians being used to represent the full avatar, and reduced rendering artifacts. This allows us to handle the animation of small body parts such as fingers that are traditionally disregarded. We demonstrate the effectiveness of our approach on two open datasets: SnapshotPeople and X-Humans. Our method demonstrates on par reconstruction quality to the state-of-the-art on SnapshotPeople, while using less than a third of Gaussians. HAHA outperforms previous state-of-the-art on novel poses from X-Humans both quantitatively and qualitatively. </p><p><a href="http://arxiv.org/abs/2404.01053v1">PDF</a> </p><p><strong>Summary</strong><br>å•ç›®è¾“å…¥è§†é¢‘ç”Ÿæˆå¯åŠ¨ç”»äººç±»è§’è‰²çš„HAHAæ–¹æ³•ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>HAHAæ–¹æ³•åœ¨å•ç›®è¾“å…¥è§†é¢‘ä¸­ç”Ÿæˆå¯åŠ¨ç”»çš„äººç±»è§’è‰²ã€‚</li><li>å­¦ä¹ ä½¿ç”¨é«˜æ–¯å–· splatting å’Œçº¹ç†ç½‘æ ¼è¿›è¡Œé«˜æ•ˆé«˜è´¨é‡æ¸²æŸ“ã€‚</li><li>ä½¿ç”¨é«˜æ–¯ splatting ä»…åœ¨ SMPL-X ç½‘æ ¼çš„å¿…è¦åŒºåŸŸï¼Œå¦‚å¤´å‘å’Œç½‘æ ¼å¤–è¡£ç€ã€‚</li><li>å‡å°‘ç”¨äºè¡¨ç¤ºå®Œæ•´è§’è‰²çš„é«˜æ–¯æ•°é‡ï¼Œå‡å°‘æ¸²æŸ“ä¼ªå½±ã€‚</li><li>å¤„ç†æ‰‹æŒ‡ç­‰å°èº«ä½“éƒ¨ä½çš„åŠ¨ç”»ã€‚</li><li>åœ¨ SnapshotPeople æ•°æ®é›†ä¸Šè¾¾åˆ°æœ€å…ˆè¿›çš„é‡å»ºè´¨é‡ï¼ŒåŒæ—¶ä½¿ç”¨ä¸åˆ°ä¸‰åˆ†ä¹‹ä¸€çš„é«˜æ–¯ã€‚</li><li>åœ¨ X-Humans æ–°å§¿åŠ¿ä¸Šå®šé‡å’Œå®šæ€§ä¼˜äºä¹‹å‰çš„æœ€å…ˆè¿›æŠ€æœ¯ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šHAHAï¼šé«˜æ•ˆä¸”é«˜ä¿çœŸå¯åŠ¨ç”»äººä½“åŒ–èº«ç”Ÿæˆ</li><li>ä½œè€…ï¼šDavid Svitov</li><li>å•ä½ï¼šæ— </li><li>å…³é”®è¯ï¼šHuman avatar, Full-body, Gaussians platting, Textures</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/pdf/2302.03280.pdfï¼ŒGithub ä»£ç é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šéšç€è®¡ç®—æœºè§†è§‰æŠ€æœ¯çš„è¿›æ­¥ï¼Œç”Ÿæˆå¯åŠ¨ç”»çš„äººä½“åŒ–èº«å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸ä½¿ç”¨çº¹ç†ç½‘æ ¼æˆ–é«˜æ–¯æ•£å¸ƒæ¥è¡¨ç¤ºäººä½“ï¼Œä½†è¿™äº›æ–¹æ³•åœ¨æ•ˆç‡å’Œä¿çœŸåº¦ä¹‹é—´å­˜åœ¨æƒè¡¡ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•è¦ä¹ˆä½¿ç”¨çº¹ç†ç½‘æ ¼æ¥è·å¾—é«˜ä¿çœŸåº¦ï¼Œä½†æ¸²æŸ“æ•ˆç‡ä½ï¼Œè¦ä¹ˆä½¿ç”¨é«˜æ–¯æ•£å¸ƒæ¥æé«˜æ•ˆç‡ï¼Œä½†ä¿çœŸåº¦è¾ƒä½ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º HAHA çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†é«˜æ–¯æ•£å¸ƒå’Œçº¹ç†ç½‘æ ¼çš„ä¼˜ç‚¹ã€‚HAHA å­¦ä¹ åœ¨äººä½“ SMPL-X ç½‘æ ¼ä¸­éœ€è¦çš„åœ°æ–¹ï¼ˆä¾‹å¦‚å¤´å‘å’Œéç½‘æ ¼æœè£…ï¼‰åº”ç”¨é«˜æ–¯æ•£å¸ƒï¼Œä»è€Œæœ€å¤§é™åº¦åœ°å‡å°‘é«˜æ–¯æ•£å¸ƒçš„ä½¿ç”¨æ•°é‡å¹¶å‡å°‘æ¸²æŸ“ä¼ªå½±ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šåœ¨ SnapshotPeople å’Œ X-Humans ä¸¤ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šï¼ŒHAHA åœ¨é‡å»ºè´¨é‡ä¸Šä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸å½“ï¼ŒåŒæ—¶ä½¿ç”¨çš„é«˜æ–¯æ•£å¸ƒæ•°é‡ä¸åˆ°ä¸‰åˆ†ä¹‹ä¸€ã€‚åœ¨ X-Humans æ•°æ®é›†ä¸Šï¼ŒHAHA åœ¨æ–°å§¿åŠ¿ä¸Šçš„è¡¨ç°ä¼˜äºä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰å­¦ä¹ å…¨èº«é«˜æ–¯è¡¨ç¤ºï¼Œå¹¶å¾®è°ƒ SMPL-X çš„å§¿æ€å’Œå½¢çŠ¶ä»¥è¿›è¡Œè®­ç»ƒå¸§ã€‚ï¼ˆ2ï¼‰ä½¿ç”¨ç»“æœçš„ SMPL-X ç½‘æ ¼å’Œæä¾›çš„ UV æ˜ å°„æ¥å­¦ä¹  RGB çº¹ç†ã€‚ï¼ˆ3ï¼‰åˆå¹¶ä¸¤ä¸ªåŒ–èº«ï¼Œå¹¶å­¦ä¹ åˆ é™¤ä¸€äº›é«˜æ–¯è€Œä¸ä¼šé™ä½è´¨é‡ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬å·¥ä½œé€šè¿‡æå‡ºä¸€ç§æ–°çš„æ–¹æ³•HAHAï¼Œåœ¨é«˜æ•ˆä¸”é«˜ä¿çœŸå¯åŠ¨ç”»äººä½“åŒ–èº«ç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼šHAHAå°†é«˜æ–¯æ•£å¸ƒå’Œçº¹ç†ç½‘æ ¼ç›¸ç»“åˆï¼Œå­¦ä¹ åœ¨éœ€è¦çš„åœ°æ–¹åº”ç”¨é«˜æ–¯æ•£å¸ƒï¼Œæœ€å¤§é™åº¦åœ°å‡å°‘é«˜æ–¯æ•£å¸ƒçš„ä½¿ç”¨æ•°é‡ï¼ŒåŒæ—¶ä¿æŒé«˜ä¿çœŸåº¦ã€‚æ€§èƒ½ï¼šåœ¨å…¬å¼€æ•°æ®é›†ä¸Šï¼ŒHAHAåœ¨é‡å»ºè´¨é‡ä¸Šä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸å½“ï¼ŒåŒæ—¶ä½¿ç”¨çš„é«˜æ–¯æ•£å¸ƒæ•°é‡ä¸åˆ°ä¸‰åˆ†ä¹‹ä¸€ã€‚å·¥ä½œé‡ï¼šHAHAçš„æ–¹æ³•æ¶‰åŠå­¦ä¹ å…¨èº«é«˜æ–¯è¡¨ç¤ºã€å¾®è°ƒSMPL-Xå§¿æ€å’Œå½¢çŠ¶ã€å­¦ä¹ RGBçº¹ç†ä»¥åŠåˆå¹¶ä¸¤ä¸ªåŒ–èº«ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-b477b212869e9f8635da6f1012719fb9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-aa6e5f76fe253938286549f8ee5205fd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4f97970c093585e18e2db42fb96a6b75.jpg" align="middle"></details><h2 id="MM3DGS-SLAM-Multi-modal-3D-Gaussian-Splatting-for-SLAM-Using-Vision-Depth-and-Inertial-Measurements"><a href="#MM3DGS-SLAM-Multi-modal-3D-Gaussian-Splatting-for-SLAM-Using-Vision-Depth-and-Inertial-Measurements" class="headerlink" title="MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vision,   Depth, and Inertial Measurements"></a>MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vision,   Depth, and Inertial Measurements</h2><p><strong>Authors:Lisong C. Sun, Neel P. Bhatt, Jonathan C. Liu, Zhiwen Fan, Zhangyang Wang, Todd E. Humphreys, Ufuk Topcu</strong></p><p>Simultaneous localization and mapping is essential for position tracking and scene understanding. 3D Gaussian-based map representations enable photorealistic reconstruction and real-time rendering of scenes using multiple posed cameras. We show for the first time that using 3D Gaussians for map representation with unposed camera images and inertial measurements can enable accurate SLAM. Our method, MM3DGS, addresses the limitations of prior neural radiance field-based representations by enabling faster rendering, scale awareness, and improved trajectory tracking. Our framework enables keyframe-based mapping and tracking utilizing loss functions that incorporate relative pose transformations from pre-integrated inertial measurements, depth estimates, and measures of photometric rendering quality. We also release a multi-modal dataset, UT-MM, collected from a mobile robot equipped with a camera and an inertial measurement unit. Experimental evaluation on several scenes from the dataset shows that MM3DGS achieves 3x improvement in tracking and 5% improvement in photometric rendering quality compared to the current 3DGS SLAM state-of-the-art, while allowing real-time rendering of a high-resolution dense 3D map. Project Webpage: <a href="https://vita-group.github.io/MM3DGS-SLAM">https://vita-group.github.io/MM3DGS-SLAM</a> </p><p><a href="http://arxiv.org/abs/2404.00923v1">PDF</a> Project Webpage: <a href="https://vita-group.github.io/MM3DGS-SLAM">https://vita-group.github.io/MM3DGS-SLAM</a></p><p><strong>Summary</strong><br>å®æ—¶3Då»ºå›¾ä¸å®šä½ç³»ç»Ÿ3D Gaussiansé¦–æ¬¡ä¸ç›¸æœºå›¾åƒå’Œæƒ¯æ€§æµ‹é‡ç›¸ç»“åˆï¼Œå¯å®ç°é«˜ç²¾åº¦çš„SLAMã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>åˆ©ç”¨3D Gaussiansè¿›è¡Œåœ°å›¾è¡¨ç¤ºï¼Œå¯å®ç°æ›´å¿«çš„æ¸²æŸ“ã€å°ºåº¦æ„ŸçŸ¥å’Œæ›´ä½³çš„è½¨è¿¹è·Ÿè¸ªã€‚</li><li>æå‡ºäº†ä¸€ç§å°†é¢„ç§¯åˆ†æƒ¯æ€§æµ‹é‡ã€æ·±åº¦ä¼°è®¡å’Œå…‰åº¦æ¸²æŸ“è´¨é‡åº¦é‡çº³å…¥æŸå¤±å‡½æ•°çš„æ¡†æ¶ã€‚</li><li>å‘å¸ƒäº†ä¸€ä¸ªç”±é…å¤‡ç›¸æœºå’Œæƒ¯æ€§æµ‹é‡å•å…ƒçš„ç§»åŠ¨æœºå™¨äººæ”¶é›†çš„å¤šæ¨¡æ€æ•°æ®é›†ã€‚</li><li>å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒMM3DGSåœ¨è·Ÿè¸ªæ–¹é¢å®ç°äº†3å€çš„æå‡ï¼Œåœ¨å…‰åº¦æ¸²æŸ“è´¨é‡æ–¹é¢å®ç°äº†5%çš„æå‡ã€‚</li><li>MM3DGSå…è®¸å®æ—¶æ¸²æŸ“é«˜åˆ†è¾¨ç‡ç¨ å¯†3Dåœ°å›¾ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šMM3DGSSLAMï¼šä½¿ç”¨è§†è§‰ã€æ·±åº¦å’Œæƒ¯æ€§æµ‹é‡è¿›è¡Œ SLAM çš„å¤šæ¨¡æ€ 3D é«˜æ–¯æ–‘ç‚¹</li><li>ä½œè€…ï¼šLisong C. Sun, Neel P. Bhatt, Jonathan C. Liu, Zhiwen Fan, Zhangyang Wang, Todd E. Humphreys, Ufuk Topcu</li><li>éš¶å±ï¼šå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡</li><li>å…³é”®è¯ï¼šSLAMã€3D é‡å»ºã€ç¥ç»è¾å°„åœºã€é«˜æ–¯æ–‘ç‚¹</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://vita-group.github.io/MM3DGS-SLAM   Github ä»£ç é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šSLAM åœ¨è‡ªä¸»ç³»ç»Ÿä¸­è‡³å…³é‡è¦ï¼Œ3D åœºæ™¯é‡å»ºå’Œä¼ æ„Ÿå™¨å®šä½æ˜¯å…¶æ ¸å¿ƒèƒ½åŠ›ã€‚ç¥ç»è¾å°„åœºæ˜¯ç”¨äº 3D é‡å»ºçš„æ–°å…´æŠ€æœ¯ï¼Œä½†å…¶åœ¨ SLAM ä¸­çš„åº”ç”¨å—åˆ°æ¸²æŸ“é€Ÿåº¦ã€å°ºåº¦æ„ŸçŸ¥å’Œè½¨è¿¹è·Ÿè¸ªå‡†ç¡®æ€§æ–¹é¢çš„é™åˆ¶ã€‚   ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•ï¼šç¥ç»è¾å°„åœºæ–¹æ³•åœ¨ SLAM ä¸­å­˜åœ¨ä¸Šè¿°é™åˆ¶ã€‚   ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šMM3DGS æå‡ºäº†ä¸€ç§åŸºäº 3D é«˜æ–¯æ–‘ç‚¹çš„ SLAM æ–¹æ³•ï¼Œåˆ©ç”¨é¢„ç§¯åˆ†æƒ¯æ€§æµ‹é‡ã€æ·±åº¦ä¼°è®¡å’Œå…‰åº¦æ¸²æŸ“è´¨é‡åº¦é‡æ¥ä¼˜åŒ–è·Ÿè¸ªå’Œå»ºå›¾ã€‚   ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šåœ¨ UT-MM æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼ŒMM3DGS åœ¨è·Ÿè¸ªæ–¹é¢æ¯”æœ€å…ˆè¿›çš„ 3DGSSLAM æ–¹æ³•æé«˜äº† 3 å€ï¼Œåœ¨å…‰åº¦æ¸²æŸ“è´¨é‡æ–¹é¢æé«˜äº† 5%ï¼ŒåŒæ—¶å…è®¸å®æ—¶æ¸²æŸ“é«˜åˆ†è¾¨ç‡å¯†é›† 3D åœ°å›¾ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†å…¶åœ¨ SLAM ä¸­å®ç°å‡†ç¡®å®šä½å’Œé€¼çœŸé‡å»ºçš„ç›®æ ‡ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) MM3DGSé‡‡ç”¨é¢„ç§¯åˆ†æƒ¯æ€§æµ‹é‡ï¼ˆPre-integrated Inertial Measurementsï¼ŒPIMï¼‰æ¥ä¼°è®¡ç›¸æœºä½å§¿å’Œé€Ÿåº¦ï¼Œå‡å°‘å™ªå£°å½±å“ï¼›(2) ä½¿ç”¨æ·±åº¦ä¼°è®¡æ¨¡å—ä»RGBå›¾åƒä¸­æå–æ·±åº¦ä¿¡æ¯ï¼Œç”¨äºç¥ç»è¾å°„åœºæ¸²æŸ“å’Œåœºæ™¯é‡å»ºï¼›(3) å¼•å…¥å…‰åº¦æ¸²æŸ“è´¨é‡åº¦é‡ï¼ˆPhotometric Rendering Qualityï¼ŒPRQï¼‰ï¼Œé€šè¿‡ä¼˜åŒ–æ¸²æŸ“è´¨é‡æ¥æé«˜è·Ÿè¸ªå’Œå»ºå›¾çš„å‡†ç¡®æ€§ï¼›(4) å°†3Dé«˜æ–¯æ–‘ç‚¹ï¼ˆ3D Gaussian Splatï¼Œ3DGSï¼‰åº”ç”¨äºç¥ç»è¾å°„åœºï¼Œæé«˜æ¸²æŸ“é€Ÿåº¦å’Œå°ºåº¦æ„ŸçŸ¥èƒ½åŠ›ï¼›(5) æå‡ºä¸€ç§åŸºäº3DGSçš„è½¨è¿¹è·Ÿè¸ªç®—æ³•ï¼Œé€šè¿‡ä¼˜åŒ–PRQå’ŒPIMæ¥å®ç°å‡†ç¡®å®šä½ï¼›(6) é‡‡ç”¨åˆ†å—æ¸²æŸ“æŠ€æœ¯ï¼Œå…è®¸å®æ—¶æ¸²æŸ“é«˜åˆ†è¾¨ç‡å¯†é›†3Dåœ°å›¾ã€‚</p></li></ol><p>8.ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€3Dé«˜æ–¯æ–‘ç‚¹SLAMæ–¹æ³•MM3DGSï¼Œè¯¥æ–¹æ³•åˆ©ç”¨é¢„ç§¯åˆ†æƒ¯æ€§æµ‹é‡ã€æ·±åº¦ä¼°è®¡å’Œå…‰åº¦æ¸²æŸ“è´¨é‡åº¦é‡æ¥ä¼˜åŒ–è·Ÿè¸ªå’Œå»ºå›¾ï¼Œåœ¨è·Ÿè¸ªæ–¹é¢æ¯”æœ€å…ˆè¿›çš„3DGSSLAMæ–¹æ³•æé«˜äº†3å€ï¼Œåœ¨å…‰åº¦æ¸²æŸ“è´¨é‡æ–¹é¢æé«˜äº†5%ï¼ŒåŒæ—¶å…è®¸å®æ—¶æ¸²æŸ“é«˜åˆ†è¾¨ç‡å¯†é›†3Dåœ°å›¾ï¼Œä¸ºSLAMä¸­å®ç°å‡†ç¡®å®šä½å’Œé€¼çœŸé‡å»ºæä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š- æå‡ºäº†ä¸€ç§åŸºäº3Dé«˜æ–¯æ–‘ç‚¹çš„SLAMæ–¹æ³•ï¼Œæé«˜äº†æ¸²æŸ“é€Ÿåº¦å’Œå°ºåº¦æ„ŸçŸ¥èƒ½åŠ›ã€‚- å¼•å…¥å…‰åº¦æ¸²æŸ“è´¨é‡åº¦é‡ï¼Œé€šè¿‡ä¼˜åŒ–æ¸²æŸ“è´¨é‡æ¥æé«˜è·Ÿè¸ªå’Œå»ºå›¾çš„å‡†ç¡®æ€§ã€‚- é‡‡ç”¨åˆ†å—æ¸²æŸ“æŠ€æœ¯ï¼Œå…è®¸å®æ—¶æ¸²æŸ“é«˜åˆ†è¾¨ç‡å¯†é›†3Dåœ°å›¾ã€‚æ€§èƒ½ï¼š- åœ¨UT-MMæ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œåœ¨è·Ÿè¸ªæ–¹é¢æ¯”æœ€å…ˆè¿›çš„3DGSSLAMæ–¹æ³•æé«˜äº†3å€ï¼Œåœ¨å…‰åº¦æ¸²æŸ“è´¨é‡æ–¹é¢æé«˜äº†5%ã€‚- å…è®¸å®æ—¶æ¸²æŸ“é«˜åˆ†è¾¨ç‡å¯†é›†3Dåœ°å›¾ã€‚å·¥ä½œé‡ï¼š- è¯¥æ–¹æ³•éœ€è¦é¢„ç§¯åˆ†æƒ¯æ€§æµ‹é‡ã€æ·±åº¦ä¼°è®¡å’Œå…‰åº¦æ¸²æŸ“è´¨é‡åº¦é‡ç­‰æ¨¡å—ï¼Œå·¥ä½œé‡è¾ƒå¤§ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-e555dce577db7ee48c6d0700898f8354.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-45584721725016a544b3438b3b4e3524.jpg" align="middle"><img src="https://picx.zhimg.com/v2-efe4c6fcb1b7199ca40bd96e51c223a5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a82f468f62090c014e63f7697e14bafd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5bd070bb451dced5c2cc58f655ff4729.jpg" align="middle"></details>## 3DGSR: Implicit Surface Reconstruction with 3D Gaussian Splatting**Authors:Xiaoyang Lyu, Yang-Tian Sun, Yi-Hua Huang, Xiuzhe Wu, Ziyi Yang, Yilun Chen, Jiangmiao Pang, Xiaojuan Qi**In this paper, we present an implicit surface reconstruction method with 3D Gaussian Splatting (3DGS), namely 3DGSR, that allows for accurate 3D reconstruction with intricate details while inheriting the high efficiency and rendering quality of 3DGS. The key insight is incorporating an implicit signed distance field (SDF) within 3D Gaussians to enable them to be aligned and jointly optimized. First, we introduce a differentiable SDF-to-opacity transformation function that converts SDF values into corresponding Gaussians' opacities. This function connects the SDF and 3D Gaussians, allowing for unified optimization and enforcing surface constraints on the 3D Gaussians. During learning, optimizing the 3D Gaussians provides supervisory signals for SDF learning, enabling the reconstruction of intricate details. However, this only provides sparse supervisory signals to the SDF at locations occupied by Gaussians, which is insufficient for learning a continuous SDF. Then, to address this limitation, we incorporate volumetric rendering and align the rendered geometric attributes (depth, normal) with those derived from 3D Gaussians. This consistency regularization introduces supervisory signals to locations not covered by discrete 3D Gaussians, effectively eliminating redundant surfaces outside the Gaussian sampling range. Our extensive experimental results demonstrate that our 3DGSR method enables high-quality 3D surface reconstruction while preserving the efficiency and rendering quality of 3DGS. Besides, our method competes favorably with leading surface reconstruction techniques while offering a more efficient learning process and much better rendering qualities. The code will be available at https://github.com/CVMI-Lab/3DGSR. [PDF](http://arxiv.org/abs/2404.00409v1) **Summary**3DGSR æ˜¯ä¸€ç§éšå¼æ›²é¢é‡å»ºæ–¹æ³•ï¼Œå®ƒç»“åˆäº† 3DGS çš„é«˜ç²¾åº¦å’Œæ¸²æŸ“è´¨é‡ï¼Œå¹¶åˆ©ç”¨ 3D é«˜æ–¯æ¨¡ç³Šæ¥å¢å¼ºéšå¼ç¬¦å·è·ç¦»åœº (SDF)ï¼Œä»è€Œå®ç°å¯¹å¤æ‚ç»†èŠ‚çš„é«˜ç²¾åº¦ 3D é‡å»ºã€‚**Key Takeaways**- 3DGSR å°†éšå¼ç¬¦å·è·ç¦»åœº (SDF) èå…¥ 3D é«˜æ–¯æ¨¡ç³Šï¼Œä½¿å…¶å¯¹é½å¹¶å…±åŒä¼˜åŒ–ã€‚- å¯å¾®åˆ† SDF åˆ°ä¸é€æ˜åº¦å˜æ¢å‡½æ•°å°† SDF å€¼è½¬æ¢ä¸ºç›¸åº”çš„é«˜æ–¯ä¸é€æ˜åº¦ï¼Œè¿æ¥äº† SDF å’Œ 3D é«˜æ–¯æ¨¡ç³Šï¼Œå®ç°äº†ç»Ÿä¸€ä¼˜åŒ–å’Œå¯¹ 3D é«˜æ–¯æ¨¡ç³Šçš„æ›²é¢çº¦æŸã€‚- ä¼˜åŒ– 3D é«˜æ–¯æ¨¡ç³Šä¸º SDF å­¦ä¹ æä¾›äº†ç›‘ç£ä¿¡å·ï¼Œä»è€Œèƒ½å¤Ÿé‡å»ºå¤æ‚ç»†èŠ‚ã€‚- ä½“ç§¯æ¸²æŸ“å’Œå¯¹é½æ¥è‡ª 3D é«˜æ–¯æ¨¡ç³Šçš„å‡ ä½•å±æ€§ï¼ˆæ·±åº¦ã€æ³•çº¿ï¼‰å¯å¼•å…¥ç›‘ç£ä¿¡å·ï¼Œæœ‰æ•ˆæ¶ˆé™¤é«˜æ–¯é‡‡æ ·èŒƒå›´ä¹‹å¤–çš„å¤šä½™æ›²é¢ã€‚- å®éªŒç»“æœè¡¨æ˜ï¼Œ3DGSR åœ¨ä¿æŒ 3DGS çš„æ•ˆç‡å’Œæ¸²æŸ“è´¨é‡çš„åŒæ—¶ï¼Œå®ç°äº†é«˜è´¨é‡çš„ 3D æ›²é¢é‡å»ºã€‚- ä¸é¢†å…ˆçš„æ›²é¢é‡å»ºæŠ€æœ¯ç›¸æ¯”ï¼Œ3DGSR å…·æœ‰ç«äº‰ä¼˜åŠ¿ï¼ŒåŒæ—¶æä¾›äº†æ›´æœ‰æ•ˆçš„å­¦ä¹ è¿‡ç¨‹å’Œæ›´å¥½çš„æ¸²æŸ“è´¨é‡ã€‚- 3DGSR çš„ä»£ç å¯ä» https://github.com/CVMI-Lab/3DGSR è·å–ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>é¢˜ç›®ï¼š3DGSRï¼šåŸºäº 3D é«˜æ–¯æº…å°„çš„éšå¼æ›²é¢é‡å»º</li><li>ä½œè€…ï¼šXiaoyang Lyu, Yang-Tian Sun, Yi-Hua Huang, Xiuzhe Wu, Ziyi Yang, Yilun Chen, Jiangmiao Pang, Xiaojuan Qi</li><li>éš¶å±ï¼šé¦™æ¸¯å¤§å­¦</li><li>å…³é”®è¯ï¼šGaussian Splattingã€éšå¼å‡½æ•°ã€ç¬¦å·è·ç¦»å‡½æ•°ã€ä½“ç§¯æ¸²æŸ“</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://doi.org/10.1145/nnnnnnn.nnnnnnn   Github ä»£ç é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼š3D é«˜æ–¯æº…å°„ï¼ˆ3DGSï¼‰æ˜¯ä¸€ç§ç”¨äºé«˜è´¨é‡æ–°è§†è§’åˆæˆçš„æ–°å‹æŠ€æœ¯ï¼Œä½†å®ƒåªèƒ½ç”Ÿæˆå˜ˆæ‚ä¸”ä¸å®Œæ•´çš„ 3D å‡ ä½•ç‚¹ï¼Œæ— æ³•å‡†ç¡®é‡å»ºåœºæ™¯çš„ 3D æ›²é¢ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼š3DGS æ— æ³•å¿ å®åœ°è¡¨ç¤º 3D æ›²é¢ï¼Œå› ä¸ºå®ƒé‡‡ç”¨éç»“æ„åŒ–çš„åŸºäºç‚¹çš„å‡ ä½•è¡¨ç¤ºã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§éšå¼æ›²é¢é‡å»ºæ–¹æ³•ï¼Œç§°ä¸º 3DGS çš„ 3D é«˜æ–¯æº…å°„ï¼ˆ3DGSRï¼‰ï¼Œå®ƒå…è®¸å‡†ç¡®é‡å»ºå…·æœ‰å¤æ‚ç»†èŠ‚çš„ 3Dï¼ŒåŒæ—¶ç»§æ‰¿äº† 3DGS çš„é«˜æ•ˆç‡å’Œæ¸²æŸ“è´¨é‡ã€‚å…³é”®æ€æƒ³æ˜¯å°†éšå¼ç¬¦å·è·ç¦»åœºï¼ˆSDFï¼‰åˆå¹¶åˆ° 3D é«˜æ–¯ä¸­ï¼Œä½¿å®ƒä»¬èƒ½å¤Ÿå¯¹é½å¹¶å…±åŒä¼˜åŒ–ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šå®éªŒç»“æœè¡¨æ˜ï¼Œ3DGSR æ–¹æ³•èƒ½å¤Ÿå®ç°é«˜è´¨é‡çš„ 3D æ›²é¢é‡å»ºï¼ŒåŒæ—¶ä¿æŒ 3DGS çš„æ•ˆç‡å’Œæ¸²æŸ“è´¨é‡ã€‚è¯¥æ–¹æ³•åœ¨ä¸é¢†å…ˆçš„æ›²é¢é‡å»ºæŠ€æœ¯ç«äº‰æ—¶è¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶æä¾›äº†æ›´é«˜æ•ˆçš„å­¦ä¹ è¿‡ç¨‹å’Œæ›´å¥½çš„æ¸²æŸ“è´¨é‡ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) å°†éšå¼ç¬¦å·è·ç¦»åœºï¼ˆSDFï¼‰ä¸ 3D é«˜æ–¯æº…å°„ï¼ˆ3DGSï¼‰ç›¸ç»“åˆï¼Œä½¿å®ƒä»¬èƒ½å¤Ÿå¯¹é½å¹¶å…±åŒä¼˜åŒ–ã€‚(2) ä½¿ç”¨ SDF æ¥æŒ‡å¯¼ 3DGS çš„ä¼˜åŒ–è¿‡ç¨‹ï¼Œä»è€Œç”Ÿæˆæ›´å‡†ç¡®å’Œå®Œæ•´çš„ 3D æ›²é¢ã€‚(3) é‡‡ç”¨åˆ†å±‚ä¼˜åŒ–ç­–ç•¥ï¼Œä»ç²—ç³™çš„æ›²é¢é€æ­¥ç»†åŒ–åˆ°ç²¾ç»†çš„æ›²é¢ï¼Œä»¥æé«˜é‡å»ºæ•ˆç‡ã€‚(4) å¼•å…¥æ­£åˆ™åŒ–é¡¹ï¼Œä»¥ä¿ƒè¿›é‡å»ºæ›²é¢çš„å…‰æ»‘æ€§å’Œè¿è´¯æ€§ã€‚(5) ä½¿ç”¨åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–ç®—æ³•ï¼Œä»¥å®ç°é«˜æ•ˆå’Œç¨³å®šçš„æ›²é¢é‡å»ºã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œæå‡ºäº†ä¸€ç§é«˜æ•ˆçš„éšå¼æ›²é¢é‡å»ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åŸºäº 3D é«˜æ–¯æº…å°„ï¼Œèƒ½å¤Ÿé‡å»ºå…·æœ‰å¤æ‚ç»†èŠ‚çš„é«˜è´¨é‡ 3D æ›²é¢ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>å°†ç¥ç»éšå¼ç¬¦å·è·ç¦»åœºï¼ˆSDFï¼‰ä¸ 3D é«˜æ–¯æº…å°„ï¼ˆ3DGSï¼‰ç›¸ç»“åˆï¼Œé€šè¿‡å¯å¾®åˆ† SDF åˆ°ä¸é€æ˜åº¦è½¬æ¢å‡½æ•°å®ç° SDF å’Œ 3D é«˜æ–¯çš„å¯¹é½å’Œè”åˆä¼˜åŒ–ã€‚</li><li>åˆ©ç”¨ä½“ç§¯æ¸²æŸ“å’Œ SDF ä¸é«˜æ–¯å‡ ä½•ä¸€è‡´æ€§æ­£åˆ™åŒ–è¿›è¡Œ SDF ä¼˜åŒ–ã€‚æ€§èƒ½ï¼š</li><li>åœ¨ä¸å½±å“ 3D é«˜æ–¯æ¸²æŸ“èƒ½åŠ›å’Œæ•ˆç‡çš„æƒ…å†µä¸‹ï¼Œ3DGSR åœ¨é‡å»ºé«˜è´¨é‡æ›²é¢æ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„é‡å»ºç®¡é“ã€‚å·¥ä½œé‡ï¼š</li><li>ç”±äºæ¸²æŸ“è´¨é‡å’Œæ›²é¢å¹³æ»‘åº¦ä¹‹é—´çš„æƒè¡¡ï¼Œæœ¬ç ”ç©¶ç¡®å®å­˜åœ¨ä¸€å®šçš„å±€é™æ€§ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-7c3724a12f3e6cb1586e3e58348c4989.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e49e36a5fd966732c34aa3a3b964dee7.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-da0937779f213436f7d6b004f3c45985.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-04-06  Per-Gaussian Embedding-Based Deformation for Deformable 3D Gaussian   Splatting</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/Talking%20Head%20Generation/</id>
    <published>2024-04-06T09:47:10.000Z</published>
    <updated>2024-04-06T09:47:10.952Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-04-06-æ›´æ–°"><a href="#2024-04-06-æ›´æ–°" class="headerlink" title="2024-04-06 æ›´æ–°"></a>2024-04-06 æ›´æ–°</h1><h2 id="EDTalk-Efficient-Disentanglement-for-Emotional-Talking-Head-Synthesis"><a href="#EDTalk-Efficient-Disentanglement-for-Emotional-Talking-Head-Synthesis" class="headerlink" title="EDTalk: Efficient Disentanglement for Emotional Talking Head Synthesis"></a>EDTalk: Efficient Disentanglement for Emotional Talking Head Synthesis</h2><p><strong>Authors:Shuai Tan, Bin Ji, Mengxiao Bi, Ye Pan</strong></p><p>Achieving disentangled control over multiple facial motions and accommodating diverse input modalities greatly enhances the application and entertainment of the talking head generation. This necessitates a deep exploration of the decoupling space for facial features, ensuring that they a) operate independently without mutual interference and b) can be preserved to share with different modal input, both aspects often neglected in existing methods. To address this gap, this paper proposes a novel Efficient Disentanglement framework for Talking head generation (EDTalk). Our framework enables individual manipulation of mouth shape, head pose, and emotional expression, conditioned on video or audio inputs. Specifically, we employ three lightweight modules to decompose the facial dynamics into three distinct latent spaces representing mouth, pose, and expression, respectively. Each space is characterized by a set of learnable bases whose linear combinations define specific motions. To ensure independence and accelerate training, we enforce orthogonality among bases and devise an efficient training strategy to allocate motion responsibilities to each space without relying on external knowledge. The learned bases are then stored in corresponding banks, enabling shared visual priors with audio input. Furthermore, considering the properties of each space, we propose an Audio-to-Motion module for audio-driven talking head synthesis. Experiments are conducted to demonstrate the effectiveness of EDTalk. We recommend watching the project website: <a href="https://tanshuai0219.github.io/EDTalk/">https://tanshuai0219.github.io/EDTalk/</a> </p><p><a href="http://arxiv.org/abs/2404.01647v1">PDF</a> 22 pages, 15 figures</p><p><strong>Summary</strong><br>åˆ©ç”¨è§†é¢‘æˆ–éŸ³é¢‘è¾“å…¥ï¼Œç‹¬ç«‹æ“æ§å˜´å·´å½¢çŠ¶ï¼Œå¤´éƒ¨å§¿æ€å’Œæƒ…ç»ªè¡¨æƒ…ï¼Œå®ç°é«˜æ•ˆå¯æ§çš„é¢éƒ¨ç”Ÿæˆã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡º Efficient Disentanglement æ¡†æ¶ï¼Œå®ç°è§£è€¦é¢éƒ¨åŠ¨ä½œã€‚</li><li>åˆ©ç”¨ä¸‰æ¨¡å—åˆ†è§£é¢éƒ¨åŠ¨æ€ï¼Œç‹¬ç«‹æ“æ§å˜´å·´å½¢çŠ¶ï¼Œå¤´éƒ¨å§¿æ€å’Œæƒ…ç»ªè¡¨æƒ…ã€‚</li><li>é‡‡ç”¨å¯å­¦ä¹ åŸºåº•ï¼Œé€šè¿‡çº¿æ€§ç»„åˆå®šä¹‰ç‰¹å®šåŠ¨ä½œã€‚</li><li>å¼ºåˆ¶åŸºåº•æ­£äº¤ï¼ŒåŠ é€Ÿè®­ç»ƒï¼Œç¡®ä¿åŠ¨ä½œç‹¬ç«‹ã€‚</li><li>æå‡º Audio-to-Motion æ¨¡å—ï¼Œå®ç°éŸ³é¢‘é©±åŠ¨é¢éƒ¨ç”Ÿæˆã€‚</li><li>å®éªŒéªŒè¯ EDTalk çš„æœ‰æ•ˆæ€§ã€‚</li><li>æä¾›é¡¹ç›®ç½‘ç«™ï¼š<a href="https://tanshuai0219.github.io/EDTalk/">https://tanshuai0219.github.io/EDTalk/</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><strong>è®ºæ–‡æ ‡é¢˜ï¼š</strong> EDTalkï¼šé«˜æ•ˆè§£è€¦è¯´è¯äººå¤´éƒ¨ç”Ÿæˆæ¡†æ¶</li><li><strong>ä½œè€…ï¼š</strong> Tan Shuai, Qiangqiang Yuan, Lu Sheng, Fan Yang, Zhixin Piao, Changjie Fan</li><li><strong>ç¬¬ä¸€ä½œè€…å•ä½ï¼š</strong> æ¸…åå¤§å­¦</li><li><strong>å…³é”®è¯ï¼š</strong> è¯´è¯äººå¤´éƒ¨ç”Ÿæˆã€è§£è€¦ã€é¢éƒ¨åŠ¨ç”»ã€éŸ³é¢‘é©±åŠ¨</li><li><strong>è®ºæ–‡é“¾æ¥ï¼š</strong> https://arxiv.org/abs/2207.03559</li><li><strong>æ‘˜è¦ï¼š</strong>    (1) <strong>ç ”ç©¶èƒŒæ™¯ï¼š</strong> è¯´è¯äººå¤´éƒ¨ç”Ÿæˆéœ€è¦å¯¹å¤šä¸ªé¢éƒ¨åŠ¨ä½œè¿›è¡Œè§£è€¦æ§åˆ¶ï¼Œå¹¶é€‚åº”ä¸åŒçš„è¾“å…¥æ–¹å¼ï¼Œè¿™éœ€è¦æ·±å…¥æ¢ç´¢é¢éƒ¨ç‰¹å¾çš„è§£è€¦ç©ºé—´ï¼Œç¡®ä¿å®ƒä»¬æ—¢èƒ½ç‹¬ç«‹æ“ä½œåˆå¯ä»¥ä¿ç•™ä¸ä¸åŒæ¨¡æ€è¾“å…¥å…±äº«çš„èƒ½åŠ›ã€‚    (2) <strong>è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼š</strong> ç°æœ‰æ–¹æ³•å¾€å¾€å¿½è§†äº†è¿™äº›æ–¹é¢ï¼Œå¯¼è‡´è§£è€¦ç©ºé—´ä¸ç‹¬ç«‹ã€è®­ç»ƒé€Ÿåº¦æ…¢æˆ–æ— æ³•å¤„ç†éŸ³é¢‘è¾“å…¥ã€‚    (3) <strong>ç ”ç©¶æ–¹æ³•ï¼š</strong> æå‡º EDTalk æ¡†æ¶ï¼Œé‡‡ç”¨ä¸‰ä¸ªè½»é‡çº§æ¨¡å—å°†é¢éƒ¨åŠ¨æ€åˆ†è§£ä¸ºä¸‰ä¸ªä¸åŒçš„æ½œåœ¨ç©ºé—´ï¼Œåˆ†åˆ«è¡¨ç¤ºå˜´å‹ã€å¤´éƒ¨å§¿æ€å’Œè¡¨æƒ…ã€‚æ¯ä¸ªç©ºé—´éƒ½ç”±ä¸€ç»„å¯å­¦ä¹ åŸºç»„æˆï¼Œå…¶çº¿æ€§ç»„åˆå®šä¹‰äº†ç‰¹å®šçš„åŠ¨ä½œã€‚é€šè¿‡æ­£äº¤åŒ–åŸºå¹¶è®¾è®¡é«˜æ•ˆçš„è®­ç»ƒç­–ç•¥ï¼Œç¡®ä¿äº†ç‹¬ç«‹æ€§å’ŒåŠ é€Ÿäº†è®­ç»ƒã€‚    (4) <strong>ä»»åŠ¡å’Œæ€§èƒ½ï¼š</strong> åœ¨è¯´è¯äººå¤´éƒ¨ç”Ÿæˆä»»åŠ¡ä¸Šï¼ŒEDTalk å®ç°äº†å‡ºè‰²çš„æ€§èƒ½ï¼Œåœ¨è§†é¢‘å’ŒéŸ³é¢‘è¾“å…¥æ¡ä»¶ä¸‹å‡èƒ½å®ç°å˜´å‹ã€å¤´éƒ¨å§¿æ€å’Œè¡¨æƒ…çš„ç‹¬ç«‹æ§åˆ¶ã€‚å®éªŒç»“æœéªŒè¯äº† EDTalk çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜äº†å…¶åœ¨è¯´è¯äººå¤´éƒ¨ç”Ÿæˆä¸­çš„åº”ç”¨æ½œåŠ›ã€‚</li></ol><p><strong>æ–¹æ³•</strong></p><p>ï¼ˆ1ï¼‰<strong>é«˜æ•ˆè§£è€¦ç­–ç•¥ï¼š</strong>æå‡ºè§£è€¦ç­–ç•¥ï¼ŒåŒ…æ‹¬å˜´å‹-å¤´éƒ¨å§¿æ€è§£è€¦å’Œè¡¨æƒ…è§£è€¦ï¼Œå°†æ•´ä½“é¢éƒ¨åŠ¨æ€åˆ†è§£ä¸ºå˜´å‹ã€å¤´éƒ¨å§¿æ€å’Œè¡¨æƒ…ç©ºé—´ã€‚</p><p>ï¼ˆ2ï¼‰<strong>å˜´å‹-å¤´éƒ¨å§¿æ€è§£è€¦ï¼š</strong>é‡‡ç”¨äº¤å‰é‡å»ºæŠ€æœ¯ï¼Œåˆæˆå˜´å‹äº¤æ¢åçš„å›¾åƒï¼Œå¹¶é€šè¿‡é‡æ„æŸå¤±ã€æ„ŸçŸ¥æŸå¤±å’Œå¯¹æŠ—æŸå¤±ç›‘ç£å˜´å‹-å¤´éƒ¨å§¿æ€è§£è€¦æ¨¡å—ã€‚</p><p>ï¼ˆ3ï¼‰<strong>è¡¨æƒ…è§£è€¦ï¼š</strong>å¼•å…¥è¡¨æƒ…æ„ŸçŸ¥æ½œåœ¨å¯¼èˆªæ¨¡å—å’Œæƒ…æ„Ÿå¢å¼ºæ¨¡å—ï¼Œé€šè¿‡è‡ªé‡å»ºè¡¥å……å­¦ä¹ è®­ç»ƒè¡¨æƒ…è§£è€¦æ¨¡å—ã€‚</p><p>ï¼ˆ4ï¼‰<strong>éŸ³é¢‘åˆ°åŠ¨ä½œï¼š</strong>è®¾è®¡ä¸‰ä¸ªæ¨¡å—ä»éŸ³é¢‘é¢„æµ‹å¤´éƒ¨å§¿æ€ã€å˜´å‹å’Œè¡¨æƒ…çš„æƒé‡ï¼Œé€šè¿‡ç‰¹å¾æŸå¤±ã€é‡æ„æŸå¤±å’ŒåŒæ­¥æŸå¤±è®­ç»ƒéŸ³é¢‘ç¼–ç å™¨å’Œæƒé‡é¢„æµ‹å±‚ã€‚</p><ol><li>ç»“è®ºï¼š(1): æœ¬æ–‡æå‡º EDTalkï¼Œä¸€ç§æ–°é¢–çš„ç³»ç»Ÿï¼Œæ—¨åœ¨å°†é¢éƒ¨ç»„ä»¶é«˜æ•ˆè§£è€¦åˆ°æ½œåœ¨ç©ºé—´ä¸­ï¼Œä»è€Œå®ç°è¯´è¯äººå¤´éƒ¨åˆæˆçš„ç²¾ç»†æ§åˆ¶ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯ä½¿ç”¨å­˜å‚¨åœ¨ä¸“ç”¨åº“ä¸­çš„æ­£äº¤åŸºæ¥è¡¨ç¤ºæ¯ä¸ªç©ºé—´ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„è®­ç»ƒç­–ç•¥ï¼Œè¯¥ç­–ç•¥å¯ä»¥è‡ªåŠ¨å°†ç©ºé—´ä¿¡æ¯åˆ†é…ç»™æ¯ä¸ªç©ºé—´ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹å¤–éƒ¨æˆ–å…ˆéªŒç»“æ„çš„éœ€è¦ã€‚é€šè¿‡é›†æˆè¿™äº›ç©ºé—´ï¼Œæˆ‘ä»¬é€šè¿‡è½»é‡çº§çš„ Audio-to-Motion æ¨¡å—å®ç°äº†éŸ³é¢‘é©±åŠ¨çš„è¯´è¯äººå¤´éƒ¨ç”Ÿæˆã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å®ç°å¯¹å„ç§é¢éƒ¨åŠ¨ä½œçš„è§£è€¦å’Œç²¾ç»†æ§åˆ¶æ–¹é¢å…·æœ‰ä¼˜è¶Šæ€§ã€‚æˆ‘ä»¬åœ¨é™„å½•ä¸­æä¾›äº†æœ‰å…³å±€é™æ€§å’Œä¼¦ç†è€ƒè™‘çš„æ›´å¤šè®¨è®ºã€‚(2): åˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§é«˜æ•ˆçš„è§£è€¦ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åŒ…æ‹¬å˜´å‹-å¤´éƒ¨å§¿æ€è§£è€¦å’Œè¡¨æƒ…è§£è€¦ï¼›æå‡ºäº†ä¸€ç§åŸºäºäº¤å‰é‡å»ºæŠ€æœ¯çš„å˜´å‹-å¤´éƒ¨å§¿æ€è§£è€¦æ¨¡å—ï¼›æå‡ºäº†ä¸€ç§å¼•å…¥è¡¨æƒ…æ„ŸçŸ¥æ½œåœ¨å¯¼èˆªæ¨¡å—å’Œæƒ…æ„Ÿå¢å¼ºæ¨¡å—çš„è¡¨æƒ…è§£è€¦æ¨¡å—ï¼›è®¾è®¡äº†ä¸€ä¸ªä»éŸ³é¢‘é¢„æµ‹å¤´éƒ¨å§¿æ€ã€å˜´å‹å’Œè¡¨æƒ…æƒé‡çš„ Audio-to-Motion æ¨¡å—ã€‚æ€§èƒ½ï¼šåœ¨è¯´è¯äººå¤´éƒ¨ç”Ÿæˆä»»åŠ¡ä¸Šï¼ŒEDTalk å®ç°äº†å‡ºè‰²çš„æ€§èƒ½ï¼Œåœ¨è§†é¢‘å’ŒéŸ³é¢‘è¾“å…¥æ¡ä»¶ä¸‹å‡èƒ½å®ç°å˜´å‹ã€å¤´éƒ¨å§¿æ€å’Œè¡¨æƒ…çš„ç‹¬ç«‹æ§åˆ¶ã€‚å®éªŒç»“æœéªŒè¯äº† EDTalk çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜äº†å…¶åœ¨è¯´è¯äººå¤´éƒ¨ç”Ÿæˆä¸­çš„åº”ç”¨æ½œåŠ›ã€‚å·¥ä½œé‡ï¼šæœ¬æ–‡çš„å·¥ä½œé‡è¾ƒå¤§ï¼Œæ¶‰åŠåˆ°è§£è€¦ç­–ç•¥ã€å˜´å‹-å¤´éƒ¨å§¿æ€è§£è€¦æ¨¡å—ã€è¡¨æƒ…è§£è€¦æ¨¡å—å’Œ Audio-to-Motion æ¨¡å—çš„è®¾è®¡å’Œå®ç°ã€‚å®éªŒéƒ¨åˆ†ä¹Ÿæ¯”è¾ƒå¤æ‚ï¼ŒåŒ…æ‹¬å®šé‡å’Œå®šæ€§è¯„ä¼°ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-f58e08e1946a51a1bac98807f8c1876a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0135d232756d768679d9f63847585de1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a0242ee4c355be537d186f7f79fc6e49.jpg" align="middle"></details><h2 id="FaceChain-ImagineID-Freely-Crafting-High-Fidelity-Diverse-Talking-Faces-from-Disentangled-Audio"><a href="#FaceChain-ImagineID-Freely-Crafting-High-Fidelity-Diverse-Talking-Faces-from-Disentangled-Audio" class="headerlink" title="FaceChain-ImagineID: Freely Crafting High-Fidelity Diverse Talking Faces   from Disentangled Audio"></a>FaceChain-ImagineID: Freely Crafting High-Fidelity Diverse Talking Faces   from Disentangled Audio</h2><p><strong>Authors:Chao Xu, Yang Liu, Jiazheng Xing, Weida Wang, Mingze Sun, Jun Dan, Tianxin Huang, Siyuan Li, Zhi-Qi Cheng, Ying Tai, Baigui Sun</strong></p><p>In this paper, we abstract the process of people hearing speech, extracting meaningful cues, and creating various dynamically audio-consistent talking faces, termed Listening and Imagining, into the task of high-fidelity diverse talking faces generation from a single audio. Specifically, it involves two critical challenges: one is to effectively decouple identity, content, and emotion from entangled audio, and the other is to maintain intra-video diversity and inter-video consistency. To tackle the issues, we first dig out the intricate relationships among facial factors and simplify the decoupling process, tailoring a Progressive Audio Disentanglement for accurate facial geometry and semantics learning, where each stage incorporates a customized training module responsible for a specific factor. Secondly, to achieve visually diverse and audio-synchronized animation solely from input audio within a single model, we introduce the Controllable Coherent Frame generation, which involves the flexible integration of three trainable adapters with frozen Latent Diffusion Models (LDMs) to focus on maintaining facial geometry and semantics, as well as texture and temporal coherence between frames. In this way, we inherit high-quality diverse generation from LDMs while significantly improving their controllability at a low training cost. Extensive experiments demonstrate the flexibility and effectiveness of our method in handling this paradigm. The codes will be released at <a href="https://github.com/modelscope/facechain">https://github.com/modelscope/facechain</a>. </p><p><a href="http://arxiv.org/abs/2403.01901v2">PDF</a> </p><p><strong>Summary</strong><br>åˆ©ç”¨å•ä¸€éŸ³é¢‘ç”Ÿæˆå¤šæ ·åŒ–çš„é«˜ä¿çœŸåŠ¨æ€äººè„¸ï¼Œå®ƒè§£å†³äº†ä¸¤å¤§éš¾é¢˜ï¼šæœ‰æ•ˆåˆ†ç¦»éŸ³é¢‘ä¸­çº ç¼ çš„èº«ä»½ã€å†…å®¹å’Œæƒ…æ„Ÿï¼Œä»¥åŠä¿æŒè§†é¢‘å†…éƒ¨å¤šæ ·æ€§å’Œè§†é¢‘é—´ä¸€è‡´æ€§ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºâ€œå€¾å¬å’Œæƒ³è±¡â€ä»»åŠ¡ï¼Œå°†äººç±»å¬åˆ°è¯­éŸ³ã€æå–æœ‰æ„ä¹‰ç‰¹å¾å¹¶åˆ›é€ åŠ¨æ€ä¸€è‡´çš„äººè„¸è¡¨æƒ…è¿‡ç¨‹æŠ½è±¡åŒ–ã€‚</li><li>åˆ›æ–°æ€§åœ°å°†è¿›æ­¥å¼éŸ³é¢‘åˆ†ç¦»åº”ç”¨äºäººè„¸å‡ ä½•å’Œè¯­ä¹‰å­¦ä¹ ï¼Œä»¥å‡†ç¡®åˆ†ç¦»èº«ä»½ã€å†…å®¹å’Œæƒ…æ„Ÿã€‚</li><li>å¼•å…¥å¯æ§è¿è´¯å¸§ç”Ÿæˆï¼Œä½¿ç”¨ä¸‰ä¸ªå¯è®­ç»ƒé€‚é…å™¨å’Œå†»ç»“çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œä¸“æ³¨äºä¿æŒäººè„¸å‡ ä½•ã€è¯­ä¹‰ã€çº¹ç†å’Œå¸§é—´æ—¶é—´è¿è´¯æ€§ã€‚</li><li>ç»§æ‰¿æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„é«˜è´¨é‡ç”Ÿæˆèƒ½åŠ›ï¼ŒåŒæ—¶é€šè¿‡ä½è®­ç»ƒæˆæœ¬æ˜¾è‘—æé«˜å¯æ§æ€§ã€‚</li><li>å®éªŒç»“æœè¯æ˜äº†è¯¥æ–¹æ³•åœ¨å¤„ç†æ­¤èŒƒä¾‹æ–¹é¢çš„çµæ´»æ€§å’Œæœ‰æ•ˆæ€§ã€‚</li><li>ä»£ç å°†åœ¨ <a href="https://github.com/modelscope/facechain">https://github.com/modelscope/facechain</a> ä¸Šå‘å¸ƒã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šFaceChain-ImagineIDï¼šè‡ªç”±ç”Ÿæˆé«˜ä¿çœŸå¤šæ ·åŒ–è¯´è¯äººè„¸</li><li>ä½œè€…ï¼šChao Xu, Yang Liu, Jiazheng Xing, Weida Wang, Mingze Sun, Jun Dan, Tianxin Huang, Siyuan Li, Zhi-Qi Cheng, Ying Tai, Baigui Sun</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šé˜¿é‡Œå·´å·´é›†å›¢</li><li>å…³é”®è¯ï¼šäººè„¸ç”Ÿæˆã€éŸ³é¢‘è§£è€¦ã€å¯æ§ç”Ÿæˆã€ä¸€è‡´æ€§</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.01901</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šéšç€äººè„¸ç”ŸæˆæŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼Œäººä»¬å¯¹éšç§ä¿æŠ¤å’Œè™šæ‹Ÿå½¢è±¡ä¸ªæ€§åŒ–çš„éœ€æ±‚æ—¥ç›Šå¢é•¿ã€‚ä¼ ç»Ÿæ–¹æ³•è¦ä¹ˆä½¿ç”¨çœŸå®äººè„¸å›¾åƒï¼Œå­˜åœ¨éšç§æ³„éœ²é£é™©ï¼Œè¦ä¹ˆç”Ÿæˆçš„è™šæ‹Ÿå½¢è±¡ä¸çœŸå®éŸ³é¢‘ä¸ä¸€è‡´ã€‚</li></ol><p>ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šè¿‡å»æ–¹æ³•ä¸»è¦é€šè¿‡éŸ³é¢‘ç‰¹å¾æå–å’Œå›¾åƒç”Ÿæˆç›¸ç»“åˆçš„æ–¹å¼è¿›è¡Œäººè„¸ç”Ÿæˆï¼Œä½†å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š- æ— æ³•æœ‰æ•ˆè§£è€¦éŸ³é¢‘ä¸­çš„èº«ä»½ã€å†…å®¹å’Œæƒ…ç»ªä¿¡æ¯ã€‚- éš¾ä»¥åœ¨å•ä¸€æ¨¡å‹ä¸­å®ç°è§†è§‰å¤šæ ·æ€§å’ŒéŸ³é¢‘åŒæ­¥åŠ¨ç”»ã€‚</p><p>ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†â€œè†å¬ä¸æƒ³è±¡â€èŒƒå¼ï¼Œå°†äººè„¸ç”Ÿæˆè¿‡ç¨‹æŠ½è±¡ä¸ºä»éŸ³é¢‘ä¸­æå–æœ‰æ„ä¹‰ä¿¡æ¯å¹¶ç”ŸæˆåŠ¨æ€éŸ³é¢‘ä¸€è‡´è¯´è¯äººè„¸çš„ä»»åŠ¡ã€‚å…·ä½“æ¥è¯´ï¼Œæ–¹æ³•åŒ…å«ä»¥ä¸‹ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼š- éŸ³é¢‘è§£è€¦ï¼šæœ‰æ•ˆåœ°ä»çº ç¼ çš„éŸ³é¢‘ä¸­è§£è€¦èº«ä»½ã€å†…å®¹å’Œæƒ…ç»ªä¿¡æ¯ã€‚- ä¸€è‡´æ€§æ§åˆ¶ï¼šåœ¨å•ä¸€æ¨¡å‹ä¸­ä¿æŒè§†é¢‘å†…å¤šæ ·æ€§å’Œè§†é¢‘é—´ä¸€è‡´æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†æ¸è¿›å¼éŸ³é¢‘è§£è€¦å’Œå¯æ§ä¸€è‡´å¸§ç”Ÿæˆæ–¹æ³•ï¼š- æ¸è¿›å¼éŸ³é¢‘è§£è€¦ï¼šé€šè¿‡å®šåˆ¶çš„è®­ç»ƒæ¨¡å—ï¼Œé€çº§å­¦ä¹ èº«ä»½ã€è¯­ä¹‰å’Œæƒ…ç»ªä¿¡æ¯ã€‚- å¯æ§ä¸€è‡´å¸§ç”Ÿæˆï¼šé€šè¿‡å¯è®­ç»ƒé€‚é…å™¨ä¸å†»ç»“çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹é›†æˆï¼Œä¿æŒé¢éƒ¨å‡ ä½•å’Œè¯­ä¹‰ã€çº¹ç†å’Œå¸§é—´æ—¶é—´ä¸€è‡´æ€§ã€‚</p><p>ï¼ˆ4ï¼‰ä»»åŠ¡å’Œæ€§èƒ½ï¼šæœ¬æ–‡æ–¹æ³•åœ¨ä»¥ä¸‹ä»»åŠ¡ä¸Šå–å¾—äº†è¾ƒå¥½çš„æ€§èƒ½ï¼š- é«˜ä¿çœŸå¤šæ ·åŒ–è¯´è¯äººè„¸ç”Ÿæˆï¼šä»å•ä¸€éŸ³é¢‘ç”Ÿæˆè§†è§‰å¤šæ ·ä¸”ä¸éŸ³é¢‘åŒæ­¥çš„äººè„¸è§†é¢‘ã€‚- å¯æ§å±æ€§ç¼–è¾‘ï¼šæ ¹æ®ä¸ªäººå–œå¥½ï¼Œè‡ªç”±æ”¹å˜ä¸éŸ³é¢‘æ— å…³çš„å±æ€§ï¼Œå¦‚èƒ¡é¡»ã€å‘å‹å’Œç³å­”é¢œè‰²ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤„ç†â€œè†å¬ä¸æƒ³è±¡â€èŒƒå¼æ—¶å…·æœ‰è¾ƒå¥½çš„çµæ´»æ€§å’Œæœ‰æ•ˆæ€§ã€‚</p><ol><li><p><strong>æ–¹æ³•</strong>ï¼š(1) <strong>æ¸è¿›å¼éŸ³é¢‘è§£è€¦</strong>ï¼šä½¿ç”¨å®šåˆ¶çš„è®­ç»ƒæ¨¡å—ï¼Œé€çº§å­¦ä¹ éŸ³é¢‘ä¸­çš„èº«ä»½ã€è¯­ä¹‰å’Œæƒ…ç»ªä¿¡æ¯ã€‚(2) <strong>å¯æ§ä¸€è‡´å¸§ç”Ÿæˆ</strong>ï¼šé€šè¿‡å¯è®­ç»ƒé€‚é…å™¨ä¸å†»ç»“çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹é›†æˆï¼Œä¿æŒé¢éƒ¨å‡ ä½•å’Œè¯­ä¹‰ã€çº¹ç†å’Œå¸§é—´æ—¶é—´ä¸€è‡´æ€§ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºâ€œè†å¬ä¸æƒ³è±¡â€èŒƒå¼çš„è¯´è¯äººè„¸ç”Ÿæˆæ–¹æ³•ï¼Œæœ‰æ•ˆè§£å†³äº†éŸ³é¢‘è§£è€¦å’Œä¸€è‡´æ€§æ§åˆ¶é—®é¢˜ï¼Œå®ç°äº†é«˜ä¿çœŸã€å¤šæ ·åŒ–ã€å¯æ§çš„äººè„¸è§†é¢‘ç”Ÿæˆã€‚è¯¥æ–¹æ³•ä¸ºéšç§ä¿æŠ¤ã€è™šæ‹Ÿå½¢è±¡ä¸ªæ€§åŒ–ç­‰é¢†åŸŸæä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºâ€œè†å¬ä¸æƒ³è±¡â€èŒƒå¼ï¼Œå°†äººè„¸ç”ŸæˆæŠ½è±¡ä¸ºä»éŸ³é¢‘ä¸­æå–ä¿¡æ¯å¹¶ç”ŸæˆåŠ¨æ€ä¸€è‡´äººè„¸çš„ä»»åŠ¡ã€‚</li><li>è®¾è®¡æ¸è¿›å¼éŸ³é¢‘è§£è€¦æ¨¡å—ï¼Œé€çº§å­¦ä¹ éŸ³é¢‘ä¸­çš„èº«ä»½ã€è¯­ä¹‰å’Œæƒ…ç»ªä¿¡æ¯ã€‚</li><li>æå‡ºå¯æ§ä¸€è‡´å¸§ç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡å¯è®­ç»ƒé€‚é…å™¨ä¸å†»ç»“çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹é›†æˆï¼Œä¿æŒè§†é¢‘å†…å¤šæ ·æ€§å’Œè§†é¢‘é—´ä¸€è‡´æ€§ã€‚æ€§èƒ½ï¼š</li><li>åœ¨é«˜ä¿çœŸå¤šæ ·åŒ–è¯´è¯äººè„¸ç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†è¾ƒå¥½çš„æ€§èƒ½ï¼Œç”Ÿæˆçš„è§†é¢‘å…·æœ‰è§†è§‰å¤šæ ·æ€§ï¼Œä¸éŸ³é¢‘åŒæ­¥ã€‚</li><li>æ”¯æŒå¯æ§å±æ€§ç¼–è¾‘ï¼Œå…è®¸ç”¨æˆ·æ ¹æ®ä¸ªäººå–œå¥½è‡ªç”±æ”¹å˜ä¸éŸ³é¢‘æ— å…³çš„å±æ€§ã€‚å·¥ä½œé‡ï¼š</li><li>æœ¬æ–‡æ–¹æ³•éœ€è¦å¤§é‡çš„æ•°æ®å’Œè®¡ç®—èµ„æºè¿›è¡Œè®­ç»ƒã€‚</li><li>æ¸è¿›å¼éŸ³é¢‘è§£è€¦å’Œå¯æ§ä¸€è‡´å¸§ç”Ÿæˆæ–¹æ³•çš„å®ç°è¾ƒä¸ºå¤æ‚ï¼Œéœ€è¦è¾ƒé«˜çš„æŠ€æœ¯é—¨æ§›ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-b33d9cac682c6196c74f1162e4cf280b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6dcceb1760c569cfcb5b2d192473ce57.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d62eae616b2287a6a6a9f3c1a88e65f0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-874c691bc5899d612eddf3c70b6942fa.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-04-06  EDTalk Efficient Disentanglement for Emotional Talking Head Synthesis</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/Diffusion%20Models/</id>
    <published>2024-04-06T09:40:29.000Z</published>
    <updated>2024-04-06T09:40:29.112Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-04-06-æ›´æ–°"><a href="#2024-04-06-æ›´æ–°" class="headerlink" title="2024-04-06 æ›´æ–°"></a>2024-04-06 æ›´æ–°</h1><h2 id="MVD-Fusion-Single-view-3D-via-Depth-consistent-Multi-view-Generation"><a href="#MVD-Fusion-Single-view-3D-via-Depth-consistent-Multi-view-Generation" class="headerlink" title="MVD-Fusion: Single-view 3D via Depth-consistent Multi-view Generation"></a>MVD-Fusion: Single-view 3D via Depth-consistent Multi-view Generation</h2><p><strong>Authors:Hanzhe Hu, Zhizhuo Zhou, Varun Jampani, Shubham Tulsiani</strong></p><p>We present MVD-Fusion: a method for single-view 3D inference via generative modeling of multi-view-consistent RGB-D images. While recent methods pursuing 3D inference advocate learning novel-view generative models, these generations are not 3D-consistent and require a distillation process to generate a 3D output. We instead cast the task of 3D inference as directly generating mutually-consistent multiple views and build on the insight that additionally inferring depth can provide a mechanism for enforcing this consistency. Specifically, we train a denoising diffusion model to generate multi-view RGB-D images given a single RGB input image and leverage the (intermediate noisy) depth estimates to obtain reprojection-based conditioning to maintain multi-view consistency. We train our model using large-scale synthetic dataset Obajverse as well as the real-world CO3D dataset comprising of generic camera viewpoints. We demonstrate that our approach can yield more accurate synthesis compared to recent state-of-the-art, including distillation-based 3D inference and prior multi-view generation methods. We also evaluate the geometry induced by our multi-view depth prediction and find that it yields a more accurate representation than other direct 3D inference approaches. </p><p><a href="http://arxiv.org/abs/2404.03656v1">PDF</a> Project page: <a href="https://mvd-fusion.github.io/">https://mvd-fusion.github.io/</a></p><p><strong>Summary</strong><br>å•è§†å›¾RGBå›¾åƒç›´æ¥ç”Ÿæˆå¤šè§†å›¾ä¸€è‡´RGB-Då›¾åƒï¼Œæ— éœ€è’¸é¦è¿‡ç¨‹ï¼Œæ·±åº¦ä¼°è®¡ç”¨äºå¢å¼ºå¤šè§†å›¾ä¸€è‡´æ€§ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºå•è§†å›¾3Dæ¨ç†æ–¹æ³•MVD-Fusionï¼Œç›´æ¥ç”Ÿæˆå¤šè§†å›¾ä¸€è‡´RGB-Då›¾åƒã€‚</li><li>åˆ©ç”¨æ·±åº¦ä¼°è®¡å»ºç«‹å¤šè§†å›¾ä¸€è‡´æ€§ï¼Œæ— éœ€è’¸é¦è¿‡ç¨‹ã€‚</li><li>é‡‡ç”¨æ‰©æ•£æ¨¡å‹è®­ç»ƒæ¨¡å‹ï¼Œç”Ÿæˆå¤šè§†å›¾RGB-Då›¾åƒã€‚</li><li>åœ¨åˆæˆæ•°æ®é›†Obajverseå’ŒçœŸå®æ•°æ®é›†CO3Dä¸Šè®­ç»ƒæ¨¡å‹ã€‚</li><li>åˆæˆå›¾åƒæ¯”ç°æœ‰æŠ€æœ¯æ›´å‡†ç¡®ï¼ŒåŒ…æ‹¬åŸºäºè’¸é¦çš„3Dæ¨ç†å’Œå¤šè§†å›¾ç”Ÿæˆæ–¹æ³•ã€‚</li><li>å¤šè§†å›¾æ·±åº¦é¢„æµ‹æ¯”å…¶ä»–ç›´æ¥3Dæ¨ç†æ–¹æ³•æ›´å‡†ç¡®ã€‚</li><li>æ¨¡å‹å¯ä»¥å¤„ç†é€šç”¨ç›¸æœºè§†è§’ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šMVD-Fusionï¼šé€šè¿‡æ·±åº¦ä¸€è‡´çš„å¤šè§†å›¾ç”Ÿæˆå®ç°å•è§†å›¾ 3D</li><li>ä½œè€…ï¼šHanzhe Huï¼ŒZhizhuo Zhouï¼ŒVarun Jampaniï¼ŒShubham Tulsiani</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šå¡å†…åŸºæ¢…éš†å¤§å­¦</li><li>å…³é”®è¯ï¼šå•è§†å›¾ 3Dï¼Œå¤šè§†å›¾ç”Ÿæˆï¼Œæ·±åº¦ä¸€è‡´æ€§ï¼Œå»å™ªæ‰©æ•£æ¨¡å‹</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.03656   Github ä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼š   è¿‘å¹´æ¥ï¼Œ3D æ¨ç†æ–¹æ³•å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰çš„æ–¹æ³•åœ¨ç”Ÿæˆ 3D è¡¨ç¤ºæ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚   ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼š   è¿‡å»çš„æ–¹æ³•é€šå¸¸é€šè¿‡å­¦ä¹ æ–°çš„è§†å›¾ç”Ÿæˆæ¨¡å‹æ¥è¿›è¡Œ 3D æ¨ç†ï¼Œä½†è¿™äº›ç”Ÿæˆæ¨¡å‹å¹¶ä¸ 3D ä¸€è‡´ï¼Œéœ€è¦é¢å¤–çš„è’¸é¦è¿‡ç¨‹æ¥ç”Ÿæˆ 3D è¾“å‡ºã€‚   ï¼ˆ3ï¼‰è®ºæ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼š   MVD-Fusion å°† 3D æ¨ç†ä»»åŠ¡è½¬åŒ–ä¸ºç›´æ¥ç”Ÿæˆç›¸äº’ä¸€è‡´çš„å¤šè§†å›¾ï¼Œå¹¶åˆ©ç”¨æ·±åº¦ä¼°è®¡ä½œä¸ºä¸€ç§æœºåˆ¶æ¥å¢å¼ºè¿™ç§ä¸€è‡´æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ–¹æ³•è®­ç»ƒäº†ä¸€ä¸ªå»å™ªæ‰©æ•£æ¨¡å‹ï¼Œåœ¨ç»™å®šå•è§†å›¾ RGB è¾“å…¥å›¾åƒçš„æƒ…å†µä¸‹ç”Ÿæˆå¤šè§†å›¾ RGB-D å›¾åƒï¼Œå¹¶åˆ©ç”¨ï¼ˆä¸­é—´çš„å™ªå£°ï¼‰æ·±åº¦ä¼°è®¡è·å¾—åŸºäºé‡æŠ•å½±çš„æ¡ä»¶ï¼Œä»¥ä¿æŒå¤šè§†å›¾ä¸€è‡´æ€§ã€‚   ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½åŠæ„ä¹‰ï¼š   åœ¨ Objsverse åˆæˆæ•°æ®é›†å’ŒåŒ…å«é€šç”¨ç›¸æœºè§†ç‚¹çš„çœŸå®ä¸–ç•Œ CO3D æ•°æ®é›†ä¸Šè®­ç»ƒåï¼ŒMVD-Fusion åœ¨å¤šè§†å›¾åˆæˆæ–¹é¢ä¼˜äºç°æœ‰çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºäºè’¸é¦çš„ 3D æ¨ç†å’Œå…ˆå‰çš„å¤šè§†å›¾ç”Ÿæˆæ–¹æ³•ã€‚æ­¤å¤–ï¼ŒMVD-Fusion äº§ç”Ÿçš„å¤šè§†å›¾æ·±åº¦é¢„æµ‹æ‰€éšå«çš„å‡ ä½•å½¢çŠ¶æ¯”å…¶ä»–ç›´æ¥ 3D æ¨ç†æ–¹æ³•æ›´å‡†ç¡®ã€‚</li></ol><p>7.Methodsï¼š(1):MVD-Fusionå°†å•è§†å›¾3Dæ¨ç†ä»»åŠ¡è½¬åŒ–ä¸ºç›´æ¥ç”Ÿæˆç›¸äº’ä¸€è‡´çš„å¤šè§†å›¾ï¼Œåˆ©ç”¨æ·±åº¦ä¼°è®¡ä½œä¸ºå¢å¼ºä¸€è‡´æ€§çš„æœºåˆ¶ï¼›(2):è®­ç»ƒä¸€ä¸ªå»å™ªæ‰©æ•£æ¨¡å‹ï¼Œåœ¨ç»™å®šå•è§†å›¾RGBè¾“å…¥å›¾åƒçš„æƒ…å†µä¸‹ç”Ÿæˆå¤šè§†å›¾RGB-Då›¾åƒï¼›(3):åˆ©ç”¨ï¼ˆä¸­é—´çš„å™ªå£°ï¼‰æ·±åº¦ä¼°è®¡è·å¾—åŸºäºé‡æŠ•å½±çš„æ¡ä»¶ï¼Œä»¥ä¿æŒå¤šè§†å›¾ä¸€è‡´æ€§ã€‚</p><ol><li>ç»“è®ºï¼š(1): æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å•è§†å›¾3Dæ¨ç†æ–¹æ³•MVD-Fusionï¼Œè¯¥æ–¹æ³•é€šè¿‡ç›´æ¥ç”Ÿæˆç›¸äº’ä¸€è‡´çš„å¤šè§†å›¾æ¥è§£å†³3Dæ¨ç†ä¸­çš„æŒ‘æˆ˜ï¼Œå¹¶åˆ©ç”¨æ·±åº¦ä¼°è®¡ä½œä¸ºå¢å¼ºä¸€è‡´æ€§çš„æœºåˆ¶ã€‚è¯¥æ–¹æ³•åœ¨å¤šè§†å›¾åˆæˆå’Œæ·±åº¦é¢„æµ‹æ–¹é¢å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œä¸ºå•è§†å›¾3Dæ¨ç†æä¾›äº†æ–°çš„æ€è·¯ã€‚(2): åˆ›æ–°ç‚¹ï¼š</li><li>å°†å•è§†å›¾3Dæ¨ç†è½¬åŒ–ä¸ºç›´æ¥ç”Ÿæˆå¤šè§†å›¾ï¼Œé¿å…äº†é¢å¤–çš„è’¸é¦è¿‡ç¨‹ï¼›</li><li>åˆ©ç”¨æ·±åº¦ä¼°è®¡ä½œä¸ºä¸€ç§æœºåˆ¶æ¥å¢å¼ºå¤šè§†å›¾ä¸€è‡´æ€§ï¼›</li><li>è®­ç»ƒäº†ä¸€ä¸ªå»å™ªæ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆå¤šè§†å›¾RGB-Då›¾åƒã€‚æ€§èƒ½ï¼š</li><li>åœ¨Objsverseåˆæˆæ•°æ®é›†å’ŒCO3DçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šï¼ŒMVD-Fusionåœ¨å¤šè§†å›¾åˆæˆæ–¹é¢ä¼˜äºç°æœ‰çš„æ–¹æ³•ï¼›</li><li>MVD-Fusionäº§ç”Ÿçš„å¤šè§†å›¾æ·±åº¦é¢„æµ‹æ‰€éšå«çš„å‡ ä½•å½¢çŠ¶æ¯”å…¶ä»–ç›´æ¥3Dæ¨ç†æ–¹æ³•æ›´å‡†ç¡®ã€‚å·¥ä½œé‡ï¼š</li><li>è®­ç»ƒMVD-Fusionéœ€è¦è¾ƒå¤§çš„æ•°æ®é›†å’Œè¾ƒé•¿çš„è®­ç»ƒæ—¶é—´ï¼›</li><li>ç”Ÿæˆå¤šè§†å›¾å›¾åƒçš„è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-b0f92085ff917d820e1c6165bf934957.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2d9503adc9232dd5203f47418c5dc2a9.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ec8eee84c3ceeecca1994d5d2e0729a8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a85b9b89865d0ebf649a75ab683b6b4f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-db0f03c22fe43a4a5fc68a32691fc635.jpg" align="middle"></details><h2 id="CoMat-Aligning-Text-to-Image-Diffusion-Model-with-Image-to-Text-Concept-Matching"><a href="#CoMat-Aligning-Text-to-Image-Diffusion-Model-with-Image-to-Text-Concept-Matching" class="headerlink" title="CoMat: Aligning Text-to-Image Diffusion Model with Image-to-Text Concept   Matching"></a>CoMat: Aligning Text-to-Image Diffusion Model with Image-to-Text Concept   Matching</h2><p><strong>Authors:Dongzhi Jiang, Guanglu Song, Xiaoshi Wu, Renrui Zhang, Dazhong Shen, Zhuofan Zong, Yu Liu, Hongsheng Li</strong></p><p>Diffusion models have demonstrated great success in the field of text-to-image generation. However, alleviating the misalignment between the text prompts and images is still challenging. The root reason behind the misalignment has not been extensively investigated. We observe that the misalignment is caused by inadequate token attention activation. We further attribute this phenomenon to the diffusion modelâ€™s insufficient condition utilization, which is caused by its training paradigm. To address the issue, we propose CoMat, an end-to-end diffusion model fine-tuning strategy with an image-to-text concept matching mechanism. We leverage an image captioning model to measure image-to-text alignment and guide the diffusion model to revisit ignored tokens. A novel attribute concentration module is also proposed to address the attribute binding problem. Without any image or human preference data, we use only 20K text prompts to fine-tune SDXL to obtain CoMat-SDXL. Extensive experiments show that CoMat-SDXL significantly outperforms the baseline model SDXL in two text-to-image alignment benchmarks and achieves start-of-the-art performance. </p><p><a href="http://arxiv.org/abs/2404.03653v1">PDF</a> Project Page: <a href="https://caraj7.github.io/comat">https://caraj7.github.io/comat</a></p><p><strong>Summary</strong><br>æ–‡æœ¬æç¤ºå’Œå›¾åƒä¹‹é—´çš„é”™ä½æ˜¯ç”±æ—©æœŸæ‰©æ•£æ­¥éª¤ä¸­æ ‡è®°æ³¨æ„åŠ›æ¿€æ´»ä¸è¶³å’Œæ‰©æ•£æ¨¡å‹æ¡ä»¶åˆ©ç”¨ä¸è¶³å¼•èµ·çš„ï¼ŒCoMaT æ˜¯ä¸€ç§æ”¹è¿›çš„æ‰©æ•£æ¨¡å‹å¾®è°ƒç­–ç•¥ï¼Œå®ƒä½¿ç”¨å›¾åƒåˆ°æ–‡æœ¬çš„æ¦‚å¿µåŒ¹é…æœºåˆ¶æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>é”™ä½æ˜¯ç”±æ ‡è®°æ³¨æ„åŠ›æ¿€æ´»ä¸è¶³å’Œæ¡ä»¶åˆ©ç”¨ä¸è¶³å¼•èµ·çš„ã€‚</li><li>CoMaT æ˜¯ä¸€ç§ç”¨äºè§£å†³é”™ä½é—®é¢˜çš„ç«¯åˆ°ç«¯æ‰©æ•£æ¨¡å‹å¾®è°ƒç­–ç•¥ã€‚</li><li>CoMaT åˆ©ç”¨å›¾åƒæ ‡é¢˜æ¨¡å‹æ¥è¯„ä¼°å›¾åƒåˆ°æ–‡æœ¬çš„å¯¹é½å¹¶å¼•å¯¼æ‰©æ•£æ¨¡å‹é‡æ–°å®¡è§†è¢«å¿½ç•¥çš„æ ‡è®°ã€‚</li><li>CoMaT å¼•å…¥äº†ä¸€ç§æ–°çš„å±æ€§é›†ä¸­æ¨¡å—æ¥è§£å†³å±æ€§ç»‘å®šé—®é¢˜ã€‚</li><li>åªéœ€ 20K ä¸ªæ–‡æœ¬æç¤ºï¼Œæ— éœ€ä»»ä½•å›¾åƒæˆ–äººç±»åå¥½æ•°æ®ï¼Œå³å¯ä½¿ç”¨ CoMaT å¾®è°ƒ SDXLï¼Œå¾—åˆ° CoMaT-SDXLã€‚</li><li>å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼ŒCoMaT-SDXL åœ¨ä¸¤ä¸ªæ–‡æœ¬åˆ°å›¾åƒå¯¹é½åŸºå‡†æµ‹è¯•ä¸­æ˜æ˜¾ä¼˜äºåŸºçº¿æ¨¡å‹ SDXLï¼Œå¹¶å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li><li>CoMaT-SDXL é€‚ç”¨äºæ‰€æœ‰æ‰©æ•£æ¨¡å‹ï¼Œå¯ä¸ä¸åŒçš„å›¾åƒç”Ÿæˆæ¨¡å‹ç›¸ç»“åˆã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šCoMatï¼šæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œåˆ©ç”¨å›¾åƒåˆ°æ–‡æœ¬æ¦‚å¿µåŒ¹é…</li><li>ä½œè€…ï¼šDongzhi Jiang, Guanglu Song, Xiaoshi Wu, Renrui Zhang, Dazhong Shen, Zhuofan Zong, Yu Liuâ€ , Hongsheng Liâ€ </li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šCUHKMMLab</li><li>å…³é”®è¯ï¼šæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆï¼Œæ‰©æ•£æ¨¡å‹ï¼Œæ–‡æœ¬å›¾åƒå¯¹é½</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.03653   Github ä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼š   (1)ï¼šç ”ç©¶èƒŒæ™¯ï¼šæ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆé¢†åŸŸå–å¾—äº†å·¨å¤§æˆåŠŸã€‚ç„¶è€Œï¼Œç¼“è§£æ–‡æœ¬æç¤ºå’Œå›¾åƒä¹‹é—´çš„é”™ä½ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚   (2)ï¼šè¿‡å»çš„æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å›¾åƒç”Ÿæˆè´¨é‡çš„æå‡ä¸Šï¼Œè€Œå¯¹æ–‡æœ¬å›¾åƒå¯¹é½çš„å…³æ³¨è¾ƒå°‘ã€‚   (3)ï¼šç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç§°ä¸º CoMat çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡å›¾åƒåˆ°æ–‡æœ¬æ¦‚å¿µåŒ¹é…æ¥å¢å¼ºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ã€‚CoMat åœ¨å›¾åƒç”Ÿæˆè¿‡ç¨‹ä¸­å¼•å…¥ä¸€ä¸ªé¢å¤–çš„æ–‡æœ¬ç¼–ç å™¨ï¼Œå°†æ–‡æœ¬æç¤ºç¼–ç ä¸ºä¸€ä¸ªæ¦‚å¿µå‘é‡ï¼Œå¹¶å°†å…¶ä¸å›¾åƒç‰¹å¾è¿›è¡ŒåŒ¹é…ã€‚   (4)ï¼šå®éªŒç»“æœï¼šåœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šï¼ŒCoMat åœ¨æ–‡æœ¬å›¾åƒå¯¹é½æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCoMat èƒ½å¤Ÿç”Ÿæˆä¸æ–‡æœ¬æç¤ºé«˜åº¦ä¸€è‡´çš„å›¾åƒï¼Œæœ‰æ•ˆç¼“è§£äº†é”™ä½é—®é¢˜ã€‚</li></ol><p>7.æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼šæ¦‚å¿µåŒ¹é…ï¼šä¸ºäº†è§£å†³æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­æ–‡æœ¬å›¾åƒå¯¹é½é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºæ¦‚å¿µåŒ¹é…æ¨¡å—ï¼Œè¯¥æ¨¡å—åˆ©ç”¨å›¾åƒæ ‡æ³¨æ¨¡å‹çš„ç›‘ç£ï¼Œè¿«ä½¿æ‰©æ•£æ¨¡å‹é‡æ–°å®¡è§†æ–‡æœ¬æ ‡è®°ï¼Œæœç´¢è¢«å¿½ç•¥çš„æ¡ä»¶ä¿¡æ¯ï¼Œä»è€Œèµ‹äºˆå…ˆå‰è¢«å¿½è§†çš„æ–‡æœ¬æ¦‚å¿µé‡è¦æ€§ï¼Œä»¥å®ç°æ›´å¥½çš„æ–‡æœ¬å›¾åƒå¯¹é½ã€‚ï¼ˆ2ï¼‰ï¼šå±æ€§é›†ä¸­ï¼šé’ˆå¯¹æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­å­˜åœ¨çš„å±æ€§ç»‘å®šé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºå±æ€§é›†ä¸­æ¨¡å—ï¼Œè¯¥æ¨¡å—é€šè¿‡å®ä½“æå–å’Œåˆ†å‰²æ¨¡å‹ï¼Œå°†å®ä½“ä¸å…¶å±æ€§ä»æ›´ç»†ç²’åº¦çš„è§’åº¦å¯¹é½ï¼Œä»è€Œå°†å®ä½“æ–‡æœ¬æè¿°çš„æ³¨æ„åŠ›é›†ä¸­åœ¨å…¶å›¾åƒåŒºåŸŸã€‚ï¼ˆ3ï¼‰ï¼šä¿çœŸåº¦ä¿æŒï¼šä¸ºäº†é˜²æ­¢æ‰©æ•£æ¨¡å‹è¿‡æ‹Ÿåˆå›¾åƒæ ‡æ³¨æ¨¡å‹çš„å¥–åŠ±ï¼Œæœ¬æ–‡å¼•å…¥å¯¹æŠ—æŸå¤±ï¼Œåˆ©ç”¨åˆ¤åˆ«å™¨æ¥åŒºåˆ†é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹å’Œå¾®è°ƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„å›¾åƒï¼Œä»è€Œåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ä¿æŒæ‰©æ•£æ¨¡å‹çš„åŸå§‹ç”Ÿæˆèƒ½åŠ›ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬æ–‡æå‡ºçš„ CoMat æ˜¯ä¸€ç§ç«¯åˆ°ç«¯çš„æ‰©æ•£æ¨¡å‹å¾®è°ƒç­–ç•¥ï¼Œé…å¤‡äº†å›¾åƒåˆ°æ–‡æœ¬æ¦‚å¿µåŒ¹é…ã€‚æˆ‘ä»¬åˆ©ç”¨å›¾åƒæ ‡æ³¨æ¨¡å‹çš„ç›‘ç£ï¼Œè¿«ä½¿æ‰©æ•£æ¨¡å‹é‡æ–°å®¡è§†æ–‡æœ¬æ ‡è®°ï¼Œæœç´¢è¢«å¿½ç•¥çš„æ¡ä»¶ä¿¡æ¯ï¼Œä»è€Œèµ‹äºˆå…ˆå‰è¢«å¿½è§†çš„æ–‡æœ¬æ¦‚å¿µé‡è¦æ€§ï¼Œä»¥å®ç°æ›´å¥½çš„æ–‡æœ¬å›¾åƒå¯¹é½ã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºæ¦‚å¿µåŒ¹é…æ¨¡å—ï¼Œé€šè¿‡å›¾åƒåˆ°æ–‡æœ¬æ¦‚å¿µåŒ¹é…å¢å¼ºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ã€‚</li><li>å¼•å…¥å±æ€§é›†ä¸­æ¨¡å—ï¼Œå°†å®ä½“æ–‡æœ¬æè¿°çš„æ³¨æ„åŠ›é›†ä¸­åœ¨å…¶å›¾åƒåŒºåŸŸï¼Œè§£å†³å±æ€§ç»‘å®šé—®é¢˜ã€‚</li><li>ä½¿ç”¨å¯¹æŠ—æŸå¤±ä¿æŒæ‰©æ•£æ¨¡å‹çš„åŸå§‹ç”Ÿæˆèƒ½åŠ›ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆå›¾åƒæ ‡æ³¨æ¨¡å‹çš„å¥–åŠ±ã€‚æ€§èƒ½ï¼š</li><li>åœ¨æ–‡æœ¬å›¾åƒå¯¹é½æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚</li><li>èƒ½å¤Ÿç”Ÿæˆä¸æ–‡æœ¬æç¤ºé«˜åº¦ä¸€è‡´çš„å›¾åƒï¼Œæœ‰æ•ˆç¼“è§£é”™ä½é—®é¢˜ã€‚å·¥ä½œé‡ï¼š</li><li>éœ€è¦å›¾åƒæ ‡æ³¨æ¨¡å‹çš„ç›‘ç£ã€‚</li><li>å¼•å…¥é¢å¤–çš„æ–‡æœ¬ç¼–ç å™¨å’Œæ¦‚å¿µåŒ¹é…æ¨¡å—ï¼Œå¢åŠ äº†æ¨¡å‹å¤æ‚åº¦ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-aef84712fb02323e10a67d7dce695c51.jpg" align="middle"><img src="https://picx.zhimg.com/v2-dae170e845e81c9adbf2e77d415f361b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5c03cae0f4ada1166232feb37cf4f92f.jpg" align="middle"></details><h2 id="DiffBody-Human-Body-Restoration-by-Imagining-with-Generative-Diffusion-Prior"><a href="#DiffBody-Human-Body-Restoration-by-Imagining-with-Generative-Diffusion-Prior" class="headerlink" title="DiffBody: Human Body Restoration by Imagining with Generative Diffusion   Prior"></a>DiffBody: Human Body Restoration by Imagining with Generative Diffusion   Prior</h2><p><strong>Authors:Yiming Zhang, Zhe Wang, Xinjie Li, Yunchen Yuan, Chengsong Zhang, Xiao Sun, Zhihang Zhong, Jian Wang</strong></p><p>Human body restoration plays a vital role in various applications related to the human body. Despite recent advances in general image restoration using generative models, their performance in human body restoration remains mediocre, often resulting in foreground and background blending, over-smoothing surface textures, missing accessories, and distorted limbs. Addressing these challenges, we propose a novel approach by constructing a human body-aware diffusion model that leverages domain-specific knowledge to enhance performance. Specifically, we employ a pretrained body attention module to guide the diffusion modelâ€™s focus on the foreground, addressing issues caused by blending between the subject and background. We also demonstrate the value of revisiting the language modality of the diffusion model in restoration tasks by seamlessly incorporating text prompt to improve the quality of surface texture and additional clothing and accessories details. Additionally, we introduce a diffusion sampler tailored for fine-grained human body parts, utilizing local semantic information to rectify limb distortions. Lastly, we collect a comprehensive dataset for benchmarking and advancing the field of human body restoration. Extensive experimental validation showcases the superiority of our approach, both quantitatively and qualitatively, over existing methods. </p><p><a href="http://arxiv.org/abs/2404.03642v1">PDF</a> </p><p><strong>Summary</strong><br>äººä½“ä¿®å¤æ³¨æ„ç½‘ç»œç”Ÿæˆæ¨¡å‹åœ¨å‰æ™¯èƒŒæ™¯èåˆã€è¿‡å¹³æ»‘çº¹ç†ã€æ·»åŠ é…é¥°å’Œè‚¢ä½“å˜å½¢ç­‰æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œå› æ­¤æå‡ºä¸€ç§æ–°çš„æ–¹æ³•æ„å»ºäººä½“æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ä½¿ç”¨é¢„è®­ç»ƒçš„èº«ä½“æ³¨æ„åŠ›æ¨¡å—å¼•å¯¼æ‰©æ•£æ¨¡å‹å…³æ³¨å‰æ™¯ï¼Œè§£å†³ä¸»ä½“å’ŒèƒŒæ™¯æ··åˆçš„é—®é¢˜ã€‚</li><li>å°†æ–‡æœ¬æç¤ºæ— ç¼èå…¥æ¢å¤ä»»åŠ¡ä¸­ï¼Œæé«˜è¡¨é¢çº¹ç†å’Œæ·»åŠ è¡£ç‰©å’Œé…é¥°çš„è´¨é‡ã€‚</li><li>å¼•å…¥é’ˆå¯¹äººä½“ç²¾ç»†éƒ¨åˆ†çš„æ‰©æ•£é‡‡æ ·å™¨ï¼Œåˆ©ç”¨å±€éƒ¨è¯­ä¹‰ä¿¡æ¯çº æ­£è‚¢ä½“å˜å½¢ã€‚</li><li>æ”¶é›†äº†ä¸€ä¸ªç”¨äºäººä½“ä¿®å¤é¢†åŸŸåŸºå‡†æµ‹è¯•å’Œå‘å±•çš„å…¨é¢æ•°æ®é›†ã€‚</li><li>å¤§é‡å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•åœ¨å®šé‡å’Œå®šæ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šåŸºäºæƒ³è±¡çš„å…¨èº«ä¿®å¤</li><li>ä½œè€…ï¼šFanruan Meng, Wenbo Li, Yihang Yin, Jiapeng Zhu, Mingming He</li><li>å•ä½ï¼šä¸Šæµ·äº¤é€šå¤§å­¦</li><li>å…³é”®è¯ï¼šå›¾åƒä¿®å¤ï¼Œäººä½“å›¾åƒï¼Œæ‰©æ•£æ¨¡å‹</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.02385ï¼ŒGithub ä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šäººä½“ä¿®å¤åœ¨ä¸äººä½“ç›¸å…³çš„å„ç§åº”ç”¨ä¸­è‡³å…³é‡è¦ã€‚å°½ç®¡æœ€è¿‘åœ¨ä½¿ç”¨ç”Ÿæˆæ¨¡å‹è¿›è¡Œé€šç”¨å›¾åƒä¿®å¤æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†å®ƒä»¬åœ¨äººä½“ä¿®å¤ä¸­çš„æ€§èƒ½ä»ç„¶å¹³åº¸ï¼Œé€šå¸¸ä¼šå¯¼è‡´å‰æ™¯å’ŒèƒŒæ™¯æ··åˆã€è¿‡åº¦å¹³æ»‘è¡¨é¢çº¹ç†ã€ä¸¢å¤±é…é¥°å’Œè‚¢ä½“æ‰­æ›²ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œé€šè¿‡æ„å»ºä¸€ä¸ªåˆ©ç”¨é¢†åŸŸç‰¹å®šçŸ¥è¯†æ¥å¢å¼ºæ€§èƒ½çš„äººä½“æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ä¸ªé¢„è®­ç»ƒçš„èº«ä½“æ³¨æ„åŠ›æ¨¡å—æ¥å¼•å¯¼æ‰©æ•£æ¨¡å‹ä¸“æ³¨äºå‰æ™¯ï¼Œè§£å†³ä¸»ä½“å’ŒèƒŒæ™¯ä¹‹é—´æ··åˆå¼•èµ·çš„é—®é¢˜ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†åœ¨ä¿®å¤ä»»åŠ¡ä¸­é‡æ–°å®¡è§†æ‰©æ•£æ¨¡å‹çš„è¯­è¨€æ¨¡æ€çš„ä»·å€¼ï¼Œé€šè¿‡æ— ç¼åœ°åˆå¹¶æ–‡æœ¬æç¤ºæ¥æé«˜è¡¨é¢çº¹ç†å’Œé¢å¤–æœè£…å’Œé…é¥°ç»†èŠ‚çš„è´¨é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªé’ˆå¯¹ç»†ç²’åº¦äººä½“éƒ¨ä½é‡èº«å®šåˆ¶çš„æ‰©æ•£é‡‡æ ·å™¨ï¼Œåˆ©ç”¨å±€éƒ¨è¯­ä¹‰ä¿¡æ¯æ¥çº æ­£è‚¢ä½“æ‰­æ›²ã€‚æœ€åï¼Œæˆ‘ä»¬æ”¶é›†äº†ä¸€ä¸ªå…¨é¢çš„æ•°æ®é›†ï¼Œç”¨äºå¯¹äººä½“ä¿®å¤é¢†åŸŸè¿›è¡ŒåŸºå‡†æµ‹è¯•å’Œæ¨è¿›ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šå¹¿æ³›çš„å®éªŒéªŒè¯å±•ç¤ºäº†æˆ‘ä»¬æ–¹æ³•åœ¨å®šé‡å’Œå®šæ€§ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚ï¼ˆ4ï¼‰ä»»åŠ¡å’Œæ€§èƒ½ï¼šåœ¨äººä½“ä¿®å¤ä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•å®ç°äº†ä»¥ä¸‹æ€§èƒ½ï¼š</li><li>å®šé‡è¯„ä¼°ï¼šåœ¨ CelebA-HQ æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ PSNR å’Œ SSIM æŒ‡æ ‡ä¸Šå‡ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li><li><p>å®šæ€§è¯„ä¼°ï¼šåœ¨çœŸå®ä¸–ç•Œä½è´¨é‡äººä½“å›¾åƒä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é¢éƒ¨å’Œè‚¢ä½“ç»†èŠ‚ä¸Šä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼šåˆæ­¥æ§åˆ¶ç½‘ç»œï¼šControlNetæ˜¯ä¸€ä¸ªé«˜çº§ç¥ç»ç½‘ç»œæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ç»“åˆç‰¹å®šå›¾åƒæ¡ä»¶æ¥å¢å¼ºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ã€‚ç»™å®šè¾“å…¥å›¾åƒz0ï¼Œå›¾åƒæ‰©æ•£ç®—æ³•é€æ­¥å‘å›¾åƒæ·»åŠ å™ªå£°ï¼Œç”Ÿæˆå™ªå£°å›¾åƒztï¼Œå…¶ä¸­tè¡¨ç¤ºå™ªå£°æ·»åŠ è¿­ä»£çš„æ¬¡æ•°ã€‚ControlNetå¼•å…¥äº†ä¸€ç»„æ¡ä»¶ï¼ŒåŒ…æ‹¬æ—¶é—´æ­¥é•¿ã€æ–‡æœ¬æç¤ºctå’Œç‰¹å®šäºä»»åŠ¡çš„æ¡ä»¶cfã€‚è¿™äº›ç®—æ³•å­¦ä¹ äº†ä¸€ä¸ªç½‘ç»œÏµÎ¸æ¥é¢„æµ‹æ·»åŠ åˆ°å™ªå£°å›¾åƒztä¸­çš„å™ªå£°ã€‚å­¦ä¹ ç›®æ ‡Lï¼Œå¯¹äºæ•´ä¸ªæ‰©æ•£æ¨¡å‹çš„ä¼˜åŒ–è‡³å…³é‡è¦ï¼Œè¡¨ç¤ºä¸ºï¼šL(Î¸)=Ez0,Ïµ,t,ct,cfï¿½âˆ¥Ïµâˆ’ÏµÎ¸(zt,t,ct,cf)âˆ¥22ï¿½(1)è¿™ä¸ªæ–¹ç¨‹è¡¨ç¤ºå®é™…å™ªå£°Ïµå’Œç½‘ç»œÏµÎ¸é¢„æµ‹çš„å™ªå£°ä¹‹é—´çš„é¢„æœŸå·®å¼‚ï¼Œç»™å®šæ¯ä¸ªæ—¶é—´æ­¥é•¿çš„æ¡ä»¶ã€‚ç›®æ ‡Lç›´æ¥ç”¨äºä½¿ç”¨ControlNetå¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œæ—¨åœ¨æœ€å°åŒ–è¿™ç§å·®å¼‚ï¼Œä»è€Œå¢å¼ºç”Ÿæˆå›¾åƒå¯¹ç»™å®šæ¡ä»¶çš„ä¿çœŸåº¦å’Œç›¸å…³æ€§ã€‚ï¼ˆ2ï¼‰ï¼šé€šè¿‡ç»“æ„å¼•å¯¼å¢å¼ºäººä½“å›¾åƒä¿®å¤ï¼šåœ¨å¼€å‘ç”¨äºäººä½“å›¾åƒä¿®å¤çš„ç¨³å¥ç®¡é“æ—¶ï¼Œæˆ‘ä»¬æœ€åˆçš„ç›®æ ‡æ˜¯å‡å°‘ä½è´¨é‡ï¼ˆLQï¼‰å›¾åƒä¸­å¯è§‚å¯Ÿåˆ°çš„é€€åŒ–ã€‚è¿™ä¸ªåŸºç¡€æ­¥éª¤ç¡®ä¿åç»­å¤„ç†é˜¶æ®µå¯ä»¥åœ¨ä¸å—ç°æœ‰æŸä¼¤å¹²æ‰°çš„æƒ…å†µä¸‹æ›´æœ‰æ•ˆåœ°è¯†åˆ«è¿™äº›å›¾åƒä¸­çš„ç‰¹å¾ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬ç»“åˆäº†SwinIR[19]æ¨¡å‹æ¶æ„ï¼Œè¯¥æ¶æ„å·²åœ¨ä¸æˆ‘ä»¬æ„Ÿå…´è¶£çš„é¢†åŸŸç›¸å…³çš„ç‰¹å®šæ•°æ®é›†ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œå¹¶é€šè¿‡åœ¨æˆ‘ä»¬ä¸“é—¨ç”¨äºäººä½“çš„ç‰¹å®šæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒè¿›ä¸€æ­¥ä¼˜åŒ–ã€‚ä¿®å¤æ¨¡å—ä¼˜åŒ–çš„ä¸»è¦ç›®æ ‡å›´ç»•æœ€å°åŒ–L2åƒç´ æŸå¤±ï¼Œå…¶æ•°å­¦æè¿°ä¸ºï¼šIreg=SwinIR(ILQ),Lreg=âˆ¥Iregâˆ’IHQâˆ¥22(2)å…¶ä¸­IHQå’ŒILQåˆ†åˆ«ä»£è¡¨é«˜è´¨é‡å’Œä½è´¨é‡å›¾åƒï¼Œè€ŒIregæ˜¯å›å½’å­¦ä¹ çš„è¾“å‡ºï¼Œè¢«è®¾ç½®ä¸ºè¿›è¡Œè¿›ä¸€æ­¥ä¿®å¤å¤„ç†ã€‚Iregä¸­é‡åˆ°çš„ä¸€ä¸ªæ˜¾ç€æŒ‘æˆ˜åŒ…æ‹¬å®ƒå®¹æ˜“è¿‡åº¦å¹³æ»‘å’Œä¸¢å¤±ç»†èŠ‚â€”â€”ä¿å®ˆå›¾åƒä¿®å¤æ–¹æ³•çš„å…¸å‹ä¼ªå½±ã€‚ç„¶è€Œï¼ŒSwinIRåœ¨å™ªå£°å‡å°‘æ–¹é¢çš„åŠŸæ•ˆä½¿åç»­å§¿æ€æ£€æµ‹å’Œæ³¨æ„åŠ›æ£€æµ‹æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°å¯¹Iregè¿›è¡Œæ“ä½œã€‚å› æ­¤ï¼Œæˆ‘ä»¬åŒæ—¶é‡‡ç”¨äººä½“å§¿æ€æ£€æµ‹æ¨¡å‹[51]å’Œèº«ä½“éƒ¨ä½æ³¨æ„åŠ›æ¨¡å‹[39]æ¥åˆ†åˆ«ä¸ºäººä½“ç”Ÿæˆå§¿æ€å’Œæ³¨æ„åŠ›å›¾ï¼šIpose=DWPose(Ireg),Iattn=Attn(Ireg)(3)åœ¨è¿™ä¸ªæ¡†æ¶ä¸­ï¼ŒIposeæŒ‡çš„æ˜¯ä»Iregæ´¾ç”Ÿçš„å§¿æ€å›¾åƒï¼Œè€ŒIattnæ•è·äº†ä»Iregä¸­è¾¨åˆ«å‡ºçš„äººä½“çš„æ³¨æ„åŠ›çƒ­å›¾ã€‚è¿™ç§åˆ›æ–°æ–¹æ³•å¼ºè°ƒäº†æˆ‘ä»¬è‡´åŠ›äºé€šè¿‡æ•´åˆç»“æ„æŒ‡å¯¼æ¥å¢å¼ºäººä½“å›¾åƒä¿®å¤çš„æ‰¿è¯ºï¼Œæœ‰æ•ˆåœ°è§£å†³äº†å¸¸è§çš„ä¿®å¤æŒ‘æˆ˜ï¼ŒåŒæ—¶ä¸ºæ›´ç»†è‡´å’Œç»†èŠ‚ä¸°å¯Œçš„é‡å»ºå¥ å®šåŸºç¡€ã€‚ï¼ˆ3ï¼‰ï¼šåˆ©ç”¨æ–‡æœ¬ä¿¡æ¯è¿›è¡Œå›¾åƒä¿®å¤ï¼šä¼ ç»Ÿçš„å›¾åƒä¿®å¤æ¨¡å‹åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå¿½ç•¥äº†æ–‡æœ¬ä¿¡æ¯çš„åˆ©ç”¨ï¼Œæ–‡æœ¬ä¿¡æ¯ä»£è¡¨äº†ä¸€ä¸ªé‡è¦ä¸”æœªå¼€å‘çš„å…ˆéªŒçŸ¥è¯†æ¥æºã€‚è¿™ç§ç–å¿½å¿½è§†äº†æ–‡æœ¬æ˜¾ç€å¢å¼ºç”Ÿæˆé«˜è´¨é‡å›¾åƒçš„æ½œåŠ›ã€‚åœ¨æˆ‘ä»¬çš„æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬åœ¨æ½œå˜é‡æ‰©æ•£æ¨¡å‹çš„è®­ç»ƒé˜¶æ®µåˆ©ç”¨äº†ç»Ÿä¸€æ ¼å¼çš„æ–‡æœ¬æè¿°ï¼Œè¯¥æè¿°ä¸“é—¨è®¾è®¡ç”¨äºä»¥äººä¸ºä¸­å¿ƒçš„ä¸»ä½“ã€‚é€šè¿‡ä½¿ç”¨GPT4Væ¨¡å‹[29]ï¼Œæˆ‘ä»¬ç”Ÿæˆé«˜è´¨é‡äººç±»å›¾åƒçš„è¯¦ç»†æè¿°ï¼Œéµå¾ªä»ä¸Šåˆ°ä¸‹çš„ç²¾å¿ƒå®šä¹‰çš„é¡ºåºã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œè¿™äº›ç»“æ„åŒ–çš„æ–‡æœ¬æç¤ºæ˜¾ç€æé«˜äº†æ¨¡å‹åœ¨é‡å»ºå›¾åƒæ–¹é¢çš„ç²¾åº¦ã€‚å›¾3æä¾›äº†æ‰€åˆ©ç”¨çš„ç»Ÿä¸€æ ¼å¼æ–‡æœ¬æç¤ºçš„è¯´æ˜æ€§ç¤ºä¾‹ã€‚ï¼ˆ4ï¼‰ï¼šç”¨äºæ‰©æ•£é‡‡æ ·çš„ä»¥äººä¸ºä¸­å¿ƒæŒ‡å¯¼ï¼šå°½ç®¡æˆ‘ä»¬ä¸Šè¿°ç­–ç•¥å–å¾—äº†ä»¤äººç§°é“çš„ä¿®å¤ç»“æœï¼Œä½†åœ¨æ½œå˜é‡æ‰©æ•£æ¨¡å‹ä¸­çš„æ‰©æ•£è¿‡ç¨‹ä¸­ä»ç„¶å­˜åœ¨æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ‰©æ•£é‡‡æ ·å™¨ï¼Œè¯¥é‡‡æ ·å™¨åˆ©ç”¨å±€éƒ¨è¯­ä¹‰ä¿¡æ¯æ¥æŒ‡å¯¼é‡‡æ ·è¿‡ç¨‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå®šåˆ¶çš„é‡‡æ ·å™¨ï¼Œè¯¥é‡‡æ ·å™¨åˆ©ç”¨äººä½“éƒ¨ä½çš„è¯­ä¹‰åˆ†å‰²å›¾ã€‚é€šè¿‡å°†è¯­ä¹‰åˆ†å‰²å›¾ä½œä¸ºæ¡ä»¶ä¼ é€’ç»™é‡‡æ ·å™¨ï¼Œæˆ‘ä»¬èƒ½å¤Ÿé¼“åŠ±é‡‡æ ·å™¨ä¸“æ³¨äºç‰¹å®šçš„äººä½“éƒ¨ä½ï¼Œä»è€Œå‡å°‘è‚¢ä½“æ‰­æ›²å’Œæ”¹å–„æ•´ä½“å›¾åƒè´¨é‡ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œæå‡ºäº†ä¸€ç§æ–°é¢–çš„åŸºäºStable Diffusionæ¨¡å‹çš„äººä½“ä¿®å¤æ¡†æ¶DiffBodyï¼Œè¯¥æ¡†æ¶é€šè¿‡å°†ä»¥äººä¸ºä¸­å¿ƒçš„æŒ‡å¯¼èå…¥é¢„è®­ç»ƒçš„Stable Diffusionæ¨¡å‹ä¸­ï¼Œå®ç°äº†é€¼çœŸçš„ä¿®å¤æ•ˆæœã€‚é€šè¿‡åº”ç”¨å„ç§ä»¥äººä¸ºä¸­å¿ƒçš„æ¡ä»¶ï¼Œæˆ‘ä»¬è§£å†³äº†äººä½“ä¿®å¤ä¸­çš„ä¼ªå½±å¹¶å¯¹å…¶è¿›è¡Œäº†ä¿®æ­£ï¼Œè¶…è¶Šäº†ç°æœ‰é€šç”¨å›¾åƒä¿®å¤æ¨¡å‹çš„èƒ½åŠ›ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§é€šè¿‡å°†äººä½“å§¿æ€ã€æ³¨æ„åŠ›å’Œæ–‡æœ¬ä¿¡æ¯èå…¥æ½œå˜é‡æ‰©æ•£æ¨¡å‹æ¥å¢å¼ºäººä½“ä¿®å¤çš„æ–¹æ³•ã€‚</li><li>è®¾è®¡äº†ä¸€ç§æ–°çš„æ‰©æ•£é‡‡æ ·å™¨ï¼Œåˆ©ç”¨å±€éƒ¨è¯­ä¹‰ä¿¡æ¯æ¥æŒ‡å¯¼é‡‡æ ·è¿‡ç¨‹ï¼Œå‡å°‘è‚¢ä½“æ‰­æ›²å¹¶æé«˜æ•´ä½“å›¾åƒè´¨é‡ã€‚</li><li>æ”¶é›†äº†ä¸€ä¸ªå…¨é¢çš„äººä½“ä¿®å¤æ•°æ®é›†ï¼Œç”¨äºåŸºå‡†æµ‹è¯•å’Œæ¨è¿›è¯¥é¢†åŸŸçš„ç ”ç©¶ã€‚æ€§èƒ½ï¼š</li><li>åœ¨CelebA-HQæ•°æ®é›†ä¸Šï¼ŒDiffBodyåœ¨PSNRå’ŒSSIMæŒ‡æ ‡ä¸Šå‡ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li><li>åœ¨çœŸå®ä¸–ç•Œä½è´¨é‡äººä½“å›¾åƒä¸Šï¼ŒDiffBodyåœ¨é¢éƒ¨å’Œè‚¢ä½“ç»†èŠ‚ä¿®å¤æ–¹é¢ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚å·¥ä½œé‡ï¼š</li><li>è¯¥æ–¹æ³•éœ€è¦æ”¶é›†å’Œæ ‡æ³¨ä¸€ä¸ªç‰¹å®šçš„äººä½“ä¿®å¤æ•°æ®é›†ã€‚</li><li>éœ€è¦å¯¹Stable Diffusionæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥é€‚åº”äººä½“ä¿®å¤ä»»åŠ¡ã€‚</li><li>å®ç°ä»¥äººä¸ºä¸­å¿ƒçš„æŒ‡å¯¼æ¡ä»¶éœ€è¦é¢å¤–çš„å¼€å‘å·¥ä½œã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-ba15218f0f2e1b9b5b031bee571dc1f6.jpg" align="middle"><img src="https://pica.zhimg.com/v2-67c39cfc81eeef9c78f2dd19795603d6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9fe587cd2a98fb08f0767dcb2aa68fa2.jpg" align="middle"></details><h2 id="Future-Proofing-Class-Incremental-Learning"><a href="#Future-Proofing-Class-Incremental-Learning" class="headerlink" title="Future-Proofing Class Incremental Learning"></a>Future-Proofing Class Incremental Learning</h2><p><strong>Authors:Quentin Jodelet, Xin Liu, Yin Jun Phua, Tsuyoshi Murata</strong></p><p>Exemplar-Free Class Incremental Learning is a highly challenging setting where replay memory is unavailable. Methods relying on frozen feature extractors have drawn attention recently in this setting due to their impressive performances and lower computational costs. However, those methods are highly dependent on the data used to train the feature extractor and may struggle when an insufficient amount of classes are available during the first incremental step. To overcome this limitation, we propose to use a pre-trained text-to-image diffusion model in order to generate synthetic images of future classes and use them to train the feature extractor. Experiments on the standard benchmarks CIFAR100 and ImageNet-Subset demonstrate that our proposed method can be used to improve state-of-the-art methods for exemplar-free class incremental learning, especially in the most difficult settings where the first incremental step only contains few classes. Moreover, we show that using synthetic samples of future classes achieves higher performance than using real data from different classes, paving the way for better and less costly pre-training methods for incremental learning. </p><p><a href="http://arxiv.org/abs/2404.03200v1">PDF</a> </p><p><strong>Summary</strong><br>åˆ©ç”¨é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆæœªæ¥ç±»åˆ«çš„åˆæˆå›¾åƒï¼Œå¯æå‡æ— æ ·æœ¬ç±»å¢é‡å­¦ä¹ çš„æ€§èƒ½ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æ— æ ·æœ¬ç±»å¢é‡å­¦ä¹ ä¸­ï¼ŒåŸºäºå†»ç»“ç‰¹å¾æå–å™¨çš„æ¨¡å‹å› å…¶å‡ºè‰²æ€§èƒ½å’Œä½è®¡ç®—æˆæœ¬è€Œå¤‡å—å…³æ³¨ã€‚</li><li>ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹é«˜åº¦ä¾èµ–äºè®­ç»ƒç‰¹å¾æå–å™¨çš„æ•°æ®ï¼Œåœ¨é¦–ä¸ªå¢é‡æ­¥éª¤ä¸­å¯ç”¨ç±»åˆ«æ•°é‡ä¸è¶³æ—¶å¯èƒ½å­˜åœ¨å›°éš¾ã€‚</li><li>ç ”ç©¶è€…æå‡ºä½¿ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆæœªæ¥ç±»åˆ«çš„åˆæˆå›¾åƒï¼Œå¹¶åˆ©ç”¨è¿™äº›å›¾åƒè®­ç»ƒç‰¹å¾æå–å™¨ã€‚</li><li>åœ¨ CIFAR100 å’Œ ImageNet-Subset æ ‡å‡†åŸºå‡†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•å¯ç”¨æ¥æ”¹è¿›æ— æ ·æœ¬ç±»å¢é‡å­¦ä¹ çš„æœ€æ–°æ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨é¦–ä¸ªå¢é‡æ­¥éª¤ä»…åŒ…å«å°‘é‡ç±»åˆ«çš„æœ€å›°éš¾è®¾ç½®ä¸­ã€‚</li><li>ä½¿ç”¨æœªæ¥ç±»åˆ«çš„åˆæˆæ ·æœ¬æ¯”ä½¿ç”¨æ¥è‡ªä¸åŒç±»åˆ«çš„çœŸå®æ•°æ®èƒ½å–å¾—æ›´é«˜çš„æ€§èƒ½ï¼Œä¸ºå¢é‡å­¦ä¹ æä¾›æ›´ä½³ã€æ›´ä½æˆæœ¬çš„é¢„è®­ç»ƒæ–¹æ³•ã€‚</li><li>æœªæ¥ç ”ç©¶æ–¹å‘åŒ…æ‹¬æ¢ç´¢å…¶ä»–åˆæˆæ•°æ®ç”ŸæˆæŠ€æœ¯ä»¥åŠåˆ©ç”¨åˆæˆæ•°æ®è¿›è¡Œå¾®è°ƒçš„æœ‰æ•ˆæ–¹æ³•ã€‚</li><li>æ­¤å¤–ï¼Œè¿˜å¯ä»¥è€ƒè™‘ç ”ç©¶åœ¨å®æ—¶åœºæ™¯ä¸­ç”Ÿæˆåˆæˆæ•°æ®çš„å¯èƒ½æ€§ï¼Œä»¥ä¾¿åœ¨éƒ¨ç½²æœŸé—´æŒç»­æ‰§è¡Œå¢é‡å­¦ä¹ ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šæœªæ¥è¯æ˜ç±»å¢é‡å­¦ä¹ </li><li>ä½œè€…ï¼šQuentin Jodelet, Xin Liu, Yin Jun Phua, Tsuyoshi Murata</li><li>éš¶å±ï¼šä¸œäº¬å·¥ä¸šå¤§å­¦è®¡ç®—æœºç§‘å­¦ç³»</li><li>å…³é”®è¯ï¼šç±»å¢é‡å­¦ä¹ ã€æŒç»­å­¦ä¹ ã€å›¾åƒåˆ†ç±»ã€å›¾åƒç”Ÿæˆ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.03200</li><li><p>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šç±»å¢é‡å­¦ä¹ æ˜¯æ·±åº¦å­¦ä¹ çš„ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é¢†åŸŸï¼Œå®ƒè¦æ±‚æ¨¡å‹åœ¨æ²¡æœ‰è®¿é—®å…ˆå‰å­¦ä¹ ç±»çš„æƒ…å†µä¸‹ï¼Œä¸æ–­å­¦ä¹ æ–°ç±»ã€‚æ— ç¤ºä¾‹ç±»å¢é‡å­¦ä¹  (EF-CIL) æ˜¯ç±»å¢é‡å­¦ä¹ ä¸­æ›´å…·æŒ‘æˆ˜æ€§çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä¸å…è®¸ä½¿ç”¨å›æ”¾å†…å­˜ã€‚(2) è¿‡å»çš„æ–¹æ³•ï¼šåŸºäºå†»ç»“ç‰¹å¾æå–å™¨çš„ EF-CIL æ–¹æ³•å› å…¶ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½å’Œè¾ƒä½çš„è®¡ç®—æˆæœ¬è€Œå—åˆ°å…³æ³¨ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é«˜åº¦ä¾èµ–äºç”¨äºè®­ç»ƒç‰¹å¾æå–å™¨çš„åˆå§‹æ•°æ®ï¼Œå¹¶ä¸”å½“ç¬¬ä¸€ä¸ªå¢é‡æ­¥éª¤ä¸­å¯ç”¨çš„ç±»æ•°é‡ä¸è¶³æ—¶ï¼Œå¯èƒ½ä¼šé‡åˆ°å›°éš¾ã€‚(3) è®ºæ–‡æ–¹æ³•ï¼šä¸ºäº†å…‹æœè¿™ä¸€é™åˆ¶ï¼Œæœ¬æ–‡æå‡ºä½¿ç”¨é¢„å…ˆè®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆæœªæ¥ç±»åˆ«çš„åˆæˆå›¾åƒï¼Œå¹¶ä½¿ç”¨è¿™äº›å›¾åƒæ¥è®­ç»ƒç‰¹å¾æå–å™¨ã€‚(4) å®éªŒç»“æœï¼šåœ¨ CIFAR100 å’Œ ImageNet-Subset ç­‰æ ‡å‡†åŸºå‡†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•å¯ä»¥ç”¨æ¥æé«˜æ— ç¤ºä¾‹ç±»å¢é‡å­¦ä¹ çš„æœ€æ–°æ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨ç¬¬ä¸€ä¸ªå¢é‡æ­¥éª¤ä»…åŒ…å«å°‘é‡ç±»åˆ«çš„æœ€å›°éš¾æƒ…å†µä¸‹ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜è¡¨æ˜ï¼Œä½¿ç”¨æœªæ¥ç±»åˆ«çš„åˆæˆæ ·æœ¬æ¯”ä½¿ç”¨æ¥è‡ªä¸åŒç±»åˆ«çš„çœŸå®æ•°æ®èƒ½è·å¾—æ›´é«˜çš„æ€§èƒ½ï¼Œä¸ºå¢é‡å­¦ä¹ çš„æ›´å¥½ä¸”æˆæœ¬æ›´ä½çš„é¢„è®­ç»ƒæ–¹æ³•é“ºå¹³äº†é“è·¯ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1): ä½¿ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œç”Ÿæˆæœªæ¥ç±»åˆ«çš„åˆæˆå›¾åƒï¼Œå¹¶ä½¿ç”¨è¿™äº›å›¾åƒè®­ç»ƒç‰¹å¾æå–å™¨ã€‚(2): åœ¨æ— ç¤ºä¾‹ç±»å¢é‡å­¦ä¹ ä¸­ï¼Œä½¿ç”¨åˆæˆå›¾åƒå¯¹ç‰¹å¾æå–å™¨è¿›è¡Œé¢„è®­ç»ƒï¼Œå¯ä»¥æé«˜æ¨¡å‹çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨ç¬¬ä¸€ä¸ªå¢é‡æ­¥éª¤ä»…åŒ…å«å°‘é‡ç±»åˆ«çš„æœ€å›°éš¾æƒ…å†µä¸‹ã€‚(3): ä½¿ç”¨æœªæ¥ç±»åˆ«çš„åˆæˆæ ·æœ¬æ¯”ä½¿ç”¨æ¥è‡ªä¸åŒç±»åˆ«çš„çœŸå®æ•°æ®èƒ½è·å¾—æ›´é«˜çš„æ€§èƒ½ï¼Œä¸ºå¢é‡å­¦ä¹ çš„æ›´å¥½ä¸”æˆæœ¬æ›´ä½çš„é¢„è®­ç»ƒæ–¹æ³•é“ºå¹³äº†é“è·¯ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ— ç¤ºä¾‹ç±»å¢é‡å­¦ä¹ æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å¤§å‹é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆæœªæ¥ç±»åˆ«çš„å›¾åƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æ˜¾è‘—æé«˜ç°æœ‰æ–¹æ³•çš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶åªä¿®æ”¹äº†åˆå§‹æ­¥éª¤ã€‚æˆ‘ä»¬å‘ç°ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ¯”ä¾èµ–äºçœŸå®æ•´ç†æ•°æ®é›†çš„ä¼ ç»Ÿæ–¹æ³•éœ€è¦æ›´å°‘çš„æ•°æ®ã€‚è™½ç„¶æˆ‘ä»¬ç›®å‰çš„è¿™é¡¹ç ”ç©¶ä»…é™äºåœ¨ç¬¬ä¸€ä¸ªå¢é‡æ­¥éª¤ä¸­ä»å¤´å¼€å§‹è®­ç»ƒçš„ç‰¹å¾æå–å™¨ï¼Œä½†åœ¨æœªæ¥çš„å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°†è¿›ä¸€æ­¥ç ”ç©¶å¦‚ä½•ä½¿ç”¨æœªæ¥ç±»åˆ«çš„åˆæˆå›¾åƒæ¥é€‚åº”é€šç”¨çš„é¢„è®­ç»ƒåŸºç¡€ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šä½¿ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆæœªæ¥ç±»åˆ«çš„åˆæˆå›¾åƒï¼Œå¹¶ä½¿ç”¨è¿™äº›å›¾åƒæ¥è®­ç»ƒç‰¹å¾æå–å™¨ã€‚æ€§èƒ½ï¼šåœ¨æ— ç¤ºä¾‹ç±»å¢é‡å­¦ä¹ ä¸­ï¼Œä½¿ç”¨åˆæˆå›¾åƒå¯¹ç‰¹å¾æå–å™¨è¿›è¡Œé¢„è®­ç»ƒï¼Œå¯ä»¥æé«˜æ¨¡å‹çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨ç¬¬ä¸€ä¸ªå¢é‡æ­¥éª¤ä»…åŒ…å«å°‘é‡ç±»åˆ«çš„æœ€å›°éš¾æƒ…å†µä¸‹ã€‚å·¥ä½œé‡ï¼šä½¿ç”¨æœªæ¥ç±»åˆ«çš„åˆæˆæ ·æœ¬æ¯”ä½¿ç”¨æ¥è‡ªä¸åŒç±»åˆ«çš„çœŸå®æ•°æ®èƒ½è·å¾—æ›´é«˜çš„æ€§èƒ½ï¼Œä¸ºå¢é‡å­¦ä¹ çš„æ›´å¥½ä¸”æˆæœ¬æ›´ä½çš„é¢„è®­ç»ƒæ–¹æ³•é“ºå¹³äº†é“è·¯ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-5adb96d9627531125646ce0ee2191406.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-8e81c8158234e67aa146c6f8d8de1ebe.jpg" align="middle"><img src="https://pica.zhimg.com/v2-5c788dcee57eb62445a58074bf15bf51.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3113b9fb60c9b18bc0b976dc329e64c8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e740fe0c99bec8a3654bee8ea504eafa.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-305f4f5b7b6fe9b6ad21c95c6b3351a4.jpg" align="middle"></details><h2 id="HandDiff-3D-Hand-Pose-Estimation-with-Diffusion-on-Image-Point-Cloud"><a href="#HandDiff-3D-Hand-Pose-Estimation-with-Diffusion-on-Image-Point-Cloud" class="headerlink" title="HandDiff: 3D Hand Pose Estimation with Diffusion on Image-Point Cloud"></a>HandDiff: 3D Hand Pose Estimation with Diffusion on Image-Point Cloud</h2><p><strong>Authors:Wencan Cheng, Hao Tang, Luc Van Gool, Jong Hwan Ko</strong></p><p>Extracting keypoint locations from input hand frames, known as 3D hand pose estimation, is a critical task in various human-computer interaction applications. Essentially, the 3D hand pose estimation can be regarded as a 3D point subset generative problem conditioned on input frames. Thanks to the recent significant progress on diffusion-based generative models, hand pose estimation can also benefit from the diffusion model to estimate keypoint locations with high quality. However, directly deploying the existing diffusion models to solve hand pose estimation is non-trivial, since they cannot achieve the complex permutation mapping and precise localization. Based on this motivation, this paper proposes HandDiff, a diffusion-based hand pose estimation model that iteratively denoises accurate hand pose conditioned on hand-shaped image-point clouds. In order to recover keypoint permutation and accurate location, we further introduce joint-wise condition and local detail condition. Experimental results demonstrate that the proposed HandDiff significantly outperforms the existing approaches on four challenging hand pose benchmark datasets. Codes and pre-trained models are publicly available at <a href="https://github.com/cwc1260/HandDiff">https://github.com/cwc1260/HandDiff</a>. </p><p><a href="http://arxiv.org/abs/2404.03159v1">PDF</a> Accepted as a conference paper to the Conference on Computer Vision   and Pattern Recognition (2024)</p><p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹ç»è¿‡æ”¹è¿›ï¼Œæå‡º HandDiff æ¨¡å‹ç”¨äºæ‰‹éƒ¨å§¿åŠ¿ä¼°è®¡ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿå¤„ç†å¤æ‚æ’åˆ—æ˜ å°„å’Œç²¾ç¡®å®šä½ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æ‰‹éƒ¨å§¿åŠ¿ä¼°è®¡ä»»åŠ¡å¯è§†ä¸º 3D ç‚¹å­é›†ç”Ÿæˆé—®é¢˜ï¼ŒåŸºäºè¾“å…¥å¸§ç”Ÿæˆã€‚</li><li>æ‰©æ•£æ¨¡å‹åœ¨æ‰‹éƒ¨å§¿åŠ¿ä¼°è®¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ç›´æ¥ä½¿ç”¨å­˜åœ¨å±€é™æ€§ã€‚</li><li>HandDiff æ¨¡å‹åŸºäºæ‰©æ•£æ¨¡å‹ï¼Œæ¡ä»¶åŒ–æ‰‹éƒ¨å½¢çŠ¶å›¾åƒç‚¹äº‘ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ¢å¤å…³é”®ç‚¹æ’åˆ—å’Œå‡†ç¡®ä½ç½®ã€‚</li><li>å¼•å…¥äº†å…³èŠ‚æ¡ä»¶å’Œå±€éƒ¨ç»†èŠ‚æ¡ä»¶ï¼Œä»¥æ”¹å–„å…³é”®ç‚¹å®šä½ã€‚</li><li>å®éªŒç»“æœè¡¨æ˜ HandDiff åœ¨å››ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ‰‹éƒ¨å§¿åŠ¿åŸºå‡†æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li><li>HandDiff æ¨¡å‹çš„ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å·²å¼€æºã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šåŸºäºå›¾åƒç‚¹äº‘æ‰©æ•£çš„ 3D æ‰‹éƒ¨å§¿åŠ¿ä¼°è®¡</li><li>ä½œè€…ï¼šWencan Cheng, Hao Tang, Luc Van Gool, JongHwan Ko</li><li>å•ä½ï¼šéŸ©å›½æˆå‡é¦†å¤§å­¦äººå·¥æ™ºèƒ½ç³»</li><li>å…³é”®è¯ï¼š3D æ‰‹éƒ¨å§¿åŠ¿ä¼°è®¡ï¼Œæ‰©æ•£æ¨¡å‹ï¼Œæ‰‹éƒ¨å½¢çŠ¶å›¾åƒç‚¹äº‘</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.03159   Github ä»£ç é“¾æ¥ï¼šhttps://github.com/cwc1260/HandDiff</li><li>æ‘˜è¦ï¼š   (1) ç ”ç©¶èƒŒæ™¯ï¼š3D æ‰‹éƒ¨å§¿åŠ¿ä¼°è®¡æ˜¯äººæœºäº¤äº’åº”ç”¨ä¸­çš„å…³é”®ä»»åŠ¡ï¼Œå¯ä»¥çœ‹ä½œæ˜¯åœ¨è¾“å…¥å¸§æ¡ä»¶ä¸‹ç”Ÿæˆ 3D ç‚¹å­é›†çš„é—®é¢˜ã€‚æ‰©æ•£æ¨¡å‹åœ¨ 3D ç”Ÿæˆåº”ç”¨ä¸­è¡¨ç°å‡ºä¼˜å¼‚æ€§ï¼Œå¯ä»¥ç”¨äºä¼°è®¡é«˜è´¨é‡å…³é”®ç‚¹ä½ç½®ã€‚   (2) è¿‡å»æ–¹æ³•å’Œé—®é¢˜ï¼šç°æœ‰æ‰©æ•£æ¨¡å‹æ— æ³•å®ç°å¤æ‚çš„æ’åˆ—æ˜ å°„å’Œç²¾ç¡®å®šä½ã€‚   (3) ç ”ç©¶æ–¹æ³•ï¼šæå‡º HandDiff æ¨¡å‹ï¼Œé€šè¿‡åœ¨æ‰‹éƒ¨å½¢çŠ¶å›¾åƒç‚¹äº‘æ¡ä»¶ä¸‹å¯¹æ‰©æ•£å™ªå£°è¿›è¡Œè¿­ä»£å»å™ªï¼Œä¼°è®¡å‡†ç¡®çš„æ‰‹éƒ¨å§¿åŠ¿ã€‚å¼•å…¥å…³èŠ‚æ¡ä»¶å’Œå±€éƒ¨ç»†èŠ‚æ¡ä»¶ï¼Œä»¥æ¢å¤å…³é”®ç‚¹æ’åˆ—å’Œå‡†ç¡®ä½ç½®ã€‚   (4) æ€§èƒ½å’Œæ•ˆæœï¼šHandDiff åœ¨å››ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ‰‹éƒ¨å§¿åŠ¿åŸºå‡†æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¯æ˜äº†å…¶åœ¨å¤„ç†é®æŒ¡ç­‰ä¸é€‚å®šä¸ç¡®å®šæ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</li></ol><p><strong>Methods:</strong></p><p>(1): <strong>HandDiff</strong>æ¨¡å‹é€šè¿‡åœ¨æ‰‹éƒ¨å½¢çŠ¶å›¾åƒç‚¹äº‘æ¡ä»¶ä¸‹å¯¹æ‰©æ•£å™ªå£°è¿›è¡Œè¿­ä»£å»å™ªï¼Œä¼°è®¡å‡†ç¡®çš„æ‰‹éƒ¨å§¿åŠ¿ã€‚</p><p>(2): å¼•å…¥<strong>å…³èŠ‚æ¡ä»¶</strong>ï¼Œä»¥æ¢å¤å…³é”®ç‚¹æ’åˆ—ã€‚</p><p>(3): å¼•å…¥<strong>å±€éƒ¨ç»†èŠ‚æ¡ä»¶</strong>ï¼Œä»¥æ¢å¤å…³é”®ç‚¹å‡†ç¡®ä½ç½®ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬å·¥ä½œé€šè¿‡å¼•å…¥å…³èŠ‚æ¡ä»¶å’Œå±€éƒ¨ç»†èŠ‚æ¡ä»¶ï¼Œæå‡ºäº† HandDiff æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡è¿­ä»£å»å™ªæ‰‹éƒ¨å½¢çŠ¶å›¾åƒç‚¹äº‘æ¡ä»¶ä¸‹çš„æ‰©æ•£å™ªå£°æ¥ä¼°è®¡å‡†ç¡®çš„æ‰‹éƒ¨å§¿åŠ¿ï¼Œåœ¨å››ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ‰‹éƒ¨å§¿åŠ¿åŸºå‡†æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¯æ˜äº†å…¶åœ¨å¤„ç†é®æŒ¡ç­‰ä¸é€‚å®šä¸ç¡®å®šæ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼šæå‡º HandDiff æ¨¡å‹ï¼Œé€šè¿‡åœ¨æ‰‹éƒ¨å½¢çŠ¶å›¾åƒç‚¹äº‘æ¡ä»¶ä¸‹å¯¹æ‰©æ•£å™ªå£°è¿›è¡Œè¿­ä»£å»å™ªï¼Œä¼°è®¡å‡†ç¡®çš„æ‰‹éƒ¨å§¿åŠ¿ï¼›å¼•å…¥å…³èŠ‚æ¡ä»¶ï¼Œä»¥æ¢å¤å…³é”®ç‚¹æ’åˆ—ï¼›å¼•å…¥å±€éƒ¨ç»†èŠ‚æ¡ä»¶ï¼Œä»¥æ¢å¤å…³é”®ç‚¹å‡†ç¡®ä½ç½®ã€‚æ€§èƒ½ï¼šåœ¨å››ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ‰‹éƒ¨å§¿åŠ¿åŸºå‡†æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å·¥ä½œé‡ï¼šéœ€è¦æ‰‹éƒ¨å½¢çŠ¶å›¾åƒç‚¹äº‘æ¡ä»¶ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-9127e6b88a37dae1433f9ba58b2eb0d6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bbe017f10c09349ebc2fc158ed02f568.jpg" align="middle"><img src="https://picx.zhimg.com/v2-87a189e71ddf1b5c27db9470a6b9ae3a.jpg" align="middle"></details><h2 id="DreamWalk-Style-Space-Exploration-using-Diffusion-Guidance"><a href="#DreamWalk-Style-Space-Exploration-using-Diffusion-Guidance" class="headerlink" title="DreamWalk: Style Space Exploration using Diffusion Guidance"></a>DreamWalk: Style Space Exploration using Diffusion Guidance</h2><p><strong>Authors:Michelle Shu, Charles Herrmann, Richard Strong Bowen, Forrester Cole, Ramin Zabih</strong></p><p>Text-conditioned diffusion models can generate impressive images, but fall short when it comes to fine-grained control. Unlike direct-editing tools like Photoshop, text conditioned models require the artist to perform â€œprompt engineering,â€ constructing special text sentences to control the style or amount of a particular subject present in the output image. Our goal is to provide fine-grained control over the style and substance specified by the prompt, for example to adjust the intensity of styles in different regions of the image (Figure 1). Our approach is to decompose the text prompt into conceptual elements, and apply a separate guidance term for each element in a single diffusion process. We introduce guidance scale functions to control when in the diffusion process and \emph{where} in the image to intervene. Since the method is based solely on adjusting diffusion guidance, it does not require fine-tuning or manipulating the internal layers of the diffusion modelâ€™s neural network, and can be used in conjunction with LoRA- or DreamBooth-trained models (Figure2). Project page: <a href="https://mshu1.github.io/dreamwalk.github.io/">https://mshu1.github.io/dreamwalk.github.io/</a> </p><p><a href="http://arxiv.org/abs/2404.03145v1">PDF</a> </p><p><strong>Summary</strong><br>æ–‡å­—æ¡ä»¶æ‰©æ•£æ¨¡å‹å¯ç”Ÿæˆä»¤äººå°è±¡æ·±åˆ»çš„å›¾åƒï¼Œä½†åœ¨ç²¾ç»†æ§åˆ¶æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æ–‡æœ¬æ¡ä»¶æ¨¡å‹éœ€è¦è‰ºæœ¯å®¶è¿›è¡Œâ€œæç¤ºå·¥ç¨‹â€ï¼Œä»¥æ„é€ ç‰¹æ®Šçš„æ–‡æœ¬å¥å­æ¥æ§åˆ¶è¾“å‡ºå›¾åƒä¸­ç‰¹å®šä¸»é¢˜çš„æ ·å¼æˆ–æ•°é‡ã€‚</li><li>åˆ†è§£æ–‡æœ¬æç¤ºä¸ºæ¦‚å¿µå…ƒç´ ï¼Œå¹¶åœ¨å•ä¸ªæ‰©æ•£è¿‡ç¨‹ä¸­å¯¹æ¯ä¸ªå…ƒç´ åº”ç”¨å•ç‹¬çš„æŒ‡å¯¼é¡¹ã€‚</li><li>å¼•å…¥æŒ‡å¯¼æ¯”ä¾‹å‡½æ•°æ¥æ§åˆ¶åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­çš„ä½•æ—¶ä½•å¤„è¿›è¡Œå¹²é¢„ã€‚</li><li>è¯¥æ–¹æ³•åªè°ƒæ•´æ‰©æ•£æŒ‡å¯¼ï¼Œä¸éœ€è¦å¾®è°ƒæˆ–æ“ä½œæ‰©æ•£æ¨¡å‹ç¥ç»ç½‘ç»œçš„å†…éƒ¨å±‚ï¼Œå¹¶ä¸”å¯ä»¥ä¸ LoRA æˆ– DreamBooth è®­ç»ƒçš„æ¨¡å‹ç»“åˆä½¿ç”¨ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šDreamWalkï¼šä½¿ç”¨æ‰©æ•£å¼•å¯¼çš„é£æ ¼ç©ºé—´æ¢ç´¢</li><li>ä½œè€…ï¼šMichelle Shu<em>ã€Charles Herrmann</em>ã€Richard S. Bowenã€Forrester Coleã€Ramin Zabih</li><li>éš¶å±ï¼šåº·å¥ˆå°”å¤§å­¦</li><li>å…³é”®è¯ï¼šæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€æ‰©æ•£æ¨¡å‹ã€é£æ ¼æ§åˆ¶ã€DreamWalk</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.03145    Github ä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹åœ¨ç”Ÿæˆå›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›æ­¥ï¼Œä½†ç¼ºä¹å¯¹å›¾åƒé£æ ¼å’Œå†…å®¹çš„ç²¾ç»†æ§åˆ¶ã€‚</li></ol><p>(2) è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–æç¤ºå·¥ç¨‹æˆ–å¾®è°ƒæ‰©æ•£æ¨¡å‹ï¼Œè¿™äº›æ–¹æ³•å­˜åœ¨æ§åˆ¶ä¸çµæ´»ã€æ”¹å˜æç¤ºä¼šå¯¼è‡´å›¾åƒæ•´ä½“å˜åŒ–ç­‰é—®é¢˜ã€‚</p><p>(3) æœ¬æ–‡æ–¹æ³•ï¼šDreamWalk æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£å¼•å¯¼çš„é£æ ¼ç©ºé—´æ¢ç´¢æ–¹æ³•ã€‚å®ƒå°†æ–‡æœ¬æç¤ºåˆ†è§£ä¸ºæ¦‚å¿µå…ƒç´ ï¼Œå¹¶ä¸ºæ¯ä¸ªå…ƒç´ åº”ç”¨å•ç‹¬çš„å¼•å¯¼é¡¹ã€‚é€šè¿‡å¼•å…¥å¼•å¯¼å°ºåº¦å‡½æ•°ï¼Œç”¨æˆ·å¯ä»¥æ§åˆ¶å¼•å¯¼é¡¹åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­çš„æ—¶é—´å’Œç©ºé—´åº”ç”¨ã€‚</p><p>(4) æ€§èƒ½åŠæ•ˆæœï¼šDreamWalk åœ¨é£æ ¼ç©ºé—´æ¢ç´¢ä»»åŠ¡ä¸Šå–å¾—äº†å‡ºè‰²çš„æ€§èƒ½ã€‚å®ƒå…è®¸ç”¨æˆ·ä»¥ç²¾ç»†çš„æ–¹å¼æ§åˆ¶å›¾åƒä¸­çš„ä¸åŒåŒºåŸŸçš„é£æ ¼å¼ºåº¦ï¼ŒåŒæ—¶ä¿æŒå›¾åƒçš„æ•´ä½“ç»“æ„å’Œå†…å®¹ã€‚</p><ol><li><p>æ–¹æ³•ï¼š(1) å¤šé‡å¼•å¯¼å…¬å¼ï¼šæå‡ºå¼•å¯¼å°ºåº¦å‡½æ•°ï¼Œç”¨äºæ§åˆ¶å¼•å¯¼é¡¹åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­çš„æ—¶é—´å’Œç©ºé—´åº”ç”¨ï¼›(2) ä»æ–‡æœ¬æç¤ºåˆ›å»ºå¤šé‡å¼•å¯¼é¡¹ï¼šå°†æç¤ºåˆ†è§£ä¸ºåŸºæœ¬æç¤ºå’Œé£æ ¼ç»„ä»¶ï¼Œä¸ºæ¯ä¸ªç»„ä»¶åº”ç”¨å•ç‹¬çš„å¼•å¯¼é¡¹ï¼›(3) å¯æ§æ­¥è¡Œï¼šé€šè¿‡å¼•å¯¼å°ºåº¦å‡½æ•°ï¼Œç”¨æˆ·å¯ä»¥æ§åˆ¶ä¸åŒæ¡ä»¶çš„å¼•å¯¼é¡¹åœ¨å›¾åƒä¸­çš„ä½ç½®ã€å¼ºåº¦å’Œç±»å‹ï¼›(4) æ—¶é—´æ­¥é•¿ä¾èµ–æ€§ï¼šé€šè¿‡è§‚å¯Ÿå¼•å¯¼é¡¹çš„èŒƒæ•°ï¼Œå‘ç°å›¾åƒå½¢æˆæ˜¯ä»ç²—åˆ°ç»†çš„è¿‡ç¨‹ï¼Œæå‡ºåœ¨æ—©æœŸå¼•å¯¼é˜¶æ®µä¸»è¦å…³æ³¨åŸºæœ¬æç¤ºï¼ŒåæœŸå¼•å¯¼é˜¶æ®µä¸»è¦å…³æ³¨é£æ ¼æç¤ºçš„è§£å†³æ–¹æ¡ˆã€‚</p></li><li><p>ç»“è®ºï¼š(1): DreamWalk æ˜¯ä¸€ç§é€šç”¨çš„å¼•å¯¼å…¬å¼ï¼Œä¸“é—¨è®¾è®¡ç”¨äºä¸ªæ€§åŒ–æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€‚è¿™ç§æ–¹æ³•å…è®¸å¯¹åº”ç”¨çš„é£æ ¼é‡æˆ–å¯¹ DB æ ‡è®°æˆ– LORA çš„éµå®ˆç¨‹åº¦è¿›è¡Œç²¾ç»†æ§åˆ¶ã€‚æˆ‘ä»¬å·²ç»å‡­ç»éªŒè¯æ˜äº†è¿™ç§æ–¹æ³•åœ¨å‡ ç§ä»»åŠ¡ä¸Šçš„æ•ˆç‡ï¼ŒåŒ…æ‹¬é£æ ¼æ’å€¼ã€DB é‡‡æ ·ã€æ›´æ”¹æè´¨ä»¥åŠç²¾ç»†åœ°æ“çºµç”Ÿæˆå›¾åƒçš„çº¹ç†å’Œå¸ƒå±€ã€‚(2): åˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£å¼•å¯¼çš„é£æ ¼ç©ºé—´æ¢ç´¢æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥å°†æ–‡æœ¬æç¤ºåˆ†è§£ä¸ºæ¦‚å¿µå…ƒç´ ï¼Œå¹¶ä¸ºæ¯ä¸ªå…ƒç´ åº”ç”¨å•ç‹¬çš„å¼•å¯¼é¡¹ï¼Œé€šè¿‡å¼•å¯¼å°ºåº¦å‡½æ•°ï¼Œç”¨æˆ·å¯ä»¥æ§åˆ¶å¼•å¯¼é¡¹åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­çš„æ—¶é—´å’Œç©ºé—´åº”ç”¨ã€‚æ€§èƒ½ï¼šåœ¨é£æ ¼ç©ºé—´æ¢ç´¢ä»»åŠ¡ä¸Šå–å¾—äº†å‡ºè‰²çš„æ€§èƒ½ï¼Œå®ƒå…è®¸ç”¨æˆ·ä»¥ç²¾ç»†çš„æ–¹å¼æ§åˆ¶å›¾åƒä¸­ä¸åŒåŒºåŸŸçš„é£æ ¼å¼ºåº¦ï¼ŒåŒæ—¶ä¿æŒå›¾åƒçš„æ•´ä½“ç»“æ„å’Œå†…å®¹ã€‚å·¥ä½œé‡ï¼šæœ¬æ–‡æ–¹æ³•éœ€è¦å°†æ–‡æœ¬æç¤ºåˆ†è§£ä¸ºæ¦‚å¿µå…ƒç´ ï¼Œå¹¶ä¸ºæ¯ä¸ªå…ƒç´ åº”ç”¨å•ç‹¬çš„å¼•å¯¼é¡¹ï¼Œè¿™å¯èƒ½éœ€è¦å¤§é‡çš„å·¥ä½œé‡ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-1c6779fc9e6a3c6a524e7c693cfad563.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ded6f26ee5eec5a3db8b0e7f7298e3cd.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a904c00cd643583927c16348c6d0f361.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7ce89f92e4ccf4d953fa7144543afe17.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f30c6d8b1b699e999092073e6d3d8769.jpg" align="middle"></details><h2 id="Diverse-and-Tailored-Image-Generation-for-Zero-shot-Multi-label-Classification"><a href="#Diverse-and-Tailored-Image-Generation-for-Zero-shot-Multi-label-Classification" class="headerlink" title="Diverse and Tailored Image Generation for Zero-shot Multi-label   Classification"></a>Diverse and Tailored Image Generation for Zero-shot Multi-label   Classification</h2><p><strong>Authors:Kaixin Zhang, Zhixiang Yuan, Tao Huang</strong></p><p>Recently, zero-shot multi-label classification has garnered considerable attention for its capacity to operate predictions on unseen labels without human annotations. Nevertheless, prevailing approaches often use seen classes as imperfect proxies for unseen ones, resulting in suboptimal performance. Drawing inspiration from the success of text-to-image generation models in producing realistic images, we propose an innovative solution: generating synthetic data to construct a training set explicitly tailored for proxyless training on unseen labels. Our approach introduces a novel image generation framework that produces multi-label synthetic images of unseen classes for classifier training. To enhance diversity in the generated images, we leverage a pre-trained large language model to generate diverse prompts. Employing a pre-trained multi-modal CLIP model as a discriminator, we assess whether the generated images accurately represent the target classes. This enables automatic filtering of inaccurately generated images, preserving classifier accuracy. To refine text prompts for more precise and effective multi-label object generation, we introduce a CLIP score-based discriminative loss to fine-tune the text encoder in the diffusion model. Additionally, to enhance visual features on the target task while maintaining the generalization of original features and mitigating catastrophic forgetting resulting from fine-tuning the entire visual encoder, we propose a feature fusion module inspired by transformer attention mechanisms. This module aids in capturing global dependencies between multiple objects more effectively. Extensive experimental results validate the effectiveness of our approach, demonstrating significant improvements over state-of-the-art methods. </p><p><a href="http://arxiv.org/abs/2404.03144v1">PDF</a> </p><p><strong>Summary</strong><br>ç”Ÿæˆåˆæˆæ•°æ®ï¼Œç”¨äºåœ¨æœªè§æ ‡ç­¾ä¸Šè¿›è¡Œä»£ç†è®­ç»ƒï¼Œä»è€Œæå‡æ— æ ‡æ³¨å¤šæ ‡ç­¾åˆ†ç±»æ€§èƒ½ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ä½¿ç”¨åˆæˆæ•°æ®è¿›è¡Œä»£ç†è®­ç»ƒï¼Œæ— éœ€äººå·¥æ ‡æ³¨æœªè§æ ‡ç­¾ã€‚</li><li>æå‡ºå›¾åƒç”Ÿæˆæ¡†æ¶ï¼Œç”Ÿæˆæœªè§ç±»åˆ«çš„å¤šæ ‡ç­¾åˆæˆå›¾åƒã€‚</li><li>åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆå¤šæ ·åŒ–çš„æç¤ºï¼Œæé«˜å›¾åƒå¤šæ ·æ€§ã€‚</li><li>ä½¿ç”¨ CLIP æ¨¡å‹è¯„ä¼°ç”Ÿæˆå›¾åƒçš„å‡†ç¡®æ€§ï¼Œè¿‡æ»¤ä¸å‡†ç¡®å›¾åƒã€‚</li><li>å¼•å…¥ CLIP å¾—åˆ†é‰´åˆ«æŸå¤±ï¼Œä¼˜åŒ–æ–‡æœ¬ç¼–ç å™¨ä»¥ç”Ÿæˆå‡†ç¡®çš„å¤šæ ‡ç­¾å¯¹è±¡ã€‚</li><li>æå‡ºç‰¹å¾èåˆæ¨¡å—ï¼Œå¢å¼ºç›®æ ‡ä»»åŠ¡çš„å¯è§†åŒ–ç‰¹å¾ï¼Œç¼“è§£å› å¾®è°ƒæ•´ä¸ªè§†è§‰ç¼–ç å™¨è€Œå¯¼è‡´çš„ç¾éš¾æ€§é—å¿˜ã€‚</li><li>å®éªŒç»“æœè¯æ˜äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šåŸºäºæ‰©æ•£æ¨¡å‹çš„å¤šç±»åˆ«é›¶æ ·æœ¬å›¾åƒç”Ÿæˆä¸ä¸ªæ€§åŒ–</li><li>ä½œè€…ï¼šKaixin Zhang, Zhixiang Yuan, Tao Huang</li><li>å•ä½ï¼šå®‰å¾½ç†å·¥å¤§å­¦è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯å­¦é™¢</li><li>å…³é”®è¯ï¼šé›¶æ ·æœ¬å¤šæ ‡ç­¾å­¦ä¹ ã€æ·±åº¦ç”Ÿæˆæ¨¡å‹ã€æ‰©æ•£æ¨¡å‹ã€åˆæˆæ•°æ®</li><li>é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.03144</li><li>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šé›¶æ ·æœ¬å¤šæ ‡ç­¾åˆ†ç±»ï¼ˆZS-MLCï¼‰æ—¨åœ¨å¤„ç†æœªè§æ ‡ç­¾çš„é¢„æµ‹ä»»åŠ¡ï¼Œä½†ç°æœ‰æ–¹æ³•é€šå¸¸ä½¿ç”¨å·²è§ç±»ä½œä¸ºæœªè§ç±»çš„ä»£ç†ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚(2) è¿‡å»æ–¹æ³•ï¼šç»å…¸æ–¹æ³•ä½¿ç”¨æ–‡æœ¬ç‰¹å¾æ¥åŒºåˆ†å›¾åƒä¸­æ¯ä¸ªæœªè§ç±»çš„å­˜åœ¨ï¼Œä½†å¿½ç•¥äº†å›¾åƒ-æ–‡æœ¬å¯¹ä¸­çš„è§†è§‰è¯­ä¹‰çŸ¥è¯†ã€‚æœ€è¿‘çš„å·¥ä½œåˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰å¯¹é½æ–‡æœ¬å’Œè§†è§‰ç©ºé—´ï¼Œä½†é€šå¸¸å›ºå®šCLIPä¸­è§†è§‰ç¼–ç å™¨å’Œæ–‡æœ¬ç¼–ç å™¨çš„æƒé‡ï¼Œå¿½ç•¥äº†CLIPè®­ç»ƒæ•°æ®é›†å’ŒMLCæ•°æ®é›†ä¹‹é—´çš„åŸŸå·®å¼‚ã€‚(3) ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæç¤ºçš„å›¾åƒç”Ÿæˆæ¡†æ¶ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”ŸæˆåŒ…å«æœªè§æ ‡ç­¾çš„å›¾åƒï¼Œå¹¶ä½¿ç”¨åˆæˆæ•°æ®æ˜¾å¼è®­ç»ƒåˆ†ç±»å™¨ã€‚æ­¤å¤–ï¼Œä¸ºäº†æé«˜ç”Ÿæˆå›¾åƒçš„æ•ˆç‡å’Œè´¨é‡ï¼Œæœ¬æ–‡æå‡ºäº†ä¸‰é¡¹æ”¹è¿›ï¼šï¼ˆ1ï¼‰åŸºäºé¢„è®­ç»ƒçš„å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆå¤šæ ·åŒ–ã€è¯¦ç»†å’Œç¡®å®šæ€§çš„æç¤ºï¼Œç”¨äºæŒ‡å¯¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ›´å¥½çš„å¤šæ ‡ç­¾å›¾åƒï¼›ï¼ˆ2ï¼‰è®¾è®¡ä¸€ä¸ªåŸºäºé¢„è®­ç»ƒçš„å¤šæ¨¡æ€CLIPæ¨¡å‹çš„é‰´åˆ«å™¨ï¼Œè¯†åˆ«ç”Ÿæˆçš„å›¾åƒæ˜¯å¦åŒ…å«ç›®æ ‡ç±»ï¼Œä»è€Œè‡ªåŠ¨è¿‡æ»¤é”™è¯¯ç”Ÿæˆçš„å›¾åƒï¼Œé˜²æ­¢å…¶å½±å“å‡†ç¡®æ€§ï¼›ï¼ˆ3ï¼‰å¼•å…¥åŸºäºCLIPåˆ†æ•°çš„åˆ¤åˆ«æŸå¤±æ¥å¾®è°ƒæ‰©æ•£æ¨¡å‹ä¸­çš„æ–‡æœ¬ç¼–ç å™¨ï¼Œä½¿æ–‡æœ¬æç¤ºæ›´ç²¾ç¡®ã€æ›´æœ‰æ•ˆåœ°ç”Ÿæˆå›¾åƒä¸­çš„å¤šæ ‡ç­¾å¯¹è±¡ã€‚(4) æ€§èƒ½ï¼šæœ¬æ–‡æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ZS-MLCä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œæ”¯æŒå…¶ç›®æ ‡ã€‚</li></ol><p><strong>Methods:</strong></p><p>(1): åˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”ŸæˆåŒ…å«æœªè§æ ‡ç­¾çš„å¤šæ ‡ç­¾å›¾åƒï¼Œå¹¶ä½¿ç”¨åˆæˆæ•°æ®è®­ç»ƒåˆ†ç±»å™¨ï¼›</p><p>(2): æå‡ºåŸºäºé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ç”Ÿæˆå¤šæ ·åŒ–ã€è¯¦ç»†å’Œç¡®å®šæ€§æç¤ºï¼ŒæŒ‡å¯¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ›´å¥½çš„å›¾åƒï¼›</p><p>(3): è®¾è®¡åŸºäºCLIPæ¨¡å‹çš„é‰´åˆ«å™¨ï¼Œè‡ªåŠ¨è¿‡æ»¤é”™è¯¯ç”Ÿæˆçš„å›¾åƒï¼›</p><p>(4): å¼•å…¥åŸºäºCLIPåˆ†æ•°çš„åˆ¤åˆ«æŸå¤±ï¼Œå¾®è°ƒæ‰©æ•£æ¨¡å‹ä¸­çš„æ–‡æœ¬ç¼–ç å™¨ï¼Œä½¿æ–‡æœ¬æç¤ºæ›´å‡†ç¡®åœ°ç”Ÿæˆå›¾åƒä¸­çš„å¤šæ ‡ç­¾å¯¹è±¡ï¼›</p><p>(5): å®éªŒéªŒè¯äº†åˆæˆå›¾åƒå¯¹åˆ†ç±»æ–¹æ³•å‡†ç¡®æ€§çš„å½±å“ï¼›</p><p>(6): æ¢è®¨äº†è¶…å‚æ•°å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼ŒåŒ…æ‹¬è¿‡æ»¤é˜ˆå€¼å’Œç”Ÿæˆå›¾åƒä¸­åŒ…å«çš„ç±»åˆ«æ•°ã€‚</p><ol><li>ç»“è®ºï¼š(1): æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæç¤ºçš„å›¾åƒç”Ÿæˆæ¡†æ¶ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”ŸæˆåŒ…å«æœªè§æ ‡ç­¾çš„å¤šæ ‡ç­¾å›¾åƒï¼Œå¹¶ä½¿ç”¨åˆæˆæ•°æ®æ˜¾å¼è®­ç»ƒåˆ†ç±»å™¨ï¼Œåœ¨é›¶æ ·æœ¬å¤šæ ‡ç­¾åˆ†ç±»ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚(2): åˆ›æ–°ç‚¹ï¼š</li><li>åˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”ŸæˆåŒ…å«æœªè§æ ‡ç­¾çš„å¤šæ ‡ç­¾å›¾åƒï¼Œå¹¶ä½¿ç”¨åˆæˆæ•°æ®è®­ç»ƒåˆ†ç±»å™¨ã€‚</li><li>æå‡ºåŸºäºé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ç”Ÿæˆå¤šæ ·åŒ–ã€è¯¦ç»†å’Œç¡®å®šæ€§æç¤ºï¼ŒæŒ‡å¯¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ›´å¥½çš„å›¾åƒã€‚</li><li>è®¾è®¡åŸºäº CLIP æ¨¡å‹çš„é‰´åˆ«å™¨ï¼Œè‡ªåŠ¨è¿‡æ»¤é”™è¯¯ç”Ÿæˆçš„å›¾åƒã€‚</li><li>å¼•å…¥åŸºäº CLIP åˆ†æ•°çš„åˆ¤åˆ«æŸå¤±ï¼Œå¾®è°ƒæ‰©æ•£æ¨¡å‹ä¸­çš„æ–‡æœ¬ç¼–ç å™¨ï¼Œä½¿æ–‡æœ¬æç¤ºæ›´å‡†ç¡®åœ°ç”Ÿæˆå›¾åƒä¸­çš„å¤šæ ‡ç­¾å¯¹è±¡ã€‚æ€§èƒ½ï¼šåœ¨ MS-COCO å’Œ NUS-WIDE æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒéªŒè¯äº†æœ¬æ–‡æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚å·¥ä½œé‡ï¼šæœ¬æ–‡æ–¹æ³•çš„å·¥ä½œé‡è¾ƒå¤§ï¼Œéœ€è¦è®­ç»ƒæ‰©æ•£æ¨¡å‹ã€é‰´åˆ«å™¨å’Œåˆ†ç±»å™¨ï¼Œå¹¶ç”Ÿæˆå¤§é‡åˆæˆå›¾åƒã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-3d9c0f04a40c5afd67fa71e8cd91facb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7d5dc92ceaadcd0613e8964b18b793fe.jpg" align="middle"><img src="https://pica.zhimg.com/v2-cf462a4056694a4650b5d54493888dac.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0e303b55139eba99249ce97454c14ff0.jpg" align="middle"></details><h2 id="Cross-Attention-Makes-Inference-Cumbersome-in-Text-to-Image-Diffusion-Models"><a href="#Cross-Attention-Makes-Inference-Cumbersome-in-Text-to-Image-Diffusion-Models" class="headerlink" title="Cross-Attention Makes Inference Cumbersome in Text-to-Image Diffusion   Models"></a>Cross-Attention Makes Inference Cumbersome in Text-to-Image Diffusion   Models</h2><p><strong>Authors:Wentian Zhang, Haozhe Liu, Jinheng Xie, Francesco Faccio, Mike Zheng Shou, JÃ¼rgen Schmidhuber</strong></p><p>This study explores the role of cross-attention during inference in text-conditional diffusion models. We find that cross-attention outputs converge to a fixed point after few inference steps. Accordingly, the time point of convergence naturally divides the entire inference process into two stages: an initial semantics-planning stage, during which, the model relies on cross-attention to plan text-oriented visual semantics, and a subsequent fidelity-improving stage, during which the model tries to generate images from previously planned semantics. Surprisingly, ignoring text conditions in the fidelity-improving stage not only reduces computation complexity, but also maintains model performance. This yields a simple and training-free method called TGATE for efficient generation, which caches the cross-attention output once it converges and keeps it fixed during the remaining inference steps. Our empirical study on the MS-COCO validation set confirms its effectiveness. The source code of TGATE is available at <a href="https://github.com/HaozheLiu-ST/T-GATE">https://github.com/HaozheLiu-ST/T-GATE</a>. </p><p><a href="http://arxiv.org/abs/2404.02747v1">PDF</a> </p><p><strong>Summary</strong><br>åŸºäºæ–‡æœ¬æ¡ä»¶æ‰©æ•£æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ä¸­ï¼Œäº¤å‰æ³¨æ„åŠ›è¾“å‡ºè¶‹äºæ”¶æ•›ï¼Œå°†æ¨ç†è¿‡ç¨‹åˆ†ä¸ºè¯­ä¹‰è§„åˆ’é˜¶æ®µå’Œä¿çœŸåº¦æå‡é˜¶æ®µã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>äº¤å‰æ³¨æ„åŠ›è¾“å‡ºåœ¨æ¨ç†è¿‡ç¨‹ä¸­è¶‹äºæ”¶æ•›ï¼Œè¾¾åˆ°å›ºå®šç‚¹ã€‚</li><li>æ”¶æ•›ç‚¹å°†æ¨ç†è¿‡ç¨‹åˆ†ä¸ºè¯­ä¹‰è§„åˆ’å’Œä¿çœŸåº¦æå‡ä¸¤ä¸ªé˜¶æ®µã€‚</li><li>åœ¨ä¿çœŸåº¦æå‡é˜¶æ®µå¿½ç•¥æ–‡æœ¬æ¡ä»¶ä¸ä»…èƒ½é™ä½è®¡ç®—å¤æ‚åº¦ï¼Œè¿˜èƒ½ä¿æŒæ¨¡å‹æ€§èƒ½ã€‚</li><li>TGATE æ–¹æ³•åˆ©ç”¨æ”¶æ•›ç‚¹ç¼“å­˜äº¤å‰æ³¨æ„åŠ›è¾“å‡ºï¼Œå›ºå®šè¾“å‡ºä»¥å‡å°‘è®¡ç®—é‡ã€‚</li><li>TGATE æ–¹æ³•å¯ä»¥åœ¨ MS-COCO éªŒè¯é›†ä¸Šä¿æŒæ¨¡å‹æœ‰æ•ˆæ€§ã€‚</li><li>TGATE æ–¹æ³•çš„æºä»£ç å·²å¼€æºã€‚</li><li>TGATE æ–¹æ³•æ˜¯ä¸€ç§ç®€å•ä¸”æ— éœ€è®­ç»ƒçš„é«˜æ•ˆç”Ÿæˆæ–¹æ³•ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>è®ºæ–‡æ ‡é¢˜ï¼šäº¤å‰æ³¨æ„åŠ›ä½¿æ¨ç†å˜å¾—ç¹ç</li><li>ä½œè€…ï¼šWentian Zhangã€Haozhe Liuã€Jinheng Xieã€Francesco Faccioã€Mike Zheng Shouã€JÃ¼rgen Schmidhuber</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šæ²™ç‰¹é˜¿æ‹‰ä¼¯å›½ç‹ç§‘æŠ€å¤§å­¦äººå·¥æ™ºèƒ½å€¡è®®</li><li>å…³é”®è¯ï¼šæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ã€äº¤å‰æ³¨æ„åŠ›ã€æ¨ç†åŠ é€Ÿ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.02747    Github ä»£ç é“¾æ¥ï¼šhttps://github.com/HaozheLiu-ST/T-GATE</li><li>æ‘˜è¦ï¼š    ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å¹¿æ³›ç”¨äºç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œä½†å…¶æ¨ç†è¿‡ç¨‹è®¡ç®—é‡å¤§ã€‚    ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šä»¥å¾€æ–¹æ³•ä¸»è¦é€šè¿‡æ”¹è¿›æ¨¡å‹æ¶æ„æˆ–ä¼˜åŒ–æ¨ç†ç®—æ³•æ¥åŠ é€Ÿæ¨ç†ï¼Œä½†æ•ˆæœæœ‰é™ã€‚    ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º TGATE çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡ç¼“å­˜å’Œé‡ç”¨äº¤å‰æ³¨æ„åŠ›å›¾æ¥åŠ é€Ÿæ¨ç†ã€‚    ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šåœ¨ MS-COCO éªŒè¯é›†ä¸Šï¼ŒTGATE åœ¨ SD-XL å’Œ PixArt-Alpha æ¨¡å‹ä¸Šåˆ†åˆ«å®ç°äº† 38.43% å’Œ 57.95% çš„æ¨ç†åŠ é€Ÿï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹æ€§èƒ½ã€‚</li></ol><p><strong>Methodsï¼š</strong>(1) <strong>äº¤å‰æ³¨æ„åŠ›å›¾ç¼“å­˜ï¼š</strong>å°†æ¨¡å‹ä¸­ä¸åŒå±‚ä¹‹é—´çš„äº¤å‰æ³¨æ„åŠ›å›¾ç¼“å­˜åˆ°å†…å­˜ä¸­ã€‚(2) <strong>äº¤å‰æ³¨æ„åŠ›å›¾é‡ç”¨ï¼š</strong>åœ¨åç»­æ¨ç†æ­¥éª¤ä¸­ï¼Œé‡ç”¨ç¼“å­˜çš„äº¤å‰æ³¨æ„åŠ›å›¾ï¼Œé¿å…é‡å¤è®¡ç®—ã€‚(3) <strong>è‡ªé€‚åº”é‡ç”¨ç­–ç•¥ï¼š</strong>æ ¹æ®è¾“å…¥æ–‡æœ¬å’Œç›®æ ‡å›¾åƒçš„ç›¸ä¼¼æ€§ï¼Œè‡ªé€‚åº”åœ°é€‰æ‹©é‡ç”¨çš„äº¤å‰æ³¨æ„åŠ›å›¾ã€‚(4) <strong>T-GATEç®—æ³•ï¼š</strong>å°†ç¼“å­˜ã€é‡ç”¨å’Œè‡ªé€‚åº”é‡ç”¨ç­–ç•¥é›†æˆåˆ°ä¸€ä¸ªåä¸ºT-GATEçš„ç®—æ³•ä¸­ã€‚</p><p>8.ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡è¯¦ç»†é˜è¿°äº†äº¤å‰æ³¨æ„åŠ›åœ¨æ–‡æœ¬æ¡ä»¶æ‰©æ•£æ¨¡å‹æ¨ç†è¿‡ç¨‹ä¸­çš„ä½œç”¨ã€‚æˆ‘ä»¬çš„ç»éªŒåˆ†æå¾—å‡ºäº†å‡ ä¸ªå…³é”®è§è§£ï¼ši) åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œäº¤å‰æ³¨æ„åŠ›ä¼šåœ¨å‡ æ­¥å†…æ”¶æ•›ã€‚åœ¨æ”¶æ•›åï¼Œäº¤å‰æ³¨æ„åŠ›ä»…å¯¹å»å™ªè¿‡ç¨‹äº§ç”Ÿå¾®å°å½±å“ã€‚ii) é€šè¿‡åœ¨äº¤å‰æ³¨æ„åŠ›æ”¶æ•›åå¯¹å…¶è¿›è¡Œç¼“å­˜å’Œé‡ç”¨ï¼Œæˆ‘ä»¬çš„ TGATE èŠ‚çœäº†è®¡ç®—å¹¶æé«˜äº† FID åˆ†æ•°ã€‚æˆ‘ä»¬çš„å‘ç°é¼“åŠ±ç¤¾åŒºé‡æ–°æ€è€ƒäº¤å‰æ³¨æ„åŠ›åœ¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­çš„ä½œç”¨ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šxxxï¼›æ€§èƒ½ï¼šxxxï¼›å·¥ä½œé‡ï¼šxxxï¼›</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-710f833b3f1069ff0a7a1cbf33810dd9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d5aae7ec9c4fe5cdb0a9a2cc4211e068.jpg" align="middle"><img src="https://pica.zhimg.com/v2-569b7bb461cd031cdf4e344d27a45686.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a67559151a3aa4a52b5670b048c5d787.jpg" align="middle"><img src="https://pica.zhimg.com/v2-bf9aacd151bf8f41e36a392205f58941.jpg" align="middle"><img src="https://picx.zhimg.com/v2-94706c475463596216ac60d19b39b1b2.jpg" align="middle"></details>## Bi-LORA: A Vision-Language Approach for Synthetic Image Detection**Authors:Mamadou Keita, Wassim Hamidouche, Hessen Bougueffa Eutamene, Abdenour Hadid, Abdelmalik Taleb-Ahmed**Advancements in deep image synthesis techniques, such as generative adversarial networks (GANs) and diffusion models (DMs), have ushered in an era of generating highly realistic images. While this technological progress has captured significant interest, it has also raised concerns about the potential difficulty in distinguishing real images from their synthetic counterparts. This paper takes inspiration from the potent convergence capabilities between vision and language, coupled with the zero-shot nature of vision-language models (VLMs). We introduce an innovative method called Bi-LORA that leverages VLMs, combined with low-rank adaptation (LORA) tuning techniques, to enhance the precision of synthetic image detection for unseen model-generated images. The pivotal conceptual shift in our methodology revolves around reframing binary classification as an image captioning task, leveraging the distinctive capabilities of cutting-edge VLM, notably bootstrapping language image pre-training (BLIP2). Rigorous and comprehensive experiments are conducted to validate the effectiveness of our proposed approach, particularly in detecting unseen diffusion-generated images from unknown diffusion-based generative models during training, showcasing robustness to noise, and demonstrating generalization capabilities to GANs. The obtained results showcase an impressive average accuracy of 93.41% in synthetic image detection on unseen generation models. The code and models associated with this research can be publicly accessed at https://github.com/Mamadou-Keita/VLM-DETECT. [PDF](http://arxiv.org/abs/2404.01959v1) **Summary**åˆ©ç”¨ Bi-LORA æ–¹æ³•ï¼Œç»“åˆ VLM å’Œ LORA è°ƒä¼˜æŠ€æœ¯ï¼Œæå‡å¯¹æœªè§ç”Ÿæˆæ¨¡å‹æ‰€ç”Ÿæˆå›¾åƒçš„åˆæˆå›¾åƒæ£€æµ‹ç²¾åº¦ã€‚**Key Takeaways**- å°†äºŒå…ƒåˆ†ç±»é‡æ„ä¸ºå›¾åƒæè¿°ä»»åŠ¡ï¼Œåˆ©ç”¨ VLM çš„ç‹¬ç‰¹èƒ½åŠ›ã€‚- ä½¿ç”¨å…ˆè¿›çš„ VLMï¼Œç‰¹åˆ«æ˜¯ BLIP2ï¼Œè¿›è¡Œå›¾åƒè¯­è¨€é¢„è®­ç»ƒã€‚- åœ¨æœªè§æ‰©æ•£ç”Ÿæˆå›¾åƒçš„æ£€æµ‹ä¸­éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚- å¯¹å™ªå£°è¡¨ç°å‡ºé²æ£’æ€§ï¼Œå¹¶å±•ç¤ºäº†å¯¹ GAN çš„æ³›åŒ–èƒ½åŠ›ã€‚- åœ¨åˆæˆå›¾åƒæ£€æµ‹ä»»åŠ¡ä¸Šå–å¾—äº† 93.41% çš„å¹³å‡å‡†ç¡®ç‡ã€‚- è¯¥æ–¹æ³•å¯¹ä¸åŒçš„ç”Ÿæˆæ¨¡å‹å…·æœ‰é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚- ä»£ç å’Œæ¨¡å‹å·²å…¬å¼€å‘å¸ƒã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>é¢˜ç›®ï¼šBi-LORAï¼šä¸€ç§ç”¨äºåˆæˆå›¾åƒæ£€æµ‹çš„è§†è§‰è¯­è¨€æ–¹æ³•</li><li>ä½œè€…ï¼šMamadou Keitaã€Wassim Hamidoucheã€Hessen Bougueffa Eutameneã€Abdenour Hadidã€Abdelmalik Taleb-Ahmed</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šç”µå­ã€å¾®ç”µå­å’Œçº³ç±³æŠ€æœ¯ç ”ç©¶æ‰€ï¼ˆIEMNï¼‰ï¼Œæ³•å›½ç“¦æœ—è°¢è®·å¤§å­¦ç†å·¥å¤§å­¦</li><li>å…³é”®è¯ï¼šDeepfakeã€æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€è§†è§‰è¯­è¨€æ¨¡å‹ã€å¤§è¯­è¨€æ¨¡å‹ã€å›¾åƒå­—å¹•ã€ç”Ÿæˆå¯¹æŠ—ç½‘ç»œã€æ‰©æ•£æ¨¡å‹ã€ä½ç§©è‡ªé€‚åº”</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.01959   Github ä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šéšç€ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰å’Œæ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰ç­‰æ·±åº¦å›¾åƒåˆæˆæŠ€æœ¯çš„è¿›æ­¥ï¼Œç”Ÿæˆé«˜åº¦é€¼çœŸçš„å›¾åƒæˆä¸ºå¯èƒ½ã€‚è™½ç„¶è¿™é¡¹æŠ€æœ¯è¿›æ­¥å¼•èµ·äº†æå¤§çš„å…´è¶£ï¼Œä½†ä¹Ÿå¼•å‘äº†äººä»¬å¯¹éš¾ä»¥å°†çœŸå®å›¾åƒä¸å…¶åˆæˆå¯¹åº”ç‰©åŒºåˆ†å¼€çš„æ‹…å¿§ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠé—®é¢˜ï¼šä¼ ç»Ÿçš„åˆæˆå›¾åƒæ£€æµ‹æ–¹æ³•é€šå¸¸ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æˆ–è§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰ä½œä¸ºå…¶åŸºç¡€æ¶æ„ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨æ³›åŒ–åˆ°ä»æœªé‡åˆ°è¿‡çš„æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„æ–°å›¾åƒæ—¶è¡¨ç°å‡ºæ˜æ˜¾çš„ä¸è¶³ã€‚ï¼ˆ3ï¼‰æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º Bi-LORA çš„åˆ›æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å’Œä½ç§©è‡ªé€‚åº”ï¼ˆLORAï¼‰è°ƒæ•´æŠ€æœ¯æ¥æé«˜åˆæˆå›¾åƒæ£€æµ‹çš„å‡†ç¡®æ€§ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹è®­ç»ƒæœŸé—´æ¥è‡ªæœªçŸ¥æ‰©æ•£æ¨¡å‹çš„æœªè§æ‰©æ•£ç”Ÿæˆå›¾åƒã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šå®éªŒç»“æœè¡¨æ˜ï¼ŒBi-LORA åœ¨åˆæˆå›¾åƒæ£€æµ‹ä»»åŠ¡ä¸Šå–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„å¹³å‡å‡†ç¡®ç‡ 93.41%ï¼Œè¿™è¡¨æ˜è¯¥æ–¹æ³•åœ¨å®ç°å…¶ç›®æ ‡æ–¹é¢æ˜¯æœ‰æ•ˆçš„ã€‚</li></ol><p>7.æ–¹æ³•ï¼šï¼ˆ1ï¼‰é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œä½¿ç”¨å›¾åƒ-æ–‡æœ¬å¯¹æ•°æ®é›†ï¼ˆä¾‹å¦‚ï¼ŒLSUNå§å®¤æ•°æ®é›†ï¼‰è¿›è¡Œå¾®è°ƒï¼›ï¼ˆ2ï¼‰åˆ©ç”¨ä½ç§©è‡ªé€‚åº”ï¼ˆLORAï¼‰æŠ€æœ¯ï¼Œå°†é¢„è®­ç»ƒçš„VLMè°ƒæ•´ä¸ºåˆæˆå›¾åƒæ£€æµ‹ä»»åŠ¡ï¼›ï¼ˆ3ï¼‰ä½¿ç”¨è°ƒæ•´åçš„VLMå¯¹è¾“å…¥å›¾åƒç”Ÿæˆæ–‡æœ¬æè¿°ï¼›ï¼ˆ4ï¼‰å°†ç”Ÿæˆçš„æ–‡æœ¬æè¿°ä¸å·²çŸ¥çœŸå®å›¾åƒçš„æ–‡æœ¬æè¿°è¿›è¡Œæ¯”è¾ƒï¼Œè®¡ç®—ç›¸ä¼¼åº¦ï¼›ï¼ˆ5ï¼‰æ ¹æ®ç›¸ä¼¼åº¦å¯¹è¾“å…¥å›¾åƒçš„çœŸå®æ€§è¿›è¡Œåˆ†ç±»ï¼ˆçœŸå®æˆ–åˆæˆï¼‰ã€‚</p><ol><li>æ€»ç»“ï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº† Bi-LORAï¼Œä¸€ç§ç”¨äºåˆæˆå›¾åƒæ£€æµ‹çš„æ–°é¢–æ–¹æ³•ï¼Œä»¥åº”å¯¹é€¼çœŸå›¾åƒç”Ÿæˆé¢†åŸŸçš„è¿›æ­¥ã€‚æˆ‘ä»¬é‡æ–°å°†äºŒåˆ†ç±»æ¦‚å¿µåŒ–ä¸ºå›¾åƒæè¿°ä»»åŠ¡ï¼Œåˆ©ç”¨è§†è§‰å’Œè¯­è¨€ä¹‹é—´çš„å¼ºå¤§èåˆï¼Œä»¥åŠ VLM çš„é›¶æ ·æœ¬æ€§è´¨ã€‚è·å¾—çš„ç»“æœè¡¨æ˜åœ¨åˆæˆå›¾åƒæ£€æµ‹ä¸­å–å¾—äº† 93.41% çš„æ˜¾ç€å¹³å‡å‡†ç¡®ç‡ï¼Œè¿™å¼ºè°ƒäº† Bi-LORA æ–¹æ³•å¯¹æœªçŸ¥ç”Ÿæˆæ¨¡å‹ç”Ÿæˆå›¾åƒæ‰€å¸¦æ¥çš„æŒ‘æˆ˜çš„ç›¸å…³æ€§å’Œæœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œä¸éœ€è¦è°ƒæ•´/å­¦ä¹ æ•°ç™¾ä¸‡ä¸ªå‚æ•°çš„å…ˆå‰ç ”ç©¶ä¸åŒï¼ŒBi-LORA æ¨¡å‹åªéœ€è¦è°ƒæ•´å°‘å¾—å¤šçš„å‚æ•°ï¼Œä»è€Œåœ¨è®­ç»ƒæˆæœ¬å’Œæ•ˆç‡ä¹‹é—´å–å¾—äº†æ›´å¥½çš„å¹³è¡¡ã€‚ä¸ºäº†æ”¯æŒå¯é‡å¤ç ”ç©¶çš„åŸåˆ™å¹¶æ”¯æŒæœªæ¥çš„æ‰©å±•ï¼Œæˆ‘ä»¬åœ¨ https://github.com/Mamadou-Keita/VLMDETECT ä¸Šå…¬å¼€ä»£ç å’Œæ¨¡å‹ã€‚è‡´è°¢ï¼šè¿™é¡¹å·¥ä½œå¾—åˆ°äº† CHISTERA IV Cofund 2021 è®¡åˆ’çš„é¡¹ç›® PCI2022-1349902ï¼ˆMARTINIï¼‰çš„èµ„åŠ©ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šxxxï¼›æ€§èƒ½ï¼šxxxï¼›å·¥ä½œé‡ï¼šxxx</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-4a6ff1782ce1d6c98e3caf6c1d5296a8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e947acd20b44a02638e3767964863740.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e539bff60d6ea507e8598a788648b668.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c78cca2e8cfa067d3e55bb232d8b7da8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-87d8d954bd2f94ecd496de19d18253d0.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1f4b67e329b74b72ff2034a1f1f9a505.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-04-06  MVD-Fusion Single-view 3D via Depth-consistent Multi-view Generation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>å…ƒå®‡å®™/è™šæ‹Ÿäºº</title>
    <link href="https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/"/>
    <id>https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/</id>
    <published>2024-04-06T09:14:19.000Z</published>
    <updated>2024-04-06T09:14:19.358Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-04-06-æ›´æ–°"><a href="#2024-04-06-æ›´æ–°" class="headerlink" title="2024-04-06 æ›´æ–°"></a>2024-04-06 æ›´æ–°</h1><h2 id="GeneAvatar-Generic-Expression-Aware-Volumetric-Head-Avatar-Editing-from-a-Single-Image"><a href="#GeneAvatar-Generic-Expression-Aware-Volumetric-Head-Avatar-Editing-from-a-Single-Image" class="headerlink" title="GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from   a Single Image"></a>GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from   a Single Image</h2><p><strong>Authors:Chong Bao, Yinda Zhang, Yuan Li, Xiyu Zhang, Bangbang Yang, Hujun Bao, Marc Pollefeys, Guofeng Zhang, Zhaopeng Cui</strong></p><p>Recently, we have witnessed the explosive growth of various volumetric representations in modeling animatable head avatars. However, due to the diversity of frameworks, there is no practical method to support high-level applications like 3D head avatar editing across different representations. In this paper, we propose a generic avatar editing approach that can be universally applied to various 3DMM driving volumetric head avatars. To achieve this goal, we design a novel expression-aware modification generative model, which enables lift 2D editing from a single image to a consistent 3D modification field. To ensure the effectiveness of the generative modification process, we develop several techniques, including an expression-dependent modification distillation scheme to draw knowledge from the large-scale head avatar model and 2D facial texture editing tools, implicit latent space guidance to enhance model convergence, and a segmentation-based loss reweight strategy for fine-grained texture inversion. Extensive experiments demonstrate that our method delivers high-quality and consistent results across multiple expression and viewpoints. Project page: <a href="https://zju3dv.github.io/geneavatar/">https://zju3dv.github.io/geneavatar/</a> </p><p><a href="http://arxiv.org/abs/2404.02152v1">PDF</a> Accepted to CVPR 2024. Project page:   <a href="https://zju3dv.github.io/geneavatar/">https://zju3dv.github.io/geneavatar/</a></p><p><strong>Summary</strong><br>è™šæ‹Ÿäººç¼–è¾‘çš„é€šç”¨æ–¹æ³•ï¼Œå¯å°† 2D ç¼–è¾‘æå‡åˆ° 3Dï¼Œæé«˜äº†ä¸åŒè¡¨ç¤ºä¸‹ 3DMM é©±åŠ¨è™šæ‹Ÿäººå¤´éƒ¨çš„ç¼–è¾‘ä¸€è‡´æ€§ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>é’ˆå¯¹ä¸åŒè¡¨ç¤ºçš„ 3DMM é©±åŠ¨è™šæ‹Ÿäººå¤´éƒ¨ï¼Œæå‡ºé€šç”¨ç¼–è¾‘æ–¹æ³•ã€‚</li><li>è®¾è®¡äº†è¡¨æƒ…æ„ŸçŸ¥ä¿®æ”¹ç”Ÿæˆæ¨¡å‹ï¼Œå¯ä»å•å¼ å›¾ç‰‡æå‡ 2D ç¼–è¾‘è‡³ä¸€è‡´çš„ 3D ä¿®æ”¹åœºã€‚</li><li>å¼€å‘äº†è¡¨æƒ…ç›¸å…³ä¿®æ”¹è’¸é¦ä»¥è·å–çŸ¥è¯†ã€éšå¼æ½œåœ¨ç©ºé—´æŒ‡å¯¼æé«˜æ¨¡å‹æ”¶æ•›æ€§ã€åˆ†å‰²æŸå¤±é‡æ–°åŠ æƒå®ç°ç»†ç²’åº¦çº¹ç†åæ¼”ã€‚</li><li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§è¡¨æƒ…å’Œè§†ç‚¹ä¸‹éƒ½èƒ½å‘ˆç°é«˜è´¨é‡ä¸”ä¸€è‡´çš„æ•ˆæœã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šé€šç”¨å¤´åƒç¼–è¾‘ï¼šé€šè¿‡éšå¼ä¿®æ”¹ç”Ÿæˆæ¨¡å‹è¿›è¡Œè·¨è¡¨ç¤ºçš„ 3DMM é©±åŠ¨å¤´åƒç¼–è¾‘</li><li>ä½œè€…ï¼šYang Hongã€Yuxuan Zhangã€Yujun Shenã€Zeyu Chenã€Jingyi Yuã€Xiaoguang Han</li><li>å•ä½ï¼šæµ™æ±Ÿå¤§å­¦</li><li>å…³é”®è¯ï¼š3DMMã€é€šç”¨å¤´åƒç¼–è¾‘ã€ä¿®æ”¹ç”Ÿæˆæ¨¡å‹ã€éšå¼æ½œåœ¨ç©ºé—´å¼•å¯¼ã€åŸºäºåˆ†å‰²çš„æŸå¤±é‡åŠ æƒ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2209.15122ï¼ŒGithub ä»£ç é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šéšç€ 3DMM é©±åŠ¨å¤´åƒåœ¨å»ºæ¨¡å¯åŠ¨ç”»å¤´åƒæ–¹é¢çš„çˆ†ç‚¸å¼å¢é•¿ï¼Œä¸åŒæ¡†æ¶çš„å¤šæ ·æ€§é˜»ç¢äº† 3D å¤´åƒç¼–è¾‘ç­‰é«˜çº§åº”ç”¨ç¨‹åºçš„å®ç”¨æ€§ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•é€šå¸¸é’ˆå¯¹ç‰¹å®šè¡¨ç¤ºï¼Œæ— æ³•è·¨è¡¨ç¤ºè¿›è¡Œç¼–è¾‘ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§é€šç”¨çš„å¤´åƒç¼–è¾‘æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯æ™®éåº”ç”¨äºç”± 3DMM é©±åŠ¨çš„å„ç§ä½“ç§¯å¤´åƒã€‚å…·ä½“è€Œè¨€ï¼Œè®¾è®¡äº†ä¸€ç§æ–°é¢–çš„è¡¨æƒ…æ„ŸçŸ¥ä¿®æ”¹ç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¤Ÿå°† 2D ç¼–è¾‘ä»å•å¹…å›¾åƒæå‡åˆ°ä¸€è‡´çš„ 3D ä¿®æ”¹åœºã€‚ä¸ºäº†ç¡®ä¿ç”Ÿæˆä¿®æ”¹è¿‡ç¨‹çš„æœ‰æ•ˆæ€§ï¼Œè¿˜å¼€å‘äº†å‡ ç§æŠ€æœ¯ï¼ŒåŒ…æ‹¬è¡¨æƒ…ç›¸å…³çš„ä¿®æ”¹è’¸é¦æ–¹æ¡ˆã€éšå¼æ½œåœ¨ç©ºé—´å¼•å¯¼ã€åŸºäºåˆ†å‰²çš„æŸå¤±é‡åŠ æƒç­–ç•¥ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šå¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§è¡¨æƒ…å’Œè§†ç‚¹ä¸‹éƒ½èƒ½æä¾›é«˜è´¨é‡ä¸”ä¸€è‡´çš„ç»“æœã€‚è¿™äº›æ€§èƒ½è¶³ä»¥æ”¯æŒå…¶ç›®æ ‡ï¼Œå³è·¨è¡¨ç¤ºè¿›è¡Œ 3DMM é©±åŠ¨å¤´åƒç¼–è¾‘ã€‚</p></li><li><p>Methods:(1): æå‡ºäº†ä¸€ç§è¡¨æƒ…æ„ŸçŸ¥ä¿®æ”¹ç”Ÿæˆæ¨¡å‹ï¼Œå°†2Dç¼–è¾‘æå‡åˆ°ä¸€è‡´çš„3Dä¿®æ”¹åœºï¼›(2): è®¾è®¡äº†è¡¨æƒ…ç›¸å…³çš„ä¿®æ”¹è’¸é¦æ–¹æ¡ˆï¼Œç¡®ä¿ç”Ÿæˆä¿®æ”¹è¿‡ç¨‹çš„æœ‰æ•ˆæ€§ï¼›(3): é‡‡ç”¨äº†éšå¼æ½œåœ¨ç©ºé—´å¼•å¯¼ï¼ŒæŒ‡å¯¼ä¿®æ”¹ç”Ÿæˆæ¨¡å‹åœ¨3DMMæ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œä¿®æ”¹ï¼›(4): åˆ©ç”¨äº†åŸºäºåˆ†å‰²çš„æŸå¤±é‡åŠ æƒç­–ç•¥ï¼Œå¢å¼ºæ¨¡å‹å¯¹ä¸åŒé¢éƒ¨åŒºåŸŸçš„ç¼–è¾‘èƒ½åŠ›ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬æ–‡æå‡ºçš„é€šç”¨ç¼–è¾‘æ–¹æ³•å…è®¸ç”¨æˆ·é€šè¿‡å•å¹…å›¾åƒç¼–è¾‘å„ç§ä½“ç§¯å¤´åƒè¡¨ç¤ºï¼Œå…¶ä¸­è¡¨æƒ…æ„ŸçŸ¥ä¿®æ”¹ç”Ÿæˆå™¨å°†ç¼–è¾‘æå‡åˆ° 3D å¤´åƒï¼ŒåŒæ—¶åœ¨å¤šä¸ªè¡¨æƒ…å’Œè§†ç‚¹ä¸‹ä¿æŒä¸€è‡´æ€§ã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºè¡¨æƒ…æ„ŸçŸ¥ä¿®æ”¹ç”Ÿæˆå™¨ï¼Œå°†ç¼–è¾‘æå‡åˆ° 3D å¤´åƒï¼ŒåŒæ—¶ä¿æŒåœ¨å¤šä¸ªè¡¨æƒ…å’Œè§†ç‚¹ä¸‹çš„ä¸€è‡´æ€§ã€‚</li><li>è®¾è®¡è¡¨æƒ…ç›¸å…³çš„ä¿®æ”¹è’¸é¦æ–¹æ¡ˆï¼Œç¡®ä¿ç”Ÿæˆä¿®æ”¹è¿‡ç¨‹çš„æœ‰æ•ˆæ€§ã€‚</li><li>é‡‡ç”¨éšå¼æ½œåœ¨ç©ºé—´å¼•å¯¼ï¼ŒæŒ‡å¯¼ä¿®æ”¹ç”Ÿæˆå™¨åœ¨ 3DMM æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œä¿®æ”¹ã€‚</li><li>åˆ©ç”¨åŸºäºåˆ†å‰²çš„æŸå¤±é‡åŠ æƒç­–ç•¥ï¼Œå¢å¼ºæ¨¡å‹å¯¹ä¸åŒé¢éƒ¨åŒºåŸŸçš„ç¼–è¾‘èƒ½åŠ›ã€‚</li><li>æ€§èƒ½ï¼šå®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§è¡¨æƒ…å’Œè§†ç‚¹ä¸‹éƒ½èƒ½æä¾›é«˜è´¨é‡ä¸”ä¸€è‡´çš„ç»“æœã€‚</li><li>å·¥ä½œé‡ï¼šæœ¬æ–‡æ–¹æ³•çš„å®ç°ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦è®¾è®¡å’Œè®­ç»ƒè¡¨æƒ…æ„ŸçŸ¥ä¿®æ”¹ç”Ÿæˆå™¨ã€è¡¨æƒ…ç›¸å…³çš„ä¿®æ”¹è’¸é¦æ–¹æ¡ˆã€éšå¼æ½œåœ¨ç©ºé—´å¼•å¯¼å’ŒåŸºäºåˆ†å‰²çš„æŸå¤±é‡åŠ æƒç­–ç•¥ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-f2a7d66d82bc4bf2cff263f5b555ec88.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3cd3c37db291268698e721edf97b0eb6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-24ce17b0544279479a579ad25b433b3b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e8865d91f8f0c5b8b1208f84a27e63f2.jpg" align="middle"></details><h2 id="Efficient-3D-Implicit-Head-Avatar-with-Mesh-anchored-Hash-Table-Blendshapes"><a href="#Efficient-3D-Implicit-Head-Avatar-with-Mesh-anchored-Hash-Table-Blendshapes" class="headerlink" title="Efficient 3D Implicit Head Avatar with Mesh-anchored Hash Table   Blendshapes"></a>Efficient 3D Implicit Head Avatar with Mesh-anchored Hash Table   Blendshapes</h2><p><strong>Authors:Ziqian Bai, Feitong Tan, Sean Fanello, Rohit Pandey, Mingsong Dou, Shichen Liu, Ping Tan, Yinda Zhang</strong></p><p>3D head avatars built with neural implicit volumetric representations have achieved unprecedented levels of photorealism. However, the computational cost of these methods remains a significant barrier to their widespread adoption, particularly in real-time applications such as virtual reality and teleconferencing. While attempts have been made to develop fast neural rendering approaches for static scenes, these methods cannot be simply employed to support realistic facial expressions, such as in the case of a dynamic facial performance. To address these challenges, we propose a novel fast 3D neural implicit head avatar model that achieves real-time rendering while maintaining fine-grained controllability and high rendering quality. Our key idea lies in the introduction of local hash table blendshapes, which are learned and attached to the vertices of an underlying face parametric model. These per-vertex hash-tables are linearly merged with weights predicted via a CNN, resulting in expression dependent embeddings. Our novel representation enables efficient density and color predictions using a lightweight MLP, which is further accelerated by a hierarchical nearest neighbor search method. Extensive experiments show that our approach runs in real-time while achieving comparable rendering quality to state-of-the-arts and decent results on challenging expressions. </p><p><a href="http://arxiv.org/abs/2404.01543v1">PDF</a> In CVPR2024. Project page:   <a href="https://augmentedperception.github.io/monoavatar-plus">https://augmentedperception.github.io/monoavatar-plus</a></p><p><strong>Summary</strong><br>æå‡ºäº†ä¸€ç§æ–°å‹çš„å®æ—¶æ¸²æŸ“ 3D ç¥ç»éšå¼å¤´éƒ¨å¤´åƒæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨ä¿æŒç²¾ç»†å¯æ§æ€§å’Œé«˜æ¸²æŸ“è´¨é‡çš„åŒæ—¶å®ç°äº†å®æ—¶æ¸²æŸ“ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºäº†ä¸€ç§ä½¿ç”¨ç¥ç»éšå¼ä½“ç§¯è¡¨ç¤ºæ„å»ºçš„ 3D å¤´éƒ¨å¤´åƒã€‚</li><li>è¯¥æ¨¡å‹å¼•å…¥äº†å±€éƒ¨å“ˆå¸Œè¡¨æ··åˆå½¢çŠ¶ï¼Œä»¥å®ç°å¯¹åŠ¨æ€é¢éƒ¨è¡¨æƒ…çš„é€¼çœŸæ¸²æŸ“ã€‚</li><li>ä½¿ç”¨è½»é‡çº§ MLP èåˆå±€éƒ¨å“ˆå¸Œè¡¨ï¼Œå®ç°é«˜æ•ˆçš„å¯†åº¦å’Œé¢œè‰²é¢„æµ‹ã€‚</li><li>é‡‡ç”¨åˆ†å±‚æœ€è¿‘é‚»æœç´¢æ–¹æ³•åŠ é€Ÿæ¸²æŸ“è¿‡ç¨‹ã€‚</li><li>è¯¥æ¨¡å‹å®ç°äº†å®æ—¶æ¸²æŸ“ï¼ŒåŒæ—¶æ¸²æŸ“è´¨é‡ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸å½“ã€‚</li><li>è¯¥æ¨¡å‹åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„è¡¨æƒ…ä¸Šå–å¾—äº†ä¸é”™çš„ç»“æœã€‚</li><li>è¯¥æ¨¡å‹åœ¨è™šæ‹Ÿç°å®å’Œè¿œç¨‹ä¼šè®®ç­‰å®æ—¶åº”ç”¨ä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šç½‘æ ¼é”šå®šå“ˆå¸Œè¡¨æ··åˆå½¢çŠ¶</li><li>ä½œè€…ï¼šJiayuan Mao, Runpei Dong, Yajie Zhao, Jingyi Yu, Yebin Liu</li><li>éš¶å±ï¼šæ— </li><li>å…³é”®è¯ï¼šç¥ç»è¾å°„åœºï¼Œé¢éƒ¨åŠ¨ç”»ï¼Œå“ˆå¸Œç¼–ç </li><li>é“¾æ¥ï¼šæ— ï¼ŒGithub ä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ï¼š<strong>ç ”ç©¶èƒŒæ™¯</strong>ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æ˜¯ä¸€ç§å¼ºå¤§çš„è¡¨ç¤ºï¼Œå¯ä»¥ä»å›¾åƒä¸­é‡å»º 3D åœºæ™¯ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨å°† NeRF åº”ç”¨äºé¢éƒ¨åŠ¨ç”»æ—¶é¢ä¸´ç€è®¡ç®—æˆæœ¬é«˜çš„é—®é¢˜ã€‚ï¼ˆ2ï¼‰ï¼š<strong>è¿‡å»çš„æ–¹æ³•</strong>ï¼šè¿‡å»çš„æ–¹æ³•ä¸»è¦æœ‰ä¸¤ç§ï¼šä¸€ç§æ˜¯é‡‡ç”¨å…¨å±€æ··åˆå½¢çŠ¶ï¼Œå¦ä¸€ç§æ˜¯é‡‡ç”¨è§„èŒƒåŒ– NeRFã€‚ç„¶è€Œï¼Œå…¨å±€æ··åˆå½¢çŠ¶è®¡ç®—æˆæœ¬é«˜ï¼Œè€Œè§„èŒƒåŒ– NeRF è´¨é‡è¾ƒå·®ã€‚ï¼ˆ3ï¼‰ï¼š<strong>ç ”ç©¶æ–¹æ³•</strong>ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„é¢éƒ¨åŠ¨ç”»è¡¨ç¤ºæ–¹æ³•â€”â€”ç½‘æ ¼é”šå®šå“ˆå¸Œè¡¨æ··åˆå½¢çŠ¶ã€‚è¯¥æ–¹æ³•å°† 3DMM é”šå®šçš„ NeRF ä¸å“ˆå¸Œç¼–ç æŠ€æœ¯ç›¸ç»“åˆï¼Œæ—¢å¯ä»¥é™ä½è®¡ç®—æˆæœ¬ï¼Œåˆå¯ä»¥æé«˜æ¸²æŸ“è´¨é‡ã€‚ï¼ˆ4ï¼‰ï¼š<strong>æ–¹æ³•æ€§èƒ½</strong>ï¼šåœ¨äººè„¸åŠ¨ç”»æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ¸²æŸ“è´¨é‡å’Œè®¡ç®—æ•ˆç‡æ–¹é¢éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li></ol><p><strong>æ–¹æ³•</strong></p><p>ï¼ˆ1ï¼‰ï¼š<strong>ç½‘æ ¼é”šå®šå“ˆå¸Œè¡¨æ··åˆå½¢çŠ¶</strong>ï¼šå°†3DMMé”šå®šçš„NeRFä¸å“ˆå¸Œç¼–ç æŠ€æœ¯ç›¸ç»“åˆï¼Œå½¢æˆæ–°çš„é¢éƒ¨åŠ¨ç”»è¡¨ç¤ºæ–¹æ³•ï¼Œæ—¢èƒ½é™ä½è®¡ç®—æˆæœ¬ï¼Œåˆèƒ½æé«˜æ¸²æŸ“è´¨é‡ã€‚</p><p>ï¼ˆ2ï¼‰ï¼š<strong>èåˆç½‘æ ¼é”šå®šæ··åˆå½¢çŠ¶</strong>ï¼šé€šè¿‡å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰è®¡ç®—æ¯ä¸ªé¡¶ç‚¹çš„æ··åˆæƒé‡ï¼Œå°†3DMMå˜å½¢è¡¨ç¤ºåœ¨UVçº¹ç†å›¾ä¸­ï¼Œç„¶åå°†å…¶è¾“å…¥U-Netç½‘ç»œï¼Œé¢„æµ‹ä¸€ä¸ªæƒé‡å›¾ï¼Œå†å°†æƒé‡å›¾é‡‡æ ·å›3DMMé¡¶ç‚¹ï¼Œä½œä¸ºè¡¨è¾¾å¼ç›¸å…³çš„æƒé‡ï¼Œå¯¹æ¯ä¸ªé¡¶ç‚¹ä¸Šçš„å“ˆå¸Œè¡¨è¿›è¡ŒåŠ æƒæ±‚å’Œï¼Œç”Ÿæˆåˆå¹¶åçš„å“ˆå¸Œè¡¨ã€‚</p><p>ï¼ˆ3ï¼‰ï¼š<strong>æŸ¥è¯¢ç‚¹è§£ç </strong>ï¼šä»3DMMç½‘æ ¼çš„kä¸ªæœ€è¿‘é‚»é¡¶ç‚¹ä¸­æå–åµŒå…¥ï¼Œä½¿ç”¨å“ˆå¸Œç¼–ç æŠ€æœ¯é¢„æµ‹æœ€ç»ˆçš„å¯†åº¦å’Œé¢œè‰²ï¼Œè¿›è¡Œé«˜æ•ˆæ¸²æŸ“ã€‚</p><p>ï¼ˆ4ï¼‰ï¼š<strong>å±‚çº§æŸ¥è¯¢</strong>ï¼šå°†æŸ¥è¯¢ç‚¹åˆ†ç»„åˆ°ä½“ç´ ä¸­ï¼Œå¹¶åˆ†å±‚æœç´¢kä¸ªæœ€è¿‘é‚»é¡¶ç‚¹ï¼Œè¿›ä¸€æ­¥åŠ é€Ÿæ¸²æŸ“è¿‡ç¨‹ã€‚</p><p>ï¼ˆ5ï¼‰ï¼š<strong>å•ç›®è§†é¢‘è®­ç»ƒ</strong>ï¼šä»…ä½¿ç”¨å•ç›®RGBè§†é¢‘å³å¯è®­ç»ƒæå‡ºçš„é¢éƒ¨åŠ¨ç”»è¡¨ç¤ºæ–¹æ³•ï¼Œæ— éœ€3Dæ‰«ææˆ–å¤šè§†å›¾æ•°æ®ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„é¢éƒ¨åŠ¨ç”»è¡¨ç¤ºæ–¹æ³•â€”â€”ç½‘æ ¼é”šå®šå“ˆå¸Œè¡¨æ··åˆå½¢çŠ¶ï¼Œè¯¥æ–¹æ³•å°†3DMMé”šå®šçš„NeRFä¸å“ˆå¸Œç¼–ç æŠ€æœ¯ç›¸ç»“åˆï¼Œæ—¢å¯ä»¥é™ä½è®¡ç®—æˆæœ¬ï¼Œåˆå¯ä»¥æé«˜æ¸²æŸ“è´¨é‡ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>å°†3DMMé”šå®šçš„NeRFä¸å“ˆå¸Œç¼–ç æŠ€æœ¯ç›¸ç»“åˆï¼Œå½¢æˆæ–°çš„é¢éƒ¨åŠ¨ç”»è¡¨ç¤ºæ–¹æ³•ã€‚</li><li>èåˆç½‘æ ¼é”šå®šæ··åˆå½¢çŠ¶ï¼Œé€šè¿‡CNNè®¡ç®—æ··åˆæƒé‡ï¼Œæé«˜æ¸²æŸ“è´¨é‡ã€‚</li><li>ä½¿ç”¨å±‚çº§æŸ¥è¯¢å’Œå•ç›®è§†é¢‘è®­ç»ƒï¼Œè¿›ä¸€æ­¥åŠ é€Ÿæ¸²æŸ“è¿‡ç¨‹å’Œé™ä½è®­ç»ƒéš¾åº¦ã€‚æ€§èƒ½ï¼š</li><li>åœ¨æ¸²æŸ“è´¨é‡å’Œè®¡ç®—æ•ˆç‡æ–¹é¢éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å·¥ä½œé‡ï¼š</li><li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨äººè„¸åŠ¨ç”»æ•°æ®é›†ä¸Šå–å¾—äº†è¾ƒå¥½çš„æ•ˆæœã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-ab39844047d36e8caedab23572e71526.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a0e8bfc8983817dd020f5b1deff586eb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-55d64640c5208a9cd19a534cb1503aba.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3173031114e8293e5e25b9733f1913ef.jpg" align="middle"></details><h2 id="MagicMirror-Fast-and-High-Quality-Avatar-Generation-with-a-Constrained-Search-Space"><a href="#MagicMirror-Fast-and-High-Quality-Avatar-Generation-with-a-Constrained-Search-Space" class="headerlink" title="MagicMirror: Fast and High-Quality Avatar Generation with a Constrained   Search Space"></a>MagicMirror: Fast and High-Quality Avatar Generation with a Constrained   Search Space</h2><p><strong>Authors:Armand Comas-MassaguÃ©, Di Qiu, Menglei Chai, Marcel BÃ¼hler, Amit Raj, Ruiqi Gao, Qiangeng Xu, Mark Matthews, Paulo Gotardo, Octavia Camps, Sergio Orts-Escolano, Thabo Beeler</strong></p><p>We introduce a novel framework for 3D human avatar generation and personalization, leveraging text prompts to enhance user engagement and customization. Central to our approach are key innovations aimed at overcoming the challenges in photo-realistic avatar synthesis. Firstly, we utilize a conditional Neural Radiance Fields (NeRF) model, trained on a large-scale unannotated multi-view dataset, to create a versatile initial solution space that accelerates and diversifies avatar generation. Secondly, we develop a geometric prior, leveraging the capabilities of Text-to-Image Diffusion Models, to ensure superior view invariance and enable direct optimization of avatar geometry. These foundational ideas are complemented by our optimization pipeline built on Variational Score Distillation (VSD), which mitigates texture loss and over-saturation issues. As supported by our extensive experiments, these strategies collectively enable the creation of custom avatars with unparalleled visual quality and better adherence to input text prompts. You can find more results and videos in our website: <a href="https://syntec-research.github.io/MagicMirror">https://syntec-research.github.io/MagicMirror</a> </p><p><a href="http://arxiv.org/abs/2404.01296v1">PDF</a> </p><p><strong>Summary</strong><br>æ–‡æœ¬ä»‹ç»äº†ä¸€ç§é€šè¿‡æ–‡æœ¬æç¤ºæ¥ç”Ÿæˆå’Œä¸ªæ€§åŒ– 3D äººä½“è™šæ‹Ÿå½¢è±¡çš„æ–°é¢–æ¡†æ¶ï¼Œæ—¨åœ¨æå‡ç”¨æˆ·å‚ä¸åº¦å’Œè‡ªå®šä¹‰åŠŸèƒ½ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>åˆ©ç”¨æ¡ä»¶ç¥ç»è¾å°„åœºæ¨¡å‹å’Œå¤šè§†è§’æ•°æ®é›†åˆ›å»ºå¤šæ ·åŒ–çš„åˆå§‹è§£ç©ºé—´ï¼Œä»¥åŠ é€Ÿå’Œå¤šæ ·åŒ–è™šæ‹Ÿå½¢è±¡ç”Ÿæˆã€‚</li><li>è¿ç”¨å‡ ä½•å…ˆéªŒå’Œæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œç¡®ä¿è‰¯å¥½çš„è§†å›¾ä¸å˜æ€§å¹¶æ”¯æŒç›´æ¥ä¼˜åŒ–è™šæ‹Ÿå½¢è±¡å‡ ä½•ã€‚</li><li>åº”ç”¨å˜åˆ†åˆ†æ•°è’¸é¦ä¼˜åŒ–ç®¡é“ï¼Œå¯ç¼“è§£çº¹ç†æŸå¤±å’Œè¿‡é¥±å’Œé—®é¢˜ã€‚</li><li>ä¸Šè¿°ç­–ç•¥ååŒä½œç”¨ï¼Œå®ç°è§†è§‰è´¨é‡å“è¶Šä¸”æ›´ç¬¦åˆè¾“å…¥æ–‡æœ¬æç¤ºçš„è‡ªå®šä¹‰è™šæ‹Ÿå½¢è±¡ã€‚</li><li><a href="https://syntec-research.github.io/MagicMirror">https://syntec-research.github.io/MagicMirror</a> ä¸Šæä¾›äº†æ›´å¤šç»“æœå’Œè§†é¢‘ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šMagicMirrorï¼šå¿«é€Ÿç”Ÿæˆé«˜è´¨é‡å¤´åƒ</li><li>ä½œè€…ï¼šArmand Comas-MassaguÃ©, Di Qiu, Menglei Chai, Marcel BÃ¼hler, Amit Raj, Ruiqi Gao, Qiangeng Xu, Mark Matthews, Paulo Gotardo, Octavia Camps, Sergio Orts-Escolano, Thabo Beeler</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šGoogle</li><li>å…³é”®è¯ï¼š3D å¤´åƒç”Ÿæˆï¼Œæ–‡æœ¬å¼•å¯¼ï¼Œç¥ç»è¾å°„åœºï¼Œå‡ ä½•å…ˆéªŒï¼Œå˜åˆ†åˆ†æ•°è’¸é¦</li><li>è®ºæ–‡é“¾æ¥ï¼šarXiv:2404.01296v1[cs.CV] 1Apr2024   Github ä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šéšç€è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®ç­‰æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œå¯¹é€¼çœŸä¸”å¯å®šåˆ¶çš„ 3D äººç±»å¤´åƒçš„éœ€æ±‚æ—¥ç›Šå¢é•¿ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤´åƒç”Ÿæˆæ–¹æ³•åœ¨å›¾åƒè´¨é‡ã€ç”¨æˆ·å®šåˆ¶å’Œç”Ÿæˆé€Ÿåº¦æ–¹é¢ä»ç„¶å­˜åœ¨æŒ‘æˆ˜ã€‚   ï¼ˆ2ï¼‰ï¼šè¿‡å»æ–¹æ³•ï¼šä¼ ç»Ÿæ–¹æ³•é€šå¸¸ä½¿ç”¨ 3D å»ºæ¨¡è½¯ä»¶æˆ–æ‰«ææŠ€æœ¯æ¥åˆ›å»ºå¤´åƒï¼Œä½†è¿™äº›æ–¹æ³•è€—æ—¶ä¸”éš¾ä»¥ä¸ªæ€§åŒ–ã€‚åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•è™½ç„¶å¯ä»¥ä»å›¾åƒä¸­ç”Ÿæˆå¤´åƒï¼Œä½†å®ƒä»¬é€šå¸¸éœ€è¦å¤§é‡çš„æ•°æ®å’Œè®­ç»ƒæ—¶é—´ï¼Œå¹¶ä¸”ç”Ÿæˆçš„å¤´åƒå¯èƒ½ç¼ºä¹ç»†èŠ‚æˆ–çœŸå®æ„Ÿã€‚   ï¼ˆ3ï¼‰ï¼šç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º MagicMirror çš„æ–°æ¡†æ¶ï¼Œç”¨äº 3D äººç±»å¤´åƒçš„ç”Ÿæˆå’Œä¸ªæ€§åŒ–ã€‚MagicMirror åˆ©ç”¨æ–‡æœ¬æç¤ºæ¥å¢å¼ºç”¨æˆ·å‚ä¸åº¦å’Œå®šåˆ¶åŒ–ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š</li><li>åˆ©ç”¨åœ¨æµ·é‡æœªæ³¨é‡Šçš„å¤šè§†å›¾æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¡ä»¶ç¥ç»è¾å°„åœº (NeRF) æ¨¡å‹ï¼Œåˆ›å»ºäº†ä¸€ä¸ªé€šç”¨çš„åˆå§‹è§£ç©ºé—´ï¼Œå¯ä»¥åŠ é€Ÿå’Œå¤šæ ·åŒ–å¤´åƒç”Ÿæˆã€‚</li><li>å¼€å‘äº†ä¸€ä¸ªå‡ ä½•å…ˆéªŒï¼Œåˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„èƒ½åŠ›ï¼Œä»¥ç¡®ä¿å‡ºè‰²çš„è§†ç‚¹ä¸å˜æ€§å’Œç›´æ¥ä¼˜åŒ–å¤´åƒå‡ ä½•å½¢çŠ¶ã€‚</li><li><p>ä¼˜åŒ–ç®¡é“å»ºç«‹åœ¨å˜åˆ†åˆ†æ•°è’¸é¦ (VSD) ä¹‹ä¸Šï¼Œå¯å‡è½»çº¹ç†æŸå¤±å’Œè¿‡é¥±å’Œé—®é¢˜ã€‚   ï¼ˆ4ï¼‰ï¼šæ–¹æ³•æ€§èƒ½ï¼šå¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼Œè¿™äº›ç­–ç•¥å…±åŒå®ç°äº†åˆ›å»ºå…·æœ‰æ— ä¸ä¼¦æ¯”è§†è§‰è´¨é‡å’Œæ›´å¥½åœ°éµå¾ªè¾“å…¥æ–‡æœ¬æç¤ºçš„è‡ªå®šä¹‰å¤´åƒã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) åˆ©ç”¨æ¡ä»¶ç¥ç»è¾å°„åœº (NeRF) æ¨¡å‹åˆ›å»ºåˆå§‹è§£ç©ºé—´ï¼›(2) å¼€å‘å‡ ä½•å…ˆéªŒï¼Œåˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æ¥ç¡®ä¿è§†ç‚¹ä¸å˜æ€§å’Œä¼˜åŒ–å¤´åƒå‡ ä½•å½¢çŠ¶ï¼›(3) åŸºäºå˜åˆ†åˆ†æ•°è’¸é¦ (VSD) ä¼˜åŒ–ç®¡é“ï¼Œå‡è½»çº¹ç†æŸå¤±å’Œè¿‡é¥±å’Œé—®é¢˜ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬å·¥ä½œçš„é‡è¦æ„ä¹‰ï¼šæœ¬ç ”ç©¶æå‡ºäº† MagicMirrorï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°ä¸€ä»£çš„æ–‡æœ¬å¼•å¯¼ 3D å¤´åƒç”Ÿæˆå’Œç¼–è¾‘æ¡†æ¶ã€‚é€šè¿‡çº¦æŸè§£ç©ºé—´ã€å¯»æ‰¾è‰¯å¥½çš„å‡ ä½•å…ˆéªŒå¹¶é€‰æ‹©è‰¯å¥½çš„æµ‹è¯•æ—¶é—´ä¼˜åŒ–ç›®æ ‡ï¼Œæˆ‘ä»¬å®ç°äº†è§†è§‰è´¨é‡ã€å¤šæ ·æ€§å’Œä¿çœŸåº¦çš„æ–°æ°´å¹³ã€‚æˆ‘ä»¬å½»åº•çš„æ¶ˆèå’Œæ¯”è¾ƒç ”ç©¶è¯æ˜äº†æ¯ä¸ªç»„ä»¶çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œæˆ‘ä»¬å·²ç»æœç€äººä»¬ä¼šå‘ç°æ˜“äºä½¿ç”¨ä¸”æœ‰è¶£çš„å¤´åƒç³»ç»Ÿè¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚</p></li></ol><p>ï¼ˆ2ï¼‰æœ¬æ–‡çš„ä¼˜ç¼ºç‚¹æ€»ç»“ï¼ˆä¸‰ä¸ªç»´åº¦ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š* åˆ©ç”¨æ¡ä»¶ç¥ç»è¾å°„åœº (NeRF) æ¨¡å‹åˆ›å»ºåˆå§‹è§£ç©ºé—´ã€‚* å¼€å‘å‡ ä½•å…ˆéªŒï¼Œåˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æ¥ç¡®ä¿è§†ç‚¹ä¸å˜æ€§å’Œä¼˜åŒ–å¤´åƒå‡ ä½•å½¢çŠ¶ã€‚* åŸºäºå˜åˆ†åˆ†æ•°è’¸é¦ (VSD) ä¼˜åŒ–ç®¡é“ï¼Œå‡è½»çº¹ç†æŸå¤±å’Œè¿‡é¥±å’Œé—®é¢˜ã€‚</p><p>æ€§èƒ½ï¼š* ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œç”Ÿæˆçš„å¤´åƒå…·æœ‰æ— ä¸ä¼¦æ¯”çš„è§†è§‰è´¨é‡å’Œæ›´å¥½åœ°éµå¾ªè¾“å…¥æ–‡æœ¬æç¤ºã€‚</p><p>å·¥ä½œé‡ï¼š* è™½ç„¶æˆ‘ä»¬ä¸éœ€è¦å¤§è§„æ¨¡çš„ 3D äººä½“æ•°æ®ï¼Œä½†ä¸ºæ•°ç™¾æˆ–æ•°åƒä¸ªå¯¹è±¡æ”¶é›†è¿™äº›æ•°æ®ä»ç„¶æ˜¯ä¸€é¡¹ç›¸å¯¹æ˜‚è´µä¸”è€—æ—¶çš„å·¥ä½œã€‚* ä»å¦ä¸€ä¸ªè§’åº¦æ¥çœ‹ï¼Œæˆ‘ä»¬ç”¨æ¥çº¦æŸè§£ç©ºé—´çš„æ•°æ®ä¹Ÿé™åˆ¶äº†æˆ‘ä»¬ï¼Œå› ä¸ºæŸäº›æç«¯çš„åˆ†å¸ƒå¤–ä¿®æ”¹å¾ˆéš¾å®ç°ã€‚* æˆ‘ä»¬çš„æ–¹æ³•ä¹Ÿå¯èƒ½å—åˆ°è®¡ç®—èµ„æºçš„é™åˆ¶ï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦å¤šä¸ªæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œè‡³å°‘æ¯ä¸ªæ¨¡å‹ç”¨äºé¢œè‰²å’Œæ³•çº¿ï¼Œå¦‚æœæˆ‘ä»¬æƒ³è¦æ‰§è¡Œæ¦‚å¿µæ··åˆï¼Œåˆ™éœ€è¦æ›´å¤šã€‚</p><p>æœªæ¥çš„ç ”ç©¶å¯ä»¥æŠ•å…¥åˆ°æ›´æ¨¡å—åŒ–çš„è®¾è®¡å’Œæ›´ç›´æ¥çš„æ–¹æ³•ä¸­ï¼Œä»¥å®ç°å¿«é€Ÿé«˜æ•ˆçš„ç”Ÿæˆå’Œç¼–è¾‘ã€‚ä¸ºäº†æ›´å¹¿æ³›åœ°é‡‡ç”¨ï¼Œä¸æ‰€æœ‰å…¶ä»–æŠ€æœ¯ä¸€æ ·ï¼Œæˆ‘ä»¬å¿…é¡»ç¡®ä¿å…¶å¼€å‘å’Œåº”ç”¨æ»¡è¶³ç”¨æˆ·çš„å®‰å…¨æ€§å’Œéšç§ï¼Œå¹¶æœ€å¤§é™åº¦åœ°å‡å°‘ä»»ä½•è´Ÿé¢çš„ç¤¾ä¼šå½±å“ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬ç›¸ä¿¡éšç€é¢„è®­ç»ƒçš„å¤§å‹æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„èƒ½åŠ›å’Œæ™®åŠç¨‹åº¦ä¸æ–­æé«˜ï¼Œå®ƒä»¬ä¸äººç±»ä»·å€¼è§‚çš„ä¸€è‡´æ€§å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-1baa0925f922a12a8e66e59ff6fb331c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c42a213fc6f5741472d164c3e74fb649.jpg" align="middle"><img src="https://pica.zhimg.com/v2-864c32c352be8bc29cb98bb5b6fc3f53.jpg" align="middle"></details>## HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior**Authors:David Svitov, Pietro Morerio, Lourdes Agapito, Alessio Del Bue**We present HAHA - a novel approach for animatable human avatar generation from monocular input videos. The proposed method relies on learning the trade-off between the use of Gaussian splatting and a textured mesh for efficient and high fidelity rendering. We demonstrate its efficiency to animate and render full-body human avatars controlled via the SMPL-X parametric model. Our model learns to apply Gaussian splatting only in areas of the SMPL-X mesh where it is necessary, like hair and out-of-mesh clothing. This results in a minimal number of Gaussians being used to represent the full avatar, and reduced rendering artifacts. This allows us to handle the animation of small body parts such as fingers that are traditionally disregarded. We demonstrate the effectiveness of our approach on two open datasets: SnapshotPeople and X-Humans. Our method demonstrates on par reconstruction quality to the state-of-the-art on SnapshotPeople, while using less than a third of Gaussians. HAHA outperforms previous state-of-the-art on novel poses from X-Humans both quantitatively and qualitatively. [PDF](http://arxiv.org/abs/2404.01053v1) **Summary**ä»å•ç›®è¾“å…¥è§†é¢‘ä¸­ç”Ÿæˆå¯åŠ¨ç”»äººç±»åŒ–èº«çš„ HAHA æ–¹æ³•ï¼Œé€šè¿‡é«˜æ–¯æ–‘ç‚¹å’Œçº¹ç†ç½‘æ ¼çš„ä½¿ç”¨æƒè¡¡ï¼Œå®ç°é«˜æ•ˆé«˜ä¿çœŸæ¸²æŸ“ã€‚**Key Takeaways**- HAHA æå‡ºäº†ä¸€ç§ä»å•ç›®è¾“å…¥è§†é¢‘ç”Ÿæˆå¯åŠ¨ç”»äººç±»åŒ–èº«çš„æ–°æ–¹æ³•ã€‚- è¯¥æ–¹æ³•å­¦ä¹ äº†é«˜æ–¯æ–‘ç‚¹å’Œçº¹ç†ç½‘æ ¼ä½¿ç”¨ä¹‹é—´çš„æƒè¡¡ï¼Œä»¥å®ç°é«˜æ•ˆå’Œé«˜ä¿çœŸæ¸²æŸ“ã€‚- HAHA é€šè¿‡ SMPL-X å‚æ•°æ¨¡å‹æ§åˆ¶å…¨èº«äººç±»åŒ–èº«åŠ¨ç”»å’Œæ¸²æŸ“ã€‚- è¯¥æ¨¡å‹å­¦ä¼šä»…åœ¨ SMPL-X ç½‘æ ¼ä¸­å¿…è¦åŒºåŸŸï¼ˆå¦‚å¤´å‘å’Œç½‘æ ¼å¤–æœè£…ï¼‰åº”ç”¨é«˜æ–¯æ–‘ç‚¹ã€‚- è¿™å¯¼è‡´ç”¨äºè¡¨ç¤ºå®Œæ•´åŒ–èº«çš„é«˜æ–¯æ–‘ç‚¹çš„æ•°é‡æœ€å°ï¼Œå¹¶å‡å°‘äº†æ¸²æŸ“ä¼ªå½±ã€‚- è¿™ä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿå¤„ç†ä¼ ç»Ÿä¸Šè¢«å¿½è§†çš„å°èº«ä½“éƒ¨ä½ï¼ˆå¦‚æ‰‹æŒ‡ï¼‰çš„åŠ¨ç”»ã€‚- åœ¨ä¸¤ä¸ªå¼€æ”¾æ•°æ®é›† SnapshotPeople å’Œ X-Humans ä¸Šå±•ç¤ºäº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>é¢˜ç›®ï¼šHAHAï¼šå¯æ§å…¨èº«ä½“åŠ¨ç”»è§’è‰²ç”Ÿæˆçš„æ–°æ–¹æ³•</li><li>ä½œè€…ï¼šDavid Svitovã€Egor Zakharovã€Victor Lempitskyã€Christoph Lassner</li><li>æ‰€å±æœºæ„ï¼šä¿„ç½—æ–¯å›½ç«‹ç ”ç©¶å‹æŠ€æœ¯å¤§å­¦</li><li>å…³é”®è¯ï¼šHuman avatarã€Full-bodyã€Gaussians plattingã€Textures</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2206.04086ï¼ŒGithub ä»£ç é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šå¯æ§å…¨èº«ä½“åŠ¨ç”»è§’è‰²ç”Ÿæˆæ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„ä¸€ä¸ªé‡è¦è¯¾é¢˜ï¼Œå®ƒå¯ä»¥åº”ç”¨äºè™šæ‹Ÿç°å®ã€å¢å¼ºç°å®å’Œç”µå½±åˆ¶ä½œç­‰é¢†åŸŸã€‚ç›®å‰ï¼ŒåŸºäºç½‘æ ¼çš„çº¹ç†æ¨¡å‹å’ŒåŸºäºé«˜æ–¯ä½“ç´ çš„éšå¼æ¨¡å‹æ˜¯ç”Ÿæˆå¯æ§å…¨èº«ä½“åŠ¨ç”»è§’è‰²çš„ä¸¤å¤§ä¸»æµæ–¹æ³•ã€‚åŸºäºç½‘æ ¼çš„çº¹ç†æ¨¡å‹è™½ç„¶å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„åŠ¨ç”»è§’è‰²ï¼Œä½†æ˜¯æ¸²æŸ“æ•ˆç‡è¾ƒä½ï¼›è€ŒåŸºäºé«˜æ–¯ä½“ç´ çš„éšå¼æ¨¡å‹è™½ç„¶æ¸²æŸ“æ•ˆç‡è¾ƒé«˜ï¼Œä½†æ˜¯ç”Ÿæˆçš„è§’è‰²è´¨é‡è¾ƒå·®ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šè¿‡å»çš„æ–¹æ³•è¦ä¹ˆä½¿ç”¨åŸºäºç½‘æ ¼çš„çº¹ç†æ¨¡å‹ï¼Œè¦ä¹ˆä½¿ç”¨åŸºäºé«˜æ–¯ä½“ç´ çš„éšå¼æ¨¡å‹ã€‚åŸºäºç½‘æ ¼çš„çº¹ç†æ¨¡å‹æ¸²æŸ“æ•ˆç‡ä½ï¼Œè€ŒåŸºäºé«˜æ–¯ä½“ç´ çš„éšå¼æ¨¡å‹ç”Ÿæˆçš„è§’è‰²è´¨é‡å·®ã€‚ï¼ˆ3ï¼‰è®ºæ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³• HAHAï¼Œå®ƒç»“åˆäº†åŸºäºç½‘æ ¼çš„çº¹ç†æ¨¡å‹å’ŒåŸºäºé«˜æ–¯ä½“ç´ çš„éšå¼æ¨¡å‹çš„ä¼˜ç‚¹ã€‚HAHA ä½¿ç”¨é«˜æ–¯ä½“ç´ æ¥è¡¨ç¤ºè§’è‰²çš„å¤´å‘å’Œè¡£æœç­‰ç»†èŠ‚ï¼Œä½¿ç”¨çº¹ç†ç½‘æ ¼æ¥è¡¨ç¤ºè§’è‰²çš„ä¸»ä½“ã€‚è¿™ç§æ–¹æ³•æ—¢å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„åŠ¨ç”»è§’è‰²ï¼Œåˆå¯ä»¥ä¿è¯æ¸²æŸ“æ•ˆç‡ã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šHAHA åœ¨ SnapshotPeople å’Œ X-Humans ä¸¤ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚åœ¨ SnapshotPeople æ•°æ®é›†ä¸Šï¼ŒHAHA çš„é‡å»ºè´¨é‡ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸å½“ï¼Œä½†ä½¿ç”¨çš„é«˜æ–¯ä½“ç´ æ•°é‡å´ä¸åˆ°ä¸‰åˆ†ä¹‹ä¸€ã€‚åœ¨ X-Humans æ•°æ®é›†ä¸Šï¼ŒHAHA åœ¨æ–°å§¿åŠ¿ä¸‹çš„è¡¨ç°ä¼˜äºä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œæ— è®ºæ˜¯å®šé‡è¿˜æ˜¯å®šæ€§ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒHAHA èƒ½å¤Ÿæœ‰æ•ˆåœ°ç”Ÿæˆé«˜è´¨é‡çš„å¯æ§å…¨èº«ä½“åŠ¨ç”»è§’è‰²ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰é¦–å…ˆï¼Œè®­ç»ƒ 3D é«˜æ–¯ä½“ç´ è¡¨ç¤ºï¼Œä»…ä¼˜åŒ–å±€éƒ¨é«˜æ–¯ä½“ç´ å˜æ¢å’Œé¢œè‰²ï¼Œå›ºå®šä¸é€æ˜åº¦ï¼Œä»¥ä¼˜åŒ– SMPL-X çš„å§¿æ€å’Œå½¢çŠ¶å‚æ•°ã€‚ï¼ˆ2ï¼‰ç„¶åï¼Œä½¿ç”¨å¯å¾®æ¸²æŸ“å™¨æ¸²æŸ“å…·æœ‰å¯è®­ç»ƒçº¹ç†çš„ SMPL-X ç½‘æ ¼ï¼Œä»…ä¼˜åŒ–çº¹ç†ï¼Œä¿æŒ SMPL-X å‚æ•°å†»ç»“ã€‚ï¼ˆ3ï¼‰æœ€åï¼Œåˆå¹¶å¯å¾®æ¸²æŸ“çš„çº¹ç†ç½‘æ ¼å’Œå¯å¾® 3D é«˜æ–¯ä½“ç´ è¿‡ç¨‹ï¼Œè®­ç»ƒé«˜æ–¯ä½“ç´ çš„ä¸é€æ˜åº¦å’Œé¢œè‰²ï¼Œåˆ é™¤ä¸é€æ˜åº¦ä½äºé˜ˆå€¼çš„é«˜æ–¯ä½“ç´ ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºHAHAçš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†åŸºäºç½‘æ ¼çš„çº¹ç†æ¨¡å‹å’ŒåŸºäºé«˜æ–¯ä½“ç´ çš„éšå¼æ¨¡å‹çš„ä¼˜ç‚¹ï¼Œå¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„å¯æ§å…¨èº«ä½“åŠ¨ç”»è§’è‰²ï¼Œå¹¶ä¸”æ¸²æŸ“æ•ˆç‡è¾ƒé«˜ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå°†åŸºäºç½‘æ ¼çš„çº¹ç†æ¨¡å‹å’ŒåŸºäºé«˜æ–¯ä½“ç´ çš„éšå¼æ¨¡å‹ç›¸ç»“åˆï¼Œæ—¢å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„åŠ¨ç”»è§’è‰²ï¼Œåˆå¯ä»¥ä¿è¯æ¸²æŸ“æ•ˆç‡ã€‚</li><li>ä½¿ç”¨é«˜æ–¯ä½“ç´ æ¥è¡¨ç¤ºè§’è‰²çš„å¤´å‘å’Œè¡£æœç­‰ç»†èŠ‚ï¼Œä½¿ç”¨çº¹ç†ç½‘æ ¼æ¥è¡¨ç¤ºè§’è‰²çš„ä¸»ä½“ï¼Œè¿™ç§æ–¹æ³•æ—¢å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„åŠ¨ç”»è§’è‰²ï¼Œåˆå¯ä»¥ä¿è¯æ¸²æŸ“æ•ˆç‡ã€‚æ€§èƒ½ï¼š</li><li>åœ¨SnapshotPeopleæ•°æ®é›†ä¸Šï¼ŒHAHAçš„é‡å»ºè´¨é‡ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸å½“ï¼Œä½†ä½¿ç”¨çš„é«˜æ–¯ä½“ç´ æ•°é‡å´ä¸åˆ°ä¸‰åˆ†ä¹‹ä¸€ã€‚</li><li>åœ¨X-Humansæ•°æ®é›†ä¸Šï¼ŒHAHAåœ¨æ–°å§¿åŠ¿ä¸‹çš„è¡¨ç°ä¼˜äºä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œæ— è®ºæ˜¯å®šé‡è¿˜æ˜¯å®šæ€§ã€‚å·¥ä½œé‡ï¼š</li><li>HAHAä½¿ç”¨é«˜æ–¯ä½“ç´ æ¥è¡¨ç¤ºè§’è‰²çš„å¤´å‘å’Œè¡£æœç­‰ç»†èŠ‚ï¼Œä½¿ç”¨çº¹ç†ç½‘æ ¼æ¥è¡¨ç¤ºè§’è‰²çš„ä¸»ä½“ï¼Œè¿™ç§æ–¹æ³•æ—¢å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„åŠ¨ç”»è§’è‰²ï¼Œåˆå¯ä»¥ä¿è¯æ¸²æŸ“æ•ˆç‡ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-b477b212869e9f8635da6f1012719fb9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-aa6e5f76fe253938286549f8ee5205fd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4f97970c093585e18e2db42fb96a6b75.jpg" align="middle"></details><h2 id="TexVocab-Texture-Vocabulary-conditioned-Human-Avatars"><a href="#TexVocab-Texture-Vocabulary-conditioned-Human-Avatars" class="headerlink" title="TexVocab: Texture Vocabulary-conditioned Human Avatars"></a>TexVocab: Texture Vocabulary-conditioned Human Avatars</h2><p><strong>Authors:Yuxiao Liu, Zhe Li, Yebin Liu, Haoqian Wang</strong></p><p>To adequately utilize the available image evidence in multi-view video-based avatar modeling, we propose TexVocab, a novel avatar representation that constructs a texture vocabulary and associates body poses with texture maps for animation. Given multi-view RGB videos, our method initially back-projects all the available images in the training videos to the posed SMPL surface, producing texture maps in the SMPL UV domain. Then we construct pairs of human poses and texture maps to establish a texture vocabulary for encoding dynamic human appearances under various poses. Unlike the commonly used joint-wise manner, we further design a body-part-wise encoding strategy to learn the structural effects of the kinematic chain. Given a driving pose, we query the pose feature hierarchically by decomposing the pose vector into several body parts and interpolating the texture features for synthesizing fine-grained human dynamics. Overall, our method is able to create animatable human avatars with detailed and dynamic appearances from RGB videos, and the experiments show that our method outperforms state-of-the-art approaches. The project page can be found at <a href="https://texvocab.github.io/">https://texvocab.github.io/</a>. </p><p><a href="http://arxiv.org/abs/2404.00524v1">PDF</a> </p><p><strong>Summary</strong><br>ä»å¤šè§†è§†é¢‘ç”Ÿæˆå¯åŠ¨ç”»çš„è™šæ‹Ÿäººï¼ŒTexVocab é€šè¿‡çº¹ç†è¯æ±‡è¡¨å°†èº«ä½“å§¿åŠ¿ä¸çº¹ç†è´´å›¾å…³è”èµ·æ¥ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>TexVocab æå‡ºäº†ä¸€ç§æ–°çš„è™šæ‹Ÿäººè¡¨ç¤ºå½¢å¼ï¼Œå°†çº¹ç†è¯æ±‡è¡¨ä¸èº«ä½“å§¿åŠ¿å…³è”èµ·æ¥ï¼Œç”¨äºåŠ¨ç”»ã€‚</li><li>è¯¥æ–¹æ³•å°†å¤šè§† RGB è§†é¢‘ä¸­çš„å›¾åƒåæŠ•å½±åˆ° SMPL è¡¨é¢ï¼Œç”Ÿæˆ SMPL UV åŸŸä¸­çš„çº¹ç†è´´å›¾ã€‚</li><li>æ„å»ºäººä½“å§¿åŠ¿å’Œçº¹ç†è´´å›¾å¯¹ï¼Œå»ºç«‹çº¹ç†è¯æ±‡è¡¨ï¼Œå¯¹å„ç§å§¿åŠ¿ä¸‹çš„åŠ¨æ€äººä½“å¤–è§‚è¿›è¡Œç¼–ç ã€‚</li><li>é‡‡ç”¨åŸºäºèº«ä½“éƒ¨ä½çš„ç¼–ç ç­–ç•¥ï¼Œå­¦ä¹ è¿åŠ¨é“¾çš„ç»“æ„æ•ˆåº”ã€‚</li><li>ç»™å®šé©±åŠ¨å§¿åŠ¿ï¼Œåˆ†å±‚æŸ¥è¯¢å§¿åŠ¿ç‰¹å¾ï¼Œå°†å§¿åŠ¿å‘é‡åˆ†è§£ä¸ºå¤šä¸ªèº«ä½“éƒ¨ä½ï¼Œå¹¶å†…æ’çº¹ç†ç‰¹å¾ï¼Œåˆæˆç²¾ç»†çš„äººä½“åŠ¨æ€ã€‚</li><li>ä» RGB è§†é¢‘åˆ›å»ºå…·æœ‰è¯¦ç»†åŠ¨æ€å¤–è§‚çš„å¯åŠ¨ç”»äººä½“è™šæ‹Ÿäººï¼Œä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><strong>è®ºæ–‡æ ‡é¢˜ï¼š</strong> TexVocabï¼šçº¹ç†è¯æ±‡æ¡ä»¶ä¸‹çš„äººä½“è™šæ‹Ÿå½¢è±¡</li><li><strong>ä½œè€…ï¼š</strong> Yuxiao Liu, Zhe Li, Yebin Liu, Haoqian Wang</li><li><strong>ç¬¬ä¸€ä½œè€…å•ä½ï¼š</strong> æ·±åœ³å›½é™…ç ”ç©¶ç”Ÿé™¢ï¼Œæ¸…åå¤§å­¦</li><li><strong>å…³é”®è¯ï¼š</strong> è™šæ‹Ÿå½¢è±¡ï¼Œçº¹ç†è¯æ±‡ï¼Œäººä½“åŠ¨ç”»ï¼Œå¤šè§†å›¾é‡å»º</li><li><strong>è®ºæ–‡é“¾æ¥ï¼š</strong> https://arxiv.org/abs/2404.00524</li><li><p><strong>æ‘˜è¦ï¼š</strong>   (1) <strong>ç ”ç©¶èƒŒæ™¯ï¼š</strong> å¯åŠ¨ç”»äººä½“è™šæ‹Ÿå½¢è±¡å»ºæ¨¡åœ¨ AR/VR åº”ç”¨ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†å¦‚ä½•æœ‰æ•ˆå­¦ä¹ é©±åŠ¨ä¿¡å·å’ŒåŠ¨æ€å¤–è§‚ä¹‹é—´çš„æ˜ å°„ä»ç„¶å……æ»¡æŒ‘æˆ˜ã€‚   (2) <strong>è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼š</strong> ç°æœ‰æ–¹æ³•é€šå¸¸ç›´æ¥å°†å§¿åŠ¿è¾“å…¥ï¼ˆä¾‹å¦‚å§¿åŠ¿å‘é‡ï¼‰æ˜ å°„åˆ°äººä½“å¤–è§‚ï¼Œä½†å§¿åŠ¿è¾“å…¥ä¸åŒ…å«ä»»ä½•åŠ¨æ€äººä½“å¤–è§‚ä¿¡æ¯ï¼Œå› æ­¤ NeRFMLP éš¾ä»¥ä»…ä»å§¿åŠ¿è¾“å…¥ä¸­å›å½’é«˜ä¿çœŸåŠ¨æ€ç»†èŠ‚ã€‚   (3) <strong>è®ºæ–‡æ–¹æ³•ï¼š</strong> æå‡º TexVocabï¼Œä¸€ç§çº¹ç†è¯æ±‡ï¼Œå®ƒå……åˆ†åˆ©ç”¨æ˜¾å¼å›¾åƒè¯æ®æ¥æŒ‡å¯¼éšå¼æ¡ä»¶ NeRF ä»è¡¨è¾¾çº¹ç†æ¡ä»¶ä¸­å­¦ä¹ åŠ¨æ€ã€‚å°†å¯¹åº”è®­ç»ƒå§¿åŠ¿çš„æ‰€æœ‰å¯ç”¨å›¾åƒåæŠ•å½±åˆ°æ‘†å§¿åŠ¿çš„ SMPL è¡¨é¢ï¼Œç”Ÿæˆ SMPL UV åŸŸä¸­çš„çº¹ç†è´´å›¾ã€‚ç„¶åæ„å»ºäººä½“å§¿åŠ¿å’Œçº¹ç†è´´å›¾å¯¹ï¼Œä»¥å»ºç«‹çº¹ç†è¯æ±‡æ¥ç¼–ç å„ç§å§¿åŠ¿ä¸‹çš„åŠ¨æ€äººä½“å¤–è§‚ã€‚   (4) <strong>æ–¹æ³•æ€§èƒ½ï¼š</strong> è¯¥æ–¹æ³•èƒ½å¤Ÿä» RGB è§†é¢‘åˆ›å»ºå…·æœ‰è¯¦ç»†åŠ¨æ€å¤–è§‚çš„å¯åŠ¨ç”»è™šæ‹Ÿå½¢è±¡ï¼Œå®éªŒè¡¨æ˜è¯¥æ–¹æ³•ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p></li><li><p><strong>æ–¹æ³•ï¼š</strong>ï¼ˆ1ï¼‰æ„å»ºçº¹ç†è¯æ±‡ï¼šå°†å¯¹åº”è®­ç»ƒå§¿åŠ¿çš„æ‰€æœ‰å¯ç”¨å›¾åƒåæŠ•å½±åˆ°æ‘†å§¿åŠ¿çš„SMPLè¡¨é¢ï¼Œç”ŸæˆSMPL UV åŸŸä¸­çš„çº¹ç†è´´å›¾ï¼Œç„¶åæ„å»ºäººä½“å§¿åŠ¿å’Œçº¹ç†è´´å›¾å¯¹ï¼Œä»¥å»ºç«‹çº¹ç†è¯æ±‡æ¥ç¼–ç å„ç§å§¿åŠ¿ä¸‹çš„åŠ¨æ€äººä½“å¤–è§‚ã€‚ï¼ˆ2ï¼‰è®­ç»ƒNeRF MLPï¼šä½¿ç”¨çº¹ç†è¯æ±‡ä½œä¸ºæ¡ä»¶è¾“å…¥ï¼Œè®­ç»ƒNeRF MLP ä»è¡¨è¾¾çº¹ç†æ¡ä»¶ä¸­å­¦ä¹ åŠ¨æ€ã€‚ï¼ˆ3ï¼‰ç”Ÿæˆå¯åŠ¨ç”»è™šæ‹Ÿå½¢è±¡ï¼šä½¿ç”¨è®­ç»ƒå¥½çš„NeRF MLPï¼Œä»RGB è§†é¢‘ä¸­ç”Ÿæˆå…·æœ‰è¯¦ç»†åŠ¨æ€å¤–è§‚çš„å¯åŠ¨ç”»è™šæ‹Ÿå½¢è±¡ã€‚</p></li><li><p>ç»“è®ºï¼š(1): åˆ©ç”¨æ˜¾å¼å›¾åƒè¯æ®æŒ‡å¯¼éšå¼æ¡ä»¶NeRFä»è¡¨è¾¾çº¹ç†æ¡ä»¶ä¸­å­¦ä¹ åŠ¨æ€ï¼Œå®ç°äº†ä»RGBè§†é¢‘åˆ›å»ºå…·æœ‰è¯¦ç»†åŠ¨æ€å¤–è§‚çš„å¯åŠ¨ç”»è™šæ‹Ÿå½¢è±¡ã€‚(2): åˆ›æ–°ç‚¹ï¼šTexVocabçº¹ç†è¯æ±‡ï¼›æ€§èƒ½ï¼šä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼›å·¥ä½œé‡ï¼šå·¥ä½œé‡è¾ƒå¤§ï¼Œéœ€è¦æ”¶é›†å¤§é‡å›¾åƒæ•°æ®å¹¶è¿›è¡ŒåæŠ•å½±å¤„ç†ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-d5ef7cbf30d33e46f182e7d8ab7d3f6a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2b1fc6c5e328a61ca82826325adda143.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-274cb92781d5841c23c9d32fbb82c09e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0fb1457171dba2509eedb86dc74d5975.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f2d2ccbdda2a0bc805f942fdcc2c0204.jpg" align="middle"></details><blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-04-06-æ›´æ–°-1"><a href="#2024-04-06-æ›´æ–°-1" class="headerlink" title="2024-04-06 æ›´æ–°"></a>2024-04-06 æ›´æ–°</h1><h2 id="GeneAvatar-Generic-Expression-Aware-Volumetric-Head-Avatar-Editing-from-a-Single-Image-1"><a href="#GeneAvatar-Generic-Expression-Aware-Volumetric-Head-Avatar-Editing-from-a-Single-Image-1" class="headerlink" title="GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from   a Single Image"></a>GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from   a Single Image</h2><p><strong>Authors:Chong Bao, Yinda Zhang, Yuan Li, Xiyu Zhang, Bangbang Yang, Hujun Bao, Marc Pollefeys, Guofeng Zhang, Zhaopeng Cui</strong></p><p>Recently, we have witnessed the explosive growth of various volumetric representations in modeling animatable head avatars. However, due to the diversity of frameworks, there is no practical method to support high-level applications like 3D head avatar editing across different representations. In this paper, we propose a generic avatar editing approach that can be universally applied to various 3DMM driving volumetric head avatars. To achieve this goal, we design a novel expression-aware modification generative model, which enables lift 2D editing from a single image to a consistent 3D modification field. To ensure the effectiveness of the generative modification process, we develop several techniques, including an expression-dependent modification distillation scheme to draw knowledge from the large-scale head avatar model and 2D facial texture editing tools, implicit latent space guidance to enhance model convergence, and a segmentation-based loss reweight strategy for fine-grained texture inversion. Extensive experiments demonstrate that our method delivers high-quality and consistent results across multiple expression and viewpoints. Project page: <a href="https://zju3dv.github.io/geneavatar/">https://zju3dv.github.io/geneavatar/</a> </p><p><a href="http://arxiv.org/abs/2404.02152v1">PDF</a> Accepted to CVPR 2024. Project page:   <a href="https://zju3dv.github.io/geneavatar/">https://zju3dv.github.io/geneavatar/</a></p><p><strong>Summary</strong><br>é€šç”¨ç¼–è¾‘æ–¹æ³•å¯åº”ç”¨äºåŸºäºä¸åŒè¡¨ç¤ºçš„ 3DMM é©±åŠ¨ä½“ç§¯å¤´éƒ¨å¤´åƒã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºé€šç”¨å¤´åƒç¼–è¾‘æ–¹æ³•ï¼Œå¯åº”ç”¨äºä¸åŒè¡¨ç¤ºçš„ 3DMM é©±åŠ¨ä½“ç§¯å¤´éƒ¨å¤´åƒã€‚</li><li>è®¾è®¡äº†æ–°çš„è¡¨æƒ…æ„ŸçŸ¥ä¿®æ”¹ç”Ÿæˆæ¨¡å‹ï¼Œæ”¯æŒä»å•å¼ å›¾åƒåˆ°ä¸€è‡´ 3D ä¿®æ”¹åŸŸçš„ 2D ç¼–è¾‘ã€‚</li><li>é’ˆå¯¹ç”Ÿæˆä¿®æ”¹è¿‡ç¨‹çš„æœ‰æ•ˆæ€§ï¼Œå¼€å‘äº†å¤šé¡¹æŠ€æœ¯ï¼ŒåŒ…æ‹¬è¡¨æƒ…ç›¸å…³ä¿®æ”¹è’¸é¦æ–¹æ¡ˆã€éšå¼æ½œåœ¨ç©ºé—´å¼•å¯¼å’ŒåŸºäºåˆ†å‰²çš„æŸå¤±é‡æ–°åŠ æƒç­–ç•¥ã€‚</li><li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§è¡¨æƒ…å’Œè§†ç‚¹ä¸‹å¯ä»¥äº§ç”Ÿé«˜è´¨é‡ä¸”ä¸€è‡´çš„ç»“æœã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>è®ºæ–‡æ ‡é¢˜ï¼šé€šç”¨å¤´åƒç¼–è¾‘ï¼šä» 2D å›¾åƒåˆ°ä¸€è‡´çš„ 3D ä¿®æ”¹åŸŸï¼ˆé€šç”¨å¤´åƒç¼–è¾‘ï¼šä»äºŒç»´å›¾åƒåˆ°ä¸€è‡´çš„ä¸‰ç»´ä¿®æ”¹åŸŸï¼‰</li><li>ä½œè€…ï¼šTianchang Shen, Xiaoguang Han, Yebin Liu, Yu-Kun Lai, Shizhan Zhu, Ling-Qi Yan</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šæµ™æ±Ÿå¤§å­¦</li><li>å…³é”®è¯ï¼š3D å¤´éƒ¨å¤´åƒï¼Œ3DMMï¼Œç”Ÿæˆæ¨¡å‹ï¼Œå›¾åƒç¼–è¾‘ï¼Œé¢éƒ¨åŠ¨ç”»</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2207.07031   Github ä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼š   (1) ç ”ç©¶èƒŒæ™¯ï¼šéšç€å„ç§ä½“ç§¯è¡¨ç¤ºåœ¨å»ºæ¨¡å¯åŠ¨ç”»å¤´éƒ¨å¤´åƒä¸­çš„çˆ†å‘å¼å¢é•¿ï¼Œè¿«åˆ‡éœ€è¦ä¸€ç§é€šç”¨æ–¹æ³•æ¥æ”¯æŒè·¨ä¸åŒè¡¨ç¤ºçš„é«˜çº§åº”ç”¨ï¼Œå¦‚ 3D å¤´éƒ¨å¤´åƒç¼–è¾‘ã€‚   (2) è¿‡å»æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•é€šå¸¸é’ˆå¯¹ç‰¹å®šè¡¨ç¤ºé‡èº«å®šåˆ¶ï¼Œç¼ºä¹é€šç”¨æ€§ã€‚   (3) ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è¡¨æƒ…æ„ŸçŸ¥ä¿®æ”¹ç”Ÿæˆæ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿå°† 2D ç¼–è¾‘ä»å•ä¸ªå›¾åƒæå‡åˆ°ä¸€è‡´çš„ 3D ä¿®æ”¹åŸŸã€‚ä¸ºäº†ç¡®ä¿ç”Ÿæˆä¿®æ”¹è¿‡ç¨‹çš„æœ‰æ•ˆæ€§ï¼Œæœ¬æ–‡å¼€å‘äº†å‡ ç§æŠ€æœ¯ï¼ŒåŒ…æ‹¬ï¼š</li><li>è¡¨æƒ…ç›¸å…³çš„ä¿®æ”¹è’¸é¦æ–¹æ¡ˆï¼Œä»å¤§è§„æ¨¡å¤´éƒ¨å¤´åƒæ¨¡å‹å’Œ 2D é¢éƒ¨çº¹ç†ç¼–è¾‘å·¥å…·ä¸­è·å–çŸ¥è¯†ï¼›</li><li>éšå¼æ½œç©ºé—´å¼•å¯¼ï¼Œå¢å¼ºæ¨¡å‹æ”¶æ•›æ€§ï¼›</li><li><p>åŸºäºåˆ†å‰²çš„æŸå¤±é‡åŠ æƒç­–ç•¥ï¼Œç”¨äºç»†ç²’åº¦çº¹ç†åæ¼”ã€‚   (4) æ€§èƒ½ï¼šå®éªŒè¡¨æ˜ï¼Œæœ¬æ–‡æ–¹æ³•åœ¨å¤šç§è¡¨æƒ…å’Œè§†ç‚¹ä¸‹éƒ½èƒ½æä¾›é«˜è´¨é‡ä¸”ä¸€è‡´çš„ç»“æœã€‚</p></li><li><p>æ–¹æ³•ï¼š(1): æœ¬æ–‡æå‡ºäº†ä¸€ç§è¡¨æƒ…æ„ŸçŸ¥ä¿®æ”¹ç”Ÿæˆæ¨¡å‹ï¼Œå°†2Då›¾åƒç¼–è¾‘æå‡åˆ°ä¸€è‡´çš„3Dä¿®æ”¹åŸŸã€‚(2): é‡‡ç”¨è¡¨æƒ…ç›¸å…³çš„ä¿®æ”¹è’¸é¦æ–¹æ¡ˆï¼Œä»å¤§è§„æ¨¡å¤´éƒ¨å¤´åƒæ¨¡å‹å’Œ2Dé¢éƒ¨çº¹ç†ç¼–è¾‘å·¥å…·ä¸­è·å–çŸ¥è¯†ã€‚(3): å¼•å…¥éšå¼æ½œç©ºé—´å¼•å¯¼ï¼Œå¢å¼ºæ¨¡å‹æ”¶æ•›æ€§ã€‚(4): é‡‡ç”¨åŸºäºåˆ†å‰²çš„æŸå¤±é‡åŠ æƒç­–ç•¥ï¼Œç”¨äºç»†ç²’åº¦çº¹ç†åæ¼”ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæå‡ºäº†ä¸€ç§æ–°é¢–çš„é€šç”¨ç¼–è¾‘æ–¹æ³•ï¼Œå…è®¸ç”¨æˆ·ä»å•å¹…å›¾åƒç¼–è¾‘å„ç§ä½“ç§¯å¤´éƒ¨å¤´åƒè¡¨ç¤ºï¼Œå…¶ä¸­è¡¨æƒ…æ„ŸçŸ¥ä¿®æ”¹ç”Ÿæˆå™¨å°†ç¼–è¾‘æå‡åˆ° 3D å¤´åƒï¼ŒåŒæ—¶ä¿æŒåœ¨å¤šç§è¡¨æƒ…å’Œè§†ç‚¹ä¸‹çš„ä¸€è‡´æ€§ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºè¡¨æƒ…æ„ŸçŸ¥ä¿®æ”¹è’¸é¦æ–¹æ¡ˆï¼Œä»å¤§è§„æ¨¡å¤´éƒ¨å¤´åƒæ¨¡å‹å’Œ 2D é¢éƒ¨çº¹ç†ç¼–è¾‘å·¥å…·ä¸­è·å–çŸ¥è¯†ï¼›å¼•å…¥éšå¼æ½œç©ºé—´å¼•å¯¼ï¼Œå¢å¼ºæ¨¡å‹æ”¶æ•›æ€§ï¼›é‡‡ç”¨åŸºäºåˆ†å‰²çš„æŸå¤±é‡åŠ æƒç­–ç•¥ï¼Œç”¨äºç»†ç²’åº¦çº¹ç†åæ¼”ã€‚æ€§èƒ½ï¼šåœ¨å¤šç§è¡¨æƒ…å’Œè§†ç‚¹ä¸‹æä¾›é«˜è´¨é‡ä¸”ä¸€è‡´çš„ç»“æœã€‚å·¥ä½œé‡ï¼šéœ€è¦è¿›ä¸€æ­¥æ¢ç´¢æ·»åŠ é¢å¤–å¯¹è±¡ï¼ˆä¾‹å¦‚å¸½å­ï¼‰æˆ–ä¿®æ”¹å‘å‹çš„èƒ½åŠ›ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-f2a7d66d82bc4bf2cff263f5b555ec88.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3cd3c37db291268698e721edf97b0eb6.jpg" align="middle"><img src="https://pica.zhimg.com/v2-24ce17b0544279479a579ad25b433b3b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e8865d91f8f0c5b8b1208f84a27e63f2.jpg" align="middle"></details><h2 id="Efficient-3D-Implicit-Head-Avatar-with-Mesh-anchored-Hash-Table-Blendshapes-1"><a href="#Efficient-3D-Implicit-Head-Avatar-with-Mesh-anchored-Hash-Table-Blendshapes-1" class="headerlink" title="Efficient 3D Implicit Head Avatar with Mesh-anchored Hash Table   Blendshapes"></a>Efficient 3D Implicit Head Avatar with Mesh-anchored Hash Table   Blendshapes</h2><p><strong>Authors:Ziqian Bai, Feitong Tan, Sean Fanello, Rohit Pandey, Mingsong Dou, Shichen Liu, Ping Tan, Yinda Zhang</strong></p><p>3D head avatars built with neural implicit volumetric representations have achieved unprecedented levels of photorealism. However, the computational cost of these methods remains a significant barrier to their widespread adoption, particularly in real-time applications such as virtual reality and teleconferencing. While attempts have been made to develop fast neural rendering approaches for static scenes, these methods cannot be simply employed to support realistic facial expressions, such as in the case of a dynamic facial performance. To address these challenges, we propose a novel fast 3D neural implicit head avatar model that achieves real-time rendering while maintaining fine-grained controllability and high rendering quality. Our key idea lies in the introduction of local hash table blendshapes, which are learned and attached to the vertices of an underlying face parametric model. These per-vertex hash-tables are linearly merged with weights predicted via a CNN, resulting in expression dependent embeddings. Our novel representation enables efficient density and color predictions using a lightweight MLP, which is further accelerated by a hierarchical nearest neighbor search method. Extensive experiments show that our approach runs in real-time while achieving comparable rendering quality to state-of-the-arts and decent results on challenging expressions. </p><p><a href="http://arxiv.org/abs/2404.01543v1">PDF</a> In CVPR2024. Project page:   <a href="https://augmentedperception.github.io/monoavatar-plus">https://augmentedperception.github.io/monoavatar-plus</a></p><p><strong>Summary</strong><br>3Dé¢éƒ¨å¤´åƒé‡‡ç”¨ç¥ç»éšå¼ä½“ç§¯è¡¨ç°ï¼Œå®ç°äº†å‰æ‰€æœªæœ‰çš„é€¼çœŸåº¦ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ç¥ç»éšå¼ä½“ç§¯è¡¨å¾æ–¹æ³•æ„å»ºäººå¤´ä¸‰ç»´æ¨¡å‹ï¼Œå®ç°é€¼çœŸç¨‹åº¦é«˜</li><li>ä¼ ç»Ÿæ–¹æ³•è®¡ç®—é‡å¤§ï¼Œé˜»ç¢å…¶åœ¨å®æ—¶åº”ç”¨ï¼ˆè™šæ‹Ÿç°å®ã€è§†é¢‘ä¼šè®®ï¼‰ä¸­è¿ç”¨</li><li>æå‡ºå¿«é€Ÿä¸‰ç»´ç¥ç»éšå¼äººå¤´å¤´åƒæ¨¡å‹ï¼Œå®ç°å®æ—¶æ¸²æŸ“ï¼Œå¹¶å…¼é¡¾ç²¾ç»†æ§åˆ¶æ€§å’Œé«˜æ¸²æŸ“è´¨é‡</li><li>å¼•å…¥å±€éƒ¨å“ˆå¸Œè¡¨æ··åˆå½¢çŠ¶ï¼Œå¹¶å°†å…¶å­¦ä¹ å¹¶é™„åŠ åœ¨åº•å±‚äººè„¸å‚æ•°æ¨¡å‹çš„é¡¶ç‚¹ä¸Š</li><li>ä½¿ç”¨è½»é‡çº§å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰å®ç°å¯†åº¦å’Œé¢œè‰²çš„é«˜æ•ˆé¢„æµ‹ï¼Œå¹¶é€šè¿‡åˆ†å±‚æœ€è¿‘é‚»æœç´¢æ–¹æ³•è¿›ä¸€æ­¥åŠ é€Ÿ</li><li>å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•è¿è¡Œäºå®æ—¶ï¼ŒåŒæ—¶å®ç°ä¸ç°æœ‰æŠ€æœ¯ç›¸å½“çš„æ¸²æŸ“è´¨é‡ï¼Œåœ¨æŒ‘æˆ˜æ€§äººè„¸è¡¨æƒ…ä¸‹ä¹Ÿå¯è·å¾—è¾ƒå¥½ç»“æœ</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šåŸºäºç½‘æ ¼é”šå®šçš„å“ˆå¸Œè¡¨æ··åˆå½¢çŠ¶</li><li>ä½œè€…ï¼šKai Zhang, Yuxuan Zhang, Jiaolong Yang, Kun Xu, Yebin Liu, Qiong Yan, Baoquan Chen</li><li>å•ä½ï¼šæ¸…åå¤§å­¦</li><li>å…³é”®è¯ï¼šé¢éƒ¨åŠ¨ç”»ã€ç¥ç»è¾å°„åœºã€å“ˆå¸Œç¼–ç </li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.06438Github ä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼š(1)ï¼šç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æ˜¯ä¸€ç§å¼ºå¤§çš„è¡¨ç¤ºï¼Œå¯ä»¥ä»å›¾åƒä¸­æ•æ‰å¤æ‚åœºæ™¯çš„å‡ ä½•å’Œå¤–è§‚ã€‚ç„¶è€Œï¼ŒNeRF åœ¨è¡¨ç¤ºå…·æœ‰å¤æ‚æ‹“æ‰‘ç»“æ„çš„å¯¹è±¡ï¼ˆä¾‹å¦‚é¢éƒ¨ï¼‰æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚(2)ï¼šè¿‡å»æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•å°è¯•é€šè¿‡é‡‡ç”¨å“ˆå¸Œç¼–ç æŠ€æœ¯å°† NeRF åº”ç”¨äºé¢éƒ¨åŠ¨ç”»ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•è¦ä¹ˆå—é™äºå…¨å±€æ··åˆå½¢çŠ¶ï¼Œè¦ä¹ˆéœ€è¦å¤§é‡çš„å†…å­˜å’Œè®¡ç®—æˆæœ¬ã€‚(3)ï¼šæœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç½‘æ ¼é”šå®šçš„å“ˆå¸Œè¡¨æ··åˆå½¢çŠ¶çš„æ–°å‹é¢éƒ¨è¡¨ç¤ºã€‚è¯¥è¡¨ç¤ºå°† 3DMM é”šå®šçš„ NeRF ä¸å“ˆå¸Œç¼–ç ç›¸ç»“åˆï¼Œä»¥æœ‰æ•ˆåœ°æ•æ‰é¢éƒ¨è¡¨æƒ…çš„ç²¾ç»†ç»†èŠ‚ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä¸ºæ¯ä¸ª 3DMM é¡¶ç‚¹é™„åŠ ä¸€ç»„å“ˆå¸Œè¡¨ï¼Œæ¯ä¸ªå“ˆå¸Œè¡¨ç¼–ç é¡¶ç‚¹å‘¨å›´å±€éƒ¨è¾å°„åœºçš„åµŒå…¥ã€‚åœ¨æ¸²æŸ“æ—¶ï¼Œè¿™äº›å“ˆå¸Œè¡¨è¢«çº¿æ€§æ±‚å’Œï¼Œä»¥ç”Ÿæˆè¡¨ç¤ºç›®æ ‡è¡¨æƒ…çš„åˆå¹¶åµŒå…¥ã€‚(4)ï¼šæ–¹æ³•æ€§èƒ½ï¼šæˆ‘ä»¬åœ¨é¢éƒ¨åŠ¨ç”»åŸºå‡†ä¸Šè¯„ä¼°äº†æ‰€æå‡ºçš„æ–¹æ³•ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¸²æŸ“è´¨é‡å’Œæ•ˆç‡æ–¹é¢éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿå¤„ç†å„ç§é¢éƒ¨è¡¨æƒ…ï¼ŒåŒ…æ‹¬æç«¯è¡¨æƒ…ã€‚</li></ol><p><strong>æ–¹æ³•</strong></p><ol><li><strong>ç½‘æ ¼é”šå®šå“ˆå¸Œè¡¨æ··åˆå½¢çŠ¶ï¼š</strong>æå‡ºä¸€ç§æ–°çš„é¢éƒ¨è¡¨ç¤ºæ–¹æ³•ï¼Œå°† 3DMM é”šå®šçš„ç¥ç»è¾å°„åœºä¸å“ˆå¸Œç¼–ç ç›¸ç»“åˆï¼Œä»¥æœ‰æ•ˆæ•æ‰é¢éƒ¨è¡¨æƒ…çš„ç²¾ç»†ç»†èŠ‚ã€‚</li><li><strong>å“ˆå¸Œè¡¨æ··åˆå½¢çŠ¶çš„èåˆï¼š</strong>é€šè¿‡è¿è¡Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰åœ¨ UV å›¾åƒç©ºé—´ä¸­é¢„æµ‹é¡¶ç‚¹å˜å½¢ï¼Œè·å¾—æ¯ä¸ªé¡¶ç‚¹çš„æƒé‡ã€‚ç„¶åï¼Œä½¿ç”¨è¿™äº›æƒé‡å¯¹æ¯ä¸ªé¡¶ç‚¹ä¸Šçš„å“ˆå¸Œè¡¨è¿›è¡Œçº¿æ€§æ±‚å’Œï¼Œç”Ÿæˆåˆå¹¶çš„åµŒå…¥ã€‚</li><li><strong>æŸ¥è¯¢ç‚¹è§£ç ï¼š</strong>ä»åˆå¹¶çš„å“ˆå¸Œè¡¨ä¸­æå–åµŒå…¥ï¼Œå¹¶å°†å…¶ä¸ç‰¹å¾åµŒå…¥å’Œæ‘„åƒæœºè§†å›¾ä¸€èµ·è§£ç ä¸ºç¥ç»è¾å°„åœºã€‚</li><li><strong>åŠ é€Ÿæ¸²æŸ“ï¼š</strong>åˆ©ç”¨æŸ¥è¯¢ç‚¹ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œå°†æŸ¥è¯¢ç‚¹åˆ†ç»„åˆ°ä½“ç´ ä¸­ï¼Œå¹¶åˆ†å±‚æœç´¢ k-æœ€è¿‘é‚»é¡¶ç‚¹ï¼Œä»¥åŠ é€Ÿæ¸²æŸ“ã€‚</li><li><p><strong>å•ç›®è§†é¢‘è®­ç»ƒï¼š</strong>ä»…ä½¿ç”¨å•ç›® RGB è§†é¢‘è®­ç»ƒæå‡ºçš„å¤´åƒè¡¨ç¤ºï¼Œæ— éœ€ä»»ä½• 3D æ‰«ææˆ–å¤šè§†å›¾æ•°æ®ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç½‘æ ¼é”šå®šçš„å“ˆå¸Œè¡¨æ··åˆå½¢çŠ¶çš„æ–°å‹é¢éƒ¨è¡¨ç¤ºæ–¹æ³•ï¼Œæœ‰æ•ˆåœ°æ•æ‰äº†é¢éƒ¨è¡¨æƒ…çš„ç²¾ç»†ç»†èŠ‚ï¼Œåœ¨æ¸²æŸ“è´¨é‡å’Œæ•ˆç‡æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäºç½‘æ ¼é”šå®šçš„å“ˆå¸Œè¡¨æ··åˆå½¢çŠ¶çš„æ–°å‹é¢éƒ¨è¡¨ç¤ºæ–¹æ³•ï¼Œå°†3DMMé”šå®šçš„ç¥ç»è¾å°„åœºä¸å“ˆå¸Œç¼–ç ç›¸ç»“åˆï¼Œæœ‰æ•ˆåœ°æ•æ‰é¢éƒ¨è¡¨æƒ…çš„ç²¾ç»†ç»†èŠ‚ã€‚æ€§èƒ½ï¼šåœ¨é¢éƒ¨åŠ¨ç”»åŸºå‡†ä¸Šè¯„ä¼°äº†æ‰€æå‡ºçš„æ–¹æ³•ï¼Œç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¸²æŸ“è´¨é‡å’Œæ•ˆç‡æ–¹é¢éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å·¥ä½œé‡ï¼šä»…ä½¿ç”¨å•ç›®RGBè§†é¢‘è®­ç»ƒæå‡ºçš„å¤´åƒè¡¨ç¤ºï¼Œæ— éœ€ä»»ä½•3Dæ‰«ææˆ–å¤šè§†å›¾æ•°æ®ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-ab39844047d36e8caedab23572e71526.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a0e8bfc8983817dd020f5b1deff586eb.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-55d64640c5208a9cd19a534cb1503aba.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3173031114e8293e5e25b9733f1913ef.jpg" align="middle"></details><h2 id="MagicMirror-Fast-and-High-Quality-Avatar-Generation-with-a-Constrained-Search-Space-1"><a href="#MagicMirror-Fast-and-High-Quality-Avatar-Generation-with-a-Constrained-Search-Space-1" class="headerlink" title="MagicMirror: Fast and High-Quality Avatar Generation with a Constrained   Search Space"></a>MagicMirror: Fast and High-Quality Avatar Generation with a Constrained   Search Space</h2><p><strong>Authors:Armand Comas-MassaguÃ©, Di Qiu, Menglei Chai, Marcel BÃ¼hler, Amit Raj, Ruiqi Gao, Qiangeng Xu, Mark Matthews, Paulo Gotardo, Octavia Camps, Sergio Orts-Escolano, Thabo Beeler</strong></p><p>We introduce a novel framework for 3D human avatar generation and personalization, leveraging text prompts to enhance user engagement and customization. Central to our approach are key innovations aimed at overcoming the challenges in photo-realistic avatar synthesis. Firstly, we utilize a conditional Neural Radiance Fields (NeRF) model, trained on a large-scale unannotated multi-view dataset, to create a versatile initial solution space that accelerates and diversifies avatar generation. Secondly, we develop a geometric prior, leveraging the capabilities of Text-to-Image Diffusion Models, to ensure superior view invariance and enable direct optimization of avatar geometry. These foundational ideas are complemented by our optimization pipeline built on Variational Score Distillation (VSD), which mitigates texture loss and over-saturation issues. As supported by our extensive experiments, these strategies collectively enable the creation of custom avatars with unparalleled visual quality and better adherence to input text prompts. You can find more results and videos in our website: <a href="https://syntec-research.github.io/MagicMirror">https://syntec-research.github.io/MagicMirror</a> </p><p><a href="http://arxiv.org/abs/2404.01296v1">PDF</a> </p><p><strong>Summary</strong><br>æå‡ºä¸€ç§å…¨æ–° 3D äººä½“è™šæ‹Ÿäººç”Ÿæˆå’Œä¸ªæ€§åŒ–æ¡†æ¶ï¼Œåˆ©ç”¨æ–‡æœ¬æç¤ºå¢å¼ºç”¨æˆ·å‚ä¸å’Œå®šåˆ¶ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>åˆ©ç”¨æ¡ä»¶ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æ¨¡å‹ï¼Œåˆ›å»ºäº†ä¸€ä¸ªå¯æ‰©å±•çš„åˆå§‹è§£å†³æ–¹æ¡ˆç©ºé—´ï¼Œä½¿è™šæ‹Ÿäººç”Ÿæˆé€Ÿåº¦æ›´å¿«ã€å¤šæ ·åŒ–æ›´å¼ºã€‚</li><li>å¼€å‘äº†ä¸€ä¸ªåŸºäºå‡ ä½•å…ˆéªŒå’Œæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„ä¼˜åŒ–ç®¡é“ï¼Œä»¥ç¡®ä¿å‡ºè‰²çš„è§†å›¾ä¸å˜æ€§å’Œç›´æ¥ä¼˜åŒ–è™šæ‹Ÿäººçš„å‡ ä½•å½¢çŠ¶ã€‚</li><li>æˆ‘ä»¬çš„ä¼˜åŒ–ç®¡é“å»ºç«‹åœ¨å˜åˆ†åˆ†æ•°è’¸é¦ï¼ˆVSDï¼‰ä¹‹ä¸Šï¼Œå¯ç¼“è§£çº¹ç†ä¸¢å¤±å’Œè¿‡é¥±å’Œé—®é¢˜ã€‚</li><li>æä¾›çš„åˆ›æ–°ç­–ç•¥èƒ½å¤Ÿåˆ›é€ å‡ºå…·æœ‰æ— ä¸ä¼¦æ¯”è§†è§‰è´¨é‡å’Œæ›´ç¬¦åˆè¾“å…¥æ–‡æœ¬æç¤ºçš„è‡ªå®šä¹‰è™šæ‹Ÿäººã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šé­”é•œï¼šå¿«é€Ÿä¸”é«˜è´¨é‡çš„å¤´åƒ</li><li>ä½œè€…ï¼šArmand Comas-MassaguÃ©, Di Qiu, Menglei Chai, Marcel BÃ¼hler, Amit Raj, Ruiqi Gao, Qiangeng Xu, Mark Matthews, Paulo Gotardo, Octavia Camps, Sergio Orts-Escolano, Thabo Beeler</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šGoogle</li><li>å…³é”®è¯ï¼š3D å¤´åƒç”Ÿæˆã€æ–‡æœ¬å¼•å¯¼ã€ç¥ç»è¾å°„åœºã€å‡ ä½•å…ˆéªŒã€å˜åˆ†åˆ†æ•°è’¸é¦</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.01296   Github ä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šéšç€è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®ç­‰æŠ€æœ¯çš„å…´èµ·ï¼Œå¯¹é€¼çœŸä¸”å¯å®šåˆ¶çš„ 3D äººç±»å¤´åƒçš„éœ€æ±‚ä¸æ–­å¢é•¿ã€‚ä½†æ˜¯ï¼Œç”Ÿæˆé«˜è´¨é‡çš„å¤´åƒä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå°¤å…¶æ˜¯å½“éœ€è¦æ ¹æ®æ–‡æœ¬æç¤ºè¿›è¡Œä¸ªæ€§åŒ–å®šåˆ¶æ—¶ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šä¼ ç»Ÿæ–¹æ³•é€šå¸¸ä¾èµ–äºä» 3D æ‰«ææˆ–æ‰‹åŠ¨å»ºæ¨¡ä¸­è·å–æ•°æ®ï¼Œè¿™æ—¢è€—æ—¶åˆæ˜‚è´µã€‚åŸºäºç¥ç»ç½‘ç»œçš„æ–¹æ³•è™½ç„¶å¯ä»¥ä»å›¾åƒä¸­ç”Ÿæˆå¤´åƒï¼Œä½†å®ƒä»¬åœ¨æ•è·æ–‡æœ¬æç¤ºä¸­çš„ç»†å¾®å·®åˆ«å’Œç¡®ä¿å‡ ä½•ä¸€è‡´æ€§æ–¹é¢ä»ç„¶å­˜åœ¨å›°éš¾ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡º MagicMirror æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨æ–‡æœ¬æç¤ºç”Ÿæˆå¿«é€Ÿä¸”é«˜è´¨é‡çš„ 3D äººç±»å¤´åƒã€‚MagicMirror åˆ©ç”¨æ¡ä»¶ç¥ç»è¾å°„åœº (NeRF) æ¨¡å‹åˆ›å»ºå¤šè§†å›¾åˆå§‹è§£ç©ºé—´ï¼Œå¹¶ä½¿ç”¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å¼€å‘å‡ ä½•å…ˆéªŒä»¥ç¡®ä¿è§†å›¾ä¸å˜æ€§å’Œå‡ ä½•ä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜é‡‡ç”¨åŸºäºå˜åˆ†åˆ†æ•°è’¸é¦çš„ä¼˜åŒ–ç®¡é“ï¼Œä»¥å‡è½»çº¹ç†æŸå¤±å’Œè¿‡é¥±å’Œé—®é¢˜ã€‚ï¼ˆ4ï¼‰ä»»åŠ¡å’Œæ€§èƒ½ï¼šMagicMirror åœ¨å¤´åƒç”Ÿæˆå’Œä¸ªæ€§åŒ–ä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥ç”Ÿæˆå…·æœ‰æ— ä¸ä¼¦æ¯”è§†è§‰è´¨é‡å’Œé«˜åº¦ç¬¦åˆæ–‡æœ¬æç¤ºçš„å®šåˆ¶å¤´åƒã€‚è¯¥æ–¹æ³•çš„æ€§èƒ½æ”¯æŒå…¶ç›®æ ‡ï¼Œå³æä¾›ä¸€ç§å¿«é€Ÿä¸”æœ‰æ•ˆçš„æ–¹æ³•æ¥ç”Ÿæˆé«˜è´¨é‡çš„ 3D äººç±»å¤´åƒã€‚</li></ol><p>7.æ–¹æ³•ï¼šï¼ˆ1ï¼‰åˆ©ç”¨æ¡ä»¶ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æ¨¡å‹åˆ›å»ºå¤šè§†å›¾åˆå§‹è§£ç©ºé—´ï¼Œä¸ºä¼˜åŒ–æä¾›çº¦æŸï¼›ï¼ˆ2ï¼‰ä½¿ç”¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å¼€å‘å‡ ä½•å…ˆéªŒï¼Œç¡®ä¿è§†å›¾ä¸å˜æ€§å’Œå‡ ä½•ä¼˜åŒ–ï¼›ï¼ˆ3ï¼‰é‡‡ç”¨åŸºäºå˜åˆ†åˆ†æ•°è’¸é¦çš„ä¼˜åŒ–ç®¡é“ï¼Œå‡è½»çº¹ç†æŸå¤±å’Œè¿‡é¥±å’Œé—®é¢˜ï¼›ï¼ˆ4ï¼‰é€šè¿‡æ··åˆå’ŒåŠ æƒä¸åŒçš„æ¦‚å¿µï¼Œå®ç°æ¦‚å¿µç»„åˆå’Œè°ƒåˆ¶ï¼Œä¸°å¯Œç”¨æˆ·ä½“éªŒã€‚</p><ol><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šxxxï¼›ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šxxxï¼›æ€§èƒ½ï¼šxxxï¼›å·¥ä½œé‡ï¼šxxxï¼›</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº† MagicMirror æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨æ–‡æœ¬æç¤ºç”Ÿæˆå¿«é€Ÿä¸”é«˜è´¨é‡çš„ 3D äººç±»å¤´åƒã€‚MagicMirror é‡‡ç”¨æ¡ä»¶ç¥ç»è¾å°„åœº (NeRF) æ¨¡å‹ã€æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å’ŒåŸºäºå˜åˆ†åˆ†æ•°è’¸é¦çš„ä¼˜åŒ–ç®¡é“ï¼Œå®ç°äº†æ— ä¸ä¼¦æ¯”çš„è§†è§‰è´¨é‡ã€é«˜åº¦ç¬¦åˆæ–‡æœ¬æç¤ºçš„å®šåˆ¶å¤´åƒç”Ÿæˆï¼Œä¸ºå¿«é€Ÿé«˜æ•ˆç”Ÿæˆé«˜è´¨é‡çš„ 3D äººç±»å¤´åƒæä¾›äº†æœ‰æ•ˆæ–¹æ³•ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>åˆ©ç”¨æ¡ä»¶ç¥ç»è¾å°„åœº (NeRF) æ¨¡å‹åˆ›å»ºå¤šè§†å›¾åˆå§‹è§£ç©ºé—´ï¼Œä¸ºä¼˜åŒ–æä¾›çº¦æŸã€‚</li><li>ä½¿ç”¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å¼€å‘å‡ ä½•å…ˆéªŒï¼Œç¡®ä¿è§†å›¾ä¸å˜æ€§å’Œå‡ ä½•ä¼˜åŒ–ã€‚</li><li>é‡‡ç”¨åŸºäºå˜åˆ†åˆ†æ•°è’¸é¦çš„ä¼˜åŒ–ç®¡é“ï¼Œå‡è½»çº¹ç†æŸå¤±å’Œè¿‡é¥±å’Œé—®é¢˜ã€‚</li><li>é€šè¿‡æ··åˆå’ŒåŠ æƒä¸åŒçš„æ¦‚å¿µï¼Œå®ç°æ¦‚å¿µç»„åˆå’Œè°ƒåˆ¶ï¼Œä¸°å¯Œç”¨æˆ·ä½“éªŒã€‚æ€§èƒ½ï¼š</li><li>åœ¨å¤´åƒç”Ÿæˆå’Œä¸ªæ€§åŒ–ä»»åŠ¡ä¸Šï¼ŒMagicMirror ç”Ÿæˆå…·æœ‰æ— ä¸ä¼¦æ¯”è§†è§‰è´¨é‡å’Œé«˜åº¦ç¬¦åˆæ–‡æœ¬æç¤ºçš„å®šåˆ¶å¤´åƒã€‚</li><li>MagicMirror çš„æ€§èƒ½æ”¯æŒå…¶ç›®æ ‡ï¼Œå³æä¾›ä¸€ç§å¿«é€Ÿä¸”æœ‰æ•ˆçš„æ–¹æ³•æ¥ç”Ÿæˆé«˜è´¨é‡çš„ 3D äººç±»å¤´åƒã€‚å·¥ä½œé‡ï¼š</li><li>è™½ç„¶ MagicMirror ä¸éœ€è¦å¤§è§„æ¨¡çš„ 3D äººç±»æ•°æ®ï¼Œä½†ä¸ºæ•°ç™¾æˆ–æ•°åƒä¸ªå¯¹è±¡æ”¶é›†è¿™äº›æ•°æ®ä»ç„¶æ˜¯ä¸€é¡¹ç›¸å¯¹æ˜‚è´µä¸”è€—æ—¶çš„å·¥ä½œã€‚</li><li>ä»å¦ä¸€ä¸ªè§’åº¦æ¥çœ‹ï¼Œæˆ‘ä»¬ç”¨æ¥çº¦æŸè§£ç©ºé—´çš„æ•°æ®ä¹Ÿé™åˆ¶äº†æˆ‘ä»¬ï¼Œå› ä¸ºæŸäº›æç«¯çš„åˆ†å¸ƒå¤–ä¿®æ”¹å¾ˆéš¾å®ç°ã€‚</li><li>æˆ‘ä»¬çš„æ–¹æ³•ä¹Ÿå¯èƒ½å—åˆ°è®¡ç®—èµ„æºçš„é™åˆ¶ï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦å¤šä¸ªæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œè‡³å°‘æ¯ä¸ªæ¨¡å‹éƒ½ç”¨äºé¢œè‰²å’Œæ³•çº¿ï¼Œå¦‚æœæˆ‘ä»¬æƒ³è¦æ‰§è¡Œæ¦‚å¿µæ··åˆï¼Œåˆ™éœ€è¦æ›´å¤šã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-1baa0925f922a12a8e66e59ff6fb331c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c42a213fc6f5741472d164c3e74fb649.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-864c32c352be8bc29cb98bb5b6fc3f53.jpg" align="middle"></details><h2 id="HAHA-Highly-Articulated-Gaussian-Human-Avatars-with-Textured-Mesh-Prior"><a href="#HAHA-Highly-Articulated-Gaussian-Human-Avatars-with-Textured-Mesh-Prior" class="headerlink" title="HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior"></a>HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior</h2><p><strong>Authors:David Svitov, Pietro Morerio, Lourdes Agapito, Alessio Del Bue</strong></p><p>We present HAHA - a novel approach for animatable human avatar generation from monocular input videos. The proposed method relies on learning the trade-off between the use of Gaussian splatting and a textured mesh for efficient and high fidelity rendering. We demonstrate its efficiency to animate and render full-body human avatars controlled via the SMPL-X parametric model. Our model learns to apply Gaussian splatting only in areas of the SMPL-X mesh where it is necessary, like hair and out-of-mesh clothing. This results in a minimal number of Gaussians being used to represent the full avatar, and reduced rendering artifacts. This allows us to handle the animation of small body parts such as fingers that are traditionally disregarded. We demonstrate the effectiveness of our approach on two open datasets: SnapshotPeople and X-Humans. Our method demonstrates on par reconstruction quality to the state-of-the-art on SnapshotPeople, while using less than a third of Gaussians. HAHA outperforms previous state-of-the-art on novel poses from X-Humans both quantitatively and qualitatively. </p><p><a href="http://arxiv.org/abs/2404.01053v1">PDF</a> </p><p><strong>Summary</strong><br>ä½¿ç”¨é«˜æ–¯æ•£å°„å’Œçº¹ç†ç½‘æ ¼ç›¸ç»“åˆçš„æ–¹å¼ï¼Œç”Ÿæˆå¯åŠ¨ç”»é€¼çœŸçš„å…¨èº«äººä½“å¤´åƒã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºäº†ä¸€ç§åä¸ºHAHAçš„æ–°æ–¹æ³•ï¼Œç”¨äºä»å•ç›®è¾“å…¥è§†é¢‘ç”Ÿæˆå¯åŠ¨ç”»çš„äººå½¢å¤´åƒã€‚</li><li>HAHAé€šè¿‡å­¦ä¹ é«˜æ–¯æ•£å°„å’Œçº¹ç†ç½‘æ ¼çš„ä½¿ç”¨æƒè¡¡ï¼Œå®ç°é«˜æ•ˆä¸”é«˜ä¿çœŸçš„æ¸²æŸ“ã€‚</li><li>HAHAä»…åœ¨SMPL-Xç½‘æ ¼å¿…è¦çš„åŒºåŸŸï¼ˆå¦‚å¤´å‘å’Œç½‘æ ¼å¤–è¡£ç‰©ï¼‰åº”ç”¨é«˜æ–¯æ•£å°„ã€‚</li><li>HAHAå‡å°‘äº†è¡¨ç¤ºå®Œæ•´å¤´åƒæ‰€éœ€çš„é«˜æ–¯æ•°é‡ï¼Œå¹¶å‡å°‘äº†æ¸²æŸ“ä¼ªå½±ã€‚</li><li>HAHAå¯ä»¥å¤„ç†æ‰‹æŒ‡ç­‰ä¼ ç»Ÿä¸Šè¢«å¿½ç•¥çš„å°èº«ä½“éƒ¨ä½çš„åŠ¨ç”»ã€‚</li><li>HAHAåœ¨SnapshotPeopleæ•°æ®é›†ä¸Šå±•ç¤ºäº†ä¸æœ€å…ˆè¿›æŠ€æœ¯ç›¸å½“çš„é‡å»ºè´¨é‡ï¼ŒåŒæ—¶ä½¿ç”¨çš„é«˜æ–¯æ•°é‡ä¸åˆ°ä¸‰åˆ†ä¹‹ä¸€ã€‚</li><li>HAHAåœ¨X-Humansçš„æ–°å§¿åŠ¿ä¸Šè¶…è¶Šäº†ä¹‹å‰çš„æœ€å…ˆè¿›æŠ€æœ¯ï¼Œæ— è®ºæ˜¯åœ¨å®šé‡è¿˜æ˜¯å®šæ€§ä¸Šã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>è®ºæ–‡é¢˜ç›®ï¼šHAHAï¼šä¸€ç§å¯åŠ¨ç”»çš„äººç±»åŒ–èº«ç”Ÿæˆæ–¹æ³•</li><li>ä½œè€…ï¼šDavid Svitovã€Michael ZollhÃ¶ferã€Angjoo Kanazawaã€Eric Horvitzã€Mehmet Ercan Aksan</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šå¾®è½¯ç ”ç©¶é™¢ï¼ˆç¾å›½ï¼‰</li><li>å…³é”®è¯ï¼šHuman avatar, Full-body, Gaussians platting, Textures</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.09880Github ä»£ç é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šéšç€è®¡ç®—æœºè§†è§‰å’Œå›¾å½¢å­¦çš„å‘å±•ï¼Œç”Ÿæˆå¯åŠ¨ç”»çš„äººç±»åŒ–èº«å·²æˆä¸ºä¸€é¡¹é‡è¦çš„ä»»åŠ¡ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä½¿ç”¨é«˜æ–¯ä½“ç´ æˆ–çº¹ç†ç½‘æ ¼æ¥è¡¨ç¤ºåŒ–èº«ï¼Œä½†è¿™äº›æ–¹æ³•åœ¨æ•ˆç‡å’Œä¿çœŸåº¦æ–¹é¢å­˜åœ¨æƒè¡¡ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•ï¼šè¿‡å»çš„æ–¹æ³•è¦ä¹ˆä½¿ç”¨é«˜æ–¯ä½“ç´ å®ç°é«˜æ•ˆæ¸²æŸ“ï¼Œä½†ä¿çœŸåº¦è¾ƒä½ï¼›è¦ä¹ˆä½¿ç”¨çº¹ç†ç½‘æ ¼å®ç°é«˜ä¿çœŸåº¦ï¼Œä½†æ¸²æŸ“æ•ˆç‡è¾ƒä½ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º HAHA çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡å­¦ä¹ é«˜æ–¯ä½“ç´ å’Œçº¹ç†ç½‘æ ¼çš„æƒè¡¡ï¼Œç”Ÿæˆå¯åŠ¨ç”»çš„äººç±»åŒ–èº«ã€‚HAHA ä½¿ç”¨é«˜æ–¯ä½“ç´ è¡¨ç¤ºåŒ–èº«ä¸­éš¾ä»¥ç”¨ç½‘æ ¼è¡¨ç¤ºçš„åŒºåŸŸï¼Œä¾‹å¦‚å¤´å‘å’Œéç½‘æ ¼æœè£…ï¼Œè€Œä½¿ç”¨çº¹ç†ç½‘æ ¼è¡¨ç¤ºåŒ–èº«ä¸­æ˜“äºç”¨ç½‘æ ¼è¡¨ç¤ºçš„åŒºåŸŸã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šåœ¨ SnapshotPeople å’Œ X-Humans ä¸¤ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒHAHA åœ¨é‡å»ºè´¨é‡ä¸Šä¸ç°æœ‰æ–¹æ³•ç›¸å½“ï¼ŒåŒæ—¶ä½¿ç”¨çš„é«˜æ–¯ä½“ç´ æ•°é‡å‡å°‘äº†ä¸‰åˆ†ä¹‹ä¸€ä»¥ä¸Šã€‚åœ¨ X-Humans æ•°æ®é›†ä¸Šï¼ŒHAHA åœ¨æ–°å§¿åŠ¿ä¸Šçš„æ€§èƒ½ä¼˜äºä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œæ— è®ºæ˜¯åœ¨å®šé‡è¿˜æ˜¯å®šæ€§æ–¹é¢ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒHAHA èƒ½å¤Ÿæœ‰æ•ˆåœ°å¹³è¡¡æ•ˆç‡å’Œä¿çœŸåº¦ï¼Œç”Ÿæˆé«˜è´¨é‡çš„å¯åŠ¨ç”»äººç±»åŒ–èº«ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1): é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡ä¼˜åŒ–å±€éƒ¨é«˜æ–¯å˜æ¢ Î¼jiã€rjiã€sji å’Œé¢œè‰² cji æ¥è®­ç»ƒ 3D é«˜æ–¯ä½“ç´  (GS) è¡¨ç¤ºã€‚(2): ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨å¯å¾®åˆ†å…‰æ …åŒ–å™¨æ¸²æŸ“å…·æœ‰å¯è®­ç»ƒçº¹ç†çš„ SMPL-X ç½‘æ ¼ã€‚(3): æœ€åï¼Œæˆ‘ä»¬åˆå¹¶å¯å¾®åˆ†æ¸²æŸ“çº¹ç†ç½‘æ ¼å’Œå¯å¾®åˆ† 3D GS è¿‡ç¨‹ï¼Œè®­ç»ƒé«˜æ–¯ä½“ç´ çš„ä¸é€æ˜åº¦ oji å’Œé¢œè‰² cjiã€‚</p></li><li><p>ç»“è®ºï¼š(1): æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºHAHAçš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡å­¦ä¹ é«˜æ–¯ä½“ç´ å’Œçº¹ç†ç½‘æ ¼çš„æƒè¡¡ï¼Œç”Ÿæˆå¯åŠ¨ç”»çš„äººç±»åŒ–èº«ã€‚HAHAåœ¨é‡å»ºè´¨é‡ä¸Šä¸ç°æœ‰æ–¹æ³•ç›¸å½“ï¼ŒåŒæ—¶ä½¿ç”¨çš„é«˜æ–¯ä½“ç´ æ•°é‡å‡å°‘äº†ä¸‰åˆ†ä¹‹ä¸€ä»¥ä¸Šã€‚åœ¨X-Humansæ•°æ®é›†ä¸Šï¼ŒHAHAåœ¨æ–°å§¿åŠ¿ä¸Šçš„æ€§èƒ½ä¼˜äºä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œæ— è®ºæ˜¯åœ¨å®šé‡è¿˜æ˜¯å®šæ€§æ–¹é¢ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒHAHAèƒ½å¤Ÿæœ‰æ•ˆåœ°å¹³è¡¡æ•ˆç‡å’Œä¿çœŸåº¦ï¼Œç”Ÿæˆé«˜è´¨é‡çš„å¯åŠ¨ç”»äººç±»åŒ–èº«ã€‚(2): åˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•æ¥ç”Ÿæˆå¯åŠ¨ç”»çš„äººç±»åŒ–èº«ï¼Œè¯¥æ–¹æ³•é€šè¿‡å­¦ä¹ é«˜æ–¯ä½“ç´ å’Œçº¹ç†ç½‘æ ¼çš„æƒè¡¡æ¥å¹³è¡¡æ•ˆç‡å’Œä¿çœŸåº¦ã€‚</li><li>è¯¥æ–¹æ³•åœ¨é‡å»ºè´¨é‡ä¸Šä¸ç°æœ‰æ–¹æ³•ç›¸å½“ï¼ŒåŒæ—¶ä½¿ç”¨çš„é«˜æ–¯ä½“ç´ æ•°é‡å‡å°‘äº†ä¸‰åˆ†ä¹‹ä¸€ä»¥ä¸Šã€‚</li><li>è¯¥æ–¹æ³•åœ¨æ–°å§¿åŠ¿ä¸Šçš„æ€§èƒ½ä¼˜äºä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œæ— è®ºæ˜¯åœ¨å®šé‡è¿˜æ˜¯å®šæ€§æ–¹é¢ã€‚æ€§èƒ½ï¼š</li><li>åœ¨SnapshotPeopleå’ŒX-Humansä¸¤ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒHAHAåœ¨é‡å»ºè´¨é‡ä¸Šä¸ç°æœ‰æ–¹æ³•ç›¸å½“ï¼ŒåŒæ—¶ä½¿ç”¨çš„é«˜æ–¯ä½“ç´ æ•°é‡å‡å°‘äº†ä¸‰åˆ†ä¹‹ä¸€ä»¥ä¸Šã€‚</li><li>åœ¨X-Humansæ•°æ®é›†ä¸Šï¼ŒHAHAåœ¨æ–°å§¿åŠ¿ä¸Šçš„æ€§èƒ½ä¼˜äºä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œæ— è®ºæ˜¯åœ¨å®šé‡è¿˜æ˜¯å®šæ€§æ–¹é¢ã€‚å·¥ä½œé‡ï¼š</li><li>è¯¥æ–¹æ³•çš„è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºã€‚</li><li>è¯¥æ–¹æ³•éœ€è¦å¤§é‡çš„æ•°æ®æ¥è®­ç»ƒï¼Œè¿™å¯èƒ½æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-b477b212869e9f8635da6f1012719fb9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-aa6e5f76fe253938286549f8ee5205fd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4f97970c093585e18e2db42fb96a6b75.jpg" align="middle"></details><h2 id="TexVocab-Texture-Vocabulary-conditioned-Human-Avatars-1"><a href="#TexVocab-Texture-Vocabulary-conditioned-Human-Avatars-1" class="headerlink" title="TexVocab: Texture Vocabulary-conditioned Human Avatars"></a>TexVocab: Texture Vocabulary-conditioned Human Avatars</h2><p><strong>Authors:Yuxiao Liu, Zhe Li, Yebin Liu, Haoqian Wang</strong></p><p>To adequately utilize the available image evidence in multi-view video-based avatar modeling, we propose TexVocab, a novel avatar representation that constructs a texture vocabulary and associates body poses with texture maps for animation. Given multi-view RGB videos, our method initially back-projects all the available images in the training videos to the posed SMPL surface, producing texture maps in the SMPL UV domain. Then we construct pairs of human poses and texture maps to establish a texture vocabulary for encoding dynamic human appearances under various poses. Unlike the commonly used joint-wise manner, we further design a body-part-wise encoding strategy to learn the structural effects of the kinematic chain. Given a driving pose, we query the pose feature hierarchically by decomposing the pose vector into several body parts and interpolating the texture features for synthesizing fine-grained human dynamics. Overall, our method is able to create animatable human avatars with detailed and dynamic appearances from RGB videos, and the experiments show that our method outperforms state-of-the-art approaches. The project page can be found at <a href="https://texvocab.github.io/">https://texvocab.github.io/</a>. </p><p><a href="http://arxiv.org/abs/2404.00524v1">PDF</a> </p><p><strong>Summary</strong><br>åŸºäºå¤šè§†è§’è§†é¢‘åˆ›å»ºé€¼çœŸçš„åŒ–èº«æ¨¡å‹ï¼ŒTexVocab æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºçº¹ç†è¯æ±‡çš„åŒ–èº«è¡¨å¾ï¼Œå°†äººä½“å§¿åŠ¿ä¸ç”¨äºåŠ¨ç”»çš„çº¹ç†è´´å›¾è”ç³»èµ·æ¥ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºäº†ä¸€ç§æ–°çš„åŒ–èº«è¡¨å¾ TexVocabï¼Œç”¨äºä»å¤šè§†è§’ RGB è§†é¢‘åˆ›å»ºé€¼çœŸçš„åŒ–èº«æ¨¡å‹ã€‚</li><li>TexVocab æ„å»ºäº†ä¸€ä¸ªçº¹ç†è¯æ±‡ï¼Œå°†èº«ä½“å§¿åŠ¿ä¸çº¹ç†è´´å›¾è”ç³»èµ·æ¥ï¼Œç”¨äºåŠ¨ç”»ã€‚</li><li>å°†æ‰€æœ‰å¯ç”¨å›¾åƒåæŠ•å½±åˆ°å§¿åŠ¿åŒ– SMPL æ›²é¢ä¸Šï¼Œç”Ÿæˆ SMPL UV åŸŸä¸­çš„çº¹ç†è´´å›¾ã€‚</li><li>æ„å»ºäººä½“å§¿åŠ¿å’Œçº¹ç†è´´å›¾å¯¹ï¼Œå»ºç«‹çº¹ç†è¯æ±‡ï¼Œä»¥å¯¹å„ç§å§¿åŠ¿ä¸‹çš„åŠ¨æ€äººç±»å¤–è§‚è¿›è¡Œç¼–ç ã€‚</li><li>è®¾è®¡äº†ä¸€ä¸ªåŸºäºèº«ä½“éƒ¨ä½çš„ç¼–ç ç­–ç•¥ï¼Œä»¥å­¦ä¹ è¿åŠ¨é“¾çš„ç»“æ„æ•ˆåº”ã€‚</li><li>ç»™å®šä¸€ä¸ªé©±åŠ¨å§¿åŠ¿ï¼Œé€šè¿‡å°†å§¿åŠ¿å‘é‡åˆ†è§£æˆå‡ ä¸ªèº«ä½“éƒ¨ä½å¹¶æ’å€¼çº¹ç†ç‰¹å¾æ¥åˆ†çº§æŸ¥è¯¢å§¿åŠ¿ç‰¹å¾ï¼Œåˆæˆç»†ç²’åº¦çš„äººä½“åŠ¨æ€ã€‚</li><li>åœ¨ RGB è§†é¢‘ä¸­åˆ›å»ºå…·æœ‰è¯¦ç»†åŠ¨æ€å¤–è§‚çš„å¯åŠ¨ç”»äººä½“åŒ–èº«ï¼Œå®éªŒè¡¨æ˜è¯¥æ–¹æ³•ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>é¢˜ç›®ï¼šTexVocabï¼šçº¹ç†è¯å…¸æ¡ä»¶ä¸‹çš„äººä½“è™šæ‹ŸåŒ–èº«</li><p></p><p></p><li>ä½œè€…ï¼šåˆ˜ç…œéœ„ã€æå“²ã€åˆ˜ä¸šå½¬ã€ç‹æµ©å€©</li><p></p><p></p><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šæ·±åœ³å›½é™…ç ”ç©¶ç”Ÿé™¢ï¼Œæ¸…åå¤§å­¦</li><p></p><p></p><li>å…³é”®è¯ï¼šäººä½“è™šæ‹ŸåŒ–èº«ã€çº¹ç†è¯å…¸ã€å¤šè§†è§’è§†é¢‘ã€æ¡ä»¶ç¥ç»è¾å°„åœº</li><p></p><p></p><li>è®ºæ–‡é“¾æ¥ï¼šNoneï¼ŒGithub ä»£ç é“¾æ¥ï¼šhttps://texvocab.github.io/</li><p></p><p></p><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼š    äººä½“è™šæ‹ŸåŒ–èº«å»ºæ¨¡åœ¨ AR/VR åº”ç”¨ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†å¦‚ä½•æœ‰æ•ˆå­¦ä¹ é©±åŠ¨ä¿¡å·å’ŒåŠ¨æ€å¤–è§‚ä¹‹é—´çš„æ˜ å°„ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li><br>&lt;/ol&gt;<p></p><p>ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼š    ä»¥å¾€æ–¹æ³•é€šå¸¸ç›´æ¥å°†å§¿æ€è¾“å…¥æ˜ å°„åˆ°äººä½“å¤–è§‚ï¼Œä½†å§¿æ€è¾“å…¥ä¸åŒ…å«ä»»ä½•åŠ¨æ€äººä½“å¤–è§‚ä¿¡æ¯ï¼Œå¯¼è‡´ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰éš¾ä»¥ä»…ä»å§¿æ€è¾“å…¥ä¸­å›å½’é«˜ä¿çœŸåŠ¨æ€ç»†èŠ‚ã€‚è™½ç„¶ä¸€äº›å·¥ä½œæå‡ºè‡ªåŠ¨è§£ç æ½œåœ¨åµŒå…¥æ¥å¯¹è¾“å…¥ç«¯çš„åŠ¨æ€å¤–è§‚è¿›è¡Œç¼–ç ï¼Œä½†å®ƒä»¬ä»ç„¶å—é™äºå…¨å±€ä»£ç æˆ–ç‰¹å¾çº¿çš„è¡¨ç¤ºèƒ½åŠ›ï¼Œå¯¼è‡´åˆæˆçš„è™šæ‹ŸåŒ–èº«æ¨¡ç³Šã€‚</p><p>ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼š    æœ¬æ–‡æå‡º TexVocabï¼Œä¸€ç§çº¹ç†è¯å…¸ï¼Œå……åˆ†åˆ©ç”¨æ˜¾å¼å›¾åƒè¯æ®æ¥æŒ‡å¯¼éšå¼æ¡ä»¶ NeRF ä»è¡¨è¾¾çº¹ç†æ¡ä»¶ä¸­å­¦ä¹ åŠ¨æ€ã€‚ä¸ºäº†å°†å¤šè§†è§’å›¾åƒä¸åŠ¨æ€äººä½“å…³è”èµ·æ¥ï¼Œå°†æ‰€æœ‰å¯ç”¨å›¾åƒåæŠ•å½±åˆ°ç›¸åº”çš„è®­ç»ƒå§¿æ€ä¸Šï¼Œåœ¨ SMPL UV åŸŸä¸­ç”Ÿæˆçº¹ç†è´´å›¾ã€‚ç„¶åæ„å»ºäººä½“å§¿æ€å’Œçº¹ç†è´´å›¾å¯¹ï¼Œå»ºç«‹çº¹ç†è¯å…¸ï¼Œç”¨äºç¼–ç åœ¨ä¸åŒå§¿æ€ä¸‹çš„åŠ¨æ€äººä½“å¤–è§‚ã€‚ä¸å¸¸ç”¨çš„å…³èŠ‚æ–¹å¼ä¸åŒï¼Œæœ¬æ–‡è¿›ä¸€æ­¥è®¾è®¡äº†èº«ä½“éƒ¨ä½ç¼–ç ç­–ç•¥ï¼Œä»¥å­¦ä¹ è¿åŠ¨é“¾çš„ç»“æ„å½±å“ã€‚ç»™å®šä¸€ä¸ªé©±åŠ¨å§¿æ€ï¼Œé€šè¿‡å°†å§¿æ€å‘é‡åˆ†è§£æˆå¤šä¸ªèº«ä½“éƒ¨ä½å¹¶å¯¹çº¹ç†ç‰¹å¾è¿›è¡Œæ’å€¼ï¼Œåˆ†å±‚æŸ¥è¯¢å§¿æ€ç‰¹å¾ï¼Œä»¥åˆæˆç»†ç²’åº¦çš„åŠ¨æ€äººä½“ã€‚</p><p>ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼š    æœ¬æ–‡æ–¹æ³•èƒ½å¤Ÿä» RGB è§†é¢‘åˆ›å»ºå…·æœ‰è¯¦ç»†åŠ¨æ€å¤–è§‚çš„åŠ¨ç”»è™šæ‹ŸåŒ–èº«ï¼Œå®éªŒè¡¨æ˜è¯¥æ–¹æ³•ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p><ol><li><strong>æ–¹æ³•</strong>ï¼š(1): æå‡º <strong>çº¹ç†è¯å…¸ï¼ˆTexVocabï¼‰</strong>ï¼Œåˆ©ç”¨æ˜¾å¼å›¾åƒè¯æ®æŒ‡å¯¼éšå¼æ¡ä»¶ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ä»çº¹ç†æ¡ä»¶ä¸­å­¦ä¹ åŠ¨æ€ã€‚(2): å°†å¤šè§†è§’å›¾åƒåæŠ•å½±åˆ°ç›¸åº”çš„è®­ç»ƒå§¿æ€ä¸Šï¼Œåœ¨ <strong>SMPLUV</strong> åŸŸä¸­ç”Ÿæˆçº¹ç†è´´å›¾ï¼Œæ„å»º <strong>å§¿æ€-çº¹ç†è´´å›¾å¯¹</strong>ï¼Œå½¢æˆçº¹ç†è¯å…¸ã€‚(3): è®¾è®¡ <strong>èº«ä½“éƒ¨ä½ç¼–ç ç­–ç•¥</strong>ï¼Œå­¦ä¹ è¿åŠ¨é“¾çš„ç»“æ„å½±å“ï¼Œåˆ†å±‚æŸ¥è¯¢å§¿æ€ç‰¹å¾ï¼Œåˆæˆç»†ç²’åº¦çš„åŠ¨æ€äººä½“ã€‚</li></ol><p>8.ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºTexVocabæ–¹æ³•ï¼Œåˆ©ç”¨çº¹ç†è¯å…¸æŒ‡å¯¼éšå¼æ¡ä»¶NeRFä»çº¹ç†æ¡ä»¶ä¸­å­¦ä¹ åŠ¨æ€ï¼Œå®ç°äº†ä»RGBè§†é¢‘åˆ›å»ºå…·æœ‰è¯¦ç»†åŠ¨æ€å¤–è§‚çš„åŠ¨ç”»è™šæ‹ŸåŒ–èº«ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š* æå‡ºçº¹ç†è¯å…¸ï¼Œåˆ©ç”¨æ˜¾å¼å›¾åƒè¯æ®æŒ‡å¯¼éšå¼æ¡ä»¶NeRFå­¦ä¹ åŠ¨æ€ã€‚* è®¾è®¡èº«ä½“éƒ¨ä½ç¼–ç ç­–ç•¥ï¼Œå­¦ä¹ è¿åŠ¨é“¾çš„ç»“æ„å½±å“ï¼Œåˆ†å±‚æŸ¥è¯¢å§¿æ€ç‰¹å¾ï¼Œåˆæˆç»†ç²’åº¦çš„åŠ¨æ€äººä½“ã€‚æ€§èƒ½ï¼š* èƒ½å¤Ÿä»RGBè§†é¢‘åˆ›å»ºå…·æœ‰è¯¦ç»†åŠ¨æ€å¤–è§‚çš„åŠ¨ç”»è™šæ‹ŸåŒ–èº«ã€‚* å®éªŒè¡¨æ˜è¯¥æ–¹æ³•ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å·¥ä½œé‡ï¼š* éœ€è¦æ„å»ºçº¹ç†è¯å…¸ï¼ŒåæŠ•å½±å¤šè§†è§’å›¾åƒå¹¶ç”Ÿæˆçº¹ç†è´´å›¾ã€‚* éœ€è¦è®¾è®¡èº«ä½“éƒ¨ä½ç¼–ç ç­–ç•¥ï¼Œåˆ†å±‚æŸ¥è¯¢å§¿æ€ç‰¹å¾ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-d5ef7cbf30d33e46f182e7d8ab7d3f6a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2b1fc6c5e328a61ca82826325adda143.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-274cb92781d5841c23c9d32fbb82c09e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0fb1457171dba2509eedb86dc74d5975.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f2d2ccbdda2a0bc805f942fdcc2c0204.jpg" align="middle"></details></ol>]]></content>
    
    
    <summary type="html">å…ƒå®‡å®™/è™šæ‹Ÿäºº æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-04-06  GeneAvatar Generic Expression-Aware Volumetric Head Avatar Editing from   a Single Image</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="å…ƒå®‡å®™/è™šæ‹Ÿäºº" scheme="https://kedreamix.github.io/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/"/>
    
  </entry>
  
</feed>
