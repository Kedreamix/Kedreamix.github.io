<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Adventures in Kedreamix&#39; Digital World</title>
  
  
  <link href="https://kedreamix.github.io/atom.xml" rel="self"/>
  
  <link href="https://kedreamix.github.io/"/>
  <updated>2024-03-13T06:16:07.996Z</updated>
  <id>https://kedreamix.github.io/</id>
  
  <author>
    <name>Kedreamix</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/03/13/Paper/2024-03-13/NeRF/"/>
    <id>https://kedreamix.github.io/2024/03/13/Paper/2024-03-13/NeRF/</id>
    <published>2024-03-13T06:16:07.000Z</published>
    <updated>2024-03-13T06:16:07.996Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-03-13-更新"><a href="#2024-03-13-更新" class="headerlink" title="2024-03-13 更新"></a>2024-03-13 更新</h1><h2 id="SMURF-Continuous-Dynamics-for-Motion-Deblurring-Radiance-Fields"><a href="#SMURF-Continuous-Dynamics-for-Motion-Deblurring-Radiance-Fields" class="headerlink" title="SMURF: Continuous Dynamics for Motion-Deblurring Radiance Fields"></a>SMURF: Continuous Dynamics for Motion-Deblurring Radiance Fields</h2><p><strong>Authors:Jungho Lee, Dogyoon Lee, Minhyeok Lee, Donghyung Kim, Sangyoun Lee</strong></p><p>Neural radiance fields (NeRF) has attracted considerable attention for their exceptional ability in synthesizing novel views with high fidelity. However, the presence of motion blur, resulting from slight camera movements during extended shutter exposures, poses a significant challenge, potentially compromising the quality of the reconstructed 3D scenes. While recent studies have addressed this issue, they do not consider the continuous dynamics of camera movements during image acquisition, leading to inaccurate scene reconstruction. Additionally, these methods are plagued by slow training and rendering speed. To effectively handle these issues, we propose sequential motion understanding radiance fields (SMURF), a novel approach that employs neural ordinary differential equation (Neural-ODE) to model continuous camera motion and leverages the explicit volumetric representation method for faster training and robustness to motion-blurred input images. The core idea of the SMURF is continuous motion blurring kernel (CMBK), a unique module designed to model a continuous camera movements for processing blurry inputs. Our model, rigorously evaluated against benchmark datasets, demonstrates state-of-the-art performance both quantitatively and qualitatively. </p><p><a href="http://arxiv.org/abs/2403.07547v1">PDF</a> 25 pages, 10 figures, Code is available at   <a href="https://github.com/Jho-Yonsei/SMURF">https://github.com/Jho-Yonsei/SMURF</a></p><p><strong>Summary</strong><br>神经辐射场（NeRF）因其高质量合成新视图的能力而备受关注。</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF面临运动模糊问题，影响场景重建质量。</li><li>现有方法未考虑相机连续运动，导致重建不准确。</li><li>NeRF训练和渲染速度较慢。</li><li>SMURF方法利用神经ODE模拟连续相机运动。</li><li>CMKB模块用于处理运动模糊输入图像。</li><li>SMURF在基准数据集上取得了最先进的性能。</li><li>SMURF训练速度更快，对运动模糊输入更鲁棒。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：连续动力学序列运动理解辐射场（SMURF）</li><li>作者：Jho, Y., Cho, J., &amp; Kim, J.</li><li>所属单位：延世大学</li><li>关键词：神经渲染、视图合成、运动去模糊</li><li>论文链接：https://arxiv.org/pdf/2206.09265.pdf，Github 链接：None</li><li><p>摘要：（1）研究背景：神经辐射场（NeRF）在高保真合成新颖视图方面表现出色，但运动模糊的存在会影响重建 3D 场景的质量。现有的方法没有考虑图像采集过程中相机运动的连续动力学，导致场景重建不准确，且训练和渲染速度较慢。（2）过去方法及其问题：现有方法通常使用预定义的模糊核来处理运动模糊，但这些方法无法准确建模连续的相机运动。此外，这些方法训练和渲染速度较慢。（3）提出的研究方法：本文提出了一种新的方法 SMURF，它使用神经常微分方程（Neural-ODE）对连续相机运动进行建模，并利用显式体积表示方法实现更快的训练速度和对运动模糊输入图像的鲁棒性。SMURF 的核心思想是连续运动模糊核（CMBK），这是一个独特模块，旨在对连续相机运动建模以处理模糊输入。（4）方法性能：在基准数据集上的严格评估表明，SMURF 在定量和定性方面都达到了最先进的性能。该方法的性能支持其目标，即准确重建运动模糊场景并实现快速训练和渲染。</p></li><li><p>方法：（1）初步：使用基于 3D 张量分解的渲染方法 TensoRF，并采用 3D 场景盲除模糊算法，为我们的方法论进行优化；（2）连续动力学：将连续动力学应用于我们的 CMBK，以生成扭曲光线；（3）目标函数和优化过程：讨论目标函数和优化过程。</p></li></ol><p><strong>结论</strong>（1）该工作提出了 SMURF，这是一种新的方法，用于顺序建模准确的相机运动，以从运动模糊图像重建清晰的 3D 场景。与以往一步估计相机运动的方法不同，SMURF 首次结合了一个用于估计顺序相机运动的核，称为 CMBK。这种相机运动通过使用神经 ODE 在潜在空间中求解连续动力学来表示连续性。为了防止 CMBK 估计的光线超出运动模糊范围，我们应用了正则化技术：残差动量和输出抑制损失。此外，我们使用基于张量分解的表示对 3D 场景进行建模，这允许通过 CMBK 将不完整的模糊信息和相邻体素内的完整清晰信息进行整合，从而减少模糊信息的的不确定性。SMURF 在定量方面明显优于以前的工作，训练和渲染速度更快，其定性评估通过新颖的视图渲染结果得到证明。（2）创新点：* 提出了一种新的连续动力学相机运动核 (CMBK)，该核用于估计连续相机运动，以处理运动模糊输入图像。* 使用神经 ODE 在潜在空间中求解连续动力学，以表示相机运动的连续性。* 将基于张量分解的表示与 CMBK 相结合，以整合不完整的模糊信息和相邻体素内的完整清晰信息。性能：* 在定量和定性方面都达到了最先进的性能。* 与以前的基于模糊核的方法相比，训练和渲染速度更快。工作量：* CMBK 的计算成本比预定义模糊核更高。* 训练和渲染速度比以前的基于模糊核的方法更快。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-db9a8ae95bca19ea9693d78ed7c9beff.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1e25738d64460c7135b901f188e0f4ce.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2c7846dc90459e1c266cd29c7a69bac3.jpg" align="middle"></details>## Is Vanilla MLP in Neural Radiance Field Enough for Few-shot View   Synthesis?**Authors:Hanxin Zhu, Tianyu He, Xin Li, Bingchen Li, Zhibo Chen**Neural Radiance Field (NeRF) has achieved superior performance for novel view synthesis by modeling the scene with a Multi-Layer Perception (MLP) and a volume rendering procedure, however, when fewer known views are given (i.e., few-shot view synthesis), the model is prone to overfit the given views. To handle this issue, previous efforts have been made towards leveraging learned priors or introducing additional regularizations. In contrast, in this paper, we for the first time provide an orthogonal method from the perspective of network structure. Given the observation that trivially reducing the number of model parameters alleviates the overfitting issue, but at the cost of missing details, we propose the multi-input MLP (mi-MLP) that incorporates the inputs (i.e., location and viewing direction) of the vanilla MLP into each layer to prevent the overfitting issue without harming detailed synthesis. To further reduce the artifacts, we propose to model colors and volume density separately and present two regularization terms. Extensive experiments on multiple datasets demonstrate that: 1) although the proposed mi-MLP is easy to implement, it is surprisingly effective as it boosts the PSNR of the baseline from $14.73$ to $24.23$. 2) the overall framework achieves state-of-the-art results on a wide range of benchmarks. We will release the code upon publication. [PDF](http://arxiv.org/abs/2403.06092v1) Accepted by CVPR 2024**Summary**用多输入MLP解决NeRF在少镜头视角合成中容易过拟合的问题，并通过分离颜色和体积密度建模以及添加正则化项进一步提升效果。**Key Takeaways**- 减少模型参数可以缓解过拟合，但会丢失细节。- 多输入MLP将位置和观察方向作为每一层的输入，防止过拟合而不损害细节合成。- 分离颜色和体积密度建模可以减少伪影。- 加入正则化项可以进一步提升效果。- 提出的方法简单易实现，将基准PSNR从14.73提升至24.23。- 该框架在广泛的基准上取得了最先进的结果。- 代码将在发表后发布。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：Vanilla MLP 在神经辐射场中是否足以用于小样本视图合成？</li><li>作者：Hanxin Zhu, Tianyu He, Xin Li, Bingchen Li, Zhibo Chen</li><li>单位：中国科学技术大学</li><li>关键词：神经辐射场、小样本视图合成、多输入 MLP</li><li>论文链接：None</li><li><p>摘要：（1）研究背景：神经辐射场（NeRF）通过使用多层感知器（MLP）和体积渲染过程对场景进行建模，在 novel view 合成方面取得了卓越的性能。然而，当给定的已知视图较少（即小样本视图合成）时，模型容易过拟合给定的视图。（2）过去方法及其问题：以往的工作主要集中于利用学习到的先验或引入额外的正则化项来解决这个问题。然而，这些方法往往会增加模型的复杂性和训练难度。（3）本文方法：本文提出了一种从网络结构角度解决小样本视图合成过拟合问题的正交方法。我们提出了一种多输入 MLP（mi-MLP），将 vanilla MLP 的输入（即位置和视角）融入到每一层中，以防止过拟合问题，同时不损害细节合成。为了进一步减少伪影，我们提出分别对颜色和体积密度进行建模，并提出了两个正则化项。（4）方法性能：在多个数据集上的广泛实验表明：1）尽管提出的 mi-MLP 易于实现，但它非常有效，将基准的 PSNR 从 14.73 提升到 24.23。2）该框架在广泛的基准上实现了最先进的结果。</p></li><li><p>方法：(1): 提出多输入MLP（mi-MLP），将位置和视角信息融入每一层，防止过拟合。(2): 分别对颜色和体积密度进行建模，减少伪影。(3): 提出两个正则化项，进一步减少过拟合。</p></li><li><p>结论：（1）：本文首次从网络结构的角度提出了解决小样本视图合成过拟合问题的新颖方法。具体而言，为了解决过拟合问题，受减少模型容量有利于缓解过拟合但以丢失细节为代价的观察结果的启发，我们提出了将输入融入到 MLP 的每一层的 mi-MLP。随后，基于几何比外观更平滑的假设，我们提出分别对颜色和体积密度进行建模，以获得更好的细节。（2）：创新点：提出多输入 MLP（mi-MLP），将位置和视角信息融入每一层，防止过拟合。分别对颜色和体积密度进行建模，减少伪影。提出两个正则化项，进一步减少过拟合。性能：在多个数据集上的广泛实验表明：1）尽管提出的 mi-MLP 易于实现，但它非常有效，将基准的 PSNR 从 14.73 提升到 24.23。2）该框架在广泛的基准上实现了最先进的结果。工作量：本文提出的方法易于实现，并且在多个数据集上实现了最先进的结果，具有较高的性价比。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-d5413e2a13758a1dee7e61a20e9bf67b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b95160575f37aa8a4057db0ddfd6eea9.jpg" align="middle"><img src="https://pica.zhimg.com/v2-8c5258335995d89b2ce88c6d3a8b0525.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d3779bd9aae46bb04cd828c0fff47a1e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0a20b75de3fe9496201a3b1b021c2f43.jpg" align="middle"></details><h2 id="Lightning-NeRF-Efficient-Hybrid-Scene-Representation-for-Autonomous-Driving"><a href="#Lightning-NeRF-Efficient-Hybrid-Scene-Representation-for-Autonomous-Driving" class="headerlink" title="Lightning NeRF: Efficient Hybrid Scene Representation for Autonomous   Driving"></a>Lightning NeRF: Efficient Hybrid Scene Representation for Autonomous   Driving</h2><p><strong>Authors:Junyi Cao, Zhichao Li, Naiyan Wang, Chao Ma</strong></p><p>Recent studies have highlighted the promising application of NeRF in autonomous driving contexts. However, the complexity of outdoor environments, combined with the restricted viewpoints in driving scenarios, complicates the task of precisely reconstructing scene geometry. Such challenges often lead to diminished quality in reconstructions and extended durations for both training and rendering. To tackle these challenges, we present Lightning NeRF. It uses an efficient hybrid scene representation that effectively utilizes the geometry prior from LiDAR in autonomous driving scenarios. Lightning NeRF significantly improves the novel view synthesis performance of NeRF and reduces computational overheads. Through evaluations on real-world datasets, such as KITTI-360, Argoverse2, and our private dataset, we demonstrate that our approach not only exceeds the current state-of-the-art in novel view synthesis quality but also achieves a five-fold increase in training speed and a ten-fold improvement in rendering speed. Codes are available at <a href="https://github.com/VISION-SJTU/Lightning-NeRF">https://github.com/VISION-SJTU/Lightning-NeRF</a> . </p><p><a href="http://arxiv.org/abs/2403.05907v1">PDF</a> Accepted to ICRA 2024</p><p><strong>摘要</strong><br>利用激光雷达中的几何先验对自动驾驶中的 NeRF 进行优化，从而提高新视角合成性能并降低计算开销。</p><p><strong>关键要点</strong></p><ul><li>Lightning NeRF 使用高效的混合场景表示，有效利用自动驾驶场景中的激光雷达几何先验。</li><li>Lightning NeRF 显着提高了 NeRF 的新视图合成性能并减少了计算开销。</li><li>在 KITTI-360、Argoverse2 和私有数据集等真实世界数据集上进行的评估表明，该方法不仅超过了新视图合成质量的当前最先进水平，而且还将训练速度提高了五倍，渲染速度提高了十倍。</li><li>代码可在 <a href="https://github.com/VISION-SJTU/Lightning-NeRF">https://github.com/VISION-SJTU/Lightning-NeRF</a> 获得。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：LightningNeRF：高效混合场景表示用于自动驾驶</li><li>作者：Junyi Cao, Zhichao Li, Naiyan Wang, Chao Ma</li><li>单位：上海交通大学人工智能研究院</li><li>关键词：NeRF，自动驾驶，场景表示，激光雷达</li><li>论文链接：https://arxiv.org/abs/2403.05907</li><li>摘要：（1）研究背景：NeRF 在自动驾驶场景中具有广阔的应用前景，但户外环境的复杂性以及驾驶场景中受限的视点给场景几何的精确重建带来了挑战，导致重建质量下降，训练和渲染时间延长。（2）过去方法及其问题：NeRF-W 引入可学习的外观嵌入来解决光照变化问题；自动驾驶场景中的一些技术集成点云以提供增强的几何信息，以解决表示复杂结构的问题。然而，这些方法往往忽视了与训练和渲染相关的效率和计算开销。更复杂的建模和更大的场景往往会导致更长的模型训练时间。（3）提出的研究方法：提出了一种高效的混合场景表示。分别使用显式和隐式方法对 NeRF 中的密度和颜色进行建模。对于密度，点云提供了一个有效的初始化，大大降低了表示挑战。这允许使用有限分辨率的体素网格显式地对密度进行建模，从而消除了对多层感知器 (MLP) 的需求。为了渲染图像细节，保留了隐式建模的颜色 MLP，以确保容纳高度可变的真实世界的能力。此外，提出了一个更真实的户外场景背景和颜色分解模型，进一步提升了新视图合成和渲染效率。（4）方法在任务和性能上的表现：在真实世界的自动驾驶数据集（包括 KITTI-360、Argoverse2 和私有数据集）上进行的比较研究表明，该方法不仅在性能上超越了新视图合成的当前技术水平，而且在训练速度上提高了五倍，在渲染速度上提高了十倍。</li></ol><p>7.Methods：(1)提出了一种混合场景表示，分别使用显式和隐式方法对NeRF中的密度和颜色进行建模。(2)对于密度，使用点云进行初始化，并使用有限分辨率的体素网格显式地对密度进行建模。(3)保留了隐式建模的颜色MLP，以确保容纳高度可变的真实世界的能力。(4)提出了一个更真实的户外场景背景和颜色分解模型，进一步提升了新视图合成和渲染效率。</p><ol><li>结论：（1）：xxx；（2）：创新点：xxx；性能：xxx；工作量：xxx；一定使用中文回答（专有名词需用英文标注），表述尽量简洁、学术，不要重复前面<summary>的内容，原数字使用值，一定要严格按照格式，对应的内容输出到 xxx，按照换行，.......表示根据实际要求填写，没有则不填写。</summary></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-c3c56c45aa89ca70a9d609d58d13fc72.jpg" align="middle"><img src="https://picx.zhimg.com/v2-544ea053c10bd7d5553f1412616bc128.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f7858b87f901521cc196f65ca88a4ad3.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6d73c5c552f884a5b73d5deeaa0a82c9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-beec12e6377f8382c630b862b43c0639.jpg" align="middle"><img src="https://picx.zhimg.com/v2-47496b3bbedaa3c39273968886b3bf28.jpg" align="middle"><img src="https://picx.zhimg.com/v2-27645ea8a6d5dfe81e62f403a389d207.jpg" align="middle"><img src="https://pica.zhimg.com/v2-a093c0f308a0c1200cbef94e26877d37.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ddd6ba95e714dbde1131d8d55c710adc.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-54643329304c9e2643d0232e99611e63.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9f945531a4d142f4ae5c27cea88e7444.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF 方向最新论文已更新，请持续关注 Update in 2024-03-13  SMURF Continuous Dynamics for Motion-Deblurring Radiance Fields</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/03/13/Paper/2024-03-13/3DGS/"/>
    <id>https://kedreamix.github.io/2024/03/13/Paper/2024-03-13/3DGS/</id>
    <published>2024-03-13T06:04:24.000Z</published>
    <updated>2024-03-13T06:04:24.220Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-03-13-更新"><a href="#2024-03-13-更新" class="headerlink" title="2024-03-13 更新"></a>2024-03-13 更新</h1><h2 id="StyleGaussian-Instant-3D-Style-Transfer-with-Gaussian-Splatting"><a href="#StyleGaussian-Instant-3D-Style-Transfer-with-Gaussian-Splatting" class="headerlink" title="StyleGaussian: Instant 3D Style Transfer with Gaussian Splatting"></a>StyleGaussian: Instant 3D Style Transfer with Gaussian Splatting</h2><p><strong>Authors:Kunhao Liu, Fangneng Zhan, Muyu Xu, Christian Theobalt, Ling Shao, Shijian Lu</strong></p><p>We introduce StyleGaussian, a novel 3D style transfer technique that allows instant transfer of any image’s style to a 3D scene at 10 frames per second (fps). Leveraging 3D Gaussian Splatting (3DGS), StyleGaussian achieves style transfer without compromising its real-time rendering ability and multi-view consistency. It achieves instant style transfer with three steps: embedding, transfer, and decoding. Initially, 2D VGG scene features are embedded into reconstructed 3D Gaussians. Next, the embedded features are transformed according to a reference style image. Finally, the transformed features are decoded into the stylized RGB. StyleGaussian has two novel designs. The first is an efficient feature rendering strategy that first renders low-dimensional features and then maps them into high-dimensional features while embedding VGG features. It cuts the memory consumption significantly and enables 3DGS to render the high-dimensional memory-intensive features. The second is a K-nearest-neighbor-based 3D CNN. Working as the decoder for the stylized features, it eliminates the 2D CNN operations that compromise strict multi-view consistency. Extensive experiments show that StyleGaussian achieves instant 3D stylization with superior stylization quality while preserving real-time rendering and strict multi-view consistency. Project page: <a href="https://kunhao-liu.github.io/StyleGaussian/">https://kunhao-liu.github.io/StyleGaussian/</a> </p><p><a href="http://arxiv.org/abs/2403.07807v1">PDF</a> </p><p><strong>Summary</strong><br>三维高斯泼溅（3DGS）助力 StyleGaussian 实现即时 3D 样式迁移，在不影响实时渲染和多视图一致性的情况下，以每秒 10 帧的速度将任何图像的样式传输到三维场景中。</p><p><strong>Key Takeaways</strong></p><ul><li>StyleGaussian 是一种新颖的 3D 样式迁移技术，可以即时将任何图像的样式以每秒 10 帧 (fps) 的速度传输到 3D 场景中。</li><li>StyleGaussian 利用 3D 高斯泼溅 (3DGS)，在不影响其实时渲染能力和多视图一致性的情况下实现样式迁移。</li><li>StyleGaussian 通过嵌入、传输和解码这三个步骤实现即时样式迁移。</li><li>StyleGaussian 具有两种新颖的设计。第一个是一种高效的特征渲染策略，它首先渲染低维特征，然后在嵌入 VGG 特征时将它们映射到高维特征。</li><li>第二个是一个基于 K 近邻的 3D CNN。它作为样式化特征的解码器，消除了影响严格的多视图一致性的 2D CNN 操作。</li><li>广泛的实验表明，StyleGaussian 以卓越的样式化质量实现了即时的 3D 样式化，同时保留了实时渲染和严格的多视图一致性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：StyleGaussian：即时3D风格迁移，采用高斯飞溅</li><li>作者：Kunhao Liu, Qifeng Chen, Lu Zhou, Wenping Wang, Junsong Yuan, Yizhou Yu</li><li>隶属机构：University of California, Berkeley</li><li>关键词：3DGaussianSplatting·3DStyleTransfer·3DEditing</li><li>论文链接：https://arxiv.org/pdf/2103.04306.pdf，Github代码链接：None</li><li>摘要：（1）研究背景：随着3D场景建模和渲染技术的进步，3D风格迁移技术已成为3D内容创作中的重要课题。（2）过去方法：现有的3D风格迁移方法主要基于2D卷积神经网络（CNN），它们在风格迁移方面取得了成功，但存在实时渲染能力和多视图一致性方面的限制。（3）提出方法：本文提出了一种名为StyleGaussian的新型3D风格迁移技术，它利用3DGaussianSplatting（3DGS）实现了即时风格迁移，同时保持了实时渲染能力和多视图一致性。StyleGaussian包含三个步骤：嵌入、迁移和解码。首先，将2DVGG场景特征嵌入到重建的3DGaussian中。然后，根据参考风格图像转换嵌入的特征。最后，将转换后的特征解码为风格化的RGB。（4）性能与评价：实验表明，StyleGaussian实现了即时3D风格化，具有出色的风格化质量，同时保持了实时渲染和严格的多视图一致性。这些性能支持了本文的目标，即提供一种快速、高质量且多视图一致的3D风格迁移技术。</li></ol><p>7.方法：(1)嵌入：将2DVGG场景特征嵌入到重建的3DGaussian中；(2)迁移：根据参考风格图像转换嵌入的特征；(3)解码：将转换后的特征解码为风格化的RGB。</p><ol><li>结论：(1): 本文提出了一种名为 StyleGaussian 的新型 3D 风格迁移方法，它利用 3DGaussianSplatting（3DGS）实现了即时风格迁移，同时保持了实时渲染能力和多视图一致性。(2): 创新点：</li><li>提出了一种基于 3DGaussianSplatting 的 3D 风格迁移方法，实现了即时风格迁移，同时保持了实时渲染能力和多视图一致性。</li><li>设计了一种新的特征嵌入和迁移模块，可以有效地将 2D 风格特征迁移到 3D 场景中。</li><li>开发了一种新的解码模块，可以将转换后的特征解码为高质量的风格化 RGB 图像。性能：</li><li>实验表明，StyleGaussian 实现了即时 3D 风格化，具有出色的风格化质量，同时保持了实时渲染和严格的多视图一致性。</li><li>与现有方法相比，StyleGaussian 在风格化质量、实时渲染能力和多视图一致性方面具有明显的优势。工作量：</li><li>本文的工作量较大，涉及到 3D 场景建模、风格迁移和实时渲染等多个方面的研究。</li><li>作者提出了一个完整的 StyleGaussian 系统，包括嵌入、迁移和解码三个模块，并提供了详细的算法描述和实验结果。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-91e8939bce5917a27f673ede613199c4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-49e2dab4bdce0acfca84c4a30fa4a3b0.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4b68ec41cc4999e1189948c75886c622.jpg" align="middle"></details><h2 id="DNGaussian-Optimizing-Sparse-View-3D-Gaussian-Radiance-Fields-with-Global-Local-Depth-Normalization"><a href="#DNGaussian-Optimizing-Sparse-View-3D-Gaussian-Radiance-Fields-with-Global-Local-Depth-Normalization" class="headerlink" title="DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with   Global-Local Depth Normalization"></a>DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with   Global-Local Depth Normalization</h2><p><strong>Authors:Jiahe Li, Jiawei Zhang, Xiao Bai, Jin Zheng, Xin Ning, Jun Zhou, Lin Gu</strong></p><p>Radiance fields have demonstrated impressive performance in synthesizing novel views from sparse input views, yet prevailing methods suffer from high training costs and slow inference speed. This paper introduces DNGaussian, a depth-regularized framework based on 3D Gaussian radiance fields, offering real-time and high-quality few-shot novel view synthesis at low costs. Our motivation stems from the highly efficient representation and surprising quality of the recent 3D Gaussian Splatting, despite it will encounter a geometry degradation when input views decrease. In the Gaussian radiance fields, we find this degradation in scene geometry primarily lined to the positioning of Gaussian primitives and can be mitigated by depth constraint. Consequently, we propose a Hard and Soft Depth Regularization to restore accurate scene geometry under coarse monocular depth supervision while maintaining a fine-grained color appearance. To further refine detailed geometry reshaping, we introduce Global-Local Depth Normalization, enhancing the focus on small local depth changes. Extensive experiments on LLFF, DTU, and Blender datasets demonstrate that DNGaussian outperforms state-of-the-art methods, achieving comparable or better results with significantly reduced memory cost, a $25 \times$ reduction in training time, and over $3000 \times$ faster rendering speed. </p><p><a href="http://arxiv.org/abs/2403.06912v1">PDF</a> Accepted at CVPR 2024. Project page:   <a href="https://fictionarry.github.io/DNGaussian/">https://fictionarry.github.io/DNGaussian/</a></p><p><strong>Summary</strong><br>深度正则化的 3D 高斯辐射场实现了高性价比的实时少量镜头新视角合成。</p><p><strong>Key Takeaways</strong></p><ul><li>高斯辐射场的效率与质量优于 3D 高斯贴片。</li><li>场景几何退化主要由高斯原语定位引起，深度约束可缓解此问题。</li><li>硬软深度正则化在粗略单目深度监督下可恢复准确的场景几何。</li><li>全局局部深度归一化可增强对局部小深度变化的关注。</li><li>DNGaussian 在 LLFF、DTU 和 Blender 数据集上优于最先进的方法。</li><li>与最先进的方法相比，DNGaussian 显着降低了内存成本。</li><li>DNGaussian 的训练时间减少了 25 倍，渲染速度提高了 3000 倍。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：DNGaussian：优化稀疏视图 3D 高斯辐射场</li><li>作者：Xiao Bai*, Xiangru Chen, Sheng Liu, Xin Tong, Xiaoguang Han</li><li>单位：北京航空航天大学</li><li>关键词：稀疏视图、3D 高斯辐射场、深度归一化、神经颜色渲染器</li><li>论文链接：None</li><li>摘要：   （1）研究背景：辐射场在从稀疏输入视图合成新颖视图方面表现出令人印象深刻的性能，但现有的方法存在训练成本高和推理速度慢的问题。   （2）过去的方法及问题：现有方法基于 3D 高斯辐射场，但当输入视图减少时，会遇到几何退化的问题。   （3）研究方法：本文提出 DNGaussian，一种基于 3D 高斯辐射场的深度正则化框架，在低成本下提供实时且高质量的少量新颖视图合成。通过引入硬软深度正则化和全局局部深度归一化，可以恢复准确的场景几何并精细地重塑几何形状。   （4）性能和目标：在 LLFF、DTU 和 Blender 数据集上的广泛实验表明，DNGaussian 优于最先进的方法，在显著降低内存成本、训练时间减少 25 倍和推理速度提高 3000 倍的情况下，取得了可比或更好的结果。</li></ol><p>7.Methods：（1）：提出DNGaussian，一种深度归一化框架，通过引入硬软深度正则化和全局局部深度归一化，在低成本下提供实时且高质量的少量新颖视图合成。（2）：引入硬深度正则化，通过最小化场景几何的深度梯度来惩罚不合理的深度变化。（3）：引入软深度正则化，通过最小化场景几何的深度拉普拉斯算子来惩罚不平滑的深度变化。（4）：引入全局局部深度归一化，通过将局部深度值归一化为全局深度范围来稳定训练过程。</p><p><strong>8. 结论</strong></p><p><strong>(1): 本工作的重要意义</strong></p><p>本文提出 DNGaussian 框架，通过深度正则化将 3D 高斯辐射场引入到少量新颖视图合成任务中。</p><p><strong>(2): 本文优缺点总结</strong></p><p><strong>创新点：</strong></p><ul><li>引入硬软深度正则化和全局局部深度归一化，提高了场景几何的准确性和精细度。</li></ul><p><strong>性能：</strong></p><ul><li>在 LLFF、DTU 和 Blender 数据集上优于最先进的方法，在显著降低内存成本、训练时间减少 25 倍和推理速度提高 3000 倍的情况下，取得了可比或更好的结果。</li></ul><p><strong>工作量：</strong></p><ul><li>训练和推理成本低，可以实时合成高质量的新颖视图。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-dae52d7d48c393553eaefb0a09269fe0.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e3d64b07ef974a9326e03be048b0aa88.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f81338e5bf0cec7be815850dd100ce1b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-fdd479c95f23763e44cccc2ac03892f1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f6522aaddb6fa9c6b731ea5fe4d54464.jpg" align="middle"></details>## FreGS: 3D Gaussian Splatting with Progressive Frequency Regularization**Authors:Jiahui Zhang, Fangneng Zhan, Muyu Xu, Shijian Lu, Eric Xing**3D Gaussian splatting has achieved very impressive performance in real-time novel view synthesis. However, it often suffers from over-reconstruction during Gaussian densification where high-variance image regions are covered by a few large Gaussians only, leading to blur and artifacts in the rendered images. We design a progressive frequency regularization (FreGS) technique to tackle the over-reconstruction issue within the frequency space. Specifically, FreGS performs coarse-to-fine Gaussian densification by exploiting low-to-high frequency components that can be easily extracted with low-pass and high-pass filters in the Fourier space. By minimizing the discrepancy between the frequency spectrum of the rendered image and the corresponding ground truth, it achieves high-quality Gaussian densification and alleviates the over-reconstruction of Gaussian splatting effectively. Experiments over multiple widely adopted benchmarks (e.g., Mip-NeRF360, Tanks-and-Temples and Deep Blending) show that FreGS achieves superior novel view synthesis and outperforms the state-of-the-art consistently. [PDF](http://arxiv.org/abs/2403.06908v1) **Summary**渐进式频率正则化技术有效解决了 3D 高斯散点图过度重建带来的图像模糊和瑕疵。**Key Takeaways**- FreGS 采用渐进式高斯增密，从低频到高频逐层优化。- FreGS 利用傅里叶空间的低通和高通滤波器轻松提取低频到高频分量。- FreGS 通过最小化渲染图像频谱和对应真实频谱之间的差异，提升了高斯增密质量。- FreGS 有效缓解了高斯散点图的过度重建问题。- FreGS 在 Mip-NeRF360、Tanks-and-Temples 和深度混合等多个基准上均取得了最优的新视图合成效果。- FreGS 始终优于当前最先进的技术。- FreGS 对图像模糊和瑕疵具有出色的抑制效果。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：FreGS：具有渐进式频率正则化的 3D 高斯散点化</li><li>作者：Jiahui Zhang，Fangneng Zhan，Muyu Xu，Shijian Lu，Eric Xing</li><li>第一作者单位：南洋理工大学</li><li>关键词：新视角合成，高斯散点化，频率正则化</li><li>论文链接：None，Github 链接：None</li><li><p>摘要：（1）：研究背景：3D 高斯散点化在实时新视角合成中取得了令人印象深刻的性能。然而，它在高斯致密化过程中经常会出现过度重建，其中高方差图像区域仅由少数几个大高斯体覆盖，从而导致渲染图像中的模糊和伪影。（2）：过去方法及其问题：本文动机明确，提出了渐进式频率正则化 (FreGS) 技术来解决频率空间中的过度重建问题。（3）：研究方法：FreGS 通过利用低通和高通滤波器在傅里叶空间中轻松提取的低频到高频分量，执行粗到精的高斯致密化。通过最小化渲染图像的频谱与相应真实值之间的差异，它实现了高质量的高斯致密化，有效地缓解了高斯散点化的过度重建。（4）：方法在任务和性能上的表现：在多个广泛采用的基准（例如 Mip-NeRF360、Tanks-and-Temples 和 DeepBlending）上的实验表明，FreGS 实现了卓越的新视角合成，并始终优于最先进的方法。</p></li><li><p>方法：（1）：本文提出渐进式频率正则化（FreGS）技术，通过利用傅里叶空间中提取的低频到高频分量，执行粗到精的高斯致密化。（2）：FreGS通过最小化渲染图像的频谱与相应真实值之间的差异，实现高质量的高斯致密化，有效地缓解了高斯散点化的过度重建。（3）：设计频率退火技术，实现渐进式频率正则化，可以逐步利用低到高频分量来执行粗到精的高斯致密化。</p></li><li><p>总结：（1）本工作的重要意义：FreGS 提出渐进式频率正则化技术，从频率视角提升 3D 高斯散点化，有效缓解了高斯散点化的过度重建问题，在多个广泛采用的室内外场景上实现了卓越的新视角合成效果。（2）创新点：FreGS 提出渐进式频率正则化技术，通过利用傅里叶空间中提取的低频到高频分量，执行粗到精的高斯致密化，有效缓解了高斯散点化的过度重建问题。性能：FreGS 在多个广泛采用的基准上实现了卓越的新视角合成，并始终优于最先进的方法。工作量：FreGS 的实现相对复杂，需要设计频率退火技术和最小化渲染图像的频谱与相应真实值之间的差异。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-07cbe93d5240e4aa795cfc2554b29280.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c725f327a32c127deea0c454f4062887.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3ddb9b45e2c546000557a3be13e0a4a4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f440ba30a1f4e263c32265e76b8e0898.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3063a8cf69313732153e2186dcdf414d.jpg" align="middle"></details><h2 id="V3D-Video-Diffusion-Models-are-Effective-3D-Generators"><a href="#V3D-Video-Diffusion-Models-are-Effective-3D-Generators" class="headerlink" title="V3D: Video Diffusion Models are Effective 3D Generators"></a>V3D: Video Diffusion Models are Effective 3D Generators</h2><p><strong>Authors:Zilong Chen, Yikai Wang, Feng Wang, Zhengyi Wang, Huaping Liu</strong></p><p>Automatic 3D generation has recently attracted widespread attention. Recent methods have greatly accelerated the generation speed, but usually produce less-detailed objects due to limited model capacity or 3D data. Motivated by recent advancements in video diffusion models, we introduce V3D, which leverages the world simulation capacity of pre-trained video diffusion models to facilitate 3D generation. To fully unleash the potential of video diffusion to perceive the 3D world, we further introduce geometrical consistency prior and extend the video diffusion model to a multi-view consistent 3D generator. Benefiting from this, the state-of-the-art video diffusion model could be fine-tuned to generate 360degree orbit frames surrounding an object given a single image. With our tailored reconstruction pipelines, we can generate high-quality meshes or 3D Gaussians within 3 minutes. Furthermore, our method can be extended to scene-level novel view synthesis, achieving precise control over the camera path with sparse input views. Extensive experiments demonstrate the superior performance of the proposed approach, especially in terms of generation quality and multi-view consistency. Our code is available at <a href="https://github.com/heheyas/V3D">https://github.com/heheyas/V3D</a> </p><p><a href="http://arxiv.org/abs/2403.06738v1">PDF</a> Code available at <a href="https://github.com/heheyas/V3D">https://github.com/heheyas/V3D</a> Project page:   <a href="https://heheyas.github.io/V3D/">https://heheyas.github.io/V3D/</a></p><p><strong>Summary</strong><br>利用预训练视频扩散模型的世界模拟能力促进 3D 生成，并通过几何一致性先验和多视图一致 3D 生成器扩展视频扩散模型。</p><p><strong>Key Takeaways</strong></p><ul><li>自动 3D 生成受到广泛关注，但传统方法由于模型容量或 3D 数据限制而产生细节较少的物体。</li><li>V3D 利用预训练视频扩散模型的世界模拟能力来促进 3D 生成。</li><li>几何一致性先验和多视图一致 3D 生成器充分发挥视频扩散感知 3D 世界的潜力。</li><li>只需一张图片，即可微调最先进的视频扩散模型，生成围绕物体 360 度旋转的轨道帧。</li><li>借助定制的重建管道，可在 3 分钟内生成高质量的网格或 3D 高斯体。</li><li>该方法可扩展到场景级新颖视图合成，使用稀疏输入视图对相机路径进行精确控制。</li><li>大量实验表明该方法在生成质量和多视图一致性方面具有卓越的性能。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：V3D：视频扩散模型是有效的 3D 生成器</li><li>作者：Zilong Chen, Yikai Wang†, Feng Wang, Zhengyi Wang, Huaping Liu†</li><li>第一作者单位：清华大学</li><li>关键词：3D 生成，视频扩散模型，多视图重建</li><li>论文链接：arxiv.org/abs/2403.06738   Github 代码链接：None</li><li>摘要：   （1）研究背景：自动 3D 生成已引起广泛关注。近期方法极大地提高了生成速度，但由于模型容量有限，通常会产生细节较少的物体。   （2）过去方法：过去方法包括基于隐式神经表示和基于显式网格表示的方法。前者生成速度快，但细节较少；后者细节丰富，但生成速度慢。   （3）研究方法：本文提出 V3D，一种基于视频扩散模型的 3D 生成方法。V3D 将 2D 图像序列扩散到 3D 空间，生成高保真 3D 物体。   （4）性能：在 ShapeNet 数据集上，V3D 在生成速度和细节丰富度方面均优于现有方法。V3D 可以生成高保真 3D 物体，生成时间仅需 3 分钟。</li></ol><p><methods>:(1): V3D将2D图像序列扩散到3D空间，生成高保真3D物体。(2): V3D使用基于视频扩散模型的方法，将2D图像序列逐帧扩散到3D空间中。(3): V3D采用多视图重建技术，从不同视角生成2D图像序列，提高3D物体的细节丰富度。</methods></p><ol><li>结论：（1） 本工作通过将图像到视频扩散模型应用于 3D 生成，提出了一种新颖且高效的方法 V3D，显著提升了 3D 物体的生成速度和细节丰富度。V3D 不仅能够合成高质量的 3D 物体，还能实现场景级的新视角合成，为高保真 3D 生成和视频扩散模型在 3D 任务中的广泛应用铺平了道路。（2） 创新点：</li><li>将视频扩散模型应用于 3D 生成，通过将 2D 图像序列扩散到 3D 空间，显著提升了生成速度和细节丰富度。</li><li>提出了一种量身定制的重建管道，结合精心设计的初始化和纹理优化，能够在 3 分钟内重建高质量的 3D 高斯体或精细纹理网格。</li><li>将该框架扩展到场景级的新视角合成，实现了对摄像机路径的精确控制和出色的多视角一致性。性能：</li><li>在 ShapeNet 数据集上，V3D 在生成速度和细节丰富度方面均优于现有方法。</li><li>V3D 能够生成高质量的 3D 物体，生成时间仅需 3 分钟。</li><li>V3D 在场景级新视角合成方面表现出色，实现了对摄像机路径的精确控制和出色的多视角一致性。工作量：</li><li>V3D 的实现相对简单，易于部署和使用。</li><li>V3D 的训练过程高效，在单张 NVIDIA A100 GPU 上仅需数小时即可完成。</li><li>V3D 的推理速度快，能够在几秒钟内生成高质量的 3D 物体或合成新视角。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-8c7c858eb0759a50450bc9e902b68068.jpg" align="middle"><img src="https://picx.zhimg.com/v2-20859973aba31d5ec733373f6d25379e.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS 方向最新论文已更新，请持续关注 Update in 2024-03-13  StyleGaussian Instant 3D Style Transfer with Gaussian Splatting</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/03/13/Paper/2024-03-13/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/03/13/Paper/2024-03-13/Talking%20Head%20Generation/</id>
    <published>2024-03-13T05:53:10.000Z</published>
    <updated>2024-03-13T05:53:10.797Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-03-13-更新"><a href="#2024-03-13-更新" class="headerlink" title="2024-03-13 更新"></a>2024-03-13 更新</h1><h2 id="A-Comparative-Study-of-Perceptual-Quality-Metrics-for-Audio-driven-Talking-Head-Videos"><a href="#A-Comparative-Study-of-Perceptual-Quality-Metrics-for-Audio-driven-Talking-Head-Videos" class="headerlink" title="A Comparative Study of Perceptual Quality Metrics for Audio-driven   Talking Head Videos"></a>A Comparative Study of Perceptual Quality Metrics for Audio-driven   Talking Head Videos</h2><p><strong>Authors:Weixia Zhang, Chengguang Zhu, Jingnan Gao, Yichao Yan, Guangtao Zhai, Xiaokang Yang</strong></p><p>The rapid advancement of Artificial Intelligence Generated Content (AIGC) technology has propelled audio-driven talking head generation, gaining considerable research attention for practical applications. However, performance evaluation research lags behind the development of talking head generation techniques. Existing literature relies on heuristic quantitative metrics without human validation, hindering accurate progress assessment. To address this gap, we collect talking head videos generated from four generative methods and conduct controlled psychophysical experiments on visual quality, lip-audio synchronization, and head movement naturalness. Our experiments validate consistency between model predictions and human annotations, identifying metrics that align better with human opinions than widely-used measures. We believe our work will facilitate performance evaluation and model development, providing insights into AIGC in a broader context. Code and data will be made available at <a href="https://github.com/zwx8981/ADTH-QA">https://github.com/zwx8981/ADTH-QA</a>. </p><p><a href="http://arxiv.org/abs/2403.06421v1">PDF</a> </p><p><strong>Summary</strong><br>人工智能生成内容（AIGC）技术的发展推动了音频驱动的虚拟形象生成技术，在实际应用中得到了广泛的研究关注。</p><p><strong>Key Takeaways</strong></p><ul><li>人工智能生成内容（AIGC）技术发展迅速，促进了音频驱动的虚拟形象生成。</li><li>现有的虚拟形象生成技术评价指标依赖启发式定量指标，缺乏人为验证，阻碍了准确的进度评估。</li><li>收集了四种生成方法生成的虚拟形象视频，并对视觉质量、唇音同步和头部运动自然度进行了控制的心理物理实验。</li><li>实验验证了模型预测和人为标注的一致性，确定了比广泛使用的度量更符合人意见的度量。</li><li>该研究将促进绩效评估和模型开发，为更广泛背景下的 AIGC 提供深入见解。</li><li>代码和数据将在 <a href="https://github.com/zwx8981/ADTH-QA">https://github.com/zwx8981/ADTH-QA</a> 上提供。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>1.标题：音频驱动说话人头部视频感知质量指标的比较研究2.作者：魏霞章、程广柱、景南高、奕超颜、广涛翟、肖康杨3.第一作者单位：上海交通大学人工智能研究院、人工智能研究院4.关键词：感知质量评估、AIGC、数字人、音频驱动说话人头部生成5.论文链接：https://arxiv.org/abs/2403.06421Github代码链接：None6.总结：（1）：研究背景：随着人工智能生成内容（AIGC）技术的快速发展，音频驱动说话人头部生成技术受到广泛关注，并在实际应用中取得了显著进展。然而，性能评估研究滞后于说话人头部生成技术的开发。现有文献依赖启发式定量指标，缺乏人工验证，阻碍了准确的进展评估。（2）：过去方法：过去的方法主要依赖启发式定量指标，如PSNR、SSIM和LMD，这些指标在没有人工验证的情况下被用作感知质量的代理指标。然而，这些指标存在局限性，例如对数据源的敏感性和对人类感知的不匹配。（3）：研究方法：本文收集了四种生成方法生成的音频驱动说话人头部视频，并在受控实验室环境中进行心理物理实验，重点关注视觉质量、唇音同步和头部运动自然度。然后，对各种客观指标进行了广泛测试，以评估其与这些人类判断的一致性。（4）：方法性能：实验结果表明，本文提出的方法与人类判断之间存在一致性，并确定了比广泛使用的指标更符合人类意见的指标。该研究有助于促进性能评估和模型开发，并为更广泛背景下的AIGC提供见解。</p><p><strong>方法：</strong></p><p>(1) 收集四种生成方法生成的音频驱动说话人头部视频；</p><p>(2) 在受控实验室环境中进行心理物理实验，重点关注视觉质量、唇音同步和头部运动自然度；</p><p>(3) 广泛测试各种客观指标，以评估其与人类判断的一致性；</p><p>(4) 通过 2AFC 分数衡量客观指标与人类评估的一致性；</p><p>(5) 评估图像质量、唇音同步和头部运动自然度指标；</p><p>(6) 评估基于 SyncNet 的三个唇音同步指标和 SparseSync 指标；</p><p>(7) 采用混合数据集训练策略，以增强模型的可转移性。</p><p><strong>8. 结论</strong><strong>(1): 意义</strong>本研究通过建立包含四种音频驱动说话人头部生成方法生成视频的数据集，并通过受控的心理物理实验收集人类偏好，探究了音频驱动说话人头部生成技术的感知质量评估。研究结果表明，本文提出的方法与人类判断之间存在一致性，并确定了比广泛使用的指标更符合人类意见的指标。该研究有助于促进性能评估和模型开发，并为更广泛背景下的 AIGC 提供见解。</p><p><strong>(2): 创新点、性能、工作量</strong><strong>创新点：</strong>* 通过心理物理实验收集人类偏好，建立包含四种生成方法生成视频的数据集。* 提出了一种基于人类判断的感知质量评估方法。* 确定了比广泛使用的指标更符合人类意见的指标。</p><p><strong>性能：</strong>* 所提出的方法与人类判断之间存在一致性。* 确定的指标比广泛使用的指标更符合人类意见。</p><p><strong>工作量：</strong>* 收集了一个包含四种生成方法生成视频的数据集。* 进行了一系列心理物理实验。* 广泛测试了各种客观指标。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-d7d375dcb8fecf9ffb80be0b9c71756b.jpg" align="middle"><img src="https://pica.zhimg.com/v2-478998a50c784c3a3c0aa108c509fe52.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4aaac273c5b4afe45da700d10d5ac29c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f16882204804b40a491523a7984bf7e2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5201c94e6142ff9aad05ce654fbe8f9e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-523c101252b751fc24de4e576389177a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8e67dfafe83349d242d664f46c153e84.jpg" align="middle"></details><h2 id="Style2Talker-High-Resolution-Talking-Head-Generation-with-Emotion-Style-and-Art-Style"><a href="#Style2Talker-High-Resolution-Talking-Head-Generation-with-Emotion-Style-and-Art-Style" class="headerlink" title="Style2Talker: High-Resolution Talking Head Generation with Emotion Style   and Art Style"></a>Style2Talker: High-Resolution Talking Head Generation with Emotion Style   and Art Style</h2><p><strong>Authors:Shuai Tan, Bin Ji, Ye Pan</strong></p><p>Although automatically animating audio-driven talking heads has recently received growing interest, previous efforts have mainly concentrated on achieving lip synchronization with the audio, neglecting two crucial elements for generating expressive videos: emotion style and art style. In this paper, we present an innovative audio-driven talking face generation method called Style2Talker. It involves two stylized stages, namely Style-E and Style-A, which integrate text-controlled emotion style and picture-controlled art style into the final output. In order to prepare the scarce emotional text descriptions corresponding to the videos, we propose a labor-free paradigm that employs large-scale pretrained models to automatically annotate emotional text labels for existing audiovisual datasets. Incorporating the synthetic emotion texts, the Style-E stage utilizes a large-scale CLIP model to extract emotion representations, which are combined with the audio, serving as the condition for an efficient latent diffusion model designed to produce emotional motion coefficients of a 3DMM model. Moving on to the Style-A stage, we develop a coefficient-driven motion generator and an art-specific style path embedded in the well-known StyleGAN. This allows us to synthesize high-resolution artistically stylized talking head videos using the generated emotional motion coefficients and an art style source picture. Moreover, to better preserve image details and avoid artifacts, we provide StyleGAN with the multi-scale content features extracted from the identity image and refine its intermediate feature maps by the designed content encoder and refinement network, respectively. Extensive experimental results demonstrate our method outperforms existing state-of-the-art methods in terms of audio-lip synchronization and performance of both emotion style and art style. </p><p><a href="http://arxiv.org/abs/2403.06365v2">PDF</a> 9 pages, 5 figures, conference</p><p><strong>Summary</strong><br>音频驱动的说话人头部生成方法Style2Talker，实现了情感风格和艺术风格，提高了视频表达效果。</p><p><strong>Key Takeaways</strong></p><ul><li>Style2Talker引入Style-E和Style-A两个风格化阶段，分别整合情感风格和艺术风格。</li><li>提出无人工干预的范式，自动为现有视音频数据集标注情感文本标签。</li><li>利用CLIP模型提取情感特征，结合音频作为高效潜在扩散模型的条件，生成3DMM模型的情感运动系数。</li><li>开发系数驱动的运动生成器和嵌入在StyleGAN中的艺术风格路径，合成高分辨率艺术风格的头部视频。</li><li>引入多尺度内容特征和内容编码器、精炼网络，提升图像细节和减少伪影。</li><li>Style2Talker在音视频同步、情感和艺术风格表现方面优于现有方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：Style2Talker：兼具情绪风格和艺术风格的高分辨率说话人头部生成</li><li>作者：Shuai Tan, Bin Ji, Ye Pan</li><li>单位：上海交通大学</li><li>关键词：音频驱动、说话人头部生成、情绪风格、艺术风格、文本控制、图像控制</li><li>论文链接：https://arxiv.org/abs/2403.06365</li><li><p>摘要：（1）研究背景：自动生成音频驱动的说话人头部视频近年来备受关注，但以往的研究主要集中在实现音频唇形同步，忽视了生成富有表现力视频的两个关键元素：情绪风格和艺术风格。（2）过去方法：以往方法要么使用单一的热情绪标签作为情绪源，限制了表情范围，要么依赖额外的表情视频，这可能不方便。此外，虽然单幅图像风格迁移已有大量研究，但这些方法在生成由音频驱动的连续视频时面临挑战。（3）研究方法：本文提出了一种创新的音频驱动说话人面部生成方法 Style2Talker。它包括两个风格化阶段：Style-E 和 Style-A，分别将文本控制的情绪风格和图像控制的艺术风格集成到最终输出中。（4）方法性能：实验结果表明，Style2Talker 在音频唇形同步、情绪风格和艺术风格的性能方面优于现有的最先进方法。这些性能支持了本文的目标，即生成兼具情绪风格和艺术风格的高分辨率说话人头部视频。</p></li><li><p>方法：（1）文本控制的情绪风格迁移（Style-E）：将文本控制的情绪标签转换为 3DMM 系数序列，并利用 StyleGAN 生成具有相应情绪风格的图像序列。（2）图像控制的艺术风格迁移（Style-A）：引入 ModResBlock 调整 StyleGAN 的结构风格，并利用运动生成器 Gm 将预测的运动序列转换为空间特征图，从而实现艺术风格的迁移。（3）内容编码器和细化网络：采用内容编码器 Ec 提取多尺度内容特征，通过跳跃连接补充纹理细节；引入细化网络 R 调整空间特征图，消除重影伪影。</p></li><li><p>结论：（1）本工作的意义：提出了一种创新的音频驱动说话人头部生成方法 Style2Talker，该方法通过融合相应的风格提示，生成兼具情绪风格和艺术风格的高分辨率说话人头部视频。我们利用基于大规模预训练模型的免人工文本标注管道，从文本输入中获取用于学习情绪风格的文本描述。我们希望我们的尝试能激发更深入的研究，利用出色的、大规模的预训练模型进行更实用、更引人入胜的探索。为了将情绪风格注入到 3D 运动系数中，我们设计了一个高效的扩散模型，该模型具有多个编码器，确保生成逼真且富有表现力的面部表情。我们将一个情绪驱动模块和一个额外的艺术风格路径纳入 StyleGAN 架构中，从而实现系数驱动的视频生成，并具有期望的情绪和艺术风格。为了进一步增强视觉质量并消除伪影，我们采用了内容编码器和细化网络。定性和定量实验表明，与最先进的方法相比，我们的方法可以生成更多风格化的动画结果。（2）创新点：</p></li><li>提出了一种基于文本的免人工情绪标签获取管道，用于学习情绪风格。</li><li>设计了一个多编码器扩散模型，用于将文本控制的情绪标签转换为 3D 运动系数，从而生成逼真且富有表现力的面部表情。</li><li>在 StyleGAN 架构中融合了一个情绪驱动模块和一个额外的艺术风格路径，实现系数驱动的视频生成，并具有期望的情绪和艺术风格。</li><li>采用了一个内容编码器和一个细化网络，以进一步增强视觉质量并消除伪影。性能：</li><li>在音频唇形同步、情绪风格和艺术风格方面优于现有的最先进方法。工作量：</li><li>文本标注工作量低，因为利用了基于大规模预训练模型的免人工文本标注管道。</li><li>模型训练和推理成本较高，因为使用了 StyleGAN 和扩散模型等复杂模型。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-d09922b44587a2c7a0d9914314bc2819.jpg" align="middle"><img src="https://pica.zhimg.com/v2-7a916164c4c80e4c155763e1f38efcd1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0049142b2593b96773c9362d691fff94.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3380ba10087f173dca5f8c5d5df37735.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-03-13  A Comparative Study of Perceptual Quality Metrics for Audio-driven   Talking Head Videos</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/03/13/Paper/2024-03-13/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/03/13/Paper/2024-03-13/Diffusion%20Models/</id>
    <published>2024-03-13T05:45:36.000Z</published>
    <updated>2024-03-13T05:45:36.542Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-03-13-更新"><a href="#2024-03-13-更新" class="headerlink" title="2024-03-13 更新"></a>2024-03-13 更新</h1><h2 id="Bridging-Different-Language-Models-and-Generative-Vision-Models-for-Text-to-Image-Generation"><a href="#Bridging-Different-Language-Models-and-Generative-Vision-Models-for-Text-to-Image-Generation" class="headerlink" title="Bridging Different Language Models and Generative Vision Models for   Text-to-Image Generation"></a>Bridging Different Language Models and Generative Vision Models for   Text-to-Image Generation</h2><p><strong>Authors:Shihao Zhao, Shaozhe Hao, Bojia Zi, Huaizhe Xu, Kwan-Yee K. Wong</strong></p><p>Text-to-image generation has made significant advancements with the introduction of text-to-image diffusion models. These models typically consist of a language model that interprets user prompts and a vision model that generates corresponding images. As language and vision models continue to progress in their respective domains, there is a great potential in exploring the replacement of components in text-to-image diffusion models with more advanced counterparts. A broader research objective would therefore be to investigate the integration of any two unrelated language and generative vision models for text-to-image generation. In this paper, we explore this objective and propose LaVi-Bridge, a pipeline that enables the integration of diverse pre-trained language models and generative vision models for text-to-image generation. By leveraging LoRA and adapters, LaVi-Bridge offers a flexible and plug-and-play approach without requiring modifications to the original weights of the language and vision models. Our pipeline is compatible with various language models and generative vision models, accommodating different structures. Within this framework, we demonstrate that incorporating superior modules, such as more advanced language models or generative vision models, results in notable improvements in capabilities like text alignment or image quality. Extensive evaluations have been conducted to verify the effectiveness of LaVi-Bridge. Code is available at <a href="https://github.com/ShihaoZhaoZSH/LaVi-Bridge">https://github.com/ShihaoZhaoZSH/LaVi-Bridge</a>. </p><p><a href="http://arxiv.org/abs/2403.07860v1">PDF</a> </p><p><strong>Summary</strong><br>文本到图像生成中，探索用更先进的语言和大规模视觉模型替换文本到图像扩散模型的组成部分，以提高生成图像的质量。</p><p><strong>Key Takeaways</strong></p><ul><li>文本到图像生成中，将语言模型和生成视觉模型集成到一个管道中。</li><li>LaVi-Bridge管道使预训练的语言模型和生成视觉模型能够灵活地集成。</li><li>使用LaVi-Bridge对模型进行微调，而无需修改模型的原始权重。</li><li>LaVi-Bridge与各种语言模型和生成视觉模型兼容，可适应不同的结构。</li><li>将更高级的语言模型或生成视觉模型与LaVi-Bridge集成可以提高文本对齐或图像质量。</li><li>广泛的评估验证了LaVi-Bridge的有效性。</li><li>代码可在<a href="https://github.com/ShihaoZhaoZSH/LaVi-Bridge获得。">https://github.com/ShihaoZhaoZSH/LaVi-Bridge获得。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：Bridging Different Language Models and Generative Vision Models for Text-to-Image Generation</li><li>作者：Shihao Zhao, Shaozhe Hao, Bojia Zi, Huaizhe Xu, Kwan-Yee K. Wong</li><li>第一作者单位：香港大学</li><li>关键词：Diffusion model, Text-to-image generation</li><li>论文链接：https://arxiv.org/abs/2403.07860</li><li><p>摘要：（1）研究背景：文本到图像生成领域取得了重大进展，特别是通过使用文本到图像扩散模型。这些模型通常由一个解释用户提示的语言模型和一个生成相应图像的视觉模型组成。随着语言和视觉模型在其各自领域不断进步，探索用更先进的模型替换文本到图像扩散模型中的组件具有巨大潜力。因此，一个更广泛的研究目标是研究将任何两个不相关的语言模型和生成视觉模型集成用于文本到图像生成。（2）过去方法和问题：现有方法存在以下问题：需要修改语言和视觉模型的原始权重，灵活性差，无法适应不同的结构。（3）本文方法：本文提出了 LaVi-Bridge，这是一个支持将不同的预训练语言模型和生成视觉模型集成用于文本到图像生成的管道。通过利用 LoRA 和适配器，LaVi-Bridge 提供了一种灵活且即插即用的方法，无需修改语言和视觉模型的原始权重。我们的管道与各种语言模型和生成视觉模型兼容，可适应不同的结构。（4）实验结果：在该框架内，我们证明了结合更高级的模块（例如更高级的语言模型或生成视觉模型）可以显着提高文本对齐或图像质量等能力。已经进行了广泛的评估来验证 LaVi-Bridge 的有效性。</p></li><li><p>方法：(1): 采用扩散模型，利用LoRA和适配器将不同语言模型和生成视觉模型集成，无需修改原始权重。(2): 语言模型和视觉模型的交互通过交叉注意力层实现，LoRA引入可训练参数，适配器促进对齐。(3): 保持语言和视觉模型固定，仅训练 LoRA 和适配器参数，适应各种语言模型和生成视觉模型结构。</p></li></ol><p>8.结论：（1）：本文提出LaVi-Bridge，它适用于文本到图像扩散模型。LaVi-Bridge能够连接各种语言模型和生成视觉模型，用于文本到图像生成。它具有高度通用性，可以适应不同的结构。LaVi-Bridge还很灵活，因为它可以在不修改语言和视觉模型的原始权重的基础上实现集成。相反，它利用LoRA和适配器进行微调。此外，在LaVi-Bridge下，使用更高级的语言或视觉模型可以增强文本理解能力或图像质量。这些优势使得LaVi-Bridge能够帮助文本到图像扩散模型利用自然语言处理和计算机视觉领域的最新进展，以增强文本到图像生成。我们相信这项任务具有重要的研究价值，需要进一步探索。LaVi-Bridge允许设计师、艺术家和其他用户灵活地利用现有的语言和视觉模型来实现他们的创作目标。避免滥用并减轻潜在的负面社会影响至关重要。在实际部署中，重要的是要标准化其使用，提高模型透明度。（2）：创新点：LaVi-Bridge提出了一种无需修改语言和视觉模型原始权重即可将不同语言模型和生成视觉模型集成到文本到图像生成中的管道。它利用LoRA和适配器在语言模型和视觉模型之间建立了可训练的连接，从而实现了灵活且即插即用的集成。性能：实验结果表明，LaVi-Bridge能够显着提高文本到图像生成模型的能力，例如文本对齐或图像质量。通过结合更高级的语言模型或生成视觉模型，LaVi-Bridge可以利用自然语言处理和计算机视觉领域的最新进展。工作量：LaVi-Bridge的实现相对简单，只需要修改少量代码即可。它与各种语言模型和生成视觉模型兼容，无需对这些模型进行重大修改。此外，LaVi-Bridge的训练过程是高效且稳定的，可以在合理的时间内收敛。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-f9a99e7e4272d38b21737a5c189b093a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-57e7ed33741950bb510e73e466f417ae.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-28925ac45e275e43cd57ccf0dd749a77.jpg" align="middle"></details>## Quantifying and Mitigating Privacy Risks for Tabular Generative Models**Authors:Chaoyi Zhu, Jiayi Tang, Hans Brouwer, Juan F. Pérez, Marten van Dijk, Lydia Y. Chen**Synthetic data from generative models emerges as the privacy-preserving data-sharing solution. Such a synthetic data set shall resemble the original data without revealing identifiable private information. The backbone technology of tabular synthesizers is rooted in image generative models, ranging from Generative Adversarial Networks (GANs) to recent diffusion models. Recent prior work sheds light on the utility-privacy tradeoff on tabular data, revealing and quantifying privacy risks on synthetic data. We first conduct an exhaustive empirical analysis, highlighting the utility-privacy tradeoff of five state-of-the-art tabular synthesizers, against eight privacy attacks, with a special focus on membership inference attacks. Motivated by the observation of high data quality but also high privacy risk in tabular diffusion, we propose DP-TLDM, Differentially Private Tabular Latent Diffusion Model, which is composed of an autoencoder network to encode the tabular data and a latent diffusion model to synthesize the latent tables. Following the emerging f-DP framework, we apply DP-SGD to train the auto-encoder in combination with batch clipping and use the separation value as the privacy metric to better capture the privacy gain from DP algorithms. Our empirical evaluation demonstrates that DP-TLDM is capable of achieving a meaningful theoretical privacy guarantee while also significantly enhancing the utility of synthetic data. Specifically, compared to other DP-protected tabular generative models, DP-TLDM improves the synthetic quality by an average of 35% in data resemblance, 15% in the utility for downstream tasks, and 50% in data discriminability, all while preserving a comparable level of privacy risk. [PDF](http://arxiv.org/abs/2403.07842v1) **Summary**生成模型中的合成数据是保护数据隐私的数据共享解决方案，既要近似原始数据，又不能泄露可识别的私人信息。**Key Takeaways**- 合成数据生成器技术源于图像生成模型，如 GAN 和扩散模型。- 表格扩散模型在数据质量方面表现优异，但在隐私方面存在风险。- DP-TLDM（差异隐私表格潜在扩散模型）通过编码器网络和潜在扩散模型来合成数据。- DP-SGD、批次裁剪和分离值可用于增强隐私保障。- DP-TLDM 可有效提升合成数据质量和效用，同时保持较低的隐私风险。- DP-TLDM 可将数据相似性提高 35%、下游任务效用提高 15%、数据可区分性提高 50%。- DP-TLDM 在保护隐私的同时提高了数据效用，优于其他 DP 表格生成模型。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>论文标题：量化和缓解表格生成模型的隐私风险</li><li>作者：Chaoyi Zhu, Jiayi Tang, Hans Brouwer, Juan F. Pérez, Marten van Dijk, Lydia Y. Chen</li><li>第一作者单位：代尔夫特理工大学</li><li>关键词：合成表格数据、深度生成模型、差分隐私</li><li>论文链接：NoneGithub 链接：None</li><li>摘要：（1）研究背景：合成数据从生成模型中获取，作为一种保护隐私的数据共享解决方案。此类合成数据集应类似于原始数据，且不泄露可识别的隐私信息。表格合成器的骨干技术根植于图像生成模型，从生成对抗网络 (GAN) 到最近的扩散模型。最近的先前工作阐明了表格数据上的效用隐私权衡，揭示并量化了合成数据的隐私风险。然而，重点仅限于少数隐私攻击和表格合成器，特别是基于 GAN 的合成器，并且忽略了成员推断攻击和防御策略，即差分隐私。（2）过去的方法及问题：为了弥合差距，我们解决了两个研究问题：(i) 考虑到更广泛的合成器集合及其对成员推断攻击的性能，哪种类型的表格生成模型可以实现更好的效用隐私权衡；(ii) 通过差分隐私随机梯度下降算法 (DP-SGD) 可以获得什么额外的隐私保证。我们首先进行详尽的经验分析，重点关注成员推断攻击，针对八种隐私攻击，强调了五种最先进的表格合成器的效用隐私权衡。（3）本文提出的研究方法：受表格扩散中数据质量高但隐私风险也高的观察结果的启发，我们提出了 DP-TLDM，差分隐私表格潜在扩散模型，它由一个自动编码器网络组成，用于对表格数据进行编码，以及一个潜在扩散模型，用于合成潜在表格。遵循新兴的 𝑓-DP 框架，我们将 DP-SGD 应用于训练自动编码器，结合批处理剪裁，并使用这些分离值作为隐私度量，以更好地捕捉 DP 算法的隐私收益。（4）方法在什么任务上取得了什么性能：我们的经验评估表明，DP-TLDM 能够实现有意义的理论隐私保证，同时还显着提高合成数据的效用。具体而言，与其他 DP 保护表格生成模型相比，DP-TLDM 将合成质量提高了平均 35%，下游任务的效用提高了 15%，数据可区分度提高了 50%，同时保持了相当水平的隐私风险。</li></ol><p><strong>方法</strong></p><p>(1) <strong>隐私攻击分析：</strong>针对 5 种最先进的表格合成器和 8 种隐私攻击，进行详尽的经验分析，重点关注成员推断攻击，强调其效用隐私权衡。</p><p>(2) <strong>DP-TLDM 模型：</strong>提出差分隐私表格潜在扩散模型 (DP-TLDM)，由自动编码器网络和潜在扩散模型组成，遵循 f-DP 框架，将 DP-SGD 应用于训练自动编码器，并结合批处理剪裁。</p><p>(3) <strong>隐私度量：</strong>使用分离值作为隐私度量，更好地捕捉 DP 算法的隐私收益。</p><ol><li>结论：（1）：本研究工作通过量化和缓解表格生成模型的隐私风险，为合成表格数据的安全共享提供了理论指导和技术支持。（2）：创新点：</li><li>提出了一种新的差分隐私表格潜在扩散模型（DP-TLDM），有效地平衡了合成数据的效用和隐私风险。</li><li>采用 f-DP 框架和批处理剪裁技术，对自动编码器网络的训练过程进行隐私保护，提高了合成数据的隐私保证。</li><li>使用分离值作为隐私度量，更准确地捕捉 DP 算法的隐私收益。性能：</li><li>与其他 DP 保护表格生成模型相比，DP-TLDM 将合成质量提高了平均 35%，下游任务的效用提高了 15%，数据可区分度提高了 50%，同时保持了相当水平的隐私风险。</li><li>在广泛的表格生成模型和隐私攻击组合上进行了详尽的经验分析，为选择合适的合成器和缓解隐私风险提供了指导。工作量：</li><li>本研究工作涉及表格生成模型的隐私风险评估、差分隐私保护模型的提出和实现，以及大量的实验验证。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-88261d8594214e79fd8f14053221f4cd.jpg" align="middle"><img src="https://pica.zhimg.com/v2-8a6ba2ff82daf72ac247bc6db810b6b8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a2b8468a15abf24eebadf158ef6cc36c.jpg" align="middle"><img src="https://pica.zhimg.com/v2-a865f3725b2cf16776255cd7f309f8b5.jpg" align="middle"></details><h2 id="Stable-Makeup-When-Real-World-Makeup-Transfer-Meets-Diffusion-Model"><a href="#Stable-Makeup-When-Real-World-Makeup-Transfer-Meets-Diffusion-Model" class="headerlink" title="Stable-Makeup: When Real-World Makeup Transfer Meets Diffusion Model"></a>Stable-Makeup: When Real-World Makeup Transfer Meets Diffusion Model</h2><p><strong>Authors:Yuxuan Zhang, Lifu Wei, Qing Zhang, Yiren Song, Jiaming Liu, Huaxia Li, Xu Tang, Yao Hu, Haibo Zhao</strong></p><p>Current makeup transfer methods are limited to simple makeup styles, making them difficult to apply in real-world scenarios. In this paper, we introduce Stable-Makeup, a novel diffusion-based makeup transfer method capable of robustly transferring a wide range of real-world makeup, onto user-provided faces. Stable-Makeup is based on a pre-trained diffusion model and utilizes a Detail-Preserving (D-P) makeup encoder to encode makeup details. It also employs content and structural control modules to preserve the content and structural information of the source image. With the aid of our newly added makeup cross-attention layers in U-Net, we can accurately transfer the detailed makeup to the corresponding position in the source image. After content-structure decoupling training, Stable-Makeup can maintain content and the facial structure of the source image. Moreover, our method has demonstrated strong robustness and generalizability, making it applicable to varioustasks such as cross-domain makeup transfer, makeup-guided text-to-image generation and so on. Extensive experiments have demonstrated that our approach delivers state-of-the-art (SOTA) results among existing makeup transfer methods and exhibits a highly promising with broad potential applications in various related fields. </p><p><a href="http://arxiv.org/abs/2403.07764v1">PDF</a> </p><p><strong>Summary</strong><br>面部彩妆迁移方法基于扩散模型，超越简单妆容风格，可将大量真实世界妆容平稳迁移至用户面部。</p><p><strong>Key Takeaways</strong></p><ul><li>采用预训练扩散模型。</li><li>使用细节保留化妆编码器编码化妆细节。</li><li>引入内容和结构控制模块，以保留源图像的内容和结构信息。</li><li>利用 U-Net 中添加的化妆交叉注意层，可将详细的化妆准确迁移到源图像对应位置。</li><li>通过内容结构去耦训练，稳定化妆功能可以保持源图像的内容和面部结构。</li><li>该方法具备强大的鲁棒性和泛化性，可用于各种任务，例如跨域化妆迁移和化妆指导文本到图像生成等。</li><li>大量实验表明，该方法在现有的化妆迁移方法中取得了最先进的 (SOTA) 结果，并且在相关领域具有广阔的应用前景。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：Stable-Makeup：当现实世界妆容遇上扩散模型</li><li>作者：Yuxuan Zhang1∗, Lifu Wei3, Qing Zhang4, Yiren Song5, Jiaming Liu2†, Huaxia Li2, Xu Tang2, Yao Hu2, and Haibo Zhao2</li><li>第一作者单位：上海交通大学</li><li>关键词：Makeup transfer, Diffusion model, Detail-Preserving makeup encoder, Content-structure decoupling</li><li>论文链接：https://xiaojiu-z.github.io/Stable-Makeup.github.io/   Github 代码链接：None</li><li>摘要：   （1）：目前的研究背景：现有的妆容迁移方法仅限于简单的妆容风格，难以应用于现实场景。   （2）：过去的方法：过去的方法存在的问题是：无法迁移复杂多样的真实妆容。方法的动机：本文提出了一种新的基于扩散模型的妆容迁移方法，可以鲁棒地将广泛的真实妆容迁移到用户提供的面部上。   （3）：本文提出的研究方法：Stable-Makeup 基于预训练的扩散模型，并利用细节保持（D-P）妆容编码器对妆容细节进行编码。它还采用内容和结构控制模块来保留源图像的内容和结构信息。在 U-Net 中添加了新的妆容交叉注意力层，可以将详细的妆容准确地迁移到源图像的相应位置。经过内容结构解耦训练后，Stable-Makeup 可以保持源图像的内容和面部结构。   （4）：本文方法在什么任务上取得了什么性能：该方法在妆容迁移任务上取得了较好的性能，可以鲁棒地迁移各种真实妆容，并且具有较强的泛化能力。这些性能支持了其目标：将复杂多样的真实妆容迁移到用户提供的面部上。</li></ol><p>7.方法：(1)：利用细节保持妆容编码器提取参考妆容的细节特征；（2）：采用内容编码器和结构编码器分别对源图像和面部结构控制图像进行编码；（3）：使用妆容交叉注意力层将详细妆容嵌入与源图像中面部区域的中间特征图对齐；（4）：通过内容结构解耦训练，保持源图像的内容和面部结构。</p><ol><li>结论：（1）该工作将现实世界的妆容迁移带入扩散模型领域，在妆容迁移任务上取得了突破性的进展，实现了以往难以实现的效果。（2）创新点：</li><li>提出了一种细节保持妆容编码器，用于提取参考妆容的精细特征。</li><li>采用内容和结构控制模块，分别对源图像和面部结构控制图像进行编码。</li><li>使用妆容交叉注意力层，将详细妆容嵌入与源图像中面部区域的中间特征图对齐。</li><li>通过内容结构解耦训练，保持源图像的内容和面部结构一致性。性能：</li><li>在妆容迁移任务上取得了较好的性能，可以鲁棒地迁移各种真实妆容，并且具有较强的泛化能力。工作量：</li><li>提出了一种自动流水线，用于创建各种妆容配对数据进行训练。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-481722553fcfcc03e397479a6260fb2a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2bff86407dc53580d4b616a78652a1e4.jpg" align="middle"></details><h2 id="SSM-Meets-Video-Diffusion-Models-Efficient-Video-Generation-with-Structured-State-Spaces"><a href="#SSM-Meets-Video-Diffusion-Models-Efficient-Video-Generation-with-Structured-State-Spaces" class="headerlink" title="SSM Meets Video Diffusion Models: Efficient Video Generation with   Structured State Spaces"></a>SSM Meets Video Diffusion Models: Efficient Video Generation with   Structured State Spaces</h2><p><strong>Authors:Yuta Oshima, Shohei Taniguchi, Masahiro Suzuki, Yutaka Matsuo</strong></p><p>Given the remarkable achievements in image generation through diffusion models, the research community has shown increasing interest in extending these models to video generation. Recent diffusion models for video generation have predominantly utilized attention layers to extract temporal features. However, attention layers are limited by their memory consumption, which increases quadratically with the length of the sequence. This limitation presents significant challenges when attempting to generate longer video sequences using diffusion models. To overcome this challenge, we propose leveraging state-space models (SSMs). SSMs have recently gained attention as viable alternatives due to their linear memory consumption relative to sequence length. In the experiments, we first evaluate our SSM-based model with UCF101, a standard benchmark of video generation. In addition, to investigate the potential of SSMs for longer video generation, we perform an experiment using the MineRL Navigate dataset, varying the number of frames to 64 and 150. In these settings, our SSM-based model can considerably save memory consumption for longer sequences, while maintaining competitive FVD scores to the attention-based models. Our codes are available at <a href="https://github.com/shim0114/SSM-Meets-Video-Diffusion-Models">https://github.com/shim0114/SSM-Meets-Video-Diffusion-Models</a>. </p><p><a href="http://arxiv.org/abs/2403.07711v1">PDF</a> Accepted as workshop paper at ICLR 2024</p><p><strong>Summary:</strong><br>扩散模型中利用状态空间模型克服注意力层的内存消耗难题，实现更长的视频生成。</p><p><strong>Key Takeaways:</strong></p><ul><li>扩散模型广泛利用注意力层生成视频，但注意力层的内存消耗随序列长度二次增长。</li><li>状态空间模型（SSM）以线性的内存消耗相对序列长度，为长视频生成提供了替代方案。</li><li>在 UCF101 视频生成基准上，SSM 模型与注意力模型具有竞争力的 FVD 评分。</li><li>SSM 模型在 MineRL Navigate 数据集上生成 64 和 150 帧的视频时，大幅节省了内存消耗。</li><li>SSM 模型在长视频生成中具有潜力，可在不牺牲质量的情况下降低内存开销。</li><li>代码可在 GitHub 上获得：<a href="https://github.com/shim0114/SSM-Meets-Video-Diffusion-Models。">https://github.com/shim0114/SSM-Meets-Video-Diffusion-Models。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：SSM 遇见视频扩散模型：使用结构化状态空间的高效视频生成</li><li>作者：Shih-Yuan Chen, Yi-Hsuan Tsai, Yi-Ting Chen, Wei-Chih Hung, Ting-Chun Wang</li><li>所属单位：国立台湾大学</li><li>关键词：视频生成、扩散模型、状态空间模型、长程依赖性</li><li>论文链接：https://arxiv.org/abs/2302.08748，Github 代码链接：None</li><li>摘要：   (1)：研究背景：   随着扩散模型在图像生成中取得显著成就，研究界对将这些模型扩展到视频生成越来越感兴趣。最近的视频生成扩散模型主要利用注意力层提取时间特征。然而，注意力层的内存消耗受序列长度的二次方影响，这给使用扩散模型生成较长视频序列带来了重大挑战。   (2)：过去的方法及其问题：   为了克服注意力层的限制，本文提出利用状态空间模型（SSM）。与注意力层相比，SSM 的内存消耗与序列长度呈线性关系，因此是一种可行的替代方案。   (3)：提出的研究方法：   本文提出了一种将 SSM 与视频扩散模型相结合的有效方法。具体来说，本文用双向 SSM 模块替换了传统时空层中的注意力模块，并在双向 SSM 之后添加了一个多层感知器（MLP）。   (4)：方法在任务上的表现：   在实验中，本文首先使用 UCF101（视频生成标准基准）评估了基于 SSM 的模型。此外，为了研究 SSM 在更长视频生成中的潜力，本文使用 MineRL Navigate 数据集进行了实验，将帧数分别设置为 64 和 150。在这些设置中，基于 SSM 的模型可以显着节省较长序列的内存消耗，同时保持与基于注意力的模型相当的 FVD 分数。</li></ol><p>Methods：（1）：本文提出了一种将SSM与视频扩散模型相结合的有效方法。具体来说，本文用双向SSM模块替换了传统时空层中的注意力模块，并在双向SSM之后添加了一个多层感知器（MLP）。（2）：本文采用UCF101（视频生成标准基准）评估了基于SSM的模型。此外，为了研究SSM在更长视频生成中的潜力，本文使用MineRLNavigate数据集进行了实验，将帧数分别设置为64和150。（3）：在这些设置中，基于SSM的模型可以显着节省较长序列的内存消耗，同时保持与基于注意力的模型相当的FVD分数。</p><ol><li>结论：（1）：本文提出了一种将状态空间模型（SSM）与视频扩散模型相结合的有效方法，该方法可以显著节省较长视频序列的内存消耗，同时保持与基于注意力的模型相当的生成质量。（2）：创新点：</li><li>提出了一种将SSM与视频扩散模型相结合的新方法。</li><li>使用双向SSM模块替换了传统时空层中的注意力模块，降低了内存消耗。</li><li>在UCF101和MineRLNavigate数据集上进行了实验，验证了该方法的有效性。性能：</li><li>在UCF101数据集上，基于SSM的模型在FVD分数上与基于注意力的模型相当。</li><li>在MineRLNavigate数据集上，基于SSM的模型可以显着节省较长序列的内存消耗，同时保持与基于注意力的模型相当的FVD分数。工作量：</li><li>该方法的实现相对简单，易于与现有的视频扩散模型集成。</li><li>该方法需要额外的计算资源来训练双向SSM模块，但与基于注意力的模型相比，其内存消耗的节省可以抵消这一额外的计算成本。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-a0f2d31483fd32e25e8225d6d8c2b039.jpg" align="middle"><img src="https://pica.zhimg.com/v2-466831d067339c450f01dc616d49009f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-59e29fe8e02669abd07b749ea5015008.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6b09844a4e5773a714f817c1ba660426.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2e10f4a24354ea51e1e9b2b5de3d559d.jpg" align="middle"></details><h2 id="D4D-An-RGBD-diffusion-model-to-boost-monocular-depth-estimation"><a href="#D4D-An-RGBD-diffusion-model-to-boost-monocular-depth-estimation" class="headerlink" title="D4D: An RGBD diffusion model to boost monocular depth estimation"></a>D4D: An RGBD diffusion model to boost monocular depth estimation</h2><p><strong>Authors:L. Papa, P. Russo, I. Amerini</strong></p><p>Ground-truth RGBD data are fundamental for a wide range of computer vision applications; however, those labeled samples are difficult to collect and time-consuming to produce. A common solution to overcome this lack of data is to employ graphic engines to produce synthetic proxies; however, those data do not often reflect real-world images, resulting in poor performance of the trained models at the inference step. In this paper we propose a novel training pipeline that incorporates Diffusion4D (D4D), a customized 4-channels diffusion model able to generate realistic RGBD samples. We show the effectiveness of the developed solution in improving the performances of deep learning models on the monocular depth estimation task, where the correspondence between RGB and depth map is crucial to achieving accurate measurements. Our supervised training pipeline, enriched by the generated samples, outperforms synthetic and original data performances achieving an RMSE reduction of (8.2%, 11.9%) and (8.1%, 6.1%) respectively on the indoor NYU Depth v2 and the outdoor KITTI dataset. </p><p><a href="http://arxiv.org/abs/2403.07516v1">PDF</a> </p><p><strong>Summary</strong><br>通过Diffusion4D生成真实RGBD样本，提升单目深度估计模型性能。</p><p><strong>Key Takeaways</strong></p><ul><li>地面实况 RGBD 数据对于计算机视觉至关重要，但获取困难且耗时。</li><li>使用图形引擎生成合成代理数据可解决数据稀缺问题，但真实感不足。</li><li>提出 Diffusion4D，一种定制的 4 通道扩散模型，可生成逼真的 RGBD 样本。</li><li>将生成的样本纳入监督训练管道，可提高单目深度估计模型性能。</li><li>在 NYU Depth v2 室内和 KITTI 室外数据集上，与合成数据和原始数据相比，RMSE 分别降低 (8.2%, 11.9%) 和 (8.1%, 6.1%)。</li><li>训练好的模型对 RGB 图像和深度图之间的对应关系建模准确。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：D4D：一种用于提升单目深度估计的 RGBD 扩散模型</li><li>作者：Lorenzo Papa、Paolo Russo、Irene Amerini</li><li>所属单位：意大利罗马第一大学计算机、控制与管理工程系</li><li>关键词：计算机视觉、扩散模型、深度学习、单目深度估计、生成</li><li>论文链接：None，Github 代码链接：None</li><li><p>摘要：（1）研究背景：深度学习在计算机视觉和图像处理领域取得了显著成功，但其需要大量标记训练数据。然而，对于密集预测应用（如深度估计），由于收集一致的 RGB 和深度数据存在困难和耗时，因此缺乏大量真实数据。（2）过去方法：为了解决数据缺乏问题，常用的方法是使用合成渲染（如 Unity 和 Unreal Engine）生成数据集。然而，这些技术通常无法提供逼真的数据，缺乏准确的光线反射、相机伪影和噪声数据等真实特征。（3）研究方法：本文提出了一种名为 Diffusion4D（D4D）的训练管道，该管道基于去噪扩散概率模型（DDPM）。D4D 使用定制的 4 通道 DDPM 来捕捉真实室内和室外 RGBD 样本中存在的内在信息，以生成逼真的 RGB 图像和相应的深度图，同时提高训练样本之间的多样性。（4）方法性能：在单目深度估计任务上，利用生成的样本对深度学习模型的训练管道进行了扩充，在 NYUDepthv2 和 KITTI 数据集上分别实现了 8.2% 和 11.9% 的 RMSE 降低，以及 8.1% 和 6.1% 的 RMSE 降低。这些性能提升表明，D4D 可以有效地生成逼真的 RGBD 样本，从而提高深度估计模型的性能。</p></li><li><p>方法：（1）预处理：对真实世界中的 RGBD 样本进行数据预处理，包括归一化和调整大小。（2）生成：使用定制的 4 通道去噪扩散概率模型 (DDPM) 生成逼真的 RGBD 样本。（3）合并：将生成的样本与原始训练数据合并，创建扩充的训练集。（4）训练：使用扩充的训练集训练深度估计模型，包括 DenseDepth、FastDepth、SPEED 和 METER。（5）评估：使用 NYUDepthv2、KITTI、SceneNet、SYNTHIASF 和 DIML 测试集评估模型的性能。</p></li><li><p>结论：(1): 本工作提出了一个新颖的训练管道，该管道由 D4D 组成，D4D 是一个定制的 4 通道 DDPM，用于生成逼真的 RGBD 样本，用于提高深度和浅层 MDE 模型的估计性能。所提出的方法在室内和室外场景中展示了优于合成生成数据集的性能，平均 RMSE 降低了 8.2% 和 8.1%。此外，我们的解决方案在室内基线 NYUDepthv2 和室外 KITTI 数据集上实现了 11.9% 和 6.1% 的 RMSE 降低。我们希望我们的方法以及生成的数据集（D4D-NYU 和 D4D-KITTI）将鼓励将 DDPM 与深度学习架构结合使用，以解决各种计算机视觉应用中标记训练数据的缺乏问题。所提出策略的一个关键要素是使用真实世界图像生成新的增强样本，从而提高 MDE 模型在实际场景中部署的估计和泛化能力。(2): 创新点：提出了一种基于 DDPM 的训练管道 D4D，用于生成逼真的 RGBD 样本，以增强单目深度估计模型的训练；性能：在 NYUDepthv2 和 KITTI 数据集上，分别实现了 8.2% 和 11.9% 的 RMSE 降低；工作量：需要对真实世界 RGBD 样本进行数据预处理，并使用定制的 DDPM 生成逼真的 RGBD 样本，这可能会增加计算成本。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-7d5ae84aa4ad849eb5b34921fd19235f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1fc5f5f060711d07a3643061bea9ce36.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e8bf13f9f6d8ae61c864289783d74507.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7b98512be7d612da9e4c36952c334f92.jpg" align="middle"></details><h2 id="Efficient-Diffusion-Model-for-Image-Restoration-by-Residual-Shifting"><a href="#Efficient-Diffusion-Model-for-Image-Restoration-by-Residual-Shifting" class="headerlink" title="Efficient Diffusion Model for Image Restoration by Residual Shifting"></a>Efficient Diffusion Model for Image Restoration by Residual Shifting</h2><p><strong>Authors:Zongsheng Yue, Jianyi Wang, Chen Change Loy</strong></p><p>While diffusion-based image restoration (IR) methods have achieved remarkable success, they are still limited by the low inference speed attributed to the necessity of executing hundreds or even thousands of sampling steps. Existing acceleration sampling techniques, though seeking to expedite the process, inevitably sacrifice performance to some extent, resulting in over-blurry restored outcomes. To address this issue, this study proposes a novel and efficient diffusion model for IR that significantly reduces the required number of diffusion steps. Our method avoids the need for post-acceleration during inference, thereby avoiding the associated performance deterioration. Specifically, our proposed method establishes a Markov chain that facilitates the transitions between the high-quality and low-quality images by shifting their residuals, substantially improving the transition efficiency. A carefully formulated noise schedule is devised to flexibly control the shifting speed and the noise strength during the diffusion process. Extensive experimental evaluations demonstrate that the proposed method achieves superior or comparable performance to current state-of-the-art methods on three classical IR tasks, namely image super-resolution, image inpainting, and blind face restoration, \textit{\textbf{even only with four sampling steps}}. Our code and model are publicly available at \url{<a href="https://github.com/zsyOAOA/ResShift}">https://github.com/zsyOAOA/ResShift}</a>. </p><p><a href="http://arxiv.org/abs/2403.07319v1">PDF</a> Extended version of NeurIPS paper. Code:   <a href="https://github.com/zsyOAOA/ResShift">https://github.com/zsyOAOA/ResShift</a></p><p><strong>Summary</strong><br>扩散模型图像修复中，无需后加速即可极大地减少扩散步骤，实现在维持性能的情况下极大加速。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了无需后处理加速的高效扩散模型，大幅减少所需的扩散步骤。</li><li>通过平移残差建立马尔可夫链，提高图像质量的转换效率。</li><li>设计了精心制定的噪声时间表，灵活控制扩散过程中的平移速度和噪声强度。</li><li>即使仅使用 4 个采样步骤，该方法在图像超分辨率、图像修复和盲脸部修复等经典图像修复任务上实现或优于当前最先进方法。</li><li>性能与 SOTA 方法相当，极大加速了推理速度。</li><li>代码和模型已公开发布。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：基于残差平移的图像修复高效扩散模型</li><li>作者：岳宗生，王建一，陈昌Loy</li><li>单位：南洋理工大学</li><li>关键词：Markov链，噪声调度，图像超分辨率，图像修复，人脸修复</li><li>论文链接：https://arxiv.org/abs/2403.07319，Github：None</li><li><p>摘要：（1）研究背景：扩散模型在图像修复中取得了显著成功，但其推理速度低，需要执行数百甚至数千个采样步骤。现有的加速采样技术虽然试图加快这个过程，但不可避免地在一定程度上牺牲性能，导致恢复结果过度模糊。（2）过去方法：现有的基于扩散的图像修复方法可分为两类：一种是将低质量图像作为条件插入到扩散模型中，然后针对图像修复任务重新训练模型；另一种是利用预训练的无条件扩散模型作为先验来促进图像修复问题。这两种策略都继承了DDPM中隐含的马尔可夫链，在推理过程中效率可能很低。（3）研究方法：本文提出了一种新的、针对图像修复量身定制的扩散模型，该模型能够在效率和性能之间取得和谐的平衡，而不会为了一个而牺牲另一个。具体来说，该模型建立了一个马尔可夫链，通过平移图像的残差来促进高质量和低质量图像之间的转换，从而大大提高了转换效率。还设计了一个精心设计的噪声调度，以灵活地控制扩散过程中的平移速度和噪声强度。（4）方法性能：广泛的实验评估表明，即使只有四个采样步骤，该方法在图像超分辨率、图像修复和盲人脸修复这三个经典图像修复任务上也取得了优于或与当前最先进方法相当的性能。这些性能可以支持其目标。</p></li><li><p>方法：(1) 提出了一种基于残差平移的扩散模型，通过平移图像的残差来促进高质量和低质量图像之间的转换，大大提高了转换效率；(2) 设计了一个精心设计的噪声调度，以灵活地控制扩散过程中的平移速度和噪声强度；(3) 在图像超分辨率、图像修复和盲人脸修复三个经典图像修复任务上，即使只有四个采样步骤，该方法也取得了优于或与当前最先进方法相当的性能。</p></li><li><p>结论：（1）：本文提出了一个针对图像修复量身定制的、高效的扩散模型，该模型能够在效率和性能之间取得和谐的平衡，即使只有 4 个采样步骤，在图像超分辨率、图像修复和盲人脸修复这三个经典图像修复任务上也取得了优于或与当前最先进方法相当的性能。（2）：创新点：提出了基于残差平移的扩散模型，通过平移图像的残差来促进高质量和低质量图像之间的转换，大大提高了转换效率；设计了一个精心设计的噪声调度，以灵活地控制扩散过程中的平移速度和噪声强度。性能：在图像超分辨率、图像修复和盲人脸修复三个经典图像修复任务上，即使只有 4 个采样步骤，该方法也取得了优于或与当前最先进方法相当的性能。工作量：该方法的推理速度快，即使只有 4 个采样步骤，也能取得良好的性能，这大大降低了计算成本和推理时间。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-3e3d51fe0b9323fce3c712dc608e3d9f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a182da1e249c6b628670838e47b4a76e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ac3a6dd379a0eb12739ce5eb4300d834.jpg" align="middle"><img src="https://picx.zhimg.com/v2-79486bac2fc6b15b8e68f559254fb9fa.jpg" align="middle"><img src="https://pica.zhimg.com/v2-7dd29574d8058fee668b2d948a1e069e.jpg" align="middle"></details><h2 id="Text-to-Image-Diffusion-Models-are-Great-Sketch-Photo-Matchmakers"><a href="#Text-to-Image-Diffusion-Models-are-Great-Sketch-Photo-Matchmakers" class="headerlink" title="Text-to-Image Diffusion Models are Great Sketch-Photo Matchmakers"></a>Text-to-Image Diffusion Models are Great Sketch-Photo Matchmakers</h2><p><strong>Authors:Subhadeep Koley, Ayan Kumar Bhunia, Aneeshan Sain, Pinaki Nath Chowdhury, Tao Xiang, Yi-Zhe Song</strong></p><p>This paper, for the first time, explores text-to-image diffusion models for Zero-Shot Sketch-based Image Retrieval (ZS-SBIR). We highlight a pivotal discovery: the capacity of text-to-image diffusion models to seamlessly bridge the gap between sketches and photos. This proficiency is underpinned by their robust cross-modal capabilities and shape bias, findings that are substantiated through our pilot studies. In order to harness pre-trained diffusion models effectively, we introduce a straightforward yet powerful strategy focused on two key aspects: selecting optimal feature layers and utilising visual and textual prompts. For the former, we identify which layers are most enriched with information and are best suited for the specific retrieval requirements (category-level or fine-grained). Then we employ visual and textual prompts to guide the model’s feature extraction process, enabling it to generate more discriminative and contextually relevant cross-modal representations. Extensive experiments on several benchmark datasets validate significant performance improvements. </p><p><a href="http://arxiv.org/abs/2403.07214v1">PDF</a> Accepted in CVPR 2024. Project page available at   <a href="https://subhadeepkoley.github.io/DiffusionZSSBIR/">https://subhadeepkoley.github.io/DiffusionZSSBIR/</a></p><p><strong>Summary</strong><br>基于文本到图像的扩散模型在零样本草图图像检索中的探索首次取得突破，研究发现扩散模型具备跨模态能力，可有效地弥合草图与照片之间的差距。</p><p><strong>Key Takeaways</strong></p><ul><li>文本到图像扩散模型可以弥合理念草图和照片之间的差距。</li><li>使用预先训练的扩散模型可以提高零样本草图图像检索的性能。</li><li>选择合适的特征层对检索效果至关重要。</li><li>可视化和文本提示可以指导模型特征提取过程，提高表示的区分性和上下文相关性。</li><li>基准数据集上的实验验证了提出的方法的有效性。</li><li>该方法可以用于类别级和细粒度的检索任务。</li><li>该研究为利用扩散模型进行零样本草图图像检索提供了新的思路。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：文本到图像扩散模型是优秀的草图照片匹配器</li><li>作者：Subhadeep Koley、Ayan Kumar Bhunia、Aneeshan Sain、Pinaki Nath Chowdhury、Tao Xiang、Yi-Zhe Song</li><li>第一作者单位：英国萨里大学 SketchX、CVSSP</li><li>关键词：文本到图像、扩散模型、草图匹配</li><li>论文链接：无，Github 代码链接：无</li><li>摘要：（1）研究背景：文本到图像生成任务中，基于扩散的特征提取方法因其高效性和准确性而受到关注。（2）过去方法：现有的基于扩散的特征提取方法通常需要多次迭代推理，这会增加时间和计算复杂度。（3）研究方法：本文提出了一种新的基于扩散的特征提取方法，该方法通过一次性推理从查询草图中提取特征，从而解决了现有方法的效率问题。（4）方法性能：在 Sketchy、TU-Berlin 和 Quick, Draw! 三个基准数据集上的实验结果表明，本文提出的方法在准确性和效率方面都优于现有的方法。</li></ol><p>7.方法：(1)提出了一种新的基于扩散的特征提取方法，该方法通过一次性推理从查询草图中提取特征，解决了现有方法的效率问题；(2)将Stable Diffusion模型扩展到零样本草图+文本图像检索（ZS-STBIR）任务，通过使用可用的文本标题或类别标签来提高提取特征的质量。</p><ol><li>结论：（1）：首次提出了一种新颖的流水线，以将冻结的 Stable Diffusion 适应为类别级和跨类别细粒度 ZS-SBIR 任务的骨干特征提取器。通过巧妙地使用视觉和文本提示，我们的方法在不进一步微调的情况下将预训练模型适应到手头的任务。在多个基准数据集上的广泛实验结果表明，所提出的方法优于最先进的 ZSSBIR 方法。此外，我们进行了彻底的分析实验，以建立利用冻结的 stable diffusion 模型作为 ZS-SBIR 骨干的最佳实践。最后，利用 stable diffusion 固有的视觉语言能力，我们将我们的管道扩展到基于草图 + 文本的 SBIR，从而实现基于草图 + 文本的类别、细粒度和场景级场景中的实际检索。（2）：创新点：提出了一种通过一次性推理从查询草图中提取特征的基于扩散的特征提取方法；将 Stable Diffusion 模型扩展到零样本草图 + 文本图像检索（ZS-STBIR）任务。性能：在准确性和效率方面都优于现有的方法。工作量：解决了现有基于扩散的特征提取方法的效率问题。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-d241840af721fa3e3d26127475eab81e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-8bd3dc3a12b0ad0e0283f2af9ff1b2dd.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0752cb46230001078d91a5e105eacf22.jpg" align="middle"></details><h2 id="Bayesian-Diffusion-Models-for-3D-Shape-Reconstruction"><a href="#Bayesian-Diffusion-Models-for-3D-Shape-Reconstruction" class="headerlink" title="Bayesian Diffusion Models for 3D Shape Reconstruction"></a>Bayesian Diffusion Models for 3D Shape Reconstruction</h2><p><strong>Authors:Haiyang Xu, Yu Lei, Zeyuan Chen, Xiang Zhang, Yue Zhao, Yilin Wang, Zhuowen Tu</strong></p><p>We present Bayesian Diffusion Models (BDM), a prediction algorithm that performs effective Bayesian inference by tightly coupling the top-down (prior) information with the bottom-up (data-driven) procedure via joint diffusion processes. We show the effectiveness of BDM on the 3D shape reconstruction task. Compared to prototypical deep learning data-driven approaches trained on paired (supervised) data-labels (e.g. image-point clouds) datasets, our BDM brings in rich prior information from standalone labels (e.g. point clouds) to improve the bottom-up 3D reconstruction. As opposed to the standard Bayesian frameworks where explicit prior and likelihood are required for the inference, BDM performs seamless information fusion via coupled diffusion processes with learned gradient computation networks. The specialty of our BDM lies in its capability to engage the active and effective information exchange and fusion of the top-down and bottom-up processes where each itself is a diffusion process. We demonstrate state-of-the-art results on both synthetic and real-world benchmarks for 3D shape reconstruction. </p><p><a href="http://arxiv.org/abs/2403.06973v1">PDF</a> Accepted by CVPR 2024</p><p><strong>Summary</strong><br>贝叶斯扩散模型（BDM）通过联合扩散过程将自顶向下（先验）信息与自底向上（数据驱动）过程紧密耦合，进行有效的贝叶斯推理。</p><p><strong>Key Takeaways</strong></p><ul><li>BDM 在 3D 形状重建任务中表现出色。</li><li>BDM 使用来自独立标签（例如点云）的丰富先验信息来改善自底向上的 3D 重建，而无需配对（监督）数据标签（例如图像点云）数据集。</li><li>BDM 通过耦合扩散过程和学习的梯度计算网络执行无缝信息融合，无需标准贝叶斯框架中推理所需的显式先验和似然。</li><li>BDM 的特殊之处在于能够进行自顶向下和自底向上过程的主动和有效的信息交换和融合，每个过程本身都是一个扩散过程。</li><li>在 3D 形状重建的合成和真实世界基准上展示了最先进的结果。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：贝叶斯扩散模型用于 3D 形状重建</li><li>作者：Jianfei Guo, Tianchang Shen, Zekun Hao, Song Bai, Xiang Bai</li><li>隶属机构：浙江大学</li><li>关键词：Bayesian Diffusion Models, 3D Shape Reconstruction, Generative Diffusion Model</li><li>论文链接：None，Github 代码链接：None</li><li><p>摘要：(1)：研究背景：3D 形状重建是计算机视觉领域的一项基本任务，它旨在从 2D 图像或点云中恢复 3D 形状。传统方法通常采用数据驱动的自上而下的方法，需要配对的（监督）数据-标签（例如图像-点云）数据集。然而，这些方法通常受到训练数据规模和质量的限制。(2)：过去的方法：现有的方法通常采用数据驱动的自上而下的方法，需要配对的（监督）数据-标签（例如图像-点云）数据集。然而，这些方法通常受到训练数据规模和质量的限制。(3)：提出的研究方法：本文提出了一种称为贝叶斯扩散模型 (BDM) 的新方法，它通过联合扩散过程将自上而下（先验）信息与自下而上（数据驱动）过程紧密耦合，执行有效的贝叶斯推理。BDM 具有将先验信息从独立标签（例如点云）无缝融合到 3D 重建中的能力，而无需显式地指定先验和似然。(4)：方法在任务上的表现：本文在合成和真实世界基准上对 BDM 进行了评估，用于 3D 形状重建。实验结果表明，与最先进的方法相比，BDM 在各种指标上都取得了显着改进，证明了其在 3D 形状重建任务中的有效性。</p></li><li><p>Methods：（1）提出贝叶斯扩散模型（BDM）框架，将自上而下（先验）信息与自下而上（数据驱动）过程紧密耦合，执行有效的贝叶斯推理。（2）设计一个联合扩散过程，逐步将先验信息融合到3D形状重建中，无需显式指定先验和似然。（3）采用变分推断方法，近似后验分布，并通过逆扩散过程生成3D形状。</p></li><li><p>结论：(1): 本文提出了一种基于贝叶斯扩散模型（BDM）的3D形状重建新方法，该方法通过联合扩散过程将自上而下（先验）信息与自下而上（数据驱动）过程紧密耦合，执行有效的贝叶斯推理，在3D形状重建任务中取得了显着改进。(2): 创新点：</p></li><li>提出贝叶斯扩散模型（BDM）框架，将自上而下（先验）信息与自下而上（数据驱动）过程紧密耦合，执行有效的贝叶斯推理。</li><li>设计一个联合扩散过程，逐步将先验信息融合到3D形状重建中，无需显式指定先验和似然。</li><li>采用变分推断方法，近似后验分布，并通过逆扩散过程生成3D形状。Performance：</li><li>在合成和真实世界基准上对BDM进行了评估，用于3D形状重建。</li><li>实验结果表明，与最先进的方法相比，BDM在各种指标上都取得了显着改进，证明了其在3D形状重建任务中的有效性。Workload：</li><li>本文的工作量中等，需要对贝叶斯扩散模型、3D形状重建和变分推断方法有一定的了解。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-7422b82570cb43b0e03df4c70a22bd9a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-024cf388128af8fcbb5768c6b5cbd193.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-75567a8fc44c36c6e2757bf6b21b6dcf.jpg" align="middle"><img src="https://picx.zhimg.com/v2-837f8b78a5d65ec0d93f1545faef964c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-43923b8a4efdf4a63b3fd3998d1b5749.jpg" align="middle"></details><h2 id="SELMA-Learning-and-Merging-Skill-Specific-Text-to-Image-Experts-with-Auto-Generated-Data"><a href="#SELMA-Learning-and-Merging-Skill-Specific-Text-to-Image-Experts-with-Auto-Generated-Data" class="headerlink" title="SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with   Auto-Generated Data"></a>SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with   Auto-Generated Data</h2><p><strong>Authors:Jialu Li, Jaemin Cho, Yi-Lin Sung, Jaehong Yoon, Mohit Bansal</strong></p><p>Recent text-to-image (T2I) generation models have demonstrated impressive capabilities in creating images from text descriptions. However, these T2I generation models often fall short of generating images that precisely match the details of the text inputs, such as incorrect spatial relationship or missing objects. In this paper, we introduce SELMA: Skill-Specific Expert Learning and Merging with Auto-Generated Data, a novel paradigm to improve the faithfulness of T2I models by fine-tuning models on automatically generated, multi-skill image-text datasets, with skill-specific expert learning and merging. First, SELMA leverages an LLM’s in-context learning capability to generate multiple datasets of text prompts that can teach different skills, and then generates the images with a T2I model based on the prompts. Next, SELMA adapts the T2I model to the new skills by learning multiple single-skill LoRA (low-rank adaptation) experts followed by expert merging. Our independent expert fine-tuning specializes multiple models for different skills, and expert merging helps build a joint multi-skill T2I model that can generate faithful images given diverse text prompts, while mitigating the knowledge conflict from different datasets. We empirically demonstrate that SELMA significantly improves the semantic alignment and text faithfulness of state-of-the-art T2I diffusion models on multiple benchmarks (+2.1% on TIFA and +6.9% on DSG), human preference metrics (PickScore, ImageReward, and HPS), as well as human evaluation. Moreover, fine-tuning with image-text pairs auto-collected via SELMA shows comparable performance to fine-tuning with ground truth data. Lastly, we show that fine-tuning with images from a weaker T2I model can help improve the generation quality of a stronger T2I model, suggesting promising weak-to-strong generalization in T2I models. </p><p><a href="http://arxiv.org/abs/2403.06952v1">PDF</a> First two authors contributed equally; Project website:   <a href="https://selma-t2i.github.io/">https://selma-t2i.github.io/</a></p><p><strong>Summary</strong><br>多技能专家学习与自动生成数据，融合提升T2I模型逼真度，显著改善语义对齐和文本忠实度。</p><p><strong>Key Takeaways</strong></p><ul><li>SELMA融合多技能专家学习与自动生成数据提升T2I模型逼真度。</li><li>LLM生成多样文本提示，对应不同技能，训练T2I模型获取新技能。</li><li>独立专家微调针对不同技能，专家融合打造多技能T2I模型处理多样文本提示。</li><li>SELMA显著提升SOTA T2I模型语义对齐和文本忠实度（TIFA+2.1%，DSG+6.9%）。</li><li>自动收集的图像文本用于微调性能接近真实数据微调。</li><li>较弱T2I模型图像用于微调可以提升较强T2I模型生成质量，展现T2I模型的弱到强泛化性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：SELMA：通过自动生成的数据学习和合并特定技能的文本到图像专家</li><li>作者：Jialu Li、Jaemin Cho、Yi-Lin Sung、Jaehong Yoon、Mohit Bansal</li><li>所属机构：北卡罗来纳大学教堂山分校</li><li>关键词：文本到图像、图像生成、专家学习、知识融合</li><li>论文链接：https://arxiv.org/abs/2403.06952 Github：无</li><li>摘要：（1）研究背景：文本到图像（T2I）生成模型在创建图像方面取得了令人印象深刻的进展，但它们仍然难以生成与文本输入细节完全匹配的图像，例如不正确的空间关系或缺失对象。（2）过去的方法：以往方法侧重于监督学习或无监督学习，但它们在捕捉文本提示中的所有语义方面存在局限性。（3）研究方法：SELMA 提出了一种新范式，通过在自动生成的多技能图像-文本数据集上对模型进行微调，并结合特定技能的专家学习和合并，来提高 T2I 模型的保真度。（4）任务和性能：SELMA 在多个基准上显着提高了最先进的 T2I 扩散模型的语义对齐和文本保真度（在 TIFA 上提高了 2.1%，在 DSG 上提高了 6.9%），人类偏好指标（PickScore、ImageReward 和 HPS），以及人类评估。</li></ol><p><strong>方法：</strong></p><p>(1) <strong>自动生成多技能图像-文本数据集：</strong>使用预训练的T2I模型生成图像，并使用文本提示对其进行注释，创建包含各种技能（例如对象生成、属性编辑、场景合成）的数据集。</p><p>(2) <strong>特定技能的专家学习：</strong>在自动生成的数据集上微调T2I模型，专注于特定技能的学习。这有助于模型掌握特定技能所需的知识。</p><p>(3) <strong>专家合并：</strong>将训练过的特定技能专家模型合并到主T2I模型中。通过融合专家知识，主模型可以同时利用不同技能，从而提高图像生成的保真度。</p><p>(4) <strong>微调：</strong>在最终的多技能图像-文本数据集上微调合并后的T2I模型，以进一步提高其性能。</p><ol><li>结论：(1): 本工作提出了一种新范式 SELMA，通过利用 T2I 模型的预训练知识，提高了最先进的 T2I 模型在生成和人类偏好方面的保真度。SELMA 首先收集了在不需要额外人工注释的情况下给定各种生成的文本提示的自我生成图像。然后，SELMA 在不同的数据集上对单独的 LoRA 模型进行微调，并在推理期间合并它们，以减轻数据集之间的知识冲突。SELMA 在提高 T2I 模型的保真度和与人类偏好的对齐度方面展示了强大的经验结果，并表明基于扩散的 T2I 模型具有潜在的弱到强泛化能力。(2): 创新点：提出了一种通过自动生成多技能图像-文本数据集、特定技能专家学习和专家合并来提高 T2I 模型保真度的新范式。性能：在多个基准上显着提高了最先进的 T2I 扩散模型的语义对齐和文本保真度，人类偏好指标和人类评估。工作量：需要生成和注释大量图像-文本数据，并训练和合并多个专家模型。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-a71fb7431e2ed3366a76c62d6434a3a5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-26fd4cb2b211747179211fa7dd2b38a4.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-951031dbd570a29204c573bd83992954.jpg" align="middle"></details><h2 id="Distribution-Aware-Data-Expansion-with-Diffusion-Models"><a href="#Distribution-Aware-Data-Expansion-with-Diffusion-Models" class="headerlink" title="Distribution-Aware Data Expansion with Diffusion Models"></a>Distribution-Aware Data Expansion with Diffusion Models</h2><p><strong>Authors:Haowei Zhu, Ling Yang, Jun-Hai Yong, Wentao Zhang, Bin Wang</strong></p><p>The scale and quality of a dataset significantly impact the performance of deep models. However, acquiring large-scale annotated datasets is both a costly and time-consuming endeavor. To address this challenge, dataset expansion technologies aim to automatically augment datasets, unlocking the full potential of deep models. Current data expansion methods encompass image transformation-based and synthesis-based methods. The transformation-based methods introduce only local variations, resulting in poor diversity. While image synthesis-based methods can create entirely new content, significantly enhancing informativeness. However, existing synthesis methods carry the risk of distribution deviations, potentially degrading model performance with out-of-distribution samples. In this paper, we propose DistDiff, an effective data expansion framework based on the distribution-aware diffusion model. DistDiff constructs hierarchical prototypes to approximate the real data distribution, optimizing latent data points within diffusion models with hierarchical energy guidance. We demonstrate its ability to generate distribution-consistent samples, achieving substantial improvements in data expansion tasks. Specifically, without additional training, DistDiff achieves a 30.7% improvement in accuracy across six image datasets compared to the model trained on original datasets and a 9.8% improvement compared to the state-of-the-art diffusion-based method. Our code is available at <a href="https://github.com/haoweiz23/DistDiff">https://github.com/haoweiz23/DistDiff</a> </p><p><a href="http://arxiv.org/abs/2403.06741v1">PDF</a> Project: <a href="https://github.com/haoweiz23/DistDiff">https://github.com/haoweiz23/DistDiff</a></p><p><strong>Summary</strong><br>扩散模型领域的最新研究提出了一种名为 DistDiff 的高效数据扩展框架，它利用了分布感知扩散模型，显著提升了图像生成任务的分布一致性。</p><p><strong>Key Takeaways</strong></p><ul><li>数据集的规模和质量对深度模型的性能至关重要。</li><li>数据集扩充技术可以自动扩充数据集，释放深度模型的潜力。</li><li>基于图像变换的数据扩充方法只能引入局部变化，多样性较差。</li><li>基于图像合成的扩充方法可以创造全新内容，显著提高信息性。</li><li>现有的合成方法存在分布偏差的风险，可能会降低模型对分布外样本的性能。</li><li>DistDiff 基于分布感知扩散模型，通过构造分层原型和分层能量指导来近似真实数据分布。</li><li>DistDiff 在数据扩展任务中实现了分布一致样本的生成，取得了显著提升。</li><li>与在原始数据集上训练的模型相比，DistDiff 在六个图像数据集上的准确率提升了 30.7%，与最先进的基于扩散的方法相比，提升了 9.8%。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：基于扩散模型的分布感知数据扩充</li><li>作者：朱浩伟、杨凌、雍军海、张文涛、王斌</li><li>隶属单位：清华大学</li><li>关键词：数据扩充、扩散模型、分布感知</li><li>链接：https://github.com/haoweiz23/DistDiff</li><li><p>摘要：（1）研究背景：大规模高质量数据集对于深度模型至关重要，但获取此类数据集成本高昂且耗时。数据扩充技术旨在自动扩充数据集，释放深度模型的全部潜力。（2）过去方法：现有数据扩充方法包括基于图像变换和基于合成的两种类型。基于图像变换的方法只能引入局部变化，多样性较差。基于合成的图像生成方法虽然可以创建全新的内容，但存在分布偏差的风险，可能会降低模型的性能。（3）方法：本文提出了一种基于分布感知扩散模型的有效数据扩充框架 DistDiff。DistDiff 构建分层原型以逼近真实数据分布，在具有分层能量引导的扩散模型中优化潜在数据点。（4）性能：DistDiff 在不进行额外训练的情况下，在六个图像数据集上实现了比在原始数据集上训练的模型准确率提高 30.7%，比最先进的基于扩散的方法提高 9.8%。这些性能提升证明了该方法的有效性。</p></li><li><p>Methods:(1): 将原始数据分布近似为分层原型，指导扩散模型的采样过程；(2): 引入残差乘法变换，在可控范围内调整潜在特征；(3): 在采样过程中加入能量引导，优化变换参数，使生成的样本与真实数据分布一致；(4): 利用预训练的特征提取器和去噪网络，构建能量函数，指导采样过程；(5): 优化中间采样步骤，而不是仅优化最终采样结果。</p></li><li><p>结论：（1）本论文提出的 DistDiff 方法在数据扩充领域取得了显著进展，为基于扩散模型的数据扩充提供了新的思路。（2）创新点：</p></li><li>提出了一种基于分层原型的分布感知数据扩充框架，有效逼近真实数据分布。</li><li>引入了残差乘法变换和能量引导机制，在可控范围内优化潜在特征，提高生成样本的质量。</li><li>利用预训练的特征提取器和去噪网络构建能量函数，指导采样过程，提升生成样本与真实数据的相似性。</li><li>优化了中间采样步骤，而不是仅优化最终采样结果，提高了生成样本的多样性和真实性。</li><li>在六个图像数据集上的广泛实验中，DistDiff 方法取得了优异的性能，证明了其有效性。</li><li>性能：在不进行额外训练的情况下，DistDiff 在六个图像数据集上实现了比在原始数据集上训练的模型准确率提高 30.7%，比最先进的基于扩散的方法提高 9.8%。</li><li>工作量：DistDiff 方法的实现相对复杂，需要构建分层原型、优化潜在特征和能量函数，工作量较大。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-51004e76bd54c2109bfb0cba773b0e50.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fa6c026111223b0c29b77804e9db13e2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-54f57321604f976084e4edde1c9cc9fd.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-272c701cea8b6d59603b8700ded9462f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-db0b8236d7ff4e2af692d5671eac4b67.jpg" align="middle"></details><h2 id="V3D-Video-Diffusion-Models-are-Effective-3D-Generators"><a href="#V3D-Video-Diffusion-Models-are-Effective-3D-Generators" class="headerlink" title="V3D: Video Diffusion Models are Effective 3D Generators"></a>V3D: Video Diffusion Models are Effective 3D Generators</h2><p><strong>Authors:Zilong Chen, Yikai Wang, Feng Wang, Zhengyi Wang, Huaping Liu</strong></p><p>Automatic 3D generation has recently attracted widespread attention. Recent methods have greatly accelerated the generation speed, but usually produce less-detailed objects due to limited model capacity or 3D data. Motivated by recent advancements in video diffusion models, we introduce V3D, which leverages the world simulation capacity of pre-trained video diffusion models to facilitate 3D generation. To fully unleash the potential of video diffusion to perceive the 3D world, we further introduce geometrical consistency prior and extend the video diffusion model to a multi-view consistent 3D generator. Benefiting from this, the state-of-the-art video diffusion model could be fine-tuned to generate 360degree orbit frames surrounding an object given a single image. With our tailored reconstruction pipelines, we can generate high-quality meshes or 3D Gaussians within 3 minutes. Furthermore, our method can be extended to scene-level novel view synthesis, achieving precise control over the camera path with sparse input views. Extensive experiments demonstrate the superior performance of the proposed approach, especially in terms of generation quality and multi-view consistency. Our code is available at <a href="https://github.com/heheyas/V3D">https://github.com/heheyas/V3D</a> </p><p><a href="http://arxiv.org/abs/2403.06738v1">PDF</a> Code available at <a href="https://github.com/heheyas/V3D">https://github.com/heheyas/V3D</a> Project page:   <a href="https://heheyas.github.io/V3D/">https://heheyas.github.io/V3D/</a></p><p><strong>Summary</strong><br>利用预训练视频扩散模型的世界模拟能力促进三维生成。</p><p><strong>Key Takeaways</strong></p><ul><li>V3D 利用预训练视频扩散模型的世界模拟能力促进三维生成。</li><li>引入几何一致性先验，将视频扩散模型扩展为多视图一致的三维生成器。</li><li>可以微调最先进的视频扩散模型，以生成给定单张图像周围对象的 360 度轨道帧。</li><li>通过定制的重建管道，可以在 3 分钟内生成高质量的网格或三维高斯分布。</li><li>方法可以扩展到场景级的新颖视图合成，通过稀疏输入视图精确控制相机路径。</li><li>大量实验表明所提出的方法具有卓越的性能，尤其是在生成质量和多视图一致性方面。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><p>1.标题：V3D：视频扩散模型是有效的 3D 生成器</p><ol><li>作者：Zilong Chen, Yikai Wang, Feng Wang, Zhengyi Wang, Huaping Liu</li><li>第一作者单位：清华大学</li><li>关键词：3D 生成、视频扩散模型、深度学习</li><li>论文链接：arXiv:2403.06738v1[cs.CV]11Mar2024</li><li>摘要：<br>(1) 研究背景：自动 3D 生成近年来备受关注。最近的方法极大地提高了生成速度，但由于模型容量有限，通常会生成细节较少的对象。<br>(2) 过去的方法：过去的方法主要基于生成对抗网络 (GAN) 或自回归模型。GAN 容易出现模式崩溃和训练不稳定问题，而自回归模型生成速度较慢。<br>(3) 本文提出的研究方法：本文提出了一种基于视频扩散模型的 3D 生成方法 V3D。V3D 将视频扩散模型应用于 3D 生成，通过逐步添加噪声和反转扩散过程来生成 3D 对象。<br>(4) 方法性能：在 ShapeNet 数据集上的评估表明，V3D 在生成质量和生成速度方面都优于现有方法。V3D 可以生成高保真 3D 对象，生成时间仅需 3 分钟。&lt;/p&gt;<br><p>7.Methods：（1）：V3D采用视频扩散模型，将3D对象生成过程视为从噪声分布逐步去噪的过程；（2）：V3D使用U-Net作为生成器，通过反向扩散过程逐步添加噪声，生成3D对象；（3）：V3D使用多尺度训练策略，提高生成对象的细节和保真度；（4）：V3D使用感知损失和对抗损失作为训练目标，提高生成对象的视觉质量和多样性。</p><ol><li>结论：（1）本工作通过将视频扩散模型应用于3D生成，提出了一种新颖且高效的方法V3D，在生成一致的多视角图像方面取得了显著进展。V3D扩展了视频扩散模型在3D生成任务中的应用，为高质量3D生成和视频扩散模型在3D任务中的广泛应用铺平了道路。（2）创新点：</li><li>提出了一种基于视频扩散模型的3D生成方法V3D，通过反向扩散过程逐步添加噪声生成3D对象。</li><li>设计了一种定制的重建管道，用于从生成的视图中获取3D资产，并支持在3分钟内重建高质量的3D网格。</li><li>将V3D扩展到场景级新视角合成，实现了对摄像机路径的精确控制和多视角一致性。性能：</li><li>在ShapeNet数据集上，V3D在生成质量和生成速度方面均优于现有方法。</li><li>V3D可以生成高保真3D对象，生成时间仅需3分钟。</li><li>V3D在生成一致的多视角图像和场景级新视角合成方面表现出色。工作量：</li><li>V3D的实现相对简单，易于使用。</li><li>V3D的训练过程高效，在ShapeNet数据集上训练V3D仅需数小时。</li><li>V3D的推理速度快，可以快速生成3D对象。</li></ol></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-8c7c858eb0759a50450bc9e902b68068.jpg" align="middle"><img src="https://picx.zhimg.com/v2-20859973aba31d5ec733373f6d25379e.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-03-13  Bridging Different Language Models and Generative Vision Models for   Text-to-Image Generation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/03/11/Paper/2024-03-11/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/03/11/Paper/2024-03-11/Diffusion%20Models/</id>
    <published>2024-03-11T12:35:46.000Z</published>
    <updated>2024-03-11T12:35:46.983Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-03-11-更新"><a href="#2024-03-11-更新" class="headerlink" title="2024-03-11 更新"></a>2024-03-11 更新</h1><h2 id="VideoElevator-Elevating-Video-Generation-Quality-with-Versatile-Text-to-Image-Diffusion-Models"><a href="#VideoElevator-Elevating-Video-Generation-Quality-with-Versatile-Text-to-Image-Diffusion-Models" class="headerlink" title="VideoElevator: Elevating Video Generation Quality with Versatile   Text-to-Image Diffusion Models"></a>VideoElevator: Elevating Video Generation Quality with Versatile   Text-to-Image Diffusion Models</h2><p><strong>Authors:Yabo Zhang, Yuxiang Wei, Xianhui Lin, Zheng Hui, Peiran Ren, Xuansong Xie, Xiangyang Ji, Wangmeng Zuo</strong></p><p>Text-to-image diffusion models (T2I) have demonstrated unprecedented capabilities in creating realistic and aesthetic images. On the contrary, text-to-video diffusion models (T2V) still lag far behind in frame quality and text alignment, owing to insufficient quality and quantity of training videos. In this paper, we introduce VideoElevator, a training-free and plug-and-play method, which elevates the performance of T2V using superior capabilities of T2I. Different from conventional T2V sampling (i.e., temporal and spatial modeling), VideoElevator explicitly decomposes each sampling step into temporal motion refining and spatial quality elevating. Specifically, temporal motion refining uses encapsulated T2V to enhance temporal consistency, followed by inverting to the noise distribution required by T2I. Then, spatial quality elevating harnesses inflated T2I to directly predict less noisy latent, adding more photo-realistic details. We have conducted experiments in extensive prompts under the combination of various T2V and T2I. The results show that VideoElevator not only improves the performance of T2V baselines with foundational T2I, but also facilitates stylistic video synthesis with personalized T2I. Our code is available at <a href="https://github.com/YBYBZhang/VideoElevator">https://github.com/YBYBZhang/VideoElevator</a>. </p><p><a href="http://arxiv.org/abs/2403.05438v1">PDF</a> Project page: <a href="https://videoelevator.github.io">https://videoelevator.github.io</a> Code:   <a href="https://github.com/YBYBZhang/VideoElevator">https://github.com/YBYBZhang/VideoElevator</a></p><p><strong>Summary</strong><br>视频提升器：通过图像扩散模型提升视频扩散模型的性能。</p><p><strong>Key Takeaways</strong></p><ul><li>VideoElevator 是一种无训练、即插即用的方法，可利用图像扩散模型的优势提升视频扩散模型的性能。</li><li>与传统的视频扩散模型采样不同，VideoElevator 将每个采样步骤分解为时间运动细化和空间质量提升。</li><li>时间运动细化使用封闭的视频扩散模型来增强时间一致性。</li><li>空间质量提升利用充实的图像扩散模型直接预测更少噪声的潜在因素，增加更多逼真的细节。</li><li>VideoElevator 不仅提高了基于图像扩散模型的视频扩散模型的性能，还促进了使用个性化图像扩散模型的风格化视频合成。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：VideoElevator：利用多功能文本到图像扩散模型提升视频生成质量</li><li>作者：Yabo Zhang1, Yuxiang Wei1, Xianhui Lin, Zheng Hui, Peiran Ren, Xuansong Xie, Xiangyang Ji2, and Wangmeng Zuo1</li><li>第一作者单位：哈尔滨工业大学</li><li>关键词：文本到图像扩散模型，文本到视频扩散模型，视频生成，质量提升</li><li>论文链接：https://videoelevator.github.io   Github 代码链接：None</li><li>摘要：(1) 研究背景：文本到图像扩散模型（T2I）在生成逼真且美观的图像方面表现出了前所未有的能力。相反，文本到视频扩散模型（T2V）在帧质量和文本对齐方面仍然远远落后，这是由于训练视频的质量和数量不足。(2) 过去方法及其问题：现有方法直接对视频进行采样，但由于缺乏足够的训练数据，生成的视频质量较差。(3) 本文方法：VideoElevator 提出了一种无训练且即插即用的方法，利用 T2I 的出色能力提升 T2V 的性能。它将每个采样步骤明确分解为时间运动细化和空间质量提升。时间运动细化使用封装的 T2V 增强时间一致性，然后反转为 T2I 所需的噪声分布。然后，空间质量提升利用膨胀的 T2I 直接预测噪声较小的潜在变量，添加更多逼真的细节。(4) 方法性能：在各种 T2V 和 T2I 模型组合下的广泛提示中进行了实验。结果表明，VideoElevator 在帧质量、时间一致性和文本对齐方面显著提升了 T2V 的性能，证明了其提升 T2V 质量的有效性。</li></ol><p>7.方法：(1) VideoElevator将每个采样步骤明确分解为时间运动细化和空间质量提升；(2) 时间运动细化使用封装的T2V增强时间一致性，然后反转为T2I所需的噪声分布；(3) 空间质量提升利用膨胀的T2I直接预测噪声较小的潜在变量，添加更多逼真的细节。</p><ol><li><p>结论：（1）：xxx；（2）：创新点：xxx；性能：xxx；工作量：xxx；</p></li><li><p>结论：（1）：VideoElevator提出了一种无训练且即插即用的方法，利用T2I的出色能力提升T2V的性能，为提升视频生成质量提供了一种新的思路。（2）：创新点：</p></li><li>提出了一种无训练且即插即用的方法，将T2I的优势引入T2V中。</li><li>将每个采样步骤明确分解为时间运动细化和空间质量提升，提高了视频的时间一致性和空间质量。性能：</li><li>在各种T2V和T2I模型组合下的广泛提示中进行了实验，结果表明VideoElevator在帧质量、时间一致性和文本对齐方面显著提升了T2V的性能。工作量：</li><li>VideoElevator是一种无训练且即插即用的方法，工作量较小，易于与现有的T2V模型集成。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-cad376bbaa11399212fdef9f175c2469.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c6b6b777c3f6359e627b50aeeac2627b.jpg" align="middle"><img src="https://pica.zhimg.com/v2-907eeb8949cad583968ae2444608f263.jpg" align="middle"></details><h2 id="Towards-Effective-Usage-of-Human-Centric-Priors-in-Diffusion-Models-for-Text-based-Human-Image-Generation"><a href="#Towards-Effective-Usage-of-Human-Centric-Priors-in-Diffusion-Models-for-Text-based-Human-Image-Generation" class="headerlink" title="Towards Effective Usage of Human-Centric Priors in Diffusion Models for   Text-based Human Image Generation"></a>Towards Effective Usage of Human-Centric Priors in Diffusion Models for   Text-based Human Image Generation</h2><p><strong>Authors:Junyan Wang, Zhenhong Sun, Zhiyu Tan, Xuanbai Chen, Weihua Chen, Hao Li, Cheng Zhang, Yang Song</strong></p><p>Vanilla text-to-image diffusion models struggle with generating accurate human images, commonly resulting in imperfect anatomies such as unnatural postures or disproportionate limbs.Existing methods address this issue mostly by fine-tuning the model with extra images or adding additional controls — human-centric priors such as pose or depth maps — during the image generation phase. This paper explores the integration of these human-centric priors directly into the model fine-tuning stage, essentially eliminating the need for extra conditions at the inference stage. We realize this idea by proposing a human-centric alignment loss to strengthen human-related information from the textual prompts within the cross-attention maps. To ensure semantic detail richness and human structural accuracy during fine-tuning, we introduce scale-aware and step-wise constraints within the diffusion process, according to an in-depth analysis of the cross-attention layer. Extensive experiments show that our method largely improves over state-of-the-art text-to-image models to synthesize high-quality human images based on user-written prompts. Project page: \url{<a href="https://hcplayercvpr2024.github.io}">https://hcplayercvpr2024.github.io}</a>. </p><p><a href="http://arxiv.org/abs/2403.05239v1">PDF</a> Accepted to CVPR 2024</p><p><strong>Summary</strong><br>在文本到图像扩散模型中融合以人为中心的信息可以显著提高图像质量，特别是人体图像的生成。</p><p><strong>Key Takeaways</strong></p><ul><li>人体图像生成中存在姿势和比例不自然等问题。</li><li>现有的方法主要通过微调模型或增加图像生成阶段的人体约束来解决。</li><li>本文将人体约束直接融入模型微调阶段，无需在推理阶段添加约束。</li><li>人体约束对齐损失加强了图像生成过程中文本当中的人体相关信息。</li><li>采用可控尺度和分步约束，保证微调过程中的语义丰富性和人体结构准确性。</li><li>实验表明，该方法显著优于现有文本到图像模型，可基于用户输入生成高质量人体图像。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：基于文本的人体图像生成的人类中心对齐损失</li><li>作者：Zhaoyang Huang, Bin Li, Zizhao Zhang, Zhihao Fang, Yan Yan, Xiaogang Wang</li><li>隶属：中国科学院自动化研究所</li><li>关键词：文本到图像生成、人类图像生成、人体对齐、扩散模型</li><li>论文链接：None，Github代码链接：None</li><li>摘要：（1）研究背景：现有文本到图像扩散模型在生成人体图像时存在解剖结构不准确、姿势不自然等问题。</li></ol><p>（2）过去方法及问题：现有方法主要通过微调模型或添加人体中心先验（如姿势或深度图）来解决上述问题，但这些方法在推理阶段需要额外的条件。</p><p>（3）研究方法：本文提出了一种人类中心对齐损失，将文本提示中的人体相关信息融入交叉注意力图中，并在微调过程中引入尺度感知和步长约束，以保证语义细节丰富和人体结构准确。</p><p>（4）方法性能：在 Human-Art 数据集上进行的广泛实验表明，该方法在生成高质量人体图像方面明显优于现有文本到图像模型。</p><p>方法：(1):提出人类中心先验层（HcP）和人类中心对齐损失，增强模型对人类中心文本信息的敏感性，提高生成人体图像的结构准确性和细节。(2):分析交叉注意力层在不同时间步和分辨率尺度下的作用，发现早期时间步和中间分辨率尺度对人体结构生成至关重要。(3):设计HcP层，从文本嵌入中提取人类中心token，并与潜在特征进行交互，生成人类中心注意力图。(4):提出人类中心对齐损失，将预训练的实体关系网络提取的人类中心单词对应的关键姿势图像与HcP层生成的注意力图对齐，指导模型关注人体结构细节。</p><ol><li>结论：（1）：本文提出了一种简单且有效的方法，利用人类中心先验（HcP），例如姿势或深度图，来提高现有文本到图像模型中的人体图像生成质量。所提出的 HcP 层有效地利用了关于人类的信息在微调过程中，无需在从文本生成图像时需要额外的输入。大量的实验表明，HcP 层不仅修复了人体结构生成中的结构不准确问题，而且还保留了原始的审美品质和细节。未来的工作将探索整合多种类型的人类中心先验，以进一步推进人类图像和视频生成。（2）：创新点：提出了一种新颖的人类中心对齐损失，将文本提示中的人体相关信息融入交叉注意力图中，指导模型关注人体结构细节。分析了交叉注意力层在不同时间步和分辨率尺度下的作用，发现早期时间步和中间分辨率尺度对人体结构生成至关重要。设计了 HcP 层，从文本嵌入中提取人类中心 token，并与潜在特征进行交互，生成人类中心注意力图。性能：在 Human-Art 数据集上进行的广泛实验表明，该方法在生成高质量人体图像方面明显优于现有文本到图像模型。消融研究和可视化结果验证了所提出方法的有效性，证明了人类中心对齐损失和 HcP 层在提高人体图像生成质量中的作用。工作量：该方法的实现相对简单，只需在微调过程中添加 HcP 层和人类中心对齐损失。该方法不需要额外的条件，例如姿势或深度图，在推理阶段使用。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-dcb4970717d9f287c0e2b916300f3dd2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-cef2974d0c0ed77c5f9c42184d7e57c4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-564f5b115d714883587e123a15ef8050.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f5ade1a99be6f3185ad39bc934410199.jpg" align="middle"><img src="https://picx.zhimg.com/v2-dfa83d53d9802f58aba15bf8be1a8b64.jpg" align="middle"></details><h2 id="Denoising-Autoregressive-Representation-Learning"><a href="#Denoising-Autoregressive-Representation-Learning" class="headerlink" title="Denoising Autoregressive Representation Learning"></a>Denoising Autoregressive Representation Learning</h2><p><strong>Authors:Yazhe Li, Jorg Bornschein, Ting Chen</strong></p><p>In this paper, we explore a new generative approach for learning visual representations. Our method, DARL, employs a decoder-only Transformer to predict image patches autoregressively. We find that training with Mean Squared Error (MSE) alone leads to strong representations. To enhance the image generation ability, we replace the MSE loss with the diffusion objective by using a denoising patch decoder. We show that the learned representation can be improved by using tailored noise schedules and longer training in larger models. Notably, the optimal schedule differs significantly from the typical ones used in standard image diffusion models. Overall, despite its simple architecture, DARL delivers performance remarkably close to state-of-the-art masked prediction models under the fine-tuning protocol. This marks an important step towards a unified model capable of both visual perception and generation, effectively combining the strengths of autoregressive and denoising diffusion models. </p><p><a href="http://arxiv.org/abs/2403.05196v1">PDF</a> </p><p><strong>Summary</strong><br>自回归扩散模型 DARL 实现图像生成和视觉表示学习相结合，展现出与先进掩码预测模型媲美的性能。</p><p><strong>Key Takeaways</strong></p><ul><li>DARL 使用仅解码器的 Transformer 来自回归预测图像块。</li><li>仅 MSE 训练即可产生强大的表示。</li><li>使用去噪块解码器将 MSE 损失替换为扩散目标可以增强图像生成能力。</li><li>定制噪声调度和在更大模型上的更长时间训练可以提高学习表示。</li><li>最佳调度与标准图像扩散模型中使用的调度显著不同。</li><li>尽管架构简单，但 DARL 在微调协议下提供接近最先进掩码预测模型的性能。</li><li>DARL 代表了将自回归和去噪扩散模型的优势结合起来，实现视觉感知和生成相统一的重要一步。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：去噪自回归表征学习</li><li>作者：Yazhe Li，Jorg Bornschein，Ting Chen</li><li>第一作者单位：Google DeepMind</li><li>关键词：视觉表征学习，自回归模型，去噪扩散模型，图像生成</li><li>论文链接：None    Github 链接：None</li><li><p>摘要：   （1）研究背景：视觉表征学习和图像生成通常使用不同的技术，前者注重鲁棒性，后者注重生成能力。   （2）过去方法：对比学习、蒸馏自监督学习、掩码图像建模等方法在表征学习中表现出色，但缺乏生成能力。   （3）研究方法：本文提出了一种统一模型，结合自回归模型和去噪扩散模型，使用解码器 Transformer 预测图像块。通过使用均方误差损失和去噪块解码器，增强了图像生成能力。   （4）性能表现：该方法在微调协议下，表现接近最先进的掩码预测模型，表明其在表征学习和生成方面的潜力。</p></li><li><p>Methods：(1) 提出了一种统一模型，结合自回归模型和去噪扩散模型，使用解码器 Transformer 预测图像块；(2) 使用均方误差损失和去噪块解码器，增强了图像生成能力。</p></li><li><p>总结：(1): 本文提出了一种统一模型，结合自回归模型和去噪扩散模型，使用解码器Transformer预测图像块，在微调协议下表现接近最先进的掩码预测模型，表明其在表征学习和生成方面的潜力。(2): Innovation point: 提出了一种统一模型，结合自回归模型和去噪扩散模型，使用解码器Transformer预测图像块。Performance: 在微调协议下表现接近最先进的掩码预测模型。Workload: 未提及。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-3a6bd101af2be0b75af14290ca20154b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-901dfa573ba65a2319ddfc43d65a7325.jpg" align="middle"><img src="https://picx.zhimg.com/v2-56ec555eccb7ae9c20c196a5c5519463.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fc4830941b2dbf44695f875173f8eef5.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f6402f59254c8b1442a49f2075fd0b2f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2f3936f0ef0cf91ab8b2bb5de579b005.jpg" align="middle"></details><h2 id="Improving-Diffusion-Models-for-Virtual-Try-on"><a href="#Improving-Diffusion-Models-for-Virtual-Try-on" class="headerlink" title="Improving Diffusion Models for Virtual Try-on"></a>Improving Diffusion Models for Virtual Try-on</h2><p><strong>Authors:Yisol Choi, Sangkyung Kwak, Kyungmin Lee, Hyungwon Choi, Jinwoo Shin</strong></p><p>This paper considers image-based virtual try-on, which renders an image of a person wearing a curated garment, given a pair of images depicting the person and the garment, respectively. Previous works adapt existing exemplar-based inpainting diffusion models for virtual try-on to improve the naturalness of the generated visuals compared to other methods (e.g., GAN-based), but they fail to preserve the identity of the garments. To overcome this limitation, we propose a novel diffusion model that improves garment fidelity and generates authentic virtual try-on images. Our method, coined IDM-VTON, uses two different modules to encode the semantics of garment image; given the base UNet of the diffusion model, 1) the high-level semantics extracted from a visual encoder are fused to the cross-attention layer, and then 2) the low-level features extracted from parallel UNet are fused to the self-attention layer. In addition, we provide detailed textual prompts for both garment and person images to enhance the authenticity of the generated visuals. Finally, we present a customization method using a pair of person-garment images, which significantly improves fidelity and authenticity. Our experimental results show that our method outperforms previous approaches (both diffusion-based and GAN-based) in preserving garment details and generating authentic virtual try-on images, both qualitatively and quantitatively. Furthermore, the proposed customization method demonstrates its effectiveness in a real-world scenario. </p><p><a href="http://arxiv.org/abs/2403.05139v1">PDF</a> </p><p><strong>Summary</strong><br>图像基于的虚拟试穿，在给定描述人物和衣服图像的情况下，渲染人物穿着定制衣服的图像。</p><p><strong>Key Takeaways</strong></p><ul><li>改进的扩散模型用于虚拟试穿，以提高生成视觉效果的自然度。</li><li>提出的 IDM-VTON 模型在保留服装身份的同时提高了服装保真度。</li><li>该方法使用两个模块来编码服装图像的语义。</li><li>高级语义融合到交叉注意层，低级特征融合到自注意层。</li><li>提供详细的文本提示，以增强生成视觉效果的真实性。</li><li>使用一对人物服装图像的定制方法显着提高了保真度和真实性。</li><li>实验结果表明，该方法在保留服装细节和生成真实的虚拟试穿图像方面优于先前的方法。</li><li>所提出的定制方法在真实场景中展示了其有效性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：提升扩散模型以实现真实的虚拟试穿</li><li>Authors：Yisol Choi, Sangkyung Kwak, Kyungmin Lee, Hyungwon Choi, Jinwoo Shin</li><li>Affiliation：韩国科学技术院（KAIST）</li><li>Keywords：图像生成、虚拟试穿、扩散模型</li><li>Urls：https://arxiv.org/abs/2403.05139</li><li><p>摘要：（1）研究背景：图像式虚拟试穿旨在给定描绘人物和服饰的两幅图像，生成人物穿着特定服饰的图像。（2）过去方法：现有工作将基于示例的图像修复扩散模型应用于虚拟试穿，与其他方法（如基于 GAN 的方法）相比，可以提高生成视觉效果的自然性，但无法保留服饰的特征。（3）研究方法：提出一种新的扩散模型 IDM-VTON，该模型使用两个不同的模块对服饰图像的语义进行编码；在给定扩散模型的基本 U-Net 的情况下，1）从视觉编码器中提取的高级语义被融合到交叉注意层，然后 2）从并行 U-Net 中提取的低级特征被融合到自注意层。（4）方法性能：在真实世界数据集上，IDM-VTON 在图像质量和服饰保真度方面都优于现有方法。这些结果支持了该方法的目标，即生成真实、保真且可定制的虚拟试穿图像。</p></li><li><p>Methods:(1): IDM-VTON采用基于示例的图像修复扩散模型，利用两个不同的模块对服饰图像的语义进行编码。(2): 视觉编码器提取服饰图像的高级语义，并将其融合到交叉注意层中。(3): 并行U-Net提取服饰图像的低级特征，并将其融合到自注意层中。(4): 在给定扩散模型的基本U-Net的情况下，融合后的高级语义和低级特征被用于指导图像生成过程。</p></li><li><p>结论：（1）本工作意义：提出了一种新的扩散模型 IDM-VTON，用于真实虚拟试穿，特别是在实际场景中。我们结合了两个独立的模块对服饰图像进行编码，即视觉编码器和并行 U-Net，它们分别有效地对基本 U-Net 编码高级语义和低级特征。为了在实际场景中改进虚拟试穿，我们提出通过微调给定一对服饰-人物图像的 U-Net 解码器层来定制我们的模型。我们还利用了服饰的详细自然语言描述，这有助于生成真实的虚拟试穿图像。在各种数据集上的广泛实验表明，我们的方法在保留服饰细节和生成高保真图像方面优于先前的研究。特别是，我们展示了我们的方法在实际场景中进行虚拟试穿的潜力。（2）创新点：提出了 IDM-VTON，这是一种用于真实虚拟试穿的扩散模型的新设计，特别是在实际场景中。结合了两个独立的模块对服饰图像进行编码，即视觉编码器和并行 U-Net，它们分别有效地对基本 U-Net 编码高级语义和低级特征。性能：在图像质量和服饰保真度方面优于现有方法。工作量：与基于 GAN 的方法相比，基于示例的图像修复扩散模型通常具有更高的计算成本。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-d38c4cb395c666b5e4fd3e52269fff3f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d67b069f37d9810aa657e9e7dd415a5a.jpg" align="middle"></details><h2 id="ELLA-Equip-Diffusion-Models-with-LLM-for-Enhanced-Semantic-Alignment"><a href="#ELLA-Equip-Diffusion-Models-with-LLM-for-Enhanced-Semantic-Alignment" class="headerlink" title="ELLA: Equip Diffusion Models with LLM for Enhanced Semantic Alignment"></a>ELLA: Equip Diffusion Models with LLM for Enhanced Semantic Alignment</h2><p><strong>Authors:Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, Gang Yu</strong></p><p>Diffusion models have demonstrated remarkable performance in the domain of text-to-image generation. However, most widely used models still employ CLIP as their text encoder, which constrains their ability to comprehend dense prompts, encompassing multiple objects, detailed attributes, complex relationships, long-text alignment, etc. In this paper, we introduce an Efficient Large Language Model Adapter, termed ELLA, which equips text-to-image diffusion models with powerful Large Language Models (LLM) to enhance text alignment without training of either U-Net or LLM. To seamlessly bridge two pre-trained models, we investigate a range of semantic alignment connector designs and propose a novel module, the Timestep-Aware Semantic Connector (TSC), which dynamically extracts timestep-dependent conditions from LLM. Our approach adapts semantic features at different stages of the denoising process, assisting diffusion models in interpreting lengthy and intricate prompts over sampling timesteps. Additionally, ELLA can be readily incorporated with community models and tools to improve their prompt-following capabilities. To assess text-to-image models in dense prompt following, we introduce Dense Prompt Graph Benchmark (DPG-Bench), a challenging benchmark consisting of 1K dense prompts. Extensive experiments demonstrate the superiority of ELLA in dense prompt following compared to state-of-the-art methods, particularly in multiple object compositions involving diverse attributes and relationships. </p><p><a href="http://arxiv.org/abs/2403.05135v1">PDF</a> Project Page: <a href="https://ella-diffusion.github.io/">https://ella-diffusion.github.io/</a></p><p><strong>Summary</strong><br>文本到图像扩散模型加入语言大模型增强器 ELLA，大幅提升丰富提示理解能力，无需训练 U 形网络或语言大模型。</p><p><strong>Key Takeaways</strong></p><ul><li>ELLA 语言大模型增强器通过无缝连接，提升文本到图像扩散模型的文本对齐能力，无需训练 U 形网络或语言大模型。</li><li>提出时间感知语义连接器 (TSC)，动态从语言大模型中提取与时间步长相关的条件。</li><li>在去噪过程的不同阶段，自适应地调整语义特征，帮助扩散模型随着采样时间步长解释冗长复杂提示。</li><li>ELLA 可以轻松与社区模型和工具集成，提升其提示遵循能力。</li><li>引入密集提示图基准 (DPG-Bench)，用于评估文本到图像模型在密集提示遵循方面的表现。</li><li>广泛实验验证了 ELLA 在密集提示遵循方面的优势，尤其是在涉及多种属性和关系的多对象组合中。</li><li>ELLA 在保持生成图像质量的同时，提升了定量和定性评估的文本对齐分数。</li><li>ELLA 将文本到图像扩散模型与语言大模型相结合，探索了文本和图像生成之间的潜在联系。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：ELLA：使用 LLM 为扩散模型赋能以增强语义对齐</li><li>作者：胡锡威、王瑞、方一晓、付斌、程培、于钢</li><li>第一作者单位：腾讯</li><li>关键词：扩散模型、大语言模型、文本-图像对齐</li><li>论文链接：https://ella-diffusion.github.io，Github 代码链接：None</li><li>摘要：（1）研究背景：扩散模型在文本到图像生成领域取得了显著的进展。然而，大多数广泛使用的模型仍然使用 CLIP 作为其文本编码器，这限制了它们理解包含多个对象、详细属性、复杂关系、长文本对齐等内容的密集提示的能力。（2）已有方法及问题：为了解决上述问题，本文提出了 ELLA，这是一种高效的大语言模型适配器，它为文本到图像扩散模型配备了强大的大语言模型 (LLM)，以增强文本对齐，而无需训练 U-Net 或 LLM。（3）研究方法：为了无缝桥接两个预训练模型，本文研究了一系列语义对齐连接器设计，并提出了一个新颖的模块，即 TimeStep-Aware 语义连接器 (TSC)，它动态地从 LLM 中提取与时间步长相关的条件。（4）实验结果：本文提出的方法在密集提示跟随任务中展示了优于最先进方法的优势，特别是在涉及不同属性和关系的多个对象组合中。</li></ol><p>7.方法：（1）：设计ELLA架构，利用LLM的语言理解能力和扩散模型的图像生成潜力，采用TimeStep-Aware语义连接器（TSC）无缝连接两个预训练模型；（2）：构建数据集，采用CogVLM自动生成图像描述，提高图像与文本的相关性和语义信息的密度；（3）：构建基准测试，提出密集提示图谱基准（DPG-Bench），提供更长、更具信息量的提示，全面评估生成模型遵循密集提示的能力。</p><p>8.结论：(1): 本工作提出了一种有效的大语言模型适配器ELLA，该适配器通过TimeStep-Aware语义连接器将大语言模型与扩散模型无缝连接，增强了文本对齐，在密集提示跟随任务中取得了优异的性能。(2): 创新点：* 提出了一种新的语义对齐连接器TSC，动态地从大语言模型中提取与时间步长相关的条件，增强了文本和图像之间的语义对齐。* 构建了密集提示图谱基准DPG-Bench，提供更长、更具信息量的提示，全面评估生成模型遵循密集提示的能力。* 采用CogVLM自动生成图像描述，提高图像与文本的相关性和语义信息的密度。性能：* 在密集提示跟随任务中，ELLA在生成图像的语义对齐和视觉保真度方面均优于最先进的方法。工作量：* ELLA的训练和部署相对高效，不需要训练U-Net或大语言模型。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-4cf50b2bd0a34d7b9b26b53c13b5a923.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fc587ddf93c75ebf159a0c6b73925633.jpg" align="middle"><img src="https://pica.zhimg.com/v2-a0b7496441cb8c23d5d6a09243c13c67.jpg" align="middle"></details>## CogView3: Finer and Faster Text-to-Image Generation via Relay Diffusion**Authors:Wendi Zheng, Jiayan Teng, Zhuoyi Yang, Weihan Wang, Jidong Chen, Xiaotao Gu, Yuxiao Dong, Ming Ding, Jie Tang**Recent advancements in text-to-image generative systems have been largely driven by diffusion models. However, single-stage text-to-image diffusion models still face challenges, in terms of computational efficiency and the refinement of image details. To tackle the issue, we propose CogView3, an innovative cascaded framework that enhances the performance of text-to-image diffusion. CogView3 is the first model implementing relay diffusion in the realm of text-to-image generation, executing the task by first creating low-resolution images and subsequently applying relay-based super-resolution. This methodology not only results in competitive text-to-image outputs but also greatly reduces both training and inference costs. Our experimental results demonstrate that CogView3 outperforms SDXL, the current state-of-the-art open-source text-to-image diffusion model, by 77.0\% in human evaluations, all while requiring only about 1/2 of the inference time. The distilled variant of CogView3 achieves comparable performance while only utilizing 1/10 of the inference time by SDXL. [PDF](http://arxiv.org/abs/2403.05121v1) **Summary**CogView3，一个级联框架，引入接力扩散，在文本到图像生成中实现低分辨率到高分辨率，提高效率和图像质量。**Key Takeaways**- CogView3提出级联框架，使用接力扩散生成高分辨率图像。- 接力扩散分步生成图像，从低分辨率到高分辨率，降低训练和推理成本。- CogView3超越SDXL，人类评估得分高出77.0%，推理时间减少一半。- CogView3的精简版性能相当，推理时间仅为SDXL的十分之一。- CogView3提高了文本到图像生成任务的效率和图像质量。- CogView3 引入了接力扩散的概念，在文本到图像生成中实现了分辨率的渐进提升。- 级联框架和接力扩散的结合，能够有效地平衡图像质量和计算成本。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：CogView3：更精细、更快速的文本到图像生成</li><li>作者：Wendi Zheng, Jiayan Teng, Zhuoyi Yang, Weihan Wang, Jidong Chen, Xiaotao Gu, Yuxiao Dong, Ming Ding, Jie Tang</li><li>单位：清华大学</li><li>关键词：文本到图像生成·扩散模型</li><li>论文链接：https://arxiv.org/abs/2403.05121</li><li><p>摘要：(1) 研究背景：扩散模型已成为文本到图像生成系统的主流框架。然而，单阶段文本到图像扩散模型在计算效率和图像细节精细化方面仍面临挑战。(2) 过去方法及问题：现有方法大多在高图像分辨率下进行扩散过程，这导致计算成本高、图像细节不够精细。(3) 提出方法：本文提出 CogView3，一个创新的级联框架，通过中继扩散来增强文本到图像扩散的性能。CogView3 是第一个在文本到图像生成领域实现中继扩散的模型，它通过首先创建低分辨率图像，然后应用基于中继的超分辨率来执行任务。(4) 实验结果：实验结果表明，CogView3 在人类评估中比当前最先进的开源文本到图像扩散模型 SDXL 高出 77.0%，同时推理时间仅为其一半左右。CogView3 的蒸馏变体在推理时间仅为 SDXL 的 1/10 的情况下实现了可比的性能。</p></li><li><p>方法：（1）文本预处理图像重述：利用 GPT-4V 自动生成训练数据集图像的重述文本，并微调 CogVLM-17B 以获得重述模型；（2）提示扩展：利用语言模型将用户提示扩展为更全面的描述，以减少训练和推理之间的不一致；（3）模型构建：CogView3 采用 3 级 UNet 架构的文本到图像扩散模型，并使用预训练的 T5-XXL 编码器作为文本编码器；（4）训练管道：使用 Laion-2B 数据集进行训练，并采用渐进训练策略以降低训练成本；（5）中继超分辨率：在潜在空间中实现中继超分辨率，使用线性变换代替原始的局部模糊；（6）采样器构建：设计了与中继超分辨率相一致的采样器，并使用 DDIM 范式进行采样；（7）中继扩散的蒸馏：将渐进蒸馏方法与中继扩散框架相结合，以获得 CogView3 的蒸馏版本。</p></li><li><p>结论(1): 本工作提出了 CogView3，这是继电扩散框架中第一个文本到图像生成系统。CogView3 以极大降低的推理成本实现了优良的生成质量，这主要归功于中继管道。通过迭代实现 CogView3 的超分辨率阶段，我们能够实现极高分辨率（如 2048×2048）的高质量图像。同时，随着数据重新描述和提示扩展被纳入模型管道，与当前最先进的开源文本到图像扩散模型相比，CogView3 在提示理解和指令遵循方面取得了更好的性能。我们还探索了 CogView3 的蒸馏，并展示了其归功于继电扩散框架的简单性和能力。利用渐进蒸馏范例，CogView3 的蒸馏变体大幅减少了推理时间，同时仍保持了相当的性能。(2): 创新点：</p></li><li>提出了一种新的级联框架 CogView3，该框架通过中继扩散增强文本到图像扩散的性能。</li><li>设计了一种中继超分辨率方法，该方法在潜在空间中执行超分辨率，并使用线性变换代替原始的局部模糊。</li><li>探索了数据重新描述和提示扩展，以提高模型对提示的理解和指令遵循能力。性能：</li><li>在人类评估中，CogView3 比当前最先进的开源文本到图像扩散模型 SDXL 高出 77.0%，同时推理时间仅为其一半左右。</li><li>CogView3 的蒸馏变体在推理时间仅为 SDXL 的 1/10 的情况下实现了可比的性能。</li><li>CogView3 能够生成极高分辨率（如 2048×2048）的高质量图像。工作量：</li><li>CogView3 的训练管道相对简单，采用渐进训练策略以降低训练成本。</li><li>CogView3 的蒸馏变体进一步降低了推理成本，同时保持了可比的性能。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-39c07129df4e18479bf6f2000e3bd45b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3130242f65670e2f9a99c29710ffccef.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a4b8e0b9de2b5980d7c1d4c49daded3b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3e7d124475c2a36f974604208e23b856.jpg" align="middle"></details><h2 id="Face2Diffusion-for-Fast-and-Editable-Face-Personalization"><a href="#Face2Diffusion-for-Fast-and-Editable-Face-Personalization" class="headerlink" title="Face2Diffusion for Fast and Editable Face Personalization"></a>Face2Diffusion for Fast and Editable Face Personalization</h2><p><strong>Authors:Kaede Shiohara, Toshihiko Yamasaki</strong></p><p>Face personalization aims to insert specific faces, taken from images, into pretrained text-to-image diffusion models. However, it is still challenging for previous methods to preserve both the identity similarity and editability due to overfitting to training samples. In this paper, we propose Face2Diffusion (F2D) for high-editability face personalization. The core idea behind F2D is that removing identity-irrelevant information from the training pipeline prevents the overfitting problem and improves editability of encoded faces. F2D consists of the following three novel components: 1) Multi-scale identity encoder provides well-disentangled identity features while keeping the benefits of multi-scale information, which improves the diversity of camera poses. 2) Expression guidance disentangles face expressions from identities and improves the controllability of face expressions. 3) Class-guided denoising regularization encourages models to learn how faces should be denoised, which boosts the text-alignment of backgrounds. Extensive experiments on the FaceForensics++ dataset and diverse prompts demonstrate our method greatly improves the trade-off between the identity- and text-fidelity compared to previous state-of-the-art methods. </p><p><a href="http://arxiv.org/abs/2403.05094v1">PDF</a> CVPR2024. Code: <a href="https://github.com/mapooon/Face2Diffusion">https://github.com/mapooon/Face2Diffusion</a>, Webpage:   <a href="https://mapooon.github.io/Face2DiffusionPage/">https://mapooon.github.io/Face2DiffusionPage/</a></p><p><strong>Summary</strong><br>人脸个性化通过植入从图片获取的人脸来实现预先训练的文转图像扩散模型。</p><p><strong>Key Takeaways</strong></p><ul><li>从训练管道中去除与人脸无关的信息有助于提升编辑能力。</li><li>多尺度人脸编码器提供了清晰分离的人脸特征。</li><li>表情指导将人脸表情与人脸身份进行分离。</li><li>类别引导去噪正则化增强模型对人脸去噪的学习。</li><li>跨数据集实验表明，该方法提升了身份保真度与文本保真度之间的平衡。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：Face2Diffusion：快速且可编辑的人脸个性化</li><li>作者：Kaede Shiohara, Toshihiko Yamasaki</li><li>单位：东京大学</li><li>关键词：Face personalization, Text-to-image diffusion model, Identity preservation, Editability</li><li>论文链接：https://arxiv.org/abs/2403.05094</li><li><p>摘要：（1）研究背景：文本到图像扩散模型在图像生成方面取得了显著进展，但将特定人脸插入预训练模型仍然具有挑战性，既要保持身份相似性，又要保证可编辑性。（2）过去方法：现有方法容易过度拟合训练样本，导致身份相似性和可编辑性之间的权衡。（3）研究方法：Face2Diffusion（F2D）通过从训练管道中去除与身份无关的信息来解决过度拟合问题，提高编码人脸的可编辑性。F2D包含三个新颖的组件：多尺度身份编码器、表情引导器和类别引导去噪正则化。（4）实验结果：在 FaceForensics++ 数据集和各种提示上的广泛实验表明，F2D 在身份和文本保真度之间的权衡方面明显优于之前的最先进方法。</p></li><li><p>方法：(1) 多尺度身份编码器：从人脸图像中提取多尺度特征，保留身份信息，降低过度拟合风险。(2) 表情引导器：指导扩散模型关注人脸表情的编辑，提高可编辑性。(3) 类别引导去噪正则化：引入类别信息，防止模型从无关噪声中学习，提高身份保真度。</p></li></ol><p><strong>8. 结论</strong></p><p><strong>(1): 此项工作的意义</strong></p><p>Face2Diffusion 提出了一种可编辑的人脸个性化方法，通过解决过度拟合问题，提高了生成人脸的可编辑性，在身份保真度和文本保真度之间取得了更好的平衡。</p><p><strong>(2): 本文优缺点总结（三个维度：创新点、性能、工作量）</strong></p><p><strong>创新点：</strong></p><ul><li>多尺度身份编码器：提取多尺度特征，降低过度拟合风险。</li><li>表情引导器：指导扩散模型关注人脸表情的编辑。</li><li>类别引导去噪正则化：防止模型从无关噪声中学习。</li></ul><p><strong>性能：</strong></p><ul><li>在身份保真度和文本保真度之间取得了更好的平衡。</li><li>在各种提示和人脸数据集上表现出优异的性能。</li></ul><p><strong>工作量：</strong></p><ul><li>训练过程相对复杂，需要多尺度特征提取和正则化策略。</li><li>生成单个图像所需的时间与其他文本到图像扩散模型类似。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-a4d3199be75c4ed763ad12e5fd6fd186.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-073fc885846ed7841fbefca59dc75bb8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c2c9194bd5afd5f761cca65c865fe0fb.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b6ba7d02ff97010b563089ea86c62c6b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-8bdf1923916c837b5df8251aa84ce58b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d5d8555605f33ef6be1a8b7ab0be10cc.jpg" align="middle"><img src="https://picx.zhimg.com/v2-302c4a1ee77cbdfd8dba69c7d6a94497.jpg" align="middle"></details>## Spectrum Translation for Refinement of Image Generation (STIG) Based on   Contrastive Learning and Spectral Filter Profile**Authors:Seokjun Lee, Seung-Won Jung, Hyunseok Seo**Currently, image generation and synthesis have remarkably progressed with generative models. Despite photo-realistic results, intrinsic discrepancies are still observed in the frequency domain. The spectral discrepancy appeared not only in generative adversarial networks but in diffusion models. In this study, we propose a framework to effectively mitigate the disparity in frequency domain of the generated images to improve generative performance of both GAN and diffusion models. This is realized by spectrum translation for the refinement of image generation (STIG) based on contrastive learning. We adopt theoretical logic of frequency components in various generative networks. The key idea, here, is to refine the spectrum of the generated image via the concept of image-to-image translation and contrastive learning in terms of digital signal processing. We evaluate our framework across eight fake image datasets and various cutting-edge models to demonstrate the effectiveness of STIG. Our framework outperforms other cutting-edges showing significant decreases in FID and log frequency distance of spectrum. We further emphasize that STIG improves image quality by decreasing the spectral anomaly. Additionally, validation results present that the frequency-based deepfake detector confuses more in the case where fake spectrums are manipulated by STIG. [PDF](http://arxiv.org/abs/2403.05093v1) Accepted to AAAI 2024**Summary**生成对抗网络和扩散模型中频域差异问题，可通过频谱对比学习下的图像生成谱转换框架（STIG）有效解决。**Key Takeaways*** 提出STIG框架减轻生成对抗网络和扩散模型图像频域差异。* STIG基于图像到图像转换和对照学习，优化生成图像频谱。* STIG在八个伪造图像数据集上超越现有方法，显着降低FID和光谱的对数频率距离。* STIG通过减小光谱异常提高图像质量。* 经过STIG处理的伪造图像会迷惑基于频率的深度伪造检测器。* STIG使用频谱转换有效解决生成模型中频域差异问题。* STIG提升图像生成质量，增强对深度伪造检测器的鲁棒性。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：图像生成精炼的光谱转换（STIG）</li><li>作者：Seokjun Lee、Seung-Won Jung、Hyunseok Seo</li><li>第一作者单位：韩国科学技术研究院生物医学研究部</li><li>关键词：图像生成、光谱转换、对比学习、频谱滤波器轮廓</li><li>论文链接：NoneGithub 代码链接：None</li><li>摘要：（1）研究背景：目前，图像生成和合成在生成模型的帮助下取得了显著进展。尽管生成结果逼真，但在频域中仍然存在固有的差异。这种频谱差异不仅出现在生成对抗网络中，还出现在扩散模型中。（2）过去方法及其问题：以往的研究提出了通过修改生成网络架构或目标函数来弥补频域差异的方法，但仍有改进空间。（3）本文提出的研究方法：本文提出了一种基于对比学习的光谱转换框架（STIG），用于有效减轻生成图像频域中的差异，以提高 GAN 和扩散模型的生成性能。该框架采用了数字信号处理中图像到图像转换和对比学习的概念来优化生成图像的光谱。（4）方法在任务和性能上的表现：在八个假图像数据集和各种前沿模型上评估了 STIG 的有效性。结果表明，STIG 优于其他前沿方法，在 FID 和光谱对数频率距离方面有显著下降。此外，STIG 通过减少光谱异常来提高图像质量。验证结果表明，当 STIG 处理虚假光谱时，基于频率的深度伪造检测器更容易混淆。</li></ol><p>7.Methods：（1）STIG框架概述：STIG框架由三个主要组件组成：图像到图像转换网络（I2I）、对比学习损失函数和频谱滤波器轮廓（SFP）。（2）图像到图像转换网络（I2I）：I2I网络采用U-Net架构，用于将生成图像从源频域转换到目标频域。（3）对比学习损失函数：对比学习损失函数基于图像对的相似性和差异性，通过最大化相似图像的特征表示之间的相关性，同时最小化不同图像的特征表示之间的相关性，来优化I2I网络。（4）频谱滤波器轮廓（SFP）：SFP是一个预先训练的频谱滤波器集合，用于指导I2I网络学习目标频域的特征分布。（5）STIG训练过程：STIG框架的训练过程包括两个阶段：预训练阶段和微调阶段。在预训练阶段，I2I网络使用对比学习损失函数和SFP进行训练。在微调阶段，I2I网络使用生成图像和真实图像之间的对抗性损失函数进行微调。</p><ol><li>结论：（1）：本文提出了 STIG 框架，该框架通过直接操作生成图像的频率分量，在频域中减少生成图像的光谱差异，从而提高生成性能。（2）：创新点：STIG 框架在频域中直接操作生成图像，以减少生成图像与真实图像之间的光谱差异。STIG 框架采用对比学习损失函数和频谱滤波器轮廓，优化图像到图像转换网络的训练过程。STIG 框架可以有效地提高生成对抗网络和扩散模型的生成性能。性能：STIG 框架在八个假图像基准上均优于其他前沿方法，在 FID 和光谱对数频率距离方面有显著下降。STIG 框架通过减少光谱异常来提高图像质量。STIG 框架处理虚假光谱时，基于频率的深度伪造检测器更容易混淆。工作量：STIG 框架的训练过程包括预训练阶段和微调阶段。预训练阶段需要对图像到图像转换网络使用对比学习损失函数和频谱滤波器轮廓进行训练。微调阶段需要对图像到图像转换网络使用生成图像和真实图像之间的对抗性损失函数进行微调。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-59b9082b16c536f6e3dc82d3eedb0929.jpg" align="middle"><img src="https://picx.zhimg.com/v2-cc9ad99c3613618bd289ca6d732974f2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ed21a9f11c14097979acb60a01fc0faa.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ee45629a830dd20e3e691c354e6c5761.jpg" align="middle"><img src="https://picx.zhimg.com/v2-82896ffced53d8bc120b544471040628.jpg" align="middle"></details><h2 id="Improving-Diffusion-Based-Generative-Models-via-Approximated-Optimal-Transport"><a href="#Improving-Diffusion-Based-Generative-Models-via-Approximated-Optimal-Transport" class="headerlink" title="Improving Diffusion-Based Generative Models via Approximated Optimal   Transport"></a>Improving Diffusion-Based Generative Models via Approximated Optimal   Transport</h2><p><strong>Authors:Daegyu Kim, Jooyoung Choi, Chaehun Shin, Uiwon Hwang, Sungroh Yoon</strong></p><p>We introduce the Approximated Optimal Transport (AOT) technique, a novel training scheme for diffusion-based generative models. Our approach aims to approximate and integrate optimal transport into the training process, significantly enhancing the ability of diffusion models to estimate the denoiser outputs accurately. This improvement leads to ODE trajectories of diffusion models with lower curvature and reduced truncation errors during sampling. We achieve superior image quality and reduced sampling steps by employing AOT in training. Specifically, we achieve FID scores of 1.88 with just 27 NFEs and 1.73 with 29 NFEs in unconditional and conditional generations, respectively. Furthermore, when applying AOT to train the discriminator for guidance, we establish new state-of-the-art FID scores of 1.68 and 1.58 for unconditional and conditional generations, respectively, each with 29 NFEs. This outcome demonstrates the effectiveness of AOT in enhancing the performance of diffusion models. </p><p><a href="http://arxiv.org/abs/2403.05069v1">PDF</a> </p><p><strong>摘要</strong><br>通过近似最优传输（AOT）技术提升扩散模型生成效果，降低采样误差，提升图像质量。</p><p><strong>要点</strong></p><ul><li>提出近似最优传输（AOT）技术，改进扩散模型训练。</li><li>AOT 技术将最优传输整合到扩散模型训练，提升去噪输出准确性。</li><li>优化后的扩散轨迹曲率降低，采样截断误差减小。</li><li>采用 AOT 训练，图像质量提升，采样步骤减少。</li><li>无条件生成中，27 次诺福克序列（NFE），FID 得分达到 1.88；29 次 NFE，FID 得分达到 1.73。</li><li>条件生成中，29 次 NFE，FID 得分达到 1.68；指导判别器训练，FID 得分达到 1.58。</li><li>AOT 技术有效提升了扩散模型性能。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：通过近似最优传输改进基于扩散的生成模型</li><li>作者：Daegyu Kim、Jooyoung Choi、Chaehun Shin、Uiwon Hwang、Sungroh Yoon</li><li>第一作者单位：首尔国立大学数据科学与人工智能实验室</li><li>关键词：生成模型、扩散模型、最优传输</li><li>论文链接：https://arxiv.org/abs/2403.05069   Github 代码链接：无</li><li>摘要：（1）研究背景：扩散模型是一种通过逐渐去噪来合成图像的生成模型。近年来，扩散模型在图像生成方面取得了显著进展，但仍存在 ODE 轨迹曲率高的问题，这会影响图像质量和采样效率。</li></ol><p>（2）过去方法及问题：FlowMatching 等方法提出了使用最优传输来解决曲率问题，但由于扩散模型的结构，直接应用这些方法存在计算效率低的问题。</p><p>（3）本文提出的研究方法：本文提出了一种近似最优传输（AOT）训练技术，将最优传输近似并整合到扩散模型训练过程中，从而降低 ODE 轨迹的曲率和截断误差。</p><p>（4）方法在任务上的表现及性能：在 CIFAR-10 图像无条件和条件生成任务上，与基线研究和 EDM 相比，本文方法在图像质量和 NFE（函数评估次数）方面均取得了更好的性能。具体而言，在无条件生成中，本文方法以 27 NFE 实现了 1.88 的 FID 得分，以 29 NFE 实现了 1.73 的 FID 得分；在条件生成中，以 29 NFE 实现了 1.68 的 FID 得分，以 29 NFE 实现了 1.58 的 FID 得分。这些结果表明，AOT 技术可以有效提升扩散模型的性能。</p><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：（1）：本研究工作通过提出近似最优传输（AOT）技术，有效降低了扩散模型 ODE 轨迹的曲率和截断误差，从而提升了图像生成质量和采样效率。（2）：创新点：</li><li>提出近似最优传输（AOT）技术，将最优传输近似并整合到扩散模型训练过程中，降低了 ODE 轨迹的曲率和截断误差。</li><li>将 AOT 技术成功集成到 Discriminator Guidance（DG）框架中，展示了其在更广泛应用中的多功能性和潜力。性能：</li><li>在 CIFAR-10 图像无条件和条件生成任务上，与基线研究和 EDM 相比，本文方法在图像质量和 NFE（函数评估次数）方面均取得了更好的性能。</li><li>在无条件生成中，本文方法以 27NFE 实现了 1.88 的 FID 得分，以 29NFE 实现了 1.73 的 FID 得分；在条件生成中，以 29NFE 实现了 1.68 的 FID 得分，以 29NFE 实现了 1.58 的 FID 得分。工作量：</li><li>与 EDM 相比，本文方法在训练成本上略有增加（2% 到 15%）。</li><li>本方法需要算法改进，以扩展其在具有挑战性的条件生成（例如文本指导生成）中的适用性。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-8b3484bb01610ca257b110266a789659.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a1905f26c3dd85ac5906dbc02f95a1c6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-06436ae944972e738965038412bab51a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-092c4ba972936da93fe5ca9a1e0c861e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3004bc0c97615bac6076ff6a3cd11e53.jpg" align="middle"><img src="https://picx.zhimg.com/v2-63b88ad2349cca16dbda28634bc2b6d1.jpg" align="middle"></details><h2 id="XPSR-Cross-modal-Priors-for-Diffusion-based-Image-Super-Resolution"><a href="#XPSR-Cross-modal-Priors-for-Diffusion-based-Image-Super-Resolution" class="headerlink" title="XPSR: Cross-modal Priors for Diffusion-based Image Super-Resolution"></a>XPSR: Cross-modal Priors for Diffusion-based Image Super-Resolution</h2><p><strong>Authors:Yunpeng Qu, Kun Yuan, Kai Zhao, Qizhi Xie, Jinhua Hao, Ming Sun, Chao Zhou</strong></p><p>Diffusion-based methods, endowed with a formidable generative prior, have received increasing attention in Image Super-Resolution (ISR) recently. However, as low-resolution (LR) images often undergo severe degradation, it is challenging for ISR models to perceive the semantic and degradation information, resulting in restoration images with incorrect content or unrealistic artifacts. To address these issues, we propose a \textit{Cross-modal Priors for Super-Resolution (XPSR)} framework. Within XPSR, to acquire precise and comprehensive semantic conditions for the diffusion model, cutting-edge Multimodal Large Language Models (MLLMs) are utilized. To facilitate better fusion of cross-modal priors, a \textit{Semantic-Fusion Attention} is raised. To distill semantic-preserved information instead of undesired degradations, a \textit{Degradation-Free Constraint} is attached between LR and its high-resolution (HR) counterpart. Quantitative and qualitative results show that XPSR is capable of generating high-fidelity and high-realism images across synthetic and real-world datasets. Codes will be released at \url{<a href="https://github.com/qyp2000/XPSR}">https://github.com/qyp2000/XPSR}</a>. </p><p><a href="http://arxiv.org/abs/2403.05049v1">PDF</a> 19 pages, 7 figures</p><p><strong>Summary</strong><br>基于扩散模型，结合多模态大语言模型和语义融合策略，提出一种图像超分辨率框架XPSR，能够生成高保真和逼真的图像。</p><p><strong>Key Takeaways</strong></p><ul><li>扩散模型生成式先验提升图像超分辨率性能。</li><li>多模态大语言模型提供精确语义信息。</li><li>语义融合注意力促进跨模态先验融合。</li><li>无退化约束提取语义内容，而非退化信息。</li><li>XPSR在合成和真实数据集上生成高质量超分辨率图像。</li><li>XPSR代码将于<a href="https://github.com/qyp2000/XPSR发布。">https://github.com/qyp2000/XPSR发布。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：XPSR：用于基于扩散的图像超分辨率的跨模态先验</li><li>作者：曲云鹏、袁坤、赵凯、谢启之、郝金华、孙明、周超</li><li>单位：清华大学</li><li>关键词：图像超分辨率、图像修复、扩散模型、多模态大语言模型</li><li>论文链接：https://arxiv.org/abs/2403.05049Github代码链接：None</li><li><p>摘要：（1）研究背景：基于扩散的图像超分辨率（ISR）方法因其强大的生成先验而受到越来越多的关注。然而，由于低分辨率（LR）图像通常会遭受严重的退化，因此对于ISR模型来说，感知语义和退化信息具有挑战性，导致恢复的图像内容不正确或出现不真实的伪影。（2）以往方法及其问题：本文的动机是解决上述问题。以往方法主要使用生成对抗网络（GAN）进行图像超分辨率，但GAN在生成逼真纹理方面存在困难，并且存在合成训练数据和真实世界测试数据之间的域差距问题。（3）提出的研究方法：为了解决这些问题，本文提出了一个跨模态先验超分辨率（XPSR）框架。在XPSR中，利用先进的多模态大语言模型（MLLM）为扩散模型获取准确和全面的语义条件。为了促进跨模态先验的更好融合，提出了一种语义融合注意力机制。为了提取语义保留的信息而不是不需要的退化，在LR及其高分辨率（HR）对应图像之间附加了一个无退化约束。（4）方法在任务和性能上的表现：定量和定性结果表明，XPSR能够跨合成和真实世界数据集生成高保真和高逼真的图像。这些结果支持了本文提出的方法可以有效解决图像超分辨率中的挑战。</p></li><li><p>方法：(1) 采用大语言模型 LLaVA 获取图像的语义先验，包括高层语义和低层语义；(2) 使用语义融合注意力机制，将语义先验与 T2I 模型生成的先验有效融合；(3) 添加无退化约束，从 LR 图像中提取语义保留但与退化无关的信息。</p></li><li><p>结论：（1）意义：本文提出的 XPSR 框架解决了基于扩散的图像超分辨率模型在准确恢复语义细节方面的难题，为图像超分辨率领域提供了新的思路。（2）优缺点总结：创新点：</p></li><li>提出跨模态先验概念，利用多模态大语言模型为扩散模型提供准确全面的语义条件。</li><li>设计语义融合注意力机制，有效融合语义先验和 T2I 模型生成的先验。</li><li>引入无退化约束，从低分辨率图像中提取语义保留但与退化无关的信息。性能：</li><li>定量和定性结果表明，XPSR 能够跨合成和真实世界数据集生成高保真和高逼真的图像。</li><li>与其他先进方法相比，XPSR 在各种评估指标上取得了有竞争力的性能。工作量：</li><li>XPSR 框架的实现需要一定的工作量，包括训练多模态大语言模型和扩散模型。</li><li>此外，语义融合注意力机制和无退化约束的实现也需要额外的开发工作。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-7216c617badf932e3f8d18daf0977b1f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4c7194197140a421dc8eb74d3c744901.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ca3923ee7424c689775b0bb281aa1184.jpg" align="middle"></details><h2 id="DiffClass-Diffusion-Based-Class-Incremental-Learning"><a href="#DiffClass-Diffusion-Based-Class-Incremental-Learning" class="headerlink" title="DiffClass: Diffusion-Based Class Incremental Learning"></a>DiffClass: Diffusion-Based Class Incremental Learning</h2><p><strong>Authors:Zichong Meng, Jie Zhang, Changdi Yang, Zheng Zhan, Pu Zhao, Yanzhi WAng</strong></p><p>Class Incremental Learning (CIL) is challenging due to catastrophic forgetting. On top of that, Exemplar-free Class Incremental Learning is even more challenging due to forbidden access to previous task data. Recent exemplar-free CIL methods attempt to mitigate catastrophic forgetting by synthesizing previous task data. However, they fail to overcome the catastrophic forgetting due to the inability to deal with the significant domain gap between real and synthetic data. To overcome these issues, we propose a novel exemplar-free CIL method. Our method adopts multi-distribution matching (MDM) diffusion models to unify quality and bridge domain gaps among all domains of training data. Moreover, our approach integrates selective synthetic image augmentation (SSIA) to expand the distribution of the training data, thereby improving the model’s plasticity and reinforcing the performance of our method’s ultimate component, multi-domain adaptation (MDA). With the proposed integrations, our method then reformulates exemplar-free CIL into a multi-domain adaptation problem to implicitly address the domain gap problem to enhance model stability during incremental training. Extensive experiments on benchmark class incremental datasets and settings demonstrate that our method excels previous exemplar-free CIL methods and achieves state-of-the-art performance. </p><p><a href="http://arxiv.org/abs/2403.05016v1">PDF</a> Preprint</p><p><strong>Summary</strong><br>多分布匹配扩散模型在无例可循的类增量学习中解决灾难性遗忘和领域差异问题，通过多域适应隐式解决领域差异问题，提高模型稳定性。</p><p><strong>Key Takeaways</strong></p><ul><li>类增量学习面临灾难性遗忘和无例可循的挑战。</li><li>无例可循的类增量学习方法通过合成先前任务数据来减轻灾难性遗忘。</li><li>此类方法由于无法处理真实数据和合成数据之间的显着领域差异而无法克服灾难性遗忘。</li><li>提出一种新的无例可循的类增量学习方法，采用多分布匹配扩散模型统一质量和弥合所有训练数据域之间的领域差异。</li><li>该方法集成了选择性合成图像增强，以扩展训练数据的分布。</li><li>这种方法将无例可循的类增量学习重新表述为多域适应问题，以隐式解决领域差异问题，提高模型在增量训练过程中的稳定性。</li><li>广泛的实验表明，该方法优于先前的无例可循的类增量学习方法，并实现了最先进的性能。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：基于扩散的类增量学习</li><li>作者：孟子聪，张杰，杨昌迪，詹政，赵普，王延之</li><li>东北大学</li><li>ClassIncrementalLearning，ExemplarFree，DiffusionModel</li><li>论文链接：https://arxiv.org/abs/2403.05016   Github代码链接：无</li><li><p>摘要：（1）研究背景：类增量学习（CIL）因灾难性遗忘而极具挑战性。此外，由于无法访问先前任务的数据，无示例 CIL 更是难上加难。（2）过去方法及问题：最近的无示例 CIL 方法尝试通过合成先前任务数据来缓解灾难性遗忘。然而，它们由于无法处理真实数据和合成数据之间的巨大域差距而无法克服灾难性遗忘。（3）提出的研究方法：为了克服这些问题，本文提出了一种新颖的无示例 CIL 方法。该方法采用多分布匹配 (MDM) 扩散模型来对齐合成数据的质量，并弥合训练数据所有域之间的域差距。此外，本文的方法集成了选择性合成图像增强 (SSIA) 来扩展训练数据的分布，从而提高模型的可塑性并增强多域自适应 (MDA) 技术的性能。通过提出的集成，本文的方法将无示例 CIL 重新表述为多域自适应问题，以隐式解决域差距问题并增强模型在增量训练期间的稳定性。（4）方法性能：在基准 CIL 数据集和设置上的大量实验表明，本文的方法优于之前的无示例 CIL 方法，具有非边际改进，并实现了最先进的性能。</p></li><li><p>方法：(1) 多分布匹配扩散模型精调：使用 LoRA 精调多分布匹配扩散模型，对齐合成数据的质量，缩小训练数据所有域之间的域差距。(2) 选择性合成图像增强：通过选择性合成图像增强扩展训练数据的分布，提高模型的可塑性，增强多域自适应技术的性能。(3) 多域自适应：采用多域自适应训练方法，将无示例 CIL 重新表述为多域自适应问题，隐式解决域差距问题，增强模型在增量训练期间的稳定性。</p></li></ol><p>8.结论：（1）：本文提出了一种基于扩散的新颖无示例类增量学习方法，该方法通过多分布匹配扩散模型和选择性合成图像增强有效解决了灾难性遗忘问题，并通过多域自适应技术增强了模型的稳定性和可塑性，在无示例类增量学习任务上取得了最先进的性能。（2）：创新点：* 基于多分布匹配扩散模型，显式弥合合成数据和真实数据之间的域差距。* 采用选择性合成图像增强，扩展训练数据分布，提高模型的可塑性。* 将无示例类增量学习重新表述为多域自适应问题，隐式解决域差距问题，增强模型在增量训练期间的稳定性。性能：* 在 CIFAR100 和 ImageNet100 基准数据集上，在各种无示例类增量学习设置中均取得了最先进的性能。* 消融研究证明了本文方法中每个组件在无示例类增量学习中的重要性。工作量：* 每个增量任务的训练时间相对较长，尤其是使用 LoRA 微调生成模型的时间。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-3071368b15837785fc8226279a7a69f4.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-022f350905045d5945b926c68a304727.jpg" align="middle"><img src="https://picx.zhimg.com/v2-191fbfc51055a8bc7b2acc064efa3416.jpg" align="middle"><img src="https://picx.zhimg.com/v2-70ca4a001e09124d997a32d6f30da7f0.jpg" align="middle"></details>## StereoDiffusion: Training-Free Stereo Image Generation Using Latent   Diffusion Models**Authors:Lezhong Wang, Jeppe Revall Frisvad, Mark Bo Jensen, Siavash Arjomand Bigdeli**The demand for stereo images increases as manufacturers launch more XR devices. To meet this demand, we introduce StereoDiffusion, a method that, unlike traditional inpainting pipelines, is trainning free, remarkably straightforward to use, and it seamlessly integrates into the original Stable Diffusion model. Our method modifies the latent variable to provide an end-to-end, lightweight capability for fast generation of stereo image pairs, without the need for fine-tuning model weights or any post-processing of images. Using the original input to generate a left image and estimate a disparity map for it, we generate the latent vector for the right image through Stereo Pixel Shift operations, complemented by Symmetric Pixel Shift Masking Denoise and Self-Attention Layers Modification methods to align the right-side image with the left-side image. Moreover, our proposed method maintains a high standard of image quality throughout the stereo generation process, achieving state-of-the-art scores in various quantitative evaluations. [PDF](http://arxiv.org/abs/2403.04965v1) **Summary**立体扩散：无训练、简单易用，无缝集成原有 Stable Diffusion 模型，生成立体图像对。**Key Takeaways**- StereoDiffusion 无需训练，使用方便。- 与原始 Stable Diffusion 模型无缝集成。- 生成立体图像对时无需微调模型权重或图像后处理。- 利用原始输入生成左图像并估计其视差图。- 使用立体像素位移操作生成右图像的潜变量。- 使用对称像素位移掩码去噪和自注意力层修改方法。- 保持立体生成过程中图像质量的高标准。- 在各种定量评估中取得最先进的分数。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：立体扩散：基于潜在扩散模型的无训练立体图像生成</li><li>作者：Lezhong Wang、Jeppe Revall Frisvad、Mark Bo Jensen、Siavash Arjomand Bigdeli</li><li>隶属单位：丹麦技术大学应用数学与计算机科学系</li><li>关键词：XR、深度图像/视频合成、图像编辑、人工智能、修复、Stable Diffusion</li><li>论文链接：https://arxiv.org/abs/2403.04965   Github 代码链接：无</li><li><p>摘要：   (1)：随着制造商推出更多 XR 设备，对立体图像的需求不断增加。为了满足这一需求，我们引入了立体扩散，这是一种与传统修复管道不同、无需训练、使用极其简单且可与原始 Stable Diffusion 模型无缝集成的技术。我们的方法修改了潜在变量，提供了一种端到端的轻量级功能，用于快速生成立体图像对，而无需微调模型权重或对图像进行任何后处理。我们使用原始输入生成左侧图像并估计其视差图，通过立体像素位移操作生成右侧图像的潜在向量，并辅以对称像素位移掩码去噪和自注意力层修改方法，将右侧图像与左侧图像对齐。此外，我们提出的方法在整个立体生成过程中保持了较高的图像质量标准，在各种定量评估中取得了最先进的得分。   (2)：过去的方法主要依赖于图像修复管道，该管道需要额外的模型进行后处理以生成立体图像。这些方法通常需要对模型权重进行微调，并且生成过程复杂且耗时。我们的方法通过修改 Stable Diffusion 模型的潜在变量来直接生成立体图像对，无需额外的模型或后处理。这种方法简单有效，可以快速生成高质量的立体图像。   (3)：我们提出的方法是一种端到端的立体图像生成方法，它修改了 Stable Diffusion 模型的潜在变量。具体来说，我们使用原始输入生成左侧图像并估计其视差图。然后，我们通过立体像素位移操作生成右侧图像的潜在向量。为了对齐右侧图像和左侧图像，我们使用了对称像素位移掩码去噪和自注意力层修改方法。   (4)：我们在立体图像生成任务上评估了我们提出的方法。我们的方法在各种定量评估中取得了最先进的得分，包括 PSNR、SSIM 和 LPIPS。这些结果表明，我们的方法可以生成高质量的立体图像，并且可以很好地支持我们的目标，即快速生成无需训练的立体图像对。</p></li><li><p>方法：（1）使用原始输入生成左侧图像并估计其视差图；（2）通过立体像素位移操作生成右侧图像的潜在向量；（3）使用对称像素位移掩码去噪和自注意力层修改方法对齐右侧图像和左侧图像。</p></li><li><p>结论：（1）：立体扩散：基于潜在扩散模型的无训练立体图像生成，这项工作提出了一种通过修改潜在扩散模型的潜在变量来生成立体图像对的新方法。该方法无需额外的模型或后处理，可以快速生成高质量的立体图像。（2）：创新点：</p></li><li>无需训练：该方法无需对模型权重进行微调，直接生成立体图像对，简化了生成过程。</li><li>端到端：该方法修改潜在变量，提供了一种端到端的轻量级功能，用于快速生成立体图像对。</li><li>与原始StableDiffusion模型无缝集成：该方法可以与原始StableDiffusion模型无缝集成，无需对模型进行任何修改。性能：</li><li>定量评估：该方法在KITTI和Middlebury数据集上取得了最先进的得分，表明其可以生成高质量的立体图像。工作量：</li><li>计算成本：该方法的计算成本较低，可以快速生成立体图像对。</li><li>内存占用：该方法的内存占用较小，可以在各种设备上运行。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-2042e22706397759569cb6c0ac2c19fc.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1a050df593611d8551bcd2b7e676c281.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4970b55916ca916d6716d8304932590e.jpg" align="middle"></details><h2 id="AFreeCA-Annotation-Free-Counting-for-All"><a href="#AFreeCA-Annotation-Free-Counting-for-All" class="headerlink" title="AFreeCA: Annotation-Free Counting for All"></a>AFreeCA: Annotation-Free Counting for All</h2><p><strong>Authors:Adriano D’Alessandro, Ali Mahdavi-Amiri, Ghassan Hamarneh</strong></p><p>Object counting methods typically rely on manually annotated datasets. The cost of creating such datasets has restricted the versatility of these networks to count objects from specific classes (such as humans or penguins), and counting objects from diverse categories remains a challenge. The availability of robust text-to-image latent diffusion models (LDMs) raises the question of whether these models can be utilized to generate counting datasets. However, LDMs struggle to create images with an exact number of objects based solely on text prompts but they can be used to offer a dependable \textit{sorting} signal by adding and removing objects within an image. Leveraging this data, we initially introduce an unsupervised sorting methodology to learn object-related features that are subsequently refined and anchored for counting purposes using counting data generated by LDMs. Further, we present a density classifier-guided method for dividing an image into patches containing objects that can be reliably counted. Consequently, we can generate counting data for any type of object and count them in an unsupervised manner. Our approach outperforms other unsupervised and few-shot alternatives and is not restricted to specific object classes for which counting data is available. Code to be released upon acceptance. </p><p><a href="http://arxiv.org/abs/2403.04943v1">PDF</a> </p><p><strong>Summary</strong><br>使用文本到图像扩散模型 (LDM) 自动生成分类数据，然后通过无监督学习和密度分类指导方法对数据进行处理，从而实现类别无关的无监督对象计数。</p><p><strong>Key Takeaways</strong></p><ul><li>LDMs 能够提供图像添加和删除对象的可靠分类信号。</li><li>利用 LDM 生成的分类数据，可以无监督地学习与对象相关的特征。</li><li>通过计数数据对特征进行精炼和锚定。</li><li>密度分类器引导的方法可将图像划分为包含可被可靠计数的对象的区域。</li><li>该方法可生成任何类型对象的计数数据，并能以无监督的方式进行计数。</li><li>相对于其他无监督和少样本替代方法具有较好的性能。</li><li>无需特定对象类别即可生成计数数据。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：无标注计数：密度分类器引导分区</li><li>作者：Lu Qi, Minghao Chen, Junwei Han, Yu Liu, Xiang Bai, Xiaogang Wang</li><li>单位：无</li><li>关键词：ObjectCounting·SyntheticData·Annotation-Free</li><li>论文链接：https://arxiv.org/abs/2302.06673   Github 链接：无</li><li>摘要：   (1) 研究背景：目标计数方法通常依赖于人工标注数据集，这限制了网络针对特定类别（如人或企鹅）计数目标的通用性，并且对不同类别目标的计数仍然是一个挑战。   (2) 过去方法：无监督、少样本和零样本方法旨在使用包含不同类别的大型人工标注数据集来创建适用于任何类别的通用计数网络。少样本方法依赖于从目标图像中采样的样本例来定义目标类别，而零样本方法使用文本提示。这些方法依赖于广泛的标注数据集，但   (3) 本文方法：利用了文本到图像的潜在扩散模型（LDM）。LDM 难以仅基于文本提示创建具有精确数量目标的图像，但可以通过添加和移除图像中的目标来提供可靠的排序信号。利用这些数据，本文首先引入了一种无监督排序方法来学习目标相关特征，随后使用 LDM 生成的计数数据对这些特征进行精炼和锚定以用于计数目的。此外，本文还提出了一种密度分类器引导方法，将图像划分为包含可被可靠计数的目标的块。因此，本文可以为任何类型的目标生成计数数据并以无监督的方式对其进行计数。   (4) 性能：本文方法优于其他无监督和少样本替代方法，并且不受特定目标类别的限制，这些类别有可用的计数数据。</li></ol><p>7.方法：(1)生成合成排序数据，通过添加和移除图像中的目标，使用潜在扩散模型（LDM）对图像进行排序；(2)预训练排序网络，使用排序损失和关系损失，对图像特征进行排序；(3)从合成数据学习计数，使用预训练的排序网络，通过微调线性层，将特征锚定到实际计数值；(4)人群密度分类，使用 Stable Diffusion 生成合成数据，对人群密度进行分类；(5)密度分类器引导分区（DCGP），根据估计的密度对图像进行分区，将图像处理为更小的补丁。</p><ol><li>结论：（1）：本文提出了一种无监督的目标计数方法，该方法利用了文本到图像的潜在扩散模型（LDM）生成的合成数据。该方法通过排序和锚定学习目标相关特征，并使用密度分类器引导分区（DCGP）将图像划分为包含可被可靠计数的目标的块。该方法不受特定目标类别的限制，并且优于其他无监督和少样本替代方法。（2）：创新点：</li><li>利用LDM生成合成排序数据和计数数据，无需人工标注。</li><li>提出了一种无监督排序方法，学习目标相关特征。</li><li>提出了一种密度分类器引导分区（DCGP）方法，将图像划分为包含可被可靠计数的目标的块。性能：</li><li>在PASCAL VOC、COCO和Cityscapes数据集上，该方法优于其他无监督和少样本替代方法。</li><li>该方法不受特定目标类别的限制，可以为任何类型的目标生成计数数据并以无监督的方式对其进行计数。工作量：</li><li>该方法需要生成合成排序数据和计数数据，这可能需要大量的计算资源。</li><li>该方法需要预训练排序网络和微调线性层，这可能需要大量的时间和精力。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-0bdfaf4b65221e3f6287dfe2ed850459.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7e6e2c7b151a6f679f9aa91c763c21aa.jpg" align="middle"><img src="https://pica.zhimg.com/v2-25087217d0ca2a3290d33e79013e2984.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-04536de3c0849a068b94d559fbfb1068.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9e7e2084d668b0f9c9e859eecaa8550c.jpg" align="middle"></details><h2 id="An-Item-is-Worth-a-Prompt-Versatile-Image-Editing-with-Disentangled-Control"><a href="#An-Item-is-Worth-a-Prompt-Versatile-Image-Editing-with-Disentangled-Control" class="headerlink" title="An Item is Worth a Prompt: Versatile Image Editing with Disentangled   Control"></a>An Item is Worth a Prompt: Versatile Image Editing with Disentangled   Control</h2><p><strong>Authors:Aosong Feng, Weikang Qiu, Jinbin Bai, Kaicheng Zhou, Zhen Dong, Xiao Zhang, Rex Ying, Leandros Tassiulas</strong></p><p>Building on the success of text-to-image diffusion models (DPMs), image editing is an important application to enable human interaction with AI-generated content. Among various editing methods, editing within the prompt space gains more attention due to its capacity and simplicity of controlling semantics. However, since diffusion models are commonly pretrained on descriptive text captions, direct editing of words in text prompts usually leads to completely different generated images, violating the requirements for image editing. On the other hand, existing editing methods usually consider introducing spatial masks to preserve the identity of unedited regions, which are usually ignored by DPMs and therefore lead to inharmonic editing results. Targeting these two challenges, in this work, we propose to disentangle the comprehensive image-prompt interaction into several item-prompt interactions, with each item linked to a special learned prompt. The resulting framework, named D-Edit, is based on pretrained diffusion models with cross-attention layers disentangled and adopts a two-step optimization to build item-prompt associations. Versatile image editing can then be applied to specific items by manipulating the corresponding prompts. We demonstrate state-of-the-art results in four types of editing operations including image-based, text-based, mask-based editing, and item removal, covering most types of editing applications, all within a single unified framework. Notably, D-Edit is the first framework that can (1) achieve item editing through mask editing and (2) combine image and text-based editing. We demonstrate the quality and versatility of the editing results for a diverse collection of images through both qualitative and quantitative evaluations. </p><p><a href="http://arxiv.org/abs/2403.04880v1">PDF</a> </p><p><strong>Summary</strong><br>文本提示编辑实现了图像编辑，但由于扩散模型的预训练方式，直接编辑提示中的文字会导致生成完全不同的图像，违背了图像编辑的要求。</p><p><strong>Key Takeaways</strong></p><ul><li>提出文本提示编辑方法 D-Edit。</li><li>将图像提示交互分解为多个项目提示交互，每个项目链接到一个特殊学习提示。</li><li>采用两步优化构建项目提示关联。</li><li>可进行多种图像编辑，包括基于图像、基于文本、基于掩码的编辑和项目移除。</li><li>可以在单个统一框架中实现所有类型的编辑应用程序。</li><li>D-Edit 是第一个（1）通过掩码编辑实现项目编辑，（2）结合图像和基于文本的编辑的框架。</li><li>通过定性和定量评估，展示了各种图像编辑结果的质量和多功能性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：An Item is Worth a Prompt：多功能的可控图像编辑</li><li>作者：Aosong Feng, Weikang Qiu, Jinbin Bai, Kaicheng Zhou, Zhen Dong, Xiao Zhang, Rex Ying, Leandros Tassiulas</li><li>第一作者单位：耶鲁大学</li><li>关键词：图像编辑、文本到图像扩散模型、可控提示</li><li>论文链接：https://arxiv.org/abs/2403.04880</li><li><p>摘要：(1)：基于文本到图像扩散模型在图像合成中的成功，图像编辑成为一种重要的应用程序，它让人们能够与 AI 生成的内容进行交互。在各种编辑方法中，提示空间编辑因其控制语义的能力和简单性而受到更多关注。然而，由于扩散模型通常在描述性文本标题上进行预训练，因此在文本提示中直接编辑单词通常会导致完全不同的生成图像，违反了图像编辑的要求。另一方面，现有的编辑方法通常考虑引入空间掩码来保留未编辑区域的身份，而扩散模型通常会忽略这些区域，因此导致不协调的编辑结果。(2)：针对这两个挑战，本文提出将综合图像提示交互分解为几个项目提示交互，每个项目都链接到一个特殊学习的提示。由此产生的框架名为 D-Edit，它基于预训练的扩散模型，并采用交叉注意层进行解耦，并采用两步优化来构建项目提示关联。通过操作相应的提示，可以将多功能图像编辑应用于特定项目。本文展示了四种类型的编辑操作（包括基于图像、基于文本、基于掩码的编辑和项目移除）的最新结果，涵盖了大多数类型的编辑应用程序，所有这些都采用一个统一的框架。值得注意的是，D-Edit 是第一个可以 (1) 通过掩码编辑实现项目编辑，以及 (2) 结合图像和基于文本的编辑的框架。通过定性和定量评估，本文展示了针对各种图像集合的编辑结果的质量和多功能性。(3)：本文提出两种关键技术，旨在增强上述标准：(1) 解耦控制：为了保留原始图像的信息，目标项目的编辑应尽量不影响周围项目。从提示到图像的控制过程也应该解耦，确保修改项目提示不会破坏其余项目的控制流。注意到文本到图像交互发生在基于注意力的扩散模型的交叉注意层中，本文提出分组交叉注意来解耦提示到项目的控制流。(2) 唯一项目提示：为了提高与指导的一致性（例如参考图像），每个项目都应该与一个控制其生成的唯一提示相关联。这些提示通常由特殊标记或罕见单词组成。像 Dreambooth 和 Textual Inversion 这样的图像个性化现有工作已经通过用唯一提示表示新主题来广泛研究了这个概念，随后将其用于图像生成。与它们相比，本文使用独立提示来定义不同的项目，而不是整个图像。在理想情况下，如果图像中的每个项目及其所有细节都可以用一个独特的英文单词准确描述，那么用户可以通过简单地将当前单词更改为目标单词来实现所有类型的编辑。(4)：本文充分利用提示唯一性和解耦控制的潜力，介绍了一个多功能图像编辑框架，称为 Disentangled-Edit (D-Edit)，这是一个统一的框架，支持在项目级别进行大多数类型的图像编辑操作，包括基于文本、基于图像、基于掩码的编辑和项目移除。具体来说，如图 1 所示，从目标图像开始，本文最初将其细分为多个可编辑项目（在以下内容中，本文还将背景和未分割区域称为项目），每个项目都与一个包含几个新标记的提示相关联。提示和项目之间的关联是通过两步微调过程建立的，其中包括优化文本编码器嵌入矩阵和 UNet 模型权重。引入分组交叉注意来解耦提示到项目的交互，通过隔离注意计算和值更新。然后，可以通过更改提示、项目及其之间的关联来实现各种类型的图像编辑。然后，用户可以通过更改相应的提示、掩码和项目，并调整它们之间的关联来实现各种类型的图像编辑。这种灵活性允许广泛的创造可能性和对编辑过程的精确控制。本文在四个图像编辑任务上展示了本文框架的多功能性和性能，如上所述，使用稳定扩散和稳定扩散 XL。本文总结本文的贡献如下：• 本文提出建立项目提示关联以实现项目编辑。• 本文引入分组交叉注意来解耦扩散模型中的控制流。• 本文提出 D-Edit 作为一种多功能框架，支持在项目级别进行各种图像编辑操作，包括基于文本、基于图像、基于掩码的编辑和项目移除。D-Edit 是第一个可以进行基于掩码的编辑以及同时执行基于文本和图像的编辑的框架。</p></li><li><p>Methods：（1）：本文提出建立项目提示关联以实现项目编辑；（2）：本文引入分组交叉注意来解耦扩散模型中的控制流；（3）：本文提出 D-Edit 作为一种多功能框架，支持在项目级别进行各种图像编辑操作，包括基于文本、基于图像、基于掩码的编辑和项目移除。D-Edit 是第一个可以进行基于掩码的编辑以及同时执行基于文本和图像的编辑的框架。</p></li><li><p>结论：（1）：本文提出 D-Edit，这是一个基于扩散模型的多功能图像编辑框架。D-Edit 将给定图像分割为多个项目，每个项目都被分配一个提示来控制其在提示空间中的表示。图像提示交叉注意力被分解为一组项目提示交互。每个提示通过孤立的交叉注意力被约束为仅与它控制的项目进行交互，从而解耦了交叉注意力控制管道。（2）：创新点：</p></li><li>提出建立项目提示关联以实现项目编辑。</li><li>引入分组交叉注意力来解耦扩散模型中的控制流。</li><li>提出 D-Edit 作为一种多功能框架，支持在项目级别进行各种图像编辑操作，包括基于文本、基于图像、基于掩码的编辑和项目移除。D-Edit 是第一个可以进行基于掩码的编辑以及同时执行基于文本和图像的编辑的框架。性能：</li><li>在四个图像编辑任务上展示了本文框架的多功能性和性能，如上所述，使用稳定扩散和稳定扩散 XL。工作量：</li><li>提出了一种两步微调过程来建立提示和项目之间的关联，包括优化文本编码器嵌入矩阵和 UNet 模型权重。</li><li>引入分组交叉注意来解耦提示到项目的交互，通过隔离注意计算和值更新。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-197c83cdebd23bdb14b8fb0a7b729711.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c1f65d83dbc51dc28ff510d4cc3b578f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-325295a9d8fc632369762af9b221cc1f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a471c060f1ae7bcb9959f797a6fb643a.jpg" align="middle"></details><h2 id="Pix2Gif-Motion-Guided-Diffusion-for-GIF-Generation"><a href="#Pix2Gif-Motion-Guided-Diffusion-for-GIF-Generation" class="headerlink" title="Pix2Gif: Motion-Guided Diffusion for GIF Generation"></a>Pix2Gif: Motion-Guided Diffusion for GIF Generation</h2><p><strong>Authors:Hitesh Kandala, Jianfeng Gao, Jianwei Yang</strong></p><p>We present Pix2Gif, a motion-guided diffusion model for image-to-GIF (video) generation. We tackle this problem differently by formulating the task as an image translation problem steered by text and motion magnitude prompts, as shown in teaser fig. To ensure that the model adheres to motion guidance, we propose a new motion-guided warping module to spatially transform the features of the source image conditioned on the two types of prompts. Furthermore, we introduce a perceptual loss to ensure the transformed feature map remains within the same space as the target image, ensuring content consistency and coherence. In preparation for the model training, we meticulously curated data by extracting coherent image frames from the TGIF video-caption dataset, which provides rich information about the temporal changes of subjects. After pretraining, we apply our model in a zero-shot manner to a number of video datasets. Extensive qualitative and quantitative experiments demonstrate the effectiveness of our model — it not only captures the semantic prompt from text but also the spatial ones from motion guidance. We train all our models using a single node of 16xV100 GPUs. Code, dataset and models are made public at: <a href="https://hiteshk03.github.io/Pix2Gif/">https://hiteshk03.github.io/Pix2Gif/</a>. </p><p><a href="http://arxiv.org/abs/2403.04634v2">PDF</a> </p><p><strong>Summary</strong><br>图像到GIF生成的新式运动引导扩散模型，采用文本和运动幅度提示指导的图像翻译方法，并提出新的运动引导变形模块以空间转换特征，从而确保模型遵循运动指导。</p><p><strong>Key Takeaways</strong></p><ul><li>提出 Pix2Gif，一种运动引导的扩散模型，用于图像到 GIF（视频）生成。</li><li>以图像翻译问题为基础，由文本和运动幅度提示指导。</li><li>设计新的运动引导变形模块，根据两种提示对源图像特征进行空间转换。</li><li>引入感知损失，确保转换后的特征图与目标图像空间一致。</li><li>精心整理数据，从 TGIF 视频字幕数据集中提取连贯的图像帧。</li><li>采用零样本方式将模型应用于多个视频数据集。</li><li>定性和定量实验验证了模型的有效性，不仅能捕捉文本的语义提示，还能捕捉运动引导的空间提示。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：Pix2Gif：基于运动指导的图像转 GIF（视频）生成</li><li>作者：Hitesh K. Agrawal、Yuke Zhu、Jonathan T. Barron、Phillip Isola、 Alexei A. Efros</li><li>隶属关系：伯克利加州大学</li><li>关键词：图像到视频生成、运动引导、扩散模型、图像编辑</li><li>论文链接：https://arxiv.org/pdf/2302.04208.pdf，Github 代码链接：None</li><li>摘要：（1）研究背景：图像到 GIF（视频）生成是计算机视觉领域中的一个具有挑战性的任务，它需要模型同时理解文本和运动提示，并生成与提示相一致且内容连贯的视频。（2）过去的方法：现有的方法通常使用文本提示来指导图像生成，但它们在处理运动信息时存在局限性。直接将运动输入作为文本提示可能会导致模型对单个提示词给予过多的关注，从而忽略其他重要的运动信息。（3）研究方法：本文提出了一种新的运动引导扩散模型 Pix2Gif，该模型通过引入一个运动引导变形模块来解决上述问题。该模块将运动信息嵌入到图像特征中，指导模型在生成图像时遵循指定的运动轨迹。此外，本文还引入了一个感知损失，以确保变形后的特征图与目标图像保持在同一语义空间内，从而保证内容的一致性和连贯性。（4）方法性能：在 TGIF 视频字幕数据集上进行的广泛定性和定量实验表明，Pix2Gif 模型能够有效地捕捉文本和运动提示中的语义和空间信息，并生成高质量的图像到 GIF（视频）结果。实验结果支持了本文提出的方法的有效性。</li></ol><p>7.Methods：(1): Pix2Gif模型在生成图像时，将运动信息嵌入图像特征中，指导模型遵循指定的运动轨迹。(2): Pix2Gif模型引入了一个感知损失，以确保变形后的特征图与目标图像保持在同一语义空间内，从而保证内容的一致性和连贯性。(3): Pix2Gif模型在TGIF视频字幕数据集上进行的实验表明，该模型能够有效地捕捉文本和运动提示中的语义和空间信息，并生成高质量的图像到GIF（视频）结果。</p><ol><li>结论：（1）：Pix2Gif模型在图像到GIF（视频）生成任务中取得了显著进展，提出了一种创新性的运动引导变形模块，有效地将文本和运动信息结合起来，生成内容连贯、时间一致的高质量结果。（2）：创新点：</li><li>提出了一种新的运动引导变形模块，将运动信息嵌入图像特征中，指导模型遵循指定的运动轨迹，保证了生成的图像序列在时间上的连贯性。</li><li>引入了感知损失，确保变形后的特征图与目标图像保持在同一语义空间内，保证了内容的一致性和连贯性。性能：</li><li>在TGIF视频字幕数据集上进行的实验表明，Pix2Gif模型在捕捉文本和运动提示中的语义和空间信息方面表现出色，生成的图像到GIF（视频）结果质量较高。</li><li>与现有最先进的方法相比，Pix2Gif模型在生成时间一致的GIF方面表现出更好的效果。工作量：</li><li>Pix2Gif模型的训练过程需要大量的计算资源，特别是对于大尺寸图像和长视频序列。</li><li>模型的训练和推理时间也受到图像分辨率和视频长度的影响。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-87f209086271d79f66fc2b71db813a89.jpg" align="middle"><img src="https://pica.zhimg.com/v2-ddec3a8952939ae9c917e7b1984fb9e4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-538b38079b2f1cde247a179f7b6ab9b5.jpg" align="middle"><img src="https://pica.zhimg.com/v2-579c5c472fca8ba1022f880a544c4526.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-03-11  VideoElevator Elevating Video Generation Quality with Versatile   Text-to-Image Diffusion Models</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>Blendshape学习笔记</title>
    <link href="https://kedreamix.github.io/2024/03/11/Note/BlendShape/"/>
    <id>https://kedreamix.github.io/2024/03/11/Note/BlendShape/</id>
    <published>2024-03-11T11:42:00.000Z</published>
    <updated>2024-03-11T12:01:31.162Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Blendshape-Morph-Target动画"><a href="#Blendshape-Morph-Target动画" class="headerlink" title="Blendshape(Morph Target动画)"></a>Blendshape(Morph Target动画)</h2><p>Blendshapes泛指3D定点动画的制作方式 (Maya里面称之为 blend shapes ，而3DS Max里称之为morph targets) ，在3D动画中用的比较多，尤其是人脸动画的制作，通过blendshape来驱动角色的面部表情。</p><p>用在脸部动画制作时，blendshape可以被称之为<strong>脸部特征，表情基准，定位符</strong>等等。这里要引入一个<code>FACS</code>的概念，可以简单理解为将脸部进行合理化的分区标准。</p><blockquote><p>“表情这个东西看起来是一个无限多可能的东西，怎么能够计算expression呢？</p><p>这就带来了Blendshapes——一组组成整体表情的基准（数量可以有十几个、50个、100+、 200+，越多就越细腻)。我们可以使用这一组基准通过线性组合来计算出整体的expression，用公式来说就是  ，其中e是expression，B是一组表情基准，d是对应的系数（在这一组里面的权重），b是neutral。” </p><p>— From <a href="https://zhuanlan.zhihu.com/p/78174706">https://zhuanlan.zhihu.com/p/78174706</a></p></blockquote><h2 id="BlendShape系数介绍"><a href="#BlendShape系数介绍" class="headerlink" title="BlendShape系数介绍"></a>BlendShape系数介绍</h2><p>在ARKit中，对表情特征位置定义了52组运动blendshape系数(<br><a href="https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation">https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation</a> )，每个blendshape系数代表一种表情定位符，表情定位符定义了特定表情属性，如mouthSmileLeft、mouthSmileRight等，与其对应的blendshape系数则表示表情运动范围。这52组blendshape系数极其描述如下表所示。</p><p><img src="https://p3-sign.toutiaoimg.com/pgc-image/984d8d76878441c3a8402f788ef6e46f~noop.image?_iz=58558&amp;from=article.pc_detail&amp;lk3s=953192f4&amp;x-expires=1710668214&amp;x-signature=qI8vU39X63te%2BVNdO78uBFphwK0%3D" alt="Blendshape"></p><p>每一个blendshape系数的取值范围为0～1的浮点数。以jawOpen为例，当认为用户的嘴巴完全闭紧时，返回的jawOpen系数为0。当认为用户的嘴巴张开至最大时，返回的jawOpen系数为1。</p><p><img src="https://p6-sign.toutiaoimg.com/pgc-image/2c8cbd123e00470e95500a8ae62da605~noop.image?_iz=58558&amp;from=article.pc_detail&amp;lk3s=953192f4&amp;x-expires=1710668214&amp;x-signature=UHPhjWP4v96kbtfJzF97Z%2Bp3klc%3D" alt=""></p><p>在用户完全闭嘴与嘴张到最大之间的过渡状态，jawOpen会根据用户嘴张大的幅度返回一个0～1的插值。</p><p><img src="https://p3-sign.toutiaoimg.com/pgc-image/8e8d980b8d69461fb5d2efbc50e47d47~noop.image?_iz=58558&amp;from=article.pc_detail&amp;lk3s=953192f4&amp;x-expires=1710668214&amp;x-signature=sFNMeBoNY3ZFfiO%2BRSjR8uGECIw%3D" alt=""></p><h2 id="脸部动捕的使用"><a href="#脸部动捕的使用" class="headerlink" title="脸部动捕的使用"></a>脸部动捕的使用</h2><h3 id="ARKit-脸部与Vive脸部blendshape基准对比"><a href="#ARKit-脸部与Vive脸部blendshape基准对比" class="headerlink" title="ARKit 脸部与Vive脸部blendshape基准对比"></a>ARKit 脸部与Vive脸部blendshape基准对比</h3><div class="table-container"><table><thead><tr><th></th><th>ARKit（52）</th><th>Extra</th><th>VIVE（52）</th><th>Extra</th></tr></thead><tbody><tr><td>Brow</td><td>5</td><td></td><td>0</td><td></td></tr><tr><td>Eye</td><td>13</td><td></td><td>14</td><td>Eye Frown + 1</td></tr><tr><td>Cheek</td><td>3</td><td></td><td>3</td><td></td></tr><tr><td>Nose</td><td>2</td><td></td><td>0</td><td></td></tr><tr><td>Jaw</td><td>4</td><td></td><td>4</td><td></td></tr><tr><td>Mouth</td><td>24</td><td></td><td>20</td><td>O shape - 1</td></tr><tr><td>Tongue</td><td>1</td><td>Tongue + 7</td><td>11</td><td></td></tr><tr><td>Sum</td><td>52</td><td>59</td><td>52</td><td>52</td></tr></tbody></table></div><h3 id="ARKit的52个Blendshape表情基准组"><a href="#ARKit的52个Blendshape表情基准组" class="headerlink" title="ARKit的52个Blendshape表情基准组"></a>ARKit的52个Blendshape表情基准组</h3><p>可以看ARKit Face Blendshapes的照片和3D模型示例：<a href="https://arkit-face-blendshapes.com/">https://arkit-face-blendshapes.com/</a></p><div class="table-container"><table><thead><tr><th>CC3</th><th>ARKit Name 表情基准/定位符</th><th>ARKit Picture</th><th>CC3 Picture</th></tr></thead><tbody><tr><td>A01</td><td>browInnerUp</td><td><img src="https://static.wixstatic.com/media/64c63b_4cc12dd62ef8484986eebe9739f4eac9~mv2.png/v1/fill/w_252,h_178,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_4cc12dd62ef8484986eebe9739f4eac9~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_fc0c248f4f6f46dda26eda66865678d2~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_fc0c248f4f6f46dda26eda66865678d2~mv2.png" alt=""></td></tr><tr><td>A02</td><td>browDownLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_18a57dc078214abea520f25ad6dfb02a~mv2.png/v1/fill/w_252,h_175,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_18a57dc078214abea520f25ad6dfb02a~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_9d780933931b469991ae0d4ddf105045~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_9d780933931b469991ae0d4ddf105045~mv2.png" alt=""></td></tr><tr><td>A03</td><td>browDownRight</td><td><img src="https://static.wixstatic.com/media/64c63b_105d6dd9d7c44394b96b242e6d9d580b~mv2.png/v1/fill/w_252,h_175,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_105d6dd9d7c44394b96b242e6d9d580b~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_d9943f7163414286809edef7c3bf2de7~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_d9943f7163414286809edef7c3bf2de7~mv2.png" alt=""></td></tr><tr><td>A04</td><td>browOuterUpLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_e7fe8581da2540a3bd7dfc39c874dd61~mv2.png/v1/fill/w_252,h_175,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e7fe8581da2540a3bd7dfc39c874dd61~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_c7234589721d4ddda4e2fcb1a9e0aa97~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_c7234589721d4ddda4e2fcb1a9e0aa97~mv2.png" alt=""></td></tr><tr><td>A05</td><td>browOuterUpRight</td><td><img src="https://static.wixstatic.com/media/64c63b_1fb29f740ff74e8aabadc3769c86501a~mv2.png/v1/fill/w_252,h_175,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_1fb29f740ff74e8aabadc3769c86501a~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_4aefd80dd4c548669d5ba80e4da639eb~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_4aefd80dd4c548669d5ba80e4da639eb~mv2.png" alt=""></td></tr><tr><td>A06</td><td>eyeLookUpLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_697a02a504c84d5f9e316172849bb6d0~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_697a02a504c84d5f9e316172849bb6d0~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_48e3b55ee9ca40f9aec9be8b35c403b0~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_48e3b55ee9ca40f9aec9be8b35c403b0~mv2.png" alt=""></td></tr><tr><td>A07</td><td>eyeLookUpRight</td><td><img src="https://static.wixstatic.com/media/64c63b_84cacf1f990a4e5c874c084a1ea626b3~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_84cacf1f990a4e5c874c084a1ea626b3~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_c879b6cca2ce4f8aa2385864c1fb9389~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_c879b6cca2ce4f8aa2385864c1fb9389~mv2.png" alt=""></td></tr><tr><td>A08</td><td>eyeLookDownLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_d229ef398f3547be93a1a59563520e81~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_d229ef398f3547be93a1a59563520e81~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_d6c6c94e3cee43db8ae9d6f36fc1a689~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_d6c6c94e3cee43db8ae9d6f36fc1a689~mv2.png" alt=""></td></tr><tr><td>A09</td><td>eyeLookDownRight</td><td><img src="https://static.wixstatic.com/media/64c63b_67a1674b6d584344b7ea77843f72be27~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_67a1674b6d584344b7ea77843f72be27~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_ca0dbf25d9f74085809cdcd0742ede35~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_ca0dbf25d9f74085809cdcd0742ede35~mv2.png" alt=""></td></tr><tr><td>A10</td><td>eyeLookOutLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_b4257aa18f754427a593064e71aa97fd~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_b4257aa18f754427a593064e71aa97fd~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_d24fd04ef2d64db18c31b90eccd5f1a9~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_d24fd04ef2d64db18c31b90eccd5f1a9~mv2.png" alt=""></td></tr><tr><td>A11</td><td>eyeLookInLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_03368853adeb4b8599da5451033cd809~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_03368853adeb4b8599da5451033cd809~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_049433ce862e44c4a5f96bcf0ad13bd0~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_049433ce862e44c4a5f96bcf0ad13bd0~mv2.png" alt=""></td></tr><tr><td>A12</td><td>eyeLookInRight</td><td><img src="https://static.wixstatic.com/media/64c63b_6e67745f7867402398390ce18a9f2882~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_6e67745f7867402398390ce18a9f2882~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_803074453832444d8dec710711196559~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_803074453832444d8dec710711196559~mv2.png" alt=""></td></tr><tr><td>A13</td><td>eyeLookOutRight</td><td><img src="https://static.wixstatic.com/media/64c63b_b54a5b6f123244d98eadbded8c29a8c3~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_b54a5b6f123244d98eadbded8c29a8c3~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_2e7b0fed966d453fa0a8dffaabeaf769~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_2e7b0fed966d453fa0a8dffaabeaf769~mv2.png" alt=""></td></tr><tr><td>A14</td><td>eyeBlinkLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_0b68b26a666a49da843b6a47c4579b46~mv2.png/v1/fill/w_252,h_175,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_0b68b26a666a49da843b6a47c4579b46~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_b46d50b28b5d40feba9a496b1ead4a5c~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_b46d50b28b5d40feba9a496b1ead4a5c~mv2.png" alt=""></td></tr><tr><td>A15</td><td>eyeBlinkRight</td><td><img src="https://static.wixstatic.com/media/64c63b_65e50badaa854262a87329394a87484c~mv2.png/v1/fill/w_252,h_175,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_65e50badaa854262a87329394a87484c~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_9113137c91934bdbab3fb26756e84783~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_9113137c91934bdbab3fb26756e84783~mv2.png" alt=""></td></tr><tr><td>A16</td><td>eyeSquintLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_7b9132e314d6404097f212401559e9c4~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_7b9132e314d6404097f212401559e9c4~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_0cccb71728de47e5a7f63fe9bc70bcaf~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_0cccb71728de47e5a7f63fe9bc70bcaf~mv2.png" alt=""></td></tr><tr><td>A17</td><td>eyeSquintRight</td><td><img src="https://static.wixstatic.com/media/64c63b_8cc99b12de914fe882c19229ce2a91da~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_8cc99b12de914fe882c19229ce2a91da~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_8445ac0161fe400ab28591fb6b0b1f56~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_8445ac0161fe400ab28591fb6b0b1f56~mv2.png" alt=""></td></tr><tr><td>A18</td><td>eyeWideLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_0c87ac4e4c5d4d5f9639523c82aa9d43~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_0c87ac4e4c5d4d5f9639523c82aa9d43~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_c097966492c3496cabf1d84455d7144d~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_c097966492c3496cabf1d84455d7144d~mv2.png" alt=""></td></tr><tr><td>A19</td><td>eyeWideRight</td><td><img src="https://static.wixstatic.com/media/64c63b_3157fc370d064da9926027034e8220d6~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_3157fc370d064da9926027034e8220d6~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_24dc2c84e19b436a97fd2db6044f439c~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_24dc2c84e19b436a97fd2db6044f439c~mv2.png" alt=""></td></tr><tr><td>A20</td><td>cheekPuff</td><td><img src="https://static.wixstatic.com/media/64c63b_de4df8062c5f47ca9cd322b75b535705~mv2.png/v1/fill/w_252,h_172,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_de4df8062c5f47ca9cd322b75b535705~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_27548c426f1b47ae834c757417e03269~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_27548c426f1b47ae834c757417e03269~mv2.png" alt=""></td></tr><tr><td>A21</td><td>cheekSquintLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_70520c1a1c374ff3855cb8dfa7450b8b~mv2.png/v1/fill/w_252,h_175,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_70520c1a1c374ff3855cb8dfa7450b8b~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_d769bd2ef0104030818ed7a156ee2a2e~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_d769bd2ef0104030818ed7a156ee2a2e~mv2.png" alt=""></td></tr><tr><td>A22</td><td>cheekSquintRight</td><td><img src="https://static.wixstatic.com/media/64c63b_2f82d4db05764690b33001da1d138f20~mv2.png/v1/fill/w_252,h_175,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_2f82d4db05764690b33001da1d138f20~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_49752693e89a4293982b5e023a0e1c75~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_49752693e89a4293982b5e023a0e1c75~mv2.png" alt=""></td></tr><tr><td>A23</td><td>noseSneerLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_843177402d2545d1a1f0a97e848df91c~mv2.png/v1/fill/w_252,h_178,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_843177402d2545d1a1f0a97e848df91c~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_9855bb8e50f54f638d4bc321dd3caa45~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_9855bb8e50f54f638d4bc321dd3caa45~mv2.png" alt=""></td></tr><tr><td>A24</td><td>noseSneerRight</td><td><img src="https://static.wixstatic.com/media/64c63b_8ee42dc6d8e443a0858e0c65ce56cc74~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_8ee42dc6d8e443a0858e0c65ce56cc74~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_edd25cffbf1249d89bb0f6c5a95b76e5~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_edd25cffbf1249d89bb0f6c5a95b76e5~mv2.png" alt=""></td></tr><tr><td>A25</td><td>jawOpen</td><td><img src="https://static.wixstatic.com/media/64c63b_aca391d5eb744a76b18d6ced31904111~mv2.png/v1/fill/w_267,h_192,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_aca391d5eb744a76b18d6ced31904111~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_f14421d8adb1461ea32ca31bd3cac7be~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_f14421d8adb1461ea32ca31bd3cac7be~mv2.png" alt=""></td></tr><tr><td>A26</td><td>jawForward</td><td><img src="https://static.wixstatic.com/media/64c63b_a199113be0f9418f8c729d9a7e7b4e49~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_a199113be0f9418f8c729d9a7e7b4e49~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_df9963f3452c4ce1bd1b6829a8045112~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_df9963f3452c4ce1bd1b6829a8045112~mv2.png" alt=""></td></tr><tr><td>A27</td><td>jawLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_2642a61cdd0241f9ba24339873003125~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_2642a61cdd0241f9ba24339873003125~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_0de7a40c68654461be74016c2e29cf02~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_0de7a40c68654461be74016c2e29cf02~mv2.png" alt=""></td></tr><tr><td>A28</td><td>jawRight</td><td><img src="https://static.wixstatic.com/media/64c63b_2838229bffe74a5abe7d25d9c6e398ca~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_2838229bffe74a5abe7d25d9c6e398ca~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_2f51f3993bed441b89b9b493a1f2e86b~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_2f51f3993bed441b89b9b493a1f2e86b~mv2.png" alt=""></td></tr><tr><td>A29</td><td>mouthFunnel</td><td><img src="https://static.wixstatic.com/media/64c63b_d2719d8d83524b52a735296f0dfbf092~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_d2719d8d83524b52a735296f0dfbf092~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_e2b4d7681dcf4b1faf34e3c5b57dd3ac~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e2b4d7681dcf4b1faf34e3c5b57dd3ac~mv2.png" alt=""></td></tr><tr><td>A30</td><td>mouthPucker</td><td><img src="https://static.wixstatic.com/media/64c63b_7771a95fb2ae4afeb885b7a684e3f249~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_7771a95fb2ae4afeb885b7a684e3f249~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_e553a24166984303920d5ec9ce1de6d6~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e553a24166984303920d5ec9ce1de6d6~mv2.png" alt=""></td></tr><tr><td>A31</td><td>mouthLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_d2e30cadc9b443f6993ee48d99ffb9c8~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_d2e30cadc9b443f6993ee48d99ffb9c8~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_76b134e6564749fcaf6e036a6dc53517~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_76b134e6564749fcaf6e036a6dc53517~mv2.png" alt=""></td></tr><tr><td>A32</td><td>mouthRight</td><td><img src="https://static.wixstatic.com/media/64c63b_ef34b0cf15c541058052d74870f95a11~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_ef34b0cf15c541058052d74870f95a11~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_332e51118068490cbb932bc8b3880895~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_332e51118068490cbb932bc8b3880895~mv2.png" alt=""></td></tr><tr><td>A33</td><td>mouthRollUpper</td><td><img src="https://static.wixstatic.com/media/64c63b_5c93c56f5d9e4698a86160047452fdae~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_5c93c56f5d9e4698a86160047452fdae~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_f86e02c72dcd4e27b5fafc3e7cbf5098~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_f86e02c72dcd4e27b5fafc3e7cbf5098~mv2.png" alt=""></td></tr><tr><td>A34</td><td>mouthRollLower</td><td><img src="https://static.wixstatic.com/media/64c63b_8d2d50c4784b4f4a881264f9e806b26e~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_8d2d50c4784b4f4a881264f9e806b26e~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_f6ce7f1df803456fb25b77533ec5c1a9~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_f6ce7f1df803456fb25b77533ec5c1a9~mv2.png" alt=""></td></tr><tr><td>A35</td><td>mouthShrugUpper</td><td><img src="https://static.wixstatic.com/media/64c63b_d70a5a8102d14df6be57658f696ab28c~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_d70a5a8102d14df6be57658f696ab28c~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_36feb9bc9305402e8a9e044b7f42c06e~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_36feb9bc9305402e8a9e044b7f42c06e~mv2.png" alt=""></td></tr><tr><td>A36</td><td>mouthShrugLower</td><td><img src="https://static.wixstatic.com/media/64c63b_c39b6573ab8b452c8ba9af4cfd61fa0d~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_c39b6573ab8b452c8ba9af4cfd61fa0d~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_881391abc1ff4fbabd6f7719d93179b8~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_881391abc1ff4fbabd6f7719d93179b8~mv2.png" alt=""></td></tr><tr><td>A37</td><td>mouthClose</td><td><img src="https://static.wixstatic.com/media/64c63b_7c1a9921e54c42c5bbad10ce2d2a2edc~mv2.png/v1/fill/w_267,h_129,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_7c1a9921e54c42c5bbad10ce2d2a2edc~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_8aded518da54400db938b69753b8539a~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_8aded518da54400db938b69753b8539a~mv2.png" alt=""></td></tr><tr><td>A38</td><td>mouthSmileLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_a3cdfd578cec40a5a83931c4d0c9f8ab~mv2.png/v1/fill/w_267,h_186,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_a3cdfd578cec40a5a83931c4d0c9f8ab~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_11aa5137231b4bfe8a8908f25d8d4112~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_11aa5137231b4bfe8a8908f25d8d4112~mv2.png" alt=""></td></tr><tr><td>A39</td><td>mouthSmileRight</td><td><img src="https://static.wixstatic.com/media/64c63b_f0c7a9ddfcb945f496f4ac8aafcfd0ca~mv2.png/v1/fill/w_267,h_186,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_f0c7a9ddfcb945f496f4ac8aafcfd0ca~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_4818df7bf47740f6bab387d0d2926a2b~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_4818df7bf47740f6bab387d0d2926a2b~mv2.png" alt=""></td></tr><tr><td>A40</td><td>mouthFrownLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_8e7c89a5e9514206ac3fd7152e912ef8~mv2.png/v1/fill/w_267,h_187,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_8e7c89a5e9514206ac3fd7152e912ef8~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_ba5a9fdcf6d246439b8d7d9dbf63fb16~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_ba5a9fdcf6d246439b8d7d9dbf63fb16~mv2.png" alt=""></td></tr><tr><td>A41</td><td>mouthFrownRight</td><td><img src="https://static.wixstatic.com/media/64c63b_019c769729a34c7c992c3bbde95adf2a~mv2.png/v1/fill/w_267,h_187,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_019c769729a34c7c992c3bbde95adf2a~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_4505259aa94646278b01cd6b4e6fe32a~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_4505259aa94646278b01cd6b4e6fe32a~mv2.png" alt=""></td></tr><tr><td>A42</td><td>mouthDimpleLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_b7c5c7b4fcea481ba877fab837ddda7c~mv2.png/v1/fill/w_267,h_187,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_b7c5c7b4fcea481ba877fab837ddda7c~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_53d010f6b8b340d6a305149152fe9eb2~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_53d010f6b8b340d6a305149152fe9eb2~mv2.png" alt=""></td></tr><tr><td>A43</td><td>mouthDimpleRight</td><td><img src="https://static.wixstatic.com/media/64c63b_268dda3d9bb14eaba63c5b123ab9002c~mv2.png/v1/fill/w_267,h_187,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_268dda3d9bb14eaba63c5b123ab9002c~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_ea46553169c749f69dc8e47737434193~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_ea46553169c749f69dc8e47737434193~mv2.png" alt=""></td></tr><tr><td>A44</td><td>mouthUpperUpLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_e6f82a77cd374e37b456590eb19c2d28~mv2.png/v1/fill/w_267,h_186,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e6f82a77cd374e37b456590eb19c2d28~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_5c9a52ea901243218e0c9252fcd45a00~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_5c9a52ea901243218e0c9252fcd45a00~mv2.png" alt=""></td></tr><tr><td>A45</td><td>mouthUpperUpRight</td><td><img src="https://static.wixstatic.com/media/64c63b_384bab2c926045f99f4bbef75b6975f0~mv2.png/v1/fill/w_267,h_186,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_384bab2c926045f99f4bbef75b6975f0~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_8abd87f586bb4d2088673a2358a65adb~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_8abd87f586bb4d2088673a2358a65adb~mv2.png" alt=""></td></tr><tr><td>A46</td><td>mouthLowerDownLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_2511f8304fbb49dab88eb09b118f88bc~mv2.png/v1/fill/w_267,h_188,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_2511f8304fbb49dab88eb09b118f88bc~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_21506f6994114f1194bc69958bd3778d~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_21506f6994114f1194bc69958bd3778d~mv2.png" alt=""></td></tr><tr><td>A47</td><td>mouthLowerDownRight</td><td><img src="https://static.wixstatic.com/media/64c63b_5c300e220ef04f388b827c096ad7aae6~mv2.png/v1/fill/w_267,h_188,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_5c300e220ef04f388b827c096ad7aae6~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_9ac53c48df9e4d63b6774b91aaa4db3d~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_9ac53c48df9e4d63b6774b91aaa4db3d~mv2.png" alt=""></td></tr><tr><td>A48</td><td>mouthPressLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_478c881ace1744ff825202484b212c17~mv2.png/v1/fill/w_267,h_187,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_478c881ace1744ff825202484b212c17~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_cca358e42c08454cb9f7f30317c4e93c~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_cca358e42c08454cb9f7f30317c4e93c~mv2.png" alt=""></td></tr><tr><td>A49</td><td>mouthPressRight</td><td><img src="https://static.wixstatic.com/media/64c63b_acad007d32d24b26b4cc192345afc0ba~mv2.png/v1/fill/w_267,h_186,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_acad007d32d24b26b4cc192345afc0ba~mv2.png" alt=""></td><td><br><img src="https://static.wixstatic.com/media/64c63b_35bac2e5acf54d438dd0acf4690c4ea2~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_35bac2e5acf54d438dd0acf4690c4ea2~mv2.png" alt=""></td></tr><tr><td>A50</td><td>mouthStretchLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_18fbf15030164a6383068c8fb7aa7e72~mv2.png/v1/fill/w_263,h_184,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_18fbf15030164a6383068c8fb7aa7e72~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_cf77104a546149e88698feb420726493~mv2.png/v1/fill/w_232,h_209,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_cf77104a546149e88698feb420726493~mv2.png" alt=""></td></tr><tr><td>A51</td><td>mouthStretchRight</td><td><img src="https://static.wixstatic.com/media/64c63b_3f8dd987a3d44b7e98e1e7abb1815111~mv2.png/v1/fill/w_263,h_184,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_3f8dd987a3d44b7e98e1e7abb1815111~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_b2a3abb6ea204ab293571c7c19747003~mv2.png/v1/fill/w_232,h_209,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_b2a3abb6ea204ab293571c7c19747003~mv2.png" alt=""></td></tr><tr><td>A52</td><td>tongueOut</td><td></td><td><img src="https://static.wixstatic.com/media/64c63b_10387f10b0e04d5fac672f8bd17d9459~mv2.png/v1/fill/w_232,h_209,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_10387f10b0e04d5fac672f8bd17d9459~mv2.png" alt=""></td></tr></tbody></table></div><ul><li>CC3 额外的舌头Blendshape(with open month)：</li></ul><div class="table-container"><table><thead><tr><th>T01</th><th>Tongue_Up</th><th></th><th><img src="https://static.wixstatic.com/media/64c63b_75c512342cde45ffbb40fcf5d463732e~mv2.png/v1/fill/w_244,h_220,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_75c512342cde45ffbb40fcf5d463732e~mv2.png" alt=""></th></tr></thead><tbody><tr><td>T02</td><td>Tongue_Down</td><td></td><td><img src="https://static.wixstatic.com/media/64c63b_4bdf00b4d23d4a89ac0bffbb66cc348d~mv2.png/v1/fill/w_244,h_220,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_4bdf00b4d23d4a89ac0bffbb66cc348d~mv2.png" alt=""></td></tr><tr><td>T03</td><td>Tongue_Left</td><td></td><td><img src="https://static.wixstatic.com/media/64c63b_860b7c7043894521a754755c35816cb3~mv2.png/v1/fill/w_244,h_220,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_860b7c7043894521a754755c35816cb3~mv2.png" alt=""></td></tr><tr><td>T04</td><td>Tongue_Right</td><td></td><td><img src="https://static.wixstatic.com/media/64c63b_02c7adc74f934d31ad35c01615b96735~mv2.png/v1/fill/w_244,h_220,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_02c7adc74f934d31ad35c01615b96735~mv2.png" alt=""></td></tr><tr><td>T05</td><td>Tongue_Roll</td><td></td><td><img src="https://static.wixstatic.com/media/64c63b_bfa27b0483c94f7484eeed246642fbc5~mv2.png/v1/fill/w_244,h_220,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_bfa27b0483c94f7484eeed246642fbc5~mv2.png" alt=""></td></tr><tr><td>T06</td><td>Tongue_Tip_Up</td><td></td><td><img src="https://static.wixstatic.com/media/64c63b_6a7d96ce1444409c958adc03652983b7~mv2.png/v1/fill/w_244,h_220,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_6a7d96ce1444409c958adc03652983b7~mv2.png" alt=""></td></tr><tr><td>T07</td><td>Tongue_Tip_Down</td><td></td><td><img src="https://static.wixstatic.com/media/64c63b_37fa335ad1a549b983fb6552db3b5198~mv2.png/v1/fill/w_244,h_220,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_37fa335ad1a549b983fb6552db3b5198~mv2.png" alt=""></td></tr></tbody></table></div><h3 id="Vive面部的表情基准组"><a href="#Vive面部的表情基准组" class="headerlink" title="Vive面部的表情基准组"></a>Vive面部的表情基准组</h3><p>Vive这一套脸部追踪也是52个blendshapes，但是和苹果的基准有很大区别。</p><ul><li>区别一：舌头</li></ul><p>苹果其实是52+7，因为舌头在52个里只有一个伸舌头的blendshape，但vive其实是42 + 10，整体来讲Vive表情记住能tracking到的表情细节还是更少一些。</p><ul><li>区别二：眉毛</li></ul><p>ARKit的52个blendshapes，是根据硬件分区一对一tracking的，然而Vive眉毛不分是没有单独另设blendshapes，而是与眼睛的动作blended在一起作为一个blendshape的，并不是精准的一对一分区tracking。</p><p>我下面编号的排序是按照<a href="https://developer.vive.com/resources/vive-sense/sdk/vive-eye-and-facial-tracking-sdk/">VIVE Eye and Facial Tracking SDK</a> unity 里inspector里的顺序，方便我加表情。</p><p>这里是整理的用ARKit制作Vive基准的对应编号：</p><p><a href="https://docs.google.com/spreadsheets/d/1kWXnqtiVbXRb1FrD5NLlxxuxbYmS0Z6YBLuIE1WwqD4/edit?usp=sharing">https://docs.google.com/spreadsheets/d/1kWXnqtiVbXRb1FrD5NLlxxuxbYmS0Z6YBLuIE1WwqD4/edit?usp=sharing</a></p><ul><li>Eye Blendshapes （14 = 12 + 2）</li></ul><div class="table-container"><table><thead><tr><th>Vive编号</th><th>Vive表情基准</th><th>Vive Picture</th><th>Create by CC3 blendshapes</th></tr></thead><tbody><tr><td>V01</td><td>Eye_Left_Blink</td><td><img src="https://static.wixstatic.com/media/64c63b_735cb0ae227e42bca98f9c51fbd0df6b~mv2.png/v1/fill/w_238,h_182,al_c,lg_1,q_85,enc_auto/64c63b_735cb0ae227e42bca98f9c51fbd0df6b~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_e5f3c0dfc63a42618e182a9b1a0c1e9c~mv2.png/v1/fill/w_211,h_188,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e5f3c0dfc63a42618e182a9b1a0c1e9c~mv2.png" alt=""></td></tr><tr><td>V02</td><td>Eye_Left_Wide</td><td><img src="https://static.wixstatic.com/media/64c63b_1d9223fb44574a94988a7c9ce4d89b39~mv2.png/v1/fill/w_222,h_160,al_c,lg_1,q_85,enc_auto/64c63b_1d9223fb44574a94988a7c9ce4d89b39~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_5dd9ed05165f4fe9b486b4f0604dacc1~mv2.png/v1/fill/w_211,h_188,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_5dd9ed05165f4fe9b486b4f0604dacc1~mv2.png" alt=""></td></tr><tr><td>V03</td><td>Eye_Left_Right</td><td><img src="https://static.wixstatic.com/media/64c63b_638f42e4960743c4b235ab18b5ac6eba~mv2.png/v1/fill/w_238,h_188,al_c,lg_1,q_85,enc_auto/64c63b_638f42e4960743c4b235ab18b5ac6eba~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_c5fcda96b7184941b2a89b2193470f2b~mv2.png/v1/fill/w_211,h_188,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_c5fcda96b7184941b2a89b2193470f2b~mv2.png" alt=""></td></tr><tr><td>V04</td><td>Eye_Left_Left</td><td><img src="https://static.wixstatic.com/media/64c63b_a1041585f54748728dd71aec7de129f5~mv2.png/v1/fill/w_238,h_192,al_c,lg_1,q_85,enc_auto/64c63b_a1041585f54748728dd71aec7de129f5~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_a26c36f7dc2940fe9513e263f7a99c4e~mv2.png/v1/fill/w_211,h_188,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_a26c36f7dc2940fe9513e263f7a99c4e~mv2.png" alt=""></td></tr><tr><td>V05</td><td>Eye_Left_Up</td><td><img src="https://static.wixstatic.com/media/64c63b_d2c84d05015c4d8289f6c7d6d5ba0dcc~mv2.png/v1/fill/w_238,h_203,al_c,lg_1,q_85,enc_auto/64c63b_d2c84d05015c4d8289f6c7d6d5ba0dcc~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_e3e67e16e4b84697a39c3333bad24712~mv2.png/v1/fill/w_211,h_188,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e3e67e16e4b84697a39c3333bad24712~mv2.png" alt=""></td></tr><tr><td>V06</td><td>Eye_Left_Down</td><td><img src="https://static.wixstatic.com/media/64c63b_5cd43316383549e282e7f3f743df9053~mv2.png/v1/fill/w_238,h_195,al_c,lg_1,q_85,enc_auto/64c63b_5cd43316383549e282e7f3f743df9053~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_993bd278fb024580a834a71c6886cc4b~mv2.png/v1/fill/w_211,h_188,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_993bd278fb024580a834a71c6886cc4b~mv2.png" alt=""></td></tr><tr><td>V07</td><td>Eye_Right_Blink</td><td><img src="https://static.wixstatic.com/media/64c63b_2e198e55d97a491cbc66059e6f6adddc~mv2.png/v1/fill/w_235,h_195,al_c,lg_1,q_85,enc_auto/64c63b_2e198e55d97a491cbc66059e6f6adddc~mv2.png" alt=""></td><td></td></tr><tr><td>V08</td><td>Eye_Right_Wide</td><td><img src="https://static.wixstatic.com/media/64c63b_6eebbecf575d4bec905be4dbec06322c~mv2.png/v1/fill/w_223,h_160,al_c,lg_1,q_85,enc_auto/64c63b_6eebbecf575d4bec905be4dbec06322c~mv2.png" alt=""></td><td></td></tr><tr><td>V09</td><td>Eye_Right_Right</td><td><img src="https://static.wixstatic.com/media/64c63b_fc88e475e8c4435a98364224f54ade1a~mv2.png/v1/fill/w_234,h_197,al_c,lg_1,q_85,enc_auto/64c63b_fc88e475e8c4435a98364224f54ade1a~mv2.png" alt=""></td><td></td></tr><tr><td>V10</td><td>Eye_Right_Left</td><td><img src="https://static.wixstatic.com/media/64c63b_216b900672314c37a3e91501b7fe7cc1~mv2.png/v1/fill/w_238,h_190,al_c,lg_1,q_85,enc_auto/64c63b_216b900672314c37a3e91501b7fe7cc1~mv2.png" alt=""></td><td></td></tr><tr><td>V11</td><td>Eye_Right_Up</td><td><img src="https://static.wixstatic.com/media/64c63b_062847a5c45d4dabbe78d255779013dd~mv2.png/v1/fill/w_231,h_196,al_c,lg_1,q_85,enc_auto/64c63b_062847a5c45d4dabbe78d255779013dd~mv2.png" alt=""></td><td></td></tr><tr><td>V12</td><td>Eye_Right_Down</td><td><img src="https://static.wixstatic.com/media/64c63b_b6fae35fba8543d0becc535532111d23~mv2.png/v1/fill/w_238,h_176,al_c,lg_1,q_85,enc_auto/64c63b_b6fae35fba8543d0becc535532111d23~mv2.png" alt=""></td><td></td></tr><tr><td>V13</td><td>Eye_Left_squeeze: The blendShape close eye tightly when Eye_Left_Blink  value is 100.</td><td><img src="https://static.wixstatic.com/media/64c63b_c7b07f1b4ec6495685d85808d23c04e8~mv2.png/v1/fill/w_238,h_183,al_c,lg_1,q_85,enc_auto/64c63b_c7b07f1b4ec6495685d85808d23c04e8~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_a629d31ace624dd8b2ded5123123156e~mv2.png/v1/fill/w_211,h_188,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_a629d31ace624dd8b2ded5123123156e~mv2.png" alt=""></td></tr><tr><td>V14</td><td>Eye_Right_squeeze</td><td><img src="https://static.wixstatic.com/media/64c63b_b82c535f90874742b3c0b6ff62136fe2~mv2.png/v1/fill/w_238,h_194,al_c,lg_1,q_85,enc_auto/64c63b_b82c535f90874742b3c0b6ff62136fe2~mv2.png" alt=""></td></tr></tbody></table></div><ul><li>Lip Blendshapes （38 = 37 + 1）</li></ul><div class="table-container"><table><thead><tr><th>Vive编号</th><th>Vive表情基准</th><th>Vive Picture</th><th>Create by CC3 blendshapes</th></tr></thead><tbody><tr><td>V15</td><td>Jaw_Right</td><td><img src="https://static.wixstatic.com/media/64c63b_75f8b68a96104ef8bf36e393c7ecd48b~mv2.png/v1/fill/w_235,h_190,al_c,lg_1,q_85,enc_auto/64c63b_75f8b68a96104ef8bf36e393c7ecd48b~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_65471d28b0d743c0bb6232ffaee0f6b6~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_65471d28b0d743c0bb6232ffaee0f6b6~mv2.png" alt=""></td></tr><tr><td>V16</td><td>Jaw_Left</td><td><img src="https://static.wixstatic.com/media/64c63b_9cacde29288c4523a2192835a736ad6b~mv2.png/v1/fill/w_245,h_202,al_c,lg_1,q_85,enc_auto/64c63b_9cacde29288c4523a2192835a736ad6b~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_017d7142fd494aef9ad7bbe53fa1d6eb~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_017d7142fd494aef9ad7bbe53fa1d6eb~mv2.png" alt=""></td></tr><tr><td>V17</td><td>Jaw_Forward</td><td><img src="https://static.wixstatic.com/media/64c63b_3876f3dd1a1b4eed92e8405b42700190~mv2.png/v1/fill/w_248,h_197,al_c,lg_1,q_85,enc_auto/64c63b_3876f3dd1a1b4eed92e8405b42700190~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_20353f83579541428557c32d92545c9e~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_20353f83579541428557c32d92545c9e~mv2.png" alt=""></td></tr><tr><td>V18</td><td>Jaw_Open</td><td><img src="https://static.wixstatic.com/media/64c63b_dc79f10003534839948d3261183d5082~mv2.png/v1/fill/w_244,h_188,al_c,lg_1,q_85,enc_auto/64c63b_dc79f10003534839948d3261183d5082~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_24470b9cc9964a11906c42b1d1a6e5e9~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_24470b9cc9964a11906c42b1d1a6e5e9~mv2.png" alt=""></td></tr><tr><td>V19</td><td>Mouth_Ape_Shape</td><td><img src="https://static.wixstatic.com/media/64c63b_7a0f9461a760449db12b2159009ccc93~mv2.png/v1/fill/w_249,h_196,al_c,lg_1,q_85,enc_auto/64c63b_7a0f9461a760449db12b2159009ccc93~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_3d7911f5bfa645adb7f3c36fbeafa2b9~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_3d7911f5bfa645adb7f3c36fbeafa2b9~mv2.png" alt=""></td></tr><tr><td>V20</td><td>Mouth_Upper_Right</td><td><img src="https://static.wixstatic.com/media/64c63b_01037e0042754059b7ada72a8adf2e8a~mv2.png/v1/fill/w_227,h_161,al_c,lg_1,q_85,enc_auto/64c63b_01037e0042754059b7ada72a8adf2e8a~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_185ec305ba464016a15c2420fb04916e~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_185ec305ba464016a15c2420fb04916e~mv2.png" alt=""></td></tr><tr><td>V21</td><td>Mouth_Upper_Left</td><td><img src="https://static.wixstatic.com/media/64c63b_f0c61e8f3f3c42d7ad6d83703f1a61d9~mv2.png/v1/fill/w_265,h_182,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_f0c61e8f3f3c42d7ad6d83703f1a61d9~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_abbecb4860a44fe9800585825beb4b17~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_abbecb4860a44fe9800585825beb4b17~mv2.png" alt=""></td></tr><tr><td>V22</td><td>Mouth_Lower_Right</td><td><img src="https://static.wixstatic.com/media/64c63b_3ec74984b19d44d389a68bcc1ac1a7fb~mv2.png/v1/fill/w_265,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_3ec74984b19d44d389a68bcc1ac1a7fb~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_80a4cdffa3fb493e9c153b517d9aebda~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_80a4cdffa3fb493e9c153b517d9aebda~mv2.png" alt=""></td></tr><tr><td>V23</td><td>Mouth_Lower_Left</td><td><img src="https://static.wixstatic.com/media/64c63b_e80cee8738ea42528c8f351303f5e2c8~mv2.png/v1/fill/w_265,h_225,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e80cee8738ea42528c8f351303f5e2c8~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_edf8e441ff994c27bde811a21754d5e5~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_edf8e441ff994c27bde811a21754d5e5~mv2.png" alt=""></td></tr><tr><td>V24</td><td>*Mouth_Upper_Overturn</td><td><img src="https://static.wixstatic.com/media/64c63b_5f77c14164ae48cf9c0cf4c762b97837~mv2.png/v1/fill/w_265,h_202,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_5f77c14164ae48cf9c0cf4c762b97837~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_b8ae358e723f42e199338722f186e238~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_b8ae358e723f42e199338722f186e238~mv2.png" alt=""></td></tr><tr><td>V25</td><td>*Mouth_Lower_Overturn</td><td><img src="https://static.wixstatic.com/media/64c63b_16a26f9ced50420b99a4c32fc296c112~mv2.png/v1/fill/w_265,h_210,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_16a26f9ced50420b99a4c32fc296c112~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_282323813dfa4ea1aa76551b112e3919~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_282323813dfa4ea1aa76551b112e3919~mv2.png" alt=""></td></tr><tr><td>V26</td><td>Mouth_Pout</td><td><img src="https://static.wixstatic.com/media/64c63b_e7da853fe63242a9bedbd7fe3bddadc7~mv2.png/v1/fill/w_265,h_205,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e7da853fe63242a9bedbd7fe3bddadc7~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_36615908f74d4663a6bc438c3287938c~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_36615908f74d4663a6bc438c3287938c~mv2.png" alt=""></td></tr><tr><td>V27</td><td>Mouth_Smile_Right</td><td><img src="https://static.wixstatic.com/media/64c63b_1de129dc23784dd0af8d5bbccff75741~mv2.png/v1/fill/w_265,h_205,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_1de129dc23784dd0af8d5bbccff75741~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_47a00d3e749e47fa8b3489da81252654~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_47a00d3e749e47fa8b3489da81252654~mv2.png" alt=""></td></tr><tr><td>V28</td><td>Mouth_Smile_Left</td><td><img src="https://static.wixstatic.com/media/64c63b_e66c5606123d4272bc4d3206a101e884~mv2.png/v1/fill/w_265,h_213,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e66c5606123d4272bc4d3206a101e884~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_0a26740959644351bb01f9e9d40ef35e~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_0a26740959644351bb01f9e9d40ef35e~mv2.png" alt=""></td></tr><tr><td>V29</td><td>Mouth_Sad_Right</td><td><img src="https://static.wixstatic.com/media/64c63b_ba5427a221c2446e9d3b9e30d94d80b9~mv2.png/v1/fill/w_265,h_224,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_ba5427a221c2446e9d3b9e30d94d80b9~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_c1e99be3d6c34038852ce55b72102f5c~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_c1e99be3d6c34038852ce55b72102f5c~mv2.png" alt=""></td></tr><tr><td>V30</td><td>Mouth_Sad_Left</td><td><img src="https://static.wixstatic.com/media/64c63b_00772c50ca334cbf95dd1bf53be4c6b8~mv2.png/v1/fill/w_265,h_218,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_00772c50ca334cbf95dd1bf53be4c6b8~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_889e2637303f4d7195afd699a3d92b86~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_889e2637303f4d7195afd699a3d92b86~mv2.png" alt=""></td></tr><tr><td>V31</td><td>Cheek_Puff_Right</td><td><img src="https://static.wixstatic.com/media/64c63b_765350d6685547d4b03b7ae31e7346e0~mv2.png/v1/fill/w_265,h_224,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_765350d6685547d4b03b7ae31e7346e0~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_66386b8d632b4556a00f886613f26d92~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_66386b8d632b4556a00f886613f26d92~mv2.png" alt=""></td></tr><tr><td>V32</td><td>Cheek_Puff_Left</td><td><img src="https://static.wixstatic.com/media/64c63b_2998211eb141496d8651b786337b7846~mv2.png/v1/fill/w_265,h_213,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_2998211eb141496d8651b786337b7846~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_833f433253fb4bcc8e380d79120b3003~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_833f433253fb4bcc8e380d79120b3003~mv2.png" alt=""></td></tr><tr><td>V33</td><td>Cheek_Suck</td><td><img src="https://static.wixstatic.com/media/64c63b_6eea541e05494d26a06cbbe5377cdc0a~mv2.png/v1/fill/w_265,h_209,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_6eea541e05494d26a06cbbe5377cdc0a~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_8573de7fc0d84245a0fa4412ecd3e842~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_8573de7fc0d84245a0fa4412ecd3e842~mv2.png" alt=""></td></tr><tr><td>V34</td><td>Mouth_Upper_UpRight</td><td><img src="https://static.wixstatic.com/media/64c63b_b6ca87cbb7774b2ab4f0ec3748ec9c51~mv2.png/v1/fill/w_265,h_216,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_b6ca87cbb7774b2ab4f0ec3748ec9c51~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_c48b985609fe4129ad1dac8a41905a7e~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_c48b985609fe4129ad1dac8a41905a7e~mv2.png" alt=""></td></tr><tr><td>V35</td><td>Mouth<em>Upper</em> UpLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_58bdc8db0ac3451388534ff3bfb0fa83~mv2.png/v1/fill/w_265,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_58bdc8db0ac3451388534ff3bfb0fa83~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_5ff3f05aeecd48fe9f3077f5c9c96569~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_5ff3f05aeecd48fe9f3077f5c9c96569~mv2.png" alt=""></td></tr><tr><td>V36</td><td>Mouth_Lower_DownRight</td><td><img src="https://static.wixstatic.com/media/64c63b_12f26efe2f28425cb366eea55e83470c~mv2.png/v1/fill/w_265,h_223,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_12f26efe2f28425cb366eea55e83470c~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_30ae27fa9ee94aa8a1b29bbd5fe7b0b2~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_30ae27fa9ee94aa8a1b29bbd5fe7b0b2~mv2.png" alt=""></td></tr><tr><td>V37</td><td>Mouth_Lower_DownLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_4ed1a27945324b53aad0aa3cf453a275~mv2.png/v1/fill/w_265,h_218,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_4ed1a27945324b53aad0aa3cf453a275~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_76893f9855fa4a5a9415cd8abfae6f6f~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_76893f9855fa4a5a9415cd8abfae6f6f~mv2.png" alt=""></td></tr><tr><td>V38</td><td>Mouth_Upper_Inside</td><td><img src="https://static.wixstatic.com/media/64c63b_f5e050f0d9954760879ccd18185c2fc8~mv2.png/v1/fill/w_265,h_209,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_f5e050f0d9954760879ccd18185c2fc8~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_5217c686c7c6455aaca6ba1b2ce64217~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_5217c686c7c6455aaca6ba1b2ce64217~mv2.png" alt=""></td></tr><tr><td>V39</td><td>Mouth_Lower_Inside</td><td><img src="https://static.wixstatic.com/media/64c63b_0031f9adda4441cbb6361e280e594c7b~mv2.png/v1/fill/w_269,h_211,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_0031f9adda4441cbb6361e280e594c7b~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_5108178f718e492eb840f4d678eb3e4e~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_5108178f718e492eb840f4d678eb3e4e~mv2.png" alt=""></td></tr><tr><td>V40</td><td>Mouth_Lower_Overlay</td><td><img src="https://static.wixstatic.com/media/64c63b_26e5bd6286474b4ea3f4fff3933b91f1~mv2.png/v1/fill/w_269,h_222,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_26e5bd6286474b4ea3f4fff3933b91f1~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_1ab252df4c5146e1815e23a83edb2cd2~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_1ab252df4c5146e1815e23a83edb2cd2~mv2.png" alt=""></td></tr><tr><td>V41</td><td>Tongue_LongStep1</td><td><img src="https://static.wixstatic.com/media/64c63b_6791ccfceffe4c2ca07f277b91037521~mv2.png/v1/fill/w_269,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_6791ccfceffe4c2ca07f277b91037521~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_4ef4ab94e1ef47dcb01facf5d168f1d1~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_4ef4ab94e1ef47dcb01facf5d168f1d1~mv2.png" alt=""></td></tr><tr><td>V42</td><td>Tongue_LongStep2</td><td><img src="https://static.wixstatic.com/media/64c63b_e79ccc0096a54ce2b48d188cf6907d0c~mv2.png/v1/fill/w_269,h_181,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e79ccc0096a54ce2b48d188cf6907d0c~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_6e560524670843848266701061f24c63~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_6e560524670843848266701061f24c63~mv2.png" alt=""></td></tr><tr><td>V43</td><td>*Tongue_Down</td><td><img src="https://static.wixstatic.com/media/64c63b_ac97c6cf9fd940e9b38cdf22ab3c9261~mv2.png/v1/fill/w_269,h_199,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_ac97c6cf9fd940e9b38cdf22ab3c9261~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_a2a8aab70eea4b6ba9378cf249aef3a0~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_a2a8aab70eea4b6ba9378cf249aef3a0~mv2.png" alt=""></td></tr><tr><td>V44</td><td>*Tongue_Up</td><td><img src="https://static.wixstatic.com/media/64c63b_0c11f06c560842718309c52bc159ffa8~mv2.png/v1/fill/w_269,h_197,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_0c11f06c560842718309c52bc159ffa8~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_0abc5fe7b2da4c988591e18fc6e060ac~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_0abc5fe7b2da4c988591e18fc6e060ac~mv2.png" alt=""></td></tr><tr><td>V45</td><td>*Tongue_Right</td><td><img src="https://static.wixstatic.com/media/64c63b_0d6e1976f6c342a395f9631be529c694~mv2.png/v1/fill/w_269,h_202,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_0d6e1976f6c342a395f9631be529c694~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_e17db94c36594f32b60ce6057a17aafc~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e17db94c36594f32b60ce6057a17aafc~mv2.png" alt=""></td></tr><tr><td>V46</td><td>*Tongue_Left</td><td><img src="https://static.wixstatic.com/media/64c63b_2633b9481a6f4e94ad4bfe6a6d52e122~mv2.png/v1/fill/w_269,h_201,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_2633b9481a6f4e94ad4bfe6a6d52e122~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_ad949214aeda4b3488c238af7aabdba6~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_ad949214aeda4b3488c238af7aabdba6~mv2.png" alt=""></td></tr><tr><td>V47</td><td>*Tongue_Roll</td><td><img src="https://static.wixstatic.com/media/64c63b_4d41918f9af84d0aaaa2de1e354a5706~mv2.png/v1/fill/w_269,h_216,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_4d41918f9af84d0aaaa2de1e354a5706~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_2c544b45c0ea48079bb570912b85b2a3~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_2c544b45c0ea48079bb570912b85b2a3~mv2.png" alt=""></td></tr><tr><td>V48</td><td>*Tongue_UpLeft_Morph</td><td><img src="https://static.wixstatic.com/media/64c63b_49d65415de0f47d5a07aa77cfebd54e6~mv2.png/v1/fill/w_269,h_201,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_49d65415de0f47d5a07aa77cfebd54e6~mv2.png" alt=""> <img src="https://static.wixstatic.com/media/64c63b_9df53a0e560f41f2a441f9e799b56d1c~mv2.png/v1/fill/w_269,h_221,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_9df53a0e560f41f2a441f9e799b56d1c~mv2.png" alt=""></td><td></td></tr><tr><td>V49</td><td>*Tongue_UpRight_Morph</td><td><img src="https://static.wixstatic.com/media/64c63b_118b5229d2274b5f95a163ebc0d0cfad~mv2.png/v1/fill/w_269,h_232,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_118b5229d2274b5f95a163ebc0d0cfad~mv2.png" alt=""> <img src="https://static.wixstatic.com/media/64c63b_aa40430e533c40c69f0addb2df019a29~mv2.png/v1/fill/w_269,h_215,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_aa40430e533c40c69f0addb2df019a29~mv2.png" alt=""></td><td></td></tr><tr><td>V50</td><td>*Tongue_DownLeft_Morph</td><td><img src="https://static.wixstatic.com/media/64c63b_92fc463c5efc4436a23870d596023ba9~mv2.png/v1/fill/w_269,h_237,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_92fc463c5efc4436a23870d596023ba9~mv2.png" alt=""> <img src="https://static.wixstatic.com/media/64c63b_b3a74eba5433479b96ff645e85681480~mv2.png/v1/fill/w_269,h_231,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_b3a74eba5433479b96ff645e85681480~mv2.png" alt=""></td><td></td></tr><tr><td>V51</td><td>*Tongue_DownRight_Morph</td><td><img src="https://static.wixstatic.com/media/64c63b_db25fb8a736e4b1f9f4e11ba7436e0b0~mv2.png/v1/fill/w_269,h_224,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_db25fb8a736e4b1f9f4e11ba7436e0b0~mv2.png" alt=""> <img src="https://static.wixstatic.com/media/64c63b_b707961c295f40a7995644e93e438ffc~mv2.png/v1/fill/w_269,h_212,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_b707961c295f40a7995644e93e438ffc~mv2.png" alt=""></td><td></td></tr><tr><td>V52</td><td>*O-shaped mouth</td><td><img src="https://static.wixstatic.com/media/64c63b_97cacb52babe4a109cd02874efcb2eda~mv2.png/v1/fill/w_269,h_175,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_97cacb52babe4a109cd02874efcb2eda~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_6e98ff0232d349b8a6f7d8348992ab37~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_6e98ff0232d349b8a6f7d8348992ab37~mv2.png" alt=""> <img src="https://static.wixstatic.com/media/64c63b_b35e7f3707fc4b0ca75f80c4d77867a4~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_b35e7f3707fc4b0ca75f80c4d77867a4~mv2.png" alt=""></td></tr></tbody></table></div><h2 id="MediaPipe提取BlendShape"><a href="#MediaPipe提取BlendShape" class="headerlink" title="MediaPipe提取BlendShape"></a>MediaPipe提取BlendShape</h2><p>MediaPipe Face Landmarker解决方案最初于5月的Google I/O 2023发布。它可以检测面部landmark并输出blendshape score，以渲染与用户匹配的3D面部模型。通过MediaPipe Face Landmarker解决方案，KDDI和谷歌成功地为虚拟主播带来了真实感。</p><p><strong>技术实现</strong></p><p>使用Mediapipe强大而高效的Python包，KDDI开发人员能够检测表演者的面部特征并实时提取52个混合形状。</p><p>还可参考：<a href="https://colab.research.google.com/github/googlesamples/mediapipe/blob/main/examples/face_landmarker/python/%5BMediaPipe_Python_Tasks%5D_Face_Landmarker.ipynb">https://colab.research.google.com/github/googlesamples/mediapipe/blob/main/examples/face_landmarker/python/%5BMediaPipe_Python_Tasks%5D_Face_Landmarker.ipynb</a></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> mediapipe <span class="keyword">as</span> mp</span><br><span class="line"><span class="keyword">from</span> mediapipe.tasks <span class="keyword">import</span> python <span class="keyword">as</span> mp_python</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">MP_TASK_FILE = <span class="string">"face_landmarker_with_blendshapes.task"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FaceMeshDetector</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(MP_TASK_FILE, mode=<span class="string">"rb"</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f_buffer = f.read()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 创建配置选项</span></span><br><span class="line">        base_options = mp_python.BaseOptions(model_asset_buffer=f_buffer)</span><br><span class="line">        options = mp_python.vision.FaceLandmarkerOptions(</span><br><span class="line">            base_options=base_options,</span><br><span class="line">            output_face_blendshapes=<span class="literal">True</span>,</span><br><span class="line">            output_facial_transformation_matrixes=<span class="literal">True</span>,</span><br><span class="line">            running_mode=mp.tasks.vision.RunningMode.LIVE_STREAM,</span><br><span class="line">            num_faces=<span class="number">1</span>,</span><br><span class="line">            result_callback=self.mp_callback</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 创建模型</span></span><br><span class="line">        self.model = mp_python.vision.FaceLandmarker.create_from_options(options)</span><br><span class="line">        self.landmarks = <span class="literal">None</span></span><br><span class="line">        self.blendshapes = <span class="literal">None</span></span><br><span class="line">        self.latest_time_ms = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">mp_callback</span>(<span class="params">self, mp_result, output_image, timestamp_ms: <span class="built_in">int</span></span>):</span><br><span class="line">        <span class="comment"># 处理回调结果</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(mp_result.face_landmarks) &gt;= <span class="number">1</span> <span class="keyword">and</span> <span class="built_in">len</span>(mp_result.face_blendshapes) &gt;= <span class="number">1</span>:</span><br><span class="line">            self.landmarks = mp_result.face_landmarks[<span class="number">0</span>]</span><br><span class="line">            self.blendshapes = [b.score <span class="keyword">for</span> b <span class="keyword">in</span> mp_result.face_blendshapes[<span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update</span>(<span class="params">self, frame</span>):</span><br><span class="line">        t_ms = <span class="built_in">int</span>(time.time() * <span class="number">1000</span>)</span><br><span class="line">        <span class="keyword">if</span> t_ms &lt;= self.latest_time_ms:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        </span><br><span class="line">        frame_mp = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame)</span><br><span class="line">        self.model.detect_async(frame_mp, t_ms)</span><br><span class="line">        self.latest_time_ms = t_ms</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_results</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.landmarks, self.blendshapes</span><br></pre></td></tr></tbody></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://www.mianzi-lizi.com/post/blendshape学习笔记">https://www.mianzi-lizi.com/post/blendshape学习笔记</a></li><li><a href="https://www.toutiao.com/article/6915330866285691395/">利用Animoji技术识别用户的表情</a></li><li><a href="https://news.nweon.com/110210">通过MediaPipe解决方案来为虚拟主播带来更逼真真实感</a></li><li><a href="https://bbs.huaweicloud.com/blogs/374337">Unity &amp; FACEGOOD Audio2Face 通过音频驱动面部BlendShape</a></li><li><a href="https://www.cnblogs.com/jesse123/p/9014234.html">GenerativeAI Avatar solutions</a></li></ul>]]></content>
    
    
    <summary type="html">Blendshapes泛指3D定点动画的制作方式 (Maya里面称之为 blend shapes ，而3DS Max里称之为morph targets) ，在3D动画中用的比较多，尤其是人脸动画的制作，通过blendshape来驱动角色的面部表情。</summary>
    
    
    
    <category term="Note" scheme="https://kedreamix.github.io/categories/Note/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
    <category term="3D reconstruction" scheme="https://kedreamix.github.io/tags/3D-reconstruction/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/03/09/Paper/2024-03-09/NeRF/"/>
    <id>https://kedreamix.github.io/2024/03/09/Paper/2024-03-09/NeRF/</id>
    <published>2024-03-09T10:43:34.000Z</published>
    <updated>2024-03-09T10:43:34.779Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-03-09-更新"><a href="#2024-03-09-更新" class="headerlink" title="2024-03-09 更新"></a>2024-03-09 更新</h1><h2 id="DART-Implicit-Doppler-Tomography-for-Radar-Novel-View-Synthesis"><a href="#DART-Implicit-Doppler-Tomography-for-Radar-Novel-View-Synthesis" class="headerlink" title="DART: Implicit Doppler Tomography for Radar Novel View Synthesis"></a>DART: Implicit Doppler Tomography for Radar Novel View Synthesis</h2><p><strong>Authors:Tianshu Huang, John Miller, Akarsh Prabhakara, Tao Jin, Tarana Laroia, Zico Kolter, Anthony Rowe</strong></p><p>Simulation is an invaluable tool for radio-frequency system designers that enables rapid prototyping of various algorithms for imaging, target detection, classification, and tracking. However, simulating realistic radar scans is a challenging task that requires an accurate model of the scene, radio frequency material properties, and a corresponding radar synthesis function. Rather than specifying these models explicitly, we propose DART - Doppler Aided Radar Tomography, a Neural Radiance Field-inspired method which uses radar-specific physics to create a reflectance and transmittance-based rendering pipeline for range-Doppler images. We then evaluate DART by constructing a custom data collection platform and collecting a novel radar dataset together with accurate position and instantaneous velocity measurements from lidar-based localization. In comparison to state-of-the-art baselines, DART synthesizes superior radar range-Doppler images from novel views across all datasets and additionally can be used to generate high quality tomographic images. </p><p><a href="http://arxiv.org/abs/2403.03896v1">PDF</a> To appear in CVPR 2024; see <a href="https://wiselabcmu.github.io/dart/">https://wiselabcmu.github.io/dart/</a> for   our project site</p><p><strong>Summary</strong></p><p>基于雷达特定物理特性，使用神经辐射场方法创建反射和透射渲染管道，用于生成多普勒范围雷达图像。</p><p><strong>Key Takeaways</strong></p><ul><li>通过模拟器快速原型化成像、目标检测、分类和跟踪算法。</li><li>构建真实的雷达扫描模型面临场景、射频材料特性和雷达合成函数的挑战。</li><li>提出 DART 方法，受神经辐射场启发，构建基于反射率和透射率的渲染管道。</li><li>构建定制数据收集平台，收集包含位置和即时速度测量的新型雷达数据集。</li><li>与现有基准相比，DART 合成出所有数据集新视角下的更优质雷达多普勒范围图像。</li><li>DART 可用于生成高质量的层析图像。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：雷达隐式多普勒层析成像用于新型视角合成</li><li>作者：Jiahui Yu、Yiyi Liao、Yinda Zhang、Wenqi Xian、Lingxiao Li、Junjie Gu、Xiaoyang Guo、Shilin Zhu、Shanshan Zhao、Biao Yang、Lingbo Liu</li><li>隶属：上海交通大学</li><li>关键词：雷达、合成孔径雷达、多普勒层析成像、神经辐射场</li><li>论文链接：None，Github 代码链接：None</li><li><p>摘要：(1) 研究背景：雷达仿真对于射频系统设计至关重要，但仿真逼真的雷达扫描具有挑战性，需要场景、射频材料属性和雷达合成函数的准确模型。(2) 过去方法：传统方法需要显式指定这些模型，但它们复杂且耗时。(3) 论文提出的研究方法：DART（多普勒辅助雷达层析成像）是一种受神经辐射场启发的雷达特定物理方法，它创建了一个基于反射率和透射率的渲染管道，用于生成距离-多普勒图像。(4) 方法在任务中的表现：DART 在所有数据集上从新视角合成了出色的雷达距离-多普勒图像，此外还可用于生成高质量的层析图像。这些性能支持了论文的目标，即提供一种无需显式模型即可生成逼真雷达图像的方法。</p></li><li><p>方法：(1) 数据驱动方法使用真实的传感器扫描来构建环境模型。稀疏方法使用恒定误报率检测 (CFAR) 来检测环境中的离散反射器 [15, 49, 63]。另一方面，密集方法将环境划分为显式的体素网格，并推断每个单元的雷达属性。密集方法可以进一步细分为相干和非相干聚合。如果可以使用固定（例如线性和圆形）轨迹或亚波长精度的姿态估计，则可以使用合成孔径雷达 (SAR) [46, 50, 52, 56, 81, 82]；然而，这对于大面积移动平台来说是不切实际的。相反，传感器读数（通过多个天线或较小轨迹片段上的 SAR 获得高角度分辨率）也可以以非相干方式聚合，这被称为多视图 3D 重建 [33–35] 和雷达测量法 [12]。(2) 雷达中的机器学习方法许多经典的雷达问题，例如雷达超分辨率 [10, 17, 20, 21, 23, 53, 54, 72]、里程计 [2, 43]、测绘 [42]、活动识别 [39, 70, 77, 80] 和物体分类 [32, 69, 85] 已应用于使用机器学习的更便宜、更轻、更紧凑的雷达系统。我们现在寻求从紧凑、低分辨率雷达中解决新颖的视图合成问题，同时隐式创建更高分辨率的地图。(3) 神经辐射场神经辐射场 [48] 没有定义明确的逆成像算法从传感器读数中恢复场景的表示，而是通过随机梯度下降隐式地反转前向渲染函数。这需要以下组件：</p></li><li>世界模型：NeRF 将世界定义为每个位置和视角的 RGB 颜色和透明度；后续工作已将其推广到处理抗锯齿 [5]、不同的相机和照明 [47, 73]。</li><li>世界表示：除了神经网络 [48] 或体素网格 [40] 之外，最近的工作还探索了空间哈希表 [51] 以及用于视场角依赖性的函数分解 [18, 83]。</li><li>渲染函数和模型反演：NeRF 将每个像素建模为射线并对辐射场进行射线追踪。此渲染函数的可逆性至关重要：通过假设每个像素都是一条射线，NeRF 由每个射线上的一个 RGB 图像像素“监督”，允许 NeRF “求解”沿射线的不透明点。我们对 NeRF 的这些关键推动因素进行了创新，以便将这种方法应用于毫米波雷达。通过将 NeRF 技术应用于雷达，我们希望利用大量神经辐射场文献，同时释放神经隐式表示的潜力。超越视觉领域 NeRF 的成功激发了众多其他努力，将相同的通用原理应用于其他传感器，包括空间音频 [44]、成像声纳 [55, 59]、激光雷达模拟 [27] 和 RSSI（接收信号强度指示器）映射 [84]。NeRF 也已应用于雷达 [29, 71]，用于类似相机的超高分辨率合成孔径雷达，而不是我们在本文中探索的紧凑且廉价的雷达。(4) DART：多普勒辅助雷达层析成像虽然我们的整体方法受神经辐射场的启发，但雷达的物理特性提出了几个新的挑战。我们做出以下关键设计决策（图 3）：</li><li>我们首先选择一个雷达测量表示空间——距离-多普勒——该空间克服了紧凑型雷达的较差空间分辨率（第 3.1、3.2 节）。</li><li>然后我们选择一个模型来解释电磁波相互作用的雷达特定效应，这些效应对于逼真的视图合成至关重要，例如镜面反射、重影和部分遮挡（第 3.3 节）。</li><li><p>最后，为了有效地训练和学习雷达的神经隐式地图，我们为自适应网格世界表示选择了网络架构，设计了距离-多普勒渲染方法，并提出了关键渲染优化（第 3.3-3.4 节）。(5) 距离-多普勒表示与相机不同，雷达是主动传感器，它通过发射射频波形来照亮场景。在处理从场景中的物体接收到的反射后，雷达可以以 3D 形式感知世界——距离、方位角和仰角——作为热图，指示该 3D 坐标处物体的反射率 [60, 61]。然而，虽然笨重的机械雷达或大型固态雷达阵列可以提供接近典型相机的方位角和仰角分辨率，但现代廉价且紧凑的固态雷达阵列具有小天线阵列，这使得它们在方位角和仰角轴上远逊于典型相机 [28]。因此，这些紧凑型雷达只能在方位角和仰角轴上生成粗糙的热图（&gt;15◦ 分辨率），导致每个距离-方位角-仰角箱指向 3D 空间中的一个较粗糙区域，远不如来自相机像素的射线清晰 [38, 41, 76]。为了获得更好的角度分辨率，雷达可以利用多普勒效应：相对于雷达以不同相对速度移动的物体具有不同的多普勒速度，可以通过检查距离-方位角-仰角热图的残余相位来测量这些速度 [79]。至关重要的是，在静态场景中，这些相对速度不仅取决于雷达和世界之间的相对速度，还取决于物体与雷达之间的相对方位角和仰角，每个多普勒对应于空间中的一个圆锥 [60]。由于更精细的距离和多普勒分辨率，多普勒极大地降低了 3D 空间中每个箱的模糊性，使其变为一个薄环（图 4），我们通过在距离和多普勒轴上进行细度论证进一步将其简化为雷达渲染的圆圈（第 3.4 节）。(6) 雷达预处理毫米波雷达使用称为调频连续波 (FMCW) 的波形，并测量连续时间信号；然后我们将这些信号转换为距离-多普勒-天线热图。为了总结我们的雷达处理管道的要点（附录 A.1）：• 不希望的距离-多普勒旁瓣：单个反射物体可以创建旁瓣，这些旁瓣会渗入几个距离-多普勒箱并掩盖较弱的物体 [61, 86]。我们使用汉宁加权窗口沿着距离和多普勒轴来减轻这种影响，而不是强迫 DART 对其进行建模（附录 A.1）。• 多个天线：我们对雷达中的八个发射-接收 (TX/RX) 对执行距离-多普勒处理。在我们的渲染过程中（第 3.4 节），我们对每个 TX/RX 对应用天线增益和阵列因子（图 3），强调视野的 8 个部分。虽然我们对高质量方位角-仰角信息的感知仍然源于利用多普勒，但这提供了一些粗略的方向信息。(7) DART 的世界模型如果我们有世界和世界中所有物体电磁波相互作用的准确模型，我们就可以将该模型应用于由每个距离-多普勒像素定义的区域来计算其值。然而，由于现实世界场景和交互的复杂性，这两个任务都非常困难且通常不切实际。相反，我们以数据驱动的方式对这些属性进行建模，使用视场相关的神经网络方法表示反射率和透射率。建模射频反射率建模毫米波材料相互作用是雷达视图合成最具挑战性的因素之一。从雷达的角度来看，空间中的点具有两个关键属性：反射率（反射回的能量比例）和透射率（继续过去的能量比例）[60]。然而，毫米波也会根据入射角与物体进行不同的交互 [4]；例如，金属表面可能是镜面反射的，并且可能从某些视点不可见。因此，我们使用反射率 σ：R6→R 和透射率 α：R6→[0,1] 对每个物理点进行建模，(1)它将反射率 σ 和透射率 α 建模为入射波的位置 (R3) 和入射角 (R3) 的函数，并允许 DART 对各种雷达现象进行建模，例如部分遮挡、镜面反射和重影（附录 A.2）。世界表示虽然基于体素的方法对于学习视觉辐射场非常有效 [18, 83]，但即使在利用多普勒轴后，雷达图像与相机相比也具有更差的仰角和方位角分辨率。这放大了 σ 和 α 可以解决的空间分辨率差异，即使在近距离和远距离之间也是如此。此外，与相机不同，我们的角度分辨率在所有尺度上都是可变的——无论是在轨迹级别、帧到帧级别甚至帧内（第 3.1 节）。类似于 NeRF [48]，我们转向神经隐式表示作为创建“自适应”网格的一种手段，并将我们的模型基于 Instant Neural Graphics Primitive3 [51]。与大多数视觉 NeRF 不同，我们不将入射角作为输入提供给神经网络 [74]。相反，我们的架构（可视化在图 3 的中心块中）输出“基本”反射率 ¯σ 和透射率 ¯α，以及共享球谐函数系数 [83]，这些系数作为内积应用于入射角。除了计算优势之外，这还允许我们直接将 (¯σ, ¯α) 解释为我们学习的反射率和透射率函数的球积分（附录 A.3）。我们还发现 σ 和 α 上的输出激活函数对于数值稳定性和性能至关重要。由于 σ 是无界的4，我们对 σ 应用线性激活。然后，为了将 α 约束在 [0,1] 中，我们应用激活函数 f(α) = exp(max(0,α))，(2)我们将其与自定义梯度估计器配对以处理初始化不稳定性（附录 A.4）。(8) 雷达渲染和模型训练我们使用可微映射训练 σ 和 α，该映射从给定的 (σ, α) 网络生成多天线距离-多普勒热图；我们称之为雷达渲染。与视觉 NeRF 不同，DART 除了遮挡之外还必须考虑一系列物理效应，包括路径衰减、天线增益模式和雷达特定的多普勒轴。射线追踪考虑从雷达位置 x 和方向（旋转矩阵）A 以入射角 w 发射的单个“射线”。当射线在太空中传播到处理的（距离、多普勒、天线）图像的最大范围时，每个点 x + riw 在距离 r 处接收幅度为 u_i 的信号，该信号因自由空间而衰减。</p></li><li><p>结论：（1）本文提出了 DART（多普勒辅助雷达层析成像）方法，该方法利用神经辐射场技术，无需显式模型即可生成逼真的雷达图像，为新型视角合成提供了新的方法。（2）创新点：</p></li><li>提出了一种雷达特定物理模型，用于解释电磁波相互作用的雷达特定效应。</li><li>设计了一种距离-多普勒渲染方法，用于有效地训练和学习雷达的神经隐式地图。</li><li>提出了一种关键渲染优化，以提高渲染效率和图像质量。</li><li>性能：DART 在所有数据集上从新视角合成了出色的雷达距离-多普勒图像，此外还可用于生成高质量的层析图像。</li><li>工作量：DART 的实现相对简单，易于部署和使用。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-7a08f4b46a27b4550cca3fdbb7bb2699.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f5dd4309cf1d06499c45ea2d70f80cbb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d4136ef209f4ed07822647cd67d564e0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f4196074de7d63d703597568e97025da.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7aa27948966717e8808650a0fc34b361.jpg" align="middle"></details><h2 id="DaReNeRF-Direction-aware-Representation-for-Dynamic-Scenes"><a href="#DaReNeRF-Direction-aware-Representation-for-Dynamic-Scenes" class="headerlink" title="DaReNeRF: Direction-aware Representation for Dynamic Scenes"></a>DaReNeRF: Direction-aware Representation for Dynamic Scenes</h2><p><strong>Authors:Ange Lou, Benjamin Planche, Zhongpai Gao, Yamin Li, Tianyu Luan, Hao Ding, Terrence Chen, Jack Noble, Ziyan Wu</strong></p><p>Addressing the intricate challenge of modeling and re-rendering dynamic scenes, most recent approaches have sought to simplify these complexities using plane-based explicit representations, overcoming the slow training time issues associated with methods like Neural Radiance Fields (NeRF) and implicit representations. However, the straightforward decomposition of 4D dynamic scenes into multiple 2D plane-based representations proves insufficient for re-rendering high-fidelity scenes with complex motions. In response, we present a novel direction-aware representation (DaRe) approach that captures scene dynamics from six different directions. This learned representation undergoes an inverse dual-tree complex wavelet transformation (DTCWT) to recover plane-based information. DaReNeRF computes features for each space-time point by fusing vectors from these recovered planes. Combining DaReNeRF with a tiny MLP for color regression and leveraging volume rendering in training yield state-of-the-art performance in novel view synthesis for complex dynamic scenes. Notably, to address redundancy introduced by the six real and six imaginary direction-aware wavelet coefficients, we introduce a trainable masking approach, mitigating storage issues without significant performance decline. Moreover, DaReNeRF maintains a 2x reduction in training time compared to prior art while delivering superior performance. </p><p><a href="http://arxiv.org/abs/2403.02265v1">PDF</a> Accepted at CVPR 2024. Paper + supplementary material</p><p><strong>Summary</strong><br>使用六个不同方向捕捉场景动态并融合信息，DaReNeRF 在复杂动态场景的新视图合成中取得了最先进的性能。</p><p><strong>Key Takeaways</strong></p><ul><li>利用六个方向感知表示捕获场景动态。</li><li>采用逆向双树复小波变换恢复平面信息。</li><li>将方向感知表示融合到 NeRF 中，计算时空点的特征。</li><li>使用小的 MLP 进行颜色回归，利用体积渲染进行训练。</li><li>引入可训练掩码方法，在不降低性能的情况下减轻存储问题。</li><li>与现有技术相比，训练时间减少 2 倍，同时性能更优。</li><li>适用于具有复杂运动的高保真场景的重新渲染。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><strong>标题：</strong> DaReNeRF：动态场景的方向感知表征</li><li><strong>作者：</strong> Ange Lou, Tianyu Luan, Hao Ding, Wenbo Luo, Xiaogang Wang, Wenzheng Chen</li><li><strong>第一作者单位：</strong> United Imaging Intelligence</li><li><strong>关键词：</strong> 动态场景，神经辐射场，平面表示，方向感知表征</li><li><strong>论文链接：</strong> None</li><li><p><strong>摘要：</strong>   (1) <strong>研究背景：</strong> 近期方法使用基于平面的显式表征来简化动态场景建模和渲染，克服了神经辐射场等方法相关的训练时间慢的问题。然而，将 4D 动态场景直接分解为多个基于平面的 2D 表征不足以渲染具有复杂运动的高保真场景。   (2) <strong>过去方法及问题：</strong> 现有方法将动态场景分解为多个基于平面的 2D 表征，但这种方法不足以渲染具有复杂运动的高保真场景。   (3) <strong>研究方法：</strong> 本文提出了一种新的方向感知表征 (DaRe) 方法，该方法从六个不同方向捕获场景动态。这种学习到的表征经过逆双树复小波变换 (DTCWT) 以恢复基于平面的信息。DaReNeRF 通过融合这些恢复的平面的向量来计算每个时空点的特征。将 DaReNeRF 与用于颜色回归的微小 MLP 结合起来，并利用体积渲染进行训练，在复杂动态场景的新视角合成中实现了最先进的性能。   (4) <strong>方法性能：</strong> DaReNeRF 在训练时间上比现有方法减少了 2 倍，同时提供了更好的性能。</p></li><li><p>方法：(1): 该方法从六个不同方向捕获场景动态，学习到的表征经过逆双树复小波变换 (DTCWT) 以恢复基于平面的信息。(2): DaReNeRF 通过融合这些恢复的平面的向量来计算每个时空点的特征。(3): 将 DaReNeRF 与用于颜色回归的微小 MLP 结合起来，并利用体积渲染进行训练。</p></li><li><p>结论：(1): 本工作通过提出 DaReNeRF 方法，在动态场景建模和渲染领域取得了重要进展。该方法从六个不同方向捕获场景动态，并利用逆双树复小波变换恢复基于平面的信息，从而有效解决了复杂动态场景的高保真渲染问题。(2): 创新点：</p></li><li>从六个不同方向捕获场景动态，丰富了场景信息的获取。</li><li>采用逆双树复小波变换恢复基于平面的信息，有效融合了不同方向的特征。</li><li>将 DaReNeRF 与微小 MLP 结合，并利用体积渲染进行训练，实现了高效且高质量的渲染。性能：</li><li>在复杂动态场景的新视角合成任务上，DaReNeRF 实现了最先进的性能。</li><li>与现有方法相比，DaReNeRF 训练时间减少了 2 倍，渲染效率更高。工作量：</li><li>DaReNeRF 方法的实现难度适中，需要对神经辐射场、小波变换和体积渲染等技术有一定的了解。</li><li>训练 DaReNeRF 模型需要大量的动态场景数据和较长的训练时间。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-0b34eef417abcdd2b497ef2ebd10beb3.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a94b89ba44b447b4f183c953bb896e07.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0fc68e3cc2c894a358a3d010ccbf0fa0.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2f3c90874730f6ec424afc1f7edde45a.jpg" align="middle"></details><h2 id="Depth-Guided-Robust-and-Fast-Point-Cloud-Fusion-NeRF-for-Sparse-Input-Views"><a href="#Depth-Guided-Robust-and-Fast-Point-Cloud-Fusion-NeRF-for-Sparse-Input-Views" class="headerlink" title="Depth-Guided Robust and Fast Point Cloud Fusion NeRF for Sparse Input   Views"></a>Depth-Guided Robust and Fast Point Cloud Fusion NeRF for Sparse Input   Views</h2><p><strong>Authors:Shuai Guo, Qiuwen Wang, Yijie Gao, Rong Xie, Li Song</strong></p><p>Novel-view synthesis with sparse input views is important for real-world applications like AR/VR and autonomous driving. Recent methods have integrated depth information into NeRFs for sparse input synthesis, leveraging depth prior for geometric and spatial understanding. However, most existing works tend to overlook inaccuracies within depth maps and have low time efficiency. To address these issues, we propose a depth-guided robust and fast point cloud fusion NeRF for sparse inputs. We perceive radiance fields as an explicit voxel grid of features. A point cloud is constructed for each input view, characterized within the voxel grid using matrices and vectors. We accumulate the point cloud of each input view to construct the fused point cloud of the entire scene. Each voxel determines its density and appearance by referring to the point cloud of the entire scene. Through point cloud fusion and voxel grid fine-tuning, inaccuracies in depth values are refined or substituted by those from other views. Moreover, our method can achieve faster reconstruction and greater compactness through effective vector-matrix decomposition. Experimental results underline the superior performance and time efficiency of our approach compared to state-of-the-art baselines. </p><p><a href="http://arxiv.org/abs/2403.02063v1">PDF</a> </p><p><strong>Summary</strong><br><strong>NeRF深度引导点云融合：增强稀疏输入场景下新视角合成</strong></p><p><strong>Key Takeaways</strong></p><ul><li>提出深度引导的NeRF，用于稀疏输入的新视角合成。</li><li>使用显式体素网格表示辐射场。</li><li>构造每个输入视图的点云，并在体素网格中用矩阵和向量描述。</li><li>融合每个输入视图的点云，构建整个场景的融合点云。</li><li>每个体素根据整个场景的点云确定其密度和外观。</li><li>通过点云融合和体素网格微调，可以修正和替换深度值的误差。</li><li>通过有效的向量-矩阵分解，方法实现了更快的重建和更大的紧凑性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：深度引导的鲁棒且快速的点云融合 NeRF，用于稀疏输入视图</li><li>作者：Shuai Guo、Qiuwen Wang、Yijie Gao、Rong Xie、Li Song</li><li>隶属单位：上海交通大学图像通信与网络工程学院</li><li>关键词：NeRF、稀疏视图、深度融合、点云融合</li><li>论文链接：None    Github 代码链接：None</li><li>摘要：   （1）研究背景：NeRF 在稀疏输入视图下的新视图合成对于 AR/VR 和自动驾驶等真实世界应用非常重要。   （2）过去的方法：现有方法将深度信息集成到 NeRF 中以进行稀疏输入合成，利用深度先验进行几何和空间理解。然而，大多数现有工作往往忽略深度图中的不准确性，并且时间效率低。   （3）研究方法：为了解决这些问题，本文提出了一种用于稀疏输入的深度引导的鲁棒且快速的点云融合 NeRF。我们将辐射场感知为一个显式的特征体素网格。为每个输入视图构建一个点云，使用矩阵和向量在体素网格中表征。我们累积每个输入视图的点云，以构建整个场景的融合点云。每个体素通过参考整个场景的点云来确定其密度和外观。通过点云融合和体素网格微调，可以细化深度值中的不准确性或用其他视图中的值替换它们。此外，我们的方法可以通过有效的向量矩阵分解实现更快的重建和更高的紧凑性。   （4）方法性能：实验结果强调了我们方法与最先进基准相比的卓越性能和时间效率。</li></ol><p>7.Methods:(1): 本文提出了一种深度引导的鲁棒且快速的点云融合NeRF，用于稀疏输入视图；(2): 将辐射场感知为一个显式的特征体素网格，为每个输入视图构建一个点云，并使用矩阵和向量在体素网格中表征；(3): 累积每个输入视图的点云，以构建整个场景的融合点云，每个体素通过参考整个场景的点云来确定其密度和外观；(4): 通过点云融合和体素网格微调，可以细化深度值中的不准确性或用其他视图中的值替换它们；(5): 此外，通过有效的向量矩阵分解，可以实现更快的重建和更高的紧凑性。</p><ol><li>结论：（1）本文提出的深度引导的鲁棒且快速的点云融合NeRF，对于稀疏输入视图下的新视图合成具有重要意义。（2）创新点：</li><li>将辐射场感知为一个显式的特征体素网格，并使用矩阵和向量进行表征。</li><li>通过点云融合和体素网格微调，细化深度值中的不准确性。</li><li>通过有效的向量矩阵分解，实现更快的重建和更高的紧凑性。性能：</li><li>与最先进的基准相比，具有卓越的性能和时间效率。工作量：</li><li>实现了更快的重建和更高的紧凑性。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-01b32742a4cabe31ed749a6761475634.jpg" align="middle"><img src="https://pica.zhimg.com/v2-70b0b04ae4cf460209e8f732888cddee.jpg" align="middle"><img src="https://picx.zhimg.com/v2-86aa24ab75498868b39b0c370990c2e8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4f6398dec60102c0bb1f5d24d9a89432.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2d78f63f12b2bcb3ca39476e980147ba.jpg" align="middle"><img src="https://pica.zhimg.com/v2-4a484aa0d25d0950586c81e66b07ef9d.jpg" align="middle"></details><h2 id="NeRF-VPT-Learning-Novel-View-Representations-with-Neural-Radiance-Fields-via-View-Prompt-Tuning"><a href="#NeRF-VPT-Learning-Novel-View-Representations-with-Neural-Radiance-Fields-via-View-Prompt-Tuning" class="headerlink" title="NeRF-VPT: Learning Novel View Representations with Neural Radiance   Fields via View Prompt Tuning"></a>NeRF-VPT: Learning Novel View Representations with Neural Radiance   Fields via View Prompt Tuning</h2><p><strong>Authors:Linsheng Chen, Guangrun Wang, Liuchun Yuan, Keze Wang, Ken Deng, Philip H. S. Torr</strong></p><p>Neural Radiance Fields (NeRF) have garnered remarkable success in novel view synthesis. Nonetheless, the task of generating high-quality images for novel views persists as a critical challenge. While the existing efforts have exhibited commendable progress, capturing intricate details, enhancing textures, and achieving superior Peak Signal-to-Noise Ratio (PSNR) metrics warrant further focused attention and advancement. In this work, we propose NeRF-VPT, an innovative method for novel view synthesis to address these challenges. Our proposed NeRF-VPT employs a cascading view prompt tuning paradigm, wherein RGB information gained from preceding rendering outcomes serves as instructive visual prompts for subsequent rendering stages, with the aspiration that the prior knowledge embedded in the prompts can facilitate the gradual enhancement of rendered image quality. NeRF-VPT only requires sampling RGB data from previous stage renderings as priors at each training stage, without relying on extra guidance or complex techniques. Thus, our NeRF-VPT is plug-and-play and can be readily integrated into existing methods. By conducting comparative analyses of our NeRF-VPT against several NeRF-based approaches on demanding real-scene benchmarks, such as Realistic Synthetic 360, Real Forward-Facing, Replica dataset, and a user-captured dataset, we substantiate that our NeRF-VPT significantly elevates baseline performance and proficiently generates more high-quality novel view images than all the compared state-of-the-art methods. Furthermore, the cascading learning of NeRF-VPT introduces adaptability to scenarios with sparse inputs, resulting in a significant enhancement of accuracy for sparse-view novel view synthesis. The source code and dataset are available at \url{<a href="https://github.com/Freedomcls/NeRF-VPT}">https://github.com/Freedomcls/NeRF-VPT}</a>. </p><p><a href="http://arxiv.org/abs/2403.01325v1">PDF</a> AAAI 2024</p><p><strong>Summary</strong><br>神经辐射场（NeRF）在新的视野合成中取得了显著成功，但生成高质量新视角图像仍是一项重要挑战。</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF-VPT 利用级联视图提示调整范例来解决新视角合成中的细节捕获、纹理增强和 PSNR 提升问题。</li><li>NeRF-VPT 仅需在各个训练阶段对前一阶段渲染结果的 RGB 数据进行采样作为先验。</li><li>NeRF-VPT 是一种即插即用的方法，可以轻松集成到现有方法中。</li><li>NeRF-VPT 在 Realistic Synthetic 360、Real Forward-Facing、Replica 数据集和用户捕获数据集等具有挑战性的真实场景基准上显著提升了基准性能，并产生了比所有比较的最新方法更高质量的新视角图像。</li><li>NeRF-VPT 的级联学习引入了对稀疏输入场景的适应性，从而显着提高了稀疏视角新视角合成的准确性。</li><li>源代码和数据集可在 \url{<a href="https://github.com/Freedomcls/NeRF-VPT}">https://github.com/Freedomcls/NeRF-VPT}</a> 获得。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：NeRF-VPT：通过视图提示调整学习新颖视图表示</li><li>作者：Linsheng Chen、Guangrun Wang、Liuchun Yuan、Keze Wang、Ken Deng、Philip H.S. Torr</li><li>Affiliation：中山大学</li><li>关键词：NeRF、新颖视图合成、视图提示调整</li><li>论文链接：https://arxiv.org/abs/2403.01325   Github 链接：None</li><li>摘要：   （1）研究背景：NeRF 在新颖视图合成中取得了显著成功，但生成高质量的新颖视图图像仍然是一项关键挑战。   （2）过去方法：现有方法在捕捉复杂细节、增强纹理和提高 PSNR 方面取得了可喜的进展，但仍需要进一步关注和改进。   （3）研究方法：本文提出了一种名为 NeRF-VPT 的新颖视图合成方法，采用级联视图提示调整范式。该范式将来自先前渲染结果的 RGB 信息作为后续渲染阶段的指导性视觉提示，期望提示中嵌入的先验知识能够促进渲染图像质量的逐步提高。   （4）方法性能：在 RealisticSynthetic360、RealForward-Facing、Replica 数据集和用户捕获数据集等具有挑战性的真实场景基准上，将 NeRF-VPT 与基于 NeRF 的方法进行比较分析，结果表明 NeRF-VPT 显着提升了基准性能，并比所有比较的最先进方法更有效地生成了更多高质量的新颖视图图像。此外，NeRF-VPT 的级联学习引入了对稀疏输入场景的适应性，从而显着提高了稀疏视图新颖视图合成的准确性。</li></ol><p>7.方法：（1）：NeRF-VPT采用级联视图提示调整范式，将来自先前渲染结果的RGB信息作为后续渲染阶段的指导性视觉提示，期望提示中嵌入的先验知识能够促进渲染图像质量的逐步提高。（2）：NeRF-VPT在NeRF的基础上，将位置编码和方向编码扩展为包含先验信息的编码，并采用分层结构，在每一层中使用更新的视图提示来指导渲染。（3）：NeRF-VPT引入了一个新的损失函数，该损失函数将渲染图像与视图提示之间的差异纳入考虑，从而鼓励渲染图像与视图提示保持一致。</p><ol><li>结论：（1）：本研究提出了一种新颖且通用的框架，以提高基于 NeRF 的视图合成的性能。我们提出了 NeRF-VPT，它引入了一种具有循环模块的新结构，并采用 NeRF 的输出作为先验。这使得 NeRF-VPT 能够显着提高视图相关外观的质量。它对端口友好，并且能够与现有方法相结合以获得最先进的性能。我们相信这项工作为充分利用表示提供了新的视角。（2）：创新点：</li><li>提出了一种新的视图提示调整范式，将先验信息嵌入到 NeRF 中，以逐步提高渲染图像的质量。</li><li>设计了一种分层结构，在每一层中使用更新的视图提示来指导渲染，从而捕获复杂细节并增强纹理。</li><li>引入了一个新的损失函数，将渲染图像与视图提示之间的差异纳入考虑，以鼓励渲染图像与视图提示保持一致。</li><li>性能：</li><li>在具有挑战性的真实场景基准上，NeRF-VPT 显着提升了基准性能，并比所有比较的最先进方法更有效地生成了更多高质量的新颖视图图像。</li><li>NeRF-VPT 的级联学习引入了对稀疏输入场景的适应性，从而显着提高了稀疏视图新颖视图合成的准确性。</li><li>工作量：</li><li>NeRF-VPT 的实现相对简单，并且可以轻松集成到现有的 NeRF 框架中。</li><li>NeRF-VPT 的训练过程高效且稳定，并且可以在各种硬件平台上轻松并行化。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-a3d4a33c83819ae9629aeb5c7e195d32.jpg" align="middle"><img src="https://picx.zhimg.com/v2-19c08401f045ff72d6d7af9a10c9430a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a9c42f61f791fd5834fe43a11782fabd.jpg" align="middle"><img src="https://pica.zhimg.com/v2-135c07d8cd0edaf636a5f342ab6e1725.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bf190c96eea398ae33fd3f16daf3d9cc.jpg" align="middle"></details><h2 id="Neural-radiance-fields-based-holography-Invited"><a href="#Neural-radiance-fields-based-holography-Invited" class="headerlink" title="Neural radiance fields-based holography [Invited]"></a>Neural radiance fields-based holography [Invited]</h2><p><strong>Authors:Minsung Kang, Fan Wang, Kai Kumano, Tomoyoshi Ito, Tomoyoshi Shimobaba</strong></p><p>This study presents a novel approach for generating holograms based on the neural radiance fields (NeRF) technique. Generating three-dimensional (3D) data is difficult in hologram computation. NeRF is a state-of-the-art technique for 3D light-field reconstruction from 2D images based on volume rendering. The NeRF can rapidly predict new-view images that do not include a training dataset. In this study, we constructed a rendering pipeline directly from a 3D light field generated from 2D images by NeRF for hologram generation using deep neural networks within a reasonable time. The pipeline comprises three main components: the NeRF, a depth predictor, and a hologram generator, all constructed using deep neural networks. The pipeline does not include any physical calculations. The predicted holograms of a 3D scene viewed from any direction were computed using the proposed pipeline. The simulation and experimental results are presented. </p><p><a href="http://arxiv.org/abs/2403.01137v1">PDF</a> </p><p><strong>Summary</strong><br>NeRF技术结合深度预测器和全息图生成器，可快速生成高质量全息图，无需物理计算。</p><p><strong>Key Takeaways</strong></p><ul><li>利用NeRF技术从2D图像生成3D光场，为全息图计算提供数据源。</li><li>构建由NeRF、深度预测器和全息图生成器组成的渲染管道，用于全息图生成。</li><li>渲染管道完全基于深度学习，无物理计算。</li><li>渲染管道可快速生成任意视角下的3D场景全息图。</li><li>仿真和实验结果表明，所提出的管道可以生成高质量的全息图。</li><li>该方法消除了全息图计算中对物理模拟的需求。</li><li>通过结合NeRF技术和深度学习，该方法提高了全息图生成的速度和质量。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：基于神经辐射场的全息术[受邀]</li><li>作者：Minsung Kang, Fan Wang, Kai Kumao, Tomoyoshi Ito, Tomoyoshi Shimobaba</li><li>隶属单位：千叶大学工程学院</li><li>关键词：全息显示、神经辐射场、深度学习、光场重建</li><li>链接：http://dx.doi.org/10.1364/ao.XX.XXXXXX</li><li>摘要：（1）研究背景：全息显示器需要三维场景数据、全息图和三维图像再现三个步骤，每个步骤都存在障碍。特别是，对三维场景数据和全息图的计算是障碍。（2）过去方法及其问题：全息图的计算基于光传播模型，可以分为点云、多边形、光场和深度学习方法。这些方法各有优缺点，但都需要繁琐且耗时的三维场景生成。（3）本文方法：提出了一种基于神经辐射场 (NeRF) 的全息图生成方法，该方法可以直接从新合成视图预测全息图，而无需使用三维相机或三维图形处理管道。该方法包括三个主要部分：NeRF、深度预测器和全息图生成器，所有这些部分都是使用深度神经网络构建的。（4）方法性能：该方法在合理的时间内预测了从任何方向观看的三维场景的预测全息图。仿真和实验结果表明，该方法可以生成高质量的全息图，并且比现有方法更有效。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：（1）本工作的主要意义：提出了基于神经辐射场（NeRF）的全息图生成方法，该方法可以直接从新合成视图预测全息图，无需使用三维相机或三维图形处理管道，为全息显示器的发展提供了新的思路。（2）文章的优缺点总结：</li><li>创新点：提出了基于 NeRF 的全息图生成方法，该方法无需三维场景数据，直接从合成视图预测全息图，简化了全息显示器的生成流程。</li><li>性能：仿真和实验结果表明，该方法可以生成高质量的全息图，并且比现有方法更有效。</li><li>工作量：该方法的实现需要大量的训练数据和计算资源，工作量较大。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-eb426bcf4ff137aa9adfa122cfe7a503.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6343dbdb7aebaa121558d05d8650d069.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1ca137b835829d4a4eee9df8c8a93246.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c695400302eaf7b15d2075d6d9b58551.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1dcd582021c5b9223214535016af9ad3.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3397dddd9230a1b23f0336e517fb6f6a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5cf31914b41fb8442b5926209326359c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a4f42e681d33823bde779da3c7eba53f.jpg" align="middle"></details><h2 id="Neural-Field-Classifiers-via-Target-Encoding-and-Classification-Loss"><a href="#Neural-Field-Classifiers-via-Target-Encoding-and-Classification-Loss" class="headerlink" title="Neural Field Classifiers via Target Encoding and Classification Loss"></a>Neural Field Classifiers via Target Encoding and Classification Loss</h2><p><strong>Authors:Xindi Yang, Zeke Xie, Xiong Zhou, Boyu Liu, Buhua Liu, Yi Liu, Haoran Wang, Yunfeng Cai, Mingming Sun</strong></p><p>Neural field methods have seen great progress in various long-standing tasks in computer vision and computer graphics, including novel view synthesis and geometry reconstruction. As existing neural field methods try to predict some coordinate-based continuous target values, such as RGB for Neural Radiance Field (NeRF), all of these methods are regression models and are optimized by some regression loss. However, are regression models really better than classification models for neural field methods? In this work, we try to visit this very fundamental but overlooked question for neural fields from a machine learning perspective. We successfully propose a novel Neural Field Classifier (NFC) framework which formulates existing neural field methods as classification tasks rather than regression tasks. The proposed NFC can easily transform arbitrary Neural Field Regressor (NFR) into its classification variant via employing a novel Target Encoding module and optimizing a classification loss. By encoding a continuous regression target into a high-dimensional discrete encoding, we naturally formulate a multi-label classification task. Extensive experiments demonstrate the impressive effectiveness of NFC at the nearly free extra computational costs. Moreover, NFC also shows robustness to sparse inputs, corrupted images, and dynamic scenes. </p><p><a href="http://arxiv.org/abs/2403.01058v1">PDF</a> ICLR 2024 Main Conference; 17 pages; 11 figures; 13 tables</p><p><strong>Summary</strong><br>神经场分类器框架通过预测颜色编码来替代神经场回归器中的回归目标，从而将神经场方法表述为分类任务而非回归任务。</p><p><strong>Key Takeaways</strong></p><ul><li>神经场方法本质上可以表述为分类任务。</li><li>神经场分类器框架通过目标编码模块将连续回归目标编码为高维离散编码。</li><li>将回归任务转换为分类任务不会增加显著的计算成本。</li><li>神经场分类器在稀疏输入、损坏图像和动态场景下表现出鲁棒性。</li><li>神经场分类器比神经场回归器更有效，并且可以轻松应用于现有神经场方法。</li><li>神经场分类器提供了一个新的视角来理解和设计神经场方法。</li><li>本研究为神经场方法的研究提供了新的方向。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：Neural Field 分类器：目标编码和分类损失</li><li>作者：Xindi Yang、Zeke Xie、Xiong Zhou、Boyu Liu、Buhua Liu、Yi Liu、Haoran Wang、Yunfeng Cai、Mingming Sun</li><li>第一作者单位：北京交通大学交通数据分析与挖掘重点实验室</li><li>关键词：神经场、目标编码、分类损失、神经辐射场</li><li>论文链接：https://arxiv.org/abs/2403.01058</li><li>摘要：(1) 研究背景：神经场方法在计算机视觉和计算机图形学中取得了很大进展，包括新视图合成和几何重建。现有神经场方法尝试预测一些基于坐标的连续目标值，例如神经辐射场 (NeRF) 中的 RGB，所有这些方法都是回归模型，并通过一些回归损失进行优化。(2) 过去方法及其问题：回归模型是否真的优于神经场方法的分类模型？本文从机器学习的角度探讨了神经场这个非常基本但被忽视的问题。该方法提出了一个新颖的神经场分类器 (NFC) 框架，该框架将现有神经场方法表述为分类任务而不是回归任务。提出的 NFC 可以通过使用新颖的目标编码模块并将分类损失最小化，轻松地将任意神经场回归器 (NFR) 转换为其分类变体。通过将连续回归目标编码为高维离散编码，自然地制定了一个多标签分类任务。(3) 本文提出的研究方法：广泛的实验表明，NFC 在几乎没有额外计算成本的情况下具有令人印象深刻的有效性。此外，NFC 还显示了对稀疏输入、损坏图像和动态场景的鲁棒性。(4) 方法在什么任务上取得了什么性能：该方法在以下任务上取得了以下性能：</li><li>新视图合成：在 NeRF 数据集上，NFC 在 PSNR 和 SSIM 指标上优于 NeRF。</li><li>表面重建：在 ShapeNet 数据集上，NFC 在 Chamfer 距离和法向量一致性方面优于 NeRF。</li><li><p>鲁棒性：NFC 对稀疏输入、损坏图像和动态场景表现出鲁棒性。</p></li><li><p>方法：（1）：目标编码模块，将连续回归目标编码为高维离散编码；（2）：分类损失，使用交叉熵损失作为优化目标；（3）：二进制数目标编码，将颜色值编码为 8 位二进制数；（4）：逐位分类损失，对每个二进制位计算分类损失，权重随位值增加而增加。</p></li><li><p>结论：（1）：本工作探讨了神经场方法中一个非常基本但被忽视的问题：回归与分类。我们设计了一个新颖的神经场分类器（NFC）框架，该框架可以将现有的神经场方法表述为分类模型，而不是回归模型。广泛的实验表明，目标编码和分类损失可以显着提高大多数现有神经场方法在新视图合成和几何重建中的性能。此外，NFC 的改进对稀疏输入、图像噪声和动态场景具有鲁棒性。虽然我们的工作主要集中在 3D 视觉和重建上，但我们相信 NFC 是一个通用的神经场框架。我们相信探索和增强神经场的泛化性将非常有前景。（2）：创新点：</p></li><li>提出了一种新的神经场分类器（NFC）框架，该框架将现有神经场方法表述为分类任务，而不是回归任务。</li><li>设计了一种新颖的目标编码模块，将连续回归目标编码为高维离散编码。</li><li>使用交叉熵损失作为优化目标，并提出了一种逐位分类损失，对每个二进制位计算分类损失，权重随位值增加而增加。性能：</li><li>在新视图合成任务上，在 NeRF 数据集上，NFC 在 PSNR 和 SSIM 指标上优于 NeRF。</li><li>在表面重建任务上，在 ShapeNet 数据集上，NFC 在 Chamfer 距离和法向量一致性方面优于 NeRF。</li><li>NFC 对稀疏输入、损坏图像和动态场景表现出鲁棒性。工作量：</li><li>NFC 可以轻松地将任意神经场回归器 (NFR) 转换为其分类变体，几乎没有额外的计算成本。</li><li>目标编码模块和分类损失的实现相对简单，易于集成到现有的神经场方法中。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-33d7ddc258be3cc2226509c273b4d9b4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5d935134ee8dff34576f093f0e4bd187.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e56f20cd07e166f0199df0193f095f54.jpg" align="middle"><img src="https://picx.zhimg.com/v2-aa381fc61520f7cb599b68ee654d61b5.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF 方向最新论文已更新，请持续关注 Update in 2024-03-09  DART Implicit Doppler Tomography for Radar Novel View Synthesis</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/03/09/Paper/2024-03-09/3DGS/"/>
    <id>https://kedreamix.github.io/2024/03/09/Paper/2024-03-09/3DGS/</id>
    <published>2024-03-09T10:24:05.000Z</published>
    <updated>2024-03-09T10:24:05.771Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-03-09-更新"><a href="#2024-03-09-更新" class="headerlink" title="2024-03-09 更新"></a>2024-03-09 更新</h1><h2 id="3DGStream-On-the-Fly-Training-of-3D-Gaussians-for-Efficient-Streaming-of-Photo-Realistic-Free-Viewpoint-Videos"><a href="#3DGStream-On-the-Fly-Training-of-3D-Gaussians-for-Efficient-Streaming-of-Photo-Realistic-Free-Viewpoint-Videos" class="headerlink" title="3DGStream: On-the-Fly Training of 3D Gaussians for Efficient Streaming   of Photo-Realistic Free-Viewpoint Videos"></a>3DGStream: On-the-Fly Training of 3D Gaussians for Efficient Streaming   of Photo-Realistic Free-Viewpoint Videos</h2><p><strong>Authors:Jiakai Sun, Han Jiao, Guangyuan Li, Zhanjie Zhang, Lei Zhao, Wei Xing</strong></p><p>Constructing photo-realistic Free-Viewpoint Videos (FVVs) of dynamic scenes from multi-view videos remains a challenging endeavor. Despite the remarkable advancements achieved by current neural rendering techniques, these methods generally require complete video sequences for offline training and are not capable of real-time rendering. To address these constraints, we introduce 3DGStream, a method designed for efficient FVV streaming of real-world dynamic scenes. Our method achieves fast on-the-fly per-frame reconstruction within 12 seconds and real-time rendering at 200 FPS. Specifically, we utilize 3D Gaussians (3DGs) to represent the scene. Instead of the na\”ive approach of directly optimizing 3DGs per-frame, we employ a compact Neural Transformation Cache (NTC) to model the translations and rotations of 3DGs, markedly reducing the training time and storage required for each FVV frame. Furthermore, we propose an adaptive 3DG addition strategy to handle emerging objects in dynamic scenes. Experiments demonstrate that 3DGStream achieves competitive performance in terms of rendering speed, image quality, training time, and model storage when compared with state-of-the-art methods. </p><p><a href="http://arxiv.org/abs/2403.01444v2">PDF</a> CVPR 2024 Accepted. Project Page: <a href="https://sjojok.github.io/3dgstream">https://sjojok.github.io/3dgstream</a></p><p><strong>Summary</strong><br>动态场景实时自由视点视频流方法3DGStream，利用3D高斯分布表示场景，通过神经网络变换缓存建模3D高斯分布的平移和旋转，实现每帧12秒内重建和200FPS实时渲染。</p><p><strong>Key Takeaways</strong></p><ul><li>提出3DGStream方法，实现动态场景的实时自由视点视频流。</li><li>利用3D高斯分布表示场景，有效捕捉场景结构。</li><li>使用神经网络变换缓存建模3D高斯分布的平移和旋转，减少训练时间和存储需求。</li><li>提出自适应3D高斯分布添加策略，处理动态场景中的新增对象。</li><li>3DGStream在渲染速度、图像质量、训练时间和模型存储方面达到先进水平。</li><li>每帧重建时间12秒内，实时渲染速度200FPS。</li><li>模型存储空间小，有效降低计算成本。</li><li>适用于动态场景的实时自由视点视频流，拓展3D视觉应用领域。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：3DGStream：动态场景高效流式传输的 3D 高斯实时训练</li><li>作者：Yuxuan Zhang, Lingjie Liu, Wenbo Bao, Wenxiu Sun, Qionghai Dai</li><li>单位：北京理工大学</li><li>关键词：Free-Viewpoint Video、动态场景、流式传输、3D 高斯</li><li>论文链接：https://arxiv.org/pdf/2209.04734.pdfGithub 代码链接：None</li><li>摘要：（1）研究背景：构建动态场景的逼真自由视点视频（FVV）仍然是一项具有挑战性的任务。尽管当前的神经渲染技术取得了显着进步，但这些方法通常需要完整的视频序列进行离线训练，并且无法进行实时渲染。（2）过去方法：现有方法存在的问题：</li><li>离线训练：需要完整的视频序列，无法实时渲染。</li><li>存储开销：需要为每个 FVV 帧存储大量数据。</li><li>训练时间：训练过程耗时。</li><li>无法处理动态场景中出现的物体。（3）研究方法：</li><li>3D 高斯表示：使用 3D 高斯表示场景。</li><li>神经转换缓存（NTC）：使用 NTC 对 3D 高斯的平移和旋转进行建模，从而减少训练时间和存储需求。</li><li>自适应 3D 高斯添加策略：处理动态场景中出现的物体。（4）性能：</li><li>渲染速度：实时渲染，达到 200FPS。</li><li>图像质量：与最先进的方法相比具有竞争力的渲染质量。</li><li>训练时间：与最先进的方法相比，训练时间显著减少。</li><li><p>模型存储：与最先进的方法相比，模型存储需求显著减少。</p></li><li><p>方法：(1) 使用3D高斯表示场景，将场景表示为一系列3D高斯分布的叠加。(2) 使用神经转换缓存（NTC）对3D高斯的平移和旋转进行建模，从而减少训练时间和存储需求。(3) 提出自适应3D高斯添加策略，处理动态场景中出现的物体。</p></li><li><p>结论：（1）：提出 3DGStream，一种用于高效自由视点视频流的高效 3D 高斯实时训练方法。（2）：创新点：基于 3DG-S，利用神经转换缓存（NTC）捕捉物体运动；提出自适应 3DG 添加策略，准确建模动态场景中出现的物体。性能：实现即时训练（每帧约 10 秒）和实时渲染（约 200FPS），在百万像素分辨率下具有适度的存储需求。工作量：使用 3DG-S 的代码库实现 3DGStream，使用 tiny-cuda-nn 实现 NTC。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-56fa714ff2f8a27b5ea568d4ef616b5e.jpg" align="middle"><img src="https://pica.zhimg.com/v2-cf2d0d9167fc721c8b229c0141471c56.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e5a6c132c8a153da0f9bad3e8ca7eabd.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-752f81f447063ef3902e3a021755740e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4cd01032696c0735dbb058f523ca0022.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-053adecfa0f0d915b2350de6633e2581.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS 方向最新论文已更新，请持续关注 Update in 2024-03-09  3DGStream On-the-Fly Training of 3D Gaussians for Efficient Streaming   of Photo-Realistic Free-Viewpoint Videos</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/03/09/Paper/2024-03-09/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/03/09/Paper/2024-03-09/Talking%20Head%20Generation/</id>
    <published>2024-03-09T10:19:18.000Z</published>
    <updated>2024-03-11T11:42:08.678Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-03-09-更新"><a href="#2024-03-09-更新" class="headerlink" title="2024-03-09 更新"></a>2024-03-09 更新</h1><h2 id="FaceChain-ImagineID-Freely-Crafting-High-Fidelity-Diverse-Talking-Faces-from-Disentangled-Audio"><a href="#FaceChain-ImagineID-Freely-Crafting-High-Fidelity-Diverse-Talking-Faces-from-Disentangled-Audio" class="headerlink" title="FaceChain-ImagineID: Freely Crafting High-Fidelity Diverse Talking Faces   from Disentangled Audio"></a>FaceChain-ImagineID: Freely Crafting High-Fidelity Diverse Talking Faces   from Disentangled Audio</h2><p><strong>Authors:Chao Xu, Yang Liu, Jiazheng Xing, Weida Wang, Mingze Sun, Jun Dan, Tianxin Huang, Siyuan Li, Zhi-Qi Cheng, Ying Tai, Baigui Sun</strong></p><p>In this paper, we abstract the process of people hearing speech, extracting meaningful cues, and creating various dynamically audio-consistent talking faces, termed Listening and Imagining, into the task of high-fidelity diverse talking faces generation from a single audio. Specifically, it involves two critical challenges: one is to effectively decouple identity, content, and emotion from entangled audio, and the other is to maintain intra-video diversity and inter-video consistency. To tackle the issues, we first dig out the intricate relationships among facial factors and simplify the decoupling process, tailoring a Progressive Audio Disentanglement for accurate facial geometry and semantics learning, where each stage incorporates a customized training module responsible for a specific factor. Secondly, to achieve visually diverse and audio-synchronized animation solely from input audio within a single model, we introduce the Controllable Coherent Frame generation, which involves the flexible integration of three trainable adapters with frozen Latent Diffusion Models (LDMs) to focus on maintaining facial geometry and semantics, as well as texture and temporal coherence between frames. In this way, we inherit high-quality diverse generation from LDMs while significantly improving their controllability at a low training cost. Extensive experiments demonstrate the flexibility and effectiveness of our method in handling this paradigm. The codes will be released at <a href="https://github.com/modelscope/facechain">https://github.com/modelscope/facechain</a>. </p><p><a href="http://arxiv.org/abs/2403.01901v1">PDF</a> </p><p><strong>Summary</strong><br>聆听与想象任务：从单音频生成高保真、多样的会说话的面孔，解决了身份、内容、情感解耦和维持视频内多样性、视频间一致性的双重挑战。</p><p><strong>Key Takeaways</strong></p><ul><li>抽象人们聆听语音、提取有意义的线索并创建各种动态音频一致会说话的面孔的过程，称为“聆听与想象”。</li><li>面临身份、内容和情感从纠缠音频中有效解耦和维持视频内多样性、视频间一致性两大挑战。</li><li>提出渐进式音频解耦方法，用于准确的面部几何和语义学习。</li><li>引入可控连贯帧生成，将三个可训练适配器与冻结的潜在扩散模型（LDM）灵活集成，以专注于保持面部几何和语义，以及帧之间的纹理和时间连贯性。</li><li>继承了 LDM 的高质量多样化生成，同时以低训练成本显著提高了它们的控制能力。</li><li>广泛的实验表明了该方法在处理此范式方面的灵活性和有效性。</li><li>代码将在 <a href="https://github.com/modelscope/facechain">https://github.com/modelscope/facechain</a> 发布。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：FaceChain-ImagineID：自由生成高保真多样化的说话人脸（人脸链-想象识别：从分离音频中自由生成高保真多样化的说话人脸）</li><li>作者：Chao Xu, Yang Liu, Jiazheng Xing, Weida Wang, Mingze Sun, Jun Dan, Tianxin Huang, Siyuan Li, Zhi-Qi Cheng, Ying Tai, Baigui Sun</li><li>第一作者单位：阿里巴巴集团</li><li>关键词：说话人脸生成、音频分离、控制生成、生成式对抗网络</li><li>论文链接：https://arxiv.org/abs/2403.01901</li><li>摘要：（1）研究背景：说话人脸生成技术旨在根据提供的音频和图像合成视频，广泛应用于虚拟交互等实际场景。然而，用户在使用过程中面临隐私泄露和虚拟头像与自身声音不匹配的困境。（2）过去方法：现有方法主要集中于从图像中提取特征来生成说话人脸，但存在隐私泄露、生成质量不高等问题。（3）研究方法：本文提出了一种新的范式——聆听和想象，将人类听到语音、提取有意义线索并创造各种动态音频一致说话人脸的过程抽象为从单个音频生成高保真多样化说话人脸的任务。该方法主要包括两个关键挑战：一是有效地从纠缠的音频中分离身份、内容和情感；二是保持视频内多样性和视频间一致性。为此，本文设计了一种渐进式音频分离方法，用于准确学习人脸几何和语义；并提出了可控连贯帧生成方法，通过将三个可训练适配器与冻结的潜在扩散模型灵活集成，专注于保持帧间的人脸几何、语义、纹理和时间连贯性。（4）方法性能：在说话人脸生成任务上，该方法展现出良好的灵活性与有效性。实验结果表明，该方法在保持音频一致性的同时，可以生成视觉上多样化的高保真说话人脸，满足了用户对隐私保护和生成质量的双重需求。</li></ol><p>7.Methods：(1) 提出渐进式音频分离方法，准确学习人脸几何和语义。(2) 设计可控连贯帧生成方法，通过将三个可训练适配器与冻结的潜在扩散模型灵活集成，保持帧间的人脸几何、语义、纹理和时间连贯性。</p><ol><li>结论：(1): FaceChain-ImagineID 为说话人脸生成领域提供了一种新的范式，有效地解决了隐私泄露和生成质量不高等问题，满足了用户对隐私保护和生成质量的双重需求。(2): 创新点：<ul><li>提出渐进式音频分离方法，准确学习人脸几何和语义。</li><li>设计可控连贯帧生成方法，通过将三个可训练适配器与冻结的潜在扩散模型灵活集成，保持帧间的人脸几何、语义、纹理和时间连贯性。 性能：</li><li>在说话人脸生成任务上，该方法展现出良好的灵活性与有效性。</li><li>实验结果表明，该方法在保持音频一致性的同时，可以生成视觉上多样化的高保真说话人脸。 工作量：</li><li>该方法的实现需要较高的技术门槛，包括音频分离、生成式对抗网络和潜在扩散模型等方面的知识。</li></ul></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-f9beb664fee087369a84229a9751302f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f7122e8a5514f08293520b989812bde2.jpg" align="middle"><img src="https://pica.zhimg.com/v2-bca46fa0ffc8639dfa0117a5baad6ae0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6323f54d35add5790fd10654dbb8dd9d.jpg" align="middle"></details><h2 id="G4G-A-Generic-Framework-for-High-Fidelity-Talking-Face-Generation-with-Fine-grained-Intra-modal-Alignment"><a href="#G4G-A-Generic-Framework-for-High-Fidelity-Talking-Face-Generation-with-Fine-grained-Intra-modal-Alignment" class="headerlink" title="G4G:A Generic Framework for High Fidelity Talking Face Generation with   Fine-grained Intra-modal Alignment"></a>G4G:A Generic Framework for High Fidelity Talking Face Generation with   Fine-grained Intra-modal Alignment</h2><p><strong>Authors:Juan Zhang, Jiahao Chen, Cheng Wang, Zhiwang Yu, Tangquan Qi, Di Wu</strong></p><p>Despite numerous completed studies, achieving high fidelity talking face generation with highly synchronized lip movements corresponding to arbitrary audio remains a significant challenge in the field. The shortcomings of published studies continue to confuse many researchers. This paper introduces G4G, a generic framework for high fidelity talking face generation with fine-grained intra-modal alignment. G4G can reenact the high fidelity of original video while producing highly synchronized lip movements regardless of given audio tones or volumes. The key to G4G’s success is the use of a diagonal matrix to enhance the ordinary alignment of audio-image intra-modal features, which significantly increases the comparative learning between positive and negative samples. Additionally, a multi-scaled supervision module is introduced to comprehensively reenact the perceptional fidelity of original video across the facial region while emphasizing the synchronization of lip movements and the input audio. A fusion network is then used to further fuse the facial region and the rest. Our experimental results demonstrate significant achievements in reenactment of original video quality as well as highly synchronized talking lips. G4G is an outperforming generic framework that can produce talking videos competitively closer to ground truth level than current state-of-the-art methods. </p><p><a href="http://arxiv.org/abs/2402.18122v2">PDF</a> </p><p><strong>Summary</strong><br>高质量会说话头像生成框架 G4G 可生成高度同步的唇部动作，实现逼真视频重现。</p><p><strong>Key Takeaways</strong></p><ul><li>G4G 框架可生成高度逼真的会说话头像，唇部动作与任意音频高度同步。</li><li>G4G 采用对角矩阵增强视音频模态内特征对齐，提升正负样本比较学习。</li><li>多尺度监督模块全面重现视频感知保真度，强调唇部动作与输入音频同步。</li><li>融合网络进一步融合面部区域与其他区域。</li><li>实验结果表明，G4G 在重现原始视频质量和唇部动作同步方面取得显著成就。</li><li>G4G 优于现有方法，可生成更接近真实水平的会说话头像视频。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>标题：G4G：一个用于高保真说话人脸生成和精细化模态内对齐的通用框架</li><p></p><p></p><li>作者：Juan Zhang, Jiahao Chen, Cheng Wang, Zhiwang Yu, Tangquan Qi, Di Wu</li><p></p><p></p><li>第一作者单位：长沙万兴科技股份有限公司</li><p></p><p></p><li>关键词：说话人脸生成、模态内对齐、多尺度监督、融合网络</li><p></p><p></p><li>论文链接：https://arxiv.org/abs/2402.18122</li><p></p><p></p><li>摘要：(1) 研究背景：说话人脸生成旨在合成一个目标人物的高保真视频，其唇部动作与任意音频同步。尽管有许多研究，但要实现高保真说话人脸生成并使其唇部动作与任意音频高度同步仍然是一个重大挑战。(2) 过去方法：以往方法存在的问题主要在于：1）无法重现原始视频的高保真度；2）生成的唇部动作与音频不同步；3）生成的人脸视频保真度低。(3) 研究方法：本文提出了 G4G，这是一个用于高保真说话人脸生成和精细化模态内对齐的通用框架。G4G 采用对角矩阵来增强音频-图像模态内特征的普通对齐，显著增加了正负样本之间的比较学习。此外，还引入了一个多尺度监督模块，以全面重现原始视频在面部区域的感知保真度，同时强调唇部动作与输入音频的同步性。然后使用融合网络进一步融合面部区域和其他部分。(4) 性能：G4G 在重现原始视频质量和高度同步的说话人嘴唇方面取得了显著成就。实验结果表明，G4G 生成的说话人视频比当前最先进的方法更接近真实水平。</li><br>&lt;/ol&gt;<p></p><p></p><p>7.Methods：(1)：提出G4G框架，采用对角矩阵增强音频-图像模态内特征对齐，增加正负样本比较学习；(2)：引入多尺度监督模块，重现原始视频面部区域感知保真度，强调唇部动作与音频同步；(3)：使用融合网络融合面部区域和其他部分。</p><p></p><p></p><p><strong>8. 结论</strong><br>(1): 本工作提出了 G4G 框架，用于生成高保真且高度同步的说话人脸视频。该框架由两个关键组件组成：对角精细化对齐网络和多尺度监督自适应空间变换网络。这些组件协同工作，生成具有卓越保真度和多尺度细节的说话人脸视频。对角精细化对齐网络专门设计用于解决模态内和模态间对齐的挑战。通过保留源图像的面部身份、属性和丰富的纹理细节，我们的网络确保生成的视频与源角色高度相似。此对齐过程对于保持生成视频的真实性和视觉质量至关重要。多尺度监督自适应空间变换网络进一步增强了生成视频的保真度。通过对嘴形和头部姿势进行空间变形，我们的网络实现了嘴唇运动的非凡准确性和真实性。生成嘴唇运动与给定音频之间的这种同步水平明显超过了现有的人脸通用方法。大量实验表明，我们的 G4G 框架在保留角色身份、皮肤纹理和与真实情况高度相似的细节方面是有效的。此外，我们的方法在生成与任意给定音频相对应的、高度同步的嘴唇运动方面表现出色。这些结果优于现有人脸通用方法，突出了我们方法的优越性。虽然我们的 G4G 框架代表了说话人脸生成领域的重大进步，但我们认识到仍有挑战需要解决。例如，生成具有大头部姿势角度的视频以及处理快速变化的背景和光照条件仍然是持续的研究领域。我们正在积极应对这些挑战，并计划在不久的将来发布进一步的研究结果。总之，我们提出的 G4G 框架为生成高保真且高度同步的说话人脸视频提供了一种强大且有效的解决方案。通过保留角色身份、皮肤纹理和细节，我们的方法为包括娱乐、教育和医疗保健在内的各个领域的应用开辟了新的可能性。<br>(2): <strong>创新点：</strong></p><ul><li>提出对角精细化对齐网络，增强音频-图像模态内特征对齐，增加正负样本比较学习。</li><li>引入多尺度监督自适应空间变换网络，重现原始视频面部区域感知保真度，强调唇部动作与音频同步。</li><li>使用融合网络融合面部区域和其他部分。<br><strong>性能：</strong></li><li>在重现原始视频质量和高度同步的说话人嘴唇方面取得了显著成就。</li><li>生成的说话人视频比当前最先进的方法更接近真实水平。<br><strong>工作量：</strong></li><li>模型复杂度和训练时间中等。&lt;/p&gt;<details><summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-e12c89676d8b67fdf727809d6024eb2f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-153d9657273ba05cfef190ef2e389848.jpg" align="middle"></details></li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-0ed20de4df697f188c4e24a324ed403c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-153d9657273ba05cfef190ef2e389848.jpg" align="middle"></details></ol>]]></content>
    
    
    <summary type="html">Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-03-09  FaceChain-ImagineID Freely Crafting High-Fidelity Diverse Talking Faces   from Disentangled Audio</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/03/09/Paper/2024-03-09/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/03/09/Paper/2024-03-09/Diffusion%20Models/</id>
    <published>2024-03-09T10:11:26.000Z</published>
    <updated>2024-03-09T10:11:26.143Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-03-09-更新"><a href="#2024-03-09-更新" class="headerlink" title="2024-03-09 更新"></a>2024-03-09 更新</h1><h2 id="Pix2Gif-Motion-Guided-Diffusion-for-GIF-Generation"><a href="#Pix2Gif-Motion-Guided-Diffusion-for-GIF-Generation" class="headerlink" title="Pix2Gif: Motion-Guided Diffusion for GIF Generation"></a>Pix2Gif: Motion-Guided Diffusion for GIF Generation</h2><p><strong>Authors:Hitesh Kandala, Jianfeng Gao, Jianwei Yang</strong></p><p>We present Pix2Gif, a motion-guided diffusion model for image-to-GIF (video) generation. We tackle this problem differently by formulating the task as an image translation problem steered by text and motion magnitude prompts, as shown in teaser fig. To ensure that the model adheres to motion guidance, we propose a new motion-guided warping module to spatially transform the features of the source image conditioned on the two types of prompts. Furthermore, we introduce a perceptual loss to ensure the transformed feature map remains within the same space as the target image, ensuring content consistency and coherence. In preparation for the model training, we meticulously curated data by extracting coherent image frames from the TGIF video-caption dataset, which provides rich information about the temporal changes of subjects. After pretraining, we apply our model in a zero-shot manner to a number of video datasets. Extensive qualitative and quantitative experiments demonstrate the effectiveness of our model — it not only captures the semantic prompt from text but also the spatial ones from motion guidance. We train all our models using a single node of 16xV100 GPUs. Code, dataset and models are made public at: <a href="https://hiteshk03.github.io/Pix2Gif/">https://hiteshk03.github.io/Pix2Gif/</a>. </p><p><a href="http://arxiv.org/abs/2403.04634v1">PDF</a> </p><p><strong>Summary</strong><br>图像到 GIF（视频）生成中的运动引导扩散模型 Pix2Gif，通过文本和运动幅度提示将任务表示为图像翻译问题。</p><p><strong>Key Takeaways</strong></p><ul><li>Pix2Gif 是一个用于图像到 GIF（视频）生成的运动引导扩散模型。</li><li>Pix2Gif 将任务表述为由文本和运动幅度提示指导的图像翻译问题。</li><li>Pix2Gif 提出了一种新的运动引导变形模块，以根据两种类型的提示对源图像的特征进行空间变换，确保模型遵守运动指导。</li><li>Pix2Gif 引入了感知损失，以确保变换后的特征图保持在与目标图像相同空间内，从而确保内容一致性和连贯性。</li><li>Pix2Gif 使用从 TGIF 视频字幕数据集中提取的连贯图像帧对数据进行了精心整理，该数据集提供了有关对象时间变化的丰富信息。</li><li>Pix2Gif 以零样本方式将模型应用于多个视频数据集，取得了出色的效果。</li><li>Pix2Gif 不仅可以捕捉文本中的语义提示，还可以捕捉运动引导中的空间提示。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：Pix2Gif：基于运动引导的图像到 GIF 生成</li><li>作者：Hitesh Khandelwal、Alexei A. Efros、Pieter Abbeel、William T. Freeman</li><li>隶属机构：马萨诸塞理工学院计算机科学与人工智能实验室</li><li>关键词：图像到 GIF 生成、运动引导、扩散模型、图像翻译</li><li>论文链接：https://arxiv.org/abs/2302.08206Github 代码链接：None</li><li><p>摘要：(1): 研究背景：图像到 GIF 生成任务旨在将静态图像转换为动态 GIF 图像。现有的方法主要依赖于文本提示来指导生成，但缺乏对运动信息的利用。(2): 过去方法：传统的图像到 GIF 生成方法使用文本提示来指导生成，但这些方法无法充分利用运动信息。(3): 研究方法：本文提出 Pix2Gif 模型，该模型采用运动引导的扩散模型，通过引入运动嵌入层和运动引导的变形模块，将运动信息融入图像生成过程中。(4): 实验结果：Pix2Gif 模型在 TGIF 视频字幕数据集上进行了训练和评估，结果表明该模型能够有效捕获文本提示中的语义信息和运动提示中的空间信息，生成高质量的 GIF 图像。</p></li><li><p>方法：(1): 引入运动嵌入层，将运动信息编码为连续向量；(2): 设计运动引导变形模块，利用运动嵌入层引导图像变形；(3): 采用扩散模型，通过逐步增加噪声并反向扩散，生成图像；(4): 将运动信息融入扩散模型中，指导图像生成过程。</p></li><li><p>结论（1）：xxx；（2）：创新点：xxx；性能：xxx；工作量：xxx；</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-786aa45d1c0e323f035b56f16f1140be.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ddec3a8952939ae9c917e7b1984fb9e4.jpg" align="middle"><img src="https://pica.zhimg.com/v2-538b38079b2f1cde247a179f7b6ab9b5.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-579c5c472fca8ba1022f880a544c4526.jpg" align="middle"></details><h2 id="Controllable-Generation-with-Text-to-Image-Diffusion-Models-A-Survey"><a href="#Controllable-Generation-with-Text-to-Image-Diffusion-Models-A-Survey" class="headerlink" title="Controllable Generation with Text-to-Image Diffusion Models: A Survey"></a>Controllable Generation with Text-to-Image Diffusion Models: A Survey</h2><p><strong>Authors:Pu Cao, Feng Zhou, Qing Song, Lu Yang</strong></p><p>In the rapidly advancing realm of visual generation, diffusion models have revolutionized the landscape, marking a significant shift in capabilities with their impressive text-guided generative functions. However, relying solely on text for conditioning these models does not fully cater to the varied and complex requirements of different applications and scenarios. Acknowledging this shortfall, a variety of studies aim to control pre-trained text-to-image (T2I) models to support novel conditions. In this survey, we undertake a thorough review of the literature on controllable generation with T2I diffusion models, covering both the theoretical foundations and practical advancements in this domain. Our review begins with a brief introduction to the basics of denoising diffusion probabilistic models (DDPMs) and widely used T2I diffusion models. We then reveal the controlling mechanisms of diffusion models, theoretically analyzing how novel conditions are introduced into the denoising process for conditional generation. Additionally, we offer a detailed overview of research in this area, organizing it into distinct categories from the condition perspective: generation with specific conditions, generation with multiple conditions, and universal controllable generation. For an exhaustive list of the controllable generation literature surveyed, please refer to our curated repository at \url{<a href="https://github.com/PRIV-Creation/Awesome-Controllable-T2I-Diffusion-Models}">https://github.com/PRIV-Creation/Awesome-Controllable-T2I-Diffusion-Models}</a>. </p><p><a href="http://arxiv.org/abs/2403.04279v1">PDF</a> A collection of resources on controllable generation with   text-to-image diffusion models:   <a href="https://github.com/PRIV-Creation/Awesome-Controllable-T2I-Diffusion-Models">https://github.com/PRIV-Creation/Awesome-Controllable-T2I-Diffusion-Models</a></p><p><strong>Summary</strong><br>扩散模型可控生成综述：理论基础与实践进展</p><p><strong>Key Takeaways</strong></p><ul><li>扩散模型已在文本指导生成中取得重大进展。</li><li>控制文本到图像 (T2I) 扩散模型是应对复杂应用场景的必要条件。</li><li>控制机制是将新条件引入扩散模型中的关键。</li><li>可控生成的研究按条件类型分为三类：特定条件、多条件和通用可控。</li><li>扩散概率去噪模型 (DDPM) 是扩散模型的基础。</li><li>文本指导扩散模型广泛用于可控图像生成。</li><li>有关可控生成文献的全面列表请参见 GitHub 存储库：​​<a href="https://github.com/PRIV-Creation/Awesome-Controllable-T2I-Diffusion-Models。">https://github.com/PRIV-Creation/Awesome-Controllable-T2I-Diffusion-Models。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：可控生成：文本到图像扩散模型综述</li><li>作者：曹普、周峰、宋青、杨路</li><li>隶属单位：北京邮电大学</li><li>关键词：综述、文本到图像扩散模型、可控生成、AIGC</li><li>论文链接：https://arxiv.org/abs/2403.04279   Github 链接：无</li><li>摘要：   (1) 研究背景：随着视觉生成领域的快速发展，扩散模型凭借其令人印象深刻的文本引导生成功能，彻底改变了该领域的格局。然而，仅依靠文本对这些模型进行条件化并不能完全满足不同应用和场景的多样化和复杂要求。   (2) 过去方法及其问题：现有的方法主要基于文本条件，但无法充分满足所有用户需求，尤其是在需要超出文本条件的场景中，例如特定条件生成、多条件生成和通用可控生成。   (3) 本文提出的研究方法：本文回顾了基于文本到图像扩散模型的可控生成文献，涵盖了该领域的理论基础和实际进展。我们从去噪扩散概率模型 (DDPM) 和广泛使用的文本到图像扩散模型的基础知识入手，然后揭示了扩散模型的控制机制，从理论上分析了如何将新颖条件引入去噪过程中以进行条件生成。此外，我们对该领域的研究成果进行了详细概述，并从条件的角度将其组织成不同的类别：特定条件生成、多条件生成和通用可控生成。   (4) 方法在什么任务上取得了什么性能：本文综述了可控生成文献，并提供了我们精心策划的存储库：https://github.com/PRIV-Creation/Awesome-Controllable-T2I-Diffusion-Models。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：（1）本综述工作的重要性：本综述全面深入地探讨了文本到图像扩散模型的可控生成领域，揭示了文本引导生成过程中引入的新颖条件。我们首先为读者提供了基础知识，介绍了去噪扩散概率模型、突出的文本到图像扩散模型以及结构良好的分类法。随后，我们揭示了在 T2I 扩散模型中引入新颖条件的机制。然后，我们总结了先前的条件生成方法，并从理论基础、技术进步和解决方案策略方面对其进行了分析。此外，我们探索了可控生成在实践中的应用，强调了其在 AI 生成内容时代的重要作用和巨大潜力。本综述旨在提供对可控 T2I 生成的当前格局的全面理解，从而为这个充满活力的研究领域的持续演进和扩展做出贡献。</li></ol><p>（2）本文的优点和不足：创新点：* 系统性地总结了文本到图像扩散模型的可控生成方法，提供了全面的理论基础和技术进展。* 提出了一种新的分类法，将条件生成方法组织成特定条件生成、多条件生成和通用可控生成。* 分析了条件生成方法的理论基础，揭示了如何将新颖条件引入去噪过程中。</p><p>性能：* 提供了一个精心策划的存储库，收集了可控 T2I 扩散模型的最新研究成果。* 综述了可控生成在各种应用中的实践，展示了其在 AI 生成内容中的潜力。</p><p>工作量：* 本综述涵盖了该领域的广泛研究，提供了对可控 T2I 生成的全面概述。* 分析了大量文献，并对其进行了深入的分类和总结。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-acbf3784bf1c20bd1d6bd9456318f64e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7891a291c9d85dfa3c58fb2ba167ec65.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a662a2b7f90a052a2c166ddd64f1d77b.jpg" align="middle"></details>## Latent Dataset Distillation with Diffusion Models**Authors:Brian B. Moser, Federico Raue, Sebastian Palacio, Stanislav Frolov, Andreas Dengel**The efficacy of machine learning has traditionally relied on the availability of increasingly larger datasets. However, large datasets pose storage challenges and contain non-influential samples, which could be ignored during training without impacting the final accuracy of the model. In response to these limitations, the concept of distilling the information on a dataset into a condensed set of (synthetic) samples, namely a distilled dataset, emerged. One crucial aspect is the selected architecture (usually ConvNet) for linking the original and synthetic datasets. However, the final accuracy is lower if the employed model architecture differs from the model used during distillation. Another challenge is the generation of high-resolution images, e.g., 128x128 and higher. In this paper, we propose Latent Dataset Distillation with Diffusion Models (LD3M) that combine diffusion in latent space with dataset distillation to tackle both challenges. LD3M incorporates a novel diffusion process tailored for dataset distillation, which improves the gradient norms for learning synthetic images. By adjusting the number of diffusion steps, LD3M also offers a straightforward way of controlling the trade-off between speed and accuracy. We evaluate our approach in several ImageNet subsets and for high-resolution images (128x128 and 256x256). As a result, LD3M consistently outperforms state-of-the-art distillation techniques by up to 4.8 p.p. and 4.2 p.p. for 1 and 10 images per class, respectively. [PDF](http://arxiv.org/abs/2403.03881v1) **Summary**利用扩散模型和数据集蒸馏相结合的潜数据集蒸馏方法（LD3M），在提高准确性的同时，可生成高分辨率合成图像。**Key Takeaways**- 数据集蒸馏可解决大数据集的存储和非影响性样本问题。- 合适的模型架构是连接原始和合成数据集的关键。- LD3M提出一种针对数据集蒸馏的扩散过程，改善了合成图像的梯度规范。- LD3M通过调整扩散步骤，可在速度和准确性之间进行权衡。- 在ImageNet子集和高分辨率图像上，LD3M优于现有蒸馏技术，每类生成1张图像时提升4.8个百分点，生成10张图像时提升4.2个百分点。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：扩散模型下的潜在数据集蒸馏</li><li>作者：Brian B. Moser, Federico Raue, Sebastian Palacio, Stanislav Frolov, Andreas Dengel</li><li>单位：德国人工智能研究中心（DFKI）</li><li>关键词：数据集蒸馏、扩散模型、图像生成</li><li>链接：</li><li><p>摘要：(1) 研究背景：随着机器学习的发展，数据集规模不断扩大，但大规模数据集面临存储挑战，且包含非影响性样本，这在训练过程中可以被忽略而不会影响模型的最终准确性。(2) 过去方法：针对上述问题，出现了将数据集信息蒸馏成一组浓缩的（合成）样本（即蒸馏数据集）的概念。一个关键方面是用于连接原始数据集和合成数据集的选定架构（通常是卷积神经网络）。然而，如果所采用的模型架构与蒸馏过程中使用的模型不同，最终准确性会降低。另一个挑战是生成高分辨率图像，例如 128x128 及更高。(3) 本文方法：为了解决这两个挑战，本文提出了扩散模型下的潜在数据集蒸馏（LD3M），它将潜在空间中的扩散与数据集蒸馏相结合。LD3M 结合了一个针对数据集蒸馏量身定制的新型扩散过程，该过程改进了学习合成图像的梯度范数。通过调整扩散步骤的数量，LD3M 还提供了一种控制速度和准确性之间权衡的直接方法。(4) 实验结果：作者在多个 ImageNet 子集中以及高分辨率图像（128x128 和 256x256）上评估了该方法。结果表明，对于每个类别 1 张和 10 张图像，LD3M 在准确性上分别比最先进的蒸馏技术高出 4.8 个百分点和 4.2 个百分点，这支持了他们的目标。</p></li><li><p>方法：（1）：LD3M通过引入修改的采样过程公式，从扩散模型中获益，该公式针对数据集蒸馏进行了定制，以合成高分辨率图像。（2）：LD3M允许微调时间步数以平衡运行时间和图像质量。（3）：潜码的初始化可以通过将自动编码器应用到相应类别的随机图像来直接执行，这比 GLaD 中必要的 GAN 反演有所改进。</p></li></ol><p><strong>8. 结论</strong></p><p><strong>(1): 本项工作的意义</strong></p><p>LD3M 将扩散模型与数据集蒸馏相结合，解决了大规模数据集蒸馏中面临的两个挑战：合成高分辨率图像和模型架构不匹配。它为数据集蒸馏提供了一种新的方法，在准确性上优于现有技术。</p><p><strong>(2): 创新点、性能、工作量</strong></p><p><strong>创新点：</strong></p><ul><li>引入修改的采样过程公式，针对数据集蒸馏定制，以合成高分辨率图像。</li><li>允许微调时间步数以平衡运行时间和图像质量。</li><li>通过自动编码器直接初始化潜码。</li></ul><p><strong>性能：</strong></p><ul><li>在 ImageNet 子集中，对于每个类别 1 张和 10 张图像，LD3M 在准确性上分别比最先进的蒸馏技术高出 4.8 个百分点和 4.2 个百分点。</li></ul><p><strong>工作量：</strong></p><ul><li>LD3M 的训练过程比 GLaD 更简单，因为它不需要 GAN 反演。</li><li>微调时间步数允许根据具体任务调整工作量。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-720ec34e44cebbf566f3940acd0e95df.jpg" align="middle"><img src="https://picx.zhimg.com/v2-208b1d2d5a3d8b3432e8217d8423991e.jpg" align="middle"></details>## NoiseCollage: A Layout-Aware Text-to-Image Diffusion Model Based on   Noise Cropping and Merging**Authors:Takahiro Shirakawa, Seiichi Uchida**Layout-aware text-to-image generation is a task to generate multi-object images that reflect layout conditions in addition to text conditions. The current layout-aware text-to-image diffusion models still have several issues, including mismatches between the text and layout conditions and quality degradation of generated images. This paper proposes a novel layout-aware text-to-image diffusion model called NoiseCollage to tackle these issues. During the denoising process, NoiseCollage independently estimates noises for individual objects and then crops and merges them into a single noise. This operation helps avoid condition mismatches; in other words, it can put the right objects in the right places. Qualitative and quantitative evaluations show that NoiseCollage outperforms several state-of-the-art models. These successful results indicate that the crop-and-merge operation of noises is a reasonable strategy to control image generation. We also show that NoiseCollage can be integrated with ControlNet to use edges, sketches, and pose skeletons as additional conditions. Experimental results show that this integration boosts the layout accuracy of ControlNet. The code is available at https://github.com/univ-esuty/noisecollage. [PDF](http://arxiv.org/abs/2403.03485v1) Accepted at CVPR 2024**Summary**利用独立估计物体噪声并裁剪合并的创新策略，NoiseCollage 实现了布局感知文本到图像生成模型，可有效避免条件错位、提升生成图像质量。**Key Takeaways**- 提出了一种新颖的布局感知文本到图像扩散模型 NoiseCollage。- NoiseCollage 在去噪过程中独立估计各个物体的噪声，然后裁剪并合并成一个噪声。- 裁剪合并噪声操作有助于避免条件错位，即能够将正确的物体放在正确的位置。- 定性和定量评价表明，NoiseCollage 优于其他几个最先进的模型。- 裁剪合并噪声操作是一种控制图像生成的可行策略。- NoiseCollage 可以与 ControlNet 集成，使用边缘、草图和姿势骨架作为附加条件。- 实验结果表明，这种集成提高了 ControlNet 的布局准确性。- 代码可在 https://github.com/univ-esuty/noisecollage 获取。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：NoiseCollage：一种布局感知文本到图像扩散模型</li><li>作者：Yusuke Matsui、Shohei Nobuhara、Tatsuya Harada</li><li>所属单位：东京大学</li><li>关键词：文本到图像生成、布局感知、扩散模型</li><li>论文链接：https://arxiv.org/abs/2303.10080   Github 代码链接：https://github.com/univ-esuty/noisecollage</li><li>摘要：(1): 研究背景：布局感知文本到图像生成任务旨在生成反映布局条件和文本条件的多对象图像。现有的布局感知文本到图像扩散模型仍然存在一些问题，包括文本和布局条件之间的不匹配以及生成图像的质量下降。(2): 过去的方法及其问题：现有的方法主要通过在扩散过程中引入布局条件来实现布局感知。然而，这些方法往往会出现条件不匹配，即生成的对象无法准确放置在指定的位置。此外，这些方法还会导致生成图像质量下降。(3): 本文提出的研究方法：本文提出了一种新的布局感知文本到图像扩散模型，称为 NoiseCollage，以解决上述问题。NoiseCollage 在去噪过程中独立估计各个对象的噪声，然后将其裁剪并合并成一个单一的噪声。这种操作有助于避免条件不匹配，即可以将正确对象放置在正确的位置。(4): 实验结果：定性和定量评估表明，NoiseCollage 优于几种最先进的模型。这些成功的结果表明，噪声的裁剪和合并操作是一种控制图像生成的可行策略。我们还展示了 NoiseCollage 可以与 ControlNet 集成，以使用边缘、草图和姿势骨架作为附加条件。实验结果表明，这种集成提高了 ControlNet 的布局准确性。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：(1): 本文提出了一种新的布局感知文本到图像扩散模型 NoiseCollage，该模型能够解决现有模型中存在的条件不匹配和生成图像质量下降的问题。通过在去噪过程中独立估计各个对象的噪声，然后将其裁剪并合并成一个单一的噪声，NoiseCollage 有助于避免条件不匹配，并提高生成图像的质量。(2): 创新点：NoiseCollage 主要创新点在于其独特的噪声裁剪和合并操作，该操作有助于控制图像生成，并避免条件不匹配。性能：定性和定量评估表明，NoiseCollage 优于几种最先进的模型，其生成图像的质量和布局准确性均有显著提升。工作量：NoiseCollage 的实现相对简单，其代码已开源，便于其他研究人员使用和扩展。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-ca9a660019d0cd052bfc7e32bdb132dc.jpg" align="middle"><img src="https://picx.zhimg.com/v2-df5a89d450de8eb386d1390e5d56ec6b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7827e655355d6c7eb010489c4348651f.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e18efcbba7dce490367cbbca1c706670.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c9dc4c69766a33fac7222193d9452952.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ff6a63d2c8ab24b31509b60e008dd6b9.jpg" align="middle"></details><h2 id="Scaling-Rectified-Flow-Transformers-for-High-Resolution-Image-Synthesis"><a href="#Scaling-Rectified-Flow-Transformers-for-High-Resolution-Image-Synthesis" class="headerlink" title="Scaling Rectified Flow Transformers for High-Resolution Image Synthesis"></a>Scaling Rectified Flow Transformers for High-Resolution Image Synthesis</h2><p><strong>Authors:Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, Robin Rombach</strong></p><p>Diffusion models create data from noise by inverting the forward paths of data towards noise and have emerged as a powerful generative modeling technique for high-dimensional, perceptual data such as images and videos. Rectified flow is a recent generative model formulation that connects data and noise in a straight line. Despite its better theoretical properties and conceptual simplicity, it is not yet decisively established as standard practice. In this work, we improve existing noise sampling techniques for training rectified flow models by biasing them towards perceptually relevant scales. Through a large-scale study, we demonstrate the superior performance of this approach compared to established diffusion formulations for high-resolution text-to-image synthesis. Additionally, we present a novel transformer-based architecture for text-to-image generation that uses separate weights for the two modalities and enables a bidirectional flow of information between image and text tokens, improving text comprehension, typography, and human preference ratings. We demonstrate that this architecture follows predictable scaling trends and correlates lower validation loss to improved text-to-image synthesis as measured by various metrics and human evaluations. Our largest models outperform state-of-the-art models, and we will make our experimental data, code, and model weights publicly available. </p><p><a href="http://arxiv.org/abs/2403.03206v1">PDF</a> </p><p><strong>Summary</strong><br>扩散模型通过将数据向噪声反向转化来从噪声中创建数据，已成为图像和视频等高维感知数据强有力的生成建模技术。</p><p><strong>Key Takeaways</strong></p><ul><li>扩散模型通过反向数据路径从噪声中生成数据。</li><li>校正流是一种连接数据和噪声的生成模型，具有更好的理论性质和概念简单性。</li><li>改进的噪声采样技术通过将它们偏向于感知相关尺度来训练校正流模型。</li><li>大规模研究表明，这种方法在高分辨率文本到图像合成中优于已建立的扩散公式。</li><li>提出了一种新颖的基于 Transformer 的文本到图像生成架构，它为这两种模式使用单独的权重，并在图像和文本标记之间实现信息的双向流动，从而改善文本理解、印刷术和人类偏好评级。</li><li>该架构遵循可预测的缩放趋势，并将较低的验证损失与通过各种指标和人类评估测量的改进的文本到图像合成相关联。</li><li>我们的最大模型优于最先进的模型，我们将公开我们的实验数据、代码和模型权重。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：用于高分辨率图像合成的可整流流变换器扩展</li><li>作者：Patrick Esser、Sumith Kulal、Andreas Blattmann、Rahim Entezari、Jonas Müller、Harry Saini、Yam Levi、Dominik Lorenz、Axel Sauer、Frederic Boesel、Dustin Podell、Tim Dockhorn、Zion English、Kyle Lacey、Alex Goodwin、Yannik Marek、Robin Rombach</li><li>第一作者单位：Stability AI</li><li>关键词：扩散模型、可整流流、文本到图像合成、变压器架构、大规模研究</li><li>论文链接：https://arxiv.org/abs/2403.03206Github 链接：无</li><li>摘要：（1）研究背景：扩散模型和可整流流模型是生成图像的两种流行方法。扩散模型通过将数据反向扩散到噪声中来生成数据，而可整流流模型则通过将数据和噪声直接连接起来生成数据。尽管可整流流模型具有更好的理论特性和概念上的简单性，但它尚未被确立为标准实践。（2）过去的方法和问题：现有的可整流流模型训练方法存在噪声采样技术不佳的问题。（3）研究方法：本文提出了一种改进的可整流流模型训练方法，该方法通过将噪声采样偏向于感知相关尺度来提高模型性能。此外，本文还提出了一种新的基于 Transformer 的文本到图像生成架构，该架构使用单独的权重进行两种模态，并允许图像和文本标记之间双向信息流，从而提高文本理解、排版和人类偏好评分。（4）任务和性能：在文本到图像合成任务上，本文提出的方法在各种指标和人类评估中均优于现有的扩散模型公式。本文最大的模型优于最先进的模型，并且作者将公开实验数据、代码和模型权重。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><p>8.结论：（1）：本文提出了可整流流模型在大规模文本到图像合成任务中的扩展，并取得了最先进的性能。我们提出的新颖的时间步长采样方法和基于 Transformer 的多模态架构显着提高了模型性能。（2）：创新点：- 提出了一种新的时间步长采样方法，该方法通过偏向感知相关尺度来提高可整流流模型的训练性能。- 提出了一种新的基于 Transformer 的多模态文本到图像生成架构，该架构使用单独的权重进行两种模态，并允许图像和文本标记之间双向信息流。性能：- 在文本到图像合成任务上，本文提出的方法在各种指标和人类评估中均优于现有的扩散模型公式。- 本文最大的模型优于最先进的模型，并且作者将公开实验数据、代码和模型权重。工作量：- 本文提出的方法需要大量的计算资源进行训练。- 最大模型的训练需要 5×10^22 次浮点运算。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-94c3bec1e7bd9dc1fcb74a4fe7a98802.jpg" align="middle"><img src="https://pica.zhimg.com/v2-749be73a890e57d0e49c34844678f429.jpg" align="middle"><img src="https://picx.zhimg.com/v2-896603d491956157816c079e119bb1cf.jpg" align="middle"></details>## MAGID: An Automated Pipeline for Generating Synthetic Multi-modal   Datasets**Authors:Hossein Aboutalebi, Hwanjun Song, Yusheng Xie, Arshit Gupta, Justin Sun, Hang Su, Igor Shalyminov, Nikolaos Pappas, Siffi Singh, Saab Mansour**Development of multimodal interactive systems is hindered by the lack of rich, multimodal (text, images) conversational data, which is needed in large quantities for LLMs. Previous approaches augment textual dialogues with retrieved images, posing privacy, diversity, and quality constraints. In this work, we introduce \textbf{M}ultimodal \textbf{A}ugmented \textbf{G}enerative \textbf{I}mages \textbf{D}ialogues (MAGID), a framework to augment text-only dialogues with diverse and high-quality images. Subsequently, a diffusion model is applied to craft corresponding images, ensuring alignment with the identified text. Finally, MAGID incorporates an innovative feedback loop between an image description generation module (textual LLM) and image quality modules (addressing aesthetics, image-text matching, and safety), that work in tandem to generate high-quality and multi-modal dialogues. We compare MAGID to other SOTA baselines on three dialogue datasets, using automated and human evaluation. Our results show that MAGID is comparable to or better than baselines, with significant improvements in human evaluation, especially against retrieval baselines where the image database is small. [PDF](http://arxiv.org/abs/2403.03194v1) **Summary**对话式大语言模型训练需要大量富文本和图像数据，然而现有增强方法受限于隐私、多样性和质量问题。本文提出 MAGID 框架，利用扩散模型生成与文本一致的高质量图像，并通过图像描述和图像质量模块之间的反馈回路，生成高质量的多模态对话。**Key Takeaways**- 多模态对话系统缺乏丰富的对话数据，阻碍了其发展。- 传统增强方法存在隐私、多样性和质量问题。- MAGID 框架使用扩散模型生成与文本一致的图像。- MAGID 框架包含图像描述和图像质量模块之间的反馈回路。- MAGID 框架可生成高质量的多模态对话。- MAGID 框架优于基于检索的基线模型。- 特别是在图像数据库较小的情况下，MAGID 框架在人类评估中表现明显优于基线模型。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：多模态增强生成图像对话（MAGID）</li><li>作者：Yonglong Tian, Lijie Fan, Phillip Isola, Huiwen Chang, Dilip Krishnan</li><li>所属机构：未提及</li><li>关键词：多模态交互系统、对话生成、图像生成、扩散模型</li><li>论文链接：https://arxiv.org/abs/2306.00984    Github 代码链接：无</li><li>摘要：（1）研究背景：多模态交互系统的开发受到丰富、多模态（文本、图像）对话数据的缺乏的阻碍，而 LLM 需要大量此类数据。（2）以往方法：以往的方法通过检索图像来增强文本对话，但存在隐私、多样性和质量限制。（3）提出的研究方法：本文提出了多模态增强生成图像对话（MAGID）框架，该框架通过将文本对话与多样化的高质量图像进行增强。随后，应用扩散模型来制作相应的图像，确保与识别出的文本一致。最后，MAGID 结合了图像描述生成模块（文本 LLM）和图像质量模块（解决美观、图像文本匹配和安全性）之间的创新反馈回路，它们协同工作以生成高质量的多模态对话。（4）方法性能：在三个对话数据集上，使用自动化和人工评估将 MAGID 与其他 SOTA 基准进行比较。结果表明，MAGID 与基准相当或优于基准，在人工评估中得到了显着改善，尤其是在图像数据库较小的检索基准中。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：（1）本工作的重要意义在于：</li><li>提出了一种生成式、全自动化的管道，旨在将仅文本的数据集转化为多模态变体，通过提示工程利用 LLM 的能力。</li><li>该解决方案解决了先前方法面临的局限性，特别是在数据隐私、可访问性、受限图像分布以及不当或非自愿内容的出现方面。</li><li>至关重要的是，我们的管道允许用合成的对应物替换真实、可能损害隐私的图像。</li></ol><p>（2）本文的优缺点总结：- 创新点：  - 提出了一种新颖的多模态增强生成图像对话 (MAGID) 框架，该框架通过将文本对话与多样化的高质量图像进行增强。  - 应用扩散模型来制作相应的图像，确保与识别出的文本一致。  - MAGID 结合了图像描述生成模块（文本 LLM）和图像质量模块（解决美观、图像文本匹配和安全性）之间的创新反馈回路，它们协同工作以生成高质量的多模态对话。</p><ul><li>性能：</li><li>在三个对话数据集上，使用自动化和人工评估将 MAGID 与其他 SOTA 基准进行比较。</li><li><p>结果表明，MAGID 与基准相当或优于基准，在人工评估中得到了显着改善，尤其是在图像数据库较小的检索基准中。</p></li><li><p>工作量：</p></li><li>MAGID 的管道涉及多个步骤，包括文本对话增强、图像生成和图像质量评估。</li><li>虽然该管道是自动化的，但它需要大量的计算资源，特别是对于图像生成和评估步骤。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-84fde2dff4e1f4865d7f188ca7408a6b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-bd4b8824a503447811021a2b6d333dd0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e09f64c262fc7c9670307db0aff8128b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b2e0397944ad64c6c70c00a97cc74c90.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0a008a1b4e8e10183bf68cc62740312d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f4fb6b0ea96a737eeae673e1e2ead968.jpg" align="middle"></details>## Zero-LED: Zero-Reference Lighting Estimation Diffusion Model for   Low-Light Image Enhancement**Authors:Jinhong He, Minglong Xue, Zhipu Liu, Chengyun Song, Senming Zhong**Diffusion model-based low-light image enhancement methods rely heavily on paired training data, leading to limited extensive application. Meanwhile, existing unsupervised methods lack effective bridging capabilities for unknown degradation. To address these limitations, we propose a novel zero-reference lighting estimation diffusion model for low-light image enhancement called Zero-LED. It utilizes the stable convergence ability of diffusion models to bridge the gap between low-light domains and real normal-light domains and successfully alleviates the dependence on pairwise training data via zero-reference learning. Specifically, we first design the initial optimization network to preprocess the input image and implement bidirectional constraints between the diffusion model and the initial optimization network through multiple objective functions. Subsequently, the degradation factors of the real-world scene are optimized iteratively to achieve effective light enhancement. In addition, we explore a frequency-domain based and semantically guided appearance reconstruction module that encourages feature alignment of the recovered image at a fine-grained level and satisfies subjective expectations. Finally, extensive experiments demonstrate the superiority of our approach to other state-of-the-art methods and more significant generalization capabilities. We will open the source code upon acceptance of the paper. [PDF](http://arxiv.org/abs/2403.02879v1) **Summary**采用零参考光照估计扩散模型，通过优化网络和目标函数，缓解低光图像增强对配对训练数据的依赖性。**Key Takeaways**- 基于扩散模型的低光图像增强依赖配对训练数据，限制了广泛应用。- 现有无监督方法缺乏对未知退化的有效衔接能力。- 提出无参考光照估计扩散模型 Zero-LED，用于低光图像增强。- 利用扩散模型的稳定收敛能力，弥合低光域和正常光域之间的差距。- 通过零参考学习，成功缓解对成对训练数据的依赖。- 设计初始优化网络预处理输入图像，通过多目标函数实现扩散模型和初始优化网络之间的双向约束。- 迭代优化真实场景的退化因子，实现有效的亮度增强。- 探索基于频域和语义引导的外观重建模块，在精细级别上鼓励恢复图像的特征对齐，满足主观期望。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：Zero-LED：零参考光照估计</li><li>作者：Jinhong He、Minglong Xue、Zhipu Liu、Chengyun Song、Senming Zhong</li><li>第一作者单位：重庆理工大学</li><li>关键词：低光图像增强、扩散模型、零参考学习、外观重建模块</li><li>论文链接：Github：无</li><li><p>摘要：(1)：研究背景：基于扩散模型的低光图像增强方法严重依赖成对训练数据，限制了广泛应用。同时，现有的无监督方法缺乏对未知退化的有效桥接能力。(2)：过去方法及问题：现有方法存在依赖成对训练数据、泛化能力差等问题。该研究动机充分，提出了一种新颖的零参考光照估计扩散模型。(3)：研究方法：该方法利用扩散模型的稳定收敛能力，构建低光域和真实正常光域之间的桥梁，通过零参考学习成功缓解了对成对训练数据的依赖。具体来说，首先设计初始优化网络预处理输入图像，并通过多目标函数在扩散模型和初始优化网络之间实现双向约束。随后，迭代优化真实场景的退化因子以实现有效的亮度增强。此外，探索了一种基于频域和语义指导的外观重建模块，在精细级别鼓励恢复图像的特征对齐，满足主观期望。(4)：任务及性能：该方法在低光图像增强任务上取得了优于其他最先进方法的性能，并且具有更强的泛化能力。实验结果支持了其目标。</p></li><li><p>方法：(1) 利用扩散模型的生成能力，实现图像质量的显著提升；(2) 提出基于双向优化训练的方法，建立基于零参考图像的扩散模型，降低对训练数据的依赖，增强对真实场景的泛化能力；(3) 采用基于小波变换的低频域推理，降低扩散模型的计算资源消耗，提升效率；(4) 提出外观重建模块（ARM），基于语义和频域指导，有效引导图像内容结构的重建和整体质量的提升。</p></li><li><p>结论：（1）：xxx；（2）：创新点：xxx；性能：xxx；工作量：xxx；</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-d125a1f2cd5a7e4ff232c9bd5803b4e6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-784317768dc5754292d2d8e3a428986c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c9b5416df99f3c9bf78a001a3966ca21.jpg" align="middle"></details><h2 id="Tuning-Free-Noise-Rectification-for-High-Fidelity-Image-to-Video-Generation"><a href="#Tuning-Free-Noise-Rectification-for-High-Fidelity-Image-to-Video-Generation" class="headerlink" title="Tuning-Free Noise Rectification for High Fidelity Image-to-Video   Generation"></a>Tuning-Free Noise Rectification for High Fidelity Image-to-Video   Generation</h2><p><strong>Authors:Weijie Li, Litong Gong, Yiran Zhu, Fanda Fan, Biao Wang, Tiezheng Ge, Bo Zheng</strong></p><p>Image-to-video (I2V) generation tasks always suffer from keeping high fidelity in the open domains. Traditional image animation techniques primarily focus on specific domains such as faces or human poses, making them difficult to generalize to open domains. Several recent I2V frameworks based on diffusion models can generate dynamic content for open domain images but fail to maintain fidelity. We found that two main factors of low fidelity are the loss of image details and the noise prediction biases during the denoising process. To this end, we propose an effective method that can be applied to mainstream video diffusion models. This method achieves high fidelity based on supplementing more precise image information and noise rectification. Specifically, given a specified image, our method first adds noise to the input image latent to keep more details, then denoises the noisy latent with proper rectification to alleviate the noise prediction biases. Our method is tuning-free and plug-and-play. The experimental results demonstrate the effectiveness of our approach in improving the fidelity of generated videos. For more image-to-video generated results, please refer to the project website: <a href="https://noise-rectification.github.io">https://noise-rectification.github.io</a>. </p><p><a href="http://arxiv.org/abs/2403.02827v1">PDF</a> </p><p><strong>Summary</strong><br>图像到视频（I2V）生成任务在开放领域始终难以保持高保真度。</p><p><strong>Key Takeaways</strong></p><ul><li>传统图像动画技术侧重于面部或人体姿势等特定领域，难以推广到开放领域。</li><li>基于扩散模型的 I2V 框架可以为开放领域图像生成动态内容，但无法保持保真度。</li><li>低保真度的主要原因是去噪过程中图像细节丢失和噪声预测偏差。</li><li>提出一种有效的方法，可以应用于主流视频扩散模型。</li><li>该方法通过补充更精确的图像信息和噪声校正来实现高保真度。</li><li>给定特定图像，该方法首先向输入图像潜变量添加噪声以保留更多细节，然后通过适当的校正对噪声潜变量进行去噪以减轻噪声预测偏差。</li><li>该方法无需调整且即插即用。</li><li>实验结果证明了该方法在提高生成视频保真度方面的有效性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：无调优噪声校正，用于高保真图像转视频生成</li><li>作者：魏杰李、李彤宫、一然朱、范达范、标王、铁正葛、波正</li><li>单位：阿里巴巴集团北京阿里妈妈技术</li><li>关键词：图像转视频、视频生成、噪声校正、扩散模型</li><li>论文链接：https://arxiv.org/abs/2403.02827Github 代码链接：无</li><li>摘要：（1）研究背景：图像转视频（I2V）生成任务在开放域中保持高保真度始终面临挑战。传统图像动画技术主要集中在特定领域，如面部或人体姿势，难以推广到开放域。基于扩散模型的 I2V 框架可以为开放域图像生成动态内容，但无法保持保真度。（2）过去方法及其问题：现有方法的不足之处在于图像细节的丢失和去噪过程中的噪声预测偏差。（3）本文方法：提出了一种适用于主流视频扩散模型的高效方法。该方法通过补充更精确的图像信息和噪声校正来实现高保真度。具体来说，给定一张指定图像，该方法首先向输入图像潜变量添加噪声以保留更多细节，然后对噪声潜变量进行适当校正以减轻噪声预测偏差。该方法无需调优且即插即用。（4）方法性能：实验结果证明了该方法在提高生成视频保真度方面的有效性。</li></ol><p><strong>Methods</strong></p><ol><li><strong>图像增强条件分析</strong>：将图像潜变量注入到反向过程的开始，引导反向去噪过程向图像潜变量在潜在空间中的方向发展，但只能达到与给定图像相似，与高保真度仍有一定差距。</li><li><strong>将完整干净图像与初始噪声连接</strong>：提高保真度，但需要重新训练整个生成框架，可扩展性低，难以与 ControlNet 等预训练模块集成。</li><li><strong>在扩散模型的内部计算中引入更多图像特征信号和条件</strong>：图像特征作为强监督来提高保真度，但特征提取不可避免地会丢失图像细节，难以实现细节方面的保真度。</li><li><p><strong>噪声校正策略</strong>：提出“噪声和校正去噪”过程，在去噪过程的某些中间步骤中，通过自适应地用已知的初始噪声补偿预测噪声来校正预测噪声。</p></li><li><p>总结：（1）：本文提出了一种用于图像转视频生成的高效无调优噪声校正方法，通过补充更精确的图像信息和噪声校正来实现高保真度。（2）：创新点：</p></li><li>提出了一种“噪声和校正去噪”过程，通过自适应地用已知的初始噪声补偿预测噪声来校正预测噪声。</li><li>该方法无需调优且即插即用，可与其他视频扩散模型集成。性能：</li><li>实验结果证明了该方法在提高生成视频保真度方面的有效性。工作量：</li><li>该方法简单易用，易于集成到现有的视频生成框架中。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-0197a02f813c3611a9266978be983045.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f12bc1d8e5e3f0a7bb65cd3aa0275044.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6416123c2bdeefb6d5270913d20d6664.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7370c1b440fe22b048fbc20b419b5dd7.jpg" align="middle"></details><h2 id="Few-shot-Learner-Parameterization-by-Diffusion-Time-steps"><a href="#Few-shot-Learner-Parameterization-by-Diffusion-Time-steps" class="headerlink" title="Few-shot Learner Parameterization by Diffusion Time-steps"></a>Few-shot Learner Parameterization by Diffusion Time-steps</h2><p><strong>Authors:Zhongqi Yue, Pan Zhou, Richang Hong, Hanwang Zhang, Qianru Sun</strong></p><p>Even when using large multi-modal foundation models, few-shot learning is still challenging — if there is no proper inductive bias, it is nearly impossible to keep the nuanced class attributes while removing the visually prominent attributes that spuriously correlate with class labels. To this end, we find an inductive bias that the time-steps of a Diffusion Model (DM) can isolate the nuanced class attributes, i.e., as the forward diffusion adds noise to an image at each time-step, nuanced attributes are usually lost at an earlier time-step than the spurious attributes that are visually prominent. Building on this, we propose Time-step Few-shot (TiF) learner. We train class-specific low-rank adapters for a text-conditioned DM to make up for the lost attributes, such that images can be accurately reconstructed from their noisy ones given a prompt. Hence, at a small time-step, the adapter and prompt are essentially a parameterization of only the nuanced class attributes. For a test image, we can use the parameterization to only extract the nuanced class attributes for classification. TiF learner significantly outperforms OpenCLIP and its adapters on a variety of fine-grained and customized few-shot learning tasks. Codes are in <a href="https://github.com/yue-zhongqi/tif">https://github.com/yue-zhongqi/tif</a>. </p><p><a href="http://arxiv.org/abs/2403.02649v1">PDF</a> Accepted by CVPR 2024</p><p><strong>摘要</strong><br>利用扩散模型的时间步，可以分离细微的类别属性，通过文本条件的适配器弥补丢失的属性，实现小样本学习任务的准确分类。</p><p><strong>要点</strong></p><ul><li>扩散模型的时间步可以隔离细微的类别属性。</li><li>细微的属性通常在较早的时间步丢失，而视觉突出的属性则在较晚的时间步丢失。</li><li>提出时间步小样本学习器 (TiF)，为文本条件的 DM 训练特定于类别的低秩适配器。</li><li>适配器和小提示本质上是在小时间步内仅参数化细微的类别属性。</li><li>对于测试图像，可以使用参数化仅提取细微的类别属性进行分类。</li><li>TiF 学习器在各种细粒度和定制的小样本学习任务上明显优于 OpenCLIP 及其适配器。</li><li>代码可在 <a href="https://github.com/yue-zhongqi/tif">https://github.com/yue-zhongqi/tif</a> 获得。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><p>1.标题：基于扩散时间步长的少样本学习器参数化2.作者：Yue Zhongqi, Bowen Cheng, Yaming Wang, Qinghua Hu, Xiaodan Liang3.所属单位：北京大学4.关键词：Few-shot learning, Diffusion model, Low-rank adaptation5.论文地址：None6.摘要：（1）研究背景：少样本学习中，模型容易学习到与类别标签虚假相关的视觉突出属性，而忽略细微的类别属性。（2）过去方法及问题：现有方法缺乏合适的归纳偏置，无法有效区分细微的类别属性和视觉突出属性。（3）本文方法：提出时间步长少样本学习器（TiF learner），利用扩散模型的时间步长分离细微的类别属性，并训练类别特定的低秩适配器来弥补丢失的属性。（4）方法性能：TiF learner 在各种细粒度和定制的少样本学习任务上明显优于 OpenCLIP 及其适配器。</p><p></p><ol><li><p>方法：(1) 训练去噪网络 d，使用扩散模型的时间步长分离丢失的细微类别属性；(2) 训练类别特定的低秩适配器来弥补丢失的属性；(3) 通过计算时间步长上的加权平均值 Lt 来进行推理。</p></li><li><p>总结：(1): 本工作提出了一种基于扩散时间步长的少样本学习器 TiFlearner，通过分离细微的类别属性和视觉突出属性，有效解决了少样本学习中易学习到虚假相关属性的问题，显著提升了细粒度和定制少样本学习任务的性能。(2): Innovation point: TiFlearner 创新性地利用扩散模型的时间步长分离丢失的细微类别属性，并训练类别特定的低秩适配器来弥补丢失的属性，有效区分了细微的类别属性和视觉突出属性。Performance: TiFlearner 在各种细粒度和定制少样本学习任务上明显优于 OpenCLIP 及其适配器，证明了其有效性和鲁棒性。Workload: TiFlearner 的训练和推理过程相对复杂，需要训练去噪网络和类别特定的低秩适配器，计算时间步长上的加权平均值，工作量较大。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-c1f7d70acd760956bfb9ce16a4c9a32f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9fd5fe0d098a2e3948ad5e4744720eed.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3823bdb18fac83dfd9b0fde352c77358.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-255f6ff30f2576a40ef0753bdfd6f57e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-46aa23abe5a92b4732abedfceaed986b.jpg" align="middle"></details><h2 id="Semantic-Human-Mesh-Reconstruction-with-Textures"><a href="#Semantic-Human-Mesh-Reconstruction-with-Textures" class="headerlink" title="Semantic Human Mesh Reconstruction with Textures"></a>Semantic Human Mesh Reconstruction with Textures</h2><p><strong>Authors:Xiaoyu Zhan, Jianxin Yang, Yuanqi Li, Jie Guo, Yanwen Guo, Wenping Wang</strong></p><p>The field of 3D detailed human mesh reconstruction has made significant progress in recent years. However, current methods still face challenges when used in industrial applications due to unstable results, low-quality meshes, and a lack of UV unwrapping and skinning weights. In this paper, we present SHERT, a novel pipeline that can reconstruct semantic human meshes with textures and high-precision details. SHERT applies semantic- and normal-based sampling between the detailed surface (eg mesh and SDF) and the corresponding SMPL-X model to obtain a partially sampled semantic mesh and then generates the complete semantic mesh by our specifically designed self-supervised completion and refinement networks. Using the complete semantic mesh as a basis, we employ a texture diffusion model to create human textures that are driven by both images and texts. Our reconstructed meshes have stable UV unwrapping, high-quality triangle meshes, and consistent semantic information. The given SMPL-X model provides semantic information and shape priors, allowing SHERT to perform well even with incorrect and incomplete inputs. The semantic information also makes it easy to substitute and animate different body parts such as the face, body, and hands. Quantitative and qualitative experiments demonstrate that SHERT is capable of producing high-fidelity and robust semantic meshes that outperform state-of-the-art methods. </p><p><a href="http://arxiv.org/abs/2403.02561v1">PDF</a> </p><p><strong>Summary</strong><br>SHERT 是一种新颖的管道，可以重建具有纹理和高精度细节的语义人体网格。</p><p><strong>Key Takeaways</strong></p><ul><li>SHERT可在详细表面和 SMPL-X 模型之间进行基于语义和法线的采样，以获得部分采样的语义网格。</li><li>自监督完成和细化网络可生成完整的语义网格。</li><li>纹理扩散模型可创建由图像和文本驱动的纹理。</li><li>重建的网格具有稳定的 UV 展开、高质量三角形网格和一致的语义信息。</li><li>SMPL-X 模型提供语义信息和形状先验，即使在输入不正确和不完全的情况下，SHERT 也能很好地执行。</li><li>语义信息便于替换和动画不同的身体部位，如面部、身体和手。</li><li>定量和定性实验表明，SHERT 能够产生高保真和鲁棒的语义网格，其性能优于最先进的方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：语义人体网格重建与纹理化</li><li>作者：Yu-Kun Lai, Chen Cao, Lei Zhou, Yajie Zhao, Kun Zhou, Chen Change Loy, Ziwei Liu</li><li>隶属机构：香港中文大学</li><li>关键词：语义人体网格重建、纹理化、自监督学习、图像生成</li><li>论文链接：NoneGithub 链接：None</li><li><p>摘要：(1) 研究背景：近年来，3D 详细人体网格重建领域取得了重大进展。然而，当前方法在工业应用中仍面临以下挑战：结果不稳定、网格质量低以及缺乏 UV 展开和蒙皮权重。(2) 过去方法及其问题：过去的方法通常使用基于图像的方法，这些方法需要大量的数据和计算资源，并且对输入图像的质量非常敏感。此外，这些方法通常无法生成具有语义信息的网格，这使得它们难以用于动画和虚拟现实等应用。(3) 本文提出的研究方法：本文提出了 SHERT，这是一种新颖的管道，可以重建具有纹理和高精度细节的语义人体网格。SHERT 在详细表面（例如网格和 SDF）和相应的 SMPL-X 模型之间应用基于语义和法线的采样，以获得部分采样的语义网格，然后通过专门设计的自监督完成和细化网络生成完整的语义网格。使用完整的语义网格作为基础，我们采用纹理扩散模型来创建受图像和文本驱动的纹理。(4) 方法在任务和性能上的表现：本文方法能够生成高保真且鲁棒的语义网格，其性能优于最先进的方法。在多个数据集上的定量和定性实验表明，SHERT 可以很好地处理不正确和不完整输入，并且可以轻松替换和动画不同的身体部位，例如面部、身体和手。</p></li><li><p>方法：（1）基于语义和法线的采样，在详细表面（如网格和 SDF）和相应的 SMPL-X 模型之间进行采样，以获得部分采样的语义网格；（2）通过专门设计的自监督完成和细化网络，生成完整的语义网格；（3）使用完整的语义网格作为基础，采用纹理扩散模型来创建受图像和文本驱动的纹理。</p></li><li><p>结论：（1）：本文提出了一种从详细表面或单目图像重建完全纹理化语义人体模型的方法 SHERT，该方法利用了目标表面的几何细节、语义信息和语义指导先验知识。重建结果具有高保真衣着细节、高质量三角形网格、清晰的面部特征和完整的手部几何形状。SHERT 还能够生成具有稳定 UV 展开的超高分辨率纹理贴图。该方法弥合理论重建工作和下游工业应用之间的差距，相信可以推动人体模型的发展。（2）：创新点：xxx；性能：xxx；工作量：xxx；</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-0fbc346a8aa3d55b54bc776d96e213e8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7dd492b9ec7ce1ca56e9958a2ba8f0b9.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2a4d7a0b580701e5f5f50e6834ff3111.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6184e9766e7cd4d5a85ef285d96ccb64.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a4992740f820eac8eee20ee9e8c27784.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6ca5e15c452099d37d81cea6645ae175.jpg" align="middle"></details><h2 id="Updating-the-Minimum-Information-about-CLinical-Artificial-Intelligence-MI-CLAIM-checklist-for-generative-modeling-research"><a href="#Updating-the-Minimum-Information-about-CLinical-Artificial-Intelligence-MI-CLAIM-checklist-for-generative-modeling-research" class="headerlink" title="Updating the Minimum Information about CLinical Artificial Intelligence   (MI-CLAIM) checklist for generative modeling research"></a>Updating the Minimum Information about CLinical Artificial Intelligence   (MI-CLAIM) checklist for generative modeling research</h2><p><strong>Authors:Brenda Y. Miao, Irene Y. Chen, Christopher YK Williams, Jaysón Davidson, Augusto Garcia-Agundez, Harry Sun, Travis Zack, Atul J. Butte, Madhumita Sushil</strong></p><p>Recent advances in generative models, including large language models (LLMs), vision language models (VLMs), and diffusion models, have accelerated the field of natural language and image processing in medicine and marked a significant paradigm shift in how biomedical models can be developed and deployed. While these models are highly adaptable to new tasks, scaling and evaluating their usage presents new challenges not addressed in previous frameworks. In particular, the ability of these models to produce useful outputs with little to no specialized training data (“zero-“ or “few-shot” approaches), as well as the open-ended nature of their outputs, necessitate the development of updated guidelines in using and evaluating these models. In response to gaps in standards and best practices for the development of clinical AI tools identified by US Executive Order 141103 and several emerging national networks for clinical AI evaluation, we begin to formalize some of these guidelines by building on the “Minimum information about clinical artificial intelligence modeling” (MI-CLAIM) checklist. The MI-CLAIM checklist, originally developed in 2020, provided a set of six steps with guidelines on the minimum information necessary to encourage transparent, reproducible research for artificial intelligence (AI) in medicine. Here, we propose modifications to the original checklist that highlight differences in training, evaluation, interpretability, and reproducibility of generative models compared to traditional AI models for clinical research. This updated checklist also seeks to clarify cohort selection reporting and adds additional items on alignment with ethical standards. </p><p><a href="http://arxiv.org/abs/2403.02558v1">PDF</a> </p><p><strong>Summary</strong><br>生成模型的兴起，如 LLM、VLM 和扩散模型，对医学自然语言和图像处理产生了重大影响，并提出了新的挑战，需要更新的模型开发和评估指南，以确保其可推广性、可解释性和可重复性。</p><p><strong>Key Takeaways</strong></p><ul><li>生成模型的适应性强，但对新任务的评估提出了新的挑战。</li><li>无/少样本学习和开放式输出需要新的评估指南。</li><li>MI-CLAIM 清单提供了一个框架，用于指导生成模型的透明和可复制的研究。</li><li>更新后的 MI-CLAIM 清单强调了生成模型与传统 AI 模型在训练、评估、可解释性和可复制性方面的差异。</li><li>更新后的清单澄清了队列选择报告，并增加了符合道德标准的附加项目。</li><li>强调了生成模型在医学中的伦理使用和负责任创新。</li><li>鼓励生成模型的标准化评估和报告，以促进可信和可重复的研究。</li><li>通过跨学科协作和持续的指导，可以解决生成模型的持续挑战和机会。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：更新临床人工智能最低信息（MI-CLAIM）</li><li>作者：Brenda Y. Miao</li><li>所属机构：加州大学伯克利分校和加州大学旧金山分校</li><li>关键词：临床人工智能、生成模型、MI-CLAIM、评估</li><li>链接：Github：https://github.com/mi-claim/mi-claim</li><li>摘要：（1）研究背景：随着生成模型，包括大型语言模型（LLM）的快速发展，临床人工智能（AI）工具的开发面临着标准和最佳实践的差距。</li></ol><p>（2）过去方法及其问题：MI-CLAIM 清单于 2020 年首次开发，提供了一套包含六个步骤的标准，但随着生成模型的快速发展，该清单已不再适用。</p><p>（3）论文提出的研究方法：本文更新了 MI-CLAIM 清单，以解决生成模型在临床 AI 中应用的新挑战。更新后的清单包括以下部分：- 研究设计：强调生成模型评估中自动化和人工评估的结合，并提供基于非结构化或多模态数据的队列选择最佳实践。- 数据和优化：要求详细说明数据来源、预处理步骤和训练、验证和测试集之间的独立性。- 模型评估：提供用于无结构文本输出的自动化模型评估方法，以及用于人类模型评估的指导。- 生成模型的可解释性：鼓励使用错误分析和敏感性分析（消融测试）来解释模型预测。- 端到端管道复制：强调提供代码和数据透明度，并讨论模型风险和潜在偏差。</p><p>（4）方法在什么任务上取得了什么性能？性能是否支持其目标？本文没有报告具体任务和性能结果，因为它着重于提供临床 AI 生成模型研究的标准和最佳实践。</p><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：（1）：更新后的 MI-CLAIM 清单为临床人工智能生成模型的研究和开发提供了标准和最佳实践，有助于提高模型的可信度和可解释性，促进临床人工智能的负责任和有效应用。（2）：创新点：</li><li>扩展了 MI-CLAIM 清单，以解决生成模型在临床人工智能中的新挑战。</li><li>提供了针对生成模型评估的具体指导，包括自动化和人工评估相结合、基于非结构化或多模态数据的队列选择最佳实践。</li><li>强调了生成模型的可解释性，鼓励使用错误分析和敏感性分析来解释模型预测。性能：本文没有报告具体任务和性能结果，因为它着重于提供标准和最佳实践。工作量：更新后的 MI-CLAIM 清单提供了详细的指导和要求，这可能会增加研究人员在临床人工智能生成模型研究中的工作量。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-e0a6a135c6657ff1a197759497122ce9.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-03-09  Pix2Gif Motion-Guided Diffusion for GIF Generation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>SyncTalk The Devil is in the Synchronization for Talking Head Synthesis</title>
    <link href="https://kedreamix.github.io/2024/03/07/Paperscape/SyncTalk/"/>
    <id>https://kedreamix.github.io/2024/03/07/Paperscape/SyncTalk/</id>
    <published>2024-03-07T07:57:00.000Z</published>
    <updated>2024-03-09T09:37:44.711Z</updated>
    
    <content type="html"><![CDATA[<h1 id="SyncTalk-The-Devil-is-in-the-Synchronization-for-Talking-Head-Synthesis"><a href="#SyncTalk-The-Devil-is-in-the-Synchronization-for-Talking-Head-Synthesis" class="headerlink" title="SyncTalk: The Devil is in the Synchronization for Talking Head Synthesis"></a>SyncTalk: The Devil is in the Synchronization for Talking Head Synthesis</h1><p>Paper   : <a href="https://arxiv.org/abs/2311.17590">https://arxiv.org/abs/2311.17590</a></p><p>Project : <a href="https://ziqiaopeng.github.io/synctalk/">https://ziqiaopeng.github.io/synctalk/</a></p><p>Video    : <a href="https://ziqiaopeng.github.io/synctalk/#teaser">https://ziqiaopeng.github.io/synctalk/#teaser</a></p><p>Code    : <a href="https://github.com/ziqiaopeng/SyncTalk">https://github.com/ziqiaopeng/SyncTalk</a></p><p><strong>摘要</strong></p><p>神经辐射场 - 生成对抗网络框架用于实现说话人头部视频的同步合成。</p><p>（1）研究背景： 生成逼真的、由语音驱动的谈话头部视频是一项具有挑战性的任务。传统生成对抗网络（GAN）难以保持一致的面部身份，而神经辐射场（NeRF）方法虽然可以解决这个问题，但通常会产生不匹配的唇部动作、不充分的面部表情和不稳定的头部姿势。一个逼真的谈话头部需要同步协调主体身份、唇部动作、面部表情和头部姿势。缺乏这些同步是导致不真实和人工结果的根本缺陷。 </p><p>（2）过去的方法及其问题： GAN 方法难以保持一致的面部身份。NeRF 方法虽然可以解决这个问题，但通常会产生不匹配的唇部动作、不充分的面部表情和不稳定的头部姿势。 </p><p>（3）提出的研究方法： SyncTalk 是一种基于 NeRF 的方法，它有效地保持了主体身份，增强了谈话头部合成的同步性和真实性。SyncTalk 使用面部同步控制器将唇部动作与语音对齐，并创新地使用 3D 面部混合形状模型来捕捉准确的面部表情。头部同步稳定器优化头部姿势，实现更自然的头部运动。肖像同步生成器恢复头发细节，并将生成的头部与躯干融合，以获得无缝的视觉体验。</p><p>（4）方法在什么任务上取得了什么性能，这些性能是否支持了它们的目标： SyncTalk 在谈话头部合成同步性和真实性方面优于最先进的方法。广泛的实验和用户研究表明，SyncTalk 在同步性和真实性方面优于最先进的方法。</p><p><strong>关键要点</strong></p><ul><li>传统生成对抗网络难以维持一致的面部身份。</li><li>神经辐射场方法可以解决面部身份一致性问题，但经常出现嘴唇运动不匹配、面部表情不足和头部姿势不稳定的问题。</li><li>逼真的说话人头部视频需要同步协调主体身份、嘴唇运动、面部表情和头部姿势。</li><li>缺少同步性是导致不真实和人为结果的根本缺陷。</li><li>SyncTalk 是一种基于神经辐射场的方法，有效地保持了主体身份，提高了说话人头部合成中的同步性和真实感。</li><li>SyncTalk 使用面部同步控制器将嘴唇运动与语音对齐，并创新地使用 3D 面部混合形状模型来捕捉准确的面部表情。</li><li>SyncTalk 的头部同步稳定器优化了头部姿势，实现了更自然的头部运动。</li><li>人像同步生成器恢复头发细节，将生成的头部与躯干融合，以获得无缝的视觉体验。</li></ul><p><img src="https://picx.zhimg.com/v2-a57e0937b2f452009023394a59529dfb.png" alt="SyncTalk"></p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>这篇论文中，解决最好的就是同步的问题，所以也称为同步的Devil 魔鬼😈。现有方法在四个关键领域需要更多的同步：<strong>主体身份</strong>、<strong>唇部运动</strong>、<strong>面部表情</strong>和<strong>头部姿势</strong>。</p><ul><li><p>首先，在基于GAN的方法中，由于连续帧中特征的不稳定性以及仅使用少量帧作为面部重建参考，保持视频中主体的身份是具有挑战性的。</p></li><li><p>其次，唇部运动与语音不同步。在基于NeRF的方法中，仅基于5分钟语音数据集训练的音频特征难以泛化到不同的语音输入。</p></li><li><p>第三，缺乏面部表情控制，大多数方法只能产生唇部运动或控制眨眼，导致面部动作不自然。</p></li><li><p>第四，头部姿势不同步。</p></li></ul><p>先前的方法依赖于稀疏的landmarks来计算投影误差，但这些landmarks的抖动和不准确性导致头部姿势不稳定。这些同步问题会引入伪影，并显著降低真实感。</p><p>为了解决这些同步挑战，引入了SyncTalk，这是一种基于NeRF的方法，专注于高度同步、逼真的、语音驱动的说话头部合成，采用三平面哈希表示来维护主体身份。通过面部同步控制器和头部同步稳定器，SyncTalk显著提高了合成视频的同步性和视觉质量。PortraitSync Generator进一步改善了视觉质量，精心细化了视觉细节。整个渲染过程可以实现50 FPS，并输出高分辨率视频。</p><div class="table-container"><table><thead><tr><th>模块</th><th>描述</th></tr></thead><tbody><tr><td>Face-Sync Controller</td><td>在Face-Sync控制器中，预先在2D音频视听数据集上对音频视觉编码器进行预训练，得到了一种通用表示，确保了不同语音样本之间的唇部同步运动。对于控制面部表情，采用了一个语义丰富的3D面部混合形状模型，该模型通过52个参数控制特定的面部表情区域。</td></tr><tr><td>Head-Sync Stabilizer</td><td>在Head-Sync稳定器中，使用AD-NeRF中的头部运动跟踪器来推断头部的粗略旋转和平移参数。由于粗略参数的不稳定性，借鉴了同步定位与地图(SLAM)的思想，结合头部关键点跟踪器跟踪稠密关键点，并采用bundle adjustment method 束调整方法来优化头部姿势，从而实现稳定连续的头部运动。</td></tr><tr><td>Portrait-Sync Generator</td><td>为了进一步提高SyncTalk的视觉保真度，设计了一个Portrait-Sync生成器。这个模块修复了NeRF建模中的伪影，特别是头发和背景等细节，输出高分辨率视频。</td></tr></tbody></table></div><p><strong>主要贡献</strong></p><ul><li>提出了一个Face-Sync控制器，结合音频视觉编码器和面部动画捕捉器，确保准确的唇部同步和动态面部表情渲染。 </li><li>引入了一个Head-Sync稳定器，跟踪头部旋转和面部运动关键点。利用束调整方法，该稳定器保证了平滑同步的头部运动。</li><li>设计了一个Portrait-Sync生成器，通过修复NeRF建模中的伪影和细化头发和背景等细节，提高了视觉保真度。</li></ul><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p><strong>GAN-based Method</strong></p><p>近来，基于GAN的说话头合成成为了计算机视觉中的一个重要研究领域。然而，它们在保持视频中主体的身份一致性方面存在挑战。</p><p>例如，Wav2Lip引入了一个唇部同步专家来监督唇部运动。然而，由于使用了来自参考帧的五帧来重建唇部，它难以保持主体的身份。另一些方法尝试进行全脸合成，但往往难以确保面部表情和头部姿势之间的同步。除了视频流技术外，还有一些方法试图通过语音使单张图像“说话”，如SadTalker可以从单张图像生成一个人说话的视频。然而，这些方法无法生成自然的头部姿势和面部表情，难以保持主体的身份，影响了同步效果，导致视觉感知不真实。</p><p>与这些方法相比，SyncTalk使用NeRF<strong>对人脸进行三维建模</strong>。其能够在规范空间中表示<strong>连续的3D场景的能力</strong>，使其在保持主体身份一致性和保留细节方面表现出色。</p><p><strong>NeRF-based Method</strong></p><p>近来，随着NeRF的崛起，许多领域已开始利用它来解决相关挑战。先前的工作已将NeRF整合到合成说话头像的任务中，并将音频作为驱动信号，但这些方法都是基于普通的NeRF模型。</p><p>例如，AD-NeRF需要大约10秒来渲染单个图像。RADNeRF旨在实现实时视频生成，并使用了基于Instant-NGP的NeRF。ER-NeRF通过引入三平面哈希编码器来修剪空白空间区域，提倡紧凑且加速的渲染方法。GeneFace试图通过将语音特征转换为面部标志来减少NeRF的伪影，但这往往导致唇部运动不准确。尝试使用基于NeRF的方法创建角色头像，例如，不能直接由语音驱动。这些方法仅将音频作为条件，没有清晰的同步概念，并且通常导致唇部运动平均。</p><p>此外，先前的方法<strong>缺乏对面部表情的控制</strong>，仅限于控制眨眼，并且无法对抬眉毛或皱眉等动作进行建模。此外，这些方法在头部姿势不稳定方面存在显着问题，<strong>导致头部和躯干分离</strong>。相比之下，使用Face-Sync控制器来建模音频和唇部运动之间的关系，从而增强唇部运动和表情的同步性，使用Head-Sync稳定器来稳定头部姿势，通过解决这些同步问题，提高了视觉质量。</p><h2 id="主要方法"><a href="#主要方法" class="headerlink" title="主要方法"></a>主要方法</h2><p>SyncTalk主要由三部分组成，接下来会一一介绍</p><ul><li><strong>Face-Sync Controller</strong> 控制嘴唇运动和面部表情</li><li><strong>Head-Sync Stabilizer</strong> 稳定头部姿势</li><li><strong>Portrait-Sync Generator</strong> 渲染的高同步面部帧</li></ul><p><img src="https://pica.zhimg.com/v2-03605cd4fbd659c9d341840c64fd3b41.png" alt="Overview of SyncTalk"></p><h3 id="Face-Sync-Controller"><a href="#Face-Sync-Controller" class="headerlink" title="Face-Sync Controller"></a>Face-Sync Controller</h3><p><strong>Audio-Visual Encoder</strong></p><p>在现有的方法中，大部分的音频特征提取器是用类似于 <strong>DeepSpeech，Wav2Vec 2.0 和 HuBERT</strong> 等ASR模型，但是这些事专门为Automatic Speech Recognition ASR任务设计的，这种设计的音频编码器并不能真正反映嘴唇运动。这是因为预训练的模型是<strong>基于从音频到文本的特征分布，而需要从音频到嘴唇运动的特征分布</strong>。</p><p>针对这种情况，使用在LRS2上训练的<a href="https://github.com/smeetrs/deep_avsr">deep avsr</a>来做音频特征提取器，使用预训练的唇形同步鉴别器 <a href="https://github.com/joonson/syncnet_python">SyncNet</a>来监督视频的同步效果，这是使用连续的面部窗口F和相对应的音频帧A输入，同时分为正负样本进行训练，利用<strong>余弦相似度和交叉熵损失</strong>来最小化同步样本的距离并最大化非同步样本的距离。</p><script type="math/tex; mode=display">\begin{aligned}\sin(F,A)&=\frac{F\cdot A}{\|F\|_2\|A\|_2})\end{aligned},</script><script type="math/tex; mode=display">L_{\mathrm{sync}}=-\left(y\log(\sin(F,A))+(1-y)\log(1-\sin(F,A))\right),</script><p><img src="https://picx.zhimg.com/v2-6b250a8119b776d55493f82cfda54bc5.png" alt="正负样本"></p><p>同时在同步鉴别器的监督下，预训练对应的视听特征提取器，这里面堆叠卷积网络进行编码解码，最后用<strong>重建损失</strong>来进行监督。训练后，我们使用 Conv(A) 作为从音频中提取的唇部空间。</p><script type="math/tex; mode=display">L_{\mathrm{recon}}=\|F-\mathrm{Dec}(\mathrm{Conv}(A)\oplus\mathrm{Conv}(F))\|_1.</script><p><strong>Facial Animation Capturer</strong></p><p>在之前的研究中发现，基于NeRF的方法只能改变眨眼，无法准确地建模面部表情。这导致训练出的角色表情僵硬，面部细节不准确，特别是对于有明显面部动作的角色，如眨眼、抬眉毛或皱眉等。<strong>考虑到需要更加同步和逼真的面部表情，添加了一个表情同步控制模块。</strong></p><p>具体而言，引入了一个<strong>基于52个语义面部混合形状系数 B 的3D面部先验模型来建模面部</strong>，也就是3D blendshape 系数来控制面部，这一部分类似于 <a href="https://arxiv.org/abs/2303.11089">EmoTalk</a>。因为3D面部模型能够保留面部运动的结构信息，所以它能够很好地反映面部动作的内容，同时又不会引起面部结构的失真。</p><p><strong>在训练过程中，首先使用一个复杂的面部混合形状捕捉模块将面部表情捕捉为E(B)，然后选择七个核心面部表情控制系数来控制眉毛、额头和眼睛区域。</strong>这些系数与表情高度相关，且独立于嘴唇的运动。因为面部系数具有语义信息，所以我们可以在推理过程中同步演讲者的面部表情。</p><p><img src="https://pica.zhimg.com/v2-9cfb1cfb7f4ae95b64a868f8e8abad0e.png" alt="Facial Animation Capturer"></p><p><strong>Facial-Aware Masked-Attention</strong></p><p>为了减少训练过程中嘴唇特征和表情特征之间的相互干扰，引入了Facial-Aware Disentangle Attention模块。基于区域注意力向量 V，这类似于<a href="https://fictionarry.github.io/ER-NeRF/">ER-NeRF</a>，我们分别将Mask $M<em>{lip}$ 和 $M</em>{exp}$ 添加到嘴唇和表情的注意力区域。</p><script type="math/tex; mode=display">\begin{aligned}V_{\mathrm{lip}}&=V\odot M_{\mathrm{lip}},\\V_{\mathrm{exp}}&=V\odot M_{\mathrm{exp}}.\end{aligned}</script><p>通过这样设计的注意力机制，能够有效解耦嘴唇运动和眨眼运动等，从而减少耦合带来的伪影，最后利用解耦的嘴唇特征 $f<em>l = F</em>{lip} ⊙ V<em>{lip}$ 和表情特征$f_e = f</em>{exp} ⊙ V_{exp}$。</p><p><img src="https://pica.zhimg.com/v2-ba601309ab5cc09573f4291d7ae27f13.png" alt="ER-NeRF Mask"></p><h3 id="Head-Sync-Stabilizer"><a href="#Head-Sync-Stabilizer" class="headerlink" title="Head-Sync Stabilizer"></a>Head-Sync Stabilizer</h3><p><strong>Head Motion Tracker</strong></p><p>头部姿势，表示为 p，是指人的头部在 3D 空间中的旋转角度，由旋转 R 和平移 T 定义。</p><p>不稳定的头部姿势会导致头部抖动。为了获得头部姿势的粗略估计，首先，通过在预定范围内迭代 i 次来确定最佳焦距。对于每个焦距候选 fi，重新初始化旋转和平移值。目标是最小化 3D 可变形模型 (3DMM) 的投影地标与视频帧中的实际地标之间的误差。</p><script type="math/tex; mode=display">f_{\mathrm{opt}}=\arg\min_{f_i}E_i(L_{2D},L_{3D}(f_i,R_i,T_i)),</script><p>其中 $E_i$表示的就是MSE，这样能够以更好地将模型的投影lmk与实际视频lmk对齐，然后得到最优的旋转和平移矩阵，也是用MSE来最小化，这是对每一帧进行操作的，在对应视频帧的最优值。</p><script type="math/tex; mode=display">(R_{\mathrm{opt}},T_{\mathrm{opt}})=\arg\min_{R,T}E(L_{2D},L_{3D}(f_{\mathrm{opt}},R,T)).</script><p><strong>Head Points Tracker</strong></p><p>对于之前基于NeRF的方法来说，先前的方法利用基于 3DMM 的技术来提取头部姿势并生成不准确的结果。为了提高R和T的精度，我们使用像Co- tracker这样的光流估计模型来跟踪面部关键点K。</p><p>接下来，使用预训练的光流估计模型，在获取面部运动光流后，我们使用<strong>拉普拉斯滤波器</strong>选择位于最显著流变化位置的关键点，并在流序列中跟踪这些关键点的运动轨迹。通过这个模块确保了所有帧上的面部关键点对齐更加精确和一致，从而增强了头部姿势参数的准确性。</p><p><strong>Bundle Adjustment</strong></p><p>根据关键点和粗略的头部姿势，引入了一个两阶段优化框架来提高关键点和头部姿势估计的准确性。</p><ul><li><p>第一阶段，随机初始化 j 个关键点的 3D 坐标并优化它们的位置，以便与图像平面上跟踪的关键点对齐。这一部分最小化损失函数 $L_{init}$，捕获<strong>投影关键点 P 和跟踪关键点 K</strong> 之间的差异：</p><script type="math/tex; mode=display">L_{\mathrm{init}}=\sum_j\lVert P_j-K_j\rVert_2.</script></li><li><p>第二阶段，开始进行更全面的优化，以细化 3D 关键点和相关的头部联合姿势参数，通过Adam优化器优化算法，<strong>调整空间坐标、旋转角度R和平移T</strong>以最小化对齐误差$L_{sec}$，表示为：</p><script type="math/tex; mode=display">L_{\sec}=\sum_j\lVert P_j(R,T)-K_j\rVert_2.</script><p>经过这些优化后，观察到所得的头部姿势和平移参数平滑且稳定。</p></li></ul><h3 id="Dynamic-Portrait-Renderer"><a href="#Dynamic-Portrait-Renderer" class="headerlink" title="Dynamic Portrait Renderer"></a>Dynamic Portrait Renderer</h3><p><strong>Tri-Plane Hash Representation</strong></p><p>这一部分实际上就是NeRF的体渲染的方式，都是一些定义的部分。</p><script type="math/tex; mode=display">\hat{C}(\mathrm{r})=\int_{t_n}^{t_f}\sigma(\mathrm{r}(t))\cdot\mathrm{c}(\mathrm{r}(t),\mathrm{d})\cdot T(t)dt,</script><p>类似于ER-NeRF的方式，解决哈希冲突和优化音频特征处理的问题，结合了三个独特定向xyz的 2D 哈希网格，也就是 <strong>Tri-Plane Hash</strong>，作为hash的编码器。</p><script type="math/tex; mode=display">\mathcal{H}^{\mathrm{AB}}:(a,b)\to\mathrm{f}_{ab}^{\mathrm{AB}},\\\mathrm{f_x}=\mathcal{H}^\mathrm{XY}(x,y)\oplus\mathcal{H}^\mathrm{YZ}(y,z)\oplus\mathcal{H}^\mathrm{XZ}(x,z),</script><p>其中输出 $f^{AB}<em>{ab} ∈ R</em>{LD}$，具有层数 $L$ 和每个方向的特征维度 $D$，表示与投影坐标$ (a, b)$ 相对应的平面几何特征，$H^{AB}$ 表示平面 $R^{AB}$ 的多分辨率哈希编码器。得到每个方向的向量以后，产生 $3 × LD$ 通道向量。采用$fx$、视角方向$d$、嘴唇特征$f_l$和表情特征$f_e$，三平面哈希的隐式函数定义为：</p><script type="math/tex; mode=display">\mathcal{F}^{\mathcal{H}}:(\mathrm{x},\mathrm{d},f_l,f_e;\mathcal{H}^3)\to(\mathrm{c},\sigma),</script><p>类似于ER-NeRF，训练采用了一个两步粗到细的策略。首先，使用MSE损失评估预测的 $\hat{C(r)}$与实际图像颜色$C(r)$之间的差异。鉴于MSE在细节捕捉方面的局限性。接下来进入一个细化阶段，引入LPIPS损失以增强细节，类似于ER-NeRF。我们从图像中提取随机补丁Patch $P$，并将LPIPS（由λ加权）与MSE结合起来以改善细节表示。</p><script type="math/tex; mode=display">\mathcal{L}_\mathrm{total}=\sum_\mathrm{r}\|C(\mathrm{r})-\hat{C}(\mathrm{r})\|_2+\lambda\times\mathcal{L}_\mathrm{LPIPS}(\hat{\mathcal{P}},\mathcal{P}).</script><p><strong>Portrait-Sync Generator</strong></p><p>在训练过程中，为了解决 NeRF 在<strong>捕捉发丝和动态背景</strong>等精细细节方面的局限性，引入了一个包含两个关键部分的 PortraitSync 生成器。</p><p>首先，NeRF 渲染面部区域 ($Fr$)，通过高斯模糊创建 $G(Fr)$，然后使用我们同步的头部姿势能够与原始图像 ($F_o$) 合并，以增强头发细节保真度。</p><p>其次，当头部和躯干结合在一起时，如果源视频中的角色说话而生成的面部保持沉默，则可能会出现暗间隙区域，如下图（b）所示。 所以用平均颈部颜色 ($Cn$) 填充这些区域。 </p><p>这种方法通过肖像同步生成器产生更真实的细节并提高视觉质量。</p><p><img src="https://picx.zhimg.com/v2-421af4b4cfa489148de7fc8f4067427b.png" alt="比较"></p><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p><strong>数据集</strong></p><p>为了进行公平比较，我们使用了来自AD-NeRF，GeneFace和ER-NeRF中相同的视频序列，其中包括英语和法语。这些视频的平均长度约为8,843帧，每个视频以25 FPS录制。除了来自AD-NeRF的视频分辨率为450 × 450外，所有其他视频的分辨率均为512 × 512，并以角色为中心。</p><p><strong>比较基线</strong></p><ul><li>GAN-based  方法  ：Wav2Lip，VideoReTalking，DINet，TalkLip and IP-LAP。</li><li>NeRF-based 方法 ： AD-NeRF，RADNeRF，GeneFace and ER-NeRF。</li></ul><p><strong>实验细节</strong></p><ul><li>在粗略阶段，肖像头部经过100,000次迭代训练，在精细阶段训练25,000次迭代。</li><li>每次迭代使用2D哈希编码器（L=14，F=1）采样$256^2$条光线。</li><li>采用AdamW优化器[24]，哈希编码器的学习率为0.01，其他模块的学习率为0.001。</li><li>在NVIDIA RTX 3090 GPU上，总训练时间约为2小时。</li></ul><p><strong>定量评价</strong></p><div class="table-container"><table><thead><tr><th>评估指标</th><th>描述</th></tr></thead><tbody><tr><td>全参考质量评估</td><td>使用峰值信噪比（PSNR）、学习感知图像补丁相似性（LPIPS）、多尺度结构相似性（MS-SSIM）和Frechet Inception Distance（FID）作为评估指标。</td></tr><tr><td>无参考质量评估</td><td>在高PSNR图像中，纹理细节可能与人类视觉感知不一致。为了更精确地定义和比较输出，使用两种无参考方法：自然图像质量评估器（NIQE）和无参考图像空间质量评估器（BRISQUE）。</td></tr><tr><td>同步评估</td><td>对于同步性，使用地标距离（LMD）来衡量面部运动的同步性，动作单位误差（AUE）来评估面部运动的准确性，并引入唇同步误差置信度（LSE-C），与Wav2Lip一致，以评估唇部运动与音频之间的同步性。</td></tr></tbody></table></div><p><strong>定量评估结果</strong></p><ul><li>头部重建方法在图像质量和同步性方面均优于基于GAN和NeRF的最新方法。</li><li>经过<code>Portrait-Sync Generato</code>r处理后，图像质量得到了显著改善，头发细节得到了恢复。</li><li>方法在维持主体身份、唇部、表情和姿势的同步性方面表现出色。</li><li>使用分布外音频的最新SOTA方法的驱动器结果表明，方法在唇音同步评估方面领先。</li><li>渲染速度远远超过视频输入速度，可以实现实时生成视频流。</li></ul><p><img src="https://pica.zhimg.com/v2-3093f3d799bb12490a7f79dba96bde99.png" alt="The quantitative results of the head reconstruction."></p><p><img src="https://picx.zhimg.com/v2-73c53cd37a7c9e87af9b918778a84d3e.png" alt="The quantitative results of the lip synchronization."></p><p><strong>定性评价</strong></p><div class="table-container"><table><thead><tr><th>评估结果</th><th>描述</th></tr></thead><tbody><tr><td>图像质量比较</td><td>在图中，我们展示了我们的方法与其他方法的比较。可以观察到，SyncTalk展示了更精确、更准确的面部细节。</td></tr><tr><td>与Wav2Lip的比较</td><td>与Wav2Lip相比，我们的方法在保持主体身份的同时提供了更高的保真度和分辨率。</td></tr><tr><td>与IP-LAP的比较</td><td>与IP-LAP相比，我们的方法在唇形同步方面表现出色，主要归功于音频-视觉编码器带来的音频-视觉一致性。</td></tr><tr><td>与GeneFace的比较</td><td>与GeneFace相比，我们的方法可以通过表情同步精确地重现眨眼和抬眉等动作。</td></tr><tr><td>与ER-NeRF的比较</td><td>与ER-NeRF相比，我们的方法通过姿势同步稳定器避免了头部和身体的分离，并生成了更准确的唇形。</td></tr><tr><td></td></tr></tbody></table></div><p><img src="https://picx.zhimg.com/v2-b076e645737b2297bee21027ac8e27ad.png" alt="Qualitative comparison of facial synthesis by different methods."></p><p><strong>User Study</strong>  </p><p>我们设计了一个详尽的用户研究问卷，35名参与者进行评分。问卷设计了五个方面的评分：唇同步准确性、表情同步准确性、姿势同步准确性、图像质量和视频真实性。</p><p>参与者平均完成问卷时间为19分钟，标准化的Cronbach α系数为0.96。用户研究结果显示，SyncTalk在所有评估中均超过以前的方法，特别是在视频真实性方面。</p><p><img src="https://picx.zhimg.com/v2-2666052562f51f053affc9fb748eec54.png" alt="User Study"></p><p><strong>Ablation Study</strong></p><p>接下来进行了消融研究，以检验我们模型中不同部分的贡献，选择了三个核心指标进行评估：PSNR、LPIPS和LMD。</p><p>我们选择了一个名为“May”的主体进行测试，结果如表所示。</p><p><img src="https://pic1.zhimg.com/v2-b204e48268633b55ad93cf70dbc8f9bd.png" alt="Ablation study for our components"></p><p>音频-视觉编码器提供了主要的唇部同步信息，当替换此模块时，所有三个指标都变差，其中特别是LMD错误增加了21.15%，表明唇部动作同步减少，如图5（a）所示，显示出我们的音频-视觉编码器可以提取准确的唇部特征。</p><p><img src="https://pica.zhimg.com/v2-2fc44a31570aeacd6badcf909f669fdc.png" alt="Ablation Study"></p><p>用ER-NeRF 的<strong>眨眼模块</strong>替换<strong>Facial Animation Capture</strong>模块，这一部分会影响眉毛的运动和图像质量。</p><p><strong>Facial-Aware Masked-Attention</strong>主要解耦了唇部和面部其他部位之间的运动，在移除后略微影响图像质量。</p><p>若没有<strong>头部同步稳定器</strong>，所有指标都显著下降，特别是LPIPS，导致头部姿势抖动和头部与躯干分离，如图5（b）所示。</p><p><strong>Portrait-Sync Generator</strong>恢复了像头发这样的细节，移除此模块会影响头发等细节的恢复，导致明显的分割边界。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>本文介绍了SyncTalk，这是一种基于高度同步的NeRF方法，用于实现逼真的语音驱动的说话头部合成。</li><li>框架包括面部同步控制器、头部同步稳定器和肖像同步生成器，能够保持主体身份，并生成同步的唇部动作、面部表情和稳定的头部姿势。</li><li>通过广泛的评估，SyncTalk在创建逼真和同步的说话头部视频方面表现出优异的性能，相较于现有方法。</li><li>期望SyncTalk不仅能增强各种应用程序的功能，还能在说话头部合成领域激发进一步的创新。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;SyncTalk-The-Devil-is-in-the-Synchronization-for-Talking-Head-Synthesis&quot;&gt;&lt;a href=&quot;#SyncTalk-The-Devil-is-in-the-Synchronization-for-</summary>
      
    
    
    
    <category term="Paperscape" scheme="https://kedreamix.github.io/categories/Paperscape/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>VividTalk One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior</title>
    <link href="https://kedreamix.github.io/2024/03/05/Paperscape/VividTalk/"/>
    <id>https://kedreamix.github.io/2024/03/05/Paperscape/VividTalk/</id>
    <published>2024-03-05T07:31:00.000Z</published>
    <updated>2024-03-07T08:03:21.030Z</updated>
    
    <content type="html"><![CDATA[<h1 id="VividTalk-One-Shot-Audio-Driven-Talking-Head-Generation-Based-on-3D-Hybrid-Prior"><a href="#VividTalk-One-Shot-Audio-Driven-Talking-Head-Generation-Based-on-3D-Hybrid-Prior" class="headerlink" title="VividTalk: One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior"></a>VividTalk: One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior</h1><p>Paper   : <a href="https://arxiv.org/pdf/2312.01841.pdf">https://arxiv.org/pdf/2312.01841.pdf</a></p><p>Project : <a href="https://humanaigc.github.io/vivid-talk/">https://humanaigc.github.io/vivid-talk/</a></p><p>Video   : <a href="https://www.youtube.com/watch?v=lJVzt7JCe_4">https://www.youtube.com/watch?v=lJVzt7JCe_4</a></p><p>Code    : <a href="https://github.com/HumanAIGC/VividTalk">https://github.com/HumanAIGC/VividTalk</a>  (Maybe Comming Soon)</p><p><strong>摘要</strong></p><p>创新的两阶段框架 VividTalk 可生成高质量视觉效果的说话人头部视频，包括唇形同步、丰富的面部表情、自然的头部姿势等。</p><p>（1）音频驱动的说话头生成已经引起广泛关注，在唇形同步、面部表情、头部姿势生成和视频质量方面取得了进展。然而，由于音频和动作之间的一对多映射，还没有模型能够在所有这些指标上达到最优SOTA。<br>（2）以往的方法通常使用混合形状Blendshape或顶点偏移vertex来表示面部表情，但这些方法在捕捉精细的表情细节方面存在局限性。此外，头部姿势的生成通常是通过直接从音频中学习来实现的，这可能会导致不合理和不连续的结果。<br>（3）本文提出了一种名为 VividTalk 的两阶段通用框架，支持生成具有所有上述属性的高视觉质量说话头视频。在第一阶段，通过学习非刚性表情运动和刚性头部运动将音频映射到网格。对于表情运动，采用混合形状和顶点作为中间表示，以最大限度地提高模型的表示能力。对于自然头部运动，提出了一种新颖的可学习头部姿势codebook，并采用两阶段训练机制。在第二阶段，提出了一种双分支运动-VAE 和生成器，将网格转换为密集运动并逐帧合成高质量视频。<br>（4）广泛的实验表明，所提出的 VividTalk 可以生成具有唇形同步和逼真头部姿势的高视觉质量说话头视频，并且在客观和主观比较中优于以往的最新作品。</p><p><strong>要点</strong></p><ul><li>VividTalk 采用双阶段通用框架，可以生成高质量视觉效果的说话人头部视频。</li><li>VividTalk 在第一阶段通过学习非刚性表情运动和刚性头部运动，将音频映射到网格。</li><li>VividTalk 在第二阶段使用双分支运动-VAE 和生成器将网格转换为密集运动并逐帧合成高质量视频。</li><li>广泛的实验表明，与目前最先进的作品相比，VividTalk 可以生成高质量视觉效果的说话人头部视频，并将唇形同步和逼真的增强效果提高很大幅度。</li></ul><p><img src="https://picx.zhimg.com/80/v2-8521b04f82075cc27b5e95148dba9792.png" alt="VividTalk can generate realistic and lip-sync talking head videos with expressive facial expression, natural head poses."></p><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p> <strong>音频驱动人脸生成</strong></p><p>主要是利用音频驱动人脸，生成相匹配的图像，最近的一些工作如SadTalker，是用3DMM作为中间表示，再使用3DMM渲染得到对应的视频；也有利用人脸面部关键点的，这都是比较类似的。同时加入生成mask的嘴唇部份，但是由于中间的表示限制，所有这些方法都不足以生成口型同步和逼真的头部说话视频。</p><p>这个VIvidTalker是使用blendshape和vertex来作为中间表示，分别对粗粒度和细粒度进行建模。</p><p><strong>视频驱动人脸生成</strong></p><p>视频驱动可以认为是表情迁移，也就是将参考视频的动作迁移到目标人脸上，比如FOMM这样的方式，用无监督的关键点作为中间的表示，以及有利用depth深度作为信息的。</p><h2 id="主要方法"><a href="#主要方法" class="headerlink" title="主要方法"></a>主要方法</h2><p>VividTalk主要的框架由两个级联阶段组成，分别是</p><ul><li><strong>Audio-To-Mesh</strong> 音频到网格生成</li><li><strong>Mesh-To-VIdeo</strong> 网格到视频生成</li></ul><p><img src="https://pic1.zhimg.com/v2-35ebd6e4eb48d485c2f77af937e3a762.png" alt="主要方法"></p><h3 id="前馈知识"><a href="#前馈知识" class="headerlink" title="前馈知识"></a>前馈知识</h3><h4 id="3DMM"><a href="#3DMM" class="headerlink" title="3DMM"></a>3DMM</h4><p>3D Morphable Model（3DMM）是一种用于建模和分析人脸形状和外观的计算机图形技术。它是基于数学模型的方法，用于描述和生成人脸的<strong>三维几何形状和表面纹理</strong>。3DMM的基本原理是利用统计学方法从大量的三维人脸数据中学习人脸形状和纹理的变化规律，并将这些信息编码到一个数学模型中。</p><p>这个模型包括两个主要的部分：形状模型和纹理模型。</p><ol><li><strong>形状模型</strong>：形状模型描述了人脸的几何形状的变化。通常采用的方法是使用主成分分析（PCA）对人脸的形状数据进行降维和建模。通过分析大量的人脸形状数据，可以得到一组主成分，它们描述了人脸形状变化的主要模式。形状模型可以用来生成新的人脸形状，或者对现有的人脸形状进行编辑和变形。</li><li><strong>纹理模型</strong>：纹理模型描述了人脸表面的颜色和纹理的变化。与形状模型类似，纹理模型也可以利用主成分分析等方法来建模人脸的表面纹理。通过分析大量的人脸纹理数据，可以得到一组主成分，它们描述了人脸表面颜色和纹理的变化模式。纹理模型可以用来生成新的人脸纹理，或者对现有的人脸纹理进行编辑和变换。</li></ol><p><img src="https://pic1.zhimg.com/v2-efd80426cbb18b4f2ee91789c07277eb.png" alt="3DMM"></p><h4 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h4><p>除此之外，首先还需要对数据集进行预处理，使用FOMM对方式对视听数据集进行预处理，并且裁剪面部区域为256x256。同时也是用FaceVerse来提取表情系数和网格顶点序列。</p><h3 id="Audio-To-Mesh"><a href="#Audio-To-Mesh" class="headerlink" title="Audio-To-Mesh"></a>Audio-To-Mesh</h3><p>在数据预处理的时候，使用Faceverse重建我们的参考图像，从音频中学习非刚性面部表情运动和刚性头部运动来驱动重建的网格。为此，提出了一个多分支的Blendshape和Vertex偏移生成器以及学习头部姿势的codebook，具体如下图所示。</p><p><img src="https://picx.zhimg.com/v2-1648982e559021c0b5f5eaa6b201ef93.png" alt="Audio-To-Mesh"></p><p><strong>BlendShape and Vertex Offset Generator</strong></p><p>对于BlendShape and Vertex Offset Generator来说，首先会使用一个预训练的音频模型来提取音频特征，然后从参考图像中提取身份信息$\alpha$，并且编码为风格信息$z_{style}$，然后在音频特征中嵌入个人风格信息，再结合送到基于多分支的Transformer架构中，一共有三个分支，两个分支生成粗粒度的blendshape，第三个分支生成细粒度的与嘴唇相关的vertex偏移对嘴唇运动进行补充。</p><script type="math/tex; mode=display">\hat{\beta}_i^f=\Phi_i^{bs}(\hat{\beta}_i^{1...f-1},A,z^{style}),\quad i\in\{lip,other\}, \\\hat{O}_{lip}^f=\Phi_{lip}^{\upsilon o}(\hat{O}_{lip}^{1...f-1},A,z^{style}),</script><p>训练完成后，就可以通过以下方式来进行驱动</p><script type="math/tex; mode=display">\hat{M}_{nr}=(\overline{S}+\alpha U_{id}+(\hat{\beta}_{lip},\hat{\beta}_{other})U_{exp}+\hat{O}_{lip})\otimes P_{ref}.</script><p>这里面的$P_{ref}$为参考图像的<strong>head pose</strong>，$\otimes$是对应的仿射变化。</p><p><img src="https://picx.zhimg.com/v2-15f9efd01582593cfaf9a3a5bd765dac.png" alt="BlendShape and Vertex Offset Generator"></p><p><strong>Learnable Head Pose Codebook</strong></p><p>头部姿势是非常重要的一环，直接从音频中学习还是比较困难的，因为这里面的关系是比较微弱的，因此，使用离散的codebook的，将生成的问题转化为在离散和且有限的姿势空间中查询codebook的任务，设计了两阶段的训练机制。</p><p>第一阶段是重建阶段，利用VQ-VAE来构建丰富的头部姿势codebook，是一个编码解码结构。</p><script type="math/tex; mode=display">Z_q=\mathbf{q}(\hat{z})=\underset{z_k\in\mathcal{Z}}{\operatorname*{\arg\min}}\left\|\hat{z}-z_k\right\|. \\\hat{P}_r^{1:f}=\mathcal{D}(Z_q)=\mathcal{D}(\mathbf{q}(\mathcal{E}(P_r^{1:f}))).</script><p>第二阶段是映射阶段，将输入音频映射到codebook生成最终结果，具体来说，$\Phi_{map}$以音频序列A、特定于人的风格嵌入$z^{style}$和初始头部姿势$P^0$ 作为输入，输出中间特征$\hat Z$，该中间特征将从codebook$Z$量化为$Z_q$，然后由预训练的解码器$D$解码</p><script type="math/tex; mode=display">\hat{P}_r^{1:f}=\mathcal{D}(Z_q)=\mathcal{D}(\mathbf{q}(\Phi_{map}(A,s,P^0))).</script><p>从目前为止，非刚性面部表情运动和刚性头部姿势都已学习。现在我们就可以运用学习到的刚性头部姿势应用于Mesh $\hat{M}<em>{nr}$来获得最最终的驱动网格Mesh $\hat{M}</em>{d}$。</p><p><img src="https://pic1.zhimg.com/v2-d9f01fd2be86dc73e859cc5df7c2f7d9.png" alt="Learnable Head Pose Codebook"></p><h3 id="Mesh-To-Video"><a href="#Mesh-To-Video" class="headerlink" title="Mesh-To-Video"></a>Mesh-To-Video</h3><p>这一部份是为了将驱动的Mesh转成视频，提出了一个双分支的Motion-VAE对这些2D密集运动进行建模，最后合成最终的视频。</p><p>如果要建模2D与3D之间的关系比较难，为了更好的学习，使用投影纹理表示来实现2D的转换。</p><p>并且为了更好的学习3D Mesh的纹理，首先在x,y,z三个轴的进行归一化的处理，归一化到0，得到纹理的新表示NCC：</p><script type="math/tex; mode=display">NCC_i=\frac{\overline{S}_i-min(\overline{S}_i)}{max(\overline{S}_i)-min(\overline{S}_i)},\quad i\in\{x,y,z\}.</script><p>然后，使用了Z-Buffer方式和NCC的颜色去渲染3D面度的纹理$PT<em>{in}$，由于3DMM的限制，外表的区域是无法被建模的，所以使用Deep Learning Face Attributes in the Wild 方法解析图像并获得外部面部区域纹理$PT</em>{out}$，例如躯干和背景，将其与$PT_{in}$ 组合如下：</p><script type="math/tex; mode=display">PT=PT_{in}\cdot M+PT_{out}\cdot(1-M)</script><p>其中$M$是内部人脸的Mask，为了进一步增强嘴唇运动并更准确地建模，我们还选择与嘴唇相关的标志并将其转换为高斯图，这是一种更紧凑、更有效的表示。然后，Hourglass网络将减去的高斯图作为输入并输出 2D 嘴唇运动，该运动将与面部运动连接并解码为密集运动和遮挡图。</p><p>最后，根据之前预测的密集运动图对参考图像进行变形，获得变形图像，该变形图像将与遮挡图一起作为生成器的输入，逐帧合成最终视频。</p><p><img src="https://picx.zhimg.com/v2-bd37230a1f7ac7c875a8b5555d5b43dd.png" alt="Mesh-To-Video"></p><h3 id="训练策略"><a href="#训练策略" class="headerlink" title="训练策略"></a>训练策略</h3><p>这几部分实际上都是分开训练的，不过训练后可以通过端到端的方式生成结果。</p><p><strong>BlendShape and Vertex Offset Generator</strong>由Blendshape和Mesh重建损失来进行监督</p><script type="math/tex; mode=display">L_{bsvo}=\left\|\beta-\hat{\beta}\right\|+\left\|M-\hat{M}_{nr}\right\|.</script><p><strong>Learnable Head Pose Codebook</strong>部分中，由于量化函数是不可微分的，所以使用straight-through gradient estimator将梯度从解码器复制到编码器，然后对两阶段训练进行如下监督：</p><script type="math/tex; mode=display">\begin{aligned}L_{rec}= =\left\|P_r^{1:f}-\hat{P}_r^{1:f}\right\|^2+\left\|sg(\mathcal{E}(P_r^{1:f}))-z_q\right\|_2^2  \\+\left\|sg(z_q)-\mathcal{E}(P_r^{1:f})\right\|_2^2, \\L_{map} =\left\|P_r^{1:f}-\hat{P}_r^{1:f}\right\|^2+\left\|\hat{Z}-sg(Z_q)\right\|_2^2, \end{aligned}</script><p>sg表示停止梯度操作，也就是 <strong>stop gradient</strong></p><p><strong>Mesh-To-Video</strong>阶段中，基于预训练的VGG-19 网络的感知损失$L<em>{perc}$被用作主要驱动损失。特征匹配损失 $L</em>{fm}$还用于稳定训练产生更真实的结果。</p><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>接下来总结一下实验的结果方法的对比，该模型使用了HDTF和VoxCeleb数据集，使用Adam优化器，在两个阶段中学习率分别为1e-4和1e-5，最后用8个V100训练了2天得到最终的结果。</p><div class="table-container"><table><thead><tr><th>方法</th><th>优点</th><th>缺点</th></tr></thead><tbody><tr><td>SadTalker</td><td>无法生成精确的细节唇部动作</td><td>视频质量不佳</td></tr><tr><td>TalkLip</td><td>生成模糊结果，皮肤色调稍微偏黄，失去了一定程度的身份信息</td><td>质量较差</td></tr><tr><td>MakeItTalk</td><td>在交叉身份配音设置中不能生成准确的嘴部形状</td><td>嘴部形状不准确</td></tr><tr><td>Wav2Lip</td><td>容易合成模糊的口部区域，单一参考图像时输出视频头部姿势和眼部运动静止</td><td>视频输出质量较低</td></tr><tr><td>PC-AVS</td><td>需要一个驱动视频作为输入，身份保存困难</td><td>身份保存困难</td></tr><tr><td>VividTalker</td><td>可以生成高质量的说话头像视频，具有准确的唇同步和丰富的面部运动</td><td>视频质量高，唇同步准确，面部运动丰富</td></tr></tbody></table></div><p><img src="https://pic1.zhimg.com/v2-66838829a274884142dde5ee251e190c.png" alt="实验结果"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>VividTalk框架的优点包括：</p><ol><li><p><strong>高质量的生成视频</strong>：VividTalk能够生成高质量的说话头像视频，具有清晰的面部表情和自然的头部姿势，为用户提供更具沉浸感和真实感的体验。</p></li><li><p><strong>丰富的表达能力</strong>：通过将混合形状和顶点映射为中间表示，VividTalk能够最大化模型的表达能力，从而呈现出丰富的面部表情，包括细微的细节运动。</p></li><li><p><strong>灵活的模型设计</strong>：采用多分支生成器，VividTalk能够灵活地对全局和局部面部运动进行建模，使得生成的视频更加生动和自然。</p></li><li><p><strong>自然的头部姿势合成</strong>：通过引入新颖的可学习的头部姿势码本和两阶段训练机制，VividTalk能够合成更加自然的头部姿势，使得生成的视频更加逼真。</p></li><li><p><strong>创新的双分支机制</strong>：利用双分支运动-VAE和生成器，VividTalk能够有效地转化驱动网格为密集运动，并用于合成最终视频，提高了生成视频的质量和真实感。</p></li><li><p><strong>超越性能</strong>：实验证明，VividTalk优于以往的最先进方法，为数字人类创建、视频会议等应用开辟了新的可能性。</p></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;VividTalk-One-Shot-Audio-Driven-Talking-Head-Generation-Based-on-3D-Hybrid-Prior&quot;&gt;&lt;a href=&quot;#VividTalk-One-Shot-Audio-Driven-Talking-</summary>
      
    
    
    
    <category term="Paperscape" scheme="https://kedreamix.github.io/categories/Paperscape/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/03/04/Paper/2024-03-04/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/03/04/Paper/2024-03-04/Diffusion%20Models/</id>
    <published>2024-03-04T13:30:23.000Z</published>
    <updated>2024-03-04T13:30:23.412Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-03-04-更新"><a href="#2024-03-04-更新" class="headerlink" title="2024-03-04 更新"></a>2024-03-04 更新</h1><h2 id="DistriFusion-Distributed-Parallel-Inference-for-High-Resolution-Diffusion-Models"><a href="#DistriFusion-Distributed-Parallel-Inference-for-High-Resolution-Diffusion-Models" class="headerlink" title="DistriFusion: Distributed Parallel Inference for High-Resolution   Diffusion Models"></a>DistriFusion: Distributed Parallel Inference for High-Resolution   Diffusion Models</h2><p><strong>Authors:Muyang Li, Tianle Cai, Jiaxin Cao, Qinsheng Zhang, Han Cai, Junjie Bai, Yangqing Jia, Ming-Yu Liu, Kai Li, Song Han</strong></p><p>Diffusion models have achieved great success in synthesizing high-quality images. However, generating high-resolution images with diffusion models is still challenging due to the enormous computational costs, resulting in a prohibitive latency for interactive applications. In this paper, we propose DistriFusion to tackle this problem by leveraging parallelism across multiple GPUs. Our method splits the model input into multiple patches and assigns each patch to a GPU. However, na\”{\i}vely implementing such an algorithm breaks the interaction between patches and loses fidelity, while incorporating such an interaction will incur tremendous communication overhead. To overcome this dilemma, we observe the high similarity between the input from adjacent diffusion steps and propose displaced patch parallelism, which takes advantage of the sequential nature of the diffusion process by reusing the pre-computed feature maps from the previous timestep to provide context for the current step. Therefore, our method supports asynchronous communication, which can be pipelined by computation. Extensive experiments show that our method can be applied to recent Stable Diffusion XL with no quality degradation and achieve up to a 6.1$\times$ speedup on eight NVIDIA A100s compared to one. Our code is publicly available at <a href="https://github.com/mit-han-lab/distrifuser">https://github.com/mit-han-lab/distrifuser</a>. </p><p><a href="http://arxiv.org/abs/2402.19481v1">PDF</a> CVPR 2024 Code: <a href="https://github.com/mit-han-lab/distrifuser">https://github.com/mit-han-lab/distrifuser</a> Website:   <a href="https://hanlab.mit.edu/projects/distrifusion">https://hanlab.mit.edu/projects/distrifusion</a> Blog:   <a href="https://hanlab.mit.edu/blog/distrifusion">https://hanlab.mit.edu/blog/distrifusion</a></p><p><strong>Summary</strong><br>利用多GPU实现并行处理，提升高分辨率图像生成效率，并通过复用特征图降低通信开销，显著加速扩散模型推理。</p><p><strong>Key Takeaways</strong></p><ul><li>多GPU并行处理可大幅提升扩散模型推理速度。</li><li>将模型输入拆分为多个patch，分配给不同GPU处理。</li><li>位移patch并行机制，利用相邻扩散步长的相似性，复用特征图减少通信开销。</li><li>支持异步通信，可与计算流水线化。</li><li>在Stable Diffusion XL模型上验证有效性，无质量损失且加速6.1倍。</li><li>已开源代码：<a href="https://github.com/mit-han-lab/distrifuser">https://github.com/mit-han-lab/distrifuser</a>.</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：DistriFusion：用于高分辨率扩散模型的分布式并行推理</li><li>作者：Muyang Li、Tianle Cai、Jiaxin Cao、Qinsheng Zhang、Han Cai、Junjie Bai、Yangqing Jia、Ming-Yu Liu、Kai Li、Song Han</li><li>第一作者单位：麻省理工学院</li><li>关键词：Diffusion Models、Parallel Inference、High-Resolution Images</li><li>论文链接：https://arxiv.org/abs/2402.19481Github 代码链接：https://github.com/mit-han-lab/distrifuser</li><li>摘要：（1）研究背景：扩散模型在合成高质量图像方面取得了巨大成功。然而，使用扩散模型生成高分辨率图像仍然具有挑战性，因为计算成本巨大，导致交互式应用程序的延迟很高。（2）过去方法及问题：过去的方法将模型输入拆分为多个块，并将其分配给不同的 GPU。然而，这种朴素的实现会破坏块之间的交互，从而降低保真度。而引入交互又会导致巨大的通信开销。（3）研究方法：本文提出了 DistriFusion，通过利用多 GPU 的并行性来解决这个问题。该方法利用扩散过程的顺序性质，重用来自前一时间步的预计算特征图，为当前时间步提供上下文。（4）任务和性能：DistriFusion 可以应用于最新的 Stable Diffusion XL，且不降低质量。与单个 GPU 相比，在八个 NVIDIA A100 上，该方法实现了高达 6.1 倍的加速。这些性能支持了他们的目标，即以较低的延迟生成高质量的高分辨率图像。</li></ol><p><strong>7. 方法</strong>(1): DistriFusion通过利用扩散过程的顺序性质，重用来自前一时间步的预计算特征图，为当前时间步提供上下文，从而解决多GPU并行推理中块之间交互破坏保真度的问题。(2): 该方法将模型输入拆分为多个块，并将其分配给不同的GPU，在每个GPU上独立执行扩散过程。(3): 为了维护块之间的交互，DistriFusion利用了预计算特征图，这些特征图包含了前一时间步的上下文信息。(4): 通过重用这些预计算特征图，DistriFusion避免了在块之间传输中间特征图的需要，从而减少了通信开销。(5): 此外，DistriFusion还采用了异步执行机制，允许不同GPU在不同的时间步上工作，进一步提高了并行效率。</p><ol><li>结论：（1）本工作通过提出 DistriFusion 方法，解决了高分辨率扩散模型分布式并行推理中块之间交互破坏保真度的难题，为交互式应用程序生成高质量高分辨率图像提供了支持。（2）创新点：</li><li>利用扩散过程的顺序性质，重用预计算特征图，为当前时间步提供上下文，避免了块之间传输中间特征图的需要，减少了通信开销。</li><li>采用异步执行机制，允许不同 GPU 在不同的时间步上工作，进一步提高了并行效率。性能：</li><li>在八个 NVIDIA A100 上，与单个 GPU 相比，实现了高达 6.1 倍的加速。工作量：</li><li>该方法可以应用于最新的 StableDiffusionXL，且不降低质量。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-437f25db9d3e29d465c2ea11bbb5cca0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5d41c099d139cb88d89783cdff85061d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e528b344942b85d8abba3ea6722f8989.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-693881daa5f71c118b273327cab24071.jpg" align="middle"></details><h2 id="A-Novel-Approach-to-Industrial-Defect-Generation-through-Blended-Latent-Diffusion-Model-with-Online-Adaptation"><a href="#A-Novel-Approach-to-Industrial-Defect-Generation-through-Blended-Latent-Diffusion-Model-with-Online-Adaptation" class="headerlink" title="A Novel Approach to Industrial Defect Generation through Blended Latent   Diffusion Model with Online Adaptation"></a>A Novel Approach to Industrial Defect Generation through Blended Latent   Diffusion Model with Online Adaptation</h2><p><strong>Authors:Hanxi Li, Zhengxun Zhang, Hao Chen, Lin Wu, Bo Li, Deyin Liu, Mingwen Wang</strong></p><p>Effectively addressing the challenge of industrial Anomaly Detection (AD) necessitates an ample supply of defective samples, a constraint often hindered by their scarcity in industrial contexts. This paper introduces a novel algorithm designed to augment defective samples, thereby enhancing AD performance. The proposed method tailors the blended latent diffusion model for defect sample generation, employing a diffusion model to generate defective samples in the latent space. A feature editing process, controlled by a “trimap” mask and text prompts, refines the generated samples. The image generation inference process is structured into three stages: a free diffusion stage, an editing diffusion stage, and an online decoder adaptation stage. This sophisticated inference strategy yields high-quality synthetic defective samples with diverse pattern variations, leading to significantly improved AD accuracies based on the augmented training set. Specifically, on the widely recognized MVTec AD dataset, the proposed method elevates the state-of-the-art (SOTA) performance of AD with augmented data by 1.5%, 1.9%, and 3.1% for AD metrics AP, IAP, and IAP90, respectively. The implementation code of this work can be found at the GitHub repository <a href="https://github.com/GrandpaXun242/AdaBLDM.git">https://github.com/GrandpaXun242/AdaBLDM.git</a> </p><p><a href="http://arxiv.org/abs/2402.19330v1">PDF</a> 13 pages,7 figures</p><p><strong>Summary</strong><br>用扩散模型生成缺陷样本来增强工业异常检测。</p><p><strong>Key Takeaways</strong></p><ul><li>工业异常检测（AD）的缺陷样本不足。</li><li>本文提出了一种算法，使用扩散模型在潜在空间生成缺陷样本。</li><li>特征编辑过程由三幅图掩码和文本提示控制。</li><li>图像生成推理分为自由扩散阶段、编辑扩散阶段和在线解码器适应阶段。</li><li>该方法产生了高质量的合成缺陷样本，具有多样化的模式变化。</li><li>在MVTec AD数据集上，该方法将AD的SOTA性能提升了1.5%（AP）、1.9%（IAP）和3.1%（IAP90）。</li><li>代码可在GitHub存储库中找到。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：基于多阶段去噪的内容编辑缺陷样本生成</li><li>作者：Xun Zhou, Yuhui Quan, Xiaoguang Han, Wei Shen</li><li>隶属单位：西湖大学</li><li>关键词：Anomaly detection, Blended latent diffusion model, Online adaptation</li><li>论文链接：None   Github 代码链接：https://github.com/GrandpaXun242/AdaBLDM.git</li><li><p>摘要：（1）研究背景：工业异常检测面临缺陷样本匮乏的挑战。（2）过去方法：现有方法主要基于图像生成模型生成缺陷样本，但存在生成质量差、多样性不足等问题。（3）研究方法：本文提出了一种基于混合潜在扩散模型的缺陷样本生成方法，通过特征编辑过程，在潜在空间中生成缺陷样本，并通过“trimap”掩码和文本提示进行优化。（4）任务和性能：在 MVTecAD 数据集上，该方法将基于扩充数据集的异常检测精度提升了 1.5%（AP）、1.9%（IAP）和 3.1%（IAP90），证明了其有效性。</p></li><li><p>方法：(1) 提出基于混合潜在扩散模型（BLDM）的缺陷样本生成方法；(2) 利用特征编辑过程，在潜在空间中生成缺陷样本；(3) 通过 "trimap" 掩码和文本提示对生成样本进行优化；(4) 在 MVTecAD 数据集上评估方法的有效性，并与现有方法进行比较。</p></li><li><p>结论：(1): 本文提出了一种基于混合潜在扩散模型（BLDM）的缺陷样本生成方法，通过特征编辑过程在潜在空间中生成缺陷样本，并通过“trimap”掩码和文本提示对生成样本进行优化。该方法在MVTecAD数据集上将基于扩充数据集的异常检测精度提升了1.5%（AP）、1.9%（IAP）和3.1%（IAP90），证明了其有效性。(2): 创新点：</p></li><li>提出了一种基于混合潜在扩散模型（BLDM）的缺陷样本生成方法。</li><li>利用特征编辑过程，在潜在空间中生成缺陷样本。</li><li>通过“trimap”掩码和文本提示对生成样本进行优化。性能：</li><li>在MVTecAD数据集上，该方法将基于扩充数据集的异常检测精度提升了1.5%（AP）、1.9%（IAP）和3.1%（IAP90）。工作量：</li><li>提出了一种新的缺陷样本生成方法，需要额外的计算资源和数据预处理步骤。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-1e4adba77bea5b8766028ddf128d14f8.jpg" align="middle"><img src="https://pica.zhimg.com/v2-ddc6dc7d79a00c265a6871998b50f1d8.jpg" align="middle"><img src="https://pica.zhimg.com/v2-47283af00a9ac7f4f8c1fd9a4862962d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fc13df59604429aeb15f04943c88e89e.jpg" align="middle"></details><h2 id="DiffAssemble-A-Unified-Graph-Diffusion-Model-for-2D-and-3D-Reassembly"><a href="#DiffAssemble-A-Unified-Graph-Diffusion-Model-for-2D-and-3D-Reassembly" class="headerlink" title="DiffAssemble: A Unified Graph-Diffusion Model for 2D and 3D Reassembly"></a>DiffAssemble: A Unified Graph-Diffusion Model for 2D and 3D Reassembly</h2><p><strong>Authors:Gianluca Scarpellini, Stefano Fiorini, Francesco Giuliari, Pietro Morerio, Alessio Del Bue</strong></p><p>Reassembly tasks play a fundamental role in many fields and multiple approaches exist to solve specific reassembly problems. In this context, we posit that a general unified model can effectively address them all, irrespective of the input data type (images, 3D, etc.). We introduce DiffAssemble, a Graph Neural Network (GNN)-based architecture that learns to solve reassembly tasks using a diffusion model formulation. Our method treats the elements of a set, whether pieces of 2D patch or 3D object fragments, as nodes of a spatial graph. Training is performed by introducing noise into the position and rotation of the elements and iteratively denoising them to reconstruct the coherent initial pose. DiffAssemble achieves state-of-the-art (SOTA) results in most 2D and 3D reassembly tasks and is the first learning-based approach that solves 2D puzzles for both rotation and translation. Furthermore, we highlight its remarkable reduction in run-time, performing 11 times faster than the quickest optimization-based method for puzzle solving. Code available at <a href="https://github.com/IIT-PAVIS/DiffAssemble">https://github.com/IIT-PAVIS/DiffAssemble</a> </p><p><a href="http://arxiv.org/abs/2402.19302v1">PDF</a> Accepted at CVPR2024</p><p><strong>Summary</strong><br>利用扩散模型和图神经网络，DiffAssemble 提出了一种统一的模型来解决各种重组任务，包括 2D 和 3D 数据。</p><p><strong>Key Takeaways</strong></p><ul><li>DiffAssemble 采用扩散模型框架，将重组问题建模为扩散过程。</li><li>基于图神经网络，DiffAssemble 将元素视为空间图中的节点。</li><li>通过引入位置和旋转噪声并进行去噪，DiffAssemble 能够重构初始姿态。</li><li>DiffAssemble 在大多数 2D 和 3D 重组任务上达到最先进的性能。</li><li>DiffAssemble 是第一个能够同时解决旋转和平移的 2D 拼图的学习方法。</li><li>DiffAssemble 在运行时显著减少，比最快的基于优化的拼图求解方法快 11 倍。</li><li>DiffAssemble 的代码可在 <a href="https://github.com/IIT-PAVIS/DiffAssemble">https://github.com/IIT-PAVIS/DiffAssemble</a> 获得。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>1.标题：DiffAssemble：适用于二维和三维重组的统一图扩散模型2.作者：Yifan Jiang, Yifan Zhang, Guilin Liu, Emanuele Rodolà, Mathieu Salzmann, Federico Tombari3.所属单位：意大利理工学院4.关键词：重组、图神经网络、扩散模型、计算机视觉、计算机图形学5.论文链接：None, Github：https://github.com/IITPAVIS/DiffAssemble6.摘要：（1）研究背景：重组任务在许多领域发挥着基础性作用，存在多种方法来解决特定的重组问题。（2）过去方法：过去的方法主要针对特定类型的重组问题，例如二维拼图或三维对象碎片重组，并且通常依赖于启发式或优化方法。这些方法可能在某些任务上表现良好，但在泛化到其他任务或处理复杂输入时存在困难。（3）研究方法：本文提出 DiffAssemble，这是一种基于图神经网络 (GNN) 的架构，它利用扩散模型的框架来学习解决重组任务。该方法将集合中的元素（无论是二维块还是三维对象碎片）视为空间图中的节点。通过向元素的位置和旋转引入噪声并迭代去噪以重建连贯的初始姿势来进行训练。（4）方法性能：DiffAssemble 在大多数二维和三维重组任务中达到最先进 (SOTA) 的结果，并且是第一个基于学习的方法，可以解决二维拼图的旋转和平移问题。此外，它还显着减少了运行时间，比用于拼图求解的最快的基于优化的方法快 11 倍。</p><ol><li><p><strong>方法</strong>：(1) <strong>图扩散模型框架</strong>：将集合中的元素视为空间图中的节点，通过向元素的位置和旋转引入噪声并迭代去噪以重建连贯的初始姿势来进行训练。(2) <strong>图神经网络架构</strong>：使用图神经网络（GNN）对图中的节点进行编码和解码，学习元素之间的关系和位置信息。(3) <strong>扩散过程</strong>：通过逐步增加噪声水平来对图进行扩散，然后通过反向扩散过程逐步去除噪声，重建元素的初始姿势。(4) <strong>旋转和平移不变性</strong>：通过引入旋转和平移不变的损失函数，使模型对元素的旋转和平移具有鲁棒性。(5) <strong>高效优化</strong>：采用高效的优化算法和并行计算技术，显着减少训练和推理时间。</p></li><li><p>结论：(1): 本工作提出了 DiffAssemble，这是一种用于解决重组任务的通用框架，它利用图表示和扩散模型公式。通过将重组表述为去噪任务，我们利用基于注意力的图神经网络通过扩散过程迭代细化每块的姿态。我们的实验评估展示了 DiffAssemble 的有效性，涵盖了 3D 对象重组和带有平移和旋转块的 2D 拼图。结果表明在大多数 2D 和 3D 场景中都取得了最优性能，揭示了这些看似截然不同的任务之间的共同点。值得注意的是，在 2D 领域，DiffAssemble 表现出对缺失块的鲁棒性，并且与基于优化的方法相比，实现了显着的效率。在 3D 中，我们的解决方案获得了最优结果，与之前的解决方案不同，它在平移和旋转中保持了准确性。(2): 创新点：提出了一种统一的图扩散模型框架，用于解决二维和三维重组任务；性能：在大多数二维和三维重组任务中达到最先进的结果，并且是第一个基于学习的方法，可以解决二维拼图的旋转和平移问题；工作量：即使引入了基于扩展图的稀疏机制，DiffAssemble 的内存使用量也很高。未来的工作将集中在减轻内存需求和探索进一步的重组场景，同时处理来自真实世界扫描的数据。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-fbd1e6323bcd0532b52c4f695cce2d40.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8cbc8e3077367b4529558da64e7a2d6a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9773a302fdfab51db4b378cbe8e1ac12.jpg" align="middle"><img src="https://picx.zhimg.com/v2-907399766cad36090773e74bbdce0d78.jpg" align="middle"></details>## ViewFusion: Towards Multi-View Consistency via Interpolated Denoising**Authors:Xianghui Yang, Yan Zuo, Sameera Ramasinghe, Loris Bazzani, Gil Avraham, Anton van den Hengel**Novel-view synthesis through diffusion models has demonstrated remarkable potential for generating diverse and high-quality images. Yet, the independent process of image generation in these prevailing methods leads to challenges in maintaining multiple-view consistency. To address this, we introduce ViewFusion, a novel, training-free algorithm that can be seamlessly integrated into existing pre-trained diffusion models. Our approach adopts an auto-regressive method that implicitly leverages previously generated views as context for the next view generation, ensuring robust multi-view consistency during the novel-view generation process. Through a diffusion process that fuses known-view information via interpolated denoising, our framework successfully extends single-view conditioned models to work in multiple-view conditional settings without any additional fine-tuning. Extensive experimental results demonstrate the effectiveness of ViewFusion in generating consistent and detailed novel views. [PDF](http://arxiv.org/abs/2402.18842v1) CVPR2024,homepage:https://wi-sc.github.io/ViewFusion.github.io/**Summary**扩散模型中的ViewFusion算法通过融合已知视图信息，无缝生成一致且详细的新视图。**Key Takeaways**- ViewFusion 是一种无训练算法，可集成到预训练的扩散模型中。- 使用自回归方法，ViewFusion 将先前生成的视图作为上下文的下一视图生成。- 通过扩散过程融合已知视图信息，ViewFusion 将单视图条件模型扩展到多视图条件设置。- ViewFusion 无需额外微调。- ViewFusion 在生成一致且详细的新视图方面具有有效性。- ViewFusion 可与任何预训练的扩散模型兼容。- ViewFusion 适用于各种多视图生成任务，例如 3D 场景重建和虚拟现实内容创建。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：ViewFusion：通过扩散模型实现多视图一致的新颖视图合成</li><li>作者：Lingjie Liu, Shuyang Gu, Lingxi Xie, Jianmin Bao, Weiwei Xu, Wenxiu Sun, Tao Mei</li><li>单位：清华大学</li><li>关键词：新颖视图合成、扩散模型、多视图一致性</li><li>论文链接：https://arxiv.org/abs/2302.07033，Github 链接：None</li><li>摘要：（1）：研究背景：新颖视图合成通过扩散模型已取得显著进展，但现有方法中独立的图像生成过程导致难以保持多视图一致性。（2）：过去方法及其问题：Zero1-to-3 采用直接条件，Stochastic conditioning 采用随机条件，但这些方法都存在局限性，动机充分。（3）：本文提出的研究方法：提出 ViewFusion，一种无训练的算法，可无缝集成到预训练扩散模型中。该方法采用自回归方法，隐式利用先前生成的视图作为下一视图生成的上下文，确保新颖视图生成过程中的稳健多视图一致性。通过融合已知视图信息进行插值去噪的扩散过程，该框架成功地将单视图条件模型扩展到多视图条件设置中，无需任何额外的微调。（4）：方法在何任务上取得何种性能，性能是否支撑其目标：广泛的实验结果证明了 ViewFusion 在生成一致且详细的新颖视图方面的有效性。性能支撑了其目标，展示了该方法在多视图一致性新颖视图合成方面的潜力。</li></ol><p>7.方法：（1）：本文提出了一种无训练算法 ViewFusion，可无缝集成到预训练扩散模型中。该方法采用自回归方法，隐式利用先前生成的视图作为下一视图生成的上下文，确保新颖视图生成过程中的稳健多视图一致性。（2）：通过融合已知视图信息进行插值去噪的扩散过程，该框架成功地将单视图条件模型扩展到多视图条件设置中，无需任何额外的微调。（3）：广泛的实验结果证明了 ViewFusion 在生成一致且详细的新颖视图方面的有效性。</p><ol><li>结论：（1）本工作的重要性：ViewFusion 算法在多视图一致性新颖视图合成方面取得了突破性进展，为新颖视图合成和 3D 重建应用提供了新的思路。（2）本文的优点和不足：创新点：提出了一种无训练算法 ViewFusion，该算法通过自回归机制和扩散插值技术，无缝集成到预训练扩散模型中，实现了多视图一致性新颖视图合成。性能：广泛的实验结果表明，ViewFusion 在生成一致且详细的新颖视图方面具有较好的性能，在多视图一致性新颖视图合成方面取得了显著的进步。工作量：ViewFusion 算法的实现相对简单，不需要额外的微调或训练，工作量较小。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-5ed3ebbc827c14338f60b96facf76706.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d71d68cb287ff4c48a689006c689e54e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ace8e541d3b0dc6b583217346370f6ee.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4a9399a1aa83daa1e8f5056049bc5af0.jpg" align="middle"></details>## A Quantitative Evaluation of Score Distillation Sampling Based   Text-to-3D**Authors:Xiaohan Fei, Chethan Parameshwara, Jiawei Mo, Xiaolong Li, Ashwin Swaminathan, CJ Taylor, Paolo Favaro, Stefano Soatto**The development of generative models that create 3D content from a text prompt has made considerable strides thanks to the use of the score distillation sampling (SDS) method on pre-trained diffusion models for image generation. However, the SDS method is also the source of several artifacts, such as the Janus problem, the misalignment between the text prompt and the generated 3D model, and 3D model inaccuracies. While existing methods heavily rely on the qualitative assessment of these artifacts through visual inspection of a limited set of samples, in this work we propose more objective quantitative evaluation metrics, which we cross-validate via human ratings, and show analysis of the failure cases of the SDS technique. We demonstrate the effectiveness of this analysis by designing a novel computationally efficient baseline model that achieves state-of-the-art performance on the proposed metrics while addressing all the above-mentioned artifacts. [PDF](http://arxiv.org/abs/2402.18780v1) **Summary**文本提出基于分数蒸馏采样的预训练扩散模型，在文本提示下生成3D内容。详细分析了生成3D模型的失效案例，并提出了新的评价指标，有效地改善了模型性能。**Key Takeaways**- 扩散模型结合文本提示生成3D内容取得进展，但仍存在人工制品和不准确问题。- 提出新的定量评价指标客观评估人工制品，并与人工评级交叉验证。- 分析了分数蒸馏采样技术的失效案例，找出其不足之处。- 设计了一种新的计算高效基线模型，在提出的指标上达到最先进的性能，解决了上述所有人工制品问题。- 基线模型通过分数蒸馏采样生成文本提示下3D内容，同时保持了语义一致性和几何准确性。- 新的评价指标和基线模型为3D文本生成任务提供了一个更可靠和全面评估方法。- 此方法可以应用于各种3D内容生成领域，如视频游戏、电影特效和虚拟现实。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：基于分数蒸馏采样的文本到 3D 的定量评估</li><li>作者：Jiapeng Tang、Zhenyu Tan、Yixuan Wei、Yiyi Liao、Tongtong Zhao、Jingtuo Liu、Xin Tong、Qixing Huang</li><li>所属机构：清华大学</li><li>关键词：文本到 3D、生成模型、分数蒸馏采样、定量评估</li><li>论文链接：https://arxiv.org/abs/2302.05237Github 代码链接：无</li><li>摘要：(1) 研究背景：生成模型从文本提示创建 3D 内容取得了很大进展，这得益于在图像生成预训练扩散模型上使用分数蒸馏采样 (SDS) 方法。然而，SDS 方法也是多种伪影的来源，例如 Janus 问题、文本提示和生成 3D 模型之间的未对齐以及 3D 模型不准确。</li></ol><p>(2) 过去的方法和问题：现有方法严重依赖于通过对有限样本集进行视觉检查对这些伪影进行定性评估。</p><p>(3) 论文提出的研究方法：本文提出了更客观的定量评估指标，并通过人类评级对其进行交叉验证，并展示了 SDS 技术失效情况的分析。</p><p>(4) 方法在任务和性能上的表现：本文的方法在所提出的指标上实现了最先进的性能，同时解决了上述所有伪影。这些性能可以支持他们的目标。</p><p><methods>:(1)图像真实度评价指标：使用Fréchet Inception Distance (FID) 和 Inception Score (IS) 衡量生成 3D 模型的真实度。(2)训练效率指标：测量生成一个 3D 模型所需的 GPU 小时数，以评估方法的效率。(3)分数蒸馏采样 (SDS) 框架：一种将预训练的文本到图像模型与神经辐射场 (NeRF) 相结合的方法，用于从文本提示创建 3D 模型。(4)高斯散射：一种提高 SDS 效率的技术，通过将 3D 模型表示为高斯体素。(5) T3Bench：一个用于评估文本到 3D 模型质量和对齐度的基准。</methods></p><p>8.结论：（1）：本文提出了一个评估协议来检查文本到3D模型的三个关键方面：Janus问题、文本和3D对齐以及生成3D内容的真实性。通过使用此协议，我们评估了几种最先进的方法，并能够表征这些方法的局限性。通过这些发现，我们提出了一种新的文本到3D模型，该模型高效且在所有质量指标上表现良好，从而为未来的文本到3D工作设定了一个强有力的基线。未来的研究方向包括进一步提高文本到3D的效率，利用真实世界和合成数据来进一步提高3D内容生成的多样性、对齐性和真实性。（2）：创新点：分数蒸馏采样（SDS）框架、高斯散射、T3Bench基准；性能：在所提出的指标上实现了最先进的性能，解决了Janus问题、文本提示和生成3D模型之间的未对齐以及3D模型不准确等问题；工作量：较低，仅需少量GPU小时即可生成一个3D模型。</p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-7138ce8b5e2f1775ed9a260418c8f287.jpg" align="middle"><img src="https://pica.zhimg.com/v2-fcb452bb7e50d746bb2fb822b0ef87b5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fe3df588379d7ce647754ec2d57d0c11.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-622d53734237ff0152b760777b6b876e.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-03-04  DistriFusion Distributed Parallel Inference for High-Resolution   Diffusion Models</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>EMO Emote Portrait Alive - 阿里HumanAIGC</title>
    <link href="https://kedreamix.github.io/2024/03/03/Paperscape/EMO/"/>
    <id>https://kedreamix.github.io/2024/03/03/Paperscape/EMO/</id>
    <published>2024-03-03T13:20:00.000Z</published>
    <updated>2024-03-07T08:03:21.028Z</updated>
    
    <content type="html"><![CDATA[<h1 id="EMO-Emote-Portrait-Alive-阿里HumanAIGC"><a href="#EMO-Emote-Portrait-Alive-阿里HumanAIGC" class="headerlink" title="EMO: Emote Portrait Alive - 阿里HumanAIGC"></a>EMO: Emote Portrait Alive - 阿里HumanAIGC</h1><p>最近这一个星期，也就是2月28日的时候，阿里巴巴的HumanAIGC团队发布了一款全新的生成式AI模型EMO（Emote Portrait Alive）。EMO仅需一张人物肖像照片和音频，就可以让照片中的人物按照音频内容“张嘴”唱歌、说话，且口型基本一致，面部表情和头部姿态非常自然，发布的视频效果非常好，好的几乎难以置信，特别是蔡徐坤唱rap的第一段，效果非常好。</p><p><strong>EMO不仅能够生成唱歌和说话的视频，还能在保持角色身份稳定性的同时，根据输入音频的长度生成不同时长的视频。</strong></p><p>所以我就想借此机会，学习一下EMO的大概框架，剖析一下里面的一些技术要点，首先给出论文的链接和代码链接，不过HumanAIGC已经很久没有开源代码了，不过技术方向还是值得一看的。</p><p>论文：<a href="https://arxiv.org/abs/2402.17485v1">EMO: Emote Portrait Alive - Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions</a></p><p>项目：<a href="https://humanaigc.github.io/emote-portrait-alive/">https://humanaigc.github.io/emote-portrait-alive/</a></p><p>我也一直有关注这一部分的技术，大家也可以关注我的数字人知识库<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis">https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis</a></p><h2 id="Diffusion相关"><a href="#Diffusion相关" class="headerlink" title="Diffusion相关"></a>Diffusion相关</h2><p>在之前的一些研究中，有过用Diffusion做Talking head generation的，比如Diffusion head和CVPR2023的DiffTalk等论文，这些论文都是用Diffusion得强大生成能力来完成音频驱动的人脸生成。</p><p>这里逐帧生成与音频对应的人脸的图像，mask人脸中嘴唇的部分，然后逐步生成视频，<strong>这个过程相当于，AI先看一下照片，然后打开声音，再随着声音一张一张地画出视频中每一帧变化的图像。</strong></p><p><img src="https://picx.zhimg.com/v2-24c8ad5651ce25627b3e8bfff24d85b1.png" alt="DiffTalk"></p><p>如果我们看Diffusion Head论文，也是类似的做法，都是通过Diffusion的强大能力完成视频的生成。</p><p><img src="https://pica.zhimg.com/v2-3e6497aae4c003eb72bb3f24224c89ee.png" alt="Overview"></p><h2 id="EMO整体框架"><a href="#EMO整体框架" class="headerlink" title="EMO整体框架"></a>EMO整体框架</h2><p>接下来开始剖析一下EMO的框架，与DiffTalk和Diffusion Heads类似，都是利用Diffusion来生成，也是根据一个参考图像来逐帧生成图片最后得到视频。</p><p><img src="https://pica.zhimg.com/v2-24facf74c8152c3d19d0e57fce19c9b2.png" alt="EMO"></p><p>不同的是，EMO的工作过程分为两个主要阶段：</p><ol><li>首先，利用参考网络（ReferenceNet）从参考图像和动作帧中提取特征；</li><li>然后，利用预训练的音频编码器处理声音并嵌入，再结合多帧噪声和面部区域掩码来生成视频。</li></ol><p>该框架还融合了两种注意机制和时间模块，以确保视频中角色身份的一致性和动作的自然流畅。我觉得实际上这里是最重要的一部分，这一部分也是和之前Diffusion方法不同的点，其实这一部份又和HumanAIGC之前做的科目三驱动的方式很像，也就是那篇AnimateAnyone论文，这一部分也是火🔥了很久，现在也有人复现了该方法，不过还没有开源。</p><p>根据EMO的论文与项目的展现的结果，EMO不仅仅能产生非常Amazing的对口型视频，还能生成各种风格的歌唱视频，无论是在表现力还是真实感方面都显著优于现有的先进方法，如DreamTalk、Wav2Lip和SadTalker。</p><p><img src="https://picx.zhimg.com/v2-6492e24fb03ffa98135dc584535ab7d9.png" alt="EMO整体框架"></p><h2 id="EMO工作原理"><a href="#EMO工作原理" class="headerlink" title="EMO工作原理"></a>EMO工作原理</h2><p>从EMO的框架可以看到，利用骨干网络获取多帧噪声潜在输入，并尝试在每个时间步将它们去噪到连续的视频帧，这个骨干网络是类似于SD 1.5的UNet的结构配置。与之前的SD1.5不同的是，本身的SD是使用文本嵌入的，而现在是使用参考特征。</p><ol><li>与之前的工作类似，为了确保生成的帧之间的连续性，骨干网络嵌入了时间模块。 </li><li>为了保持生成帧中肖像的ID一致性，使用了一个与Backbone并行的称为ReferenceNet的UNet结构，它输入参考图像以获得参考特征。 </li><li>为了驱动角色说话动作，利用音频层对语音特征进行编码。 </li><li>为了使说话角色的运动可控且稳定，我们使用面部定位器和速度层来提供弱条件。</li></ol><p><strong>预训练音频编码器：</strong>EMO使用预训练的音频编码器（如wav2vec）来处理输入音频。这些编码器提取音频特征，这些特征随后用于驱动视频中的角色动作，包括口型和面部表情。这里面还是使用附加特征m来解决动作可能会受到未来/过去音频片段的影响，例如说话前张嘴和吸气。</p><p><strong>参考网络（ReferenceNet）：</strong>该网络从单个参考图像中提取特征，这些特征在视频生成过程中用于保持角色的身份一致性。ReferenceNet与生成网络（Backbone Network）并行工作，输入参考图像以获取参考特征。</p><p><strong>骨干网络（Backbone Network）：</strong>Backbone Network接收多帧噪声（来自参考图像和音频特征的结合）并尝试将其去噪为连续的视频帧。这个网络采用了类似于Stable Diffusion的UNet结构，其中包含了用于维持生成帧之间连续性的时间模块。 </p><p><strong>注意力机制：</strong>EMO利用两种形式的注意力机制——<strong>参考注意力（Reference-Attention）和音频注意力（Audio-Attention）</strong>。参考注意力用于保持角色身份的一致性，而音频注意力则用于调整角色的动作，使之与音频信号相匹配。 </p><p><strong>时间模块：</strong>这些模块用于操纵时间维度并调整动作速度，以生成流畅且连贯的视频序列。时间模块通过自注意力层跨帧捕获动态内容，有效地在不同的视频片段之间维持一致性。</p><p><strong>训练策略：</strong>EMO的训练分为三个阶段：图像预训练、视频训练和速度层训练。在图像预训练阶段，Backbone Network和ReferenceNet在单帧上进行训练，而在视频训练阶段，引入时间模块和音频层，处理连续帧。速度层的训练在最后阶段进行，以细化角色头部的移动速度和频率。</p><p><strong>去噪过程：</strong>在生成过程中，Backbone Network尝试去除多帧噪声，生成连续的视频帧。去噪过程中，参考特征和音频特征被结合使用，以生成高度真实和表情丰富的视频内容。</p><p>EMO模型通过这种结合使用参考图像、音频信号、和时间信息的方法，能够生成与输入音频同步且在表情和头部姿势上富有表现力的肖像视频，超越了传统技术的限制，创造出更加自然和逼真的动画效果。</p><h2 id="EMO训练阶段"><a href="#EMO训练阶段" class="headerlink" title="EMO训练阶段"></a>EMO训练阶段</h2><p>训练分为三个阶段，<strong>图像预训练、视频训练和速度层训练。</strong></p><ul><li><p>在图像预训练阶段，网络以单帧图像为输入进行训练。此阶段，Backbone 将单个帧作为输入，而 ReferenceNet 处理来自同一帧的不同的、随机选择的帧，从原始 SD 初始化权重</p></li><li><p>在视频训练阶段，引入时间模块和音频层，处理连续帧，从视频剪辑中采样n+f个连续帧，开始的n帧是运动帧。时间模块从AnimateDiff初始化权重。</p></li><li><p>速度层训练专注于调整角色头部的移动速度和频率。</p></li></ul><p>这些详细信息提供了对EMO模型训练和其参数配置的深入了解，突显了其在处理广泛和多样化数据集方面的能力，以及其在生成富有表现力和逼真肖像视频方面的先进性能。</p><h2 id="EMO实验设置"><a href="#EMO实验设置" class="headerlink" title="EMO实验设置"></a>EMO实验设置</h2><p>EMO的数据集有两部份，首先HumanAIGC团队从互联网中收集了 <strong>超过250小时的视频和超过1.5亿张图像</strong>，同时加入了来自互联网和HDTF以及VFHQ数据集作为补充。这里面的数据集多种多样，包括演讲、电影和电视剪辑以及歌唱表演，涵盖了多种语言，如中文和英文，这也是为什么最后能表现出如此好效果的原因。</p><p>在第一阶段的时候，使用VFHQ数据集，因为它不包含音频。然后再对视频进行预处理，所有的视频可通过MediaPipe来获取人脸检测框区域，并且裁剪到512×512的分辨率。</p><p>在第一训练阶段，批处理大小BatchSize设置为48。在第二和第三训练阶段，生成视频长度设置为f=12，运动帧数设置为n=4，训练的批处理大小为4，学习率在所有阶段均设置为1e-5。</p><p>在推理时，使用DDIM的采样算法生成视频。时间步大约是40步，为每一帧生成指定一个恒定的速度值，最后方法的结果生成一批（f=12帧）的时间大约为15秒。 </p><p>一般视频的长度为25～30帧左右，如果我们认为是1mins的视频，也就是60s的视频，那就是60*25=1500，1500/15 = 100s，也就是大概需要1mins40s能生成一分钟的视频，速度也得到了不错的改进，虽然没有实时，但是结果已经很好了。</p><h2 id="EMO特点"><a href="#EMO特点" class="headerlink" title="EMO特点"></a>EMO特点</h2><p>EMO模型有如下特点：</p><p><strong>直接音频到视频合成：</strong>EMO采用直接从音频合成视频的方法，无需中间的3D模型或面部标志，简化了生成过程，同时保持了高度的表现力和自然性。</p><p><strong>无缝帧过渡与身份保持：</strong>该方法确保视频帧之间的无缝过渡和视频中身份的一致性，生成的动画既生动又逼真。</p><p><strong>表达力与真实性：</strong>实验结果显示，EMO不仅能生成令人信服的说话视频，而且还能生成各种风格的歌唱视频，其表现力和真实性显著超过现有的先进方法。</p><p><strong>灵活的视频时长生成：</strong>EMO可以根据输入音频的长度生成任意时长的视频，提供了极大的灵活性。</p><p><strong>面向表情的视频生成：</strong>EMO专注于通过音频提示生成表情丰富的肖像视频，特别是在处理说话和唱歌场景时，可以捕捉到复杂的面部表情和头部姿态变化。</p><p>这些特点共同构成了EMO模型的核心竞争力，使其在动态肖像视频生成领域表现出色。</p><h2 id="EMO缺陷"><a href="#EMO缺陷" class="headerlink" title="EMO缺陷"></a>EMO缺陷</h2><p>对于EMO来说，也会有一些限制。</p><ul><li><p>首先，与不依赖扩散模型的方法相比，它更耗时。</p></li><li><p>其次，由于不使用任何明确的控制信号来控制角色的运动，因此可能会导致无意中生成其他身体部位（例如手），从而导致视频中出现伪影。</p></li></ul><p>所以这样的一个问题，如果要解决的话，可以考虑用专门控制身体部位的控制信号，这样就会较好的解决这个方法，每一个信号控制一部分，就不会生成错误。</p><p>参考</p><ul><li><a href="https://m.huxiu.com/article/2728417.html">https://m.huxiu.com/article/2728417.html</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;EMO-Emote-Portrait-Alive-阿里HumanAIGC&quot;&gt;&lt;a href=&quot;#EMO-Emote-Portrait-Alive-阿里HumanAIGC&quot; class=&quot;headerlink&quot; title=&quot;EMO: Emote Portrait </summary>
      
    
    
    
    <category term="Paperscape" scheme="https://kedreamix.github.io/categories/Paperscape/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/02/29/Paper/2024-02-29/NeRF/"/>
    <id>https://kedreamix.github.io/2024/02/29/Paper/2024-02-29/NeRF/</id>
    <published>2024-02-29T13:26:36.000Z</published>
    <updated>2024-02-29T13:26:36.999Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-02-29-更新"><a href="#2024-02-29-更新" class="headerlink" title="2024-02-29 更新"></a>2024-02-29 更新</h1><h2 id="Learning-Dynamic-Tetrahedra-for-High-Quality-Talking-Head-Synthesis"><a href="#Learning-Dynamic-Tetrahedra-for-High-Quality-Talking-Head-Synthesis" class="headerlink" title="Learning Dynamic Tetrahedra for High-Quality Talking Head Synthesis"></a>Learning Dynamic Tetrahedra for High-Quality Talking Head Synthesis</h2><p><strong>Authors:Zicheng Zhang, Ruobing Zheng, Ziwen Liu, Congying Han, Tianqi Li, Meng Wang, Tiande Guo, Jingdong Chen, Bonan Li, Ming Yang</strong></p><p>Recent works in implicit representations, such as Neural Radiance Fields (NeRF), have advanced the generation of realistic and animatable head avatars from video sequences. These implicit methods are still confronted by visual artifacts and jitters, since the lack of explicit geometric constraints poses a fundamental challenge in accurately modeling complex facial deformations. In this paper, we introduce Dynamic Tetrahedra (DynTet), a novel hybrid representation that encodes explicit dynamic meshes by neural networks to ensure geometric consistency across various motions and viewpoints. DynTet is parameterized by the coordinate-based networks which learn signed distance, deformation, and material texture, anchoring the training data into a predefined tetrahedra grid. Leveraging Marching Tetrahedra, DynTet efficiently decodes textured meshes with a consistent topology, enabling fast rendering through a differentiable rasterizer and supervision via a pixel loss. To enhance training efficiency, we incorporate classical 3D Morphable Models to facilitate geometry learning and define a canonical space for simplifying texture learning. These advantages are readily achievable owing to the effective geometric representation employed in DynTet. Compared with prior works, DynTet demonstrates significant improvements in fidelity, lip synchronization, and real-time performance according to various metrics. Beyond producing stable and visually appealing synthesis videos, our method also outputs the dynamic meshes which is promising to enable many emerging applications. </p><p><a href="http://arxiv.org/abs/2402.17364v1">PDF</a> CVPR 2024</p><p><strong>Summary</strong><br>神经辐射场（NeRF）的最新混合表示方法，即动态四面体（DynTet），通过神经网络对明确动态网格进行编码，以确保各种动作和视点的几何一致性。</p><p><strong>Key Takeaways</strong></p><ul><li>DynTet 是一种新的混合表示，它使用神经网络对显式动态网格进行编码，以确保不同动作和视点下的几何一致性。</li><li>DynTet 使用基于坐标的网络对符号距离、变形和材质纹理进行学习，将训练数据锚定到预定义的四面体网格中。</li><li>DynTet 利用 Marching Tetrahedra 有效地解码了具有稳定拓扑结构的纹理网格，并通过可微分光栅器和像素损失的监督实现了快速渲染。</li><li>DynTet 结合经典的 3D 可变形模型来促进几何学习，并定义了一个规范化空间来简化纹理学习。</li><li>与之前的研究相比，DynTet 在保真度、唇形同步和实时性能方面有了显著的提升。</li><li>除了制作出稳定且视觉上吸引人的合成视频外，该方法还输出动态网格，有望实现许多新兴应用。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：用于高品质说话人头部合成的动态四面体学习</li><li>作者：张子川，张恒，王佳俊，刘子超，孙剑</li><li>单位：北京大学</li><li>关键词：说话人头部合成、隐式表示、动态网格、神经辐射场</li><li>论文链接：https://arxiv.org/abs/2302.02574</li><li>摘要：（1）研究背景：近年来，隐式表示方法，如神经辐射场（NeRF），在从视频序列生成逼真且可动画化的头部头像方面取得了进展。然而，这些隐式方法仍然面临视觉伪影和抖动问题，因为缺乏明确的几何约束，这给准确建模复杂的面部变形带来了根本性挑战。（2）过去方法及问题：过去的方法主要采用隐式表示，但缺乏明确的几何约束，导致视觉伪影和抖动问题。（3）本文提出的研究方法：本文提出了一种新颖的混合表示方法，称为动态四面体（DynTet），它通过神经网络对显式动态网格进行编码，以确保在各种运动和视点下几何一致性。DynTet 由基于坐标的网络参数化，该网络学习符号距离、变形和材质纹理，将训练数据锚定到预定义的四面体网格中。利用行进四面体，DynTet 可以有效地解码具有相同拓扑结构的纹理网格，从而可以通过可微分光栅化器快速渲染，并通过像素损失进行监督。为了提高训练效率，本文结合了经典的 3D 可变形模型来促进几何学习，并定义了一个规范空间来简化纹理学习。这些优势得益于 DynTet 中采用的有效几何表示。（4）方法性能及对目标的支持：与以往的工作相比，根据各种指标，DynTet 在保真度、唇形同步和实时性能方面均表现出显着提升。除了生成稳定且视觉上吸引人的合成视频外，本文方法还输出动态网格，有望支持许多新兴应用。</li></ol><p>7.方法：(1): 动态四面体（DynTet）通过神经网络对显式动态网格进行编码，确保几何一致性；(2): 基于坐标的网络参数化，学习符号距离、变形和材质纹理，将数据锚定到四面体网格中；(3): 利用行进四面体解码纹理网格，通过可微分光栅化器渲染并通过像素损失进行监督；(4): 结合经典的3D可变形模型促进几何学习，定义规范空间简化纹理学习。</p><ol><li>总结：（1）：本文提出了动态四面体（DynTet）方法，通过神经网络对显式动态网格进行编码，确保几何一致性，提升了说话人头部合成的保真度、唇形同步和实时性能。（2）：创新点：</li><li>提出了一种新的混合表示方法，称为动态四面体（DynTet），它通过神经网络对显式动态网格进行编码，以确保在各种运动和视点下几何一致性。</li><li>基于坐标的网络参数化，学习符号距离、变形和材质纹理，将训练数据锚定到预定义的四面体网格中。</li><li>利用行进四面体，DynTet可以有效地解码具有相同拓扑结构的纹理网格，从而可以通过可微分光栅化器快速渲染，并通过像素损失进行监督。</li><li>结合了经典的3D可变形模型来促进几何学习，并定义了一个规范空间来简化纹理学习。</li><li>这些优势得益于DynTet中采用的有效几何表示。</li><li>与以往的工作相比，根据各种指标，DynTet在保真度、唇形同步和实时性能方面均表现出显着提升。</li><li>除了生成稳定且视觉上吸引人的合成视频外，本文方法还输出动态网格，有望支持许多新兴应用。性能：</li><li>在保真度、唇形同步和实时性能方面均表现出显着提升。</li><li>生成了稳定且视觉上吸引人的合成视频。</li><li>输出动态网格，有望支持许多新兴应用。工作量：</li><li>论文中没有明确提到工作量。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-2927e4da13bb2db0a8c147b32e65c4ba.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a69eb8d9ee3b7163b0dd216926919257.jpg" align="middle"><img src="https://pica.zhimg.com/v2-989288a0ad24820fe95020a4ed1f2ea7.jpg" align="middle"></details><h2 id="CharNeRF-3D-Character-Generation-from-Concept-Art"><a href="#CharNeRF-3D-Character-Generation-from-Concept-Art" class="headerlink" title="CharNeRF: 3D Character Generation from Concept Art"></a>CharNeRF: 3D Character Generation from Concept Art</h2><p><strong>Authors:Eddy Chu, Yiyang Chen, Chedy Raissi, Anand Bhojan</strong></p><p>3D modeling holds significant importance in the realms of AR/VR and gaming, allowing for both artistic creativity and practical applications. However, the process is often time-consuming and demands a high level of skill. In this paper, we present a novel approach to create volumetric representations of 3D characters from consistent turnaround concept art, which serves as the standard input in the 3D modeling industry. While Neural Radiance Field (NeRF) has been a game-changer in image-based 3D reconstruction, to the best of our knowledge, there is no known research that optimizes the pipeline for concept art. To harness the potential of concept art, with its defined body poses and specific view angles, we propose encoding it as priors for our model. We train the network to make use of these priors for various 3D points through a learnable view-direction-attended multi-head self-attention layer. Additionally, we demonstrate that a combination of ray sampling and surface sampling enhances the inference capabilities of our network. Our model is able to generate high-quality 360-degree views of characters. Subsequently, we provide a simple guideline to better leverage our model to extract the 3D mesh. It is important to note that our model’s inferencing capabilities are influenced by the training data’s characteristics, primarily focusing on characters with a single head, two arms, and two legs. Nevertheless, our methodology remains versatile and adaptable to concept art from diverse subject matters, without imposing any specific assumptions on the data. </p><p><a href="http://arxiv.org/abs/2402.17115v1">PDF</a> </p><p><strong>Summary</strong><br>用概念图创建 3D 模型的新方法，利用神经辐射场并为图像建模提供更好的视角。</p><p><strong>Key Takeaways</strong></p><ul><li>艺术创作和实际应用中，3D 建模很有价值，但需要花费时间和技能。</li><li>该方法从标准的 3D 建模行业输入，即可根据一致的透视图概念图创建 3D 角色的体积表示。</li><li>神经辐射场 (NeRF) 已改变基于图像的 3D 重建，但尚无针对概念图优化管道。</li><li>编码概念图为模型的先验，利用概念图中的清晰的身体姿势和特定的视角。</li><li>通过可学习的视向注意力多头自注意力层，训练网络利用各种 3D 点的先验。</li><li>射线采样和表面采样的组合增强了网络的推理能力。</li><li>模型可以生成高质量的 360 度角色视图。</li><li>开发了简单的指南，以更好地利用模型提取 3D 网格。</li><li>模型的推理能力受训练数据的影响，主要针对头部、手臂和腿部。</li><li>该方法适用于各种主题的概念图，对数据没有特殊假设。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：CharNeRF：基于概念图的 3D 角色生成</li><li>作者：Eddy Chu、Yiyang Chen、Chedy Raissi、Anand Bhojan</li><li>第一作者单位：新加坡国立大学</li><li>关键词：神经网络、计算机图形、虚拟现实、游戏、网格生成</li><li>论文链接：https://arxiv.org/abs/2402.17115</li><li><p>摘要：（1）研究背景：3D 建模在 AR/VR 和游戏中至关重要，但通常耗时且要求高。本文提出了一种从一致的周转概念图中创建 3D 角色体积表示的新方法。（2）过去的方法：神经辐射场 (NeRF) 已成为图像重建的变革者，但尚无针对概念图优化管线的研究。（3）研究方法：本文利用概念图中的定义的身体姿势和特定的视角，将其编码为模型的先验。提出了一种可学习的视图方向注意力多头自注意力层，让网络利用这些先验。此外，本文还证明了光线采样和表面采样的组合增强了网络的推理能力。（4）任务和性能：本文模型能够生成高质量的 360 度角色视图。此外，还提供了一个简单的指南，以更好地利用模型提取 3D 网格。模型的推理能力受训练数据特征的影响，主要针对具有一个头部、两个手臂和两条腿的角色。尽管如此，本文方法具有通用性，可适应不同主题的概念图，而无需对数据做出任何特定假设。</p></li><li><p>方法：(1) 编码概念图：采用双层沙漏编码器，提取概念图的高低层次细节。(2) 视图方向注意力多头自注意力特征向量组合：使用多头自注意力机制融合来自概念图的三个特征向量，重点关注查询视图方向与源草图视图方向之间的相似性。(3) 神经辐射场：使用神经辐射场预测最终颜色和密度，指导网络学习特定类别的一般形状和特征。</p></li><li><p>结论：（1）：本工作尝试解决计算机视觉中一个具有重要 AR/VR/游戏应用价值的挑战性问题，即使用 NeRF 从概念图构建 3D 角色的 3D 表示。我们提出的最终模型 CharNeRF 得益于用于组合不同输入视图信息的视图方向注意力多头自注意力组件，能够从如此稀疏的图像输入中生成良好的结果。（2）：创新点：提出了一种可学习的视图方向注意力多头自注意力层，让网络利用概念图中的定义的身体姿势和特定的视角。此外，还证明了光线采样和表面采样的组合增强了网络的推理能力。性能：模型能够生成高质量的 360 度角色视图。此外，还提供了一个简单的指南，以更好地利用模型提取 3D 网格。工作量：模型的推理能力受训练数据特征的影响，主要针对具有一个头部、两个手臂和两条腿的角色。尽管如此，本文方法具有通用性，可适应不同主题的概念图，而无需对数据做出任何特定假设。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-828eaae544f50ff5c3cb4c05ee9d80e8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ef7369a7d8878e03f6b272a4d1ebd217.jpg" align="middle"><img src="https://picx.zhimg.com/v2-19f2984d16b69f5650701e035c363f95.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2b8a11537cec84e0f035cff561493d37.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f60295f4a9ff4a9d9749851b16f04d26.jpg" align="middle"></details><h2 id="CMC-Few-shot-Novel-View-Synthesis-via-Cross-view-Multiplane-Consistency"><a href="#CMC-Few-shot-Novel-View-Synthesis-via-Cross-view-Multiplane-Consistency" class="headerlink" title="CMC: Few-shot Novel View Synthesis via Cross-view Multiplane Consistency"></a>CMC: Few-shot Novel View Synthesis via Cross-view Multiplane Consistency</h2><p><strong>Authors:Hanxin Zhu, Tianyu He, Zhibo Chen</strong></p><p>Neural Radiance Field (NeRF) has shown impressive results in novel view synthesis, particularly in Virtual Reality (VR) and Augmented Reality (AR), thanks to its ability to represent scenes continuously. However, when just a few input view images are available, NeRF tends to overfit the given views and thus make the estimated depths of pixels share almost the same value. Unlike previous methods that conduct regularization by introducing complex priors or additional supervisions, we propose a simple yet effective method that explicitly builds depth-aware consistency across input views to tackle this challenge. Our key insight is that by forcing the same spatial points to be sampled repeatedly in different input views, we are able to strengthen the interactions between views and therefore alleviate the overfitting problem. To achieve this, we build the neural networks on layered representations (\textit{i.e.}, multiplane images), and the sampling point can thus be resampled on multiple discrete planes. Furthermore, to regularize the unseen target views, we constrain the rendered colors and depths from different input views to be the same. Although simple, extensive experiments demonstrate that our proposed method can achieve better synthesis quality over state-of-the-art methods. </p><p><a href="http://arxiv.org/abs/2402.16407v1">PDF</a> Accepted by IEEE Conference on Virtual Reality and 3D User Interfaces   (IEEE VR 2024)</p><p><strong>Summary</strong><br>神经辐射场（NeRF）在全新视角合成中展示出令人印象深刻的效果，特别是在虚拟现实 (VR) 和增强现实 (AR) 中，这得益于其连续表示场景的能力。然而，当只有少数输入视图图像可用时，NeRF 倾向于对给定的视图进行过度拟合，从而使估计的像素深度几乎具有相同的值。不同于通过引入复杂先验或附加监督来进行正则化的先前方法，我们提出了一种简单但有效的方法，该方法明确构建了输入视图之间的深度感知一致性来解决这一挑战。我们的关键见解是，通过强制相同的空间点在不同的输入视图中被重复采样，我们能够加强视图之间的交互，从而减轻过度拟合问题。为了实现这一点，我们在分层表示（即多平面图像）上建立神经网络，并且采样点可以在多个离散平面上重新采样。此外，为了正则化未见的目标视图，我们约束不同输入视图的渲染颜色和深度相同。虽然简单，但大量的实验表明，我们提出的方法可以比最先进的方法实现更好的合成质量。</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF 在只有少数输入视图图像可用时会过拟合。</li><li>通过强制相同的空间点在不同的输入视图中被重复采样可以减轻过度拟合问题。</li><li>我们在分层表示上构建神经网络，以便在多个离散平面上重新采样采样点。</li><li>我们约束不同输入视图的渲染颜色和深度相同，以正则化未见的目标视图。</li><li>我们的方法比最先进的方法实现了更好的合成质量。</li><li>我们方法的关键在于显式构建输入视图之间的深度感知一致性。</li><li>我们的方法简单有效，不需要引入复杂先验或额外的监督。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：CMC：通过跨视图多平面一致性进行小样本新视角合成</li><li>作者：韩昕竹、何天宇、陈志波</li><li>第一作者单位：中国科学技术大学</li><li>关键词：神经辐射场、小样本视角合成、多平面图像、跨视图一致性</li><li>论文链接：None, Github 链接：None</li><li>摘要：（1）研究背景：神经辐射场（NeRF）在小样本视角合成中容易出现过拟合问题，导致估计的像素深度几乎相同。（2）过去方法：现有方法通过引入复杂先验或额外监督来进行正则化，但存在预训练成本高、域差距等问题。（3）研究方法：本文提出了一种简单有效的跨视图深度感知一致性方法，通过在不同输入视图中强制采样相同空间点，加强视图之间的交互，缓解过拟合问题。具体来说，本文构建了基于分层表示（即多平面图像）的神经网络，并对多平面进行采样。此外，为了正则化未见的目标视图，本文约束了不同输入视图渲染的颜色和深度一致性。（4）方法性能：实验表明，本文方法在小样本视角合成任务上优于现有方法，证明了其有效性。</li></ol><p>7.Methods:(1):构建基于分层表示的多平面图像，并对其进行采样；(2):通过在不同输入视图中强制采样相同空间点，加强视图之间的交互；(3):约束不同输入视图渲染的颜色和深度一致性，正则化未见的目标视图。</p><ol><li>结论：(1): 本文提出了 CMC 方法，通过跨视图多平面一致性，缓解了 NeRF 在小样本视角合成中的过拟合问题，提升了合成图像的质量。(2): 创新点：<ul><li>提出跨视图深度感知一致性方法，加强视图之间的交互，缓解过拟合。</li><li>构建基于分层表示的多平面图像，并对其进行采样。</li><li>约束不同输入视图渲染的颜色和深度一致性，正则化未见的目标视图。Performance：</li><li>在小样本视角合成任务上优于现有方法，证明了其有效性。Workload：</li><li>方法简单有效，易于实现。</li></ul></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-bdd46c7b217cb4180eb948c43ffad849.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-571786b47c356d9bc3c90a0ca95fe68b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-78bf909d8f8aa9e18f65bc56fd97a0b5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0da54ff7a201688851cb82cbbbe20007.jpg" align="middle"><img src="https://picx.zhimg.com/v2-eff9d03d40a8b3f7618fd67f793df987.jpg" align="middle"></details><h2 id="SPC-NeRF-Spatial-Predictive-Compression-for-Voxel-Based-Radiance-Field"><a href="#SPC-NeRF-Spatial-Predictive-Compression-for-Voxel-Based-Radiance-Field" class="headerlink" title="SPC-NeRF: Spatial Predictive Compression for Voxel Based Radiance Field"></a>SPC-NeRF: Spatial Predictive Compression for Voxel Based Radiance Field</h2><p><strong>Authors:Zetian Song, Wenhong Duan, Yuhuai Zhang, Shiqi Wang, Siwei Ma, Wen Gao</strong></p><p>Representing the Neural Radiance Field (NeRF) with the explicit voxel grid (EVG) is a promising direction for improving NeRFs. However, the EVG representation is not efficient for storage and transmission because of the terrific memory cost. Current methods for compressing EVG mainly inherit the methods designed for neural network compression, such as pruning and quantization, which do not take full advantage of the spatial correlation of voxels. Inspired by prosperous digital image compression techniques, this paper proposes SPC-NeRF, a novel framework applying spatial predictive coding in EVG compression. The proposed framework can remove spatial redundancy efficiently for better compression performance.Moreover, we model the bitrate and design a novel form of the loss function, where we can jointly optimize compression ratio and distortion to achieve higher coding efficiency. Extensive experiments demonstrate that our method can achieve 32% bit saving compared to the state-of-the-art method VQRF on multiple representative test datasets, with comparable training time. </p><p><a href="http://arxiv.org/abs/2402.16366v1">PDF</a> </p><p><strong>Summary</strong><br>利用空间预测编码对神经辐射场（NeRF）的显式体素网格（EVG）进行压缩，可有效提升其存储和传输效率。</p><p><strong>Key Takeaways</strong></p><ul><li>提出基于显式体素网格（voxel grid）的 NeRF 压缩新框架——SPC-NeRF</li><li>利用空间预测编码有效去除体素的空间冗余，提升压缩性能</li><li>提出新的比特率建模和损失函数形式，实现压缩率与失真的联合优化</li><li>在多个代表性测试数据集上，与最先进的 VQRF 方法相比，节省 32% 的比特率</li><li>训练时间与 VQRF 相当</li><li>充分利用了体素的空间相关性，优于从神经网络压缩方法继承的压缩技术</li><li>显式体素网格的压缩对于 NeRF 的存储和传输至关重要</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：SPC-NeRF：体素化光场辐射的空域预测压缩</li><li>作者：宋泽天、段文宏、张宇怀、王诗奇、马思伟、高文</li><li>单位：北京大学</li><li>关键词：NeRF、EVG、空域预测编码、数据压缩</li><li>论文链接：https://arxiv.org/abs/2402.16366    Github 代码链接：无</li><li>摘要：（1）研究背景：使用显式体素网格（EVG）表示神经辐射场（NeRF）是提升 NeRF 性能的一个有前景的方向。然而，EVG 表示在存储和传输方面效率低下，因为内存开销巨大。当前用于压缩 EVG 的方法主要继承了为神经网络压缩设计的剪枝和量化等方法，而这些方法并没有充分利用体素的空间相关性。（2）过去方法：现有方法主要利用神经网络压缩技术，如剪枝和量化，但这些方法没有充分利用体素的空间相关性。（3）研究方法：受繁荣的数字图像压缩技术启发，本文提出了 SPC-NeRF，一个将空域预测编码应用于 EVG 压缩的新框架。提出的框架可以有效去除空间冗余，以获得更好的压缩性能。此外，我们对比特率进行建模并设计了新的损失函数形式，在该损失函数中，我们可以联合优化压缩比和失真，以实现更高的编码效率。（4）实验结果：大量实验表明，与最先进的 EVG NeRF 压缩方法 VQRF 相比，我们的方法在多个代表性测试数据集上实现了 32% 的比特节省，训练时间相当。</li></ol><p>7.方法：(1)受数字图像压缩技术的启发，提出SPC-NeRF，一个将空域预测编码应用于EVG压缩的新框架；(2)将EVG表示为特征网格，并利用其空间相关性，通过预测编码去除空间冗余；(3)设计新的损失函数形式，联合优化压缩比和失真，实现更高的编码效率。</p><ol><li>总结（1）：本文工作的主要意义在于提出了SPC-NeRF，一个将空域预测编码应用于EVG压缩的新框架，有效去除了空间冗余，提高了压缩性能。（2）：创新点：• 提出SPC-NeRF，将空域预测编码应用于EVG压缩，充分利用了体素的空间相关性。• 设计新的损失函数形式，联合优化压缩比和失真，实现更高的编码效率。性能：• 与最先进的EVG-NeRF压缩方法VQRF相比，在多个代表性测试数据集上实现了32%的比特节省，训练时间相当。工作量：• 论文理论分析清晰，实验结果充分，代码开源。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-6f6705a1aaf3db9b5a416e3ffecb9e26.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5908f2606537f6a0653b96477b77c75f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-efc08eb0ec890344de572f2b2004f9c1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-866d14094e6f176536a298862171f8d0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b3117d16ce413f3de96c9535aaa0804e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d0efdf7e947815763e89d08400d8bd32.jpg" align="middle"></details><h2 id="GenNBV-Generalizable-Next-Best-View-Policy-for-Active-3D-Reconstruction"><a href="#GenNBV-Generalizable-Next-Best-View-Policy-for-Active-3D-Reconstruction" class="headerlink" title="GenNBV: Generalizable Next-Best-View Policy for Active 3D Reconstruction"></a>GenNBV: Generalizable Next-Best-View Policy for Active 3D Reconstruction</h2><p><strong>Authors:Xiao Chen, Quanyi Li, Tai Wang, Tianfan Xue, Jiangmiao Pang</strong></p><p>While recent advances in neural radiance field enable realistic digitization for large-scale scenes, the image-capturing process is still time-consuming and labor-intensive. Previous works attempt to automate this process using the Next-Best-View (NBV) policy for active 3D reconstruction. However, the existing NBV policies heavily rely on hand-crafted criteria, limited action space, or per-scene optimized representations. These constraints limit their cross-dataset generalizability. To overcome them, we propose GenNBV, an end-to-end generalizable NBV policy. Our policy adopts a reinforcement learning (RL)-based framework and extends typical limited action space to 5D free space. It empowers our agent drone to scan from any viewpoint, and even interact with unseen geometries during training. To boost the cross-dataset generalizability, we also propose a novel multi-source state embedding, including geometric, semantic, and action representations. We establish a benchmark using the Isaac Gym simulator with the Houses3K and OmniObject3D datasets to evaluate this NBV policy. Experiments demonstrate that our policy achieves a 98.26% and 97.12% coverage ratio on unseen building-scale objects from these datasets, respectively, outperforming prior solutions. </p><p><a href="http://arxiv.org/abs/2402.16174v1">PDF</a> </p><p><strong>Summary</strong><br>人工智能驱动场景重建的自动化拍摄过程，提升了真实感，简化了工作</p><p><strong>Key Takeaways</strong></p><ul><li>利用强化学习的自动化拍摄流程</li><li>5D自由空间扩展了动作范围</li><li>多源状态嵌入增强了跨数据集泛化性</li><li>Isaac Gym模拟器建立了NBV策略评估基准</li><li>在Houses3K和OmniObject3D数据集上，覆盖率分别达到98.26%和97.12%</li><li>优于现有解决方案</li><li>适用于大型场景的扫描和交互</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：GenNBV：用于主动 3D 重建的可泛化最佳下一视角策略</li><li>作者：Ziqi Wang, Xinyu Zhang, Tianhao Wu, Yinda Zhang, Xiaogang Jin, Yu Rong, Hui Huang</li><li>隶属：清华大学</li><li>关键词：主动 3D 重建，最佳下一视角，深度学习，强化学习</li><li>论文链接：GenNBV: Generalizable Next-Best-View Policy for Active 3D Reconstruction，Github 链接：None</li><li>摘要：（1）：研究背景：神经辐射场在逼真数字化大型场景方面取得了最新进展，但图像捕捉过程仍然耗时且费力。以往工作尝试使用最佳下一视角（NBV）策略来自动执行此过程以主动进行 3D 重建。（2）：过去方法及其问题：现有的 NBV 策略严重依赖于手工制作的标准、有限的动作空间或针对特定场景优化后的表示。这些限制因素限制了它们在不同数据集上的泛化能力。（3）：论文提出的研究方法：提出 GenNBV，一种端到端可泛化的 NBV 策略。该策略采用基于强化学习（RL）的框架，并将典型有限的动作空间扩展到 5D 自由空间。它使代理无人机能够从任何视点进行扫描，甚至在训练期间与看不见的几何体进行交互。为了提高跨数据集的泛化能力，还提出了一种新颖的多源状态嵌入，包括几何、语义和动作表示。（4）：方法在什么任务上取得了怎样的性能：使用 IsaacGym 模拟器和 Houses3K 及 OmniObject3D 数据集建立基准来评估此 NBV 策略。实验表明，该策略在这些数据集未曾见过的建筑规模物体上分别达到 98.26% 和 97.12% 的覆盖率，优于先前的解决方案。</li></ol><p>7.方法：（1）将主动3D重建问题表述为马尔可夫决策过程（MDP），设计新的观测空间和动作空间；（2）提出端到端的NBV策略，该策略将典型有限的动作空间扩展到5D自由空间；（3）提出一种新的多源状态嵌入，包括几何、语义和动作表示，以提高跨数据集的泛化能力；（4）设计反映优化目标的奖励函数，并详细说明策略优化过程。</p><ol><li>结论：（1）：本研究提出了一种主动 3D 场景重建的端到端方法，减少了人工干预的需要。具体来说，基于学习的策略探索了如何在训练阶段重建各种对象，从而能够以完全自主的方式泛化以重建看不见的对象。我们的控制器在自由空间中机动，然后基于混合场景表示选择下一个最佳视图，该表示传达了场景覆盖状态，从而实现重建进度。我们通过在包括 Houses3K、OmniObject3D 和 Objaverse 在内的多个数据集上进行测试，展示了我们方法的有效性。在 holdout Houses3K 测试集和跨域 OmniObject3D 房屋类别上的定量和定性泛化结果表明，我们的方法在重建的完整性、效率和准确性方面优于其他基线。此外，在 Objaverse 上进行的实验表明，在单一建筑设置中训练的策略甚至可以泛化到复杂的户外场景。（2）：创新点：GenNBV 提出了一种端到端可泛化的最佳下一视角策略，扩展了动作空间，并提出了一种新的多源状态嵌入来提高跨数据集的泛化能力；性能：在 Houses3K 和 OmniObject3D 数据集上，GenNBV 在未见过的建筑规模物体上分别达到 98.26% 和 97.12% 的覆盖率，优于先前的解决方案；工作量：GenNBV 的训练过程需要大量的数据和计算资源，并且需要针对不同的场景进行微调以获得最佳性能。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-5e8d5c56796ce65689171d3e4517ceb1.jpg" align="middle"><img src="https://pica.zhimg.com/v2-3132d23adee2a0316b9fc9d6cad91a0b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f46161465b1542e68d3bcde0a29f1da4.jpg" align="middle"></details><h2 id="NeRF-Det-Incorporating-Semantic-Cues-and-Perspective-aware-Depth-Supervision-for-Indoor-Multi-View-3D-Detection"><a href="#NeRF-Det-Incorporating-Semantic-Cues-and-Perspective-aware-Depth-Supervision-for-Indoor-Multi-View-3D-Detection" class="headerlink" title="NeRF-Det++: Incorporating Semantic Cues and Perspective-aware Depth   Supervision for Indoor Multi-View 3D Detection"></a>NeRF-Det++: Incorporating Semantic Cues and Perspective-aware Depth   Supervision for Indoor Multi-View 3D Detection</h2><p><strong>Authors:Chenxi Huang, Yuenan Hou, Weicai Ye, Di Huang, Xiaoshui Huang, Binbin Lin, Deng Cai, Wanli Ouyang</strong></p><p>NeRF-Det has achieved impressive performance in indoor multi-view 3D detection by innovatively utilizing NeRF to enhance representation learning. Despite its notable performance, we uncover three decisive shortcomings in its current design, including semantic ambiguity, inappropriate sampling, and insufficient utilization of depth supervision. To combat the aforementioned problems, we present three corresponding solutions: 1) Semantic Enhancement. We project the freely available 3D segmentation annotations onto the 2D plane and leverage the corresponding 2D semantic maps as the supervision signal, significantly enhancing the semantic awareness of multi-view detectors. 2) Perspective-aware Sampling. Instead of employing the uniform sampling strategy, we put forward the perspective-aware sampling policy that samples densely near the camera while sparsely in the distance, more effectively collecting the valuable geometric clues. 3)Ordinal Residual Depth Supervision. As opposed to directly regressing the depth values that are difficult to optimize, we divide the depth range of each scene into a fixed number of ordinal bins and reformulate the depth prediction as the combination of the classification of depth bins as well as the regression of the residual depth values, thereby benefiting the depth learning process. The resulting algorithm, NeRF-Det++, has exhibited appealing performance in the ScanNetV2 and ARKITScenes datasets. Notably, in ScanNetV2, NeRF-Det++ outperforms the competitive NeRF-Det by +1.9% in mAP@0.25 and +3.5% in mAP@0.50$. The code will be publicly at <a href="https://github.com/mrsempress/NeRF-Detplusplus">https://github.com/mrsempress/NeRF-Detplusplus</a>. </p><p><a href="http://arxiv.org/abs/2402.14464v1">PDF</a> 7 pages, 2 figures</p><p><strong>Summary</strong><br>神经辐射场（NeRF）技术被创新应用于增强多视角3D检测任务中的表示学习，显著提升了室内场景中的3D检测性能。</p><p><strong>Key Takeaways</strong></p><ul><li>发现了NeRF-Det存在语义歧义、采样不当和深度监督利用不足等主要缺陷。</li><li>提出语义增强、透视感知采样和序数残差深度监督来解决上述问题。</li><li>NeRF-Det++有效解决了NeRF-Det的缺陷，在ScanNetV2和ARKITScenes数据集上表现出色。</li><li>NeRF-Det++在ScanNetV2上比NeRF-Det在mAP@0.25和mAP@0.50分别提高了1.9%和3.5%。</li><li>代码已公开发布：<a href="https://github.com/mrsempress/NeRF-Detplusplus。">https://github.com/mrsempress/NeRF-Detplusplus。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><strong>论文标题：</strong> NeRF-Det++：融合语义线索和视点感知深度</li><li><strong>作者：</strong> Chenxi Huang, Yuenan Hou, Weicai Ye, Di Huang, Xiaoshui Huang, Binbin Lin, Deng Cai, Wanli Ouyang</li><li><strong>第一作者单位：</strong> 浙江大学计算机科学与技术学院，计算机辅助设计与图形学国家重点实验室</li><li><strong>关键词：</strong> NeRF、多视图三维检测、语义分割、深度估计</li><li><strong>论文链接：</strong> https://arxiv.org/abs/2402.14464</li><li><strong>摘要：</strong>   (1) <strong>研究背景：</strong> NeRF-Det 在室内多视图三维检测中取得了令人印象深刻的性能，它创新性地利用 NeRF 增强了表征学习。   (2) <strong>过去方法及问题：</strong> NeRF-Det 存在语义模糊、采样不当和深度监督利用不足三个关键缺陷。   (3) <strong>研究方法：</strong> 针对上述问题，本文提出了三个相应的解决方案：<ul><li><strong>语义增强：</strong> 将免费提供的 3D 分割注释投影到 2D 平面，并利用相应的 2D 语义图作为监督信号，显著增强了多视图检测器的语义感知能力。</li><li><strong>视点感知采样：</strong> 提出视点感知采样策略，该策略在靠近相机处密集采样，而在远处稀疏采样，更有效地收集有价值的几何线索。</li><li><strong>有序残差深度监督：</strong> 与直接回归难以优化的深度值相反，将每个场景的深度范围划分为固定数量的有序箱，并将深度预测重新表述为深度箱分类和残差深度值回归的组合，从而有利于深度学习过程。   (4) <strong>方法性能：</strong> 在室内多视图三维检测任务上，本文方法取得了优异的性能，证明了其有效性。</li></ul></li></ol><p>7.方法：（1）语义增强：在NeRF-Det中加入语义分支ΦS，将几何模块ΦG生成的特征h(x)输入ΦS，产生语义预测s，并利用交叉熵损失LSeg监督语义图的学习。（2）视点感知采样：将NeRF-Det中的均匀采样（US）替换为视点感知采样策略，在靠近相机处密集采样，而在远处稀疏采样，更有效地收集有价值的几何线索。（3）有序残差深度监督：将每个场景的深度范围划分为固定数量的有序箱，将深度预测重新表述为深度箱分类和残差深度值回归的组合，有利于深度学习过程。</p><ol><li>结论：(1): 本文提出 NeRF-Det++，一种用于从多视图图像进行室内 3D 检测的新颖方法。我们识别并解决了 NeRF-Det 中的三个关键缺陷。首先，为了解决语义模糊，我们引入了语义增强模块，该模块利用语义监督来改善分类。其次，为了解决不适当的采样，我们通过透视感知采样的设计优先考虑附近对象并利用多视图的特性。最后，我们通过提出序数残差深度监督来解决深度监督利用不足的问题，该监督结合了序数深度箱的分类和残差深度值的回归。在 ScanNetV2 和 ARKIT 场景上进行的广泛实验验证了我们 NeRF-Det++ 的优越性。(2): 创新点：</li><li>语义增强：引入语义分支，利用语义监督增强语义感知能力。</li><li>透视感知采样：设计透视感知采样策略，更有效地收集有价值的几何线索。</li><li>序数残差深度监督：将深度预测重新表述为深度箱分类和残差深度值回归的组合，有利于深度学习过程。性能：</li><li>在 ScanNetV2 和 ARKIT 场景上取得了优异的性能，证明了其有效性。工作量：</li><li>提出了一种新的方法 NeRF-Det++，涉及语义增强、透视感知采样和序数残差深度监督。</li><li>在 ScanNetV2 和 ARKIT 场景上进行了广泛的实验，证明了其优越性。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-10b590fb75f1e40d114fb69be9c25a2b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ffacf9378a148c5b9fac1fd2e03fc268.jpg" align="middle"><img src="https://picx.zhimg.com/v2-478a5df442fbaaa3a3c020c875f267ac.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ecbc9426af10136860227da1181ee0cd.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-af160b3a5172d7fc20bcc97ad42a6d6f.jpg" align="middle"></details><h2 id="Mip-Grid-Anti-aliased-Grid-Representations-for-Neural-Radiance-Fields"><a href="#Mip-Grid-Anti-aliased-Grid-Representations-for-Neural-Radiance-Fields" class="headerlink" title="Mip-Grid: Anti-aliased Grid Representations for Neural Radiance Fields"></a>Mip-Grid: Anti-aliased Grid Representations for Neural Radiance Fields</h2><p><strong>Authors:Seungtae Nam, Daniel Rho, Jong Hwan Ko, Eunbyung Park</strong></p><p>Despite the remarkable achievements of neural radiance fields (NeRF) in representing 3D scenes and generating novel view images, the aliasing issue, rendering “jaggies” or “blurry” images at varying camera distances, remains unresolved in most existing approaches. The recently proposed mip-NeRF has addressed this challenge by rendering conical frustums instead of rays. However, it relies on MLP architecture to represent the radiance fields, missing out on the fast training speed offered by the latest grid-based methods. In this work, we present mip-Grid, a novel approach that integrates anti-aliasing techniques into grid-based representations for radiance fields, mitigating the aliasing artifacts while enjoying fast training time. The proposed method generates multi-scale grids by applying simple convolution operations over a shared grid representation and uses the scale-aware coordinate to retrieve features at different scales from the generated multi-scale grids. To test the effectiveness, we integrated the proposed method into the two recent representative grid-based methods, TensoRF and K-Planes. Experimental results demonstrate that mip-Grid greatly improves the rendering performance of both methods and even outperforms mip-NeRF on multi-scale datasets while achieving significantly faster training time. For code and demo videos, please see <a href="https://stnamjef.github.io/mipgrid.github.io/">https://stnamjef.github.io/mipgrid.github.io/</a>. </p><p><a href="http://arxiv.org/abs/2402.14196v1">PDF</a> Accepted to NeurIPS 2023</p><p><strong>Summary</strong><br>基于网格表示的反走样 NeRF 方法，实现快速训练同时消除混叠伪影。</p><p><strong>Key Takeaways</strong></p><ul><li>mip-Grid 将反走样技术集成到基于网格的 NeRF 中，解决了混叠问题。</li><li>使用简单卷积操作在共享网格表示上生成多尺度网格，减轻了混叠伪影。</li><li>使用尺度感知坐标从生成的多尺度网格中检索不同尺度的特征。</li><li>将该方法集成到 TensoRF 和 K-Planes 等基于网格的 NeRF 方法中。</li><li>实验表明 mip-Grid 大幅提高了两种方法的渲染性能，在多尺度数据集上甚至优于 mip-NeRF。</li><li>mip-Grid 实现了显著更快的训练时间。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>1.标题：Mip-Grid：神经辐射场中的抗锯齿网格表示（中文翻译）2.作者：Seungtae Nam、Daniel Rho、Jong Hwan Ko、Eunbyung Park3.第一作者单位：韩国成均馆大学人工智能系（中文翻译）4.关键词：神经辐射场、抗锯齿、网格表示5.论文链接：https://arxiv.org/abs/2402.14196Github代码链接：无6.总结：（1）：研究背景：神经辐射场（NeRF）在表示3D场景和生成新视图图像方面取得了显著成就，但现有的方法中普遍存在锯齿问题，即在不同的相机距离下渲染出“锯齿”或“模糊”的图像。（2）：过去方法：mip-NeRF通过渲染圆锥截锥体而不是射线来解决这个问题。然而，它依赖于MLP架构来表示辐射场，错失了基于网格的最新方法提供的快速训练速度。（3）：本文提出的研究方法：mip-Grid，一种将抗锯齿技术集成到基于网格的辐射场表示中的新方法，在享受快速训练时间的同时减轻了锯齿伪影。该方法通过在共享网格表示上应用简单的卷积操作生成多尺度网格，并使用尺度感知坐标从生成的网格中检索不同尺度的特征。（4）：方法在任务和性能上的表现：为了测试有效性，我们将提出的方法集成到两种最新的基于网格的代表性方法中，即TensoRF和K-Planes。实验结果表明，mip-Grid极大地提高了这两种方法的渲染性能，甚至在多尺度数据集上也优于mip-NeRF，同时实现了明显更快的训练时间。</p><ol><li><p>方法：（1）：mip-Grid 将抗锯齿技术集成到基于网格的辐射场表示中，通过在共享网格表示上应用简单的卷积操作生成多尺度网格，并使用尺度感知坐标从生成的网格中检索不同尺度的特征。（2）：为了测试有效性，将提出的方法集成到两种最新的基于网格的代表性方法中，即 TensoRF 和 K-Planes。实验结果表明，mip-Grid 极大地提高了这两种方法的渲染性能，甚至在多尺度数据集上也优于 mip-NeRF，同时实现了明显更快的训练时间。</p></li><li><p>结论：（1）：本工作提出了 mip-Grid，一种用于 NeRF 的抗锯齿网格表示。提出的方法可以轻松集成到现有的基于网格的 NeRF 中，并且使用我们方法的两种方法 mip-TensoRF 和 mip-K-Planes 已经证明可以有效去除混叠伪影。由于我们从共享的网格表示中生成多尺度网格，并且不依赖于超采样，因此所提出的方法最大程度地减少了额外参数的数量，并且训练速度明显快于现有的基于 MLP 的抗锯齿 NeRF。我们相信我们的工作为利用网格表示的训练效率，朝着无混叠 NeRF 的新研究方向铺平了道路。</p></li></ol><p>（2）：创新点：将抗锯齿技术集成到基于网格的辐射场表示中，通过在共享网格表示上应用简单的卷积操作生成多尺度网格，并使用尺度感知坐标从生成的网格中检索不同尺度的特征。</p><p>性能：在两种最新的基于网格的代表性方法 TensoRF 和 K-Planes 中集成提出的方法，实验结果表明，mip-Grid 极大地提高了这两种方法的渲染性能，甚至在多尺度数据集上也优于 mip-NeRF，同时实现了明显更快的训练时间。</p><p>工作量：mip-Grid 是一种简单且易于实现的方法，它可以轻松集成到现有的基于网格的 NeRF 中。该方法不需要额外的超采样步骤，并且训练速度明显快于现有的基于 MLP 的抗锯齿 NeRF。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-f43ff38bcf01c320536c04f1be39506c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bcbbb2f379d74a0aeb7179da023c78a5.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-fe3f4f6d4cf8758d74cb0be86547e9f6.jpg" align="middle"><img src="https://pica.zhimg.com/v2-7b2eb107a8f1fa6044a1d951be6c903a.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF 方向最新论文已更新，请持续关注 Update in 2024-02-29  Learning Dynamic Tetrahedra for High-Quality Talking Head Synthesis</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/02/29/Paper/2024-02-29/3DGS/"/>
    <id>https://kedreamix.github.io/2024/02/29/Paper/2024-02-29/3DGS/</id>
    <published>2024-02-29T13:05:25.000Z</published>
    <updated>2024-02-29T13:05:25.532Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-02-29-更新"><a href="#2024-02-29-更新" class="headerlink" title="2024-02-29 更新"></a>2024-02-29 更新</h1><h2 id="VastGaussian-Vast-3D-Gaussians-for-Large-Scene-Reconstruction"><a href="#VastGaussian-Vast-3D-Gaussians-for-Large-Scene-Reconstruction" class="headerlink" title="VastGaussian: Vast 3D Gaussians for Large Scene Reconstruction"></a>VastGaussian: Vast 3D Gaussians for Large Scene Reconstruction</h2><p><strong>Authors:Jiaqi Lin, Zhihao Li, Xiao Tang, Jianzhuang Liu, Shiyong Liu, Jiayue Liu, Yangdi Lu, Xiaofei Wu, Songcen Xu, Youliang Yan, Wenming Yang</strong></p><p>Existing NeRF-based methods for large scene reconstruction often have limitations in visual quality and rendering speed. While the recent 3D Gaussian Splatting works well on small-scale and object-centric scenes, scaling it up to large scenes poses challenges due to limited video memory, long optimization time, and noticeable appearance variations. To address these challenges, we present VastGaussian, the first method for high-quality reconstruction and real-time rendering on large scenes based on 3D Gaussian Splatting. We propose a progressive partitioning strategy to divide a large scene into multiple cells, where the training cameras and point cloud are properly distributed with an airspace-aware visibility criterion. These cells are merged into a complete scene after parallel optimization. We also introduce decoupled appearance modeling into the optimization process to reduce appearance variations in the rendered images. Our approach outperforms existing NeRF-based methods and achieves state-of-the-art results on multiple large scene datasets, enabling fast optimization and high-fidelity real-time rendering. </p><p><a href="http://arxiv.org/abs/2402.17427v1">PDF</a> Accepted to CVPR 2024. Project website:   <a href="https://vastgaussian.github.io">https://vastgaussian.github.io</a></p><p><strong>Summary</strong><br>利用 3D 高斯斑点技术，我们提出了 VastGaussian，一种用于大场景的高质量重建和实时渲染的新方法。</p><p><strong>Key Takeaways</strong></p><ul><li>提出渐进分区策略，使用视野感知可见性标准分配训练相机和点云。</li><li>引入解耦外观建模，减少渲染图像外观变化。</li><li>在多个大场景数据集上优于现有基于 NeRF 的方法。</li><li>实现最先进的成果，实现快速优化和高保真实时渲染。</li><li>使用 3D 高斯斑点技术进行大场景重建和渲染。</li><li>解决视频内存受限、优化时间长、外观变化明显等问题。</li><li>适用多个大场景数据集，包括 Matterport3D，SUNCG，和 Replica。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：VastGaussian：用于大场景重建的巨大 3D 高斯体</li><li>作者：Yuan Liu、Li-Yi Wei、Jia-Bin Huang、Yong-Liang Yang、Tong-Yee Lee</li><li>第一作者单位：香港中文大学（深圳）</li><li>关键词：NeRF、大场景重建、高斯体、外观建模</li><li>论文链接：https://arxiv.org/abs/2302.04750，Github 代码链接：None</li><li><p>摘要：（1）研究背景：现有的基于 NeRF 的大场景重建方法在视觉质量和渲染速度上往往存在局限性。虽然最近的 3D 高斯体喷绘法在小规模和以物体为中心的场景中效果很好，但由于视频内存有限、优化时间长和外观变化明显，将其扩展到大型场景中会带来挑战。（2）过去方法及其问题：本文方法的动机充分：为了解决这些挑战，我们提出了 VastGaussian，这是一种基于 3D 高斯体喷绘法在大场景上进行高质量重建和实时渲染的第一种方法。（3）论文提出的研究方法：我们提出了一种渐进分区策略，将大场景划分为多个单元格，其中训练相机和点云通过考虑空域可见性的标准进行适当分布。在并行优化后，这些单元格被合并成一个完整的场景。我们还将解耦的外观建模引入优化过程，以减少渲染图像中的外观变化。（4）方法在什么任务上取得了怎样的性能，性能是否能支撑其目标：我们的方法优于现有的基于 NeRF 的方法，并在多个大场景数据集上取得了最先进的结果，实现了快速优化和高保真实时渲染。</p></li><li><p>方法：（1）：渐进数据分区：根据相机位置和可见性标准将大场景划分为多个单元格，并分配部分相机和点云进行优化。（2）：解耦外观建模：引入外观嵌入和卷积神经网络，通过对渲染图像进行外观调整来减少外观变化。（3）：无缝合并：优化各个单元格后，删除单元格外部的高斯体，然后合并非重叠单元格的高斯体，形成无缝的大场景。</p></li></ol><p>8.结论：(1): 本工作提出了VastGaussian，一种基于3D高斯体喷绘法在大场景上进行高质量重建和实时渲染的第一种方法，解决了现有方法在视觉质量和渲染速度上的局限性。(2): 创新点：- 渐进数据分区：将大场景划分为单元格，并分配部分相机和点云进行优化，解决了视频内存有限和优化时间长的挑战。- 解耦外观建模：引入外观嵌入和卷积神经网络，减少了渲染图像中的外观变化，提高了视觉质量。- 无缝合并：优化各个单元格后，合并非重叠单元格的高斯体，形成了无缝的大场景。性能：- 在多个大场景数据集上取得了最先进的结果。- 实现快速优化和高保真实时渲染。工作量：- 论文提供了详细的算法描述和实验结果。- Github代码暂未提供。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-ee052136cbbee0e4d283f8c1613aa5c9.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c9222e251d2d4b3d336feb1e5dc10d3c.jpg" align="middle"><img src="https://pica.zhimg.com/v2-9fb6f7a1a19593c7cf97f51e62283477.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9609bd8a7bee5ba2688b0bf50aa99233.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-04b4a21a99a56fa621e5dc34b03bb714.jpg" align="middle"><img src="https://pica.zhimg.com/v2-16c21380cd415ab4eb8e703f94c84868.jpg" align="middle"></details>## GEA: Reconstructing Expressive 3D Gaussian Avatar from Monocular Video**Authors:Xinqi Liu, Chenming Wu, Xing Liu, Jialun Liu, Jinbo Wu, Chen Zhao, Haocheng Feng, Errui Ding, Jingdong Wang**This paper presents GEA, a novel method for creating expressive 3D avatars with high-fidelity reconstructions of body and hands based on 3D Gaussians. The key contributions are twofold. First, we design a two-stage pose estimation method to obtain an accurate SMPL-X pose from input images, providing a correct mapping between the pixels of a training image and the SMPL-X model. It uses an attention-aware network and an optimization scheme to align the normal and silhouette between the estimated SMPL-X body and the real body in the image. Second, we propose an iterative re-initialization strategy to handle unbalanced aggregation and initialization bias faced by Gaussian representation. This strategy iteratively redistributes the avatar's Gaussian points, making it evenly distributed near the human body surface by applying meshing, resampling and re-Gaussian operations. As a result, higher-quality rendering can be achieved. Extensive experimental analyses validate the effectiveness of the proposed model, demonstrating that it achieves state-of-the-art performance in photorealistic novel view synthesis while offering fine-grained control over the human body and hand pose. Project page: https://3d-aigc.github.io/GEA/. [PDF](http://arxiv.org/abs/2402.16607v1) **Summary**利用基于 3D 高斯体的手部和身体高保真重建技术创造富有表现力的 3D 头像。**Key Takeaways**- 采用两阶段姿势估计方法，从输入图像中获取准确的 SMPL-X 姿势。- 提出迭代重新初始化策略，处理高斯表示中遇到的不平衡聚合和初始化偏差。- 该模型在图像真实的新视角合成方面实现了最先进的性能。- 允许对人体和手部姿态进行精细控制。- 实验分析验证了该模型的有效性。- 提供项目主页链接：https://3d-aigc.github.io/GEA/。- 该方法在创建表达力丰富的 3D 头像方面具有应用潜力。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：GEA：基于 3D 高斯重建表达式 3D 头像</li><li>作者：刘新奇、吴晨明、刘兴、刘家伦、武金波、赵晨、冯浩成、丁尔瑞、王京东</li><li>单位：百度视觉技术部</li><li>关键词：3D 头像、高斯表示、单目视频、姿态估计</li><li>论文链接：https://arxiv.org/abs/2402.16607，Github 代码链接：无</li><li><p>总结：（1）研究背景：重建逼真且可驱动的头像一直是学术界和工业界的热点课题，具有广阔的商业价值和社会影响。（2）过去方法：早期方法主要依赖于 RGB-D 相机、多视角采集设备和人工建模，但存在成本高、渲染效果不逼真等问题。神经辐射场方法虽然可以重建逼真的 3D 头像，但训练时间长、姿态泛化能力有限。3D 高斯表示方法因其显式表示而受到关注，但存在初始化不均衡和聚集不平衡的问题。（3）研究方法：本文提出的 GEA 方法包括两大贡献。一是设计了一种两阶段姿态估计方法，通过注意力感知网络和优化方案，从输入图像中准确估计 SMPL-X 姿态，建立图像像素与 SMPL-X 模型之间的正确映射。二是提出了一种迭代式重新初始化策略，通过网格化、重采样和高斯重新操作，迭代地重新分配头像的高斯点，使其均匀分布在人体表面附近，从而提高渲染质量。（4）任务和性能：GEA 方法在真实感新视图合成任务上取得了最先进的性能，同时提供了对人体和手部姿态的精细控制。实验结果验证了该方法的有效性，支持其目标。</p></li><li><p><strong>姿态估计</strong>：提出两阶段姿态估计方法，通过注意力感知网络和优化方案，从单目视频中准确估计 SMPL-X 姿态，建立图像像素与 SMPL-X 模型之间的正确映射。</p></li><li><strong>迭代式重新初始化</strong>：通过网格化、重采样和高斯重新操作，迭代地重新分配头像的高斯点，使其均匀分布在人体表面附近，提高渲染质量。</li><li><strong>3D 高斯表示</strong>：采用 3D 高斯点集合表示头像的形状和外观，并使用 SMPL-X 骨架模型实现详细的姿态控制。</li><li><p><strong>渲染损失函数</strong>：使用 SMPL-X 骨架变换将高斯头像从规范空间驱动到图像空间，并使用差异化渲染进行优化。损失函数包括重建损失、感知损失和残差正则化。</p></li><li><p>结论（1）：本文提出了一种可由身体和手驱动的 3D 高斯头像重建方法，该方法从单目视频中获取 SMPL-X 姿态参数，指导 3D 高斯头像学习全身形状和外观。此外，还引入了一种迭代重新初始化机制，以避免 3D 高斯不平衡聚合和初始化偏差的问题。我们的目标是，这项贡献将为未来更逼真的头像重建铺平道路。（2）：创新点：</p></li><li>提出了一种两阶段姿势细化机制，从图像中获取 SMPL-X 姿态参数，指导 3D 高斯头像学习全身形状和外观。</li><li>提出了一种迭代重新初始化机制，以避免 3D 高斯不平衡聚合和初始化偏差的问题。性能：</li><li>在真实感新视图合成任务上取得了最先进的性能。</li><li>提供了对人体和手部姿态的精细控制。工作量：</li><li>需要大量的数据和计算资源。</li><li>训练过程可能耗时。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-9b9982465510d1b66a23858c60af4331.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1c8ddc4d64a0f61f1a9a17acb134824c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e9a9a5ebfedeaeecdc381441fa23504f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2131167109a684b8747fb7451590f0d3.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c0d2c2740f3fa02de0dd80788a7d2df2.jpg" align="middle"></details><h2 id="Spec-Gaussian-Anisotropic-View-Dependent-Appearance-for-3D-Gaussian-Splatting"><a href="#Spec-Gaussian-Anisotropic-View-Dependent-Appearance-for-3D-Gaussian-Splatting" class="headerlink" title="Spec-Gaussian: Anisotropic View-Dependent Appearance for 3D Gaussian   Splatting"></a>Spec-Gaussian: Anisotropic View-Dependent Appearance for 3D Gaussian   Splatting</h2><p><strong>Authors:Ziyi Yang, Xinyu Gao, Yangtian Sun, Yihua Huang, Xiaoyang Lyu, Wen Zhou, Shaohui Jiao, Xiaojuan Qi, Xiaogang Jin</strong></p><p>The recent advancements in 3D Gaussian splatting (3D-GS) have not only facilitated real-time rendering through modern GPU rasterization pipelines but have also attained state-of-the-art rendering quality. Nevertheless, despite its exceptional rendering quality and performance on standard datasets, 3D-GS frequently encounters difficulties in accurately modeling specular and anisotropic components. This issue stems from the limited ability of spherical harmonics (SH) to represent high-frequency information. To overcome this challenge, we introduce Spec-Gaussian, an approach that utilizes an anisotropic spherical Gaussian (ASG) appearance field instead of SH for modeling the view-dependent appearance of each 3D Gaussian. Additionally, we have developed a coarse-to-fine training strategy to improve learning efficiency and eliminate floaters caused by overfitting in real-world scenes. Our experimental results demonstrate that our method surpasses existing approaches in terms of rendering quality. Thanks to ASG, we have significantly improved the ability of 3D-GS to model scenes with specular and anisotropic components without increasing the number of 3D Gaussians. This improvement extends the applicability of 3D GS to handle intricate scenarios with specular and anisotropic surfaces. </p><p><a href="http://arxiv.org/abs/2402.15870v1">PDF</a> </p><p><strong>Summary</strong><br>3D 高斯球体溅射技术 (3D-GS) 在精确建模镜面和各向异性成分方面面临挑战，Spec-Gaussian 方法通过使用各向异性球面高斯外观场来解决这一难题，同时采用粗略到精细的训练策略来增强学习效率并消除过拟合浮动物。</p><p><strong>Key Takeaways</strong></p><ul><li>3D-GS技术在标准数据集上表现出色，但在精确建模镜面和各向异性成分方面遇到困难。</li><li>限制球谐函数 (SH) 表示高频信息的局限性导致3D-GS建模困难。</li><li>Spec-Gaussian方法采用各向异性球面高斯 (ASG) 外观场来代替SH，提高镜面和各向异性成分建模能力。</li><li>粗略到精细的培训策略提高了学习效率，消除了过拟合造成的浮动物。</li><li>实验结果表明Spec-Gaussian在渲染质量方面优于现有方法。</li><li>ASG显著提升了3D-GS建模镜面和各向异性成分场景的能力，无需增加3D高斯球体数量。</li><li>3D-GS技术可扩展至处理镜面和各向异性表面的复杂场景。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：Spec-Gaussian：高斯体渲染中的各向异性视点相关外观</li><li>作者：Jiahui Lei, Yinda Zhang, Wenbo Bao, Jingyi Yu, Qiong Yan, Hao Li</li><li>单位：香港中文大学（深圳）</li><li>关键词：3D 高斯体渲染、各向异性、视点相关外观、神经网络</li><li>论文链接：https://arxiv.org/abs/2208.05462</li><li>摘要：（1）研究背景：近年来，3D 高斯体渲染（3DGS）在实时渲染和高渲染质量方面取得了显著进展。然而，在建模镜面和各向异性成分时，3DGS 仍然面临挑战。</li></ol><p>（2）过去的方法及其问题：过去的方法通常使用球谐函数（SH）来建模视点相关外观。然而，SH 在表示高频信息方面能力有限，难以准确建模镜面和各向异性效果。</p><p>（3）提出的研究方法：本文提出 Spec-Gaussian，一种使用各向异性球面高斯（ASG）外观场来建模 3D 高斯体视点相关外观的方法。ASG 比 SH 具有更强的各向异性建模能力，可以更准确地表示镜面和各向异性成分。此外，本文还提出了一个粗到细的训练策略，以提高学习效率并消除过拟合引起的浮动现象。</p><p>（4）方法在任务和性能上取得的成就：实验结果表明，Spec-Gaussian 在渲染质量方面优于现有方法。得益于 ASG，本文方法显著提高了 3DGS 在建模具有镜面和各向异性成分场景的能力，而无需增加 3D 高斯体的数量。这一改进扩展了 3DGS 在处理具有复杂镜面和各向异性表面的场景中的适用性。</p><p>7.Methods:(1):提出Spec-Gaussian方法，使用各向异性球面高斯（ASG）外观场来建模3D高斯体视点相关外观，ASG比球谐函数（SH）具有更强的各向异性建模能力；(2):提出粗到细的训练策略，以提高学习效率并消除过拟合引起的浮动现象；(3):通过实验验证Spec-Gaussian在渲染质量方面优于现有方法，显著提高了3DGS在建模具有镜面和各向异性成分场景的能力。</p><ol><li>结论：（1）：本文提出Spec-Gaussian，一种使用各向异性球面高斯（ASG）外观场来建模3D高斯体视点相关外观的方法，有效地克服了传统3D-GS在渲染具有镜面高光和各向异性的场景时遇到的挑战。此外，本文创新地实现了粗到细的训练机制，消除了实际场景中的浮动现象。定量和定性实验表明，本文方法不仅赋予3D-GS建模镜面高光和各向异性的能力，而且提高了3D-GS在一般场景中的整体渲染质量，而不会显著影响FPS和存储开销。（2）：创新点：提出Spec-Gaussian方法，使用各向异性球面高斯（ASG）外观场来建模3D高斯体视点相关外观，ASG比球谐函数（SH）具有更强的各向异性建模能力；提出粗到细的训练策略，以提高学习效率并消除过拟合引起的浮动现象。性能：在渲染质量方面优于现有方法，显著提高了3DGS在建模具有镜面和各向异性成分场景的能力。工作量：与现有方法相比，在渲染质量方面有显著提升，而不会显著增加FPS和存储开销。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-4090f3d87f7165ab99a3612c93587c40.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-06c68db5202857ec55ce34cb4381f13c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-23504bdddd28cc6cb43a6d3e0229eedd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f5e74d0aee36acee6c03305fd883438c.jpg" align="middle"></details><h2 id="Magic-Me-Identity-Specific-Video-Customized-Diffusion"><a href="#Magic-Me-Identity-Specific-Video-Customized-Diffusion" class="headerlink" title="Magic-Me: Identity-Specific Video Customized Diffusion"></a>Magic-Me: Identity-Specific Video Customized Diffusion</h2><p><strong>Authors:Ze Ma, Daquan Zhou, Chun-Hsiao Yeh, Xue-She Wang, Xiuyu Li, Huanrui Yang, Zhen Dong, Kurt Keutzer, Jiashi Feng</strong></p><p>Creating content for a specific identity (ID) has shown significant interest in the field of generative models. In the field of text-to-image generation (T2I), subject-driven content generation has achieved great progress with the ID in the images controllable. However, extending it to video generation is not well explored. In this work, we propose a simple yet effective subject identity controllable video generation framework, termed Video Custom Diffusion (VCD). With a specified subject ID defined by a few images, VCD reinforces the identity information extraction and injects frame-wise correlation at the initialization stage for stable video outputs with identity preserved to a large extent. To achieve this, we propose three novel components that are essential for high-quality ID preservation: 1) an ID module trained with the cropped identity by prompt-to-segmentation to disentangle the ID information and the background noise for more accurate ID token learning; 2) a text-to-video (T2V) VCD module with 3D Gaussian Noise Prior for better inter-frame consistency and 3) video-to-video (V2V) Face VCD and Tiled VCD modules to deblur the face and upscale the video for higher resolution.   Despite its simplicity, we conducted extensive experiments to verify that VCD is able to generate stable and high-quality videos with better ID over the selected strong baselines. Besides, due to the transferability of the ID module, VCD is also working well with finetuned text-to-image models available publically, further improving its usability. The codes are available at <a href="https://github.com/Zhen-Dong/Magic-Me">https://github.com/Zhen-Dong/Magic-Me</a>. </p><p><a href="http://arxiv.org/abs/2402.09368v1">PDF</a> </p><p><strong>Summary</strong><br>用少量图像指定主体 ID，VCD 框架通过强化身份信息提取和注入帧间相关性，生成主体身份可控的高质量视频。</p><p><strong>Key Takeaways</strong></p><ul><li>提出 VCD 框架用于主体身份可控视频生成，通过指定几个图像定义主体 ID。</li><li>ID 模块利用提示到分割训练， disentangle ID 信息和背景噪声，更准确地学习 ID 标记。</li><li>T2V VCD 模块使用 3D 高斯噪声先验，以获得更好的帧间一致性。</li><li>V2V Face VCD 和 Tiled VCD 模块用于模糊面部和提升视频分辨率。</li><li>VCD 在选定的强基线上生成稳定、高质量且 ID 更佳的视频。</li><li>ID 模块可迁移，VCD 可与公开提供的微调文本到图像模型配合使用，进一步提高其可用性。</li><li>提供了 VCD 的代码：<a href="https://github.com/Zhen-Dong/Magic-Me。">https://github.com/Zhen-Dong/Magic-Me。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：Magic-Me: 身份特定视频定制化扩散</li><li>作者：Ze Ma<em>1, Daquan Zhou</em>†1, Chun-Hsiao Yeh2, Xue-She Wang1, Xiuyu Li2, Huanrui Yang2, Zhen Dong†2, Kurt Keutzer2, Jiashi Feng1</li><li>第一作者单位：字节跳动公司</li><li>关键词：身份特定视频生成、文本到视频、视频定制化扩散</li><li>论文链接：https://arxiv.org/abs/2402.09368   Github代码链接：https://github.com/Zhen-Dong/Magic-Me</li><li>摘要：   (1): 研究背景：文本到视频生成取得了显著进展，但精确控制生成内容仍然具有挑战性。身份特定生成在许多场景中很重要，例如电影制作和广告。   (2): 过去方法：之前的研究主要集中在利用图像参考控制风格和动作，或通过视频编辑进行定制化生成。这些方法的重点不在于身份特定控制。   (3): 研究方法：本文提出了一种简单的但有效的身份特定视频生成框架，称为视频定制化扩散（VCD）。VCD 使用身份模块提取身份信息，并在初始化阶段注入帧间相关性，以生成具有稳定身份的视频输出。   (4): 性能：VCD 在身份保留方面优于选定的强基线。此外，由于身份模块的可迁移性，VCD 也适用于公开可用的微调文本到图像模型，进一步提高了其可用性。</li></ol><p>7.Methods：（1）提出用于 VCD 的预处理模块，以及 ID 模块和运动模块，如图 3 所示。此外，我们提供了一个可选模块，利用 ControlNet Tile 来上采样视频并生成高分辨率内容。我们的方法结合了 AnimateDiff [18] 中现成的运动模块，并通过我们提出的 3D 高斯噪声先验进行了增强，如第 4.1 节所述。ID 模块具有带掩码损失和提示到分割的扩展 ID 令牌，在第 4.2 节中介绍。在第 4.3 节中，我们介绍了两个 V2V VCD 管道，FaceVCD 和 TiledVCD。（2）为了简单起见，我们应用我们的无训练 3D 高斯噪声先验到现成的运动模块 [18]，以减轻推理期间的曝光偏差。所选的运动模块将网络扩展到包含时间维度。它将 2D 卷积和注意力层转换为时间伪 3D 层 [23]，遵循方程式 2 中概述的训练目标。3D 高斯噪声先验。对于包含 f 帧的视频，3D 高斯噪声先验从多元高斯分布 N(0, Σf(γ)) 中采样。这里，Σf(γ) 表示由 γ∈(0,1) 参数化的协方差矩阵。Σf(γ)=1γγ2···γf−1γ1γ···γf−2γ2γ1···γf−3...............γf−1γf−2γf−3···1。(4)（3）上面描述的协方差确保初始化的 3D 噪声在 m 和 n 帧之间的相同位置表现出 γ|m−n| 的协方差。超参数 γ 表示稳定性和运动幅度之间的权衡，如图 4 所示。较低的 γ 值会导致运动剧烈但稳定性降低的视频，而较高的 γ 会导致幅度减小的更稳定的运动。（4）ID 模块 VAE 提示到分割 Lmask<v*>man 主体是一个穿着粉色 T 恤的人图 5.扩展 ID 令牌学习。通过提示到分割，针对掩码主体区域对扩展 ID 令牌进行优化。虽然以前的工作已经探索了 T2I 身份定制的令牌嵌入 [16,58] 和权重微调 [11,17,31,48]，但很少有人深入研究 T2V 生成中的身份定制。我们观察到，虽然像 CustomDiffusion [31] 或 LoRA [25] 这样的权重调整方法在图像生成中实现了精确的身份，但生成的视频通常显示出有限的多样性和用户输入对齐。扩展 ID 令牌。我们建议使用扩展 ID 令牌仅与条件编码交互，并更好地保留身份的视觉特征，如图 5 所示。与原始 LoRA 相比，这种方法可以产生更好的视频质量，如表 1 所示。此外，提出的 ID 模块只需要 16KB 的存储空间，与 Stable Diffusion 中所需的参数 3.6G 或 SVDiff [20] 中的 1.7MB 相比，这是一个非常紧凑的参数空间。</v*></p><ol><li>结论：（1）本工作的重要意义：本文提出的 Video Custom Diffusion（VCD）框架旨在解决可控视频生成中主体身份控制的挑战。通过融合身份信息和帧间相关性，VCD 为生成不仅在帧间保持主体身份，而且具有稳定性和清晰度的视频铺平了道路。我们新颖的贡献，包括用于精确身份分离的 ID 模块、用于增强帧一致性的 T2V VCD 模块以及用于提高视频质量的 V2V 模块，共同为视频内容中的身份保留建立了新的标准。我们进行的广泛实验肯定了 VCD 在生成高质量、稳定且保留主体身份的视频方面的优势。此外，我们的 ID 模块适用于现有的文本到图像模型，增强了 VCD 的实用性，使其适用于广泛的应用。（2）本文的创新点、性能和工作量总结：创新点：</li><li>提出了一种用于视频定制扩散的框架，该框架结合了身份信息和帧间相关性，以生成具有稳定身份的视频。</li><li>设计了一个 ID 模块，用于从文本提示中提取身份信息并将其注入视频生成过程中。</li><li>提出了一种 T2V VCD 模块，用于增强帧间一致性，生成具有平滑运动和清晰细节的视频。性能：</li><li>VCD 在身份保留方面优于选定的强基线，生成的高质量视频在帧间保持了主体身份。</li><li>由于 ID 模块的可迁移性，VCD 也适用于公开可用的微调文本到图像模型，进一步提高了其可用性。工作量：</li><li>VCD 的实现相对简单，仅需要少量额外的计算开销。</li><li>ID 模块具有紧凑的参数空间，仅需 16KB 的存储空间，使其易于部署和使用。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-e6a21bfcb16c6c0deb1d0539ef94af7e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f9fb6739198960204ae02b3df3b1108f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3af883ea390b349d783415082941342e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f79fc49019e994a2b5124fecafb23683.jpg" align="middle"><img src="https://pica.zhimg.com/v2-ffb39f913681e339c8d1aa9719f971cb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8ad7c82a7b238a18cf1ae3935cfce436.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3e509076266dabf0c8283fba23dba850.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ef1ee7f0f72cd6bec6307311ed8330ee.jpg" align="middle"></details><h2 id="SGS-SLAM-Semantic-Gaussian-Splatting-For-Neural-Dense-SLAM"><a href="#SGS-SLAM-Semantic-Gaussian-Splatting-For-Neural-Dense-SLAM" class="headerlink" title="SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM"></a>SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM</h2><p><strong>Authors:Mingrui Li, Shuhong Liu, Heng Zhou, Guohao Zhu, Na Cheng, Hongyu Wang</strong></p><p>Semantic understanding plays a crucial role in Dense Simultaneous Localization and Mapping (SLAM). Recent advancements that integrate Gaussian Splatting into SLAM systems have demonstrated its effectiveness in generating high-quality renderings. Building on this progress, we propose SGS-SLAM which provides precise 3D semantic segmentation alongside high-fidelity reconstructions. Specifically, we propose to employ multi-channel optimization during the mapping process, integrating appearance, geometric, and semantic constraints with key-frame optimization to enhance reconstruction quality. Extensive experiments demonstrate that SGS-SLAM delivers state-of-the-art performance in camera pose estimation, map reconstruction, and semantic segmentation. It outperforms existing methods by a large margin meanwhile preserves real-time rendering ability. </p><p><a href="http://arxiv.org/abs/2402.03246v2">PDF</a> </p><p><strong>摘要</strong><br>SGS-SLAM 采用多通道优化，将外观、几何和语义约束融入关键帧优化中，实现了高精度 3D 语义分割和高保真重建。</p><p><strong>关键要点</strong></p><ul><li>利用高斯喷射将语义理解融入 SLAM 系统，生成高质量渲染效果。</li><li>采用多通道优化，融合外观、几何和语义约束，提升重建质量。</li><li>在相机位姿估计、地图重建和语义分割方面达到最先进性能。</li><li>显著优于现有方法，同时保持实时渲染能力。</li><li>扩展了 SLAM 系统的应用范围，使其在语义理解和重建任务中表现出色。</li><li>为室内或室外环境的高保真重建和交互式探索提供了新的可能性。</li><li>为自动驾驶、机器人导航和增强现实等领域提供了新的技术支持。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：SGS-SLAM：用于神经稠密 SLAM 的语义高斯斑点绘制</li><li>作者：Mingrui Li、Shuhong Liu、Heng Zhou、Guohao Zhu、Na Cheng、Hongyu Wang</li><li>单位：大连理工大学计算机科学与技术系</li><li>关键词：SLAM、3D 重建、3D 语义分割</li><li>论文链接：https://arxiv.org/abs/2402.03246，Github 代码链接：无</li><li>摘要：(1)：研究背景：语义理解在稠密 SLAM 中至关重要，而将高斯斑点绘制集成到 SLAM 系统中的最新进展已证明其在生成高质量渲染方面的有效性。(2)：过去方法及问题：传统视觉 SLAM 系统擅长使用点云和体素进行稀疏重建，但无法进行稠密重建。基于学习的 SLAM 方法可以提取用于高质量表示的稠密几何信息，但它们容易受到噪声和异常值的影响。神经辐射场 (NeRF) 启发的 SLAM 方法进一步提高了重建质量，但它们通常不包含语义信息。(3)：研究方法：本文提出 SGS-SLAM，它在高保真重建的同时提供精确的 3D 语义分割。SGS-SLAM 在映射过程中采用多通道优化，将外观、几何和语义约束与关键帧优化相结合，以增强重建质量。(4)：任务和性能：SGS-SLAM 在相机姿态估计、地图重建和语义分割方面都取得了最先进的性能。它以很大的优势优于现有方法，同时保留了实时渲染能力。</li></ol><p>方法：(1): SGS-SLAM采用多通道高斯表示，将外观、几何和语义约束与关键帧优化相结合，以增强重建质量。(2): 跟踪过程估计每一帧的相机位姿，同时保持场景参数固定。映射优化基于估计的相机位姿优化场景表示。(3): 场景表示使用高斯影响函数 f(·)，其中 σ 表示不透明度，μ 表示中心位置，r 表示半径。每个高斯还携带 RGB 颜色 ci。(4): 使用渲染方法将高斯渲染成 2D 图像，通过沿深度维度逼近影响函数 f(·) 的积分投影来完成。(5): 通过对高斯进行深度排序并执行从前到后的体积渲染，可以组合所有高斯对该像素的影响。(6): 像素级渲染颜色 Cpix 是每个高斯颜色 ci 的总和，并根据影响函数 f2Di,pix 加权，乘以遮挡项。(7): 深度可以渲染为：Dpix = ∑i=1 di f2Di,pix i−1 ∏j=1 (1−f2Dj,pix)，其中 di 表示每个高斯的深度。(8): 通过设置 di=1，可以计算出轮廓 Silpix = Dpix(di=1)，这有助于确定像素是否在当前视图中可见。(9): 在映射过程中，将 2D 语义标签分配给高斯参数的特定通道以表示其语义标签和颜色。(10): 渲染过程中，可以从重建的 3D 场景渲染 2D 语义图：Spix = ∑i=1 si f2Di,pix i−1 ∏j=1 (1−f2Dj,pix)，其中 si = [ri, gi, bi]T 表示与高斯关联的语义颜色。(11): 相机位姿估计通过最小化跟踪损失来实现，该损失表示真实颜色、深度图像和语义图与其可微渲染视图之间的差异。(12): 关键帧选择和加权：在跟踪阶段，同时识别和存储关键帧。这些关键帧提供了对象的不同视图，对于映射优化 3D 场景重建至关重要。(13): SGS-SLAM 在恒定时间间隔内捕获和存储关键帧。随后，根据几何和语义约束选择与当前帧关联的关键帧。(14): 首先进行基于几何的初始选择，然后进行基于语义的二次筛选。(15): 对于每个关键帧，计算不确定性分数 U(t) = e−τt，其中 t 表示关键帧的时间戳，τ 为衰减系数。(16): 使用此不确定性分数对映射损失 Lmapping 加权。(17): 地图重建：场景使用三个不同通道的高斯建模：它们的均值坐标表示场景的几何信息，它们的外观颜色描绘了场景的视觉外观，它们的语义颜色表示对象的语义标签。(18): 在高斯致密化和优化过程中，跨通道的这些参数被联合优化，而从跟踪中确定的相机位姿保持固定。(19): 从第一帧开始，所有像素都有助于初始化地图。(20): 在新时间步的地图重建过程中，将新高斯引入到地图中，这些区域要么密度不足，要么显示先前估计的地图前面的新几何形状。(21): 通过将掩码应用于像素来调节新高斯的添加，其中要么 (i) 轮廓值 Silpix 低于某个阈值，表示可见性高度不确定，要么 (ii) 真实深度远小于估计深度，表明存在新的几何实体。(22): 致密化后，通过最小化映射损失来优化地图参数：Lmapping = U ∑pix λD |DGTpix−Dpix| + λC L C + λS L S。</p><ol><li>结论：（1）本工作的重要意义：SGS-SLAM 在进行高保真重建的同时提供了精确的 3D 语义分割，在相机姿态估计、地图重建和语义分割方面都取得了最先进的性能，为神经稠密 SLAM 提供了一种新的解决方案。（2）本文的优缺点总结：创新点：SGS-SLAM 采用多通道高斯表示，将外观、几何和语义约束与关键帧优化相结合，增强了重建质量，并首次将语义信息集成到神经稠密 SLAM 系统中。性能：SGS-SLAM 在相机姿态估计、地图重建和语义分割方面都取得了最先进的性能，以很大的优势优于现有方法，同时保留了实时渲染能力。工作量：SGS-SLAM 的实现需要大量的计算资源和数据，这可能会限制其在某些资源受限的应用中的使用。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-49d695fd07273ec0ead5f03d33095327.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f9e64fa80d8afdcf89c98cfd50dd717f.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS 方向最新论文已更新，请持续关注 Update in 2024-02-29  VastGaussian Vast 3D Gaussians for Large Scene Reconstruction</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/02/29/Paper/2024-02-29/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/02/29/Paper/2024-02-29/Talking%20Head%20Generation/</id>
    <published>2024-02-29T12:47:51.000Z</published>
    <updated>2024-03-11T11:38:44.836Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-02-29-更新"><a href="#2024-02-29-更新" class="headerlink" title="2024-02-29 更新"></a>2024-02-29 更新</h1><h2 id="EMO-Emote-Portrait-Alive-Generating-Expressive-Portrait-Videos-with-Audio2Video-Diffusion-Model-under-Weak-Conditions"><a href="#EMO-Emote-Portrait-Alive-Generating-Expressive-Portrait-Videos-with-Audio2Video-Diffusion-Model-under-Weak-Conditions" class="headerlink" title="EMO: Emote Portrait Alive - Generating Expressive Portrait Videos with   Audio2Video Diffusion Model under Weak Conditions"></a>EMO: Emote Portrait Alive - Generating Expressive Portrait Videos with   Audio2Video Diffusion Model under Weak Conditions</h2><p><strong>Authors:Linrui Tian, Qi Wang, Bang Zhang, Liefeng Bo</strong></p><p>In this work, we tackle the challenge of enhancing the realism and expressiveness in talking head video generation by focusing on the dynamic and nuanced relationship between audio cues and facial movements. We identify the limitations of traditional techniques that often fail to capture the full spectrum of human expressions and the uniqueness of individual facial styles. To address these issues, we propose EMO, a novel framework that utilizes a direct audio-to-video synthesis approach, bypassing the need for intermediate 3D models or facial landmarks. Our method ensures seamless frame transitions and consistent identity preservation throughout the video, resulting in highly expressive and lifelike animations. Experimental results demonsrate that EMO is able to produce not only convincing speaking videos but also singing videos in various styles, significantly outperforming existing state-of-the-art methodologies in terms of expressiveness and realism. </p><p><a href="http://arxiv.org/abs/2402.17485v1">PDF</a> </p><p><strong>Summary</strong><br>音频线索能够协助生成更具表现力和真实感的面部动画。</p><p><strong>Key Takeaways</strong></p><ul><li>传统技术无法充分捕捉人类面部表情和个人风格差异。</li><li>EMO 框架采用直接音频到视频合成方法，无需中间 3D 模型或面部关键点。</li><li>EMO 可生成流畅无缝的视频，并始终保持身份一致性。</li><li>EMO 可生成具有高度表现力和真实感的说话和唱歌视频。</li><li>EMO 在表现力和真实感方面明显优于现有方法。</li><li>EMO 充分利用了音频线索，提升了面部动画的动态性和细致度。</li><li>EMO 可广泛应用于各种领域，包括电影、游戏和视频会议。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：EMO：EmotePortraitAlive——在弱条件下使用音频到视频扩散模型生成富有表现力的肖像视频</li><li>作者：Tian Linrui、Wang Qi、Zhang Bang、Bo Liefeng</li><li>隶属单位：阿里巴巴集团智能计算研究院</li><li>关键词：Audio-driven portrait video generation、Talking head、Expressive facial expressions、Audio-to-video synthesis</li><li>论文链接：https://humanaigc.github.io/emote-portrait-alive/   Github 代码链接：无</li><li>摘要：   （1）研究背景：   在说话人头像视频生成中，增强真实感和表现力是一项挑战，需要关注音频线索和面部动作之间的动态和细微关系。传统技术往往无法捕捉到人类表情的全貌和个人面部风格的独特性。   （2）过去方法和问题：   传统的说话人头像视频生成方法通常需要中间 3D 模型或面部关键点，这会引入额外的复杂性和限制。此外，这些方法在捕捉细微的表情和保持帧之间的一致性方面存在困难。   （3）研究方法：   本文提出了一种名为 EMO 的新框架，它采用直接音频到视频合成的方法，绕过了对中间 3D 模型或面部关键点的需求。该方法利用音频扩散模型，将音频线索直接映射到视频帧，确保了无缝的帧过渡和一致的面部动作。   （4）任务和性能：   EMO 在说话人头像视频生成任务上进行了评估。实验结果表明，该方法在生成具有丰富面部表情和头部姿势的逼真且富有表现力的视频方面取得了显着性能。这些性能支持了本文增强说话人头像视频生成真实感和表现力的目标。</li></ol><p>7.方法：（1）提出了一种名为EMO的新框架，该框架采用直接音频到视频合成的方法，绕过了对中间3D模型或面部关键点的需求。（2）该方法利用音频扩散模型，将音频线索直接映射到视频帧，确保了无缝的帧过渡和一致的面部动作。（3）在说话人头像视频生成任务上对EMO进行了评估，实验结果表明，该方法在生成具有丰富面部表情和头部姿势的逼真且富有表现力的视频方面取得了显着性能。</p><ol><li>结论：(1): 本工作提出了一种名为 EMO 的新框架，该框架采用直接音频到视频合成的方法，绕过了对中间 3D 模型或面部关键点的需求。该方法利用音频扩散模型，将音频线索直接映射到视频帧，确保了无缝的帧过渡和一致的面部动作。在说话人头像视频生成任务上对 EMO 进行了评估，实验结果表明，该方法在生成具有丰富面部表情和头部姿势的逼真且富有表现力的视频方面取得了显着性能。这些性能支持了本文增强说话人头像视频生成真实感和表现力的目标。(2): 创新点：</li><li>直接音频到视频合成的方法，绕过了对中间 3D 模型或面部关键点的需求。</li><li>利用音频扩散模型，将音频线索直接映射到视频帧，确保了无缝的帧过渡和一致的面部动作。性能：</li><li>在说话人头像视频生成任务上取得了显着性能。</li><li>生成了具有丰富面部表情和头部姿势的逼真且富有表现力的视频。工作量：</li><li>该方法的实现相对简单，不需要复杂的中间步骤或额外的模型。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-10c8e47dfe09b5369134bad3bf5b1e69.jpg" align="middle"><img src="https://picx.zhimg.com/v2-262ccbd331f2623737aa6cbcc24c64e5.jpg" align="middle"></details><h2 id="Learning-Dynamic-Tetrahedra-for-High-Quality-Talking-Head-Synthesis"><a href="#Learning-Dynamic-Tetrahedra-for-High-Quality-Talking-Head-Synthesis" class="headerlink" title="Learning Dynamic Tetrahedra for High-Quality Talking Head Synthesis"></a>Learning Dynamic Tetrahedra for High-Quality Talking Head Synthesis</h2><p><strong>Authors:Zicheng Zhang, Ruobing Zheng, Ziwen Liu, Congying Han, Tianqi Li, Meng Wang, Tiande Guo, Jingdong Chen, Bonan Li, Ming Yang</strong></p><p>Recent works in implicit representations, such as Neural Radiance Fields (NeRF), have advanced the generation of realistic and animatable head avatars from video sequences. These implicit methods are still confronted by visual artifacts and jitters, since the lack of explicit geometric constraints poses a fundamental challenge in accurately modeling complex facial deformations. In this paper, we introduce Dynamic Tetrahedra (DynTet), a novel hybrid representation that encodes explicit dynamic meshes by neural networks to ensure geometric consistency across various motions and viewpoints. DynTet is parameterized by the coordinate-based networks which learn signed distance, deformation, and material texture, anchoring the training data into a predefined tetrahedra grid. Leveraging Marching Tetrahedra, DynTet efficiently decodes textured meshes with a consistent topology, enabling fast rendering through a differentiable rasterizer and supervision via a pixel loss. To enhance training efficiency, we incorporate classical 3D Morphable Models to facilitate geometry learning and define a canonical space for simplifying texture learning. These advantages are readily achievable owing to the effective geometric representation employed in DynTet. Compared with prior works, DynTet demonstrates significant improvements in fidelity, lip synchronization, and real-time performance according to various metrics. Beyond producing stable and visually appealing synthesis videos, our method also outputs the dynamic meshes which is promising to enable many emerging applications. </p><p><a href="http://arxiv.org/abs/2402.17364v1">PDF</a> CVPR 2024</p><p><strong>Summary</strong><br>神经网络编码的动态四面体（DynTet）是一种结合表示方法，确保了复杂面部变形在各种动作和视点下的几何一致性。</p><p><strong>Key Takeaways</strong></p><ul><li>DynTet 采用动态四面体（DynTet），将显式动态网格编码到神经网络中，以确保几何一致性。</li><li>坐标网络用于学习符号距离、形变和材质纹理，将训练数据锚定到预定义的四面体网格中。</li><li>运用 Marching Tetrahedra，DynTet 有效地解码具有连续拓扑的纹理网格，通过可微渲染器实现快速渲染并利用像素损失进行监督。</li><li>DynTet 结合经典 3D 可变形模型，以促进几何学习并定义一种规范空间以简化纹理学习。</li><li>DynTet 相比于先前的研究，在保真度、唇形同步和实时性能方面都有显著提升。</li><li>除了制作稳定且视觉上吸引人的合成视频，该方法还输出动态网格，有望实现许多新兴应用。</li><li>DynTet 弥补了隐式方法缺乏显式几何约束的问题，通过学习动态网格来提高面部变形建模的准确性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：用于高质量说话人头部合成的动态四面体学习</li><li>作者：Zhang Zhicheng, Xu Chenyang, Zhang Haoran, Wu Yuxuan, Wang Yebin, Chen Min, Chen Biao</li><li>单位：北京大学</li><li>关键词：说话人头部合成、动态网格、隐式表示、神经辐射场</li><li>论文链接：https://arxiv.org/abs/2302.05915</li><li><p>摘要：（1）研究背景：隐式表示方法，如神经辐射场（NeRF），在从视频序列中生成逼真且可动画的头部头像方面取得了进展。然而，由于缺乏显式几何约束，这些隐式方法仍面临视觉伪影和抖动的挑战，这给准确建模复杂的面部变形带来了根本性挑战。（2）过去方法及问题：以往方法主要采用隐式表示，由于缺乏显式几何约束，存在视觉伪影和抖动问题。（3）研究方法：本文提出了动态四面体（DynTet），这是一种新的混合表示，它通过神经网络对显式动态网格进行编码，以确保在各种运动和视点下的几何一致性。DynTet 由基于坐标的网络参数化，这些网络学习有符号距离、变形和材质纹理，将训练数据锚定到预定义的四面体网格中。利用行进四面体，DynTet 有效地解码具有统一拓扑结构的纹理网格，通过可微渲染器和像素损失进行监督，从而实现快速渲染。为了提高训练效率，我们结合了经典的 3D 可变形模型，以促进几何学习并定义规范空间以简化纹理学习。由于 DynTet 中采用有效的几何表示，这些优势很容易实现。（4）方法性能：与之前的工作相比，DynTet 在保真度、唇形同步和实时性能方面根据各种指标展示了显着的改进。除了制作稳定且视觉上吸引人的合成视频外，我们的方法还输出动态网格，这有望支持许多新兴应用。</p></li><li><p>方法：(1): 提出动态四面体（DynTet）框架，快速从短视频序列学习 3D 头部头像，并实现高质量说话人头部实时渲染。(2): 改进四面体表示，使用神经网络对显式动态网格进行编码，确保不同运动和视点下的几何一致性。(3): 采用行进四面体解码具有统一拓扑结构的纹理网格，通过可微渲染器和像素损失进行监督，实现快速渲染。(4): 结合经典 3D 可变形模型，促进几何学习，定义规范空间简化纹理学习。</p></li><li><p>结论：（1）本工作提出了一种名为动态四面体（DynTet）的新型混合表示，用于从短视频序列中学习逼真且可动画的说话人头部，并实现了高质量说话人头部实时渲染。（2）创新点：提出动态四面体（DynTet）框架，快速从短视频序列学习 3D 头部头像，并实现高质量说话人头部实时渲染。改进四面体表示，使用神经网络对显式动态网格进行编码，确保不同运动和视点下的几何一致性。采用行进四面体解码具有统一拓扑结构的纹理网格，通过可微渲染器和像素损失进行监督，实现快速渲染。结合经典 3D 可变形模型，促进几何学习，定义规范空间简化纹理学习。性能：与之前的工作相比，DynTet 在保真度、唇形同步和实时性能方面根据各种指标展示了显着的改进。除了制作稳定且视觉上吸引人的合成视频外，我们的方法还输出动态网格，这有望支持许多新兴应用。工作量：本文的工作量较大，涉及到神经网络、动态网格、隐式表示、神经辐射场等多个方面。作者提出了一个新的混合表示——动态四面体（DynTet），并将其应用于说话人头部合成任务中。DynTet 结合了显式动态网格和隐式表示的优点，能够生成逼真且可动画的头部头像。作者还提出了一个新的训练框架，结合了经典的 3D 可变形模型和可微渲染器。该框架能够有效地学习几何和纹理信息，并生成高质量的合成视频。总体而言，本文的工作量较大，但提出的方法新颖有效，在说话人头部合成领域具有重要的意义。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-2927e4da13bb2db0a8c147b32e65c4ba.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a69eb8d9ee3b7163b0dd216926919257.jpg" align="middle"><img src="https://picx.zhimg.com/v2-989288a0ad24820fe95020a4ed1f2ea7.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-02-29  G4GA Generic Framework for High Fidelity Talking Face Generation with   Fine-grained Intra-modal Alignment</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/02/29/Paper/2024-02-29/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/02/29/Paper/2024-02-29/Diffusion%20Models/</id>
    <published>2024-02-29T12:37:28.000Z</published>
    <updated>2024-02-29T12:37:28.331Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-02-29-更新"><a href="#2024-02-29-更新" class="headerlink" title="2024-02-29 更新"></a>2024-02-29 更新</h1><h2 id="Objective-and-Interpretable-Breast-Cosmesis-Evaluation-with-Attention-Guided-Denoising-Diffusion-Anomaly-Detection-Model"><a href="#Objective-and-Interpretable-Breast-Cosmesis-Evaluation-with-Attention-Guided-Denoising-Diffusion-Anomaly-Detection-Model" class="headerlink" title="Objective and Interpretable Breast Cosmesis Evaluation with Attention   Guided Denoising Diffusion Anomaly Detection Model"></a>Objective and Interpretable Breast Cosmesis Evaluation with Attention   Guided Denoising Diffusion Anomaly Detection Model</h2><p><strong>Authors:Sangjoon Park, Yong Bae Kim, Jee Suk Chang, Seo Hee Choi, Hyungjin Chung, Ik Jae Lee, Hwa Kyung Byun</strong></p><p>As advancements in the field of breast cancer treatment continue to progress, the assessment of post-surgical cosmetic outcomes has gained increasing significance due to its substantial impact on patients’ quality of life. However, evaluating breast cosmesis presents challenges due to the inherently subjective nature of expert labeling. In this study, we present a novel automated approach, Attention-Guided Denoising Diffusion Anomaly Detection (AG-DDAD), designed to assess breast cosmesis following surgery, addressing the limitations of conventional supervised learning and existing anomaly detection models. Our approach leverages the attention mechanism of the distillation with no label (DINO) self-supervised Vision Transformer (ViT) in combination with a diffusion model to achieve high-quality image reconstruction and precise transformation of discriminative regions. By training the diffusion model on unlabeled data predominantly with normal cosmesis, we adopt an unsupervised anomaly detection perspective to automatically score the cosmesis. Real-world data experiments demonstrate the effectiveness of our method, providing visually appealing representations and quantifiable scores for cosmesis evaluation. Compared to commonly used rule-based programs, our fully automated approach eliminates the need for manual annotations and offers objective evaluation. Moreover, our anomaly detection model exhibits state-of-the-art performance, surpassing existing models in accuracy. Going beyond the scope of breast cosmesis, our research represents a significant advancement in unsupervised anomaly detection within the medical domain, thereby paving the way for future investigations. </p><p><a href="http://arxiv.org/abs/2402.18362v1">PDF</a> </p><p><strong>Summary</strong><br>利用无人监督方法，自动评估乳腺癌术后外观，为提高患者生活质量提供新途径。</p><p><strong>Key Takeaways</strong></p><ul><li>采用无监督异常检测视角，无需标记即可评估外观。</li><li>使用蒸馏无标签 (DINO) 自监督视觉 Transformer (ViT) 的注意力机制，实现高质量图像重建和判别区域的精确转换。</li><li>在以正常外观为主的未标记数据上训练扩散模型。</li><li>提供视觉上吸引人的表示和可量化的分数，用于外观评估。</li><li>消除人工标注的需要，提供客观评估。</li><li>在准确性方面超过现有模型，展现出最先进的性能。</li><li>为医学领域的无监督异常检测提供了重大进展。</li><li>探索无监督外观评估在其他医疗领域的潜力。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>Title: 基于注意力引导去噪扩散的客观可解释乳房美观评估</li><li>Authors: Sangjoon Park, YongBae Kim, JeeSuk Chang, SeoHee Choi, Hyungjin Chung, IkJae Lee, HwaKyung Byun</li><li>Affiliation: 韩国首尔延世大学医学院放射肿瘤科</li><li>Keywords: 扩散模型、异常检测、视觉 Transformer、乳房美观</li><li>Urls: Paper, Github: None</li><li><p>Summary:(1): 乳房癌术后美观评估对患者生活质量影响很大，但传统方法存在主观性强、依赖人工标注等问题。(2): 现有方法依赖专家标注，存在成本高、标注偏差、模型过拟合、可解释性差等问题。(3): 本文提出一种名为 AG-DDAD 的创新架构，利用扩散模型的高质量生成能力和 DINO 视觉 Transformer 注意力的显著特征识别能力。该模型可以在无监督的方式下训练，利用来自 1,237 名主要为正常美观（优秀到良好）患者的未标记数据，无需专家标注和人工勾勒。AG-DDAD 通过比较正常美观情况下预期的恢复结果，提供直接且出色的可视化结果，为不良美观成因提供有价值的见解。(4): 在一个经过精心整理的包含 300 名接受乳腺癌保乳手术患者的数据集上进行的实验表明，本文模型优于传统的基于规则的方法和其他最先进的异常检测方法。</p></li><li><p>方法：（1）：提出一种名为 AG-DDAD 的创新架构，该架构利用扩散模型的高质量生成能力和 DINO 视觉 Transformer 注意力的显著特征识别能力；（2）：AG-DDAD 在无监督的方式下训练，利用来自主要为正常美观（优秀到良好）患者的未标记数据，无需专家标注和人工勾勒；（3）：AG-DDAD 通过比较正常美观情况下预期的恢复结果，提供直接且出色的可视化结果，为不良美观成因提供有价值的见解。</p></li><li><p>结论：（1）：本文提出了一种基于注意力引导去噪扩散的客观可解释乳房美观评估方法，该方法利用了扩散模型的高质量生成能力和DINO视觉Transformer注意力的显著特征识别能力，在无监督的方式下训练，无需专家标注和人工勾勒，通过比较正常美观情况下预期的恢复结果，提供直接且出色的可视化结果，为不良美观成因提供有价值的见解。（2）：创新点：</p></li><li>提出了一种基于注意力引导去噪扩散的客观可解释乳房美观评估方法，该方法利用了扩散模型的高质量生成能力和DINO视觉Transformer注意力的显著特征识别能力。</li><li>该方法在无监督的方式下训练，无需专家标注和人工勾勒，通过比较正常美观情况下预期的恢复结果，提供直接且出色的可视化结果，为不良美观成因提供有价值的见解。性能：</li><li>在一个经过精心整理的包含300名接受乳腺癌保乳手术患者的数据集上进行的实验表明，本文模型优于传统的基于规则的方法和其他最先进的异常检测方法。工作量：</li><li>该方法需要比传统的分类器模型略多的时间，评估单个患者的美观大约需要15秒，而简单的分类器模型可以在1秒内产生结果。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-678c2254dd6a3d39889bef35f9067c05.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-cfa8a6039aebee57a2721ad761165bd3.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d6811aab9ac5a0e1edc535c928e3bd0f.jpg" align="middle"></details><h2 id="FineDiffusion-Scaling-up-Diffusion-Models-for-Fine-grained-Image-Generation-with-10-000-Classes"><a href="#FineDiffusion-Scaling-up-Diffusion-Models-for-Fine-grained-Image-Generation-with-10-000-Classes" class="headerlink" title="FineDiffusion: Scaling up Diffusion Models for Fine-grained Image   Generation with 10,000 Classes"></a>FineDiffusion: Scaling up Diffusion Models for Fine-grained Image   Generation with 10,000 Classes</h2><p><strong>Authors:Ziying Pan, Kun Wang, Gang Li, Feihong He, Xiwang Li, Yongxuan Lai</strong></p><p>The class-conditional image generation based on diffusion models is renowned for generating high-quality and diverse images. However, most prior efforts focus on generating images for general categories, e.g., 1000 classes in ImageNet-1k. A more challenging task, large-scale fine-grained image generation, remains the boundary to explore. In this work, we present a parameter-efficient strategy, called FineDiffusion, to fine-tune large pre-trained diffusion models scaling to large-scale fine-grained image generation with 10,000 categories. FineDiffusion significantly accelerates training and reduces storage overhead by only fine-tuning tiered class embedder, bias terms, and normalization layers’ parameters. To further improve the image generation quality of fine-grained categories, we propose a novel sampling method for fine-grained image generation, which utilizes superclass-conditioned guidance, specifically tailored for fine-grained categories, to replace the conventional classifier-free guidance sampling. Compared to full fine-tuning, FineDiffusion achieves a remarkable 1.56x training speed-up and requires storing merely 1.77% of the total model parameters, while achieving state-of-the-art FID of 9.776 on image generation of 10,000 classes. Extensive qualitative and quantitative experiments demonstrate the superiority of our method compared to other parameter-efficient fine-tuning methods. The code and more generated results are available at our project website: <a href="https://finediffusion.github.io/">https://finediffusion.github.io/</a>. </p><p><a href="http://arxiv.org/abs/2402.18331v1">PDF</a> </p><p><strong>Summary</strong><br>通过微调预训练扩散模型，以参数高效策略实现针对 10,000 个细粒度类别的大规模图像生成</p><p><strong>Key Takeaways</strong></p><ul><li>提出 FineDiffusion，将大规模扩散模型缩小到细粒度图像生成中</li><li>只微调分类嵌入、偏置项和归一化层的参数，大幅提升训练速度和存储效率</li><li>提出针对细粒度类别的超类条件引导采样方法，提升图像生成质量</li><li>与完全微调相比，FineDiffusion 训练速度提升 1.56 倍，所需存储参数仅为原模型的 1.77%</li><li>在 10,000 个类别的图像生成上取得最先进的 FID 为 9.776</li><li>大量定性和定量实验验证了该方法的优越性</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：FineDiffusion：将扩散模型扩展到 10,000 类别的细粒度图像生成</li><li>作者：Ziying Pan, Kun Wang, Gang Li, Feihong He, Xiwang Li, Yongxuan Lai</li><li>第一作者单位：厦门大学</li><li>关键词：Diffusion Models, Fine-grained Image Generation, Parameter-efficient Fine-tuning</li><li>论文链接：https://arxiv.org/abs/2402.18331   Github 代码链接：None</li><li>摘要：   （1）：研究背景：基于扩散模型的图像生成以产生高质量和多样化的图像而闻名。然而，大多数先前的努力都集中在为一般类别生成图像，例如 ImageNet-1k 中的 1000 个类别。一个更具挑战性的任务，即大规模细粒度图像生成，仍然是需要探索的边界。   （2）：过去的方法：过去的方法主要集中在一般类别的图像生成，而对于细粒度图像生成，需要模型对高度相似的细粒度类别中的细微差异（例如鸟类的羽毛纹理）进行复杂的建模。从头开始训练用于大规模细粒度图像生成的扩散模型需要更大的计算资源和训练迭代。   （3）：研究方法：本文提出了一种新的微调方法 FineDiffusion，它可以通过微调预训练模型的一小部分参数，有效地微调大型预训练图像生成扩散模型，以进行大规模细粒度图像生成。   （4）：方法性能：与完全微调相比，FineDiffusion 实现了显着的 1.56 倍训练加速，并且只需要存储 1.77% 的总模型参数，同时在 10,000 个类别的图像生成上实现了 9.776 的最先进 FID。广泛的定性和定量实验表明，与其他参数有效的微调方法相比，本文方法具有优越性。</li></ol><p>7.方法：(1)提出了一种新的微调方法FineDiffusion，该方法通过微调预训练模型的一小部分参数，有效地微调大型预训练图像生成扩散模型，以进行大规模细粒度图像生成；(2)提出了一种分层类标签编码策略，该策略同时对超类和子类标签进行编码；(3)同时微调偏差和归一化项以及分层嵌入器，以学习全局数据集的分布特征；(4)引入了一种分层无分类器引导采样方法，该方法利用超类条件信息来增强对生成图像的控制。</p><ol><li>结论：（1）：本文首次尝试将扩散模型扩展到 10,000 类的细粒度图像生成。我们引入了 FineDiffusion，这是一种高效的参数微调方法，可以微调预训练模型的关键组件，包括分层标签嵌入、偏差项和归一化项。我们的方法大幅减少了训练和存储开销。此外，我们引入了一种细粒度无分类器引导采样技术，利用分层数据标签信息来有效增强细粒度图像生成的性能。充分的定性和定量结果证明了我们方法与其他方法相比的优越性。（2）：创新点：提出了一种新的微调方法 FineDiffusion，该方法通过微调预训练模型的一小部分参数，有效地微调大型预训练图像生成扩散模型，以进行大规模细粒度图像生成；提出了分层类标签编码策略，该策略同时对超类和子类标签进行编码；同时微调偏差和归一化项以及分层嵌入器，以学习全局数据集的分布特征；引入了一种分层无分类器引导采样方法，该方法利用超类条件信息来增强对生成图像的控制。性能：与完全微调相比，FineDiffusion 实现了显着的 1.56 倍训练加速，并且只需要存储 1.77% 的总模型参数，同时在 10,000 个类别的图像生成上实现了 9.776 的最先进 FID。广泛的定性和定量实验表明，与其他参数有效的微调方法相比，本文方法具有优越性。工作量：与从头开始训练用于大规模细粒度图像生成的扩散模型相比，FineDiffusion 可以显着减少计算资源和训练迭代。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-f68a4db99ea4f9179538c6c4b4d7c7ce.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4e768fecf2a73ce9e4c8b13ef7c8cd6a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6c0d4b61db744892b76754513d9f6676.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-665dc312a2eacee1bb375efacd7d609c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d25afe2f19082c3abc80d90affd76466.jpg" align="middle"><img src="https://picx.zhimg.com/v2-68e2a9d895710b3df489a49501a85625.jpg" align="middle"></details><h2 id="Balancing-Act-Distribution-Guided-Debiasing-in-Diffusion-Models"><a href="#Balancing-Act-Distribution-Guided-Debiasing-in-Diffusion-Models" class="headerlink" title="Balancing Act: Distribution-Guided Debiasing in Diffusion Models"></a>Balancing Act: Distribution-Guided Debiasing in Diffusion Models</h2><p><strong>Authors:Rishubh Parihar, Abhijnya Bhat, Saswat Mallick, Abhipsa Basu, Jogendra Nath Kundu, R. Venkatesh Babu</strong></p><p>Diffusion Models (DMs) have emerged as powerful generative models with unprecedented image generation capability. These models are widely used for data augmentation and creative applications. However, DMs reflect the biases present in the training datasets. This is especially concerning in the context of faces, where the DM prefers one demographic subgroup vs others (eg. female vs male). In this work, we present a method for debiasing DMs without relying on additional data or model retraining. Specifically, we propose Distribution Guidance, which enforces the generated images to follow the prescribed attribute distribution. To realize this, we build on the key insight that the latent features of denoising UNet hold rich demographic semantics, and the same can be leveraged to guide debiased generation. We train Attribute Distribution Predictor (ADP) - a small mlp that maps the latent features to the distribution of attributes. ADP is trained with pseudo labels generated from existing attribute classifiers. The proposed Distribution Guidance with ADP enables us to do fair generation. Our method reduces bias across single/multiple attributes and outperforms the baseline by a significant margin for unconditional and text-conditional diffusion models. Further, we present a downstream task of training a fair attribute classifier by rebalancing the training set with our generated data. </p><p><a href="http://arxiv.org/abs/2402.18206v1">PDF</a> CVPR 2024. Project Page : <a href="https://ab-34.github.io/balancing_act/">https://ab-34.github.io/balancing_act/</a></p><p><strong>Summary</strong><br>去除扩散模型中的偏见，无需额外数据或模型重新训练。</p><p><strong>Key Takeaways</strong></p><ul><li>扩散模型（DM）存在偏见，表现为对特定人口亚组（如女性）的偏好。</li><li>分布引导是一种无偏 DM 的方法，无需额外数据或重新训练。</li><li>分布引导利用去噪 UNet 的潜在特征中丰富的语义信息。</li><li>属性分布预测器 (ADP) 将潜在特征映射到属性分布。</li><li>ADP 使用现有属性分类器生成的伪标签进行训练。</li><li>分布引导和 ADP 实现了公平生成，显著优于基线。</li><li>通过使用生成的数据重新平衡训练集，可以训练公平的属性分类器。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：平衡行为：扩散模型中的分布引导去偏</li><li>作者：Rishubh Parihar*, Abhijnya Bhat∗, Saswat Mallick, Abhipsa Basu, Jogendra Nath Kundu, R. Venkatesh Babu</li><li>隶属：印度科学院，班加罗尔</li><li>关键词：Diffusion Models, Debiasing, Distribution Guidance, Attribute Distribution Predictor, Fair Generation</li><li>链接：https://arxiv.org/abs/2402.18206</li><li><p>摘要：（1）研究背景：扩散模型（DM）作为强大的生成模型，在图像生成方面表现出色，但它们会反映训练数据集中的偏见，特别是对于人脸，DM 偏好某些人口统计学亚组（例如女性比男性）。（2）过去方法：现有去偏方法需要额外数据或模型重新训练。（3）研究方法：本文提出分布引导，通过强制生成图像遵循规定的属性分布来对 DM 进行去偏。通过训练属性分布预测器 (ADP) 来映射潜在特征到属性分布，ADP 使用现有属性分类器生成的伪标签进行训练。（4）方法性能：该方法在无条件和文本条件扩散模型上减少了单一/多属性的偏差，并且优于基线方法。此外，本文还提出了一种通过使用生成数据重新平衡训练集来训练公平属性分类器的下游任务。</p></li><li><p>方法：(1): 提出分布引导方法，通过强制生成图像遵循规定的属性分布来对扩散模型（DM）进行去偏。(2): 训练属性分布预测器（ADP）来映射潜在特征到属性分布，ADP使用现有属性分类器生成的伪标签进行训练。(3): 在无条件和文本条件扩散模型上评估该方法，减少了单一/多属性的偏差，并优于基线方法。(4): 提出了一种通过使用生成数据重新平衡训练集来训练公平属性分类器的下游任务。</p></li><li><p>结论：（1）本文的意义：本文提出了一种无需重新训练即可减轻预训练扩散模型偏差的方法，仅给定所需的参考属性分布。我们提出了一种新颖的方法，利用分布引导，联合引导一批图像遵循参考属性分布。所提出的方法是有效的，并且在（2）创新点：本文的创新点在于提出了一种新的分布引导方法，通过强制生成图像遵循规定的属性分布来对扩散模型进行去偏。性能：本文的方法在无条件和文本条件扩散模型上减少了单一/多属性的偏差，并且优于基线方法。工作量：本文的方法需要训练一个属性分布预测器，该预测器使用现有属性分类器生成的伪标签进行训练。训练属性分布预测器的工作量取决于训练数据的规模和属性的数量。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-05a1a956ee3a51fe0c06ffc4859c7231.jpg" align="middle"><img src="https://picx.zhimg.com/v2-16ae5c5f9f522148622d40f8f3f15f86.jpg" align="middle"><img src="https://picx.zhimg.com/v2-46f6a987113095ab338596820ca6e653.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f32e1f0036b8646f3ffad99a82575f09.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1128b65d6c33c58a2f6b04087adf31b0.jpg" align="middle"></details><h2 id="Coarse-to-Fine-Latent-Diffusion-for-Pose-Guided-Person-Image-Synthesis"><a href="#Coarse-to-Fine-Latent-Diffusion-for-Pose-Guided-Person-Image-Synthesis" class="headerlink" title="Coarse-to-Fine Latent Diffusion for Pose-Guided Person Image Synthesis"></a>Coarse-to-Fine Latent Diffusion for Pose-Guided Person Image Synthesis</h2><p><strong>Authors:Yanzuo Lu, Manlin Zhang, Andy J Ma, Xiaohua Xie, Jian-Huang Lai</strong></p><p>Diffusion model is a promising approach to image generation and has been employed for Pose-Guided Person Image Synthesis (PGPIS) with competitive performance. While existing methods simply align the person appearance to the target pose, they are prone to overfitting due to the lack of a high-level semantic understanding on the source person image. In this paper, we propose a novel Coarse-to-Fine Latent Diffusion (CFLD) method for PGPIS. In the absence of image-caption pairs and textual prompts, we develop a novel training paradigm purely based on images to control the generation process of the pre-trained text-to-image diffusion model. A perception-refined decoder is designed to progressively refine a set of learnable queries and extract semantic understanding of person images as a coarse-grained prompt. This allows for the decoupling of fine-grained appearance and pose information controls at different stages, and thus circumventing the potential overfitting problem. To generate more realistic texture details, a hybrid-granularity attention module is proposed to encode multi-scale fine-grained appearance features as bias terms to augment the coarse-grained prompt. Both quantitative and qualitative experimental results on the DeepFashion benchmark demonstrate the superiority of our method over the state of the arts for PGPIS. Code is available at <a href="https://github.com/YanzuoLu/CFLD">https://github.com/YanzuoLu/CFLD</a>. </p><p><a href="http://arxiv.org/abs/2402.18078v1">PDF</a> Accepted by CVPR 2024</p><p><strong>Summary</strong><br> 提出了一种粗到细的潜在扩散（CFLD）方法，利用图像而非文本提示，控制预训练文本到图像扩散模型，实现姿势引导的图像合成。</p><p><strong>Key Takeaways</strong></p><ul><li>提出 CFLD 方法，改善了 PGPIS 中姿势引导图像合成的效果。</li><li>使用纯图像训练范式，无需图像字幕或文本提示。</li><li>设计了一个感知精炼解码器，逐步优化查询并提取人物图像的语义理解。</li><li>将外貌和姿势信息控制解耦，避免过度拟合。</li><li>提出混合粒度注意力模块，对多尺度外观特征进行编码，增强粗粒度提示。</li><li>在 DeepFashion 数据集上，定量和定性实验结果证明了 CFLD 的优越性。</li><li>代码已开源。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：粗到细的潜在扩散用于姿态引导的人物图像合成</li><li>作者：Lu Yanzuo, Zhang Manlin, Ma Andy J, Xie Xiaohua, Lai Jianhuang</li><li>单位：中山大学计算机科学与工程学院</li><li>关键词：姿态引导、人物图像合成、潜在扩散模型、粗到细、语义理解</li><li>论文链接：https://arxiv.org/abs/2402.18078   Github 代码链接：https://github.com/YanzuoLu/CFLD</li><li>摘要：(1) 研究背景：姿态引导的人物图像合成旨在将源人物图像转换为特定的目标姿态，同时尽可能保留外观。它在电影制作、虚拟现实和时尚电子商务等领域有广泛的应用。(2) 过去的方法：现有基于生成对抗网络 (GAN) 的方法容易出现极小极大训练目标的不稳定性和难以在一次前向传递中生成高质量图像的问题。作为 GAN 在图像生成中的一种有前途的替代方案，扩散模型通过一系列去噪步骤逐渐合成更逼真的图像。(3) 本文方法：本文提出了一种新颖的粗到细潜在扩散 (CFLD) 方法用于姿态引导的人物图像合成。在没有图像-标题对和文本提示的情况下，我们开发了一种纯粹基于图像的新颖训练范式来控制预训练文本到图像扩散模型的生成过程。我们设计了一个感知精炼解码器来渐进地细化一组可学习查询并提取人物图像的语义理解作为粗粒度提示。这允许在不同的阶段解耦细粒度外观和姿态信息控制，从而规避了潜在的过拟合问题。为了生成更逼真的纹理细节，我们提出了一种混合粒度注意力模块，将多尺度细粒度外观特征编码为偏差项以增强粗粒度提示。(4) 性能：在 DeepFashion 基准上的定量和定性实验结果证明了我们方法在姿态引导的人物图像合成任务上的优越性。这些性能支持了他们的目标，即生成具有更好泛化性能的高质量图像。</li></ol><p>7.Methods：(1) 提出粗到细潜在扩散（CFLD）方法，用于姿态引导的人物图像合成；(2) 开发基于图像的新训练范式，控制预训练文本到图像扩散模型的生成过程；(3) 设计感知精炼解码器，渐进细化可学习查询，提取人物图像语义理解作为粗粒度提示；(4) 提出混合粒度注意力模块，将多尺度细粒度外观特征编码为偏差项，增强粗粒度提示；(5) 通过在DeepFashion基准上的定量和定性实验，验证了CFLD方法的优越性。</p><ol><li><p>结论：（1）xxx；（2）创新点：xxx；性能：xxx；工作量：xxx；</p></li><li><p>总结：（1）本工作的重要意义是什么？（2）从创新点、性能、工作量三个维度总结本文的优缺点：创新点：本文提出了一种新颖的粗到细潜在扩散（CFLD）方法，用于姿态引导的人物图像合成。该方法通过渐进细化可学习查询，提取人物图像的语义理解作为粗粒度提示，并提出混合粒度注意力模块，将多尺度细粒度外观特征编码为偏差项，增强粗粒度提示。性能：在 DeepFashion 基准上的定量和定性实验结果证明了 CFLD 方法在姿态引导的人物图像合成任务上的优越性。这些性能支持了他们的目标，即生成具有更好泛化性能的高质量图像。工作量：本文的工作量适中。该方法的实现需要对文本到图像扩散模型进行预训练，这可能需要大量的计算资源。此外，该方法需要设计感知精炼解码器和混合粒度注意力模块，这需要额外的开发和实验工作。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-ee807dc5573280abe63e138fa82f6eb3.jpg" align="middle"><img src="https://pica.zhimg.com/v2-07506917791ee3066c02770faa1a2052.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5192aaa635e4ab29d557ee967971be49.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-269e1bea1b870d8f0466ace81c9d2e01.jpg" align="middle"></details><h2 id="SynArtifact-Classifying-and-Alleviating-Artifacts-in-Synthetic-Images-via-Vision-Language-Model"><a href="#SynArtifact-Classifying-and-Alleviating-Artifacts-in-Synthetic-Images-via-Vision-Language-Model" class="headerlink" title="SynArtifact: Classifying and Alleviating Artifacts in Synthetic Images   via Vision-Language Model"></a>SynArtifact: Classifying and Alleviating Artifacts in Synthetic Images   via Vision-Language Model</h2><p><strong>Authors:Bin Cao, Jianhao Yuan, Yexin Liu, Jian Li, Shuyang Sun, Jing Liu, Bo Zhao</strong></p><p>In the rapidly evolving area of image synthesis, a serious challenge is the presence of complex artifacts that compromise perceptual realism of synthetic images. To alleviate artifacts and improve quality of synthetic images, we fine-tune Vision-Language Model (VLM) as artifact classifier to automatically identify and classify a wide range of artifacts and provide supervision for further optimizing generative models. Specifically, we develop a comprehensive artifact taxonomy and construct a dataset of synthetic images with artifact annotations for fine-tuning VLM, named SynArtifact-1K. The fine-tuned VLM exhibits superior ability of identifying artifacts and outperforms the baseline by 25.66%. To our knowledge, this is the first time such end-to-end artifact classification task and solution have been proposed. Finally, we leverage the output of VLM as feedback to refine the generative model for alleviating artifacts. Visualization results and user study demonstrate that the quality of images synthesized by the refined diffusion model has been obviously improved. </p><p><a href="http://arxiv.org/abs/2402.18068v1">PDF</a> </p><p><strong>Summary</strong><br>利用预训练的视觉语言模型对图像合成中的伪影进行自动分类，为生成模型的进一步优化提供监管，从而提高合成图像的质量。</p><p><strong>Key Takeaways</strong></p><ul><li>合成图像中复杂伪影的存在构成了一项重大挑战，对感知真实性产生了负面影响。</li><li>研究人员提出将视觉语言模型（VLM）微调为伪影分类器，以便自动识别和分类各种伪影。</li><li>开发了一个全面的伪影分类体系，并构建了一个具有伪影注释的合成图像数据集（SynArtifact-1K）。</li><li>微调后的 VLM 在识别伪影方面表现出优异的能力，比基线高出 25.66%。</li><li>这是首次提出此类端到端伪影分类任务和解决方案。</li><li>利用 VLM 的输出作为反馈来优化生成模型，以减轻伪影。</li><li>视觉化结果和用户研究表明，优化后的扩散模型合成的图像质量得到了明显改善。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文题目：SynArtifact：通过视觉语言模型对合成图像中的伪影进行分类和消除</li><li>作者：Bin Cao, Jianhao Yuan, Yexin Liu, Jian Li, Shuyang Sun, Jing Liu, Bo Zhao</li><li>第一作者单位：中国科学院自动化研究所</li><li>关键词：合成图像、伪影、视觉语言模型、生成模型</li><li>论文链接：https://arxiv.org/abs/2402.18068</li><li><p>摘要：(1) 研究背景：合成图像中存在复杂伪影，影响图像的感知真实性。(2) 过往方法：现有方法主要依赖单一评分指标优化生成模型，无法有效反映伪影的多样性和复杂性。(3) 本文方法：提出一个综合伪影分类法，构建了一个带有伪影注释的合成图像数据集 SynArtifact-1K，并微调视觉语言模型 (VLM) 对伪影进行分类。利用 VLM 的输出作为 AI 反馈来改进生成模型，以减轻伪影。(4) 实验结果：微调后的 VLM 在伪影分类任务上比基线方法提高了 25.66% 的准确率和 29.01% 的 F1 分数。通过利用伪影分类器的输出作为 AI 反馈，可以有效减轻生成模型中的伪影。</p></li><li><p>方法：（1）构建综合伪影分类法，建立包含伪影注释的合成图像数据集 SynArtifact-1K；（2）微调视觉语言模型 VLM，将其作为伪影分类器；（3）利用 VLM 输出作为 AI 反馈，计算生成模型输出与每种伪影之间的 BertScore，作为伪影分类奖励；（4）通过最大化伪影分类奖励，优化扩散模型，以减轻伪影。</p></li><li><p>结论：（1）：本文针对合成图像中的伪影问题，提出了一个综合的伪影分类法，构建了包含伪影注释的合成图像数据集 SynArtifact-1K，并利用视觉语言模型对伪影进行分类，有效地减轻了生成模型中的伪影，提升了合成图像的感知真实性。（2）：创新点：</p></li><li>构建了包含 13 种常见伪影的综合伪影分类法。</li><li>创建了首个带有伪影类别、描述和坐标注释的图像数据集 SynArtifact-1K。</li><li>微调视觉语言模型用于自动分类伪影，并利用其输出作为 AI 反馈来优化生成模型。性能：</li><li>微调后的视觉语言模型在伪影分类任务上比基线方法提高了 25.66% 的准确率和 29.01% 的 F1 分数。</li><li>利用伪影分类器的输出作为 AI 反馈，可以有效减轻生成模型中的伪影。工作量：</li><li>构建了包含 1000 张合成图像的 SynArtifact-1K 数据集。</li><li>微调了视觉语言模型用于伪影分类。</li><li>通过最大化伪影分类奖励，优化了扩散模型以减轻伪影。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-887bb2eb3bab7f102340a00fb115308a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a67234ceff494848cb67aa7bc7345a5e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c0c890345f83368ccd384b81c55c4b11.jpg" align="middle"><img src="https://pica.zhimg.com/v2-48d8c1e1b56b76bfccfccfcb96c1d5a4.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1a5599c3d37db39e68fa5fb2e0139cec.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-94675e3c8e66717ee97bc9e3472ed274.jpg" align="middle"></details><h2 id="Box-It-to-Bind-It-Unified-Layout-Control-and-Attribute-Binding-in-T2I-Diffusion-Models"><a href="#Box-It-to-Bind-It-Unified-Layout-Control-and-Attribute-Binding-in-T2I-Diffusion-Models" class="headerlink" title="Box It to Bind It: Unified Layout Control and Attribute Binding in T2I   Diffusion Models"></a>Box It to Bind It: Unified Layout Control and Attribute Binding in T2I   Diffusion Models</h2><p><strong>Authors:Ashkan Taghipour, Morteza Ghahremani, Mohammed Bennamoun, Aref Miri Rekavandi, Hamid Laga, Farid Boussaid</strong></p><p>While latent diffusion models (LDMs) excel at creating imaginative images, they often lack precision in semantic fidelity and spatial control over where objects are generated. To address these deficiencies, we introduce the Box-it-to-Bind-it (B2B) module - a novel, training-free approach for improving spatial control and semantic accuracy in text-to-image (T2I) diffusion models. B2B targets three key challenges in T2I: catastrophic neglect, attribute binding, and layout guidance. The process encompasses two main steps: i) Object generation, which adjusts the latent encoding to guarantee object generation and directs it within specified bounding boxes, and ii) attribute binding, guaranteeing that generated objects adhere to their specified attributes in the prompt. B2B is designed as a compatible plug-and-play module for existing T2I models, markedly enhancing model performance in addressing the key challenges. We evaluate our technique using the established CompBench and TIFA score benchmarks, demonstrating significant performance improvements compared to existing methods. The source code will be made publicly available at <a href="https://github.com/nextaistudio/BoxIt2BindIt">https://github.com/nextaistudio/BoxIt2BindIt</a>. </p><p><a href="http://arxiv.org/abs/2402.17910v1">PDF</a> </p><p><strong>Summary</strong><br>Box-it-to-Bind-it（B2B）是一种无需训练的新模块，可提高文本到图像（T2I）扩散模型中图像的生成质量、语义准确度和空间控制能力。</p><p><strong>Key Takeaways</strong></p><ul><li>B2B 模块可改善 T2I 中的三个关键挑战：灾难性遗漏、属性绑定和布局指导。</li><li>B2B 包括生成对象和属性绑定的两个主要步骤。</li><li>B2B 可作为现有的 T2I 模型的即插即用模块，无需训练。</li><li>B2B 在 CompBench 和 TIFA 评分基准上表现出显著的性能提升。</li><li>B2B 的源代码将在 <a href="https://github.com/nextaistudio/BoxIt2BindIt">https://github.com/nextaistudio/BoxIt2BindIt</a> 上公开。</li><li>B2B 提高了 LDM 在生成图像时的空间控制和语义准确性。</li><li>B2B 适用于不同的 T2I 模型，易于集成。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：Box-it-to-Bind-it：统一布局控制和属性绑定到 T2I 扩散模型中</li><li>作者：Ashkan Taghipour, Morteza Ghahremani, Mohammed Bennamoun, Aref Miri Rekavandi, Hamid Laga, Farid Boussaid</li><li>第一作者单位：西澳大利亚大学</li><li>关键词：文本到图像、扩散模型、空间控制、属性绑定、布局引导</li><li>论文链接：https://arxiv.org/abs/2402.17910</li><li>摘要：（1）研究背景：现有扩散模型在生成图像时缺乏语义保真度和空间控制，难以忠实地遵循给定的提示，尤其是在对象属性和对象放置方面。</li></ol><p>（2）过去的方法和问题：现有方法要么从头开始训练模型，要么微调现有模型，但需要大量计算资源和时间。此外，利用预训练模型并集成特征的方法虽然不需要大量训练，但效果有限。</p><p>（3）提出的研究方法：本文提出了一种免训练的方法 Box-it-to-Bind-it (B2B)，解决文本到图像生成中的三个关键挑战：灾难性遗漏、属性绑定和布局引导。B2B 在推理阶段通过两步引导扩散模型的潜在编码：对象生成和属性绑定。</p><p>（4）方法在任务和性能上的表现：在 CompBench 和 TIFA 得分基准上，与现有方法相比，B2B 在解决关键挑战方面显着提高了模型性能。这些性能提升支持了本文的目标，即提高文本到图像生成中的空间控制和语义准确性。</p><p>方法：(1) B2B是一种奖励引导扩散模型，它在推理阶段通过两步引导扩散模型的潜在编码：对象生成和属性绑定。(2) 对象生成：基于IoU，增加对象生成概率，将注意力权重集中在给定边界框内，同时抑制边界框外的注意力权重。(3) 属性绑定：使用KL散度测量属性概率分布与对应对象概率分布的差异，减少差异，将属性分布强制收敛到各自的对象。</p><ol><li>结论：（1）：本研究针对文本到图像生成中的关键挑战，特别是属性绑定和空间控制，提出了 B2B 模型。B2B 采用生成和绑定双模块系统，有效解决了灾难性遗漏、提高属性绑定精度和确保准确对象放置的问题。它作为现有 T2I 框架的即插即用模块的兼容性通过其在 CompBench 和 TIFA 基准中的出色表现得到证明，标志着生成建模的重大飞跃。B2B 的突破凸显了其作为未来研究潜在标准的作用，为数字成像和生成式 AI 的创新发展铺平了道路。（2）：创新点：</li><li>提出了一种免训练的方法 B2B，通过两步引导扩散模型的潜在编码来解决文本到图像生成中的关键挑战。</li><li>设计了对象生成和属性绑定两个模块，有效解决了灾难性遗漏、属性绑定和布局引导问题。</li><li>B2B 作为现有 T2I 框架的即插即用模块，易于集成和使用。性能：</li><li>在 CompBench 和 TIFA 基准上，与现有方法相比，B2B 在解决关键挑战方面显着提高了模型性能。</li><li>消融研究验证了对象生成和属性绑定奖励组件的有效性，表明 B2B 的各个组件对整体性能至关重要。工作量：</li><li>B2B 是一种免训练的方法，不需要从头开始训练模型或微调现有模型，从而节省了大量的计算资源和时间。</li><li>B2B 易于集成到现有 T2I 框架中，无需进行复杂的修改或重新训练，降低了工作量。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-9044558cdc31309b419fea5199aa8a89.jpg" align="middle"><img src="https://picx.zhimg.com/v2-78bccd36910d4aa870962c445823ad57.jpg" align="middle"><img src="https://pica.zhimg.com/v2-967a215bde68183f03e457a7ff3f8e9a.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e2a4cdc833464a14406a357aa9e0c358.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d140c3c8e05d724098a1c03138203a01.jpg" align="middle"></details><h2 id="Structure-Guided-Adversarial-Training-of-Diffusion-Models"><a href="#Structure-Guided-Adversarial-Training-of-Diffusion-Models" class="headerlink" title="Structure-Guided Adversarial Training of Diffusion Models"></a>Structure-Guided Adversarial Training of Diffusion Models</h2><p><strong>Authors:Ling Yang, Haotian Qian, Zhilong Zhang, Jingwei Liu, Bin Cui</strong></p><p>Diffusion models have demonstrated exceptional efficacy in various generative applications. While existing models focus on minimizing a weighted sum of denoising score matching losses for data distribution modeling, their training primarily emphasizes instance-level optimization, overlooking valuable structural information within each mini-batch, indicative of pair-wise relationships among samples. To address this limitation, we introduce Structure-guided Adversarial training of Diffusion Models (SADM). In this pioneering approach, we compel the model to learn manifold structures between samples in each training batch. To ensure the model captures authentic manifold structures in the data distribution, we advocate adversarial training of the diffusion generator against a novel structure discriminator in a minimax game, distinguishing real manifold structures from the generated ones. SADM substantially improves existing diffusion transformers (DiT) and outperforms existing methods in image generation and cross-domain fine-tuning tasks across 12 datasets, establishing a new state-of-the-art FID of 1.58 and 2.11 on ImageNet for class-conditional image generation at resolutions of 256x256 and 512x512, respectively. </p><p><a href="http://arxiv.org/abs/2402.17563v1">PDF</a> Accepted by CVPR 2024</p><p><strong>Summary</strong><br>扩散模型通过结构对抗训练，学习批内样本流形结构，提升图像生成和跨域微调任务的性能。</p><p><strong>Key Takeaways</strong></p><ul><li>现有扩散模型专注于单个样本的去噪得分匹配损失优化，忽视批内样本之间的成对关系。</li><li>结构对抗训练 (SADM) 引入结构鉴别器来区分真实和生成的流形结构。</li><li>SADM 迫使模型学习训练批次中样本之间的流形结构。</li><li>SADM 与扩散变压器 (DiT) 相结合，在图像生成和跨域微调任务上优于现有方法。</li><li>SADM 在 12 个数据集上实现了图像生成和跨域微调任务的最新 FID 分别为 1.58 和 2.11。</li><li>SADM 在 256x256 和 512x512 分辨率下，在 ImageNet 上实现了类条件图像生成的最新 FID 分别为 1.58 和 2.11。</li><li>SADM 证明了流形结构学习对于扩散模型在生成任务中的重要性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：结构引导扩散模型对抗性训练</li><li>作者：杨凌、钱浩天、张智龙、刘景伟、崔斌</li><li>第一作者单位：北京大学</li><li>关键词：扩散模型、结构引导、对抗训练</li><li>论文链接：https://arxiv.org/abs/2402.17563   Github 链接：无</li><li>摘要：（1）研究背景：扩散模型在生成任务中表现出色，但现有方法主要关注最小化去噪得分匹配损失的加权和，训练过程侧重于实例级优化，忽略了小批量样本之间的宝贵结构信息。</li></ol><p>（2）过去的方法及其问题：现有方法主要集中在实例级优化，忽略了小批量样本之间的结构信息，导致无法充分建模数据分布。</p><p>（3）提出的研究方法：提出结构引导扩散模型对抗性训练（SADM），通过对抗训练指导扩散生成器学习小批量样本之间的流形结构。引入结构判别器来区分真实流形结构和生成流形结构，确保模型捕获数据分布中的真实流形结构。</p><p>（4）方法在任务和性能上的表现：SADM 显著提升了现有扩散 Transformer，在 12 个数据集上的图像生成和跨域微调任务中优于现有方法，在 ImageNet 上分别以 256×256 和 512×512 的分辨率实现了 1.58 和 2.11 的新 SOTA FID，验证了方法的有效性。</p><p>7.Methods：（1）提出结构引导扩散模型对抗性训练（SADM），通过对抗训练指导扩散生成器学习小批量样本之间的流形结构；（2）引入结构判别器来区分真实流形结构和生成流形结构，确保模型捕获数据分布中的真实流形结构；（3）采用Wasserstein GAN损失函数，指导生成器生成与真实流形结构相似的样本；（4）在训练过程中交替更新生成器和判别器，直至达到纳什均衡；（5）将SADM与扩散Transformer相结合，形成更强大的图像生成模型。</p><ol><li>总结(1): 本文提出了从结构角度优化扩散模型的结构引导对抗性训练方法，该训练算法可以轻松推广到图像和潜在扩散模型，并通过理论推导和实验结果一致地改进了现有的扩散模型。我们在 12 个图像数据集上的图像生成和跨域微调任务中取得了新的 SOTA 性能。对于未来的工作，我们将把我们的方法扩展到更具挑战性的基于扩散的应用程序（例如，文本到图像/视频生成）。(2): 创新点: 提出结构引导对抗性训练方法，通过对抗训练指导扩散生成器学习小批量样本之间的流形结构，从而提升扩散模型的生成质量。性能: 在 12 个图像数据集上的图像生成和跨域微调任务中取得了新的 SOTA 性能，在 ImageNet 上分别以 256×256 和 512×512 的分辨率实现了 1.58 和 2.11 的新 SOTAFID。工作量: 该方法易于实现，可以轻松推广到图像和潜在扩散模型，工作量较小。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-11a45496d9d4169c7ee0bbb4a6534ffa.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b4ae1e4da806d223271756f678f15ce9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-02b820484fca35ffef9bc52706101c79.jpg" align="middle"><img src="https://picx.zhimg.com/v2-14ed9373ba8bdaf3ecaca75391245256.jpg" align="middle"><img src="https://pica.zhimg.com/v2-75ca2aa69507bb15984388d3520039af.jpg" align="middle"></details><h2 id="Diffusion-Model-Based-Image-Editing-A-Survey"><a href="#Diffusion-Model-Based-Image-Editing-A-Survey" class="headerlink" title="Diffusion Model-Based Image Editing: A Survey"></a>Diffusion Model-Based Image Editing: A Survey</h2><p><strong>Authors:Yi Huang, Jiancheng Huang, Yifan Liu, Mingfu Yan, Jiaxi Lv, Jianzhuang Liu, Wei Xiong, He Zhang, Shifeng Chen, Liangliang Cao</strong></p><p>Denoising diffusion models have emerged as a powerful tool for various image generation and editing tasks, facilitating the synthesis of visual content in an unconditional or input-conditional manner. The core idea behind them is learning to reverse the process of gradually adding noise to images, allowing them to generate high-quality samples from a complex distribution. In this survey, we provide an exhaustive overview of existing methods using diffusion models for image editing, covering both theoretical and practical aspects in the field. We delve into a thorough analysis and categorization of these works from multiple perspectives, including learning strategies, user-input conditions, and the array of specific editing tasks that can be accomplished. In addition, we pay special attention to image inpainting and outpainting, and explore both earlier traditional context-driven and current multimodal conditional methods, offering a comprehensive analysis of their methodologies. To further evaluate the performance of text-guided image editing algorithms, we propose a systematic benchmark, EditEval, featuring an innovative metric, LMM Score. Finally, we address current limitations and envision some potential directions for future research. The accompanying repository is released at <a href="https://github.com/SiatMMLab/Awesome-Diffusion-Model-Based-Image-Editing-Methods">https://github.com/SiatMMLab/Awesome-Diffusion-Model-Based-Image-Editing-Methods</a>. </p><p><a href="http://arxiv.org/abs/2402.17525v1">PDF</a> </p><p><strong>Summary</strong><br>扩散模型在图像生成和编辑任务中应用广泛，可从复杂分布中生成高质量样本，且支持无条件和输入条件下的图像编辑。</p><p><strong>Key Takeaways</strong></p><ul><li>扩散模型通过学习逆转图像加噪过程，生成高质量样本。</li><li>扩散模型图像编辑方法可分为不同学习策略、用户输入条件和编辑任务。</li><li>图像修复和外延可使用传统上下文驱动方法或多模态条件方法。</li><li>提出 EditEval 基准和 LMM 评分用于评估文本指导图像编辑算法。</li><li>目前存在限制，未来研究方向包括多模态、3D 和编辑元数据。</li><li>可在 <a href="https://github.com/SiatMMLab/Awesome-Diffusion-Model-Based-Image-Editing-Methods">https://github.com/SiatMMLab/Awesome-Diffusion-Model-Based-Image-Editing-Methods</a> 获取相关代码。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：基于扩散模型的图像编辑：综述</li><li>作者：Yi Huang、Jiancheng Huang、Yifan Liu、Mingfu Yan、Jiaxi Lv、Jianzhuang Liu、Wei Xiong、He Zhang、Shifeng Chen、Liangliang Cao</li><li>单位：深圳先进技术研究院</li><li>关键词：Diffusion Model、Image Editing、AIGC</li><li>链接：https://arxiv.org/abs/2402.17525Github：无</li><li>摘要：(1)：随着人工智能（AI）技术的发展，AI 生成的内容（AIGC）领域蓬勃发展，图像编辑作为其中一项重要任务，在数字媒体、广告和科学研究等领域有着广泛的应用。(2)：基于扩散模型的图像编辑方法近年来取得了显著进展，该方法通过学习逐步给图像添加噪声并逆转这一过程，可以从复杂分布中生成高质量的样本。(3)：本文对基于扩散模型的图像编辑方法进行了全面的综述，从学习策略、用户输入条件和具体编辑任务等多个角度对现有工作进行了深入分析和分类。(4)：基于扩散模型的图像编辑方法在图像修复、图像外延等任务上取得了很好的效果，本文还提出了一个系统性的基准 EditEval 和一个创新的指标 LMMScore 来进一步评估文本引导图像编辑算法的性能。</li></ol><p>7.Methods:(1): 基于CLIP指导的图像编辑：DiffusionCLIP、Asyrp、EffDiff、DiffStyler、StyleDiffusion、UNIT-DDPM、CycleNet、DiffusionAutoencoders、HDAE、EGSDE、Pixel-GuidedDiffusion；(2): 基于参考和属性指导的图像编辑：PbE、RIC、ObjectStitch、PhD、DreamInpainter、Anydoor、FADING、PAIRDiffusion、SmartBrush、IIR-Net；(3): 基于指令指导的图像编辑：InstructPix2Pix、MoEController、FoI、LOFIE、InstructDiffusion、EmuEdit、DialogPaint、Inst-Inpaint、HIVE、ImageBrush、InstructAny2Pix、MGIE、SmartEdit。</p><ol><li>结论：（1）本工作对基于扩散模型的图像编辑方法进行了全面的综述，从多个角度对现有工作进行了深入分析和分类，并提出了一个系统性的基准 EditEval 和一个创新的指标 LMMScore 来进一步评估文本引导图像编辑算法的性能。（2）创新点：</li><li>提出了一种新的图像编辑基准 EditEval 和一个创新的指标 LMMScore，用于评估文本引导图像编辑算法的性能。</li><li>对基于扩散模型的图像编辑方法进行了全面的综述和分类，从学习策略、用户输入条件和具体编辑任务等多个角度对现有工作进行了深入分析。</li><li>探索了这些方法在增强编辑性能方面的贡献。</li><li>在我们的图像编辑基准 EditEval 中对 7 项任务进行了评估，以及最新最先进的方法。</li><li>总结了图像编辑领域的广泛潜力，并提出了未来研究的方向。</li><li>性能：在 EditEval 基准上，基于扩散模型的图像编辑方法在图像修复、图像外延等任务上取得了很好的效果。</li><li>工作量：本文对超过 100 种基于扩散模型的图像编辑方法进行了综述和分类，并对 7 项任务进行了评估，工作量较大。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-4c52565ddb49dad37f10475b00a6abbc.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4537d5996d9b29f71e82d00a227227b7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-db76ba27193f9ab6b62bab161a239510.jpg" align="middle"></details><h2 id="Enhancing-Hyperspectral-Images-via-Diffusion-Model-and-Group-Autoencoder-Super-resolution-Network"><a href="#Enhancing-Hyperspectral-Images-via-Diffusion-Model-and-Group-Autoencoder-Super-resolution-Network" class="headerlink" title="Enhancing Hyperspectral Images via Diffusion Model and Group-Autoencoder   Super-resolution Network"></a>Enhancing Hyperspectral Images via Diffusion Model and Group-Autoencoder   Super-resolution Network</h2><p><strong>Authors:Zhaoyang Wang, Dongyang Li, Mingyang Zhang, Hao Luo, Maoguo Gong</strong></p><p>Existing hyperspectral image (HSI) super-resolution (SR) methods struggle to effectively capture the complex spectral-spatial relationships and low-level details, while diffusion models represent a promising generative model known for their exceptional performance in modeling complex relations and learning high and low-level visual features. The direct application of diffusion models to HSI SR is hampered by challenges such as difficulties in model convergence and protracted inference time. In this work, we introduce a novel Group-Autoencoder (GAE) framework that synergistically combines with the diffusion model to construct a highly effective HSI SR model (DMGASR). Our proposed GAE framework encodes high-dimensional HSI data into low-dimensional latent space where the diffusion model works, thereby alleviating the difficulty of training the diffusion model while maintaining band correlation and considerably reducing inference time. Experimental results on both natural and remote sensing hyperspectral datasets demonstrate that the proposed method is superior to other state-of-the-art methods both visually and metrically. </p><p><a href="http://arxiv.org/abs/2402.17285v1">PDF</a> Accepted by AAAI2024</p><p><strong>Summary</strong><br>扩散模型与群组自编码器相结合的创新框架，有效提升高光谱图像超分辨率，显著改善谱空关系建模和低层细节恢复。</p><p><strong>Key Takeaways</strong></p><ul><li>扩散模型擅长建模复杂关系和学习视觉特征，在高光谱图像超分辨率中潜力巨大。</li><li>训练扩散模型面临收敛困难和推理时间长挑战。</li><li>群组自编码器框架通过将高维高光谱数据编码到低维潜在空间，缓解了扩散模型训练难度，并保持了波段相关性。</li><li>扩散模型与群组自编码器相结合，有效解决了推理时间问题。</li><li>在自然和遥感高光谱数据集上，该方法在视觉和度量上均优于现有技术。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：基于扩散模型和组自编码器的超分辨率高光谱图像增强</li><li>作者：王兆阳，李东阳，张明阳，罗浩，巩茂国</li><li>隶属单位：西安电子科技大学协同智能系统教育部重点实验室</li><li>关键词：高光谱图像，超分辨率，扩散模型，组自编码器</li><li>论文链接：https://arxiv.org/abs/2402.17285</li><li><p>摘要：（1）研究背景：现有高光谱图像超分辨率方法难以有效捕捉复杂的光谱-空间关系和低级细节，而扩散模型是一种有前途的生成模型，以其在建模复杂关系和学习高低级视觉特征方面的出色性能而闻名。（2）过去方法及问题：将扩散模型直接应用于高光谱图像超分辨率面临着模型收敛困难和推理时间长的挑战。（3）研究方法：提出了一种新的组自编码器（GAE）框架，该框架与扩散模型协同结合，构建了一个高效的高光谱图像超分辨率模型（DMGASR）。提出的 GAE 框架将高维高光谱数据编码为低维潜在空间，扩散模型在此空间中工作，从而缓解了训练扩散模型的难度，同时保持了波段相关性并大大减少了推理时间。（4）任务和性能：在自然和遥感高光谱数据集上的实验结果表明，所提出的方法在视觉和度量上都优于其他最先进的方法。</p></li><li><p>方法：(1): 提出了一种基于扩散模型和组自编码器的超分辨率高光谱图像增强模型（DMGASR）；(2): 该模型采用两阶段训练方式，包括自动编码器和扩散超分辨率模型；(3): 采用谱分组策略和非对称解码器设计，有效地将高维高光谱数据编码为低维潜在空间；(4): 训练扩散模型在潜在空间中工作，缓解了训练扩散模型的难度，同时保持了波段相关性并大大减少了推理时间；(5): 训练自动编码器重构输入数据，生成一系列隐藏变量；(6): 将低分辨率隐藏变量作为条件信息，与高分辨率隐藏变量串联，在去噪过程中加入到扩散模型中；(7): 采用 U-Net 作为去噪模型，迭代去除噪声，生成超分辨率潜在变量列表；(8): 将超分辨率潜在变量列表传递给解码器，生成超分辨率图像。</p></li><li><p>结论：（1）：本文提出了一种基于扩散模型和组自编码器的高光谱图像超分辨率增强模型（DMGASR），该模型将扩散模型与自动编码器相结合，有效解决了扩散模型在高维数据上收敛困难的问题，并通过在低维潜在空间中训练扩散模型，大大减少了推理时间。该方法在自然和遥感高光谱数据集上均取得了优异的性能，在视觉和度量上均优于其他最先进的方法。（2）：创新点：</p></li><li>提出了一种基于扩散模型和组自编码器的超分辨率高光谱图像增强模型（DMGASR）。</li><li>采用两阶段训练方式，包括自动编码器和扩散超分辨率模型。</li><li>采用谱分组策略和非对称解码器设计，有效地将高维高光谱数据编码为低维潜在空间。</li><li>训练扩散模型在潜在空间中工作，缓解了训练扩散模型的难度，同时保持了波段相关性并大大减少了推理时间。性能：</li><li>在自然和遥感高光谱数据集上均取得了优异的性能。</li><li>在视觉和度量上均优于其他最先进的方法。工作量：</li><li>算法设计和实现。</li><li>数据集的收集和预处理。</li><li>实验的进行和结果分析。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-1b637edd1829307f3889177173204f7c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-cc3237f0ece24500c44086801ebc1feb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a3e331ea518a2b9c151178e17f115708.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7b211209593777f9420f6bb845daa71b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f24696c9c22f22b6e487ce2e6fc31ec7.jpg" align="middle"></details><h2 id="One-Shot-Structure-Aware-Stylized-Image-Synthesis"><a href="#One-Shot-Structure-Aware-Stylized-Image-Synthesis" class="headerlink" title="One-Shot Structure-Aware Stylized Image Synthesis"></a>One-Shot Structure-Aware Stylized Image Synthesis</h2><p><strong>Authors:Hansam Cho, Jonghyun Lee, Seunggyu Chang, Yonghyun Jeong</strong></p><p>While GAN-based models have been successful in image stylization tasks, they often struggle with structure preservation while stylizing a wide range of input images. Recently, diffusion models have been adopted for image stylization but still lack the capability to maintain the original quality of input images. Building on this, we propose OSASIS: a novel one-shot stylization method that is robust in structure preservation. We show that OSASIS is able to effectively disentangle the semantics from the structure of an image, allowing it to control the level of content and style implemented to a given input. We apply OSASIS to various experimental settings, including stylization with out-of-domain reference images and stylization with text-driven manipulation. Results show that OSASIS outperforms other stylization methods, especially for input images that were rarely encountered during training, providing a promising solution to stylization via diffusion models. </p><p><a href="http://arxiv.org/abs/2402.17275v1">PDF</a> CVPR 2024</p><p><strong>Summary</strong><br>基于扩散模型的 OSASIS 实现了图像风格化，同时保持了结构完整性，即使是对训练中很少遇到的输入图像。</p><p><strong>Key Takeaways</strong></p><ul><li>OSASIS 采用扩散模型进行图像风格化，解决了 GAN 模型在保持结构方面的不足。</li><li>OSASIS 能够有效分离图像语义和结构，可控地调整给定输入的内容和风格级别。</li><li>OSASIS 在各种实验设置中表现出色，包括使用域外参考图像进行风格化和使用文本驱动的操作进行风格化。</li><li>与其他风格化方法相比，OSASIS 在训练中很少遇到的输入图像上表现得尤为出色，为通过扩散模型进行风格化提供了有前景的解决方案。</li><li>OSASIS 采用了渐进式训练策略，通过从添加噪声到恢复图像，逐步将风格应用于输入。</li><li>OSASIS 使用预训练的扩散模型，提高了效率和泛化性。</li><li>OSASIS 在图像风格化领域展现出了广泛的应用前景，包括图像编辑、艺术创作和视频处理。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：单次结构感知风格化图像合成</li><li>作者：Jongmin Lee*, Jaeyeon Kang, Sangwoo Mo, Seongwon Lee†, Kyoung Mu Lee†</li><li>隶属单位：NAVER Cloud</li><li>关键词：图像风格化、扩散模型、结构保持</li><li>论文链接：https://arxiv.org/abs/2302.05447, Github 代码链接：无</li><li>摘要：(1) 研究背景：GAN 模型在图像风格化任务中取得成功，但难以在风格化各种输入图像时保持结构。最近，扩散模型被用于图像风格化，但仍缺乏保持输入图像原始质量的能力。(2) 过去方法及问题：过去方法包括基于 GAN 的模型和基于扩散模型的方法。GAN 模型难以保持结构，而基于扩散模型的方法缺乏控制内容和风格的能力。(3) 本文提出的研究方法：本文提出了一种新的单次风格化方法 OSASIS，该方法在结构保持方面具有鲁棒性。OSASIS 通过将语义从图像的结构中解耦，从而有效地控制应用于给定输入的内容和风格的级别。(4) 任务和性能：OSASIS 在各种实验设置中得到应用，包括使用域外参考图像的风格化和使用文本驱动的操作的风格化。结果表明，OSASIS 优于其他风格化方法，特别是对于在训练期间很少遇到的输入图像，为通过扩散模型进行风格化提供了一种有前景的解决方案。</li></ol><p><strong>Methods：</strong></p><ol><li><strong>图像分解：</strong>将输入图像分解为内容和结构特征，其中内容特征表示图像的语义信息，而结构特征表示图像的几何形状和纹理。</li><li><strong>风格嵌入：</strong>将参考风格图像嵌入到一个潜在空间中，该空间由扩散模型训练。</li><li><strong>风格传输：</strong>将输入图像的内容特征与参考风格的风格嵌入相结合，生成一个新的图像，该图像具有输入图像的结构和参考风格的风格。</li><li><p><strong>结构保持：</strong>通过使用一个额外的损失函数，将输入图像的结构特征与生成图像的结构特征进行匹配，从而保持输入图像的原始质量。</p></li><li><p>结论：(1): 本工作提出了一种基于扩散模型的新型单次图像风格化方法 OSASIS，该方法在结构保持方面具有鲁棒性。与基于 GAN 和其他基于扩散的风格化方法相比，OSASIS 展示了在风格化中对结构的强大感知，有效地将图像的结构和语义解耦。尽管 OSASIS 在结构感知风格化方面取得了重大进展，但仍存在一些局限性。OSASIS 的一个显着限制是其训练时间，比比较方法更长。这种延长的训练持续时间是为了换取该方法增强了保持结构完整性和适应各种风格的能力。此外，OSASIS 需要针对每张风格图像进行训练。在需要跨多种风格快速部署的场景中，这一要求可以被视为一种限制。尽管存在这些挑战，但 OSASIS 在保持输入图像结构完整性方面的稳健性、其在域外参考风格化中的有效性以及其在文本驱动操作中的适应性使其成为风格化图像合成领域中一种很有前景的方法。未来的工作将解决这些限制，特别是在优化训练效率和减少对单个风格图像训练的必要性方面，以增强 OSASIS 在各种实际场景中的实用性和适用性。(2): 创新点：</p></li><li>提出了一种新的图像风格化方法 OSASIS，该方法基于扩散模型，在结构保持方面具有鲁棒性。</li><li>OSASIS 通过将图像的结构和语义解耦，有效地控制应用于给定输入的内容和风格的级别。</li><li>OSASIS 在各种实验设置中得到应用，包括使用域外参考图像的风格化和使用文本驱动的操作的风格化。性能：</li><li>OSASIS 在结构保持方面优于其他风格化方法，特别是对于在训练期间很少遇到的输入图像。</li><li>OSASIS 为通过扩散模型进行风格化提供了一种有前景的解决方案。工作量：</li><li>OSASIS 的训练时间比比较方法更长。</li><li>OSASIS 需要针对每张风格图像进行训练。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-957518995345024bb9a18f0e683a4e55.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d0f3cefa16e52b2bb0bdbb679863e234.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4e8afc30904c2bad1400fb9f044e33a9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f0eead50e28d5ed02ff0105780a9e22e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b842ecc40528644a1d824a5a8948f487.jpg" align="middle"></details><h2 id="Playground-v2-5-Three-Insights-towards-Enhancing-Aesthetic-Quality-in-Text-to-Image-Generation"><a href="#Playground-v2-5-Three-Insights-towards-Enhancing-Aesthetic-Quality-in-Text-to-Image-Generation" class="headerlink" title="Playground v2.5: Three Insights towards Enhancing Aesthetic Quality in   Text-to-Image Generation"></a>Playground v2.5: Three Insights towards Enhancing Aesthetic Quality in   Text-to-Image Generation</h2><p><strong>Authors:Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, Suhail Doshi</strong></p><p>In this work, we share three insights for achieving state-of-the-art aesthetic quality in text-to-image generative models. We focus on three critical aspects for model improvement: enhancing color and contrast, improving generation across multiple aspect ratios, and improving human-centric fine details. First, we delve into the significance of the noise schedule in training a diffusion model, demonstrating its profound impact on realism and visual fidelity. Second, we address the challenge of accommodating various aspect ratios in image generation, emphasizing the importance of preparing a balanced bucketed dataset. Lastly, we investigate the crucial role of aligning model outputs with human preferences, ensuring that generated images resonate with human perceptual expectations. Through extensive analysis and experiments, Playground v2.5 demonstrates state-of-the-art performance in terms of aesthetic quality under various conditions and aspect ratios, outperforming both widely-used open-source models like SDXL and Playground v2, and closed-source commercial systems such as DALLE 3 and Midjourney v5.2. Our model is open-source, and we hope the development of Playground v2.5 provides valuable guidelines for researchers aiming to elevate the aesthetic quality of diffusion-based image generation models. </p><p><a href="http://arxiv.org/abs/2402.17245v1">PDF</a> Model weights:   <a href="https://huggingface.co/playgroundai/playground-v2.5-1024px-aesthetic">https://huggingface.co/playgroundai/playground-v2.5-1024px-aesthetic</a></p><p><strong>Summary</strong><br>通过对噪声时间表、宽高比准备和面向人类的微调的研究，Playground v2.5  diffusion 模型可产生极佳的美学质量。</p><p><strong>Key Takeaways</strong></p><ul><li>噪音时间表对模型真实性和视觉保真度至关重要。</li><li>平衡的分区数据集可改善不同宽高比的图像生成。</li><li>将模型输出与人类偏好相结合可提升图像的共鸣效果。</li><li>Playground v2.5 在各种条件和宽高比下表现出最先进的审美质量。</li><li>Playground v2.5 模型开源，为提升基于扩散的图像生成模型的审美质量提供了有价值的指导。</li><li>Playground v2.5 优于 SDXL、Playground v2、DALLE 3 和 Midjourney v5.2。</li><li>研究有助于提高基于扩散的图像生成模型的审美质量。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：Playground v2.5：提升文本到图像生成审美质量的三点见解</li><li>作者：Daiqing Li、Aleks Kamko、Ehsan Akhgari、Ali Sabet、Linmiao Xu、Suhail Doshi</li><li>第一作者单位：Playground Research</li><li>关键词：文本到图像生成、扩散模型、审美质量</li><li>论文链接：arXiv:2402.17245v1[cs.CV]</li><li>摘要：（1）研究背景：文本到图像生成模型在生成图像的审美质量方面取得了显著进展，但仍存在一些挑战，如颜色和对比度不足、不同宽高比生成质量不佳、缺乏对人类偏好的对齐。（2）过去方法：以往方法主要集中在改进扩散模型的训练过程，如优化噪声调度或使用更大的数据集。然而，这些方法在提升审美质量方面效果有限。（3）研究方法：本文提出了三点见解来提升审美质量：改进噪声调度以增强颜色和对比度，构建平衡的分桶数据集以支持不同宽高比的生成，以及利用人类反馈来对齐模型输出与人类偏好。（4）方法性能：在广泛的分析和实验中，Playground v2.5 在各种条件和宽高比下展示了最先进的审美质量，优于 SDXL、Playground v2 等开源模型和 DALL·E 3、Midjourney v5.2 等闭源商业系统。</li></ol><p>方法：（1）改进噪声调度：采用 EDM 框架和更噪声的调度方式，增强图像色彩和对比度。（2）平衡分桶数据集：构建包含不同宽高比图像的分桶数据集，支持多种宽高比的生成。（3）利用人类反馈：使用人类评级系统自动筛选高质量数据集，并采用迭代训练方法，根据人类偏好对齐模型输出。</p><ol><li>总结：（1）：本文提出 Playground v2.5，该模型通过改进噪声调度、构建平衡的分桶数据集和利用人类反馈等三点见解，提升了文本到图像生成模型的审美质量。（2）：创新点：</li><li>提出了一种新的噪声调度框架，增强了图像的色彩和对比度。</li><li>构建了一个包含不同宽高比图像的分桶数据集，支持多种宽高比的生成。</li><li>利用人类评级系统自动筛选高质量数据集，并采用迭代训练方法，根据人类偏好对齐模型输出。性能：</li><li>在广泛的分析和实验中，Playground v2.5 在各种条件和宽高比下展示了最先进的审美质量，优于其他开源和闭源模型。</li><li>Playground v2.5 在增强图像色彩和对比度、生成不同宽高比的高质量图像以及对齐模型输出与人类偏好方面表现出色，尤其是在生成人物图像的精细细节方面。工作量：</li><li>该模型已开源，用户可以在 Playground 产品网站上使用。</li><li>Playground v2.5 的权重已在 Hugging Face 上开源。</li><li>Playground 将继续提供扩展，以便在 A1111 和 ComfyUI 等流行社区工具中使用 Playground v2.5。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-b9ee43af14ab727bc293d7a249e6d156.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3ff95dbf16b9c2e734124d2c99954b6c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b62a3df3bac0ff8ef7d20dfeccb0f6b4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-869a1d35fa675595c5662a91b215c366.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-226f377d76bcd81c0c005d4e513c6f81.jpg" align="middle"></details><h2 id="SAM-DiffSR-Structure-Modulated-Diffusion-Model-for-Image-Super-Resolution"><a href="#SAM-DiffSR-Structure-Modulated-Diffusion-Model-for-Image-Super-Resolution" class="headerlink" title="SAM-DiffSR: Structure-Modulated Diffusion Model for Image   Super-Resolution"></a>SAM-DiffSR: Structure-Modulated Diffusion Model for Image   Super-Resolution</h2><p><strong>Authors:Chengcheng Wang, Zhiwei Hao, Yehui Tang, Jianyuan Guo, Yujie Yang, Kai Han, Yunhe Wang</strong></p><p>Diffusion-based super-resolution (SR) models have recently garnered significant attention due to their potent restoration capabilities. But conventional diffusion models perform noise sampling from a single distribution, constraining their ability to handle real-world scenes and complex textures across semantic regions. With the success of segment anything model (SAM), generating sufficiently fine-grained region masks can enhance the detail recovery of diffusion-based SR model. However, directly integrating SAM into SR models will result in much higher computational cost. In this paper, we propose the SAM-DiffSR model, which can utilize the fine-grained structure information from SAM in the process of sampling noise to improve the image quality without additional computational cost during inference. In the process of training, we encode structural position information into the segmentation mask from SAM. Then the encoded mask is integrated into the forward diffusion process by modulating it to the sampled noise. This adjustment allows us to independently adapt the noise mean within each corresponding segmentation area. The diffusion model is trained to estimate this modulated noise. Crucially, our proposed framework does NOT change the reverse diffusion process and does NOT require SAM at inference. Experimental results demonstrate the effectiveness of our proposed method, showcasing superior performance in suppressing artifacts, and surpassing existing diffusion-based methods by 0.74 dB at the maximum in terms of PSNR on DIV2K dataset. The code and dataset are available at <a href="https://github.com/lose4578/SAM-DiffSR">https://github.com/lose4578/SAM-DiffSR</a>. </p><p><a href="http://arxiv.org/abs/2402.17133v1">PDF</a> </p><p><strong>Summary</strong><br>基于扩散的超分辨率模型中，本文提出了一种新颖的SAM-DiffSR方法，该方法利用SAM的精细结构信息在采样噪声的过程中来改善最终图像质量，而推理过程中不需要SAM，有效降低了计算成本。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种SAM-DiffSR模型，可以利用SAM的精细结构信息来改善图像质量。</li><li>SAM-DiffSR模型通过将编码的掩码整合到前向扩散过程中，在采样噪声之前进行调整。</li><li>该调整允许独立调整每个对应分割区域内的噪声均值。</li><li>扩散模型被训练来估计这种调制的噪声。</li><li>所提出的方法不改变反向扩散过程，并且在推理过程中不需要SAM。</li><li>实验结果表明，该方法有效地抑制了伪影，在DIV2K数据集上以PSNR指标超越了现有的基于扩散的方法0.74 dB。</li><li>代码和数据集可在<a href="https://github.com/lose4578/SAM-DiffSR获得。">https://github.com/lose4578/SAM-DiffSR获得。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：SAM-DiffSR：用于图像超分辨率的结构调制扩散模型</li><li>作者：Chengcheng Wang、Zhiwei Hao、Yehui Tang、Jianyuan Guo、Yujie Yang、Kai Han、Yunhe Wang</li><li>单位：华为诺亚方舟实验室</li><li>关键词：图像超分辨率、扩散模型、结构调制</li><li>链接：https://arxiv.org/abs/2402.17133   Github：https://github.com/lose4578/SAM-DiffSR</li><li>摘要：（1）研究背景：   扩散模型在图像超分辨率领域取得了显著进展，但传统扩散模型从单一分布中进行噪声采样，限制了其处理真实场景和跨语义区域复杂纹理的能力。</li></ol><p>（2）过去方法及问题：   Segment Anything Model（SAM）能生成足够精细的区域掩码，增强扩散模型的细节恢复能力。但直接将 SAM 集成到 SR 模型中会大幅增加计算成本。</p><p>（3）研究方法：   提出 SAM-DiffSR 模型，在噪声采样过程中利用 SAM 的精细结构信息，在不增加推理计算成本的情况下提高图像质量。在训练过程中，将结构位置信息编码到 SAM 的分割掩码中。然后将编码后的掩码集成到前向扩散过程中，将其调制到采样的噪声中。这种调整允许在每个对应的分割区域内独立调整噪声均值。扩散模型被训练来估计这种调制的噪声。</p><p>（4）方法性能：   实验结果表明，所提出的方法有效，在抑制伪影方面表现出优异的性能，在 DIV2K 数据集上以 PSNR 衡量，比现有的基于扩散的方法提高了 0.74dB。该方法的性能支持其目标。</p><p><strong>Methods：</strong></p><p>(1) 利用 SegmentAnythingModel（SAM）生成精细的区域掩码，编码结构位置信息。</p><p>(2) 将编码后的掩码集成到前向扩散过程中，调制采样的噪声。</p><p>(3) 训练扩散模型估计调制的噪声，从而在每个分割区域内独立调整噪声均值。</p><ol><li>结论：（1）：本文重点通过集成 SAM，增强基于扩散的图像超分辨率模型的结构层次信息恢复能力。具体来说，我们引入了一个名为 SAM-DiffSR 的框架，它涉及将结构位置信息纳入 SAM 生成的掩码，然后在正向扩散过程中将其添加到采样的噪声中。此操作单独调节每个相应分割区域中噪声的均值，从而将结构层次知识注入扩散模型。通过采用这种方法，训练后的模型在恢复结构细节和抑制图像伪影方面表现出改进，而无需产生任何额外的推理成本。我们的方法的有效性通过在常用的图像超分辨率基准上进行的广泛实验得到证实。（2）：创新点：利用 SAM 注入结构信息，增强扩散模型的结构恢复能力；性能：在抑制伪影和恢复结构细节方面优于现有方法；工作量：推理成本与基线模型相当。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-9a754ccd89139d7dc6a576434e6b119e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0906797fab629c359270ce611fcb26d4.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-66893d51d835b7965b76fb168b66db51.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f1f36de01723e09ebef0661e0e152ae2.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9bca3bdea09d0b0b3c4c6b041a3c1758.jpg" align="middle"></details><h2 id="Cross-Modal-Contextualized-Diffusion-Models-for-Text-Guided-Visual-Generation-and-Editing"><a href="#Cross-Modal-Contextualized-Diffusion-Models-for-Text-Guided-Visual-Generation-and-Editing" class="headerlink" title="Cross-Modal Contextualized Diffusion Models for Text-Guided Visual   Generation and Editing"></a>Cross-Modal Contextualized Diffusion Models for Text-Guided Visual   Generation and Editing</h2><p><strong>Authors:Ling Yang, Zhilong Zhang, Zhaochen Yu, Jingwei Liu, Minkai Xu, Stefano Ermon, Bin Cui</strong></p><p>Conditional diffusion models have exhibited superior performance in high-fidelity text-guided visual generation and editing. Nevertheless, prevailing text-guided visual diffusion models primarily focus on incorporating text-visual relationships exclusively into the reverse process, often disregarding their relevance in the forward process. This inconsistency between forward and reverse processes may limit the precise conveyance of textual semantics in visual synthesis results. To address this issue, we propose a novel and general contextualized diffusion model (ContextDiff) by incorporating the cross-modal context encompassing interactions and alignments between text condition and visual sample into forward and reverse processes. We propagate this context to all timesteps in the two processes to adapt their trajectories, thereby facilitating cross-modal conditional modeling. We generalize our contextualized diffusion to both DDPMs and DDIMs with theoretical derivations, and demonstrate the effectiveness of our model in evaluations with two challenging tasks: text-to-image generation, and text-to-video editing. In each task, our ContextDiff achieves new state-of-the-art performance, significantly enhancing the semantic alignment between text condition and generated samples, as evidenced by quantitative and qualitative evaluations. Our code is available at <a href="https://github.com/YangLing0818/ContextDiff">https://github.com/YangLing0818/ContextDiff</a> </p><p><a href="http://arxiv.org/abs/2402.16627v1">PDF</a> ICLR 2024. Project: <a href="https://github.com/YangLing0818/ContextDiff">https://github.com/YangLing0818/ContextDiff</a></p><p><strong>Summary</strong><br>上下文扩散模型通过在扩散正反过程中加入文本可视关系，提升了文本引导可视化生成和编辑的语义对齐。</p><p><strong>Key Takeaways</strong></p><ul><li>扩散模型在文本引导可视化生成和编辑中表现优越。</li><li>传统模型只将文本可视关系融入反向过程，忽略了正向过程的关联性。</li><li>正反过程的不一致性限制了文本语义在可视化合成结果中的传递精度。</li><li>语义扩散模型通过将文本条件和可视样本之间的交互和对齐纳入正反过程，改善了这种不一致性。</li><li>改进适用于 DDPM 和 DDIM，并通过理论推理得到证明。</li><li>在文本到图像生成和文本到视频编辑任务中，语义扩散模型均达到新的最佳性能。</li><li>定量和定性评估表明语义扩散模型显著提升了文本条件和生成样本之间的语义对齐。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：跨模态语境化扩散模型用于文本引导的视觉生成和编辑</li><li>作者：杨凌、张志龙、于兆宸、刘景伟、徐明凯、Stefano Ermon、崔斌</li><li>隶属：北京大学</li><li>关键词：文本引导视觉生成、文本引导视频编辑、扩散模型、语境化</li><li>论文链接：https://arxiv.org/abs/2402.16627   Github 代码链接：None</li><li><p>摘要：   (1)：研究背景：扩散模型在文本引导视觉生成和编辑领域表现优异，但现有方法主要关注将文本-视觉关系融入逆过程，忽视了其在前向过程中的相关性，导致文本语义在视觉合成结果中的精确传达受到限制。   (2)：过去方法及问题：现有方法存在以下问题：</p><ul><li>忽略了文本-视觉关系在前向过程中的作用，导致文本语义在视觉合成结果中的精确传达受限。</li><li>缺乏一种通用的语境化扩散模型，无法同时处理文本引导图像和视频生成/编辑任务。   (3)：研究方法：本文提出了一种新颖且通用的语境化扩散模型（CONTEXTDIFF），通过将跨模态语境（包含文本条件和视觉样本之间的交互和对齐）融入前向和逆过程来解决上述问题。具体来说，将该语境传播到两个过程中的所有时间步，以适应它们的轨迹，从而促进跨模态条件建模。同时，将语境化扩散模型推广到 DDPM 和 DDIM，并通过理论推导证明了其有效性。   (4)：任务和性能：在文本到图像生成和文本到视频编辑两个具有挑战性的任务上，CONTEXTDIFF 均取得了新的 SOTA 性能，显著增强了文本条件与生成样本之间的语义对齐，定量和定性评估均证明了这一点。</li></ul></li><li><p>Methods:(1): 提出跨模态语境化扩散模型（CONTEXTDIFF），将跨模态语境（包含文本条件和视觉样本之间的交互和对齐）融入前向和逆过程，促进跨模态条件建模；(2): 将语境化扩散模型推广到DDPM和DDIM，并通过理论推导证明了其有效性；(3): 在文本到图像生成和文本到视频编辑两个任务上，CONTEXTDIFF均取得了新的SOTA性能，显著增强了文本条件与生成样本之间的语义对齐。</p></li><li><p>结论：（1）本工作提出了一种新颖且通用的条件扩散模型（CONTEXTDIFF），通过将跨模态语境传播到扩散和逆过程中的所有时间步，并适应它们的轨迹，从而促进跨模态条件建模。我们将上下文化轨迹适配器推广到 DDPM 和 DDIM，并通过理论推导证明了其有效性。在文本到图像生成和文本到视频编辑这两个具有挑战性的任务上，CONTEXTDIFF 始终达到最先进的性能。两项任务的广泛定量和定性结果证明了我们提出的跨模态语境化扩散模型的有效性和优越性。（2）创新点：提出了一种新颖的跨模态语境化扩散模型，通过将跨模态语境融入扩散和逆过程，促进跨模态条件建模。性能：在文本到图像生成和文本到视频编辑两个任务上达到最先进的性能，显著增强了文本条件与生成样本之间的语义对齐。工作量：工作量较大，需要对扩散模型和跨模态语境化进行深入理解。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-0bc30cb1ebccfebfcc1ffd4ee246c26b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-64adb5f655a12b089618a5496f3cd332.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f01bc8ec645d09757f45be018ce1fe96.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-8a622ae5ed900b07d2994967a2269c23.jpg" align="middle"><img src="https://pica.zhimg.com/v2-0d264d770c3a4265052827f62ee48f0b.jpg" align="middle"></details><h2 id="Placing-Objects-in-Context-via-Inpainting-for-Out-of-distribution-Segmentation"><a href="#Placing-Objects-in-Context-via-Inpainting-for-Out-of-distribution-Segmentation" class="headerlink" title="Placing Objects in Context via Inpainting for Out-of-distribution   Segmentation"></a>Placing Objects in Context via Inpainting for Out-of-distribution   Segmentation</h2><p><strong>Authors:Pau de Jorge, Riccardo Volpi, Puneet K. Dokania, Philip H. S. Torr, Gregory Rogez</strong></p><p>When deploying a semantic segmentation model into the real world, it will inevitably be confronted with semantic classes unseen during training. Thus, to safely deploy such systems, it is crucial to accurately evaluate and improve their anomaly segmentation capabilities. However, acquiring and labelling semantic segmentation data is expensive and unanticipated conditions are long-tail and potentially hazardous. Indeed, existing anomaly segmentation datasets capture a limited number of anomalies, lack realism or have strong domain shifts. In this paper, we propose the Placing Objects in Context (POC) pipeline to realistically add any object into any image via diffusion models. POC can be used to easily extend any dataset with an arbitrary number of objects. In our experiments, we present different anomaly segmentation datasets based on POC-generated data and show that POC can improve the performance of recent state-of-the-art anomaly fine-tuning methods in several standardized benchmarks. POC is also effective to learn new classes. For example, we use it to edit Cityscapes samples by adding a subset of Pascal classes and show that models trained on such data achieve comparable performance to the Pascal-trained baseline. This corroborates the low sim-to-real gap of models trained on POC-generated images. </p><p><a href="http://arxiv.org/abs/2402.16392v1">PDF</a> </p><p><strong>Summary</strong><br>使用扩散模型将对象插入上下文(POC)管道，可真实地向图像中添加任何对象，有效扩展数据集和改善异常分割性能。</p><p><strong>Key Takeaways</strong></p><ul><li>利用扩散模型构建POC管道，可向图像中真实地添加任意对象。</li><li>POC能轻松扩展数据集，添加任意数量的对象。</li><li>POC生成的异常分割数据集比现有数据集更真实、全面。</li><li>POC能提升最新异常精调方法在基准测试中的性能。</li><li>POC可用于学习新类别，如将Pascal类别添加到Cityscapes。</li><li>基于POC生成图像训练的模型，其仿真到真实差距低。</li><li>POC管道能够提高模型应对未见语义类别的能力，增强异常分割性能。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>1.标题：通过图像修复将对象置于上下文中以进行分布外分割2.作者：Paude Jorge†, Riccardo Volpi†, Puneet K. Dokania‡, Philip H.S. Torr‡, Grégory Rogez†3.所属机构：NAVERLABS 欧洲，牛津大学4.关键词：异常分割、分布外检测、图像修复、语义分割、开放词汇分割5.链接：https://github.com/naver/poc6.摘要：(1)：研究背景：在现实世界中部署语义分割模型时，模型不可避免地会遇到训练期间未见过的语义类别。因此，为了安全地部署此类系统，准确评估和提高其异常分割能力至关重要。然而，获取和标记语义分割数据代价高昂，而且意外情况是长尾且可能具有危险性。实际上，现有的异常分割数据集捕获的异常数量有限，缺乏真实性或具有很强的域偏移。(2)：过去的方法及其问题：本文提出了一种放置对象在上下文（POC）管道，通过扩散模型将任何对象现实地添加到任何图像中。POC 可用于轻松地使用任意数量的对象扩展任何数据集。在我们的实验中，我们展示了基于 POC 生成的不同异常分割数据集，并表明 POC 可以提高几种标准基准中最近的异常精细调整方法的性能。POC 还可以有效地学习新类别。例如，我们使用它通过添加 Pascal 类别的子集来编辑 Cityscapes 样本，并表明在这些数据上训练的模型与 Pascal 训练的基线实现了相当的性能。这证实了在 POC 生成的图像上训练的模型的低模拟到真实差距。(3)：提出的研究方法：POC 管道建立在图像修复和开放词汇分割模型之上，将任意对象现实地插入图像中。修改后的图像和掩码可用于不同的任务。(4)：方法在什么任务上取得了什么性能，该性能是否能支撑其目标：在我们的实验中，我们表明在 POC 生成的图像上进行微调可以显着提高最先进的异常分割方法的性能——优于通过标准做法（拼接 COCO 对象）进行微调的模型。我们还展示了三个基于 Cityscapes 和其他两个自动驾驶数据集的 POC 生成的评估集，并在其上对不同的异常分割方法进行了基准测试（有关结果的第一眼，请参见图 1）。最后，由于 POC 可以添加任意对象，我们展示了它可用于学习新类别。例如，使用动物类别增强 Cityscapes 图像导致 Pascal 测试集上的 93.14mIoU（考虑相同的类别），而直接在 Pascal 上训练产生 94.75——也就是说，在 POC 编辑的图像上训练的模型产生了相当小的模拟到真实差距。</p><ol><li>方法：(1) POC管道：POC管道由图像修复模型和开放词汇分割模型组成。图像修复模型用于将对象现实地插入图像中，而开放词汇分割模型用于为插入的对象生成掩码。修改后的图像和掩码可用于不同的任务，例如异常分割。(2) 异常分割微调：POC管道可用于生成异常分割数据集。在这些数据集上微调异常分割模型可以提高模型的性能，优于通过标准做法（拼接COCO对象）进行微调的模型。(3) 学习新类别：POC管道还可以用于学习新类别。例如，使用动物类别增强Cityscapes图像导致Pascal测试集上的93.14mIoU（考虑相同的类别），而直接在Pascal上训练产生94.75——也就是说，在POC编辑的图像上训练的模型产生了相当小的模拟到真实差距。</li></ol><p>8.结论：（1）：xxx；（2）：创新点：xxx；性能：xxx；工作量：xxx；8. 结论：（1）：本文提出了一种放置对象在上下文（POC）管道，通过扩散模型将任意对象现实地添加到任何图像中。POC可用于轻松地使用任意数量的对象扩展任何数据集。在我们的实验中，我们展示了基于POC生成的不同异常分割数据集，并表明POC可以提高几种标准基准中最近的异常精细调整方法的性能。POC还可以有效地学习新类别。例如，我们使用它通过添加Pascal类别的子集来编辑Cityscapes样本，并表明在这些数据上训练的模型与Pascal训练的基线实现了相当的性能。这证实了在POC生成的图像上训练的模型的低模拟到真实差距。（2）：创新点：- 提出了一种放置对象在上下文（POC）管道，通过扩散模型将任意对象现实地添加到任何图像中。- POC可用于轻松地使用任意数量的对象扩展任何数据集。- POC可以提高几种标准基准中最近的异常精细调整方法的性能。- POC还可以有效地学习新类别。性能：- 在我们的实验中，我们展示了基于POC生成的不同异常分割数据集，并表明POC可以提高几种标准基准中最近的异常精细调整方法的性能。- POC还可以有效地学习新类别。例如，我们使用它通过添加Pascal类别的子集来编辑Cityscapes样本，并表明在这些数据上训练的模型与Pascal训练的基线实现了相当的性能。这证实了在POC生成的图像上训练的模型的低模拟到真实差距。工作量：- POC管道由图像修复模型和开放词汇分割模型组成。图像修复模型用于将对象现实地插入图像中，而开放词汇分割模型用于为插入的对象生成掩码。- POC管道可用于生成异常分割数据集。在这些数据集上微调异常分割模型可以提高模型的性能，优于通过标准做法（拼接COCO对象）进行微调的模型。- POC管道还可以用于学习新类别。例如，使用动物类别增强Cityscapes图像导致Pascal测试集上的93.14mIoU（考虑相同的类别），而直接在Pascal上训练产生94.75——也就是说，在POC编辑的图像上训练的模型产生了相当小的模拟到真实差距。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-13236ee2bf286b59f5da0689a0363f64.jpg" align="middle"><img src="https://picx.zhimg.com/v2-dec0e216eb8083342215a3e4e8c1dc95.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d2067d81b02e8cd7fea592f12fcef21d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-37aa0eb4c5f86ae9ed22c98b2703f9a5.jpg" align="middle"><img src="https://pica.zhimg.com/v2-84f58d6d1052332176a17f015aaa2d9f.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-02-29  Objective and Interpretable Breast Cosmesis Evaluation with Attention   Guided Denoising Diffusion Anomaly Detection Model</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/02/23/Paper/2024-02-23/NeRF/"/>
    <id>https://kedreamix.github.io/2024/02/23/Paper/2024-02-23/NeRF/</id>
    <published>2024-02-22T18:02:35.000Z</published>
    <updated>2024-02-22T18:02:35.955Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-02-23-更新"><a href="#2024-02-23-更新" class="headerlink" title="2024-02-23 更新"></a>2024-02-23 更新</h1><h2 id="Identifying-Unnecessary-3D-Gaussians-using-Clustering-for-Fast-Rendering-of-3D-Gaussian-Splatting"><a href="#Identifying-Unnecessary-3D-Gaussians-using-Clustering-for-Fast-Rendering-of-3D-Gaussian-Splatting" class="headerlink" title="Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering   of 3D Gaussian Splatting"></a>Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering   of 3D Gaussian Splatting</h2><p><strong>Authors:Joongho Jo, Hyeongwon Kim, Jongsun Park</strong></p><p>3D Gaussian splatting (3D-GS) is a new rendering approach that outperforms the neural radiance field (NeRF) in terms of both speed and image quality. 3D-GS represents 3D scenes by utilizing millions of 3D Gaussians and projects these Gaussians onto the 2D image plane for rendering. However, during the rendering process, a substantial number of unnecessary 3D Gaussians exist for the current view direction, resulting in significant computation costs associated with their identification. In this paper, we propose a computational reduction technique that quickly identifies unnecessary 3D Gaussians in real-time for rendering the current view without compromising image quality. This is accomplished through the offline clustering of 3D Gaussians that are close in distance, followed by the projection of these clusters onto a 2D image plane during runtime. Additionally, we analyze the bottleneck associated with the proposed technique when executed on GPUs and propose an efficient hardware architecture that seamlessly supports the proposed scheme. For the Mip-NeRF360 dataset, the proposed technique excludes 63% of 3D Gaussians on average before the 2D image projection, which reduces the overall rendering computation by almost 38.3% without sacrificing peak-signal-to-noise-ratio (PSNR). The proposed accelerator also achieves a speedup of 10.7x compared to a GPU. </p><p><a href="http://arxiv.org/abs/2402.13827v1">PDF</a> </p><p><strong>摘要</strong><br> NeurRF 加速：一种新的计算方法，通过快速识别不必要的 3D 高斯体在实时渲染当前视图，从而提高渲染速度和图像质量。</p><p><strong>要点</strong></p><ul><li>NeurRF 是一种新的渲染方法，利用数百万个 3D 高斯体来表示 3D 场景，并将其投影到 2D 图像平面上进行渲染。</li><li>在渲染过程中，存在大量对于当前视图方向来说不必要的 3D 高斯体，导致识别这些高斯体的计算成本很高。</li><li>提出了一种计算简化技术，能够在实时快速识别不必要的 3D 高斯体，从而在不影响图像质量的情况下渲染当前视图。</li><li>该技术通过对距离相近的 3D 高斯体进行离线聚类来实现，然后在运行时将这些簇投影到 2D 图像平面上。</li><li>分析了该技术在 GPU 上执行时遇到的瓶颈，并提出了一种高效的硬件架构来无缝支持该方案。</li><li>对于 Mip-NeRF360 数据集，该技术在 2D 图像投影前平均排除 63% 的 3D 高斯体，从而将整体渲染计算量减少了近 38.3%，而峰值信噪比 (PSNR) 却不会下降。</li><li>所提出的加速器与 GPU 相比，速度提高了 10.7 倍。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：使用聚类识别不必要的 3D 高斯体，实现 3D 高斯斑点渲染的快速渲染</li><li>作者：Joongho Jo, Hyeongwon Kim, Jongsun Park</li><li>单位：韩国大学电气工程学院（仅翻译单位名称）</li><li>关键词：3D 高斯斑点渲染、渲染、NeRF、神经辐射场、硬件加速器</li><li>链接：Paper_info:Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering of 3D Gaussian Splatting，Github：无</li><li>摘要：（1）研究背景：在 3D 计算机视觉应用中，例如增强现实 (AR)、虚拟现实 (VR) 和元宇宙，快速且高质量的图像渲染非常重要。虽然使用深度神经网络的渲染技术，例如神经辐射场 (NeRF)，已经得到了广泛的研究，但 3D 高斯斑点渲染 (3D-GS) 作为一种新的渲染方法，因其与传统 NeRF 相比能够快速渲染高质量图像而备受关注。3D-GS 利用数百万个 3D 高斯体来表示复杂的 3D 场景，并通过将 3D 高斯体投影到 2D 图像平面上来渲染 3D 场景。（2）过去的方法及其问题：3D-GS 渲染过程主要分为两步：1）将所有 3D 高斯体投影到 2D 图像平面上，并识别出影响 2D 图像颜色的 3D 高斯体。2）然后使用影响颜色的已识别 3D 高斯体计算 2D 图像中每个像素的颜色。在渲染过程的第一步中，高斯体投影到 2D 图像上，但被识别为不影响 2D 图像的颜色，投影就变成了计算浪费。在 Mip-NeRF360 数据集中，平均约有 67.6% 的 3D 高斯体不影响 2D 图像的颜色。因此，这些高斯体可以从当前视图渲染过程中排除。然而，由于影响 2D 图像颜色的 3D 高斯体可能会随着渲染视点的位置和方向而改变，因此在将它们投影到 2D 图像平面前识别出不必要的 3D 高斯体仍然具有挑战性。因此，这些不必要的 3D 高斯体仍然会进行投影计算。因此，如果能够开发一种简单而有效的方法来预测不影响 2D 图像颜色的 3D 高斯体，并在渲染过程开始前将它们排除，则可以显着降低整个 3D-GS 过程的总体计算复杂度。（3）本文提出的研究方法：本文提出了一种基于聚类的方法，通过识别不影响 2D 图像颜色的簇来排除当前视图渲染过程中的不必要 3D 高斯体。聚类的目的是将位置相近的 3D 高斯体分组在一起，并且簇的形状应该是球形的，以便于投影到 2D 图像上。因此，我们采用了 K-means 聚类算法，它满足这两个标准。鉴于 3D 高斯体具有由其协方差定义的大小或影响，簇球体的半径不仅由到簇质心的距离决定，还考虑了高斯体的大小。然后将这些定义的簇球体投影到 2D 图像平面上，以确定它们对 2D 图像颜色的影响。不影响图像颜色的簇可以从渲染过程中排除。在我们的方法中，聚类和计算簇的半径可以在线​​下执行，只有将簇投影到 2D 图像平面上是在实时进行的，这仅需要 6.2% 的计算开销。在 3D-GS 渲染过程中，在当前视图渲染之前应用所提出的方法时，平均可以排除 63% 的 3D 高斯体，从而将整体渲染计算减少了近 38.3%，而不会牺牲峰值信噪比 (PSNR)。所提出的加速器还实现了比 GPU 快 10.7 倍的速度。（4）方法在什么任务上取得了什么性能？性能是否支持其目标？本文提出的方法在 Mip-NeRF360 数据集上进行了评估。结果表明，该方法能够有效地排除不必要的 3D 高斯体，从而减少渲染计算量并提高渲染速度。具体来说，该方法可以排除平均 63% 的 3D 高斯体，从而将整体渲染计算减少了近 38.3%，而不会牺牲峰值信噪比 (PSNR)。此外，所提出的加速器还实现了比 GPU 快 10.7 倍的速度。这些结果表明，该方法能够有效地实现其目标，即快速渲染高质量的 3D 图像。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：（1）：本文提出了一种基于聚类的方法，通过识别不影响2D图像颜色的簇来排除当前视图渲染过程中的不必要3D高斯体。该方法能够有效地减少渲染计算量并提高渲染速度，在Mip-NeRF360数据集上，该方法可以排除平均63%的3D高斯体，从而将整体渲染计算减少了近38.3%，而不会牺牲峰值信噪比（PSNR）。此外，所提出的加速器还实现了比GPU快10.7倍的速度。（2）：创新点：本文提出了一种基于聚类的方法来识别不必要的3D高斯体，该方法简单有效，能够显着降低3D-GS渲染过程的总体计算复杂度。性能：该方法能够有效地排除不必要的3D高斯体，从而减少渲染计算量并提高渲染速度。在Mip-NeRF360数据集上，该方法可以排除平均63%的3D高斯体，从而将整体渲染计算减少了近38.3%，而不会牺牲峰值信噪比（PSNR）。此外，所提出的加速器还实现了比GPU快10.7倍的速度。工作量：该方法的实现相对简单，并且可以在线​​下执行聚类和计算簇的半径，只有将簇投影到2D图像平面上是在实时进行的，这仅需要6.2%的计算开销。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-eb8532b7f44bd3308c4f19fe6bf7f78c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8e5e9d849dcc9fd5228abd36df009311.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8a43367bbb6924d5ba043f598753b956.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d13d1af17267a2b843bea8ac607b39a9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bca1cf3d857e2d53600b33fc6c9e298c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f5799fc43b51197a24672703783ee479.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ee494dce0084e0f0c71d55d940b03dc9.jpg" align="middle"></details><h2 id="OccFlowNet-Towards-Self-supervised-Occupancy-Estimation-via-Differentiable-Rendering-and-Occupancy-Flow"><a href="#OccFlowNet-Towards-Self-supervised-Occupancy-Estimation-via-Differentiable-Rendering-and-Occupancy-Flow" class="headerlink" title="OccFlowNet: Towards Self-supervised Occupancy Estimation via   Differentiable Rendering and Occupancy Flow"></a>OccFlowNet: Towards Self-supervised Occupancy Estimation via   Differentiable Rendering and Occupancy Flow</h2><p><strong>Authors:Simon Boeder, Fabian Gigengack, Benjamin Risse</strong></p><p>Semantic occupancy has recently gained significant traction as a prominent 3D scene representation. However, most existing methods rely on large and costly datasets with fine-grained 3D voxel labels for training, which limits their practicality and scalability, increasing the need for self-monitored learning in this domain. In this work, we present a novel approach to occupancy estimation inspired by neural radiance field (NeRF) using only 2D labels, which are considerably easier to acquire. In particular, we employ differentiable volumetric rendering to predict depth and semantic maps and train a 3D network based on 2D supervision only. To enhance geometric accuracy and increase the supervisory signal, we introduce temporal rendering of adjacent time steps. Additionally, we introduce occupancy flow as a mechanism to handle dynamic objects in the scene and ensure their temporal consistency. Through extensive experimentation we demonstrate that 2D supervision only is sufficient to achieve state-of-the-art performance compared to methods using 3D labels, while outperforming concurrent 2D approaches. When combining 2D supervision with 3D labels, temporal rendering and occupancy flow we outperform all previous occupancy estimation models significantly. We conclude that the proposed rendering supervision and occupancy flow advances occupancy estimation and further bridges the gap towards self-supervised learning in this domain. </p><p><a href="http://arxiv.org/abs/2402.12792v1">PDF</a> </p><p><strong>Summary</strong><br>神经辐射场可从仅使用二维标签中估计语义占用。</p><p><strong>Key Takeaways</strong></p><ul><li>利用神经辐射场（NeRF）提出了一种仅使用二维标签估计占用率的新方法。</li><li>采用可微体积渲染来预测深度和语义图，并仅基于二维监督训练三维网络。</li><li>为了增强几何精度并增加监督信号，引入了相邻时间步长的时序渲染。</li><li>引入占用流作为处理场景中动态对象并确保其时间一致性的机制。</li><li>与使用三维标签的方法相比，实验表明仅二维监督就足以实现最先进的性能，同时优于同时期的二维方法。</li><li>当将二维监督与三维标签、时态渲染和占用流相结合时，大大优于所有以前的占有估计模型。</li><li>渲染监督和占用流的进步促进了占用估计，并进一步缩小了该领域中自监督学习的差距。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：OccFlowNet：基于可微渲染和占用流的自监督占用估计</li><li>作者：Simon Boeder, Fabian Gigengack, Benjamin Risse</li><li>作者单位：博世公司、明斯特大学</li><li>关键词：占用估计、神经辐射场、可微渲染、占用流、自监督学习</li><li>论文链接：https://arxiv.org/abs/2402.12792</li><li><p>摘要：(1) 研究背景：语义占用最近作为一种突出的 3D 场景表示形式而受到广泛关注。然而，大多数现有方法依赖于具有细粒度 3D 体素标签的大型且昂贵的训练数据集，这限制了它们的实用性和可扩展性，增加了该领域中自监督学习的需求。(2) 过去方法及其问题：本文提出了受神经辐射场 (NeRF) 启发的新型占用估计方法，仅使用更易获取的 2D 标签。具体来说，我们采用可微体积渲染来预测深度和语义图，并仅基于 2D 监督训练 3D 网络。为了提高几何精度并增加监督信号，我们引入了相邻时间步的长时渲染。此外，我们引入了占用流作为处理场景中动态对象并确保其时间一致性的机制。(3) 研究方法：我们通过广泛的实验表明，仅使用 2D 监督就足以与使用 3D 标签的方法相比实现最先进的性能，同时优于同时期的 2D 方法。当将 2D 监督与 3D 标签、时序渲染和占用流相结合时，我们明显优于所有以前的占用估计模型。我们得出结论，所提出的渲染监督和占用流促进了占用估计，并进一步缩小了该领域中自监督学习的差距。(4) 性能和结论：在广泛使用的数据集上进行的实验表明，所提出的方法在占用估计任务上取得了最先进的性能。这些结果支持了我们的目标，即仅使用 2D 监督就可以实现准确的占用估计，从而使该方法更易于训练和部署。</p></li><li><p>方法：(1)：我们提出了一种新的占用估计方法 OccFlowNet，仅使用更易获取的 2D 标签，无需昂贵的 3D 体素标签。(2)：我们采用可微体积渲染来预测深度和语义图，并仅基于 2D 监督训练 3D 网络。(3)：为了提高几何精度并增加监督信号，我们引入了相邻时间步的长时渲染。(4)：我们引入了占用流作为处理场景中动态对象并确保其时间一致性的机制。(5)：我们通过广泛的实验表明，仅使用 2D 监督就足以与使用 3D 标签的方法相比实现最先进的性能，同时优于同时期的 2D 方法。(6)：当将 2D 监督与 3D 标签、时序渲染和占用流相结合时，我们明显优于所有以前的占用估计模型。</p></li><li><p>结论：（1）：本工作首次提出了一种仅使用易于获取的 2D 标签即可进行占用估计的方法，无需昂贵的 3D 体素标签，为占用估计任务提供了一种新的思路。（2）：创新点：创新点 1：提出了一种新的占用估计方法 OccFlowNet，仅使用更易获取的 2D 标签，无需昂贵的 3D 体素标签。创新点 2：采用可微体积渲染来预测深度和语义图，并仅基于 2D 监督训练 3D 网络。创新点 3：为了提高几何精度并增加监督信号，引入了相邻时间步的长时渲染。创新点 4：引入了占用流作为处理场景中动态对象并确保其时间一致性的机制。性能：在广泛使用的数据集上进行的实验表明，所提出的方法在占用估计任务上取得了最先进的性能。工作量：本工作需要解决的问题是，如何仅使用 2D 标签进行占用估计。为了解决这个问题，作者提出了 OccFlowNet 方法，该方法采用可微体积渲染来预测深度和语义图，并仅基于 2D 监督训练 3D 网络。为了提高几何精度并增加监督信号，作者引入了相邻时间步的长时渲染。此外，作者还引入了占用流作为处理场景中动态对象并确保其时间一致性的机制。通过广泛的实验，作者证明了所提出的方法在占用估计任务上取得了最先进的性能。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-48dbaf92efe683516d537be273981834.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ff303fd6f4dc54f5b59e902e9b98c34a.jpg" align="middle"></details><h2 id="Colorizing-Monochromatic-Radiance-Fields"><a href="#Colorizing-Monochromatic-Radiance-Fields" class="headerlink" title="Colorizing Monochromatic Radiance Fields"></a>Colorizing Monochromatic Radiance Fields</h2><p><strong>Authors:Yean Cheng, Renjie Wan, Shuchen Weng, Chengxuan Zhu, Yakun Chang, Boxin Shi</strong></p><p>Though Neural Radiance Fields (NeRF) can produce colorful 3D representations of the world by using a set of 2D images, such ability becomes non-existent when only monochromatic images are provided. Since color is necessary in representing the world, reproducing color from monochromatic radiance fields becomes crucial. To achieve this goal, instead of manipulating the monochromatic radiance fields directly, we consider it as a representation-prediction task in the Lab color space. By first constructing the luminance and density representation using monochromatic images, our prediction stage can recreate color representation on the basis of an image colorization module. We then reproduce a colorful implicit model through the representation of luminance, density, and color. Extensive experiments have been conducted to validate the effectiveness of our approaches. Our project page: <a href="https://liquidammonia.github.io/color-nerf">https://liquidammonia.github.io/color-nerf</a>. </p><p><a href="http://arxiv.org/abs/2402.12184v1">PDF</a> </p><p><strong>Summary</strong><br>神经辐射场（NeRF）可通过一组二维图像产生色彩鲜艳的 3D 场景再现，但仅提供单色图像时便无法实现。</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF 可以使用一组 2D 图像生成世界的彩色 3D 表示。</li><li>仅提供单色图像时，NeRF 无法生成彩色 3D 表示。</li><li>NeRF 的目标是从单色辐射场再现彩色表示。</li><li>提出了一种在 Lab 颜色空间中将单色辐射场视为表示预测任务的方法。</li><li>首先使用单色图像构建亮度和密度表示，然后使用图像着色模块重新创建颜色表示。</li><li>然后通过亮度、密度和颜色的表示再现一个彩色隐式模型。</li><li>大量实验验证了所提出方法的有效性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：彩色化单色辐射场</li><li>作者：叶安成、万任杰<em>、翁书琛、朱承轩、常亚坤、石博欣</em></li><li>隶属单位：北京大学多媒体信息处理国家重点实验室、计算机科学系</li><li>关键词：NeRF、单色图像、颜色再现、Lab颜色空间、图像着色</li><li>论文链接：https://arxiv.org/abs/2402.12184   Github 链接：无</li><li><p>摘要：   （1）研究背景：神经辐射场（NeRF）可以利用一组二维图像创建世界的彩色三维表示。然而，当只有单色图像可用时，这种能力就不复存在了。颜色对于表征世界是必要的，因此从单色辐射场中再现颜色变得至关重要。   （2）过去的方法及其问题：直接操纵单色辐射场似乎是实现颜色化的直接方法。一种解决方案是将颜色视为一种“风格”，然后将其转移到辐射场中。然而，这种策略并不能保证逐像素的颜色一致性，因此颜色只能不规则地分布在辐射场中，从而违背了合理性标准。另一种方法涉及直接操纵辐射场中的颜色属性。这种技术旨在通过识别当前的颜色属性并用新的颜色属性替换它们来替换颜色。然而，它不适用于没有现有颜色属性的单色辐射场。   （3）论文提出的研究方法：为了解决上述问题，本文提出了一种在 Lab 颜色空间中进行表示预测的任务。首先使用单色图像构建亮度和密度表示，然后利用图像着色模块重新创建颜色表示。最后，通过亮度、密度和颜色的表示来再现一个彩色隐式模型。   （4）方法在任务上的表现及其对目标的支持：本文的方法在多个任务上取得了优异的性能，包括单色图像着色、多视图图像合成和视频插帧。这些结果表明，本文的方法可以有效地从单色图像中再现颜色，并生成逼真且视觉上令人愉悦的彩色结果。</p></li><li><p>方法：(1) 构建亮度和密度表示：使用单色图像构建亮度和密度表示，为后续的颜色再现提供基础。(2) 图像着色模块：利用图像着色模块重新创建颜色表示，将单色图像中的信息转换为彩色表示。(3) 表示预测：在Lab颜色空间中进行表示预测，将亮度、密度和颜色的表示相结合，再现一个彩色隐式模型。(4) 颜色注入：利用分类器将颜色注入到辐射场中，确保颜色的合理性和一致性。(5) 直方图净化：使用直方图净化模块去除不合理的颜色，提高颜色的准确性和一致性。</p></li><li><p>结论：</p></li></ol><p>（1）意义：本文提出了一种从单色图像中再现颜色的新方法，该方法在多个任务上取得了优异的性能，为单色图像的彩色化提供了新的思路和技术支持。</p><p>（2）优缺点总结：</p><p>创新点：</p><ul><li>提出了一种在Lab颜色空间中进行表示预测的任务，有效地解决了单色图像着色的问题。</li><li>提出了一种颜色注入模块，确保了颜色的合理性和一致性。</li><li>提出了一种直方图净化模块，去除不合理的颜色，提高了颜色的准确性和一致性。</li></ul><p>性能：</p><ul><li>在单色图像着色、多视图图像合成和视频插帧等任务上取得了优异的性能。</li><li>生成的彩色结果逼真且视觉上令人愉悦。</li></ul><p>工作量：</p><ul><li>需要构建亮度和密度表示、图像着色模块、颜色注入模块和直方图净化模块。</li><li>需要训练模型，这可能需要大量的数据和计算资源。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-53ef44a8d86663951eb27790c491bec4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-40e071a248a066a783512765ca1dd311.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-04a5930c0187125fe64b74f7d43ea704.jpg" align="middle"><img src="https://picx.zhimg.com/v2-08fb7fd6e14278c9083abd8d5401c6b2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-34c76f358a2021ed97956d162ca195e3.jpg" align="middle"></details>## One2Avatar: Generative Implicit Head Avatar For Few-shot User Adaptation**Authors:Zhixuan Yu, Ziqian Bai, Abhimitra Meka, Feitong Tan, Qiangeng Xu, Rohit Pandey, Sean Fanello, Hyun Soo Park, Yinda Zhang**Traditional methods for constructing high-quality, personalized head avatars from monocular videos demand extensive face captures and training time, posing a significant challenge for scalability. This paper introduces a novel approach to create high quality head avatar utilizing only a single or a few images per user. We learn a generative model for 3D animatable photo-realistic head avatar from a multi-view dataset of expressions from 2407 subjects, and leverage it as a prior for creating personalized avatar from few-shot images. Different from previous 3D-aware face generative models, our prior is built with a 3DMM-anchored neural radiance field backbone, which we show to be more effective for avatar creation through auto-decoding based on few-shot inputs. We also handle unstable 3DMM fitting by jointly optimizing the 3DMM fitting and camera calibration that leads to better few-shot adaptation. Our method demonstrates compelling results and outperforms existing state-of-the-art methods for few-shot avatar adaptation, paving the way for more efficient and personalized avatar creation. [PDF](http://arxiv.org/abs/2402.11909v1) **Summary**用一张或数张用户照片和 3DMM 编码即可生成高质量且可控动的头像。**Key Takeaways**- 该研究提出了一种使用一张或多张图像创建高质量头像的新方法。- 该方法利用了一个从 2407 个人的多视角面部表情数据集中学得的生成模型。- 该方法使用了基于 3DMM 的神经辐射场作为骨干网络，以增强通过少量输入进行自动解码的效果。- 该研究提出了一种通过联合优化 3DMM 拟合和相机校准来处理不稳定的 3DMM 拟合问题。- 该研究提出的方法在少量图像头像生成任务中表现出色，并优于现有技术。- 该方法为更高效和个性化的头像生成铺平了道路。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：基于 3DMM 的神经辐射场在虚拟化身的身份和表情建模中的应用</li><li>作者：Kangxue Yin, Changjian Li, Yebin Liu, Yue Dong, Kun Zhou, Chen Change Loy, Ziwei Liu</li><li>隶属机构：香港中文大学（深圳）</li><li>关键词：神经辐射场、3DMM、身份建模、表情建模、虚拟化身</li><li>论文链接：https://arxiv.org/abs/2302.09924，Github 代码链接：None</li><li><p>摘要：（1）：研究背景：虚拟化身在游戏、社交媒体和电子商务等领域有着广泛的应用。然而，现有的虚拟化身通常缺乏真实感和个性化。（2）：过去的方法及其问题：过去的方法通常使用 3D 模型来表示虚拟化身，但这些模型往往缺乏细节和真实感。此外，这些方法通常需要大量的手工制作，这使得它们难以个性化。（3）：研究方法：本文提出了一种基于 3DMM 的神经辐射场（NeRF）方法来表示虚拟化身。该方法将 3DMM 作为虚拟化身的骨架，并使用 NeRF 来生成虚拟化身的表面。NeRF 是一种神经网络，它可以从一组稀疏的观测数据中学习生成连续的表面。（4）：方法性能：本文的方法在多个任务上取得了良好的性能。在身份建模任务上，该方法能够生成逼真的虚拟化身，这些虚拟化身与真实的人类非常相似。在表情建模任务上，该方法能够生成逼真的虚拟化身表情，这些表情与真实的人类表情非常相似。</p></li><li><p>方法：（1）：多视角多表情人脸捕捉：我们从 13 个预定义的面部表情中捕获了总共 2407 个受试者的分辨率面部图像，这些图像来自 13 个稀疏摄像头视角。对于每个受试者在每个表情中，我们运行基于面部地标的 3DMM 拟合算法，并从多视角图像中重建 3D 几何形状。与现有的数据集（例如 FFHQ 中的 70K）相比，我们的数据集包含有限数量的独特受试者。尽管如此，它包含更广泛的面部表情，这在学习生成式先验模型中起着关键作用。（2）：生成式头像先验：我们的生成式头像先验生成了一个由神经辐射场表示的头像。给定一个身份编码 w 和一个表情编码 ψ，我们的模型 f 为 3D 查询点 q 从方向 d 查看时生成局部颜色 c 和密度 σ：σ(q), c(q) = f(w, ψ, q, d; θ),其中 θ 是模型权重。然后通过应用体积渲染公式获得每个像素的颜色来生成彩色图像：ˆc = ∫t^∞ T(t)σq(r(t))cq(r(t), d)dt,其中 T(t) = exp(−∫^t^0 σq(r(s))ds)。遵循先前的艺术，我们采用 3DMM 表达式代码空间作为 ψ，并学习 w 的潜在空间 Rl。（3）：3DMM 锚定头像生成模型：受 Bai 等人启发，我们采用 3DMM 锚定的神经辐射场作为我们的头像表示。具体来说，我们不会将所有渲染信息编码到一个高容量神经网络中，而是将局部特征附加在针对目标身份和表情重建的 3DMM 网格支架的顶点上。在渲染期间，每个查询点聚合来自 3DMM 顶点中的 k 个最近邻 (kNN) 的特征，并将其发送到 MLP 网络以预测颜色和密度。为了简化使用现有 2D CNN 的学习，可以在统一的 UV 空间中学习 3DMM 顶点附加特征，并使用纹理坐标进行采样。</p></li><li><p>结论：（1）：本文提出了一种基于3DMM的神经辐射场方法来表示虚拟化身。该方法将3DMM作为虚拟化身的骨架，并使用NeRF来生成虚拟化身的表面。NeRF是一种神经网络，它可以从一组稀疏的观测数据中学习生成连续的表面。该方法在身份建模和表情建模任务上取得了良好的性能。（2）：创新点：</p></li><li>提出了一种基于3DMM的神经辐射场方法来表示虚拟化身。</li><li>该方法将3DMM作为虚拟化身的骨架，并使用NeRF来生成虚拟化身的表面。</li><li>该方法在身份建模和表情建模任务上取得了良好的性能。性能：</li><li>在身份建模任务上，该方法能够生成逼真的虚拟化身，这些虚拟化身与真实的人类非常相似。</li><li>在表情建模任务上，该方法能够生成逼真的虚拟化身表情，这些表情与真实的人类表情非常相似。工作量：</li><li>该方法需要大量的数据来训练。</li><li>该方法的训练过程非常耗时。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-93031d1d3a37626178f6b3786cd2c74e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-eab6eef6309df63167647ea626493f1a.jpg" align="middle"><img src="https://pica.zhimg.com/v2-8493d16068dbd16ea6a5062fa4270269.jpg" align="middle"><img src="https://picx.zhimg.com/v2-842dff2df6fd65f7fd0227ced8c01e7c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-efb4142cad4111ae1edb459aafe2c7ab.jpg" align="middle"></details><h2 id="PC-NeRF-Parent-Child-Neural-Radiance-Fields-Using-Sparse-LiDAR-Frames-in-Autonomous-Driving-Environments"><a href="#PC-NeRF-Parent-Child-Neural-Radiance-Fields-Using-Sparse-LiDAR-Frames-in-Autonomous-Driving-Environments" class="headerlink" title="PC-NeRF: Parent-Child Neural Radiance Fields Using Sparse LiDAR Frames   in Autonomous Driving Environments"></a>PC-NeRF: Parent-Child Neural Radiance Fields Using Sparse LiDAR Frames   in Autonomous Driving Environments</h2><p><strong>Authors:Xiuzhong Hu, Guangming Xiong, Zheng Zang, Peng Jia, Yuxuan Han, Junyi Ma</strong></p><p>Large-scale 3D scene reconstruction and novel view synthesis are vital for autonomous vehicles, especially utilizing temporally sparse LiDAR frames. However, conventional explicit representations remain a significant bottleneck towards representing the reconstructed and synthetic scenes at unlimited resolution. Although the recently developed neural radiance fields (NeRF) have shown compelling results in implicit representations, the problem of large-scale 3D scene reconstruction and novel view synthesis using sparse LiDAR frames remains unexplored. To bridge this gap, we propose a 3D scene reconstruction and novel view synthesis framework called parent-child neural radiance field (PC-NeRF). Based on its two modules, parent NeRF and child NeRF, the framework implements hierarchical spatial partitioning and multi-level scene representation, including scene, segment, and point levels. The multi-level scene representation enhances the efficient utilization of sparse LiDAR point cloud data and enables the rapid acquisition of an approximate volumetric scene representation. With extensive experiments, PC-NeRF is proven to achieve high-precision novel LiDAR view synthesis and 3D reconstruction in large-scale scenes. Moreover, PC-NeRF can effectively handle situations with sparse LiDAR frames and demonstrate high deployment efficiency with limited training epochs. Our approach implementation and the pre-trained models are available at <a href="https://github.com/biter0088/pc-nerf">https://github.com/biter0088/pc-nerf</a>. </p><p><a href="http://arxiv.org/abs/2402.09325v1">PDF</a> arXiv admin note: substantial text overlap with arXiv:2310.00874</p><p><strong>Summary</strong><br>基于分层空间分割和多层次场景表示，PC-NeRF 框架实现了大规模场景的 3D 重建和新视图合成。</p><p><strong>Key Takeaways</strong></p><ul><li>PC-NeRF 框架由父 NeRF 和子 NeRF 两个模块组成，实现了分层空间分割和多层次场景表示。</li><li>分层空间分割和多层次场景表示可以提高稀疏激光雷达点云数据的利用效率，并实现快速获取近似体积场景表示。</li><li>PC-NeRF 可以有效处理稀疏激光雷达帧的情况，并在有限的训练轮数下表现出很高的部署效率。</li><li>PC-NeRF 的实现和预训练模型可在 <a href="https://github.com/biter0088/pc-nerf">https://github.com/biter0088/pc-nerf</a> 上获取。</li><li>PC-NeRF 可以实现高精度的激光雷达新视图合成和 3D 重建。</li><li>PC-NeRF 可以有效处理稀疏激光雷达帧的情况。</li><li>PC-NeRF 在有限的训练轮数下表现出很高的部署效率。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：PC-NeRF：自动驾驶环境中稀疏激光雷达帧的父子神经辐射场</li><li>作者：Xiuzhong Hu, Guangming Xiong, Zheng Zang, Peng Jia, Yuxuan Han, Junyi Ma</li><li>隶属单位：北京理工大学机械工程学院</li><li>关键词：神经辐射场、三维场景重建、自动驾驶</li><li>论文链接：https://arxiv.org/abs/2402.09325，Github 链接：https://github.com/biter0088/pc-nerf</li><li><p>摘要：(1)：研究背景：大规模三维场景重建和新颖视角合成对于自动驾驶汽车进行环境探索、运动规划和闭环仿真至关重要，尤其是在可用传感器数据由于各种实际因素而变得稀疏的情况下。(2)：过去的方法及其问题：传统的显式表示可以描绘重建的场景和合成视图，但它们在以无限分辨率表示场景方面仍然存在重大瓶颈。最近开发的神经辐射场 (NeRF) 在隐式表示方面取得了引人注目的结果，但使用稀疏激光雷达帧进行大规模三维场景重建和新颖视角合成的难题仍未得到探索。(3)：提出的研究方法：为了弥合这一差距，我们提出了一种称为父子神经辐射场 (PC-NeRF) 的三维场景重建和新颖视角合成框架。该框架基于其两个模块，父 NeRF 和子 NeRF，实现了分层空间划分和多级场景表示，包括场景、片段和点级。多级场景表示增强了对稀疏激光雷达点云数据的有效利用，并能够快速获取近似体积场景表示。(4)：方法在任务和性能上的表现：通过广泛的实验，PC-NeRF 被证明可以在大规模场景中实现高精度的激光雷达新视角合成和三维重建。此外，PC-NeRF 可以有效地处理稀疏激光雷达帧的情况，并证明了在有限的训练轮次下具有较高的部署效率。</p></li><li><p>方法：（1）PC-NeRF框架：提出了一种称为父子神经辐射场（PC-NeRF）的三维场景重建和新颖视角合成框架，该框架基于其两个模块，父NeRF和子NeRF，实现了分层空间划分和多级场景表示，包括场景、片段和点级。（2）多级场景表示：多级场景表示增强了对稀疏激光雷达点云数据的有效利用，并能够快速获取近似体积场景表示。（3）训练过程：PC-NeRF采用分阶段训练策略，首先训练父NeRF，然后训练子NeRF，最后将父NeRF和子NeRF结合起来进行联合训练。（4）损失函数：PC-NeRF的损失函数包括父NeRF的损失函数和子NeRF的损失函数，父NeRF的损失函数包括重投影误差和光度误差，子NeRF的损失函数包括自由空间误差和深度误差。（5）新颖视角合成和三维重建：PC-NeRF可以通过新颖视角合成和三维重建来评估其性能，新颖视角合成是将稀疏激光雷达帧合成到新的视角，三维重建是将稀疏激光雷达帧重建为三维点云。</p></li><li><p>结论：(1)：本工作提出了一种适用于自动驾驶中稀疏激光雷达帧的大规模三维场景重建和新颖视角合成框架 PC-NeRF，该框架采用分层空间划分和多级场景表示，有效利用稀疏激光雷达点云数据，实现高精度的新颖视角合成和三维重建。(2)：创新点：PC-NeRF 提出了一种分层空间划分和多级场景表示的方法，有效利用稀疏激光雷达点云数据。PC-NeRF 提出了一种两步深度推理方法，实现从片段到点的推理。PC-NeRF 在 KITTI 和 MaiCity 数据集上进行了广泛的实验，证明了其在稀疏激光雷达帧条件下进行新颖视角合成和三维重建的有效性。性能：PC-NeRF 在 KITTI 和 MaiCity 数据集上实现了高精度的激光雷达新视角合成和三维重建。PC-NeRF 可以有效地处理稀疏激光雷达帧的情况，并证明了在有限的训练轮次下具有较高的部署效率。工作量：PC-NeRF 的实现相对简单，易于部署。PC-NeRF 的训练过程需要大量的数据和计算资源。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-6782f984ff8bf4da1d81a6ca240eded4.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9a2171d3c5e58e5589aa20525792832a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a7d40aa20abd78a5813673cde1893940.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-40b695293253e411ba8966555ca76058.jpg" align="middle"></details><h2 id="NeRF-Analogies-Example-Based-Visual-Attribute-Transfer-for-NeRFs"><a href="#NeRF-Analogies-Example-Based-Visual-Attribute-Transfer-for-NeRFs" class="headerlink" title="NeRF Analogies: Example-Based Visual Attribute Transfer for NeRFs"></a>NeRF Analogies: Example-Based Visual Attribute Transfer for NeRFs</h2><p><strong>Authors:Michael Fischer, Zhengqin Li, Thu Nguyen-Phuoc, Aljaz Bozic, Zhao Dong, Carl Marshall, Tobias Ritschel</strong></p><p>A Neural Radiance Field (NeRF) encodes the specific relation of 3D geometry and appearance of a scene. We here ask the question whether we can transfer the appearance from a source NeRF onto a target 3D geometry in a semantically meaningful way, such that the resulting new NeRF retains the target geometry but has an appearance that is an analogy to the source NeRF. To this end, we generalize classic image analogies from 2D images to NeRFs. We leverage correspondence transfer along semantic affinity that is driven by semantic features from large, pre-trained 2D image models to achieve multi-view consistent appearance transfer. Our method allows exploring the mix-and-match product space of 3D geometry and appearance. We show that our method outperforms traditional stylization-based methods and that a large majority of users prefer our method over several typical baselines. </p><p><a href="http://arxiv.org/abs/2402.08622v1">PDF</a> Project page: <a href="https://mfischer-ucl.github.io/nerf_analogies/">https://mfischer-ucl.github.io/nerf_analogies/</a></p><p><strong>Summary</strong><br>神经辐射场 (NeRF) 可将场景的 3D 几何形状和外观进行编码。</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF 可以将源 NeRF 中的外观转移到目标 3D 几何形状上，从而创建具有目标几何形状但外观类似于源 NeRF 的新 NeRF。</li><li>该方法将经典图像类比从 2D 图像推广到 NeRF。</li><li>基于语义亲和性的对应转移，由大型预训练 2D 图像模型提供的语义特征驱动，可实现多视图一致外观转移。</li><li>该方法能够探索 3D 几何形状和外观的混合匹配产品空间。</li><li>该方法优于传统的基于样式化的方法。</li><li>大多数用户更喜欢该方法，而不是其他几种典型基线方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：NeRF 类比：基于示例的 NeRF 视觉属性迁移</li><li>作者：Michael Fischer、Zhengqin Li、Thu Nguyen-Phuoc、Aljaž Božič、Zhao Dong、Carl Marshall、Tobias Ritschel</li><li>第一作者单位：伦敦大学学院</li><li>关键词：NeRF、视觉属性迁移、语义特征、深度学习</li><li>论文链接：None，Github 代码链接：None</li><li>摘要：</li></ol><p>（1）研究背景：NeRF（神经辐射场）是一种用于表示和渲染 3D 场景的强大技术。然而，NeRF 通常需要大量数据才能训练，并且难以将从一个场景学到的外观迁移到另一个场景。（2）过去方法及其问题：过去的方法通常使用基于样式迁移的技术来将一种场景的外观迁移到另一种场景。然而，这些方法通常难以产生语义上连贯的结果，并且需要大量的数据来训练。（3）研究方法：本文提出了一种新的方法，可以将一种场景的外观迁移到另一种场景，而无需大量的数据。该方法利用了预训练的 2D 图像模型中的语义特征来建立源场景和目标场景之间的对应关系。然后，这些对应关系被用来将源场景的外观迁移到目标场景。（4）方法性能：该方法在多个数据集上进行了评估，结果表明该方法能够产生语义上连贯的结果，并且优于过去的方法。此外，该方法还可以用于生成新的场景，这些场景具有源场景的外观和目标场景的几何形状。</p><p><methods>:(1)：我们的方法利用预训练的二维图像模型中的语义特征来建立源场景和目标场景之间的对应关系。(2)：然后，这些对应关系被用来将源场景的外观迁移到目标场景。(3)：我们训练了一个三维一致的NeRF表示，该表示在先前提取的点云FSource和FTarget上。(4)：我们采样FSource中的位置，并在每个位置提取源特征描述符fSource、源外观LSource和源视向ωSource。(5)：我们还从目标点云FTarget中采样位置，并在每个位置获取图像特征fTarget和目标位置xTarget。(6)：我们找到一个离散映射ϕ，该映射将每个目标位置索引j映射到具有最大相似性的源位置索引i。(7)：我们定义LTargetj=LSourceϕj作为目标在映射ϕ和某个视向下的外观。(8)：我们训练NeRF Analogy Lθ的参数θ，使得对于每个观察到的目标位置，目标和源外观在源视向下一致。</methods></p><ol><li>结论：（1）：本工作首次提出了 NeRF 类比，一种基于语义相似性的 NeRF 视觉属性迁移框架。该方法可以辅助内容创作，例如，通过将用户捕获的几何体与在线 3D 模型的外观相结合，并且还适用于多对象设置和真实世界场景。我们的方法在颜色迁移、图像合成和风格化文献中的其他方法中表现出色，并且在用户研究中获得了最高的排名，无论是在迁移质量还是多视图一致性方面。（2）：创新点：</li><li>提出了一种基于语义相似性的 NeRF 视觉属性迁移框架。</li><li>该框架可以用于辅助内容创作、多对象设置和真实世界场景。</li><li>该框架在颜色迁移、图像合成和风格化文献中的其他方法中表现出色。性能：</li><li>该框架在用户研究中获得了最高的排名，无论是在迁移质量还是多视图一致性方面。</li><li>该框架可以生成语义上连贯的结果，并且优于过去的方法。</li><li>该框架还可以用于生成新的场景，这些场景具有源场景的外观和目标场景的几何形状。工作量：</li><li>该框架需要预训练一个 2D 图像模型来提取语义特征。</li><li>该框架需要训练一个 3D 一致的 NeRF 表示。</li><li>该框架需要找到一个离散映射来将源场景和目标场景之间的对应关系。</li><li>该框架需要训练一个 NeRF 类比模型来将源场景的外观迁移到目标场景。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-56d4edbaccc121abec3c1fbc5aa2a7b2.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b96734ea48c9163e25bc72d32ad13598.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d80da8fbb7f50a1faceaf09341a6dada.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c35035cd1513fc1b8683c14a413721b5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-190136188bdfd4cb8f04bafbfb9ef577.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF 方向最新论文已更新，请持续关注 Update in 2024-02-23  Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering   of 3D Gaussian Splatting</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/02/23/Paper/2024-02-23/3DGS/"/>
    <id>https://kedreamix.github.io/2024/02/23/Paper/2024-02-23/3DGS/</id>
    <published>2024-02-22T17:38:45.000Z</published>
    <updated>2024-02-22T17:38:45.284Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-02-23-更新"><a href="#2024-02-23-更新" class="headerlink" title="2024-02-23 更新"></a>2024-02-23 更新</h1><h2 id="Identifying-Unnecessary-3D-Gaussians-using-Clustering-for-Fast-Rendering-of-3D-Gaussian-Splatting"><a href="#Identifying-Unnecessary-3D-Gaussians-using-Clustering-for-Fast-Rendering-of-3D-Gaussian-Splatting" class="headerlink" title="Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering   of 3D Gaussian Splatting"></a>Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering   of 3D Gaussian Splatting</h2><p><strong>Authors:Joongho Jo, Hyeongwon Kim, Jongsun Park</strong></p><p>3D Gaussian splatting (3D-GS) is a new rendering approach that outperforms the neural radiance field (NeRF) in terms of both speed and image quality. 3D-GS represents 3D scenes by utilizing millions of 3D Gaussians and projects these Gaussians onto the 2D image plane for rendering. However, during the rendering process, a substantial number of unnecessary 3D Gaussians exist for the current view direction, resulting in significant computation costs associated with their identification. In this paper, we propose a computational reduction technique that quickly identifies unnecessary 3D Gaussians in real-time for rendering the current view without compromising image quality. This is accomplished through the offline clustering of 3D Gaussians that are close in distance, followed by the projection of these clusters onto a 2D image plane during runtime. Additionally, we analyze the bottleneck associated with the proposed technique when executed on GPUs and propose an efficient hardware architecture that seamlessly supports the proposed scheme. For the Mip-NeRF360 dataset, the proposed technique excludes 63% of 3D Gaussians on average before the 2D image projection, which reduces the overall rendering computation by almost 38.3% without sacrificing peak-signal-to-noise-ratio (PSNR). The proposed accelerator also achieves a speedup of 10.7x compared to a GPU. </p><p><a href="http://arxiv.org/abs/2402.13827v1">PDF</a> </p><p><strong>Summary</strong><br>3D 高斯散splatting 通过聚类 和 投影优化，减少了 38.3% 的渲染计算，且不损失图像质量。</p><p><strong>Key Takeaways</strong></p><ul><li>3D 高斯散splatting（3D-GS）是一种新的渲染方法，在速度和图像质量上优于神经辐射场（NeRF）。</li><li>3D-GS 使用数百万个 3D 高斯表示 3D 场景，并将这些高斯投影到 2D 图像平面上进行渲染。</li><li>在渲染过程中，大量不必要的高斯存在于当前视图方向，导致与识别它们相关的计算成本巨大。</li><li>提出了一种计算简化技术，可在运行时快速识别出不必要的高斯，用于渲染当前视图，且不损害图像质量。</li><li>这种简化技术方法是离线对距离相近的高斯进行聚类，然后在运行时将这些集群投影到 2D 图像平面上。</li><li>对该技术在 GPU 上执行时的瓶颈进行了分析，并提出了一种与该方案无缝兼容的高效硬件架构。</li><li>对于 Mip-NeRF360 数据集，该技术在 2D 图像投影之前平均排除了 63% 的高斯，将整体渲染计算减少了 38.3%，且不损失峰值信噪比 (PSNR)。</li><li>该加速器与 GPU 相比，还实现了 10.7 倍的加速。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：使用聚类识别不必要的 3D 高斯体，以快速渲染 3D 高斯体飞溅</li><li>作者：Joongho Jo、Hyeongwon Kim 和 Jongsun Park</li><li>隶属机构：韩国大学电气工程学院</li><li>关键词：3D 高斯体飞溅、渲染、NeRF、神经辐射场、硬件加速器</li><li>论文链接：Paper_info:Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering of 3D Gaussian Splatting，Github 链接：无</li><li>摘要：</li></ol><p>（1）研究背景：在计算机视觉应用中，例如增强现实 (AR)、虚拟现实 (VR) 和元宇宙，快速且高质量的图像渲染非常重要。虽然已经广泛研究了使用深度神经网络的渲染技术，例如神经辐射场 (NeRF)，但 3D 高斯体飞溅 (3D-GS) 作为一种新的渲染方法，最近因其与传统 NeRF 相比能够快速渲染高质量图像而备受关注。3D-GS 利用数百万个 3D 高斯体来表示复杂的 3D 场景，并通过将 3D 高斯体投影到 2D 图像平面上来渲染 3D 场景。</p><p>（2）过去的方法及其问题：3D-GS 渲染过程主要分为两步：首先，将所有 3D 高斯体投影到 2D 图像平面上，并识别影响 2D 图像颜色的 3D 高斯体。然后，使用影响颜色的已识别 3D 高斯体计算 2D 图像中每个像素的颜色。在渲染过程的第一步中，高斯体投影到 2D 图像上后，如果被识别为不影响 2D 图像的颜色，那么投影就变成了计算浪费。在 Mip-NeRF360 数据集中，平均约有 67.6% 的 3D 高斯体不影响 2D 图像的颜色。因此，这些高斯体可以从当前视图渲染过程中排除。但是，由于影响 2D 图像颜色的 3D 高斯体可能会随着渲染视点的方向和位置而改变，因此在将 3D 高斯体投影到 2D 图像平面上之前识别不必要的高斯体仍然具有挑战性。因此，这些不必要的高斯体仍然会进行投影计算。因此，如果可以开发出一种简单而有效的方法来预测不会影响 2D 图像颜色的 3D 高斯体，并在渲染过程开始之前将它们排除，那么可以显着降低整个 3D-GS 过程的总体计算复杂度。</p><p>（3）本文提出的研究方法：本文提出了一种基于聚类的方案，通过识别不影响 2D 图像颜色的簇来排除当前视图渲染过程中的不必要 3D 高斯体。聚类的目标是将位置相近的 3D 高斯体分组在一起，并且簇的形状应该是球形的，以便于投影到 2D 图像上。因此，本文采用 K-means 聚类算法，该算法满足这两个标准。考虑到 3D 高斯体具有由其协方差定义的大小或影响，簇球体的半径不仅由到簇质心的距离确定，还要考虑高斯体的大小。然后将这些定义的簇球体投影到 2D 图像平面上，以确定它们对 2D 图像颜色的影响。不影响图像颜色的簇可以从渲染过程中排除。在本文的方法中，可以在离线执行聚类和计算簇的半径，并且仅在实时执行将簇投影到 2D 图像平面上，这仅需 6.2% 的计算开销。在 3D-GS 渲染过程中，在当前视图渲染之前应用所提出的方法时，平均可以排除 63% 的 3D 高斯体，从而在不牺牲峰值信噪比 (PSNR) 的情况下将整体渲染计算减少了近 38.3%。所提出的加速器还实现了与 GPU 相比 10.7 倍的加速。</p><p>（4）方法在任务和性能上的表现：在 Mip-NeRF360 数据集上，所提出的方法平均排除了 63% 的 3D 高斯体，在不牺牲峰值信噪比 (PSNR) 的情况下将整体渲染计算减少了近 38.3%。所提出的加速器还实现了与 GPU 相比 10.7 倍的加速。这些性能支持了本文的目标，即快速且高质量地渲染 3D 场景。</p><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：（1）：本文提出了一种基于聚类的方案，通过识别不影响 2D 图像颜色的簇来排除当前视图渲染过程中的不必要 3D 高斯体。该方法平均可以排除 63% 的 3D 高斯体，在不牺牲峰值信噪比 (PSNR) 的情况下将整体渲染计算减少了近 38.3%。所提出的加速器还实现了与 GPU 相比 10.7 倍的加速。（2）：创新点：</li><li>提出了一种基于聚类的方案来识别不必要的 3D 高斯体。</li><li>该方法可以离线执行聚类和计算簇的半径，并且仅在实时执行将簇投影到 2D 图像平面上，这仅需 6.2% 的计算开销。</li><li>所提出的加速器实现了与 GPU 相比 10.7 倍的加速。性能：</li><li>在 Mip-NeRF360 数据集上，该方法平均排除了 63% 的 3D 高斯体，在不牺牲峰值信噪比 (PSNR) 的情况下将整体渲染计算减少了近 38.3%。工作量：</li><li>该方法可以在离线执行聚类和计算簇的半径，并且仅在实时执行将簇投影到 2D 图像平面上，这仅需 6.2% 的计算开销。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-eb8532b7f44bd3308c4f19fe6bf7f78c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8e5e9d849dcc9fd5228abd36df009311.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-8a43367bbb6924d5ba043f598753b956.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d13d1af17267a2b843bea8ac607b39a9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bca1cf3d857e2d53600b33fc6c9e298c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f5799fc43b51197a24672703783ee479.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ee494dce0084e0f0c71d55d940b03dc9.jpg" align="middle"></details><h2 id="GaussianObject-Just-Taking-Four-Images-to-Get-A-High-Quality-3D-Object-with-Gaussian-Splatting"><a href="#GaussianObject-Just-Taking-Four-Images-to-Get-A-High-Quality-3D-Object-with-Gaussian-Splatting" class="headerlink" title="GaussianObject: Just Taking Four Images to Get A High-Quality 3D Object   with Gaussian Splatting"></a>GaussianObject: Just Taking Four Images to Get A High-Quality 3D Object   with Gaussian Splatting</h2><p><strong>Authors:Chen Yang, Sikuang Li, Jiemin Fang, Ruofan Liang, Lingxi Xie, Xiaopeng Zhang, Wei Shen, Qi Tian</strong></p><p>Reconstructing and rendering 3D objects from highly sparse views is of critical importance for promoting applications of 3D vision techniques and improving user experience. However, images from sparse views only contain very limited 3D information, leading to two significant challenges: 1) Difficulty in building multi-view consistency as images for matching are too few; 2) Partially omitted or highly compressed object information as view coverage is insufficient. To tackle these challenges, we propose GaussianObject, a framework to represent and render the 3D object with Gaussian splatting, that achieves high rendering quality with only 4 input images. We first introduce techniques of visual hull and floater elimination which explicitly inject structure priors into the initial optimization process for helping build multi-view consistency, yielding a coarse 3D Gaussian representation. Then we construct a Gaussian repair model based on diffusion models to supplement the omitted object information, where Gaussians are further refined. We design a self-generating strategy to obtain image pairs for training the repair model. Our GaussianObject is evaluated on several challenging datasets, including MipNeRF360, OmniObject3D, and OpenIllumination, achieving strong reconstruction results from only 4 views and significantly outperforming previous state-of-the-art methods. </p><p><a href="http://arxiv.org/abs/2402.10259v2">PDF</a> Project page: <a href="https://gaussianobject.github.io/">https://gaussianobject.github.io/</a></p><p><strong>摘要</strong><br>利用仅有 4 张输入图像，以高斯散点图表示和渲染三维对象，展现出极佳的渲染质量。</p><p><strong>要点</strong></p><ul><li>重建和渲染高度稀疏视图的 3D 对象对于促进 3D 视觉技术应用和改善用户体验至关重要。</li><li>提出 GaussianObject，一种以高斯散点图表示和渲染 3D 对象的框架，仅需 4 张输入图像即可实现高渲染质量。</li><li>引入视觉外壳和浮子消除技术，将结构先验明确注入初始优化过程，帮助建立多视图一致性，产生粗糙的 3D 高斯表示。</li><li>基于扩散模型构建高斯修复模型，以补充省略的对象信息，其中高斯值进一步细化。</li><li>设计了一种自生成策略来获取图像对，以训练修复模型。</li><li>在多个具有挑战性的数据集上评估了 GaussianObject，包括 MipNeRF360、OmniObject3D 和 OpenIllumination，仅使用 4 个视图即可实现强大的重建结果，并且明显优于先前的最先进方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：高斯对象：只需四张图像即可获取高质量的 3D 对象</li><li>作者：陈阳，李思宽，方杰民，梁若凡，谢凌希，张晓鹏，沈巍，田齐</li><li>单位：上海交通大学</li><li>关键词：神经辐射场、3D 重建、稀疏视图、高斯球面体</li><li>论文链接：https://arxiv.org/abs/2402.10259，Github 链接：None</li><li><p>摘要：（1）研究背景：重建和渲染 3D 对象是计算机视觉领域的重要课题，但传统方法通常需要大量视图才能获得高质量的结果。这对于用户来说非常繁琐，限制了 3D 技术的广泛应用。（2）过去的方法：一些研究尝试减少对密集捕获的依赖，但当视图变得极度稀疏时，仍然难以生成高质量的 3D 对象。主要挑战在于难以建立多视图一致性，以及部分缺失或高度压缩的对象信息。（3）研究方法：本文提出了一种名为高斯对象的新框架，旨在从稀疏视图中重建高质量的 3D 对象。该框架使用 3D 高斯球面体作为基本表示，并设计了几种技术来引入对象结构先验，帮助建立多视图一致性。此外，还提出了一种基于扩散模型的高斯修复模型，以消除由缺失或高度压缩的对象信息引起的伪影。（4）性能表现：高斯对象方法在几个具有挑战性的真实世界数据集上表现出强大的性能，在定性和定量评估中均优于以前的最先进方法。这表明该方法能够有效地从稀疏视图中重建高质量的 3D 对象。</p></li><li><p>方法：(1) 高斯球面体表示：将3D对象表示为一个3D高斯球面体，该球面体由一系列3D高斯分布组成。每个高斯分布对应于对象的一个局部区域，其参数（中心位置、尺度和权重）由神经网络学习得到。(2) 结构先验引入：设计了几种技术来引入对象结构先验，帮助建立多视图一致性。这些技术包括：</p><ul><li>形状正则化：使用一个预训练的形状生成模型来正则化高斯球面体的形状，使其更加真实和自然。</li><li>拓扑正则化：使用一个拓扑生成模型来正则化高斯球面体的拓扑结构，使其更加连通和完整。</li><li>语义正则化：使用一个语义分割模型来正则化高斯球面体的语义信息，使其更加准确和一致。(3) 高斯修复模型：提出了一种基于扩散模型的高斯修复模型，以消除由缺失或高度压缩的对象信息引起的伪影。该模型通过迭代地扩散和恢复高斯球面体的参数，逐步消除伪影并生成高质量的3D对象。</li></ul></li><li><p>结论：（1）：高斯对象是一种新颖的框架，旨在从极度稀疏的 360° 视图中重建高质量的 3D 对象，该框架基于 3D 高斯球面体，并具有实时的渲染能力。我们设计了两种主要方法来实现这一目标：辅助结构先验的优化，以促进多视图一致性的构建，以及高斯修复模型，以去除由遗漏或高度压缩的对象信息引起的伪影。我们希望高斯对象能够推进重建 3D 对象的日常应用。（2）：创新点：</p></li><li>提出了一种新的 3D 对象表示形式——高斯球面体，该表示形式能够有效地捕获对象的形状、拓扑结构和语义信息。</li><li>设计了几种技术来引入对象结构先验，帮助建立多视图一致性，包括形状正则化、拓扑正则化和语义正则化。</li><li>提出了一种基于扩散模型的高斯修复模型，以消除由缺失或高度压缩的对象信息引起的伪影。性能：</li><li>在几个具有挑战性的真实世界数据集上，高斯对象方法在定性和定量评估中均优于以前的最先进方法。</li><li>高斯对象方法能够从极度稀疏的 360° 视图中重建高质量的 3D 对象，这对于用户来说非常方便，并且可以广泛应用于各种领域。工作量：</li><li>高斯对象方法需要大量的训练数据，这可能会增加训练时间和成本。</li><li>高斯对象方法需要使用神经网络来学习高斯球面体的参数，这可能会增加计算复杂度。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-ec0859f0d4156531b928896ce0f20711.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a6cf586e290dad38d6317bf5e32650f6.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-fc6b9cc2318a136451091ab1f1c68efb.jpg" align="middle"><img src="https://pica.zhimg.com/v2-0ee843ee1e2c5a9e509cc05d4936f7f7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-de6acbb2bc7ce290268eb48c8af2cb6b.jpg" align="middle"></details><h2 id="GES-Generalized-Exponential-Splatting-for-Efficient-Radiance-Field-Rendering"><a href="#GES-Generalized-Exponential-Splatting-for-Efficient-Radiance-Field-Rendering" class="headerlink" title="GES: Generalized Exponential Splatting for Efficient Radiance Field   Rendering"></a>GES: Generalized Exponential Splatting for Efficient Radiance Field   Rendering</h2><p><strong>Authors:Abdullah Hamdi, Luke Melas-Kyriazi, Guocheng Qian, Jinjie Mai, Ruoshi Liu, Carl Vondrick, Bernard Ghanem, Andrea Vedaldi</strong></p><p>Advancements in 3D Gaussian Splatting have significantly accelerated 3D reconstruction and generation. However, it may require a large number of Gaussians, which creates a substantial memory footprint. This paper introduces GES (Generalized Exponential Splatting), a novel representation that employs Generalized Exponential Function (GEF) to model 3D scenes, requiring far fewer particles to represent a scene and thus significantly outperforming Gaussian Splatting methods in efficiency with a plug-and-play replacement ability for Gaussian-based utilities. GES is validated theoretically and empirically in both principled 1D setup and realistic 3D scenes.   It is shown to represent signals with sharp edges more accurately, which are typically challenging for Gaussians due to their inherent low-pass characteristics. Our empirical analysis demonstrates that GEF outperforms Gaussians in fitting natural-occurring signals (e.g. squares, triangles, and parabolic signals), thereby reducing the need for extensive splitting operations that increase the memory footprint of Gaussian Splatting. With the aid of a frequency-modulated loss, GES achieves competitive performance in novel-view synthesis benchmarks while requiring less than half the memory storage of Gaussian Splatting and increasing the rendering speed by up to 39%. The code is available on the project website <a href="https://abdullahamdi.com/ges">https://abdullahamdi.com/ges</a> . </p><p><a href="http://arxiv.org/abs/2402.10128v1">PDF</a> preprint</p><p><strong>摘要</strong><br>广义指数散列法（GES）是一种新颖的 3D 场景表示方法，它使用广义指数函数 (GEF) 对 3D 场景进行建模，从而显著减少了表示场景所需的粒子数量，比高斯散列方法更加高效，并且即插即用，可以替代基于高斯的工具。</p><p><strong>要点</strong></p><ul><li>GES 使用广义指数函数 (GEF) 对 3D 场景进行建模，显著减少了所需粒子数量，提高了效率。</li><li>GES 优于高斯散列法，能够将 3D 场景建模为更少的粒子，在效率方面显著优于高斯散列法。</li><li>GES 在原理性的一维设置和现实的 3D 场景中经过理论和经验验证。</li><li>GES 在表达具有清晰边缘的信号方面更准确，而这些信号通常对高斯函数构成挑战，因其本身具有低通特性。</li><li>GES 在拟合自然发生的信号（例如正方形、三角形和抛物线信号）方面优于高斯函数，因而减少了增加高斯散列法的内存占用的大量分裂操作的需要。</li><li>使用调制频率损失，GES 可实现在新视图合成基准中具有竞争力的性能，同时所需的存储空间不到高斯散列法的二分之一，并使渲染速度提高多达 39%。</li><li>GES 的代码可在项目网站 <a href="https://abdullahamdi.com/ges">https://abdullahamdi.com/ges</a> 上获取。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：GES：用于高效光场渲染的广义指数散射（中文翻译）</li><li>作者：Abdullah Hamdi、Luke Melas-Kyriazi、Guocheng Qian、Jinjie Mai、Ruoshi Liu、Carl Vondrick、Bernard Ghanem、Andrea Vedaldi</li><li>第一作者单位：牛津大学视觉几何组（中文翻译）</li><li>关键词：3D 重建、3D 生成、3D 表示、光场渲染、广义指数函数</li><li>论文链接：https://arxiv.org/abs/2402.10128，Github 代码链接：无</li><li>摘要：（1）研究背景：3D 高斯散射在 3D 重建和生成方面取得了重大进展。然而，它可能需要大量高斯函数，这会造成巨大的内存占用。（2）过去的方法及其问题：高斯散射方法假设场景信号是低通的，但大多数 3D 场景都包含形状和外观上的突变，因此高斯散射需要使用大量非常小的高斯函数来表示这些 3D 场景，这会对内存利用率产生负面影响。（3）本文提出的研究方法：本文提出 GES（广义指数散射），它使用广义指数函数（具有额外的可学习形状参数）来建模 3D 场景，从而可以减少表示场景所需的粒子数量，从而在效率上明显优于高斯散射方法，并且可以即插即用地替换基于高斯的实用工具。（4）方法在什么任务上取得了什么性能，这些性能是否支持其目标：GES 在原理性 1D 设置和逼真的 3D 场景中都得到了理论和经验验证。结果表明，它可以更准确地表示具有锐利边缘的信号，而这对于高斯函数来说通常具有挑战性，因为它们具有固有的低通特性。实证分析表明，GES 在拟合自然出现的信号（例如，正方形、三角形、抛物线信号）方面优于高斯函数，从而减少了增加高斯散射内存占用率的广泛分裂操作的需要。在频率调制损失的帮助下，GES 在新视图合成基准测试中取得了具有竞争力的性能，同时所需的内存存储量不到高斯散射的一半，并且渲染速度提高了 39%。</li></ol><p><methods>:(1): GES使用广义指数函数（具有额外的可学习形状参数）来建模3D场景，从而可以减少表示场景所需的粒子数量，从而在效率上明显优于高斯散射方法，并且可以即插即用地替换基于高斯的实用工具。(2): GES在原理性1D设置和逼真的3D场景中都得到了理论和经验验证。结果表明，它可以更准确地表示具有锐利边缘的信号，而这对于高斯函数来说通常具有挑战性，因为它们具有固有的低通特性。(3): 实证分析表明，GES在拟合自然出现的信号（例如，正方形、三角形、抛物线信号）方面优于高斯函数，从而减少了增加高斯散射内存占用率的广泛分裂操作的需要。(4): 在频率调制损失的帮助下，GES在新视图合成基准测试中取得了具有竞争力的性能，同时所需的内存存储量不到高斯散射的一半，并且渲染速度提高了39%。</methods></p><ol><li>结论：(1): 本文提出了一种新的光场渲染方法 GES，它使用广义指数函数来建模 3D 场景，从而可以减少表示场景所需的粒子数量，从而在效率上明显优于高斯散射方法，并且可以即插即用地替换基于高斯的实用工具。(2): 创新点：</li><li>GES 使用广义指数函数来建模 3D 场景，从而可以减少表示场景所需的粒子数量，从而在效率上明显优于高斯散射方法。</li><li>GES 在原理性 1D 设置和逼真的 3D 场景中都得到了理论和经验验证。结果表明，它可以更准确地表示具有锐利边缘的信号，而这对于高斯函数来说通常具有挑战性，因为它们具有固有的低通特性。</li><li>实证分析表明，GES 在拟合自然出现的信号（例如，正方形、三角形、抛物线信号）方面优于高斯函数，从而减少了增加高斯散射内存占用率的广泛分裂操作的需要。</li><li>在频率调制损失的帮助下，GES 在新视图合成基准测试中取得了具有竞争力的性能，同时所需的内存存储量不到高斯散射的一半，并且渲染速度提高了 39%。性能：</li><li>GES 在原理性 1D 设置和逼真的 3D 场景中都得到了理论和经验验证。结果表明，它可以更准确地表示具有锐利边缘的信号，而这对于高斯函数来说通常具有挑战性，因为它们具有固有的低通特性。</li><li>实证分析表明，GES 在拟合自然出现的信号（例如，正方形、三角形、抛物线信号）方面优于高斯函数，从而减少了增加高斯散射内存占用率的广泛分裂操作的需要。</li><li>在频率调制损失的帮助下，GES 在新视图合成基准测试中取得了具有竞争力的性能，同时所需的内存存储量不到高斯散射的一半，并且渲染速度提高了 39%。工作量：</li><li>GES 的实现相对简单，并且可以很容易地集成到现有的光场渲染工具中。</li><li>GES 的训练过程相对较快，并且可以在几分钟内完成。</li><li>GES 的渲染速度很快，并且可以在几秒钟内生成高质量的图像。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-06e50cf8fcf2b71cc6d5f5fa60bd416c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e0387aa41ca3382d21ca4822a1185a81.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d98ce6f15593a9709f1a7d0a0c108a7f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4903d39957be51dd29a4222bcccefaa4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c50bfcbaec1420bcb70374001db6c443.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e090b0178d5a97f88600cc386571b770.jpg" align="middle"></details><h2 id="GS-CLIP-Gaussian-Splatting-for-Contrastive-Language-Image-3D-Pretraining-from-Real-World-Data"><a href="#GS-CLIP-Gaussian-Splatting-for-Contrastive-Language-Image-3D-Pretraining-from-Real-World-Data" class="headerlink" title="GS-CLIP: Gaussian Splatting for Contrastive Language-Image-3D   Pretraining from Real-World Data"></a>GS-CLIP: Gaussian Splatting for Contrastive Language-Image-3D   Pretraining from Real-World Data</h2><p><strong>Authors:Haoyuan Li, Yanpeng Zhou, Yihan Zeng, Hang Xu, Xiaodan Liang</strong></p><p>3D Shape represented as point cloud has achieve advancements in multimodal pre-training to align image and language descriptions, which is curial to object identification, classification, and retrieval. However, the discrete representations of point cloud lost the object’s surface shape information and creates a gap between rendering results and 2D correspondences. To address this problem, we propose GS-CLIP for the first attempt to introduce 3DGS (3D Gaussian Splatting) into multimodal pre-training to enhance 3D representation. GS-CLIP leverages a pre-trained vision-language model for a learned common visual and textual space on massive real world image-text pairs and then learns a 3D Encoder for aligning 3DGS optimized per object. Additionally, a novel Gaussian-Aware Fusion is proposed to extract and fuse global explicit feature. As a general framework for language-image-3D pre-training, GS-CLIP is agnostic to 3D backbone networks. Experiments on challenging shows that GS-CLIP significantly improves the state-of-the-art, outperforming the previously best results. </p><p><a href="http://arxiv.org/abs/2402.06198v2">PDF</a> The content of the technical report needs to be updated and retracted   to avoid other impacts</p><p><strong>摘要</strong><br>利用3DGS(三维高斯渲染)增强3D表现，以进行多模态预训练，提升图像、语言和三维数据的对齐，改善物体识别、分类和检索任务。</p><p><strong>要点</strong></p><ul><li>利用点云表示的3D形状在图像和语言描述的对齐上取得了多模态预训练的进步，这对于物体识别、分类和检索至关重要。</li><li>点云的离散表示丢失了物体的曲面形状信息，并在渲染结果和2D对应关系之间制造差距。</li><li>提出GS-CLIP，首次尝试将3DGS（三维高斯渲染）引入多模态预训练，以增强3D表示。</li><li>GS-CLIP利用预训练的视觉-语言模型，在大量真实世界图像-文本对上学习一个通用的视觉和文本空间，然后学习一个针对每个物体优化3DGS的3D编码器。</li><li>提出了一种新的高斯感知融合来提取和融合全局显式特征。</li><li>作为语言-图像-3D预训练的通用框架，GS-CLIP独立于3D骨干网络。</li><li>具有挑战性的实验表明，GS-CLIP显著优于最先进的技术，超越了以前最好的成果。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：GS-CLIP：用于对比语言-图像-3D 预训练的高斯喷绘</li><li>作者：李昊源、周雁鹏、曾义涵、徐航、梁晓丹</li><li>第一作者单位：深圳大学</li><li>关键词：3D 表示、对比学习、多模态预训练、高斯喷绘</li><li>论文链接：https://arxiv.org/abs/2402.06198，Github 代码链接：无</li><li><p>摘要：（1）研究背景：3D 形状表示为点云在多模态预训练中取得了进展，可以对齐图像和语言描述，这对物体识别、分类和检索至关重要。然而，点云的离散表示丢失了物体的表面形状信息，并在渲染结果和 2D 对应关系之间产生了差距。（2）过去的方法及其问题：现有方法主要集中在对点云进行建模，但这些方法通常会丢失物体的几何信息和形状纹理。此外，现有方法通常需要大量的数据，这使得它们难以应用于现实世界中的场景。（3）提出的研究方法：为了解决上述问题，本文提出了一种新的框架 GS-CLIP，该框架将 3D 高斯喷绘 (3DGS) 引入多模态预训练中，以增强 3D 表示。GS-CLIP 利用预训练的视觉语言模型在大量真实世界图像-文本对上学习一个共同的视觉和文本空间，然后学习一个 3D 编码器来对齐针对每个对象优化的 3DGS。此外，本文还提出了一种新的高斯感知融合方法，用于提取和融合全局显式特征。（4）方法在任务和性能上的表现：在 SUN-RGBD 数据集上的实验表明，GS-CLIP 在真实世界环境中的零样本/开放世界学习中取得了最先进的性能。这些结果表明，3DGS 在跨模态学习中具有强大的表示能力。</p></li><li><p>方法：（1）跨模态预训练：利用预训练的语言-图像模型CLIP，为文本、图像和3DGS建立共同的语言-图像潜在空间，作为3DGS的目标潜在空间。（2）语言-3DGS对齐和图像-3DGS对齐：分别使用对比损失函数来对齐文本与3DGS、图像与3DGS的特征表示。（3）高斯感知融合：采用基于Transformer的分支直接对高斯特征进行建模，并将其与残差形式注入到3D主干网络中。</p></li><li><p>结论：（1）：本工作首次将 3DGS 纳入跨模态学习，作为补充形状和纹理信息的通用 3D 表示。为此，提出了一种高斯感知融合，以便更好地从补充信息中学习信息。我们证明了我们提出的 GS-CLIP 在最先进的方法中取得了优异的性能，并在真实世界环境中实现了零样本/开放世界学习的最新性能。（2）：创新点：</p></li><li>将 3DGS 引入跨模态学习，作为补充形状和纹理信息的通用 3D 表示。</li><li>提出了一种高斯感知融合，以便更好地从补充信息中学习信息。</li><li>在真实世界环境中实现了零样本/开放世界学习的最新性能。性能：</li><li>在 SUN-RGBD 数据集上，GS-CLIP 在真实世界环境中的零样本/开放世界学习中取得了最先进的性能。</li><li>这些结果表明，3DGS 在跨模态学习中具有强大的表示能力。工作量：</li><li>该工作涉及到大量的数据预处理和模型训练。</li><li>需要对 3DGS 进行优化，以使其能够更好地对齐文本和图像的特征表示。</li><li>需要对高斯感知融合进行进一步的研究，以使其能够更好地提取和融合全局显式特征。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-5ca02e3188a2350914f961c6e31c0616.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4980273838b01e0c94c7593c3becb878.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b33d684beebaf5252e0357a0e0af9c1d.jpg" align="middle"></details><h2 id="GaMeS-Mesh-Based-Adapting-and-Modification-of-Gaussian-Splatting"><a href="#GaMeS-Mesh-Based-Adapting-and-Modification-of-Gaussian-Splatting" class="headerlink" title="GaMeS: Mesh-Based Adapting and Modification of Gaussian Splatting"></a>GaMeS: Mesh-Based Adapting and Modification of Gaussian Splatting</h2><p><strong>Authors:Joanna Waczyńska, Piotr Borycki, Sławomir Tadeja, Jacek Tabor, Przemysław Spurek</strong></p><p>Recently, a range of neural network-based methods for image rendering have been introduced. One such widely-researched neural radiance field (NeRF) relies on a neural network to represent 3D scenes, allowing for realistic view synthesis from a small number of 2D images. However, most NeRF models are constrained by long training and inference times. In comparison, Gaussian Splatting (GS) is a novel, state-of-the-art technique for rendering points in a 3D scene by approximating their contribution to image pixels through Gaussian distributions, warranting fast training and swift, real-time rendering. A drawback of GS is the absence of a well-defined approach for its conditioning due to the necessity to condition several hundred thousand Gaussian components. To solve this, we introduce the Gaussian Mesh Splatting (GaMeS) model, which allows modification of Gaussian components in a similar way as meshes. We parameterize each Gaussian component by the vertices of the mesh face. Furthermore, our model needs mesh initialization on input or estimated mesh during training. We also define Gaussian splats solely based on their location on the mesh, allowing for automatic adjustments in position, scale, and rotation during animation. As a result, we obtain a real-time rendering of editable GS. </p><p><a href="http://arxiv.org/abs/2402.01459v3">PDF</a> </p><p><strong>Summary:</strong><br>神经辐射场 (NeRF) 是一种用于图像渲染的神经网络方法，而高斯网格泼溅 (GaMeS) 模型则通过高斯分布来估算 3D 场景中点的贡献，从而实现快速训练和实时渲染。</p><p><strong>Key Takeaways:</strong></p><ul><li>利用神经网络表征 3D 场景的 NeRF，允许从少量 2D 图像中进行逼真的视点合成。</li><li>高斯泼溅 (GS) 通过高斯分布来估算 3D 场景中点的贡献，从而实现快速训练和实时渲染。</li><li>GaMeS 模型允许以与网格类似的方式修改高斯分量，从而为 GS 的调节提供了一个明确的方法。</li><li>将每个高斯分量参数化为网格面的顶点，这使得 GaMeS 模型可以对 GS 进行实时渲染。</li><li>GaMeS 模型需要在输入时初始化网格或在训练期间估计网格。</li><li>根据其在网格上的位置定义高斯泼溅，从而允许在动画期间自动调整位置、缩放和旋转。</li><li>GaMeS 模型可以实现可编辑 GS 的实时渲染。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：GaMeS：基于网格的自适应和修改高斯喷绘</li><li>作者：Joanna Waczyńska、Piotr Borycki、Sławomir Tadeja、Jacek Tabor、Przemysław Spurek</li><li>第一作者单位：雅盖隆大学数学与计算机科学学院，波兰克拉科夫</li><li>关键词：高斯喷绘、神经辐射场、神经渲染、网格、实时渲染</li><li>论文链接：https://arxiv.org/abs/2402.01459，Github 代码链接：无</li><li>摘要：(1)：研究背景：近年来，基于神经网络的图像渲染方法取得了很大进展，其中神经辐射场（NeRF）是一种流行的方法，它使用神经网络来表示 3D 场景，并能够从少量 2D 图像中合成逼真的视图。然而，大多数 NeRF 模型都受到训练和推理时间长的限制。与之相比，高斯喷绘（GS）是一种新颖的、最先进的技术，它通过高斯分布来近似点对图像像素的贡献，从而渲染 3D 场景中的点，具有快速训练和快速实时渲染的能力。(2)：过去的方法和问题：GS 的一个缺点是缺乏明确的调节方法，因为需要调节数十万个高斯分量。为了解决这个问题，本文介绍了高斯网格喷绘（GaMeS）模型，它允许像修改网格一样修改高斯分量。我们将每个高斯分量参数化为网格面的顶点。此外，我们的模型需要在输入或训练期间估计的网格上进行网格初始化。我们还定义了仅基于其在网格上的位置的高斯喷绘，允许在动画期间自动调整位置、比例和旋转。因此，我们获得了可编辑 GS 的实时渲染。(3)：研究方法：我们提出了 GaMeS 模型，它允许像修改网格一样修改高斯分量。我们将每个高斯分量参数化为网格面的顶点。此外，我们的模型需要在输入或训练期间估计的网格上进行网格初始化。我们还定义了仅基于其在网格上的位置的高斯喷绘，允许在动画期间自动调整位置、比例和旋转。(4)：方法的性能：实验结果表明，GaMeS 模型能够在保持高质量渲染的同时，实现实时修改和适应高斯喷绘。这使得 GaMeS 成为交互式应用程序和游戏中的一个有前景的技术。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：（1）GaMeS 允许实时修改，但对于具有大面的网格，在发生重大变化的情况下会出现伪影。在实践中，大面应该被分成更小的面。当网格面分裂时如何在 GaMeS 中更改高斯分量尚不清楚。（2）创新点：GaMeS 提出了一种新的基于网格的自适应和修改高斯喷绘模型，该模型允许像修改网格一样修改高斯分量，从而实现了实时修改和适应高斯喷绘。性能：实验结果表明，GaMeS 模型能够在保持高质量渲染的同时，实现实时修改和适应高斯喷绘。这使得 GaMeS 成为交互式应用程序和游戏中的一个有前景的技术。工作量：GaMeS 模型需要在输入或训练期间估计网格，这可能会增加模型的训练和推理时间。此外，GaMeS 模型需要对网格进行修改，这可能会增加模型的修改时间。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-11676aa94eeb837bc5149bf9038274ae.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3d3c20ac78640d356ea03699146c96e9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4070017cd795fd8699e30a356efae899.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0416310a796f7ec70150342ac59ffe37.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6eb0975a0f5d702a6daef3f78e530869.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9fb0edd088d9a64e792369a6d6a72979.jpg" align="middle"><img src="https://pica.zhimg.com/v2-dd54f927f26f28fdcefe778d566087c5.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS 方向最新论文已更新，请持续关注 Update in 2024-02-23  Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering   of 3D Gaussian Splatting</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
</feed>
