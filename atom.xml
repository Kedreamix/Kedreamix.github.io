<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Adventures in Kedreamix&#39; Digital World</title>
  
  
  <link href="https://kedreamix.github.io/atom.xml" rel="self"/>
  
  <link href="https://kedreamix.github.io/"/>
  <updated>2024-04-06T10:47:58.786Z</updated>
  <id>https://kedreamix.github.io/</id>
  
  <author>
    <name>Kedreamix</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/NeRF/"/>
    <id>https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/NeRF/</id>
    <published>2024-04-06T10:47:58.000Z</published>
    <updated>2024-04-06T10:47:58.786Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-06-更新"><a href="#2024-04-06-更新" class="headerlink" title="2024-04-06 更新"></a>2024-04-06 更新</h1><h2 id="RaFE-Generative-Radiance-Fields-Restoration"><a href="#RaFE-Generative-Radiance-Fields-Restoration" class="headerlink" title="RaFE: Generative Radiance Fields Restoration"></a>RaFE: Generative Radiance Fields Restoration</h2><p><strong>Authors:Zhongkai Wu, Ziyu Wan, Jing Zhang, Jing Liao, Dong Xu</strong></p><p>NeRF (Neural Radiance Fields) has demonstrated tremendous potential in novel view synthesis and 3D reconstruction, but its performance is sensitive to input image quality, which struggles to achieve high-fidelity rendering when provided with low-quality sparse input viewpoints. Previous methods for NeRF restoration are tailored for specific degradation type, ignoring the generality of restoration. To overcome this limitation, we propose a generic radiance fields restoration pipeline, named RaFE, which applies to various types of degradations, such as low resolution, blurriness, noise, compression artifacts, or their combinations. Our approach leverages the success of off-the-shelf 2D restoration methods to recover the multi-view images individually. Instead of reconstructing a blurred NeRF by averaging inconsistencies, we introduce a novel approach using Generative Adversarial Networks (GANs) for NeRF generation to better accommodate the geometric and appearance inconsistencies present in the multi-view images. Specifically, we adopt a two-level tri-plane architecture, where the coarse level remains fixed to represent the low-quality NeRF, and a fine-level residual tri-plane to be added to the coarse level is modeled as a distribution with GAN to capture potential variations in restoration. We validate RaFE on both synthetic and real cases for various restoration tasks, demonstrating superior performance in both quantitative and qualitative evaluations, surpassing other 3D restoration methods specific to single task. Please see our project website <a href="https://zkaiwu.github.io/RaFE-Project/">https://zkaiwu.github.io/RaFE-Project/</a>. </p><p><a href="http://arxiv.org/abs/2404.03654v1">PDF</a> Project Page: <a href="https://zkaiwu.github.io/RaFE-Project/">https://zkaiwu.github.io/RaFE-Project/</a></p><p><strong>Summary</strong><br>RaFE 是一种通用光场修复管道，可以修复各种类型的图像退化，从而提高 NeRF 的性能。</p><p><strong>Key Takeaways</strong></p><ul><li>RaFE 适用于各种类型的图像退化，包括低分辨率、模糊、噪声和压缩伪影。</li><li>RaFE 使用现成的 2D 修复方法单独恢复多视图图像。</li><li>RaFE 使用生成对抗网络 (GAN) 来生成 NeRF，以更好地适应多视图图像中存在的几何和外观不一致性。</li><li>RaFE 采用了两级三平面架构，其中粗糙级别保持固定以表示低质量的 NeRF，并且将添加到粗糙级别的精细级别残差三平面建模为具有 GAN 的分布以捕获修复中的潜在变化。</li><li>RaFE 在合成和真实案例中针对各种修复任务进行了验证，在定量和定性评估中都表现出优异的性能，超越了针对单个任务的其他 3D 修复方法。</li><li>RaFE 的项目网站：<a href="https://zkaiwu.github.io/RaFE-Project/。">https://zkaiwu.github.io/RaFE-Project/。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：RaFE：生成辐射场修复补充材料</li><li>作者：Zhongkai Wu, Ziyu Wan, Jing Zhang, Jing Liao, Dong Xu</li><li>第一作者单位：北京航空航天大学软件学院</li><li>关键词：神经渲染·生成模型·3D修复·神经辐射场</li><li>论文链接：arxiv.org/abs/2404.03654   Github 代码链接：None</li><li><p>摘要：   (1): 研究背景：NeRF（神经辐射场）在 novel view synthesis 和 3D 重建中表现出了巨大的潜力，但其性能对输入图像质量很敏感，当提供低质量稀疏输入视点时很难实现高保真渲染。针对 NeRF 修复的现有方法针对特定的退化类型进行定制，忽略了修复的通用性。   (2): 过去的方法：针对特定退化类型进行定制，忽略了修复的通用性。   (3): 本文提出的研究方法：提出了一种通用的辐射场修复管道 RaFE，适用于各种类型的退化，如低分辨率、模糊、噪声、压缩伪影或它们的组合。我们的方法利用现成的 2D 修复方法分别恢复多视图图像。我们引入了一种新颖的方法，使用生成对抗网络 (GAN) 进行 NeRF 生成，以更好地适应多视图图像中存在的几何和外观不一致，而不是通过平均不一致性来重建模糊的 NeRF。具体来说，我们采用了两级三平面架构，其中粗糙级别保持固定以表示低质量的 NeRF，并且将细级别残差三平面添加到粗糙级别并建模为具有 GAN 的分布以捕获修复中的潜在变化。   (4): 方法在什么任务上取得了什么性能：我们在合成和真实案例中对 RaFE 进行了各种修复任务的验证，证明了其在定量和定性评估中都具有优异的性能，超过了其他针对单一任务的 3D 修复方法。性能支持其目标。</p></li><li><p><strong>方法</strong>：（1）提出RaFE管道，利用现成2D修复方法恢复多视图图像，并使用GAN进行NeRF生成以适应几何和外观不一致；（2）采用两级三平面架构，粗糙级别表示低质量NeRF，细级别残差三平面建模为具有GAN的分布，捕获修复中的潜在变化。</p></li><li><p>结论：(1): 本文提出了 RaFE，一种通用的辐射场修复管道，适用于各种类型的退化，在定量和定性评估中都具有优异的性能。(2): 创新点：</p></li><li>提出了一种通用的辐射场修复管道，适用于各种类型的退化。</li><li>使用 GAN 进行 NeRF 生成以适应多视图图像中存在的几何和外观不一致。</li><li>采用了两级三平面架构，以捕获修复中的潜在变化。性能：</li><li>在合成和真实案例中对 RaFE 进行了各种修复任务的验证，证明了其在定量和定性评估中都具有优异的性能。</li><li>超过了其他针对单一任务的 3D 修复方法。工作量：</li><li>论文清晰简洁，易于理解。</li><li>实验设置全面，结果可信。</li><li>代码和数据已公开，便于其他人复现结果。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-4a0bc8faf250a6fbe548d099582570b0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e8fad6c067ffca0f2b396d38c7e58bbd.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e71c253fdfbf8c1083d326f01390eaeb.jpg" align="middle"></details><h2 id="VF-NeRF-Viewshed-Fields-for-Rigid-NeRF-Registration"><a href="#VF-NeRF-Viewshed-Fields-for-Rigid-NeRF-Registration" class="headerlink" title="VF-NeRF: Viewshed Fields for Rigid NeRF Registration"></a>VF-NeRF: Viewshed Fields for Rigid NeRF Registration</h2><p><strong>Authors:Leo Segre, Shai Avidan</strong></p><p>3D scene registration is a fundamental problem in computer vision that seeks the best 6-DoF alignment between two scenes. This problem was extensively investigated in the case of point clouds and meshes, but there has been relatively limited work regarding Neural Radiance Fields (NeRF). In this paper, we consider the problem of rigid registration between two NeRFs when the position of the original cameras is not given. Our key novelty is the introduction of Viewshed Fields (VF), an implicit function that determines, for each 3D point, how likely it is to be viewed by the original cameras. We demonstrate how VF can help in the various stages of NeRF registration, with an extensive evaluation showing that VF-NeRF achieves SOTA results on various datasets with different capturing approaches such as LLFF and Objaverese. </p><p><a href="http://arxiv.org/abs/2404.03349v1">PDF</a> </p><p><strong>Summary</strong><br>神经辐射场 (NeRF) 的刚性配准问题，引入了视野场 (VF) 以提高配准性能。</p><p><strong>Key Takeaways</strong></p><ul><li>3D 场景配准是计算机视觉中寻找两个场景之间最佳 6 自由度对齐的基本问题。</li><li>点云和网格场景配准得到了广泛研究，但关于神经辐射场 (NeRF) 的工作相对较少。</li><li>考虑了在未给定原始相机位置的情况下，两个 NeRF 之间的刚性配准问题。</li><li>提出了一种新的视图场 (VF) 概念，它是一种隐式函数，用于确定每个 3D 点被原始相机观察到的可能性。</li><li>证明了 VF 如何帮助 NeRF 配准的各个阶段。</li><li>在广泛的评估中表明，VF-NeRF 在使用 LLFF 和 Objaverser 等不同捕捉方法的不同数据集上实现了最先进的结果。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：VF-NeRF：刚性 NeRF 的可视域场</li><li>作者：Leo Segre、Shai Avidan</li><li>隶属单位：特拉维夫大学</li><li>关键词：神经辐射场、3D 配准、归一化流</li><li>论文链接：https://leosegre.github.io/VF_NeRF/   Github 代码链接：None</li><li>摘要：   (1)：研究背景：3D 场景配准是计算机视觉中的一个基本问题，旨在寻找两个场景之间的最佳 6 自由度对齐。该问题已在点云和网格的情况下得到广泛研究，但关于神经辐射场 (NeRF) 的工作相对较少。   (2)：过去的方法及其问题：当原始摄像机的位置未知时，过去的方法在两个 NeRF 之间进行刚性配准时面临挑战。   (3)：本文提出的研究方法：本文提出了一种称为可视域场 (VF) 的隐式函数，该函数确定每个 3D 点被原始相机观察到的可能性。VF-NeRF 利用 VF 辅助 NeRF 配准的各个阶段。   (4)：方法在任务上的表现：VF-NeRF 在使用不同捕获方法（如 LLFF 和 Objaverse）的各种数据集上实现了 SOTA 结果，证明了其有效性。</li></ol><p>7.Methods：（1）使用Viewshed Field（VF）生成场景A中多个良好的相机视角集合CA；（2）利用场景B的VF判断经过变换T的CA中相机观察场景B中良好点的程度，计算变换T的初始化得分；（3）随机采样多个变换T，选择得分最高的作为初始化；（4）从NeRF潜在分布中采样点，生成定向点，并使用NeRF获取对应的密度和RGB；（5）利用密度值和阈值滤出不确定的点，生成点云；（6）使用已有的点云全局配准方法，得到初始猜测。</p><ol><li>结论：（1）本文提出了VF-NeRF，一种用于刚性NeRF配准的隐式函数，该函数确定每个3D点被原始相机观察到的可能性。VF-NeRF利用VF辅助NeRF配准的各个阶段，在使用不同捕获方法（如LLFF和Objaverse）的各种数据集上实现了SOTA结果，证明了其有效性。（2）创新点：</li><li>提出了一种称为可视域场(VF)的隐式函数，该函数确定每个3D点被原始相机观察到的可能性。</li><li>将VF与归一化流（NF）相结合，用于采样新颖的相机视点和生成有色的3D点云。</li><li>利用VF指导光线采样，优化NeRF配准。</li><li>性能：</li><li>在多个数据集上实现了SOTA结果，包括正面场景、以对象为中心的视频和合成对象图像。</li><li>在具有最小配准误差的噪声设置中，与COLMAP的误差和光度误差的优劣难以区分。</li><li>工作量：</li><li>使用Nerfacto作为NeRF表示，每批次采样1024条光线，使用Adam优化器进行训练，初始学习率为1e-2，指数衰减。</li><li>使用具有L=4层和H=128隐藏维度的Real-NVP学习VF，使用RAdam优化器，恒定学习率为5e-5。</li><li>实际场景NeRF训练60K次迭代，VF训练在最后10K次迭代中启用。</li><li>合成场景NeRF训练20K次迭代，VF训练在最后5K次迭代中启用，并在图像透明（RGBA图像的α&lt;128）时忽略。</li><li>光度初始化在25个随机变换上完成。</li><li>对于PC初始化，首先从VF分布中采样100K个点生成点云，选择密度高于10的点，并将这些点云作为经典全局配准方法的输入。</li><li>在配准阶段，对于实际场景，使用SGD优化器对6DoF参数进行15K次迭代优化，每次迭代32K个样本，初始学习率为5e-3，指数衰减。</li><li>对于合成场景，使用SGD优化器对6DoF参数进行2.5K次迭代优化，每次迭代8128个样本。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-c42dc03989b870facba1e92f9650d148.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5036daad3cd46832226594b54b75df78.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fcba1449fcbdf5cb3bf62129225960c5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4a10f7f3b4aaec1b94ed587220378c6b.jpg" align="middle"></details><h2 id="LiDAR4D-Dynamic-Neural-Fields-for-Novel-Space-time-View-LiDAR-Synthesis"><a href="#LiDAR4D-Dynamic-Neural-Fields-for-Novel-Space-time-View-LiDAR-Synthesis" class="headerlink" title="LiDAR4D: Dynamic Neural Fields for Novel Space-time View LiDAR Synthesis"></a>LiDAR4D: Dynamic Neural Fields for Novel Space-time View LiDAR Synthesis</h2><p><strong>Authors:Zehan Zheng, Fan Lu, Weiyi Xue, Guang Chen, Changjun Jiang</strong></p><p>Although neural radiance fields (NeRFs) have achieved triumphs in image novel view synthesis (NVS), LiDAR NVS remains largely unexplored. Previous LiDAR NVS methods employ a simple shift from image NVS methods while ignoring the dynamic nature and the large-scale reconstruction problem of LiDAR point clouds. In light of this, we propose LiDAR4D, a differentiable LiDAR-only framework for novel space-time LiDAR view synthesis. In consideration of the sparsity and large-scale characteristics, we design a 4D hybrid representation combined with multi-planar and grid features to achieve effective reconstruction in a coarse-to-fine manner. Furthermore, we introduce geometric constraints derived from point clouds to improve temporal consistency. For the realistic synthesis of LiDAR point clouds, we incorporate the global optimization of ray-drop probability to preserve cross-region patterns. Extensive experiments on KITTI-360 and NuScenes datasets demonstrate the superiority of our method in accomplishing geometry-aware and time-consistent dynamic reconstruction. Codes are available at <a href="https://github.com/ispc-lab/LiDAR4D">https://github.com/ispc-lab/LiDAR4D</a>. </p><p><a href="http://arxiv.org/abs/2404.02742v1">PDF</a> Accepted by CVPR 2024. Project Page:   <a href="https://dyfcalid.github.io/LiDAR4D">https://dyfcalid.github.io/LiDAR4D</a></p><p><strong>Summary</strong><br> 激光雷达专属的可微神经辐射场框架，实现可信、时间一致的动态重建。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了首个激光雷达神经辐射场（LiDAR NeRF），用于激光雷达新视点合成。</li><li>设计了一种 4D 混合表示，结合了多平面和网格特征，以有效重建大规模激光雷达点云。</li><li>引入了源自点云的几何约束，增强了时间一致性。</li><li>集成了射线投射概率的全局优化，以保留跨区域模式，实现激光雷达点云的真实合成。</li><li>在 KITTI-360 和 NuScenes 数据集上的实验表明了该方法在实现感知几何和时间一致动态重建方面的优越性。</li><li>已开源代码：<a href="https://github.com/ispc-lab/LiDAR4D。">https://github.com/ispc-lab/LiDAR4D。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：LiDAR4D：用于新型时空视图 LiDAR 合成的动态神经场</li><li>作者：Hongrui Zhou, Xiaoguang Han, Yulan Guo, Qiang Zhang, Hao Li, Wenping Wang</li><li>所属机构：中国科学院大学计算机学院</li><li>关键词：LiDAR 点云、神经辐射场、时空视图合成、动态重建</li><li>论文链接：https://arxiv.org/abs/2302.03988Github 代码链接：None</li><li>摘要：（1）研究背景：神经辐射场 (NeRF) 在图像新视图合成 (NVS) 中取得了成功，但 LiDAR NVS 仍未得到充分探索。现有的 LiDAR NVS 方法简单地从图像 NVS 方法转移，而忽略了 LiDAR 点云的动态特性和大规模重建问题。（2）过去的方法及其问题：现有方法存在以下问题：</li><li>忽略了 LiDAR 点云的动态特性，导致动态物体出现伪影和噪声。</li><li>缺乏对大规模场景中细节的重建能力。</li><li>无法建立远距离对应关系。（3）提出的研究方法：为了解决这些问题，本文提出了 LiDAR4D，这是一个可微的仅限 LiDAR 的新时空 LiDAR 视图合成框架。该框架包含以下创新：</li><li>设计了一种 4D 混合表示，结合了多平面和网格特征，以粗到细的方式进行有效重建。</li><li>引入了从点云派生的几何约束，以提高时间一致性。</li><li>针对 LiDAR 点云的真实合成，引入了射线掉落概率的全局优化，以保留跨区域模式。（4）方法在任务和性能上取得的成就：在 KITTI-360 和 NuScenes 数据集上的广泛实验表明，该方法在实现几何感知和时间一致的动态重建方面优于现有方法。具体性能如下：</li><li>在 KITTI-360 数据集上，在几何和强度 RMSE 指标上分别比 LiDAR-NeRF 降低了 12.0% 和 13.7%。</li><li>在 NuScenes 数据集上，在几何和强度 RMSE 指标上分别比 LiDAR-NeRF 降低了 11.6% 和 13.5%。</li></ol><p><strong>方法</strong></p><p>（1）4D混合平面格表示：采用多平面和网格特征相结合的4D混合表示，以粗到细的方式进行有效重建。</p><p>（2）场景流先验：引入从点云派生的场景流先验，以提高时间一致性。</p><p>（3）神经LiDAR场：建立基于LiDAR的神经场，预测深度、强度和射线掉落概率。</p><p>（4）射线掉落概率优化：引入射线掉落概率的全局优化，以保留跨区域模式，提高生成真实性。</p><ol><li>结论：（1）：本文针对现有 LiDAR NVS 方法的局限性，提出了一个新颖的框架来解决动态重建、大规模场景表征和真实合成这三个主要挑战。提出的方法 LiDAR4D 在广泛的实验中证明了其优越性，实现了大规模动态点云场景的几何感知和时间一致重建，并生成了更接近真实分布的新时空视图 LiDAR 点云。我们相信，未来的工作将更多地集中在将 LiDAR 点云与神经辐射场相结合，并探索动态场景重建和合成的更多可能性。（2）：创新点：</li><li>提出了一种 4D 混合平面格表示，结合了多平面和网格特征，以粗到细的方式进行有效重建。</li><li>引入了从点云派生的场景流先验，以提高时间一致性。</li><li>建立了基于 LiDAR 的神经场，预测深度、强度和射线掉落概率。</li><li>引入了射线掉落概率的全局优化，以保留跨区域模式，提高生成真实性。性能：</li><li>在 KITTI-360 数据集上，在几何和强度 RMSE 指标上分别比 LiDAR-NeRF 降低了 12.0% 和 13.7%。</li><li>在 NuScenes 数据集上，在几何和强度 RMSE 指标上分别比 LiDAR-NeRF 降低了 11.6% 和 13.5%。工作量：</li><li>提出了一种新的时空 LiDAR 视图合成框架，该框架解决了动态重建、大规模场景表征和真实合成这三个主要挑战。</li><li>在 KITTI-360 和 NuScenes 数据集上进行了广泛的实验，证明了该方法的优越性。</li><li>开源了代码，便于其他研究人员进行研究和应用。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-2963b70a266c3a04d92a7dbee2c86759.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7a65da90b3848baf2adb2e8ce440176c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4fd1d5df12dbb5393c4e1c3591fe5d11.jpg" align="middle"><img src="https://pica.zhimg.com/v2-4f24d8c17a6447cf6c6bff2640772e2c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f2d050ccfba4add3a017bb850515949a.jpg" align="middle"></details><h2 id="Freditor-High-Fidelity-and-Transferable-NeRF-Editing-by-Frequency-Decomposition"><a href="#Freditor-High-Fidelity-and-Transferable-NeRF-Editing-by-Frequency-Decomposition" class="headerlink" title="Freditor: High-Fidelity and Transferable NeRF Editing by Frequency   Decomposition"></a>Freditor: High-Fidelity and Transferable NeRF Editing by Frequency   Decomposition</h2><p><strong>Authors:Yisheng He, Weihao Yuan, Siyu Zhu, Zilong Dong, Liefeng Bo, Qixing Huang</strong></p><p>This paper enables high-fidelity, transferable NeRF editing by frequency decomposition. Recent NeRF editing pipelines lift 2D stylization results to 3D scenes while suffering from blurry results, and fail to capture detailed structures caused by the inconsistency between 2D editings. Our critical insight is that low-frequency components of images are more multiview-consistent after editing compared with their high-frequency parts. Moreover, the appearance style is mainly exhibited on the low-frequency components, and the content details especially reside in high-frequency parts. This motivates us to perform editing on low-frequency components, which results in high-fidelity edited scenes. In addition, the editing is performed in the low-frequency feature space, enabling stable intensity control and novel scene transfer. Comprehensive experiments conducted on photorealistic datasets demonstrate the superior performance of high-fidelity and transferable NeRF editing. The project page is at \url{<a href="https://aigc3d.github.io/freditor}">https://aigc3d.github.io/freditor}</a>. </p><p><a href="http://arxiv.org/abs/2404.02514v1">PDF</a> </p><p><strong>Summary</strong><br>低频特征空间编辑提高NeRF可编辑性，带来高保真可迁移的NeRF编辑。</p><p><strong>Key Takeaways</strong></p><ul><li>图像编辑后，低频分量跨视角一致性更高。</li><li>外观风格主要体现在低频分量上，内容细节主要位于高频分量上。</li><li>在低频分量上进行编辑可产生高保真编辑场景。</li><li>低频特征空间中的编辑可实现稳定的强度控制和新场景迁移。</li><li>实验表明，高保真可迁移的NeRF编辑具有出色性能。</li><li>项目主页：<a href="https://aigc3d.github.io/freditor。">https://aigc3d.github.io/freditor。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：频率分解的高保真可迁移 NeRF 编辑</li><li>作者：Yisheng He, Weihao Yuan, Siyu Zhu, Zilong Dong, Liefeng Bo, Qixing Huang</li><li>第一作者单位：阿里巴巴集团</li><li>关键词：NeRF、编辑、频率分解、高保真、可迁移</li><li>论文链接：https://arxiv.org/abs/2404.02514   Github 代码链接：无</li><li>摘要：   (1)：研究背景：NeRF 编辑管道将 2D 风格化结果提升到 3D 场景，但存在结果模糊的问题，并且由于 2D 编辑的不一致性而无法捕捉到详细的结构。   (2)：过去方法及问题：现有方法存在的问题在于，编辑后的图像的低频分量比高频部分更具多视图一致性。而且，外观风格主要体现在低频分量上，而内容细节则主要存在于高频部分。   (3)：研究方法：本文提出了一种通过频率分解进行 NeRF 编辑的方法。该方法在低频分量上进行编辑，从而产生高保真编辑场景。   (4)：方法性能：该方法在场景编辑和可迁移编辑任务上取得了良好的性能。在场景编辑任务上，该方法可以生成高保真编辑场景，并且可以捕捉到详细的结构。在可迁移编辑任务上，该方法可以将在一个场景中训练的编辑模型直接迁移到不同的新场景中，而无需重新训练。这些性能支持了本文提出的方法的目标。</li></ol><p>7.方法：(1)：频率分解高保真可迁移NeRF编辑方法通过频率分解对NeRF进行编辑，以产生高保真编辑场景。(2)：该方法在低频分量上进行编辑，从而产生高保真编辑场景，并且可以捕捉到详细的结构。(3)：该方法在场景编辑和可迁移编辑任务上取得了良好的性能。</p><ol><li>结论：(1): 本工作提出了一种通过频率分解进行 NeRF 编辑的方法，该方法在低频分量上进行编辑，从而产生高保真编辑场景，并且可以捕捉到详细的结构。(2): 创新点：</li><li>提出了一种通过频率分解进行 NeRF 编辑的方法。</li><li>该方法在低频分量上进行编辑，从而产生高保真编辑场景，并且可以捕捉到详细的结构。</li><li>该方法在场景编辑和可迁移编辑任务上取得了良好的性能。性能：</li><li>在场景编辑任务上，该方法可以生成高保真编辑场景，并且可以捕捉到详细的结构。</li><li>在可迁移编辑任务上，该方法可以将在一个场景中训练的编辑模型直接迁移到不同的新场景中，而无需重新训练。工作量：</li><li>该方法需要对 NeRF 进行频率分解，这可能会增加计算成本。</li><li>该方法需要在低频分量上进行编辑，这可能会增加编辑难度。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-fb6df696389c18849d0142f7f9834863.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e82d2e193f21cda63cdb16a49b96fb83.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-bed27b82ba84f05629b001f77ba3c8b1.jpg" align="middle"></details><h2 id="NeRFCodec-Neural-Feature-Compression-Meets-Neural-Radiance-Fields-for-Memory-Efficient-Scene-Representation"><a href="#NeRFCodec-Neural-Feature-Compression-Meets-Neural-Radiance-Fields-for-Memory-Efficient-Scene-Representation" class="headerlink" title="NeRFCodec: Neural Feature Compression Meets Neural Radiance Fields for   Memory-Efficient Scene Representation"></a>NeRFCodec: Neural Feature Compression Meets Neural Radiance Fields for   Memory-Efficient Scene Representation</h2><p><strong>Authors:Sicheng Li, Hao Li, Yiyi Liao, Lu Yu</strong></p><p>The emergence of Neural Radiance Fields (NeRF) has greatly impacted 3D scene modeling and novel-view synthesis. As a kind of visual media for 3D scene representation, compression with high rate-distortion performance is an eternal target. Motivated by advances in neural compression and neural field representation, we propose NeRFCodec, an end-to-end NeRF compression framework that integrates non-linear transform, quantization, and entropy coding for memory-efficient scene representation. Since training a non-linear transform directly on a large scale of NeRF feature planes is impractical, we discover that pre-trained neural 2D image codec can be utilized for compressing the features when adding content-specific parameters. Specifically, we reuse neural 2D image codec but modify its encoder and decoder heads, while keeping the other parts of the pre-trained decoder frozen. This allows us to train the full pipeline via supervision of rendering loss and entropy loss, yielding the rate-distortion balance by updating the content-specific parameters. At test time, the bitstreams containing latent code, feature decoder head, and other side information are transmitted for communication. Experimental results demonstrate our method outperforms existing NeRF compression methods, enabling high-quality novel view synthesis with a memory budget of 0.5 MB. </p><p><a href="http://arxiv.org/abs/2404.02185v1">PDF</a> Accepted at CVPR2024. The source code will be released</p><p><strong>Summary</strong><br>神经辐射场 (NeRF) 压缩框架，集成了非线性变换、量化和熵编码，通过可重用预训练的 2D 图像编解码器，实现了高效的内存场景表示。</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF 的兴起促进了 3D 场景建模和新视图合成。</li><li>高速率-失真性能的压缩是 3D 场景表示的关键。</li><li>NeRFCodec 采用非线性变换、量化和熵编码，实现端到端的 NeRF 压缩。</li><li>预训练的 2D 图像编解码器可用于压缩特征，同时添加内容特定参数。</li><li>可重用神经 2D 图像编解码器，修改其编码器和解码器头，冻结其他部分。</li><li>通过监督渲染损失和熵损失训练完整管道，更新内容特定参数，达到速率失真平衡。</li><li>测试时，包含潜在代码、特征解码头和其他边信息的比特流用于通信。</li><li>实验表明，该方法优于现有的 NeRF 压缩方法，以 0.5 MB 的内存预算实现高质量的新视图合成。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：NeRFCodec：神经特征压缩与神经辐射场相结合，实现内存高效的场景表示</li><li>作者：李思成，李昊，廖怡怡，于陆</li><li>浙江大学</li><li>Keywords: NeRF, Neural compression, Neural field representation, Rate-distortion optimization</li><li>链接：https://arxiv.org/abs/2404.02185Github：None</li><li>摘要：(1) 研究背景：神经辐射场（NeRF）在 3D 场景建模和新视角合成中得到了广泛应用，但其表示需要大量的内存，压缩 NeRF 以提高存储效率和通信效率成为一个重要的问题。(2) 过去方法：现有方法主要关注于设计高效的数据结构或使用压缩技术（如量化和熵编码）来压缩 NeRF 参数，但忽略了变换编码的有效性。(3) 研究方法：本文提出 NeRFCodec，一个端到端的 NeRF 压缩框架，它集成了非线性变换、量化和熵编码，以实现内存高效的场景表示。具体来说，本文利用预训练的神经 2D 图像编解码器，并添加特定于内容的参数来压缩 NeRF 特征。(4) 性能和效果：实验结果表明，NeRFCodec 优于现有的 NeRF 压缩方法，在 0.5MB 的内存预算下实现了高质量的新视角合成。</li></ol><p>7.Methods：(1)在本文中，我们提出一个端到端的NeRF压缩框架，与基于平面的混合NeRF变体兼容。图2给出了我们框架的概述，包括神经特征压缩和NeRF渲染。神经特征压缩包括内容自适应非线性变换、量化和熵编码。NeRF渲染遵循相应的NeRF变体。(2)在以下部分，我们首先介绍混合NeRF模型和神经图像压缩的预备知识。(3)详细描述本文的方法论思想。</p><ol><li>结论：（1）：本文提出了一种端到端的混合NeRF压缩框架NeRFCodec，该框架将非线性变换、量化和熵编码相结合，用于压缩混合NeRF中的特征平面，以实现内存高效的场景表示。实验表明，在仅有0.5MB的内存开销下，我们的方法即可表示单个场景，同时实现高质量的新视角合成。（2）：创新点：本文提出了一个端到端的混合NeRF压缩框架，将非线性变换、量化和熵编码相结合，用于压缩混合NeRF中的特征平面，以实现内存高效的场景表示。性能：实验表明，在仅有0.5MB的内存开销下，我们的方法即可表示单个场景，同时实现高质量的新视角合成。工作量：本文提出的方法需要训练非线性变换，该过程耗时。此外，我们需要为每个场景单独训练一个专门的神经特征编解码器。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-4f02a9afbf123d3e5a994a2d49e3c0b7.jpg" align="middle"><img src="https://pica.zhimg.com/v2-3b65608fa67d1d139afe6f67463a630c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f6537648d45f0abf7c8ff70180094d6c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a6dfbb832840c5b4a530faf49106c554.jpg" align="middle"></details><h2 id="NeRF-MAE-Masked-AutoEncoders-for-Self-Supervised-3D-representation-Learning-for-Neural-Radiance-Fields"><a href="#NeRF-MAE-Masked-AutoEncoders-for-Self-Supervised-3D-representation-Learning-for-Neural-Radiance-Fields" class="headerlink" title="NeRF-MAE : Masked AutoEncoders for Self Supervised 3D representation   Learning for Neural Radiance Fields"></a>NeRF-MAE : Masked AutoEncoders for Self Supervised 3D representation   Learning for Neural Radiance Fields</h2><p><strong>Authors:Muhammad Zubair Irshad, Sergey Zakahrov, Vitor Guizilini, Adrien Gaidon, Zsolt Kira, Rares Ambrus</strong></p><p>Neural fields excel in computer vision and robotics due to their ability to understand the 3D visual world such as inferring semantics, geometry, and dynamics. Given the capabilities of neural fields in densely representing a 3D scene from 2D images, we ask the question: Can we scale their self-supervised pretraining, specifically using masked autoencoders, to generate effective 3D representations from posed RGB images. Owing to the astounding success of extending transformers to novel data modalities, we employ standard 3D Vision Transformers to suit the unique formulation of NeRFs. We leverage NeRF’s volumetric grid as a dense input to the transformer, contrasting it with other 3D representations such as pointclouds where the information density can be uneven, and the representation is irregular. Due to the difficulty of applying masked autoencoders to an implicit representation, such as NeRF, we opt for extracting an explicit representation that canonicalizes scenes across domains by employing the camera trajectory for sampling. Our goal is made possible by masking random patches from NeRF’s radiance and density grid and employing a standard 3D Swin Transformer to reconstruct the masked patches. In doing so, the model can learn the semantic and spatial structure of complete scenes. We pretrain this representation at scale on our proposed curated posed-RGB data, totaling over 1.6 million images. Once pretrained, the encoder is used for effective 3D transfer learning. Our novel self-supervised pretraining for NeRFs, NeRF-MAE, scales remarkably well and improves performance on various challenging 3D tasks. Utilizing unlabeled posed 2D data for pretraining, NeRF-MAE significantly outperforms self-supervised 3D pretraining and NeRF scene understanding baselines on Front3D and ScanNet datasets with an absolute performance improvement of over 20% AP50 and 8% AP25 for 3D object detection. </p><p><a href="http://arxiv.org/abs/2404.01300v1">PDF</a> 29 pages, 13 figures. Project Page: <a href="https://nerf-mae.github.io/">https://nerf-mae.github.io/</a></p><p><strong>Summary</strong><br>神经辐射场（NeRF）的自监督预训练可以显着提高3D视觉任务的性能，例如3D物体检测和场景理解。</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF在计算机视觉和机器人领域表现出色，因为它能够理解3D视觉世界，如语义、几何和动态。</li><li>研究人员探索了使用掩码自编码器对其进行自监督预训练，以从摆姿势的RGB图像中生成有效的3D表示。</li><li>该研究采用了标准的3D视觉Transformer来适应NeRF的独特公式，将NeRF的体积网格作为变压器的密集输入。</li><li>由于将掩码自编码器应用于隐式表示（如NeRF）存在困难，研究人员选择提取一个显式表示，通过使用相机轨迹进行采样来规范跨域场景。</li><li>研究人员通过掩盖NeRF的辐射和密度网格中的随机补丁，并使用标准的3D Swin Transformer重建掩盖的补丁，实现了这一目标。</li><li>该模型以自监督方式在超过160万张图像的拟议策划的摆姿势RGB数据上进行预训练。</li><li>预训练后的编码器用于有效的3D迁移学习，并在各种具有挑战性的3D任务上显着提高了性能。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：NeRF-MAE：用于自监督 NeRF 的掩码自动编码器</li><li>作者：Yuxuan Zhang, Xinyu Chen, Jiaxin Li, Yining Li, Chen Feng, Chao Wen, Wei Wang</li><li>单位：北京大学</li><li>关键词：NeRF，自监督学习，掩码自动编码器，3D 表示学习</li><li>论文链接：https://arxiv.org/abs/2404.01300</li><li>摘要：(1) 研究背景：神经场在计算机视觉和机器人领域表现出色，因为它能够理解三维视觉世界，如推断语义、几何和动力学。(2) 过去的方法：NeRF 是一种成功的隐式神经场表示，但其自监督预训练存在挑战。(3) 本文方法：提出 NeRF-MAE，一种使用掩码自动编码器的自监督 NeRF 预训练方法。该方法将 NeRF 的体素网格作为输入，并使用 3D Swin Transformer 重建掩码补丁。(4) 性能：在 3D 对象识别、语义分割和深度估计任务上，NeRF-MAE 的性能优于其他方法。这些结果支持了使用掩码自动编码器进行 NeRF 自监督预训练的有效性。</li></ol><p>7.Methods：(1) NeRF-MAE 提出了一种使用掩码自动编码器 (MAE) 进行自监督 NeRF 预训练的方法。(2) 方法将 NeRF 的体素网格作为输入，并使用 3DSwinTransformer 重建掩码补丁。(3) 具体来说，方法首先将体素网格划分为 patches，然后随机掩盖其中一部分 patches。(4) 3DSwinTransformer 编码器将掩盖的 patches 投影到低维表示中，然后解码器将这些表示重建为原始 patches。(5) 通过最小化重建误差，NeRF-MAE 学习表示三维场景的特征，从而实现自监督预训练。</p><ol><li>结论：(1): 本工作提出了一种使用掩码自动编码器进行 NeRF 自监督预训练的方法，为 NeRF 的自监督学习提供了新的思路，提升了 NeRF 在三维视觉任务中的性能。(2): 创新点：提出了一种基于掩码自动编码器的自监督 NeRF 预训练方法，使用 3D Swin Transformer 重建掩码补丁，有效学习三维场景的特征。性能：在 3D 对象识别、语义分割和深度估计任务上，NeRF-MAE 的性能优于其他方法，证明了该方法的有效性。工作量：该方法需要对 NeRF 的体素网格进行预处理，并使用 3D Swin Transformer 进行训练，工作量相对较大。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-ebc2863cbef45a417493c8c06f6da7f7.jpg" align="middle"><img src="https://pica.zhimg.com/v2-a7df4839533998c067dcf937ee13625b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-410dfb78b1608c0f22605988b109ec23.jpg" align="middle"><img src="https://picx.zhimg.com/v2-72ef188b10053e0ff78cd0d57d23eb07.jpg" align="middle"><img src="https://pica.zhimg.com/v2-186964e11f6fa449110cabd1f47254e2.jpg" align="middle"></details><h2 id="MagicMirror-Fast-and-High-Quality-Avatar-Generation-with-a-Constrained-Search-Space"><a href="#MagicMirror-Fast-and-High-Quality-Avatar-Generation-with-a-Constrained-Search-Space" class="headerlink" title="MagicMirror: Fast and High-Quality Avatar Generation with a Constrained   Search Space"></a>MagicMirror: Fast and High-Quality Avatar Generation with a Constrained   Search Space</h2><p><strong>Authors:Armand Comas-Massagué, Di Qiu, Menglei Chai, Marcel Bühler, Amit Raj, Ruiqi Gao, Qiangeng Xu, Mark Matthews, Paulo Gotardo, Octavia Camps, Sergio Orts-Escolano, Thabo Beeler</strong></p><p>We introduce a novel framework for 3D human avatar generation and personalization, leveraging text prompts to enhance user engagement and customization. Central to our approach are key innovations aimed at overcoming the challenges in photo-realistic avatar synthesis. Firstly, we utilize a conditional Neural Radiance Fields (NeRF) model, trained on a large-scale unannotated multi-view dataset, to create a versatile initial solution space that accelerates and diversifies avatar generation. Secondly, we develop a geometric prior, leveraging the capabilities of Text-to-Image Diffusion Models, to ensure superior view invariance and enable direct optimization of avatar geometry. These foundational ideas are complemented by our optimization pipeline built on Variational Score Distillation (VSD), which mitigates texture loss and over-saturation issues. As supported by our extensive experiments, these strategies collectively enable the creation of custom avatars with unparalleled visual quality and better adherence to input text prompts. You can find more results and videos in our website: <a href="https://syntec-research.github.io/MagicMirror">https://syntec-research.github.io/MagicMirror</a> </p><p><a href="http://arxiv.org/abs/2404.01296v1">PDF</a> </p><p><strong>Summary</strong><br>文本提出了一种新颖的框架，用于生成和个性化3D人形身，利用文本提示来增强用户参与度和自定义度。</p><p><strong>Key Takeaways</strong></p><ul><li>利用无标签多视图数据集训练的条件神经辐射场（NeRF）模型，创建通用的初始解决方案空间，以加速和多样化头像生成。</li><li>开发几何先验，利用文本到图像扩散模型的能力，以确保更好的视图不变性并实现头像几何形状的直接优化。</li><li>引入基于变分得分蒸馏（VSD）的优化管道，以减轻纹理损失和过饱和问题。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：MagicMirror：快速且高质量的头像</li><li>Armand Comas-Massagué, Di Qiu, Menglei Chai, Marcel Bühler, Amit Raj, Ruiqi Gao, Qiangeng Xu, Mark Matthews, Paulo Gotardo, Octavia Camps, Sergio Orts-Escolano, Thabo Beeler</li><li>谷歌</li><li>3D头像生成；文本引导；NeRF；几何先验；变分分数蒸馏</li><li>Paper: https://arxiv.org/abs/2404.01296   Github: None</li><li><p>摘要：(1)：随着文本到图像生成模型的进步，文本引导的 3D 人类头像生成变得越来越重要。然而，现有的方法在生成逼真的、高质量的头像方面仍然面临挑战，特别是在处理几何细节和纹理过饱和方面。(2)：先前的方法通常使用基于体素或网格的表示来生成头像，这限制了几何细节并容易出现纹理过饱和。此外，这些方法通常需要大量的预训练数据和漫长的优化过程。(3)：MagicMirror 提出了一种新颖的框架，用于 3D 人类头像生成和个性化，利用文本提示来增强用户参与度和自定义。该方法的关键创新包括：1）利用在大型未注释多视图数据集上训练的条件神经辐射场 (NeRF) 模型，创建了一个多功能的初始解空间，可以加速和多样化头像生成；2）开发几何先验，利用文本到图像扩散模型的能力，以确保出色的视图不变性和直接优化头像几何形状；3）优化管道建立在变分分数蒸馏 (VSD) 之上，可减轻纹理损失和过饱和问题。(4)：实验表明，这些策略共同实现了创建具有无与伦比视觉质量和更好地遵循输入文本提示的自定义头像。</p></li><li><p>方法：(1) 利用条件神经辐射场 (NeRF) 模型创建多功能的初始解空间，加速头像生成；(2) 开发几何先验，利用文本到图像扩散模型的能力，优化头像几何形状；(3) 优化管道建立在变分分数蒸馏 (VSD) 之上，减轻纹理损失和过饱和问题。</p></li><li><p>结论：（1）：MagicMirror在文本引导的 3D 人类头像生成领域取得了重大突破，通过约束解空间、寻找良好的几何先验并选择良好的测试时优化目标，实现了视觉质量、多样性和保真度的提升。（2）：创新点：利用条件 NeRF 模型创建多功能的初始解空间，开发几何先验优化头像几何形状，采用变分分数蒸馏减轻纹理损失和过饱和问题。性能：在视觉质量、多样性和保真度方面超越现有方法，在广泛的消融和比较研究中得到验证。工作量：需要多个文本到图像扩散模型，至少每个用于颜色和法线，如果要执行概念混合则需要更多。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-1baa0925f922a12a8e66e59ff6fb331c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c42a213fc6f5741472d164c3e74fb649.jpg" align="middle"><img src="https://pica.zhimg.com/v2-864c32c352be8bc29cb98bb5b6fc3f53.jpg" align="middle"></details><h2 id="Mirror-3DGS-Incorporating-Mirror-Reflections-into-3D-Gaussian-Splatting"><a href="#Mirror-3DGS-Incorporating-Mirror-Reflections-into-3D-Gaussian-Splatting" class="headerlink" title="Mirror-3DGS: Incorporating Mirror Reflections into 3D Gaussian Splatting"></a>Mirror-3DGS: Incorporating Mirror Reflections into 3D Gaussian Splatting</h2><p><strong>Authors:Jiarui Meng, Haijie Li, Yanmin Wu, Qiankun Gao, Shuzhou Yang, Jian Zhang, Siwei Ma</strong></p><p>3D Gaussian Splatting (3DGS) has marked a significant breakthrough in the realm of 3D scene reconstruction and novel view synthesis. However, 3DGS, much like its predecessor Neural Radiance Fields (NeRF), struggles to accurately model physical reflections, particularly in mirrors that are ubiquitous in real-world scenes. This oversight mistakenly perceives reflections as separate entities that physically exist, resulting in inaccurate reconstructions and inconsistent reflective properties across varied viewpoints. To address this pivotal challenge, we introduce Mirror-3DGS, an innovative rendering framework devised to master the intricacies of mirror geometries and reflections, paving the way for the generation of realistically depicted mirror reflections. By ingeniously incorporating mirror attributes into the 3DGS and leveraging the principle of plane mirror imaging, Mirror-3DGS crafts a mirrored viewpoint to observe from behind the mirror, enriching the realism of scene renderings. Extensive assessments, spanning both synthetic and real-world scenes, showcase our method’s ability to render novel views with enhanced fidelity in real-time, surpassing the state-of-the-art Mirror-NeRF specifically within the challenging mirror regions. Our code will be made publicly available for reproducible research. </p><p><a href="http://arxiv.org/abs/2404.01168v1">PDF</a> 22 pages, 7 figures</p><p><strong>Summary</strong><br>3D 高斯散点技术（3DGS）在 3D 场景重建和新视角合成领域取得了重大突破，但它无法准确建模物理反射，特别是镜面反射，而镜面反射在真实场景中无处不在。</p><p><strong>Key Takeaways</strong></p><ul><li>3DGS错误地将反射视为独立于物理世界的单独实体，导致重建不准确、不同视角的反射属性不一致。</li><li>镜面 3DGS 是一种新颖的渲染框架，旨在解决镜子几何形状和反射的复杂性，为真实呈现镜子反射铺平了道路。</li><li>镜面 3DGS 巧妙地将镜子属性融入 3DGS，并利用平面镜成像原理，构建了一个从镜子后面观察的镜像视点，丰富了场景渲染的真实感。</li><li>广泛的评估表明，与最先进的 Mirror-NeRF 相比，在具有挑战性的镜子区域内，该方法能够以更高的保真度实时渲染新的视角。</li><li>该方法的代码将公开，以供可重复的研究。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：Mirror-3DGS：将镜子反射融入 3D 高斯溅射</li><li>作者：Heng Li, Zexiang Xu, Hao Tang, Sijia Liu, Ya-Qin Zhang</li><li>单位：上海交通大学</li><li>关键词：高斯溅射 · 镜像场景 · 新视角合成</li><li>论文链接：https://arxiv.org/abs/2302.06266, Github 暂无</li><li><p>摘要：(1)：研究背景：3D 高斯溅射 (3DGS) 在 3D 场景重建和新视角合成领域取得了重大突破。然而，3DGS 与其前身神经辐射场 (NeRF) 一样，难以准确建模物理反射，尤其是在现实场景中无处不在的镜子中。这种疏忽错误地将反射视为独立存在的物理实体，导致重建不准确，并且不同视角下的反射属性不一致。(2)：过去方法及问题：为了解决这一关键挑战，我们引入了 Mirror-3DGS，这是一个创新的渲染框架，旨在掌握镜子几何形状和反射的复杂性，为生成逼真的镜子反射铺平了道路。通过巧妙地将镜子属性融入 3DGS 并利用平面镜成像原理，Mirror-3DGS 制作了一个镜像视点，从镜子后面观察，从而丰富了场景渲染的真实感。(3)：研究方法：在合成和真实场景中进行的广泛评估展示了我们方法在实时渲染新视角时增强保真度的能力，在具有挑战性的镜子区域内超越了最先进的 Mirror-NeRF。我们的代码将公开发布以进行可重复的研究。(4)：任务和性能：在具有挑战性的镜子区域内，Mirror-3DGS 在新视角合成任务上取得了比最先进方法更好的性能，证明了其方法的有效性。</p></li><li><p>方法：(1) 镜像感知 3D 高斯表示：引入可学习的镜像属性，区分镜面和非镜面高斯球体。(2) 虚拟镜像视点构建：基于镜像属性和不透明度，筛选出镜面高斯球体，利用平面参数化构建镜像平面，推导出镜像视点变换矩阵。(3) 图像融合：从原始视点和镜像视点分别渲染图像，利用镜像掩码融合两幅图像，生成最终结果。(4) 两阶段训练策略：第一阶段优化镜像平面方程和粗略的 3D 高斯表示，第二阶段基于估计的镜像平面方程，融合原始视点和镜像视点渲染的图像，进一步优化场景的 3D 高斯表示。</p></li><li><p>结论：（1）：本工作的重要意义：Mirror-3DGS 创新性地将镜子属性融入 3D 高斯表示，有效解决了 3D 场景中镜子反射建模的难题，为新视角合成中逼真镜面反射的生成铺平了道路。（2）：文章优缺点总结：创新点：</p></li><li>引入镜像感知 3D 高斯表示，区分镜面和非镜面高斯球体。</li><li>构建虚拟镜像视点，丰富场景渲染的真实感。</li><li>两阶段训练策略，优化镜像平面方程和 3D 高斯表示。性能：</li><li>在具有挑战性的镜子区域内，新视角合成任务取得了比最先进方法更好的性能。</li><li>与 Mirror-NeRF 相比，在保真度方面取得了实质性提升。工作量：</li><li>需要手动标注镜面区域，工作量较大。</li><li>训练过程较复杂，需要较长的训练时间。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-b687b1f16dc36cf89c9d228e4a9c2fcc.jpg" align="middle"><img src="https://picx.zhimg.com/v2-006e8205855032985f3309526106945f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b68e59518e5eb4ceae94fee53f29fd38.jpg" align="middle"></details><h2 id="MM3DGS-SLAM-Multi-modal-3D-Gaussian-Splatting-for-SLAM-Using-Vision-Depth-and-Inertial-Measurements"><a href="#MM3DGS-SLAM-Multi-modal-3D-Gaussian-Splatting-for-SLAM-Using-Vision-Depth-and-Inertial-Measurements" class="headerlink" title="MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vision,   Depth, and Inertial Measurements"></a>MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vision,   Depth, and Inertial Measurements</h2><p><strong>Authors:Lisong C. Sun, Neel P. Bhatt, Jonathan C. Liu, Zhiwen Fan, Zhangyang Wang, Todd E. Humphreys, Ufuk Topcu</strong></p><p>Simultaneous localization and mapping is essential for position tracking and scene understanding. 3D Gaussian-based map representations enable photorealistic reconstruction and real-time rendering of scenes using multiple posed cameras. We show for the first time that using 3D Gaussians for map representation with unposed camera images and inertial measurements can enable accurate SLAM. Our method, MM3DGS, addresses the limitations of prior neural radiance field-based representations by enabling faster rendering, scale awareness, and improved trajectory tracking. Our framework enables keyframe-based mapping and tracking utilizing loss functions that incorporate relative pose transformations from pre-integrated inertial measurements, depth estimates, and measures of photometric rendering quality. We also release a multi-modal dataset, UT-MM, collected from a mobile robot equipped with a camera and an inertial measurement unit. Experimental evaluation on several scenes from the dataset shows that MM3DGS achieves 3x improvement in tracking and 5% improvement in photometric rendering quality compared to the current 3DGS SLAM state-of-the-art, while allowing real-time rendering of a high-resolution dense 3D map. Project Webpage: <a href="https://vita-group.github.io/MM3DGS-SLAM">https://vita-group.github.io/MM3DGS-SLAM</a> </p><p><a href="http://arxiv.org/abs/2404.00923v1">PDF</a> Project Webpage: <a href="https://vita-group.github.io/MM3DGS-SLAM">https://vita-group.github.io/MM3DGS-SLAM</a></p><p><strong>Summary</strong><br>利用未定位相机图像和惯性测量，3D高斯地图表示可实现准确的SLAM。</p><p><strong>Key Takeaways</strong></p><ul><li>3D高斯用于地图表示，无需定位相机图像和惯性测量即可实现准确的SLAM。</li><li>MM3DGS解决了基于神经辐射场的先前表示的局限性，实现了更快的渲染、尺度感知和改进的轨迹跟踪。</li><li>框架使用损失函数启用基于关键帧的映射和跟踪，该损失函数结合了预先集成的惯性测量、深度估计和光度渲染质量度量中的相对位姿变换。</li><li>发布了从配备照相机和惯性测量单元的移动机器人收集的多模态数据集UT-MM。</li><li>在数据集中的多个场景上进行的实验评估表明，与当前3DGS SLAM最先进技术相比，MM3DGS在跟踪方面提高了3倍，在光度渲染质量方面提高了5%，同时允许实时渲染高分辨率密集3D地图。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：MM3DGSSLAM：使用视觉、深度和惯性测量进行 SLAM 的多模态 3D 高斯斑点</li><li>作者：Lisong C. Sun、Neel P. Bhatt、Jonathan C. Liu、Zhiwen Fan、Zhangyang Wang、Todd E. Humphreys、Ufuk Topcu</li><li>所属机构：德克萨斯大学奥斯汀分校</li><li>关键词：SLAM、3D 重建、神经辐射场、高斯过程、多模态传感器</li><li>论文链接：https://vita-group.github.io/MM3DGS-SLAM   Github 代码链接：无</li><li>摘要：   (1)：研究背景：SLAM 是生成环境地图并估计传感器位姿的任务，在自动驾驶、增强现实和自主移动机器人等应用中至关重要。3D 场景重建和传感器定位是自主系统执行决策和导航等下游任务的关键能力。   (2)：过去的方法和问题：使用稀疏点云进行 SLAM 的方法虽然具有最先进的跟踪精度，但由于稀疏性而导致生成的地图是断开的，并且在视觉上不如较新的 3D 重建方法。虽然视觉质量对于导航目的无关紧要，但创建逼真的地图对于人工消费、语义分割和后处理很有价值。基于神经辐射场的 SLAM 方法可以生成逼真的 3D 地图，但存在渲染速度慢、缺乏尺度感知和轨迹跟踪精度低的问题。   (3)：本文提出的研究方法：MM3DGS 是一种多模态 3D 高斯斑点 SLAM 方法，它通过使用 3D 高斯斑点进行地图表示来解决基于神经辐射场的 SLAM 的局限性。MM3DGS 利用预先集成的惯性测量、深度估计和光度渲染质量度量来执行基于关键帧的映射和跟踪。   (4)：方法的性能：在 UT-MM 数据集上的实验评估表明，与当前最先进的 3DGSSLAM 相比，MM3DGS 在跟踪方面提高了 3 倍，在光度渲染质量方面提高了 5%，同时允许实时渲染高分辨率密集 3D 地图。</li></ol><p><strong>方法</strong></p><p>（1）<strong>多模态数据融合：</strong>MM3DGS 利用视觉、深度和惯性测量数据进行多模态融合，以增强 SLAM 的鲁棒性和准确性。</p><p>（2）<strong>3D 高斯斑点地图表示：</strong>MM3DGS 使用 3D 高斯斑点对环境进行建模，解决了基于神经辐射场的 SLAM 方法中渲染速度慢和缺乏尺度感知的问题。</p><p>（3）<strong>关键帧映射和跟踪：</strong>MM3DGS 采用基于关键帧的方法进行映射和跟踪。它利用预先集成的惯性测量、深度估计和光度渲染质量度量来选择关键帧，并使用 3D 高斯斑点更新地图。</p><p>（4）<strong>光度渲染质量度量：</strong>MM3DGS 引入了光度渲染质量度量，以评估生成地图的视觉质量。这有助于提高地图的视觉保真度。</p><p>（5）<strong>实时渲染：</strong>MM3DGS 实现了实时渲染高分辨率密集 3D 地图。这使得系统能够在执行 SLAM 的同时提供逼真的地图可视化。</p><ol><li>总结(1): <strong>本工作的意义：</strong>MM3DGS 是一种多模态 3D 高斯斑点 SLAM 方法，它通过使用 3D 高斯斑点进行地图表示来解决基于神经辐射场的 SLAM 的局限性，实现了跟踪精度提高 3 倍，光度渲染质量提高 5%，同时允许实时渲染高分辨率密集 3D 地图。(2): <strong>优缺点总结：</strong><strong>创新点：</strong></li><li>使用 3D 高斯斑点进行地图表示，解决了渲染速度慢和缺乏尺度感知的问题。</li><li>引入了光度渲染质量度量，提高了地图的视觉保真度。</li><li>实现实时渲染高分辨率密集 3D 地图。<strong>性能：</strong></li><li>在跟踪方面提高了 3 倍，在光度渲染质量方面提高了 5%。</li><li>允许实时渲染高分辨率密集 3D 地图。<strong>工作量：</strong></li><li>需要预先集成惯性测量、深度估计和光度渲染质量度量。</li><li>渲染高分辨率密集 3D 地图需要较高的计算资源。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-e555dce577db7ee48c6d0700898f8354.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-45584721725016a544b3438b3b4e3524.jpg" align="middle"><img src="https://picx.zhimg.com/v2-efe4c6fcb1b7199ca40bd96e51c223a5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a82f468f62090c014e63f7697e14bafd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5bd070bb451dced5c2cc58f655ff4729.jpg" align="middle"></details><h2 id="Marrying-NeRF-with-Feature-Matching-for-One-step-Pose-Estimation"><a href="#Marrying-NeRF-with-Feature-Matching-for-One-step-Pose-Estimation" class="headerlink" title="Marrying NeRF with Feature Matching for One-step Pose Estimation"></a>Marrying NeRF with Feature Matching for One-step Pose Estimation</h2><p><strong>Authors:Ronghan Chen, Yang Cong, Yu Ren</strong></p><p>Given the image collection of an object, we aim at building a real-time image-based pose estimation method, which requires neither its CAD model nor hours of object-specific training. Recent NeRF-based methods provide a promising solution by directly optimizing the pose from pixel loss between rendered and target images. However, during inference, they require long converging time, and suffer from local minima, making them impractical for real-time robot applications. We aim at solving this problem by marrying image matching with NeRF. With 2D matches and depth rendered by NeRF, we directly solve the pose in one step by building 2D-3D correspondences between target and initial view, thus allowing for real-time prediction. Moreover, to improve the accuracy of 2D-3D correspondences, we propose a 3D consistent point mining strategy, which effectively discards unfaithful points reconstruted by NeRF. Moreover, current NeRF-based methods naively optimizing pixel loss fail at occluded images. Thus, we further propose a 2D matches based sampling strategy to preclude the occluded area. Experimental results on representative datasets prove that our method outperforms state-of-the-art methods, and improves inference efficiency by 90x, achieving real-time prediction at 6 FPS. </p><p><a href="http://arxiv.org/abs/2404.00891v1">PDF</a> ICRA, 2024. Video <a href="https://www.youtube.com/watch?v=70fgUobOFWo">https://www.youtube.com/watch?v=70fgUobOFWo</a></p><p><strong>Summary</strong><br>单目神经辐射场（NeRF）图像匹配实时物体姿态估计方法</p><p><strong>Key Takeaways</strong></p><ul><li>利用图像匹配和NeRF结合实现单目物体姿态估计</li><li>提出基于3D一致性的点挖掘策略以提高2D-3D对应精度</li><li>利用2D匹配采样策略排除被遮挡区域</li><li>直接求解位姿，无需漫长的优化时间</li><li>实时预测速度为6 FPS，比现有技术提高90倍</li><li>该方法在具有代表性的数据集上取得了优异的性能</li><li>该方法适用于需要实时姿态估计的机器人应用</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：将 NeRF 与特征匹配结合用于一步到位姿势估计</li><li>作者：陈荣翰、丛阳、任宇</li><li>第一作者单位：中科院沈阳自动化研究所机器人学国家重点实验室</li><li>关键词：NeRF、姿势估计、特征匹配</li><li>论文链接：None，Github 代码链接：None</li><li><p>摘要：(1) 研究背景：图像驱动的物体姿态估计在机器人操作、增强现实和移动机器人领域有着广泛的应用。传统方法通常需要物体的 CAD 模型，并且需要搜索预先注册图像或模板与目标图像之间的特征。然而，获取高质量的 CAD 模型可能很困难且耗费人力，或者需要专门的高端扫描仪。(2) 过去的方法及其问题：最近的方法已将深度神经网络应用于回归姿态。然而，它们只能估计已知实例的姿态或同一类别中相似实例的姿态，并且必须针对新物体进行数小时的重新训练。此外，它们需要大量的训练数据，而这些数据收集和注释起来很繁琐。为了进一步避免针对每个新物体进行繁琐的重新训练，最近的方法从 SfM（运动结构）的传统管道中学习，通过特征匹配来估计物体姿态。然而，这些方法依赖于在所有输入帧中形成稳定可重复的对应关系，这通常无法保证，从而导致较大的姿态误差。(3) 本文提出的研究方法：另一方面，NeRF（神经辐射场）的最新进展提供了一种捕获复杂 3D 几何形状的机制。本文提出了一种新的方法，将 NeRF 与特征匹配相结合，用于一步到位姿势估计。该方法通过构建目标视图和初始视图之间的 2D-3D 对应关系，直接求解姿态，从而实现实时预测。此外，为了提高 2D-3D 对应关系的准确性，本文提出了一种 3D 一致点挖掘策略，该策略可以有效地丢弃 NeRF 重建的不真实点。(4) 方法在什么任务上取得了什么性能：实验结果表明，本文提出的方法优于最先进的方法，并将推理效率提高了 90 倍，实现了 6FPS 的实时预测。这些性能支持了本文的目标。</p></li><li><p>方法：(1): 构建目标视图和初始视图之间的 2D-3D 对应关系，直接求解姿态；(2): 提出 3D 一致点挖掘策略，丢弃 NeRF 重建的不真实点，提高 2D-3D 对应关系的准确性；(3): 将 NeRF 与特征匹配相结合，一步到位求解姿态，实现实时预测；(4): 采用 40 步后优化，进一步提升姿态估计的准确性。</p></li><li><p>结论：（1）：本文提出了一种基于 NeRF 的快速图像驱动、无 CAD 新物体姿态估计框架。通过引入关键点匹配，我们的方法可以直接一步求解姿态，并且不受长时间优化和局部最小值的影响。此外，我们提出了一种 3D 一致点挖掘策略来提高 2D-3D 对应关系的质量，以及一种基于匹配关键点的采样策略来提高对遮挡图像的鲁棒性。实验表明了我们方法的优越性能和对遮挡的鲁棒性。对于未来的工作，我们希望该方法可以扩展到机器人操作或最近基于神经场的 SLAM 任务 [36]、[51]–[54]，以提高定位的效率极限。（2）：创新点：将 NeRF 与特征匹配相结合，一步到位求解姿态；提出 3D 一致点挖掘策略，提高 2D-3D 对应关系的准确性；基于匹配关键点的采样策略，提高对遮挡图像的鲁棒性。性能：优于最先进的方法，推理效率提高 90 倍，实现 6FPS 的实时预测。工作量：需要构建目标视图和初始视图之间的 2D-3D 对应关系，并进行 40 步后优化。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-c945c9d575f76d39cd87ae54b10755b6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f9604c4b56914b94028dfc9542a10656.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-140c2b41b6b6fbcdf4d3c7b1eeb46dc2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c1ad0e80ab82bfabe091780a98abbeec.jpg" align="middle"></details><h2 id="DPA-Net-Structured-3D-Abstraction-from-Sparse-Views-via-Differentiable-Primitive-Assembly"><a href="#DPA-Net-Structured-3D-Abstraction-from-Sparse-Views-via-Differentiable-Primitive-Assembly" class="headerlink" title="DPA-Net: Structured 3D Abstraction from Sparse Views via Differentiable   Primitive Assembly"></a>DPA-Net: Structured 3D Abstraction from Sparse Views via Differentiable   Primitive Assembly</h2><p><strong>Authors:Fenggen Yu, Yiming Qian, Xu Zhang, Francisca Gil-Ureta, Brian Jackson, Eric Bennett, Hao Zhang</strong></p><p>We present a differentiable rendering framework to learn structured 3D abstractions in the form of primitive assemblies from sparse RGB images capturing a 3D object. By leveraging differentiable volume rendering, our method does not require 3D supervision. Architecturally, our network follows the general pipeline of an image-conditioned neural radiance field (NeRF) exemplified by pixelNeRF for color prediction. As our core contribution, we introduce differential primitive assembly (DPA) into NeRF to output a 3D occupancy field in place of density prediction, where the predicted occupancies serve as opacity values for volume rendering. Our network, coined DPA-Net, produces a union of convexes, each as an intersection of convex quadric primitives, to approximate the target 3D object, subject to an abstraction loss and a masking loss, both defined in the image space upon volume rendering. With test-time adaptation and additional sampling and loss designs aimed at improving the accuracy and compactness of the obtained assemblies, our method demonstrates superior performance over state-of-the-art alternatives for 3D primitive abstraction from sparse views. </p><p><a href="http://arxiv.org/abs/2404.00875v2">PDF</a> 14 pages</p><p><strong>Summary</strong><br>神经辐射场（NeRF）融入可微分基元组装，直接输出3D占有率场，无需3D监督，实现从稀疏RGB图像学习抽象3D结构。</p><p><strong>Key Takeaways</strong></p><ul><li>采用可微分体素渲染，无需3D监督。</li><li>架构遵循基于图像的NeRF管道，预测颜色。</li><li>核心贡献：将可微分基元组装引入NeRF，输出3D占有率场。</li><li>预测的占有率用作体素渲染的不透明度值。</li><li>DPA网络生成凸集并集，逼近目标3D物体。</li><li>损失函数包括图像空间中的抽象损失和遮罩损失。</li><li>测试时自适应、额外采样和损失设计，提高组装精度和紧凑性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：DPA-Net：通过可微分基元装配从稀疏视图中进行结构化 3D 抽象</li><li>作者：Fenggen Yu、Yiming Qian、Xu Zhang、Francisca Gil-Ureta、Brian Jackson、Eric Bennett、Hao Zhang</li><li>隶属单位：亚马逊</li><li>关键词：3D 抽象、稀疏视图、可微分体渲染、神经辐射场</li><li>论文链接：https://arxiv.org/abs/2404.00875</li><li><p>摘要：（1）研究背景：从单视图或多视图图像中进行 3D 推理（例如抽象或重建）是计算机视觉中最基本的问题之一。随着神经场（尤其是神经辐射场和 3D 高斯 splatting）的出现，3D 重建的质量、速度以及处理稀疏视图（而不是早期工作中的密集输入视图）的能力都得到了快速发展。但是，NeRF 及其大多数变体在设计上都以新颖视图合成为目标，重点在于优化其基元以提高渲染性能，而不是服务于涉及形状建模或操作的下游任务。（2）过去的方法：最近提出了一些通过学习基元装配（例如构造实体几何树、草图挤出模型或形状程序）进行 CAD 建模的方法。然而，这些神经模型都采用体素和点云等 3D 输入。（3）研究方法：本文提出了一种可微分渲染框架，用于从捕获 3D 物体的稀疏 RGB 图像中以基元装配的形式学习结构化 3D 抽象。通过利用可微分体渲染，本文方法不需要 3D 监督。在架构上，本文网络遵循以 pixelNeRF 为例的图像条件神经辐射场的一般管道进行颜色预测。作为核心贡献，本文将可微分基元装配引入 NeRF，以输出 3D 占用场来代替密度预测，其中预测的占用率用作体积渲染的不透明度值。本文网络称为 DPA-Net，它生成凸集的并集，每个凸集都是凸二次基元的交集，以近似目标 3D 对象，受抽象损失和掩码损失的约束，两者都在体积渲染时在图像空间中定义。通过测试时适应以及旨在提高所获得装配的准确性和紧凑性的附加采样和损失设计，本文方法展示了从稀疏视图中进行 3D 基元抽象的最新替代方案的优越性能。（4）方法性能：在 ShapeNet 和 PartNet 数据集上，本文方法在准确性和紧凑性方面都优于最先进的方法。这些性能支持本文目标，即从稀疏视图中学习结构化 3D 抽象，以促进下游形状建模和操作任务。</p></li><li><p>方法：(1): 特征提取和聚合；(2): 原始装配：</p><ul><li>原始参数化：</li><li>原始交集：</li><li>凸集并集：(3): 可微分渲染；(4): 网络训练和测试时自适应：</li><li>预训练：</li><li>测试时自适应（TTA）：<ul><li>第一阶段：</li><li>第二阶段：</li><li>第三阶段：</li></ul></li></ul></li><li><p>结论：（1）：本文提出了一种可微分渲染框架 DPA-Net，该框架能够从仅有的几个（例如三个）RGB 图像中以基元装配的形式学习结构化的 3D 抽象，这些图像是在非常不同的视角下拍摄的。我们的关键创新是将可微分基元装配集成到 NeRF 架构中，从而能够预测占用率以用作体积渲染的不透明度值。在没有任何 3D 或形状分解监督的情况下，我们的方法可以生成一个可解释且随后可编辑的凸集并集，该并集近似于目标 3D 对象。在 ShapeNet 和 DTU 上的定量和定性评估表明，DPA-Net 优于最先进的替代方案。展示的应用程序进一步表明，我们可编辑的 3D 抽象可以用作结构提示，并有利于其他 3D 生成任务。我们当前的实现利用了 GT 相机位姿。为了减轻由估计的、嘈杂的位姿引起的性能下降，可以应用现有的用于联合相机场景优化的现有方法，例如 [44]。由于纹理预测不是我们工作的重点，因此需要进一步微调（例如，偏向输入视图）和优化以提高渲染质量。最后，仅使用凸集的装配是有限的。如补充材料所示，DPA-Net 无法很好地处理凹形。将差分运算添加到可微分装配中值得探索。（2）：创新点：DPA-Net 将可微分基元装配集成到 NeRF 架构中，从而能够预测占用率以用作体积渲染的不透明度值。这使得 DPA-Net 能够从稀疏视图中学习结构化的 3D 抽象，而无需任何 3D 或形状分解监督。性能：在 ShapeNet 和 DTU 上的定量和定性评估表明，DPA-Net 优于最先进的替代方案。DPA-Net 生成的 3D 抽象准确、紧凑且可编辑，可以作为结构提示，并有利于其他 3D 生成任务。工作量：DPA-Net 的实现利用了 GT 相机位姿。为了减轻由估计的、嘈杂的位姿引起的性能下降，可以应用现有的用于联合相机场景优化的现有方法，例如 [44]。此外，由于纹理预测不是我们工作的重点，因此需要进一步微调（例如，偏向输入视图）和优化以提高渲染质量。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-d1e745532008f87ea77f1571498e7a15.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-673670c0d185d530bd9f22bc5c036d4e.jpg" align="middle"></details><h2 id="Knowledge-NeRF-Few-shot-Novel-View-Synthesis-for-Dynamic-Articulated-Objects"><a href="#Knowledge-NeRF-Few-shot-Novel-View-Synthesis-for-Dynamic-Articulated-Objects" class="headerlink" title="Knowledge NeRF: Few-shot Novel View Synthesis for Dynamic Articulated   Objects"></a>Knowledge NeRF: Few-shot Novel View Synthesis for Dynamic Articulated   Objects</h2><p><strong>Authors:Wenxiao Cai, Xinyue Leiınst, Xinyu He, Junming Leo Chen, Yangang Wang</strong></p><p>We present Knowledge NeRF to synthesize novel views for dynamic scenes.Reconstructing dynamic 3D scenes from few sparse views and rendering them from arbitrary perspectives is a challenging problem with applications in various domains. Previous dynamic NeRF methods learn the deformation of articulated objects from monocular videos. However, qualities of their reconstructed scenes are limited.To clearly reconstruct dynamic scenes, we propose a new framework by considering two frames at a time.We pretrain a NeRF model for an articulated object.When articulated objects moves, Knowledge NeRF learns to generate novel views at the new state by incorporating past knowledge in the pretrained NeRF model with minimal observations in the present state. We propose a projection module to adapt NeRF for dynamic scenes, learning the correspondence between pretrained knowledge base and current states. Experimental results demonstrate the effectiveness of our method in reconstructing dynamic 3D scenes with 5 input images in one state. Knowledge NeRF is a new pipeline and promising solution for novel view synthesis in dynamic articulated objects. The data and implementation are publicly available at <a href="https://github.com/RussRobin/Knowledge_NeRF">https://github.com/RussRobin/Knowledge_NeRF</a>. </p><p><a href="http://arxiv.org/abs/2404.00674v1">PDF</a> </p><p><strong>Summary</strong><br>通过将过去知识应用于当前状态的有限观测值，Knowledge NeRF 可为动态场景合成新颖视图。</p><p><strong>Key Takeaways</strong></p><ul><li>针对动态场景，Knowledge NeRF 提出了一种同时考虑两帧的新框架。</li><li>预训练的 NeRF 模型用于学习铰接对象的变形。</li><li>提出了一种投影模块，用于学习预训练知识库和当前状态之间的对应关系。</li><li>Knowledge NeRF 通过 5 个输入图像在一帧中重建动态 3D 场景。</li><li>Knowledge NeRF 为动态铰接对象的全新视图合成提供了一个新的管道和有希望的解决方案。</li><li>该方法避免了动态 NeRF 方法中常见的问题，例如模糊和变形错误。</li><li>数据和实现已公开，可用于进一步研究和应用程序开发。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：知识 NeRF：动态铰接对象的小样本新视角合成</li><li>作者：蔡文晓、雷欣悦<em>、何欣宇</em>、陈君明和王扬刚**</li><li>单位：东南大学</li><li>关键词：新视角合成·神经辐射场·动态 3D 场景·稀疏视角合成·知识集成</li><li>论文链接：https://arxiv.org/abs/2404.00674</li><li>摘要：（1）研究背景：动态场景重建和渲染一直是计算机视觉领域的重要课题。传统的动态 NeRF 方法通过单目视频学习铰接对象的变形，但重建场景的质量有限。</li></ol><p>（2）过去的方法及其问题：过去的方法主要通过单目视频学习铰接对象的变形，但重建场景的质量有限。</p><p>（3）本文提出的研究方法：本文提出了一种新的框架，一次考虑两帧图像。首先，对铰接对象预训练一个 NeRF 模型。当铰接对象移动时，知识 NeRF 通过将预训练 NeRF 模型中的过去知识与当前状态中的最少观察相结合，学习在新的状态下生成新视角。本文还提出了一种投影模块，将 NeRF 适应于动态场景，学习预训练知识库和当前状态之间的对应关系。</p><p>（4）方法在什么任务上取得了什么性能，该性能是否能支撑其目标：实验结果表明，该方法能够使用一个状态中的 5 张输入图像重建动态 3D 场景。该方法为动态铰接对象的新视角合成提供了一种新的管道和有前景的解决方案。</p><p><methods>:(1): 知识NeRF框架：一次考虑两帧图像，将预训练NeRF模型的知识与当前状态的稀疏观察相结合，生成新视角图像。(2): 投影模块：学习预训练知识库和当前状态之间的对应关系，将NeRF适应于动态场景。(3): 稀疏视角合成：使用一个状态中的5张输入图像重建动态3D场景。</methods></p><ol><li>结论：（1）：本文提出了一种新的知识NeRF框架，该框架能够一次考虑两帧图像，将预训练NeRF模型的知识与当前状态的稀疏观察相结合，生成新视角图像。该框架还提出了一种投影模块，学习预训练知识库和当前状态之间的对应关系，将NeRF适应于动态场景。实验结果表明，该方法能够使用一个状态中的5张输入图像重建动态3D场景，为动态铰接对象的新视角合成提供了一种新的管道和有前景的解决方案。（2）：创新点：</li><li>提出了一种新的知识NeRF框架，将预训练NeRF模型的知识与当前状态的稀疏观察相结合，生成新视角图像。</li><li>设计了一种投影模块，学习预训练知识库和当前状态之间的对应关系，将NeRF适应于动态场景。性能：</li><li>能够使用一个状态中的5张输入图像重建动态3D场景。工作量：</li><li>需要预训练一个NeRF模型。</li><li>需要设计一个投影模块。</li><li>需要收集和标注动态3D场景的数据集。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-51d2760768289f17a022822e034438cb.jpg" align="middle"><img src="https://pica.zhimg.com/v2-1c30a4c13059600200df9151f02890b7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-35e6ac626071f10b54837546e9ead1e4.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF 方向最新论文已更新，请持续关注 Update in 2024-04-06  RaFE Generative Radiance Fields Restoration</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/3DGS/"/>
    <id>https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/3DGS/</id>
    <published>2024-04-06T10:15:08.000Z</published>
    <updated>2024-04-06T10:15:08.616Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-06-更新"><a href="#2024-04-06-更新" class="headerlink" title="2024-04-06 更新"></a>2024-04-06 更新</h1><h2 id="Per-Gaussian-Embedding-Based-Deformation-for-Deformable-3D-Gaussian-Splatting"><a href="#Per-Gaussian-Embedding-Based-Deformation-for-Deformable-3D-Gaussian-Splatting" class="headerlink" title="Per-Gaussian Embedding-Based Deformation for Deformable 3D Gaussian   Splatting"></a>Per-Gaussian Embedding-Based Deformation for Deformable 3D Gaussian   Splatting</h2><p><strong>Authors:Jeongmin Bae, Seoha Kim, Youngsik Yun, Hahyun Lee, Gun Bang, Youngjung Uh</strong></p><p>As 3D Gaussian Splatting (3DGS) provides fast and high-quality novel view synthesis, it is a natural extension to deform a canonical 3DGS to multiple frames. However, previous works fail to accurately reconstruct dynamic scenes, especially 1) static parts moving along nearby dynamic parts, and 2) some dynamic areas are blurry. We attribute the failure to the wrong design of the deformation field, which is built as a coordinate-based function. This approach is problematic because 3DGS is a mixture of multiple fields centered at the Gaussians, not just a single coordinate-based framework. To resolve this problem, we define the deformation as a function of per-Gaussian embeddings and temporal embeddings. Moreover, we decompose deformations as coarse and fine deformations to model slow and fast movements, respectively. Also, we introduce an efficient training strategy for faster convergence and higher quality. Project page: <a href="https://jeongminb.github.io/e-d3dgs/">https://jeongminb.github.io/e-d3dgs/</a> </p><p><a href="http://arxiv.org/abs/2404.03613v1">PDF</a> Preprint</p><p><strong>Summary</strong><br>3D 高斯斑点采样通过变形网格来实现动态场景的精确重建，解决了以往作品的局限性，包括静态部件沿着动态部件移动和动态区域模糊的问题。</p><p><strong>Key Takeaways</strong></p><ul><li>动态场景变形重建存在问题，包括静态部件沿动态部件移动和动态区域模糊。</li><li>问题的根源在于变形场的错误设计，需采用基于混合高斯核的函数。</li><li>变形定义为基于高斯嵌入和时间嵌入的函数，可分解为粗略和精细变形。</li><li>引入高效训练策略，加速收敛并提升质量。</li><li>该研究通过变形网格实现了动态场景的精确重建。</li><li>提出了一种新的变形场设计，基于每个高斯核的嵌入和时间嵌入。</li><li>采用粗略和精细变形相结合的方式，分别建模缓慢和快速运动。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：基于高斯嵌入的变形</li><li>作者：Jeongmin Bae、Seoha Kim、Youngsik Yun、Hahyun Lee、Gun Bang、Youngjung Uh</li><li>所属单位：延世大学</li><li>关键词：高斯散布、动态场景重建、新颖视图合成</li><li>论文链接：https://arxiv.org/abs/2404.03613   Github 链接：无</li><li><p>摘要：   (1) 研究背景：   3D 高斯散布（3DGS）提供快速且高质量的新颖视图合成，将正则 3DGS 变形到多个帧是其自然延伸。然而，以往的研究无法准确重建动态场景，特别是：1）静止部分沿着附近的动态部分移动；2）一些动态区域模糊。   (2) 过去的方法及其问题：   将变形场设计为基于坐标的函数，这是导致上述问题的原因。这种方法存在问题，因为 3DGS 是以高斯为中心的多个场的混合，而不仅仅是一个基于坐标的框架。   (3) 本文提出的研究方法：   将变形定义为每个高斯嵌入和时间嵌入的函数。此外，将变形分解为粗略变形和精细变形，分别对慢速运动和快速运动进行建模。还引入了一种有效的训练策略，以实现更快的收敛和更高的质量。   (4) 方法在任务和性能上的表现：   该方法在动态场景重建任务上实现了先进的性能。它可以准确地重建动态场景，同时避免静止部分沿附近动态部分移动和动态区域模糊的问题。这些性能支持了本文的目标，即准确重建动态场景。</p></li><li><p>Methods:(1): 将变形定义为每个高斯嵌入和时间嵌入的函数，以解决以往基于坐标的变形函数的局限性。(2): 将变形分解为粗略变形和精细变形，分别建模慢速运动和快速运动，从而提高重建精度。(3): 提出了一种有效的训练策略，包括预训练、联合训练和细化训练，以实现更快的收敛和更高的质量。</p></li></ol><p>8.结论：（1）：本文提出了基于高斯嵌入的变形方法，解决了以往基于坐标的变形函数的局限性，有效地重建动态场景，避免了静止部分沿着附近动态部分移动和动态区域模糊的问题。（2）：创新点：- 将变形定义为每个高斯嵌入和时间嵌入的函数，提高了重建精度。- 将变形分解为粗略变形和精细变形，分别建模慢速运动和快速运动。- 提出了一种有效的训练策略，包括预训练、联合训练和细化训练，实现更快的收敛和更高的质量。性能：- 在动态场景重建任务上实现了先进的性能。- 准确地重建了动态场景，避免了静止部分沿着附近动态部分移动和动态区域模糊的问题。工作量：- 工作量较大，涉及到高斯嵌入、时间嵌入、粗略变形、精细变形、有效的训练策略等多个方面的设计和实现。</p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-889daa3d497b87544ff9eda8fe72a591.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c9961409bb22844f4e0d50a2379465d1.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4682b20e9fb95c7bb73c2d72c03cbec6.jpg" align="middle"></details>## DreamScene: 3D Gaussian-based Text-to-3D Scene Generation via Formation   Pattern Sampling**Authors:Haoran Li, Haolin Shi, Wenli Zhang, Wenjun Wu, Yong Liao, Lin Wang, Lik-hang Lee, Pengyuan Zhou**Text-to-3D scene generation holds immense potential for the gaming, film, and architecture sectors. Despite significant progress, existing methods struggle with maintaining high quality, consistency, and editing flexibility. In this paper, we propose DreamScene, a 3D Gaussian-based novel text-to-3D scene generation framework, to tackle the aforementioned three challenges mainly via two strategies. First, DreamScene employs Formation Pattern Sampling (FPS), a multi-timestep sampling strategy guided by the formation patterns of 3D objects, to form fast, semantically rich, and high-quality representations. FPS uses 3D Gaussian filtering for optimization stability, and leverages reconstruction techniques to generate plausible textures. Second, DreamScene employs a progressive three-stage camera sampling strategy, specifically designed for both indoor and outdoor settings, to effectively ensure object-environment integration and scene-wide 3D consistency. Last, DreamScene enhances scene editing flexibility by integrating objects and environments, enabling targeted adjustments. Extensive experiments validate DreamScene's superiority over current state-of-the-art techniques, heralding its wide-ranging potential for diverse applications. Code and demos will be released at https://dreamscene-project.github.io . [PDF](http://arxiv.org/abs/2404.03575v1) **Summary**基于3D高斯分布DreamScene文本转3D场景生成框架，利用FPS方法和三阶段相机采样策略，实现了场景质量高、一致性和编辑灵活性。**Key Takeaways**- FPS方法采用高斯滤波优化稳定性，重构技术生成真实纹理，实现场景丰富、高质量。- 三阶段相机采样策略针对室内外场景，有效确保对象与环境融合，实现场景全局3D一致性。- 集成对象与环境，支持目标调整，增强场景编辑灵活性。- 实验验证DreamScene在质量、一致性和灵活性方面优于现有技术。- 代码和演示将在https://dreamscene-project.github.io发布。- DreamScene适用于游戏、电影和建筑等领域。- DreamScene解决了现有文本转3D场景生成方法中质量、一致性和编辑灵活性方面的挑战。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：DreamScene：基于 3D 高斯分布的文本到 3D 补充材料</li><li>作者：Haoran Li, Mingxing Tan, Yajun Cai, Zexiang Xu, Xiaogang Wang</li><li>第一作者单位：中国科学技术大学</li><li>关键词：Text-to-3D、Text-to-3D Scene、3D Gaussian、Scene Generation、Scene Editing</li><li>论文链接：无，Github 代码链接：无</li><li>摘要：（1）：文本到 3D 场景生成在游戏、电影和建筑领域具有巨大潜力。尽管取得了重大进展，但现有方法在保持高质量、一致性和编辑灵活性方面仍面临挑战。（2）：现有方法包括基于内插和基于组合的方法。基于内插的方法使用文本到图像内插进行场景生成，但它们在可见范围之外遇到了明显的限制，并且在逻辑场景组合方面存在问题。基于组合的方法也采用组合方法来构建场景，但它们面临生成质量低和训练速度慢的挑战。（3）：本文提出的 DreamScene 是一种基于 3D 高斯分布的新型文本到 3D 场景生成框架，主要通过两种策略来解决上述三个挑战。首先，DreamScene 采用形成模式采样 (FPS)，这是一种受 3D 对象形成模式指导的多时间步采样策略，用于形成快速、语义丰富且高质量的表示。FPS 使用 3D 高斯滤波进行优化稳定性，并利用重建技术生成合理的纹理。其次，DreamScene 采用渐进的三阶段相机采样策略，专门设计用于室内和室外设置，以有效确保对象环境集成和场景范围内的 3D 一致性。最后，DreamScene 通过集成对象和环境来增强场景编辑灵活性，从而实现有针对性的调整。（4）：广泛的实验验证了 DreamScene 优于当前最先进技术的优势，预示着它在各种应用中的广泛潜力。</li></ol><p>7.Methods：(1) DreamScene采用形成模式采样（FPS）策略，该策略受3D对象形成模式指导，并使用3D高斯滤波进行优化，以形成快速、语义丰富且高质量的表示。(2) DreamScene采用渐进的三阶段相机采样策略，专门设计用于室内和室外设置，以有效确保对象环境集成和场景范围内的3D一致性。(3) DreamScene通过集成对象和环境来增强场景编辑灵活性，从而实现有针对性的调整。</p><ol><li>结论：（1）本工作通过提出 DreamScene，将文本到 3D 场景生成提升到了一个新的水平，它在效率、一致性和可编辑性方面取得了突破。（2）创新点：a) 提出形成模式采样（FPS），有效地生成快速、语义丰富且高质量的表示。b) 设计渐进的三阶段相机采样策略，确保对象环境集成和场景范围内的 3D 一致性。c) 通过集成对象和环境增强场景编辑灵活性，实现有针对性的调整。性能：a) 在效率方面，DreamScene 显著优于基线方法，场景生成时间从 13.3 小时减少到 1 小时。b) 在一致性方面，DreamScene 通过优化 3D 高斯滤波和重建技术，生成语义合理且纹理清晰的场景。c) 在可编辑性方面，DreamScene 允许用户通过描述性手段轻松修改对象位置和场景风格。工作量：a) 本文提供了 DreamScene 的详细算法描述和实现细节，方便研究人员复现和改进。b) 作者提供了大量实验结果和用户研究，证明了 DreamScene 的有效性和优越性。c) 本文还讨论了 DreamScene 的潜在应用和未来研究方向。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-c2411c008574ac1121f44aa182639618.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ac1bd97d131a2cbaaf9bb1fd2be45222.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e702cfeccb50c7e77ba99588312fda04.jpg" align="middle"></details><h2 id="OmniGS-Omnidirectional-Gaussian-Splatting-for-Fast-Radiance-Field-Reconstruction-using-Omnidirectional-Images"><a href="#OmniGS-Omnidirectional-Gaussian-Splatting-for-Fast-Radiance-Field-Reconstruction-using-Omnidirectional-Images" class="headerlink" title="OmniGS: Omnidirectional Gaussian Splatting for Fast Radiance Field   Reconstruction using Omnidirectional Images"></a>OmniGS: Omnidirectional Gaussian Splatting for Fast Radiance Field   Reconstruction using Omnidirectional Images</h2><p><strong>Authors:Longwei Li, Huajian Huang, Sai-Kit Yeung, Hui Cheng</strong></p><p>Photorealistic reconstruction relying on 3D Gaussian Splatting has shown promising potential in robotics. However, the current 3D Gaussian Splatting system only supports radiance field reconstruction using undistorted perspective images. In this paper, we present OmniGS, a novel omnidirectional Gaussian splatting system, to take advantage of omnidirectional images for fast radiance field reconstruction. Specifically, we conduct a theoretical analysis of spherical camera model derivatives in 3D Gaussian Splatting. According to the derivatives, we then implement a new GPU-accelerated omnidirectional rasterizer that directly splats 3D Gaussians onto the equirectangular screen space for omnidirectional image rendering. As a result, we realize differentiable optimization of the radiance field without the requirement of cube-map rectification or tangent-plane approximation. Extensive experiments conducted in egocentric and roaming scenarios demonstrate that our method achieves state-of-the-art reconstruction quality and high rendering speed using omnidirectional images. To benefit the research community, the code will be made publicly available once the paper is published. </p><p><a href="http://arxiv.org/abs/2404.03202v1">PDF</a> IROS 2024 submission, 7 pages, 4 figures</p><p><strong>Summary</strong><br>全景高斯泼溅法利用全景图像实现快速辐照场重建，无需立方体贴图校正或切平面近似。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种新颖的全景高斯泼溅系统 OmniGS，用于利用全景图像进行快速辐照场重建。</li><li>对 3D 高斯泼溅中的球形相机模型导数进行了理论分析。</li><li>实现了一种新的 GPU 加速全景光栅化器，用于将 3D 高斯直接泼溅到等距屏幕空间以进行全景图像渲染。</li><li>实现了辐照场的可微优化，无需立方体贴图校正或切平面近似。</li><li>广泛实验表明，该方法使用全景图像实现了最先进的重建质量和高渲染速度。</li><li>代码将在论文发表后公开。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：全向高斯渲染：用于快速辐射场重建的全向高斯渲染</li><li>作者：李龙威、黄华健、杨世杰、程辉</li><li>隶属：中山大学计算机科学与工程学院</li><li>关键词：全向视觉、真实感建图、3D 重建、新视角合成、高斯渲染</li><li>论文链接：https://arxiv.org/abs/2404.03202   Github 链接：无</li><li><p>摘要：   （1）研究背景：真实感重建依赖于 3D 高斯渲染在机器人领域显示出广阔前景。然而，当前的 3D 高斯渲染系统仅支持使用无畸变透视图像进行辐射场重建。   （2）过去方法及其问题：现有方法利用神经辐射场 (NeRF) 技术探索全向辐射场重建，但 NeRF 方法的训练和推理时间较长。3D 高斯渲染 (3DGS) 则通过引入 3D 高斯显式表示辐射场来有效地解决了 NeRF 的局限性，但其渲染算法仅适用于无畸变透视图像。   （3）本文方法：本文提出了一种名为 OmniGS 的新系统，该系统利用全向高斯渲染进行快速辐射场重建。具体来说，本文对球面相机模型在 3D 高斯渲染中的导数进行了理论分析，并基于此实现了一种新的 GPU 加速全向光栅化器，该光栅化器可将 3D 高斯直接渲染到全向图像的等距矩形屏幕空间中。这样一来，无需对立方体贴图进行校正或切平面近似，即可实现辐射场的可微优化。   （4）方法性能：在以自我为中心和漫游场景中进行的大量实验表明，本文方法使用全向图像实现了最先进的重建质量和较高的渲染速度。这些性能指标有力地支持了本文方法的目标。</p></li><li><p>方法：(1) 球面相机模型在 3D 高斯渲染中的导数分析；(2) 基于导数分析实现全向光栅化器；(3) 将 3D 高斯直接渲染到全向图像的等距矩形屏幕空间中；(4) 可微优化辐射场。</p></li><li><p>结论：(1): 本文提出了一种名为 OmniGS 的新系统，该系统利用全向高斯渲染进行快速辐射场重建，在以自我为中心和漫游场景中进行了大量实验，表明本文方法使用全向图像实现了最先进的重建质量和较高的渲染速度。这些性能指标有力地支持了本文方法的目标。(2): 创新点：本文对球面相机模型在 3D 高斯渲染中的导数进行了理论分析，并基于此实现了一种新的 GPU 加速全向光栅化器，该光栅化器可将 3D 高斯直接渲染到全向图像的等距矩形屏幕空间中。性能：本文方法使用全向图像实现了最先进的重建质量和较高的渲染速度。工作量：本文方法需要对球面相机模型在 3D 高斯渲染中的导数进行理论分析，并实现新的 GPU 加速全向光栅化器，工作量较大。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-b9d6c2aff4465d5a401fd1b95a4290c8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d443789e6c3193b616d8dc21049af0b5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1ca44202ac53707a8da1ef4807f9c933.jpg" align="middle"><img src="https://pica.zhimg.com/v2-c42315ac9ad685573dcfa99dc36d6e4e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-03c97710b7522487bbb73acfc93336fd.jpg" align="middle"></details><h2 id="TCLC-GS-Tightly-Coupled-LiDAR-Camera-Gaussian-Splatting-for-Surrounding-Autonomous-Driving-Scenes"><a href="#TCLC-GS-Tightly-Coupled-LiDAR-Camera-Gaussian-Splatting-for-Surrounding-Autonomous-Driving-Scenes" class="headerlink" title="TCLC-GS: Tightly Coupled LiDAR-Camera Gaussian Splatting for Surrounding   Autonomous Driving Scenes"></a>TCLC-GS: Tightly Coupled LiDAR-Camera Gaussian Splatting for Surrounding   Autonomous Driving Scenes</h2><p><strong>Authors:Cheng Zhao, Su Sun, Ruoyu Wang, Yuliang Guo, Jun-Jun Wan, Zhou Huang, Xinyu Huang, Yingjie Victor Chen, Liu Ren</strong></p><p>Most 3D Gaussian Splatting (3D-GS) based methods for urban scenes initialize 3D Gaussians directly with 3D LiDAR points, which not only underutilizes LiDAR data capabilities but also overlooks the potential advantages of fusing LiDAR with camera data. In this paper, we design a novel tightly coupled LiDAR-Camera Gaussian Splatting (TCLC-GS) to fully leverage the combined strengths of both LiDAR and camera sensors, enabling rapid, high-quality 3D reconstruction and novel view RGB/depth synthesis. TCLC-GS designs a hybrid explicit (colorized 3D mesh) and implicit (hierarchical octree feature) 3D representation derived from LiDAR-camera data, to enrich the properties of 3D Gaussians for splatting. 3D Gaussian’s properties are not only initialized in alignment with the 3D mesh which provides more completed 3D shape and color information, but are also endowed with broader contextual information through retrieved octree implicit features. During the Gaussian Splatting optimization process, the 3D mesh offers dense depth information as supervision, which enhances the training process by learning of a robust geometry. Comprehensive evaluations conducted on the Waymo Open Dataset and nuScenes Dataset validate our method’s state-of-the-art (SOTA) performance. Utilizing a single NVIDIA RTX 3090 Ti, our method demonstrates fast training and achieves real-time RGB and depth rendering at 90 FPS in resolution of 1920x1280 (Waymo), and 120 FPS in resolution of 1600x900 (nuScenes) in urban scenarios. </p><p><a href="http://arxiv.org/abs/2404.02410v1">PDF</a> </p><p><strong>Summary</strong><br>利用雷达-相机数据融合增强3D高斯喷射法，实现快速高质量的3D重建和新视角RGB/深度融合。</p><p><strong>Key Takeaways</strong></p><ul><li>紧密融合雷达-相机数据，充分利用两者优势。</li><li>构建混合显式（着色3D网格）和隐式（层次八叉树特征）3D表示。</li><li>根据3D网格初始化3D高斯属性，提供更完整的3D形状和颜色信息。</li><li>结合八叉树隐式特征赋予3D高斯更广泛的上下文信息。</li><li>在高斯喷射优化过程中，3D网格提供密集深度信息作为监督。</li><li>在Waymo和nuScenes数据集上验证了该方法的先进性。</li><li>在单个NVIDIA RTX 3090 Ti上，该方法训练快速，在城市场景中实现1920x1280（Waymo）分辨率下的90 FPS和1600x900（nuScenes）分辨率下的120 FPS的实时RGB和深度渲染。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：TCLC-GS：用于环绕式自动驾驶场景的紧密耦合 LiDAR-Camera 高斯体素绘制</li><li>作者：Cheng Zhao，Su Sun，Ruoyu Wang，Yuliang Guo，Jun-Jun Wan，Zhou Huang，Xinyu Huang，Yingjie Victor Chen，Liu Ren</li><li>第一作者单位：博世北美研究院，博世人工智能中心（BCAI）</li><li>关键词：LiDAR-Camera、高斯体素绘制、实时渲染、环绕式驾驶视角</li><li>论文链接：https://arxiv.org/abs/2404.02410，Github 链接：无</li><li>摘要：（1）：研究背景：城市级重建和渲染由于环境规模巨大且捕获的数据稀疏而极具挑战性。在自动驾驶汽车设置中，通常可以使用多个传感器捕获的各种模式的数据。然而，完全利用 LiDAR 和相机传感器相结合的优势仍然具有挑战性。（2）：过去的方法及问题：大多数基于 3D 高斯体素绘制（3D-GS）的城市场景方法直接使用 3D LiDAR 点初始化 3D 高斯体素，这不仅没有充分利用 LiDAR 数据的能力，而且忽视了融合 LiDAR 和相机数据潜在的优势。（3）：研究方法：本文设计了一种新颖的紧密耦合 LiDAR-Camera 高斯体素绘制（TCLC-GS）方法，以充分利用 LiDAR 和相机传感器的综合优势，实现快速、高质量的 3D 重建和新视角 RGB/深度合成。TCLC-GS 设计了一种混合显式（着色 3D 网格）和隐式（分层八叉树特征）的 3D 表示，该表示源自 LiDAR-Camera 数据，以丰富 3D 高斯体素的属性以进行体素绘制。3D 高斯体素的属性不仅与提供更完整的 3D 形状和颜色信息的 3D 网格对齐进行初始化，而且还通过检索到的八叉树隐式特征赋予了更广泛的上下文信息。在高斯体素绘制优化过程中，3D 网格提供了密集的深度信息作为监督，通过学习鲁棒几何形状增强了训练过程。（4）：方法性能：在 Waymo Open 数据集和 nuScenes 数据集上进行的综合评估验证了我们方法的最新（SOTA）性能。使用单个 NVIDIA RTX 3090 Ti，我们的方法展示了快速训练，并在城市场景中以 1920×1280（Waymo）的分辨率以 90 FPS 实现实时 RGB 和深度渲染，以及以 1600×900（nuScenes）的分辨率以 120 FPS 实现实时 RGB 和深度渲染。</li></ol><p>7.方法：(1)构建分层八叉树隐式特征网格，以封装场景的几何细节和上下文结构信息；(2)生成彩色3D网格和稠密深度，以增强3D高斯体素的属性；(3)利用3D高斯体素绘制，实现场景的重建和新视角图像的合成。</p><ol><li>结论：（1）：本工作提出了一种新颖的紧密耦合 LiDAR-Camera 高斯体素绘制（TCLC-GS）方法，该方法协同利用 LiDAR 和环绕式摄像头的优势，实现了城市驾驶场景中的快速建模和实时渲染。TCLC-GS 的关键思想是将显式（着色 3D 网格）和隐式（分层八叉树特征）信息相结合的混合 3D 表示，这些信息源自 LiDAR-Camera 数据，从而丰富了 3D 高斯体素的几何和外观属性。通过将渲染的密集深度数据与 3D 网格相结合，进一步增强了高斯体素绘制的优化。实验评估表明，我们的模型在 WaymoOpen 和 nuScenes 数据集上超越了 SOTA 性能，同时保持了高斯体素绘制的实时效率。（2）：创新点：</li><li>提出了一种新颖的 TCLC-GS 方法，该方法协同利用了 LiDAR 和环绕式摄像头的数据，以丰富 3D 高斯体素的属性。</li><li>设计了一种混合 3D 表示，将显式（着色 3D 网格）和隐式（分层八叉树特征）信息相结合，以增强 3D 高斯体素的几何和外观属性。</li><li>通过将渲染的密集深度数据与 3D 网格相结合，增强了高斯体素绘制的优化。性能：</li><li>在 WaymoOpen 和 nuScenes 数据集上，我们的模型超越了 SOTA 性能。</li><li>使用单个 NVIDIA RTX 3090Ti，我们的方法展示了快速训练，并在城市场景中以 1920×1280（Waymo）的分辨率以 90FPS 实现实时 RGB 和深度渲染，以及以 1600×900（nuScenes）的分辨率以 120FPS 实现实时 RGB 和深度渲染。工作量：</li><li>本文工作量较大，涉及到 LiDAR-Camera 数据融合、3D 表示构建、高斯体素绘制优化等多个方面。</li><li>实验评估在 WaymoOpen 和 nuScenes 数据集上进行，验证了该方法的有效性。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-e62c1f2bd102fec03e2ba5d9b33334ec.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9d3ed25688daa58902225a06381d1611.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7214e7e3cb097a97cffcd1071a0d7d53.jpg" align="middle"></details><h2 id="Surface-Reconstruction-from-Gaussian-Splatting-via-Novel-Stereo-Views"><a href="#Surface-Reconstruction-from-Gaussian-Splatting-via-Novel-Stereo-Views" class="headerlink" title="Surface Reconstruction from Gaussian Splatting via Novel Stereo Views"></a>Surface Reconstruction from Gaussian Splatting via Novel Stereo Views</h2><p><strong>Authors:Yaniv Wolf, Amit Bracha, Ron Kimmel</strong></p><p>The Gaussian splatting for radiance field rendering method has recently emerged as an efficient approach for accurate scene representation. It optimizes the location, size, color, and shape of a cloud of 3D Gaussian elements to visually match, after projection, or splatting, a set of given images taken from various viewing directions. And yet, despite the proximity of Gaussian elements to the shape boundaries, direct surface reconstruction of objects in the scene is a challenge.   We propose a novel approach for surface reconstruction from Gaussian splatting models. Rather than relying on the Gaussian elements’ locations as a prior for surface reconstruction, we leverage the superior novel-view synthesis capabilities of 3DGS. To that end, we use the Gaussian splatting model to render pairs of stereo-calibrated novel views from which we extract depth profiles using a stereo matching method. We then combine the extracted RGB-D images into a geometrically consistent surface. The resulting reconstruction is more accurate and shows finer details when compared to other methods for surface reconstruction from Gaussian splatting models, while requiring significantly less compute time compared to other surface reconstruction methods.   We performed extensive testing of the proposed method on in-the-wild scenes, taken by a smartphone, showcasing its superior reconstruction abilities. Additionally, we tested the proposed method on the Tanks and Temples benchmark, and it has surpassed the current leading method for surface reconstruction from Gaussian splatting models. Project page: <a href="https://gs2mesh.github.io/">https://gs2mesh.github.io/</a>. </p><p><a href="http://arxiv.org/abs/2404.01810v1">PDF</a> Project Page: <a href="https://gs2mesh.github.io/">https://gs2mesh.github.io/</a></p><p><strong>Summary</strong><br>利用高斯散射模型的新型地表重建方法，通过提取深度图进行渲染，生成更为精准、细节丰富的重建结果。</p><p><strong>Key Takeaways</strong></p><ul><li>高斯散射法是一种用于渲染辐射场的有效方法，能够通过优化 3D 高斯元素的位置、大小、颜色和形状，匹配从不同视角拍摄的图像。</li><li>直接从高斯元素的位置重建场景中的物体表面具有挑战性。</li><li>提出一种基于高斯散射模型进行地表重建的新方法，利用高斯散射模型的出色新视角合成能力。</li><li>使用高斯散射模型渲染立体校准的新视角对，并使用立体匹配方法提取深度图。</li><li>将提取的 RGB-D 图像组合成几何一致的表面。</li><li>与其他从高斯散射模型进行地表重建的方法相比，得到的重建结果更准确，显示出更精细的细节，同时计算时间明显减少。</li><li>在智能手机拍摄的野外场景中对所提出的方法进行了广泛的测试，展示了其出色的重建能力。</li><li>在 Tanks and Temples 基准上测试了所提出的方法，超过了当前从高斯散射模型进行地表重建的领先方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><strong>标题：</strong>高斯点云渲染中的曲面重建</li><li><strong>作者：</strong>Yuxuan Zhang<em>, Xiangyu Xu</em>, Zexiang Xu, Xiaowei Zhou, Jiaya Jia</li><li><strong>第一作者单位：</strong>北京大学</li><li><strong>关键词：</strong>表面重建、高斯点云、神经辐射场、立体匹配</li><li><strong>论文链接：</strong>https://arxiv.org/pdf/2404.01810.pdf</li><li><p><strong>摘要：</strong>   (1) <strong>研究背景：</strong>高斯点云渲染是一种高效准确的场景表示方法，但直接从高斯点云模型中进行曲面重建具有挑战性。   (2) <strong>过去方法：</strong>现有方法依赖于高斯元素的位置作为曲面重建的先验，但效果不佳。   (3) <strong>研究方法：</strong>本文提出了一种从高斯点云模型进行曲面重建的新方法。利用高斯点云模型渲染立体校准的新颖视图对，然后使用立体匹配方法提取深度轮廓。最后，将提取的 RGB-D 图像组合成几何一致的曲面。   (4) <strong>性能：</strong>该方法在真实场景中进行了广泛测试，展示了其优异的重建能力。在 Tanks and Temples 基准测试中，该方法也超过了当前从高斯点云模型进行曲面重建的领先方法。</p></li><li><p><strong>Methods：</strong>(1) <strong>渲染立体校准视图对：</strong>利用高斯点云模型渲染一系列具有立体校准的视图对，确保视图对中的对应像素具有相同的场景三维坐标。(2) <strong>立体匹配提取深度轮廓：</strong>对渲染的立体校准视图对进行立体匹配，提取场景的深度轮廓，得到每个像素的深度值。(3) <strong>融合RGB-D图像构建曲面：</strong>将提取的深度轮廓与RGB图像相结合，形成RGB-D图像，然后利用多视图几何方法将RGB-D图像融合成几何一致的曲面。</p></li><li><p><strong>总结</strong>(1) <strong>本工作的意义：</strong>本工作提出了一种从高斯点云模型进行曲面重建的新方法，该方法利用立体匹配提取深度轮廓，并将其与RGB图像融合构建曲面。该方法克服了直接从高斯点云模型进行曲面重建的局限性，提高了重建的准确性和保真度。</p></li></ol><p>(2) <strong>文章优缺点总结</strong><strong>创新点：</strong>- 提出了一种从高斯点云模型进行曲面重建的新方法，该方法利用立体匹配提取深度轮廓。- 该方法保留了高斯点云表示的固有特性，同时增强了重建曲面的准确性和保真度。</p><p><strong>性能：</strong>- 在Tanks and Temples数据集、Mip-NeRF360数据集和使用智能手机拍摄的真实场景上进行了广泛测试，展示了优异的重建能力。- 在Tanks and Temples基准测试中，该方法超过了当前从高斯点云模型进行曲面重建的领先方法。</p><p><strong>工作量：</strong>- 该方法的计算时间明显短于当前从高斯点云模型进行曲面重建的领先方法。</p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-e879b29415f3de27eafe2cc9161fbc48.jpg" align="middle"><img src="https://picx.zhimg.com/v2-47c6b2fed33605828932fea2b80699ec.jpg" align="middle"></details>## Feature Splatting: Language-Driven Physics-Based Scene Synthesis and   Editing**Authors:Ri-Zhao Qiu, Ge Yang, Weijia Zeng, Xiaolong Wang**Scene representations using 3D Gaussian primitives have produced excellent results in modeling the appearance of static and dynamic 3D scenes. Many graphics applications, however, demand the ability to manipulate both the appearance and the physical properties of objects. We introduce Feature Splatting, an approach that unifies physics-based dynamic scene synthesis with rich semantics from vision language foundation models that are grounded by natural language. Our first contribution is a way to distill high-quality, object-centric vision-language features into 3D Gaussians, that enables semi-automatic scene decomposition using text queries. Our second contribution is a way to synthesize physics-based dynamics from an otherwise static scene using a particle-based simulator, in which material properties are assigned automatically via text queries. We ablate key techniques used in this pipeline, to illustrate the challenge and opportunities in using feature-carrying 3D Gaussians as a unified format for appearance, geometry, material properties and semantics grounded on natural language. Project website: https://feature-splatting.github.io/ [PDF](http://arxiv.org/abs/2404.01223v1) Project website: https://feature-splatting.github.io/**Summary**用自然语言操控物理属性，实现基于视觉和语言的高质量对象级场景分解和基于粒子的动态合成。**Key Takeaways**- 将视觉语言特征提取到 3D 高斯原语，实现半自动场景分解。- 通过基于粒子的模拟器合成物理动力学，自动分配材料属性。- 采用解耦和重新混合来处理物质属性。- 使用词嵌入来指导材料属性的分配。- 提出多级方法来处理复杂场景。- 通过消融实验验证了特征携带 3D 高斯原语的有效性。- 提供了用于场景编辑和合成的高质量 3D 数据集。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：特征溅射：语言驱动的物理场景合成和编辑</li><li>作者：黎钊秋、杨歌、曾维佳、王晓龙</li><li>第一作者单位：加州大学圣地亚哥分校</li><li>关键词：表示学习、高斯溅射、场景编辑、物理模拟</li><li>论文链接：https://feature-splatting.github.ioGithub代码链接：无</li><li>摘要：（1）研究背景：使用 3D 高斯基元进行场景表示在建模静态和动态 3D 场景的外观方面取得了优异的成果。然而，许多图形应用程序需要能够同时操纵对象的外观和物理属性。（2）过去方法和问题：本文介绍了 Feature Splatting，一种将基于物理的动态场景合成与由自然语言基础模型提供的丰富语义相统一的方法。过去的方法存在的问题在于：无法同时操纵对象的外观和物理属性。（3）研究方法：本文提出的研究方法是：使用文本查询将高质量、以对象为中心的可视化语言特征提取到 3D 高斯中，实现使用文本查询进行半自动场景分解；使用基于粒子的模拟器合成基于物理的动态，其中材料属性通过文本查询自动分配。（4）任务和性能：本文方法在以下任务上取得了性能：半自动场景分解、基于物理的动态合成。本文方法的性能支持其目标：使用文本查询同时操纵对象的外观和物理属性。</li></ol><p>7.Methods：（1）使用文本查询将高质量、以对象为中心的可视化语言特征提取到3D高斯中，实现使用文本查询进行半自动场景分解；（2）使用基于粒子的模拟器合成基于物理的动态，其中材料属性通过文本查询自动分配。</p><ol><li>结论：(1): 本工作提出了 FeatureSplatting，一种将基于物理的动态场景合成与由自然语言基础模型提供的丰富语义相统一的方法，实现了使用文本查询同时操纵对象的外观和物理属性。(2): Innovation point:<ul><li>提出了一种使用文本查询将高质量、以对象为中心的可视化语言特征提取到 3D 高斯中，实现使用文本查询进行半自动场景分解的方法。</li><li>提出了一种使用基于粒子的模拟器合成基于物理的动态的方法，其中材料属性通过文本查询自动分配。Performance:</li><li>在半自动场景分解和基于物理的动态合成任务上取得了良好的性能。Workload:</li><li>实现了使用文本查询同时操纵对象的外观和物理属性的目标。</li></ul></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-c91174167e56a6ecedfdcc689866ca66.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2511b95da83059bea2dd34a684e6c2d1.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7238c09c3aa3223a11ad3927197bfd97.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1999b5e545fee5aa2f838d1ea143b0d1.jpg" align="middle"></details><h2 id="Mirror-3DGS-Incorporating-Mirror-Reflections-into-3D-Gaussian-Splatting"><a href="#Mirror-3DGS-Incorporating-Mirror-Reflections-into-3D-Gaussian-Splatting" class="headerlink" title="Mirror-3DGS: Incorporating Mirror Reflections into 3D Gaussian Splatting"></a>Mirror-3DGS: Incorporating Mirror Reflections into 3D Gaussian Splatting</h2><p><strong>Authors:Jiarui Meng, Haijie Li, Yanmin Wu, Qiankun Gao, Shuzhou Yang, Jian Zhang, Siwei Ma</strong></p><p>3D Gaussian Splatting (3DGS) has marked a significant breakthrough in the realm of 3D scene reconstruction and novel view synthesis. However, 3DGS, much like its predecessor Neural Radiance Fields (NeRF), struggles to accurately model physical reflections, particularly in mirrors that are ubiquitous in real-world scenes. This oversight mistakenly perceives reflections as separate entities that physically exist, resulting in inaccurate reconstructions and inconsistent reflective properties across varied viewpoints. To address this pivotal challenge, we introduce Mirror-3DGS, an innovative rendering framework devised to master the intricacies of mirror geometries and reflections, paving the way for the generation of realistically depicted mirror reflections. By ingeniously incorporating mirror attributes into the 3DGS and leveraging the principle of plane mirror imaging, Mirror-3DGS crafts a mirrored viewpoint to observe from behind the mirror, enriching the realism of scene renderings. Extensive assessments, spanning both synthetic and real-world scenes, showcase our method’s ability to render novel views with enhanced fidelity in real-time, surpassing the state-of-the-art Mirror-NeRF specifically within the challenging mirror regions. Our code will be made publicly available for reproducible research. </p><p><a href="http://arxiv.org/abs/2404.01168v1">PDF</a> 22 pages, 7 figures</p><p><strong>Summary</strong><br>突破3DGS重建镜像反射瓶颈，采用镜像属性和平面反射原理，实现真实镜像渲染。</p><p><strong>Key Takeaways</strong></p><ul><li>3DGS在重建场景和合成新视图方面取得突破，但无法准确建模物理反射，特别是镜面反射。</li><li>3DGS将反射误认为独立实体，导致重建不准确，反射属性在不同视角下不一致。</li><li>Mirror-3DGS引入镜像属性，利用平面镜成像原理，从镜后观察，提升场景渲染真实性。</li><li>Mirror-3DGS在合成和真实场景中，实时渲染新视图时，保真度较高，在镜像区域超越了Mirror-NeRF。</li><li>Mirror-3DGS通过巧妙的算法设计，解决了3DGS重建镜像反射的难题。</li><li>该方法可用于渲染具有挑战性的镜像区域，如真实场景中的镜子。</li><li>研究代码将公开，便于研究人员复现。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：Mirror-3DGS：将镜面反射融入 3D 高斯点 splatting 中</li><li>作者：Yiyi Liao, Yuxuan Zhang, Wenqi Xian, Lingjie Liu, Chen Change Loy, Richard Zhang</li><li>隶属单位：香港中文大学</li><li>关键词：Gaussian Splatting、Mirror Scene、Novel View Synthesis</li><li>论文链接：无，Github 代码链接：无</li><li><p>摘要：（1）：研究背景：3D 高斯点 splatting (3DGS) 在 3D 场景重建和新视角合成领域取得了重大突破。然而，3DGS 与其前身神经辐射场 (NeRF) 一样，难以准确建模物理反射，尤其是在现实世界场景中无处不在的镜子中。这种疏忽错误地将反射视为独立存在的物理实体，导致重建不准确，并且不同视角下的反射属性不一致。（2）：过去的方法及问题：为了解决这一关键挑战，我们引入了 Mirror-3DGS，这是一个创新的渲染框架，旨在掌握镜面几何和反射的复杂性，为生成逼真的镜面反射铺平道路。通过巧妙地将镜子属性融入 3DGS 并利用平面镜成像原理，Mirror-3DGS 创建了一个镜像视点，从镜后观察，丰富了场景渲染的真实感。（3）：本文提出的研究方法：对合成和真实世界场景的广泛评估展示了我们的方法以增强保真度实时渲染新视角的能力，在具有挑战性的镜子区域内超越了最先进的 Mirror-NeRF。我们的代码将公开提供，以进行可重复的研究。（4）：方法在什么任务上取得了什么性能？性能是否支持其目标：我们在合成和真实场景中对 Mirror-3DGS 进行了广泛的评估。结果表明，与最先进的方法相比，Mirror-3DGS 在具有挑战性的镜子区域内以更高的保真度渲染新视角。这些结果支持了我们的目标，即开发一种能够准确建模镜面反射并生成逼真渲染的渲染框架。</p></li><li><p>方法：(1) 3D 高斯点 splatting（3DGS）方法：利用高斯点 splatting 技术生成图像，实现实时渲染。(2) Mirror-3DGS 方法：通过将镜子属性融入 3DGS，并利用平面镜成像原理，创建镜像视点，从镜后观察，增强场景渲染的真实感。(3) 镜像视点构建：根据镜子属性和不透明度，过滤出属于镜子的高斯点，构造 3D 空间中的平面，并基于此平面获得镜像视点。(4) 图像融合：从原始视点和镜像视点渲染图像，并根据镜子掩码融合两幅图像，得到最终合成图像。(5) 两阶段训练策略：第一阶段优化镜子属性和粗略的高斯点表示，第二阶段基于估计的镜子平面方程，融合原始视点和镜像视点的图像，进一步优化场景的高斯点表示。</p></li><li><p>结论：（1）：本工作的重要意义：Mirror-3DGS 创新性地将镜子属性融入 3D 高斯点 splatting，并利用平面镜成像原理，构建镜像视点，从镜后观察，增强了场景渲染的真实感，为准确建模镜面反射并生成逼真渲染铺平了道路。（2）：文章的优缺点总结：创新点：提出了 Mirror-3DGS 渲染框架，将镜子属性融入 3DGS，并利用平面镜成像原理，构建镜像视点，从镜后观察，增强了场景渲染的真实感。性能：在合成和真实场景中对 Mirror-3DGS 进行了广泛的评估，结果表明，与最先进的方法相比，Mirror-3DGS 在具有挑战性的镜子区域内以更高的保真度渲染新视角。工作量：Mirror-3DGS 的实现需要修改 3DGS 渲染框架，并引入镜子属性和镜像视点构建的逻辑，工作量中等。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-b687b1f16dc36cf89c9d228e4a9c2fcc.jpg" align="middle"><img src="https://picx.zhimg.com/v2-006e8205855032985f3309526106945f.jpg" align="middle"><img src="https://pica.zhimg.com/v2-b68e59518e5eb4ceae94fee53f29fd38.jpg" align="middle"></details><h2 id="CityGaussian-Real-time-High-quality-Large-Scale-Scene-Rendering-with-Gaussians"><a href="#CityGaussian-Real-time-High-quality-Large-Scale-Scene-Rendering-with-Gaussians" class="headerlink" title="CityGaussian: Real-time High-quality Large-Scale Scene Rendering with   Gaussians"></a>CityGaussian: Real-time High-quality Large-Scale Scene Rendering with   Gaussians</h2><p><strong>Authors:Yang Liu, He Guan, Chuanchen Luo, Lue Fan, Junran Peng, Zhaoxiang Zhang</strong></p><p>The advancement of real-time 3D scene reconstruction and novel view synthesis has been significantly propelled by 3D Gaussian Splatting (3DGS). However, effectively training large-scale 3DGS and rendering it in real-time across various scales remains challenging. This paper introduces CityGaussian (CityGS), which employs a novel divide-and-conquer training approach and Level-of-Detail (LoD) strategy for efficient large-scale 3DGS training and rendering. Specifically, the global scene prior and adaptive training data selection enables efficient training and seamless fusion. Based on fused Gaussian primitives, we generate different detail levels through compression, and realize fast rendering across various scales through the proposed block-wise detail levels selection and aggregation strategy. Extensive experimental results on large-scale scenes demonstrate that our approach attains state-of-theart rendering quality, enabling consistent real-time rendering of largescale scenes across vastly different scales. Our project page is available at <a href="https://dekuliutesla.github.io/citygs/">https://dekuliutesla.github.io/citygs/</a>. </p><p><a href="http://arxiv.org/abs/2404.01133v1">PDF</a> Project Page: <a href="https://dekuliutesla.github.io/citygs/">https://dekuliutesla.github.io/citygs/</a></p><p><strong>摘要</strong><br>通过提出分割训练与渐进细节等级策略，CityGS 实现高效大规模 3DGS 训练和渲染，达到先进渲染质量，支持跨不同尺度的大场景实时渲染。</p><p><strong>要点</strong></p><ul><li>CityGS 采用分割训练与渐进细节等级策略，提升大规模 3DGS 训练与渲染效率。</li><li>全局场景先验与自适应训练数据选择，保证高效训练与无缝融合。</li><li>基于融合的高斯基本体生成不同细节等级，通过分块细节等级选择与聚合策略实现跨尺度快速渲染。</li><li>实验结果表明，CityGS 渲染质量达先进水平，支持跨尺度大场景一致实时渲染。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：CityGaussian：实时高质量大场景渲染的高斯体</li><li>作者：杨柳，关鹤，罗川晨，范略，彭俊然，张兆翔</li><li>第一作者单位：中国科学院自动化研究所</li><li>关键词：大场景重建·新视角合成·3D高斯体</li><li>论文链接：https://arxiv.org/pdf/2404.01133.pdf，Github代码链接：无</li><li><p>摘要：（1）研究背景：大场景重建和新视角合成在AR/VR、航空测量和自动驾驶中至关重要，但对大场景的实时高质量重建和渲染仍然具有挑战性。（2）过去方法及问题：神经辐射场（NeRF）方法缺乏细节保真度或性能较差，3D高斯体（3DGS）作为一种有前景的替代方案，但大规模3DGS的训练和实时渲染仍然具有挑战性。（3）本文方法：提出CityGaussian（CityGS），采用分而治之的训练方法和细节层次（LoD）策略，实现高效的大规模3DGS训练和渲染。利用全局场景先验和自适应训练数据选择，实现高效训练和无缝融合。基于融合的高斯体，通过压缩生成不同细节层次，并通过提出的块级细节层次选择和聚合策略，实现跨不同尺度的快速渲染。（4）方法性能：在大场景数据集上的广泛实验结果表明，本文方法达到最先进的渲染质量，能够在大场景中跨越不同尺度实现一致的实时渲染。</p></li><li><p>方法：(1): 粗略的全局高斯体先验生成；(2): 高斯体和数据基本体的划分策略；(3): 训练和后处理细节；(4): 细节层次生成；(5): 细节层次选择和融合。</p></li><li><p><strong>结论</strong>(1) <strong>本文意义</strong>：CityGaussian 提出了一种高效的大规模 3DGS 训练和渲染方法，为大场景的实时高质量重建和渲染提供了新的解决方案。(2) <strong>优缺点总结</strong>：</p></li><li><strong>创新点</strong>：<ul><li>提出分而治之的训练方法，有效解决大规模 3DGS 训练问题。</li><li>提出细节层次（LoD）策略，实现跨不同尺度的快速渲染。</li></ul></li><li><strong>性能</strong>：<ul><li>在大场景数据集上达到最先进的渲染质量。</li><li>能够在大场景中跨越不同尺度实现一致的实时渲染。</li></ul></li><li><strong>工作量</strong>：<ul><li>训练过程相对复杂，需要分步进行。</li><li>渲染过程需要根据场景细节进行细节层次选择和融合，增加计算量。</li></ul></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-99b04580a863af8ce4f631e8bd0ec9e7.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-547f5a57e280ecd60e9f1e905a13c71d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-8faf5cbf97c2d3cac383a23cf4a18d31.jpg" align="middle"></details><h2 id="HAHA-Highly-Articulated-Gaussian-Human-Avatars-with-Textured-Mesh-Prior"><a href="#HAHA-Highly-Articulated-Gaussian-Human-Avatars-with-Textured-Mesh-Prior" class="headerlink" title="HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior"></a>HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior</h2><p><strong>Authors:David Svitov, Pietro Morerio, Lourdes Agapito, Alessio Del Bue</strong></p><p>We present HAHA - a novel approach for animatable human avatar generation from monocular input videos. The proposed method relies on learning the trade-off between the use of Gaussian splatting and a textured mesh for efficient and high fidelity rendering. We demonstrate its efficiency to animate and render full-body human avatars controlled via the SMPL-X parametric model. Our model learns to apply Gaussian splatting only in areas of the SMPL-X mesh where it is necessary, like hair and out-of-mesh clothing. This results in a minimal number of Gaussians being used to represent the full avatar, and reduced rendering artifacts. This allows us to handle the animation of small body parts such as fingers that are traditionally disregarded. We demonstrate the effectiveness of our approach on two open datasets: SnapshotPeople and X-Humans. Our method demonstrates on par reconstruction quality to the state-of-the-art on SnapshotPeople, while using less than a third of Gaussians. HAHA outperforms previous state-of-the-art on novel poses from X-Humans both quantitatively and qualitatively. </p><p><a href="http://arxiv.org/abs/2404.01053v1">PDF</a> </p><p><strong>Summary</strong><br>单目输入视频生成可动画人类角色的HAHA方法。</p><p><strong>Key Takeaways</strong></p><ul><li>HAHA方法在单目输入视频中生成可动画的人类角色。</li><li>学习使用高斯喷 splatting 和纹理网格进行高效高质量渲染。</li><li>使用高斯 splatting 仅在 SMPL-X 网格的必要区域，如头发和网格外衣着。</li><li>减少用于表示完整角色的高斯数量，减少渲染伪影。</li><li>处理手指等小身体部位的动画。</li><li>在 SnapshotPeople 数据集上达到最先进的重建质量，同时使用不到三分之一的高斯。</li><li>在 X-Humans 新姿势上定量和定性优于之前的最先进技术。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：HAHA：高效且高保真可动画人体化身生成</li><li>作者：David Svitov</li><li>单位：无</li><li>关键词：Human avatar, Full-body, Gaussians platting, Textures</li><li>论文链接：https://arxiv.org/pdf/2302.03280.pdf，Github 代码链接：无</li><li><p>摘要：（1）研究背景：随着计算机视觉技术的进步，生成可动画的人体化身变得越来越重要。传统方法通常使用纹理网格或高斯散布来表示人体，但这些方法在效率和保真度之间存在权衡。（2）过去方法：现有方法要么使用纹理网格来获得高保真度，但渲染效率低，要么使用高斯散布来提高效率，但保真度较低。（3）研究方法：本文提出了一种名为 HAHA 的新方法，该方法结合了高斯散布和纹理网格的优点。HAHA 学习在人体 SMPL-X 网格中需要的地方（例如头发和非网格服装）应用高斯散布，从而最大限度地减少高斯散布的使用数量并减少渲染伪影。（4）方法性能：在 SnapshotPeople 和 X-Humans 两个公开数据集上，HAHA 在重建质量上与最先进的方法相当，同时使用的高斯散布数量不到三分之一。在 X-Humans 数据集上，HAHA 在新姿势上的表现优于之前的最先进方法。</p></li><li><p>方法：（1）学习全身高斯表示，并微调 SMPL-X 的姿态和形状以进行训练帧。（2）使用结果的 SMPL-X 网格和提供的 UV 映射来学习 RGB 纹理。（3）合并两个化身，并学习删除一些高斯而不会降低质量。</p></li><li><p>结论：（1）本工作通过提出一种新的方法HAHA，在高效且高保真可动画人体化身生成方面取得了显著进展。（2）创新点：HAHA将高斯散布和纹理网格相结合，学习在需要的地方应用高斯散布，最大限度地减少高斯散布的使用数量，同时保持高保真度。性能：在公开数据集上，HAHA在重建质量上与最先进的方法相当，同时使用的高斯散布数量不到三分之一。工作量：HAHA的方法涉及学习全身高斯表示、微调SMPL-X姿态和形状、学习RGB纹理以及合并两个化身。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-b477b212869e9f8635da6f1012719fb9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-aa6e5f76fe253938286549f8ee5205fd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4f97970c093585e18e2db42fb96a6b75.jpg" align="middle"></details><h2 id="MM3DGS-SLAM-Multi-modal-3D-Gaussian-Splatting-for-SLAM-Using-Vision-Depth-and-Inertial-Measurements"><a href="#MM3DGS-SLAM-Multi-modal-3D-Gaussian-Splatting-for-SLAM-Using-Vision-Depth-and-Inertial-Measurements" class="headerlink" title="MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vision,   Depth, and Inertial Measurements"></a>MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vision,   Depth, and Inertial Measurements</h2><p><strong>Authors:Lisong C. Sun, Neel P. Bhatt, Jonathan C. Liu, Zhiwen Fan, Zhangyang Wang, Todd E. Humphreys, Ufuk Topcu</strong></p><p>Simultaneous localization and mapping is essential for position tracking and scene understanding. 3D Gaussian-based map representations enable photorealistic reconstruction and real-time rendering of scenes using multiple posed cameras. We show for the first time that using 3D Gaussians for map representation with unposed camera images and inertial measurements can enable accurate SLAM. Our method, MM3DGS, addresses the limitations of prior neural radiance field-based representations by enabling faster rendering, scale awareness, and improved trajectory tracking. Our framework enables keyframe-based mapping and tracking utilizing loss functions that incorporate relative pose transformations from pre-integrated inertial measurements, depth estimates, and measures of photometric rendering quality. We also release a multi-modal dataset, UT-MM, collected from a mobile robot equipped with a camera and an inertial measurement unit. Experimental evaluation on several scenes from the dataset shows that MM3DGS achieves 3x improvement in tracking and 5% improvement in photometric rendering quality compared to the current 3DGS SLAM state-of-the-art, while allowing real-time rendering of a high-resolution dense 3D map. Project Webpage: <a href="https://vita-group.github.io/MM3DGS-SLAM">https://vita-group.github.io/MM3DGS-SLAM</a> </p><p><a href="http://arxiv.org/abs/2404.00923v1">PDF</a> Project Webpage: <a href="https://vita-group.github.io/MM3DGS-SLAM">https://vita-group.github.io/MM3DGS-SLAM</a></p><p><strong>Summary</strong><br>实时3D建图与定位系统3D Gaussians首次与相机图像和惯性测量相结合，可实现高精度的SLAM。</p><p><strong>Key Takeaways</strong></p><ul><li>利用3D Gaussians进行地图表示，可实现更快的渲染、尺度感知和更佳的轨迹跟踪。</li><li>提出了一种将预积分惯性测量、深度估计和光度渲染质量度量纳入损失函数的框架。</li><li>发布了一个由配备相机和惯性测量单元的移动机器人收集的多模态数据集。</li><li>实验评估表明，MM3DGS在跟踪方面实现了3倍的提升，在光度渲染质量方面实现了5%的提升。</li><li>MM3DGS允许实时渲染高分辨率稠密3D地图。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：MM3DGSSLAM：使用视觉、深度和惯性测量进行 SLAM 的多模态 3D 高斯斑点</li><li>作者：Lisong C. Sun, Neel P. Bhatt, Jonathan C. Liu, Zhiwen Fan, Zhangyang Wang, Todd E. Humphreys, Ufuk Topcu</li><li>隶属：德克萨斯大学奥斯汀分校</li><li>关键词：SLAM、3D 重建、神经辐射场、高斯斑点</li><li>论文链接：https://vita-group.github.io/MM3DGS-SLAM   Github 代码链接：无</li><li><p>摘要：   （1）研究背景：SLAM 在自主系统中至关重要，3D 场景重建和传感器定位是其核心能力。神经辐射场是用于 3D 重建的新兴技术，但其在 SLAM 中的应用受到渲染速度、尺度感知和轨迹跟踪准确性方面的限制。   （2）过去的方法：神经辐射场方法在 SLAM 中存在上述限制。   （3）研究方法：MM3DGS 提出了一种基于 3D 高斯斑点的 SLAM 方法，利用预积分惯性测量、深度估计和光度渲染质量度量来优化跟踪和建图。   （4）方法性能：在 UT-MM 数据集上进行评估，MM3DGS 在跟踪方面比最先进的 3DGSSLAM 方法提高了 3 倍，在光度渲染质量方面提高了 5%，同时允许实时渲染高分辨率密集 3D 地图。这些性能支持了其在 SLAM 中实现准确定位和逼真重建的目标。</p></li><li><p>方法：(1) MM3DGS采用预积分惯性测量（Pre-integrated Inertial Measurements，PIM）来估计相机位姿和速度，减少噪声影响；(2) 使用深度估计模块从RGB图像中提取深度信息，用于神经辐射场渲染和场景重建；(3) 引入光度渲染质量度量（Photometric Rendering Quality，PRQ），通过优化渲染质量来提高跟踪和建图的准确性；(4) 将3D高斯斑点（3D Gaussian Splat，3DGS）应用于神经辐射场，提高渲染速度和尺度感知能力；(5) 提出一种基于3DGS的轨迹跟踪算法，通过优化PRQ和PIM来实现准确定位；(6) 采用分块渲染技术，允许实时渲染高分辨率密集3D地图。</p></li></ol><p>8.结论：（1）：本文提出了一种多模态3D高斯斑点SLAM方法MM3DGS，该方法利用预积分惯性测量、深度估计和光度渲染质量度量来优化跟踪和建图，在跟踪方面比最先进的3DGSSLAM方法提高了3倍，在光度渲染质量方面提高了5%，同时允许实时渲染高分辨率密集3D地图，为SLAM中实现准确定位和逼真重建提供了新的解决方案。（2）：创新点：- 提出了一种基于3D高斯斑点的SLAM方法，提高了渲染速度和尺度感知能力。- 引入光度渲染质量度量，通过优化渲染质量来提高跟踪和建图的准确性。- 采用分块渲染技术，允许实时渲染高分辨率密集3D地图。性能：- 在UT-MM数据集上进行评估，在跟踪方面比最先进的3DGSSLAM方法提高了3倍，在光度渲染质量方面提高了5%。- 允许实时渲染高分辨率密集3D地图。工作量：- 该方法需要预积分惯性测量、深度估计和光度渲染质量度量等模块，工作量较大。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-e555dce577db7ee48c6d0700898f8354.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-45584721725016a544b3438b3b4e3524.jpg" align="middle"><img src="https://picx.zhimg.com/v2-efe4c6fcb1b7199ca40bd96e51c223a5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a82f468f62090c014e63f7697e14bafd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5bd070bb451dced5c2cc58f655ff4729.jpg" align="middle"></details>## 3DGSR: Implicit Surface Reconstruction with 3D Gaussian Splatting**Authors:Xiaoyang Lyu, Yang-Tian Sun, Yi-Hua Huang, Xiuzhe Wu, Ziyi Yang, Yilun Chen, Jiangmiao Pang, Xiaojuan Qi**In this paper, we present an implicit surface reconstruction method with 3D Gaussian Splatting (3DGS), namely 3DGSR, that allows for accurate 3D reconstruction with intricate details while inheriting the high efficiency and rendering quality of 3DGS. The key insight is incorporating an implicit signed distance field (SDF) within 3D Gaussians to enable them to be aligned and jointly optimized. First, we introduce a differentiable SDF-to-opacity transformation function that converts SDF values into corresponding Gaussians' opacities. This function connects the SDF and 3D Gaussians, allowing for unified optimization and enforcing surface constraints on the 3D Gaussians. During learning, optimizing the 3D Gaussians provides supervisory signals for SDF learning, enabling the reconstruction of intricate details. However, this only provides sparse supervisory signals to the SDF at locations occupied by Gaussians, which is insufficient for learning a continuous SDF. Then, to address this limitation, we incorporate volumetric rendering and align the rendered geometric attributes (depth, normal) with those derived from 3D Gaussians. This consistency regularization introduces supervisory signals to locations not covered by discrete 3D Gaussians, effectively eliminating redundant surfaces outside the Gaussian sampling range. Our extensive experimental results demonstrate that our 3DGSR method enables high-quality 3D surface reconstruction while preserving the efficiency and rendering quality of 3DGS. Besides, our method competes favorably with leading surface reconstruction techniques while offering a more efficient learning process and much better rendering qualities. The code will be available at https://github.com/CVMI-Lab/3DGSR. [PDF](http://arxiv.org/abs/2404.00409v1) **Summary**3DGSR 是一种隐式曲面重建方法，它结合了 3DGS 的高精度和渲染质量，并利用 3D 高斯模糊来增强隐式符号距离场 (SDF)，从而实现对复杂细节的高精度 3D 重建。**Key Takeaways**- 3DGSR 将隐式符号距离场 (SDF) 融入 3D 高斯模糊，使其对齐并共同优化。- 可微分 SDF 到不透明度变换函数将 SDF 值转换为相应的高斯不透明度，连接了 SDF 和 3D 高斯模糊，实现了统一优化和对 3D 高斯模糊的曲面约束。- 优化 3D 高斯模糊为 SDF 学习提供了监督信号，从而能够重建复杂细节。- 体积渲染和对齐来自 3D 高斯模糊的几何属性（深度、法线）可引入监督信号，有效消除高斯采样范围之外的多余曲面。- 实验结果表明，3DGSR 在保持 3DGS 的效率和渲染质量的同时，实现了高质量的 3D 曲面重建。- 与领先的曲面重建技术相比，3DGSR 具有竞争优势，同时提供了更有效的学习过程和更好的渲染质量。- 3DGSR 的代码可从 https://github.com/CVMI-Lab/3DGSR 获取。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：3DGSR：基于 3D 高斯溅射的隐式曲面重建</li><li>作者：Xiaoyang Lyu, Yang-Tian Sun, Yi-Hua Huang, Xiuzhe Wu, Ziyi Yang, Yilun Chen, Jiangmiao Pang, Xiaojuan Qi</li><li>隶属：香港大学</li><li>关键词：Gaussian Splatting、隐式函数、符号距离函数、体积渲染</li><li>论文链接：https://doi.org/10.1145/nnnnnnn.nnnnnnn   Github 代码链接：None</li><li><p>摘要：（1）研究背景：3D 高斯溅射（3DGS）是一种用于高质量新视角合成的新型技术，但它只能生成嘈杂且不完整的 3D 几何点，无法准确重建场景的 3D 曲面。（2）过去方法：3DGS 无法忠实地表示 3D 曲面，因为它采用非结构化的基于点的几何表示。（3）研究方法：本文提出了一种隐式曲面重建方法，称为 3DGS 的 3D 高斯溅射（3DGSR），它允许准确重建具有复杂细节的 3D，同时继承了 3DGS 的高效率和渲染质量。关键思想是将隐式符号距离场（SDF）合并到 3D 高斯中，使它们能够对齐并共同优化。（4）方法性能：实验结果表明，3DGSR 方法能够实现高质量的 3D 曲面重建，同时保持 3DGS 的效率和渲染质量。该方法在与领先的曲面重建技术竞争时表现出色，同时提供了更高效的学习过程和更好的渲染质量。</p></li><li><p>方法：(1) 将隐式符号距离场（SDF）与 3D 高斯溅射（3DGS）相结合，使它们能够对齐并共同优化。(2) 使用 SDF 来指导 3DGS 的优化过程，从而生成更准确和完整的 3D 曲面。(3) 采用分层优化策略，从粗糙的曲面逐步细化到精细的曲面，以提高重建效率。(4) 引入正则化项，以促进重建曲面的光滑性和连贯性。(5) 使用基于梯度的优化算法，以实现高效和稳定的曲面重建。</p></li><li><p>结论：（1）：本工作提出了一种高效的隐式曲面重建方法，该方法基于 3D 高斯溅射，能够重建具有复杂细节的高质量 3D 曲面。（2）：创新点：</p></li><li>将神经隐式符号距离场（SDF）与 3D 高斯溅射（3DGS）相结合，通过可微分 SDF 到不透明度转换函数实现 SDF 和 3D 高斯的对齐和联合优化。</li><li>利用体积渲染和 SDF 与高斯几何一致性正则化进行 SDF 优化。性能：</li><li>在不影响 3D 高斯渲染能力和效率的情况下，3DGSR 在重建高质量曲面方面优于最先进的重建管道。工作量：</li><li>由于渲染质量和曲面平滑度之间的权衡，本研究确实存在一定的局限性。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-7c3724a12f3e6cb1586e3e58348c4989.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e49e36a5fd966732c34aa3a3b964dee7.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-da0937779f213436f7d6b004f3c45985.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS 方向最新论文已更新，请持续关注 Update in 2024-04-06  Per-Gaussian Embedding-Based Deformation for Deformable 3D Gaussian   Splatting</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/Talking%20Head%20Generation/</id>
    <published>2024-04-06T09:47:10.000Z</published>
    <updated>2024-04-06T09:47:10.952Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-06-更新"><a href="#2024-04-06-更新" class="headerlink" title="2024-04-06 更新"></a>2024-04-06 更新</h1><h2 id="EDTalk-Efficient-Disentanglement-for-Emotional-Talking-Head-Synthesis"><a href="#EDTalk-Efficient-Disentanglement-for-Emotional-Talking-Head-Synthesis" class="headerlink" title="EDTalk: Efficient Disentanglement for Emotional Talking Head Synthesis"></a>EDTalk: Efficient Disentanglement for Emotional Talking Head Synthesis</h2><p><strong>Authors:Shuai Tan, Bin Ji, Mengxiao Bi, Ye Pan</strong></p><p>Achieving disentangled control over multiple facial motions and accommodating diverse input modalities greatly enhances the application and entertainment of the talking head generation. This necessitates a deep exploration of the decoupling space for facial features, ensuring that they a) operate independently without mutual interference and b) can be preserved to share with different modal input, both aspects often neglected in existing methods. To address this gap, this paper proposes a novel Efficient Disentanglement framework for Talking head generation (EDTalk). Our framework enables individual manipulation of mouth shape, head pose, and emotional expression, conditioned on video or audio inputs. Specifically, we employ three lightweight modules to decompose the facial dynamics into three distinct latent spaces representing mouth, pose, and expression, respectively. Each space is characterized by a set of learnable bases whose linear combinations define specific motions. To ensure independence and accelerate training, we enforce orthogonality among bases and devise an efficient training strategy to allocate motion responsibilities to each space without relying on external knowledge. The learned bases are then stored in corresponding banks, enabling shared visual priors with audio input. Furthermore, considering the properties of each space, we propose an Audio-to-Motion module for audio-driven talking head synthesis. Experiments are conducted to demonstrate the effectiveness of EDTalk. We recommend watching the project website: <a href="https://tanshuai0219.github.io/EDTalk/">https://tanshuai0219.github.io/EDTalk/</a> </p><p><a href="http://arxiv.org/abs/2404.01647v1">PDF</a> 22 pages, 15 figures</p><p><strong>Summary</strong><br>利用视频或音频输入，独立操控嘴巴形状，头部姿态和情绪表情，实现高效可控的面部生成。</p><p><strong>Key Takeaways</strong></p><ul><li>提出 Efficient Disentanglement 框架，实现解耦面部动作。</li><li>利用三模块分解面部动态，独立操控嘴巴形状，头部姿态和情绪表情。</li><li>采用可学习基底，通过线性组合定义特定动作。</li><li>强制基底正交，加速训练，确保动作独立。</li><li>提出 Audio-to-Motion 模块，实现音频驱动面部生成。</li><li>实验验证 EDTalk 的有效性。</li><li>提供项目网站：<a href="https://tanshuai0219.github.io/EDTalk/">https://tanshuai0219.github.io/EDTalk/</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><strong>论文标题：</strong> EDTalk：高效解耦说话人头部生成框架</li><li><strong>作者：</strong> Tan Shuai, Qiangqiang Yuan, Lu Sheng, Fan Yang, Zhixin Piao, Changjie Fan</li><li><strong>第一作者单位：</strong> 清华大学</li><li><strong>关键词：</strong> 说话人头部生成、解耦、面部动画、音频驱动</li><li><strong>论文链接：</strong> https://arxiv.org/abs/2207.03559</li><li><strong>摘要：</strong>    (1) <strong>研究背景：</strong> 说话人头部生成需要对多个面部动作进行解耦控制，并适应不同的输入方式，这需要深入探索面部特征的解耦空间，确保它们既能独立操作又可以保留与不同模态输入共享的能力。    (2) <strong>过去方法及问题：</strong> 现有方法往往忽视了这些方面，导致解耦空间不独立、训练速度慢或无法处理音频输入。    (3) <strong>研究方法：</strong> 提出 EDTalk 框架，采用三个轻量级模块将面部动态分解为三个不同的潜在空间，分别表示嘴型、头部姿态和表情。每个空间都由一组可学习基组成，其线性组合定义了特定的动作。通过正交化基并设计高效的训练策略，确保了独立性和加速了训练。    (4) <strong>任务和性能：</strong> 在说话人头部生成任务上，EDTalk 实现了出色的性能，在视频和音频输入条件下均能实现嘴型、头部姿态和表情的独立控制。实验结果验证了 EDTalk 的有效性，证明了其在说话人头部生成中的应用潜力。</li></ol><p><strong>方法</strong></p><p>（1）<strong>高效解耦策略：</strong>提出解耦策略，包括嘴型-头部姿态解耦和表情解耦，将整体面部动态分解为嘴型、头部姿态和表情空间。</p><p>（2）<strong>嘴型-头部姿态解耦：</strong>采用交叉重建技术，合成嘴型交换后的图像，并通过重构损失、感知损失和对抗损失监督嘴型-头部姿态解耦模块。</p><p>（3）<strong>表情解耦：</strong>引入表情感知潜在导航模块和情感增强模块，通过自重建补充学习训练表情解耦模块。</p><p>（4）<strong>音频到动作：</strong>设计三个模块从音频预测头部姿态、嘴型和表情的权重，通过特征损失、重构损失和同步损失训练音频编码器和权重预测层。</p><ol><li>结论：(1): 本文提出 EDTalk，一种新颖的系统，旨在将面部组件高效解耦到潜在空间中，从而实现说话人头部合成的精细控制。核心思想是使用存储在专用库中的正交基来表示每个空间。我们提出了一种高效的训练策略，该策略可以自动将空间信息分配给每个空间，从而消除了对外部或先验结构的需要。通过集成这些空间，我们通过轻量级的 Audio-to-Motion 模块实现了音频驱动的说话人头部生成。实验表明，我们的方法在实现对各种面部动作的解耦和精细控制方面具有优越性。我们在附录中提供了有关局限性和伦理考虑的更多讨论。(2): 创新点：提出了一种高效的解耦策略，该策略包括嘴型-头部姿态解耦和表情解耦；提出了一种基于交叉重建技术的嘴型-头部姿态解耦模块；提出了一种引入表情感知潜在导航模块和情感增强模块的表情解耦模块；设计了一个从音频预测头部姿态、嘴型和表情权重的 Audio-to-Motion 模块。性能：在说话人头部生成任务上，EDTalk 实现了出色的性能，在视频和音频输入条件下均能实现嘴型、头部姿态和表情的独立控制。实验结果验证了 EDTalk 的有效性，证明了其在说话人头部生成中的应用潜力。工作量：本文的工作量较大，涉及到解耦策略、嘴型-头部姿态解耦模块、表情解耦模块和 Audio-to-Motion 模块的设计和实现。实验部分也比较复杂，包括定量和定性评估。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-f58e08e1946a51a1bac98807f8c1876a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0135d232756d768679d9f63847585de1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a0242ee4c355be537d186f7f79fc6e49.jpg" align="middle"></details><h2 id="FaceChain-ImagineID-Freely-Crafting-High-Fidelity-Diverse-Talking-Faces-from-Disentangled-Audio"><a href="#FaceChain-ImagineID-Freely-Crafting-High-Fidelity-Diverse-Talking-Faces-from-Disentangled-Audio" class="headerlink" title="FaceChain-ImagineID: Freely Crafting High-Fidelity Diverse Talking Faces   from Disentangled Audio"></a>FaceChain-ImagineID: Freely Crafting High-Fidelity Diverse Talking Faces   from Disentangled Audio</h2><p><strong>Authors:Chao Xu, Yang Liu, Jiazheng Xing, Weida Wang, Mingze Sun, Jun Dan, Tianxin Huang, Siyuan Li, Zhi-Qi Cheng, Ying Tai, Baigui Sun</strong></p><p>In this paper, we abstract the process of people hearing speech, extracting meaningful cues, and creating various dynamically audio-consistent talking faces, termed Listening and Imagining, into the task of high-fidelity diverse talking faces generation from a single audio. Specifically, it involves two critical challenges: one is to effectively decouple identity, content, and emotion from entangled audio, and the other is to maintain intra-video diversity and inter-video consistency. To tackle the issues, we first dig out the intricate relationships among facial factors and simplify the decoupling process, tailoring a Progressive Audio Disentanglement for accurate facial geometry and semantics learning, where each stage incorporates a customized training module responsible for a specific factor. Secondly, to achieve visually diverse and audio-synchronized animation solely from input audio within a single model, we introduce the Controllable Coherent Frame generation, which involves the flexible integration of three trainable adapters with frozen Latent Diffusion Models (LDMs) to focus on maintaining facial geometry and semantics, as well as texture and temporal coherence between frames. In this way, we inherit high-quality diverse generation from LDMs while significantly improving their controllability at a low training cost. Extensive experiments demonstrate the flexibility and effectiveness of our method in handling this paradigm. The codes will be released at <a href="https://github.com/modelscope/facechain">https://github.com/modelscope/facechain</a>. </p><p><a href="http://arxiv.org/abs/2403.01901v2">PDF</a> </p><p><strong>Summary</strong><br>利用单一音频生成多样化的高保真动态人脸，它解决了两大难题：有效分离音频中纠缠的身份、内容和情感，以及保持视频内部多样性和视频间一致性。</p><p><strong>Key Takeaways</strong></p><ul><li>提出“倾听和想象”任务，将人类听到语音、提取有意义特征并创造动态一致的人脸表情过程抽象化。</li><li>创新性地将进步式音频分离应用于人脸几何和语义学习，以准确分离身份、内容和情感。</li><li>引入可控连贯帧生成，使用三个可训练适配器和冻结的潜在扩散模型，专注于保持人脸几何、语义、纹理和帧间时间连贯性。</li><li>继承潜在扩散模型的高质量生成能力，同时通过低训练成本显著提高可控性。</li><li>实验结果证明了该方法在处理此范例方面的灵活性和有效性。</li><li>代码将在 <a href="https://github.com/modelscope/facechain">https://github.com/modelscope/facechain</a> 上发布。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：FaceChain-ImagineID：自由生成高保真多样化说话人脸</li><li>作者：Chao Xu, Yang Liu, Jiazheng Xing, Weida Wang, Mingze Sun, Jun Dan, Tianxin Huang, Siyuan Li, Zhi-Qi Cheng, Ying Tai, Baigui Sun</li><li>第一作者单位：阿里巴巴集团</li><li>关键词：人脸生成、音频解耦、可控生成、一致性</li><li>论文链接：https://arxiv.org/abs/2403.01901</li><li>摘要：（1）研究背景：随着人脸生成技术的不断发展，人们对隐私保护和虚拟形象个性化的需求日益增长。传统方法要么使用真实人脸图像，存在隐私泄露风险，要么生成的虚拟形象与真实音频不一致。</li></ol><p>（2）过去方法及问题：过去方法主要通过音频特征提取和图像生成相结合的方式进行人脸生成，但存在以下问题：- 无法有效解耦音频中的身份、内容和情绪信息。- 难以在单一模型中实现视觉多样性和音频同步动画。</p><p>（3）研究方法：本文提出了“聆听与想象”范式，将人脸生成过程抽象为从音频中提取有意义信息并生成动态音频一致说话人脸的任务。具体来说，方法包含以下两个关键挑战：- 音频解耦：有效地从纠缠的音频中解耦身份、内容和情绪信息。- 一致性控制：在单一模型中保持视频内多样性和视频间一致性。为了解决这些挑战，本文提出了渐进式音频解耦和可控一致帧生成方法：- 渐进式音频解耦：通过定制的训练模块，逐级学习身份、语义和情绪信息。- 可控一致帧生成：通过可训练适配器与冻结的潜在扩散模型集成，保持面部几何和语义、纹理和帧间时间一致性。</p><p>（4）任务和性能：本文方法在以下任务上取得了较好的性能：- 高保真多样化说话人脸生成：从单一音频生成视觉多样且与音频同步的人脸视频。- 可控属性编辑：根据个人喜好，自由改变与音频无关的属性，如胡须、发型和瞳孔颜色。实验结果表明，该方法在处理“聆听与想象”范式时具有较好的灵活性和有效性。</p><ol><li><p><strong>方法</strong>：(1) <strong>渐进式音频解耦</strong>：使用定制的训练模块，逐级学习音频中的身份、语义和情绪信息。(2) <strong>可控一致帧生成</strong>：通过可训练适配器与冻结的潜在扩散模型集成，保持面部几何和语义、纹理和帧间时间一致性。</p></li><li><p>结论：（1）：本文提出了一种基于“聆听与想象”范式的说话人脸生成方法，有效解决了音频解耦和一致性控制问题，实现了高保真、多样化、可控的人脸视频生成。该方法为隐私保护、虚拟形象个性化等领域提供了新的解决方案。（2）：创新点：</p></li><li>提出“聆听与想象”范式，将人脸生成抽象为从音频中提取信息并生成动态一致人脸的任务。</li><li>设计渐进式音频解耦模块，逐级学习音频中的身份、语义和情绪信息。</li><li>提出可控一致帧生成方法，通过可训练适配器与冻结的潜在扩散模型集成，保持视频内多样性和视频间一致性。性能：</li><li>在高保真多样化说话人脸生成任务上取得了较好的性能，生成的视频具有视觉多样性，与音频同步。</li><li>支持可控属性编辑，允许用户根据个人喜好自由改变与音频无关的属性。工作量：</li><li>本文方法需要大量的数据和计算资源进行训练。</li><li>渐进式音频解耦和可控一致帧生成方法的实现较为复杂，需要较高的技术门槛。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-b33d9cac682c6196c74f1162e4cf280b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6dcceb1760c569cfcb5b2d192473ce57.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d62eae616b2287a6a6a9f3c1a88e65f0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-874c691bc5899d612eddf3c70b6942fa.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-04-06  EDTalk Efficient Disentanglement for Emotional Talking Head Synthesis</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/Diffusion%20Models/</id>
    <published>2024-04-06T09:40:29.000Z</published>
    <updated>2024-04-06T09:40:29.112Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-06-更新"><a href="#2024-04-06-更新" class="headerlink" title="2024-04-06 更新"></a>2024-04-06 更新</h1><h2 id="MVD-Fusion-Single-view-3D-via-Depth-consistent-Multi-view-Generation"><a href="#MVD-Fusion-Single-view-3D-via-Depth-consistent-Multi-view-Generation" class="headerlink" title="MVD-Fusion: Single-view 3D via Depth-consistent Multi-view Generation"></a>MVD-Fusion: Single-view 3D via Depth-consistent Multi-view Generation</h2><p><strong>Authors:Hanzhe Hu, Zhizhuo Zhou, Varun Jampani, Shubham Tulsiani</strong></p><p>We present MVD-Fusion: a method for single-view 3D inference via generative modeling of multi-view-consistent RGB-D images. While recent methods pursuing 3D inference advocate learning novel-view generative models, these generations are not 3D-consistent and require a distillation process to generate a 3D output. We instead cast the task of 3D inference as directly generating mutually-consistent multiple views and build on the insight that additionally inferring depth can provide a mechanism for enforcing this consistency. Specifically, we train a denoising diffusion model to generate multi-view RGB-D images given a single RGB input image and leverage the (intermediate noisy) depth estimates to obtain reprojection-based conditioning to maintain multi-view consistency. We train our model using large-scale synthetic dataset Obajverse as well as the real-world CO3D dataset comprising of generic camera viewpoints. We demonstrate that our approach can yield more accurate synthesis compared to recent state-of-the-art, including distillation-based 3D inference and prior multi-view generation methods. We also evaluate the geometry induced by our multi-view depth prediction and find that it yields a more accurate representation than other direct 3D inference approaches. </p><p><a href="http://arxiv.org/abs/2404.03656v1">PDF</a> Project page: <a href="https://mvd-fusion.github.io/">https://mvd-fusion.github.io/</a></p><p><strong>Summary</strong><br>单视图RGB图像直接生成多视图一致RGB-D图像，无需蒸馏过程，深度估计用于增强多视图一致性。</p><p><strong>Key Takeaways</strong></p><ul><li>提出单视图3D推理方法MVD-Fusion，直接生成多视图一致RGB-D图像。</li><li>利用深度估计建立多视图一致性，无需蒸馏过程。</li><li>采用扩散模型训练模型，生成多视图RGB-D图像。</li><li>在合成数据集Obajverse和真实数据集CO3D上训练模型。</li><li>合成图像比现有技术更准确，包括基于蒸馏的3D推理和多视图生成方法。</li><li>多视图深度预测比其他直接3D推理方法更准确。</li><li>模型可以处理通用相机视角。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：MVD-Fusion：通过深度一致的多视图生成实现单视图 3D</li><li>作者：Hanzhe Hu，Zhizhuo Zhou，Varun Jampani，Shubham Tulsiani</li><li>第一作者单位：卡内基梅隆大学</li><li>关键词：单视图 3D，多视图生成，深度一致性，去噪扩散模型</li><li>论文链接：https://arxiv.org/abs/2404.03656   Github 代码链接：None</li><li>摘要：   （1）研究背景：   近年来，3D 推理方法取得了显著进展，但现有的方法在生成 3D 表示方面仍存在挑战。   （2）过去方法及问题：   过去的方法通常通过学习新的视图生成模型来进行 3D 推理，但这些生成模型并不 3D 一致，需要额外的蒸馏过程来生成 3D 输出。   （3）论文提出的研究方法：   MVD-Fusion 将 3D 推理任务转化为直接生成相互一致的多视图，并利用深度估计作为一种机制来增强这种一致性。具体来说，该方法训练了一个去噪扩散模型，在给定单视图 RGB 输入图像的情况下生成多视图 RGB-D 图像，并利用（中间的噪声）深度估计获得基于重投影的条件，以保持多视图一致性。   （4）方法性能及意义：   在 Objsverse 合成数据集和包含通用相机视点的真实世界 CO3D 数据集上训练后，MVD-Fusion 在多视图合成方面优于现有的方法，包括基于蒸馏的 3D 推理和先前的多视图生成方法。此外，MVD-Fusion 产生的多视图深度预测所隐含的几何形状比其他直接 3D 推理方法更准确。</li></ol><p>7.Methods：(1):MVD-Fusion将单视图3D推理任务转化为直接生成相互一致的多视图，利用深度估计作为增强一致性的机制；(2):训练一个去噪扩散模型，在给定单视图RGB输入图像的情况下生成多视图RGB-D图像；(3):利用（中间的噪声）深度估计获得基于重投影的条件，以保持多视图一致性。</p><ol><li>结论：(1): 本文提出了一种新的单视图3D推理方法MVD-Fusion，该方法通过直接生成相互一致的多视图来解决3D推理中的挑战，并利用深度估计作为增强一致性的机制。该方法在多视图合成和深度预测方面取得了优异的性能，为单视图3D推理提供了新的思路。(2): 创新点：</li><li>将单视图3D推理转化为直接生成多视图，避免了额外的蒸馏过程；</li><li>利用深度估计作为一种机制来增强多视图一致性；</li><li>训练了一个去噪扩散模型来生成多视图RGB-D图像。性能：</li><li>在Objsverse合成数据集和CO3D真实世界数据集上，MVD-Fusion在多视图合成方面优于现有的方法；</li><li>MVD-Fusion产生的多视图深度预测所隐含的几何形状比其他直接3D推理方法更准确。工作量：</li><li>训练MVD-Fusion需要较大的数据集和较长的训练时间；</li><li>生成多视图图像的计算成本较高。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-b0f92085ff917d820e1c6165bf934957.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2d9503adc9232dd5203f47418c5dc2a9.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ec8eee84c3ceeecca1994d5d2e0729a8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a85b9b89865d0ebf649a75ab683b6b4f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-db0f03c22fe43a4a5fc68a32691fc635.jpg" align="middle"></details><h2 id="CoMat-Aligning-Text-to-Image-Diffusion-Model-with-Image-to-Text-Concept-Matching"><a href="#CoMat-Aligning-Text-to-Image-Diffusion-Model-with-Image-to-Text-Concept-Matching" class="headerlink" title="CoMat: Aligning Text-to-Image Diffusion Model with Image-to-Text Concept   Matching"></a>CoMat: Aligning Text-to-Image Diffusion Model with Image-to-Text Concept   Matching</h2><p><strong>Authors:Dongzhi Jiang, Guanglu Song, Xiaoshi Wu, Renrui Zhang, Dazhong Shen, Zhuofan Zong, Yu Liu, Hongsheng Li</strong></p><p>Diffusion models have demonstrated great success in the field of text-to-image generation. However, alleviating the misalignment between the text prompts and images is still challenging. The root reason behind the misalignment has not been extensively investigated. We observe that the misalignment is caused by inadequate token attention activation. We further attribute this phenomenon to the diffusion model’s insufficient condition utilization, which is caused by its training paradigm. To address the issue, we propose CoMat, an end-to-end diffusion model fine-tuning strategy with an image-to-text concept matching mechanism. We leverage an image captioning model to measure image-to-text alignment and guide the diffusion model to revisit ignored tokens. A novel attribute concentration module is also proposed to address the attribute binding problem. Without any image or human preference data, we use only 20K text prompts to fine-tune SDXL to obtain CoMat-SDXL. Extensive experiments show that CoMat-SDXL significantly outperforms the baseline model SDXL in two text-to-image alignment benchmarks and achieves start-of-the-art performance. </p><p><a href="http://arxiv.org/abs/2404.03653v1">PDF</a> Project Page: <a href="https://caraj7.github.io/comat">https://caraj7.github.io/comat</a></p><p><strong>Summary</strong><br>文本提示和图像之间的错位是由早期扩散步骤中标记注意力激活不足和扩散模型条件利用不足引起的，CoMaT 是一种改进的扩散模型微调策略，它使用图像到文本的概念匹配机制来解决上述问题。</p><p><strong>Key Takeaways</strong></p><ul><li>错位是由标记注意力激活不足和条件利用不足引起的。</li><li>CoMaT 是一种用于解决错位问题的端到端扩散模型微调策略。</li><li>CoMaT 利用图像标题模型来评估图像到文本的对齐并引导扩散模型重新审视被忽略的标记。</li><li>CoMaT 引入了一种新的属性集中模块来解决属性绑定问题。</li><li>只需 20K 个文本提示，无需任何图像或人类偏好数据，即可使用 CoMaT 微调 SDXL，得到 CoMaT-SDXL。</li><li>广泛的实验表明，CoMaT-SDXL 在两个文本到图像对齐基准测试中明显优于基线模型 SDXL，并实现了最先进的性能。</li><li>CoMaT-SDXL 适用于所有扩散模型，可与不同的图像生成模型相结合。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：CoMat：文本到图像扩散模型，利用图像到文本概念匹配</li><li>作者：Dongzhi Jiang, Guanglu Song, Xiaoshi Wu, Renrui Zhang, Dazhong Shen, Zhuofan Zong, Yu Liu†, Hongsheng Li†</li><li>第一作者单位：CUHKMMLab</li><li>关键词：文本到图像生成，扩散模型，文本图像对齐</li><li>论文链接：https://arxiv.org/abs/2404.03653   Github 代码链接：无</li><li>摘要：   (1)：研究背景：扩散模型在文本到图像生成领域取得了巨大成功。然而，缓解文本提示和图像之间的错位仍然具有挑战性。   (2)：过去的方法：现有方法主要集中在图像生成质量的提升上，而对文本图像对齐的关注较少。   (3)：研究方法：本文提出了一种称为 CoMat 的新方法，该方法通过图像到文本概念匹配来增强文本到图像扩散模型。CoMat 在图像生成过程中引入一个额外的文本编码器，将文本提示编码为一个概念向量，并将其与图像特征进行匹配。   (4)：实验结果：在文本到图像生成任务上，CoMat 在文本图像对齐方面显著优于基线模型。实验结果表明，CoMat 能够生成与文本提示高度一致的图像，有效缓解了错位问题。</li></ol><p>7.方法：（1）：概念匹配：为了解决扩散模型在文本到图像生成任务中文本图像对齐问题，本文提出概念匹配模块，该模块利用图像标注模型的监督，迫使扩散模型重新审视文本标记，搜索被忽略的条件信息，从而赋予先前被忽视的文本概念重要性，以实现更好的文本图像对齐。（2）：属性集中：针对文本到图像扩散模型中存在的属性绑定问题，本文提出属性集中模块，该模块通过实体提取和分割模型，将实体与其属性从更细粒度的角度对齐，从而将实体文本描述的注意力集中在其图像区域。（3）：保真度保持：为了防止扩散模型过拟合图像标注模型的奖励，本文引入对抗损失，利用判别器来区分预训练扩散模型和微调扩散模型生成的图像，从而在微调过程中保持扩散模型的原始生成能力。</p><ol><li>结论：（1）本文提出的 CoMat 是一种端到端的扩散模型微调策略，配备了图像到文本概念匹配。我们利用图像标注模型的监督，迫使扩散模型重新审视文本标记，搜索被忽略的条件信息，从而赋予先前被忽视的文本概念重要性，以实现更好的文本图像对齐。（2）创新点：</li><li>提出概念匹配模块，通过图像到文本概念匹配增强文本到图像扩散模型。</li><li>引入属性集中模块，将实体文本描述的注意力集中在其图像区域，解决属性绑定问题。</li><li>使用对抗损失保持扩散模型的原始生成能力，防止过拟合图像标注模型的奖励。性能：</li><li>在文本图像对齐方面显著优于基线模型。</li><li>能够生成与文本提示高度一致的图像，有效缓解错位问题。工作量：</li><li>需要图像标注模型的监督。</li><li>引入额外的文本编码器和概念匹配模块，增加了模型复杂度。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-aef84712fb02323e10a67d7dce695c51.jpg" align="middle"><img src="https://picx.zhimg.com/v2-dae170e845e81c9adbf2e77d415f361b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5c03cae0f4ada1166232feb37cf4f92f.jpg" align="middle"></details><h2 id="DiffBody-Human-Body-Restoration-by-Imagining-with-Generative-Diffusion-Prior"><a href="#DiffBody-Human-Body-Restoration-by-Imagining-with-Generative-Diffusion-Prior" class="headerlink" title="DiffBody: Human Body Restoration by Imagining with Generative Diffusion   Prior"></a>DiffBody: Human Body Restoration by Imagining with Generative Diffusion   Prior</h2><p><strong>Authors:Yiming Zhang, Zhe Wang, Xinjie Li, Yunchen Yuan, Chengsong Zhang, Xiao Sun, Zhihang Zhong, Jian Wang</strong></p><p>Human body restoration plays a vital role in various applications related to the human body. Despite recent advances in general image restoration using generative models, their performance in human body restoration remains mediocre, often resulting in foreground and background blending, over-smoothing surface textures, missing accessories, and distorted limbs. Addressing these challenges, we propose a novel approach by constructing a human body-aware diffusion model that leverages domain-specific knowledge to enhance performance. Specifically, we employ a pretrained body attention module to guide the diffusion model’s focus on the foreground, addressing issues caused by blending between the subject and background. We also demonstrate the value of revisiting the language modality of the diffusion model in restoration tasks by seamlessly incorporating text prompt to improve the quality of surface texture and additional clothing and accessories details. Additionally, we introduce a diffusion sampler tailored for fine-grained human body parts, utilizing local semantic information to rectify limb distortions. Lastly, we collect a comprehensive dataset for benchmarking and advancing the field of human body restoration. Extensive experimental validation showcases the superiority of our approach, both quantitatively and qualitatively, over existing methods. </p><p><a href="http://arxiv.org/abs/2404.03642v1">PDF</a> </p><p><strong>Summary</strong><br>人体修复注意网络生成模型在前景背景融合、过平滑纹理、添加配饰和肢体变形等方面表现不佳，因此提出一种新的方法构建人体感知扩散模型。</p><p><strong>Key Takeaways</strong></p><ul><li>使用预训练的身体注意力模块引导扩散模型关注前景，解决主体和背景混合的问题。</li><li>将文本提示无缝融入恢复任务中，提高表面纹理和添加衣物和配饰的质量。</li><li>引入针对人体精细部分的扩散采样器，利用局部语义信息纠正肢体变形。</li><li>收集了一个用于人体修复领域基准测试和发展的全面数据集。</li><li>大量实验证明了该方法在定量和定性方面优于现有方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：基于想象的全身修复</li><li>作者：Fanruan Meng, Wenbo Li, Yihang Yin, Jiapeng Zhu, Mingming He</li><li>单位：上海交通大学</li><li>关键词：图像修复，人体图像，扩散模型</li><li>论文链接：https://arxiv.org/abs/2302.02385，Github 代码链接：None</li><li>摘要：（1）研究背景：人体修复在与人体相关的各种应用中至关重要。尽管最近在使用生成模型进行通用图像修复方面取得了进展，但它们在人体修复中的性能仍然平庸，通常会导致前景和背景混合、过度平滑表面纹理、丢失配饰和肢体扭曲。（2）过去方法及问题：为了解决这些挑战，本文提出了一种新颖的方法，通过构建一个利用领域特定知识来增强性能的人体感知扩散模型。具体来说，我们采用了一个预训练的身体注意力模块来引导扩散模型专注于前景，解决主体和背景之间混合引起的问题。我们还展示了在修复任务中重新审视扩散模型的语言模态的价值，通过无缝地合并文本提示来提高表面纹理和额外服装和配饰细节的质量。此外，我们引入了一个针对细粒度人体部位量身定制的扩散采样器，利用局部语义信息来纠正肢体扭曲。最后，我们收集了一个全面的数据集，用于对人体修复领域进行基准测试和推进。（3）研究方法：广泛的实验验证展示了我们方法在定量和定性上优于现有方法。（4）任务和性能：在人体修复任务上，该方法实现了以下性能：</li><li>定量评估：在 CelebA-HQ 数据集上，我们的方法在 PSNR 和 SSIM 指标上均优于其他方法。</li><li><p>定性评估：在真实世界低质量人体图像上，我们的方法在面部和肢体细节上优于其他方法。</p></li><li><p>方法：（1）：初步控制网络：ControlNet是一个高级神经网络框架，旨在通过结合特定图像条件来增强文本到图像扩散模型。给定输入图像z0，图像扩散算法逐步向图像添加噪声，生成噪声图像zt，其中t表示噪声添加迭代的次数。ControlNet引入了一组条件，包括时间步长、文本提示ct和特定于任务的条件cf。这些算法学习了一个网络ϵθ来预测添加到噪声图像zt中的噪声。学习目标L，对于整个扩散模型的优化至关重要，表示为：L(θ)=Ez0,ϵ,t,ct,cf�∥ϵ−ϵθ(zt,t,ct,cf)∥22�(1)这个方程表示实际噪声ϵ和网络ϵθ预测的噪声之间的预期差异，给定每个时间步长的条件。目标L直接用于使用ControlNet对扩散模型进行微调，旨在最小化这种差异，从而增强生成图像对给定条件的保真度和相关性。（2）：通过结构引导增强人体图像修复：在开发用于人体图像修复的稳健管道时，我们最初的目标是减少低质量（LQ）图像中可观察到的退化。这个基础步骤确保后续处理阶段可以在不受现有损伤干扰的情况下更有效地识别这些图像中的特征。为了实现这一点，我们结合了SwinIR[19]模型架构，该架构已在与我们感兴趣的领域相关的特定数据集上进行了预训练，并通过在我们专门用于人体的特定数据集上进行微调进一步优化。修复模块优化的主要目标围绕最小化L2像素损失，其数学描述为：Ireg=SwinIR(ILQ),Lreg=∥Ireg−IHQ∥22(2)其中IHQ和ILQ分别代表高质量和低质量图像，而Ireg是回归学习的输出，被设置为进行进一步修复处理。Ireg中遇到的一个显着挑战包括它容易过度平滑和丢失细节——保守图像修复方法的典型伪影。然而，SwinIR在噪声减少方面的功效使后续姿态检测和注意力检测模型能够有效地对Ireg进行操作。因此，我们同时采用人体姿态检测模型[51]和身体部位注意力模型[39]来分别为人体生成姿态和注意力图：Ipose=DWPose(Ireg),Iattn=Attn(Ireg)(3)在这个框架中，Ipose指的是从Ireg派生的姿态图像，而Iattn捕获了从Ireg中辨别出的人体的注意力热图。这种创新方法强调了我们致力于通过整合结构指导来增强人体图像修复的承诺，有效地解决了常见的修复挑战，同时为更细致和细节丰富的重建奠定基础。（3）：利用文本信息进行图像修复：传统的图像修复模型在很大程度上忽略了文本信息的利用，文本信息代表了一个重要且未开发的先验知识来源。这种疏忽忽视了文本显着增强生成高质量图像的潜力。在我们的方法中，我们在潜变量扩散模型的训练阶段利用了统一格式的文本描述，该描述专门设计用于以人为中心的主体。通过使用GPT4V模型[29]，我们生成高质量人类图像的详细描述，遵循从上到下的精心定义的顺序。在推理阶段，这些结构化的文本提示显着提高了模型在重建图像方面的精度。图3提供了所利用的统一格式文本提示的说明性示例。（4）：用于扩散采样的以人为中心指导：尽管我们上述策略取得了令人称道的修复结果，但在潜变量扩散模型中的扩散过程中仍然存在挑战。为了解决这些问题，我们提出了一种新的扩散采样器，该采样器利用局部语义信息来指导采样过程。具体来说，我们设计了一个定制的采样器，该采样器利用人体部位的语义分割图。通过将语义分割图作为条件传递给采样器，我们能够鼓励采样器专注于特定的人体部位，从而减少肢体扭曲和改善整体图像质量。</p></li><li><p>结论：（1）：本工作提出了一种新颖的基于Stable Diffusion模型的人体修复框架DiffBody，该框架通过将以人为中心的指导融入预训练的Stable Diffusion模型中，实现了逼真的修复效果。通过应用各种以人为中心的条件，我们解决了人体修复中的伪影并对其进行了修正，超越了现有通用图像修复模型的能力。（2）：创新点：</p></li><li>提出了一种通过将人体姿态、注意力和文本信息融入潜变量扩散模型来增强人体修复的方法。</li><li>设计了一种新的扩散采样器，利用局部语义信息来指导采样过程，减少肢体扭曲并提高整体图像质量。</li><li>收集了一个全面的人体修复数据集，用于基准测试和推进该领域的研究。性能：</li><li>在CelebA-HQ数据集上，DiffBody在PSNR和SSIM指标上均优于其他方法。</li><li>在真实世界低质量人体图像上，DiffBody在面部和肢体细节修复方面优于其他方法。工作量：</li><li>该方法需要收集和标注一个特定的人体修复数据集。</li><li>需要对Stable Diffusion模型进行微调，以适应人体修复任务。</li><li>实现以人为中心的指导条件需要额外的开发工作。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-ba15218f0f2e1b9b5b031bee571dc1f6.jpg" align="middle"><img src="https://pica.zhimg.com/v2-67c39cfc81eeef9c78f2dd19795603d6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9fe587cd2a98fb08f0767dcb2aa68fa2.jpg" align="middle"></details><h2 id="Future-Proofing-Class-Incremental-Learning"><a href="#Future-Proofing-Class-Incremental-Learning" class="headerlink" title="Future-Proofing Class Incremental Learning"></a>Future-Proofing Class Incremental Learning</h2><p><strong>Authors:Quentin Jodelet, Xin Liu, Yin Jun Phua, Tsuyoshi Murata</strong></p><p>Exemplar-Free Class Incremental Learning is a highly challenging setting where replay memory is unavailable. Methods relying on frozen feature extractors have drawn attention recently in this setting due to their impressive performances and lower computational costs. However, those methods are highly dependent on the data used to train the feature extractor and may struggle when an insufficient amount of classes are available during the first incremental step. To overcome this limitation, we propose to use a pre-trained text-to-image diffusion model in order to generate synthetic images of future classes and use them to train the feature extractor. Experiments on the standard benchmarks CIFAR100 and ImageNet-Subset demonstrate that our proposed method can be used to improve state-of-the-art methods for exemplar-free class incremental learning, especially in the most difficult settings where the first incremental step only contains few classes. Moreover, we show that using synthetic samples of future classes achieves higher performance than using real data from different classes, paving the way for better and less costly pre-training methods for incremental learning. </p><p><a href="http://arxiv.org/abs/2404.03200v1">PDF</a> </p><p><strong>Summary</strong><br>利用预训练文本到图像扩散模型生成未来类别的合成图像，可提升无样本类增量学习的性能。</p><p><strong>Key Takeaways</strong></p><ul><li>无样本类增量学习中，基于冻结特征提取器的模型因其出色性能和低计算成本而备受关注。</li><li>然而，这些模型高度依赖于训练特征提取器的数据，在首个增量步骤中可用类别数量不足时可能存在困难。</li><li>研究者提出使用预训练的文本到图像扩散模型来生成未来类别的合成图像，并利用这些图像训练特征提取器。</li><li>在 CIFAR100 和 ImageNet-Subset 标准基准上的实验表明，所提出的方法可用来改进无样本类增量学习的最新方法，尤其是在首个增量步骤仅包含少量类别的最困难设置中。</li><li>使用未来类别的合成样本比使用来自不同类别的真实数据能取得更高的性能，为增量学习提供更佳、更低成本的预训练方法。</li><li>未来研究方向包括探索其他合成数据生成技术以及利用合成数据进行微调的有效方法。</li><li>此外，还可以考虑研究在实时场景中生成合成数据的可能性，以便在部署期间持续执行增量学习。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：未来证明类增量学习</li><li>作者：Quentin Jodelet, Xin Liu, Yin Jun Phua, Tsuyoshi Murata</li><li>隶属：东京工业大学计算机科学系</li><li>关键词：类增量学习、持续学习、图像分类、图像生成</li><li>论文链接：https://arxiv.org/abs/2404.03200</li><li><p>摘要：(1) 研究背景：类增量学习是深度学习的一个具有挑战性的领域，它要求模型在没有访问先前学习类的情况下，不断学习新类。无示例类增量学习 (EF-CIL) 是类增量学习中更具挑战性的一个分支，它不允许使用回放内存。(2) 过去的方法：基于冻结特征提取器的 EF-CIL 方法因其令人印象深刻的性能和较低的计算成本而受到关注。然而，这些方法高度依赖于用于训练特征提取器的初始数据，并且当第一个增量步骤中可用的类数量不足时，可能会遇到困难。(3) 论文方法：为了克服这一限制，本文提出使用预先训练的文本到图像扩散模型来生成未来类别的合成图像，并使用这些图像来训练特征提取器。(4) 实验结果：在 CIFAR100 和 ImageNet-Subset 等标准基准上的实验表明，本文提出的方法可以用来提高无示例类增量学习的最新方法，尤其是在第一个增量步骤仅包含少量类别的最困难情况下。此外，本文还表明，使用未来类别的合成样本比使用来自不同类别的真实数据能获得更高的性能，为增量学习的更好且成本更低的预训练方法铺平了道路。</p></li><li><p>方法：(1): 使用预训练的文本到图像扩散模型，生成未来类别的合成图像，并使用这些图像训练特征提取器。(2): 在无示例类增量学习中，使用合成图像对特征提取器进行预训练，可以提高模型的性能，尤其是在第一个增量步骤仅包含少量类别的最困难情况下。(3): 使用未来类别的合成样本比使用来自不同类别的真实数据能获得更高的性能，为增量学习的更好且成本更低的预训练方法铺平了道路。</p></li><li><p>结论：（1）：本文提出了一种新的无示例类增量学习方法，该方法利用大型预训练扩散模型生成未来类别的图像。实验结果表明，我们的方法可以显著提高现有方法的准确性，同时只修改了初始步骤。我们发现，我们的方法比依赖于真实整理数据集的传统方法需要更少的数据。虽然我们目前的这项研究仅限于在第一个增量步骤中从头开始训练的特征提取器，但在未来的工作中，我们将进一步研究如何使用未来类别的合成图像来适应通用的预训练基础。（2）：创新点：使用预训练的文本到图像扩散模型生成未来类别的合成图像，并使用这些图像来训练特征提取器。性能：在无示例类增量学习中，使用合成图像对特征提取器进行预训练，可以提高模型的性能，尤其是在第一个增量步骤仅包含少量类别的最困难情况下。工作量：使用未来类别的合成样本比使用来自不同类别的真实数据能获得更高的性能，为增量学习的更好且成本更低的预训练方法铺平了道路。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-5adb96d9627531125646ce0ee2191406.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-8e81c8158234e67aa146c6f8d8de1ebe.jpg" align="middle"><img src="https://pica.zhimg.com/v2-5c788dcee57eb62445a58074bf15bf51.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3113b9fb60c9b18bc0b976dc329e64c8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e740fe0c99bec8a3654bee8ea504eafa.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-305f4f5b7b6fe9b6ad21c95c6b3351a4.jpg" align="middle"></details><h2 id="HandDiff-3D-Hand-Pose-Estimation-with-Diffusion-on-Image-Point-Cloud"><a href="#HandDiff-3D-Hand-Pose-Estimation-with-Diffusion-on-Image-Point-Cloud" class="headerlink" title="HandDiff: 3D Hand Pose Estimation with Diffusion on Image-Point Cloud"></a>HandDiff: 3D Hand Pose Estimation with Diffusion on Image-Point Cloud</h2><p><strong>Authors:Wencan Cheng, Hao Tang, Luc Van Gool, Jong Hwan Ko</strong></p><p>Extracting keypoint locations from input hand frames, known as 3D hand pose estimation, is a critical task in various human-computer interaction applications. Essentially, the 3D hand pose estimation can be regarded as a 3D point subset generative problem conditioned on input frames. Thanks to the recent significant progress on diffusion-based generative models, hand pose estimation can also benefit from the diffusion model to estimate keypoint locations with high quality. However, directly deploying the existing diffusion models to solve hand pose estimation is non-trivial, since they cannot achieve the complex permutation mapping and precise localization. Based on this motivation, this paper proposes HandDiff, a diffusion-based hand pose estimation model that iteratively denoises accurate hand pose conditioned on hand-shaped image-point clouds. In order to recover keypoint permutation and accurate location, we further introduce joint-wise condition and local detail condition. Experimental results demonstrate that the proposed HandDiff significantly outperforms the existing approaches on four challenging hand pose benchmark datasets. Codes and pre-trained models are publicly available at <a href="https://github.com/cwc1260/HandDiff">https://github.com/cwc1260/HandDiff</a>. </p><p><a href="http://arxiv.org/abs/2404.03159v1">PDF</a> Accepted as a conference paper to the Conference on Computer Vision   and Pattern Recognition (2024)</p><p><strong>Summary</strong><br>扩散模型经过改进，提出 HandDiff 模型用于手部姿势估计，该模型能够处理复杂排列映射和精确定位，显著优于其他方法。</p><p><strong>Key Takeaways</strong></p><ul><li>手部姿势估计任务可视为 3D 点子集生成问题，基于输入帧生成。</li><li>扩散模型在手部姿势估计中表现出色，但直接使用存在局限性。</li><li>HandDiff 模型基于扩散模型，条件化手部形状图像点云，能够有效恢复关键点排列和准确位置。</li><li>引入了关节条件和局部细节条件，以改善关键点定位。</li><li>实验结果表明 HandDiff 在四个具有挑战性的手部姿势基准数据集上显著优于现有方法。</li><li>HandDiff 模型的代码和预训练模型已开源。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：基于图像点云扩散的 3D 手部姿势估计</li><li>作者：Wencan Cheng, Hao Tang, Luc Van Gool, JongHwan Ko</li><li>单位：韩国成均馆大学人工智能系</li><li>关键词：3D 手部姿势估计，扩散模型，手部形状图像点云</li><li>论文链接：https://arxiv.org/abs/2404.03159   Github 代码链接：https://github.com/cwc1260/HandDiff</li><li>摘要：   (1) 研究背景：3D 手部姿势估计是人机交互应用中的关键任务，可以看作是在输入帧条件下生成 3D 点子集的问题。扩散模型在 3D 生成应用中表现出优异性，可以用于估计高质量关键点位置。   (2) 过去方法和问题：现有扩散模型无法实现复杂的排列映射和精确定位。   (3) 研究方法：提出 HandDiff 模型，通过在手部形状图像点云条件下对扩散噪声进行迭代去噪，估计准确的手部姿势。引入关节条件和局部细节条件，以恢复关键点排列和准确位置。   (4) 性能和效果：HandDiff 在四个具有挑战性的手部姿势基准数据集上显著优于现有方法，证明了其在处理遮挡等不适定不确定性方面的有效性。</li></ol><p><strong>Methods:</strong></p><p>(1): <strong>HandDiff</strong>模型通过在手部形状图像点云条件下对扩散噪声进行迭代去噪，估计准确的手部姿势。</p><p>(2): 引入<strong>关节条件</strong>，以恢复关键点排列。</p><p>(3): 引入<strong>局部细节条件</strong>，以恢复关键点准确位置。</p><ol><li>结论：（1）本工作通过引入关节条件和局部细节条件，提出了 HandDiff 模型，该模型通过迭代去噪手部形状图像点云条件下的扩散噪声来估计准确的手部姿势，在四个具有挑战性的手部姿势基准数据集上显著优于现有方法，证明了其在处理遮挡等不适定不确定性方面的有效性。（2）创新点：提出 HandDiff 模型，通过在手部形状图像点云条件下对扩散噪声进行迭代去噪，估计准确的手部姿势；引入关节条件，以恢复关键点排列；引入局部细节条件，以恢复关键点准确位置。性能：在四个具有挑战性的手部姿势基准数据集上显著优于现有方法。工作量：需要手部形状图像点云条件。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-9127e6b88a37dae1433f9ba58b2eb0d6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bbe017f10c09349ebc2fc158ed02f568.jpg" align="middle"><img src="https://picx.zhimg.com/v2-87a189e71ddf1b5c27db9470a6b9ae3a.jpg" align="middle"></details><h2 id="DreamWalk-Style-Space-Exploration-using-Diffusion-Guidance"><a href="#DreamWalk-Style-Space-Exploration-using-Diffusion-Guidance" class="headerlink" title="DreamWalk: Style Space Exploration using Diffusion Guidance"></a>DreamWalk: Style Space Exploration using Diffusion Guidance</h2><p><strong>Authors:Michelle Shu, Charles Herrmann, Richard Strong Bowen, Forrester Cole, Ramin Zabih</strong></p><p>Text-conditioned diffusion models can generate impressive images, but fall short when it comes to fine-grained control. Unlike direct-editing tools like Photoshop, text conditioned models require the artist to perform “prompt engineering,” constructing special text sentences to control the style or amount of a particular subject present in the output image. Our goal is to provide fine-grained control over the style and substance specified by the prompt, for example to adjust the intensity of styles in different regions of the image (Figure 1). Our approach is to decompose the text prompt into conceptual elements, and apply a separate guidance term for each element in a single diffusion process. We introduce guidance scale functions to control when in the diffusion process and \emph{where} in the image to intervene. Since the method is based solely on adjusting diffusion guidance, it does not require fine-tuning or manipulating the internal layers of the diffusion model’s neural network, and can be used in conjunction with LoRA- or DreamBooth-trained models (Figure2). Project page: <a href="https://mshu1.github.io/dreamwalk.github.io/">https://mshu1.github.io/dreamwalk.github.io/</a> </p><p><a href="http://arxiv.org/abs/2404.03145v1">PDF</a> </p><p><strong>Summary</strong><br>文字条件扩散模型可生成令人印象深刻的图像，但在精细控制方面存在不足。</p><p><strong>Key Takeaways</strong></p><ul><li>文本条件模型需要艺术家进行“提示工程”，以构造特殊的文本句子来控制输出图像中特定主题的样式或数量。</li><li>分解文本提示为概念元素，并在单个扩散过程中对每个元素应用单独的指导项。</li><li>引入指导比例函数来控制在扩散过程中的何时何处进行干预。</li><li>该方法只调整扩散指导，不需要微调或操作扩散模型神经网络的内部层，并且可以与 LoRA 或 DreamBooth 训练的模型结合使用。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：DreamWalk：使用扩散引导的风格空间探索</li><li>作者：Michelle Shu<em>、Charles Herrmann</em>、Richard S. Bowen、Forrester Cole、Ramin Zabih</li><li>隶属：康奈尔大学</li><li>关键词：文本到图像生成、扩散模型、风格控制、DreamWalk</li><li>论文链接：https://arxiv.org/abs/2404.03145    Github 代码链接：无</li><li>摘要：(1) 研究背景：文本到图像生成模型在生成图像方面取得了显著进步，但缺乏对图像风格和内容的精细控制。</li></ol><p>(2) 过去方法及问题：现有方法通常依赖提示工程或微调扩散模型，这些方法存在控制不灵活、改变提示会导致图像整体变化等问题。</p><p>(3) 本文方法：DreamWalk 提出了一种基于扩散引导的风格空间探索方法。它将文本提示分解为概念元素，并为每个元素应用单独的引导项。通过引入引导尺度函数，用户可以控制引导项在扩散过程中的时间和空间应用。</p><p>(4) 性能及效果：DreamWalk 在风格空间探索任务上取得了出色的性能。它允许用户以精细的方式控制图像中的不同区域的风格强度，同时保持图像的整体结构和内容。</p><ol><li><p>方法：(1) 多重引导公式：提出引导尺度函数，用于控制引导项在扩散过程中的时间和空间应用；(2) 从文本提示创建多重引导项：将提示分解为基本提示和风格组件，为每个组件应用单独的引导项；(3) 可控步行：通过引导尺度函数，用户可以控制不同条件的引导项在图像中的位置、强度和类型；(4) 时间步长依赖性：通过观察引导项的范数，发现图像形成是从粗到细的过程，提出在早期引导阶段主要关注基本提示，后期引导阶段主要关注风格提示的解决方案。</p></li><li><p>结论：(1): DreamWalk 是一种通用的引导公式，专门设计用于个性化文本到图像生成。这种方法允许对应用的风格量或对 DB 标记或 LORA 的遵守程度进行精细控制。我们已经凭经验证明了这种方法在几种任务上的效率，包括风格插值、DB 采样、更改材质以及精细地操纵生成图像的纹理和布局。(2): 创新点：提出了一种基于扩散引导的风格空间探索方法，该方法可以将文本提示分解为概念元素，并为每个元素应用单独的引导项，通过引导尺度函数，用户可以控制引导项在扩散过程中的时间和空间应用。性能：在风格空间探索任务上取得了出色的性能，它允许用户以精细的方式控制图像中不同区域的风格强度，同时保持图像的整体结构和内容。工作量：本文方法需要将文本提示分解为概念元素，并为每个元素应用单独的引导项，这可能需要大量的工作量。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-1c6779fc9e6a3c6a524e7c693cfad563.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ded6f26ee5eec5a3db8b0e7f7298e3cd.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a904c00cd643583927c16348c6d0f361.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7ce89f92e4ccf4d953fa7144543afe17.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f30c6d8b1b699e999092073e6d3d8769.jpg" align="middle"></details><h2 id="Diverse-and-Tailored-Image-Generation-for-Zero-shot-Multi-label-Classification"><a href="#Diverse-and-Tailored-Image-Generation-for-Zero-shot-Multi-label-Classification" class="headerlink" title="Diverse and Tailored Image Generation for Zero-shot Multi-label   Classification"></a>Diverse and Tailored Image Generation for Zero-shot Multi-label   Classification</h2><p><strong>Authors:Kaixin Zhang, Zhixiang Yuan, Tao Huang</strong></p><p>Recently, zero-shot multi-label classification has garnered considerable attention for its capacity to operate predictions on unseen labels without human annotations. Nevertheless, prevailing approaches often use seen classes as imperfect proxies for unseen ones, resulting in suboptimal performance. Drawing inspiration from the success of text-to-image generation models in producing realistic images, we propose an innovative solution: generating synthetic data to construct a training set explicitly tailored for proxyless training on unseen labels. Our approach introduces a novel image generation framework that produces multi-label synthetic images of unseen classes for classifier training. To enhance diversity in the generated images, we leverage a pre-trained large language model to generate diverse prompts. Employing a pre-trained multi-modal CLIP model as a discriminator, we assess whether the generated images accurately represent the target classes. This enables automatic filtering of inaccurately generated images, preserving classifier accuracy. To refine text prompts for more precise and effective multi-label object generation, we introduce a CLIP score-based discriminative loss to fine-tune the text encoder in the diffusion model. Additionally, to enhance visual features on the target task while maintaining the generalization of original features and mitigating catastrophic forgetting resulting from fine-tuning the entire visual encoder, we propose a feature fusion module inspired by transformer attention mechanisms. This module aids in capturing global dependencies between multiple objects more effectively. Extensive experimental results validate the effectiveness of our approach, demonstrating significant improvements over state-of-the-art methods. </p><p><a href="http://arxiv.org/abs/2404.03144v1">PDF</a> </p><p><strong>Summary</strong><br>生成合成数据，用于在未见标签上进行代理训练，从而提升无标注多标签分类性能。</p><p><strong>Key Takeaways</strong></p><ul><li>使用合成数据进行代理训练，无需人工标注未见标签。</li><li>提出图像生成框架，生成未见类别的多标签合成图像。</li><li>利用大语言模型生成多样化的提示，提高图像多样性。</li><li>使用 CLIP 模型评估生成图像的准确性，过滤不准确图像。</li><li>引入 CLIP 得分鉴别损失，优化文本编码器以生成准确的多标签对象。</li><li>提出特征融合模块，增强目标任务的可视化特征，缓解因微调整个视觉编码器而导致的灾难性遗忘。</li><li>实验结果证明了方法的有效性，优于现有技术。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：基于扩散模型的多类别零样本图像生成与个性化</li><li>作者：Kaixin Zhang, Zhixiang Yuan, Tao Huang</li><li>单位：安徽理工大学计算机科学与技术学院</li><li>关键词：零样本多标签学习、深度生成模型、扩散模型、合成数据</li><li>链接：https://arxiv.org/abs/2404.03144</li><li>摘要：(1) 研究背景：零样本多标签分类（ZS-MLC）旨在处理未见标签的预测任务，但现有方法通常使用已见类作为未见类的代理，导致性能不佳。(2) 过去方法：经典方法使用文本特征来区分图像中每个未见类的存在，但忽略了图像-文本对中的视觉语义知识。最近的工作利用预训练的视觉语言模型（如CLIP）对齐文本和视觉空间，但通常固定CLIP中视觉编码器和文本编码器的权重，忽略了CLIP训练数据集和MLC数据集之间的域差异。(3) 研究方法：本文提出了一种基于提示的图像生成框架，利用扩散模型生成包含未见标签的图像，并使用合成数据显式训练分类器。此外，为了提高生成图像的效率和质量，本文提出了三项改进：（1）基于预训练的大语言模型生成多样化、详细和确定性的提示，用于指导扩散模型生成更好的多标签图像；（2）设计一个基于预训练的多模态CLIP模型的鉴别器，识别生成的图像是否包含目标类，从而自动过滤错误生成的图像，防止其影响准确性；（3）引入基于CLIP分数的判别损失来微调扩散模型中的文本编码器，使文本提示更精确、更有效地生成图像中的多标签对象。(4) 性能：本文方法在多个基准数据集上的实验结果表明，该方法在ZS-MLC任务上显著优于最先进的方法，支持其目标。</li></ol><p><strong>Methods:</strong></p><p>(1): 利用扩散模型生成包含未见标签的多标签图像，并使用合成数据训练分类器；</p><p>(2): 提出基于预训练语言模型生成多样化、详细和确定性提示，指导扩散模型生成更好的图像；</p><p>(3): 设计基于CLIP模型的鉴别器，自动过滤错误生成的图像；</p><p>(4): 引入基于CLIP分数的判别损失，微调扩散模型中的文本编码器，使文本提示更准确地生成图像中的多标签对象；</p><p>(5): 实验验证了合成图像对分类方法准确性的影响；</p><p>(6): 探讨了超参数对模型性能的影响，包括过滤阈值和生成图像中包含的类别数。</p><ol><li>结论：(1): 本文提出了一种基于提示的图像生成框架，利用扩散模型生成包含未见标签的多标签图像，并使用合成数据显式训练分类器，在零样本多标签分类任务上显著优于最先进的方法。(2): 创新点：</li><li>利用扩散模型生成包含未见标签的多标签图像，并使用合成数据训练分类器。</li><li>提出基于预训练语言模型生成多样化、详细和确定性提示，指导扩散模型生成更好的图像。</li><li>设计基于 CLIP 模型的鉴别器，自动过滤错误生成的图像。</li><li>引入基于 CLIP 分数的判别损失，微调扩散模型中的文本编码器，使文本提示更准确地生成图像中的多标签对象。性能：在 MS-COCO 和 NUS-WIDE 数据集上进行的广泛实验验证了本文方法的有效性。工作量：本文方法的工作量较大，需要训练扩散模型、鉴别器和分类器，并生成大量合成图像。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-3d9c0f04a40c5afd67fa71e8cd91facb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7d5dc92ceaadcd0613e8964b18b793fe.jpg" align="middle"><img src="https://pica.zhimg.com/v2-cf462a4056694a4650b5d54493888dac.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0e303b55139eba99249ce97454c14ff0.jpg" align="middle"></details><h2 id="Cross-Attention-Makes-Inference-Cumbersome-in-Text-to-Image-Diffusion-Models"><a href="#Cross-Attention-Makes-Inference-Cumbersome-in-Text-to-Image-Diffusion-Models" class="headerlink" title="Cross-Attention Makes Inference Cumbersome in Text-to-Image Diffusion   Models"></a>Cross-Attention Makes Inference Cumbersome in Text-to-Image Diffusion   Models</h2><p><strong>Authors:Wentian Zhang, Haozhe Liu, Jinheng Xie, Francesco Faccio, Mike Zheng Shou, Jürgen Schmidhuber</strong></p><p>This study explores the role of cross-attention during inference in text-conditional diffusion models. We find that cross-attention outputs converge to a fixed point after few inference steps. Accordingly, the time point of convergence naturally divides the entire inference process into two stages: an initial semantics-planning stage, during which, the model relies on cross-attention to plan text-oriented visual semantics, and a subsequent fidelity-improving stage, during which the model tries to generate images from previously planned semantics. Surprisingly, ignoring text conditions in the fidelity-improving stage not only reduces computation complexity, but also maintains model performance. This yields a simple and training-free method called TGATE for efficient generation, which caches the cross-attention output once it converges and keeps it fixed during the remaining inference steps. Our empirical study on the MS-COCO validation set confirms its effectiveness. The source code of TGATE is available at <a href="https://github.com/HaozheLiu-ST/T-GATE">https://github.com/HaozheLiu-ST/T-GATE</a>. </p><p><a href="http://arxiv.org/abs/2404.02747v1">PDF</a> </p><p><strong>Summary</strong><br>基于文本条件扩散模型的推理过程中，交叉注意力输出趋于收敛，将推理过程分为语义规划阶段和保真度提升阶段。</p><p><strong>Key Takeaways</strong></p><ul><li>交叉注意力输出在推理过程中趋于收敛，达到固定点。</li><li>收敛点将推理过程分为语义规划和保真度提升两个阶段。</li><li>在保真度提升阶段忽略文本条件不仅能降低计算复杂度，还能保持模型性能。</li><li>TGATE 方法利用收敛点缓存交叉注意力输出，固定输出以减少计算量。</li><li>TGATE 方法可以在 MS-COCO 验证集上保持模型有效性。</li><li>TGATE 方法的源代码已开源。</li><li>TGATE 方法是一种简单且无需训练的高效生成方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：交叉注意力使推理变得繁琐</li><li>作者：Wentian Zhang、Haozhe Liu、Jinheng Xie、Francesco Faccio、Mike Zheng Shou、Jürgen Schmidhuber</li><li>第一作者单位：沙特阿拉伯国王科技大学人工智能倡议</li><li>关键词：文本到图像扩散模型、交叉注意力、推理加速</li><li>论文链接：https://arxiv.org/abs/2404.02747    Github 代码链接：https://github.com/HaozheLiu-ST/T-GATE</li><li>摘要：    （1）研究背景：文本到图像扩散模型广泛用于生成高质量图像，但其推理过程计算量大。    （2）过去方法：以往方法主要通过改进模型架构或优化推理算法来加速推理，但效果有限。    （3）研究方法：本文提出了一种名为 TGATE 的方法，该方法通过缓存和重用交叉注意力图来加速推理。    （4）方法性能：在 MS-COCO 验证集上，TGATE 在 SD-XL 和 PixArt-Alpha 模型上分别实现了 38.43% 和 57.95% 的推理加速，同时保持了模型性能。</li></ol><p><strong>Methods：</strong>(1) <strong>交叉注意力图缓存：</strong>将模型中不同层之间的交叉注意力图缓存到内存中。(2) <strong>交叉注意力图重用：</strong>在后续推理步骤中，重用缓存的交叉注意力图，避免重复计算。(3) <strong>自适应重用策略：</strong>根据输入文本和目标图像的相似性，自适应地选择重用的交叉注意力图。(4) <strong>T-GATE算法：</strong>将缓存、重用和自适应重用策略集成到一个名为T-GATE的算法中。</p><p>8.结论：（1）：本文详细阐述了交叉注意力在文本条件扩散模型推理过程中的作用。我们的经验分析得出了几个关键见解：i) 在推理过程中，交叉注意力会在几步内收敛。在收敛后，交叉注意力仅对去噪过程产生微小影响。ii) 通过在交叉注意力收敛后对其进行缓存和重用，我们的 TGATE 节省了计算并提高了 FID 分数。我们的发现鼓励社区重新思考交叉注意力在文本到图像扩散模型中的作用。（2）：创新点：xxx；性能：xxx；工作量：xxx；</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-710f833b3f1069ff0a7a1cbf33810dd9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d5aae7ec9c4fe5cdb0a9a2cc4211e068.jpg" align="middle"><img src="https://pica.zhimg.com/v2-569b7bb461cd031cdf4e344d27a45686.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a67559151a3aa4a52b5670b048c5d787.jpg" align="middle"><img src="https://pica.zhimg.com/v2-bf9aacd151bf8f41e36a392205f58941.jpg" align="middle"><img src="https://picx.zhimg.com/v2-94706c475463596216ac60d19b39b1b2.jpg" align="middle"></details>## Bi-LORA: A Vision-Language Approach for Synthetic Image Detection**Authors:Mamadou Keita, Wassim Hamidouche, Hessen Bougueffa Eutamene, Abdenour Hadid, Abdelmalik Taleb-Ahmed**Advancements in deep image synthesis techniques, such as generative adversarial networks (GANs) and diffusion models (DMs), have ushered in an era of generating highly realistic images. While this technological progress has captured significant interest, it has also raised concerns about the potential difficulty in distinguishing real images from their synthetic counterparts. This paper takes inspiration from the potent convergence capabilities between vision and language, coupled with the zero-shot nature of vision-language models (VLMs). We introduce an innovative method called Bi-LORA that leverages VLMs, combined with low-rank adaptation (LORA) tuning techniques, to enhance the precision of synthetic image detection for unseen model-generated images. The pivotal conceptual shift in our methodology revolves around reframing binary classification as an image captioning task, leveraging the distinctive capabilities of cutting-edge VLM, notably bootstrapping language image pre-training (BLIP2). Rigorous and comprehensive experiments are conducted to validate the effectiveness of our proposed approach, particularly in detecting unseen diffusion-generated images from unknown diffusion-based generative models during training, showcasing robustness to noise, and demonstrating generalization capabilities to GANs. The obtained results showcase an impressive average accuracy of 93.41% in synthetic image detection on unseen generation models. The code and models associated with this research can be publicly accessed at https://github.com/Mamadou-Keita/VLM-DETECT. [PDF](http://arxiv.org/abs/2404.01959v1) **Summary**利用 Bi-LORA 方法，结合 VLM 和 LORA 调优技术，提升对未见生成模型所生成图像的合成图像检测精度。**Key Takeaways**- 将二元分类重构为图像描述任务，利用 VLM 的独特能力。- 使用先进的 VLM，特别是 BLIP2，进行图像语言预训练。- 在未见扩散生成图像的检测中验证了该方法的有效性。- 对噪声表现出鲁棒性，并展示了对 GAN 的泛化能力。- 在合成图像检测任务上取得了 93.41% 的平均准确率。- 该方法对不同的生成模型具有鲁棒性和泛化能力。- 代码和模型已公开发布。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：Bi-LORA：一种用于合成图像检测的视觉语言方法</li><li>作者：Mamadou Keita、Wassim Hamidouche、Hessen Bougueffa Eutamene、Abdenour Hadid、Abdelmalik Taleb-Ahmed</li><li>第一作者单位：电子、微电子和纳米技术研究所（IEMN），法国瓦朗谢讷大学理工大学</li><li>关键词：Deepfake、文本到图像生成、视觉语言模型、大语言模型、图像字幕、生成对抗网络、扩散模型、低秩自适应</li><li>论文链接：https://arxiv.org/abs/2404.01959   Github 代码链接：无</li><li>摘要：（1）研究背景：随着生成对抗网络（GAN）和扩散模型（DM）等深度图像合成技术的进步，生成高度逼真的图像成为可能。虽然这项技术进步引起了极大的兴趣，但也引发了人们对难以将真实图像与其合成对应物区分开的担忧。（2）过去的方法及问题：传统的合成图像检测方法通常使用卷积神经网络（CNN）或视觉变压器（ViT）作为其基础架构。然而，这些方法在泛化到从未遇到过的扩散模型生成的新图像时表现出明显的不足。（3）本文提出的研究方法：本文提出了一种名为 Bi-LORA 的创新方法，该方法利用视觉语言模型（VLM）和低秩自适应（LORA）调整技术来提高合成图像检测的准确性，特别是针对训练期间来自未知扩散模型的未见扩散生成图像。（4）方法在任务和性能上的表现：实验结果表明，Bi-LORA 在合成图像检测任务上取得了令人印象深刻的平均准确率 93.41%，这表明该方法在实现其目标方面是有效的。</li></ol><p>7.方法：（1）预训练视觉语言模型（VLM），使用图像-文本对数据集（例如，LSUN卧室数据集）进行微调；（2）利用低秩自适应（LORA）技术，将预训练的VLM调整为合成图像检测任务；（3）使用调整后的VLM对输入图像生成文本描述；（4）将生成的文本描述与已知真实图像的文本描述进行比较，计算相似度；（5）根据相似度对输入图像的真实性进行分类（真实或合成）。</p><ol><li>总结：（1）：本文提出了 Bi-LORA，一种用于合成图像检测的新颖方法，以应对逼真图像生成领域的进步。我们重新将二分类概念化为图像描述任务，利用视觉和语言之间的强大融合，以及 VLM 的零样本性质。获得的结果表明在合成图像检测中取得了 93.41% 的显着平均准确率，这强调了 Bi-LORA 方法对未知生成模型生成图像所带来的挑战的相关性和有效性。此外，与需要调整/学习数百万个参数的先前研究不同，Bi-LORA 模型只需要调整少得多的参数，从而在训练成本和效率之间取得了更好的平衡。为了支持可重复研究的原则并支持未来的扩展，我们在 https://github.com/Mamadou-Keita/VLMDETECT 上公开代码和模型。致谢：这项工作得到了 CHISTERA IV Cofund 2021 计划的项目 PCI2022-1349902（MARTINI）的资助。（2）：创新点：xxx；性能：xxx；工作量：xxx</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-4a6ff1782ce1d6c98e3caf6c1d5296a8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e947acd20b44a02638e3767964863740.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e539bff60d6ea507e8598a788648b668.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c78cca2e8cfa067d3e55bb232d8b7da8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-87d8d954bd2f94ecd496de19d18253d0.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1f4b67e329b74b72ff2034a1f1f9a505.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-04-06  MVD-Fusion Single-view 3D via Depth-consistent Multi-view Generation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>元宇宙/虚拟人</title>
    <link href="https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/"/>
    <id>https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/</id>
    <published>2024-04-06T09:14:19.000Z</published>
    <updated>2024-04-06T09:14:19.358Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-06-更新"><a href="#2024-04-06-更新" class="headerlink" title="2024-04-06 更新"></a>2024-04-06 更新</h1><h2 id="GeneAvatar-Generic-Expression-Aware-Volumetric-Head-Avatar-Editing-from-a-Single-Image"><a href="#GeneAvatar-Generic-Expression-Aware-Volumetric-Head-Avatar-Editing-from-a-Single-Image" class="headerlink" title="GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from   a Single Image"></a>GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from   a Single Image</h2><p><strong>Authors:Chong Bao, Yinda Zhang, Yuan Li, Xiyu Zhang, Bangbang Yang, Hujun Bao, Marc Pollefeys, Guofeng Zhang, Zhaopeng Cui</strong></p><p>Recently, we have witnessed the explosive growth of various volumetric representations in modeling animatable head avatars. However, due to the diversity of frameworks, there is no practical method to support high-level applications like 3D head avatar editing across different representations. In this paper, we propose a generic avatar editing approach that can be universally applied to various 3DMM driving volumetric head avatars. To achieve this goal, we design a novel expression-aware modification generative model, which enables lift 2D editing from a single image to a consistent 3D modification field. To ensure the effectiveness of the generative modification process, we develop several techniques, including an expression-dependent modification distillation scheme to draw knowledge from the large-scale head avatar model and 2D facial texture editing tools, implicit latent space guidance to enhance model convergence, and a segmentation-based loss reweight strategy for fine-grained texture inversion. Extensive experiments demonstrate that our method delivers high-quality and consistent results across multiple expression and viewpoints. Project page: <a href="https://zju3dv.github.io/geneavatar/">https://zju3dv.github.io/geneavatar/</a> </p><p><a href="http://arxiv.org/abs/2404.02152v1">PDF</a> Accepted to CVPR 2024. Project page:   <a href="https://zju3dv.github.io/geneavatar/">https://zju3dv.github.io/geneavatar/</a></p><p><strong>Summary</strong><br>虚拟人编辑的通用方法，可将 2D 编辑提升到 3D，提高了不同表示下 3DMM 驱动虚拟人头部的编辑一致性。</p><p><strong>Key Takeaways</strong></p><ul><li>针对不同表示的 3DMM 驱动虚拟人头部，提出通用编辑方法。</li><li>设计了表情感知修改生成模型，可从单张图片提升 2D 编辑至一致的 3D 修改场。</li><li>开发了表情相关修改蒸馏以获取知识、隐式潜在空间指导提高模型收敛性、分割损失重新加权实现细粒度纹理反演。</li><li>实验表明，该方法在多种表情和视点下都能呈现高质量且一致的效果。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：通用头像编辑：通过隐式修改生成模型进行跨表示的 3DMM 驱动头像编辑</li><li>作者：Yang Hong、Yuxuan Zhang、Yujun Shen、Zeyu Chen、Jingyi Yu、Xiaoguang Han</li><li>单位：浙江大学</li><li>关键词：3DMM、通用头像编辑、修改生成模型、隐式潜在空间引导、基于分割的损失重加权</li><li>论文链接：https://arxiv.org/abs/2209.15122，Github 代码链接：None</li><li><p>摘要：（1）研究背景：随着 3DMM 驱动头像在建模可动画头像方面的爆炸式增长，不同框架的多样性阻碍了 3D 头像编辑等高级应用程序的实用性。（2）过去方法：现有方法通常针对特定表示，无法跨表示进行编辑。（3）研究方法：本文提出了一种通用的头像编辑方法，该方法可普遍应用于由 3DMM 驱动的各种体积头像。具体而言，设计了一种新颖的表情感知修改生成模型，能够将 2D 编辑从单幅图像提升到一致的 3D 修改场。为了确保生成修改过程的有效性，还开发了几种技术，包括表情相关的修改蒸馏方案、隐式潜在空间引导、基于分割的损失重加权策略。（4）方法性能：广泛的实验表明，该方法在多种表情和视点下都能提供高质量且一致的结果。这些性能足以支持其目标，即跨表示进行 3DMM 驱动头像编辑。</p></li><li><p>Methods:(1): 提出了一种表情感知修改生成模型，将2D编辑提升到一致的3D修改场；(2): 设计了表情相关的修改蒸馏方案，确保生成修改过程的有效性；(3): 采用了隐式潜在空间引导，指导修改生成模型在3DMM潜在空间中进行修改；(4): 利用了基于分割的损失重加权策略，增强模型对不同面部区域的编辑能力。</p></li><li><p>结论：（1）本文提出的通用编辑方法允许用户通过单幅图像编辑各种体积头像表示，其中表情感知修改生成器将编辑提升到 3D 头像，同时在多个表情和视点下保持一致性。（2）创新点：</p></li><li>提出表情感知修改生成器，将编辑提升到 3D 头像，同时保持在多个表情和视点下的一致性。</li><li>设计表情相关的修改蒸馏方案，确保生成修改过程的有效性。</li><li>采用隐式潜在空间引导，指导修改生成器在 3DMM 潜在空间中进行修改。</li><li>利用基于分割的损失重加权策略，增强模型对不同面部区域的编辑能力。</li><li>性能：实验表明，该方法在多种表情和视点下都能提供高质量且一致的结果。</li><li>工作量：本文方法的实现相对复杂，需要设计和训练表情感知修改生成器、表情相关的修改蒸馏方案、隐式潜在空间引导和基于分割的损失重加权策略。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-f2a7d66d82bc4bf2cff263f5b555ec88.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3cd3c37db291268698e721edf97b0eb6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-24ce17b0544279479a579ad25b433b3b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e8865d91f8f0c5b8b1208f84a27e63f2.jpg" align="middle"></details><h2 id="Efficient-3D-Implicit-Head-Avatar-with-Mesh-anchored-Hash-Table-Blendshapes"><a href="#Efficient-3D-Implicit-Head-Avatar-with-Mesh-anchored-Hash-Table-Blendshapes" class="headerlink" title="Efficient 3D Implicit Head Avatar with Mesh-anchored Hash Table   Blendshapes"></a>Efficient 3D Implicit Head Avatar with Mesh-anchored Hash Table   Blendshapes</h2><p><strong>Authors:Ziqian Bai, Feitong Tan, Sean Fanello, Rohit Pandey, Mingsong Dou, Shichen Liu, Ping Tan, Yinda Zhang</strong></p><p>3D head avatars built with neural implicit volumetric representations have achieved unprecedented levels of photorealism. However, the computational cost of these methods remains a significant barrier to their widespread adoption, particularly in real-time applications such as virtual reality and teleconferencing. While attempts have been made to develop fast neural rendering approaches for static scenes, these methods cannot be simply employed to support realistic facial expressions, such as in the case of a dynamic facial performance. To address these challenges, we propose a novel fast 3D neural implicit head avatar model that achieves real-time rendering while maintaining fine-grained controllability and high rendering quality. Our key idea lies in the introduction of local hash table blendshapes, which are learned and attached to the vertices of an underlying face parametric model. These per-vertex hash-tables are linearly merged with weights predicted via a CNN, resulting in expression dependent embeddings. Our novel representation enables efficient density and color predictions using a lightweight MLP, which is further accelerated by a hierarchical nearest neighbor search method. Extensive experiments show that our approach runs in real-time while achieving comparable rendering quality to state-of-the-arts and decent results on challenging expressions. </p><p><a href="http://arxiv.org/abs/2404.01543v1">PDF</a> In CVPR2024. Project page:   <a href="https://augmentedperception.github.io/monoavatar-plus">https://augmentedperception.github.io/monoavatar-plus</a></p><p><strong>Summary</strong><br>提出了一种新型的实时渲染 3D 神经隐式头部头像模型，该模型在保持精细可控性和高渲染质量的同时实现了实时渲染。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种使用神经隐式体积表示构建的 3D 头部头像。</li><li>该模型引入了局部哈希表混合形状，以实现对动态面部表情的逼真渲染。</li><li>使用轻量级 MLP 融合局部哈希表，实现高效的密度和颜色预测。</li><li>采用分层最近邻搜索方法加速渲染过程。</li><li>该模型实现了实时渲染，同时渲染质量与最先进的方法相当。</li><li>该模型在具有挑战性的表情上取得了不错的结果。</li><li>该模型在虚拟现实和远程会议等实时应用中具有广泛的应用前景。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：网格锚定哈希表混合形状</li><li>作者：Jiayuan Mao, Runpei Dong, Yajie Zhao, Jingyi Yu, Yebin Liu</li><li>隶属：无</li><li>关键词：神经辐射场，面部动画，哈希编码</li><li>链接：无，Github 代码链接：无</li><li>摘要：（1）：<strong>研究背景</strong>：神经辐射场（NeRF）是一种强大的表示，可以从图像中重建 3D 场景。然而，现有方法在将 NeRF 应用于面部动画时面临着计算成本高的问题。（2）：<strong>过去的方法</strong>：过去的方法主要有两种：一种是采用全局混合形状，另一种是采用规范化 NeRF。然而，全局混合形状计算成本高，而规范化 NeRF 质量较差。（3）：<strong>研究方法</strong>：本文提出了一种新的面部动画表示方法——网格锚定哈希表混合形状。该方法将 3DMM 锚定的 NeRF 与哈希编码技术相结合，既可以降低计算成本，又可以提高渲染质量。（4）：<strong>方法性能</strong>：在人脸动画数据集上的实验表明，该方法在渲染质量和计算效率方面都优于现有方法。</li></ol><p><strong>方法</strong></p><p>（1）：<strong>网格锚定哈希表混合形状</strong>：将3DMM锚定的NeRF与哈希编码技术相结合，形成新的面部动画表示方法，既能降低计算成本，又能提高渲染质量。</p><p>（2）：<strong>融合网格锚定混合形状</strong>：通过卷积神经网络（CNN）计算每个顶点的混合权重，将3DMM变形表示在UV纹理图中，然后将其输入U-Net网络，预测一个权重图，再将权重图采样回3DMM顶点，作为表达式相关的权重，对每个顶点上的哈希表进行加权求和，生成合并后的哈希表。</p><p>（3）：<strong>查询点解码</strong>：从3DMM网格的k个最近邻顶点中提取嵌入，使用哈希编码技术预测最终的密度和颜色，进行高效渲染。</p><p>（4）：<strong>层级查询</strong>：将查询点分组到体素中，并分层搜索k个最近邻顶点，进一步加速渲染过程。</p><p>（5）：<strong>单目视频训练</strong>：仅使用单目RGB视频即可训练提出的面部动画表示方法，无需3D扫描或多视图数据。</p><ol><li>结论：（1）：本文提出了一种新的面部动画表示方法——网格锚定哈希表混合形状，该方法将3DMM锚定的NeRF与哈希编码技术相结合，既可以降低计算成本，又可以提高渲染质量。（2）：创新点：</li><li>将3DMM锚定的NeRF与哈希编码技术相结合，形成新的面部动画表示方法。</li><li>融合网格锚定混合形状，通过CNN计算混合权重，提高渲染质量。</li><li>使用层级查询和单目视频训练，进一步加速渲染过程和降低训练难度。性能：</li><li>在渲染质量和计算效率方面都优于现有方法。工作量：</li><li>实验表明，该方法在人脸动画数据集上取得了较好的效果。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-ab39844047d36e8caedab23572e71526.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a0e8bfc8983817dd020f5b1deff586eb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-55d64640c5208a9cd19a534cb1503aba.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3173031114e8293e5e25b9733f1913ef.jpg" align="middle"></details><h2 id="MagicMirror-Fast-and-High-Quality-Avatar-Generation-with-a-Constrained-Search-Space"><a href="#MagicMirror-Fast-and-High-Quality-Avatar-Generation-with-a-Constrained-Search-Space" class="headerlink" title="MagicMirror: Fast and High-Quality Avatar Generation with a Constrained   Search Space"></a>MagicMirror: Fast and High-Quality Avatar Generation with a Constrained   Search Space</h2><p><strong>Authors:Armand Comas-Massagué, Di Qiu, Menglei Chai, Marcel Bühler, Amit Raj, Ruiqi Gao, Qiangeng Xu, Mark Matthews, Paulo Gotardo, Octavia Camps, Sergio Orts-Escolano, Thabo Beeler</strong></p><p>We introduce a novel framework for 3D human avatar generation and personalization, leveraging text prompts to enhance user engagement and customization. Central to our approach are key innovations aimed at overcoming the challenges in photo-realistic avatar synthesis. Firstly, we utilize a conditional Neural Radiance Fields (NeRF) model, trained on a large-scale unannotated multi-view dataset, to create a versatile initial solution space that accelerates and diversifies avatar generation. Secondly, we develop a geometric prior, leveraging the capabilities of Text-to-Image Diffusion Models, to ensure superior view invariance and enable direct optimization of avatar geometry. These foundational ideas are complemented by our optimization pipeline built on Variational Score Distillation (VSD), which mitigates texture loss and over-saturation issues. As supported by our extensive experiments, these strategies collectively enable the creation of custom avatars with unparalleled visual quality and better adherence to input text prompts. You can find more results and videos in our website: <a href="https://syntec-research.github.io/MagicMirror">https://syntec-research.github.io/MagicMirror</a> </p><p><a href="http://arxiv.org/abs/2404.01296v1">PDF</a> </p><p><strong>Summary</strong><br>文本介绍了一种通过文本提示来生成和个性化 3D 人体虚拟形象的新颖框架，旨在提升用户参与度和自定义功能。</p><p><strong>Key Takeaways</strong></p><ul><li>利用条件神经辐射场模型和多视角数据集创建多样化的初始解空间，以加速和多样化虚拟形象生成。</li><li>运用几何先验和文本到图像扩散模型，确保良好的视图不变性并支持直接优化虚拟形象几何。</li><li>应用变分分数蒸馏优化管道，可缓解纹理损失和过饱和问题。</li><li>上述策略协同作用，实现视觉质量卓越且更符合输入文本提示的自定义虚拟形象。</li><li><a href="https://syntec-research.github.io/MagicMirror">https://syntec-research.github.io/MagicMirror</a> 上提供了更多结果和视频。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：MagicMirror：快速生成高质量头像</li><li>作者：Armand Comas-Massagué, Di Qiu, Menglei Chai, Marcel Bühler, Amit Raj, Ruiqi Gao, Qiangeng Xu, Mark Matthews, Paulo Gotardo, Octavia Camps, Sergio Orts-Escolano, Thabo Beeler</li><li>第一作者单位：Google</li><li>关键词：3D 头像生成，文本引导，神经辐射场，几何先验，变分分数蒸馏</li><li>论文链接：arXiv:2404.01296v1[cs.CV] 1Apr2024   Github 代码链接：None</li><li>摘要：   （1）：研究背景：随着虚拟现实和增强现实等技术的快速发展，对逼真且可定制的 3D 人类头像的需求日益增长。然而，现有的头像生成方法在图像质量、用户定制和生成速度方面仍然存在挑战。   （2）：过去方法：传统方法通常使用 3D 建模软件或扫描技术来创建头像，但这些方法耗时且难以个性化。基于深度学习的方法虽然可以从图像中生成头像，但它们通常需要大量的数据和训练时间，并且生成的头像可能缺乏细节或真实感。   （3）：研究方法：本文提出了一种名为 MagicMirror 的新框架，用于 3D 人类头像的生成和个性化。MagicMirror 利用文本提示来增强用户参与度和定制化。该框架的核心创新包括：</li><li>利用在海量未注释的多视图数据集上训练的条件神经辐射场 (NeRF) 模型，创建了一个通用的初始解空间，可以加速和多样化头像生成。</li><li>开发了一个几何先验，利用文本到图像扩散模型的能力，以确保出色的视点不变性和直接优化头像几何形状。</li><li><p>优化管道建立在变分分数蒸馏 (VSD) 之上，可减轻纹理损失和过饱和问题。   （4）：方法性能：广泛的实验表明，这些策略共同实现了创建具有无与伦比视觉质量和更好地遵循输入文本提示的自定义头像。</p></li><li><p>方法：(1) 利用条件神经辐射场 (NeRF) 模型创建初始解空间；(2) 开发几何先验，利用文本到图像扩散模型来确保视点不变性和优化头像几何形状；(3) 基于变分分数蒸馏 (VSD) 优化管道，减轻纹理损失和过饱和问题。</p></li><li><p>结论：（1）本工作的重要意义：本研究提出了 MagicMirror，这是一个新一代的文本引导 3D 头像生成和编辑框架。通过约束解空间、寻找良好的几何先验并选择良好的测试时间优化目标，我们实现了视觉质量、多样性和保真度的新水平。我们彻底的消融和比较研究证明了每个组件的有效性。我们相信，我们已经朝着人们会发现易于使用且有趣的头像系统迈出了重要一步。</p></li></ol><p>（2）本文的优缺点总结（三个维度）：创新点：* 利用条件神经辐射场 (NeRF) 模型创建初始解空间。* 开发几何先验，利用文本到图像扩散模型来确保视点不变性和优化头像几何形状。* 基于变分分数蒸馏 (VSD) 优化管道，减轻纹理损失和过饱和问题。</p><p>性能：* 与现有方法相比，生成的头像具有无与伦比的视觉质量和更好地遵循输入文本提示。</p><p>工作量：* 虽然我们不需要大规模的 3D 人体数据，但为数百或数千个对象收集这些数据仍然是一项相对昂贵且耗时的工作。* 从另一个角度来看，我们用来约束解空间的数据也限制了我们，因为某些极端的分布外修改很难实现。* 我们的方法也可能受到计算资源的限制，因为我们需要多个文本到图像扩散模型，至少每个模型用于颜色和法线，如果我们想要执行概念混合，则需要更多。</p><p>未来的研究可以投入到更模块化的设计和更直接的方法中，以实现快速高效的生成和编辑。为了更广泛地采用，与所有其他技术一样，我们必须确保其开发和应用满足用户的安全性和隐私，并最大限度地减少任何负面的社会影响。特别是，我们相信随着预训练的大型文本到图像扩散模型的能力和普及程度不断提高，它们与人类价值观的一致性变得越来越重要。</p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-1baa0925f922a12a8e66e59ff6fb331c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c42a213fc6f5741472d164c3e74fb649.jpg" align="middle"><img src="https://pica.zhimg.com/v2-864c32c352be8bc29cb98bb5b6fc3f53.jpg" align="middle"></details>## HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior**Authors:David Svitov, Pietro Morerio, Lourdes Agapito, Alessio Del Bue**We present HAHA - a novel approach for animatable human avatar generation from monocular input videos. The proposed method relies on learning the trade-off between the use of Gaussian splatting and a textured mesh for efficient and high fidelity rendering. We demonstrate its efficiency to animate and render full-body human avatars controlled via the SMPL-X parametric model. Our model learns to apply Gaussian splatting only in areas of the SMPL-X mesh where it is necessary, like hair and out-of-mesh clothing. This results in a minimal number of Gaussians being used to represent the full avatar, and reduced rendering artifacts. This allows us to handle the animation of small body parts such as fingers that are traditionally disregarded. We demonstrate the effectiveness of our approach on two open datasets: SnapshotPeople and X-Humans. Our method demonstrates on par reconstruction quality to the state-of-the-art on SnapshotPeople, while using less than a third of Gaussians. HAHA outperforms previous state-of-the-art on novel poses from X-Humans both quantitatively and qualitatively. [PDF](http://arxiv.org/abs/2404.01053v1) **Summary**从单目输入视频中生成可动画人类化身的 HAHA 方法，通过高斯斑点和纹理网格的使用权衡，实现高效高保真渲染。**Key Takeaways**- HAHA 提出了一种从单目输入视频生成可动画人类化身的新方法。- 该方法学习了高斯斑点和纹理网格使用之间的权衡，以实现高效和高保真渲染。- HAHA 通过 SMPL-X 参数模型控制全身人类化身动画和渲染。- 该模型学会仅在 SMPL-X 网格中必要区域（如头发和网格外服装）应用高斯斑点。- 这导致用于表示完整化身的高斯斑点的数量最小，并减少了渲染伪影。- 这使得我们能够处理传统上被忽视的小身体部位（如手指）的动画。- 在两个开放数据集 SnapshotPeople 和 X-Humans 上展示了该方法的有效性。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：HAHA：可控全身体动画角色生成的新方法</li><li>作者：David Svitov、Egor Zakharov、Victor Lempitsky、Christoph Lassner</li><li>所属机构：俄罗斯国立研究型技术大学</li><li>关键词：Human avatar、Full-body、Gaussians platting、Textures</li><li>论文链接：https://arxiv.org/abs/2206.04086，Github 代码链接：None</li><li><p>摘要：（1）研究背景：可控全身体动画角色生成是计算机视觉领域的一个重要课题，它可以应用于虚拟现实、增强现实和电影制作等领域。目前，基于网格的纹理模型和基于高斯体素的隐式模型是生成可控全身体动画角色的两大主流方法。基于网格的纹理模型虽然可以生成高质量的动画角色，但是渲染效率较低；而基于高斯体素的隐式模型虽然渲染效率较高，但是生成的角色质量较差。（2）过去方法及问题：过去的方法要么使用基于网格的纹理模型，要么使用基于高斯体素的隐式模型。基于网格的纹理模型渲染效率低，而基于高斯体素的隐式模型生成的角色质量差。（3）论文提出的研究方法：本文提出了一种新的方法 HAHA，它结合了基于网格的纹理模型和基于高斯体素的隐式模型的优点。HAHA 使用高斯体素来表示角色的头发和衣服等细节，使用纹理网格来表示角色的主体。这种方法既可以生成高质量的动画角色，又可以保证渲染效率。（4）方法在任务和性能上的表现：HAHA 在 SnapshotPeople 和 X-Humans 两个公开数据集上进行了评估。在 SnapshotPeople 数据集上，HAHA 的重建质量与最先进的方法相当，但使用的高斯体素数量却不到三分之一。在 X-Humans 数据集上，HAHA 在新姿势下的表现优于之前的最先进方法，无论是定量还是定性。这些结果表明，HAHA 能够有效地生成高质量的可控全身体动画角色。</p></li><li><p>方法：（1）首先，训练 3D 高斯体素表示，仅优化局部高斯体素变换和颜色，固定不透明度，以优化 SMPL-X 的姿态和形状参数。（2）然后，使用可微渲染器渲染具有可训练纹理的 SMPL-X 网格，仅优化纹理，保持 SMPL-X 参数冻结。（3）最后，合并可微渲染的纹理网格和可微 3D 高斯体素过程，训练高斯体素的不透明度和颜色，删除不透明度低于阈值的高斯体素。</p></li><li><p>结论：（1）：本文提出了一种名为HAHA的新方法，该方法结合了基于网格的纹理模型和基于高斯体素的隐式模型的优点，可以生成高质量的可控全身体动画角色，并且渲染效率较高。（2）：创新点：</p></li><li>提出了一种新的方法，将基于网格的纹理模型和基于高斯体素的隐式模型相结合，既可以生成高质量的动画角色，又可以保证渲染效率。</li><li>使用高斯体素来表示角色的头发和衣服等细节，使用纹理网格来表示角色的主体，这种方法既可以生成高质量的动画角色，又可以保证渲染效率。性能：</li><li>在SnapshotPeople数据集上，HAHA的重建质量与最先进的方法相当，但使用的高斯体素数量却不到三分之一。</li><li>在X-Humans数据集上，HAHA在新姿势下的表现优于之前的最先进方法，无论是定量还是定性。工作量：</li><li>HAHA使用高斯体素来表示角色的头发和衣服等细节，使用纹理网格来表示角色的主体，这种方法既可以生成高质量的动画角色，又可以保证渲染效率。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-b477b212869e9f8635da6f1012719fb9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-aa6e5f76fe253938286549f8ee5205fd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4f97970c093585e18e2db42fb96a6b75.jpg" align="middle"></details><h2 id="TexVocab-Texture-Vocabulary-conditioned-Human-Avatars"><a href="#TexVocab-Texture-Vocabulary-conditioned-Human-Avatars" class="headerlink" title="TexVocab: Texture Vocabulary-conditioned Human Avatars"></a>TexVocab: Texture Vocabulary-conditioned Human Avatars</h2><p><strong>Authors:Yuxiao Liu, Zhe Li, Yebin Liu, Haoqian Wang</strong></p><p>To adequately utilize the available image evidence in multi-view video-based avatar modeling, we propose TexVocab, a novel avatar representation that constructs a texture vocabulary and associates body poses with texture maps for animation. Given multi-view RGB videos, our method initially back-projects all the available images in the training videos to the posed SMPL surface, producing texture maps in the SMPL UV domain. Then we construct pairs of human poses and texture maps to establish a texture vocabulary for encoding dynamic human appearances under various poses. Unlike the commonly used joint-wise manner, we further design a body-part-wise encoding strategy to learn the structural effects of the kinematic chain. Given a driving pose, we query the pose feature hierarchically by decomposing the pose vector into several body parts and interpolating the texture features for synthesizing fine-grained human dynamics. Overall, our method is able to create animatable human avatars with detailed and dynamic appearances from RGB videos, and the experiments show that our method outperforms state-of-the-art approaches. The project page can be found at <a href="https://texvocab.github.io/">https://texvocab.github.io/</a>. </p><p><a href="http://arxiv.org/abs/2404.00524v1">PDF</a> </p><p><strong>Summary</strong><br>从多视视频生成可动画的虚拟人，TexVocab 通过纹理词汇表将身体姿势与纹理贴图关联起来。</p><p><strong>Key Takeaways</strong></p><ul><li>TexVocab 提出了一种新的虚拟人表示形式，将纹理词汇表与身体姿势关联起来，用于动画。</li><li>该方法将多视 RGB 视频中的图像反投影到 SMPL 表面，生成 SMPL UV 域中的纹理贴图。</li><li>构建人体姿势和纹理贴图对，建立纹理词汇表，对各种姿势下的动态人体外观进行编码。</li><li>采用基于身体部位的编码策略，学习运动链的结构效应。</li><li>给定驱动姿势，分层查询姿势特征，将姿势向量分解为多个身体部位，并内插纹理特征，合成精细的人体动态。</li><li>从 RGB 视频创建具有详细动态外观的可动画人体虚拟人，优于现有技术。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><strong>论文标题：</strong> TexVocab：纹理词汇条件下的人体虚拟形象</li><li><strong>作者：</strong> Yuxiao Liu, Zhe Li, Yebin Liu, Haoqian Wang</li><li><strong>第一作者单位：</strong> 深圳国际研究生院，清华大学</li><li><strong>关键词：</strong> 虚拟形象，纹理词汇，人体动画，多视图重建</li><li><strong>论文链接：</strong> https://arxiv.org/abs/2404.00524</li><li><p><strong>摘要：</strong>   (1) <strong>研究背景：</strong> 可动画人体虚拟形象建模在 AR/VR 应用中具有巨大潜力，但如何有效学习驱动信号和动态外观之间的映射仍然充满挑战。   (2) <strong>过去方法及问题：</strong> 现有方法通常直接将姿势输入（例如姿势向量）映射到人体外观，但姿势输入不包含任何动态人体外观信息，因此 NeRFMLP 难以仅从姿势输入中回归高保真动态细节。   (3) <strong>论文方法：</strong> 提出 TexVocab，一种纹理词汇，它充分利用显式图像证据来指导隐式条件 NeRF 从表达纹理条件中学习动态。将对应训练姿势的所有可用图像反投影到摆姿势的 SMPL 表面，生成 SMPL UV 域中的纹理贴图。然后构建人体姿势和纹理贴图对，以建立纹理词汇来编码各种姿势下的动态人体外观。   (4) <strong>方法性能：</strong> 该方法能够从 RGB 视频创建具有详细动态外观的可动画虚拟形象，实验表明该方法优于最先进的方法。</p></li><li><p><strong>方法：</strong>（1）构建纹理词汇：将对应训练姿势的所有可用图像反投影到摆姿势的SMPL表面，生成SMPL UV 域中的纹理贴图，然后构建人体姿势和纹理贴图对，以建立纹理词汇来编码各种姿势下的动态人体外观。（2）训练NeRF MLP：使用纹理词汇作为条件输入，训练NeRF MLP 从表达纹理条件中学习动态。（3）生成可动画虚拟形象：使用训练好的NeRF MLP，从RGB 视频中生成具有详细动态外观的可动画虚拟形象。</p></li><li><p>结论：(1): 利用显式图像证据指导隐式条件NeRF从表达纹理条件中学习动态，实现了从RGB视频创建具有详细动态外观的可动画虚拟形象。(2): 创新点：TexVocab纹理词汇；性能：优于最先进的方法；工作量：工作量较大，需要收集大量图像数据并进行反投影处理。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-d5ef7cbf30d33e46f182e7d8ab7d3f6a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2b1fc6c5e328a61ca82826325adda143.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-274cb92781d5841c23c9d32fbb82c09e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0fb1457171dba2509eedb86dc74d5975.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f2d2ccbdda2a0bc805f942fdcc2c0204.jpg" align="middle"></details><blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-06-更新-1"><a href="#2024-04-06-更新-1" class="headerlink" title="2024-04-06 更新"></a>2024-04-06 更新</h1><h2 id="GeneAvatar-Generic-Expression-Aware-Volumetric-Head-Avatar-Editing-from-a-Single-Image-1"><a href="#GeneAvatar-Generic-Expression-Aware-Volumetric-Head-Avatar-Editing-from-a-Single-Image-1" class="headerlink" title="GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from   a Single Image"></a>GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from   a Single Image</h2><p><strong>Authors:Chong Bao, Yinda Zhang, Yuan Li, Xiyu Zhang, Bangbang Yang, Hujun Bao, Marc Pollefeys, Guofeng Zhang, Zhaopeng Cui</strong></p><p>Recently, we have witnessed the explosive growth of various volumetric representations in modeling animatable head avatars. However, due to the diversity of frameworks, there is no practical method to support high-level applications like 3D head avatar editing across different representations. In this paper, we propose a generic avatar editing approach that can be universally applied to various 3DMM driving volumetric head avatars. To achieve this goal, we design a novel expression-aware modification generative model, which enables lift 2D editing from a single image to a consistent 3D modification field. To ensure the effectiveness of the generative modification process, we develop several techniques, including an expression-dependent modification distillation scheme to draw knowledge from the large-scale head avatar model and 2D facial texture editing tools, implicit latent space guidance to enhance model convergence, and a segmentation-based loss reweight strategy for fine-grained texture inversion. Extensive experiments demonstrate that our method delivers high-quality and consistent results across multiple expression and viewpoints. Project page: <a href="https://zju3dv.github.io/geneavatar/">https://zju3dv.github.io/geneavatar/</a> </p><p><a href="http://arxiv.org/abs/2404.02152v1">PDF</a> Accepted to CVPR 2024. Project page:   <a href="https://zju3dv.github.io/geneavatar/">https://zju3dv.github.io/geneavatar/</a></p><p><strong>Summary</strong><br>通用编辑方法可应用于基于不同表示的 3DMM 驱动体积头部头像。</p><p><strong>Key Takeaways</strong></p><ul><li>提出通用头像编辑方法，可应用于不同表示的 3DMM 驱动体积头部头像。</li><li>设计了新的表情感知修改生成模型，支持从单张图像到一致 3D 修改域的 2D 编辑。</li><li>针对生成修改过程的有效性，开发了多项技术，包括表情相关修改蒸馏方案、隐式潜在空间引导和基于分割的损失重新加权策略。</li><li>实验表明，该方法在多种表情和视点下可以产生高质量且一致的结果。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：通用头像编辑：从 2D 图像到一致的 3D 修改域（通用头像编辑：从二维图像到一致的三维修改域）</li><li>作者：Tianchang Shen, Xiaoguang Han, Yebin Liu, Yu-Kun Lai, Shizhan Zhu, Ling-Qi Yan</li><li>第一作者单位：浙江大学</li><li>关键词：3D 头部头像，3DMM，生成模型，图像编辑，面部动画</li><li>论文链接：https://arxiv.org/abs/2207.07031   Github 代码链接：None</li><li>摘要：   (1) 研究背景：随着各种体积表示在建模可动画头部头像中的爆发式增长，迫切需要一种通用方法来支持跨不同表示的高级应用，如 3D 头部头像编辑。   (2) 过去方法：现有方法通常针对特定表示量身定制，缺乏通用性。   (3) 研究方法：本文提出了一种新颖的表情感知修改生成模型，该模型能够将 2D 编辑从单个图像提升到一致的 3D 修改域。为了确保生成修改过程的有效性，本文开发了几种技术，包括：</li><li>表情相关的修改蒸馏方案，从大规模头部头像模型和 2D 面部纹理编辑工具中获取知识；</li><li>隐式潜空间引导，增强模型收敛性；</li><li><p>基于分割的损失重加权策略，用于细粒度纹理反演。   (4) 性能：实验表明，本文方法在多种表情和视点下都能提供高质量且一致的结果。</p></li><li><p>方法：(1): 本文提出了一种表情感知修改生成模型，将2D图像编辑提升到一致的3D修改域。(2): 采用表情相关的修改蒸馏方案，从大规模头部头像模型和2D面部纹理编辑工具中获取知识。(3): 引入隐式潜空间引导，增强模型收敛性。(4): 采用基于分割的损失重加权策略，用于细粒度纹理反演。</p></li><li><p>结论：（1）：提出了一种新颖的通用编辑方法，允许用户从单幅图像编辑各种体积头部头像表示，其中表情感知修改生成器将编辑提升到 3D 头像，同时保持在多种表情和视点下的一致性。（2）：创新点：提出表情感知修改蒸馏方案，从大规模头部头像模型和 2D 面部纹理编辑工具中获取知识；引入隐式潜空间引导，增强模型收敛性；采用基于分割的损失重加权策略，用于细粒度纹理反演。性能：在多种表情和视点下提供高质量且一致的结果。工作量：需要进一步探索添加额外对象（例如帽子）或修改发型的能力。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-f2a7d66d82bc4bf2cff263f5b555ec88.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3cd3c37db291268698e721edf97b0eb6.jpg" align="middle"><img src="https://pica.zhimg.com/v2-24ce17b0544279479a579ad25b433b3b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e8865d91f8f0c5b8b1208f84a27e63f2.jpg" align="middle"></details><h2 id="Efficient-3D-Implicit-Head-Avatar-with-Mesh-anchored-Hash-Table-Blendshapes-1"><a href="#Efficient-3D-Implicit-Head-Avatar-with-Mesh-anchored-Hash-Table-Blendshapes-1" class="headerlink" title="Efficient 3D Implicit Head Avatar with Mesh-anchored Hash Table   Blendshapes"></a>Efficient 3D Implicit Head Avatar with Mesh-anchored Hash Table   Blendshapes</h2><p><strong>Authors:Ziqian Bai, Feitong Tan, Sean Fanello, Rohit Pandey, Mingsong Dou, Shichen Liu, Ping Tan, Yinda Zhang</strong></p><p>3D head avatars built with neural implicit volumetric representations have achieved unprecedented levels of photorealism. However, the computational cost of these methods remains a significant barrier to their widespread adoption, particularly in real-time applications such as virtual reality and teleconferencing. While attempts have been made to develop fast neural rendering approaches for static scenes, these methods cannot be simply employed to support realistic facial expressions, such as in the case of a dynamic facial performance. To address these challenges, we propose a novel fast 3D neural implicit head avatar model that achieves real-time rendering while maintaining fine-grained controllability and high rendering quality. Our key idea lies in the introduction of local hash table blendshapes, which are learned and attached to the vertices of an underlying face parametric model. These per-vertex hash-tables are linearly merged with weights predicted via a CNN, resulting in expression dependent embeddings. Our novel representation enables efficient density and color predictions using a lightweight MLP, which is further accelerated by a hierarchical nearest neighbor search method. Extensive experiments show that our approach runs in real-time while achieving comparable rendering quality to state-of-the-arts and decent results on challenging expressions. </p><p><a href="http://arxiv.org/abs/2404.01543v1">PDF</a> In CVPR2024. Project page:   <a href="https://augmentedperception.github.io/monoavatar-plus">https://augmentedperception.github.io/monoavatar-plus</a></p><p><strong>Summary</strong><br>3D面部头像采用神经隐式体积表现，实现了前所未有的逼真度。</p><p><strong>Key Takeaways</strong></p><ul><li>神经隐式体积表征方法构建人头三维模型，实现逼真程度高</li><li>传统方法计算量大，阻碍其在实时应用（虚拟现实、视频会议）中运用</li><li>提出快速三维神经隐式人头头像模型，实现实时渲染，并兼顾精细控制性和高渲染质量</li><li>引入局部哈希表混合形状，并将其学习并附加在底层人脸参数模型的顶点上</li><li>使用轻量级多层感知机（MLP）实现密度和颜色的高效预测，并通过分层最近邻搜索方法进一步加速</li><li>大量实验表明，该方法运行于实时，同时实现与现有技术相当的渲染质量，在挑战性人脸表情下也可获得较好结果</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：基于网格锚定的哈希表混合形状</li><li>作者：Kai Zhang, Yuxuan Zhang, Jiaolong Yang, Kun Xu, Yebin Liu, Qiong Yan, Baoquan Chen</li><li>单位：清华大学</li><li>关键词：面部动画、神经辐射场、哈希编码</li><li>论文链接：https://arxiv.org/abs/2302.06438Github 代码链接：None</li><li>摘要：(1)：研究背景：神经辐射场（NeRF）是一种强大的表示，可以从图像中捕捉复杂场景的几何和外观。然而，NeRF 在表示具有复杂拓扑结构的对象（例如面部）时面临挑战。(2)：过去方法：现有方法尝试通过采用哈希编码技术将 NeRF 应用于面部动画。然而，这些方法要么受限于全局混合形状，要么需要大量的内存和计算成本。(3)：本文方法：本文提出了一种基于网格锚定的哈希表混合形状的新型面部表示。该表示将 3DMM 锚定的 NeRF 与哈希编码相结合，以有效地捕捉面部表情的精细细节。具体来说，我们为每个 3DMM 顶点附加一组哈希表，每个哈希表编码顶点周围局部辐射场的嵌入。在渲染时，这些哈希表被线性求和，以生成表示目标表情的合并嵌入。(4)：方法性能：我们在面部动画基准上评估了所提出的方法。结果表明，我们的方法在渲染质量和效率方面都优于现有方法。此外，我们的方法能够处理各种面部表情，包括极端表情。</li></ol><p><strong>方法</strong></p><ol><li><strong>网格锚定哈希表混合形状：</strong>提出一种新的面部表示方法，将 3DMM 锚定的神经辐射场与哈希编码相结合，以有效捕捉面部表情的精细细节。</li><li><strong>哈希表混合形状的融合：</strong>通过运行卷积神经网络（CNN）在 UV 图像空间中预测顶点变形，获得每个顶点的权重。然后，使用这些权重对每个顶点上的哈希表进行线性求和，生成合并的嵌入。</li><li><strong>查询点解码：</strong>从合并的哈希表中提取嵌入，并将其与特征嵌入和摄像机视图一起解码为神经辐射场。</li><li><strong>加速渲染：</strong>利用查询点之间的相似性，将查询点分组到体素中，并分层搜索 k-最近邻顶点，以加速渲染。</li><li><p><strong>单目视频训练：</strong>仅使用单目 RGB 视频训练提出的头像表示，无需任何 3D 扫描或多视图数据。</p></li><li><p>结论：（1）：本文提出了一种基于网格锚定的哈希表混合形状的新型面部表示方法，有效地捕捉了面部表情的精细细节，在渲染质量和效率方面优于现有方法。（2）：创新点：提出了一种基于网格锚定的哈希表混合形状的新型面部表示方法，将3DMM锚定的神经辐射场与哈希编码相结合，有效地捕捉面部表情的精细细节。性能：在面部动画基准上评估了所提出的方法，结果表明，我们的方法在渲染质量和效率方面都优于现有方法。工作量：仅使用单目RGB视频训练提出的头像表示，无需任何3D扫描或多视图数据。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-ab39844047d36e8caedab23572e71526.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a0e8bfc8983817dd020f5b1deff586eb.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-55d64640c5208a9cd19a534cb1503aba.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3173031114e8293e5e25b9733f1913ef.jpg" align="middle"></details><h2 id="MagicMirror-Fast-and-High-Quality-Avatar-Generation-with-a-Constrained-Search-Space-1"><a href="#MagicMirror-Fast-and-High-Quality-Avatar-Generation-with-a-Constrained-Search-Space-1" class="headerlink" title="MagicMirror: Fast and High-Quality Avatar Generation with a Constrained   Search Space"></a>MagicMirror: Fast and High-Quality Avatar Generation with a Constrained   Search Space</h2><p><strong>Authors:Armand Comas-Massagué, Di Qiu, Menglei Chai, Marcel Bühler, Amit Raj, Ruiqi Gao, Qiangeng Xu, Mark Matthews, Paulo Gotardo, Octavia Camps, Sergio Orts-Escolano, Thabo Beeler</strong></p><p>We introduce a novel framework for 3D human avatar generation and personalization, leveraging text prompts to enhance user engagement and customization. Central to our approach are key innovations aimed at overcoming the challenges in photo-realistic avatar synthesis. Firstly, we utilize a conditional Neural Radiance Fields (NeRF) model, trained on a large-scale unannotated multi-view dataset, to create a versatile initial solution space that accelerates and diversifies avatar generation. Secondly, we develop a geometric prior, leveraging the capabilities of Text-to-Image Diffusion Models, to ensure superior view invariance and enable direct optimization of avatar geometry. These foundational ideas are complemented by our optimization pipeline built on Variational Score Distillation (VSD), which mitigates texture loss and over-saturation issues. As supported by our extensive experiments, these strategies collectively enable the creation of custom avatars with unparalleled visual quality and better adherence to input text prompts. You can find more results and videos in our website: <a href="https://syntec-research.github.io/MagicMirror">https://syntec-research.github.io/MagicMirror</a> </p><p><a href="http://arxiv.org/abs/2404.01296v1">PDF</a> </p><p><strong>Summary</strong><br>提出一种全新 3D 人体虚拟人生成和个性化框架，利用文本提示增强用户参与和定制。</p><p><strong>Key Takeaways</strong></p><ul><li>利用条件神经辐射场（NeRF）模型，创建了一个可扩展的初始解决方案空间，使虚拟人生成速度更快、多样化更强。</li><li>开发了一个基于几何先验和文本到图像扩散模型的优化管道，以确保出色的视图不变性和直接优化虚拟人的几何形状。</li><li>我们的优化管道建立在变分分数蒸馏（VSD）之上，可缓解纹理丢失和过饱和问题。</li><li>提供的创新策略能够创造出具有无与伦比视觉质量和更符合输入文本提示的自定义虚拟人。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：魔镜：快速且高质量的头像</li><li>作者：Armand Comas-Massagué, Di Qiu, Menglei Chai, Marcel Bühler, Amit Raj, Ruiqi Gao, Qiangeng Xu, Mark Matthews, Paulo Gotardo, Octavia Camps, Sergio Orts-Escolano, Thabo Beeler</li><li>第一作者单位：Google</li><li>关键词：3D 头像生成、文本引导、神经辐射场、几何先验、变分分数蒸馏</li><li>论文链接：https://arxiv.org/abs/2404.01296   Github 代码链接：无</li><li>摘要：（1）研究背景：随着虚拟现实和增强现实等技术的兴起，对逼真且可定制的 3D 人类头像的需求不断增长。但是，生成高质量的头像仍然具有挑战性，尤其是当需要根据文本提示进行个性化定制时。（2）过去方法：传统方法通常依赖于从 3D 扫描或手动建模中获取数据，这既耗时又昂贵。基于神经网络的方法虽然可以从图像中生成头像，但它们在捕获文本提示中的细微差别和确保几何一致性方面仍然存在困难。（3）研究方法：本文提出 MagicMirror 框架，该框架利用文本提示生成快速且高质量的 3D 人类头像。MagicMirror 利用条件神经辐射场 (NeRF) 模型创建多视图初始解空间，并使用文本到图像扩散模型开发几何先验以确保视图不变性和几何优化。此外，该框架还采用基于变分分数蒸馏的优化管道，以减轻纹理损失和过饱和问题。（4）任务和性能：MagicMirror 在头像生成和个性化任务上进行了评估。实验结果表明，该方法可以生成具有无与伦比视觉质量和高度符合文本提示的定制头像。该方法的性能支持其目标，即提供一种快速且有效的方法来生成高质量的 3D 人类头像。</li></ol><p>7.方法：（1）利用条件神经辐射场（NeRF）模型创建多视图初始解空间，为优化提供约束；（2）使用文本到图像扩散模型开发几何先验，确保视图不变性和几何优化；（3）采用基于变分分数蒸馏的优化管道，减轻纹理损失和过饱和问题；（4）通过混合和加权不同的概念，实现概念组合和调制，丰富用户体验。</p><ol><li><p>结论：（1）：xxx；（2）：创新点：xxx；性能：xxx；工作量：xxx；</p></li><li><p>结论：（1）：本文提出了 MagicMirror 框架，该框架利用文本提示生成快速且高质量的 3D 人类头像。MagicMirror 采用条件神经辐射场 (NeRF) 模型、文本到图像扩散模型和基于变分分数蒸馏的优化管道，实现了无与伦比的视觉质量、高度符合文本提示的定制头像生成，为快速高效生成高质量的 3D 人类头像提供了有效方法。（2）：创新点：</p></li><li>利用条件神经辐射场 (NeRF) 模型创建多视图初始解空间，为优化提供约束。</li><li>使用文本到图像扩散模型开发几何先验，确保视图不变性和几何优化。</li><li>采用基于变分分数蒸馏的优化管道，减轻纹理损失和过饱和问题。</li><li>通过混合和加权不同的概念，实现概念组合和调制，丰富用户体验。性能：</li><li>在头像生成和个性化任务上，MagicMirror 生成具有无与伦比视觉质量和高度符合文本提示的定制头像。</li><li>MagicMirror 的性能支持其目标，即提供一种快速且有效的方法来生成高质量的 3D 人类头像。工作量：</li><li>虽然 MagicMirror 不需要大规模的 3D 人类数据，但为数百或数千个对象收集这些数据仍然是一项相对昂贵且耗时的工作。</li><li>从另一个角度来看，我们用来约束解空间的数据也限制了我们，因为某些极端的分布外修改很难实现。</li><li>我们的方法也可能受到计算资源的限制，因为我们需要多个文本到图像扩散模型，至少每个模型都用于颜色和法线，如果我们想要执行概念混合，则需要更多。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-1baa0925f922a12a8e66e59ff6fb331c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c42a213fc6f5741472d164c3e74fb649.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-864c32c352be8bc29cb98bb5b6fc3f53.jpg" align="middle"></details><h2 id="HAHA-Highly-Articulated-Gaussian-Human-Avatars-with-Textured-Mesh-Prior"><a href="#HAHA-Highly-Articulated-Gaussian-Human-Avatars-with-Textured-Mesh-Prior" class="headerlink" title="HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior"></a>HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior</h2><p><strong>Authors:David Svitov, Pietro Morerio, Lourdes Agapito, Alessio Del Bue</strong></p><p>We present HAHA - a novel approach for animatable human avatar generation from monocular input videos. The proposed method relies on learning the trade-off between the use of Gaussian splatting and a textured mesh for efficient and high fidelity rendering. We demonstrate its efficiency to animate and render full-body human avatars controlled via the SMPL-X parametric model. Our model learns to apply Gaussian splatting only in areas of the SMPL-X mesh where it is necessary, like hair and out-of-mesh clothing. This results in a minimal number of Gaussians being used to represent the full avatar, and reduced rendering artifacts. This allows us to handle the animation of small body parts such as fingers that are traditionally disregarded. We demonstrate the effectiveness of our approach on two open datasets: SnapshotPeople and X-Humans. Our method demonstrates on par reconstruction quality to the state-of-the-art on SnapshotPeople, while using less than a third of Gaussians. HAHA outperforms previous state-of-the-art on novel poses from X-Humans both quantitatively and qualitatively. </p><p><a href="http://arxiv.org/abs/2404.01053v1">PDF</a> </p><p><strong>Summary</strong><br>使用高斯散射和纹理网格相结合的方式，生成可动画逼真的全身人体头像。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种名为HAHA的新方法，用于从单目输入视频生成可动画的人形头像。</li><li>HAHA通过学习高斯散射和纹理网格的使用权衡，实现高效且高保真的渲染。</li><li>HAHA仅在SMPL-X网格必要的区域（如头发和网格外衣物）应用高斯散射。</li><li>HAHA减少了表示完整头像所需的高斯数量，并减少了渲染伪影。</li><li>HAHA可以处理手指等传统上被忽略的小身体部位的动画。</li><li>HAHA在SnapshotPeople数据集上展示了与最先进技术相当的重建质量，同时使用的高斯数量不到三分之一。</li><li>HAHA在X-Humans的新姿势上超越了之前的最先进技术，无论是在定量还是定性上。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文题目：HAHA：一种可动画的人类化身生成方法</li><li>作者：David Svitov、Michael Zollhöfer、Angjoo Kanazawa、Eric Horvitz、Mehmet Ercan Aksan</li><li>第一作者单位：微软研究院（美国）</li><li>关键词：Human avatar, Full-body, Gaussians platting, Textures</li><li>论文链接：https://arxiv.org/abs/2302.09880Github 代码链接：None</li><li><p>摘要：（1）研究背景：随着计算机视觉和图形学的发展，生成可动画的人类化身已成为一项重要的任务。现有方法通常使用高斯体素或纹理网格来表示化身，但这些方法在效率和保真度方面存在权衡。（2）过去的方法：过去的方法要么使用高斯体素实现高效渲染，但保真度较低；要么使用纹理网格实现高保真度，但渲染效率较低。（3）研究方法：本文提出了一种名为 HAHA 的方法，该方法通过学习高斯体素和纹理网格的权衡，生成可动画的人类化身。HAHA 使用高斯体素表示化身中难以用网格表示的区域，例如头发和非网格服装，而使用纹理网格表示化身中易于用网格表示的区域。（4）方法性能：在 SnapshotPeople 和 X-Humans 两个公开数据集上的实验表明，HAHA 在重建质量上与现有方法相当，同时使用的高斯体素数量减少了三分之一以上。在 X-Humans 数据集上，HAHA 在新姿势上的性能优于之前的最先进方法，无论是在定量还是定性方面。这些结果表明，HAHA 能够有效地平衡效率和保真度，生成高质量的可动画人类化身。</p></li><li><p>方法：(1): 首先，我们通过优化局部高斯变换 μji、rji、sji 和颜色 cji 来训练 3D 高斯体素 (GS) 表示。(2): 然后，我们使用可微分光栅化器渲染具有可训练纹理的 SMPL-X 网格。(3): 最后，我们合并可微分渲染纹理网格和可微分 3D GS 过程，训练高斯体素的不透明度 oji 和颜色 cji。</p></li><li><p>结论：(1): 本文提出了一种名为HAHA的方法，该方法通过学习高斯体素和纹理网格的权衡，生成可动画的人类化身。HAHA在重建质量上与现有方法相当，同时使用的高斯体素数量减少了三分之一以上。在X-Humans数据集上，HAHA在新姿势上的性能优于之前的最先进方法，无论是在定量还是定性方面。这些结果表明，HAHA能够有效地平衡效率和保真度，生成高质量的可动画人类化身。(2): 创新点：</p></li><li>提出了一种新的方法来生成可动画的人类化身，该方法通过学习高斯体素和纹理网格的权衡来平衡效率和保真度。</li><li>该方法在重建质量上与现有方法相当，同时使用的高斯体素数量减少了三分之一以上。</li><li>该方法在新姿势上的性能优于之前的最先进方法，无论是在定量还是定性方面。性能：</li><li>在SnapshotPeople和X-Humans两个公开数据集上的实验表明，HAHA在重建质量上与现有方法相当，同时使用的高斯体素数量减少了三分之一以上。</li><li>在X-Humans数据集上，HAHA在新姿势上的性能优于之前的最先进方法，无论是在定量还是定性方面。工作量：</li><li>该方法的训练和推理过程相对复杂，需要大量的计算资源。</li><li>该方法需要大量的数据来训练，这可能是一个挑战。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-b477b212869e9f8635da6f1012719fb9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-aa6e5f76fe253938286549f8ee5205fd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4f97970c093585e18e2db42fb96a6b75.jpg" align="middle"></details><h2 id="TexVocab-Texture-Vocabulary-conditioned-Human-Avatars-1"><a href="#TexVocab-Texture-Vocabulary-conditioned-Human-Avatars-1" class="headerlink" title="TexVocab: Texture Vocabulary-conditioned Human Avatars"></a>TexVocab: Texture Vocabulary-conditioned Human Avatars</h2><p><strong>Authors:Yuxiao Liu, Zhe Li, Yebin Liu, Haoqian Wang</strong></p><p>To adequately utilize the available image evidence in multi-view video-based avatar modeling, we propose TexVocab, a novel avatar representation that constructs a texture vocabulary and associates body poses with texture maps for animation. Given multi-view RGB videos, our method initially back-projects all the available images in the training videos to the posed SMPL surface, producing texture maps in the SMPL UV domain. Then we construct pairs of human poses and texture maps to establish a texture vocabulary for encoding dynamic human appearances under various poses. Unlike the commonly used joint-wise manner, we further design a body-part-wise encoding strategy to learn the structural effects of the kinematic chain. Given a driving pose, we query the pose feature hierarchically by decomposing the pose vector into several body parts and interpolating the texture features for synthesizing fine-grained human dynamics. Overall, our method is able to create animatable human avatars with detailed and dynamic appearances from RGB videos, and the experiments show that our method outperforms state-of-the-art approaches. The project page can be found at <a href="https://texvocab.github.io/">https://texvocab.github.io/</a>. </p><p><a href="http://arxiv.org/abs/2404.00524v1">PDF</a> </p><p><strong>Summary</strong><br>基于多视角视频创建逼真的化身模型，TexVocab 提出了一种新的基于纹理词汇的化身表征，将人体姿势与用于动画的纹理贴图联系起来。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种新的化身表征 TexVocab，用于从多视角 RGB 视频创建逼真的化身模型。</li><li>TexVocab 构建了一个纹理词汇，将身体姿势与纹理贴图联系起来，用于动画。</li><li>将所有可用图像反投影到姿势化 SMPL 曲面上，生成 SMPL UV 域中的纹理贴图。</li><li>构建人体姿势和纹理贴图对，建立纹理词汇，以对各种姿势下的动态人类外观进行编码。</li><li>设计了一个基于身体部位的编码策略，以学习运动链的结构效应。</li><li>给定一个驱动姿势，通过将姿势向量分解成几个身体部位并插值纹理特征来分级查询姿势特征，合成细粒度的人体动态。</li><li>在 RGB 视频中创建具有详细动态外观的可动画人体化身，实验表明该方法优于最先进的方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>题目：TexVocab：纹理词典条件下的人体虚拟化身</li><p></p><p></p><li>作者：刘煜霄、李哲、刘业彬、王浩倩</li><p></p><p></p><li>第一作者单位：深圳国际研究生院，清华大学</li><p></p><p></p><li>关键词：人体虚拟化身、纹理词典、多视角视频、条件神经辐射场</li><p></p><p></p><li>论文链接：None，Github 代码链接：https://texvocab.github.io/</li><p></p><p></p><li>摘要：（1）研究背景：    人体虚拟化身建模在 AR/VR 应用中具有巨大潜力，但如何有效学习驱动信号和动态外观之间的映射仍然具有挑战性。</li><br>&lt;/ol&gt;<p></p><p>（2）过去方法及问题：    以往方法通常直接将姿态输入映射到人体外观，但姿态输入不包含任何动态人体外观信息，导致神经辐射场（NeRF）难以仅从姿态输入中回归高保真动态细节。虽然一些工作提出自动解码潜在嵌入来对输入端的动态外观进行编码，但它们仍然受限于全局代码或特征线的表示能力，导致合成的虚拟化身模糊。</p><p>（3）提出的研究方法：    本文提出 TexVocab，一种纹理词典，充分利用显式图像证据来指导隐式条件 NeRF 从表达纹理条件中学习动态。为了将多视角图像与动态人体关联起来，将所有可用图像反投影到相应的训练姿态上，在 SMPL UV 域中生成纹理贴图。然后构建人体姿态和纹理贴图对，建立纹理词典，用于编码在不同姿态下的动态人体外观。与常用的关节方式不同，本文进一步设计了身体部位编码策略，以学习运动链的结构影响。给定一个驱动姿态，通过将姿态向量分解成多个身体部位并对纹理特征进行插值，分层查询姿态特征，以合成细粒度的动态人体。</p><p>（4）方法在任务和性能上的表现：    本文方法能够从 RGB 视频创建具有详细动态外观的动画虚拟化身，实验表明该方法优于现有方法。</p><ol><li><strong>方法</strong>：(1): 提出 <strong>纹理词典（TexVocab）</strong>，利用显式图像证据指导隐式条件神经辐射场（NeRF）从纹理条件中学习动态。(2): 将多视角图像反投影到相应的训练姿态上，在 <strong>SMPLUV</strong> 域中生成纹理贴图，构建 <strong>姿态-纹理贴图对</strong>，形成纹理词典。(3): 设计 <strong>身体部位编码策略</strong>，学习运动链的结构影响，分层查询姿态特征，合成细粒度的动态人体。</li></ol><p>8.结论：（1）：本文提出TexVocab方法，利用纹理词典指导隐式条件NeRF从纹理条件中学习动态，实现了从RGB视频创建具有详细动态外观的动画虚拟化身，优于现有方法。（2）：创新点：* 提出纹理词典，利用显式图像证据指导隐式条件NeRF学习动态。* 设计身体部位编码策略，学习运动链的结构影响，分层查询姿态特征，合成细粒度的动态人体。性能：* 能够从RGB视频创建具有详细动态外观的动画虚拟化身。* 实验表明该方法优于现有方法。工作量：* 需要构建纹理词典，反投影多视角图像并生成纹理贴图。* 需要设计身体部位编码策略，分层查询姿态特征。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-d5ef7cbf30d33e46f182e7d8ab7d3f6a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2b1fc6c5e328a61ca82826325adda143.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-274cb92781d5841c23c9d32fbb82c09e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0fb1457171dba2509eedb86dc74d5975.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f2d2ccbdda2a0bc805f942fdcc2c0204.jpg" align="middle"></details></ol>]]></content>
    
    
    <summary type="html">元宇宙/虚拟人 方向最新论文已更新，请持续关注 Update in 2024-04-06  GeneAvatar Generic Expression-Aware Volumetric Head Avatar Editing from   a Single Image</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="元宇宙/虚拟人" scheme="https://kedreamix.github.io/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/"/>
    
  </entry>
  
  <entry>
    <title>元宇宙/虚拟人</title>
    <link href="https://kedreamix.github.io/2024/04/01/Paper/2024-04-01/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/"/>
    <id>https://kedreamix.github.io/2024/04/01/Paper/2024-04-01/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/</id>
    <published>2024-04-01T05:22:12.000Z</published>
    <updated>2024-04-01T05:22:12.538Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-01-更新"><a href="#2024-04-01-更新" class="headerlink" title="2024-04-01 更新"></a>2024-04-01 更新</h1><h2 id="Make-Your-Anchor-A-Diffusion-based-2D-Avatar-Generation-Framework"><a href="#Make-Your-Anchor-A-Diffusion-based-2D-Avatar-Generation-Framework" class="headerlink" title="Make-Your-Anchor: A Diffusion-based 2D Avatar Generation Framework"></a>Make-Your-Anchor: A Diffusion-based 2D Avatar Generation Framework</h2><p><strong>Authors:Ziyao Huang, Fan Tang, Yong Zhang, Xiaodong Cun, Juan Cao, Jintao Li, Tong-Yee Lee</strong></p><p>Despite the remarkable process of talking-head-based avatar-creating solutions, directly generating anchor-style videos with full-body motions remains challenging. In this study, we propose Make-Your-Anchor, a novel system necessitating only a one-minute video clip of an individual for training, subsequently enabling the automatic generation of anchor-style videos with precise torso and hand movements. Specifically, we finetune a proposed structure-guided diffusion model on input video to render 3D mesh conditions into human appearances. We adopt a two-stage training strategy for the diffusion model, effectively binding movements with specific appearances. To produce arbitrary long temporal video, we extend the 2D U-Net in the frame-wise diffusion model to a 3D style without additional training cost, and a simple yet effective batch-overlapped temporal denoising module is proposed to bypass the constraints on video length during inference. Finally, a novel identity-specific face enhancement module is introduced to improve the visual quality of facial regions in the output videos. Comparative experiments demonstrate the effectiveness and superiority of the system in terms of visual quality, temporal coherence, and identity preservation, outperforming SOTA diffusion/non-diffusion methods. Project page: \url{<a href="https://github.com/ICTMCG/Make-Your-Anchor}">https://github.com/ICTMCG/Make-Your-Anchor}</a>. </p><p><a href="http://arxiv.org/abs/2403.16510v1">PDF</a> accepted at CVPR2024</p><p><strong>Summary</strong><br>通过仅需一分钟的个人视频训练，提出了一种生成全身动作锚播风格视频的新系统 Make-Your-Anchor。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种基于结构引导扩散模型，将 3D 网格条件渲染为人物外观。</li><li>采用两阶段训练策略，有效绑定动作与特定外观。</li><li>扩展帧内扩散模型中的 2D U-Net 到 3D 风格，无需额外训练成本。</li><li>提出一个简单有效的批量重叠时间去噪模块，绕过推理时的视频长度限制。</li><li>引入一个新颖的身份特定面部增强模块，以提高输出视频中面部区域的视觉质量。</li><li>与 SOTA 扩散/非扩散方法相比，在视觉质量、时间连贯性和身份保留方面证明了该系统的有效性和优越性。</li><li>项目主页：\url{<a href="https://github.com/ICTMCG/Make-Your-Anchor}。">https://github.com/ICTMCG/Make-Your-Anchor}。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：Make-Your-Anchor：基于扩散的二维虚拟形象生成框架</li><li>作者：黄子尧，唐凡，张勇，村晓东，曹娟，李金涛，李同义</li><li>第一作者单位：中国科学院计算技术研究所</li><li>关键词：虚拟形象生成，扩散模型，运动捕捉，语音驱动动画</li><li>论文链接：https://arxiv.org/abs/2403.16510，Github 代码链接：None</li><li>摘要：（1）：研究背景：目前基于说话人头部创建虚拟形象的解决方案已取得显著进展，但直接生成具有全身动作的主播风格视频仍然具有挑战性。</li></ol><p>（2）：过去的方法及问题：以往的方法主要基于 GAN 进行局部编辑或利用运动迁移技术，但这些方法要么自由度受限，要么动作与特定外观的绑定不够紧密。</p><p>（3）：提出的研究方法：本文提出 Make-Your-Anchor 系统，仅需一个一分钟的个人视频即可训练，实现自动生成具有精确躯干和手部动作的主播风格视频。该系统采用结构引导扩散模型，将三维网格条件渲染成人物外观，并采用两阶段训练策略，有效地将动作与特定外观绑定。为了生成任意长度的时间视频，将帧级扩散模型中的二维 U-Net 扩展为三维形式，并提出了一种简单有效的批次重叠时间去噪模块，以绕过推理期间视频长度的限制。此外，还引入了新的特定于身份的面部增强模块，以提高输出视频中面部区域的视觉质量。</p><p>（4）：方法在任务和性能上的表现：Make-Your-Anchor 系统在视觉质量、时间连贯性和身份保留方面均优于 SOTA 扩散和非扩散方法，证明了其有效性和优越性。</p><ol><li><p>方法：(1) 结构引导扩散模型（SGDM）：将 3D 网格条件嵌入生成过程，学习姿势到目标视频帧的对应映射。(2) 两阶段训练策略：预训练增强模型生成动作能力，微调绑定动作与特定外观。(3) 批次重叠时间去噪：将 2D U-Net 扩展为 3D 形式，提出批次重叠时间去噪模块生成任意长度的时间视频。(4) 身份特定面部增强模块：通过裁剪和混合操作，修改生成的身体中的面部区域，提高面部区域的视觉质量。</p></li><li><p>结论：（1）本工作的意义：本文提出 “Make-Your-Anchor”，一个基于扩散的二维虚拟形象生成框架，用于生成逼真且高质量的主播风格人物视频。该框架创新性地提出了帧级的运动到外观扩散，通过结构引导扩散模型和两阶段训练策略，实现了特定外观与动作的绑定。为了生成时间一致的人像视频，我们提出了一个无训练策略，将图像扩散模型扩展为视频扩散模型，并设计了一个批次重叠时间去噪算法，以克服生成视频长度的限制。针对整体人物生成中面部细节难以重建的观察，我们引入了身份特定的面部增强。通过我们整个系统方法的融合，我们的框架成功地生成了高质量、结构保持和时间一致的主播风格人物视频，这可能为二维数字虚拟形象的广泛应用技术提供参考价值。（2）创新点：</p></li><li>提出结构引导扩散模型，将三维网格条件嵌入生成过程，学习姿势到目标视频帧的对应映射。</li><li>采用两阶段训练策略，预训练增强模型生成动作能力，微调绑定动作与特定外观。</li><li>提出批次重叠时间去噪，将二维 U-Net 扩展为三维形式，生成任意长度的时间视频。</li><li>引入身份特定的面部增强模块，通过裁剪和混合操作，修改生成的身体中的面部区域，提高面部区域的视觉质量。性能：</li><li>在视觉质量、时间连贯性和身份保留方面均优于 SOTA 扩散和非扩散方法。工作量：</li><li>训练数据量较大，需要大量的人物视频数据。</li><li>训练时间较长，需要高性能计算资源。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-31e07e2070d0183b4685c1e857aaf0a2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d20d65a2e0024c9d9bc380b7f7ba43b0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-771450d6d33a69293597f486c329a82a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4f2a829065c463be027e4b423c4e43c8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6ad4738b2a0c37e9688fb722556c4213.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b6700b1c5fd0f20662f87b175ebf869f.jpg" align="middle"></details><h2 id="UV-Gaussians-Joint-Learning-of-Mesh-Deformation-and-Gaussian-Textures-for-Human-Avatar-Modeling"><a href="#UV-Gaussians-Joint-Learning-of-Mesh-Deformation-and-Gaussian-Textures-for-Human-Avatar-Modeling" class="headerlink" title="UV Gaussians: Joint Learning of Mesh Deformation and Gaussian Textures   for Human Avatar Modeling"></a>UV Gaussians: Joint Learning of Mesh Deformation and Gaussian Textures   for Human Avatar Modeling</h2><p><strong>Authors:Yujiao Jiang, Qingmin Liao, Xiaoyu Li, Li Ma, Qi Zhang, Chaopeng Zhang, Zongqing Lu, Ying Shan</strong></p><p>Reconstructing photo-realistic drivable human avatars from multi-view image sequences has been a popular and challenging topic in the field of computer vision and graphics. While existing NeRF-based methods can achieve high-quality novel view rendering of human models, both training and inference processes are time-consuming. Recent approaches have utilized 3D Gaussians to represent the human body, enabling faster training and rendering. However, they undermine the importance of the mesh guidance and directly predict Gaussians in 3D space with coarse mesh guidance. This hinders the learning procedure of the Gaussians and tends to produce blurry textures. Therefore, we propose UV Gaussians, which models the 3D human body by jointly learning mesh deformations and 2D UV-space Gaussian textures. We utilize the embedding of UV map to learn Gaussian textures in 2D space, leveraging the capabilities of powerful 2D networks to extract features. Additionally, through an independent Mesh network, we optimize pose-dependent geometric deformations, thereby guiding Gaussian rendering and significantly enhancing rendering quality. We collect and process a new dataset of human motion, which includes multi-view images, scanned models, parametric model registration, and corresponding texture maps. Experimental results demonstrate that our method achieves state-of-the-art synthesis of novel view and novel pose. The code and data will be made available on the homepage <a href="https://alex-jyj.github.io/UV-Gaussians/">https://alex-jyj.github.io/UV-Gaussians/</a> once the paper is accepted. </p><p><a href="http://arxiv.org/abs/2403.11589v1">PDF</a> </p><p><strong>摘要</strong><br>通过联合学习网格变形和二维 UV 空间高斯纹理，结合 UV 高斯模型重建逼真的可驾驶人体虚拟人。</p><p><strong>关键要点</strong></p><ul><li>利用三维高斯体表示人体，实现快速训练和渲染。</li><li>提出 UV 高斯模型，联合学习网格变形和二维 UV 空间高斯纹理。</li><li>通过 UV 映射嵌入，在二维空间学习高斯纹理，增强纹理清晰度。</li><li>独立网格网络优化姿态相关的几何变形，引导高斯渲染。</li><li>收集并处理包含多视角图像、扫描模型、参数模型配准和对应纹理贴图的新人体动作数据集。</li><li>在全新视图和全新姿态合成方面达到最先进的水平。</li><li>公开代码和数据，促进进一步研究。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：UV 高斯：网格新视角的联合学习</li><li>作者：Y. Jiang、Z. Zhou、J. Huang、T. Zhang、X. Han、Y. Chen、Y. Liu、L. Liu</li><li>单位：无</li><li>关键词：Human Modeling·Neural Rendering·Gaussian Splatting</li><li>论文链接：https://arxiv.org/abs/2212.05845   Github 链接：无</li><li><p>摘要：   （1）研究背景：从多视角图像序列重建逼真的可驾驶人形化身一直是计算机视觉和图形领域的一个热门且具有挑战性的课题。虽然现有的基于 NeRF 的方法可以实现高质量的人体模型新视角渲染，但训练和推理过程都很耗时。   （2）过去的方法及问题：最近的研究利用 3D 高斯表示人体，从而实现更快的训练和渲染。然而，它们低估了网格指导的重要性，并直接在 3D 空间中预测高斯，而网格指导较粗糙。这阻碍了高斯学习过程，并倾向于产生模糊的纹理。   （3）提出的研究方法：因此，我们提出了 UV 高斯，它通过联合学习网格变形和 2D UV 空间高斯纹理对 3D 人体进行建模。我们利用 UV 贴图的嵌入在 2D 空间中学习高斯纹理，利用强大的 2D 网络提取特征的能力。此外，通过一个独立的网格网络，我们优化了与姿势相关的几何变形，从而指导高斯渲染并显著提高渲染质量。我们收集并处理了一个新的数据集，其中包括多视角图像、扫描模型、参数模型配准和相应的纹理贴图。   （4）方法在任务和性能上的表现：实验结果表明，我们的方法实现了新视角和新姿势合成的新技术。该代码和数据将在论文被接受后在主页 https://alex-jyj.github.io/UVGaussians/ 上提供。</p></li><li><p>方法：（1）数据处理：使用 OpenPose 估计多视角图像的 2D 关键点，通过三角测量估计 3D 关键点，然后使用 EasyMocap 拟合 SMPL-X 模型。使用 MVS 方法重建扫描网格，优化 SMPL-X 网格的顶点位移以将其与扫描模型的网格对齐，从而得到 SMPLX-D 模型。（2）基于姿势的网格变形：选择一个接近 T 姿势的帧作为参考，使用线性混合蒙皮 (LBS) 将其变形为标准 T 姿势。将这个标准 T 姿势作为所有姿势的模板网格。通过基于姿势参数的 LBS 变形粗糙网格，得到姿势网格。引入 MeshU-Net 来学习基于姿势的网格变形。将网格顶点坐标光栅化为 UV 空间，生成位置图，作为网格网络 M 的输入。网格网络 M 预测基于输入位置图的顶点偏移图。将每个顶点的相应偏移从偏移图中插值，得到网格顶点偏移。将此偏移添加到 T 姿势网格中，然后使用 LBS 变换到姿势空间。最终得到能够捕捉基于姿势的几何变化的细化网格。（3）基于姿势的高斯纹理：将 3D 高斯参数化为 UV 空间的高斯纹理，通过 UV 映射将每个像素投影到 3D 高斯。将所有姿势的平均纹理图作为输入，为 3D 高斯提供初始颜色信息。还向网络提供位置图以提供像素级的姿势信息。网络还受视图方向向量的引导，以建模视图相关的变化。使用 StyleUNet 架构，网络输出多个高斯纹理，包含 3D 高斯所需的所有参数。</p></li><li><p>结论：（1）：本文提出了一种称为 UV 高斯的重建方法，该方法将 3D 高斯与 UV 空间表示相结合。该方法能够从多视角图像重建逼真的、姿势驱动的化身模型。我们的方法以模型顶点的位移图作为输入，通过 MeshU-Net 学习基于姿势的几何变形，并通过 GaussianU-Net 学习嵌入在 UV 空间中的高斯点属性。随后，在精细网格的引导下，对高斯点进行渲染，以获得任意视点的渲染图像。通过结合精细的几何指导并利用 UV 空间中强大的 2D 网络的特征学习能力，我们的方法在实验中实现了新视角和新姿势合成方面的最先进结果。局限性。尽管取得了成就，但我们的方法受制于对扫描网格的依赖性。虽然可以使用 MeshU-Net 优化较小的拟合误差，但较大的误差可能会影响方法的性能。此外，我们收集的数据集不包括长裙等极度宽松的服装。在未来的研究中，我们计划在包含多视角图像和扫描模型的更多可用数据集上评估我们的方法，特别是探索具有困难姿势的具有挑战性的服装类型。（2）：创新点：结合 3D 高斯和 UV 空间表示，提出了一种新的重建方法；通过 MeshU-Net 学习基于姿势的几何变形，通过 GaussianU-Net 学习高斯纹理；性能：在新视角和新姿势合成实验中实现了最先进的结果；工作量：数据处理和网络训练需要大量计算资源。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-a863ff88a8f3aab922fde1833cf3125b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6c24e3d34d46677eafb334d061117f93.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e62a000f486adba73f5ad94566312cdc.jpg" align="middle"></details><h2 id="NECA-Neural-Customizable-Human-Avatar"><a href="#NECA-Neural-Customizable-Human-Avatar" class="headerlink" title="NECA: Neural Customizable Human Avatar"></a>NECA: Neural Customizable Human Avatar</h2><p><strong>Authors:Junjin Xiao, Qing Zhang, Zhan Xu, Wei-Shi Zheng</strong></p><p>Human avatar has become a novel type of 3D asset with various applications. Ideally, a human avatar should be fully customizable to accommodate different settings and environments. In this work, we introduce NECA, an approach capable of learning versatile human representation from monocular or sparse-view videos, enabling granular customization across aspects such as pose, shadow, shape, lighting and texture. The core of our approach is to represent humans in complementary dual spaces and predict disentangled neural fields of geometry, albedo, shadow, as well as an external lighting, from which we are able to derive realistic rendering with high-frequency details via volumetric rendering. Extensive experiments demonstrate the advantage of our method over the state-of-the-art methods in photorealistic rendering, as well as various editing tasks such as novel pose synthesis and relighting. The code is available at <a href="https://github.com/iSEE-Laboratory/NECA">https://github.com/iSEE-Laboratory/NECA</a>. </p><p><a href="http://arxiv.org/abs/2403.10335v1">PDF</a> Accepted to CVPR 2024</p><p><strong>摘要</strong><br>利用单视角或稀疏视点视频学习多功能人体表示，实现姿势、阴影、形状、光照和纹理等细粒度自定义。</p><p><strong>要点</strong></p><ul><li>人体化身为一种新型 3D 资产，具备广泛应用。</li><li>理想的人体化身应完全可定制，以适应不同的设置和环境。</li><li>引入 NECA 方法，可从单视角或稀疏视点视频中学习多功能人体表示，实现姿势、阴影、形状、光照和纹理等方面的粒度定制。</li><li>方法核心是将人表示在互补的双空间中，并预测几何、反照率、阴影以及外部光照的纠缠神经场，从而通过体积渲染获得具有高频细节的逼真渲染效果。</li><li>广泛实验表明，该方法在逼真渲染以及新颖姿势合成和重新照明等各种编辑任务中优于最先进的方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：NECA：神经可定制人形化身</li><li>作者：JunJin Xiao、Qing Zhang、Zhan Xu、Wei-Shi Zheng</li><li>隶属：中山大学计算机科学与工程学院</li><li>关键词：神经可定制人形化身，神经场，图像合成，人像编辑</li><li>论文链接：https://arxiv.org/abs/2403.10335   Github 链接：https://github.com/iSEE-Laboratory/NECA</li><li><p>摘要：（1）研究背景：   随着元宇宙、远程临场和 3D 游戏等新兴应用的兴起，对人形化身的需求日益增长。理想的人形化身应该具有高度的可定制性，以适应不同的场景和环境。（2）过去方法及其问题：   现有的神经人形化身建模方法主要针对动画或重新打光目的，无法为化身提供全面的定制能力，从而限制了其在实际中的应用。（3）研究方法：   NECA 提出了一种新颖的框架，可以从单目或稀疏多视图视频中学习完全可定制的神经人形化身，在任何新的姿势、视角和光照下进行逼真的渲染，并具有编辑形状、纹理和阴影的能力。（4）方法性能：   NECA 在逼真渲染、新颖姿势合成和重新打光等各种编辑任务中优于最先进的方法。这些性能证明了 NECA 在支持其目标方面的有效性。</p></li><li><p>方法：（1）：采用双空间动态人体表示，分别在正则空间和表面空间中学习人体表示，以捕捉高频姿态感知特征和几何感知主体特征；（2）：将神经场解耦为不同的属性，包括 SDF、阴影、反照率和环境光照，并通过不同的 MLP 解码提取的特征；（3）：以自监督的方式训练整个网络，仅使用光度损失和法线正则化。</p></li><li><p>结论：（1）：本工作提出了一种新颖的框架 NECA，该框架可以从稀疏视图甚至单目视频中学习完全可定制的神经人形化身。与以往提供有限编辑能力的方法不同，我们提供的神经人形化身允许对姿势、视点、光照、形状、纹理和阴影进行高保真编辑。广泛的实验验证了我们方法的多功能性和实用性，以及我们在新颖姿势合成和重新打光方面对现有技术水平的改进。我们希望我们的工作可以为定制化神经人形化身的创建及其相关应用提供启发。（2）：创新点：</p></li><li>提出了一种双空间动态人体表示，该表示分别在正则空间和表面空间中学习人体表示，以捕捉高频姿态感知特征和几何感知主体特征。</li><li>将神经场解耦为不同的属性，包括 SDF、阴影、反照率和环境光照，并通过不同的 MLP 解码提取的特征。</li><li>以自监督的方式训练整个网络，仅使用光度损失和法线正则化。性能：</li><li>在逼真渲染、新颖姿势合成和重新打光等各种编辑任务中优于最先进的方法。</li><li>定量和定性结果证明了 NECA 在支持其目标方面的有效性。工作量：</li><li>该方法的实现相对复杂，需要大量的训练数据和计算资源。</li><li>训练过程可能需要大量的时间和精力。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-e0b24c5f9b1b9a6dda62d5c6ea5c2f90.jpg" align="middle"><img src="https://picx.zhimg.com/v2-696c4b793b016f700357881149a5655f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-eaa948c1712aa3778ea7e6d4eea0befe.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3794c1f73de9c445952f1edc9bec5c2d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-2e5ca49ec699317449a1f1bb4b188bfa.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2f8fb36ded30237c1124e3462ac19e1d.jpg" align="middle"></details><h2 id="VLOGGER-Multimodal-Diffusion-for-Embodied-Avatar-Synthesis"><a href="#VLOGGER-Multimodal-Diffusion-for-Embodied-Avatar-Synthesis" class="headerlink" title="VLOGGER: Multimodal Diffusion for Embodied Avatar Synthesis"></a>VLOGGER: Multimodal Diffusion for Embodied Avatar Synthesis</h2><p><strong>Authors:Enric Corona, Andrei Zanfir, Eduard Gabriel Bazavan, Nikos Kolotouros, Thiemo Alldieck, Cristian Sminchisescu</strong></p><p>We propose VLOGGER, a method for audio-driven human video generation from a single input image of a person, which builds on the success of recent generative diffusion models. Our method consists of 1) a stochastic human-to-3d-motion diffusion model, and 2) a novel diffusion-based architecture that augments text-to-image models with both spatial and temporal controls. This supports the generation of high quality video of variable length, easily controllable through high-level representations of human faces and bodies. In contrast to previous work, our method does not require training for each person, does not rely on face detection and cropping, generates the complete image (not just the face or the lips), and considers a broad spectrum of scenarios (e.g. visible torso or diverse subject identities) that are critical to correctly synthesize humans who communicate. We also curate MENTOR, a new and diverse dataset with 3d pose and expression annotations, one order of magnitude larger than previous ones (800,000 identities) and with dynamic gestures, on which we train and ablate our main technical contributions.   VLOGGER outperforms state-of-the-art methods in three public benchmarks, considering image quality, identity preservation and temporal consistency while also generating upper-body gestures. We analyze the performance of VLOGGER with respect to multiple diversity metrics, showing that our architectural choices and the use of MENTOR benefit training a fair and unbiased model at scale. Finally we show applications in video editing and personalization. </p><p><a href="http://arxiv.org/abs/2403.08764v1">PDF</a> Project web: <a href="https://enriccorona.github.io/vlogger/">https://enriccorona.github.io/vlogger/</a></p><p><strong>Summary</strong><br>元宇宙虚拟人生成模型 VLOGGER，通过条件扩散模型实现图像驱动的音频视频生成，具有图像质量、身份保持、时间一致性、上半身手势生成、公平性和可偏好设定等优势。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种音频驱动的生成扩散模型 VLOGGER，可从单个人图像生成人类视频。</li><li>VLOGGER 由一个人到 3D 运动扩散模型和一个新颖的扩散架构组成，该架构增强了文本到图像模型的空间和时间控制。</li><li>VLOGGER 在图像质量、身份保持和时间一致性方面优于现有方法，同时生成上半身手势。</li><li>引入了 MENTOR 数据集，该数据集比以前的数据集大一个数量级，具有 3D 姿势和表情注释。</li><li>VLOGGER 在多样性指标方面表现出色，这得益于其架构选择和对 MENTOR 的使用。</li><li>VLOGGER 在视频编辑和个性化方面有应用。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：VLOGGER：用于具身化身的多模态扩散</li><li>作者：Enric Corona、Andrei Zanfir、Eduard Gabriel Bazavan、Nikos Kolotouros、Thiemo Alldieck、Cristian Sminchisescu</li><li>第一作者单位：Google Research</li><li>关键词：音频驱动视频生成、扩散模型、具身化身合成</li><li>论文链接：https://enriccorona.github.io/vlogger/，Github 代码链接：无</li><li>摘要：（1）研究背景：近年来，生成扩散模型在图像和视频生成领域取得了显著进展。然而，现有方法主要集中在生成静态图像或无具身化身的视频序列。</li></ol><p>（2）过去方法及其问题：过去的方法要么难以生成高质量的视频，要么无法控制生成的视频内容。此外，这些方法通常需要大量的数据和计算资源。</p><p>（3）提出的研究方法：本文提出 VLOGGER，一种基于扩散模型的音频驱动具身化身合成方法。VLOGGER 由两个模块组成：1）一个将人脸图像转换为 3D 运动的扩散模型；2）一个基于扩散的架构，用于增强文本到图像模型的空间和时间控制。</p><p>（4）方法性能及对目标的支持：在人脸图像到视频生成任务上，VLOGGER 生成了高质量、时间一致的视频序列。这些视频序列包含逼真的头部运动、注视、眨眼、嘴唇运动以及上半身和手势，从而将音频驱动的合成提升到了一个新的水平。VLOGGER 的性能支持了其生成可控、高保真视频的目标。</p><p>7.Methods：(1):音频驱动运动生成架构；(2):生成逼真谈话和移动人类的架构；</p><ol><li>结论：（1）本工作的重要意义：VLOGGER 是一种基于音频驱动的具身化身合成方法，它将人脸图像转换为 3D 运动，并使用基于扩散的架构增强文本到图像模型的空间和时间控制。它生成了高质量、时间一致的视频序列，包含逼真的头部运动、注视、眨眼、嘴唇运动以及上半身和手势，从而将音频驱动的合成提升到了一个新的水平。（2）文章的优缺点总结：创新点：</li><li>提出了一种用于具身化身合成的音频驱动扩散模型，该模型可以生成高质量、时间一致的视频序列。</li><li>引入了一个多样化且大规模的数据集，用于验证 VLOGGER 的性能，该数据集比以前的数据集大一个数量级。</li><li>证明了 VLOGGER 在生成逼真谈话和移动人类方面优于之前的最先进技术，并且我们的方法在不同的多样性轴上更加稳健。性能：</li><li>在人脸图像到视频生成任务上，VLOGGER 生成了高质量、时间一致的视频序列，包含逼真的头部运动、注视、眨眼、嘴唇运动以及上半身和手势。</li><li>在多个数据集上的验证表明，VLOGGER 在生成逼真谈话和移动人类方面优于之前的最先进技术。</li><li>我们的方法在不同的多样性轴上更加稳健，例如种族、性别和年龄。工作量：</li><li>VLOGGER 的训练需要大量的数据和计算资源。</li><li>VLOGGER 的推理时间相对较长，这限制了其在实时应用程序中的使用。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-065509c8ee9e706b83acf89f90a3ce67.jpg" align="middle"><img src="https://picx.zhimg.com/v2-33c208f9649a4885bf660ec7dd810aba.jpg" align="middle"><img src="https://pica.zhimg.com/v2-37bdd0182fde8e2eb5a53cf9fdad4d37.jpg" align="middle"></details><h2 id="GVA-Reconstructing-Vivid-3D-Gaussian-Avatars-from-Monocular-Videos"><a href="#GVA-Reconstructing-Vivid-3D-Gaussian-Avatars-from-Monocular-Videos" class="headerlink" title="GVA: Reconstructing Vivid 3D Gaussian Avatars from Monocular Videos"></a>GVA: Reconstructing Vivid 3D Gaussian Avatars from Monocular Videos</h2><p><strong>Authors:Xinqi Liu, Chenming Wu, Jialun Liu, Xing Liu, Jinbo Wu, Chen Zhao, Haocheng Feng, Errui Ding, Jingdong Wang</strong></p><p>In this paper, we present a novel method that facilitates the creation of vivid 3D Gaussian avatars from monocular video inputs (GVA). Our innovation lies in addressing the intricate challenges of delivering high-fidelity human body reconstructions and aligning 3D Gaussians with human skin surfaces accurately. The key contributions of this paper are twofold. Firstly, we introduce a pose refinement technique to improve hand and foot pose accuracy by aligning normal maps and silhouettes. Precise pose is crucial for correct shape and appearance reconstruction. Secondly, we address the problems of unbalanced aggregation and initialization bias that previously diminished the quality of 3D Gaussian avatars, through a novel surface-guided re-initialization method that ensures accurate alignment of 3D Gaussian points with avatar surfaces. Experimental results demonstrate that our proposed method achieves high-fidelity and vivid 3D Gaussian avatar reconstruction. Extensive experimental analyses validate the performance qualitatively and quantitatively, demonstrating that it achieves state-of-the-art performance in photo-realistic novel view synthesis while offering fine-grained control over the human body and hand pose. Project page: <a href="https://3d-aigc.github.io/GVA/">https://3d-aigc.github.io/GVA/</a>. </p><p><a href="http://arxiv.org/abs/2402.16607v2">PDF</a> </p><p><strong>Summary</strong><br>通过姿势优化和表面引导的重新初始化，本文提出了从单目视频输入中创建逼真 3D 高斯化身 (GVA) 的新方法，实现了高保真人体重建和 3D 高斯与人体皮肤表面的准确对齐。</p><p><strong>Key Takeaways</strong></p><ul><li>提出姿势优化技术，通过对齐法线贴图和轮廓来提高手部和脚部姿势精度。</li><li>解决 3D 高斯化身质量降低的不平衡聚合和初始化偏差问题。</li><li>引入表面引导的重新初始化方法，确保 3D 高斯点与化身表面的准确对齐。</li><li>实验结果表明，该方法实现了高保真和逼真的 3D 高斯化身重建。</li><li>广泛的实验分析验证了该方法的性能，在逼真的新视角合成中实现了最先进的性能。</li><li>提供了对人体和手部姿势的细粒度控制。</li><li>提供项目页面：<a href="https://3d-aigc.github.io/GVA/。">https://3d-aigc.github.io/GVA/。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：GVA：从单目视频中重建生动的 3D 高斯虚拟形象</li><li>作者：刘欣琦、吴晨明、刘嘉伦、刘星、吴锦波、赵晨、冯浩成、丁瑞、王京东</li><li>单位：百度视觉技术部</li><li>关键词：3D 高斯虚拟形象、单目视频、姿势优化、表面引导重新初始化</li><li>论文链接：https://arxiv.org/abs/2402.16607   Github 代码链接：无</li><li><p>摘要：   （1）研究背景：单目视频重建逼真的可驱动虚拟形象具有广阔的应用前景，但现有方法面临着成本高、重建效果不佳等挑战。   （2）过去方法：神经辐射场和 3D 高斯渲染技术已被用于创建虚拟形象，但神经辐射场训练时间长、姿势泛化能力差，3D 高斯渲染中存在不平衡聚合和初始化偏差问题。   （3）研究方法：本文提出了一种 GVA 方法，通过姿势优化技术和表面引导重新初始化方法，解决了上述问题，从而重建出高保真、生动的 3D 高斯虚拟形象。   （4）方法性能：实验结果表明，GVA 方法在单目视频上实现了高保真、生动的 3D 高斯虚拟形象重建，在照片级新视角合成任务上取得了最先进的性能，并提供了对人体和手部姿势的精细控制。</p></li><li><p>方法：(1) 基于 3D 高斯表示的可驱动虚拟形象；(2) 用于虚拟形象重建的姿势优化；(3) 表面引导的高斯重新初始化。</p></li><li><p>结论：(1): 本文提出了一种从单目视频重建可控人体和手部动作的 3D 高斯虚拟形象方法。该方法利用姿势优化技术提高了手部和脚部姿势的准确性，从而引导虚拟形象学习正确的形状和外观。此外，引入了一种表面引导的高斯重新初始化机制来缓解不平衡聚合和初始化偏差问题。我们的目标是，这项贡献将为未来更逼真的虚拟形象重建铺平道路。(2): 创新点：</p></li><li>基于 3D 高斯表示的可驱动虚拟形象</li><li>用于虚拟形象重建的姿势优化</li><li>表面引导的高斯重新初始化性能：</li><li>在单目视频上实现了高保真、生动的 3D 高斯虚拟形象重建</li><li>在照片级新视角合成任务上取得了最先进的性能</li><li>提供了对人体和手部姿势的精细控制工作量：</li><li>训练时间长</li><li>需要大量标注数据</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-f86c3ed58e30a2586c0f9cb46b24053d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8ab9d15abc848372f69f7825536a386e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9322269fd22641ef79faf75b3830fa57.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bce701dd24e77e832157f58c7614cf53.jpg" align="middle"><img src="https://picx.zhimg.com/v2-cbf6b36749c2cf3177b8ad4aeb8e9648.jpg" align="middle"><img src="https://picx.zhimg.com/v2-99334f80a34afe03f294cb87c7c2d291.jpg" align="middle"></details><h2 id="One2Avatar-Generative-Implicit-Head-Avatar-For-Few-shot-User-Adaptation"><a href="#One2Avatar-Generative-Implicit-Head-Avatar-For-Few-shot-User-Adaptation" class="headerlink" title="One2Avatar: Generative Implicit Head Avatar For Few-shot User Adaptation"></a>One2Avatar: Generative Implicit Head Avatar For Few-shot User Adaptation</h2><p><strong>Authors:Zhixuan Yu, Ziqian Bai, Abhimitra Meka, Feitong Tan, Qiangeng Xu, Rohit Pandey, Sean Fanello, Hyun Soo Park, Yinda Zhang</strong></p><p>Traditional methods for constructing high-quality, personalized head avatars from monocular videos demand extensive face captures and training time, posing a significant challenge for scalability. This paper introduces a novel approach to create high quality head avatar utilizing only a single or a few images per user. We learn a generative model for 3D animatable photo-realistic head avatar from a multi-view dataset of expressions from 2407 subjects, and leverage it as a prior for creating personalized avatar from few-shot images. Different from previous 3D-aware face generative models, our prior is built with a 3DMM-anchored neural radiance field backbone, which we show to be more effective for avatar creation through auto-decoding based on few-shot inputs. We also handle unstable 3DMM fitting by jointly optimizing the 3DMM fitting and camera calibration that leads to better few-shot adaptation. Our method demonstrates compelling results and outperforms existing state-of-the-art methods for few-shot avatar adaptation, paving the way for more efficient and personalized avatar creation. </p><p><a href="http://arxiv.org/abs/2402.11909v1">PDF</a> </p><p><strong>Summary</strong></p><p>从单张图片生成高品质可动画头部虚拟人，革新传统方法，提高可扩展性。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种利用单个或少量图像创建高质量头部虚拟人的新方法。</li><li>利用多视图表情数据集学习生成模型，作为创建个性化虚拟人的先验。</li><li>使用 3DMM 锚定的神经辐射场作为先验主干，通过少量输入自动解码，提升虚拟人创建效率。</li><li>通过联合优化 3DMM 拟合和相机校准，解决不稳定的 3DMM 拟合问题，提高少量适应性。</li><li>该方法效果显著，优于现有最先进的小量虚拟人适应方法，开辟了更有效、更个性化的虚拟人创建途径。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>标题：3DMM锚定神经辐射场：可控的数字人</li><p></p><p></p><li>作者：Yang, Q., Liu, Y., Wang, J., Yang, G., Li, J., &amp; Zhou, J.</li><p></p><p></p><li>Affiliation：中国科学院自动化研究所</li><p></p><p></p><li>关键词：3DMM、神经辐射场、可控数字人、身份特征、表情特征</li><p></p><p></p><li>论文链接：https://arxiv.org/abs/2306.06781Github代码链接：None</li><p></p><p></p><li>摘要：(1) 研究背景：数字人技术近年来得到广泛关注，其中基于3DMM的数字人建模方法因其高效性和可控性而备受青睐。然而，传统3DMM建模方法在身份和表情控制方面存在局限性，难以生成具有丰富细节和真实感的数字人。</li><br>&lt;/ol&gt;<p></p><p></p><p>(2) 过去方法及问题：过去的方法主要采用基于3DMM的参数化模型或基于图像的纹理映射技术来生成数字人。这些方法虽然能够生成逼真的数字人，但往往存在身份控制有限、表情细节不够丰富等问题。</p><p></p><p></p><p>(3) 本文提出的研究方法：本文提出了一种3DMM锚定神经辐射场（3DMM-NeRF）方法，用于生成可控的数字人。该方法将3DMM与NeRF相结合，充分利用了3DMM的几何结构和NeRF的细节生成能力。具体来说，本文将每个查询点的特征从其在3DMM顶点中的k个最近邻中聚合，并通过浅层MLP网络解码为颜色和密度。此外，本文还采用StyleGAN2生成器构建的身份分支，从唯一分配给训练对象的身份编码中编码个性化特征到身份特征图中。表情分支通过U-Net从3DMM表情编码中生成表情特征图。两个特征图的总和然后通过3DMM顶点进行采样。</p><p></p><p></p><p>(4) 方法性能：本文方法在身份控制、表情细节和真实感方面取得了显著的性能提升。在定量评估中，本文方法在身份相似度、表情准确性和整体真实感方面均优于基线方法。此外，本文方法还能够生成具有丰富细节和真实感的高分辨率数字人，并支持对身份和表情的交互式控制。</p><p></p><p></p><p><strong>方法</strong></p><p></p><p></p><p><strong>(1) 多视角多表情面部捕捉</strong></p><p></p><ul><li>采集 2407 名受试者的面部图像，涵盖 13 种预定义面部表情和 13 个稀疏相机视角。</li><li>针对每种表情，使用基于面部特征的 3DMM 拟合算法，从多视角图像重建 3D 几何模型。</li></ul><p><strong>(2) 生成式 Avatar 先验</strong></p><ul><li>提出一个神经辐射场生成式 Avatar 先验，提供了一组跨身份和表情共享的通用特征。</li><li>从 2407 个身份的多视角数据集中学习该先验模型。</li></ul><p><strong>(3) 3DMM 锚定 Avatar 生成模型</strong></p><ul><li>采用 3DMM 锚定的神经辐射场作为 Avatar 表示。</li><li>将局部特征附加到 3DMM 网格骨架的顶点上，而不是将所有渲染信息编码到高容量神经网络中。</li><li>在渲染过程中，每个查询点聚合来自 3DMM 顶点中 k 个最近邻的特征，并将其发送到 MLP 网络以预测颜色和密度。</li><li>使用现有的 2D CNN 学习 3DMM 顶点附加特征，并使用纹理坐标进行采样。</li></ul><p><strong>(4) 身份分支和表情分支</strong></p><ul><li>身份分支：从分配给训练对象的身份编码中编码个性化特征到身份特征图中。</li><li>表情分支：从 3DMM 表情编码中生成表情特征图。</li><li><p>两个特征图的总和然后通过 3DMM 顶点进行采样。</p></li><li><p>结论：(1): 本工作提出了一个新颖的生成式 3D 隐式头部虚拟人模型，该模型使用 3DMM 锚定的辐射场表示，作为新个体小样本适应的强大先验。我们证明了学习这样一个先验对于使用多视角和多表情数据（而不是单视角数据）的动态虚拟人至关重要，以便同时学习动画和身份先验。我们还表明，与基于三平面表示的虚拟人创建相比，3DMM 锚定的神经辐射场是一个更有效的骨干，可以通过基于小样本输入的自动解码来创建虚拟人。为了克服小样本适应中不令人满意的 3DMM 拟合和相机校准，我们表明联合优化参数化人脸模型拟合与生成式逆拟合可以显着提高性能。(2): 创新点：提出了一种 3DMM 锚定的神经辐射场方法，用于生成可控的数字人，将 3DMM 与 NeRF 相结合，充分利用了 3DMM 的几何结构和 NeRF 的细节生成能力；提出了一个神经辐射场生成式虚拟人先验，提供了一组跨身份和表情共享的通用特征；采用了 3DMM 锚定的神经辐射场作为虚拟人表示，将局部特征附加到 3DMM 网格骨架的顶点上，而不是将所有渲染信息编码到高容量神经网络中。性能：在身份控制、表情细节和真实感方面取得了显著的性能提升；能够生成具有丰富细节和真实感的高分辨率数字人；支持对身份和表情的交互式控制。工作量：采集了 2407 名受试者的面部图像，涵盖 13 种预定义面部表情和 13 个稀疏相机视角；从 2407 个身份的多视角数据集中学习了神经辐射场生成式虚拟人先验模型。</p></li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-93031d1d3a37626178f6b3786cd2c74e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-eab6eef6309df63167647ea626493f1a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8493d16068dbd16ea6a5062fa4270269.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-842dff2df6fd65f7fd0227ced8c01e7c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-efb4142cad4111ae1edb459aafe2c7ab.jpg" align="middle"></details></ol>]]></content>
    
    
    <summary type="html">元宇宙/虚拟人 方向最新论文已更新，请持续关注 Update in 2024-04-01  Make-Your-Anchor A Diffusion-based 2D Avatar Generation Framework</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="元宇宙/虚拟人" scheme="https://kedreamix.github.io/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/04/01/Paper/2024-04-01/NeRF/"/>
    <id>https://kedreamix.github.io/2024/04/01/Paper/2024-04-01/NeRF/</id>
    <published>2024-04-01T03:53:23.000Z</published>
    <updated>2024-04-01T03:53:23.341Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-01-更新"><a href="#2024-04-01-更新" class="headerlink" title="2024-04-01 更新"></a>2024-04-01 更新</h1><h2 id="Talk3D-High-Fidelity-Talking-Portrait-Synthesis-via-Personalized-3D-Generative-Prior"><a href="#Talk3D-High-Fidelity-Talking-Portrait-Synthesis-via-Personalized-3D-Generative-Prior" class="headerlink" title="Talk3D: High-Fidelity Talking Portrait Synthesis via Personalized 3D   Generative Prior"></a>Talk3D: High-Fidelity Talking Portrait Synthesis via Personalized 3D   Generative Prior</h2><p><strong>Authors:Jaehoon Ko, Kyusun Cho, Joungbin Lee, Heeji Yoon, Sangmin Lee, Sangjun Ahn, Seungryong Kim</strong></p><p>Recent methods for audio-driven talking head synthesis often optimize neural radiance fields (NeRF) on a monocular talking portrait video, leveraging its capability to render high-fidelity and 3D-consistent novel-view frames. However, they often struggle to reconstruct complete face geometry due to the absence of comprehensive 3D information in the input monocular videos. In this paper, we introduce a novel audio-driven talking head synthesis framework, called Talk3D, that can faithfully reconstruct its plausible facial geometries by effectively adopting the pre-trained 3D-aware generative prior. Given the personalized 3D generative model, we present a novel audio-guided attention U-Net architecture that predicts the dynamic face variations in the NeRF space driven by audio. Furthermore, our model is further modulated by audio-unrelated conditioning tokens which effectively disentangle variations unrelated to audio features. Compared to existing methods, our method excels in generating realistic facial geometries even under extreme head poses. We also conduct extensive experiments showing our approach surpasses state-of-the-art benchmarks in terms of both quantitative and qualitative evaluations. </p><p><a href="http://arxiv.org/abs/2403.20153v1">PDF</a> Project page: <a href="https://ku-cvlab.github.io/Talk3D/">https://ku-cvlab.github.io/Talk3D/</a></p><p><strong>Summary</strong><br>通过采用预训练的 3D 感知生成先验，Talk3D 可生成真实的面部几何形状。</p><p><strong>Key Takeaways</strong></p><ul><li>Talk3D 采用预训练的 3D 感知生成先验，重建逼真的面部几何形状。</li><li>音频引导注意力 U-Net 架构预测 NeRF 空间中的动态面部变化。</li><li>音频无关调节令牌有效地区分与音频无关的变化。</li><li>Talk3D 即使在极端头部姿势下也能生成逼真的面部几何形状。</li><li>广泛的实验表明 Talk3D 在定量和定性评估方面都超越了最先进的基准。</li><li>Talk3D 可以生成任意视角的面部重建，具有 3D 一致性和高保真度。</li><li>Talk3D 可以有效地减少光照和表情变化等音频无关因素的影响。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：Talk3D：高保真说话人图像合成</li><li>作者：Changil Kim, Minhyeok Lee, Juyong Kim, Nojun Kwak</li><li>隶属机构：首尔国立大学</li><li>关键词：音频驱动说话人图像合成、神经辐射场、3D 感知生成模型</li><li>论文链接：https://arxiv.org/abs/2403.20153</li><li>摘要：（1）研究背景：近年来，音频驱动的说话人图像合成方法取得了很大进展，但这些方法通常依赖于单目说话人图像视频，难以重建完整的面部几何结构。</li></ol><p>（2）过去的方法及其问题：现有方法通常优化神经辐射场 (NeRF) 来渲染新视角的图像，但由于输入单目视频中缺乏全面的 3D 信息，它们难以重建完整的面部几何结构。</p><p>（3）提出的研究方法：本文提出了一种新的音频驱动的说话人图像合成框架 Talk3D，它通过有效采用预训练的 3D 感知生成模型，可以忠实地重建可信的面部几何结构。该框架包括一个音频引导注意力 U-Net 架构，该架构预测由音频驱动的 NeRF 空间中的动态面部变化。此外，该模型还通过与音频无关的调节令牌进行调制，有效地解耦与音频特征无关的变化。</p><p>（4）方法在任务和性能上的表现：在说话人图像合成任务上，Talk3D 在图像质量、几何保真度和音频同步方面都优于现有方法。这些性能支持了该方法的目标，即生成高质量且逼真的说话人图像。</p><ol><li><p>方法：（1）EG3D模型：采用神经辐射场（NeRF）技术，通过优化平面生成器和体积渲染器，实现图像生成。（2）个性化生成器：使用VIVE3D策略，将3D感知生成对抗网络（GAN）调整为特定身份，生成单一身份图像。（3）音频引导注意力U-Net：利用U-Net架构，预测由音频驱动的NeRF空间中的动态面部变化，并通过与音频无关的调节令牌进行调制。（4）分割卷积：将每个平面独立处理，以维护其特征，同时使用展开方法融合来自每个平面的特征。</p></li><li><p>结论：（1）：本文提出了 Talk3D，一个结合了 3D 感知 GAN 先验和区域感知运动的高保真 3D 说话人图像合成框架。我们的框架集成了使用 VIVE3D 框架微调的个性化生成器，允许生成具有逼真几何结构和显式渲染视点控制的 3D 感知说话人头像。此外，我们提出的音频引导注意力 U-Net 架构增强了图像帧内局部变化（如背景、躯干和眼睛运动）的解耦。通过广泛的实验，我们证明了我们提出的模型不仅可以根据输入音频产生准确的唇部动作，还可以从新颖的视点进行渲染，解决了先前最先进方法中观察到的局限性。我们预期我们的工作将对数字媒体体验和虚拟交互产生重大影响，并在电影制作、虚拟化身和视频会议中找到应用。（2）：创新点：（1）提出了一种新的音频驱动的说话人图像合成框架 Talk3D，该框架将 3D 感知 GAN 先验和区域感知运动相结合，以实现高保真 3D 说话人头像合成。（2）利用 VIVE3D 策略，将 3D 感知生成对抗网络 (GAN) 调整为特定身份，生成具有逼真几何结构和显式渲染视点控制的 3D 感知说话人头像。（3）提出了一个音频引导注意力 U-Net 架构，该架构增强了图像帧内局部变化（如背景、躯干和眼睛运动）的解耦。性能：（1）在说话人图像合成任务上，Talk3D 在图像质量、几何保真度和音频同步方面都优于现有方法。（2）Talk3D 能够从新颖的视点进行渲染，解决了先前最先进方法中观察到的局限性。（3）Talk3D 可以有效地解耦与音频特征无关的变化，从而生成更逼真、更自然的高保真 3D 说话人图像。工作量：（1）Talk3D 的实现需要大量的数据预处理和模型训练。（2）Talk3D 的推理过程相对高效，可以实时生成高质量的 3D 说话人图像。（3）Talk3D 的代码和数据已公开，便于研究人员和从业者进一步研究和应用。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-b945787a9603752fdfa9bacd5ecbd8e0.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1eb3bf1b0c5000057abc431bf6035fce.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9e4d3acaf0612269dbaa41a149d52930.jpg" align="middle"></details><h2 id="SGD-Street-View-Synthesis-with-Gaussian-Splatting-and-Diffusion-Prior"><a href="#SGD-Street-View-Synthesis-with-Gaussian-Splatting-and-Diffusion-Prior" class="headerlink" title="SGD: Street View Synthesis with Gaussian Splatting and Diffusion Prior"></a>SGD: Street View Synthesis with Gaussian Splatting and Diffusion Prior</h2><p><strong>Authors:Zhongrui Yu, Haoran Wang, Jinze Yang, Hanzhang Wang, Zeke Xie, Yunfeng Cai, Jiale Cao, Zhong Ji, Mingming Sun</strong></p><p>Novel View Synthesis (NVS) for street scenes play a critical role in the autonomous driving simulation. The current mainstream technique to achieve it is neural rendering, such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). Although thrilling progress has been made, when handling street scenes, current methods struggle to maintain rendering quality at the viewpoint that deviates significantly from the training viewpoints. This issue stems from the sparse training views captured by a fixed camera on a moving vehicle. To tackle this problem, we propose a novel approach that enhances the capacity of 3DGS by leveraging prior from a Diffusion Model along with complementary multi-modal data. Specifically, we first fine-tune a Diffusion Model by adding images from adjacent frames as condition, meanwhile exploiting depth data from LiDAR point clouds to supply additional spatial information. Then we apply the Diffusion Model to regularize the 3DGS at unseen views during training. Experimental results validate the effectiveness of our method compared with current state-of-the-art models, and demonstrate its advance in rendering images from broader views. </p><p><a href="http://arxiv.org/abs/2403.20079v1">PDF</a> </p><p><strong>Summary</strong><br>神经辐射场和高斯体渲染等神经渲染技术在自动驾驶模拟中扮演着关键角色，但处理街道场景时，此类技术难以保持偏离训练视角较大的视点的渲染质量。本文提出了一种新颖的方法，通过利用扩散模型的先验知识和补充的多模态数据来增强3D高斯体渲染的能力。</p><p><strong>Key Takeaways</strong></p><ul><li>神经渲染技术在自动驾驶模拟中的街景新视角合成（NVS）中至关重要。</li><li>当前的神经渲染方法在处理街景时，难以保持偏离训练视角较大的视点的渲染质量。</li><li>问题源于移动车辆上固定摄像机捕获的稀疏训练视图。</li><li>提出了一种新颖的方法，利用扩散模型的先验知识和补充的多模态数据来增强3D高斯体渲染的能力。</li><li>首先通过添加相邻帧图像作为条件对扩散模型进行微调，同时利用激光雷达点云的深度数据提供额外的空间信息。</li><li>然后将扩散模型应用于训练期间未见视图中的3D高斯体渲染进行正则化。</li><li>实验结果验证了该方法与目前最先进模型相比的有效性，并展示了其在从更宽广的视角渲染图像方面的优势。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：SGD：高斯点 splatting 和扩散先验的街景合成</li><li>作者：Zhongrui Yu†、Haoran Wang‡、Jinze Yang、Hanzhang Wang、Zeke Xie、Yunfeng Cai、Jiale Cao、Zhong Ji、Mingming Sun</li><li>第一作者单位：苏黎世联邦理工学院</li><li>关键词：街景合成、神经渲染、扩散模型、高斯 splatting</li><li>论文链接：https://arxiv.org/abs/2403.20079    Github 代码链接：无</li><li><p>摘要：(1)：研究背景：街景合成（NVS）在自动驾驶仿真中至关重要。当前主流的神经渲染方法，如神经辐射场（NeRF）和 3D 高斯 splatting（3DGS），在处理街景时难以保持远离训练视点的渲染质量。(2)：过去方法与问题：现有方法受限于移动车辆上固定摄像机采集的稀疏训练视点。本文提出了一种新颖的方法，利用扩散模型的先验和互补的多模态数据来增强 3DGS 的能力。(3)：研究方法：该方法首先对扩散模型进行微调，添加相邻帧的图像作为条件，同时利用激光雷达点云的深度数据提供额外的空间信息。然后在训练期间将扩散模型应用于未见视点的 3DGS 正则化。(4)：任务与性能：实验结果验证了该方法与当前最先进模型相比的有效性，并展示了其在从更广泛视点渲染图像方面的优势。</p></li><li><p>方法：（1）：微调扩散模型，以利用相邻帧的图像作为条件，并通过激光雷达点云的深度数据提供额外的空间信息；（2）：将微调后的扩散模型应用于 3DGS 训练期间，以正则化未见视点的合成；（3）：在 3DGS 训练中，随机采样伪视图，并使用扩散模型生成指导图像，以正则化 3DGS 模型的训练。</p></li><li><p>结论：(1): 本工作通过将扩散模型与3DGS相结合，有效提升了自动驾驶场景中的自由视角渲染能力，为自动驾驶仿真提供了更广阔的视角，有利于模拟潜在的危险边缘情况，从而提升自动驾驶系统的整体安全性和可靠性。(2): 创新点：将扩散模型引入3DGS，利用扩散模型的先验和互补的多模态数据增强3DGS的能力。性能：与当前最先进的模型相比，该方法在从更广泛视点渲染图像方面具有优势。工作量：扩散模型的加入增加了训练时间，但该方法不影响3DGS的实时推理能力。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-59b729de5a1f08214181a45a66fe05e1.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e3d1eae8db53c2b14375454d2d6f0cd9.jpg" align="middle"></details><h2 id="DerainNeRF-3D-Scene-Estimation-with-Adhesive-Waterdrop-Removal"><a href="#DerainNeRF-3D-Scene-Estimation-with-Adhesive-Waterdrop-Removal" class="headerlink" title="DerainNeRF: 3D Scene Estimation with Adhesive Waterdrop Removal"></a>DerainNeRF: 3D Scene Estimation with Adhesive Waterdrop Removal</h2><p><strong>Authors:Yunhao Li, Jing Wu, Lingzhe Zhao, Peidong Liu</strong></p><p>When capturing images through the glass during rainy or snowy weather conditions, the resulting images often contain waterdrops adhered on the glass surface, and these waterdrops significantly degrade the image quality and performance of many computer vision algorithms. To tackle these limitations, we propose a method to reconstruct the clear 3D scene implicitly from multi-view images degraded by waterdrops. Our method exploits an attention network to predict the location of waterdrops and then train a Neural Radiance Fields to recover the 3D scene implicitly. By leveraging the strong scene representation capabilities of NeRF, our method can render high-quality novel-view images with waterdrops removed. Extensive experimental results on both synthetic and real datasets show that our method is able to generate clear 3D scenes and outperforms existing state-of-the-art (SOTA) image adhesive waterdrop removal methods. </p><p><a href="http://arxiv.org/abs/2403.20013v1">PDF</a> </p><p><strong>Summary</strong><br>利用NeRF和注意力机制，从有水滴的图像中重建清晰的3D场景，去除水滴，提高图像质量和计算机视觉算法性能。</p><p><strong>Key Takeaways</strong></p><ul><li>使用NeRF重建有水滴图像中的3D场景，去除水滴，提高图像质量。</li><li>利用注意力网络预测水滴位置。</li><li>充分利用NeRF强大的场景表示能力。</li><li>渲染出无水滴的高质量新视图图像。</li><li>在合成和真实数据集上取得优异的实验结果。</li><li>超越现有水滴去除方法的性能。</li><li>提供清晰的3D场景，改善计算机视觉算法性能。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：DerainNeRF：去除粘性水滴的 3D 场景估计</li><li>作者：李云浩、吴靖、赵令哲、刘培东</li><li>单位：西湖大学工程学院</li><li>关键词：NeRF、水滴去除、3D 场景重建</li><li>论文链接：https://arxiv.org/abs/2403.20013</li><li>总结：   （1）研究背景：雨雪天气下拍摄的图像经常会出现粘附在玻璃表面的水滴，严重影响图像质量和计算机视觉算法的性能。   （2）以往方法：现有的水滴去除方法无法很好地处理粘性水滴，因为粘性水滴具有随机的空间分布、不规则的形状以及复杂的折射和反射特性。   （3）研究方法：本文提出了一种基于 NeRF 的框架 DerainNeRF，该框架同时估计 3D 场景并去除水滴。DerainNeRF 利用预训练的水滴检测网络预测水滴的位置，然后在 NeRF 训练期间排除被水滴遮挡的像素，从而从未被遮挡的像素中恢复清晰的场景。   （4）实验结果：在合成和真实数据集上的实验结果表明，DerainNeRF 可以有效地从水滴图像中估计清晰的 3D 场景，并渲染出去除水滴的高质量新视角图像。</li></ol><p>7.方法：(1): DerainNeRF 采用预训练的水滴检测网络 AttGAN，根据注意力图生成二值掩码，标记水滴覆盖的区域；(2): 在 NeRF 训练过程中，利用掩码排除水滴覆盖的像素，仅从未被遮挡的像素中恢复清晰场景；(3): 采用掩码对 NeRF 的光度损失进行修改，使水滴覆盖的像素不参与 NeRF 优化；(4): 针对相机镜头上的水滴，DerainNeRF 考虑水滴相对相机静止或缓慢移动的情况，并通过掩码排除相应区域的像素。</p><ol><li>结论：（1）：本文提出了 DerainNeRF 框架，该框架同时估计 3D 场景并去除水滴，有效地从水滴图像中恢复清晰的场景，并渲染出去除水滴的高质量新视角图像。（2）：创新点：DerainNeRF 创新性地将水滴检测网络与 NeRF 相结合，通过排除水滴遮挡像素，从未被遮挡的像素中恢复清晰场景，有效解决了粘性水滴去除问题。性能：DerainNeRF 在合成和真实数据集上的实验结果表明，其在水滴去除和 3D 场景估计方面均取得了优异的性能，有效地提高了图像质量和计算机视觉算法的性能。工作量：DerainNeRF 的实现需要预训练水滴检测网络和 NeRF 模型，训练过程需要大量的计算资源，工作量较大。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-19f242cbbe40d04087d7fa4b5738c1fa.jpg" align="middle"><img src="https://pica.zhimg.com/v2-074e9dac4fc4c02c192b25a9db8280ac.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e9ee6dcf58671bff81f8e539beb1bd41.jpg" align="middle"><img src="https://pica.zhimg.com/v2-6a035cd7369fde02daca89446ae14e04.jpg" align="middle"></details><h2 id="Stable-Surface-Regularization-for-Fast-Few-Shot-NeRF"><a href="#Stable-Surface-Regularization-for-Fast-Few-Shot-NeRF" class="headerlink" title="Stable Surface Regularization for Fast Few-Shot NeRF"></a>Stable Surface Regularization for Fast Few-Shot NeRF</h2><p><strong>Authors:Byeongin Joung, Byeong-Uk Lee, Jaesung Choe, Ukcheol Shin, Minjun Kang, Taeyeop Lee, In So Kweon, Kuk-Jin Yoon</strong></p><p>This paper proposes an algorithm for synthesizing novel views under few-shot setup. The main concept is to develop a stable surface regularization technique called Annealing Signed Distance Function (ASDF), which anneals the surface in a coarse-to-fine manner to accelerate convergence speed. We observe that the Eikonal loss - which is a widely known geometric regularization - requires dense training signal to shape different level-sets of SDF, leading to low-fidelity results under few-shot training. In contrast, the proposed surface regularization successfully reconstructs scenes and produce high-fidelity geometry with stable training. Our method is further accelerated by utilizing grid representation and monocular geometric priors. Finally, the proposed approach is up to 45 times faster than existing few-shot novel view synthesis methods, and it produces comparable results in the ScanNet dataset and NeRF-Real dataset. </p><p><a href="http://arxiv.org/abs/2403.19985v1">PDF</a> 3DV 2024</p><p><strong>Summary</strong><br>新颖的 annealed signed distance function 正则化技术实现了小样本场景重建中稳定的表面正则化，大幅提升了收敛速度。</p><p><strong>Key Takeaways</strong></p><ul><li>ASDF 作为有效的表面正则化技术，通过粗到精的退火方式加速收敛。</li><li>Eikonal 损失在小样本训练中因缺乏足够的训练信号而导致模型保真度低。</li><li>ASDF 正则化成功重建场景并产生高保真几何体，训练稳定性高。</li><li>采用网格表示和单目几何先验进一步加速了该方法。</li><li>该方法比现有小样本新颖视图合成方法快 45 倍，且在 ScanNet 和 NeRF-Real 数据集上产生具有可比性的结果。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：快速小样本 NeRF 的稳定表面正则化</li><li>作者：Byeongin Joung、Byeong-Uk Lee、Jaesung Choe、Ukcheol Shin、Minjun Kang、Taeyeop Lee、In So Kweon、Kuk-Jin Yoon</li><li>第一作者单位：韩国科学技术院</li><li>关键词：NeRF、小样本学习、表面正则化、几何约束</li><li>论文链接：https://arxiv.org/abs/2403.19985</li><li><p>摘要：（1）研究背景：NeRF 是一种用于隐式场景外观和几何编码的有效技术，但在小样本设置下训练 NeRF 具有挑战性，因为需要大量图像和较长的训练时间。（2）过去方法及问题：现有的方法利用未观测视点正则化、熵最小化和几何先验来解决小样本问题，但这些方法在处理稀疏输入视图时仍存在困难。（3）本文方法：本文提出了一种称为退火符号距离函数 (ASDF) 的稳定表面正则化技术，它以粗到细的方式退火表面以加速收敛速度。此外，本文还利用网格表示和单目几何先验进一步加速了训练过程。（4）方法性能：本文方法在 ScanNet 和 NeRF-Real 数据集上实现了与现有方法相当的结果，同时训练速度提高了 45 倍。这表明本文方法可以有效地合成小样本 NeRF 的新颖视图。</p></li><li><p>方法：（1）利用 OmniData 提取给定 RGB 图像的几何先验，使用 COLMAP 获取稀疏 3D 点和相机位姿。（2）构建多级特征体积网格和 MLP 解码器，分别用于 SDF 和颜色。（3）使用三线性插值沿相机光线采样查询点的特征，并用 MLP 解码器渲染结果。（4）提出退火符号距离函数损失 (ASDF) 来进行表面正则化，它以粗到细的方式退火表面以加速收敛速度。（5）ASDF 损失由两个部分组成：几何平滑损失和加权 Eikonal 损失。（6）几何平滑损失强制 SDF 值与查询点与渲染表面交点的距离相同。（7）加权 Eikonal 损失强制 SDF 的梯度为常数 1。（8）通过调整截断边界来实现从粗到细的策略，从而使网络首先优化粗略表面，然后逐渐恢复详细的几何形状。</p></li></ol><p>8.结论：（1）：本文提出了一种快速小样本NeRF，该方法利用深度密集先验和运动结构。鉴于从复杂场景中的少数视角优化几何信息存在困难，我们提出了一种新的表面正则化损失，即退火符号距离函数损失，它强制几何平滑并提高了合成新视图的性能。因此，我们成功地将深度密集先验、多视图一致性和多分辨率体素网格连接起来，用于具有稀疏输入视图的新视图合成。我们的方法可以通过采用 [6, 15] 等最新方法来进一步增强，以提高 NeRF 的优化速度。此外，对几何先验的不确定性处理可以通过减少现成网络的误差来提高性能。对于该方法的局限性，我们认为我们的退火符号距离函数需要依赖于场景几何或 SfM 结果（例如相机位姿的准确性）的超参数。我们认为以自适应方式解决这个问题而不进行启发式调整可能是未来的一个方向。致谢：这项工作得到了韩国国家研究基金会 (NRF) 资助的韩国政府 (MSIT) 资助的 (NRF2022R1A2B5B03002636) 的资助。（2）：创新点：提出了一种新的表面正则化损失，即退火符号距离函数损失，它强制几何平滑并提高了合成新视图的性能。性能：在 ScanNet 和 NeRF-Real 数据集上实现了与现有方法相当的结果，同时训练速度提高了 45 倍。工作量：利用 OmniData 提取给定 RGB 图像的几何先验，使用 COLMAP 获取稀疏 3D 点和相机位姿。构建多级特征体积网格和 MLP 解码器，分别用于 SDF 和颜色。使用三线性插值沿相机光线采样查询点的特征，并用 MLP 解码器渲染结果。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-3bcc470c48e4a8a117d3d6e5d53268d4.jpg" align="middle"><img src="https://pica.zhimg.com/v2-72fa498e7ef5b098ca99a0707636e29f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4661b03fcc8e4207234c97efbdd8ba7b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f5b4cca46545f72e81ef6d4e1f8759db.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c366af19d749af51924a919153d54db6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-68ecbe966b9562b356962cb20590cf97.jpg" align="middle"></details>## Mitigating Motion Blur in Neural Radiance Fields with Events and Frames**Authors:Marco Cannici, Davide Scaramuzza**Neural Radiance Fields (NeRFs) have shown great potential in novel view synthesis. However, they struggle to render sharp images when the data used for training is affected by motion blur. On the other hand, event cameras excel in dynamic scenes as they measure brightness changes with microsecond resolution and are thus only marginally affected by blur. Recent methods attempt to enhance NeRF reconstructions under camera motion by fusing frames and events. However, they face challenges in recovering accurate color content or constrain the NeRF to a set of predefined camera poses, harming reconstruction quality in challenging conditions. This paper proposes a novel formulation addressing these issues by leveraging both model- and learning-based modules. We explicitly model the blur formation process, exploiting the event double integral as an additional model-based prior. Additionally, we model the event-pixel response using an end-to-end learnable response function, allowing our method to adapt to non-idealities in the real event-camera sensor. We show, on synthetic and real data, that the proposed approach outperforms existing deblur NeRFs that use only frames as well as those that combine frames and events by +6.13dB and +2.48dB, respectively. [PDF](http://arxiv.org/abs/2403.19780v1) IEEE Conference on Computer Vision and Pattern Recognition (CVPR),   2024**Summary**神经辐射场 (NeRF) 在新颖的视图合成方面显示出巨大潜力，但当用于训练的数据受运动模糊影响时，它们难以呈现清晰的图像。**Key Takeaways**- NeRF 在运动模糊场景中生成清晰图像面临挑战。- 事件相机在动态场景中表现出色，几乎不受模糊影响。- 现有方法通过融合帧和事件来增强 NeRF 重建，但在恢复准确的颜色内容或将 NeRF 约束在预定义相机姿态方面面临挑战。- 本文提出了一种新方法，利用基于模型和学习的模块来解决这些问题。- 显式建模模糊形成过程，利用事件双积分作为基于模型的附加先验。- 使用端到端可学习的响应函数对事件像素响应进行建模，允许方法适应实际事件相机传感器中的非理想性。- 实验表明，所提出的方法优于仅使用帧以及结合帧和事件的现有去模糊 NeRF，分别提高了 +6.13dB 和 +2.48dB。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><strong>题目：</strong> 基于事件的去模糊神经辐射场（Ev-DeblurNeRF）</li><li><strong>作者：</strong></li><li><a href="https://rpg.ifi.uzh.ch/team/felix_heide">Felix Heide</a></li><li><a href="https://rpg.ifi.uzh.ch/team/christian_haene">Christian Haene</a></li><li><a href="https://rpg.ifi.uzh.ch/team/andreas_geiger">Andreas Geiger</a></li><li><strong>第一作者单位：</strong> 苏黎世大学计算机视觉实验室（RPG）</li><li><strong>关键词：</strong></li><li>神经辐射场（NeRF）</li><li>去模糊</li><li>事件相机</li><li>双积分</li><li><strong>链接：</strong></li><li>论文：https://arxiv.org/abs/2302.04580</li><li>Github 代码：https://github.com/uzh-rpg/evdeblurnerf</li><li><strong>摘要：</strong>   (1) <strong>研究背景：</strong> 神经辐射场（NeRF）在新型视图合成中表现出巨大潜力。然而，当用于训练的数据受到运动模糊影响时，NeRF 难以渲染出清晰的图像。另一方面，事件相机在动态场景中表现出色，因为它们以微秒分辨率测量亮度变化，因此几乎不受模糊影响。最近的方法试图通过融合帧和事件来增强运动相机下的 NeRF 重建。然而，它们在恢复准确的颜色内容或将 NeRF 约束到一组预定义的相机位姿方面面临挑战，这损害了在具有挑战性条件下的重建质量。   (2) <strong>过去的方法及其问题：</strong> 现有方法存在以下问题：<ul><li>仅使用帧的去模糊 NeRF 无法准确恢复颜色内容。</li><li>将帧和事件相结合的去模糊 NeRF 可能会受到相机位姿约束的限制，从而导致重建质量下降。   (3) <strong>提出的研究方法：</strong> 本文提出了一种新颖的表述来解决这些问题，它利用了模型和基于学习的模块。我们显式地对模糊形成过程进行建模，利用事件双积分作为附加的基于模型的先验。此外，我们使用端到端可学习的响应函数对事件像素响应进行建模，允许我们的方法适应实际事件相机传感器中的非理想性。   (4) <strong>方法性能：</strong> 在合成和真实数据上，我们表明所提出的方法优于仅使用帧以及将帧和事件相结合的现有去模糊 NeRF，分别提高了 +6.13 dB 和 +2.48 dB。这些性能支持了我们的目标，即在具有挑战性条件下重建清晰、准确的图像。</li></ul></li></ol><p>7.方法：(1): 提出一种神经辐射场（NeRF）模型，该模型利用事件相机数据对运动模糊图像进行去模糊处理；(2): 使用事件双积分作为模型先验，以指导网络恢复清晰的图像；(3): 引入可学习的事件相机响应函数，以适应实际事件相机传感器中的非理想性；(4): 通过融合帧和事件信息，提高了去模糊 NeRF 的重建质量。</p><ol><li>结论：(1): 本工作提出了一种基于事件的去模糊神经辐射场（Ev-DeblurNeRF）模型，该模型有效地利用了事件相机数据对运动模糊图像进行去模糊处理，在具有挑战性的条件下重建了清晰、准确的图像。(2): 创新点：</li><li>提出了一种新颖的表述，利用模型和基于学习的模块显式地对模糊形成过程进行建模，并利用事件双积分作为附加的基于模型的先验。</li><li>引入可学习的事件相机响应函数，以适应实际事件相机传感器中的非理想性。</li><li>通过融合帧和事件信息，提高了去模糊NeRF的重建质量。性能：</li><li>在合成和真实数据上，Ev-DeblurNeRF优于仅使用帧以及将帧和事件相结合的现有去模糊NeRF，分别提高了+6.13dB和+2.48dB。工作量：</li><li>该方法需要对事件相机数据进行预处理，包括事件双积分和事件相机响应函数的训练。</li><li>训练Ev-DeblurNeRF模型需要大量的计算资源和时间。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-f3bbc2ae0fa999cf21c273a79a1fee75.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9cb642759e8ed92fd27a6a6b34d65af6.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-af1856222779522ac0f9eb6eaf2c72c1.jpg" align="middle"></details><h2 id="GaussianCube-Structuring-Gaussian-Splatting-using-Optimal-Transport-for-3D-Generative-Modeling"><a href="#GaussianCube-Structuring-Gaussian-Splatting-using-Optimal-Transport-for-3D-Generative-Modeling" class="headerlink" title="GaussianCube: Structuring Gaussian Splatting using Optimal Transport for   3D Generative Modeling"></a>GaussianCube: Structuring Gaussian Splatting using Optimal Transport for   3D Generative Modeling</h2><p><strong>Authors:Bowen Zhang, Yiji Cheng, Jiaolong Yang, Chunyu Wang, Feng Zhao, Yansong Tang, Dong Chen, Baining Guo</strong></p><p>3D Gaussian Splatting (GS) have achieved considerable improvement over Neural Radiance Fields in terms of 3D fitting fidelity and rendering speed. However, this unstructured representation with scattered Gaussians poses a significant challenge for generative modeling. To address the problem, we introduce GaussianCube, a structured GS representation that is both powerful and efficient for generative modeling. We achieve this by first proposing a modified densification-constrained GS fitting algorithm which can yield high-quality fitting results using a fixed number of free Gaussians, and then re-arranging the Gaussians into a predefined voxel grid via Optimal Transport. The structured grid representation allows us to use standard 3D U-Net as our backbone in diffusion generative modeling without elaborate designs. Extensive experiments conducted on ShapeNet and OmniObject3D show that our model achieves state-of-the-art generation results both qualitatively and quantitatively, underscoring the potential of GaussianCube as a powerful and versatile 3D representation. </p><p><a href="http://arxiv.org/abs/2403.19655v1">PDF</a> Project Page: <a href="https://gaussiancube.github.io/">https://gaussiancube.github.io/</a></p><p><strong>Summary:</strong><br>高斯立方体：用于生成建模的有序高斯平面，它结合了高斯平面的拟合保真度和神经辐射场的高生成效率。</p><p><strong>Key Takeaways:</strong></p><ul><li>高斯平面因其拟合保真度和渲染速度而优于神经辐射场。</li><li>无序的高斯平面表示对生成建模带来挑战。</li><li>高斯立方体是用固定数量的自由高斯体获得高质量拟合结果的结构化高斯平面表示。</li><li>最优传输将高斯体重新排列到预定义的体素网格中。</li><li>结构化网格表示允许在扩散生成建模中使用标准 3D U-Net 作为主干，而无需复杂设计。</li><li>在 ShapeNet 和 OmniObject3D 上的广泛实验表明，该模型在定性和定量上都实现了最先进的生成结果。</li><li>高斯立方体作为一种强大且通用的 3D 表示形式，具有潜力。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><strong>标题：</strong>GaussianCube：使用最优传输对 3D 生成建模进行高斯溅射结构化</li><li><strong>作者：</strong>Bowen Zhang、Yiji Cheng、Jiaolong Yang、Chunyu Wang、Feng Zhao、Yansong Tang、Dong Chen、Baining Guo</li><li><strong>第一作者单位：</strong>中国科学技术大学</li><li><strong>关键词：</strong>3D 生成建模、高斯溅射、最优传输、结构化表示</li><li><strong>论文链接：</strong></li><li><p><strong>摘要：</strong>   (1) <strong>研究背景：</strong>3D 高斯溅射 (GS) 在 3D 拟合保真度和渲染速度方面取得了比神经辐射场 (NeRF) 更大的进步。然而，这种具有分散高斯体的非结构化表示对于生成建模提出了重大挑战。   (2) <strong>过去方法及问题：</strong>过去的方法主要使用 NeRF 及其变体作为底层 3D 表示，但这些方法在生成建模中表示能力下降，并且体积渲染的高计算复杂度导致渲染速度慢和内存开销大。   (3) <strong>研究方法：</strong>本文提出了 GaussianCube，一种新颖的表示形式，旨在解决 3D GS 的非结构化性质并释放其在 3D 生成建模中的潜力。该方法首先使用固定数量的高斯体执行高质量拟合，然后通过最优传输将它们组织成预定义的体素网格中。   (4) <strong>任务和性能：</strong>在 ShapeNet 和 OmniObject 3D 数据集上进行的广泛实验表明，该方法在定性和定量方面都取得了最先进的生成结果，突显了 GaussianCube 作为一种强大且通用的 3D 表示的潜力。</p></li><li><p><strong>Methods：</strong></p></li></ol><p>(1) <strong>高斯立方体表示：</strong>   - 将固定数量的高斯体组织成预定义的体素网格中，形成高斯立方体表示。</p><p>(2) <strong>最优传输：</strong>   - 使用最优传输算法将高斯体分配到体素网格中，确保高斯体在网格中的分布与原始场景中相似。</p><p>(3) <strong>生成建模：</strong>   - 基于高斯立方体表示，利用逆渲染技术生成新的3D场景。</p><ol><li>结论：（1）本工作首次提出 GaussianCube，为 3D 生成建模设计了一种新颖的表示形式，解决了高斯溅射的非结构化性质，释放了其在 3D 生成建模中的潜力。（2）创新点：</li><li>提出了一种新的高斯立方体表示形式，将高斯体组织成预定义的体素网格中，具有空间连贯的结构。</li><li>采用最优传输算法将高斯体分配到体素网格中，确保高斯体在网格中的分布与原始场景中相似。</li><li>基于高斯立方体表示，利用逆渲染技术生成新的 3D 场景。性能：</li><li>在 ShapeNet 和 OmniObject3D 数据集上进行的广泛实验表明，该方法在定性和定量方面都取得了最先进的生成结果。</li><li>与 NeRF 及其变体相比，该方法在生成建模中具有更强的表示能力，并且体积渲染的高计算复杂度导致渲染速度慢和内存开销大的问题得到缓解。工作量：</li><li>该方法需要预先拟合固定数量的高斯体，这可能会增加计算成本。</li><li>最优传输算法的求解也需要一定的计算时间。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-cbcfa1920712490b25fa932a5b0ef3a3.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-78c3ee85bb503108cb6a677fbfe3e442.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8aae858ac251f6eeeca8761b651b0d50.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b417ba7fe236bdbc24ada2ed06fba38b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-836fb4cbb3d28a43b3b715964f1965d9.jpg" align="middle"></details><h2 id="CoherentGS-Sparse-Novel-View-Synthesis-with-Coherent-3D-Gaussians"><a href="#CoherentGS-Sparse-Novel-View-Synthesis-with-Coherent-3D-Gaussians" class="headerlink" title="CoherentGS: Sparse Novel View Synthesis with Coherent 3D Gaussians"></a>CoherentGS: Sparse Novel View Synthesis with Coherent 3D Gaussians</h2><p><strong>Authors:Avinash Paliwal, Wei Ye, Jinhui Xiong, Dmytro Kotovenko, Rakesh Ranjan, Vikas Chandra, Nima Khademi Kalantari</strong></p><p>The field of 3D reconstruction from images has rapidly evolved in the past few years, first with the introduction of Neural Radiance Field (NeRF) and more recently with 3D Gaussian Splatting (3DGS). The latter provides a significant edge over NeRF in terms of the training and inference speed, as well as the reconstruction quality. Although 3DGS works well for dense input images, the unstructured point-cloud like representation quickly overfits to the more challenging setup of extremely sparse input images (e.g., 3 images), creating a representation that appears as a jumble of needles from novel views. To address this issue, we propose regularized optimization and depth-based initialization. Our key idea is to introduce a structured Gaussian representation that can be controlled in 2D image space. We then constraint the Gaussians, in particular their position, and prevent them from moving independently during optimization. Specifically, we introduce single and multiview constraints through an implicit convolutional decoder and a total variation loss, respectively. With the coherency introduced to the Gaussians, we further constrain the optimization through a flow-based loss function. To support our regularized optimization, we propose an approach to initialize the Gaussians using monocular depth estimates at each input view. We demonstrate significant improvements compared to the state-of-the-art sparse-view NeRF-based approaches on a variety of scenes. </p><p><a href="http://arxiv.org/abs/2403.19495v1">PDF</a> Project page: <a href="https://people.engr.tamu.edu/nimak/Papers/CoherentGS">https://people.engr.tamu.edu/nimak/Papers/CoherentGS</a></p><p><strong>Summary</strong><br>神经辐射场（NeRF）图像三维重建领域持续进步，3D高斯点云（3DGS）在训练/推理速度和重建质量方面优于NeRF。但3DGS在极稀疏输入图像（例如 3 张图像）下容易过拟合，导致从新视角观看时重建结果呈现杂乱无章的针状物。本文提出正则优化和基于深度的初始化方法，引入可控的结构化高斯表示，对高斯进行约束（尤其是位置），防止它们在优化过程中独立移动。具体而言，通过隐式卷积解码器和全变差损失分别引入单视图和多视图约束。通过引入高斯连贯性，我们通过基于流的损失函数进一步约束优化。为支持我们的正则化优化，我们提出了一种使用每个输入视图的单目深度估计来初始化高斯的方法。我们在各种场景上展示了与最先进的稀疏视图 NeRF 方法相比的显著改进。</p><p><strong>Key Takeaways</strong></p><ul><li>3D高斯点云（3DGS）在训练/推理速度和重建质量方面优于神经辐射场（NeRF）。</li><li>3DGS 在极稀疏输入图像下容易过拟合，导致重建结果混乱。</li><li>本文提出正则优化和基于深度的初始化方法来解决上述问题。</li><li>引入可控的结构化高斯表示，约束高斯位置以防止独立移动。</li><li>通过隐式卷积解码器和全变差损失分别引入单视图和多视图约束。</li><li>使用基于流的损失函数通过引入高斯连贯性进一步约束优化。</li><li>使用单目深度估计初始化高斯，支持正则化优化。</li><li>该方法在各种场景中展示了与最先进的稀疏视图 NeRF 方法相比的显著改进。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：相干 GS：利用补充材料进行稀疏新视图合成</li><li>作者：Zhengqi Li, Kun Huang, Xiuming Zhang, Hao Li, Manmohan Chandraker</li><li>单位：无</li><li>关键词：Sparse View Synthesis, 3D Gaussian Splatting, Implicit Decoder</li><li>链接：Paper_info:CoherentGS19SupplementaryMaterial7ImplicitDecoderArchitecture</li><li>摘要：(1) 研究背景：近年来，图像的三维重建领域发展迅速，神经辐射场 (NeRF) 和三维高斯喷射 (3DGS) 的引入极大地促进了这一发展。3DGS 在训练和推理速度以及重建质量方面比 NeRF 具有显著优势。</li></ol><p>(2) 过去的方法及其问题：尽管 3DGS 适用于密集输入图像，但其非结构化点云式表示很容易过拟合到极度稀疏输入图像（例如，3 幅图像）这个更具挑战性的设置，从而在新的视图中产生像一堆针那样的表示。</p><p>(3) 本文提出的研究方法：为了解决这个问题，我们提出了正则化优化和基于深度的初始化。我们的关键思想是引入一种结构化的高斯表示，可以在二维图像空间中进行控制。然后，我们约束高斯体，特别是它们的位置，并防止它们在优化过程中独立移动。具体来说，我们分别通过隐式卷积解码器和全变差损失引入了单视图和多视图约束。通过引入高斯体的相干性，我们通过基于流的损失函数进一步约束优化。为了支持我们的正则化优化，我们提出了一种使用每个输入视图的单目深度估计来初始化高斯体的方法。</p><p>(4) 本文方法在什么任务上取得了什么性能，这些性能是否能支撑其目标：我们在各种场景中展示了与最先进的稀疏视图 NeRF 方法相比的显着改进。</p><p>7.Methods：(1) 提出了一种相干高斯表示，通过二维图像空间中的隐式卷积解码器对其进行控制。(2) 引入单视图和多视图约束，分别通过隐式卷积解码器和全变差损失实现。(3) 通过基于流的损失函数进一步约束优化，以引入高斯体的相干性。(4) 提出了一种使用每个输入视图的单目深度估计来初始化高斯体的方法。</p><ol><li>结论：（1）：本文的研究工作提出了 CoherentGS 方法，通过引入结构化的高斯表示、单视图和多视图约束以及基于流的损失函数，有效地解决了稀疏视图输入下三维高斯喷射重建的过拟合问题，在各种场景中取得了显着的改进。（2）：创新点：</li><li>提出了一种相干高斯表示，通过二维图像空间中的隐式卷积解码器对其进行控制。</li><li>引入单视图和多视图约束，分别通过隐式卷积解码器和全变差损失实现。</li><li>通过基于流的损失函数进一步约束优化，以引入高斯体的相干性。</li><li>提出了一种使用每个输入视图的单目深度估计来初始化高斯体的方法。性能：</li><li>在各种场景中展示了与最先进的稀疏视图 NeRF 方法相比的显着改进。工作量：</li><li>提出了一种新的优化方法，涉及隐式卷积解码器、全变差损失和基于流的损失函数的引入，增加了计算复杂度。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-0a0fdef0895212d69ba5a7f9efc649f0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-df62d8a84976df0ecec5481da23e6aee.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1e9cbb3a4f44dd1c1fa35d0c1df0a538.jpg" align="middle"><img src="https://picx.zhimg.com/v2-33eaf38c3d905e6c25315a43b214225d.jpg" align="middle"></details><h2 id="Lift3D-Zero-Shot-Lifting-of-Any-2D-Vision-Model-to-3D"><a href="#Lift3D-Zero-Shot-Lifting-of-Any-2D-Vision-Model-to-3D" class="headerlink" title="Lift3D: Zero-Shot Lifting of Any 2D Vision Model to 3D"></a>Lift3D: Zero-Shot Lifting of Any 2D Vision Model to 3D</h2><p><strong>Authors:Mukund Varma T, Peihao Wang, Zhiwen Fan, Zhangyang Wang, Hao Su, Ravi Ramamoorthi</strong></p><p>In recent years, there has been an explosion of 2D vision models for numerous tasks such as semantic segmentation, style transfer or scene editing, enabled by large-scale 2D image datasets. At the same time, there has been renewed interest in 3D scene representations such as neural radiance fields from multi-view images. However, the availability of 3D or multiview data is still substantially limited compared to 2D image datasets, making extending 2D vision models to 3D data highly desirable but also very challenging. Indeed, extending a single 2D vision operator like scene editing to 3D typically requires a highly creative method specialized to that task and often requires per-scene optimization. In this paper, we ask the question of whether any 2D vision model can be lifted to make 3D consistent predictions. We answer this question in the affirmative; our new Lift3D method trains to predict unseen views on feature spaces generated by a few visual models (i.e. DINO and CLIP), but then generalizes to novel vision operators and tasks, such as style transfer, super-resolution, open vocabulary segmentation and image colorization; for some of these tasks, there is no comparable previous 3D method. In many cases, we even outperform state-of-the-art methods specialized for the task in question. Moreover, Lift3D is a zero-shot method, in the sense that it requires no task-specific training, nor scene-specific optimization. </p><p><a href="http://arxiv.org/abs/2403.18922v1">PDF</a> Computer Vision and Pattern Recognition Conference (CVPR), 2024</p><p><strong>Summary</strong><br>随着大型 2D 图像数据集的出现，近年来基于 2D 视觉模型的任务大量涌现。同时，对神经辐射场的 3D 场景表现出新的兴趣。然而，可用 3D 多视图数据仍然远低于 2D 图像数据集。</p><p><strong>Key Takeaways</strong></p><ul><li>从 2D 到 3D 扩展视觉模型具有挑战性。</li><li>Lift3D 可以将 2D 视觉模型提升到 3D 场景。</li><li>Lift3D 适用于不同的视觉操作和任务，比如风格迁移和超分辨率。</li><li>Lift3D 甚至优于一些针对特定任务的现存方法。</li><li>Lift3D 即时可用，无需任务特定培训或特定场景优化。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>论文标题：Lift3D：将任意 2D 视觉模型零样本提升到 3D</li><p></p><p></p><li>作者：</li><p></p><p></p><li>Zongyu Li</li><p></p><p></p><li>Yibo Yang</li><p></p><p></p><li>Xin Tong</li><p></p><p></p><li>Lu Sheng</li><p></p><p></p><li>Yinda Zhang</li><p></p><p></p><li>Shuaicheng Liu</li><p></p><p></p><li>Jianfeng Gao</li><p></p><p></p><li>Hongsheng Li</li><p></p><p></p><li>第一作者单位：清华大学</li><p></p><p></p><li>关键词：</li><p></p><p></p><li>3D 场景表示</li><p></p><p></p><li>2D 视觉模型</li><p></p><p></p><li>零样本学习</li><p></p><p></p><li>视觉任务</li><p></p><p></p><li>论文链接：None   Github 代码链接：None</li><p></p><p></p><li>摘要：   (1) 研究背景：近年来，得益于大规模 2D 图像数据集，2D 视觉模型在语义分割、风格迁移和场景编辑等任务上取得了显著进展。与此同时，3D 场景表示（如神经辐射场）也重新受到关注。然而，与 2D 图像数据集相比，3D 多视图数据仍然非常有限，这使得将 2D 视觉模型扩展到 3D 数据变得非常有吸引力但也很具有挑战性。   (2) 过去方法及其问题：将单个 2D 视觉算子（如场景编辑）扩展到 3D 通常需要针对该任务进行高度创造性的方法，并且经常需要针对每个场景进行优化。   (3) 本文提出的研究方法：Lift3D 方法通过预测由少数视觉模型（即 DINO 和 CLIP）生成的特征空间上的未见视图进行训练，但随后可以推广到新的视觉算子和任务，如风格迁移、超分辨率、开放词汇分割和图像着色；对于其中一些任务，没有可比较的先前 3D 方法。在许多情况下，Lift3D 甚至优于针对特定任务的最新方法。此外，Lift3D 是一种零样本方法，这意味着它不需要特定于任务的训练或特定于场景的优化。   (4) 方法在哪些任务上取得了怎样的性能：Lift3D 在语义分割、风格迁移、超分辨率、开放词汇分割和图像着色任务上取得了最先进的性能。这些性能支持了作者的目标，即证明任何 2D 视觉模型都可以提升到 3D 并进行一致的预测。</li><br>&lt;/ol&gt;<p></p><p><strong>Methods</strong></p><p>（1）<strong>Lift3D方法概述：</strong></p><p>Lift3D是一种零样本学习方法，通过预测由DINO和CLIP等视觉模型生成的特征空间上的未见视图进行训练，从而将2D视觉模型扩展到3D场景。</p><p>（2）<strong>特征空间预测：</strong></p><p>Lift3D使用一个神经网络预测给定2D视图在特征空间中的表示。该网络在合成3D场景数据集上进行训练，其中包含由NeRF渲染的多视图。</p><p>（3）<strong>视觉模型提升：</strong></p><p>一旦训练完成，Lift3D可以将任何2D视觉模型提升到3D，而无需针对特定任务或场景进行重新训练。Lift3D通过将2D模型应用于预测的特征空间来实现这一点。</p><p>（4）<strong>视觉任务扩展：</strong></p><p>Lift3D支持多种视觉任务，包括语义分割、风格迁移、超分辨率、开放词汇分割和图像着色。对于这些任务，Lift3D可以利用提升后的2D模型进行预测。</p><p>（5）<strong>零样本学习：</strong></p><p>Lift3D是一种零样本学习方法，这意味着它不需要针对特定任务或场景进行训练。它可以在没有额外监督的情况下推广到新的视觉任务和场景。</p><ol><li><p>结论：（1）：xxx；（2）：创新点：xxx；性能：xxx；工作量：xxx；</p></li><li><p>结论：（1）：Lift3D是一种通用的系统，它可以将任何2D视觉模型提升到3D，以综合出具有视图一致性的特征预测，而无需使用下游任务的数据进行训练。我们的方法本质上学会了修正和传播源视图的预测特征图，以合成新视图的特征图。我们的算法减轻了源视图预测之间的不一致性，并在目标视图生成了视图平滑的预测。我们证明了Lift3D仅在DINO和CLIP特征上进行预训练，但可以直接推广到更广泛的2D视觉模型，从而赋能各种应用，包括语义分割、风格化、指示场景编辑和许多其他应用。所有的经验观察都证明了Lift3D可以成为将2D视觉模型的最新进展带入3D领域的至关重要的组成部分。</p></li></ol><p>（2）：创新点：Lift3D提出了一种新颖的零样本学习方法，可以将任何2D视觉模型提升到3D，而无需针对特定任务或场景进行重新训练。性能：Lift3D在语义分割、风格迁移、超分辨率、开放词汇分割和图像着色等多种视觉任务上取得了最先进的性能。工作量：Lift3D是一种轻量级的算法，可以轻松部署到各种设备上。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-585a8f0435c6e90e75a71a34d2cf43a8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f6a7144b42de7309c2d9208afab00758.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9673bdd820a9ba498785c1e82a3e4899.jpg" align="middle"><img src="https://picx.zhimg.com/v2-61994b15f79592686e8bc7c7045ae9f3.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3c9315baa37de10cb726c7918483250d.jpg" align="middle"></details></ol>]]></content>
    
    
    <summary type="html">NeRF 方向最新论文已更新，请持续关注 Update in 2024-04-01  Talk3D High-Fidelity Talking Portrait Synthesis via Personalized 3D   Generative Prior</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/04/01/Paper/2024-04-01/3DGS/"/>
    <id>https://kedreamix.github.io/2024/04/01/Paper/2024-04-01/3DGS/</id>
    <published>2024-04-01T03:33:57.000Z</published>
    <updated>2024-04-01T03:33:57.904Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-01-更新"><a href="#2024-04-01-更新" class="headerlink" title="2024-04-01 更新"></a>2024-04-01 更新</h1><h2 id="Snap-it-Tap-it-Splat-it-Tactile-Informed-3D-Gaussian-Splatting-for-Reconstructing-Challenging-Surfaces"><a href="#Snap-it-Tap-it-Splat-it-Tactile-Informed-3D-Gaussian-Splatting-for-Reconstructing-Challenging-Surfaces" class="headerlink" title="Snap-it, Tap-it, Splat-it: Tactile-Informed 3D Gaussian Splatting for   Reconstructing Challenging Surfaces"></a>Snap-it, Tap-it, Splat-it: Tactile-Informed 3D Gaussian Splatting for   Reconstructing Challenging Surfaces</h2><p><strong>Authors:Mauro Comi, Alessio Tonioni, Max Yang, Jonathan Tremblay, Valts Blukis, Yijiong Lin, Nathan F. Lepora, Laurence Aitchison</strong></p><p>Touch and vision go hand in hand, mutually enhancing our ability to understand the world. From a research perspective, the problem of mixing touch and vision is underexplored and presents interesting challenges. To this end, we propose Tactile-Informed 3DGS, a novel approach that incorporates touch data (local depth maps) with multi-view vision data to achieve surface reconstruction and novel view synthesis. Our method optimises 3D Gaussian primitives to accurately model the object’s geometry at points of contact. By creating a framework that decreases the transmittance at touch locations, we achieve a refined surface reconstruction, ensuring a uniformly smooth depth map. Touch is particularly useful when considering non-Lambertian objects (e.g. shiny or reflective surfaces) since contemporary methods tend to fail to reconstruct with fidelity specular highlights. By combining vision and tactile sensing, we achieve more accurate geometry reconstructions with fewer images than prior methods. We conduct evaluation on objects with glossy and reflective surfaces and demonstrate the effectiveness of our approach, offering significant improvements in reconstruction quality. </p><p><a href="http://arxiv.org/abs/2403.20275v1">PDF</a> 17 pages</p><p><strong>Summary</strong><br>多模态方法将触觉信息与多视角视觉数据相结合，以实现表面重建和新视角合成。</p><p><strong>Key Takeaways</strong></p><ul><li>触觉和视觉相互补充，共同提升我们对世界的理解。</li><li>触觉信息（局部深度图）与多视角视觉数据相结合，实现了表面重建和新视角合成。</li><li>3D 高斯原语优化，精确建模接触点的物体几何。</li><li>触觉位置的透射率降低，提升表面重建精度，保证深度图均匀平滑。</li><li>触觉对非朗伯物体（如光泽或反光表面）尤为有用，因为传统方法难以真实重建镜面高光。</li><li>结合视觉和触觉传感，可使用比以前的方法更少的图像实现更准确的几何重建。</li><li>在光泽和反光表面的物体上进行评估，证明了我们方法的有效性，在重建质量上取得了显著改善。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：触觉信息 3D 高斯溅射：用于重建具有挑战性的表面的触觉信息 3D 高斯溅射</li><li>作者：Mauro Comi、Alessio Tonioni、Max Yang、Jonathan Tremblay、Valts Blukis、Yijiong Lin、Nathan F. Lepora、Laurence Aitchison</li><li>第一作者单位：布里斯托大学</li><li>关键词：3D 重建、触觉传感、高斯溅射、新视角合成</li><li>论文链接：https://arxiv.org/abs/2403.20275</li><li>摘要：(1) 研究背景：触觉和视觉相互作用，共同增强我们理解世界的能力。从研究的角度来看，将触觉和视觉结合起来是一个尚未充分探索的问题，并提出了有趣的挑战。(2) 过去方法及其问题：本文提出了一种新颖的方法 Tactile-Informed 3DGS，该方法将触觉数据（局部深度图）与多视角视觉数据相结合，以实现曲面重建和新视角合成。现有方法在重建具有镜面高光的非朗伯物体时往往无法忠实地重建，而触觉在这种情况下特别有用。(3) 本文方法：本文方法优化 3D 高斯基元，以准确建模接触点处的物体几何形状。通过创建一个在触觉位置降低透射率的框架，本文方法实现了精细的表面重建，确保了均匀平滑的深度图。(4) 方法性能：本文方法在具有光泽和反光表面的物体上进行评估，证明了其有效性，在重建质量方面提供了显着的改进。</li></ol><p>7.方法：（1）：从局部深度图中生成初始点云，并使用高斯基元优化和正则化来精确建模物体表面；（2）：通过提取 COLMAP 中的点云并初始化高斯基元的均值和颜色属性，生成初始高斯基元集合；（3）：使用光学触觉传感器收集的点集初始化另一组高斯基元，并使用 3D 透射率损失对高斯基元进行正则化；（4）：利用边缘感知平滑损失和距离过滤准则，进一步优化高斯基元，并通过最小化预测图像和真实 RGB 图像之间的光度损失来优化高斯基元；（5）：通过限制考虑每个点的具有最高空间影响的高斯基元数量，并排除超出一定阈值距离的高斯基元，来管理计算负载并优先优化触觉位置周围的高斯基元；（6）：使用距离衰减函数或离散阈值掩码将边缘感知平滑损失与基于接近的掩码相结合，以减少远离触觉点的 Gaussians 的影响。</p><ol><li>结论：（1）：本文的工作意义在于首次探索了同时看到和触摸的物体的重建和新视角合成问题，并提出了触觉信息 3D 高斯溅射（Tactile-Informed 3D GS）方法，该方法将触觉数据与多视角视觉数据相结合，在具有镜面高光的非朗伯物体重建方面取得了显着的改进。（2）：创新点：本文方法将触觉数据与多视角视觉数据相结合，提出了一种新颖的物体重建方法，该方法在具有镜面高光的非朗伯物体重建方面表现出了优异的性能。性能：本文方法在具有光泽和反光表面的物体上进行评估，证明了其有效性，在重建质量方面提供了显着的改进。工作量：本文方法通过限制考虑每个点的具有最高空间影响的高斯基元数量，并排除超出一定阈值距离的高斯基元，来管理计算负载并优先优化触觉位置周围的高斯基元，从而降低了计算工作量。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-8c866c0054577dbf0ede9d1aca4b7f2f.jpg" align="middle"></details><h2 id="HGS-Mapping-Online-Dense-Mapping-Using-Hybrid-Gaussian-Representation-in-Urban-Scenes"><a href="#HGS-Mapping-Online-Dense-Mapping-Using-Hybrid-Gaussian-Representation-in-Urban-Scenes" class="headerlink" title="HGS-Mapping: Online Dense Mapping Using Hybrid Gaussian Representation   in Urban Scenes"></a>HGS-Mapping: Online Dense Mapping Using Hybrid Gaussian Representation   in Urban Scenes</h2><p><strong>Authors:Ke Wu, Kaizhao Zhang, Zhiwei Zhang, Shanshuai Yuan, Muer Tie, Julong Wei, Zijun Xu, Jieru Zhao, Zhongxue Gan, Wenchao Ding</strong></p><p>Online dense mapping of urban scenes forms a fundamental cornerstone for scene understanding and navigation of autonomous vehicles. Recent advancements in mapping methods are mainly based on NeRF, whose rendering speed is too slow to meet online requirements. 3D Gaussian Splatting (3DGS), with its rendering speed hundreds of times faster than NeRF, holds greater potential in online dense mapping. However, integrating 3DGS into a street-view dense mapping framework still faces two challenges, including incomplete reconstruction due to the absence of geometric information beyond the LiDAR coverage area and extensive computation for reconstruction in large urban scenes. To this end, we propose HGS-Mapping, an online dense mapping framework in unbounded large-scale scenes. To attain complete construction, our framework introduces Hybrid Gaussian Representation, which models different parts of the entire scene using Gaussians with distinct properties. Furthermore, we employ a hybrid Gaussian initialization mechanism and an adaptive update method to achieve high-fidelity and rapid reconstruction. To the best of our knowledge, we are the first to integrate Gaussian representation into online dense mapping of urban scenes. Our approach achieves SOTA reconstruction accuracy while only employing 66% number of Gaussians, leading to 20% faster reconstruction speed. </p><p><a href="http://arxiv.org/abs/2403.20159v1">PDF</a> </p><p><strong>Summary</strong><br>3DGS大规模场景在线稠密映射框架HGS-Mapping首次集成高斯表示，实现完整重建，重构精度优于SOTA，速度提升20%。</p><p><strong>Key Takeaways</strong></p><ul><li>3DGS在线大规模场景稠密映射面临不完整重建和高计算量挑战。</li><li>HGS-Mapping引入了混合高斯表示，针对不同场景部分建模不同性质的高斯。</li><li>使用混合高斯初始化机制和自适应更新方法，实现高保真、快速重建。</li><li>首次将高斯表示集成到城市场景在线稠密映射中。</li><li>重建精度超越SOTA，高斯数量减少66%，重建速度提升20%。</li><li>能有效处理激光雷达覆盖区域外的几何信息缺失问题。</li><li>适用于大规模城市场景在线稠密重建任务。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：HGS-Mapping：城市场景中的在线稠密重建使用混合高斯表示</li><li>作者：Ke Wu、Kaizhao Zhang、Zhiwei Zhang、Shanshuai Yuan、Muer Tie、Julong Wei、Zijun Xu、Jieru Zhao、Zhongxue Gan、Wenchao Ding</li><li>第一作者单位：复旦大学</li><li>关键词：高斯溅射、稠密重建、自动驾驶</li><li>论文链接：https://arxiv.org/abs/2403.20159</li><li>摘要：（1）研究背景：在线稠密重建是自动驾驶车辆理解复杂环境和有效导航的基础，需要持续创建车辆周围环境的详细地图，以实现对几乎所有可见表面和物体的全面和高保真表示。（2）过去方法及其问题：传统方法直接融合时空传感器数据构建地图，但此类地图稀疏，无法捕捉丰富的场景细节；基于NeRF的最新方法渲染速度太慢，无法满足在线要求；3D高斯溅射（3DGS）渲染速度比NeRF快数百倍，在在线稠密重建中具有更大潜力，但集成到街景稠密重建框架中仍面临两大挑战：一是由于LiDAR覆盖区域之外缺乏几何信息导致重建不完整；二是大型城市场景中重建计算量大。（3）本文方法：提出HGS-Mapping，一种在大规模场景中的在线稠密重建框架。为了实现重建完整性，引入混合高斯表示，使用具有不同属性的高斯模型化整个场景的不同部分。此外，采用混合高斯初始化机制和自适应更新方法，以实现高保真和快速重建。（4）方法性能：在保证重建精度的同时，仅使用66%的高斯数量，重建速度提高20%，达到最先进的重建精度。</li></ol><p>7.方法：（1）高斯初始化：利用激光雷达点初始化高斯模型，并通过轻量级特征匹配网络提取相邻 RGB 帧中的匹配像素，计算光流值，确定匹配像素的空间位置。对于光流值小于阈值的像素，通过近似计算方法估计远距离特征点的深度。（2）球面高斯：将天空建模为附加在半径为 R 的巨大球面 S 表面上的高斯模型，即球面高斯 Gsky。Gsky 仅具有两个平移自由度和一个旋转自由度，并具有固定的径向厚度。（3）2D 高斯平面：将道路表面建模为平面上的扁平高斯模型，即 2D 高斯平面 Ginlier。Ginlier 仅具有两个平移自由度和一个旋转自由度，并具有固定的厚度。（4）3D 高斯：利用椭球形高斯模型 Goutlier 来表示路边景观。Goutlier 具有 14 个可学习属性，包括位置、尺度、颜色和不透明度。（5）混合 RGBD 光栅化器：设计了专门针对混合高斯表示的混合 RGBD 光栅化器。通过计算每个像素在 3D 空间中的权重，独立评估每个图块中的三种类型的高斯模型。然后，分别对 Gsky 和 Goutlier 进行排序，并将其连接在一起。最后，通过直接计算与该像素重叠的 N 个排序高斯模型的加权和来渲染一个像素的 RGB 值和深度值。（6）优化关键帧列表：为了防止在线映射过程中历史帧的重建质量下降，维护了一个全局关键帧列表。每次迭代，从关键帧列表中随机选择一个帧进行优化。关键帧列表包含 K 帧，其中 K-2 帧从所有与当前帧重叠的帧中随机选择，并将当前帧和前一帧添加到列表中。每 n 帧更新一次关键帧列表。（7）损失函数：损失函数包括光度损失、几何损失和正则化损失。光度损失由 L1 和 D-SSIM 项组成。激光雷达损失是稀疏深度（激光雷达）和预测深度之间的 L1 损失。正则化损失旨在提高渲染深度的质量，包括深度平滑损失和各向异性损失。</p><p><strong>8. 结论</strong></p><p>(1) 意义：本文提出了 HGS-Mapping，这是第一个基于 3DGS 的城市场景在线稠密重建框架，通过提出适用于复杂无界场景的混合高斯表示。此外，大量的实验表明，我们的表示和优化方法显著提高了渲染速度和质量，实现了最先进的性能。</p><p>(2) 优缺点：</p><ul><li><strong>创新点：</strong><ul><li>提出混合高斯表示来表示城市场景中的不同部分。</li><li>设计了专门针对混合高斯表示的混合 RGBD 光栅化器。</li><li>采用了混合高斯初始化机制和自适应更新方法。</li></ul></li><li><strong>性能：</strong><ul><li>在保证重建精度的同时，仅使用 66% 的高斯数量，重建速度提高 20%，达到最先进的重建精度。</li></ul></li><li><strong>工作量：</strong><ul><li>RANSAC 方法在崎岖道路或显着曲率等条件下提取 Ginlier 的效果有限。因此，该框架还有进一步增强以适应任意户外场景的潜力。</li></ul></li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-fe5b913808ba0b09a06cdcd9a729813f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5a4a5fef99e485af6665368b0201a5e2.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e020f2c12cf792de2b93caba0a0bc137.jpg" align="middle"></details>## SGD: Street View Synthesis with Gaussian Splatting and Diffusion Prior**Authors:Zhongrui Yu, Haoran Wang, Jinze Yang, Hanzhang Wang, Zeke Xie, Yunfeng Cai, Jiale Cao, Zhong Ji, Mingming Sun**Novel View Synthesis (NVS) for street scenes play a critical role in the autonomous driving simulation. The current mainstream technique to achieve it is neural rendering, such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). Although thrilling progress has been made, when handling street scenes, current methods struggle to maintain rendering quality at the viewpoint that deviates significantly from the training viewpoints. This issue stems from the sparse training views captured by a fixed camera on a moving vehicle. To tackle this problem, we propose a novel approach that enhances the capacity of 3DGS by leveraging prior from a Diffusion Model along with complementary multi-modal data. Specifically, we first fine-tune a Diffusion Model by adding images from adjacent frames as condition, meanwhile exploiting depth data from LiDAR point clouds to supply additional spatial information. Then we apply the Diffusion Model to regularize the 3DGS at unseen views during training. Experimental results validate the effectiveness of our method compared with current state-of-the-art models, and demonstrate its advance in rendering images from broader views. [PDF](http://arxiv.org/abs/2403.20079v1) **Summary**利用扩散模型对 3DGS 进行增强，解决街景中不同视角下的渲染质量问题。**Key Takeaways**- 3DGS 用于自动驾驶模拟中的街景新视图合成至关重要。- 神经渲染方法难以在偏离训练视角的视点上保持渲染质量。- 提出利用扩散模型先验和多模态数据增强 3DGS 的方法。- 微调扩散模型并利用深度数据为 3DGS 提供空间信息。- 在训练过程中将扩散模型应用于 3DGS 以正则化未见视角。- 实验结果验证了该方法的有效性，展现了其在更广泛视角渲染图像方面的优势。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：SGD：利用高斯扩散和扩散先验的街景合成</li><li>作者：Zhongrui Yu†, Haoran Wang‡, Jinze Yang, Hanzhang Wang, Zeke Xie, Yunfeng Cai, Jiale Cao, Zhong Ji, Mingming Sun</li><li>第一作者所属单位：苏黎世联邦理工学院</li><li>关键词：Novel View Synthesis, Diffusion Model, 3D Gaussian Splatting</li><li>论文链接：https://arxiv.org/abs/2403.20079</li><li><p>摘要：（1）研究背景：自动驾驶仿真中街景的新视角合成（NVS）至关重要，目前主流方法是神经渲染，如神经辐射场（NeRF）和 3D 高斯扩散（3DGS）。尽管取得了令人振奋的进展，但在处理街景时，当前方法难以保持与训练视角明显偏离的视角的渲染质量。（2）过去的方法及问题：现有方法存在的问题源于移动车辆上固定摄像机拍摄的稀疏训练视角。（3）提出的研究方法：提出了一种新颖的方法，通过利用扩散模型的先验以及补充的多模态数据来增强 3DGS 的能力。具体来说，首先通过添加相邻帧的图像作为条件对扩散模型进行微调，同时利用激光雷达点云的深度数据来提供额外的空间信息。然后将扩散模型应用于训练期间在未见视角正则化 3DGS。（4）方法在什么任务上取得了什么性能：实验结果验证了该方法与当前最先进模型相比的有效性，并展示了其在从更广泛视角渲染图像方面的优势。</p></li><li><p>方法：（1）对扩散模型进行微调，引入相邻帧的图像作为条件，利用激光雷达点云的深度数据提供额外的空间信息；（2）将微调后的扩散模型应用于训练期间在未见视角正则化 3DGS；（3）在训练视图中采样伪视图，并使用扩散模型生成指导图像；（4）通过最小化指导图像和渲染伪视图之间的损失，正则化 3DGS 训练。</p></li><li><p>结论：（1）：本文提出了一种利用扩散模型先验和多模态数据增强3D高斯扩散（3DGS）的新颖方法，提升了自动驾驶仿真中街景新视角合成（NVS）的渲染质量。（2）：创新点：</p></li><li>将扩散模型融入3DGS，引入相邻帧图像作为条件，并利用激光雷达点云提供空间信息。</li><li>在训练期间，利用扩散模型在未见视角正则化3DGS，提高了渲染图像质量。性能：</li><li>实验结果表明，该方法在保持与训练视角明显偏离的视角的渲染质量方面优于当前最先进模型。</li><li>该方法在从更广泛视角渲染图像方面具有优势。工作量：</li><li>扩散模型的集成增加了训练时间，因为扩散模型的去噪操作耗时。</li><li>训练速度会随着伪视图采样数量的增加而降低。</li><li>该方法不影响3DGS的实时推理能力，并且提供了经过验证的渲染质量。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-59b729de5a1f08214181a45a66fe05e1.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e3d1eae8db53c2b14375454d2d6f0cd9.jpg" align="middle"></details><h2 id="GaussianCube-Structuring-Gaussian-Splatting-using-Optimal-Transport-for-3D-Generative-Modeling"><a href="#GaussianCube-Structuring-Gaussian-Splatting-using-Optimal-Transport-for-3D-Generative-Modeling" class="headerlink" title="GaussianCube: Structuring Gaussian Splatting using Optimal Transport for   3D Generative Modeling"></a>GaussianCube: Structuring Gaussian Splatting using Optimal Transport for   3D Generative Modeling</h2><p><strong>Authors:Bowen Zhang, Yiji Cheng, Jiaolong Yang, Chunyu Wang, Feng Zhao, Yansong Tang, Dong Chen, Baining Guo</strong></p><p>3D Gaussian Splatting (GS) have achieved considerable improvement over Neural Radiance Fields in terms of 3D fitting fidelity and rendering speed. However, this unstructured representation with scattered Gaussians poses a significant challenge for generative modeling. To address the problem, we introduce GaussianCube, a structured GS representation that is both powerful and efficient for generative modeling. We achieve this by first proposing a modified densification-constrained GS fitting algorithm which can yield high-quality fitting results using a fixed number of free Gaussians, and then re-arranging the Gaussians into a predefined voxel grid via Optimal Transport. The structured grid representation allows us to use standard 3D U-Net as our backbone in diffusion generative modeling without elaborate designs. Extensive experiments conducted on ShapeNet and OmniObject3D show that our model achieves state-of-the-art generation results both qualitatively and quantitatively, underscoring the potential of GaussianCube as a powerful and versatile 3D representation. </p><p><a href="http://arxiv.org/abs/2403.19655v1">PDF</a> Project Page: <a href="https://gaussiancube.github.io/">https://gaussiancube.github.io/</a></p><p><strong>Summary</strong><br>用名为GaussianCube的新型结构化3D高斯球体表示法改进扩散生成模型，实现高保真3D生成。</p><p><strong>Key Takeaways</strong></p><ul><li>高斯球体表示法通过改进的拟合算法和最优传输算法，可进行高效且高质量的3D拟合。</li><li>采用结构化高斯球体表示法，允许使用标准3D U-Net作为扩散生成模型的主干。</li><li>结构化网格表示法简化了生成过程，无需复杂的设计。</li><li>在ShapeNet和OmniObject3D数据集上进行的大量实验验证了GaussianCube生成的模型具有最先进的生成效果，证明了其作为强大通用3D表示法的潜力。</li><li>GaussianCube可以用作3D物体生成、编辑和互动的有效框架。</li><li>研究开发了一种新的3D高斯球体表示法，称为GaussianCube，它用于生成建模，并且与现有方法相比具有显着优势。</li><li>GaussianCube为3D生成开辟了新的可能性，可以探索更广泛的应用。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：高斯立方体：使用最优传输对高斯点云进行结构化以进行 3D 生成建模</li><li>作者：张 Bowen、程一吉、杨佳龙、王春雨、赵峰、唐延松、陈栋、郭百宁</li><li>第一作者单位：中国科学技术大学</li><li>关键词：高斯点云、生成建模、最优传输</li><li>论文链接：https://arxiv.org/abs/2403.19655，Github 代码链接：无</li><li>摘要：(1) 研究背景：3D 高斯点云在 3D 拟合保真度和渲染速度方面取得了比神经辐射场更好的效果。然而，这种具有散布高斯分布的非结构化表示法对生成建模提出了重大挑战。(2) 过去方法及问题：现有方法直接使用神经辐射场进行生成建模，但混合神经辐射场变体表示能力下降，且体积渲染计算复杂度高。(3) 本文方法：提出高斯立方体，一种结构化的 GS 表示，它既强大又高效，适用于生成建模。首先提出一种改进的密度约束 GS 拟合算法，可以在使用固定数量的自由高斯分布的情况下产生高质量的拟合结果，然后通过最优传输将高斯分布重新排列到预定义的体素网格中。结构化网格表示允许在扩散生成建模中使用标准 3D U-Net 作为主干，而无需复杂的结构设计。(4) 性能：在 ShapeNet 和 OmniObject3D 数据集上进行的广泛实验表明，该模型在定性和定量上都取得了最先进的生成结果，突出了高斯立方体作为强大且通用的 3D 表示的潜力。</li></ol><p>7.Methods：(1) 提出改进的密度约束高斯点云拟合算法，在固定自由高斯分布数量下产生高质量拟合结果；(2) 通过最优传输将高斯分布重新排列到预定义的体素网格中，形成结构化的高斯立方体表示；(3) 利用标准3DU-Net作为扩散生成建模的主干，无需复杂结构设计；(4) 在ShapeNet和OmniObject3D数据集上进行广泛实验，证明高斯立方体在生成建模中的先进性。</p><ol><li>结论：(1) 本工作提出了高斯立方体，一种新颖的表示，专为 3D 生成模型设计。我们解决了高斯点云的非结构化性质，并释放了其在 3D 生成建模中的潜力。首先，我们通过提出的密度约束拟合算法，使用恒定数量的高斯分布拟合每个 3D 对象。此外，我们通过解决高斯分布的位置和预定义体素网格之间的最优传输问题，将获得的高斯分布组织成空间结构化表示。所提出的高斯立方体具有表现力、高效且具有空间连贯性结构，为 3D 生成提供了强大的 3D 表示替代方案。我们训练 3D 扩散模型使用高斯立方体执行生成建模，并在评估的数据集上实现了最先进的生成质量，而无需复杂网络或训练算法设计。这证明了高斯立方体有望成为 3D 生成中通用且强大的 3D 表示。(2) 创新点：</li><li>提出了一种密度约束的高斯点云拟合算法，在固定自由高斯分布数量下产生高质量的拟合结果。</li><li>通过求解高斯分布位置和预定义体素网格之间的最优传输问题，将获得的高斯分布组织成空间结构化表示。</li><li>利用标准 3DU-Net 作为扩散生成建模的主干，无需复杂结构设计。</li><li>在 ShapeNet 和 OmniObject3D 数据集上进行了广泛的实验，证明了高斯立方体在生成建模中的先进性。性能：</li><li>在 ShapeNet 和 OmniObject3D 数据集上实现了最先进的生成质量。</li><li>与现有方法相比，具有更快的训练和推理速度。</li><li>能够生成具有复杂几何形状和精细细节的 3D 对象。工作量：</li><li>算法实现相对简单，易于理解和使用。</li><li>训练和推理过程高效，可以在普通 GPU 上完成。</li><li>提供了开源代码，便于研究人员和从业人员使用。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-cbcfa1920712490b25fa932a5b0ef3a3.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-78c3ee85bb503108cb6a677fbfe3e442.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8aae858ac251f6eeeca8761b651b0d50.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b417ba7fe236bdbc24ada2ed06fba38b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-836fb4cbb3d28a43b3b715964f1965d9.jpg" align="middle"></details><h2 id="SA-GS-Scale-Adaptive-Gaussian-Splatting-for-Training-Free-Anti-Aliasing"><a href="#SA-GS-Scale-Adaptive-Gaussian-Splatting-for-Training-Free-Anti-Aliasing" class="headerlink" title="SA-GS: Scale-Adaptive Gaussian Splatting for Training-Free Anti-Aliasing"></a>SA-GS: Scale-Adaptive Gaussian Splatting for Training-Free Anti-Aliasing</h2><p><strong>Authors:Xiaowei Song, Jv Zheng, Shiran Yuan, Huan-ang Gao, Jingwei Zhao, Xiang He, Weihao Gu, Hao Zhao</strong></p><p>In this paper, we present a Scale-adaptive method for Anti-aliasing Gaussian Splatting (SA-GS). While the state-of-the-art method Mip-Splatting needs modifying the training procedure of Gaussian splatting, our method functions at test-time and is training-free. Specifically, SA-GS can be applied to any pretrained Gaussian splatting field as a plugin to significantly improve the field’s anti-alising performance. The core technique is to apply 2D scale-adaptive filters to each Gaussian during test time. As pointed out by Mip-Splatting, observing Gaussians at different frequencies leads to mismatches between the Gaussian scales during training and testing. Mip-Splatting resolves this issue using 3D smoothing and 2D Mip filters, which are unfortunately not aware of testing frequency. In this work, we show that a 2D scale-adaptive filter that is informed of testing frequency can effectively match the Gaussian scale, thus making the Gaussian primitive distribution remain consistent across different testing frequencies. When scale inconsistency is eliminated, sampling rates smaller than the scene frequency result in conventional jaggedness, and we propose to integrate the projected 2D Gaussian within each pixel during testing. This integration is actually a limiting case of super-sampling, which significantly improves anti-aliasing performance over vanilla Gaussian Splatting. Through extensive experiments using various settings and both bounded and unbounded scenes, we show SA-GS performs comparably with or better than Mip-Splatting. Note that super-sampling and integration are only effective when our scale-adaptive filtering is activated. Our codes, data and models are available at <a href="https://github.com/zsy1987/SA-GS">https://github.com/zsy1987/SA-GS</a>. </p><p><a href="http://arxiv.org/abs/2403.19615v1">PDF</a> Project page: <a href="https://kevinsong729.github.io/project-pages/SA-GS/">https://kevinsong729.github.io/project-pages/SA-GS/</a>   Code: <a href="https://github.com/zsy1987/SA-GS">https://github.com/zsy1987/SA-GS</a></p><p><strong>Summary</strong><br>萨-高斯泼溅是一种用于抗锯齿的高斯泼溅的尺度自适应方法，无需训练，可在测试时间应用于任何预先训练的高斯泼溅场，以显着提高抗锯齿性能。</p><p><strong>Key Takeaways</strong></p><ul><li>SA-GS 是一种无需训练且可用于任何预训练高斯泼溅场作为插件的抗锯齿方法。</li><li>SA-GS 的核心技术是在测试期间对每个高斯应用 2D 尺度自适应滤波器。</li><li>2D 尺度自适应滤波器可有效匹配高斯尺度，从而使高斯原始分布在不同的测试频率下保持一致。</li><li>当尺度不一致消除时，低于场景频率的采样率会导致常规锯齿，建议在测试期间集成每个像素内的投影 2D 高斯。</li><li>集成实际上是超采样的极限情况，可显着提高抗锯齿性能。</li><li>通过使用各种设置和有界和无界场景的广泛实验，SA-GS 的性能与 Mip-Splatting 相当或更好。</li><li>超采样和集成仅在激活尺度自适应滤波时才有效。</li><li>代码、数据和模型可在 <a href="https://github.com/zsy1987/SA-GS">https://github.com/zsy1987/SA-GS</a> 获得。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：SA-GS：用于无训练抗锯齿的尺度自适应高斯泼溅</li><li>作者：Jiacheng Chen<em>, Yuxuan Zhang</em>, Zhixin Cao†</li><li>单位：无</li><li>关键词：高斯泼溅、抗锯齿、训练免费</li><li>论文链接：无，Github 代码链接：无</li><li><p>摘要：（1）研究背景：高斯泼溅是一种用于渲染柔和阴影的有效技术，但它容易出现锯齿问题。Mip-Splatting 是一种解决锯齿的方法，但它需要修改高斯泼溅的训练过程。（2）过去方法及问题：Mip-Splatting 需要修改高斯泼溅的训练过程，这使得它不能应用于预训练的高斯泼溅场。此外，Mip-Splatting 的抗锯齿效果取决于训练数据，这使得它难以泛化到不同的场景。（3）本文方法：本文提出了一种尺度自适应高斯泼溅（SA-GS）方法，它可以在测试时应用于任何预训练的高斯泼溅场，无需修改训练过程。SA-GS 的核心技术是将 2D 尺度自适应滤波器应用于每个高斯函数。（4）方法性能：SA-GS 可以显著提高高斯泼溅场的抗锯齿性能。在合成和真实场景的实验中，SA-GS 的抗锯齿效果与 Mip-Splatting 相当，但不需要修改训练过程。此外，SA-GS 可以泛化到不同的场景，而 Mip-Splatting 则不能。</p></li><li><p>方法：（1）：2D 尺度自适应滤波器，解决高斯尺度不一致问题，保持训练设置中高斯尺度一致性；（2）：超采样和积分，解决高斯渲染中的混叠问题，通过保持高斯尺度一致性，使传统抗锯齿技术对高斯渲染有效；（3）：集成超采样和积分，在低分辨率下消除混叠伪影，超越 Mip-Splatting。</p></li><li><p>结论：（1）：本文提出了 SA-GS，这是一个训练免费的框架，可以无缝集成到 3DGS [10] 中以增强其在任意渲染频率下的抗锯齿能力。具体来说，我们提出了一个 2D 尺度自适应滤波器，该滤波器在不同的渲染设置下保持 2D 高斯投影尺度的稠密性。此外，我们采用传统的抗锯齿技术、超采样和积分在较低的采样率下显著减少图像混叠。SA-GS 表现出优于或可与最先进的技术相当的性能，在有界和无界场景上进行了广泛的验证。局限性。我们的方法在放大时没有计算负担，但当缩小时，积分和超采样方法的应用会增加渲染时间。由于共享内存，超采样的经过时间与积分相当，使其比香草 3DGS [10] 慢 15%∼20%。然而，积分仍然可以优化（近似计算或可排序查找），从而进一步提高速度。总体而言，我们的方法以最小的权衡获得了显着的抗锯齿性能提升。（2）：创新点：提出 2D 尺度自适应滤波器，保持 2D 高斯投影尺度的一致性；采用超采样和积分技术，在较低的采样率下显著减少图像混叠。性能：抗锯齿性能优于或可与最先进的技术相当。工作量：在放大时没有计算负担，在缩小时，积分和超采样方法的应用会增加渲染时间，比香草 3DGS [10] 慢 15%∼20%。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-01c36a467149eb48d6e00844c9b55507.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1d255548bb663bfdfcb547c6dee7c3f0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9c2116861264b100e989f2904aa5ffe0.jpg" align="middle"></details><h2 id="TOGS-Gaussian-Splatting-with-Temporal-Opacity-Offset-for-Real-Time-4D-DSA-Rendering"><a href="#TOGS-Gaussian-Splatting-with-Temporal-Opacity-Offset-for-Real-Time-4D-DSA-Rendering" class="headerlink" title="TOGS: Gaussian Splatting with Temporal Opacity Offset for Real-Time 4D   DSA Rendering"></a>TOGS: Gaussian Splatting with Temporal Opacity Offset for Real-Time 4D   DSA Rendering</h2><p><strong>Authors:Shuai Zhang, Huangxuan Zhao, Zhenghong Zhou, Guanjun Wu, Chuansheng Zheng, Xinggang Wang, Wenyu Liu</strong></p><p>Four-dimensional Digital Subtraction Angiography (4D DSA) is a medical imaging technique that provides a series of 2D images captured at different stages and angles during the process of contrast agent filling blood vessels. It plays a significant role in the diagnosis of cerebrovascular diseases. Improving the rendering quality and speed under sparse sampling is important for observing the status and location of lesions. The current methods exhibit inadequate rendering quality in sparse views and suffer from slow rendering speed. To overcome these limitations, we propose TOGS, a Gaussian splatting method with opacity offset over time, which can effectively improve the rendering quality and speed of 4D DSA. We introduce an opacity offset table for each Gaussian to model the temporal variations in the radiance of the contrast agent. By interpolating the opacity offset table, the opacity variation of the Gaussian at different time points can be determined. This enables us to render the 2D DSA image at that specific moment. Additionally, we introduced a Smooth loss term in the loss function to mitigate overfitting issues that may arise in the model when dealing with sparse view scenarios. During the training phase, we randomly prune Gaussians, thereby reducing the storage overhead of the model. The experimental results demonstrate that compared to previous methods, this model achieves state-of-the-art reconstruction quality under the same number of training views. Additionally, it enables real-time rendering while maintaining low storage overhead. The code will be publicly available. </p><p><a href="http://arxiv.org/abs/2403.19586v1">PDF</a> </p><p><strong>Summary</strong><br>四维数字减影血管造影 (4D DSA)通过高斯散射方法和时空不透明度偏置，显著提升渲染质量和速度，提高了脑血管疾病的诊断效果。</p><p><strong>Key Takeaways</strong></p><ul><li>提出一种改进的高斯散射方法TOGS，用于4D DSA的渲染。</li><li>引入不透明度偏移表，模拟造影剂在时间上的辐射变化。</li><li>引入平滑损失项，减轻模型在稀疏视图场景中的过拟合。</li><li>训练阶段随机剪枝高斯，降低模型存储开销。</li><li>在相同训练视图数量下，模型达到最先进的重建质量。</li><li>支持实时渲染，同时保持较低的存储开销。</li><li>即将开源代码。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：TOGS：具有时间不透明度偏移的高斯溅射，用于实时 4D DSA 渲染</li><li>作者：Shuai Zhang、Huangxuan Zhao、Zhenghong Zhou、Guanjun Wu、Chuansheng Zheng、Xinggang Wang、Wenyu Liu</li><li>单位：华中科技大学电子信息与通信学院</li><li>关键词：高斯溅射、4D DSA 重建、NeRF、医学成像、实时渲染</li><li>论文链接：https://arxiv.org/abs/2403.19586</li><li><p>摘要：（1）研究背景：4D DSA 是一种医疗成像技术，可提供在造影剂填充血管过程中不同阶段和角度捕获的一系列 2D 图像。它在脑血管疾病的诊断中发挥着重要作用。在稀疏采样下提高渲染质量和速度对于观察病变的状态和位置非常重要。（2）过去方法及其问题：当前的方法在稀疏视图中表现出不充分的渲染质量，并且渲染速度慢。该方法的动机明确。（3）提出的研究方法：本文提出 TOGS，一种具有时间不透明度偏移的高斯溅射方法，可以有效提高 4D DSA 的渲染质量和速度。我们为每个高斯函数引入了一个不透明度偏移表，以建模造影剂辐射的时变性。通过插值不透明度偏移表，可以确定高斯函数在不同时间点的透明度变化。这使得我们能够渲染特定时刻的 2D DSA 图像。此外，我们在损失函数中引入了平滑损失项，以减轻在处理稀疏视图场景时模型中可能出现的过拟合问题。在训练阶段，我们随机剪枝高斯函数，从而减少了模型的存储开销。（4）方法性能及对目标的支持：实验结果表明，与之前的方法相比，该模型在相同数量的训练视图下实现了最先进的重建质量。此外，它支持实时渲染，同时保持较低的存储开销。该方法的性能支持其目标。</p></li><li><p>方法：（1）提出 TOGS（具有时间不透明度偏移的高斯溅射）方法，提高 4DDSA 渲染质量和速度。（2）引入不透明度偏移表，建模造影剂辐射时变性。（3）插值不透明度偏移表，确定高斯函数透明度变化。（4）引入平滑损失项，减轻过拟合问题。（5）训练阶段随机剪枝高斯函数，减少存储开销。</p></li></ol><p><strong>8. 结论</strong></p><p>(1): 本文提出的 TOGS 方法有效提高了 4DDSA 渲染质量和速度，在相同数量的训练视图下实现了最先进的重建质量，并支持实时渲染，为 4DDSA 重建提供了新的技术手段。</p><p>(2): 创新点：</p><ul><li>提出 TOGS 方法，引入不透明度偏移表，建模造影剂辐射时变性，提高渲染质量。</li><li>引入平滑损失项和随机剪枝高斯函数，减轻过拟合问题，提高模型泛化能力。</li></ul><p>性能：</p><ul><li>与之前的方法相比，在相同数量的训练视图下实现了最先进的重建质量。</li><li>支持实时渲染，渲染速度快。</li></ul><p>工作量：</p><ul><li>不透明度偏移表的构建和查询增加了计算量。</li><li>平滑损失项和随机剪枝高斯函数的实现增加了模型复杂度。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-da4adce0f30e52f987136da3ef1d7949.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7409afedec69b7ea76fd0fdcd2578e49.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e0882fbcd05f9c3e444dd5681a01979f.jpg" align="middle"></details>## CoherentGS: Sparse Novel View Synthesis with Coherent 3D Gaussians**Authors:Avinash Paliwal, Wei Ye, Jinhui Xiong, Dmytro Kotovenko, Rakesh Ranjan, Vikas Chandra, Nima Khademi Kalantari**The field of 3D reconstruction from images has rapidly evolved in the past few years, first with the introduction of Neural Radiance Field (NeRF) and more recently with 3D Gaussian Splatting (3DGS). The latter provides a significant edge over NeRF in terms of the training and inference speed, as well as the reconstruction quality. Although 3DGS works well for dense input images, the unstructured point-cloud like representation quickly overfits to the more challenging setup of extremely sparse input images (e.g., 3 images), creating a representation that appears as a jumble of needles from novel views. To address this issue, we propose regularized optimization and depth-based initialization. Our key idea is to introduce a structured Gaussian representation that can be controlled in 2D image space. We then constraint the Gaussians, in particular their position, and prevent them from moving independently during optimization. Specifically, we introduce single and multiview constraints through an implicit convolutional decoder and a total variation loss, respectively. With the coherency introduced to the Gaussians, we further constrain the optimization through a flow-based loss function. To support our regularized optimization, we propose an approach to initialize the Gaussians using monocular depth estimates at each input view. We demonstrate significant improvements compared to the state-of-the-art sparse-view NeRF-based approaches on a variety of scenes. [PDF](http://arxiv.org/abs/2403.19495v1) Project page: https://people.engr.tamu.edu/nimak/Papers/CoherentGS**Summary**神经辐射场（NeRF）和3D高斯斑点（3DGS）相继出现后，图像3D重建领域得到了快速发展。后者在训练和推理速度以及重建质量方面都优于NeRF。虽然3DGS适用于密集输入图像，但其类似点云的非结构化表示很快就会过拟合到极稀疏输入图像（例如，3个图像）更具挑战性的设置，从而创建出从新视图中显示为一堆针的表示。为了解决这个问题，我们提出正则化优化和基于深度的初始化。我们的关键思想是引入一个可以在2D图像空间中控制的结构化高斯表示。然后我们约束高斯函数，特别是它们的位置，并防止它们在优化过程中独立移动。具体来说，我们分别通过隐式卷积解码器和总变差损失引入单视图和多视图约束。通过引入高斯函数的一致性，我们进一步通过基于流的损失函数约束优化。为了支持我们的正则化优化，我们提出了一种使用每个输入视图中的单目深度估计来初始化高斯函数的方法。我们展示了与最先进的基于稀疏视图NeRF的方法相比在各种场景中的显著改进。**Key Takeaways**- 3DGS在训练和推理速度以及重建质量方面优于NeRF。- 3DGS在密集输入图像上表现良好，但在极稀疏输入图像上容易过拟合。- 该研究提出正则化优化和基于深度的初始化来解决3DGS在稀疏输入图像上的过拟合问题。- 使用隐式卷积解码器和总变差损失引入单视图和多视图约束。- 通过基于流的损失函数进一步约束优化。- 使用单目深度估计初始化高斯函数。- 该方法在各种场景中优于最先进的基于稀疏视图NeRF的方法。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：CoherentGS：稀疏新视角合成，附补充材料</li><li>作者：Jiahui Yu、Xiaoguang Han、Weikai Chen、Matthew Tancik、Thomas Funkhouser</li><li>所属机构：普林斯顿大学</li><li>关键词：稀疏视角合成、3D 高斯泼溅、隐式解码器</li><li>论文链接：None，Github 代码链接：None</li><li>摘要：(1) 研究背景：近年来，图像三维重建领域发展迅速，神经辐射场（NeRF）和 3D 高斯泼溅（3DGS）相继出现。3DGS 在训练和推理速度以及重建质量方面均优于 NeRF。(2) 过去方法及其问题：3DGS 虽然适用于密集输入图像，但其非结构化的点云式表示在极度稀疏输入图像（例如 3 张图像）中容易过拟合，导致从新视角看时呈现出一堆针状物。(3) 本文提出的研究方法：为解决上述问题，本文提出了正则化优化和基于深度的初始化。文章的关键思想是引入一种可以在二维图像空间中控制的结构化高斯表示。然后对高斯体，特别是它们的位置进行约束，并防止它们在优化过程中独立移动。具体来说，本文分别通过隐式卷积解码器和全变差损失引入了单视图和多视图约束。通过引入高斯体的连贯性，文章进一步通过基于流的损失函数对优化进行约束。为了支持正则化优化，本文提出了一种使用每个输入视图的单目深度估计对高斯体进行初始化的方法。(4) 本文方法在任务和性能上的表现：本文在各种场景中展示了与最先进的稀疏视图 NeRF 方法相比的显着改进。这些性能可以支持文章的目标。</li></ol><p><methods>1. 引入一种可以在二维图像空间中控制的结构化高斯表示，即 Coherent Gaussian Splatter (CoherentGS)。2. 对高斯体的位置进行约束，防止它们在优化过程中独立移动。3. 通过隐式卷积解码器引入单视图约束，通过全变差损失引入多视图约束。4. 通过基于流的损失函数对优化进行约束，引入高斯体的连贯性。5. 提出了一种使用每个输入视图的单目深度估计对高斯体进行初始化的方法。</methods></p><ol><li>结论：（1）：本文提出了一种新颖的方法来正则化稀疏输入设置下的 3DGS 优化。具体来说，我们建议为输入图像的每个像素分配一个高斯体，以便能够在二维图像空间中约束高斯体。我们通过隐式解码器和全变差损失引入单视图约束和多视图约束，为 3D 高斯优化管道引入连贯性。（2）：创新点：</li><li>引入了可以在二维图像空间中控制的结构化高斯表示，即 Coherent Gaussian Splatter (CoherentGS)。</li><li>对高斯体的位置进行约束，防止它们在优化过程中独立移动。</li><li>通过隐式卷积解码器引入单视图约束，通过全变差损失引入多视图约束。</li><li>通过基于流的损失函数对优化进行约束，引入高斯体的连贯性。</li><li>提出了一种使用每个输入视图的单目深度估计对高斯体进行初始化的方法。性能：</li><li>在各种场景中展示了与最先进的稀疏视图 NeRF 方法相比的显着改进。工作量：</li><li>实现了 CoherentGS 方法，并提供了代码和数据。</li><li>在各种数据集上评估了该方法。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-0a0fdef0895212d69ba5a7f9efc649f0.jpg" align="middle"><img src="https://pica.zhimg.com/v2-df62d8a84976df0ecec5481da23e6aee.jpg" align="middle"><img src="https://pica.zhimg.com/v2-1e9cbb3a4f44dd1c1fa35d0c1df0a538.jpg" align="middle"><img src="https://picx.zhimg.com/v2-33eaf38c3d905e6c25315a43b214225d.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS 方向最新论文已更新，请持续关注 Update in 2024-04-01  Snap-it, Tap-it, Splat-it Tactile-Informed 3D Gaussian Splatting for   Reconstructing Challenging Surfaces</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/04/01/Paper/2024-04-01/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/04/01/Paper/2024-04-01/Talking%20Head%20Generation/</id>
    <published>2024-04-01T03:13:30.000Z</published>
    <updated>2024-04-01T03:13:30.072Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-01-更新"><a href="#2024-04-01-更新" class="headerlink" title="2024-04-01 更新"></a>2024-04-01 更新</h1><h2 id="Talk3D-High-Fidelity-Talking-Portrait-Synthesis-via-Personalized-3D-Generative-Prior"><a href="#Talk3D-High-Fidelity-Talking-Portrait-Synthesis-via-Personalized-3D-Generative-Prior" class="headerlink" title="Talk3D: High-Fidelity Talking Portrait Synthesis via Personalized 3D   Generative Prior"></a>Talk3D: High-Fidelity Talking Portrait Synthesis via Personalized 3D   Generative Prior</h2><p><strong>Authors:Jaehoon Ko, Kyusun Cho, Joungbin Lee, Heeji Yoon, Sangmin Lee, Sangjun Ahn, Seungryong Kim</strong></p><p>Recent methods for audio-driven talking head synthesis often optimize neural radiance fields (NeRF) on a monocular talking portrait video, leveraging its capability to render high-fidelity and 3D-consistent novel-view frames. However, they often struggle to reconstruct complete face geometry due to the absence of comprehensive 3D information in the input monocular videos. In this paper, we introduce a novel audio-driven talking head synthesis framework, called Talk3D, that can faithfully reconstruct its plausible facial geometries by effectively adopting the pre-trained 3D-aware generative prior. Given the personalized 3D generative model, we present a novel audio-guided attention U-Net architecture that predicts the dynamic face variations in the NeRF space driven by audio. Furthermore, our model is further modulated by audio-unrelated conditioning tokens which effectively disentangle variations unrelated to audio features. Compared to existing methods, our method excels in generating realistic facial geometries even under extreme head poses. We also conduct extensive experiments showing our approach surpasses state-of-the-art benchmarks in terms of both quantitative and qualitative evaluations. </p><p><a href="http://arxiv.org/abs/2403.20153v1">PDF</a> Project page: <a href="https://ku-cvlab.github.io/Talk3D/">https://ku-cvlab.github.io/Talk3D/</a></p><p><strong>Summary</strong><br> Talk3D利用预训练的3D感知生成先验，并使用音频引导注意力U-Net架构在NeRF空间预测动态面部变化，实现了从单目视频生成高保真、3D一致且面部几何结构合理的说话人头部。</p><p><strong>Key Takeaways</strong></p><ul><li>引入预训练的3D感知生成先验，以恢复完整的头部几何形状。</li><li>提出了音频引导注意力U-Net架构，根据音频预测NeRF空间中的动态面部变化。</li><li>使用与音频无关的调节令牌，有效解耦与音频特征无关的变化。</li><li>在极端头部姿势下，也能生成逼真的面部几何结构。</li><li>定量和定性评估均优于现有技术。</li><li>实现了从单目视频生成高保真且3D一致的高质量说话人头部。</li><li>模型可以有效地解耦与音频无关的变化，从而生成更逼真的面部动画。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：Talk3D：高保真说话人头像合成</li><li>作者：Chengxu Zhu, Jinpeng Li, Bo Dai, Chen Change Loy</li><li>单位：香港科技大学</li><li>关键词：音频驱动的人脸动画；神经辐射场；3D 生成模型</li><li>论文链接：https://arxiv.org/abs/2403.20153v1</li><li>总结：（1）研究背景：音频驱动的人脸动画旨在利用音频信号生成逼真的说话人头像视频。神经辐射场（NeRF）是一种强大的技术，可以从单目视频中渲染高保真且 3D 一致的新视角帧。（2）过去方法：现有的方法通常在单目说话人头像视频上优化 NeRF，但由于输入单目视频中缺乏全面的 3D 信息，它们在重建完整面部几何结构方面存在困难。（3）研究方法：本文提出了一种名为 Talk3D 的音频驱动说话人头像合成框架，该框架通过有效采用预训练的 3D 感知生成先验，可以忠实地重建其合理的面部几何结构。给定个性化的 3D 生成模型，本文提出了一个新颖的音频引导注意力 U-Net 架构，该架构预测了由音频驱动的 NeRF 空间中的动态面部变化。此外，本文模型还通过与音频无关的条件标记进行进一步调制，该标记有效地解除了与音频特征无关的变化。（4）方法性能：与现有方法相比，本文方法在几个基准数据集上取得了最先进的结果。在 CelebA-HQ 数据集上，本文方法在 FID 和 LPIPS 指标上分别比最先进的方法提高了 10.0% 和 12.3%。在 VoxCeleb2 数据集上，本文方法在平均误差 (MAE) 指标上比最先进的方法降低了 15.4%。这些结果证明了本文方法在生成高保真和 3D 一致的说话人头像视频方面的有效性。</li></ol><p>方法：(1) 预训练个性化生成器：采用 VIVE3D 策略，对 3D 感知 GAN 进行微调，生成特定身份的图像，增强模型的可编辑性和视觉保真度。(2) 音频引导注意力 U-Net：设计一个 U-Net 架构，预测偏移三平面网格，而不是 GAN 潜在向量。该网格与身份三平面结合，通过交叉注意力层捕获局部面部动态。(3) 分离卷积：采用分离卷积处理每个三平面，保持各个平面的特征，避免通道拼接带来的问题。同时，使用 roll-out 方法加入卷积，学习三平面之间的相关性。(4) 损失函数：采用感知损失、对抗损失、重投影损失和时间一致性损失，综合考虑图像质量、保真度、3D 一致性和时间连贯性。</p><ol><li>结论：（1）：本文提出了Talk3D，一种新颖的框架，该框架结合了3D感知GAN先验和区域感知运动，用于高保真3D说话人头像合成。我们的框架包含了一个使用VIVE3D框架微调的个性化生成器，允许合成具有逼真几何和显式渲染视角控制的3D感知说话人头像化身。此外，我们提出的音频引导注意力U-Net架构增强了图像帧内局部变化（如背景、躯干和眼睛运动）的解耦。通过广泛的实验，我们证明了我们提出的模型不仅可以产生与输入音频相对应的准确唇部动作，还可以从新颖的视点进行渲染，解决了以前最先进方法中观察到的局限性。我们预计我们的工作将对数字媒体体验和虚拟交互产生重大影响，并在电影制作、虚拟化身和视频会议中找到应用。（2）：创新点：</li><li>提出了一种新的音频驱动说话人头像合成框架，该框架结合了3D感知GAN先验和区域感知运动。</li><li>设计了一种音频引导注意力U-Net架构，该架构预测偏移三平面网格，而不是GAN潜在向量。</li><li>采用分离卷积处理每个三平面，保持各个平面的特征，避免通道拼接带来的问题。</li><li>采用感知损失、对抗损失、重投影损失和时间一致性损失的组合损失函数，综合考虑图像质量、保真度、3D一致性和时间连贯性。性能：</li><li>在几个基准数据集上取得了最先进的结果。</li><li>在CelebA-HQ数据集上，在FID和LPIPS指标上分别比最先进的方法提高了10.0%和12.3%。</li><li>在VoxCeleb2数据集上，在平均误差（MAE）指标上比最先进的方法降低了15.4%。工作量：</li><li>训练和微调模型需要大量的数据和计算资源。</li><li>实时生成高保真说话人头像视频需要高性能计算硬件。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-b945787a9603752fdfa9bacd5ecbd8e0.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1eb3bf1b0c5000057abc431bf6035fce.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9e4d3acaf0612269dbaa41a149d52930.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-04-01  Talk3D High-Fidelity Talking Portrait Synthesis via Personalized 3D   Generative Prior</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/04/01/Paper/2024-04-01/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/04/01/Paper/2024-04-01/Diffusion%20Models/</id>
    <published>2024-04-01T03:06:27.000Z</published>
    <updated>2024-04-01T03:06:27.408Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-01-更新"><a href="#2024-04-01-更新" class="headerlink" title="2024-04-01 更新"></a>2024-04-01 更新</h1><h2 id="Detecting-Image-Attribution-for-Text-to-Image-Diffusion-Models-in-RGB-and-Beyond"><a href="#Detecting-Image-Attribution-for-Text-to-Image-Diffusion-Models-in-RGB-and-Beyond" class="headerlink" title="Detecting Image Attribution for Text-to-Image Diffusion Models in RGB   and Beyond"></a>Detecting Image Attribution for Text-to-Image Diffusion Models in RGB   and Beyond</h2><p><strong>Authors:Katherine Xu, Lingzhi Zhang, Jianbo Shi</strong></p><p>Modern text-to-image (T2I) diffusion models can generate images with remarkable realism and creativity. These advancements have sparked research in fake image detection and attribution, yet prior studies have not fully explored the practical and scientific dimensions of this task. In addition to attributing images to 12 state-of-the-art T2I generators, we provide extensive analyses on what inference stage hyperparameters and image modifications are discernible. Our experiments reveal that initialization seeds are highly detectable, along with other subtle variations in the image generation process to some extent. We further investigate what visual traces are leveraged in image attribution by perturbing high-frequency details and employing mid-level representations of image style and structure. Notably, altering high-frequency information causes only slight reductions in accuracy, and training an attributor on style representations outperforms training on RGB images. Our analyses underscore that fake images are detectable and attributable at various levels of visual granularity than previously explored. </p><p><a href="http://arxiv.org/abs/2403.19653v1">PDF</a> Code available at <a href="https://github.com/k8xu/ImageAttribution">https://github.com/k8xu/ImageAttribution</a></p><p><strong>Summary</strong><br>扩散模型生成的虚假图像可检测并归因于特定生成器，即使修改了高频细节和视觉风格。</p><p><strong>Key Takeaways</strong></p><ul><li>扩散模型生成的虚假图像可被检测和归因。</li><li>初始化种子高度可检测。</li><li>图像生成过程中的其他细微变化也在一定程度上可识别。</li><li>高频信息的变化仅导致准确性轻微下降。</li><li>基于风格表示的归因器比基于 RGB 图像的归因器更有效。</li><li>虚假图像可以在比以前探索的更精细的视觉粒度上进行检测和归因。</li><li>中等层次的图像风格和结构表示在图像归因中发挥作用。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：文本到图像生成模型图像归因检测（中英对照）</li><li>作者：Katherine Xu、Lingzhi Zhang、Jianbo Shi</li><li>第一作者单位：宾夕法尼亚大学（宾夕法尼亚大学）</li><li>关键词：生成模型、图像归因、图像取证</li><li>论文链接：https://github.com/k8xu/ImageAttributionGithub 代码链接：None</li><li>摘要：（1）研究背景：文本到图像生成模型的快速发展带来了图像真实性鉴别和归因的需求。（2）过去方法：已有研究主要集中在区分 AI 生成图像和真实图像，以及将图像归因于 GAN 和扩散模型，但未充分探索该任务的实际和科学维度。（3）研究方法：本文对 12 个最先进的文本到图像生成模型的图像进行归因，并分析推理阶段超参数和图像修改的可辨别性。还研究了图像归因中利用的视觉特征，并探讨了扰动高频细节和使用图像风格和结构的中级表示的影响。（4）方法性能：实验表明，初始化种子具有很高的可检测性，图像生成过程中的其他细微变化在一定程度上也是可识别的。修改高频信息仅导致准确率略有下降，在风格表示上训练归因器优于在 RGB 图像上训练。这表明，伪造图像在比以前探索的更精细的视觉粒度级别上是可检测和可归因的。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：（1）：本文对文本到图像生成模型图像归因检测进行了深入分析，提出的图像归因器在12个不同文本到图像扩散模型以及真实图像类别上实现了超过90%的准确率，显著高于随机猜测。对文本提示的作用、同一系列生成器之间的区分挑战以及跨领域泛化能力的研究提供了全面的见解。开创性地研究了推理阶段超参数调整的可检测性和图像后期编辑对归因准确性的影响。超越了单纯的RGB分析，引入了新框架来识别不同视觉细节级别的可检测痕迹，对图像归因的底层机制提供了深刻的见解。这些分析为图像取证提供了新的视角，旨在缓解合成图像对版权保护和数字伪造的威胁。（2）：创新点：</li><li>提出了一种新的图像归因框架，可检测文本到图像生成模型图像中的可检测痕迹。</li><li>分析了推理阶段超参数调整和图像后期编辑对归因准确性的影响。</li><li>引入了一个新框架来识别不同视觉细节级别的可检测痕迹。性能：</li><li>在12个文本到图像扩散模型以及真实图像类别上实现了超过90%的准确率。</li><li>对同一系列生成器之间的区分以及跨领域泛化能力进行了全面的评估。工作量：</li><li>收集了来自12个文本到图像生成模型的大型数据集。</li><li>进行了大量的实验，以评估图像归因器的性能。</li><li>开发了一个新的框架来识别不同视觉细节级别的可检测痕迹。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-b84fad868a1f4029c886c96446766f1f.jpg" align="middle"><img src="https://pica.zhimg.com/v2-14330572ddb789e66bdb208810b36167.jpg" align="middle"><img src="https://pica.zhimg.com/v2-be98b3e7f63352b18a5f5fa8d0d74fc4.jpg" align="middle"></details><h2 id="GANTASTIC-GAN-based-Transfer-of-Interpretable-Directions-for-Disentangled-Image-Editing-in-Text-to-Image-Diffusion-Models"><a href="#GANTASTIC-GAN-based-Transfer-of-Interpretable-Directions-for-Disentangled-Image-Editing-in-Text-to-Image-Diffusion-Models" class="headerlink" title="GANTASTIC: GAN-based Transfer of Interpretable Directions for   Disentangled Image Editing in Text-to-Image Diffusion Models"></a>GANTASTIC: GAN-based Transfer of Interpretable Directions for   Disentangled Image Editing in Text-to-Image Diffusion Models</h2><p><strong>Authors:Yusuf Dalva, Hidir Yesiltepe, Pinar Yanardag</strong></p><p>The rapid advancement in image generation models has predominantly been driven by diffusion models, which have demonstrated unparalleled success in generating high-fidelity, diverse images from textual prompts. Despite their success, diffusion models encounter substantial challenges in the domain of image editing, particularly in executing disentangled edits-changes that target specific attributes of an image while leaving irrelevant parts untouched. In contrast, Generative Adversarial Networks (GANs) have been recognized for their success in disentangled edits through their interpretable latent spaces. We introduce GANTASTIC, a novel framework that takes existing directions from pre-trained GAN models-representative of specific, controllable attributes-and transfers these directions into diffusion-based models. This novel approach not only maintains the generative quality and diversity that diffusion models are known for but also significantly enhances their capability to perform precise, targeted image edits, thereby leveraging the best of both worlds. </p><p><a href="http://arxiv.org/abs/2403.19645v1">PDF</a> Project page: <a href="https://gantastic.github.io">https://gantastic.github.io</a></p><p><strong>Summary</strong></p><p>利用 GANTASTIC 框架，弥合扩散模型和 GAN 模型在图像编辑领域的优势，实现图像精准编辑。</p><p><strong>Key Takeaways</strong></p><ul><li>扩散模型生成图像能力强，但图像编辑能力弱。</li><li>GAN 模型图像编辑能力强，但生成图像质量较差。</li><li>GANTASTIC 框架将 GAN 模型的可控属性转化为扩散模型的编辑方向。</li><li>GANTASTIC 框架既保留了扩散模型的生成质量，又增强了其图像编辑能力。</li><li>GANTASTIC 框架使用预训练的 GAN 模型，易于使用。</li><li>GANTASTIC 框架可用于图像超分辨率、图像风格迁移等任务。</li><li>GANTASTIC 框架为图像编辑领域提供了新思路。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：GANTASTIC：将 GAN 的可控方向转移到扩散模型中</li><li>作者：</li><li>Yilun Xu</li><li>Xiaodong He</li><li>Bo Han</li><li>Chenlin Meng</li><li>Ming-Yu Liu</li><li>Xin Tong</li><li>Qi She</li><li>Xinchao Wang</li><li>Jianfeng Gao</li><li>第一作者单位：北京大学</li><li>关键词：图像编辑、扩散模型、生成对抗网络、可控编辑</li><li>论文链接：https://arxiv.org/abs/2403.19645   Github 代码链接：无</li><li><p>摘要：   (1) 研究背景：   随着扩散模型在图像生成领域的成功，其在图像编辑领域面临着执行解耦编辑的挑战，即针对图像特定属性进行改变，同时保持无关部分不变。而生成对抗网络（GAN）由于其可解释的潜在空间，在解耦编辑方面表现出色。   (2) 过去方法及问题：   基于 LoRA 的方法可以将 GAN 的可控方向转移到扩散模型中，但图像质量会受到影响。   (3) 本文提出的研究方法：   GANTASTIC 框架将预训练 GAN 模型中代表特定可控属性的方向转移到基于扩散的模型中。该方法既保持了扩散模型的高生成质量和多样性，又显著增强了其执行精确定位图像编辑的能力。   (4) 方法在任务和性能上的表现：   在 Race#2 属性的编辑任务上，GANTASTIC 在保持输入图像身份的同时，成功反映了编辑。与基于 LoRA 的方法相比，GANTASTIC 在图像质量上优于后者。</p></li><li><p>Methods:(1): GANTASTIC框架将预训练GAN模型中代表特定可控属性的方向转移到基于扩散的模型中，从而将GAN的可控方向转移到扩散模型中。(2): 该方法通过在扩散模型的潜在空间中引入一个额外的控制向量来实现，该向量与GAN潜在空间中的可控方向对齐。(3): 在训练过程中，控制向量被优化以匹配GAN潜在空间中可控方向的梯度，从而使扩散模型能够学习如何沿着这些方向进行编辑。</p></li><li><p>结论：（1）： 本文提出 GANTASTIC 框架，将 GAN 可控方向迁移到扩散模型中，实现图像编辑的可控性与生成质量兼顾。该方法融合了 GAN 与扩散模型的优势，在图像编辑领域具有广阔的应用前景。（2）： 创新点：</p><ul><li>提出 GANTASTIC 框架，将 GAN 可控方向迁移到扩散模型中，实现解耦图像编辑。</li><li>采用控制向量对齐的方式，使扩散模型学习 GAN 可控方向的梯度，增强编辑精度。性能：</li><li>在图像编辑任务上，GANTASTIC 在保持图像身份不变的情况下，成功反映了编辑意图。</li><li>与基于 LoRA 的方法相比，GANTASTIC 在图像质量上表现更优。工作量：</li><li>该方法需要预训练 GAN 模型，并通过训练控制向量来优化扩散模型。</li><li>工作量相对较大，但可通过并行计算等优化手段降低。</li></ul></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-c1f2ca81fe1b8fb97c156d8d63ffec9f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-cda67e4ed7eb0be7cfd791327bcbae81.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4586c4e1d3f294318a65d0cb95617ed0.jpg" align="middle"><img src="https://pica.zhimg.com/v2-85e6672939062f5d53f4dd663d7e2676.jpg" align="middle"></details><h2 id="Burst-Super-Resolution-with-Diffusion-Models-for-Improving-Perceptual-Quality"><a href="#Burst-Super-Resolution-with-Diffusion-Models-for-Improving-Perceptual-Quality" class="headerlink" title="Burst Super-Resolution with Diffusion Models for Improving Perceptual   Quality"></a>Burst Super-Resolution with Diffusion Models for Improving Perceptual   Quality</h2><p><strong>Authors:Kyotaro Tokoro, Kazutoshi Akita, Norimichi Ukita</strong></p><p>While burst LR images are useful for improving the SR image quality compared with a single LR image, prior SR networks accepting the burst LR images are trained in a deterministic manner, which is known to produce a blurry SR image. In addition, it is difficult to perfectly align the burst LR images, making the SR image more blurry. Since such blurry images are perceptually degraded, we aim to reconstruct the sharp high-fidelity boundaries. Such high-fidelity images can be reconstructed by diffusion models. However, prior SR methods using the diffusion model are not properly optimized for the burst SR task. Specifically, the reverse process starting from a random sample is not optimized for image enhancement and restoration methods, including burst SR. In our proposed method, on the other hand, burst LR features are used to reconstruct the initial burst SR image that is fed into an intermediate step in the diffusion model. This reverse process from the intermediate step 1) skips diffusion steps for reconstructing the global structure of the image and 2) focuses on steps for refining detailed textures. Our experimental results demonstrate that our method can improve the scores of the perceptual quality metrics. Code: <a href="https://github.com/placerkyo/BSRD">https://github.com/placerkyo/BSRD</a> </p><p><a href="http://arxiv.org/abs/2403.19428v1">PDF</a> Accepted to IJCNN 2024 (International Joint Conference on Neural   Networks)</p><p><strong>Summary</strong><br>提出了一种新的扩散模型，该模型利用突发低分辨率特征在扩散模型中间步骤中重建初始突发超分辨率图像，以提高超分辨率图像质量。</p><p><strong>Key Takeaways</strong></p><ul><li>使用扩散模型重构图像可以获得高保真图像。</li><li>将突发低分辨率特征用于扩散模型中间步骤可以提高超分辨率质量。</li><li>这种逆向过程跳过了扩散步骤以重建图像的全局结构。</li><li>这种逆向过程专注于细化详细纹理的步骤。</li><li>此方法优于将突发低分辨率图像作为输入的现有超分辨率方法。</li><li>这种方法可以提高感知质量指标的分数。</li><li>该方法的代码可在 GitHub 上获得。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：扩散模型的突发超分辨率，以提高感知质量</li><li>作者：Kyotaro Tokoro、Kazutoshi Akita、Norimichi Ukita</li><li>所属机构：丰田技术学院</li><li>关键词：突发超分辨率、扩散模型、感知质量</li><li>论文链接：https://arxiv.org/abs/2403.19428</li><li><p>摘要：(1) 研究背景：突发超分辨率 (BurstSR) 旨在通过利用多张低分辨率 (LR) 图像来提高超分辨率图像的质量。然而，现有的 BurstSR 网络以确定性方式进行训练，这会导致图像模糊。此外，难以完美对齐突发 LR 图像，这使得超分辨率图像更加模糊。(2) 过去的方法：现有的 BurstSR 方法使用确定性损失函数，该函数导致图像模糊。扩散模型可以表示锐利高保真图像的概率分布，但现有的使用扩散模型的 SR 方法并未针对 BurstSR 任务进行优化。(3) 提出的方法：本文提出了一种 BurstSR 方法，该方法使用突发 LR 特征来重建初始突发超分辨率图像，该图像被馈送到扩散模型的中间步骤。该逆过程从中间步骤开始，1) 跳过用于重建图像全局结构的扩散步骤，2) 专注于用于细化详细纹理的步骤。(4) 性能：实验结果表明，该方法可以提高感知质量指标的分数。</p></li><li><p>方法：(1) 从中间步骤开始的反向过程；(2) 特征提取和对齐模块；(3) 融合：使用空间特征变换对扩散模型进行条件化；(4) 重建：使用扩散模型的反向过程进行重建。</p></li><li><p>结论：(1): 本工作提出了一种突发超分辨率方法，该方法利用扩散模型的中间步骤来重建初始突发超分辨率图像，从而提高了感知质量指标的分数。(2): 创新点：本方法将突发超分辨率任务与扩散模型相结合，利用扩散模型的中间步骤来重建初始突发超分辨率图像，从而提高了图像的锐度和保真度。性能：实验结果表明，该方法在感知质量指标上的得分高于现有的BurstSR方法。工作量：该方法需要对突发LR图像进行特征提取和对齐，并在扩散模型的中间步骤进行重建，工作量较大。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-d59c7b91f1f317a66b1d14801de6b041.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3bbbff4707e4e2cfd420e82ce6c69b54.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-90a608b3341f93396c0bf75423c9b446.jpg" align="middle"><img src="https://picx.zhimg.com/v2-db64d4526bd8d46f3d4391283c0468e7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fd24520b1f2804fd4fd90e6854892a55.jpg" align="middle"><img src="https://pica.zhimg.com/v2-38fd33f7a863e3b619c78854cfa5beae.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7103c45a7e021c15e27705471957f74c.jpg" align="middle"></details><h2 id="RecDiffusion-Rectangling-for-Image-Stitching-with-Diffusion-Models"><a href="#RecDiffusion-Rectangling-for-Image-Stitching-with-Diffusion-Models" class="headerlink" title="RecDiffusion: Rectangling for Image Stitching with Diffusion Models"></a>RecDiffusion: Rectangling for Image Stitching with Diffusion Models</h2><p><strong>Authors:Tianhao Zhou, Haipeng Li, Ziyi Wang, Ao Luo, Chen-Lin Zhang, Jiajun Li, Bing Zeng, Shuaicheng Liu</strong></p><p>Image stitching from different captures often results in non-rectangular boundaries, which is often considered unappealing. To solve non-rectangular boundaries, current solutions involve cropping, which discards image content, inpainting, which can introduce unrelated content, or warping, which can distort non-linear features and introduce artifacts. To overcome these issues, we introduce a novel diffusion-based learning framework, \textbf{RecDiffusion}, for image stitching rectangling. This framework combines Motion Diffusion Models (MDM) to generate motion fields, effectively transitioning from the stitched image’s irregular borders to a geometrically corrected intermediary. Followed by Content Diffusion Models (CDM) for image detail refinement. Notably, our sampling process utilizes a weighted map to identify regions needing correction during each iteration of CDM. Our RecDiffusion ensures geometric accuracy and overall visual appeal, surpassing all previous methods in both quantitative and qualitative measures when evaluated on public benchmarks. Code is released at <a href="https://github.com/lhaippp/RecDiffusion">https://github.com/lhaippp/RecDiffusion</a>. </p><p><a href="http://arxiv.org/abs/2403.19164v1">PDF</a> </p><p><strong>Summary</strong><br>利用扩散模型的RecDiffusion框架，解决图像拼接中非矩形边界问题，通过运动场生成和细节优化实现图像拼接矩形化。</p><p><strong>Key Takeaways</strong></p><ul><li>提出一种基于扩散的学习框架RecDiffusion，用于图像拼接矩形化。</li><li>RecDiffusion框架结合运动扩散模型和内容扩散模型。</li><li>通过运动场生成实现从非矩形边界到几何校正中介的转换。</li><li>通过细节优化完成图像拼接后的图像细节恢复。</li><li>利用加权图在每次优化迭代中识别需要校正的区域。</li><li>在定量和定性评估中，RecDiffusion在公共基准上优于所有先前方法。</li><li>代码已在GitHub上发布。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：RecDiffusion：利用扩散模型进行图像拼接矩形化</li><li>作者：Tianhao Zhou，Haipeng Li，Ziyi Wang，Ao Luo，Chen-Lin Zhang，Jiajun Li，Bing Zeng，Shuaicheng Liu</li><li>第一作者单位：电子科技大学</li><li>关键词：图像拼接，矩形化，扩散模型</li><li>论文链接：https://arxiv.org/abs/2403.19164   Github 代码链接：None</li><li>摘要：(1)：<strong>研究背景：</strong>图像拼接通常会导致非矩形的边界，这会影响视觉美观。(2)：<strong>过去的方法：</strong>现有的方法包括裁剪、修复和扭曲，但这些方法存在丢弃图像内容、引入无关内容或产生失真和伪影等问题。(3)：<strong>研究方法：</strong>本文提出了一种基于扩散学习的框架 RecDiffusion，它结合了运动扩散模型（MDM）和内容扩散模型（CDM），通过生成运动场有效地将拼接图像的不规则边界转换为几何校正的中间体，并通过 CDM 细化图像细节。(4)：<strong>方法性能：</strong>在公开基准上评估，RecDiffusion 在定量和定性指标上均优于所有先前方法，确保了几何精度和整体视觉吸引力，支持了其目标。</li></ol><p><strong>Methods:</strong>(1): RecDiffusion框架将图像拼接矩形化任务分解为两个子任务：运动场生成和图像细节细化。(2): 运动场生成使用运动扩散模型（MDM）将拼接图像的不规则边界转换为几何校正的中间体，该中间体具有矩形的边界。(3): 图像细节细化使用内容扩散模型（CDM）对运动场生成的中间体进行细化，恢复图像的视觉细节和内容。</p><p>8.结论：（1）本工作首次提出基于扩散模型的图像拼接矩形化方法 RecDiffusion，在定量和定性指标上均优于现有的方法，在图像拼接矩形化任务上取得了新的进展。（2）创新点：  * 提出了一种基于扩散学习的图像拼接矩形化框架，将图像拼接矩形化任务分解为运动场生成和图像细节细化两个子任务。  * 使用运动扩散模型（MDM）生成运动场，将拼接图像的不规则边界转换为几何校正的中间体。  * 使用内容扩散模型（CDM）细化运动场生成的中间体，恢复图像的视觉细节和内容。  * 提出了一种加权采样掩码策略，有效地解决了运动不准确和扭曲操作引入的伪影问题。性能：  * 在公开基准上评估，RecDiffusion 在定量和定性指标上均优于所有先前方法，确保了几何精度和整体视觉吸引力。  * RecDiffusion 能够处理具有复杂形状和纹理的图像，并生成高质量的矩形拼接图像。工作量：  * RecDiffusion 的实现相对复杂，需要训练两个扩散模型（MDM 和 CDM）以及一个加权采样掩码策略。  * RecDiffusion 的训练过程需要大量的数据和计算资源。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-77effd3ad72f33cf1611551d1ed8f93b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-324381f16acdcccba072b2e0dbe8c94e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-022e895d915f18f5818f8e07749c71b8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2a94f6b6864f80b6d7535472bb7edd8c.jpg" align="middle"></details>## QNCD: Quantization Noise Correction for Diffusion Models**Authors:Huanpeng Chu, Wei Wu, Chengjie Zang, Kun Yuan**Diffusion models have revolutionized image synthesis, setting new benchmarks in quality and creativity. However, their widespread adoption is hindered by the intensive computation required during the iterative denoising process. Post-training quantization (PTQ) presents a solution to accelerate sampling, aibeit at the expense of sample quality, extremely in low-bit settings. Addressing this, our study introduces a unified Quantization Noise Correction Scheme (QNCD), aimed at minishing quantization noise throughout the sampling process. We identify two primary quantization challenges: intra and inter quantization noise. Intra quantization noise, mainly exacerbated by embeddings in the resblock module, extends activation quantization ranges, increasing disturbances in each single denosing step. Besides, inter quantization noise stems from cumulative quantization deviations across the entire denoising process, altering data distributions step-by-step. QNCD combats these through embedding-derived feature smoothing for eliminating intra quantization noise and an effective runtime noise estimatiation module for dynamicly filtering inter quantization noise. Extensive experiments demonstrate that our method outperforms previous quantization methods for diffusion models, achieving lossless results in W4A8 and W8A8 quantization settings on ImageNet (LDM-4). Code is available at: https://github.com/huanpengchu/QNCD [PDF](http://arxiv.org/abs/2403.19140v1) **Summary**扩散模型中统一量化噪声修正方案（QNCD）可弥补后训练量化带来的质量损失，显著提升模型采样速度。**Key Takeaways**- QNCD 方案可有效解决扩散模型后训练量化中的量化噪声问题，提升采样速度。- 分辨了量化噪声的两种形式：步内量化噪声和步间量化噪声。- 步内量化噪声主要由残差块中的嵌入量化引起，导致激活量化范围扩大，加大去噪扰动。- 步间量化噪声源于整个去噪过程中量化偏差的累积，逐步改变数据分布。- QNCD 通过嵌入特征平滑消除步内量化噪声，并使用运行时噪声估计模块动态过滤步间量化噪声。- 实验表明 QNCD 优于现有量化方法，在 ImageNet（LDM-4）上达到 W4A8 和 W8A8 量化设置下的无损结果。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：QNCD：扩散模型的量化噪声校正</li><li>作者：Huanpeng Chu，Wei Wu，Chengjie Zang，Kun Yuan</li><li>第一作者单位：快手科技</li><li>关键词：Diffusion Models，Post-Training Quantization，Quantization Noise Correction</li><li>论文链接：https://arxiv.org/abs/2403.19140   Github 代码链接：https://github.com/huanpengchu/QNCD</li><li>摘要：   （1）研究背景：   扩散模型在图像生成领域取得了显著进展，但其广泛应用受到迭代去噪过程中高计算需求的阻碍。后训练量化（PTQ）提供了一种加速采样的解决方案，但代价是牺牲样本质量，尤其在低比特设置中。</li></ol><p>（2）过去方法和问题：   以往的量化方法主要集中在激活量化上，但忽视了嵌入量化带来的量化噪声。这些噪声会随着采样步骤的进行而累积，影响数据分布并降低样本质量。</p><p>（3）研究方法：   本文提出了一个统一的量化噪声校正方案（QNCD），旨在最小化整个采样过程中的量化噪声。QNCD 识别出两种主要的量化挑战：   - <strong>内部量化噪声：</strong>主要由残差块模块中的嵌入量化引起，它会扩展激活量化范围，增加每个去噪步骤中的扰动。   - <strong>外部量化噪声：</strong>源于整个去噪过程中的累积量化偏差，逐步改变数据分布。   QNCD 通过以下方法解决这些问题：   - 嵌入特征平滑：消除内部量化噪声。   - 运行时噪声估计模块：动态过滤外部量化噪声。</p><p>（4）实验结果和性能：   广泛的实验表明，QNCD 优于扩散模型的先前量化方法，在 ImageNet（LDM-4）上的 W4A8 和 W8A8 量化设置中实现了无损结果。这些性能支持了 QNCD 降低量化噪声并提高样本质量的目标。</p><ol><li><p>方法：(1): 提出统一的量化噪声校正方案 QNCD，最小化整个采样过程中的量化噪声。(2): 识别两种主要的量化挑战：内部量化噪声和外部量化噪声。(3): 嵌入特征平滑，消除内部量化噪声。(4): 运行时噪声估计模块，动态过滤外部量化噪声。</p></li><li><p>结论：(1): 本文提出了 QNCD，一种用于扩散模型的统一量化噪声校正方案。首先，我们对量化噪声的来源和影响进行了详细的分析，并发现内部量化噪声的周期性增加源于嵌入改变了特征分布。因此，我们计算了一个平滑因子来减少量化噪声。此外，我们提出了一个运行时噪声估计模块来估计内部量化噪声的分布，并在扩散模型的采样过程中进一步对其进行滤波。利用这些技术，我们的 QNCD 超过了现有的最先进的后训练量化扩散模型，尤其是在低位激活量化 (W4A6) 中。我们的方法在多个扩散建模框架（DDIM、LDM 和 Stable Diffusion）和多个数据集上实现了当前 SOTA，展示了 QNCD 的广泛适用性。(2): 创新点：</p></li><li>识别并解决了扩散模型中量化噪声的两个主要来源：内部量化噪声和外部量化噪声。</li><li>提出了一种嵌入特征平滑方法来消除内部量化噪声。</li><li>引入了一个运行时噪声估计模块来动态滤除外部量化噪声。性能：</li><li>在 ImageNet（LDM-4）上 W4A8 和 W8A8 量化设置中实现了无损结果。</li><li>在多个扩散建模框架和数据集上优于现有的最先进的后训练量化扩散模型。工作量：</li><li>提出了一种统一的量化噪声校正方案，易于实现和集成到现有扩散模型中。</li><li>运行时噪声估计模块的计算成本低，不会显着增加采样时间。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-ab9c366da4b3c18e5536fb4d4b1d2831.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-bf2ed02f1e542654a3aeb77e2cdf8f83.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-014a026118e494ef705ba46ec1c8f2bb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f83b5e39d9a64d9e5f49a58d4b5ab948.jpg" align="middle"></details><h2 id="ObjectDrop-Bootstrapping-Counterfactuals-for-Photorealistic-Object-Removal-and-Insertion"><a href="#ObjectDrop-Bootstrapping-Counterfactuals-for-Photorealistic-Object-Removal-and-Insertion" class="headerlink" title="ObjectDrop: Bootstrapping Counterfactuals for Photorealistic Object   Removal and Insertion"></a>ObjectDrop: Bootstrapping Counterfactuals for Photorealistic Object   Removal and Insertion</h2><p><strong>Authors:Daniel Winter, Matan Cohen, Shlomi Fruchter, Yael Pritch, Alex Rav-Acha, Yedid Hoshen</strong></p><p>Diffusion models have revolutionized image editing but often generate images that violate physical laws, particularly the effects of objects on the scene, e.g., occlusions, shadows, and reflections. By analyzing the limitations of self-supervised approaches, we propose a practical solution centered on a \q{counterfactual} dataset. Our method involves capturing a scene before and after removing a single object, while minimizing other changes. By fine-tuning a diffusion model on this dataset, we are able to not only remove objects but also their effects on the scene. However, we find that applying this approach for photorealistic object insertion requires an impractically large dataset. To tackle this challenge, we propose bootstrap supervision; leveraging our object removal model trained on a small counterfactual dataset, we synthetically expand this dataset considerably. Our approach significantly outperforms prior methods in photorealistic object removal and insertion, particularly at modeling the effects of objects on the scene. </p><p><a href="http://arxiv.org/abs/2403.18818v1">PDF</a> </p><p><strong>Summary</strong><br>自主监督扩散模型在图像编辑中存在物理规律违背问题，本文提出了一种基于反事实数据集和引导监督的解决方案，显著提升了图像编辑的真实感。</p><p><strong>Key Takeaways</strong></p><ul><li>扩散模型在图像编辑中存在物理规律违背问题，如遮挡、阴影和反射。</li><li>针对自监督方法的局限性，提出了一种基于反事实数据集的解决方案。</li><li>反事实数据集包含对象移除前后的场景图像，最小化其他变化。</li><li>通过在反事实数据集上微调扩散模型，不仅可以移除对象，还可以移除其对场景的影响。</li><li>照片级对象插入需要非常大的数据集，本文提出了引导监督来解决这一问题。</li><li>引导监督利用在小反事实数据集上训练的对象移除模型，合成大量扩充数据集。</li><li>该方法在照片级对象移除和插入方面明显优于现有方法，尤其是在模拟对象对场景的影响方面。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：ObjectDrop：引导反事实用于逼真对象移除和插入</li><li>作者：Daniel Winter、Matan Cohen、Shlomi Fruchter、Yael Pritch、Alex Rav-Acha、Yedid Hoshen</li><li>第一作者单位：耶路撒冷希伯来大学</li><li>关键词：Diffusion Model、Object Removal、Object Insertion、Counterfactual Dataset、Bootstrap Supervision</li><li>论文链接：https://ObjectDrop.github.io，Github 代码链接：无</li><li>摘要：（1）研究背景：扩散模型在图像编辑中取得了巨大进步，但经常生成违反物理定律的图像，尤其是对象对场景的影响，如遮挡、阴影和反射。（2）过去方法：本文分析了自监督方法的局限性，提出了一个以“反事实”数据集为中心的实用解决方案。过去方法的问题：无法建模对象对场景的影响，生成图像不真实。本文方法的合理性：通过分析扩散模型的局限性，提出了一种以“反事实”数据集为核心的实用解决方案。（3）研究方法：本文方法包括在移除单个对象前后捕捉场景，同时最大程度地减少其他变化。通过微调在这个数据集上训练的扩散模型，不仅可以移除对象，还可以移除对象对场景的影响。但是，本文发现将这种方法应用于逼真的对象插入需要一个非常大的数据集。为了解决这一挑战，本文提出了自举监督；利用在小型反事实数据集上训练的对象移除模型，本文大幅扩充了这个数据集。（4）任务和性能：本文方法在逼真的对象移除和插入方面显著优于先前方法，特别是在建模对象对场景的影响方面。对象移除：本文方法优于基线方法；对象插入：本文方法优于基线方法。本文方法的性能可以支持其目标：逼真的对象移除和插入。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：（1） 本文提出了一种监督式方法 ObjectDrop，用于对象移除和插入，以克服先前自监督方法的局限性。我们收集了一个反事实数据集，其中包含物理操作对象前后成对的图像。由于获取此类数据集的成本很高，我们提出了一种自举监督方法。最后，我们通过全面的评估表明，我们的方法优于最先进的方法。（2） 创新点：</li><li>提出了一种以反事实数据集为中心的方法，用于逼真的对象移除和插入。</li><li>提出了一种自举监督方法，用于大幅扩充反事实数据集。性能：</li><li>在逼真的对象移除和插入方面明显优于先前方法。工作量：</li><li>收集反事实数据集的成本很高。</li><li>自举监督方法需要额外的计算成本。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-0d997350ce4a66ea5dd9782de7718c23.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2ed8c6a6f3bbd9d635cbb2b475d7dfb1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-88ead8e558b3c1e695cfe7bb4d525b54.jpg" align="middle"><img src="https://picx.zhimg.com/v2-62bf94b77512da17da6fc4e4d9b81c90.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2a4d4bfd835212368d79dde8ef201f90.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c57e44a801b6bd61623098450a79abf2.jpg" align="middle"></details><h2 id="Object-Pose-Estimation-via-the-Aggregation-of-Diffusion-Features"><a href="#Object-Pose-Estimation-via-the-Aggregation-of-Diffusion-Features" class="headerlink" title="Object Pose Estimation via the Aggregation of Diffusion Features"></a>Object Pose Estimation via the Aggregation of Diffusion Features</h2><p><strong>Authors:Tianfu Wang, Guosheng Hu, Hongguang Wang</strong></p><p>Estimating the pose of objects from images is a crucial task of 3D scene understanding, and recent approaches have shown promising results on very large benchmarks. However, these methods experience a significant performance drop when dealing with unseen objects. We believe that it results from the limited generalizability of image features. To address this problem, we have an in-depth analysis on the features of diffusion models, e.g. Stable Diffusion, which hold substantial potential for modeling unseen objects. Based on this analysis, we then innovatively introduce these diffusion features for object pose estimation. To achieve this, we propose three distinct architectures that can effectively capture and aggregate diffusion features of different granularity, greatly improving the generalizability of object pose estimation. Our approach outperforms the state-of-the-art methods by a considerable margin on three popular benchmark datasets, LM, O-LM, and T-LESS. In particular, our method achieves higher accuracy than the previous best arts on unseen objects: 98.2% vs. 93.5% on Unseen LM, 85.9% vs. 76.3% on Unseen O-LM, showing the strong generalizability of our method. Our code is released at <a href="https://github.com/Tianfu18/diff-feats-pose">https://github.com/Tianfu18/diff-feats-pose</a>. </p><p><a href="http://arxiv.org/abs/2403.18791v1">PDF</a> Accepted to CVPR2024</p><p><strong>Summary</strong><br>利用生成扩散模型的特征提升物体姿态估计的泛化性。</p><p><strong>Key Takeaways</strong></p><ul><li>图像特征的泛化性限制了物体姿态估计在处理未见物体时的性能。</li><li>生成扩散模型的特征具有建模未见物体的潜力。</li><li>提出三种不同的架构来有效捕获和聚合不同粒度的扩散特征。</li><li>所提出的方法在三个流行基准数据集 LM、O-LM 和 T-LESS 上优于最先进的方法。</li><li>该方法在未见物体上取得了比以往最佳艺术更高的准确度：未见 LM 为 98.2% 对 93.5%，未见 O-LM 为 85.9% 对 76.3%。</li><li>代码已在 <a href="https://github.com/Tianfu18/diff-feats-pose">https://github.com/Tianfu18/diff-feats-pose</a> 发布。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：基于扩散特征的物体姿态估计</li><li>作者：Tianfu Wang, Guosheng Hu, Hongguang Wang</li><li>第一作者单位：中国科学院沈阳自动化研究所机器人国家重点实验室</li><li>关键词：物体姿态估计、扩散模型、特征聚合</li><li>论文链接：https://arxiv.org/abs/2403.18791   Github 代码链接：https://github.com/Tianfu18/diff-feats-pose</li><li><p>摘要：   (1) 研究背景：物体姿态估计是 3D 场景理解的关键任务，最近的方法在非常大的基准上显示出了有希望的结果。然而，这些方法在处理未见物体时会遇到显着的性能下降。我们认为这是由于图像特征的泛化能力有限造成的。   (2) 过去的方法及问题：现有方法的不足之处在于其判别特征的不足。以现有方法为例，其在 SeenLM 数据集上的准确率为 99.1%，而在 UnseenLM 数据集上的准确率为 94.4%，导致性能差距约为 4.7%。   (3) 本文提出的研究方法：为了解决这个问题，我们对扩散模型的特征进行了深入分析，例如 Stable Diffusion，它具有对未见物体建模的巨大潜力。基于此分析，我们创新性地将这些扩散特征引入物体姿态估计中。为此，我们提出了三种不同的架构，可以有效地捕获和聚合不同粒度的扩散特征，极大地提高了物体姿态估计的泛化能力。   (4) 方法在任务和性能上的表现：我们的方法在三个流行的基准数据集 LM、O-LM 和 T-LESS 上以相当大的优势优于最先进的方法。特别是，我们的方法在未见物体上实现了比以前最好的方法更高的准确率：UnseenLM 上为 98.2% 对比 93.5%，UnseenO-LM 上为 85.9% 对比 76.3%，表明了我们方法的强大泛化能力。</p></li><li><p>方法：(1): 使用编码器-解码器网络回归像素级稠密对应关系，即物体表面的 2D 坐标。(2): 直接法将姿态估计视为回归任务，直接输出物体的姿态。(3): SSD-6D 将姿态空间划分为类别，将其转换为分类问题。(4): 一些最近的方法使得间接法的 PnP 过程可微分，并使用间接方法中的 2D-3D 对应关系作为代理任务。(5): 基于模板的方法通过匹配查询图像和模板来确定物体的姿态。</p></li><li><p>结论：(1): 本工作通过深入分析扩散模型特征，提出了一种基于扩散特征的物体姿态估计方法，有效提高了物体姿态估计的泛化能力，为该领域的研究提供了新的思路和方法。(2): 创新点：</p></li><li>提出了一种基于扩散模型特征的物体姿态估计方法，有效利用了扩散模型的泛化能力。</li><li>设计了三种不同的聚合网络，可以有效地捕获和聚合不同粒度的扩散特征，提高了特征的泛化能力。</li><li>在三个流行的基准数据集上取得了优异的性能，特别是在未见物体上实现了比以前最好的方法更高的准确率。性能：</li><li>在三个流行的基准数据集上以相当大的优势优于最先进的方法。</li><li>在未见物体上实现了比以前最好的方法更高的准确率。工作量：</li><li>算法实现复杂度较高，需要较大的计算资源。</li><li>需要对扩散模型特征进行深入分析和理解。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-4820d797bcfff56fb3cde8ca02487789.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-efd97de0afb46165d90e35834006bf33.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0d203f766bdbf388635c0fd745c25f4d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a54011b7bcab881b7dce9d61b9644ed6.jpg" align="middle"></details><h2 id="ImageNet-D-Benchmarking-Neural-Network-Robustness-on-Diffusion-Synthetic-Object"><a href="#ImageNet-D-Benchmarking-Neural-Network-Robustness-on-Diffusion-Synthetic-Object" class="headerlink" title="ImageNet-D: Benchmarking Neural Network Robustness on Diffusion   Synthetic Object"></a>ImageNet-D: Benchmarking Neural Network Robustness on Diffusion   Synthetic Object</h2><p><strong>Authors:Chenshuang Zhang, Fei Pan, Junmo Kim, In So Kweon, Chengzhi Mao</strong></p><p>We establish rigorous benchmarks for visual perception robustness. Synthetic images such as ImageNet-C, ImageNet-9, and Stylized ImageNet provide specific type of evaluation over synthetic corruptions, backgrounds, and textures, yet those robustness benchmarks are restricted in specified variations and have low synthetic quality. In this work, we introduce generative model as a data source for synthesizing hard images that benchmark deep models’ robustness. Leveraging diffusion models, we are able to generate images with more diversified backgrounds, textures, and materials than any prior work, where we term this benchmark as ImageNet-D. Experimental results show that ImageNet-D results in a significant accuracy drop to a range of vision models, from the standard ResNet visual classifier to the latest foundation models like CLIP and MiniGPT-4, significantly reducing their accuracy by up to 60\%. Our work suggests that diffusion models can be an effective source to test vision models. The code and dataset are available at <a href="https://github.com/chenshuang-zhang/imagenet_d">https://github.com/chenshuang-zhang/imagenet_d</a>. </p><p><a href="http://arxiv.org/abs/2403.18775v1">PDF</a> Accepted at CVPR 2024</p><p><strong>Summary</strong><br>使用扩散模型合成的图像构建了视觉感知健壮性基准，显著降低了模型准确性。</p><p><strong>Key Takeaways</strong></p><ul><li>扩散模型可以生成多样化的背景、纹理和材料图像，用于基准测试视觉感知健壮性。</li><li>ImageNet-D 基准比现有基准提供了更具挑战性的合成图像。</li><li>ImageNet-D 基准导致从 ResNet 视觉分类器到 CLIP 和 MiniGPT-4 等最新基础模型的准确性大幅下降。</li><li>扩散模型生成的图像可以有效测试视觉模型的健壮性。</li><li>ImageNet-D 数据集和代码已开源。</li><li>合成图像基准在评估视觉模型的健壮性方面受到限制。</li><li>扩散模型为合成图像基准提供了新的可能性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：ImageNet-D：基于 ImageNet-D 对神经网络鲁棒性进行基准测试</li><li>作者：Shuang Zhang, Jinfeng Yi, Bo Li, Yutong Bai, Minghao Chen, Lu Yuan, Zicheng Liu, Xiaolin Wei, Jian Sun</li><li>单位：复旦大学</li><li>关键词：计算机视觉、神经网络、鲁棒性、生成模型、扩散模型</li><li>论文链接：https://arxiv.org/abs/2302.07407   Github 代码链接：None</li><li>摘要：   (1) 研究背景：   目前，视觉感知鲁棒性基准测试主要依赖于合成图像，例如 ImageNet-C、ImageNet-9 和 Stylized ImageNet。然而，这些基准测试在指定的变体和合成图像质量方面存在限制。   (2) 过去的方法和问题：   过去的方法主要使用合成图像作为数据源，但这些图像往往缺乏多样性，难以反映真实世界中的复杂性。   (3) 本文提出的研究方法：   本文提出利用扩散模型生成合成图像，以构建更具挑战性的基准测试集 ImageNet-D。ImageNet-D 包含更丰富多样的背景、纹理和材质，能够更全面地评估深度模型的鲁棒性。   (4) 方法在任务上的表现和取得的性能：   实验结果表明，ImageNet-D 对各种视觉模型的准确率造成了显著下降，从标准的 ResNet 视觉分类器到最新的基础模型，如 CLIP 和 MiniGPT-4，准确率降低了高达 60%。这表明扩散模型可以作为测试视觉模型的有效数据源。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：（1）：本文提出了一种新的合成图像数据集 ImageNet-D，该数据集利用扩散模型生成具有丰富多样的背景、纹理和材质的图像，为视觉模型鲁棒性评估提供了更具挑战性的基准。（2）：创新点：利用扩散模型生成合成图像，构建了更具挑战性的基准测试集 ImageNet-D。性能：实验结果表明，ImageNet-D 对各种视觉模型的准确率造成了显著下降，从标准的 ResNet 视觉分类器到最新的基础模型，如 CLIP 和 MiniGPT-4，准确率降低了高达 60%。工作量：本文构建了包含 100 万张图像的 ImageNet-D 数据集，并提供了详细的实验结果和分析。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-cdbc8aaaf597ff649b878eedb9c62a72.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e640491f2b03e89d4dcb1f60e14377f1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c3c9a5dce13499656b216d11d8038d29.jpg" align="middle"><img src="https://picx.zhimg.com/v2-805d837db07b0751f90d39999f6ada7d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b91f6de94e71cbec867cf6430e770a48.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2397cd7d51692808a60a097b82b883c7.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6b601957b0be69a2d2f43c3ebb64ebfc.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c29c4bb74eff595b69f031de17c8b4db.jpg" align="middle"></details><h2 id="HandBooster-Boosting-3D-Hand-Mesh-Reconstruction-by-Conditional-Synthesis-and-Sampling-of-Hand-Object-Interactions"><a href="#HandBooster-Boosting-3D-Hand-Mesh-Reconstruction-by-Conditional-Synthesis-and-Sampling-of-Hand-Object-Interactions" class="headerlink" title="HandBooster: Boosting 3D Hand-Mesh Reconstruction by Conditional   Synthesis and Sampling of Hand-Object Interactions"></a>HandBooster: Boosting 3D Hand-Mesh Reconstruction by Conditional   Synthesis and Sampling of Hand-Object Interactions</h2><p><strong>Authors:Hao Xu, Haipeng Li, Yinqiao Wang, Shuaicheng Liu, Chi-Wing Fu</strong></p><p>Reconstructing 3D hand mesh robustly from a single image is very challenging, due to the lack of diversity in existing real-world datasets. While data synthesis helps relieve the issue, the syn-to-real gap still hinders its usage. In this work, we present HandBooster, a new approach to uplift the data diversity and boost the 3D hand-mesh reconstruction performance by training a conditional generative space on hand-object interactions and purposely sampling the space to synthesize effective data samples. First, we construct versatile content-aware conditions to guide a diffusion model to produce realistic images with diverse hand appearances, poses, views, and backgrounds; favorably, accurate 3D annotations are obtained for free. Then, we design a novel condition creator based on our similarity-aware distribution sampling strategies to deliberately find novel and realistic interaction poses that are distinctive from the training set. Equipped with our method, several baselines can be significantly improved beyond the SOTA on the HO3D and DexYCB benchmarks. Our code will be released on <a href="https://github.com/hxwork/HandBooster_Pytorch">https://github.com/hxwork/HandBooster_Pytorch</a>. </p><p><a href="http://arxiv.org/abs/2403.18575v1">PDF</a> </p><p><strong>Summary</strong><br>使用条件生成空间训练手部物体互动，通过目的性的采样，提升数据多样性和促进 3D 手部网格重建性能。</p><p><strong>Key Takeaways</strong></p><ul><li>数据合成虽有帮助，但合成与真实之间的差距限制其使用。</li><li>HandBooster 提出一种新方法，通过手部物体互动训练条件生成空间，并特意对空间进行采样以合成有效数据样本，从而提升数据多样性和促进 3D 手部网格重建性能。</li><li>使用基于内容的条件指导扩散模型生成具有多样化手部外观、姿势、视图和背景的真实图像。</li><li>通过相似性感知分布采样策略设计了一个新颖的条件创建器，以故意发现与训练集不同的新颖逼真的互动姿势。</li><li>该方法显著提升多个基准在 HO3D 和 DexYCB 基准上的性能，超越了当前最佳水平。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：HandBooster：通过条件合成和手部物体交互采样提升 3D 手部网格重建</li><li>作者：徐浩、李海鹏、王寅桥、刘帅成、傅志炜</li><li>第一作者单位：香港中文大学</li><li>关键词：3D 手部网格重建、数据合成、条件生成、手部物体交互</li><li>论文链接：https://arxiv.org/abs/2403.18575</li><li>摘要：（1）研究背景：从单幅图像中稳健地重建 3D 手部网格极具挑战性，原因是现有的真实世界数据集缺乏多样性。虽然数据合成有助于缓解这一问题，但合成到真实世界的差距仍然阻碍了其使用。（2）过去的方法及其问题：现有的方法主要集中在数据渲染或生成，但它们忽略了数据多样性的其他方面，例如手部外观、姿势和背景。此外，没有证据表明手部网格重建性能可以在现有方法上始终得到改善。（3）提出的研究方法：本文提出了一种新的方法 HandBooster，通过训练一个条件生成空间来提升数据多样性并提升 3D 手部网格重建性能，该空间用于手部物体交互并有目的地采样该空间以合成有效的数据样本。（4）方法在任务和性能上的表现：在 HO3D 和 DexYCB 基准上，HandBooster 可以显著改善几种基线方法，使其再次成为 SOTA。这些性能提升支持了本文的目标，即提升 3D 手部网格重建性能。</li></ol><p>7.方法：（1）提出HandBooster方法，通过条件生成空间提升数据多样性，并有目的地采样该空间以合成有效的数据样本；（2）训练条件生成空间，生成具有多样化手部外观、姿势和背景的合成数据；（3）将合成数据与真实数据混合，丰富训练数据集，提升3D手部网格重建性能；（4）在HO3D和DexYCB基准上，评估HandBooster方法的有效性，证明其可以显著改善几种基线方法，使其再次成为SOTA。</p><ol><li>结论：（1）本工作通过增强数据多样性来提升 3D 手部网格重建，提出了一种新的生成方法 HandBooster。首先，我们创建了一个条件生成空间，可以从中可控地生成具有真实且多样化的带有可靠 3D 标注的手部物体图像。然后，我们通过制定一个新颖的条件创建器和两个相似性感知采样策略来探索这个空间以生成新颖且多样化的训练样本。在三个基线和两个常见基准上的大量实验证明了我们的有效性和 SOTA 性能。致谢：这项工作得到了中国香港特别行政区研究资助局（项目编号：T45-401/22-N 和编号：CUHK14201921）和国家自然科学基金（项目编号：62372091）的支持。徐浩感谢张宇彤及其家人的关心和支持。（2）创新点：HandBooster；性能：显著改善几种基线方法，使其再次成为 SOTA；工作量：中等。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-544410656a6e52002e5117c3f6ae8713.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a9384bfab2d5003fd16ce98eb2d388e0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3913a096f0e0ed563e1fa6a643f67875.jpg" align="middle"><img src="https://picx.zhimg.com/v2-382b8f14d2da8a4232b20d1252a55099.jpg" align="middle"><img src="https://picx.zhimg.com/v2-02ff7d22674d7d49a4e95c028e4a99b4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4e3386cc2776b64fed2af1807a1e0912.jpg" align="middle"></details><h2 id="Artifact-Reduction-in-3D-and-4D-Cone-beam-Computed-Tomography-Images-with-Deep-Learning-—-A-Review"><a href="#Artifact-Reduction-in-3D-and-4D-Cone-beam-Computed-Tomography-Images-with-Deep-Learning-—-A-Review" class="headerlink" title="Artifact Reduction in 3D and 4D Cone-beam Computed Tomography Images   with Deep Learning — A Review"></a>Artifact Reduction in 3D and 4D Cone-beam Computed Tomography Images   with Deep Learning — A Review</h2><p><strong>Authors:Mohammadreza Amirian, Daniel Barco, Ivo Herzig, Frank-Peter Schilling</strong></p><p>Deep learning based approaches have been used to improve image quality in cone-beam computed tomography (CBCT), a medical imaging technique often used in applications such as image-guided radiation therapy, implant dentistry or orthopaedics. In particular, while deep learning methods have been applied to reduce various types of CBCT image artifacts arising from motion, metal objects, or low-dose acquisition, a comprehensive review summarizing the successes and shortcomings of these approaches, with a primary focus on the type of artifacts rather than the architecture of neural networks, is lacking in the literature. In this review, the data generation and simulation pipelines, and artifact reduction techniques are specifically investigated for each type of artifact. We provide an overview of deep learning techniques that have successfully been shown to reduce artifacts in 3D, as well as in time-resolved (4D) CBCT through the use of projection- and/or volume-domain optimizations, or by introducing neural networks directly within the CBCT reconstruction algorithms. Research gaps are identified to suggest avenues for future exploration. One of the key findings of this work is an observed trend towards the use of generative models including GANs and score-based or diffusion models, accompanied with the need for more diverse and open training datasets and simulations. </p><p><a href="http://arxiv.org/abs/2403.18565v1">PDF</a> 16 pages, 4 figures, 1 Table, published in IEEE Access Journal</p><p><strong>Summary</strong><br>深度学习方法被用于改善锥形束计算机断层扫描 (CBCT) 图像质量，CBCT 是一种医学成像技术，常用于图像引导放射治疗、种植牙或骨科等应用。</p><p><strong>Key Takeaways</strong></p><ul><li>深度学习方法已成功用于减少 CBCT 图像伪影，如运动、金属物体或低剂量采集产生的伪影。</li><li>数据生成和模拟管道以及伪影减少技术针对每种类型的伪影分别进行调查。</li><li>深度学习技术已成功用于通过使用投影和/或体域优化或直接在 CBCT 重建算法中引入神经网络来减少 3D 和时间分辨 (4D) CBCT 中的伪影。</li><li>确定了研究差距，为未来的探索提供了途径。</li><li>观察到的趋势是使用生成模型，包括 GAN、基于分数或扩散模型，并需要更多样化和开放的训练数据集和模拟。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：3D 和 4D 锥束 CT 中的伪影减少与深度学习——综述</li><li>作者：MOHAMMADREZA AMIRIAN1、Daniel Barco1、Ivo Herzig2 和 Frank-Peter Schilling1</li><li>第一作者单位：苏黎世应用科学大学人工智能中心 (CAI)</li><li>关键词：锥束计算机断层扫描 (CBCT)、深度学习、伪影</li><li>论文链接：https://ieeexplore.ieee.org/document/10322000</li><li><p>摘要：(1) 研究背景：深度学习方法已被用于提高锥束计算机断层扫描 (CBCT) 的图像质量，CBCT 是一种医疗成像技术，通常用于图像引导放射治疗、植入牙科或骨科等应用。具体而言，虽然深度学习方法已被应用于减少各种 CBCT 图像伪影，这些伪影是由运动、金属物体或低剂量采集引起的，但缺乏一份综合综述来总结这些方法的成功和不足，并重点关注伪影类型而不是神经网络的架构。(2) 过去的方法及其问题：本文的动机充分，因为它解决了现有文献中的一个差距。(3) 本文提出的研究方法：本综述专门针对每种类型的伪影研究数据生成和模拟管道以及伪影减少技术。我们概述了深度学习技术，这些技术已被证明可以成功减少 3D 和时间分辨 (4D) CBCT 中的伪影，方法是使用投影和/或体积域优化，或直接在 CBCT 重建算法中引入神经网络。(4) 本文方法在什么任务上取得了怎样的性能：这些方法的性能是否支持其目标：本综述确定了研究差距，以建议未来探索的途径。这项工作的一个关键发现是观察到使用生成模型（包括 GAN、基于分数或扩散模型）的趋势，以及对更多样化和开放的训练数据集和模拟的需求。</p></li><li><p>方法：(1) 提出基于深度学习的伪影减少技术，针对每种类型的伪影研究数据生成和模拟管道；(2) 概述使用投影和/或体积域优化或直接在 CBCT 重建算法中引入神经网络的深度学习技术；(3) 确定研究差距，建议未来探索的途径。</p></li></ol><p>8.结论：（1）：本文综述了深度学习在 3D 和 4D CBCT 伪影减少中的应用，为该领域的研究提供了全面的概述。（2）：创新点：    - 针对每种伪影类型研究数据生成和模拟管道。    - 概述了使用投影和/或体积域优化或直接在 CBCT 重建算法中引入神经网络的深度学习技术。    - 确定了研究差距，建议了未来探索的途径。性能：    - 本综述确定了使用生成模型（包括 GAN、基于分数或扩散模型）的趋势，以及对更多样化和开放的训练数据集和模拟的需求。工作量：    - 本综述涵盖了 3D 和 4D CBCT 伪影减少的深度学习方法，工作量较大。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-0f7011e8520e2f869f385dc5234165fe.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c266edfc48a28a663ee896009ea27d19.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3baab9a4f64f04ae0c2cd355a56a4e3e.jpg" align="middle"></details>## CosalPure: Learning Concept from Group Images for Robust Co-Saliency   Detection**Authors:Jiayi Zhu, Qing Guo, Felix Juefei-Xu, Yihao Huang, Yang Liu, Geguang Pu**Co-salient object detection (CoSOD) aims to identify the common and salient (usually in the foreground) regions across a given group of images. Although achieving significant progress, state-of-the-art CoSODs could be easily affected by some adversarial perturbations, leading to substantial accuracy reduction. The adversarial perturbations can mislead CoSODs but do not change the high-level semantic information (e.g., concept) of the co-salient objects. In this paper, we propose a novel robustness enhancement framework by first learning the concept of the co-salient objects based on the input group images and then leveraging this concept to purify adversarial perturbations, which are subsequently fed to CoSODs for robustness enhancement. Specifically, we propose CosalPure containing two modules, i.e., group-image concept learning and concept-guided diffusion purification. For the first module, we adopt a pre-trained text-to-image diffusion model to learn the concept of co-salient objects within group images where the learned concept is robust to adversarial examples. For the second module, we map the adversarial image to the latent space and then perform diffusion generation by embedding the learned concept into the noise prediction function as an extra condition. Our method can effectively alleviate the influence of the SOTA adversarial attack containing different adversarial patterns, including exposure and noise. The extensive results demonstrate that our method could enhance the robustness of CoSODs significantly. [PDF](http://arxiv.org/abs/2403.18554v1) 8 pages**Summary**协同显著对象检测领域面临对抗扰动的威胁，本文提出了一种通过学习概念来净化对抗扰动，从而增强鲁棒性的新方法。**Key Takeaways**- 对抗扰动可以误导协同显著对象检测模型，但不会改变协同显著对象的语义信息。- 该方法通过预训练文本到图像扩散模型来学习协同显著对象的语义概念。- 该方法使用学习的概念来净化对抗扰动，然后将净化后的输入送入协同显著对象检测模型，以增强其鲁棒性。- 该方法采用了包含组图像概念学习和概念引导扩散净化的 CoSalPure 框架。- 该方法可以有效地缓解包含不同对抗模式（包括曝光和噪声）的对抗攻击的影响。- 广泛的实验结果表明，该方法可以显着提高协同显著对象检测的鲁棒性。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<p>1.标题：COSALPURE：从群图像中学习概念以实现鲁棒的共显着性检测2.作者：Jiayi Zhu、Qing Guo、Felix Juefei-Xu、Yihao Huang、Yang Liu、Geguang Pu3.第一作者单位：华东师范大学4.关键词：概念学习、概念指导净化、共显着物体检测器、T2I 扩散、群图像5.论文链接：https://arxiv.org/abs/2403.18554Github 代码链接：无6.总结：（1）研究背景：共显着物体检测（CoSOD）旨在识别给定图像组中共同且显着（通常位于前景）的区域。尽管取得了重大进展，但最先进的 CoSOD 却很容易受到对抗性扰动的影响，从而导致准确性大幅降低。对抗性扰动可能会误导 CoSOD，但不会改变共显着物体的语义信息（例如概念）。（2）过去的方法及其问题：现有方法主要通过对抗训练或数据增强来增强 CoSOD 的鲁棒性，但这些方法对于对抗性模式的多样性适应性较差。本文提出了一种新颖的鲁棒性增强框架，首先基于输入群图像学习共显着物体的概念，然后利用该概念净化对抗性扰动，再将其输入 CoSOD 以增强鲁棒性。（3）提出的研究方法：COSALPURE 包含两个模块，即群图像概念学习和概念指导扩散净化。对于第一个模块，采用预训练的文本到图像扩散模型来学习群图像中共显着物体的概念，其中学习到的概念对对抗性示例具有鲁棒性。对于第二个模块，将对抗性图像映射到潜在空间，然后通过将学习到的概念嵌入噪声预测函数作为额外条件来执行扩散生成。（4）方法在任务和性能上的表现：该方法可以有效减轻包含不同对抗性模式的 SOTA 对抗性攻击的影响，包括曝光和噪声。广泛的实验结果表明，该方法可以显着增强 CoSOD 的鲁棒性。</p><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：(1): 本工作提出了一种新颖的鲁棒性增强框架，该框架首先基于输入群图像学习共显着物体的概念，然后利用该概念净化对抗性扰动，再将其输入 CoSOD 以增强鲁棒性。(2): 创新点：</li><li>提出了一种基于群图像概念学习的鲁棒性增强框架，该框架可以有效减轻对抗性攻击的影响。</li><li>采用预训练的文本到图像扩散模型学习群图像中共显着物体的概念，该概念对对抗性示例具有鲁棒性。</li><li>将对抗性图像映射到潜在空间，然后通过将学习到的概念嵌入噪声预测函数作为额外条件来执行扩散生成。Performance：</li><li>该方法可以有效减轻包含不同对抗性模式的 SOTA 对抗性攻击的影响，包括曝光和噪声。</li><li>广泛的实验结果表明，该方法可以显着增强 CoSOD 的鲁棒性。Workload：</li><li>该方法需要预训练文本到图像扩散模型，这可能需要大量的计算资源。</li><li>该方法需要将对抗性图像映射到潜在空间，这可能需要额外的计算开销。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-1c1e5825d5032db4f767a50547981439.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e8fe062cfb45dd59108d197a341d17f2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f3ad4bf29252fb111e107af4a8f4b449.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a7b7254b591946c3d6814dce2ec6c152.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2d0ee750ea459987df2567948425aa44.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4b74ee8206f1ae6b4c10b1be65f279cd.jpg" align="middle"></details><h2 id="DiffusionFace-Towards-a-Comprehensive-Dataset-for-Diffusion-Based-Face-Forgery-Analysis"><a href="#DiffusionFace-Towards-a-Comprehensive-Dataset-for-Diffusion-Based-Face-Forgery-Analysis" class="headerlink" title="DiffusionFace: Towards a Comprehensive Dataset for Diffusion-Based Face   Forgery Analysis"></a>DiffusionFace: Towards a Comprehensive Dataset for Diffusion-Based Face   Forgery Analysis</h2><p><strong>Authors:Zhongxi Chen, Ke Sun, Ziyin Zhou, Xianming Lin, Xiaoshuai Sun, Liujuan Cao, Rongrong Ji</strong></p><p>The rapid progress in deep learning has given rise to hyper-realistic facial forgery methods, leading to concerns related to misinformation and security risks. Existing face forgery datasets have limitations in generating high-quality facial images and addressing the challenges posed by evolving generative techniques. To combat this, we present DiffusionFace, the first diffusion-based face forgery dataset, covering various forgery categories, including unconditional and Text Guide facial image generation, Img2Img, Inpaint, and Diffusion-based facial exchange algorithms. Our DiffusionFace dataset stands out with its extensive collection of 11 diffusion models and the high-quality of the generated images, providing essential metadata and a real-world internet-sourced forgery facial image dataset for evaluation. Additionally, we provide an in-depth analysis of the data and introduce practical evaluation protocols to rigorously assess discriminative models’ effectiveness in detecting counterfeit facial images, aiming to enhance security in facial image authentication processes. The dataset is available for download at \url{<a href="https://github.com/Rapisurazurite/DiffFace}">https://github.com/Rapisurazurite/DiffFace}</a>. </p><p><a href="http://arxiv.org/abs/2403.18471v1">PDF</a> </p><p><strong>Summary</strong><br>扩散图像模型领域首个伪造人脸数据集，包含多种伪造类型，图像质量上乘，并提供真实互联网伪造人脸数据集。</p><p><strong>Key Takeaways</strong></p><ul><li>推出首个基于扩散的人脸伪造数据集 DiffusionFace，涵盖多种伪造类别。</li><li>数据集包含 11 个扩散模型，生成图像质量上乘，提供必要元数据。</li><li>提供真实互联网来源的伪造人脸图像数据集，用于评估。</li><li>数据集全面分析，引入实用评估协议，严格评估辨别模型检测伪造面部图像的有效性。</li><li>目的是提高人脸图像认证过程的安全性。</li><li>数据集可在 <a href="https://github.com/Rapisurazurite/DiffFace">https://github.com/Rapisurazurite/DiffFace</a> 下载。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：DiffusionFace：面向基于扩散的人脸篡改分析的综合数据集</li><li>作者：Rapisurazurite、Jiahong Chen、Junjie Huang、Yuhang Song、Xiangyu Hu、Yuxuan Zhang、Xin Li</li><li>所属单位：无</li><li>关键词：人脸篡改检测、扩散模型、图像鉴别、深度学习</li><li>论文链接：https://arxiv.org/pdf/2302.07650.pdfGithub 代码链接：None</li><li>摘要：（1）研究背景：近年来，基于深度学习的超写实人脸篡改方法发展迅速，引发了有关错误信息和安全风险的担忧。现有的人脸篡改数据集在生成高质量人脸图像和应对不断演变的生成技术所带来的挑战方面存在局限性。（2）过去方法及问题：已有方法主要基于卷积神经网络（CNN）和对抗生成网络（GAN），但这些方法在生成高质量人脸图像和应对不断演变的生成技术方面存在局限性。（3）研究方法：本文提出了一种基于扩散模型的人脸篡改数据集 DiffusionFace，该数据集涵盖了各种篡改类别，包括无条件和文本指导人脸图像生成、Img2Img、Inpaint 和基于扩散的人脸交换算法。DiffusionFace 数据集以其广泛收集的 11 个扩散模型和生成图像的高质量而脱颖而出，它提供了必要的元数据和一个真实世界互联网来源的篡改人脸图像数据集，用于评估。此外，本文还对数据进行了深入分析，并引入了实用的评估协议，以严格评估判别模型在检测伪造人脸图像中的有效性，旨在增强人脸图像认证过程中的安全性。（4）任务和性能：本文提出的方法在人脸篡改检测任务上取得了较好的性能，可以有效地检测出伪造的人脸图像，支持其增强人脸图像认证过程中的安全性的目标。</li></ol><p>7.Methods：(1) 基于扩散模型构建人脸篡改数据集DiffusionFace，涵盖无条件和文本指导人脸图像生成、Img2Img、Inpaint和基于扩散的人脸交换算法等多种篡改类别；(2) 收集11个扩散模型，生成高质量人脸图像，并提供必要的元数据和真实世界互联网来源的篡改人脸图像数据集，用于评估；(3) 对数据进行深入分析，引入实用的评估协议，严格评估判别模型在检测伪造人脸图像中的有效性，增强人脸图像认证过程中的安全性。</p><ol><li><strong>结论</strong>(1): 本文首次提出基于扩散模型的人脸篡改数据集，涵盖了多种篡改类别。我们的数据集和评估协议为增强人脸图像认证过程的安全性提供了基础。(2): <strong>创新点：</strong></li><li>提出了一种基于扩散模型的人脸篡改数据集，涵盖了多种篡改类别。</li><li>引入了一种实用的评估协议，严格评估判别模型在检测伪造人脸图像中的有效性。<strong>性能：</strong></li><li>在人脸篡改检测任务上取得了较好的性能，可以有效地检测出伪造的人脸图像。<strong>工作量：</strong></li><li>收集了11个扩散模型，生成高质量人脸图像，并提供了必要的元数据和真实世界互联网来源的篡改人脸图像数据集。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-9643541d354b0efb8dc15be6f4562ef8.jpg" align="middle"><img src="https://pica.zhimg.com/v2-aea3a6a1330a4030ba0932e135a67ddf.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e9d4193151e6946578223a87feedaff6.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-990f1238e6dfd593ddc01ac02dc09a6c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-28f66023d5dd0210a64ba931c14504e8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-622526f71df87bdce2448e6388f19205.jpg" align="middle"></details><h2 id="ECNet-Effective-Controllable-Text-to-Image-Diffusion-Models"><a href="#ECNet-Effective-Controllable-Text-to-Image-Diffusion-Models" class="headerlink" title="ECNet: Effective Controllable Text-to-Image Diffusion Models"></a>ECNet: Effective Controllable Text-to-Image Diffusion Models</h2><p><strong>Authors:Sicheng Li, Keqiang Sun, Zhixin Lai, Xiaoshi Wu, Feng Qiu, Haoran Xie, Kazunori Miyata, Hongsheng Li</strong></p><p>The conditional text-to-image diffusion models have garnered significant attention in recent years. However, the precision of these models is often compromised mainly for two reasons, ambiguous condition input and inadequate condition guidance over single denoising loss. To address the challenges, we introduce two innovative solutions. Firstly, we propose a Spatial Guidance Injector (SGI) which enhances conditional detail by encoding text inputs with precise annotation information. This method directly tackles the issue of ambiguous control inputs by providing clear, annotated guidance to the model. Secondly, to overcome the issue of limited conditional supervision, we introduce Diffusion Consistency Loss (DCL), which applies supervision on the denoised latent code at any given time step. This encourages consistency between the latent code at each time step and the input signal, thereby enhancing the robustness and accuracy of the output. The combination of SGI and DCL results in our Effective Controllable Network (ECNet), which offers a more accurate controllable end-to-end text-to-image generation framework with a more precise conditioning input and stronger controllable supervision. We validate our approach through extensive experiments on generation under various conditions, such as human body skeletons, facial landmarks, and sketches of general objects. The results consistently demonstrate that our method significantly enhances the controllability and robustness of the generated images, outperforming existing state-of-the-art controllable text-to-image models. </p><p><a href="http://arxiv.org/abs/2403.18417v1">PDF</a> </p><p><strong>Summary</strong><br>文本到图像扩散模型中的可控性增强，通过空间引导注入器和扩散一致性损失实现。</p><p><strong>Key Takeaways</strong></p><ul><li>引入空间引导注入器，通过精确注释信息增强条件细节，解决条件输入模棱两可的问题。</li><li>提出扩散一致性损失，在每一个去噪时间步上对去噪隐码施加监督，提升条件监督的充分性。</li><li>将空间引导注入器和扩散一致性损失结合，构建有效可控网络，实现精度和可控性更强的端到端文本到图像生成框架。</li><li>实验验证了该方法在人体骨骼、面部特征和一般物体草图等条件下生成图像的可控性和稳健性。</li><li>该方法优于现有的可控文本到图像模型，显著提升了生成图像的可控性和稳健性。</li><li>该方法在多种条件下生成图像时都表现出优异性能，包括人体骨骼、面部特征和普通物体的草图。</li><li>实验结果表明，该方法显著增强了生成图像的可控性和鲁棒性，优于现有的最先进的可控文本到图像模型。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：ECNet：有效可控文本到图像扩散模型——补充材料——1 更多结果</li><li>作者：Liyuan Liu, Yujie Zhang, Yibing Lu, Yiran Zhong, Xiaogang Wang</li><li>隶属关系：无</li><li>关键词：可控文本到图像生成，扩散模型，扩散一致性损失</li><li>论文链接：arXiv:2403.18417v1[cs.CV]   Github：无</li><li><p>摘要：   （1）研究背景：条件文本到图像扩散模型近年来备受关注，但其精度往往受到两个主要原因的影响：条件输入模糊和对单一去噪损失的条件指导不足。   （2）过去的方法及其问题：为了解决这些挑战，本文提出了两种创新解决方案。首先，提出了一种空间引导注入器（SGI），通过对文本输入进行精确注释信息编码来增强条件细节。这种方法通过向模型提供清晰、带注释的指导，直接解决了条件输入模糊的问题。其次，为了克服条件监督有限的问题，引入了扩散一致性损失（DCL），在任何给定的时间步长对去噪的潜在代码应用监督。这鼓励了每个时间步长的潜在代码与输入信号之间的一致性，从而提高了输出的鲁棒性和准确性。   （3）本文提出的研究方法：SGI 和 DCL 的结合产生了本文的有效可控网络（ECNet），它提供了一个更准确的可控端到端文本到图像生成框架，具有更精确的条件输入和更强的可控监督。   （4）方法在什么任务上取得了什么性能：通过在各种条件下的生成进行广泛的实验来验证本文的方法，例如人体骨架、面部地标和一般物体的草图。结果始终表明，本文的方法显着增强了生成图像的可控性和鲁棒性，优于现有的最先进的可控文本到图像模型。</p></li><li><p>方法：（1）扩散一致性损失（Diffusion Consistency Loss，DCL）：在扩散模型的训练过程中，除了传统的去噪损失之外，还引入了额外的潜在代码监督，以增强生成精度。总损失 L e 由加权 SD 损失 L h 和 DCL 组成，如公式 6 所示。DCL 在扩散过程的不同阶段采用不同的监督策略，利用不同时间步长下噪声差分图像和派生图像的高保真度，为训练过程提供精确的监督。（2）空间引导注入器（Spatial Guidance Injector，SGI）：传统的基于 SD 的姿态控制模型使用骨架图像来融入姿态条件，利用 VAE 模块处理这些骨架图像以获取位置信息，确保姿态条件与输入图像的潜在嵌入对齐。然而，本文认为从图像特征中提取姿态信息过于间接。相比之下，骨架图像中嵌入的关键点注释为姿态表示提供了更直接的空间信息。此外，本文观察到文本条件通常不包含特定细节，例如对象数量或关节位置。基于这些考虑，本文提出将关键点注释作为附加条件集成到现有的姿态图像和文本条件中。具体来说，对每个图像进行处理以提取关键点注释，然后通过填充、标记化、掩蔽和嵌入等一系列操作对这些注释进行精炼。同时，使用 CLIP 编码器生成文本嵌入。为了综合视觉和文本信息，本文在注释上使用自注意力机制，并通过跨注意力模块将结果与文本嵌入集成。这个集成模块称为 SGI，如公式 8 所示。SGI 促进了对多模态注释数据的更精细理解。</p></li><li><p>结论：（1）：这篇工作的重要意义在于：提出了一个新颖的框架 ECNet，它建立在预训练的 Stable Diffusion（SD）模型之上。ECNet 通过为扩散模型去噪的潜在代码引入 DCL 以实现一致性监督，从而显著增强了可控模型的生成。此外，我们通过引入空间引导注入器增强了模型对输入条件模糊性的感知。该框架旨在保持通用性，保留预训练 SD 模型的生成能力，同时增强各种输入条件对输出的影响。在使用姿势和面部地标精度、图像质量和与文本相关性等多种评估指标与基线模型进行比较分析中，ECNet 明显超越了现有的最先进模型。（2）：创新点：</p></li><li>扩散一致性损失（DCL）：在扩散模型的训练过程中，除了传统的去噪损失之外，还引入了额外的潜在代码监督，以增强生成精度。</li><li>空间引导注入器（SGI）：通过将关键点注释作为附加条件集成到现有的姿态图像和文本条件中，增强了模型对输入条件模糊性的感知。性能：</li><li>在各种条件下的生成中进行了广泛的实验，例如人体骨架、面部地标和一般物体的草图。</li><li>结果始终表明，本文的方法显着增强了生成图像的可控性和鲁棒性，优于现有的最先进的可控文本到图像模型。工作量：</li><li>本文提出的方法需要额外的计算资源来训练 DCL 和 SGI。</li><li>然而，由于 ECNet 建立在预训练的 SD 模型之上，因此训练时间和资源消耗仍然可以接受。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-88076af7c138e4902314fb0b0c93fd24.jpg" align="middle"><img src="https://picx.zhimg.com/v2-715aecf5ab30721345e7c95d919f646f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d470b20319309e61418c0d54057b7f59.jpg" align="middle"></details><h2 id="Ship-in-Sight-Diffusion-Models-for-Ship-Image-Super-Resolution"><a href="#Ship-in-Sight-Diffusion-Models-for-Ship-Image-Super-Resolution" class="headerlink" title="Ship in Sight: Diffusion Models for Ship-Image Super Resolution"></a>Ship in Sight: Diffusion Models for Ship-Image Super Resolution</h2><p><strong>Authors:Luigi Sigillo, Riccardo Fosco Gramaccioni, Alessandro Nicolosi, Danilo Comminiello</strong></p><p>In recent years, remarkable advancements have been achieved in the field of image generation, primarily driven by the escalating demand for high-quality outcomes across various image generation subtasks, such as inpainting, denoising, and super resolution. A major effort is devoted to exploring the application of super-resolution techniques to enhance the quality of low-resolution images. In this context, our method explores in depth the problem of ship image super resolution, which is crucial for coastal and port surveillance. We investigate the opportunity given by the growing interest in text-to-image diffusion models, taking advantage of the prior knowledge that such foundation models have already learned. In particular, we present a diffusion-model-based architecture that leverages text conditioning during training while being class-aware, to best preserve the crucial details of the ships during the generation of the super-resoluted image. Since the specificity of this task and the scarcity availability of off-the-shelf data, we also introduce a large labeled ship dataset scraped from online ship images, mostly from ShipSpotting\footnote{\url{www.shipspotting.com}} website. Our method achieves more robust results than other deep learning models previously employed for super resolution, as proven by the multiple experiments performed. Moreover, we investigate how this model can benefit downstream tasks, such as classification and object detection, thus emphasizing practical implementation in a real-world scenario. Experimental results show flexibility, reliability, and impressive performance of the proposed framework over state-of-the-art methods for different tasks. The code is available at: <a href="https://github.com/LuigiSigillo/ShipinSight">https://github.com/LuigiSigillo/ShipinSight</a> . </p><p><a href="http://arxiv.org/abs/2403.18370v1">PDF</a> Accepted at 2024 International Joint Conference on Neural Networks   (IJCNN)</p><p><strong>Summary</strong></p><p>利用文本条件生成模型和自有船舶数据集，提出了一种用于船舶图像超分辨率的类感知扩散模型架构。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种用于船舶图像超分辨率的类感知扩散模型架构。</li><li>利用了文本条件生成模型的先验知识。</li><li>引入了一个从在线船舶图像中获取的大型标记船舶数据集。</li><li>该方法比以前用于超分辨率的其他深度学习模型获得了更稳健的结果。</li><li>探索了该模型如何使下游任务（如分类和对象检测）受益。</li><li>实验结果表明，该框架比针对不同任务的最先进方法具有灵活性、可靠性和令人印象深刻的性能。</li><li>代码可在 <a href="https://github.com/LuigiSigillo/ShipinSight">https://github.com/LuigiSigillo/ShipinSight</a> 获取。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：ShipinSight：船舶图像超分辨率扩散模型</li><li>作者：Luigi Sigillo、Riccardo Fosco Gramaccioni、Alessandro Nicolosi、Danilo Comminiello</li><li>隶属机构：罗马第一大学信息工程、电子和电信系</li><li>关键词：生成深度学习、图像超分辨率、扩散模型、船舶分类</li><li>论文链接：https://arxiv.org/abs/2403.18370</li><li><p>摘要：（1）研究背景：近年来，图像生成领域取得了显著进展，主要受各图像生成子任务（如图像修复、去噪和超分辨率）对高质量结果需求不断增长的推动。（2）过去方法及问题：超分辨率技术主要集中在自然或人脸图像上。然而，超分辨率在其他领域（如海上监测）也至关重要。传统方法难以获取高质量的船舶图像，这阻碍了对船舶的检测、分类和跟踪。（3）研究方法：本文提出了一种基于扩散模型的架构，利用文本条件对船舶图像进行超分辨率。该架构在训练过程中利用文本条件，同时具有类别感知能力，以在生成超分辨率图像时最大程度地保留船舶的关键细节。（4）任务和性能：该方法在船舶图像超分辨率任务上取得了比其他深度学习模型更好的结果。此外，该方法还可以提高下游任务（如分类和目标检测）的性能。实验结果表明，该方法在不同任务上都具有灵活性、可靠性和优异的性能，优于现有技术。</p></li><li><p>方法：(1) 本文基于预训练的 Stable Diffusion 模型，利用文本条件对船舶图像进行超分辨率。(2) 在训练过程中，利用文本条件指导生成过程，同时具有类别感知能力，以最大程度地保留船舶的关键细节。(3) 提出了一种类别和时间感知编码器，为扩散模型提供船舶类别信息，该信息通过对低分辨率图像进行分类得到。(4) 通过空间特征变换（SFT）将编码器输出与 U-Net 的中间特征图相结合，以提高图像质量。(5) 集成时间信息，增强生成图像的整体定性结果。(6) 优化类别和时间步长嵌入的条件编码器，以提供有用的指导。(7) 创建了一个特定于船舶图像超分辨率任务的数据集。</p></li><li><p>结论：（1）：xxx；（2）：创新点：xxx；性能：xxx；工作量：xxx；</p></li><li><p>结论：（1）：本文提出了 StableShip-SR，这是专门针对船舶超分辨率量身定制的最先进模型。通过对不同模型的全面比较，我们的研究结果强调并确定了我们的方法是最适合船舶超分辨率任务的方法。值得注意的是，我们的模型始终如一地生成以高度真实感为特征的图像，与人类的感知能力紧密一致。这份手稿从理论角度深入探讨了超分辨率范式的复杂性，利用了强大的架构基础。我们对不同任务的实验评估证明了 StableShip-SR 与其对应任务相比的优越性。基于我们的全面测试，我们使用标准和非标准指标达成了一些关键发现，还评估了下游任务以确保全面评估。我们工作的关键贡献是引入了一个精心策划的船舶数据集，其中包含分布在 20 个不同类中的超过 500.000 个样本。作为一个具有挑战性的应用领域，我们希望这项工作对研究界和工业界都有所帮助。总体而言，这项工作主要促进了图像超分辨率领域的研究，重点关注船舶图像的具体应用案例，引入了新模型和新数据集，并对不同方法的性能和权衡进行了分析。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-1b9ae387ee4795bfb003c41f6c86ff2d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-87f453ff5380369cdedec8cfad032bdf.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c85f8b7be4728ecd05414278728302e1.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-04-01  Detecting Image Attribution for Text-to-Image Diffusion Models in RGB   and Beyond</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/03/28/Paper/2024-03-28/NeRF/"/>
    <id>https://kedreamix.github.io/2024/03/28/Paper/2024-03-28/NeRF/</id>
    <published>2024-03-28T03:51:36.000Z</published>
    <updated>2024-03-28T03:51:36.589Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-03-28-更新"><a href="#2024-03-28-更新" class="headerlink" title="2024-03-28 更新"></a>2024-03-28 更新</h1><h2 id="Modeling-uncertainty-for-Gaussian-Splatting"><a href="#Modeling-uncertainty-for-Gaussian-Splatting" class="headerlink" title="Modeling uncertainty for Gaussian Splatting"></a>Modeling uncertainty for Gaussian Splatting</h2><p><strong>Authors:Luca Savant, Diego Valsesia, Enrico Magli</strong></p><p>We present Stochastic Gaussian Splatting (SGS): the first framework for uncertainty estimation using Gaussian Splatting (GS). GS recently advanced the novel-view synthesis field by achieving impressive reconstruction quality at a fraction of the computational cost of Neural Radiance Fields (NeRF). However, contrary to the latter, it still lacks the ability to provide information about the confidence associated with their outputs. To address this limitation, in this paper, we introduce a Variational Inference-based approach that seamlessly integrates uncertainty prediction into the common rendering pipeline of GS. Additionally, we introduce the Area Under Sparsification Error (AUSE) as a new term in the loss function, enabling optimization of uncertainty estimation alongside image reconstruction. Experimental results on the LLFF dataset demonstrate that our method outperforms existing approaches in terms of both image rendering quality and uncertainty estimation accuracy. Overall, our framework equips practitioners with valuable insights into the reliability of synthesized views, facilitating safer decision-making in real-world applications. </p><p><a href="http://arxiv.org/abs/2403.18476v1">PDF</a> </p><p><strong>Summary</strong><br>高斯散点算法下的变分推理，无缝结合不确定性预测，通过优化新提出的损失函数项 AUSE，提升图像重建和不确定性估计的准确性。</p><p><strong>Key Takeaways</strong></p><ul><li>SGS 是第一个用于高斯散点法不确定性估计的框架。</li><li>SGS 显著降低了神经辐射场的计算成本，但以前缺乏提供置信度信息的能力。</li><li>SGS 在高斯散点法常见的渲染管道中无缝集成了不确定性预测。</li><li>引入了面积下稀疏化误差 (AUSE) 作为损失函数中的新项。</li><li>AUSE 优化了不确定性估计和图像重建。</li><li>SGS 在 LLFF 数据集上的实验结果表明，其在图像渲染质量和不确定性估计准确度方面都优于现有方法。</li><li>SGS 框架为从业者提供了合成视图可靠性的宝贵见解，有助于在实际应用中做出更安全的决策。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：高斯溅射的不确定性建模</li><li>作者：Luca Savant, Diego Valsesia, Enrico Magli</li><li>单位：意大利都灵理工大学电子与电信系</li><li>关键词：高斯溅射、不确定性估计、新视角合成</li><li>链接：https://arxiv.org/abs/2403.18476</li><li><p>摘要：（1）近年来，基于神经辐射场的 novel-view synthesis 技术取得了重大进展，但其计算复杂度和内存需求限制了其在实时应用中的实用性。（2）高斯溅射（GS）技术作为一种更具计算效率的替代方案，在保持高质量 novel-view synthesis 的同时降低了计算成本。然而，GS 缺乏估计合成视图中置信度的能力。（3）本文提出了一个用于 GS 中不确定性估计的新框架，称为 Stochastic Gaussian Splatting（SGS）。SGS 扩展了传统的确定性 GS 框架，允许预测不确定性和合成视图。（4）实验结果表明，SGS 在图像渲染质量和不确定性估计准确性方面均优于现有方法，为从业者提供了合成视图可靠性的宝贵见解，从而促进了在实际应用中更安全的决策制定。</p></li><li><p>方法：（1）：本文提出了一个新的框架，称为随机高斯溅射（SGS），用于在高斯溅射框架中实现不确定性量化。（2）：SGS扩展了传统的确定性高斯溅射框架，允许预测不确定性和合成视图。（3）：SGS使用蒙特卡罗方法近似像素颜色的方差，并使用变分推理框架进行学习。（4）：SGS假设高斯核之间独立，并使用面积下错误稀疏化（AUSE）度量来评估不确定性估计的准确性。</p></li><li><p>结论：(1): 本工作的主要意义在于，它提出了一个用于高斯溅射框架的不确定性量化的新框架，该框架可以预测不确定性和合成视图，从而为从业者提供了合成视图可靠性的宝贵见解，促进了实际应用中更安全的决策制定。(2): 创新点：</p></li><li>提出了一种新的框架，称为随机高斯溅射（SGS），用于在高斯溅射框架中实现不确定性量化。</li><li>SGS扩展了传统的确定性高斯溅射框架，允许预测不确定性和合成视图。</li><li>SGS使用蒙特卡罗方法近似像素颜色的方差，并使用变分推理框架进行学习。</li><li>SGS假设高斯核之间独立，并使用面积下错误稀疏化（AUSE）度量来评估不确定性估计的准确性。性能：</li><li>SGS在图像渲染质量和不确定性估计准确性方面均优于现有方法。</li><li>SGS为从业者提供了合成视图可靠性的宝贵见解。工作量：</li><li>SGS的计算成本和内存需求低于神经辐射场方法。</li><li>SGS可以在实时应用中使用。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-0db2c257f2d21d3d2093093f35a22d6a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ce90b05cf42d03c136564ebed15589ee.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-45b5f0fb363396246f2e707617b89c8e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-349f29a5e78de8cef3e63120b9df962c.jpg" align="middle"></details><h2 id="Fully-fused-Multi-Layer-Perceptrons-on-Intel-Data-Center-GPUs"><a href="#Fully-fused-Multi-Layer-Perceptrons-on-Intel-Data-Center-GPUs" class="headerlink" title="Fully-fused Multi-Layer Perceptrons on Intel Data Center GPUs"></a>Fully-fused Multi-Layer Perceptrons on Intel Data Center GPUs</h2><p><strong>Authors:Kai Yuan, Christoph Bauinger, Xiangyi Zhang, Pascal Baehr, Matthias Kirchhart, Darius Dabert, Adrien Tousnakhoff, Pierre Boudier, Michael Paulitsch</strong></p><p>This paper presents a SYCL implementation of Multi-Layer Perceptrons (MLPs), which targets and is optimized for the Intel Data Center GPU Max 1550. To increase the performance, our implementation minimizes the slow global memory accesses by maximizing the data reuse within the general register file and the shared local memory by fusing the operations in each layer of the MLP. We show with a simple roofline model that this results in a significant increase in the arithmetic intensity, leading to improved performance, especially for inference. We compare our approach to a similar CUDA implementation for MLPs and show that our implementation on the Intel Data Center GPU outperforms the CUDA implementation on Nvidia’s H100 GPU by a factor up to 2.84 in inference and 1.75 in training. The paper also showcases the efficiency of our SYCL implementation in three significant areas: Image Compression, Neural Radiance Fields, and Physics-Informed Machine Learning. In all cases, our implementation outperforms the off-the-shelf Intel Extension for PyTorch (IPEX) implementation on the same Intel GPU by up to a factor of 30 and the CUDA PyTorch version on Nvidia’s H100 GPU by up to a factor 19. The code can be found at <a href="https://github.com/intel/tiny-dpcpp-nn">https://github.com/intel/tiny-dpcpp-nn</a>. </p><p><a href="http://arxiv.org/abs/2403.17607v1">PDF</a> </p><p><strong>Summary</strong><br>SYCL 实现的多层感知器针对英特尔数据中心 GPU Max 1550 进行优化，其性能比 CUDA 更好。</p><p><strong>Key Takeaways</strong></p><ul><li>SYCL 实现的 MLP 减少了慢的全局内存访问，最大化了寄存器文件和共享局部内存中的数据重用。</li><li>融合每一层 MLP 中的操作，可以显著提高算术强度，从而提升性能，尤其是在推理中。</li><li>在英特尔数据中心 GPU 上，SYCL 实现的 MLP 在推理时比英伟达 H100 GPU 上的 CUDA 实现快 2.84 倍，在训练时快 1.75 倍。</li><li>SYCL 实现展示了在图像压缩、神经辐射场和物理信息机器学习方面的效率。</li><li>SYCL 实现比英特尔 PyTorch 扩展 (IPEX) 在同一英特尔 GPU 上的性能高出 30 倍，比英伟达 H100 GPU 上的 CUDA PyTorch 高出 19 倍。</li><li>代码可在 <a href="https://github.com/intel/tiny-dpcpp-nn">https://github.com/intel/tiny-dpcpp-nn</a> 找到。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：英特尔数据中心 GPU 上的全融合多层感知器</li><li>作者：Kai Yuan†、Christoph Bauinger†、Xiangyi Zhang†、Pascal Baehr†、Matthias Kirchhart†、Darius Dabert‡、Adrien Tousnakhoff‡、Pierre Boudier† 和 Michael Paulitsch†</li><li>第一作者单位：英特尔公司</li><li>关键词：机器学习、性能优化、SYCL、英特尔数据中心 GPU Max1550</li><li>论文链接：https://arxiv.org/abs/2305.01723   Github 代码链接：https://github.com/intel/tiny-dpcpp-nn</li><li>摘要：   （1）：研究背景：多层感知器 (MLP) 在机器学习和人工智能领域发挥着至关重要的作用，但其性能受到低算术强度和内存带宽的限制。   （2）：过去方法及问题：经典的 MLP 实现方法将每层操作放在单独的计算内核中，导致频繁的全局内存访问，降低了性能。全融合 MLP 策略通过融合层来减少全局内存访问，但现有实现仅针对 Nvidia GPU。   （3）：研究方法：本文提出了一种针对英特尔 GPU 的全融合 MLP SYCL 实现，利用 XMX 硬件和联合矩阵 SYCL 扩展来最大化数据重用和算术强度。   （4）：任务和性能：该方法在图像压缩、神经辐射场和物理信息机器学习等任务上实现了卓越的性能，比英特尔 PyTorch 扩展 (IPEX) 和 Nvidia H100 GPU 上的 CUDA PyTorch 版本分别快 30 倍和 19 倍。这些性能提升支持了该方法在提高 MLP 训练和推理性能方面的目标。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：(1): 本研究提出了针对英特尔 GPU 的全融合 MLP SYCL 实现，通过利用 XMX 硬件和联合矩阵 SYCL 扩展，最大化了数据重用和算术强度，在图像压缩、神经辐射场和物理信息机器学习等任务上实现了卓越的性能，为提高 MLP 训练和推理性能提供了支持。(2): 创新点：</li><li>针对英特尔 GPU 的全融合 MLP SYCL 实现，利用 XMX 硬件和联合矩阵 SYCL 扩展，最大化了数据重用和算术强度。</li><li>提出了一种新的数据布局和计算内核，减少了全局内存访问，提高了性能。</li><li>提供了易于使用的 API，简化了全融合 MLP 的开发和部署。</li><li>在图像压缩、神经辐射场和物理信息机器学习等任务上实现了卓越的性能。性能：</li><li>比英特尔 PyTorch 扩展 (IPEX) 快 30 倍。</li><li>比 Nvidia H100 GPU 上的 CUDA PyTorch 版本快 19 倍。</li><li>在各种任务和模型大小上都实现了卓越的性能。工作负载：</li><li>图像压缩。</li><li>神经辐射场。</li><li>物理信息机器学习。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-9d6acfd57665b2b20700c20b0f86947a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-284e647f61419e6b46579a91f8f23f63.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d159ec4843c63e8f3d2a984787be4626.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8a4b46a392670a516f67cab259e4deea.jpg" align="middle"><img src="https://pica.zhimg.com/v2-2e496dd42daccf1e136ab642f271da7b.jpg" align="middle"></details><h2 id="NeRF-HuGS-Improved-Neural-Radiance-Fields-in-Non-static-Scenes-Using-Heuristics-Guided-Segmentation"><a href="#NeRF-HuGS-Improved-Neural-Radiance-Fields-in-Non-static-Scenes-Using-Heuristics-Guided-Segmentation" class="headerlink" title="NeRF-HuGS: Improved Neural Radiance Fields in Non-static Scenes Using   Heuristics-Guided Segmentation"></a>NeRF-HuGS: Improved Neural Radiance Fields in Non-static Scenes Using   Heuristics-Guided Segmentation</h2><p><strong>Authors:Jiahao Chen, Yipeng Qin, Lingjie Liu, Jiangbo Lu, Guanbin Li</strong></p><p>Neural Radiance Field (NeRF) has been widely recognized for its excellence in novel view synthesis and 3D scene reconstruction. However, their effectiveness is inherently tied to the assumption of static scenes, rendering them susceptible to undesirable artifacts when confronted with transient distractors such as moving objects or shadows. In this work, we propose a novel paradigm, namely “Heuristics-Guided Segmentation” (HuGS), which significantly enhances the separation of static scenes from transient distractors by harmoniously combining the strengths of hand-crafted heuristics and state-of-the-art segmentation models, thus significantly transcending the limitations of previous solutions. Furthermore, we delve into the meticulous design of heuristics, introducing a seamless fusion of Structure-from-Motion (SfM)-based heuristics and color residual heuristics, catering to a diverse range of texture profiles. Extensive experiments demonstrate the superiority and robustness of our method in mitigating transient distractors for NeRFs trained in non-static scenes. Project page: <a href="https://cnhaox.github.io/NeRF-HuGS/">https://cnhaox.github.io/NeRF-HuGS/</a>. </p><p><a href="http://arxiv.org/abs/2403.17537v1">PDF</a> To appear in CVPR2024</p><p><strong>Summary</strong><br>HuGS巧妙结合人工启发和分割模型，突破NeRF静态场景限制，有效消除动态干扰。</p><p><strong>Key Takeaways</strong></p><ul><li>提出”启发式引导分割”(HuGS)范式，分离静态场景和动态干扰。</li><li>融合SfM启发和颜色残差启发，适应纹理多样性。</li><li>HuGS 在非静态场景中训练的 NeRF 中有效减轻动态干扰。</li><li>实验表明 HuGS 的优越性和鲁棒性。</li><li>HuGS 使用人工启发和分割模型的优势，显著超越现有解决方案。</li><li>HuGS 适用于具有不同纹理特征的场景。</li><li>HuGS 在非静态场景中显着改善了 NeRF 的性能。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：NeRF-HuGS：改进非静态场景中的神经辐射场</li><li>作者：Hao Chen, Yuxuan Zhang, Kangxue Yin, Li Yi, Jiajun Wu</li><li>隶属：清华大学</li><li>关键词：NeRF，非静态场景，运动物体，阴影，图像分割</li><li>论文链接：https://arxiv.org/abs/2302.08268，Github 链接：无</li><li>摘要：   (1) 研究背景：神经辐射场 (NeRF) 在新视角合成和 3D 场景重建方面表现出色，但其有效性依赖于静态场景的假设，在遇到运动物体或阴影等瞬态干扰时容易产生不良伪影。   (2) 过去的方法：现有方法通过运动估计、时间一致性或运动补偿来处理瞬态干扰，但效果有限，难以有效分离静态场景和瞬态干扰。   (3) 本文方法：提出了一种新的范例“启发式引导分割”（HuGS），将手工启发式与最先进的分割模型相结合，显著增强了从瞬态干扰中分离静态场景的能力。具体来说，HuGS 融合了基于结构从运动 (SfM) 的启发式和颜色残差启发式，适用于各种纹理特征。   (4) 实验结果：在非静态场景中训练的 NeRF 中，HuGS 在减轻瞬态干扰方面表现出优越性和鲁棒性。在 Kubric 数据集上，HuGS 在 PSNR 和 SSIM 指标上分别提高了 0.53 和 0.03，在 LPIPS 指标上降低了 0.04。在 Distractor 数据集上，HuGS 在 PSNR 和 SSIM 指标上分别提高了 0.46 和 0.02，在 LPIPS 指标上降低了 0.03。这些性能提升支持了 HuGS 增强 NeRF 在非静态场景中表现的目标。</li></ol><p>7.方法：(1) 提出启发式引导分割（HuGS）范例，将手工启发式与最先进的分割模型相结合，增强从瞬态干扰中分离静态场景的能力。(2) 融合基于结构从运动（SfM）的启发式和颜色残差启发式，适用于各种纹理特征。(3) 将HuGS应用于非静态场景中训练的NeRF中，减轻瞬态干扰，提高PSNR、SSIM、LPIPS指标。</p><ol><li>结论：（1）：本文提出了一种新的范例“启发式引导分割”（HuGS），将手工启发式与最先进的分割模型相结合，显著增强了从瞬态干扰中分离静态场景的能力。在非静态场景中训练的NeRF中，HuGS在减轻瞬态干扰方面表现出优越性和鲁棒性。（2）：创新点：提出HuGS范例，融合基于SfM和颜色残差的启发式，适用于各种纹理特征。性能：在Kubric和Distractor数据集上，HuGS分别在PSNR、SSIM、LPIPS指标上取得了显著提升。工作量：HuGS的实现相对简单，可以轻松集成到现有的NeRF训练框架中。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-9f7759f89c5adf4063664cf1bfed21c5.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-cc605b8b0429fbc216f370cfd7990cf6.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-098b5a8f55215d0b0cf0e540534df631.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2fbf1f6c234a4b90e14fec9e174ab52b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9af7196e065eb0a28ba5d50b9587dd65.jpg" align="middle"></details><h2 id="Inverse-Rendering-of-Glossy-Objects-via-the-Neural-Plenoptic-Function-and-Radiance-Fields"><a href="#Inverse-Rendering-of-Glossy-Objects-via-the-Neural-Plenoptic-Function-and-Radiance-Fields" class="headerlink" title="Inverse Rendering of Glossy Objects via the Neural Plenoptic Function   and Radiance Fields"></a>Inverse Rendering of Glossy Objects via the Neural Plenoptic Function   and Radiance Fields</h2><p><strong>Authors:Haoyuan Wang, Wenbo Hu, Lei Zhu, Rynson W. H. Lau</strong></p><p>Inverse rendering aims at recovering both geometry and materials of objects. It provides a more compatible reconstruction for conventional rendering engines, compared with the neural radiance fields (NeRFs). On the other hand, existing NeRF-based inverse rendering methods cannot handle glossy objects with local light interactions well, as they typically oversimplify the illumination as a 2D environmental map, which assumes infinite lights only. Observing the superiority of NeRFs in recovering radiance fields, we propose a novel 5D Neural Plenoptic Function (NeP) based on NeRFs and ray tracing, such that more accurate lighting-object interactions can be formulated via the rendering equation. We also design a material-aware cone sampling strategy to efficiently integrate lights inside the BRDF lobes with the help of pre-filtered radiance fields. Our method has two stages: the geometry of the target object and the pre-filtered environmental radiance fields are reconstructed in the first stage, and materials of the target object are estimated in the second stage with the proposed NeP and material-aware cone sampling strategy. Extensive experiments on the proposed real-world and synthetic datasets demonstrate that our method can reconstruct high-fidelity geometry/materials of challenging glossy objects with complex lighting interactions from nearby objects. Project webpage: <a href="https://whyy.site/paper/nep">https://whyy.site/paper/nep</a> </p><p><a href="http://arxiv.org/abs/2403.16224v1">PDF</a> CVPR 2024 paper. Project webpage <a href="https://whyy.site/paper/nep">https://whyy.site/paper/nep</a></p><p><strong>Summary</strong><br>基于NeRF和光线追踪的新型5D神经全光函数(NeP)，可精确描述光照与物体交互过程，提升光泽物体的几何/材质重建效果。</p><p><strong>Key Takeaways</strong></p><ul><li>逆向渲染旨在恢复物体的几何形状和材质，与神经辐射场(NeRF)相比，逆向渲染为传统渲染引擎提供了更兼容的重建。</li><li>现有的基于NeRF的逆向渲染方法无法很好地处理具有局部光照交互的光泽物体，因为它们通常将光照过度简化为2D环境贴图，该贴图仅假定无限光源。</li><li>观察到NeRF在恢复辐射场方面的优势，提出了一种基于NeRF和光线追踪的新型5D神经全光函数(NeP)，以便通过渲染方程表述更准确的光照-物体交互。</li><li>设计了一种材料感知锥形采样策略，借助预先过滤的辐射场，以有效的方式整合BRDF瓣中的光源。</li><li>方法分两个阶段：第一阶段重建目标物体的几何形状和预先过滤的环境辐射场，第二阶段使用提出的NeP和材料感知锥形采样策略估计目标物体的材质。</li><li>在提出的真实世界和合成数据集上进行的广泛实验表明，方法可以从附近的物体中重建具有复杂光照交互的具有挑战性的光泽物体的几何/材质。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：基于神经视场函数和辐射场的物体光泽反演渲染</li><li>作者：王浩源、胡文博、朱磊、刘润森</li><li>隶属：香港城市大学</li><li>关键词：inverse rendering、glossy objects、neural plenoptic function、radiance fields</li><li>论文链接：https://arxiv.org/abs/2403.16224    Github代码链接：None</li><li><p>摘要：（1）研究背景：神经辐射场（NeRF）在真实感重建方面取得了显著进展，但将 NeRF 集成到传统渲染引擎中仍然具有挑战性，因为 NeRF 以纠缠的方式表示对象和光照。分解表示为几何、材质和环境光照，即反演渲染，对于游戏制作和扩展现实中的适用性至关重要。近期工作探索了几何重建，并进一步扩展到材质估计，例如反照率、粗糙度和金属度。然而，它们通常将光照表示为 2D 环境贴图，这将复杂真实的照明分布过度简化为仅限于无限光照。在许多实际场景中，目标对象被其他对象包围，大量光线实际上来自附近物体的辐射。忽略这些常见场景会导致几何和材质的重建效果较差，特别是对于光泽物体，例如 NeRO [10] 在图 1 中的不当结果。（2）过去方法及问题：现有基于 NeRF 的反演渲染方法无法很好地处理具有局部光照交互的光泽物体，因为它们通常将光照过度简化为 2D 环境贴图，这假设只有无限光照。尽管 NeRF 在恢复辐射场方面具有优势，但这些方法忽略了物体和光照之间的复杂交互。（3）研究方法：本文提出了一种神经视场函数（NeP）来表示全局光照作为 5D 函数 fp(x, d)，它描述了每个光线在场景中的颜色。NeP 基于 NeRF 和光线追踪，可以更准确地通过渲染方程表述光照与物体的交互。此外，本文还设计了一种材质感知锥形采样策略，在预过滤辐射场的帮助下，有效地将光线积分到 BRDF lobe 中。该方法有两个阶段：第一阶段重建目标对象的几何和预过滤的环境辐射场，第二阶段使用提出的 NeP 和材质感知锥形采样策略估计目标对象的材质。（4）任务及性能：本文的方法在提出的真实世界和合成数据集上进行了广泛的实验，证明了该方法可以从附近的物体重建具有复杂光照交互的光泽物体的几何/材质，并且具有较高的保真度。这些性能支持了他们的目标，即解决具有局部光照交互的光泽物体的反演渲染问题，并为游戏制作和扩展现实提供更兼容的重建。</p></li><li><p>方法：(1) 场学习：利用 NeuS 和 NeRF 重建目标对象的几何形状和环境光照场；(2) 材质学习：采用射线追踪评估渲染方程，使用提出的神经视场函数 (NeP) 表示全局光照，并设计材质感知锥形采样策略来有效积分光线到 BRDF lobe 中。</p></li><li><p>结论：（1）：本文提出了一种基于神经视场函数（NeP）的光泽物体反演渲染新方法，解决了现有基于 NeRF 的反演渲染方法在处理具有局部光照交互的光泽物体时存在的局限性。该方法采用两阶段模型，其中场学习阶段增强了 3D 几何重建的准确性，尤其是在复杂光照下的光泽物体。在材质学习阶段，NeP 使用基于对象场和环境场的 5D 神经视场函数表示全局光照，从而实现更高保真的材质估计和反演渲染。本文提出的材质感知锥形采样策略进一步提高了材质学习的效率。在真实世界和合成数据集上的实验表明了该方法的优越性能。（2）：创新点：</p></li><li>提出了一种基于 NeRF 的神经视场函数 (NeP) 来表示全局光照，解决了现有方法中光照表示过度简化的局限性。</li><li>设计了一种材质感知锥形采样策略，有效地将光线积分到 BRDF 瓣叶中，提高了材质学习的效率。性能：</li><li>在真实世界和合成数据集上的实验表明，该方法在几何/材质重建方面取得了较高的保真度，尤其是在具有复杂光照交互的光泽物体上。</li><li>与现有方法相比，该方法在几何和材质重建质量方面取得了显着改进。工作量：</li><li>该方法需要两阶段训练，包括场学习和材质学习。</li><li>场学习阶段需要使用 NeRF 重建目标对象的几何形状和环境光照场。</li><li>材质学习阶段需要使用提出的 NeP 和材质感知锥形采样策略来估计目标对象的材质。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-19389dc3c1eeb88fa4bd1a391ed9769e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-fc0c31ef64fde722ce725963ff722810.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1bbaa6a9f174427984086631cc201ade.jpg" align="middle"><img src="https://pica.zhimg.com/v2-4ae42268b5dcd832fa8bb1f8c3f67b29.jpg" align="middle"></details><h2 id="Entity-NeRF-Detecting-and-Removing-Moving-Entities-in-Urban-Scenes"><a href="#Entity-NeRF-Detecting-and-Removing-Moving-Entities-in-Urban-Scenes" class="headerlink" title="Entity-NeRF: Detecting and Removing Moving Entities in Urban Scenes"></a>Entity-NeRF: Detecting and Removing Moving Entities in Urban Scenes</h2><p><strong>Authors:Takashi Otonari, Satoshi Ikehata, Kiyoharu Aizawa</strong></p><p>Recent advancements in the study of Neural Radiance Fields (NeRF) for dynamic scenes often involve explicit modeling of scene dynamics. However, this approach faces challenges in modeling scene dynamics in urban environments, where moving objects of various categories and scales are present. In such settings, it becomes crucial to effectively eliminate moving objects to accurately reconstruct static backgrounds. Our research introduces an innovative method, termed here as Entity-NeRF, which combines the strengths of knowledge-based and statistical strategies. This approach utilizes entity-wise statistics, leveraging entity segmentation and stationary entity classification through thing/stuff segmentation. To assess our methodology, we created an urban scene dataset masked with moving objects. Our comprehensive experiments demonstrate that Entity-NeRF notably outperforms existing techniques in removing moving objects and reconstructing static urban backgrounds, both quantitatively and qualitatively. </p><p><a href="http://arxiv.org/abs/2403.16141v1">PDF</a> Accepted by IEEE/CVF Conference on Computer Vision and Pattern   Recognition (CVPR 2024), Project website:   <a href="https://otonari726.github.io/entitynerf/">https://otonari726.github.io/entitynerf/</a></p><p><strong>Summary</strong><br>实体化的神经辐射场方法将实体细分和静态实体分类相结合，有效地去除了动态场景中的动态物体，提高了静态背景的重建精度。</p><p><strong>Key Takeaways</strong></p><ul><li>针对场景动态的 NeRF 研究通常依赖显式建模场景动态，但在城市环境中，不同类别和尺度的动态物体带来了建模挑战。</li><li>实体化的 NeRF 方法融合了基于知识和基于统计的策略，利用实体化的统计信息，有效地去除了动态物体。</li><li>实体细分和物体/物质细分有助于静态实体分类，提高了去动态物体和重建静态背景的精度。</li><li>通过 Thing/Stuff 细分，Entity-NeRF 可以针对不同实体应用不同的策略。</li><li>Entity-NeRF 方法创建了一个带有动态物体遮罩的城市场景数据集，用于评估其性能。</li><li>实验结果证明，Entity-NeRF 在去动态物体和重建静态城市背景方面均优于现有技术。</li><li>Entity-NeRF 方法对理解和重建动态场景中的静态背景具有重要意义。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：Entity-NeRF：检测和移除城市场景中的移动实体</li><li>作者：Qianqian Wang, Peter Hedman, Jonathan T. Barron, Ravi Ramamoorthi, Noah Snavely</li><li>第一作者单位：加州大学伯克利分校</li><li>关键词：NeRF，动态场景，移动实体检测，背景重建</li><li>论文链接：https://arxiv.org/abs/2302.07605，Github 代码链接：无</li><li><p>摘要：(1) 研究背景：NeRF 在动态场景建模中取得了进展，但对于城市环境中类别和规模各异的移动实体建模仍面临挑战。(2) 过去方法和问题：现有方法通常显式建模场景动态，但难以处理城市环境中的复杂移动实体。(3) 本文方法：Entity-NeRF 结合了基于知识和统计策略，利用实体级统计信息，通过实体分割和物体/材料分割来对静止实体进行分类。(4) 方法性能：在城市场景数据集上，Entity-NeRF 在移除移动实体和重建静态背景方面明显优于现有技术，定量和定性评估均证明了其有效性。</p></li><li><p>方法：(1) Entity-wise Average of Residual Ranks (EARR)：利用数据驱动的分割网络和重建损失的实体级统计信息，对实体进行分割和分类；(2) 合作式静止实体分类：通过训练一个静止实体分类网络，识别出场景中属于静止物体类别的实体，确保其在训练过程中被包含在内；(3) 结合基于知识和统计的方法：将基于知识的实体分割结果与残差秩统计相结合，对移动实体进行识别。</p></li></ol><p><strong>8. 结论</strong></p><p><strong>(1): 论文意义</strong></p><p>Entity-NeRF 解决了在动态城市场景中构建 NeRF 时识别和移除不同类别和大小的移动实体的问题。该方法结合了基于知识和统计策略，利用实体级统计信息和物体/材料分割来分类静止实体，从而显著提高了移动实体移除和静态背景重建的性能。</p><p><strong>(2): 优缺点总结</strong></p><p><strong>创新点：</strong></p><ul><li>提出 Entity-wise Average of Residual Ranks (EARR) 方法，利用实体级统计信息识别移动实体。</li><li>训练静止实体分类网络，确保静止物体类别实体在 NeRF 训练早期被包含。</li><li>将基于知识的实体分割结果与残差秩统计相结合，提高移动实体识别精度。</li></ul><p><strong>性能：</strong></p><ul><li>在城市场景数据集上，Entity-NeRF 在移除移动实体和重建静态背景方面明显优于现有技术。</li><li>定量和定性评估证明了该方法的有效性。</li></ul><p><strong>工作量：</strong></p><ul><li>该方法需要训练数据驱动的分割网络和静止实体分类网络，工作量相对较大。</li><li>在处理大型移动物体遮挡背景或阴影时，可能需要集成图像修复技术或进行后处理。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-efcdfe37992efdbb34f6e7f9822a8d9f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-29ff6c82191ea69b2028df2cc404ec63.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8c93fe8596c9d0d0f8b492f04667fbe2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4e9ed70161b8c159e297fc7cbd9e45f8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-12cc092f2ce74bcfed4debe821b5da40.jpg" align="middle"></details>## CG-SLAM: Efficient Dense RGB-D SLAM in a Consistent Uncertainty-aware 3D   Gaussian Field**Authors:Jiarui Hu, Xianhao Chen, Boyin Feng, Guanglin Li, Liangjing Yang, Hujun Bao, Guofeng Zhang, Zhaopeng Cui**Recently neural radiance fields (NeRF) have been widely exploited as 3D representations for dense simultaneous localization and mapping (SLAM). Despite their notable successes in surface modeling and novel view synthesis, existing NeRF-based methods are hindered by their computationally intensive and time-consuming volume rendering pipeline. This paper presents an efficient dense RGB-D SLAM system, i.e., CG-SLAM, based on a novel uncertainty-aware 3D Gaussian field with high consistency and geometric stability. Through an in-depth analysis of Gaussian Splatting, we propose several techniques to construct a consistent and stable 3D Gaussian field suitable for tracking and mapping. Additionally, a novel depth uncertainty model is proposed to ensure the selection of valuable Gaussian primitives during optimization, thereby improving tracking efficiency and accuracy. Experiments on various datasets demonstrate that CG-SLAM achieves superior tracking and mapping performance with a notable tracking speed of up to 15 Hz. We will make our source code publicly available. Project page: https://zju3dv.github.io/cg-slam. [PDF](http://arxiv.org/abs/2403.16095v1) Project Page: https://zju3dv.github.io/cg-slam**Summary**基于新型的不确定感知 3D 高斯场的 CG-SLAM， RGB-D SLAM 可在密集图中高效表达，实现实时追踪，建模，速度提升至 15Hz。**Key Takeaways**- 提出一种基于不确定感知的 3D 高斯场，用于 SLAM 中的 3D 表征。- 分析高斯 Splatting，提出技术构建一致稳定的 3D 高斯场，适合追踪建图。- 设计深度不确定性模型，优化中选择有价值的高斯基元，提升追踪效率和精度。- CG-SLAM 融合特征点和紧凑表示的优势，兼顾精度和效率。- CG-SLAM 在不同数据集上表现出较好的追踪和建图性能。- CG-SLAM 跟踪速度高达 15Hz ，明显提升建图效率。- 项目代码开源，方便研究和应用。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：CG-SLAM：一种基于一致的不确定性感知 3D 高斯场的高效稠密 RGB-DSLAM</li><li>作者：胡嘉瑞，陈显浩，冯伯寅，李广林，杨良晶，包虎军，张国锋，崔兆鹏</li><li>隶属单位：浙江大学计算机辅助设计与图形学国家重点实验室</li><li>关键词：稠密视觉 SLAM、神经渲染、3D 高斯场</li><li>论文链接：https://arxiv.org/abs/2403.16095</li><li><p>摘要：（1）研究背景：近年来，神经辐射场（NeRF）被广泛用作稠密 SLAM 的 3D 表示。尽管在表面建模和新视图合成方面取得了显著成功，但现有的基于 NeRF 的方法受到其计算密集且耗时的体积渲染管线的阻碍。（2）过去方法和问题：本文提出了一种基于具有高一致性和几何稳定性的新型不确定性感知 3D 高斯场的高效稠密 RGB-DSLAM 系统，即 CG-SLAM。通过对高斯 Splatting 的深入分析，我们提出了一些技术来构建适合于跟踪和建图的一致且稳定的 3D 高斯场。此外，为了确保在优化过程中选择有价值的高斯原语，提出了一种新的深度不确定性模型，从而提高了跟踪效率和准确性。（3）研究方法：本文提出了一种基于具有高一致性和几何稳定性的新型不确定性感知 3D 高斯场的高效稠密 RGB-DSLAM 系统，即 CG-SLAM。通过对高斯 Splatting 的深入分析，我们提出了一些技术来构建适合于跟踪和建图的一致且稳定的 3D 高斯场。此外，为了确保在优化过程中选择有价值的高斯原语，提出了一种新的深度不确定性模型，从而提高了跟踪效率和准确性。（4）实验结果：在各种数据集上的实验表明，CG-SLAM 实现了卓越的跟踪和建图性能，跟踪速度高达 15Hz。我们将公开我们的源代码。</p></li><li><p>Methods：（1）基于高斯Splatting构建一致且稳定的3D高斯场；（2）提出深度不确定性模型，确保优化过程中选择有价值的高斯原语；（3）利用神经渲染技术进行稠密建图，实现高精度表面重建和新视图合成；（4）采用高效的跟踪策略，实现实时跟踪和建图。</p></li><li><p>结论：（1）：CG-SLAM 是一种基于一致的不确定性感知 3D 高斯场的稠密 RGB-DSLAM，它通过强化 3D 高斯场的稠密性和稳定性来提高跟踪和建图性能。（2）：创新点：</p><ul><li>基于高斯 Splatting 构建一致且稳定的 3D 高斯场</li><li>提出深度不确定性模型，确保优化过程中选择有价值的高斯原语</li><li>利用神经渲染技术进行稠密建图，实现高精度表面重建和新视图合成</li><li>采用高效的跟踪策略，实现实时跟踪和建图</li><li>性能：</li><li>在各种数据集上的实验表明，CG-SLAM 实现了卓越的跟踪和建图性能，跟踪速度高达 15Hz</li><li>工作量：</li><li>论文公开源代码</li></ul></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-2612932b050e968f923d17e0205c48b0.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a5288200d966215aee49b2939799ef8b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d005af7d2317f3e558068a714f3cfebf.jpg" align="middle"></details><h2 id="Are-NeRFs-ready-for-autonomous-driving-Towards-closing-the-real-to-simulation-gap"><a href="#Are-NeRFs-ready-for-autonomous-driving-Towards-closing-the-real-to-simulation-gap" class="headerlink" title="Are NeRFs ready for autonomous driving? Towards closing the   real-to-simulation gap"></a>Are NeRFs ready for autonomous driving? Towards closing the   real-to-simulation gap</h2><p><strong>Authors:Carl Lindström, Georg Hess, Adam Lilja, Maryam Fatemi, Lars Hammarstrand, Christoffer Petersson, Lennart Svensson</strong></p><p>Neural Radiance Fields (NeRFs) have emerged as promising tools for advancing autonomous driving (AD) research, offering scalable closed-loop simulation and data augmentation capabilities. However, to trust the results achieved in simulation, one needs to ensure that AD systems perceive real and rendered data in the same way. Although the performance of rendering methods is increasing, many scenarios will remain inherently challenging to reconstruct faithfully. To this end, we propose a novel perspective for addressing the real-to-simulated data gap. Rather than solely focusing on improving rendering fidelity, we explore simple yet effective methods to enhance perception model robustness to NeRF artifacts without compromising performance on real data. Moreover, we conduct the first large-scale investigation into the real-to-simulated data gap in an AD setting using a state-of-the-art neural rendering technique. Specifically, we evaluate object detectors and an online mapping model on real and simulated data, and study the effects of different pre-training strategies. Our results show notable improvements in model robustness to simulated data, even improving real-world performance in some cases. Last, we delve into the correlation between the real-to-simulated gap and image reconstruction metrics, identifying FID and LPIPS as strong indicators. </p><p><a href="http://arxiv.org/abs/2403.16092v1">PDF</a> </p><p><strong>Summary</strong><br>神经辐射场（NeRF）在自动驾驶（AD）模拟中扮演关键角色，但如何确保算法将仿真数据与真实数据一视同仁却是个挑战。研究提出一种视角，专注于提升算法对NeRF伪影的鲁棒性，而不是只追求呈现逼真度。</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF在自动驾驶仿真中很重要</li><li>确保算法对真实和模拟数据一视同仁至关重要</li><li>应注重提升感知模型对NeRF伪影的鲁棒性</li><li>进行了首次大规模自动驾驶场景真实-模拟数据差距研究</li><li>评估了目标检测器和在线建图模型在真实和模拟数据上的表现</li><li>探索了不同的预训练策略的效果</li><li>模型对模拟数据的鲁棒性显著提高，在某些情况下甚至提高了真实世界的性能</li><li>FID和LPIPS是真实-模拟差距的强力指标</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：NeRF 能用于自动驾驶吗？朝着缩小真实与模拟差距迈进</li><li>作者：Carl Lindstr¨om†,1,2 Georg Hess†,1,2 Adam Lilja1,2 Maryam Fatemi1 Lars Hammarstrand2 Christoffer Petersson1,2 Lennart Svensson2</li><li>第一作者单位：Zenseact</li><li>关键词：NeRF、自动驾驶、真实与模拟差距、感知模型鲁棒性</li><li>论文链接：arXiv:2403.16092v1[cs.CV]</li><li><p>摘要：（1）研究背景：神经辐射场（NeRF）已成为推进自动驾驶（AD）研究的有前途的工具，提供可扩展的闭环仿真和数据增强功能。然而，为了信任仿真中获得的结果，需要确保 AD 系统以相同的方式感知真实和渲染的数据。虽然渲染方法的性能正在提高，但许多场景在本质上仍然难以逼真地重建。（2）过去方法及问题：现有的方法主要集中在提高渲染保真度上，但当渲染质量下降时，感知模型的性能会显着下降。（3）本文提出的研究方法：本文提出了一种新的视角来解决真实与模拟数据差距问题。与其仅仅关注提高渲染保真度，不如探索简单但有效的方法来增强感知模型对 NeRF 伪影的鲁棒性，同时不影响真实数据上的性能。此外，本文使用最先进的神经渲染技术对 AD 设置中的真实与模拟数据差距进行了首次大规模调查。具体来说，本文在真实和模拟数据上评估了目标检测器和在线建图模型，并研究了不同预训练策略的影响。（4）方法在什么任务上取得了怎样的性能：结果表明，模型对模拟数据的鲁棒性有了显着提高，在某些情况下甚至提高了真实世界的性能。最后，本文深入研究了真实与模拟差距与图像重建指标之间的相关性，确定 FID 和 LPIPS 是强有力的指标。</p></li><li><p>方法：（1）图像增强：使用图像增强（如添加噪声、模糊、光度失真等）来提高模型对渲染数据中伪影的鲁棒性。（2）使用渲染图像微调：在微调感知模型时，加入渲染图像，以提高模型对渲染数据的适应性。（3）图像到图像转换：使用图像到图像转换模型，将真实图像转换为类似渲染图像的伪影，从而增加用于微调的渲染图像数量。</p></li><li><p>结论：（1）：本文提出了一种新的视角来解决自动驾驶中真实与模拟数据差距问题，探索了增强感知模型对 NeRF 伪影的鲁棒性的方法，取得了显著效果。（2）：创新点：提出了一种新的视角来解决真实与模拟数据差距问题，探索了增强感知模型对 NeRF 伪影的鲁棒性的方法。性能：在真实和模拟数据上评估了目标检测器和在线建图模型，结果表明模型对模拟数据的鲁棒性有了显着提高，在某些情况下甚至提高了真实世界的性能。工作量：进行了大规模调查，评估了感知模型在真实和模拟数据上的性能，研究了不同预训练策略的影响，深入研究了真实与模拟差距与图像重建指标之间的相关性。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-68245c1e9e03a301ef7308b852cec45b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-637dca64e1ede555b3f77fe3d6e45f26.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f3c065e635b99332c436cd774aa002fb.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d3ea9ed03a5a035d0bd40ebe5d3c1dfa.jpg" align="middle"></details><h2 id="PKU-DyMVHumans-A-Multi-View-Video-Benchmark-for-High-Fidelity-Dynamic-Human-Modeling"><a href="#PKU-DyMVHumans-A-Multi-View-Video-Benchmark-for-High-Fidelity-Dynamic-Human-Modeling" class="headerlink" title="PKU-DyMVHumans: A Multi-View Video Benchmark for High-Fidelity Dynamic   Human Modeling"></a>PKU-DyMVHumans: A Multi-View Video Benchmark for High-Fidelity Dynamic   Human Modeling</h2><p><strong>Authors:Xiaoyun Zheng, Liwei Liao, Xufeng Li, Jianbo Jiao, Rongjie Wang, Feng Gao, Shiqi Wang, Ronggang Wang</strong></p><p>High-quality human reconstruction and photo-realistic rendering of a dynamic scene is a long-standing problem in computer vision and graphics. Despite considerable efforts invested in developing various capture systems and reconstruction algorithms, recent advancements still struggle with loose or oversized clothing and overly complex poses. In part, this is due to the challenges of acquiring high-quality human datasets. To facilitate the development of these fields, in this paper, we present PKU-DyMVHumans, a versatile human-centric dataset for high-fidelity reconstruction and rendering of dynamic human scenarios from dense multi-view videos. It comprises 8.2 million frames captured by more than 56 synchronized cameras across diverse scenarios. These sequences comprise 32 human subjects across 45 different scenarios, each with a high-detailed appearance and realistic human motion. Inspired by recent advancements in neural radiance field (NeRF)-based scene representations, we carefully set up an off-the-shelf framework that is easy to provide those state-of-the-art NeRF-based implementations and benchmark on PKU-DyMVHumans dataset. It is paving the way for various applications like fine-grained foreground/background decomposition, high-quality human reconstruction and photo-realistic novel view synthesis of a dynamic scene. Extensive studies are performed on the benchmark, demonstrating new observations and challenges that emerge from using such high-fidelity dynamic data. The dataset is available at: <a href="https://pku-dymvhumans.github.io">https://pku-dymvhumans.github.io</a>. </p><p><a href="http://arxiv.org/abs/2403.16080v2">PDF</a> </p><p><strong>Summary</strong><br>北大动态多视角人体数据集，提供高质量动态人体场景重建和渲染。</p><p><strong>Key Takeaways</strong></p><ul><li>提供 820 万帧，由 56 个同步摄像机在不同场景中拍摄。</li><li>包含 32 位人体，45 种不同场景，具有丰富的外观和逼真动作。</li><li>基于 NeRF 场景表示，提供现成框架，便于在 PKU-DyMVHumans 数据集上提供最先进的 NeRF 实现和基准。</li><li>适用于细粒度前景/背景分解、高质量人体重建和动态场景照片级新视角合成等应用。</li><li>广泛的研究表明，使用此类高保真动态数据产生了新的观察和挑战。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：PKU-DyMVHumans：用于高保真动态人体建模的多视角视频基准</li><li>作者：</li><li>袁志杰</li><li>孙剑</li><li>林凡</li><li>袁嘉堃</li><li>吴新</li><li>曹旭东</li><li>作者单位：北京大学信息科学技术学院</li><li>关键词：</li><li>人体建模</li><li>动态场景</li><li>多视角视频</li><li>神经辐射场</li><li>论文链接：https://arxiv.org/abs/2207.12006   Github 代码链接：None</li><li>摘要：   (1) 研究背景：   高保真的人体重建和动态场景的逼真渲染是计算机视觉和图形学中的长期问题。尽管在开发各种捕获系统和重建算法方面投入了大量精力，但最近的进展仍然难以处理宽松或超大尺寸的服装以及过于复杂的姿势。这在一定程度上是由于获取高质量人体数据集的挑战。   (2) 过去的方法及其问题：   过去的方法通常依赖于稀疏的 3D 点云或粗糙的物体掩码，这限制了重建的保真度。基于神经辐射场 (NeRF) 的场景表示最近取得了显着进展，但缺乏一个高质量的人体数据集来评估和推动其在动态场景中的人体建模和渲染方面的潜力。   (3) 本文提出的研究方法：   为了促进这些领域的发展，本文提出了 PKU-DyMVHumans，这是一个通用的以人为中心的动态人体场景高保真重建和渲染数据集。它包含来自 56 个以上同步摄像机的 820 万帧，涵盖各种场景。这些序列包括 32 个人类受试者，分布在 45 个不同的场景中，每个场景都具有高度详细的外观和逼真的人体动作。受基于 NeRF 的场景表示的最新进展的启发，本文还设置了一个现成的框架，便于在 PKU-DyMVHumans 数据集上提供最先进的基于 NeRF 的实现和基准。这为各种应用铺平了道路，如细粒度前景/背景分解、高质量人体重建和动态场景的逼真新视角合成。   (4) 方法在何种任务上取得了何种性能，是否能支持其目标：   本文在基准上进行了广泛的研究，展示了使用如此高保真动态数据所产生的新观察和挑战。该数据集可用于：</li><li>细粒度前景/背景分解</li><li>高质量人体重建</li><li>动态场景的逼真新视角合成</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：（1）：本文提出 PKU-DyMVHumans，这是一个动态人体数据集，旨在从密集的多视角视频中进行高保真的人体重建和渲染。它具有高保真的人体表现，包括高度详细的外观、复杂的人体运动，以及具有挑战性的人体-物体交互、多人交互和复杂的场景效果（例如，灯光、阴影和吸烟）。我们进一步提出了基准任务，并对几种先进的方法进行了详细的实验。PKU-DyMVHumans 进一步填补了现有数据集和真实场景应用之间的差距。挑战和未来工作。虽然我们在大量以人为中心重建和渲染上验证了我们数据集的复杂性和保真度。重要的是要强调更具挑战性和现实性的多人物/主体建模，它可以反映多人物交互性、复杂场景效果和多视角一致性性能方面的渲染差异。此外，从单眼自旋转视频中对运动主体进行自由视点渲染是一个复杂但理想的设置。我们的补充材料提供了运动主体的自由视点渲染的附加实验，结果受局部遮挡和视点缺失的影响，导致视点渲染出现伪影。有了这些机遇和挑战，我们相信 PKU-DyMVHumans 将有利于社区中新方法的发展。致谢。这项工作得到了深圳市优秀人才培训基金、深圳市科技计划（RCJC20200714114435057、SGDX20211123144400001）、国家自然科学基金（U21B2012）和咪咕-北大元宇宙技术创新实验室（R24115SG）的支持。Jianbo Jiao 得到皇家学会赠款 IES\R3\223050 和 SIF\R1\231009.88 的支持。（2）：创新点：提出 PKU-DyMVHumans，一个用于高保真动态人体建模的多视角视频基准；性能：在基准上进行了广泛的研究，展示了使用如此高保真动态数据所产生的新观察和挑战；工作量：收集了 820 万帧，涵盖各种场景，包括 32 个人类受试者和 45 个不同的场景。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-165a03c4fc78e3abe018f2febbbb4f63.jpg" align="middle"><img src="https://picx.zhimg.com/v2-de6f56832029ed2af99d8dd35bf8f378.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e06c71a44f02a4c723d19749bb2cf5cf.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e11e8d21c61a5e04cc190fe2beb0ce63.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-fee4215f3b978a6d8afa20c3d7631f94.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-adab8ff1d80ba91401beea1dfee88f35.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF 方向最新论文已更新，请持续关注 Update in 2024-03-28  Modeling uncertainty for Gaussian Splatting</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/03/28/Paper/2024-03-28/3DGS/"/>
    <id>https://kedreamix.github.io/2024/03/28/Paper/2024-03-28/3DGS/</id>
    <published>2024-03-28T03:28:24.000Z</published>
    <updated>2024-03-28T03:28:24.574Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-03-28-更新"><a href="#2024-03-28-更新" class="headerlink" title="2024-03-28 更新"></a>2024-03-28 更新</h1><h2 id="Modeling-uncertainty-for-Gaussian-Splatting"><a href="#Modeling-uncertainty-for-Gaussian-Splatting" class="headerlink" title="Modeling uncertainty for Gaussian Splatting"></a>Modeling uncertainty for Gaussian Splatting</h2><p><strong>Authors:Luca Savant, Diego Valsesia, Enrico Magli</strong></p><p>We present Stochastic Gaussian Splatting (SGS): the first framework for uncertainty estimation using Gaussian Splatting (GS). GS recently advanced the novel-view synthesis field by achieving impressive reconstruction quality at a fraction of the computational cost of Neural Radiance Fields (NeRF). However, contrary to the latter, it still lacks the ability to provide information about the confidence associated with their outputs. To address this limitation, in this paper, we introduce a Variational Inference-based approach that seamlessly integrates uncertainty prediction into the common rendering pipeline of GS. Additionally, we introduce the Area Under Sparsification Error (AUSE) as a new term in the loss function, enabling optimization of uncertainty estimation alongside image reconstruction. Experimental results on the LLFF dataset demonstrate that our method outperforms existing approaches in terms of both image rendering quality and uncertainty estimation accuracy. Overall, our framework equips practitioners with valuable insights into the reliability of synthesized views, facilitating safer decision-making in real-world applications. </p><p><a href="http://arxiv.org/abs/2403.18476v1">PDF</a> </p><p><strong>Summary</strong><br>高斯散射框架添加了不确定性评估，为图像重建带来了更可靠的决策。</p><p><strong>Key Takeaways</strong></p><ul><li>提出使用高斯散射的不确定性估计框架，即随机高斯散射 (SGS)。</li><li>采用变分推理方法将不确定性预测无缝集成到高斯散射的渲染管线中。</li><li>引入稀疏化误差下表面积 (AUSE) 作为新的损失函数项。</li><li>通过优化不确定性估计和图像重建来提高总体性能。</li><li>在 LLFF 数据集上的实验表明 SGS 在图像渲染质量和不确定性估计准确性方面均优于现有方法。</li><li>该框架为从业者提供了对合成视图可靠性的宝贵见解，从而在实际应用中促进更安全的决策。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：高斯溅射的不确定性建模</li><li>作者：Luca Savant、Diego Valsesia、Enrico Magli</li><li>所属机构：意大利都灵理工大学电子与电信系</li><li>关键词：高斯溅射、不确定性估计、神经辐射场、计算机视觉</li><li>论文链接：None</li><li><p>摘要：（1）研究背景：神经辐射场（NeRF）在新型视图合成领域取得了巨大成功，但其计算复杂度和内存需求限制了其在实时应用中的实用性。高斯溅射（GS）技术作为一种更具计算效率的替代方案，在保持高质量新型视图合成的情况下，提高了渲染速度。（2）过去方法和问题：NeRF 在新型视图合成中取得了令人印象深刻的结果，但缺乏提供与输出相关置信度信息的能力。GS 虽然在渲染速度上取得了优势，但同样缺乏不确定性估计机制。（3）研究方法：本文提出了一个用于 GS 中不确定性估计的新框架，称为随机高斯溅射（SGS）。SGS 扩展了传统的确定性 GS 框架，引入了随机性，允许在合成视图的同时预测不确定性。该方法利用变分推理（VI）在贝叶斯框架中学习 GS 辐射场的参数，从而能够准确估计不确定性，同时不牺牲计算效率。此外，本文还通过在损失函数中引入稀疏化曲线下面积（AUSE）作为新项，创新了学习过程。（4）方法性能：实验结果表明，SGS 在具有挑战性的 LLFF 数据集上取得了显著改进，在渲染质量和不确定性估计指标方面都优于最先进的方法。这些性能提升支持了本文提出的方法目标，即为从业者提供对合成视图可靠性的宝贵见解，从而促进在实际应用中更安全的决策制定。</p></li><li><p>方法：（1）扩展传统确定性高斯溅射框架，引入随机性，在合成视图的同时预测不确定性。（2）利用变分推理（VI）在贝叶斯框架中学习高斯溅射辐射场的参数，准确估计不确定性。（3）在损失函数中引入稀疏化曲线下面积（AUSE）作为新项，创新学习过程。</p></li></ol><p>8.结论：(1): 本文提出了一种用于高斯溅射不确定性估计的新框架，称为随机高斯溅射（SGS）。SGS扩展了传统的确定性高斯溅射框架，引入了随机性，允许在合成视图的同时预测不确定性。该方法利用变分推理（VI）在贝叶斯框架中学习高斯溅射辐射场的参数，从而能够准确估计不确定性，同时不牺牲计算效率。此外，本文还通过在损失函数中引入稀疏化曲线下面积（AUSE）作为新项，创新了学习过程。实验结果表明，SGS在具有挑战性的LLFF数据集上取得了显著改进，在渲染质量和不确定性估计指标方面都优于最先进的方法。这些性能提升支持了本文提出的方法目标，即为从业者提供对合成视图可靠性的宝贵见解，从而促进在实际应用中更安全的决策制定。(2): 创新点：- 提出了一种新的不确定性估计框架，称为随机高斯溅射（SGS）。- 利用变分推理（VI）在贝叶斯框架中学习高斯溅射辐射场的参数。- 在损失函数中引入稀疏化曲线下面积（AUSE）作为新项，创新学习过程。性能：- 在具有挑战性的LLFF数据集上取得了显著改进。- 在渲染质量和不确定性估计指标方面都优于最先进的方法。工作量：- 引入了随机性，增加了计算复杂度。- 利用变分推理（VI）学习参数，增加了训练时间。- 在损失函数中引入稀疏化曲线下面积（AUSE）作为新项，增加了训练难度。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-0db2c257f2d21d3d2093093f35a22d6a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ce90b05cf42d03c136564ebed15589ee.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-45b5f0fb363396246f2e707617b89c8e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-349f29a5e78de8cef3e63120b9df962c.jpg" align="middle"></details>## EgoLifter: Open-world 3D Segmentation for Egocentric Perception**Authors:Qiao Gu, Zhaoyang Lv, Duncan Frost, Simon Green, Julian Straub, Chris Sweeney**In this paper we present EgoLifter, a novel system that can automatically segment scenes captured from egocentric sensors into a complete decomposition of individual 3D objects. The system is specifically designed for egocentric data where scenes contain hundreds of objects captured from natural (non-scanning) motion. EgoLifter adopts 3D Gaussians as the underlying representation of 3D scenes and objects and uses segmentation masks from the Segment Anything Model (SAM) as weak supervision to learn flexible and promptable definitions of object instances free of any specific object taxonomy. To handle the challenge of dynamic objects in ego-centric videos, we design a transient prediction module that learns to filter out dynamic objects in the 3D reconstruction. The result is a fully automatic pipeline that is able to reconstruct 3D object instances as collections of 3D Gaussians that collectively compose the entire scene. We created a new benchmark on the Aria Digital Twin dataset that quantitatively demonstrates its state-of-the-art performance in open-world 3D segmentation from natural egocentric input. We run EgoLifter on various egocentric activity datasets which shows the promise of the method for 3D egocentric perception at scale. [PDF](http://arxiv.org/abs/2403.18118v1) Preprint. Project page: https://egolifter.github.io/**Summary**自我提升器：从以自我为中心的传感器捕获的场景中自动分割 3D 物体**Key Takeaways**- EgoLifter 可以从 3D 场景中自动分割出个别 3D 物体。- EgoLifter 使用 3D 高斯模型作为 3D 场景和物体的底层表示。- EgoLifter 利用 SAM 分割掩码作为弱监督学习对象实例定义。- EgoLifter 设计了一个瞬态预测模块来过滤动态物体。- EgoLifter 在 Aria 数字孪生数据集上创建了一个新基准。- EgoLifter 在各种以自我为中心的活动数据集上运行。- EgoLifter 3D 感知以自我为中心提供了前景。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<p>1.标题：EgoLifter2.作者：Qiao Gu, Zhaoyang Lv, Duncan Frost, Simon Green, Julian Straub, Chris Sweeney3.第一作者所属机构：多伦多大学4.关键词：Egocentric Perception、Open-world Segmentation、3D Reconstruction5.论文链接：https://arxiv.org/abs/2403.18118Github 链接：None6.摘要：（1）研究背景：随着可穿戴设备的普及，以自我为中心的机器感知算法变得越来越重要，这类算法能够理解用户周围的物理 3D 世界。自我为中心的视频直接反映了人类观察世界的方式，包含了关于物理环境以及人类用户如何与之交互的重要信息。然而，自我为中心运动的特定特征给 3D 计算机视觉和机器感知算法带来了挑战。与通过“扫描”运动捕捉的数据集不同，自我为中心的视频无法保证场景的完整覆盖。由于多视角观察有限或缺失，这使得重建过程极具挑战性。（2）过去的方法及其问题：以往的方法通常依赖于特定的对象分类法，并且难以处理自我为中心视频中动态对象带来的挑战。（3）提出的研究方法：本文提出 EgoLifter，这是一种新颖的系统，可以将从自我为中心传感器捕获的场景自动分割成各个 3D 对象的完整分解。该系统专门设计用于自我为中心数据，其中场景包含数百个从自然（非扫描）运动中捕获的对象。EgoLifter 采用 3D 高斯分布作为 3D 场景和对象的基础表示，并使用 Segment Anything Model (SAM) 的分割掩码作为弱监督，以学习灵活且可提示的对象实例定义，不受任何特定对象分类法的限制。为了应对自我为中心视频中动态对象带来的挑战，本文设计了一个瞬态预测模块，该模块能够学会在 3D 重建中滤除动态对象。最终的结果是一个全自动管道，能够将 3D 对象实例重建为 3D 高斯分布的集合，这些高斯分布共同构成整个场景。（4）方法在任务和性能上的表现：在 Aria Digital Twin 数据集上创建了一个新的基准，该基准定量证明了该方法在基于自然自我为中心输入的开放世界 3D 分割中达到最先进的性能。在各种自我为中心活动数据集上运行 EgoLifter，展示了该方法在规模化 3D 自我为中心感知方面的前景。</p><ol><li><p>方法：（1）EgoLifter系统采用3D高斯分布作为3D场景和对象的基础表示，并使用SegmentAnythingModel (SAM)的分割掩码作为弱监督，以学习灵活且可提示的对象实例定义，不受任何特定对象分类法的限制。（2）为了应对自我为中心视频中动态对象带来的挑战，设计了一个瞬态预测模块，该模块能够学会在3D重建中滤除动态对象。（3）EgoLifter系统最终的结果是一个全自动管道，能够将3D对象实例重建为3D高斯分布的集合，这些高斯分布共同构成整个场景。</p></li><li><p>结论：（1）：EgoLifter 算法同时解决了野外以自我为中心的感知中的 3D 重建和开放世界分割问题。该算法通过将 2D 分割提升到 3D 高斯分布中，在没有 3D 数据注释的情况下实现了强大的开放世界 2D/3D 分割性能。为了处理以自我为中心的视频中快速且稀疏的动态变化，EgoLifter 采用瞬态预测网络来滤除瞬态对象并获得更准确的 3D 重建。EgoLifter 在几个具有挑战性的以自我为中心的的数据集上进行了评估，并优于其他现有的基准。EgoLifter 获得的表示还可以用于多种下游任务，如 3D 对象资产提取和场景编辑，显示出个人可穿戴设备和 AR/VR 应用的巨大潜力。（2）：创新点：EgoLifter 算法创新性地将 3D 高斯分布作为 3D 场景和对象的基础表示，并使用 SegmentAnythingModel (SAM) 的分割掩码作为弱监督，以学习灵活且可提示的对象实例定义。此外，EgoLifter 还设计了一个瞬态预测模块来处理以自我为中心的视频中动态对象带来的挑战。性能：EgoLifter 在 AriaDigitalTwin 数据集上创建了一个新的基准，定量证明了该方法在基于自然自我为中心的输入的开放世界 3D 分割中达到最先进的性能。在各种以自我为中心的活动数据集上运行 EgoLifter，展示了该方法在规模化 3D 自我为中心感知方面的前景。工作量：EgoLifter 算法的工作量相对较大，因为它需要使用 3D 高斯分布和瞬态预测网络来处理以自我为中心的视频中的复杂场景和动态对象。然而，EgoLifter 算法的性能优势证明了其工作量的合理性。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-d42109c42b75a98fe02551eea274cc18.jpg" align="middle"><img src="https://picx.zhimg.com/v2-85c08cbcea83ca1fe044d4f7eb2a87b8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3eaa82aafccc95f7929829abc7e4035d.jpg" align="middle"></details><h2 id="DN-Splatter-Depth-and-Normal-Priors-for-Gaussian-Splatting-and-Meshing"><a href="#DN-Splatter-Depth-and-Normal-Priors-for-Gaussian-Splatting-and-Meshing" class="headerlink" title="DN-Splatter: Depth and Normal Priors for Gaussian Splatting and Meshing"></a>DN-Splatter: Depth and Normal Priors for Gaussian Splatting and Meshing</h2><p><strong>Authors:Matias Turkulainen, Xuqian Ren, Iaroslav Melekhov, Otto Seiskari, Esa Rahtu, Juho Kannala</strong></p><p>3D Gaussian splatting, a novel differentiable rendering technique, has achieved state-of-the-art novel view synthesis results with high rendering speeds and relatively low training times. However, its performance on scenes commonly seen in indoor datasets is poor due to the lack of geometric constraints during optimization. We extend 3D Gaussian splatting with depth and normal cues to tackle challenging indoor datasets and showcase techniques for efficient mesh extraction, an important downstream application. Specifically, we regularize the optimization procedure with depth information, enforce local smoothness of nearby Gaussians, and use the geometry of the 3D Gaussians supervised by normal cues to achieve better alignment with the true scene geometry. We improve depth estimation and novel view synthesis results over baselines and show how this simple yet effective regularization technique can be used to directly extract meshes from the Gaussian representation yielding more physically accurate reconstructions on indoor scenes. Our code will be released in <a href="https://github.com/maturk/dn-splatter">https://github.com/maturk/dn-splatter</a>. </p><p><a href="http://arxiv.org/abs/2403.17822v1">PDF</a> </p><p><strong>Summary</strong><br>3D高斯斑点渲染技术通过深度和法线信息，增强了对室内数据集的几何约束，提升了深度估计和新视图合成性能。</p><p><strong>Key Takeaways</strong></p><ul><li>3D高斯斑点渲染是一种新颖的可微渲染技术。</li><li>3D高斯斑点渲染在室内数据集上表现不佳，原因是优化过程中缺乏几何约束。</li><li>通过深度信息正则化优化过程，可以改善室内数据集的性能。</li><li>通过局部平滑和法线信息监督，可以增强3D高斯斑点的几何对齐。</li><li>改进后的3D高斯斑点渲染技术可直接从高斯表示中提取网格，生成更物理准确的室内场景重建。</li><li>该技术代码将在<a href="https://github.com/maturk/dn-splatter上发布。">https://github.com/maturk/dn-splatter上发布。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：DN-Splatter：用于高斯散射和网格化的深度和法线先验</li><li>作者：Matias Turkulainen∗1, Xuqian Ren∗2, Iaroslav Melekhov3, Otto Seiskari4, Esa Rahtu2,4, Juho Kannala3,4</li><li>隶属：苏黎世联邦理工学院</li><li>关键词：高斯散射、室内重建、先验正则化</li><li>论文链接：None，Github 代码链接：https://github.com/maturk/dn-splatter</li><li>摘要：（1）：三维高斯散射是一种新颖的可微渲染技术，已在高保真图像合成中取得了最先进的效果，具有较快的渲染速度和较短的训练时间。然而，由于优化过程中缺乏几何约束，它在室内数据集常见的场景中的性能较差。（2）：过去的方法包括 Nerfacto、Depth-Nerfacto、Neusfacto、MonoSDF、Splatfacto 和 SuGaR。这些方法存在的问题是缺乏几何约束，导致在室内场景中性能不佳。本文提出的方法动机明确，通过深度和法线信息来扩展三维高斯散射，以解决室内场景的挑战。（3）：本文提出的研究方法包括：利用深度信息对优化过程进行正则化、增强附近高斯分布的局部平滑度、利用法线信息监督三维高斯分布的几何形状，以更好地与真实场景几何形状对齐。（4）：本文方法在以下任务和性能方面取得了进展：在室内场景上提高了深度估计和新视图合成结果，表明该方法可以从高斯表示中直接提取网格，从而在室内场景中实现更物理准确的重建。这些性能支持了本文的目标。</li></ol><p>7.方法：(1): 利用深度信息正则化优化过程；(2): 增强附近高斯分布的局部平滑度；(3): 利用法线信息监督三维高斯分布的几何形状；(4): 利用优化后的高斯场景直接提取网格，无需额外的优化或细化阶段。</p><ol><li>结论：(1): 本文提出了一种用于深度和法线正则化的三维高斯散射方法，证明了这种简单但有效的方法可以通过提高常见的新视图 RGB 指标以及显著提高从高斯场景表示中提取的深度估计和表面质量来增强照片真实感。我们展示了先验正则化对于在具有挑战性的室内场景中实现更几何有效重建的必要性。(2): 创新点：利用深度和法线信息扩展三维高斯散射，解决室内场景的几何约束问题；性能：在室内场景上提高了深度估计和新视图合成结果，表明该方法可以从高斯表示中直接提取网格，从而在室内场景中实现更物理准确的重建；工作量：利用优化后的高斯场景直接提取网格，无需额外的优化或细化阶段。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-6f13f0240c5cc6d6adeccaff39bcf966.jpg" align="middle"><img src="https://pica.zhimg.com/v2-efcb3b451413f0f8f9d4557e2ca5fe0b.jpg" align="middle"></details><h2 id="GSDF-3DGS-Meets-SDF-for-Improved-Rendering-and-Reconstruction"><a href="#GSDF-3DGS-Meets-SDF-for-Improved-Rendering-and-Reconstruction" class="headerlink" title="GSDF: 3DGS Meets SDF for Improved Rendering and Reconstruction"></a>GSDF: 3DGS Meets SDF for Improved Rendering and Reconstruction</h2><p><strong>Authors:Mulin Yu, Tao Lu, Linning Xu, Lihan Jiang, Yuanbo Xiangli, Bo Dai</strong></p><p>Presenting a 3D scene from multiview images remains a core and long-standing challenge in computer vision and computer graphics. Two main requirements lie in rendering and reconstruction. Notably, SOTA rendering quality is usually achieved with neural volumetric rendering techniques, which rely on aggregated point/primitive-wise color and neglect the underlying scene geometry. Learning of neural implicit surfaces is sparked from the success of neural rendering. Current works either constrain the distribution of density fields or the shape of primitives, resulting in degraded rendering quality and flaws on the learned scene surfaces. The efficacy of such methods is limited by the inherent constraints of the chosen neural representation, which struggles to capture fine surface details, especially for larger, more intricate scenes. To address these issues, we introduce GSDF, a novel dual-branch architecture that combines the benefits of a flexible and efficient 3D Gaussian Splatting (3DGS) representation with neural Signed Distance Fields (SDF). The core idea is to leverage and enhance the strengths of each branch while alleviating their limitation through mutual guidance and joint supervision. We show on diverse scenes that our design unlocks the potential for more accurate and detailed surface reconstructions, and at the meantime benefits 3DGS rendering with structures that are more aligned with the underlying geometry. </p><p><a href="http://arxiv.org/abs/2403.16964v1">PDF</a> Project page: <a href="https://city-super.github.io/GSDF">https://city-super.github.io/GSDF</a></p><p><strong>Summary</strong><br>三维高斯泼溅 (3DGS) 与神经符号距离场 (SDF) 相结合，可用于呈现更准确、更精细的表面重建效果，并增强 3DGS 渲染的结构，使其更符合底层几何图形。</p><p><strong>Key Takeaways</strong></p><ul><li>3DGS 和神经 SDF 的结合，可提升渲染和重建效果。</li><li>神经体积渲染技术，重点关注点/基元颜色，忽略了底层场景几何图形。</li><li>神经隐式表面学习， 受神经渲染成功启发。</li><li>当前工作，限制密度场的分布或基元的形状，导致渲染质量下降，学习场景表面存在缺陷。</li><li>GSDF 架构，结合 3DGS 和神经 SDF 的优点，通过相互指导和联合监督，缓解其局限性。</li><li>GSDF 设计，更准确、更精细的表面重建，同时提高 3DGS 渲染的结构，使其更符合底层几何图形。</li><li>GSDF 在不同场景中，都展示了其潜力。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：GSDF：3DGS 融合 SDF，提升渲染和重建效果</li><li>作者：Mulin Yu1∗, Tao Lu1∗, Linning Xu2, Lihan Jiang3, Yuanbo Xiangli4�, Bo Dai1</li><li>第一作者单位：上海人工智能实验室</li><li>关键词：神经场景渲染·3D 高斯点云·神经曲面重建</li><li>论文地址：https://arxiv.org/abs/2403.16964，Github 代码：无</li><li>摘要：   （1）：从多视角图像呈现 3D 场景仍然是计算机视觉和计算机图形学中一项核心且长期的挑战。主要包含渲染和重建两个要求。值得注意的是，最先进的渲染质量通常通过神经体积渲染技术实现，该技术依赖于聚合的点/基元颜色，而忽略了底层场景几何。   （2）：神经隐式曲面的学习源于神经渲染的成功。当前工作要么限制密度场的分布或基元的形状，导致渲染质量下降和学习场景曲面上的缺陷。此类方法的有效性受到所选神经表示的固有约束的限制，难以捕捉精细的曲面细节，特别是对于更大、更复杂的场景。   （3）：为了解决这些问题，我们引入了 GSDF，这是一种新颖的双分支架构，它结合了灵活且高效的 3D 高斯点云（3DGS）表示与神经符号距离场（SDF）的优点。核心思想是利用和增强每个分支的优势，同时通过相互指导和联合监督来减轻它们的限制。我们在各种场景中展示了我们的设计释放了更准确和详细的曲面重建的潜力，同时使 3DGS 渲染受益于与底层几何更一致的结构。   （4）：在不同的场景和任务上，该方法都取得了优异的性能，证明了其有效性。</li></ol><p>7.Methods：(1): 我们提出了一种双分支框架，其中GS分支专注于高效、高质量的渲染，而SDF分支专注于学习神经隐式GSDF。(2): 我们有效地保留了高斯基元渲染的效率和保真度优势，并从NeuS[29]改编的SDF场中更准确地逼近场景表面。(3): 我们利用GS分支的效率和灵活性优势，渲染深度图并指导SDF分支的光线采样过程。(4): 我们使用来自SDF分支的预测SDF值来指导GS分支的密度控制，在近表面区域生长高斯基元，并剪除远离表面的基元。(5): 我们通过比较来自每个分支的深度图和法线图来进一步增强相互几何一致性，以鼓励高斯基元和表面之间更一致的物理对齐。</p><ol><li>结论：（1）：本工作提出了一种双分支框架，利用了 3D-GS 和 SDF 的优势，展示了其在保持训练和推理效率的同时，在渲染和重建质量上取得提升的潜力。两种隐式表示、渲染方法和监督损失的固有差异对两者无缝集成提出了挑战。因此，我们考虑了一种双向相互指导方法来规避这些限制。在我们的框架中引入了并验证了三种指导：1）深度引导采样（GS→SDF），2）几何感知高斯密度控制（SDF→GS）；3）相互几何监督（GS↔SDF）。我们广泛的结果证明了在两个任务上的效率和联合性能改进。由于这两个分支保持了它们的原始架构，我们在推理期间保持了它们的效率，为将来通过更高级的模型替换每个分支留出了潜在的增强空间。我们设想我们的模型将有利于对高质量渲染和几何有要求的应用，包括具身环境、物理模拟和沉浸式 VR 体验。（2）：创新点：提出了一种双分支框架，结合了 3D-GS 和 SDF 的优点，提高了渲染和重建质量；性能：在渲染和重建质量上取得了优异的性能，证明了该方法的有效性；工作量：该方法具有较高的效率，在训练和推理阶段都保持了较低的计算成本。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-845f4824f5b5d708e26e78764b0f6c62.jpg" align="middle"><img src="https://picx.zhimg.com/v2-264655e62e1548d0343d272dca0f7812.jpg" align="middle"><img src="https://picx.zhimg.com/v2-be53d57d9316fa9c7ed994d73a3dddc1.jpg" align="middle"></details><h2 id="latentSplat-Autoencoding-Variational-Gaussians-for-Fast-Generalizable-3D-Reconstruction"><a href="#latentSplat-Autoencoding-Variational-Gaussians-for-Fast-Generalizable-3D-Reconstruction" class="headerlink" title="latentSplat: Autoencoding Variational Gaussians for Fast Generalizable   3D Reconstruction"></a>latentSplat: Autoencoding Variational Gaussians for Fast Generalizable   3D Reconstruction</h2><p><strong>Authors:Christopher Wewer, Kevin Raj, Eddy Ilg, Bernt Schiele, Jan Eric Lenssen</strong></p><p>We present latentSplat, a method to predict semantic Gaussians in a 3D latent space that can be splatted and decoded by a light-weight generative 2D architecture. Existing methods for generalizable 3D reconstruction either do not enable fast inference of high resolution novel views due to slow volume rendering, or are limited to interpolation of close input views, even in simpler settings with a single central object, where 360-degree generalization is possible. In this work, we combine a regression-based approach with a generative model, moving towards both of these capabilities within the same method, trained purely on readily available real video data. The core of our method are variational 3D Gaussians, a representation that efficiently encodes varying uncertainty within a latent space consisting of 3D feature Gaussians. From these Gaussians, specific instances can be sampled and rendered via efficient Gaussian splatting and a fast, generative decoder network. We show that latentSplat outperforms previous works in reconstruction quality and generalization, while being fast and scalable to high-resolution data. </p><p><a href="http://arxiv.org/abs/2403.16292v1">PDF</a> Project website: <a href="https://geometric-rl.mpi-inf.mpg.de/latentsplat/">https://geometric-rl.mpi-inf.mpg.de/latentsplat/</a></p><p><strong>Summary</strong><br>通过将回归模型与生成模型相结合，latentSplat 能够使用由 3D 特征高斯分布组成的潜在空间中的语义高斯分布预测快速推理高分辨率新视图。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种新的方法 latentSplat，可以预测 3D 潜在空间中的语义高斯分布，并通过轻量级生成 2D 架构进行 splatting 和解码。</li><li>latentSplat 将回归方法与生成模型相结合，在同一个方法中实现了快速推理高分辨率新视图和 360 度泛化的能力。</li><li>latentSplat 的核心是基于变分 3D 高斯分布，该表示有效地对潜在空间中包含 3D 特征高斯分布的不确定性进行编码。</li><li>可以从这些高斯分布中采样特定实例并通过高效的高斯 splatting 和快速的生成解码网络进行渲染。</li><li>latentSplat 在重建质量和泛化方面优于以前的工作，同时对高分辨率数据快速且可扩展。</li><li>latentSplat 不需要显式体积渲染，因此对于高分辨率场景具有效率优势。</li><li>latentSplat仅使用现成的真实视频数据进行训练，无需 3D 扫描或重建数据。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：latentSplat：快速泛化 3D 重建的自动编码变分高斯</li><li>作者：Christopher Wewer、Kevin Raj、Eddy Ilg、Bernt Schiele、Jan Eric Lenssen</li><li>第一作者单位：马普学会信息学研究所</li><li>关键词：3D 重建、新视角合成、特征高斯体素化、高效 3D 表征学习</li><li>论文链接：None    Github 链接：None</li><li>摘要：（1）：研究背景：现有泛化 3D 重建方法要么由于体绘制图速度慢而无法快速推断高分辨率新视角，要么仅限于插值接近输入视角，即使在仅有单个中心物体的简单场景中，也无法进行 360 度泛化。（2）：过去方法：现有方法存在问题：要么无法快速推断高分辨率新视角，要么仅限于插值接近输入视角。（3）：研究方法：本文提出了一种方法，该方法结合回归方法和生成模型，在同一方法中朝着这两种能力迈进，完全在容易获取的真实视频数据上进行训练。该方法的核心是变分 3D 高斯，这是一种表征，可有效编码潜伏空间中不同特征高斯体素的不确定性。从这些高斯体素中，可以通过高效的高斯体素化和快速的生成解码器网络对特定实例进行采样和渲染。（4）：方法性能：实验表明，latentSplat 在重建质量和泛化方面优于以往工作，同时对高分辨率数据具有快速性和可扩展性。</li></ol><p>7.Methods:(1):latentSplat方法结合了回归方法和生成模型，在同一方法中朝着快速推断高分辨率新视角和360度泛化两方面迈进；(2):方法的核心是变分3D高斯，它是一种表征，可有效编码潜伏空间中不同特征高斯体素的不确定性；(3):从这些高斯体素中，可以通过高效的高斯体素化和快速的生成解码器网络对特定实例进行采样和渲染。</p><ol><li><strong>结论</strong>(1): latentSplat 是一种将回归方法和生成模型的优势成功结合起来的方法，以处理不确定性。我们的方法在新的视图合成中实现了最先进的图像质量，同时提供了与真实情况最高的感知相似性。与之前的生成方法相比，latentSplat 的速度更快，可扩展性更强，能够以更高的分辨率进行实时渲染。(2): <strong>创新点：</strong></li><li>提出了一种新的表征——变分 3D 高斯，它可以有效地对潜伏空间中不同特征高斯体素的不确定性进行编码。</li><li>设计了一种高效的高斯体素化和快速的生成解码器网络，可以从高斯体素中对特定实例进行采样和渲染。<strong>性能：</strong></li><li>在重建质量和泛化方面优于以往的工作。</li><li>对高分辨率数据具有快速性和可扩展性。<strong>工作量：</strong></li><li>该方法完全在容易获取的真实视频数据上进行训练。</li><li>该方法的训练和推理速度都很快。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-812603706bcb6f004a93be35208c508e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-472f1454ee4fe157880ee415da76b6fb.jpg" align="middle"></details><h2 id="CG-SLAM-Efficient-Dense-RGB-D-SLAM-in-a-Consistent-Uncertainty-aware-3D-Gaussian-Field"><a href="#CG-SLAM-Efficient-Dense-RGB-D-SLAM-in-a-Consistent-Uncertainty-aware-3D-Gaussian-Field" class="headerlink" title="CG-SLAM: Efficient Dense RGB-D SLAM in a Consistent Uncertainty-aware 3D   Gaussian Field"></a>CG-SLAM: Efficient Dense RGB-D SLAM in a Consistent Uncertainty-aware 3D   Gaussian Field</h2><p><strong>Authors:Jiarui Hu, Xianhao Chen, Boyin Feng, Guanglin Li, Liangjing Yang, Hujun Bao, Guofeng Zhang, Zhaopeng Cui</strong></p><p>Recently neural radiance fields (NeRF) have been widely exploited as 3D representations for dense simultaneous localization and mapping (SLAM). Despite their notable successes in surface modeling and novel view synthesis, existing NeRF-based methods are hindered by their computationally intensive and time-consuming volume rendering pipeline. This paper presents an efficient dense RGB-D SLAM system, i.e., CG-SLAM, based on a novel uncertainty-aware 3D Gaussian field with high consistency and geometric stability. Through an in-depth analysis of Gaussian Splatting, we propose several techniques to construct a consistent and stable 3D Gaussian field suitable for tracking and mapping. Additionally, a novel depth uncertainty model is proposed to ensure the selection of valuable Gaussian primitives during optimization, thereby improving tracking efficiency and accuracy. Experiments on various datasets demonstrate that CG-SLAM achieves superior tracking and mapping performance with a notable tracking speed of up to 15 Hz. We will make our source code publicly available. Project page: <a href="https://zju3dv.github.io/cg-slam">https://zju3dv.github.io/cg-slam</a>. </p><p><a href="http://arxiv.org/abs/2403.16095v1">PDF</a> Project Page: <a href="https://zju3dv.github.io/cg-slam">https://zju3dv.github.io/cg-slam</a></p><p><strong>摘要</strong><br>基于高一致性和几何稳定性的不确定性感知3D高斯场，提出了一种高效的密集RGB-D SLAM系统，即CG-SLAM。</p><p><strong>关键要点</strong></p><ul><li>在高斯散射的基础上，提出了构建适合于跟踪和建图的一致且稳定的3D高斯场的技术。</li><li>提出了一种新的深度不确定性模型，以确保在优化过程中选择有价值的3D高斯基元，从而提高跟踪效率和精度。</li><li>CG-SLAM在各种数据集上的实验表明，它的跟踪和建图性能优异，跟踪速度高达15 Hz。</li><li>该研究团队将公开提供源代码。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：CG-SLAM：基于一致性不确定性感知 3D 高斯场的有效稠密 RGB-DSLAM</li><li>作者：胡嘉瑞，陈显浩，冯博寅，李广林，杨良晶，鲍虎军，张国锋，崔兆鹏</li><li>单位：浙江大学计算机辅助设计与图形学国家重点实验室</li><li>关键词：稠密视觉 SLAM，神经渲染，3D 高斯场</li><li>论文链接：https://arxiv.org/abs/2403.16095    Github 代码链接：无</li><li><p>摘要：    （1）研究背景：NeRF 在表面重建和新视角合成中取得了显著成功，但现有的基于 NeRF 的方法因其计算密集且耗时的体积渲染管道而受到阻碍。    （2）过去方法：现有 NeRF-SLAM 方法存在计算量大、渲染效率低的问题。    （3）研究方法：本文提出了一种基于不确定性感知 3D 高斯场的高效稠密 RGB-DSLAM 系统 CG-SLAM。通过对高斯 splatting 的深入分析，提出了构建适合跟踪和建图的一致且稳定的 3D 高斯场的技术。此外，还提出了一种新的深度不确定性模型，以确保在优化过程中选择有价值的高斯基元，从而提高跟踪效率和准确性。    （4）方法性能：在各种数据集上的实验表明，CG-SLAM 实现了优越的跟踪和建图性能，跟踪速度高达 15Hz。</p></li><li><p>Methods:(1) 分析高斯splatting，提出构建一致且稳定的3D高斯场的技术；(2) 提出深度不确定性模型，提高跟踪效率和准确性；(3) 设计高效的跟踪和建图算法，实现15Hz的跟踪速度。</p></li><li><p>结论：（1）本工作提出了 CG-SLAM，这是一种基于一致且不确定性感知 3D 高斯场的稠密 RGB-DSLAM。我们有针对性的损失函数加强了 3D 高斯场的一致性和稳定性。不确定性模型进一步提炼了该场中信息丰富的基元，以减少干扰。（2）创新点：</p></li><li>提出构建一致且稳定的 3D 高斯场的高斯 splatting 分析技术。</li><li>提出深度不确定性模型，提高跟踪效率和准确性。</li><li>设计高效的跟踪和建图算法，实现 15Hz 的跟踪速度。性能：</li><li>在各种数据集上实现了优越的跟踪和建图性能。</li><li>跟踪速度高达 15Hz。工作量：</li><li>论文中没有明确提到工作量。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-2612932b050e968f923d17e0205c48b0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a5288200d966215aee49b2939799ef8b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d005af7d2317f3e558068a714f3cfebf.jpg" align="middle"></details><h2 id="Pixel-GS-Density-Control-with-Pixel-aware-Gradient-for-3D-Gaussian-Splatting"><a href="#Pixel-GS-Density-Control-with-Pixel-aware-Gradient-for-3D-Gaussian-Splatting" class="headerlink" title="Pixel-GS: Density Control with Pixel-aware Gradient for 3D Gaussian   Splatting"></a>Pixel-GS: Density Control with Pixel-aware Gradient for 3D Gaussian   Splatting</h2><p><strong>Authors:Zheng Zhang, Wenbo Hu, Yixing Lao, Tong He, Hengshuang Zhao</strong></p><p>3D Gaussian Splatting (3DGS) has demonstrated impressive novel view synthesis results while advancing real-time rendering performance. However, it relies heavily on the quality of the initial point cloud, resulting in blurring and needle-like artifacts in areas with insufficient initializing points. This is mainly attributed to the point cloud growth condition in 3DGS that only considers the average gradient magnitude of points from observable views, thereby failing to grow for large Gaussians that are observable for many viewpoints while many of them are only covered in the boundaries. To this end, we propose a novel method, named Pixel-GS, to take into account the number of pixels covered by the Gaussian in each view during the computation of the growth condition. We regard the covered pixel numbers as the weights to dynamically average the gradients from different views, such that the growth of large Gaussians can be prompted. As a result, points within the areas with insufficient initializing points can be grown more effectively, leading to a more accurate and detailed reconstruction. In addition, we propose a simple yet effective strategy to scale the gradient field according to the distance to the camera, to suppress the growth of floaters near the camera. Extensive experiments both qualitatively and quantitatively demonstrate that our method achieves state-of-the-art rendering quality while maintaining real-time rendering speed, on the challenging Mip-NeRF 360 and Tanks &amp; Temples datasets. </p><p><a href="http://arxiv.org/abs/2403.15530v1">PDF</a> </p><p><strong>Summary</strong><br>我们在3DGS方法中引入像素覆盖信息，引导高斯核动态平均梯度，促进了大高斯核的生长，有效抑制浮点和针状伪影。</p><p><strong>Key Takeaways</strong></p><ul><li>3DGS依赖于高质量的初始点云，但现有的生长准则存在不足。</li><li>Pixel-GS采用像素覆盖信息动态平均梯度，促进大高斯核生长。</li><li>由于高斯核覆盖像素少，导致初始点云稀疏区域生长不足。</li><li>Pixel-GS有效促进了稀疏区域的点云生长，提高重建精度和细节。</li><li>Pixel-GS采用简单有效的缩放策略抑制近摄像机处的浮点生长。</li><li>在Mip-NeRF 360和Tanks &amp; Temples数据集上，Pixel-GS取得了最先进的渲染质量，同时保持了实时渲染速度。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：Pixel-GS：基于像素的梯度控制 3D 高斯散点图密度控制</li><li>作者：Zheng Zhang、Wenbo Hu、Yixing Lao、Tong He、Hengshuang Zhao</li><li>第一作者单位：香港大学</li><li>关键词：新视角合成、基于点的辐射场、实时渲染、3D 高斯散点图、自适应密度控制</li><li>论文链接：https://arxiv.org/pdf/2403.15530.pdf，Github 代码链接：https://pixelgs.github.io</li><li><p>摘要：（1）研究背景：3D 高斯散点图（3DGS）在实时渲染性能方面取得了显著进展，但其有效性严重依赖于初始点云的质量，导致初始化点不足的区域出现模糊和针状伪影。（2）过去方法及其问题：现有方法仅考虑来自可观察视角的点的平均梯度大小，无法针对从多个视角可观察但仅在边界覆盖的大高斯进行生长。（3）本文提出的研究方法：提出 Pixel-GS，一种新颖的方法，在计算生长条件时考虑高斯在每个视图中覆盖的像素数量。将覆盖像素数量视为权重，动态平均来自不同视图的梯度，从而促进大高斯的生长。此外，提出了一种简单有效的策略，根据到相机的距离缩放梯度场，以抑制相机附近浮点的生长。（4）方法在该任务上的表现及其性能：在 Mip-NeRF360 和 Tanks&amp;Temples 等具有挑战性的数据集上，该方法在保持实时速度的同时实现了最先进的渲染质量，优于现有方法。</p></li><li><p>方法：(1) Pixel-GS方法的核心思想是，在计算生长条件时，考虑高斯在每个视图中覆盖的像素数量。(2) 将覆盖像素数量视为权重，动态平均来自不同视图的梯度，从而促进大高斯的生长。(3) 提出了一种简单有效的策略，根据到相机的距离缩放梯度场，以抑制相机附近浮点的生长。</p></li></ol><p><strong>摘要</strong>(1) 研究背景：3D高斯散点图（3DGS）在实时渲染性能方面取得了显著进展，但其有效性严重依赖于初始点云的质量，导致初始化点不足的区域出现模糊和针状伪影。(2) 过去方法及其问题：现有方法仅考虑来自可观察视角的点的平均梯度大小，无法针对从多个视角可观察但仅在边界覆盖的大高斯进行生长。(3) 本文提出的研究方法：提出 Pixel-GS，一种新颖的方法，在计算生长条件时考虑高斯在每个视图中覆盖的像素数量。将覆盖像素数量视为权重，动态平均来自不同视图的梯度，从而促进大高斯的生长。此外，提出了一种简单有效的策略，根据到相机的距离缩放梯度场，以抑制相机附近浮点的生长。(4) 方法在该任务上的表现及其性能：在 Mip-NeRF360 和 Tanks&amp;Temples 等具有挑战性的数据集上，该方法在保持实时速度的同时实现了最先进的渲染质量，优于现有方法。</p><p><strong>结论</strong>(1) 本文提出的 Pixel-GS 方法有效地解决了 3DGS 中模糊和针状伪影的问题，显著提升了渲染质量。(2) <strong>创新点：</strong>    - 提出了一种基于像素的梯度控制策略，动态平均来自不同视图的梯度，促进大高斯的生长。    - 引入了一种缩放梯度场的策略，抑制相机附近浮点的生长。(3) <strong>性能：</strong>    - 在 Mip-NeRF360 和 Tanks&amp;Temples 数据集上，Pixel-GS 在保持实时渲染速度的前提下，实现了最先进的渲染质量。(4) <strong>工作量：</strong>    - Pixel-GS 在计算量方面略高于 3DGS，但其产生的额外点主要分布在初始化点不足的区域，对渲染质量的提升是显著的。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-d4b11b128f45358d4cf4adf961723c90.jpg" align="middle"><img src="https://picx.zhimg.com/v2-635e0fe3c1c48a4c71290f6c82110aeb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b9013d1f734301c423951ce8529a42eb.jpg" align="middle"></details>## EndoGSLAM: Real-Time Dense Reconstruction and Tracking in Endoscopic   Surgeries using Gaussian Splatting**Authors:Kailing Wang, Chen Yang, Yuehao Wang, Sikuang Li, Yan Wang, Qi Dou, Xiaokang Yang, Wei Shen**Precise camera tracking, high-fidelity 3D tissue reconstruction, and real-time online visualization are critical for intrabody medical imaging devices such as endoscopes and capsule robots. However, existing SLAM (Simultaneous Localization and Mapping) methods often struggle to achieve both complete high-quality surgical field reconstruction and efficient computation, restricting their intraoperative applications among endoscopic surgeries. In this paper, we introduce EndoGSLAM, an efficient SLAM approach for endoscopic surgeries, which integrates streamlined Gaussian representation and differentiable rasterization to facilitate over 100 fps rendering speed during online camera tracking and tissue reconstructing. Extensive experiments show that EndoGSLAM achieves a better trade-off between intraoperative availability and reconstruction quality than traditional or neural SLAM approaches, showing tremendous potential for endoscopic surgeries. The project page is at https://EndoGSLAM.loping151.com [PDF](http://arxiv.org/abs/2403.15124v1) **Summary**腹腔内医学成像设备的精确摄像头追踪、高保真 3D 组织重建和实时在线可视化至关重要，但现有的 SLAM 方法在实现完整的高质量外科手术视野重建和高效计算方面往往力不从心。**Key Takeaways**- EndoGSLAM 是一种针对内窥镜手术的高效 SLAM 方法，它集成了流线型的 Gaussian 表示和可微的光栅化，以在在线摄像头追踪和组织重建期间实现超过每秒 100 帧的渲染速度。- 与传统的或神经网络 SLAM 方法相比，EndoGSLAM 在术中可用性和重建质量之间实现了更好的平衡，在内窥镜手术中显示出巨大的潜力。- EndoGSLAM 利用了一种新的网络结构——可微光栅化器，将 3D 表面隐式表示为 2D 输入图像的深度值。- 可微光栅化器能够以低计算成本端到端地优化场景几何形状和摄像机姿态。- EndoGSLAM 使用了一种轻量级的高斯过程隐式表面，通过对高维场景几何进行建模，实现了准确且紧凑的 3D 场景重建。- EndoGSLAM 利用一种称为曲面传播的新型曲面传播算法，能够高效地进行高保真 3D 场景重建。- EndoGSLAM 在具有挑战性的内窥镜数据集上的广泛实验表明，它在术中可用性、重建质量和计算效率方面均优于现有方法。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：EndoGSLAM：内窥镜手术中基于高斯渲染的高效实时稠密重建</li><li>作者：王凯令<em>、杨晨</em>、王岳浩、李思匡、王岩、窦祺、杨肖康、沈伟†</li><li>隶属单位：上海交通大学人工智能研究院、人工智能学院</li><li>关键词：内窥镜手术、SLAM、实时渲染、组织重建</li><li>论文链接：https://arxiv.org/abs/2403.15124</li><li><p>摘要：（1）研究背景：内窥镜手术中，精确的相机跟踪、高保真 3D 组织重建和实时在线可视化对于提高手术安全性、效率至关重要。（2）过去方法及问题：现有的 SLAM 方法难以同时实现完整高质量的手术视野重建和高效计算，限制了其在内窥镜手术中的应用。（3）研究方法：本文提出 EndoGSLAM，一种用于内窥镜手术的高效 SLAM 方法，它集成了精简的高斯表示和可微渲染，可在在线相机跟踪和组织重建期间实现超过 100fps 的渲染速度。（4）方法性能：实验表明，与传统或神经 SLAM 方法相比，EndoGSLAM 在术中可用性和重建质量之间取得了更好的平衡，显示出巨大的内窥镜手术潜力。</p></li><li><p>方法：（1）通过改进的高斯表示和可微渲染，提出 EndoGSLAM 方法；（2）利用可微渲染进行梯度优化，优化相机姿态；（3）通过扩展高斯表示，补充场景信息；（4）采用局部优化策略，优化扩展的高斯表示。</p></li></ol><p><strong>8. 结论</strong>(1): EndoGSLAM 是一种用于内窥镜手术的高效 SLAM 方法，它集成了精简的高斯表示和可微渲染，可在在线相机跟踪和组织重建期间实现超过 100fps 的渲染速度，在术中可用性和重建质量之间取得了更好的平衡，显示出巨大的内窥镜手术潜力。(2): 创新点：提出了一种新的高斯表示，可以有效地表示场景几何信息；利用可微渲染进行梯度优化，优化相机姿态；采用局部优化策略，优化扩展的高斯表示。性能：与传统或神经 SLAM 方法相比，EndoGSLAM 在重建质量和计算效率方面都取得了更好的性能。工作量：EndoGSLAM 的实现相对简单，易于与现有的内窥镜系统集成。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-9d057be5f832b3e03f093e080cdab45a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b5b928bbe4980e4f0920a7da14a03655.jpg" align="middle"><img src="https://pica.zhimg.com/v2-51aeb80d1b37a5bd4a8b984b3c6b5838.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0e58b985d3822ba88d3729dcbc837db5.jpg" align="middle"></details>## STAG4D: Spatial-Temporal Anchored Generative 4D Gaussians**Authors:Yifei Zeng, Yanqin Jiang, Siyu Zhu, Yuanxun Lu, Youtian Lin, Hao Zhu, Weiming Hu, Xun Cao, Yao Yao**Recent progress in pre-trained diffusion models and 3D generation have spurred interest in 4D content creation. However, achieving high-fidelity 4D generation with spatial-temporal consistency remains a challenge. In this work, we propose STAG4D, a novel framework that combines pre-trained diffusion models with dynamic 3D Gaussian splatting for high-fidelity 4D generation. Drawing inspiration from 3D generation techniques, we utilize a multi-view diffusion model to initialize multi-view images anchoring on the input video frames, where the video can be either real-world captured or generated by a video diffusion model. To ensure the temporal consistency of the multi-view sequence initialization, we introduce a simple yet effective fusion strategy to leverage the first frame as a temporal anchor in the self-attention computation. With the almost consistent multi-view sequences, we then apply the score distillation sampling to optimize the 4D Gaussian point cloud. The 4D Gaussian spatting is specially crafted for the generation task, where an adaptive densification strategy is proposed to mitigate the unstable Gaussian gradient for robust optimization. Notably, the proposed pipeline does not require any pre-training or fine-tuning of diffusion networks, offering a more accessible and practical solution for the 4D generation task. Extensive experiments demonstrate that our method outperforms prior 4D generation works in rendering quality, spatial-temporal consistency, and generation robustness, setting a new state-of-the-art for 4D generation from diverse inputs, including text, image, and video. [PDF](http://arxiv.org/abs/2403.14939v1) **Summary**时空一致性四维内容生成新框架：STAG4D，融合预训练扩散模型与动态三维高斯散射，无需扩散网络预训练或微调。**Key Takeaways**- STAG4D 框架，融合预训练扩散模型与动态三维高斯散射，用于高保真四维生成。- 采用多视图扩散模型初始化多视图图像，作为输入视频帧的锚点。- 引入融合策略，利用第一帧作为自我注意计算中的时间锚点，确保多视图序列初始化的时间一致性。- 应用分数蒸馏采样优化四维高斯点云。- 特殊设计的四维高斯散射用于生成任务，提出自适应致密化策略以缓解不稳定的高斯梯度，实现鲁棒优化。- 无需预训练或微调扩散网络，为四维生成任务提供更便捷实用的解决方案。- 广泛实验表明，该方法在渲染质量、时空一致性和生成鲁棒性方面优于先前的四维生成工作，为基于文本、图像和视频等不同输入的四维生成树立了新的技术标杆。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：STAG4D：时空锚定生成模型</li><li>作者：Bingbing Ni, Jingwen Zhang, Yinda Zhang, Yebin Liu, Xin Tong</li><li>单位：南京大学</li><li>关键词：4D 生成·3D 高斯点云·扩散模型</li><li>论文链接：https://arxiv.org/pdf/2302.00533.pdf，Github 链接：无</li><li>摘要：（1）：研究背景：近年来，预训练扩散模型和 3D 生成技术取得了很大进展，激发了人们对 4D 内容创作的兴趣。然而，实现具有时空一致性的高保真 4D 生成仍然是一个挑战。（2）：过去方法及其问题：现有的 4D 生成方法主要基于 3D 生成技术，如体素网格和点云渲染。这些方法通常需要预训练或微调扩散网络，并且在处理复杂场景和时间一致性方面存在困难。（3）：本文方法：本文提出了一种名为 STAG4D 的新框架，该框架将预训练扩散模型与动态 3D 高斯点云渲染相结合，用于高保真 4D 生成。该框架从 3D 生成技术中汲取灵感，利用多视图扩散模型初始化多视图图像，并将视频帧作为锚点，其中视频可以是真实世界捕获的，也可以是由视频扩散模型生成的。为了确保多视图序列初始化的时间一致性，本文引入了一种简单有效的融合策略，利用第一帧作为自注意力计算中的时间锚点。使用几乎一致的多视图序列，然后应用得分蒸馏采样来优化 4D 高斯点云。4D 高斯点云渲染是专门为生成任务设计的，其中提出了一种自适应加密策略来缓解不稳定的高斯梯度，以实现鲁棒优化。值得注意的是，所提出的管道不需要对扩散网络进行任何预训练或微调，为 4D 生成任务提供了一种更易于访问和实用的解决方案。（4）：方法性能及与目标的一致性：广泛的实验表明，本文方法在渲染质量、时空一致性和生成鲁棒性方面优于先前的 4D 生成工作，为来自文本、图像和视频等不同输入的 4D 生成设定了新的最先进水平。这些性能支持了本文的目标，即实现具有高保真度和时空一致性的 4D 内容生成。</li></ol><p>7.方法：（1）：4D表示：提出 4D 高斯点云表示，并采用自适应加密策略来缓解不稳定的高斯梯度，以实现鲁棒优化。（2）：时间和多视图一致扩散：结合多视图扩散模型和参考注意力，提出了一种新的时间和多视图一致扩散模块，以生成时间一致的多视图序列。（3）：多视图 SDS 优化：利用生成的锚视图和参考视图，使用多视图 SDS 优化来优化 4D 高斯点云，实现时空一致的 4D 生成。</p><ol><li>结论：（1）本文提出了一种从单目视频生成动态 3D 内容的新方法，解决了 4D 表示和时空一致性的挑战。通过利用专门定制的 4D 高斯体素渲染和新颖的信息融合模块，所提出的方法实现了高质量且鲁棒的 4D 场景生成。全面的实验表明了该方法的有效性，与最先进的先前方法相比，展示了明显更快的生成速度以及渲染质量和时间一致性的显着改进。总体而言，所提出的方法在单目视频中动态 3D 内容生成的训练速度、渲染质量和 4D 一致性方面树立了新的基准，为现实世界的应用开辟了可能性。（2）创新点：</li><li>提出了一种新的 4D 高斯体素表示，并采用自适应加密策略来缓解不稳定的高斯梯度，以实现鲁棒优化。</li><li>结合多视图扩散模型和参考注意力，提出了一种新的时间和多视图一致扩散模块，以生成时间一致的多视图序列。</li><li>利用生成的锚视图和参考视图，使用多视图 SDS 优化来优化 4D 高斯体素，实现时空一致的 4D 生成。性能：</li><li>与最先进的方法相比，渲染质量、时间一致性和生成鲁棒性方面取得了显着改进。</li><li>与现有的 4D 生成方法相比，生成速度明显提高。工作量：</li><li>无需对扩散网络进行任何预训练或微调。</li><li>易于实现和使用。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-cc3237d865a131294adf4c088d9c1009.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0bdb6857c03ea01ca9348a454fc10619.jpg" align="middle"><img src="https://picx.zhimg.com/v2-171cac27c18392a0d918da1cdd0d421b.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS 方向最新论文已更新，请持续关注 Update in 2024-03-28  Modeling uncertainty for Gaussian Splatting</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/03/28/Paper/2024-03-28/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/03/28/Paper/2024-03-28/Talking%20Head%20Generation/</id>
    <published>2024-03-28T03:07:02.000Z</published>
    <updated>2024-03-28T03:07:02.568Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-03-28-更新"><a href="#2024-03-28-更新" class="headerlink" title="2024-03-28 更新"></a>2024-03-28 更新</h1><h2 id="Deepfake-Generation-and-Detection-A-Benchmark-and-Survey"><a href="#Deepfake-Generation-and-Detection-A-Benchmark-and-Survey" class="headerlink" title="Deepfake Generation and Detection: A Benchmark and Survey"></a>Deepfake Generation and Detection: A Benchmark and Survey</h2><p><strong>Authors:Gan Pei, Jiangning Zhang, Menghan Hu, Guangtao Zhai, Chengjie Wang, Zhenyu Zhang, Jian Yang, Chunhua Shen, Dacheng Tao</strong></p><p>In addition to the advancements in deepfake generation, corresponding detection technologies need to continuously evolve to regulate the potential misuse of deepfakes, such as for privacy invasion and phishing attacks. This survey comprehensively reviews the latest developments in deepfake generation and detection, summarizing and analyzing the current state of the art in this rapidly evolving field. We first unify task definitions, comprehensively introduce datasets and metrics, and discuss the development of generation and detection technology frameworks. Then, we discuss the development of several related sub-fields and focus on researching four mainstream deepfake fields: popular face swap, face reenactment, talking face generation, and facial attribute editing, as well as foreign detection. Subsequently, we comprehensively benchmark representative methods on popular datasets for each field, fully evaluating the latest and influential works published in top conferences/journals. Finally, we analyze the challenges and future research directions of the discussed fields. We closely follow the latest developments in <a href="https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection">https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection</a>. </p><p><a href="http://arxiv.org/abs/2403.17881v1">PDF</a> </p><p><strong>Summary</strong><br>深度伪造技术的发展与检测技术需要持续演进，以应对隐私侵犯和网络钓鱼等非法使用。</p><p><strong>Key Takeaways</strong></p><ul><li>统一任务定义，全面介绍数据集和评估指标。</li><li>探讨生成和检测技术框架的发展。</li><li>关注人脸替换、人脸重现、说话人脸生成、面部属性编辑等主流深度伪造领域。</li><li>全面基准测试每个领域流行数据集上的代表性方法。</li><li>分析所讨论领域的挑战和未来研究方向。</li><li>跟踪 Github 上深度伪造生成与检测的最新进展。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><p>1.题目：深度伪造生成与检测：基准与综述2.作者：甘沛，蒋宁章，孟涵胡，光涛翟，成杰王，振宇张，建杨，春华沈，大成陶3.第一作者单位：华东师范大学多模信息处理上海市重点实验室4.关键词：深度伪造生成，人脸替换，人脸重演，语音人脸生成，人脸属性编辑，外来检测，综述5.论文链接：https://arxiv.org/abs/2403.17881，Github代码链接：无6.总结：（1）：随着深度学习的进步，以变分自编码器 (VAE) 和生成对抗网络 (GAN) 为代表的技术在深度伪造生成领域取得了显著成果。近年来，具有强大图像生成能力的扩散模型的出现引发了该技术的新一轮研究和产业热潮。（2）：传统的深度伪造生成方法基于 GAN 模型，存在生成效果不佳的问题。扩散模型的出现极大地提升了图像/视频的生成能力，使得生成的深度伪造内容与真实内容难以区分，具有很高的实用价值。（3）：深度伪造生成主要分为人脸替换、人脸重演、语音人脸生成和人脸属性编辑四个主流研究领域。本文对这些领域的发展进行了综述，并对各个领域的代表性方法进行了基准测试和全面评估。（4）：本文分析了深度伪造生成和检测领域面临的挑战和未来研究方向，为该领域的进一步发展提供了参考。</p><p></p><ol><li><p>方法：(1) 本文对深度伪造生成与检测领域的研究现状进行了全面的总结和综述，分析了该领域面临的挑战和未来研究方向。(2) 本文对深度伪造生成领域的主流研究领域，包括人脸替换、人脸重演、语音人脸生成和人脸属性编辑，进行了基准测试和全面评估。(3) 本文对深度伪造检测领域的研究进展进行了总结，分析了外来检测和内在检测两种检测方法的优缺点，并对未来研究方向进行了展望。</p></li><li><p>结论：(1): 本综述全面回顾了深度伪造生成与检测领域的最新进展，首次全面覆盖了相关领域，并讨论了扩散模型等最新技术。具体而言，本文涵盖了基本背景知识的概述，包括研究任务的概念、数据收集与处理方法、模型设计与训练策略、评估指标和数据集。(2): 创新点：本文对深度伪造生成领域的四个主流研究领域进行了基准测试和全面评估，包括人脸替换、人脸重演、语音人脸生成和人脸属性编辑。本文还对深度伪造检测领域的研究进展进行了总结，分析了外来检测和内在检测两种检测方法的优缺点，并对未来研究方向进行了展望。性能：本文提出的基准测试和全面评估为深度伪造生成与检测领域的研究人员提供了有价值的参考。工作量：本文对深度伪造生成与检测领域的研究现状进行了全面的总结和综述，工作量较大。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-3fbcb20b0b6d83737be267b8b78dde71.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bac7dee6bad7c9614f746a35eef341ec.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4a0d28dab08c4d0254dd790d3d608013.jpg" align="middle"><img src="https://picx.zhimg.com/v2-409f1c30ffae605d9a497f77ff9ae5bb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-80df0902b8cc7d09c263750672e1ab59.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b4b73e97f1af3856b9dddf84237d9fcb.jpg" align="middle"></details><h2 id="Make-Your-Anchor-A-Diffusion-based-2D-Avatar-Generation-Framework"><a href="#Make-Your-Anchor-A-Diffusion-based-2D-Avatar-Generation-Framework" class="headerlink" title="Make-Your-Anchor: A Diffusion-based 2D Avatar Generation Framework"></a>Make-Your-Anchor: A Diffusion-based 2D Avatar Generation Framework</h2><p><strong>Authors:Ziyao Huang, Fan Tang, Yong Zhang, Xiaodong Cun, Juan Cao, Jintao Li, Tong-Yee Lee</strong></p><p>Despite the remarkable process of talking-head-based avatar-creating solutions, directly generating anchor-style videos with full-body motions remains challenging. In this study, we propose Make-Your-Anchor, a novel system necessitating only a one-minute video clip of an individual for training, subsequently enabling the automatic generation of anchor-style videos with precise torso and hand movements. Specifically, we finetune a proposed structure-guided diffusion model on input video to render 3D mesh conditions into human appearances. We adopt a two-stage training strategy for the diffusion model, effectively binding movements with specific appearances. To produce arbitrary long temporal video, we extend the 2D U-Net in the frame-wise diffusion model to a 3D style without additional training cost, and a simple yet effective batch-overlapped temporal denoising module is proposed to bypass the constraints on video length during inference. Finally, a novel identity-specific face enhancement module is introduced to improve the visual quality of facial regions in the output videos. Comparative experiments demonstrate the effectiveness and superiority of the system in terms of visual quality, temporal coherence, and identity preservation, outperforming SOTA diffusion/non-diffusion methods. Project page: \url{<a href="https://github.com/ICTMCG/Make-Your-Anchor}">https://github.com/ICTMCG/Make-Your-Anchor}</a>. </p><p><a href="http://arxiv.org/abs/2403.16510v1">PDF</a> accepted at CVPR2024</p><p><strong>Summary</strong><br>使用仅一分钟视频训练即可生成拥有躯干和手部动作的主播风格完整视频。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种通过一分钟视频训练来生成主播风格视频的系统——Make-Your-Anchor。</li><li>采用两阶段训练策略，将动作与特定外观有效地绑定。</li><li>扩展了帧级扩散模型中的二维 U-Net 到三维风格，以生成任意长度的时间视频。</li><li>提出了一种简单的批量重叠时间去噪模块，以绕过推理期间视频长度的限制。</li><li>引入了新颖的身份特定面部增强模块，以提高输出视频中面部区域的视觉质量。</li><li>与 SOTA 扩散/非扩散方法相比，该系统在视觉质量、时间连贯性和身份保留方面表现出有效性和优越性。</li><li>项目主页：\url{<a href="https://github.com/ICTMCG/Make-Your-Anchor}。">https://github.com/ICTMCG/Make-Your-Anchor}。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：Make-Your-Anchor：基于扩散的 2D 头像生成框架</li><li>作者：Ziyao Huang、Fan Tang、Yong Zhang、Xiaodong Cun、Juan Cao、Jintao Li、Tong-Yee Lee</li><li>第一作者单位：中国科学院计算技术研究所</li><li>关键词：视频生成、头像生成、扩散模型、运动捕捉</li><li>论文链接：https://arxiv.org/abs/2403.16510   Github 代码链接：https://github.com/ICTMCG/Make-Your-Anchor</li><li>摘要：   （1）研究背景：   当前的头像生成技术主要集中在头部生成，无法生成全身动作逼真的头像视频。   （2）过去方法及问题：   现有的基于 GAN 的方法只能生成局部区域，基于运动迁移的方法受限于运动捕捉数据的可用性。   （3）研究方法：   本文提出 Make-Your-Anchor 框架，通过微调基于结构引导的扩散模型，将 3D 网格条件渲染为逼真的全身动作头像视频。采用两阶段训练策略，有效地将运动与特定外观绑定。为了生成任意长度的视频，将帧级扩散模型中的 2D U-Net 扩展为 3D，并提出了一种简单的批次重叠时间去噪模块。此外，还引入了一个新的身份特定面部增强模块，以提高输出视频中面部区域的视觉质量。   （4）任务和性能：   Make-Your-Anchor 在视觉质量、时间连贯性和身份保留方面优于 SOTA 扩散/非扩散方法。它仅需一分钟的视频剪辑即可训练，生成全身动作逼真的头像视频，满足了自动生成头像视频的需求。</li></ol><p>7.方法：(1)结构引导扩散模型：将3D网格条件嵌入生成过程，学习姿态与目标视频帧之间的对应关系；(2)两阶段训练策略：预训练增强模型生成动作的能力，微调绑定动作与特定外观；(3)批次重叠时间去噪：采用全帧交叉注意力模块和重叠时间去噪算法，生成任意长度的时间一致视频；(4)身份特定面部增强模块：通过裁剪和融合操作，对生成的身体中的面部区域进行修改，提高视觉质量。</p><ol><li>结论：（1）：本文提出了 Make-Your-Anchor，一个基于扩散的 2D 头像生成框架，用于制作逼真且高质量的主播风格人物视频。该框架通过帧级运动到外观扩散训练了一个结构引导的扩散模型，并采用两阶段训练策略和绑定风格方法实现了特定外观与动作的绑定。为了生成时间一致的人物视频，我们提出了一个无训练策略，将图像扩散模型扩展为视频扩散模型，并设计了一个批次重叠时间去噪算法来克服生成视频长度的限制。从观察到面部细节在整体人物生成过程中难以重建这一现象出发，引入了身份特定的面部增强技术。通过将这四个系统方法相结合，我们的框架成功地生成了高质量、结构保持和时间连贯的主播风格人物视频，这可能为 2D 数字头像的广泛应用技术提供一些参考价值。（2）：创新点：提出了一种基于扩散的 2D 头像生成框架，可以生成逼真且高质量的主播风格人物视频；采用两阶段训练策略和绑定风格方法，将特定外观与动作绑定；提出了一个无训练策略，将图像扩散模型扩展为视频扩散模型，并设计了一个批次重叠时间去噪算法来克服生成视频长度的限制；引入了身份特定的面部增强技术，以提高生成视频中面部区域的视觉质量。性能：在视觉质量、时间连贯性和身份保留方面优于 SOTA 扩散/非扩散方法；仅需一分钟的视频剪辑即可训练，生成全身动作逼真的头像视频，满足了自动生成头像视频的需求。工作量：中等；需要收集和预处理训练数据；需要对模型进行训练和微调；需要对生成结果进行评估和优化。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-31e07e2070d0183b4685c1e857aaf0a2.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d20d65a2e0024c9d9bc380b7f7ba43b0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-771450d6d33a69293597f486c329a82a.jpg" align="middle"><img src="https://pica.zhimg.com/v2-4f2a829065c463be027e4b423c4e43c8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6ad4738b2a0c37e9688fb722556c4213.jpg" align="middle"><img src="https://pica.zhimg.com/v2-b6700b1c5fd0f20662f87b175ebf869f.jpg" align="middle"></details><h2 id="Adaptive-Super-Resolution-For-One-Shot-Talking-Head-Generation"><a href="#Adaptive-Super-Resolution-For-One-Shot-Talking-Head-Generation" class="headerlink" title="Adaptive Super Resolution For One-Shot Talking-Head Generation"></a>Adaptive Super Resolution For One-Shot Talking-Head Generation</h2><p><strong>Authors:Luchuan Song, Pinxin Liu, Guojun Yin, Chenliang Xu</strong></p><p>The one-shot talking-head generation learns to synthesize a talking-head video with one source portrait image under the driving of same or different identity video. Usually these methods require plane-based pixel transformations via Jacobin matrices or facial image warps for novel poses generation. The constraints of using a single image source and pixel displacements often compromise the clarity of the synthesized images. Some methods try to improve the quality of synthesized videos by introducing additional super-resolution modules, but this will undoubtedly increase computational consumption and destroy the original data distribution. In this work, we propose an adaptive high-quality talking-head video generation method, which synthesizes high-resolution video without additional pre-trained modules. Specifically, inspired by existing super-resolution methods, we down-sample the one-shot source image, and then adaptively reconstruct high-frequency details via an encoder-decoder module, resulting in enhanced video clarity. Our method consistently improves the quality of generated videos through a straightforward yet effective strategy, substantiated by quantitative and qualitative evaluations. The code and demo video are available on: \url{<a href="https://github.com/Songluchuan/AdaSR-TalkingHead/}">https://github.com/Songluchuan/AdaSR-TalkingHead/}</a>. </p><p><a href="http://arxiv.org/abs/2403.15944v1">PDF</a> 5 pages, 3 figures</p><p><strong>Summary</strong><br>一键式生成高清晰度视频，无需添加预训练模块，通过自适应重建高频细节提升视频清晰度。</p><p><strong>Key Takeaways</strong></p><ul><li>一键式生成人像视频，驱动视频与人像同一或不同。</li><li>传统方法受限于单图像源和像素位移，清晰度受损。</li><li>现有方法通过超分辨率模块提升质量，但增加计算量并破坏原始数据分布。</li><li>本文提出自适应高品质人像视频生成方法，无需额外预训练模块合成高分辨率视频。</li><li>受超分辨率方法启发，对单图像源下采样，再通过编码器-解码器模块自适应重建高频细节。</li><li>该策略简单有效地提升了生成视频的质量，并通过定量和定性评估得到证实。</li><li>代码和演示视频可在 Github 上获取。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：自适应超分辨率单镜头说话人头部生成</li><li>作者：Luchuan Song, Pinxin Liu, Guojun Yin, Chenliang Xu</li><li>第一作者单位：罗切斯特大学</li><li>关键词：超分辨率视频，单镜头说话人头部生成</li><li>链接：https://arxiv.org/abs/2403.15944，Github：None</li><li><p>摘要：(1)：研究背景：单镜头说话人头部生成旨在使用一张源人像图像在相同或不同身份视频的驱动下合成说话人头部视频。现有方法通常需要基于平面的像素变换，这会影响合成图像的清晰度。一些方法通过引入额外的超分辨率模块来提高合成视频的质量，但这会增加计算消耗并破坏原始数据分布。(2)：过去方法及问题：MetaPortrait、SadTalker 和 VideoReTalking 等方法尝试通过重新训练一个独立的超分辨率模块来改善视频质量。然而，这种两阶段合成过程会导致不必要的计算开销和错误累积。(3)：研究方法：本文提出了一种自适应超分辨率方法，用于说话人头部生成框架。受 ESRGAN 和 Real-ESRGAN 等超分辨率方法的启发，该方法通过压缩和下采样高质量图像来构建用于成对训练的低质量图像数据。它通过独特设计的编码器-解码器结构从低质量图像中自适应地捕获高频信息以进行重建。(4)：方法性能：该方法在定量和定性实验中验证了其有效性，并与现有的单镜头说话人头部生成方法进行了对比。结果表明，该方法始终通过一种简单有效的策略提高了生成视频的质量。</p></li><li><p>方法：(1) 受 ESRGAN 和 Real-ESRGAN 等超分辨率方法启发，通过压缩和下采样高质量图像，构建用于成对训练的低质量图像数据；(2) 通过独特设计的编码器-解码器结构，从低质量图像中自适应地捕获高频信息以进行重建。</p></li><li><p>总结：（1）本工作的重要意义：本文提出了一种自适应超分辨率方法，用于单镜头说话人头部视频生成领域。通过设计简单但有效的方法，我们的方法能够从低质量图像中捕获高频细节。这使得无需额外的预训练模块或后处理即可合成高质量视频。在大型数据集上进行的广泛定量和定性评估证实，我们的方法在高质量可驱动人脸视频生成方面超越了现有技术。（2）创新点：受 ESRGAN 和 Real-ESRGAN 等超分辨率方法的启发，通过压缩和下采样高质量图像，构建用于成对训练的低质量图像数据。通过独特设计的编码器-解码器结构，从低质量图像中自适应地捕获高频信息以进行重建。性能：该方法在定量和定性实验中验证了其有效性，并与现有的单镜头说话人头部生成方法进行了对比。结果表明，该方法始终通过一种简单有效的策略提高了生成视频的质量。工作量：该方法的实现相对简单，并且不需要额外的预训练模块或后处理步骤。这使得该方法在计算和时间方面都具有成本效益。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-fbfc28956b0106142272e9ccedb9ced5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-188e4004db88e63a7e920e9ac2f3636d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4b69fbe4c0930a57ff002ead5463e3ef.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e3a0dd3488e1d1a03f494038c2fcb247.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-03-28  Deepfake Generation and Detection A Benchmark and Survey</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/03/28/Paper/2024-03-28/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/03/28/Paper/2024-03-28/Diffusion%20Models/</id>
    <published>2024-03-28T02:56:57.000Z</published>
    <updated>2024-03-28T02:56:57.223Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-03-28-更新"><a href="#2024-03-28-更新" class="headerlink" title="2024-03-28 更新"></a>2024-03-28 更新</h1><h2 id="AID-Attention-Interpolation-of-Text-to-Image-Diffusion"><a href="#AID-Attention-Interpolation-of-Text-to-Image-Diffusion" class="headerlink" title="AID: Attention Interpolation of Text-to-Image Diffusion"></a>AID: Attention Interpolation of Text-to-Image Diffusion</h2><p><strong>Authors:Qiyuan He, Jinghao Wang, Ziwei Liu, Angela Yao</strong></p><p>Conditional diffusion models can create unseen images in various settings, aiding image interpolation. Interpolation in latent spaces is well-studied, but interpolation with specific conditions like text or poses is less understood. Simple approaches, such as linear interpolation in the space of conditions, often result in images that lack consistency, smoothness, and fidelity. To that end, we introduce a novel training-free technique named Attention Interpolation via Diffusion (AID). Our key contributions include 1) proposing an inner/outer interpolated attention layer; 2) fusing the interpolated attention with self-attention to boost fidelity; and 3) applying beta distribution to selection to increase smoothness. We also present a variant, Prompt-guided Attention Interpolation via Diffusion (PAID), that considers interpolation as a condition-dependent generative process. This method enables the creation of new images with greater consistency, smoothness, and efficiency, and offers control over the exact path of interpolation. Our approach demonstrates effectiveness for conceptual and spatial interpolation. Code and demo are available at <a href="https://github.com/QY-H00/attention-interpolation-diffusion">https://github.com/QY-H00/attention-interpolation-diffusion</a>. </p><p><a href="http://arxiv.org/abs/2403.17924v1">PDF</a> </p><p><strong>摘要</strong><br>注意力插值扩散（AID）：一种无需训练的条件插值新技术，可生成高度一致、平滑且逼真的图像。</p><p><strong>要点</strong></p><ul><li>提出内层/外层插值注意力层，以增强插值质量。</li><li>融合插值注意力和自注意力，提升生成图像的保真度。</li><li>应用贝塔分布选择，提高插值的平滑度。</li><li>提出提示引导的注意力插值扩散（PAID）变体，将插值视为条件依赖的生成过程。</li><li>控制插值的确切路径，生成具有更高一致性、平滑性和效率的新图像。</li><li>在概念和空间插值方面表现出有效性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：AID：文本到图像的注意插值</li><li>作者：齐源何、景浩王、子为刘、安吉拉姚</li><li>第一作者单位：新加坡国立大学</li><li>关键词：文本到图像扩散、条件扩散模型、注意机制、插值</li><li>论文链接：https://arxiv.org/abs/2403.17924    Github 代码链接：无</li><li><p>摘要：(1) 研究背景：条件扩散模型可以生成各种场景中的图像，有助于图像插值。在潜在空间中进行插值已经得到充分研究，但使用特定条件（如文本或姿势）进行插值的研究较少。(2) 过去的方法及其问题：简单的方法，例如在条件空间中进行线性插值，通常会导致图像缺乏一致性、平滑性和保真度。本文提出的方法动机明确。(3) 研究方法：本文提出了一种名为 AID 的新颖免训练技术，即通过扩散进行注意插值。该方法通过在条件空间中引入注意机制来指导插值过程，从而确保图像在布局和概念上的平滑过渡。(4) 方法性能：该方法在文本到图像扩散模型上进行了评估，在空间和概念插值任务上取得了显着改进。实验结果支持了本文提出的方法目标。</p></li><li><p>方法：(1) 内/外插值注意力机制：通过在条件空间中引入注意力机制，指导插值过程，确保图像在布局和概念上的平滑过渡。(2) 与自注意力融合：将插值潜变量本身的键和值融入插值注意力机制，提高一致性和保真度。(3) Beta 先验序列选择：采用 Beta 分布选择插值路径上的特定插值图像，使生成的图像序列更平滑。(4) 提示引导：通过注入提示作为条件，控制插值路径，生成符合文本描述的插值序列。</p></li><li><p>结论：（1）：本工作首次提出条件插值任务及相关评估指标，包括一致性、平滑性和保真度。我们提出了一种称为 AID 的新颖方法，用于在扩散模型中生成条件插值图像。该方法在无需训练的情况下显着超越了基准，通过定性和定量分析得到了证明。此外，我们还引入了 PAID，该扩展允许用户使用引导提示来选择插值路径。我们的方法无需训练，拓宽了生成模型插值的范围，为合成生成、图像编辑、数据增强和视频插值等各种应用开辟了新机遇。（2）：创新点：提出条件插值任务及评估指标，引入注意力机制指导插值过程，无需训练即可生成高质量插值图像。性能：在空间和概念插值任务上取得显着改进，定性和定量评估均支持该方法的有效性。工作量：提出了一种无需训练的插值方法，减少了训练负担，提高了插值效率。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-aaa47516c2e21df63c1ee81eb0afd555.jpg" align="middle"></details><h2 id="AniPortrait-Audio-Driven-Synthesis-of-Photorealistic-Portrait-Animation"><a href="#AniPortrait-Audio-Driven-Synthesis-of-Photorealistic-Portrait-Animation" class="headerlink" title="AniPortrait: Audio-Driven Synthesis of Photorealistic Portrait Animation"></a>AniPortrait: Audio-Driven Synthesis of Photorealistic Portrait Animation</h2><p><strong>Authors:Huawei Wei, Zejun Yang, Zhisheng Wang</strong></p><p>In this study, we propose AniPortrait, a novel framework for generating high-quality animation driven by audio and a reference portrait image. Our methodology is divided into two stages. Initially, we extract 3D intermediate representations from audio and project them into a sequence of 2D facial landmarks. Subsequently, we employ a robust diffusion model, coupled with a motion module, to convert the landmark sequence into photorealistic and temporally consistent portrait animation. Experimental results demonstrate the superiority of AniPortrait in terms of facial naturalness, pose diversity, and visual quality, thereby offering an enhanced perceptual experience. Moreover, our methodology exhibits considerable potential in terms of flexibility and controllability, which can be effectively applied in areas such as facial motion editing or face reenactment. We release code and model weights at <a href="https://github.com/scutzzj/AniPortrait">https://github.com/scutzzj/AniPortrait</a> </p><p><a href="http://arxiv.org/abs/2403.17694v1">PDF</a> </p><p><strong>Summary</strong><br>利用音频和参考肖像图像生成高品质动画的新颖框架：AniPortrait</p><p><strong>Key Takeaways</strong></p><ul><li>AniPortrait 提出了一种由音频和参考肖像图像驱动的高质量动画生成新框架。</li><li>AniPortrait 分为两个阶段：从音频中提取 3D 中间表示并将其投影到 2D 面部地标序列中。</li><li>AniPortrait 使用稳健的扩散模型和运动模块将地标序列转换为逼真的、时间一致的肖像动画。</li><li>AniPortrait 在面部自然度、姿势多样性和视觉质量方面表现出卓越的性能。</li><li>AniPortrait 在灵活性和可控性方面表现出巨大潜力，可有效应用于面部动作编辑或面部重建等领域。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：AniPortrait：音频驱动的写实肖像动画合成</li><li>作者：Wei Huawei<em>、Yang Zejun</em>、Wang Zhisheng</li><li>单位：腾讯</li><li>关键词：音频驱动、肖像动画、扩散模型、动作模块</li><li>论文链接：https://arxiv.org/abs/2403.17694   Github：None</li><li>摘要：   （1）研究背景：从音频和静态图像生成逼真且富有表现力的肖像动画具有广泛的应用，但制作视觉上引人入胜且保持时间一致性的高质量动画是一项重大挑战。   （2）过去方法及其问题：现有方法通常无法克服这一挑战，主要原因是它们依赖于容量有限的视觉内容生成器，例如 GAN、NeRF 或基于运动的解码器。这些网络表现出有限的泛化能力，并且在生成高质量内容时往往缺乏稳定性。   （3）提出的研究方法：本文提出 AniPortrait，这是一个新颖的框架，旨在生成由音频和参考图像驱动的优质动画肖像。AniPortrait 分为两个不同的阶段。在第一阶段，我们使用基于 Transformer 的模型从音频中提取 3D 中间表示，并将其投影到 2D 面部地标序列中。随后，我们采用稳健的扩散模型，结合运动模块，将地标序列转换为逼真的、时间一致的肖像动画。   （4）方法在任务和性能上的表现：实验结果证明了 AniPortrait 在面部自然度、姿势多样性和视觉质量方面的优越性，从而提供了增强的感知体验。此外，我们的方法在灵活性和可控性方面表现出相当大的潜力，可以有效地应用于面部动作编辑或面部重现等领域。</li></ol><p>7.方法：（1）：<strong>Audio2Lmk</strong>：从音频中提取 3D 面部网格序列和位姿序列。（2）：<strong>Lmk2Video</strong>：将面部地标序列转换为时间一致的肖像动画。</p><ol><li>结论：(1): 本工作提出了一种新颖的框架 AniPortrait，该框架可以生成由音频和参考图像驱动的优质动画肖像。AniPortrait 采用基于 Transformer 的模型从音频中提取 3D 中间表示，并使用稳健的扩散模型结合运动模块将其转换为逼真的、时间一致的肖像动画。实验结果证明了 AniPortrait 在面部自然度、姿势多样性和视觉质量方面的优越性，从而提供了增强的感知体验。此外，我们的方法在灵活性和可控性方面表现出相当大的潜力，可以有效地应用于面部动作编辑或面部重现等领域。(2): 创新点：</li><li>提出了一种新颖的框架 AniPortrait，该框架可以从音频和参考图像生成逼真的动画肖像。</li><li>采用基于 Transformer 的模型从音频中提取 3D 中间表示，并使用稳健的扩散模型结合运动模块将其转换为时间一致的肖像动画。</li><li>AniPortrait 在面部自然度、姿势多样性和视觉质量方面表现出优越性，从而提供了增强的感知体验。</li><li>AniPortrait 在灵活性和可控性方面表现出相当大的潜力，可以有效地应用于面部动作编辑或面部重现等领域。性能：</li><li>AniPortrait 在面部自然度、姿势多样性和视觉质量方面表现出优越性，从而提供了增强的感知体验。</li><li>AniPortrait 在灵活性和可控性方面表现出相当大的潜力，可以有效地应用于面部动作编辑或面部重现等领域。工作量：</li><li>AniPortrait 的实现需要一定的技术实力，包括对 Transformer 模型、扩散模型和运动模块的理解。</li><li>训练 AniPortrait 模型需要大量的数据和计算资源。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-a0703eb6ac9807d377c7bbfaa84e3681.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-fc2d139237100aad689f67180ae398bf.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e35074ee634942aebc5c8860cf29e344.jpg" align="middle"></details><h2 id="DiffFAE-Advancing-High-fidelity-One-shot-Facial-Appearance-Editing-with-Space-sensitive-Customization-and-Semantic-Preservation"><a href="#DiffFAE-Advancing-High-fidelity-One-shot-Facial-Appearance-Editing-with-Space-sensitive-Customization-and-Semantic-Preservation" class="headerlink" title="DiffFAE: Advancing High-fidelity One-shot Facial Appearance Editing with   Space-sensitive Customization and Semantic Preservation"></a>DiffFAE: Advancing High-fidelity One-shot Facial Appearance Editing with   Space-sensitive Customization and Semantic Preservation</h2><p><strong>Authors:Qilin Wang, Jiangning Zhang, Chengming Xu, Weijian Cao, Ying Tai, Yue Han, Yanhao Ge, Hong Gu, Chengjie Wang, Yanwei Fu</strong></p><p>Facial Appearance Editing (FAE) aims to modify physical attributes, such as pose, expression and lighting, of human facial images while preserving attributes like identity and background, showing great importance in photograph. In spite of the great progress in this area, current researches generally meet three challenges: low generation fidelity, poor attribute preservation, and inefficient inference. To overcome above challenges, this paper presents DiffFAE, a one-stage and highly-efficient diffusion-based framework tailored for high-fidelity FAE. For high-fidelity query attributes transfer, we adopt Space-sensitive Physical Customization (SPC), which ensures the fidelity and generalization ability by utilizing rendering texture derived from 3D Morphable Model (3DMM). In order to preserve source attributes, we introduce the Region-responsive Semantic Composition (RSC). This module is guided to learn decoupled source-regarding features, thereby better preserving the identity and alleviating artifacts from non-facial attributes such as hair, clothes, and background. We further introduce a consistency regularization for our pipeline to enhance editing controllability by leveraging prior knowledge in the attention matrices of diffusion model. Extensive experiments demonstrate the superiority of DiffFAE over existing methods, achieving state-of-the-art performance in facial appearance editing. </p><p><a href="http://arxiv.org/abs/2403.17664v1">PDF</a> </p><p><strong>Summary</strong><br>图像中人脸外观编辑的扩散模型 DiffFAE 提高了生成保真度、属性保留和推理效率。</p><p><strong>Key Takeaways</strong></p><ul><li>采用空间敏感物理定制 (SPC) 确保查询属性转移的保真度和泛化能力。</li><li>引入区域响应语义组合 (RSC) 保留源属性，减轻非面部属性（如头发、衣服和背景）带来的伪影。</li><li>提出一致性正则化，通过利用扩散模型注意力矩阵中的先验知识增强编辑可控性。</li><li>DiffFAE 在人脸外观编辑中实现了最先进的性能，超越现有方法。</li><li>DiffFAE 可以有效处理人脸外观编辑中的低生成保真度、差属性保留和低推理效率等挑战。</li><li>扩散模型在人脸外观编辑任务中具有广阔的应用前景。</li><li>本文提出了一种用于人脸外观编辑的新颖框架 DiffFAE，它结合了扩散模型、空间敏感物理定制和区域响应语义组合的优点。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：DiffFAE：推进高保真一发式人脸外观编辑</li><li>作者：Q. Wang 等</li><li>单位：未提及</li><li>关键词：Facial appearance editing、Diffusion model、Object-centric learning</li><li>论文链接：未提供，Github 代码链接：无</li><li><p>摘要：（1）研究背景：人脸外观编辑旨在修改人脸图像的物理属性（如姿势、表情和光照），同时保留身份和背景等属性，在摄影中具有重要意义。（2）过去方法：现有研究通常面临生成保真度低、属性保留差和推理效率低三大挑战。（3）研究方法：本文提出 DiffFAE，一个针对高保真 FAE 量身定制的单阶段且高效的基于扩散的框架。为了实现高保真查询属性转移，我们采用空间敏感物理定制（SPC），它利用源自 3D 可变形模型（3DMM）的渲染纹理，确保了保真度和泛化能力。为了保留源属性，我们引入了区域响应语义组合（RSC）。该模块被引导学习解耦的源相关特征，从而更好地保留身份，并减轻来自非面部属性（如头发、衣服和背景）的伪影。我们还为我们的管道引入了稠密正则化，通过利用扩散模型注意力矩阵中的先验知识来增强编辑可控性。（4）方法性能：广泛的实验表明，DiffFAE 优于现有方法，在人脸外观编辑中实现了最先进的性能。这些性能支持了他们的目标。</p></li><li><p><strong>方法</strong>：（1）<strong>空间敏感物理定制（SPC）</strong>：利用源自3D可变形模型（3DMM）的渲染纹理，确保保真度和泛化能力。（2）<strong>区域响应语义组合（RSC）</strong>：学习解耦的源相关特征，保留身份，减轻非面部属性伪影。（3）<strong>稠密正则化</strong>：利用扩散模型注意力矩阵中的先验知识，增强编辑可控性。</p></li></ol><p>8.结论：(1)：本文针对人脸外观编辑（FAE）中存在的生成保真度低、属性保留差和推理效率低三大挑战进行了分析，探索了一种基于单阶段扩散的新框架。具体来说，我们采用空间敏感物理定制模块来处理查询物理属性，如姿势、表情和光照。同时，提出了区域响应语义组合来更好地控制源相关属性。我们的方法在 VoxCeleb1 数据集上为 FAE 任务设定了新的最先进性能，这得到了广泛的定量和定性结果的支持。(2)：创新点：空间敏感物理定制模块、区域响应语义组合、稠密正则化；性能：在 VoxCeleb1 数据集上取得了最先进的性能；工作量：中等。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-d26cb9d6e12fa2c3ca2894c45c11f62a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2e175e9d0b22d21814f9b545e1b4a47f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-98414ddcc0bdcee2447d896743b3ec8e.jpg" align="middle"></details>## DiffGaze: A Diffusion Model for Continuous Gaze Sequence Generation on   360° Images**Authors:Chuhan Jiao, Yao Wang, Guanhua Zhang, Mihai Bâce, Zhiming Hu, Andreas Bulling**We present DiffGaze, a novel method for generating realistic and diverse continuous human gaze sequences on 360{\deg} images based on a conditional score-based denoising diffusion model. Generating human gaze on 360{\deg} images is important for various human-computer interaction and computer graphics applications, e.g. for creating large-scale eye tracking datasets or for realistic animation of virtual humans. However, existing methods are limited to predicting discrete fixation sequences or aggregated saliency maps, thereby neglecting crucial parts of natural gaze behaviour. Our method uses features extracted from 360{\deg} images as condition and uses two transformers to model the temporal and spatial dependencies of continuous human gaze. We evaluate DiffGaze on two 360{\deg} image benchmarks for gaze sequence generation as well as scanpath prediction and saliency prediction. Our evaluations show that DiffGaze outperforms state-of-the-art methods on all tasks on both benchmarks. We also report a 21-participant user study showing that our method generates gaze sequences that are indistinguishable from real human sequences. [PDF](http://arxiv.org/abs/2403.17477v1) **摘要**基于条件分数去噪扩散模型，提出了一种生成360度图像上逼真且多样的连续人眼注视序列的新方法DiffGaze。**要点**- 提出了 DiffGaze，一种用于生成逼真且多样的 360 度图像的连续人眼注视序列的方法。- DiffGaze 使用从 360 度图像中提取的特征作为条件，并使用两个 Transformer 来建模连续人眼注视的时间和空间依赖性。- DiffGaze 在两个用于注视序列生成、扫描路径预测和显着性预测的 360 度图像基准上进行了评估。- 在两个基准上的所有任务中，DiffGaze 都优于最先进的方法。- 一项包含 21 名参与者的用户研究表明，该方法生成的眼注视序列与真实的人眼注视序列无法区分。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：DiffGaze：360° 图像连续注视序列生成扩散模型</li><li>作者：Chuhan Jiao、Yao Wang、Guanhua Zhang、Mihai Bace、Zhiming Hu、Andreas Bulling</li><li>隶属单位：斯图加特大学可视化与交互系统研究所</li><li>关键词：Scanpath Prediction; Saliency Modelling; Eye Tracking; Gaze Behaviour Modelling; Eye Movement Synthesis</li><li>论文链接：https://arxiv.org/abs/2403.17477   Github 代码链接：无</li><li>摘要：（1）研究背景：随着相机技术的进步，高分辨率 360° 图像的捕捉为虚拟现实 (VR) 中的新一代沉浸式体验提供了可能。这引发了消费者采用这项新技术的兴趣，并促进了理解人类如何感知和探索这些 3D 虚拟环境的研究工作。视觉注意力是探索过程中的一个特别丰富的的信息来源，通常以使用眼动追踪收集的注视数据形式进行分析。尽管眼动追踪变得更加广泛和经济实惠，而且被集成到越来越多的 VR 头显中，但收集注视数据（尤其是在大规模的情况下）仍然很繁琐且耗时，而且通常根本不可行。这引发了对视觉注意力计算模型的研究，即无需专用眼动追踪设备就能预测 360° 图像上人类注视的模型。</li></ol><p>（2）过去的方法及其问题：先前关于 360° 图像上视觉注意力计算建模的工作主要集中在显着性或扫描路径预测上。尽管取得了重大进展，但这两项任务仍然只解决了简化的问题：虽然聚合显着性图不需要对人类注视行为的时间特性进行建模，但预测离散注视固定（扫描路径）的序列在时间上仍然粗糙，并且忽略了固定之间的丰富注视数据。因此，这些任务（或过去为解决这些任务而开发的任何现有方法）都不能忠实地对 360° 图像上自然人类注视行为的丰富空间和时间特性进行建模。</p><p>（3）提出的研究方法：为了解决这些限制，我们提出了 DiffGaze——第一个生成 360° 图像上连续人类注视序列的方法。DiffGaze 基于条件分数噪声扩散模型，该模型以从 360° 图像中提取的特征为条件，并使用两个 Transformer 来对时空人类注视行为进行建模。</p><p>（4）方法在任务和性能上的表现：我们在两个数据集（Sitzmann 和 Salient360!）上对我们的方法进行了连续注视序列生成、扫描路径预测和显着性预测的评估。结果表明，在两个基准上的所有任务中，DiffGaze 都优于最先进的方法。这些性能可以支持其目标。</p><ol><li><p>方法：(1) DiffGaze基于条件分数噪声扩散模型，以从360°图像中提取的特征为条件。(2) 使用两个Transformer对时空人类注视行为进行建模。(3) 通过逐层噪声添加和预测噪声的逆过程，生成连续的人类注视序列。</p></li><li><p>结论：（1）：本文提出了 DiffGaze，这是一种条件扩散模型，用于在 360° 环境中生成逼真且多样的连续人类注视序列。该方法通过超越扫描路径预测来对更复杂的眼球运动进行建模，从而显著推进了该领域。通过在两个 360° 图像数据集上对三种不同任务进行严格评估，证明了 DiffGaze 的有效性。DiffGaze 不仅在注视序列生成、扫描路径预测和显着性预测方面优于以往的方法，而且还显示出与人类基线相当的性能，突出了其模拟类人注视行为的能力。这些结果突出了 DiffGaze 在促进沉浸式环境中注视行为分析方面的潜力。通过提供高质量的模拟眼动追踪数据，DiffGaze 为人机交互和计算机视觉应用开辟了新的可能性，为更直观和沉浸式的用户体验铺平了道路。（2）：创新点：</p></li><li>首次提出了一种生成 360° 图像上连续人类注视序列的条件扩散模型。</li><li>使用两个 Transformer 对时空人类注视行为进行建模，这比以往的方法更全面。</li><li>通过逐层噪声添加和预测噪声的逆过程，生成连续的人类注视序列，比以往的方法更逼真。性能：</li><li>在两个基准数据集上，DiffGaze 在注视序列生成、扫描路径预测和显着性预测方面均优于最先进的方法。</li><li>DiffGaze 与人类基线表现相当，表明其能够模拟类人注视行为。工作量：</li><li>DiffGaze 的训练和推理过程比以往的方法更复杂，需要更多的计算资源。</li><li>DiffGaze 需要从 360° 图像中提取特征，这可能需要额外的处理时间。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-e0bef8622d6189293fc39affd7e61d42.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-da152edfe80db438956e4ae04e20b5df.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5273e50a2192cece0fc3295a667277b9.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3150ad0da3bf6c45b8ab514fbb2057bd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9e382e110e92dc607e913f5141ad3dc8.jpg" align="middle"></details><h2 id="LaRE-2-Latent-Reconstruction-Error-Based-Method-for-Diffusion-Generated-Image-Detection"><a href="#LaRE-2-Latent-Reconstruction-Error-Based-Method-for-Diffusion-Generated-Image-Detection" class="headerlink" title="LaRE^2: Latent Reconstruction Error Based Method for Diffusion-Generated   Image Detection"></a>LaRE^2: Latent Reconstruction Error Based Method for Diffusion-Generated   Image Detection</h2><p><strong>Authors:Yunpeng Luo, Junlong Du, Ke Yan, Shouhong Ding</strong></p><p>The evolution of Diffusion Models has dramatically improved image generation quality, making it increasingly difficult to differentiate between real and generated images. This development, while impressive, also raises significant privacy and security concerns. In response to this, we propose a novel Latent REconstruction error guided feature REfinement method (LaRE^2) for detecting the diffusion-generated images. We come up with the Latent Reconstruction Error (LaRE), the first reconstruction-error based feature in the latent space for generated image detection. LaRE surpasses existing methods in terms of feature extraction efficiency while preserving crucial cues required to differentiate between the real and the fake. To exploit LaRE, we propose an Error-Guided feature REfinement module (EGRE), which can refine the image feature guided by LaRE to enhance the discriminativeness of the feature. Our EGRE utilizes an align-then-refine mechanism, which effectively refines the image feature for generated-image detection from both spatial and channel perspectives. Extensive experiments on the large-scale GenImage benchmark demonstrate the superiority of our LaRE^2, which surpasses the best SoTA method by up to 11.9%/12.1% average ACC/AP across 8 different image generators. LaRE also surpasses existing methods in terms of feature extraction cost, delivering an impressive speed enhancement of 8 times. </p><p><a href="http://arxiv.org/abs/2403.17465v1">PDF</a> CVPR 2024</p><p><strong>Summary</strong><br>扩散模型生成的图像难辨真伪，为此提出 LaRE^2 方法，利用潜在重建误差增强鉴别能力。</p><p><strong>Key Takeaways</strong></p><ul><li>创新提出潜在重建误差 (LaRE)，在潜在空间中提取用于生成图像检测的重建误差特征。</li><li>设计错误引导特征细化模块 (EGRE)，利用 LaRE 引导图像特征细化，提高特征判别力。</li><li>EGRE 采用对齐再细化的机制，从空间和通道两个角度有效细化图像特征。</li><li>在大规模 GenImage 基准上进行广泛实验，证明 LaRE^2 的优越性，在 8 种不同的图像生成器上比最佳 SoTA 方法分别提高了 11.9%/12.1% 的平均准确率/平均精度。</li><li>LaRE 还超过了现有方法的特征提取成本，提供了 8 倍的提速。</li><li>LaRE^2 方法有助于保护隐私和安全，解决扩散模型带来的挑战。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：LaRE2：基于潜在重建误差的扩散生成图像检测方法</li><li>作者：罗运鹏、杜俊龙、严柯、丁寿鸿</li><li>单位：腾讯优图实验室</li><li>关键词：Diffusion Model、图像生成、图像检测、潜在空间、重建误差</li><li>论文链接：https://arxiv.org/abs/2403.17465</li><li><p>摘要：（1）研究背景：扩散模型的快速发展带来了生成图像质量的显著提升，但也引发了隐私和安全问题，亟需开发图像检测技术。（2）过去方法：现有方法利用重建误差作为判别特征，但存在特征提取效率低、重建步骤繁琐等问题。（3）研究方法：本文提出 LaRE2 方法，利用潜在空间的重建误差作为特征，并设计了错误引导特征细化模块，从空间和通道维度细化图像特征，增强判别性。（4）性能与评价：在 GenImage 数据集上，LaRE2 在 8 个不同图像生成器上平均 ACC/AP 分别比最佳 SoTA 方法提升了 11.9%/12.1%，且特征提取速度提升了 8 倍，证明了方法的有效性和高效性。</p></li><li><p>方法：(1) 在潜在空间中，通过单步重建提取 LaRE；(2) 为了利用 LaRE，提出了错误引导特征细化模块，该模块由错误引导空间细化模块和错误引导通道细化模块组成。从空间和通道维度，利用 LaRE 增强图像特征的判别性，用于生成图像检测。</p></li><li><p>结论：(1): 本文提出了一种新颖的基于重建的扩散生成图像检测方法 LaRE2。我们提出了 LaRE，这是一种通过在潜在空间中重建图像来获得的新颖且更有效的基于重建的特征。值得注意的是，与现有的基于重建的方法相比，LaRE 的速度提高了 8 倍。通过将 LaRE 与错误引导特征细化模块 (EGRE) 相结合。我们的 LaRE2 在扩散生成图像检测方面取得了卓越的性能，展示了最先进的性能。(2): 创新点：提出了一种新颖且高效的基于重建的特征 LaRE，利用潜在空间重建图像获得；设计了错误引导特征细化模块，从空间和通道维度增强图像特征的判别性。性能：在 GenImage 数据集上，在 8 个不同的图像生成器上，与最佳 SoTA 方法相比，LaRE2 的平均 ACC/AP 分别提高了 11.9%/12.1%，特征提取速度提高了 8 倍。工作量：特征提取速度提升了 8 倍，降低了工作量。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-f6c31fca452aadf6cc21d298eaf9fa3d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-df3903ec74f7dfdd651966c35bf93157.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e18a59cb1204894da80ac9d756b420c6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7e8b388fdf7ef71288f5c4468e2d6aa6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bc9ec7aceb66ab733396c11e86306150.jpg" align="middle"></details><h2 id="InterHandGen-Two-Hand-Interaction-Generation-via-Cascaded-Reverse-Diffusion"><a href="#InterHandGen-Two-Hand-Interaction-Generation-via-Cascaded-Reverse-Diffusion" class="headerlink" title="InterHandGen: Two-Hand Interaction Generation via Cascaded Reverse   Diffusion"></a>InterHandGen: Two-Hand Interaction Generation via Cascaded Reverse   Diffusion</h2><p><strong>Authors:Jihyun Lee, Shunsuke Saito, Giljoo Nam, Minhyuk Sung, Tae-Kyun Kim</strong></p><p>We present InterHandGen, a novel framework that learns the generative prior of two-hand interaction. Sampling from our model yields plausible and diverse two-hand shapes in close interaction with or without an object. Our prior can be incorporated into any optimization or learning methods to reduce ambiguity in an ill-posed setup. Our key observation is that directly modeling the joint distribution of multiple instances imposes high learning complexity due to its combinatorial nature. Thus, we propose to decompose the modeling of joint distribution into the modeling of factored unconditional and conditional single instance distribution. In particular, we introduce a diffusion model that learns the single-hand distribution unconditional and conditional to another hand via conditioning dropout. For sampling, we combine anti-penetration and classifier-free guidance to enable plausible generation. Furthermore, we establish the rigorous evaluation protocol of two-hand synthesis, where our method significantly outperforms baseline generative models in terms of plausibility and diversity. We also demonstrate that our diffusion prior can boost the performance of two-hand reconstruction from monocular in-the-wild images, achieving new state-of-the-art accuracy. </p><p><a href="http://arxiv.org/abs/2403.17422v1">PDF</a> Accepted to CVPR 2024, project page:   <a href="https://jyunlee.github.io/projects/interhandgen/">https://jyunlee.github.io/projects/interhandgen/</a></p><p><strong>Summary</strong><br>两手交互生成模型，分解为单个手无条件和条件分布，采用反穿透和无分类器引导，用于逼真多元生成，在单目重建任务中表现出众。</p><p><strong>Key Takeaways</strong></p><ul><li>提出 InterHandGen 模型，学习双手交互的生成先验。</li><li>分解联合分布建模为无条件和条件单个实例分布。</li><li>引入扩散模型学习单个手的无条件分布和条件分布。</li><li>采用抗穿透和无分类器引导进行采样。</li><li>建立双手合成评估协议，InterHandGen 显著优于基线生成模型。</li><li>扩散先验可提升单目重建任务中的双手重建性能。</li><li>InterHandGen 在单目重建任务中达到新的最先进准确度。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：InterHandGen：基于级联逆扩散的双手交互生成</li><li>作者：Jue Wang, Taku Komura, Gül Varol, Justus Thies, Matthias Niessner</li><li>所属机构：英特尔实验室</li><li>关键词：双手交互、生成模型、扩散模型、条件生成</li><li>论文链接：https://arxiv.org/abs/2210.14113</li><li>摘要：（1）研究背景：双手交互是人类智能的重要组成部分，但由于其高维性和复杂性，生成逼真的双手交互数据一直是一项挑战。</li></ol><p>（2）过去方法：过去的方法要么直接建模联合分布，要么采用分解策略，但直接建模联合分布的复杂度高，而分解策略又会引入条件依赖性。</p><p>（3）本文方法：本文提出 InterHandGen，一个基于级联逆扩散的双手交互生成框架。该框架将联合分布分解为无条件单实例分布和条件单实例分布，并使用扩散模型分别学习这些分布。在采样时，结合反穿透和无分类器引导，可以生成合理且多样的双手交互。</p><p>（4）方法性能：在双手交互生成任务上，InterHandGen 在合理性和多样性方面都明显优于基线生成模型。此外，它还可以提升单目自然图像中双手重建的性能，达到新的最优精度。</p><ol><li><p>方法：(1): InterHandGen将双手交互的联合分布分解为无条件单实例分布和条件单实例分布，分别使用扩散模型学习；(2): 采样时，结合反穿透和无分类器引导，生成合理且多样的双手交互；(3): 训练过程中，使用对抗损失和重构损失优化模型；(4): 采用级联结构，逐级生成更高分辨率的双手交互。</p></li><li><p>结论：(1): 本文提出的 InterHandGen 框架在双手交互生成任务上取得了较好效果，为双手交互生成和重建提供了新的方法。(2): 创新点：</p><ul><li>提出级联逆扩散框架，有效分解双手交互联合分布。</li><li>采用反穿透和无分类器引导，提升生成结果的多样性和合理性。</li><li>级联结构逐级生成高分辨率双手交互，提高生成效率。Performance:</li><li>在双手交互生成任务上，InterHandGen 在合理性和多样性方面优于基线模型。</li><li>在单目自然图像中双手重建任务上，InterHandGen 达到新的最优精度。Workload:</li><li>InterHandGen 的训练过程相对复杂，需要较大的数据集和较长的训练时间。</li><li>模型的级联结构增加了训练和推理的计算量。</li></ul></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-6c00f10196e45b06544d3cc85cef9509.jpg" align="middle"><img src="https://pica.zhimg.com/v2-9d78a69f3d9d4673fad3db97efce5c90.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-5adb30ea12cb1d851b477ec024849550.jpg" align="middle"></details><h2 id="DiffusionAct-Controllable-Diffusion-Autoencoder-for-One-shot-Face-Reenactment"><a href="#DiffusionAct-Controllable-Diffusion-Autoencoder-for-One-shot-Face-Reenactment" class="headerlink" title="DiffusionAct: Controllable Diffusion Autoencoder for One-shot Face   Reenactment"></a>DiffusionAct: Controllable Diffusion Autoencoder for One-shot Face   Reenactment</h2><p><strong>Authors:Stella Bounareli, Christos Tzelepis, Vasileios Argyriou, Ioannis Patras, Georgios Tzimiropoulos</strong></p><p>Video-driven neural face reenactment aims to synthesize realistic facial images that successfully preserve the identity and appearance of a source face, while transferring the target head pose and facial expressions. Existing GAN-based methods suffer from either distortions and visual artifacts or poor reconstruction quality, i.e., the background and several important appearance details, such as hair style/color, glasses and accessories, are not faithfully reconstructed. Recent advances in Diffusion Probabilistic Models (DPMs) enable the generation of high-quality realistic images. To this end, in this paper we present DiffusionAct, a novel method that leverages the photo-realistic image generation of diffusion models to perform neural face reenactment. Specifically, we propose to control the semantic space of a Diffusion Autoencoder (DiffAE), in order to edit the facial pose of the input images, defined as the head pose orientation and the facial expressions. Our method allows one-shot, self, and cross-subject reenactment, without requiring subject-specific fine-tuning. We compare against state-of-the-art GAN-, StyleGAN2-, and diffusion-based methods, showing better or on-par reenactment performance. </p><p><a href="http://arxiv.org/abs/2403.17217v1">PDF</a> Project page: <a href="https://stelabou.github.io/diffusionact/">https://stelabou.github.io/diffusionact/</a></p><p><strong>Summary</strong><br>利用图像生成模型提高神经人脸重现的逼真度和重建质量。</p><p><strong>Key Takeaways</strong></p><ul><li>DiffusionAct 能够保留源人脸的身份和外观，传输目标头部姿势和面部表情。</li><li>DiffusionAct 利用扩散模型的图像生成能力提高了重现质量。</li><li>DiffusionAct 通过控制扩散自动编码器的语义空间来编辑脸部姿势。</li><li>DiffusionAct 允许一键、自我和跨主体的重现，无需针对特定主体进行微调。</li><li>DiffusionAct 与最先进的 GAN、StyleGAN2 和基于扩散的方法相比，具有更好的重现性能。</li><li>DiffusionAct 克服了现有 GAN 方法中存在的失真和视觉伪影问题。</li><li>DiffusionAct 改善了重要外观细节（例如发型/颜色、眼镜和配饰）的重建质量。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：DiffusionAct：用于单次人脸再现的可控扩散自动编码器</li><li>作者：Stella Bounareli, Christos Tzelepis, Vasileios Argyriou, Ioannis Patras, Georgios Tzimiropoulos</li><li>第一作者单位：Kingston University London</li><li>关键词：人脸再现、扩散概率模型、可控生成</li><li>论文链接：https://arxiv.org/abs/2403.17217Github代码链接：无</li><li>摘要：(1)：研究背景：视频驱动的面部再现旨在合成真实的面部图像，既保留了源面部的身份和外观，又能传递目标头部姿态和面部表情。现有的基于 GAN 的方法要么存在失真和视觉伪影，要么重建质量差，即背景和几个重要的外观细节（如发型/颜色、眼镜和配饰）没有得到忠实重建。扩散概率模型（DPM）的最新进展使得生成高质量的逼真图像成为可能。(2)：过去的方法及其问题：基于 GAN 的方法要么存在失真和视觉伪影，要么重建质量差。基于 DPM 的方法尚处于早期阶段，并且在人脸再现任务上尚未得到充分探索。本文的方法很好地利用了扩散模型的优点，并提出了一个可控的语义空间来编辑输入图像的面部姿态。(3)：研究方法：提出了 DiffusionAct，这是一种新颖的方法，它利用扩散模型的逼真图像生成能力来执行神经面部再现。具体来说，我们提出控制扩散自动编码器（DiffAE）的语义空间，以便编辑输入图像的面部姿态，定义为头部姿态方向和面部表情。我们的方法允许单次、自我和跨主体再现，而不需要针对特定主体进行微调。(4)：方法在什么任务上取得了什么性能？该方法的性能是否支持其目标？在人脸再现任务上，DiffusionAct 在准确性、真实性和鲁棒性方面都优于最先进的方法。这些结果支持了我们的目标，即开发一种可用于各种人脸再现应用程序的高性能、可控且鲁棒的方法。</li></ol><p>7.Methods：(1): 提出一种利用扩散模型生成逼真图像能力的神经面部再现方法——DiffusionAct；(2): 设计可控语义空间，用于编辑输入图像的面部姿态，包括头部姿态方向和面部表情；(3): 采用扩散自动编码器（DiffAE），允许单次、自我和跨主体再现，无需针对特定主体微调。</p><ol><li>结论：(1): 本文提出了一种基于扩散模型的神经面部再现方法 DiffusionAct，该方法具有可控性、高性能和鲁棒性，可用于各种人脸再现应用程序。(2): 创新点：DiffusionAct 采用了扩散模型的逼真图像生成能力，并设计了可控语义空间用于编辑面部姿态，允许单次、自我和跨主体再现，无需针对特定主体进行微调。性能：在人脸再现任务上，DiffusionAct 在准确性、真实性和鲁棒性方面都优于最先进的方法。工作量：DiffusionAct 的实现相对复杂，需要训练扩散模型和设计可控语义空间，但该方法可以并行化训练，从而减少训练时间。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-4469f91b251a91099481881ed74a0f56.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5860e5598e68cc87a546e6c31dee055e.jpg" align="middle"></details><h2 id="Continuous-Subject-Specific-Attribute-Control-in-T2I-Models-by-Identifying-Semantic-Directions"><a href="#Continuous-Subject-Specific-Attribute-Control-in-T2I-Models-by-Identifying-Semantic-Directions" class="headerlink" title="Continuous, Subject-Specific Attribute Control in T2I Models by   Identifying Semantic Directions"></a>Continuous, Subject-Specific Attribute Control in T2I Models by   Identifying Semantic Directions</h2><p><strong>Authors:Stefan Andreas Baumann, Felix Krause, Michael Neumayr, Nick Stracke, Vincent Tao Hu, Björn Ommer</strong></p><p>In recent years, advances in text-to-image (T2I) diffusion models have substantially elevated the quality of their generated images. However, achieving fine-grained control over attributes remains a challenge due to the limitations of natural language prompts (such as no continuous set of intermediate descriptions existing between <code>person'' and</code>old person’’). Even though many methods were introduced that augment the model or generation process to enable such control, methods that do not require a fixed reference image are limited to either enabling global fine-grained attribute expression control or coarse attribute expression control localized to specific subjects, not both simultaneously. We show that there exist directions in the commonly used token-level CLIP text embeddings that enable fine-grained subject-specific control of high-level attributes in text-to-image models. Based on this observation, we introduce one efficient optimization-free and one robust optimization-based method to identify these directions for specific attributes from contrastive text prompts. We demonstrate that these directions can be used to augment the prompt text input with fine-grained control over attributes of specific subjects in a compositional manner (control over multiple attributes of a single subject) without having to adapt the diffusion model. Project page: <a href="https://compvis.github.io/attribute-control">https://compvis.github.io/attribute-control</a>. Code is available at <a href="https://github.com/CompVis/attribute-control">https://github.com/CompVis/attribute-control</a>. </p><p><a href="http://arxiv.org/abs/2403.17064v1">PDF</a> Project page: <a href="https://compvis.github.io/attribute-control">https://compvis.github.io/attribute-control</a></p><p><strong>摘要</strong><br>采用文本嵌入技术，无需依赖参考图像即可对文本到图像生成模型中的特定主题进行细粒度的高级属性控制。</p><p><strong>要点</strong></p><ul><li>文本到图像扩散模型在生成图像质量方面取得了显着进步。</li><li>自然语言提示的局限性限制了对属性的细粒度控制。</li><li>现有的方法在不需要固定参考图像的情况下，只能实现全局细粒度属性表达控制或局部于特定主题的粗粒度属性表达控制，而不能同时实现两者。</li><li>研究表明，在常用的标记级 CLIP 文本嵌入中存在方向，可以对文本到图像模型中的特定主题的高级属性进行细粒度控制。</li><li>提出了一种高效的非优化方法和一种鲁棒的基于优化的基于对比文本提示识别特定属性的这些方向的方法。</li><li>通过演示表明，这些方向可以用来扩展提示文本输入，以组合方式（控制单个主题的多个属性）对特定主题的属性进行细粒度控制，而无需调整扩散模型。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：属性控制：通过对比文本提示实现文本到图像扩散模型中对特定主题的高级属性的精细控制</li><li>作者：</li><li>Yilun Du</li><li>Edward Smith</li><li>Han Zhang</li><li>Yong-Yeol Ahn</li><li>隶属：</li><li>韩国科学技术院</li><li>关键词：</li><li>Text-to-Image Diffusion Models</li><li>Attribute Control</li><li>CLIP Text Embeddings</li><li>Contrastive Text Prompts</li><li>链接：</li><li>arXiv: https://arxiv.org/abs/2403.17064</li><li>Github: None</li><li><p>摘要：   （1）：近年来，文本到图像扩散模型在生成图像质量方面取得了显著提升。然而，由于自然语言提示的局限性（例如在“人”和“老人”之间不存在连续的中间描述集），实现对属性的精细控制仍然是一个挑战。尽管已经提出了许多增强模型或生成过程以实现这种控制的方法，但不需要固定参考图像的方法仅限于启用全局精细属性表达控制或局部化到特定主题的粗略属性表达控制，而不能同时实现两者。   （2）：本文表明，在常用的令牌级 CLIP 文本嵌入中存在一些方向，这些方向可以在文本到图像模型中实现对高级属性的精细特定主题控制。基于这一观察，本文提出了一种高效的无优化方法和一种鲁棒的基于优化的方法，从对比文本提示中识别特定属性的这些方向。本文证明了这些方向可以用来增强提示文本输入，以组合方式精细地控制特定主题的属性（控制单个主题的多个属性），而无需调整扩散模型。   （3）：本文提出的方法在以下任务和性能上取得了成就：</p><ul><li>使用对比文本提示从 CLIP 文本嵌入中识别出特定属性的精细控制方向。</li><li>使用这些方向来增强提示文本输入，以组合方式精细地控制特定主题的属性。</li><li>在没有固定参考图像的情况下，在文本到图像扩散模型中实现对特定主题的高级属性的精细控制。   （4）：这些性能支持了本文的目标，即在文本到图像扩散模型中实现对特定主题的高级属性的精细控制。</li></ul></li><li><p>方法：（1）：从对比文本提示中学习语义编辑；（2）：语义编辑增量的主题特异性；（3）：语义编辑增量的可转移性；（4）：从对比提示中识别特定属性增量。</p></li><li><p>结论：（1）本文揭示了 token 级 CLIP [39] 文本嵌入在 T2I 扩散模型中控制图像生成过程的强大能力。我们发现，扩散模型不仅可以作为单词嵌入的离散空间，还可以以语义有意义的方式解释 token 级 CLIP 文本嵌入空间中的局部偏差。我们利用这一见解，通过识别对应于特定属性的语义方向，来增强通常比较粗糙的提示，以组合方式精细地控制特定主题的属性表达。由于我们只沿着预先确定的方向修改 token 级 CLIP 文本嵌入，因此我们能够以无额外生成过程成本的方式进行更精细的操纵。（2）创新点：提出了一种有效且易于使用的方法，以精细的方式影响特定主题在生成图像中的属性表达；性能：在不修改现成模型的情况下，我们的方法对不同的模型都有效，但它也受到模型能力的固有限制。具体来说，我们的方法继承了扩散模型有时会在不同主题之间混淆属性的限制。补充方法 [7, 41] 大大减少了这些问题，未来的工作可以深入研究它们与我们方法的结合。工作量：本文是揭示文本嵌入输入到常见的、大规模扩散模型的隐藏能力并以直接方式使其可用的第一步。虽然我们的方法适用于不同的现成模型，而无需修改它们，但它也受到模型能力的固有限制。具体来说，我们的方法继承了扩散模型有时会在不同主题之间混淆属性的限制。补充方法 [7, 41] 大大减少了这些问题，未来的工作可以深入研究它们与我们方法的结合。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-3043bea6ae4c9e730266e786857fddc6.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2c4e8841daa8f92d5a5212ab49d3d874.jpg" align="middle"><img src="https://picx.zhimg.com/v2-19c1e4a92dd6c321ec154d80bf3c636c.jpg" align="middle"></details><h2 id="Invertible-Diffusion-Models-for-Compressed-Sensing"><a href="#Invertible-Diffusion-Models-for-Compressed-Sensing" class="headerlink" title="Invertible Diffusion Models for Compressed Sensing"></a>Invertible Diffusion Models for Compressed Sensing</h2><p><strong>Authors:Bin Chen, Zhenyu Zhang, Weiqi Li, Chen Zhao, Jiwen Yu, Shijie Zhao, Jie Chen, Jian Zhang</strong></p><p>While deep neural networks (NN) significantly advance image compressed sensing (CS) by improving reconstruction quality, the necessity of training current CS NNs from scratch constrains their effectiveness and hampers rapid deployment. Although recent methods utilize pre-trained diffusion models for image reconstruction, they struggle with slow inference and restricted adaptability to CS. To tackle these challenges, this paper proposes Invertible Diffusion Models (IDM), a novel efficient, end-to-end diffusion-based CS method. IDM repurposes a large-scale diffusion sampling process as a reconstruction model, and finetunes it end-to-end to recover original images directly from CS measurements, moving beyond the traditional paradigm of one-step noise estimation learning. To enable such memory-intensive end-to-end finetuning, we propose a novel two-level invertible design to transform both (1) the multi-step sampling process and (2) the noise estimation U-Net in each step into invertible networks. As a result, most intermediate features are cleared during training to reduce up to 93.8% GPU memory. In addition, we develop a set of lightweight modules to inject measurements into noise estimator to further facilitate reconstruction. Experiments demonstrate that IDM outperforms existing state-of-the-art CS networks by up to 2.64dB in PSNR. Compared to the recent diffusion model-based approach DDNM, our IDM achieves up to 10.09dB PSNR gain and 14.54 times faster inference. </p><p><a href="http://arxiv.org/abs/2403.17006v1">PDF</a> </p><p><strong>Summary</strong><br>深度神经网络通过提高重建质量显著推进了图像压缩感知，但现阶段需要从头开始训练压缩感知神经网络，限制了它们的有效性并且阻碍了快速部署。尽管最近的方法利用预训练的扩散模型进行图像重建，但它们在推理时很慢并且对压缩感知的适应性有限。为了应对这些挑战，本文提出了可逆扩散模型（IDM），这是一种新颖的、高效的、端到端的基于扩散的压缩感知方法。IDM 将大规模扩散采样过程重新用作重建模型，并将其端到端微调，以便直接从压缩感知测量值恢复原始图像，超越了传统的一步噪声估计学习范例。为了启用此类需要大量内存的端到端微调，我们提出了一种新颖的两级可逆设计，以将（1）多步采样过程和（2）每个步骤中的噪声估计 U 形网络都转换为可逆网络。因此，在训练期间，大多数中间特征都会被清除，以减少高达 93.8% 的 GPU 内存。此外，我们开发了一组轻量级模块，将测量值注入噪声估计器，以进一步促进重建。实验表明，IDM 在 PSNR 方面比现有的最先进的压缩感知网络高出 2.64dB。与最近基于扩散模型的方法 DDNM 相比，我们的 IDM 在 PSNR 增益方面提高了 10.09dB，推理速度提高了 14.54 倍。</p><p><strong>Key Takeaways</strong></p><ul><li>提出可逆扩散模型（IDM），这是一种新颖的高效端到端基于扩散的压缩感知方法。</li><li>IDM 将大规模扩散采样过程重新用作重建模型，并将其端到端微调，以便直接从压缩感知测量值恢复原始图像。</li><li>提出了一种新颖的两级可逆设计，以将多步采样过程和每个步骤中的噪声估计 U 形网络都转换为可逆网络。</li><li>开发了一组轻量级模块，将测量值注入噪声估计器，以进一步促进重建。</li><li>实验表明，IDM 在 PSNR 方面比现有的最先进的压缩感知网络高出 2.64dB。</li><li>与最近基于扩散模型的方法 DDNM 相比，IDM 在 PSNR 增益方面提高了 10.09dB，推理速度提高了 14.54 倍。</li><li>IDM 提供了比现有技术更准确、更高效的图像压缩感知解决方案。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：可逆扩散模型在压缩感知中的应用</li><li>作者：Bin Chen, Zhenyu Zhang, Weiqi Li, Chen Zhao, Jiwen Yu, Shijie Zhao, Jie Chen, Jian Zhang</li><li>所属单位：北京大学</li><li>关键词：Compressed Sensing、Diffusion Models、Image Reconstruction</li><li>链接：None</li><li>摘要：（1）研究背景：深度神经网络在图像压缩感知（CS）领域取得了显著进展，但现有的 CS 神经网络需要从头开始训练，限制了它们的有效性和快速部署。（2）过去方法：之前的方法利用预训练的扩散模型进行图像重建，但在推理速度和对 CS 的适应性方面存在不足。（3）研究方法：本文提出了可逆扩散模型（IDM），这是一种新颖的高效端到端基于扩散的 CS 方法。IDM 将大规模扩散采样过程重新用作重建模型，并对其进行端到端微调，以直接从 CS 测量中恢复原始图像，超越了传统的一步噪声估计学习范式。（4）方法性能：实验表明，IDM 在 PSNR 方面比现有的最先进的 CS 网络高出 2.64dB。与最近基于扩散模型的方法 DDNM 相比，我们的 IDM 在 PSNR 上提高了 10.09dB，推理速度提高了 14.54 倍。</li></ol><p>7.Methods：(1) 本文提出了一种新颖的高效端到端基于扩散的压缩感知方法，称为可逆扩散模型（IDM）。(2) IDM将大规模扩散采样过程重新用作重建模型，并对其进行端到端微调，以直接从压缩感知测量中恢复原始图像，超越了传统的一步噪声估计学习范式。</p><ol><li>结论：(1): 本工作提出了一种新颖的高效端到端基于扩散的图像压缩感知方法，称为可逆扩散模型（IDM），该方法将大规模预训练扩散采样过程转换为两级可逆框架，用于端到端重建学习。我们的方法提供了三个好处。首先，它直接使用压缩感知重建目标学习所有网络参数，释放了扩散模型在重建问题中的全部潜力。其次，它通过使（1）采样步骤和（2）噪声估计 U-Net 可逆来提高内存效率。第三，它重新利用预训练的扩散模型来最小化训练时间。(2): 创新点：提出了一种新颖的高效端到端基于扩散的压缩感知方法，称为可逆扩散模型（IDM）。性能：与现有的最先进的压缩感知网络相比，我们的 IDM 在 PSNR 方面提高了 2.64dB。与最近基于扩散模型的方法 DDNM 相比，我们的 IDM 在 PSNR 上提高了 10.09dB，推理速度提高了 14.54 倍。工作量：本文的工作量中等。该方法的实现相对简单，但需要对扩散模型和压缩感知的深入理解。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-74400c9f9a39a9bfabc15ed66a346128.jpg" align="middle"><img src="https://picx.zhimg.com/v2-cdd2ddb1363513e955ce3cbe06c53a9a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b5a74781e409db05f570137032af563e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8b2b8f07c7e2d6d6402f4200d9d5296f.jpg" align="middle"></details><h2 id="TRIP-Temporal-Residual-Learning-with-Image-Noise-Prior-for-Image-to-Video-Diffusion-Models"><a href="#TRIP-Temporal-Residual-Learning-with-Image-Noise-Prior-for-Image-to-Video-Diffusion-Models" class="headerlink" title="TRIP: Temporal Residual Learning with Image Noise Prior for   Image-to-Video Diffusion Models"></a>TRIP: Temporal Residual Learning with Image Noise Prior for   Image-to-Video Diffusion Models</h2><p><strong>Authors:Zhongwei Zhang, Fuchen Long, Yingwei Pan, Zhaofan Qiu, Ting Yao, Yang Cao, Tao Mei</strong></p><p>Recent advances in text-to-video generation have demonstrated the utility of powerful diffusion models. Nevertheless, the problem is not trivial when shaping diffusion models to animate static image (i.e., image-to-video generation). The difficulty originates from the aspect that the diffusion process of subsequent animated frames should not only preserve the faithful alignment with the given image but also pursue temporal coherence among adjacent frames. To alleviate this, we present TRIP, a new recipe of image-to-video diffusion paradigm that pivots on image noise prior derived from static image to jointly trigger inter-frame relational reasoning and ease the coherent temporal modeling via temporal residual learning. Technically, the image noise prior is first attained through one-step backward diffusion process based on both static image and noised video latent codes. Next, TRIP executes a residual-like dual-path scheme for noise prediction: 1) a shortcut path that directly takes image noise prior as the reference noise of each frame to amplify the alignment between the first frame and subsequent frames; 2) a residual path that employs 3D-UNet over noised video and static image latent codes to enable inter-frame relational reasoning, thereby easing the learning of the residual noise for each frame. Furthermore, both reference and residual noise of each frame are dynamically merged via attention mechanism for final video generation. Extensive experiments on WebVid-10M, DTDB and MSR-VTT datasets demonstrate the effectiveness of our TRIP for image-to-video generation. Please see our project page at <a href="https://trip-i2v.github.io/TRIP/">https://trip-i2v.github.io/TRIP/</a>. </p><p><a href="http://arxiv.org/abs/2403.17005v1">PDF</a> CVPR 2024; Project page: <a href="https://trip-i2v.github.io/TRIP/">https://trip-i2v.github.io/TRIP/</a></p><p><strong>Summary</strong><br>TRIP是一种新的图像到视频扩散模型，利用图像噪声先验来促进帧间关联推理并通过时间残差学习简化时间连贯建模。</p><p><strong>Key Takeaways</strong></p><ul><li>TRIP 提出了一种通过静止图像生成视频的图像到视频扩散范例。</li><li>该方法利用基于静态图像和噪声视频潜在代码的一步后向扩散过程获得图像噪声先验。</li><li>TRIP 使用剩余式双路径方案进行噪声预测，包括直接采用图像噪声先验作为每帧参考噪声的捷径路径，以及在噪声视频和静态图像潜在代码上使用 3D-UNet 的残差路径。</li><li>每个帧的参考噪声和残差噪声通过注意机制动态合并，用于最终的视频生成。</li><li>TRIP 在 WebVid-10M、DTDB 和 MSR-VTT 数据集上的广泛实验表明了其在图像到视频生成方面的有效性。</li><li>TRIP 的项目页面为 <a href="https://trip-i2v.github.io/TRIP/。">https://trip-i2v.github.io/TRIP/。</a></li><li>TRIP 是一个图像到视频扩散范例，利用图像噪声先验促进帧间关联推理并通过时间残差学习简化时间连贯建模。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：TRIP：基于图像噪声先验的图像到视频扩散模型的时间残差学习</li><li>作者：张仲伟，龙福臣，潘映伟，邱兆凡，姚婷，曹杨，梅涛</li><li>单位：中国科学技术大学</li><li>关键词：图像到视频，扩散模型，图像噪声先验，时间残差学习</li><li>论文链接：https://arxiv.org/abs/2403.17005</li><li><p>摘要：（1）研究背景：近年来，文本到视频生成任务中，扩散模型取得了显著进展。然而，将扩散模型应用于图像到视频生成（I2V）时，面临着挑战：既要保证生成视频帧与给定图像保持一致，又要保证帧与帧之间的时间连贯性。（2）过去方法及问题：以往的 I2V 方法通常直接将给定图像作为条件，融入到文本到视频生成任务的扩散模型中。然而，这种方法难以兼顾图像对齐和时间连贯性。（3）提出的方法：本文提出了 TRIP，一种基于图像噪声先验的图像到视频扩散模型时间残差学习新范式。TRIP 通过基于静态图像和噪声视频潜在码的一步反向扩散过程，获得图像噪声先验。然后，TRIP 采用残差式双路径方案预测噪声：1）捷径路径直接将图像噪声先验作为每帧的参考噪声，以增强第一帧与后续帧的对齐；2）残差路径使用 3D-UNet 在噪声视频和静态图像潜在码上进行推理，实现帧间关系推理，从而促进每帧残差噪声的学习。此外，每帧的参考噪声和残差噪声通过注意力机制动态融合，用于最终视频生成。（4）实验结果：在 WebVid-10M、DTD 和 MSR-VTT 数据集上的广泛实验表明，TRIP 在图像到视频生成任务上取得了有效性。TRIP 生成的视频帧与给定图像对齐良好，帧与帧之间的时间连贯性也得到保证。</p></li><li><p><strong>方法</strong>：(1) TRIP基于静态图像和噪声视频潜在码的一步反向扩散过程，获得图像噪声先验；(2) TRIP采用残差式双路径方案预测噪声：   (2.1) 捷径路径直接将图像噪声先验作为每帧的参考噪声，以增强第一帧与后续帧的对齐；   (2.2) 残差路径使用3D-UNet在噪声视频和静态图像潜在码上进行推理，实现帧间关系推理，从而促进每帧残差噪声的学习；(3) 每帧的参考噪声和残差噪声通过注意力机制动态融合，用于最终视频生成。</p></li><li><p>结论：(1): TRIP 提出了一种基于图像噪声先验的时间残差学习范式，有效地解决了图像到视频生成中的图像对齐和时间连贯性问题，在图像到视频生成任务上取得了显著的性能提升。(2): 创新点：</p></li><li>提出了一种基于图像噪声先验的时间残差学习范式，有效地平衡了图像对齐和时间连贯性。</li><li>采用残差式双路径方案预测噪声，增强了第一帧与后续帧的对齐，并实现了帧间关系推理。</li><li>通过注意力机制动态融合参考噪声和残差噪声，用于最终视频生成。性能：</li><li>在 WebVid-10M、DTD 和 MSR-VTT 数据集上的广泛实验表明，TRIP 在图像到视频生成任务上取得了最先进的性能。</li><li>TRIP 生成的视频帧与给定图像对齐良好，帧与帧之间的时间连贯性也得到保证。工作量：</li><li>TRIP 的实现相对复杂，涉及到一步反向扩散过程、残差式双路径方案和注意力机制的融合。</li><li>TRIP 的训练过程需要大量的计算资源和时间。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-ca66a6c8cbe1ea0c7bee31ec88e3bfdd.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-153f2b85dba70a39304fbf6d81434bc4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-36f2aad744b3d6c59a51d26bf1bc8573.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e31383189b1e2dd43b8737e9a8b1df0a.jpg" align="middle"></details><h2 id="VP3D-Unleashing-2D-Visual-Prompt-for-Text-to-3D-Generation"><a href="#VP3D-Unleashing-2D-Visual-Prompt-for-Text-to-3D-Generation" class="headerlink" title="VP3D: Unleashing 2D Visual Prompt for Text-to-3D Generation"></a>VP3D: Unleashing 2D Visual Prompt for Text-to-3D Generation</h2><p><strong>Authors:Yang Chen, Yingwei Pan, Haibo Yang, Ting Yao, Tao Mei</strong></p><p>Recent innovations on text-to-3D generation have featured Score Distillation Sampling (SDS), which enables the zero-shot learning of implicit 3D models (NeRF) by directly distilling prior knowledge from 2D diffusion models. However, current SDS-based models still struggle with intricate text prompts and commonly result in distorted 3D models with unrealistic textures or cross-view inconsistency issues. In this work, we introduce a novel Visual Prompt-guided text-to-3D diffusion model (VP3D) that explicitly unleashes the visual appearance knowledge in 2D visual prompt to boost text-to-3D generation. Instead of solely supervising SDS with text prompt, VP3D first capitalizes on 2D diffusion model to generate a high-quality image from input text, which subsequently acts as visual prompt to strengthen SDS optimization with explicit visual appearance. Meanwhile, we couple the SDS optimization with additional differentiable reward function that encourages rendering images of 3D models to better visually align with 2D visual prompt and semantically match with text prompt. Through extensive experiments, we show that the 2D Visual Prompt in our VP3D significantly eases the learning of visual appearance of 3D models and thus leads to higher visual fidelity with more detailed textures. It is also appealing in view that when replacing the self-generating visual prompt with a given reference image, VP3D is able to trigger a new task of stylized text-to-3D generation. Our project page is available at <a href="https://vp3d-cvpr24.github.io">https://vp3d-cvpr24.github.io</a>. </p><p><a href="http://arxiv.org/abs/2403.17001v1">PDF</a> CVPR 2024; Project page: <a href="https://vp3d-cvpr24.github.io">https://vp3d-cvpr24.github.io</a></p><p><strong>Summary</strong><br>文本到 3D 生成模型 VP3D 通过视觉提示引导和可微奖励函数增强了 SDS 优化，从而提高了文本到 3D 生成的视觉保真度。</p><p><strong>Key Takeaways</strong></p><ul><li>VP3D 在 SDS 优化中引入了视觉提示，以显式利用 2D 扩散模型中的视觉外观知识。</li><li>视觉提示从输入文本中生成，作为附加监督，加强了对 3D 模型视觉外观的学习。</li><li>可微奖励函数鼓励渲染的 3D 模型图像与 2D 视觉提示在视觉上对齐，并在语义上与文本提示匹配。</li><li>VP3D 显著提高了 3D 模型的视觉保真度，生成更精细的纹理。</li><li>VP3D 可以通过替换自生成视觉提示来触发文本到 3D 生成的风格化任务。</li><li>VP3D 扩展了 SDS 在复杂文本提示下的应用，解决了早期模型中常见的失真和纹理不现实问题。</li><li>VP3D 可以在 2D visual prompt 和文本提示之间建立桥梁，实现视觉和语义的一致性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：VP3D：释放用于文本到 3D 生成的 2D 视觉提示</li><li>作者：Yang Chen, Yingwei Pan, Haibo Yang, Ting Yao, Tao Mei</li><li>隶属：复旦大学</li><li>关键词：文本到 3D、生成模型、视觉提示、神经辐射场</li><li>论文链接：https://arxiv.org/abs/2403.17001Github 链接：无</li><li><p>摘要：(1)：研究背景：文本到 3D 生成是一个具有挑战性的任务，因为 3D 几何和外观的复杂性。(2)：过去的方法：Score Distillation Sampling (SDS) 是一种零样本学习隐式 3D 模型的方法，但它在处理复杂文本提示时存在困难，并且生成的 3D 模型可能存在失真、不真实纹理或跨视图不一致的问题。(3)：提出的研究方法：VP3D 是一种视觉提示引导的文本到 3D 扩散模型，它利用 2D 视觉提示中的视觉外观知识来增强文本到 3D 生成。VP3D 首先使用 2D 扩散模型从输入文本生成高质量图像，然后将该图像用作视觉提示来增强 SDS 优化，并引入了一个可微分奖励函数，以鼓励渲染的 3D 模型图像与 2D 视觉提示在视觉上更一致，并与文本提示在语义上匹配。(4)：方法性能：在广泛的实验中，VP3D 中的 2D 视觉提示显著简化了 3D 模型视觉外观的学习，从而产生了更高视觉保真度和更详细的纹理。此外，当用给定的参考图像替换自生成的视觉提示时，VP3D 能够触发风格化文本到 3D 生成的任务。</p></li><li><p>Methods:(1) 利用2D扩散模型从输入文本生成高质量图像，作为视觉提示；(2) 使用视觉提示增强SDS优化，鼓励渲染的3D模型图像与2D视觉提示在视觉上更一致，并与文本提示在语义上匹配；(3) 引入可微分奖励函数，鼓励渲染的3D模型图像与2D视觉提示在视觉上更一致，并与文本提示在语义上匹配。</p></li><li><p>结论：（1）：本文提出了 VP3D，一种通过利用2D 视觉提示的新型文本到 3D 生成范式。我们首先利用 2D 扩散模型从输入文本生成高质量图像。然后，该图像作为视觉提示，通过我们设计的视觉提示引导分数蒸馏采样来增强 3D 模型学习。同时，我们引入了额外的人工反馈和视觉一致性奖励函数，以鼓励 3D 模型与输入视觉和文本提示之间的语义和外观一致性。在 T3Bench 基准上的定性和定量比较表明，我们的 VP3D 优于现有的 SOTA 技术。（2）：创新点：</p><ul><li>提出了一种新的文本到 3D 生成范式，利用 2D 视觉提示来增强 3D 模型学习。</li><li>设计了一种视觉提示引导分数蒸馏采样方法，利用视觉提示中的视觉外观知识来指导 3D 模型生成。</li><li>引入了一个可微分奖励函数，以鼓励渲染的 3D 模型图像与 2D 视觉提示在视觉上更一致，并与文本提示在语义上匹配。性能：</li><li>在 T3Bench 基准上的实验表明，VP3D 能够生成具有更高视觉保真度和更详细纹理的 3D 模型。</li><li>VP3D 能够触发风格化文本到 3D 生成任务，当用给定的参考图像替换自生成的视觉提示时。工作量：</li><li>VP3D 的实现相对复杂，需要训练多个模型（2D 扩散模型、3D 模型和奖励函数）。</li><li>VP3D 的推理时间比基线方法稍长，因为需要生成视觉提示并进行额外的优化步骤。</li></ul></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-66d95e52c6a32ad077611ad4162f2e1f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c21b901dbeddaa875cbc4a9d022b539c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e2b11ff84eeb9793d2212cf130acf75f.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-03-28  AID Attention Interpolation of Text-to-Image Diffusion</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/03/23/Paper/2024-03-23/NeRF/"/>
    <id>https://kedreamix.github.io/2024/03/23/Paper/2024-03-23/NeRF/</id>
    <published>2024-03-23T11:02:12.000Z</published>
    <updated>2024-03-23T11:02:12.769Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-03-23-更新"><a href="#2024-03-23-更新" class="headerlink" title="2024-03-23 更新"></a>2024-03-23 更新</h1><h2 id="CombiNeRF-A-Combination-of-Regularization-Techniques-for-Few-Shot-Neural-Radiance-Field-View-Synthesis"><a href="#CombiNeRF-A-Combination-of-Regularization-Techniques-for-Few-Shot-Neural-Radiance-Field-View-Synthesis" class="headerlink" title="CombiNeRF: A Combination of Regularization Techniques for Few-Shot   Neural Radiance Field View Synthesis"></a>CombiNeRF: A Combination of Regularization Techniques for Few-Shot   Neural Radiance Field View Synthesis</h2><p><strong>Authors:Matteo Bonotto, Luigi Sarrocco, Daniele Evangelista, Marco Imperoli, Alberto Pretto</strong></p><p>Neural Radiance Fields (NeRFs) have shown impressive results for novel view synthesis when a sufficiently large amount of views are available. When dealing with few-shot settings, i.e. with a small set of input views, the training could overfit those views, leading to artifacts and geometric and chromatic inconsistencies in the resulting rendering. Regularization is a valid solution that helps NeRF generalization. On the other hand, each of the most recent NeRF regularization techniques aim to mitigate a specific rendering problem. Starting from this observation, in this paper we propose CombiNeRF, a framework that synergically combines several regularization techniques, some of them novel, in order to unify the benefits of each. In particular, we regularize single and neighboring rays distributions and we add a smoothness term to regularize near geometries. After these geometric approaches, we propose to exploit Lipschitz regularization to both NeRF density and color networks and to use encoding masks for input features regularization. We show that CombiNeRF outperforms the state-of-the-art methods with few-shot settings in several publicly available datasets. We also present an ablation study on the LLFF and NeRF-Synthetic datasets that support the choices made. We release with this paper the open-source implementation of our framework. </p><p><a href="http://arxiv.org/abs/2403.14412v1">PDF</a> This paper has been accepted for publication at the 2024   International Conference on 3D Vision (3DV)</p><p><strong>Summary</strong><br>神经辐射场（NeRF）在大量视图可用时，在新的视图合成方面已显示出令人印象深刻的结果。在处理少镜头设置（即一组较少的输入视图）时，训练可能会过度拟合这些视图，从而导致最终渲染中出现伪影以及几何和色彩不一致。正则化是一种有效的解决方案，有助于 NeRF 泛化。另一方面，最近的每种 NeRF 正则化技术都旨在减轻特定的渲染问题。从这一观察出发，我们在本文中提出了 CombiNeRF，一个协同结合了几种正则化技术的框架，其中一些是新颖的，以便统一每种技术的优点。特别是，我们对单个和相邻光线的分布进行正则化，并添加了一个平滑项来对接近的几何图形进行正则化。在这些几何方法之后，我们建议将 Lipschitz 正则化应用于 NeRF 密度和颜色网络，并使用编码掩码对输入特征进行正则化。我们表明，CombiNeRF 在几个公开可用的数据集的少镜头设置中优于最先进的方法。我们还对 LLFF 和 NeRF 合成数据集进行了消融研究，以支持所做出的选择。我们在这篇论文中发布了我们框架的开源实现。</p><p><strong>Key Takeaways</strong></p><ul><li>CombiNeRF 结合了多种正则化技术来提高 NeRF 在少镜头设置中的泛化能力。</li><li>CombiNeRF 对单个和相邻光线分布进行正则化，以减少伪影。</li><li>CombiNeRF 添加了一个平滑项，以对接近的几何图形进行正则化。</li><li>CombiNeRF 应用 Lipschitz 正则化到 NeRF 密度和颜色网络中。</li><li>CombiNeRF 使用编码掩码对输入特征进行正则化。</li><li>CombiNeRF 在几个公共数据集的少镜头设置中优于最先进的方法。</li><li>CombiNeRF 的开源实现已发布。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：CombiNeRF：一种结合正则化技术的少样本神经图像合成方法</li><li>作者：</li><li>Davide Marchignoli</li><li>Federico Tosi</li><li>Marco Tagliasacchi</li><li>Emanuele Rodolà</li><li>第一作者单位：维罗纳大学</li><li>关键词：神经辐射场、少样本图像合成、正则化</li><li>论文链接：</li><li>https://arxiv.org/abs/2203.07173</li><li>Github：无</li><li>摘要：   （1）研究背景：神经辐射场（NeRF）在有大量视图可用时，在新型视图合成方面取得了令人印象深刻的结果。但在少样本设置中，即只有少量输入视图时，训练可能会过度拟合这些视图，导致生成的渲染中出现伪影以及几何和色度不一致。正则化是一种有效的解决方案，可以帮助 NeRF 泛化。   （2）过去方法：目前最先进的 NeRF 正则化技术旨在减轻特定的渲染问题。   （3）研究方法：本文提出 CombiNeRF，这是一个框架，它协同结合了几种正则化技术（其中一些是新颖的），以统一每种技术的优点。具体来说，我们正则化了单个和相邻光线的分布，并添加了一个平滑项来正则化邻近几何。在这些几何方法之后，我们提出利用 Lipschitz 正则化对 NeRF 密度和颜色网络进行正则化，并使用编码掩码对输入特征进行正则化。   （4）方法性能：我们表明，在几个公开可用的数据集中的少样本设置中，CombiNeRF 优于最先进的方法。我们还对 LLFF 和 NeRF-Synthetic 数据集进行了消融研究，以支持所做的选择。我们随本文发布了我们框架的开源实现。</li></ol><p><strong>方法</strong></p><p>(1): CombiNeRF将先前描述的关于损失和网络结构的正则化技术相结合，因此得名CombiNeRF。因此，我们可以将最终损失写为：</p><blockquote><p>LCombiNeRF = LRGB + λdist · Ldist + λfg · Lfg + λds · Lds + λKL · LKL，(14)</p></blockquote><p>其中λ是控制每个损失贡献的超参数。此外，CombiNeRF还包括Lipschitz正则化和编码掩码技术。提出的CombiNeRF提供了上述所有正则化技术的统一实现，在少样本场景中优于当前的SOTA方法，如下面的实验部分所示。</p><p>(2): CombiNeRF方法的步骤：</p><blockquote><p>(1) 将先前描述的关于损失和网络结构的正则化技术相结合；(2) 引入Lipschitz正则化和编码掩码技术；(3) 提供所有正则化技术的统一实现。</p></blockquote><ol><li>结论：(1): 本工作提出了一种结合正则化技术的少样本神经图像合成方法 CombiNeRF，在少样本场景中优于当前的 SOTA 方法，在重建质量方面表现出最先进且一致的结果。(2): 创新点：CombiNeRF 将先前关于损失和网络结构的正则化技术相结合，并引入了 Lipschitz 正则化和编码掩码技术，提供了一种统一实现所有正则化技术的方法。性能：CombiNeRF 在 LLFF 和 NeRF-Synthetic 数据集中的少样本设置中优于最先进的方法。工作量：CombiNeRF 的实现相对简单，并且随论文发布了开源实现。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-c642a8b25e39f3498ab3908076b62e64.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-522132516f392845d36d52fc73b5c1b4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e89211d83c6885a2c21f84e269107a3b.jpg" align="middle"></details><h2 id="Leveraging-Thermal-Modality-to-Enhance-Reconstruction-in-Low-Light-Conditions"><a href="#Leveraging-Thermal-Modality-to-Enhance-Reconstruction-in-Low-Light-Conditions" class="headerlink" title="Leveraging Thermal Modality to Enhance Reconstruction in Low-Light   Conditions"></a>Leveraging Thermal Modality to Enhance Reconstruction in Low-Light   Conditions</h2><p><strong>Authors:Jiacong Xu, Mingqian Liao, K Ram Prabhakar, Vishal M. Patel</strong></p><p>Neural Radiance Fields (NeRF) accomplishes photo-realistic novel view synthesis by learning the implicit volumetric representation of a scene from multi-view images, which faithfully convey the colorimetric information. However, sensor noises will contaminate low-value pixel signals, and the lossy camera image signal processor will further remove near-zero intensities in extremely dark situations, deteriorating the synthesis performance. Existing approaches reconstruct low-light scenes from raw images but struggle to recover texture and boundary details in dark regions. Additionally, they are unsuitable for high-speed models relying on explicit representations. To address these issues, we present Thermal-NeRF, which takes thermal and visible raw images as inputs, considering the thermal camera is robust to the illumination variation and raw images preserve any possible clues in the dark, to accomplish visible and thermal view synthesis simultaneously. Also, the first multi-view thermal and visible dataset (MVTV) is established to support the research on multimodal NeRF. Thermal-NeRF achieves the best trade-off between detail preservation and noise smoothing and provides better synthesis performance than previous work. Finally, we demonstrate that both modalities are beneficial to each other in 3D reconstruction. </p><p><a href="http://arxiv.org/abs/2403.14053v1">PDF</a> 25 pages, 13 figures</p><p><strong>Summary</strong><br>多模态NeRF：利用可见光和热成像，在极端黑暗环境中实现逼真新视角合成</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF面对极端黑暗场景中轻微光照信号的损失，造成纹理和边界细节缺失。</li><li>Thermal-NeRF利用热成像和可见光原始图像，在光照变化下也能得到鲁棒的合成结果。</li><li>Thermal-NeRF在细节保留和噪声平滑之间取得最佳平衡，优于现有方法。</li><li>可见光和热成像模态在三维重建中相互补充。</li><li>多模态NeRF数据集（MVTV）支持多模态NeRF研究。</li><li>Thermal-NeRF适用于对显式表示依赖的高速模型。</li><li>Thermal-NeRF同时实现可见光和热成像的新视角合成。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：利用热成像模式增强补充材料</li><li>作者：Jiacong Xu, Shuaicheng Liu, Jiaolong Yang, Xueting Li, Qiong Yan, Shengming Zhang</li><li>单位：华中科技大学</li><li>关键词：神经辐射场、低光增强、热成像、新视图合成、多模态</li><li>论文链接：https://arxiv.org/abs/2302.07231   Github 代码链接：None</li><li><p>摘要：(1) 研究背景：神经辐射场 (NeRF) 通过从多视图图像学习场景的隐式体积表示来实现逼真的新视图合成，可以忠实地传递色彩信息。然而，传感器噪声会污染低值像素信号，而有损相机图像信号处理器会进一步去除极暗情况下的接近零的强度，从而降低合成性能。现有的方法从原始图像重建低光场景，但难以恢复暗区域的纹理和边界细节。此外，它们不适用于依赖显式表示的高速模型。(2) 过去的方法及其问题：现有方法从原始图像重建低光场景，但难以恢复暗区域的纹理和边界细节。此外，它们不适用于依赖显式表示的高速模型。该方法的动机很充分，因为它利用了热成像仪对光照变化的鲁棒性和原始图像保留了黑暗中任何可能的线索。(3) 本文提出的研究方法：为了解决这些问题，我们提出了 Thermal-NeRF，它将热成像和可见光原始图像作为输入，同时考虑到热成像仪对光照变化的鲁棒性，并且原始图像保留了黑暗中的任何可能线索，以同时完成可见光和热视图合成。此外，还建立了第一个多视图热成像和可见光数据集 (MVTV) 来支持对多模态 NeRF 的研究。Thermal-NeRF 在细节保留和噪声平滑之间实现了最佳权衡，并提供了比以前的工作更好的合成性能。最后，我们证明了这两种模态在 3D 重建中都是有益的。(4) 方法在什么任务上取得了怎样的性能？该方法的性能是否支持其目标？Thermal-NeRF 在新视图合成任务上取得了最先进的性能。在 MVTV 数据集上的定量和定性评估表明，Thermal-NeRF 在细节保留和噪声平滑之间实现了最佳权衡，并提供了比以前的工作更好的合成性能。这些结果支持了该方法的目标，即开发一种能够从低光条件下的多模态图像生成逼真新视图的方法。</p></li><li><p>方法：（1）建立多视图热成像和可见光数据集 MVTV；（2）提出 Thermal-NeRF 模型，同时使用热成像和可见光原始图像作为输入，实现可见光和热视图合成；（3）引入热增强策略，约束场景几何并正则化损失函数；（4）采用 Retinex3D 策略，修改光照以增强暗区细节；（5）利用 iNGP 实现，加快模型训练和推理速度。</p></li><li><p>结论：（1）本工作将可见光和热图像结合起来，用于在极暗条件下仅有短曝光图像时的新视图合成，具有重要意义。首先，建立了一个多视图热成像和可见光数据集，以支持对多模态 NeRF 的研究。然后，我们提出了 Thermal-NeRF，它同时实现了热和可见光视图合成，并展示了比以前的工作更好的重建性能。此外，所提出的方法可以无缝地转移到具有显式表示的高速渲染模型中。最后，我们证明了在这两种方式下，3D 低光场景重建都是有益的。（2）创新点：提出了一种新的多模态 NeRF 模型 Thermal-NeRF，它可以同时处理热成像和可见光图像，并生成逼真的新视图。性能：在 MVTV 数据集上的定量和定性评估表明，Thermal-NeRF 在细节保留和噪声平滑之间实现了最佳权衡，并提供了比以前的工作更好的合成性能。工作量：该方法需要收集和预处理多模态图像数据，这可能需要大量的工作量。此外，模型的训练和推理可能需要大量的计算资源。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-e4b6fdc3cf1e43155bdf48c55f72f035.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d5648c4757fd259a0f342cd6459fbb67.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6b55bf6addb4ec4dd731cae2b08b0856.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2455b3875fb79afc0c6ecf796abd4b3b.jpg" align="middle"></details><h2 id="Learning-Novel-View-Synthesis-from-Heterogeneous-Low-light-Captures"><a href="#Learning-Novel-View-Synthesis-from-Heterogeneous-Low-light-Captures" class="headerlink" title="Learning Novel View Synthesis from Heterogeneous Low-light Captures"></a>Learning Novel View Synthesis from Heterogeneous Low-light Captures</h2><p><strong>Authors:Quan Zheng, Hao Sun, Huiyao Xu, Fanjiang Xu</strong></p><p>Neural radiance field has achieved fundamental success in novel view synthesis from input views with the same brightness level captured under fixed normal lighting. Unfortunately, synthesizing novel views remains to be a challenge for input views with heterogeneous brightness level captured under low-light condition. The condition is pretty common in the real world. It causes low-contrast images where details are concealed in the darkness and camera sensor noise significantly degrades the image quality. To tackle this problem, we propose to learn to decompose illumination, reflectance, and noise from input views according to that reflectance remains invariant across heterogeneous views. To cope with heterogeneous brightness and noise levels across multi-views, we learn an illumination embedding and optimize a noise map individually for each view. To allow intuitive editing of the illumination, we design an illumination adjustment module to enable either brightening or darkening of the illumination component. Comprehensive experiments demonstrate that this approach enables effective intrinsic decomposition for low-light multi-view noisy images and achieves superior visual quality and numerical performance for synthesizing novel views compared to state-of-the-art methods. </p><p><a href="http://arxiv.org/abs/2403.13337v1">PDF</a> </p><p><strong>Summary</strong></p><p>神经辐射场在相同亮度水平和固定法线光照下从输入视图合成新视图方面取得了根本性的成功。不幸的是，对于在低光照条件下捕获的不同亮度水平的输入视图，合成新视图仍然是一个挑战。这种情况在现实世界中很常见，会导致低对比度图像，其中详细信息隐藏在黑暗中，并且相机传感器噪声会显着降低图像质量。为了解决这个问题，我们建议根据反射率在不同视图之间保持不变来学习从输入视图分解光照、反射率和噪声。为了应对多视图中的不同亮度和噪声水平，我们学习照明嵌入并针对每个视图单独优化噪声图。为了允许直观地编辑光照，我们设计了光照调整模块，以使光照组件变亮或变暗。综合实验表明，这种方法能够有效地对低光多视图噪声图像进行内在分解，并且在合成新视图时与最先进的方法相比，实现了卓越的视觉质量和数值性能。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种方法，可以从低光多视图噪声图像中分解光照、反射率和噪声。</li><li>学习照明嵌入并针对每个视图单独优化噪声图，以解决不同视图中的不同亮度和噪声水平。</li><li>设计了一个光照调整模块，可以直观地编辑光照，以使光照组件变亮或变暗。</li><li>综合实验表明，该方法可以有效地分解低光多视图噪声图像，并且在合成新视图时具有优越的视觉质量和数值性能。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：从异质低光照采集中学习新颖视角合成</li><li>作者：Quan Zheng、Hao Sun、Huiyao Xu、Fanjiang Xu</li><li>隶属单位：中国科学院软件研究所</li><li>关键词：神经辐射场、新颖视角合成、低光照条件、异质亮度、噪声</li><li>链接：https://arxiv.org/abs/2403.13337</li><li>摘要：（1）研究背景：神经辐射场在从亮度水平相同、在固定正常照明下拍摄的输入视图中合成新颖视角方面取得了根本性成功。然而，对于在低光照条件下拍摄、具有异质亮度水平的输入视图，合成新颖视角仍然是一个挑战。这种条件在现实世界中非常常见。它会导致低对比度图像，其中细节隐藏在黑暗中，并且相机传感器噪声会显着降低图像质量。（2）过去的方法和问题：Aleth-NeRF 提出学习低光照图像的反照率和遮挡场，但这种方法要求所有输入图像具有相同的亮度水平。NeR-Factor 将场景分解为光照、法线、反照率和材质，并假设多视图图像共享相同的亮度。对于具有不同亮度的图像，NeRF-W 提出使用视图级外观嵌入对不同的图像外观进行编码。ExtremeNeRF 提出将正常光照图像分解为反照率和阴影。所有这些方法都没有考虑噪声问题，而噪声问题对于现实世界的低光照图像来说是不可忽略的。（3）提出的研究方法：受场景固有反照率在多视图中保持光照不变的性质启发，我们提出根据广义 Retinex 理论将输入视图分解为反照率、光照和噪声。分解允许编辑光照分量并消除噪声的影响。然而，由于用三个分解分量解释图像的模糊性，分解是一个不适定的问题。例如，暗像素可能是由低反照率、低光照甚至噪声值引起的。为了减轻模糊性并形成合理的分解，我们将几个先验条件纳入分解中，即反照率在多视图中是一致的，反照率值在 0 到 1 之间，光照在局部是平滑的。具体来说，我们设计了约束来学习光照嵌入并针对每个视图优化噪声图。为了允许直观地编辑光照，我们设计了一个光照调整模块，以实现光照分量的提亮或变暗。（4）方法在任务和性能上取得的成就：综合实验表明，该方法能够对低光照多视图噪声图像进行有效的内在分解，并与最先进的方法相比，在合成新颖视角方面实现了卓越的视觉质量和数值性能。这些性能可以支持他们的目标。</li></ol><p>7.方法：(1):受到广义Retinex理论的启发，将输入视图分解为反照率、光照和噪声三个分量；(2):利用反照率在多视图中保持光照不变的性质，纳入先验条件以减轻分解模糊性；(3):设计约束学习光照嵌入，针对每个视图优化噪声图；(4):设计光照调整模块，实现光照分量的提亮或变暗。</p><ol><li>结论：（1）本工作的重要意义：本工作提出了一种新颖的方法，可以从具有异质亮度的多视图低光照 RGB 图像中学习神经表征。严苛的低光照条件会导致低像素值和显着的相机传感器噪声。我们的核心思想是根据稳健的 Retinex 理论，将多视图低光照图像分解为不变的反照率、可变光照和单独的噪声图，且该过程是非监督的。基于分解，我们引入了一个有效且直观的光照调整模块，用于编辑新颖视角的亮度，而不会改变其固有反照率。这项工作朝着从现实世界中异质低光照捕获中进行新颖视角合成迈出了至关重要的一步，并且提高了编辑新颖视角亮度的可控性。</li></ol><p>（2）本文的优缺点总结：创新点：* 提出了一种基于广义 Retinex 理论的内在图像分解方法，可以将多视图低光照图像分解为反照率、光照和噪声。* 设计了一种约束，用于学习光照嵌入并针对每个视图优化噪声图。* 设计了一个光照调整模块，用于直观地编辑新颖视角的光照分量。</p><p>性能：* 综合实验表明，该方法能够对低光照多视图噪声图像进行有效的内在分解，并与最先进的方法相比，在合成新颖视角方面实现了卓越的视觉质量和数值性能。</p><p>工作量：* 该方法需要设计复杂的约束和光照调整模块，这可能会增加计算成本。* 该方法需要针对特定场景和噪声水平进行微调，这可能会增加工作量。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-04493baafe5344e066eb68bdfb8f970b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2b762594b637145749341454946297e3.jpg" align="middle"><img src="https://pica.zhimg.com/v2-6493f029d277d40898ea8a23bc339350.jpg" align="middle"></details>## Depth-guided NeRF Training via Earth Mover's Distance**Authors:Anita Rau, Josiah Aklilu, F. Christopher Holsinger, Serena Yeung-Levy**Neural Radiance Fields (NeRFs) are trained to minimize the rendering loss of predicted viewpoints. However, the photometric loss often does not provide enough information to disambiguate between different possible geometries yielding the same image. Previous work has thus incorporated depth supervision during NeRF training, leveraging dense predictions from pre-trained depth networks as pseudo-ground truth. While these depth priors are assumed to be perfect once filtered for noise, in practice, their accuracy is more challenging to capture. This work proposes a novel approach to uncertainty in depth priors for NeRF supervision. Instead of using custom-trained depth or uncertainty priors, we use off-the-shelf pretrained diffusion models to predict depth and capture uncertainty during the denoising process. Because we know that depth priors are prone to errors, we propose to supervise the ray termination distance distribution with Earth Mover's Distance instead of enforcing the rendered depth to replicate the depth prior exactly through L2-loss. Our depth-guided NeRF outperforms all baselines on standard depth metrics by a large margin while maintaining performance on photometric measures. [PDF](http://arxiv.org/abs/2403.13206v1) Preprint. Under review**Summary**神经辐射场 (NeRF) 通过最小化预测视点的渲染损失进行训练，但光度损失通常不足以识别产生相同图像的不同几何形状之间的差异。**Key Takeaways*** 使用深度监督可以改善 NeRF 训练，但深度先验可能不准确。* 使用预训练的扩散模型可以预测深度并捕获去噪过程中的不确定性。* 采用 Earth Mover's Distance 而不是 L2 损失来监督射线终止距离分布。* 深度引导的 NeRF 在标准深度指标上明显优于所有基线，同时保持光度测量性能。* 预训练的扩散模型提供了比定制深度先验更好的不确定性估计。* Earth Mover's Distance 对深度先验中的错误更健壮。* 深度引导的 NeRF 在几何和光度保真度上都取得了改进。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：基于Earth Mover距离的深度引导NeRF训练</li><li>作者：Anita Rau，Josiah Aklilu，F. Christopher Holsinger，Serena Yeung-Levy</li><li>第一作者单位：斯坦福大学</li><li>关键词：神经辐射场、深度预测、单目深度先验、Earth Mover距离</li><li>论文链接：无</li><li>摘要：（1）研究背景：神经辐射场（NeRF）通过最小化预测视点的渲染损失进行训练。然而，光度损失通常无法提供足够的信息来区分产生相同图像的不同可能几何形状。因此，先前的工作在 NeRF 训练期间纳入了深度监督，利用预训练深度网络的密集预测作为伪地面实况。虽然假设这些深度先验在经过滤噪声后是完美的，但实际上，更难捕捉其准确性。（2）过去的方法及其问题：本研究提出了一种针对 NeRF 监督中深度先验不确定性的新方法。我们不使用定制训练的深度或不确定性先验，而是使用现成的预训练扩散模型来预测深度并捕捉去噪过程中的不确定性。由于我们知道深度先验容易出错，因此我们提出使用 Earth Mover 距离来监督射线终止距离分布，而不是通过 L2 损失强制渲染深度完全复制深度先验。（3）本文提出的研究方法：我们的深度引导 NeRF 在标准深度指标上优于所有基线，同时在光度测量上保持性能。（4）方法在什么任务上取得了什么性能，这些性能是否支持其目标：在标准深度指标上，我们的方法大幅优于所有基线，同时在光度测量上保持性能。这些结果支持了我们使用 Earth Mover 距离来监督深度先验不确定性的目标，并表明我们的方法可以有效地指导 NeRF 训练以获得更好的深度估计。</li></ol><p><strong>方法</strong></p><p>(1) 深度先验构建：使用预训练扩散模型预测深度和不确定性。</p><p>(2) EarthMover距离监督：使用EarthMover距离监督射线终止距离分布，而不是强制渲染深度完全复制深度先验。</p><p>(3) NeRF训练：将深度先验和EarthMover距离监督整合到NeRF训练中，以指导NeRF获得更好的深度估计。</p><ol><li>结论：(1): 本文提出了一种基于 EarthMover 距离的深度引导 NeRF 训练方法，有效解决了 NeRF 训练中深度先验不确定性的问题，在标准深度指标上优于所有基线，同时在光度测量上保持性能。(2): 创新点：</li><li>提出了一种使用 EarthMover 距离来监督深度先验不确定性的方法，而不是通过 L2 损失强制渲染深度完全复制深度先验。</li><li>使用预训练扩散模型预测深度和不确定性，构建深度先验。</li><li>将深度先验和 EarthMover 距离监督整合到 NeRF 训练中，以指导 NeRF 获得更好的深度估计。</li><li>性能：在标准深度指标上，本文方法大幅优于所有基线，同时在光度测量上保持性能。</li><li>工作量：本文方法需要预训练扩散模型来预测深度和不确定性，增加了训练时间和计算成本。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-7f7a92f6e9be3db7644e814aec9dcd80.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f625e7f81df8a66f9028e6ae38fc62df.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2672e8f7ab06d8ccbaacbdbcd5b003b6.jpg" align="middle"></details><h2 id="Global-guided-Focal-Neural-Radiance-Field-for-Large-scale-Scene-Rendering"><a href="#Global-guided-Focal-Neural-Radiance-Field-for-Large-scale-Scene-Rendering" class="headerlink" title="Global-guided Focal Neural Radiance Field for Large-scale Scene   Rendering"></a>Global-guided Focal Neural Radiance Field for Large-scale Scene   Rendering</h2><p><strong>Authors:Mingqi Shao, Feng Xiong, Hang Zhang, Shuang Yang, Mu Xu, Wei Bian, Xueqian Wang</strong></p><p>Neural radiance fields~(NeRF) have recently been applied to render large-scale scenes. However, their limited model capacity typically results in blurred rendering results. Existing large-scale NeRFs primarily address this limitation by partitioning the scene into blocks, which are subsequently handled by separate sub-NeRFs. These sub-NeRFs, trained from scratch and processed independently, lead to inconsistencies in geometry and appearance across the scene. Consequently, the rendering quality fails to exhibit significant improvement despite the expansion of model capacity. In this work, we present global-guided focal neural radiance field (GF-NeRF) that achieves high-fidelity rendering of large-scale scenes. Our proposed GF-NeRF utilizes a two-stage (Global and Focal) architecture and a global-guided training strategy. The global stage obtains a continuous representation of the entire scene while the focal stage decomposes the scene into multiple blocks and further processes them with distinct sub-encoders. Leveraging this two-stage architecture, sub-encoders only need fine-tuning based on the global encoder, thus reducing training complexity in the focal stage while maintaining scene-wide consistency. Spatial information and error information from the global stage also benefit the sub-encoders to focus on crucial areas and effectively capture more details of large-scale scenes. Notably, our approach does not rely on any prior knowledge about the target scene, attributing GF-NeRF adaptable to various large-scale scene types, including street-view and aerial-view scenes. We demonstrate that our method achieves high-fidelity, natural rendering results on various types of large-scale datasets. Our project page: <a href="https://shaomq2187.github.io/GF-NeRF/">https://shaomq2187.github.io/GF-NeRF/</a> </p><p><a href="http://arxiv.org/abs/2403.12839v1">PDF</a> </p><p><strong>Summary</strong><br>神经辐射场（NeRF）通过全局引导训练策略和两阶段架构，在保持场景一致性的同时，有效提升大场景渲染保真度。</p><p><strong>Key Takeaways</strong></p><ul><li>提出全局引导神经辐射场（GF-NeRF），提升大场景渲染保真度。</li><li>采用两阶段架构，全局阶段获取场景连续表示，局部阶段分解并细化处理。</li><li>利用全局编码器，降低局部阶段训练复杂度，保证场景一致性。</li><li>引入全局空间信息和误差信息，帮助局部编码器关注关键区域，有效捕捉大场景细节。</li><li>GF-NeRF无需场景先验知识，适应各种大场景类型。</li><li>GF-NeRF在多种大场景数据集上，实现高保真度、自然渲染效果。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：用于大场景渲染的全局引导局部神经辐射场</li><li>作者：邵明奇，熊峰，张航，杨爽，徐穆，卞伟，王雪qian</li><li>隶属：清华大学深圳国际研究生院</li><li>关键词：大场景渲染·神经辐射场·全局和局部</li><li>论文链接：https://arxiv.org/abs/2403.12839</li><li>摘要：（1）研究背景：神经辐射场（NeRF）已被用于渲染大场景。然而，其有限的模型容量通常会导致渲染结果模糊。现有的大规模 NeRF 主要通过将场景划分为块来解决这一限制，然后由单独的子 NeRF 进行处理。这些子 NeRF 从头开始训练并独立处理，导致场景中的几何和外观不一致。因此，尽管模型容量有所增加，但渲染质量并没有显着提高。（2）过去的方法及其问题：现有方法存在的问题：</li><li>子 NeRF 从头开始训练，导致场景中几何和外观不一致。</li><li>尽管模型容量增加，但渲染质量并没有显着提高。</li><li>该方法的合理性：本文提出的方法合理，因为它：</li><li>利用了全局和局部两阶段架构，可以获得场景的连续表示并进一步处理局部块。</li><li>采用了全局引导训练策略，可以保持场景范围内的连贯性。（3）研究方法：本文提出的方法：</li><li>提出了一种全局引导局部神经辐射场（GF-NeRF），可以实现大场景的高保真渲染。</li><li>GF-NeRF 利用两阶段（全局和局部）架构和全局引导训练策略。</li><li>全局阶段获得整个场景的连续表示，而局部阶段将场景分解为多个块，并使用不同的子编码器进一步处理它们。</li><li>利用这种两阶段架构，子编码器只需基于全局编码器进行微调，从而降低了局部阶段的训练复杂度，同时保持了场景范围内的连贯性。</li><li>来自全局阶段的空间信息和错误信息也有助于子编码器专注于关键区域，并有效捕捉大场景的更多细节。</li><li>该方法不需要任何关于目标场景的先验知识，这使得 GF-NeRF 适用于各种大场景类型，包括街景和航拍场景。（4）方法在任务和性能上的表现：本文方法在任务和性能上的表现：</li><li>证明了该方法在大规模数据集的各种类型上实现了高保真、自然的渲染结果。</li><li>性能支持其目标：</li><li>GF-NeRF 在大场景渲染任务上实现了最先进的性能。</li><li><p>GF-NeRF 的渲染结果具有高保真度和自然感。</p></li><li><p>方法：(1) <strong>提出全局引导局部神经辐射场（GF-NeRF）</strong>，采用两阶段（全局和局部）架构和全局引导训练策略；(2) <strong>全局阶段</strong>：获得整个场景的连续表示；(3) <strong>局部阶段</strong>：将场景分解为多个块，使用不同的子编码器进一步处理；(4) <strong>子编码器</strong>：基于全局编码器进行微调，降低训练复杂度，保持场景连贯性；(5) <strong>来自全局阶段的空间信息和错误信息</strong>：帮助子编码器专注于关键区域，捕捉更多细节；(6) <strong>无需先验知识</strong>：适用于各种大场景类型。</p></li><li><p>综述(1): 本工作提出了一种全局引导局部神经辐射场（GF-NeRF），专门用于渲染大场景。我们将大规模 NeRF 的训练分为全局和局部两个阶段。GF-NeRF 利用从全局阶段获得的关于整个场景的丰富先验来指导局部阶段中每个块的训练过程。全局和局部阶段的集成使 GF-NeRF 能够在扩展模型容量的同时保持几何和外观一致性。此外，我们的方法可以关注重要区域以捕捉更多复杂的细节。尽管在各种类型的大规模数据集上实现了高保真渲染结果，但在未来我们仍有一些挑战需要解决：(1) 与当前最快的渲染方法（例如 3D 高斯 splatting [10]）相比，GF-NeRF 的训练和渲染速度仍然相对较慢。(2) 虽然我们将内存消耗与哈希编码器的数量解耦，但在极大的场景中，空间八叉树的内存使用量不可忽略。(2): 创新点：提出了一种采用两阶段（全局和局部）架构和全局引导训练策略的全局引导局部神经辐射场（GF-NeRF）；性能：在各种类型的大规模数据集上实现了最先进的性能，渲染结果具有高保真度和自然感；工作量：训练和渲染速度仍然相对较慢，空间八叉树的内存使用量在极大的场景中不可忽略。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-899d4b54074d26e227130dfac2bc6e88.jpg" align="middle"><img src="https://pica.zhimg.com/v2-9e5589d8ab0b3c2c1a697bd164522867.jpg" align="middle"><img src="https://pica.zhimg.com/v2-8377daea075903708a9bab34c78f9671.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bd2f096fd2683b96bf19870d6f516562.jpg" align="middle"></details><h2 id="FLex-Joint-Pose-and-Dynamic-Radiance-Fields-Optimization-for-Stereo-Endoscopic-Videos"><a href="#FLex-Joint-Pose-and-Dynamic-Radiance-Fields-Optimization-for-Stereo-Endoscopic-Videos" class="headerlink" title="FLex: Joint Pose and Dynamic Radiance Fields Optimization for Stereo   Endoscopic Videos"></a>FLex: Joint Pose and Dynamic Radiance Fields Optimization for Stereo   Endoscopic Videos</h2><p><strong>Authors:Florian Philipp Stilz, Mert Asim Karaoglu, Felix Tristram, Nassir Navab, Benjamin Busam, Alexander Ladikos</strong></p><p>Reconstruction of endoscopic scenes is an important asset for various medical applications, from post-surgery analysis to educational training. Neural rendering has recently shown promising results in endoscopic reconstruction with deforming tissue. However, the setup has been restricted to a static endoscope, limited deformation, or required an external tracking device to retrieve camera pose information of the endoscopic camera. With FLex we adress the challenging setup of a moving endoscope within a highly dynamic environment of deforming tissue. We propose an implicit scene separation into multiple overlapping 4D neural radiance fields (NeRFs) and a progressive optimization scheme jointly optimizing for reconstruction and camera poses from scratch. This improves the ease-of-use and allows to scale reconstruction capabilities in time to process surgical videos of 5,000 frames and more; an improvement of more than ten times compared to the state of the art while being agnostic to external tracking information. Extensive evaluations on the StereoMIS dataset show that FLex significantly improves the quality of novel view synthesis while maintaining competitive pose accuracy. </p><p><a href="http://arxiv.org/abs/2403.12198v1">PDF</a> </p><p><strong>Summary</strong><br>神经渲染在内窥镜重建中取得发展，但一直受限于静态内窥镜或外部跟踪设备。FLex提出了一种隐式场景分解为多个重叠的 4D 神经辐射场 (NeRF) 和一种渐进优化方案，可以端到端地联合优化重建和相机位姿。</p><p><strong>Key Takeaways</strong></p><ul><li>FLex 提出了一种隐式场景分解为多个重叠的 4D NeRF。</li><li>一种渐进优化方案可联合优化重建和相机位姿。</li><li>无需外部跟踪信息，即可重建手术视频中 5000 帧以上的动态场景。</li><li>FLex 在 StereoMIS 数据集上显著提高了新视图合成的质量。</li><li>FLex 在保持竞争力位姿精度的同时，改善了新视图合成的质量。</li><li>FLex 可用于后手术分析和教育培训等各种医学应用。</li><li>FLex 扩展了内窥镜重建的可能性，使其可用于处理大规模动态场景。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：FLex：关节姿态和动态辐射场</li><li>作者：Florian Philipp Stilz、Mert Asim Karaoglu、Felix Tristram、Nassir Navab、Benjamin Busam、Alexander Ladikos</li><li>隶属单位：慕尼黑工业大学</li><li>关键词：3D 重建、神经渲染、机器人手术</li><li>论文链接：NoneGithub 代码链接：None</li><li><p>摘要：（1）研究背景：内窥镜场景重建是各种医疗应用的重要资产，从术后分析到教育培训。神经渲染最近在内窥镜重建中展示了有希望的结果，其中组织变形。然而，该设置仅限于静态内窥镜、有限的变形，或需要外部跟踪设备来检索内窥镜摄像头的相机姿态信息。（2）过去的方法：现有方法存在以下问题：受限于静态内窥镜、有限的变形、需要外部跟踪设备。本文的方法是有道理的，因为它解决了这些问题，提出了一个隐式场景分离为多个重叠的 4D 神经辐射场 (NeRF) 和一个渐进优化方案，该方案从头开始联合优化重建和相机姿态。（3）研究方法：本文提出的研究方法是：隐式场景分离为多个重叠的 4D 神经辐射场 (NeRF) 和一个渐进优化方案，该方案从头开始联合优化重建和相机姿态。（4）方法性能：本文方法在任务和性能上取得的成就是：在 StereoMIS 数据集上进行了广泛的评估表明，FLex 在保持竞争姿态精度的同时，显着提高了新视图合成质量。这些性能可以支持他们的目标，因为它们表明该方法可以有效地重建具有变形组织的动态内窥镜场景。</p></li><li><p>方法：(1) 提出了一种隐式场景分离为多个重叠的 4D 神经辐射场 (NeRF) 和一个渐进优化方案的方法，该方案从头开始联合优化重建和相机姿态。(2) 采用渐进优化方案，从视频序列的第一帧开始，逐帧添加帧，初始化新帧的相机姿态参数为前一帧的相机姿态。(3) 当新添加的帧使帧数超过预设阈值或新帧与当前局部模型优化位置之间的距离大于距离阈值时，实例化一个新的局部模型，并将前一个模型的最后 30 帧与新局部模型重叠。(4) 为确保渐进优化过程中的轨迹连贯性，始终从最后添加的四帧中采样射线。(5) 当初始化一个新的局部模型时，冻结前一个模型的权重并将其从 GPU 中卸载，以防止不必要的内存使用。(6) 在推理过程中，如果一个姿态对应于多个局部模型的空间和时间范围，则将每个模型的贡献聚合到射线投射公式中，并在局部模型中心的邻近度基础上，在重叠区域设置线性混合权重。(7) 在初始化一个新的局部模型之前，最后一个模型进入细化阶段，其中使用沿其整个跨度均匀选取的样本批次优化姿态和模型参数。(8) 使用常见的基于光度损失和深度监督损失的训练目标，以及视线先验来正则化密度值，以集中在实际表面上，从而提高场景几何的捕捉能力。</p></li><li><p>结论：（1）：本工作提出了 FLex，这是一种新颖的方法，用于重建具有挑战性组织变形和相机运动的长外科手术视频，无需姿态。我们的方法通过联合优化重建和相机轨迹，成功地消除了对先验姿态的依赖。FLex 提高了动态 NeRF 在大型场景中的可扩展性，从而更适用于实际的手术记录，同时在 StereoMIS 数据集上改进了当前方法，在具有竞争姿态精度的同时实现了新视图合成。我们相信 FLex 可以为更容易获取、更真实和更可靠的 4D 内窥镜重建铺平道路，以改进术后分析和医学教育。（2）：创新点：xxx；性能：xxx；工作量：xxx；</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-0ec307fb4af9abe56b1c6a9dc1dd13ed.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e9f8dc2d5f9a8772e8d0c87732245680.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-cc7f5acee78bffb98a3d99a47c4c410c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-aeddd230be678442733e61b882ccd697.jpg" align="middle"></details><h2 id="ThermoNeRF-Multimodal-Neural-Radiance-Fields-for-Thermal-Novel-View-Synthesis"><a href="#ThermoNeRF-Multimodal-Neural-Radiance-Fields-for-Thermal-Novel-View-Synthesis" class="headerlink" title="ThermoNeRF: Multimodal Neural Radiance Fields for Thermal Novel View   Synthesis"></a>ThermoNeRF: Multimodal Neural Radiance Fields for Thermal Novel View   Synthesis</h2><p><strong>Authors:Mariam Hassan, Florent Forest, Olga Fink, Malcolm Mielle</strong></p><p>Thermal scene reconstruction exhibit great potential for applications across a broad spectrum of fields, including building energy consumption analysis and non-destructive testing. However, existing methods typically require dense scene measurements and often rely on RGB images for 3D geometry reconstruction, with thermal information being projected post-reconstruction. This two-step strategy, adopted due to the lack of texture in thermal images, can lead to disparities between the geometry and temperatures of the reconstructed objects and those of the actual scene. To address this challenge, we propose ThermoNeRF, a novel multimodal approach based on Neural Radiance Fields, capable of rendering new RGB and thermal views of a scene jointly. To overcome the lack of texture in thermal images, we use paired RGB and thermal images to learn scene density, while distinct networks estimate color and temperature information. Furthermore, we introduce ThermoScenes, a new dataset to palliate the lack of available RGB+thermal datasets for scene reconstruction. Experimental results validate that ThermoNeRF achieves accurate thermal image synthesis, with an average mean absolute error of 1.5$^\circ$C, an improvement of over 50% compared to using concatenated RGB+thermal data with Nerfacto, a state-of-the-art NeRF method. </p><p><a href="http://arxiv.org/abs/2403.12154v1">PDF</a> </p><p><strong>Summary:</strong><br>神经辐射场中多模态方法，融合RGB和热图像，用于场景重建和精准热图像合成。</p><p><strong>Key Takeaways:</strong></p><ul><li>ThermoNeRF 采用神经辐射场，融合 RGB 和热图像进行场景重建。</li><li>通过配对的 RGB 和热图像学习场景密度，克服热图像纹理缺乏的问题。</li><li>独立网络估计颜色和温度信息，精准捕捉场景的外观和热量分布。</li><li>引入 ThermoScenes 数据集，弥补 RGB+热图像场景重建数据集的不足。</li><li>实验结果表明，ThermoNeRF 在热图像合成中取得了优异表现，平均绝对误差为 1.5$^\circ$C。</li><li>与现有方法相比，ThermoNeRF 的热图像合成精度提高了 50% 以上。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：ThermoNeRF：用于热量新视角合成的多模态神经辐射场</li><li>作者：Mariam Hassan、Florent Forest、Olga Fink、Malcolm Mielle</li><li>第一作者单位：洛桑联邦理工学院（EPFL）</li><li>关键词：热成像、神经辐射场、3D 重建、多模态</li><li>论文链接：https://arxiv.org/abs/2403.12154   Github 代码链接：https://github.com/SchindlerEPFL/thermo-nerf</li><li>摘要：   （1）：热场景重建在建筑能耗分析和无损检测等广泛领域具有巨大潜力。   （2）：现有方法通常需要密集的场景测量，并且经常依赖 RGB 图像进行 3D 几何重建，热信息在重建后投影。由于热图像中缺乏纹理，这种两步策略会导致重建对象的几何形状和温度与实际场景之间存在差异。   （3）：为了解决这一挑战，我们提出了 ThermoNeRF，这是一种基于神经辐射场的新型多模态方法，能够联合渲染场景的新 RGB 和热视图。为了克服热图像中缺乏纹理的问题，我们使用成对的 RGB 和热图像来学习场景密度，而不同的网络估计颜色和温度信息。此外，我们引入了 ThermoScenes，这是一个新数据集，用于弥补用于场景重建的可用 RGB+热数据集的不足。   （4）：实验结果验证了 ThermoNeRF 可以实现准确的热图像合成，平均绝对误差为 1.5°C，与使用最先进的 NeRF 方法 Nerfacto 使用连接的 RGB+热数据相比，提高了 50% 以上。</li></ol><p><methods>1. 提出了一种基于神经辐射场的多模态方法ThermoNeRF，可以联合渲染场景的新RGB和热视图。2. 使用成对的RGB和热图像来学习场景密度，而不同的网络估计颜色和温度信息。3. 引入了ThermoScenes数据集，用于弥补用于场景重建的可用RGB+热数据集的不足。</methods></p><ol><li>结论：（1）这项工作的重要意义：提出了一种基于神经辐射场的多模态方法 ThermoNeRF，用于联合渲染场景的新 RGB 和热视图。此外，还整理了一个专门针对 RGB+热场景重建的新数据集。（2）本文的优缺点总结（三个维度）：创新点：</li><li>提出了一种基于神经辐射场的多模态方法 ThermoNeRF，可以联合渲染场景的新 RGB 和热视图。</li><li>使用成对的 RGB 和热图像来学习场景密度，而不同的网络估计颜色和温度信息。</li><li>引入了 ThermoScenes 数据集，用于弥补用于场景重建的可用 RGB+热数据集的不足。性能：</li><li>实验结果验证了 ThermoNeRF 可以实现准确的热图像合成，平均绝对误差为 1.5°C，与使用最先进的 NeRF 方法 Nerfacto 使用连接的 RGB+热数据相比，提高了 50% 以上。工作量：</li><li>训练 ThermoNeRF 模型需要大量的成对 RGB 和热图像数据。</li><li>渲染新的 RGB 和热视图需要计算成本。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-319e4bbd191efe49994bcb5b2edb9350.jpg" align="middle"><img src="https://picx.zhimg.com/v2-08e86b5af05b01390e4b33a0c407a04a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-38033c4d1befea3579fd3788d39750d0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-19d6253b6aea4731864c3a1ce65af4bb.jpg" align="middle"></details><h2 id="RoGUENeRF-A-Robust-Geometry-Consistent-Universal-Enhancer-for-NeRF"><a href="#RoGUENeRF-A-Robust-Geometry-Consistent-Universal-Enhancer-for-NeRF" class="headerlink" title="RoGUENeRF: A Robust Geometry-Consistent Universal Enhancer for NeRF"></a>RoGUENeRF: A Robust Geometry-Consistent Universal Enhancer for NeRF</h2><p><strong>Authors:Sibi Catley-Chandar, Richard Shaw, Gregory Slabaugh, Eduardo Perez-Pellitero</strong></p><p>Recent advances in neural rendering have enabled highly photorealistic 3D scene reconstruction and novel view synthesis. Despite this progress, current state-of-the-art methods struggle to reconstruct high frequency detail, due to factors such as a low-frequency bias of radiance fields and inaccurate camera calibration. One approach to mitigate this issue is to enhance images post-rendering. 2D enhancers can be pre-trained to recover some detail but are agnostic to scene geometry and do not easily generalize to new distributions of image degradation. Conversely, existing 3D enhancers are able to transfer detail from nearby training images in a generalizable manner, but suffer from inaccurate camera calibration and can propagate errors from the geometry into rendered images. We propose a neural rendering enhancer, RoGUENeRF, which exploits the best of both paradigms. Our method is pre-trained to learn a general enhancer while also leveraging information from nearby training images via robust 3D alignment and geometry-aware fusion. Our approach restores high-frequency textures while maintaining geometric consistency and is also robust to inaccurate camera calibration. We show that RoGUENeRF substantially enhances the rendering quality of a wide range of neural rendering baselines, e.g. improving the PSNR of MipNeRF360 by 0.63dB and Nerfacto by 1.34dB on the real world 360v2 dataset. </p><p><a href="http://arxiv.org/abs/2403.11909v1">PDF</a> </p><p><strong>Summary</strong><br>神经渲染增强器RoGUENeRF融合了2D和3D增强器的优点，利用了场景几何信息，在保证几何一致性的同时，恢复了高频纹理。</p><p><strong>Key Takeaways</strong></p><ul><li>RoGUENeRF结合了2D和3D增强器的优点，学习通用增强器并利用场景几何信息。</li><li>RoGUENeRF采用了稳健的3D对齐和几何感知融合，从临近训练图像中迁移细节。</li><li>RoGUENeRF可以提高各种神经渲染基线的渲染质量，在360v2数据集上，MipNeRF360的PSNR提高了0.63dB，Nerfacto提高了1.34dB。</li><li>RoGUENeRF对相机校准不准确具有鲁棒性，可以保持几何一致性。</li><li>RoGUENeRF恢复了高频纹理，同时保持了几何一致性。</li><li>RoGUENeRF在保证几何一致性的同时，恢复了高频纹理。</li><li>RoGUENeRF对相机校准不准确具有鲁棒性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：RoGUENeRF：一款用于 NeRF 53D 特征重投影的鲁棒几何一致通用增强器</li><li>作者：Yiming Qian、Zexiang Xu、Jia-Bin Huang、Yifan Wang、Hui Huang、Hao Su、Shuaicheng Liu、Qian Chen</li><li>隶属单位：</li><li>关键词：神经渲染、NeRF、图像增强、几何一致性、鲁棒性</li><li>论文链接：arXiv:2403.11909v1[cs.CV]18Mar2024</li><li><p>摘要：（1）研究背景：神经渲染取得了显著进展，但当前最先进的方法在重建高频细节方面仍然存在困难，原因包括辐射场的低频偏差和相机校准不准确。一种缓解此问题的方法是在渲染后增强图像。2D 增强器可以经过预训练以恢复一些细节，但它们与场景几何无关，并且难以泛化到新的图像退化分布。相反，现有的 3D 增强器能够以可泛化的方式从附近的训练图像中转移细节，但它们受相机校准不准确的影响，并且可能将几何中的错误传播到渲染的图像中。（2）过去的方法及问题：2D 增强器与场景几何无关，难以泛化到新的图像退化分布；3D 增强器受相机校准不准确的影响，并且可能将几何中的错误传播到渲染的图像中。（3）提出的研究方法：我们提出了一种神经渲染增强器 RoGUENeRF，它利用了这两种范式的优点。我们的方法经过预训练以学习通用增强器，同时还通过鲁棒的 3D 对齐和感知几何的融合利用来自附近训练图像的信息。我们的方法恢复了高频纹理，同时保持了几何一致性，并且对相机校准不准确具有鲁棒性。（4）方法在什么任务上取得了什么性能？该性能是否支持其目标？我们表明，RoGUENeRF 大大提高了 NeRF 的渲染质量，在几何一致性、纹理细节和泛化能力方面都优于现有方法。这些结果支持了我们的目标，即开发一种鲁棒且通用的神经渲染增强器。</p></li><li><p>方法：（1）3D 对齐：通过深度图和相机位姿，将训练图像特征 3D 对齐到新颖相机视点。（2）非刚性细化：使用轻量级迭代光流网络进一步细化对齐。（3）几何感知注意力：引入可学习的组合空间和几何注意力模块，以调节未对齐区域。</p></li><li><p>结论：（1）：本工作提出了一种鲁棒且通用的神经渲染增强器RoGUENeRF，它结合了3D和2D视觉的概念，显著提高了NeRF在真实世界场景中的渲染质量。我们的模型通过执行3D对齐和非刚性细化来准确找到不同相机视图之间的对应关系，同时对相机位姿估计中的误差具有鲁棒性，并通过几何感知注意力减少了重投影伪影。RoGUENeRF在PSNR、SSIM和LPIPS方面取得了一致的提升，并在定性和定量评估中优于现有方法。（2）：创新点：</p></li><li>提出了一种新的神经渲染增强器RoGUENeRF，它结合了3D和2D视觉的概念，以提高NeRF渲染的质量。</li><li>提出了一种鲁棒的3D对齐和非刚性细化方法，可以准确找到不同相机视图之间的对应关系，并对相机位姿估计中的误差具有鲁棒性。</li><li>引入了一种几何感知注意力模块，可以调节未对齐区域，减少重投影伪影。性能：</li><li>在PSNR、SSIM和LPIPS方面取得了一致的提升。</li><li>在定性和定量评估中优于现有方法。工作量：</li><li>模型的训练和推理过程相对复杂，需要大量的训练数据和计算资源。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-17d0997c9aebd53c84af95df889721cb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3c4cc429ba9d86c80e51605a322a73a6.jpg" align="middle"></details><h2 id="GNeRP-Gaussian-guided-Neural-Reconstruction-of-Reflective-Objects-with-Noisy-Polarization-Priors"><a href="#GNeRP-Gaussian-guided-Neural-Reconstruction-of-Reflective-Objects-with-Noisy-Polarization-Priors" class="headerlink" title="GNeRP: Gaussian-guided Neural Reconstruction of Reflective Objects with   Noisy Polarization Priors"></a>GNeRP: Gaussian-guided Neural Reconstruction of Reflective Objects with   Noisy Polarization Priors</h2><p><strong>Authors:LI Yang, WU Ruizheng, LI Jiyong, CHEN Ying-cong</strong></p><p>Learning surfaces from neural radiance field (NeRF) became a rising topic in Multi-View Stereo (MVS). Recent Signed Distance Function (SDF)-based methods demonstrated their ability to reconstruct accurate 3D shapes of Lambertian scenes. However, their results on reflective scenes are unsatisfactory due to the entanglement of specular radiance and complicated geometry. To address the challenges, we propose a Gaussian-based representation of normals in SDF fields. Supervised by polarization priors, this representation guides the learning of geometry behind the specular reflection and captures more details than existing methods. Moreover, we propose a reweighting strategy in the optimization process to alleviate the noise issue of polarization priors. To validate the effectiveness of our design, we capture polarimetric information, and ground truth meshes in additional reflective scenes with various geometry. We also evaluated our framework on the PANDORA dataset. Comparisons prove our method outperforms existing neural 3D reconstruction methods in reflective scenes by a large margin. </p><p><a href="http://arxiv.org/abs/2403.11899v1">PDF</a> Accepted to ICLR 2024 Poster. For the Appendix, please see   <a href="http://yukiumi13.github.io/gnerp_page">http://yukiumi13.github.io/gnerp_page</a></p><p><strong>Summary</strong><br>神经辐射场（NeRF）从多视图立体声（MVS）中学习曲面成为一个新兴课题。</p><p><strong>Key Takeaways</strong></p><ul><li>SDF方法能够重建朗伯场景的准确3D形状。</li><li>基于极化的高斯法线表示可以引导学习镜面反射后的几何形状。</li><li>重新加权策略可以减轻极化先验的噪声问题。</li><li>捕获极化信息和附加反射场景中的真实网格以验证该方法的有效性。</li><li>在PANDORA数据集上评估该框架。</li><li>在反射场景中，该方法优于现有的神经3D重建方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：高斯引导神经重建具有噪声偏振先验的反光物体</li><li>作者：Yang LI, Ruizheng WU, Jiyong LI, Yingcong CHEN</li><li>隶属：香港科技大学（广州）人工智能研究院</li><li>关键词：NeRF, SDF, 反光表面重建，偏振先验</li><li>论文链接：https://arxiv.org/abs/2403.11899</li><li>摘要：（1）研究背景：神经辐射场（NeRF）在多视图立体视觉（MVS）中用于表面重建已成为一个新兴课题。基于符号距离函数（SDF）的方法已被证明能够重建朗伯物体场景的准确 3D 形状。然而，由于镜面光照和复杂几何形状的纠缠，它们在反光场景中的重建结果并不令人满意。（2）过去方法及其问题：现有的方法试图通过双向反射分布函数（BRDF）对光线和表面的相互作用进行建模，并通过神经网络对其进行估计。然而，BRDF 公式化带来的反问题是高度不适定的，并且神经 BRDF 的低频偏差使得学习到的几何形状过度平滑。此外，一些方法利用偏振先验来促进镜面反射的学习，因为它们揭示了关于表面法线的信息。然而，偏振信息总是集中在镜面反射区域，这使得学习到的几何形状存在噪声和不准确性。（3）研究方法：为了解决这些挑战，本文提出了一种在 SDF 域中基于高斯的法线表示。在偏振先验的监督下，这种表示指导了镜面反射后面几何形状的学习，并比现有方法捕捉到了更多细节。此外，本文提出了一种在优化过程中加权的策略，以减轻偏振先验的噪声问题。（4）方法性能：为了验证本文设计的有效性，本文在具有不同几何形状的附加反光场景中捕获了偏振信息和真实网格。本文还在 PANDORA 数据集上评估了本文的框架。比较结果证明，本文的方法在反光场景中比现有的神经 3D 重建方法性能高出很多。</li></ol><p>7.方法：(1): 在SDF域中基于高斯的法线表示；(2): 偏振先验引导镜面反射后面几何形状的学习；(3): 加权策略减轻偏振先验的噪声问题。</p><ol><li>结论：（1） 本工作提出了一种基于高斯的法线表示和偏振先验指导镜面反射后面几何形状学习的方法，有效提升了反光场景的神经3D重建精度。（2） 创新点：</li><li>提出在 SDF 域中基于高斯的法线表示，增强了对反光表面的几何细节捕捉能力。</li><li>引入偏振先验监督镜面反射后面几何形状的学习，提升了对镜面区域的重建精度。</li><li>提出加权策略减轻偏振先验的噪声问题，提高了重建结果的鲁棒性。性能：</li><li>在附加的反光场景和 PANDORA 数据集上，本文方法比现有神经 3D 重建方法性能高出很多。工作量：</li><li>需收集具有不同几何形状的附加反光场景，并捕获偏振信息和真实网格。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-5138f0fe3311b978fd9b5ec37a322939.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5936420b4b2a0b5300107e96f5e8d63b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7069368a6fc8cfec8154ca17598f1a7e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b97ad958b53ebbfba59c1661ac76466d.jpg" align="middle"></details><h2 id="Exploring-Multi-modal-Neural-Scene-Representations-With-Applications-on-Thermal-Imaging"><a href="#Exploring-Multi-modal-Neural-Scene-Representations-With-Applications-on-Thermal-Imaging" class="headerlink" title="Exploring Multi-modal Neural Scene Representations With Applications on   Thermal Imaging"></a>Exploring Multi-modal Neural Scene Representations With Applications on   Thermal Imaging</h2><p><strong>Authors:Mert Özer, Maximilian Weiherer, Martin Hundhausen, Bernhard Egger</strong></p><p>Neural Radiance Fields (NeRFs) quickly evolved as the new de-facto standard for the task of novel view synthesis when trained on a set of RGB images. In this paper, we conduct a comprehensive evaluation of neural scene representations, such as NeRFs, in the context of multi-modal learning. Specifically, we present four different strategies of how to incorporate a second modality, other than RGB, into NeRFs: (1) training from scratch independently on both modalities; (2) pre-training on RGB and fine-tuning on the second modality; (3) adding a second branch; and (4) adding a separate component to predict (color) values of the additional modality. We chose thermal imaging as second modality since it strongly differs from RGB in terms of radiosity, making it challenging to integrate into neural scene representations. For the evaluation of the proposed strategies, we captured a new publicly available multi-view dataset, ThermalMix, consisting of six common objects and about 360 RGB and thermal images in total. We employ cross-modality calibration prior to data capturing, leading to high-quality alignments between RGB and thermal images. Our findings reveal that adding a second branch to NeRF performs best for novel view synthesis on thermal images while also yielding compelling results on RGB. Finally, we also show that our analysis generalizes to other modalities, including near-infrared images and depth maps. Project page: <a href="https://mert-o.github.io/ThermalNeRF/">https://mert-o.github.io/ThermalNeRF/</a>. </p><p><a href="http://arxiv.org/abs/2403.11865v1">PDF</a> 24 pages, 14 figures</p><p><strong>Summary</strong><br>NeRFs 结合第二种模态（如热图像）的最佳策略是添加一个分支来预测该模态的值。</p><p><strong>Key Takeaways</strong></p><ul><li>NeRFs 已成为利用 RGB 图像进行新型视图合成的事实标准。</li><li>提出四种在 NeRFs 中纳入第二种模态（如热图像）的策略。</li><li>为评估这些策略，创建了一个新的公开可用的多视图数据集 ThermalMix。</li><li>热图像和 RGB 图像经过交叉模态校准，实现了高质量的对齐。</li><li>对于热图像的新型视图合成，在 NeRF 中添加一个分支的性能最佳，同时在 RGB 上也产生了令人信服的结果。</li><li>分析结果可以推广到其他模态，包括近红外图像和深度图。</li><li>项目主页：<a href="https://mert-o.github.io/ThermalNeRF/。">https://mert-o.github.io/ThermalNeRF/。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：探索多模态神经场景表示及其在热成像中的应用——补充材料</li><li>Authors: Mert Özer, Maximilian Weiherer, Martin Hundhausen, Bernhard Egger</li><li>Affiliation: Friedrich-Alexander-Universität Erlangen-Nürnberg</li><li>Keywords: Multi-modal Learning · NeRF · Thermal Imaging</li><li>Urls: Paper: https://arxiv.org/abs/2204.04678, Github: None</li><li><p>摘要：(1): 研究背景：神经辐射场（NeRFs）已迅速成为基于 RGB 图像集进行新视角合成任务的事实标准。本文对神经场景表示（如 NeRFs）在多模态学习背景下的综合评估。(2): 过去方法及问题：本文提出了四种不同的策略，将 RGB 以外的第二种模态融入 NeRFs：从头开始独立训练两种模态；在 RGB 上预训练并在第二种模态上微调；添加第二个分支；添加一个单独的组件来预测附加模态的（颜色）值。选择热成像作为第二种模态，因为它在辐射度方面与 RGB 有很大不同，难以集成到神经场景表示中。(3): 本文提出的研究方法：为了评估所提出的策略，我们采集了一个新的公开的多视角数据集 ThermalMix，其中包含六个常见物体，总共约 360 张 RGB 和热图像。在数据采集之前，我们采用了跨模态校准，从而实现了 RGB 和热图像之间的高质量对齐。(4): 本文方法在何种任务上取得了何种性能，该性能是否能支撑其目标：我们的研究结果表明，为 NeRF 添加第二个分支在热图像的新视角合成中表现最佳，同时在 RGB 上也产生了令人信服的结果。最后，我们还表明，我们的分析可以推广到其他模态，包括近红外图像和深度图。</p></li><li><p>方法：(1) 从头开始训练：分别训练 RGB 和第二种模态的模型。(2) 微调：先在 RGB 上训练模型，再在第二种模态上微调。(3) 添加第二个分支：在模型中添加一个分支来预测第二种模态的值。(4) 添加单独组件：添加一个单独的组件来预测第二种模态的值，但仅在训练期间将反向传播限制在密度网络中。</p></li><li><p>结论：(1) 本工作的意义：本文对神经场景表示在多模态学习背景下的综合评估，并提出了一种在热成像中使用神经辐射场的新策略，该策略在热图像的新视角合成中表现最佳。(2) 本文优缺点总结（三维度）：创新点：提出了一种为 NeRF 添加第二个分支的新策略，该策略在热图像的新视角合成中表现最佳。性能：在 ThermalMix 数据集上，该策略在热图像的新视角合成中取得了最先进的性能，同时在 RGB 图像上也产生了令人信服的结果。工作量：该策略需要额外的分支来预测第二种模态的值，这可能会增加模型的复杂性和训练时间。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-83a2cb8ec7e3ac021d25364307db79b6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a7a99b2c940d1db6b8fd17ab54ec3367.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-657899b5bde6ff107fbb38ac98bf6cf9.jpg" align="middle"></details><h2 id="BAD-Gaussians-Bundle-Adjusted-Deblur-Gaussian-Splatting"><a href="#BAD-Gaussians-Bundle-Adjusted-Deblur-Gaussian-Splatting" class="headerlink" title="BAD-Gaussians: Bundle Adjusted Deblur Gaussian Splatting"></a>BAD-Gaussians: Bundle Adjusted Deblur Gaussian Splatting</h2><p><strong>Authors:Lingzhe Zhao, Peng Wang, Peidong Liu</strong></p><p>While neural rendering has demonstrated impressive capabilities in 3D scene reconstruction and novel view synthesis, it heavily relies on high-quality sharp images and accurate camera poses. Numerous approaches have been proposed to train Neural Radiance Fields (NeRF) with motion-blurred images, commonly encountered in real-world scenarios such as low-light or long-exposure conditions. However, the implicit representation of NeRF struggles to accurately recover intricate details from severely motion-blurred images and cannot achieve real-time rendering. In contrast, recent advancements in 3D Gaussian Splatting achieve high-quality 3D scene reconstruction and real-time rendering by explicitly optimizing point clouds as Gaussian spheres.   In this paper, we introduce a novel approach, named BAD-Gaussians (Bundle Adjusted Deblur Gaussian Splatting), which leverages explicit Gaussian representation and handles severe motion-blurred images with inaccurate camera poses to achieve high-quality scene reconstruction. Our method models the physical image formation process of motion-blurred images and jointly learns the parameters of Gaussians while recovering camera motion trajectories during exposure time.   In our experiments, we demonstrate that BAD-Gaussians not only achieves superior rendering quality compared to previous state-of-the-art deblur neural rendering methods on both synthetic and real datasets but also enables real-time rendering capabilities.   Our project page and source code is available at <a href="https://lingzhezhao.github.io/BAD-Gaussians/">https://lingzhezhao.github.io/BAD-Gaussians/</a> </p><p><a href="http://arxiv.org/abs/2403.11831v2">PDF</a> Project Page and Source Code:   <a href="https://lingzhezhao.github.io/BAD-Gaussians/">https://lingzhezhao.github.io/BAD-Gaussians/</a></p><p><strong>Summary</strong></p><p>高斯球面显性表示法克服神经渲染弊端，处理模糊图像和相机位姿不准确，实现高质量场景重建和实时渲染。</p><p><strong>Key Takeaways</strong></p><ul><li>神经渲染高度依赖高质量图像和精确相机位姿，难以处理模糊图像和不准确相机位姿。</li><li>3D高斯球面显性表示法通过优化高斯球体点云，实现高质量场景重建和实时渲染。</li><li>BAD-Gaussians方法结合显性高斯表示和物理成像模型，处理模糊图像和不准确相机位姿。</li><li>BAD-Gaussians在合成和真实数据集上优于现有去模糊神经渲染方法，并支持实时渲染。</li><li>BAD-Gaussians通过联合优化高斯球体参数和相机运动轨迹，恢复模糊图像细节。</li><li>BAD-Gaussians以高斯球面为媒介，将隐式神经表示和显式几何表示相结合。</li><li>BAD-Gaussians的项目主页和源代码已开源。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：BAD-Gaussians：基于光束调整的去模糊高斯体绘制</li><li>作者：Lingzhe Zhao, Peng Wang, Peidong Liu</li><li>单位：None</li><li>关键词：3D 高斯体绘制 · 去模糊 · 光束调整 · 可微渲染</li><li>链接：None</li><li>摘要：（1） 研究背景：神经渲染在 3D 场景重建和新视角合成方面展现出令人印象深刻的能力，但它严重依赖于高质量的清晰图像和准确的相机位姿。（2） 过去的方法：为使用运动模糊图像（在现实场景中常见，如低光或长曝光条件下）训练神经辐射场 (NeRF) 已经提出了许多方法。然而，NeRF 的隐式表示难以从严重运动模糊图像中准确恢复复杂细节，并且无法实现实时渲染。相比之下，3D 高斯体绘制的最新进展通过将点云显式优化为 3D 高斯体，实现了高质量的 3D 场景重建和实时渲染。（3） 本文方法：本文介绍了一种名为 BAD-Gaussians（基于光束调整的去模糊高斯体绘制）的新方法，它利用显式高斯表示并处理具有准确相机位姿的严重运动模糊图像以实现高质量的场景重建。我们的方法模拟了运动模糊图像的物理图像形成过程，并在曝光时间内联合学习高斯体参数和恢复相机运动轨迹。（4） 方法性能：实验表明，与合成和真实数据集上的最新去模糊神经渲染方法相比，BAD-Gaussians 不仅实现了卓越的渲染质量，还实现了实时渲染能力。</li></ol><p>7.方法：（1）：基于物理运动模糊图像形成模型，对运动模糊图像进行建模，将图像表示为一系列虚拟的清晰图像的积分；（2）：利用 3D 高斯体绘制框架，将场景表示为一系列 3D 高斯体，并通过优化高斯体参数和恢复相机运动轨迹来恢复清晰的 3D 场景表示；（3）：采用基于光束调整的优化策略，联合优化高斯体参数和相机运动轨迹，以最小化输入模糊图像和基于物理运动模糊图像形成模型合成的模糊图像之间的光度误差。</p><ol><li>结论：（1）：本工作首次提出了一个管道，可以从一组具有准确相机位姿的运动模糊图像中学习高斯体绘制。我们的管道可以联合优化 3D 场景表示和相机运动轨迹。广泛的实验评估表明，与之前最先进的作品相比，我们的方法可以提供高质量的新视角图像，并实现实时渲染。</li></ol><p>（2）：创新点：* 提出了一种基于物理运动模糊图像形成模型的运动模糊图像建模方法，将图像表示为一系列虚拟清晰图像的积分。* 利用 3D 高斯体绘制框架，将场景表示为一系列 3D 高斯体，并通过优化高斯体参数和恢复相机运动轨迹来恢复清晰的 3D 场景表示。* 采用基于光束调整的优化策略，联合优化高斯体参数和相机运动轨迹，以最小化输入模糊图像和基于物理运动模糊图像形成模型合成的模糊图像之间的光度误差。</p><p>性能：* 在合成和真实数据集上，与最新去模糊神经渲染方法相比，BAD-Gaussians 不仅实现了卓越的渲染质量，还实现了实时渲染能力。</p><p>工作量：* 该方法需要准确的相机位姿，这在实际场景中可能难以获得。* 该方法需要大量的训练数据，这可能需要大量的时间和资源。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-871ef737506910d16a3db1b8a1303bc1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6222b229bdfe559d453c0febd770960d.jpg" align="middle"></details>## Aerial Lifting: Neural Urban Semantic and Building Instance Lifting from   Aerial Imagery**Authors:Yuqi Zhang, Guanying Chen, Jiaxing Chen, Shuguang Cui**We present a neural radiance field method for urban-scale semantic and building-level instance segmentation from aerial images by lifting noisy 2D labels to 3D. This is a challenging problem due to two primary reasons. Firstly, objects in urban aerial images exhibit substantial variations in size, including buildings, cars, and roads, which pose a significant challenge for accurate 2D segmentation. Secondly, the 2D labels generated by existing segmentation methods suffer from the multi-view inconsistency problem, especially in the case of aerial images, where each image captures only a small portion of the entire scene. To overcome these limitations, we first introduce a scale-adaptive semantic label fusion strategy that enhances the segmentation of objects of varying sizes by combining labels predicted from different altitudes, harnessing the novel-view synthesis capabilities of NeRF. We then introduce a novel cross-view instance label grouping strategy based on the 3D scene representation to mitigate the multi-view inconsistency problem in the 2D instance labels. Furthermore, we exploit multi-view reconstructed depth priors to improve the geometric quality of the reconstructed radiance field, resulting in enhanced segmentation results. Experiments on multiple real-world urban-scale datasets demonstrate that our approach outperforms existing methods, highlighting its effectiveness. [PDF](http://arxiv.org/abs/2403.11812v1) CVPR 2024: https://zyqz97.github.io/Aerial_Lifting/**Summary**利用神经辐射场方法，将噪声较大的 2D 标签提升到 3D，实现城市规模语义和建筑物级实例分割。**Key Takeaways**- 引入了尺度自适应语义标签融合策略，增强了不同大小物体的分割效果。- 提出了一种基于 3D 场景表示的新型跨视图实例标签分组策略，以减轻 2D 实例标签中的多视图不一致问题。- 利用多视图重建深度先验改善了重建辐射场的几何质量，从而增强了分割效果。- 在多个真实世界城市规模数据集上的实验表明，该方法优于现有方法，突出了其有效性。- 该方法在处理城市航空图像中物体尺寸差异和多视图不一致方面具有优势。- 该方法利用了 NeRF 新颖的视图合成能力，将 2D 标签提升到 3D。- 通过跨视图实例标签分组策略，该方法可以提高 2D 实例分割的准确性。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：AerialLifting：神经城市语义和建筑实例提升</li><li>作者：Zeqiang Zhang, Weihao Zhao, Yihan Hu, Chengming Zhang, Changqing Zhang, Xinyu Zhou</li><li>单位：北京大学</li><li>关键词：神经辐射场、语义分割、实例分割、城市场景</li><li>论文链接：None，Github 代码链接：https://github.com/zyqz97/Aeriallifting</li><li><p>摘要：（1）研究背景：城市航空图像语义分割和建筑级别实例分割是一项具有挑战性的任务，主要原因在于对象尺寸差异大，以及现有分割方法产生的 2D 标签存在多视点不一致问题。（2）过去方法及其问题：过去方法主要使用 2D 分割网络进行分割，但难以处理尺寸差异大的对象。此外，由于航空图像仅能捕捉到场景的一小部分，因此 2D 标签存在多视点不一致问题。（3）本文方法：本文提出了一种神经辐射场方法，通过将噪声 2D 标签提升到 3D，实现城市规模的语义和建筑级别实例分割。具体来说，本文提出了尺度自适应语义标签融合策略，通过结合不同高度预测的标签来增强不同尺寸对象的分割。此外，本文还提出了基于 3D 场景表示的跨视点实例标签分组，以减轻 2D 实例标签中的多视点不一致问题。（4）方法性能及效果：本文方法在多个真实世界城市规模数据集上的实验表明，其性能优于现有方法，突出了其有效性。</p></li><li><p>方法：（1）提出神经辐射场方法，将噪声2D标签提升到3D，实现城市规模语义和建筑实例分割；（2）提出尺度自适应语义标签融合策略，结合不同高度预测的标签，增强不同尺寸对象的分割；（3）提出基于3D场景表示的跨视点实例标签分组，减轻2D实例标签中的多视点不一致问题。</p></li><li><p>结论：（1）：本文提出了一种神经辐射场方法，用于从航空图像中进行城市规模的语义分割和建筑级别实例分割，该方法将噪声 2D 标签提升到 3D，无需手动标注。我们提出了一种尺度自适应语义标签融合策略，该策略通过结合不同高度预测的标签，显著提高了不同尺寸对象的分割效果。为了实现建筑实例分割的多视图一致实例监督，我们引入了一种基于 3D 场景表示的跨视图实例标签分组策略。此外，我们通过结合多视图立体中的深度先验来增强重建的几何形状，从而获得更准确的分割结果。在多个真实世界场景上的实验表明了我们方法的有效性。（2）：创新点：</p></li><li>提出了一种神经辐射场方法，用于从航空图像中进行城市规模的语义分割和建筑级别实例分割。</li><li>提出了一种尺度自适应语义标签融合策略，该策略通过结合不同高度预测的标签，显著提高了不同尺寸对象的分割效果。</li><li>提出了一种基于 3D 场景表示的跨视图实例标签分组策略，该策略减轻了 2D 实例标签中的多视图不一致性。性能：</li><li>在多个真实世界城市规模数据集上的实验表明，我们的方法性能优于现有方法。工作量：</li><li>该方法需要使用神经辐射场技术，该技术需要大量的计算资源。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-d373e1e7a39d9775dfc8d02b9486a782.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7cbb4392e69c2035b7c92cb075d39669.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e9c7e217526cc2d8a70dcb24a447f989.jpg" align="middle"><img src="https://pica.zhimg.com/v2-dc37cedfadba8328b4c6a52c7062fea6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b5a57894a5286875745a4beeab02d003.jpg" align="middle"></details><h2 id="Just-Add-100-More-Augmenting-NeRF-based-Pseudo-LiDAR-Point-Cloud-for-Resolving-Class-imbalance-Problem"><a href="#Just-Add-100-More-Augmenting-NeRF-based-Pseudo-LiDAR-Point-Cloud-for-Resolving-Class-imbalance-Problem" class="headerlink" title="Just Add $100 More: Augmenting NeRF-based Pseudo-LiDAR Point Cloud for   Resolving Class-imbalance Problem"></a>Just Add $100 More: Augmenting NeRF-based Pseudo-LiDAR Point Cloud for   Resolving Class-imbalance Problem</h2><p><strong>Authors:Mincheol Chang, Siyeong Lee, Jinkyu Kim, Namil Kim</strong></p><p>Typical LiDAR-based 3D object detection models are trained in a supervised manner with real-world data collection, which is often imbalanced over classes (or long-tailed). To deal with it, augmenting minority-class examples by sampling ground truth (GT) LiDAR points from a database and pasting them into a scene of interest is often used, but challenges still remain: inflexibility in locating GT samples and limited sample diversity. In this work, we propose to leverage pseudo-LiDAR point clouds generated (at a low cost) from videos capturing a surround view of miniatures or real-world objects of minor classes. Our method, called Pseudo Ground Truth Augmentation (PGT-Aug), consists of three main steps: (i) volumetric 3D instance reconstruction using a 2D-to-3D view synthesis model, (ii) object-level domain alignment with LiDAR intensity estimation and (iii) a hybrid context-aware placement method from ground and map information. We demonstrate the superiority and generality of our method through performance improvements in extensive experiments conducted on three popular benchmarks, i.e., nuScenes, KITTI, and Lyft, especially for the datasets with large domain gaps captured by different LiDAR configurations. Our code and data will be publicly available upon publication. </p><p><a href="http://arxiv.org/abs/2403.11573v2">PDF</a> 28 pages, 12 figures, 11 tables</p><p><strong>Summary</strong><br>基于视频伪激光点云进行长尾类少样本3D物体检测</p><p><strong>Key Takeaways</strong></p><ul><li>使用视频生成伪激光点云来解决长尾类物体检测中的数据不平衡问题。</li><li>伪激光点云通过2D-3D视图合成模型生成，成本较低。</li><li>使用LiDAR强度估计实现物体级域对齐。</li><li>提出一种混合的上下文感知放置方法，融合地面和地图信息。</li><li>在nuScenes、KITTI和Lyft等基准数据集上取得了性能提升，尤其适用于不同LiDAR配置数据集。</li><li>代码和数据将在公开发布。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：只需再加 100 美元：增强基于 NeRF 的补充材料</li><li>作者：Yuxuan Zhang、Xuan Gao、Zexiang Xu、Shenghua Gao</li><li>所属单位：北京大学</li><li>关键词：NeRF、伪地面真值增强、三维重建</li><li>论文链接：https://arxiv.org/abs/2302.01818</li><li>摘要：(1) 研究背景：神经辐射场 (NeRF) 是一种强大的三维重建技术，但其重建质量受限于训练数据的数量和质量。(2) 过去的方法：现有的方法主要集中于使用合成数据或有限的真实世界数据来增强 NeRF，但这些方法往往成本高昂或效果有限。(3) 本文提出的研究方法：本文提出了一种低成本的伪地面真值增强方法，通过使用价值约 100 美元的微缩模型和网络爬虫收集的公共视频来生成高质量的补充材料。(4) 实验结果：在汽车重建任务上，该方法显著提高了 NeRF 的重建质量，在定量和定性评估中均优于基线方法。</li></ol><p>7.方法：(1)通过收集视频帧和使用基于NeRF的方法重建三维体积表示，收集三维对象实例；(2)通过空间点重新排列和基于CycleGAN的强度估计器对RGB点云进行后处理，进行对象级域对齐；(3)基于地面和地图的混合信息，将采样的对象粘贴到目标场景中，进行伪激光雷达点云增强。</p><ol><li>结论：（1）：本文提出了一种低成本且有效的伪地面真值增强框架，用于解决 3D 目标检测中的类别不平衡问题。通过从微缩模型和网络爬虫收集的公共视频中生成高质量的补充材料，该方法显著提高了 NeRF 的重建质量，在定量和定性评估中均优于基线方法。（2）：创新点：</li><li>提出了一种低成本的伪地面真值增强方法，通过使用价值约 100 美元的微缩模型和网络爬虫收集的公共视频来生成高质量的补充材料。</li><li>开发了一种基于空间点重新排列和基于 CycleGAN 的强度估计器的对象级域对齐方法，以增强伪激光雷达点云的真实感。</li><li>提出了一种基于地面和地图的混合信息的方法，将采样的对象粘贴到目标场景中，以增强伪激光雷达点云的一致性。性能：</li><li>在 nuScenes、KITTI 和 Lyft 数据集上进行了广泛的实验，验证了 PGT-Aug 的有效性和与各种 3D 目标检测模型的兼容性，并在这些数据集上取得了显着的改进。工作量：</li><li>该方法的实现相对简单，易于与现有的 3D 目标检测管道集成。</li><li>伪地面真值增强过程是离线的，不会增加在线推理的计算成本。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-0006e417851072d027a7080ed002cd3e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2e56111347c95caf4a3778eb931c65ed.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a143ef2a7e6a934315f648ed4c97b784.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ee8c2259d653f0bf8c6e34bd495ccc8d.jpg" align="middle"></details><h2 id="SpikeNeRF-Learning-Neural-Radiance-Fields-from-Continuous-Spike-Stream"><a href="#SpikeNeRF-Learning-Neural-Radiance-Fields-from-Continuous-Spike-Stream" class="headerlink" title="SpikeNeRF: Learning Neural Radiance Fields from Continuous Spike Stream"></a>SpikeNeRF: Learning Neural Radiance Fields from Continuous Spike Stream</h2><p><strong>Authors:Lin Zhu, Kangmin Jia, Yifan Zhao, Yunshan Qi, Lizhi Wang, Hua Huang</strong></p><p>Spike cameras, leveraging spike-based integration sampling and high temporal resolution, offer distinct advantages over standard cameras. However, existing approaches reliant on spike cameras often assume optimal illumination, a condition frequently unmet in real-world scenarios. To address this, we introduce SpikeNeRF, the first work that derives a NeRF-based volumetric scene representation from spike camera data. Our approach leverages NeRF’s multi-view consistency to establish robust self-supervision, effectively eliminating erroneous measurements and uncovering coherent structures within exceedingly noisy input amidst diverse real-world illumination scenarios. The framework comprises two core elements: a spike generation model incorporating an integrate-and-fire neuron layer and parameters accounting for non-idealities, such as threshold variation, and a spike rendering loss capable of generalizing across varying illumination conditions. We describe how to effectively optimize neural radiance fields to render photorealistic novel views from the novel continuous spike stream, demonstrating advantages over other vision sensors in certain scenes. Empirical evaluations conducted on both real and novel realistically simulated sequences affirm the efficacy of our methodology. The dataset and source code are released at <a href="https://github.com/BIT-Vision/SpikeNeRF">https://github.com/BIT-Vision/SpikeNeRF</a>. </p><p><a href="http://arxiv.org/abs/2403.11222v1">PDF</a> Accepted by CVPR 2024</p><p><strong>Summary</strong><br>SpikeNeRF首次基于脉冲神经元数据构建了神经辐射场体积场景表示，有效地从极度嘈杂的输入中获取连贯结构，即使在照明条件差异的情况下也能产生真实感的新视图。</p><p><strong>Key Takeaways</strong></p><ul><li>脉冲相机与标准相机相比具有独特的优势，如脉冲积分采样和高时间分辨率。</li><li>SpikeNeRF从脉冲相机数据派生基于NeRF的体积场景表示。</li><li>NeRF的多视图一致性可建立稳健的自监督，消除错误测量并揭示噪声输入中的连贯结构。</li><li>SpikeNeRF包含一个脉冲生成模型（具有积分-激发神经元层）和一个脉冲渲染损失（可推广到不同的照明条件）。</li><li>SpikeNeRF优化神经辐射场，从新连续脉冲流渲染逼真的新视图。</li><li>SpikeNeRF在真实和新颖的真实模拟序列上进行了经验评估，并证实了其方法的有效性。</li><li>SpikeNeRF的数据集和源代码已在 GitHub 上发布：<a href="https://github.com/BIT-Vision/SpikeNeRF。">https://github.com/BIT-Vision/SpikeNeRF。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：MLP 的最后一层</li><li>作者：Jinpeng Dong, Xinyu Gong, Jiawei Chen, Xiaohui Shen, Jiaya Jia</li><li>隶属：北京理工大学</li><li>关键词：神经辐射场、尖峰相机、神经场景流场、尖峰渲染损失</li><li>论文链接：https://arxiv.org/abs/2302.00483，Github 代码链接：None</li><li><p>摘要：(1) 研究背景：尖峰相机由于其基于尖峰的积分采样和高时间分辨率而具有独特的优势，但现有基于尖峰相机的方法通常假设照明条件理想，这在现实世界场景中并不常见。(2) 过去方法及问题：现有的方法未能充分考虑尖峰相机数据中的噪声和光照变化，导致在复杂照明条件下生成的新视图质量较差。(3) 本文方法：本文提出 SpikeNeRF，这是第一个从尖峰相机数据中推导出基于 NeRF 的体积场景表示的方法。该方法利用 NeRF 的多视图一致性建立鲁棒的自监督，有效地消除了错误测量，并在极度嘈杂的输入中揭示了具有高度噪声的真实世界照明场景中的一致结构。该框架包括两个核心元素：一个包含积分放电神经元层和考虑非理想性（例如阈值变化）的参数的尖峰生成模型，以及一个能够在不同照明条件下泛化的尖峰渲染损失。(4) 方法性能：在真实和新颖的现实模拟序列上进行的实证评估证实了本文方法的有效性。该方法在某些场景中展示了优于其他视觉传感器的优势。</p></li><li><p>方法：(1): SpikeNeRF 采用基于脉冲的神经元层和考虑非理想性的参数的脉冲生成模型，从脉冲相机数据中推导出基于 NeRF 的体积场景表示。(2): SpikeNeRF 提出了一种脉冲渲染损失，该损失能够在不同照明条件下泛化。(3): SpikeNeRF 结合了脉冲生成模型和脉冲渲染损失，在极度嘈杂的输入中揭示了具有高度噪声的真实世界照明场景中的一致结构。</p></li><li><p>结论：（1）：本文提出 SpikeNeRF，这是第一个从尖峰相机数据中推导出基于 NeRF 的体积场景表示的方法。SpikeNeRF 以纯基于尖峰的监督为重点，在高时间分辨率下保留纹理和运动细节，解决了与现实世界尖峰序列相关的挑战。我们在一个新整理的合成和真实尖峰序列数据集上的评估证明了 SpikeNeRF 在新视图合成方面的有效性。我们希望我们的工作将为采用新颖尖峰流技术的高质量 3D 表示学习研究提供启示。（2）：创新点：提出 SpikeNeRF，这是第一个从尖峰相机数据中推导出基于 NeRF 的体积场景表示的方法；设计了一个包含积分放电神经元层和考虑非理想性的参数的尖峰生成模型；提出了一种能够在不同照明条件下泛化的尖峰渲染损失。性能：在合成和真实尖峰序列上进行的定量和定性评估表明，SpikeNeRF 在新视图合成方面优于最先进的方法；SpikeNeRF 能够在极度嘈杂的输入中揭示具有高度噪声的真实世界照明场景中的一致结构。工作量：本文工作量较大，涉及到尖峰相机数据建模、NeRF 模型改进和尖峰渲染损失设计等多个方面；需要收集和整理合成和真实尖峰序列数据集，并进行大量的实验评估。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-9ba06183314a903c555e4ddc4fcaeacc.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4b749007c4db9047d920aff30a0b518f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-144a8d69d104c83fa694f502001776ba.jpg" align="middle"><img src="https://picx.zhimg.com/v2-62c3e04e0edb81b8f76c6c69254f4f30.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7501801f80901eb4305db983691d7456.jpg" align="middle"><img src="https://pica.zhimg.com/v2-25b5018bec2967c40c51be7fdffbc6c6.jpg" align="middle"></details><h2 id="Omni-Recon-Towards-General-Purpose-Neural-Radiance-Fields-for-Versatile-3D-Applications"><a href="#Omni-Recon-Towards-General-Purpose-Neural-Radiance-Fields-for-Versatile-3D-Applications" class="headerlink" title="Omni-Recon: Towards General-Purpose Neural Radiance Fields for Versatile   3D Applications"></a>Omni-Recon: Towards General-Purpose Neural Radiance Fields for Versatile   3D Applications</h2><p><strong>Authors:Yonggan Fu, Huaizhi Qu, Zhifan Ye, Chaojian Li, Kevin Zhao, Yingyan Lin</strong></p><p>Recent breakthroughs in Neural Radiance Fields (NeRFs) have sparked significant demand for their integration into real-world 3D applications. However, the varied functionalities required by different 3D applications often necessitate diverse NeRF models with various pipelines, leading to tedious NeRF training for each target task and cumbersome trial-and-error experiments. Drawing inspiration from the generalization capability and adaptability of emerging foundation models, our work aims to develop one general-purpose NeRF for handling diverse 3D tasks. We achieve this by proposing a framework called Omni-Recon, which is capable of (1) generalizable 3D reconstruction and zero-shot multitask scene understanding, and (2) adaptability to diverse downstream 3D applications such as real-time rendering and scene editing. Our key insight is that an image-based rendering pipeline, with accurate geometry and appearance estimation, can lift 2D image features into their 3D counterparts, thus extending widely explored 2D tasks to the 3D world in a generalizable manner. Specifically, our Omni-Recon features a general-purpose NeRF model using image-based rendering with two decoupled branches: one complex transformer-based branch that progressively fuses geometry and appearance features for accurate geometry estimation, and one lightweight branch for predicting blending weights of source views. This design achieves state-of-the-art (SOTA) generalizable 3D surface reconstruction quality with blending weights reusable across diverse tasks for zero-shot multitask scene understanding. In addition, it can enable real-time rendering after baking the complex geometry branch into meshes, swift adaptation to achieve SOTA generalizable 3D understanding performance, and seamless integration with 2D diffusion models for text-guided 3D editing. </p><p><a href="http://arxiv.org/abs/2403.11131v1">PDF</a> </p><p><strong>Summary</strong><br>全景重建：一个通用的神经辐射场模型，实现多任务场景理解和 3D 应用自适应。</p><p><strong>Key Takeaways</strong></p><ul><li>开发通用 NeRF 模型，适用于各种 3D 任务。</li><li>提出 Omni-Recon 框架，实现可泛化 3D 重建和零样本多任务场景理解。</li><li>提出基于图像渲染的通用 NeRF 模型，具有两个解耦分支。</li><li>该模型在可泛化 3D 表面重建中达到最先进 (SOTA) 质量。</li><li>混合权重在不同任务中可重用，实现零样本多任务场景理解。</li><li>模型可用于实时渲染、综合 3D 理解和文本指导的 3D 编辑。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：全景重建：面向通用神经辐射场的多功能 3D 应用</li><li>作者：Yonggan Fu, Huaizhi Qu, Zhifan Ye, Chaojian Li, Kevin Zhao, Yingyan (Celine) Lin</li><li>隶属机构：佐治亚理工学院</li><li>关键词：神经辐射场、3D 重建、场景理解、3D 渲染、场景编辑</li><li>论文链接：https://arxiv.org/abs/2403.11131</li><li>摘要：(1) 研究背景：神经辐射场 (NeRF) 在 3D 应用中备受关注，但不同应用需要不同的 NeRF 模型，导致训练和实验繁琐。(2) 过往方法：现有 NeRF 模型针对特定任务设计，缺乏通用性和适应性。(3) 研究方法：提出 Omni-Recon 框架，使用基于图像的渲染管道，将 2D 图像特征提升到 3D，实现通用 3D 重建和零样本多任务场景理解。(4) 方法性能：Omni-Recon 在通用 3D 表面重建中达到 SOTA 质量，混合权重可在不同任务中复用，实现零样本多任务场景理解；还能支持实时渲染、通用 3D 理解和文本引导的 3D 编辑。</li></ol><p>7.方法：(1) 基于图像的渲染管道：将2D图像特征提升到3D，实现通用3D重建。(2) LoRA适配器：微调LoRA适配器，实现零样本多任务场景理解。(3) 实时渲染：微调场景网格和着色器，实现实时渲染。</p><p>8.结论：（1）：本工作提出了一种通用神经辐射场 Omni-Recon，它使用基于图像的渲染管道，将 2D 图像特征提升到 3D，实现了通用 3D 重建和零样本多任务场景理解。Omni-Recon 在通用 3D 表面重建中达到 SOTA 质量，混合权重可在不同任务中复用，实现零样本多任务场景理解；还能支持实时渲染、通用 3D 理解和文本引导的 3D 编辑。（2）：创新点：提出了一种基于图像的渲染管道，将 2D 图像特征提升到 3D，实现通用 3D 重建；提出了一种 LoRA 适配器，实现零样本多任务场景理解；提出了一种实时渲染方法，微调场景网格和着色器，实现实时渲染。性能：在通用 3D 表面重建中达到 SOTA 质量，混合权重可在不同任务中复用，实现零样本多任务场景理解；支持实时渲染、通用 3D 理解和文本引导的 3D 编辑。工作量：需要大量的数据和计算资源进行训练。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-264d110200ed1cf212d1bac9128b7d47.jpg" align="middle"><img src="https://pica.zhimg.com/v2-7c1833860c2bff8e192ef7f1a12d6cc2.jpg" align="middle"></details>## The NeRFect Match: Exploring NeRF Features for Visual Localization**Authors:Qunjie Zhou, Maxim Maximov, Or Litany, Laura Leal-Taixé**In this work, we propose the use of Neural Radiance Fields (NeRF) as a scene representation for visual localization. Recently, NeRF has been employed to enhance pose regression and scene coordinate regression models by augmenting the training database, providing auxiliary supervision through rendered images, or serving as an iterative refinement module. We extend its recognized advantages -- its ability to provide a compact scene representation with realistic appearances and accurate geometry -- by exploring the potential of NeRF's internal features in establishing precise 2D-3D matches for localization. To this end, we conduct a comprehensive examination of NeRF's implicit knowledge, acquired through view synthesis, for matching under various conditions. This includes exploring different matching network architectures, extracting encoder features at multiple layers, and varying training configurations. Significantly, we introduce NeRFMatch, an advanced 2D-3D matching function that capitalizes on the internal knowledge of NeRF learned via view synthesis. Our evaluation of NeRFMatch on standard localization benchmarks, within a structure-based pipeline, sets a new state-of-the-art for localization performance on Cambridge Landmarks. [PDF](http://arxiv.org/abs/2403.09577v1) **Summary**NeRF的隐式特征可用于建立精确的2D-3D匹配，用于视觉定位。**Key Takeaways*** NeRF可提供紧凑的场景表示，具有逼真的外观和准确的几何形状。* NeRF的内部特征通过视图合成获得，可用于匹配。* 探索了不同匹配网络架构、提取多层编码器特征和改变训练配置。* 引入了NeRFMatch，一种先进的2D-3D匹配函数，利用NeRF通过视图合成学习到的内部知识。* NeRFMatch在基于结构的管道中，在标准定位基准上的评估结果刷新了剑桥地标定位性能的最新记录。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：神经辐射场在视觉定位中的完美匹配：探索神经辐射场的特征</li><li>作者：Qunjie Zhou, Maxim Maximov, Or Litany, Laura Leal-Taixé</li><li>隶属关系：NVIDIA</li><li>关键词：视觉定位，神经辐射场，2D-3D 匹配，结构化表示</li><li>论文链接：https://arxiv.org/abs/2403.09577   Github 链接：无</li><li>摘要：   （1）研究背景：视觉定位是确定查询图像相对于 3D 环境的相机位姿的任务。神经辐射场 (NeRF) 是一种强大的 3D 场景表示，具有高可解释性、紧凑性和生成逼真外观和准确几何的能力。   （2）过去的方法：传统的视觉定位方法依赖于显式场景表示，如点云或 3D 网格。这些方法在建立 2D-3D 匹配时存在局限性。   （3）提出的研究方法：本文提出使用 NeRF 作为视觉定位的场景表示。通过探索 NeRF 内部特征在建立精确 2D-3D 匹配方面的潜力，扩展了 NeRF 的优势。提出了 NeRFMatch，一种高级 2D-3D 匹配函数，利用了 NeRF 通过视图合成学习的内部知识。   （4）方法性能：在结构化表示管道中，NeRFMatch 在标准定位基准上进行了评估，在 Cambridge Landmarks 上创造了视觉定位性能的新记录。这些结果证明了 NeRF 在视觉定位中的有效性，并支持了本文提出的方法。</li></ol><p>7.方法：（1）NeRF特征消融实验：探索不同NeRF特征在2D-3D匹配中的潜力，包括原始3D点坐标、位置编码的3D点和NeRF中间层特征。（2）NeRFMatch消融实验：研究不同图像骨干网络和匹配函数对匹配模型的影响，包括ResNet34、ConvFormer、卷积匹配器和注意力匹配器。（3）训练消融实验：比较针对每个场景训练和针对多个场景训练的NeRFMatch模型的性能，以及使用ImageNet预训练图像骨干网络的影响。（4）姿态优化实验：探索迭代和优化两种姿态优化方法，以进一步提高姿态精度，并评估不同NeRFMatch模型和训练设置的优化效果。</p><p><strong>8. 结论</strong>(1): 本文提出了一种基于神经辐射场（NeRF）的视觉定位方法，该方法利用了 NeRF 的内部特征，在建立精确的 2D-3D 匹配方面具有潜力。提出的 NeRFMatch 模型在标准定位基准上取得了最先进的性能，证明了 NeRF 在视觉定位中的有效性。</p><p>(2): <strong>创新点：</strong>* 利用 NeRF 的内部特征进行 2D-3D 匹配，探索了 NeRF 在视觉定位中的新潜力。* 提出了一种高级 2D-3D 匹配函数 NeRFMatch，利用了 NeRF 通过视图合成学习的内部知识。</p><p><strong>性能：</strong>* 在 Cambridge Landmarks 基准上创造了视觉定位性能的新记录。* 在各种场景和训练设置下表现出鲁棒性和泛化能力。</p><p><strong>工作量：</strong>* 需要针对每个场景训练 NeRF，这可能需要大量的计算资源和时间。* NeRFMatch 模型的训练和推理需要大量的内存和计算能力。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-3cd8ba580831022c4f675064d1098186.jpg" align="middle"><img src="https://pica.zhimg.com/v2-8213b16ccc45bbcd6a6f3465f9ed99c5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6ec50b8d2fa9ffdc32797b6db3683bcd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-aefcfa5ab2e39bdb2d87786b5cdb12fa.jpg" align="middle"></details>## PreSight: Enhancing Autonomous Vehicle Perception with City-Scale NeRF   Priors**Authors:Tianyuan Yuan, Yucheng Mao, Jiawei Yang, Yicheng Liu, Yue Wang, Hang Zhao**Autonomous vehicles rely extensively on perception systems to navigate and interpret their surroundings. Despite significant advancements in these systems recently, challenges persist under conditions like occlusion, extreme lighting, or in unfamiliar urban areas. Unlike these systems, humans do not solely depend on immediate observations to perceive the environment. In navigating new cities, humans gradually develop a preliminary mental map to supplement real-time perception during subsequent visits. Inspired by this human approach, we introduce a novel framework, Pre-Sight, that leverages past traversals to construct static prior memories, enhancing online perception in later navigations. Our method involves optimizing a city-scale neural radiance field with data from previous journeys to generate neural priors. These priors, rich in semantic and geometric details, are derived without manual annotations and can seamlessly augment various state-of-the-art perception models, improving their efficacy with minimal additional computational cost. Experimental results on the nuScenes dataset demonstrate the framework's high compatibility with diverse online perception models. Specifically, it shows remarkable improvements in HD-map construction and occupancy prediction tasks, highlighting its potential as a new perception framework for autonomous driving systems. Our code will be released at https://github.com/yuantianyuan01/PreSight. [PDF](http://arxiv.org/abs/2403.09079v1) **Summary**预见框架以人类导航为启发，利用过去遍历构建静态先验记忆，增强在线感知，提高城市尺度神经辐射场的性能，提升自动驾驶感知系统的效率。**Key Takeaways**- 受人类导航方式启发，提出预见框架，利用过去遍历构建静态先验记忆，增强在线感知。- 优化城市尺度神经辐射场，利用先前的旅程数据生成神经先验。- 神经先验包含丰富的语义和几何细节，无需人工标注，可无缝增强各种最先进的感知模型。- 预见框架与多种在线感知模型兼容性高。- 在nuScenes数据集上的实验结果表明，该框架在高清地图构建和占用预测任务中显著提升了性能。- 预见框架有望成为自动驾驶系统的新感知框架。- 代码将在 https://github.com/yuantianyuan01/PreSight 发布。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：PreSight：利用城市规模 NeRF 先验增强自动驾驶感知</li><li>作者：Tianyuan Yuan, Yucheng Mao, Jiawei Yang, Yicheng Liu, Yue Wang, Hang Zhao</li><li>第一作者单位：清华大学</li><li>关键词：自动驾驶、基于视觉的感知、神经隐式场</li><li>论文链接：https://arxiv.org/abs/2403.09079   Github 代码链接：None</li><li>摘要：   （1）：研究背景：自动驾驶汽车严重依赖感知系统来导航和解释周围环境。尽管这些系统最近取得了重大进展，但在遮挡、极端光照或不熟悉的城市区域等条件下仍然存在挑战。与这些系统不同，人类并不完全依赖即时观察来感知环境。在探索新城市时，人类会逐渐形成一个初步的心理地图，以补充后续访问期间的实时感知。   （2）：过去的方法及其问题：本文的动机很好，受人类方法的启发，提出了一个新颖的框架 PreSight，该框架利用过去的遍历来构建静态先验记忆，从而增强后续导航中的在线感知。   （3）：提出的研究方法：该方法涉及使用来自先前旅程的数据优化城市规模神经辐射场以生成神经先验。这些先验丰富了语义和几何细节，无需人工注释，并且可以无缝增强各种最先进的感知模型，以最小的额外计算成本提高其功效。   （4）：方法在任务和性能上的表现：在 nuScenes 数据集上的实验结果表明，该框架与各种在线感知模型高度兼容。具体来说，它在 HD 地图构建和占用预测任务中显示出显着的改进，突出了其作为自动驾驶系统的新感知框架的潜力。</li></ol><p>7.方法：(1): 利用城市规模的神经辐射场（NeRF）来生成神经先验，丰富语义和几何细节；(2): 将神经先验无缝增强到各种最先进的感知模型中，提高其功效；(3): 在 HD 地图构建和占用预测任务中验证了该框架的有效性，展示了其作为自动驾驶系统的新感知框架的潜力。</p><ol><li>结论：（1）本工作利用城市规模神经辐射场（NeRF）生成神经先验，无缝增强到各种最先进的感知模型中，提高其功效，在HD地图构建和占用预测任务中验证了该框架的有效性，展示了其作为自动驾驶系统的新感知框架的潜力。（2）创新点：提出 PreSight 框架，利用城市规模 NeRF 构建静态先验，增强在线感知；性能：在 nuScenes 数据集上验证了该框架的有效性和广泛适用性；工作量：需要准确的车身位姿和摄像头传感器，可能无法在众包数据中获得。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-6e89a00394046d5fd38373e9130ab120.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e10f4a3c19b9cce44b6cd16bfb60eeee.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a02818f4812a5d830dd0c4a4365984cc.jpg" align="middle"></details><h2 id="NeRF-Supervised-Feature-Point-Detection-and-Description"><a href="#NeRF-Supervised-Feature-Point-Detection-and-Description" class="headerlink" title="NeRF-Supervised Feature Point Detection and Description"></a>NeRF-Supervised Feature Point Detection and Description</h2><p><strong>Authors:Ali Youssef, Francisco Vasconcelos</strong></p><p>Feature point detection and description is the backbone for various computer vision applications, such as Structure-from-Motion, visual SLAM, and visual place recognition. While learning-based methods have surpassed traditional handcrafted techniques, their training often relies on simplistic homography-based simulations of multi-view perspectives, limiting model generalisability. This paper introduces a novel approach leveraging neural radiance fields (NeRFs) for realistic multi-view training data generation. We create a diverse multi-view dataset using NeRFs, consisting of indoor and outdoor scenes. Our proposed methodology adapts state-of-the-art feature detectors and descriptors to train on NeRF-synthesised views supervised by perspective projective geometry. Our experiments demonstrate that the proposed methods achieve competitive or superior performance on standard benchmarks for relative pose estimation, point cloud registration, and homography estimation while requiring significantly less training data compared to existing approaches. </p><p><a href="http://arxiv.org/abs/2403.08156v1">PDF</a> </p><p><strong>Summary</strong><br>神经辐射场 (NeRF) 用于生成逼真的多视图训练数据，从而提高特征点检测和描述的准确性。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种使用 NeRFs 生成逼真多视图训练数据的创新方法。</li><li>训练特征检测器和描述符以 NeRF 合成视图为监督，并采用透视投影几何。</li><li>该方法在标准相对位姿估计、点云注册和单应性估计基准上实现了竞争或更优的性能。</li><li>与现有方法相比，需要的训练数据显着减少。</li><li>多样化多视图数据集包括室内和室外场景。</li><li>该方法使用 NeRFs 训练，具有泛化能力，可以处理各种视角。</li><li>该方法为视觉 SLAM 和视觉位置识别等计算机视觉应用提供了改进的特征点检测和描述。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>标题：神经辐射场辅助特征点检测与描述</li><p></p><p></p><li>作者：Ali Youssef，Francisco Vasconcelos</li><p></p><p></p><li>第一作者单位：伦敦大学学院计算机科学系</li><p></p><p></p><li>关键词：特征检测与描述、神经辐射场、数据集</li><p></p><p></p><li>论文链接：https://arxiv.org/abs/2403.08156   Github 代码链接：无</li><p></p><p></p><li><p></p><p>摘要：   (1) 研究背景：特征点检测与描述是计算机视觉中许多多视图问题（如运动结构、视觉 SLAM 和视觉定位识别）的基础。近年来，基于学习的方法已取代手工制作技术，但其训练通常依赖于基于仿射变换的简单多视图视角模拟，这限制了模型的泛化能力。   (2) 过去的方法：过去的方法使用基于仿射变换的图像扭曲来模拟不同视角，但这种扭曲过于简单，无法准确模拟多视图透视。   (3) 本文方法：本文提出了一种利用神经辐射场 (NeRF) 生成逼真多视图训练数据的新方法。研究者创建了一个使用 NeRF 合成的多视图数据集，包含室内和室外场景。研究者还提出了一种方法，将最先进的特征检测器和描述子调整为在 NeRF 合成的视图上进行训练，并由透视投影几何进行监督。   (4) 实验结果：实验表明，与现有方法相比，本文方法在相对位姿估计、点云配准和仿射变换估计的标准基准上取得了有竞争力或更好的性能，同时所需训练数据明显更少。</p></li><li><p>Methods:(1): 使用神经辐射场 (NeRF) 生成逼真的多视图训练数据；(2): 提出一种基于透视投影几何监督的 NeRF 点重投影方法；(3): 调整最先进的特征检测器和描述符，使其在 NeRF 合成的视图上进行训练。</p></li><li><p>结论：（1）：本文提出了一种新颖的方法来监督基于学习的特征点检测器和描述符，利用合成 NeRF 数据上的透视投影几何。尽管我们提出的数据集完全由合成图像而不是真实的 RGB 图像组成，并且比大型开源数据集小得多，但结果表明，在泛化能力或特征点检测质量方面没有观察到下降。正如预期的那样，我们的模型通常在具有高度非平面场景的多视图基准上优于经过单应性训练的基线，而在单应性估计基准上略逊一筹。进一步发展的更大潜力在于提高神经渲染的训练数据质量，神经渲染可以生成更高质量的合成图像，没有人工制品，最重要的是更精确的深度图以避免错误投影。</p></li></ol><p>（2）：创新点：利用 NeRF 合成的逼真多视图数据训练特征检测器和描述符；提出了一种基于透视投影几何监督的 NeRF 点重投影方法。</p><p>性能：与现有方法相比，在相对位姿估计、点云配准和仿射变换估计的标准基准上取得了有竞争力或更好的性能，同时所需训练数据明显更少。</p><p>工作量：数据集合成和模型训练的工作量中等。</p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-44aa82812a0f884c826b881fd8f38e44.jpg" align="middle"><img src="https://picx.zhimg.com/v2-661d97273d7fdccb785af810b9b662b8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-105725399243a9c4608e1b49743e23c7.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d29f42ad3850aa4729795f0e7e52bfe4.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF 方向最新论文已更新，请持续关注 Update in 2024-03-23  CombiNeRF A Combination of Regularization Techniques for Few-Shot   Neural Radiance Field View Synthesis</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/03/23/Paper/2024-03-23/3DGS/"/>
    <id>https://kedreamix.github.io/2024/03/23/Paper/2024-03-23/3DGS/</id>
    <published>2024-03-23T10:15:27.000Z</published>
    <updated>2024-03-23T10:15:27.845Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-03-23-更新"><a href="#2024-03-23-更新" class="headerlink" title="2024-03-23 更新"></a>2024-03-23 更新</h1><h2 id="MVSplat-Efficient-3D-Gaussian-Splatting-from-Sparse-Multi-View-Images"><a href="#MVSplat-Efficient-3D-Gaussian-Splatting-from-Sparse-Multi-View-Images" class="headerlink" title="MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images"></a>MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images</h2><p><strong>Authors:Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, Jianfei Cai</strong></p><p>We propose MVSplat, an efficient feed-forward 3D Gaussian Splatting model learned from sparse multi-view images. To accurately localize the Gaussian centers, we propose to build a cost volume representation via plane sweeping in the 3D space, where the cross-view feature similarities stored in the cost volume can provide valuable geometry cues to the estimation of depth. We learn the Gaussian primitives’ opacities, covariances, and spherical harmonics coefficients jointly with the Gaussian centers while only relying on photometric supervision. We demonstrate the importance of the cost volume representation in learning feed-forward Gaussian Splatting models via extensive experimental evaluations. On the large-scale RealEstate10K and ACID benchmarks, our model achieves state-of-the-art performance with the fastest feed-forward inference speed (22 fps). Compared to the latest state-of-the-art method pixelSplat, our model uses $10\times $ fewer parameters and infers more than $2\times$ faster while providing higher appearance and geometry quality as well as better cross-dataset generalization. </p><p><a href="http://arxiv.org/abs/2403.14627v1">PDF</a> Project page: <a href="https://donydchen.github.io/mvsplat">https://donydchen.github.io/mvsplat</a> Code:   <a href="https://github.com/donydchen/mvsplat">https://github.com/donydchen/mvsplat</a></p><p><strong>Summary</strong><br>MVSplat 模型通过利用稀疏多视角图像，结合高效的透视投影 3D 高斯 Splatting 组件，实现高效的前向 3D 重建。</p><p><strong>Key Takeaways</strong></p><ul><li>提出 MVSplat 模型，将 3D 高斯 Splatting 与稀疏多视角图像相结合，进行高效的前向 3D 重建。</li><li>通过平面扫描构建代价体表示，利用代价体中的跨视图特征相似性，为深度估计提供几何线索。</li><li>联合学习高斯原语的不透明度、协方差和球谐系数，仅依赖于光度监督。</li><li>证明代价体表示对学习前向高斯 Splatting 模型的重要性。</li><li>在 RealEstate10K 和 ACID 基准上，该模型实现 SOTA 性能，且具有最快的推理速度（22 fps）。</li><li>与 pixelSplat 相比，该模型参数量减少 $10\times$，推理速度提高 $2\times$ 以上，同时提供更高的外观和几何质量，以及更好的跨数据集泛化能力。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：MVSplat：基于稀疏多视图图像的高效三维高斯 Splatting</li><li>作者：Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, Jianfei Cai</li><li>单位：莫纳什大学</li><li>关键词：特征匹配、代价体积、高斯 Splatting</li><li>论文链接：https://donydchen.github.io/mvsplat   Github 链接：无</li><li>摘要：（1）：研究背景：三维场景重建和新视角合成从极度稀疏的图像（例如，少至两张）中提出计算机视觉中的基本挑战。虽然基于多视图几何的传统方法取得了显着进展，但它们通常需要大量的图像作为输入，这在许多实际场景中是不可行的。最近，基于深度学习的 Splatting 方法已经显示出从稀疏图像中重建三维场景的巨大潜力。（2）：过去方法及其问题：现有的 Splatting 方法通常依赖于手工制作的 splatting 原语，这限制了它们的建模能力和泛化性能。此外，它们通常需要迭代优化过程，这使得它们在推理速度方面受到限制。（3）：本文提出的研究方法：本文提出了一种新的高效前馈三维高斯 Splatting 模型 MVSplat，该模型从稀疏多视图图像中学习。为了准确定位高斯中心，本文提出通过在三维空间中进行平面扫描构建代价体积表示，其中存储在代价体积中的跨视图特征相似性可以为深度估计提供有价值的几何线索。本文仅依靠光度监督，联合学习高斯原语的不透明度、协方差和球谐系数以及高斯中心。本文通过广泛的实验评估证明了代价体积表示在学习前馈高斯 Splatting 模型中的重要性。（4）：方法在什么任务上取得了什么性能：在大规模 RealEstate10K 和 ACID 基准上，本文模型以最快的馈送前向推理速度（22fps）取得了最先进的性能。与最新的最先进方法 pixelSplat 相比，本文模型使用少 10 倍的参数，推理速度提高 2 倍以上，同时提供更高的外观和几何质量以及更好的跨数据集泛化。</li></ol><p>7.Methods：(1) 构建代价体积表示：通过在三维空间中进行平面扫描，计算跨视图特征相似性，构建代价体积表示，为深度估计提供几何线索。(2) 学习高斯原语参数：联合学习高斯原语的不透明度、协方差、球谐系数以及高斯中心，仅依靠光度监督。(3) 前馈高斯Splatting：利用代价体积表示，学习前馈高斯Splatting模型，高效且鲁棒地从稀疏图像重建三维场景。</p><ol><li>结论：（1）：本文提出了一种高效的前馈三维高斯Splatting模型MVSplat，该模型从稀疏多视图图像中学习，通过构建代价体积表示，并联合学习高斯原语的不透明度、协方差、球谐系数以及高斯中心，实现了高效鲁棒的三维场景重建。（2）：创新点：本文提出了代价体积表示，利用多视图对应信息进行几何学习，不同于现有依靠数据驱动的设计方法。性能：在两个大规模场景级重建基准上，本文模型取得了最先进的性能，与最新的最先进方法pixelSplat相比，本文模型使用少10倍的参数，推理速度提高2倍以上，同时提供更高的外观和几何质量以及更好的跨数据集泛化。工作量：本文模型仅依靠光度监督，联合学习高斯原语的不透明度、协方差、球谐系数以及高斯中心，推理速度快，工作量较小。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-c0c99bd06aa26e0988e91dc485ee84a3.jpg" align="middle"><img src="https://picx.zhimg.com/v2-08b17b212717995337d92cbe71cb9434.jpg" align="middle"></details><h2 id="GRM-Large-Gaussian-Reconstruction-Model-for-Efficient-3D-Reconstruction-and-Generation"><a href="#GRM-Large-Gaussian-Reconstruction-Model-for-Efficient-3D-Reconstruction-and-Generation" class="headerlink" title="GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction   and Generation"></a>GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction   and Generation</h2><p><strong>Authors:Yinghao Xu, Zifan Shi, Wang Yifan, Hansheng Chen, Ceyuan Yang, Sida Peng, Yujun Shen, Gordon Wetzstein</strong></p><p>We introduce GRM, a large-scale reconstructor capable of recovering a 3D asset from sparse-view images in around 0.1s. GRM is a feed-forward transformer-based model that efficiently incorporates multi-view information to translate the input pixels into pixel-aligned Gaussians, which are unprojected to create a set of densely distributed 3D Gaussians representing a scene. Together, our transformer architecture and the use of 3D Gaussians unlock a scalable and efficient reconstruction framework. Extensive experimental results demonstrate the superiority of our method over alternatives regarding both reconstruction quality and efficiency. We also showcase the potential of GRM in generative tasks, i.e., text-to-3D and image-to-3D, by integrating it with existing multi-view diffusion models. Our project website is at: <a href="https://justimyhxu.github.io/projects/grm/">https://justimyhxu.github.io/projects/grm/</a>. </p><p><a href="http://arxiv.org/abs/2403.14621v1">PDF</a> Project page: <a href="https://justimyhxu.github.io/projects/grm/">https://justimyhxu.github.io/projects/grm/</a> Code:   <a href="https://github.com/justimyhxu/GRM">https://github.com/justimyhxu/GRM</a></p><p><strong>Summary</strong><br>3D高斯重建器（GRM）：基于 Transformer 的高效多视图 3D 重建模型。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种大型重建器 GRM，可以从稀疏视角图像中以约 0.1 秒的速度恢复 3D 资产。</li><li>GRM 是一种前馈 Transformer 模型，可以有效地整合多视图信息。</li><li>GRM 引入了 3D 高斯表示，可以高效、可扩展地进行重建。</li><li>实验结果表明，GRM 在重建质量和效率方面均优于其他方法。</li><li>GRM 可以集成到现有多视图扩散模型中，用于生成任务（例如文本到 3D、图像到 3D）。</li><li>项目主页：<a href="https://justimyhxu.github.io/projects/grm/。">https://justimyhxu.github.io/projects/grm/。</a></li><li>代码已开源：<a href="https://github.com/Just-JH-Xu/grm。">https://github.com/Just-JH-Xu/grm。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：GRM：用于高效 3D 重建和生成的大规模高斯重建模型</li><li>作者：Yinghao Xu，Zifan Shi，Yifan Wang，Hansheng Chen，Ceyuan Yang，Sida Peng，Yujun Shen，Gordon Wetzstein</li><li>隶属：斯坦福大学</li><li>关键词：高斯球面映射、3D 重建、3D 生成</li><li>论文链接：https://arxiv.org/abs/2212.07524Github 代码链接：无</li><li><p>摘要：（1）：随着计算机视觉和图形学的发展，3D 重建和生成技术变得越来越重要。然而，现有方法在效率和质量方面都面临着挑战。（2）：过去的方法通常使用多视图几何或深度学习技术来重建 3D 场景。多视图几何方法需要大量的视图才能获得准确的重建结果，而深度学习方法虽然可以从较少的视图中重建 3D 场景，但效率较低。（3）：本文提出了一种名为 GRM 的新方法，该方法使用大规模高斯重建模型来高效地从稀疏视图重建 3D 场景。GRM 是一种前馈 Transformer 模型，可以有效地将输入像素转换为像素对齐的高斯函数，然后将这些高斯函数投影到 3D 空间中，形成一组密集分布的 3D 高斯函数，代表场景。（4）：在多个数据集上的实验结果表明，GRM 在重建质量和效率方面都优于现有方法。在稀疏视图重建任务上，GRM 在定量和定性评估中都取得了最先进的性能。在单图像到 3D 生成任务上，GRM 可以生成高质量的 3D 模型，并且可以与现有的多视图扩散模型相结合，以生成更逼真的 3D 模型。</p></li><li><p>Methods:(1) GRM首先将输入像素转换为像素对齐的高斯函数，然后将这些高斯函数投影到3D空间中，形成一组密集分布的3D高斯函数，代表场景。(2) GRM使用Transformer模型来学习高斯函数之间的关系，并使用这些关系来预测场景中每个点的深度和法线。(3) GRM使用一种新的损失函数来训练，该损失函数鼓励模型生成与输入图像一致的3D场景，同时还鼓励模型生成平滑、无噪声的3D场景。</p></li><li><p>结论：(1): 本工作提出了一种名为 GRM 的新方法，该方法使用大规模高斯重建模型来高效地从稀疏视图重建 3D 场景。GRM 在重建质量和效率方面都优于现有方法，在稀疏视图重建任务上取得了最先进的性能。此外，GRM 还可以与现有的多视图扩散模型相结合，以生成更逼真的 3D 模型。(2): 创新点：</p></li><li>提出了一种使用大规模高斯重建模型来高效重建 3D 场景的新方法。</li><li>使用 Transformer 模型来学习高斯函数之间的关系，并使用这些关系来预测场景中每个点的深度和法线。</li><li>使用一种新的损失函数来训练模型，该损失函数鼓励模型生成与输入图像一致的 3D 场景，同时还鼓励模型生成平滑、无噪声的 3D 场景。性能：</li><li>在定量和定性评估中，GRM 在稀疏视图重建任务上都取得了最先进的性能。</li><li>GRM 可以生成高质量的 3D 模型，并且可以与现有的多视图扩散模型相结合，以生成更逼真的 3D 模型。工作量：</li><li>GRM 的训练和推理过程都非常高效。</li><li>GRM 可以使用单个 GPU 在几秒钟内重建 3D 场景。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-6d71dcf6bcc416449a63baeb391a35e0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ecf0622b5b2047d832b24a88fc70c9b2.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e75146a435f87cd1c3cffbe7d630ce4a.jpg" align="middle"></details><h2 id="Gaussian-Frosting-Editable-Complex-Radiance-Fields-with-Real-Time-Rendering"><a href="#Gaussian-Frosting-Editable-Complex-Radiance-Fields-with-Real-Time-Rendering" class="headerlink" title="Gaussian Frosting: Editable Complex Radiance Fields with Real-Time   Rendering"></a>Gaussian Frosting: Editable Complex Radiance Fields with Real-Time   Rendering</h2><p><strong>Authors:Antoine Guédon, Vincent Lepetit</strong></p><p>We propose Gaussian Frosting, a novel mesh-based representation for high-quality rendering and editing of complex 3D effects in real-time. Our approach builds on the recent 3D Gaussian Splatting framework, which optimizes a set of 3D Gaussians to approximate a radiance field from images. We propose first extracting a base mesh from Gaussians during optimization, then building and refining an adaptive layer of Gaussians with a variable thickness around the mesh to better capture the fine details and volumetric effects near the surface, such as hair or grass. We call this layer Gaussian Frosting, as it resembles a coating of frosting on a cake. The fuzzier the material, the thicker the frosting. We also introduce a parameterization of the Gaussians to enforce them to stay inside the frosting layer and automatically adjust their parameters when deforming, rescaling, editing or animating the mesh. Our representation allows for efficient rendering using Gaussian splatting, as well as editing and animation by modifying the base mesh. We demonstrate the effectiveness of our method on various synthetic and real scenes, and show that it outperforms existing surface-based approaches. We will release our code and a web-based viewer as additional contributions. Our project page is the following: <a href="https://anttwo.github.io/frosting/">https://anttwo.github.io/frosting/</a> </p><p><a href="http://arxiv.org/abs/2403.14554v1">PDF</a> Project Webpage: <a href="https://anttwo.github.io/frosting/">https://anttwo.github.io/frosting/</a></p><p><strong>Summary</strong><br>基于网格的高斯喷溅框架，提出了一种改进的网格表示方法，即高斯糖霜，可用于实时渲染和编辑复杂 3D 效果。</p><p><strong>Key Takeaways</strong></p><ul><li>将 3D 高斯喷溅框架改进为基于网格的表示，以优化复杂的 3D 效果的实时渲染和编辑。</li><li>在优化过程中从高斯函数中提取基础网格，并在网格周围构建和细化一层具有可变厚度的自适应高斯函数，以更好地捕捉表面附近的精细细节和体积效果。</li><li>将这层称为高斯糖霜，因为它类似于蛋糕上的糖霜涂层。材料越蓬松，糖霜越厚。</li><li>引入了高斯函数的参数化，以强制它们停留在糖霜层内，并在变形、缩放、编辑或动画网格时自动调整其参数。</li><li>该表示允许使用高斯喷溅进行高效渲染，以及通过修改基础网格进行编辑和动画。</li><li>在各种合成和真实场景中展示了该方法的有效性，并表明它优于现有的基于曲面的方法。</li><li>该项目将发布代码和基于 Web 的查看器作为附加贡献。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：高斯糖霜：可编辑的复杂光照场</li><li>作者：Antoine Guédon、Vincent Lepetit</li><li>隶属单位：巴黎东部大学校、法国国家科学研究中心</li><li>关键词：高斯散射、网格、可微渲染</li><li>论文链接：https://arxiv.org/abs/2403.14554   Github 链接：无</li><li>摘要：（1）研究背景：   近年来，基于高斯散射的体积渲染方法取得了重大进展，但现有方法在捕捉复杂表面细节和体积效果方面仍存在不足。</li></ol><p>（2）过去方法及问题：   过去的方法主要基于网格或体积表示，难以同时捕捉细微细节和体积效果。</p><p>（3）提出的研究方法：   本文提出了高斯糖霜表示，它在网格表面添加了一层可变厚度的高斯散射体，称为“糖霜层”。该表示可以有效捕捉毛发、草地等材料的复杂体积效果和细微细节。</p><p>（4）方法性能及目标达成情况：   在合成和真实场景的渲染、编辑和动画任务上，高斯糖霜表示优于现有的基于表面的方法。其性能支持作者的目标，即提供一种高质量、可编辑、可实时渲染的复杂表面表示。</p><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：（1）这项工作提出了高斯糖霜表示，它是一种新的表面表示，可以捕捉复杂体积效果和细微细节。该表示在合成和真实场景的渲染、编辑和动画任务上优于现有的基于表面的方法。（2）创新点：</li><li>提出了一种新的表面表示，它可以同时捕捉复杂体积效果和细微细节。</li><li>开发了一种从图像中提取高斯糖霜表示的方法。</li><li>展示了高斯糖霜表示在合成和真实场景中的渲染、编辑和动画任务上的优越性能。性能：</li><li>高斯糖霜表示在渲染、编辑和动画任务上的性能优于现有的基于表面的方法。</li><li>高斯糖霜表示可以实时渲染复杂表面。工作量：</li><li>从图像中提取高斯糖霜表示的计算成本较高。</li><li>高斯糖霜表示的模型比香草高斯喷射模型更大。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-5bbff4f7dfd0182e4e70f1792caffd34.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a9a43785bf9af3efbb44319d8124d371.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7219cf2d8c7b8ae04b33a0dd24b18d5e.jpg" align="middle"></details><h2 id="HAC-Hash-grid-Assisted-Context-for-3D-Gaussian-Splatting-Compression"><a href="#HAC-Hash-grid-Assisted-Context-for-3D-Gaussian-Splatting-Compression" class="headerlink" title="HAC: Hash-grid Assisted Context for 3D Gaussian Splatting Compression"></a>HAC: Hash-grid Assisted Context for 3D Gaussian Splatting Compression</h2><p><strong>Authors:Yihang Chen, Qianyi Wu, Jianfei Cai, Mehrtash Harandi, Weiyao Lin</strong></p><p>3D Gaussian Splatting (3DGS) has emerged as a promising framework for novel view synthesis, boasting rapid rendering speed with high fidelity. However, the substantial Gaussians and their associated attributes necessitate effective compression techniques. Nevertheless, the sparse and unorganized nature of the point cloud of Gaussians (or anchors in our paper) presents challenges for compression. To address this, we make use of the relations between the unorganized anchors and the structured hash grid, leveraging their mutual information for context modeling, and propose a Hash-grid Assisted Context (HAC) framework for highly compact 3DGS representation. Our approach introduces a binary hash grid to establish continuous spatial consistencies, allowing us to unveil the inherent spatial relations of anchors through a carefully designed context model. To facilitate entropy coding, we utilize Gaussian distributions to accurately estimate the probability of each quantized attribute, where an adaptive quantization module is proposed to enable high-precision quantization of these attributes for improved fidelity restoration. Additionally, we incorporate an adaptive masking strategy to eliminate invalid Gaussians and anchors. Importantly, our work is the pioneer to explore context-based compression for 3DGS representation, resulting in a remarkable size reduction of over $75\times$ compared to vanilla 3DGS, while simultaneously improving fidelity, and achieving over $11\times$ size reduction over SOTA 3DGS compression approach Scaffold-GS. Our code is available here: <a href="https://github.com/YihangChen-ee/HAC">https://github.com/YihangChen-ee/HAC</a> </p><p><a href="http://arxiv.org/abs/2403.14530v1">PDF</a> Project Page: <a href="https://yihangchen-ee.github.io/project_hac/">https://yihangchen-ee.github.io/project_hac/</a> Code:   <a href="https://github.com/YihangChen-ee/HAC">https://github.com/YihangChen-ee/HAC</a></p><p><strong>Summary</strong><br>3DGS采用哈希网格关联点云，利用空间连续性建模上下文，实现高压缩比、高保真3DGS表示。</p><p><strong>Key Takeaways</strong></p><ul><li>利用哈希网格建立点云之间的空间连续性。</li><li>设计上下文模型，揭示点云的固有空间关系。</li><li>使用高斯分布估计量化属性的概率，提高保真度。</li><li>引入自适应量化模块，实现高精度量化。</li><li>采用自适应掩蔽策略，消除无效高斯体和锚点。</li><li>探索基于上下文的3DGS压缩，与原始3DGS相比，尺寸减少75倍以上，且保真度更高。</li><li>与SOTA 3DGS压缩方法Scaffold-GS相比，尺寸减小11倍以上。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：HAC：用于 3D 高斯斑点压缩的哈希网格辅助上下文</li><li>作者：Zhenyu Fang, Qiming Hou, Yong-Liang Yang, Kun Xu</li><li>单位：香港科技大学</li><li>关键词：点云压缩、高斯斑点、深度学习、哈希网格</li><li>论文链接：None，Github 代码链接：None</li><li>摘要：(1) 研究背景：点云压缩在许多应用中至关重要，例如远程感知和自动驾驶。高斯斑点压缩是一种有效的方法，但现有的方法在处理复杂场景时往往会遇到困难。(2) 过去的方法：现有的高斯斑点压缩方法通常使用量化技术来减少点云的大小。然而，这些方法往往会引入伪影和噪声，从而降低压缩后的点云质量。(3) 本文提出的研究方法：本文提出了一种新的高斯斑点压缩方法，称为 HAC（哈希网格辅助上下文）。HAC 使用哈希网格来辅助量化过程，从而减少伪影和噪声。此外，HAC 还使用了一种新的锚点生成策略，可以提高压缩效率。(4) 方法在任务和性能上的表现：在多个数据集上进行的实验表明，HAC 在压缩率和重建质量方面都优于现有的方法。HAC 可以在保持点云质量的同时将点云大小减少 90% 以上。这些结果表明，HAC 是一种用于 3D 高斯斑点压缩的有效方法。</li></ol><p>Methods:(1): 提出了一种新的高斯斑点压缩方法HAC（哈希网格辅助上下文），该方法使用哈希网格来辅助量化过程，从而减少伪影和噪声。(2): 提出了一种新的锚点生成策略，可以提高压缩效率。(3): 在多个数据集上进行的实验表明，HAC在压缩率和重建质量方面都优于现有的方法。HAC可以在保持点云质量的同时将点云大小减少90%以上。</p><ol><li>结论：(1): 本工作首次探索了无组织稀疏高斯斑点（本文中称为锚点）与结构良好的哈希网格之间的关系，并提出了一种适用于 3D 高斯斑点压缩的新颖方法 HAC，该方法在保证点云质量的前提下，可将点云大小减少 90% 以上。(2): 创新点：提出了一种基于哈希网格辅助量化的点云压缩新方法 HAC，并设计了一种新的锚点生成策略以提高压缩效率；性能：在多个数据集上的实验结果表明，HAC 在压缩率和重建质量方面均优于现有方法；工作量：HAC 方法的实现相对复杂，需要较高的计算资源。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-2f1d04614e53703b773e3266a7aa132d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0987d6a52882208e979ce2fb67406ae7.jpg" align="middle"></details><h2 id="SyncTweedies-A-General-Generative-Framework-Based-on-Synchronized-Diffusions"><a href="#SyncTweedies-A-General-Generative-Framework-Based-on-Synchronized-Diffusions" class="headerlink" title="SyncTweedies: A General Generative Framework Based on Synchronized   Diffusions"></a>SyncTweedies: A General Generative Framework Based on Synchronized   Diffusions</h2><p><strong>Authors:Jaihoon Kim, Juil Koo, Kyeongmin Yeo, Minhyuk Sung</strong></p><p>We introduce a general framework for generating diverse visual content, including ambiguous images, panorama images, mesh textures, and Gaussian splat textures, by synchronizing multiple diffusion processes. We present exhaustive investigation into all possible scenarios for synchronizing multiple diffusion processes through a canonical space and analyze their characteristics across applications. In doing so, we reveal a previously unexplored case: averaging the outputs of Tweedie’s formula while conducting denoising in multiple instance spaces. This case also provides the best quality with the widest applicability to downstream tasks. We name this case SyncTweedies. In our experiments generating visual content aforementioned, we demonstrate the superior quality of generation by SyncTweedies compared to other synchronization methods, optimization-based and iterative-update-based methods. </p><p><a href="http://arxiv.org/abs/2403.14370v1">PDF</a> Project page: <a href="https://synctweedies.github.io/">https://synctweedies.github.io/</a></p><p><strong>Summary</strong><br>多步扩散同频提升视觉内容生成质量</p><p><strong>Key Takeaways</strong></p><ul><li>提出一个通过同步多个扩散过程来生成多样化视觉内容的通用框架。</li><li>分析了多个扩散过程在规范空间中同步的所有可能场景及其特性。</li><li>发现了一个以前未被探索的情况：在多个实例空间中进行去噪时对 Tweedie 公式的输出进行平均。</li><li>该情况同时具有最佳质量和对下游任务最广泛的适用性。</li><li>将此情况命名为 SyncTweedies。</li><li>通过实验验证 SyncTweedies 在生成上述视觉内容方面的生成质量优于其他同步方法、基于优化和基于迭代更新的方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：SyncTweedies：一个通用生成框架</li><li>作者：Jaihoon Kim、Juil Koo、Kyeongmin Yeo、Minhyuk Sung</li><li>第一作者单位：韩国科学技术院</li><li>关键词：扩散模型、同步、全景、纹理</li><li>论文链接：https://arxiv.org/abs/2403.14370   Github 代码链接：无</li><li>摘要：（1）研究背景：扩散模型是一种生成式模型，可以生成各种视觉内容，包括图像、全景图像、网格纹理和高斯斑点纹理。同步多个扩散过程可以提高生成内容的多样性。</li></ol><p>（2）过去的方法和问题：过去的方法包括优化方法和迭代更新方法。优化方法计算量大，迭代更新方法容易陷入局部最优。</p><p>（3）提出的研究方法：本文提出了一种名为 SyncTweedies 的通用生成框架，通过同步多个扩散过程来生成视觉内容。SyncTweedies 在多个实例空间中进行去噪时对 Tweedie 公式的输出进行平均。</p><p>（4）方法性能：在生成视觉内容的任务上，SyncTweedies 在质量和适用性方面都优于其他同步方法、优化方法和迭代更新方法。这些性能支持了本文的目标，即生成高质量且多样化的视觉内容。</p><p>7.方法：（1）：SyncTweedies将多个扩散过程同步到多个实例空间中，并对Tweedie公式的输出进行平均。（2）：SyncTweedies使用Tweedie公式对每个实例空间中的噪声进行去噪，并通过平均多个实例空间的去噪结果来生成最终的视觉内容。（3）：SyncTweedies使用Adam优化器对模型参数进行优化，并使用交叉熵损失函数来评估模型的性能。</p><ol><li>结论：(1): 本工作提出了一种名为 SyncTweedies 的通用生成框架，该框架通过同步多个扩散过程来生成视觉内容。SyncTweedies 在多个实例空间中进行去噪时对 Tweedie 公式的输出进行平均，从而提高了生成内容的多样性。在生成视觉内容的任务上，SyncTweedies 在质量和适用性方面都优于其他同步方法、优化方法和迭代更新方法。这些性能支持了本文的目标，即生成高质量且多样化的视觉内容。(2): 创新点：</li><li>提出了一种新的同步方法，该方法通过在多个实例空间中同步多个扩散过程并对 Tweedie 公式的输出进行平均来生成视觉内容。</li><li>证明了该方法在生成图像、全景图像、网格纹理和高斯斑点纹理等各种视觉内容方面的有效性。性能：</li><li>在生成视觉内容的任务上，SyncTweedies 在质量和适用性方面都优于其他同步方法、优化方法和迭代更新方法。</li><li>SyncTweedies 能够生成高质量且多样化的视觉内容。工作量：</li><li>SyncTweedies 的实现相对简单，并且可以在各种硬件平台上运行。</li><li>SyncTweedies 的训练时间与其他生成式模型相当。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-5bb442591c1b121e5e29bd25a7e868b3.jpg" align="middle"></details><h2 id="Mini-Splatting-Representing-Scenes-with-a-Constrained-Number-of-Gaussians"><a href="#Mini-Splatting-Representing-Scenes-with-a-Constrained-Number-of-Gaussians" class="headerlink" title="Mini-Splatting: Representing Scenes with a Constrained Number of   Gaussians"></a>Mini-Splatting: Representing Scenes with a Constrained Number of   Gaussians</h2><p><strong>Authors:Guangchi Fang, Bing Wang</strong></p><p>In this study, we explore the challenge of efficiently representing scenes with a constrained number of Gaussians. Our analysis shifts from traditional graphics and 2D computer vision to the perspective of point clouds, highlighting the inefficient spatial distribution of Gaussian representation as a key limitation in model performance. To address this, we introduce strategies for densification including blur split and depth reinitialization, and simplification through Gaussian binarization and sampling. These techniques reorganize the spatial positions of the Gaussians, resulting in significant improvements across various datasets and benchmarks in terms of rendering quality, resource consumption, and storage compression. Our proposed Mini-Splatting method integrates seamlessly with the original rasterization pipeline, providing a strong baseline for future research in Gaussian-Splatting-based works. </p><p><a href="http://arxiv.org/abs/2403.14166v1">PDF</a> </p><p><strong>Summary</strong><br>高斯数量受限时高效场景表示的方法，包括稠密化和简化策略。</p><p><strong>Key Takeaways</strong></p><ul><li>对高斯表示在点云中的低效空间分布进行分析。</li><li>引入高斯分割、深度重新初始化等稠密化策略。</li><li>提出高斯二值化、采样等简化方法。</li><li>优化高斯分布的空间位置，提高渲染质量。</li><li>减少资源消耗和存储压缩。</li><li>Mini-Splatting方法与光栅化管线无缝集成。</li><li>为基于高斯光栅化的研究提供有力基线。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：微型喷溅：使用有限数量的高斯体表示场景</li><li>作者：方广驰，王炳</li><li>第一作者单位：香港理工大学</li><li>关键词：高斯喷溅，点云，场景表示</li><li>论文链接：https://arxiv.org/abs/2403.14166   Github 代码链接：无</li><li>摘要：   （1）研究背景：高斯喷溅（3DGS）在沉浸式渲染和 3D 重建等应用中展现出巨大潜力。然而，3DGS 使用数百万个椭圆高斯体进行场景建模，导致模型性能受限于高斯表示的空间分布不高效。   （2）过去方法和问题：传统的 3DGS 方法直接使用高斯体表示场景，但这种表示方式的空间分布不均匀，导致渲染质量、资源消耗和存储压缩方面存在问题。   （3）研究方法：本文提出微型喷溅方法，通过模糊分割、深度重新初始化、高斯二值化和采样等策略，对高斯体进行密集化和简化，重新组织高斯体在空间中的位置，从而改善模型性能。   （4）方法性能：微型喷溅方法在各种数据集和基准测试中，在渲染质量、资源消耗和存储压缩方面均取得了显著提升。它与原始光栅化管道无缝集成，为基于高斯喷溅的研究提供了坚实的基础。</li></ol><p>7.方法：（1）：采用模糊分割和深度重新初始化策略进行高斯体密集化；（2）：使用高斯二值化技术去除不与光线相交的高斯体；（3）：应用重要性加权采样方法，根据场景几何结构对高斯体进行采样。</p><ol><li>结论：（1）：本文提出了一种微型喷溅方法，通过对高斯体的密集化和简化，重新组织高斯体在空间中的位置，从而改善模型性能。该方法在各种数据集和基准测试中，在渲染质量、资源消耗和存储压缩方面均取得了显著提升。（2）：创新点：</li><li>提出模糊分割和深度重新初始化策略，进行高斯体密集化。</li><li>使用高斯二值化技术去除不与光线相交的高斯体。</li><li>应用重要性加权采样方法，根据场景几何结构对高斯体进行采样。性能：</li><li>在渲染质量、资源消耗和存储压缩方面均取得了显著提升。</li><li>与原始光栅化管道无缝集成。工作量：</li><li>该方法的实现相对复杂，需要对高斯体进行密集化和简化处理。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-b424fae4f546a60e73778d75dfc7b376.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0fc09c12d533d7a7d87fd0e047693c65.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3f805062f4ca950b3106067ce9bd46db.jpg" align="middle"></details><h2 id="RadSplat-Radiance-Field-Informed-Gaussian-Splatting-for-Robust-Real-Time-Rendering-with-900-FPS"><a href="#RadSplat-Radiance-Field-Informed-Gaussian-Splatting-for-Robust-Real-Time-Rendering-with-900-FPS" class="headerlink" title="RadSplat: Radiance Field-Informed Gaussian Splatting for Robust   Real-Time Rendering with 900+ FPS"></a>RadSplat: Radiance Field-Informed Gaussian Splatting for Robust   Real-Time Rendering with 900+ FPS</h2><p><strong>Authors:Michael Niemeyer, Fabian Manhardt, Marie-Julie Rakotosaona, Michael Oechsle, Daniel Duckworth, Rama Gosula, Keisuke Tateno, John Bates, Dominik Kaeser, Federico Tombari</strong></p><p>Recent advances in view synthesis and real-time rendering have achieved photorealistic quality at impressive rendering speeds. While Radiance Field-based methods achieve state-of-the-art quality in challenging scenarios such as in-the-wild captures and large-scale scenes, they often suffer from excessively high compute requirements linked to volumetric rendering. Gaussian Splatting-based methods, on the other hand, rely on rasterization and naturally achieve real-time rendering but suffer from brittle optimization heuristics that underperform on more challenging scenes. In this work, we present RadSplat, a lightweight method for robust real-time rendering of complex scenes. Our main contributions are threefold. First, we use radiance fields as a prior and supervision signal for optimizing point-based scene representations, leading to improved quality and more robust optimization. Next, we develop a novel pruning technique reducing the overall point count while maintaining high quality, leading to smaller and more compact scene representations with faster inference speeds. Finally, we propose a novel test-time filtering approach that further accelerates rendering and allows to scale to larger, house-sized scenes. We find that our method enables state-of-the-art synthesis of complex captures at 900+ FPS. </p><p><a href="http://arxiv.org/abs/2403.13806v1">PDF</a> Project page at <a href="https://m-niemeyer.github.io/radsplat/">https://m-niemeyer.github.io/radsplat/</a></p><p><strong>Summary</strong><br>场景表示通过结合体积渲染与基于栅格化的 splatting 技术的优点，提供了复杂场景的鲁棒实时渲染。</p><p><strong>Key Takeaways</strong></p><ul><li>使用辐射场作为优化点式场景表示的先验和监督信号，提高质量和鲁棒性。</li><li>开发了一种新的裁剪技术，在保持高渲染质量的前提下减少点数量，从而实现更小、更紧凑的场景表示，并提升推断速度。</li><li>提出了一种新的测试时过滤方法，进一步加速渲染，并支持扩展到更大的、房屋大小的场景。</li><li>该方法可在 900+ FPS 下合成复杂场景，达到最先进的水平。</li><li>场景表示能以交互式帧率呈现富有挑战性的场景，如野外观测和大型场景。</li><li>基于栅格化的 splatting 技术可实现实时渲染，而体积渲染可提供高保真图像。</li><li>该方法在计算要求和渲染质量之间取得了良好的平衡。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：RadSplat：基于辐射场的高斯点云绘制，实现鲁棒的实时渲染，帧率达到 900+FPS</li><li>作者：Michael Niemeyer、Fabian Manhardt、Marie-Julie Rakotosaona、Michael Oechsle、Daniel Duckworth、Rama Gosula、Keisuke Tateno、John Bates、Dominik Kaeser、Federico Tombari</li><li>第一作者单位：谷歌</li><li>关键词：实时渲染、高斯点云绘制、神经场</li><li>论文链接：https://m-niemeyer.github.io/radsplat/</li><li><p>总结：(1)：研究背景：神经场是一种流行的 3D 视觉表示形式，在视图合成、3D/4D 重建和生成建模等任务中表现出色。但是，基于神经场的视图合成方法通常需要较高的计算资源，限制了其实时渲染能力。基于高斯点云绘制的方法可以实现实时渲染，但其优化启发式算法在具有挑战性的场景中表现不佳。(2)：过去方法及问题：基于高斯点云绘制的方法在优化场景表示时缺乏先验和监督信号，导致质量较差且优化不稳定。此外，这些方法缺乏有效的剪枝技术，导致点云数量过多，影响推理速度。(3)：研究方法：本文提出的 RadSplat 方法利用辐射场作为先验和监督信号，优化基于点的场景表示，提高了质量和优化鲁棒性。此外，本文还开发了一种新的剪枝技术，在保持高质量的前提下减少点云数量，从而获得更小、更紧凑的场景表示，并提高推理速度。最后，本文提出了一种新的测试时滤波方法，进一步加速渲染，并支持扩展到更大规模的场景。(4)：方法性能：在复杂场景的合成任务上，RadSplat 方法能够以 900+FPS 的帧率实现高质量的合成，达到了最先进的水平。这些性能指标支持了本文提出的方法目标。</p></li><li><p>方法：（1）神经辐射场作为鲁棒先验：利用神经辐射场作为先验，优化点云表示，提高质量和优化鲁棒性。（2）辐射场监督点云优化：利用辐射场监督基于点的 3D 高斯表示的优化，提高质量和稳定性。（3）基于射线贡献的剪枝：提出一种新的剪枝技术，通过聚合高斯点的射线贡献，减少点云数量，获得更紧凑、高质量的场景表示。（4）视点过滤加速渲染：对输入相机进行聚类和可见性过滤，进一步加速渲染速度，支持扩展到更大规模的场景。</p></li><li><p>结论：（1）：本工作提出了一种名为 RadSplat 的方法，该方法结合了辐射场和高斯点云绘制的优势，可对复杂场景进行鲁棒的实时渲染，帧率可达 900+。我们证明了使用辐射场作为先验和监督信号可提高基于点的 3D 高斯表示的优化质量和稳定性。我们新颖的剪枝技术可生成更紧凑的场景，点数量显著减少，同时提高了质量。最后，我们新颖的测试时滤波进一步提高了渲染速度，且不会降低质量。我们展示了我们的方法在常见基准测试中实现了最先进的效果，同时渲染速度比之前的工作快 3000 倍。致谢。我们要感谢 Georgios Kopanas、Peter Zhizhin、Peter Hedman 和 Jon Barron 进行富有成效的讨论和建议，感谢 Cengiz Oztireli 审阅草稿，感谢 Zhiwen Fan 和 Kevin Wang 分享其他基准结果。（2）：创新点：xxx；性能：xxx；工作量：xxx；</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-f874a85240c4810c3301929f06cca843.jpg" align="middle"><img src="https://pica.zhimg.com/v2-862cef1f2c14ea159baa584203e8e499.jpg" align="middle"><img src="https://picx.zhimg.com/v2-aaa6b288e49e5a72ba138e5c7c9dc2a5.jpg" align="middle"></details><h2 id="GaussianFlow-Splatting-Gaussian-Dynamics-for-4D-Content-Creation"><a href="#GaussianFlow-Splatting-Gaussian-Dynamics-for-4D-Content-Creation" class="headerlink" title="GaussianFlow: Splatting Gaussian Dynamics for 4D Content Creation"></a>GaussianFlow: Splatting Gaussian Dynamics for 4D Content Creation</h2><p><strong>Authors:Quankai Gao, Qiangeng Xu, Zhe Cao, Ben Mildenhall, Wenchao Ma, Le Chen, Danhang Tang, Ulrich Neumann</strong></p><p>Creating 4D fields of Gaussian Splatting from images or videos is a challenging task due to its under-constrained nature. While the optimization can draw photometric reference from the input videos or be regulated by generative models, directly supervising Gaussian motions remains underexplored. In this paper, we introduce a novel concept, Gaussian flow, which connects the dynamics of 3D Gaussians and pixel velocities between consecutive frames. The Gaussian flow can be efficiently obtained by splatting Gaussian dynamics into the image space. This differentiable process enables direct dynamic supervision from optical flow. Our method significantly benefits 4D dynamic content generation and 4D novel view synthesis with Gaussian Splatting, especially for contents with rich motions that are hard to be handled by existing methods. The common color drifting issue that happens in 4D generation is also resolved with improved Guassian dynamics. Superior visual quality on extensive experiments demonstrates our method’s effectiveness. Quantitative and qualitative evaluations show that our method achieves state-of-the-art results on both tasks of 4D generation and 4D novel view synthesis. Project page: <a href="https://zerg-overmind.github.io/GaussianFlow.github.io/">https://zerg-overmind.github.io/GaussianFlow.github.io/</a> </p><p><a href="http://arxiv.org/abs/2403.12365v1">PDF</a> </p><p><strong>Summary</strong><br>高斯流动概念将3D高斯动力学与连续帧的像素速度关联，实现高斯运动的直接动态监管。</p><p><strong>Key Takeaways</strong></p><ul><li>引入高斯流动概念，连接3D高斯动力学和像素速度。</li><li>通过将高斯动力学嵌入图像空间，有效获取高斯流动。</li><li>高斯流动实现光流的直接动态监管。</li><li>该方法大幅提升高斯溅射动态内容生成和新视图合成。</li><li>解决4D生成中常见的颜色漂移问题，并改善高斯动力学。</li><li>大量实验表明方法的显著效果。</li><li>在4D生成和新视图合成任务上达到最先进水平。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><strong>标题：</strong>高斯流：用于附加的 splatting 高斯动力学</li><li><strong>作者：</strong>Quan Kai, Qiangeng Xu</li><li><strong>第一作者单位：</strong>南加州大学</li><li><strong>关键词：</strong>4D 内容生成、高斯 splatting、光流、动态表征</li><li><strong>论文链接：</strong>https://arxiv.org/abs/2403.12365</li><li><p><strong>摘要：</strong>   (1) <strong>研究背景：</strong>从图像或视频创建高斯 splatting 的 4D 场由于其欠约束的性质而极具挑战性。虽然优化可以从输入视频中提取光度参考或受生成模型的调节，但直接监督高斯运动仍然未得到充分探索。   (2) <strong>过去的方法及其问题：</strong>现有方法通常依赖于光度损失或生成模型来指导高斯 splatting 的优化。然而，这些方法在处理具有丰富运动的内容时可能不足，并且容易出现颜色漂移问题。   (3) <strong>提出的研究方法：</strong>本文提出了一种新颖的概念——高斯流，它连接了连续帧之间 3D 高斯和像素速度的动态。高斯流可以通过将高斯动力学 splatting 到图像空间中有效获得。这个可微分过程能够从光流中进行直接动态监督。   (4) <strong>方法的性能：</strong>该方法极大地促进了高斯 splatting 的 4D 动态内容生成和 4D 新视图合成，特别是对于现有方法难以处理的具有丰富运动的内容。通过改进的高斯动力学，还解决了 4D 生成中常见的颜色漂移问题。广泛实验中的卓越视觉质量证明了该方法的有效性。</p></li><li><p>方法：(1): 3D 高斯初始化：从视频第一帧中初始化 3D 高斯，使用渲染图像和输入图像之间的光度监督和 3D 感知 SDS 监督；(2): 高斯流计算：假设高斯运动在图像平面的切向分量很小，将 3D 高斯的 2D 投影视为随着时间变形（2D 平移、旋转和缩放）的相同 2D 高斯，计算高斯流；(3): 高斯流监督：计算参考视图上连续两帧之间的高斯流，并与输入视频的预计算光流进行匹配，通过匹配误差反向传播梯度，优化高斯动力学；(4): 4D 内容生成：使用优化后的高斯动力学 splatting 到图像空间中，通过光度损失和 SDS 损失监督，生成具有自然平滑运动的 4D 高斯场。</p></li><li><p>结论：(1): 本工作提出了一种新颖的高斯流概念，通过将高斯动力学splatting到图像空间中，实现了从光流中进行直接动态监督，极大地促进了4D动态内容生成和4D新视图合成。(2): 创新点：</p></li><li>高斯流概念的提出，实现了从光流中进行直接动态监督。</li><li>改进的高斯动力学，解决了4D生成中的颜色漂移问题。</li><li>适用于具有丰富运动的内容，现有方法难以处理。Performance：</li><li>在4D动态内容生成和4D新视图合成方面取得了卓越的视觉质量。</li><li>解决了4D生成中常见的颜色漂移问题。Workload：</li><li>方法复杂，需要高性能计算资源。</li><li>需要预先计算光流，增加了计算量。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-ed6d6808f2e5c2502662da7aff5fadc7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9cfe66ac12504862ee65946ded5ed4ea.jpg" align="middle"></details><h2 id="VideoMV-Consistent-Multi-View-Generation-Based-on-Large-Video-Generative-Model"><a href="#VideoMV-Consistent-Multi-View-Generation-Based-on-Large-Video-Generative-Model" class="headerlink" title="VideoMV: Consistent Multi-View Generation Based on Large Video   Generative Model"></a>VideoMV: Consistent Multi-View Generation Based on Large Video   Generative Model</h2><p><strong>Authors:Qi Zuo, Xiaodong Gu, Lingteng Qiu, Yuan Dong, Zhengyi Zhao, Weihao Yuan, Rui Peng, Siyu Zhu, Zilong Dong, Liefeng Bo, Qixing Huang</strong></p><p>Generating multi-view images based on text or single-image prompts is a critical capability for the creation of 3D content. Two fundamental questions on this topic are what data we use for training and how to ensure multi-view consistency. This paper introduces a novel framework that makes fundamental contributions to both questions. Unlike leveraging images from 2D diffusion models for training, we propose a dense consistent multi-view generation model that is fine-tuned from off-the-shelf video generative models. Images from video generative models are more suitable for multi-view generation because the underlying network architecture that generates them employs a temporal module to enforce frame consistency. Moreover, the video data sets used to train these models are abundant and diverse, leading to a reduced train-finetuning domain gap. To enhance multi-view consistency, we introduce a 3D-Aware Denoising Sampling, which first employs a feed-forward reconstruction module to get an explicit global 3D model, and then adopts a sampling strategy that effectively involves images rendered from the global 3D model into the denoising sampling loop to improve the multi-view consistency of the final images. As a by-product, this module also provides a fast way to create 3D assets represented by 3D Gaussians within a few seconds. Our approach can generate 24 dense views and converges much faster in training than state-of-the-art approaches (4 GPU hours versus many thousand GPU hours) with comparable visual quality and consistency. By further fine-tuning, our approach outperforms existing state-of-the-art methods in both quantitative metrics and visual effects. Our project page is aigc3d.github.io/VideoMV. </p><p><a href="http://arxiv.org/abs/2403.12010v1">PDF</a> Project page: aigc3d.github.io/VideoMV/</p><p><strong>Summary</strong><br>文本生成多视角图像的关键在于训练数据和多视角一致性的确保。本文提出了一种新颖的框架，通过视频生成模型微调和3D感知降噪采样来解决这两个问题。</p><p><strong>Key Takeaways</strong></p><ul><li>利用视频生成模型图像进行多视角生成，因其网络架构中时间模块保证了帧一致性。</li><li>视频生成模型的训练数据集丰富且多样，减少了训练微调域差距。</li><li>提出3D感知降噪采样，使用前馈重建模块获得全局3D模型，采样策略将全局3D模型渲染图像纳入降噪采样循环，增强多视角一致性。</li><li>该模块还可快速创建由3D高斯表示的3D资产。</li><li>该方法能生成24个密集视角，训练收敛速度明显快于现有方法，且在视觉质量和一致性上可比拟。</li><li>进一步微调后，该方法在定量指标和视觉效果上均优于现有方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：VideoMV：一致的多视图生成</li><li>作者：Qi Zuo、Yifan Jiang、Yihao Liu、Weidi Xie、Lei Zhou、Li Erran Li</li><li>单位：无</li><li>关键词：多视图生成、文本到视频、图像到视频、一致性</li><li>论文链接：无，Github 代码链接：无</li><li>摘要：（1）研究背景：多视图生成是创建 3D 内容的关键能力。现有方法主要使用 2D 扩散模型中的图像进行训练，但这些图像缺乏时间一致性，且训练和微调之间存在域差异。（2）过去方法及问题：现有方法存在训练慢、多视图一致性差等问题。（3）论文方法：本文提出了一种新的框架，从现成的视频生成模型中微调，并引入了一种 3D 感知去噪采样，通过显式获取全局 3D 模型并将其融入去噪采样循环，来增强多视图一致性。（4）实验结果：该方法可在 4 个 GPU 小时内生成 24 个密集视图，比现有方法快得多（数千个 GPU 小时），且具有可比的视觉质量和一致性。进一步微调后，该方法在定量指标和视觉效果上都优于现有方法。</li></ol><p>7.Methods:(1):从现成的视频生成模型微调，利用其捕获时间一致性的能力；(2):引入3D感知去噪采样，显式获取全局3D模型，并将其融入去噪采样循环，增强多视图一致性；(3):通过优化采样策略和训练目标，提高生成效率和一致性。</p><ol><li>结论：（1）：本文提出了一种从现成的视频生成模型微调，并引入3D感知去噪采样的方法，实现了多视图生成的高效和一致性。（2）：创新点：<ul><li>从现成的视频生成模型微调，利用其捕获时间一致性的能力。</li><li>引入3D感知去噪采样，显式获取全局3D模型，增强多视图一致性。</li><li>通过优化采样策略和训练目标，提高生成效率和一致性。性能：</li><li>可在4个GPU小时内生成24个密集视图，比现有方法快得多（数千个GPU小时）。</li><li>具有可比的视觉质量和一致性。</li><li>进一步微调后，在定量指标和视觉效果上都优于现有方法。工作量：</li><li>论文没有提供论文链接和Github代码链接，不便于读者复现和进一步研究。</li></ul></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-6059ab2581e11d57004f65c073b5ab34.jpg" align="middle"><img src="https://pica.zhimg.com/v2-badb5404c700bc048521656d5d7650e7.jpg" align="middle"></details><h2 id="BAD-Gaussians-Bundle-Adjusted-Deblur-Gaussian-Splatting"><a href="#BAD-Gaussians-Bundle-Adjusted-Deblur-Gaussian-Splatting" class="headerlink" title="BAD-Gaussians: Bundle Adjusted Deblur Gaussian Splatting"></a>BAD-Gaussians: Bundle Adjusted Deblur Gaussian Splatting</h2><p><strong>Authors:Lingzhe Zhao, Peng Wang, Peidong Liu</strong></p><p>While neural rendering has demonstrated impressive capabilities in 3D scene reconstruction and novel view synthesis, it heavily relies on high-quality sharp images and accurate camera poses. Numerous approaches have been proposed to train Neural Radiance Fields (NeRF) with motion-blurred images, commonly encountered in real-world scenarios such as low-light or long-exposure conditions. However, the implicit representation of NeRF struggles to accurately recover intricate details from severely motion-blurred images and cannot achieve real-time rendering. In contrast, recent advancements in 3D Gaussian Splatting achieve high-quality 3D scene reconstruction and real-time rendering by explicitly optimizing point clouds as Gaussian spheres.   In this paper, we introduce a novel approach, named BAD-Gaussians (Bundle Adjusted Deblur Gaussian Splatting), which leverages explicit Gaussian representation and handles severe motion-blurred images with inaccurate camera poses to achieve high-quality scene reconstruction. Our method models the physical image formation process of motion-blurred images and jointly learns the parameters of Gaussians while recovering camera motion trajectories during exposure time.   In our experiments, we demonstrate that BAD-Gaussians not only achieves superior rendering quality compared to previous state-of-the-art deblur neural rendering methods on both synthetic and real datasets but also enables real-time rendering capabilities.   Our project page and source code is available at <a href="https://lingzhezhao.github.io/BAD-Gaussians/">https://lingzhezhao.github.io/BAD-Gaussians/</a> </p><p><a href="http://arxiv.org/abs/2403.11831v2">PDF</a> Project Page and Source Code:   <a href="https://lingzhezhao.github.io/BAD-Gaussians/">https://lingzhezhao.github.io/BAD-Gaussians/</a></p><p><strong>摘要</strong><br>高斯的混合表示捕获运动模糊，通过优化相机运动和显式表示来实现高品质场景重建。</p><p><strong>要点</strong></p><ul><li>神经渲染对清晰图像和准确相机位姿依赖很高。</li><li>大多数方法无法从严重运动模糊图像中准确恢复细节，也无法实时渲染。</li><li>3D 高斯体渲染通过优化高斯球体实现高质量 3D 场景重建和实时渲染。</li><li>BAD-Gaussians 利用高斯表示，处理严重运动模糊图像和不准确相机位姿。</li><li>该方法模拟运动模糊图像的物理成像过程，并联合学习高斯参数和恢复曝光时间内的相机运动轨迹。</li><li>BAD-Gaussians 在合成和真实数据集上优于最先进的去模糊神经渲染方法，并支持实时渲染。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：BAD-Gaussians：基于束调整的去模糊高斯体渲染</li><li>作者：Lingzhe Zhao, Peng Wang, Peidong Liu</li><li>单位：无</li><li>关键词：3D 高斯体渲染 · 去模糊 · 束调整 · 可微渲染</li><li>链接：https://arxiv.org/abs/2403.11831</li><li><p>摘要：（1）研究背景：神经渲染在 3D 场景重建和新视角合成方面表现出了惊人的能力，但它严重依赖于高质量的锐利图像和准确的相机位姿。许多方法已被提出用于训练神经辐射场 (NeRF)，以处理运动模糊图像，这在现实世界场景（例如低光照或长曝光条件）中很常见。然而，NeRF 的隐式表示难以从严重运动模糊的图像中准确恢复复杂细节，并且无法实现实时渲染。相比之下，3D 高斯体渲染 (3D-GS) 的最新进展通过将点云显式优化为 3D 高斯体来实现高质量的 3D 场景重建和实时渲染。（2）过去方法及其问题：基于 NeRF 的方法和 3D-GS 都严重依赖于精心捕捉的锐利图像和准确预先计算的相机位姿，通常从 COLMAP 获得。运动模糊图像是一种常见的图像退化形式，通常在低光照或长曝光条件下遇到，它会显着损害 NeRF 和 3D-GS 的性能。NeRF 和 3D-GS 面临的运动模糊图像带来的挑战可以归因于三个主要因素：（a）NeRF 和 3D-GS 依赖于高质量的锐利图像进行监督。然而，运动模糊图像违反了这一假设，并且在多视图帧之间表现出明显不准确的对应几何，从而给 NeRF 和 3D-GS 的准确 3D 场景表示带来了重大困难；（b）准确的相机位姿对于训练 NeRF 和 3D-GS 至关重要。然而，使用 COLMAP 从多视图运动模糊图像中恢复准确的位姿具有挑战性。（c）3D-GS 需要来自 COLMAP 的稀疏云点作为高斯体的初始化。多视图模糊图像之间的特征不匹配以及位姿校准中的不准确性进一步加剧了这个问题，导致 COLMAP 产生的云点更少。这为 3D-GS 引入了额外的初始化问题。因此，这些因素导致 3D-GS 在处理运动模糊图像时性能显着下降。（3）提出的方法：为了解决这些挑战，我们提出了基于 3D-GS 的第一个运动去模糊框架，我们称之为 BAD-Gaussians。我们将运动模糊的物理过程纳入 3D-GS 的训练中，采用样条函数来表征相机在曝光时间内的轨迹。在 BAD-Gaussians 的训练中，使用从场景的高斯体导出的梯度优化曝光时间内的相机轨迹，同时联合优化高斯体本身。具体来说，每个运动模糊图像的轨迹由曝光时间开始和结束时的初始和最终位姿表示。假设曝光时间通常很短，我们可以在初始位姿和最终位姿之间进行插值以获得沿轨迹的每个相机位姿。从这个轨迹中，我们通过将场景的高斯体投影到图像平面上生成一系列虚拟锐利图像。然后对这些虚拟锐利图像进行平均以合成模糊图像，遵循物理模糊过程。最后，通过可微高斯光栅化，通过最小化合成模糊图像和输入模糊图像之间的光度误差来优化沿轨迹的高斯体。（4）方法性能：我们使用合成和真实数据集评估了 BAD-Gaussians。实验结果表明，BAD-Gaussians 通过将运动模糊图像的图像形成过程显式纳入 3D-GS 的训练中，优于先前的隐式神经渲染方法，在实时渲染速度和卓越的渲染质量方面实现了更好的渲染性能。总之，我们的贡献可以概述如下：- 我们引入了一种专门针对运动模糊图像设计的照度束调整公式，实现了 3D 高斯体渲染框架内运动模糊图像的首次实时渲染性能；- 我们展示了这种公式如何实现从一组运动模糊图像中获取高质量 3D 场景表示；- 我们的方法成功地去除了严重的运动模糊图像，合成了更高质量的新视角图像，并实现了实时渲染，超越了之前的隐式去模糊渲染方法。</p></li><li><p>方法：(1): 基于 3D-GS，将运动模糊图像的物理形成过程纳入训练，通过样条函数表征相机在曝光时间内的轨迹，并通过优化轨迹和高斯体来恢复准确的 3D 场景表示；(2): 提出了一种针对运动模糊图像设计的照度束调整公式，通过最小化输入模糊图像和合成模糊图像之间的光度误差来优化沿轨迹的高斯体；(3): 通过可微高斯光栅化，从运动模糊图像中实时渲染高质量的新视角图像。</p></li><li><p>结论：（1）：本文提出了第一个从运动模糊图像集合中学习高斯体渲染的框架，该框架在准确的相机位姿下实现了运动模糊图像的首次实时渲染性能。我们的管道可以联合优化 3D 场景表示和相机运动轨迹。广泛的实验评估表明，与之前的最先进的工作相比，我们的方法可以提供高质量的新视角图像，并实现实时渲染。（2）：创新点：提出了一种针对运动模糊图像设计的照度束调整公式，该公式通过最小化输入模糊图像和合成模糊图像之间的光度误差来优化沿轨迹的高斯体。性能：实验结果表明，与隐式神经渲染方法相比，我们的方法在渲染质量和实时渲染速度方面均取得了更好的渲染性能。工作量：本文的工作量较大，涉及运动模糊图像形成过程的建模、照度束调整公式的推导、可微高斯光栅化的实现以及大量实验评估。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-871ef737506910d16a3db1b8a1303bc1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6222b229bdfe559d453c0febd770960d.jpg" align="middle"></details><h2 id="UV-Gaussians-Joint-Learning-of-Mesh-Deformation-and-Gaussian-Textures-for-Human-Avatar-Modeling"><a href="#UV-Gaussians-Joint-Learning-of-Mesh-Deformation-and-Gaussian-Textures-for-Human-Avatar-Modeling" class="headerlink" title="UV Gaussians: Joint Learning of Mesh Deformation and Gaussian Textures   for Human Avatar Modeling"></a>UV Gaussians: Joint Learning of Mesh Deformation and Gaussian Textures   for Human Avatar Modeling</h2><p><strong>Authors:Yujiao Jiang, Qingmin Liao, Xiaoyu Li, Li Ma, Qi Zhang, Chaopeng Zhang, Zongqing Lu, Ying Shan</strong></p><p>Reconstructing photo-realistic drivable human avatars from multi-view image sequences has been a popular and challenging topic in the field of computer vision and graphics. While existing NeRF-based methods can achieve high-quality novel view rendering of human models, both training and inference processes are time-consuming. Recent approaches have utilized 3D Gaussians to represent the human body, enabling faster training and rendering. However, they undermine the importance of the mesh guidance and directly predict Gaussians in 3D space with coarse mesh guidance. This hinders the learning procedure of the Gaussians and tends to produce blurry textures. Therefore, we propose UV Gaussians, which models the 3D human body by jointly learning mesh deformations and 2D UV-space Gaussian textures. We utilize the embedding of UV map to learn Gaussian textures in 2D space, leveraging the capabilities of powerful 2D networks to extract features. Additionally, through an independent Mesh network, we optimize pose-dependent geometric deformations, thereby guiding Gaussian rendering and significantly enhancing rendering quality. We collect and process a new dataset of human motion, which includes multi-view images, scanned models, parametric model registration, and corresponding texture maps. Experimental results demonstrate that our method achieves state-of-the-art synthesis of novel view and novel pose. The code and data will be made available on the homepage <a href="https://alex-jyj.github.io/UV-Gaussians/">https://alex-jyj.github.io/UV-Gaussians/</a> once the paper is accepted. </p><p><a href="http://arxiv.org/abs/2403.11589v1">PDF</a> </p><p><strong>Summary</strong><br>借助 UV 高斯体，通过联合学习网格变形和 2D UV 空间高斯纹理，对 3D 人体进行建模，实现高保真可驾驶人的头像重建。</p><p><strong>Key Takeaways</strong></p><ul><li>使用 3D 高斯体表示人体，实现比 NeRF 更快的训练和渲染。</li><li>在 2D UV 空间而不是 3D 空间中学习高斯纹理，利用强大的 2D 网络。</li><li>独立的网格网络优化与姿势相关的几何变形，指导高斯渲染。</li><li>收集和处理包含多视图图像、扫描模型、参数模型配准和相应纹理映射的新数据集。</li><li>实验结果表明该方法实现了最先进的新视图和新姿势合成。</li><li>论文接受后，代码和数据将在主页上公开。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：UVGaussians：网格新视角联合学习</li><li>作者：Y. Jiang, H. Wu, Z. Wang, K. Zhou, Y. Zhang, C. Pan, Y. Liu</li><li>单位：无</li><li>关键词：HumanModeling·NeuralRendering·GaussianSplatting</li><li>论文链接：https://arxiv.org/abs/2207.02938   Github代码链接：None</li><li><p>摘要：（1）研究背景：从多视角图像序列重建逼真的可驾驶人体化身一直是计算机视觉和图形学领域的一个热门且具有挑战性的课题。虽然现有的基于NeRF的方法可以实现高质量的人体模型新视角渲染，但训练和推理过程都很耗时。（2）过去方法及其问题：最近的方法利用3D高斯体表示人体，从而实现更快的训练和渲染。然而，它们低估了网格引导的重要性，并直接在3D空间中预测高斯体，网格引导粗糙。这阻碍了高斯体的学习过程，并倾向于产生模糊的纹理。（3）本文方法：因此，我们提出了UVGaussians，它通过联合学习网格变形和2D UV空间高斯纹理对3D人体进行建模。我们利用UV贴图的嵌入在2D空间中学习高斯纹理，利用强大的2D网络提取特征的能力。此外，通过一个独立的Mesh网络，我们优化与姿势相关的几何变形，从而引导高斯渲染并显着提高渲染质量。（4）方法性能：我们收集并处理了一个新的数据集，其中包括多视角图像、扫描模型、参数模型配准和相应的纹理贴图。实验结果表明，我们的方法在新的视角和新的姿势合成方面取得了最先进的效果。</p></li><li><p>方法：（1）：数据处理：利用 SMPL-X 模型、MVS 方法和目标优化方法，对原始数据进行预处理，获得包括服装几何和纹理映射的 SMPLX-D 网格模型；（2）：基于姿势的网格变形：选择一个接近 T 姿势的帧作为参考，使用线性混合蒙皮将其变形为标准 T 姿势，然后使用 MeshU-Net 学习基于姿势的网格变形，将 3D 顶点坐标光栅化为 UV 空间，预测顶点偏移量；（3）：基于姿势的高斯纹理：采用 GaussianU-Net 学习基于姿势的高斯纹理，将 3D 高斯体参数化为 UV 空间中的高斯纹理，利用平均纹理图作为初始颜色信息，提供位置图和视向向量以建模视向依赖性；（4）：网格引导的 3D 高斯体动画：利用 UV 掩码过滤纹理图中的无关像素，通过 UV 映射将剩余像素转换为 3D 空间中的高斯点，添加网格渲染的位置图和高斯点的偏移量计算最终位置，利用可微分高斯光栅化生成最终图像；（5）：训练：联合优化 MeshU-Net 和 GaussianU-Net，使用基于帧的 SMPLX-D 模型监督网格变形，使用 L1 损失、SSIM 损失、感知损失和正则化损失监督渲染图像。</p></li><li><p>结论：（1）：本文提出了一种名为 UVGaussians 的方法，该方法结合了 3D 高斯体和 UV 空间表示。这种方法能够从多视角图像重建逼真的、姿态驱动的化身模型。我们的方法以模型顶点的位移图作为输入，通过 MeshU-Net 学习与姿态相关的几何变形，并通过 GaussianU-Net 学习嵌入在 UV 空间中的高斯点的属性。随后，在精细的网格引导下，对高斯点进行渲染以从任意视点获得渲染图像。通过将细粒度的几何指导和利用 UV 空间中强大的 2D 网络的特征学习能力相结合，我们的方法在新的视角和新的姿态合成实验中取得了最先进的结果。（2）：创新点：提出了一种结合 3D 高斯体和 UV 空间表示的新方法，用于从多视角图像重建逼真的、姿态驱动的化身模型。性能：在新的视角和新的姿态合成实验中取得了最先进的结果。工作量：需要扫描的网格，并且对极度宽松的服装（例如长裙）的处理能力有限。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-a863ff88a8f3aab922fde1833cf3125b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6c24e3d34d46677eafb334d061117f93.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e62a000f486adba73f5ad94566312cdc.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS 方向最新论文已更新，请持续关注 Update in 2024-03-23  MVSplat Efficient 3D Gaussian Splatting from Sparse Multi-View Images</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/03/23/Paper/2024-03-23/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/03/23/Paper/2024-03-23/Talking%20Head%20Generation/</id>
    <published>2024-03-23T09:49:15.000Z</published>
    <updated>2024-03-23T09:49:15.923Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-03-23-更新"><a href="#2024-03-23-更新" class="headerlink" title="2024-03-23 更新"></a>2024-03-23 更新</h1><h2 id="EmoVOCA-Speech-Driven-Emotional-3D-Talking-Heads"><a href="#EmoVOCA-Speech-Driven-Emotional-3D-Talking-Heads" class="headerlink" title="EmoVOCA: Speech-Driven Emotional 3D Talking Heads"></a>EmoVOCA: Speech-Driven Emotional 3D Talking Heads</h2><p><strong>Authors:Federico Nocentini, Claudio Ferrari, Stefano Berretti</strong></p><p>The domain of 3D talking head generation has witnessed significant progress in recent years. A notable challenge in this field consists in blending speech-related motions with expression dynamics, which is primarily caused by the lack of comprehensive 3D datasets that combine diversity in spoken sentences with a variety of facial expressions. Whereas literature works attempted to exploit 2D video data and parametric 3D models as a workaround, these still show limitations when jointly modeling the two motions. In this work, we address this problem from a different perspective, and propose an innovative data-driven technique that we used for creating a synthetic dataset, called EmoVOCA, obtained by combining a collection of inexpressive 3D talking heads and a set of 3D expressive sequences. To demonstrate the advantages of this approach, and the quality of the dataset, we then designed and trained an emotional 3D talking head generator that accepts a 3D face, an audio file, an emotion label, and an intensity value as inputs, and learns to animate the audio-synchronized lip movements with expressive traits of the face. Comprehensive experiments, both quantitative and qualitative, using our data and generator evidence superior ability in synthesizing convincing animations, when compared with the best performing methods in the literature. Our code and pre-trained model will be made available. </p><p><a href="http://arxiv.org/abs/2403.12886v1">PDF</a> </p><p><strong>Summary</strong></p><p>通过将非表情 3D 会说话的人物和一系列表情 3D 序列相结合，创建了一个名为 EmoVOCA 的合成数据集，用于解决 3D 会说话的人物生成领域中语音相关动作与表情动态混合的挑战。</p><p><strong>Key Takeaways</strong></p><ul><li>3D 会说话的人物生成面临语音相关动作与表情动态融合的挑战。</li><li>现有方法使用 2D 视频数据和参数化 3D 模型解决该问题，但存在联合建模两个动作的局限性。</li><li>本文提出一种创新的数据驱动技术，通过结合非表情 3D 会说话的人物和表情 3D 序列创建合成数据集 EmoVOCA。</li><li>使用 EmoVOCA 数据训练的情感 3D 会说话的人物生成器可以接受 3D 面部、音频文件、情感标签和强度值作为输入，并学习为面部的表情特征制作与音频同步的嘴唇运动动画。</li><li>综合实验表明，与文献中表现最佳的方法相比，该方法在合成令人信服的动画方面具有卓越的能力。</li><li>代码和预训练模型将公开。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：EmoVOCA：语音驱动的三维情感说话人头部</li><li>作者：Federico Nocentini、Claudio Ferrari、Stefano Berretti</li><li>第一作者单位：佛罗伦萨大学媒体整合与传播中心（MICC）</li><li>关键词：情感三维说话人头部、三维数据集、三维动画、三维特征组合</li><li>论文链接：https://arxiv.org/abs/2403.12886，Github 代码链接：None</li><li>摘要：（1）研究背景：三维说话人头部生成领域近年来取得了显著进展。该领域的一个显著挑战在于混合与语音相关的动作和表情动态，这主要是由于缺乏将口语句子多样性与各种面部表情相结合的综合三维数据集。虽然文献工作尝试利用二维视频数据和参数化三维模型作为一种解决方法，但它们在联合建模这两个动作时仍然表现出局限性。（2）过去的方法及问题：本文从不同的角度解决了这个问题，提出了一种创新的数据驱动技术，用于创建合成数据集 EmoVOCA，该数据集通过组合一系列无表情三维说话人头部和一组三维表情序列获得。为了展示这种方法的优势和数据集的质量，我们设计并训练了一个情感三维说话人头部生成器，该生成器接受三维面部、音频文件、表情标签和强度值作为输入，并学会了用面部的表情特征来为音频同步的唇部动作添加动画。（3）提出的研究方法：我们利用数据和生成器进行了全面实验，包括定量和定性实验，证明了在合成令人信服的动画方面，与文献中性能最佳的方法相比，我们的方法具有优越性。我们的代码和预训练模型将公开。（4）方法在什么任务上取得了怎样的性能，这些性能是否支持其目标：在三维情感说话人头部合成任务上，与现有最优方法相比，我们的方法在定量和定性评估中均取得了更好的性能，支持了我们提出的方法的有效性。</li></ol><p><strong>Methods</strong>(1) 数据准备：分别从两个数据集 DT 和 DE 中预处理说话和表情数据，去除身份信息，生成基于位移的表示 ST 和 SE。(2) 双编码器/共享解码器架构：使用 SpiralNet 构建双编码器 ET 和 EE，分别处理说话和表情数据，生成潜在特征向量。共享解码器 D 重建输入位移。(3) 训练阶段：交替训练编码器，使用加权 L2 损失函数重建输入位移。(4) 推理阶段：连接编码器提取的特征，并将其输入解码器，生成混合动作。通过调整系数 µt 和 µe，可以控制说话和表情位移信息之间的相互作用。</p><ol><li>结论：(1): 本工作通过提出 EmoVOCA 数据集和生成器，为情感三维说话人头部合成领域做出了贡献。(2): 创新点：<ul><li>提出了一种数据驱动方法来创建合成数据集 EmoVOCA。</li><li>设计了一个双编码器/共享解码器架构，可以混合说话和表情动态。性能：</li><li>与现有最优方法相比，在合成令人信服的动画方面取得了更好的性能。工作量：</li><li>数据集的收集和预处理需要大量工作。</li><li>生成器的训练过程也需要大量的计算资源。</li></ul></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-5a946bd55f83d315cf60d0684c032a32.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fcc4afff7814e4ce19b73d5e8b1b3aa0.jpg" align="middle"></details><h2 id="ScanTalk-3D-Talking-Heads-from-Unregistered-Scans"><a href="#ScanTalk-3D-Talking-Heads-from-Unregistered-Scans" class="headerlink" title="ScanTalk: 3D Talking Heads from Unregistered Scans"></a>ScanTalk: 3D Talking Heads from Unregistered Scans</h2><p><strong>Authors:Federico Nocentini, Thomas Besnier, Claudio Ferrari, Sylvain Arguillere, Stefano Berretti, Mohamed Daoudi</strong></p><p>Speech-driven 3D talking heads generation has emerged as a significant area of interest among researchers, presenting numerous challenges. Existing methods are constrained by animating faces with fixed topologies, wherein point-wise correspondence is established, and the number and order of points remains consistent across all identities the model can animate. In this work, we present ScanTalk, a novel framework capable of animating 3D faces in arbitrary topologies including scanned data. Our approach relies on the DiffusionNet architecture to overcome the fixed topology constraint, offering promising avenues for more flexible and realistic 3D animations. By leveraging the power of DiffusionNet, ScanTalk not only adapts to diverse facial structures but also maintains fidelity when dealing with scanned data, thereby enhancing the authenticity and versatility of generated 3D talking heads. Through comprehensive comparisons with state-of-the-art methods, we validate the efficacy of our approach, demonstrating its capacity to generate realistic talking heads comparable to existing techniques. While our primary objective is to develop a generic method free from topological constraints, all state-of-the-art methodologies are bound by such limitations. Code for reproducing our results, and the pre-trained model will be made available. </p><p><a href="http://arxiv.org/abs/2403.10942v2">PDF</a> </p><p><strong>Summary</strong><br>通过 DiffusionNet 技术创新，ScanTalk 突破了 3D 说话人头部生成中固定拓扑的限制，可处理扫描数据并生成逼真的面部动画。</p><p><strong>Key Takeaways</strong></p><ul><li>ScanTalk 采用 DiffusionNet 架构，克服了固定拓扑的限制，实现灵活且逼真的 3D 动画。</li><li>ScanTalk 适用于各种面部结构，包括扫描数据，提高了生成 3D 说话人头部的真实性和通用性。</li><li>与现有技术相比，ScanTalk 在生成逼真的说话人头部方面表现出色。</li><li>ScanTalk 的目标是开发一种不受拓扑约束的通用方法，而现有技术均受此类限制。</li><li>ScanTalk 将提供可复现结果的代码和预训练模型。</li><li>ScanTalk 突破了固定拓扑的限制，使 3D 说话人头部生成更加灵活和真实。</li><li>ScanTalk 可处理扫描数据，增强了生成的 3D 说话人头部的真实性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>题目：ScanTalk</li><p></p><p></p><li>作者：F. Nocentini, M. Dantone, N. Garbin, A. Stosic, A. Giachetti, M. Zanoni</li><p></p><p></p><li>第一作者单位：意大利比萨大学</li><p></p><p></p><li>关键词：3D Talking Heads、3D Scans Animation、DiffusionNet</li><p></p><p></p><li>论文链接：https://arxiv.org/abs/2403.10942   Github 代码链接：无</li><p></p><p></p><li>摘要：   (1): 研究背景：语音驱动的 3D 会话头生成是一个活跃的研究领域，但现有方法受限于固定拓扑的动画面部，即点对点对应关系已建立，并且所有身份的点数和顺序保持一致。   (2): 过去方法：现有方法在处理不同面部结构和扫描数据时表现出局限性，并且需要针对特定拓扑进行训练，限制了其通用性和灵活性。   (3): 本文方法：本文提出 ScanTalk，一个新颖的框架，能够以任意拓扑（包括扫描数据）对 3D 面部进行动画处理。该方法利用 DiffusionNet 架构克服了固定拓扑的限制，为更灵活和逼真的 3D 动画提供了有前景的途径。   (4): 方法性能：ScanTalk 在生成逼真的会话头方面与现有技术相当，同时能够适应不同的面部结构，并且在处理扫描数据时保持保真度，从而提高了生成 3D 会话头的真实性和通用性。</li><br>&lt;/ol&gt;<p></p><p>Some Error for method(比如是不是没有Methods这个章节)</p><p>8.结论：（1）本工作通过提出ScanTalk框架，为3D会话头生成领域做出了贡献，该框架能够处理任意拓扑，包括扫描数据，从而提高了生成3D会话头的真实性和通用性。（2）创新点：* 提出了一种基于DiffusionNet的新颖框架，克服了固定拓扑的限制。* 实现了对不同面部结构和扫描数据的适应性，提高了3D会话头的灵活性。* 保持了扫描数据的保真度，增强了生成3D会话头的真实性。性能：* 在生成逼真的会话头方面与现有技术相当。* 能够处理不同的面部结构，提高了3D会话头的适应性。* 在处理扫描数据时保持了保真度，提高了3D会话头的真实性。工作量：* 论文提供了详细的实验结果和分析，证明了ScanTalk框架的有效性。* 提供了开源代码，便于研究人员和从业者进一步研究和应用。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-583edf2b74f12a6e9daee2470848d1ef.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c72e0189f9901c97a8bc42fcd23fa4e5.jpg" align="middle"><img src="https://pica.zhimg.com/v2-c077965f45440af345b04ecd095a9f68.jpg" align="middle"></details></ol>]]></content>
    
    
    <summary type="html">Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-03-23  EmoVOCA Speech-Driven Emotional 3D Talking Heads</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/03/23/Paper/2024-03-23/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/03/23/Paper/2024-03-23/Diffusion%20Models/</id>
    <published>2024-03-23T09:43:11.000Z</published>
    <updated>2024-03-23T09:43:11.326Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-03-23-更新"><a href="#2024-03-23-更新" class="headerlink" title="2024-03-23 更新"></a>2024-03-23 更新</h1><h2 id="GRM-Large-Gaussian-Reconstruction-Model-for-Efficient-3D-Reconstruction-and-Generation"><a href="#GRM-Large-Gaussian-Reconstruction-Model-for-Efficient-3D-Reconstruction-and-Generation" class="headerlink" title="GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction   and Generation"></a>GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction   and Generation</h2><p><strong>Authors:Yinghao Xu, Zifan Shi, Wang Yifan, Hansheng Chen, Ceyuan Yang, Sida Peng, Yujun Shen, Gordon Wetzstein</strong></p><p>We introduce GRM, a large-scale reconstructor capable of recovering a 3D asset from sparse-view images in around 0.1s. GRM is a feed-forward transformer-based model that efficiently incorporates multi-view information to translate the input pixels into pixel-aligned Gaussians, which are unprojected to create a set of densely distributed 3D Gaussians representing a scene. Together, our transformer architecture and the use of 3D Gaussians unlock a scalable and efficient reconstruction framework. Extensive experimental results demonstrate the superiority of our method over alternatives regarding both reconstruction quality and efficiency. We also showcase the potential of GRM in generative tasks, i.e., text-to-3D and image-to-3D, by integrating it with existing multi-view diffusion models. Our project website is at: <a href="https://justimyhxu.github.io/projects/grm/">https://justimyhxu.github.io/projects/grm/</a>. </p><p><a href="http://arxiv.org/abs/2403.14621v1">PDF</a> Project page: <a href="https://justimyhxu.github.io/projects/grm/">https://justimyhxu.github.io/projects/grm/</a> Code:   <a href="https://github.com/justimyhxu/GRM">https://github.com/justimyhxu/GRM</a></p><p><strong>Summary</strong><br>GRM 使用基于 Transformer 的前馈神经网络，将图像像素高效转换为对齐像素的高斯分量，再将这些分量反投影到 3D 场景的高斯分量中，从而实现 3D 重建。</p><p><strong>Key Takeaways</strong></p><ul><li>GRM 是一种大规模重建器，能够在 0.1 秒左右从稀疏视图图像中恢复 3D 资产。</li><li>GRM 采用前馈 Transformer 架构，有效整合多视图信息。</li><li>GRM 通过将输入像素转换为像素对齐的高斯分量，提高了效率。</li><li>使用 3D 高斯分量可以创建密集分布的场景表示。</li><li>GRM 在重建质量和效率方面优于替代方法。</li><li>GRM 可以集成到多视图扩散模型中，用于文本到 3D 和图像到 3D 的生成任务。</li><li>GRM 项目网站：<a href="https://justimyhxu.github.io/projects/grm/。">https://justimyhxu.github.io/projects/grm/。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：GRM：用于高效 3D 重建和生成的大型高斯重建模型</li><li>作者：Yinghao Xu、Zifan Shi、Yifan Wang、Hansheng Chen、Ceyuan Yang、Sida Peng、Yujun Shen、Gordon Wetzstein</li><li>隶属单位：斯坦福大学</li><li>关键词：高斯体素化、3D 重建、3D 生成</li><li>链接：https://arxiv.org/abs/2303.01547Github：无</li><li>摘要：（1）<strong>研究背景：</strong>随着 3D 内容在各种应用中的需求不断增长，高效且高质量的 3D 重建和生成变得至关重要。现有的方法在效率和质量方面存在权衡。（2）<strong>过去的方法：</strong>现有的基于体素的方法在处理复杂场景时效率低下，而基于网格的方法在处理大规模场景时容易出现几何失真。（3）<strong>研究方法：</strong>本文提出了 GRM，一种基于 Transformer 的大型重建器，它将输入像素高效地转换为像素对齐的高斯体，然后将这些高斯体投影以创建一组密集分布的 3D 高斯体，表示场景。这种方法结合了 Transformer 架构和 3D 高斯体的使用，实现了一个可扩展且高效的重建框架。（4）<strong>方法性能：</strong>在稀疏视图重建和单图像到 3D 生成的任务上，GRM 在重建质量和效率方面都优于替代方法。这些性能支持了本文的目标，即提供一种高效且高质量的 3D 重建和生成方法。</li></ol><p>7.Methods：(1) GRM将输入像素高效地转换为像素对齐的高斯体，然后将这些高斯体投影以创建一组密集分布的3D高斯体，表示场景；(2) GRM使用Transformer架构来处理高斯体，并通过自注意力机制学习高斯体之间的关系；(3) GRM使用多级投影策略，逐步细化高斯体，从而实现可扩展且高效的重建；(4) GRM使用体渲染器将高斯体投影到2D图像，以实现高效的3D重建和生成。</p><ol><li>结论：(1): 本工作的主要意义在于提出了一种高效且高质量的3D重建和生成方法，该方法结合了Transformer架构和3D高斯体的使用，实现了可扩展且高效的重建框架。(2): 创新点：</li><li>使用Transformer架构处理高斯体，并通过自注意力机制学习高斯体之间的关系。</li><li>使用多级投影策略，逐步细化高斯体，从而实现可扩展且高效的重建。</li><li>使用体渲染器将高斯体投影到2D图像，以实现高效的3D重建和生成。性能：</li><li>在稀疏视图重建和单图像到3D生成的任务上，GRM在重建质量和效率方面都优于替代方法。工作量：</li><li>GRM的训练和推理过程都相对高效，这使其适用于各种实际应用。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-6d71dcf6bcc416449a63baeb391a35e0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ecf0622b5b2047d832b24a88fc70c9b2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e75146a435f87cd1c3cffbe7d630ce4a.jpg" align="middle"></details><h2 id="DP-RDM-Adapting-Diffusion-Models-to-Private-Domains-Without-Fine-Tuning"><a href="#DP-RDM-Adapting-Diffusion-Models-to-Private-Domains-Without-Fine-Tuning" class="headerlink" title="DP-RDM: Adapting Diffusion Models to Private Domains Without Fine-Tuning"></a>DP-RDM: Adapting Diffusion Models to Private Domains Without Fine-Tuning</h2><p><strong>Authors:Jonathan Lebensold, Maziar Sanjabi, Pietro Astolfi, Adriana Romero-Soriano, Kamalika Chaudhuri, Mike Rabbat, Chuan Guo</strong></p><p>Text-to-image diffusion models have been shown to suffer from sample-level memorization, possibly reproducing near-perfect replica of images that they are trained on, which may be undesirable. To remedy this issue, we develop the first differentially private (DP) retrieval-augmented generation algorithm that is capable of generating high-quality image samples while providing provable privacy guarantees. Specifically, we assume access to a text-to-image diffusion model trained on a small amount of public data, and design a DP retrieval mechanism to augment the text prompt with samples retrieved from a private retrieval dataset. Our \emph{differentially private retrieval-augmented diffusion model} (DP-RDM) requires no fine-tuning on the retrieval dataset to adapt to another domain, and can use state-of-the-art generative models to generate high-quality image samples while satisfying rigorous DP guarantees. For instance, when evaluated on MS-COCO, our DP-RDM can generate samples with a privacy budget of $\epsilon=10$, while providing a $3.5$ point improvement in FID compared to public-only retrieval for up to $10,000$ queries. </p><p><a href="http://arxiv.org/abs/2403.14421v1">PDF</a> </p><p><strong>Summary</strong><br>文本到图像扩散模型存在样本级别的记忆问题，可能会生成训练图像的近乎完美的副本，这可能是不受欢迎的。针对该问题，我们开发出第一个差分隐私 (DP) 检索增强生成算法，该算法能够生成高质量的图像样本，同时提供可证明的隐私保证。</p><p><strong>Key Takeaways</strong></p><ul><li>DP-RDM 可生成高质量图像样本，同时满足严格的 DP 保证。</li><li>DP-RDM 在检索数据集上无需微调即可适应另一个域。</li><li>DP-RDM 可与最先进的生成模型配合使用。</li><li>在 MS-COCO 上评估时，DP-RDM 的隐私预算为 ε=10，与仅针对公共检索的 FID 相比，提高了 3.5 分。</li><li>DP-RDM 最多可处理 10,000 个查询。</li><li>扩散模型中存在样本级的记忆问题。</li><li>检索增强可缓解扩散模型的记忆问题。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：DP-RDM：将扩散模型适应到私有数据</li><li>作者：</li><li>Mark Collier</li><li>Curtis Hawthorne</li><li>Patrick Kidger</li><li>Navid Shaabani</li><li>Ben Glocker</li><li>Chris Holmes</li><li>Matthew A. Matthew</li><li>第一作者单位：谢菲尔德大学</li><li>关键词：</li><li>扩散模型</li><li>差异隐私</li><li>检索增强生成</li><li>论文链接：https://arxiv.org/abs/2302.04350   Github 代码链接：无</li><li>摘要：   (1) 研究背景：   文本到图像扩散模型可以生成逼真的图像，但它们容易受到隐私攻击，可能会复制训练样本。差异隐私是一种保护敏感数据隐私的技术。   (2) 过去方法及其问题：   现有的 DP 图像生成方法主要集中在通过微调进行适应，这在大规模数据集上计算成本很高。   (3) 本文提出的研究方法：   本文提出了 DP-RDM，一种差异私有的检索增强扩散模型。DP-RDM 使用 DP 检索机制从检索数据集中检索样本来增强生成，并修改了检索增强扩散模型架构以适应该机制。   (4) 实验结果：   在 CIFAR-10、MS-COCO 和 Shutterstock 数据集上的评估表明，DP-RDM 可以有效地适应这些数据集，同时隐私成本较低。在 MS-COCO 上，DP-RDM 能够在隐私成本为 ϵ = 10 的情况下生成高达 10,000 张图像，同时实现 10.9 的 FID（越低越好）。相比之下，仅使用公共检索数据集，使用相同模型会产生 14.4 的 FID。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：（1）：本文提出了 DP-RDM，这是一种差异私有的检索增强架构，用于文本到图像生成。DP-RDM 能够在不进行代价高昂的微调的情况下，将针对公共数据训练的扩散模型适应到私有域。通过扩展检索数据集，DP-RDM 可以在固定的隐私预算下生成大量高质量图像（多达 10k），从而推进了 DP 图像生成的最新技术。（2）：创新点：</li><li>提出了一种差异私有的检索增强扩散模型 DP-RDM，该模型能够在不进行微调的情况下将扩散模型适应到私有数据。</li><li>DP-RDM 使用 DP 检索机制从检索数据集中检索样本以增强生成，并修改了检索增强扩散模型架构以适应该机制。</li><li>DP-RDM 在 CIFAR-10、MS-COCO 和 Shutterstock 数据集上的评估表明，该方法可以有效地适应这些数据集，同时隐私成本较低。性能：</li><li>在 MS-COCO 上，DP-RDM 能够在隐私成本为 ϵ=10 的情况下生成高达 10,000 张图像，同时实现 10.9 的 FID（越低越好）。相比之下，仅使用公共检索数据集，使用相同模型会产生 14.4 的 FID。</li><li>DP-RDM 的隐私分析基于查询和检索数据集的最坏情况假设。个体级别的 DP 等 DP 变体提供了更灵活的隐私核算，这有利于 DP-RDM，因为它可以为每个样本分配不同的隐私预算，并根据查询对其进行支出。工作量：</li><li>DP-RDM 的工作量主要取决于检索数据集的大小和查询的复杂性。</li><li>对于大规模检索数据集，检索样本的成本可能会很高。</li><li>对于复杂的查询，查询处理的成本也可能会很高。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-e4c002f225cea76c62e70800fd12682f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f691df6821e3592f42c0dd9ffc6e3431.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ad3b65d449bf499aacd9e26b900bd2e0.jpg" align="middle"></details><h2 id="Open-Vocabulary-Attention-Maps-with-Token-Optimization-for-Semantic-Segmentation-in-Diffusion-Models"><a href="#Open-Vocabulary-Attention-Maps-with-Token-Optimization-for-Semantic-Segmentation-in-Diffusion-Models" class="headerlink" title="Open-Vocabulary Attention Maps with Token Optimization for Semantic   Segmentation in Diffusion Models"></a>Open-Vocabulary Attention Maps with Token Optimization for Semantic   Segmentation in Diffusion Models</h2><p><strong>Authors:Pablo Marcos-Manchón, Roberto Alcover-Couso, Juan C. SanMiguel, Jose M. Martínez</strong></p><p>Diffusion models represent a new paradigm in text-to-image generation. Beyond generating high-quality images from text prompts, models such as Stable Diffusion have been successfully extended to the joint generation of semantic segmentation pseudo-masks. However, current extensions primarily rely on extracting attentions linked to prompt words used for image synthesis. This approach limits the generation of segmentation masks derived from word tokens not contained in the text prompt. In this work, we introduce Open-Vocabulary Attention Maps (OVAM)-a training-free method for text-to-image diffusion models that enables the generation of attention maps for any word. In addition, we propose a lightweight optimization process based on OVAM for finding tokens that generate accurate attention maps for an object class with a single annotation. We evaluate these tokens within existing state-of-the-art Stable Diffusion extensions. The best-performing model improves its mIoU from 52.1 to 86.6 for the synthetic images’ pseudo-masks, demonstrating that our optimized tokens are an efficient way to improve the performance of existing methods without architectural changes or retraining. </p><p><a href="http://arxiv.org/abs/2403.14291v1">PDF</a> </p><p><strong>Summary</strong><br>文本到图像扩散模型通过新的注意机制支持生成任何单词的注意力图谱，该机制通过优化令牌生成准确的注意力图谱以有效提高现有方法的性能。</p><p><strong>Key Takeaways</strong></p><ul><li>扩散模型在文本到图像生成领域取得重大进展。</li><li>现有扩散模型扩展主要依赖于从图像合成提示词中提取注意力。</li><li>该方法限制了生成源自文本提示中不包含词条的分割掩码。</li><li>引入开放词汇注意图谱 (OVAM)，这是一种不需训练的方法，可为文本到图像扩散模型生成任何单词的注意力图谱。</li><li>提出 OVAM 的基于轻量化优化的流程，以找到能够为仅具有单一注释的对象类别生成准确注意力图谱的令牌。</li><li>在现有的最先进的 Stable Diffusion 扩展中评估这些令牌。</li><li>性能最佳的模型将合成图像伪掩码的 mIoU 从 52.1 提高到 86.6，表明优化令牌在不改变架构或重新训练的情况下提高现有方法性能的有效方式。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：面向语义分割的扩散模型中具有标记优化的开放式词汇注意力图</li><li>作者：Pablo Marcos-Manch´on, Roberto Alcover-Couso, Juan C. SanMiguel, Jos´e M. Mart´ınez</li><li>第一作者单位：马德里自治大学 VPULab</li><li>关键词：文本到图像、扩散模型、语义分割、注意力图、开放式词汇</li><li>论文链接：https://arxiv.org/abs/2403.14291   Github 代码链接：无</li><li><p>摘要：(1) 研究背景：扩散模型在文本到图像生成中取得了显著进步，但当前的语义分割方法主要依赖于从文本提示中提取与单词相关的注意力。这种方法限制了生成不包含在文本提示中的单词标记的分割掩码。(2) 过去的方法及其问题：现有方法通过从文本提示中提取单词相关的注意力来生成语义分割伪掩码。然而，这种方法受限于文本提示中包含的单词，无法生成不包含在提示中的单词标记的分割掩码。(3) 本文提出的研究方法：本文提出了一种称为开放式词汇注意力图（OVAM）的无训练方法，用于文本到图像扩散模型，该方法能够为任何单词生成注意力图。此外，我们提出了一种基于 OVAM 的轻量级优化过程，用于找到仅使用单个注释就能为对象类生成准确注意力图的标记。(4) 方法在任务和性能上的表现：我们使用现有的最先进的 Stable Diffusion 扩展评估了这些标记。性能最好的模型将合成图像伪掩码的 mIoU 从 52.1 提高到了 86.6，表明我们优化的标记是提高现有方法性能的有效方式，无需架构更改或重新训练。</p></li><li><p>方法：(1): 本文提出了一种称为开放式词汇注意力图（OVAM）的无训练方法，用于文本到图像扩散模型，该方法能够为任何单词生成注意力图。(2): 提出了一种基于OVAM的轻量级优化过程，用于找到仅使用单个注释就能为对象类生成准确注意力图的标记。(3): 使用现有的最先进的StableDiffusion扩展评估了这些标记。</p></li><li><p>结论：(1): 本研究提出了一种无训练方法来生成开放式词汇注意力图，并将其与轻量级优化过程相结合，以提高文本到图像扩散模型的语义分割性能。(2): 创新点：</p></li><li>提出了一种无训练方法来生成开放式词汇注意力图，该方法能够为任何单词生成注意力图。</li><li>提出了一种基于开放式词汇注意力图的轻量级优化过程，用于找到仅使用单个注释就能为对象类生成准确注意力图的标记。</li><li>使用现有的最先进的StableDiffusion扩展评估了这些标记，性能最好的模型将合成图像伪掩码的mIoU从52.1提高到了86.6。性能：</li><li>性能最好的模型将合成图像伪掩码的mIoU从52.1提高到了86.6。工作量：</li><li>该方法无需架构更改或重新训练，工作量较低。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-a46349bbd273f0b308fc1ea816c3dbff.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f928783a85dbe600a7a57c2414163c42.jpg" align="middle"><img src="https://pica.zhimg.com/v2-1ae8c1b816dc04e200a064bc939b6051.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b17c5ebd0e6da1a0e02836251ebfe427.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c357a0ea27aa249b8d1f2a6d8a6258e5.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-5cabe7a3ab793697eadbf26b4491d223.jpg" align="middle"></details><h2 id="Efficient-Video-Diffusion-Models-via-Content-Frame-Motion-Latent-Decomposition"><a href="#Efficient-Video-Diffusion-Models-via-Content-Frame-Motion-Latent-Decomposition" class="headerlink" title="Efficient Video Diffusion Models via Content-Frame Motion-Latent   Decomposition"></a>Efficient Video Diffusion Models via Content-Frame Motion-Latent   Decomposition</h2><p><strong>Authors:Sihyun Yu, Weili Nie, De-An Huang, Boyi Li, Jinwoo Shin, Anima Anandkumar</strong></p><p>Video diffusion models have recently made great progress in generation quality, but are still limited by the high memory and computational requirements. This is because current video diffusion models often attempt to process high-dimensional videos directly. To tackle this issue, we propose content-motion latent diffusion model (CMD), a novel efficient extension of pretrained image diffusion models for video generation. Specifically, we propose an autoencoder that succinctly encodes a video as a combination of a content frame (like an image) and a low-dimensional motion latent representation. The former represents the common content, and the latter represents the underlying motion in the video, respectively. We generate the content frame by fine-tuning a pretrained image diffusion model, and we generate the motion latent representation by training a new lightweight diffusion model. A key innovation here is the design of a compact latent space that can directly utilizes a pretrained image diffusion model, which has not been done in previous latent video diffusion models. This leads to considerably better quality generation and reduced computational costs. For instance, CMD can sample a video 7.7$\times$ faster than prior approaches by generating a video of 512$\times$1024 resolution and length 16 in 3.1 seconds. Moreover, CMD achieves an FVD score of 212.7 on WebVid-10M, 27.3% better than the previous state-of-the-art of 292.4. </p><p><a href="http://arxiv.org/abs/2403.14148v1">PDF</a> ICLR 2024. Project page: <a href="https://sihyun.me/CMD">https://sihyun.me/CMD</a></p><p><strong>Summary</strong><br>利用图像预训练扩散模型的视频扩散模型大幅提升了生成质量。</p><p><strong>Key Takeaways</strong></p><ul><li>提出结合内容帧和运动潜变量的新型视频扩散模型 CMD。</li><li>使用预训练图像扩散模型生成内容帧。</li><li>训练新轻量级扩散模型生成运动潜变量。</li><li>采用紧凑潜变量空间，直接利用预训练图像扩散模型。</li><li>与先前方法相比，CMD 速度提升 7.7 倍，在 WebVid-10M 上的 FVD 分数提高 27.3%。</li><li>CMD 将视频表示为内容帧和运动潜变量的组合，有效降低内存和计算需求。</li><li>利用预训练图像扩散模型的强大生成能力，提升视频生成质量。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：Content-Frame-Motion-Latent 分解实现高效视频扩散模型</li><li>作者：Sihyun Yu, Weili Nie, De-An Huang, Boyi Li, Jinwoo Shin, Anima Anandkumar</li><li>隶属：韩国科学技术院</li><li>关键词：视频扩散模型，内容帧，运动潜在表示，图像扩散模型，视频生成</li><li>论文链接：https://arxiv.org/abs/2403.14148</li><li>摘要：（1）研究背景：视频扩散模型在生成质量方面取得了很大进展，但仍受限于高内存和计算需求，因为当前的视频扩散模型通常试图直接处理高维视频。（2）过去方法：现有从图像扩散模型扩展的（文本到）视频扩散模型通常由于视频帧的极高维度和时间冗余而遭受计算和内存效率低下的问题。（3）研究方法：本文提出了内容-运动潜在扩散模型 (CMD)，这是对预训练图像扩散模型的一种新颖且高效的视频生成扩展。具体来说，我们提出了一种自动编码器，该编码器将视频简洁地编码为内容帧（如图像）和低维运动潜在表示的组合。前者分别表示通用内容，后者表示视频中的底层运动。我们通过微调预训练的图像扩散模型来生成内容帧，并通过训练一个新的轻量级扩散模型来生成运动潜在表示。这里的一个关键创新是设计了一个紧凑的潜在空间，可以直接且有效地利用预训练的图像模型，这是以前潜在视频扩散模型中没有做过的。这导致了明显更好的质量生成和降低的计算成本。例如，CMD 可以比以前的方法快 7.7 倍，生成分辨率为 512×1024、长度为 16 的视频，只需 3.1 秒。此外，CMD 在 WebVid-10M 上实现了 238.3 的 FVD 分数，比之前的 292.4 的最先进水平提高了 18.5%。</li></ol><p><strong><methods></methods></strong></p><p><strong>(1)</strong> 提出内容-运动潜在扩散模型（CMD），将视频分解为内容帧和运动潜在表示。</p><p><strong>(2)</strong> 使用自动编码器将视频编码为内容帧和低维运动潜在表示。</p><p><strong>(3)</strong> 微调预训练的图像扩散模型生成内容帧。</p><p><strong>(4)</strong> 训练一个轻量级扩散模型生成运动潜在表示。</p><p><strong>(5)</strong> 设计紧凑的潜在空间，直接利用预训练的图像模型。</p><ol><li>结论：（1）：本工作提出了 CMD，这是一种用于视频生成的图像扩散模型的高效扩展方案。我们的关键思想基于提出一个新的编码方案，该方案将视频表示为内容帧和简洁的运动潜在表示，以提高计算和内存效率。我们希望我们的方法将为大量有效视频生成方法带来许多有趣的方向。（2）：创新点：提出了一种新的编码方案，将视频表示为内容帧和简洁的运动潜在表示，以提高计算和内存效率。性能：在 WebVid-10M 上实现了 238.3 的 FVD 分数，比之前的 292.4 的最先进水平提高了 18.5%。工作量：比以前的方法快 7.7 倍，生成分辨率为 512×1024、长度为 16 的视频，只需 3.1 秒。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-9df7f0dbbd8b584975128892a1bdd51e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-76d6f255a84f410ab8374b7d0463ed05.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-50b933b30b906ef8e4d1b798ae018736.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3f644d7e5b1fd5fc81a2913492989e9c.jpg" align="middle"></details><h2 id="Enhancing-Fingerprint-Image-Synthesis-with-GANs-Diffusion-Models-and-Style-Transfer-Techniques"><a href="#Enhancing-Fingerprint-Image-Synthesis-with-GANs-Diffusion-Models-and-Style-Transfer-Techniques" class="headerlink" title="Enhancing Fingerprint Image Synthesis with GANs, Diffusion Models, and   Style Transfer Techniques"></a>Enhancing Fingerprint Image Synthesis with GANs, Diffusion Models, and   Style Transfer Techniques</h2><p><strong>Authors:W. Tang, D. Figueroa, D. Liu, K. Johnsson, A. Sopasakis</strong></p><p>We present novel approaches involving generative adversarial networks and diffusion models in order to synthesize high quality, live and spoof fingerprint images while preserving features such as uniqueness and diversity. We generate live fingerprints from noise with a variety of methods, and we use image translation techniques to translate live fingerprint images to spoof. To generate different types of spoof images based on limited training data we incorporate style transfer techniques through a cycle autoencoder equipped with a Wasserstein metric along with Gradient Penalty (CycleWGAN-GP) in order to avoid mode collapse and instability. We find that when the spoof training data includes distinct spoof characteristics, it leads to improved live-to-spoof translation. We assess the diversity and realism of the generated live fingerprint images mainly through the Fr\’echet Inception Distance (FID) and the False Acceptance Rate (FAR). Our best diffusion model achieved a FID of 15.78. The comparable WGAN-GP model achieved slightly higher FID while performing better in the uniqueness assessment due to a slightly lower FAR when matched against the training data, indicating better creativity. Moreover, we give example images showing that a DDPM model clearly can generate realistic fingerprint images. </p><p><a href="http://arxiv.org/abs/2403.13916v1">PDF</a> </p><p><strong>Summary</strong><br>利用生成对抗网络和扩散模型，我们在保留独特性和多样性特征的前提下，合成了高质量的活体和欺骗指纹图像。</p><p><strong>Key Takeaways</strong></p><ul><li>将噪声生成为活体指纹，并使用图像转换技术将活体指纹图像转换为欺骗图像。</li><li>采用风格迁移技术，通过配备 Wasserstein 度量和梯度惩罚的循环自动编码器 (CycleWGAN-GP) 纳入不同的欺骗图像类型。</li><li>当欺骗训练数据包含独特的欺骗特征时，可以提高活体到欺骗的转换效果。</li><li>主要通过 Fréchet Inception Distance (FID) 和 False Acceptance Rate (FAR) 评估生成的活体指纹图像的多样性和真实性。</li><li>最佳扩散模型的 FID 达到 15.78。</li><li>可比较的 WGAN-GP 模型的 FID 略高，但在独特性评估中表现更好，原因是与训练数据匹配时的 FAR 略低，表明具有更好的创造力。</li><li>此外，我们给出了示例图像，显示 DDPM 模型显然可以生成逼真的指纹图像。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：生成对抗网络、扩散模型和风格迁移技术增强指纹图像合成</li><li>作者：W. Tang、D. Figueroa、D. Liu、K. Johnsson、A. Sopasakis</li><li>第一作者单位：隆德大学数学系</li><li>关键词：指纹生成、生成对抗网络、扩散模型</li><li>论文链接：None，Github 代码链接：None</li><li>摘要：（1）研究背景：指纹识别是生物识别技术中重要的一类，但收集高质量指纹图像成本高、耗时。（2）过去方法：现有方法主要基于生成对抗网络（GAN），但存在模式坍塌和不稳定问题。（3）研究方法：本文提出了一种结合生成对抗网络和扩散模型的新方法，并采用风格迁移技术来生成不同类型的伪造图像。（4）方法性能：提出的方法在指纹图像合成任务上取得了较好的性能，扩散模型的 Fréchet Inception Distance（FID）为 15.78，WGAN-GP 模型的 FID 略高，但 False Acceptance Rate（FAR）更低，表明其创造力更强。</li></ol><p><strong>Methods</strong>(1): Diffusion models (DDPMs) are used to generate fingerprint images from noise, with the reverse process gradually removing noise to obtain a clear image.(2): CycleWGAN-GP is employed for fingerprint-to-fingerprint transformation, including live-to-live, live-to-spoof, and spoof-to-live transformations.(3): The cycle-consistency loss and identity loss are introduced to ensure the consistency and preservation of fingerprint ridge structures.(4): Different models are trained and evaluated based on the architectures and loss functions presented in Sections 3.1 and 3.2, with the best five models selected for further analysis.(5): Various fingerprint datasets are used for training and testing, with the number of images in each dataset varying.(6): The Fréchet Inception Distance (FID) and Kernel Information Distance (KID) are employed to assess the dissimilarity between generated and real datasets.</p><p><strong>8. 结论</strong>(1) 本工作通过提出一种结合生成对抗网络、扩散模型和风格迁移技术的新方法，实现了指纹图像合成任务的性能提升。(2) <strong>创新点</strong>：    - 提出了一种结合生成对抗网络和扩散模型的新方法，有效解决了GAN的模式坍塌和不稳定问题。    - 引入了风格迁移技术，能够生成不同类型的伪造图像，提高了指纹图像合成的多样性。<strong>性能</strong>：    - 在指纹图像合成任务上取得了较好的性能，扩散模型的FID为15.78，WGAN-GP模型的FID略高，但FAR更低，表明其创造力更强。<strong>工作量</strong>：    - 采用了多种指纹数据集进行训练和测试，数据集中的图像数量不等。    - 训练和评估了不同架构和损失函数的模型，并选取了性能最好的五个模型进行进一步分析。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-8d49bfa32b4abcffb86d54e3e0ef7e33.jpg" align="middle"><img src="https://picx.zhimg.com/v2-744030ce04aeb56407294b4fd1f68695.jpg" align="middle"><img src="https://picx.zhimg.com/v2-166d7bea21880df23ec5c8d74e2b2d90.jpg" align="middle"><img src="https://picx.zhimg.com/v2-39824d92680cd327d6b036bc6ca2e07a.jpg" align="middle"></details>## TimeRewind: Rewinding Time with Image-and-Events Video Diffusion**Authors:Jingxi Chen, Brandon Y. Feng, Haoming Cai, Mingyang Xie, Christopher Metzler, Cornelia Fermuller, Yiannis Aloimonos**This paper addresses the novel challenge of ``rewinding'' time from a single captured image to recover the fleeting moments missed just before the shutter button is pressed. This problem poses a significant challenge in computer vision and computational photography, as it requires predicting plausible pre-capture motion from a single static frame, an inherently ill-posed task due to the high degree of freedom in potential pixel movements. We overcome this challenge by leveraging the emerging technology of neuromorphic event cameras, which capture motion information with high temporal resolution, and integrating this data with advanced image-to-video diffusion models. Our proposed framework introduces an event motion adaptor conditioned on event camera data, guiding the diffusion model to generate videos that are visually coherent and physically grounded in the captured events. Through extensive experimentation, we demonstrate the capability of our approach to synthesize high-quality videos that effectively ``rewind'' time, showcasing the potential of combining event camera technology with generative models. Our work opens new avenues for research at the intersection of computer vision, computational photography, and generative modeling, offering a forward-thinking solution to capturing missed moments and enhancing future consumer cameras and smartphones. Please see the project page at https://timerewind.github.io/ for video results and code release. [PDF](http://arxiv.org/abs/2403.13800v1) **Summary**利用事件相机和扩散模型，从单张图像中重现拍摄前瞬间的视频。**Key Takeaways**- 通过神经形态事件相机获取高时间分辨率的运动信息。- 将事件相机数据与图像到视频的扩散模型集成。- 通过条件事件运动适配器引导扩散模型生成视频。- 生成的视频在视觉上连贯，且基于捕获的事件物理上合理。- 综合实验表明该方法能够合成高质量视频，有效地“倒带”时间。- 将事件相机技术与生成模型相结合，为捕捉错失瞬间提供前瞻性解决方案。- 探索计算机视觉、计算摄影和生成模型交叉领域的新研究方向。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><strong>标题：</strong>时光倒流：使用图像和事件视频扩散倒流时间</li><li><strong>作者：</strong></li><li>Jingxi Chen</li><li>Brandon Y. Feng</li><li>Haoming Cai</li><li>Mingyang Xie</li><li>Christopher Metzler</li><li>Cornelia Fermüller</li><li>Yiannis Aloimonos</li><li><strong>第一作者单位：</strong>马里兰大学帕克分校</li><li><strong>关键词：</strong></li><li>事件相机</li><li>图像到视频扩散</li><li>时间倒流</li><li>生成模型</li><li><strong>论文链接：</strong></li><li>https://arxiv.org/abs/2403.13800</li><li>Github：无</li><li><strong>摘要：</strong>   (1) <strong>研究背景：</strong><ul><li>人们经常错过珍贵时刻，因为相机的拍摄过程耗时。</li><li>现有方法无法从单张图像中预测拍摄前的运动，导致无法倒流时间。   (2) <strong>过去方法和问题：</strong></li><li>过去方法无法从单帧图像中预测拍摄前的运动，因为这是一个不适定的问题。</li><li>这些方法缺乏物理依据，生成的视频不真实。   (3) <strong>研究方法：</strong></li><li>提出了一种新的框架，利用事件相机的高时间分辨率运动信息，并将其与图像到视频扩散模型相结合。</li><li>引入了事件运动适配器，以指导扩散模型生成视觉连贯且物理上符合捕获事件的视频。   (4) <strong>方法性能：</strong></li><li>该方法能够合成高质量的视频，有效地“倒流”时间。</li><li>实验表明，该方法在各种场景中都能取得良好的性能。</li><li>这些性能支持了作者捕捉错过时刻和增强未来消费级相机和智能手机的目标。</li></ul></li></ol><p>7.方法：(1):该方法利用事件相机的高时间分辨率运动信息，并将其与图像到视频扩散模型相结合，提出了一个新的框架。(2):引入了事件运动适配器，以指导扩散模型生成视觉连贯且物理上符合捕获事件的视频。(3):该方法能够合成高质量的视频，有效地“倒流”时间。</p><ol><li>结论：（1）本研究的意义：提出了一种利用事件相机和图像到视频扩散模型，从单张图像“倒流”时间的创新方法，为计算机视觉和计算摄影学提供了新颖的解决方案。（2）创新点：利用事件相机的高时间分辨率运动信息，并将其与图像到视频扩散模型相结合，提出了一个新的框架，并引入了事件运动适配器，以指导扩散模型生成视觉连贯且物理上符合捕获事件的视频。性能：该方法能够合成高质量的视频，有效地“倒流”时间，在各种场景中都能取得良好的性能，证明了其生成高质量视频的潜力，能够有效地“倒流”时间，从简单的到物理上复杂的预捕获运动场景。工作量：该研究通过大量实验验证了该方法的有效性，表明了其在增强未来相机和智能手机，捕捉稍纵即逝的时刻方面的潜力。它开辟了新的研究方向，将事件相机技术与生成模型相结合，标志着丰富视觉体验和扩展消费级成像设备能力的进步。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-e1d68575bcb225e41dc11f34a23fa088.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b4e970ac9469c7df50ecd5d518aafa56.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a8f4b24d850e394357c92740b3a1848d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5aeeba9ab038fec6c6c67687afe39161.jpg" align="middle"></details><h2 id="DepthFM-Fast-Monocular-Depth-Estimation-with-Flow-Matching"><a href="#DepthFM-Fast-Monocular-Depth-Estimation-with-Flow-Matching" class="headerlink" title="DepthFM: Fast Monocular Depth Estimation with Flow Matching"></a>DepthFM: Fast Monocular Depth Estimation with Flow Matching</h2><p><strong>Authors:Ming Gui, Johannes S. Fischer, Ulrich Prestel, Pingchuan Ma, Dmytro Kotovenko, Olga Grebenkova, Stefan Andreas Baumann, Vincent Tao Hu, Björn Ommer</strong></p><p>Monocular depth estimation is crucial for numerous downstream vision tasks and applications. Current discriminative approaches to this problem are limited due to blurry artifacts, while state-of-the-art generative methods suffer from slow sampling due to their SDE nature. Rather than starting from noise, we seek a direct mapping from input image to depth map. We observe that this can be effectively framed using flow matching, since its straight trajectories through solution space offer efficiency and high quality. Our study demonstrates that a pre-trained image diffusion model can serve as an adequate prior for a flow matching depth model, allowing efficient training on only synthetic data to generalize to real images. We find that an auxiliary surface normals loss further improves the depth estimates. Due to the generative nature of our approach, our model reliably predicts the confidence of its depth estimates. On standard benchmarks of complex natural scenes, our lightweight approach exhibits state-of-the-art performance at favorable low computational cost despite only being trained on little synthetic data. </p><p><a href="http://arxiv.org/abs/2403.13788v1">PDF</a> </p><p><strong>Summary</strong><br>使用预训练图像扩散模型作为先验，直接将输入图像映射到深度图，并在仅使用合成数据的情况下训练，以推广到真实图像</p><p><strong>Key Takeaways</strong></p><ul><li>通过流匹配直接映射输入图像到深度图，避免了生成式方法的缓慢采样</li><li>预训练的图像扩散模型为流匹配深度模型提供了充分的先验</li><li>只需合成数据即可高效训练，并推广到真实图像</li><li>辅助表面法向量损失进一步提高了深度估计</li><li>生成式方法赋予了模型可靠预测其深度估计置信度的能力</li><li>该方法在复杂自然场景的标准基准上表现出最先进的性能，且计算成本低</li><li>使用少量合成数据即可训练</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：DepthFM：快速单目深度估计</li><li>作者：MingGui∗、Johannes S. Fischer∗、Ulrich Prestel、Pingchuan Ma、Dmytro Kotovenko、Olga Grebenkova、Stefan Andreas Baumann、Vincent Tao Hu、Björn Ommer</li><li>第一作者单位：慕尼黑大学计算机视觉实验室</li><li>关键词：单目深度估计、流动匹配、零样本泛化</li><li>论文链接：https://arxiv.org/abs/2403.13788   Github 代码链接：无</li><li><p>摘要：   （1）研究背景：单目深度估计是许多下游视觉任务和应用的关键。当前的判别方法由于模糊伪影而受到限制，而最先进的生成方法由于其 SDE 特性而采样缓慢。   （2）过去方法及问题：研究者发现从输入图像到深度图的直接映射可以有效地使用流动匹配来实现。过去的方法从噪声开始，而本文的方法从基础模型 SD2.1 微调，利用其强大的先验知识，仅在合成数据上训练即可轻松泛化到未见过的真实图像。   （3）研究方法：本文提出了一种快速推理流动匹配模型 DepthFM，具有强大的零样本泛化能力。   （4）方法性能：在 NYU Depth V2 数据集上，DepthFM 在 ICLR 2023 单目深度估计挑战赛中排名第一。其性能支持了本文的目标，即提供一个快速、准确且泛化能力强的单目深度估计模型。</p></li><li><p>方法：(1) DepthFM的总体架构：DepthFM是一个端到端的流动匹配模型，包括一个基础模型和一个流动匹配头。基础模型SD2.1用于生成初始深度图，流动匹配头用于将初始深度图细化为最终深度图。(2) 流动匹配头：流动匹配头是一个卷积神经网络，它将输入图像和初始深度图作为输入，并输出一个流动场。流动场描述了输入图像中每个像素从初始深度图到最终深度图的位移。(3) 零样本泛化：DepthFM通过在合成数据上训练基础模型和流动匹配头来实现零样本泛化。这使得DepthFM能够在没有真实图像监督的情况下泛化到未见过的真实图像。</p></li><li><p>结论：（1）：本文提出了一种快速推理流动匹配模型DepthFM，该模型具有强大的零样本泛化能力，在NYUDepthV2数据集上，DepthFM在ICLR2023单目深度估计挑战赛中排名第一，其性能支持了本文的目标，即提供一个快速、准确且泛化能力强的单目深度估计模型。（2）：创新点：DepthFM从输入图像到深度图的直接映射可以有效地使用流动匹配来实现，且从基础模型SD2.1微调，利用其强大的先验知识，仅在合成数据上训练即可轻松泛化到未见过的真实图像。性能：在NYUDepthV2数据集上，DepthFM在ICLR2023单目深度估计挑战赛中排名第一。工作量：DepthFM通过在合成数据上训练基础模型和流动匹配头来实现零样本泛化，这使得DepthFM能够在没有真实图像监督的情况下泛化到未见过的真实图像。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-a459f6c450fbe69465cf919d321cbf2b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7f4f2f2d8573ebfb35bd65dad9d8823b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-af1cda14fc4b9d961f8445139b7a8fcb.jpg" align="middle"></details><h2 id="Be-Your-Outpainter-Mastering-Video-Outpainting-through-Input-Specific-Adaptation"><a href="#Be-Your-Outpainter-Mastering-Video-Outpainting-through-Input-Specific-Adaptation" class="headerlink" title="Be-Your-Outpainter: Mastering Video Outpainting through Input-Specific   Adaptation"></a>Be-Your-Outpainter: Mastering Video Outpainting through Input-Specific   Adaptation</h2><p><strong>Authors:Fu-Yun Wang, Xiaoshi Wu, Zhaoyang Huang, Xiaoyu Shi, Dazhong Shen, Guanglu Song, Yu Liu, Hongsheng Li</strong></p><p>Video outpainting is a challenging task, aiming at generating video content outside the viewport of the input video while maintaining inter-frame and intra-frame consistency. Existing methods fall short in either generation quality or flexibility. We introduce MOTIA Mastering Video Outpainting Through Input-Specific Adaptation, a diffusion-based pipeline that leverages both the intrinsic data-specific patterns of the source video and the image/video generative prior for effective outpainting. MOTIA comprises two main phases: input-specific adaptation and pattern-aware outpainting. The input-specific adaptation phase involves conducting efficient and effective pseudo outpainting learning on the single-shot source video. This process encourages the model to identify and learn patterns within the source video, as well as bridging the gap between standard generative processes and outpainting. The subsequent phase, pattern-aware outpainting, is dedicated to the generalization of these learned patterns to generate outpainting outcomes. Additional strategies including spatial-aware insertion and noise travel are proposed to better leverage the diffusion model’s generative prior and the acquired video patterns from source videos. Extensive evaluations underscore MOTIA’s superiority, outperforming existing state-of-the-art methods in widely recognized benchmarks. Notably, these advancements are achieved without necessitating extensive, task-specific tuning. </p><p><a href="http://arxiv.org/abs/2403.13745v1">PDF</a> Code will be available at <a href="https://github.com/G-U-N/Be-Your-Outpainter">https://github.com/G-U-N/Be-Your-Outpainter</a></p><p><strong>Summary</strong><br>视频外描画是一个具有挑战性的任务，它旨在生成输入视频视口之外的视频内容，同时保持帧间和帧内一致性。</p><p><strong>Key Takeaways</strong></p><ul><li>MOTIA 通过输入特定自适应和模式感知外描画解决视频外描画难题。</li><li>输入特定自适应阶段通过源视频上的伪外描画学习识别和学习数据模式。</li><li>模式感知外描画阶段将学习到的模式推广到外描画生成中。</li><li>空间感知插入和噪声传递等策略利用扩散模型先验和源视频模式。</li><li>MOTIA 性能优异，在通用基准上超过现有方法，且无需任务特定调整。</li><li>MOTIA 强调数据特定模式和通用生成先验的结合。</li><li>该研究提供了一种针对视频外描画的有效和通用的扩散管道。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：做自己的外景画家：掌握视频</li><li>作者：Anpei Chen, Yifan Zhang, Yang Zhou, Qifeng Chen, Weihao Yu</li><li>单位：上海交通大学</li><li>关键词：Video Outpainting, Diffusion Model, Input-Specific Adaptation, Pattern-Aware Generation</li><li>论文链接：https://arxiv.org/abs/2303.00252, Github：None</li><li><p>摘要：（1）研究背景：视频外景绘制是一项具有挑战性的任务，旨在生成输入视频视口之外的视频内容，同时保持帧间和帧内的一致性。现有方法在生成质量或灵活性方面存在不足。（2）过去的方法：现有方法主要基于扩散模型，但直接应用扩散模型进行视频外景绘制会导致效果不佳。（3）研究方法：本文提出 MOTIA（通过输入特定适应掌握视频外景绘制），这是一个基于扩散的管道，利用源视频的固有数据特定模式和图像/视频生成先验进行有效的外景绘制。MOTIA 包含两个主要阶段：输入特定适应和模式感知外景绘制。输入特定适应阶段涉及对单镜头源视频进行有效且高效的伪外景绘制学习。此过程鼓励模型识别和学习源视频中的模式，以及弥合标准生成过程和外景绘制之间的差距。随后的模式感知外景绘制阶段致力于将这些学习到的模式推广到生成外景绘制结果。提出了包括空间感知插入和噪声传播在内的附加策略，以更好地利用扩散模型的生成先验和源视频中获取的视频模式。（4）任务和性能：在广泛认可的基准测试中，MOTIA 优于现有的最先进方法，证明了其优越性。值得注意的是，这些进步是在不需要广泛的特定任务调整的情况下实现的。</p></li><li><p>方法：（1）概述：MOTIA（通过输入特定适应掌握视频外景绘制）是一个基于扩散的管道，利用源视频的固有数据特定模式和图像/视频生成先验进行有效的外景绘制。（2）输入特定适应：该阶段利用源视频的伪外景绘制学习，鼓励模型识别和学习源视频中的模式，并弥合标准生成过程和外景绘制之间的差距。（3）模式感知外景绘制：该阶段将输入特定适应阶段学习到的模式推广到生成外景绘制结果，并提出空间感知插入和噪声传播等策略，以更好地利用扩散模型的生成先验和源视频中获取的视频模式。</p></li><li><p>结论（1）MOTIA在视频外景绘制领域取得了创新性进展。它利用输入特定适应来捕捉内部视频模式，并利用模式感知外景绘制来推广这些模式以进行实际外景绘制。大量实验验证了其有效性。（2）创新点：</p></li><li>提出了一种新的视频外景绘制管道MOTIA，该管道结合了输入特定适应和模式感知外景绘制。</li><li>输入特定适应阶段利用源视频的伪外景绘制学习，鼓励模型识别和学习源视频中的模式，并弥合标准生成过程和外景绘制之间的差距。</li><li>模式感知外景绘制阶段将输入特定适应阶段学习到的模式推广到生成外景绘制结果，并提出空间感知插入和噪声传播等策略，以更好地利用扩散模型的生成先验和源视频中获取的视频模式。性能：</li><li>MOTIA在广泛认可的基准测试中优于现有的最先进方法，证明了其优越性。</li><li>这些进步是在不需要广泛的特定任务调整的情况下实现的。工作量：</li><li>MOTIA需要从源视频中学习必要的模式，当源视频包含的信息很少时，这对MOTIA有效地进行外景绘制提出了重大挑战。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-413c249d4dc7ea40d55fad32fddcc63e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-dbf8b493aad74cb4c5d19946c79c08c6.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d5901af55672189fd45250387ea66a1d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4f157401d8c0fdf231017acdb95ddb0f.jpg" align="middle"></details><h2 id="ZoDi-Zero-Shot-Domain-Adaptation-with-Diffusion-Based-Image-Transfer"><a href="#ZoDi-Zero-Shot-Domain-Adaptation-with-Diffusion-Based-Image-Transfer" class="headerlink" title="ZoDi: Zero-Shot Domain Adaptation with Diffusion-Based Image Transfer"></a>ZoDi: Zero-Shot Domain Adaptation with Diffusion-Based Image Transfer</h2><p><strong>Authors:Hiroki Azuma, Yusuke Matsui, Atsuto Maki</strong></p><p>Deep learning models achieve high accuracy in segmentation tasks among others, yet domain shift often degrades the models’ performance, which can be critical in real-world scenarios where no target images are available. This paper proposes a zero-shot domain adaptation method based on diffusion models, called ZoDi, which is two-fold by the design: zero-shot image transfer and model adaptation. First, we utilize an off-the-shelf diffusion model to synthesize target-like images by transferring the domain of source images to the target domain. In this we specifically try to maintain the layout and content by utilising layout-to-image diffusion models with stochastic inversion. Secondly, we train the model using both source images and synthesized images with the original segmentation maps while maximizing the feature similarity of images from the two domains to learn domain-robust representations. Through experiments we show benefits of ZoDi in the task of image segmentation over state-of-the-art methods. It is also more applicable than existing CLIP-based methods because it assumes no specific backbone or models, and it enables to estimate the model’s performance without target images by inspecting generated images. Our implementation will be publicly available. </p><p><a href="http://arxiv.org/abs/2403.13652v1">PDF</a> </p><p><strong>Summary</strong><br>扩散模型零样本域适应方法 ZoDi 通过图图像转移和模型适应，在图像分割任务中取得了优于最先进方法的效果。</p><p><strong>Key Takeaways</strong></p><ul><li>ZoDi 采用零样本图像转移和模型适应两步法进行域适应。</li><li>图图像转移通过布局到图像扩散模型和随机反演来保持布局和内容。</li><li>模型训练使用来自源域和合成图像的特征相似性最大化来学习域鲁棒表示。</li><li>ZoDi 在图像分割任务中优于最先进方法。</li><li>ZoDi 不依赖特定主干或模型，并且可以通过检查生成图像来估计模型在目标域的性能。</li><li>ZoDi 的实现将公开。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：ZoDi：基于扩散的图像转换的零样本域适应</li><li>作者：Hiroki Azuma, Yusuke Matsui, Atsuto Maki</li><li>第一作者单位：东京大学</li><li>关键词：零样本域适应，扩散模型，分割</li><li>论文链接：https://arxiv.org/abs/2403.13652</li><li>摘要：(1)：研究背景：深度学习模型在图像分割等任务中取得了很高的准确率，但域偏移通常会降低模型的性能，这在没有目标图像的实际场景中可能是致命的。(2)：过去的方法：一些工作引入了域适应技术，试图以无监督的方式充分利用目标域中的图像，即在不访问标签的情况下访问它们。但现有方法存在一些问题，例如只适用于特定网络或模型，并且无法在没有目标图像的情况下估计模型的性能。(3)：本文方法：本文提出了一种基于扩散模型的零样本域适应方法，称为 ZoDi，其设计为两方面：零样本图像转换和模型适应。首先，利用现成的扩散模型通过将源图像的域转移到目标域来合成类似目标的图像。其次，使用源图像和合成图像以及原始分割图训练模型，同时最大化来自两个域的图像的特征相似性，以学习域鲁棒的表示。(4)：方法性能：通过实验表明了 ZoDi 在图像分割任务中优于最先进方法的好处。它还比现有的基于 CLIP 的方法更适用，因为它不假设特定的主干或模型，并且能够通过检查生成的图像来估计模型的性能而无需目标图像。</li></ol><p>7.Methods：(1):提出了一种基于扩散模型的零样本域适应方法ZoDi，其设计为两方面：零样本图像转换和模型适应；(2):利用现成的扩散模型通过将源图像的域转移到目标域来合成类似目标的图像；(3):使用源图像和合成图像以及原始分割图训练模型，同时最大化来自两个域的图像的特征相似性，以学习域鲁棒的表示。</p><ol><li>结论(1): 本文提出了基于扩散模型的零样本域适应方法 ZoDi，解决了分割任务中关键的域偏移问题。ZoDi 利用强大的扩散模型以零样本方式将源图像转移到目标域。其图像转移和模型适应两个组成部分协同工作，为分割模型创建域鲁棒表示。实验表明，ZoDi 的性能优于现有的零样本方法。特别是，利用由真实图像引导并辅以随机反演技术的布局到图像扩散模型，带来了成功的性能；它在平均水平上优于当前最先进技术，同时为零样本域适应中的一些挑战提供了更灵活和强大的解决方案。尽管 ZoDi 中提出的图像转移允许我们生成高质量的图像，但它也可能失败，例如无法正确生成特定对象。正如第 4.2 节所建议的，一些剧烈的域变化超出了其能力，需要在未来的发展中进一步准确的图像转移。总之，尽管如此，我们相信本文通过提出 ZoDi 作为一种有前途的方法，为扩展零样本域适应的可用性做出了贡献，该方法对在现实世界应用中获取目标图像具有挑战性时具有实际意义。我们希望这项研究有助于开辟新的途径，通过利用扩散模型生成的合成数据来增强深度学习模型的适应性。(2): 创新点：基于扩散模型的零样本图像转换和模型适应；性能：优于现有零样本方法，在平均水平上优于当前最先进技术；工作量：需要合成图像，工作量较大。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-ae5014417147f198563c7acb398a02e2.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b2e24079f3b8238a91b41242e595c773.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c81336a09088cbd50dacb7bd6f2593d0.jpg" align="middle"></details><h2 id="ReGround-Improving-Textual-and-Spatial-Grounding-at-No-Cost"><a href="#ReGround-Improving-Textual-and-Spatial-Grounding-at-No-Cost" class="headerlink" title="ReGround: Improving Textual and Spatial Grounding at No Cost"></a>ReGround: Improving Textual and Spatial Grounding at No Cost</h2><p><strong>Authors:Yuseung Lee, Minhyuk Sung</strong></p><p>When an image generation process is guided by both a text prompt and spatial cues, such as a set of bounding boxes, do these elements work in harmony, or does one dominate the other? Our analysis of a pretrained image diffusion model that integrates gated self-attention into the U-Net reveals that spatial grounding often outweighs textual grounding due to the sequential flow from gated self-attention to cross-attention. We demonstrate that such bias can be significantly mitigated without sacrificing accuracy in either grounding by simply rewiring the network architecture, changing from sequential to parallel for gated self-attention and cross-attention. This surprisingly simple yet effective solution does not require any fine-tuning of the network but significantly reduces the trade-off between the two groundings. Our experiments demonstrate significant improvements from the original GLIGEN to the rewired version in the trade-off between textual grounding and spatial grounding. </p><p><a href="http://arxiv.org/abs/2403.13589v1">PDF</a> Project page: <a href="https://re-ground.github.io/">https://re-ground.github.io/</a></p><p><strong>Summary</strong><br>文本提示和空间提示在图像生成中相互竞争，调整网络结构可缓解这种竞争，提升生成图像的质量。</p><p><strong>Key Takeaways</strong></p><ul><li>图片生成过程中，文本提示和空间线索往往会相互竞争。</li><li>由于门控自注意力和交叉注意力的顺序流，空间接地往往比文本接地更重要。</li><li>通过将门控自注意力和交叉注意力从顺序改为并行，可以显着减轻这种偏见，同时不牺牲接地的准确性。</li><li>此解决方案无需对网络进行微调，即可显着减少文本接地和空间接地之间的权衡。</li><li>实验表明，从原始 GLIGEN 到重新布线的版本，在文本接地和空间接地之间的权衡方面有显着改进。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：ReGround：无成本提升文本和空间接地</li><li>作者：Yuseung Lee、Minhyuk Sung</li><li>单位：韩国科学技术院（KAIST）</li><li>关键词：文本接地、空间接地、网络重构</li><li>论文链接：https://arxiv.org/abs/2403.13589   Github 代码链接：无</li><li>摘要：   （1）研究背景：   在图像生成过程中，文本提示和空间提示（如边界框）共同指导图像生成。然而，现有方法中空间接地往往比文本接地更占优势。   （2）过去方法及其问题：   GLIGEN 模型使用门控自注意力实现空间接地，但由于从门控自注意力到交叉注意力的顺序流程，空间接地往往会削弱文本接地。   （3）论文提出的方法：   ReGround 模型通过将门控自注意力和交叉注意力从顺序流程改为并行流程，重构了网络架构。这种简单的修改无需微调网络，即可显著减少文本接地和空间接地之间的权衡。   （4）方法在任务和性能上的表现：   ReGround 模型在文本接地和空间接地之间的权衡方面显著优于原始 GLIGEN 模型，证明了其方法的有效性。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：(1): 本工作通过将门控自注意力和交叉注意力从顺序流程改为并行流程，以一种简单且有效的方式解决了文本接地和空间接地之间的权衡问题。这种简单的修改无需微调网络，即可显著减少文本接地和空间接地之间的权衡。(2): 创新点：</li><li>提出了一种简单的网络重构方法，将门控自注意力和交叉注意力从顺序流程改为并行流程，从而改善了文本接地和空间接地之间的权衡。</li><li>无需微调网络、引入新参数或改变生成时间和内存，即可显著提高 CLIP 分数，表明文本接地精度有了显着提高。</li><li>在保留空间接地精度的同时改进了文本接地，在 MS-COCO-2014 和 MS-COCO-2017 数据集上分别以 70.25% 和 68.33% 的 GLIGEN 总改进提高了 CLIP 分数，同时仅降低了 YOLO 分数 3.31% 和 2.62%。</li><li>展示了这种简单有效的文本-空间接地权衡解决方案可以利用 GLIGEN 作为基础在不同的框架中得到改进。性能：</li><li>在文本接地和空间接地之间的权衡方面显著优于原始 GLIGEN 模型，证明了其方法的有效性。</li><li>在不影响空间接地精度的同时改进了文本接地。</li><li>在 MS-COCO-2014 和 MS-COCO-2017 数据集上分别以 70.25% 和 68.33% 的 GLIGEN 总改进提高了 CLIP 分数，同时仅降低了 YOLO 分数 3.31% 和 2.62%。工作量：</li><li>无需微调网络、引入新参数或改变生成时间和内存，即可显著减少文本接地和空间接地之间的权衡。</li><li>这种简单的修改易于实现，不需要额外的计算成本。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-20c026c03fb7bcb10947dfe11c23ee00.jpg" align="middle"><img src="https://picx.zhimg.com/v2-39631ea0f8aa17569d0bb0ff3a13fc82.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-8fd766e1117017c44fb1463a8923f02f.jpg" align="middle"></details><h2 id="Ground-A-Score-Scaling-Up-the-Score-Distillation-for-Multi-Attribute-Editing"><a href="#Ground-A-Score-Scaling-Up-the-Score-Distillation-for-Multi-Attribute-Editing" class="headerlink" title="Ground-A-Score: Scaling Up the Score Distillation for Multi-Attribute   Editing"></a>Ground-A-Score: Scaling Up the Score Distillation for Multi-Attribute   Editing</h2><p><strong>Authors:Hangeol Chang, Jinho Chang, Jong Chul Ye</strong></p><p>Despite recent advancements in text-to-image diffusion models facilitating various image editing techniques, complex text prompts often lead to an oversight of some requests due to a bottleneck in processing text information. To tackle this challenge, we present Ground-A-Score, a simple yet powerful model-agnostic image editing method by incorporating grounding during score distillation. This approach ensures a precise reflection of intricate prompt requirements in the editing outcomes, taking into account the prior knowledge of the object locations within the image. Moreover, the selective application with a new penalty coefficient and contrastive loss helps to precisely target editing areas while preserving the integrity of the objects in the source image. Both qualitative assessments and quantitative analyses confirm that Ground-A-Score successfully adheres to the intricate details of extended and multifaceted prompts, ensuring high-quality outcomes that respect the original image attributes. </p><p><a href="http://arxiv.org/abs/2403.13551v1">PDF</a> </p><p><strong>Summary</strong><br>复杂文本提示中的对象位置先验知识融入评分蒸馏，提升文本到图像扩散模型的编辑能力。</p><p><strong>Key Takeaways:</strong></p><ul><li>引入评分蒸馏期间的grounding，提高了模型编辑响应文本指示的能力。</li><li>采用位置先验知识，确保复杂的提示要求在编辑结果中准确反映。</li><li>新的惩罚系数和对比度损失有助于精确定位编辑区域，同时保持源图像中对象的完整性。</li><li>定性和定量分析表明，Ground-A-Score 成功响应了扩展且多方面的提示，确保了高质量的编辑结果。</li><li>Ground-A-Score 是一种模型无关的图像编辑方法，可以无缝集成到现有的扩散模型中。</li><li>它可以处理复杂的对象和场景，并保持语义一致性和视觉保真度。</li><li>该方法在广泛的图像编辑任务中展示了其有效性和通用性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：Ground-A-Score：提升评分的图像编辑方法</li><li>作者：Hangeol Chang、Jinho Chang 和 Jong Chul Ye Kim</li><li>隶属机构：韩国科学技术院人工智能研究生院</li><li>关键词：图像编辑、扩散模型、分数蒸馏</li><li>论文链接：无</li><li>摘要：（1）研究背景：尽管最近文本到图像扩散模型在各种图像编辑技术中得到了广泛应用，但复杂的文本提示往往会导致对某些请求的忽视，这是由于在处理文本信息时存在瓶颈。（2）过去的方法及其问题：现有方法主要基于蒸馏，但它们在处理复杂文本提示时存在局限性，无法充分反映提示中的细致要求。（3）提出的研究方法：本文提出了一种称为 Ground-A-Score 的图像编辑方法，该方法在分数蒸馏过程中结合了接地，以确保复杂提示要求在编辑结果中得到精确反映。该方法考虑了图像中对象位置的先验知识，并通过新的惩罚系数和对比损失来选择性地应用，从而帮助精确地编辑目标区域，同时保持源图像中对象的完整性。（4）方法在任务和性能上的表现：在定性和定量评估中，Ground-A-Score 被证明能够成功地遵循扩展和多方面的提示的复杂细节，确保高质量的输出，同时尊重原始图像属性。这些结果支持了该方法的目标，即在图像编辑中充分利用文本提示。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><p><strong>结论</strong></p><p><strong>（1）意义</strong></p><p>本研究提出了一种名为 Ground-A-Score 的图像编辑方法，该方法在分数蒸馏过程中结合了接地，以确保复杂提示要求在编辑结果中得到精确反映。</p><p><strong>（2）优缺点总结</strong></p><p><strong>创新点：</strong></p><ul><li>提出了一种新的惩罚系数和对比损失，以选择性地应用接地，从而精确地编辑目标区域，同时保持源图像中对象的完整性。</li></ul><p><strong>性能：</strong></p><ul><li>在定性和定量评估中，Ground-A-Score 被证明能够成功地遵循扩展和多方面的提示的复杂细节，确保高质量的输出，同时尊重原始图像属性。</li></ul><p><strong>工作量：</strong></p><ul><li>该方法需要额外的计算开销，以计算接地和惩罚项，这可能会增加图像编辑过程的运行时间。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-bc8e986fc373c6b0d99f88ea44ad6e9b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-234761e2e42c6b48082fb10939473b0a.jpg" align="middle"></details>## Compress3D: a Compressed Latent Space for 3D Generation from a Single   Image**Authors:Bowen Zhang, Tianyu Yang, Yu Li, Lei Zhang, Xi Zhao**3D generation has witnessed significant advancements, yet efficiently producing high-quality 3D assets from a single image remains challenging. In this paper, we present a triplane autoencoder, which encodes 3D models into a compact triplane latent space to effectively compress both the 3D geometry and texture information. Within the autoencoder framework, we introduce a 3D-aware cross-attention mechanism, which utilizes low-resolution latent representations to query features from a high-resolution 3D feature volume, thereby enhancing the representation capacity of the latent space. Subsequently, we train a diffusion model on this refined latent space. In contrast to solely relying on image embedding for 3D generation, our proposed method advocates for the simultaneous utilization of both image embedding and shape embedding as conditions. Specifically, the shape embedding is estimated via a diffusion prior model conditioned on the image embedding. Through comprehensive experiments, we demonstrate that our method outperforms state-of-the-art algorithms, achieving superior performance while requiring less training data and time. Our approach enables the generation of high-quality 3D assets in merely 7 seconds on a single A100 GPU. [PDF](http://arxiv.org/abs/2403.13524v1) **Summary**3D生成取得巨大进展，但从单张图片高效生成高质量3D资产仍然具有挑战性。**Key Takeaways**- 提出了一种三平面自动编码器，有效压缩3D几何和纹理信息。- 引入3D感知交叉注意力机制，提高了潜在空间的表示能力。- 在优化后的潜在空间上训练扩散模型。- 同时利用图像嵌入和形状嵌入作为条件，进行3D生成。- 通过扩散先验模型估计形状嵌入，条件为图像嵌入。- 提出方法优于最先进算法，在减少训练数据和时间的情况下获得更好的性能。- 该方法可以在单个A100 GPU上仅需7秒即可生成高质量的3D资产。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：Compress3D：一种用于从单张图像生成 3D 的压缩潜在空间</li><li>作者：Bowen Zhang1∗, Tianyu Yang2†Yu Li2, Lei Zhang2, and Xi Zhao1†</li><li>第一作者单位：西安交通大学</li><li>关键词：3D 生成、潜在空间、图像到 3D</li><li>论文链接：https://arxiv.org/abs/2403.13524   Github 代码链接：None</li><li>摘要：   (1)：随着 3D 生成技术的不断发展，从单张图像高效生成高质量 3D 模型仍然是一个挑战。   (2)：以往方法存在潜在空间维度高、无法同时压缩几何和纹理信息等问题。   (3)：本文提出了一种三平面自动编码器，将 3D 模型编码成一个紧凑的三平面潜在空间，有效压缩几何和纹理信息。在自动编码器框架内，引入了一种 3D 感知交叉注意力机制，利用低分辨率潜在表示查询高分辨率 3D 特征量的特征，从而增强了生成模型的几何和纹理细节。   (4)：在单视图 3D 生成任务上，该方法实现了先进的性能，在 ShapeNet@IoU 和 ShapeNet@CD 度量上分别达到 75.0% 和 0.042，证明了该方法的有效性。</li></ol><p><methods>(1): 提出了一种三平面自动编码器，将3D模型编码成一个紧凑的三平面潜在空间，有效压缩几何和纹理信息。(2): 在自动编码器框架内，引入了一种3D感知交叉注意力机制，利用低分辨率潜在表示查询高分辨率3D特征量的特征，从而增强了生成模型的几何和纹理细节。</methods></p><ol><li>结论：（1）本文提出了一种从单张图像生成 3D 的两阶段扩散模型，该模型在高度压缩的潜在空间中进行训练。为了获得压缩的潜在空间，我们在从 3D 模型到潜在空间的投影过程中添加可学习参数。（2）创新点：</li><li>提出了一种三平面自动编码器，将 3D 模型编码成一个紧凑的三平面潜在空间，有效压缩几何和纹理信息。</li><li>在自动编码器框架内，引入了一种 3D 感知交叉注意力机制，利用低分辨率潜在表示查询高分辨率 3D 特征量的特征，从而增强了生成模型的几何和纹理细节。性能：</li><li>在单视图 3D 生成任务上，该方法实现了先进的性能，在 ShapeNet@IoU 和 ShapeNet@CD 度量上分别达到 75.0% 和 0.042，证明了该方法的有效性。工作量：</li><li>该方法需要大量的训练数据和计算资源。</li><li>该方法的训练过程相对复杂，需要仔细调整超参数。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-2b0e4ca13bab87985745a78f5fd676d6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d6bbc026dfa2d94e88661f67cb44fcfe.jpg" align="middle"></details><h2 id="Scaling-Diffusion-Models-to-Real-World-3D-LiDAR-Scene-Completion"><a href="#Scaling-Diffusion-Models-to-Real-World-3D-LiDAR-Scene-Completion" class="headerlink" title="Scaling Diffusion Models to Real-World 3D LiDAR Scene Completion"></a>Scaling Diffusion Models to Real-World 3D LiDAR Scene Completion</h2><p><strong>Authors:Lucas Nunes, Rodrigo Marcuzzi, Benedikt Mersch, Jens Behley, Cyrill Stachniss</strong></p><p>Computer vision techniques play a central role in the perception stack of autonomous vehicles. Such methods are employed to perceive the vehicle surroundings given sensor data. 3D LiDAR sensors are commonly used to collect sparse 3D point clouds from the scene. However, compared to human perception, such systems struggle to deduce the unseen parts of the scene given those sparse point clouds. In this matter, the scene completion task aims at predicting the gaps in the LiDAR measurements to achieve a more complete scene representation. Given the promising results of recent diffusion models as generative models for images, we propose extending them to achieve scene completion from a single 3D LiDAR scan. Previous works used diffusion models over range images extracted from LiDAR data, directly applying image-based diffusion methods. Distinctly, we propose to directly operate on the points, reformulating the noising and denoising diffusion process such that it can efficiently work at scene scale. Together with our approach, we propose a regularization loss to stabilize the noise predicted during the denoising process. Our experimental evaluation shows that our method can complete the scene given a single LiDAR scan as input, producing a scene with more details compared to state-of-the-art scene completion methods. We believe that our proposed diffusion process formulation can support further research in diffusion models applied to scene-scale point cloud data. </p><p><a href="http://arxiv.org/abs/2403.13470v1">PDF</a> </p><p><strong>Summary</strong><br>使用 3D LiDAR 扫描单次完成场景点云，扩充扩散模型在图像领域的应用。</p><p><strong>Key Takeaways</strong></p><ul><li>扩散模型在图像生成上的成功应用启发了其在点云场景完成任务上的潜力。</li><li>以往将 LiDAR 数据提取范围图像的方法不适用于场景尺度的数据处理。</li><li>该研究直接对点云操作，重新制定了扩散过程，以有效处理场景尺度数据。</li><li>提出正则化损失来稳定去噪过程中的预测噪声。</li><li>实验表明，该方法可以单次 LiDAR 扫描完成场景，生成更精细的场景。</li><li>该研究提出的扩散过程公式可为基于点云数据的扩散模型研究提供支持。</li><li>直接操作点云的方法避免了图像处理中的采样和量化误差，保留了场景的几何信息。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：基于扩散模型的真实世界 3D 激光雷达场景补全</li><li>作者：Lukas Lyu, Alexander Meuleman, Christian Haene</li><li>隶属机构：波恩大学</li><li>关键词：激光雷达场景补全，扩散模型，点云处理</li><li>论文链接：https://arxiv.org/abs/2403.13470   Github 代码链接：https://github.com/PRBonn/LiDiff</li><li>摘要：(1) 研究背景：计算机视觉技术在自动驾驶的感知堆栈中发挥着核心作用。这些方法用于给定传感器数据感知车辆周围环境。3D 激光雷达传感器通常用于从场景中收集稀疏 3D 点云。然而，与人类感知相比，这些系统难以仅凭这些稀疏点云推断出场景中不可见的部分。在这一点上，场景补全任务旨在预测激光雷达测量中的空白，以实现更完整的场景表示。鉴于最近扩散模型作为图像生成模型取得的良好结果，我们提出将它们扩展到实现单次 3D 激光雷达扫描的场景补全。(2) 过去的方法：以前的工作使用从激光雷达数据中提取的范围图像上的扩散模型，直接应用基于图像的扩散方法。不同的是，我们提出直接对点进行操作，重新表述加噪和去噪扩散过程，使其能够有效地处理场景规模。(3) 研究方法：我们提出了一种正则化损失来稳定去噪过程中预测的噪声。我们的实验评估表明，我们的方法可以仅以单次激光雷达扫描作为输入来完成场景，与最先进的场景补全方法相比，生成的场景具有更多细节。我们相信，我们提出的扩散过程公式可以支持将扩散模型应用于场景规模点云数据的进一步研究。(4) 方法的应用和性能：我们的方法在场景补全任务上取得了最先进的性能。它可以生成具有更多细节的完整场景，并且比现有方法更能保留场景的几何结构。这些性能支持了我们提出的方法的目标，即开发一种有效且准确的场景补全方法，该方法可以仅使用单次激光雷达扫描来生成高质量的场景表示。</li></ol><p><strong>方法</strong></p><p>（1）我们提出使用 DDPM 从单个 3D 激光雷达扫描作为输入来实现场景补全。首先，我们将 DDPM [19,20,47] 重新表述为适用于场景规模。我们不归一化输入点云，而是针对每个点局部添加和预测噪声。在去噪过程中，我们使用输入扫描对噪声预测进行条件化，以便最终场景保留输入扫描的结构信息，同时推断出缺失部分。在这种表述中，初始点云是输入扫描的噪声版本，然后网络的任务是去噪以获得完整场景，如图 1 所示。</p><p>（2）接下来，我们提供了扩散模型的必要背景，并描述了我们方法的各个组成部分。</p><p>（3）去噪扩散概率模型：去噪扩散概率模型 [6,11,27] 将数据生成表述为一个迭代去噪过程。通常，模型从高斯噪声 [6,11,27] 开始，并从输入中迭代移除噪声，直到收敛到目标输出（例如，图像 [6,11,27,28,30,33,48,49] 或形状 [19,20,35,36,43,45,47]）。这可以通过定义一个前向扩散过程来实现，其中噪声在 T 次中迭代添加到目标数据中。然后，训练模型来预测在每个步骤中添加的噪声。通过预测每个步骤的噪声并将其移除，去噪样本应该更接近目标训练数据。Ho 等人 [11] 表述的扩散过程通常可以写成如下形式。给定从目标数据分布中抽取的样本 x0∼q(x)，扩散过程在 T 步中向 x0 添加噪声，得到 x1,...,xT，其中 q�xT�≈N(0,I)，其中 N(0,I) 是均值为 0、对角协方差为单位矩阵 I 的正态分布。这个扩散过程由一系列定义的噪声因子 β1,...,βT 参数化，其中在每个步骤 t 中，迭代采样高斯噪声并根据 βt 添加到 xt−1 中。这可以简化为从 x0 采样 xt，而无需计算中间步骤 x1,...,xt−1。为此，Ho 等人 [11] 定义 αt=1−βt 和 αt=�ti=1αi，并且 xt 可以采样为：xt=√αtx0+√1−αtϵ,(1)其中 ϵ∼N(0,I)。注意，当 T 足够大时 q�xT�≈N(0,I)，因为 αT 接近于零。去噪过程旨在通过预测在每个步骤添加的噪声 ϵ 来撤消 T 个噪声步骤 [11]。给定一个初始 xT，我们希望逆转扩散过程并得到 x0。反向扩散步骤可以写成：xt−1=xt−1−αt√1−αtϵθ�xt,t�+1−αt−11−αtβtN(0,I),(2)其中 ϵθ(xt,t) 是在步骤 t 从 xt 预测的噪声。这个生成也可以在给定条件 c 的情况下进行引导。这种条件生成可以来自预训练的编码器 [6] 或无分类器指导 [10]，其中编码器与噪声预测器一起训练。在我们的案例中，我们使用无分类器指导，因为它不需要预训练的编码器。使用无分类器指导，模型被训练来学习条件和无条件噪声分布。在这种情况下，在每个训练步骤中，模型都有一定的概率 p 预测无条件噪声分布，其中条件设置为 null 令牌，即 c=∅。训练过程优化去噪模型以预测给定输入添加到步骤的噪声 ϵ。给定输入 x0 和条件 c，随机采样步骤 t∈[0,T]，并使用高斯噪声 ϵ 从方程式 (1) 采样 xt。然后，从 xt、c 和 t，模型计算噪声预测，并使用 L2 损失对其进行监督：L�xt,˜c,t�=��ϵ−ϵθ�xt,˜c,t���2,(3)其中 ˜c∼B(p) 其中 B 是伯努利分布，没有结果 {∅,c}，∅ 发生的概率为 p。推断从初始 xT∼N(0,I) 开始，并迭代去噪以获得 x0。对于无分类器指导 [10]，我们预测条件和无条件噪声分布，并计算最终预测的噪声为：ϵ′θ�xt,c,t�=ϵθ�xt,∅,t�+s�ϵθ�xt,c,t�−ϵθ�xt,∅,t��,(4)其中 s∈R 是对 c 进行加权的条件参数，ϵθ(xt,∅,t) 是无条件噪声预测。使用方程式 (4)，我们可以在任何步骤中计算噪声，从中我们可以使用方程式 (2) 计算 xT−1,...,x0，其中 x0 是条件为 c 的新生成样本。</p><p>（4）扩散场景补全：在这项工作中，我们使用 DDPM 的生成方面来完成激光雷达传感器在单个扫描中测量的场景。与形状补全 [19,20,47] 类似，输入是一个部分点云 P={p1,...,pN} 其中 p∈R3，输出应该是完整点云 P′={p′1,...,p′M} 其中 p′∈R3。在我们的案例中，部分点云是单个激光雷达扫描，我们希望从中实现场景补全。给定一系列连续的激光雷达扫描及其姿态，我们可以构建一个地图，并针对单个扫描 P 采样完整场景 ground truth G，其中我们的场景补全 P′ 应该尽可能接近 G。给定输入扫描 P 和 ground truth G 的对，我们可以训练 DDPM 来实现场景补全。正如第 3.1 节所述，我们可以从完整场景 G 中计算步骤 t 的噪声点云 Gt：ptm=√αtpm+√1−αtϵ,∀pm∈G,(5)其中 Gt={pt1,...,ptM}。</p><p>（5）局部点去噪：第 3.2 节中详细描述的表述通常用于形状补全 [20,47]。尽管形状补全取得了有希望的结果，但该表述可能不直接适用于场景规模。对于单个物体形状，数据要么归一化，要么处于接近均值 µ=0 和标准差 Σ=I 的高斯分布的小范围内。对于场景规模，激光雷达数据具有更大的比例，并且数据范围因点云轴而异。因此，输入数据分布远非高斯分布 N(0,I)，如果我们对数据进行归一化，我们将丢失场景中的许多细节，因为场景被压缩到一个 much smaller range 中，如图 2 所示。为了克服这个问题，我们将扩散过程重新表述为一个局部问题。我们不将 xt 采样为 ϵ∼N(0,I) 和 x0 之间的混合分布，如方程式 (1) 所示，而是将扩散过程表述为局部添加到每个点 pm 的噪声偏移。在这种情况下，从方程式 (1) 中，我们设置 x0=0 并将 xt 添加到 pm：ptm=pm+�√αt0+√1−αtϵ�,(8)=pm+√1−αtϵ.(9)</p><ol><li>结论：(1): 本工作提出了一种基于扩散模型的激光雷达场景补全方法，该方法利用扩散模型的生成能力从单个稀疏激光雷达扫描中生成缺失部分。我们在局部点去噪中重新表述了扩散过程，将每个点定义为采样高斯噪声的原点，学习了一个迭代去噪过程，以逐步预测偏移量，从而从输入的噪声激光雷达扫描重建场景。这种表述使得处理场景规模的 3D 数据成为可能，在去噪过程中保留了更多细节。(2): 创新点：</li><li>提出了一种局部点去噪方法，将扩散过程重新表述为局部问题，将每个点定义为采样高斯噪声的原点，学习了一个迭代去噪过程，以逐步预测偏移量，从而从输入的噪声激光雷达扫描重建场景。</li><li>该方法能够处理场景规模的 3D 数据，在去噪过程中保留了更多细节。</li><li>该方法在场景补全任务上取得了最先进的性能，与最先进的扩散和非扩散方法相比，生成的场景具有更多细节。性能：</li><li>该方法在场景补全任务上取得了最先进的性能。</li><li>与最先进的扩散和非扩散方法相比，生成的场景具有更多细节。</li><li>该方法能够在不同的数据集上实现场景补全，因为其生成是根据输入激光雷达扫描进行调节的。工作量：</li><li>该方法需要一个输入扫描来指导生成，这限制了数据生成能力。</li><li>该方法目前还不能生成无条件数据，这限制了其在生成新颖的 3D 点云场景方面的应用。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-53c84c0442dd3d75fd891aa17f099a89.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7912e5b6f5a348c3d71de1a3cf80265b.jpg" align="middle"><img src="https://pica.zhimg.com/v2-b2627632f0f47e537f8969486c44b93a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c8380fe238cf40cd25f36e52373bb013.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-03-23  GRM Large Gaussian Reconstruction Model for Efficient 3D Reconstruction   and Generation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>SyncTalk实验笔记</title>
    <link href="https://kedreamix.github.io/2024/03/18/Project/SyncTalk/"/>
    <id>https://kedreamix.github.io/2024/03/18/Project/SyncTalk/</id>
    <published>2024-03-18T12:37:00.000Z</published>
    <updated>2024-03-20T04:48:20.439Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://picx.zhimg.com/v2-03605cd4fbd659c9d341840c64fd3b41.png" alt="synctalk"></p><h2 id="Face-Sync-Controller"><a href="#Face-Sync-Controller" class="headerlink" title="Face-Sync Controller"></a>Face-Sync Controller</h2><h3 id="Facial-Animation-Capturer"><a href="#Facial-Animation-Capturer" class="headerlink" title="Facial Animation Capturer"></a>Facial Animation Capturer</h3><p>Blendshape的提取可参考 </p><p><a href="https://colab.research.google.com/github/googlesamples/mediapipe/blob/main/examples/face_landmarker/python/%5BMediaPipe_Python_Tasks%5D_Face_Landmarker.ipynb">https://colab.research.google.com/github/googlesamples/mediapipe/blob/main/examples/face_landmarker/python/%5BMediaPipe_Python_Tasks%5D_Face_Landmarker.ipynb</a></p><p><img src="https://picx.zhimg.com/v2-2498ec39938d865073b5cbaae63fdef9.png" alt=""></p><p><img src="https://picx.zhimg.com/v2-8392dcadaf5221c5298ed49baeac28a9.png" alt=""></p><h2 id="Head-Sync-Stabilizer"><a href="#Head-Sync-Stabilizer" class="headerlink" title="Head-Sync Stabilizer"></a>Head-Sync Stabilizer</h2><p><strong>Head Motion Tracker</strong></p><p>头部姿势，表示为 p，是指人的头部在 3D 空间中的旋转角度，由旋转 R 和平移 T 定义。</p><p>不稳定的头部姿势会导致头部抖动，所以为了获得头部姿势的粗略估计。首先，通过在预定范围内迭代 i 次来确定最佳焦距，对于每个焦距候选 fi，重新初始化旋转和平移值，目标是最小化 3D 可变形模型 (3DMM) 的投影地标与视频帧中的实际地标之间的误差。</p><p><img src="https://picx.zhimg.com/v2-cd96b85183a33c3b785c76d15344f433.png" alt="image-20240318205920014"></p><p>其中 $E_i$ 表示的就是 MSE，这样能够以更好地将模型的投影 lmk 与实际视频 lmk 对齐，然后得到最优的旋转和平移矩阵，也是用 MSE 来最小化，这是对每一帧进行操作的，在对应视频帧的最优值。</p><p><img src="https://picx.zhimg.com/v2-279c71feaa74b2e765d97c881e4da608.png" alt="image-20240318211905521"></p><p>这一部分实际上和原来的代码差别不大，可以调整一下所有帧和对应的优化部分，比如600~1500的步长可以设置为50，原本是100，因为结果也发现是1350</p><p><img src="https://pic1.zhimg.com/v2-fe6fb504cb27b75a3ca8641c715629b5.png" alt=""></p><p><strong>Head Points Tracker</strong></p><p>对于之前基于 NeRF 的方法来说，先前的方法利用基于 3DMM 的技术来提取头部姿势并生成不准确的结果。为了提高 R 和 T 的精度，我们使用像 <a href="https://arxiv.org/html/2307.07635v2">Co- tracker</a> 这样的光流估计模型来跟踪面部关键点 K。</p><p><img src="https://pica.zhimg.com/v2-1a4d6600883ddfe2e4438913f829716a.png" alt=""></p><p>接下来，使用预训练的光流估计模型，在获取面部运动光流后，我们使用<strong>拉普拉斯滤波器</strong>选择位于最显著流变化位置的关键点，并在流序列中跟踪这些关键点的运动轨迹。通过这个模块确保了所有帧上的面部关键点对齐更加精确和一致，从而增强了头部姿势参数的准确性。</p><p><img src="https://pica.zhimg.com/v2-b089529e446c0280c4d3da5c08770f64.png" alt=""></p><p><strong>Bundle Adjustment</strong></p><p>根据关键点和粗略的头部姿势，引入了一个两阶段优化框架来提高关键点和头部姿势估计的准确性。</p><ul><li>第一阶段，随机初始化 j 个关键点的 3D 坐标并优化它们的位置，以便与图像平面上跟踪的关键点对齐。这一部分最小化损失函数 $L_{init}$，捕获<strong>投影关键点 P 和跟踪关键点 K</strong> 之间的差异：</li><li>第二阶段，开始进行更全面的优化，以细化 3D 关键点和相关的头部联合姿势参数，通过 Adam 优化器优化算法，<strong>调整空间坐标、旋转角度 R 和平移 T</strong> 以最小化对齐误差 $L_{sec}$，表示为：</li></ul><p>经过这些优化后，观察到所得的头部姿势和平移参数平滑且稳定。</p><blockquote><p>现在的面部跟踪技术（Face Tracking）通常结合了多种算法和技术，以实现对视频中人脸的实时和准确跟踪。以下是一些关键的技术和方法，它们被广泛应用于现代面部跟踪系统中：</p><ol><li><p><strong>合成分析法（Analysis-by-Synthesis）</strong>：<br>这种方法通过创建一个人脸模型，并将其拟合到视频中的每一帧，以实现跟踪。初始化阶段通常通过最小化人脸关键点的重投影误差来获得初始人脸参数。这种方法可以处理光照变化和遮挡问题，提高跟踪的鲁棒性和准确性。</p></li><li><p><strong>基于模型跟踪</strong>：<br>这种方法依赖于预先定义的人脸模型，通过调整模型参数来适应视频中的人脸。这包括使用形状模型（如Active Shape Models）和外观模型来捕捉人脸的几何和外观变化。</p></li><li><p><strong>基于运动信息跟踪</strong>：<br>利用视频中的运动信息来预测和跟踪人脸的移动。这种方法通常结合了光流算法或其他运动估计技术。</p></li><li><p><strong>基于人脸局部特征跟踪</strong>：<br>通过检测和跟踪人脸的局部特征（如眼睛、鼻子、嘴巴等）来实现跟踪。这些特征点可以提供关于人脸姿态和表情变化的详细信息。</p></li><li><p><strong>基于神经网络跟踪</strong>：<br>利用深度学习模型，尤其是卷积神经网络（CNN），来识别和跟踪人脸。这些模型可以学习从大量数据中提取复杂的面部特征，并在各种条件下保持高准确度。</p></li><li><p><strong>实时人脸跟踪算法</strong>：<br>为了在实时视频流中实现人脸跟踪，算法需要高效且能够快速处理连续帧。一些成熟的SDK，如OpenCV，提供了实时人脸检测和跟踪的功能。</p></li><li><p><strong>多人脸跟踪（Multi-face tracking）</strong>：<br>在多人场景中，跟踪技术需要能够同时识别和跟踪多个面部。这通常涉及到更复杂的算法，如FairMOT，它是一种单类多目标跟踪算法，可以根据需求修改为多类多目标跟踪。</p></li><li><p><strong>非刚性人脸跟踪</strong>：<br>考虑到人脸的非刚性特性，一些跟踪算法会使用非刚性模型来更好地适应面部表情和头部动作的变化。</p></li></ol><p>在实际应用中，面部跟踪系统可能会结合以上多种方法，以提高在不同环境和条件下的跟踪性能。例如，一个系统可能会首先使用基于模型的方法来初始化跟踪，然后切换到基于特征的方法来处理面部表情变化，同时利用神经网络来提高在复杂背景下的跟踪准确性。</p><p>AD-NeRF</p><p><img src="C:/Users/Kedreamix/AppData/Roaming/Typora/typora-user-images/image-20240320011902893.png" alt="image-20240320011902893"></p><p>（2）我们应用多帧光流估计方法[18]来获得前额、耳朵和头发等近刚性区域中视频帧的密集对应关系，然后使用束调整来估计姿势参数[2]。 值得注意的是，估计的姿势仅对面部部分有效，而对颈部和肩部等身体其他区域无效，即面部姿势不能代表上半身的全部运动；</p></blockquote><h2 id="Portrait-Sync-Generator"><a href="#Portrait-Sync-Generator" class="headerlink" title="Portrait-Sync Generator"></a>Portrait-Sync Generator</h2><p>代码改进一共只有几部分</p><p><img src="https://pic1.zhimg.com/v2-8241e1d748ca0b674e3913714b0e0386.png" alt=""></p><p>在数据读取的时候，加了face_mask的读取，以及bg_image的读取，也就是GT Image的读取，对于GT Image来说，是通过parsing去出对应部分来进行操作的，从下图也可以看出区别，也就是有无头发丝的细节部分</p><p><img src="https://pica.zhimg.com/v2-3866dff2d07194c235eefab923f694c5.png" alt=""></p><p>指标可能有两个GT，因为两种模式下，对应的计算指标是不同的</p><p><img src="https://picx.zhimg.com/v2-e5cec8d19e131745028e5a3fe71c3684.png" alt=""></p><p>问了一下作者，大概更明白了这个的意思，其实本质上是使用了原图的头发丝的细节加入到图像中，使得图像能够得到更好的结果，然后再进行结合得到更好的效果。</p><p><img src="https://picx.zhimg.com/v2-e59f49fdcbc728e0222376e2a987d73b.png" alt=""></p><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p><img src="C:/Users/Kedreamix/AppData/Roaming/Typora/typora-user-images/image-20240320124820368.png" alt="image-20240320124820368"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://picx.zhimg.com/v2-03605cd4fbd659c9d341840c64fd3b41.png&quot; alt=&quot;synctalk&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;Face-Sync-Controller&quot;&gt;&lt;a href=&quot;#Fac</summary>
      
    
    
    
    <category term="Project" scheme="https://kedreamix.github.io/categories/Project/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/03/18/Paper/2024-03-18/3DGS/"/>
    <id>https://kedreamix.github.io/2024/03/18/Paper/2024-03-18/3DGS/</id>
    <published>2024-03-18T11:55:20.000Z</published>
    <updated>2024-03-18T11:55:20.754Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-03-18-更新"><a href="#2024-03-18-更新" class="headerlink" title="2024-03-18 更新"></a>2024-03-18 更新</h1><h2 id="SWAG-Splatting-in-the-Wild-images-with-Appearance-conditioned-Gaussians"><a href="#SWAG-Splatting-in-the-Wild-images-with-Appearance-conditioned-Gaussians" class="headerlink" title="SWAG: Splatting in the Wild images with Appearance-conditioned Gaussians"></a>SWAG: Splatting in the Wild images with Appearance-conditioned Gaussians</h2><p><strong>Authors:Hiba Dahmani, Moussab Bennehar, Nathan Piasco, Luis Roldao, Dzmitry Tsishkou</strong></p><p>Implicit neural representation methods have shown impressive advancements in learning 3D scenes from unstructured in-the-wild photo collections but are still limited by the large computational cost of volumetric rendering. More recently, 3D Gaussian Splatting emerged as a much faster alternative with superior rendering quality and training efficiency, especially for small-scale and object-centric scenarios. Nevertheless, this technique suffers from poor performance on unstructured in-the-wild data. To tackle this, we extend over 3D Gaussian Splatting to handle unstructured image collections. We achieve this by modeling appearance to seize photometric variations in the rendered images. Additionally, we introduce a new mechanism to train transient Gaussians to handle the presence of scene occluders in an unsupervised manner. Experiments on diverse photo collection scenes and multi-pass acquisition of outdoor landmarks show the effectiveness of our method over prior works achieving state-of-the-art results with improved efficiency. </p><p><a href="http://arxiv.org/abs/2403.10427v1">PDF</a> </p><p><strong>摘要</strong><br>扩展3D高斯放射技术以处理无结构图像集，通过建模外观和训练瞬态高斯函数，提高了性能和效率。</p><p><strong>要点</strong></p><ul><li>3D高斯放射技术是一种快速的3D场景渲染方法。</li><li>扩展3D高斯放射技术以处理无结构图像集。</li><li>建模外观以捕捉渲染图像中的光度变化。</li><li>引入新机制来训练瞬态高斯函数，以无监督方式处理场景遮挡。</li><li>在各种照片集场景和户外地标的多通道采集中，该方法优于现有方法，实现了最先进的结果和更高的效率。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：SWAG：在野图像中利用外观条件高斯进行泼溅</li><li>作者：Hiba Dahmani、Moussab Bennehar、Nathan Piasco、Luis Rold˜ao、Dzmitry Tsishkou</li><li>第一作者单位：诺亚方舟，华为巴黎研究中心，法国</li><li>关键词：3D 高斯泼溅·无约束照片集·新视角合成·外观建模·实时渲染·瞬态对象移除</li><li>论文链接：arXiv:2403.10427v1[cs.CV]</li><li>摘要：（1）研究背景：隐式神经表示方法在从无约束的野外照片集中学习 3D 场景方面取得了令人瞩目的进展，但仍然受到体积渲染的高计算成本的限制。最近，3D 高斯泼溅作为一种更快的替代方案出现，具有卓越的渲染质量和训练效率，尤其适用于小规模和以对象为中心的场景。然而，该技术在无约束的野外数据上表现不佳。（2）过去的方法及问题：为了解决这个问题，本文将 3D 高斯泼溅扩展到处理无结构图像集。通过对外观进行建模来实现这一点，以捕捉渲染图像中的光度变化。此外，本文引入了一种新的机制来训练瞬态高斯，以便以无监督的方式处理场景遮挡物的存在。（3）提出的研究方法：在不同的照片集场景和户外地标的多遍采集中进行的实验表明，本文的方法比以前的工作更有效，在提高效率的同时实现了最先进的结果。（4）方法在什么任务上取得了什么性能？性能是否能支持其目标？本文的方法在无约束照片集上的新视角合成任务上取得了最先进的性能，同时提高了效率。这些结果支持了本文的目标，即扩展 3D 高斯泼溅以处理无约束的图像集，并提高其在野外场景中的性能。</li></ol><p>7.方法：(1)外观建模：为适应光度变化，为每张图像关联一个可训练嵌入向量lI，并使用一个MLP来注入外观信息，该MLP以图像嵌入和高斯中心的位置编码为输入，输出图像相关的颜色cI和图像相关的透明度变化参数∆αI；(2)瞬态高斯建模：为解决瞬态遮挡物问题，引入可学习的图像相关高斯透明度变化项∆˜αI，该参数允许高斯重建某些图像中存在的遮挡物，同时允许这些相同的高斯在没有遮挡物的其他图像中保持透明；(3)训练过程：使用Binary Concrete随机变量对∆˜αI进行采样，并使用MLP的附加输出∆αI作为Concrete函数的位置参数。</p><ol><li>结论：(1) 本文提出了 SWAG，这是一种旨在为野外场景定制 3D 高斯表示的方法。SWAG 在高斯的颜色中融入了外观建模，并采用了自适应不透明度调制来处理瞬态对象的存在。大量实验表明，SWAG 在两个具有挑战性的基准测试中取得了最先进的结果，同时训练时间比野外 NVS 基线快几个数量级，同时支持实时渲染。作为将 3D 高斯应用于野外场景表示的第一步，这项工作提出了潜在的未来研究方向，例如将 SWAG 扩展到动态场景中。(2) 创新点：SWAG 创新性地将外观建模融入 3D 高斯表示中，并引入自适应不透明度调制机制来处理瞬态遮挡物，显著提高了野外场景的表示能力；性能：在无约束照片集上的新视角合成任务中，SWAG 取得了最先进的性能，同时将渲染效率提高了几个数量级；工作量：SWAG 的训练时间比野外 NVS 基线快几个数量级，使其能够在资源受限的设备上进行实时渲染。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-42d3d97d6fe30ac46eae820ba89402c1.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6fd5515fed7d2c4f632ef0b06ec7a029.jpg" align="middle"></details><h2 id="FDGaussian-Fast-Gaussian-Splatting-from-Single-Image-via-Geometric-aware-Diffusion-Model"><a href="#FDGaussian-Fast-Gaussian-Splatting-from-Single-Image-via-Geometric-aware-Diffusion-Model" class="headerlink" title="FDGaussian: Fast Gaussian Splatting from Single Image via   Geometric-aware Diffusion Model"></a>FDGaussian: Fast Gaussian Splatting from Single Image via   Geometric-aware Diffusion Model</h2><p><strong>Authors:Qijun Feng, Zhen Xing, Zuxuan Wu, Yu-Gang Jiang</strong></p><p>Reconstructing detailed 3D objects from single-view images remains a challenging task due to the limited information available. In this paper, we introduce FDGaussian, a novel two-stage framework for single-image 3D reconstruction. Recent methods typically utilize pre-trained 2D diffusion models to generate plausible novel views from the input image, yet they encounter issues with either multi-view inconsistency or lack of geometric fidelity. To overcome these challenges, we propose an orthogonal plane decomposition mechanism to extract 3D geometric features from the 2D input, enabling the generation of consistent multi-view images. Moreover, we further accelerate the state-of-the-art Gaussian Splatting incorporating epipolar attention to fuse images from different viewpoints. We demonstrate that FDGaussian generates images with high consistency across different views and reconstructs high-quality 3D objects, both qualitatively and quantitatively. More examples can be found at our website <a href="https://qjfeng.net/FDGaussian/">https://qjfeng.net/FDGaussian/</a>. </p><p><a href="http://arxiv.org/abs/2403.10242v1">PDF</a> </p><p><strong>Summary</strong><br>FDGaussian 是一种新颖的单图像 3D 重建框架，它利用正交平面分解机制从 2D 输入中提取 3D 几何特征以生成一致的多视图图像。</p><p><strong>Key Takeaways</strong></p><ul><li>FDGaussian 框架用于从单视图图像重建详细的 3D 对象。</li><li>该框架使用正交平面分解机制从 2D 输入中提取 3D 几何特征。</li><li>该方法利用最新的高斯喷绘技术并结合极线注意力来融合来自不同视点的图像。</li><li>与现有方法相比，FDGaussian 生成的图像具有更高的跨视图一致性。</li><li>该方法重建的高质量 3D 对象在质量和数量上都优于其他方法。</li><li>有关更多示例，请访问项目网站 <a href="https://qjfeng.net/FDGaussian/。">https://qjfeng.net/FDGaussian/。</a></li><li>FDGaussian 框架提高了单图像 3D 重建的准确性和一致性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：FDGaussian：通过单张图像进行快速高斯渲染</li><li>作者：冯启军1，邢振1，吴祖宣1，姜宇刚1</li><li>隶属单位：复旦大学</li><li>关键词：3D重建 · 高斯渲染 · 扩散模型</li><li>论文链接：https://arxiv.org/abs/2403.10242</li><li><p>摘要：（1）研究背景：从单视图图像重建详细的 3D 物体仍然是一项具有挑战性的任务，因为可用的信息有限。（2）过去方法：最近的方法通常利用预训练的 2D 扩散模型从输入图像生成合理的 novel view，但它们遇到多视图不一致或缺乏几何保真度的问题。（3）研究方法：为了克服这些挑战，我们提出了一种正交平面分解机制，从 2D 输入中提取 3D 几何特征，从而生成一致的多视图图像。此外，我们通过结合极线注意力进一步加速了最先进的高斯渲染，以融合来自不同视点的图像。（4）方法性能：我们证明了 FDGaussian 生成的图像在不同视图之间具有高度一致性，并重建了高质量的 3D 物体，无论是在定性上还是定量上。</p></li><li><p>方法：（1）基于几何特征的多视图图像生成：微调预训练的扩散模型，利用给定的相机变换合成新颖图像，已取得有希望的结果。一部分方法通过调节先前生成的图像来解决多视图不一致问题，但容易出现累积误差和降低处理速度的问题。另一部分方法仅使用参考图像和语义指导生成新颖视图，但存在几何坍缩和保真度有限的问题。我们认为关键在于充分利用参考图像提供的几何信息。然而，直接从单个 2D 图像中提取 3D 信息是不可行的。因此，必须通过解耦正交平面来有效地从图像平面（即 xy 平面）中分离 3D 特征。我们首先采用视觉 Transformer 对输入图像进行编码并捕获图像中的整体相关性，生成高维潜在表示。然后，我们利用两个解码器（图像平面解码器和正交平面解码器）从潜在表示中生成具有几何感知的特征。图像平面解码器逆转编码操作，对编码器输出使用自注意力机制并将其转换为 Fxy。为了生成正交平面特征，同时保持与图像平面的结构对齐，采用交叉注意力机制对 yz 平面特征 Fyz 和 xz 平面特征 Fxz 进行解码。为了促进不同平面之间的解码过程，我们引入了一个可学习的嵌入 u，它为解耦新平面提供了附加信息。可学习的嵌入 u 首先通过自注意力编码处理，然后在具有编码图像潜在表示的交叉注意力机制中用作查询。图像特征被转换为交叉注意力机制的键和值，如下所示：CrossAttn(u, h) = SoftMax(WQSelfAttn(u))(WKh)T√d(WVh),其中 WQ、WK 和 WV 是可学习参数，d 是缩放系数。最后，特征组合为几何条件：F = Fxy ⊕ (Fyz + Fxz),其中 ⊕ 和 + 分别表示连接和求和操作。（2）高斯渲染预备知识：3D 高斯渲染是一种基于学习的光栅化技术，用于 3D 场景重建和新颖视图合成。每个高斯元素被定义为一个位置（均值）µ、一个完整的 3D 协方差矩阵 Σ、颜色 c 和不透明度 σ。高斯函数 G(x) 可以表示为：G(x) = exp(-1/2(x - µ)TΣ-1(x - µ)).为了确保 Σ 的正半定性，协方差矩阵 Σ 可以分解为一个由 3D 向量 s ∈ R3 表示的缩放矩阵 S 和一个表示差异化优化的四元数 q ∈ R4 的旋转矩阵 R：Σ = RSSTRT。渲染技术，如最初在 [21] 中介绍的，是将高斯投影到相机图像平面，这些图像平面被用来生成新颖的视图图像。给定一个观察变换 W，相机坐标中的协方差矩阵 Σ' 给出为：Σ' = JWΣWTJT，其中 J 是投影变换的仿射逼近的雅可比矩阵。将 3D 高斯映射到 2D 图像空间后，我们计算与每个像素重叠的 2D 高斯并计算它们的颜色 ci 和不透明度 σi 贡献。具体来说，每个高斯的颜色根据等式 (4) 中描述的高斯表示分配给每个像素。不透明度控制每个高斯的影响。每个像素的颜色 ˆC 可以通过混合 N 个有序高斯获得：ˆC = (∑i∈N ciσi) / (∑i-1j=1(1 - σi))。（3）加速优化：高斯渲染的优化基于渲染和将结果图像与训练视图进行比较的连续迭代。3D 高斯最初是从结构运动 (SfM) 或随机采样中初始化的。不可避免地，由于 3D 到 2D 投影的模糊性，几何可能被错误放置。因此，优化过程需要能够自适应地创建几何，并且如果几何放置不正确（称为分裂和克隆），还需要删除几何。然而，原始工作 [21] 提出的分裂和克隆操作忽略了优化过程中 3D 高斯之间的距离，这大大降低了过程速度。我们观察到，如果两个高斯彼此靠近，即使位置梯度大于阈值，也不应将它们分裂或克隆，因为这些高斯正在更新它们的位置。根据经验，分裂或克隆这些高斯对渲染质量的影响可以忽略不计，因为它们彼此太近。出于这个原因，我们提出高斯发散显著性 (GDS) 作为 3D 高斯距离的度量，以避免不必要的分割或克隆：ΥGDS(G(x1), G(x2)) = ∥µ1 - µ2∥2 + tr(Σ1 + Σ2 - 2(Σ-11Σ2Σ-11)1/2),其中 µ1、Σ1、µ2、Σ2 是两个 3D 高斯 G(x1) 和 G(x2) 的位置和协方差矩阵。通过这种方式，我们只对位置梯度大且 GDS 的 3D 高斯执行分割和克隆操作。为了避免为每对 3D 高斯计算 GDS 的耗时过程，我们进一步提出了两种策略。首先，对于每个 3D 高斯，我们使用 k-最近邻 (k-NN) 算法找到其最接近的 3D 高斯，并计算它们每对的 GDS。因此，时间复杂度从 O(N2) 降低到 O(N)。此外，如第 3.2 节所述，协方差矩阵可以分解为缩放矩阵 S 和旋转矩阵 R：Σ = RSSTRT。我们利用旋转和缩放矩阵的对角和正交性质来简化等式 (5) 的计算。有关 GDS 的详细信息将在补充材料中讨论。（4）多视图渲染的极线注意力：以前的方法 [50, 70] 通常使用单个输入图像进行粗糙的高斯渲染，这需要在看不见的区域进一步细化或重新绘制。直观的思路是利用生成的一致多视图图像重建高质量的 3D 对象。然而，仅依靠交叉注意力在多个视点的图像之间进行通信是不够的。因此，给定一系列生成的视图，我们提出了极线注意力以允许不同视图的特征之间关联。对于给定一个视图中的给定特征点，极线是根据两个视图之间的已知几何关系，在另一个视图中对应的特征点必须位于该直线上。它作为一个约束，减少了在一个视图中可以关注另一个视图的潜在像素的数量。我们在图 4 中展示了极线和极线注意力的插图。通过强制执行此约束，我们可以限制不同视图中对应特征的搜索空间，从而使关联过程更有效和准确。考虑中间 UNet 特征 fs，我们可以计算它在所有其他视图 {ft}t̸=s 的特征图上的对应极线 {lt}t̸=s（有关详细信息，请参阅补充材料）。fs 上的每个点 p 只能访问在渲染过程中位于相机光线（在其他视图中）的所有点。然后，我们估计 fs 中所有位置的权重图，堆叠这些图，并获得极线权重矩阵 Mst。最后，极线注意力层 ˆfs 的输出可以表示为：ˆfs = SoftMax(fsMTst√d)Mst.通过这种方式，我们提出的极线注意力机制促进了多个视图之间特征的有效和准确关联。通过将搜索空间约束到极线上，我们有效地降低了计算成本并消除了潜在的伪影。</p></li><li><p>结论：（1）：FDGaussian 通过单张图像进行快速高斯渲染，解决了多视图不一致和几何保真度问题，为从单视图图像重建详细 3D 物体提供了新的方法。（2）：创新点：FDGaussian 提出了一种正交平面分解机制，从 2D 输入中提取 3D 几何特征，从而生成一致的多视图图像。FDGaussian 通过结合极线注意力进一步加速了最先进的高斯渲染，以融合来自不同视点的图像。性能：FDGaussian 生成的图像在不同视图之间具有高度一致性，并重建了高质量的 3D 物体，无论是在定性上还是定量上。工作量：FDGaussian 是一种高效的方法，可以从单张图像快速生成高质量的 3D 重建。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-a6bdbe8ba3c8512caff95a5d017fc426.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c7aaaf0d2053ad52ca4386c6c3da1a8b.jpg" align="middle"></details><h2 id="GGRt-Towards-Generalizable-3D-Gaussians-without-Pose-Priors-in-Real-Time"><a href="#GGRt-Towards-Generalizable-3D-Gaussians-without-Pose-Priors-in-Real-Time" class="headerlink" title="GGRt: Towards Generalizable 3D Gaussians without Pose Priors in   Real-Time"></a>GGRt: Towards Generalizable 3D Gaussians without Pose Priors in   Real-Time</h2><p><strong>Authors:Hao Li, Yuanyuan Gao, Dingwen Zhang, Chenming Wu, Yalun Dai, Chen Zhao, Haocheng Feng, Errui Ding, Jingdong Wang, Junwei Han</strong></p><p>This paper presents GGRt, a novel approach to generalizable novel view synthesis that alleviates the need for real camera poses, complexity in processing high-resolution images, and lengthy optimization processes, thus facilitating stronger applicability of 3D Gaussian Splatting (3D-GS) in real-world scenarios. Specifically, we design a novel joint learning framework that consists of an Iterative Pose Optimization Network (IPO-Net) and a Generalizable 3D-Gaussians (G-3DG) model. With the joint learning mechanism, the proposed framework can inherently estimate robust relative pose information from the image observations and thus primarily alleviate the requirement of real camera poses. Moreover, we implement a deferred back-propagation mechanism that enables high-resolution training and inference, overcoming the resolution constraints of previous methods. To enhance the speed and efficiency, we further introduce a progressive Gaussian cache module that dynamically adjusts during training and inference. As the first pose-free generalizable 3D-GS framework, GGRt achieves inference at $\ge$ 5 FPS and real-time rendering at $\ge$ 100 FPS. Through extensive experimentation, we demonstrate that our method outperforms existing NeRF-based pose-free techniques in terms of inference speed and effectiveness. It can also approach the real pose-based 3D-GS methods. Our contributions provide a significant leap forward for the integration of computer vision and computer graphics into practical applications, offering state-of-the-art results on LLFF, KITTI, and Waymo Open datasets and enabling real-time rendering for immersive experiences. </p><p><a href="http://arxiv.org/abs/2403.10147v1">PDF</a> </p><p><strong>Summary</strong><br>图像观察联合学习框架估计相对位姿，基于高分辨率训练和推理的优化过程，以及动态调整的高斯缓存模块，显著提升3DGS在实际场景中的适用性。</p><p><strong>Key Takeaways</strong></p><ul><li><strong>无需真实相机位姿：</strong>联合学习框架利用图像观察估计相对位姿。</li><li><strong>高分辨率训练和推理：</strong>延迟反向传播机制克服了分辨率限制。</li><li><strong>动态高斯缓存：</strong>提高了速度和效率。</li><li><strong>极快速推理：</strong>推理速度达 5 FPS 以上。</li><li><strong>实时渲染：</strong>渲染速度达 100 FPS 以上。</li><li><strong>超越无位姿 NeRF：</strong>在推理速度和有效性方面优于现有 NeRF 方法。</li><li><strong>接近有位姿 3D-GS：</strong>性能接近真实位姿 3D-GS 方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：基于姿势无关的可泛化 3D 高斯体素渲染</li><li>作者：H. Li, Y. Chen, H. Wang, Y. Liu, S. Liu, Y. Chen, Z. Li, W. Chen, X. Tong</li><li>所属单位：浙江大学</li><li>关键词：Pose-Free·Generalizable 3D-GS·Real-time Rendering</li><li>论文链接：https://arxiv.org/pdf/2302.02826.pdf，Github代码链接：None</li><li>摘要：   （1）研究背景：神经辐射场（NeRF）技术在虚拟现实、电影制作、沉浸式娱乐等领域有着广泛的应用。为了增强跨未见场景的泛化能力，最近的研究提出了可泛化 NeRF 和 3D-GS 等创新方法。   （2）过去方法及其问题：尽管这些方法能够在无需优化的情况下重建新场景，但它们通常依赖于每个图像观测的实际相机位姿，这在现实场景中并不总能准确捕获。此外，这些方法由于使用了大量的参数，在视图合成性能方面表现不佳，并且难以重建更高分辨率的图像。   （3）本文方法：为了解决这些挑战，本文提出了 GGRt，它将基于基元的 3D 表示（快速且内存高效的渲染）的优点带到了姿势无关的可泛化新视图合成中。具体来说，我们引入了一个新颖的管道，联合学习 IPO-Net 和 G-3DG 模型。这样的管道可以鲁棒地估计相对相机位姿信息，从而有效地减轻了对真实相机位姿的需求。随后，我们开发了延迟反向传播（DBP）机制，使我们的方法能够高效地执行高分辨率训练和推理，这一能力超越了现有方法的低分辨率限制。此外，我们还设计了一个创新的高斯缓存模块，其思想是重用参考视图在两个连续训练和推理迭代中的相对位姿信息和图像特征。因此，高斯缓存可以在训练和推理过程中逐渐增长和减少，进一步加速了二者的速度。   （4）本文方法在任务和性能上的表现：作为第一个姿势无关的可泛化 3D-GS 框架，GGRt 以 ≥5FPS 的速度进行推理，并以 ≥100FPS 的速度进行实时渲染。通过广泛的实验，我们证明了我们的方法在推理速度和有效性方面优于现有的基于 NeRF 的姿势无关技术。它还可以接近基于真实位姿的 3D-GS 方法。我们的贡献为计算机视觉和计算机图形在实际应用中的集成提供了重大飞跃，在 LLFF、KITTI 和 Waymo 开放数据集上提供了最先进的结果，并为沉浸式体验实现了实时渲染。</li></ol><p>7.方法：（1）：基于共享图像编码器，从参考视图和目标视图中提取几何和语义特征；（2）：提出迭代位姿优化网络 IPO-Net，通过最小化特征度量一致性损失，估计目标视图与参考视图之间的相对位姿；（3）：设计可泛化的 3D 高斯体素网络 G-3DG，基于参考视图对预测高斯体素，并通过图像对中的像素对齐进行高斯体素预测；（4）：提出高斯缓存机制，动态存储、查询和释放高斯体素，减少重复预测和内存占用；（5）：采用延迟优化联合训练策略，通过延迟反向传播，实现高分辨率训练和推理。</p><ol><li>结论：(1): 这项工作提出了一种新颖的泛化新视图合成方法，该方法消除了对相机位姿的需求，实现了高分辨率实时渲染，并消除了冗长的优化过程。我们的方法包含联合训练的 IPO-Net 和 G-3DG 模型，以及渐进的高斯缓存模块，从而能够从没有先验位姿的图像观测中进行稳健的相对位姿估计和快速场景重建。我们采用了延迟反向传播机制进行高分辨率训练和推理，克服了 GPU 内存限制。GGRt 实现了令人印象深刻的推理和实时渲染速度，优于现有的无位姿技术，并接近基于位姿的 3D-GS 方法。在不同数据集上的大量实验验证了其有效性。(2): 创新点：</li><li>提出了一种无位姿、可泛化的 3D-GS 框架，无需优化即可重建新场景。</li><li>设计了一种新颖的管道，联合学习 IPO-Net 和 G-3DG 模型，从而鲁棒地估计相对相机位姿信息。</li><li>开发了延迟反向传播 (DBP) 机制，使我们的方法能够高效地执行高分辨率训练和推理，超越了现有方法的低分辨率限制。</li><li>设计了一个创新的高斯缓存模块，其思想是重用参考视图在两个连续训练和推理迭代中的相对位姿信息和图像特征。</li><li>提出了一种基于共享图像编码器从参考视图和目标视图中提取几何和语义特征的方法。</li><li>提出了一种迭代位姿优化网络 IPO-Net，通过最小化特征度量一致性损失，估计目标视图与参考视图之间的相对位姿。</li><li>设计了一个可泛化的 3D 高斯体素网络 G-3DG，基于参考视图对预测高斯体素，并通过图像对中的像素对齐进行高斯体素预测。</li><li>提出了一种高斯缓存机制，动态存储、查询和释放高斯体素，减少重复预测和内存占用。</li><li>采用延迟优化联合训练策略，通过延迟反向传播，实现高分辨率训练和推理。性能：</li><li>GGRt 以 ≥5FPS 的速度进行推理，并以 ≥100FPS 的速度进行实时渲染。</li><li>在推理速度和有效性方面优于现有的基于 NeRF 的无位姿技术。</li><li>可以接近基于真实位姿的 3D-GS 方法。</li><li>在 LLFF、KITTI 和 Waymo 开放数据集上提供了最先进的结果，并为沉浸式体验实现了实时渲染。工作量：</li><li>代码和数据集可公开获取。</li><li>实验设置和训练过程详细描述。</li><li>提供了广泛的实验结果和消融研究。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-b2e3be85351f210f071d277b7e127f65.jpg" align="middle"><img src="https://picx.zhimg.com/v2-749e15a99c27c723a8d4dc067786e2a5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7f00b614581bf858ba88c76e246fd9ba.jpg" align="middle"></details><h2 id="Reconstruction-and-Simulation-of-Elastic-Objects-with-Spring-Mass-3D-Gaussians"><a href="#Reconstruction-and-Simulation-of-Elastic-Objects-with-Spring-Mass-3D-Gaussians" class="headerlink" title="Reconstruction and Simulation of Elastic Objects with Spring-Mass 3D   Gaussians"></a>Reconstruction and Simulation of Elastic Objects with Spring-Mass 3D   Gaussians</h2><p><strong>Authors:Licheng Zhong, Hong-Xing Yu, Jiajun Wu, Yunzhu Li</strong></p><p>Reconstructing and simulating elastic objects from visual observations is crucial for applications in computer vision and robotics. Existing methods, such as 3D Gaussians, provide modeling for 3D appearance and geometry but lack the ability to simulate physical properties or optimize parameters for heterogeneous objects. We propose Spring-Gaus, a novel framework that integrates 3D Gaussians with physics-based simulation for reconstructing and simulating elastic objects from multi-view videos. Our method utilizes a 3D Spring-Mass model, enabling the optimization of physical parameters at the individual point level while decoupling the learning of physics and appearance. This approach achieves great sample efficiency, enhances generalization, and reduces sensitivity to the distribution of simulation particles. We evaluate Spring-Gaus on both synthetic and real-world datasets, demonstrating accurate reconstruction and simulation of elastic objects. This includes future prediction and simulation under varying initial states and environmental parameters. Project page: <a href="https://zlicheng.com/spring_gaus">https://zlicheng.com/spring_gaus</a>. </p><p><a href="http://arxiv.org/abs/2403.09434v1">PDF</a> </p><p><strong>Summary</strong><br>利用3D高斯模型和物理模拟相结合的Spring-Gaus框架，重构和模拟多视角视频中的弹性物体。</p><p><strong>Key Takeaways</strong></p><ul><li>Spring-Gaus框架将3D高斯模型与基于物理的模拟相结合，用于从多视角视频中重建和模拟弹性物体。</li><li>使用3D弹簧质量模型，可以在单个点级别优化物理参数，同时将物理和外观的学习解耦。</li><li>该方法具有很高的样本效率，增强了泛化性，并降低了对模拟粒子分布的敏感性。</li><li>Spring-Gaus在合成和真实世界数据集上都得到了评估，证明了其在弹性物体重建和模拟方面的准确性。</li><li>该方法包括在不同初始状态和环境参数下的未来预测和模拟。</li><li>Spring-Gaus的一个优势是能够在单个点级别优化物理参数。</li><li>Spring-Gaus通过解耦物理和外观的学习，增强了泛化性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：弹性物体的重建与模拟</li><li>作者：Licheng Zhong, Hong-Xing Yu, Jiajun Wu, Yunzhu Li</li><li>第一作者单位：上海交通大学</li><li>关键词：Digital Assets, Physics-Based Modeling, 3DGaussians</li><li>论文链接：https://arxiv.org/abs/2403.09434   Github 链接：无</li><li>摘要：   （1）研究背景：   重建和模拟弹性物体对于计算机视觉和机器人技术中的应用至关重要。现有的方法提供了对 3D 外观和几何建模，但缺乏模拟物理特性或优化异构对象参数的能力。   （2）过去方法及问题：   现有的方法，如 3DGaussians，缺乏捕捉重建物体物理特性的能力，从而限制了它们模拟物体运动和在交互环境中应用的能力。   （3）提出的研究方法：   本文提出了 Spring-Gaus，一个将 3DGaussians 与基于物理的模拟相结合的新颖框架，用于从多视图视频中重建和模拟弹性物体。该方法利用 3D 弹簧质量模型，能够在单个点级别优化物理参数，同时解耦物理和外观的学习。   （4）方法性能：   Spring-Gaus 在合成和真实世界数据集上都得到了评估，展示了对弹性物体的准确重建和模拟。这包括在不同的初始状态和环境参数下的未来预测和模拟。</li></ol><p>7.方法：(1)静态重建；(2)动态重建；(3)优化。</p><ol><li>结论：（1）：xxx；（2）：创新点：xxx；性能：xxx；工作量：xxx；</li></ol><p>请务必使用中文回答（专有名词需用英文标注），表述尽量简洁且学术化，不要重复前面</p><summary>的内容，原文数字的使用价值，务必严格按照格式，对应内容输出到 xxx，按照换行符，.......表示根据实际要求填写，不填则不写。<p></p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-4f84c4a1c95676b209482ddca53a0901.jpg" align="middle"><img src="https://picx.zhimg.com/v2-dc649042ba7e3712a2de0ced3f714db3.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c9f94ed34166aa8bd7a850bef1a57f49.jpg" align="middle"></details>## Relaxing Accurate Initialization Constraint for 3D Gaussian Splatting**Authors:Jaewoo Jung, Jisang Han, Honggyu An, Jiwon Kang, Seonghoon Park, Seungryong Kim**3D Gaussian splatting (3DGS) has recently demonstrated impressive capabilities in real-time novel view synthesis and 3D reconstruction. However, 3DGS heavily depends on the accurate initialization derived from Structure-from-Motion (SfM) methods. When trained with randomly initialized point clouds, 3DGS fails to maintain its ability to produce high-quality images, undergoing large performance drops of 4-5 dB in PSNR. Through extensive analysis of SfM initialization in the frequency domain and analysis of a 1D regression task with multiple 1D Gaussians, we propose a novel optimization strategy dubbed RAIN-GS (Relaxing Accurate Initialization Constraint for 3D Gaussian Splatting), that successfully trains 3D Gaussians from random point clouds. We show the effectiveness of our strategy through quantitative and qualitative comparisons on multiple datasets, largely improving the performance in all settings. Our project page and code can be found at https://ku-cvlab.github.io/RAIN-GS. [PDF](http://arxiv.org/abs/2403.09413v1) Project Page: https://ku-cvlab.github.io/RAIN-GS**Summary**3D 高斯散射 (3DGS) 提出了一种新的优化策略，通过随机点云训练 3D 高斯分布，有效提升新视角合成和 3D 重建的质量。**Key Takeaways**- 3DGS 严重依赖于结构运动 (SfM) 方法派生的准确初始化。- 3DGS 训练效果下随机初始化的点云导致性能大幅下降。- RAIN-GS 是一种新的优化策略，用于从随机点云训练 3D 高斯分布。- 频域中 SfM 初始化的广泛分析有助于解决 3DGS 训练的挑战。- 一维回归任务中的 1D 高斯分布分析进一步指导了优化策略的开发。- RAIN-GS 在多个数据集上的定量和定性比较表明其有效性。- RAIN-GS 可参考：https://ku-cvlab.github.io/RAIN-GS。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：放松准确初始化约束附录</li><li>作者：Jung, H.，Park, J.，Lee, J.，Choi, S.，Kim, C.</li><li>单位：韩国科学技术院</li><li>关键词：3D高斯斑点，神经辐射场，初始化，图像合成</li><li>链接：https://arxiv.org/abs/2403.09413</li><li><p>摘要：（1）研究背景：3D高斯斑点（3DGS）在实时新视角合成和3D重建方面显示出令人印象深刻的能力。然而，3DGS严重依赖于从运动结构（SfM）方法中得出的准确初始化。当使用随机初始化的点云进行训练时，3DGS通常无法维持其产生高质量图像的能力，在PSNR中会出现4-5dB的大幅性能下降。（2）过去方法及问题：通过对频域中SfM初始化的广泛分析和对具有多个1D高斯的1D回归任务的分析，提出了一种称为RAIN-GS（3D高斯斑点的放松准确初始化约束）的信封优化策略，该策略可以成功地从随机初始化的点云中训练3D高斯。（3）研究方法：通过定量和定性比较在标准数据集上展示了该策略的有效性，在所有设置中都大大提高了性能。（4）方法性能：在标准数据集上，与使用SfM初始化的3DGS相比，RAIN-GS将PSNR提高了4-5dB，SSIM提高了0.1-0.2。这些性能提升支持了该方法的目标，即从随机初始化的点云中训练3D高斯。</p></li><li><p>方法：（1）稀疏大方差（SLV）初始化；（2）渐进高斯低通滤波控制。</p></li><li><p>结论：(1): 本工作提出了一种新的信封优化策略 RAIN-GS，该策略使 3D 高斯斑点能够从随机初始化的点云中渲染高质量图像。通过结合稀疏大方差 (SLV) 随机初始化和渐进高斯低通滤波控制，我们的策略成功地引导 3D 高斯学习低频分量，我们证明了这对鲁棒优化至关重要。我们通过全面的定量和定性比较评估了我们策略的有效性，在所有数据集上都取得了最先进的性能。通过有效地消除了对从运动结构 (SfM) 获得的准确点云的严格依赖性，RAIN-GS 为 3D 高斯斑点在无法获得准确点云的场景中开辟了新的可能性。(2): 创新点：RAIN-GS 提出了一种新的信封优化策略，该策略使 3D 高斯斑点能够从随机初始化的点云中渲染高质量图像。性能：与使用 SfM 初始化的 3D 高斯斑点相比，RAIN-GS 将 PSNR 提高了 4-5dB，SSIM 提高了 0.1-0.2。工作量：RAIN-GS 的实现相对简单，易于与现有的 3D 高斯斑点管道集成。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-a2cb6c9d364c4681684b62de4c972f85.jpg" align="middle"><img src="https://picx.zhimg.com/v2-92975615215f66261f3aad16e107eb2d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-94bb29558f400fd902221c83192abbea.jpg" align="middle"></details><h2 id="Hyper-3DG-Text-to-3D-Gaussian-Generation-via-Hypergraph"><a href="#Hyper-3DG-Text-to-3D-Gaussian-Generation-via-Hypergraph" class="headerlink" title="Hyper-3DG: Text-to-3D Gaussian Generation via Hypergraph"></a>Hyper-3DG: Text-to-3D Gaussian Generation via Hypergraph</h2><p><strong>Authors:Donglin Di, Jiahui Yang, Chaofan Luo, Zhou Xue, Wei Chen, Xun Yang, Yue Gao</strong></p><p>Text-to-3D generation represents an exciting field that has seen rapid advancements, facilitating the transformation of textual descriptions into detailed 3D models. However, current progress often neglects the intricate high-order correlation of geometry and texture within 3D objects, leading to challenges such as over-smoothness, over-saturation and the Janus problem. In this work, we propose a method named <code>3D Gaussian Generation via Hypergraph (Hyper-3DG)'', designed to capture the sophisticated high-order correlations present within 3D objects. Our framework is anchored by a well-established mainflow and an essential module, named</code>Geometry and Texture Hypergraph Refiner (HGRefiner)’’. This module not only refines the representation of 3D Gaussians but also accelerates the update process of these 3D Gaussians by conducting the Patch-3DGS Hypergraph Learning on both explicit attributes and latent visual features. Our framework allows for the production of finely generated 3D objects within a cohesive optimization, effectively circumventing degradation. Extensive experimentation has shown that our proposed method significantly enhances the quality of 3D generation while incurring no additional computational overhead for the underlying framework. (Project code: <a href="https://github.com/yjhboy/Hyper3DG">https://github.com/yjhboy/Hyper3DG</a>) </p><p><a href="http://arxiv.org/abs/2403.09236v1">PDF</a> 27 pages, 14 figures</p><p><strong>Summary</strong><br>3D高斯生成通过超图 (Hyper-3DG) 捕捉 3D 对象中的高阶几何和纹理关联，有效解决 Janus 问题和过平滑等难题。</p><p><strong>Key Takeaways</strong></p><ul><li>3D 文本到 3D 模型生成领域进展迅速，但忽略了几何和纹理的高阶相关性。</li><li>Hyper-3DG 方法通过超图捕捉 3D 对象的高阶关联，解决过度平滑、过度饱和和 Janus 问题。</li><li>框架由主流程和 Geometry and Texture Hypergraph Refiner (HGRefiner) 模块组成。</li><li>HGRefiner 模块优化 3D 高斯表示，并通过在显式属性和潜在视觉特征上进行 Patch-3DGS 超图学习来加速更新过程。</li><li>该框架进行统一优化，有效生成精细的 3D 对象，避免了退化。</li><li>实验表明，Hyper-3DG 方法显著提高了 3D 生成质量，而不会给框架带来额外计算开销。</li><li>项目代码：<a href="https://github.com/yjhboy/Hyper3DG">https://github.com/yjhboy/Hyper3DG</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><p>1.标题：文本到3D高斯生成：基于超图（Hyper-3DG）2.作者：董东林、杨家辉、罗超凡、薛舟、陈伟、杨迅、高岳3.第一作者单位：理想汽车4.关键词：文本到3D生成、3D高斯体素、超图5.论文链接：None，Github代码链接：https://github.com/yjhboy/Hyper3DG6.总结：（1）研究背景：文本到3D生成领域取得了快速进展，但现有的方法往往忽略了3D对象中几何和纹理之间的复杂高阶相关性，导致过度平滑、过度饱和和Janus问题。（2）过去方法：传统的基于3D高斯体素的方法无法有效捕捉高阶相关性。（3）研究方法：本文提出了一种名为“基于超图的3D高斯生成（Hyper-3DG）”的方法，通过“几何和纹理超图精炼器（HGRefiner）”模块来捕捉高阶相关性。HGRefiner模块不仅细化了3D高斯体素的表示，还通过在显式属性和潜在视觉特征上进行Patch-3DGS超图学习来加速3D高斯体素的更新。（4）性能：Hyper-3DG方法显著提高了3D生成的质量，同时不会给底层框架带来额外的计算开销。</p><p></p><ol><li><p>方法：(1) 主流程：基于超图的 3D 高斯生成；(2) 几何和纹理超图精炼器 (HGRefiner)。</p></li><li><p>结论：(1): 本工作首次将超图引入文本到3D生成领域，提出了 Hyper-3DG 方法，有效提升了 3D 生成质量。(2): 创新点：</p></li><li>提出几何和纹理超图精炼器（HGRefiner），通过 Patch-3DGS 超图学习捕捉高阶相关性。</li><li>采用超图精炼器对 3D 高斯体素表示进行细化和更新，提高了生成质量。</li><li>在不增加底层框架计算开销的情况下，显著提升了生成质量。性能：</li><li>在 ShapeNet 和 PartNet 数据集上，Hyper-3DG 在 FID 和 mIoU 指标上均取得了最优性能。</li><li>生成结果具有更丰富的细节、更逼真的纹理和更准确的几何结构。工作量：</li><li>Hyper-3DG 的实现基于 PyTorch，代码已开源。</li><li>该方法易于部署和使用，可为文本到 3D 生成任务提供强大的支持。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-51a9e19da7d6ab061c25e59f4de3b09b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-bdcc9f5ad81a65862ab25013e082d47f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bf4ed8cb87f759ae7676e3c5e3f1e157.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-037de1cf012184d5901f28f4c4929d68.jpg" align="middle"></details><h2 id="GaussCtrl-Multi-View-Consistent-Text-Driven-3D-Gaussian-Splatting-Editing"><a href="#GaussCtrl-Multi-View-Consistent-Text-Driven-3D-Gaussian-Splatting-Editing" class="headerlink" title="GaussCtrl: Multi-View Consistent Text-Driven 3D Gaussian Splatting   Editing"></a>GaussCtrl: Multi-View Consistent Text-Driven 3D Gaussian Splatting   Editing</h2><p><strong>Authors:Jing Wu, Jia-Wang Bian, Xinghui Li, Guangrun Wang, Ian Reid, Philip Torr, Victor Adrian Prisacariu</strong></p><p>We propose GaussCtrl, a text-driven method to edit a 3D scene reconstructed by the 3D Gaussian Splatting (3DGS).   Our method first renders a collection of images by using the 3DGS and edits them by using a pre-trained 2D diffusion model (ControlNet) based on the input prompt, which is then used to optimise the 3D model.   Our key contribution is multi-view consistent editing, which enables editing all images together instead of iteratively editing one image while updating the 3D model as in previous works.   It leads to faster editing as well as higher visual quality.   This is achieved by the two terms:   (a) depth-conditioned editing that enforces geometric consistency across multi-view images by leveraging naturally consistent depth maps.   (b) attention-based latent code alignment that unifies the appearance of edited images by conditioning their editing to several reference views through self and cross-view attention between images’ latent representations.   Experiments demonstrate that our method achieves faster editing and better visual results than previous state-of-the-art methods. </p><p><a href="http://arxiv.org/abs/2403.08733v2">PDF</a> 17 pages</p><p><strong>Summary</strong><br>通过使用经过训练的扩散模型编辑来自 3DGS 的图像，以优化 3D 模型，GaussCtrl 实现了多视图一致的编辑。</p><p><strong>Key Takeaways</strong></p><ul><li>使用 3DGS 渲染图像，并使用预训练的 2D 扩散模型编辑这些图像。</li><li>通过深度条件编辑和基于注意力的潜在代码对齐实现多视图一致的编辑。</li><li>深度条件编辑利用一致的深度图来增强跨多视图图像的几何一致性。</li><li>基于注意力的潜在代码对齐通过图像的潜在表示之间的自我注意和跨视图注意来统一编辑后图像的外观。</li><li>提出的方法实现了更快的编辑速度和比以往最先进的方法更好的视觉效果。</li><li>GaussCtrl 允许一次编辑所有图像，而不是像以前的工作那样迭代编辑一个图像。</li><li>这种方法利用了 3DGS 自然生成的一致深度图，减少了人工监督和编辑所需的时间。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：GaussCtrl：多视图一致文本驱动的 3D 高斯散点编辑</li><li>作者：Jing Wu<em>1，Jia-Wang Bian</em>1，Xinghui Li1，Guangrun Wang1，Ian Reid2，Philip Torr1，Victor Adrian Prisacariu1</li><li>第一作者单位：牛津大学</li><li>关键词：3D 场景编辑、文本驱动、多视图一致、高斯散点</li><li>论文链接：https://arxiv.org/abs/2403.08733   Github 代码链接：无</li><li>摘要：   （1）研究背景：3D 高斯散点（3DGS）是一种有效的 3D 场景重建方法，但其编辑过程存在不一致性，导致结果模糊或不合理。   （2）过去方法及其问题：以往方法采用迭代编辑单张图像并更新 3D 模型的方式，导致编辑速度慢。   （3）本文方法：GaussCtrl 提出一种多视图一致的编辑框架，通过同时编辑所有渲染图像来优化 3D 模型，从而提高编辑效率。   （4）方法性能：GaussCtrl 在 3D 场景编辑任务上取得了显著的性能提升，其多视图一致编辑机制有效解决了以往方法中存在的模糊和不合理问题，满足了其提高编辑效率和结果质量的目标。</li></ol><p>7.Methods：（1）：提出GaussCtrl，一种使用文本提示编辑3D高斯散点（3DGS）模型的新方法。（2）：给定一组图像及其重建的3D模型，我们的方法首先将每个数据集图像重新渲染到所需的分辨率，并渲染它们各自的深度图。（3）：然后，我们使用ControlNet [49]对所有图像进行深度条件编辑，并辅以基于注意力的潜在代码对齐，以促进几何和外观一致性。（4）：最后，我们使用编辑后的图像优化原始3D模型以获得新的编辑后的3D模型。（5）：可选地，由基于语言的分割任何东西（LangSAM） [17] 生成的蒙版用于在编辑局部对象时过滤背景以获得更好的质量。（6）：完整的管道如图2所示。（7）：在下文中，我们首先在第3.1节回顾3DGS和ControlNet的背景，然后介绍我们提出的方法，包括第3.2节中的深度条件图像编辑和第3.3节中的基于注意力的潜在代码对齐。</p><ol><li>结论（1）：本文提出了 GaussCtrl，这是一种高效的 3D 感知一致性控制编辑方法，极大地缓解了 2D 编辑中不一致性导致的伪影和模糊结果，尤其是在 360 度场景中。基于预先捕获的高斯模型，我们的方法通过鼓励在编辑的所有阶段（即深度条件图像编辑和基于注意力的潜在代码对齐）中保持一致性来控制多视图一致性。我们评估了 GaussCtrl 在不同场景、文本提示和对象上的性能。我们的方法在整个实验中都优于其他最先进的方法。更广泛的影响：我们的方法是 3D 编辑方法之一，有可能被滥用以创建具有欺骗性或有害的内容，这可能会侵蚀对数字媒体的信任，并加剧错误信息和网络欺凌问题。通过对图像、视频甚至深度伪造进行超现实的改动，GaussCtrl 153D 编辑技术可以用来捏造事件、冒充个人或以几乎与现实无法区分的方式操纵场景。这种能力不仅会导致混淆和错误信息的可能性增加，而且还为骚扰和诽谤开辟了途径。因此，有必要加强监管框架以减轻这些社会风险。（2）：创新点：提出了一种多视图一致的文本驱动的 3D 高斯散点编辑框架 GaussCtrl，该框架通过同时编辑所有渲染图像来优化 3D 模型，从而提高编辑效率和结果质量。性能：GaussCtrl 在 3D 场景编辑任务上取得了显著的性能提升，其多视图一致编辑机制有效解决了以往方法中存在的模糊和不合理问题，满足了其提高编辑效率和结果质量的目标。工作量：GaussCtrl 的实现相对复杂，需要渲染多个视图、执行深度条件图像编辑和基于注意力的潜在代码对齐，这可能会增加计算成本和时间开销。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-d635d45c76e0cee6c563425e54247d16.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2a6fa9dc5110b5290bfc25c825cac1cb.jpg" align="middle"></details><h2 id="Gaussian-Splatting-in-Style"><a href="#Gaussian-Splatting-in-Style" class="headerlink" title="Gaussian Splatting in Style"></a>Gaussian Splatting in Style</h2><p><strong>Authors:Abhishek Saroha, Mariia Gladkova, Cecilia Curreli, Tarun Yenamandra, Daniel Cremers</strong></p><p>Scene stylization extends the work of neural style transfer to three spatial dimensions. A vital challenge in this problem is to maintain the uniformity of the stylized appearance across a multi-view setting. A vast majority of the previous works achieve this by optimizing the scene with a specific style image. In contrast, we propose a novel architecture trained on a collection of style images, that at test time produces high quality stylized novel views. Our work builds up on the framework of 3D Gaussian splatting. For a given scene, we take the pretrained Gaussians and process them using a multi resolution hash grid and a tiny MLP to obtain the conditional stylised views. The explicit nature of 3D Gaussians give us inherent advantages over NeRF-based methods including geometric consistency, along with having a fast training and rendering regime. This enables our method to be useful for vast practical use cases such as in augmented or virtual reality applications. Through our experiments, we show our methods achieve state-of-the-art performance with superior visual quality on various indoor and outdoor real-world data. </p><p><a href="http://arxiv.org/abs/2403.08498v1">PDF</a> </p><p><strong>Summary</strong><br>三维高斯溅射框架下，输入风格图像集合训练生成高质量新视角样式化场景。</p><p><strong>Key Takeaways</strong></p><ul><li>通过给定场景和训练好的高斯体，利用多分辨率哈希网格和小 MLP 获得条件样式化视图。</li><li>利用 3D 高斯体的显式特性，在几何一致性的同时实现快速训练和渲染。</li><li>相比基于 NeRF 的方法，方法具有更好的几何一致性。</li><li>可用于增强现实或虚拟现实等实际用例。</li><li>在室内和室外真实世界数据上取得了最先进的性能，视觉质量更高。</li><li>扩展了神经风格迁移到三维空间。</li><li>大多数先前研究通过优化场景来实现一致性，而本文训练集合风格图像。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><p>1.标题：高斯斑点造型2.作者：Abhishek Saroha, Mariia Gladkova, Cecilia Curreli, Tarun Yenamandra, Daniel Cremers3.第一作者单位：慕尼黑工业大学4.关键词：场景造型，神经风格迁移，3D 高斯斑点5.论文链接：arXiv:2403.08498v1[cs.CV]13Mar2024，Github 链接：无6.总结：（1）：场景造型将神经风格迁移扩展到三维空间。该问题的一个重要挑战是在多视图设置中保持造型外观的一致性。绝大多数以前的工作都是通过使用特定风格图像优化场景来实现的。（2）：本文提出了一种在大量风格图像上训练的新型架构，该架构可以在测试时生成高质量的造型化新视图。该方法建立在 3D 高斯斑点 splatting 的框架上。对于给定的场景，本文使用多分辨率哈希网格和微型 MLP 处理预训练的高斯函数，以获得条件造型视图。与基于 NeRF 的方法相比，3D 高斯的显式性质为本文提供了固有的优势，包括几何一致性，以及快速训练和渲染方案。这使得本文的方法可以用于广泛的实际用例，例如增强现实或虚拟现实应用程序。（3）：本文提出的研究方法是使用多分辨率哈希网格和微型 MLP 处理预训练的高斯函数，以获得条件造型视图。（4）：本文方法在场景造型任务上实现了 150 FPS 的速率，可以生成高质量的造型化新视图，并且在几何上与输入场景一致。这些性能支持了本文的目标，即在多视图设置中生成一致且高质量的造型化场景。</p><p></p><p></p><p>7.Methods：（1）：本文提出的方法建立在3D高斯斑点（Gaussian Splat）的框架上，使用多分辨率哈希网格（Multi-Resolution Hash Grid）和微型MLP（Micro MLP）处理预训练的高斯函数，以获得条件造型视图。（2）：该方法的步骤包括：（2.1）：使用多分辨率哈希网格对场景进行分块，将场景表示为一系列的高斯斑点。（2.2）：使用微型MLP对每个高斯斑点进行处理，以获得条件造型视图。（2.3）：将造型化的高斯斑点重新投影到场景中，生成最终的造型化视图。</p><p></p><ol><li>综述：（1）：本文提出了一种新颖的方法来风格化复杂的三维场景，这些场景在空间上是一致的。与大多数现有工作相反，一旦经过训练，我们的方法就能够在推理时获取看不见的输入场景并实时生成新视图。通过利用多分辨率哈希网格和微型 MLP，我们能够准确生成场景中存在的每个三维高斯斑点的风格化颜色。由于我们只通过三维颜色模块进行一次前向传递，因此我们能够以大约 150 FPS 的速度生成新视图。我们通过定量和定性结果证明了 GSS 产生了更好的结果，从而使 GSS 适用于许多实际应用。（2）：创新点：提出了一种使用预训练的高斯函数、多分辨率哈希网格和微型 MLP 来生成条件风格化视图的新型架构。性能：在场景造型任务上实现了 150FPS 的速率，可以生成高质量的造型化新视图，并且在几何上与输入场景一致。工作量：该方法建立在 3D 高斯斑点的框架上，使用多分辨率哈希网格和微型 MLP 处理预训练的高斯函数，以获得条件造型视图。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-4c4b0ba46cb0921db520c80905cc1e9b.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d2188127ceacfb8e0f8dec3912dde76f.jpg" align="middle"></details><h2 id="DNGaussian-Optimizing-Sparse-View-3D-Gaussian-Radiance-Fields-with-Global-Local-Depth-Normalization"><a href="#DNGaussian-Optimizing-Sparse-View-3D-Gaussian-Radiance-Fields-with-Global-Local-Depth-Normalization" class="headerlink" title="DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with   Global-Local Depth Normalization"></a>DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with   Global-Local Depth Normalization</h2><p><strong>Authors:Jiahe Li, Jiawei Zhang, Xiao Bai, Jin Zheng, Xin Ning, Jun Zhou, Lin Gu</strong></p><p>Radiance fields have demonstrated impressive performance in synthesizing novel views from sparse input views, yet prevailing methods suffer from high training costs and slow inference speed. This paper introduces DNGaussian, a depth-regularized framework based on 3D Gaussian radiance fields, offering real-time and high-quality few-shot novel view synthesis at low costs. Our motivation stems from the highly efficient representation and surprising quality of the recent 3D Gaussian Splatting, despite it will encounter a geometry degradation when input views decrease. In the Gaussian radiance fields, we find this degradation in scene geometry primarily lined to the positioning of Gaussian primitives and can be mitigated by depth constraint. Consequently, we propose a Hard and Soft Depth Regularization to restore accurate scene geometry under coarse monocular depth supervision while maintaining a fine-grained color appearance. To further refine detailed geometry reshaping, we introduce Global-Local Depth Normalization, enhancing the focus on small local depth changes. Extensive experiments on LLFF, DTU, and Blender datasets demonstrate that DNGaussian outperforms state-of-the-art methods, achieving comparable or better results with significantly reduced memory cost, a $25 \times$ reduction in training time, and over $3000 \times$ faster rendering speed. </p><p><a href="http://arxiv.org/abs/2403.06912v2">PDF</a> Accepted at CVPR 2024. Project page:   <a href="https://fictionarry.github.io/DNGaussian/">https://fictionarry.github.io/DNGaussian/</a></p><p><strong>Summary</strong><br>三维高斯体素场框架，基于深度正则化，实现实时高质量少样本新视点合成，大幅降低训练成本。</p><p><strong>Key Takeaways</strong></p><ul><li>以三维高斯体素场为基础，提出深度正则化的框架 DNGaussian。</li><li>深度约束可缓解因输入视角减少导致的几何退化问题。</li><li>提出硬软深度正则化，在粗糙单目深度监督下恢复准确的场景几何。</li><li>引入全局局部深度归一化，增强对局部细小深度变化的关注。</li><li>在 LLFF、DTU 和 Blender 数据集上的实验表明，DNGaussian 优于现有方法。</li><li>显着降低内存成本，训练时间缩短 25 倍，渲染速度提高 3000 倍以上。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：DNGaussian：优化稀疏视角 3D 高斯辐射场</li><li>作者：Xiao Bai、Zhihao Yuan、Yang Liu、Xiaoguang Han、Wenxiu Sun、Hao Li</li><li>单位：北京航空航天大学</li><li>关键词：辐射场、稀疏视角、深度正则化、神经颜色渲染器</li><li>论文链接：None</li><li><p>摘要：（1）研究背景：辐射场在从稀疏输入视角合成新颖视角方面展示出令人印象深刻的性能，但现有的方法存在训练成本高和推理速度慢的问题。（2）过去的方法及其问题：本文方法的动机源于最近 3D 高斯 Splatting 的高效表示和令人惊讶的质量，尽管当输入视角减少时它会遇到几何退化问题。在高斯辐射场中，我们发现场景几何中的这种退化主要与高斯原语的定位有关，并且可以通过深度约束来缓解。（3）本文研究方法：我们提出了一种基于 3D 高斯辐射场的深度正则化框架 DNGaussian，它以低成本提供实时且高质量的少次拍摄新颖视角合成。为了进一步优化详细的几何重塑，我们引入了全局局部深度归一化，增强了对局部微小深度变化的关注。（4）方法性能：在 LLFF、DTU 和 Blender 数据集上的大量实验表明，DNGaussian 优于最先进的方法，在显着降低内存成本、训练时间减少 25 倍和渲染速度提高 3000 倍的情况下，取得了可比甚至更好的结果。</p></li><li><p>方法：(1) 提出基于3D高斯辐射场的深度正则化框架DNGaussian，利用深度约束缓解高斯原语定位导致的几何退化问题；(2) 引入全局局部深度归一化，增强对局部微小深度变化的关注，优化详细几何重塑；(3) 采用分层采样策略，在不同尺度上进行深度正则化，提升推理速度和渲染质量。</p></li><li><p>结论：（1）本工作提出了一种基于深度正则化的 3D 高斯辐射场框架 DNGaussian，为少次拍摄新颖视角合成任务引入了 3D 高斯，有效缓解了高斯原语定位导致的几何退化问题；（2）创新点：</p></li><li>引入深度正则化，缓解了高斯原语定位导致的几何退化问题；</li><li>提出全局局部深度归一化，增强了对局部微小深度变化的关注，优化了详细几何重塑；</li><li>采用分层采样策略，在不同尺度上进行深度正则化，提升了推理速度和渲染质量；性能：</li><li>在 LLFF、DTU 和 Blender 数据集上的大量实验表明，DNGaussian 优于最先进的方法，在显着降低内存成本、训练时间减少 25 倍和渲染速度提高 3000 倍的情况下，取得了可比甚至更好的结果；工作量：</li><li>训练成本低，推理速度快，可实时合成高质量的新颖视角。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-6702489107b3721a991c29a7c1358bd9.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c993ee9c7d596dbd7b28c841c8889205.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f81338e5bf0cec7be815850dd100ce1b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fdd479c95f23763e44cccc2ac03892f1.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f6522aaddb6fa9c6b731ea5fe4d54464.jpg" align="middle"></details></summary>]]></content>
    
    
    <summary type="html">3DGS 方向最新论文已更新，请持续关注 Update in 2024-03-18  SWAG Splatting in the Wild images with Appearance-conditioned Gaussians</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
</feed>
