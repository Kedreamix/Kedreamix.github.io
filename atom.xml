<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Adventures in Kedreamix&#39; Digital World</title>
  
  
  <link href="https://kedreamix.github.io/atom.xml" rel="self"/>
  
  <link href="https://kedreamix.github.io/"/>
  <updated>2024-03-20T04:48:20.439Z</updated>
  <id>https://kedreamix.github.io/</id>
  
  <author>
    <name>Kedreamix</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>SyncTalk实验笔记</title>
    <link href="https://kedreamix.github.io/Project/SyncTalk/"/>
    <id>https://kedreamix.github.io/Project/SyncTalk/</id>
    <published>2024-03-18T12:37:00.000Z</published>
    <updated>2024-03-20T04:48:20.439Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://picx.zhimg.com/v2-03605cd4fbd659c9d341840c64fd3b41.png" alt="synctalk"></p><h2 id="Face-Sync-Controller"><a href="#Face-Sync-Controller" class="headerlink" title="Face-Sync Controller"></a>Face-Sync Controller</h2><h3 id="Facial-Animation-Capturer"><a href="#Facial-Animation-Capturer" class="headerlink" title="Facial Animation Capturer"></a>Facial Animation Capturer</h3><p>Blendshape的提取可参考 </p><p><a href="https://colab.research.google.com/github/googlesamples/mediapipe/blob/main/examples/face_landmarker/python/%5BMediaPipe_Python_Tasks%5D_Face_Landmarker.ipynb">https://colab.research.google.com/github/googlesamples/mediapipe/blob/main/examples/face_landmarker/python/%5BMediaPipe_Python_Tasks%5D_Face_Landmarker.ipynb</a></p><p><img src="https://picx.zhimg.com/v2-2498ec39938d865073b5cbaae63fdef9.png" alt=""></p><p><img src="https://picx.zhimg.com/v2-8392dcadaf5221c5298ed49baeac28a9.png" alt=""></p><h2 id="Head-Sync-Stabilizer"><a href="#Head-Sync-Stabilizer" class="headerlink" title="Head-Sync Stabilizer"></a>Head-Sync Stabilizer</h2><p><strong>Head Motion Tracker</strong></p><p>头部姿势，表示为 p，是指人的头部在 3D 空间中的旋转角度，由旋转 R 和平移 T 定义。</p><p>不稳定的头部姿势会导致头部抖动，所以为了获得头部姿势的粗略估计。首先，通过在预定范围内迭代 i 次来确定最佳焦距，对于每个焦距候选 fi，重新初始化旋转和平移值，目标是最小化 3D 可变形模型 (3DMM) 的投影地标与视频帧中的实际地标之间的误差。</p><p><img src="https://picx.zhimg.com/v2-cd96b85183a33c3b785c76d15344f433.png" alt="image-20240318205920014"></p><p>其中 $E_i$ 表示的就是 MSE，这样能够以更好地将模型的投影 lmk 与实际视频 lmk 对齐，然后得到最优的旋转和平移矩阵，也是用 MSE 来最小化，这是对每一帧进行操作的，在对应视频帧的最优值。</p><p><img src="https://picx.zhimg.com/v2-279c71feaa74b2e765d97c881e4da608.png" alt="image-20240318211905521"></p><p>这一部分实际上和原来的代码差别不大，可以调整一下所有帧和对应的优化部分，比如600~1500的步长可以设置为50，原本是100，因为结果也发现是1350</p><p><img src="https://pic1.zhimg.com/v2-fe6fb504cb27b75a3ca8641c715629b5.png" alt=""></p><p><strong>Head Points Tracker</strong></p><p>对于之前基于 NeRF 的方法来说，先前的方法利用基于 3DMM 的技术来提取头部姿势并生成不准确的结果。为了提高 R 和 T 的精度，我们使用像 <a href="https://arxiv.org/html/2307.07635v2">Co- tracker</a> 这样的光流估计模型来跟踪面部关键点 K。</p><p><img src="https://pica.zhimg.com/v2-1a4d6600883ddfe2e4438913f829716a.png" alt=""></p><p>接下来，使用预训练的光流估计模型，在获取面部运动光流后，我们使用<strong>拉普拉斯滤波器</strong>选择位于最显著流变化位置的关键点，并在流序列中跟踪这些关键点的运动轨迹。通过这个模块确保了所有帧上的面部关键点对齐更加精确和一致，从而增强了头部姿势参数的准确性。</p><p><img src="https://pica.zhimg.com/v2-b089529e446c0280c4d3da5c08770f64.png" alt=""></p><p><strong>Bundle Adjustment</strong></p><p>根据关键点和粗略的头部姿势，引入了一个两阶段优化框架来提高关键点和头部姿势估计的准确性。</p><ul><li>第一阶段，随机初始化 j 个关键点的 3D 坐标并优化它们的位置，以便与图像平面上跟踪的关键点对齐。这一部分最小化损失函数 $L_{init}$，捕获<strong>投影关键点 P 和跟踪关键点 K</strong> 之间的差异：</li><li>第二阶段，开始进行更全面的优化，以细化 3D 关键点和相关的头部联合姿势参数，通过 Adam 优化器优化算法，<strong>调整空间坐标、旋转角度 R 和平移 T</strong> 以最小化对齐误差 $L_{sec}$，表示为：</li></ul><p>经过这些优化后，观察到所得的头部姿势和平移参数平滑且稳定。</p><blockquote><p>现在的面部跟踪技术（Face Tracking）通常结合了多种算法和技术，以实现对视频中人脸的实时和准确跟踪。以下是一些关键的技术和方法，它们被广泛应用于现代面部跟踪系统中：</p><ol><li><p><strong>合成分析法（Analysis-by-Synthesis）</strong>：<br>这种方法通过创建一个人脸模型，并将其拟合到视频中的每一帧，以实现跟踪。初始化阶段通常通过最小化人脸关键点的重投影误差来获得初始人脸参数。这种方法可以处理光照变化和遮挡问题，提高跟踪的鲁棒性和准确性。</p></li><li><p><strong>基于模型跟踪</strong>：<br>这种方法依赖于预先定义的人脸模型，通过调整模型参数来适应视频中的人脸。这包括使用形状模型（如Active Shape Models）和外观模型来捕捉人脸的几何和外观变化。</p></li><li><p><strong>基于运动信息跟踪</strong>：<br>利用视频中的运动信息来预测和跟踪人脸的移动。这种方法通常结合了光流算法或其他运动估计技术。</p></li><li><p><strong>基于人脸局部特征跟踪</strong>：<br>通过检测和跟踪人脸的局部特征（如眼睛、鼻子、嘴巴等）来实现跟踪。这些特征点可以提供关于人脸姿态和表情变化的详细信息。</p></li><li><p><strong>基于神经网络跟踪</strong>：<br>利用深度学习模型，尤其是卷积神经网络（CNN），来识别和跟踪人脸。这些模型可以学习从大量数据中提取复杂的面部特征，并在各种条件下保持高准确度。</p></li><li><p><strong>实时人脸跟踪算法</strong>：<br>为了在实时视频流中实现人脸跟踪，算法需要高效且能够快速处理连续帧。一些成熟的SDK，如OpenCV，提供了实时人脸检测和跟踪的功能。</p></li><li><p><strong>多人脸跟踪（Multi-face tracking）</strong>：<br>在多人场景中，跟踪技术需要能够同时识别和跟踪多个面部。这通常涉及到更复杂的算法，如FairMOT，它是一种单类多目标跟踪算法，可以根据需求修改为多类多目标跟踪。</p></li><li><p><strong>非刚性人脸跟踪</strong>：<br>考虑到人脸的非刚性特性，一些跟踪算法会使用非刚性模型来更好地适应面部表情和头部动作的变化。</p></li></ol><p>在实际应用中，面部跟踪系统可能会结合以上多种方法，以提高在不同环境和条件下的跟踪性能。例如，一个系统可能会首先使用基于模型的方法来初始化跟踪，然后切换到基于特征的方法来处理面部表情变化，同时利用神经网络来提高在复杂背景下的跟踪准确性。</p><p>AD-NeRF</p><p><img src="C:/Users/Kedreamix/AppData/Roaming/Typora/typora-user-images/image-20240320011902893.png" alt="image-20240320011902893"></p><p>（2）我们应用多帧光流估计方法[18]来获得前额、耳朵和头发等近刚性区域中视频帧的密集对应关系，然后使用束调整来估计姿势参数[2]。 值得注意的是，估计的姿势仅对面部部分有效，而对颈部和肩部等身体其他区域无效，即面部姿势不能代表上半身的全部运动；</p></blockquote><h2 id="Portrait-Sync-Generator"><a href="#Portrait-Sync-Generator" class="headerlink" title="Portrait-Sync Generator"></a>Portrait-Sync Generator</h2><p>代码改进一共只有几部分</p><p><img src="https://pic1.zhimg.com/v2-8241e1d748ca0b674e3913714b0e0386.png" alt=""></p><p>在数据读取的时候，加了face_mask的读取，以及bg_image的读取，也就是GT Image的读取，对于GT Image来说，是通过parsing去出对应部分来进行操作的，从下图也可以看出区别，也就是有无头发丝的细节部分</p><p><img src="https://pica.zhimg.com/v2-3866dff2d07194c235eefab923f694c5.png" alt=""></p><p>指标可能有两个GT，因为两种模式下，对应的计算指标是不同的</p><p><img src="https://picx.zhimg.com/v2-e5cec8d19e131745028e5a3fe71c3684.png" alt=""></p><p>问了一下作者，大概更明白了这个的意思，其实本质上是使用了原图的头发丝的细节加入到图像中，使得图像能够得到更好的结果，然后再进行结合得到更好的效果。</p><p><img src="https://picx.zhimg.com/v2-e59f49fdcbc728e0222376e2a987d73b.png" alt=""></p><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p><img src="C:/Users/Kedreamix/AppData/Roaming/Typora/typora-user-images/image-20240320124820368.png" alt="image-20240320124820368"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://picx.zhimg.com/v2-03605cd4fbd659c9d341840c64fd3b41.png&quot; alt=&quot;synctalk&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;Face-Sync-Controller&quot;&gt;&lt;a href=&quot;#Fac</summary>
      
    
    
    
    <category term="Project" scheme="https://kedreamix.github.io/categories/Project/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>REAL3D-PORTRAIT ONE-SHOT REALISTIC 3D TALKING PORTRAIT SYNTHESIS</title>
    <link href="https://kedreamix.github.io/Paperscape/Real3D-Portrait/"/>
    <id>https://kedreamix.github.io/Paperscape/Real3D-Portrait/</id>
    <published>2024-03-15T09:07:36.901Z</published>
    <updated>2024-03-18T12:25:24.362Z</updated>
    
    <content type="html"><![CDATA[<h1 id="REAL3D-PORTRAIT-ONE-SHOT-REALISTIC-3D-TALKING-PORTRAIT-SYNTHESIS"><a href="#REAL3D-PORTRAIT-ONE-SHOT-REALISTIC-3D-TALKING-PORTRAIT-SYNTHESIS" class="headerlink" title="REAL3D-PORTRAIT: ONE-SHOT REALISTIC 3D TALKING PORTRAIT SYNTHESIS"></a>REAL3D-PORTRAIT: ONE-SHOT REALISTIC 3D TALKING PORTRAIT SYNTHESIS</h1><p>Paper     : <a href="https://arxiv.org/pdf/2401.08503.pdf">https://arxiv.org/pdf/2401.08503.pdf</a></p><p>Project   : <a href="https://real3dportrait.github.io/">https://real3dportrait.github.io/</a></p><p>Code      : <a href="https://github.com/yerfor/Real3DPortrait">https://github.com/yerfor/Real3DPortrait</a></p><p>Rebuttal: <a href="https://real3dportrait.github.io/static/pages/rebuttal.html">https://real3dportrait.github.io/static/pages/rebuttal.html</a></p><p><strong>摘要</strong></p><p>(1) 研究背景：说话人像生成旨在根据驱动条件（动作序列或驱动音频）合成说话人像视频。这是一个计算机图形学和计算机视觉中长期存在的跨模态任务，具有视频会议和虚拟现实 (VR) 等多项实际应用。先前的 2D 方法可以产生逼真的视频，这要归功于生成对抗网络 (GAN) 的强大功能。然而，由于缺乏显式的 3D 建模，这些 2D 方法在头部大幅移动时会面临变形伪影和不真实的失真。在过去的几年中，基于神经辐射场 (NeRF) 的 3D 方法一直占主导地位，因为它们保持逼真的 3D 几何形状并保留丰富的纹理细节，即使在头部姿势较大的情况下也是如此。然而，在大多数方法中，模型都过度拟合特定的人，这需要为每个看不见的身份进行昂贵的单独训练。探索单次拍摄 3D 说话人像生成的任务很有希望，即给定一个看不见的人的参考图像，我们的目标是将其提升到 3D 头像并使用输入条件对其进行动画处理，以获得逼真的 3D 说话人视频。随着 3D 生成模型的最新进展，可以学习到推广到各种身份的 3D 三平面表示（EG3D，Chan et al. (2022)）的隐藏空间。虽然最近的工作 (Li et al., 2023b; Li, 2023) 开创了单次拍摄 3D 说话人像生成，但它们未能同时实现准确的重建和动画。</p><p>(2) 过去的方法：一些工作仅使用 2D 图像作为输入，而另一些工作则使用 3D 图像作为输入。使用 2D 图像作为输入的方法通常会产生质量较差的结果，因为它们无法捕获对象的 3D 形状。使用 3D 图像作为输入的方法通常会产生质量更好的结果，但它们需要昂贵的 3D 扫描设备。 本方法的动机很充分。作者认为，单次拍摄 3D 说话人像生成是一个具有挑战性的任务，需要解决许多问题。这些问题包括：</p><ul><li><p>如何从单张 2D 图像重建准确的 3D 模型？</p></li><li><p>如何将 3D 模型与驱动条件（动作序列或驱动音频）相关联？</p></li><li><p>如何合成逼真的说话人像视频？</p><p>作者提出了一种新的方法来解决这些问题，该方法包括以下几个步骤：</p></li></ul><ol><li>从单张 2D 图像重建准确的 3D 模型。</li><li>将 3D 模型与驱动条件（动作序列或驱动音频）相关联。</li><li>合成逼真的说话人像视频。 作者的方法在几个数据集上进行了评估，结果表明该方法能够生成高质量的说话人像视频。</li></ol><p>(3) 研究方法：作者提出了一种名为 Real3D-Portrait 的框架，该框架可以从单张图像生成逼真的 3D 说话人像视频。Real3D-Portrait 包括以下几个模块：</p><ul><li><p>图像到平面模型：该模块将输入图像转换为 3D 三平面表示。</p></li><li><p>运动适配器：该模块将 3D 三平面表示与驱动条件（动作序列或驱动音频）相关联。</p></li><li><p>头部躯干背景超分辨率模型：该模块合成逼真的视频，具有自然的躯干运动和可切换的背景。</p></li><li><p>音频到运动模型：该模块支持单次拍摄的音频驱动说话人像生成。</p></li></ul><p>(4) 性能：Real3D-Portrait 在几个数据集上进行了评估，结果表明该方法能够生成高质量的说话人像视频。在 TalkingHead 数据集上，Real3D-Portrait 的平均重建误差为 0.006，平均动画误差为 0.008。在 VoxCeleb 数据集上，Real3D-Portrait 的平均重建误差为 0.007，平均动画误差为 0.009。在 LRW 数据集上，Real3D-Portrait 的平均重建误差为 0.008，平均动画误差为 0.010。这些结果表明，Real3D-Portrait 能够生成高质量的说话人像视频，并且该方法可以推广到看不见的身份。</p><p><strong>要点</strong></p><ul><li>提出 Real3D-Portrait 框架，用于生成逼真的说话肖像视频。</li><li>采用大规模图像到平面模型，从 3D 人脸生成模型中提取 3D 先验知识，提高一发 3D 重建能力。</li><li>使用高效的动作适配器，实现准确的动作条件动画。</li><li>利用头部躯干背景超分辨率模型，合成具有自然躯干运动和可切换背景的逼真视频。</li><li>支持一发音频驱动的说话面部生成，使用可推广的音频到动作模型。</li><li>大量实验证明，Real3D-Portrait 在看不见的身份上具有良好的泛化能力，并且与以前的方法相比，可以生成更逼真的说话肖像视频。</li></ul><p><img src="https://picx.zhimg.com/v2-68585b79de5f83b0dfa23304f41b9b98.png" alt="The inference pipeline of Real3D-Portrait."></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;REAL3D-PORTRAIT-ONE-SHOT-REALISTIC-3D-TALKING-PORTRAIT-SYNTHESIS&quot;&gt;&lt;a href=&quot;#REAL3D-PORTRAIT-ONE-SHOT-REALISTIC-3D-TALKING-PORTRAIT-S</summary>
      
    
    
    
    <category term="Paperscape" scheme="https://kedreamix.github.io/categories/Paperscape/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Blendshape学习笔记</title>
    <link href="https://kedreamix.github.io/Note/BlendShape/"/>
    <id>https://kedreamix.github.io/Note/BlendShape/</id>
    <published>2024-03-11T11:42:00.000Z</published>
    <updated>2024-03-11T12:01:31.162Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Blendshape-Morph-Target动画"><a href="#Blendshape-Morph-Target动画" class="headerlink" title="Blendshape(Morph Target动画)"></a>Blendshape(Morph Target动画)</h2><p>Blendshapes泛指3D定点动画的制作方式 (Maya里面称之为 blend shapes ，而3DS Max里称之为morph targets) ，在3D动画中用的比较多，尤其是人脸动画的制作，通过blendshape来驱动角色的面部表情。</p><p>用在脸部动画制作时，blendshape可以被称之为<strong>脸部特征，表情基准，定位符</strong>等等。这里要引入一个<code>FACS</code>的概念，可以简单理解为将脸部进行合理化的分区标准。</p><blockquote><p>“表情这个东西看起来是一个无限多可能的东西，怎么能够计算expression呢？</p><p>这就带来了Blendshapes——一组组成整体表情的基准（数量可以有十几个、50个、100+、 200+，越多就越细腻)。我们可以使用这一组基准通过线性组合来计算出整体的expression，用公式来说就是  ，其中e是expression，B是一组表情基准，d是对应的系数（在这一组里面的权重），b是neutral。” </p><p>— From <a href="https://zhuanlan.zhihu.com/p/78174706">https://zhuanlan.zhihu.com/p/78174706</a></p></blockquote><h2 id="BlendShape系数介绍"><a href="#BlendShape系数介绍" class="headerlink" title="BlendShape系数介绍"></a>BlendShape系数介绍</h2><p>在ARKit中，对表情特征位置定义了52组运动blendshape系数(<br><a href="https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation">https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation</a> )，每个blendshape系数代表一种表情定位符，表情定位符定义了特定表情属性，如mouthSmileLeft、mouthSmileRight等，与其对应的blendshape系数则表示表情运动范围。这52组blendshape系数极其描述如下表所示。</p><p><img src="https://p3-sign.toutiaoimg.com/pgc-image/984d8d76878441c3a8402f788ef6e46f~noop.image?_iz=58558&amp;from=article.pc_detail&amp;lk3s=953192f4&amp;x-expires=1710668214&amp;x-signature=qI8vU39X63te%2BVNdO78uBFphwK0%3D" alt="Blendshape"></p><p>每一个blendshape系数的取值范围为0～1的浮点数。以jawOpen为例，当认为用户的嘴巴完全闭紧时，返回的jawOpen系数为0。当认为用户的嘴巴张开至最大时，返回的jawOpen系数为1。</p><p><img src="https://p6-sign.toutiaoimg.com/pgc-image/2c8cbd123e00470e95500a8ae62da605~noop.image?_iz=58558&amp;from=article.pc_detail&amp;lk3s=953192f4&amp;x-expires=1710668214&amp;x-signature=UHPhjWP4v96kbtfJzF97Z%2Bp3klc%3D" alt=""></p><p>在用户完全闭嘴与嘴张到最大之间的过渡状态，jawOpen会根据用户嘴张大的幅度返回一个0～1的插值。</p><p><img src="https://p3-sign.toutiaoimg.com/pgc-image/8e8d980b8d69461fb5d2efbc50e47d47~noop.image?_iz=58558&amp;from=article.pc_detail&amp;lk3s=953192f4&amp;x-expires=1710668214&amp;x-signature=sFNMeBoNY3ZFfiO%2BRSjR8uGECIw%3D" alt=""></p><h2 id="脸部动捕的使用"><a href="#脸部动捕的使用" class="headerlink" title="脸部动捕的使用"></a>脸部动捕的使用</h2><h3 id="ARKit-脸部与Vive脸部blendshape基准对比"><a href="#ARKit-脸部与Vive脸部blendshape基准对比" class="headerlink" title="ARKit 脸部与Vive脸部blendshape基准对比"></a>ARKit 脸部与Vive脸部blendshape基准对比</h3><div class="table-container"><table><thead><tr><th></th><th>ARKit（52）</th><th>Extra</th><th>VIVE（52）</th><th>Extra</th></tr></thead><tbody><tr><td>Brow</td><td>5</td><td></td><td>0</td><td></td></tr><tr><td>Eye</td><td>13</td><td></td><td>14</td><td>Eye Frown + 1</td></tr><tr><td>Cheek</td><td>3</td><td></td><td>3</td><td></td></tr><tr><td>Nose</td><td>2</td><td></td><td>0</td><td></td></tr><tr><td>Jaw</td><td>4</td><td></td><td>4</td><td></td></tr><tr><td>Mouth</td><td>24</td><td></td><td>20</td><td>O shape - 1</td></tr><tr><td>Tongue</td><td>1</td><td>Tongue + 7</td><td>11</td><td></td></tr><tr><td>Sum</td><td>52</td><td>59</td><td>52</td><td>52</td></tr></tbody></table></div><h3 id="ARKit的52个Blendshape表情基准组"><a href="#ARKit的52个Blendshape表情基准组" class="headerlink" title="ARKit的52个Blendshape表情基准组"></a>ARKit的52个Blendshape表情基准组</h3><p>可以看ARKit Face Blendshapes的照片和3D模型示例：<a href="https://arkit-face-blendshapes.com/">https://arkit-face-blendshapes.com/</a></p><div class="table-container"><table><thead><tr><th>CC3</th><th>ARKit Name 表情基准/定位符</th><th>ARKit Picture</th><th>CC3 Picture</th></tr></thead><tbody><tr><td>A01</td><td>browInnerUp</td><td><img src="https://static.wixstatic.com/media/64c63b_4cc12dd62ef8484986eebe9739f4eac9~mv2.png/v1/fill/w_252,h_178,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_4cc12dd62ef8484986eebe9739f4eac9~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_fc0c248f4f6f46dda26eda66865678d2~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_fc0c248f4f6f46dda26eda66865678d2~mv2.png" alt=""></td></tr><tr><td>A02</td><td>browDownLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_18a57dc078214abea520f25ad6dfb02a~mv2.png/v1/fill/w_252,h_175,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_18a57dc078214abea520f25ad6dfb02a~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_9d780933931b469991ae0d4ddf105045~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_9d780933931b469991ae0d4ddf105045~mv2.png" alt=""></td></tr><tr><td>A03</td><td>browDownRight</td><td><img src="https://static.wixstatic.com/media/64c63b_105d6dd9d7c44394b96b242e6d9d580b~mv2.png/v1/fill/w_252,h_175,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_105d6dd9d7c44394b96b242e6d9d580b~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_d9943f7163414286809edef7c3bf2de7~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_d9943f7163414286809edef7c3bf2de7~mv2.png" alt=""></td></tr><tr><td>A04</td><td>browOuterUpLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_e7fe8581da2540a3bd7dfc39c874dd61~mv2.png/v1/fill/w_252,h_175,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e7fe8581da2540a3bd7dfc39c874dd61~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_c7234589721d4ddda4e2fcb1a9e0aa97~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_c7234589721d4ddda4e2fcb1a9e0aa97~mv2.png" alt=""></td></tr><tr><td>A05</td><td>browOuterUpRight</td><td><img src="https://static.wixstatic.com/media/64c63b_1fb29f740ff74e8aabadc3769c86501a~mv2.png/v1/fill/w_252,h_175,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_1fb29f740ff74e8aabadc3769c86501a~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_4aefd80dd4c548669d5ba80e4da639eb~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_4aefd80dd4c548669d5ba80e4da639eb~mv2.png" alt=""></td></tr><tr><td>A06</td><td>eyeLookUpLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_697a02a504c84d5f9e316172849bb6d0~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_697a02a504c84d5f9e316172849bb6d0~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_48e3b55ee9ca40f9aec9be8b35c403b0~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_48e3b55ee9ca40f9aec9be8b35c403b0~mv2.png" alt=""></td></tr><tr><td>A07</td><td>eyeLookUpRight</td><td><img src="https://static.wixstatic.com/media/64c63b_84cacf1f990a4e5c874c084a1ea626b3~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_84cacf1f990a4e5c874c084a1ea626b3~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_c879b6cca2ce4f8aa2385864c1fb9389~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_c879b6cca2ce4f8aa2385864c1fb9389~mv2.png" alt=""></td></tr><tr><td>A08</td><td>eyeLookDownLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_d229ef398f3547be93a1a59563520e81~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_d229ef398f3547be93a1a59563520e81~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_d6c6c94e3cee43db8ae9d6f36fc1a689~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_d6c6c94e3cee43db8ae9d6f36fc1a689~mv2.png" alt=""></td></tr><tr><td>A09</td><td>eyeLookDownRight</td><td><img src="https://static.wixstatic.com/media/64c63b_67a1674b6d584344b7ea77843f72be27~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_67a1674b6d584344b7ea77843f72be27~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_ca0dbf25d9f74085809cdcd0742ede35~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_ca0dbf25d9f74085809cdcd0742ede35~mv2.png" alt=""></td></tr><tr><td>A10</td><td>eyeLookOutLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_b4257aa18f754427a593064e71aa97fd~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_b4257aa18f754427a593064e71aa97fd~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_d24fd04ef2d64db18c31b90eccd5f1a9~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_d24fd04ef2d64db18c31b90eccd5f1a9~mv2.png" alt=""></td></tr><tr><td>A11</td><td>eyeLookInLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_03368853adeb4b8599da5451033cd809~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_03368853adeb4b8599da5451033cd809~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_049433ce862e44c4a5f96bcf0ad13bd0~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_049433ce862e44c4a5f96bcf0ad13bd0~mv2.png" alt=""></td></tr><tr><td>A12</td><td>eyeLookInRight</td><td><img src="https://static.wixstatic.com/media/64c63b_6e67745f7867402398390ce18a9f2882~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_6e67745f7867402398390ce18a9f2882~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_803074453832444d8dec710711196559~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_803074453832444d8dec710711196559~mv2.png" alt=""></td></tr><tr><td>A13</td><td>eyeLookOutRight</td><td><img src="https://static.wixstatic.com/media/64c63b_b54a5b6f123244d98eadbded8c29a8c3~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_b54a5b6f123244d98eadbded8c29a8c3~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_2e7b0fed966d453fa0a8dffaabeaf769~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_2e7b0fed966d453fa0a8dffaabeaf769~mv2.png" alt=""></td></tr><tr><td>A14</td><td>eyeBlinkLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_0b68b26a666a49da843b6a47c4579b46~mv2.png/v1/fill/w_252,h_175,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_0b68b26a666a49da843b6a47c4579b46~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_b46d50b28b5d40feba9a496b1ead4a5c~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_b46d50b28b5d40feba9a496b1ead4a5c~mv2.png" alt=""></td></tr><tr><td>A15</td><td>eyeBlinkRight</td><td><img src="https://static.wixstatic.com/media/64c63b_65e50badaa854262a87329394a87484c~mv2.png/v1/fill/w_252,h_175,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_65e50badaa854262a87329394a87484c~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_9113137c91934bdbab3fb26756e84783~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_9113137c91934bdbab3fb26756e84783~mv2.png" alt=""></td></tr><tr><td>A16</td><td>eyeSquintLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_7b9132e314d6404097f212401559e9c4~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_7b9132e314d6404097f212401559e9c4~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_0cccb71728de47e5a7f63fe9bc70bcaf~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_0cccb71728de47e5a7f63fe9bc70bcaf~mv2.png" alt=""></td></tr><tr><td>A17</td><td>eyeSquintRight</td><td><img src="https://static.wixstatic.com/media/64c63b_8cc99b12de914fe882c19229ce2a91da~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_8cc99b12de914fe882c19229ce2a91da~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_8445ac0161fe400ab28591fb6b0b1f56~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_8445ac0161fe400ab28591fb6b0b1f56~mv2.png" alt=""></td></tr><tr><td>A18</td><td>eyeWideLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_0c87ac4e4c5d4d5f9639523c82aa9d43~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_0c87ac4e4c5d4d5f9639523c82aa9d43~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_c097966492c3496cabf1d84455d7144d~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_c097966492c3496cabf1d84455d7144d~mv2.png" alt=""></td></tr><tr><td>A19</td><td>eyeWideRight</td><td><img src="https://static.wixstatic.com/media/64c63b_3157fc370d064da9926027034e8220d6~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_3157fc370d064da9926027034e8220d6~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_24dc2c84e19b436a97fd2db6044f439c~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_24dc2c84e19b436a97fd2db6044f439c~mv2.png" alt=""></td></tr><tr><td>A20</td><td>cheekPuff</td><td><img src="https://static.wixstatic.com/media/64c63b_de4df8062c5f47ca9cd322b75b535705~mv2.png/v1/fill/w_252,h_172,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_de4df8062c5f47ca9cd322b75b535705~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_27548c426f1b47ae834c757417e03269~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_27548c426f1b47ae834c757417e03269~mv2.png" alt=""></td></tr><tr><td>A21</td><td>cheekSquintLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_70520c1a1c374ff3855cb8dfa7450b8b~mv2.png/v1/fill/w_252,h_175,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_70520c1a1c374ff3855cb8dfa7450b8b~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_d769bd2ef0104030818ed7a156ee2a2e~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_d769bd2ef0104030818ed7a156ee2a2e~mv2.png" alt=""></td></tr><tr><td>A22</td><td>cheekSquintRight</td><td><img src="https://static.wixstatic.com/media/64c63b_2f82d4db05764690b33001da1d138f20~mv2.png/v1/fill/w_252,h_175,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_2f82d4db05764690b33001da1d138f20~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_49752693e89a4293982b5e023a0e1c75~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_49752693e89a4293982b5e023a0e1c75~mv2.png" alt=""></td></tr><tr><td>A23</td><td>noseSneerLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_843177402d2545d1a1f0a97e848df91c~mv2.png/v1/fill/w_252,h_178,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_843177402d2545d1a1f0a97e848df91c~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_9855bb8e50f54f638d4bc321dd3caa45~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_9855bb8e50f54f638d4bc321dd3caa45~mv2.png" alt=""></td></tr><tr><td>A24</td><td>noseSneerRight</td><td><img src="https://static.wixstatic.com/media/64c63b_8ee42dc6d8e443a0858e0c65ce56cc74~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_8ee42dc6d8e443a0858e0c65ce56cc74~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_edd25cffbf1249d89bb0f6c5a95b76e5~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_edd25cffbf1249d89bb0f6c5a95b76e5~mv2.png" alt=""></td></tr><tr><td>A25</td><td>jawOpen</td><td><img src="https://static.wixstatic.com/media/64c63b_aca391d5eb744a76b18d6ced31904111~mv2.png/v1/fill/w_267,h_192,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_aca391d5eb744a76b18d6ced31904111~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_f14421d8adb1461ea32ca31bd3cac7be~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_f14421d8adb1461ea32ca31bd3cac7be~mv2.png" alt=""></td></tr><tr><td>A26</td><td>jawForward</td><td><img src="https://static.wixstatic.com/media/64c63b_a199113be0f9418f8c729d9a7e7b4e49~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_a199113be0f9418f8c729d9a7e7b4e49~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_df9963f3452c4ce1bd1b6829a8045112~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_df9963f3452c4ce1bd1b6829a8045112~mv2.png" alt=""></td></tr><tr><td>A27</td><td>jawLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_2642a61cdd0241f9ba24339873003125~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_2642a61cdd0241f9ba24339873003125~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_0de7a40c68654461be74016c2e29cf02~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_0de7a40c68654461be74016c2e29cf02~mv2.png" alt=""></td></tr><tr><td>A28</td><td>jawRight</td><td><img src="https://static.wixstatic.com/media/64c63b_2838229bffe74a5abe7d25d9c6e398ca~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_2838229bffe74a5abe7d25d9c6e398ca~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_2f51f3993bed441b89b9b493a1f2e86b~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_2f51f3993bed441b89b9b493a1f2e86b~mv2.png" alt=""></td></tr><tr><td>A29</td><td>mouthFunnel</td><td><img src="https://static.wixstatic.com/media/64c63b_d2719d8d83524b52a735296f0dfbf092~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_d2719d8d83524b52a735296f0dfbf092~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_e2b4d7681dcf4b1faf34e3c5b57dd3ac~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e2b4d7681dcf4b1faf34e3c5b57dd3ac~mv2.png" alt=""></td></tr><tr><td>A30</td><td>mouthPucker</td><td><img src="https://static.wixstatic.com/media/64c63b_7771a95fb2ae4afeb885b7a684e3f249~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_7771a95fb2ae4afeb885b7a684e3f249~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_e553a24166984303920d5ec9ce1de6d6~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e553a24166984303920d5ec9ce1de6d6~mv2.png" alt=""></td></tr><tr><td>A31</td><td>mouthLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_d2e30cadc9b443f6993ee48d99ffb9c8~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_d2e30cadc9b443f6993ee48d99ffb9c8~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_76b134e6564749fcaf6e036a6dc53517~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_76b134e6564749fcaf6e036a6dc53517~mv2.png" alt=""></td></tr><tr><td>A32</td><td>mouthRight</td><td><img src="https://static.wixstatic.com/media/64c63b_ef34b0cf15c541058052d74870f95a11~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_ef34b0cf15c541058052d74870f95a11~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_332e51118068490cbb932bc8b3880895~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_332e51118068490cbb932bc8b3880895~mv2.png" alt=""></td></tr><tr><td>A33</td><td>mouthRollUpper</td><td><img src="https://static.wixstatic.com/media/64c63b_5c93c56f5d9e4698a86160047452fdae~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_5c93c56f5d9e4698a86160047452fdae~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_f86e02c72dcd4e27b5fafc3e7cbf5098~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_f86e02c72dcd4e27b5fafc3e7cbf5098~mv2.png" alt=""></td></tr><tr><td>A34</td><td>mouthRollLower</td><td><img src="https://static.wixstatic.com/media/64c63b_8d2d50c4784b4f4a881264f9e806b26e~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_8d2d50c4784b4f4a881264f9e806b26e~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_f6ce7f1df803456fb25b77533ec5c1a9~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_f6ce7f1df803456fb25b77533ec5c1a9~mv2.png" alt=""></td></tr><tr><td>A35</td><td>mouthShrugUpper</td><td><img src="https://static.wixstatic.com/media/64c63b_d70a5a8102d14df6be57658f696ab28c~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_d70a5a8102d14df6be57658f696ab28c~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_36feb9bc9305402e8a9e044b7f42c06e~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_36feb9bc9305402e8a9e044b7f42c06e~mv2.png" alt=""></td></tr><tr><td>A36</td><td>mouthShrugLower</td><td><img src="https://static.wixstatic.com/media/64c63b_c39b6573ab8b452c8ba9af4cfd61fa0d~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_c39b6573ab8b452c8ba9af4cfd61fa0d~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_881391abc1ff4fbabd6f7719d93179b8~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_881391abc1ff4fbabd6f7719d93179b8~mv2.png" alt=""></td></tr><tr><td>A37</td><td>mouthClose</td><td><img src="https://static.wixstatic.com/media/64c63b_7c1a9921e54c42c5bbad10ce2d2a2edc~mv2.png/v1/fill/w_267,h_129,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_7c1a9921e54c42c5bbad10ce2d2a2edc~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_8aded518da54400db938b69753b8539a~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_8aded518da54400db938b69753b8539a~mv2.png" alt=""></td></tr><tr><td>A38</td><td>mouthSmileLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_a3cdfd578cec40a5a83931c4d0c9f8ab~mv2.png/v1/fill/w_267,h_186,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_a3cdfd578cec40a5a83931c4d0c9f8ab~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_11aa5137231b4bfe8a8908f25d8d4112~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_11aa5137231b4bfe8a8908f25d8d4112~mv2.png" alt=""></td></tr><tr><td>A39</td><td>mouthSmileRight</td><td><img src="https://static.wixstatic.com/media/64c63b_f0c7a9ddfcb945f496f4ac8aafcfd0ca~mv2.png/v1/fill/w_267,h_186,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_f0c7a9ddfcb945f496f4ac8aafcfd0ca~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_4818df7bf47740f6bab387d0d2926a2b~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_4818df7bf47740f6bab387d0d2926a2b~mv2.png" alt=""></td></tr><tr><td>A40</td><td>mouthFrownLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_8e7c89a5e9514206ac3fd7152e912ef8~mv2.png/v1/fill/w_267,h_187,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_8e7c89a5e9514206ac3fd7152e912ef8~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_ba5a9fdcf6d246439b8d7d9dbf63fb16~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_ba5a9fdcf6d246439b8d7d9dbf63fb16~mv2.png" alt=""></td></tr><tr><td>A41</td><td>mouthFrownRight</td><td><img src="https://static.wixstatic.com/media/64c63b_019c769729a34c7c992c3bbde95adf2a~mv2.png/v1/fill/w_267,h_187,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_019c769729a34c7c992c3bbde95adf2a~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_4505259aa94646278b01cd6b4e6fe32a~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_4505259aa94646278b01cd6b4e6fe32a~mv2.png" alt=""></td></tr><tr><td>A42</td><td>mouthDimpleLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_b7c5c7b4fcea481ba877fab837ddda7c~mv2.png/v1/fill/w_267,h_187,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_b7c5c7b4fcea481ba877fab837ddda7c~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_53d010f6b8b340d6a305149152fe9eb2~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_53d010f6b8b340d6a305149152fe9eb2~mv2.png" alt=""></td></tr><tr><td>A43</td><td>mouthDimpleRight</td><td><img src="https://static.wixstatic.com/media/64c63b_268dda3d9bb14eaba63c5b123ab9002c~mv2.png/v1/fill/w_267,h_187,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_268dda3d9bb14eaba63c5b123ab9002c~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_ea46553169c749f69dc8e47737434193~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_ea46553169c749f69dc8e47737434193~mv2.png" alt=""></td></tr><tr><td>A44</td><td>mouthUpperUpLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_e6f82a77cd374e37b456590eb19c2d28~mv2.png/v1/fill/w_267,h_186,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e6f82a77cd374e37b456590eb19c2d28~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_5c9a52ea901243218e0c9252fcd45a00~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_5c9a52ea901243218e0c9252fcd45a00~mv2.png" alt=""></td></tr><tr><td>A45</td><td>mouthUpperUpRight</td><td><img src="https://static.wixstatic.com/media/64c63b_384bab2c926045f99f4bbef75b6975f0~mv2.png/v1/fill/w_267,h_186,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_384bab2c926045f99f4bbef75b6975f0~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_8abd87f586bb4d2088673a2358a65adb~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_8abd87f586bb4d2088673a2358a65adb~mv2.png" alt=""></td></tr><tr><td>A46</td><td>mouthLowerDownLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_2511f8304fbb49dab88eb09b118f88bc~mv2.png/v1/fill/w_267,h_188,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_2511f8304fbb49dab88eb09b118f88bc~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_21506f6994114f1194bc69958bd3778d~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_21506f6994114f1194bc69958bd3778d~mv2.png" alt=""></td></tr><tr><td>A47</td><td>mouthLowerDownRight</td><td><img src="https://static.wixstatic.com/media/64c63b_5c300e220ef04f388b827c096ad7aae6~mv2.png/v1/fill/w_267,h_188,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_5c300e220ef04f388b827c096ad7aae6~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_9ac53c48df9e4d63b6774b91aaa4db3d~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_9ac53c48df9e4d63b6774b91aaa4db3d~mv2.png" alt=""></td></tr><tr><td>A48</td><td>mouthPressLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_478c881ace1744ff825202484b212c17~mv2.png/v1/fill/w_267,h_187,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_478c881ace1744ff825202484b212c17~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_cca358e42c08454cb9f7f30317c4e93c~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_cca358e42c08454cb9f7f30317c4e93c~mv2.png" alt=""></td></tr><tr><td>A49</td><td>mouthPressRight</td><td><img src="https://static.wixstatic.com/media/64c63b_acad007d32d24b26b4cc192345afc0ba~mv2.png/v1/fill/w_267,h_186,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_acad007d32d24b26b4cc192345afc0ba~mv2.png" alt=""></td><td><br><img src="https://static.wixstatic.com/media/64c63b_35bac2e5acf54d438dd0acf4690c4ea2~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_35bac2e5acf54d438dd0acf4690c4ea2~mv2.png" alt=""></td></tr><tr><td>A50</td><td>mouthStretchLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_18fbf15030164a6383068c8fb7aa7e72~mv2.png/v1/fill/w_263,h_184,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_18fbf15030164a6383068c8fb7aa7e72~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_cf77104a546149e88698feb420726493~mv2.png/v1/fill/w_232,h_209,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_cf77104a546149e88698feb420726493~mv2.png" alt=""></td></tr><tr><td>A51</td><td>mouthStretchRight</td><td><img src="https://static.wixstatic.com/media/64c63b_3f8dd987a3d44b7e98e1e7abb1815111~mv2.png/v1/fill/w_263,h_184,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_3f8dd987a3d44b7e98e1e7abb1815111~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_b2a3abb6ea204ab293571c7c19747003~mv2.png/v1/fill/w_232,h_209,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_b2a3abb6ea204ab293571c7c19747003~mv2.png" alt=""></td></tr><tr><td>A52</td><td>tongueOut</td><td></td><td><img src="https://static.wixstatic.com/media/64c63b_10387f10b0e04d5fac672f8bd17d9459~mv2.png/v1/fill/w_232,h_209,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_10387f10b0e04d5fac672f8bd17d9459~mv2.png" alt=""></td></tr></tbody></table></div><ul><li>CC3 额外的舌头Blendshape(with open month)：</li></ul><div class="table-container"><table><thead><tr><th>T01</th><th>Tongue_Up</th><th></th><th><img src="https://static.wixstatic.com/media/64c63b_75c512342cde45ffbb40fcf5d463732e~mv2.png/v1/fill/w_244,h_220,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_75c512342cde45ffbb40fcf5d463732e~mv2.png" alt=""></th></tr></thead><tbody><tr><td>T02</td><td>Tongue_Down</td><td></td><td><img src="https://static.wixstatic.com/media/64c63b_4bdf00b4d23d4a89ac0bffbb66cc348d~mv2.png/v1/fill/w_244,h_220,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_4bdf00b4d23d4a89ac0bffbb66cc348d~mv2.png" alt=""></td></tr><tr><td>T03</td><td>Tongue_Left</td><td></td><td><img src="https://static.wixstatic.com/media/64c63b_860b7c7043894521a754755c35816cb3~mv2.png/v1/fill/w_244,h_220,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_860b7c7043894521a754755c35816cb3~mv2.png" alt=""></td></tr><tr><td>T04</td><td>Tongue_Right</td><td></td><td><img src="https://static.wixstatic.com/media/64c63b_02c7adc74f934d31ad35c01615b96735~mv2.png/v1/fill/w_244,h_220,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_02c7adc74f934d31ad35c01615b96735~mv2.png" alt=""></td></tr><tr><td>T05</td><td>Tongue_Roll</td><td></td><td><img src="https://static.wixstatic.com/media/64c63b_bfa27b0483c94f7484eeed246642fbc5~mv2.png/v1/fill/w_244,h_220,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_bfa27b0483c94f7484eeed246642fbc5~mv2.png" alt=""></td></tr><tr><td>T06</td><td>Tongue_Tip_Up</td><td></td><td><img src="https://static.wixstatic.com/media/64c63b_6a7d96ce1444409c958adc03652983b7~mv2.png/v1/fill/w_244,h_220,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_6a7d96ce1444409c958adc03652983b7~mv2.png" alt=""></td></tr><tr><td>T07</td><td>Tongue_Tip_Down</td><td></td><td><img src="https://static.wixstatic.com/media/64c63b_37fa335ad1a549b983fb6552db3b5198~mv2.png/v1/fill/w_244,h_220,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_37fa335ad1a549b983fb6552db3b5198~mv2.png" alt=""></td></tr></tbody></table></div><h3 id="Vive面部的表情基准组"><a href="#Vive面部的表情基准组" class="headerlink" title="Vive面部的表情基准组"></a>Vive面部的表情基准组</h3><p>Vive这一套脸部追踪也是52个blendshapes，但是和苹果的基准有很大区别。</p><ul><li>区别一：舌头</li></ul><p>苹果其实是52+7，因为舌头在52个里只有一个伸舌头的blendshape，但vive其实是42 + 10，整体来讲Vive表情记住能tracking到的表情细节还是更少一些。</p><ul><li>区别二：眉毛</li></ul><p>ARKit的52个blendshapes，是根据硬件分区一对一tracking的，然而Vive眉毛不分是没有单独另设blendshapes，而是与眼睛的动作blended在一起作为一个blendshape的，并不是精准的一对一分区tracking。</p><p>我下面编号的排序是按照<a href="https://developer.vive.com/resources/vive-sense/sdk/vive-eye-and-facial-tracking-sdk/">VIVE Eye and Facial Tracking SDK</a> unity 里inspector里的顺序，方便我加表情。</p><p>这里是整理的用ARKit制作Vive基准的对应编号：</p><p><a href="https://docs.google.com/spreadsheets/d/1kWXnqtiVbXRb1FrD5NLlxxuxbYmS0Z6YBLuIE1WwqD4/edit?usp=sharing">https://docs.google.com/spreadsheets/d/1kWXnqtiVbXRb1FrD5NLlxxuxbYmS0Z6YBLuIE1WwqD4/edit?usp=sharing</a></p><ul><li>Eye Blendshapes （14 = 12 + 2）</li></ul><div class="table-container"><table><thead><tr><th>Vive编号</th><th>Vive表情基准</th><th>Vive Picture</th><th>Create by CC3 blendshapes</th></tr></thead><tbody><tr><td>V01</td><td>Eye_Left_Blink</td><td><img src="https://static.wixstatic.com/media/64c63b_735cb0ae227e42bca98f9c51fbd0df6b~mv2.png/v1/fill/w_238,h_182,al_c,lg_1,q_85,enc_auto/64c63b_735cb0ae227e42bca98f9c51fbd0df6b~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_e5f3c0dfc63a42618e182a9b1a0c1e9c~mv2.png/v1/fill/w_211,h_188,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e5f3c0dfc63a42618e182a9b1a0c1e9c~mv2.png" alt=""></td></tr><tr><td>V02</td><td>Eye_Left_Wide</td><td><img src="https://static.wixstatic.com/media/64c63b_1d9223fb44574a94988a7c9ce4d89b39~mv2.png/v1/fill/w_222,h_160,al_c,lg_1,q_85,enc_auto/64c63b_1d9223fb44574a94988a7c9ce4d89b39~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_5dd9ed05165f4fe9b486b4f0604dacc1~mv2.png/v1/fill/w_211,h_188,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_5dd9ed05165f4fe9b486b4f0604dacc1~mv2.png" alt=""></td></tr><tr><td>V03</td><td>Eye_Left_Right</td><td><img src="https://static.wixstatic.com/media/64c63b_638f42e4960743c4b235ab18b5ac6eba~mv2.png/v1/fill/w_238,h_188,al_c,lg_1,q_85,enc_auto/64c63b_638f42e4960743c4b235ab18b5ac6eba~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_c5fcda96b7184941b2a89b2193470f2b~mv2.png/v1/fill/w_211,h_188,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_c5fcda96b7184941b2a89b2193470f2b~mv2.png" alt=""></td></tr><tr><td>V04</td><td>Eye_Left_Left</td><td><img src="https://static.wixstatic.com/media/64c63b_a1041585f54748728dd71aec7de129f5~mv2.png/v1/fill/w_238,h_192,al_c,lg_1,q_85,enc_auto/64c63b_a1041585f54748728dd71aec7de129f5~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_a26c36f7dc2940fe9513e263f7a99c4e~mv2.png/v1/fill/w_211,h_188,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_a26c36f7dc2940fe9513e263f7a99c4e~mv2.png" alt=""></td></tr><tr><td>V05</td><td>Eye_Left_Up</td><td><img src="https://static.wixstatic.com/media/64c63b_d2c84d05015c4d8289f6c7d6d5ba0dcc~mv2.png/v1/fill/w_238,h_203,al_c,lg_1,q_85,enc_auto/64c63b_d2c84d05015c4d8289f6c7d6d5ba0dcc~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_e3e67e16e4b84697a39c3333bad24712~mv2.png/v1/fill/w_211,h_188,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e3e67e16e4b84697a39c3333bad24712~mv2.png" alt=""></td></tr><tr><td>V06</td><td>Eye_Left_Down</td><td><img src="https://static.wixstatic.com/media/64c63b_5cd43316383549e282e7f3f743df9053~mv2.png/v1/fill/w_238,h_195,al_c,lg_1,q_85,enc_auto/64c63b_5cd43316383549e282e7f3f743df9053~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_993bd278fb024580a834a71c6886cc4b~mv2.png/v1/fill/w_211,h_188,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_993bd278fb024580a834a71c6886cc4b~mv2.png" alt=""></td></tr><tr><td>V07</td><td>Eye_Right_Blink</td><td><img src="https://static.wixstatic.com/media/64c63b_2e198e55d97a491cbc66059e6f6adddc~mv2.png/v1/fill/w_235,h_195,al_c,lg_1,q_85,enc_auto/64c63b_2e198e55d97a491cbc66059e6f6adddc~mv2.png" alt=""></td><td></td></tr><tr><td>V08</td><td>Eye_Right_Wide</td><td><img src="https://static.wixstatic.com/media/64c63b_6eebbecf575d4bec905be4dbec06322c~mv2.png/v1/fill/w_223,h_160,al_c,lg_1,q_85,enc_auto/64c63b_6eebbecf575d4bec905be4dbec06322c~mv2.png" alt=""></td><td></td></tr><tr><td>V09</td><td>Eye_Right_Right</td><td><img src="https://static.wixstatic.com/media/64c63b_fc88e475e8c4435a98364224f54ade1a~mv2.png/v1/fill/w_234,h_197,al_c,lg_1,q_85,enc_auto/64c63b_fc88e475e8c4435a98364224f54ade1a~mv2.png" alt=""></td><td></td></tr><tr><td>V10</td><td>Eye_Right_Left</td><td><img src="https://static.wixstatic.com/media/64c63b_216b900672314c37a3e91501b7fe7cc1~mv2.png/v1/fill/w_238,h_190,al_c,lg_1,q_85,enc_auto/64c63b_216b900672314c37a3e91501b7fe7cc1~mv2.png" alt=""></td><td></td></tr><tr><td>V11</td><td>Eye_Right_Up</td><td><img src="https://static.wixstatic.com/media/64c63b_062847a5c45d4dabbe78d255779013dd~mv2.png/v1/fill/w_231,h_196,al_c,lg_1,q_85,enc_auto/64c63b_062847a5c45d4dabbe78d255779013dd~mv2.png" alt=""></td><td></td></tr><tr><td>V12</td><td>Eye_Right_Down</td><td><img src="https://static.wixstatic.com/media/64c63b_b6fae35fba8543d0becc535532111d23~mv2.png/v1/fill/w_238,h_176,al_c,lg_1,q_85,enc_auto/64c63b_b6fae35fba8543d0becc535532111d23~mv2.png" alt=""></td><td></td></tr><tr><td>V13</td><td>Eye_Left_squeeze: The blendShape close eye tightly when Eye_Left_Blink  value is 100.</td><td><img src="https://static.wixstatic.com/media/64c63b_c7b07f1b4ec6495685d85808d23c04e8~mv2.png/v1/fill/w_238,h_183,al_c,lg_1,q_85,enc_auto/64c63b_c7b07f1b4ec6495685d85808d23c04e8~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_a629d31ace624dd8b2ded5123123156e~mv2.png/v1/fill/w_211,h_188,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_a629d31ace624dd8b2ded5123123156e~mv2.png" alt=""></td></tr><tr><td>V14</td><td>Eye_Right_squeeze</td><td><img src="https://static.wixstatic.com/media/64c63b_b82c535f90874742b3c0b6ff62136fe2~mv2.png/v1/fill/w_238,h_194,al_c,lg_1,q_85,enc_auto/64c63b_b82c535f90874742b3c0b6ff62136fe2~mv2.png" alt=""></td></tr></tbody></table></div><ul><li>Lip Blendshapes （38 = 37 + 1）</li></ul><div class="table-container"><table><thead><tr><th>Vive编号</th><th>Vive表情基准</th><th>Vive Picture</th><th>Create by CC3 blendshapes</th></tr></thead><tbody><tr><td>V15</td><td>Jaw_Right</td><td><img src="https://static.wixstatic.com/media/64c63b_75f8b68a96104ef8bf36e393c7ecd48b~mv2.png/v1/fill/w_235,h_190,al_c,lg_1,q_85,enc_auto/64c63b_75f8b68a96104ef8bf36e393c7ecd48b~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_65471d28b0d743c0bb6232ffaee0f6b6~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_65471d28b0d743c0bb6232ffaee0f6b6~mv2.png" alt=""></td></tr><tr><td>V16</td><td>Jaw_Left</td><td><img src="https://static.wixstatic.com/media/64c63b_9cacde29288c4523a2192835a736ad6b~mv2.png/v1/fill/w_245,h_202,al_c,lg_1,q_85,enc_auto/64c63b_9cacde29288c4523a2192835a736ad6b~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_017d7142fd494aef9ad7bbe53fa1d6eb~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_017d7142fd494aef9ad7bbe53fa1d6eb~mv2.png" alt=""></td></tr><tr><td>V17</td><td>Jaw_Forward</td><td><img src="https://static.wixstatic.com/media/64c63b_3876f3dd1a1b4eed92e8405b42700190~mv2.png/v1/fill/w_248,h_197,al_c,lg_1,q_85,enc_auto/64c63b_3876f3dd1a1b4eed92e8405b42700190~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_20353f83579541428557c32d92545c9e~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_20353f83579541428557c32d92545c9e~mv2.png" alt=""></td></tr><tr><td>V18</td><td>Jaw_Open</td><td><img src="https://static.wixstatic.com/media/64c63b_dc79f10003534839948d3261183d5082~mv2.png/v1/fill/w_244,h_188,al_c,lg_1,q_85,enc_auto/64c63b_dc79f10003534839948d3261183d5082~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_24470b9cc9964a11906c42b1d1a6e5e9~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_24470b9cc9964a11906c42b1d1a6e5e9~mv2.png" alt=""></td></tr><tr><td>V19</td><td>Mouth_Ape_Shape</td><td><img src="https://static.wixstatic.com/media/64c63b_7a0f9461a760449db12b2159009ccc93~mv2.png/v1/fill/w_249,h_196,al_c,lg_1,q_85,enc_auto/64c63b_7a0f9461a760449db12b2159009ccc93~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_3d7911f5bfa645adb7f3c36fbeafa2b9~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_3d7911f5bfa645adb7f3c36fbeafa2b9~mv2.png" alt=""></td></tr><tr><td>V20</td><td>Mouth_Upper_Right</td><td><img src="https://static.wixstatic.com/media/64c63b_01037e0042754059b7ada72a8adf2e8a~mv2.png/v1/fill/w_227,h_161,al_c,lg_1,q_85,enc_auto/64c63b_01037e0042754059b7ada72a8adf2e8a~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_185ec305ba464016a15c2420fb04916e~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_185ec305ba464016a15c2420fb04916e~mv2.png" alt=""></td></tr><tr><td>V21</td><td>Mouth_Upper_Left</td><td><img src="https://static.wixstatic.com/media/64c63b_f0c61e8f3f3c42d7ad6d83703f1a61d9~mv2.png/v1/fill/w_265,h_182,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_f0c61e8f3f3c42d7ad6d83703f1a61d9~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_abbecb4860a44fe9800585825beb4b17~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_abbecb4860a44fe9800585825beb4b17~mv2.png" alt=""></td></tr><tr><td>V22</td><td>Mouth_Lower_Right</td><td><img src="https://static.wixstatic.com/media/64c63b_3ec74984b19d44d389a68bcc1ac1a7fb~mv2.png/v1/fill/w_265,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_3ec74984b19d44d389a68bcc1ac1a7fb~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_80a4cdffa3fb493e9c153b517d9aebda~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_80a4cdffa3fb493e9c153b517d9aebda~mv2.png" alt=""></td></tr><tr><td>V23</td><td>Mouth_Lower_Left</td><td><img src="https://static.wixstatic.com/media/64c63b_e80cee8738ea42528c8f351303f5e2c8~mv2.png/v1/fill/w_265,h_225,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e80cee8738ea42528c8f351303f5e2c8~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_edf8e441ff994c27bde811a21754d5e5~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_edf8e441ff994c27bde811a21754d5e5~mv2.png" alt=""></td></tr><tr><td>V24</td><td>*Mouth_Upper_Overturn</td><td><img src="https://static.wixstatic.com/media/64c63b_5f77c14164ae48cf9c0cf4c762b97837~mv2.png/v1/fill/w_265,h_202,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_5f77c14164ae48cf9c0cf4c762b97837~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_b8ae358e723f42e199338722f186e238~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_b8ae358e723f42e199338722f186e238~mv2.png" alt=""></td></tr><tr><td>V25</td><td>*Mouth_Lower_Overturn</td><td><img src="https://static.wixstatic.com/media/64c63b_16a26f9ced50420b99a4c32fc296c112~mv2.png/v1/fill/w_265,h_210,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_16a26f9ced50420b99a4c32fc296c112~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_282323813dfa4ea1aa76551b112e3919~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_282323813dfa4ea1aa76551b112e3919~mv2.png" alt=""></td></tr><tr><td>V26</td><td>Mouth_Pout</td><td><img src="https://static.wixstatic.com/media/64c63b_e7da853fe63242a9bedbd7fe3bddadc7~mv2.png/v1/fill/w_265,h_205,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e7da853fe63242a9bedbd7fe3bddadc7~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_36615908f74d4663a6bc438c3287938c~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_36615908f74d4663a6bc438c3287938c~mv2.png" alt=""></td></tr><tr><td>V27</td><td>Mouth_Smile_Right</td><td><img src="https://static.wixstatic.com/media/64c63b_1de129dc23784dd0af8d5bbccff75741~mv2.png/v1/fill/w_265,h_205,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_1de129dc23784dd0af8d5bbccff75741~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_47a00d3e749e47fa8b3489da81252654~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_47a00d3e749e47fa8b3489da81252654~mv2.png" alt=""></td></tr><tr><td>V28</td><td>Mouth_Smile_Left</td><td><img src="https://static.wixstatic.com/media/64c63b_e66c5606123d4272bc4d3206a101e884~mv2.png/v1/fill/w_265,h_213,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e66c5606123d4272bc4d3206a101e884~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_0a26740959644351bb01f9e9d40ef35e~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_0a26740959644351bb01f9e9d40ef35e~mv2.png" alt=""></td></tr><tr><td>V29</td><td>Mouth_Sad_Right</td><td><img src="https://static.wixstatic.com/media/64c63b_ba5427a221c2446e9d3b9e30d94d80b9~mv2.png/v1/fill/w_265,h_224,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_ba5427a221c2446e9d3b9e30d94d80b9~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_c1e99be3d6c34038852ce55b72102f5c~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_c1e99be3d6c34038852ce55b72102f5c~mv2.png" alt=""></td></tr><tr><td>V30</td><td>Mouth_Sad_Left</td><td><img src="https://static.wixstatic.com/media/64c63b_00772c50ca334cbf95dd1bf53be4c6b8~mv2.png/v1/fill/w_265,h_218,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_00772c50ca334cbf95dd1bf53be4c6b8~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_889e2637303f4d7195afd699a3d92b86~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_889e2637303f4d7195afd699a3d92b86~mv2.png" alt=""></td></tr><tr><td>V31</td><td>Cheek_Puff_Right</td><td><img src="https://static.wixstatic.com/media/64c63b_765350d6685547d4b03b7ae31e7346e0~mv2.png/v1/fill/w_265,h_224,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_765350d6685547d4b03b7ae31e7346e0~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_66386b8d632b4556a00f886613f26d92~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_66386b8d632b4556a00f886613f26d92~mv2.png" alt=""></td></tr><tr><td>V32</td><td>Cheek_Puff_Left</td><td><img src="https://static.wixstatic.com/media/64c63b_2998211eb141496d8651b786337b7846~mv2.png/v1/fill/w_265,h_213,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_2998211eb141496d8651b786337b7846~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_833f433253fb4bcc8e380d79120b3003~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_833f433253fb4bcc8e380d79120b3003~mv2.png" alt=""></td></tr><tr><td>V33</td><td>Cheek_Suck</td><td><img src="https://static.wixstatic.com/media/64c63b_6eea541e05494d26a06cbbe5377cdc0a~mv2.png/v1/fill/w_265,h_209,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_6eea541e05494d26a06cbbe5377cdc0a~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_8573de7fc0d84245a0fa4412ecd3e842~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_8573de7fc0d84245a0fa4412ecd3e842~mv2.png" alt=""></td></tr><tr><td>V34</td><td>Mouth_Upper_UpRight</td><td><img src="https://static.wixstatic.com/media/64c63b_b6ca87cbb7774b2ab4f0ec3748ec9c51~mv2.png/v1/fill/w_265,h_216,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_b6ca87cbb7774b2ab4f0ec3748ec9c51~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_c48b985609fe4129ad1dac8a41905a7e~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_c48b985609fe4129ad1dac8a41905a7e~mv2.png" alt=""></td></tr><tr><td>V35</td><td>Mouth<em>Upper</em> UpLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_58bdc8db0ac3451388534ff3bfb0fa83~mv2.png/v1/fill/w_265,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_58bdc8db0ac3451388534ff3bfb0fa83~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_5ff3f05aeecd48fe9f3077f5c9c96569~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_5ff3f05aeecd48fe9f3077f5c9c96569~mv2.png" alt=""></td></tr><tr><td>V36</td><td>Mouth_Lower_DownRight</td><td><img src="https://static.wixstatic.com/media/64c63b_12f26efe2f28425cb366eea55e83470c~mv2.png/v1/fill/w_265,h_223,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_12f26efe2f28425cb366eea55e83470c~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_30ae27fa9ee94aa8a1b29bbd5fe7b0b2~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_30ae27fa9ee94aa8a1b29bbd5fe7b0b2~mv2.png" alt=""></td></tr><tr><td>V37</td><td>Mouth_Lower_DownLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_4ed1a27945324b53aad0aa3cf453a275~mv2.png/v1/fill/w_265,h_218,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_4ed1a27945324b53aad0aa3cf453a275~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_76893f9855fa4a5a9415cd8abfae6f6f~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_76893f9855fa4a5a9415cd8abfae6f6f~mv2.png" alt=""></td></tr><tr><td>V38</td><td>Mouth_Upper_Inside</td><td><img src="https://static.wixstatic.com/media/64c63b_f5e050f0d9954760879ccd18185c2fc8~mv2.png/v1/fill/w_265,h_209,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_f5e050f0d9954760879ccd18185c2fc8~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_5217c686c7c6455aaca6ba1b2ce64217~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_5217c686c7c6455aaca6ba1b2ce64217~mv2.png" alt=""></td></tr><tr><td>V39</td><td>Mouth_Lower_Inside</td><td><img src="https://static.wixstatic.com/media/64c63b_0031f9adda4441cbb6361e280e594c7b~mv2.png/v1/fill/w_269,h_211,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_0031f9adda4441cbb6361e280e594c7b~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_5108178f718e492eb840f4d678eb3e4e~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_5108178f718e492eb840f4d678eb3e4e~mv2.png" alt=""></td></tr><tr><td>V40</td><td>Mouth_Lower_Overlay</td><td><img src="https://static.wixstatic.com/media/64c63b_26e5bd6286474b4ea3f4fff3933b91f1~mv2.png/v1/fill/w_269,h_222,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_26e5bd6286474b4ea3f4fff3933b91f1~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_1ab252df4c5146e1815e23a83edb2cd2~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_1ab252df4c5146e1815e23a83edb2cd2~mv2.png" alt=""></td></tr><tr><td>V41</td><td>Tongue_LongStep1</td><td><img src="https://static.wixstatic.com/media/64c63b_6791ccfceffe4c2ca07f277b91037521~mv2.png/v1/fill/w_269,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_6791ccfceffe4c2ca07f277b91037521~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_4ef4ab94e1ef47dcb01facf5d168f1d1~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_4ef4ab94e1ef47dcb01facf5d168f1d1~mv2.png" alt=""></td></tr><tr><td>V42</td><td>Tongue_LongStep2</td><td><img src="https://static.wixstatic.com/media/64c63b_e79ccc0096a54ce2b48d188cf6907d0c~mv2.png/v1/fill/w_269,h_181,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e79ccc0096a54ce2b48d188cf6907d0c~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_6e560524670843848266701061f24c63~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_6e560524670843848266701061f24c63~mv2.png" alt=""></td></tr><tr><td>V43</td><td>*Tongue_Down</td><td><img src="https://static.wixstatic.com/media/64c63b_ac97c6cf9fd940e9b38cdf22ab3c9261~mv2.png/v1/fill/w_269,h_199,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_ac97c6cf9fd940e9b38cdf22ab3c9261~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_a2a8aab70eea4b6ba9378cf249aef3a0~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_a2a8aab70eea4b6ba9378cf249aef3a0~mv2.png" alt=""></td></tr><tr><td>V44</td><td>*Tongue_Up</td><td><img src="https://static.wixstatic.com/media/64c63b_0c11f06c560842718309c52bc159ffa8~mv2.png/v1/fill/w_269,h_197,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_0c11f06c560842718309c52bc159ffa8~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_0abc5fe7b2da4c988591e18fc6e060ac~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_0abc5fe7b2da4c988591e18fc6e060ac~mv2.png" alt=""></td></tr><tr><td>V45</td><td>*Tongue_Right</td><td><img src="https://static.wixstatic.com/media/64c63b_0d6e1976f6c342a395f9631be529c694~mv2.png/v1/fill/w_269,h_202,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_0d6e1976f6c342a395f9631be529c694~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_e17db94c36594f32b60ce6057a17aafc~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e17db94c36594f32b60ce6057a17aafc~mv2.png" alt=""></td></tr><tr><td>V46</td><td>*Tongue_Left</td><td><img src="https://static.wixstatic.com/media/64c63b_2633b9481a6f4e94ad4bfe6a6d52e122~mv2.png/v1/fill/w_269,h_201,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_2633b9481a6f4e94ad4bfe6a6d52e122~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_ad949214aeda4b3488c238af7aabdba6~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_ad949214aeda4b3488c238af7aabdba6~mv2.png" alt=""></td></tr><tr><td>V47</td><td>*Tongue_Roll</td><td><img src="https://static.wixstatic.com/media/64c63b_4d41918f9af84d0aaaa2de1e354a5706~mv2.png/v1/fill/w_269,h_216,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_4d41918f9af84d0aaaa2de1e354a5706~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_2c544b45c0ea48079bb570912b85b2a3~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_2c544b45c0ea48079bb570912b85b2a3~mv2.png" alt=""></td></tr><tr><td>V48</td><td>*Tongue_UpLeft_Morph</td><td><img src="https://static.wixstatic.com/media/64c63b_49d65415de0f47d5a07aa77cfebd54e6~mv2.png/v1/fill/w_269,h_201,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_49d65415de0f47d5a07aa77cfebd54e6~mv2.png" alt=""> <img src="https://static.wixstatic.com/media/64c63b_9df53a0e560f41f2a441f9e799b56d1c~mv2.png/v1/fill/w_269,h_221,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_9df53a0e560f41f2a441f9e799b56d1c~mv2.png" alt=""></td><td></td></tr><tr><td>V49</td><td>*Tongue_UpRight_Morph</td><td><img src="https://static.wixstatic.com/media/64c63b_118b5229d2274b5f95a163ebc0d0cfad~mv2.png/v1/fill/w_269,h_232,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_118b5229d2274b5f95a163ebc0d0cfad~mv2.png" alt=""> <img src="https://static.wixstatic.com/media/64c63b_aa40430e533c40c69f0addb2df019a29~mv2.png/v1/fill/w_269,h_215,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_aa40430e533c40c69f0addb2df019a29~mv2.png" alt=""></td><td></td></tr><tr><td>V50</td><td>*Tongue_DownLeft_Morph</td><td><img src="https://static.wixstatic.com/media/64c63b_92fc463c5efc4436a23870d596023ba9~mv2.png/v1/fill/w_269,h_237,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_92fc463c5efc4436a23870d596023ba9~mv2.png" alt=""> <img src="https://static.wixstatic.com/media/64c63b_b3a74eba5433479b96ff645e85681480~mv2.png/v1/fill/w_269,h_231,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_b3a74eba5433479b96ff645e85681480~mv2.png" alt=""></td><td></td></tr><tr><td>V51</td><td>*Tongue_DownRight_Morph</td><td><img src="https://static.wixstatic.com/media/64c63b_db25fb8a736e4b1f9f4e11ba7436e0b0~mv2.png/v1/fill/w_269,h_224,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_db25fb8a736e4b1f9f4e11ba7436e0b0~mv2.png" alt=""> <img src="https://static.wixstatic.com/media/64c63b_b707961c295f40a7995644e93e438ffc~mv2.png/v1/fill/w_269,h_212,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_b707961c295f40a7995644e93e438ffc~mv2.png" alt=""></td><td></td></tr><tr><td>V52</td><td>*O-shaped mouth</td><td><img src="https://static.wixstatic.com/media/64c63b_97cacb52babe4a109cd02874efcb2eda~mv2.png/v1/fill/w_269,h_175,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_97cacb52babe4a109cd02874efcb2eda~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_6e98ff0232d349b8a6f7d8348992ab37~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_6e98ff0232d349b8a6f7d8348992ab37~mv2.png" alt=""> <img src="https://static.wixstatic.com/media/64c63b_b35e7f3707fc4b0ca75f80c4d77867a4~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_b35e7f3707fc4b0ca75f80c4d77867a4~mv2.png" alt=""></td></tr></tbody></table></div><h2 id="MediaPipe提取BlendShape"><a href="#MediaPipe提取BlendShape" class="headerlink" title="MediaPipe提取BlendShape"></a>MediaPipe提取BlendShape</h2><p>MediaPipe Face Landmarker解决方案最初于5月的Google I/O 2023发布。它可以检测面部landmark并输出blendshape score，以渲染与用户匹配的3D面部模型。通过MediaPipe Face Landmarker解决方案，KDDI和谷歌成功地为虚拟主播带来了真实感。</p><p><strong>技术实现</strong></p><p>使用Mediapipe强大而高效的Python包，KDDI开发人员能够检测表演者的面部特征并实时提取52个混合形状。</p><p>还可参考：<a href="https://colab.research.google.com/github/googlesamples/mediapipe/blob/main/examples/face_landmarker/python/%5BMediaPipe_Python_Tasks%5D_Face_Landmarker.ipynb">https://colab.research.google.com/github/googlesamples/mediapipe/blob/main/examples/face_landmarker/python/%5BMediaPipe_Python_Tasks%5D_Face_Landmarker.ipynb</a></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> mediapipe <span class="keyword">as</span> mp</span><br><span class="line"><span class="keyword">from</span> mediapipe.tasks <span class="keyword">import</span> python <span class="keyword">as</span> mp_python</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">MP_TASK_FILE = <span class="string">"face_landmarker_with_blendshapes.task"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FaceMeshDetector</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(MP_TASK_FILE, mode=<span class="string">"rb"</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f_buffer = f.read()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 创建配置选项</span></span><br><span class="line">        base_options = mp_python.BaseOptions(model_asset_buffer=f_buffer)</span><br><span class="line">        options = mp_python.vision.FaceLandmarkerOptions(</span><br><span class="line">            base_options=base_options,</span><br><span class="line">            output_face_blendshapes=<span class="literal">True</span>,</span><br><span class="line">            output_facial_transformation_matrixes=<span class="literal">True</span>,</span><br><span class="line">            running_mode=mp.tasks.vision.RunningMode.LIVE_STREAM,</span><br><span class="line">            num_faces=<span class="number">1</span>,</span><br><span class="line">            result_callback=self.mp_callback</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 创建模型</span></span><br><span class="line">        self.model = mp_python.vision.FaceLandmarker.create_from_options(options)</span><br><span class="line">        self.landmarks = <span class="literal">None</span></span><br><span class="line">        self.blendshapes = <span class="literal">None</span></span><br><span class="line">        self.latest_time_ms = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">mp_callback</span>(<span class="params">self, mp_result, output_image, timestamp_ms: <span class="built_in">int</span></span>):</span><br><span class="line">        <span class="comment"># 处理回调结果</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(mp_result.face_landmarks) &gt;= <span class="number">1</span> <span class="keyword">and</span> <span class="built_in">len</span>(mp_result.face_blendshapes) &gt;= <span class="number">1</span>:</span><br><span class="line">            self.landmarks = mp_result.face_landmarks[<span class="number">0</span>]</span><br><span class="line">            self.blendshapes = [b.score <span class="keyword">for</span> b <span class="keyword">in</span> mp_result.face_blendshapes[<span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update</span>(<span class="params">self, frame</span>):</span><br><span class="line">        t_ms = <span class="built_in">int</span>(time.time() * <span class="number">1000</span>)</span><br><span class="line">        <span class="keyword">if</span> t_ms &lt;= self.latest_time_ms:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        </span><br><span class="line">        frame_mp = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame)</span><br><span class="line">        self.model.detect_async(frame_mp, t_ms)</span><br><span class="line">        self.latest_time_ms = t_ms</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_results</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.landmarks, self.blendshapes</span><br></pre></td></tr></tbody></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://www.mianzi-lizi.com/post/blendshape学习笔记">https://www.mianzi-lizi.com/post/blendshape学习笔记</a></li><li><a href="https://www.toutiao.com/article/6915330866285691395/">利用Animoji技术识别用户的表情</a></li><li><a href="https://news.nweon.com/110210">通过MediaPipe解决方案来为虚拟主播带来更逼真真实感</a></li><li><a href="https://bbs.huaweicloud.com/blogs/374337">Unity &amp; FACEGOOD Audio2Face 通过音频驱动面部BlendShape</a></li><li><a href="https://www.cnblogs.com/jesse123/p/9014234.html">GenerativeAI Avatar solutions</a></li></ul>]]></content>
    
    
    <summary type="html">Blendshapes泛指3D定点动画的制作方式 (Maya里面称之为 blend shapes ，而3DS Max里称之为morph targets) ，在3D动画中用的比较多，尤其是人脸动画的制作，通过blendshape来驱动角色的面部表情。</summary>
    
    
    
    <category term="Note" scheme="https://kedreamix.github.io/categories/Note/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
    <category term="3D reconstruction" scheme="https://kedreamix.github.io/tags/3D-reconstruction/"/>
    
  </entry>
  
  <entry>
    <title>SyncTalk The Devil is in the Synchronization for Talking Head Synthesis</title>
    <link href="https://kedreamix.github.io/Paperscape/SyncTalk/"/>
    <id>https://kedreamix.github.io/Paperscape/SyncTalk/</id>
    <published>2024-03-07T07:57:00.000Z</published>
    <updated>2024-03-09T09:37:44.711Z</updated>
    
    <content type="html"><![CDATA[<h1 id="SyncTalk-The-Devil-is-in-the-Synchronization-for-Talking-Head-Synthesis"><a href="#SyncTalk-The-Devil-is-in-the-Synchronization-for-Talking-Head-Synthesis" class="headerlink" title="SyncTalk: The Devil is in the Synchronization for Talking Head Synthesis"></a>SyncTalk: The Devil is in the Synchronization for Talking Head Synthesis</h1><p>Paper   : <a href="https://arxiv.org/abs/2311.17590">https://arxiv.org/abs/2311.17590</a></p><p>Project : <a href="https://ziqiaopeng.github.io/synctalk/">https://ziqiaopeng.github.io/synctalk/</a></p><p>Video    : <a href="https://ziqiaopeng.github.io/synctalk/#teaser">https://ziqiaopeng.github.io/synctalk/#teaser</a></p><p>Code    : <a href="https://github.com/ziqiaopeng/SyncTalk">https://github.com/ziqiaopeng/SyncTalk</a></p><p><strong>摘要</strong></p><p>神经辐射场 - 生成对抗网络框架用于实现说话人头部视频的同步合成。</p><p>（1）研究背景： 生成逼真的、由语音驱动的谈话头部视频是一项具有挑战性的任务。传统生成对抗网络（GAN）难以保持一致的面部身份，而神经辐射场（NeRF）方法虽然可以解决这个问题，但通常会产生不匹配的唇部动作、不充分的面部表情和不稳定的头部姿势。一个逼真的谈话头部需要同步协调主体身份、唇部动作、面部表情和头部姿势。缺乏这些同步是导致不真实和人工结果的根本缺陷。 </p><p>（2）过去的方法及其问题： GAN 方法难以保持一致的面部身份。NeRF 方法虽然可以解决这个问题，但通常会产生不匹配的唇部动作、不充分的面部表情和不稳定的头部姿势。 </p><p>（3）提出的研究方法： SyncTalk 是一种基于 NeRF 的方法，它有效地保持了主体身份，增强了谈话头部合成的同步性和真实性。SyncTalk 使用面部同步控制器将唇部动作与语音对齐，并创新地使用 3D 面部混合形状模型来捕捉准确的面部表情。头部同步稳定器优化头部姿势，实现更自然的头部运动。肖像同步生成器恢复头发细节，并将生成的头部与躯干融合，以获得无缝的视觉体验。</p><p>（4）方法在什么任务上取得了什么性能，这些性能是否支持了它们的目标： SyncTalk 在谈话头部合成同步性和真实性方面优于最先进的方法。广泛的实验和用户研究表明，SyncTalk 在同步性和真实性方面优于最先进的方法。</p><p><strong>关键要点</strong></p><ul><li>传统生成对抗网络难以维持一致的面部身份。</li><li>神经辐射场方法可以解决面部身份一致性问题，但经常出现嘴唇运动不匹配、面部表情不足和头部姿势不稳定的问题。</li><li>逼真的说话人头部视频需要同步协调主体身份、嘴唇运动、面部表情和头部姿势。</li><li>缺少同步性是导致不真实和人为结果的根本缺陷。</li><li>SyncTalk 是一种基于神经辐射场的方法，有效地保持了主体身份，提高了说话人头部合成中的同步性和真实感。</li><li>SyncTalk 使用面部同步控制器将嘴唇运动与语音对齐，并创新地使用 3D 面部混合形状模型来捕捉准确的面部表情。</li><li>SyncTalk 的头部同步稳定器优化了头部姿势，实现了更自然的头部运动。</li><li>人像同步生成器恢复头发细节，将生成的头部与躯干融合，以获得无缝的视觉体验。</li></ul><p><img src="https://picx.zhimg.com/v2-a57e0937b2f452009023394a59529dfb.png" alt="SyncTalk"></p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>这篇论文中，解决最好的就是同步的问题，所以也称为同步的Devil 魔鬼😈。现有方法在四个关键领域需要更多的同步：<strong>主体身份</strong>、<strong>唇部运动</strong>、<strong>面部表情</strong>和<strong>头部姿势</strong>。</p><ul><li><p>首先，在基于GAN的方法中，由于连续帧中特征的不稳定性以及仅使用少量帧作为面部重建参考，保持视频中主体的身份是具有挑战性的。</p></li><li><p>其次，唇部运动与语音不同步。在基于NeRF的方法中，仅基于5分钟语音数据集训练的音频特征难以泛化到不同的语音输入。</p></li><li><p>第三，缺乏面部表情控制，大多数方法只能产生唇部运动或控制眨眼，导致面部动作不自然。</p></li><li><p>第四，头部姿势不同步。</p></li></ul><p>先前的方法依赖于稀疏的landmarks来计算投影误差，但这些landmarks的抖动和不准确性导致头部姿势不稳定。这些同步问题会引入伪影，并显著降低真实感。</p><p>为了解决这些同步挑战，引入了SyncTalk，这是一种基于NeRF的方法，专注于高度同步、逼真的、语音驱动的说话头部合成，采用三平面哈希表示来维护主体身份。通过面部同步控制器和头部同步稳定器，SyncTalk显著提高了合成视频的同步性和视觉质量。PortraitSync Generator进一步改善了视觉质量，精心细化了视觉细节。整个渲染过程可以实现50 FPS，并输出高分辨率视频。</p><div class="table-container"><table><thead><tr><th>模块</th><th>描述</th></tr></thead><tbody><tr><td>Face-Sync Controller</td><td>在Face-Sync控制器中，预先在2D音频视听数据集上对音频视觉编码器进行预训练，得到了一种通用表示，确保了不同语音样本之间的唇部同步运动。对于控制面部表情，采用了一个语义丰富的3D面部混合形状模型，该模型通过52个参数控制特定的面部表情区域。</td></tr><tr><td>Head-Sync Stabilizer</td><td>在Head-Sync稳定器中，使用AD-NeRF中的头部运动跟踪器来推断头部的粗略旋转和平移参数。由于粗略参数的不稳定性，借鉴了同步定位与地图(SLAM)的思想，结合头部关键点跟踪器跟踪稠密关键点，并采用bundle adjustment method 束调整方法来优化头部姿势，从而实现稳定连续的头部运动。</td></tr><tr><td>Portrait-Sync Generator</td><td>为了进一步提高SyncTalk的视觉保真度，设计了一个Portrait-Sync生成器。这个模块修复了NeRF建模中的伪影，特别是头发和背景等细节，输出高分辨率视频。</td></tr></tbody></table></div><p><strong>主要贡献</strong></p><ul><li>提出了一个Face-Sync控制器，结合音频视觉编码器和面部动画捕捉器，确保准确的唇部同步和动态面部表情渲染。 </li><li>引入了一个Head-Sync稳定器，跟踪头部旋转和面部运动关键点。利用束调整方法，该稳定器保证了平滑同步的头部运动。</li><li>设计了一个Portrait-Sync生成器，通过修复NeRF建模中的伪影和细化头发和背景等细节，提高了视觉保真度。</li></ul><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p><strong>GAN-based Method</strong></p><p>近来，基于GAN的说话头合成成为了计算机视觉中的一个重要研究领域。然而，它们在保持视频中主体的身份一致性方面存在挑战。</p><p>例如，Wav2Lip引入了一个唇部同步专家来监督唇部运动。然而，由于使用了来自参考帧的五帧来重建唇部，它难以保持主体的身份。另一些方法尝试进行全脸合成，但往往难以确保面部表情和头部姿势之间的同步。除了视频流技术外，还有一些方法试图通过语音使单张图像“说话”，如SadTalker可以从单张图像生成一个人说话的视频。然而，这些方法无法生成自然的头部姿势和面部表情，难以保持主体的身份，影响了同步效果，导致视觉感知不真实。</p><p>与这些方法相比，SyncTalk使用NeRF<strong>对人脸进行三维建模</strong>。其能够在规范空间中表示<strong>连续的3D场景的能力</strong>，使其在保持主体身份一致性和保留细节方面表现出色。</p><p><strong>NeRF-based Method</strong></p><p>近来，随着NeRF的崛起，许多领域已开始利用它来解决相关挑战。先前的工作已将NeRF整合到合成说话头像的任务中，并将音频作为驱动信号，但这些方法都是基于普通的NeRF模型。</p><p>例如，AD-NeRF需要大约10秒来渲染单个图像。RADNeRF旨在实现实时视频生成，并使用了基于Instant-NGP的NeRF。ER-NeRF通过引入三平面哈希编码器来修剪空白空间区域，提倡紧凑且加速的渲染方法。GeneFace试图通过将语音特征转换为面部标志来减少NeRF的伪影，但这往往导致唇部运动不准确。尝试使用基于NeRF的方法创建角色头像，例如，不能直接由语音驱动。这些方法仅将音频作为条件，没有清晰的同步概念，并且通常导致唇部运动平均。</p><p>此外，先前的方法<strong>缺乏对面部表情的控制</strong>，仅限于控制眨眼，并且无法对抬眉毛或皱眉等动作进行建模。此外，这些方法在头部姿势不稳定方面存在显着问题，<strong>导致头部和躯干分离</strong>。相比之下，使用Face-Sync控制器来建模音频和唇部运动之间的关系，从而增强唇部运动和表情的同步性，使用Head-Sync稳定器来稳定头部姿势，通过解决这些同步问题，提高了视觉质量。</p><h2 id="主要方法"><a href="#主要方法" class="headerlink" title="主要方法"></a>主要方法</h2><p>SyncTalk主要由三部分组成，接下来会一一介绍</p><ul><li><strong>Face-Sync Controller</strong> 控制嘴唇运动和面部表情</li><li><strong>Head-Sync Stabilizer</strong> 稳定头部姿势</li><li><strong>Portrait-Sync Generator</strong> 渲染的高同步面部帧</li></ul><p><img src="https://pica.zhimg.com/v2-03605cd4fbd659c9d341840c64fd3b41.png" alt="Overview of SyncTalk"></p><h3 id="Face-Sync-Controller"><a href="#Face-Sync-Controller" class="headerlink" title="Face-Sync Controller"></a>Face-Sync Controller</h3><p><strong>Audio-Visual Encoder</strong></p><p>在现有的方法中，大部分的音频特征提取器是用类似于 <strong>DeepSpeech，Wav2Vec 2.0 和 HuBERT</strong> 等ASR模型，但是这些事专门为Automatic Speech Recognition ASR任务设计的，这种设计的音频编码器并不能真正反映嘴唇运动。这是因为预训练的模型是<strong>基于从音频到文本的特征分布，而需要从音频到嘴唇运动的特征分布</strong>。</p><p>针对这种情况，使用在LRS2上训练的<a href="https://github.com/smeetrs/deep_avsr">deep avsr</a>来做音频特征提取器，使用预训练的唇形同步鉴别器 <a href="https://github.com/joonson/syncnet_python">SyncNet</a>来监督视频的同步效果，这是使用连续的面部窗口F和相对应的音频帧A输入，同时分为正负样本进行训练，利用<strong>余弦相似度和交叉熵损失</strong>来最小化同步样本的距离并最大化非同步样本的距离。</p><script type="math/tex; mode=display">\begin{aligned}\sin(F,A)&=\frac{F\cdot A}{\|F\|_2\|A\|_2})\end{aligned},</script><script type="math/tex; mode=display">L_{\mathrm{sync}}=-\left(y\log(\sin(F,A))+(1-y)\log(1-\sin(F,A))\right),</script><p><img src="https://picx.zhimg.com/v2-6b250a8119b776d55493f82cfda54bc5.png" alt="正负样本"></p><p>同时在同步鉴别器的监督下，预训练对应的视听特征提取器，这里面堆叠卷积网络进行编码解码，最后用<strong>重建损失</strong>来进行监督。训练后，我们使用 Conv(A) 作为从音频中提取的唇部空间。</p><script type="math/tex; mode=display">L_{\mathrm{recon}}=\|F-\mathrm{Dec}(\mathrm{Conv}(A)\oplus\mathrm{Conv}(F))\|_1.</script><p><strong>Facial Animation Capturer</strong></p><p>在之前的研究中发现，基于NeRF的方法只能改变眨眼，无法准确地建模面部表情。这导致训练出的角色表情僵硬，面部细节不准确，特别是对于有明显面部动作的角色，如眨眼、抬眉毛或皱眉等。<strong>考虑到需要更加同步和逼真的面部表情，添加了一个表情同步控制模块。</strong></p><p>具体而言，引入了一个<strong>基于52个语义面部混合形状系数 B 的3D面部先验模型来建模面部</strong>，也就是3D blendshape 系数来控制面部，这一部分类似于 <a href="https://arxiv.org/abs/2303.11089">EmoTalk</a>。因为3D面部模型能够保留面部运动的结构信息，所以它能够很好地反映面部动作的内容，同时又不会引起面部结构的失真。</p><p><strong>在训练过程中，首先使用一个复杂的面部混合形状捕捉模块将面部表情捕捉为E(B)，然后选择七个核心面部表情控制系数来控制眉毛、额头和眼睛区域。</strong>这些系数与表情高度相关，且独立于嘴唇的运动。因为面部系数具有语义信息，所以我们可以在推理过程中同步演讲者的面部表情。</p><p><img src="https://pica.zhimg.com/v2-9cfb1cfb7f4ae95b64a868f8e8abad0e.png" alt="Facial Animation Capturer"></p><p><strong>Facial-Aware Masked-Attention</strong></p><p>为了减少训练过程中嘴唇特征和表情特征之间的相互干扰，引入了Facial-Aware Disentangle Attention模块。基于区域注意力向量 V，这类似于<a href="https://fictionarry.github.io/ER-NeRF/">ER-NeRF</a>，我们分别将Mask $M<em>{lip}$ 和 $M</em>{exp}$ 添加到嘴唇和表情的注意力区域。</p><script type="math/tex; mode=display">\begin{aligned}V_{\mathrm{lip}}&=V\odot M_{\mathrm{lip}},\\V_{\mathrm{exp}}&=V\odot M_{\mathrm{exp}}.\end{aligned}</script><p>通过这样设计的注意力机制，能够有效解耦嘴唇运动和眨眼运动等，从而减少耦合带来的伪影，最后利用解耦的嘴唇特征 $f<em>l = F</em>{lip} ⊙ V<em>{lip}$ 和表情特征$f_e = f</em>{exp} ⊙ V_{exp}$。</p><p><img src="https://pica.zhimg.com/v2-ba601309ab5cc09573f4291d7ae27f13.png" alt="ER-NeRF Mask"></p><h3 id="Head-Sync-Stabilizer"><a href="#Head-Sync-Stabilizer" class="headerlink" title="Head-Sync Stabilizer"></a>Head-Sync Stabilizer</h3><p><strong>Head Motion Tracker</strong></p><p>头部姿势，表示为 p，是指人的头部在 3D 空间中的旋转角度，由旋转 R 和平移 T 定义。</p><p>不稳定的头部姿势会导致头部抖动。为了获得头部姿势的粗略估计，首先，通过在预定范围内迭代 i 次来确定最佳焦距。对于每个焦距候选 fi，重新初始化旋转和平移值。目标是最小化 3D 可变形模型 (3DMM) 的投影地标与视频帧中的实际地标之间的误差。</p><script type="math/tex; mode=display">f_{\mathrm{opt}}=\arg\min_{f_i}E_i(L_{2D},L_{3D}(f_i,R_i,T_i)),</script><p>其中 $E_i$表示的就是MSE，这样能够以更好地将模型的投影lmk与实际视频lmk对齐，然后得到最优的旋转和平移矩阵，也是用MSE来最小化，这是对每一帧进行操作的，在对应视频帧的最优值。</p><script type="math/tex; mode=display">(R_{\mathrm{opt}},T_{\mathrm{opt}})=\arg\min_{R,T}E(L_{2D},L_{3D}(f_{\mathrm{opt}},R,T)).</script><p><strong>Head Points Tracker</strong></p><p>对于之前基于NeRF的方法来说，先前的方法利用基于 3DMM 的技术来提取头部姿势并生成不准确的结果。为了提高R和T的精度，我们使用像Co- tracker这样的光流估计模型来跟踪面部关键点K。</p><p>接下来，使用预训练的光流估计模型，在获取面部运动光流后，我们使用<strong>拉普拉斯滤波器</strong>选择位于最显著流变化位置的关键点，并在流序列中跟踪这些关键点的运动轨迹。通过这个模块确保了所有帧上的面部关键点对齐更加精确和一致，从而增强了头部姿势参数的准确性。</p><p><strong>Bundle Adjustment</strong></p><p>根据关键点和粗略的头部姿势，引入了一个两阶段优化框架来提高关键点和头部姿势估计的准确性。</p><ul><li><p>第一阶段，随机初始化 j 个关键点的 3D 坐标并优化它们的位置，以便与图像平面上跟踪的关键点对齐。这一部分最小化损失函数 $L_{init}$，捕获<strong>投影关键点 P 和跟踪关键点 K</strong> 之间的差异：</p><script type="math/tex; mode=display">L_{\mathrm{init}}=\sum_j\lVert P_j-K_j\rVert_2.</script></li><li><p>第二阶段，开始进行更全面的优化，以细化 3D 关键点和相关的头部联合姿势参数，通过Adam优化器优化算法，<strong>调整空间坐标、旋转角度R和平移T</strong>以最小化对齐误差$L_{sec}$，表示为：</p><script type="math/tex; mode=display">L_{\sec}=\sum_j\lVert P_j(R,T)-K_j\rVert_2.</script><p>经过这些优化后，观察到所得的头部姿势和平移参数平滑且稳定。</p></li></ul><h3 id="Dynamic-Portrait-Renderer"><a href="#Dynamic-Portrait-Renderer" class="headerlink" title="Dynamic Portrait Renderer"></a>Dynamic Portrait Renderer</h3><p><strong>Tri-Plane Hash Representation</strong></p><p>这一部分实际上就是NeRF的体渲染的方式，都是一些定义的部分。</p><script type="math/tex; mode=display">\hat{C}(\mathrm{r})=\int_{t_n}^{t_f}\sigma(\mathrm{r}(t))\cdot\mathrm{c}(\mathrm{r}(t),\mathrm{d})\cdot T(t)dt,</script><p>类似于ER-NeRF的方式，解决哈希冲突和优化音频特征处理的问题，结合了三个独特定向xyz的 2D 哈希网格，也就是 <strong>Tri-Plane Hash</strong>，作为hash的编码器。</p><script type="math/tex; mode=display">\mathcal{H}^{\mathrm{AB}}:(a,b)\to\mathrm{f}_{ab}^{\mathrm{AB}},\\\mathrm{f_x}=\mathcal{H}^\mathrm{XY}(x,y)\oplus\mathcal{H}^\mathrm{YZ}(y,z)\oplus\mathcal{H}^\mathrm{XZ}(x,z),</script><p>其中输出 $f^{AB}<em>{ab} ∈ R</em>{LD}$，具有层数 $L$ 和每个方向的特征维度 $D$，表示与投影坐标$ (a, b)$ 相对应的平面几何特征，$H^{AB}$ 表示平面 $R^{AB}$ 的多分辨率哈希编码器。得到每个方向的向量以后，产生 $3 × LD$ 通道向量。采用$fx$、视角方向$d$、嘴唇特征$f_l$和表情特征$f_e$，三平面哈希的隐式函数定义为：</p><script type="math/tex; mode=display">\mathcal{F}^{\mathcal{H}}:(\mathrm{x},\mathrm{d},f_l,f_e;\mathcal{H}^3)\to(\mathrm{c},\sigma),</script><p>类似于ER-NeRF，训练采用了一个两步粗到细的策略。首先，使用MSE损失评估预测的 $\hat{C(r)}$与实际图像颜色$C(r)$之间的差异。鉴于MSE在细节捕捉方面的局限性。接下来进入一个细化阶段，引入LPIPS损失以增强细节，类似于ER-NeRF。我们从图像中提取随机补丁Patch $P$，并将LPIPS（由λ加权）与MSE结合起来以改善细节表示。</p><script type="math/tex; mode=display">\mathcal{L}_\mathrm{total}=\sum_\mathrm{r}\|C(\mathrm{r})-\hat{C}(\mathrm{r})\|_2+\lambda\times\mathcal{L}_\mathrm{LPIPS}(\hat{\mathcal{P}},\mathcal{P}).</script><p><strong>Portrait-Sync Generator</strong></p><p>在训练过程中，为了解决 NeRF 在<strong>捕捉发丝和动态背景</strong>等精细细节方面的局限性，引入了一个包含两个关键部分的 PortraitSync 生成器。</p><p>首先，NeRF 渲染面部区域 ($Fr$)，通过高斯模糊创建 $G(Fr)$，然后使用我们同步的头部姿势能够与原始图像 ($F_o$) 合并，以增强头发细节保真度。</p><p>其次，当头部和躯干结合在一起时，如果源视频中的角色说话而生成的面部保持沉默，则可能会出现暗间隙区域，如下图（b）所示。 所以用平均颈部颜色 ($Cn$) 填充这些区域。 </p><p>这种方法通过肖像同步生成器产生更真实的细节并提高视觉质量。</p><p><img src="https://picx.zhimg.com/v2-421af4b4cfa489148de7fc8f4067427b.png" alt="比较"></p><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p><strong>数据集</strong></p><p>为了进行公平比较，我们使用了来自AD-NeRF，GeneFace和ER-NeRF中相同的视频序列，其中包括英语和法语。这些视频的平均长度约为8,843帧，每个视频以25 FPS录制。除了来自AD-NeRF的视频分辨率为450 × 450外，所有其他视频的分辨率均为512 × 512，并以角色为中心。</p><p><strong>比较基线</strong></p><ul><li>GAN-based  方法  ：Wav2Lip，VideoReTalking，DINet，TalkLip and IP-LAP。</li><li>NeRF-based 方法 ： AD-NeRF，RADNeRF，GeneFace and ER-NeRF。</li></ul><p><strong>实验细节</strong></p><ul><li>在粗略阶段，肖像头部经过100,000次迭代训练，在精细阶段训练25,000次迭代。</li><li>每次迭代使用2D哈希编码器（L=14，F=1）采样$256^2$条光线。</li><li>采用AdamW优化器[24]，哈希编码器的学习率为0.01，其他模块的学习率为0.001。</li><li>在NVIDIA RTX 3090 GPU上，总训练时间约为2小时。</li></ul><p><strong>定量评价</strong></p><div class="table-container"><table><thead><tr><th>评估指标</th><th>描述</th></tr></thead><tbody><tr><td>全参考质量评估</td><td>使用峰值信噪比（PSNR）、学习感知图像补丁相似性（LPIPS）、多尺度结构相似性（MS-SSIM）和Frechet Inception Distance（FID）作为评估指标。</td></tr><tr><td>无参考质量评估</td><td>在高PSNR图像中，纹理细节可能与人类视觉感知不一致。为了更精确地定义和比较输出，使用两种无参考方法：自然图像质量评估器（NIQE）和无参考图像空间质量评估器（BRISQUE）。</td></tr><tr><td>同步评估</td><td>对于同步性，使用地标距离（LMD）来衡量面部运动的同步性，动作单位误差（AUE）来评估面部运动的准确性，并引入唇同步误差置信度（LSE-C），与Wav2Lip一致，以评估唇部运动与音频之间的同步性。</td></tr></tbody></table></div><p><strong>定量评估结果</strong></p><ul><li>头部重建方法在图像质量和同步性方面均优于基于GAN和NeRF的最新方法。</li><li>经过<code>Portrait-Sync Generato</code>r处理后，图像质量得到了显著改善，头发细节得到了恢复。</li><li>方法在维持主体身份、唇部、表情和姿势的同步性方面表现出色。</li><li>使用分布外音频的最新SOTA方法的驱动器结果表明，方法在唇音同步评估方面领先。</li><li>渲染速度远远超过视频输入速度，可以实现实时生成视频流。</li></ul><p><img src="https://pica.zhimg.com/v2-3093f3d799bb12490a7f79dba96bde99.png" alt="The quantitative results of the head reconstruction."></p><p><img src="https://picx.zhimg.com/v2-73c53cd37a7c9e87af9b918778a84d3e.png" alt="The quantitative results of the lip synchronization."></p><p><strong>定性评价</strong></p><div class="table-container"><table><thead><tr><th>评估结果</th><th>描述</th></tr></thead><tbody><tr><td>图像质量比较</td><td>在图中，我们展示了我们的方法与其他方法的比较。可以观察到，SyncTalk展示了更精确、更准确的面部细节。</td></tr><tr><td>与Wav2Lip的比较</td><td>与Wav2Lip相比，我们的方法在保持主体身份的同时提供了更高的保真度和分辨率。</td></tr><tr><td>与IP-LAP的比较</td><td>与IP-LAP相比，我们的方法在唇形同步方面表现出色，主要归功于音频-视觉编码器带来的音频-视觉一致性。</td></tr><tr><td>与GeneFace的比较</td><td>与GeneFace相比，我们的方法可以通过表情同步精确地重现眨眼和抬眉等动作。</td></tr><tr><td>与ER-NeRF的比较</td><td>与ER-NeRF相比，我们的方法通过姿势同步稳定器避免了头部和身体的分离，并生成了更准确的唇形。</td></tr><tr><td></td></tr></tbody></table></div><p><img src="https://picx.zhimg.com/v2-b076e645737b2297bee21027ac8e27ad.png" alt="Qualitative comparison of facial synthesis by different methods."></p><p><strong>User Study</strong>  </p><p>我们设计了一个详尽的用户研究问卷，35名参与者进行评分。问卷设计了五个方面的评分：唇同步准确性、表情同步准确性、姿势同步准确性、图像质量和视频真实性。</p><p>参与者平均完成问卷时间为19分钟，标准化的Cronbach α系数为0.96。用户研究结果显示，SyncTalk在所有评估中均超过以前的方法，特别是在视频真实性方面。</p><p><img src="https://picx.zhimg.com/v2-2666052562f51f053affc9fb748eec54.png" alt="User Study"></p><p><strong>Ablation Study</strong></p><p>接下来进行了消融研究，以检验我们模型中不同部分的贡献，选择了三个核心指标进行评估：PSNR、LPIPS和LMD。</p><p>我们选择了一个名为“May”的主体进行测试，结果如表所示。</p><p><img src="https://pic1.zhimg.com/v2-b204e48268633b55ad93cf70dbc8f9bd.png" alt="Ablation study for our components"></p><p>音频-视觉编码器提供了主要的唇部同步信息，当替换此模块时，所有三个指标都变差，其中特别是LMD错误增加了21.15%，表明唇部动作同步减少，如图5（a）所示，显示出我们的音频-视觉编码器可以提取准确的唇部特征。</p><p><img src="https://pica.zhimg.com/v2-2fc44a31570aeacd6badcf909f669fdc.png" alt="Ablation Study"></p><p>用ER-NeRF 的<strong>眨眼模块</strong>替换<strong>Facial Animation Capture</strong>模块，这一部分会影响眉毛的运动和图像质量。</p><p><strong>Facial-Aware Masked-Attention</strong>主要解耦了唇部和面部其他部位之间的运动，在移除后略微影响图像质量。</p><p>若没有<strong>头部同步稳定器</strong>，所有指标都显著下降，特别是LPIPS，导致头部姿势抖动和头部与躯干分离，如图5（b）所示。</p><p><strong>Portrait-Sync Generator</strong>恢复了像头发这样的细节，移除此模块会影响头发等细节的恢复，导致明显的分割边界。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>本文介绍了SyncTalk，这是一种基于高度同步的NeRF方法，用于实现逼真的语音驱动的说话头部合成。</li><li>框架包括面部同步控制器、头部同步稳定器和肖像同步生成器，能够保持主体身份，并生成同步的唇部动作、面部表情和稳定的头部姿势。</li><li>通过广泛的评估，SyncTalk在创建逼真和同步的说话头部视频方面表现出优异的性能，相较于现有方法。</li><li>期望SyncTalk不仅能增强各种应用程序的功能，还能在说话头部合成领域激发进一步的创新。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;SyncTalk-The-Devil-is-in-the-Synchronization-for-Talking-Head-Synthesis&quot;&gt;&lt;a href=&quot;#SyncTalk-The-Devil-is-in-the-Synchronization-for-</summary>
      
    
    
    
    <category term="Paperscape" scheme="https://kedreamix.github.io/categories/Paperscape/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>VividTalk One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior</title>
    <link href="https://kedreamix.github.io/Paperscape/VividTalk/"/>
    <id>https://kedreamix.github.io/Paperscape/VividTalk/</id>
    <published>2024-03-05T07:31:00.000Z</published>
    <updated>2024-03-15T09:15:46.598Z</updated>
    
    <content type="html"><![CDATA[<h1 id="VividTalk-One-Shot-Audio-Driven-Talking-Head-Generation-Based-on-3D-Hybrid-Prior"><a href="#VividTalk-One-Shot-Audio-Driven-Talking-Head-Generation-Based-on-3D-Hybrid-Prior" class="headerlink" title="VividTalk: One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior"></a>VividTalk: One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior</h1><p>Paper   : <a href="https://arxiv.org/pdf/2312.01841.pdf">https://arxiv.org/pdf/2312.01841.pdf</a></p><p>Project : <a href="https://humanaigc.github.io/vivid-talk/">https://humanaigc.github.io/vivid-talk/</a></p><p>Video   : <a href="https://www.youtube.com/watch?v=lJVzt7JCe_4">https://www.youtube.com/watch?v=lJVzt7JCe_4</a></p><p>Code    : <a href="https://github.com/HumanAIGC/VividTalk">https://github.com/HumanAIGC/VividTalk</a>  (Maybe Comming Soon)</p><p><strong>摘要</strong></p><p>创新的两阶段框架 VividTalk 可生成高质量视觉效果的说话人头部视频，包括唇形同步、丰富的面部表情、自然的头部姿势等。</p><p>（1）音频驱动的说话头生成已经引起广泛关注，在唇形同步、面部表情、头部姿势生成和视频质量方面取得了进展。然而，由于音频和动作之间的一对多映射，还没有模型能够在所有这些指标上达到最优SOTA。<br>（2）以往的方法通常使用混合形状Blendshape或顶点偏移vertex来表示面部表情，但这些方法在捕捉精细的表情细节方面存在局限性。此外，头部姿势的生成通常是通过直接从音频中学习来实现的，这可能会导致不合理和不连续的结果。<br>（3）本文提出了一种名为 VividTalk 的两阶段通用框架，支持生成具有所有上述属性的高视觉质量说话头视频。在第一阶段，通过学习非刚性表情运动和刚性头部运动将音频映射到网格。对于表情运动，采用混合形状和顶点作为中间表示，以最大限度地提高模型的表示能力。对于自然头部运动，提出了一种新颖的可学习头部姿势codebook，并采用两阶段训练机制。在第二阶段，提出了一种双分支运动-VAE 和生成器，将网格转换为密集运动并逐帧合成高质量视频。<br>（4）广泛的实验表明，所提出的 VividTalk 可以生成具有唇形同步和逼真头部姿势的高视觉质量说话头视频，并且在客观和主观比较中优于以往的最新作品。</p><p><strong>要点</strong></p><ul><li>VividTalk 采用双阶段通用框架，可以生成高质量视觉效果的说话人头部视频。</li><li>VividTalk 在第一阶段通过学习非刚性表情运动和刚性头部运动，将音频映射到网格。</li><li>VividTalk 在第二阶段使用双分支运动-VAE 和生成器将网格转换为密集运动并逐帧合成高质量视频。</li><li>广泛的实验表明，与目前最先进的作品相比，VividTalk 可以生成高质量视觉效果的说话人头部视频，并将唇形同步和逼真的增强效果提高很大幅度。</li></ul><p><img src="https://picx.zhimg.com/80/v2-8521b04f82075cc27b5e95148dba9792.png" alt="VividTalk can generate realistic and lip-sync talking head videos with expressive facial expression, natural head poses."></p><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p> <strong>音频驱动人脸生成</strong></p><p>主要是利用音频驱动人脸，生成相匹配的图像，最近的一些工作如SadTalker，是用3DMM作为中间表示，再使用3DMM渲染得到对应的视频；也有利用人脸面部关键点的，这都是比较类似的。同时加入生成mask的嘴唇部份，但是由于中间的表示限制，所有这些方法都不足以生成口型同步和逼真的头部说话视频。</p><p>这个VIvidTalker是使用blendshape和vertex来作为中间表示，分别对粗粒度和细粒度进行建模。</p><p><strong>视频驱动人脸生成</strong></p><p>视频驱动可以认为是表情迁移，也就是将参考视频的动作迁移到目标人脸上，比如FOMM这样的方式，用无监督的关键点作为中间的表示，以及有利用depth深度作为信息的。</p><h2 id="主要方法"><a href="#主要方法" class="headerlink" title="主要方法"></a>主要方法</h2><p>VividTalk主要的框架由两个级联阶段组成，分别是</p><ul><li><strong>Audio-To-Mesh</strong> 音频到网格生成</li><li><strong>Mesh-To-VIdeo</strong> 网格到视频生成</li></ul><p><img src="https://pic1.zhimg.com/v2-35ebd6e4eb48d485c2f77af937e3a762.png" alt="主要方法"></p><h3 id="前馈知识"><a href="#前馈知识" class="headerlink" title="前馈知识"></a>前馈知识</h3><h4 id="3DMM"><a href="#3DMM" class="headerlink" title="3DMM"></a>3DMM</h4><p>3D Morphable Model（3DMM）是一种用于建模和分析人脸形状和外观的计算机图形技术。它是基于数学模型的方法，用于描述和生成人脸的<strong>三维几何形状和表面纹理</strong>。3DMM的基本原理是利用统计学方法从大量的三维人脸数据中学习人脸形状和纹理的变化规律，并将这些信息编码到一个数学模型中。</p><p>这个模型包括两个主要的部分：形状模型和纹理模型。</p><ol><li><strong>形状模型</strong>：形状模型描述了人脸的几何形状的变化。通常采用的方法是使用主成分分析（PCA）对人脸的形状数据进行降维和建模。通过分析大量的人脸形状数据，可以得到一组主成分，它们描述了人脸形状变化的主要模式。形状模型可以用来生成新的人脸形状，或者对现有的人脸形状进行编辑和变形。</li><li><strong>纹理模型</strong>：纹理模型描述了人脸表面的颜色和纹理的变化。与形状模型类似，纹理模型也可以利用主成分分析等方法来建模人脸的表面纹理。通过分析大量的人脸纹理数据，可以得到一组主成分，它们描述了人脸表面颜色和纹理的变化模式。纹理模型可以用来生成新的人脸纹理，或者对现有的人脸纹理进行编辑和变换。</li></ol><p><img src="https://pic1.zhimg.com/v2-efd80426cbb18b4f2ee91789c07277eb.png" alt="3DMM"></p><h4 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h4><p>除此之外，首先还需要对数据集进行预处理，使用FOMM对方式对视听数据集进行预处理，并且裁剪面部区域为256x256。同时也是用FaceVerse来提取表情系数和网格顶点序列。</p><h3 id="Audio-To-Mesh"><a href="#Audio-To-Mesh" class="headerlink" title="Audio-To-Mesh"></a>Audio-To-Mesh</h3><p>在数据预处理的时候，使用Faceverse重建我们的参考图像，从音频中学习非刚性面部表情运动和刚性头部运动来驱动重建的网格。为此，提出了一个多分支的Blendshape和Vertex偏移生成器以及学习头部姿势的codebook，具体如下图所示。</p><p><img src="https://picx.zhimg.com/v2-1648982e559021c0b5f5eaa6b201ef93.png" alt="Audio-To-Mesh"></p><p><strong>BlendShape and Vertex Offset Generator</strong></p><p>对于BlendShape and Vertex Offset Generator来说，首先会使用一个预训练的音频模型来提取音频特征，然后从参考图像中提取身份信息$\alpha$，并且编码为风格信息$z_{style}$，然后在音频特征中嵌入个人风格信息，再结合送到基于多分支的Transformer架构中，一共有三个分支，两个分支生成粗粒度的blendshape，第三个分支生成细粒度的与嘴唇相关的vertex偏移对嘴唇运动进行补充。</p><script type="math/tex; mode=display">\hat{\beta}_i^f=\Phi_i^{bs}(\hat{\beta}_i^{1...f-1},A,z^{style}),\quad i\in\{lip,other\}, \\\hat{O}_{lip}^f=\Phi_{lip}^{\upsilon o}(\hat{O}_{lip}^{1...f-1},A,z^{style}),</script><p>训练完成后，就可以通过以下方式来进行驱动</p><script type="math/tex; mode=display">\hat{M}_{nr}=(\overline{S}+\alpha U_{id}+(\hat{\beta}_{lip},\hat{\beta}_{other})U_{exp}+\hat{O}_{lip})\otimes P_{ref}.</script><p>这里面的$P_{ref}$为参考图像的<strong>head pose</strong>，$\otimes$是对应的仿射变化。</p><p><img src="https://picx.zhimg.com/v2-15f9efd01582593cfaf9a3a5bd765dac.png" alt="BlendShape and Vertex Offset Generator"></p><p><strong>Learnable Head Pose Codebook</strong></p><p>头部姿势是非常重要的一环，直接从音频中学习还是比较困难的，因为这里面的关系是比较微弱的，因此，使用离散的codebook的，将生成的问题转化为在离散和且有限的姿势空间中查询codebook的任务，设计了两阶段的训练机制。</p><p>第一阶段是重建阶段，利用VQ-VAE来构建丰富的头部姿势codebook，是一个编码解码结构。</p><script type="math/tex; mode=display">Z_q=\mathbf{q}(\hat{z})=\underset{z_k\in\mathcal{Z}}{\operatorname*{\arg\min}}\left\|\hat{z}-z_k\right\|. \\\hat{P}_r^{1:f}=\mathcal{D}(Z_q)=\mathcal{D}(\mathbf{q}(\mathcal{E}(P_r^{1:f}))).</script><p>第二阶段是映射阶段，将输入音频映射到codebook生成最终结果，具体来说，$\Phi_{map}$以音频序列A、特定于人的风格嵌入$z^{style}$和初始头部姿势$P^0$ 作为输入，输出中间特征$\hat Z$，该中间特征将从codebook$Z$量化为$Z_q$，然后由预训练的解码器$D$解码</p><script type="math/tex; mode=display">\hat{P}_r^{1:f}=\mathcal{D}(Z_q)=\mathcal{D}(\mathbf{q}(\Phi_{map}(A,s,P^0))).</script><p>从目前为止，非刚性面部表情运动和刚性头部姿势都已学习。现在我们就可以运用学习到的刚性头部姿势应用于Mesh $\hat{M}<em>{nr}$来获得最最终的驱动网格Mesh $\hat{M}</em>{d}$。</p><p><img src="https://pic1.zhimg.com/v2-d9f01fd2be86dc73e859cc5df7c2f7d9.png" alt="Learnable Head Pose Codebook"></p><h3 id="Mesh-To-Video"><a href="#Mesh-To-Video" class="headerlink" title="Mesh-To-Video"></a>Mesh-To-Video</h3><p>这一部份是为了将驱动的Mesh转成视频，提出了一个双分支的Motion-VAE对这些2D密集运动进行建模，最后合成最终的视频。</p><p>如果要建模2D与3D之间的关系比较难，为了更好的学习，使用投影纹理表示来实现2D的转换。</p><p>并且为了更好的学习3D Mesh的纹理，首先在x,y,z三个轴的进行归一化的处理，归一化到0，得到纹理的新表示NCC：</p><script type="math/tex; mode=display">NCC_i=\frac{\overline{S}_i-min(\overline{S}_i)}{max(\overline{S}_i)-min(\overline{S}_i)},\quad i\in\{x,y,z\}.</script><p>然后，使用了Z-Buffer方式和NCC的颜色去渲染3D面度的纹理$PT<em>{in}$，由于3DMM的限制，外表的区域是无法被建模的，所以使用Deep Learning Face Attributes in the Wild 方法解析图像并获得外部面部区域纹理$PT</em>{out}$，例如躯干和背景，将其与$PT_{in}$ 组合如下：</p><script type="math/tex; mode=display">PT=PT_{in}\cdot M+PT_{out}\cdot(1-M)</script><p>其中$M$是内部人脸的Mask，为了进一步增强嘴唇运动并更准确地建模，我们还选择与嘴唇相关的标志并将其转换为高斯图，这是一种更紧凑、更有效的表示。然后，Hourglass网络将减去的高斯图作为输入并输出 2D 嘴唇运动，该运动将与面部运动连接并解码为密集运动和遮挡图。</p><p>最后，根据之前预测的密集运动图对参考图像进行变形，获得变形图像，该变形图像将与遮挡图一起作为生成器的输入，逐帧合成最终视频。</p><p><img src="https://picx.zhimg.com/v2-bd37230a1f7ac7c875a8b5555d5b43dd.png" alt="Mesh-To-Video"></p><h3 id="训练策略"><a href="#训练策略" class="headerlink" title="训练策略"></a>训练策略</h3><p>这几部分实际上都是分开训练的，不过训练后可以通过端到端的方式生成结果。</p><p><strong>BlendShape and Vertex Offset Generator</strong>由Blendshape和Mesh重建损失来进行监督</p><script type="math/tex; mode=display">L_{bsvo}=\left\|\beta-\hat{\beta}\right\|+\left\|M-\hat{M}_{nr}\right\|.</script><p><strong>Learnable Head Pose Codebook</strong>部分中，由于量化函数是不可微分的，所以使用straight-through gradient estimator将梯度从解码器复制到编码器，然后对两阶段训练进行如下监督：</p><script type="math/tex; mode=display">\begin{aligned}L_{rec}= =\left\|P_r^{1:f}-\hat{P}_r^{1:f}\right\|^2+\left\|sg(\mathcal{E}(P_r^{1:f}))-z_q\right\|_2^2  \\+\left\|sg(z_q)-\mathcal{E}(P_r^{1:f})\right\|_2^2, \\L_{map} =\left\|P_r^{1:f}-\hat{P}_r^{1:f}\right\|^2+\left\|\hat{Z}-sg(Z_q)\right\|_2^2, \end{aligned}</script><p>sg表示停止梯度操作，也就是 <strong>stop gradient</strong></p><p><strong>Mesh-To-Video</strong>阶段中，基于预训练的VGG-19 网络的感知损失$L<em>{perc}$被用作主要驱动损失。特征匹配损失 $L</em>{fm}$还用于稳定训练产生更真实的结果。</p><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>接下来总结一下实验的结果方法的对比，该模型使用了HDTF和VoxCeleb数据集，使用Adam优化器，在两个阶段中学习率分别为1e-4和1e-5，最后用8个V100训练了2天得到最终的结果。</p><div class="table-container"><table><thead><tr><th>方法</th><th>优点</th><th>缺点</th></tr></thead><tbody><tr><td>SadTalker</td><td>无法生成精确的细节唇部动作</td><td>视频质量不佳</td></tr><tr><td>TalkLip</td><td>生成模糊结果，皮肤色调稍微偏黄，失去了一定程度的身份信息</td><td>质量较差</td></tr><tr><td>MakeItTalk</td><td>在交叉身份配音设置中不能生成准确的嘴部形状</td><td>嘴部形状不准确</td></tr><tr><td>Wav2Lip</td><td>容易合成模糊的口部区域，单一参考图像时输出视频头部姿势和眼部运动静止</td><td>视频输出质量较低</td></tr><tr><td>PC-AVS</td><td>需要一个驱动视频作为输入，身份保存困难</td><td>身份保存困难</td></tr><tr><td>VividTalker</td><td>可以生成高质量的说话头像视频，具有准确的唇同步和丰富的面部运动</td><td>视频质量高，唇同步准确，面部运动丰富</td></tr></tbody></table></div><p><img src="https://pic1.zhimg.com/v2-66838829a274884142dde5ee251e190c.png" alt="实验结果"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>VividTalk框架的优点包括：</p><ol><li><p><strong>高质量的生成视频</strong>：VividTalk能够生成高质量的说话头像视频，具有清晰的面部表情和自然的头部姿势，为用户提供更具沉浸感和真实感的体验。</p></li><li><p><strong>丰富的表达能力</strong>：通过将混合形状和顶点映射为中间表示，VividTalk能够最大化模型的表达能力，从而呈现出丰富的面部表情，包括细微的细节运动。</p></li><li><p><strong>灵活的模型设计</strong>：采用多分支生成器，VividTalk能够灵活地对全局和局部面部运动进行建模，使得生成的视频更加生动和自然。</p></li><li><p><strong>自然的头部姿势合成</strong>：通过引入新颖的可学习的头部姿势码本和两阶段训练机制，VividTalk能够合成更加自然的头部姿势，使得生成的视频更加逼真。</p></li><li><p><strong>创新的双分支机制</strong>：利用双分支运动-VAE和生成器，VividTalk能够有效地转化驱动网格为密集运动，并用于合成最终视频，提高了生成视频的质量和真实感。</p></li><li><p><strong>超越性能</strong>：实验证明，VividTalk优于以往的最先进方法，为数字人类创建、视频会议等应用开辟了新的可能性。</p></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;VividTalk-One-Shot-Audio-Driven-Talking-Head-Generation-Based-on-3D-Hybrid-Prior&quot;&gt;&lt;a href=&quot;#VividTalk-One-Shot-Audio-Driven-Talking-</summary>
      
    
    
    
    <category term="Paperscape" scheme="https://kedreamix.github.io/categories/Paperscape/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>EMO Emote Portrait Alive - 阿里HumanAIGC</title>
    <link href="https://kedreamix.github.io/Paperscape/EMO/"/>
    <id>https://kedreamix.github.io/Paperscape/EMO/</id>
    <published>2024-03-03T13:20:00.000Z</published>
    <updated>2024-03-07T08:03:21.028Z</updated>
    
    <content type="html"><![CDATA[<h1 id="EMO-Emote-Portrait-Alive-阿里HumanAIGC"><a href="#EMO-Emote-Portrait-Alive-阿里HumanAIGC" class="headerlink" title="EMO: Emote Portrait Alive - 阿里HumanAIGC"></a>EMO: Emote Portrait Alive - 阿里HumanAIGC</h1><p>最近这一个星期，也就是2月28日的时候，阿里巴巴的HumanAIGC团队发布了一款全新的生成式AI模型EMO（Emote Portrait Alive）。EMO仅需一张人物肖像照片和音频，就可以让照片中的人物按照音频内容“张嘴”唱歌、说话，且口型基本一致，面部表情和头部姿态非常自然，发布的视频效果非常好，好的几乎难以置信，特别是蔡徐坤唱rap的第一段，效果非常好。</p><p><strong>EMO不仅能够生成唱歌和说话的视频，还能在保持角色身份稳定性的同时，根据输入音频的长度生成不同时长的视频。</strong></p><p>所以我就想借此机会，学习一下EMO的大概框架，剖析一下里面的一些技术要点，首先给出论文的链接和代码链接，不过HumanAIGC已经很久没有开源代码了，不过技术方向还是值得一看的。</p><p>论文：<a href="https://arxiv.org/abs/2402.17485v1">EMO: Emote Portrait Alive - Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions</a></p><p>项目：<a href="https://humanaigc.github.io/emote-portrait-alive/">https://humanaigc.github.io/emote-portrait-alive/</a></p><p>我也一直有关注这一部分的技术，大家也可以关注我的数字人知识库<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis">https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis</a></p><h2 id="Diffusion相关"><a href="#Diffusion相关" class="headerlink" title="Diffusion相关"></a>Diffusion相关</h2><p>在之前的一些研究中，有过用Diffusion做Talking head generation的，比如Diffusion head和CVPR2023的DiffTalk等论文，这些论文都是用Diffusion得强大生成能力来完成音频驱动的人脸生成。</p><p>这里逐帧生成与音频对应的人脸的图像，mask人脸中嘴唇的部分，然后逐步生成视频，<strong>这个过程相当于，AI先看一下照片，然后打开声音，再随着声音一张一张地画出视频中每一帧变化的图像。</strong></p><p><img src="https://picx.zhimg.com/v2-24c8ad5651ce25627b3e8bfff24d85b1.png" alt="DiffTalk"></p><p>如果我们看Diffusion Head论文，也是类似的做法，都是通过Diffusion的强大能力完成视频的生成。</p><p><img src="https://pica.zhimg.com/v2-3e6497aae4c003eb72bb3f24224c89ee.png" alt="Overview"></p><h2 id="EMO整体框架"><a href="#EMO整体框架" class="headerlink" title="EMO整体框架"></a>EMO整体框架</h2><p>接下来开始剖析一下EMO的框架，与DiffTalk和Diffusion Heads类似，都是利用Diffusion来生成，也是根据一个参考图像来逐帧生成图片最后得到视频。</p><p><img src="https://pica.zhimg.com/v2-24facf74c8152c3d19d0e57fce19c9b2.png" alt="EMO"></p><p>不同的是，EMO的工作过程分为两个主要阶段：</p><ol><li>首先，利用参考网络（ReferenceNet）从参考图像和动作帧中提取特征；</li><li>然后，利用预训练的音频编码器处理声音并嵌入，再结合多帧噪声和面部区域掩码来生成视频。</li></ol><p>该框架还融合了两种注意机制和时间模块，以确保视频中角色身份的一致性和动作的自然流畅。我觉得实际上这里是最重要的一部分，这一部分也是和之前Diffusion方法不同的点，其实这一部份又和HumanAIGC之前做的科目三驱动的方式很像，也就是那篇AnimateAnyone论文，这一部分也是火🔥了很久，现在也有人复现了该方法，不过还没有开源。</p><p>根据EMO的论文与项目的展现的结果，EMO不仅仅能产生非常Amazing的对口型视频，还能生成各种风格的歌唱视频，无论是在表现力还是真实感方面都显著优于现有的先进方法，如DreamTalk、Wav2Lip和SadTalker。</p><p><img src="https://picx.zhimg.com/v2-6492e24fb03ffa98135dc584535ab7d9.png" alt="EMO整体框架"></p><h2 id="EMO工作原理"><a href="#EMO工作原理" class="headerlink" title="EMO工作原理"></a>EMO工作原理</h2><p>从EMO的框架可以看到，利用骨干网络获取多帧噪声潜在输入，并尝试在每个时间步将它们去噪到连续的视频帧，这个骨干网络是类似于SD 1.5的UNet的结构配置。与之前的SD1.5不同的是，本身的SD是使用文本嵌入的，而现在是使用参考特征。</p><ol><li>与之前的工作类似，为了确保生成的帧之间的连续性，骨干网络嵌入了时间模块。 </li><li>为了保持生成帧中肖像的ID一致性，使用了一个与Backbone并行的称为ReferenceNet的UNet结构，它输入参考图像以获得参考特征。 </li><li>为了驱动角色说话动作，利用音频层对语音特征进行编码。 </li><li>为了使说话角色的运动可控且稳定，我们使用面部定位器和速度层来提供弱条件。</li></ol><p><strong>预训练音频编码器：</strong>EMO使用预训练的音频编码器（如wav2vec）来处理输入音频。这些编码器提取音频特征，这些特征随后用于驱动视频中的角色动作，包括口型和面部表情。这里面还是使用附加特征m来解决动作可能会受到未来/过去音频片段的影响，例如说话前张嘴和吸气。</p><p><strong>参考网络（ReferenceNet）：</strong>该网络从单个参考图像中提取特征，这些特征在视频生成过程中用于保持角色的身份一致性。ReferenceNet与生成网络（Backbone Network）并行工作，输入参考图像以获取参考特征。</p><p><strong>骨干网络（Backbone Network）：</strong>Backbone Network接收多帧噪声（来自参考图像和音频特征的结合）并尝试将其去噪为连续的视频帧。这个网络采用了类似于Stable Diffusion的UNet结构，其中包含了用于维持生成帧之间连续性的时间模块。 </p><p><strong>注意力机制：</strong>EMO利用两种形式的注意力机制——<strong>参考注意力（Reference-Attention）和音频注意力（Audio-Attention）</strong>。参考注意力用于保持角色身份的一致性，而音频注意力则用于调整角色的动作，使之与音频信号相匹配。 </p><p><strong>时间模块：</strong>这些模块用于操纵时间维度并调整动作速度，以生成流畅且连贯的视频序列。时间模块通过自注意力层跨帧捕获动态内容，有效地在不同的视频片段之间维持一致性。</p><p><strong>训练策略：</strong>EMO的训练分为三个阶段：图像预训练、视频训练和速度层训练。在图像预训练阶段，Backbone Network和ReferenceNet在单帧上进行训练，而在视频训练阶段，引入时间模块和音频层，处理连续帧。速度层的训练在最后阶段进行，以细化角色头部的移动速度和频率。</p><p><strong>去噪过程：</strong>在生成过程中，Backbone Network尝试去除多帧噪声，生成连续的视频帧。去噪过程中，参考特征和音频特征被结合使用，以生成高度真实和表情丰富的视频内容。</p><p>EMO模型通过这种结合使用参考图像、音频信号、和时间信息的方法，能够生成与输入音频同步且在表情和头部姿势上富有表现力的肖像视频，超越了传统技术的限制，创造出更加自然和逼真的动画效果。</p><h2 id="EMO训练阶段"><a href="#EMO训练阶段" class="headerlink" title="EMO训练阶段"></a>EMO训练阶段</h2><p>训练分为三个阶段，<strong>图像预训练、视频训练和速度层训练。</strong></p><ul><li><p>在图像预训练阶段，网络以单帧图像为输入进行训练。此阶段，Backbone 将单个帧作为输入，而 ReferenceNet 处理来自同一帧的不同的、随机选择的帧，从原始 SD 初始化权重</p></li><li><p>在视频训练阶段，引入时间模块和音频层，处理连续帧，从视频剪辑中采样n+f个连续帧，开始的n帧是运动帧。时间模块从AnimateDiff初始化权重。</p></li><li><p>速度层训练专注于调整角色头部的移动速度和频率。</p></li></ul><p>这些详细信息提供了对EMO模型训练和其参数配置的深入了解，突显了其在处理广泛和多样化数据集方面的能力，以及其在生成富有表现力和逼真肖像视频方面的先进性能。</p><h2 id="EMO实验设置"><a href="#EMO实验设置" class="headerlink" title="EMO实验设置"></a>EMO实验设置</h2><p>EMO的数据集有两部份，首先HumanAIGC团队从互联网中收集了 <strong>超过250小时的视频和超过1.5亿张图像</strong>，同时加入了来自互联网和HDTF以及VFHQ数据集作为补充。这里面的数据集多种多样，包括演讲、电影和电视剪辑以及歌唱表演，涵盖了多种语言，如中文和英文，这也是为什么最后能表现出如此好效果的原因。</p><p>在第一阶段的时候，使用VFHQ数据集，因为它不包含音频。然后再对视频进行预处理，所有的视频可通过MediaPipe来获取人脸检测框区域，并且裁剪到512×512的分辨率。</p><p>在第一训练阶段，批处理大小BatchSize设置为48。在第二和第三训练阶段，生成视频长度设置为f=12，运动帧数设置为n=4，训练的批处理大小为4，学习率在所有阶段均设置为1e-5。</p><p>在推理时，使用DDIM的采样算法生成视频。时间步大约是40步，为每一帧生成指定一个恒定的速度值，最后方法的结果生成一批（f=12帧）的时间大约为15秒。 </p><p>一般视频的长度为25～30帧左右，如果我们认为是1mins的视频，也就是60s的视频，那就是60*25=1500，1500/15 = 100s，也就是大概需要1mins40s能生成一分钟的视频，速度也得到了不错的改进，虽然没有实时，但是结果已经很好了。</p><h2 id="EMO特点"><a href="#EMO特点" class="headerlink" title="EMO特点"></a>EMO特点</h2><p>EMO模型有如下特点：</p><p><strong>直接音频到视频合成：</strong>EMO采用直接从音频合成视频的方法，无需中间的3D模型或面部标志，简化了生成过程，同时保持了高度的表现力和自然性。</p><p><strong>无缝帧过渡与身份保持：</strong>该方法确保视频帧之间的无缝过渡和视频中身份的一致性，生成的动画既生动又逼真。</p><p><strong>表达力与真实性：</strong>实验结果显示，EMO不仅能生成令人信服的说话视频，而且还能生成各种风格的歌唱视频，其表现力和真实性显著超过现有的先进方法。</p><p><strong>灵活的视频时长生成：</strong>EMO可以根据输入音频的长度生成任意时长的视频，提供了极大的灵活性。</p><p><strong>面向表情的视频生成：</strong>EMO专注于通过音频提示生成表情丰富的肖像视频，特别是在处理说话和唱歌场景时，可以捕捉到复杂的面部表情和头部姿态变化。</p><p>这些特点共同构成了EMO模型的核心竞争力，使其在动态肖像视频生成领域表现出色。</p><h2 id="EMO缺陷"><a href="#EMO缺陷" class="headerlink" title="EMO缺陷"></a>EMO缺陷</h2><p>对于EMO来说，也会有一些限制。</p><ul><li><p>首先，与不依赖扩散模型的方法相比，它更耗时。</p></li><li><p>其次，由于不使用任何明确的控制信号来控制角色的运动，因此可能会导致无意中生成其他身体部位（例如手），从而导致视频中出现伪影。</p></li></ul><p>所以这样的一个问题，如果要解决的话，可以考虑用专门控制身体部位的控制信号，这样就会较好的解决这个方法，每一个信号控制一部分，就不会生成错误。</p><p>参考</p><ul><li><a href="https://m.huxiu.com/article/2728417.html">https://m.huxiu.com/article/2728417.html</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;EMO-Emote-Portrait-Alive-阿里HumanAIGC&quot;&gt;&lt;a href=&quot;#EMO-Emote-Portrait-Alive-阿里HumanAIGC&quot; class=&quot;headerlink&quot; title=&quot;EMO: Emote Portrait </summary>
      
    
    
    
    <category term="Paperscape" scheme="https://kedreamix.github.io/categories/Paperscape/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>Failed building wheel for PyAudio  解决方法</title>
    <link href="https://kedreamix.github.io/Note/pyaudio/"/>
    <id>https://kedreamix.github.io/Note/pyaudio/</id>
    <published>2024-01-28T06:42:50.000Z</published>
    <updated>2024-01-28T06:46:11.408Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Failed-building-wheel-for-PyAudio-解决方法"><a href="#Failed-building-wheel-for-PyAudio-解决方法" class="headerlink" title="Failed building wheel for PyAudio  解决方法"></a>Failed building wheel for PyAudio  解决方法</h2><p>有时候在安装pyaudio的时候，总是有时候遇见一些错误，如下</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">  Building wheel <span class="keyword">for</span> pyaudio (pyproject.toml) ... error</span><br><span class="line">  error: subprocess-exited-with-error</span><br><span class="line"></span><br><span class="line">  × Building wheel <span class="keyword">for</span> pyaudio (pyproject.toml) did not run successfully.</span><br><span class="line">  │ <span class="built_in">exit</span> code: 1</span><br><span class="line">  ╰─&gt; [18 lines of output]</span><br><span class="line">      running bdist_wheel</span><br><span class="line">      running build</span><br><span class="line">      running build_py</span><br><span class="line">      creating build</span><br><span class="line">      creating build/lib.linux-x86_64-cpython-310</span><br><span class="line">      creating build/lib.linux-x86_64-cpython-310/pyaudio</span><br><span class="line">      copying src/pyaudio/__init__.py -&gt; build/lib.linux-x86_64-cpython-310/pyaudio</span><br><span class="line">      running build_ext</span><br><span class="line">      building <span class="string">'pyaudio._portaudio'</span> extension</span><br><span class="line">      creating build/temp.linux-x86_64-cpython-310</span><br><span class="line">      creating build/temp.linux-x86_64-cpython-310/src</span><br><span class="line">      creating build/temp.linux-x86_64-cpython-310/src/pyaudio</span><br><span class="line">      gcc -pthread -B anaconda3/envs/ernerf/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem anaconda3/envs/ernerf/include -fPIC -O2 -isystem anaconda3/envs/ernerf/include -fPIC -I/usr/local/include -I/usr/include -Ianaconda3/envs/ernerf/include/python3.10 -c src/pyaudio/device_api.c -o build/temp.linux-x86_64-cpython-310/src/pyaudio/device_api.o</span><br><span class="line">      src/pyaudio/device_api.c:9:10: fatal error: portaudio.h: No such file or directory</span><br><span class="line">          9 | <span class="comment">#include "portaudio.h"</span></span><br><span class="line">            |          ^~~~~~~~~~~~~</span><br><span class="line">      compilation terminated.</span><br><span class="line">      error: <span class="built_in">command</span> <span class="string">'/usr/bin/gcc'</span> failed with <span class="built_in">exit</span> code 1</span><br><span class="line">      [end of output]</span><br><span class="line"></span><br><span class="line">  note: This error originates from a subprocess, and is likely not a problem with pip.</span><br><span class="line">  ERROR: Failed building wheel <span class="keyword">for</span> pyaudio</span><br><span class="line">Successfully built python_speech_features</span><br><span class="line">Failed to build pyaudio</span><br><span class="line">ERROR: Could not build wheels <span class="keyword">for</span> pyaudio, <span class="built_in">which</span> is required to install pyproject.toml-based projects</span><br></pre></td></tr></tbody></table></figure><p>如果单纯查后面这一句，会发现找不到什么错误，最后我找到了对应的解决办法，实际上是linux有一些库没安装上，用root权限装一下即可</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 有些人说这样即可</span></span><br><span class="line">sudo apt-get install portaudio19-dev</span><br><span class="line"><span class="comment"># 如果不行就试一下这样</span></span><br><span class="line">sudo apt-get install libasound-dev portaudio19-dev libportaudio2 libportaudiocpp0</span><br></pre></td></tr></tbody></table></figure><p>这样安装完以后，我们就可以正常安装pyaudio了</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pyaudio</span><br></pre></td></tr></tbody></table></figure><p>我也在github上看到的相关帖子，大家也可以参考：<a href="https://github.com/ardha27/AI-Waifu-Vtuber/issues/49">https://github.com/ardha27/AI-Waifu-Vtuber/issues/49</a>，而且这里面有个windows的解决方法，还蛮有趣，我还没试过</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip install pipwin</span><br><span class="line">pipwin install pyaudio</span><br><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></tbody></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Failed-building-wheel-for-PyAudio-解决方法&quot;&gt;&lt;a href=&quot;#Failed-building-wheel-for-PyAudio-解决方法&quot; class=&quot;headerlink&quot; title=&quot;Failed building </summary>
      
    
    
    
    <category term="Note" scheme="https://kedreamix.github.io/categories/Note/"/>
    
    
    <category term="Linux" scheme="https://kedreamix.github.io/tags/Linux/"/>
    
    <category term="Note" scheme="https://kedreamix.github.io/tags/Note/"/>
    
  </entry>
  
  <entry>
    <title>3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</title>
    <link href="https://kedreamix.github.io/Paper/3DGS%20Survey/"/>
    <id>https://kedreamix.github.io/Paper/3DGS%20Survey/</id>
    <published>2024-01-25T09:24:11.000Z</published>
    <updated>2024-01-30T05:04:13.228Z</updated>
    
    <content type="html"><![CDATA[<p>今天想介绍的是<code>ZJU</code>带来的<code>3DGS</code>的首篇综述<code>A Survey on 3D Gaussian Splatting</code> 这是论文链接 <a href="https://arxiv.org/abs/2401.03890">arXiv:2401.03890</a>，结合一些资料，趁这个机会好好学习一下3DGS，加油入坑！！！</p><p>首先说一些自己的理解，3DGS之所以爆火，很大程度在于他的实时性，而这一部分极大程度得益于他定制的算法与自定义 CUDA 内核。除此之外，<strong>Gaussian Splatting</strong>根本不涉及任何神经网络，甚至没有一个小型的 MLP，也没有什么 “神经”的东西，场景本质上只是空间中的一组点。在大家都在研究数十亿个参数组成的模型的人工智能世界里，这种方法越来越受欢迎，令人耳目一新。它的想法源于 “Surface splatting”（2001 年），说明经典的计算机视觉方法仍然可以激发相关的解决方案。它简单明了的表述方式使<strong>Gaussian Splatting</strong>特别容易解释，这也是为什么在某些应用中选择它而不是 NeRFs。</p><h2 id="引言-INTRODUCTION"><a href="#引言-INTRODUCTION" class="headerlink" title="引言 INTRODUCTION"></a>引言 INTRODUCTION</h2><p>NeRF自从2020年开始，在多视角合成中做出来巨大的贡献，他利用神经网络，实现了空间坐标到颜色和密度的映射的，然NeRF的方法是计算密集型的，通常需要大量的训练时间和大量的渲染资源，特别是高分辨率的输出。</p><p><img src="https://pic1.zhimg.com/80/v2-c828848317a156fc6dd17c9a5310dd03.png" alt="NeRF"></p><p>针对这些问题，3DGS出现了，3DGS 采用显式表示和高度并行的工作流程，有利于更高效的计算和渲染，其创新在于其独特地融合了可微分管道和基于点的渲染技术的优点，通过用可学习的 3D 高斯函数表示场景，保留了连续体积辐射场的理想特性，这对于高质量图像合成至关重要，同时避免了与空白空间渲染相关的计算开销，这是传统 NeRF 方法的常见缺点，而3DGS很好的解决了这个问题，在不影响视觉质量的情况下达到了实时渲染。</p><p>论文中也发现，自3DGS出现以来，2023年有很多的论文在arXiv中挂出来，所以基于此也写了这样一个综述，同时促进3DGS领域的进一步研究和创新</p><p><img src="https://picx.zhimg.com/80/v2-167cd8779af5c5550c15156e2b9b52c0.png" alt="The number of papers on 3DGS is increasing every month."></p><p>以下是论文架构的图，论文的大概架构如下所示，可以看到这篇综述撰写的一个逻辑，还是非常好的，接下来，我会顺着这个架构进行解读论文来学习</p><ul><li>第2部分：主要是一些问题描述和相关研究领域的一些简要的背景</li><li>第3部分：介绍3DGS，包括3DGS的多视角的合成和3DGS的优化</li><li>第4部分：3DGS 产生重大影响的各种应用领域和任务，展示了其多功能性</li><li>第5部分：对3DGS进行了一些比较和分析</li><li>第6、7部分：对一些未来的开放性工作进行总结和调查</li></ul><p><img src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="Structure of the overall review."></p><h2 id="背景-BACKGROUND"><a href="#背景-BACKGROUND" class="headerlink" title="背景 BACKGROUND"></a>背景 BACKGROUND</h2><p>背景主要分两部分讲解</p><ul><li>辐射场的概念：隐式和显式</li><li>有关辐射场的场景重建、渲染等领域相关介绍</li></ul><h3 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h3><h4 id="辐射场"><a href="#辐射场" class="headerlink" title="辐射场"></a>辐射场</h4><p>辐射场是实际上是对三维空间中光分布的表示，它捕捉了光与环境中的表面和材质相互作用的方式。从数学上来说，辐射场可被描述为一个函数$L:\mathbb{R}^5\to\mathbb{R}^+$, 其中$L(x,y,z,\theta,\psi)$将点$(x,y,z)$和球坐标下的方向$(\theta,\phi)$映射为非负的辐射值。辐射场有显示表达和隐式表达，可用于场景表示和渲染。</p><h4 id="隐式辐射场"><a href="#隐式辐射场" class="headerlink" title="隐式辐射场"></a>隐式辐射场</h4><p>隐式辐射场是辐射场中的一种，在表示场景中的光分布时，不需显式定义场景的集合形状。这里面最常见的就是NeRF，使用神经网络来学习连续的体积表示。在NeRF中，使用MLP 网络用于将一组空间坐标 $(x, y, z)$ 和观察方向 $(\theta,\phi)$ 映射到颜色和密度值。任何点处的辐射不是显式存储的，而是通过查询神经网络实时计算得出。因此，该函数可以写成：</p><script type="math/tex; mode=display">L_\text{implicit}(x,y,z,\theta,\phi)=\text{NeuralNetwork}(x,y,z,\theta,\phi)</script><p>这种方式的好处是构建了一个可微且紧凑的复杂场景，但是由于我们总是需要对光线进行采样和体渲染的计算，会导致计算负载比较高。</p><h4 id="显式辐射场"><a href="#显式辐射场" class="headerlink" title="显式辐射场"></a>显式辐射场</h4><p>与隐式不同的是，显示是直接表示光在离散空间结构中的分布，比如体素网格或点云。该结构中的每个元素都存储了其在空间中相应位置的辐射信息，而不是像NeRF一样去执行查询的操作，所以他会更直接也更快的得到每个值，但是同时也需要更大内存使用和导致较低的分辨率。通常我们可以表示为：</p><script type="math/tex; mode=display">L_\text{explicit}{ ( x , y , z , \theta , \phi ) }=\text{DataStructure}[(x,y,z)]\cdot f(\theta,\phi)</script><p>其中，<code>DataStructure</code>可以是网格或点云，而$f(θ, ϕ)$是一个根据观察视线方向修改辐射的函数。</p><h4 id="3D-Gaussian-Splatting-（两全其美）"><a href="#3D-Gaussian-Splatting-（两全其美）" class="headerlink" title="3D Gaussian Splatting （两全其美）"></a>3D Gaussian Splatting （两全其美）</h4><p>3DGS通过利用3D 高斯函数作为其表示形式，充分利用了显示辐射场和隐式辐射场的优势。这些高斯函数被优化用于准确表示场景，结合了基于神经网络的优化和显式结构化数据存储的优点。这种混合方法能进行高质量渲染，同时具有更快的训练和实时性能，3D高斯表达可表示为：</p><script type="math/tex; mode=display">L_{\mathrm{3DGS}}(x,y,z,\theta,\phi)=\sum_{i}G(x,y,z,\mu_{i},\Sigma_{i})\cdot c_{i}(\theta,\phi)</script><p>其中 $G$ 是具有平均值 $μ_i$ 和协方差 $Σ_i$ 的高斯函数，$c$ 表示与视图相关的颜色。</p><h4 id="显式与隐式的理解"><a href="#显式与隐式的理解" class="headerlink" title="显式与隐式的理解"></a>显式与隐式的理解</h4><p>这里放一张理解显示隐式图像的图片，我还是觉得相当不错的</p><p><img src="https://pic1.zhimg.com/80/v2-e79d0183806753d34863598e544a0517.jpeg" alt="显式隐式表达"></p><h3 id="背景和术语"><a href="#背景和术语" class="headerlink" title="背景和术语"></a>背景和术语</h3><p>许多技术和研究学科与 <code>3DGS</code> 有着密切的关系，以下各节将对此进行简要介绍。</p><h4 id="场景重建与渲染"><a href="#场景重建与渲染" class="headerlink" title="场景重建与渲染"></a>场景重建与渲染</h4><p><strong>场景重建</strong>：从一组图像集合或其它数据建立场景的三维模型。</p><p><strong>渲染</strong>：将计算机可读取的信息（如场景中的3D物体）转化为图像。<br>早期技术基于光场生成逼真的图像，运动结构（SfM）与多视图立体匹配（MVS）算法通过从图像序列估计3D结构来增强光场。</p><h4 id="神经渲染和辐射场"><a href="#神经渲染和辐射场" class="headerlink" title="神经渲染和辐射场"></a>神经渲染和辐射场</h4><p><strong>神经渲染</strong>：将深度学习与传统图形技术结合生成逼真的图像。早期方法使用CNN估计混合权重或纹理空间解决方案。</p><p><strong>辐射场</strong>：一种函数表达，描述从各方向穿过空间各点的光的量。NeRF使用神经网络建模辐射场。</p><h4 id="体积表示和光线行进"><a href="#体积表示和光线行进" class="headerlink" title="体积表示和光线行进"></a>体积表示和光线行进</h4><p><strong>体积表达</strong>：不仅将物体和场景建模为表面，还将其其建模为充满材料或空白空间的体积。这样可以对如雾、烟或半透明材料进行更精确的渲染。</p><p><strong>光线行进</strong>：是体积表达渲染图像的技术，通过增量跟踪穿过“体”的光线来渲染图像。NeRF引入重要性采样和位置编码增强合成图像的质量，虽然能得到高质量的图像，但这一方法计算量大。</p><h4 id="基于点的渲染"><a href="#基于点的渲染" class="headerlink" title="基于点的渲染"></a>基于点的渲染</h4><p>基于点的渲染是一种使用点而非传统多边形来可视化3D场景的技术。该方法特别适用于渲染复杂、非结构化或稀疏的几何数据。点可以通过添加额外属性，如可学习的神经描述符来进行增强，并且可以高效地进行渲染，但这种方法可能会出现渲染中的空洞或混叠效应等问题。3DGS通过使用各向异性高斯进行更连贯的场景表达。</p><h2 id="用于显式辐射场的3DGS"><a href="#用于显式辐射场的3DGS" class="headerlink" title="用于显式辐射场的3DGS"></a>用于显式辐射场的3DGS</h2><p>3DGS能够实时渲染高分辨率的图像，并且不需要神经网络，是一个突破。</p><p>这一块主要围绕两块进行讲解</p><ul><li>3DGS的前向过程</li><li>3DGS的优化过程</li></ul><h3 id="学习3D高斯函数进行新视角合成"><a href="#学习3D高斯函数进行新视角合成" class="headerlink" title="学习3D高斯函数进行新视角合成"></a>学习3D高斯函数进行新视角合成</h3><p>假如现在有一个场景，目的是生成特定视角下的相机图像。NeRF对每一个像素使用光线行进和采样点，影响其实时性；而3DGS将3D高斯投影到图像平面，称为“泼溅”，如下图所示。然后对高斯进行排序并计算每个像素的值。NeRF和3DGS的渲染可视为互逆关系。</p><p><img src="https://pic1.zhimg.com/80/v2-9d5fff5c2390526cd03e5a14fd13f4fe.png" alt="3DGS的Splatting 泼溅"></p><p>这里面有个点很有意思，为什么说是互逆关系，我参考了知乎的一篇文章<a href="https://zhuanlan.zhihu.com/p/666465701">3D Gaussian Splatting中的数学推导</a>的说明，我觉得这个说的还不错。</p><blockquote><p> 首先，我们回忆一下体渲染的这个事情。假设读者跟我一样是从NeRF才接触体渲染的，那么回顾一下NeRF中，沿着一个像素，发出一条射线，然后这条射线“射向体数据”（在NeRF里就是沿着光线进行采样，然后查询采样点的属性）的过程。这个过程可以归结为一种<code>backward mapping</code>。</p><p> 所以很自然的，会有一种<code>forward mapping</code>的办法。形式上，就是将整个“体数据”投影到此时位姿所对应的图像平面。这种办法的前提就不能是用NeRF那种隐式表达了，需要一些显式的表达才能支持这样直接的投影。例如以三个顶点长成的三角面基元（primitive），然后将这些许多的三角面直接投影到成像平面上，判断哪些像素是什么颜色，当有多个三角形投影时，根据他们的“深度”来判断前后顺序，然后进行熟悉的alpha compositing。当然也会有其他基元，例如小的平面表示等等。</p><p> 无论是<code>backward mapping</code>还是<code>forward mapping</code>，这个过程都涉及到将连续的表示变成离散的。在<code>backward mapping</code>里，是对场进行采样；在<code>forward mapping</code>里，是需要直接生成出基元，这也是一种连续化为离散。为了理解在这个过程中，高斯分布为什么重要，我们需要牵扯到信号与系统中的概念。与混过数字信号处理考试不同的是，我们要清楚此时引入信号与系统里的工具的目的是什么。回想刚才三角面基元的情景，在实际情境中，我们其实都接触不到“连续”的表达，比如三角面，我们只会记录它的三个顶点。当投影完成后，我们只能做一些有限的操作来阻止“锯齿”，例如对结果进行一个模糊操作，这些操作一般都是局部的。我们这样做的目的，本质是“希望用离散的表达来重建原来的信号，进一步在重建好的信号上进行“resampling”。如果我们对处理后的结果，视觉上看起来没什么混叠或者锯齿上的问题，那就说明我们“resampling”是成功的。</p></blockquote><p>从下图也可以看到NeRF和Gaussian在概念上的区别，左边是NeRF沿着光线查询连续 MLP，右边是Gaussian一组与给定光线相关的离散的高斯分布</p><p><img src="https://picx.zhimg.com/80/v2-08473faff1a084b3de92e2a86f69f0fd.png" alt=""></p><p><img src="https://picx.zhimg.com/80/v2-37166011e5e81d299598141028acff42.png" alt="difference between NeRF and Gaussian Splatting"></p><p>首先简单介绍一下，3DGS是如何表示真实场景的，前面也有提过，在<strong>Gaussian Splatting</strong>中，3D世界用一组3D点表示，实际上是数百万个，大致在0.5到5百万之间。每个点是一个3D高斯，具有其独特的参数，这些参数是为每个场景拟合的，以便该场景的渲染与已知数据集图像紧密匹配，接下来就介绍他的属性。</p><p><img src="https://pica.zhimg.com/80/v2-f440b37ac00a08977b2b6e5514ffec1f.png" alt="Representing a 3D world"></p><ul><li><p><strong>3D高斯的属性</strong>： 一个3D高斯主要包括，中心（位置）$x,y,z$的均值$μ$、不透明度 $α$、3D 协方差矩阵 $Σ$ 和颜色 $c$（一般是RGB或者是球谐（SH）系数）。 其中$c$与视角有关，$c$ 由球谐函数表示。所有属性均可学习，都可以通过反向传播来学习和优化。</p></li><li><p><strong>视域剔除</strong>：给定特定的相机姿态，该步骤会判断哪些高斯位于相机的视锥外，并在后续步骤中剔除之，以节省计算。</p></li><li><p><strong>Splatting泼溅</strong>：实际上只是3D高斯（椭圆体）投影到2D图像空间（椭圆）中进行渲染。给定视图变换 $W$ 和3D协方差矩阵$\Sigma$，我们可以使用使用以下公式计算投影 2D 协方差矩阵 $\Sigma^{\prime}$</p><script type="math/tex; mode=display">\Sigma^{\prime}=JW\Sigma W^\top J^\top</script><p>其中 $J$ 为投影变换中仿射近似的雅可比矩阵。</p></li><li><p><strong>像素渲染</strong>：如果不考虑并行，采用最简单的方式：给定像素 $x$ 的位置，与其到所有重叠高斯函数的距离，即这些高斯函数的深度。这些可以通过观察变换 $W$ 计算出来，形成高斯函数的排序列表$N$。然后进行alpha混合，计算该像素的最终颜色：</p><script type="math/tex; mode=display">C=\sum_{i\in\mathcal{N}}c_i\alpha_i^{\prime}\prod_{j=1}^{i-1}\left(1-\alpha_j^{\prime}\right.)</script><p>其中 $c_i$ 是学习到的颜色，最终的不透明度 $\alpha_i^{\prime}$ 是学习的不透明度 $\alpha_i$ 与高斯的乘积:</p><script type="math/tex; mode=display">\alpha_i'=\alpha_i\times\exp\left(-\frac12(x'-\mu_i')^\top\Sigma_i'^{-1}(x'-\mu_i')\right)</script></li></ul><p>  其中 $x’$ 和 $μ’_i$ 是投影空间中的坐标，同时我也找了个gif来可视化了一下Gaussian Splatting对位置p的影响：</p><p>  <img src="/img/3dgs.gif" alt="3DGS"></p><p>  如果仔细看的话，我们会发现，实际上这个公式和<a href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution">多变量正态分布的概率密度函数</a>十分相像，是忽略了带有协方差行列式的标准化项，而是用不透明度来加权。</p><script type="math/tex; mode=display">  (2\pi)^{-k/2}\det(\boldsymbol{\Sigma})^{-1/2}\exp\biggl(-\frac12(\mathbf{x}-\boldsymbol{\mu})^\mathrm{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\biggr)</script><p>  不过如果考虑并行的话加快速度，这种列表排序实际上很难并行化，所以很有可能这个渲染程度比NeRF还慢。为了实现实时渲染，3DGS也做了一个tradeoff，3DGS做出了一些让步来适应<strong>并行计算</strong>。</p><p>  <img src="https://picx.zhimg.com/80/v2-7cea6c4b183982cd921c0456d1f689b7.png" alt="Tiles(Patches)"></p><ul><li><p><strong>Tiles (Patches)</strong>：为避免逐像素计算出现的成本，3DGS改为<strong>patch</strong>级别的渲染。具体来说，首先将图像分割为多个不重叠的patch，称为<code>tile</code>，每个图块包含 16×16 像素，如下图所示。3DGS然后确定<code>tile</code>与投影高斯的相交情况，由于投影高斯可能会与多个<code>tile</code>相交，需要进行复制，并为每个复制体分配相关tile的标识符（如<code>tile</code>的ID）。(不用判断每个像素与高斯的距离，而是判断tile就简单多了)</p><p><img src="https://picx.zhimg.com/80/v2-c81242a6677621910801fcec4c0adbee.png" alt=""></p><p>从下图可以看到排序的结果，在排序中，高位是tile的ID，低位就是深度，一起进行排序，下面的图是AI葵视频的结果，还是很好理解的</p><p><img src="https://pic1.zhimg.com/80/v2-5c74958d484c1d2588c20c8c30b58411.png" alt="3DGS排序"></p><p><img src="https://picx.zhimg.com/80/v2-3d6e3aec3a86c1d94354458830dbf17f.png" alt="3DGS排序例子(AI葵)"></p></li><li><p><strong>并行渲染</strong>：复制后，3DGS（对应字节的无序列表）结合包含了相关的tile ID（对应字节的高位）和深度信息（对应字节的低位），如上图所示。由于每一块和每一像素的计算是独立的，所以可以基于CUDA编程的块和线程来实现并行计算，同时有利于访问公共共享内存并保持统一的读取顺序。排序后的列表可直接用于渲染（alpha混合），如下图所示。</p><p><img src="https://pic1.zhimg.com/80/v2-6393ea51f715d0d0baa880cd1890a549.png" alt="并行渲染"></p><p>总的来说，3DGS在前向过程中做出了一些近似计算，以提高计算效率并保留图像合成的高质量。</p></li></ul><h3 id="3DGS的优化"><a href="#3DGS的优化" class="headerlink" title="3DGS的优化"></a>3DGS的优化</h3><p>学习到这里，我们可能会有一个问题，怎么可能在空间中的一堆圆球中得到一个像样的图像的，确实是这样，如果没有进行优化，在渲染的时候就会出现很多伪影，从下图你可以看到。</p><p><img src="https://pic1.zhimg.com/80/v2-7ad69d962fb9a18d84747130af62fe15.png" alt="An example of renders of an under-optimized scene"></p><p>3DGS的核心是<strong>3D高斯集合的优化过程</strong>。一方面需要通过可微渲染来使高斯符合场景纹理，另一方面表达场景需要的高斯数量是未知的。这分别对应参数优化与密度控制两步，这两步在优化过程中交替进行。优化过程中，需要手动设置很多超参数。</p><h4 id="参数优化-Parameter-Optimization"><a href="#参数优化-Parameter-Optimization" class="headerlink" title="参数优化 Parameter Optimization"></a>参数优化 Parameter Optimization</h4><ul><li><p><strong>损失函数</strong>：图像合成后，计算渲染图像与真实图像的差异作为损失：</p><script type="math/tex; mode=display">\mathcal{L}=(1-\lambda)\mathcal{L}_1+\lambda\mathcal{L}_{D-SSIM}</script><p>其中 $λ$ 是权重因子。与 NeRF 的损失函数略有不同，由于光线行进成本高昂，NeRF 通常在像素级别而不是图像级别进行计算，而3DGS是图像级别的。</p></li><li><p><strong>参数更新</strong>：3D高斯的多数参数可通过反向传播直接更新，但对于协方差矩阵 $\Sigma$来说，需要半正定矩阵（这里面是一个定义，应该是多元正态分布的协方差矩阵是一个半正定矩阵），直接优化可能会产生非半正定矩阵，而只有半正定矩阵才有物理意义。因此，改为优化四元数$q$和3D向量$s$。将协方差矩阵分解：</p><script type="math/tex; mode=display">\Sigma=RSS^\top R^\top</script><p>其中$R$与$S$分别由$q$和$s$推导得到的旋转和缩放矩阵。</p><ul><li>$S$是一个对角缩放矩阵，含有3个参数</li><li>$R$是一个3x3的旋转矩阵，通过旋转四元数来表示</li></ul><p>对于不透明度$α$, 其计算图较为复杂：$(q,s)\to\Sigma\to\Sigma^{\prime}\to\alpha$。为避免自动微分的计算消耗，3DGS还推导了$q$与$s$的梯度，在优化过程中直接计算之。</p></li></ul><h4 id="密度控制-Density-Control"><a href="#密度控制-Density-Control" class="headerlink" title="密度控制 Density Control"></a>密度控制 Density Control</h4><ul><li><strong>初始化</strong>：3DGS建议从SfM产生的稀疏点云初始化或随机初始化高斯，可以直接调用 <a href="https://colmap.github.io/">COLMAP</a> 库来完成这一步。。然后进行点的密集化和剪枝以控制3D高斯的密度。当由于某种原因无法获得点云时，可以使用随机初始化来代替，但可能会降低最终的重建质量。</li></ul><p><img src="https://picx.zhimg.com/80/v2-0d67e5748993593a04ed46f7519e972e_720w.png" alt="A sparse 3D point cloud produced by SfM, means initialization"></p><ul><li><p><strong>点密集化</strong>：在点密集化阶段，3DGS自适应地增加高斯的密度，以更好地捕捉场景的细节。该过程特别关注缺失几何特征或高斯过于分散的区域。密集化在一定数量的迭代后执行，比如100个迭代，针对在视图空间中具有较大位置梯度（即超过特定阈值）的高斯。其包括在未充分重建的区域克隆小高斯或在过度重建的区域分裂大高斯。对于克隆，创建高斯的复制体并朝着位置梯度移动。对于分裂，用两个较小的高斯替换一个大高斯，按照特定因子减小它们的尺度。这一步旨在在3D空间中寻求高斯的最佳分布和表示，增强重建的整体质量。</p><p>这一部分的意义是什么呢，因为SGD只能对现有点进行调整，但是在完全没有点或点太多的区域，很难找到好的参数，所以这就是点密集化的作用。</p></li><li><p><strong>点的剪枝</strong>：点的剪枝阶段移除冗余或影响较小的高斯，可以在某种程度上看作是一种正则化过程。一般消除几乎是透明的高斯（α低于指定阈值）和在世界空间或视图空间中过大的高斯。此外，为防止输入相机附近的高斯密度不合理地增加，这些高斯会在固定次数的迭代后将$\alpha$设置为接近0的值。该步骤在保证高斯的精度和有效性的情况下，能节约计算资源。</p></li></ul><p><img src="https://picx.zhimg.com/80/v2-58c80507588563289c26e2ea4066ad81.png" alt="Adaptive Gaussian densification scheme."></p><h3 id="用SH系数来表示颜色"><a href="#用SH系数来表示颜色" class="headerlink" title="用SH系数来表示颜色"></a>用SH系数来表示颜色</h3><p>在计算机图形学中，用球谐函数（Spherical Harmonics，简称SH）表示视角相关的颜色起着重要作用，最初是在Plenoxels中提出的。他能表示非兰伯特效应，比如金属表面的高光反射。不过这样也不是一定的，实际上也可以使用3个RGB值表示颜色，然后使用Gaussian Splatting。</p><p> 图形学全局环境光照技术与球谐函数息息相关，我们的环境光来源四面八方，可以理解为一个球面函数，当模拟漫反射环境光，我们用一张环境贴图进行采样，对每一个点进行半球采样出在这个像素上的颜色，<strong>球谐光照</strong>简单来说就是用几个系数存取了整张环境贴图包围在球上<strong>法线方向</strong>所对应的的颜色信息。在渲染过程中传入球谐系数。在模型上根据对应的法线信息，从球谐函数中获取对应的颜色信息。</p><p>球谐函数是定义在球面上的特殊函数，换句话说，可以对球面上的任意点计算这样一个函数并得到一个值。</p><p>这里我们简单理解一下，SH，球谐函数，归根到底只是一组基函数，至于这组基函数是怎么来的，不管他。简单点来说，每一个函数都可以由多个基函数组合起来，如果我们有很多基函数，我们可以通过对应的权重系数复原出原来的函数，不过本质上还是一个有损压缩，不一定那么准确，不过如果基函数越多，复原的函数越准确，但是计算量也变大了。</p><p>在球面基函数中，最多的就是球谐函数了。球谐函数有很多很好的性质，比如正交性，旋转不变性（这边就不介绍了）。正交性说明每个基函数都是独立的，每个基函数都不能用别的基函数加权得到。当SH的系数用的越多，那么表达能力就越强，跟原始的函数就越接近。（如果更详细的了解可以看看一些原理，我主要是宏观的了解SH是什么，简单理解就是他是一种颜色的表示）</p><p><img src="https://pic1.zhimg.com/80/v2-9e660f32e92e1897aa986b0ab2ce073e.png" alt=""></p><p>当用来描述不同方向光照的SH基函数，我们一般用二阶或者三阶，比如下面的例子就是3阶的</p><p><img src="https://picx.zhimg.com/80/v2-f6bfb715b846bf13c95013ca96c1d51d.png" alt=""></p><p>下面展示的是一个$l=2$和3阶的球谐函数，一共包括9个学习系数，我们可以根据点的视角得到相关颜色，可以看到最后是red红色分量。</p><p><img src="https://pica.zhimg.com/80/v2-8241e4f7092a89a158df31b8cde94d33.png" alt="得到l=2和9个学习系数的点的视角相关颜色（红色分量）的过程"></p><h3 id="3DGS-流程"><a href="#3DGS-流程" class="headerlink" title="3DGS 流程"></a>3DGS 流程</h3><p>最后根据论文的图来总结一下3DGS的流程</p><p><img src="https://pic1.zhimg.com/80/v2-fda180df51e9171e3e147f5b40e520b9.png" alt="3DGS 流程"></p><ol><li><p><strong>Structure from Motion</strong>：使用SfM从一组图像中估计出点云，可以直接调用 <a href="https://colmap.github.io/">COLMAP</a>  库操作</p><p><img src="https://picx.zhimg.com/80/v2-961548f1a56fb5bc81bc8b349472d8ab.png" alt="Structure from Motion"></p></li></ol><ol><li><p><strong>Convert to Gaussians</strong>：将每个点建模成一个 3D 高斯图像。从 SfM 数据中，我们能推断出每个高斯图像的位置和颜色。但如果是要得到更高质量的表征的话，还需要对每个高斯函数进行训练，以推断出更精细的位置和颜色，并推断出协方差和透明度。</p></li><li><p><strong>Training</strong>：与神经网络类似，我们使用随机梯度下降法进行训练，但这里没有神经网络的层的概念 (都是 3D 高斯函数)。</p><p>训练步骤如下:</p><ol><li>用当前所有可微高斯函数渲染出图像</li><li>根据渲染图像和真实图像之间的差异计算损失</li><li>根据损失调整每个高斯图像的参数</li><li>根据情况对当前相关高斯图像进行点的密度控制</li></ol><p>步骤 1-3 比较简单，下面我们稍微解释一下第 4 步的工作:</p><ul><li>如果某高斯图像的梯度很大 (即它错得比较离谱)，则对其进行分裂或克隆<ul><li>如果该高斯图像很小，则克隆它</li><li>如果该高斯图像很大，则将其分裂</li></ul></li><li>如果该高斯图像的 alpha 太低，则将其删除</li></ul><p>这么做能帮助高斯图像更好地拟合精细的细节，同时修剪掉不必要的高斯图像。</p></li><li><p><strong>Differentiable Gaussian Rasterization</strong>：3D Gaussian Splatting实际上是一种光栅化的方法，将数据成像到屏幕上，与其他方法相比，他有两个特点</p><ol><li>快</li><li>可微</li></ol><p>主要步骤如下：</p><ol><li>针对给定相机视角，把每个 3D 高斯投影到 2D。</li><li>按深度对高斯进行排序。</li><li>对每个像素，从前到后计算每个高斯在该像素点的值，并将所有值混合以得到最终像素值。</li></ol></li></ol><h3 id="3DGS-Limitations"><a href="#3DGS-Limitations" class="headerlink" title="3DGS Limitations"></a>3DGS Limitations</h3><p><strong>优点</strong></p><ol><li>高品质、逼真的场景</li><li>快速、实时的渲染</li><li>更快的训练速度</li></ol><p><strong>缺点</strong></p><ol><li>防止模型优化中的“破碎”的高斯：点太大、太长、冗余等</li><li>更高的显存使用率 (4GB 用于显示，12GB 用于训练)</li><li>更大的磁盘占用 (每场景 1GB+)</li><li>与现有渲染管线不兼容</li><li><del>只能重建静态场景（但是好像现在动态的Gaussian也出来了，所以这个不算缺点了）</del></li></ol><h2 id="应用领域和任务-APPLICATION-AREAS-AND-TASKS"><a href="#应用领域和任务-APPLICATION-AREAS-AND-TASKS" class="headerlink" title="应用领域和任务 APPLICATION AREAS AND TASKS"></a>应用领域和任务 APPLICATION AREAS AND TASKS</h2><h3 id="同时定位和建图（SLAM）"><a href="#同时定位和建图（SLAM）" class="headerlink" title="同时定位和建图（SLAM）"></a>同时定位和建图（SLAM）</h3><p>SLAM需要让设备实时理解自身位置并同时为环境建图，因此计算量大的表达技术难以应用。</p><p>传统SLAM使用点/surfel云或体素网格表达环境。3DGS的优势在于高效性（自适应控制高斯密度）、精确性（各向异性高斯能建模环境细节）、适应性（能用于各种尺度和复杂度的环境）。</p><h3 id="动态场景建模"><a href="#动态场景建模" class="headerlink" title="动态场景建模"></a>动态场景建模</h3><p>动态场景建模需要捕捉和表达场景随时间变化的的3D结构和外观。需要建立能精确反映场景中物体几何、运动和视觉方面的数字模型。4D高斯泼溅通过扩展3D高斯溅射的概念，引入时间维度，使得可以表达和渲染动态场景。现在也有一些方法在研究在动态场景中的一些编辑的功能，与3DGS进行交互。</p><h3 id="AI生成内容（AIGC）"><a href="#AI生成内容（AIGC）" class="headerlink" title="AI生成内容（AIGC）"></a>AI生成内容（AIGC）</h3><p>AIGC是人工智能自动创建或极大修改的数字内容，可以模仿、扩展或增强人类生成的内容。</p><p>3DGS的显式特性、实时渲染能力和可编辑水平使其与AIGC高度相关。例如，有方法使用3DGS与生成模型、化身或场景编辑结合，如3DGS-Avatar。</p><h3 id="自动驾驶"><a href="#自动驾驶" class="headerlink" title="自动驾驶"></a>自动驾驶</h3><p>自动驾驶的目标是在无人干涉的情况下导航并操作车辆，其主要目标是安全而高效地感知环境、做出决策和操作执行器。</p><p>其中，感知和理解环境需要实时重建驾驶场景，精确识别静态和动态物体，并理解其相互关系和运动。动态驾驶场景中，场景还会随时间连续变化。3DGS可以通过混合数据点（如激光雷达点）将场景重建为连贯表达，有利于处理数据点变化的密度，以及静态背景和动态物体的精确重建。</p><h2 id="性能比较-PERFORMANCE-COMPARISON"><a href="#性能比较-PERFORMANCE-COMPARISON" class="headerlink" title="性能比较 PERFORMANCE COMPARISON"></a>性能比较 PERFORMANCE COMPARISON</h2><p>在这一部分，针对3FGS在上述的领域上的一些性能评估。</p><h3 id="性能基准：定位"><a href="#性能基准：定位" class="headerlink" title="性能基准：定位"></a>性能基准：定位</h3><ul><li><p>数据集：Replica。</p></li><li><p>基准算法：Gaussian-SLAM、GS-SLAM、SplaTAM、GSS-SLAM。</p></li><li>评估指标：均方根误差（RMSE）、绝对轨迹误差（ATE），测量传感器运动轨迹上真实位置与估计位置欧式距离的均方根。</li><li>结果：基于3D高斯的SLAM方法能超过基于NeRF的密集视觉SLAM。</li></ul><p><img src="https://picx.zhimg.com/80/v2-3277500ac5a850accdd2891db0595ae6.png" alt=""></p><h3 id="性能基准：静态场景渲染"><a href="#性能基准：静态场景渲染" class="headerlink" title="性能基准：静态场景渲染"></a>性能基准：静态场景渲染</h3><ul><li>数据集：Replica。</li><li>基准算法：Gaussian-SLAM、GS-SLAM、SplaTAM、GSS-SLAM。</li><li>评估指标：峰值信噪比(PSNR)、结构相似性(SSIM)、学习的感知图像patch相似性(LPIPS),衡量RGB渲染性能。</li><li>结果：基于3D高斯的方法能超过基于<strong>NeRF</strong>的方法。</li></ul><p><img src="https://picx.zhimg.com/80/v2-94d0eea6ab0c03f32c82802f00a8102d.png" alt=""></p><h3 id="性能基准：动态场景渲染"><a href="#性能基准：动态场景渲染" class="headerlink" title="性能基准：动态场景渲染"></a>性能基准：动态场景渲染</h3><ul><li>数据集：D-NeRF。</li><li>基准算法：CoGS、4D-GS、GauFRe、4DGS。</li><li>评估指标：PSNR、SSIM、LPIPS, 用于衡量RGB渲染性能。</li><li>结果：3DGS能大幅超过基于NeRF的SOTA。但静态版本的3DGS对动态场景的重建是失败的。</li></ul><p><img src="https://picx.zhimg.com/80/v2-288ca353912cbbd221a111ee553ab607_720w.png" alt=""></p><h3 id="性能基准：驾驶场景渲染"><a href="#性能基准：驾驶场景渲染" class="headerlink" title="性能基准：驾驶场景渲染"></a>性能基准：驾驶场景渲染</h3><ul><li>数据集：nuScences。</li><li>基准算法：DrivingGaussian。</li><li>评估指标：PSNR、SSIM、LPIPS*（LPIPS× 1000）, 用于衡量RGB渲染性能。</li><li>结果：3DGS方法能大幅超过基于NeRF的方法。</li></ul><p><img src="https://pic1.zhimg.com/80/v2-8b7bb910fbb86e3d1b23fc062260dc5d_720w.png" alt=""></p><h3 id="性能基准：数字虚拟人"><a href="#性能基准：数字虚拟人" class="headerlink" title="性能基准：数字虚拟人"></a>性能基准：数字虚拟人</h3><p>该任务的目标是从给定的多视角视频渲染人体化身模型。</p><ul><li>数据集：ZJU-MoCap。</li><li>基准算法：GART、Human101、HUGS、3DGS-Avatar。</li><li>评估指标：PSNR、SSIM、LPIPS* (LPIPS×1000) ,用于衡量RGB渲染性能$_{9}$</li><li>结果：基于3DGS的方法能在渲染质量和速度上均有优势。</li></ul><p><img src="https://pica.zhimg.com/80/v2-d916a05d315e576e3cef738aa2306226.png" alt=""></p><h2 id="未来研究方向-FUTURE-RESEARCH-DIRECTIONS"><a href="#未来研究方向-FUTURE-RESEARCH-DIRECTIONS" class="headerlink" title="未来研究方向 FUTURE RESEARCH DIRECTIONS"></a>未来研究方向 FUTURE RESEARCH DIRECTIONS</h2><ul><li><strong>数据高效的3DGS解决方案</strong>：从少样本中进行新视图生成和场景重建很重要。目前的方法有探究引入深度信息、密集概率分布、像素到高斯的映射来促进该能力，实际上就是引入更多的信息。。此外，在观测不足的区域，3DGS会产生伪影，可尝试在这些区域进行数据插值或积分。</li><li><strong>存储高效的3DGS解决方案</strong>：3DGS的可扩展性较差，在大尺度环境中需要大量的存储。需要优化训练阶段和模型的存储利用，而对于NeRF来说只需要存储学习到的MLP参数。可以探索更多高效的数据结构和先进的压缩技术，如Light-Gaussian等</li><li><strong>先进的渲染算法</strong>：目前3DGS的渲染算法较为简单直接，可见性算法会导致高斯深度/混合顺序的剧烈切换，需要实施更先进的渲染算法，更好模拟光与材料属性的复杂相互作用。可结合传统计算机图形学的方法。此外，还可探索逆渲染。</li><li><strong>优化与正则化</strong>： 各向异性高斯虽然有利于表示复杂几何体，但可能产生不希望的视觉伪影。例如，特别是在具有视角依赖外观的区域，大的3D高斯可能导致弹出伪影，突然出现或消失的视觉元素打破了沉浸感。使用正则化可以增加收敛速度，平滑视觉噪声或提高图像质量。此外，3DGS中大量的超参数也会影响3DGS的泛化性。在3DGS的规则化和优化方面存在相当大的探索潜力。</li><li><strong>3D高斯在网格重建中的应用</strong>：可探索3DGS在网格重建中的潜力，从而缩小体积渲染和传统基于表面的方法的差距，以便提出新的渲染技巧和应用。</li><li><strong>赋予3DGS更多可能性</strong>： 尽管3DGS具有显著潜力，但3DGS的全范围应用仍然未被充分挖掘。一个有前景的探索方向是用额外的属性增强3D高斯，例如为特定应用定制的语言和物理属性。此外，最近的研究开始揭示3DGS在多个领域的能力，例如相机姿态估计、捕捉手对象互动和不确定性量化。这些初步发现突出了跨学科学者进一步探索3DGS的重要机会。</li></ul><h2 id="参考文献-REFERENCES"><a href="#参考文献-REFERENCES" class="headerlink" title="参考文献 REFERENCES"></a>参考文献 REFERENCES</h2><ol><li>Kerbl, B., Kopanas, G., Leimkühler, T., &amp; Drettakis, G. (2023). <a href="https://arxiv.org/abs/2308.04079">3D Gaussian Splatting for Real-Time Radiance Field Rendering.</a> SIGGRAPH 2023.</li><li>Mildenhall, B., Srinivasan, P. P., Tancik, M., Barron, J. T., Ramamoorthi, R., &amp; Ng, R. (2020). <a href="https://arxiv.org/abs/2003.08934">NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis.</a> ECCV 2020.</li><li>Zwicker, M., Pfister, H., van Baar, J., &amp; Gross, M. (2001). <a href="https://www.cs.umd.edu/~zwicker/publications/SurfaceSplatting-SIG01.pdf">Surface Splatting.</a> SIGGRAPH 2001</li><li>Luiten, J., Kopanas, G., Leibe, B., &amp; Ramanan, D. (2023). <a href="https://arxiv.org/abs/2308.09713">Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis.</a> International Conference on 3D Vision.</li><li>Zwicker, M., Pfister, H., van Baar, J., &amp; Gross, M. (2001). <a href="https://www.cs.umd.edu/~zwicker/publications/EWAVolumeSplatting-VIS01.pdf">EWA Volume Splatting.</a> IEEE Visualization 2001.</li><li>Yu, A., Fridovich-Keil, S., Tancik, M., Chen, Q., Recht, B., &amp; Kanazawa, A. (2023). <a href="https://arxiv.org/abs/2112.05131">Plenoxels: Radiance Fields without Neural Networks.</a> CVPR 2022.</li><li><a href="https://towardsdatascience.com/a-comprehensive-overview-of-gaussian-splatting-e7d570081362">A Comprehensive Overview of Gaussian Splatting</a></li><li><a href="https://github.com/huggingface/blog/blob/main/gaussian-splatting.md">Introduction to 3D Gaussian Splatting</a></li><li><a href="https://docs.nerf.studio/nerfology/model_components/visualize_samples.html#d-frustum">Sample Representation</a></li><li><a href="https://zhuanlan.zhihu.com/p/664725693">《3D Gaussian Splatting for Real-Time Radiance Field Rendering》3D高斯的理论理解</a></li><li><a href="https://blog.csdn.net/weixin_45657478/article/details/135603696">【论文笔记】A Survey on 3D Gaussian Splatting</a></li></ol>]]></content>
    
    
    <summary type="html">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</summary>
    
    
    
    <category term="Note" scheme="https://kedreamix.github.io/categories/Note/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>Arxiv学术论文查询接口详解</title>
    <link href="https://kedreamix.github.io/Note/arXiv/"/>
    <id>https://kedreamix.github.io/Note/arXiv/</id>
    <published>2024-01-24T05:40:00.000Z</published>
    <updated>2024-01-24T05:44:25.876Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Arxiv学术论文查询接口详解-转载"><a href="#Arxiv学术论文查询接口详解-转载" class="headerlink" title="Arxiv学术论文查询接口详解 转载"></a>Arxiv学术论文查询接口详解 转载</h1><blockquote><p>这篇博客主要转载自：<a href="https://hiyoungai.com/posts/arxiv%E5%AD%A6%E6%9C%AF%E8%AE%BA%E6%96%87%E6%9F%A5%E8%AF%A2%E6%8E%A5%E5%8F%A3%E8%AF%A6%E8%A7%A3/">Arxiv学术论文查询接口详解</a>，我觉得写的很好，所以我也不重新整理这一部分的API接口了。我后续使用这一部分的API接口来进行爬取得到最新的文章，还是非常方便的，所以也同时推荐给大家，能最快follow新文章</p></blockquote><p>Arxiv API 允许以编程方式获取 <a href="https://arxiv.org/">https://arxiv.org</a> 上的论文。API 的基本结构为：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://export.arxiv.org/api/{method_name}?{parameters}</span><br></pre></td></tr></tbody></table></figure><h2 id="查询接口"><a href="#查询接口" class="headerlink" title="查询接口"></a>查询接口</h2><p>查询接口的的 method_name 为 query，下面是查询方法的参数，参数之间以 <em>&amp;</em> 分隔。</p><div class="table-container"><table><thead><tr><th style="text-align:center">parameters</th><th style="text-align:center">type</th><th style="text-align:center">defaults</th><th style="text-align:center">required</th></tr></thead><tbody><tr><td style="text-align:center">search_query</td><td style="text-align:center">string</td><td style="text-align:center">None</td><td style="text-align:center">No</td></tr><tr><td style="text-align:center">id_list</td><td style="text-align:center">comma-delimited string（以 ‘，’ 分隔的字符串）</td><td style="text-align:center">None</td><td style="text-align:center">No</td></tr><tr><td style="text-align:center">start</td><td style="text-align:center">int</td><td style="text-align:center">0</td><td style="text-align:center">No</td></tr><tr><td style="text-align:center">max_results</td><td style="text-align:center">int</td><td style="text-align:center">10</td><td style="text-align:center">No</td></tr></tbody></table></div><h3 id="参数说明"><a href="#参数说明" class="headerlink" title="参数说明"></a>参数说明</h3><ul><li>如果 API 只包含 search_query（不包含 id_list），那么返回与 search_query 内容匹配的结果。</li><li>如果 API 只包含 id_list（不包含 search_query），那么返回 id_list 中每一项的结果。</li><li>如果 API 中包含了 search_query 和 id_list，那么返回在 id_list 中，并且与 search_query 匹配的文章。</li></ul><h3 id="分页查询"><a href="#分页查询" class="headerlink" title="分页查询"></a>分页查询</h3><p>通常情况下，一个查询可能有成百上千个返回结果。有时候我们不希望一次性查询到这么多数量，那么可以使用 <em>start</em> 和 <em>max_results</em> 两个字段来进行分页查询。</p><ul><li>start 是查询的起始索引，以 0 为第一个。</li><li>max_results 是查询返回的集合数。</li></ul><p>下面来举例说明一下：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">http://export.arxiv.org/api/query?search_query=all:electron&amp;start=0&amp;max_results=10 (1)</span><br><span class="line">http://export.arxiv.org/api/query?search_query=all:electron&amp;start=10&amp;max_results=10 (2)</span><br><span class="line">http://export.arxiv.org/api/query?search_query=all:electron&amp;start=20&amp;max_results=10 (3)</span><br></pre></td></tr></tbody></table></figure><p>查询结果分别为：</p><ol><li>0 - 9</li><li>10 - 19</li><li>20 - 29</li></ol><p>需要注意的是，由于 API 的限制，在多次调用 API 的情况下，建议每次调用的时间间隔为 3 秒。每次调用返回的最大数量为 2000 个。arXiv的硬限制约为 50,000 条记录； 对于与 50,000 多个原稿匹配的查询，无法接收全部结果. 解决这个问题的最简单的解决方案是将中断查询成小块，例如使用的时间片，与一系列日期的<code>submittedDate</code>或<code>lastUpdatedDate</code> 。</p><h3 id="排序查询"><a href="#排序查询" class="headerlink" title="排序查询"></a>排序查询</h3><p>对查询的结果进行排序有两个选项：<em>sortBy</em> 和 <em>sortOrder</em>。</p><ul><li>sortBy 的值有：relevance，lastUpdatedDate 和 submittedDate。</li><li>sortOrder 的值有：ascending 和 descending。</li></ul><p>示例：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://export.arxiv.org/api/query?search_query=ti:%22electron%20thermal%20conductivity%22&amp;sortBy=lastUpdatedDate&amp;sortOrder=ascending</span><br></pre></td></tr></tbody></table></figure><h2 id="结果响应"><a href="#结果响应" class="headerlink" title="结果响应"></a>结果响应</h2><p>API 的 Response 内容中是以 <em>Atom 1.0</em> 为主体的，<em>Atom</em> 是 XML 的一种语法。下面分别来说明各个标签的含义。</p><h3 id="Feed-Metadata"><a href="#Feed-Metadata" class="headerlink" title="Feed Metadata"></a>Feed Metadata</h3><p>每个 Response 都会包含的内容：</p><ol><li>版本和命名空间</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=<span class="string">"1.0"</span> encoding=<span class="string">"UTF-8"</span>?&gt;</span><br><span class="line">&lt;feed xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;</span><br></pre></td></tr></tbody></table></figure><ol><li>Title：feed 的标题，通常为查询 URL 的字符串。</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;title xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;</span><br><span class="line">    ArXiv Query:  search_query=all:electron&amp;amp;id_list=&amp;amp;start=0&amp;amp;max_results=1</span><br><span class="line">&lt;/title&gt;</span><br></pre></td></tr></tbody></table></figure><ol><li>Id：查询的唯一标识（注意不是查询的每个文章的 id），保证每个查询 id 是唯一的。</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;<span class="built_in">id</span> xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;</span><br><span class="line">    http://arxiv.org/api/cHxbiOdZaP56ODnBPIenZhzg5f8</span><br><span class="line">&lt;/id&gt;</span><br></pre></td></tr></tbody></table></figure><ol><li>Link：查询 URL 的规范化。</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;<span class="built_in">link</span> xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span> href=<span class="string">"http://arxiv.org/api/query?search_query=all:electron&amp;amp;id_list=&amp;amp;start=0&amp;amp;max_results=1"</span> rel=<span class="string">"self"</span> <span class="built_in">type</span>=<span class="string">"application/atom+xml"</span>/&gt;</span><br></pre></td></tr></tbody></table></figure><ol><li>Updated：提供了 feed 内容最后一次更新的时间。</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;updated xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;2007-10-08T00:00:00-04:00&lt;/updated&gt;</span><br></pre></td></tr></tbody></table></figure><ol><li>Opensearch：扩展元素，包含了查询的返回数量以及分页信息等。</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;opensearch:totalResults xmlns:opensearch=<span class="string">"http://a9.com/-/spec/opensearch/1.1/"</span>&gt;</span><br><span class="line">   1000</span><br><span class="line">&lt;/opensearch:totalResults&gt;</span><br><span class="line">&lt;opensearch:startIndex xmlns:opensearch=<span class="string">"http://a9.com/-/spec/opensearch/1.1/"</span>&gt;</span><br><span class="line">   0</span><br><span class="line">&lt;/opensearch:startIndex&gt;</span><br><span class="line">&lt;opensearch:itemsPerPage xmlns:opensearch=<span class="string">"http://a9.com/-/spec/opensearch/1.1/"</span>&gt;</span><br><span class="line">   1</span><br><span class="line">&lt;/opensearch:itemsPerPage&gt;</span><br></pre></td></tr></tbody></table></figure><h3 id="Entry-Metadata"><a href="#Entry-Metadata" class="headerlink" title="Entry Metadata"></a>Entry Metadata</h3><p>正常情况下，Response 返回结果中的 <em>feed</em> 标签会包含 0 个或者多个 <em>entry</em> 标签。每个 entry 表示一个查询的返回文章，下面分别说一下 entry 中的各个元素。</p><ol><li>Title：返回文章的标题</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;title xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;</span><br><span class="line">    Multi-Electron Production at High Transverse Momenta <span class="keyword">in</span> ep Collisions at HERA</span><br><span class="line">&lt;/title&gt;</span><br></pre></td></tr></tbody></table></figure><ol><li>Id：文章的 URL ，可以认为是文章的绝对路径。最后一个字段是文章的唯一标识符。</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;<span class="built_in">id</span> xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;</span><br><span class="line">    http://arxiv.org/abs/hep-ex/0307015</span><br><span class="line">&lt;/id&gt;</span><br></pre></td></tr></tbody></table></figure><ol><li>Published/Updated：文章的发布日期和更新日期。</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;published xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;</span><br><span class="line">    2007-02-27T16:02:02-05:00</span><br><span class="line">&lt;/published&gt;</span><br><span class="line">&lt;updated xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;</span><br><span class="line">    2007-06-25T17:09:59-04:00</span><br><span class="line">&lt;/updated&gt;</span><br></pre></td></tr></tbody></table></figure><ol><li>Summary：文章的摘要。</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;summary xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;</span><br><span class="line">    Multi-electron production is studied at high electron transverse momentum</span><br><span class="line">    <span class="keyword">in</span> positron- and electron-proton collisions using the H1 detector at HERA.</span><br><span class="line">    The data correspond to an integrated luminosity of 115 pb-1. Di-electron</span><br><span class="line">    and tri-electron event yields are measured. Cross sections are derived <span class="keyword">in</span></span><br><span class="line">    a restricted phase space region dominated by photon-photon collisions. In</span><br><span class="line">    general good agreement is found with the Standard Model predictions.</span><br><span class="line">    However, <span class="keyword">for</span> electron pair invariant masses above 100 GeV, three</span><br><span class="line">    di-electron events and three tri-electron events are observed, compared to</span><br><span class="line">    Standard Model expectations of 0.30 \pm 0.04 and 0.23 \pm 0.04,</span><br><span class="line">    respectively.</span><br><span class="line">&lt;/summary&gt;</span><br></pre></td></tr></tbody></table></figure><ol><li>Author：文章的作者，包含一个或者多个 name 标签，分别表示多个作者。</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;author xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;</span><br><span class="line">      &lt;name xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;H1 Collaboration&lt;/name&gt;</span><br><span class="line">&lt;/author&gt;</span><br></pre></td></tr></tbody></table></figure><ol><li>Category：文章的分类。</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;category xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span> term=<span class="string">"cs.LG"</span> scheme=<span class="string">"http://arxiv.org/schemas/atom"</span>/&gt;</span><br><span class="line">&lt;category xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span> term=<span class="string">"cs.AI"</span> scheme=<span class="string">"http://arxiv.org/schemas/atom"</span>/&gt;</span><br><span class="line">&lt;category xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span> term=<span class="string">"I.2.6"</span> scheme=<span class="string">"http://arxiv.org/schemas/atom"</span>/&gt;</span><br></pre></td></tr></tbody></table></figure><ol><li>Link，对于每个文章，最多有三个 link 元素，通过 ref 和 title 来区别，下面的表格表示 ref 和 title 的内容：</li></ol><div class="table-container"><table><thead><tr><th style="text-align:center">rel</th><th style="text-align:center">title</th><th style="text-align:center">refers to</th><th style="text-align:center">always present</th></tr></thead><tbody><tr><td style="text-align:center">alternate</td><td style="text-align:center">-</td><td style="text-align:center">abstract page</td><td style="text-align:center">yes</td></tr><tr><td style="text-align:center">related</td><td style="text-align:center">pdf</td><td style="text-align:center">pdf</td><td style="text-align:center">yes</td></tr><tr><td style="text-align:center">related</td><td style="text-align:center">doi</td><td style="text-align:center">resolved doi</td><td style="text-align:center">no</td></tr></tbody></table></div><p>例子：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;<span class="built_in">link</span> xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span> href=<span class="string">"http://arxiv.org/abs/hep-ex/0307015v1"</span> rel=<span class="string">"alternate"</span> <span class="built_in">type</span>=<span class="string">"text/html"</span>/&gt;</span><br><span class="line">&lt;<span class="built_in">link</span> xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span> title=<span class="string">"pdf"</span> href=<span class="string">"http://arxiv.org/pdf/hep-ex/0307015v1"</span> rel=<span class="string">"related"</span> <span class="built_in">type</span>=<span class="string">"application/pdf"</span>/&gt;</span><br><span class="line">&lt;<span class="built_in">link</span> xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span> title=<span class="string">"doi"</span> href=<span class="string">"http://dx.doi.org/10.1529/biophysj.104.047340"</span> rel=<span class="string">"related"</span>/&gt;</span><br></pre></td></tr></tbody></table></figure><ol><li>arxiv:primary_category：主要分类的扩展元素。</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;arxiv:primary_category xmlns:arxiv=<span class="string">"http://arxiv.org/schemas/atom"</span> term=<span class="string">"cs.LG"</span> scheme=<span class="string">"http://arxiv.org/schemas/atom"</span>/&gt;</span><br></pre></td></tr></tbody></table></figure><ol><li>arxiv:comment：评论扩展元素。</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;arxiv:comment xmlns:arxiv=<span class="string">"http://arxiv.org/schemas/atom"</span>&gt;</span><br><span class="line">   23 pages, 8 figures and 4 tables</span><br><span class="line">&lt;/arxiv:comment&gt;</span><br></pre></td></tr></tbody></table></figure><ol><li>arxiv:affiliation：作者从属关系。</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;author&gt;</span><br><span class="line">   &lt;name&gt;G. G. Kacprzak&lt;/name&gt;</span><br><span class="line">   &lt;arxiv:affiliation xmlns:arxiv=<span class="string">"http://arxiv.org/schemas/atom"</span>&gt;NMSU&lt;/arxiv:affiliation&gt;</span><br><span class="line">&lt;/author&gt;</span><br></pre></td></tr></tbody></table></figure><ol><li>arxiv:journal_ref：期刊说明</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;arxiv:journal_ref xmlns:arxiv=<span class="string">"http://arxiv.org/schemas/atom"</span>&gt;</span><br><span class="line">   Eur.Phys.J. C31 (2003) 17-29</span><br><span class="line">&lt;/arxiv:journal_ref&gt;</span><br></pre></td></tr></tbody></table></figure><ol><li>arxiv:doi：doi 说明</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;arxiv:doi xmlns:arxiv=<span class="string">"http://arxiv.org/schemas/atom"</span>&gt;</span><br><span class="line">   10.1529/biophysj.104.047340</span><br><span class="line">&lt;/arxiv:doi&gt;</span><br></pre></td></tr></tbody></table></figure><h3 id="Errors"><a href="#Errors" class="headerlink" title="Errors"></a>Errors</h3><p>返回错误，如果请求的响应出现错误，会返回一个详细的错误信息。例如下面是一个错误 id 的信息：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=<span class="string">"1.0"</span> encoding=<span class="string">"utf-8"</span>?&gt;</span><br><span class="line">&lt;feed xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span> xmlns:opensearch=<span class="string">"http://a9.com/-/spec/opensearch/1.1/"</span>&gt;</span><br><span class="line">  &lt;<span class="built_in">link</span> xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span> href=<span class="string">"http://arxiv.org/api/query?search_query=&amp;amp;id_list=1234.12345"</span> rel=<span class="string">"self"</span> <span class="built_in">type</span>=<span class="string">"application/atom+xml"</span>/&gt;</span><br><span class="line">  &lt;title xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;ArXiv Query: search_query=&amp;amp;id_list=1234.12345&lt;/title&gt;</span><br><span class="line">  &lt;<span class="built_in">id</span> xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;http://arxiv.org/api/kvuntZ8c9a4Eq5CF7KY03nMug+Q&lt;/id&gt;</span><br><span class="line">  &lt;updated xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;2007-10-12T00:00:00-04:00&lt;/updated&gt;</span><br><span class="line">  &lt;opensearch:totalResults xmlns:opensearch=<span class="string">"http://a9.com/-/spec/opensearch/1.1/"</span>&gt;1&lt;/opensearch:totalResults&gt;</span><br><span class="line">  &lt;opensearch:startIndex xmlns:opensearch=<span class="string">"http://a9.com/-/spec/opensearch/1.1/"</span>&gt;0&lt;/opensearch:startIndex&gt;</span><br><span class="line"></span><br><span class="line">  &lt;opensearch:itemsPerPage xmlns:opensearch=<span class="string">"http://a9.com/-/spec/opensearch/1.1/"</span>&gt;1&lt;/opensearch:itemsPerPage&gt;</span><br><span class="line">  &lt;entry xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;</span><br><span class="line">    &lt;<span class="built_in">id</span> xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;http://arxiv.org/api/errors<span class="comment">#incorrect_id_format_for_1234.12345&lt;/id&gt;</span></span><br><span class="line">    &lt;title xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;Error&lt;/title&gt;</span><br><span class="line">    &lt;summary xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;incorrect <span class="built_in">id</span> format <span class="keyword">for</span> 1234.12345&lt;/summary&gt;</span><br><span class="line">    &lt;updated xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;2007-10-12T00:00:00-04:00&lt;/updated&gt;</span><br><span class="line"></span><br><span class="line">    &lt;<span class="built_in">link</span> xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span> href=<span class="string">"http://arxiv.org/api/errors#incorrect_id_format_for_1234.12345"</span> rel=<span class="string">"alternate"</span> <span class="built_in">type</span>=<span class="string">"text/html"</span>/&gt;</span><br><span class="line">    &lt;author xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;</span><br><span class="line">      &lt;name xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;arXiv api core&lt;/name&gt;</span><br><span class="line">    &lt;/author&gt;</span><br><span class="line">  &lt;/entry&gt;</span><br><span class="line">&lt;/feed&gt;</span><br></pre></td></tr></tbody></table></figure><p>下面提供了一些常见的错误：</p><div class="table-container"><table><thead><tr><th style="text-align:center"><strong>Sample query 示例查询</strong></th><th style="text-align:center"><strong>Error Explanation 错误解释</strong></th></tr></thead><tbody><tr><td style="text-align:center"><a href="http://export.arxiv.org/api/query?start=not_an_int">http://export.arxiv.org/api/query?start=not_an_int</a></td><td style="text-align:center"><code>start</code> 一定是个整数</td></tr><tr><td style="text-align:center"><a href="http://export.arxiv.org/api/query?start=-1">http://export.arxiv.org/api/query?start=-1</a></td><td style="text-align:center"><code>start</code> 必须 &gt;= 0</td></tr><tr><td style="text-align:center"><a href="http://export.arxiv.org/api/query?max_results=not_an_int">http://export.arxiv.org/api/query?max_results=not_an_int</a></td><td style="text-align:center"><code>max_results</code> 一定是个整数</td></tr><tr><td style="text-align:center"><a href="http://export.arxiv.org/api/query?max_results=-1">http://export.arxiv.org/api/query?max_results=-1</a></td><td style="text-align:center"><code>max_results</code> 必须 &gt;= 0</td></tr><tr><td style="text-align:center"><a href="http://export.arxiv.org/api/query?id_list=1234.1234">http://export.arxiv.org/api/query?id_list=1234.1234</a></td><td style="text-align:center">malformed id</td></tr><tr><td style="text-align:center"><a href="http://export.arxiv.org/api/query?id_list=cond—mat/0709123">http://export.arxiv.org/api/query?id_list=cond—mat/0709123</a></td><td style="text-align:center">malformed id</td></tr></tbody></table></div><h2 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h2><h3 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h3><p>python2.7 上的简单请求：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line">url = <span class="string">'http://export.arxiv.org/api/query?search_query=all:electron&amp;start=0&amp;max_results=1'</span></span><br><span class="line">data = urllib.urlopen(url).read()</span><br><span class="line"><span class="built_in">print</span> data</span><br></pre></td></tr></tbody></table></figure><p>python3 上的请求：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request <span class="keyword">as</span> libreq</span><br><span class="line"><span class="keyword">with</span> libreq.urlopen(<span class="string">'http://export.arxiv.org/api/query?search_query=all:electron&amp;start=0&amp;max_results=1'</span>) <span class="keyword">as</span> url:</span><br><span class="line">    r = url.read()</span><br><span class="line">    <span class="built_in">print</span>(r)</span><br></pre></td></tr></tbody></table></figure><h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><h3 id="查询的详细结构"><a href="#查询的详细结构" class="headerlink" title="查询的详细结构"></a>查询的详细结构</h3><p>在 arXiv 搜索引擎中，每篇文章都被划分为许多可以单独搜索的字段。 例如，可以搜索一篇文章的标题，以及作者列表、摘要、评论和期刊参考文献。 要搜索其中一个字段，只需在搜索词前加上字段前缀和冒号即可。 例如：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://export.arxiv.org/api/query?search_query=au:del_maestro</span><br></pre></td></tr></tbody></table></figure><p>下面的表格显示所有字段的前缀：</p><div class="table-container"><table><thead><tr><th style="text-align:center"><strong>prefix</strong></th><th style="text-align:center"><strong>explanation</strong></th></tr></thead><tbody><tr><td style="text-align:center">ti</td><td style="text-align:center">Title</td></tr><tr><td style="text-align:center">au</td><td style="text-align:center">Author</td></tr><tr><td style="text-align:center">abs</td><td style="text-align:center">Abstract</td></tr><tr><td style="text-align:center">co</td><td style="text-align:center">Comment</td></tr><tr><td style="text-align:center">jr</td><td style="text-align:center">Journal Reference</td></tr><tr><td style="text-align:center">cat</td><td style="text-align:center">Subject Category</td></tr><tr><td style="text-align:center">rn</td><td style="text-align:center">Report Number</td></tr><tr><td style="text-align:center">id</td><td style="text-align:center">Id (use <code>id_list</code> instead)</td></tr><tr><td style="text-align:center">all</td><td style="text-align:center">All of the above</td></tr></tbody></table></div><p>并且查询也支持布尔运算，假设我们希望找到作者 Adrian DelMaestro 的所有文章，其标题中也包含单词 checkerboard。 我们可以使用 AND 操作符构造下面的查询：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://export.arxiv.org/api/query?search_query=au:del_maestro+AND+ti:checkerboard</span><br></pre></td></tr></tbody></table></figure><p>下面是三种可能的布尔值：</p><ul><li>AND</li><li>OR</li><li>ANDNOT</li></ul><p>下面是特殊符号的含义以及转义字符：</p><div class="table-container"><table><thead><tr><th style="text-align:left">symbol</th><th style="text-align:left">encoding</th><th style="text-align:left">explanation</th></tr></thead><tbody><tr><td style="text-align:left">( )</td><td style="text-align:left">%28 %29</td><td style="text-align:left">用于为布尔运算符优先级对布尔表达式进行分组</td></tr><tr><td style="text-align:left">“ “</td><td style="text-align:left">%22 %22</td><td style="text-align:left">用于将多个单词组合成短语以搜索特定字段</td></tr><tr><td style="text-align:left">空格</td><td style="text-align:left">+</td><td style="text-align:left">用于扩展<code>search_query</code> 包含多个字段</td></tr></tbody></table></div><h3 id="返回的详细结构"><a href="#返回的详细结构" class="headerlink" title="返回的详细结构"></a>返回的详细结构</h3><p>下表列出了返回的 Atom 结果的每个元素:</p><div class="table-container"><table><thead><tr><th style="text-align:center"><strong>element</strong></th><th style="text-align:center"><strong>explanation</strong></th></tr></thead><tbody><tr><td style="text-align:center"><strong>feed elements</strong></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">title</td><td style="text-align:center">包含规范化查询字符串的标题</td></tr><tr><td style="text-align:center">id</td><td style="text-align:center">分配给此查询的唯一 id</td></tr><tr><td style="text-align:center">updated</td><td style="text-align:center">最后一次更新此查询的搜索结果。 设置为当天的午夜</td></tr><tr><td style="text-align:center">link</td><td style="text-align:center">通过 GET 请求检索此提要的 url</td></tr><tr><td style="text-align:center">opensearch:totalResults</td><td style="text-align:center">此查询的搜索结果总数</td></tr><tr><td style="text-align:center">opensearch:startIndex</td><td style="text-align:center">总结果列表中第一个返回结果的基于0的索引</td></tr><tr><td style="text-align:center">opensearch:itemsPerPage</td><td style="text-align:center">每页返回的结果数</td></tr><tr><td style="text-align:center"><strong>entry elements</strong></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">title</td><td style="text-align:center">文章的标题</td></tr><tr><td style="text-align:center">id</td><td style="text-align:center">文章的网址<code>http://arxiv.org/abs/id</code></td></tr><tr><td style="text-align:center">published</td><td style="text-align:center">文章的发布日期</td></tr><tr><td style="text-align:center">updated</td><td style="text-align:center">文章的更新日期，如果为 v1 版本，那么与发布日期相同</td></tr><tr><td style="text-align:center">summary</td><td style="text-align:center">文章摘要</td></tr><tr><td style="text-align:center">author</td><td style="text-align:center">每个作者有一个子元素 name，包含了作者的名字</td></tr><tr><td style="text-align:center">link</td><td style="text-align:center">可以给定与这篇文章关联的 3 个网址</td></tr><tr><td style="text-align:center">category</td><td style="text-align:center">文章分类</td></tr><tr><td style="text-align:center">arxiv:primary_category</td><td style="text-align:center">主要的 arXiv 分类</td></tr><tr><td style="text-align:center">arxiv:comment</td><td style="text-align:center">作者对此发表的评论</td></tr><tr><td style="text-align:center">arxiv:affiliation</td><td style="text-align:center">作者的从属关系</td></tr><tr><td style="text-align:center">arxiv:journal_ref</td><td style="text-align:center">参考文献</td></tr><tr><td style="text-align:center">arxiv:doi</td><td style="text-align:center">已解析的 DOI 的 url，指向外部资源</td></tr></tbody></table></div><h3 id="学科的分类"><a href="#学科的分类" class="headerlink" title="学科的分类"></a>学科的分类</h3><p>下面是学科分类字段以及对应的翻译（软件脚本自动翻译，如不对请勿喷）：</p><div class="table-container"><table><thead><tr><th style="text-align:center">字段</th><th style="text-align:center">学科（英文）</th><th style="text-align:center">学科（中文）</th></tr></thead><tbody><tr><td style="text-align:center">astro-ph</td><td style="text-align:center">Astrophysics</td><td style="text-align:center">天体物理</td></tr><tr><td style="text-align:center">astro-ph.CO</td><td style="text-align:center">Cosmology and Nongalactic Astrophysics</td><td style="text-align:center">宇宙学与非规则天体物理学</td></tr><tr><td style="text-align:center">astro-ph.EP</td><td style="text-align:center">Earth and Planetary Astrophysics</td><td style="text-align:center">地球与行星天体物理学</td></tr><tr><td style="text-align:center">astro-ph.GA</td><td style="text-align:center">Astrophysics of Galaxies</td><td style="text-align:center">星系的天体物理学</td></tr><tr><td style="text-align:center">astro-ph.HE</td><td style="text-align:center">High Energy Astrophysical Phenomena</td><td style="text-align:center">高能天体物理现象</td></tr><tr><td style="text-align:center">astro-ph.IM</td><td style="text-align:center">Instrumentation and Methods for Astrophysics</td><td style="text-align:center">天体物理学的仪器和方法</td></tr><tr><td style="text-align:center">astro-ph.SR</td><td style="text-align:center">Solar and Stellar Astrophysics</td><td style="text-align:center">太阳与恒星天体物理学</td></tr><tr><td style="text-align:center">cond-mat.dis-nn</td><td style="text-align:center">Disordered Systems and Neural Networks</td><td style="text-align:center">无序系统与神经网络</td></tr><tr><td style="text-align:center">cond-mat.mes-hall</td><td style="text-align:center">Mesoscale and Nanoscale Physics</td><td style="text-align:center">中尺度和纳米尺度物理学</td></tr><tr><td style="text-align:center">cond-mat.mtrl-sci</td><td style="text-align:center">Materials Science</td><td style="text-align:center">材料科学</td></tr><tr><td style="text-align:center">cond-mat.other</td><td style="text-align:center">Other Condensed Matter</td><td style="text-align:center">其他凝聚态</td></tr><tr><td style="text-align:center">cond-mat.quant-gas</td><td style="text-align:center">Quantum Gases</td><td style="text-align:center">量子气体</td></tr><tr><td style="text-align:center">cond-mat.soft</td><td style="text-align:center">Soft Condensed Matter</td><td style="text-align:center">软凝聚物</td></tr><tr><td style="text-align:center">cond-mat.stat-mech</td><td style="text-align:center">Statistical Mechanics</td><td style="text-align:center">统计力学</td></tr><tr><td style="text-align:center">cond-mat.str-el</td><td style="text-align:center">Strongly Correlated Electrons</td><td style="text-align:center">强关联电子</td></tr><tr><td style="text-align:center">cond-mat.supr-con</td><td style="text-align:center">Superconductivity</td><td style="text-align:center">超导现象</td></tr><tr><td style="text-align:center">cs.AI</td><td style="text-align:center">Artificial Intelligence</td><td style="text-align:center">人工智能</td></tr><tr><td style="text-align:center">cs.AR</td><td style="text-align:center">Hardware Architecture</td><td style="text-align:center">硬件架构</td></tr><tr><td style="text-align:center">cs.CC</td><td style="text-align:center">Computational Complexity</td><td style="text-align:center">计算复杂性</td></tr><tr><td style="text-align:center">cs.CE</td><td style="text-align:center">Computational Engineering, Finance, and Science</td><td style="text-align:center">计算工程，金融和科学</td></tr><tr><td style="text-align:center">cs.CG</td><td style="text-align:center">Computational Geometry</td><td style="text-align:center">计算几何</td></tr><tr><td style="text-align:center">cs.CL</td><td style="text-align:center">Computation and Language</td><td style="text-align:center">计算与语言</td></tr><tr><td style="text-align:center">cs.CR</td><td style="text-align:center">Cryptography and Security</td><td style="text-align:center">密码学与保安</td></tr><tr><td style="text-align:center">cs.CV</td><td style="text-align:center">Computer Vision and Pattern Recognition</td><td style="text-align:center">计算机视觉与模式识别</td></tr><tr><td style="text-align:center">CY</td><td style="text-align:center">Computers and Society</td><td style="text-align:center">电脑与社会</td></tr><tr><td style="text-align:center">cs.DB</td><td style="text-align:center">Databases</td><td style="text-align:center">数据库</td></tr><tr><td style="text-align:center">cs.DC</td><td style="text-align:center">Distributed, Parallel, and Cluster Computing</td><td style="text-align:center">分布式、并行和集群计算</td></tr><tr><td style="text-align:center">cs.DL</td><td style="text-align:center">Digital Libraries</td><td style="text-align:center">数字仓库</td></tr><tr><td style="text-align:center">cs.DM</td><td style="text-align:center">Discrete Mathematics</td><td style="text-align:center">离散数学</td></tr><tr><td style="text-align:center">cs.DS</td><td style="text-align:center">Data Structures and Algorithms</td><td style="text-align:center">数据结构和算法</td></tr><tr><td style="text-align:center">cs.ET</td><td style="text-align:center">Emerging Technologies</td><td style="text-align:center">新兴科技</td></tr><tr><td style="text-align:center">cs.FL</td><td style="text-align:center">Formal Languages and Automata Theory</td><td style="text-align:center">形式语言与自动机理论</td></tr><tr><td style="text-align:center">cs.GL</td><td style="text-align:center">General Literature</td><td style="text-align:center">一般文学</td></tr><tr><td style="text-align:center">cs.GR</td><td style="text-align:center">Graphics</td><td style="text-align:center">图形</td></tr><tr><td style="text-align:center">cs.GT</td><td style="text-align:center">Computer Science and Game Theory</td><td style="text-align:center">计算机科学与博弈论</td></tr><tr><td style="text-align:center">cs.HC</td><td style="text-align:center">Human-Computer Interaction</td><td style="text-align:center">人机交互</td></tr><tr><td style="text-align:center">cs.IR</td><td style="text-align:center">Information Retrieval</td><td style="text-align:center">信息检索</td></tr><tr><td style="text-align:center">cs.IT</td><td style="text-align:center">Information Theory</td><td style="text-align:center">信息理论</td></tr><tr><td style="text-align:center">cs.LG</td><td style="text-align:center">Learning</td><td style="text-align:center">学习</td></tr><tr><td style="text-align:center">cs.LO</td><td style="text-align:center">Logic in Computer Science</td><td style="text-align:center">计算机科学中的逻辑</td></tr><tr><td style="text-align:center">cs.MA</td><td style="text-align:center">Multiagent Systems</td><td style="text-align:center">多代理系统</td></tr><tr><td style="text-align:center">cs.MM</td><td style="text-align:center">Multimedia</td><td style="text-align:center">多媒体</td></tr><tr><td style="text-align:center">cs.MS</td><td style="text-align:center">Mathematical Software</td><td style="text-align:center">数学软件</td></tr><tr><td style="text-align:center">cs.NA</td><td style="text-align:center">Numerical Analysis</td><td style="text-align:center">数值分析</td></tr><tr><td style="text-align:center">cs.NE</td><td style="text-align:center">Neural and Evolutionary Computing</td><td style="text-align:center">神经和进化计算</td></tr><tr><td style="text-align:center">cs.NI</td><td style="text-align:center">Networking and Internet Architecture</td><td style="text-align:center">网络与互联网架构</td></tr><tr><td style="text-align:center">cs.OH</td><td style="text-align:center">Other Computer Science</td><td style="text-align:center">其他计算机科学</td></tr><tr><td style="text-align:center">cs.OS</td><td style="text-align:center">Operating Systems</td><td style="text-align:center">操作系统</td></tr><tr><td style="text-align:center">cs.PF</td><td style="text-align:center">Performance</td><td style="text-align:center">性能</td></tr><tr><td style="text-align:center">cs.PL</td><td style="text-align:center">Programming Languages</td><td style="text-align:center">编程语言</td></tr><tr><td style="text-align:center">cs.RO</td><td style="text-align:center">Robotics</td><td style="text-align:center">机器人技术</td></tr><tr><td style="text-align:center">cs.SC</td><td style="text-align:center">Symbolic Computation</td><td style="text-align:center">符号计算</td></tr><tr><td style="text-align:center">cs.SD</td><td style="text-align:center">Sound</td><td style="text-align:center">声音</td></tr><tr><td style="text-align:center">cs.SE</td><td style="text-align:center">Software Engineering</td><td style="text-align:center">软件工程</td></tr><tr><td style="text-align:center">cs.SI</td><td style="text-align:center">Social and Information Networks</td><td style="text-align:center">社会和信息网络</td></tr><tr><td style="text-align:center">cs.SY</td><td style="text-align:center">Systems and Control</td><td style="text-align:center">系统及控制</td></tr><tr><td style="text-align:center">econ.EM</td><td style="text-align:center">Econometrics</td><td style="text-align:center">计量经济学</td></tr><tr><td style="text-align:center">eess.AS</td><td style="text-align:center">Audio and Speech Processing</td><td style="text-align:center">音频及语音处理</td></tr><tr><td style="text-align:center">eess.IV</td><td style="text-align:center">Image and Video Processing</td><td style="text-align:center">图像和视频处理</td></tr><tr><td style="text-align:center">eess.SP</td><td style="text-align:center">Signal Processing</td><td style="text-align:center">信号处理</td></tr><tr><td style="text-align:center">gr-qc</td><td style="text-align:center">General Relativity and Quantum Cosmology</td><td style="text-align:center">广义相对论和量子宇宙学</td></tr><tr><td style="text-align:center">hep-ex</td><td style="text-align:center">High Energy Physics - Experiment</td><td style="text-align:center">高能物理实验</td></tr><tr><td style="text-align:center">hep-lat</td><td style="text-align:center">High Energy Physics - Lattice</td><td style="text-align:center">高能物理-晶格</td></tr><tr><td style="text-align:center">hep-ph</td><td style="text-align:center">High Energy Physics - Phenomenology</td><td style="text-align:center">高能物理-现象学</td></tr><tr><td style="text-align:center">hep-th</td><td style="text-align:center">High Energy Physics - Theory</td><td style="text-align:center">高能物理理论</td></tr><tr><td style="text-align:center">math.AC</td><td style="text-align:center">Commutative Algebra</td><td style="text-align:center">交换代数</td></tr><tr><td style="text-align:center">math.AG</td><td style="text-align:center">Algebraic Geometry</td><td style="text-align:center">代数几何</td></tr><tr><td style="text-align:center">math.AP</td><td style="text-align:center">Analysis of PDEs</td><td style="text-align:center">偏微分方程分析</td></tr><tr><td style="text-align:center">math.AT</td><td style="text-align:center">Algebraic Topology</td><td style="text-align:center">代数拓扑</td></tr><tr><td style="text-align:center">math.CA</td><td style="text-align:center">Classical Analysis and ODEs</td><td style="text-align:center">传统分析和微分方程</td></tr><tr><td style="text-align:center">math.CO</td><td style="text-align:center">Combinatorics</td><td style="text-align:center">组合数学</td></tr><tr><td style="text-align:center">math.CT</td><td style="text-align:center">Category Theory</td><td style="text-align:center">范畴理论</td></tr><tr><td style="text-align:center">math.CV</td><td style="text-align:center">Complex Variables</td><td style="text-align:center">复杂变量</td></tr><tr><td style="text-align:center">math.DG</td><td style="text-align:center">Differential Geometry</td><td style="text-align:center">微分几何</td></tr><tr><td style="text-align:center">math.DS</td><td style="text-align:center">Dynamical Systems</td><td style="text-align:center">动力系统</td></tr><tr><td style="text-align:center">math.FA</td><td style="text-align:center">Functional Analysis</td><td style="text-align:center">功能分析</td></tr><tr><td style="text-align:center">math.GM</td><td style="text-align:center">General Mathematics</td><td style="text-align:center">普通数学</td></tr><tr><td style="text-align:center">math.GN</td><td style="text-align:center">General Topology</td><td style="text-align:center">点集拓扑学</td></tr><tr><td style="text-align:center">math.GR</td><td style="text-align:center">Group Theory</td><td style="text-align:center">群论</td></tr><tr><td style="text-align:center">math.GT</td><td style="text-align:center">Geometric Topology</td><td style="text-align:center">几何拓扑学</td></tr><tr><td style="text-align:center">math.HO</td><td style="text-align:center">History and Overview</td><td style="text-align:center">历史和概述</td></tr><tr><td style="text-align:center">math.IT</td><td style="text-align:center">Information Theory</td><td style="text-align:center">信息理论</td></tr><tr><td style="text-align:center">math.KT</td><td style="text-align:center">K-Theory and Homology</td><td style="text-align:center">K 理论与同调</td></tr><tr><td style="text-align:center">math.LO</td><td style="text-align:center">Logic</td><td style="text-align:center">逻辑</td></tr><tr><td style="text-align:center">math.MG</td><td style="text-align:center">Metric Geometry</td><td style="text-align:center">度量几何学</td></tr><tr><td style="text-align:center">math.MP</td><td style="text-align:center">Mathematical Physics</td><td style="text-align:center">数学物理</td></tr><tr><td style="text-align:center">math.NA</td><td style="text-align:center">Numerical Analysis</td><td style="text-align:center">数值分析</td></tr><tr><td style="text-align:center">math.NT</td><td style="text-align:center">Number Theory</td><td style="text-align:center">数论</td></tr><tr><td style="text-align:center">math.OA</td><td style="text-align:center">Operator Algebras</td><td style="text-align:center">算子代数</td></tr><tr><td style="text-align:center">math.OC</td><td style="text-align:center">Optimization and Control</td><td style="text-align:center">优化和控制</td></tr><tr><td style="text-align:center">math.PR</td><td style="text-align:center">Probability</td><td style="text-align:center">概率</td></tr><tr><td style="text-align:center">math.QA</td><td style="text-align:center">Quantum Algebra</td><td style="text-align:center">量子代数</td></tr><tr><td style="text-align:center">math.RA</td><td style="text-align:center">Rings and Algebras</td><td style="text-align:center">环与代数</td></tr><tr><td style="text-align:center">math.RT</td><td style="text-align:center">Representation Theory</td><td style="text-align:center">表示论</td></tr><tr><td style="text-align:center">math.SG</td><td style="text-align:center">Symplectic Geometry</td><td style="text-align:center">辛几何</td></tr><tr><td style="text-align:center">math.SP</td><td style="text-align:center">Spectral Theory</td><td style="text-align:center">光谱理论</td></tr><tr><td style="text-align:center">math.ST</td><td style="text-align:center">Statistics Theory</td><td style="text-align:center">统计学理论</td></tr><tr><td style="text-align:center">math-ph</td><td style="text-align:center">Mathematical Physics</td><td style="text-align:center">数学物理</td></tr><tr><td style="text-align:center">nlin.AO</td><td style="text-align:center">Adaptation and Self-Organizing Systems</td><td style="text-align:center">适应与自组织系统</td></tr><tr><td style="text-align:center">nlin.CD</td><td style="text-align:center">Chaotic Dynamics</td><td style="text-align:center">混沌动力学</td></tr><tr><td style="text-align:center">nlin.CG</td><td style="text-align:center">Cellular Automata and Lattice Gases</td><td style="text-align:center">元胞自动机与格子气体</td></tr><tr><td style="text-align:center">nlin.PS</td><td style="text-align:center">Pattern Formation and Solitons</td><td style="text-align:center">模式形成与孤子</td></tr><tr><td style="text-align:center">nlin.SI</td><td style="text-align:center">Exactly Solvable and Integrable Systems</td><td style="text-align:center">严格可解可积系统</td></tr><tr><td style="text-align:center">nucl-ex</td><td style="text-align:center">Nuclear Experiment</td><td style="text-align:center">核试验</td></tr><tr><td style="text-align:center">nucl-th</td><td style="text-align:center">Nuclear Theory</td><td style="text-align:center">核理论</td></tr><tr><td style="text-align:center">physics.acc-ph</td><td style="text-align:center">Accelerator Physics</td><td style="text-align:center">加速器物理学</td></tr><tr><td style="text-align:center">physics.ao-ph</td><td style="text-align:center">Atmospheric and Oceanic Physics</td><td style="text-align:center">大气和海洋物理学</td></tr><tr><td style="text-align:center">physics.app-ph</td><td style="text-align:center">Applied Physics</td><td style="text-align:center">应用物理学</td></tr><tr><td style="text-align:center">physics.atm-clus</td><td style="text-align:center">Atomic and Molecular Clusters</td><td style="text-align:center">原子和分子团簇</td></tr><tr><td style="text-align:center">physics.atom-ph</td><td style="text-align:center">Atomic Physics</td><td style="text-align:center">原子物理学</td></tr><tr><td style="text-align:center">physics.bio-ph</td><td style="text-align:center">Biological Physics</td><td style="text-align:center">生物物理学</td></tr><tr><td style="text-align:center">physics.chem-ph</td><td style="text-align:center">Chemical Physics</td><td style="text-align:center">化学物理</td></tr><tr><td style="text-align:center">physics.class-ph</td><td style="text-align:center">Classical Physics</td><td style="text-align:center">经典物理学</td></tr><tr><td style="text-align:center">physics.comp-ph</td><td style="text-align:center">Computational Physics</td><td style="text-align:center">计算物理学</td></tr><tr><td style="text-align:center">physics.data-an</td><td style="text-align:center">Data Analysis, Statistics and Probability</td><td style="text-align:center">数据分析、统计和概率</td></tr><tr><td style="text-align:center">physics.ed-ph</td><td style="text-align:center">Physics Education</td><td style="text-align:center">物理教育</td></tr><tr><td style="text-align:center">physics.flu-dyn</td><td style="text-align:center">Fluid Dynamics</td><td style="text-align:center">流体动力学</td></tr><tr><td style="text-align:center">physics.gen-ph</td><td style="text-align:center">General Physics</td><td style="text-align:center">普通物理</td></tr><tr><td style="text-align:center">physics.geo-ph</td><td style="text-align:center">Geophysics</td><td style="text-align:center">地球物理学</td></tr><tr><td style="text-align:center">physics.hist-ph</td><td style="text-align:center">History and Philosophy of Physics</td><td style="text-align:center">物理学的历史与哲学</td></tr><tr><td style="text-align:center">physics.ins-det</td><td style="text-align:center">Instrumentation and Detectors</td><td style="text-align:center">仪器和探测器</td></tr><tr><td style="text-align:center">physics.med-ph</td><td style="text-align:center">Medical Physics</td><td style="text-align:center">医学物理学</td></tr><tr><td style="text-align:center">physics.optics</td><td style="text-align:center">Optics</td><td style="text-align:center">光学</td></tr><tr><td style="text-align:center">physics.plasm-ph</td><td style="text-align:center">Plasma Physics</td><td style="text-align:center">等离子体物理</td></tr><tr><td style="text-align:center">physics.pop-ph</td><td style="text-align:center">Popular Physics</td><td style="text-align:center">大众物理</td></tr><tr><td style="text-align:center">physics.soc-ph</td><td style="text-align:center">Physics and Society</td><td style="text-align:center">物理学与社会</td></tr><tr><td style="text-align:center">physics.space-ph</td><td style="text-align:center">Space Physics</td><td style="text-align:center">空间物理学</td></tr><tr><td style="text-align:center">q-bio.BM</td><td style="text-align:center">Biomolecules</td><td style="text-align:center">生物分子</td></tr><tr><td style="text-align:center">q-bio.CB</td><td style="text-align:center">Cell Behavior</td><td style="text-align:center">细胞行为</td></tr><tr><td style="text-align:center">q-bio.GN</td><td style="text-align:center">Genomics</td><td style="text-align:center">基因组学</td></tr><tr><td style="text-align:center">q-bio.MN</td><td style="text-align:center">Molecular Networks</td><td style="text-align:center">分子网络</td></tr><tr><td style="text-align:center">q-bio.NC</td><td style="text-align:center">Neurons and Cognition</td><td style="text-align:center">神经元与认知</td></tr><tr><td style="text-align:center">q-bio.OT</td><td style="text-align:center">Other Quantitative Biology</td><td style="text-align:center">其他定量生物学</td></tr><tr><td style="text-align:center">q-bio.PE</td><td style="text-align:center">Populations and Evolution</td><td style="text-align:center">种群与进化</td></tr><tr><td style="text-align:center">q-bio.QM</td><td style="text-align:center">Quantitative Methods</td><td style="text-align:center">定量方法</td></tr><tr><td style="text-align:center">q-bio.SC</td><td style="text-align:center">Subcellular Processes</td><td style="text-align:center">亚细胞突起</td></tr><tr><td style="text-align:center">q-bio.TO</td><td style="text-align:center">Tissues and Organs</td><td style="text-align:center">组织和器官</td></tr><tr><td style="text-align:center">q-fin.CP</td><td style="text-align:center">Computational Finance</td><td style="text-align:center">金融工程</td></tr><tr><td style="text-align:center">q-fin.EC</td><td style="text-align:center">Economics</td><td style="text-align:center">经济学</td></tr><tr><td style="text-align:center">q-fin.GN</td><td style="text-align:center">General Finance</td><td style="text-align:center">财务概述</td></tr><tr><td style="text-align:center">q-fin.MF</td><td style="text-align:center">Mathematical Finance</td><td style="text-align:center">数学金融</td></tr><tr><td style="text-align:center">q-fin.PM</td><td style="text-align:center">Portfolio Management</td><td style="text-align:center">投资组合管理</td></tr><tr><td style="text-align:center">q-fin.PR</td><td style="text-align:center">Pricing of Securities</td><td style="text-align:center">证券定价</td></tr><tr><td style="text-align:center">q-fin.RM</td><td style="text-align:center">Risk Management</td><td style="text-align:center">风险管理</td></tr><tr><td style="text-align:center">q-fin.ST</td><td style="text-align:center">Statistical Finance</td><td style="text-align:center">金融统计</td></tr><tr><td style="text-align:center">q-fin.TR</td><td style="text-align:center">Trading and Market Microstructure</td><td style="text-align:center">交易与市场微观结构</td></tr><tr><td style="text-align:center">quant-ph</td><td style="text-align:center">Quantum Physics</td><td style="text-align:center">量子物理学</td></tr><tr><td style="text-align:center">stat.AP</td><td style="text-align:center">Applications</td><td style="text-align:center">应用</td></tr><tr><td style="text-align:center">stat.CO</td><td style="text-align:center">Computation</td><td style="text-align:center">计算</td></tr><tr><td style="text-align:center">stat.ME</td><td style="text-align:center">Methodology</td><td style="text-align:center">方法论</td></tr><tr><td style="text-align:center">stat.ML</td><td style="text-align:center">Machine Learning</td><td style="text-align:center">机器学习</td></tr><tr><td style="text-align:center">stat.OT</td><td style="text-align:center">Other Statistics</td><td style="text-align:center">其他统计学</td></tr><tr><td style="text-align:center">stat.TH</td><td style="text-align:center">Statistics Theory</td><td style="text-align:center">统计学理论</td></tr></tbody></table></div>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Arxiv学术论文查询接口详解-转载&quot;&gt;&lt;a href=&quot;#Arxiv学术论文查询接口详解-转载&quot; class=&quot;headerlink&quot; title=&quot;Arxiv学术论文查询接口详解 转载&quot;&gt;&lt;/a&gt;Arxiv学术论文查询接口详解 转载&lt;/h1&gt;&lt;blockquo</summary>
      
    
    
    
    <category term="Note" scheme="https://kedreamix.github.io/categories/Note/"/>
    
    
    <category term="Python" scheme="https://kedreamix.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>开机自启动登录/认证脚本设置（SZU为例）</title>
    <link href="https://kedreamix.github.io/Note/AutoLogin/"/>
    <id>https://kedreamix.github.io/Note/AutoLogin/</id>
    <published>2024-01-21T17:20:13.000Z</published>
    <updated>2024-01-21T17:42:17.298Z</updated>
    
    <content type="html"><![CDATA[<h2 id="开机自启动登录-认证前沿"><a href="#开机自启动登录-认证前沿" class="headerlink" title="开机自启动登录/认证前沿"></a>开机自启动登录/认证前沿</h2><p>有时候在学校或者在企业的时候，会出现这样一种情况，就是我们需要认证才能够上网，但是这种认证并不是非常稳定，有可能会出现断连的情况 </p><p>包括有时候电脑关机后自启后也会断掉，针对这种情况，我介绍一种开机自启动登录/认证的脚本，这样能不断的保证联网</p><h2 id="开机自启动目录"><a href="#开机自启动目录" class="headerlink" title="开机自启动目录"></a>开机自启动目录</h2><p>首先，我们既然向进行开机自启动，那么我就需要找到开机自启动目录</p><p>在Windows中想要开机自启动某些应用，可以把程序的快捷方式放到开始菜单-&gt;程序-&gt;启动目录下，但是自启动又分为用户自启动和系统自启动，前者针对单个用户，后者针对全部用户生效。</p><ul><li>用户自启动目录：<code>C:\Users\Administrator\AppData\Roaming\Microsoft\Windows\Start Menu\Programs\Startup</code></li><li>系统自启动目录：<code>C:\ProgramData\Microsoft\Windows\Start Menu\Programs\StartUp</code></li></ul><p>这里要根据用户名进行设置和修改，但是当然有更简单的方法对吧，如下，我也推荐这样的方式自动打开我们的开机自启动目录。</p><p><strong>快捷命令：按下【win+R】打开运行输入：【shell:Common Startup】</strong></p><h2 id="设置脚本运行"><a href="#设置脚本运行" class="headerlink" title="设置脚本运行"></a>设置脚本运行</h2><p>当我们已经找到了开机自启动目录后，我们就可以在这个文件下，写入<code>bat</code>文件，这样每次开机都会自动运行。</p><p>首先我们定义<code>drcom.bat</code>，我们希望他能运行一个代码来进行一个检测连接网络情况，这个代码放在了 <code>C盘</code> 的根目录下，我们也可以根据自己情况修改路径放置</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python C:\\network.py</span><br></pre></td></tr></tbody></table></figure><p>所以主要的就是这个代码了，这个代码是用Python写的，原理十分的简单</p><p>既然我们需要不断的联网，那我们就不断的看看能否ping通百度，如果ping通了说明联网了，如果没有，说明我们需要运行一个登录的<code>Shell</code>文件</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    exit_code = os.system(<span class="string">'ping www.baidu.com'</span>)</span><br><span class="line">    <span class="keyword">if</span> exit_code != <span class="number">0</span>:</span><br><span class="line">        os.system(<span class="string">r'C:\login_network.sh'</span>)</span><br><span class="line">        time.sleep(<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        time.sleep(<span class="number">10</span>)</span><br></pre></td></tr></tbody></table></figure><p>这里面的<code>Shell</code>文件可以是任何登录的脚本和命令，对于<code>SZU</code>来说，脚本如下，只要改为自己的账号和密码即可，这样就完成了开机自启动。</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"Login SZU NetWork"</span></span><br><span class="line">username=<span class="string">"1111111"</span>  &amp;&amp; password=<span class="string">"6666666"</span> &amp;&amp; curl -k https://drcom.szu.edu.cn/a70.htm --data <span class="string">"DDDDD=<span class="variable">$username</span>&amp;upass=<span class="variable">$password</span>&amp;0MKKey=123456"</span>;</span><br></pre></td></tr></tbody></table></figure><p>当然，简单的情况下，我们也可以直接在上面的代码<code>network.py</code>里面修改，比如如下</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    exit_code = os.system(<span class="string">'ping www.baidu.com'</span>)</span><br><span class="line">    <span class="keyword">if</span> exit_code != <span class="number">0</span>:</span><br><span class="line">        os.system(<span class="string">r'username="1111111"  &amp;&amp; password="6666666" &amp;&amp; curl -k https://drcom.szu.edu.cn/a70.htm --data "DDDDD=$username&amp;upass=$password&amp;0MKKey=123456";'</span>)</span><br><span class="line">        time.sleep(<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        time.sleep(<span class="number">10</span>)</span><br></pre></td></tr></tbody></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过这个脚本实现开机自动登录，解决了开机后无法连接网络的问题。本文介绍了在Windows系统下找到开机自启动目录，并放置检查网络状态和登录认证脚本的方法。</p><p>具体来说，使用Python脚本循环ping百度检测网络，如果无法连接则调用登录脚本进行登录。登录脚本可以直接写死账号密码，也可以单独保存为文件引用。</p><p>这种方法很简单实用，不需要付出额外精力就可以获得自动登录的功能。尤其是在需要频繁登录校园网或公司WiFi的环境下，可以大大提升效率。只需一次设置，之后就可以享受每次开机即可上网的体验。</p><p>总之，通过这个开机自启动脚本，轻松实现了每次开机自动登录网络的需求。给日常工作和学习生活带来了许多便利。</p><p>最后感谢木子李的代码提供，感谢感谢！！！</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;开机自启动登录-认证前沿&quot;&gt;&lt;a href=&quot;#开机自启动登录-认证前沿&quot; class=&quot;headerlink&quot; title=&quot;开机自启动登录/认证前沿&quot;&gt;&lt;/a&gt;开机自启动登录/认证前沿&lt;/h2&gt;&lt;p&gt;有时候在学校或者在企业的时候，会出现这样一种情况，就是我们需</summary>
      
    
    
    
    <category term="Note" scheme="https://kedreamix.github.io/categories/Note/"/>
    
    
    <category term="Note" scheme="https://kedreamix.github.io/tags/Note/"/>
    
  </entry>
  
  <entry>
    <title>图床转化脚本</title>
    <link href="https://kedreamix.github.io/Note/PicConvert/"/>
    <id>https://kedreamix.github.io/Note/PicConvert/</id>
    <published>2024-01-20T08:10:00.000Z</published>
    <updated>2024-01-21T10:20:56.682Z</updated>
    
    <content type="html"><![CDATA[<p>Github: <a href="https://github.com/imcyx/PicConvert">https://github.com/imcyx/PicConvert</a></p><h2 id="1-安装依赖"><a href="#1-安装依赖" class="headerlink" title="1. 安装依赖"></a>1. 安装依赖</h2><p>该脚本除了python基础依赖库，需要安装 <code>requests</code> 和 <code>requests_toolbelt</code> 两个库：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install requests</span><br><span class="line">pip install requests_toolbelt</span><br></pre></td></tr></tbody></table></figure><p>安装完成后即可正常使用脚本</p><h2 id="2-个人配置"><a href="#2-个人配置" class="headerlink" title="2. 个人配置"></a>2. 个人配置</h2><p>在目录下的<code>configs.py</code>文件里，用户可以对自己的脚本进行配置，下面对配置进行解读：</p><h3 id="a-配置默认使用图床"><a href="#a-配置默认使用图床" class="headerlink" title="a. 配置默认使用图床"></a>a. 配置默认使用图床</h3><p><img src="https://picx.zhimg.com/v2-dbaa44b516d8fc8934cb1239f57779d0.png" alt="配置默认使用图床"></p><p>在<code>configs.py</code>文件的第11行可以配置默认使用的图床网站，而使用的图床<strong>必须从下面5种中选择</strong>。可以设置一种或者多种，按表格样式配置即可。</p><p>这里推荐使用CSDN，因为目前实测不需要频繁更换cookie，可以比较稳定的使用。</p><h3 id="b-配置登录cookie"><a href="#b-配置登录cookie" class="headerlink" title="b. 配置登录cookie"></a>b. 配置登录cookie</h3><p>因为使用的各服务提供商图床需要登录cookie，所以需要用户进入自己的浏览器抓包获得对应字段cookie后填入。</p><p>下面介绍各浏览器cookie的获取方法：</p><h4 id="CSDN"><a href="#CSDN" class="headerlink" title="CSDN"></a>CSDN</h4><p>登录自己的CSDN，然后进入个人中心 (<a href="https://i.csdn.net/">https://i.csdn.net/</a>)，打开浏览器的开发者工具（chrome 默认 <code>ctrl</code>+<code>alt</code>+<code>I</code>），找到<code>UserName</code>和<code>UserToken</code>，将对应的值复制。</p><p><img src="https://pic1.zhimg.com/v2-10ea2f2f64f2fbbef8e61656dee9c1f6.png" alt="CSDN Cookie"></p><p>然后粘贴到第26行的 <code>csdn_cookies</code>内，即完成配置。</p><p><img src="https://pic1.zhimg.com/v2-4a12554481a33bba2f3e3f421a7944b3.png" alt="CSDN Cookie"></p><h4 id="知乎"><a href="#知乎" class="headerlink" title="知乎"></a>知乎</h4><p>登录自己的知乎，然后进入主页 (<a href="https://www.zhihu.com/">https://www.zhihu.com/</a>)，打开浏览器的开发者工具，找到<code>z_c0</code>，将对应的值复制，然后填入33行对应的<code>zhihu_cookies</code>里即完成配置。</p><p><img src="https://picx.zhimg.com/v2-c06388579ca46e0ea942d9292d580878.png" alt="知乎 Cookie"></p><p>知乎的图片默认支持3种，<code>src</code>, <code>watermark_src</code>, <code>original_src</code>，<code>watermark_src</code>是水印原图，<code>original_src</code>是原图，<code>src</code>是展示图，用户可以自己选择。</p><h4 id="b站"><a href="#b站" class="headerlink" title="b站"></a>b站</h4><p>登录自己的b站，然后进入主页 (<a href="https://www.bilibili.com/">https://www.bilibili.com/</a>)，打开浏览器的开发者工具，找到<code>SESSDATA</code>，将对应的值复制，然后填入41行对应的<code>bili_cookies</code>里即完成配置。</p><p><img src="https://pic1.zhimg.com/v2-73f566da43bf13b082b4cc569875bad8.png" alt="Bilibili Cookie"></p><h4 id="简书"><a href="#简书" class="headerlink" title="简书"></a>简书</h4><p>登录自己的简书，然后进入主页 (<a href="https://www.jianshu.com/">https://www.jianshu.com/</a>)，打开浏览器的开发者工具，找到<code>remember_user_token</code>和<code>_m7e_session_core</code>字段，将对应的值复制，然后填入47行对应的<code>jianshu_cookies</code>里即完成配置。</p><p><img src="https://pic1.zhimg.com/v2-ed43bbd15853b83cfe8b4c7ccd473424.png" alt="简书 Cookie"></p><h4 id="博客园"><a href="#博客园" class="headerlink" title="博客园"></a>博客园</h4><p>登录自己的博客园，然后进入主页 (<a href="https://www.cnblogs.com/">https://www.cnblogs.com/</a>)，打开浏览器的开发者工具，找到<code>.Cnblogs.AspNetCore.Cookies</code>字段，将对应的值复制，然后填入53行对应的<code>bokeyuan_cookies</code>里即完成配置。</p><p><img src="https://pica.zhimg.com/v2-2ec98121095723e0b63f0dce75441907.png" alt="博客园 Cookie"></p><h2 id="3-命令行调用"><a href="#3-命令行调用" class="headerlink" title="3.  命令行调用"></a>3.  命令行调用</h2><p>脚本的使用方法为：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python convert.py</span><br></pre></td></tr></tbody></table></figure><p>使用该命令后，默认读取当前脚本所处目录下的所有md文件，并逐个读取扫描图片链接或本地路径，按照配置里指定的转换方式，转换后再输出为{New<em>(mode)</em>(原始名)}。</p><p><img src="https://pica.zhimg.com/v2-d85a797b22394c79616e1645f70047a1.png" alt="使用命令行"></p><p>如果需要指定转化的文件，使用命令：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python convert.py -f new.md</span><br></pre></td></tr></tbody></table></figure><p>而如果不适用默认的转换图床，需要额外指定转换图床，使用命令：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python convert.py -m csdn</span><br></pre></td></tr></tbody></table></figure><p>这两个参数可以同时指定，转换效果如下：</p><p><img src="https://pic1.zhimg.com/v2-56e84953380c8dc11ee8a329c3bc1f5e.png" alt="指定参数"></p>]]></content>
    
    
    <summary type="html">一个适用于多种图床转换的脚本（知乎、b站、简书、CSDN、博客园）</summary>
    
    
    
    <category term="Note" scheme="https://kedreamix.github.io/categories/Note/"/>
    
    
    <category term="Note" scheme="https://kedreamix.github.io/tags/Note/"/>
    
    <category term="Tool" scheme="https://kedreamix.github.io/tags/Tool/"/>
    
  </entry>
  
  <entry>
    <title>数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</title>
    <link href="https://kedreamix.github.io/Project/Linly-Talker%20-%20GPT-SoVITS/"/>
    <id>https://kedreamix.github.io/Project/Linly-Talker%20-%20GPT-SoVITS/</id>
    <published>2024-01-19T16:00:00.000Z</published>
    <updated>2024-02-25T09:09:57.096Z</updated>
    
    <content type="html"><![CDATA[<h1 id="数字人的未来：数字人对话系统-Linly-Talker-克隆语音-GPT-SoVITS"><a href="#数字人的未来：数字人对话系统-Linly-Talker-克隆语音-GPT-SoVITS" class="headerlink" title="数字人的未来：数字人对话系统 Linly-Talker + 克隆语音 GPT-SoVITS"></a>数字人的未来：数字人对话系统 Linly-Talker + 克隆语音 GPT-SoVITS</h1><p><a href="https://github.com/Kedreamix/Linly-Talker">https://github.com/Kedreamix/Linly-Talker</a></p><p><strong>2023.12 更新</strong> 📆</p><p><strong>用户可以上传任意图片进行对话</strong></p><p><strong>2024.01 更新</strong> 📆</p><ul><li><strong>令人兴奋的消息！我现在已经将强大的GeminiPro和Qwen大模型融入到我们的对话场景中。用户现在可以在对话中上传任何图片，为我们的互动增添了全新的层面。</strong></li><li><strong>更新了FastAPI的部署调用方法。</strong> </li><li><strong>更新了微软TTS的高级设置选项，增加声音种类的多样性，以及加入视频字幕加强可视化。</strong><ul><li><strong>更新了GPT多轮对话系统，使得对话有上下文联系，提高数字人的交互性和真实感。</strong></li></ul></li></ul><p><strong>2024.02 更新</strong> 📆</p><ul><li><strong>更新了Gradio的版本为最新版本4.16.0，使得界面拥有更多的功能，比如可以摄像头拍摄图片构建数字人等。</strong></li><li><strong>更新了ASR和THG，其中ASR加入了阿里的FunASR，具体更快的速度；THG部分加入了Wav2Lip模型，ER-NeRF在准备中(Comming Soon)。</strong></li><li><strong>加入了语音克隆方法GPT-SoVITS模型，能够通过微调一分钟对应人的语料进行克隆，效果还是相当不错的，值得推荐。</strong></li><li><strong>集成一个WebUI界面，能够更好的运行Linly-Talker。</strong></li></ul><p>在最近一段时间，我在尝试探索，如何克隆声音，因为在数字人对话系统中，虽然可能能够重建特定的人，但是还是存在一个问题：声音是用固定的人声生成的，导致没有真实性，如果我们能够去克隆出对应的声音，并且结合特定的数字人，那是否就完成了一个数字人的完整复刻。</p><p>于是我就研究了一段时间，后面发现了两个非常有意思的项目，分别是<code>GPT-SoVITS</code>和<code>XTTS</code>两个开源项目，我认为这两个算是现在最好的两个开源项目了，像OpenVoice之类的效果还是比较差，火山效果不错，但是没有开源。</p><p>除此之外，我后续集成到了Linly-Talker之中，做了一个WebUI，能够通过我3~10s的语音大概克隆我的声音，同时也可以使用一分钟克隆训练的语音来操作，如果使用多一点的预料能够得到更好的效果，希望和大家一起努力，成功复刻出一个完整的数字人</p><p>具体也可以关注我B站的演示的视频<a href="https://www.bilibili.com/video/BV1S4421A7gh">🚀数字人的未来：Linly-Talker+GPT-SoVIT语音克隆技术的赋能之道</a>和<a href="https://www.bilibili.com/video/BV1nu4m1K7qG">Linly-Talker WebUI🚀: 在对话时悄悄偷走你的声音🎤</a></p><h2 id="GPT-SoVITS（推荐）"><a href="#GPT-SoVITS（推荐）" class="headerlink" title="GPT-SoVITS（推荐）"></a>GPT-SoVITS（推荐）</h2><p>感谢大家的开源贡献，我借鉴了当前开源的语音克隆模型 <code>GPT-SoVITS</code>，我认为效果是相当不错的，项目地址可参考<a href="https://github.com/RVC-Boss/GPT-SoVITS">https://github.com/RVC-Boss/GPT-SoVITS</a></p><p>他有以下功能：</p><ol><li><strong>零样本文本到语音（TTS）：</strong> 输入 5 秒的声音样本，即刻体验文本到语音转换。</li><li><strong>少样本 TTS：</strong> 仅需 1 分钟的训练数据即可微调模型，提升声音相似度和真实感。</li><li><strong>跨语言支持：</strong> 支持与训练数据集不同语言的推理，目前支持英语、日语和中文。</li><li><strong>WebUI 工具：</strong> 集成工具包括声音伴奏分离、自动训练集分割、中文自动语音识别(ASR)和文本标注，协助初学者创建训练数据集和 GPT/SoVITS 模型。</li></ol><p>之前很多方法都是少样本，比如<code>OpenVoice</code>和<code>XTTS</code>，我之前也想着使用他们来进行实现语音克隆部分，但是很遗憾的是，并没有感觉有很好的效果，其实<code>XTTS</code>还是不错的，如果我们简单用麦克风🎤说几句话作为参考来进行克隆，我觉得效果还是可以的。</p><p>但是如果遇到比较高的要求，我觉得可能就需要更好的模型，并且成本也要打压下来，所以我就看到了这个<code>GPT-SoVITS</code>，我觉得这个模型是相当厉害的，少样本的TTS能做，也能做跨语言支持，这样我们很有可能就可以体验到奥巴马讲中文之类的，这样就可以完成视频翻译的一些任务了，所以我是很推崇这样的简单微调，效果又好的方法的。</p><p><strong>为了尊重作者，在Linly-Talker并没有把<code>GPT-SoVITS</code>的全套代码搬过来，我写了一个关于语音克隆的类，大家可以将训练好的模型参数中，就可以在本项目使用经过语音克隆后的TTS了，希望大家玩的开心，玩的愉快。</strong></p><blockquote><p>如果使用语音克隆模型，可能需要python为3.10，pytorch为2.1左右可能比较好，我的环境已经测试过了，简单来说，先安装GPT-SoVITS的环境，再直接pip intsall -r requirements_app.txt即可使用</p><p>除此之外，还需要根据原作者的说明放入对应路径，我的预训练模型和存放位置已给出，可参考<a href="https://huggingface.co/Kedreamix/Linly-Talker">https://huggingface.co/Kedreamix/Linly-Talker</a></p></blockquote><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">pip install torch==<span class="number">2.1</span><span class="number">.0</span> torchvision==<span class="number">0.16</span><span class="number">.0</span> torchaudio==<span class="number">2.1</span><span class="number">.0</span> --index-url https://download.pytorch.org/whl/cu118</span><br><span class="line"><span class="comment"># 安装对应的依赖</span></span><br><span class="line">pip install -r VITS/requirements_gptsovits.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动如下的WebUI界面</span></span><br><span class="line">python VITS/app.py </span><br></pre></td></tr></tbody></table></figure><p><img src="C:/Users/Kedreamix/Documents/GitHub/Linly-Talker/docs/GPT-SoVITS.png" alt=""></p><h2 id="Coqui-XTTS"><a href="#Coqui-XTTS" class="headerlink" title="Coqui XTTS"></a>Coqui XTTS</h2><p>Coqui XTTS是一个领先的深度学习文本到语音任务（TTS语音生成模型）工具包，通过使用一段5秒钟以上的语音频剪辑就可以完成声音克隆<em>将语音克隆到不同的语言</em>。支持多种语言文本到语音转换，使其成为国际化应用的理想选择，这一特点特别适用于全球化的市场，其中需要生成多种语言的语音内容。所以在实验过程中，我也加入了这一部分，不过暂时使用的是默认的模型，并没有进行微调，个人认为是没有GPT-SoVITS经过微调后好的，但是其中的少样本五秒钟克隆语音还是值得称赞的。大家也可以在官方的在线体验，但是官方的可能会有生成语音限制，文字不能太长，但是还是足够我们体验了。</p><p>🐸TTS 是一个用于高级文本转语音生成的库。</p><p>🚀 超过 1100 种语言的预训练模型。</p><p>🛠️ 用于以任何语言训练新模型和微调现有模型的工具。</p><p>📚 用于数据集分析和管理的实用程序。</p><ul><li>在线体验XTTS <a href="https://huggingface.co/spaces/coqui/xtts">https://huggingface.co/spaces/coqui/xtts</a></li><li>官方Github库 <a href="https://github.com/coqui-ai/TTS">https://github.com/coqui-ai/TTS</a></li></ul><p>XTTS的环境也需要PyTorch 2.1所以，如果下载了GPT-SoVITS，也不妨体验一下XTTS的效果。</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu118</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装对应的依赖</span></span><br><span class="line">pip install -r VITS/requirements_xtts.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动如下的WebUI界面</span></span><br><span class="line">python VITS/XTTS.py</span><br></pre></td></tr></tbody></table></figure><p><img src="C:/Users/Kedreamix/Documents/GitHub/Linly-Talker/docs/XTTS.png" alt=""></p><h2 id="Linly-Talker-WebUI"><a href="#Linly-Talker-WebUI" class="headerlink" title="Linly-Talker WebUI"></a>Linly-Talker WebUI</h2><p>之前我将很多个版本都是分开来的，实际上运行多个会比较麻烦，所以后续我增加了变成WebUI一个界面即可体验，后续也会不断更新</p><p>现在已加入WebUI的功能如下</p><ul><li>[x] 文本/语音数字人对话（固定数字人，分男女角色）</li><li>[x] 任意图片数字人对话（可上传任意数字人）</li><li>[x] 多轮GPT对话（加入历史对话数据，链接上下文）</li><li>[x] 语音克隆对话（基于GPT-SoVITS设置进行语音克隆，内置烟嗓音，可根据语音对话的声音进行克隆）</li></ul><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># WebUI</span></span><br><span class="line">python webui.py</span><br></pre></td></tr></tbody></table></figure><p><img src="C:/Users/Kedreamix/Documents/GitHub/Linly-Talker/docs/WebUI.png" alt=""></p>]]></content>
    
    
    <summary type="html">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</summary>
    
    
    
    <category term="Project" scheme="https://kedreamix.github.io/categories/Project/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
    <category term="LLM" scheme="https://kedreamix.github.io/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</title>
    <link href="https://kedreamix.github.io/Project/Linly-Talker/"/>
    <id>https://kedreamix.github.io/Project/Linly-Talker/</id>
    <published>2024-01-19T16:00:00.000Z</published>
    <updated>2024-02-04T10:35:10.909Z</updated>
    
    <content type="html"><![CDATA[<p><strong>2023.12 更新</strong> 📆</p><p><strong>用户可以上传任意图片进行对话</strong></p><p><strong>2024.01 更新</strong> 📆</p><ul><li><strong>令人兴奋的消息！我现在已经将强大的GeminiPro和Qwen大模型融入到我们的对话场景中。用户现在可以在对话中上传任何图片，为我们的互动增添了全新的层面。</strong></li><li><strong>更新了FastAPI的部署调用方法。</strong> </li><li><strong>更新了微软TTS的高级设置选项，增加声音种类的多样性，以及加入视频字幕加强可视化。</strong></li><li><strong>更新了GPT多轮对话系统，使得对话有上下文联系，提高数字人的交互性和真实感</strong></li></ul><p><strong>2024.02 更新</strong> 📆</p><ul><li><strong>更新了Gradio的版本为最新版本4.16.0，使得界面拥有更多的功能，比如可以摄像头拍摄图片构建数字人等</strong></li><li><strong>更新了ASR和THG，其中ASR加入了阿里的FunASR，具体更快的速度；THG部分加入了Wav2Lip模型，ER-NeRF在准备中(Comming Soon)</strong></li></ul><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</p><p><img src="https://picx.zhimg.com/80/v2-b38722d9d71153dec12acbb9e020a5b4.png" alt="The system architecture of multimodal human–computer interaction."></p><h2 id="TO-TO-DO-LIST"><a href="#TO-TO-DO-LIST" class="headerlink" title="TO## TO DO LIST"></a>TO## TO DO LIST</h2><ul><li>[x] 基本完成对话系统流程，能够<code>语音对话</code></li><li>[x] 加入了LLM大模型，包括<code>Linly</code>，<code>Qwen</code>和<code>GeminiPro</code>的使用</li><li>[x] 可上传<code>任意数字人照片</code>进行对话</li><li>[x] Linly加入<code>FastAPI</code>调用方式</li><li>[x] 利用微软<code>TTS</code>加入高级选项，可设置对应人声以及音调等参数，增加声音的多样性</li><li>[x] 视频生成加入<code>字幕</code>，能够更好的进行可视化</li><li>[x] GPT<code>多轮对话</code>系统（提高数字人的交互性和真实感，增强数字人的智能）</li><li>[x] 优化Gradio界面，加入更多模型，如Wav2Lip，FunASR等</li><li>[ ] <code>语音克隆</code>技术（语音克隆合成自己声音，提高数字人分身的真实感和互动体验）</li><li>[ ] 加入<code>Langchain</code>的框架，建立本地知识库</li><li>[ ] <code>实时</code>语音识别（人与数字人之间就可以通过语音进行对话交流)</li></ul><p>🔆 该项目 Linly-Talker 正在进行中 - 欢迎提出PR请求！如果您有任何关于新的模型方法、研究、技术或发现运行错误的建议，请随时编辑并提交 PR。您也可以打开一个问题或通过电子邮件直接联系我。📩⭐ 如果您发现这个Github Project有用，请给它点个星！🤩</p><h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><div class="table-container"><table><thead><tr><th style="text-align:center">文字/语音对话</th><th style="text-align:center">数字人回答</th></tr></thead><tbody><tr><td style="text-align:center">应对压力最有效的方法是什么？</td><td style="text-align:center"><video src="https://github.com/Kedreamix/Linly-Talker/assets/61195303/f1deb189-b682-4175-9dea-7eeb0fb392ca"></video></td></tr><tr><td style="text-align:center">如何进行时间管理？</td><td style="text-align:center"><video src="https://github.com/Kedreamix/Linly-Talker/assets/61195303/968b5c43-4dce-484b-b6c6-0fd4d621ac03"></video></td></tr><tr><td style="text-align:center">撰写一篇交响乐音乐会评论，讨论乐团的表演和观众的整体体验。</td><td style="text-align:center"><video src="https://github.com/Kedreamix/Linly-Talker/assets/61195303/f052820f-6511-4cf0-a383-daf8402630db"></video></td></tr><tr><td style="text-align:center">翻译成中文：Luck is a dividend of sweat. The more you sweat, the luckier you get.</td><td style="text-align:center"><video src="https://github.com/Kedreamix/Linly-Talker/assets/61195303/118eec13-a9f7-4c38-b4ad-044d36ba9776"></video></td></tr></tbody></table></div><h2 id="创建环境"><a href="#创建环境" class="headerlink" title="创建环境"></a>创建环境</h2><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">conda create -n linly python=3.9 </span><br><span class="line">conda activate linly</span><br><span class="line"></span><br><span class="line"><span class="comment"># pytorch安装方式1：conda安装（推荐）</span></span><br><span class="line">conda install pytorch==1.12.1 torchvision==0.13.1 torchaudio==0.12.1 cudatoolkit=11.3 -c pytorch</span><br><span class="line"></span><br><span class="line"><span class="comment"># pytorch安装方式2：pip 安装</span></span><br><span class="line">pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113</span><br><span class="line"></span><br><span class="line">conda install -q ffmpeg <span class="comment"># ffmpeg==4.2.2</span></span><br><span class="line"></span><br><span class="line">pip install -r requirements_app.txt</span><br></pre></td></tr></tbody></table></figure><p>为了大家的部署使用方便，更新了一个<code>configs.py</code>文件，可以对其进行一些超参数修改即可</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设备运行端口 (Device running port)</span></span><br><span class="line">port = 7870</span><br><span class="line"><span class="comment"># api运行端口及IP (API running port and IP)</span></span><br><span class="line">ip = <span class="string">'127.0.0.1'</span> </span><br><span class="line">api_port = 7871</span><br><span class="line"><span class="comment"># Linly模型路径 (Linly model path)</span></span><br><span class="line">mode = <span class="string">'api'</span> <span class="comment"># api 需要先运行Linly-api-fast.py</span></span><br><span class="line">mode = <span class="string">'offline'</span></span><br><span class="line">model_path = <span class="string">'Linly-AI/Chinese-LLaMA-2-7B-hf'</span></span><br><span class="line"><span class="comment"># ssl证书 (SSL certificate) 麦克风对话需要此参数</span></span><br><span class="line">ssl_certfile = <span class="string">"/path/to/Linly-Talker/https_cert/cert.pem"</span></span><br><span class="line">ssl_keyfile = <span class="string">"/path/to/Linly-Talker/https_cert/key.pem"</span></span><br></pre></td></tr></tbody></table></figure><h2 id="ASR-Speech-Recognition"><a href="#ASR-Speech-Recognition" class="headerlink" title="ASR - Speech Recognition"></a>ASR - Speech Recognition</h2><h3 id="Whisper"><a href="#Whisper" class="headerlink" title="Whisper"></a>Whisper</h3><p>借鉴OpenAI的Whisper实现了ASR的语音识别，具体使用方法参考 <a href="https://github.com/openai/whisper">https://github.com/openai/whisper</a></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">https://github.com/openai/whisper</span></span><br><span class="line"><span class="string">pip install -U openai-whisper</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">import</span> whisper</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">WhisperASR</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model_path</span>):</span><br><span class="line">        self.LANGUAGES = {</span><br><span class="line">            <span class="string">"en"</span>: <span class="string">"english"</span>,</span><br><span class="line">            <span class="string">"zh"</span>: <span class="string">"chinese"</span>,</span><br><span class="line">        }</span><br><span class="line">        self.model = whisper.load_model(model_path)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">transcribe</span>(<span class="params">self, audio_file</span>):</span><br><span class="line">        result = self.model.transcribe(audio_file)</span><br><span class="line">        <span class="keyword">return</span> result[<span class="string">"text"</span>]</span><br></pre></td></tr></tbody></table></figure><h3 id="FunASR"><a href="#FunASR" class="headerlink" title="FunASR"></a>FunASR</h3><p>阿里的<code>FunASR</code>的语音识别效果也是相当不错，而且时间也是比whisper更快的，更能达到实时的效果，所以也将FunASR添加进去了，在ASR文件夹下的FunASR文件里可以进行体验，需要注意的是，在第一次运行的时候，需要安装以下库，参考 <a href="https://github.com/alibaba-damo-academy/FunASR">https://github.com/alibaba-damo-academy/FunASR</a></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip install funasr</span><br><span class="line">pip install modelscope</span><br><span class="line">pip install -U rotary_embedding_torch</span><br></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">Reference: https://github.com/alibaba-damo-academy/FunASR</span></span><br><span class="line"><span class="string">pip install funasr</span></span><br><span class="line"><span class="string">pip install modelscope</span></span><br><span class="line"><span class="string">pip install -U rotary_embedding_torch</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">from</span> funasr <span class="keyword">import</span> AutoModel</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"如果想使用FunASR，请先安装funasr，若使用Whisper，请忽略此条信息"</span>)   </span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FunASR</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        self.model = AutoModel(model=<span class="string">"paraformer-zh"</span>, model_revision=<span class="string">"v2.0.4"</span>,</span><br><span class="line">                vad_model=<span class="string">"fsmn-vad"</span>, vad_model_revision=<span class="string">"v2.0.4"</span>,</span><br><span class="line">                punc_model=<span class="string">"ct-punc-c"</span>, punc_model_revision=<span class="string">"v2.0.4"</span>,</span><br><span class="line">                <span class="comment"># spk_model="cam++", spk_model_revision="v2.0.2",</span></span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">transcribe</span>(<span class="params">self, audio_file</span>):</span><br><span class="line">        res = self.model.generate(<span class="built_in">input</span>=audio_file, </span><br><span class="line">            batch_size_s=<span class="number">300</span>)</span><br><span class="line">        <span class="built_in">print</span>(res)</span><br><span class="line">        <span class="keyword">return</span> res[<span class="number">0</span>][<span class="string">'text'</span>]</span><br></pre></td></tr></tbody></table></figure><h2 id="TTS-Edge-TTS"><a href="#TTS-Edge-TTS" class="headerlink" title="TTS - Edge TTS"></a>TTS - Edge TTS</h2><p>使用微软语音服务,具体使用方法参考<a href="https://github.com/rany2/edge-tts">https://github.com/rany2/edge-tts</a></p><p>我编写了一个 <code>EdgeTTS</code> 的类，能够更好的使用，并且增加了保存字幕文件的功能</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EdgeTTS</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, list_voices = <span class="literal">False</span>, proxy = <span class="literal">None</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        voices = list_voices_fn(proxy=proxy)</span><br><span class="line">        self.SUPPORTED_VOICE = [item[<span class="string">'ShortName'</span>] <span class="keyword">for</span> item <span class="keyword">in</span> voices]</span><br><span class="line">        self.SUPPORTED_VOICE.sort(reverse=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">if</span> list_voices:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">", "</span>.join(self.SUPPORTED_VOICE))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">preprocess</span>(<span class="params">self, rate, volume, pitch</span>):</span><br><span class="line">        <span class="keyword">if</span> rate &gt;= <span class="number">0</span>:</span><br><span class="line">            rate = <span class="string">f'+<span class="subst">{rate}</span>%'</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            rate = <span class="string">f'<span class="subst">{rate}</span>%'</span></span><br><span class="line">        <span class="keyword">if</span> pitch &gt;= <span class="number">0</span>:</span><br><span class="line">            pitch = <span class="string">f'+<span class="subst">{pitch}</span>Hz'</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            pitch = <span class="string">f'<span class="subst">{pitch}</span>Hz'</span></span><br><span class="line">        volume = <span class="number">100</span> - volume</span><br><span class="line">        volume = <span class="string">f'-<span class="subst">{volume}</span>%'</span></span><br><span class="line">        <span class="keyword">return</span> rate, volume, pitch</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self,TEXT, VOICE, RATE, VOLUME, PITCH, OUTPUT_FILE=<span class="string">'result.wav'</span>, OUTPUT_SUBS=<span class="string">'result.vtt'</span>, words_in_cue = <span class="number">8</span></span>):</span><br><span class="line">        <span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">amain</span>() -&gt; <span class="literal">None</span>:</span><br><span class="line">            <span class="string">"""Main function"""</span></span><br><span class="line">            rate, volume, pitch = self.preprocess(rate = RATE, volume = VOLUME, pitch = PITCH)</span><br><span class="line">            communicate = Communicate(TEXT, VOICE, rate = rate, volume = volume, pitch = pitch)</span><br><span class="line">            subs: SubMaker = SubMaker()</span><br><span class="line">            sub_file: <span class="type">Union</span>[TextIOWrapper, TextIO] = (</span><br><span class="line">                <span class="built_in">open</span>(OUTPUT_SUBS, <span class="string">"w"</span>, encoding=<span class="string">"utf-8"</span>)</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">async</span> <span class="keyword">for</span> chunk <span class="keyword">in</span> communicate.stream():</span><br><span class="line">                <span class="keyword">if</span> chunk[<span class="string">"type"</span>] == <span class="string">"audio"</span>:</span><br><span class="line">                    <span class="comment"># audio_file.write(chunk["data"])</span></span><br><span class="line">                    <span class="keyword">pass</span></span><br><span class="line">                <span class="keyword">elif</span> chunk[<span class="string">"type"</span>] == <span class="string">"WordBoundary"</span>:</span><br><span class="line">                    <span class="comment"># print((chunk["offset"], chunk["duration"]), chunk["text"])</span></span><br><span class="line">                    subs.create_sub((chunk[<span class="string">"offset"</span>], chunk[<span class="string">"duration"</span>]), chunk[<span class="string">"text"</span>])</span><br><span class="line">            sub_file.write(subs.generate_subs(words_in_cue))</span><br><span class="line">            <span class="keyword">await</span> communicate.save(OUTPUT_FILE)</span><br><span class="line">            </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># loop = asyncio.get_event_loop_policy().get_event_loop()</span></span><br><span class="line">        <span class="comment"># try:</span></span><br><span class="line">        <span class="comment">#     loop.run_until_complete(amain())</span></span><br><span class="line">        <span class="comment"># finally:</span></span><br><span class="line">        <span class="comment">#     loop.close()</span></span><br><span class="line">        asyncio.run(amain())</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(OUTPUT_SUBS, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> file:</span><br><span class="line">            vtt_lines = file.readlines()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 去掉每一行文字中的空格</span></span><br><span class="line">        vtt_lines_without_spaces = [line.replace(<span class="string">" "</span>, <span class="string">""</span>) <span class="keyword">if</span> <span class="string">"--&gt;"</span> <span class="keyword">not</span> <span class="keyword">in</span> line <span class="keyword">else</span> line <span class="keyword">for</span> line <span class="keyword">in</span> vtt_lines]</span><br><span class="line">        <span class="comment"># print(vtt_lines_without_spaces)</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(OUTPUT_SUBS, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> output_file:</span><br><span class="line">            output_file.writelines(vtt_lines_without_spaces)</span><br><span class="line">        <span class="keyword">return</span> OUTPUT_FILE, OUTPUT_SUBS</span><br></pre></td></tr></tbody></table></figure><p>同时在<code>src</code>文件夹下，写了一个简易的<code>WebUI</code></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python TTS_app.py</span><br></pre></td></tr></tbody></table></figure><p><img src="https://picx.zhimg.com/80/v2-570f3c9a069358e4d4a7b7c008e99cb7.png" alt="TTS"></p><h2 id="THG-Avatar"><a href="#THG-Avatar" class="headerlink" title="THG - Avatar"></a>THG - Avatar</h2><h3 id="SadTalker"><a href="#SadTalker" class="headerlink" title="SadTalker"></a>SadTalker</h3><p>数字人生成可使用SadTalker（CVPR 2023）,详情介绍见 <a href="https://sadtalker.github.io">https://sadtalker.github.io</a></p><p>在使用前先下载SadTalker模型:</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash scripts/sadtalker_download_models.sh  </span><br></pre></td></tr></tbody></table></figure><p><a href="https://pan.baidu.com/s/1eF13O-8wyw4B3MtesctQyg?pwd=linl">Baidu (百度云盘)</a> (Password: <code>linl</code>)</p><blockquote><p>如果百度网盘下载，记住是放在checkpoints文件夹下，百度网盘下载的默认命名为sadtalker，实际应该重命名为checkpoints</p></blockquote><h3 id="Wav2Lip"><a href="#Wav2Lip" class="headerlink" title="Wav2Lip"></a>Wav2Lip</h3><p>数字人生成还可使用Wav2Lip（ACM 2020），详情介绍见 <a href="https://github.com/Rudrabha/Wav2Lip">https://github.com/Rudrabha/Wav2Lip</a></p><p>在使用前先下载Wav2Lip模型：</p><div class="table-container"><table><thead><tr><th>Model</th><th>Description</th><th>Link to the model</th></tr></thead><tbody><tr><td>Wav2Lip</td><td>Highly accurate lip-sync</td><td><a href="https://iiitaphyd-my.sharepoint.com/:u:/g/personal/radrabha_m_research_iiit_ac_in/Eb3LEzbfuKlJiR600lQWRxgBIY27JZg80f7V9jtMfbNDaQ?e=TBFBVW">Link</a></td></tr><tr><td>Wav2Lip + GAN</td><td>Slightly inferior lip-sync, but better visual quality</td><td><a href="https://iiitaphyd-my.sharepoint.com/:u:/g/personal/radrabha_m_research_iiit_ac_in/EdjI7bZlgApMqsVoEUUXpLsBxqXbn5z8VTmoxp55YNDcIA?e=n9ljGW">Link</a></td></tr><tr><td>Expert Discriminator</td><td>Weights of the expert discriminator</td><td><a href="https://iiitaphyd-my.sharepoint.com/:u:/g/personal/radrabha_m_research_iiit_ac_in/EQRvmiZg-HRAjvI6zqN9eTEBP74KefynCwPWVmF57l-AYA?e=ZRPHKP">Link</a></td></tr><tr><td>Visual Quality Discriminator</td><td>Weights of the visual disc trained in a GAN setup</td><td><a href="https://iiitaphyd-my.sharepoint.com/:u:/g/personal/radrabha_m_research_iiit_ac_in/EQVqH88dTm1HjlK11eNba5gBbn15WMS0B0EZbDBttqrqkg?e=ic0ljo">Link</a></td></tr></tbody></table></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Wav2Lip</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, path = <span class="string">'checkpoints/wav2lip.pth'</span></span>):</span><br><span class="line">        self.fps = <span class="number">25</span></span><br><span class="line">        self.resize_factor = <span class="number">1</span></span><br><span class="line">        self.mel_step_size = <span class="number">16</span></span><br><span class="line">        self.static = <span class="literal">False</span></span><br><span class="line">        self.img_size = <span class="number">96</span></span><br><span class="line">        self.face_det_batch_size = <span class="number">2</span></span><br><span class="line">        self.box = [-<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>]</span><br><span class="line">        self.pads = [<span class="number">0</span>, <span class="number">10</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">        self.nosmooth = <span class="literal">False</span></span><br><span class="line">        self.device = <span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span></span><br><span class="line">        self.model = self.load_model(path)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">load_model</span>(<span class="params">self, checkpoint_path</span>):</span><br><span class="line">        model = wav2lip_mdoel()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"Load checkpoint from: {}"</span>.<span class="built_in">format</span>(checkpoint_path))</span><br><span class="line">        <span class="keyword">if</span> self.device == <span class="string">'cuda'</span>:</span><br><span class="line">            checkpoint = torch.load(checkpoint_path)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            checkpoint = torch.load(checkpoint_path,</span><br><span class="line">                                    map_location=<span class="keyword">lambda</span> storage, loc: storage)</span><br><span class="line">        s = checkpoint[<span class="string">"state_dict"</span>]</span><br><span class="line">        new_s = {}</span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> s.items():</span><br><span class="line">            new_s[k.replace(<span class="string">'module.'</span>, <span class="string">''</span>)] = v</span><br><span class="line">        model.load_state_dict(new_s)</span><br><span class="line"></span><br><span class="line">        model = model.to(self.device)</span><br><span class="line">        <span class="keyword">return</span> model.<span class="built_in">eval</span>()</span><br></pre></td></tr></tbody></table></figure><h3 id="ER-NeRF（Comming-Soon）"><a href="#ER-NeRF（Comming-Soon）" class="headerlink" title="ER-NeRF（Comming Soon）"></a>ER-NeRF（Comming Soon）</h3><p>ER-NeRF（ICCV2023）是使用最新的NeRF技术构建的数字人，拥有定制数字人的特性，只需要一个人的五分钟左右到视频即可重建出来，具体可参考 <a href="https://github.com/Fictionarry/ER-NeRF">https://github.com/Fictionarry/ER-NeRF</a></p><p>后续会针对此更新</p><h2 id="LLM-Conversation"><a href="#LLM-Conversation" class="headerlink" title="LLM - Conversation"></a>LLM - Conversation</h2><h3 id="Linly-AI"><a href="#Linly-AI" class="headerlink" title="Linly-AI"></a>Linly-AI</h3><p>Linly来自深圳大学数据工程国家重点实验室,参考<a href="https://github.com/CVI-SZU/Linly">https://github.com/CVI-SZU/Linly</a></p><p>下载Linly模型:<a href="https://huggingface.co/Linly-AI/Chinese-LLaMA-2-7B-hf">https://huggingface.co/Linly-AI/Chinese-LLaMA-2-7B-hf</a></p><p>可以使用<code>git</code>下载</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git lfs install</span><br><span class="line">git <span class="built_in">clone</span> https://huggingface.co/Linly-AI/Chinese-LLaMA-2-7B-hf</span><br></pre></td></tr></tbody></table></figure><p>或者使用<code>huggingface</code>的下载工具<code>huggingface-cli</code></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">pip install -U huggingface_hub</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置镜像加速</span></span><br><span class="line"><span class="comment"># Linux</span></span><br><span class="line"><span class="built_in">export</span> HF_ENDPOINT=<span class="string">"https://hf-mirror.com"</span></span><br><span class="line"><span class="comment"># windows powershell</span></span><br><span class="line"><span class="variable">$env</span>:HF_ENDPOINT=<span class="string">"https://hf-mirror.com"</span></span><br><span class="line"></span><br><span class="line">huggingface-cli download --resume-download Linly-AI/Chinese-LLaMA-2-7B-hf --local-dir Linly-AI/Chinese-LLaMA-2-7B-hf</span><br></pre></td></tr></tbody></table></figure><p>或使用API:</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 命令行</span></span><br><span class="line">curl -X POST -H <span class="string">"Content-Type: application/json"</span> -d <span class="string">'{"question": "北京有什么好玩的地方?"}'</span> http://url:port  </span><br><span class="line"></span><br><span class="line"><span class="comment"># Python</span></span><br><span class="line">import requests</span><br><span class="line"></span><br><span class="line">url = <span class="string">"http://url:port"</span></span><br><span class="line">headers = {</span><br><span class="line">  <span class="string">"Content-Type"</span>: <span class="string">"application/json"</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">data = {</span><br><span class="line">  <span class="string">"question"</span>: <span class="string">"北京有什么好玩的地方?"</span> </span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">response = requests.post(url, headers=headers, json=data)</span><br><span class="line"><span class="comment"># response_text = response.content.decode("utf-8")</span></span><br><span class="line">answer, tag = response.json()</span><br><span class="line"><span class="comment"># print(answer)</span></span><br><span class="line"><span class="keyword">if</span> tag == <span class="string">'success'</span>:</span><br><span class="line">    response_text =  answer[0]</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"fail"</span>)</span><br><span class="line"><span class="built_in">print</span>(response_text)</span><br></pre></td></tr></tbody></table></figure><p>API部署推荐<strong>FastAPI</strong>，现在更新了 FastAPI 的API使用版本，FastAPI 是一个高性能、易用且现代的Python Web 框架，它通过使用最新的Python 特性和异步编程，提供了快速开发Web API 的能力。 该框架不仅易于学习和使用，还具有自动生成文档、数据验证等强大功能。 无论是构建小型项目还是大型应用程序，FastAPI 都是一个强大而有效的工具。</p><p>首先安装部署API所使用的库</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install fastapi==0.104.1</span><br><span class="line">pip install uvicorn==0.24.0.post1</span><br></pre></td></tr></tbody></table></figure><p>其他使用方法大致相同，主要是不同代码实现方式，会更加简单边界，并且处理并发也会更好</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fastapi <span class="keyword">import</span> FastAPI, Request</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForCausalLM, GenerationConfig</span><br><span class="line"><span class="keyword">import</span> uvicorn</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> configs <span class="keyword">import</span> model_path, api_port</span><br><span class="line"><span class="comment"># 设置设备参数</span></span><br><span class="line">DEVICE = <span class="string">"cuda"</span>  <span class="comment"># 使用CUDA</span></span><br><span class="line">DEVICE_ID = <span class="string">"0"</span>  <span class="comment"># CUDA设备ID，如果未设置则为空</span></span><br><span class="line">CUDA_DEVICE = <span class="string">f"<span class="subst">{DEVICE}</span>:<span class="subst">{DEVICE_ID}</span>"</span> <span class="keyword">if</span> DEVICE_ID <span class="keyword">else</span> DEVICE  <span class="comment"># 组合CUDA设备信息</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 清理GPU内存函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">torch_gc</span>():</span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():  <span class="comment"># 检查是否可用CUDA</span></span><br><span class="line">        <span class="keyword">with</span> torch.cuda.device(CUDA_DEVICE):  <span class="comment"># 指定CUDA设备</span></span><br><span class="line">            torch.cuda.empty_cache()  <span class="comment"># 清空CUDA缓存</span></span><br><span class="line">            torch.cuda.ipc_collect()  <span class="comment"># 收集CUDA内存碎片</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建FastAPI应用</span></span><br><span class="line">app = FastAPI()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理POST请求的端点</span></span><br><span class="line"><span class="meta">@app.post(<span class="params"><span class="string">"/"</span></span>)</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">create_item</span>(<span class="params">request: Request</span>):</span><br><span class="line">    <span class="keyword">global</span> model, tokenizer  <span class="comment"># 声明全局变量以便在函数内部使用模型和分词器</span></span><br><span class="line">    json_post_raw = <span class="keyword">await</span> request.json()  <span class="comment"># 获取POST请求的JSON数据</span></span><br><span class="line">    json_post = json.dumps(json_post_raw)  <span class="comment"># 将JSON数据转换为字符串</span></span><br><span class="line">    json_post_list = json.loads(json_post)  <span class="comment"># 将字符串转换为Python对象</span></span><br><span class="line">    prompt = json_post_list.get(<span class="string">'prompt'</span>)  <span class="comment"># 获取请求中的提示</span></span><br><span class="line">    history = json_post_list.get(<span class="string">'history'</span>)  <span class="comment"># 获取请求中的历史记录</span></span><br><span class="line">    max_length = json_post_list.get(<span class="string">'max_length'</span>)  <span class="comment"># 获取请求中的最大长度</span></span><br><span class="line">    top_p = json_post_list.get(<span class="string">'top_p'</span>)  <span class="comment"># 获取请求中的top_p参数</span></span><br><span class="line">    temperature = json_post_list.get(<span class="string">'temperature'</span>)  <span class="comment"># 获取请求中的温度参数</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 调用模型进行对话生成</span></span><br><span class="line">    prompt = <span class="string">f"请用少于25个字回答以下问题 ### Instruction:<span class="subst">{prompt}</span>  ### Response:"</span></span><br><span class="line">    inputs = tokenizer(prompt, return_tensors=<span class="string">"pt"</span>).to(<span class="string">"cuda:0"</span>)</span><br><span class="line">    generate_ids = model.generate(inputs.input_ids, </span><br><span class="line">                                  max_new_tokens=max_length <span class="keyword">if</span> max_length <span class="keyword">else</span> <span class="number">2048</span>,</span><br><span class="line">                                  do_sample=<span class="literal">True</span>, </span><br><span class="line">                                  top_k=<span class="number">20</span>,</span><br><span class="line">                                  top_p=top_p,</span><br><span class="line">                                  temperature=temperature <span class="keyword">if</span> temperature <span class="keyword">else</span> <span class="number">0.84</span>,</span><br><span class="line">                                  repetition_penalty=<span class="number">1.15</span>, eos_token_id=<span class="number">2</span>, bos_token_id=<span class="number">1</span>,pad_token_id=<span class="number">0</span>)</span><br><span class="line">    response = tokenizer.batch_decode(generate_ids, skip_special_tokens=<span class="literal">True</span>, clean_up_tokenization_spaces=<span class="literal">False</span>)[<span class="number">0</span>]</span><br><span class="line">    response = response.split(<span class="string">"### Response:"</span>)[-<span class="number">1</span>]</span><br><span class="line">    now = datetime.datetime.now()  <span class="comment"># 获取当前时间</span></span><br><span class="line">    time = now.strftime(<span class="string">"%Y-%m-%d %H:%M:%S"</span>)  <span class="comment"># 格式化时间为字符串</span></span><br><span class="line">    <span class="comment"># 构建响应JSON</span></span><br><span class="line">    answer = {</span><br><span class="line">        <span class="string">"response"</span>: response,</span><br><span class="line">        <span class="comment"># "history": history,</span></span><br><span class="line">        <span class="string">"status"</span>: <span class="number">200</span>,</span><br><span class="line">        <span class="string">"time"</span>: time</span><br><span class="line">    }</span><br><span class="line">    <span class="comment"># 构建日志信息</span></span><br><span class="line">    log = <span class="string">"["</span> + time + <span class="string">"] "</span> + <span class="string">'", prompt:"'</span> + prompt + <span class="string">'", response:"'</span> + <span class="built_in">repr</span>(response) + <span class="string">'"'</span></span><br><span class="line">    <span class="built_in">print</span>(log)  <span class="comment"># 打印日志</span></span><br><span class="line">    torch_gc()  <span class="comment"># 执行GPU内存清理</span></span><br><span class="line">    <span class="keyword">return</span> answer  <span class="comment"># 返回响应</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 主函数入口</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># 加载预训练的分词器和模型</span></span><br><span class="line">    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=<span class="string">"cuda:0"</span>,</span><br><span class="line">                                                    torch_dtype=torch.bfloat16, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line">    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=<span class="literal">False</span>, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line">    model.<span class="built_in">eval</span>()  <span class="comment"># 设置模型为评估模式</span></span><br><span class="line">    <span class="comment"># 启动FastAPI应用</span></span><br><span class="line">    uvicorn.run(app, host=<span class="string">'0.0.0.0'</span>, port=api_port, workers=<span class="number">1</span>)  <span class="comment"># 在指定端口和主机上启动应用</span></span><br></pre></td></tr></tbody></table></figure><p>默认部署在 7871 端口，通过 POST 方法进行调用，可以使用curl调用，如下所示：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">curl -X POST <span class="string">"http://127.0.0.1:7871"</span> \</span><br><span class="line">     -H <span class="string">'Content-Type: application/json'</span> \</span><br><span class="line">     -d <span class="string">'{"prompt": "如何应对压力"}'</span></span><br></pre></td></tr></tbody></table></figure><p>也可以使用python中的requests库进行调用，如下所示：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_completion</span>(<span class="params">prompt</span>):</span><br><span class="line">    headers = {<span class="string">'Content-Type'</span>: <span class="string">'application/json'</span>}</span><br><span class="line">    data = {<span class="string">"prompt"</span>: prompt}</span><br><span class="line">    response = requests.post(url=<span class="string">'http://127.0.0.1:7871'</span>, headers=headers, data=json.dumps(data))</span><br><span class="line">    <span class="keyword">return</span> response.json()[<span class="string">'response'</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="built_in">print</span>(get_completion(<span class="string">'你好如何应对压力'</span>))</span><br></pre></td></tr></tbody></table></figure><p>得到的返回值如下所示：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">{</span><br><span class="line">  <span class="string">"response"</span>:<span class="string">"寻求支持和放松，并采取积极的措施解决问题。"</span>,</span><br><span class="line">  <span class="string">"status"</span>:200,</span><br><span class="line">  <span class="string">"time"</span>:<span class="string">"2024-01-12 01:43:37"</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h3 id="Qwen"><a href="#Qwen" class="headerlink" title="Qwen"></a>Qwen</h3><p>来自阿里云的Qwen，查看 <a href="https://github.com/QwenLM/Qwen">https://github.com/QwenLM/Qwen</a></p><p>下载 Qwen 模型: <a href="https://huggingface.co/Qwen/Qwen-1_8B-Chat">https://huggingface.co/Qwen/Qwen-1_8B-Chat</a></p><p>可以使用<code>git</code>下载</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git lfs install</span><br><span class="line">git <span class="built_in">clone</span> https://huggingface.co/Qwen/Qwen-1_8B-Chat</span><br></pre></td></tr></tbody></table></figure><p>或者使用<code>huggingface</code>的下载工具<code>huggingface-cli</code></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">pip install -U huggingface_hub</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置镜像加速</span></span><br><span class="line"><span class="comment"># Linux</span></span><br><span class="line"><span class="built_in">export</span> HF_ENDPOINT=<span class="string">"https://hf-mirror.com"</span></span><br><span class="line"><span class="comment"># windows powershell</span></span><br><span class="line"><span class="variable">$env</span>:HF_ENDPOINT=<span class="string">"https://hf-mirror.com"</span></span><br><span class="line"></span><br><span class="line">huggingface-cli download --resume-download Qwen/Qwen-1_8B-Chat --local-dir Qwen/Qwen-1_8B-Chat</span><br></pre></td></tr></tbody></table></figure><h3 id="Gemini-Pro"><a href="#Gemini-Pro" class="headerlink" title="Gemini-Pro"></a>Gemini-Pro</h3><p>来自 Google 的 Gemini-Pro，了解更多请访问 <a href="https://deepmind.google/technologies/gemini/">https://deepmind.google/technologies/gemini/</a></p><p>请求 API 密钥: <a href="https://makersuite.google.com/">https://makersuite.google.com/</a></p><h3 id="LLM-模型选择"><a href="#LLM-模型选择" class="headerlink" title="LLM 模型选择"></a>LLM 模型选择</h3><p>在 app.py 文件中，轻松选择您需要的模型。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 取消注释并设置您选择的模型:</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># llm = Gemini(model_path='gemini-pro', api_key=None, proxy_url=None) # 不要忘记加入您自己的 Google API 密钥</span></span><br><span class="line"><span class="comment"># llm = Qwen(mode='offline', model_path="Qwen/Qwen-1_8B-Chat")</span></span><br><span class="line"><span class="comment"># 自动下载</span></span><br><span class="line"><span class="comment"># llm = Linly(mode='offline', model_path="Linly-AI/Chinese-LLaMA-2-7B-hf")</span></span><br><span class="line"><span class="comment"># 手动下载到指定路径</span></span><br><span class="line">llm = Linly(mode=<span class="string">'offline'</span>, model_path=<span class="string">"Linly-AI/Chinese-LLaMA-2-7B-hf"</span>)</span><br></pre></td></tr></tbody></table></figure><h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h2><p>一些优化:</p><ul><li>使用固定的输入人脸图像,提前提取特征,避免每次读取</li><li>移除不必要的库,缩短总时间</li><li>只保存最终视频输出,不保存中间结果,提高性能</li><li>使用OpenCV生成最终视频,比mimwrite更快</li></ul><h2 id="Gradio"><a href="#Gradio" class="headerlink" title="Gradio"></a>Gradio</h2><p>Gradio是一个Python库,提供了一种简单的方式将机器学习模型作为交互式Web应用程序来部署。</p><p>对Linly-Talker而言,使用Gradio有两个主要目的:</p><ol><li><p><strong>可视化与演示</strong>:Gradio为模型提供一个简单的Web GUI,上传图片和文本后可以直观地看到结果。这是展示系统能力的有效方式。</p></li><li><p><strong>用户交互</strong>:Gradio的GUI可以作为前端,允许用户与Linly-Talker进行交互对话。用户可以上传自己的图片并输入问题,实时获取回答。这提供了更自然的语音交互方式。</p></li></ol><p>具体来说,我们在app.py中创建了一个Gradio的Interface,接收图片和文本输入,调用函数生成回应视频,在GUI中显示出来。这样就实现了浏览器交互而不需要编写复杂的前端。</p><p>总之,Gradio为Linly-Talker提供了可视化和用户交互的接口,是展示系统功能和让最终用户使用系统的有效途径。</p><h2 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h2><p>现在的启动一共有几种模式，可以选择特定的场景进行设置</p><p>第一种只有固定了人物问答，设置好了人物，省去了预处理时间</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python app.py</span><br></pre></td></tr></tbody></table></figure><p><img src="https://pica.zhimg.com/80/v2-fc37a5490a674e2194b88714d38f986e.png" alt=""></p><p>第二种是可以任意上传图片进行对话</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python app_img.py</span><br></pre></td></tr></tbody></table></figure><p><img src="https://pic1.zhimg.com/80/v2-7c863c3992beef67953d7ab378be99d9.png" alt=""></p><p>第三种是在第一种的基础上加入了大语言模型，加入了多轮的GPT对话</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python app_multi.py</span><br></pre></td></tr></tbody></table></figure><p><img src="https://picx.zhimg.com/80/v2-802165f64f307dd204b04b9725626cd7.png" alt=""></p><p>文件夹结构如下</p><p>权重部分可以从这下载：<a href="https://pan.baidu.com/s/1eF13O-8wyw4B3MtesctQyg?pwd=linl">Baidu (百度云盘)</a> (Password: <code>linl</code>)</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line">Linly-Talker/ </span><br><span class="line">├── app.py</span><br><span class="line">├── app_img.py</span><br><span class="line">├── utils.py</span><br><span class="line">├── Linly-api.py</span><br><span class="line">├── Linly-api-fast.py</span><br><span class="line">├── Linly-example.ipynb</span><br><span class="line">├── README.md</span><br><span class="line">├── README_zh.md</span><br><span class="line">├── request-Linly-api.py</span><br><span class="line">├── requirements_app.txt</span><br><span class="line">├── scripts</span><br><span class="line">│   └── download_models.sh</span><br><span class="line">├──src</span><br><span class="line">│&nbsp;&nbsp; ├── audio2exp_models</span><br><span class="line">│&nbsp;&nbsp; ├── audio2pose_models</span><br><span class="line">│&nbsp;&nbsp; ├── config</span><br><span class="line">│&nbsp;&nbsp; ├── cost_time.py</span><br><span class="line">│&nbsp;&nbsp; ├── face3d</span><br><span class="line">│&nbsp;&nbsp; ├── facerender</span><br><span class="line">│&nbsp;&nbsp; ├── generate_batch.py</span><br><span class="line">│&nbsp;&nbsp; ├── generate_facerender_batch.py</span><br><span class="line">│&nbsp;&nbsp; ├── Record.py</span><br><span class="line">│&nbsp;&nbsp; ├── test_audio2coeff.py</span><br><span class="line">│&nbsp;&nbsp; └── utils</span><br><span class="line">├── inputs</span><br><span class="line">│   ├── example.png</span><br><span class="line">│   └── first_frame_dir</span><br><span class="line">│       ├── example_landmarks.txt</span><br><span class="line">│       ├── example.mat</span><br><span class="line">│       └── example.png</span><br><span class="line">├── examples</span><br><span class="line">│   └── source_image</span><br><span class="line">│       ├── art_0.png</span><br><span class="line">│       ├── ......</span><br><span class="line">│       └── sad.png</span><br><span class="line">├── TFG</span><br><span class="line">│&nbsp;&nbsp; ├── __init__.py</span><br><span class="line">│&nbsp;  ├── Wav2Lip.py</span><br><span class="line">│&nbsp;&nbsp; └── SadTalker.py</span><br><span class="line">└── TTS</span><br><span class="line">│&nbsp;&nbsp; ├── __init__.py</span><br><span class="line">│&nbsp;  ├── EdgeTTS.py</span><br><span class="line">│&nbsp;  └── TTS_app.py</span><br><span class="line">├── ASR</span><br><span class="line">│&nbsp;&nbsp; ├── __init__.py</span><br><span class="line">│&nbsp;&nbsp; ├── FunASR.py</span><br><span class="line">│&nbsp;&nbsp; └── Whisper.py</span><br><span class="line">├── LLM</span><br><span class="line">│&nbsp;&nbsp; ├── __init__.py</span><br><span class="line">│&nbsp;&nbsp; ├── Gemini.py</span><br><span class="line">│&nbsp;&nbsp; ├── Linly.py</span><br><span class="line">│&nbsp;&nbsp; └── Qwen.py</span><br><span class="line">....... // 以下是需要下载的权重路径（可选）</span><br><span class="line">├── checkpoints // SadTalker 权重路径</span><br><span class="line">│   ├── mapping_00109-model.pth.tar</span><br><span class="line">│   ├── mapping_00229-model.pth.tar</span><br><span class="line">│   ├── SadTalker_V0.0.2_256.safetensors</span><br><span class="line">│   └── SadTalker_V0.0.2_512.safetensors</span><br><span class="line">│   ├── lipsync_expert.pth</span><br><span class="line">│   ├── visual_quality_disc.pth</span><br><span class="line">│   ├── wav2lip_gan.pth</span><br><span class="line">│   └── wav2lip.pth // Wav2Lip 权重陆军</span><br><span class="line">├── gfpgan // GFPGAN 权重路径</span><br><span class="line">│   └── weights</span><br><span class="line">│       ├── alignment_WFLW_4HG.pth</span><br><span class="line">│       └── detection_Resnet50_Final.pth</span><br><span class="line">├── Linly-AI // Linly 权重路径</span><br><span class="line">│   └── Chinese-LLaMA-2-7B-hf </span><br><span class="line">│       ├── config.json</span><br><span class="line">│       ├── generation_config.json</span><br><span class="line">│       ├── pytorch_model-00001-of-00002.bin</span><br><span class="line">│       ├── pytorch_model-00002-of-00002.bin</span><br><span class="line">│       ├── pytorch_model.bin.index.json</span><br><span class="line">│       ├── README.md</span><br><span class="line">│       ├── special_tokens_map.json</span><br><span class="line">│       ├── tokenizer_config.json</span><br><span class="line">│       └── tokenizer.model</span><br><span class="line">├── Qwen // Qwen 权重路径</span><br><span class="line">│   └── Qwen-1_8B-Chat</span><br><span class="line">│       ├── cache_autogptq_cuda_256.cpp</span><br><span class="line">│       ├── cache_autogptq_cuda_kernel_256.cu</span><br><span class="line">│       ├── config.json</span><br><span class="line">│       ├── configuration_qwen.py</span><br><span class="line">│       ├── cpp_kernels.py</span><br><span class="line">│       ├── examples</span><br><span class="line">│       │   └── react_prompt.md</span><br><span class="line">│       ├── generation_config.json</span><br><span class="line">│       ├── LICENSE</span><br><span class="line">│       ├── model-00001-of-00002.safetensors</span><br><span class="line">│       ├── model-00002-of-00002.safetensors</span><br><span class="line">│       ├── modeling_qwen.py</span><br><span class="line">│       ├── model.safetensors.index.json</span><br><span class="line">│       ├── NOTICE</span><br><span class="line">│       ├── qwen_generation_utils.py</span><br><span class="line">│       ├── qwen.tiktoken</span><br><span class="line">│       ├── README.md</span><br><span class="line">│       ├── tokenization_qwen.py</span><br><span class="line">│       └── tokenizer_config.json</span><br></pre></td></tr></tbody></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://github.com/openai/whisper">https://github.com/openai/whisper</a></li><li><a href="https://github.com/rany2/edge-tts">https://github.com/rany2/edge-tts</a>  </li><li><a href="https://github.com/CVI-SZU/Linly">https://github.com/CVI-SZU/Linly</a></li><li><a href="https://github.com/QwenLM/Qwen">https://github.com/QwenLM/Qwen</a></li><li><a href="https://deepmind.google/technologies/gemini/">https://deepmind.google/technologies/gemini/</a></li><li><a href="https://github.com/OpenTalker/SadTalker">https://github.com/OpenTalker/SadTalker</a></li></ul><h2 id="Star-History"><a href="#Star-History" class="headerlink" title="Star History"></a>Star History</h2><p><a href="https://star-history.com/#Kedreamix/Linly-Talker&amp;Date"><img src="https://api.star-history.com/svg?repos=Kedreamix/Linly-Talker&amp;type=Date" alt="Star History Chart"></a></p>]]></content>
    
    
    <summary type="html">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</summary>
    
    
    
    <category term="Project" scheme="https://kedreamix.github.io/categories/Project/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
    <category term="LLM" scheme="https://kedreamix.github.io/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>FastAPI 快速教程: 从零开始构建你的第一个API项目</title>
    <link href="https://kedreamix.github.io/Note/fastapi/"/>
    <id>https://kedreamix.github.io/Note/fastapi/</id>
    <published>2024-01-19T15:09:47.000Z</published>
    <updated>2024-01-21T10:20:06.402Z</updated>
    
    <content type="html"><![CDATA[<p>最近在学习大模型的时候，有时候会遇到要写API的时候，这个时候我就遇见了FastAPI，我发现这个是一个很好的库，可以很方便的让我们构建一个属于自己的API，所以今天我也写一下这个入门教程和大家一起分享一下，同时也让我们解密一下，OpenAI和一些公司的API，可能是怎么写和怎么做的。</p><p><img src="https://picx.zhimg.com/80/v2-f7dc5c12cb693d83a113359819a1f26e_720w.png?source=d16d100b" alt="FastAPI framework, high performance, easy to learn, fast to code, ready for production"></p><h2 id="FastAPI介绍"><a href="#FastAPI介绍" class="headerlink" title="FastAPI介绍"></a>FastAPI介绍</h2><p>FastAPI 是一个用于构建 API 的现代、快速（高性能）的 web 框架，使用 Python 3.8+ 并基于标准的 Python 类型提示。</p><p><strong>文档</strong>： <a href="https://fastapi.tiangolo.com/">https://fastapi.tiangolo.com</a></p><p><strong>源码</strong>： <a href="https://github.com/tiangolo/fastapi">https://github.com/tiangolo/fastapi</a></p><p>关键特性:</p><ul><li><strong>快速</strong>：可与 <strong>NodeJS</strong> 和 <strong>Go</strong> 并肩的极高性能（归功于 Starlette 和 Pydantic）。<a href="https://fastapi.tiangolo.com/zh/#_11">最快的 Python web 框架之一</a>。</li><li><strong>高效编码</strong>：提高功能开发速度约 200％ 至 300％。*</li><li><strong>更少 bug</strong>：减少约 40％ 的人为（开发者）导致错误。*</li><li><strong>智能</strong>：极佳的编辑器支持。处处皆可自动补全，减少调试时间。</li><li><strong>简单</strong>：设计的易于使用和学习，阅读文档的时间更短。</li><li><strong>简短</strong>：使代码重复最小化。通过不同的参数声明实现丰富功能。bug 更少。</li><li><strong>健壮</strong>：生产可用级别的代码。还有自动生成的交互式文档。</li><li><strong>标准化</strong>：基于（并完全兼容）API 的相关开放标准：<a href="https://github.com/OAI/OpenAPI-Specification">OpenAPI</a> (以前被称为 Swagger) 和 <a href="https://json-schema.org/">JSON Schema</a>。</li></ul><h2 id="安装及依赖"><a href="#安装及依赖" class="headerlink" title="安装及依赖"></a>安装及依赖</h2><p>Python 3.8 及更高版本</p><p>FastAPI 站在以下巨人的肩膀之上：</p><ul><li><a href="https://www.starlette.io/">Starlette</a> 负责 web 部分。</li><li><a href="https://pydantic-docs.helpmanual.io/">Pydantic</a> 负责数据部分。</li></ul><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install fastapi</span><br></pre></td></tr></tbody></table></figure><p>我们有可能还会需要一个 ASGI 服务器，可以使用<a href="https://www.uvicorn.org/">Uvicorn</a></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install unvicorn</span><br></pre></td></tr></tbody></table></figure><h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><h3 id="创建"><a href="#创建" class="headerlink" title="创建"></a>创建</h3><p>创建一个 main.py 文件并写入以下内容:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Union</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> fastapi <span class="keyword">import</span> FastAPI</span><br><span class="line"></span><br><span class="line">app = FastAPI()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.get(<span class="params"><span class="string">"/"</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_root</span>():</span><br><span class="line">    <span class="keyword">return</span> {<span class="string">"Hello"</span>: <span class="string">"World"</span>}</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.get(<span class="params"><span class="string">"/items/{item_id}"</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_item</span>(<span class="params">item_id: <span class="built_in">int</span>, q: <span class="type">Union</span>[<span class="built_in">str</span>, <span class="literal">None</span>] = <span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">return</span> {<span class="string">"item_id"</span>: item_id, <span class="string">"q"</span>: q}</span><br></pre></td></tr></tbody></table></figure><p>如果我们需要加入异步编程的话，我们就需要改一下代码，加入async/await和async def </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Union</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> fastapi <span class="keyword">import</span> FastAPI</span><br><span class="line"></span><br><span class="line">app = FastAPI()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.get(<span class="params"><span class="string">"/"</span></span>)</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">read_root</span>():</span><br><span class="line">    <span class="keyword">return</span> {<span class="string">"Hello"</span>: <span class="string">"World"</span>}</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.get(<span class="params"><span class="string">"/items/{item_id}"</span></span>)</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">read_item</span>(<span class="params">item_id: <span class="built_in">int</span>, q: <span class="type">Union</span>[<span class="built_in">str</span>, <span class="literal">None</span>] = <span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">return</span> {<span class="string">"item_id"</span>: item_id, <span class="string">"q"</span>: q}</span><br></pre></td></tr></tbody></table></figure><p>如果对于异步编程有点兴趣的话，可以看看这个讲解，我觉得还是很不错的，这也加强了代码的并发能力。</p><h3 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h3><p>通过以下命令进行运行服务器</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ uvicorn main:app --reload</span><br><span class="line"></span><br><span class="line">INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)</span><br><span class="line">INFO:     Started reloader process [28720]</span><br><span class="line">INFO:     Started server process [28722]</span><br><span class="line">INFO:     Waiting <span class="keyword">for</span> application startup.</span><br><span class="line">INFO:     Application startup complete.</span><br></pre></td></tr></tbody></table></figure><p>uvicorn main:app 命令含义如下:</p><ul><li>main：main.py 文件（一个 Python “模块”）。</li><li>app：在 main.py 文件中通过 app = FastAPI() 创建的对象。</li><li>—reload：让服务器在更新代码后重新启动。仅在开发时使用该选项。</li></ul><p>这里面着重提一下<strong>reolab参数</strong>，这个相当于我们在开发是对代码进行修改的同时，服务器也在变化，这样就方便我们进行开发和学习，但是如果开发完毕以后，我们可以去掉<strong>—reload</strong>，防止不小心动到代码改变了api的访问</p><h3 id="检查"><a href="#检查" class="headerlink" title="检查"></a><strong>检查</strong></h3><p>使用浏览器访问 <a href="http://127.0.0.1:8000/items/5?q=somequery。">http://127.0.0.1:8000/items/5?q=somequery。</a></p><p>你将会看到如下 JSON 响应：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">{"item_id": 5, "q": "somequery"}</span><br></pre></td></tr></tbody></table></figure><p>你已经创建了一个具有以下功能的 API：</p><ul><li>通过 <em>路径</em> / 和 /items/{item_id} 接受 HTTP 请求。</li><li>以上 <em>路径</em> 都接受 GET <em>操作</em>（也被称为 HTTP <em>方法</em>）。</li><li>/items/{item_id} <em>路径</em> 有一个 <em>路径参数</em> item_id 并且应该为 int 类型。</li><li>/items/{item_id} <em>路径</em> 有一个可选的 str 类型的 <em>查询参数</em> q。</li></ul><h2 id="API文档"><a href="#API文档" class="headerlink" title="API文档"></a>API文档</h2><h3 id="交互式-API-文档"><a href="#交互式-API-文档" class="headerlink" title="交互式 API 文档"></a><strong>交互式 API 文档</strong></h3><p>现在访问 <a href="http://127.0.0.1:8000/docs。">http://127.0.0.1:8000/docs。</a></p><p>你会看到自动生成的交互式 API 文档（由 <a href="https://github.com/swagger-api/swagger-ui">Swagger UI</a>生成）：</p><p><img src="https://picx.zhimg.com/80/v2-39fe8891285bf578eee8466c4aec1b52_720w.png?source=d16d100b" alt="交互式 API 文档"></p><h3 id="可选的-API-文档"><a href="#可选的-API-文档" class="headerlink" title="可选的 API 文档"></a>可选的 API 文档</h3><p>访问 <a href="http://127.0.0.1:8000/redoc。">http://127.0.0.1:8000/redoc。</a></p><p>你会看到另一个自动生成的文档（由 <a href="https://github.com/Rebilly/ReDoc">ReDoc</a> 生成）：</p><p><img src="https://picx.zhimg.com/80/v2-23ae5bafaed6faf6ced83d987b17fc98_720w.png?source=d16d100b" alt="可选的 API 文档"></p><h2 id="大模型API实战"><a href="#大模型API实战" class="headerlink" title="大模型API实战"></a>大模型API实战</h2><p>比如我现在想使用一个大模型，比如就是阿里的通义千问的大模型，我希望能写一个api接口进行对其调用，有点类似与OpenAI一样写一个接口，这样就方便我们进行去调用，而不用每次跑一堆代码。</p><h3 id="Qwen模型下载与使用"><a href="#Qwen模型下载与使用" class="headerlink" title="Qwen模型下载与使用"></a>Qwen模型下载与使用</h3><p>比如我们可以从Qwen中获取对应的模型，<a href="https://huggingface.co/Qwen/Qwen-7B-Chat">https://huggingface.co/Qwen/Qwen-7B-Chat</a></p><p>从里面我们可以看到多轮对话的快速使用代码，这对我们写API有很大的帮助，我们从中可以看到主要的流程，实际上还是蛮简单的，就是导入模型后，进行传入参数，参数一般有三个，一个是一开始定义的分词器<strong>tokenizer</strong>，另外两个就比较重要，分别是问题和历史记录，所以我们希望得到的api应该是有这两个输入的。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForCausalLM, AutoTokenizer</span><br><span class="line"><span class="keyword">from</span> transformers.generation <span class="keyword">import</span> GenerationConfig</span><br><span class="line"></span><br><span class="line"><span class="comment"># Note: The default behavior now has injection attack prevention off.</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">"Qwen/Qwen-7B-Chat"</span>, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># use bf16</span></span><br><span class="line"><span class="comment"># model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True, bf16=True).eval()</span></span><br><span class="line"><span class="comment"># use fp16</span></span><br><span class="line"><span class="comment"># model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True, fp16=True).eval()</span></span><br><span class="line"><span class="comment"># use cpu only</span></span><br><span class="line"><span class="comment"># model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="cpu", trust_remote_code=True).eval()</span></span><br><span class="line"><span class="comment"># use auto mode, automatically select precision based on the device.</span></span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(<span class="string">"Qwen/Qwen-7B-Chat"</span>, device_map=<span class="string">"auto"</span>, trust_remote_code=<span class="literal">True</span>).<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Specify hyperparameters for generation. But if you use transformers&gt;=4.32.0, there is no need to do this.</span></span><br><span class="line"><span class="comment"># model.generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-7B-Chat", trust_remote_code=True) # 可指定不同的生成长度、top_p等相关超参</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一轮对话 1st dialogue turn</span></span><br><span class="line">response, history = model.chat(tokenizer, <span class="string">"你好"</span>, history=<span class="literal">None</span>)</span><br><span class="line"><span class="built_in">print</span>(response)</span><br><span class="line"><span class="comment"># 你好！很高兴为你提供帮助。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二轮对话 2nd dialogue turn</span></span><br><span class="line">response, history = model.chat(tokenizer, <span class="string">"给我讲一个年轻人奋斗创业最终取得成功的故事。"</span>, history=history)</span><br><span class="line"><span class="built_in">print</span>(response)</span><br><span class="line"><span class="comment"># 这是一个关于一个年轻人奋斗创业最终取得成功的故事。</span></span><br><span class="line"><span class="comment"># 故事的主人公叫李明，他来自一个普通的家庭，父母都是普通的工人。从小，李明就立下了一个目标：要成为一名成功的企业家。</span></span><br><span class="line"><span class="comment"># 为了实现这个目标，李明勤奋学习，考上了大学。在大学期间，他积极参加各种创业比赛，获得了不少奖项。他还利用课余时间去实习，积累了宝贵的经验。</span></span><br><span class="line"><span class="comment"># 毕业后，李明决定开始自己的创业之路。他开始寻找投资机会，但多次都被拒绝了。然而，他并没有放弃。他继续努力，不断改进自己的创业计划，并寻找新的投资机会。</span></span><br><span class="line"><span class="comment"># 最终，李明成功地获得了一笔投资，开始了自己的创业之路。他成立了一家科技公司，专注于开发新型软件。在他的领导下，公司迅速发展起来，成为了一家成功的科技企业。</span></span><br><span class="line"><span class="comment"># 李明的成功并不是偶然的。他勤奋、坚韧、勇于冒险，不断学习和改进自己。他的成功也证明了，只要努力奋斗，任何人都有可能取得成功。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第三轮对话 3rd dialogue turn</span></span><br><span class="line">response, history = model.chat(tokenizer, <span class="string">"给这个故事起一个标题"</span>, history=history)</span><br><span class="line"><span class="built_in">print</span>(response)</span><br><span class="line"><span class="comment"># 《奋斗创业：一个年轻人的成功之路》</span></span><br></pre></td></tr></tbody></table></figure><h3 id="编写FastAPI代码"><a href="#编写FastAPI代码" class="headerlink" title="编写FastAPI代码"></a>编写FastAPI代码</h3><p>所以弄清楚了原理以后，我们就可以开始利用FastAPI写以下的代码了，为了更好的使用api，除了prompt和history两个参数之外，还加入了max_lenth和temperature等参数，这些参数实际上都是model.chat里面进行使用的，这样更好的去设置和学习。等到返回结果以后，就会返回时间和成功的标志，这样我们就获取了结果。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fastapi <span class="keyword">import</span> FastAPI, Request</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForCausalLM, GenerationConfig</span><br><span class="line"><span class="keyword">import</span> uvicorn</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置设备参数</span></span><br><span class="line">DEVICE = <span class="string">"cuda"</span>  <span class="comment"># 使用CUDA</span></span><br><span class="line">DEVICE_ID = <span class="string">"0"</span>  <span class="comment"># CUDA设备ID，如果未设置则为空</span></span><br><span class="line">CUDA_DEVICE = <span class="string">f"<span class="subst">{DEVICE}</span>:<span class="subst">{DEVICE_ID}</span>"</span> <span class="keyword">if</span> DEVICE_ID <span class="keyword">else</span> DEVICE  <span class="comment"># 组合CUDA设备信息</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 清理GPU内存函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">torch_gc</span>():</span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():  <span class="comment"># 检查是否可用CUDA</span></span><br><span class="line">        <span class="keyword">with</span> torch.cuda.device(CUDA_DEVICE):  <span class="comment"># 指定CUDA设备</span></span><br><span class="line">            torch.cuda.empty_cache()  <span class="comment"># 清空CUDA缓存</span></span><br><span class="line">            torch.cuda.ipc_collect()  <span class="comment"># 收集CUDA内存碎片</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建FastAPI应用</span></span><br><span class="line">app = FastAPI()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理POST请求的端点</span></span><br><span class="line"><span class="meta">@app.post(<span class="params"><span class="string">"/"</span></span>)</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">create_item</span>(<span class="params">request: Request</span>):</span><br><span class="line">    <span class="keyword">global</span> model, tokenizer  <span class="comment"># 声明全局变量以便在函数内部使用模型和分词器</span></span><br><span class="line">    json_post_raw = <span class="keyword">await</span> request.json()  <span class="comment"># 获取POST请求的JSON数据</span></span><br><span class="line">    json_post = json.dumps(json_post_raw)  <span class="comment"># 将JSON数据转换为字符串</span></span><br><span class="line">    json_post_list = json.loads(json_post)  <span class="comment"># 将字符串转换为Python对象</span></span><br><span class="line">    prompt = json_post_list.get(<span class="string">'prompt'</span>)  <span class="comment"># 获取请求中的提示</span></span><br><span class="line">    history = json_post_list.get(<span class="string">'history'</span>)  <span class="comment"># 获取请求中的历史记录</span></span><br><span class="line">    max_length = json_post_list.get(<span class="string">'max_length'</span>)  <span class="comment"># 获取请求中的最大长度</span></span><br><span class="line">    top_p = json_post_list.get(<span class="string">'top_p'</span>)  <span class="comment"># 获取请求中的top_p参数</span></span><br><span class="line">    temperature = json_post_list.get(<span class="string">'temperature'</span>)  <span class="comment"># 获取请求中的温度参数</span></span><br><span class="line">    <span class="comment"># 调用模型进行对话生成</span></span><br><span class="line">    response, history = model.chat(</span><br><span class="line">        tokenizer,</span><br><span class="line">        prompt,</span><br><span class="line">        history=history,</span><br><span class="line">        max_length=max_length <span class="keyword">if</span> max_length <span class="keyword">else</span> <span class="number">2048</span>,  <span class="comment"># 如果未提供最大长度，默认使用2048</span></span><br><span class="line">        top_p=top_p <span class="keyword">if</span> top_p <span class="keyword">else</span> <span class="number">0.7</span>,  <span class="comment"># 如果未提供top_p参数，默认使用0.7</span></span><br><span class="line">        temperature=temperature <span class="keyword">if</span> temperature <span class="keyword">else</span> <span class="number">0.95</span>  <span class="comment"># 如果未提供温度参数，默认使用0.95</span></span><br><span class="line">    )</span><br><span class="line">    now = datetime.datetime.now()  <span class="comment"># 获取当前时间</span></span><br><span class="line">    time = now.strftime(<span class="string">"%Y-%m-%d %H:%M:%S"</span>)  <span class="comment"># 格式化时间为字符串</span></span><br><span class="line">    <span class="comment"># 构建响应JSON</span></span><br><span class="line">    answer = {</span><br><span class="line">        <span class="string">"response"</span>: response,</span><br><span class="line">        <span class="string">"history"</span>: history,</span><br><span class="line">        <span class="string">"status"</span>: <span class="number">200</span>,</span><br><span class="line">        <span class="string">"time"</span>: time</span><br><span class="line">    }</span><br><span class="line">    <span class="comment"># 构建日志信息</span></span><br><span class="line">    log = <span class="string">"["</span> + time + <span class="string">"] "</span> + <span class="string">'", prompt:"'</span> + prompt + <span class="string">'", response:"'</span> + <span class="built_in">repr</span>(response) + <span class="string">'"'</span></span><br><span class="line">    <span class="built_in">print</span>(log)  <span class="comment"># 打印日志</span></span><br><span class="line">    torch_gc()  <span class="comment"># 执行GPU内存清理</span></span><br><span class="line">    <span class="keyword">return</span> answer  <span class="comment"># 返回响应</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 主函数入口</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># 加载预训练的分词器和模型</span></span><br><span class="line">    tokenizer = AutoTokenizer.from_pretrained(<span class="string">"Qwen/Qwen-7B-Chat"</span>, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line">    model = AutoModelForCausalLM.from_pretrained(<span class="string">"Qwen/Qwen-7B-Chat"</span>, device_map=<span class="string">"auto"</span>, trust_remote_code=<span class="literal">True</span>).<span class="built_in">eval</span>()</span><br><span class="line">    model.generation_config = GenerationConfig.from_pretrained(<span class="string">"Qwen/Qwen-7B-Chat"</span>, trust_remote_code=<span class="literal">True</span>) <span class="comment"># 可指定</span></span><br><span class="line">    model.<span class="built_in">eval</span>()  <span class="comment"># 设置模型为评估模式</span></span><br><span class="line">    <span class="comment"># 启动FastAPI应用，用6006端口映射到本地，从而在本地使用api</span></span><br><span class="line">    uvicorn.run(app, host=<span class="string">'0.0.0.0'</span>, port=<span class="number">6006</span>, workers=<span class="number">1</span>)  <span class="comment"># 在指定端口和主机上启动应用</span></span><br></pre></td></tr></tbody></table></figure><h3 id="运行及使用API"><a href="#运行及使用API" class="headerlink" title="运行及使用API"></a>运行及使用API</h3><p>我们运行方式很简单，直接在服务器终端运行</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python api.py</span><br></pre></td></tr></tbody></table></figure><p>加载完毕后出现如下信息说明成功。</p><p><img src="https://picx.zhimg.com/80/v2-25146b4a0f3b5a0fa70782b1ed749e5d_720w.png?source=d16d100b" alt="运行方式"></p><p>默认部署在 6006 端口，通过 POST 方法进行调用，可以使用curl调用，如下所示：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">curl -X POST "http://127.0.0.1:6006" \</span><br><span class="line">     -H 'Content-Type: application/json' \</span><br><span class="line">     -d '{"prompt": "你好", "history": []}'</span><br></pre></td></tr></tbody></table></figure><p>也可以使用python中的requests库进行调用，如下所示：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">import json</span><br><span class="line"></span><br><span class="line">def get_completion(prompt):</span><br><span class="line">    headers = {'Content-Type': 'application/json'}</span><br><span class="line">    data = {"prompt": prompt, "history": []}</span><br><span class="line">    response = requests.post(url='http://127.0.0.1:6006', headers=headers, data=json.dumps(data))</span><br><span class="line">    return response.json()['response']</span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line">    print(get_completion('你好'))</span><br></pre></td></tr></tbody></table></figure><p>得到的返回值如下所示：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">{</span><br><span class="line">  "response":"你好！很高兴为你服务。有什么我可以帮助你的吗？",</span><br><span class="line">  "history":[["你好","你好！很高兴为你服务。有什么我可以帮助你的吗？"]],</span><br><span class="line">  "status":200,</span><br><span class="line">  "time":"2023-11-26 1:14:20"</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="https://pic1.zhimg.com/80/v2-284a1395c44b8bb0177dae70fd54b930_720w.png?source=d16d100b" alt="运行结果"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>我觉得在有时候我们要实现一个API的时候，我们可以用FastAPI快速实现，并且得到一个不错的结果，这也是我学习的初衷，有时候一些服务器可能可以当做API来使用来调用，其实也方便去使用，也可以部署后成为商业产品，类似于OpenAI一样。</p><p>最后感谢一下FastAPI的文档，让我学习到很多，在里面还有更详细的使用方案，大家也可以去学习一下，然后再感谢一下datawhale的self-llm项目，也是在里面我学习到了使用FastAPI，大家如果对大模型感兴趣也可以关注一下。</p><p>self-llm项目：<a href="https://github.com/datawhalechina/self-llm">https://github.com/datawhalechina/self-llm</a></p><p>FastAPI学习文档：<a href="https://fastapi.tiangolo.com/zh/learn/">https://fastapi.tiangolo.com/zh/learn/</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;最近在学习大模型的时候，有时候会遇到要写API的时候，这个时候我就遇见了FastAPI，我发现这个是一个很好的库，可以很方便的让我们构建一个属于自己的API，所以今天我也写一下这个入门教程和大家一起分享一下，同时也让我们解密一下，OpenAI和一些公司的API，可能是怎么写</summary>
      
    
    
    
    <category term="Note" scheme="https://kedreamix.github.io/categories/Note/"/>
    
    
    <category term="Linux" scheme="https://kedreamix.github.io/tags/Linux/"/>
    
    <category term="Note" scheme="https://kedreamix.github.io/tags/Note/"/>
    
    <category term="Python" scheme="https://kedreamix.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Linux使用gdown从Google Drive下载文件和文件夹（命令行/代码下载）</title>
    <link href="https://kedreamix.github.io/Note/gdown/"/>
    <id>https://kedreamix.github.io/Note/gdown/</id>
    <published>2024-01-19T12:25:30.000Z</published>
    <updated>2024-01-21T10:20:20.277Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、安装gdown"><a href="#一、安装gdown" class="headerlink" title="一、安装gdown"></a>一、安装gdown</h2><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/wkentaro/gdown </span><br><span class="line"><span class="built_in">cd</span> gdown</span><br><span class="line">pip install gdown</span><br></pre></td></tr></tbody></table></figure><h2 id="二、获取Google-Drive文件链接"><a href="#二、获取Google-Drive文件链接" class="headerlink" title="二、获取Google Drive文件链接"></a>二、获取Google Drive文件链接</h2><ol><li>打开Google Drive</li><li>右键点击要下载的文件/文件夹</li><li>选择”获取链接”</li><li>确保文件/文件夹的访问权限设置为”任何人均可访问”</li><li>打开分享链接,复制地址栏中的文件ID,链接前缀都为<code>https://drive.google.com/uc?id=</code>，如<code>https://drive.google.com/uc?id=1l_5RK28JRL19wpT22B-DY9We3TVXnnQQ</code></li><li>复制链接</li></ol><h2 id="三、使用gdown下载"><a href="#三、使用gdown下载" class="headerlink" title="三、使用gdown下载"></a>三、使用gdown下载</h2><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 文件下载</span></span><br><span class="line"></span><br><span class="line">gdown https://drive.google.com/uc?<span class="built_in">id</span>=&lt;文件ID&gt;</span><br><span class="line"><span class="comment"># gdown https://drive.google.com/uc?id=1l_5RK28JRL19wpT22B-DY9We3TVXnnQQ</span></span><br><span class="line"><span class="comment"># gdown 1l_5RK28JRL19wpT22B-DY9We3TVXnnQQ</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 文件夹下载</span></span><br><span class="line">gdown https://drive.google.com/drive/folders/15uNXeRBIhVvZJIhL4yTw4IsStMhUaaxl -O /tmp/folder --folder</span><br></pre></td></tr></tbody></table></figure><p>除了命令行之外，我们也可以通过代码来进行下载</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 文件下载</span></span><br><span class="line"><span class="keyword">import</span> gdown</span><br><span class="line"></span><br><span class="line">url = <span class="string">'https://drive.google.com/file/d/1uFTzwFc3tmS-D7azjMiJcxSfn71BPqKt/view?usp=sharing'</span></span><br><span class="line">output_path = <span class="string">'graph_ML.pk'</span></span><br><span class="line">gdown.download(url, output_path, quiet=<span class="literal">False</span>,fuzzy=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 文件夹下载</span></span><br><span class="line"><span class="keyword">import</span> gdown</span><br><span class="line">url = <span class="string">"https://drive.google.com/drive/folders/1HWFHKCprFzR7H7TYhrE-W7v4bz2Vc7Ia"</span></span><br><span class="line"></span><br><span class="line">gdown.download_folder(url, quiet=<span class="literal">True</span>, use_cookies=<span class="literal">False</span>)</span><br></pre></td></tr></tbody></table></figure><blockquote><p>除此之外，还有一些命令的使用，这里就不过多解释了</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">gdown --<span class="built_in">help</span></span><br><span class="line">usage: gdown [-h] [-V] [-O OUTPUT] [-q] [--fuzzy] [--<span class="built_in">id</span>] [--proxy PROXY]</span><br><span class="line">             [--speed SPEED] [--no-cookies] [--no-check-certificate]</span><br><span class="line">             [--<span class="built_in">continue</span>] [--folder] [--remaining-ok]</span><br><span class="line">             url_or_id</span><br></pre></td></tr></tbody></table></figure></blockquote><h2 id="四、问题解决"><a href="#四、问题解决" class="headerlink" title="四、问题解决"></a>四、问题解决</h2><p>由于Google Drive文件大小限制,直接使用curl/wget下载可能会失败。</p><p>这时需要使用gdown来实现从Google Drive下载大文件。它可以解决由于文件太大导致的curl/wget下载失败问题。</p><p><strong>参考链接</strong></p><p>gdown项目地址: <a href="https://github.com/wkentaro/gdown">https://github.com/wkentaro/gdown</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;一、安装gdown&quot;&gt;&lt;a href=&quot;#一、安装gdown&quot; class=&quot;headerlink&quot; title=&quot;一、安装gdown&quot;&gt;&lt;/a&gt;一、安装gdown&lt;/h2&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tbody&gt;</summary>
      
    
    
    
    <category term="Note" scheme="https://kedreamix.github.io/categories/Note/"/>
    
    
    <category term="Linux" scheme="https://kedreamix.github.io/tags/Linux/"/>
    
    <category term="Note" scheme="https://kedreamix.github.io/tags/Note/"/>
    
  </entry>
  
  <entry>
    <title>解决Flask-Sockets连接WebSocket时出现werkzeug.routing.WebsocketMismatch的错误</title>
    <link href="https://kedreamix.github.io/Note/websocket/"/>
    <id>https://kedreamix.github.io/Note/websocket/</id>
    <published>2024-01-14T17:01:01.000Z</published>
    <updated>2024-01-25T09:31:59.874Z</updated>
    
    <content type="html"><![CDATA[<p>在使用Flask-Sockets进行WebSocket连接时，一些用户可能会遇到如下错误信息：werkzeug.routing.WebsocketMismatch: 400 Bad Request: The browser (or proxy) sent a request that this server could not understand.</p><p>这个问题的解决方法可能有两种，从整理的资料上来看，接下来分别对两种方法进行说明。</p><ol><li>降低falsk和Werkzeug版本</li><li>涉及到安装flask_sockets库并对其源代码进行手动修改。</li></ol><h3 id="方法一：降低-Flask-和-Werkzeug-版本"><a href="#方法一：降低-Flask-和-Werkzeug-版本" class="headerlink" title="方法一：降低 Flask 和 Werkzeug 版本"></a>方法一：降低 Flask 和 Werkzeug 版本</h3><p>这种错误有时是因为Flask版本过高，降级即可成功，但是这种方式可能不够理想，推荐查看方法二。</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install flask==1.1.2</span><br><span class="line">pip install Werkzeug==1.0.2</span><br></pre></td></tr></tbody></table></figure><h3 id="方法二（推荐）"><a href="#方法二（推荐）" class="headerlink" title="方法二（推荐）"></a>方法二（推荐）</h3><p>首先，确保已经安装了flask_sockets库，可以通过运行以下命令进行安装：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install flask_sockets</span><br></pre></td></tr></tbody></table></figure><p>安装完成后，需要对flask_sockets库的源代码进行手动修改。具体的修改如下：</p><p>文件：flask_sockets.py 函数：add_url_rule</p><p>修改前的代码：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.url_map.add(Rule(rule, endpoint=f))</span><br></pre></td></tr></tbody></table></figure><p>修改后的代码：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.url_map.add(Rule(rule, endpoint=f, websocket=True))</span><br></pre></td></tr></tbody></table></figure><p>这个修改的目的是为了在添加URL规则时明确指定WebSocket。通过将websocket=True添加到Rule构造函数中，可以解决400 Bad Request错误。</p><p>关于这个问题的详细讨论可以参考以下链接： <a href="https://github.com/heroku-python/flask-sockets/issues/81">https://github.com/heroku-python/flask-sockets/issues/81</a></p><p>有关这个问题的具体修改可以参考以下链接： <a href="https://github.com/slipperstree/flask-sockets/commit/cb06c69db3af2cb52fbc050f3595ffa4100bbee3">https://github.com/slipperstree/flask-sockets/commit/cb06c69db3af2cb52fbc050f3595ffa4100bbee3</a></p><p>通过对flask_sockets库的手动修改，您可以顺利解决WebSocket连接时可能遇到的400 Bad Request错误，确保您的应用正常运行。希望这篇文章对您有帮助！</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;在使用Flask-Sockets进行WebSocket连接时，一些用户可能会遇到如下错误信息：werkzeug.routing.WebsocketMismatch: 400 Bad Request: The browser (or proxy) sent a request</summary>
      
    
    
    
    <category term="Note" scheme="https://kedreamix.github.io/categories/Note/"/>
    
    
    <category term="Bug" scheme="https://kedreamix.github.io/tags/Bug/"/>
    
  </entry>
  
  <entry>
    <title>简便快捷获取Hugging Face模型（使用镜像站点）</title>
    <link href="https://kedreamix.github.io/Note/HuggingFace/"/>
    <id>https://kedreamix.github.io/Note/HuggingFace/</id>
    <published>2024-01-05T06:40:00.000Z</published>
    <updated>2024-01-21T10:20:35.964Z</updated>
    
    <content type="html"><![CDATA[<p>通常，我们可以通过代码直接下载模型，但有时会遇到诸多问题，例如下载速度慢或其他一些问题。在这里，我将主要介绍一些常用的简便方法。如果你想了解更多用法，请查看这篇知乎文章：<a href="https://zhuanlan.zhihu.com/p/663712983，">https://zhuanlan.zhihu.com/p/663712983，</a> 其中包含许多多线程的方法。</p><p><a href="https://zhuanlan.zhihu.com/p/663712983">padeoe：如何快速下载huggingface模型</a></p><p><img src="https://picx.zhimg.com/v2-9e8901ddec21dae36a31bb438dae03a8_r.jpg?source=172ae18b" alt="img"></p><p>我的关注点主要是<strong>断点续传和多线程下载</strong>的方式，因为这样可以避免每次都重新下载，同时在网络不稳定时能够保持相对较好的下载速度。</p><h3 id="浏览器下载"><a href="#浏览器下载" class="headerlink" title="浏览器下载"></a>浏览器下载</h3><p>首先，最简单的方法是通过浏览器下载。找到相应的文件，一个一个地下载即可。然而，这样会耗费时间和精力。</p><p><img src="https://picx.zhimg.com/80/v2-83aaadbd16b90a345f15a08954aa9e2e_720w.png?source=d16d100b" alt="浏览器下载"></p><p>浏览器下载</p><h3 id="直接使用URL下载"><a href="#直接使用URL下载" class="headerlink" title="直接使用URL下载"></a>直接使用URL下载</h3><p>使用URL直接下载时，将 huggingface.co 直接替换为域名 hf-mirror.com。可以使用浏览器，或者命令行工具如 wget -c、curl -L、aria2c 等。对于需要登录的模型，需在命令行中添加 —header hf_<em>*</em> 参数，具体获取token的方法请参见前文。</p><p>Hugging Face提供的包会获取系统变量，因此可以通过设置变量来解决：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HF_ENDPOINT=https://hf-mirror.com python your_script.py</span><br></pre></td></tr></tbody></table></figure><h3 id="Git克隆下载"><a href="#Git克隆下载" class="headerlink" title="Git克隆下载"></a>Git克隆下载</h3><p>此外，还有一种通过 git clone repo_url 下载的方式由官方提供。尽管这种方法相对简单，但却被认为是<strong>最不建议直接采用的途径</strong>，主要有以下三个弊端：</p><ol><li>不支持断点续传，一旦中断，需要重新启动整个下载过程。</li><li>占用大量磁盘空间,即使没有历史版本,也会存储一份元信息和当前版本文件拷贝;</li><li>对于一些存在历史版本的模型，<strong>下载时间会超过两倍</strong>。</li></ol><p>一个较好的方法是设置环境变量GIT_LFS_SKIP_SMUDGE=1开启git clone,之后单独使用其他支持断点续传的工具下载大文件。这种方式可以很好地兼顾断点续传和磁盘空间占用。</p><p><img src="https://picx.zhimg.com/80/v2-3e4aaa6d94bd5dd6b7cea0f745e581f8_720w.png?source=d16d100b" alt="Git下载"></p><h3 id="Hugging-Face-CLI命令行工具"><a href="#Hugging-Face-CLI命令行工具" class="headerlink" title="Hugging Face CLI命令行工具"></a>Hugging Face CLI命令行工具</h3><p>Hugging Face官方提供的 <a href="https://zhuanlan.zhihu.com/p/676222159///hf-mirror.com/docs/huggingface_hub/guides/download#download-from-the-cli"><strong>huggingface-cli</strong> </a> <strong>命令行工具功能很强大,极力推荐使用。它支持下载模型、数据集,登录注册,且长期持续更新维护。</strong></p><p>使用镜像网站可以加快速度。基本操作如下:安装依赖、设置环境变量、执行下载、处理需要登录的模型等情况。此外,可以尝试使用hf_transfer进行加速。</p><ol><li>安装依赖</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -U huggingface_hub</span><br></pre></td></tr></tbody></table></figure><ol><li>基本命令示例：</li></ol><p>这里使用了镜像网站<a href="https://hf-mirror.com/">https://hf-mirror.com</a>，会加快速度一点，<strong>hf-mirror.com</strong>，用于镜像 <a href="https://huggingface.co/">huggingface.co</a> 域名。</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HF_ENDPOINT=https://hf-mirror.com</span><br><span class="line"><span class="comment"># 下载模型</span></span><br><span class="line">huggingface-cli download --resume-download --local-dir-use-symlinks False bigscience/bloom-560m --local-dir bloom-560m</span><br><span class="line"><span class="comment"># 下载数据集</span></span><br><span class="line">huggingface-cli download --resume-download --repo-type dataset lavita/medical-qa-shared-task-v1-toy</span><br></pre></td></tr></tbody></table></figure><ol><li>下载需要登录的模型（Gated Model）</li></ol><p>请添加 —token hf<em><em>*</em> 参数，其中 hf</em><em>** 是 </em>access token*，请在 <a href="https://huggingface.co/settings/tokens">Hugging Face官网这里</a> 获取。示例：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">huggingface-cli download --token hf_*** --resume-download --local-dir-use-symlinks False meta-llama/Llama-2-7b-hf --local-dir Llama-2-7b-hf</span><br></pre></td></tr></tbody></table></figure><h3 id="hf-transfer加速"><a href="#hf-transfer加速" class="headerlink" title="hf_transfer加速"></a>hf_transfer加速</h3><p>hf_transfer 依附并兼容 huggingface-cli，是 hugging face 官方专门为提高下载速度基于 Rust 开发的一个模块。</p><p><strong>开启方法</strong></p><ol><li>安装依赖</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -U hf-transfer</span><br></pre></td></tr></tbody></table></figure><ol><li>设置 HF_HUB_ENABLE_HF_TRANSFER 环境变量为 1。</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HF_HUB_ENABLE_HF_TRANSFER=1</span><br></pre></td></tr></tbody></table></figure><ol><li>开启后使用方法同 huggingface-cli：</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">huggingface-cli download --resume-download bigscience/bloom-560m --local-dir bloom-560m</span><br></pre></td></tr></tbody></table></figure><p>以前hf_transfer没有进度条,需要根据是否有进度条判断是否成功开启。但从0.19版本开始已支持进度条,开启更为便捷。</p><p>不过,hf_transfer的鲁棒性较低,在网络不稳定时可能会报错。这是因为它还在完善过程中,对国内较差的网络状况支持不够好。如果出现错误,可以尝试关闭该模块以提高程序的容错性。</p><p>总体来说,hf_transfer能有效提速,但在网络不稳定时可能会出现问题。基于自身网络环境调整是否使用它是个不错的选择。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>总体来说，选择下载模型的方式取决于个人需求和网络环境。浏览器下载简单直接，但耗时耗力；Git克隆方式虽简单，但不支持断点续传且占用磁盘空间较大；Hugging Face CLI命令行工具功能强大，推荐使用，尤其搭配 hf_transfer 进行加速，但在网络不稳定时可能会遇到问题。根据实际情况，选择合适的方式可以更高效地获取所需模型。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;通常，我们可以通过代码直接下载模型，但有时会遇到诸多问题，例如下载速度慢或其他一些问题。在这里，我将主要介绍一些常用的简便方法。如果你想了解更多用法，请查看这篇知乎文章：&lt;a href=&quot;https://zhuanlan.zhihu.com/p/663712983，&quot;&gt;ht</summary>
      
    
    
    
    <category term="Note" scheme="https://kedreamix.github.io/categories/Note/"/>
    
    
    <category term="Note" scheme="https://kedreamix.github.io/tags/Note/"/>
    
  </entry>
  
  <entry>
    <title>Tailscale：随时随地远程和使用服务器</title>
    <link href="https://kedreamix.github.io/Note/Taiscale/"/>
    <id>https://kedreamix.github.io/Note/Taiscale/</id>
    <published>2024-01-03T05:28:38.000Z</published>
    <updated>2024-08-22T08:46:22.540Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Tailscale是什么？"><a href="#Tailscale是什么？" class="headerlink" title="Tailscale是什么？"></a>Tailscale是什么？</h2><p>网上有时候提到tailscale，总是介绍好多，比如以下介绍，但是太官方了</p><blockquote><p>Tailscale 是一种基于 WireGuard 的虚拟组网工具，和 Netmaker 类似，<strong>最大的区别在于 Tailscale 是在用户态实现了 WireGuard 协议，而 Netmaker 直接使用了内核态的 WireGuard</strong>。</p></blockquote><p>这里面简单介绍一下，实际上Tailscale属于一种虚拟组网工具，基于WireGuard。简单来说他能帮助我们把安装了Tailscale服务的机器，都放到同一个局域网。比如我们的NAS或者PC，或者在其他地方的NAS和PC，甚至云服务器都能放到同一个局域网。这样就实现了一个内网穿透，有时候我们就可以随时随地远程和使用我们的服务器。</p><h2 id="Tailscale能做什么？"><a href="#Tailscale能做什么？" class="headerlink" title="Tailscale能做什么？"></a><strong>Tailscale能做什么？</strong></h2><p>只需将你的设备连接到公网，Tailscale就能让所有设备加入同一个私有子网。这意味着，无论你身在何处，都可以轻松实现设备间的连接，就像它们在同一个局域网中一样。</p><p>举个例子，我的台式机和笔记本都登录了相同的Tailscale账号，它们共享一个100.64/10的子网，可以方便地互联。即使我的笔记本在公司内网，无法直接连接到家里的台式机，通过Tailscale的relay功能，它们依然能够直接连接，实现畅通无阻的通信。</p><h3 id="1、传输文件"><a href="#1、传输文件" class="headerlink" title="1、传输文件"></a><strong>1、传输文件</strong></h3><p>Tailscale内置了taildrop，可用于设备之间的文件传输。由于Tailscale支持android/ios/mac/windows/linux，因此它也是一个跨平台文件传输工具。如果设备在同一局域网内，传输速度将非常快速。</p><h3 id="2、远程开发"><a href="#2、远程开发" class="headerlink" title="2、远程开发"></a><strong>2、远程开发</strong></h3><p>举例来说，如果我的台式电脑运行Windows系统，我可以启动WSL2，安装SSHD，相当于将它变成一个服务器。这样，无论我身在何处，都可以通过笔记本上的VSCode Remote SSH随时打开台式机上的VSCode Server，实现远程开发。</p><p>对我而言，移动办公的真谛不在于随身携带一台笔记本，而是在任何地方只要有网络，就能使用任何设备接入统一的办公环境。</p><h3 id="3、代理"><a href="#3、代理" class="headerlink" title="3、代理"></a><strong>3、代理</strong></h3><p>具体可参考，这里不主要介绍，<a href="https://github.com/nadoo/glider">https://github.com/nadoo/glider</a></p><h2 id="Tailscale怎么用？"><a href="#Tailscale怎么用？" class="headerlink" title="Tailscale怎么用？"></a><strong>Tailscale怎么用？</strong></h2><p>参照这个官方页面安装，然后登录即可：<a href="https://tailscale.com/download">https://tailscale.com/download</a></p><p><img src="https://picx.zhimg.com/v2-0f0f761a65f56a0818e53fdef6592e11.png" alt="Tailscale官网"></p><p>如下图所示，一旦登录，每台设备都会被分配一个对应的IP地址。此时，所有设备实际上都在同一个局域网内，接下来我们可以启动设备的SSH功能。无论身在何处，只需使用分配给设备的IP地址，就能连接到该设备，从而实现远程办公的功能，例如连接到服务器等。</p><p><img src="https://picx.zhimg.com/v2-5c27422f68e185584f24f59621ae1ebf.png" alt="设备"></p><h2 id="Windows下安装OpenSSH"><a href="#Windows下安装OpenSSH" class="headerlink" title="Windows下安装OpenSSH"></a><strong>Windows下安装OpenSSH</strong></h2><p>OpenSSH 是 SSH （Secure SHell） 协议的免费开源实现。OpenSSH提供了服务端后台程序和客户端工具，用来加密远程控制和文件传输过程中的数据。安装以后，我们就可以把我们的电脑作为一台服务器进行链接，这样就成功可以随时随地进行远程连接了。</p><h3 id="在线安装"><a href="#在线安装" class="headerlink" title="在线安装"></a><strong>在线安装</strong></h3><p>一般windows自带SSH server，直接启动即可</p><p>开启方法： 安装openssh</p><p>设置-应用-应用和功能-可选功能-添加功能</p><p>安装OpenSSH服务器即可</p><p><img src="https://picx.zhimg.com/v2-aeb2d713f14ed7cf9c25f2340190a631.webp" alt="安装SSH步骤1"></p><p>访问可选功能屏幕。</p><p><img src="https://picx.zhimg.com/v2-f94b788ac25b51a54a85bf0074d81941.webp" alt="安装SSH步骤2"></p><p>选择添加要素的选项。</p><p><img src="https://picx.zhimg.com/v2-15d13a44cccd6e77e83d833b5ca28b7e.webp" alt="安装SSH步骤3"></p><p>选择 SSH 服务器包，然后单击”安装”按钮。</p><p><img src="https://pic1.zhimg.com/v2-7087d6f462dc72d88429d3825ac9f160.webp" alt="安装SSH步骤4"></p><p>等待 Openssh 服务器安装完成。</p><p><img src="https://pica.zhimg.com/v2-cf48691afcf3e5e8340c490e46d59c94.webp" alt="安装SSH步骤5"></p><p><strong>若是win11，可以在系统中，最下面找到可选功能，在可选功能里面，添加以上可选功能即可</strong></p><p>作为管理员，启动 Powershell 命令行的提升版本。</p><p><img src="https://picx.zhimg.com/v2-967a457fd357ab32cd55c5440953af9a.webp" alt="安装SSH步骤6"></p><p>将 SSH 服务配置为自动启动。</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sc config sshd start=auto</span><br></pre></td></tr></tbody></table></figure><p>启动 SSH 服务。</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net start sshd</span><br></pre></td></tr></tbody></table></figure><p>创建防火墙规则以允许在 SSH 端口上输入数据包。</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">netsh advfirewall firewall add rule name="SSH PORT 22" dir=in action=allow protocol=TCP localport=22</span><br></pre></td></tr></tbody></table></figure><p>祝贺！ 您已完成在 Windows 上安装 SSH 服务器。</p><p>上述成功以后可以测试一下，也就是简单的连接自己本机电脑，从这里面可以看到自己的用户名，密码是开机密码</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh localhost</span><br></pre></td></tr></tbody></table></figure><h3 id="离线安装"><a href="#离线安装" class="headerlink" title="离线安装"></a><strong>离线安装</strong></h3><p>下载最新版本 <a href="https://github.com/PowerShell/Win32-OpenSSH/releases">适用于Windows二进制文件的OpenSSH</a> （包OpenSSH-Win64.zip或OpenSSH-Win32.zip）</p><p>建议直接下载msi 即可，自动安装以后就可以按照上述方法进行启动ssh服务了</p><h2 id="连接SSH服务器"><a href="#连接SSH服务器" class="headerlink" title="连接SSH服务器"></a><strong>连接SSH服务器</strong></h2><p>接下来，我们就可以使用常规的SSH的方式来连接我们配置好的windows了，我们只需要找到对应的IP地址，对其进行ssh连接即可。</p><p>这里一定要注意，<strong>密码是你微软账户的密码</strong>。</p><p>如果在tailscale中已经分配了一个IP地址，这样就可以直接连接这个IP地址即可</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh 用户名@100.*.*.*</span><br></pre></td></tr></tbody></table></figure><p>连接成功后即可远程办公了，比如我在家里，我们可以设置公司的电脑的ssh，后续我们就可以连接公司的电脑后对其进行操作，这样就比较的方便，当然，也可以对其进行远程，比如todesk等远程工具，这个可以见仁见智，我觉得都可以，有时候我只使用终端，我觉得都还好。</p><p>参考</p><p><a href="https://blog.laisky.com/p/tailscale/">Laisky’s Blog</a></p><p><a href="https://zhuanlan.zhihu.com/p/452890363">小辣椒高效Office：Windows系统开启Ssh Server服务</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Tailscale是什么？&quot;&gt;&lt;a href=&quot;#Tailscale是什么？&quot; class=&quot;headerlink&quot; title=&quot;Tailscale是什么？&quot;&gt;&lt;/a&gt;Tailscale是什么？&lt;/h2&gt;&lt;p&gt;网上有时候提到tailscale，总是介绍好多，比如</summary>
      
    
    
    
    <category term="Note" scheme="https://kedreamix.github.io/categories/Note/"/>
    
    
    <category term="Linux" scheme="https://kedreamix.github.io/tags/Linux/"/>
    
    <category term="Note" scheme="https://kedreamix.github.io/tags/Note/"/>
    
  </entry>
  
  <entry>
    <title>超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis</title>
    <link href="https://kedreamix.github.io/Paper/Awesome-Talking-Head-Synthesis/"/>
    <id>https://kedreamix.github.io/Paper/Awesome-Talking-Head-Synthesis/</id>
    <published>2023-12-31T17:01:01.000Z</published>
    <updated>2024-01-30T05:04:13.235Z</updated>
    
    <content type="html"><![CDATA[<p>Gihub：<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis">https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis</a></p><p>这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。</p><p>论文合集及发布代码整理。✍️</p><p>大多数论文链接到“arXiv”或学术会议/期刊的PDF。但是,一些论文可能需要学术许可才能查看。</p><p>这个Awesome Talking Head Synthesis项目将持续更新 - 欢迎Pull Request。如果您有任何论文缺失、新增论文、关键研究人员或错别字建议,请编辑提交PR。您也可以打开Issue或直接通过电子邮件联系我。</p><p>如果您觉得这个仓库有用,请star⭐支持!</p><p><strong>2023年12月更新 📆</strong></p><p>感谢<a href="https://github.com/Curated-Awesome-Lists/awesome-ai-talking-heads">https://github.com/Curated-Awesome-Lists/awesome-ai-talking-heads</a>, 我增加了一些其内容,例如Tools&amp;Software和Slides&amp;Presentations模块。 希望这对您有帮助。</p><p>如果您对扩展这个聚合资源有任何想法或反馈,请打开Issue或PR——社区贡献对推进我们共同的知识至关重要。</p><p>让我们继续努力,实现更逼真的数字人脸表现!我们已经走了很长一段路,但还有很长的路要走。通过持续的研究和合作,我相信我们一定会达到目标!</p><p>如果您觉得这个仓库很有价值,请star⭐并分享给他人。您的支持可以激励我持续改进和维护它。如果您还有任何其他问题,请告诉我!</p><p>This repository organizes papers, codes and resources related to generative adversarial networks (GANs) 🤗 and neural radiance fields (NeRF) 🎨, with a main focus on image-driven and audio-driven talking head synthesis papers and released codes. 👤</p><p>Papers for Talking Head Synthesis, released codes collections. ✍️</p><p>Most papers are linked to PDFs on “arXiv” or journal/conference websites 📚. However, some papers require an academic license to view 🔐.</p><p>🔆 This project Awesome-Talking-Head-Synthesis is ongoing - pull requests are welcome! If you have any suggestions (missing papers, new papers, key researchers or typos), please feel free to edit and submit a PR. You can also open an issue or contact me directly via email. 📩</p><p>⭐ If you find this repo useful, please give it a star! 🤩</p><p><strong>2023.12 Update</strong> 📆</p><p>Thank you to <a href="https://github.com/Curated-Awesome-Lists/awesome-ai-talking-heads">https://github.com/Curated-Awesome-Lists/awesome-ai-talking-heads</a>, I have added some of its contents, such as <code>Tools &amp; Software</code> and <code>Slides &amp; Presentations</code>. 🙏 I hope this will be helpful.😊</p><p>If you have any feedback or ideas on extending this aggregated resource, please open an issue or PR - community contributions are vital to advancing this shared knowledge. 🤝</p><p>Let’s keep pushing forward to recreate ever more realistic digital human faces! 💪 We’ve come so far but still have a long way to go. With continued research 🔬 and collaboration, I’m sure we’ll get there! 🤗</p><p>Please feel free to star ⭐ and share this repo if you find it a valuable resource. Your support helps motivate me to keep maintaining and improving it. 🥰 Let me know if you have any other questions!</p><h2 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h2><p><img src="https://img-blog.csdnimg.cn/direct/841257d9dee74547bbd4f717794a9492.png#pic_center" alt="在这里插入图片描述"></p><div class="table-container"><table><thead><tr><th>Dataset</th><th>Download Link</th><th>Description</th></tr></thead><tbody><tr><td>Faceforensics++</td><td><a href="https://github.com/ondyari/FaceForensics">Download link</a></td><td></td></tr><tr><td>CelebV</td><td><a href="https://drive.google.com/file/d/1jQ6d76T5GQuvQH4dq8_Wq1T0cxvN0_xp/view">Download link</a></td><td></td></tr><tr><td>VoxCeleb</td><td><a href="https://www.robots.ox.ac.uk/~vgg/data/voxceleb/">Download link</a></td><td><code>VoxCeleb</code>, a comprehensive audio-visual dataset for speaker recognition, encompasses both VoxCeleb1 and VoxCeleb2 datasets.</td></tr><tr><td>VoxCeleb1</td><td><a href="https://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1.html">Download link</a></td><td><code>VoxCeleb1</code> contains over 100,000 utterances for 1,251 celebrities, extracted from videos uploaded to YouTube.</td></tr><tr><td>VoxCeleb2</td><td><a href="https://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox2.html">Download link</a></td><td>Extracted from YouTube videos, VoxCeleb2 includes video URLs and discourse timestamps. As the largest public audio-visual dataset, it is primarily used for speaker recognition tasks. However, it can also be utilized for training talking-head generation models. To obtain download permission and access the dataset, apply <a href="https://www.robots.ox.ac.uk/~vgg/data/voxceleb/">here</a>. Requires 300 GB+ storage space.</td></tr><tr><td>ObamaSet</td><td><a href="https://github.com/supasorn/synthesizing_obama_network_training">Download link</a></td><td><code>ObamaSet</code> is a specialized audio-visual dataset focused on analyzing the visual speech of former US President Barack Obama. All video samples are collected from his weekly address footage. Unlike previous datasets, it exclusively centers on Barack Obama and does not provide any human annotations.</td></tr><tr><td>TalkingHead-1KH</td><td><a href="https://github.com/tcwang0509/TalkingHead-1KH">Download link</a></td><td>The dataset consists of 500k video clips, of which about 80k are greater than 512x512 resolution. Only videos under permissive licenses are included. Note that the number of videos differ from that in the original paper because a more robust preprocessing script was used to split the videos.</td></tr><tr><td>LRW (Lip Reading in the Wild)</td><td><a href="https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrw1.html">Download link</a></td><td>LRW, a diverse English-speaking video dataset from the BBC program, features over 1000 speakers with various speaking styles and head poses. Each video is 1.16 seconds long (29 frames) and involves the target word along with context.</td></tr><tr><td>MEAD 2020</td><td><a href="https://github.com/uniBruce/Mead">Download link</a></td><td>MEAD 2020 is a Talking Head dataset annotated with emotion labels and intensity labels. The dataset focuses on facial generation for natural emotional speech, covering eight different emotions on three intensity levels.</td></tr><tr><td>CelebV-HQ</td><td><a href="https://github.com/CelebV-HQ/CelebV-HQ">Download link</a></td><td>CelebV-HQ is a high-quality video dataset comprising 35,666 clips with a resolution of at least 512x512. It includes 15,653 identities, and each clip is manually labeled with 83 facial attributes, spanning appearance, action, and emotion. The dataset’s diversity and temporal coherence make it a valuable resource for tasks like unconditional video generation and video facial attribute editing.</td></tr><tr><td>HDTF</td><td><a href="https://github.com/MRzzm/HDTF">Download link</a></td><td>HDTF, the High-definition Talking-Face Dataset, is a large in-the-wild high-resolution audio-visual dataset consisting of approximately 362 different videos totaling 15.8 hours. Original video resolutions are 720 P or 1080 P, and each cropped video is resized to 512 × 512.</td></tr><tr><td>CREMA-D</td><td><a href="https://github.com/CheyneyComputerScience/CREMA-D">Download link</a></td><td>CREMA-D is a diverse dataset with 7,442 original clips featuring 91 actors, including 48 male and 43 female actors aged 20 to 74, representing various races and ethnicities. The dataset includes recordings of actors speaking from a set of 12 sentences, expressing six different emotions (Anger, Disgust, Fear, Happy, Neutral, and Sad) at four emotion levels (Low, Medium, High, and Unspecified). Emotion and intensity ratings were gathered through crowd-sourcing, with 2,443 participants rating 90 unique clips each (30 audio, 30 visual, and 30 audio-visual). Over 95% of the clips have more than 7 ratings. For additional details on CREMA-D, refer to the <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4313618/">paper link</a>.</td></tr><tr><td>LRS2</td><td><a href="https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrs2.html">Download link</a></td><td>LRS2 is a lip reading dataset that includes videos recorded in diverse settings, suitable for studying lip reading and visual speech recognition.</td></tr><tr><td>GRID</td><td><a href="http://spandh.dcs.shef.ac.uk/avlombard/">Download link</a></td><td>The GRID dataset was recorded in a laboratory setting with 34 volunteers, each speaking 1000 phrases, totaling 34,000 utterance instances. Phrases follow specific rules, with six words randomly selected from six categories: “command,” “color,” “preposition,” “letter,” “number,” and “adverb.” Access the dataset <a href="https://spandh.dcs.shef.ac.uk/gridcorpus/">here</a>.</td></tr><tr><td>SAVEE</td><td><a href="http://kahlan.eps.surrey.ac.uk/savee/Download.html">Download link</a></td><td>The SAVEE (Surrey Audio-Visual Expressed Emotion) database is a crucial component for developing an automatic emotion recognition system. It features recordings from 4 male actors expressing 7 different emotions, totaling 480 British English utterances. These sentences, selected from the standard TIMIT corpus, are phonetically balanced for each emotion. Recorded in a high-quality visual media lab, the data undergoes processing and labeling. Performance evaluation involves 10 subjects rating recordings under audio, visual, and audio-visual conditions. Classification systems for each modality achieve speaker-independent recognition rates of 61%, 65%, and 84% for audio, visual, and audio-visual, respectively.</td></tr><tr><td>BIWI(3D)</td><td><a href="https://data.vision.ee.ethz.ch/cvl/datasets/b3dac2.en.html">Download link</a></td><td>The Biwi 3D Audiovisual Corpus of Affective Communication serves as a compromise between data authenticity and quality, acquired at ETHZ in collaboration with SYNVO GmbH.</td></tr><tr><td>VOCA</td><td><a href="https://voca.is.tue.mpg.de/">Download link</a></td><td>VOCA is a 4D-face dataset with approximately 29 minutes of 4D face scans and synchronized audio from 12-bit speakers. It greatly facilitates research in 3D VSG.</td></tr><tr><td>Multiface(3D)</td><td><a href="https://github.com/facebookresearch/multiface">Download link</a></td><td>The Multiface Dataset consists of high-quality multi-view video recordings of 13 people displaying various facial expressions. It contains approximately 12,200 to 23,000 frames per subject, captured at 30 fps from around 40 to 160 camera views with uniform lighting. The dataset’s size is 65TB and includes raw images (2048x1334 resolution), tracked and meshed heads, 1024x1024 unwrapped face textures, camera calibration metadata, and audio. This repository provides code for downloading the dataset and building a codec avatar using a deep appearance model.</td></tr><tr><td>MMFace4D</td><td><a href="https://wuhaozhe.github.io/mmface4d/">Download link</a></td><td>The MMFace4D dataset is a large-scale multi-modal dataset for audio-driven 3D facial animation research. It contains over 35,000 sequences captured from 431 subjects ranging in age from 15 to 68 years old. Various sentences from scenarios such as news broadcasting, conversations and storytelling were recorded, totaling around 11,000 utterances. High-fidelity data was captured using three synchronized RGB-D cameras to obtain high-resolution 3D meshes and textures. A reconstruction pipeline was developed to fuse the multi-view data and generate topology-consistent 3D mesh sequences. In addition to the 3D facial motions, synchronized speech audio is also provided. The final dataset covers a wide range of expressive talking styles and facial expressions through a diverse set of subjects and utterances. With its large scale, high quality of data and strong diversity, the MMFace4D dataset provides an ideal benchmark for developing and evaluating audio-driven 3D facial animation models.</td></tr></tbody></table></div><hr><h2 id="Survey"><a href="#Survey" class="headerlink" title="Survey"></a>Survey</h2><div class="table-container"><table><thead><tr><th>Year</th><th>Title</th><th>Conference/Journal</th></tr></thead><tbody><tr><td>2024</td><td><a href="https://arxiv.org/pdf/2401.03890.pdf">A Survey on 3D Gaussian Splatting</a> 3DGS🔥🔥🔥<strong>on going</strong></td><td>arXiv 2024</td></tr><tr><td>2024</td><td><a href="https://arxiv.org/pdf/2304.10050.pdf">Neural Radiance Fields: Past, Present, and Future</a>  NeRF🔥🔥🔥 <strong>Amazing 413 pages</strong></td><td>arXiv 2024</td></tr><tr><td>2023</td><td><a href="https://arxiv.org/abs/2308.16041">From Pixels to Portraits: A Comprehensive Survey of Talking Head Generation Techniques and Applications</a></td><td>arXiv 2023</td></tr><tr><td>2023</td><td><a href="https://www.mdpi.com/2079-9292/12/1/218">Human-Computer Interaction System: A Survey of Talking-Head Generation</a></td><td>IEEE</td></tr><tr><td>2023</td><td><a href="https://dl.acm.org/doi/10.1016/j.eswa.2023.119678">Talking human face generation: A survey</a></td><td>ACM</td></tr><tr><td>2022</td><td><a href="https://arxiv.org/abs/2205.10839">Deep Learning for Visual Speech Analysis: A Survey</a></td><td>arXiv 2022</td></tr><tr><td>2020</td><td><a href="https://arxiv.org/abs/2005.03201">What comprises a good talking-head video generation?: A Survey and Benchmark</a></td><td>arXiv 2020</td></tr></tbody></table></div><hr><h2 id="Funny-Work"><a href="#Funny-Work" class="headerlink" title="Funny Work"></a>Funny Work</h2><div class="table-container"><table><thead><tr><th>Year</th><th>Title</th><th>Code</th><th>Project</th><th>Keywords</th></tr></thead><tbody><tr><td>2024</td><td>[Audio2Photoreal] <a href="https://arxiv.org/pdf/2401.01885.pdf">From Audio to Photoreal Embodiment: Synthesizing Humans in Conversations</a></td><td><a href="https://github.com/facebookresearch/audio2photoreal/">Code</a></td><td><a href="https://people.eecs.berkeley.edu/~evonne_ng/projects/audio2photoreal/#">Project</a></td><td>Photoreal</td></tr><tr><td>2024</td><td>[Animate Anyone] <a href="https://arxiv.org/pdf/2311.17117.pdf">Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation</a></td><td><a href="https://github.com/HumanAIGC/AnimateAnyone">Code</a></td><td><a href="https://humanaigc.github.io/animate-anyone/">Project</a></td><td>🔥Animate (阿里科目三驱动)</td></tr><tr><td>2024</td><td>[3DGAN] <a href="https://research.nvidia.com/labs/nxp/wysiwyg/media/WYSIWYG.pdf">What You See Is What You GAN: Rendering Every Pixel for High-Fidelity Geometry in 3D GANs</a></td><td></td><td><a href="https://research.nvidia.com/labs/nxp/wysiwyg/">Project</a></td><td>🔥Nvidia</td></tr></tbody></table></div><hr><h2 id="Audio-driven"><a href="#Audio-driven" class="headerlink" title="Audio-driven"></a>Audio-driven</h2><div class="table-container"><table><thead><tr><th>Year</th><th>Title</th><th>Conference/Journal</th><th>Code</th><th>Project</th><th>Keywords</th></tr></thead><tbody><tr><td>2024</td><td>[Real3D-Portrait] <a href="http://arxiv.org/abs/2401.08503v2">Real3D-Portrait: One-shot Realistic 3D Talking Portrait Synthesis</a></td><td>ICLR 2024</td><td></td><td><a href="https://real3dportrait.github.io/">Project</a></td><td>3D, One-Shot,Realistic</td></tr><tr><td>2024</td><td>[AdaMesh] <a href="http://arxiv.org/abs/2310.07236v2">AdaMesh: Personalized Facial Expressions and Head Poses for Adaptive   Speech-Driven 3D Facial Animation</a></td><td>Arix 2024</td><td><a href="https://github.com/adamesh/adamesh">Code</a></td><td><a href="https://adamesh.github.io">Project</a></td><td>3D,Mesh</td></tr><tr><td>2024</td><td>[DREAM-Talk] <a href="http://arxiv.org/abs/2312.13578v1">DREAM-Talk: Diffusion-based Realistic Emotional Audio-driven Method for Single Image Talking Face Generation</a></td><td>Arix 2024</td><td></td><td><a href="https://magic-research.github.io/dream-talk/">Project</a></td><td>Emotion</td></tr><tr><td>2024</td><td>[AE-NeRF] <a href="http://arxiv.org/abs/2312.10921v1">AE-NeRF: Audio Enhanced Neural Radiance Field for Few Shot Talking Head Synthesis</a></td><td>AAAI 2024</td><td></td><td></td><td></td></tr><tr><td>2024</td><td>[VectorTalker] <a href="http://arxiv.org/abs/2312.11568v1">VectorTalker: SVG Talking Face Generation with Progressive Vectorisation</a></td><td>Arix 2024</td><td></td><td></td><td>SVG</td></tr><tr><td>2024</td><td>[VectorTalker] <a href="http://arxiv.org/abs/2312.11568v1">VectorTalker: SVG Talking Face Generation with Progressive Vectorisation</a></td><td>Arix 2024</td><td></td><td></td><td></td></tr><tr><td>2024</td><td>[Mimic] <a href="http://arxiv.org/abs/2312.10877v1">Mimic: Speaking Style Disentanglement for Speech-Driven 3D Facial Animation</a></td><td>AAAI 2024</td><td></td><td></td><td>3D</td></tr><tr><td>2024</td><td>[DreamTalk] <a href="http://arxiv.org/abs/2312.09767v1">DreamTalk: When Expressive Talking Head Generation Meets Diffusion Probabilistic Models</a></td><td>Arix 2024</td><td><a href="https://github.com/damo-vilab/i2vgen-xl">Code</a></td><td><a href="https://dreamtalk-project.github.io">Project</a></td><td>Diffusion</td></tr><tr><td>2024</td><td>[FaceTalk] <a href="http://arxiv.org/abs/2312.08459v1">FaceTalk: Audio-Driven Motion Diffusion for Neural Parametric Head Models</a></td><td>Arix 2024</td><td><a href="https://github.com/shivangi-aneja/FaceTalk">Code</a></td><td><a href="https://shivangi-aneja.github.io/projects/facetalk/">Project</a></td><td></td></tr><tr><td>2024</td><td>[GSmoothFace] <a href="http://arxiv.org/abs/2312.07385v1">GSmoothFace: Generalized Smooth Talking Face Generation via Fine Grained   3D Face Guidance</a></td><td>Arix 2024</td><td></td><td></td><td>3D</td></tr><tr><td>2024</td><td>[GMTalker] <a href="http://arxiv.org/abs/2312.07669v1">GMTalker: Gaussian Mixture based Emotional talking video Portraits</a></td><td>Arix 2024</td><td></td><td><a href="https://bob35buaa.github.io/GMTalker">Project</a></td><td>Emotion</td></tr><tr><td>2024</td><td>[VividTalk] <a href="http://arxiv.org/abs/2312.01841v2">VividTalk: One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior</a></td><td>Arix 2024</td><td></td><td></td><td>Mesh</td></tr><tr><td>2024</td><td>[GAIA] <a href="https://arxiv.org/pdf/2311.15230.pdf">GAIA: Zero-shot Talking Avatar Generation</a></td><td>Arix 2024</td><td>Code(coming)</td><td><a href="https://microsoft.github.io/GAIA/">Project</a></td><td>😲😲😲</td></tr><tr><td>2023</td><td><a href="https://arxiv.org/abs/2307.10008">Implicit Identity Representation Conditioned Memory Compensation Network for Talking Head Video Generation</a></td><td>ICCV 2023</td><td><a href="https://github.com/harlanhong/ICCV2023-MCNET">Code</a></td><td><a href="https://harlanhong.github.io/publications/mcnet.html">Project</a></td><td>-</td></tr><tr><td>2023</td><td>[ToonTalker] <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Gong_ToonTalker_Cross-Domain_Face_Reenactment_ICCV_2023_paper.pdf">ToonTalker: Cross-Domain Face Reenactment</a></td><td>ICCV 2023</td><td>-</td><td>-</td><td>-</td></tr><tr><td>2023</td><td><a href="https://arxiv.org/abs/2309.04946">Efficient Emotional Adaptation for Audio-Driven Talking-Head Generation</a></td><td>ICCV 2023</td><td><a href="https://github.com/yuangan/EAT_code">Code</a></td><td><a href="https://yuangan.github.io/eat/">Project</a></td><td>-</td></tr><tr><td>2023</td><td>[EMMN] <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Tan_EMMN_Emotional_Motion_Memory_Network_for_Audio-driven_Emotional_Talking_Face_ICCV_2023_paper.pdf">EMMN: Emotional Motion Memory Network for Audio-driven Emotional Talking Face Generation</a></td><td>ICCV 2023</td><td>-</td><td>-</td><td>Emotion</td></tr><tr><td>2023</td><td><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Song_Emotional_Listener_Portrait_Neural_Listener_Head_Generation_with_Emotion_ICCV_2023_paper.pdf">Emotional Listener Portrait: Realistic Listener Motion Simulation in Conversation</a></td><td>ICCV 2023</td><td>-</td><td>-</td><td>Emotion,LHG</td></tr><tr><td>2023</td><td>[MODA] <a href="https://arxiv.org/abs/2307.10008">MODA: Mapping-Once Audio-driven Portrait Animation with Dual Attentions</a></td><td>ICCV 2023</td><td>-</td><td>-</td><td>-</td></tr><tr><td>2023</td><td>[Facediffuser] <a href="https://dl.acm.org/doi/abs/10.1145/3623264.3624447">Facediffuser: Speech-driven 3d facial animation synthesis using diffusion</a></td><td>ACM SIGGRAPH MIG 2023</td><td><a href="https://github.com/uuembodiedsocialai/FaceDiffuser">Code</a></td><td><a href="https://uuembodiedsocialai.github.io/FaceDiffuser/">Project</a></td><td>🔥Diffusion,3D</td></tr><tr><td>2023</td><td><a href="https://arxiv.org/abs/2309.00030">Audio-Driven Dubbing for User Generated Contents via Style-Aware Semi-Parametric Synthesis</a></td><td>TCSVT 2023</td><td>-</td><td>-</td><td></td></tr><tr><td>2023</td><td>[SadTalker] <a href="https://arxiv.org/pdf/2211.12194.pdf">SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation</a></td><td>CVPR 2023</td><td><a href="https://github.com/Winfredy/SadTalker">Code</a></td><td><a href="https://sadtalker.github.io/">Project</a></td><td>3D,Single Image</td></tr><tr><td>2023</td><td>[EmoTalk] <a href="https://arxiv.org/abs/2303.11089">EmoTalk: Speech-Driven Emotional Disentanglement for 3D Face Animation</a></td><td>ICCV 2023</td><td><a href="https://github.com/ZiqiaoPeng/EmoTalk">Code</a></td><td></td><td>3D,Emotion</td></tr><tr><td>2023</td><td><a href="https://arxiv.org/abs/2306.03594">Emotional Talking Head Generation based on Memory-Sharing and Attention-Augmented Networks</a></td><td>InterSpeech 2023</td><td></td><td></td><td>Emotion</td></tr><tr><td>2023</td><td>[DINet] <a href="https://fuxivirtualhuman.github.io/pdf/AAAI2023_FaceDubbing.pdf">DINet: Deformation Inpainting Network for Realistic Face Visually Dubbing on High Resolution Video</a></td><td>AAAI 2023</td><td><a href="https://github.com/MRzzm/DINet">Code</a></td><td>-</td><td></td></tr><tr><td>2023</td><td>[StyleTalk] <a href="https://arxiv.org/abs/2301.01081">StyleTalk: One-shot Talking Head Generation with Controllable Speaking Styles</a></td><td>AAAI 2023</td><td><a href="https://github.com/FuxiVirtualHuman/styletalk">Code</a></td><td>-</td><td>Style</td></tr><tr><td>2023</td><td><a href="https://arxiv.org/abs/2305.02572">High-fidelity Generalized Emotional Talking Face Generation with Multi-modal Emotion Space Learning</a></td><td>CVPR 2023</td><td>-</td><td>-</td><td>Emotion</td></tr><tr><td>2023</td><td>[StyleSync] <a href="https://arxiv.org/pdf/2305.05445.pdf">StyleSync: High-Fidelity Generalized and Personalized Lip Sync in Style-based Generator</a></td><td>CVPR 2023</td><td><a href="https://github.com/guanjz20/StyleSync">Code</a></td><td><a href="https://hangz-nju-cuhk.github.io/projects/StyleSync">Project</a></td><td>-</td></tr><tr><td>2023</td><td>[TalkLip] <a href="https://arxiv.org/pdf/2303.17480.pdf">TalkLip: Seeing What You Said - Talking Face Generation Guided by a Lip Reading Expert</a></td><td>CVPR 2023</td><td><a href="https://github.com/Sxjdwang/TalkLip">Code</a></td><td>-</td><td>-</td></tr><tr><td>2023</td><td>[CodeTalker] <a href="https://arxiv.org/abs/2301.02379">CodeTalker: Speech-Driven 3D Facial Animation with Discrete Motion Prior</a></td><td>CVPR 2023</td><td><a href="https://github.com/Doubiiu/CodeTalker">Code</a></td><td><a href="https://doubiiu.github.io/projects/codetalker/">Project</a></td><td>3D,codebook</td></tr><tr><td>2023</td><td>[EmoGen] <a href="https://arxiv.org/pdf/2303.11548.pdf">Emotionally Enhanced Talking Face Generation</a></td><td>Arxiv 2023</td><td><a href="https://github.com/sahilg06/EmoGen">Code</a></td><td>-</td><td>Emotion</td></tr><tr><td>2023</td><td>[DAE-Talker] <a href="https://arxiv.org/pdf/2303.17550.pdf">DAE-Talker: High Fidelity Speech-Driven Talking Face Generation with Diffusion Autoencoder</a></td><td>Arxiv 2023</td><td>-</td><td><a href="https://mstypulkowski.github.io/diffusedheads/">Project</a></td><td>🔥Diffusion</td></tr><tr><td>2023</td><td>[READ] <a href="READ Avatars: Realistic Emotion-controllable Audio Driven Avatars">READ Avatars: Realistic Emotion-controllable Audio Driven Avatars</a></td><td>Arxiv 2023</td><td>-</td><td>-</td><td>-</td></tr><tr><td>2023</td><td>[DiffTalk] <a href="https://arxiv.org/abs/2301.03786">DiffTalk: Crafting Diffusion Models for Generalized Talking Head Synthesis</a></td><td>CVPR 2023</td><td><a href="https://github.com/sstzal/DiffTalk">Code</a></td><td><a href="https://sstzal.github.io/DiffTalk/">Project</a></td><td>🔥Diffusion</td></tr><tr><td>2023</td><td>[Diffused Heads] <a href="https://mstypulkowski.github.io/diffusedheads/diffused_heads.pdf">Diffused Heads: Diffusion Models Beat GANs on Talking-Face Generation</a></td><td>Arxiv 2023</td><td>-</td><td><a href="https://mstypulkowski.github.io/diffusedheads/">Project</a></td><td>🔥Diffusion</td></tr><tr><td>2022</td><td>[MemFace] <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Liang_Expressive_Talking_Head_Generation_With_Granular_Audio-Visual_Control_CVPR_2022_paper.pdf">Expressive Talking Head Generation with Granular Audio-Visual Control</a></td><td>CVPR 2022</td><td>-</td><td>-</td><td>-</td></tr><tr><td>2022</td><td><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Song_Talking_Face_Generation_With_Multilingual_TTS_CVPR_2022_paper.pdf">Talking Face Generation with Multilingual TTS</a></td><td>CVPR 2022</td><td><a href="https://huggingface.co/spaces/CVPR/ml-talking-face">Demo Track</a></td><td>-</td><td>-</td></tr><tr><td>2022</td><td>[EAMM] <a href="https://arxiv.org/pdf/2205.15278.pdf">EAMM: One-Shot Emotional Talking Face via Audio-Based Emotion-Aware Motion Model</a></td><td>SIGGRAPH 2022</td><td>-</td><td>-</td><td>Emotion</td></tr><tr><td>2022</td><td>[SPACEx] <a href="https://arxiv.org/pdf/2211.09809.pdf">SPACEx 🚀: Speech-driven Portrait Animation with Controllable Expression</a></td><td>arXiv 2022</td><td>-</td><td><a href="https://deepimagination.cc/SPACEx/">Project</a></td><td>-</td></tr><tr><td>2022</td><td>[AV-CAT] <a href="https://arxiv.org/pdf/2212.04970.pdf">Masked Lip-Sync Prediction by Audio-Visual Contextual Exploitation in Transformers</a></td><td>SIGGRAPH Asia 2022</td><td>-</td><td>-</td><td>-</td></tr><tr><td>2022</td><td>[MemFace] <a href="https://arxiv.org/pdf/2212.05005.pdf">Memories are One-to-Many Mapping Alleviators in Talking Face Generation</a></td><td>arXiv 2022</td><td>-</td><td>-</td><td>-</td></tr><tr><td>2021</td><td>[PC-AVS] <a href="https://arxiv.org/abs/2104.11116">PC-AVS: Pose-Controllable Talking Face Generation by Implicitly Modularized Audio-Visual Representation</a></td><td>CVPR 2021</td><td><a href="https://github.com/Hangz-nju-cuhk/Talking-Face_PC-AVS">Code</a></td><td><a href="https://hangz-nju-cuhk.github.io/projects/PC-AVS">Project</a></td><td>-</td></tr><tr><td>2021</td><td>[IATS] <a href="https://arxiv.org/abs/2111.00203">Imitating Arbitrary Talking Style for Realistic Audio-Driven Talking Face Synthesis</a></td><td>ACM MM 2021</td><td>-</td><td>-</td><td>-</td></tr><tr><td>2021</td><td>[Speech2Talking-Face] <a href="https://www.ijcai.org/proceedings/2021/0141.pdf">Speech2Talking-Face: Inferring and Driving a Face with Synchronized Audio-Visual Representation</a></td><td>IJCAI 2021</td><td>-</td><td>-</td><td>-</td></tr><tr><td>2021</td><td>[FAU] <a href="https://arxiv.org/pdf/2110.09951.pdf">Talking Head Generation with Audio and Speech Related Facial Action Units</a></td><td>BMVC 2021</td><td>-</td><td>-</td><td>AU</td></tr><tr><td>2021</td><td>[EVP] <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Ji_Audio-Driven_Emotional_Video_Portraits_CVPR_2021_paper.pdf">Audio-Driven Emotional Video Portraits</a></td><td>CVPR 2021</td><td><a href="https://github.com/jixinya/EVP">Code</a></td><td>-</td><td>Emotion</td></tr><tr><td>2021</td><td>[IATS] <a href="https://dl.acm.org/doi/pdf/10.1145/3474085.3475280">IATS: Imitating Arbitrary Talking Style for Realistic Audio-Driven Talking Face Synthesis</a></td><td>ACM Multimedia 2021</td><td>-</td><td>-</td><td>-</td></tr><tr><td>2020</td><td>[Wav2Lip] <a href="http://arxiv.org/pdf/2008.10010.pdf">A Lip Sync Expert Is All You Need for Speech to Lip Generation In The Wild</a></td><td>ACM Multimedia 2020</td><td><a href="https://github.com/Rudrabha/Wav2Lip">Code</a></td><td><a href="http://cvit.iiit.ac.in/research/projects/cvit-projects/a-lip-sync-expert-is-all-you-need-for-speech-to-lip-generation-in-the-wild/">Project</a></td><td>-</td></tr><tr><td>2020</td><td>[RhythmicHead] <a href="https://arxiv.org/pdf/2007.08547v1.pdf">Talking-head Generation with Rhythmic Head Motion</a></td><td>ECCV 2020</td><td><a href="https://github.com/lelechen63/Talking-head-Generation-with-Rhythmic-Head-Motion">Code</a></td><td>-</td><td>-</td></tr><tr><td>2020</td><td>[MakeItTalk] <a href="https://arxiv.org/pdf/2006.09661.pdf">Speaker-Aware Talking-Head Animation</a></td><td>SIGGRAPH Asia 2020</td><td><a href="https://github.com/yzhou359/MakeItTalk">Code</a></td><td><a href="https://people.umass.edu/~yangzhou/MakeItTalk/">Project</a></td><td>-</td></tr><tr><td>2020</td><td>[Neural Voice Puppetry] <a href="https://arxiv.org/pdf/1912.05566.pdf">Audio-driven Facial Reenactment</a></td><td>ECCV 2020</td><td>-</td><td><a href="https://justusthies.github.io/posts/neural-voice-puppetry/">Project</a></td><td>-</td></tr><tr><td>2020</td><td>[MEAD] <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660698.pdf">A Large-scale Audio-visual Dataset for Emotional Talking-face Generation</a></td><td>ECCV 2020</td><td><a href="https://github.com/uniBruce/Mead">Code</a></td><td><a href="https://wywu.github.io/projects/MEAD/MEAD.html">Project</a></td><td>-</td></tr><tr><td>2020</td><td><a href="https://arxiv.org/pdf/1906.06337.pdf">Realistic Speech-Driven Facial Animation with GANs</a></td><td>IJCV 2020</td><td>-</td><td>-</td><td>-</td></tr><tr><td>2019</td><td>[DAVS] <a href="https://arxiv.org/pdf/1807.07860.pdf">Talking Face Generation by Adversarially Disentangled Audio-Visual Representation</a></td><td>AAAI 2019</td><td><a href="https://github.com/Hangz-nju-cuhk/Talking-Face-Generation-DAVS">Code</a></td><td>-</td><td>-</td></tr><tr><td>2019</td><td>[ATVGnet] <a href="https://www.cs.rochester.edu/~cxu22/p/cvpr2019_facegen_paper.pdf">Hierarchical Cross-modal Talking Face Generation with Dynamic Pixel-wise Loss</a></td><td>CVPR 2019</td><td><a href="https://github.com/lelechen63/ATVGnet">Code</a></td><td>-</td><td>-</td></tr><tr><td>2018</td><td><a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Lele_Chen_Lip_Movements_Generation_ECCV_2018_paper.pdf">Lip Movements Generation at a Glance</a></td><td>ECCV 2018</td><td><a href="https://github.com/lelechen63/3d_gan">Code</a></td><td>-</td><td>-</td></tr><tr><td>2018</td><td>[VisemeNet] <a href="https://arxiv.org/pdf/1805.09488.pdf">Audio-Driven Animator-Centric Speech Animation</a></td><td>SIGGRAPH 2018</td><td>-</td><td>-</td><td>-</td></tr><tr><td>2017</td><td>[Synthesizing Obama] <a href="https://grail.cs.washington.edu/projects/AudioToObama/siggraph17_obama.pdf">Learning Lip Sync From Audio</a></td><td>SIGGRAPH 2017</td><td>-</td><td><a href="https://grail.cs.washington.edu/projects/AudioToObama/">Project</a></td><td>-</td></tr><tr><td>2017</td><td>[You Said That?] <a href="https://arxiv.org/abs/1705.02966">Synthesising Talking Faces From Audio</a></td><td>BMVC 2019</td><td><a href="https://github.com/joonson/yousaidthat">Code</a></td><td>-</td><td>-</td></tr><tr><td>2017</td><td><a href="https://users.aalto.fi/~laines9/publications/karras2017siggraph_paper.pdf">Audio-Driven Facial Animation by Joint End-to-End Learning of Pose and Emotion</a></td><td>SIGGRAPH 2017</td><td>-</td><td>-</td><td>-</td></tr><tr><td>2017</td><td><a href="https://home.ttic.edu/~taehwan/taylor_etal_siggraph2017.pdf">A Deep Learning Approach for Generalized Speech Animation</a></td><td>SIGGRAPH 2017</td><td>-</td><td>-</td><td>-</td></tr><tr><td>2016</td><td>[LRW] <a href="https://www.robots.ox.ac.uk/~vgg/publications/2016/Chung16/chung16.pdf">Lip Reading in the Wild</a></td><td>ACCV 2016</td><td>-</td><td>-</td><td>-</td></tr></tbody></table></div><hr><h2 id="Text-driven"><a href="#Text-driven" class="headerlink" title="Text-driven"></a>Text-driven</h2><div class="table-container"><table><thead><tr><th>Year</th><th>Title</th><th>Conference/Journal</th><th>Code/Proj</th></tr></thead><tbody><tr><td>2023</td><td><a href="https://arxiv.org/pdf/2304.00334.pdf">TalkCLIP: Talking Head Generation with Text-Guided Expressive Speaking Styles</a></td><td>Arxiv</td><td></td></tr><tr><td>2021</td><td><a href="https://arxiv.org/abs/2104.07995">Write-a-speaker: Text-based Emotional and Rhythmic Talking-head Generation</a></td><td>AAAI</td><td><a href="https://github.com/FuxiVirtualHuman/Write-a-Speaker">Code</a></td></tr><tr><td>2021</td><td><a href="https://arxiv.org/abs/2106.14014v3">Txt2vid: Ultra-low bitrate compression of talking-head videos via text</a></td><td>Arxiv</td><td><a href="https://github.com/tpulkit/txt2vid">Code</a></td></tr></tbody></table></div><hr><h2 id="NeRF-amp-3D-amp-Gaussian-Splatting"><a href="#NeRF-amp-3D-amp-Gaussian-Splatting" class="headerlink" title="NeRF &amp; 3D &amp; Gaussian Splatting"></a>NeRF &amp; 3D &amp; Gaussian Splatting</h2><div class="table-container"><table><thead><tr><th>Year</th><th>Title</th><th>Conference/Journal</th><th>Code</th><th>Project</th><th>Keywords</th></tr></thead><tbody><tr><td>2024</td><td>[UltrAvatar] <a href="http://arxiv.org/abs/2401.11078v1">UltrAvatar: A Realistic Animatable 3D Avatar Diffusion Model with Authenticity Guided Textures</a></td><td>Arxiv 2024</td><td></td><td><a href="http://usrc-sea.github.io/UltrAvatar/">Project</a></td><td>Diffusion,Avatar</td></tr><tr><td>2024</td><td>[GaussianBody] <a href="http://arxiv.org/abs/2401.09720v1">GaussianBody: Clothed Human Reconstruction via 3d Gaussian Splatting</a></td><td>Arxiv 2024</td><td></td><td></td><td>🔥Gaussian Splatting</td></tr><tr><td>2024</td><td><a href="http://arxiv.org/abs/2401.02616v1">FED-NeRF: Achieve High 3D Consistency and Temporal Coherence for Face Video Editing on Dynamic NeRF</a></td><td></td><td><a href="https://github.com/ZHANG1023/FED-NeRF">Code</a></td><td></td><td>4D face video editor</td></tr><tr><td>2024</td><td>[AGG] <a href="http://arxiv.org/abs/2401.04099v1">AGG: Amortized Generative 3D Gaussians for Single Image to 3D</a></td><td>Arxiv 2024</td><td></td><td><a href="https://ir1d.github.io/AGG/">Project</a></td><td>🔥Gaussian Splatting</td></tr><tr><td>2024</td><td><a href="http://arxiv.org/abs/2401.06116v1">Gaussian Shadow Casting for Neural Characters</a></td><td>Arxiv 2024</td><td></td><td></td><td>🔥Gaussian Splatting</td></tr><tr><td>2024</td><td>[Human101] <a href="http://arxiv.org/abs/2312.15258v1">Human101: Training 100+FPS Human Gaussians in 100s from 1 View</a></td><td>Arxiv 2024</td><td></td><td><a href="https://github.com/longxiang-ai/Human101">Project</a></td><td>🔥Gaussian Splatting</td></tr><tr><td>2024</td><td><a href="http://arxiv.org/abs/2312.15059v1">Deformable 3D Gaussian Splatting for Animatable Human Avatars</a></td><td>Arxiv 2024</td><td></td><td></td><td>🔥Gaussian Splatting</td></tr><tr><td>2024</td><td>[4DGen] <a href="http://arxiv.org/abs/2312.17225v1">4DGen: Grounded 4D Content Generation with Spatial-temporal Consistency</a></td><td>Arxiv 2024</td><td></td><td><a href="https://vita-group.github.io/4DGen/">Project</a></td><td>🔥Gaussian Splatting</td></tr><tr><td>2024</td><td>[3DGAN] <a href="https://research.nvidia.com/labs/nxp/wysiwyg/media/WYSIWYG.pdf">What You See Is What You GAN: Rendering Every Pixel for High-Fidelity Geometry in 3D GANs</a></td><td>Arxiv 2024</td><td></td><td><a href="https://research.nvidia.com/labs/nxp/wysiwyg/">Project</a></td><td></td></tr><tr><td>2024</td><td>[3DGS-Avatar] <a href="http://arxiv.org/abs/2312.09228v2">3DGS-Avatar: Animatable Avatars via Deformable 3D Gaussian Splatting</a></td><td>Arxiv 2024</td><td><a href="https://github.com/mikeqzy/3dgs-avatar-release">Code</a></td><td><a href="https://neuralbodies.github.io/3DGS-Avatar">Project</a></td><td>🔥Gaussian Splatting</td></tr><tr><td>2024</td><td><a href="http://arxiv.org/abs/2312.10422v2">Learning Dense Correspondence for NeRF-Based Face Reenactment</a></td><td>AAAI 2024</td><td></td><td></td><td>one-shot multi-view face reenactmen</td></tr><tr><td>2024</td><td>[R2-Talker] <a href="http://arxiv.org/abs/2312.05572v1">R2-Talker: Realistic Real-Time Talking Head Synthesis with Hash Grid Landmarks Encoding and Progressive Multilayer Conditioning</a></td><td>Arxiv 2024</td><td></td><td></td><td>based-RAD-NeRF</td></tr><tr><td>2024</td><td>[GaussianHead] <a href="https://arxiv.org/abs/2312.01632">GaussianHead: Impressive 3D Gaussian-based Head Avatars with Dynamic Hybrid Neural Field</a></td><td>Arxiv 2024</td><td><a href="https://github.com/chiehwangs/gaussian-head">Code</a></td><td></td><td>🔥Gaussian Splatting</td></tr><tr><td>2024</td><td>[MonoGaussianAvatar] <a href="https://arxiv.org/abs/2312.00846">MonoGaussianAvatar: Monocular Gaussian Point-based Head Avatar</a></td><td>Arxiv 2024</td><td></td><td></td><td>🔥Gaussian Splatting</td></tr><tr><td>2024</td><td>[Gaussian Head Avatar] <a href="http://arxiv.org/abs/2312.03029v1">Gaussian Head Avatar: Ultra High-fidelity Head Avatar via Dynamic Gaussians</a></td><td>Arxiv 2024</td><td><a href="https://github.com/YuelangX/Gaussian-Head-Avatar">Code</a></td><td><a href="https://yuelangx.github.io/gaussianheadavatar/">Project</a></td><td></td></tr><tr><td>2024</td><td>[HeadGaS] <a href="http://arxiv.org/abs/2312.02902v1">HeadGaS: Real-Time Animatable Head Avatars via 3D Gaussian Splatting</a></td><td>Arxiv 2024</td><td></td><td></td><td>🔥Gaussian Splatting</td></tr><tr><td>2024</td><td>[GaussianAvatars] <a href="http://arxiv.org/abs/2312.02069v1">GaussianAvatars: Photorealistic Head Avatars with Rigged 3D Gaussians</a></td><td>Arxiv 2024</td><td></td><td><a href="https://shenhanqian.github.io/gaussian-avatars">Project</a></td><td>🔥Gaussian Splatting</td></tr><tr><td>2024</td><td>[SyncTalk] <a href="https://arxiv.org/abs/2311.17590">SyncTalk: The Devil😈 is in the Synchronization for Talking Head Synthesis</a></td><td>CVPR 2024?</td><td><a href="https://github.com/ZiqiaoPeng/SyncTalk">Code</a></td><td><a href="https://ziqiaopeng.github.io/synctalk/">Project</a></td><td>😈</td></tr><tr><td>2024</td><td>[R2-Talk] <a href="https://arxiv.org/abs/2312.05572">R2-Talker: Realistic Real-Time Talking Head Synthesis with Hash Grid Landmarks Encoding and Progressive Multilayer Conditioning</a></td><td>Arxiv 2024</td><td></td><td></td><td></td></tr><tr><td>2024</td><td>[DT-NeRF] <a href="https://arxiv.org/pdf/2309.07752">DT-NeRF: Decomposed Triplane-Hash Neural Radiance Fields for High-Fidelity Talking Portrait Synthesis</a></td><td>ICASSP 2024</td><td>-</td><td>-</td><td>ER-NeRF</td></tr><tr><td>2023</td><td>[ER-NeRF] <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Efficient_Region-Aware_Neural_Radiance_Fields_for_High-Fidelity_Talking_Portrait_Synthesis_ICCV_2023_paper.pdf">Efficient Region-Aware Neural Radiance Fields for High-Fidelity Talking Portrait Synthesis</a></td><td>ICCV 2023</td><td><a href="https://github.com/Fictionarry/ER-NeRF">Code</a></td><td><a href="https://fictionarry.github.io/ER-NeRF/">Project</a></td><td>Tri-plane</td></tr><tr><td>2023</td><td>[LipNeRF] <a href="https://www.amazon.science/publications/lipnerf-what-is-the-right-feature-space-to-lip-sync-a-nerf">LipNeRF: What is the right feature space to lip-sync a NeRF?</a></td><td>FG 2023</td><td>Code</td><td><a href="https://aggelinacha.github.io/LipNeRF/">Project</a></td><td>Wav2lip</td></tr><tr><td>2023</td><td>[SD-NeRF] <a href="https://ieeexplore.ieee.org/document/10229247">SD-NeRF: Towards Lifelike Talking Head Animation via Spatially-adaptive Dual-driven NeRFs</a></td><td>IEEE 2023</td><td>-</td><td>-</td><td></td></tr><tr><td>2023</td><td>[Instruct-NeuralTalker] <a href="https://arxiv.org/abs/2306.10813">Instruct-NeuralTalker: Editing Audio-Driven Talking Radiance Fields with Instructions</a></td><td>Arxiv 2023</td><td></td><td></td><td></td></tr><tr><td>2023</td><td>[GeneFace++] <a href="https://arxiv.org/abs/2305.00787">Generalized and Stable Real-Time Audio-Driven 3D Talking Face Generation</a></td><td>Arxiv 2023</td><td>-</td><td><a href="https://genefaceplusplus.github.io/">Project</a></td><td>-</td></tr><tr><td>2023</td><td>[GeneFace] <a href="https://arxiv.org/abs/2301.13430">GeneFace: Generalized and High-Fidelity Audio-Driven 3D Talking Face Synthesis</a></td><td>ICLR 2023</td><td><a href="https://github.com/yerfor/GeneFace">Code</a></td><td><a href="https://geneface.github.io/">Project</a></td><td>-</td></tr><tr><td>2022</td><td>[RAD-NeRF] <a href="https://arxiv.org/pdf/2211.12368.pdf">RAD-NeRF: Real-time Neural Talking Portrait Synthesis</a></td><td>Arxiv 2022</td><td><a href="https://github.com/ashawkey/RAD-NeRF">Code</a></td><td><a href="https://ashawkey.github.io/radnerf/">Project</a></td><td>InstantNGP</td></tr><tr><td>2022</td><td>[DFRF] <a href="https://arxiv.org/abs/2207.11770">DFRF：Learning Dynamic Facial Radiance Fields for Few-Shot Talking Head Synthesis</a></td><td>ECCV 2022</td><td><a href="https://github.com/sstzal/DFRF">Code</a></td><td><a href="https://sstzal.github.io/DFRF/">Project</a></td><td></td></tr><tr><td>2022</td><td>[DialogueNeRF] <a href="https://arxiv.org/pdf/2203.07931.pdf">DialogueNeRF: Towards Realistic Avatar Face-to-face Conversation Video Generation</a></td><td>Arxiv 2022</td><td>-</td><td>-</td><td>-</td></tr><tr><td>2022</td><td>[NeRFInvertor] <a href="https://arxiv.org/pdf/2211.17235.pdf">NeRFInvertor: High Fidelity NeRF-GAN Inversion for Single-shot Real Image Animation</a></td><td>Arxiv 2022</td><td><a href="https://github.com/YuYin1/NeRFInvertor">Code</a></td><td><a href="https://yuyin1.github.io/NeRFInvertor_Homepage/">Project</a></td><td>-</td></tr><tr><td>2022</td><td>[Next3D] <a href="https://arxiv.org/pdf/2211.11208.pdf">Next3D: Generative Neural Texture Rasterization for 3D-Aware Head Avatars</a></td><td>Arxiv 2022</td><td><a href="https://mrtornado24.github.io/Next3D/">Code</a></td><td><a href="https://mrtornado24.github.io/Next3D/">Project</a></td><td>-</td></tr><tr><td>2022</td><td>[3DFaceShop] <a href="https://arxiv.org/pdf/2209.05434">3DFaceShop: Explicitly Controllable 3D-Aware Portrait Generation</a></td><td>Arxiv 2022</td><td><a href="https://github.com/junshutang/3DFaceShop">Code</a></td><td><a href="https://junshutang.github.io/control/index.html">Project</a></td><td>-</td></tr><tr><td>2022</td><td>[FNeVR] <a href="https://arxiv.org/abs/2209.10340">FNeVR: Neural Volume Rendering for Face Animation</a></td><td>Arxiv 2022</td><td><a href="https://github.com/zengbohan0217/FNeVR">Code</a></td><td>-</td><td>-</td></tr><tr><td>2022</td><td>[ROME] <a href="https://arxiv.org/pdf/2206.08343.pdf">ROME: Realistic One-shot Mesh-based Head Avatars</a></td><td>ECCV 2022</td><td><a href="https://github.com/SamsungLabs/rome">Code</a></td><td><a href="https://samsunglabs.github.io/rome/">Project</a></td><td>-</td></tr><tr><td>2022</td><td>[IMavatar] <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zheng_I_M_Avatar_Implicit_Morphable_Head_Avatars_From_Videos_CVPR_2022_paper.pdf">IMavatar: Implicit Morphable Head Avatars from Videos</a></td><td>CVPR 2022</td><td><a href="https://ait.ethz.ch/projects/2022/IMavatar/">Code</a></td><td><a href="https://ait.ethz.ch/projects/2022/IMavatar/">Project</a></td><td>-</td></tr><tr><td>2022</td><td>[HeadNeRF] <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Grassal_Neural_Head_Avatars_From_Monocular_RGB_Videos_CVPR_2022_paper.pdf">HeadNeRF: A Real-time NeRF-based Parametric Head Model</a></td><td>CVPR 2022</td><td><a href="https://github.com/CrisHY1995/headnerf">Code</a></td><td><a href="https://hy1995.top/HeadNeRF-Project/">Project</a></td><td>-</td></tr><tr><td>2022</td><td>[SSP-NeRF] <a href="https://arxiv.org/pdf/2201.07786.pdf">Semantic-Aware Implicit Neural Audio-Driven Video Portrait Generation</a></td><td>Arxiv 2022</td><td><a href="https://github.com/alvinliu0/SSP-NeRF">Code</a></td><td><a href="https://alvinliu0.github.io/projects/SSP-NeRF">Project</a></td><td>-</td></tr><tr><td>2021</td><td>[AD-NeRF] <a href="https://arxiv.org/abs/2103.11078">AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis</a></td><td>ICCV 2021</td><td><a href="https://github.com/YudongGuo/AD-NeRF">Code</a></td><td><a href="https://yudongguo.github.io/ADNeRF/">Project</a></td><td>-</td></tr><tr><td>2021</td><td>[NerFACE] <a href="https://arxiv.org/pdf/2012.03065">NerFACE: Dynamic Neural Radiance Fields for Monocular 4D Facial Avatar Reconstruction</a></td><td>CVPR 2021 Oral</td><td><a href="https://github.com/gafniguy/4D-Facial-Avatars">Code</a></td><td><a href="https://gafniguy.github.io/4D-Facial-Avatars/">Project</a></td><td>-</td></tr><tr><td>2021</td><td>[DFA-NeRF] <a href="https://arxiv.org/pdf/2201.00791v1.pdf">DFA-NeRF: Personalized Talking Head Generation via Disentangled Face Attributes Neural Rendering</a></td><td>Arxiv 2021</td><td><a href="https://github.com/ShunyuYao/DFA-NeRF">Code</a></td><td>-</td><td>-</td></tr></tbody></table></div><hr><h2 id="Metrics"><a href="#Metrics" class="headerlink" title="Metrics"></a>Metrics</h2><div class="table-container"><table><thead><tr><th>Metrics</th><th>Paper</th><th>Link</th></tr></thead><tbody><tr><td>PSNR (peak signal-to-noise ratio)</td><td>-</td><td></td></tr><tr><td>SSIM (structural similarity index measure)</td><td>Image quality assessment: from error visibility to structural similarity.</td><td></td></tr><tr><td>CPBD(cumulative probability of blur detection)</td><td>A no-reference image blur metric based on the cumulative probability of blur detection</td><td></td></tr><tr><td>LPIPS (Learned Perceptual Image Patch Similarity) -</td><td>The Unreasonable Effectiveness of Deep Features as a Perceptual Metric</td><td><a href="https://arxiv.org/pdf/1801.03924.pdf">paper</a></td></tr><tr><td>NIQE (Natural Image Quality Evaluator)</td><td>Making a ‘Completely Blind’ Image Quality Analyzer</td><td><a href="http://live.ece.utexas.edu/research/Quality/niqe_spl.pdf">paper</a></td></tr><tr><td>FID (Fréchet inception distance)</td><td>GANs trained by a two time-scale update rule converge to a local nash equilibrium</td><td></td></tr><tr><td>LMD (landmark distance error)</td><td>Lip Movements Generation at a Glance</td><td></td></tr><tr><td>LRA (lip-reading accuracy)</td><td>Talking Face Generation by Conditional Recurrent Adversarial Network</td><td><a href="https://arxiv.org/pdf/1804.04786.pdf">paper</a></td></tr><tr><td>WER(word error rate)</td><td>Lipnet: end-to-end sentencelevel lipreading.</td><td></td></tr><tr><td>LSE-D (Lip Sync Error - Distance)</td><td>Out of time: automated lip sync in the wild</td><td></td></tr><tr><td>LSE-C (Lip Sync Error - Confidence)</td><td>Out of time: automated lip sync in the wild</td><td></td></tr><tr><td>ACD(Average content distance)</td><td>Facenet: a unified embedding for face recognition and clustering.</td><td></td></tr><tr><td>CSIM(cosine similarity)</td><td>Arcface: additive angular margin loss for deep face recognition.</td><td></td></tr><tr><td>EAR(eye aspect ratio)</td><td>Real-time eye blink detection using facial landmarks. In: Computer Vision Winter Workshop</td><td></td></tr><tr><td>ESD(emotion similarity distance)</td><td>What comprises a good talking-head video generation?: A Survey and Benchmark</td></tr></tbody></table></div><hr><h2 id="Tools-amp-Software"><a href="#Tools-amp-Software" class="headerlink" title="Tools &amp; Software"></a>Tools &amp; Software</h2><div class="table-container"><table><thead><tr><th>Tool/Resource</th><th>Description</th></tr></thead><tbody><tr><td><a href="https://sourceforge.net/projects/lucia/">LUCIA</a></td><td>Development of a MPEG-4 Talking Head Engine. 💻</td></tr><tr><td><a href="https://www.g2.com/products/yepic-studio/reviews">Yepic Studio</a></td><td>Create and dub talking head-style videos in minutes without expensive equipment. 🎥</td></tr><tr><td><a href="https://sourceforge.net/projects/talkbots/">Mel McGee’s Talkbots</a></td><td>A complete multi-browser, multi-platform talking head application in SVG suitable for web sites or as an avatar. 🗣️</td></tr><tr><td><a href="https://sourceforge.net/projects/face3dchung/">face3D_chung</a></td><td>Create 3D character avatar head objects with texture from a single photo for your games. 🎮</td></tr><tr><td><a href="https://www.g2.com/products/crazytalk/reviews">CrazyTalk</a></td><td>Exciting features for 3D head creation and automation. 🤪</td></tr><tr><td><a href="https://sourceforge.net/directory/?q=tts%20avatar">tts avatar free download - SourceForge</a></td><td>Mel McGee’s Talkbots is a complete multi-browser, multi-platform talking head. (🔧👄)</td></tr><tr><td><a href="https://www.producthunt.com/products/verbatim-ai">Verbatim AI - Product Information, Latest Updates, and Reviews 2023</a></td><td>A simple yet powerful API to generate AI “talking head” videos in near real-time with Verbatim AI. Add interest, intrigue, and dynamism to your chat bots! (🔧👄)</td></tr><tr><td><a href="https://sourceforge.net/directory/3d-modeling/basic/">Best Open Source BASIC 3D Modeling Software</a></td><td>Includes talk3D_chung, a small example using obj models created with face3D_chung, and speak3D_chung_dll, a dll to load and display face3D_chung talking avatars. (🛠️🎭)</td></tr><tr><td><a href="https://sourceforge.net/p/dvdstyler/discussion/318795/thread/82dcb647/">DVDStyler / Discussion / Help: ffmpeg-vbr or internal</a></td><td>Talking heads would get a bitrate which is unnecessarily high while using DVDStyler. (🛠️👄)</td></tr><tr><td><a href="https://sourceforge.net/directory/lisp/?q=puffin+web+browser">puffin web browser free download - SourceForge</a></td><td>Mel McGee’s Talkbots is a complete multi-browser, multi-platform talking head. (🔧👄)</td></tr><tr><td>[12 best AI video generators to use in 2023 Free and paid \</td><td>Product …](<a href="https://www.producthunt.com/stories/best-ai-video-generators-free">https://www.producthunt.com/stories/best-ai-video-generators-free</a>)</td><td>Whether you’re an entrepreneur, small business owner, or run a large company, AI video generators make it super easy to create high-quality videos from scratch. (🔧🎥)</td></tr></tbody></table></div><hr><h2 id="Slides-amp-Presentations"><a href="#Slides-amp-Presentations" class="headerlink" title="Slides &amp; Presentations"></a>Slides &amp; Presentations</h2><div class="table-container"><table><thead><tr><th>Presentation Title</th><th>Description</th></tr></thead><tbody><tr><td><a href="https://www.slideshare.net/ssuserc9d82a/paper-reviewfewshot-adversarial-learning-of-realistic-neural-talking-head-models">Few-Shot Adversarial Learning of Realistic Neural Talking Head Models</a></td><td>Presentation reviewing the few-shot adversarial learning of realistic neural talking head models.</td></tr><tr><td><a href="https://www.slideshare.net/ZULHICZARARIETINARBU/nethania-michelles-character">Nethania Michelle’s Character</a></td><td>PPT: Presentation discussing the improvement of a 3D talking head for use in an avatar of a virtual meeting room.</td></tr><tr><td><a href="https://support.prezi.com/hc/en-us/articles/360036679953-Presenting-you-Top-tips-on-presenting-with-Prezi-Video">Presenting you: Top tips on presenting with Prezi Video – Prezi</a></td><td>Article providing top tips for presenting with Prezi Video.</td></tr><tr><td><a href="https://pt.slideshare.net/willg_36/research-presentation-presentation-956726">Research Presentation</a></td><td>PPT: Resident Research Presentation Slide Deck.</td></tr><tr><td><a href="https://support.prezi.com/hc/en-us/articles/360038281894-Adding-narration-to-your-presentation-using-Prezi-Video-">Adding narration to your presentation (using Prezi Video) – Prezi</a></td><td>Learn how to add narration to your Prezi presentation with Prezi Video.</td></tr></tbody></table></div><hr><h2 id="Star-History"><a href="#Star-History" class="headerlink" title="Star History"></a>Star History</h2><p><a href="https://star-history.com/#Kedreamix/Awesome-Talking-Head-Synthesis&amp;Date"><img src="https://api.star-history.com/svg?repos=Kedreamix/Awesome-Talking-Head-Synthesis&amp;type=Date" alt="Star History Chart"></a></p>]]></content>
    
    
    <summary type="html">超赞的数字人生成知识库  Awesome-Talking-Head-Synthesis， 这份资源库整理了与生成对抗网络(GAN)和神经辐射场(NeRF)相关的论文、代码和资源,重点关注基于图像和音频的虚拟讲话头合成论文及已发布代码。如果您觉得这个仓库有用,请star⭐支持!</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>conda和pip换源</title>
    <link href="https://kedreamix.github.io/Linux/Anaconda%E6%8D%A2%E6%BA%90/"/>
    <id>https://kedreamix.github.io/Linux/Anaconda%E6%8D%A2%E6%BA%90/</id>
    <published>2023-12-31T16:00:00.000Z</published>
    <updated>2024-01-28T09:39:15.310Z</updated>
    
    <content type="html"><![CDATA[<h2 id="pip换源"><a href="#pip换源" class="headerlink" title="pip换源"></a>pip换源</h2><h3 id="设为默认清华源"><a href="#设为默认清华源" class="headerlink" title="设为默认清华源"></a>设为默认清华源</h3><p>升级 pip 到最新的版本 (&gt;=10.0.0) 后进行配置：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python -m pip install --upgrade pip</span><br><span class="line">pip config <span class="built_in">set</span> global.index-url https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></tbody></table></figure><p>如果您到 pip 默认源的网络连接较差，临时使用本镜像站来升级 pip：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m pip install -i https://pypi.tuna.tsinghua.edu.cn/simple --upgrade pip</span><br></pre></td></tr></tbody></table></figure><p><a href="https://blog.csdn.net/wyf2017/article/details/118676765">https://blog.csdn.net/wyf2017/article/details/118676765</a></p><h3 id="修改配置"><a href="#修改配置" class="headerlink" title="修改配置"></a>修改配置</h3><p>vim ~/.pip/pip.conf<br>注意，这里设置的豆瓣源</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[global]</span><br><span class="line"></span><br><span class="line">index-url = https://pypi.doubanio.com/simple</span><br><span class="line"></span><br><span class="line">trusted-host = pypi.doubanio.com</span><br></pre></td></tr></tbody></table></figure><h2 id="conda换源"><a href="#conda换源" class="headerlink" title="conda换源"></a>conda换源</h2><h3 id="清华源"><a href="#清华源" class="headerlink" title="清华源"></a>清华源</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/conda</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/conda</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/conda</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge</span><br><span class="line">conda config --<span class="built_in">set</span> show_channel_urls <span class="built_in">yes</span></span><br></pre></td></tr></tbody></table></figure><p><strong>提前说</strong>：如果配置镜像源后报 HTTP 错误，只需要将源链接中的 https://… 中的 s 删掉就行 清华源</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forgeconda config --<span class="built_in">set</span> show_channel_urls yes</span><br></pre></td></tr></tbody></table></figure><h3 id="中科大源"><a href="#中科大源" class="headerlink" title="中科大源"></a>中科大源</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">conda config --add channels https://mirrors.ustc.edu.cn/anaconda/pkgs/main/conda</span><br><span class="line">conda config --add channels https://mirrors.ustc.edu.cn/anaconda/pkgs/free/conda </span><br><span class="line">conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/conda-forge/conda </span><br><span class="line">conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/msys2/conda </span><br><span class="line">conda config --<span class="built_in">set</span> show_channel_urls yes</span><br></pre></td></tr></tbody></table></figure><h3 id="修改配置-1"><a href="#修改配置-1" class="headerlink" title="修改配置"></a>修改配置</h3><p>我们还刻有直接修改配置<br>vim ~/.condarc</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">channels:</span><br><span class="line">  - defaults</span><br><span class="line">show_channel_urls: <span class="literal">true</span></span><br><span class="line">default_channels:</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2</span><br><span class="line">custom_channels:</span><br><span class="line">  conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br></pre></td></tr></tbody></table></figure><h3 id="换为默认源"><a href="#换为默认源" class="headerlink" title="换为默认源"></a>换为默认源</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda config --remove-key channels</span><br></pre></td></tr></tbody></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;pip换源&quot;&gt;&lt;a href=&quot;#pip换源&quot; class=&quot;headerlink&quot; title=&quot;pip换源&quot;&gt;&lt;/a&gt;pip换源&lt;/h2&gt;&lt;h3 id=&quot;设为默认清华源&quot;&gt;&lt;a href=&quot;#设为默认清华源&quot; class=&quot;headerlink&quot; title=</summary>
      
    
    
    
    <category term="Linux" scheme="https://kedreamix.github.io/categories/Linux/"/>
    
    
    <category term="Linux" scheme="https://kedreamix.github.io/tags/Linux/"/>
    
  </entry>
  
</feed>
