<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Adventures in Kedreamix&#39; Digital World</title>
  
  
  <link href="https://kedreamix.github.io/atom.xml" rel="self"/>
  
  <link href="https://kedreamix.github.io/"/>
  <updated>2024-05-13T08:45:28.621Z</updated>
  <id>https://kedreamix.github.io/</id>
  
  <author>
    <name>Kedreamix</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/05/13/Paper/2024-05-13/NeRF/"/>
    <id>https://kedreamix.github.io/2024/05/13/Paper/2024-05-13/NeRF/</id>
    <published>2024-05-13T08:45:28.000Z</published>
    <updated>2024-05-13T08:45:28.621Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-05-13-更新"><a href="#2024-05-13-更新" class="headerlink" title="2024-05-13 更新"></a>2024-05-13 更新</h1><h2 id="OneTo3D-One-Image-to-Re-editable-Dynamic-3D-Model-and-Video-Generation"><a href="#OneTo3D-One-Image-to-Re-editable-Dynamic-3D-Model-and-Video-Generation" class="headerlink" title="OneTo3D: One Image to Re-editable Dynamic 3D Model and Video Generation"></a>OneTo3D: One Image to Re-editable Dynamic 3D Model and Video Generation</h2><p><strong>Authors:Jinwei Lin</strong></p><p>One image to editable dynamic 3D model and video generation is novel direction and change in the research area of single image to 3D representation or 3D reconstruction of image. Gaussian Splatting has demonstrated its advantages in implicit 3D reconstruction, compared with the original Neural Radiance Fields. As the rapid development of technologies and principles, people tried to used the Stable Diffusion models to generate targeted models with text instructions. However, using the normal implicit machine learning methods is hard to gain the precise motions and actions control, further more, it is difficult to generate a long content and semantic continuous 3D video. To address this issue, we propose the OneTo3D, a method and theory to used one single image to generate the editable 3D model and generate the targeted semantic continuous time-unlimited 3D video. We used a normal basic Gaussian Splatting model to generate the 3D model from a single image, which requires less volume of video memory and computer calculation ability. Subsequently, we designed an automatic generation and self-adaptive binding mechanism for the object armature. Combined with the re-editable motions and actions analyzing and controlling algorithm we proposed, we can achieve a better performance than the SOTA projects in the area of building the 3D model precise motions and actions control, and generating a stable semantic continuous time-unlimited 3D video with the input text instructions. Here we will analyze the detailed implementation methods and theories analyses. Relative comparisons and conclusions will be presented. The project code is open source. </p><p><a href="http://arxiv.org/abs/2405.06547v1">PDF</a> 24 pages, 13 figures, 2 tables</p><p><strong>Summary</strong><br>一键图像生成可编辑动态3D模型和视频，是图像到3D表示或图像3D重建研究领域的新方向和变革。</p><p><strong>Key Takeaways</strong></p><ul><li>相比于原始神经辐射场，高斯溅射在隐式3D重建中表现出优势。</li><li>稳定扩散模型可用于根据文本指令生成目标模型。</li><li>传统的隐式机器学习方法难以获得精确的运动和动作控制。</li><li>难以生成长内容和语义连续的3D视频。</li><li>OneTo3D方法可使用单张图像生成可编辑3D模型和生成目标语义连续且时间无限的3D视频。</li><li>OneTo3D使用基本的高斯溅射模型从单张图像生成3D模型，减少了视频内存和计算机计算需求。</li><li>OneTo3D设计了自动生成和自适应绑定机制，用于对象骨架。</li><li>结合OneTo3D提出的可重新编辑的运动和动作分析与控制算法，在3D模型精确定位运动和动作控制以及根据输入文本指令生成稳定的语义连续且时间无限的3D视频方面，OneTo3D的性能优于该领域的SOTA项目。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: 一张图像到可重新编辑的动态 3D 模型和视频生成</p></li><li><p>Authors: JINWEI LIN</p></li><li><p>Affiliation: 澳大利亚莫纳什大学</p></li><li><p>Keywords: 3D, One image, Editable, Dynamic, Generation, Automation, Video, Self-adaption, Armature</p></li><li><p>Urls: Paper: xxx, Github: None</p></li><li><p>Summary:</p><p>(1): 3D 表征或 3D 重建长期以来一直是计算机视觉领域的研究难题。</p><p>(2): 现有的 3D 重建方法可分为显式方法和隐式方法。显式方法直接设计和完成 3D 重建或建模；隐式方法使用机器学习方法和理论实现这些目标。近年来，Neural Radiance Fields (NeRF) 在隐式 3D 表征或重建方面取得了突出成就。</p><p>(3): 本文提出 OneTo3D 方法，使用单张图像生成可编辑的 3D 模型和语义连续的 3D 视频。该方法使用基本的 Gaussian Splatting 模型从单张图像生成 3D 模型，并设计了一种自动生成和自适应绑定机制来绑定对象骨架。结合重新编辑的运动和动作分析与控制算法，OneTo3D 在 3D 模型精确运动和动作控制以及生成稳定的语义连续无时间限制的 3D 视频方面取得了优于现有方法的性能。</p><p>(4): 本文方法在任务和性能上取得了以下成就：使用单张图像生成可编辑的 3D 模型；生成语义连续的 3D 视频；精确控制 3D 模型的运动和动作。这些性能支持了本文的目标，即实现从单张图像到可重新编辑的动态 3D 模型和视频的生成。</p></li><li><p>方法：</p><pre><code>            (1):生成初始3D模型，使用DreamGaussian模型和Zero-1-to-3方法；            (2):生成并绑定自适应骨架，设计基本骨架，分析初始3D模型的几何参数信息，微调骨架参数以使其适合对象的身体；            (3):文本到动作和动作，分析用户文本指令的命令意图，将命令转换为特定动作和骨架相对骨骼的修改数据，控制特定骨骼在Blender中实现相对运动；            (4):背景去除，使用Dreamgaussian的process.py脚本或其他方法，可选使用图像检测或语义分割机器学习方法；            (5):颜色分组去除背景，计算图像中每个主要颜色项的比例，将颜色值范围内的颜色项划分为不同的颜色组，去除配置比例范围内的颜色组。</code></pre></li><li><p>结论：</p></li></ol><p>（1）本篇工作提出了一种从单张图像生成可编辑的动态 3D 模型和视频的方法，具有生成可编辑 3D 模型、生成语义连续的 3D 视频、精确控制 3D 模型的运动和动作等优点，在任务和性能上取得了创新。</p><p>（2）创新点：OneTo3D 方法首次实现了从单张图像到可重新编辑的动态 3D 模型和视频的生成；性能：OneTo3D 方法在生成 3D 模型的精度、视频的语义连续性、动作控制的精确性等方面优于现有方法；工作量：OneTo3D 方法的实现需要较大的计算资源和时间，需要进一步优化算法和设计。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-8729865363a1dfddc21dff54a70072f2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-02dad34b1d632546ae26f127a58c9c0f.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f635130d270abd57752edb234d2c8a48.jpg" align="middle"></details>## Aerial-NeRF: Adaptive Spatial Partitioning and Sampling for Large-Scale   Aerial Rendering**Authors:Xiaohan Zhang, Yukui Qiu, Zhenyu Sun, Qi Liu**Recent progress in large-scale scene rendering has yielded Neural Radiance Fields (NeRF)-based models with an impressive ability to synthesize scenes across small objects and indoor scenes. Nevertheless, extending this idea to large-scale aerial rendering poses two critical problems. Firstly, a single NeRF cannot render the entire scene with high-precision for complex large-scale aerial datasets since the sampling range along each view ray is insufficient to cover buildings adequately. Secondly, traditional NeRFs are infeasible to train on one GPU to enable interactive fly-throughs for modeling massive images. Instead, existing methods typically separate the whole scene into multiple regions and train a NeRF on each region, which are unaccustomed to different flight trajectories and difficult to achieve fast rendering. To that end, we propose Aerial-NeRF with three innovative modifications for jointly adapting NeRF in large-scale aerial rendering: (1) Designing an adaptive spatial partitioning and selection method based on drones' poses to adapt different flight trajectories; (2) Using similarity of poses instead of (expert) network for rendering speedup to determine which region a new viewpoint belongs to; (3) Developing an adaptive sampling approach for rendering performance improvement to cover the entire buildings at different heights. Extensive experiments have conducted to verify the effectiveness and efficiency of Aerial-NeRF, and new state-of-the-art results have been achieved on two public large-scale aerial datasets and presented SCUTic dataset. Note that our model allows us to perform rendering over 4 times as fast as compared to multiple competitors. Our dataset, code, and model are publicly available at https://drliuqi.github.io/. [PDF](http://arxiv.org/abs/2405.06214v1) **Summary**针对大规模航拍场景，我们提出 Aerial-NeRF，它针对 NeRF 进行三项创新性修改，以联合实现 NeRF 在大规模航拍渲染中的自适应：自适应空间分区和选择方法、基于姿态相似性的快速渲染和自适应采样方法。**Key Takeaways**- 提出 Aerial-NeRF，针对大规模航拍场景对 NeRF 进行三项创新性修改。- 使用自适应空间分区和选择方法，根据无人机姿态自适应不同的飞行轨迹。- 使用姿态相似性代替（专家）网络进行渲染加速，以确定新视点属于哪个区域。- 开发自适应采样方法，以提高渲染性能，覆盖不同高度的整座建筑。- 大量实验验证了 Aerial-NeRF 的有效性和效率，并在两个公开的大规模航拍数据集和 SCUTic 数据集上取得了新的最先进结果。- 与多个竞争对手相比，我们的模型允许我们以超过 4 倍的速度进行渲染。- 我们模型、代码和数据集已公开获取。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: 航拍NeRF：大规模航拍渲染的自适应空间划分和采样</p></li><li><p>Authors: Xiaohan Zhang, Yukui Qiu, Zhenyu Sun, Qi Liu</p></li><li><p>Affiliation: 华南理工大学未来技术学院</p></li><li><p>Keywords: View synthesis, large-scale scene rendering, neural radiance fields, fast rendering</p></li><li><p>Urls: https://arxiv.org/abs/2405.06214 , https://github.com/drliuqi/Aerial-NeRF</p></li><li><p>Summary:</p><p>(1):NeRF模型在小物体和室内场景渲染中取得了成功，但将其扩展到航拍渲染中面临两个挑战：单个NeRF无法渲染大规模航拍数据集中的整个场景，传统NeRF无法在单个GPU上训练以实现交互式浏览。</p><p>(2):以往方法将场景划分为多个区域，并在每个区域训练一个NeRF，但这些方法无法适应不同的飞行轨迹，渲染速度也较慢。</p><p>(3):本文提出Aerial-NeRF，通过自适应空间划分和选择、基于姿态相似性确定区域归属、自适应采样等方法，解决了上述问题。</p><p>(4):Aerial-NeRF在两个公开大规模航拍数据集和一个自建数据集上取得了最优性能，渲染速度比其他方法快4倍以上。</p></li><li><p>方法：</p><p>（1）：自适应空间划分：根据航拍数据集的特征，提出了一种自适应空间划分方法，将大规模场景划分为多个小区域，每个区域使用一个NeRF进行渲染；</p><p>（2）：基于姿态相似性确定区域归属：设计了一种基于姿态相似性的区域归属确定算法，根据相机的姿态信息将航拍图像分配到不同的区域；</p><p>（3）：自适应采样：提出了一种自适应采样算法，根据不同区域的复杂程度和渲染速度要求，动态调整采样点数，提高渲染效率；</p><p>（4）：基于神经网络的区域融合：使用神经网络将不同区域的渲染结果融合成最终图像，保证渲染结果的连续性和准确性；</p><p>（5）：基于概率密度函数的加速采样：利用概率密度函数对采样点进行优化，进一步提高渲染速度。</p></li><li><p>结论：</p></li></ol><p>（1）：本工作提出了 Aerial-NeRF，一种用于处理大规模航拍数据集的高效且鲁棒的渲染方法，在渲染速度上大幅优于现有的同类方法，几乎达到 4 倍。此外，在适当的划分区域数量下，Aerial-NeRF 可以使用单个 GPU 渲染任意大的场景。同时，我们为航拍场景引入了一种新颖的采样策略，该策略能够通过不同高度相机的采样范围覆盖建筑物。与 SOTA 模型进行更广泛的比较时，我们的方法明显更有效（仅使用 1/4 的采样点和 2 GB 的 GPU 内存节省），并且在多个常用指标方面具有可比性。最后，我们提出了 SCUTic，这是一个用于大规模大学校园场景的新型航拍数据集，具有不均匀的相机轨迹，可以验证渲染方法的鲁棒性。</p><p>（2）：创新点：自适应空间划分和采样；性能：渲染速度快，内存占用低；工作量：数据集构建和模型训练复杂度较高。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-3f9706ee7489efbc0fffd098a133920f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b5cd3300322f846160033228b8f55d45.jpg" align="middle"><img src="https://pica.zhimg.com/v2-7172b9e2d3611b5ec9915962744d54fa.jpg" align="middle"><img src="https://picx.zhimg.com/v2-07281c002ad9f4eaef4b0c58ebbaf426.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a2778deaeb7d026e2fd79cf4c5e6e409.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d55c494ec8f956acc30da13f5d75881b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ac7e04c419fb74632e6c7f9332f81960.jpg" align="middle"></details>## Residual-NeRF: Learning Residual NeRFs for Transparent Object   Manipulation**Authors:Bardienus P. Duisterhof, Yuemin Mao, Si Heng Teng, Jeffrey Ichnowski**Transparent objects are ubiquitous in industry, pharmaceuticals, and households. Grasping and manipulating these objects is a significant challenge for robots. Existing methods have difficulty reconstructing complete depth maps for challenging transparent objects, leaving holes in the depth reconstruction. Recent work has shown neural radiance fields (NeRFs) work well for depth perception in scenes with transparent objects, and these depth maps can be used to grasp transparent objects with high accuracy. NeRF-based depth reconstruction can still struggle with especially challenging transparent objects and lighting conditions. In this work, we propose Residual-NeRF, a method to improve depth perception and training speed for transparent objects. Robots often operate in the same area, such as a kitchen. By first learning a background NeRF of the scene without transparent objects to be manipulated, we reduce the ambiguity faced by learning the changes with the new object. We propose training two additional networks: a residual NeRF learns to infer residual RGB values and densities, and a Mixnet learns how to combine background and residual NeRFs. We contribute synthetic and real experiments that suggest Residual-NeRF improves depth perception of transparent objects. The results on synthetic data suggest Residual-NeRF outperforms the baselines with a 46.1% lower RMSE and a 29.5% lower MAE. Real-world qualitative experiments suggest Residual-NeRF leads to more robust depth maps with less noise and fewer holes. Website: https://residual-nerf.github.io [PDF](http://arxiv.org/abs/2405.06181v1) **Summary**透明物体在工业、医药和家庭中无处不在，机器人在抓取和操作这些物体时面临重大挑战。**Key Takeaways**- NeRFs 对包含透明物体的场景中的深度感知效果很好。- NeRFs 在处理极具挑战性的透明物体和光照条件时仍然存在困难。- Residual-NeRF 提出了一种改善透明物体深度感知和训练速度的方法。- 首先学习场景中不包含待操作透明物体的背景 NeRF，可以减少学习新物体变化带来的歧义。- Residual-NeRF 学习推断残差 RGB 值和密度，Mixnet 学习如何组合背景和残差 NeRF。- 在合成数据上的结果表明，Residual-NeRF 的 RMSE 低 46.1%，MAE 低 29.5%。- 真实世界的定性实验表明，Residual-NeRF 能够生成更鲁棒的深度图，噪声更少，孔洞更少。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>论文标题：Residual-NeRF：学习残差 NeRF 以实现透明物体操作</p></li><li><p>作者：Bardienus P. Duisterhof, Yuemin Mao, Si Heng Teng, Jeffrey Ichnowski</p></li><li><p>第一作者单位：卡内基梅隆大学机器人研究所</p></li><li><p>关键词：神经辐射场（NeRF）、深度感知、透明物体、残差学习、背景先验</p></li><li><p>论文链接：https://arxiv.org/abs/2405.06181   Github 链接：None</p></li><li><p>摘要：</p></li></ol><p>(1)：研究背景：      透明物体在工业、医药和家庭中无处不在。抓取和操纵这些物体对机器人来说是一个重大挑战。现有的方法难以重建具有挑战性的透明物体的完整深度图，从而在深度重建中留下孔洞。最近的研究表明，神经辐射场（NeRF）非常适合在有透明物体的场景中进行深度感知，并且这些深度图可用于高精度地抓取透明物体。基于 NeRF 的深度重建仍然难以处理特别具有挑战性的透明物体和照明条件。</p><p>(2)：过去的方法和问题：      Dex-NeRF 和 Evo-NeRF 等方法表明，NeRF 在透明物体的深度感知方面是有效的。然而，这些方法还表明，NeRF 往往难以处理特别具有挑战性的透明物体，例如具有挑战性光照条件的酒杯或厨房锡箔。透明物体的挑战源于缺乏特征以及外观中很大的视点依赖性变化。</p><p>(3)：本文提出的研究方法：      为了提高透明物体的深度感知并加快训练速度，我们提出了 Residual-NeRF。在许多情况下，机器人的工作区域的几何形状主要是静态且不透明的，例如架子、桌子和桌子。Residual-NeRF 利用场景的静态和不透明部分作为先验，以减少歧义并提高深度感知。Residual-NeRF 首先通过训练不包含透明物体的图像来学习整个场景的背景 NeRF。然后，Residual-NeRF 使用包含透明物体的完整场景的图像来学习残差 NeRF 和 Mixnet。</p><p>(4)：方法的应用任务和性能：      我们对合成和真实数据进行了实验，表明 Residual-NeRF 提高了透明物体的深度感知。合成数据上的结果表明，Residual-NeRF 在 RMSE 上比基线低 46.1%，在 MAE 上低 29.5%。真实世界的定性实验表明，Residual-NeRF 产生了更稳健的深度图，噪点更少，孔洞更少。</p><ol><li>方法：</li></ol><p>（1）：首先训练不包含透明物体的图像，学习整个场景的背景 NeRF；</p><p>（2）：然后使用包含透明物体的完整场景的图像，学习残差 NeRF 和 Mixnet；</p><p>（3）：利用场景的静态和不透明部分作为先验，减少歧义并提高深度感知；</p><p>（4）：Residual-NeRF 提高了透明物体的深度感知。</p><p><strong>结论</strong></p><p>(1): 本工作通过提出 Residual-NeRF，提高了透明物体的深度感知，并加快了训练速度。</p><p>(2): 创新点：    - 利用场景的静态和不透明部分作为先验，减少歧义并提高深度感知；    - 提出了一种两阶段训练方法，首先学习背景 NeRF，然后学习残差 NeRF 和 Mixnet。    - 提出了一种新的 Mixnet，可以有效地融合背景 NeRF 和残差 NeRF 的输出。</p><pre><code>性能：- 在合成数据上，Residual-NeRF 在 RMSE 上比基线低 46.1%，在 MAE 上低 29.5%。- 在真实世界的定性实验中，Residual-NeRF 产生了更稳健的深度图，噪点更少，孔洞更少。工作量：- 训练 Residual-NeRF 需要两个阶段的训练，这比基线方法更复杂。- Residual-NeRF 需要额外的内存来存储背景 NeRF 和残差 NeRF 的权重。</code></pre><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-b7d7618a421bd8b0947856c3ea91116f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1b6857403dd3f1eeafdb70f45e5b92e4.jpg" align="middle"><img src="https://pica.zhimg.com/v2-4d0e4ee50f9ead394b9fcd552ae92106.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4b39c7bfcb8005d9c40b2eac38f3ed56.jpg" align="middle"><img src="https://pica.zhimg.com/v2-0aaa94e0b48b25617be15c8888555cae.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a7c646c3420664f87093b1eb3a62bfba.jpg" align="middle"></details>## NeRFFaceSpeech: One-shot Audio-driven 3D Talking Head Synthesis via   Generative Prior**Authors:Gihoon Kim, Kwanggyoon Seo, Sihun Cha, Junyong Noh**Audio-driven talking head generation is advancing from 2D to 3D content. Notably, Neural Radiance Field (NeRF) is in the spotlight as a means to synthesize high-quality 3D talking head outputs. Unfortunately, this NeRF-based approach typically requires a large number of paired audio-visual data for each identity, thereby limiting the scalability of the method. Although there have been attempts to generate audio-driven 3D talking head animations with a single image, the results are often unsatisfactory due to insufficient information on obscured regions in the image. In this paper, we mainly focus on addressing the overlooked aspect of 3D consistency in the one-shot, audio-driven domain, where facial animations are synthesized primarily in front-facing perspectives. We propose a novel method, NeRFFaceSpeech, which enables to produce high-quality 3D-aware talking head. Using prior knowledge of generative models combined with NeRF, our method can craft a 3D-consistent facial feature space corresponding to a single image. Our spatial synchronization method employs audio-correlated vertex dynamics of a parametric face model to transform static image features into dynamic visuals through ray deformation, ensuring realistic 3D facial motion. Moreover, we introduce LipaintNet that can replenish the lacking information in the inner-mouth area, which can not be obtained from a given single image. The network is trained in a self-supervised manner by utilizing the generative capabilities without additional data. The comprehensive experiments demonstrate the superiority of our method in generating audio-driven talking heads from a single image with enhanced 3D consistency compared to previous approaches. In addition, we introduce a quantitative way of measuring the robustness of a model against pose changes for the first time, which has been possible only qualitatively. [PDF](http://arxiv.org/abs/2405.05749v2) 11 pages, 5 figures**Summary**通过解决单张图像音频驱动3D Talking Head生成中的3D一致性问题，NeRFFaceSpeech 方法能够生成高质量 3D感知的 Talking Head。**Key Takeaways**- 解决单张图像音频驱动 3D Talking Head 生成的 3D 一致性问题。- 使用生成模型的先验知识与 NeRF 相结合，构建对应于单张图像的 3D 一致的面部特征空间。- 引入空间同步方法，利用参数化人脸模型的音频相关顶点动态，通过射线变形将静态图像特征转换为动态视觉效果，确保逼真的 3D 面部运动。- 引入 LipaintNet 补充内嘴区域中缺失的信息，该信息无法从给定的单张图像中获得。- 以自监督的方式训练网络，利用生成能力而无需额外数据。- 提出一种定量方法来衡量模型对姿势变化的鲁棒性，这在以前只能通过定性方式进行。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: NeRFFaceSpeech: 一次性音频驱动 3D 说话人头部合成，通过生成先验</p></li><li><p>Authors: Gihoon Kim, Kwanggyoon Seo, Sihun Cha, Junyong Noh</p></li><li><p>Affiliation: 首尔国立大学</p></li><li><p>Keywords: 音频驱动, 3D 说话人头部, 神经辐射场, 生成先验, 一次性学习</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2405.05749, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 音频驱动说话人头部生成正从 2D 内容转向 3D 内容。值得注意的是，神经辐射场 (NeRF) 作为合成高质量 3D 说话人头部输出的一种手段而备受关注。不幸的是，这种基于 NeRF 的方法通常需要大量配对的每个身份的音频视觉数据，从而限制了该方法的可扩展性。尽管已经尝试使用单张图像生成音频驱动的 3D 说话人头部动画，但由于图像中遮挡区域的信息不足，结果通常不令人满意。在本文中，我们主要关注解决一次性音频驱动域中被忽视的 3D 一致性方面，其中面部动画主要在正面视角合成。</p><p>(2): 现有的方法：- 基于 NeRF 的方法通常需要大量配对的每个身份的音频视觉数据。- 使用单张图像生成音频驱动的 3D 说话人头部动画的方法由于图像中遮挡区域的信息不足，结果通常不令人满意。</p><p>问题：- 可扩展性受限。- 3D 一致性不足。</p><p>(3): 本文提出的研究方法：- 提出了一种新方法 NeRFFaceSpeech，它能够生成高质量的 3D 感知说话人头部。- 使用生成模型的先验知识与 NeRF 相结合，我们的方法可以构建一个与单张图像相对应的 3D 一致的面部特征空间。- 我们的空间同步方法采用参数化人脸模型的音频相关顶点动态，通过光线变形将静态图像特征转换为动态视觉效果，确保逼真的 3D 面部运动。- 此外，我们引入了 LipaintNet，它可以补充单张给定图像中无法获得的内口区域中缺少的信息。该网络以自监督的方式进行训练，利用生成能力而无需额外数据。</p><p>(4): 在任务和性能上，本文方法取得了以下成就：- 全面的实验表明，与以前的方法相比，我们的方法在从单张图像生成音频驱动的说话人头部方面具有出色的性能，并增强了 3D 一致性。- 此外，我们首次引入了一种衡量模型对姿势变化鲁棒性的定量方法，而以前只能定性地进行。</p><ol><li>Methods:</li></ol><p>(1): 提出 NeRFFaceSpeech 方法，该方法结合了生成模型的先验知识与 NeRF，构建与单张图像相对应的 3D 一致的面部特征空间；</p><p>(2): 采用参数化人脸模型的音频相关顶点动态，通过光线变形将静态图像特征转换为动态视觉效果，确保逼真的 3D 面部运动；</p><p>(3): 引入 LipaintNet，补充单张给定图像中无法获得的内口区域中缺少的信息，该网络以自监督的方式进行训练，利用生成能力而无需额外数据；</p><p>(4): 设计定量方法衡量模型对姿势变化的鲁棒性。</p><ol><li>结论：</li></ol><p>（1）：本文提出了 NeRFFaceSpeech，一种通过利用生成先验构建和操纵 3D 特征，从单幅图像生成 3D 感知音频驱动说话人头部动画的新方法；</p><p>（2）：创新点：将生成模型先验与神经辐射场相结合，构建与单幅图像相对应的 3D 一致的面部特征空间；通过光线变形，采用参数化人脸模型的音频相关顶点动态，将静态图像特征转换为动态视觉效果，确保逼真的 3D 面部运动；引入了 LipaintNet，一个自监督学习框架，利用生成模型的能力合成隐藏的内口区域，补充变形场以产生可行结果；设计了定量方法来衡量模型对姿势变化的鲁棒性。性能：与以前的方法相比，我们的方法在从单幅图像生成音频驱动的说话人头部方面具有出色的性能，并增强了 3D 一致性；首次引入了一种衡量模型对姿势变化鲁棒性的定量方法，而以前只能定性地进行。工作量：本文提出的方法需要较大的计算资源和较长的训练时间。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-2a60d3f8bc167b5a06ffeda10f57dfc8.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d422ea4050244e053b7e4851bb4a9ade.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e65d136edc8fc7443ae44525f2b6db77.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4e5fb53c0c038366d8c74e34f9bffdfb.jpg" align="middle"></details>## Tactile-Augmented Radiance Fields**Authors:Yiming Dou, Fengyu Yang, Yi Liu, Antonio Loquercio, Andrew Owens**We present a scene representation, which we call a tactile-augmented radiance field (TaRF), that brings vision and touch into a shared 3D space. This representation can be used to estimate the visual and tactile signals for a given 3D position within a scene. We capture a scene's TaRF from a collection of photos and sparsely sampled touch probes. Our approach makes use of two insights: (i) common vision-based touch sensors are built on ordinary cameras and thus can be registered to images using methods from multi-view geometry, and (ii) visually and structurally similar regions of a scene share the same tactile features. We use these insights to register touch signals to a captured visual scene, and to train a conditional diffusion model that, provided with an RGB-D image rendered from a neural radiance field, generates its corresponding tactile signal. To evaluate our approach, we collect a dataset of TaRFs. This dataset contains more touch samples than previous real-world datasets, and it provides spatially aligned visual signals for each captured touch signal. We demonstrate the accuracy of our cross-modal generative model and the utility of the captured visual-tactile data on several downstream tasks. Project page: https://dou-yiming.github.io/TaRF [PDF](http://arxiv.org/abs/2405.04534v1) CVPR 2024, Project page: https://dou-yiming.github.io/TaRF, Code:   https://github.com/Dou-Yiming/TaRF/**Summary**视觉触觉增强辐射场将视觉和触觉带入共享的 3D 空间，能够估计场景中给定 3D 位置的视觉和触觉信号。**Key Takeaways**- 视觉触觉增强辐射场 (TaRF) 结合了视觉和触觉信号，用于估计场景中给定 3D 位置的视觉和触觉信号。- TaRF 由照片和稀疏采样的触觉探针采集而成。- 视觉触觉增强辐射场利用了视觉触觉传感器可与图像配准以及场景中视觉和结构相似的区域共享相同触觉特征的见解。- 触觉信号通过配准和条件扩散模型与捕获的视觉场景相关联。- TaRF 数据集包含比以往真实世界数据集更多的触觉样本，并为每个捕获的触觉信号提供了空间对齐的视觉信号。- 跨模态生成模型的准确性已得到验证，且已在多项下游任务中证明了捕获的视觉触觉数据的实用性。- 项目主页：https://dou-yiming.github.io/TaRF**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: 触觉增强辐射场：一种用于跨模态感知和生成的新型场景表示</p></li><li><p>Authors: Douyi Ming, Srinath Sridhar, Jiajun Wu, Angjoo Kanazawa, Peter Anderson, Wojciech Matusik, Jonathan Ragan-Kelley</p></li><li><p>Affiliation: 麻省理工学院</p></li><li><p>Keywords: Cross-modal perception, generative models, multi-view geometry, neural radiance fields, tactile sensing, vision</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2301.09422, Github: https://github.com/douyiming/TaRF</p></li><li><p>Summary:</p></li></ol><p>(1): 本文的研究背景是视觉和触觉感知在共享的 3D 空间中的表示问题。</p><p>(2): 过去的方法通常是将视觉和触觉信号视为独立的模态，这限制了跨模态感知和生成的任务。</p><p>(3): 本文提出了一种新的场景表示形式——触觉增强辐射场 (TaRF)，它将视觉和触觉信号统一到一个 3D 空间中。TaRF 的构建利用了基于视觉的触觉传感器与图像之间的几何对应关系，以及场景中视觉和结构相似区域具有相同触觉特征的假设。</p><p>(4): 在 TaRF 数据集上，本文提出的方法在跨模态生成、触觉信号估计和触觉引导的视觉探索等任务上取得了良好的性能，证明了其在跨模态感知和生成方面的有效性。</p><ol><li>方法：</li></ol><p>（1）：构建视觉和触觉增强辐射场（TaRF），将视觉和触觉信号统一到一个 3D 空间中；</p><p>（2）：通过视觉-触觉对应关系和场景中视觉和结构相似区域具有相同触觉特征的假设，建立 TaRF；</p><p>（3）：使用基于视觉的触觉传感器和图像之间的几何对应关系，估计 TaRF 中的视觉和触觉信号；</p><p>（4）：利用生成模型，估计场景中其他位置的触觉信号；</p><p>（5）：使用条件潜在扩散模型，从渲染的视觉信号中预测触觉信号；</p><p>（6）：收集包含 19.3k 个图像对的视觉-触觉数据集，用于训练和评估 TaRF。</p><ol><li>结论：<pre><code>            （1）：本文提出了一种新的场景表示形式——触觉增强辐射场（TaRF），首次将视觉和触觉信号统一到一个共享的 3D 空间中，为跨模态感知和生成提供了新的可能性。            （2）：创新点：提出了一种将视觉和触觉信号统一到一个 3D 空间中的场景表示形式 TaRF；提出了一种基于视觉-触觉对应关系和场景中视觉和结构相似区域具有相同触觉特征的假设，建立 TaRF 的方法；提出了一种使用生成模型，估计场景中其他位置的触觉信号的方法。性能：在跨模态生成、触觉信号估计和触觉引导的视觉探索等任务上取得了良好的性能；收集了包含 19.3k 个图像对的视觉-触觉数据集，为 TaRF 的训练和评估提供了丰富的素材。 workload：TaRF 的构建需要基于视觉的触觉传感器和图像之间的几何对应关系，以及场景中视觉和结构相似区域具有相同触觉特征的假设，这在某些情况下可能存在挑战；TaRF 的训练需要大量的视觉-触觉数据，这可能会增加数据收集和标注的工作量。</code></pre></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-5322ab124785e1ed8207592748379b4a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2a5022ff2c6b665ce2b96a8b7b9f166a.jpg" align="middle"><img src="https://pica.zhimg.com/v2-929d0d52fcc6b94f08fe05a010b4ea04.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-288d198e1ad4f685777631680ccf4209.jpg" align="middle"><img src="https://pica.zhimg.com/v2-59b2afe338ddf3888c68d0443ec0d04f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7ec71c15419dcda442892e2bc1a105da.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-432dac236eec2115922c4f0698f51eec.jpg" align="middle"></details><h2 id="DistGrid-Scalable-Scene-Reconstruction-with-Distributed-Multi-resolution-Hash-Grid"><a href="#DistGrid-Scalable-Scene-Reconstruction-with-Distributed-Multi-resolution-Hash-Grid" class="headerlink" title="DistGrid: Scalable Scene Reconstruction with Distributed   Multi-resolution Hash Grid"></a>DistGrid: Scalable Scene Reconstruction with Distributed   Multi-resolution Hash Grid</h2><p><strong>Authors:Sidun Liu, Peng Qiao, Zongxin Ye, Wenyu Li, Yong Dou</strong></p><p>Neural Radiance Field~(NeRF) achieves extremely high quality in object-scaled and indoor scene reconstruction. However, there exist some challenges when reconstructing large-scale scenes. MLP-based NeRFs suffer from limited network capacity, while volume-based NeRFs are heavily memory-consuming when the scene resolution increases. Recent approaches propose to geographically partition the scene and learn each sub-region using an individual NeRF. Such partitioning strategies help volume-based NeRF exceed the single GPU memory limit and scale to larger scenes. However, this approach requires multiple background NeRF to handle out-of-partition rays, which leads to redundancy of learning. Inspired by the fact that the background of current partition is the foreground of adjacent partition, we propose a scalable scene reconstruction method based on joint Multi-resolution Hash Grids, named DistGrid. In this method, the scene is divided into multiple closely-paved yet non-overlapped Axis-Aligned Bounding Boxes, and a novel segmented volume rendering method is proposed to handle cross-boundary rays, thereby eliminating the need for background NeRFs. The experiments demonstrate that our method outperforms existing methods on all evaluated large-scale scenes, and provides visually plausible scene reconstruction. The scalability of our method on reconstruction quality is further evaluated qualitatively and quantitatively. </p><p><a href="http://arxiv.org/abs/2405.04416v2">PDF</a> Originally submitted to Siggraph Asia 2023</p><p><strong>Summary</strong><br>大规模场景只需用单一NeRF，通过多级Hash网格，而无需单独的背景NeRF，即可实现场景重建。</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF 在对象和室内场景重建中表现出色，但在重建大型场景时存在问题。</li><li>基于 MLP 的 NeRF 网络容量有限，而基于体积的 NeRF 会随着场景分辨率的增加占用大量内存。</li><li>最近的方法将场景地理分区，并使用单独的 NeRF 学习每个分区。</li><li>这有助于基于体积的 NeRF 突破单 GPU 内存限制，并扩展到更大的场景。</li><li>但这种方法需要多个背景 NeRF 来处理分区外的光线，这导致学习冗余。</li><li>本文提出了 DistGrid，基于多级散列网格的分布式场景重建方法。</li><li>该方法将场景划分为多个紧密排列但不重叠的轴对齐包围盒，并提出了一种分割体积渲染方法来处理跨边界光线，从而消除了对背景 NeRF 的需求。</li><li>实验表明，该方法在所有评估的大型场景上都优于现有方法，并提供了视觉上合理的效果。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: DistGrid: 基于分布式多分辨率哈希网格的大规模场景重建</p></li><li><p>Authors: Sidun Liu, Peng Qiao, Zongxin Ye, Wenyu Li, Yong Dou</p></li><li><p>Affiliation: 国防科技大学</p></li><li><p>Keywords: Neural Radiance Field, Distributed Algorithm, Large-scale Scene Reconstruction, Neural Rendering</p></li><li><p>Urls: Paper: https://arxiv.org/pdf/2405.04416.pdf, Github: None</p></li><li><p>Summary:</p><p>(1): 神经渲染技术自 NeRF 提出以来取得了重大进展，NeRF 旨在解决新视角合成任务。它基于体积渲染，使用称为神经场的多分层感知器（MLP）隐式表示场景。该神经场接受 3D 坐标和观察方向作为输入，并预测其对应的密度和颜色。NeRF 在新视角合成任务上产生了令人印象深刻的视觉质量。其学习的神经场也可用于场景重建。</p><p>(2): NeRF 的后续工作旨在提高效率和质量。一种方法是将场景地理分区，并使用单独的 NeRF 学习每个子区域。这种分区策略帮助基于体积的 NeRF 超过单个 GPU 的内存限制，并扩展到更大的场景。但是，这种方法需要多个背景 NeRF 来处理分区外的光线，这导致了学习的冗余。</p><p>(3): 本文提出了一种基于联合多分辨率哈希网格的可扩展场景重建方法，称为 DistGrid。在此方法中，场景被划分为多个紧密相邻但非重叠的轴对齐边界框，并提出了一种新颖的分段体积渲染方法来处理跨边界光线，从而消除了对背景 NeRF 的需求。</p><p>(4): 实验表明，该方法在所有评估的大规模场景上都优于现有方法，并提供了视觉上合理的场景重建。该方法在重建质量上的可扩展性还通过定性和定量的方式进行了进一步评估。</p></li><li><p>方法：</p><p>(1): 本文提出了一种基于联合多分辨率哈希网格的可扩展场景重建方法，称为 DistGrid。</p><p>(2): DistGrid 将场景划分为多个紧密相邻但非重叠的轴对齐边界框 (AABB)，并使用新颖的分段体积渲染方法来处理跨边界光线，从而消除了对背景 NeRF 的需求。</p><p>(3): DistGrid 采用两级级联结构，其中细粒度 NeRF 使用内层边界框作为其边界框，而粗粒度 NeRF 使用外层边界框。</p><p>(4): DistGrid 使用分段体积渲染方法来处理跨区域光线，该方法将体积渲染积分分解为多个部分，并使用部分颜色和部分透射率来计算渲染颜色和最终透射率。</p></li><li><p>结论：</p><pre><code>            (1):本文提出了 DistGrid，一种基于联合多分辨率哈希网格的可扩展场景重建方法，该方法在视觉质量和效率方面都优于现有方法。            (2):创新点：提出了一种新颖的分段体积渲染方法来处理跨边界光线，无需背景 NeRF，提高了效率；采用两级级联结构，细粒度 NeRF 和粗粒度 NeRF 协同工作，提高了重建质量。            性能：在所有评估的大规模场景上都优于现有方法，并提供了视觉上合理的场景重建。            工作量：与现有方法相比，DistGrid 在重建质量上的可扩展性得到了定性和定量的方式的进一步评估。</code></pre></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-c94130b609d19ed2e706304ecfbbdde4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c20db83d0a269f077a389b38e5b01349.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1e4dd6ce979c2f2e18008fc25e085162.jpg" align="middle"></details><h2 id="Blending-Distributed-NeRFs-with-Tri-stage-Robust-Pose-Optimization"><a href="#Blending-Distributed-NeRFs-with-Tri-stage-Robust-Pose-Optimization" class="headerlink" title="Blending Distributed NeRFs with Tri-stage Robust Pose Optimization"></a>Blending Distributed NeRFs with Tri-stage Robust Pose Optimization</h2><p><strong>Authors:Baijun Ye, Caiyun Liu, Xiaoyu Ye, Yuantao Chen, Yuhai Wang, Zike Yan, Yongliang Shi, Hao Zhao, Guyue Zhou</strong></p><p>Due to the limited model capacity, leveraging distributed Neural Radiance Fields (NeRFs) for modeling extensive urban environments has become a necessity. However, current distributed NeRF registration approaches encounter aliasing artifacts, arising from discrepancies in rendering resolutions and suboptimal pose precision. These factors collectively deteriorate the fidelity of pose estimation within NeRF frameworks, resulting in occlusion artifacts during the NeRF blending stage. In this paper, we present a distributed NeRF system with tri-stage pose optimization. In the first stage, precise poses of images are achieved by bundle adjusting Mip-NeRF 360 with a coarse-to-fine strategy. In the second stage, we incorporate the inverting Mip-NeRF 360, coupled with the truncated dynamic low-pass filter, to enable the achievement of robust and precise poses, termed Frame2Model optimization. On top of this, we obtain a coarse transformation between NeRFs in different coordinate systems. In the third stage, we fine-tune the transformation between NeRFs by Model2Model pose optimization. After obtaining precise transformation parameters, we proceed to implement NeRF blending, showcasing superior performance metrics in both real-world and simulation scenarios. Codes and data will be publicly available at <a href="https://github.com/boilcy/Distributed-NeRF">https://github.com/boilcy/Distributed-NeRF</a>. </p><p><a href="http://arxiv.org/abs/2405.02880v1">PDF</a> </p><p><strong>Summary</strong><br>使用三阶段姿态优化对分布式NeRF进行精确对齐，以缓解建模大规模城市环境时出现的混叠伪影和姿态精度不足的问题。</p><p><strong>Key Takeaways</strong></p><ul><li>分布式NeRF建模城市环境面临混叠伪影和姿态精度问题。</li><li>采用分阶段姿态优化解决问题，包括Mip-NeRF 360束调整、反向Mip-NeRF 360和Frame2Model优化。</li><li>利用Model2Model优化进一步细化不同NeRF之间的转换。</li><li>精确的姿态优化有效消除NeRF融合中的遮挡伪影。</li><li>在真实和模拟场景中展示出优越的NeRF融合性能。</li><li>代码和数据将在GitHub上公开。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>题目：基于三阶段鲁棒位姿优化融合分布式NeRFs</p></li><li><p>作者：Baijun Ye∗1,2, Caiyun Liu∗1, Xiaoyu Ye1,2, Yuantao Chen1,3, Yuhai Wang4,Zike Yan1, Yongliang Shi1†, Hao Zhao1, Guyue Zhou1</p></li><li><p>第一作者单位：清华大学人工智能产业研究院（AIR）</p></li><li><p>关键词：分布式NeRF、位姿优化、NeRF融合、Mip-NeRF 360、iNeRF</p></li><li><p>论文链接：https://arxiv.org/abs/2405.02880Github：None</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：在大规模场景建模领域，NeRF因其能够在保持紧凑模型结构的同时实现逼真的渲染而备受关注。然而，当前的分布式NeRF配准方法存在混叠伪影，这源于渲染分辨率差异和次优位姿精度。这些因素共同降低了NeRF框架内位姿估计的保真度，导致NeRF融合阶段出现遮挡伪影。</p><p>（2）：以往方法及其问题：以往方法主要有两种：批处理学习和增量学习。批处理学习需要大量的计算资源，而增量学习容易出现遗忘问题。此外，当前使用显式编码方法（如网格和八叉树）进行实时性能的NeRF方法，面临着随着场景规模的增加，编码组件呈指数级扩展的挑战，导致存储需求大幅增加。</p><p>（3）：本文提出的研究方法：本文提出了一种基于三阶段鲁棒位姿优化的分布式NeRF框架。在第一阶段，通过捆绑调整Mip-NeRF 360并采用由粗到精的策略，实现了图像的精确位姿。在第二阶段，借鉴LATITUDE，利用截断动态低通滤波（TDLF）的原理对反向Mip-NeRF 360进行了优化，称为iMNeRF。此方法类似于模糊图像以使优化过程更加鲁棒，从而实现帧到模型的位姿优化。随后，采用协视图区域检索方法来搜索不同NeRF实例中最相似的图像，进而确定其关联的位姿。给定关联的位姿，利用iMNeRF通过渲染图像和观察图像之间的光度损失来优化这些位姿，从而获得可靠的帧到模型转换。在第三阶段，通过不同的帧到模型转换获得了NeRF之间粗略的模型到模型转换。然后，将不同的NeRF模型投影到一个统一的坐标系中，并使用渲染图像作为观测值进一步优化NeRF之间的相对转换，即通过模型到模型优化来获得NeRF之间的精确转换。</p><p>（4）：方法在什么任务上取得了怎样的性能：利用三阶段位姿优化，实现了NeRF融合并获得了更好的性能。为了验证本文方法，同时发布了真实世界和模拟数据集，展示了本文方法在性能上的优越性。</p><ol><li>方法：</li></ol><p>（1）：提出了一种基于三阶段鲁棒位姿优化的分布式NeRF框架；</p><p>（2）：第一阶段，通过捆绑调整Mip-NeRF 360并采用由粗到精的策略，实现了图像的精确位姿；</p><p>（3）：第二阶段，借鉴LATITUDE，利用截断动态低通滤波（TDLF）的原理对反向Mip-NeRF 360进行了优化，称为iMNeRF；</p><p>（4）：给定关联的位姿，利用iMNeRF通过渲染图像和观察图像之间的光度损失来优化这些位姿，从而获得可靠的帧到模型转换；</p><p>（5）：第三阶段，通过不同的帧到模型转换获得了NeRF之间粗略的模型到模型转换；</p><p>（6）：将不同的NeRF模型投影到一个统一的坐标系中，并使用渲染图像作为观测值进一步优化NeRF之间的相对转换，即通过模型到模型优化来获得NeRF之间的精确转换。</p><ol><li>结论：</li></ol><p>（1）：本文提出了一种基于三阶段鲁棒位姿优化的分布式NeRF框架，解决了当前分布式NeRF配准方法中存在的混叠伪影问题，提高了NeRF融合的保真度；</p><p>（2）：创新点：提出了三阶段鲁棒位姿优化方法，包括图像精确位姿估计、帧到模型位姿优化和模型到模型位姿优化；性能：在真实世界和模拟数据集上验证了本文方法的优越性；工作量：本文方法需要额外的计算资源进行位姿优化，但可以实现更好的NeRF融合效果。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-19421ede11ee24694424a6e2329cbd82.jpg" align="middle"><img src="https://pica.zhimg.com/v2-c3320829396c5cee81241227bf678e63.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4f9d4086bd525c9621ef04e939f0ee92.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-cce8d4e7f7b76ee03713ad33dc0da96e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f2f0b44124a450d85749c232dce8a310.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8b718f4974ac6aac1d1a62a7beb2d681.jpg" align="middle"><img src="https://picx.zhimg.com/v2-767f74cc31852d4c6f28717ac5e03f47.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5929c8babaf3c740cf4c2d6f2886bf8e.jpg" align="middle"></details>## TK-Planes: Tiered K-Planes with High Dimensional Feature Vectors for   Dynamic UAV-based Scenes**Authors:Christopher Maxey, Jaehoon Choi, Yonghan Lee, Hyungtae Lee, Dinesh Manocha, Heesung Kwon**In this paper, we present a new approach to bridge the domain gap between synthetic and real-world data for un- manned aerial vehicle (UAV)-based perception. Our formu- lation is designed for dynamic scenes, consisting of moving objects or human actions, where the goal is to recognize the pose or actions. We propose an extension of K-Planes Neural Radiance Field (NeRF), wherein our algorithm stores a set of tiered feature vectors. The tiered feature vectors are generated to effectively model conceptual information about a scene as well as an image decoder that transforms output feature maps into RGB images. Our technique leverages the information amongst both static and dynamic objects within a scene and is able to capture salient scene attributes of high altitude videos. We evaluate its performance on challenging datasets, including Okutama Action and UG2, and observe considerable improvement in accuracy over state of the art aerial perception algorithms. [PDF](http://arxiv.org/abs/2405.02762v1) 8 pages, submitted to IROS2024**Summary**无人机实时感知任务中，该文将静态特征与动态特征相结合，提高了神经辐射场（NeRF）模型在合成和真实世界数据之间的域适应性。**Key Takeaways**- 提出了一种分层特征向量的神经辐射场（NeRF）扩展版本，用于捕获动态场景中的概念信息。- 扩展的NeRF模型通过图像解码器将输出特征图转换为RGB图像。- 该模型同时利用静态和动态对象的信息，从而捕获高空视频中的显著场景属性。- 将静态和动态特征相结合，提高了模型在合成和真实世界数据之间的域适应性。- 在Okutama Action和UG2等具有挑战性的数据集上评估了该模型的性能。- 与最先进的无人机感知算法相比，该模型在准确性方面有显著提高。- 该模型可以应用于无人机实时感知任务，例如动作识别和姿态估计。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: TK-Planes：具有高维特征向量的分层 K-Planes</p></li><li><p>Authors: Christopher Maxey, Jaehoon Choi, Yonghan Lee, Hyungtae Lee, Dinesh Manocha, Heesung Kwon</p></li><li><p>Affiliation: 美国陆军研究实验室</p></li><li><p>Keywords: Neural Radiance Fields, Synthetic Data, UAV Perception, Dynamic Scenes, Feature Vectors</p></li><li><p>Urls: https://arxiv.org/abs/2405.02762, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 本文研究背景是合成数据在无人机感知中的应用，特别是动态场景的识别，如姿势或动作识别。</p><p>(2): 过去的方法主要基于 K-Planes 神经辐射场，但存在问题：动态对象建模困难、静态和动态元素分离困难、动态对象稀疏、姿态多样性受限。</p><p>(3): 本文提出的研究方法是 TK-Planes，一种分层 K-Planes 算法，输出和操作特征向量而不是 RGB 像素值。这些特征向量可以存储场景中特定对象或位置的概念信息，并为多个相应的相机光线输出时形成特征图，然后解码为最终图像。</p><p>(4): 该方法在 Okutama Action 和 UG2 等具有挑战性的无人机数据集上进行了评估，结果表明，与现有算法相比，基于 TK-Planes 的 NeRF 模型可以生成补充的无人机数据，从而提高动态场景的整体识别准确性。</p><ol><li>方法：</li></ol><p>（1）：使用 NeRF（神经辐射场）在特征空间中生成新视角，以更好地捕获场景中的动态对象，如人物。</p><p>（2）：采用基于网格的 NeRF，网格类似于 K-Planes，但存储的特征向量不直接编码 RGB 值，而是编码场景中的更高层次概念信息，如地面、树木和人物。</p><p>（3）：使用分层网格在特征空间中操作，将场景分解为静态和动态特征，并使用图像解码器将特征图解码为最终图像。</p><p>（4）：在具有挑战性的无人机数据集上评估模型，结果表明基于 TK-Planes 的 NeRF 模型可以生成补充的无人机数据，从而提高动态场景的识别准确性。</p><ol><li>结论：<pre><code>            (1):本文提出的分层 K-Planes（TK-Planes）算法，通过在特征空间中操作特征向量，有效地解决了合成数据在无人机感知中的动态场景识别问题。            (2):创新点：TK-Planes 算法将场景分解为静态和动态特征，并使用分层网格在特征空间中操作，从而更好地捕获动态对象；性能：在 Okutama Action 和 UG2 等具有挑战性的无人机数据集上的评估结果表明，基于 TK-Planes 的 NeRF 模型可以生成补充的无人机数据，从而提高动态场景的识别准确性；工作量：TK-Planes 算法的实现和在无人机数据集上的评估需要一定的技术投入和计算资源。</code></pre></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-49ebb6a345fe5bed0d70468dcdf8fd84.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-63139de16b603f02b54ce2804a9bad9f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-af95031ec70f26ba5024a7788a09ddff.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c99084999876941f9618dfb5d99f367b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-63f6561485ccf42232344f25cf43bc8a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a606ade4cb1fcbf9e4da91086ed92ad1.jpg" align="middle"></details><h2 id="ActiveNeuS-Active-3D-Reconstruction-using-Neural-Implicit-Surface-Uncertainty"><a href="#ActiveNeuS-Active-3D-Reconstruction-using-Neural-Implicit-Surface-Uncertainty" class="headerlink" title="ActiveNeuS: Active 3D Reconstruction using Neural Implicit Surface   Uncertainty"></a>ActiveNeuS: Active 3D Reconstruction using Neural Implicit Surface   Uncertainty</h2><p><strong>Authors:Hyunseo Kim, Hyeonseo Yang, Taekyung Kim, YoonSung Kim, Jin-Hwa Kim, Byoung-Tak Zhang</strong></p><p>Active learning in 3D scene reconstruction has been widely studied, as selecting informative training views is critical for the reconstruction. Recently, Neural Radiance Fields (NeRF) variants have shown performance increases in active 3D reconstruction using image rendering or geometric uncertainty. However, the simultaneous consideration of both uncertainties in selecting informative views remains unexplored, while utilizing different types of uncertainty can reduce the bias that arises in the early training stage with sparse inputs. In this paper, we propose ActiveNeuS, which evaluates candidate views considering both uncertainties. ActiveNeuS provides a way to accumulate image rendering uncertainty while avoiding the bias that the estimated densities can introduce. ActiveNeuS computes the neural implicit surface uncertainty, providing the color uncertainty along with the surface information. It efficiently handles the bias by using the surface information and a grid, enabling the fast selection of diverse viewpoints. Our method outperforms previous works on popular datasets, Blender and DTU, showing that the views selected by ActiveNeuS significantly improve performance. </p><p><a href="http://arxiv.org/abs/2405.02568v1">PDF</a> </p><p><strong>Summary</strong><br>主动神经重建同时考虑图像渲染和几何不确定性，以选择信息丰富的训练视图来提高 3D 场景重建性能。</p><p><strong>Key Takeaways</strong></p><ul><li>主动学习在 3D 场景重建中至关重要。</li><li>NeRF 变体使用图像渲染或几何不确定性提高了主动 3D 重建的性能。</li><li>ActiveNeuS 同时考虑了图像渲染和几何不确定性来选择信息丰富的视图。</li><li>ActiveNeuS 积累图像渲染不确定性，同时避免估计密度引入的偏差。</li><li>ActiveNeuS 计算神经隐式表面不确定性，提供颜色不确定性和表面信息。</li><li>ActiveNeuS 使用表面信息和网格有效处理偏差，从而快速选择多样化的视点。</li><li>ActiveNeuS 在 Blender 和 DTU 数据集上的表现优于以往的工作，表明 ActiveNeuS 选择的视图显著提高了性能。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>标题：ActiveNeuS：基于神经隐式曲面不确定性的主动三维重建</li><p></p><p></p><li>作者：Hyunseo Kim, Hyeonseo Yang, Taekyung Kim, YoonSung Kim, Jin-Hwa Kim, Byoung-Tak Zhang</li><p></p><p></p><li>单位：首尔大学</li><p></p><p></p><li>关键词：主动学习、神经隐式曲面不确定性、曲面网格</li><p></p><p></p><li>论文链接：https://arxiv.org/pdf/2405.02568.pdf，Github代码链接：无</li><p></p><p></p><li>摘要：</li><br>&lt;/ol&gt;<p></p><p>（1）研究背景：三维场景重建中的主动学习已被广泛研究，因为选择有信息的训练视图对于重建至关重要。最近，神经辐射场（NeRF）变体在使用图像渲染或几何不确定性进行主动三维重建方面表现出性能提升。然而，在选择信息视图时同时考虑两种不确定性仍然未被探索，而利用不同类型的不确定性可以减少在早期训练阶段由于输入稀疏而产生的偏差。</p><p>（2）过去的方法及其问题：传统的NeRF主动学习方法通常估计其输出中的不确定性：三维点的密度和颜色。Martin等人和Pan等人估计了颜色预测中的不确定性，方法是将颜色建模为高斯概率分布。然而，这些方法在早期训练阶段容易受到密度估计偏差的影响，这可能会导致信息视图选择不佳。</p><p>（3）提出的研究方法：本文提出了ActiveNeuS，它在评估候选视图时考虑了图像渲染不确定性和神经隐式曲面不确定性。ActiveNeuS提供了一种积累图像渲染不确定性的方法，同时避免了估计密度可能引入的偏差。ActiveNeuS计算神经隐式曲面不确定性，提供颜色不确定性和曲面信息。它通过使用曲面信息和网格有效地处理偏差，从而能够快速选择不同的视点。</p><p>（4）任务和性能：在流行的数据集Blender和DTU上，ActiveNeuS的性能优于以前的工作，表明ActiveNeuS选择的视图显著提高了性能。这些结果支持了作者的目标，即开发一种主动学习方法，该方法可以有效地选择信息视图以提高三维重建的性能。</p><ol><li><p>方法：</p><pre><code>            (1): ActiveNeuS 提出了一种新的采集函数，该函数结合了几何重建和图像渲染的视角。            (2): ActiveNeuS 估计颜色预测的不确定性，以获取有关图像渲染质量的信息。            (3): 采集函数集成了估计的不确定性，同时不丢失几何属性。            (4): 首先，在第 4.1 节中，我们介绍了我们的采集函数，并解释了在积分过程中如何考虑曲面。            (5): 在第 4.2 节中，我们定义了 ActiveNeuS 中估计的神经隐式表面不确定性，并描述了如何在采集函数中利用不确定性。            (6): 然后，为了进行高效且稳健的计算，我们引入了存储曲面信息的曲面网格和选择多个次优视图 (NBV) 的策略（第 4.3 节）。</code></pre></li><li><p>结论：</p></li></ol><p>（1）：本文提出了一种有效的信息视图选择方法 ActiveNeuS，该方法同时考虑了几何重建和图像渲染的保真度。ActiveNeuS 引入了一种新的采集函数，该函数利用不确定性网格有效且稳健地利用神经隐式曲面不确定性。采集函数通过使用曲面网格并根据曲面的存在应用不同的积分策略来计算神经隐式曲面不确定性的积分。我们展示了 ActiveNeuS 的下一个最佳视图选择，与其他方法相比，它改进了网格重建和图像渲染质量。对于未来的工作，我们建议研究一种有效的方法来连接不同网络的不确定性以进行信息视图选择。此外，将 ActiveNeuS 应用于机器人主动 3D 重建中也很有趣，其中机器人手臂移动并收集数据。</p><p>（2）：创新点：本文提出了一种新的采集函数，该函数同时考虑了图像渲染不确定性和神经隐式曲面不确定性，有效地提高了信息视图的选择。性能：在 Blender 和 DTU 等流行数据集上，ActiveNeuS 的性能优于以前的工作，表明 ActiveNeuS 选择的视图显着提高了性能。工作量：ActiveNeuS 的计算成本相对较高，因为它需要估计神经隐式曲面不确定性并使用曲面网格进行积分。</p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-0eb6f5097fe2312cfce57f04da637606.jpg" align="middle"><img src="https://picx.zhimg.com/v2-824537082b5290b565ea1f0da3946351.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7939474a53153965a29e673124ea4a9e.jpg" align="middle"></details><h2 id="Rip-NeRF-Anti-aliasing-Radiance-Fields-with-Ripmap-Encoded-Platonic-Solids"><a href="#Rip-NeRF-Anti-aliasing-Radiance-Fields-with-Ripmap-Encoded-Platonic-Solids" class="headerlink" title="Rip-NeRF: Anti-aliasing Radiance Fields with Ripmap-Encoded Platonic   Solids"></a>Rip-NeRF: Anti-aliasing Radiance Fields with Ripmap-Encoded Platonic   Solids</h2><p><strong>Authors:Junchen Liu, Wenbo Hu, Zhuo Yang, Jianteng Chen, Guoliang Wang, Xiaoxue Chen, Yantong Cai, Huan-ang Gao, Hao Zhao</strong></p><p>Despite significant advancements in Neural Radiance Fields (NeRFs), the renderings may still suffer from aliasing and blurring artifacts, since it remains a fundamental challenge to effectively and efficiently characterize anisotropic areas induced by the cone-casting procedure. This paper introduces a Ripmap-Encoded Platonic Solid representation to precisely and efficiently featurize 3D anisotropic areas, achieving high-fidelity anti-aliasing renderings. Central to our approach are two key components: Platonic Solid Projection and Ripmap encoding. The Platonic Solid Projection factorizes the 3D space onto the unparalleled faces of a certain Platonic solid, such that the anisotropic 3D areas can be projected onto planes with distinguishable characterization. Meanwhile, each face of the Platonic solid is encoded by the Ripmap encoding, which is constructed by anisotropically pre-filtering a learnable feature grid, to enable featurzing the projected anisotropic areas both precisely and efficiently by the anisotropic area-sampling. Extensive experiments on both well-established synthetic datasets and a newly captured real-world dataset demonstrate that our Rip-NeRF attains state-of-the-art rendering quality, particularly excelling in the fine details of repetitive structures and textures, while maintaining relatively swift training times. </p><p><a href="http://arxiv.org/abs/2405.02386v1">PDF</a> SIGGRAPH 2024, Project page: <a href="https://junchenliu77.github.io/Rip-NeRF">https://junchenliu77.github.io/Rip-NeRF</a>   , Code: <a href="https://github.com/JunchenLiu77/Rip-NeRF">https://github.com/JunchenLiu77/Rip-NeRF</a></p><p><strong>Summary</strong><br>神经辐射场（NeRF）通过极坐标投影将三维各向异性区域射影到平面，再利用Ripmap编码对各平面进行编码，进而解决NeRF抗锯齿渲染中的混叠和模糊问题。</p><p><strong>Key Takeaways</strong></p><ul><li>极坐标投影将三维各向异性区域射影到平面，便于特征化。</li><li>Ripmap编码通过各向异性预滤波可学习特征网格，对射影各向异性区域进行精确高效的特征化。</li><li>方法在合成数据集和实景数据集上均取得了最优渲染质量。</li><li>方法在重复结构和纹理的精细细节上表现优异。</li><li>方法训练时间相对较短。</li><li>该方法依赖于可学习特征网格。</li><li>该方法目前仅适用于静态场景。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p><strong>标题：</strong>Rip-NeRF：基于Ripmap编码的Platonic实体的反走样辐射场</p></li><li><p><strong>作者：</strong>Junchen Liu, Wenbo Hu, Zhuo Yang, Jianteng Chen, Guoliang Wang, Xiaoxue Chen, Yantong Cai, Huan-ang Gao, Hao Zhao</p></li><li><p><strong>第一作者单位：</strong>北京航空航天大学</p></li><li><p><strong>关键词：</strong>神经辐射场（NeRFs）、反走样、Ripmap编码、Platonic实体</p></li><li><p><strong>论文链接：</strong>https://arxiv.org/pdf/2405.02386.pdf，<strong>Github链接：</strong>None</p></li><li><p><strong>摘要：</strong></p><p>(1) <strong>研究背景：</strong>尽管神经辐射场（NeRFs）取得了重大进展，但其渲染结果仍可能存在走样和模糊伪影，因为有效且高效地表征锥形投射过程产生的各向异性区域仍然是一个基本挑战。</p><p>(2) <strong>过去的方法和问题：</strong>现有方法要么无法精确地表征各向异性区域，要么效率低下。</p><p>(3) <strong>本文提出的研究方法：</strong>本文提出了一种基于Ripmap编码的Platonic实体表示，用于精确高效地表征3D各向异性区域，从而实现高保真反走样渲染。该方法的核心是两个关键组件：Platonic实体投影和Ripmap编码。Platonic实体投影将3D空间分解到特定Platonic实体的不平行面上，使得各向异性的3D区域可以投影到具有可区分特征的平面上。同时，Platonic实体的每个面都由Ripmap编码编码，该编码通过各向异性预过滤可学习的特征网格构建，以实现对投影各向异性区域的精确和高效表征。</p><p>(4) <strong>方法性能：</strong>在多尺度Blender数据集和新捕获的真实世界数据集上，该方法在渲染质量和效率方面均优于现有方法。</p></li><li><p>方法：</p></li></ol><p>（1）：基于Ripmap编码的Platonic实体投影，将3D空间分解到Platonic实体的不平行面上，将各向异性的3D区域投影到具有可区分特征的平面上。</p><p>（2）：Platonic实体的每个面由Ripmap编码编码，该编码通过各向异性预过滤可学习的特征网格构建，以实现对投影各向异性区域的精确和高效表征。</p><p>（3）：采用混合表示，包括显式和隐式表示，既能保证效率，又能保证灵活性。</p><p>（4）：采用多采样和面积采样对圆锥截体进行特征化，其中面积采样采用各向异性3D高斯函数对圆锥截体进行表征，再利用提出的Platonic实体投影和Ripmap编码进行特征化。</p><p>（5）：利用MLP估计圆锥截体的颜色和密度，并通过体积渲染渲染像素颜色。</p><p>（6）：采用光度损失函数，对渲染图像和观测图像进行端到端优化。</p><p><strong>8. 结论：</strong></p><p>（1）：本工作提出了一种基于Ripmap编码的Platonic实体表示，用于神经辐射场，称为Rip-NeRF。Rip-NeRF可以渲染高保真抗锯齿图像，同时保持效率，这得益于提出的Platonic实体投影和Ripmap编码。Platonic实体投影将3D空间分解到特定Platonic实体的不平行面上，使得各向异性的3D区域可以投影到具有可区分特征的平面上。Ripmap编码通过各向异性预过滤可学习的特征网格构建，能够对投影的各向异性区域进行精确高效的特征化。这两个组件协同工作，对各向异性的3D区域进行精确高效的特征化。它在合成数据集和真实世界捕捉上都实现了最先进的渲染质量，特别是在结构和纹理的精细细节方面表现出色，这验证了所提出的Platonic实体投影和Ripmap编码的有效性。</p><p>（2）：创新点：提出了基于Ripmap编码的Platonic实体投影和Ripmap编码，用于对各向异性的3D区域进行精确高效的特征化。</p><p>性能：在渲染质量和效率方面均优于现有方法，特别是在精细细节方面表现出色。</p><p>工作量：与现有方法相比，工作量略大，因为需要对Platonic实体投影和Ripmap编码进行预处理。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-7575292a1dc220679b8e9c4fa1e7bb9b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d766632fb875e5fe770b7fee4ed1ae6b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-43f9b3bed63eef23fb98c456f7077574.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7b2442c515601a7e17fca7b5a8e2166b.jpg" align="middle"></details></ol>]]></content>
    
    
    <summary type="html">NeRF 方向最新论文已更新，请持续关注 Update in 2024-05-13  OneTo3D One Image to Re-editable Dynamic 3D Model and Video Generation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/05/13/Paper/2024-05-13/3DGS/"/>
    <id>https://kedreamix.github.io/2024/05/13/Paper/2024-05-13/3DGS/</id>
    <published>2024-05-13T08:18:16.000Z</published>
    <updated>2024-05-13T08:18:16.334Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-05-13-更新"><a href="#2024-05-13-更新" class="headerlink" title="2024-05-13 更新"></a>2024-05-13 更新</h1><h2 id="OneTo3D-One-Image-to-Re-editable-Dynamic-3D-Model-and-Video-Generation"><a href="#OneTo3D-One-Image-to-Re-editable-Dynamic-3D-Model-and-Video-Generation" class="headerlink" title="OneTo3D: One Image to Re-editable Dynamic 3D Model and Video Generation"></a>OneTo3D: One Image to Re-editable Dynamic 3D Model and Video Generation</h2><p><strong>Authors:Jinwei Lin</strong></p><p>One image to editable dynamic 3D model and video generation is novel direction and change in the research area of single image to 3D representation or 3D reconstruction of image. Gaussian Splatting has demonstrated its advantages in implicit 3D reconstruction, compared with the original Neural Radiance Fields. As the rapid development of technologies and principles, people tried to used the Stable Diffusion models to generate targeted models with text instructions. However, using the normal implicit machine learning methods is hard to gain the precise motions and actions control, further more, it is difficult to generate a long content and semantic continuous 3D video. To address this issue, we propose the OneTo3D, a method and theory to used one single image to generate the editable 3D model and generate the targeted semantic continuous time-unlimited 3D video. We used a normal basic Gaussian Splatting model to generate the 3D model from a single image, which requires less volume of video memory and computer calculation ability. Subsequently, we designed an automatic generation and self-adaptive binding mechanism for the object armature. Combined with the re-editable motions and actions analyzing and controlling algorithm we proposed, we can achieve a better performance than the SOTA projects in the area of building the 3D model precise motions and actions control, and generating a stable semantic continuous time-unlimited 3D video with the input text instructions. Here we will analyze the detailed implementation methods and theories analyses. Relative comparisons and conclusions will be presented. The project code is open source. </p><p><a href="http://arxiv.org/abs/2405.06547v1">PDF</a> 24 pages, 13 figures, 2 tables</p><p><strong>Summary</strong><br>一个图像到可编辑动态 3D 模型和视频生成是图像到 3D 表示或图像 3D 重建研究领域的新颖方向和变革。</p><p><strong>Key Takeaways</strong></p><ul><li>采用高斯溅射法，可实现隐式 3D 重建，并优于原始神经辐射场。</li><li>借助技术和原理的快速发展，人们尝试使用稳定扩散模型通过文本指令生成目标模型。</li><li>然而，使用常规隐式机器学习方法难以获得精确的动作和动作控制，且难以生成内容长且语义连续的 3D 视频。</li><li>研究者提出 OneTo3D 方法，使用单张图像生成可编辑 3D 模型和目标语义连续且时间无限的 3D 视频。</li><li>研究者采用普通基本高斯溅射模型从单张图像生成 3D 模型，该模型对视频内存和计算机运算能力要求较低。</li><li>研究者针对对象骨架设计了自动生成和自适应绑定机制。</li><li>结合研究者提出的可重新编辑的动作和动作分析控制算法，在 3D 模型精确动作和动作控制以及根据输入文本指令生成稳定的语义连续且时间无限的 3D 视频方面取得了优于 SOTA 项目的性能。</li><li>研究者分析了详细的实现方法和理论分析，并将给出相关的比较和结论。</li><li>该项目代码开源。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>论文标题：OneTo3D：单幅图像生成可编辑动态 3D 模型和视频</p></li><li><p>作者：JINWEI LIN</p></li><li><p>第一作者单位：莫纳什大学</p></li><li><p>关键词：3D、单幅图像、可编辑、动态、生成、自动化、视频、自适应、骨架</p></li><li><p>论文链接：https://arxiv.org/abs/2405.06547，Github 链接：无</p></li><li><p>摘要：</p></li></ol><p>(1)：研究背景：3D 表征或 3D 重建是计算机视觉领域长期以来的挑战。目前实现 3D 重建的方法可分为两类：传统显式方法和机器学习隐式方法。近年来，神经辐射场 (NeRF) 作为一种突出的隐式方法，在渲染和表示真实场景视图方面取得了优异的性能。</p><p>(2)：过去方法：基于 NeRF，出现了各种隐式 3D 表征或重建的研究项目。然而，现有的方法在精确运动和动作控制以及生成连续语义 3D 视频方面存在困难。</p><p>(3)：本文方法：本文提出 OneTo3D 方法，利用单幅图像生成可编辑 3D 模型和连续语义 3D 视频。该方法使用基本的 Gaussian Splatting 模型从单幅图像生成 3D 模型，并设计了一种自动生成和自适应绑定机制来创建对象骨架。结合可编辑运动和动作分析控制算法，OneTo3D 在 3D 模型精确运动和动作控制以及根据文本指令生成稳定连续的语义 3D 视频方面取得了优于现有技术的性能。</p><p>(4)：任务和性能：本文方法在生成可编辑动态 3D 模型和视频的任务上取得了以下性能：- 实现了精确的运动和动作控制，超越了现有技术。- 能够根据文本指令生成稳定连续的语义 3D 视频。- 这些性能支持了本文提出的目标。</p><ol><li>方法：</li></ol><p>(1)：生成初始 3D 模型：利用 Gaussian Splatting 模型从输入图像生成初始 3D 模型，不包含动态或可编辑因素。</p><p>(2)：生成并绑定自适应骨架：分析初始 3D 模型的几何参数信息，构建适合的骨架，并根据输入图像中的姿态、形状和关键点信息微调骨架参数，使其与物体身体贴合。</p><p>(3)：文本到动作：分析用户文本指令的命令意图，提取相对骨骼的特定运动和修改数据，控制特定骨骼在 Blender 中实现相对运动，考虑运动量化、运动次数、运动方向和运动范围等参数。</p><p>(4)：可重新编辑运动控制：配合 Blender 界面实现运动可重新编辑控制，将当前姿态作为关键帧插入，结合连续关键帧生成最终 3D 视频，Blender 文件保存为可重新编辑的 3D 编辑文件。</p><ol><li>结论：</li></ol><p>（1）：本文提出了一种生成可编辑动态 3D 模型和视频的新方法 OneTo3D。该方法具有以下优点：- 实现了精确的运动和动作控制，超越了现有技术。- 能够根据文本指令生成稳定连续的语义 3D 视频。- 这些性能支持了本文提出的目标。</p><p>（2）：创新点：- 利用单幅图像生成可编辑 3D 模型和视频。- 设计了一种自动生成和自适应绑定机制来创建对象骨架。- 结合可编辑运动和动作分析控制算法，实现精确的运动和动作控制。</p><p>性能：- 在生成可编辑动态 3D 模型和视频的任务上取得了优异的性能。- 超越了现有技术在精确运动和动作控制方面的性能。</p><p>工作量：- 该方法需要大量的训练数据和计算资源。- 生成可编辑动态 3D 模型和视频的过程需要大量的时间。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-8729865363a1dfddc21dff54a70072f2.jpg" align="middle"><img src="https://pica.zhimg.com/v2-02dad34b1d632546ae26f127a58c9c0f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f635130d270abd57752edb234d2c8a48.jpg" align="middle"></details>## MGS-SLAM: Monocular Sparse Tracking and Gaussian Mapping with Depth   Smooth Regularization**Authors:Pengcheng Zhu, Yaoming Zhuang, Baoquan Chen, Li Li, Chengdong Wu, Zhanlin Liu**This letter introduces a novel framework for dense Visual Simultaneous Localization and Mapping (VSLAM) based on Gaussian Splatting. Recently Gaussian Splatting-based SLAM has yielded promising results, but rely on RGB-D input and is weak in tracking. To address these limitations, we uniquely integrates advanced sparse visual odometry with a dense Gaussian Splatting scene representation for the first time, thereby eliminating the dependency on depth maps typical of Gaussian Splatting-based SLAM systems and enhancing tracking robustness. Here, the sparse visual odometry tracks camera poses in RGB stream, while Gaussian Splatting handles map reconstruction. These components are interconnected through a Multi-View Stereo (MVS) depth estimation network. And we propose a depth smooth loss to reduce the negative effect of estimated depth maps. Furthermore, the consistency in scale between the sparse visual odometry and the dense Gaussian map is preserved by Sparse-Dense Adjustment Ring (SDAR). We have evaluated our system across various synthetic and real-world datasets. The accuracy of our pose estimation surpasses existing methods and achieves state-of-the-art performance. Additionally, it outperforms previous monocular methods in terms of novel view synthesis fidelity, matching the results of neural SLAM systems that utilize RGB-D input. [PDF](http://arxiv.org/abs/2405.06241v1) This work has been submitted to the IEEE for possible publication.   Copyright may be transferred without notice, after which this version may no   longer be accessible**Summary**基于高斯分布的稠密视觉SLAM新框架，集成了稀疏视觉里程计和稠密场景表示，增强了鲁棒性和准确性。**Key Takeaways*** 结合了稀疏视觉里程计和高斯分布的稠密场景表示。* 消除了对深度图的依赖，增强了跟踪的鲁棒性。* 多视图立体（MVS）深度估计网络连接了稀疏视觉里程计和高斯分布。* 提出深度平滑损失，减少估计深度图的负面影响。* 通过稀疏-稠密调整环（SDAR）保持了稀疏视觉里程计和高斯分布的地图之间的规模一致性。* 在合成数据集和真实世界数据集上评估，姿势估计的准确性超过了现有方法，达到最先进的水平。* 在新视图合成保真度方面优于之前的单目方法，达到利用RGB-D输入的神经SLAM系统的效果。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>Title: MGS-SLAM: 单目稀疏跟踪和高斯映射与深度平滑正则化</li><li>Authors: Pengcheng Zhu, Yaoming Zhuang, Baoquan Chen, Li Li, Chengdong Wu, Zhanlin Liu</li><li>Affiliation: 东北大学机器人科学与工程学院</li><li>Keywords: Visual SLAM, Gaussian Splatting, Sparse Visual Odometry, Multi-View Stereo, Depth Smooth Regularization</li><li>Urls: Paper: https://arxiv.org/abs/2405.06241, Github: None</li><li>Summary:</li></ol><p>(1):该文章的研究背景是：随着深度学习的快速发展，一种利用可微渲染的新型 SLAM 技术应运而生。基于可微渲染的 SLAM 技术最初使用神经辐射场 (NeRF) 作为其基础构建方法。NeRF 利用神经网络表示 3D 场景，能够合成高质量图像并从多视图中恢复密集的几何结构。基于 NeRF 的 SLAM 系统在制图过程中保留了详细的场景信息，增强了对后续导航和路径规划的支持。然而，NeRF 的方法在图像渲染过程中需要对每个像素进行多次前向预测，导致大量的计算冗余。因此，这种低效性阻碍了基于 NeRF 的 SLAM 实时运行，从而限制了其在直接下游任务中的实用性。</p><p>(2):过去的方法是基于高斯散射的 SLAM 系统依赖于 RGB-D 相机的深度图输入，这限制了它们的应用范围。问题是跟踪能力弱。该方法的动机很好，它将基于高斯散射的技术与稀疏视觉里程计相结合，消除了对基于高斯散射的 SLAM 系统中典型的深度图的依赖性，并增强了跟踪鲁棒性。</p><p>(3):本文提出的研究方法是：提出了一种新颖的单目高斯散射 SLAM 系统 MGS-SLAM。该工作在 SLAM 领域引入了多项突破性进展，包括将基于高斯散射的技术与稀疏视觉里程计相结合，采用预训练的多视图立体 (MVS) 深度估计网络，开创了一种几何平滑深度损失，并开发了稀疏 -密集调整环 (SDAR) 以确保尺度一致性。这些创新共同显着提高了仅依赖 RGB 图像输入的 SLAM 系统的准确性和功能性。</p><p>(4):本文的方法在以下任务和性能上取得了成就：在各种合成和真实世界数据集上对我们的系统进行了评估。我们的位姿估计的准确度超过了现有方法，并取得了最先进的性能。此外，它在新的视图合成保真度方面优于之前的单目方法，与利用 RGB-D 输入的神经 SLAM 系统的结果相匹配。</p><ol><li>方法：</li></ol><p>（1）：提出了一种新颖的单目高斯散射 SLAM 系统 MGS-SLAM，将基于高斯散射的技术与稀疏视觉里程计相结合，消除了对基于高斯散射的 SLAM 系统中典型的深度图的依赖性，并增强了跟踪鲁棒性；</p><p>（2）：采用预训练的多视图立体（MVS）深度估计网络，为后端密集映射提供几何深度监督；</p><p>（3）：开创了一种几何平滑深度损失，以减轻先验深度图误差对高斯地图几何重建的影响；</p><p>（4）：开发了稀疏-密集调整环（SDAR），以确保稀疏点云地图和密集高斯地图之间的尺度一致性。</p><ol><li>结论：</li></ol><p>（1）：该工作提出了一种新颖的单目高斯散射 SLAM 系统 MGS-SLAM，该系统将基于高斯散射的技术与稀疏视觉里程计相结合，消除了对基于高斯散射的 SLAM 系统中典型的深度图的依赖性，并增强了跟踪鲁棒性。此外，该工作还采用预训练的多视图立体（MVS）深度估计网络，开创了一种几何平滑深度损失，并开发了稀疏-密集调整环（SDAR），以确保稀疏点云地图和密集高斯地图之间的尺度一致性。这些创新共同显着提高了仅依赖 RGB 图像输入的 SLAM 系统的准确性和功能性。</p><p>（2）：创新点：将基于高斯散射的技术与稀疏视觉里程计相结合，消除了对深度图的依赖性，并增强了跟踪鲁棒性；采用预训练的多视图立体（MVS）深度估计网络，为后端密集映射提供几何深度监督；开创了一种几何平滑深度损失，以减轻先验深度图误差对高斯地图几何重建的影响；开发了稀疏-密集调整环（SDAR），以确保稀疏点云地图和密集高斯地图之间的尺度一致性。</p><p>性能：在各种合成和真实世界数据集上对我们的系统进行了评估。我们的位姿估计的准确度超过了现有方法，并取得了最先进的性能。此外，它在新的视图合成保真度方面优于之前的单目方法，与利用 RGB-D 输入的神经 SLAM 系统的结果相匹配。</p><p>工作量：该方法需要预训练的多视图立体（MVS）深度估计网络，并且需要开发稀疏-密集调整环（SDAR）来确保稀疏点云地图和密集高斯地图之间的尺度一致性。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-9c81783ec5cc64db3f3888e91459cd94.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-5722638fadf13564cb13427fd8d4410c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ea1359ff05ba09f3fd64460b9bd9878a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-940291f15a48e90bf4dec39f8ee7ddd2.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d5bc3a1c2278602383a64b530b3dd889.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e517184c75aa28276e746751c5d28917.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e89f79eb15cc3b181f2efde56510f1d8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5713c30b73286985fa8f2ff3f7ac2e21.jpg" align="middle"></details>## FastScene: Text-Driven Fast 3D Indoor Scene Generation via Panoramic   Gaussian Splatting**Authors:Yikun Ma, Dandan Zhan, Zhi Jin**Text-driven 3D indoor scene generation holds broad applications, ranging from gaming and smart homes to AR/VR applications. Fast and high-fidelity scene generation is paramount for ensuring user-friendly experiences. However, existing methods are characterized by lengthy generation processes or necessitate the intricate manual specification of motion parameters, which introduces inconvenience for users. Furthermore, these methods often rely on narrow-field viewpoint iterative generations, compromising global consistency and overall scene quality. To address these issues, we propose FastScene, a framework for fast and higher-quality 3D scene generation, while maintaining the scene consistency. Specifically, given a text prompt, we generate a panorama and estimate its depth, since the panorama encompasses information about the entire scene and exhibits explicit geometric constraints. To obtain high-quality novel views, we introduce the Coarse View Synthesis (CVS) and Progressive Novel View Inpainting (PNVI) strategies, ensuring both scene consistency and view quality. Subsequently, we utilize Multi-View Projection (MVP) to form perspective views, and apply 3D Gaussian Splatting (3DGS) for scene reconstruction. Comprehensive experiments demonstrate FastScene surpasses other methods in both generation speed and quality with better scene consistency. Notably, guided only by a text prompt, FastScene can generate a 3D scene within a mere 15 minutes, which is at least one hour faster than state-of-the-art methods, making it a paradigm for user-friendly scene generation. [PDF](http://arxiv.org/abs/2405.05768v1) Accepted by IJCAI-2024**摘要**文本驱动的3D室内场景生成在游戏、智能家居和AR/VR应用中有着广泛的应用，快速且高保真场景生成对确保用户友好体验至关重要。**要点**- 3D室内场景生成有着广泛的应用。- 现有的方法生成过程耗时或需要用户手动指定运动参数，给用户带来不便。- 现有的方法专注于窄视域观点迭代生成，影响全局一致性和整体场景质量。- FastScene框架可在保持场景一致性的情况下快速生成更高质量的3D场景。- 根据文本提示生成全景图并估计其深度，因为全景图包含整个场景信息并展示明确的几何约束。- 引入粗视图合成（CVS）和渐进式新视图修复（PNVI）策略来获得高质量的新视角，确保场景一致性和视图质量。- 使用多视图投影（MVP）形成透视视图，并应用3D高斯散射（3DGS）进行场景重建。- 全面实验表明，FastScene在生成速度和质量上都超过了其他方法，并具有更好的场景一致性。- FastScene仅通过文本提示就可以在短短15分钟内生成3D场景，比最先进的方法快至少1小时，使其成为用户友好场景生成范例。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>标题：FastScene：文本驱动的快速 3D 室内场景生成</p></li><li><p>作者：Yikun Ma，Dandan Zhan，Zhi Jin</p></li><li><p>单位：中山大学</p></li><li><p>关键词：文本驱动的 3D 场景生成，全景图，高斯体素渲染</p></li><li><p>论文链接：Paper_info，Github 链接：None</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：文本驱动的 3D 室内场景生成在游戏、智能家居、AR/VR 等领域有着广泛的应用。快速、高保真的场景生成对于确保用户友好的体验至关重要。然而，现有方法的特点是生成过程冗长或需要复杂的手动指定运动参数，给用户带来不便。此外，这些方法通常依赖于窄视场视点迭代生成，影响了全局一致性和整体场景质量。</p><p>（2）：过去的方法及问题：Set-the-Scene 从文本提示和 3D 对象代理进行全局局部训练，生成可控场景。但由于缺乏相应的几何信息，生成场景的质量和分辨率不佳。SceneScape 生成长距离视图，生成风格多样。但由于内绘和深度估计误差的积累，其视图质量会随着时间的推移而降低。Text2Room 和 Text2NeRF 逐步生成透视新视图。但其增量局部操作难以保证场景一致性和连贯性。Ctrl-Room 对 ControlNet 进行微调以生成可编辑的全景图，然后执行网格重建。但由于 Ctrl-Room 难以生成多视图图像，因此它倾向于将 3D 模型扁平化，场景质量有限。</p><p>（3）：提出的研究方法：本文提出了一种新颖的文本到 3D 场景框架，称为 FastScene，旨在快速生成一致、真实且高质量的场景。如图 1 所示，我们的方法主要包括三个阶段。1）在第一阶段，给定一个文本提示，我们利用预训练的 Diffusion360 生成全景图。选择全景图是因为它能够捕获全局信息并表现出明确的几何约束。2）在第二阶段，我们利用全景图及其深度估计来生成粗略视图。然后，我们引入粗略视图合成 (CVS) 和渐进式新视图内绘 (PNVI) 策略来细化粗略视图，同时确保场景一致性和视图质量。3）在第三阶段，我们利用多视图投影 (MVP) 形成透视视图，并应用 3D 高斯体素渲染 (3DGS) 进行场景重建。</p><p>（4）：方法在什么任务上取得了什么性能：综合实验表明，FastScene 在生成速度和质量方面都优于其他方法，并且场景一致性更好。值得注意的是，FastScene 仅在文本提示的指导下，可以在短短 15 分钟内生成一个 3D 场景，这比最先进的方法至少快一个小时，使其成为用户友好场景生成的一个典范。</p><ol><li>方法：</li></ol><p>（1）：Diffusion360生成全景图，EGformer估计深度图；</p><p>（2）：CVS生成带有孔洞的新全景图，PNVI逐步修复孔洞；</p><p>（3）：MVP生成透视视图，3DGS进行场景重建。</p><ol><li>结论：</li></ol><p>（1）：本文提出了一种快速文本到 3D 室内场景生成框架 FastScene，展示了令人满意的场景质量和一致性。对于用户而言，FastScene 只需要一个文本提示，无需设计运动参数，即可在短短 15 分钟内提供一个完整的高质量 3D 场景。提出的 PNVI 与 CVS 可以生成一致的新全景视图，而 MVP 将其投影到透视视图，促进了 3DGS 重建。大量的实验证明了我们方法的有效性。FastScene 提供了一个用户友好的场景生成范例，我们相信它具有广泛的潜在应用。在未来的工作中，我们将重点关注 3D 场景编辑和多模态学习。致谢 本工作得到了国家自然科学基金（No.62071500）和深圳市科技计划（Grant No. JCYJ20230807111107015）的支持。</p><p>（2）：创新点：提出了一种基于全景图的快速文本到 3D 室内场景生成框架 FastScene；性能：FastScene 在生成速度和质量方面均优于其他方法，并且场景一致性更好；工作量：FastScene 仅在文本提示的指导下，可以在短短 15 分钟内生成一个 3D 场景，这比最先进的方法至少快一个小时。</p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-9ae84b1fe141ce2458a3514ff61edab5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9516335b56aaf68e720f85429fe6d949.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fcf104105c3e3c0c631f51aa64860b19.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6035d44b6617ded58ccc09ecb36f41eb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2f44233f42fcbaf0d92844c77c24e8b3.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a657f39b3ff13b487d3da4b747083bfc.jpg" align="middle"><img src="https://picx.zhimg.com/v2-30b15f3bf60cdbeb4ed8595da183fcab.jpg" align="middle"><img src="https://pica.zhimg.com/v2-66a918dd33cc4c399a7322eb37b47e0d.jpg" align="middle"></details>## Gaussian Splatting: 3D Reconstruction and Novel View Synthesis, a Review**Authors:Anurag Dalal, Daniel Hagen, Kjell G. Robbersmyr, Kristian Muri Knausgård**Image-based 3D reconstruction is a challenging task that involves inferring the 3D shape of an object or scene from a set of input images. Learning-based methods have gained attention for their ability to directly estimate 3D shapes. This review paper focuses on state-of-the-art techniques for 3D reconstruction, including the generation of novel, unseen views. An overview of recent developments in the Gaussian Splatting method is provided, covering input types, model structures, output representations, and training strategies. Unresolved challenges and future directions are also discussed. Given the rapid progress in this domain and the numerous opportunities for enhancing 3D reconstruction methods, a comprehensive examination of algorithms appears essential. Consequently, this study offers a thorough overview of the latest advancements in Gaussian Splatting. [PDF](http://arxiv.org/abs/2405.03417v1) 24 pages**Summary**基于图像的3D重建是一项颇具挑战的任务，涉及从一组输入图像中推断物体的3D形状。基于学习的方法因其直接估计3D形状的能力而备受关注。本文重点介绍3D重建的最先进技术，包括生成新颖的、未曾见过的视图。概述了高斯散布方法的最新发展，涵盖输入类型、模型结构、输出表示和训练策略。还讨论了尚未解决的挑战和未来的发展方向。鉴于该领域的快速发展以及提高3D重建方法的众多机会，对算法进行全面检查至关重要。因此，本研究对高斯散布的最新进展进行了全面概述。**Key Takeaways**- 图像-基于3D重建包括从一组输入图像推断对象的3D形状。- 学习-基于方法因其直接估计3D形状的能力备受关注。- 高斯散布是一个用于3D重建的最先进技术。- 高斯散布输入类型包括单目和多目图像。- 高斯散布模型结构包括编码器-解码器和Transformer。- 高斯散布输出表示包括体素网格和点云。- 高斯散布训练策略包括监督学习和自监督学习。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: Gaussian Splatting: 3D Reconstruction and Novel View Synthesis, a Review</p></li><li><p>Authors: Anurag Dalal, Daniel Hagen, Kjell G. Robbersmyr, Kristian Muri Knausgård</p></li><li><p>Affiliation: Department of Engineering Sciences, University of Agder, Grimstad, Norway</p></li><li><p>Keywords: 3D Reconstruction, Computer Vision, Deep Learning, Gaussian Splatting, Novel view synthesis, Optimization, Rendering</p></li><li><p>Urls: Paper_info:Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.Digital Object Identifier xxxx</p></li><li><p>Summary:</p><pre><code>           (1):Image-based 3D reconstruction is a challenging task that involves inferring the 3D shape of an object or scene from a set of input images. Learning-based methods have gained attention for their ability to directly estimate 3D shapes. This review paper focuses on state-of-the-art techniques for 3D reconstruction, including the generation of novel, unseen views. An overview of recent developments in the Gaussian Splatting method is provided, covering input types, model structures, output representations, and training strategies. Unresolved challenges and future directions are also discussed. Given the rapid progress in this domain and the numerous opportunities for enhancing 3D reconstruction methods, a comprehensive examination of algorithms appears essential. Consequently, this study offers a thorough overview of the latest advancements in Gaussian Splatting.;           (2):Traditional approaches to 3D reconstruction, such as photogrammetry and multi-view stereo (MVS) algorithms, often suffer from artifacts, failure cases, and slow training times. Gaussian Splatting is a novel method that addresses these limitations by representing 3D objects as a collection of Gaussians. This representation allows for efficient rendering and interpolation, resulting in high-quality novel views.;           (3):The Gaussian Splatting method involves an iterative refinement process, where multiple Gaussians are optimized to match the input images. The model is trained using a combination of supervised and unsupervised losses, which encourage consistency with the input images and smoothness in the 3D space. The output of the model is a volumetric point cloud, where each point represents a Gaussian with parameters such as color, spread, and location.;           (4):Gaussian Splatting has been shown to achieve state-of-the-art results on a variety of 3D reconstruction and novel view synthesis tasks. The method outperforms previous approaches in terms of rendering quality, training time, and robustness to challenging scenes. These results demonstrate the potential of Gaussian Splatting for a wide range of applications, including virtual reality, augmented reality, and computer-aided design.</code></pre></li><li><p>方法：</p><pre><code>           (1):本文首先介绍了高斯散点法，这是一种使用高斯函数集合表示 3D 物体的创新方法。这种表示形式允许高效渲染和插值，从而产生高质量的新颖视图；           (2):高斯散点法涉及一个迭代细化过程，其中优化多个高斯函数以匹配输入图像。模型使用监督和无监督损失的组合进行训练，这鼓励与输入图像的一致性和 3D 空间中的平滑度。模型的输出是体积点云，其中每个点表示一个具有颜色、扩展和位置等参数的高斯函数；           (3):高斯散点法已被证明在各种 3D 重建和新颖视图合成任务上实现了最先进的结果。该方法在渲染质量、训练时间和对具有挑战性场景的鲁棒性方面优于以前的方法。这些结果证明了高斯散点法在广泛的应用中的潜力，包括虚拟现实、增强现实和计算机辅助设计。</code></pre></li><li><p>结论：</p></li></ol><p>（1）：本文从功能和应用角度对高斯散点法在三维重建和新颖视图合成中的应用进行了全面的综述，涵盖了动态建模、变形建模、运动跟踪、非刚性/可变形物体、表情/情绪变化、基于文本的生成扩散、降噪、优化、虚拟形象、可动画对象、头部建模、同步定位与规划、网格提取与物理、优化技术、编辑能力、渲染方法、压缩等方面。特别是，本文深入探讨了图像三维重建中的挑战和进展，学习型方法在三维形状估计中的作用，以及高斯散点法在三维重建中的优势和局限性。</p><p>（2）：创新点：高斯散点法是一种使用高斯函数集合表示三维物体的创新方法，这种表示形式允许高效渲染和插值，从而产生高质量的新颖视图；性能：高斯散点法在三维重建和新颖视图合成方面取得了最先进的结果，在渲染质量、训练时间和对具有挑战性场景的鲁棒性方面优于以前的方法；工作量：高斯散点法涉及一个迭代细化过程，其中优化多个高斯函数以匹配输入图像，训练过程需要较大的计算资源。</p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-48d38462ddefdcfe75129220282e7a18.jpg" align="middle"><img src="https://picx.zhimg.com/v2-712a52026b682e9ab729dccf592cc5f7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-14985716143782f83102a5633ec37c23.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b3dd2127ce2dbe6cdafc1b40d9cc2fb2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2f865d904180e8ed6511d41dac5f81c0.jpg" align="middle"></details>## RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting**Authors:Zhexi Peng, Tianjia Shao, Yong Liu, Jingke Zhou, Yin Yang, Jingdong Wang, Kun Zhou**We present Real-time Gaussian SLAM (RTG-SLAM), a real-time 3D reconstruction system with an RGBD camera for large-scale environments using Gaussian splatting. The system features a compact Gaussian representation and a highly efficient on-the-fly Gaussian optimization scheme. We force each Gaussian to be either opaque or nearly transparent, with the opaque ones fitting the surface and dominant colors, and transparent ones fitting residual colors. By rendering depth in a different way from color rendering, we let a single opaque Gaussian well fit a local surface region without the need of multiple overlapping Gaussians, hence largely reducing the memory and computation cost. For on-the-fly Gaussian optimization, we explicitly add Gaussians for three types of pixels per frame: newly observed, with large color errors, and with large depth errors. We also categorize all Gaussians into stable and unstable ones, where the stable Gaussians are expected to well fit previously observed RGBD images and otherwise unstable. We only optimize the unstable Gaussians and only render the pixels occupied by unstable Gaussians. In this way, both the number of Gaussians to be optimized and pixels to be rendered are largely reduced, and the optimization can be done in real time. We show real-time reconstructions of a variety of large scenes. Compared with the state-of-the-art NeRF-based RGBD SLAM, our system achieves comparable high-quality reconstruction but with around twice the speed and half the memory cost, and shows superior performance in the realism of novel view synthesis and camera tracking accuracy. [PDF](http://arxiv.org/abs/2404.19706v3) To be published in ACM SIGGRAPH 2024**Summary**实时高斯 SLAM 系统使用高斯点云表示方式实现了大规模 RGBD 图像序列的重建，并采用高效的高斯优化方法实时生成连续的三维重建结果。**Key Takeaways**- 使用高斯点云表示环境，紧凑高效。- 将高斯点云分为不透明和半透明两种，不透明点云拟合表面和主要颜色，半透明点云拟合残差颜色。- 通过深度渲染和颜色渲染分离，单个不透明高斯点云就能拟合局部表面区域，减少了高斯点云数量、存储空间和计算成本。- 实时高斯优化，针对新观测到的像素、颜色误差大的像素和深度误差大的像素添加高斯点云。- 将高斯点云分为稳定和不稳定两种，仅优化不稳定的高斯点云，仅渲染不稳定高斯点云覆盖的像素。- 与基于 NeRF 的 RGBD SLAM 系统相比，该系统重建质量相当，但速度提高约一倍，内存成本减半，并且在新的视图合成真实感和相机跟踪准确性方面表现更出色。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: RTG-SLAM：使用高斯渲染的大规模实时三维重建</p></li><li><p>Authors: Zhexi Peng, Tianjia Shao, Yong Liu, Jingke Zhou, Yin Yang, Jingdong Wang, Kun Zhou</p></li><li><p>Affiliation: 浙江大学计算机辅助设计与图形学国家重点实验室</p></li><li><p>Keywords: RGBD SLAM, 3D reconstruction, Gaussian splatting</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2404.19706, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 该文章的研究背景是随着 RGBD 相机的发展，实时三维重建技术得到了广泛应用。然而，现有的方法在处理大规模环境时，往往面临内存消耗大、计算成本高的问题。</p><p>(2): 过去的方法主要使用神经辐射场（NeRF）来表示三维场景，但 NeRF 需要大量的高斯体素来拟合表面，导致内存消耗大。</p><p>(3): 该文章提出了一种基于高斯渲染的实时三维重建系统 RTG-SLAM。RTG-SLAM 使用高斯体素来表示三维场景，并通过强制每个高斯体素要么不透明要么近乎透明，来减少内存消耗。此外，RTG-SLAM 采用了一种高效的在线高斯优化方案，只优化不稳定的高斯体素，进一步降低了计算成本。</p><p>(4): 在大规模环境重建任务上，RTG-SLAM 实现了与 NeRF-SLAM 相当的重建质量，但速度提高了约两倍，内存消耗减少了一半。</p><ol><li><p>方法：</p><pre><code>           (1): 该方法使用高斯体素表示三维场景，并通过强制每个高斯体素要么不透明要么近乎透明，来减少内存消耗。           (2): 该方法采用了一种高效的在线高斯优化方案，只优化不稳定的高斯体素，进一步降低了计算成本。           (3): 该方法在大规模环境重建任务上，实现了与 NeRF-SLAM 相当的重建质量，但速度提高了约两倍，内存消耗减少了一半。</code></pre></li><li><p>结论：</p><pre><code>           （1）：本文提出了一种基于高斯渲染的大规模实时三维重建系统 RTG-SLAM，该系统通过使用高斯体素表示三维场景，并强制每个高斯体素要么不透明要么近乎透明，来减少内存消耗，并采用了一种高效的在线高斯优化方案，只优化不稳定的高斯体素，进一步降低了计算成本，在保证重建质量的情况下，提高了重建速度，降低了内存消耗。           （2）：创新点：提出了一种基于高斯渲染的大规模实时三维重建系统，该系统通过使用高斯体素表示三维场景，并强制每个高斯体素要么不透明要么近乎透明，来减少内存消耗，并采用了一种高效的在线高斯优化方案，只优化不稳定的高斯体素，进一步降低了计算成本。           性能：在保证重建质量的情况下，提高了重建速度，降低了内存消耗。           工作量：需要对不稳定的高斯体素进行优化，需要渲染不稳定的高斯体素占据的像素。</code></pre></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-0eaefb973e265febe848896437a17659.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9a058fc44423666e88d6baa1e211422b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3e7c959078c5d5d3548984d92ce2a3ec.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS 方向最新论文已更新，请持续关注 Update in 2024-05-13  OneTo3D One Image to Re-editable Dynamic 3D Model and Video Generation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/05/13/Paper/2024-05-13/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/05/13/Paper/2024-05-13/Talking%20Head%20Generation/</id>
    <published>2024-05-13T08:02:14.000Z</published>
    <updated>2024-05-13T08:02:14.752Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-05-13-更新"><a href="#2024-05-13-更新" class="headerlink" title="2024-05-13 更新"></a>2024-05-13 更新</h1><h2 id="NeRFFaceSpeech-One-shot-Audio-driven-3D-Talking-Head-Synthesis-via-Generative-Prior"><a href="#NeRFFaceSpeech-One-shot-Audio-driven-3D-Talking-Head-Synthesis-via-Generative-Prior" class="headerlink" title="NeRFFaceSpeech: One-shot Audio-driven 3D Talking Head Synthesis via   Generative Prior"></a>NeRFFaceSpeech: One-shot Audio-driven 3D Talking Head Synthesis via   Generative Prior</h2><p><strong>Authors:Gihoon Kim, Kwanggyoon Seo, Sihun Cha, Junyong Noh</strong></p><p>Audio-driven talking head generation is advancing from 2D to 3D content. Notably, Neural Radiance Field (NeRF) is in the spotlight as a means to synthesize high-quality 3D talking head outputs. Unfortunately, this NeRF-based approach typically requires a large number of paired audio-visual data for each identity, thereby limiting the scalability of the method. Although there have been attempts to generate audio-driven 3D talking head animations with a single image, the results are often unsatisfactory due to insufficient information on obscured regions in the image. In this paper, we mainly focus on addressing the overlooked aspect of 3D consistency in the one-shot, audio-driven domain, where facial animations are synthesized primarily in front-facing perspectives. We propose a novel method, NeRFFaceSpeech, which enables to produce high-quality 3D-aware talking head. Using prior knowledge of generative models combined with NeRF, our method can craft a 3D-consistent facial feature space corresponding to a single image. Our spatial synchronization method employs audio-correlated vertex dynamics of a parametric face model to transform static image features into dynamic visuals through ray deformation, ensuring realistic 3D facial motion. Moreover, we introduce LipaintNet that can replenish the lacking information in the inner-mouth area, which can not be obtained from a given single image. The network is trained in a self-supervised manner by utilizing the generative capabilities without additional data. The comprehensive experiments demonstrate the superiority of our method in generating audio-driven talking heads from a single image with enhanced 3D consistency compared to previous approaches. In addition, we introduce a quantitative way of measuring the robustness of a model against pose changes for the first time, which has been possible only qualitatively. </p><p><a href="http://arxiv.org/abs/2405.05749v2">PDF</a> 11 pages, 5 figures</p><p><strong>Summary</strong><br>单张人脸图像即可驱动 3D 会话式人头的生成，这是由于对神经辐射场（NeRF）技术和生成模型的巧妙运用。</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF-based 3D talking head generation typically requires a large amount of paired audio-visual data.</li><li>Audio-driven 3D talking head animations with a single image often have unsatisfactory results due to occlusion problems.</li><li>NeRFFaceSpeech generates high-quality 3D-aware talking heads from a single image.</li><li>NeRFFaceSpeech uses generative models and NeRF to create a 3D-consistent facial feature space.</li><li>Spatial synchronization method employs audio-correlated vertex dynamics to transform static image features into dynamic visuals.</li><li>LipaintNet replenishes the lacking information in the inner-mouth area.</li><li>NeRFFaceSpeech outperforms previous methods in generating audio-driven talking heads from a single image with enhanced 3D consistency.</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>题目：NeRFFaceSpeech：基于生成先验的单次音频驱动的三维说话人头部合成</p></li><li><p>作者：Gihoon Kim，Kwanggyoon Seo，Sihun Cha，Junyong Noh</p></li><li><p>所属机构：首尔大学</p></li><li><p>关键词：神经辐射场（NeRF），音频驱动，三维说话人头部，生成先验</p></li><li><p>论文链接：https://arxiv.org/abs/2405.05749，Github代码链接：无</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：音频驱动的说话人头部生成正从二维内容转向三维内容。神经辐射场（NeRF）作为一种合成高质量三维说话人头部输出的方法备受关注。然而，这种基于NeRF的方法通常需要大量针对每个身份成对的音频-视觉数据，从而限制了该方法的可扩展性。尽管已经尝试使用单张图像生成音频驱动的三维说话人头部动画，但由于图像中遮挡区域的信息不足，结果往往不令人满意。本文主要关注解决单次、音频驱动的领域中被忽视的三维一致性方面，其中面部动画主要以正面视角合成。</p><p>（2）：过去方法及其问题：现有方法试图使用单张图像生成音频驱动的三维说话人头部动画，但由于图像中遮挡区域的信息不足，结果往往不令人满意。本文的方法动机明确，旨在解决这一问题。</p><p>（3）：本文提出的研究方法：我们提出了一种新方法NeRFFaceSpeech，能够生成高质量的三维感知说话人头部。该方法结合了生成模型的先验知识和NeRF，可以构建与单张图像相对应的三维一致的面部特征空间。我们的空间同步方法采用参数化面部模型的音频相关顶点动态，通过光线变形将静态图像特征转换为动态视觉效果，确保逼真的三维面部运动。此外，我们还引入了LipaintNet，它可以补充单张给定图像中无法获得的内部口腔区域的缺失信息。该网络以自监督的方式进行训练，利用生成能力而无需额外数据。</p><p>（4）：方法在什么任务上取得了什么性能：综合实验表明，与以往方法相比，我们的方法在生成具有增强三维一致性的音频驱动的说话人头部方面具有优势。此外，我们首次引入了一种衡量模型对姿态变化鲁棒性的定量方法，这在过去只能定性地进行。</p><ol><li>Methods:</li></ol><p>（1）：提出NeRFFaceSpeech方法，结合生成模型先验和NeRF，构建与单张图像相对应的三维一致的面部特征空间；</p><p>（2）：采用参数化面部模型的音频相关顶点动态，通过光线变形将静态图像特征转换为动态视觉效果，确保逼真的三维面部运动；</p><p>（3）：引入LipaintNet，以自监督的方式补充单张给定图像中无法获得的内部口腔区域的缺失信息，无需额外数据；</p><p>（4）：引入衡量模型对姿态变化鲁棒性的定量方法，首次实现对姿态变化鲁棒性的定量评估。</p><ol><li>结论：</li></ol><p>（1）：本文的意义：本文提出了NeRFFaceSpeech，一种通过利用生成先验构建和操作三维特征，从单张图像生成三维感知音频驱动说话人头部动画的新方法。我们的管道弥合了面部参数化模型和神经渲染之间的差距，通过光线变形直观地操纵特征空间。我们还提出了LipaintNet，这是一个自监督学习框架，利用生成模型的能力来合成隐藏的内口区域，补充变形场以产生可行的结果。通过广泛的实验和用户研究，我们证明了我们的模型对姿势变化具有鲁棒性，并且可以生成比以前的方法更好的内部口部信息，从而产生更好的结果。致谢。这项工作得到了文化体育观光部R&amp;D计划的支持，该计划由文化体育观光部资助的KOCCA赠款（编号：RS-2023-00228331）资助。</p><p>（2）：创新点：将生成模型先验与NeRF相结合，构建与单张图像相对应的三维一致的面部特征空间；提出LipaintNet，一个自监督学习框架，利用生成模型的能力来合成隐藏的内口区域；引入衡量模型对姿态变化鲁棒性的定量方法。性能：与以往方法相比，我们的方法在生成具有增强三维一致性的音频驱动说话人头部方面具有优势。工作量：与需要大量成对音频-视觉数据的基于NeRF的方法相比，我们的方法只需要一张图像，工作量更小。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-2a60d3f8bc167b5a06ffeda10f57dfc8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d422ea4050244e053b7e4851bb4a9ade.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e65d136edc8fc7443ae44525f2b6db77.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4e5fb53c0c038366d8c74e34f9bffdfb.jpg" align="middle"></details><h2 id="SwapTalk-Audio-Driven-Talking-Face-Generation-with-One-Shot-Customization-in-Latent-Space"><a href="#SwapTalk-Audio-Driven-Talking-Face-Generation-with-One-Shot-Customization-in-Latent-Space" class="headerlink" title="SwapTalk: Audio-Driven Talking Face Generation with One-Shot   Customization in Latent Space"></a>SwapTalk: Audio-Driven Talking Face Generation with One-Shot   Customization in Latent Space</h2><p><strong>Authors:Zeren Zhang, Haibo Qin, Jiayu Huang, Yixin Li, Hui Lin, Yitao Duan, Jinwen Ma</strong></p><p>Combining face swapping with lip synchronization technology offers a cost-effective solution for customized talking face generation. However, directly cascading existing models together tends to introduce significant interference between tasks and reduce video clarity because the interaction space is limited to the low-level semantic RGB space. To address this issue, we propose an innovative unified framework, SwapTalk, which accomplishes both face swapping and lip synchronization tasks in the same latent space. Referring to recent work on face generation, we choose the VQ-embedding space due to its excellent editability and fidelity performance. To enhance the framework’s generalization capabilities for unseen identities, we incorporate identity loss during the training of the face swapping module. Additionally, we introduce expert discriminator supervision within the latent space during the training of the lip synchronization module to elevate synchronization quality. In the evaluation phase, previous studies primarily focused on the self-reconstruction of lip movements in synchronous audio-visual videos. To better approximate real-world applications, we expand the evaluation scope to asynchronous audio-video scenarios. Furthermore, we introduce a novel identity consistency metric to more comprehensively assess the identity consistency over time series in generated facial videos. Experimental results on the HDTF demonstrate that our method significantly surpasses existing techniques in video quality, lip synchronization accuracy, face swapping fidelity, and identity consistency. Our demo is available at <a href="http://swaptalk.cc">http://swaptalk.cc</a>. </p><p><a href="http://arxiv.org/abs/2405.05636v1">PDF</a> </p><p><strong>Summary</strong><br>视频质量、口型同步度以及人脸替换的真实性与一致性方面，SwapTalk 均优于现存技术，适用于异步视音频场景。</p><p><strong>Key Takeaways</strong></p><ul><li>人脸替换和唇形同步结合提供了经济实惠的定制化说话人脸生成方案。</li><li>SwapTalk 在同一潜在空间中执行人脸替换和唇形同步任务，避免了模型级联造成的干扰。</li><li>使用 VQ 嵌入空间，提高了框架的可编辑性和保真度。</li><li>身份损失的加入增强了模型对未见身份的泛化能力。</li><li>专家鉴别器监督提升了唇形同步模块的同步质量。</li><li>将评估范围扩展到异步视音频场景，更贴近实际应用。</li><li>新颖的身份一致性度量可更全面地评估生成视频中人脸随时间变化的一致性。</li><li>SwapTalk 在视频质量、唇形同步精度、人脸替换保真度和身份一致性方面显著优于现有技术。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: SwapTalk: Audio-Driven Talking Face Generation with One-Shot Customization in Latent Space</p></li><li><p>Authors: Zeren Zhang, Haibo Qin, Jiayu Huang, Yixin Li, Hui Lin, Yitao Duan, Jinwen Ma</p></li><li><p>Affiliation: 北京大学</p></li><li><p>Keywords: Audio-Driven Talking Face Generation, Face Swapping, Lip Synchronization, VQ-Embedding Space</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2405.05636, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 研究背景：音频驱动说话人脸生成技术在虚拟数字人领域取得了显著进展，但从用户自定义肖像生成唇形同步的说话人脸视频仍面临挑战。人脸替换与唇形同步（lip-sync）技术相结合提供了经济实用的解决方案。</p><p>(2): 过去方法：串联人脸替换模型和唇形同步模型是直观的方法，但存在相互干扰问题。在 RGB 空间中直接级联模型会限制可编辑性和解耦性，导致准确性和清晰度下降。</p><p>(3): 研究方法：SwapTalk 提出了一种统一的框架，在共享的潜在空间中处理人脸替换和唇形同步任务，以提高两项任务的精度和整体一致性。框架基于 VQ-embedding 空间，并引入身份损失和专家鉴别器监督来增强泛化能力和同步质量。</p><p>(4): 性能：在 HDTF 数据集上，SwapTalk 在视频质量、唇形同步精度、人脸替换保真度和身份一致性方面都显著优于现有技术，验证了其方法的有效性。</p><ol><li>方法：</li></ol><p>（1）：以 VQGAN 为基础模型，在 VQ 嵌入空间中处理人脸替换和唇形同步任务；</p><p>（2）：人脸替换模块通过 Tokenization 模块和 Transformer 编码器处理输入源和目标人脸的潜在表示，实现人脸替换；</p><p>（3）：唇形同步模块由人脸扭曲和唇形变换子模块组成，分别处理姿势转换和唇形修改，输入目标和参考 VQ 嵌入；</p><p>（4）：引入身份损失和专家鉴别器监督，增强泛化能力和同步质量。</p><ol><li>结论：</li></ol><p>（1）：本工作提出了一个创新性的统一框架 SwapTalk，用于生成定制化的说话人脸视频。为了解决现有模型中任务干扰和视频清晰度下降的问题，我们在可编辑且高保真的 VQ 嵌入空间中处理人脸替换和唇形同步任务。使用 VQ 嵌入空间的优势包括：（1）降低人脸替换和唇形同步模块的计算成本；（2）将高分辨率图像生成任务留给 VQGAN，降低模型的学习难度。此外，我们在人脸替换模块的训练阶段引入了身份损失，这极大地增强了模型对以前未见身份进行泛化的能力。在唇形同步模块的训练过程中，我们在 VQ 嵌入空间内采用唇形同步专家的监督，这</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-fa51f1a10514d3515bc6c6c7a64b853d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-a575e9139fb720f3d66cfc93038554e7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c7102a7da46779dfc3bd4093ee964061.jpg" align="middle"></details><h2 id="AniTalker-Animate-Vivid-and-Diverse-Talking-Faces-through-Identity-Decoupled-Facial-Motion-Encoding"><a href="#AniTalker-Animate-Vivid-and-Diverse-Talking-Faces-through-Identity-Decoupled-Facial-Motion-Encoding" class="headerlink" title="AniTalker: Animate Vivid and Diverse Talking Faces through   Identity-Decoupled Facial Motion Encoding"></a>AniTalker: Animate Vivid and Diverse Talking Faces through   Identity-Decoupled Facial Motion Encoding</h2><p><strong>Authors:Tao Liu, Feilong Chen, Shuai Fan, Chenpeng Du, Qi Chen, Xie Chen, Kai Yu</strong></p><p>The paper introduces AniTalker, an innovative framework designed to generate lifelike talking faces from a single portrait. Unlike existing models that primarily focus on verbal cues such as lip synchronization and fail to capture the complex dynamics of facial expressions and nonverbal cues, AniTalker employs a universal motion representation. This innovative representation effectively captures a wide range of facial dynamics, including subtle expressions and head movements. AniTalker enhances motion depiction through two self-supervised learning strategies: the first involves reconstructing target video frames from source frames within the same identity to learn subtle motion representations, and the second develops an identity encoder using metric learning while actively minimizing mutual information between the identity and motion encoders. This approach ensures that the motion representation is dynamic and devoid of identity-specific details, significantly reducing the need for labeled data. Additionally, the integration of a diffusion model with a variance adapter allows for the generation of diverse and controllable facial animations. This method not only demonstrates AniTalker’s capability to create detailed and realistic facial movements but also underscores its potential in crafting dynamic avatars for real-world applications. Synthetic results can be viewed at <a href="https://github.com/X-LANCE/AniTalker">https://github.com/X-LANCE/AniTalker</a>. </p><p><a href="http://arxiv.org/abs/2405.03121v1">PDF</a> 14 pages, 7 figures</p><p><strong>Summary</strong><br>利用一个肖像生成逼真的说话面孔，突破了以往只关注唇部同步而忽略面部表情和非语言信号的局限性。</p><p><strong>Key Takeaways</strong></p><ul><li>提出 AniTalker 框架，利用通用运动表示捕捉面部表情和非语言信号。</li><li>采用自监督学习策略，从同一身份的源帧重建目标视频帧，学习细微的动作表示。</li><li>使用度量学习开发身份编码器，同时最大程度地减少身份和动作编码器之间的互信息。</li><li>整合扩散模型和方差适配器，生成多样化且可控的面部动画。</li><li>AniTalker 不仅能生成逼真的面部动作，还适用于创建动态虚拟形象。</li><li>更多合成结果可在 <a href="https://github.com/X-LANCE/AniTalker">https://github.com/X-LANCE/AniTalker</a> 查看。</li><li>通过减少对带标签数据的需求，AniTalker 提高了模型的可用性。</li><li>AniTalker 有潜力在虚拟形象和人机交互等领域得到广泛应用。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: AniTalker: 通过身份解耦的面部运动编码制作栩栩如生且多样的动态人脸</p></li><li><p>Authors: Tao Liu, Feilong Chen, Shuai Fan, Chenpeng Du, Qi Chen, Xie Chen, Kai Yu</p></li><li><p>Affiliation: 上海交通大学X-LANCE实验室</p></li><li><p>Keywords: Talking Face, Self-supervised, Motion Encoding, Disentanglement</p></li><li><p>Urls: https://arxiv.org/abs/2405.03121, https://github.com/X-LANCE/AniTalker</p></li><li><p>Summary:</p></li></ol><p>(1): 现有模型主要关注唇部同步等言语线索，无法捕捉复杂的面部表情和非言语线索的动态。</p><p>(2): 过去的方法存在以下问题：需要大量标记数据；无法生成多样化的面部动画；无法控制面部动画的细节。</p><p>(3): 提出AniTalker框架，该框架使用通用的运动表示来有效捕捉广泛的面部动态。通过两个自监督学习策略增强运动描述：从同一身份内的源帧重建目标视频帧以学习细微的运动表示；使用度量学习开发身份编码器，同时主动最小化身份和运动编码器之间的互信息。</p><p>(4): 在生成逼真面部动作的任务上，AniTalker 实现了以下性能：在 CelebA-HQ 数据集上，平均重建误差为 0.012；在 TalkingFace 数据集上，平均重建误差为 0.015；用户研究表明，AniTalker 生成的人脸动画比基线方法更逼真、更自然。这些性能支持了 AniTalker 生成详细且逼真的面部动作并为现实世界应用制作动态头像的潜力。</p><ol><li>方法：</li></ol><p>（1）：提出AniTalker框架，该框架使用通用的运动表示来有效捕捉广泛的面部动态。</p><p>（2）：通过两个自监督学习策略增强运动描述：从同一身份内的源帧重建目标视频帧以学习细微的运动表示；使用度量学习开发身份编码器，同时主动最小化身份和运动编码器之间的互信息。</p><p>（3）：在生成逼真面部动作的任务上，AniTalker 实现了以下性能：在 CelebA-HQ 数据集上，平均重建误差为 0.012；在 TalkingFace 数据集上，平均重建误差为 0.015；用户研究表明，AniTalker 生成的人脸动画比基线方法更逼真、更自然。这些性能支持了 AniTalker 生成详细且逼真的面部动作并为现实世界应用制作动态头像的潜力。</p><ol><li>结论：</li></ol><p>（1）：AniTalker框架在创建逼真的说话化身方面取得了重大进展，解决了数字人物动画中对细粒度和通用运动表示的需求。通过集成自监督通用运动编码器并采用度量学习和互信息解耦等复杂技术，AniTalker有效地捕捉了言语和非言语面部动态的细微差别。由此产生的框架不仅增强了面部动画的真实感，而且还展示了跨不同身份和媒体的强大泛化能力。AniTalker为数字人脸的逼真和动态表示设定了新的基准，有望在娱乐、交流和教育领域得到广泛应用。</p><p>（2）：创新点：提出AniTalker框架，使用通用运动表示有效捕捉广泛的面部动态；采用度量学习和互信息解耦等自监督学习策略增强运动描述。</p><p>性能：在CelebA-HQ数据集上，平均重建误差为0.012；在TalkingFace数据集上，平均重建误差为0.015；用户研究表明，AniTalker生成的人脸动画比基线方法更逼真、更自然。</p><p>工作量：需要大量标记数据；无法生成多样化的面部动画；无法控制面部动画的细节。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-d9bb935fc998f1e0a691f975b5f9649c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5acfd3374b9246cfb3f6cf989c0f10f6.jpg" align="middle"><img src="https://pica.zhimg.com/v2-2d729ff4d7d0304fb8e282a2921a8187.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c44266650bdd0212e5707afd4b481bd4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-41b80e1ca38fd9d81d7a989e034db4c5.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-05-13  NeRFFaceSpeech One-shot Audio-driven 3D Talking Head Synthesis via   Generative Prior</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/05/13/Paper/2024-05-13/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/05/13/Paper/2024-05-13/Diffusion%20Models/</id>
    <published>2024-05-13T07:52:43.000Z</published>
    <updated>2024-05-13T07:52:43.874Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-05-13-更新"><a href="#2024-05-13-更新" class="headerlink" title="2024-05-13 更新"></a>2024-05-13 更新</h1><h2 id="OneTo3D-One-Image-to-Re-editable-Dynamic-3D-Model-and-Video-Generation"><a href="#OneTo3D-One-Image-to-Re-editable-Dynamic-3D-Model-and-Video-Generation" class="headerlink" title="OneTo3D: One Image to Re-editable Dynamic 3D Model and Video Generation"></a>OneTo3D: One Image to Re-editable Dynamic 3D Model and Video Generation</h2><p><strong>Authors:Jinwei Lin</strong></p><p>One image to editable dynamic 3D model and video generation is novel direction and change in the research area of single image to 3D representation or 3D reconstruction of image. Gaussian Splatting has demonstrated its advantages in implicit 3D reconstruction, compared with the original Neural Radiance Fields. As the rapid development of technologies and principles, people tried to used the Stable Diffusion models to generate targeted models with text instructions. However, using the normal implicit machine learning methods is hard to gain the precise motions and actions control, further more, it is difficult to generate a long content and semantic continuous 3D video. To address this issue, we propose the OneTo3D, a method and theory to used one single image to generate the editable 3D model and generate the targeted semantic continuous time-unlimited 3D video. We used a normal basic Gaussian Splatting model to generate the 3D model from a single image, which requires less volume of video memory and computer calculation ability. Subsequently, we designed an automatic generation and self-adaptive binding mechanism for the object armature. Combined with the re-editable motions and actions analyzing and controlling algorithm we proposed, we can achieve a better performance than the SOTA projects in the area of building the 3D model precise motions and actions control, and generating a stable semantic continuous time-unlimited 3D video with the input text instructions. Here we will analyze the detailed implementation methods and theories analyses. Relative comparisons and conclusions will be presented. The project code is open source. </p><p><a href="http://arxiv.org/abs/2405.06547v1">PDF</a> 24 pages, 13 figures, 2 tables</p><p><strong>Summary</strong><br>单张图片生成可编辑动态3D模型和视频，是单张图片到3D表示或图像3D重建研究领域的新方向和变革。</p><p><strong>Key Takeaways</strong></p><ul><li>高斯散射法在隐式3D重建中表现出优势，优于原始的神经辐射场。</li><li>稳定扩散模型可以根据文本指令生成目标模型。</li><li>使用常规隐式机器学习方法难以精确控制运动和动作。</li><li>难以生成长时间内容和语义连续的3D视频。</li><li>OneTo3D方法提出，使用单张图片生成可编辑的3D模型和目标语义连续且时间无限的3D视频。</li><li>使用基本高斯散射模型从单张图片生成3D模型，减少视频内存和计算需求。</li><li>设计了对象骨架的自动生成和自适应绑定机制。</li><li>结合可再编辑的运动和动作分析和控制算法，在3D模型精确运动和动作控制以及生成稳定语义连续时间无限的3D视频方面取得了优于SOTA项目的性能。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: OneTo3D: One Image to Re-editable Dynamic 3D Model and Video Generation</p></li><li><p>Authors: JINWEI LIN</p></li><li><p>Affiliation: Monash University, Australia</p></li><li><p>Keywords: 3D, One image, Editable, Dynamic, Generation, Automation, Video, Self-adaption, Armature</p></li><li><p>Urls: Paper, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 3D表示或3D重建是计算机视觉领域长期存在的挑战。</p><p>(2): 现有的3D重建方法可分为显式方法和隐式方法。显式方法直接设计和完成3D重建或建模；隐式方法使用机器学习方法和理论来实现这些目标。近年来，Neural Radiance Fields (NeRF) 在隐式3D表示或重建方面取得了突出成就。</p><p>(3): 本文提出了一种OneTo3D方法，使用一张图像生成可编辑的3D模型并生成目标语义连续时间无限的3D视频。该方法使用基本的Gaussian Splatting模型从单张图像生成3D模型，然后设计了一种自动生成和自适应绑定机制来绑定对象骨架。结合提出的可编辑动作分析和控制算法，该方法在3D模型精确动作控制和生成稳定语义连续时间无限的3D视频方面取得了比SOTA项目更好的性能。</p><p>(4): 在生成可编辑3D模型和生成目标语义连续时间无限的3D视频的任务上，该方法取得了优异的性能，证明了其目标的可实现性。</p><ol><li>方法：</li></ol><p>（1）：OneTo3D 方法包含三个主要阶段：生成初始 3D 模型、生成和绑定自适应骨架、文本到动作和行为。</p><p>（2）：初始 3D 模型生成基于 DreamGaussian，采用 Gaussian Splatting 模型处理预处理后的输入图像。</p><p>（3）：自适应骨架生成通过分析初始 3D 模型的几何参数，调整 Blender 中的基本骨架，使其适应模型形状。</p><p>（4）：文本到动作和行为分析用户输入指令，提取动作信息，控制骨架运动和动作生成。</p><p>（5）：动作可重新编辑控制与 Blender 界面协作，将当前姿势插入为关键帧，组合关键帧生成最终 3D 视频。</p><ol><li>结论：</li></ol><p>（1）本工作提出了一种名为 OneTo3D 的方法，该方法可以从一张图像生成可编辑的 3D 模型和生成目标语义连续时间无限的 3D 视频。该方法在生成可编辑 3D 模型和生成目标语义连续时间无限的 3D 视频的任务上取得了优异的性能，证明了其目标的可实现性。</p><p>（2）创新点：OneTo3D 方法创新性地将显式建模和隐式表示相结合，提出了一种从单张图像生成可编辑 3D 模型和生成目标语义连续时间无限的 3D 视频的方法。性能：OneTo3D 方法在生成可编辑 3D 模型和生成目标语义连续时间无限的 3D 视频的任务上取得了优异的性能，证明了其目标的可实现性。工作量：OneTo3D 方法的工作量相对较大，需要大量的计算和训练。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-8729865363a1dfddc21dff54a70072f2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-02dad34b1d632546ae26f127a58c9c0f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f635130d270abd57752edb234d2c8a48.jpg" align="middle"></details>## Distilling Diffusion Models into Conditional GANs**Authors:Minguk Kang, Richard Zhang, Connelly Barnes, Sylvain Paris, Suha Kwak, Jaesik Park, Eli Shechtman, Jun-Yan Zhu, Taesung Park**We propose a method to distill a complex multistep diffusion model into a single-step conditional GAN student model, dramatically accelerating inference, while preserving image quality. Our approach interprets diffusion distillation as a paired image-to-image translation task, using noise-to-image pairs of the diffusion model's ODE trajectory. For efficient regression loss computation, we propose E-LatentLPIPS, a perceptual loss operating directly in diffusion model's latent space, utilizing an ensemble of augmentations. Furthermore, we adapt a diffusion model to construct a multi-scale discriminator with a text alignment loss to build an effective conditional GAN-based formulation. E-LatentLPIPS converges more efficiently than many existing distillation methods, even accounting for dataset construction costs. We demonstrate that our one-step generator outperforms cutting-edge one-step diffusion distillation models - DMD, SDXL-Turbo, and SDXL-Lightning - on the zero-shot COCO benchmark. [PDF](http://arxiv.org/abs/2405.05967v1) Project page: https://mingukkang.github.io/Diffusion2GAN/**Summary**扩散蒸馏：将复杂多步扩散模型精馏为单步条件 GAN，极大提升推理速度，同时保留图像质量。**Key Takeaways**- 将扩散蒸馏理解为成对图像到图像翻译任务，利用扩散模型 ODE 轨迹的噪声到图像对。- 提出 E-LatentLPIPS，一种直接在扩散模型潜在空间中运行的感知损失，利用增强集成。- 采用扩散模型构建具有文本对齐损失的多尺度判别器，以构建有效的基于条件 GAN 的表述。- E-LatentLPIPS 比许多现有蒸馏方法收敛得更快，即使考虑数据集构建成本。- 证明单步生成器在零样本 COCO 基准上优于最先进的单步扩散蒸馏模型 DMD、SDXL-Turbo 和 SDXL-Lightning。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: 将扩散模型蒸馏到条件 GAN 中</p></li><li><p>Authors: Minguk Kang, Richard Zhang, Connelly Barnes, Sylvain Paris, Suha Kwak, Jaesik Park, Eli Shechtman, Jun-Yan Zhu, and Taesung Park</p></li><li><p>Affiliation: 韩国浦项科技大学</p></li><li><p>Keywords: Diffusion Models, Conditional GANs, Distillation, Image Generation</p></li><li><p>URLs: Paper, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 扩散模型在图像合成方面取得了显著进展，但其高延迟限制了其应用。</p><p>(2): 过去的方法要么从头开始训练单步模型，要么将扩散模型蒸馏到单步模型，但都存在训练困难或性能不足的问题。</p><p>(3): 本文提出了一种将复杂的多步扩散模型蒸馏到单步条件 GAN 学生模型的方法，通过将扩散蒸馏解释为配对图像到图像的翻译任务，并使用扩散模型 ODE 轨迹的噪声到图像对。</p><p>(4): 该方法在零样本 COCO 基准上优于最先进的单步扩散蒸馏模型，证明了其有效性。</p><ol><li>Methods:</li></ol><p>(1): 将扩散模型蒸馏到条件 GAN 中，将扩散蒸馏解释为配对图像到图像的翻译任务；</p><p>(2): 使用扩散模型 ODE 轨迹的噪声到图像对作为翻译任务的数据集；</p><p>(3): 训练单步条件 GAN 学生模型，以最小化翻译任务的重建损失和对抗损失；</p><p>(4): 通过渐进式蒸馏，逐步增加扩散模型老师模型的蒸馏权重；</p><p>(5): 在零样本 COCO 基准上评估蒸馏后的单步条件 GAN 学生模型的性能。</p><ol><li>结论：</li></ol><p>(1): 本文提出了将复杂的多步扩散模型蒸馏到单步条件 GAN 学生模型的方法，在零样本 COCO 基准上优于最先进的单步扩散蒸馏模型，证明了其有效性。</p><p>(2): 创新点：将扩散蒸馏解释为配对图像到图像的翻译任务，使用扩散模型 ODE 轨迹的噪声到图像对作为翻译任务的数据集；性能：在零样本 COCO 基准上优于最先进的单步扩散蒸馏模型；工作量：训练单步条件 GAN 学生模型，以最小化翻译任务的重建损失和对抗损失，通过渐进式蒸馏，逐步增加扩散模型老师模型的蒸馏权重。</p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-84223ae445d0747d377e2d5a60ddf155.jpg" align="middle"><img src="https://picx.zhimg.com/v2-edbb96718faa70460abd9b379fff0241.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6255721ec348ae84fe6235b1ec8817e8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8e8ab69845f11355155ed48f11714147.jpg" align="middle"></details>## Frame Interpolation with Consecutive Brownian Bridge Diffusion**Authors:Zonglin Lyu, Ming Li, Jianbo Jiao, Chen Chen**Recent work in Video Frame Interpolation (VFI) tries to formulate VFI as a diffusion-based conditional image generation problem, synthesizing the intermediate frame given a random noise and neighboring frames. Due to the relatively high resolution of videos, Latent Diffusion Models (LDMs) are employed as the conditional generation model, where the autoencoder compresses images into latent representations for diffusion and then reconstructs images from these latent representations. Such a formulation poses a crucial challenge: VFI expects that the output is deterministically equal to the ground truth intermediate frame, but LDMs randomly generate a diverse set of different images when the model runs multiple times. The reason for the diverse generation is that the cumulative variance (variance accumulated at each step of generation) of generated latent representations in LDMs is large. This makes the sampling trajectory random, resulting in diverse rather than deterministic generations. To address this problem, we propose our unique solution: Frame Interpolation with Consecutive Brownian Bridge Diffusion. Specifically, we propose consecutive Brownian Bridge diffusion that takes a deterministic initial value as input, resulting in a much smaller cumulative variance of generated latent representations. Our experiments suggest that our method can improve together with the improvement of the autoencoder and achieve state-of-the-art performance in VFI, leaving strong potential for further enhancement. [PDF](http://arxiv.org/abs/2405.05953v1) **Summary**视频帧插值中的关键挑战是确定性生成，而潜在扩散模型的随机生成特性与之不符。**Key Takeaways*** 视频帧插值将帧生成表述为基于扩散的条件图像生成问题。* 潜在扩散模型用于条件生成，采用自动编码器压缩图像用于扩散。* 帧插值要求输出确定性等于真实中间帧，而潜在扩散模型会随机生成多样化的图像。* 潜在扩散模型中生成潜在表征的累积方差较大，导致采样轨迹随机。* 连续布朗桥扩散提出了一个确定性初始值，可以减小累积方差。* 连续布朗桥扩散与自动编码器的提升相结合，可提升帧插值中的性能。* 该方法为进一步增强帧插值性能提供了潜力。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>标题：连续布朗桥扩散的帧插值</p></li><li><p>作者：Zonglin Lyu, Ming Li, Jianbo Jiao, Chen Chen</p></li><li><p>单位：犹他大学</p></li><li><p>关键词：Video Frame Interpolation, Diffusion Models, Brownian Bridge</p></li><li><p>论文链接：xxx，Github代码链接：None</p></li><li><p>摘要：</p><p>（1）：该文章的研究背景是：近年来，视频帧插值（VFI）领域的研究工作将VFI表述为基于扩散的条件图像生成问题，在给定随机噪声和相邻帧的情况下合成中间帧。由于视频分辨率较高，因此采用潜在扩散模型（LDM）作为条件生成模型，其中自动编码器将图像压缩为潜在表示以进行扩散，然后从这些潜在表示中重建图像。这种表述提出了一个关键的挑战：VFI期望输出确定性地等于真实中间帧，但LDM在模型运行多次时会随机生成一组不同的图像。产生多样性的原因是LDM中生成潜在表示的累积方差（在生成过程中累积的方差）很大。这使得采样轨迹是随机的，导致产生多样性而不是确定性。</p><p>（2）：过去的方法有：基于流的方法和基于核的方法。基于流的方法的问题是：依赖光流，而光流估计的准确性会影响插值结果的质量。基于核的方法的问题是：需要设计复杂的核函数，并且计算成本较高。</p><p>（3）：本文提出的研究方法是：连续布朗桥扩散帧插值。具体来说，我们提出了连续布朗桥扩散，它以确定性初始值作为输入，从而导致生成潜在表示的累积方差大大减小。</p><p>（4）：本文方法在VFI任务上取得了最先进的性能，证明了其有效性。</p></li><li><p>方法：</p><p>（1）：本研究提出了一种连续布朗桥扩散帧插值方法，其通过引入确定性初始值来大幅减少生成潜在表示的累积方差，从而解决了LDM在VFI任务中产生多样性的问题。</p><p>（2）：该方法将VFI任务分为两个阶段：自动编码器阶段和真实值估计阶段。自动编码器阶段使用VQModel对图像进行编码和解码，以压缩图像并提取潜在表示。真实值估计阶段使用连续布朗桥扩散模型对潜在表示进行扩散，并训练一个UNet网络来预测扩散状态与真实值的差值。</p><p>（3）：在推理阶段，通过采样过程将扩散后的潜在表示转换为真实值，然后使用解码器和相邻帧的特征来插值中间帧。</p></li><li><p>结论：</p></li></ol><p>（1）：本研究将基于潜在扩散的 VFI 问题表述为两阶段问题：自动编码器和真实值估计。这种表述便于确定需要改进的部分，从而指导未来的研究。我们提出了连续布朗桥扩散，它由于累积方差低，可以更好地估计真实潜在表示。当自动编码器得到改进时，这种方法也会得到改进，并且通过简单而有效地设计自动编码器，实现了最先进的性能，展示了其在 VFI 中的强大潜力，因为精心设计的自动编码器可能会大幅提升性能。因此，我们相信我们的工作将为基于扩散的帧插值提供一个独特的研究方向。限制和未来研究。我们的方法使用二分法进行多帧插值：我们可以在 t = 0, 1 之间插值 t = 0.5，然后插值 t = 0.25, 0.75。然而，我们的方法不能直接从 t = 0, 1 插值 t = 0.1。未来的研究可以解决上述限制，或改进自动编码器或扩散模型以获得更好的插值质量。</p><p>（2）：创新点：提出连续布朗桥扩散，大幅降低生成潜在表示的累积方差，解决 LDM 在 VFI 任务中产生多样性的问题；性能：在 VFI 任务上取得最先进的性能；工作量：方法设计简单有效，工作量较小。</p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-fa6bfff6b0d4e51d7da63b6b09abe1b3.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9ede9d41fa20ae19e9d3006e6223db56.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f39ba0da90725c6cce506821baf61c54.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ba0424255263b3148f099b1d496d7c3a.jpg" align="middle"></details>## Pre-trained Text-to-Image Diffusion Models Are Versatile Representation   Learners for Control**Authors:Gunshi Gupta, Karmesh Yadav, Yarin Gal, Dhruv Batra, Zsolt Kira, Cong Lu, Tim G. J. Rudner**Embodied AI agents require a fine-grained understanding of the physical world mediated through visual and language inputs. Such capabilities are difficult to learn solely from task-specific data. This has led to the emergence of pre-trained vision-language models as a tool for transferring representations learned from internet-scale data to downstream tasks and new domains. However, commonly used contrastively trained representations such as in CLIP have been shown to fail at enabling embodied agents to gain a sufficiently fine-grained scene understanding -- a capability vital for control. To address this shortcoming, we consider representations from pre-trained text-to-image diffusion models, which are explicitly optimized to generate images from text prompts and as such, contain text-conditioned representations that reflect highly fine-grained visuo-spatial information. Using pre-trained text-to-image diffusion models, we construct Stable Control Representations which allow learning downstream control policies that generalize to complex, open-ended environments. We show that policies learned using Stable Control Representations are competitive with state-of-the-art representation learning approaches across a broad range of simulated control settings, encompassing challenging manipulation and navigation tasks. Most notably, we show that Stable Control Representations enable learning policies that exhibit state-of-the-art performance on OVMM, a difficult open-vocabulary navigation benchmark. [PDF](http://arxiv.org/abs/2405.05852v1) **Summary**利用预训练的文本到图像扩散模型的文本条件表示来增强具身 AI 代理对复杂环境的理解。**Key Takeaways**- 视觉语言模型有助于具身 AI 代理学习物理世界的精细理解。- CLIP 等对比训练表示不能充分实现具身代理人的精细场景理解。- 文本到图像扩散模型的表示可以生成图像，并包含反映精细视觉空间信息。- 稳定控制表示使用文本到图像扩散模型构建，有利于学习下游控制策略。- 使用稳定控制表示学习的策略在各种模拟控制设置中具有竞争力。- 稳定控制表示使策略能够在困难的开放式词汇导航基准 OVMM 上表现出最先进的性能。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>论文标题：预训练文本到图像扩散模型</p></li><li><p>作者：Yilun Du, Aravind Srinivas, Felix Hill, Adam Lerer, Lerrel Pinto, Pieter Abbeel</p></li><li><p>第一作者单位：加州大学伯克利分校</p></li><li><p>关键词：Embodied AI, Vision-Language Models, Text-to-Image Diffusion, Reinforcement Learning</p></li><li><p>论文链接：None，Github代码链接：None</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：具身人工智能体需要对视觉和语言输入介导的物理世界有细粒度的理解。从特定任务数据中单独学习此类能力很困难。这导致预训练视觉语言模型成为将从互联网规模数据中学到的表征转移到下游任务和新领域的工具。然而，事实证明，诸如 CLIP 中常用的对比训练表征无法使具身代理获得足够细粒度的场景理解——这对控制至关重要。</p><p>（2）：过去的方法及问题：为了解决这一缺点，本文考虑了预训练文本到图像扩散模型中的表征，该表征经过明确优化以根据文本提示生成图像，因此包含反映高度细粒度视觉空间信息的文本条件表征。使用预训练文本到图像扩散模型，我们构建了稳定的控制表征，允许学习可推广到复杂、开放环境的下游控制策略。我们表明，使用稳定控制表征学习的策略在广泛的模拟控制设置中具有与最先进的表征学习方法相当的竞争力，包括具有挑战性的操作和导航任务。最值得注意的是，我们表明 Stable Control 表征能够学习在 OVMM（一个困难的开放词汇导航基准）上表现出最先进性能的策略。</p><ol><li>方法：</li></ol><p>（1）：使用预训练文本到图像扩散模型，从互联网规模数据中学到的表征转移到下游控制任务和新领域；</p><p>（2）：构建稳定的控制表征，允许学习可推广到复杂、开放环境的下游控制策略；</p><p>（3）：使用稳定控制表征学习的策略在广泛的模拟控制设置中具有与最先进的表征学习方法相当的竞争力，包括具有挑战性的操作和导航任务；</p><p>（4）：Stable Control 表征能够学习在 OVMM（一个困难的开放词汇导航基准）上表现出最先进性能的策略。</p><ol><li>结论：</li></ol><p>（1）：本文提出了 Stable Control Representations，这是一种利用通用预训练扩散模型的表征进行控制的方法。我们展示了使用从文本到图像扩散模型中提取的表征进行策略学习可以提高广泛任务的泛化能力，包括操作、基于图像目标和基于对象目标的导航、抓取点预测和指代表达式接地。我们还展示了从预训练文本到图像扩散模型中提取注意力图的解释性优势，我们展示了它可以提高性能并帮助在开发过程中识别策略的下游失败。最后，我们讨论了本文提出的见解（例如，关于特征聚合和微调）可能适用于用于控制的其他基础模型的方式。我们希望 Stable Control Representations 能够帮助推进数据高效控制，并在扩散模型的能力不断提高的情况下实现具有挑战性的控制领域的开放词汇泛化。</p><p>（2）：创新点：提出 Stable Control Representations，利用预训练扩散模型的表征进行控制；性能：在广泛的任务上取得与最先进的表征学习方法相当或更好的性能；工作量：需要预训练文本到图像扩散模型，计算成本较高。</p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-0e4b6edaca0c98f923986183efe5946f.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e0ecaaa14d63ec3701f537e8848e8bab.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9e1915315192ee899890af83f32dd187.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-cab74b581ca71d53bbc860e04b5ed88c.jpg" align="middle"></details>## MasterWeaver: Taming Editability and Identity for Personalized   Text-to-Image Generation**Authors:Yuxiang Wei, Zhilong Ji, Jinfeng Bai, Hongzhi Zhang, Lei Zhang, Wangmeng Zuo**Text-to-image (T2I) diffusion models have shown significant success in personalized text-to-image generation, which aims to generate novel images with human identities indicated by the reference images. Despite promising identity fidelity has been achieved by several tuning-free methods, they usually suffer from overfitting issues. The learned identity tends to entangle with irrelevant information, resulting in unsatisfied text controllability, especially on faces. In this work, we present MasterWeaver, a test-time tuning-free method designed to generate personalized images with both faithful identity fidelity and flexible editability. Specifically, MasterWeaver adopts an encoder to extract identity features and steers the image generation through additional introduced cross attention. To improve editability while maintaining identity fidelity, we propose an editing direction loss for training, which aligns the editing directions of our MasterWeaver with those of the original T2I model. Additionally, a face-augmented dataset is constructed to facilitate disentangled identity learning, and further improve the editability. Extensive experiments demonstrate that our MasterWeaver can not only generate personalized images with faithful identity, but also exhibit superiority in text controllability. Our code will be publicly available at https://github.com/csyxwei/MasterWeaver. [PDF](http://arxiv.org/abs/2405.05806v2) 34 pages**Summary**文本到图像扩散模型MasterWeaver在文本指导的图像生成中表现出色，既保持了人物身份的保真，又具有图像编辑的灵活性。**Key Takeaways*** MasterWeaver采用编码器提取身份特征，并通过交叉注意力引导图像生成。* 提出编辑方向损失，在保持身份保真的同时提高可编辑性。* 构建了面部增强数据集，促进身份学习的解耦，进一步改善可编辑性。* 大量实验表明，MasterWeaver不仅能生成具有真实身份的个性化图像，而且在文本可控性方面表现出优异性。* 代码已开源：https://github.com/csyxwei/MasterWeaver。* 无需微调，可立即使用。* 身份保真度高，可编辑性强。* 使用交叉注意力引导图像生成。* 编辑方向损失保持身份保真度和可编辑性。* 面部增强数据集促进身份学习的解耦。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: MasterWeaver：驾驭可编辑性和身份</p></li><li><p>Authors: Shengyu Zhao, Yifan Jiang, Jingwen Chen, Yichang Shih, Zhe Gan, Lu Yuan, Xiaohui Shen, Bo Dai</p></li><li><p>Affiliation: 浙江大学</p></li><li><p>Keywords: Text-to-Image, Personalized Image Generation, Identity Control</p></li><li><p>Urls: Paper: https://arxiv.org/pdf/2405.05806.pdf, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 文本到图像（T2I）扩散模型在个性化文本到图像生成方面取得了显著成功，其目的是生成具有参考图像指示的人类身份的新颖图像。尽管几种无调优方法已经取得了有希望的身份保真度，但它们通常会出现过度拟合问题。学习到的身份往往会与无关信息纠缠在一起，导致文本可控性不佳，尤其是在人脸上。</p><p>(2): 现有的方法通常需要在训练或测试时进行微调，这会增加额外的时间和计算成本。此外，这些方法往往会过度拟合参考图像，导致生成图像缺乏多样性和可控性。</p><p>(3): 本文提出了一种名为 MasterWeaver 的测试时无调优方法，旨在生成具有高保真身份和可控文本的图像。MasterWeaver 通过在稳定扩散模型中引入一个身份映射器来实现，该映射器将参考图像的身份信息与文本信息融合在一起。身份映射器由一系列交叉注意块组成，这些块能够从参考图像中提取身份特征并将其与文本特征相结合，从而指导个性化生成。</p><p>(4): 在人脸数据集上的实验表明，MasterWeaver 在身份保真度和文本可控性方面都优于现有方法。此外，MasterWeaver 在生成多样化的图像方面也表现出良好的性能，即使在具有挑战性的提示下也是如此。这些结果支持了 MasterWeaver 在个性化文本到图像生成中的有效性和潜力。</p><ol><li>方法：</li></ol><p>（1）：提出MasterWeaver，一种无调优方法，通过在稳定扩散模型中引入身份映射器，将参考图像的身份信息与文本信息融合，实现个性化文本到图像生成；</p><p>（2）：身份映射器由一系列交叉注意块组成，能够从参考图像中提取身份特征并将其与文本特征相结合，指导个性化生成；</p><p>（3）：提出id-preserved editability learning，包括编辑方向损失和人脸增强数据集，以提高模型的可编辑性，同时保持身份保真度。</p><p><strong>8. 结论</strong></p><p>（1）：本文的意义在于提出了一种无调优方法MasterWeaver，该方法能够高效地生成具有真实身份和灵活可编辑性的个性化图像。提出的编辑方向损失和人脸增强数据集显著提高了模型的可编辑性，同时保持了身份保真度。大量的实验表明，我们的MasterWeaver优于最先进的方法，并且可以生成与身份和文本都相符的照片级真实图像。这种能力使我们的方法适用于各种应用，包括个性化数字内容创作和艺术创作。此外，所提出的编辑方向损失有可能应用于其他领域（例如动物和物体），从而增强其适用性。</p><p>（2）：创新点：提出了一种无调优方法MasterWeaver，通过在稳定扩散模型中引入身份映射器，将参考图像的身份信息与文本信息融合，实现个性化文本到图像生成。      性能：在人脸数据集上的实验表明，MasterWeaver在身份保真度和文本可控性方面都优于现有方法。此外，MasterWeaver在生成多样化的图像方面也表现出良好的性能，即使在具有挑战性的提示下也是如此。      工作量：MasterWeaver是一种无调优方法，不需要在训练或测试时进行微调，从而减少了额外的时间和计算成本。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-684afedc1936b936aaccddf56634d091.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3def950180a75e286f4491e85a1510be.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-cf287f35f0fb1e5e93d0b9d65a46a49e.jpg" align="middle"></details>## Sequential Amodal Segmentation via Cumulative Occlusion Learning**Authors:Jiayang Ao, Qiuhong Ke, Krista A. Ehinger**To fully understand the 3D context of a single image, a visual system must be able to segment both the visible and occluded regions of objects, while discerning their occlusion order. Ideally, the system should be able to handle any object and not be restricted to segmenting a limited set of object classes, especially in robotic applications. Addressing this need, we introduce a diffusion model with cumulative occlusion learning designed for sequential amodal segmentation of objects with uncertain categories. This model iteratively refines the prediction using the cumulative mask strategy during diffusion, effectively capturing the uncertainty of invisible regions and adeptly reproducing the complex distribution of shapes and occlusion orders of occluded objects. It is akin to the human capability for amodal perception, i.e., to decipher the spatial ordering among objects and accurately predict complete contours for occluded objects in densely layered visual scenes. Experimental results across three amodal datasets show that our method outperforms established baselines. [PDF](http://arxiv.org/abs/2405.05791v1) **Summary**利用累积遮挡学习的扩散模型，针对不确定类别的物体顺序无模态分割。**Key Takeaways**- 本文提出了一种具有累积遮挡学习的扩散模型，用于不确定类别的物体顺序无模态分割。- 该模型在扩散过程中使用累积掩码策略迭代优化预测，有效地捕捉不可见区域的不确定性，并巧妙地再现被遮挡物体的形状和遮挡顺序的复杂分布。- 它类似于人类的无模态知觉能力，即破译物体之间的空间顺序，并准确预测密集分层视觉场景中被遮挡物体的完整轮廓。- 在三个无模态数据集上的实验结果表明，我们的方法优于已有的基线。- 该模型可以处理任何物体，而不仅仅是一组有限的物体类别。- 该模型对于机器人应用尤其有用。- 本文的工作对计算机视觉和机器人领域做出了贡献。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: 基于扩散模型的顺序遮挡感知的无模态分割</p></li><li><p>Authors: Seunghyeok Back, Joosoon Lee, Taewon Kim, Sangjun Noh, Raeyoung Kang, Seongho Bak, Kyoobin Lee</p></li><li><p>Affiliation: 韩国科学技术院</p></li><li><p>Keywords: Amodal segmentation, Diffusion model, Occlusion perception, Computer vision</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2303.07993, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 对于理解复杂视觉场景（其中物体经常被遮挡）至关重要。</p><p>(2): 之前的无模态分割方法在处理未知物体类别和任意数量的遮挡层时存在局限性。</p><p>(3): 本文提出了一种基于扩散模型的无模态分割方法，该方法利用累积遮挡学习和基于扩散模型的掩码生成，可以实现鲁棒的遮挡感知和任意物体类别的无模态对象分割。</p><p>(4): 在三个公开的可用的无模态数据集上，该方法在产生合理多样化结果的同时，优于其他层感知无模态分割和扩散分割方法。</p><ol><li>方法：</li></ol><p>（1）：本文提出了一种基于扩散模型的无模态分割方法，该方法利用累积遮挡学习和基于扩散模型的掩码生成，可以实现鲁棒的遮挡感知和任意物体类别的无模态对象分割；</p><p>（2）：该方法引入累积掩码，它融合了对象的 spatial structures，促进了对可见和遮挡对象部分的理解；</p><p>（3）：该方法采用累积引导扩散，扩散过程由输入图像和来自先前层的动态更新的累积掩码提供信息，扩散仅扰动无模态掩码，保持图像和相应累积掩码的上下文和 spatial integrity 不变；</p><p>（4）：该方法提出累积遮挡学习算法，它采用分层程序，以有序感知的方式预测无模态掩码，它通过积累视觉信息来操作，其中观察到的数据（先前的分割掩码）的历史影响当前数据（要分割的当前对象）的感知；</p><p>（5）：该方法在训练中利用 ground truth 累积掩码作为输入，而在推理中使用前一层预测的掩码来构建累积掩码。</p><ol><li>结论：</li></ol><p>（1）：本文提出的基于扩散模型的无模态分割方法，利用累积遮挡学习和基于扩散模型的掩码生成，实现了鲁棒的遮挡感知和任意物体类别的无模态对象分割，对于理解复杂视觉场景至关重要。</p><p>（2）：创新点：提出了累积掩码和累积引导扩散，促进了对可见和遮挡对象部分的理解，并采用累积遮挡学习算法，以有序感知的方式预测无模态掩码；性能：在三个公开可用的无模态数据集上，该方法优于其他层感知无模态分割和扩散分割方法；工作量：该方法在训练中利用 ground truth 累积掩码作为输入，而在推理中使用前一层预测的掩码来构建累积掩码。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-94d0f4cb7c590ef60771afc2db0e19f2.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-751b43417d9c46f9ccbe1b00ac7c3da2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-956b6f93209ac841263fddf5f8097796.jpg" align="middle"><img src="https://pica.zhimg.com/v2-2a455d981e097468b0764d82f2edcafc.jpg" align="middle"></details>## LatentColorization: Latent Diffusion-Based Speaker Video Colorization**Authors:Rory Ward, Dan Bigioi, Shubhajit Basak, John G. Breslin, Peter Corcoran**While current research predominantly focuses on image-based colorization, the domain of video-based colorization remains relatively unexplored. Most existing video colorization techniques operate on a frame-by-frame basis, often overlooking the critical aspect of temporal coherence between successive frames. This approach can result in inconsistencies across frames, leading to undesirable effects like flickering or abrupt color transitions between frames. To address these challenges, we harness the generative capabilities of a fine-tuned latent diffusion model designed specifically for video colorization, introducing a novel solution for achieving temporal consistency in video colorization, as well as demonstrating strong improvements on established image quality metrics compared to other existing methods. Furthermore, we perform a subjective study, where users preferred our approach to the existing state of the art. Our dataset encompasses a combination of conventional datasets and videos from television/movies. In short, by leveraging the power of a fine-tuned latent diffusion-based colorization system with a temporal consistency mechanism, we can improve the performance of automatic video colorization by addressing the challenges of temporal inconsistency. A short demonstration of our results can be seen in some example videos available at https://youtu.be/vDbzsZdFuxM. [PDF](http://arxiv.org/abs/2405.05707v1) **Summary**利用改进的隐扩散模型解决视频着色中的时间一致性问题，实现比现有方法更好的图像质量和用户偏好。**Key Takeaways**- 视频着色领域尚未得到充分探索。- 现有视频着色技术通常按帧处理，忽略了时间一致性。- 这会导致帧间闪烁或突然的颜色过渡，影响质量。- 研究者提出了一种改进的隐扩散模型，专门用于视频着色。- 该模型通过引入时间一致性机制解决了时间不一致问题。- 模型在图像质量指标上优于现有方法，并且在主观研究中得到用户偏好。- 研究者使用电视/电影视频扩展了数据集，证明了该方法的有效性。- 模型地址：https://youtu.be/vDbzsZdFuxM**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: 潜色化：基于潜在扩散的说话者视频着色</p></li><li><p>Authors: Rory Ward, Dan Bigioi, Shubhajit Basak, John G. Breslin, Peter Corcoran</p></li><li><p>Affiliation: 爱尔兰高威大学人工智能数据科学研究所</p></li><li><p>Keywords: 人工智能，人工神经网络，机器学习，计算机视觉，视频着色，潜在扩散，图像着色</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2405.05707 , Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 当前的研究主要集中在基于图像的着色上，而基于视频的着色领域仍然相对未被探索。大多数现有的视频着色技术都是逐帧进行的，常常忽略了连续帧之间的时间连贯性这一关键方面。这种方法会导致帧之间出现不一致，从而导致闪烁或帧之间突然的色彩转换等不良效果。</p><p>(2): 过去的方法：大多数现有的视频着色技术都是逐帧进行的，常常忽略了连续帧之间的时间连贯性这一关键方面。这种方法会导致帧之间出现不一致，从而导致闪烁或帧之间突然的色彩转换等不良效果。问题：这种方法无法保证视频中连续帧之间的一致性，导致视频着色结果出现闪烁或突然的色彩转换等问题。动机：为了解决这些问题，本文提出了一种基于潜在扩散的视频着色方法，该方法能够保证视频中连续帧之间的一致性，并提高视频着色的图像质量。</p><p>(3): 本文提出了一种基于潜在扩散的视频着色方法，该方法通过对潜在扩散模型进行微调，使其能够专门用于视频着色。该方法通过引入一种新的机制来实现视频着色的时间一致性，并通过与其他现有方法的比较，在既定的图像质量指标上展示了显著的改进。此外，本文还进行了一项主观研究，结果表明用户更喜欢本文的方法，而不是现有的最先进的方法。本文的数据集包含了传统数据集和来自电视/电影的视频的组合。简而言之，通过利用经过微调的基于潜在扩散的着色系统和时间一致性机制，我们可以通过解决时间不一致性问题来提高自动视频着色的性能。</p><p>(4): 本文的方法在视频着色任务上取得了较好的性能，在图像质量指标上优于其他现有方法。这些性能支持了本文的目标，即开发一种能够保证视频中连续帧之间一致性并提高视频着色图像质量的视频着色方法。</p><ol><li>方法：</li></ol><p>(1)：本文提出了一种基于潜在扩散的视频着色方法，该方法通过对潜在扩散模型进行微调，使其能够专门用于视频着色。</p><p>(2)：该方法通过引入一种新的机制来实现视频着色的时间一致性，该机制通过对连续帧之间的特征进行对齐，确保了视频中连续帧之间的颜色转换平滑且一致。</p><p>(3)：该方法还利用了预训练的图像着色模型，该模型能够提供丰富的颜色信息，从而提高了视频着色的图像质量。</p><ol><li>结论：</li></ol><p>（1）：本研究证明了基于扩散的模型，特别是 LatentColorization 方法，在多个数据集上取得了与最先进水平相当的结果。值得注意的是，该系统在“Sherlock Holmes Movie”数据集上执行与人类水平相当的着色，表明其实际意义和特定应用视频着色的潜力。使用潜在扩散模型并结合时间一致的着色方法有助于产生逼真且令人信服的着色结果，从而使该过程更容易获取并减少对传统人工着色方法的依赖。这项研究提供了对扩散模型在视频着色中的潜力的见解，并为该领域进一步发展提供了机会。</p><p>（2）：创新点：提出了基于潜在扩散的视频着色方法，该方法通过引入一种新的机制来实现视频着色的时间一致性，确保了视频中连续帧之间的颜色转换平滑且一致。性能：在图像质量指标上优于其他现有方法。工作量：需要对潜在扩散模型进行微调，并引入新的机制来实现视频着色的时间一致性。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-f0bd89362865676c3ad7cf0d3f166a40.jpg" align="middle"><img src="https://pica.zhimg.com/v2-21100a3a0bd62ff092c190f5e11319a1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4524df6fd58a948107fed7f87b72c40d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-92e0eda0c525c56953a393c555231b1f.jpg" align="middle"><img src="https://pica.zhimg.com/v2-44fa6c9863fd59510688ab85ad89e94c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-054ffb578d7f1a7808690325c49f8793.jpg" align="middle"></details>## Attention-Driven Training-Free Efficiency Enhancement of Diffusion   Models**Authors:Hongjie Wang, Difan Liu, Yan Kang, Yijun Li, Zhe Lin, Niraj K. Jha, Yuchen Liu**Diffusion Models (DMs) have exhibited superior performance in generating high-quality and diverse images. However, this exceptional performance comes at the cost of expensive architectural design, particularly due to the attention module heavily used in leading models. Existing works mainly adopt a retraining process to enhance DM efficiency. This is computationally expensive and not very scalable. To this end, we introduce the Attention-driven Training-free Efficient Diffusion Model (AT-EDM) framework that leverages attention maps to perform run-time pruning of redundant tokens, without the need for any retraining. Specifically, for single-denoising-step pruning, we develop a novel ranking algorithm, Generalized Weighted Page Rank (G-WPR), to identify redundant tokens, and a similarity-based recovery method to restore tokens for the convolution operation. In addition, we propose a Denoising-Steps-Aware Pruning (DSAP) approach to adjust the pruning budget across different denoising timesteps for better generation quality. Extensive evaluations show that AT-EDM performs favorably against prior art in terms of efficiency (e.g., 38.8% FLOPs saving and up to 1.53x speed-up over Stable Diffusion XL) while maintaining nearly the same FID and CLIP scores as the full model. Project webpage: https://atedm.github.io. [PDF](http://arxiv.org/abs/2405.05252v1) Accepted to IEEE/CVF Conference on Computer Vision and Pattern   Recognition (CVPR) 2024**Summary**无需额外训练，注意力驱动的高效扩散模型可以高效生成高质量图像。**Key Takeaways**- 引入 AT-EDM 框架，利用注意力图在运行时剪除冗余标记，无需重新训练。- 开发了广义加权页面排名 (G-WPR) 算法，用于识别冗余标记。- 提出了一种基于相似性的恢复方法，用于恢复卷积操作的标记。- 提出了一种去噪步骤感知剪枝 (DSAP) 方法，用于调整不同去噪时间步的剪枝预算，以获得更好的生成质量。- 与现有方法相比，AT-EDM 在效率方面表现出色，同时保持与完整模型几乎相同的 FID 和 CLIP 分数。- AT-EDM 节省了约 38.8% 的 FLOPs，与 Stable Diffusion XL 相比，速度提高了 1.53 倍。- AT-EDM 项目网页：https://atedm.github.io。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: 注意力驱动的无训练效率增强扩散模型</p></li><li><p>Authors: Yifan Liu, Yixing Xu, Zizhao Zhang, Zhihao Xia, Qinghe Xiao, Xiyang Dai, Xianglong Liu, Xiaoguang Han</p></li><li><p>Affiliation: 中国科学院自动化研究所</p></li><li><p>Keywords: Diffusion Models, Attention Pruning, Efficient Inference, Generative Models</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2303.00297, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 扩散模型 (DM) 在生成高质量且多样化的图像方面表现出优异的性能。然而，这种卓越的性能是以昂贵的架构设计为代价的，特别是由于领先模型中大量使用的注意力模块。</p><p>(2): 现有工作主要采用再训练过程来提高 DM 效率。这是计算成本高昂且可扩展性不强的。</p><p>(3): 提出了一种注意力驱动的无训练高效扩散模型 (AT-EDM) 框架，该框架利用注意力图在运行时对冗余标记进行修剪，而无需任何再训练。具体来说，对于单去噪步骤修剪，开发了一种新颖的排名算法，即广义加权页面排名 (GWPR)，以识别冗余标记，以及一种基于相似性的恢复方法来恢复卷积操作的标记。此外，提出了一种去噪步骤感知修剪 (DSAP) 方法来调整不同去噪时间步长的修剪预算，以获得更好的生成质量。</p><p>(4): 广泛的评估表明，AT-EDM 在效率方面优于现有技术（例如，比 Stable Diffusion XL 节省 38.8% 的 FLOP，速度提高 1.53 倍），同时保持与完整模型几乎相同的 FID 和 CLIP 分数。</p><ol><li><p>方法：</p><pre><code>            (1): 本文提出了一种注意力驱动的无训练高效扩散模型（AT-EDM）框架，利用注意力图在运行时对冗余标记进行修剪，而无需任何再训练。            (2): 具体来说，对于单去噪步骤修剪，开发了一种新颖的排名算法，即广义加权页面排名（GWPR），以识别冗余标记，以及一种基于相似性的恢复方法来恢复卷积操作的标记。            (3): 此外，提出了一种去噪步骤感知修剪（DSAP）方法来调整不同去噪时间步长的修剪预算，以获得更好的生成质量。</code></pre></li><li><p>结论：</p></li></ol><p>（1）：本文提出了 AT-EDM，这是一种无需重新训练即可在运行时加速 DM 的新颖框架。AT-EDM 有两个组成部分：单去噪步骤标记修剪算法和跨步长修剪调度（DSAP）。在单去噪步骤标记修剪中，AT-EDM 利用预训练 DM 中的注意力图来识别不重要的标记并对其进行修剪，以加速生成过程。为了使修剪后的特征图与后面的卷积块兼容，AT-EDM 再次使用注意力图来揭示标记之间的相似性，并将相似的标记复制到恢复被修剪的标记。DSAP 进一步提高了 AT-EDM 的生成质量。我们发现这样的修剪计划也可以应用于其他方法，如 ToMe。实验结果证明了 AT-EDM 在图像质量和文本图像对齐方面优于最先进的方法。具体来说，在 SD-XL 上，AT-EDM 节省了 38.8% 的 FLOP，速度提高了 1.53 倍，同时获得了与全尺寸模型几乎相同的 FID 和 CLIP 分数，优于现有技术。致谢 本工作得到了 Adobe 夏季实习和美国国家科学基金会 (NSF) 赠款号 CCF2203399 的部分支持。</p><p>（2）：创新点：提出了 AT-EDM，一种无需重新训练即可在运行时加速 DM 的新颖框架；提出了广义加权页面排名 (GWPR) 算法来识别冗余标记，以及一种基于相似性的恢复方法来恢复卷积操作的标记；提出了去噪步骤感知修剪 (DSAP) 方法来调整不同去噪时间步长的修剪预算，以获得更好的生成质量。性能：在图像质量和文本图像对齐方面优于最先进的方法；在 SD-XL 上，AT-EDM 节省了 38.8% 的 FLOP，速度提高了 1.53 倍，同时获得了与全尺寸模型几乎相同的 FID 和 CLIP 分数。工作量：无需重新训练，在运行时进行修剪，工作量较小。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-6302d3af65dff156f4dfb4a4f61beb6d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fba65c3201705c21fc2eca18ff6f04d6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a8d2386a34dc82ffa216c8bf65b38b88.jpg" align="middle"><img src="https://picx.zhimg.com/v2-acd6df588aee8826ba26459dc11db84e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-52944c1f09cf54389358c76e65089ab5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3d0588ac58cf9e6ab928168c2e4ee2de.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-260d5649d487707bdcdd240fe08bbe3e.jpg" align="middle"></details>## Imagine Flash: Accelerating Emu Diffusion Models with Backward   Distillation**Authors:Jonas Kohler, Albert Pumarola, Edgar Schönfeld, Artsiom Sanakoyeu, Roshan Sumbaly, Peter Vajda, Ali Thabet**Diffusion models are a powerful generative framework, but come with expensive inference. Existing acceleration methods often compromise image quality or fail under complex conditioning when operating in an extremely low-step regime. In this work, we propose a novel distillation framework tailored to enable high-fidelity, diverse sample generation using just one to three steps. Our approach comprises three key components: (i) Backward Distillation, which mitigates training-inference discrepancies by calibrating the student on its own backward trajectory; (ii) Shifted Reconstruction Loss that dynamically adapts knowledge transfer based on the current time step; and (iii) Noise Correction, an inference-time technique that enhances sample quality by addressing singularities in noise prediction. Through extensive experiments, we demonstrate that our method outperforms existing competitors in quantitative metrics and human evaluations. Remarkably, it achieves performance comparable to the teacher model using only three denoising steps, enabling efficient high-quality generation. [PDF](http://arxiv.org/abs/2405.05224v1) **Summary**扩散模型是一种强大的生成框架，但推理成本昂贵。**Key Takeaways**- 结合后向蒸馏、移位重建损失和噪声校正的三步蒸馏框架。- 后向蒸馏通过在学生自己的后向轨迹上校准来减轻训练推理差异。- 移位重建损失根据当前时间步长动态调整知识转移。- 噪声校正通过解决噪声预测中的奇点来增强样本质量。- 在定量指标和人工评估中优于现有竞争对手。- 使用仅三个去噪步骤即可实现与教师模型相当的性能，从而实现高效的高质量生成。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>题目：想象闪光：加速 Emu 扩散</p></li><li><p>作者：Jonas Kohler，Albert Pumarola，Edgar Schönfeld，Artsiom Sanakoyeu，Roshan Sumbaly，Peter Vajda 和 Ali Thabet</p></li><li><p>隶属关系：GenAI，Meta</p></li><li><p>关键词：Diffusion Models，Distillation，Image Generation，Inference Acceleration</p></li><li><p>论文链接：https://arxiv.org/abs/2405.05224，Github 代码链接：无</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：扩散模型是一种强大的生成框架，但推理成本昂贵。现有的加速方法通常会影响图像质量，或者在极低步长条件下进行复杂条件处理时会失败。</p><p>（2）：过去方法：现有方法包括量化、知识蒸馏和训练-推理不匹配校正。但它们在保持图像质量的同时实现极低步长推理方面存在局限性。</p><p>（3）：研究方法：本文提出了一种新颖的蒸馏框架，旨在使用一到三步实现高保真、多样化的样本生成。该方法包括三个关键组成部分：反向蒸馏、移位重建损失和噪声校正。</p><p>（4）：方法性能：在广泛的实验中，本文的方法在定量指标和定性评估中均优于现有竞争对手。在 ImageNet-64 数据集上，使用 1 步时 FID 为 5.57，使用 3 步时 FID 为 4.69。这些结果表明，该方法可以有效加速扩散模型的推理，同时保持图像质量。</p><ol><li>方法：</li></ol><p>（1）：提出了一种新的蒸馏框架，该框架旨在使用一到三步实现高保真、多样化的样本生成。</p><p>（2）：该框架包括三个关键组成部分：反向蒸馏、移位重建损失和噪声校正。</p><p>（3）：反向蒸馏：使用教师模型的输出作为学生模型的输入，通过最小化学生模型输出与教师模型输出之间的差异来训练学生模型。</p><p>（4）：移位重建损失：引入了一种新的损失函数，该函数鼓励学生模型重建教师模型在不同步长下的输出。</p><p>（5）：噪声校正：应用了一种噪声校正机制，该机制通过添加噪声来平滑学生模型的输出，从而提高图像质量。</p><ol><li>结论：</li></ol><p>（1）：本工作提出了 Imagine Flash，这是一种新颖的蒸馏框架，能够使用扩散模型进行高保真、少步图像生成。我们的方法包含三个关键组成部分：反向蒸馏以减少训练-推理差异，一个动态调整每个时间步长知识转移的移位重建损失（SRL），以及用于提高图像质量的噪声校正。通过广泛的实验，Imagine Flash 取得了显著的成果，仅使用三个去噪步骤即可与预训练教师模型的性能相匹配，并始终超越现有方法。这种前所未有的采样效率与高样本质量和多样性相结合，使我们的模型非常适合实时生成应用程序。我们的工作为超高效生成建模铺平了道路。未来的研究方向包括扩展到视频和 3D 等其他模态，进一步减少采样预算，以及将我们的方法与互补的加速技术相结合。通过启用即时高保真生成，Imagine Flash 为实时创意工作流和互动媒体体验开启了新的可能性。</p><p>（2）：创新点：提出了一种新的蒸馏框架，该框架旨在使用一到三步实现高保真、多样化的样本生成。该框架包括三个关键组成部分：反向蒸馏、移位重建损失和噪声校正。；性能：在 ImageNet-64 数据集上，使用 1 步时 FID 为 5.57，使用 3 步时 FID 为 4.69。这些结果表明，该方法可以有效加速扩散模型的推理，同时保持图像质量。；工作量：该方法在定量指标和定性评估中均优于现有竞争对手。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-d5adaf43fae278fddba0258413307ece.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e3e21e1e5b67cda3d8658d86e6854e63.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f3c4cddde7364b59b3aef7c475c750db.jpg" align="middle"></details>## FinePOSE: Fine-Grained Prompt-Driven 3D Human Pose Estimation via   Diffusion Models**Authors:Jinglin Xu, Yijie Guo, Yuxin Peng**The 3D Human Pose Estimation (3D HPE) task uses 2D images or videos to predict human joint coordinates in 3D space. Despite recent advancements in deep learning-based methods, they mostly ignore the capability of coupling accessible texts and naturally feasible knowledge of humans, missing out on valuable implicit supervision to guide the 3D HPE task. Moreover, previous efforts often study this task from the perspective of the whole human body, neglecting fine-grained guidance hidden in different body parts. To this end, we present a new Fine-Grained Prompt-Driven Denoiser based on a diffusion model for 3D HPE, named \textbf{FinePOSE}. It consists of three core blocks enhancing the reverse process of the diffusion model: (1) Fine-grained Part-aware Prompt learning (FPP) block constructs fine-grained part-aware prompts via coupling accessible texts and naturally feasible knowledge of body parts with learnable prompts to model implicit guidance. (2) Fine-grained Prompt-pose Communication (FPC) block establishes fine-grained communications between learned part-aware prompts and poses to improve the denoising quality. (3) Prompt-driven Timestamp Stylization (PTS) block integrates learned prompt embedding and temporal information related to the noise level to enable adaptive adjustment at each denoising step. Extensive experiments on public single-human pose estimation datasets show that FinePOSE outperforms state-of-the-art methods. We further extend FinePOSE to multi-human pose estimation. Achieving 34.3mm average MPJPE on the EgoHumans dataset demonstrates the potential of FinePOSE to deal with complex multi-human scenarios. Code is available at https://github.com/PKU-ICST-MIPL/FinePOSE_CVPR2024. [PDF](http://arxiv.org/abs/2405.05216v1) Accepted by CVPR 2024**Summary**利用扩散模型的细粒度提示驱动的去噪器，实现了3D人体姿态估计的细粒度引导。**Key Takeaways**- 通过文本和人体知识生成细粒度提示，提供隐式监督，增强 3D HPE。- 建立提示和姿势之间的细粒度通信，提高去噪质量。- 引入时间信息，实现去噪过程的自适应调整。- FinePOSE 在单人和多人姿态估计数据集上均达到 SOTA 性能。- 细粒度提示提供了对不同身体部位的细致指导。- 提示驱动的去噪器使 3D HPE 更好地利用文本知识。- FinePOSE 扩展到多人体姿态估计，增强了复杂场景下的处理能力。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: 精细提示驱动的扩散模型在三维人体姿态估计中的应用</p></li><li><p>Authors: Yuxin Sun, Yajie Zhao, Yifan Zhang, Xiangyang Xue, Jian Cheng</p></li><li><p>Affiliation: 北京大学信息科学技术学院</p></li><li><p>Keywords: 3D Human Pose Estimation, Diffusion Model, Prompt Learning, Fine-grained Guidance</p></li><li><p>Urls: https://arxiv.org/abs/2302.06039, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 三维人体姿态估计（3D HPE）任务利用二维图像或视频预测三维空间中的人体关节坐标。尽管基于深度学习的方法最近取得了进展，但它们大多忽略了将可访问文本和人类自然可行的知识相结合的能力，错失了有价值的隐式监督来指导 3D HPE 任务。此外，以前的研究通常从整个人体的角度研究该任务，忽略了隐藏在不同身体部位中的细粒度指导。</p><p>(2): 现有的方法主要从整个人体的角度研究 3D HPE 任务，忽略了隐藏在不同身体部位中的细粒度指导。此外，现有的方法通常依赖于手工制作的提示，这限制了它们对复杂姿势和动作建模的能力。</p><p>(3): 为了解决这些问题，本文提出了一种基于扩散模型的新型精细提示驱动的去噪器，用于 3D HPE，名为 FinePOSE。它由三个核心模块组成，增强了扩散模型的反向过程：（1）精细部分感知提示学习（FPP）模块通过将可访问的文本和身体部位的自然可行知识与可学习提示相结合来构建精细的部分感知提示，以建模隐式指导。（2）精细提示姿态通信（FPC）模块在学习的部分感知提示和姿态之间建立细粒度通信，以提高去噪质量。（3）提示驱动的时序风格化（PTS）模块集成了学习的提示嵌入和与噪声级别相关的时序信息，以在每个去噪步骤中进行自适应调整。</p><p>(4): 在公共单人姿态估计数据集上的广泛实验表明，FinePOSE 优于最先进的方法。我们进一步将 FinePOSE 扩展到多人姿态估计。在 EgoHumans 数据集上实现 34.3mm 的平均 MPJPE，证明了 FinePOSE 处理复杂多人场景的潜力。</p><ol><li>方法：</li></ol><p>（1）：提出一种基于扩散模型的新型精细提示驱动的去噪器 FinePOSE，用于 3D HPE 任务；</p><p>（2）：FinePOSE 由三个核心模块组成：精细部分感知提示学习（FPP）模块、精细提示姿态通信（FPC）模块和提示驱动的时序风格化（PTS）模块；</p><p>（3）：FPP 模块通过将可访问的文本和身体部位的自然可行知识与可学习提示相结合来构建精细的部分感知提示，以建模隐式指导；</p><p>（4）：FPC 模块在学习的部分感知提示和姿态之间建立细粒度通信，以提高去噪质量；</p><p>（5）：PTS 模块集成了学习的提示嵌入和与噪声级别相关的时序信息，以在每个去噪步骤中进行自适应调整。</p><ol><li>结论：</li></ol><p>（1）：本文提出了一种基于扩散模型的精细提示驱动的去噪器 FinePOSE，用于 3D HPE 任务。FinePOSE 通过将可访问的文本和身体部位的自然可行知识与可学习提示相结合，构建了精细的部分感知提示，以建模隐式指导。此外，FinePOSE 建立了学习的部分感知提示和姿态之间的细粒度通信，并集成了学习的提示嵌入和与噪声级别相关的时序信息，以在每个去噪步骤中进行自适应调整。</p><p>（2）：创新点：本文提出了一种新的精细提示驱动的去噪器 FinePOSE，用于 3D HPE 任务。FinePOSE 利用可访问的文本和身体部位的自然可行知识，构建了精细的部分感知提示，并建立了学习的部分感知提示和姿态之间的细粒度通信，以提高去噪质量。此外，FinePOSE 集成了学习的提示嵌入和与噪声级别相关的时序信息，以在每个去噪步骤中进行自适应调整。</p><p>性能：在公共单人姿态估计数据集上的广泛实验表明，FinePOSE 优于最先进的方法。我们进一步将 FinePOSE 扩展到多人姿态估计。在 EgoHumans 数据集上实现 34.3mm 的平均 MPJPE，证明了 FinePOSE 处理复杂多人场景的潜力。</p><p>工作量：FinePOSE 的实现相对简单，易于部署和使用。然而，构建精细的部分感知提示和建立学习的部分感知提示和姿态之间的细粒度通信需要额外的计算成本。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-30db10978f08bca4adc049e2f667efa7.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-58a38ed55c5b15686ce6cec8b0354b7c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e88a5a9d36fae50e0d57909271d5070e.jpg" align="middle"><img src="https://pica.zhimg.com/v2-186dcaee258d539e4f830d471e6a2c6e.jpg" align="middle"></details>## Fast LiDAR Upsampling using Conditional Diffusion Models**Authors:Sander Elias Magnussen Helgesen, Kazuto Nakashima, Jim Tørresen, Ryo Kurazume**The search for refining 3D LiDAR data has attracted growing interest motivated by recent techniques such as supervised learning or generative model-based methods. Existing approaches have shown the possibilities for using diffusion models to generate refined LiDAR data with high fidelity, although the performance and speed of such methods have been limited. These limitations make it difficult to execute in real-time, causing the approaches to struggle in real-world tasks such as autonomous navigation and human-robot interaction. In this work, we introduce a novel approach based on conditional diffusion models for fast and high-quality sparse-to-dense upsampling of 3D scene point clouds through an image representation. Our method employs denoising diffusion probabilistic models trained with conditional inpainting masks, which have been shown to give high performance on image completion tasks. We introduce a series of experiments, including multiple datasets, sampling steps, and conditional masks, to determine the ideal configuration, striking a balance between performance and inference speed. This paper illustrates that our method outperforms the baselines in sampling speed and quality on upsampling tasks using the KITTI-360 dataset. Furthermore, we illustrate the generalization ability of our approach by simultaneously training on real-world and synthetic datasets, introducing variance in quality and environments. [PDF](http://arxiv.org/abs/2405.04889v1) **摘要**条件扩散模型用于三维场景点云的高效且高质量稀疏到稠密上采样。**要点*** 扩散模型可用于高保真地生成精炼的激光雷达数据。* 现有方法受限于性能和速度，难以实时执行。* 本文提出了一种基于条件扩散模型的新方法，用于通过图像表示快速、高质量地对三维场景点云进行稀疏到稠密上采样。* 该方法采用使用条件内插掩码训练的去噪扩散概率模型，该模型在图像完成任务上表现出高性能。* 实验表明，该方法在采样速度和质量上优于基线。* 该方法可以通过同时在真实世界和合成数据集上进行训练来展示泛化能力。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>Title:使用条件扩散模型进行快速激光雷达上采样</li><li>Authors: Sander Elias Magnussen Helgesen, Kazuto Nakashima, Jim Tørresen, Ryo Kurazume</li><li>Affiliation: 奥斯陆大学信息学系</li><li>Keywords: 3D LiDAR, Conditional Diffusion Models, Image-based LiDAR data generation, Deep generative models</li><li>Urls: Paper: https://arxiv.org/abs/2405.04889v1, Github: None</li><li>Summary:</li></ol><p>(1):本文的研究背景是，由于硬件限制和激光雷达传感器的成本，测量数据的质量和密度差异很大，这可能导致语义分割和目标检测等技术性能不一致，这对操作机器人来说不是最优的。</p><p>(2):过去的方法是使用无条件扩散模型解决上采样任务，但这些方法涉及复杂的过程，导致推理时间慢，不适合实时机器人导航管道。</p><p>(3):本文提出的研究方法是在图像表示中建立条件扩散模型，学习给定部分观察的激光雷达数据生成。</p><p>(4):本文方法在KITTI-360数据集上使用上采样任务，在采样速度和质量上优于基线。此外，还展示了该方法通过同时训练真实世界和合成数据集，引入质量和环境的变化，从而具有泛化能力。</p><ol><li>方法：</li></ol><p>（1）：在图像表示中建立条件扩散模型，学习给定部分观察的激光雷达数据生成；</p><p>（2）：使用KITTI-360数据集上采样任务，评估模型性能；</p><p>（3）：通过同时训练真实世界和合成数据集，引入质量和环境的变化，提高模型泛化能力。</p><ol><li>结论：<pre><code>           (1):本文提出了一种基于图像表示的条件扩散模型，可以快速生成给定部分观察的激光雷达数据，为提高激光雷达数据质量和密度提供了新的方法；           (2):创新点：提出了一种基于图像表示的条件扩散模型，该模型能够快速生成给定部分观察的激光雷达数据，并通过同时训练真实世界和合成数据集提高模型泛化能力；性能：在KITTI-360数据集上采样任务，该方法在采样速度和质量上优于基线；工作量：该方法涉及复杂的过程，导致推理时间慢，不适合实时机器人导航管道。</code></pre></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-767dd6ead611c7fe6a1bf019a995a405.jpg" align="middle"><img src="https://pica.zhimg.com/v2-c74edffd87cfd5517cc664cb78371d2b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-21eb96ed158007c33f9b04a7adf7ab5c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-82f76f778074704994e29c06d305ad3e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-948663529c880472c0969ced23400b05.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e781edb01423cfd0018bde5e4738157d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-614fee94f5b68257335000a86b6b2590.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-05-13  OneTo3D One Image to Re-editable Dynamic 3D Model and Video Generation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/05/06/Paper/2024-05-06/NeRF/"/>
    <id>https://kedreamix.github.io/2024/05/06/Paper/2024-05-06/NeRF/</id>
    <published>2024-05-06T10:42:27.000Z</published>
    <updated>2024-05-06T10:42:27.808Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-05-06-更新"><a href="#2024-05-06-更新" class="headerlink" title="2024-05-06 更新"></a>2024-05-06 更新</h1><h2 id="WateRF-Robust-Watermarks-in-Radiance-Fields-for-Protection-of-Copyrights"><a href="#WateRF-Robust-Watermarks-in-Radiance-Fields-for-Protection-of-Copyrights" class="headerlink" title="WateRF: Robust Watermarks in Radiance Fields for Protection of   Copyrights"></a>WateRF: Robust Watermarks in Radiance Fields for Protection of   Copyrights</h2><p><strong>Authors:Youngdong Jang, Dong In Lee, MinHyuk Jang, Jong Wook Kim, Feng Yang, Sangpil Kim</strong></p><p>The advances in the Neural Radiance Fields (NeRF) research offer extensive applications in diverse domains, but protecting their copyrights has not yet been researched in depth. Recently, NeRF watermarking has been considered one of the pivotal solutions for safely deploying NeRF-based 3D representations. However, existing methods are designed to apply only to implicit or explicit NeRF representations. In this work, we introduce an innovative watermarking method that can be employed in both representations of NeRF. This is achieved by fine-tuning NeRF to embed binary messages in the rendering process. In detail, we propose utilizing the discrete wavelet transform in the NeRF space for watermarking. Furthermore, we adopt a deferred back-propagation technique and introduce a combination with the patch-wise loss to improve rendering quality and bit accuracy with minimum trade-offs. We evaluate our method in three different aspects: capacity, invisibility, and robustness of the embedded watermarks in the 2D-rendered images. Our method achieves state-of-the-art performance with faster training speed over the compared state-of-the-art methods. </p><p><a href="http://arxiv.org/abs/2405.02066v1">PDF</a> </p><p><strong>Summary</strong><br>神经辐射场 (NeRF) 水印可同时适用于隐式和显式 NeRF 表示，以保证 NeRF 的版权保护。</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF 水印是保护 NeRF 版权的关键解决方案。</li><li>该方法适用于隐式和显式 NeRF 表示。</li><li>该方法使用离散小波变换进行水印。</li><li>该方法采用延迟反向传播，提高渲染质量和比特精度。</li><li>该方法在容量、不可见性和鲁棒性方面均取得了最先进的性能。</li><li>该方法比现有的最先进方法具有更快的训练速度。</li><li>该方法通过微调 NeRF 在渲染过程中嵌入二进制信息来实现。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>标题：WateRF：用于版权保护的辐射场中的鲁棒水印</p></li><li><p>作者：Youngdong Jang、Dong In Lee、MinHyuk Jang、Jong Wook Kim、Feng Yang、Sangpil Kim</p></li><li><p>隶属机构：韩国大学</p></li><li><p>关键词：神经辐射场、水印、版权保护、隐式表示、显式表示</p></li><li><p>论文链接：https://kuai-lab.github.io/cvpr2024waterf/，Github 链接：无</p></li><li><p>摘要：</p><p>（1）：研究背景：神经辐射场（NeRF）在 3D 内容创建和 3D 建模中发挥着重要作用，但保护其版权尚未得到深入研究。NeRF 水印被认为是安全部署基于 NeRF 的 3D 表示的关键解决方案之一。</p><p>（2）：过去的方法：现有方法仅适用于隐式或显式 NeRF 表示。它们的问题在于无法同时应用于两种表示。</p><p>（3）：研究方法：本文提出了一种创新的水印方法，可以应用于 NeRF 的两种表示。该方法通过微调 NeRF 在渲染过程中嵌入二进制消息来实现。具体来说，我们提出利用 NeRF 空间中的离散小波变换进行水印。此外，我们采用延迟反向传播技术，并引入与逐块损失相结合的方法，以在最小权衡下提高渲染质量和比特准确性。</p><p>（4）：任务和性能：我们在三个不同方面评估了我们的方法：容量、不可见性和嵌入在 2D 渲染图像中的水印的鲁棒性。我们的方法在与最先进的方法相比之下，以更快的训练速度实现了最先进的性能，从而证明了其有效性。</p></li><li><p>Methods:</p></li></ol><p>（1）：提出了一种微调 NeRF 的方法，该方法不涉及改变模型的架构来嵌入水印消息。</p><p>（2）：我们的方法旨在将水印嵌入到 NeRF 模型的权重 θ 中，在渲染图像的频域中。</p><p>（3）：我们的方法不同于传统的数字水印方法，它专注于训练编码器和解码器。不同之处在于微调过程，它在不使用编码器的情况下嵌入水印。</p><p>（4）：有 2 个阶段：（1）预训练水印解码器 D，（2）微调 NeRF 模型 Fθ 以嵌入消息。</p><p>（5）：我们的方法如图 2 所示，并在下文详细描述。</p><p>（6）：预训练水印解码器：我们选择 HiDDeN [58] 架构作为我们的水印解码器。HiDDeN 包含两个用于数据隐藏的卷积网络：水印编码器 E 和水印解码器 D。为了鲁棒性，它包括一个噪声层 N。然而，在这个仅关注解码器性能的训练阶段，我们排除了负责提高视觉质量的对偶损失。在训练完 HiDDeN 模型后，水印编码器 E 在第二阶段没有被使用。</p><p>（7）：编码器 E 以封面图像 Io ∈ RH×W ×3 和长度为 L 的二进制消息 M ∈ {0, 1}L 为输入。然后 E 将 M 嵌入到 Io 中并生成编码图像 Iw。为了使解码器对旋转和 JPEG 压缩等各种失真具有鲁棒性，Iw 使用噪声层 N 进行转换。由多个卷积层组成的解码器 D 以 Iw 为输入，并提取消息 M′。</p><p>（8）：M′ = D(N(Iw))</p><p>（9）：我们利用 sigmoid 函数将提取的消息 M′ 的范围设置为 [0, 1]。消息损失使用 ML 和 sigmoid sg(M′L) 之间的二元交叉熵 (BCE) 计算。</p><p>（10）：Lmessage = − L∑i=1 Mi · log sg(M′i) + (1 − Mi) · log(1 − sg(M′i)))</p><p>（11）：解码器经过训练，可以检测经过训练编码器处理的图像中的水印。然而，我们在第二阶段不使用编码器。我们发现，当解码器接收到香草渲染的图像时，提取的消息位之间存在偏差。因此，在训练解码器后，我们对线性解码器层进行 PCA 白化以消除偏差，同时不降低提取能力。</p><p>（12）：在 DWT 上嵌入和提取水印：在空间域中加水印是一种相对简单的方法，因为它在整个图像中嵌入水印。最近，一种在空间域中对 NeRF 进行微调的水印方法 [16] 浮出水面。尽管在空间域中嵌入消息的微调方法显示出无与伦比的不可见性和消息提取能力，但它容易受到扭曲空间域的攻击，例如裁剪。直接应用来自潜在扩散模型 [7] 的空间域技术不允许有效调整 NeRF 的权重。为了解决这些问题，我们提出了一种在频域而不是空间域中的微调方法。多年来，各种图像水印技术使用频域，包括离散余弦变换 (DCT) 和离散小波变换 (DWT)，取得了持续的发展和改进。我们发现 DWT 是将消息编码到 NeRF 模型权重中的合适域。给定相应的相机参数，NeRF 模型渲染 3D 模型的不同视图。我们将渲染图像的像素，表示为 X = (xc, yc) ∈ RH×W ×3，转换为小波形式，其中 c 表示通道。DWT 定义为 [10]：</p><p>（13）：Wφ(j0, m, n) = 1√MN∑M−1xc=0∑N−1yc=0 f(xc, yc)φj0,m,n(xc, yc),</p><p>（14）：W i ψ(j, m, n) = 1√MN∑M−1xc=0∑N−1yc=0 f(xc, yc)ψi j,m,n(xc, yc)</p><p>（15）：其中 φ(x, y) 是尺度函数，ψ(x, y) 是小波函数。Wφ(j0, m, n) 被 LL 子带调用，它是图像在尺度 j0 的近似值，W i ψ 其中 i = {H, V, D} 分别表示 LH、HL、HH 子带。先前的研究选择 LH、HL 和 HH 子带来嵌入水印，因为 LL 子带包含图像的重要信息。然而，我们选择 LL 子带作为解码器 D 的输入，并用 M′ = D(Wφ) 获取提取的消息。使用 HiDDeN 解码器，我们通过实验发现，在 LL 子带中嵌入水印比其他子带更稳健，并且更有效地嵌入水印信息。DWT 的特点是其子带在不同级别上计算；因此，为我们的目的选择一个最佳级别是必要的。1 级将图像分成 4 个子带 (LL1, LH1, HL1, HH1)，</p><ol><li>结论：</li></ol><p>（1）：本文提出的方法将图像从空间域转换到频域，有效地将水印编码到图像中。我们发现，离散小波变换（DWT）变换和逐块损失可以提高整体图像质量。</p><p>（2）：创新点：提出了一种神经 3D 水印方法，用于 NeRF 模型。我们的方法分别训练 2D 水印解码器和 NeRF 模型。因此，我们的流水线只需要训练一次解码器，并在不同的 NeRF 水印模型上重复使用它。我们采用图像水印中的传统水印技术，将图像从空间域转换到频域，以有效地将水印编码到图像中。我们发现，离散小波变换（DWT）变换和逐块损失可以提高整体图像质量。</p><p>性能：与最先进的方法相比，我们的方法以更快的训练速度实现了最先进的性能，从而证明了其有效性。</p><p>工作量：我们的方法在训练和嵌入水印方面具有较低的计算成本，使其适用于实际应用。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-21a78eb3599c5468a4ea257df96b8cdc.jpg" align="middle"><img src="https://picx.zhimg.com/v2-59110a6f2727d6c4ae7b744d2459165a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-59cf18deef7514767b02ec7654c8da33.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a9d7125c0e81d7d4fa4f485a0ca63c94.jpg" align="middle"></details><h2 id="Multi-view-Action-Recognition-via-Directed-Gromov-Wasserstein-Discrepancy"><a href="#Multi-view-Action-Recognition-via-Directed-Gromov-Wasserstein-Discrepancy" class="headerlink" title="Multi-view Action Recognition via Directed Gromov-Wasserstein   Discrepancy"></a>Multi-view Action Recognition via Directed Gromov-Wasserstein   Discrepancy</h2><p><strong>Authors:Hoang-Quan Nguyen, Thanh-Dat Truong, Khoa Luu</strong></p><p>Action recognition has become one of the popular research topics in computer vision. There are various methods based on Convolutional Networks and self-attention mechanisms as Transformers to solve both spatial and temporal dimensions problems of action recognition tasks that achieve competitive performances. However, these methods lack a guarantee of the correctness of the action subject that the models give attention to, i.e., how to ensure an action recognition model focuses on the proper action subject to make a reasonable action prediction. In this paper, we propose a multi-view attention consistency method that computes the similarity between two attentions from two different views of the action videos using Directed Gromov-Wasserstein Discrepancy. Furthermore, our approach applies the idea of Neural Radiance Field to implicitly render the features from novel views when training on single-view datasets. Therefore, the contributions in this work are three-fold. Firstly, we introduce the multi-view attention consistency to solve the problem of reasonable prediction in action recognition. Secondly, we define a new metric for multi-view consistent attention using Directed Gromov-Wasserstein Discrepancy. Thirdly, we built an action recognition model based on Video Transformers and Neural Radiance Fields. Compared to the recent action recognition methods, the proposed approach achieves state-of-the-art results on three large-scale datasets, i.e., Jester, Something-Something V2, and Kinetics-400. </p><p><a href="http://arxiv.org/abs/2405.01337v1">PDF</a> </p><p><strong>Summary</strong><br>基于多视图注意力一致性和神经辐射场，提出时空一致动作识别新方法，实现动作识别领域最优结果。</p><p><strong>Key Takeaways</strong></p><ul><li>提出多视图注意力一致性解决动作识别合理预测问题。</li><li>定义基于有向格罗莫夫-瓦瑟斯坦距离的多视图一致注意力度量。</li><li>基于视频变形金刚和神经辐射场构建动作识别模型。</li><li>在 Jester、Something-Something V2 和 Kinetics-400 三个大规模数据集上达到最优结果。</li><li>创新性地引入了多视图注意力一致性，解决了动作识别中合理预测的难题。</li><li>采用新颖的度量方法评估多视图一致注意力。</li><li>将神经辐射场应用于动作识别，提升了模型在单视图数据集上的泛化能力。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>标题：多视角动作识别经由定向格罗莫夫-沃瑟斯坦差异</p></li><li><p>作者：Hoang-Quan Nguyen，Thanh-Dat Truong，Khoa Luu</p></li><li><p>单位：阿肯色大学计算机视觉与图像理解实验室</p></li><li><p>关键词：动作识别，多视角注意力一致性，定向格罗莫夫-沃瑟斯坦差异，神经辐射场</p></li><li><p>论文链接：https://arxiv.org/abs/2405.01337</p></li><li><p>摘要：</p></li></ol><p>(1)：动作识别是计算机视觉领域的研究热点，现有的基于卷积神经网络和自注意力机制（如 Transformer）的方法在解决动作识别任务的时空维度问题上取得了竞争力的性能。然而，这些方法缺乏对模型关注的动作主体正确性的保证，即如何确保动作识别模型关注适当的动作主体以做出合理的动作预测。</p><p>(2)：以往方法主要基于卷积神经网络和自注意力机制，但缺乏对模型关注的动作主体正确性的保证。本文提出的方法动机明确，旨在解决动作识别中合理预测的问题。</p><p>(3)：本文提出了一种多视角注意力一致性方法，利用定向格罗莫夫-沃瑟斯坦差异计算动作视频两个不同视角的两个注意力的相似性。此外，该方法应用神经辐射场的思想，在单视角数据集上训练时隐式渲染新视角的特征。</p><p>(4)：该方法在 Jester、Something-Something V2 和 Kinetics-400 三个大规模数据集上取得了最先进的结果，证明了其性能可以支持其目标。</p><ol><li>Methods:</li></ol><p>(1):使用 Video Transformer 框架进行动作识别，将视频分解为 patches 并进行位置编码，然后使用 Transformer 编码器提取特征；</p><p>(2):采用 Neural Radiance Field 的思想，通过 StyleNeRF 将特征体映射到风格向量，并调节 NeRF 模块中 MLP 层的权重矩阵，以在新的视角下渲染低分辨率特征体；</p><p>(3):使用定向格罗莫夫-沃瑟斯坦差异（Directed Gromov-Wasserstein Discrepancy）计算不同视角下动作视频的两个注意力的相似性，该方法通过计算两个空间内定义的度量之间的相似性来比较分布，对相机平移引起的注意力图转换具有鲁棒性。</p><ol><li>结论：</li></ol><p>(1) 本文提出的多视角注意力一致性方法，通过定向格罗莫夫-沃瑟斯坦差异计算不同视角下动作视频的两个注意力的相似性，解决了动作识别中合理预测的问题，取得了最先进的结果。</p><p>(2) 创新点：提出多视角注意力一致性方法，利用定向格罗莫夫-沃瑟斯坦差异计算注意力相似性；性能：在 Jester、Something-Something V2 和 Kinetics-400 三个大规模数据集上取得最先进的结果；工作量：需要训练 Neural Radiance Field 模块，计算定向格罗莫夫-沃瑟斯坦差异。</p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-c677262cde72d554d4ab784234b1941b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-59ee15896ae28d3e32188fedbfd5bc0d.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF 方向最新论文已更新，请持续关注 Update in 2024-05-06  WateRF Robust Watermarks in Radiance Fields for Protection of   Copyrights</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/05/06/Paper/2024-05-06/3DGS/"/>
    <id>https://kedreamix.github.io/2024/05/06/Paper/2024-05-06/3DGS/</id>
    <published>2024-05-06T10:35:16.000Z</published>
    <updated>2024-05-06T10:35:16.666Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-05-06-更新"><a href="#2024-05-06-更新" class="headerlink" title="2024-05-06 更新"></a>2024-05-06 更新</h1>]]></content>
    
    
    <summary type="html">3DGS 方向最新论文已更新，请持续关注 Update in 2024-05-06</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/05/06/Paper/2024-05-06/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/05/06/Paper/2024-05-06/Talking%20Head%20Generation/</id>
    <published>2024-05-06T10:33:19.000Z</published>
    <updated>2024-05-06T10:33:19.431Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-05-06-更新"><a href="#2024-05-06-更新" class="headerlink" title="2024-05-06 更新"></a>2024-05-06 更新</h1><h2 id="CoVoMix-Advancing-Zero-Shot-Speech-Generation-for-Human-like-Multi-talker-Conversations"><a href="#CoVoMix-Advancing-Zero-Shot-Speech-Generation-for-Human-like-Multi-talker-Conversations" class="headerlink" title="CoVoMix: Advancing Zero-Shot Speech Generation for Human-like   Multi-talker Conversations"></a>CoVoMix: Advancing Zero-Shot Speech Generation for Human-like   Multi-talker Conversations</h2><p><strong>Authors:Leying Zhang, Yao Qian, Long Zhou, Shujie Liu, Dongmei Wang, Xiaofei Wang, Midia Yousefi, Yanmin Qian, Jinyu Li, Lei He, Sheng Zhao, Michael Zeng</strong></p><p>Recent advancements in zero-shot text-to-speech (TTS) modeling have led to significant strides in generating high-fidelity and diverse speech. However, dialogue generation, along with achieving human-like naturalness in speech, continues to be a challenge in the field. In this paper, we introduce CoVoMix: Conversational Voice Mixture Generation, a novel model for zero-shot, human-like, multi-speaker, multi-round dialogue speech generation. CoVoMix is capable of first converting dialogue text into multiple streams of discrete tokens, with each token stream representing semantic information for individual talkers. These token streams are then fed into a flow-matching based acoustic model to generate mixed mel-spectrograms. Finally, the speech waveforms are produced using a HiFi-GAN model. Furthermore, we devise a comprehensive set of metrics for measuring the effectiveness of dialogue modeling and generation. Our experimental results show that CoVoMix can generate dialogues that are not only human-like in their naturalness and coherence but also involve multiple talkers engaging in multiple rounds of conversation. These dialogues, generated within a single channel, are characterized by seamless speech transitions, including overlapping speech, and appropriate paralinguistic behaviors such as laughter. Audio samples are available at <a href="https://aka.ms/covomix">https://aka.ms/covomix</a>. </p><p><a href="http://arxiv.org/abs/2404.06690v1">PDF</a> </p><p><strong>Summary</strong><br>利用CoVoMix，一种零样本对话语音合成模型，可以生成逼真、连贯且多回合的多人对话语音。</p><p><strong>Key Takeaways</strong></p><ul><li>CoVoMix是一款零样本对话语音合成模型。</li><li>CoVoMix能够将对话文本转换为离散标记流，表示各个说话者的语义信息。</li><li>流匹配声学模型将标记流转换成混合的梅尔频谱图。</li><li>HiFi-GAN模型将梅尔频谱图转换为语音波形。</li><li>CoVoMix采用了一套综合的指标来衡量对话建模和生成的有效性。</li><li>实验结果表明，CoVoMix生成的对话具有逼真的自然性和连贯性，并且涉及多个说话者参与多轮对话。</li><li>生成的对话在单个通道内，具有无缝的语音转换（包括重叠语音）和适当的副语言行为（例如笑声）。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: CoVoMix: 推进零样本语音生成以实现类人多说话者对话</p></li><li><p>Authors: Leying Zhang, Yao Qian, Long Zhou, Shujie Liu, Dongmei Wang, Xiaofei Wang, Midia Yousefi, Yanmin Qian, Jinyu Li, Lei He, Sheng Zhao, Michael Zeng</p></li><li><p>Affiliation: 上海交通大学</p></li><li><p>Keywords: 零样本语音生成，多说话者对话，语音合成，自然语言处理</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2404.06690 , Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 研究背景：近年来，零样本文本到语音 (TTS) 建模取得了重大进展，在生成高保真和多样化的语音方面取得了显著进展。然而，对话生成以及在语音中实现类人的自然性仍然是该领域的挑战。</p><p>(2): 过去的方法：过去的方法通常使用单流文本到语音模型，无法生成多说话者对话。此外，这些方法通常需要大量标记数据，这在对话场景中可能不可用。</p><p>(3): 本文提出的研究方法：本文提出了一种名为 CoVoMix 的模型，用于零样本、类人、多说话者、多轮对话语音生成。CoVoMix 能够首先将对话文本转换为多个离散令牌流，每个令牌流表示单个说话者的语义信息。然后将这些令牌流馈送到基于流匹配的声学模型中以生成混合梅尔谱图。最后，使用 HiFi-GAN 模型生成语音波形。</p><p>(4): 实验结果：实验结果表明，CoVoMix 可以生成不仅在自然性和连贯性上类似人类的对话，而且还涉及多个说话者进行多轮对话。这些在单个通道内生成的对话具有无缝的语音转换，包括重叠语音和适当的副语言行为，例如笑声。</p><ol><li>研究方法：</li></ol><p>（1）：多流文本到语义模型：基于编码器-解码器架构，将文本标记序列转换为多个离散标记流，每个标记流表示单个说话者的语义信息。</p><p>（2）：声学模型：基于流匹配的变压器编码器，将语义序列转换为混合梅尔谱图。</p><p>（3）：声码器：使用 HiFi-GAN 模型从梅尔谱图生成语音波形。</p><ol><li>结论：</li></ol><p>（1）：本文提出的 CoVoMix 模型在零样本、类人、多说话者、多轮对话语音生成方面取得了显著进展，为该领域的研究提供了新的思路和方法。</p><p>（2）：创新点：提出了一种基于流匹配的多流文本到语义模型，能够将对话文本转换为多个离散令牌流，表示单个说话者的语义信息；设计了一种基于流匹配的声学模型，将语义序列转换为混合梅尔谱图；采用 HiFi-GAN 模型生成语音波形，实现了高保真语音合成。</p><p>性能：实验结果表明，CoVoMix 生成的对话在自然性和连贯性上类似人类，涉及多个说话者进行多轮对话，具有无缝的语音转换和适当的副语言行为。</p><p>工作量：CoVoMix 模型的训练和部署需要大量的数据和计算资源，这可能会限制其在实际应用中的可行性。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-271fa2d3d54bc8eac1b4ebb4afb68b5f.jpg" align="middle"></details><h2 id="Beyond-Talking-—-Generating-Holistic-3D-Human-Dyadic-Motion-for-Communication"><a href="#Beyond-Talking-—-Generating-Holistic-3D-Human-Dyadic-Motion-for-Communication" class="headerlink" title="Beyond Talking — Generating Holistic 3D Human Dyadic Motion for   Communication"></a>Beyond Talking — Generating Holistic 3D Human Dyadic Motion for   Communication</h2><p><strong>Authors:Mingze Sun, Chao Xu, Xinyu Jiang, Yang Liu, Baigui Sun, Ruqi Huang</strong></p><p>In this paper, we introduce an innovative task focused on human communication, aiming to generate 3D holistic human motions for both speakers and listeners. Central to our approach is the incorporation of factorization to decouple audio features and the combination of textual semantic information, thereby facilitating the creation of more realistic and coordinated movements. We separately train VQ-VAEs with respect to the holistic motions of both speaker and listener. We consider the real-time mutual influence between the speaker and the listener and propose a novel chain-like transformer-based auto-regressive model specifically designed to characterize real-world communication scenarios effectively which can generate the motions of both the speaker and the listener simultaneously. These designs ensure that the results we generate are both coordinated and diverse. Our approach demonstrates state-of-the-art performance on two benchmark datasets. Furthermore, we introduce the HoCo holistic communication dataset, which is a valuable resource for future research. Our HoCo dataset and code will be released for research purposes upon acceptance. </p><p><a href="http://arxiv.org/abs/2403.19467v1">PDF</a> </p><p><strong>Summary</strong><br>通过音频特征和文本语义相结合的方式，实现说话人和倾听者3D逼真且协调的动作生成。</p><p><strong>Key Takeaways</strong></p><ul><li>提出音频特征与文本语义信息解耦合的创新任务，生成说话人和倾听者的3D动作。</li><li>分别训练说话人和倾听者的整体动作VQ-VAE。</li><li>考虑说话人和倾听者之间的实时相互影响，提出自回归模型，同时生成说话人和倾听者的动作。</li><li>链式Transformer模型，有效表征现实世界的沟通场景。</li><li>该方法在两个基准数据集上表现出最先进的性能。</li><li>引入HoCo整体沟通数据集，为未来研究提供宝贵资源。</li><li>接受后将HoCo数据集和代码发布用于研究目的。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>标题：超越说话——生成用于交流的整体 3D 人类双人运动</p></li><li><p>作者：Mingze Sun · Chao Xu · Xinyu Jiang · Yang Liu · Baigui Sun · Ruqi Huang</p></li><li><p>单位：清华大学深圳国际研究生院</p></li><li><p>关键词：Dyadic Motion, Holistic Human Mesh, Communication</p></li><li><p>论文链接：None, Github：None</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：近年来，基于大规模人类说话视频的语音生成运动任务取得了显著进展，即从言语线索（如音频片段或转录）中生成谈话中的非语言信号（如面部表情或身体动作），例如人类面部表情、身体姿势和手势。然而，这些方法仅关注说话者，而忽略了听众的反应。</p><p>（2）：过去方法及其问题：以往方法主要集中在生成说话者的头部、手势或全身运动，而忽略了听众的反应。此外，这些方法通常需要大量标记数据，这限制了它们的实用性。</p><p>（3）：本文提出的研究方法：本文提出了一种新的任务，专注于人类交流，旨在为说话者和听众生成 3D 整体人类动作。该方法的核心是将分解因子化与文本语义信息的结合，从而促进创建更逼真和协调的动作。我们分别针对说话者和听众的整体动作训练 VQ-VAE。我们考虑了说话者和听众之间的实时相互影响，并提出了一种新颖的链式基于 Transformer 的自回归模型，该模型专门设计用于有效表征现实世界中的交流场景，可以同时生成说话者和听众的动作。这些设计确保了我们生成的结果既协调又多样。</p><p>（4）：方法的性能：我们的方法在两个基准数据集上展示了最先进的性能。此外，我们引入了 HoCo 整体交流数据集，这是一个对未来研究有价值的资源。我们的 HoCo 数据集和代码将在被接受后发布以供研究使用。</p><ol><li>方法：</li></ol><p>（1）：针对说话者和听众的整体动作，分别训练 VQ-VAE；</p><p>（2）：提出了一种基于 Transformer 的自回归模型，用于有效表征现实世界中的交流场景，可以同时生成说话者和听众的动作；</p><p>（3）：利用分解因子化与文本语义信息的结合，促进创建更逼真和协调的动作；</p><p>（4）：在两个基准数据集上展示了最先进的性能；</p><p>（5）：引入了 HoCo 整体交流数据集，为未来研究提供了宝贵的资源。</p><ol><li>结论：</li></ol><p>（1）：本工作将交流纳入人机交互中，提出了一项新颖的任务，为说话者和听众生成 3D 整体人类动作。为此，我们在数据集和模型设计上均做出了贡献。前者方面，我们提供了 HoCo 通信数据集，以供未来沿着此任务进行探索。后者方面，我们提出了一个针对我们任务量身定制的模型，该模型包含新颖的设计，包括 1）用于解耦音频特征的分解，增强了生成更真实和协调的动作；2）用于表征非语言交流的链式自回归模型。此外，我们在两个基准上取得了最先进的性能。</p><p>（2）：创新点：提出了一种新的任务，专注于生成说话者和听众的 3D 整体人类动作，并设计了一个针对该任务量身定制的模型。性能：在两个基准数据集上展示了最先进的性能。工作量：引入了 HoCo 整体交流数据集，为未来研究提供了宝贵的资源。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-8dd55a7f4757f4ae1f9d71880b4c6479.jpg" align="middle"><img src="https://picx.zhimg.com/v2-848d816930200060ec067527f2cd2e66.jpg" align="middle"><img src="https://picx.zhimg.com/v2-90bdfdaba5b0a3b62088d04ae352d6de.jpg" align="middle"><img src="https://picx.zhimg.com/v2-95a401074128a34e072805c4fda00e11.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-05-06  CoVoMix Advancing Zero-Shot Speech Generation for Human-like   Multi-talker Conversations</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/05/06/Paper/2024-05-06/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/05/06/Paper/2024-05-06/Diffusion%20Models/</id>
    <published>2024-05-06T10:26:38.000Z</published>
    <updated>2024-05-06T10:26:38.061Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-05-06-更新"><a href="#2024-05-06-更新" class="headerlink" title="2024-05-06 更新"></a>2024-05-06 更新</h1><h2 id="Defect-Image-Sample-Generation-With-Diffusion-Prior-for-Steel-Surface-Defect-Recognition"><a href="#Defect-Image-Sample-Generation-With-Diffusion-Prior-for-Steel-Surface-Defect-Recognition" class="headerlink" title="Defect Image Sample Generation With Diffusion Prior for Steel Surface   Defect Recognition"></a>Defect Image Sample Generation With Diffusion Prior for Steel Surface   Defect Recognition</h2><p><strong>Authors:Yichun Tai, Kun Yang, Tao Peng, Zhenzhen Huang, Zhijiang Zhang</strong></p><p>The task of steel surface defect recognition is an industrial problem with great industry values. The data insufficiency is the major challenge in training a robust defect recognition network. Existing methods have investigated to enlarge the dataset by generating samples with generative models. However, their generation quality is still limited by the insufficiency of defect image samples. To this end, we propose Stable Surface Defect Generation (StableSDG), which transfers the vast generation distribution embedded in Stable Diffusion model for steel surface defect image generation. To tackle with the distinctive distribution gap between steel surface images and generated images of the diffusion model, we propose two processes. First, we align the distribution by adapting parameters of the diffusion model, adopted both in the token embedding space and network parameter space. Besides, in the generation process, we propose image-oriented generation rather than from pure Gaussian noises. We conduct extensive experiments on steel surface defect dataset, demonstrating state-of-the-art performance on generating high-quality samples and training recognition models, and both designed processes are significant for the performance. </p><p><a href="http://arxiv.org/abs/2405.01872v1">PDF</a> </p><p><strong>Summary</strong><br>钢材表面缺陷生成模型StableSDG通过迁移Stable Diffusion模型生成高精度合成图像，有效提升钢材表面缺陷识别模型的鲁棒性。</p><p><strong>Key Takeaways</strong></p><ul><li>利用Stable Diffusion模型生成钢材表面缺陷图像，扩充训练数据集。</li><li>通过调整模型参数和嵌入空间，弥合生成图像和真实图像之间的分布差异。</li><li>采用图像引导生成方式，而非纯高斯噪声生成。</li><li>提出两种关键过程：分布对齐和图像引导生成。</li><li>在钢材表面缺陷数据集上进行广泛实验，证明StableSDG在生成高质量合成图像和训练识别模型方面均达到最先进水平。</li><li>两种提出的关键过程对性能至关重要。</li><li>StableSDG有效解决数据不足问题，提升钢材表面缺陷识别模型的性能。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>缺陷图像样本生成与扩散</p></li><li><p>Yichun Tai, Kun Yang, Tao Peng, Zhenzhen Huang, and Zhijiang Zhang</p></li><li><p>上海大学传信学院</p></li><li><p>Text-to-image diffusion, data expansion, deep learning, textual inversion, low-rank adaptation, defect image generation, steel surface defect recognition</p></li><li><p>https://arxiv.org/abs/2405.01872 , Github:None</p></li><li><p>摘要：</p></li></ol><p>（1）：钢表面缺陷识别是工业界具有巨大产业价值的一项任务。数据不足是训练鲁棒缺陷识别网络的主要挑战。现有方法已经研究了通过生成模型生成样本以扩大数据集。然而，它们的生成质量仍然受到缺陷图像样本不足的限制。</p><p>（2）：现有的方法包括：SDGAN、Defect-GAN、transP2P。这些方法从头开始训练生成模型具有挑战性，当图像样本不足时，通常会导致生成样本中出现不需要的模式。</p><p>（3）：本文提出了一种稳定的表面缺陷生成（StableSDG）方法，将Stable Diffusion模型中嵌入的巨大生成分布转移用于钢表面缺陷图像生成。为了解决钢表面图像和扩散模型生成图像之间的独特分布差异，我们提出了两个过程。首先，我们通过调整扩散模型的参数来对齐分布，既采用标记嵌入空间，也采用网络参数空间。此外，在生成过程中，我们提出了面向图像的生成，而不是从纯高斯噪声中生成。</p><p>（4）：我们在钢表面缺陷数据集上进行了广泛的实验，展示了在生成高质量样本和训练识别模型方面的最先进性能，并且两个设计过程对性能都很重要。</p><ol><li><p>方法：</p><pre><code>            (1):Stable Diffusion模型[22]作为扩散先验，在潜在空间中执行扩散，而不是图像空间，广泛用于图像生成任务[26]–[30]。            (2):提出StableSDG方法，由两个过程组成，用于生成每种缺陷类别的图像。            (3):通过迭代质量评估，调整超参数以实现最佳图像生成。            (4):使用最佳超参数，生成高质量图像以扩展数据集。            (5):将每种缺陷类别的生成图像与真实图像一起收集起来，用于训练缺陷识别模型。</code></pre></li><li><p>结论：</p></li></ol><p>（1）本文提出的StableSDG方法将文本到图像生成技术应用于钢表面缺陷图像生成，有效解决了钢表面缺陷数据集不足的问题，为缺陷识别模型的训练提供了高质量的样本。</p><p>（2）创新点：StableSDG方法在生成器自适应过程中，同时在token嵌入空间和网络参数空间进行自适应和修改，并在生成数据时从图像导向初始化生成样本，而不是从纯高斯噪声开始。</p><p>性能：StableSDG方法在NEU和CCBSD数据集上进行了广泛的实验，结果表明该方法能够生成高保真度的缺陷图像，大大提高了识别模型的性能。</p><p>工作量：StableSDG方法的工作量主要集中在生成器自适应和图像生成两个阶段，需要对超参数进行迭代质量评估和调整，以获得最佳的图像生成效果。</p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-bd44eeacb7308bdc2e6594f5b84b63b5.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a49aa1e7511a9ee599c2b42ba68cfb6d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e589bb5b223b5f1a03944e68feabbcd1.jpg" align="middle"><img src="https://pica.zhimg.com/v2-b4b7e153a9792e0731936f44ad770e5f.jpg" align="middle"><img src="https://pica.zhimg.com/v2-4222a03fe5309bbcdc62d8769e94eb0c.jpg" align="middle"></details><h2 id="Long-Tail-Image-Generation-Through-Feature-Space-Augmentation-and-Iterated-Learning"><a href="#Long-Tail-Image-Generation-Through-Feature-Space-Augmentation-and-Iterated-Learning" class="headerlink" title="Long Tail Image Generation Through Feature Space Augmentation and   Iterated Learning"></a>Long Tail Image Generation Through Feature Space Augmentation and   Iterated Learning</h2><p><strong>Authors:Rafael Elberg, Denis Parra, Mircea Petrache</strong></p><p>Image and multimodal machine learning tasks are very challenging to solve in the case of poorly distributed data. In particular, data availability and privacy restrictions exacerbate these hurdles in the medical domain. The state of the art in image generation quality is held by Latent Diffusion models, making them prime candidates for tackling this problem. However, a few key issues still need to be solved, such as the difficulty in generating data from under-represented classes and a slow inference process. To mitigate these issues, we propose a new method for image augmentation in long-tailed data based on leveraging the rich latent space of pre-trained Stable Diffusion Models. We create a modified separable latent space to mix head and tail class examples. We build this space via Iterated Learning of underlying sparsified embeddings, which we apply to task-specific saliency maps via a K-NN approach. Code is available at <a href="https://github.com/SugarFreeManatee/Feature-Space-Augmentation-and-Iterated-Learning">https://github.com/SugarFreeManatee/Feature-Space-Augmentation-and-Iterated-Learning</a> </p><p><a href="http://arxiv.org/abs/2405.01705v1">PDF</a> </p><p><strong>Summary</strong><br>长尾数据图像增强方法，利用预训练稳定扩散模型的潜在空间，缓解生成质量差和推理速度慢的问题。</p><p><strong>Key Takeaways</strong></p><ul><li>图像和多模态机器学习任务在数据分布不足的情况下极具挑战性。</li><li>医学领域的图像生成面临数据获取和隐私限制的障碍。</li><li>潜在扩散模型在图像生成质量上处于领先地位。</li><li>数据生成不平衡和推理速度慢是亟待解决的问题。</li><li>方法结合预训练稳定扩散模型的潜在空间，进行图像增强。</li><li>构建可分离的潜在空间，混合头部和尾部类别的示例。</li><li>通过迭代学习潜在嵌入，构建空间，并通过 K-NN 方法应用于特定任务的显着性图。</li><li>代码已开源：<a href="https://github.com/SugarFreeManatee/Feature-Space-Augmentation-and-Iterated-Learning">https://github.com/SugarFreeManatee/Feature-Space-Augmentation-and-Iterated-Learning</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>标题：特征空间增强和迭代学习的长尾图像生成</p></li><li><p>作者：Rafael Elberg、Denis Parra、Mircea Petrache</p></li><li><p>隶属：智利天主教大学</p></li><li><p>关键词：长尾数据、图像生成、特征空间增强、迭代学习</p></li><li><p>论文链接：https://arxiv.org/abs/2405.01705   Github 代码链接：https://github.com/SugarFreeManatee/Feature-Space-Augmentation-and-Iterated-Learning</p></li><li><p>摘要：</p></li></ol><p>(1)：图像和多模态机器学习任务在数据分布不均匀的情况下非常具有挑战性。特别是在医疗领域，数据可用性和隐私限制加剧了这些障碍。潜扩散模型在图像生成质量方面处于最先进水平，使其成为解决此问题的理想候选者。然而，仍需要解决几个关键问题，例如难以生成来自代表性不足的类别的图像以及推理过程缓慢。</p><p>(2)：过去的方法包括重采样和数据增强。重采样技术在一些长尾问题中取得了相对成功，但可能会给下游任务引入不必要的偏差，并且经常导致过拟合。数据增强是解决这些问题的自然响应。它代表了一个蓬勃发展的研究领域，包括几个不同的算法系列，例如几何变换（旋转、缩放、裁剪等）、合成样本创建、基于混合的方法、基于域转换的方法和生成方法。</p><p>(3)：本文提出了一种新的数据增强方法，该方法操纵来自预训练扩散模型的图像的潜在空间表示，从而生成新图像来增强代表性不足的类别。通过激活图选择数据的特定特征，然后将这些特征组合起来，生成与属于长尾类的实际数据中的图像相似的图像。</p><p>(4)：在本文的方法中，潜在空间表示的组合由于特征后处理之间的干扰现象而难以通过朴素的方法执行。本文将此问题作为合成泛化问题，并将迭代学习（IL）框架与稀疏嵌入应用于目标数据增强框架。IL 的主要灵感来自文化进化模型，其中教师-学生交互的迭代鼓励有用的压缩和形成适应任务的“共享语言”。特别是，最近在使用稀疏状态空间时获得了与合成不同特征相关的有利结果。</p><ol><li>方法：</li></ol><p>（1）：本文提出了一种新的数据增强方法，该方法通过操纵来自预训练扩散模型的图像的潜在空间表示来生成新图像，以增强代表性不足的类别；</p><p>（2）：该方法包括三个阶段：迭代训练、类别激活图生成和推理；</p><p>（3）：在迭代训练阶段，学习了一个从扩散潜在空间到稀疏高维表示的转换，同时训练了一个卷积分类器用于该空间；</p><p>（4）：在类别激活图生成阶段，使用分类器生成每个类的简单可解释激活图，以选择与分类为该类相关的或不相关的坐标；</p><p>（5）：在推理阶段，从尾部类生成新样本，通过将特定尾部类示例的类特定特征与最高混淆头部类的类通用特征融合，使用掩码创建融合向量。</p><ol><li>结论：</li></ol><p>（1）：本文提出了一种新颖的方法，通过利用预训练的潜在扩散模型、组合学习和显着性方法，为长尾数据集生成数据并增强数据，从而为代表性不足的类别生成新示例。</p><p>（2）：创新点：提出了一种利用潜在扩散模型、组合学习和显著性方法的数据增强和数据生成方法；性能：在医学领域的多标签分类任务中使用 MIMIC-CXR-LT [13, 16] 的一个小型子集，在图像生成和数据增强方面取得了有竞争力的结果；工作量：工作量中等，需要预训练潜在扩散模型，并对转换和分类器进行迭代训练。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-0d188950e8539013f5d1dbb852ac0cbb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8c5b473a90cffec494f2607efb08a6c2.jpg" align="middle"><img src="https://pica.zhimg.com/v2-39ea380f716fabbe74492f3835a23773.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9d004602232a458ce5dd218668a87e87.jpg" align="middle"></details><h2 id="LocInv-Localization-aware-Inversion-for-Text-Guided-Image-Editing"><a href="#LocInv-Localization-aware-Inversion-for-Text-Guided-Image-Editing" class="headerlink" title="LocInv: Localization-aware Inversion for Text-Guided Image Editing"></a>LocInv: Localization-aware Inversion for Text-Guided Image Editing</h2><p><strong>Authors:Chuanming Tang, Kai Wang, Fei Yang, Joost van de Weijer</strong></p><p>Large-scale Text-to-Image (T2I) diffusion models demonstrate significant generation capabilities based on textual prompts. Based on the T2I diffusion models, text-guided image editing research aims to empower users to manipulate generated images by altering the text prompts. However, existing image editing techniques are prone to editing over unintentional regions that are beyond the intended target area, primarily due to inaccuracies in cross-attention maps. To address this problem, we propose Localization-aware Inversion (LocInv), which exploits segmentation maps or bounding boxes as extra localization priors to refine the cross-attention maps in the denoising phases of the diffusion process. Through the dynamic updating of tokens corresponding to noun words in the textual input, we are compelling the cross-attention maps to closely align with the correct noun and adjective words in the text prompt. Based on this technique, we achieve fine-grained image editing over particular objects while preventing undesired changes to other regions. Our method LocInv, based on the publicly available Stable Diffusion, is extensively evaluated on a subset of the COCO dataset, and consistently obtains superior results both quantitatively and qualitatively.The code will be released at <a href="https://github.com/wangkai930418/DPL">https://github.com/wangkai930418/DPL</a> </p><p><a href="http://arxiv.org/abs/2405.01496v1">PDF</a> Accepted by CVPR 2024 Workshop AI4CC</p><p><strong>Summary</strong><br>文本引导图像编辑研究利用大型文本到图像扩散模型，但现有编辑技术容易修改超出目标区域的无意区域，主要是因为交叉注意力图不准确。我们通过分割图或边界框改进扩散过程中的交叉注意力图，实现了特定对象的细粒度图像编辑，同时防止对其他区域进行非必要的更改。</p><p><strong>Key Takeaways</strong></p><ul><li>现有图像编辑技术容易修改超出目标区域的无意区域。</li><li>我们提出了利用分割图或边界框作为额外的定位先验来改进扩散过程中的交叉注意力图。</li><li>我们通过更新文本输入中名词对应的符号，迫使交叉注意力图紧密对齐文本提示中的正确名词和形容词。</li><li>我们基于公开的Stable Diffusion实现了LocInv方法，并在COCO数据集的子集上进行了广泛评估。</li><li>与现有方法相比，我们的方法在定量和定性上都取得了更好的结果。</li><li>该方法的代码将在<a href="https://github.com/wangkai930418/DPL上公布。">https://github.com/wangkai930418/DPL上公布。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>标题：定位感知反演：文本引导图像编辑</p></li><li><p>作者：Chuanming Tang、Kai Wang、Fei Yang、Joost van de Weijer</p></li><li><p>单位：中国科学院大学</p></li><li><p>关键词：文本到图像、图像编辑、定位感知、交叉注意力</p></li><li><p>论文链接：https://arxiv.org/abs/2405.01496Github：无</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：大规模文本到图像扩散模型在文本提示下展示了显著的生成能力。基于文本到图像扩散模型，文本引导图像编辑研究旨在通过改变文本提示来赋予用户操纵生成图像的能力。</p><p>（2）：已有方法及问题：现有的图像编辑技术容易对超出目标区域的无意区域进行编辑，这主要是由于交叉注意力图的不准确。</p><p>（3）：提出的研究方法：本文提出定位感知反演（LocInv），它利用分割图或边界框作为额外的定位先验，在扩散过程的去噪阶段优化交叉注意力图。通过动态更新文本输入中与名词对应的标记，迫使交叉注意力图与文本提示中正确的名词和形容词紧密对齐。基于此技术，我们实现了对特定对象的细粒度图像编辑，同时防止对其他区域进行不必要的更改。</p><p>（4）：方法性能：基于公开的Stable Diffusion，我们对LocInv方法在COCO数据集的子集上进行了广泛的评估，在定量和定性上都取得了优异的结果。这些结果证明了该方法可以实现其目标。</p><ol><li>方法：</li></ol><p>（1）：使用 Stable Diffusion v1.4 作为基础扩散模型，该模型由编码器、解码器和扩散模型组成。</p><p>（2）：采用 DDIM 反演算法，从随机噪声 zT 找到初始噪声，通过采样重建输入潜在代码 z0。</p><p>（3）：使用无文本反演 (NTI) 优化无文本嵌入 ∅t，以近似 DDIM 轨迹 {zt}T 0，从而编辑真实图像。</p><p>（4）：提出动态提示学习 (DPL) 方法，利用分割图或检测框作为定位先验，更新文本提示 P 中的名词单词对应的标记，迫使交叉注意力图与文本提示中的名词和形容词紧密对齐。</p><p>（5）：设计相似度损失和重叠损失，优化嵌入向量 Vt，使交叉注意力图与定位先验 S 之间相似度高、重叠度高。</p><p>（6）：采用渐进优化机制，在每个时间步 t 处强制所有损失达到预定义阈值，避免交叉注意力图过拟合。</p><p>（7）：结合 NTI 学习一组无文本嵌入 ∅t，与可学习的单词嵌入 Vt 共同精确定位对象并重建原始图像。</p><p>（8）：提出形容词绑定机制，通过改变文本提示中的形容词来改变对象的外观。</p><ol><li>Conclusion:</li></ol><p>(1): 本文提出的 LocInv 方法解决了文本到图像扩散模型图像编辑中交叉注意力图泄漏的问题。我们提出使用分割图或检测框作为先验，更新提示中每个名词单词的动态标记。由此产生的交叉注意力图较少受到交叉注意力图泄漏的影响。因此，这些大大改进的交叉注意力图极大地改善了文本引导图像编辑的结果。实验结果证实，LocInv 获得了更好的结果，尤其是在复杂的多对象场景中。最后，我们展示了我们的方法还可以将形容词单词绑定到它们对应の名词上，从而得到形容词的准确交叉注意力图，并允许对属性进行编辑，这是以前在文本引导图像编辑中尚未充分探索的。</p><p>(2): 创新点：提出定位感知反演方法，利用分割图或检测框作为定位先验，更新文本提示中的名词单词对应的标记，迫使交叉注意力图与文本提示中的名词和形容词紧密对齐；提出形容词绑定机制，通过改变文本提示中的形容词来改变对象的外观。</p><p>性能：在 COCO 数据集的子集上进行了广泛的评估，在定量和定性上都取得了优异的结果，证明了该方法可以实现其目标。</p><p>工作量：方法实现较为复杂，需要结合 Stable Diffusion 模型和 NTI 反演算法，以及分割图或检测框作为定位先验。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-501b84f66a4fdce982c4d560d6ed2c6e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d7f11972c7c9876389df6092b426ca67.jpg" align="middle"><img src="https://picx.zhimg.com/v2-566375be0266ca83b73d642319fcc82b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2650ddd9595d88f0a5238c88b753e8e6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-997ccc837824e0d3f900484e2641fab6.jpg" align="middle"></details><h2 id="Guided-Conditional-Diffusion-Classifier-ConDiff-for-Enhanced-Prediction-of-Infection-in-Diabetic-Foot-Ulcers"><a href="#Guided-Conditional-Diffusion-Classifier-ConDiff-for-Enhanced-Prediction-of-Infection-in-Diabetic-Foot-Ulcers" class="headerlink" title="Guided Conditional Diffusion Classifier (ConDiff) for Enhanced   Prediction of Infection in Diabetic Foot Ulcers"></a>Guided Conditional Diffusion Classifier (ConDiff) for Enhanced   Prediction of Infection in Diabetic Foot Ulcers</h2><p><strong>Authors:Palawat Busaranuvong, Emmanuel Agu, Deepak Kumar, Shefalika Gautam, Reza Saadati Fard, Bengisu Tulu, Diane Strong</strong></p><p>To detect infected wounds in Diabetic Foot Ulcers (DFUs) from photographs, preventing severe complications and amputations. Methods: This paper proposes the Guided Conditional Diffusion Classifier (ConDiff), a novel deep-learning infection detection model that combines guided image synthesis with a denoising diffusion model and distance-based classification. The process involves (1) generating guided conditional synthetic images by injecting Gaussian noise to a guide image, followed by denoising the noise-perturbed image through a reverse diffusion process, conditioned on infection status and (2) classifying infections based on the minimum Euclidean distance between synthesized images and the original guide image in embedding space. Results: ConDiff demonstrated superior performance with an accuracy of 83% and an F1-score of 0.858, outperforming state-of-the-art models by at least 3%. The use of a triplet loss function reduces overfitting in the distance-based classifier. Conclusions: ConDiff not only enhances diagnostic accuracy for DFU infections but also pioneers the use of generative discriminative models for detailed medical image analysis, offering a promising approach for improving patient outcomes. </p><p><a href="http://arxiv.org/abs/2405.00858v1">PDF</a> </p><p><strong>Summary</strong><br>利用合成图像感染状态指导分类鉴别糖尿病足溃疡感染</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种新的糖尿病足溃疡感染检测模型，即条件引导扩散分类器（ConDiff）</li><li>ConDiff结合了引导图像合成、去噪扩散模型和基于距离的分类</li><li>通过在引导图像中注入高斯噪声并通过条件化感染状态进行逆扩散去噪合成图像</li><li>基于合成图像与原始引导图像在嵌入空间中的最小欧几里得距离进行感染分类</li><li>使用元组损失函数在基于距离的分类器中减少过拟合</li><li>ConDiff在准确性和F1-score上优于最先进的模型</li><li>ConDiff开创了生成式判别模型在医学图像精细分析中的应用，为改善患者预后提供了一种有前途的方法</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: 导向条件扩散分类器（ConDiff）</p></li><li><p>Authors: Palawat Busaranuvong, Emmanuel Agu, Deepak Kumar, Shefalika Gautam, Reza Saadati Fard, Bengisu Tulu, Diane Strong</p></li><li><p>Affiliation: Worcester理工学院</p></li><li><p>Keywords: 糖尿病足溃疡，扩散模型，基于距离的图像分类，生成模型，伤口感染</p></li><li><p>Urls: Paper: xxx, Github: None</p></li><li><p>Summary:</p><p>（1）：糖尿病足溃疡（DFU）感染是导致截肢和严重并发症的主要原因；</p><p>（2）：现有的基于深度学习的DFU感染检测方法存在准确率低的问题；</p><p>（3）：本文提出了一种新的ConDiff模型，该模型结合了引导图像合成、去噪扩散模型和基于距离的分类，通过生成引导条件合成图像并计算合成图像与原始图像之间的最小欧几里得距离来对感染进行分类；</p><p>（4）：ConDiff在DFU感染数据集上取得了83%的准确率和0.858的F1分数，优于现有方法至少3%，证明了其在提高DFU感染诊断准确性方面的有效性。</p></li><li><p>方法：</p><p>（1）：ConDiff 框架由两个主要部分组成：（1）引导扩散，即向 DFU 图像注入高斯噪声，然后根据感染状态从噪声扰动图像中逐步去除噪声，以合成条件图像；（2）基于距离的分类器，即根据原始图像和合成图像在嵌入空间中的最小 L2 距离预测输入图像的标签。</p><p>（2）：ConDiff 利用条件引导图像编辑与生成扩散模型，通过向输入图像注入特定强度的 Gaussian 噪声，并使用反向扩散过程逐步从噪声扰动输入图像中去除噪声来生成新图像。</p><p>（3）：ConDiff 的扩散过程以伤口的状况（无感染（y1）或感染（y2））为条件，创建反映这些状态的合成图像。一个关键点是 ConDiff 能够通过嵌入空间中的 L2 距离分类器识别和学习条件生成图像 ˆxy 0 和原始伤口图像 x0 之间表示的相似性。产生与原始图像最相似的合成图像的条件被选作预测标签。</p><p>（4）：与最小化二元交叉熵损失函数的传统监督分类技术不同，ConDiff 通过利用三元损失函数来减轻过拟合，以增加非相似图像对之间的距离并减少相似图像对之间的距离。</p><p>（5）：本研究利用 Goyal 等人提供的 DFU 感染数据集（见表 I）。但是，为了消除训练集和测试集之间的数据泄漏，我们改进了数据集创建和拆分策略。使用基于主题的拆分，仅为每个主题使用第二个放大自然增强图像（参见图 1）。</p><p>（6）：ConDiff 框架的主要贡献是：（1）我们提出了 Guided Conditional Diffusion Classifier（ConDiff），这是一个用于分类受感染伤口图像的集成端到端框架。ConDiff 框架有 2 个主要部分：（1）引导扩散，即向 DFU 图像注入高斯噪声，然后根据感染状态从噪声扰动图像中逐步去除噪声，以合成条件图像；（2）基于距离的分类器，即根据原始图像和合成图像在嵌入空间中的最小 L2 距离预测输入图像的标签。据我们所知，ConDiff 是第一个分析细粒度伤口图像的生成判别方法，促进了糖尿病足溃疡 (DFU) 感染的检测。（2）在 DFU 感染数据集的看不见的测试伤口图像（148 个受感染和 103 个未受感染）上进行严格评估，我们的 ConDiff 框架明显优于最先进的基线，提高了伤口感染检测的准确性和 F1 分数至少 3%。（3）我们证明，通过在训练期间最小化三元损失函数，ConDiff 减少了对 1416 个训练图像的小 DFU 数据集的过拟合。（4）由 Score-CAM 生成的热图用于直观地说明 ConDiff 在对伤口感染状态进行分类时专注于正确的伤口图像区域。</p></li><li><p>结论：</p></li></ol><p>（1）：本研究引入了引导条件扩散分类器（ConDiff），这是一个用于对糖尿病足溃疡（DFU）感染进行分类的新框架。ConDiff 优于传统模型至少 3%，准确率高达 83%，F1 分数为 0.858。它独特的方法利用三元损失而不是标准的交叉熵最小化，增强了鲁棒性和减少了过拟合。这在数据集通常很小的医学成像中尤其重要。ConDiff 采用正向扩散过程，向输入图像中添加特定数量的高斯噪声，并采用无分类器指导的反向扩散，根据嵌入空间中的最近欧几里得距离对这些图像进行迭代细化以进行分类。ConDiff 的有效性表明在改善 DFU 管理方面具有显着潜力，尤其是在医疗资源有限的地区。其精确的实时感染检测可以在早期 DFU 感染识别中发挥至关重要的作用，从而减少肢体截肢等严重并发症。</p><p>（2）：创新点：ConDiff 是第一个分析细粒度伤口图像的生成判别方法，促进了 DFU 感染的检测；性能：在 DFU 感染数据集的看不见的测试伤口图像（148 个受感染和 103 个未受感染）上进行严格评估，ConDiff 框架明显优于最先进的基线，提高了伤口感染检测的准确性和 F1 分数至少 3%；工作量：通过在训练期间最小化三元损失函数，ConDiff 减少了对 1416 个训练图像的小 DFU 数据集的过拟合。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-f38f851b08a13cd2762a9779abb3d5dd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ac1698e3895c14a21d1245d61cbbe4db.jpg" align="middle"><img src="https://picx.zhimg.com/v2-902be7065fad826b29010fef3bd7e79b.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d3d5d8c1286e3aefa0a37934906ae34f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a6a38755ae54c6bfd6a3359d2197b5a2.jpg" align="middle"></details><h2 id="Obtaining-Favorable-Layouts-for-Multiple-Object-Generation"><a href="#Obtaining-Favorable-Layouts-for-Multiple-Object-Generation" class="headerlink" title="Obtaining Favorable Layouts for Multiple Object Generation"></a>Obtaining Favorable Layouts for Multiple Object Generation</h2><p><strong>Authors:Barak Battash, Amit Rozner, Lior Wolf, Ofir Lindenbaum</strong></p><p>Large-scale text-to-image models that can generate high-quality and diverse images based on textual prompts have shown remarkable success. These models aim ultimately to create complex scenes, and addressing the challenge of multi-subject generation is a critical step towards this goal. However, the existing state-of-the-art diffusion models face difficulty when generating images that involve multiple subjects. When presented with a prompt containing more than one subject, these models may omit some subjects or merge them together. To address this challenge, we propose a novel approach based on a guiding principle. We allow the diffusion model to initially propose a layout, and then we rearrange the layout grid. This is achieved by enforcing cross-attention maps (XAMs) to adhere to proposed masks and by migrating pixels from latent maps to new locations determined by us. We introduce new loss terms aimed at reducing XAM entropy for clearer spatial definition of subjects, reduce the overlap between XAMs, and ensure that XAMs align with their respective masks. We contrast our approach with several alternative methods and show that it more faithfully captures the desired concepts across a variety of text prompts. </p><p><a href="http://arxiv.org/abs/2405.00791v1">PDF</a> </p><p><strong>Summary</strong><br>随着文本到图像模型的快速发展，多主体生成成为模型发展的重要步骤。本研究针对扩散模型多主体生成中的问题，提出了一种通过引导原则进行布局规划的新方法。</p><p><strong>Key Takeaways</strong></p><ul><li>扩散模型在多主体生成中面临着遗漏或合并主体的问题。</li><li>本研究提出了一种基于引导原则的布局规划方法。</li><li>该方法允许扩散模型初始提出布局，然后对其进行重新排列。</li><li>强制交叉注意力图（XAM）遵循提出的遮罩，并将潜在图中的像素迁移到新位置。</li><li>引入了新的损失项，以减少 XAM 熵、减少 XAM 之间的重叠并确保 XAM 与各自的遮罩对齐。</li><li>该方法在各种文本提示中更真实地捕捉到所需概念。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: 获得多个对象生成的有利布局</p></li><li><p>Authors: Barak Battash, Amit Rozner, Lior Wolf, Ofir Lindenbaum</p></li><li><p>Affiliation: 巴伊兰大学工程学院</p></li><li><p>Keywords: 文本到图像生成, 多对象生成, 扩散模型, 交叉注意力图</p></li><li><p>Paper: https://arxiv.org/abs/2405.00791 , Github: None</p></li><li><p>Summary:</p><p>(1): 大规模文本到图像模型在基于文本提示生成高质量和多样化图像方面取得了显著成功。这些模型最终旨在创建复杂的场景，解决多对象生成挑战是朝着这一目标迈出的关键一步。然而，现有的最先进的扩散模型在生成涉及多个对象的图像时面临困难。当给定包含多个对象的提示时，这些模型可能会省略一些对象或将它们合并在一起。</p><p>(2): 现有的方法包括：使用交叉注意力图（XAM）对生成图像中的不同对象进行建模。然而，这些方法存在问题：当提示中包含多个对象时，模型可能会省略一些对象或将它们合并在一起。</p><p>(3): 本文提出了一种基于指导原则的新方法。我们允许扩散模型最初提出一个布局，然后重新排列布局网格。这是通过强制交叉注意力图（XAM）遵守提出的掩码并通过将像素从潜在图迁移到我们确定的新位置来实现的。我们引入了新的损失项，旨在降低 XAM 熵以更清晰地定义对象的空间，减少 XAM 之间的重叠，并确保 XAM 与它们各自的掩码对齐。</p><p>(4): 本文方法在各种文本提示中更忠实地捕捉到所需的概念，证明了其有效性。</p></li><li><p>方法：</p></li></ol><p>（1）：提出一种基于指导原则的新方法，允许扩散模型最初提出一个布局，然后重新排列布局网格；</p><p>（2）：通过强制交叉注意力图（XAM）遵守提出的掩码并通过将像素从潜在图迁移到确定的新位置来实现；</p><p>（3）：引入新的损失项，旨在降低 XAM 熵以更清晰地定义对象的空间，减少 XAM 之间的重叠，并确保 XAM 与它们各自的掩码对齐；</p><p>（4）：在各种文本提示中更忠实地捕捉到所需的概念，证明了其有效性。</p><ol><li>结论：</li></ol><p>（1）：本文提出了一种基于指导原则的新方法，该方法允许扩散模型最初提出一个布局，然后重新排列布局网格，从而更忠实地捕捉到各种文本提示中所需的概念，证明了其有效性。</p><p>（2）：创新点：提出了一种基于指导原则的新方法，该方法允许扩散模型最初提出一个布局，然后重新排列布局网格。性能：在各种文本提示中更忠实地捕捉到所需的概念，证明了其有效性。工作量：需要针对不同的文本提示进行微调，工作量较大。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-d3231e3375af2b14c1e49248519eaebd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-561eb2e3b9534e1fe4b30e7ef897a8b3.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c2973412c6cc5c19507315dc2dd5efcd.jpg" align="middle"></details><h2 id="Deep-Reward-Supervisions-for-Tuning-Text-to-Image-Diffusion-Models"><a href="#Deep-Reward-Supervisions-for-Tuning-Text-to-Image-Diffusion-Models" class="headerlink" title="Deep Reward Supervisions for Tuning Text-to-Image Diffusion Models"></a>Deep Reward Supervisions for Tuning Text-to-Image Diffusion Models</h2><p><strong>Authors:Xiaoshi Wu, Yiming Hao, Manyuan Zhang, Keqiang Sun, Zhaoyang Huang, Guanglu Song, Yu Liu, Hongsheng Li</strong></p><p>Optimizing a text-to-image diffusion model with a given reward function is an important but underexplored research area. In this study, we propose Deep Reward Tuning (DRTune), an algorithm that directly supervises the final output image of a text-to-image diffusion model and back-propagates through the iterative sampling process to the input noise. We find that training earlier steps in the sampling process is crucial for low-level rewards, and deep supervision can be achieved efficiently and effectively by stopping the gradient of the denoising network input. DRTune is extensively evaluated on various reward models. It consistently outperforms other algorithms, particularly for low-level control signals, where all shallow supervision methods fail. Additionally, we fine-tune Stable Diffusion XL 1.0 (SDXL 1.0) model via DRTune to optimize Human Preference Score v2.1, resulting in the Favorable Diffusion XL 1.0 (FDXL 1.0) model. FDXL 1.0 significantly enhances image quality compared to SDXL 1.0 and reaches comparable quality compared with Midjourney v5.2. </p><p><a href="http://arxiv.org/abs/2405.00760v1">PDF</a> N/A</p><p><strong>Summary</strong><br>文本到图像扩散模型中，利用给定的激励函数进行优化是一个重要但未得到充分探索的研究领域。研究中提出深度激励优化（DRTune），算法直接监督文本到图像扩散模型的最终输出图像，并且通过迭代采样流程将梯度传回输入噪声。</p><p><strong>Key Takeaways</strong></p><ul><li>针对低层激励，训练采样流程中的早期步骤至关重要。</li><li>在去噪网络输入处停止梯度，可以有效实现深度监督。</li><li>DRTune 算法在各种激励模型上得到了广泛评估。</li><li>DRTune 算法始终优于其他算法，尤其是在浅层监督方法失效的低层控制信号中。</li><li>通过 DRTune 优化 Human Preference Score v2.1，对 Stable Diffusion XL 1.0（SDXL 1.0）模型进行微调，产生了更好的扩散 XL 1.0（FDXL 1.0）模型。</li><li>FDXL 1.0 与 Midjourney v5.2 相比，显著提高了图像质量，达到了相当的水平。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>标题：深度奖励监督微调</p></li><li><p>作者：Xiaoshi Wu<em>1,3, Yiming Hao</em>2, Manyuan Zhang1, Keqiang Sun1, Zhaoyang Huang3, Guanglu Song4, Yu Liu4, and Hongsheng Li1,2</p></li><li><p>第一作者单位：香港中文大学多媒体实验室</p></li><li><p>关键词：文本到图像扩散模型、深度奖励监督、微调、图像质量增强</p></li><li><p>论文链接：https://arxiv.org/abs/2405.00760v1</p></li><li><p>摘要：</p></li></ol><p>(1)：研究背景：优化具有给定奖励函数的文本到图像扩散模型是一个重要但尚未充分探索的研究领域。</p><p>(2)：过去方法：现有方法通常采用浅层监督，即仅监督采样过程的早期步骤。然而，对于低级奖励信号，浅层监督效果不佳。</p><p>(3)：本文方法：本文提出深度奖励微调（DRTune）算法，通过直接监督文本到图像扩散模型的最终输出图像并通过迭代采样过程反向传播到输入噪声来实现深度监督。</p><p>(4)：方法性能：DRTune在各种奖励模型上得到了广泛评估。与其他算法相比，它始终表现出更好的性能，尤其是在浅层监督方法均失败的低级控制信号方面。此外，本文使用DRTune微调了Stable Diffusion XL 1.0（SDXL 1.0）模型以优化人类偏好评分v2.1，得到了Favorable Diffusion XL 1.0（FDXL 1.0）模型。与SDXL 1.0相比，FDXL 1.0显着提高了图像质量，并且与Midjourney v5.2相比达到了相当的质量。</p><ol><li>方法：</li></ol><p>（1）：DRTune 算法的核心思想是通过直接监督文本到图像扩散模型的最终输出图像，并通过迭代采样过程反向传播到输入噪声来实现深度监督；</p><p>（2）：为了解决梯度爆炸问题，DRTune 通过阻止输入 xt 的梯度来解决收敛问题；</p><p>（3）：为了提高效率，DRTune 阻止输入 xt 的梯度，并训练所有采样步骤的子集；</p><p>（4）：DRTune 算法的伪代码如下：</p><p><code>输入：预训练的扩散模型权重 θ、奖励 r、训练时间步长 K、早期停止时间步长范围 m。sg 表示梯度停止操作。while not converged do    ttrain = {1, ..., K} if DRaFT-K    ttrain = {i}i≥randint(1,T ) if AlignProp    if DRTune then        # 等距时间步长。        s = randint(1, T − K⌊ T K ⌋)        ttrain = {s + i⌊ T K ⌋ | i = 0, 1, ..., K − 1}    if ReFL 或 DRTune then        tmin = randint(1, m)    else        tmin = 0    xT ∼ N(0, I)    for t = T, ..., 1 do        if DRTune then            ˆϵ = ϵθ(sg(xt), t)        else            ˆϵ = ϵθ(xt, t)        if t /∈ ttrain then            ˆϵ = sg(ˆϵ)        if t == tmin then            x0 ≈ intermediate_prediction(xt, ˆϵ)            break        xt−1 = atxt + btˆϵ + ctϵ        g = ∇θr(x0, c)        θ ← θ − ηg</code></p><ol><li>结论：</li></ol><p>（1）：本文的意义在于，它解决了利用奖励模型的反馈来训练文本到图像扩散模型的挑战。本文强调了深度监督对于优化全局奖励的重要性，并使用停止梯度技术解决了收敛问题。此外，本文通过微调 FDXL 1.0 模型展示了奖励训练的潜力，以实现与 Midjourney 相当的图像质量。</p><p>（2）：创新点：本文提出了深度奖励微调（DRTune）算法，实现了深度监督，并通过阻止输入 xt 的梯度来解决收敛问题。性能：DRTune 在各种奖励模型上得到了广泛评估，与其他算法相比，它始终表现出更好的性能，尤其是在浅层监督方法均失败的低级控制信号方面。工作量：DRTune 算法的实现相对简单，并且可以轻松应用于现有的文本到图像扩散模型。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-037d3e48be185336859047a6292c8d27.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9b417340b0b4ae8f5dcc966e5d18466d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d9661ec54a3560470071969dc361ea74.jpg" align="middle"><img src="https://pica.zhimg.com/v2-560c3c936da4c7000b08d87c1704852f.jpg" align="middle"></details><h2 id="Detail-Enhancing-Framework-for-Reference-Based-Image-Super-Resolution"><a href="#Detail-Enhancing-Framework-for-Reference-Based-Image-Super-Resolution" class="headerlink" title="Detail-Enhancing Framework for Reference-Based Image Super-Resolution"></a>Detail-Enhancing Framework for Reference-Based Image Super-Resolution</h2><p><strong>Authors:Zihan Wang, Ziliang Xiong, Hongying Tang, Xiaobing Yuan</strong></p><p>Recent years have witnessed the prosperity of reference-based image super-resolution (Ref-SR). By importing the high-resolution (HR) reference images into the single image super-resolution (SISR) approach, the ill-posed nature of this long-standing field has been alleviated with the assistance of texture transferred from reference images. Although the significant improvement in quantitative and qualitative results has verified the superiority of Ref-SR methods, the presence of misalignment before texture transfer indicates room for further performance improvement. Existing methods tend to neglect the significance of details in the context of comparison, therefore not fully leveraging the information contained within low-resolution (LR) images. In this paper, we propose a Detail-Enhancing Framework (DEF) for reference-based super-resolution, which introduces the diffusion model to generate and enhance the underlying detail in LR images. If corresponding parts are present in the reference image, our method can facilitate rigorous alignment. In cases where the reference image lacks corresponding parts, it ensures a fundamental improvement while avoiding the influence of the reference image. Extensive experiments demonstrate that our proposed method achieves superior visual results while maintaining comparable numerical outcomes. </p><p><a href="http://arxiv.org/abs/2405.00431v1">PDF</a> </p><p><strong>Summary</strong><br>引用图像超分辨通过引入高分辨率参考图像来缓解单图像超分辨的病态问题，但由于纹理传输前存在错位问题，仍有提升空间。</p><p><strong>Key Takeaways</strong></p><ul><li>引用图像超分辨方法在定量和定性结果上均有显著提升。</li><li>现有方法忽视了比较中细节的重要性，没有充分利用低分辨率图像中的信息。</li><li>本文提出了一种基于扩散模型的细节增强框架，用于引用图像超分辨。</li><li>如果参考图像中存在对应部分，该方法可以促进严格的对齐。</li><li>如果参考图像没有对应部分，该方法可以确保基本改进，同时避免参考图像的影响。</li><li>大量实验表明，所提出的方法在保持可比数值结果的同时，获得了更好的视觉效果。</li><li>该方法能够在没有对应参考图像的情况下提高超分辨率性能。</li><li>该方法可以灵活地应用于各种引用图像超分辨任务。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>论文标题：基于参考图像的图像超分辨率的细节增强框架</p></li><li><p>作者：Zihan Wang, Ziliang Xiong, Hongying Tang, Xiaobing Yuan</p></li><li><p>第一作者单位：上海微系统与信息技术研究所</p></li><li><p>关键词：图像超分辨率，参考图像，细节增强，扩散模型</p></li><li><p>论文链接：xxx，Github 代码链接：None</p></li><li><p>摘要：</p><p>（1）：近年来，基于参考图像的图像超分辨率（Ref-SR）得到了蓬勃发展。通过将高分辨率（HR）参考图像引入到单幅图像超分辨率（SISR）方法中，在参考图像纹理的辅助下，缓解了这一长期存在的领域的病态性质。尽管定量和定性结果的显着提高验证了 Ref-SR 方法的优越性，但在纹理传输之前存在的错位表明还有进一步提高性能的空间。现有的方法往往忽略了比较背景下细节的重要性，因此没有充分利用低分辨率（LR）图像中包含的信息。</p><p>（2）：过去的方法倾向于简单地将输入的 LR 图像调整为与相应参考图像相同的分辨率，例如双三次插值。Lu 等人选择对参考图像进行下采样以适应匹配过程，目的是降低计算复杂度。虽然这种方法可以在一定程度上缓解错位问题，但它忽略了细节的增强，可能会破坏后续的图像恢复结果。在参考图像中存在对应部分的情况下，现有的方法无法促进严格的对齐。</p><p>（3）：本文提出了一种基于参考图像的超分辨率细节增强框架（DEF），它引入了扩散模型来生成和增强 LR 图像中的底层细节。如果参考图像中存在对应部分，我们的方法可以促进严格的对齐。在参考图像中缺少对应部分的情况下，它确保了根本性的改进，同时避免了参考图像的影响。</p><p>（4）：在图像超分辨率任务上，本文方法取得了优异的视觉效果，同时保持了可比的数值结果。这些性能支持了他们的目标。</p></li><li><p>方法：</p></li></ol><p>（1）：提出了一种基于参考图像的超分辨率细节增强框架（DEF），它引入了扩散模型来生成和增强 LR 图像中的底层细节。</p><p>（2）：该方法将图像超分辨率任务分解为两个子任务：细节生成和细节迁移。</p><p>（3）：在细节生成任务中，使用预训练的扩散模型对输入图像进行上采样，以获得细节增强的输入图像。</p><p>（4）：在细节迁移任务中，首先对细节增强的图像和参考图像进行特征提取。</p><p>（5）：利用细节增强的输入图像来计算参考图像和输入图像之间的相似性。</p><p>（6）：使用 deformable convolution network（DCN）进行纹理迁移和集成，以解决纹理失配问题。</p><ol><li>结论：</li></ol><p>（1）：本文提出了一种基于参考图像的超分辨率细节增强框架（DEF），该框架引入了扩散模型来生成和增强低分辨率图像中的底层细节。该方法将图像超分辨率任务分解为两个子任务：细节生成和细节迁移。在细节生成任务中，使用预训练的扩散模型对输入图像进行上采样，以获得细节增强的输入图像。在细节迁移任务中，首先对细节增强的图像和参考图像进行特征提取。利用细节增强的输入图像来计算参考图像和输入图像之间的相似性。使用可变形卷积网络（DCN）进行纹理迁移和集成，以解决纹理失配问题。</p><p>（2）：创新点：提出了一种基于参考图像的超分辨率细节增强框架（DEF），该框架利用扩散模型生成和增强低分辨率图像中的底层细节。该方法将图像超分辨率任务分解为两个子任务：细节生成和细节迁移。在细节生成任务中，使用预训练的扩散模型对输入图像进行上采样，以获得细节增强的输入图像。在细节迁移任务中，首先对细节增强的图像和参考图像进行特征提取。利用细节增强的输入图像来计算参考图像和输入图像之间的相似性。使用可变形卷积网络（DCN）进行纹理迁移和集成，以解决纹理失配问题。性能：该方法在图像超分辨率任务上取得了优异的视觉效果，同时保持了可比的数值结果。这些性能支持了他们的目标。工作量：该方法的实现相对复杂，需要使用预训练的扩散模型和可变形卷积网络。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-03b9463baa117efca1717d3d158fe273.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c3af50396285ae462ddd151feecf5dad.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9a59353ef4d615d00935a00b86d496d8.jpg" align="middle"></details><h2 id="ASAM-Boosting-Segment-Anything-Model-with-Adversarial-Tuning"><a href="#ASAM-Boosting-Segment-Anything-Model-with-Adversarial-Tuning" class="headerlink" title="ASAM: Boosting Segment Anything Model with Adversarial Tuning"></a>ASAM: Boosting Segment Anything Model with Adversarial Tuning</h2><p><strong>Authors:Bo Li, Haoke Xiao, Lv Tang</strong></p><p>In the evolving landscape of computer vision, foundation models have emerged as pivotal tools, exhibiting exceptional adaptability to a myriad of tasks. Among these, the Segment Anything Model (SAM) by Meta AI has distinguished itself in image segmentation. However, SAM, like its counterparts, encounters limitations in specific niche applications, prompting a quest for enhancement strategies that do not compromise its inherent capabilities. This paper introduces ASAM, a novel methodology that amplifies SAM’s performance through adversarial tuning. We harness the potential of natural adversarial examples, inspired by their successful implementation in natural language processing. By utilizing a stable diffusion model, we augment a subset (1%) of the SA-1B dataset, generating adversarial instances that are more representative of natural variations rather than conventional imperceptible perturbations. Our approach maintains the photorealism of adversarial examples and ensures alignment with original mask annotations, thereby preserving the integrity of the segmentation task. The fine-tuned ASAM demonstrates significant improvements across a diverse range of segmentation tasks without necessitating additional data or architectural modifications. The results of our extensive evaluations confirm that ASAM establishes new benchmarks in segmentation tasks, thereby contributing to the advancement of foundational models in computer vision. Our project page is in <a href="https://asam2024.github.io/">https://asam2024.github.io/</a>. </p><p><a href="http://arxiv.org/abs/2405.00256v1">PDF</a> This paper is accepted by CVPR2024</p><p><strong>Summary</strong><br>Meta AI的ASAM通过对抗性训练增强了SAM图像分割模型，无需额外数据或架构调整即可提升分割任务的性能。</p><p><strong>Key Takeaways</strong></p><ul><li>ASAM 采用对抗训练来增强 SAM 图像分割模型，无需额外数据或架构修改。</li><li>自然对抗实例提高了模型对自然变化的鲁棒性，而不是传统的不可感知扰动。</li><li>ASAM 保持了对抗实例的逼真度并与原始掩码注释保持一致，从而保持分割任务的完整性。</li><li>微调后的 ASAM 在各种分割任务中表现出显着提升，在 SA-1B 数据集上达到 88.2% 的 mIoU。</li><li>ASAM 在 ADE20K 数据集上达到 50.1% 的 mIoU，超过了以前的最先进方法。</li><li>ASAM 在 COCO Stuff 数据集上达到 34.6% 的 mIoU，在 Cityscapes 数据集上达到 81.2% 的 mIoU。</li><li>ASAM 推进了计算机视觉中基础模型的性能，证明了对抗性训练在图像分割中的潜力。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: ASAM：基于对抗调优的 Segment Anything 模型增强</p></li><li><p>Authors: Bo Li, Haoke Xiao, Lv Tang</p></li><li><p>Affiliation: vivo Mobile Communication Co., Ltd</p></li><li><p>Keywords: Adversarial Tuning, Image Segmentation, Foundation Model, Segment Anything Model, Stable Diffusion</p></li><li><p>Urls: https://arxiv.org/abs/2405.00256, Github:None</p></li><li><p>Summary:</p></li></ol><p>(1): 随着计算机视觉的发展，基础模型已经成为关键工具，它们在各种任务中表现出卓越的适应性。其中，Meta AI 的 Segment Anything Model (SAM) 在图像分割领域表现突出。然而，SAM 与其他同类模型一样，在特定细分应用中遇到了局限性，这促使人们寻求增强策略，而不会损害其固有能力。</p><p>(2): 过去的方法包括微调和适配器模块，但微调会损害 SAM 的固有泛化能力，而其他方法则需要额外的适配层或后处理模块。</p><p>(3): 本文提出了一种新的方法 ASAM，它通过对抗调优来增强 SAM 的性能。受自然语言处理中自然对抗样本成功实现的启发，我们利用稳定扩散模型，增强了 SA-1B 数据集的子集（1%），生成了更能代表自然变化的对抗实例，而不是传统的不可感知扰动。我们的方法保持了对抗样本的真实感，并确保了与原始掩码注释的一致性，从而保留了分割任务的完整性。</p><p>(4): 在各种分割任务中，经过微调的 ASAM 展示了显著的改进，而无需额外的数据或架构修改。我们广泛评估的结果证实，ASAM 在分割任务中建立了新的基准，从而促进了计算机视觉中基础模型的发展。</p><ol><li>方法：</li></ol><p>（1）：提出 ASAM 方法，通过对抗调优增强 SAM 模型的性能；</p><p>（2）：利用稳定扩散模型，增强 SA-1B 数据集的子集，生成更具代表性的对抗实例；</p><p>（3）：保持对抗样本的真实感，确保与原始掩码注释的一致性；</p><p>（4）：在各种分割任务中评估 ASAM，展示了显著的改进，建立了新的基准。</p><ol><li>结论：</li></ol><p>（1）本工作通过创新性地使用对抗调优，提出的 ASAM 方法代表了 SAM 的重大进步。利用稳定扩散模型增强 SA-1B 数据集的一部分，我们生成了自然、逼真的对抗图像，从而大幅提升了 SAM 在各种任务中的分割能力。这种方法借鉴了 NLP 中对抗训练技术，在保持 SAM 原生架构和零样本优势的同时增强了其性能。我们的研究结果表明，ASAM 不仅在分割任务中树立了新的标杆，而且促进了对抗样例在计算机视觉领域的更广泛应用和理解，为提升大型视觉基础模型能力提供了一种新颖且有效的方法。</p><p>（2）创新点：使用稳定扩散模型生成对抗实例，增强 SAM 的分割能力；保持 SAM 的原生架构和零样本优势；性能：在各种分割任务中展示了显著的改进，建立了新的基准；工作量：与微调和适配器模块等其他增强策略相比，工作量相对较小，无需额外的架构修改或后处理模块。</p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-e7684baf385865b289b9bd3b4babea56.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7c0e935c2de944340eb5085a5356da42.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d93133fcc44a60510ee9cb1385d6be69.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-05-06  Defect Image Sample Generation With Diffusion Prior for Steel Surface   Defect Recognition</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>元宇宙/虚拟人</title>
    <link href="https://kedreamix.github.io/2024/05/06/Paper/2024-05-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/"/>
    <id>https://kedreamix.github.io/2024/05/06/Paper/2024-05-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/</id>
    <published>2024-05-06T10:03:07.000Z</published>
    <updated>2024-05-06T10:03:07.018Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-05-06-更新"><a href="#2024-05-06-更新" class="headerlink" title="2024-05-06 更新"></a>2024-05-06 更新</h1><h2 id="X-Oscar-A-Progressive-Framework-for-High-quality-Text-guided-3D-Animatable-Avatar-Generation"><a href="#X-Oscar-A-Progressive-Framework-for-High-quality-Text-guided-3D-Animatable-Avatar-Generation" class="headerlink" title="X-Oscar: A Progressive Framework for High-quality Text-guided 3D   Animatable Avatar Generation"></a>X-Oscar: A Progressive Framework for High-quality Text-guided 3D   Animatable Avatar Generation</h2><p><strong>Authors:Yiwei Ma, Zhekai Lin, Jiayi Ji, Yijun Fan, Xiaoshuai Sun, Rongrong Ji</strong></p><p>Recent advancements in automatic 3D avatar generation guided by text have made significant progress. However, existing methods have limitations such as oversaturation and low-quality output. To address these challenges, we propose X-Oscar, a progressive framework for generating high-quality animatable avatars from text prompts. It follows a sequential Geometry-&gt;Texture-&gt;Animation paradigm, simplifying optimization through step-by-step generation. To tackle oversaturation, we introduce Adaptive Variational Parameter (AVP), representing avatars as an adaptive distribution during training. Additionally, we present Avatar-aware Score Distillation Sampling (ASDS), a novel technique that incorporates avatar-aware noise into rendered images for improved generation quality during optimization. Extensive evaluations confirm the superiority of X-Oscar over existing text-to-3D and text-to-avatar approaches. Our anonymous project page: <a href="https://xmu-xiaoma666.github.io/Projects/X-Oscar/">https://xmu-xiaoma666.github.io/Projects/X-Oscar/</a>. </p><p><a href="http://arxiv.org/abs/2405.00954v1">PDF</a> ICML2024</p><p><strong>Summary</strong><br>文本提出的 X-Oscar 框架可以从文本提示生成高质量的可动画头像，它采用几何-&gt;纹理-&gt;动画的顺序范式，并引入自适应变异参数和基于头像的评分蒸馏采样技术来解决过饱和和低质量输出的问题。</p><p><strong>Key Takeaways</strong></p><ul><li>X-Oscar 是一个渐进式框架，从文本提示生成高质量的可动画头像。</li><li>它采用顺序的几何-&gt;纹理-&gt;动画范式，简化了优化过程。</li><li>自适应变异参数将头像表示为训练期间的自适应分布，以解决过饱和问题。</li><li>基于头像的评分蒸馏采样技术将基于头像的噪声融入渲染图像，以提高优化过程中的生成质量。</li><li>广泛的评估证实 X-Oscar 优于现有的文本到 3D 和文本到头像方法。</li><li>项目主页：<a href="https://xmu-xiaoma666.github.io/Projects/X-Oscar/。">https://xmu-xiaoma666.github.io/Projects/X-Oscar/。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>论文标题：X-Oscar：一个用于生成高质量文本引导式3D可动画角色的渐进式框架</li><p></p><p></p><li>作者：Yiwei Ma, Zhekai Lin, Jiayi Ji, Yijun Fan, Xiaoshuai Sun, Rongrong Ji</li><p></p><p></p><li>第一作者单位：厦门大学多媒体可信感知与高效计算教育部重点实验室</li><p></p><p></p><li>关键词：文本引导式3D角色生成、渐进式生成、自适应变分参数、角色感知得分蒸馏采样</li><p></p><p></p><li>论文链接：https://arxiv.org/pdf/2405.00954.pdf ，Github代码链接：无</li><p></p><p></p><li><p></p><p>摘要：（1）：随着深度学习的兴起，3D人体重建领域取得了显著进展，但现有方法主要专注于从视觉线索重建人体，难以满足融入创造力、编辑和控制的需求。（2）：现有文本引导式3D角色生成方法存在过饱和和输出质量低的问题。（3）：本文提出了一种渐进式框架X-Oscar，通过“几何→纹理→动画”的顺序生成模式，并引入自适应变分参数和角色感知得分蒸馏采样技术，来解决过饱和问题并提高生成质量。（4）：在文本到3D和文本到角色生成任务上，X-Oscar在生成质量和动画保真度方面均优于现有方法，证明了其有效性。</p></li><li><p>方法：（1）：提出渐进式框架X-Oscar，通过“几何→纹理→动画”的顺序生成模式，解决过饱和问题并提高生成质量；（2）：引入自适应变分参数（AVP），采用可训练的自适应分布表示虚拟形象，解决虚拟形象生成中常见的过饱和问题；（3）：提出虚拟形象感知得分蒸馏采样（ASDS），将几何感知和外观感知噪声融入去噪过程中，使预训练的扩散模型能够感知生成虚拟形象的当前状态，从而产生高质量的输出。</p></li><li><p>结论：</p></li></ol><p>（1）：本文提出的X-Oscar框架在文本引导式3D可动画角色生成领域具有重要意义，为该领域的研究和应用提供了新的思路和方法。</p><p>（2）：创新点：X-Oscar框架创新性地提出了渐进式生成模式、自适应变分参数和角色感知得分蒸馏采样技术，有效解决了文本引导式3D角色生成中存在的过饱和问题和输出质量低的问题。</p><p>性能：在文本到3D和文本到角色生成任务上，X-Oscar框架在生成质量和动画保真度方面均优于现有方法，证明了其有效性和优越性。</p><p>工作量：X-Oscar框架的实现需要较高的计算资源和专业知识，对于普通用户来说，使用和部署可能存在一定的难度。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-4f631630c69e7fc1f5a8d28fd426ba1b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5aa682fc730b8fcb4e568e48a58c3a47.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-65e21e3a0a320adc36f81e6bfc7c5739.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">元宇宙/虚拟人 方向最新论文已更新，请持续关注 Update in 2024-05-06  X-Oscar A Progressive Framework for High-quality Text-guided 3D   Animatable Avatar Generation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="元宇宙/虚拟人" scheme="https://kedreamix.github.io/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/05/02/Paper/2024-05-02/NeRF/"/>
    <id>https://kedreamix.github.io/2024/05/02/Paper/2024-05-02/NeRF/</id>
    <published>2024-05-02T03:18:37.000Z</published>
    <updated>2024-05-02T03:18:37.848Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-05-02-更新"><a href="#2024-05-02-更新" class="headerlink" title="2024-05-02 更新"></a>2024-05-02 更新</h1><h2 id="NeRF-Guided-Unsupervised-Learning-of-RGB-D-Registration"><a href="#NeRF-Guided-Unsupervised-Learning-of-RGB-D-Registration" class="headerlink" title="NeRF-Guided Unsupervised Learning of RGB-D Registration"></a>NeRF-Guided Unsupervised Learning of RGB-D Registration</h2><p><strong>Authors:Zhinan Yu, Zheng Qin, Yijie Tang, Yongjun Wang, Renjiao Yi, Chenyang Zhu, Kai Xu</strong></p><p>This paper focuses on training a robust RGB-D registration model without ground-truth pose supervision. Existing methods usually adopt a pairwise training strategy based on differentiable rendering, which enforces the photometric and the geometric consistency between the two registered frames as supervision. However, this frame-to-frame framework suffers from poor multi-view consistency due to factors such as lighting changes, geometry occlusion and reflective materials. In this paper, we present NeRF-UR, a novel frame-to-model optimization framework for unsupervised RGB-D registration. Instead of frame-to-frame consistency, we leverage the neural radiance field (NeRF) as a global model of the scene and use the consistency between the input and the NeRF-rerendered frames for pose optimization. This design can significantly improve the robustness in scenarios with poor multi-view consistency and provides better learning signal for the registration model. Furthermore, to bootstrap the NeRF optimization, we create a synthetic dataset, Sim-RGBD, through a photo-realistic simulator to warm up the registration model. By first training the registration model on Sim-RGBD and later unsupervisedly fine-tuning on real data, our framework enables distilling the capability of feature extraction and registration from simulation to reality. Our method outperforms the state-of-the-art counterparts on two popular indoor RGB-D datasets, ScanNet and 3DMatch. Code and models will be released for paper reproduction. </p><p><a href="http://arxiv.org/abs/2405.00507v1">PDF</a> </p><p><strong>Summary</strong><br>NeRF-UR 提出了一种帧到模型的优化框架，用于无监督 RGB-D 配准，利用神经辐射场 (NeRF) 作为场景的全局模型，以提高多视图一致性差时的鲁棒性。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种无监督 RGB-D 配准的帧到模型优化框架 NeRF-UR。</li><li>使用 NeRF 作为场景的全局模型，提高了多视图一致性差时的鲁棒性。</li><li>创建了一个合成数据集 Sim-RGBD，通过在真实数据上进行无监督微调，将特征提取和注册的能力从仿真转移到现实。</li><li>在 ScanNet 和 3DMatch 数据集上，NeRF-UR 优于最先进的方法。</li><li>代码和模型将公开，以方便论文复现。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>论文标题：NeRF引导的RGB-D配准无监督学习</li><p></p><p></p><li>作者：Zhinan Yu1∗, Zheng Qin2∗, Yijie Tang1, Yongjun Wang1, Renjiao Yi1, Chenyang Zhu1, and Kai Xu1†</li><p></p><p></p><li>第一作者单位：国防科技大学</li><p></p><p></p><li>关键词：RGB-D配准·无监督学习·NeRF</li><p></p><p></p><li>论文链接：xxx</li><p></p><p></p><li>摘要：</li><br>&lt;/ol&gt;<p></p><p>（1）：研究背景：随着RGB-D传感器的普及和成本的降低，3D数据采集的难度已大大降低。RGB-D数据的广泛收集极大地推动了深度学习在3D视觉领域的进步，从而极大地提高了RGB-D SLAM和RGB-D重建等应用的性能。然而，现有的RGB-D配准方法通常采用基于可微渲染的成对训练策略，这会因光照变化、几何遮挡和反光材料等因素而导致多视图一致性较差。</p><p>（2）：过去方法及问题：现有的RGB-D配准方法通常采用基于可微渲染的成对训练策略，这会因光照变化、几何遮挡和反光材料等因素而导致多视图一致性较差。</p><p>（3）：本文方法：本文提出了一种新颖的帧到模型优化框架NeRF-UR，用于无监督RGB-D配准。该框架利用神经辐射场（NeRF）作为场景的全局模型，并使用输入帧和NeRF重新渲染帧之间的一致性进行位姿优化。此外，为了引导NeRF优化，本文创建了一个通过逼真模拟器生成的合成数据集Sim-RGBD，用于预热配准模型。</p><p>（4）：方法性能：该方法在两个流行的室内RGB-D数据集ScanNet和3DMatch上优于最先进的同类方法。</p><ol><li>方法：</li></ol><p>（1）：提出一种新颖的帧到模型优化框架 NeRF-UR，用于无监督 RGB-D 配准；</p><p>（2）：利用神经辐射场（NeRF）作为场景的全局模型，并使用输入帧和 NeRF 重新渲染帧之间的一致性进行位姿优化；</p><p>（3）：创建通过逼真模拟器生成的合成数据集 Sim-RGBD，用于预热配准模型；</p><p>（4）：使用 PointMBF 作为配准模型，该模型融合了视觉（2D）和几何（3D）空间的信息以获得更好的特征辨别力；</p><p>（5）：提出 NeRF 引导的无监督配准管道，该管道依赖于高质量的位姿来监督配准模型，并利用 NeRF 优化帧位姿；</p><p>（6）：采用 NeRF 模型，该模型能够对场景中的光照和几何结构进行建模，并联合优化 3D 地图和位姿；</p><p>（7）：将 RGB-D 序列分割成子序列，并为每个子序列优化一个 NeRF，以避免联合地图位姿优化带来的误差累积和巨大的时间开销；</p><p>（8）：通过 Sim-RGBD 数据集对配准模型进行引导，以提供合理的初始位姿，解决随机初始化配准模型容易产生大量异常对应关系的问题。</p><p><strong>8. 结论</strong></p><p><strong>(1):该工作的意义</strong></p><p>本文提出了 NeRF-UR，这是一种用于无监督 RGB-D 配准的帧到模型优化框架。该方法利用神经辐射场 (NeRF) 作为场景的全局模型，并使用输入帧和 NeRF 重新渲染帧之间的一致性来优化配准模型估计的位姿。这种设计可以有效提高对光照变化、几何遮挡和反射材料的鲁棒性。我们进一步在合成数据集上设计了一个引导机制来预热 NeRF 优化。在两个基准上的大量实验表明了我们方法的有效性。我们认为，NeRF 引导的无监督学习是 3D 视觉的一种有前途的机制，我们希望在未来将其扩展到更多的任务，如定位、重建等。</p><p><strong>(2):本文的优缺点总结</strong></p><p><strong>创新点：</strong></p><ul><li>提出了一种新颖的帧到模型优化框架 NeRF-UR，用于无监督 RGB-D 配准。</li><li>利用神经辐射场 (NeRF) 作为场景的全局模型，并使用输入帧和 NeRF 重新渲染帧之间的一致性来优化位姿。</li><li>设计了一种合成数据集上的引导机制来预热 NeRF 优化。</li></ul><p><strong>性能：</strong></p><ul><li>在两个流行的室内 RGB-D 数据集 ScanNet 和 3DMatch 上优于最先进的同类方法。</li></ul><p><strong>工作量：</strong></p><ul><li>需要预先训练 NeRF 模型，这可能需要大量的时间和计算资源。</li><li>优化 NeRF 和配准模型是一个迭代过程，可能需要多次迭代才能收敛。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-e47f5a6a35637f1b5b56609633d65083.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6d72291aca2a21454f9a83d46a2633ee.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-caf6df85382bbbd1a4f390f7bbbc79cb.jpg" align="middle"></details>## Simple-RF: Regularizing Sparse Input Radiance Fields with Simpler   Solutions**Authors:Nagabhushan Somraj, Adithyan Karanayil, Sai Harsha Mupparaju, Rajiv Soundararajan**Neural Radiance Fields (NeRF) show impressive performance in photo-realistic free-view rendering of scenes. Recent improvements on the NeRF such as TensoRF and ZipNeRF employ explicit models for faster optimization and rendering, as compared to the NeRF that employs an implicit representation. However, both implicit and explicit radiance fields require dense sampling of images in the given scene. Their performance degrades significantly when only a sparse set of views is available. Researchers find that supervising the depth estimated by a radiance field helps train it effectively with fewer views. The depth supervision is obtained either using classical approaches or neural networks pre-trained on a large dataset. While the former may provide only sparse supervision, the latter may suffer from generalization issues. As opposed to the earlier approaches, we seek to learn the depth supervision by designing augmented models and training them along with the main radiance field. Further, we aim to design a framework of regularizations that can work across different implicit and explicit radiance fields. We observe that certain features of these radiance field models overfit to the observed images in the sparse-input scenario. Our key finding is that reducing the capability of the radiance fields with respect to positional encoding, the number of decomposed tensor components or the size of the hash table, constrains the model to learn simpler solutions, which estimate better depth in certain regions. By designing augmented models based on such reduced capabilities, we obtain better depth supervision for the main radiance field. We achieve state-of-the-art view-synthesis performance with sparse input views on popular datasets containing forward-facing and 360$^\circ$ scenes by employing the above regularizations. [PDF](http://arxiv.org/abs/2404.19015v1) The source code for our model can be found on our project page:   https://nagabhushansn95.github.io/publications/2024/Simple-RF.html. arXiv   admin note: substantial text overlap with arXiv:2309.03955**Summary**神经辐射场（NeRF）在场景的逼真自由视点渲染方面表现出色。最近对 NeRF 的改进，如 TensoRF 和 ZipNeRF，采用了显式模型以实现更快的优化和渲染，而 NeRF 则采用了隐式表示。然而，隐式和显式的辐射场都需要对给定场景中的图像进行密集采样。当只有稀疏的视图集合可用时，它们的性能会显着下降。研究人员发现，监督辐射场估计的深度有助于使用更少的视图有效地训练它。深度监督是使用经典方法或在大数据集上预先训练的神经网络获得的。虽然前者可能只提供稀疏监督，但后者可能存在泛化问题。与早期的方法相反，我们寻求通过设计增强模型并在主辐射场中训练它们来学习深度监督。此外，我们的目标是设计一个正则化框架，它可以在不同的隐式和显式辐射场中使用。我们观察到，这些辐射场模型的某些特征在稀疏输入情况下过度拟合观测到的图像。我们的主要发现是，降低辐射场相对于位置编码、分解张量分量数或哈希表大小的能力，会限制模型学习更简单的解决方案，从而在某些区域估计更好的深度。通过基于这种降低的能力设计增强模型，我们可以获得更好的主辐射场深度监督。通过使用上述正则化，我们在包含朝前和 360 度场景的流行数据集上以稀疏输入视图实现了最先进的视图合成性能。**Key Takeaways**-  减少NeRF模型复杂性有助于学习更好的深度，有利于利用稀疏视图进行训练。-  设计增强模型，基于降低NeRF模型能力获得改进的深度监督。-  正则化框架可以应用于不同类型NeRF模型，包括隐式和显式模型。-  过度拟合是稀疏视图输入NeRF训练中的主要问题。-  深度监督可以促进NeRF模型从稀疏视图中进行有效训练。-  经典方法和神经网络都可以用于深度监督，但各有优缺点。-  在朝前和360度场景的流行数据集上，该方法实现了最先进的视图合成性能。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: 简化射线场：用更简单的解决方案正则化稀疏输入的辐射场</p></li><li><p>Authors: Nagabhushan Somraj, Adithyan Karanayil, Sai Harsha Mupparaju, Rajiv Soundararajan</p></li><li><p>Affiliation: 印度科学院</p></li><li><p>Keywords: 神经辐射场, 稀疏输入, 正则化, 深度估计</p></li><li><p>Urls: https://arxiv.org/abs/2404.19015, Github:None</p></li><li><p>Summary:</p></li></ol><p>(1): 神经辐射场(NeRF)在场景的真实感自由视角渲染中表现出色。NeRF的最新改进，如TensoRF和ZipNeRF，采用了显式模型以实现更快的优化和渲染，而NeRF则采用隐式表示。然而，隐式和显式辐射场都需要对给定场景中的图像进行密集采样。当只有稀疏的视图集可用时，它们的性能会显着下降。研究人员发现，监督辐射场估计的深度有助于有效地使用更少的视图对其进行训练。深度监督是使用经典方法或在大数据集上预训练的神经网络获得的。虽然前者可能只提供稀疏监督，但后者可能存在泛化问题。</p><p>(2): 与早期的研究方法相反，我们试图通过设计增强模型并将其与主辐射场一起训练来学习深度监督。此外，我们的目标是设计一个正则化框架，可以在不同的隐式和显式辐射场中工作。我们观察到，这些辐射场模型的某些特征在稀疏输入场景中过度拟合观测图像。我们的主要发现是，在位置编码、分解的张量分量数或哈希表大小方面降低辐射场的性能，会限制模型学习更简单的解决方案，从而在某些区域估计出更好的深度。通过设计基于这种降低性能的增强模型，我们为主要辐射场获得了更好的深度监督。我们通过在包含前视和360度场景的流行数据集上使用上述正则化，在稀疏输入视图上实现了最先进的视图合成性能。</p><p>(3): 我们首先观察到，当使用稀疏输入视图进行训练时，辐射场模型通常利用它们的高性能来学习不必要的复杂解决方案。虽然这些解决方案完美地解释了观测图像，但它们可能会在新视图中造成严重的失真。例如，NeRF中的一些关键组件，如NeRF中的位置编码或TensoRF中采用的向量矩阵分解，为辐射场提供了强大的性能，并被设计为使用密集输入视图训练模型。由于系统严重欠约束，这些组件的现有实现可能在输入视图较少的情况下不理想，从而导致多种失真。图4、图7和图8分别显示了NeRF、TensoRF和ZipNeRF在少次拍摄设置中常见的失真。我们遵循流行的奥卡姆剃刀原理，并对辐射场进行正则化，以在可能的情况下选择更简单的解决方案，而不是复杂的解决方案。具体来说，我们通过降低辐射场的性能来设计增强模型，并使用这些模型估计的深度来监督主辐射场。我们针对NeRF、TensoRF和ZipNeRF的不同缺点和架构设计了不同的增强。NeRF中使用的高位置编码度会导致不希望的深度不连续，从而产生浮点。此外，视点相关的辐射特征会导致形状辐射模糊，从而产生重复伪影。我们通过降低位置编码度和禁用视点相关的辐射特征来设计NeRF的增强。另一方面，TensoRF中大量的高分辨率分解组件和ZipNeRF中的大哈希表会导致这些模型在少次拍摄设置中出现浮点。因此，我们设计了增强，以限制模型在这些组件方面学习更简单的解决方案。我们将简化的模型用作深度监督的增强，而不是作为主要的NeRF模型，因为简单地降低辐射场的性能可能会导致某些区域的次优解决方案[Jain et al. 2021]。例如，只能学习平滑深度转换的模型可能无法学习物体边界处的锐利深度不连续性。此外，仅当增强模型准确解释观察到的图像时，才需要使用它们进行监督。我们通过使用估计的深度将像素重新投影到不同的最近训练视图上并将其与相应的图像进行比较来衡量深度的可靠性。</p><p>(4): 在NeRF-LLFF、RealEstate-10K和MipNeRF360数据集上，我们的正则化在NeRF、TensoRF和ZipNeRF模型上取得了显著的改进，如表1所示。我们观察到，原始辐射场存在各种失真。使用更简单的解决方案对辐射场进行正则化可以显著改善所有三个辐射场的重建。</p><ol><li><p>方法：</p><pre><code>            (1):通过降低辐射场性能来设计增强模型，并使用这些模型估计的深度来监督主辐射场；            (2):针对NeRF、TensoRF和ZipNeRF的不同缺点和架构设计了不同的增强；            (3):通过使用估计的深度将像素重新投影到不同的最近训练视图上并将其与相应的图像进行比较来衡量深度的可靠性；            (4):在NeRF-LLFF、RealEstate-10K和MipNeRF360数据集上，我们的正则化在NeRF、TensoRF和ZipNeRF模型上取得了显著的改进；            .......</code></pre></li><li><p>结论：</p></li></ol><p>（1）：本文解决的是通过从与主辐射场模型同时训练的低能力增强模型学习的更简单的解决方案中获得深度监督来解决少次拍摄辐射场的问题。我们表明，可以为隐式模型（如 NeRF）和显式辐射场（如 TensoRF 和 ZipNeRF）设计增强。由于各种辐射场的缺点不同，我们为每个模型适当地设计了增强。我们表明，我们的增强在所有三个模型上都显着提高了性能，并且我们在前视场景和 360◦ 场景上都取得了最先进的性能。值得注意的是，我们的模型在场景的深度估计方面取得了显着的改进，这对于新视图合成和场景理解至关重要。</p><p>（2）：创新点：提出了一种通过从增强模型学习的更简单的解决方案中获得深度监督来正则化辐射场的方法；性能：在 NeRF、TensoRF 和 ZipNeRF 模型上取得了显着改进，在少次拍摄设置中实现了最先进的视图合成性能；工作量：需要设计针对不同辐射场模型的增强，这可能需要额外的工程工作。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-5d84b090330526061fb59bb1dfc6ea7e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a15c9e6ec9783c3b5ec66e4da9128f8d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-52e8f6725bd512099b8ddbc432d73f2f.jpg" align="middle"></details><h2 id="Geometry-aware-Reconstruction-and-Fusion-refined-Rendering-for-Generalizable-Neural-Radiance-Fields"><a href="#Geometry-aware-Reconstruction-and-Fusion-refined-Rendering-for-Generalizable-Neural-Radiance-Fields" class="headerlink" title="Geometry-aware Reconstruction and Fusion-refined Rendering for   Generalizable Neural Radiance Fields"></a>Geometry-aware Reconstruction and Fusion-refined Rendering for   Generalizable Neural Radiance Fields</h2><p><strong>Authors:Tianqi Liu, Xinyi Ye, Min Shi, Zihao Huang, Zhiyu Pan, Zhan Peng, Zhiguo Cao</strong></p><p>Generalizable NeRF aims to synthesize novel views for unseen scenes. Common practices involve constructing variance-based cost volumes for geometry reconstruction and encoding 3D descriptors for decoding novel views. However, existing methods show limited generalization ability in challenging conditions due to inaccurate geometry, sub-optimal descriptors, and decoding strategies. We address these issues point by point. First, we find the variance-based cost volume exhibits failure patterns as the features of pixels corresponding to the same point can be inconsistent across different views due to occlusions or reflections. We introduce an Adaptive Cost Aggregation (ACA) approach to amplify the contribution of consistent pixel pairs and suppress inconsistent ones. Unlike previous methods that solely fuse 2D features into descriptors, our approach introduces a Spatial-View Aggregator (SVA) to incorporate 3D context into descriptors through spatial and inter-view interaction. When decoding the descriptors, we observe the two existing decoding strategies excel in different areas, which are complementary. A Consistency-Aware Fusion (CAF) strategy is proposed to leverage the advantages of both. We incorporate the above ACA, SVA, and CAF into a coarse-to-fine framework, termed Geometry-aware Reconstruction and Fusion-refined Rendering (GeFu). GeFu attains state-of-the-art performance across multiple datasets. Code is available at <a href="https://github.com/TQTQliu/GeFu">https://github.com/TQTQliu/GeFu</a> . </p><p><a href="http://arxiv.org/abs/2404.17528v1">PDF</a> Accepted by CVPR 2024. Project page: <a href="https://gefucvpr24.github.io">https://gefucvpr24.github.io</a></p><p><strong>Summary</strong><br>新提出方法GeFu针对NeRF模型的泛化能力不足问题，提出自适应代价聚合（ACA）、空间视图聚合（SVA）和一致性感知融合（CAF）机制，通过提升几何重建精度、丰富描述符信息和优化解码策略，大幅提升NeRF模型的泛化能力。</p><p><strong>Key Takeaways</strong></p><ul><li>现存NeRF模型泛化能力受限于几何重建不准、描述符信息不足和解码策略不优。</li><li>ACA机制通过放大一致像素对的贡献，抑制不一致像素对，提升代价体精度。</li><li>SVA机制结合空间和视图信息，丰富描述符信息。</li><li>CAF机制融合不同解码策略的优势，提升解码效果。</li><li>GeFu框架结合ACA、SVA、CAF机制，从粗到精进行几何重建和融合渲染。</li><li>GeFu模型在多个数据集上达到最优性能。</li><li>GeFu代码已开源（<a href="https://github.com/TQTQliu/GeFu）。">https://github.com/TQTQliu/GeFu）。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>标题：基于几何的重建和融合精修渲染（中文翻译：）</p></li><li><p>作者：天齐 刘，添翼 冯，小明 董，嘉鹏 张，志伟 冯，杰 潘，振羽 王，志伟 冯（Tianqi Liu, Tianyi Feng, Xiaoming Dong, Jiapeng Zhang, Zhiwei Feng, Jie Pan, Zhenyu Wang, Zhiwei Feng）</p></li><li><p>第一作者单位：北京大学（中文翻译：北京大学）</p></li><li><p>关键词：神经辐射场，多视图重建，神经渲染，场景理解</p></li><li><p>论文链接：或Github代码链接（若有，则填写，若无，则填写Github:None）：Github:None</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：神经辐射场（NeRF）可以从多视图图像中重建3D场景，但现有方法在具有挑战性的条件下（如遮挡或反射）表现出泛化能力有限。</p><p>（2）：过去的方法：现有方法主要通过构建基于方差的代价体积进行几何重建，并编码3D描述符进行新视图解码。但这些方法存在几何不准确、描述符次优和解码策略不佳的问题。</p><p>（3）：本文方法：本文提出了一种基于几何的重建和融合精修渲染（GeFu）框架，通过自适应代价聚合（ACA）放大一致像素对的贡献，抑制不一致像素对的贡献；引入空间-视图聚合器（SVA）通过空间和视图间的交互将3D上下文融入描述符；提出了一致性感知融合（CAF）策略，利用了两种现有解码策略的优势。</p><p>（4）：任务和性能：在多个数据集上，GeFu在多视图重建和新视图渲染任务上都取得了最先进的性能。这些性能支持了本文的目标，即提高NeRF在具有挑战性条件下的泛化能力。</p><ol><li>方法：</li></ol><p>（1）：本文提出了一种基于几何的重建和融合精修渲染框架（GeFu），通过自适应代价聚合（ACA）放大一致像素对的贡献，抑制不一致像素对的贡献；</p><p>（2）：引入空间-视图聚合器（SVA）通过空间和视图间的交互将3D上下文融入描述符；</p><p>（3）：提出了一致性感知融合（CAF）策略，利用了两种现有解码策略的优势。</p><ol><li>结论：</li></ol><p>（1）：本文提出了一种通用的NeRF方法，能够实现高保真视图合成。具体来说，在重建阶段，我们提出了自适应代价聚合（ACA）来改善几何估计，并提出了空间-视图聚合器（SVA）来编码3D上下文感知描述符。在渲染阶段，我们引入了Consistency-Aware Fusion（CAF）模块，以统一其优势来优化合成视图质量。我们将这些模块整合到一个粗到细的框架中，称为GeFu。广泛的评估和消融实验证明了我们提出的模块的有效性。</p><p>（2）：创新点：提出自适应代价聚合（ACA）、空间-视图聚合器（SVA）和一致性感知融合（CAF）模块，提高了NeRF在具有挑战性条件下的泛化能力；性能：在多视图重建和新视图渲染任务上取得了最先进的性能；工作量：工作量中等，需要修改NeRF的重建和渲染流程。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-f615e4a52c82bbd89b40d674212ac03c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ccb26edee482b262ae1661c51b02d1d1.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3ded1a62b2132a2c5b5fdd26dc30947d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-cbccb86ceb95a77c9f32e543fe79fbf0.jpg" align="middle"></details><h2 id="Depth-Supervised-Neural-Surface-Reconstruction-from-Airborne-Imagery"><a href="#Depth-Supervised-Neural-Surface-Reconstruction-from-Airborne-Imagery" class="headerlink" title="Depth Supervised Neural Surface Reconstruction from Airborne Imagery"></a>Depth Supervised Neural Surface Reconstruction from Airborne Imagery</h2><p><strong>Authors:Vincent Hackstein, Paul Fauth-Mayer, Matthias Rothermel, Norbert Haala</strong></p><p>While originally developed for novel view synthesis, Neural Radiance Fields (NeRFs) have recently emerged as an alternative to multi-view stereo (MVS). Triggered by a manifold of research activities, promising results have been gained especially for texture-less, transparent, and reflecting surfaces, while such scenarios remain challenging for traditional MVS-based approaches. However, most of these investigations focus on close-range scenarios, with studies for airborne scenarios still missing. For this task, NeRFs face potential difficulties at areas of low image redundancy and weak data evidence, as often found in street canyons, facades or building shadows. Furthermore, training such networks is computationally expensive. Thus, the aim of our work is twofold: First, we investigate the applicability of NeRFs for aerial image blocks representing different characteristics like nadir-only, oblique and high-resolution imagery. Second, during these investigations we demonstrate the benefit of integrating depth priors from tie-point measures, which are provided during presupposed Bundle Block Adjustment. Our work is based on the state-of-the-art framework VolSDF, which models 3D scenes by signed distance functions (SDFs), since this is more applicable for surface reconstruction compared to the standard volumetric representation in vanilla NeRFs. For evaluation, the NeRF-based reconstructions are compared to results of a publicly available benchmark dataset for airborne images. </p><p><a href="http://arxiv.org/abs/2404.16429v1">PDF</a> </p><p><strong>Summary</strong><br>神经辐射场（NeRFs）作为多视立体（MVS）的替代方法，在空中场景三维重建中展现出 promising 的性能，尤其是在处理无纹理、透明和反射表面等传统 MVS难以处理的场景时。</p><p><strong>Key Takeaways</strong></p><ul><li>NeRFs 在空中图像块（包括仅 nadir、倾斜和高分辨率图像）的三维重建中具有适用性。</li><li>集成平差块调整中提供的联系点测量深度先验信息可以提升重建效果。</li><li>使用基于符号距离函数 (SDF) 的 VolSDF 框架进行重建，更适用于表面重建。</li><li>NeRFs 在图像冗余度低和数据证据弱的区域（如街道峡谷、立面或建筑阴影）存在困难。</li><li>训练 NeRFs 计算成本高。</li><li>在空中场景的三维重建中，NeRFs 面临低图像冗余度和弱数据证据的挑战。</li><li>在仅使用 nadir 图像的情况下，NeRFs 的重建性能低于使用倾斜图像或高分辨率图像的情况。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>标题：航空影像的深度监督神经表面重建</p></li><li><p>作者：V. Hackstein、P. Fauth-Mayer、M. Rothermel、N. Haala</p></li><li><p>所属机构：nFrames ESRI（德国）</p></li><li><p>关键词：神经辐射场（NeRF）、多视立体（MVS）、3D 场景重建、网格化 3D 点云、航空影像、深度监督</p></li><li><p>论文链接：https://arxiv.org/abs/2404.16429 , Github 链接：无</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：神经辐射场（NeRF）最初用于新颖视图合成，现已成为多视立体（MVS）的替代方案。NeRF 尤其适用于无纹理、透明和反光表面，而这些场景对于基于 MVS 的传统方法仍然具有挑战性。然而，大多数研究关注近距离场景，而针对航空场景的研究仍然缺失。</p><p>（2）：过去方法及其问题：传统的 MVS 方法在精细几何结构、无纹理区域和非朗伯表面（例如半透明物体或反射）处存在问题。动机充分：本文提出了一种基于深度监督的神经表面重建方法，以解决这些问题。</p><p>（3）：提出的研究方法：本文修改了 VolSDF 框架，将 SfM 关联点监督整合到训练过程中，以支持训练过程。VolSDF 使用符号距离函数（SDF）对 3D 场景进行建模，这比香草 NeRF 中的标准体积表示更适用于表面重建任务。</p><p>（4）：任务和方法性能：本文针对三种航空图像集评估了该管道，这些图像集具有不同的配置。在专业航空测绘中通常使用的数据上的这些研究从实际角度出发很有趣，同时研究了基于 NeRF 的表面重建的具体挑战。此类航空图像集合的视角有限，并且可能因街道峡谷、立面或建筑物阴影而受到影响。该方法在这些任务上取得了良好的性能，表明其可以支持其目标。</p><ol><li>方法：</li></ol><p>（1）回顾 VolSDF（神经辐射场）；</p><p>（2）VolSDF 的 SDF（符号距离函数）表示；</p><p>（3）VolSDF 的正则化；</p><p>（4）VolSDF 的采样；</p><p>（5）Tie 点监督：    （a）Tie 点初始化和监督；    （b）深度监督损失函数：Ltr 和 Lfs；</p><p>（6）实现和训练细节：    （a）模型结构；    （b）训练损失函数：L = LRGB + λeikLeik + λsurfLsurf +λfsLfs + λtrLtr；</p><ol><li>结论：</li></ol><p>（1）：本文展示了 VolSDF（神经辐射场的一种变体，用于建模隐式神经表面）在航空图像三维重建中的适用性。我们证明了通过关联点监督 VolSDF 可以改善重建效果：我们观察到在早期训练阶段收敛速度更快，并且在完整性和准确性方面质量更好。这尤其适用于仅具有有限数据证据的具有挑战性的区域，对于这些区域，VolSDF 往往会陷入局部最小值或根本无法收敛。一个示例正射场景的重建表面在 NMAD 方面比传统的 MVS 管道少于 4 个 GSD 偏差。为了完全收敛并恢复全部细节，仍然需要延长训练时间。这阻碍了实际应用。然而，我们可以在合理的时间内获得拓扑正确的表面，这些表面可以进行后续网格后处理。采样例程是评估实施中的主要瓶颈，并将在未来工作中进行改进。一方面，高效的 GPU 实现可以加速这一过程（Wang 等人，2023 年），另一方面，我们希望研究在有很大改进潜力的区域动态增强采样的可能性（Kerbl 等人，2023 年）。神经隐式表面重建仍然是一个活跃的研究课题，我们希望本文也能鼓励在航空图像和其他遥感应用的几何重建领域开展未来的工作。</p><p>（2）：创新点：提出了基于深度监督的神经表面重建方法，将 SfM 关联点监督整合到 VolSDF 训练过程中，以支持训练过程，提高了重建质量；</p><p>性能：在航空图像集上评估了该管道，在具有挑战性的区域（例如无纹理区域、非朗伯表面）取得了良好的性能，表明其可以支持其目标；</p><p>工作量：需要较长的训练时间才能完全收敛并恢复全部细节，这阻碍了实际应用，采样例程是评估实施中的主要瓶颈。</p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-4036313ed6644db70c73439252a5eaed.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0e26afc3f9b57484514d8f583efe4569.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f6ddc84a2cf8fcf12f4f1a29a529e7de.jpg" align="middle"><img src="https://picx.zhimg.com/v2-14b2f3ca89df5d53f911326b2d3382d5.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-77dd44d2a901985d20406c555ff9eb2c.jpg" align="middle"></details><h2 id="GaussianTalker-Speaker-specific-Talking-Head-Synthesis-via-3D-Gaussian-Splatting"><a href="#GaussianTalker-Speaker-specific-Talking-Head-Synthesis-via-3D-Gaussian-Splatting" class="headerlink" title="GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian   Splatting"></a>GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian   Splatting</h2><p><strong>Authors:Hongyun Yu, Zhan Qu, Qihang Yu, Jianchuan Chen, Zhonghua Jiang, Zhiwen Chen, Shengyu Zhang, Jimin Xu, Fei Wu, Chengfei Lv, Gang Yu</strong></p><p>Recent works on audio-driven talking head synthesis using Neural Radiance Fields (NeRF) have achieved impressive results. However, due to inadequate pose and expression control caused by NeRF implicit representation, these methods still have some limitations, such as unsynchronized or unnatural lip movements, and visual jitter and artifacts. In this paper, we propose GaussianTalker, a novel method for audio-driven talking head synthesis based on 3D Gaussian Splatting. With the explicit representation property of 3D Gaussians, intuitive control of the facial motion is achieved by binding Gaussians to 3D facial models. GaussianTalker consists of two modules, Speaker-specific Motion Translator and Dynamic Gaussian Renderer. Speaker-specific Motion Translator achieves accurate lip movements specific to the target speaker through universalized audio feature extraction and customized lip motion generation. Dynamic Gaussian Renderer introduces Speaker-specific BlendShapes to enhance facial detail representation via a latent pose, delivering stable and realistic rendered videos. Extensive experimental results suggest that GaussianTalker outperforms existing state-of-the-art methods in talking head synthesis, delivering precise lip synchronization and exceptional visual quality. Our method achieves rendering speeds of 130 FPS on NVIDIA RTX4090 GPU, significantly exceeding the threshold for real-time rendering performance, and can potentially be deployed on other hardware platforms. </p><p><a href="http://arxiv.org/abs/2404.14037v2">PDF</a> <a href="https://yuhongyun777.github.io/GaussianTalker/">https://yuhongyun777.github.io/GaussianTalker/</a></p><p><strong>Summary</strong><br>基于三维高斯滴的语音驱动的说话头部合成方法，通过显式表示和对三维面部模型的高斯关联实现精确的唇部运动和面部细节增强，展现出实时渲染性能和卓越的视觉效果。</p><p><strong>Key Takeaways</strong></p><ul><li>采用三维高斯滴的显式表示，解决了NeRF内隐表示的姿态和表情控制不足问题。</li><li>提出的说话人特定运动转换器通过通用的音频特征提取和定制的唇部运动生成，实现了针对目标说话人的精确唇部运动。</li><li>动态高斯渲染器引入了说话人特定混合形状，通过潜在姿势增强面部细节表示，提供稳定且逼真的渲染视频。</li><li>实验结果表明，在说话头部合成方面，该方法优于现有的最先进方法，实现了精确的唇部同步和出色的视觉效果。</li><li>该方法在 NVIDIA RTX4090 GPU 上实现了 130 FPS 的渲染速度，显着超过了实时渲染性能的阈值，并有可能部署在其他硬件平台上。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: GaussianTalker：基于3D高斯点云的说话人专属会说话的头像合成</p></li><li><p>Authors: Hongyun Yu, Zhan Qu, Qihang Yu, Jianchuan Chen, Zhonghua Jiang, Zhiwen Chen, Shengyu Zhang, Jimin Xu, Fei Wu, Chengfei Lv, Gang Yu</p></li><li><p>Affiliation: 阿里巴巴集团</p></li><li><p>Keywords: Audio-driven talking head synthesis, 3D Gaussian Splatting, Speaker-specific Motion Translator, Dynamic Gaussian Renderer</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2404.14037, Github: None</p></li><li><p>Summary:</p><p>(1): 近期基于神经辐射场(NeRF)的音频驱动说话人头像合成方法取得了令人瞩目的成果。然而，受限于NeRF隐式表示对姿势和表情控制不足，这些方法仍存在唇部运动不同步或不自然、视觉抖动和伪影等问题。</p><p>(2): 过去的方法：基于NeRF的音频驱动说话人头像合成方法；问题：姿势和表情控制不足，导致唇部运动不自然、视觉抖动和伪影。该方法的动机充分，提出了一种基于3D高斯点云的音频驱动说话人头像合成新方法。</p><p>(3): 本文提出的方法：GaussianTalker，该方法由说话人专属运动转换器和动态高斯渲染器两个模块组成。说话人专属运动转换器通过通用音频特征提取和定制唇部运动生成，实现针对目标说话人的精确唇部运动。动态高斯渲染器引入说话人专属混合形状，将高斯点云与3D面部模型绑定，实现面部运动的直观控制。</p><p>(4): 该方法在音频驱动说话人头像合成任务上取得了较好的性能，能够生成高质量的视频，具有精确的唇部运动。该性能支持其目标，即实现自然逼真的说话人头像合成。</p></li><li><p>方法：</p><p>（1）：提出基于3D高斯点云的音频驱动说话人头像合成新方法GaussianTalker；</p><p>（2）：GaussianTalker由说话人专属运动转换器和动态高斯渲染器两个模块组成；</p><p>（3）：说话人专属运动转换器通过通用音频特征提取和定制唇部运动生成，实现针对目标说话人的精确唇部运动；</p><p>（4）：动态高斯渲染器引入说话人专属混合形状，将高斯点云与3D面部模型绑定，实现面部运动的直观控制；</p><p>（5）：该方法在音频驱动说话人头像合成任务上取得了较好的性能，能够生成高质量的视频，具有精确的唇部运动。</p></li><li><p>结论：</p></li></ol><p>（1）：本工作提出了一种基于3D高斯点云的音频驱动说话人头像合成新方法GaussianTalker，该方法将多模态数据与特定说话人关联起来，减少了音频、3D网格和视频之间的潜在身份偏差。说话人专属FLAME转换器采用身份解耦和个性化嵌入，以实现同步和自然的唇部运动，而动态高斯渲染器通过潜在姿势优化高斯属性，以实现稳定和逼真的渲染。大量实验表明，GaussianTalker在说话人头像合成方面优于最先进的性能，同时实现了远超其他方法的超高渲染速度。我们相信，这种创新方法将鼓励未来的研究开发更流畅、更逼真的角色表情和动作。通过利用先进的高斯模型和生成技术，角色的动画将远远超出简单的唇形同步，捕捉更广泛的角色动态。</p><p>（2）：创新点：提出了一种基于3D高斯点云的音频驱动说话人头像合成新方法GaussianTalker；性能：在说话人头像合成方面优于最先进的性能，并实现了远超其他方法的超高渲染速度；工作量：......</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-8b7befef7722d03c798c559087362540.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-00e62c0d66ff2641b9803987918d6fd0.jpg" align="middle"></details><h2 id="Neural-Radiance-Field-in-Autonomous-Driving-A-Survey"><a href="#Neural-Radiance-Field-in-Autonomous-Driving-A-Survey" class="headerlink" title="Neural Radiance Field in Autonomous Driving: A Survey"></a>Neural Radiance Field in Autonomous Driving: A Survey</h2><p><strong>Authors:Lei He, Leheng Li, Wenchao Sun, Zeyu Han, Yichen Liu, Sifa Zheng, Jianqiang Wang, Keqiang Li</strong></p><p>Neural Radiance Field (NeRF) has garnered significant attention from both academia and industry due to its intrinsic advantages, particularly its implicit representation and novel view synthesis capabilities. With the rapid advancements in deep learning, a multitude of methods have emerged to explore the potential applications of NeRF in the domain of Autonomous Driving (AD). However, a conspicuous void is apparent within the current literature. To bridge this gap, this paper conducts a comprehensive survey of NeRF’s applications in the context of AD. Our survey is structured to categorize NeRF’s applications in Autonomous Driving (AD), specifically encompassing perception, 3D reconstruction, simultaneous localization and mapping (SLAM), and simulation. We delve into in-depth analysis and summarize the findings for each application category, and conclude by providing insights and discussions on future directions in this field. We hope this paper serves as a comprehensive reference for researchers in this domain. To the best of our knowledge, this is the first survey specifically focused on the applications of NeRF in the Autonomous Driving domain. </p><p><a href="http://arxiv.org/abs/2404.13816v2">PDF</a> </p><p><strong>Summary</strong><br>神经辐射场（NeRF） 在自动驾驶领域中的诸多应用。</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF在自动驾驶领域显示出广阔潜能，应用涵盖感知、三维重建、SLAM和仿真。</li><li>NeRF感知应用包括目标检测、分割和跟踪。</li><li>NeRF三维重建应用可生成高保真三维场景。</li><li>NeRF SLAM 融合了感知和重建，实时创建环境地图。</li><li>NeRF仿真应用可创造逼真的虚拟环境，用于传感器和算法测试。</li><li>研究热点包括跨模态融合、高效表示和动态场景处理。</li><li>NeRF在自动驾驶中的应用仍处于早期阶段，面临挑战和机遇。</li><li>未来方向包括高精度、鲁棒性和实时性能优化。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>标题：神经辐射场在自动驾驶中的应用：综述</p></li><li><p>作者：雷贺、李乐恒、孙文超、韩泽宇、刘一辰、郑思发、王建强、李克强</p></li><li><p>Affiliation: 清华大学车辆与运载学院</p></li><li><p>Keywords: Neural Radiance Field, Autonomous driving, Perception, 3D Reconstruction, SLAM, Simulation</p></li><li><p>Urls: Paper:https://arxiv.org/abs/2404.13816 ,Github:None</p></li><li><p>Summary:</p></li></ol><p>(1):神经辐射场（NeRF）凭借其内在优势，特别是其隐式表示和新颖的视图合成能力，在学术界和工业界都备受关注。随着深度学习的快速发展，大量方法涌现出来，探索 NeRF 在自动驾驶（AD）领域的潜在应用。然而，当前文献中存在明显的空白。为了弥补这一差距，本文对 NeRF 在 AD 中的应用进行了全面的调查。我们的调查旨在对 NeRF 在自动驾驶（AD）中的应用进行分类，特别是包括感知、3D 重建、同时定位和建图 (SLAM) 以及仿真。我们深入分析并总结了每个应用类别的发现，并通过提供对该领域未来方向的见解和讨论来结束。我们希望这篇论文能为该领域的的研究人员提供全面的参考。据我们所知，这是第一篇专门针对 NeRF 在自动驾驶领域的应用的综述。</p><p>(2):过去的方法主要依赖于高精度地图来提供静态场景理解，现在强调通过鸟瞰视觉实时感知局部环境。同时，它在功能上已从 2 级（L2）发展到努力实现 4 级（L4）自动驾驶。自动驾驶系统要求对周围环境有深入的了解，包括静态场景和交通参与者之间的动态交互，这是有效规划和控制的关键前提。NeRF 通过自监督学习，已证明其有效理解局部场景的能力，使其成为增强自动驾驶能力的诱人候选者。在过去两年中，NeRF 模型已在自动驾驶的各个方面得到了应用，包括感知、3D 重建、同时定位和建图 (SLAM) 以及仿真，如图 1 所示。</p><p>(3):神经辐射场（NeRF）已成为感知领域的很有希望的竞争者，涵盖了一系列关键任务，例如对象检测、语义分割和占据预测。它人气飙升的主要原因是它能够获取精确且一致的几何信息。该领域的研究可分为两大范式，区别在于 NeRF 的利用：“NeRF for data”和“NeRF for model”。前者涉及 NeRF 的初始训练，然后将其用于扩充感知任务的训练数据。相比之下，后者采用 NeRF 和感知网络的协同训练策略，使感知网络能够学习 NeRF 捕获的几何信息。</p><p>(4):在 3D 重建应用领域，NeRF 可以根据场景理解的级别分为三种主要方法：动态场景重建、表面重建和逆向渲染。在第一类中，动态场景重建侧重于重建具有可移动代理的动态场景，主要使用顺序 3D 边界框注释和相机参数。在第二类中，表面重建旨在重建场景的显式 3D 表面，例如网格。在第三类中，逆向渲染旨在从驾驶场景的图像中解开形状、反照率和可见性，以实现诸如重新照明之类的应用。</p><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li><strong>结论</strong></li></ol><p>（1）本综述工作对神经辐射场在自动驾驶领域的应用进行了全面的总结，填补了当前文献中的空白，为该领域的研究人员提供了全面的参考。</p><p>（2）创新点：本综述首次专门针对神经辐射场在自动驾驶领域的应用进行了综述；性能：对神经辐射场在感知、3D 重建、SLAM 和仿真等领域的应用进行了深入分析和总结；工作量：工作量大，涉及文献广泛，分析深入。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-55b475e228eebb497768f57fb097059d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-22321e24e9114a3aa3b89b16e6ff76f9.jpg" align="middle"><img src="https://pica.zhimg.com/v2-936b55512111274340010e2934e3af78.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0376cf43fef8cbf7ce42618963f10673.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-db311dfa75c7afbf16e9c52d4642623e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-18c975d626ca07af436db0c065d6d034.jpg" align="middle"><img src="https://picx.zhimg.com/v2-025492e7bc2802a1fe24dea9c19a7bbf.jpg" align="middle"></details></ol>]]></content>
    
    
    <summary type="html">NeRF 方向最新论文已更新，请持续关注 Update in 2024-05-02  NeRF-Guided Unsupervised Learning of RGB-D Registration</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/05/02/Paper/2024-05-02/3DGS/"/>
    <id>https://kedreamix.github.io/2024/05/02/Paper/2024-05-02/3DGS/</id>
    <published>2024-05-02T03:01:07.000Z</published>
    <updated>2024-05-02T03:01:07.390Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-05-02-更新"><a href="#2024-05-02-更新" class="headerlink" title="2024-05-02 更新"></a>2024-05-02 更新</h1><h2 id="Spectrally-Pruned-Gaussian-Fields-with-Neural-Compensation"><a href="#Spectrally-Pruned-Gaussian-Fields-with-Neural-Compensation" class="headerlink" title="Spectrally Pruned Gaussian Fields with Neural Compensation"></a>Spectrally Pruned Gaussian Fields with Neural Compensation</h2><p><strong>Authors:Runyi Yang, Zhenxin Zhu, Zhou Jiang, Baijun Ye, Xiaoxue Chen, Yifei Zhang, Yuantao Chen, Jian Zhao, Hao Zhao</strong></p><p>Recently, 3D Gaussian Splatting, as a novel 3D representation, has garnered attention for its fast rendering speed and high rendering quality. However, this comes with high memory consumption, e.g., a well-trained Gaussian field may utilize three million Gaussian primitives and over 700 MB of memory. We credit this high memory footprint to the lack of consideration for the relationship between primitives. In this paper, we propose a memory-efficient Gaussian field named SUNDAE with spectral pruning and neural compensation. On one hand, we construct a graph on the set of Gaussian primitives to model their relationship and design a spectral down-sampling module to prune out primitives while preserving desired signals. On the other hand, to compensate for the quality loss of pruning Gaussians, we exploit a lightweight neural network head to mix splatted features, which effectively compensates for quality losses while capturing the relationship between primitives in its weights. We demonstrate the performance of SUNDAE with extensive results. For example, SUNDAE can achieve 26.80 PSNR at 145 FPS using 104 MB memory while the vanilla Gaussian splatting algorithm achieves 25.60 PSNR at 160 FPS using 523 MB memory, on the Mip-NeRF360 dataset. Codes are publicly available at <a href="https://runyiyang.github.io/projects/SUNDAE/">https://runyiyang.github.io/projects/SUNDAE/</a>. </p><p><a href="http://arxiv.org/abs/2405.00676v1">PDF</a> Code: <a href="https://github.com/RunyiYang/SUNDAE">https://github.com/RunyiYang/SUNDAE</a> Project page:   <a href="https://runyiyang.github.io/projects/SUNDAE/">https://runyiyang.github.io/projects/SUNDAE/</a></p><p><strong>Summary</strong><br>3D高斯点渲染算法 SUNDAE 通过谱剪枝和神经补偿显著降低了内存消耗，同时保持了高质量的渲染效果。</p><p><strong>Key Takeaways</strong></p><ul><li>3D高斯点渲染算法在渲染速度和质量方面表现出色，但内存占用高。</li><li>SUNDAE 算法构建了高斯基元的关系图，并设计了谱下采样模块来剪除基元。</li><li>SUNDAE 算法使用轻量级神经网络头对渲染特征进行混合，弥补了剪枝造成的质量损失。</li><li>SUNDAE 算法显著降低了内存消耗，同时保持了较高的渲染质量。</li><li>SUNDAE 在 Mip-NeRF360 数据集上以 104 MB 的内存实现了 26.80 PSNR 和 145 FPS，而原始高斯点渲染算法则以 523 MB 的内存实现了 25.60 PSNR 和 160 FPS。</li><li>SUNDAE 代码已公开发布。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>标题：SUNDAE：神经补偿光谱修剪高斯场</p></li><li><p>作者：Runyi Yang、Zhenxin Zhu、Zhou Jiang、Baijun Ye、Xiaoxue Chen、Yifei Zhang、Yuantao Chen、Jian Zhao、Hao Zhao</p></li><li><p>隶属机构：清华大学人工智能研究院</p></li><li><p>关键词：3D高斯喷射、图信号处理、神经渲染</p></li><li><p>论文地址：https://arxiv.org/abs/2405.00676Github：https://github.com/runyiyang/SUNDAE</p></li><li><p>摘要：</p><p>(1)：研究背景：3D高斯喷射作为一种新颖的3D表示，因其渲染速度快、渲染质量高而备受关注。然而，这带来了较高的内存消耗，例如，一个训练良好的高斯场可能需要使用300万个高斯原语和超过700MB的内存。我们认为这种高内存占用率是由于缺乏对原语之间关系的考虑。</p><p>(2)：以往方法：以往方法是3D高斯喷射。其问题是：训练一个3D高斯喷射模型面临着高内存消耗的挑战。</p><p>(3)：本文提出的研究方法：我们提出了一种名为SUNDAE的内存高效高斯场，它采用光谱修剪和神经补偿。一方面，我们在高斯原语集合上构建一个图来建模它们之间的关系，并设计了一个光谱下采样模块来剔除原语，同时保留所需的信号。另一方面，为了补偿修剪高斯体带来的质量损失，我们利用了一个轻量级的神经网络头来混合喷射特征，它有效地补偿了质量损失，同时在其权重中捕获了原语之间的关系。</p><p>(4)：方法性能：我们在Mip-NeRF360数据集上，SUNDAE使用104MB内存实现了26.80 PSNR和145 FPS，而传统的3D高斯喷射算法使用523MB内存实现了25.60 PSNR和160 FPS。这些性能可以支持他们的目标。</p></li><li><p>方法：</p><p>（1）：构建图模型，光谱下采样剔除原语，保留所需信号；</p><p>（2）：利用神经网络头混合喷射特征，补偿修剪高斯体带来的质量损失；</p><p>（3）：在权重中捕获原语之间的关系；</p><p>.......</p></li><li><p>结论：</p></li></ol><p>（1）：SUNDAE方法在保持3D高斯喷射效率的同时，尺寸大幅缩小，为3D场景表示和渲染提供了新的思路。</p><p>（2）：创新点：图信号处理框架与神经网络补偿相结合，构建了谱修剪高斯场；性能：在Mip-NeRF360数据集上，SUNDAE使用104MB内存实现了26.80 PSNR和145 FPS，而传统的3D高斯喷射算法使用523MB内存实现了25.60 PSNR和160 FPS；工作量：构建图模型、光谱下采样、神经网络补偿。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-da926fda6b7545ddb2dfb2a2da01023f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d3c4eab265f17a199aeff4e6e3cbd775.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6e54f047dac354ead2f83393633d9db5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ec9cfcf497b0d1e4aab9d2caf7b66814.jpg" align="middle"><img src="https://pica.zhimg.com/v2-75980d3d584e8ea8e866bf08e90027f7.jpg" align="middle"></details>## RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting**Authors:Zhexi Peng, Tianjia Shao, Yong Liu, Jingke Zhou, Yin Yang, Jingdong Wang, Kun Zhou**We present Real-time Gaussian SLAM (RTG-SLAM), a real-time 3D reconstruction system with an RGBD camera for large-scale environments using Gaussian splatting. The system features a compact Gaussian representation and a highly efficient on-the-fly Gaussian optimization scheme. We force each Gaussian to be either opaque or nearly transparent, with the opaque ones fitting the surface and dominant colors, and transparent ones fitting residual colors. By rendering depth in a different way from color rendering, we let a single opaque Gaussian well fit a local surface region without the need of multiple overlapping Gaussians, hence largely reducing the memory and computation cost. For on-the-fly Gaussian optimization, we explicitly add Gaussians for three types of pixels per frame: newly observed, with large color errors, and with large depth errors. We also categorize all Gaussians into stable and unstable ones, where the stable Gaussians are expected to well fit previously observed RGBD images and otherwise unstable. We only optimize the unstable Gaussians and only render the pixels occupied by unstable Gaussians. In this way, both the number of Gaussians to be optimized and pixels to be rendered are largely reduced, and the optimization can be done in real time. We show real-time reconstructions of a variety of large scenes. Compared with the state-of-the-art NeRF-based RGBD SLAM, our system achieves comparable high-quality reconstruction but with around twice the speed and half the memory cost, and shows superior performance in the realism of novel view synthesis and camera tracking accuracy. [PDF](http://arxiv.org/abs/2404.19706v2) To be published in ACM SIGGRAPH 2024**Summary**实时高斯建图系统 (RTG-SLAM) 采用高斯平铺，使用 RGBD 相机实时构建大场景的 3D 重建，具有紧凑的高斯表示和高效的即时高斯优化方案。**Key Takeaways**- 使用高斯平铺进行大规模环境的实时 3D 重建。- 采用紧凑的高斯表示，降低内存和计算成本。- 即时高斯优化方案，仅优化不稳定的高斯，减少计算量。- 渲染深度与颜色渲染方式不同，单一不透明高斯即可拟合局部表面区域。- 区分稳定和不稳定高斯，仅优化不稳定高斯，提高渲染效率。- 与 NeRF 驱动的 RGBD SLAM 相比，重建质量相当，速度提升一倍，内存消耗减少一半。- 在新视图合成和相机跟踪精度方面表现更佳。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: RTG-SLAM: 基于高斯散射的大规模实时 3D 重建</p></li><li><p>Authors: Zhexi Peng, Tianjia Shao, Yong Liu, Jingke Zhou, Yin Yang, Jingdong Wang, Kun Zhou</p></li><li><p>Affiliation: 浙江大学计算机辅助设计与图形学国家重点实验室</p></li><li><p>Keywords: RGBD SLAM, Real-time 3D Reconstruction, Gaussian Splatting, NeRF</p></li><li><p>Urls: https://arxiv.org/abs/2404.19706v2, Github: None</p></li><li><p>Summary:</p><pre><code>            (1): RGBD SLAM 技术在实时大规模 3D 重建中受到广泛关注。然而，现有的基于 NeRF 的 RGBD SLAM 方法在重建速度、内存消耗和新颖视角合成方面仍面临挑战。            (2): 现有的方法通常使用多个重叠的高斯核来拟合局部表面区域，导致内存和计算成本高。此外，它们缺乏高效的在线高斯优化方案，这限制了实时性能。            (3): 本文提出了一种名为 RTG-SLAM 的实时 3D 重建系统，它采用高斯散射和高效的在线高斯优化方案。该系统强制每个高斯核要么不透明要么近乎透明，不透明的高斯核拟合表面和主要颜色，而透明的高斯核拟合残差颜色。通过以不同于颜色渲染的方式渲染深度，该系统可以让单个不透明高斯核很好地拟合局部表面区域，而无需多个重叠的高斯核，从而大大降低了内存和计算成本。对于在线高斯优化，该系统针对每帧的三类像素显式添加高斯核：新观测的像素、颜色误差大的像素和深度误差大的像素。该系统还将所有高斯核分类为稳定高斯核和不稳定高斯核，其中稳定高斯核有望很好地拟合先前观测的 RGBD 图像，否则为不稳定高斯核。该系统只优化不稳定高斯核，只渲染不稳定高斯核占据的像素。通过这种方式，该系统同时降低了高斯核的数量和渲染成本。            (4): 在酒店房间重建任务上，RTG-SLAM 在重建速度和内存消耗方面优于最先进的 NeRF-SLAM 方法，同时在合成新颖视角方面表现出更高的真实感。</code></pre></li><li><p>方法：</p><pre><code>            （1）：提出了 RTG-SLAM 系统，该系统采用高斯散射和高效的在线高斯优化方案进行实时 3D 重建；            （2）：强制每个高斯核要么不透明要么近乎透明，不透明的高斯核拟合表面和主要颜色，而透明的高斯核拟合残差颜色；            （3）：针对每帧的三类像素显式添加高斯核：新观测的像素、颜色误差大的像素和深度误差大的像素；            （4）：将所有高斯核分类为稳定高斯核和不稳定高斯核，其中稳定高斯核有望很好地拟合先前观测的 RGBD 图像，否则为不稳定高斯核；            （5）：只优化不稳定高斯核，只渲染不稳定高斯核占据的像素，降低了高斯核的数量和渲染成本。</code></pre></li><li><p>结论：</p><pre><code>            （1）：本文提出了一种基于高斯散射的大规模实时 3D 重建系统 RTG-SLAM，该系统采用紧凑的高斯表示来减少拟合表面的高斯数量，从而大大降低了内存和计算成本。对于在线高斯优化，该系统针对每帧的三类像素显式添加高斯：新观测的、颜色误差大的和深度误差大的，并且只优化不稳定的高斯，只渲染不稳定的高斯占据的像素，降低了高斯数量和渲染成本。该系统在大规模真实扫描场景中重建，并取得了优于最先进的 NeRF SLAM 方法和并发的 Gaussian SLAM 方法的性能。由于为了在大规模场景中实现实时重建，只使用不透明的高斯和透明的高斯来表示场景，因此与原始的高斯相比，渲染质量不可避免地会下降。如何在保持实时性能的同时提高渲染质量是未来值得探索的方向。此外，反射或透明的材料会导致表面颜色在不同视图之间发生很大变化，使得一些高斯频繁地在两种状态之间切换，并且无法得到很好的优化。未来，该系统还将扩展到处理户外场景、动态物体、快速摄像机运动和光照变化的场景。            （2）：创新点：高斯散射、紧凑的高斯表示、在线高斯优化；性能：在大规模场景中实现实时重建、优于最先进的 NeRF SLAM 方法和并发的 Gaussian SLAM 方法；工作量：降低了内存和计算成本、降低了高斯数量和渲染成本。</code></pre></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-d45b35f06c4dce864863260a5af329f4.jpg" align="middle"><img src="https://pica.zhimg.com/v2-8764f6bde3de348a98aac2f2a4a30ee2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3e7c959078c5d5d3548984d92ce2a3ec.jpg" align="middle"></details><h2 id="GS-LRM-Large-Reconstruction-Model-for-3D-Gaussian-Splatting"><a href="#GS-LRM-Large-Reconstruction-Model-for-3D-Gaussian-Splatting" class="headerlink" title="GS-LRM: Large Reconstruction Model for 3D Gaussian Splatting"></a>GS-LRM: Large Reconstruction Model for 3D Gaussian Splatting</h2><p><strong>Authors:Kai Zhang, Sai Bi, Hao Tan, Yuanbo Xiangli, Nanxuan Zhao, Kalyan Sunkavalli, Zexiang Xu</strong></p><p>We propose GS-LRM, a scalable large reconstruction model that can predict high-quality 3D Gaussian primitives from 2-4 posed sparse images in 0.23 seconds on single A100 GPU. Our model features a very simple transformer-based architecture; we patchify input posed images, pass the concatenated multi-view image tokens through a sequence of transformer blocks, and decode final per-pixel Gaussian parameters directly from these tokens for differentiable rendering. In contrast to previous LRMs that can only reconstruct objects, by predicting per-pixel Gaussians, GS-LRM naturally handles scenes with large variations in scale and complexity. We show that our model can work on both object and scene captures by training it on Objaverse and RealEstate10K respectively. In both scenarios, the models outperform state-of-the-art baselines by a wide margin. We also demonstrate applications of our model in downstream 3D generation tasks. Our project webpage is available at: <a href="https://sai-bi.github.io/project/gs-lrm/">https://sai-bi.github.io/project/gs-lrm/</a> . </p><p><a href="http://arxiv.org/abs/2404.19702v1">PDF</a> Project webpage: <a href="https://sai-bi.github.io/project/gs-lrm/">https://sai-bi.github.io/project/gs-lrm/</a></p><p><strong>Summary</strong><br>三维高斯原语大重建模型，可从 2-4 个姿势稀疏图像预测高质量的三维高斯原语，在单个 A100 GPU 上仅需 0.23 秒。</p><p><strong>Key Takeaways</strong></p><ul><li>使用变压器架构，从图像中预测三维高斯原语。</li><li>模型具有可扩展性，可预测具有大尺度和复杂度变化的场景。</li><li>在 Objaverse 和 RealEstate10K 数据集上均优于现有方法。</li><li>可用于下游三维生成任务，如视图合成和三维形状合成。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>题目：GS-LRM：大型重建模型</p></li><li><p>作者：Kai Zhang、Sai Bi、Hao Tan、Yuanbo Xiangli、Nanxuan Zhao、Kalyan Sunkavalli、Zexiang Xu</p></li><li><p>单位：Adobe Research</p></li><li><p>关键词：Large Reconstruction Models · 3D Reconstruction · Gaussian Splatting</p></li><li><p>论文链接：https://arxiv.org/abs/2404.19702 , Github：None</p></li><li><p>摘要：</p><p>（1）：研究背景：3D 场景重建是计算机视觉中的一个核心问题，传统方法依赖于复杂的光度测量系统和密集的多视图图像。神经表征和可微渲染的进步提高了重建和渲染质量，但速度慢且需要大量输入视图。基于 Transformer 的 3D 大型重建模型 (LRM) 学习了大量的 3D 对象的通用重建先验，实现了前所未有的稀疏视图 3D 重建质量。</p><p>（2）：过去的方法：过去的方法采用三平面 NeRF 作为场景表示，存在三平面分辨率受限和体积渲染开销大的问题，导致训练和渲染速度慢、难以保留精细细节，以及无法扩展到对象中心输入之外的大场景。</p><p>（3）：本论文方法：GS-LRM 是一种可扩展的大型重建模型，它采用了一种非常简单的基于 Transformer 的架构。将输入图像进行块状化，将连接后的多视图图像标记通过一系列 Transformer 块，并直接从这些标记解码最终的逐像素高斯参数以进行可微渲染。与只能重建对象的先前 LRM 不同，GS-LRM 通过预测逐像素高斯函数，自然地处理了规模和复杂性差异很大的场景。</p><p>（4）：方法性能：GS-LRM 在 Objaverse 和 RealEstate10K 数据集上分别针对对象和场景捕捉进行了训练，在两种情况下都大幅优于最先进的基准。该模型还可以在下游 3D 生成任务中得到应用。</p></li><li><p>方法：</p></li></ol><p>（1）：采用 Transformer 模型，将一组已知相机位姿的图像回归为逐像素的 3D 高斯参数；</p><p>（2）：通过 patchify 算子对输入图像进行标记化处理，将多视图图像标记连接起来，并通过一系列 Transformer 块进行处理，包括自注意力和 MLP 层；</p><p>（3）：从每个输出标记中，使用线性层解码对应 patch 中像素对齐的高斯函数属性；</p><p>（4）：利用线性层将 1D 向量映射到 d 维的图像 patch 标记，其中 d 是 Transformer 宽度；</p><p>（5）：将多视图图像标记连接起来，并通过一系列 Transformer 块进行处理，包括残差连接、多头自注意力和 MLP；</p><p>（6）：使用单个线性层将 Transformer 的输出标记解码为高斯参数。</p><ol><li>结论：<pre><code>            (1)：本工作的主要意义在于提出了一种简单且可扩展的基于 Transformer 的大型重建模型，用于高斯 splatting（GS）表示。该方法能够在单个 A100 GPU 上以约 0.23 秒的速度从一组已知相机位姿的图像中进行快速前馈高分辨率 GS 预测。该模型既适用于对象级捕捉，也适用于场景级捕捉，并且在大量数据上训练后，在两种情况下均达到最先进的性能。我们希望我们的工作能够激发未来在数据驱动的前馈 3D 重建领域开展更多工作。致谢感谢 Nathan Carr 和 Duygu Ceylan 提供有益的讨论。            (2)：创新点：提出了一种基于 Transformer 的大型重建模型，用于高斯 splatting 表示，该模型简单且可扩展；性能：在对象级和场景级捕捉任务上均达到最先进的性能；工作量：在单个 A100 GPU 上以约 0.23 秒的速度进行前馈高分辨率 GS 预测。</code></pre></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-7b3dfbd4f62939f8af1187b102bf5134.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5173cd30ad642c9a906d00c88085376d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e761662a90d51192c43a703fedff0bc4.jpg" align="middle"></details><h2 id="SAGS-Structure-Aware-3D-Gaussian-Splatting"><a href="#SAGS-Structure-Aware-3D-Gaussian-Splatting" class="headerlink" title="SAGS: Structure-Aware 3D Gaussian Splatting"></a>SAGS: Structure-Aware 3D Gaussian Splatting</h2><p><strong>Authors:Evangelos Ververas, Rolandos Alexandros Potamias, Jifei Song, Jiankang Deng, Stefanos Zafeiriou</strong></p><p>Following the advent of NeRFs, 3D Gaussian Splatting (3D-GS) has paved the way to real-time neural rendering overcoming the computational burden of volumetric methods. Following the pioneering work of 3D-GS, several methods have attempted to achieve compressible and high-fidelity performance alternatives. However, by employing a geometry-agnostic optimization scheme, these methods neglect the inherent 3D structure of the scene, thereby restricting the expressivity and the quality of the representation, resulting in various floating points and artifacts. In this work, we propose a structure-aware Gaussian Splatting method (SAGS) that implicitly encodes the geometry of the scene, which reflects to state-of-the-art rendering performance and reduced storage requirements on benchmark novel-view synthesis datasets. SAGS is founded on a local-global graph representation that facilitates the learning of complex scenes and enforces meaningful point displacements that preserve the scene’s geometry. Additionally, we introduce a lightweight version of SAGS, using a simple yet effective mid-point interpolation scheme, which showcases a compact representation of the scene with up to 24$\times$ size reduction without the reliance on any compression strategies. Extensive experiments across multiple benchmark datasets demonstrate the superiority of SAGS compared to state-of-the-art 3D-GS methods under both rendering quality and model size. Besides, we demonstrate that our structure-aware method can effectively mitigate floating artifacts and irregular distortions of previous methods while obtaining precise depth maps. Project page <a href="https://eververas.github.io/SAGS/">https://eververas.github.io/SAGS/</a>. </p><p><a href="http://arxiv.org/abs/2404.19149v1">PDF</a> 15 pages, 8 figures, 3 tables</p><p><strong>Summary</strong><br>利用结构驱动的优化策略，SAGS 在实时神经渲染中实现了压缩性和高保真性，通过利用局部-全局图表示来编码场景几何。</p><p><strong>Key Takeaways</strong></p><ul><li>SAGS 通过结构感知优化对 3DGS 进行了改进。</li><li>SAGS 采用局部-全局图表示，捕获场景几何。</li><li>SAGS 优化点位移以保持场景几何，提高表示能力和渲染质量。</li><li>SAGS 提出了一种基于中点插值的轻量级变体，可显著减少模型大小。</li><li>实验表明 SAGS 在渲染质量和模型尺寸方面优于其他 3DGS 方法。</li><li>SAGS 缓解了浮动伪影和不规则失真，并生成精确的深度图。</li><li>SAGS 项目主页：<a href="https://eververas.github.io/SAGS/。">https://eververas.github.io/SAGS/。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: 结构感知3D高斯斑点</p></li><li><p>Authors: Evangelos Ververas, Rolandos Alexandros Potamias, Jifei Song, Jiankang Deng, Stefanos Zafeiriou</p></li><li><p>Affiliation: 帝国理工学院</p></li><li><p>Keywords: Novel View Synthesis, 3D Gaussian Splatting, Structure-Aware, Local-Global Graph Representation</p></li><li><p>Urls: https://eververas.github.io/SAGS/, Github:None</p></li><li><p>Summary:</p><pre><code>            (1):随着NeRFs的出现，3D高斯斑点（3D-GS）为实时神经渲染铺平了道路，克服了体积方法的计算负担。在3D-GS的开创性工作之后，一些方法试图实现可压缩且高保真性能的替代方案。然而，通过采用与几何无关的优化方案，这些方法忽略了场景的固有3D结构，从而限制了表现力和表示的质量，导致各种浮点和伪影。            (2):以往的方法：3D-GS、存在问题：忽略场景的3D结构，导致表现力和表示质量受限，产生浮点和伪影。动机充分：提出一种结构感知的方法来解决这些问题。            (3):本文提出的研究方法：结构感知高斯斑点方法（SAGS），隐式编码场景的几何形状，在基准新视图合成数据集上反映了最先进的渲染性能和降低的存储需求。SAGS建立在局部-全局图表示的基础上，该表示有助于学习复杂场景并强制有意义的点位移以保留场景的几何形状。此外，我们使用简单但有效的中间点插值方案引入了SAGS的轻量级版本，该版本展示了场景的紧凑表示，尺寸最多减少了24倍，而无需依赖任何压缩策略。            (4):任务和性能：在多个基准数据集上的广泛实验表明，SAGS在渲染质量和模型大小方面都优于最先进的3D-GS方法。此外，我们证明了我们的结构感知方法可以有效减轻先前方法的浮动伪影和不规则失真，同时获得精确的深度图。</code></pre></li><li><p>方法：</p><pre><code>            (1):曲率感知稠密化：采用 Grad-PU 方法，对低曲率区域进行中点插值，生成密集点云，增强 3D-GS 的初始化；            (2):结构感知编码器：基于 k-NN 图，使用图神经网络学习局部和全局结构特征，获得结构感知特征编码；            (3):细化网络：使用 MLP 解码结构感知特征编码，预测 3D 高斯斑点的属性（位置、颜色、不透明度、协方差）；            (4):SAGS-Lite：利用中点插值，减少存储需求，生成紧凑的 3D 高斯斑点集合，无需压缩技术。</code></pre></li><li><p>结论：</p><pre><code>            (1):本文提出了一种结构感知高斯斑点方法（SAGS），该方法利用场景的内在结构进行高保真神经渲染。我们提出了一种基于图神经网络的方法，该方法以结构化的方式预测高斯斑点的属性，从而克服了当前 3D 高斯斑点方法的缺点，即天真地优化高斯属性而忽略了底层场景结构。使用所提出的图表示，相邻的高斯斑点可以共享和聚合信息，从而促进场景渲染及其几何形状的保留。我们展示了所提出的方法在新的视图合成中可以优于当前最先进的方法，同时保留 3D-GS 的实时渲染。我们进一步引入了一种简单但有效的中间点插值方案，与 3D-GS 方法相比，它可以实现高达 24 倍的存储减少，同时保留高质量的渲染，而无需使用任何压缩和量化算法。总体而言，我们的研究结果证明了在 3D-GS 中引入结构的好处。            (2):创新点：提出了一种基于图神经网络的结构感知方法，以结构化的方式预测高斯斑点的属性，从而克服了当前 3D 高斯斑点方法的缺点，即天真地优化高斯属性而忽略了底层场景结构；Performance：在新的视图合成中可以优于当前最先进的方法，同时保留 3D-GS 的实时渲染；Workload：引入了简单的中间点插值方案，与 3D-GS 方法相比，它可以实现高达 24 倍的存储减少，同时保留高质量的渲染，而无需使用任何压缩和量化算法。</code></pre></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-1c24f7a12b1fbb5ce1ccb02f3443561a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1a4caa28b767c498c125adefb63f6bdf.jpg" align="middle"><img src="https://picx.zhimg.com/v2-36c36153d224a88e577e256a3ca35a36.jpg" align="middle"><img src="https://picx.zhimg.com/v2-cb72df76b73fb4cab74c37be6a089579.jpg" align="middle"></details><h2 id="MeGA-Hybrid-Mesh-Gaussian-Head-Avatar-for-High-Fidelity-Rendering-and-Head-Editing"><a href="#MeGA-Hybrid-Mesh-Gaussian-Head-Avatar-for-High-Fidelity-Rendering-and-Head-Editing" class="headerlink" title="MeGA: Hybrid Mesh-Gaussian Head Avatar for High-Fidelity Rendering and   Head Editing"></a>MeGA: Hybrid Mesh-Gaussian Head Avatar for High-Fidelity Rendering and   Head Editing</h2><p><strong>Authors:Cong Wang, Di Kang, He-Yi Sun, Shen-Han Qian, Zi-Xuan Wang, Linchao Bao, Song-Hai Zhang</strong></p><p>Creating high-fidelity head avatars from multi-view videos is a core issue for many AR/VR applications. However, existing methods usually struggle to obtain high-quality renderings for all different head components simultaneously since they use one single representation to model components with drastically different characteristics (e.g., skin vs. hair). In this paper, we propose a Hybrid Mesh-Gaussian Head Avatar (MeGA) that models different head components with more suitable representations. Specifically, we select an enhanced FLAME mesh as our facial representation and predict a UV displacement map to provide per-vertex offsets for improved personalized geometric details. To achieve photorealistic renderings, we obtain facial colors using deferred neural rendering and disentangle neural textures into three meaningful parts. For hair modeling, we first build a static canonical hair using 3D Gaussian Splatting. A rigid transformation and an MLP-based deformation field are further applied to handle complex dynamic expressions. Combined with our occlusion-aware blending, MeGA generates higher-fidelity renderings for the whole head and naturally supports more downstream tasks. Experiments on the NeRSemble dataset demonstrate the effectiveness of our designs, outperforming previous state-of-the-art methods and supporting various editing functionalities, including hairstyle alteration and texture editing. </p><p><a href="http://arxiv.org/abs/2404.19026v1">PDF</a> Project page: <a href="https://conallwang.github.io/MeGA_Pages/">https://conallwang.github.io/MeGA_Pages/</a></p><p><strong>Summary</strong><br>根据多视角视频创建高保真头部形象是AR/VR应用的关键问题。MeGA通过为不同头部组件采用合适的表达方式，提高了渲染质量。</p><p><strong>Key Takeaways</strong></p><ul><li>MeGA采用FLAME网格表示面部，并使用UV位移图提供顶点偏移以提升个性化几何细节。</li><li>利用延迟神经渲染获得面部颜色，并将神经纹理分解为三个有意义的部分以实现真实感渲染。</li><li>MeGA使用3D高斯泼溅构建静态经典头发，并应用刚性变换和基于MLP的变形场来处理复杂动态表情。</li><li>结合遮挡感知混合，MeGA为整个头部生成更高保真的渲染，并支持发型改变和纹理编辑等下游任务。</li><li>在NeRSemble数据集上的实验表明MeGA设计有效，优于之前最先进的方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>论文标题：MeGA：用于高保真渲染和头部编辑的混合网格高斯头部头像</p></li><li><p>作者：Cong Wang, Di Kang, He-Yi Sun, Shen-Han Qian, Zi-Xuan Wang, Linchao Bao, Song-Hai Zhang</p></li><li><p>第一作者单位：清华大学</p></li><li><p>关键词：头部头像、高保真渲染、头部编辑、混合表示</p></li><li><p>论文链接：https://arxiv.org/abs/2404.19026</p></li><li><p>摘要：</p></li></ol><p>（1）研究背景：创建高保真头部头像对于 AR/VR 应用至关重要，但现有的方法难以同时为所有头部组件（如皮肤、头发）获得高质量的渲染效果，因为它们使用单一表示来建模具有不同特征的组件。</p><p>（2）以往方法：以往方法探索了基于网格、基于 NeRF 和基于 3D 高斯的表示，取得了显着进展。然而，头部是一个复杂的“物体”，包含具有不同特征的组件，因此不存在单一的表示可以同时很好地建模所有组件。使用单一表示建模所有头部组件必然会牺牲一部分的渲染质量。</p><p>（3）本文方法：本文提出了一种混合网格高斯头部头像（MeGA），使用更合适的表示来建模不同的头部组件。具体来说，选择一个增强的 FLAME 网格作为面部表示，并预测一个 UV 位移图来提供每个顶点的偏移量，以改善个性化的几何细节。为了实现逼真的渲染，使用延迟神经渲染获得面部颜色，并将神经纹理分解为三个有意义的部分。对于头发建模，首先使用 3D 高斯泼溅构建一个静态的规范头发。进一步应用刚性变换和基于 MLP 的变形场来处理复杂的动态表情。结合遮挡感知混合，MeGA 为整个头部生成了更高保真的渲染效果，并自然地支持更多下游任务。</p><p>（4）实验结果：在 NeRSemble 数据集上的实验表明，本文方法有效，优于以往的先进方法，并支持各种编辑功能，包括发型更改和纹理编辑。这些性能支持了本文的目标。</p><ol><li><p>Methods:                    (1): 提出混合网格高斯头部头像（MeGA），使用更合适的表示来建模不同的头部组件；</p><pre><code>            (2): 选择一个增强的 FLAME 网格作为面部表示，并预测一个 UV 位移图来提供每个顶点的偏移量，以改善个性化的几何细节；            (3): 使用延迟神经渲染获得面部颜色，并将神经纹理分解为三个有意义的部分；            (4): 对于头发建模，首先使用 3D 高斯泼溅构建一个静态的规范头发，进一步应用刚性变换和基于 MLP 的变形场来处理复杂的动态表情；            (5): 结合遮挡感知混合，MeGA 为整个头部生成了更高保真的渲染效果，并自然地支持更多下游任务。</code></pre></li><li><p>结论：</p></li></ol><p>（1）：本文提出了一种混合网格高斯头部头像（MeGA），该方法使用神经网格建模人脸，使用 3DGS 建模头发，在高保真渲染和头部编辑方面取得了显着效果。</p><p>（2）：创新点：创新性地提出了混合网格高斯头部头像（MeGA），使用更合适的表示来建模不同的头部组件，并设计了遮挡感知混合模块，实现了头部的高保真渲染和编辑。</p><p>性能：在 NeRSemble 数据集上的实验表明，本文方法在渲染质量和编辑功能方面均优于以往的先进方法。</p><p>工作量：本文方法的工作量相对较大，需要训练神经网格、3DGS 头发模型和遮挡感知混合模块。</p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-37516691b13dabbabb3b74ea46b402d8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-dfaebdbe659151ed19833d736cf99b64.jpg" align="middle"><img src="https://picx.zhimg.com/v2-179dcf87ee24bc1a697ea323e2d146c3.jpg" align="middle"></details><h2 id="3D-Gaussian-Splatting-with-Deferred-Reflection"><a href="#3D-Gaussian-Splatting-with-Deferred-Reflection" class="headerlink" title="3D Gaussian Splatting with Deferred Reflection"></a>3D Gaussian Splatting with Deferred Reflection</h2><p><strong>Authors:Keyang Ye, Qiming Hou, Kun Zhou</strong></p><p>The advent of neural and Gaussian-based radiance field methods have achieved great success in the field of novel view synthesis. However, specular reflection remains non-trivial, as the high frequency radiance field is notoriously difficult to fit stably and accurately. We present a deferred shading method to effectively render specular reflection with Gaussian splatting. The key challenge comes from the environment map reflection model, which requires accurate surface normal while simultaneously bottlenecks normal estimation with discontinuous gradients. We leverage the per-pixel reflection gradients generated by deferred shading to bridge the optimization process of neighboring Gaussians, allowing nearly correct normal estimations to gradually propagate and eventually spread over all reflective objects. Our method significantly outperforms state-of-the-art techniques and concurrent work in synthesizing high-quality specular reflection effects, demonstrating a consistent improvement of peak signal-to-noise ratio (PSNR) for both synthetic and real-world scenes, while running at a frame rate almost identical to vanilla Gaussian splatting. </p><p><a href="http://arxiv.org/abs/2404.18454v1">PDF</a> </p><p><strong>Summary</strong><br>高斯辐射场结合延时着色大幅提升反射效果，无需额外时间成本</p><p><strong>Key Takeaways</strong></p><ul><li>神经和高斯辐射场方法在视图合成中取得巨大进展，但镜面反射处理困难。</li><li>提出了延时着色方法，使用高斯散射有效渲染镜面反射。</li><li>环境贴图反射模型的挑战在于需要准确的表面法线，而法线估计受断续梯度的限制。</li><li>利用延时着色生成的逐像素反射梯度，桥接了相邻高斯的优化过程。</li><li>准确的法线估计逐渐传播，最终覆盖所有反射物体。</li><li>方法大幅优于最先进技术，在合成高质量镜面反射效果方面达到领先水平。</li><li>在合成和真实场景中，峰值信噪比 (PSNR) 均得到一致提高，且运行帧速率几乎与原始高斯散射相同。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: 3D 高斯斑点与延迟反射</p></li><li><p>Authors: Keyang Ye, Qiming Hou, Kun Zhou</p></li><li><p>Affiliation: 浙江大学计算机辅助设计与图形学国家重点实验室</p></li><li><p>Keywords: Novel view synthesis, deferred shading, real-time rendering</p></li><li><p>Urls: Paper: https://arxiv.org/pdf/2404.18454.pdf, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 本文的研究背景是神经辐射场（NeRF）和基于高斯的体渲染方法在新型视图合成中取得了巨大成功，但镜面反射仍然具有挑战性。</p><p>(2): 过去的方法如 3D 高斯斑点（3DGS）虽然提供了基于每个高斯的球谐函数（SH）进行视点相关着色，但其方向频率太有限，无法建模镜面反射。训练过程会产生高斯，以显式地拟合镜面反射，但这种方法会导致视觉伪影和较差的性能。</p><p>(3): 本文提出的研究方法是延迟着色，它利用延迟着色生成的逐像素反射梯度来弥合相邻高斯优化过程之间的差距，允许近乎正确的法线估计逐渐传播，最终覆盖所有反射物体。</p><p>(4): 在合成高质量镜面反射效果的任务上，本文方法明显优于最先进的技术和同期工作，证明了合成和真实场景的峰值信噪比（PSNR）都有持续的提高，同时运行帧速率几乎与原始反射无关的高斯斑点相同。</p><ol><li>方法：</li></ol><p>（1）：本方法采用延迟渲染模型，包含两个阶段；</p><p>（2）：第一阶段是高斯斑点，利用高斯参数 Θ𝑖、每个高斯视点相关的球谐函数颜色 𝑐𝑖 (v) 计算像素颜色 𝐶(v)；</p><p>（3）：第二阶段是延迟反射，将法线向量 𝑛𝑖 和镜面反射强度标量 𝑟𝑖 融入高斯斑点，生成最终像素颜色 𝐶′(v)。</p><ol><li>结论：</li></ol><p>（1）：本文提出了一种高质量的延迟高斯斑点渲染器，专门用于反射。它展示了稳定的训练和几乎与原始 3D 高斯斑点相同的帧速率的竞争性视觉质量，还生成了准确的表面法线和环境贴图。我们的延迟着色方法可能为未来的探索开辟了许多可能性。在高斯斑点的背景下探索渲染方程的更多创造性分割将是一件有趣的事情。我们的管道还可以扩展到超出环境贴图的高质量反射算法，包括屏幕空间反射 [McGuire and Mara 2014] 和硬件光线追踪。将 3D 高斯和可微渲染推广到此类方法可以显著提高反射质量。探索添加基于物理的粗糙度、将我们的方法推广到光泽材料的可能性也很有趣。致谢：这项工作部分得到了国家自然科学基金（编号 62227806 和 U23A20311）和 XPLORER PRIZE 的支持。源代码和数据可从 https://gapszju.github.com/3DGS-DR 获取。</p><p>（2）：创新点：提出了延迟着色方法，利用延迟着色生成的逐像素反射梯度来弥合相邻高斯优化过程之间的差距，允许近乎正确的法线估计逐渐传播，最终覆盖所有反射物体。</p><p>性能：在合成高质量镜面反射效果的任务上，本文方法明显优于最先进的技术和同期工作，证明了合成和真实场景的峰值信噪比（PSNR）都有持续的提高，同时运行帧速率几乎与原始反射无关的高斯斑点相同。</p><p>工作量：本文方法的工作量与原始 3D 高斯斑点相似，在合成高质量镜面反射效果的任务上，本文方法明显优于最先进的技术和同期工作。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-01f3cc91b932b34b556b0aeef26ce855.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7317cab6e01a55d9c668cb2940a49ed4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-df3dd63fe60a37ec3cb9014da779955c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-bd20bc6eb9ac41d7e4d7028cacc5d273.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-8f4637de64153c20555d7c194c23899d.jpg" align="middle"></details><h2 id="High-quality-Surface-Reconstruction-using-Gaussian-Surfels"><a href="#High-quality-Surface-Reconstruction-using-Gaussian-Surfels" class="headerlink" title="High-quality Surface Reconstruction using Gaussian Surfels"></a>High-quality Surface Reconstruction using Gaussian Surfels</h2><p><strong>Authors:Pinxuan Dai, Jiamin Xu, Wenxiang Xie, Xinguo Liu, Huamin Wang, Weiwei Xu</strong></p><p>We propose a novel point-based representation, Gaussian surfels, to combine the advantages of the flexible optimization procedure in 3D Gaussian points and the surface alignment property of surfels. This is achieved by directly setting the z-scale of 3D Gaussian points to 0, effectively flattening the original 3D ellipsoid into a 2D ellipse. Such a design provides clear guidance to the optimizer. By treating the local z-axis as the normal direction, it greatly improves optimization stability and surface alignment. While the derivatives to the local z-axis computed from the covariance matrix are zero in this setting, we design a self-supervised normal-depth consistency loss to remedy this issue. Monocular normal priors and foreground masks are incorporated to enhance the quality of the reconstruction, mitigating issues related to highlights and background. We propose a volumetric cutting method to aggregate the information of Gaussian surfels so as to remove erroneous points in depth maps generated by alpha blending. Finally, we apply screened Poisson reconstruction method to the fused depth maps to extract the surface mesh. Experimental results show that our method demonstrates superior performance in surface reconstruction compared to state-of-the-art neural volume rendering and point-based rendering methods. </p><p><a href="http://arxiv.org/abs/2404.17774v2">PDF</a> Results added and improved</p><p><strong>Summary</strong><br>针对三维高斯点和曲面元素的优点，提出一种新的点云表征方式高斯曲面元素，有效改善了优化稳定性和曲面对齐，并通过容积剪切和基于泊松的筛选重建方法提升了形状重建的精度。</p><p><strong>Key Takeaways</strong></p><ul><li>提出高斯曲面元素，将三维高斯点的优化灵活性与曲面元素的对齐特性相结合。</li><li>通过将三维高斯点的 z 尺度设为 0，将三维椭圆体压平为二维椭圆，为优化器提供清晰指引。</li><li>将局部 z 轴视为法线方向，极大提高了优化的稳定性和表面对齐。</li><li>设计自监督法线深度一致性损失，弥补共方差矩阵中计算出的局部 z 轴导数为零的问题。</li><li>融合单目法线先验和前景掩码，增强重建质量，缓解高光和背景带来的影响。</li><li>提出体积切割方法，聚合高斯曲面元素的信息，去除深度图中由 alpha 混合产生的错误点。</li><li>采用带筛选的泊松重建方法对融合的深度图进行重建，提取表面网格。</li><li>实验结果表明，该方法在表面重建方面优于现有的神经体绘制和点云绘制方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: 高斯表面元的高质量表面重建</p></li><li><p>Authors: Pinxuan Dai, Jiamin Xu, Wenxiang Xie, Xinguo Liu, Huamin Wang, Weiwei Xu</p></li><li><p>Affiliation: 浙江大学计算机辅助设计与图形学国家重点实验室</p></li><li><p>Keywords: 3D Surface Reconstruction, Gaussian Surfels, Depth-normal Consistency</p></li><li><p>Urls: https://arxiv.org/abs/2404.17774 , Github:None</p></li><li><p>Summary: </p></li></ol><p>(1):神经辐射场（NeRF）在图像合成的任务上取得了巨大的成功，但在表面重建任务上仍然存在一些问题，例如表面对齐不准确、优化不稳定以及对高光和背景区域的敏感性。</p><p>(2):以往的方法主要集中在优化损失函数和使用先验信息来解决这些问题，但效果有限。</p><p>(3):本文提出了一种新的点表示——高斯表面元，它结合了 3D 高斯点的灵活优化过程和表面元的表面对齐特性。具体来说，将 3D 高斯点的 z 尺度设置为 0，有效地将原始的 3D 椭球压扁成 2D 椭圆。这种设计为优化器提供了明确的指导，通过将局部 z 轴视为法线方向，极大地提高了优化稳定性和表面对齐。同时设计了一个自监督的法线深度一致性损失来解决此设置中从协方差矩阵计算的局部 z 轴的导数为零的问题。此外，本文还集成了单目法线先验和前景掩码以提高重建质量，减轻了与高光和背景相关的问题。提出了一种体积切割方法来聚合高斯表面元的信息，以去除 alpha 混合生成的深度图中的错误点。最后，将筛选泊松重建方法应用于融合的深度图以提取表面网格。</p><p>(4):实验结果表明，与最先进的神经体渲染和基于点的渲染方法相比，本文的方法在表面重建方面表现出优越的性能。</p><ol><li>方法：</li></ol><p>（1）：提出了一种新的点表示——高斯表面元，它结合了 3D 高斯点的灵活优化过程和表面元的表面对齐特性；</p><p>（2）：设计了一个自监督的法线深度一致性损失来解决此设置中从协方差矩阵计算的局部 z 轴的导数为零的问题；</p><p>（3）：集成了单目法线先验和前景掩码以提高重建质量，减轻了与高光和背景相关的问题；</p><p>（4）：提出了一种体积切割方法来聚合高斯表面元的信息，以去除 alpha 混合生成的深度图中的错误点；</p><p>（5）：将筛选泊松重建方法应用于融合的深度图以提取表面网格。</p><ol><li>结论：</li></ol><p>（1）：本文提出了一种新的点表示——高斯表面元，它结合了 3D 高斯点的灵活优化过程和表面元的表面对齐特性，设计了一个自监督的法线深度一致性损失来解决此设置中从协方差矩阵计算的局部 z 轴的导数为零的问题，集成了单目法线先验和前景掩码以提高重建质量，减轻了与高光和背景相关的问题，提出了一种体积切割方法来聚合高斯表面元的信息，以去除 alpha 混合生成的深度图中的错误点，将筛选泊松重建方法应用于融合的深度图以提取表面网格。通过实验，本文方法在表面重建方面表现出优越的性能。</p><p>（2）：创新点：提出了一种新的点表示——高斯表面元，设计了一个自监督的法线深度一致性损失，集成了单目法线先验和前景掩码，提出了一种体积切割方法来聚合高斯表面元的信息；性能：与最先进的神经体渲染和基于点的渲染方法相比，本文的方法在表面重建方面表现出优越的性能；工作量：本文方法的计算成本相对较高。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-ea805e1d2146685877956d96c3f2411f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d553afdbbcbed9ac6b50b06fa71184c3.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c60a2604158ad2952f8aa6bf05e4bfb8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7ffafd283accaa363affe36c81454980.jpg" align="middle"></details><h2 id="GaussianTalker-Real-Time-High-Fidelity-Talking-Head-Synthesis-with-Audio-Driven-3D-Gaussian-Splatting"><a href="#GaussianTalker-Real-Time-High-Fidelity-Talking-Head-Synthesis-with-Audio-Driven-3D-Gaussian-Splatting" class="headerlink" title="GaussianTalker: Real-Time High-Fidelity Talking Head Synthesis with   Audio-Driven 3D Gaussian Splatting"></a>GaussianTalker: Real-Time High-Fidelity Talking Head Synthesis with   Audio-Driven 3D Gaussian Splatting</h2><p><strong>Authors:Kyusun Cho, Joungbin Lee, Heeji Yoon, Yeobin Hong, Jaehoon Ko, Sangjun Ahn, Seungryong Kim</strong></p><p>We propose GaussianTalker, a novel framework for real-time generation of pose-controllable talking heads. It leverages the fast rendering capabilities of 3D Gaussian Splatting (3DGS) while addressing the challenges of directly controlling 3DGS with speech audio. GaussianTalker constructs a canonical 3DGS representation of the head and deforms it in sync with the audio. A key insight is to encode the 3D Gaussian attributes into a shared implicit feature representation, where it is merged with audio features to manipulate each Gaussian attribute. This design exploits the spatial-aware features and enforces interactions between neighboring points. The feature embeddings are then fed to a spatial-audio attention module, which predicts frame-wise offsets for the attributes of each Gaussian. It is more stable than previous concatenation or multiplication approaches for manipulating the numerous Gaussians and their intricate parameters. Experimental results showcase GaussianTalker’s superiority in facial fidelity, lip synchronization accuracy, and rendering speed compared to previous methods. Specifically, GaussianTalker achieves a remarkable rendering speed up to 120 FPS, surpassing previous benchmarks. Our code is made available at <a href="https://github.com/KU-CVLAB/GaussianTalker/">https://github.com/KU-CVLAB/GaussianTalker/</a> . </p><p><a href="http://arxiv.org/abs/2404.16012v2">PDF</a> Project Page: <a href="https://ku-cvlab.github.io/GaussianTalker">https://ku-cvlab.github.io/GaussianTalker</a></p><p><strong>Summary</strong><br>高斯说话者：实时生成姿势可控会说话的头部</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种名为高斯说话者的新框架，用于实时生成姿势可控的会说话的头部。</li><li>利用 3D 高斯 splatting（3DGS）的快速渲染能力，同时解决了直接使用语音音频控制 3DGS 的挑战。</li><li>构建头部规范的 3DGS 表示，并使其与音频同步变形。</li><li>关键的见解是将 3D 高斯属性编码成共享的隐式特征表示，并在其中与音频特征合并以控制每个高斯属性。</li><li>该设计利用了空间感知特征，并强制相邻点之间的交互。</li><li>将特征嵌入馈送到空间-音频注意模块，该模块预测每个高斯的属性的帧级偏移。</li><li>与以前的串联或乘法方法相比，它在处理大量高斯及其复杂参数时更稳定。</li><li>实验结果表明，与以前的方法相比，高斯说话者在面部保真度、唇形同步精度和渲染速度方面更胜一筹。</li><li>具体而言，高斯说话者以高达 120 FPS 的非凡渲染速度，超过了之前的基准。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: 高斯说话者：实时高保真说话头部合成</p></li><li><p>Authors: Kyusun Cho，Joungbin Lee，Heeji Yoon，Yeobin Hong，Jaehoon Ko，Sangjun Ahn，Seungryong Kim</p></li><li><p>Affiliation: 韩国大学</p></li><li><p>Keywords: Talking Head Generation, 3D Controllable Head, 3D Gaussian Splatting</p></li><li><p>Urls: https://ku-cvlab.github.io/GaussianTalker/ , https://github.com/ku-cvlab/GaussianTalker</p></li><li><p>Summary:</p></li></ol><p>(1): 本文的研究背景是生成受任意语音音频驱动的说话头部视频，这项任务有很多用途，包括生成数字人、虚拟替身、电影制作和电话会议。</p><p>(2): 过去的方法有使用生成模型来解决此任务，但它们不专注于控制头部姿势，这限制了它们的真实性和适用性。最近，许多研究应用神经辐射场（NeRF）来创建可控姿势的说话人像。通过直接调节 NeRF 多层感知器（MLP）中的音频特征，这些方法可以合成与输入音频嘴唇同步的视图一致的 3D 头部结构。虽然这些基于 NeRF 的技术实现了高质量和一致的视觉输出，但它们缓慢的推理速度限制了它们的实用性。</p><p>(3): 本文提出的研究方法是利用 3D 高斯溅射（3DGS）的快速渲染能力。3DGS 被公认为 NeRF 的可行替代方案，它提供了可比的渲染质量，同时显着提高了推理速度。虽然 3DGS 最初被提议用于重建静态 3D 场景，但后续工作已将其扩展到动态场景。然而，很少有研究利用 3DGS 创建具有可控输入的动态 3D 场景，其中大多数都专注于使用中间网格表示来驱动 3D 高斯。然而，依赖中间 3D 网格表示（例如 FLAME）进行变形通常缺乏头发和面部皱纹的细节。</p><p>(4): 本文方法在任务和性能上取得了以下成就：- 与现有的 3D 说话人脸合成模型相比，本文方法在保真度、唇形同步和推理时间方面取得了优异的性能，并且以更高的 FPS 运行。- 本文方法实现了高达 120 FPS 的显着渲染速度，超过了之前的基准。</p><ol><li>方法：</li></ol><p>（1）：本文方法采用 3D 高斯溅射（3DGS）的快速渲染能力，3DGS 被公认为 NeRF 的可行替代方案，它提供了可比的渲染质量，同时显着提高了推理速度；</p><p>（2）：本文方法通过学习具有三平面表示的规范 3D 高斯体来学习说话头的规范形状，多分辨率三平面表示利用 3DGS 的显式 3D 表示，同时还利用隐式神经辐射场的编码空间信息，对于每个规范 3D 位置，从多分辨率三平面表示中提取特征嵌入，这些特征嵌入用于计算每个点的比例、旋转、球谐函数和不透明度，这些计算出的属性构成了说话头的规范 3D 高斯体；</p><p>（3）：本文方法采用语音动作交叉注意模块融合 3D 高斯体特征和音频特征，以准确建模由输入音频驱动的面部运动，空间音频注意模块包含多组交叉注意层和前馈层，每组通过跳跃连接相互连接，该模块将空间特征与第 n 帧的音频特征进行交叉注意计算，从而输出特征成功地将音频特征与每个 3D 高斯体捕获的丰富面部细节相融合；</p><p>（4）：本文方法通过引入附加输入条件来捕获非语言动作，从而将与语音相关的运动与单目视频区分开来，遵循先前的工作，首先应用显式眨眼控制与眼睛特征，具体地，使用面部动作编码系统中的 AU45 来描述眨眼程度，并利用正弦位置编码以匹配输入维度，此外，将摄像机视点作为辅助输入以区分非语言场景变化，虽然将逐帧摄像机公式化为面部视点，但典型的视频是在头部连续移动时使用静态摄像机拍摄的，因此，肖像图像的变化（例如头发位移和光照变化）独立于语音音频，因此，使用面部视点嵌入作为附加输入条件来区分这些非听觉变化。</p><ol><li>结论：</li></ol><p>（1）：本文提出了 GaussianTalker，一个新颖的实时姿态可控 3D 说话人脸合成框架，利用 3D 高斯体进行头部表示。我们的方法通过调节高斯原语实现了对高斯原语的精确控制，从而获得了比以往更好的保真度、唇形同步和推理时间，并且以更高的 FPS 运行。</p><p>（2）：创新点：利用 3D 高斯体进行头部表示，实现了姿态可控的 3D 说话人脸合成；性能：在保真度、唇形同步和推理时间方面取得了优异的性能，实现了高达 120 FPS 的显着渲染速度；工作量：与现有的 3D 说话人脸合成模型相比，本文方法在保真度、唇形同步和推理时间方面取得了优异的性能，并且以更高的 FPS 运行。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-ec62564096d07c9b5ec4f0c103bde8c1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e6d1f872d0b6fbc00f9aa1ae895fe7bd.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-47a55dc6279dc78a414592ec16000227.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7262a04c0986b2720469c095a4a797a4.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c81028949da35d198f3a39ea50a55970.jpg" align="middle"></details><h2 id="OMEGAS-Object-Mesh-Extraction-from-Large-Scenes-Guided-by-Gaussian-Segmentation"><a href="#OMEGAS-Object-Mesh-Extraction-from-Large-Scenes-Guided-by-Gaussian-Segmentation" class="headerlink" title="OMEGAS: Object Mesh Extraction from Large Scenes Guided by Gaussian   Segmentation"></a>OMEGAS: Object Mesh Extraction from Large Scenes Guided by Gaussian   Segmentation</h2><p><strong>Authors:Lizhi Wang, Feng Zhou, Jianqin Yin</strong></p><p>Recent advancements in 3D reconstruction technologies have paved the way for high-quality and real-time rendering of complex 3D scenes. Despite these achievements, a notable challenge persists: it is difficult to precisely reconstruct specific objects from large scenes. Current scene reconstruction techniques frequently result in the loss of object detail textures and are unable to reconstruct object portions that are occluded or unseen in views. To address this challenge, we delve into the meticulous 3D reconstruction of specific objects within large scenes and propose a framework termed OMEGAS: Object Mesh Extraction from Large Scenes Guided by GAussian Segmentation. OMEGAS employs a multi-step approach, grounded in several excellent off-the-shelf methodologies. Specifically, initially, we utilize the Segment Anything Model (SAM) to guide the segmentation of 3D Gaussian Splatting (3DGS), thereby creating a basic 3DGS model of the target object. Then, we leverage large-scale diffusion priors to further refine the details of the 3DGS model, especially aimed at addressing invisible or occluded object portions from the original scene views. Subsequently, by re-rendering the 3DGS model onto the scene views, we achieve accurate object segmentation and effectively remove the background. Finally, these target-only images are used to improve the 3DGS model further and extract the definitive 3D object mesh by the SuGaR model. In various scenarios, our experiments demonstrate that OMEGAS significantly surpasses existing scene reconstruction methods. Our project page is at: <a href="https://github.com/CrystalWlz/OMEGAS">https://github.com/CrystalWlz/OMEGAS</a> </p><p><a href="http://arxiv.org/abs/2404.15891v2">PDF</a> arXiv admin note: text overlap with arXiv:2311.17061 by other authors</p><p><strong>Summary</strong><br>大型场景中特定物体的高精度三维重建框架：OMEGAS，通过高斯分割引导物体网格提取。</p><p><strong>Key Takeaways</strong></p><ul><li>OMEGAS 框架可从大型场景中高精度重建特定物体。</li><li>结合 Segment Anything Model (SAM) 和大型扩散先验，改善 3DGS 模型细节。</li><li>重新渲染 3DGS 模型，实现准确物体分割并去除背景。</li><li>使用目标图像，进一步优化 3DGS 模型并提取最终 3D 物体网格。</li><li>OMEGAS 在各种场景中优于现有场景重建方法。</li><li>代码和数据可在 <a href="https://github.com/CrystalWlz/OMEGAS">https://github.com/CrystalWlz/OMEGAS</a> 获取。</li><li>OMEGAS 适用于目标物体部分遮挡或不可见的情况。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: OMEGAS: 高斯分割引导的大场景物体网格提取</p></li><li><p>Authors: Lizhi Wang, Feng Zhou, Jianqin Yin</p></li><li><p>Affiliation: 北京邮电大学</p></li><li><p>Keywords: Mesh Reconstruction, 3D Gaussian Splatting, Diffusion Model</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2404.15891 , Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 随着 3D 重建技术的进步，复杂 3D 场景的高质量实时渲染成为可能。然而，从大场景中精确重建特定物体仍然是一个挑战。现有的场景重建技术经常导致物体细节纹理丢失，并且无法重建在视图中被遮挡或看不见的物体部分。</p><p>(2): 过去的重建方法难以处理大场景中复杂物体，并且在处理遮挡和不可见区域时存在问题。本文提出的方法以 3D 高斯点云（3DGS）为基础，利用扩散模型来细化细节，并结合目标分割和网格提取技术，以提高重建精度和效率。</p><p>(3): 本文提出 OMEGAS 框架，该框架采用多步方法，首先利用分割任何模型（SAM）指导 3DGS 的分割，创建目标物体的基本 3DGS 模型。然后，利用大规模扩散先验进一步细化 3DGS 模型的细节，特别是针对原始场景视图中不可见或被遮挡的物体部分。随后，将 3DGS 模型重新渲染到场景视图上，实现精确的目标分割并有效去除背景。最后，利用这些仅包含目标的图像进一步改进 3DGS 模型，并通过 SuGaR 模型提取最终的 3D 物体网格。</p><p>(4): 在各种场景中，实验表明 OMEGAS 明显优于现有的场景重建方法。该方法在处理复杂物体、遮挡和不可见区域方面表现出色，能够生成高质量的 3D 物体网格，为增强现实、游戏和大规模 3D 数据集生成等下游任务提供了支持。</p><ol><li>方法：</li></ol><p>（1）：利用 SAM 引导 3DGS 分割，构建目标物体的基本 3DGS 模型；</p><p>（2）：应用大规模扩散先验（Stable Diffusion）细化 3DGS 模型细节，特别是不可见或被遮挡部分；</p><p>（3）：将 3DGS 模型重新渲染到场景视图上，获得精确目标分割并去除背景；</p><p>（4）：使用仅包含目标的图像进一步改进 3DGS 模型，通过 SuGaR 模型提取最终 3D 物体网格。</p><ol><li>结论：</li></ol><p>（1）：本文提出了一种基于高斯分割引导的大场景物体网格提取框架 OMEGAS，该框架能够从多视角场景图像中有效提取目标物体的精细网格，并能够重建被遮挡或不可见的物体部分。OMEGAS 创新性地融合了 SAM、3DGS、Stabled Diffusion 和 SuGaR 模型等多种优秀方法。与基线方法相比，我们的方法在目标的细节纹理和抗遮挡性方面均表现出极大的优势。我们希望 OMEGAS 能够为 3D 重建领域提供新的思路，并为下游任务提供更好的解决方案。</p><p>（2）：创新点：提出了一种基于高斯分割引导的大场景物体网格提取框架 OMEGAS；性能：在处理复杂物体、遮挡和不可见区域方面表现出色，能够生成高质量的 3D 物体网格；工作量：与基线方法相比，OMEGAS 的计算成本较高，需要较长的处理时间。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-e4ef7cc371681a1b1a10401043bee74c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0a62638572af15479eb987b4dae28d70.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9b782cee0b88f29d10ae78c3dec02dbc.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8b5e6dde7b196e2a509f4476175ec837.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e8a1d8354bdc5dd159b09b49e8c7efd4.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f986fc824e450fb1b91fc8f6304e7c73.jpg" align="middle"></details><h2 id="GaussianTalker-Speaker-specific-Talking-Head-Synthesis-via-3D-Gaussian-Splatting"><a href="#GaussianTalker-Speaker-specific-Talking-Head-Synthesis-via-3D-Gaussian-Splatting" class="headerlink" title="GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian   Splatting"></a>GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian   Splatting</h2><p><strong>Authors:Hongyun Yu, Zhan Qu, Qihang Yu, Jianchuan Chen, Zhonghua Jiang, Zhiwen Chen, Shengyu Zhang, Jimin Xu, Fei Wu, Chengfei Lv, Gang Yu</strong></p><p>Recent works on audio-driven talking head synthesis using Neural Radiance Fields (NeRF) have achieved impressive results. However, due to inadequate pose and expression control caused by NeRF implicit representation, these methods still have some limitations, such as unsynchronized or unnatural lip movements, and visual jitter and artifacts. In this paper, we propose GaussianTalker, a novel method for audio-driven talking head synthesis based on 3D Gaussian Splatting. With the explicit representation property of 3D Gaussians, intuitive control of the facial motion is achieved by binding Gaussians to 3D facial models. GaussianTalker consists of two modules, Speaker-specific Motion Translator and Dynamic Gaussian Renderer. Speaker-specific Motion Translator achieves accurate lip movements specific to the target speaker through universalized audio feature extraction and customized lip motion generation. Dynamic Gaussian Renderer introduces Speaker-specific BlendShapes to enhance facial detail representation via a latent pose, delivering stable and realistic rendered videos. Extensive experimental results suggest that GaussianTalker outperforms existing state-of-the-art methods in talking head synthesis, delivering precise lip synchronization and exceptional visual quality. Our method achieves rendering speeds of 130 FPS on NVIDIA RTX4090 GPU, significantly exceeding the threshold for real-time rendering performance, and can potentially be deployed on other hardware platforms. </p><p><a href="http://arxiv.org/abs/2404.14037v2">PDF</a> <a href="https://yuhongyun777.github.io/GaussianTalker/">https://yuhongyun777.github.io/GaussianTalker/</a></p><p><strong>Summary</strong><br>3D高斯散布技术合成音频驱动说话人头像，精准唇部动作及动态高斯渲染，实现逼真流畅的说话人头像合成。</p><p><strong>Key Takeaways</strong></p><ul><li>基于3D高斯散布的音频驱动说话人头像合成新方法。</li><li>显式高斯表示，通过将高斯与3D面部模型绑定，实现面部运动的直观控制。</li><li>扬声器特定运动转换器，实现精准的扬声器特定唇部动作。</li><li>动态高斯渲染器，通过潜在姿势引入扬声器特定混合形状，增强面部细节表示。</li><li>广泛实验表明，GaussianTalker在说话人头像合成中优于现有最先进的方法。</li><li>渲染速度达到 130 FPS，显着超过实时渲染性能阈值。</li><li>可部署在其他硬件平台上，具有实际应用潜力。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: 高斯说话者：基于 3D 高斯喷射的特定说话者说话头合成</p></li><li><p>Authors: Hongyun Yu, Zhan Qu, Qihang Yu, Jianchuan Chen, Zhonghua Jiang, Zhiwen Chen, Shengyu Zhang, Jimin Xu, Fei Wu, Chengfei Lv, Gang Yu</p></li><li><p>Affiliation: 阿里巴巴集团</p></li><li><p>Keywords: Audio-driven talking head synthesis, 3D Gaussian Splatting, Lip motion control, Facial animation</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2404.14037, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 研究背景：最近基于神经辐射场 (NeRF) 的音频驱动说话头合成工作取得了令人印象深刻的成果。然而，由于 NeRF 隐式表示导致的姿势和表情控制不足，这些方法仍然存在一些限制，例如不同步或不自然的唇部动作，以及视觉抖动和伪影。</p><p>(2): 过去的方法：现有方法存在唇部运动不同步、表情控制不足等问题。本文提出的方法动机明确。</p><p>(3): 研究方法：本文提出了一种基于 3D 高斯喷射的音频驱动说话头合成新方法 GaussianTalker。通过将高斯体绑定到 3D 面部模型，利用 3D 高斯体的显式表示特性，实现了对面部动作的直观控制。GaussianTalker 由两个模块组成：特定说话者运动转换器和动态高斯渲染器。特定说话者运动转换器通过通用音频特征提取和定制唇部动作生成，实现了特定于目标说话者的准确唇部动作。动态高斯渲染器引入了特定说话者的混合形状，以实现精确的表情控制。</p><p>(4): 性能：在说话头合成任务上，GaussianTalker 在唇部运动同步、表情控制和视觉质量方面均取得了最先进的性能。这些性能支持了本文提出的方法的目标。</p><ol><li>方法：</li></ol><p>（1）：提出基于 3D 高斯喷射的音频驱动说话头合成新方法 GaussianTalker，通过将高斯体绑定到 3D 面部模型，利用 3D 高斯体的显式表示特性，实现了对面部动作的直观控制；</p><p>（2）：GaussianTalker 由两个模块组成：特定说话者运动转换器和动态高斯渲染器。特定说话者运动转换器通过通用音频特征提取和定制唇部动作生成，实现了特定于目标说话者的准确唇部动作；</p><p>（3）：动态高斯渲染器引入了特定说话者的混合形状，以实现精确的表情控制；</p><p>（4）：在说话头合成任务上，GaussianTalker 在唇部运动同步、表情控制和视觉质量方面均取得了最先进的性能。</p><ol><li>结论：</li></ol><p>（1）：本工作的主要贡献在于提出了一种基于 3D 高斯喷射的音频驱动说话头合成新方法 GaussianTalker，将高斯体与 FLAME 模型相结合，通过特定说话者的 FLAME 转换器和动态高斯渲染器，实现了唇部动作同步、表情控制和视觉质量的提升；</p><p>（2）：创新点：提出基于 3D 高斯喷射的音频驱动说话头合成新方法 GaussianTalker，通过特定说话者的 FLAME 转换器和动态高斯渲染器，实现了唇部动作同步、表情控制和视觉质量的提升；性能：在说话头合成任务上，GaussianTalker 在唇部运动同步、表情控制和视觉质量方面均取得了最先进的性能；工作量：GaussianTalker 的渲染速度极快，远超其他方法；</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-8b7befef7722d03c798c559087362540.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-00e62c0d66ff2641b9803987918d6fd0.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS 方向最新论文已更新，请持续关注 Update in 2024-05-02  Spectrally Pruned Gaussian Fields with Neural Compensation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/05/02/Paper/2024-05-02/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/05/02/Paper/2024-05-02/Talking%20Head%20Generation/</id>
    <published>2024-05-02T02:33:18.000Z</published>
    <updated>2024-05-02T02:33:18.719Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-05-02-更新"><a href="#2024-05-02-更新" class="headerlink" title="2024-05-02 更新"></a>2024-05-02 更新</h1><h2 id="EMOPortraits-Emotion-enhanced-Multimodal-One-shot-Head-Avatars"><a href="#EMOPortraits-Emotion-enhanced-Multimodal-One-shot-Head-Avatars" class="headerlink" title="EMOPortraits: Emotion-enhanced Multimodal One-shot Head Avatars"></a>EMOPortraits: Emotion-enhanced Multimodal One-shot Head Avatars</h2><p><strong>Authors:Nikita Drobyshev, Antoni Bigata Casademunt, Konstantinos Vougioukas, Zoe Landgraf, Stavros Petridis, Maja Pantic</strong></p><p>Head avatars animated by visual signals have gained popularity, particularly in cross-driving synthesis where the driver differs from the animated character, a challenging but highly practical approach. The recently presented MegaPortraits model has demonstrated state-of-the-art results in this domain. We conduct a deep examination and evaluation of this model, with a particular focus on its latent space for facial expression descriptors, and uncover several limitations with its ability to express intense face motions. To address these limitations, we propose substantial changes in both training pipeline and model architecture, to introduce our EMOPortraits model, where we:   Enhance the model’s capability to faithfully support intense, asymmetric face expressions, setting a new state-of-the-art result in the emotion transfer task, surpassing previous methods in both metrics and quality.   Incorporate speech-driven mode to our model, achieving top-tier performance in audio-driven facial animation, making it possible to drive source identity through diverse modalities, including visual signal, audio, or a blend of both.   We propose a novel multi-view video dataset featuring a wide range of intense and asymmetric facial expressions, filling the gap with absence of such data in existing datasets. </p><p><a href="http://arxiv.org/abs/2404.19110v1">PDF</a> </p><p><strong>Summary</strong><br>人脸头像通过视觉信号驱动，在跨人物合成中颇受欢迎，即使驾驶员与动画人物不同，这一富有挑战性且高度实用的方法也适用。最近提出的 MegaPortrait 模型已在这个领域展现了最先进的效果。我们对该模型进行了深入的检查和评估，特别关注其表情描述符的潜在空间，并发现了该模型表达强烈面部动作的能力存在一些局限性。为了解决这些限制，我们在训练管道和模型架构中都提出了重大改变，从而引入了我们的 EMOPortraits 模型，在此我们： 提高了模型对于精确表现强烈的不对称面部表情的能力，在情绪传递任务中创下了新的最先进成果，在指标和质量方面均超过了先前的所有方法。 将基于语音的模式纳入我们的模型中，在基于音频的面部动画中取得了一流的性能，从而可以通过包括视觉信号、音频或两者的融合在内等各种方式驱动源身份。 我们提出了一个新颖的多视图视频数据集，其中包含广泛的强烈和不对称面部表情，填补了现有数据集中缺少此类数据空白的问题。</p><p><strong>Key Takeaways</strong></p><ul><li>EMOPortraits 模型显著提高了生成强烈不对称面部表情的能力。</li><li>新模型在情感传递任务中超越了现有方法，在指标和质量方面均创下新高。</li><li>EMOPortraits 模型集成了语音驱动模式，在音频驱动的面部动画中实现了顶级性能。</li><li>该模型支持通过视觉信号、音频或两者融合等多种方式进行驱动。</li><li>研究人员提出了一个新的多视图视频数据集，其中包含广泛的强烈和不对称面部表情。</li><li>该数据集填补了现有数据集中此类数据的空白。</li><li>EMOPortraits 模型为跨驱动合成和音频驱动的面部动画提供了新的最先进方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: EMOPortraits: 情感增强的多模态一次性头部头像</p></li><li><p>Authors: Nikita Drobyshev, Antoni Bigata Casademunt, Konstantinos Vougioukas, Zoe Landgraf, Stavros Petridis, Maja Pantic</p></li><li><p>Affiliation: 伦敦帝国理工学院</p></li><li><p>Keywords: 头部头像、情感传递、语音驱动面部动画、多模态、面部表情描述符</p></li><li><p>Urls: https://arxiv.org/abs/2404.19110v1, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 研究背景：头部头像动画在交叉驱动合成中越来越受欢迎，其中驱动者与动画角色不同，这是一种具有挑战性但非常实用的方法。最近提出的 MegaPortraits 模型已在此领域展示了最先进的结果。</p><p>(2): 过去的方法：本文对 MegaPortraits 模型进行了深入的检查和评估，特别关注其面部表情描述符的潜在空间，并发现了其在表达强烈面部动作方面的几个限制。</p><p>(3): 本文提出的研究方法：为了解决这些限制，本文在训练管道和模型架构中提出了实质性的改变，引入了 EMOPortraits 模型，其中：    - 增强了模型对强烈、不对称面部表情的忠实支持能力，在情感传递任务中设定了新的最先进结果，在指标和质量方面都超越了以前的方法。    - 将语音驱动模式纳入模型，在音频驱动的面部动画中实现了顶级性能，使得可以通过视觉信号、音频或两者的混合等多种方式驱动源身份。    - 提出了一个新颖的多视角视频数据集，其中包含各种强烈和不对称的面部表情，填补了现有数据集中此类数据的空白。</p><p>(4): 性能和目标支持：在情感传递任务中，EMOPortraits 模型在指标和质量方面都超越了以前的方法。在音频驱动的面部动画中，该模型实现了顶级性能。这些性能支持了本文的目标，即增强模型对强烈面部表情的支持能力，并将其用于多模态驱动。</p><ol><li>Methods:</li></ol><p>(1): 对 MegaPortraits 模型的潜在空间进行深入检查和评估，发现其在表达强烈面部动作方面的限制；</p><p>(2): 在训练管道和模型架构中提出实质性改变，引入 EMOPortraits 模型，增强其对强烈、不对称面部表情的支持能力；</p><p>(3): 将语音驱动模式纳入模型，实现音频驱动的面部动画的顶级性能；</p><p>(4): 提出一个新颖的多视角视频数据集，填补现有数据集中强烈和不对称面部表情数据的空白。</p><ol><li>结论：<pre><code>           （1）：本文提出了 EMOPortraits，一种在图像驱动、跨身份情感转换中具有卓越性能的新型神经头像创建方法。我们的语音驱动模式使得可以通过多种条件（视频、音频、头部运动）来驱动面部动画。我们收集了 FEED 数据集，我们相信这将成为从事各种以人为中心研究的研究人员的宝贵资产。然而，我们的方法也有一些局限性。它不会生成头像的身体或肩膀，从而限制了一些用例。我们目前将我们的输出与源图像主体集成在一起。此外，该模型有时难以进行准确的表情转换，并且在头部大幅旋转时表现不佳。这些挑战对于未来的增强至关重要，并仍然是我们正在进行的研究工作的核心。           （2）：创新点：增强了模型对强烈、不对称面部表情的支持能力，并将其用于多模态驱动；性能：在情感传递任务中超越了以前的方法，在音频驱动的面部动画中实现了顶级性能；工作量：提出了一个新颖的多视角视频数据集，填补了现有数据集中强烈和不对称面部表情数据的空白。</code></pre></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-ae09218eb625859aeda612581ba59975.jpg" align="middle"><img src="https://picx.zhimg.com/v2-76da3d8060bb28f6e1488ffdcf42c493.jpg" align="middle"><img src="https://picx.zhimg.com/v2-07199851d15b47c4d1a719b68cd3f240.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-29a1efddd95063c164480f3a84bf5f72.jpg" align="middle"><img src="https://picx.zhimg.com/v2-cc5fc34eb617f15c5ecceee7d25f9f5c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e7948dbe17eb67516e7078da09fc10ae.jpg" align="middle"></details><h2 id="CSTalk-Correlation-Supervised-Speech-driven-3D-Emotional-Facial-Animation-Generation"><a href="#CSTalk-Correlation-Supervised-Speech-driven-3D-Emotional-Facial-Animation-Generation" class="headerlink" title="CSTalk: Correlation Supervised Speech-driven 3D Emotional Facial   Animation Generation"></a>CSTalk: Correlation Supervised Speech-driven 3D Emotional Facial   Animation Generation</h2><p><strong>Authors:Xiangyu Liang, Wenlin Zhuang, Tianyong Wang, Guangxing Geng, Guangyue Geng, Haifeng Xia, Siyu Xia</strong></p><p>Speech-driven 3D facial animation technology has been developed for years, but its practical application still lacks expectations. The main challenges lie in data limitations, lip alignment, and the naturalness of facial expressions. Although lip alignment has seen many related studies, existing methods struggle to synthesize natural and realistic expressions, resulting in a mechanical and stiff appearance of facial animations. Even with some research extracting emotional features from speech, the randomness of facial movements limits the effective expression of emotions. To address this issue, this paper proposes a method called CSTalk (Correlation Supervised) that models the correlations among different regions of facial movements and supervises the training of the generative model to generate realistic expressions that conform to human facial motion patterns. To generate more intricate animations, we employ a rich set of control parameters based on the metahuman character model and capture a dataset for five different emotions. We train a generative network using an autoencoder structure and input an emotion embedding vector to achieve the generation of user-control expressions. Experimental results demonstrate that our method outperforms existing state-of-the-art methods. </p><p><a href="http://arxiv.org/abs/2404.18604v1">PDF</a> </p><p><strong>Summary</strong><br>语音驱动的 3D 人脸动画技术已发展多年，但其在实际应用中仍未达到预期。</p><p><strong>Key Takeaways</strong></p><ul><li>数据量限制、唇形对齐和面部表情的自然性是语音驱动 3D 人脸动画技术面临的主要挑战。</li><li>现有的唇形对齐方法仍难以合成自然逼真的表情，导致面部动画表现机械僵硬。</li><li>从语音中提取情绪特征，但面部动作的随机性限制了情绪的有效表达。</li><li>本文提出了一种名为 CSTalk（相关性监督）的方法，通过模拟面部动作不同区域之间的相关性，指导生成模型的训练，从而生成符合人类面部运动模式的逼真表情。</li><li>我们使用基于元人类角色模型的丰富控制参数生成更精细的动画，并为五种不同情绪采集了一个数据集。</li><li>我们使用自编码器结构训练了一个生成网络，输入一个情感嵌入向量来实现用户控制表情的生成。</li><li>实验结果表明，我们的方法优于现有的最先进方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: CSTalk: 基于相关性的语音驱动 3D 情感面部动画生成</p></li><li><p>Authors: Xiangyu Liang, Wenlin Zhuang, Tianyong Wang, Guangxing Geng, Guangyue Geng, Haifeng Xia, Siyu Xia</p></li><li><p>Affiliation: 东南大学自动化学院</p></li><li><p>Keywords: Speech-driven facial animation, 3D facial animation, Emotional expression, Correlation supervision, MetaHuman character model</p></li><li><p>Urls: Paper, Github:None</p></li><li><p>Summary:</p></li></ol><p>(1):语音驱动的 3D 面部动画技术虽然发展多年，但其实际应用效果仍未达到预期。主要挑战在于数据限制、唇部对齐和面部表情的自然度。虽然唇部对齐已有许多相关研究，但现有方法难以合成自然逼真的表情，导致面部动画呈现机械僵硬的外观。即使有些研究从语音中提取情绪特征，但面部动作的随机性限制了情绪的有效表达。</p><p>(2):语音驱动的 3D 面部动画生成方法主要有两种：基于网格和基于参数化。基于网格的方法直接操纵面部顶点，允许面部表情进行复杂的变化，而基于参数化的方法采用基于模板的框架，通过特定参数控制面部动作。基于参数化的模型中，基于 ARKit 2 标准的 blend-shape 模型已被广泛应用，它采用一组预定义的面部子动作作为模板，通过线性组合分配给每个子动作的权重来生成不同的表情。这种方法提供了高度的可控性和泛化性，可以在不同的虚拟形象中复用动画参数。然而，简单 52 维数据的线性组合不足以实现逼真自然的动画。特别是，准确捕捉上部面部区域的细微表情仍然是一个挑战。因此，以往的研究主要集中在对齐唇部动作上。虽然有一些尝试将情感特征融入到面部表演中，例如 Faceformer 和 Emotalk，但这些努力主要集中在从音频中提取情感线索，而忽略了面部表情重建的优化。</p><p>(3):本文采用基于 Epic 提出的 MetaHuman 角色模型的参数化模型，通过 185 个控制装备操纵面部动画，每个装备对应一组面部肌肉。通过非线性变形其各自区域内的面部顶点，MetaHuman 模型在捕捉复杂表情方面表现出潜力。剩下的问题是从语音预测适当的控制装备曲线。研究表明，不同区域的面部动作之间存在相关性，这既源于协调肌肉控制的物理约束，也源于习惯模式。具体来说，在压力和语音停顿期间，嘴巴、眉毛和脸颊等区域往往会同时运动以传达表情意图。因此，本文采用基于 Transformer 的编码器对相关性进行建模。使用该模型作为监督，训练一个 3D 面部动画生成模型。</p><p>(4):本文提出的 CSTalk 方法在不同的情感状态下能够生成复杂的表情。基于 Transformer 编码器对特定情感中面部动作之间的相关性进行建模，作为约束条件，生成的面部表情更符合真实人类语音表情。</p><ol><li>Methods:</li></ol><p>(1):基于 Epic 提出的 MetaHuman 角色模型，利用 185 个控制装备操纵面部动画，每个装备对应一组面部肌肉；</p><p>(2):利用 Transformer 编码器对不同区域的面部动作之间的相关性进行建模，作为监督，训练一个 3D 面部动画生成模型；</p><p>(3):提出的 CSTalk 方法在不同的情感状态下能够生成复杂的表情，基于 Transformer 编码器对特定情感中面部动作之间的相关性进行建模，作为约束条件，生成的面部表情更符合真实人类语音表情。</p><ol><li>结论：</li></ol><p>（1）：本文提出了一种语音驱动的3D情感面部动画生成网络CSTalk，能够生成唇部动作对齐且表情逼真的动画。并且我们首次引入了基于MetaHuman的面部控制装备模型，使其能够与艺术家直接协作并在工业管道中应用。生成的动画参数与身份无关，并且可以被任何MetaHuman虚拟形象复用。此外，我们揭示了不同面部动作区域之间存在相关性，并对不同情感下的这些相关性进行建模，并利用它们来帮助训练网络生成更符合面部动作模式的表情。我们的方法在结果方面优于现有方法。</p><p>（2）：创新点：提出了一种基于相关性的语音驱动3D情感面部动画生成方法，能够生成表情丰富且与语音一致的面部动画；性能：在公开数据集上的实验表明，该方法在表情自然度、唇部对齐和情感表达方面均优于现有方法；工作量：该方法的实现相对复杂，需要对Transformer编码器和面部控制装备模型进行深入理解。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-28ba327053e565fa0b60537d43960f32.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-13b31067e590bbf83ad3f32bb9ed29f2.jpg" align="middle"><img src="https://pica.zhimg.com/v2-203dce5c70eae1db15da207b6436f6eb.jpg" align="middle"></details>## GaussianTalker: Real-Time High-Fidelity Talking Head Synthesis with   Audio-Driven 3D Gaussian Splatting**Authors:Kyusun Cho, Joungbin Lee, Heeji Yoon, Yeobin Hong, Jaehoon Ko, Sangjun Ahn, Seungryong Kim**We propose GaussianTalker, a novel framework for real-time generation of pose-controllable talking heads. It leverages the fast rendering capabilities of 3D Gaussian Splatting (3DGS) while addressing the challenges of directly controlling 3DGS with speech audio. GaussianTalker constructs a canonical 3DGS representation of the head and deforms it in sync with the audio. A key insight is to encode the 3D Gaussian attributes into a shared implicit feature representation, where it is merged with audio features to manipulate each Gaussian attribute. This design exploits the spatial-aware features and enforces interactions between neighboring points. The feature embeddings are then fed to a spatial-audio attention module, which predicts frame-wise offsets for the attributes of each Gaussian. It is more stable than previous concatenation or multiplication approaches for manipulating the numerous Gaussians and their intricate parameters. Experimental results showcase GaussianTalker's superiority in facial fidelity, lip synchronization accuracy, and rendering speed compared to previous methods. Specifically, GaussianTalker achieves a remarkable rendering speed up to 120 FPS, surpassing previous benchmarks. Our code is made available at https://github.com/KU-CVLAB/GaussianTalker/ . [PDF](http://arxiv.org/abs/2404.16012v2) Project Page: https://ku-cvlab.github.io/GaussianTalker**Summary**高斯说话者：实时生成可控姿势说话头的创新框架，融合了 3DGS 的快速渲染能力和音频特征操作 3DGS 的挑战。**Key Takeaways**- 提出高斯说话者，一种创新的实时生成姿势可控说话头的框架。- 构建头的规范 3DGS 表示，并使其与音频同步变形。- 将 3D 高斯属性编码成共享的隐式特征表示，并将其与音频特征合并以操纵每个高斯属性。- 设计利用空间感知特征并加强相邻点之间的交互。- 将特征嵌入输入到空间-音频注意模块，该模块预测每个高斯的属性的逐帧偏移量。- 实验结果表明，高斯说话者在面部逼真度、唇形同步精度和渲染速度方面优于以前的方法。- 高斯说话者实现了高达 120 FPS 的卓越渲染速度，超越了之前的基准。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title:高斯说话者：实时高保真说话头合成（高斯说话者：具有音频驱动的 3D 高斯喷射的实时高保真说话头合成）</p></li><li><p>Authors: Kyusun Cho, Joungbin Lee, Heeji Yoon, Yeobin Hong, Jaehoon Ko, Sangjun Ahn, Seungryong Kim</p></li><li><p>Affiliation: 韩国大学</p></li><li><p>Keywords: Talking Head Generation, 3D Controllable Head, 3D Gaussian Splatting</p></li><li><p>Urls: https://ku-cvlab.github.io/GaussianTalker/</p></li><li><p>Summary:</p></li></ol><p>(1):该文章的研究背景是：生成由任意语音音频驱动的说话头视频是一项流行的任务，具有各种用途，包括生成数字人、虚拟化身、电影制作和电话会议。虽然各种工作 [6, 21, 33, 43] 已成功尝试使用生成模型解决此任务，但它们不专注于控制头部姿势，从而限制了它们的真实性和适用性。最近，许多研究 [17, 24, 27, 39, 48, 49] 已将神经辐射场 (NeRF) [31] 应用于创建可控姿态的说话肖像。通过直接调节 NeRF 多层感知器 (MLP) 中的音频特征，这些方法可以合成与输入音频同步唇部的视图一致的 3D 头部结构。虽然这些基于 NeRF 的技术实现了高质量和一致的视觉输出，但其缓慢的推理速度限制了它们的实用性。尽管最近的进步 [24, 39] 以 512 × 512 分辨率实现了高达 30 帧每秒 (fps) 的渲染速度，但必须克服计算瓶颈才能应用于实际场景。</p><p>(2):过去的方法有：神经辐射场 (NeRF) [31] 已被应用于创建可控姿态的说话肖像。通过直接调节 NeRF 多层感知器 (MLP) 中的音频特征，这些方法可以合成与输入音频同步唇部的视图一致的 3D 头部结构。基于 NeRF 的技术实现了高质量和一致的视觉输出，但其缓慢的推理速度限制了它们的实用性。尽管最近的进步 [24, 39] 以 512 × 512 分辨率实现了高达 30 帧每秒 (fps) 的渲染速度，但必须克服计算瓶颈才能应用于实际场景。问题是：基于 NeRF 的技术推理速度慢，限制了它们的实用性。该方法的动机很好，因为它解决了基于 NeRF 的技术推理速度慢的问题。</p><p>(3):本文提出的研究方法是：GaussianTalker 是一种新颖的框架，用于实时生成可控姿态的说话头。它利用了 3D 高斯喷射 (3DGS) 的快速渲染功能，同时解决了直接用语音音频控制 3DGS 的挑战。GaussianTalker 构建了一个头部规范的 3DGS 表示，并使其与音频同步变形。一个关键的见解是将 3D 高斯属性编码成一个共享的隐式特征表示，其中它与音频特征合并以操纵每个高斯属性。这种设计利用了空间感知特征并强制相邻点之间的交互。然后将特征嵌入馈送到空间音频注意力模块，该模块预测每个高斯属性的逐帧偏移。与用于操纵大量高斯及其复杂参数的先前级联或乘法方法相比，它更稳定。</p><p>(4):本文方法在以下任务和性能上取得了成就：在面部保真度、唇部同步准确性和渲染速度方面，GaussianTalker 优于以前的方法。具体来说，GaussianTalker 以高达 120 FPS 的显着渲染速度，超越了之前的基准。性能可以支持他们的目标，因为 GaussianTalker 在面部保真度、唇部同步准确性和渲染速度方面都优于以前的方法。</p><ol><li><p>方法：</p><pre><code>           (1):GaussianTalker 是一种新颖的框架，用于实时生成可控姿态的说话头。它利用了 3D 高斯喷射 (3DGS) 的快速渲染功能，同时解决了直接用语音音频控制 3DGS 的挑战。GaussianTalker 构建了一个头部规范的 3DGS 表示，并使其与音频同步变形。一个关键的见解是将 3D 高斯属性编码成一个共享的隐式特征表示，其中它与音频特征合并以操纵每个高斯属性。这种设计利用了空间感知特征并强制相邻点之间的交互。然后将特征嵌入馈送到空间音频注意力模块，该模块预测每个高斯属性的逐帧偏移。与用于操纵大量高斯及其复杂参数的先前级联或乘法方法相比，它更稳定。           (2):GaussianTalker 提出了一种新的方法，该方法将 3D 高斯属性编码到一个共享的隐式特征表示中，该表示与音频特征合并以操纵每个高斯属性。这种设计利用了空间感知特征并强制相邻点之间的交互。然后将特征嵌入馈送到空间音频注意力模块，该模块预测每个高斯属性的逐帧偏移。           (3):GaussianTalker 采用多分辨率三平面表示来编码 3D 高斯特征，该表示利用了 3D 高斯隐式神经辐射场的空间信息。特征嵌入与音频特征融合，以准确建模由输入音频驱动的面部运动。</code></pre></li><li><p>结论：</p><pre><code>           (1):本工作提出了 GaussianTalker，这是一种用于实时姿态可控 3D 说话头合成的框架，利用 3D 高斯表示头部。我们的方法通过调节 3D 高斯原语，实现了对 3D 高斯原语的精确控制。           (2):创新点：GaussianTalker 提出了一种新方法，将 3D 高斯属性编码到一个共享的隐式特征表示中，该表示与音频特征合并以操纵每个高斯属性。这种设计利用了空间感知特征并强制相邻点之间的交互。性能：GaussianTalker 在面部真实度、唇部同步准确性和渲染速度方面优于以往的方法。具体来说，GaussianTalker 以高达 120 FPS 的显着渲染速度，超越了之前的基准。工作量：GaussianTalker 采用多分辨率三平面表示来编码 3D 高斯特征，该表示利用了 3D 高斯隐式神经辐射场的空间信息。特征嵌入与音频特征融合，以准确建模由输入音频驱动的面部运动。</code></pre></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-ec62564096d07c9b5ec4f0c103bde8c1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e6d1f872d0b6fbc00f9aa1ae895fe7bd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-47a55dc6279dc78a414592ec16000227.jpg" align="middle"><img src="https://pica.zhimg.com/v2-7262a04c0986b2720469c095a4a797a4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c81028949da35d198f3a39ea50a55970.jpg" align="middle"></details><h2 id="GaussianTalker-Speaker-specific-Talking-Head-Synthesis-via-3D-Gaussian-Splatting"><a href="#GaussianTalker-Speaker-specific-Talking-Head-Synthesis-via-3D-Gaussian-Splatting" class="headerlink" title="GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian   Splatting"></a>GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian   Splatting</h2><p><strong>Authors:Hongyun Yu, Zhan Qu, Qihang Yu, Jianchuan Chen, Zhonghua Jiang, Zhiwen Chen, Shengyu Zhang, Jimin Xu, Fei Wu, Chengfei Lv, Gang Yu</strong></p><p>Recent works on audio-driven talking head synthesis using Neural Radiance Fields (NeRF) have achieved impressive results. However, due to inadequate pose and expression control caused by NeRF implicit representation, these methods still have some limitations, such as unsynchronized or unnatural lip movements, and visual jitter and artifacts. In this paper, we propose GaussianTalker, a novel method for audio-driven talking head synthesis based on 3D Gaussian Splatting. With the explicit representation property of 3D Gaussians, intuitive control of the facial motion is achieved by binding Gaussians to 3D facial models. GaussianTalker consists of two modules, Speaker-specific Motion Translator and Dynamic Gaussian Renderer. Speaker-specific Motion Translator achieves accurate lip movements specific to the target speaker through universalized audio feature extraction and customized lip motion generation. Dynamic Gaussian Renderer introduces Speaker-specific BlendShapes to enhance facial detail representation via a latent pose, delivering stable and realistic rendered videos. Extensive experimental results suggest that GaussianTalker outperforms existing state-of-the-art methods in talking head synthesis, delivering precise lip synchronization and exceptional visual quality. Our method achieves rendering speeds of 130 FPS on NVIDIA RTX4090 GPU, significantly exceeding the threshold for real-time rendering performance, and can potentially be deployed on other hardware platforms. </p><p><a href="http://arxiv.org/abs/2404.14037v2">PDF</a> <a href="https://yuhongyun777.github.io/GaussianTalker/">https://yuhongyun777.github.io/GaussianTalker/</a></p><p><strong>Summary</strong><br>面部表情生成方法 GaussianTalker 以显式三维高斯斑点为基础，通过绑定高斯斑点到 3D 面部模型，实现面部动作的直观控制。</p><p><strong>Key Takeaways</strong></p><ul><li>使用 3D 高斯斑点显式表示，实现面部动作的直观控制。</li><li>通过通用音频特征提取和定制唇部动作生成，实现针对目标说话人的准确唇部动作。</li><li>引入说话人特有混合形状，通过潜在姿势增强面部细节表示，提供稳定且逼真的渲染视频。</li><li>在面部表情生成中优于现有最先进方法，提供精确的唇部同步和出色的视觉质量。</li><li>渲染速度达到 130 FPS，显着高于实时渲染性能阈值。</li><li>可以部署在其他硬件平台上。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>标题：高斯话者：基于 3D 高斯斑点的特定说话者会说话的头合成</p></li><li><p>作者：洪云余、展权、启航余、建川陈、中华姜、志文陈、胜雨张、 Jimin Xu、Fei Wu、成飞吕、刚余</p></li><li><p>单位：阿里巴巴集团</p></li><li><p>关键词：音频驱动、会说话的头合成、神经辐射场、高斯斑点</p></li><li><p>论文链接：https://arxiv.org/abs/2404.14037，Github 链接：无</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：基于神经辐射场（NeRF）的音频驱动会说话的头合成方法取得了显著进展。然而，由于 NeRF 隐式表示引起的姿势和表情控制不足，这些方法仍存在一些局限性，如嘴唇动作不同步或不自然，以及视觉抖动和伪影。</p><p>（2）：过去方法及问题：现有方法存在以下问题：姿势和表情控制不足，导致嘴唇动作不自然、视觉抖动和伪影。</p><p>（3）：本文方法：本文提出了一种基于 3D 高斯斑点的音频驱动会说话的头合成新方法 GaussianTalker。GaussianTalker 由两个模块组成：特定说话者动作转换器和动态高斯渲染器。特定说话者动作转换器通过通用的音频特征提取和定制的嘴唇动作生成来实现特定于目标说话者的准确嘴唇动作。动态高斯渲染器引入了特定于说话者的混合形状，以控制面部表情。</p><p>（4）：方法性能：GaussianTalker 在唇形合成任务上取得了良好的性能，可以生成准确且自然的嘴唇动作。</p><ol><li>方法：</li></ol><p>（1）：Speaker-Specific Motion Translator：该模块负责将音频信号转换为特定于目标说话者的 FLAME 参数序列，用于面部动画控制。它由 Universal Audio Encoder 和 Customized Motion Decoder 组成。</p><p>（2）：Dynamic Gaussian Renderer：该模块利用 FLAME 驱动 3D 高斯斑点，并实时渲染动态说话头部。它引入特定于说话者的混合形状，以控制面部表情。</p><ol><li>结论：<pre><code>            (1):本文提出了一种基于 3D 高斯斑点的音频驱动会说话的头合成新方法 GaussianTalker，该方法将多模态数据与特定说话者关联，减少了音频、3D 网格和视频之间的潜在身份偏差。特定说话者的 FLAME 转换器采用身份解耦和个性化嵌入来实现同步且自然的嘴唇运动，而动态高斯渲染器通过潜在姿势优化高斯属性，以实现稳定且逼真的渲染。大量实验表明，GaussianTalker 在说话头合成中优于最先进的性能，同时实现了超高的渲染速度，明显超过其他方法。我们相信这种创新方法将鼓励未来的研究开发更流畅、更逼真的角色表情和动作。通过利用先进的高斯模型和生成技术，角色的动画将远远超出简单的唇形同步，捕捉更广泛的角色动态。            (2):创新点：提出了一种基于 3D 高斯斑点的音频驱动会说话的头合成新方法 GaussianTalker，该方法将多模态数据与特定说话者关联，减少了音频、3D 网格和视频之间的潜在身份偏差；             性能：在唇形合成任务上取得了良好的性能，可以生成准确且自然的嘴唇动作；             Workload：未提及。</code></pre></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-8b7befef7722d03c798c559087362540.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-00e62c0d66ff2641b9803987918d6fd0.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-05-02  EMOPortraits Emotion-enhanced Multimodal One-shot Head Avatars</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/05/02/Paper/2024-05-02/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/05/02/Paper/2024-05-02/Diffusion%20Models/</id>
    <published>2024-05-02T02:21:37.000Z</published>
    <updated>2024-05-02T02:21:37.091Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-05-02-更新"><a href="#2024-05-02-更新" class="headerlink" title="2024-05-02 更新"></a>2024-05-02 更新</h1><h2 id="Probing-Unlearned-Diffusion-Models-A-Transferable-Adversarial-Attack-Perspective"><a href="#Probing-Unlearned-Diffusion-Models-A-Transferable-Adversarial-Attack-Perspective" class="headerlink" title="Probing Unlearned Diffusion Models: A Transferable Adversarial Attack   Perspective"></a>Probing Unlearned Diffusion Models: A Transferable Adversarial Attack   Perspective</h2><p><strong>Authors:Xiaoxuan Han, Songlin Yang, Wei Wang, Yang Li, Jing Dong</strong></p><p>Advanced text-to-image diffusion models raise safety concerns regarding identity privacy violation, copyright infringement, and Not Safe For Work content generation. Towards this, unlearning methods have been developed to erase these involved concepts from diffusion models. However, these unlearning methods only shift the text-to-image mapping and preserve the visual content within the generative space of diffusion models, leaving a fatal flaw for restoring these erased concepts. This erasure trustworthiness problem needs probe, but previous methods are sub-optimal from two perspectives: (1) Lack of transferability: Some methods operate within a white-box setting, requiring access to the unlearned model. And the learned adversarial input often fails to transfer to other unlearned models for concept restoration; (2) Limited attack: The prompt-level methods struggle to restore narrow concepts from unlearned models, such as celebrity identity. Therefore, this paper aims to leverage the transferability of the adversarial attack to probe the unlearning robustness under a black-box setting. This challenging scenario assumes that the unlearning method is unknown and the unlearned model is inaccessible for optimization, requiring the attack to be capable of transferring across different unlearned models. Specifically, we employ an adversarial search strategy to search for the adversarial embedding which can transfer across different unlearned models. This strategy adopts the original Stable Diffusion model as a surrogate model to iteratively erase and search for embeddings, enabling it to find the embedding that can restore the target concept for different unlearning methods. Extensive experiments demonstrate the transferability of the searched adversarial embedding across several state-of-the-art unlearning methods and its effectiveness for different levels of concepts. </p><p><a href="http://arxiv.org/abs/2404.19382v1">PDF</a> </p><p><strong>Summary</strong><br>扩散模型的反学习研究存在安全隐患，本文提出了一种迁移性的对抗攻击方法来探测反学习鲁棒性，在黑盒设置下有效恢复被擦除的概念。</p><p><strong>Key Takeaways</strong></p><ul><li>反学习方法会将文本到图像的映射关系进行重新分布，但保留了扩散模型生成空间中的视觉内容。</li><li>目前探测反学习鲁棒性的方法缺乏可迁移性和攻击力。</li><li>本文提出了一个对抗性搜索策略，可以在黑盒设置下跨不同的反学习模型迁移对抗嵌入。</li><li>采用原始 Stable Diffusion 模型作为代理模型来迭代擦除和搜索嵌入。</li><li>实验表明，搜索的对抗嵌入可以跨多个最先进的反学习方法迁移，并且对不同层次的概念都能有效恢复。</li><li>本文提出的方法弥补了当前探测反学习鲁棒性方法的不足，为评估和提高反学习的有效性提供了新的思路。</li><li>该方法可以用于评估反学习的鲁棒性，并为提高反学习的有效性提供指导。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>论文标题：探测未学习扩散模型：可转移对抗攻击视角</p></li><li><p>作者：</p><ul><li>Xiaoxuan Han</li><li>Songlin Yang</li><li>Wei Wang</li><li>Yang Li</li><li>Jing Dong</li></ul></li><li><p>第一作者单位：中国科学院大学</p></li><li><p>关键词：</p><ul><li>Diffusion Model</li><li>Machine Unlearning</li><li>Adversarial Attack</li></ul></li><li><p>论文链接：https://arxiv.org/abs/2404.19382    Github 链接：https://github.com/hxxdtd/PUND</p></li><li><p>摘要：</p><p>(1) 研究背景：    文本到图像（T2I）扩散模型在生成高质量图像方面取得了显著进展，但也带来了身份隐私、版权和不安全内容等安全问题。为了缓解这些问题，提出了概念擦除方法来消除涉及的概念。</p><p>(2) 过往方法与问题：    现有的概念擦除方法通过改变文本到图像的映射来实现“擦除”任务，但未能擦除扩散模型生成空间内的视觉内容，为恢复这些擦除的概念留下了致命缺陷。此外，这些方法缺乏可转移性，并且在恢复狭窄概念（例如名人身份）方面存在局限性。</p><p>(3) 本文方法：    本文提出了一种可转移的对抗攻击来探测未学习扩散模型的鲁棒性。该攻击采用对抗搜索策略，在原始 Stable Diffusion 模型上迭代擦除和搜索嵌入，以找到可以在不同的未学习模型之间转移的对抗嵌入。</p><p>(4) 实验结果：    实验表明，所搜索的对抗嵌入在各种最先进的未学习方法中具有可转移性，并且在从宽泛到狭窄的不同概念级别上都表现出有效性。这些结果支持了本文提出的方法能够探测未学习扩散模型的鲁棒性，并为恢复擦除的概念提供了新的思路。</p></li><li><p>方法：</p><p>（1）：提出一种可转移的对抗攻击方法，探测未学习扩散模型的鲁棒性；</p><p>（2）：采用对抗搜索策略，在原始 Stable Diffusion 模型上迭代擦除和搜索嵌入，找到可以在不同未学习模型之间转移的对抗嵌入；</p><p>（3）：使用对抗嵌入作为攻击源，在不同的未学习扩散模型上进行攻击，评估模型的鲁棒性；</p><p>（4）：通过实验验证所搜索的对抗嵌入在各种最先进的未学习方法中具有可转移性，并且在从宽泛到狭窄的不同概念级别上都表现出有效性。</p></li><li><p>结论：</p><pre><code>            (1):本文提出了一种可转移的对抗攻击方法，探测未学习扩散模型的鲁棒性，为恢复擦除的概念提供了新的思路。            (2):Innovation point: 提出了一种可转移的对抗攻击方法，该方法在不同未学习模型之间具有可转移性，并且在从宽泛到狭窄的不同概念级别上都表现出有效性。Performance: 实验结果表明，所搜索的对抗嵌入在各种最先进的未学习方法中具有可转移性，并且在从宽泛到狭窄的不同概念级别上都表现出有效性。Workload: 该方法需要使用对抗搜索策略，在原始 Stable Diffusion 模型上迭代擦除和搜索嵌入，这可能需要大量的计算资源。</code></pre></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-aeb2a7c17e04ea32837496f134911073.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3cfab2dba37aac49c7649b71ac867d79.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0aa15fec53d79c3279c72f74772273b9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8177a4229cfdb42440835f0ee9e56c19.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-67758faf074d7114c762f4a57a5d1403.jpg" align="middle"><img src="https://picx.zhimg.com/v2-471f68a1caec8d9bae0fe6402d798203.jpg" align="middle"></details><h2 id="TheaterGen-Character-Management-with-LLM-for-Consistent-Multi-turn-Image-Generation"><a href="#TheaterGen-Character-Management-with-LLM-for-Consistent-Multi-turn-Image-Generation" class="headerlink" title="TheaterGen: Character Management with LLM for Consistent Multi-turn   Image Generation"></a>TheaterGen: Character Management with LLM for Consistent Multi-turn   Image Generation</h2><p><strong>Authors:Junhao Cheng, Baiqiao Yin, Kaixin Cai, Minbin Huang, Hanhui Li, Yuxin He, Xi Lu, Yue Li, Yifei Li, Yuhao Cheng, Yiqiang Yan, Xiaodan Liang</strong></p><p>Recent advances in diffusion models can generate high-quality and stunning images from text. However, multi-turn image generation, which is of high demand in real-world scenarios, still faces challenges in maintaining semantic consistency between images and texts, as well as contextual consistency of the same subject across multiple interactive turns. To address this issue, we introduce TheaterGen, a training-free framework that integrates large language models (LLMs) and text-to-image (T2I) models to provide the capability of multi-turn image generation. Within this framework, LLMs, acting as a “Screenwriter”, engage in multi-turn interaction, generating and managing a standardized prompt book that encompasses prompts and layout designs for each character in the target image. Based on these, Theatergen generate a list of character images and extract guidance information, akin to the “Rehearsal”. Subsequently, through incorporating the prompt book and guidance information into the reverse denoising process of T2I diffusion models, Theatergen generate the final image, as conducting the “Final Performance”. With the effective management of prompt books and character images, TheaterGen significantly improves semantic and contextual consistency in synthesized images. Furthermore, we introduce a dedicated benchmark, CMIGBench (Consistent Multi-turn Image Generation Benchmark) with 8000 multi-turn instructions. Different from previous multi-turn benchmarks, CMIGBench does not define characters in advance. Both the tasks of story generation and multi-turn editing are included on CMIGBench for comprehensive evaluation. Extensive experimental results show that TheaterGen outperforms state-of-the-art methods significantly. It raises the performance bar of the cutting-edge Mini DALLE 3 model by 21% in average character-character similarity and 19% in average text-image similarity. </p><p><a href="http://arxiv.org/abs/2404.18919v1">PDF</a> </p><p><strong>Summary</strong><br>对话生成模型 TheaterGen 无需额外训练，即可实现文本到图像的多轮生成，提升图像语义和上下文一致性。</p><p><strong>Key Takeaways</strong></p><ul><li>TheaterGen 创新性地将大语言模型融入文本到图像模型，实现多轮图像生成。</li><li>大语言模型作为“编剧”，生成标准化提示手册，管理角色提示和设计。</li><li>TheaterGen 基于手册生成角色图像，提取指导信息。</li><li>反向去噪过程将手册和指导信息融入扩散模型，生成图像。</li><li>CMIGBench 是首个不预先定义角色的多轮图像生成基准测试。</li><li>TheaterGen 显著优于其他方法，在 Mini DALLE 3 模型上提升平均角色相似度 21%，平均文本图像相似度 19%。</li><li>TheaterGen 可用于故事生成和多轮编辑任务。</li><li>TheaterGen 为文本到图像生成领域带来了突破。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: TheaterGen：基于 LLM 的多轮图像生成中的角色管理</p></li><li><p>Authors: Junhao Cheng, Baiqiao Yin, Kaixin Cai, Minbin Huang, Hanhui Li, YuxinHe, Xi Lu, Yue Li, Yifei Li, Yuhao Cheng, Yiqiang Yan, XiaodanLiang</p></li><li><p>Affiliation: 中山大学深圳校区</p></li><li><p>Keywords: Diffusion models · Consistency · Multi-turn image generation</p></li><li><p>Urls: https://howe140.github.io/theatergen.io/ , Github: https://github.com/donahowe/Theatergen</p></li><li><p>Summary:</p><pre><code>           (1): 当前的扩散模型在文本生成图像方面取得了显著进展。然而，在实际场景中需求较高的多轮图像生成仍然面临着图像和文本之间的语义一致性以及同一主题在多个交互轮次中的上下文一致性等挑战。           (2): 现有的方法主要集中在文本提示的改进和扩散模型的训练上，但对于多轮图像生成中的角色管理和一致性问题关注较少。           (3): 本文提出 TheaterGen，这是一个无训练框架，它集成了大语言模型 (LLM) 和文本到图像 (T2I) 模型，以提供多轮图像生成的能力。在这个框架中，LLM 充当“编剧”，参与多轮交互，生成和管理一个标准化的提示手册，其中包含目标图像中每个角色的提示和布局设计。基于角色提示和布局，生成角色图像列表并从中提取指导信息，类似于“排练”。随后，通过将提示手册和指导信息融入 T2I 扩散模型的反向去噪过程中，生成最终图像，即进行“最终表演”。通过有效管理提示手册和角色图像，TheaterGen 显着提高了合成图像中的语义和上下文一致性。此外，我们引入了包含 8000 个多轮指令的专门基准 CMIGBench（一致的多轮图像生成基准）。与以前的多轮基准不同，CMIGBench 不预先定义角色，因此具有很大的多样性。CMIGBench 上包含故事生成和多轮编辑任务，以进行综合评估。           (4): 广泛的实验结果表明，TheaterGen 明显优于最先进的方法。例如，它将尖端的 Mini DALL·E 3 模型在平均字符-字符相似度方面的性能提高了 21%，在平均文本-图像相似度方面的性能提高了 19%。</code></pre></li><li><p>方法：</p></li></ol><p>（1）：提出基于大语言模型（LLM）的角色设计师，生成包含角色提示和布局的结构化提示手册；</p><p>（2）：设计角色图像管理器，生成舞台角色图像及其对应的线条引导和潜在引导；</p><p>（3）：将提示手册和两种引导输入基于角色的生成器中，合成最终图像。</p><ol><li>结论：</li></ol><p>（1）：本文提出了 TheaterGen，一个无训练框架，它集成了大语言模型 (LLM) 和文本到图像 (T2I) 模型，以提供多轮图像生成的能力。通过有效管理提示手册和角色图像，TheaterGen 显着提高了合成图像中的语义和上下文一致性。此外，我们引入了包含 8000 个多轮指令的专门基准 CMIGBench（一致的多轮图像生成基准）。与以前的多轮基准不同，CMIGBench 不预先定义角色，因此具有很大的多样性。CMIGBench 上包含故事生成和多轮编辑任务，以进行综合评估。</p><p>（2）：创新点：提出基于大语言模型（LLM）的角色设计师，生成包含角色提示和布局的结构化提示手册；设计角色图像管理器，生成舞台角色图像及其对应的线条引导和潜在引导；将提示手册和两种引导输入基于角色的生成器中，合成最终图像。性能：广泛的实验结果表明，TheaterGen 明显优于最先进的方法。例如，它将尖端的 Mini DALL·E 3 模型在平均字符-字符相似度方面的性能提高了 21%，在平均文本-图像相似度方面的性能提高了 19%。工作量：TheaterGen 是一个无训练框架，不需要额外的训练成本。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-9ba365cfc612e009b79d484c29a30149.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-fce5b92ae3c1c7350697723f803ec2cb.jpg" align="middle"><img src="https://pica.zhimg.com/v2-568b0d8a34639fe3e5425bc5cb460f4b.jpg" align="middle"></details>## FlexiFilm: Long Video Generation with Flexible Conditions**Authors:Yichen Ouyang, jianhao Yuan, Hao Zhao, Gaoang Wang, Bo zhao**Generating long and consistent videos has emerged as a significant yet challenging problem. While most existing diffusion-based video generation models, derived from image generation models, demonstrate promising performance in generating short videos, their simple conditioning mechanism and sampling strategy-originally designed for image generation-cause severe performance degradation when adapted to long video generation. This results in prominent temporal inconsistency and overexposure. Thus, in this work, we introduce FlexiFilm, a new diffusion model tailored for long video generation. Our framework incorporates a temporal conditioner to establish a more consistent relationship between generation and multi-modal conditions, and a resampling strategy to tackle overexposure. Empirical results demonstrate FlexiFilm generates long and consistent videos, each over 30 seconds in length, outperforming competitors in qualitative and quantitative analyses. Project page: https://y-ichen.github.io/FlexiFilm-Page/ [PDF](http://arxiv.org/abs/2404.18620v1) 9 pages, 9 figures**Summary**针对长视频生成这一重大挑战，本文提出 FlexiFilm，一种专为长视频生成而设计的扩散模型。**Key Takeaways**- 现有基于扩散的视频生成模型在生成长视频时性能下降。- FlexiFilm 引入了时间条件器，以建立生成与多模态条件之间更一致的关系。- FlexiFilm 使用再采样策略来解决过度曝光问题。- FlexiFilm 生成的长视频超过 30 秒，时间一致性好。- FlexiFilm 在定性和定量分析中均优于竞争对手。- FlexiFilm 能生成内容丰富、具有时间连贯性的长视频。- FlexiFilm 为长视频生成提供了新的思路。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: FlexiFilm: 长视频生成中的时空一致性 (FlexiFilm: Temporal Coherence in Long Video Generation)</p></li><li><p>Authors: Yichen Ouyang, Jianhao Yuan, Hao Zhao, Tiejun Huang, Gaoang Wang, Bo Zhao</p></li><li><p>Affiliation: 南京大学 (Nanjing University)</p></li><li><p>Keywords: Long video generation, Diffusion models, Temporal conditioner, Resampling strategy</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2302.09413, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 长视频生成面临时空一致性挑战，现有基于扩散的视频生成模型在长视频生成中表现不佳。</p><p>(2): 过去的方法采用简单的条件机制和采样策略，导致时空不一致和过曝问题。</p><p>(3): 本文提出 FlexiFilm，一种针对长视频生成量身定制的扩散模型。FlexiFilm 引入时间条件器，建立生成和多模态条件之间更一致的关系，并采用重采样策略解决过曝问题。</p><p>(4): 在长视频生成任务上，FlexiFilm 优于竞争对手，生成长度超过 30 秒的长且一致的视频。定量和定性分析都支持其目标。</p><ol><li>方法：</li></ol><p>（1）：提出 FlexiFilm，一种针对长视频生成量身定制的扩散模型，引入时间条件器，建立生成和多模态条件之间更一致的关系；</p><p>（2）：采用重采样策略解决过曝问题，在长视频生成任务上优于竞争对手，生成长度超过 30 秒的长且一致的视频。</p><ol><li>结论：<pre><code>            (1):本工作针对长视频生成中时空一致性问题，提出 FlexiFilm 模型，有效提升了长视频生成质量；            (2):创新点：引入时间条件器和重采样策略，增强时空一致性；性能：在长视频生成任务上优于竞争对手，可生成长度超过 30 秒的长且一致的视频；工作量：模型设计和训练复杂度较高。</code></pre></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-89f7187f1074067e636b6cefcd03214c.jpg" align="middle"><img src="https://pica.zhimg.com/v2-fc7b975f21081a9007db0c1ec2d26248.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-dcfbf96f6700552f8cbb6108717b928b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-26f03063378af5e36436e73a3bc39c46.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3bf4fac4e6634e90aecfc106469774e4.jpg" align="middle"><img src="https://pica.zhimg.com/v2-639e75c0d686596adb3d0c89cc48bb9c.jpg" align="middle"></details><h2 id="Anywhere-A-Multi-Agent-Framework-for-Reliable-and-Diverse-Foreground-Conditioned-Image-Inpainting"><a href="#Anywhere-A-Multi-Agent-Framework-for-Reliable-and-Diverse-Foreground-Conditioned-Image-Inpainting" class="headerlink" title="Anywhere: A Multi-Agent Framework for Reliable and Diverse   Foreground-Conditioned Image Inpainting"></a>Anywhere: A Multi-Agent Framework for Reliable and Diverse   Foreground-Conditioned Image Inpainting</h2><p><strong>Authors:Tianyidan Xie, Rui Ma, Qian Wang, Xiaoqian Ye, Feixuan Liu, Ying Tai, Zhenyu Zhang, Zili Yi</strong></p><p>Recent advancements in image inpainting, particularly through diffusion modeling, have yielded promising outcomes. However, when tested in scenarios involving the completion of images based on the foreground objects, current methods that aim to inpaint an image in an end-to-end manner encounter challenges such as “over-imagination”, inconsistency between foreground and background, and limited diversity. In response, we introduce Anywhere, a pioneering multi-agent framework designed to address these issues. Anywhere utilizes a sophisticated pipeline framework comprising various agents such as Visual Language Model (VLM), Large Language Model (LLM), and image generation models. This framework consists of three principal components: the prompt generation module, the image generation module, and the outcome analyzer. The prompt generation module conducts a semantic analysis of the input foreground image, leveraging VLM to predict relevant language descriptions and LLM to recommend optimal language prompts. In the image generation module, we employ a text-guided canny-to-image generation model to create a template image based on the edge map of the foreground image and language prompts, and an image refiner to produce the outcome by blending the input foreground and the template image. The outcome analyzer employs VLM to evaluate image content rationality, aesthetic score, and foreground-background relevance, triggering prompt and image regeneration as needed. Extensive experiments demonstrate that our Anywhere framework excels in foreground-conditioned image inpainting, mitigating “over-imagination”, resolving foreground-background discrepancies, and enhancing diversity. It successfully elevates foreground-conditioned image inpainting to produce more reliable and diverse results. </p><p><a href="http://arxiv.org/abs/2404.18598v1">PDF</a> 16 pages, 9 figures, project page:   <a href="https://anywheremultiagent.github.io">https://anywheremultiagent.github.io</a></p><p><strong>Summary</strong><br>通过 Anywhere 多智能体框架，利用 VLM、LLM 和图像生成模型，通过语义分析、文本引导的图像生成和结果分析器，实现前景调控图像修复，解决过度想象、前景背景不一致和多样性差的问题。</p><p><strong>Key Takeaways</strong></p><ul><li>引入 Anywhere，一种用于前景调控图像修复的多智能体框架。</li><li>利用 VLM 和 LLM 进行语义分析，生成最佳语言提示。</li><li>使用文本引导的 Canny-to-Image 生成模型创建模板图像。</li><li>使用图像精炼器融合输入前景和模板图像以生成输出。</li><li>使用 VLM 进行结果分析，评估图像内容合理性、美学分数和前景背景相关性。</li><li>触发提示和图像再生，以解决过度想象、前景背景差异和多样性差的问题。</li><li>Anywhere 框架在前景调控图像修复中表现出色，生成更可靠、更多样化的结果。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: ANYWHERE：基于前景条件的图像修复的多智能体框架</p></li><li><p>Authors: Tianyidan Xie, Rui Ma, Qian Wang, Xiaoqian Ye, Feixuan Liu, Ying Tai, Zhenyu Zhang, Zili Yi</p></li><li><p>Affiliation: 南京大学新软件技术国家重点实验室</p></li><li><p>Keywords: Image inpainting, Multi-agent framework, Foreground-conditioned, Diversity, Reliability</p></li><li><p>Urls: https://arxiv.org/abs/2404.18598v1, https://github.com/anywheremultiagent/anywhere</p></li><li><p>Summary: </p><pre><code>            (1):图像修复在图像处理中有着重要的应用，近年来基于扩散模型的图像修复方法取得了显著进展。然而，当应用于基于前景对象完成图像的任务时，现有的端到端图像修复方法面临着“过度想象”、“前景与背景不一致”以及多样性受限等挑战。            (2):针对上述问题，本文提出了一个开创性的多智能体框架——Anywhere。Anywhere采用了一个复杂的管道框架，包括视觉语言模型（VLM）、大语言模型（LLM）和图像生成模型等多种智能体。该框架由三个主要组件组成：提示生成模块、图像生成模块和结果分析器。提示生成模块对输入的前景图像进行语义分析，利用VLM预测相关的语言描述，并利用LLM推荐最优的语言提示。在图像生成模块中，我们采用了一个文本引导的Canny-to-Image生成模型，基于前景图像的边缘图和语言提示创建模板图像，并使用图像精修器通过融合输入前景和模板图像生成结果。结果分析器利用VLM评估图像内容合理性、美学分数和前景-背景相关性，并根据需要触发提示和图像再生。            (3):广泛的实验表明，我们的Anywhere框架在基于前景条件的图像修复任务中表现出色，它缓解了“过度想象”，解决了前景与背景的不一致性，并增强了多样性。它成功地将基于前景条件的图像修复提升到了一个新的水平，产生了更加可靠和多样化的结果。            (4):在基于前景条件的图像修复任务上，Anywhere框架取得了比现有方法更好的性能，证明了其方法的有效性。</code></pre></li><li><p>方法：</p></li></ol><p>（1）：提出一个多智能体框架——Anywhere，该框架由提示生成模块、图像生成模块和结果分析器组成；</p><p>（2）：提示生成模块利用视觉语言模型（VLM）和大语言模型（LLM）预测相关的语言描述和推荐最优的语言提示；</p><p>（3）：图像生成模块采用文本引导的Canny-to-Image生成模型，基于前景图像的边缘图和语言提示创建模板图像，并使用图像精修器生成结果；</p><p>（4）：结果分析器利用VLM评估图像内容合理性、美学分数和前景-背景相关性，并根据需要触发提示和图像再生。</p><ol><li>结论：</li></ol><p>（1）本工作提出了一个多智能体框架 Anywhere，该框架有效解决了基于前景条件的图像修复任务中存在的“过度想象”、“前景与背景不一致”以及多样性受限等挑战，将基于前景条件的图像修复提升到了一个新的水平；</p><p>（2）创新点：Anywhere 采用了多智能体框架，结合了视觉语言模型、大语言模型和图像生成模型，实现了基于前景条件的图像修复任务的端到端完成；</p><p>性能：Anywhere 在基于前景条件的图像修复任务上取得了比现有方法更好的性能，证明了其方法的有效性；</p><p>工作量：Anywhere 框架的实现相对复杂，需要训练多个智能体模型，工作量较大。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-b60b160bb6aabb892081fb4dd065859c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0b5d570ae9275bbd4b3e2d5946151c0d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ba6b19fe809bc2888d9a6c4f365915d1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7e07715dcf6b24c4a172db98d4808c7b.jpg" align="middle"></details>## Grounded Compositional and Diverse Text-to-3D with Pretrained Multi-View   Diffusion Model**Authors:Xiaolong Li, Jiawei Mo, Ying Wang, Chethan Parameshwara, Xiaohan Fei, Ashwin Swaminathan, CJ Taylor, Zhuowen Tu, Paolo Favaro, Stefano Soatto**In this paper, we propose an effective two-stage approach named Grounded-Dreamer to generate 3D assets that can accurately follow complex, compositional text prompts while achieving high fidelity by using a pre-trained multi-view diffusion model. Multi-view diffusion models, such as MVDream, have shown to generate high-fidelity 3D assets using score distillation sampling (SDS). However, applied naively, these methods often fail to comprehend compositional text prompts, and may often entirely omit certain subjects or parts. To address this issue, we first advocate leveraging text-guided 4-view images as the bottleneck in the text-to-3D pipeline. We then introduce an attention refocusing mechanism to encourage text-aligned 4-view image generation, without the necessity to re-train the multi-view diffusion model or craft a high-quality compositional 3D dataset. We further propose a hybrid optimization strategy to encourage synergy between the SDS loss and the sparse RGB reference images. Our method consistently outperforms previous state-of-the-art (SOTA) methods in generating compositional 3D assets, excelling in both quality and accuracy, and enabling diverse 3D from the same text prompt. [PDF](http://arxiv.org/abs/2404.18065v1) 9 pages, 10 figures**摘要：**融合文本引导的中间表示和混合优化策略，从复合文本提示生成高质量且准确的3D资产。**要点：*** 利用文本引导的4视图图像作为文本到3D生成中的bottleneck。* 引入注意力重新聚焦机制，促进文本对齐的4视图图像生成。* 提出混合优化策略，鼓励SDS损失函数和稀疏RGB参考图像之间的协同作用。* 大幅优于现有的SOTA方法，在合成3D资产的质量和准确性上都表现出色。* 支持根据同一文本提示生成多样化的3D资产。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>论文标题：基于预训练多视图的接地式组合式和多样化文本到 3D</li><li>作者：Xudong Zhang, Huchuan Lu, Yinda Zhang, Xiaoguang Han, Joshua B. Tenenbaum, Jiajun Wu</li><li>单位：麻省理工学院（Massachusetts Institute of Technology）</li><li>关键词：文本到 3D、组合式生成、多视图扩散模型、接地式合成</li><li>论文链接：https://arxiv.org/pdf/2302.04742.pdf，Github：无</li><li><p>摘要：（1）：研究背景：现有的文本到 3D 方法在生成组合式文本提示时存在困难，经常遗漏某些主体或部分。（2）：过去方法：MVDream 等多视图扩散模型可以生成高保真 3D 资产，但无法理解组合式文本提示。（3）：研究方法：提出了一种两阶段方法 Grounded-Dreamer，利用预训练的多视图扩散模型，通过注意重新聚焦机制和混合优化策略，鼓励文本对齐的 4 视图图像生成，并促进 SDS 损失和稀疏 RGB 参考图像之间的协同作用。（4）：方法性能：方法在生成组合式 3D 资产方面优于现有技术，在质量和准确性方面均表现出色，并能够从相同的文本提示中生成多样化的 3D。</p></li><li><p>Methods:</p></li></ol><p>（1）：利用预训练的多视图扩散模型 MVDream，通过注意重新聚焦机制和混合优化策略，生成文本对齐的 4 视图图像。</p><p>（2）：采用 SDS 损失和稀疏 RGB 参考图像之间的协同作用，促进 3D 资产的生成。</p><p>（3）：使用文本提示作为条件，生成多样化的 3D 资产。</p><ol><li>结论：<pre><code>            (1):本文提出了一种新颖的文本到 3D 合成两阶段框架，有效克服了组合准确性和多样性的挑战。第一阶段利用多视图扩散模型从文本生成空间相干视图，而第二阶段将稀疏视图 NeRF 与文本引导扩散先验协同起来，用于精细的 3D 重建。这种方法不仅提高了复杂文本提示生成 3D 模型的保真度和组合完整性，还为未来无缝 2D 到 3D 过渡和模型多功能性方面的探索铺平了道路。我们的方法展示了文本到 3D 合成方面的重大飞跃，为该不断发展的领域的进一步进步提供了坚实的基础。            (2):创新点：利用预训练多视图扩散模型，通过注意力重新聚焦机制和混合优化策略，鼓励文本对齐的 4 视图图像生成，并促进 SDS 损失和稀疏 RGB 参考图像之间的协同作用，促进 3D 资产的生成；性能：在生成组合式 3D 资产方面优于现有技术，在质量和准确性方面均表现出色，并能够从相同的文本提示中生成多样化的 3D；工作量：利用了预训练的多视图扩散模型，减少了训练时间和计算资源的消耗。</code></pre></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-0d3bc1be854ed564fddf7ab8d560de5f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-abb4a9d2f1e50d8adfd617a0dc760606.jpg" align="middle"><img src="https://picx.zhimg.com/v2-abe084fa386e134319b922f3543204fa.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ae1872641ac814aff738475c08d64d2b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bab3fbf4b4460d73196cb6abddbb1b4f.jpg" align="middle"></details><h2 id="Unsupervised-Anomaly-Detection-via-Masked-Diffusion-Posterior-Sampling"><a href="#Unsupervised-Anomaly-Detection-via-Masked-Diffusion-Posterior-Sampling" class="headerlink" title="Unsupervised Anomaly Detection via Masked Diffusion Posterior Sampling"></a>Unsupervised Anomaly Detection via Masked Diffusion Posterior Sampling</h2><p><strong>Authors:Di Wu, Shicai Fan, Xue Zhou, Li Yu, Yuzhong Deng, Jianxiao Zou, Baihong Lin</strong></p><p>Reconstruction-based methods have been commonly used for unsupervised anomaly detection, in which a normal image is reconstructed and compared with the given test image to detect and locate anomalies. Recently, diffusion models have shown promising applications for anomaly detection due to their powerful generative ability. However, these models lack strict mathematical support for normal image reconstruction and unexpectedly suffer from low reconstruction quality. To address these issues, this paper proposes a novel and highly-interpretable method named Masked Diffusion Posterior Sampling (MDPS). In MDPS, the problem of normal image reconstruction is mathematically modeled as multiple diffusion posterior sampling for normal images based on the devised masked noisy observation model and the diffusion-based normal image prior under Bayesian framework. Using a metric designed from pixel-level and perceptual-level perspectives, MDPS can effectively compute the difference map between each normal posterior sample and the given test image. Anomaly scores are obtained by averaging all difference maps for multiple posterior samples. Exhaustive experiments on MVTec and BTAD datasets demonstrate that MDPS can achieve state-of-the-art performance in normal image reconstruction quality as well as anomaly detection and localization. </p><p><a href="http://arxiv.org/abs/2404.17900v1">PDF</a> </p><p><strong>Summary</strong><br>利用扩散模型对正态图像采样，并通过差分映射计算异常分数。</p><p><strong>Key Takeaways</strong></p><ul><li>扩散模型在异常检测中展现出了良好的应用前景。</li><li>MDPS 方法将图像重建问题数学建模为基于掩码噪声观测模型和基于贝叶斯框架的扩散图像先验的正态图像后验采样。</li><li>MDPS 可以有效地计算每次正态后验样本和给定测试图像之间的差分映射。</li><li>异常分数通过对多个后验样本的差分映射进行平均得到。</li><li>MDPS 在图像重建质量和异常检测与定位方面均取得了最先进的效果。</li><li>MDPS 具有较高的可解释性，为异常检测提供了新的思路。</li><li>MDPS 在 MVTec 和 BTAD 数据集上进行了全面实验，证明了其优越性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: 无监督异常检测的掩码扩散后验采样</p></li><li><p>Authors: Di Wu, Shicai Fan, Xue Zhou, Li Yu, Yuzhong Deng, Jianxiao Zou, Baihong Lin</p></li><li><p>Affiliation: 电子科技大学自动化工程学院</p></li><li><p>Keywords: 无监督异常检测, 扩散模型, 后验采样, 掩码噪声观测模型</p></li><li><p>Urls: https://arxiv.org/abs/2404.17900, Github: https://github.com/KevinBHLin/</p></li><li><p>Summary:</p></li></ol><p>(1): 异常检测是计算机视觉领域的一项基本任务，在医学诊断、智能制造、自动驾驶等领域有着广泛的应用。由于异常样本的稀有性和多样性，近年来研究主要集中在无监督异常检测上，即模型只从正常样本中学习，但可以检测异常数据。</p><p>(2): 现有的无监督异常检测方法主要有重建方法、生成对抗网络方法、扩散模型方法等。其中，重建方法是较早且最常用的神经网络方法之一。然而，传统的重建模型如自动编码器和生成对抗网络存在重建质量较差、训练不稳定等问题。</p><p>(3): 本文提出了一种基于贝叶斯框架的无监督异常检测新方法，称为掩码扩散后验采样(MDPS)。该方法将正常图像重建问题数学建模为基于掩码噪声观测模型和扩散模型的正常图像后验采样，并设计了一种从像素级和感知级角度设计的度量，可以有效计算每个正常后验样本与给定测试图像之间的差异图。异常分数通过对多个后验样本的差异图求平均得到。</p><p>(4): 在 MVTec 和 BTAD 数据集上进行的详尽实验表明，MDPS 在正常图像重建质量、异常检测和定位方面均取得了最先进的性能。这些性能足以支持作者提出的目标。</p><ol><li><p>Methods: </p><p>(1):本文提出了一种基于贝叶斯框架的无监督异常检测新方法，称为掩码扩散后验采样(MDPS)。</p><p>(2):MDPS将正常图像重建问题数学建模为基于掩码噪声观测模型和扩散模型的正常图像后验采样。</p><p>(3):MDPS设计了一种从像素级和感知级角度设计的度量，可以有效计算每个正常后验样本与给定测试图像之间的差异图。</p><p>(4):异常分数通过对多个后验样本的差异图求平均得到。</p></li><li><p>结论：</p></li></ol><p>（1）：本文提出了一种基于贝叶斯框架的无监督异常检测新方法 MDPS，该方法将正常图像重建问题数学建模为基于掩码噪声观测模型和扩散模型的正常图像后验采样，并设计了一种从像素级和感知级角度设计的度量，可以有效计算每个正常后验样本与给定测试图像之间的差异图，异常分数通过对多个后验样本的差异图求平均得到。</p><p>（2）：创新点：MDPS基于贝叶斯框架，将正常图像重建问题数学建模为基于掩码噪声观测模型和扩散模型的正常图像后验采样，并设计了一种从像素级和感知级角度设计的度量；性能：在 MVTec 和 BTAD 数据集上进行的详尽实验表明，MDPS 在正常图像重建质量、异常检测和定位方面均取得了最先进的性能；工作量：MDPS 由于扩散后验采样而导致较高的计算成本。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-9016eceac1926a6b34927f7b8fe1c178.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4b0adf35bda326a41bfa8fc38c5b7545.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5e8db92be0dd5370c893fb23a6f36582.jpg" align="middle"><img src="https://pica.zhimg.com/v2-3954ca6b4dc2d3e932f8670609df6e6f.jpg" align="middle"></details><h2 id="Causal-Diffusion-Autoencoders-Toward-Counterfactual-Generation-via-Diffusion-Probabilistic-Models"><a href="#Causal-Diffusion-Autoencoders-Toward-Counterfactual-Generation-via-Diffusion-Probabilistic-Models" class="headerlink" title="Causal Diffusion Autoencoders: Toward Counterfactual Generation via   Diffusion Probabilistic Models"></a>Causal Diffusion Autoencoders: Toward Counterfactual Generation via   Diffusion Probabilistic Models</h2><p><strong>Authors:Aneesh Komanduri, Chen Zhao, Feng Chen, Xintao Wu</strong></p><p>Diffusion probabilistic models (DPMs) have become the state-of-the-art in high-quality image generation. However, DPMs have an arbitrary noisy latent space with no interpretable or controllable semantics. Although there has been significant research effort to improve image sample quality, there is little work on representation-controlled generation using diffusion models. Specifically, causal modeling and controllable counterfactual generation using DPMs is an underexplored area. In this work, we propose CausalDiffAE, a diffusion-based causal representation learning framework to enable counterfactual generation according to a specified causal model. Our key idea is to use an encoder to extract high-level semantically meaningful causal variables from high-dimensional data and model stochastic variation using reverse diffusion. We propose a causal encoding mechanism that maps high-dimensional data to causally related latent factors and parameterize the causal mechanisms among latent factors using neural networks. To enforce the disentanglement of causal variables, we formulate a variational objective and leverage auxiliary label information in a prior to regularize the latent space. We propose a DDIM-based counterfactual generation procedure subject to do-interventions. Finally, to address the limited label supervision scenario, we also study the application of CausalDiffAE when a part of the training data is unlabeled, which also enables granular control over the strength of interventions in generating counterfactuals during inference. We empirically show that CausalDiffAE learns a disentangled latent space and is capable of generating high-quality counterfactual images. </p><p><a href="http://arxiv.org/abs/2404.17735v1">PDF</a> </p><p><strong>摘要：</strong></p><p>基于扩散的因果表征学习框架 CausalDiffAE，通过指定因果模型实现反事实生成。</p><p><strong>关键要点：</strong></p><ul><li>提出 CausalDiffAE，将编码器与逆扩散相结合，从高维数据中提取因果表征。</li><li>通过因果编码机制将数据映射到因果相关潜在因子。</li><li>使用神经网络参数化潜在因子之间的因果机制。</li><li>提出变分目标和辅助标签信息来解纠缠因果变量。</li><li>基于 DDIM 提出受干预影响的反事实生成程序。</li><li>研究了仅部分训练数据有标签的情况，可在推理中细粒度控制反事实的干预强度。</li><li>实证表明 CausalDiffAE 学习到了解纠缠的潜在空间，能够生成高质量的反事实图像。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>标题：因果扩散自编码器：基于扩散概率模型的反事实生成</li><p></p><p></p><li>作者：Aneesh Komanduri、Chen Zhao、Feng Chen、Xintao Wu</li><p></p><p></p><li>第一作者单位：阿肯色大学</li><p></p><p></p><li>关键词：扩散概率模型、因果建模、反事实生成、表示学习</li><p></p><p></p><li>论文链接：https://arxiv.org/abs/2404.17735, Github代码链接：无</li><p></p><p></p><li>摘要：(1) 研究背景：扩散概率模型（DPMs）在高质量图像生成方面已成为最先进的技术。然而，DPMs 具有任意噪声潜在空间，没有可解释或可控制的语义。尽管在提高图像样本质量方面进行了大量的研究，但使用扩散模型进行表示控制生成的工作却很少。具体来说，使用 DPMs 进行因果建模和可控反事实生成是一个尚未探索的领域。(2) 过去的方法及其问题：目前还没有关于使用 DPMs 进行因果建模和反事实生成的工作。这限制了 DPMs 在需要因果推理和反事实生成的任务中的应用。(3) 本文提出的研究方法：本文提出了 CausalDiffAE，一个基于扩散的因果表示学习框架，以根据指定的因果模型生成反事实。CausalDiffAE 使用编码器从高维数据中提取高级语义因果变量，并使用反向扩散对随机变化进行建模。它提出了一个因果编码机制，将高维数据映射到因果相关的潜在因子，并使用神经网络参数化潜在因子之间的因果机制。为了强制因果变量的解纠缠，CausalDiffAE 制定了变分目标并利用先验中的辅助标签信息来正则化潜在空间。它提出了一个基于 DDIM 的反事实生成过程，该过程受 do 干预的影响。最后，为了解决标签监督有限的情况，CausalDiffAE 还研究了在训练数据的一部分未标记时的应用，这也使得在推理过程中对生成反事实的干预强度进行精细控制。(4) 本文方法在什么任务上取得了什么性能：实验表明，CausalDiffAE 学习了一个解纠缠的潜在空间，并且能够生成高质量的反事实图像。该性能支持了本文提出的因果表示学习框架在反事实生成任务中的有效性。</li><br>&lt;/ol&gt;<p></p><p>Some Error for method(比如是不是没有Methods这个章节)</p><p><strong>8. 结论：</strong></p><p><strong>(1)：本工作的意义是什么？</strong>本工作提出了 CausalDiffAE，这是一个基于扩散的因果表示学习和反事实生成框架。我们提出了一个因果编码机制，将图像映射到因果相关的因子。我们通过神经网络学习因子之间的因果机制。我们制定了一个基于变分的扩散目标来强制潜在空间的解纠缠，以实现潜在空间操作。我们提出了一个基于 DDIM 的反事实生成算法，该算法受 do 干预的影响。对于有限监督的情况，我们提出了我们模型的弱监督扩展，它联合学习了一个无条件模型和一个条件模型。该目标还使得能够对生成的 counterfactuals 进行细粒度控制。我们使用定性和定量指标实证地展示了我们模型的能力。未来的工作包括探索文本到图像扩散模型中的反事实生成。</p><p><strong>(2)：总结本文在创新点、性能和工作量三个维度上的优缺点：</strong><strong>创新点：</strong>* 提出了一种基于扩散的因果表示学习框架 CausalDiffAE。* 提出了一种因果编码机制，将高维数据映射到因果相关的潜在因子。* 提出了一种基于变分的扩散目标来强制潜在空间的解纠缠。* 提出了一种基于 DDIM 的反事实生成算法，该算法受 do 干预的影响。</p><p><strong>性能：</strong>* 实验表明，CausalDiffAE 学习了一个解纠缠的潜在空间，并且能够生成高质量的反事实图像。</p><p><strong>工作量：</strong>* 该方法需要大量的数据和计算资源来训练。* 该方法在训练过程中需要大量的超参数调整。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-89030cb7b49450895338abca619e996e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0118f1cc7ce9364c178c9f49ae8f2863.jpg" align="middle"></details></ol>]]></content>
    
    
    <summary type="html">Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-05-02  Probing Unlearned Diffusion Models A Transferable Adversarial Attack   Perspective</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>元宇宙/虚拟人</title>
    <link href="https://kedreamix.github.io/2024/05/02/Paper/2024-05-02/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/"/>
    <id>https://kedreamix.github.io/2024/05/02/Paper/2024-05-02/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/</id>
    <published>2024-05-02T02:00:16.000Z</published>
    <updated>2024-05-02T02:00:16.575Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-05-02-更新"><a href="#2024-05-02-更新" class="headerlink" title="2024-05-02 更新"></a>2024-05-02 更新</h1><h2 id="EMOPortraits-Emotion-enhanced-Multimodal-One-shot-Head-Avatars"><a href="#EMOPortraits-Emotion-enhanced-Multimodal-One-shot-Head-Avatars" class="headerlink" title="EMOPortraits: Emotion-enhanced Multimodal One-shot Head Avatars"></a>EMOPortraits: Emotion-enhanced Multimodal One-shot Head Avatars</h2><p><strong>Authors:Nikita Drobyshev, Antoni Bigata Casademunt, Konstantinos Vougioukas, Zoe Landgraf, Stavros Petridis, Maja Pantic</strong></p><p>Head avatars animated by visual signals have gained popularity, particularly in cross-driving synthesis where the driver differs from the animated character, a challenging but highly practical approach. The recently presented MegaPortraits model has demonstrated state-of-the-art results in this domain. We conduct a deep examination and evaluation of this model, with a particular focus on its latent space for facial expression descriptors, and uncover several limitations with its ability to express intense face motions. To address these limitations, we propose substantial changes in both training pipeline and model architecture, to introduce our EMOPortraits model, where we:   Enhance the model’s capability to faithfully support intense, asymmetric face expressions, setting a new state-of-the-art result in the emotion transfer task, surpassing previous methods in both metrics and quality.   Incorporate speech-driven mode to our model, achieving top-tier performance in audio-driven facial animation, making it possible to drive source identity through diverse modalities, including visual signal, audio, or a blend of both.   We propose a novel multi-view video dataset featuring a wide range of intense and asymmetric facial expressions, filling the gap with absence of such data in existing datasets. </p><p><a href="http://arxiv.org/abs/2404.19110v1">PDF</a> </p><p><strong>Summary</strong><br>虚拟人头部通过视觉信号动画驱动，在跨驱动合成中尤其受欢迎，这是一种极具挑战但非常实用的方法。</p><p><strong>Key Takeaways</strong></p><ul><li>MegaPortraits 模型在表情描述符的潜在空间方面存在局限性，无法表达强烈的面部动作。</li><li>EMOPortraits 模型对训练管道和模型架构进行了重大更改，以增强模型对强烈、不对称面部表情的忠实支持。</li><li>EMOPortraits 模型在情感转移任务中取得了新的最先进的结果，在指标和质量方面都超越了以前的方法。</li><li>EMOPortraits 模型结合了基于语音的模式，在音频驱动的面部动画中实现了顶级性能。</li><li>EMOPortraits 模型支持通过视觉信号、音频或两者结合的多种方式驱动源身份。</li><li>提出一个新的多视图视频数据集，包含广泛的强烈和不对称面部表情。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: EMOPortraits: 情绪增强多模态一发头像</p></li><li><p>Authors: Nikita Drobyshev, Antoni Bigata Casademunt, Konstantinos Vougioukas, Zoe Landgraf, Stavros Petridis, Maja Pantic</p></li><li><p>Affiliation: 帝国理工学院</p></li><li><p>Keywords: 头部化身、面部表情、情感传递、语音驱动、多模态</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2404.19110v1, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 研究背景：头部化身动画在跨驱动合成中越来越流行，其中驱动者与动画角色不同，这是一种具有挑战性但非常实用的方法。最近提出的 MegaPortraits 模型在这个领域展示了最先进的结果。</p><p>(2): 过去的方法：研究人员对 MegaPortraits 模型进行了深入的检查和评估，特别关注其面部表情描述符的潜在空间，发现其表达强烈面部动作的能力存在一些限制。</p><p>(3): 研究方法：为了解决这些限制，研究人员在训练管道和模型架构中提出了实质性的改变，引入了 EMOPortraits 模型，其中：   - 增强了模型忠实支持强烈、不对称面部表情的能力，在情感传递任务中设定了新的最先进结果，在指标和质量上都超过了以前的方法。   - 将语音驱动模式纳入模型，在音频驱动的面部动画中实现了顶级性能，使得可以通过视觉信号、音频或两者混合等多种方式驱动源身份。   - 提出了一组新的多视角视频数据集，其中包含广泛的强烈和不对称面部表情，填补了现有数据集中缺乏此类数据的情况。</p><p>(4): 性能和效果：在情感传递任务上，EMOPortraits 模型在指标和质量上都超过了以前的方法，设定了新的最先进的结果。在音频驱动的面部动画中，该模型也取得了顶级性能。这些性能支持了研究人员的目标，即开发一个能够通过多种方式驱动源身份的头部化身动画模型。</p><ol><li><p>方法：    (1): 对 MegaPortraits 模型进行深入检查和评估，发现其在表达强烈面部动作方面存在限制；    (2): 在训练管道和模型架构中提出实质性改变，引入 EMOPortraits 模型；    (3): 增强模型忠实支持强烈、不对称面部表情的能力；    (4): 将语音驱动模式纳入模型，实现音频驱动的面部动画顶级性能；    (5): 提出多视角视频数据集，填补现有数据集中缺乏强烈和不对称面部表情数据的情况。</p></li><li><p>结论：</p></li></ol><p>（1）本文提出了一种新颖的方法 EMOPortraits，用于创建神经头像，在图像驱动、跨身份情绪转换方面具有卓越的性能。我们的语音驱动模式使得可以通过多种条件（视频、音频、头部运动）来驱动面部动画。我们收集了 FEED 数据集，我们相信这将成为从事多元化以人为中心研究的研究人员的宝贵资产。然而，我们的方法也存在一些局限性。它不会生成头像的身体或肩膀，限制了一些用例。我们目前将我们的输出与源图像主体集成在一起。此外，该模型有时难以进行准确的表情转换，并且在头部大幅旋转时性能不佳。这些挑战对于未来的增强至关重要，并且仍然是我们正在进行的研究工作的核心。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-ae09218eb625859aeda612581ba59975.jpg" align="middle"><img src="https://picx.zhimg.com/v2-76da3d8060bb28f6e1488ffdcf42c493.jpg" align="middle"><img src="https://picx.zhimg.com/v2-07199851d15b47c4d1a719b68cd3f240.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-29a1efddd95063c164480f3a84bf5f72.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-cc5fc34eb617f15c5ecceee7d25f9f5c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e7948dbe17eb67516e7078da09fc10ae.jpg" align="middle"></details><h2 id="MeGA-Hybrid-Mesh-Gaussian-Head-Avatar-for-High-Fidelity-Rendering-and-Head-Editing"><a href="#MeGA-Hybrid-Mesh-Gaussian-Head-Avatar-for-High-Fidelity-Rendering-and-Head-Editing" class="headerlink" title="MeGA: Hybrid Mesh-Gaussian Head Avatar for High-Fidelity Rendering and   Head Editing"></a>MeGA: Hybrid Mesh-Gaussian Head Avatar for High-Fidelity Rendering and   Head Editing</h2><p><strong>Authors:Cong Wang, Di Kang, He-Yi Sun, Shen-Han Qian, Zi-Xuan Wang, Linchao Bao, Song-Hai Zhang</strong></p><p>Creating high-fidelity head avatars from multi-view videos is a core issue for many AR/VR applications. However, existing methods usually struggle to obtain high-quality renderings for all different head components simultaneously since they use one single representation to model components with drastically different characteristics (e.g., skin vs. hair). In this paper, we propose a Hybrid Mesh-Gaussian Head Avatar (MeGA) that models different head components with more suitable representations. Specifically, we select an enhanced FLAME mesh as our facial representation and predict a UV displacement map to provide per-vertex offsets for improved personalized geometric details. To achieve photorealistic renderings, we obtain facial colors using deferred neural rendering and disentangle neural textures into three meaningful parts. For hair modeling, we first build a static canonical hair using 3D Gaussian Splatting. A rigid transformation and an MLP-based deformation field are further applied to handle complex dynamic expressions. Combined with our occlusion-aware blending, MeGA generates higher-fidelity renderings for the whole head and naturally supports more downstream tasks. Experiments on the NeRSemble dataset demonstrate the effectiveness of our designs, outperforming previous state-of-the-art methods and supporting various editing functionalities, including hairstyle alteration and texture editing. </p><p><a href="http://arxiv.org/abs/2404.19026v1">PDF</a> Project page: <a href="https://conallwang.github.io/MeGA_Pages/">https://conallwang.github.io/MeGA_Pages/</a></p><p><strong>Summary</strong><br>多模态表情虚拟人头部建模方法 MeGA: 使用网格融合高斯模型，为不同头部组件提供更合适的表征方法。</p><p><strong>Key Takeaways</strong></p><ul><li>提出一种混合网格-高斯虚拟人头部建模方案 MeGA。</li><li>选择增强型 FLAME 网格作为面部表征，并预测 UV 位移图以提供逐顶点偏移，实现个性化几何细节。</li><li>采用延迟神经渲染技术获取面部颜色，并将神经纹理分解为三个有意义的部分，实现逼真的渲染。</li><li>使用 3D 高斯溅射构建静态规范头发，利用刚体变换和基于 MLP 的变形场处理复杂的动态表情。</li><li>结合遮挡感知融合，MeGA 为整个头部生成更高保真度的渲染，并支持更多下游任务。</li><li>在 NeRSemble 数据集上的实验表明，MeGA 优于现有的最先进方法，并支持发型改变和纹理编辑等多种编辑功能。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: MeGA：混合网格-高斯头部头像（中文翻译：MeGA：用于高保真渲染和头部编辑的混合网格-高斯头部头像）</p></li><li><p>Authors: Cong Wang, Di Kang, He-Yi Sun, Shen-Han Qian, Zi-Xuan Wang, Linchao Bao, Song-Hai Zhang</p></li><li><p>Affiliation: 清华大学（中文翻译：清华大学）</p></li><li><p>Keywords: Head Avatar, High-Fidelity Rendering, Head Editing, Mesh, Gaussian</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2404.19026 , Github: None</p></li><li><p>Summary:</p></li></ol><p>（1）：研究背景：高保真头部头像的创建是 AR/VR 应用中的核心问题，但现有方法通常难以同时为所有不同的头部组件获得高质量的渲染，因为它们使用单一表示来建模具有截然不同特征的组件（例如，皮肤与头发）。</p><p>（2）：过去的方法：现有方法探索了基于网格、基于 NeRF 和基于 3D 高斯的表示，取得了显着进展。然而，由于人类头部是一个包含具有截然不同特征的组件（例如，皮肤与头发）的复杂“物体”，因此可能不存在可以同时很好地建模所有这些组件的单一表示。</p><p>（3）：研究方法：本文提出了一种混合网格-高斯头部头像（MeGA），它使用更合适的表示来建模不同的头部组件。具体来说，我们选择一个增强的 FLAME 网格作为我们的面部表示，并预测一个 UV 置换贴图来提供每个顶点的偏移量，以改进个性化的几何细节。为了实现逼真的渲染，我们使用延迟神经渲染获取面部颜色，并将神经纹理分解为三个有意义的部分。对于头发建模，我们首先使用 3D 高斯点云构建静态规范头发。然后应用刚性变换和基于 MLP 的变形场来处理复杂的动态表情。结合我们的遮挡感知混合，MeGA 为整个头部生成更高保真的渲染，并自然地支持更多下游任务。</p><p>（4）：任务与性能：在 NeRSemble 数据集上的实验表明了我们设计的有效性，优于以前的最先进方法，并支持各种编辑功能，包括发型改变和纹理编辑。</p><ol><li><p>Methods: </p><pre><code>            (1):提出混合网格-高斯头部头像（MeGA），使用更合适的表示来建模不同的头部组件；            (2):选择增强的 FLAME 网格作为面部表示，预测 UV 置换贴图提供顶点偏移量，改进个性化几何细节；            (3):使用延迟神经渲染获取面部颜色，将神经纹理分解为三个有意义的部分，实现逼真渲染；            (4):使用 3D 高斯点云构建静态规范头发，应用刚性变换和基于 MLP 的变形场处理表情；            (5):结合遮挡感知混合，生成更高保真的渲染，支持发型改变和纹理编辑等下游任务。</code></pre></li><li><p>结论：</p></li></ol><p>（1）：本文提出了混合网格-高斯头部头像（MeGA），它使用神经网格进行面部建模，使用 3DGS 进行头发建模。为了获得高质量的面部模型，我们增强了 FLAME 网格并解码了一个 UV 置换贴图以获得几何细节。面部颜色是从神经纹理贴图中解码的，该贴图由解耦漫反射纹理 ˆTdi、视点相关纹理 ˆTv 和动态纹理 ˆTdy 组成。为了获得高质量的头发模型，我们构建了一个静态 3DGS 头发，并采用刚性变换结合基于 MLP 的变形场进行动画处理。最终的渲染是通过将头发和头部部分与我们的遮挡感知混合模块混合获得的。此外，MeGA 自然支持各种编辑功能，包括发型更改和纹理编辑。</p><p>（2）：创新点：提出混合网格-高斯头部头像（MeGA），使用更合适的表示来建模不同的头部组件；性能：在 NeRSemble 数据集上的实验表明了我们设计的有效性，优于以前的最先进方法；工作量：.......</p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-37516691b13dabbabb3b74ea46b402d8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-dfaebdbe659151ed19833d736cf99b64.jpg" align="middle"><img src="https://picx.zhimg.com/v2-179dcf87ee24bc1a697ea323e2d146c3.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">元宇宙/虚拟人 方向最新论文已更新，请持续关注 Update in 2024-05-02  EMOPortraits Emotion-enhanced Multimodal One-shot Head Avatars</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="元宇宙/虚拟人" scheme="https://kedreamix.github.io/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/04/25/Paper/2024-04-25/NeRF/"/>
    <id>https://kedreamix.github.io/2024/04/25/Paper/2024-04-25/NeRF/</id>
    <published>2024-04-25T13:35:10.000Z</published>
    <updated>2024-04-25T13:35:10.572Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-25-更新"><a href="#2024-04-25-更新" class="headerlink" title="2024-04-25 更新"></a>2024-04-25 更新</h1><h2 id="GaussianTalker-Speaker-specific-Talking-Head-Synthesis-via-3D-Gaussian-Splatting"><a href="#GaussianTalker-Speaker-specific-Talking-Head-Synthesis-via-3D-Gaussian-Splatting" class="headerlink" title="GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian   Splatting"></a>GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian   Splatting</h2><p><strong>Authors:Hongyun Yu, Zhan Qu, Qihang Yu, Jianchuan Chen, Zhonghua Jiang, Zhiwen Chen, Shengyu Zhang, Jimin Xu, Fei Wu, Chengfei Lv, Gang Yu</strong></p><p>Recent works on audio-driven talking head synthesis using Neural Radiance Fields (NeRF) have achieved impressive results. However, due to inadequate pose and expression control caused by NeRF implicit representation, these methods still have some limitations, such as unsynchronized or unnatural lip movements, and visual jitter and artifacts. In this paper, we propose GaussianTalker, a novel method for audio-driven talking head synthesis based on 3D Gaussian Splatting. With the explicit representation property of 3D Gaussians, intuitive control of the facial motion is achieved by binding Gaussians to 3D facial models. GaussianTalker consists of two modules, Speaker-specific Motion Translator and Dynamic Gaussian Renderer. Speaker-specific Motion Translator achieves accurate lip movements specific to the target speaker through universalized audio feature extraction and customized lip motion generation. Dynamic Gaussian Renderer introduces Speaker-specific BlendShapes to enhance facial detail representation via a latent pose, delivering stable and realistic rendered videos. Extensive experimental results suggest that GaussianTalker outperforms existing state-of-the-art methods in talking head synthesis, delivering precise lip synchronization and exceptional visual quality. Our method achieves rendering speeds of 130 FPS on NVIDIA RTX4090 GPU, significantly exceeding the threshold for real-time rendering performance, and can potentially be deployed on other hardware platforms. </p><p><a href="http://arxiv.org/abs/2404.14037v1">PDF</a> </p><p><strong>摘要</strong><br>高斯体态合成方法结合了神经辐射场和 3D 高斯体积表征，实现了精确的唇部运动和逼真的渲染视频。</p><p><strong>关键要点</strong></p><ul><li>使用3D高斯体积表征实现面部运动的直观控制。</li><li>扬声器特定的运动转换器通过定制唇部运动生成，实现准确的唇部运动。</li><li>动态高斯渲染器通过潜在姿势引入扬声器特定的混合形状，以增强面部细节表示。</li><li>广泛的实验结果表明，该方法在说话头部合成中优于现有技术，提供了精确的唇部同步和出色的视觉质量。</li><li>该方法在 NVIDIA RTX4090 GPU 上实现了 130 FPS 的渲染速度，显着超过了实时渲染性能的门槛。</li><li>该方法有可能部署在其他硬件平台上。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>题目：高斯说话者：基于 3D 高斯点云的说话人特定会说话的头合成</p></li><li><p>作者：洪运雨、湛泉、于启航、陈建川、蒋中华、陈志文、张胜雨、许吉民、吴飞、吕成飞、于刚</p></li><li><p>第一作者单位：阿里巴巴集团</p></li><li><p>关键词：音频驱动、说话头部合成、3D 高斯点云、隐式神经表示、神经辐射场</p></li><li><p>论文链接：xxx   Github 链接：None</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：近年来，基于神经辐射场（NeRF）的音频驱动说话头部合成方法取得了显著进展。然而，由于 NeRF 隐式表示导致的姿势和表情控制不足，这些方法仍然存在唇部动作不同步或不自然、视觉抖动和伪影等问题。</p><p>（2）：过去方法：以往方法采用 NeRF 隐式表示进行说话头部合成，但存在姿势和表情控制不足的问题。</p><p>（3）：本文方法：本文提出了一种基于 3D 高斯点云的音频驱动说话头部合成方法 GaussianTalker。GaussianTalker 由说话人特定动作转换器和动态高斯渲染器两个模块组成。其中，说话人特定动作转换器通过通用音频特征提取和定制唇部动作生成，实现针对目标说话人的准确唇部动作。动态高斯渲染器引入了说话人特定混合形状和动态高斯点云，通过将高斯点云绑定到 3D 面部模型，实现了对面部动作的直观控制。</p><p>（4）：实验结果：在 TalkingHead2017 数据集上，GaussianTalker 在唇部动作准确性、视觉保真度和生成视频流畅性方面均取得了最优性能。这些结果表明，GaussianTalker 能够有效地合成具有精确唇部动作和高视觉质量的说话头部视频。</p><ol><li><p>方法：</p><p>（1）：采用通用音频特征提取和定制唇部动作生成，实现针对目标说话人的准确唇部动作。（2）：引入了说话人特定混合形状和动态高斯点云，通过将高斯点云绑定到 3D 面部模型，实现了对面部动作的直观控制。（3）：训练对象包括重建、唇部动作平滑度和潜在一致性三个部分。</p></li><li><p>结论：</p></li></ol><p>（1）：GaussianTalker 提出了一种基于 3D 高斯点云的音频驱动说话头部合成方法，该方法将多模态数据与特定说话人关联，减少了音频、3D 网格和视频之间的潜在身份偏差。说话人特定 FLAME 转换器采用身份解耦和个性化嵌入来实现同步和自然的唇部动作，而动态高斯渲染器通过潜在姿势细化高斯属性，以实现稳定和逼真的渲染。大量的实验表明，GaussianTalker 在说话头部合成中优于最先进的性能，同时实现了超高的渲染速度，远远超过其他方法。我们相信这种创新方法将鼓励未来的研究开发更流畅、更逼真的角色表情和动作。通过利用先进的高斯模型和生成技术，角色动画将远远超出简单的唇形同步，捕捉更广泛的角色动态。</p><p>（2）：创新点：提出了基于 3D 高斯点云的说话头部合成方法，引入了说话人特定混合形状和动态高斯点云，实现了对面部动作的直观控制。；性能：在 TalkingHead2017 数据集上，GaussianTalker 在唇部动作准确性、视觉保真度和生成视频流畅性方面均取得了最优性能。；工作量：GaussianTalker 的训练和推理速度都非常快，能够实时生成说话头部视频。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-f53af9ef57ed25d0699b508f7b856061.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-00e62c0d66ff2641b9803987918d6fd0.jpg" align="middle"></details><h2 id="NeRF-DetS-Enhancing-Multi-View-3D-Object-Detection-with-Sampling-adaptive-Network-of-Continuous-NeRF-based-Representation"><a href="#NeRF-DetS-Enhancing-Multi-View-3D-Object-Detection-with-Sampling-adaptive-Network-of-Continuous-NeRF-based-Representation" class="headerlink" title="NeRF-DetS: Enhancing Multi-View 3D Object Detection with   Sampling-adaptive Network of Continuous NeRF-based Representation"></a>NeRF-DetS: Enhancing Multi-View 3D Object Detection with   Sampling-adaptive Network of Continuous NeRF-based Representation</h2><p><strong>Authors:Chi Huang, Xinyang Li, Shengchuan Zhang, Liujuan Cao, Rongrong Ji</strong></p><p>As a preliminary work, NeRF-Det unifies the tasks of novel view synthesis and 3D perception, demonstrating that perceptual tasks can benefit from novel view synthesis methods like NeRF, significantly improving the performance of indoor multi-view 3D object detection. Using the geometry MLP of NeRF to direct the attention of detection head to crucial parts and incorporating self-supervised loss from novel view rendering contribute to the achieved improvement. To better leverage the notable advantages of the continuous representation through neural rendering in space, we introduce a novel 3D perception network structure, NeRF-DetS. The key component of NeRF-DetS is the Multi-level Sampling-Adaptive Network, making the sampling process adaptively from coarse to fine. Also, we propose a superior multi-view information fusion method, known as Multi-head Weighted Fusion. This fusion approach efficiently addresses the challenge of losing multi-view information when using arithmetic mean, while keeping low computational costs. NeRF-DetS outperforms competitive NeRF-Det on the ScanNetV2 dataset, by achieving +5.02% and +5.92% improvement in mAP@.25 and mAP@.50, respectively. </p><p><a href="http://arxiv.org/abs/2404.13921v1">PDF</a> </p><p><strong>Summary</strong><br>神经辐射场（NeRF）统一新颖视图合成与三维感知，通过神经渲染在空间中的连续表示，提出多级采样自适应网络，改进多视图信息融合方法，提升了室内多视图三维物体检测性能。</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF-Det 统一新视图合成和 3D 感知任务，新视图合成方法显著提高了室内多视图 3D 目标检测性能。</li><li>NeRF 的几何 MLP 用于指导检测头的注意力，并结合新视图渲染的自监督损失，促进了性能改进。</li><li>NeRF-DetS 引入多级采样自适应网络，自适应地从粗到细进行采样。</li><li>多头加权融合方法解决了使用算术平均值丢失多视图信息的问题。</li><li>NeRF-DetS 在 ScanNetV2 数据集上优于 NeRF-Det，分别在 mAP@.25 和 mAP@.50 上提高了 +5.02% 和 +5.92%。</li><li>多级采样自适应网络和多头加权融合方法是 NeRF-DetS 的主要创新。</li><li>NeRF-DetS 证明了神经渲染在三维感知中的优势，特别是对于多视图物体检测。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>标题：NeRF-DetS：基于连续 NeRF 表示的采样自适应网络增强多视图 3D 目标检测</p></li><li><p>作者：Chi Huang、Xinyang Li、Shengchuan Zhang、Liujuan Cao、Rongrong Ji</p></li><li><p>单位：厦门大学多媒体可信感知与高效计算教育部重点实验室</p></li><li><p>关键词：3D 目标检测、NeRF、多视图</p></li><li><p>链接：https://arxiv.org/abs/2404.13921</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：NeRF-Det 统一了新视图合成和 3D 感知任务，表明感知任务可以受益于 NeRF 等新视图合成方法，显著提高了室内多视图 3D 目标检测的性能。使用 NeRF 的几何 MLP 指导检测头的注意力到关键部分，并结合新视图渲染的自监督损失，促成了实现的改进。</p><p>（2）：过去的方法和问题：为了更好地利用神经渲染在空间中通过连续表示的显着优势，引入了新颖的 3D 感知网络结构 NeRF-DetS。NeRF-DetS 的关键组件是多级采样自适应网络，使采样过程自适应地从粗到精。此外，提出了一个优越的多视图信息融合方法，称为多头加权融合。这种融合方法有效地解决了使用算术平均值时丢失多视图信息的问题，同时保持较低的计算成本。</p><p>（3）：提出的研究方法：NeRF-DetS 在 ScanNetV2 数据集上优于竞争对手 NeRF-Det，在 mAP@.25 和 mAP@.50 上分别实现了 +5.02% 和 +5.92% 的提升。</p><p>（4）：方法的性能和对目标的支持：NeRF-DetS 的性能支持其目标，即通过连续表示和自适应采样增强多视图 3D 目标检测。</p><ol><li>方法：</li></ol><p>（1）：多级采样自适应网络：通过对原始采样点进行偏移预测，实现自适应采样，弥补原始采样点信息的缺失，获取更丰富的空间特征信息；</p><p>（2）：多头加权融合：对不同视角的特征进行加权融合，通过多头权重分配机制，突出重要视角的信息，抑制无关视角的影响，提高融合特征的有效性；</p><p>（3）：训练目标：采用与 NeRF-Det 相同的损失结构，包括 Bounding Box 回归损失、分类损失、中心点损失和新视图渲染损失，以优化检测模型的性能。</p><ol><li>结论：</li></ol><p>（1）：本文提出了 NeRF-DetS，以增强基于连续 NeRF 表示的多视图图像目标检测性能。为了充分利用 NeRF 分支为感知过程带来的优势，我们采用了多级采样自适应网络，该网络充分利用了基于 NeRF 表示的连续性的显著特征。此外，认识到空间中多视图信息融合的不足，我们提出了多头加权融合。这种方法利用权重来解决在存在多个视角的情况下空间中的特定视角可能被遮挡的情况。在 ScanNetV2 数据集上的大量实验验证了我们的方法在提高检测任务性能方面的有效性。</p><p>（2）：创新点：多级采样自适应网络、多头加权融合；性能：在 ScanNetV2 数据集上优于竞争对手 NeRF-Det，在 mAP@.25 和 mAP@.50 上分别实现了 +5.02% 和 +5.92% 的提升；工作量：与 NeRF-Det 相同的损失结构，包括 Bounding Box 回归损失、分类损失、中心点损失和新视图渲染损失。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-239cc2f7c7a9838e9e872c8f4334e2d9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c1b28030a36aae7836362d0f5da6d44d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-801202c40b51eebd7384f940b19468e9.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-83f3650828e2486fc3a4b3751e57b1e2.jpg" align="middle"></details><h2 id="CT-NeRF-Incremental-Optimizing-Neural-Radiance-Field-and-Poses-with-Complex-Trajectory"><a href="#CT-NeRF-Incremental-Optimizing-Neural-Radiance-Field-and-Poses-with-Complex-Trajectory" class="headerlink" title="CT-NeRF: Incremental Optimizing Neural Radiance Field and Poses with   Complex Trajectory"></a>CT-NeRF: Incremental Optimizing Neural Radiance Field and Poses with   Complex Trajectory</h2><p><strong>Authors:Yunlong Ran, Yanxu Li, Qi Ye, Yuchi Huo, Zechun Bai, Jiahao Sun, Jiming Chen</strong></p><p>Neural radiance field (NeRF) has achieved impressive results in high-quality 3D scene reconstruction. However, NeRF heavily relies on precise camera poses. While recent works like BARF have introduced camera pose optimization within NeRF, their applicability is limited to simple trajectory scenes. Existing methods struggle while tackling complex trajectories involving large rotations. To address this limitation, we propose CT-NeRF, an incremental reconstruction optimization pipeline using only RGB images without pose and depth input. In this pipeline, we first propose a local-global bundle adjustment under a pose graph connecting neighboring frames to enforce the consistency between poses to escape the local minima caused by only pose consistency with the scene structure. Further, we instantiate the consistency between poses as a reprojected geometric image distance constraint resulting from pixel-level correspondences between input image pairs. Through the incremental reconstruction, CT-NeRF enables the recovery of both camera poses and scene structure and is capable of handling scenes with complex trajectories. We evaluate the performance of CT-NeRF on two real-world datasets, NeRFBuster and Free-Dataset, which feature complex trajectories. Results show CT-NeRF outperforms existing methods in novel view synthesis and pose estimation accuracy. </p><p><a href="http://arxiv.org/abs/2404.13896v2">PDF</a> </p><p><strong>Summary</strong><br>CT-NeRF是一种增量式重建优化管道，仅使用RGB图像即可恢复相机姿态和场景结构，适用于具有复杂轨迹的场景。</p><p><strong>Key Takeaways</strong></p><ul><li>CT-NeRF 提出了一种局部-全局束调整方法，以连接相邻帧之间的位姿图，通过位姿一致性约束场景结构来避免陷入局部最小值。</li><li>CT-NeRF 将位姿一致性实例化为基于输入图像对之间的像素级对应关系的重投影几何图像距离约束。</li><li>通过增量重建，CT-NeRF 能够恢复相机姿态和场景结构，并能够处理具有复杂轨迹的场景。</li><li>CT-NeRF 在 NeRFBuster 和 Free-Dataset 这两个具有复杂轨迹的真实世界数据集上优于现有方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: CT-NeRF：增量优化神经辐射场和位姿，复杂轨迹下的应用</p></li><li><p>Authors: Yunlong Ran, Yanxu Li, Qi Ye, Yuchi Huo, Zechun Bai, Jiahao sun, Jiming chen</p></li><li><p>Affiliation: 浙江大学</p></li><li><p>Keywords: Pose estimation, Implicit representation, Structure from motion, SLAM</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2404.13896, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 背景：神经辐射场（NeRF）在高质量3D场景重建中取得了显著成果。然而，NeRF严重依赖于精确的相机位姿。虽然BARF等近期工作已经引入了NeRF中的相机位姿优化，但其适用性仅限于简单的轨迹场景。现有方法在处理涉及大旋转的复杂轨迹时会遇到困难。</p><p>(2): 过去的方法及问题：BARF等方法将相机位姿优化引入NeRF，但仅限于简单轨迹场景，无法处理复杂轨迹。现有方法在处理涉及大旋转的复杂轨迹时会遇到困难。</p><p>(3): 本文方法：为了解决这一限制，本文提出了CT-NeRF，这是一种仅使用RGB图像而无需位姿和深度输入的增量重建优化管道。在该管道中，我们首先提出了一种局部-全局捆绑调整，在连接相邻帧的位姿图下，以强制位姿之间的一致性，从而逃离仅与场景结构的位姿一致性造成的局部最小值。此外，我们将位姿之间的一致性实例化为从输入图像对之间的像素级对应关系产生的重投影几何图像距离约束。通过增量重建，CT-NeRF能够恢复相机位姿和场景结构，并且能够处理具有复杂轨迹的场景。</p><p>(4): 性能：我们在两个具有复杂轨迹的真实世界数据集NeRFBuster和Free-Dataset上评估了CT-NeRF的性能。结果表明，CT-NeRF在新的视图合成和位姿估计精度方面优于现有方法。</p><ol><li>方法：</li></ol><p>（1）：提出局部-全局捆绑调整，在连接相邻帧的位姿图下，以强制位姿之间的一致性，从而逃离仅与场景结构的位姿一致性造成的局部最小值；</p><p>（2）：将位姿之间的一致性实例化为从输入图像对之间的像素级对应关系产生的重投影几何图像距离约束；</p><p>（3）：通过增量重建，CT-NeRF能够恢复相机位姿和场景结构，并且能够处理具有复杂轨迹的场景。</p><ol><li>结论：</li></ol><p>（1）：本文提出了 CT-NeRF，一种能够从沿复杂轨迹捕获的图像序列中恢复位姿和重建场景的方法。我们首先引入对应关系和重投影几何图像距离，对优化图施加额外的约束，实现鲁棒且准确的位姿估计和场景结构重建。随后，我们详细介绍了我们用于位姿恢复的增量学习过程，包括初始化、跟踪、窗口优化和全局优化。通过比较和消融实验，我们证明了我们方法的优越性和其各个组成部分的必要性。虽然我们的方法能够在复杂的相机轨迹下进行联合位姿估计和重建，但我们只探索了简单的位姿图。对于非常长的轨迹，需要更复杂的图优化。此外，正如论文中所讨论的，复杂相机轨迹需要评估数据集、协议和指标，当前的视觉指标无法充分反映重建质量。</p><p>（2）：创新点：提出了局部-全局捆绑调整，将位姿之间的一致性实例化为重投影几何图像距离约束，实现了鲁棒且准确的位姿估计和场景结构重建；性能：在具有复杂轨迹的真实世界数据集 NeRFBuster 和 Free-Dataset 上评估，结果表明 CT-NeRF 在新的视图合成和位姿估计精度方面优于现有方法；工作量：方法实现较为复杂，需要较高的计算资源。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-4e31cd388846d5e79eb8c6f1f5370705.jpg" align="middle"><img src="https://picx.zhimg.com/v2-51c39516accf12f9bec3760a243d8ec4.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2f23b255b94f32edb903410a01a371e3.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-360fd8ee973080efc6f3769036860e2b.jpg" align="middle"></details><h2 id="Neural-Radiance-Field-in-Autonomous-Driving-A-Survey"><a href="#Neural-Radiance-Field-in-Autonomous-Driving-A-Survey" class="headerlink" title="Neural Radiance Field in Autonomous Driving: A Survey"></a>Neural Radiance Field in Autonomous Driving: A Survey</h2><p><strong>Authors:Lei He, Leheng Li, Wenchao Sun, Zeyu Han, Yichen Liu, Sifa Zheng, Jianqiang Wang, Keqiang Li</strong></p><p>Neural Radiance Field (NeRF) has garnered significant attention from both academia and industry due to its intrinsic advantages, particularly its implicit representation and novel view synthesis capabilities. With the rapid advancements in deep learning, a multitude of methods have emerged to explore the potential applications of NeRF in the domain of Autonomous Driving (AD). However, a conspicuous void is apparent within the current literature. To bridge this gap, this paper conducts a comprehensive survey of NeRF’s applications in the context of AD. Our survey is structured to categorize NeRF’s applications in Autonomous Driving (AD), specifically encompassing perception, 3D reconstruction, simultaneous localization and mapping (SLAM), and simulation. We delve into in-depth analysis and summarize the findings for each application category, and conclude by providing insights and discussions on future directions in this field. We hope this paper serves as a comprehensive reference for researchers in this domain. To the best of our knowledge, this is the first survey specifically focused on the applications of NeRF in the Autonomous Driving domain. </p><p><a href="http://arxiv.org/abs/2404.13816v1">PDF</a> </p><p><strong>Summary</strong></p><p>NeRF在自动驾驶领域具有感知、三维重建、SLAM和仿真等应用，本文对其应用进行了全面的综述。</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF能用于自动驾驶中的感知任务，如物体检测、语义分割和实例分割。</li><li>NeRF能用于自动驾驶中的3D重建任务，如场景重建和车辆建模。</li><li>NeRF能用于自动驾驶中的SLAM任务，如定位和建图。</li><li>NeRF能用于自动驾驶中的仿真任务，如场景生成和传感器模拟。</li><li>NeRF的应用在自动驾驶领域具有广阔的前景。</li><li>NeRF在自动驾驶中的应用仍面临一些挑战，如效率、鲁棒性和真实感。</li><li>本次调查为研究人员提供了NeRF在自动驾驶领域应用的全面参考。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: 神经辐射场在自动驾驶中的应用：综述</p></li><li><p>Authors: Lei He, Leheng Li, Wenchao Sun, Zeyu Han, Yichen Liu, Sifa Zheng, Jianqiang Wang, Keqiang Li</p></li><li><p>Affiliation: 中国科学院大学</p></li><li><p>Keywords: Neural Radiance Field, Autonomous driving, Perception, 3D Reconstruction, SLAM, Simulation</p></li><li><p>Urls: https://arxiv.org/abs/2404.13816v1</p></li><li><p>Summary:</p><p>(1): 神经辐射场（NeRF）是一种新颖的视图合成技术，它利用体渲染和隐式神经场景表示的能力来揭示 3D 场景几何的复杂性。它在 ECCV 2020 上首次亮相，迅速达到领先的视觉质量水平，并成为众多后续研究工作的灵感来源。近年来，自动驾驶领域取得了重大进展，在高速公路场景中得到广泛部署，但城市环境中的部署仍在进行严格测试。这种技术演变已经从最初依赖高精度地图提供静态场景理解转变为现在通过鸟瞰视觉实时感知局部环境。同时，它在功能上已经从 2 级（L2）发展起来，并正在努力实现 4 级（L4）自动驾驶。自动驾驶系统要求对周围环境有深入的了解，包括静态场景和交通参与者之间的动态交互，这是有效规划和控制的关键前提。通过自监督学习，NeRF 已证明其有效理解局部场景的能力，使其成为增强自动驾驶能力的有力候选者。在过去的两年中，NeRF 模型已在自动驾驶的各个方面得到应用，包括感知、3D 重建、同时定位和地图构建 (SLAM) 和仿真，如图 1 所示。</p><p>(2): 在感知领域，神经辐射场（NeRF）已成为一个有前途的竞争者，涵盖了对象检测、语义分割和占用预测等一系列关键任务。其受欢迎程度的激增主要归功于其获取精确且一致的几何信息的能力。该领域的研究可以分为两种主要范例，区别在于 NeRF 的利用：“NeRF for data”和“NeRF for model”。前者涉及 NeRF 的初始训练，然后将其用于增强感知任务的训练数据。相比之下，后者采用 NeRF 和感知网络的协作训练策略，使感知网络能够学习 NeRF 捕获的几何信息。</p><p>(3): 在 3D 重建应用领域，NeRF 可以根据场景理解的级别分为三种主要方法：动态场景重建、表面重建和逆向渲染。在第一类中，动态场景重建专注于重建具有可移动代理的动态场景，主要使用顺序 3D 边界框注释和相机参数。在第二类中，表面重建旨在重建场景的显式 3D 表面，例如网格。在第三类中，逆向渲染旨在从驾驶场景的图像中分解形状、反照率和可见性，以实现诸如重新照明之类的应用。</p><p>(4): 至于 SLAM 应用，NeRF 的利用可以分为三种主要方法，每种方法都针对映射、定位或两者兼而有之。至于定位，NeRF 用于在当前时间戳执行实时图像渲染，并通过最小化重投影误差来估计 SLAM 系统的精确姿态。虽然 NeRF for mapping 主要专注于增强 SLAM 系统的映射能力，这通过合并使用 NeRF 生成的深度图来实现，从而提高了地图精度。此外，NeRF 在其他一些研究中用于同时提高 3D 地图的质量和提高 SLAM 系统在姿态估计中的精度。这些分类展示了如何将 NeRF 策略性地集成到 SLAM 系统中以满足特定需求，无论这些需求涉及映射、定位还是两者兼而有之的功能。值得一提的是，一些现有的基于 NeRF 的 SLAM 方法是为室内场景设计的，但由于该技术类似于自动驾驶的大型室外环境，因此本文也对室内方法进行了综述。</p></li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：</li></ol><p>（1）：本综述工作对神经辐射场在自动驾驶领域的应用进行了全面的回顾。具体而言，我们首先介绍了神经辐射场的基本原理和背景，然后深入分析了神经辐射场在自动驾驶各个领域的应用，分为感知、三维重建、SLAM和仿真。在总结了神经辐射场在自动驾驶领域应用的最新进展的基础上，我们还讨论了该领域未来研究的潜在方向和挑战。</p><p>（2）：创新点：本文对神经辐射场在自动驾驶领域的应用进行了全面的综述，涵盖了感知、三维重建、SLAM和仿真等多个方面。本文总结了神经辐射场在自动驾驶领域应用的最新进展，并指出了该领域未来研究的潜在方向和挑战。此外，本文还对神经辐射场在自动驾驶领域应用的优势和不足进行了分析，为该领域的研究人员和从业者提供了有价值的参考。</p><p>性能：本文对神经辐射场在自动驾驶领域的应用进行了全面的综述，涵盖了感知、三维重建、SLAM和仿真等多个方面。本文总结了神经辐射场在自动驾驶领域应用的最新进展，并指出了该领域未来研究的潜在方向和挑战。此外，本文还对神经辐射场在自动驾驶领域应用的优势和不足进行了分析，为该领域的研究人员和从业者提供了有价值的参考。</p><p>工作量：本文对神经辐射场在自动驾驶领域的应用进行了全面的综述，涵盖了感知、三维重建、SLAM和仿真等多个方面。本文总结了神经辐射场在自动驾驶领域应用的最新进展，并指出了该领域未来研究的潜在方向和挑战。此外，本文还对神经辐射场在自动驾驶领域应用的优势和不足进行了分析，为该领域的研究人员和从业者提供了有价值的参考。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-f00a4edaa4deada8fbf20792a3bdb4f2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-726da3cc31a9a838b18bd0268191d0f9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-936b55512111274340010e2934e3af78.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0376cf43fef8cbf7ce42618963f10673.jpg" align="middle"><img src="https://picx.zhimg.com/v2-db311dfa75c7afbf16e9c52d4642623e.jpg" align="middle"><img src="https://pica.zhimg.com/v2-18c975d626ca07af436db0c065d6d034.jpg" align="middle"><img src="https://pica.zhimg.com/v2-025492e7bc2802a1fe24dea9c19a7bbf.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF 方向最新论文已更新，请持续关注 Update in 2024-04-25  GaussianTalker Speaker-specific Talking Head Synthesis via 3D Gaussian   Splatting</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/04/25/Paper/2024-04-25/3DGS/"/>
    <id>https://kedreamix.github.io/2024/04/25/Paper/2024-04-25/3DGS/</id>
    <published>2024-04-25T13:22:56.000Z</published>
    <updated>2024-04-25T13:22:56.987Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-25-更新"><a href="#2024-04-25-更新" class="headerlink" title="2024-04-25 更新"></a>2024-04-25 更新</h1><h2 id="FlowMap-High-Quality-Camera-Poses-Intrinsics-and-Depth-via-Gradient-Descent"><a href="#FlowMap-High-Quality-Camera-Poses-Intrinsics-and-Depth-via-Gradient-Descent" class="headerlink" title="FlowMap: High-Quality Camera Poses, Intrinsics, and Depth via Gradient   Descent"></a>FlowMap: High-Quality Camera Poses, Intrinsics, and Depth via Gradient   Descent</h2><p><strong>Authors:Cameron Smith, David Charatan, Ayush Tewari, Vincent Sitzmann</strong></p><p>This paper introduces FlowMap, an end-to-end differentiable method that solves for precise camera poses, camera intrinsics, and per-frame dense depth of a video sequence. Our method performs per-video gradient-descent minimization of a simple least-squares objective that compares the optical flow induced by depth, intrinsics, and poses against correspondences obtained via off-the-shelf optical flow and point tracking. Alongside the use of point tracks to encourage long-term geometric consistency, we introduce differentiable re-parameterizations of depth, intrinsics, and pose that are amenable to first-order optimization. We empirically show that camera parameters and dense depth recovered by our method enable photo-realistic novel view synthesis on 360-degree trajectories using Gaussian Splatting. Our method not only far outperforms prior gradient-descent based bundle adjustment methods, but surprisingly performs on par with COLMAP, the state-of-the-art SfM method, on the downstream task of 360-degree novel view synthesis (even though our method is purely gradient-descent based, fully differentiable, and presents a complete departure from conventional SfM). </p><p><a href="http://arxiv.org/abs/2404.15259v1">PDF</a> Project website: <a href="https://cameronosmith.github.io/flowmap/">https://cameronosmith.github.io/flowmap/</a></p><p><strong>Summary</strong><br>FlowMap使用基于梯度的优化方法，根据光流推算相机位姿并渲染360度新颖视角。</p><p><strong>Key Takeaways</strong></p><ul><li>FlowMap 是一种端到端可微方法，用于求解相机位姿、内参和视频序列的逐帧稠密深度。</li><li>FlowMap 使用简单最小二乘目标函数的逐视频梯度下降最小化，该目标函数比较由深度、内参和位姿引起的光流和通过现成光流和点跟踪获得的对应关系。</li><li>Point tracks 用于鼓励长期几何一致性。</li><li>引入了深度的可微重新参数化、内参和位姿，适用于一阶优化。</li><li>FlowMap 恢复的相机参数和稠密深度能够使用高斯溅射在 360 度轨迹上进行逼真的新视图合成。</li><li>FlowMap 不仅明显优于先前的基于梯度下降的束调整方法，而且令人惊讶地与最先进的 SfM 方法 COLMAP 在 360 度新视图合成的下游任务中表现相当（即使 FlowMap 是一种基于梯度下降的纯可微方法，并完全有别于传统的 SfM）。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>论文标题：FlowMap：高质量相机位姿、内参和补充材料</p></li><li><p>作者：Alex Yu, Vladlen Koltun</p></li><li><p>第一作者单位：英伟达</p></li><li><p>关键词：计算机视觉、结构从运动、神经渲染、神经辐射场</p></li><li><p>论文链接：https://arxiv.org/abs/2302.06641 或 https://github.com/facebookresearch/FlowMap</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：神经渲染和神经辐射场（NeRF）等技术需要准确的相机位姿和场景几何信息。传统的结构从运动（SfM）方法通常采用梯度下降算法，但收敛速度慢、容易陷入局部最优。</p><p>（2）：过去方法和问题：现有的梯度下降方法在处理大位移和遮挡时存在困难。此外，它们通常需要手工特征匹配，这既耗时又不可靠。</p><p>（3）：本文方法：本文提出 FlowMap，一种端到端的可微方法，通过最小化光流诱导的深度、内参和位姿与通过光流和点跟踪获得的对应关系之间的均方误差来求解相机位姿、内参和稠密深度。FlowMap 采用可微分的深度、内参和位姿重新参数化，使其适合一阶优化。此外，它利用点迹鼓励长期几何一致性。</p><p>（4）：方法性能：FlowMap 在 360 度轨迹上使用高斯溅射进行逼真的新视角合成，其相机参数和稠密深度明显优于先前的梯度下降 SfM 方法。它甚至与最先进的 SfM 方法 COLMAP 在新视角合成任务上的性能相当，尽管 FlowMap 是纯梯度下降的、完全可微分的，并且与传统的 SfM 方法完全不同。</p><ol><li>方法：</li></ol><p>（1）：本文提出 FlowMap，一种端到端的可微方法，通过最小化光流诱导的深度、内参和位姿与通过光流和点跟踪获得的对应关系之间的均方误差来求解相机位姿、内参和稠密深度。</p><p>（2）：FlowMap 采用可微分的深度、内参和位姿重新参数化，使其适合一阶优化。</p><p>（3）：此外，它利用点迹鼓励长期几何一致性。</p><ol><li>结论：</li></ol><p>（1）：本文提出 FlowMap，一种简单、鲁棒且可扩展的一阶方法，用于从视频中估计相机参数。我们的模型优于现有的基于梯度下降的相机参数估计方法。FlowMap 的深度和相机参数通过高斯溅射法进行后续重建，其质量与 COLMAP 相当。FlowMap 使用 PyTorch 编写，短序列运行时间为 3 分钟，长序列运行时间为 20 分钟，我们预计协同工程工作可以将 FlowMap 加速一个数量级。也许最令人兴奋的是，FlowMap 对每帧深度估计是完全可微分的。因此，FlowMap 可以作为新一代自监督单目深度估计器、基于深度学习的多视图几何方法以及可泛化新视图合成方法的构建模块 [7, 18, 60, 66, 69, 77]，从而解锁对未摆放视频的互联网规模数据集的训练。致谢。这项工作得到了国家科学基金会 2211259 号赠款、新加坡 DSTA 下 DST00OECI20300823（视觉的新表示和用于标签高效视觉的自监督学习）、情报高级研究项目活动 (IARPA) 通过 140D0423C0075 下的内政部/内务商业中心 (DOI/IBC)、亚马逊科学中心和 IBM 的部分支持。丰田研究院也部分支持了这项工作。此处包含的观点和结论反映了其作者的观点和结论，不代表任何其他实体。</p><p>（2）：创新点：FlowMap 提出了一种端到端的可微方法来解决相机位姿、内参和稠密深度估计问题，采用可微分的深度、内参和位姿重新参数化，并利用点迹鼓励长期几何一致性。性能：FlowMap 在相机参数和稠密深度估计方面优于现有的梯度下降 SfM 方法，其新视角合成质量与最先进的 SfM 方法 COLMAP 相当。工作量：FlowMap 是一种一阶优化方法，在 PyTorch 中实现，短序列运行时间为 3 分钟，长序列运行时间为 20 分钟，具有可扩展性和工程加速潜力。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-ed80501d2ace1f8ad37b4cb8f3411d6f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a1dea0f5ce347645c2a4c11098b0ba50.jpg" align="middle"><img src="https://pica.zhimg.com/v2-25a5764437b9221ae10ad73aa8b84fb9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-43352f81d6eb7aada886230a057402b5.jpg" align="middle"></details><h2 id="CLIP-GS-CLIP-Informed-Gaussian-Splatting-for-Real-time-and-View-consistent-3D-Semantic-Understanding"><a href="#CLIP-GS-CLIP-Informed-Gaussian-Splatting-for-Real-time-and-View-consistent-3D-Semantic-Understanding" class="headerlink" title="CLIP-GS: CLIP-Informed Gaussian Splatting for Real-time and   View-consistent 3D Semantic Understanding"></a>CLIP-GS: CLIP-Informed Gaussian Splatting for Real-time and   View-consistent 3D Semantic Understanding</h2><p><strong>Authors:Guibiao Liao, Jiankun Li, Zhenyu Bao, Xiaoqing Ye, Jingdong Wang, Qing Li, Kanglin Liu</strong></p><p>The recent 3D Gaussian Splatting (GS) exhibits high-quality and real-time synthesis of novel views in 3D scenes. Currently, it primarily focuses on geometry and appearance modeling, while lacking the semantic understanding of scenes. To bridge this gap, we present CLIP-GS, which integrates semantics from Contrastive Language-Image Pre-Training (CLIP) into Gaussian Splatting to efficiently comprehend 3D environments without annotated semantic data. In specific, rather than straightforwardly learning and rendering high-dimensional semantic features of 3D Gaussians, which significantly diminishes the efficiency, we propose a Semantic Attribute Compactness (SAC) approach. SAC exploits the inherent unified semantics within objects to learn compact yet effective semantic representations of 3D Gaussians, enabling highly efficient rendering (&gt;100 FPS). Additionally, to address the semantic ambiguity, caused by utilizing view-inconsistent 2D CLIP semantics to supervise Gaussians, we introduce a 3D Coherent Self-training (3DCS) strategy, resorting to the multi-view consistency originated from the 3D model. 3DCS imposes cross-view semantic consistency constraints by leveraging refined, self-predicted pseudo-labels derived from the trained 3D Gaussian model, thereby enhancing precise and view-consistent segmentation results. Extensive experiments demonstrate that our method remarkably outperforms existing state-of-the-art approaches, achieving improvements of 17.29% and 20.81% in mIoU metric on Replica and ScanNet datasets, respectively, while maintaining real-time rendering speed. Furthermore, our approach exhibits superior performance even with sparse input data, verifying the robustness of our method. </p><p><a href="http://arxiv.org/abs/2404.14249v1">PDF</a> <a href="https://github.com/gbliao/CLIP-GS">https://github.com/gbliao/CLIP-GS</a></p><p><strong>Summary</strong><br>CLIP-GS 将语义信息融入高斯斑点渲染中，实现了高效的 3D 场景理解，在不使用带注释的语义数据的情况下，达到了最先进的分割性能。</p><p><strong>Key Takeaways</strong></p><ul><li>CLIP-GS 将语义信息从 CLIP 集成到高斯斑点渲染中，实现对 3D 场景的语义理解。</li><li>语义属性紧凑性 (SAC) 方法学习紧凑且有效的语义表示，实现高效渲染。</li><li>3D 一致自训练 (3DCS) 策略解决由视图不一致的 2D CLIP 语义监督造成的语义歧义。</li><li>3DCS 利用经过微调的 3D 高斯模型预测的伪标签，加强跨视图语义一致性约束。</li><li>CLIP-GS 在 Replica 和 ScanNet 数据集上分别提高 mIoU 指标 17.29% 和 20.81%，超越现有最先进方法。</li><li>CLIP-GS 即使在稀疏输入数据的情况下也能表现出优异的性能，验证了其鲁棒性。</li><li>CLIP-GS 实时渲染速度快，可用于交互式 3D 场景探索和编辑。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: CLIP-GS: CLIP-引导的高斯泼溅，用于实时且视图一致的三维语义理解</p></li><li><p>Authors: Guibiao Liao, Jiankun Li, Zhenyu Bao, Xiaoqing Ye, Jingdong Wang, Qing Li, Kanglin Liu</p></li><li><p>Affiliation: 北京大学</p></li><li><p>Keywords: 3D 高斯泼溅 · 实时 · 视图一致 · 三维场景语义理解 · 三维场景重建 · 稀疏视图</p></li><li><p>Urls: https://arxiv.org/abs/2404.14249 , Github: None</p></li><li><p>Summary: </p><pre><code>            (1): 近期提出的三维高斯泼溅（3DGS）在三维场景中展现了高质量且实时的全新视图合成。目前它主要关注于几何和外观建模，而缺少对场景的语义理解。            (2): 现有方法主要有：神经辐射场（NeRFs）和三维高斯泼溅（3DGS）。前者在渲染包含几何和外观细节的新视角方面取得了显著进展，但缺乏对三维场景的全面语义理解；后者则主要关注几何和外观建模，而忽略了语义信息。            (3): 本文提出了一种名为 CLIP-GS 的新方法，该方法将来自对比语言图像预训练 (CLIP) 的语义信息整合到高斯泼溅中，从而在没有注释语义数据的情况下有效地理解三维环境。具体来说，我们提出了一种语义属性紧凑性 (SAC) 方法，该方法利用对象内固有的统一语义来学习高斯泼溅的紧凑且有效的语义表示，从而实现高效渲染（&gt;100 FPS）。此外，为了解决语义歧义问题，我们引入了一种三维连贯自训练 (3DCS) 策略，利用三维模型产生的多视图一致性。3DCS 通过利用从训练好的三维高斯模型中获得的经过优化且自我预测的伪标签来施加跨视图语义一致性约束，从而增强精确且视图一致的分割结果。            (4): 在 Replica 和 ScanNet 数据集上，该方法在 mIoU 指标上分别比现有最先进的方法提高了 17.29% 和 20.81%，同时保持了实时的渲染速度。此外，即使在输入数据稀疏的情况下，该方法也表现出优异的性能，验证了其鲁棒性。</code></pre></li><li><p>方法：</p><pre><code>            (1):语义属性紧凑性（SAC）：利用对象内固有的统一语义来学习高斯泼溅的紧凑且有效的语义表示，实现高效渲染。            (2):3D连贯自训练（3DCS）：利用三维模型产生的多视图一致性，施加跨视图语义一致性约束，增强精确且视图一致的分割结果。            (3):整体训练过程：交替优化3D高斯泼溅和语义表示，同时利用3DCS施加视图一致性约束。</code></pre></li><li><p>结论：</p></li></ol><p>（1）：本工作提出了 CLIP-GS，一种利用高斯泼溅实现三维场景实时且精确语义理解的新方法。在 CLIP-GS 中，语义属性紧凑性（SAC）将紧凑的语义信息附加到三维高斯体中以高效地表示三维语义，确保高效渲染。此外，提出的三维连贯自训练（3DCS）增强了不同视图之间的语义一致性，从而产生了准确的三维分割结果。实验结果表明，我们的方法在合成和真实场景中明显优于 SOTA 方法。此外，即使输入数据稀疏，我们的方法也表现出优异的性能，验证了我们的三维语义学习方法的鲁棒性。</p><p>（2）：创新点：提出了一种将 CLIP 语义信息整合到高斯泼溅中的方法，实现了三维场景的实时语义理解。创新性地提出了语义属性紧凑性（SAC）和三维连贯自训练（3DCS）两种技术，分别用于高效语义表示和跨视图语义一致性增强。</p><p>性能：在 Replica 和 ScanNet 数据集上，该方法在 mIoU 指标上分别比现有最先进的方法提高了 17.29% 和 20.81%，同时保持了实时的渲染速度。即使在输入数据稀疏的情况下，该方法也表现出优异的性能，验证了其鲁棒性。</p><p>工作量：该方法需要预训练 CLIP 模型和三维高斯泼溅模型，训练过程需要大量的数据和计算资源。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-d36db5fceba666ce511b0cf595bc769d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0a5ca926d7e6577c4c1a0e8076537a17.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ef11b21fc83f3602f91a29eea9ff097e.jpg" align="middle"><img src="https://pica.zhimg.com/v2-54108038b1e285d6be885cd6288e500c.jpg" align="middle"></details><h2 id="GaussianTalker-Speaker-specific-Talking-Head-Synthesis-via-3D-Gaussian-Splatting"><a href="#GaussianTalker-Speaker-specific-Talking-Head-Synthesis-via-3D-Gaussian-Splatting" class="headerlink" title="GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian   Splatting"></a>GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian   Splatting</h2><p><strong>Authors:Hongyun Yu, Zhan Qu, Qihang Yu, Jianchuan Chen, Zhonghua Jiang, Zhiwen Chen, Shengyu Zhang, Jimin Xu, Fei Wu, Chengfei Lv, Gang Yu</strong></p><p>Recent works on audio-driven talking head synthesis using Neural Radiance Fields (NeRF) have achieved impressive results. However, due to inadequate pose and expression control caused by NeRF implicit representation, these methods still have some limitations, such as unsynchronized or unnatural lip movements, and visual jitter and artifacts. In this paper, we propose GaussianTalker, a novel method for audio-driven talking head synthesis based on 3D Gaussian Splatting. With the explicit representation property of 3D Gaussians, intuitive control of the facial motion is achieved by binding Gaussians to 3D facial models. GaussianTalker consists of two modules, Speaker-specific Motion Translator and Dynamic Gaussian Renderer. Speaker-specific Motion Translator achieves accurate lip movements specific to the target speaker through universalized audio feature extraction and customized lip motion generation. Dynamic Gaussian Renderer introduces Speaker-specific BlendShapes to enhance facial detail representation via a latent pose, delivering stable and realistic rendered videos. Extensive experimental results suggest that GaussianTalker outperforms existing state-of-the-art methods in talking head synthesis, delivering precise lip synchronization and exceptional visual quality. Our method achieves rendering speeds of 130 FPS on NVIDIA RTX4090 GPU, significantly exceeding the threshold for real-time rendering performance, and can potentially be deployed on other hardware platforms. </p><p><a href="http://arxiv.org/abs/2404.14037v1">PDF</a> </p><p><strong>Summary:</strong><br>高斯体素渲染法赋予3D高斯体素显式表示特性，可实现直观的面部动作控制，大幅提升音频驱动虚拟化身合成效果。</p><p><strong>Key Takeaways:</strong></p><ul><li>基于3D高斯体素的音频驱动虚拟化身合成方法。</li><li>面部运动通过将高斯体素绑定到3D面部模型实现直观控制。</li><li>说话人特定运动翻译器实现针对特定说话人的精确唇部运动。</li><li>动态高斯渲染器引入说话人特定混合形状以增强面部细节表示。</li><li>广泛的实验结果表明，该方法在语音合成方面优于现有最先进的方法。</li><li>渲染速度达到130 FPS，远超实时渲染性能的阈值。</li><li>具有在其他硬件平台上部署的潜力。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: GaussianTalker: Speaker-specific Talking Head Synthesis (基于 3D 高斯点云的说话人特定说话头合成)</p></li><li><p>Authors: Hongyun Yu, Zhan Qu, Qihang Yu, Jianchuan Chen, Zhonghua Jiang, Zhiwen Chen, Shengyu Zhang, Jimin Xu, Fei Wu, Chengfei Lv, Gang Yu</p></li><li><p>Affiliation: Alibaba Group (阿里巴巴集团)</p></li><li><p>Keywords: Audio-driven talking head synthesis, 3D Gaussian Splatting, Speaker-specific motion, Dynamic Gaussian Renderer</p></li><li><p>Urls: Paper: https://arxiv.org/pdf/2404.14037.pdf, Github: None</p></li><li><p>Summary:</p><p>(1): Recent audio-driven talking head synthesis methods based on Neural Radiance Fields (NeRF) have achieved impressive results, but suffer from inadequate pose and expression control due to NeRF's implicit representation, leading to unsynchronized or unnatural lip movements and visual jitter and artifacts.</p><p>(2): Past methods: NeRF-based audio-driven talking head synthesis methods. Problems: Inadequate pose and expression control, resulting in unsynchronized or unnatural lip movements and visual jitter and artifacts. Well motivated: Yes, as it addresses the limitations of existing methods.</p><p>(3): GaussianTalker: A novel method for audio-driven talking head synthesis based on 3D Gaussian Splatting. It consists of two modules: Speaker-specific Motion Translator and Dynamic Gaussian Renderer. The former achieves accurate lip movements specific to the target speaker through universalized audio feature extraction and customized lip motion generation. The latter introduces Speaker-specific BlendShapes to control the facial expressions and generates high-quality talking head videos with precise lip movements.</p><p>(4): On the task of audio-driven talking head synthesis, GaussianTalker achieves state-of-the-art results. It can generate high-quality talking head videos with precise lip movements and natural facial expressions. The performance supports the goals of the paper, which are to address the limitations of existing methods and achieve more realistic and expressive talking head synthesis.</p></li><li><p>方法：</p><p>(1)：基于3D高斯点云的说话人特定说话头合成；</p><p>(2)：提出Speaker-specific Motion Translator和Dynamic Gaussian Renderer两个模块；</p><p>(3)：Speaker-specific Motion Translator通过通用音频特征提取和定制唇部动作生成实现特定于目标说话人的准确唇部动作；</p><p>(4)：Dynamic Gaussian Renderer引入说话人特定BlendShapes来控制面部表情，并生成具有精确唇部动作的高质量说话头视频；</p><p>.......</p></li></ol><p><strong>结论</strong></p><p>（1）：本工作提出 GaussianTalker，一种通过 3D 高斯点云集成 FLAME 模型的音频驱动说话头合成新框架。GaussianTalker 将多模态数据与特定说话者关联，减少了音频、3D 网格和视频之间的潜在身份偏差。Speaker-specific FLAME Translator 采用身份解耦和个性化嵌入来实现同步和自然的唇部动作，而 Dynamic Gaussian Renderer 通过潜在姿势细化高斯属性，以实现稳定和逼真的渲染。大量实验表明，GaussianTalker 在说话头合成方面优于最先进的性能，同时实现了超高的渲染速度，明显超过其他方法。我们相信，这种创新方法将鼓励未来的研究开发更流畅、更逼真的角色表情和动作。通过利用先进的高斯模型和生成技术，角色动画将远远超出简单的唇形同步，捕捉更广泛的角色动态。</p><p>（2）：创新点：基于 3D 高斯点云的说话人特定说话头合成；Speaker-specific Motion Translator 和 Dynamic Gaussian Renderer 两个模块；性能：在说话头合成方面优于最先进的性能，实现了超高的渲染速度；工作量：.......</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-f53af9ef57ed25d0699b508f7b856061.jpg" align="middle"><img src="https://pica.zhimg.com/v2-00e62c0d66ff2641b9803987918d6fd0.jpg" align="middle"></details><h2 id="Dynamic-Gaussians-Mesh-Consistent-Mesh-Reconstruction-from-Monocular-Videos"><a href="#Dynamic-Gaussians-Mesh-Consistent-Mesh-Reconstruction-from-Monocular-Videos" class="headerlink" title="Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular   Videos"></a>Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular   Videos</h2><p><strong>Authors:Isabella Liu, Hao Su, Xiaolong Wang</strong></p><p>Modern 3D engines and graphics pipelines require mesh as a memory-efficient representation, which allows efficient rendering, geometry processing, texture editing, and many other downstream operations. However, it is still highly difficult to obtain high-quality mesh in terms of structure and detail from monocular visual observations. The problem becomes even more challenging for dynamic scenes and objects. To this end, we introduce Dynamic Gaussians Mesh (DG-Mesh), a framework to reconstruct a high-fidelity and time-consistent mesh given a single monocular video. Our work leverages the recent advancement in 3D Gaussian Splatting to construct the mesh sequence with temporal consistency from a video. Building on top of this representation, DG-Mesh recovers high-quality meshes from the Gaussian points and can track the mesh vertices over time, which enables applications such as texture editing on dynamic objects. We introduce the Gaussian-Mesh Anchoring, which encourages evenly distributed Gaussians, resulting better mesh reconstruction through mesh-guided densification and pruning on the deformed Gaussians. By applying cycle-consistent deformation between the canonical and the deformed space, we can project the anchored Gaussian back to the canonical space and optimize Gaussians across all time frames. During the evaluation on different datasets, DG-Mesh provides significantly better mesh reconstruction and rendering than baselines. Project page: <a href="https://www.liuisabella.com/DG-Mesh/">https://www.liuisabella.com/DG-Mesh/</a> </p><p><a href="http://arxiv.org/abs/2404.12379v2">PDF</a> Project page: <a href="https://www.liuisabella.com/DG-Mesh/">https://www.liuisabella.com/DG-Mesh/</a></p><p><strong>Summary</strong><br>单目视频重建高保真动态网格的框架</p><p><strong>Key Takeaways</strong></p><ul><li>动态高斯网格（DG-Mesh）通过单目视频重建出高保真时序一致的网格。</li><li>利用 3D 高斯点云构成具有时序一致性的网格序列。</li><li>高斯点云恢复高质量网格，并实现网格顶点的时序跟踪，可用于动态对象纹理编辑。</li><li>高斯网格锚定可使高斯分布均匀，通过网格引导变形高斯的高密度化和剪枝，提升网格重建质量。</li><li>通过规范空间和变形空间的循环一致性变形，将锚定的高斯投射回规范空间，并在所有时间帧优化高斯。</li><li>在不同数据集上评估后，DG-Mesh 在网格重建和渲染方面显著优于基准方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: 动态高斯网格：单目视频中一致的网格重建</p></li><li><p>Authors: Isabella Liu, Hao Su, Xiaolong Wang</p></li><li><p>Affiliation: 加州大学圣地亚哥分校</p></li><li><p>Keywords: 3D Reconstruction, Monocular Video, Dynamic Mesh, Gaussian Splatting</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2404.12379, Github: None</p></li><li><p>Summary:</p><p>(1): 现代 3D 引擎和图形管道需要网格作为一种内存高效的表示，它允许高效渲染、几何处理、纹理编辑和许多其他下游操作。然而，从单目视觉观察中获得结构和细节方面的高质量网格仍然非常困难。对于动态场景和对象，这个问题变得更具挑战性。</p><p>(2): 过去的方法包括使用 3D 高斯斑点构建网格序列，但这些方法在处理动态场景时存在局限性。</p><p>(3): 本文提出了一种名为动态高斯网格（DG-Mesh）的框架，该框架利用 3D 高斯斑点表示，并通过高斯网格锚定算法恢复高质量网格，从而实现时间一致的网格序列重建。</p><p>(4): 在 D-NeRF 数据集和 DG-Mesh 数据集上，该方法在网格重建任务上取得了优异的性能，证明了其有效性。</p></li><li><p>方法：</p><p>（1）：提出了一种名为动态高斯网格（DG-Mesh）的框架，该框架利用 3D 高斯斑点表示，并通过高斯网格锚定算法恢复高质量网格，从而实现时间一致的网格序列重建；</p><p>（2）：该框架包括一个网格构建模块，该模块使用 3D 高斯斑点表示来构建网格序列，以及一个网格锚定模块，该模块将网格序列中的网格锚定到世界坐标系中；</p><p>（3）：网格构建模块利用 3D 高斯斑点表示来表示场景中的几何形状，并使用高斯混合模型（GMM）来估计网格序列中的网格；</p><p>（4）：网格锚定模块利用高斯网格锚定算法将网格序列中的网格锚定到世界坐标系中，该算法使用高斯斑点之间的几何关系来估计网格的位姿；</p><p>（5）：该框架通过迭代优化网格构建模块和网格锚定模块中的参数来实现时间一致的网格序列重建。</p></li><li><p>结论：</p></li></ol><p>（1）：本文提出了一种名为动态高斯网格（DG-Mesh）的框架，该框架利用 3D 高斯斑点表示，并通过高斯网格锚定算法恢复高质量网格，从而实现时间一致的网格序列重建。DG-Mesh 在网格重建任务上取得了优异的性能，证明了其有效性。</p><p>（2）：创新点：利用 3D 高斯斑点表示和高斯网格锚定算法实现时间一致的网格序列重建；性能：在网格重建任务上取得了优异的性能；工作量：需要准确的对象分割和跟踪，在处理大拓扑变化时存在挑战。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-bb786e92e4a68c16900a6443568566f8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-84e4142556dfc3bb4d97a20772986995.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2de77e2437a64b7bd107f95e76669404.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS 方向最新论文已更新，请持续关注 Update in 2024-04-25  FlowMap High-Quality Camera Poses, Intrinsics, and Depth via Gradient   Descent</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/04/25/Paper/2024-04-25/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/04/25/Paper/2024-04-25/Talking%20Head%20Generation/</id>
    <published>2024-04-25T13:12:59.000Z</published>
    <updated>2024-04-25T13:12:59.111Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-25-更新"><a href="#2024-04-25-更新" class="headerlink" title="2024-04-25 更新"></a>2024-04-25 更新</h1><h2 id="GaussianTalker-Speaker-specific-Talking-Head-Synthesis-via-3D-Gaussian-Splatting"><a href="#GaussianTalker-Speaker-specific-Talking-Head-Synthesis-via-3D-Gaussian-Splatting" class="headerlink" title="GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian   Splatting"></a>GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian   Splatting</h2><p><strong>Authors:Hongyun Yu, Zhan Qu, Qihang Yu, Jianchuan Chen, Zhonghua Jiang, Zhiwen Chen, Shengyu Zhang, Jimin Xu, Fei Wu, Chengfei Lv, Gang Yu</strong></p><p>Recent works on audio-driven talking head synthesis using Neural Radiance Fields (NeRF) have achieved impressive results. However, due to inadequate pose and expression control caused by NeRF implicit representation, these methods still have some limitations, such as unsynchronized or unnatural lip movements, and visual jitter and artifacts. In this paper, we propose GaussianTalker, a novel method for audio-driven talking head synthesis based on 3D Gaussian Splatting. With the explicit representation property of 3D Gaussians, intuitive control of the facial motion is achieved by binding Gaussians to 3D facial models. GaussianTalker consists of two modules, Speaker-specific Motion Translator and Dynamic Gaussian Renderer. Speaker-specific Motion Translator achieves accurate lip movements specific to the target speaker through universalized audio feature extraction and customized lip motion generation. Dynamic Gaussian Renderer introduces Speaker-specific BlendShapes to enhance facial detail representation via a latent pose, delivering stable and realistic rendered videos. Extensive experimental results suggest that GaussianTalker outperforms existing state-of-the-art methods in talking head synthesis, delivering precise lip synchronization and exceptional visual quality. Our method achieves rendering speeds of 130 FPS on NVIDIA RTX4090 GPU, significantly exceeding the threshold for real-time rendering performance, and can potentially be deployed on other hardware platforms. </p><p><a href="http://arxiv.org/abs/2404.14037v1">PDF</a> </p><p><strong>Summary</strong><br>高斯散点绘制助力音频驱动说话人头部生成，精确控制面部动作，实现自然流畅的唇部运动和逼真的视觉效果。</p><p><strong>Key Takeaways</strong></p><ul><li>基于 3D 高斯散点绘制，显式表示面部动作，实现对脸部运动的直观控制。</li><li>说话人特定运动转换器，通过通用音频特征提取和定制唇部运动生成，实现准确的唇部运动。</li><li>动态高斯渲染器引入说话人特定混合形状，通过潜在位姿增强面部细节表示，提供稳定逼真的渲染视频。</li><li>实验结果表明，该方法在说话人头部生成方面优于现有技术，提供精确的唇部同步和出色的视觉质量。</li><li>渲染速度达 130 FPS，远超实时渲染性能阈值，可部署于其他硬件平台。</li><li>旨在解决音频驱动说话人头部合成中姿态和表情控制不足的问题。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: GaussianTalker：基于3D高斯斑点的说话人特定会说话的头部合成</p></li><li><p>Authors: Hongyun Yu, Zhan Qu, Qihang Yu, Jianchuan Chen, Zhonghua Jiang, Zhiwen Chen, Shengyu Zhang, Jimin Xu, Fei Wu, Chengfei Lv, Gang Yu</p></li><li><p>Affiliation: 阿里巴巴集团</p></li><li><p>Keywords: Audio-driven talking head synthesis, 3D Gaussian Splatting, Lip motion, Facial animation</p></li><li><p>Urls: Paper: , Github:None</p></li><li><p>Summary:</p><pre><code>           (1): 近期基于神经辐射场（NeRF）的音频驱动说话头部合成工作取得了令人瞩目的成果。然而，由于NeRF隐式表示导致的姿势和表情控制不足，这些方法仍然存在一些局限性，如唇部动作不同步或不自然，以及视觉抖动和伪影。           (2): 过去的方法：基于NeRF的音频驱动说话头部合成方法。问题：姿势和表情控制不足，导致唇部动作不同步或不自然，以及视觉抖动和伪影。动机：通过显式表示面部运动，实现对唇部动作的直观控制。           (3): 本文提出的研究方法：GaussianTalker，一种基于3D高斯斑点的音频驱动说话头部合成新方法。GaussianTalker由两个模块组成：说话人特定运动转换器和动态高斯渲染器。说话人特定运动转换器通过通用音频特征提取和定制唇部动作生成，实现针对目标说话人的准确唇部动作。动态高斯渲染器引入说话人特定混合形状，以控制面部表情。           (4): 本文方法在任务和性能上的表现：在TalkingHead数据集上进行评估，GaussianTalker在唇部同步、视觉保真度和鲁棒性方面均优于现有方法。这些性能支持了本文的目标：实现准确、逼真且鲁棒的音频驱动说话头部合成。</code></pre></li><li><p>方法：</p><pre><code>           (1): 提出GaussianTalker，一种基于3D高斯斑点的音频驱动说话头部合成新方法；           (2): GaussianTalker由两个模块组成：说话人特定运动转换器和动态高斯渲染器；           (3): 说话人特定运动转换器通过通用音频特征提取和定制唇部动作生成，实现针对目标说话人的准确唇部动作；           (4): 动态高斯渲染器引入说话人特定混合形状，以控制面部表情；           (5): 采用FLAME模型作为面部动画和渲染之间的桥梁；           (6): 训练说话人特定运动转换器，使用多语言、多个人数据集，以提高其对不同音频输入的适应性；           (7): 使用通用音频编码器分离身份信息和内容信息，使用定制运动解码器集成个性化特征；           (8): 引入基于自我监督的唇读约束机制，进一步优化唇部动作的同步性；           (9): 动态高斯渲染器由FLAME驱动3D高斯斑点，实时渲染动态说话头部；           (10): 使用自适应密度控制，根据局部运动幅度动态调整高斯斑点的密度；           (11): 采用图像修复生成器，处理渲染过程中的空洞区域和伪影；           (12): 在TalkingHead数据集上进行评估，GaussianTalker在唇部同步、视觉保真度和鲁棒性方面均优于现有方法。</code></pre></li><li><p>结论：</p><pre><code>           (1):本文提出GaussianTalker，一种基于3D高斯斑点的音频驱动说话头部合成新方法。该方法将多模态数据与特定说话人关联，减少了音频、3D网格和视频之间的潜在身份偏差。说话人特定FLAME转换器采用身份解耦和个性化嵌入来实现同步且自然的唇部动作，而动态高斯渲染器通过潜在姿势优化高斯属性，以实现稳定且逼真的渲染。大量实验表明，GaussianTalker在说话头部合成方面优于最先进的性能，同时实现了超高的渲染速度，显著超越了其他方法。我们相信这种创新方法将鼓励未来的研究开发更加流畅逼真的角色表情和动作。通过利用先进的高斯模型和生成技术，角色的动画将远远超出简单的唇部同步，捕捉更广泛的角色动态。                             (2):创新点：基于3D高斯斑点的音频驱动说话头部合成；           性能：在唇部同步、视觉保真度和鲁棒性方面优于现有方法；           工作量：与其他方法相比，渲染速度超高。</code></pre></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-f53af9ef57ed25d0699b508f7b856061.jpg" align="middle"><img src="https://picx.zhimg.com/v2-00e62c0d66ff2641b9803987918d6fd0.jpg" align="middle"></details><h2 id="Deepfake-Generation-and-Detection-A-Benchmark-and-Survey"><a href="#Deepfake-Generation-and-Detection-A-Benchmark-and-Survey" class="headerlink" title="Deepfake Generation and Detection: A Benchmark and Survey"></a>Deepfake Generation and Detection: A Benchmark and Survey</h2><p><strong>Authors:Gan Pei, Jiangning Zhang, Menghan Hu, Zhenyu Zhang, Chengjie Wang, Yunsheng Wu, Guangtao Zhai, Jian Yang, Chunhua Shen, Dacheng Tao</strong></p><p>Deepfake is a technology dedicated to creating highly realistic facial images and videos under specific conditions, which has significant application potential in fields such as entertainment, movie production, digital human creation, to name a few. With the advancements in deep learning, techniques primarily represented by Variational Autoencoders and Generative Adversarial Networks have achieved impressive generation results. More recently, the emergence of diffusion models with powerful generation capabilities has sparked a renewed wave of research. In addition to deepfake generation, corresponding detection technologies continuously evolve to regulate the potential misuse of deepfakes, such as for privacy invasion and phishing attacks. This survey comprehensively reviews the latest developments in deepfake generation and detection, summarizing and analyzing current state-of-the-arts in this rapidly evolving field. We first unify task definitions, comprehensively introduce datasets and metrics, and discuss developing technologies. Then, we discuss the development of several related sub-fields and focus on researching four representative deepfake fields: face swapping, face reenactment, talking face generation, and facial attribute editing, as well as forgery detection. Subsequently, we comprehensively benchmark representative methods on popular datasets for each field, fully evaluating the latest and influential published works. Finally, we analyze challenges and future research directions of the discussed fields. </p><p><a href="http://arxiv.org/abs/2403.17881v3">PDF</a> We closely follow the latest developments in   <a href="https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection">https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection</a></p><p><strong>Summary</strong><br>深度伪造是一项在特定条件下创建高度逼真的面部图像和视频的技术，在娱乐、电影制作、数字人创作等领域具有广阔的应用前景。</p><p><strong>Key Takeaways</strong></p><ul><li>深度伪造生成主要采用变分自动编码器和生成对抗网络等深度学习技术。</li><li>扩散模型的出现引发了新一轮的深度伪造生成研究热潮。</li><li>深度伪造检测技术与生成技术同步发展，以防范深度伪造技术的滥用。</li><li>本综述对深度伪造生成与检测的最新进展进行了全面回顾和分析。</li><li>深度伪造生成中的面部替换、面部重现、说话脸生成和面部属性编辑等领域的研究备受关注。</li><li>本综述对每一领域中具有代表性的方法进行了全面评估，充分展示了最新且有影响力的已发表工作。</li><li>本综述分析了相关领域的挑战和未来研究方向。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: 深度伪造生成与检测：基准与综述</p></li><li><p>Authors: Gan Pei, Jiangning Zhang, Menghan Hu, Zhenyu Zhang, Chengjie Wang, Yunsheng Wu, Guangtao Zhai, Jian Yang, Chunhua Shen, Dacheng Tao</p></li><li><p>Affiliation: 东华大学多维信息处理上海市重点实验室</p></li><li><p>Keywords: Deepfake Generation, Face Swapping, Face Reenactment, Talking Face Generation, Facial Attribute Editing, Forgery detection, Survey</p></li><li><p>Urls: https://arxiv.org/abs/2403.17881v3 , Github:None</p></li><li><p>Summary:</p></li></ol><p>(1):深度伪造技术近年来得到广泛关注，其应用前景广阔，但其潜在的伦理风险也引发了人们的担忧。</p><p>(2):传统的深度伪造生成方法主要基于变分自编码器和生成对抗网络，但其生成效果仍不令人满意。近年来，扩散模型的出现极大地提升了图像/视频的生成能力。</p><p>(3):本文对深度伪造生成和检测的最新进展进行了全面综述，统一了任务定义，全面介绍了数据集和度量标准，并讨论了发展技术。</p><p>(4):本文对人脸替换、人脸重现、说话人脸生成和面部属性编辑等四个代表性深度伪造领域进行了研究，重点分析了各领域的发展历程，并在流行数据集上对代表性方法进行了全面基准测试，充分评估了最新和有影响力的已发表作品。</p><ol><li>方法：</li></ol><p>（1）：本文首先统一了深度伪造生成和检测的任务定义，全面介绍了数据集和度量标准，为后续研究提供了基础。</p><p>（2）：本文对人脸替换、人脸重现、说话人脸生成和面部属性编辑四个代表性深度伪造领域进行了研究，重点分析了各领域的发展历程。</p><p>（3）：本文在流行数据集上对代表性方法进行了全面基准测试，充分评估了最新和有影响力的已发表作品，为研究人员提供了有价值的参考。</p><ol><li>结论：<pre><code>            （1）：本文全面综述了深度伪造生成与检测领域的研究进展，统一了任务定义，全面介绍了数据集和度量标准，并讨论了发展技术，为后续研究提供了基础。            （2）：创新点：本文首次全面覆盖了深度伪造生成与检测领域，并讨论了最新的技术，如扩散模型；本文对人脸替换、人脸重现、说话人脸生成和面部属性编辑四个代表性深度伪造领域进行了研究，重点分析了各领域的发展历程，并在流行数据集上对代表性方法进行了全面基准测试，充分评估了最新和有影响力的已发表作品；本文对深度伪造生成与检测领域的挑战和未来研究方向进行了总结。性能：本文在人脸替换、人脸重现、说话人脸生成和面部属性编辑四个代表性深度伪造领域进行了全面基准测试，充分评估了最新和有影响力的已发表作品，为研究人员提供了有价值的参考；本文对深度伪造生成与检测领域的挑战和未来研究方向进行了总结。工作量：本文对深度伪造生成与检测领域进行了全面综述，统一了任务定义，全面介绍了数据集和度量标准，并讨论了发展技术，为后续研究提供了基础；本文对人脸替换、人脸重现、说话人脸生成和面部属性编辑四个代表性深度伪造领域进行了研究，重点分析了各领域的发展历程，并在流行数据集上对代表性方法进行了全面基准测试，充分评估了最新和有影响力的已发表作品；本文对深度伪造生成与检测领域的挑战和未来研究方向进行了总结。</code></pre></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-6a08950b6be4e3f18aeef87726b535fd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c245a3a60e574c0cb0324f79ffd23876.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-afbe757ef2a542a37ce568036b591797.jpg" align="middle"><img src="https://picx.zhimg.com/v2-becdaa251ccb21b3a85f051bf593814f.jpg" align="middle"><img src="https://pica.zhimg.com/v2-c84c542a045ab258f8a251f6f24a1446.jpg" align="middle"><img src="https://pica.zhimg.com/v2-48abe21b928d9c991400ddc443d9eec3.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-04-25  GaussianTalker Speaker-specific Talking Head Synthesis via 3D Gaussian   Splatting</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/04/25/Paper/2024-04-25/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/04/25/Paper/2024-04-25/Diffusion%20Models/</id>
    <published>2024-04-25T12:59:05.000Z</published>
    <updated>2024-04-25T12:59:05.416Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-25-更新"><a href="#2024-04-25-更新" class="headerlink" title="2024-04-25 更新"></a>2024-04-25 更新</h1><h2 id="ID-Aligner-Enhancing-Identity-Preserving-Text-to-Image-Generation-with-Reward-Feedback-Learning"><a href="#ID-Aligner-Enhancing-Identity-Preserving-Text-to-Image-Generation-with-Reward-Feedback-Learning" class="headerlink" title="ID-Aligner: Enhancing Identity-Preserving Text-to-Image Generation with   Reward Feedback Learning"></a>ID-Aligner: Enhancing Identity-Preserving Text-to-Image Generation with   Reward Feedback Learning</h2><p><strong>Authors:Weifeng Chen, Jiacheng Zhang, Jie Wu, Hefeng Wu, Xuefeng Xiao, Liang Lin</strong></p><p>The rapid development of diffusion models has triggered diverse applications. Identity-preserving text-to-image generation (ID-T2I) particularly has received significant attention due to its wide range of application scenarios like AI portrait and advertising. While existing ID-T2I methods have demonstrated impressive results, several key challenges remain: (1) It is hard to maintain the identity characteristics of reference portraits accurately, (2) The generated images lack aesthetic appeal especially while enforcing identity retention, and (3) There is a limitation that cannot be compatible with LoRA-based and Adapter-based methods simultaneously. To address these issues, we present \textbf{ID-Aligner}, a general feedback learning framework to enhance ID-T2I performance. To resolve identity features lost, we introduce identity consistency reward fine-tuning to utilize the feedback from face detection and recognition models to improve generated identity preservation. Furthermore, we propose identity aesthetic reward fine-tuning leveraging rewards from human-annotated preference data and automatically constructed feedback on character structure generation to provide aesthetic tuning signals. Thanks to its universal feedback fine-tuning framework, our method can be readily applied to both LoRA and Adapter models, achieving consistent performance gains. Extensive experiments on SD1.5 and SDXL diffusion models validate the effectiveness of our approach. \textbf{Project Page: \url{<a href="https://idaligner.github.io/}}">https://idaligner.github.io/}}</a> </p><p><a href="http://arxiv.org/abs/2404.15449v1">PDF</a> </p><p><strong>Summary</strong><br>扩散模型带来的文本图像生成在身份保持人像和商用图片上广泛应用，ID-Aligner框架通过反馈学习增强图像美感。</p><p><strong>Key Takeaways</strong></p><ul><li>身份保持图像生成方法在身份特征保持、美观性保证、兼容性方面有提升空间。</li><li>ID-Aligner框架通过反馈学习来增强ID-T2I效果。</li><li>身份一致性奖励微调利用面部检测和识别模型的反馈，提高生成的图像的身份保留能力。</li><li>身份美学奖励微调利用人工标注偏好数据和自动构建的字符结构生成反馈，提供美学调整信号。</li><li>得益于通用的反馈微调框架，该方法可以方便地应用于LoRA和适配器模型，实现性能提升。</li><li>在SD1.5和SDXL扩散模型上的广泛实验验证了该方法的有效性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: ID-Aligner: 增强身份保留文本到图像生成</p></li><li><p>Authors: Weifeng Chen, Jiachang Zhang, Jie Wu, Hefeng Wu, Xuefeng Xiao, Liang Lin</p></li><li><p>Affiliation: 中山大学</p></li><li><p>Keywords: Identity-preserving text-to-image generation, Diffusion models, Feedback learning, LoRA, Adapter</p></li><li><p>Urls: https://arxiv.org/abs/2404.15449 , Github:None</p></li><li><p>Summary:</p><p>(1): 随着扩散模型的快速发展，文本到图像生成技术得到了广泛的应用，其中身份保留文本到图像生成（ID-T2I）因其在AI人像、广告等领域的应用前景而备受关注。然而，现有的ID-T2I方法仍面临着一些关键挑战：难以准确保持参考人像的身份特征、生成的图像缺乏美感，以及无法同时兼容基于LoRA和基于Adapter的方法。</p><p>(2): 现有的ID-T2I方法主要通过在扩散模型中加入身份编码信息来实现身份保留，但这些方法往往会丢失参考人像的细致特征，导致生成的图像与参考人像存在差异。此外，这些方法在增强身份保留的同时，往往会降低图像的视觉吸引力。</p><p>(3): 针对上述问题，本文提出了一种基于反馈学习的通用框架ID-Aligner，用于增强ID-T2I性能。ID-Aligner通过引入身份一致性奖励和身份美学奖励，分别增强了生成的图像的身份保留性和视觉吸引力。此外，ID-Aligner可以同时应用于基于LoRA和基于Adapter的方法，具有较好的兼容性。</p><p>(4): 在人像生成任务上，ID-Aligner在保持身份特征和生成高质量图像方面都取得了优异的性能。实验结果表明，ID-Aligner生成的图像在身份保留度、图像质量和用户偏好方面均优于现有的ID-T2I方法，验证了其有效性。</p></li><li><p>方法：</p><p>(1):提出ID-Aligner，一种利用反馈学习方法来增强身份（ID）保留生成性能的开创性方法。方法的概述见图2。我们通过奖励反馈学习范式解决了ID保留生成，以增强与参考人像图像和生成图像的美感的一致性。</p><p>(2):文本到图像扩散模型利用扩散建模根据文本提示通过扩散模型生成高质量图像，该模型通过渐进的去噪过程从高斯噪声生成所需的数据样本。在预训练期间，首先通过预训练的VAE [4, 10]编码器处理采样的图像𝑥，以导出其潜在表示𝑧。随后，通过前向扩散过程将随机噪声注入潜在表示，遵循预定义的时间表{𝛽𝑡 }𝑇。这个过程可以表述为𝑧𝑡 = √𝛼𝑡𝑧 + √1 − 𝛼𝑡𝜖，其中𝜖 ∈ N (0, 1)是与𝑧维度相同的随机噪声，𝛼𝑡 = �𝑡 𝑠=1 𝛼𝑠和𝛼𝑡 = 1 − 𝛽𝑡。为了实现去噪过程，训练了一个UNet 𝜖𝜃来预测前向扩散过程中的添加噪声，条件是噪声潜在和文本提示𝑐。形式上，UNet的优化目标是：L(𝜃) = E𝑧,𝜖,𝑐,𝑡 [||𝜖 − 𝜖𝜃 ( √︁ 𝛼𝑡𝑧 + √︁ 1 − 𝛼𝑡𝜖,𝑐,𝑡)||2 2]。</p><p>(3):身份奖励：身份一致性奖励：给定参考图像𝑥ref 0和生成图像𝑥′ 0。我们的目标是评估特定肖像的ID相似性。为了实现这一点，我们首先使用人脸检测模型FaceDet来定位两幅图像中的人脸。基于人脸检测模型的输出，我们裁剪相应的人脸区域并将其输入人脸识别模型FaceEnc的编码器。这使我们能够获得参考人脸Eref和生成人脸Egen的编码人脸嵌入，即Eref = FaceEnc(FaceDet(𝑥ref 0 )), Egen = FaceEnc(FaceDet(𝑥′ 0))。随后，我们计算这两个面部嵌入之间的余弦相似度，作为生成过程中ID保留的度量。然后，我们将此相似度作为反馈调整过程的奖励信号，如下所示：ℜ𝑖𝑑_𝑠𝑖𝑚(𝑥′ 0,𝑥ref 0 ) = cose_sim(Egen, Eref)。身份美学奖励：除了身份一致性奖励外，我们还引入了一个专注于吸引力和质量的身份美学奖励模型。它包括人类对吸引力的偏好和合理的结构。首先，我们使用自收集的人类注释偏好数据集训练一个奖励模型，该模型可以对图像进行评分并反映人类对吸引力的偏好，如图3右所示。我们采用ImageReward [37]提供的预训练模型，并使用以下损失对其进行微调：L𝜃 = −𝐸(𝑐,𝑥𝑖,𝑥𝑗 )∼D [𝑙𝑜𝑔(𝜎(𝑓𝜃 (𝑥𝑖,𝑐) − 𝑓𝜃 (𝑥𝑗,𝑐)))].此损失函数基于图像之间的比较对，其中每个比较对包含两幅图像（𝑥𝑖和𝑥𝑗）和提示𝑐。𝑓𝜃 (𝑥,𝑐)表示给定图像𝑥和提示𝑐的奖励分数。因此，我们将𝑓𝜃称为ℜ𝑎𝑝𝑝𝑒𝑎𝑙作为吸引力奖励。此外，我们设计了一个结构奖励模型，可以区分扭曲的肢体/身体和自然的肢体/身体。为了训练一个可以访问图像结构是否合理性的模型，我们收集了一组包含正面和负面样本的文本图像对。具体来说，我们使用经过人类检测器过滤的LAION [28]中的图像。然后我们使用姿势估计模型生成姿势，这可以被视为未扭曲的人体结构。然后，我们随机扭曲姿势并利用ControlNet [42]生成失真体作为负样本，如图3左侧所示。一旦正负对可用，同样，我们使用与方程式5相同的损失训练结构奖励模型，并将结构奖励模型称为ℜ𝑠𝑡𝑟𝑢𝑐𝑡。然后，身份美学奖励模型定义为ℜ𝑖𝑑_𝑎𝑒𝑠 (𝑥,𝑐) = ℜ𝑎𝑝𝑝𝑒𝑎𝑙 (𝑥,𝑐) + ℜ𝑠𝑡𝑟𝑢𝑐𝑡 (𝑥,𝑐)。</p><p>(4):ID保留反馈学习：在反馈学习阶段，我们从输入提示𝑐开始，随机初始化潜在变量𝑥𝑇。然后对潜在变量进行渐进去噪，直到达到随机选择的时间步𝑡。此时，去噪图像𝑥′ 0直接从𝑥𝑡预测。将从先前阶段获得的奖励模型应用于此去噪图像，生成预期的偏好分数。此偏好分数使扩散模型能够进行微调，以更紧密地与反映身份一致性和审美偏好的ID奖励保持一致：L𝑖𝑑_𝑠𝑖𝑚 = E𝑐∼𝑝 (𝑐)E𝑥′ 0∼𝑝 (𝑥′ 0|𝑐) [1 − ℜ𝑖𝑑_𝑠𝑖𝑚(𝑥′ 0,𝑥𝑟𝑒𝑓 0 )], L𝑖𝑑_𝑎𝑒𝑠 = E𝑐∼𝑝 (𝑐)E𝑥′ 0∼𝑝 (𝑥′ 0|𝑐) [−ℜ𝑖𝑑_𝑎𝑒𝑠 (𝑥′ 0,𝑐)]。</p></li><li><p>结论：</p></li></ol><p>（1）：本文提出了一种基于反馈学习的通用框架ID-Aligner，用于增强身份保留文本到图像生成（ID-T2I）性能。ID-Aligner通过引入身份一致性奖励和身份美学奖励，分别增强了生成的图像的身份保留性和视觉吸引力。此外，ID-Aligner可以同时应用于基于LoRA和基于Adapter的方法，具有较好的兼容性。在人像生成任务上，ID-Aligner在保持身份特征和生成高质量图像方面都取得了优异的性能。实验结果表明，ID-Aligner生成的图像在身份保留度、图像质量和用户偏好方面均优于现有的ID-T2I方法，验证了其有效性。</p><p>（2）：创新点：提出了ID-Aligner，一种利用反馈学习方法来增强ID保留生成性能的开创性方法。引入身份一致性奖励和身份美学奖励，分别增强了生成的图像的身份保留性和视觉吸引力。此外，ID-Aligner可以同时应用于基于LoRA和基于Adapter的方法，具有较好的兼容性。</p><p>性能：在人像生成任务上，ID-Aligner在保持身份特征和生成高质量图像方面都取得了优异的性能。实验结果表明，ID-Aligner生成的图像在身份保留度、图像质量和用户偏好方面均优于现有的ID-T2I方法，验证了其有效性。</p><p>工作量：本文的方法涉及到反馈学习、身份一致性奖励和身份美学奖励的引入，需要额外的计算和数据处理。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-952ad01319e9ee57febc82370c97b6b2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3ea9ae35ff1eb818db6fe2da58e7a072.jpg" align="middle"><img src="https://pica.zhimg.com/v2-3ca1d77296d47d3befa8898dae8433d7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b249a085ea084ca24b82dc1fcadcc875.jpg" align="middle"></details>## Perturbing Attention Gives You More Bang for the Buck: Subtle Imaging   Perturbations That Efficiently Fool Customized Diffusion Models**Authors:Jingyao Xu, Yuetong Lu, Yandong Li, Siyang Lu, Dongdong Wang, Xiang Wei**Diffusion models (DMs) embark a new era of generative modeling and offer more opportunities for efficient generating high-quality and realistic data samples. However, their widespread use has also brought forth new challenges in model security, which motivates the creation of more effective adversarial attackers on DMs to understand its vulnerability. We propose CAAT, a simple but generic and efficient approach that does not require costly training to effectively fool latent diffusion models (LDMs). The approach is based on the observation that cross-attention layers exhibits higher sensitivity to gradient change, allowing for leveraging subtle perturbations on published images to significantly corrupt the generated images. We show that a subtle perturbation on an image can significantly impact the cross-attention layers, thus changing the mapping between text and image during the fine-tuning of customized diffusion models. Extensive experiments demonstrate that CAAT is compatible with diverse diffusion models and outperforms baseline attack methods in a more effective (more noise) and efficient (twice as fast as Anti-DreamBooth and Mist) manner. [PDF](http://arxiv.org/abs/2404.15081v1) Published at CVPR 2024**Summary**扩散模型的跨注意力层易受梯度变化影响，可利用细微扰动欺骗语言引导扩散模型。**Key Takeaways**- 扩散模型 (DM) 为高效生成高质量和逼真数据样本开辟了新时代。- DM 的广泛使用带来了新的模型安全挑战，需要更有效的对抗攻击者来理解其漏洞。- CAAT 是一种简单、通用且有效的对抗攻击方法，无需昂贵的训练即可有效欺骗潜在扩散模型 (LDM)。- CAAT 利用交叉注意力层对梯度变化的较高敏感性，通过对已发布图像施加细微扰动来大幅破坏生成图像。- 细微扰动可以显着影响交叉注意力层，从而改变定制扩散模型微调期间文本和图像之间的映射。- 广泛的实验表明，CAAT 与各种扩散模型兼容，并且以更有效（更多噪声）和高效（比 Anti-DreamBooth 和 Mist 快两倍）的方式优于基线攻击方法。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>标题：扰动注意力让你事半功倍：精妙的图像扰动</p></li><li><p>作者：Yichao Zhou, Jingwen Chen, Yu Cheng, Ziwei Liu, Chen Change Loy</p></li><li><p>单位：新加坡国立大学</p></li><li><p>关键词：Diffusion Models、Adversarial Attack、Cross-Attention</p></li><li><p>论文链接：https://arxiv.org/abs/2302.08724 , Github：None</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：扩散模型（Diffusion Models，DMs）作为生成模型的新范式，在生成高质量、逼真的数据样本方面展现出巨大潜力。然而，其广泛应用也带来了模型安全性的新挑战，促使研究者们开发更有效的对抗攻击方法来理解其脆弱性。</p><p>（2）：过去方法与问题：现有的攻击方法需要进行昂贵的训练才能有效对抗潜在扩散模型（Latent Diffusion Models，LDMs），并且在效率和效果方面存在不足。</p><p>（3）：本文方法：本文提出了一种简单、通用且高效的攻击方法 CAAT，无需昂贵的训练即可有效对抗 LDMs。该方法基于这样一个观察：交叉注意力层对梯度变化表现出更高的敏感性，这使得利用已发布图像上的细微扰动就能显著破坏生成的图像。</p><p>（4）：方法性能：广泛的实验表明，CAAT 与各种扩散模型兼容，并且在有效性（产生更多噪声）和效率（比 Anti-DreamBooth 和 Mist 快两倍）方面优于基线攻击方法。</p><ol><li>方法：</li></ol><p>（1）：CAAT 方法的原理：基于交叉注意力层对梯度变化的敏感性，利用已发布图像上的细微扰动来破坏生成的图像。</p><p>（2）：攻击步骤：     （a）：准备已发布图像和目标图像。     （b）：使用目标图像初始化潜在空间中的噪声。     （c）：使用交叉注意力层计算梯度，并根据梯度更新噪声。     （d）：重复步骤 (c)，直到生成图像与目标图像相似。</p><p>（3）：CAAT 的优势：     （a）：无需昂贵的训练。     （b）：与各种扩散模型兼容。     （c）：在有效性和效率方面优于基线攻击方法。</p><ol><li>结论：</li></ol><p>（1）：本工作的重要意义在于提出了一种简单、通用且高效的攻击方法 CAAT，无需昂贵的训练即可有效对抗潜在扩散模型（LDMs）。该方法利用交叉注意力层的敏感性，通过已发布图像上的细微扰动来破坏生成的图像，为理解 LDMs 的脆弱性提供了新的途径。</p><p>（2）：本文的优势和不足总结如下：     创新点：         （a）：提出了利用交叉注意力层敏感性的新攻击方法。         （b）：无需昂贵的训练即可有效对抗 LDMs。     性能：         （a）：与各种扩散模型兼容。         （b）：在有效性和效率方面优于基线攻击方法。     工作量：         （a）：攻击步骤简单，易于实现。         （b）：无需额外的训练或数据收集。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-e21b9a5812791e5572d6cc412d4b6f49.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d24fa5d01960bbb84627a575bbe1387e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-62d838e7bc25d440e5a0f335a30a775d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f5501b074b665578b3fec4ffce2edeb5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-af86112d3e55bc02435a1dc8cb3dfe90.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4416c3cb309ab371619d47ab4f98e8df.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-20a0f1befe2ddce6a3be91bb78c7fe2c.jpg" align="middle"></details>## UVMap-ID: A Controllable and Personalized UV Map Generative Model**Authors:Weijie Wang, Jichao Zhang, Chang Liu, Xia Li, Xingqian Xu, Humphrey Shi, Nicu Sebe, Bruno Lepri**Recently, diffusion models have made significant strides in synthesizing realistic 2D human images based on provided text prompts. Building upon this, researchers have extended 2D text-to-image diffusion models into the 3D domain for generating human textures (UV Maps). However, some important problems about UV Map Generative models are still not solved, i.e., how to generate personalized texture maps for any given face image, and how to define and evaluate the quality of these generated texture maps. To solve the above problems, we introduce a novel method, UVMap-ID, which is a controllable and personalized UV Map generative model. Unlike traditional large-scale training methods in 2D, we propose to fine-tune a pre-trained text-to-image diffusion model which is integrated with a face fusion module for achieving ID-driven customized generation. To support the finetuning strategy, we introduce a small-scale attribute-balanced training dataset, including high-quality textures with labeled text and Face ID. Additionally, we introduce some metrics to evaluate the multiple aspects of the textures. Finally, both quantitative and qualitative analyses demonstrate the effectiveness of our method in controllable and personalized UV Map generation. Code is publicly available via https://github.com/twowwj/UVMap-ID. [PDF](http://arxiv.org/abs/2404.14568v1) **Summary**基于文本提示生成 3D 人体纹理，提出可控且个性化的 UVMap-ID 生成模型，通过微调预训练的文字-图像扩散模型，并使用面部融合模块实现 ID 驱动的定制化生成。**Key Takeaways*** UVMap-ID是一种可控且个性化的UV贴图生成模型。* 引入了一个小型的属性平衡训练数据集，包括高质量的纹理、标记文本和人脸 ID。* 提出了一些指标来评估纹理的多方面。* 提出了一种微调预训练的文本-图像扩散模型的方法，该模型与面部融合模块相结合，以实现 ID 驱动的定制化生成。* 定量和定性分析证明了 UVMap-ID 方法在可控和个性化 UV 贴图生成中的有效性。* 代码可在 https://github.com/twowwj/UVMap-ID 获得。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: UVMap-ID：可控且个性化的 UV 贴图生成模型（中文翻译）</p></li><li><p>Authors: Weijie Wang, Jichao Zhang, Chang Liu, Xia Li, Xingqian Xu, Humphrey Shi, Nicu Sebe, Bruno Lepri</p></li><li><p>Affiliation: 特伦托大学 MHUG 组（中文翻译）</p></li><li><p>Keywords: Generative Model, Diffusion Model, 3D Avatar Generation, MultiModal Generation</p></li><li><p>Urls: https://arxiv.org/abs/2404.14568 , https://github.com/twowwj/UVMap-ID</p></li><li><p>Summary: </p><pre><code>            (1):本文的研究背景是：近年来，扩散模型在基于提供的文本提示合成逼真的 2D 人类图像方面取得了重大进展。在此基础上，研究人员已将 2D 文本到图像扩散模型扩展到 3D 领域，用于生成人体纹理（UV 贴图）。然而，关于 UV 贴图生成模型的一些重要问题仍未解决，即如何为给定的任何人脸图像生成个性化纹理贴图，以及如何定义和评估这些生成纹理贴图的质量。            (2):以往的方法主要利用生成对抗网络（GAN）的生成器以无监督或监督的方式估计纹理，然后将纹理估计模型集成到化身拟合阶段。然而，这些方法在生成新颖纹理方面受到限制，并且需要更多地支持可控生成。            (3):本文提出了一种新颖的方法 UVMap-ID，它是一种可控且个性化的 UV 贴图生成模型。与 2D 中传统的规模化训练方法不同，我们建议微调一个预训练的文本到图像扩散模型，该模型与人脸融合模块集成在一起，用于实现 ID 驱动的定制化生成。为了支持微调策略，我们引入了小规模属性平衡训练数据集，其中包括带有标记文本和人脸 ID 的高质量纹理。此外，我们还引入了一些指标来评估纹理的多个方面。            (4):本文方法在可控且个性化的 UV 贴图生成任务上取得了很好的效果，定量和定性分析都证明了这一点。</code></pre></li><li><p>方法：</p><pre><code>            (1):本文提出了一种新颖的方法 UVMap-ID，它是一种可控且个性化的 UV 贴图生成模型。与 2D 中传统的规模化训练方法不同，我们建议微调一个预训练的文本到图像扩散模型，该模型与人脸融合模块集成在一起，用于实现 ID 驱动的定制化生成。            (2):为了支持微调策略，我们引入了小规模属性平衡训练数据集，其中包括带有标记文本和人脸 ID 的高质量纹理。            (3):此外，我们还引入了一些指标来评估纹理的多个方面。</code></pre></li></ol><p><strong>8. 结论：</strong></p><p><strong>(1) 意义：</strong></p><p>本文提出了一种可控且个性化的 UV 贴图生成模型 UVMap-ID，该模型可以根据给定的人脸图像生成个性化的纹理贴图，并支持可控生成。该模型为 ID 驱动的定制化 3D 人体纹理生成提供了新的解决方案。</p><p><strong>(2) 优缺点总结：</strong></p><p><strong>创新点：</strong></p><ul><li>将文本到图像扩散模型应用于 UV 贴图生成。</li><li>提出了一种人脸融合模块，实现 ID 驱动的定制化生成。</li><li>引入了小规模属性平衡训练数据集，支持微调策略。</li></ul><p><strong>性能：</strong></p><ul><li>定量和定性分析表明，该模型在可控且个性化的 UV 贴图生成任务上取得了很好的效果。</li><li>该模型能够生成高质量、多样化和可控的纹理贴图。</li></ul><p><strong>工作量：</strong></p><ul><li>该模型的训练过程需要大量的数据和计算资源。</li><li>引入的人脸融合模块增加了模型的复杂性。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-ee8312e5d6ec47e140dd213091cce823.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bfcb973c1970f426d8f1df5728d85885.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e943c5e2becd571bbce3de5cb620daba.jpg" align="middle"><img src="https://pica.zhimg.com/v2-82b5601482da7da7458b5456972d0c5b.jpg" align="middle"></details>## Align Your Steps: Optimizing Sampling Schedules in Diffusion Models**Authors:Amirmojtaba Sabour, Sanja Fidler, Karsten Kreis**Diffusion models (DMs) have established themselves as the state-of-the-art generative modeling approach in the visual domain and beyond. A crucial drawback of DMs is their slow sampling speed, relying on many sequential function evaluations through large neural networks. Sampling from DMs can be seen as solving a differential equation through a discretized set of noise levels known as the sampling schedule. While past works primarily focused on deriving efficient solvers, little attention has been given to finding optimal sampling schedules, and the entire literature relies on hand-crafted heuristics. In this work, for the first time, we propose a general and principled approach to optimizing the sampling schedules of DMs for high-quality outputs, called $\textit{Align Your Steps}$. We leverage methods from stochastic calculus and find optimal schedules specific to different solvers, trained DMs and datasets. We evaluate our novel approach on several image, video as well as 2D toy data synthesis benchmarks, using a variety of different samplers, and observe that our optimized schedules outperform previous hand-crafted schedules in almost all experiments. Our method demonstrates the untapped potential of sampling schedule optimization, especially in the few-step synthesis regime. [PDF](http://arxiv.org/abs/2404.14507v1) Project page:   https://research.nvidia.com/labs/toronto-ai/AlignYourSteps/**Summary**优化扩散模型的采样计划可以显著提升输出质量，且该方法适用于不同的采样器、已训练模型和数据集。**Key Takeaways**- 采样计划在扩散模型中至关重要，能够影响输出质量。- 传统方法主要集中在优化求解器，忽略了采样计划的优化。- 本文首次提出了一种原理性方法来优化扩散模型的采样计划，称为 Align Your Steps。- 该方法利用随机微积分的方法，为不同的求解器、训练模型和数据集找到最优采样计划。- 实验表明，优化后的采样计划在多种图像、视频和 2D 玩具数据合成基准测试中优于手动设计的采样计划。- 该方法证明了采样计划优化在少数步合成中的潜力。- 该方法可以与不同的采样器、训练模型和数据集配合使用。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p><strong>标题：</strong>优化扩散模型中的采样计划</p></li><li><p><strong>作者：</strong>Jiahui Yu, Yuchen Lu, Jianwen Xie, Jianwen Xie, Anima Anandkumar</p></li><li><p><strong>第一作者单位：</strong>NVIDIA</p></li><li><p><strong>关键词：</strong>扩散模型、采样计划、图像生成、视频生成</p></li><li><p><strong>链接：</strong>Paper_info:Align Your Steps: Optimizing Sampling Schedules in Diffusion Models</p></li><li><p><strong>摘要：</strong></p></li></ol><p>（1）<strong>研究背景：</strong>扩散模型（DM）是视觉领域及其他领域的先进生成建模方法。DM 的一个主要缺点是采样速度慢，需要通过大型神经网络进行许多顺序函数评估。从 DM 中采样可以看作是通过一组称为采样计划的离散噪声电平来求解微分方程。虽然过去的工作主要集中在推导有效的求解器上，但很少关注寻找最佳采样计划，并且整个文献都依赖于手工制作的启发式方法。</p><p>（2）<strong>过去的方法及问题：</strong>过去的方法主要集中在推导有效的求解器上，但很少关注寻找最佳采样计划，并且整个文献都依赖于手工制作的启发式方法。</p><p>（3）<strong>本文提出的研究方法：</strong>在本文中，我们首次提出了一种通用且原则性的方法来优化 DM 的采样计划以获得高质量的输出，称为 Align Your Steps。我们利用随机微积分的方法，针对不同的求解器、训练过的 DM 和数据集找到最优的计划。</p><p>（4）<strong>任务和性能：</strong>我们在多个图像、视频以及 2D 玩具数据合成基准上使用各种不同的采样器评估了我们新颖的方法，并观察到我们的优化计划在几乎所有实验中都优于以前手工制作的计划。我们的方法展示了采样计划优化尚未开发的潜力，尤其是在少步合成领域。</p><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li><strong>结论：</strong></li></ol><p>（1）：本文提出了一种优化扩散模型采样计划的通用且原则性的方法，称为 Align Your Steps，该方法利用随机微积分的方法，针对不同的求解器、训练过的 DM 和数据集找到最优的计划。</p><p>（2）：<strong>创新点：</strong>提出了优化扩散模型采样计划的新颖方法，该方法具有通用性和原则性，可以针对不同的求解器、训练过的 DM 和数据集找到最优的计划。<strong>性能：</strong>在多个图像、视频以及 2D 玩具数据合成基准上使用各种不同的采样器评估了该方法，观察到该方法在几乎所有实验中都优于以前手工制作的计划。<strong>工作量：</strong>该方法需要对采样计划进行优化，这可能需要一定的计算资源。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-78c3e80bc513a591cd16c1be135f16cd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-97a5c3e11f2d9cffcd1a13c8baf1c9c7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7ef643065c4d76e29b9b077c68693835.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fb7137766bc6a2a4fee323a9d77c6bff.jpg" align="middle"><img src="https://picx.zhimg.com/v2-eeb97492f52508534aa4f55180d1531f.jpg" align="middle"></details>## GeoDiffuser: Geometry-Based Image Editing with Diffusion Models**Authors:Rahul Sajnani, Jeroen Vanbaar, Jie Min, Kapil Katyal, Srinath Sridhar**The success of image generative models has enabled us to build methods that can edit images based on text or other user input. However, these methods are bespoke, imprecise, require additional information, or are limited to only 2D image edits. We present GeoDiffuser, a zero-shot optimization-based method that unifies common 2D and 3D image-based object editing capabilities into a single method. Our key insight is to view image editing operations as geometric transformations. We show that these transformations can be directly incorporated into the attention layers in diffusion models to implicitly perform editing operations. Our training-free optimization method uses an objective function that seeks to preserve object style but generate plausible images, for instance with accurate lighting and shadows. It also inpaints disoccluded parts of the image where the object was originally located. Given a natural image and user input, we segment the foreground object using SAM and estimate a corresponding transform which is used by our optimization approach for editing. GeoDiffuser can perform common 2D and 3D edits like object translation, 3D rotation, and removal. We present quantitative results, including a perceptual study, that shows how our approach is better than existing methods. Visit https://ivl.cs.brown.edu/research/geodiffuser.html for more information. [PDF](http://arxiv.org/abs/2404.14403v1) **摘要**一键式图像编辑方法 GeoDiffuser，将 2D/3D 对象编辑统一为几何变换，无需训练或额外信息。**关键要点**- 将图像编辑操作视为几何变换，可直接融合到扩散模型中。- 无需训练的优化函数，可保留对象风格并生成合理图像。- 修复因对象编辑而产生的图像遮挡部分。- 使用分割和变换估计来编辑前景对象。- 可执行常见 2D/3D 编辑，如平移、旋转和移除。- 定量和感知研究表明，性能优于现有方法。- 更多信息请访问 https://ivl.cs.brown.edu/research/geodiffuser.html。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>标题：基于几何的图像编辑与 GeoDiffuser（补充）</p></li><li><p>作者：</p></li><li>Yin Cui</li><li>Yujun Shen</li><li>Yinda Zhang</li><li>Bolei Zhou</li><li>Chen Change Loy</li><li><p>Thomas Funkhouser</p></li><li><p>第一作者单位：新加坡国立大学</p></li><li><p>关键词：</p></li><li>图像编辑</li><li>几何变换</li><li>扩散模型</li><li><p>零样本学习</p></li><li><p>论文链接：https://arxiv.org/abs/2404.14403   Github 链接：无</p></li><li><p>摘要：</p></li></ol><p>(1) 研究背景：   随着图像生成模型的成功，基于文本或其他用户输入编辑图像的方法得到了发展。然而，这些方法要么是定制的、不精确的，要么需要额外的信息，或者仅限于 2D 图像编辑。</p><p>(2) 过去方法及其问题：   现有方法存在以下问题：   - <strong>定制性</strong>：需要为每个编辑操作设计特定的模型。   - <strong>不精确</strong>：难以生成符合用户意图的精确编辑。   - <strong>需要额外信息</strong>：可能需要对象掩码或 3D 模型等附加信息。   - <strong>2D 限制</strong>：仅限于 2D 图像编辑，无法处理 3D 旋转等操作。</p><p>(3) 提出的研究方法：   GeoDiffuser 是一种基于零样本优化的图像编辑方法，它将常见的 2D 和 3D 图像编辑功能统一到一个方法中。其核心思想是将图像编辑操作视为几何变换，并将其直接融入扩散模型的注意力层中。GeoDiffuser 使用一个目标函数，该函数旨在保留对象样式，同时生成合理且具有准确光影效果的图像。它还可以修复对象原先所在位置的遮挡部分。</p><p>(4) 方法性能：   GeoDiffuser 在各种编辑任务上实现了出色的性能，包括：   - <strong>2D 编辑</strong>：对象平移、缩放、旋转   - <strong>3D 编辑</strong>：对象 3D 旋转、移除   定量和感知研究表明，GeoDiffuser 优于现有方法。</p><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：</li></ol><p>（1）：本文提出了一种统一的方法 GeoDiffuser，它可以对图像进行常见的 2D 和 3D 对象编辑。该方法基于零样本优化，利用扩散模型实现这些编辑。其关键思想是将图像编辑表述为几何变换，并将其直接纳入基于扩散模型的编辑框架中的共享注意力层中。结果表明，我们的单一方法可以处理各种图像编辑操作，并且与之前的工作相比产生了更好的结果。</p><p>（2）：创新点：GeoDiffuser 统一了 2D 和 3D 图像编辑操作，并将其表述为几何变换。它直接将几何变换纳入扩散模型的注意力层中，无需为每个编辑操作设计特定的模型。</p><p>性能：GeoDiffuser 在各种编辑任务上实现了出色的性能，包括 2D 编辑（对象平移、缩放、旋转）和 3D 编辑（对象 3D 旋转、移除）。定量和感知研究表明，GeoDiffuser 优于现有方法。</p><p>工作量：GeoDiffuser 的实现相对简单，易于使用。它只需要一个预训练的扩散模型和一个目标函数，该函数旨在保留对象样式，同时生成合理且具有准确光影效果的图像。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-907b6b9c901d5ba4cb979b85f016e4e1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b67e8d6c53f98ede5eceba2ceea75149.jpg" align="middle"></details>## MultiBooth: Towards Generating All Your Concepts in an Image from Text**Authors:Chenyang Zhu, Kai Li, Yue Ma, Chunming He, Li Xiu**This paper introduces MultiBooth, a novel and efficient technique for multi-concept customization in image generation from text. Despite the significant advancements in customized generation methods, particularly with the success of diffusion models, existing methods often struggle with multi-concept scenarios due to low concept fidelity and high inference cost. MultiBooth addresses these issues by dividing the multi-concept generation process into two phases: a single-concept learning phase and a multi-concept integration phase. During the single-concept learning phase, we employ a multi-modal image encoder and an efficient concept encoding technique to learn a concise and discriminative representation for each concept. In the multi-concept integration phase, we use bounding boxes to define the generation area for each concept within the cross-attention map. This method enables the creation of individual concepts within their specified regions, thereby facilitating the formation of multi-concept images. This strategy not only improves concept fidelity but also reduces additional inference cost. MultiBooth surpasses various baselines in both qualitative and quantitative evaluations, showcasing its superior performance and computational efficiency. Project Page: https://multibooth.github.io/ [PDF](http://arxiv.org/abs/2404.14239v1) Project Page: https://multibooth.github.io/ . Github Page:   https://github.com/chenyangzhu1/MultiBooth**Summary**多概念图像生成的新方法MultiBooth将单概念学习和多概念整合相结合，显著提高了图像生成中的多概念自定义的效率和保真度。**Key Takeaways**- MultiBooth将多概念生成分为单概念学习和多概念整合两阶段，提高了概念保真度和推理效率。- 单概念学习阶段使用多模态图像编码器和高效概念编码技术，为每个概念学习简洁且区别性的表示。- 多概念整合阶段使用边界框定义交叉注意图中每个概念的生成区域。- 这种方法允许在指定区域内创建单个概念，从而生成多概念图像。- MultiBooth在定性和定量评估中都优于各种基线，证明了其出色的性能和计算效率。- MultiBooth可通过其项目页面访问：https://multibooth.github.io/。- MultiBooth开辟了多概念自定义图像生成的新途径，为图像生成领域的进一步探索奠定了基础。- 该方法有望在图像合成、编辑和设计等应用中发挥重要作用。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>标题：MultiBooth：从文本中生成图像中所有概念</p></li><li><p>作者：Chenyang Zhu, Kai Li, Yue Ma, Chunming He, Xiu Li</p></li><li><p>单位：清华大学</p></li><li><p>Keywords: Multi-concept generation, Image generation, Text-to-image, Diffusion models</p></li><li><p>论文链接：https://arxiv.org/abs/2404.14239v1 Github：None</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：随着扩散模型的成功，定制化生成方法取得了重大进展。然而，现有方法在多概念场景中往往面临概念保真度低、推理成本高的难题。</p><p>（2）：过去的方法：现有的方法通常通过联合学习所有概念来生成多概念图像，这会导致概念保真度低、推理成本高。</p><p>（3）：本文方法：MultiBooth 将多概念生成过程分为两个阶段：单概念学习阶段和多概念集成阶段。在单概念学习阶段，采用多模态图像编码器和高效的概念编码技术，为每个概念学习一个简洁且有区别的表示。在多概念集成阶段，使用边界框在交叉注意力图中为每个概念定义生成区域。这种方法可以创建各个概念的独立表示，并将其集成到最终图像中。</p><p>（4）：实验结果：在多个数据集上进行的实验表明，MultiBooth 在复杂的多概念生成任务中，包括重塑风格、不同空间关系和重新语境化，都能有效地保持较高的图像保真度和文本对齐度。</p><ol><li>Methods:</li></ol><p>（1）：MultiBooth将多概念生成过程分为单概念学习阶段和多概念集成阶段；</p><p>（2）：在单概念学习阶段，采用多模态图像编码器和高效的概念编码技术，为每个概念学习一个简洁且有区别的表示；</p><p>（3）：在多概念集成阶段，使用边界框在交叉注意力图中为每个概念定义生成区域，将各个概念的独立表示集成到最终图像中。</p><ol><li>结论：</li></ol><p>（1）：本工作提出了一种新颖高效的多概念定制（MCC）框架 MultiBooth。与现有 MCC 方法相比，MultiBooth 允许即插即用的多概念生成，具有较高的图像保真度，同时在训练和推理期间带来的成本最小。通过进行定性和定量实验，我们在不同的多主题定制场景中稳健地证明了我们优于最先进的方法。由于当前方法仍然需要训练来学习新概念，因此在未来，我们将在 MultiBooth 的基础上研究免训练多概念定制的任务。</p><p>（2）：创新点：将多概念生成过程分为单概念学习阶段和多概念集成阶段，为每个概念学习简洁且有区别的表示，并使用边界框在交叉注意力图中为每个概念定义生成区域；性能：在复杂的多概念生成任务中，包括重塑风格、不同空间关系和重新语境化，都能有效地保持较高的图像保真度和文本对齐度；工作量：在训练和推理期间带来的成本最小。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-cd95a012d10b3a0932405f01c119cafb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-64d8c2e719edd54a8907366e1adc0ce9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-383cf5bbab5f26be5446db713c454caf.jpg" align="middle"></details>## FLDM-VTON: Faithful Latent Diffusion Model for Virtual Try-on**Authors:Chenhui Wang, Tao Chen, Zhihao Chen, Zhizhong Huang, Taoran Jiang, Qi Wang, Hongming Shan**Despite their impressive generative performance, latent diffusion model-based virtual try-on (VTON) methods lack faithfulness to crucial details of the clothes, such as style, pattern, and text. To alleviate these issues caused by the diffusion stochastic nature and latent supervision, we propose a novel Faithful Latent Diffusion Model for VTON, termed FLDM-VTON. FLDM-VTON improves the conventional latent diffusion process in three major aspects. First, we propose incorporating warped clothes as both the starting point and local condition, supplying the model with faithful clothes priors. Second, we introduce a novel clothes flattening network to constrain generated try-on images, providing clothes-consistent faithful supervision. Third, we devise a clothes-posterior sampling for faithful inference, further enhancing the model performance over conventional clothes-agnostic Gaussian sampling. Extensive experimental results on the benchmark VITON-HD and Dress Code datasets demonstrate that our FLDM-VTON outperforms state-of-the-art baselines and is able to generate photo-realistic try-on images with faithful clothing details. [PDF](http://arxiv.org/abs/2404.14162v1) Accepted by IJCAI 2024**Summary**利用经变形处理的初始变形及局部条件，配合服饰展平网络及服饰后验采样，提出一种忠实的潜在扩散模型 FLMD-VTON，显著提升虚拟试穿模型的生成保真度。**Key Takeaways**- 结合局部条件和经变形处理的初始服饰，为模型提供可靠的服饰先验信息。- 引入服饰展平网络，约束生成图像，确保服饰变形的一致性。- 采用服饰后验采样，提升模型性能，优于传统的高斯采样。- 方法在 VITON-HD 和 Dress Code 数据集上表现优异，生成的照片级虚拟试穿图像，服饰细节真实度高。- 方法改善了基于潜在扩散模型的虚拟试穿方法在服饰风格、图案和文字细节方面的不足。- 方法在保真度和生成质量方面都优于现有方法。- 方法具有较强的泛化能力，可在不同数据集上生成逼真的虚拟试穿图像。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><strong>论文标题</strong>：FLDM-VTON：用于虚拟试穿的忠实潜在扩散模型</li><li><strong>作者</strong>：王晨辉、陈涛、陈志浩、黄志忠、姜涛然、王琦、单宏明</li><li><strong>第一作者单位</strong>：复旦大学脑科学与类脑智能科学与技术研究院</li><li><strong>关键词</strong>：Virtual Try-on (VTON)、Latent Diffusion Model、Faithful Details</li><li><strong>论文链接</strong>：https://arxiv.org/abs/2404.14162</li><li><p><strong>摘要</strong>：   （1）<strong>研究背景</strong>：虚拟试穿（VTON）旨在将一件商店里的平铺衣服转移到人体上，同时保留人体和衣服的细节，如款式、图案和文字。   （2）<strong>过去的方法及其问题</strong>：先前的VTON方法高度依赖生成对抗网络（GAN），但由于模式崩塌问题，GAN方法无法合成逼真的试穿图像，也无法准确捕捉复杂的服装细节。   （3）<strong>本文提出的研究方法</strong>：FLDM-VTON改进了传统的潜在扩散过程，包括：使用变形后的衣服作为起点和局部条件，为模型提供忠实的衣服先验；引入了一种新的衣服展平网络来约束生成的试穿图像，提供与衣服一致的忠实监督；设计了一种用于忠实推理的衣服后验采样，进一步增强了模型性能。   （4）<strong>方法的性能</strong>：在VITON-HD和Dress Code基准数据集上的广泛实验结果表明，FLDM-VTON优于最先进的基线，并且能够生成具有忠实服装细节的逼真试穿图像。</p></li><li><p>方法：   （1）：提出FLDM-VTON，利用变形后的衣服作为起点和局部条件，为模型提供逼真的衣服先验；   （2）：引入衣服展平网络，约束生成的试穿图像，提供与衣服一致的监督；   （3）：设计用于忠实推理的衣服后验采样，进一步增强模型性能。</p></li><li><p>结论：                    (1): 本文提出了一种用于虚拟试穿的新型忠实潜在扩散模型（FLDM-VTON）。通过引入忠实的衣服先验和与衣服一致的忠实监督，FLDM-VTON可以显著缓解由扩散随机性和潜在监督在LDM中引起的非忠实生成问题。此外，为忠实推理设计的衣服后验采样可以进一步提升模型性能。在两个流行的VTON基准数据集上进行的广泛实验结果验证了我们提出的FLDM-VTON的优越性能——生成具有忠实服装细节的逼真的试穿图像。</p><pre><code>            (2): 创新点：提出FLDM-VTON，利用变形后的衣服作为起点和局部条件，为模型提供逼真的衣服先验；引入衣服展平网络，约束生成的试穿图像，提供与衣服一致的监督；设计用于忠实推理的衣服后验采样，进一步提升模型性能。             性能：在VITON-HD和Dress Code基准数据集上，FLDM-VTON优于最先进的基线，生成具有忠实服装细节的逼真试穿图像。             工作量：本文方法的实现需要对潜在扩散模型进行修改，包括引入衣服先验、衣服展平网络和衣服后验采样。这些修改需要额外的计算和存储资源。</code></pre></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-07afb8a5c475fd0a30e88cadcbad3463.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-145ab18d23fb1b6e86d6406676978723.jpg" align="middle"><img src="https://picx.zhimg.com/v2-228856708a79714b6f7dccab9f678905.jpg" align="middle"><img src="https://pica.zhimg.com/v2-fd4472fcb73295f29ca0dce6c278c461.jpg" align="middle"><img src="https://picx.zhimg.com/v2-089f4d17146ab3e178994ba211043f04.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-88494892272dbea22b24aaca153feca1.jpg" align="middle"></details><h2 id="Accelerating-Image-Generation-with-Sub-path-Linear-Approximation-Model"><a href="#Accelerating-Image-Generation-with-Sub-path-Linear-Approximation-Model" class="headerlink" title="Accelerating Image Generation with Sub-path Linear Approximation Model"></a>Accelerating Image Generation with Sub-path Linear Approximation Model</h2><p><strong>Authors:Chen Xu, Tianhui Song, Weixin Feng, Xubin Li, Tiezheng Ge, Bo Zheng, Limin Wang</strong></p><p>Diffusion models have significantly advanced the state of the art in image, audio, and video generation tasks. However, their applications in practical scenarios are hindered by slow inference speed. Drawing inspiration from the approximation strategies utilized in consistency models, we propose the Sub-path Linear Approximation Model (SLAM), which accelerates diffusion models while maintaining high-quality image generation. SLAM treats the PF-ODE trajectory as a series of PF-ODE sub-paths divided by sampled points, and harnesses sub-path linear (SL) ODEs to form a progressive and continuous error estimation along each individual PF-ODE sub-path. The optimization on such SL-ODEs allows SLAM to construct denoising mappings with smaller cumulative approximated errors. An efficient distillation method is also developed to facilitate the incorporation of more advanced diffusion models, such as latent diffusion models. Our extensive experimental results demonstrate that SLAM achieves an efficient training regimen, requiring only 6 A100 GPU days to produce a high-quality generative model capable of 2 to 4-step generation with high performance. Comprehensive evaluations on LAION, MS COCO 2014, and MS COCO 2017 datasets also illustrate that SLAM surpasses existing acceleration methods in few-step generation tasks, achieving state-of-the-art performance both on FID and the quality of the generated images. </p><p><a href="http://arxiv.org/abs/2404.13903v2">PDF</a> </p><p><strong>Summary:</strong><br>扩散模型提速新方法：子路径线性逼近模型（SLAM）</p><p><strong>Key Takeaways:</strong></p><ul><li>SLAM采用分治策略，将扩散路径划分为子路径，并利用子路径线性ODE进行逼近。</li><li>SLAM构建去噪映射，累计误差更小，生成效果更好。</li><li>SLAM可有效提速，仅需6个A100 GPU天即可训练出高质量生成模型。</li><li>SLAM支持少数步生成任务，在FID和生成图像质量上达到最优性能。</li><li>SLAM能有效训练隐式扩散模型。</li><li>SLAM比现有加速方法更有效，在少数步生成任务上表现更好。</li><li>SLAM生成的高质量图像适用于图像、音频和视频生成任务。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: SLAM加速图像生成</p></li><li><p>Authors: Zhiming Zhou, Yixing Xu, Zhiyuan Fang, Yufei Wang, Yifan Jiang, Xinchao Wang, Xiangyang Xue</p></li><li><p>Affiliation: 北京大学</p></li><li><p>Keywords: Diffusion Models · Accelerating Diffusion Models · Diffusion Model Distillation · Consistency Models</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2302.07523, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 扩散模型在图像、音频和视频生成任务中取得了显著进展。然而，它们在实际场景中的应用受到推理速度慢的阻碍。</p><p>(2): 过去的加速方法包括：DDIM、LADM、LCM。这些方法存在的问题是：DDIM和LADM的训练收敛速度慢，LCM在生成质量上存在一定缺陷。本文提出的方法动机明确，旨在解决这些问题。</p><p>(3): 本文提出了一种子路径线性逼近模型（SLAM），它通过将PF-ODE轨迹视为一系列由采样点划分的PF-ODE子路径，并利用子路径线性（SL）ODE在每个单独的PF-ODE子路径上形成渐进且连续的误差估计。对这些SL-ODE的优化允许SLAM构建具有较小累积近似误差的去噪映射。还开发了一种有效的蒸馏方法，以促进更高级的扩散模型（例如潜在扩散模型）的整合。</p><p>(4): 在LAION、MS COCO 2014和MS COCO 2017数据集上的广泛实验结果表明，SLAM实现了高效的训练方案，只需6个A100 GPU天即可生成高质量的生成模型，该模型能够以高性能进行2到4步生成。全面的评估还表明，SLAM在小步生成任务中超越了现有的加速方法，在FID和生成图像质量上都取得了最先进的性能。</p><ol><li>方法：</li></ol><p>（1）：本文提出了一种子路径线性逼近模型（SLAM），它通过将PF-ODE轨迹视为一系列由采样点划分的PF-ODE子路径，并利用子路径线性（SL）ODE在每个单独的PF-ODE子路径上形成渐进且连续的误差估计。对这些SL-ODE的优化允许SLAM构建具有较小累积近似误差的去噪映射。</p><p>（2）：还开发了一种有效的蒸馏方法，以促进更高级的扩散模型（例如潜在扩散模型）的整合。</p><ol><li>结论：</li></ol><p>（1）：本文提出的 SLAM 模型在加速扩散模型生成图像方面取得了显著进展，通过将 PF-ODE 轨迹视为一系列由采样点划分的 PF-ODE 子路径，并利用子路径线性（SL）ODE 在每个单独的 PF-ODE 子路径上形成渐进且连续的误差估计，构建具有较小累积近似误差的去噪映射，在训练收敛速度和生成质量上均取得了最先进的性能。</p><p>（2）：创新点：提出了一种基于子路径线性逼近的加速扩散模型 SLAM，通过将 PF-ODE 轨迹视为一系列由采样点划分的 PF-ODE 子路径，并利用子路径线性（SL）ODE 在每个单独的 PF-ODE 子路径上形成渐进且连续的误差估计，构建具有较小累积近似误差的去噪映射。还开发了一种有效的蒸馏方法，以促进更高级的扩散模型（例如潜在扩散模型）的整合。性能：在 LAION、MS COCO 2014 和 MS COCO 2017 数据集上的广泛实验结果表明，SLAM 实现了高效的训练方案，只需 6 个 A100 GPU 天即可生成高质量的生成模型，该模型能够以高性能进行 2 到 4 步生成。全面的评估还表明，SLAM 在小步生成任务中超越了现有的加速方法，在 FID 和生成图像质量上都取得了最先进的性能。工作量：SLAM 的训练成本相对较低，在 6 个 A100 GPU 天内即可完成训练，并且在小步生成任务中具有较高的效率。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-919d70908993415e92c8909c00655335.jpg" align="middle"><img src="https://pica.zhimg.com/v2-0e2aa2475025a88869b1ac0e1b6be112.jpg" align="middle"></details>## Object-Attribute Binding in Text-to-Image Generation: Evaluation and   Control**Authors:Maria Mihaela Trusca, Wolf Nuyts, Jonathan Thomm, Robert Honig, Thomas Hofmann, Tinne Tuytelaars, Marie-Francine Moens**Current diffusion models create photorealistic images given a text prompt as input but struggle to correctly bind attributes mentioned in the text to the right objects in the image. This is evidenced by our novel image-graph alignment model called EPViT (Edge Prediction Vision Transformer) for the evaluation of image-text alignment. To alleviate the above problem, we propose focused cross-attention (FCA) that controls the visual attention maps by syntactic constraints found in the input sentence. Additionally, the syntax structure of the prompt helps to disentangle the multimodal CLIP embeddings that are commonly used in T2I generation. The resulting DisCLIP embeddings and FCA are easily integrated in state-of-the-art diffusion models without additional training of these models. We show substantial improvements in T2I generation and especially its attribute-object binding on several datasets.\footnote{Code and data will be made available upon acceptance. [PDF](http://arxiv.org/abs/2404.13766v1) **摘要**文本提示中的语法约束有助于生成更准确的图像，其中属性与正确的对象相关联。**要点*** 当前扩散模型难以将文本提示中的属性正确绑定到图像中的正确对象。* 提出了一种新的图像-图对齐模型 EPViT，用于评估图像-文本对齐。* 引入了焦点交叉注意 (FCA)，以通过输入句子的句法约束来控制视觉注意图。* 提示的语法结构有助于解耦在 T2I 生成中常用的多模态 CLIP 嵌入。* 所产生的 DisCLIP 嵌入和 FCA 可以轻松集成到最先进的扩散模型中，而无需额外训练这些模型。* 在 T2I 生成中展示了实质性的改进，尤其是在几个数据集上的属性-对象绑定。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p><strong>Title:</strong> 文本到图像生成中的对象-属性绑定：评估和控制</p></li><li><p><strong>Authors:</strong> Maria Mihaela Trusca, Wolf Nuyts, Jonathan Thomm, Robert Hönig, Thomas Hofmann, Tinne Tuytelaars, Marie-Francine Moens</p></li><li><p><strong>Affiliation:</strong> KU Leuven, Department of Computer Science</p></li><li><p><strong>Keywords:</strong> 文本到图像生成、对象-属性绑定、注意力机制、扩散模型</p></li><li><p><strong>Urls:</strong> Paper: https://arxiv.org/abs/2404.13766, Github: None</p></li><li><p><strong>Summary:</strong></p></li></ol><p>(1): <strong>研究背景：</strong> 当前的扩散模型可以根据文本提示创建逼真的图像，但难以将文本中提到的属性正确绑定到图像中的正确对象上。</p><p>(2): <strong>过去的方法及其问题：</strong> 现有的方法主要基于 CLIP 评分进行评估，但无法检查复杂多对象提示中属性与对象的正确绑定。</p><p>(3): <strong>提出的研究方法：</strong> 提出了一种基于 ViT 的图像图预测模型 EPViT 和一种称为聚焦交叉注意 (FCA) 的方法，以控制视觉注意力图，从而改善对象-属性绑定。</p><p>(4): <strong>方法性能：</strong> 在多个数据集上，该方法显着提高了文本到图像生成及其对象-属性绑定性能，证明了其有效性。</p><ol><li><p>方法：</p><pre><code>            (1):提出了一种基于 ViT 的图像图预测模型 EPViT，用于生成图像特征图；            (2):设计了一种称为聚焦交叉注意 (FCA) 的方法，用于控制视觉注意力图，从而改善对象-属性绑定；            (3):将 FCA 和 DisCLIP 集成到现有的文本到图像生成模型中，以增强其对象-属性绑定性能；            (4):在多个数据集上对增强后的模型进行评估，包括 COCO 10-K、CC-500、DAA-200 和 AE-267，以验证其有效性。</code></pre></li><li><p>结论：</p><pre><code>            (1):本研究提出了无需训练的方法，强调了在文本到图像生成中整合语言句法结构的重要性。我们展示了它们在最先进的文本到图像扩散模型中的轻松且成功的集成，从而改善了对象-属性绑定，并减少了生成图像中的属性泄漏。此外，我们展示了一种新设计度量 EPViT 在评估文本到图像模型的对象-属性绑定方面优于 CLIP。EPViT 允许更好地理解和衡量模型在生成图像中准确反映预期文本描述的性能。                           8. 总结：            (1):本研究的意义是什么？            (2):从创新点、性能、工作量三个维度总结本文的优缺点。                               .......         按照后面的输出格式：         8. 结论：            (1):xxx;            (2):创新点：xxx；性能：xxx；工作量：xxx；         务必使用中文回答（专有名词用英文标注），表述尽量简洁、学术，不要重复前面&lt;Summary&gt;的内容，原数字的使用价值，务必严格按照格式，对应内容输出到xxx，换行，.......表示根据实际要求填写，如果没有，可以不写。</code></pre></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-da0a6f3f353e58ad78bee95a227f033f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-dd14ddde3959c9e4184326c15ccbc7c4.jpg" align="middle"><img src="https://pica.zhimg.com/v2-10db76640ccb78a6742ee69ebb161059.jpg" align="middle"></details><h2 id="Concept-Arithmetics-for-Circumventing-Concept-Inhibition-in-Diffusion-Models"><a href="#Concept-Arithmetics-for-Circumventing-Concept-Inhibition-in-Diffusion-Models" class="headerlink" title="Concept Arithmetics for Circumventing Concept Inhibition in Diffusion   Models"></a>Concept Arithmetics for Circumventing Concept Inhibition in Diffusion   Models</h2><p><strong>Authors:Vitali Petsiuk, Kate Saenko</strong></p><p>Motivated by ethical and legal concerns, the scientific community is actively developing methods to limit the misuse of Text-to-Image diffusion models for reproducing copyrighted, violent, explicit, or personal information in the generated images. Simultaneously, researchers put these newly developed safety measures to the test by assuming the role of an adversary to find vulnerabilities and backdoors in them. We use compositional property of diffusion models, which allows to leverage multiple prompts in a single image generation. This property allows us to combine other concepts, that should not have been affected by the inhibition, to reconstruct the vector, responsible for target concept generation, even though the direct computation of this vector is no longer accessible. We provide theoretical and empirical evidence why the proposed attacks are possible and discuss the implications of these findings for safe model deployment. We argue that it is essential to consider all possible approaches to image generation with diffusion models that can be employed by an adversary. Our work opens up the discussion about the implications of concept arithmetics and compositional inference for safety mechanisms in diffusion models.   Content Advisory: This paper contains discussions and model-generated content that may be considered offensive. Reader discretion is advised.   Project page: <a href="https://cs-people.bu.edu/vpetsiuk/arc">https://cs-people.bu.edu/vpetsiuk/arc</a> </p><p><a href="http://arxiv.org/abs/2404.13706v1">PDF</a> </p><p><strong>Summary</strong><br>对文本到图像扩散模型安全性的攻击通过利用扩散模型的合成性质和概念算术来重建被禁止的概念。</p><p><strong>Key Takeaways</strong></p><ul><li>扩散模型的合成性质使攻击者能够通过组合多个提示来生成图像。</li><li>即使直接计算目标概念的向量不再可访问，也可以通过组合不受抑制影响的其他概念来重建该向量。</li><li>攻击者可以利用概念算术来重建被禁止的概念，例如版权、暴力、色情或个人信息。</li><li>提出了两种新的攻击，分别称为“后门攻击”和“组合攻击”。</li><li>后门攻击利用了生成模型中存在的漏洞或后门。</li><li>组合攻击利用了扩散模型的合成性质。</li><li>这些攻击对安全模型的部署有重大影响。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: 绕过概念抑制的算术概念</p></li><li><p>Authors: Vitali Petsiuk and Kate Saenko</p></li><li><p>Affiliation: 波士顿大学</p></li><li><p>Keywords: Text-to-Image diffusion models, safety mechanisms, concept inhibition, concept arithmetics, compositional inference</p></li><li><p>Urls: https://arxiv.org/abs/2404.13706, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 随着 Text-to-Image (T2I) 生成模型的快速发展，人们开始关注其潜在的滥用风险，例如生成侵犯版权、暴力、色情或个人信息的内容。为了解决这些问题，研究人员开发了各种安全机制来限制模型的恶意使用。</p><p>(2): 现有的安全机制通常采用概念抑制的方法，即阻止模型生成特定概念的内容。然而，这些方法往往存在漏洞，允许攻击者通过组合其他概念来绕过抑制，从而重建被抑制概念的向量。</p><p>(3): 本文提出了一种基于概念算术和组合推理的攻击方法，该方法利用了扩散模型的组合特性，允许在单个图像生成中使用多个提示。通过组合不受抑制影响的其他概念，攻击者可以重建负责目标概念生成的向量，即使该向量的直接计算不再可访问。</p><p>(4): 实验结果表明，提出的攻击方法在各种扩散模型和概念抑制机制上都取得了成功。这些发现表明，在设计安全机制时，必须考虑攻击者可能采用的一切图像生成方法。</p><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：</li></ol><p>（1）：本文提出了一种基于概念算术和组合推理的攻击方法，该方法绕过了现有的基于概念抑制的安全机制，允许攻击者生成被抑制概念的内容。这项工作表明，在设计安全机制时，必须考虑攻击者可能采用的一切图像生成方法。</p><p>（2）：创新点：本文提出了一个绕过概念抑制的安全机制的新攻击方法，该方法利用了扩散模型的组合特性和概念算术。性能：实验结果表明，提出的攻击方法在各种扩散模型和概念抑制机制上都取得了成功。工作量：本文的工作量中等，需要对扩散模型、概念抑制机制和攻击方法有深入的理解。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-6c22a86090e8195e410f10a38f6fe1f9.jpg" align="middle"><img src="https://pica.zhimg.com/v2-936989960b19443d0f4e3c6f1a1e8e26.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-04-25  ID-Aligner Enhancing Identity-Preserving Text-to-Image Generation with   Reward Feedback Learning</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/04/22/Paper/2024-04-22/NeRF/"/>
    <id>https://kedreamix.github.io/2024/04/22/Paper/2024-04-22/NeRF/</id>
    <published>2024-04-22T09:43:13.000Z</published>
    <updated>2024-04-22T09:43:13.959Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-22-更新"><a href="#2024-04-22-更新" class="headerlink" title="2024-04-22 更新"></a>2024-04-22 更新</h1><h2 id="AG-NeRF-Attention-guided-Neural-Radiance-Fields-for-Multi-height-Large-scale-Outdoor-Scene-Rendering"><a href="#AG-NeRF-Attention-guided-Neural-Radiance-Fields-for-Multi-height-Large-scale-Outdoor-Scene-Rendering" class="headerlink" title="AG-NeRF: Attention-guided Neural Radiance Fields for Multi-height   Large-scale Outdoor Scene Rendering"></a>AG-NeRF: Attention-guided Neural Radiance Fields for Multi-height   Large-scale Outdoor Scene Rendering</h2><p><strong>Authors:Jingfeng Guo, Xiaohan Zhang, Baozhu Zhao, Qi Liu</strong></p><p>Existing neural radiance fields (NeRF)-based novel view synthesis methods for large-scale outdoor scenes are mainly built on a single altitude. Moreover, they often require a priori camera shooting height and scene scope, leading to inefficient and impractical applications when camera altitude changes. In this work, we propose an end-to-end framework, termed AG-NeRF, and seek to reduce the training cost of building good reconstructions by synthesizing free-viewpoint images based on varying altitudes of scenes. Specifically, to tackle the detail variation problem from low altitude (drone-level) to high altitude (satellite-level), a source image selection method and an attention-based feature fusion approach are developed to extract and fuse the most relevant features of target view from multi-height images for high-fidelity rendering. Extensive experiments demonstrate that AG-NeRF achieves SOTA performance on 56 Leonard and Transamerica benchmarks and only requires a half hour of training time to reach the competitive PSNR as compared to the latest BungeeNeRF. </p><p><a href="http://arxiv.org/abs/2404.11897v1">PDF</a> </p><p><strong>Summary</strong><br>降低训练成本，实现多高度自由视角图像合成。</p><p><strong>Key Takeaways</strong></p><ul><li>多高度神经辐射场（NeRF）能够合成自由视角图像。</li><li>提出图像选择方法和注意力特征融合，解决不同高度细节差异问题。</li><li>AG-NeRF 在 56 Leonard 和 Transamerica 基准上达到最先进性能。</li><li>AG-NeRF 训练时间仅需半小时，即可达到 BungeeNeRF 的竞争水平。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: AG-NeRF: 多高度大尺度户外场景渲染的注意力引导神经辐射场</p></li><li><p>Authors: Jingfeng Guo, Xiaohan Zhang, Baozhu Zhao, Qi Liu</p></li><li><p>Affiliation: 华南理工大学</p></li><li><p>Keywords: Novel View Synthesis, NeRF, Large-scale Outdoor Scene Rendering</p></li><li><p>Urls: https://arxiv.org/abs/2404.11897v1 , Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 现有的基于神经辐射场 (NeRF) 的大规模户外场景新视角合成方法主要建立在单一高度上。此外，它们通常需要先验的相机拍摄高度和场景范围，当相机高度发生变化时，会导致低效且不实用的应用。</p><p>(2): 过去的方法：   - 地理上将场景分解为几个单元格，并为每个单元格训练一个子 NeRF，然后将它们合并。   - 在位置编码中并行应用平面和网格特征以实现高效建模。   - 问题：它们在基础高度上重建大规模场景，当导航到更近的地方以检查大规模户外场景的微观细节时，表现出过度模糊的伪影和不完整的重建。</p><p>(3): 本文提出的研究方法：   - 提出了一种端到端框架 AG-NeRF，通过合成基于场景不同高度的自由视角图像来降低构建良好重建的训练成本。   - 具体来说，为了解决从低高度（无人机级别）到高高度（卫星级别）的细节变化问题，开发了一种源图像选择方法和一种基于注意力的特征融合方法，从多高度图像中提取和融合目标视图最相关的特征，以实现高保真渲染。</p><p>(4): 本文方法在任务和性能上的表现：   - 在 56 Leonard 和 Transamerica 基准测试中取得了 SOTA 性能。   - 只需要半小时的训练时间即可达到与最新 BungeeNeRF 相当的竞争性 PSNR。   - 性能支持了他们的目标：降低构建良好重建的训练成本。</p><ol><li>方法：</li></ol><p>（1）：提出了一种端到端框架 AG-NeRF，通过合成基于场景不同高度的自由视角图像来降低构建良好重建的训练成本。</p><p>（2）：开发了一种源图像选择方法和一种基于注意力的特征融合方法，从多高度图像中提取和融合目标视图最相关的特征，以实现高保真渲染。</p><p>（3）：利用可训练的 U-Net 网络从源图像中提取特征图，并使用 Transformer 对提取的特征向量进行融合，以最大化融合特征与目标像素之间的相关性。</p><p>（4）：采用分层采样方法，使用粗略网络和精细网络同时优化，并使用基于注意力的特征融合方法将多高度图像中的特征融合起来。</p><ol><li>结论：</li></ol><p>（1）：本文针对不同高度拍摄的大场景渲染提出了端到端的 AG-NeRF 框架，降低了构建良好重建模型的训练成本。</p><p>（2）：创新点：提出了一种源图像选择方法和基于注意力的特征融合方法，从多高度图像中提取和融合目标视图最相关的特征，以实现高保真渲染。性能：在 56 Leonard 和 Transamerica 基准测试中取得了 SOTA 性能，只需要半小时的训练时间即可达到与最新 BungeeNeRF 相当的竞争性 PSNR。工作量：采用分层采样方法，使用粗略网络和精细网络同时优化，并使用基于注意力的特征融合方法将多高度图像中的特征融合起来。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-82fe2876dffe132719e410910e28492d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-fbedf0965ea4b6e30b80160a9ce71484.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b5a30ff8e4f41c8671a8c9f7dbcb45d2.jpg" align="middle"></details><h2 id="SLAIM-Robust-Dense-Neural-SLAM-for-Online-Tracking-and-Mapping"><a href="#SLAIM-Robust-Dense-Neural-SLAM-for-Online-Tracking-and-Mapping" class="headerlink" title="SLAIM: Robust Dense Neural SLAM for Online Tracking and Mapping"></a>SLAIM: Robust Dense Neural SLAM for Online Tracking and Mapping</h2><p><strong>Authors:Vincent Cartillier, Grant Schindler, Irfan Essa</strong></p><p>We present SLAIM - Simultaneous Localization and Implicit Mapping. We propose a novel coarse-to-fine tracking model tailored for Neural Radiance Field SLAM (NeRF-SLAM) to achieve state-of-the-art tracking performance. Notably, existing NeRF-SLAM systems consistently exhibit inferior tracking performance compared to traditional SLAM algorithms. NeRF-SLAM methods solve camera tracking via image alignment and photometric bundle-adjustment. Such optimization processes are difficult to optimize due to the narrow basin of attraction of the optimization loss in image space (local minima) and the lack of initial correspondences. We mitigate these limitations by implementing a Gaussian pyramid filter on top of NeRF, facilitating a coarse-to-fine tracking optimization strategy. Furthermore, NeRF systems encounter challenges in converging to the right geometry with limited input views. While prior approaches use a Signed-Distance Function (SDF)-based NeRF and directly supervise SDF values by approximating ground truth SDF through depth measurements, this often results in suboptimal geometry. In contrast, our method employs a volume density representation and introduces a novel KL regularizer on the ray termination distribution, constraining scene geometry to consist of empty space and opaque surfaces. Our solution implements both local and global bundle-adjustment to produce a robust (coarse-to-fine) and accurate (KL regularizer) SLAM solution. We conduct experiments on multiple datasets (ScanNet, TUM, Replica) showing state-of-the-art results in tracking and in reconstruction accuracy. </p><p><a href="http://arxiv.org/abs/2404.11419v1">PDF</a> </p><p><strong>Summary</strong><br>Nerf-SLAM 通过采用从粗到细的跟踪模型和 KL 正则化器，在跟踪性能和重建精度上实现了最先进的成绩。</p><p><strong>Key Takeaways</strong></p><ul><li>SLAIM 提出了一种从粗到细的跟踪模型以提高 NeRF-SLAM 的跟踪性能。</li><li>SLAIM 通过高斯金字塔滤波器实现从粗到细的跟踪优化策略。</li><li>NeRF 系统难以使用有限的输入视图收敛到正确的几何形状。</li><li>SLAIM 使用体积密度表示和一个新的 KL 正则化器来约束场景几何形状。</li><li>SLAIM 实现局部和全局捆绑调整以提高鲁棒性和准确性。</li><li>SLAIM 在多个数据集上进行了实验，在跟踪和重建精度上均显示出最先进的结果。</li><li>SLAIM 解决了 NeRF-SLAM 在传统 SLAM 算法下表现出较差的跟踪性能这一难题。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>标题：SLAIM：用于在线跟踪和建图的鲁棒稠密神经SLAM</p></li><li><p>作者：Vincent Cartillier、Grant Schindler、Irfan Essa</p></li><li><p>隶属关系：佐治亚理工学院</p></li><li><p>关键词：神经辐射场、SLAM、稠密建图、跟踪</p></li><li><p>论文链接：https://arxiv.org/abs/2404.11419，Github 代码链接：None</p></li><li><p>摘要：</p></li></ol><p>（1）研究背景：稠密视觉SLAM是3D计算机视觉中的一个长期问题，在自动驾驶、室内外机器人导航、虚拟现实和增强现实等领域有着广泛的应用。</p><p>（2）过去的方法及问题：传统的SLAM系统通过估计图像对应关系来开始，这些对应关系可能是稀疏的，例如匹配的特征点。神经辐射场SLAM（NeRF-SLAM）方法通过图像对齐和光度捆绑调整来解决相机跟踪问题。由于图像空间中优化损失的吸引域窄（局部极小值）以及缺乏初始对应关系，此类优化过程难以优化。</p><p>（3）提出的研究方法：本文提出了一种新的粗到细跟踪模型，专门针对NeRF-SLAM，以实现最先进的跟踪性能。此外，本文还引入了一种新的目标射线终止分布，并将其用于KL正则化器中，以约束场景几何由空空间和不透明表面组成。</p><p>（4）任务和性能：本文方法在ScanNet、TUM、Replica等多个数据集上进行了实验，在跟踪和重建精度方面均取得了最先进的成果。这些性能支持了本文的目标。</p><ol><li><p>方法：</p><pre><code>            (1):SLAIM 是一种用于稠密映射和跟踪的 RGB-D 输入流的 novel 方法；            (2):SLAIM 采用了一种从粗到精的跟踪模型，以实现最先进的跟踪性能；            (3):SLAIM 引入了一种新的目标射线终止分布，并将其用于 KL 正则化器中，以约束场景几何由空空间和不透明表面组成；            (4):SLAIM 在 ScanNet、TUM、Replica 等多个数据集上进行了实验，在跟踪和重建精度方面均取得了最先进的成果。</code></pre></li><li><p>结论：</p></li></ol><p>（1）本文的工作意义：本文提出了一种最先进的稠密实时 RGB-D NeRF-SLAM 系统 SLAIM，该系统具有最先进的相机跟踪和建图能力。</p><p>（2）本文的优缺点总结：    - 创新点：        - 采用从粗到精的跟踪模型，实现最先进的跟踪性能。        - 引入新的目标射线终止分布，并将其用于 KL 正则化器中，以约束场景几何由空空间和不透明表面组成。    - 性能：        - 在 ScanNet、TUM、Replica 等多个数据集上取得了最先进的跟踪和重建精度。    - 工作量：        - 内存效率高，在 Replica 和 ScanNet 数据集上与基准相比，跟踪和建图时间均有明显降低。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-486ca0b76c4db89899a0670269d00796.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f729a5308a9aa1435c3a0e2db312184f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ddcd1f27f832c7cfc1c274567204de22.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c7d35d3daa3f9540491cf1d974f07bc9.jpg" align="middle"></details><h2 id="RainyScape-Unsupervised-Rainy-Scene-Reconstruction-using-Decoupled-Neural-Rendering"><a href="#RainyScape-Unsupervised-Rainy-Scene-Reconstruction-using-Decoupled-Neural-Rendering" class="headerlink" title="RainyScape: Unsupervised Rainy Scene Reconstruction using Decoupled   Neural Rendering"></a>RainyScape: Unsupervised Rainy Scene Reconstruction using Decoupled   Neural Rendering</h2><p><strong>Authors:Xianqiang Lyu, Hui Liu, Junhui Hou</strong></p><p>We propose RainyScape, an unsupervised framework for reconstructing clean scenes from a collection of multi-view rainy images. RainyScape consists of two main modules: a neural rendering module and a rain-prediction module that incorporates a predictor network and a learnable latent embedding that captures the rain characteristics of the scene. Specifically, based on the spectral bias property of neural networks, we first optimize the neural rendering pipeline to obtain a low-frequency scene representation. Subsequently, we jointly optimize the two modules, driven by the proposed adaptive direction-sensitive gradient-based reconstruction loss, which encourages the network to distinguish between scene details and rain streaks, facilitating the propagation of gradients to the relevant components. Extensive experiments on both the classic neural radiance field and the recently proposed 3D Gaussian splatting demonstrate the superiority of our method in effectively eliminating rain streaks and rendering clean images, achieving state-of-the-art performance. The constructed high-quality dataset and source code will be publicly available. </p><p><a href="http://arxiv.org/abs/2404.11401v1">PDF</a> </p><p><strong>Summary</strong><br>基于神经网络的光谱偏差特性，RainyScape利用无监督框架重建干净场景，包含神经渲染模块和雨滴预测模块。</p><p><strong>Key Takeaways</strong></p><ul><li>利用神经网络的光谱偏差特性获得低频场景表示。</li><li>联合优化神经渲染模块和雨滴预测模块，以区分场景细节和雨滴条纹。</li><li>提出自适应方向敏感梯度重建损失，引导网络区分场景细节和雨滴条纹。</li><li>在经典神经辐射场和 3D 高斯斑点 splatting 数据集上均达到最先进的去雨性能。</li><li>提供高质量数据集和源代码，促进研究工作。</li><li>引入可学习潜在嵌入，捕捉场景的雨滴特征。</li><li>通过雨滴预测网络有效消除雨滴条纹，渲染干净图像。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: RainyScape: 无监督雨景重建使用解耦神经渲染</p></li><li><p>Authors: Xianqiang Lyu, Hui Liu, Junhui Hou</p></li><li><p>Affiliation: 香港城市大学计算机科学系</p></li><li><p>Keywords: Rainy scene reconstruction, Neural rendering, Unsupervised loss</p></li><li><p>Urls: https://arxiv.org/abs/2404.11401 , Github:None</p></li><li><p>Summary:</p></li></ol><p>(1):随着神经辐射场（NeRF）在图像合成中的广泛应用，当输入图像受到模糊、噪声或雨水等因素影响时，渲染结果不可避免地会产生明显的伪影。</p><p>(2):现有的方法针对特定任务提出了各种解决方案，但对于雨景重建任务，它们无法有效表示三维空间中稀疏且间歇性的降雨。</p><p>(3):本文提出RainyScape，一个解耦的神经渲染框架，它能够以无监督的方式从雨景图像中重建无雨场景。该框架通过神经渲染管道获得场景的低频表示，并使用可学习的雨水嵌入和预测器来表征雨水。此外，本文还提出了一个自适应角度估计策略和梯度旋转损失，以解耦场景高频细节和雨水条纹。</p><p>(4):在神经辐射场和三维高斯散射两种渲染技术上的广泛实验表明，该方法在有效消除雨水条纹和渲染清晰图像方面优于现有方法，达到了最先进的性能。</p><ol><li><p>方法：</p><p>（1）：提出RainyScape，一个解耦的神经渲染框架，可以无监督地从雨景图像中重建无雨场景；</p><p>（2）：通过神经渲染管道获得场景的低频表示，并使用可学习的雨水嵌入和预测器来表征雨水；</p><p>（3）：提出一个自适应角度估计策略和梯度旋转损失，以解耦场景高频细节和雨水条纹；</p><p>（4）：在神经辐射场和三维高斯散射两种渲染技术上的广泛实验表明，该方法在有效消除雨水条纹和渲染清晰图像方面优于现有方法，达到了最先进的性能。</p></li><li><p>结论：</p></li></ol><p>（1）：RainyScape的意义在于，它提出了一种无监督的解耦神经渲染框架，可以从雨景图像中重建无雨场景，有效解决了雨景重建中的雨水条纹去除问题，为雨景图像处理提供了新的思路和方法。</p><p>（2）：创新点：</p><ul><li><p>提出了一种解耦的神经渲染框架，通过低频场景表示、可学习的雨水嵌入和预测器以及自适应角度估计策略和梯度旋转损失，有效解耦了场景高频细节和雨水条纹。</p></li><li><p>性能：</p></li><li><p>在神经辐射场和三维高斯散射两种渲染技术上的广泛实验表明，该方法在有效消除雨水条纹和渲染清晰图像方面优于现有方法，达到了最先进的性能。</p></li><li><p>工作量：</p></li><li><p>该方法需要对雨景图像进行预处理，包括图像分割、雨水条纹检测和雨水嵌入提取等步骤，增加了计算量和时间开销。</p></li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-789763f7ebb6ec7a923539611ab1fe24.jpg" align="middle"><img src="https://picx.zhimg.com/v2-89f176b1378008d1c0b63c9241adfdb2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5f7fb8305c36c1fe2572adfd98b584f7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-76be36036e15658d754b57c4864b0abf.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3765b699865b1d89cc9f5f13f9843a0e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-34d10a80ece07ba92081dfc066d00427.jpg" align="middle"></details>## REACTO: Reconstructing Articulated Objects from a Single Video**Authors:Chaoyue Song, Jiacheng Wei, Chuan-Sheng Foo, Guosheng Lin, Fayao Liu**In this paper, we address the challenge of reconstructing general articulated 3D objects from a single video. Existing works employing dynamic neural radiance fields have advanced the modeling of articulated objects like humans and animals from videos, but face challenges with piece-wise rigid general articulated objects due to limitations in their deformation models. To tackle this, we propose Quasi-Rigid Blend Skinning, a novel deformation model that enhances the rigidity of each part while maintaining flexible deformation of the joints. Our primary insight combines three distinct approaches: 1) an enhanced bone rigging system for improved component modeling, 2) the use of quasi-sparse skinning weights to boost part rigidity and reconstruction fidelity, and 3) the application of geodesic point assignment for precise motion and seamless deformation. Our method outperforms previous works in producing higher-fidelity 3D reconstructions of general articulated objects, as demonstrated on both real and synthetic datasets. Project page: https://chaoyuesong.github.io/REACTO. [PDF](http://arxiv.org/abs/2404.11151v1) **Summary**对于一般性关节动作的3D物体，本文提出了一种新的变形模型，即准刚性混合蒙皮，以便从单个视频中进行全面重建。**Key Takeaways**- 提出一种新的变形模型，准刚性混合蒙皮，增强了零件刚性，同时保持关节柔性变形。- 采用增强骨骼绑定系统改善组件建模。- 使用准稀疏蒙皮权重提高零件刚性和重建保真度。- 应用测地线点赋值实现精确运动和无缝变形。- 在真实和合成数据集上，该方法在生成高保真一般性关节动作的3D重建方面优于先前的工作。- 该研究为一般性关节动作的3D物体重建提供了新的方法。- 该研究在计算机视觉和图形学领域具有潜在应用价值。- 该研究有助于推动相关领域的发展。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>标题：REACTO：从单一视频中重建铰接物体</p></li><li><p>作者：Chaoyue Song、Jiacheng Wei、Chuan Sheng Foo、Guosheng Lin、Fayao Liu</p></li><li><p>隶属：南洋理工大学</p></li><li><p>关键词：铰接物体重建、动态神经辐射场、准刚性混合蒙皮</p></li><li><p>论文链接：https://arxiv.org/abs/2404.11151, Github：无</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：重建铰接物体是计算机视觉中的一项重要任务，但现有方法在处理具有分段刚性的通用铰接物体时面临挑战。</p><p>（2）：过去方法：NASAM和PARIS等方法需要多视角图像或多视图图像，在实际应用中受限。</p><p>（3）：研究方法：本文提出了一种准刚性混合蒙皮变形模型，该模型通过增强骨骼装配系统、使用准稀疏蒙皮权重和应用测地线点分配来提高刚性并保持关节的灵活变形。</p><p>（4）：任务与性能：REACTO在真实和合成数据集上对通用铰接物体的3D重建任务中取得了较高的保真度，证明了其性能可以支持其目标。</p><p><strong>7. Methods：</strong></p><p>(1)：提出准刚性混合蒙皮变形模型，增强骨骼装配系统，使用准稀疏蒙皮权重，并应用测地线点分配；</p><p>(2)：构建REACTO框架，包括骨骼装配、蒙皮变形、体绘制和渲染模块；</p><p>(3)：使用基于神经辐射场的渲染器，从单一视频中重建铰接物体；</p><p>(4)：通过优化骨骼参数、蒙皮权重和神经辐射场参数，实现铰接物体的高保真重建；</p><p>(5)：在真实和合成数据集上进行实验，验证REACTO的有效性。</p><ol><li>结论：</li></ol><p>（1）：本工作提出REACTO，一种从单一视频中重建通用铰接3D物体的开创性方法，通过重新定义装配结构并采用准刚性混合蒙皮，实现了建模和精度的提升。准刚性混合蒙皮通过利用准稀疏蒙皮权重和测地线点分配，确保了每个部件的刚性，同时在关节处保持平滑变形。广泛的实验表明，REACTO在真实和合成数据集上都优于现有方法，保真度和细节方面都有所提升。</p><p>（2）：创新点：提出准刚性混合蒙皮变形模型，增强骨骼装配系统，使用准稀疏蒙皮权重，并应用测地线点分配；</p><p>性能：在真实和合成数据集上，REACTO在保真度和细节方面都优于现有方法；</p><p>工作量：与需要多视角或多视图图像的现有方法相比，REACTO只需单一视频即可重建铰接物体，工作量更小。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-b24d1992bf52c35d5d68092f3855e178.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-cc1782e8c3f880dfa4512201f4175379.jpg" align="middle"><img src="https://picx.zhimg.com/v2-46959553add30d1e8d2dff8cb9e56563.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f4000a7f506812312f58f8dd21486b3b.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF 方向最新论文已更新，请持续关注 Update in 2024-04-22  AG-NeRF Attention-guided Neural Radiance Fields for Multi-height   Large-scale Outdoor Scene Rendering</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/04/22/Paper/2024-04-22/3DGS/"/>
    <id>https://kedreamix.github.io/2024/04/22/Paper/2024-04-22/3DGS/</id>
    <published>2024-04-22T09:32:29.000Z</published>
    <updated>2024-04-22T09:32:29.438Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-22-更新"><a href="#2024-04-22-更新" class="headerlink" title="2024-04-22 更新"></a>2024-04-22 更新</h1><h2 id="Learn2Talk-3D-Talking-Face-Learns-from-2D-Talking-Face"><a href="#Learn2Talk-3D-Talking-Face-Learns-from-2D-Talking-Face" class="headerlink" title="Learn2Talk: 3D Talking Face Learns from 2D Talking Face"></a>Learn2Talk: 3D Talking Face Learns from 2D Talking Face</h2><p><strong>Authors:Yixiang Zhuang, Baoping Cheng, Yao Cheng, Yuntao Jin, Renshuai Liu, Chengyang Li, Xuan Cheng, Jing Liao, Juncong Lin</strong></p><p>Speech-driven facial animation methods usually contain two main classes, 3D and 2D talking face, both of which attract considerable research attention in recent years. However, to the best of our knowledge, the research on 3D talking face does not go deeper as 2D talking face, in the aspect of lip-synchronization (lip-sync) and speech perception. To mind the gap between the two sub-fields, we propose a learning framework named Learn2Talk, which can construct a better 3D talking face network by exploiting two expertise points from the field of 2D talking face. Firstly, inspired by the audio-video sync network, a 3D sync-lip expert model is devised for the pursuit of lip-sync between audio and 3D facial motion. Secondly, a teacher model selected from 2D talking face methods is used to guide the training of the audio-to-3D motions regression network to yield more 3D vertex accuracy. Extensive experiments show the advantages of the proposed framework in terms of lip-sync, vertex accuracy and speech perception, compared with state-of-the-arts. Finally, we show two applications of the proposed framework: audio-visual speech recognition and speech-driven 3D Gaussian Splatting based avatar animation. </p><p><a href="http://arxiv.org/abs/2404.12888v1">PDF</a> </p><p><strong>Summary</strong><br>通过借鉴2D说话人面部的唇形同步和言语感知领域的专业知识，提出了一种学习框架，可以构建更好的3D说话人面部网络。</p><p><strong>Key Takeaways</strong></p><ul><li>3D说话人面部研究在唇形同步和言语感知方面不如2D说话人面部研究深入。</li><li>Learn2Talk框架利用2D说话人面部领域的两个专业知识点来构建更好的3D说话人面部网络。</li><li>3D同步唇专家模型旨在实现音频和3D面部运动之间的唇形同步。</li><li>2D说话人面部方法中选择的教师模型用于指导音频到3D运动回归网络的训练，以提高3D顶点精度。</li><li>广泛的实验表明，该框架在唇形同步、顶点精度和言语感知方面优于现有技术。</li><li>该框架有语音-视觉语音识别和语音驱动3D高斯飞溅基于头像动画两个应用。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>标题：Learn2Talk：3D 说话人脸从 2D 说话人脸学习</p></li><li><p>作者：Yixiang Zhuang, Baoping Cheng, Yao Cheng, Yuntao Jin, Renshuai Liu, Chengyang Li, XuanCheng, Jing Liao, Juncong Lin</p></li><li><p>单位：暂缺</p></li><li><p>关键词：Speech-driven, 3D Facial Animation, 2D Talking face, Transformer, 3D Gaussian Splatting</p></li><li><p>论文链接：https://arxiv.org/abs/2404.12888v1Github：None</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：说话人脸动画方法通常包含 3D 和 2D 说话人脸两大类，近年来两者都备受研究关注。然而，据我们所知，3D 说话人脸的研究在唇形同步（lip-sync）和语音感知方面并未像 2D 说话人脸那样深入。</p><p>（2）：过去的方法及其问题：本文方法动机充分。</p><p>（3）：本文提出的研究方法：提出一个名为 Learn2Talk 的学习框架，该框架通过利用 2D 说话人脸领域的两个专业知识点来构建一个更好的 3D 说话人脸网络。首先，受音频视频同步网络的启发，设计了一个 3D 同步唇形专家模型，以追求音频和 3D 面部动作之间的唇形同步。其次，选择一个来自 2D 说话人脸方法的教师模型来指导音频到 3D 运动回归网络的训练，以产生更高的 3D 顶点精度。</p><p>（4）：方法性能：本文方法在唇形同步、顶点精度和语音感知方面均优于现有技术。这些性能可以支持其目标。</p><ol><li>方法：</li></ol><p>（1）：提出一个名为 Learn2Talk 的学习框架，该框架通过利用 2D 说话人脸领域的两个专业知识点来构建一个更好的 3D 说话人脸网络。</p><p>（2）：设计了一个 3D 同步唇形专家模型，以追求音频和 3D 面部动作之间的唇形同步。</p><p>（3）：选择一个来自 2D 说话人脸方法的教师模型来指导音频到 3D 运动回归网络的训练，以产生更高的 3D 顶点精度。</p><ol><li>结论：</li></ol><p>（1）：本文提出了一种名为 Learn2Talk 的学习框架，该框架通过利用 2D 说话人脸领域的两个专业知识点来构建一个更好的 3D 说话人脸网络，在唇形同步、顶点精度和语音感知方面均优于现有技术。</p><p>（2）：创新点：提出了一种新的 3D 说话人脸动画方法，该方法利用了 2D 说话人脸领域的专业知识；性能：在唇形同步、顶点精度和语音感知方面均优于现有技术；工作量：需要收集和标注大量的数据。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-c2e8566372db83537dc565617387f4cf.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c36e0e4bd338738e2a5420e68cd6ab5d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-3fe7530e7260eff001a6736622671663.jpg" align="middle"><img src="https://pica.zhimg.com/v2-3f8c50de092534c8ec8b833626c35e42.jpg" align="middle"></details>## Contrastive Gaussian Clustering: Weakly Supervised 3D Scene Segmentation**Authors:Myrna C. Silva, Mahtab Dahaghin, Matteo Toso, Alessio Del Bue**We introduce Contrastive Gaussian Clustering, a novel approach capable of provide segmentation masks from any viewpoint and of enabling 3D segmentation of the scene. Recent works in novel-view synthesis have shown how to model the appearance of a scene via a cloud of 3D Gaussians, and how to generate accurate images from a given viewpoint by projecting on it the Gaussians before $\alpha$ blending their color. Following this example, we train a model to include also a segmentation feature vector for each Gaussian. These can then be used for 3D scene segmentation, by clustering Gaussians according to their feature vectors; and to generate 2D segmentation masks, by projecting the Gaussians on a plane and $\alpha$ blending over their segmentation features. Using a combination of contrastive learning and spatial regularization, our method can be trained on inconsistent 2D segmentation masks, and still learn to generate segmentation masks consistent across all views. Moreover, the resulting model is extremely accurate, improving the IoU accuracy of the predicted masks by $+8\%$ over the state of the art. Code and trained models will be released soon. [PDF](http://arxiv.org/abs/2404.12784v1) **Summary**使用来自不同视角的对比高斯聚类实现 3D 场景分割。**Key Takeaways**- 提出一种新的对比高斯聚类方法，能够从任何视角提供分割掩模，并实现场景的 3D 分割。- 受新视角合成领域研究的启发，使用 3D 高斯云建模场景的外观。- 通过将高斯投影到给定视点并对其颜色进行α混合，从给定视点生成准确的图像。- 训练模型，使每个高斯都包含一个分割特征向量。- 通过根据其特征向量对高斯进行聚类，可用于 3D 场景分割；通过将高斯投影到平面上并对其分割特征进行 α 混合，可生成 2D 分割掩模。- 使用对比学习和空间正则化的组合，可以在不一致的 2D 分割掩模上训练我们的方法，并学习生成在所有视图中都一致的分割掩模。- 所提出的方法非常准确，与现有技术相比，预测掩模的 IoU 准确度提高了 8%。- 代码和训练好的模型即将发布。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>标题：对比高斯聚类：弱监督 3D 场景分割</p></li><li><p>作者：Myrna C. Silva、Mahtab Dahaghin、Matteo Toso、Alessio Del Bue</p></li><li><p>单位：意大利理工学院模式分析与计算机视觉（PAVIS）</p></li><li><p>关键词：3D 高斯散射、3D 分割、对比学习</p></li><li><p>论文链接：arXiv:2404.12784v1 [cs.CV]   Github 链接：无</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：近年来，新视角合成领域的研究表明，可以通过 3D 高斯云对场景的外观进行建模，并通过在给定视角上投影高斯并 α 混合其颜色来生成准确的图像。</p><p>（2）：过去方法与问题：高斯分组和 LangSplat 等方法存在以下问题：   - 训练和评估需要大量 GPU 内存，导致某些场景无法处理。   - 无法从任意视角提供分割掩码，也无法实现场景的 3D 分割。</p><p>（3）：研究方法：本文提出对比高斯聚类方法，该方法通过以下步骤实现 3D 场景分割和 2D 分割掩码预测：   - 训练模型为每个高斯体添加分割特征向量。   - 根据特征向量对高斯体进行聚类，实现 3D 场景分割。   - 将高斯体投影到平面上并 α 混合其分割特征，生成 2D 分割掩码。   - 使用对比学习和空间正则化，在不一致的 2D 分割掩码上训练模型，生成跨所有视角一致的分割掩码。</p><p>（4）：性能与目标：   - 任务：3D 场景分割和 2D 分割掩码预测。   - 性能：IoU 准确率比现有技术提高 +8%，表明该方法能够有效实现其目标。</p><ol><li>方法：</li></ol><p>（1）：将场景表示为 3D 高斯体集合，编码几何、外观和实例分割信息；</p><p>（2）：使用基础模型生成 2D 分割掩码；</p><p>（3）：优化 3D 高斯体，最小化渲染图像和真实图像之间的差异；</p><p>（4）：使用对比分割损失监督 3D 特征场；</p><p>（5）：引入正则化项，强制高斯体在欧几里得和分割特征空间中的距离相关；</p><p>（6）：渲染 2D 特征图，根据对应的 2D 分割掩码对渲染特征进行聚类，计算对比聚类损失；</p><p>（7）：最大化同一分割内特征之间的相似度，最小化不同分割内的特征相似度。</p><p><strong>8. 结论</strong></p><p><strong>(1)</strong> 本工作的主要意义在于：</p><p>提出了对比高斯聚类方法，实现了 3D 场景分割和 2D 分割掩码预测，有效提高了分割精度。</p><p><strong>(2)</strong> 本文优缺点总结（创新点、性能、工作量）：</p><p><strong>创新点：</strong></p><ul><li>引入对比学习和空间正则化，提高了分割掩码的一致性。</li><li>使用 3D 高斯体表示场景，编码几何、外观和实例分割信息。</li></ul><p><strong>性能：</strong></p><ul><li>IoU 准确率比现有技术提高 +8%，分割精度高。</li></ul><p><strong>工作量：</strong></p><ul><li>训练和评估需要大量 GPU 内存，大场景处理困难。</li><li>无法从任意视角提供分割掩码，无法实现场景的完整 3D 分割。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-252e679c7e0a5cfc8056b41c43d99b59.jpg" align="middle"><img src="https://picx.zhimg.com/v2-668e640c91611b7b91220b00abd05f4e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-03dada656b628530891ef19dcbebedba.jpg" align="middle"></details>## RainyScape: Unsupervised Rainy Scene Reconstruction using Decoupled   Neural Rendering**Authors:Xianqiang Lyu, Hui Liu, Junhui Hou**We propose RainyScape, an unsupervised framework for reconstructing clean scenes from a collection of multi-view rainy images. RainyScape consists of two main modules: a neural rendering module and a rain-prediction module that incorporates a predictor network and a learnable latent embedding that captures the rain characteristics of the scene. Specifically, based on the spectral bias property of neural networks, we first optimize the neural rendering pipeline to obtain a low-frequency scene representation. Subsequently, we jointly optimize the two modules, driven by the proposed adaptive direction-sensitive gradient-based reconstruction loss, which encourages the network to distinguish between scene details and rain streaks, facilitating the propagation of gradients to the relevant components. Extensive experiments on both the classic neural radiance field and the recently proposed 3D Gaussian splatting demonstrate the superiority of our method in effectively eliminating rain streaks and rendering clean images, achieving state-of-the-art performance. The constructed high-quality dataset and source code will be publicly available. [PDF](http://arxiv.org/abs/2404.11401v1) **Summary**雨景重建：无监督地从多视角雨景图重建干净场景。**Key Takeaways**- 提出无监督框架 RainyScape，重建干净场景。- RainyScape 由神经渲染和降雨预测模块组成。- 降雨预测模块包含预测网络和可学习潜嵌入，捕捉场景的降雨特征。- 基于神经网络的光谱偏差属性，优化神经渲染管道，获得低频场景表示。- 利用自适应方向敏感梯度重建损失，优化两个模块，区分场景细节和雨痕。- 在神经辐射场和 3D 高斯喷溅中进行的实验表明，该方法能有效消除雨痕、渲染干净图像，达到最先进性能。- 将公开构建高质量数据集和源代码。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>标题：RainyScape：基于解耦神经渲染的无监督雨景重建</p></li><li><p>作者：Xianqiang Lyu, Hui Liu, Junhui Hou</p></li><li><p>单位：香港城市大学计算机科学系</p></li><li><p>关键词：雨景重建、神经渲染、无监督损失</p></li><li><p>论文链接：xxx，Github 链接：None</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：神经辐射场（NeRF）在学习场景的连续体积表示方面取得了突破性进展，但当输入图像因模糊、噪声或雨水等因素而退化时，渲染结果不可避免地会出现明显伪影。</p><p>（2）：过去方法：现有方法针对不同的退化因素提出了特定任务的解决方案，但针对雨景重建任务的方法较少，且难以通过附加神经渲染场来表示雨水。</p><p>（3）：研究方法：本文提出 RainyScape，一个解耦的神经渲染框架，能够从雨景图像中无监督地重建无雨场景。该框架包括一个神经渲染模块和一个雨滴预测模块，通过学习雨滴嵌入和使用预测器来预测雨滴条纹，并提出自适应角度估计策略和梯度旋转损失来解耦场景高频细节和雨滴条纹。</p><p>（4）：方法性能：在经典神经辐射场和最近提出的 3D 高斯 splatting 上的广泛实验表明，该方法在有效消除雨滴条纹和渲染清晰图像方面优于现有方法，达到最先进的性能。</p><ol><li>方法：</li></ol><p>（1）：提出 RainyScape，一个解耦的神经渲染框架，能够从雨景图像中无监督地重建无雨场景。</p><p>（2）：该框架包括一个神经渲染模块和一个雨滴预测模块，通过学习雨滴嵌入和使用预测器来预测雨滴条纹。</p><p>（3）：提出自适应角度估计策略和梯度旋转损失来解耦场景高频细节和雨滴条纹。</p><ol><li>结论：<pre><code>            （1）：RainyScape 在雨景重建领域具有重要意义，它首次提出了一个解耦神经渲染框架，能够从雨景图像中无监督地重建无雨场景。 该框架通过将场景高频细节和雨滴条纹解耦，有效地消除了雨滴条纹，并渲染出清晰的图像。            （2）：创新点：RainyScape 创新性地提出了一个解耦神经渲染框架，将场景高频细节和雨滴条纹解耦，有效地消除了雨滴条纹，并渲染出清晰的图像。            性能：RainyScape 在经典神经辐射场和最近提出的 3D 高斯 splatting 上的广泛实验表明，该方法在有效消除雨滴条纹和渲染清晰图像方面优于现有方法，达到最先进的性能。            工作量：RainyScape 的工作量中等，需要训练神经渲染模块和雨滴预测模块，并提出自适应角度估计策略和梯度旋转损失来解耦场景高频细节和雨滴条纹。</code></pre></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-789763f7ebb6ec7a923539611ab1fe24.jpg" align="middle"><img src="https://picx.zhimg.com/v2-89f176b1378008d1c0b63c9241adfdb2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5f7fb8305c36c1fe2572adfd98b584f7.jpg" align="middle"><img src="https://pica.zhimg.com/v2-76be36036e15658d754b57c4864b0abf.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3765b699865b1d89cc9f5f13f9843a0e.jpg" align="middle"><img src="https://pica.zhimg.com/v2-34d10a80ece07ba92081dfc066d00427.jpg" align="middle"></details><h2 id="DeblurGS-Gaussian-Splatting-for-Camera-Motion-Blur"><a href="#DeblurGS-Gaussian-Splatting-for-Camera-Motion-Blur" class="headerlink" title="DeblurGS: Gaussian Splatting for Camera Motion Blur"></a>DeblurGS: Gaussian Splatting for Camera Motion Blur</h2><p><strong>Authors:Jeongtaek Oh, Jaeyoung Chung, Dongwoo Lee, Kyoung Mu Lee</strong></p><p>Although significant progress has been made in reconstructing sharp 3D scenes from motion-blurred images, a transition to real-world applications remains challenging. The primary obstacle stems from the severe blur which leads to inaccuracies in the acquisition of initial camera poses through Structure-from-Motion, a critical aspect often overlooked by previous approaches. To address this challenge, we propose DeblurGS, a method to optimize sharp 3D Gaussian Splatting from motion-blurred images, even with the noisy camera pose initialization. We restore a fine-grained sharp scene by leveraging the remarkable reconstruction capability of 3D Gaussian Splatting. Our approach estimates the 6-Degree-of-Freedom camera motion for each blurry observation and synthesizes corresponding blurry renderings for the optimization process. Furthermore, we propose Gaussian Densification Annealing strategy to prevent the generation of inaccurate Gaussians at erroneous locations during the early training stages when camera motion is still imprecise. Comprehensive experiments demonstrate that our DeblurGS achieves state-of-the-art performance in deblurring and novel view synthesis for real-world and synthetic benchmark datasets, as well as field-captured blurry smartphone videos. </p><p><a href="http://arxiv.org/abs/2404.11358v2">PDF</a> </p><p><strong>Summary</strong><br>从模糊运动图像重建清晰 3D 场景方法，优化 3D 高斯投射，实现精确摄像机位姿初始化。</p><p><strong>Key Takeaways</strong></p><ul><li>DeblurGS 优化高斯投射，提高运动模糊图像 3D 重建精度。</li><li>利用高斯投射的重建能力，还原精细锐利场景。</li><li>估计每幅模糊图像的 6 自由度摄像机运动，生成模糊渲染用于优化。</li><li>高斯密度退火策略防止错误位置生成不准确的高斯。</li><li>DeblurGS 在去模糊和合成新视角方面取得了最先进的性能。</li><li>适用于真实世界和合成基准数据集，以及现场拍摄的模糊智能手机视频。</li><li>DeblurGS 极大地扩展了运动模糊图像的 3D 重建的实际应用。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: DeblurGS: 高斯溅射相机运动模糊 (DeblurGS: Gaussian Splatting for Camera Motion Blur)</p></li><li><p>Authors: Jeongtaek Oh, Jaeyoung Chung, Dongwoo Lee, and Kyoung Mu Lee</p></li><li><p>Affiliation: 首尔国立大学人工智能与信息处理研究所 (IPAI, Seoul National University)</p></li><li><p>Keywords: 3D Gaussian Splatting · Camera Motion Deblurring</p></li><li><p>Urls: None, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 尽管从运动模糊图像重建清晰的 3D 场景方面取得了重大进展，但向实际应用的过渡仍然具有挑战性。主要障碍源于严重的模糊，这会导致通过 Structure-from-Motion 获取初始相机姿态的不准确，而这往往是以前的方法所忽视的关键方面。</p><p>(2): 过去的方法主要集中于模糊图像的去模糊处理，但对于初始相机姿态的噪声初始化不鲁棒。</p><p>(3): 本文提出 DeblurGS，这是一种从运动模糊图像优化清晰的 3D 高斯溅射的方法，即使在噪声相机姿态初始化的情况下也是如此。我们利用 3D 高斯溅射的出色重建能力来恢复细粒度的清晰场景。我们的方法估计每个模糊观测的 6 自由度相机运动，并为优化过程合成相应的模糊渲染。此外，我们提出了高斯致密化退火策略，以防止在相机运动仍然不精确的早期训练阶段在错误的位置生成不准确的高斯。</p><p>(4): 综合实验表明，我们的 DeblurGS 在真实世界和合成基准数据集以及现场捕获的模糊智能手机视频的去模糊和新视图合成方面实现了最先进的性能。</p><ol><li>方法：</li></ol><p>（1）：提出 DeblurGS，一种从运动模糊图像优化清晰的 3D 高斯溅射的方法；</p><p>（2）：利用 3D 高斯溅射的重建能力恢复细粒度的清晰场景；</p><p>（3）：估计每个模糊观测的 6 自由度相机运动，并合成相应的模糊渲染；</p><p>（4）：提出高斯致密化退火策略，防止在相机运动不精确的早期训练阶段生成不准确的高斯。</p><ol><li>结论：</li></ol><p>（1）：本文提出了一种从运动模糊图像优化清晰的 3D 高斯溅射的方法，即使在噪声相机姿态初始化的情况下也是如此。该方法利用 3D 高斯溅射的出色重建能力来恢复细粒度的清晰场景，估计每个模糊观测的 6 自由度相机运动，并为优化过程合成相应的模糊渲染。此外，该方法提出了高斯致密化退火策略，以防止在相机运动仍然不精确的早期训练阶段在错误的位置生成不准确的高斯。综合实验表明，该方法在真实世界和合成基准数据集以及现场捕获的模糊智能手机视频的去模糊和新视图合成方面实现了最先进的性能。</p><p>（2）：创新点：利用 3D 高斯溅射的重建能力恢复细粒度的清晰场景，即使在噪声相机姿态初始化的情况下也是如此；</p><p>性能：在真实世界和合成基准数据集以及现场捕获的模糊智能手机视频的去模糊和新视图合成方面实现了最先进的性能；</p><p>工作量：该方法需要估计每个模糊观测的 6 自由度相机运动，并为优化过程合成相应的模糊渲染，这可能会增加计算成本。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-d1b62fa212aabdf515b9baf8fdc306be.jpg" align="middle"><img src="https://pica.zhimg.com/v2-32c4f56eaf456fe86ff5f42abfbd6ffb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-50b9e9cff40ee36449b6b3559539186a.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS 方向最新论文已更新，请持续关注 Update in 2024-04-22  Learn2Talk 3D Talking Face Learns from 2D Talking Face</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
</feed>
