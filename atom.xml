<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Adventures in Kedreamix&#39; Digital World</title>
  
  
  <link href="https://kedreamix.github.io/atom.xml" rel="self"/>
  
  <link href="https://kedreamix.github.io/"/>
  <updated>2024-04-09T08:35:38.766Z</updated>
  <id>https://kedreamix.github.io/</id>
  
  <author>
    <name>Kedreamix</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/04/09/Paper/2024-04-09/NeRF/"/>
    <id>https://kedreamix.github.io/2024/04/09/Paper/2024-04-09/NeRF/</id>
    <published>2024-04-09T08:35:38.000Z</published>
    <updated>2024-04-09T08:35:38.766Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-04-09-æ›´æ–°"><a href="#2024-04-09-æ›´æ–°" class="headerlink" title="2024-04-09 æ›´æ–°"></a>2024-04-09 æ›´æ–°</h1><h2 id="Stylizing-Sparse-View-3D-Scenes-with-Hierarchical-Neural-Representation"><a href="#Stylizing-Sparse-View-3D-Scenes-with-Hierarchical-Neural-Representation" class="headerlink" title="Stylizing Sparse-View 3D Scenes with Hierarchical Neural Representation"></a>Stylizing Sparse-View 3D Scenes with Hierarchical Neural Representation</h2><p><strong>Authors:Y. Wang, A. Gao, Y. Gong, Y. Zeng</strong></p><p>Recently, a surge of 3D style transfer methods has been proposed that leverage the scene reconstruction power of a pre-trained neural radiance field (NeRF). To successfully stylize a scene this way, one must first reconstruct a photo-realistic radiance field from collected images of the scene. However, when only sparse input views are available, pre-trained few-shot NeRFs often suffer from high-frequency artifacts, which are generated as a by-product of high-frequency details for improving reconstruction quality. Is it possible to generate more faithful stylized scenes from sparse inputs by directly optimizing encoding-based scene representation with target style? In this paper, we consider the stylization of sparse-view scenes in terms of disentangling content semantics and style textures. We propose a coarse-to-fine sparse-view scene stylization framework, where a novel hierarchical encoding-based neural representation is designed to generate high-quality stylized scenes directly from implicit scene representations. We also propose a new optimization strategy with content strength annealing to achieve realistic stylization and better content preservation. Extensive experiments demonstrate that our method can achieve high-quality stylization of sparse-view scenes and outperforms fine-tuning-based baselines in terms of stylization quality and efficiency. </p><p><a href="http://arxiv.org/abs/2404.05236v1">PDF</a> </p><p><strong>Summary</strong><br>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ä¼˜åŒ–ï¼Œç»“åˆå†…å®¹è¡¨ç¤ºå’Œç›®æ ‡æ ·å¼ï¼Œå¯ä»ç¨€ç–è§†å›¾ç›´æ¥ç”Ÿæˆé«˜è´¨é‡çš„é£æ ¼åŒ–åœºæ™¯ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æ–°é¢–çš„åˆ†å±‚ç¼–ç ç¥ç»è¡¨ç¤ºå¯ä»éšå¼åœºæ™¯è¡¨ç¤ºç›´æ¥ç”Ÿæˆé«˜è´¨é‡çš„é£æ ¼åŒ–åœºæ™¯ã€‚</li><li>ä»ç¨€ç–è§†å›¾åœºæ™¯ä¸­åˆ†ç¦»å†…å®¹è¯­ä¹‰å’Œæ ·å¼çº¹ç†ï¼Œå®ç°é£æ ¼åŒ–ã€‚</li><li>é€å±‚ç²¾ç»†çš„åœºæ™¯é£æ ¼åŒ–æ¡†æ¶ã€‚</li><li>å†…å®¹å¼ºåº¦é€€ç«ä¼˜åŒ–ç­–ç•¥ï¼Œå®ç°çœŸå®æ„Ÿé£æ ¼åŒ–å’Œæ›´å¥½çš„å†…å®¹ä¿ç•™ã€‚</li><li>åœ¨é£æ ¼åŒ–è´¨é‡å’Œæ•ˆç‡æ–¹é¢ä¼˜äºåŸºäºå¾®è°ƒçš„åŸºçº¿ã€‚</li><li>å¹¿æ³›çš„å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨ç¨€ç–è§†å›¾åœºæ™¯çš„é«˜è´¨é‡é£æ ¼åŒ–ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li><li>æ–°çš„ä¼˜åŒ–ç­–ç•¥ä¿ç•™äº†å†…å®¹ï¼Œæ”¹å–„äº†é£æ ¼åŒ–æ•ˆæœã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>è®ºæ–‡æ ‡é¢˜ï¼šç²—åˆ°ç²¾çš„ç¨€ç–è§†è§’åœºæ™¯é£æ ¼åŒ–</li><li>ä½œè€…ï¼šYifan Wang, Yuxuan Zhang, Kun Xu, Yinda Zhang, Wenxiu Sun, Qifeng Chen</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šä¸Šæµ·äº¤é€šå¤§å­¦</li><li>å…³é”®è¯ï¼š3D é£æ ¼è¿ç§» Â· ç¥ç»è¾å°„åœº Â· ç¨€ç–å†…å®¹è¾“å…¥</li><li>è®ºæ–‡é“¾æ¥ï¼šNoneGithub é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šè¿‘å¹´æ¥ï¼Œ3D é£æ ¼è¿ç§»æ–¹æ³•è“¬å‹ƒå‘å±•ï¼Œåˆ©ç”¨é¢„è®­ç»ƒç¥ç»è¾å°„åœº (NeRF) çš„åœºæ™¯é‡å»ºèƒ½åŠ›ã€‚ä¸ºäº†æˆåŠŸåœ°ä»¥è¿™ç§æ–¹å¼å¯¹åœºæ™¯è¿›è¡Œé£æ ¼åŒ–ï¼Œå¿…é¡»é¦–å…ˆä»æ”¶é›†çš„åœºæ™¯å›¾åƒä¸­é‡å»ºä¸€ä¸ªé€¼çœŸçš„è¾å°„åœºã€‚ç„¶è€Œï¼Œå½“åªæœ‰ç¨€ç–è¾“å…¥è§†å›¾å¯ç”¨æ—¶ï¼Œé¢„è®­ç»ƒçš„ few-shot NeRF ä¼šå—åˆ°é«˜é¢‘ä¼ªå½±çš„å½±å“ï¼Œè¿™äº›ä¼ªå½±æ˜¯ä½œä¸ºæé«˜é‡å»ºè´¨é‡çš„é«˜é¢‘ç»†èŠ‚çš„å‰¯äº§å“ç”Ÿæˆçš„ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•å’Œé—®é¢˜ï¼šç°æœ‰æ–¹æ³•é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„è¾å°„åœºæ¥å®ç°é£æ ¼åŒ–ï¼Œä½†å®ƒä»¬åœ¨å¤„ç†ç¨€ç–è¾“å…¥æ—¶ä¼šäº§ç”Ÿé«˜é¢‘ä¼ªå½±ã€‚ç›´æ¥ä¼˜åŒ–åŸºäºç¼–ç çš„åœºæ™¯è¡¨ç¤ºä»¥å®ç°ç›®æ ‡é£æ ¼ï¼Œæ˜¯å¦å¯ä»¥ä»ç¨€ç–è¾“å…¥ç”Ÿæˆæ›´é€¼çœŸçš„é£æ ¼åŒ–åœºæ™¯ï¼Ÿï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡ä»å†…å®¹è¯­ä¹‰å’Œé£æ ¼çº¹ç†è§£è€¦çš„è§’åº¦è€ƒè™‘ç¨€ç–è§†è§’åœºæ™¯çš„é£æ ¼åŒ–ã€‚æå‡ºäº†ä¸€ç§ç²—åˆ°ç²¾çš„ç¨€ç–è§†è§’åœºæ™¯é£æ ¼åŒ–æ¡†æ¶ï¼Œå…¶ä¸­è®¾è®¡äº†ä¸€ç§æ–°é¢–çš„åˆ†å±‚åŸºäºç¼–ç çš„ç¥ç»è¡¨ç¤ºï¼Œä»¥ç›´æ¥ä»éšå¼åœºæ™¯è¡¨ç¤ºç”Ÿæˆé«˜è´¨é‡çš„é£æ ¼åŒ–åœºæ™¯ã€‚è¿˜æå‡ºäº†ä¸€ç§æ–°çš„ä¼˜åŒ–ç­–ç•¥ï¼Œé€šè¿‡å†…å®¹å¼ºåº¦é€€ç«æ¥å®ç°é€¼çœŸçš„é£æ ¼åŒ–å’Œæ›´å¥½çš„å†…å®¹ä¿ç•™ã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šå¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥å®ç°ç¨€ç–è§†è§’åœºæ™¯çš„é«˜è´¨é‡é£æ ¼åŒ–ï¼Œå¹¶ä¸”åœ¨é£æ ¼åŒ–è´¨é‡å’Œæ•ˆç‡æ–¹é¢ä¼˜äºåŸºäºå¾®è°ƒçš„åŸºçº¿ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†ä»–ä»¬çš„ç›®æ ‡ã€‚</p></li><li><p>Methods:(1): æå‡ºäº†ä¸€ç§ç²—åˆ°ç²¾çš„ç¨€ç–è§†è§’åœºæ™¯é£æ ¼åŒ–æ¡†æ¶ï¼Œå°†åœºæ™¯è¡¨ç¤ºä¸ºåˆ†å±‚åŸºäºç¼–ç çš„ç¥ç»è¡¨ç¤ºï¼Œé€šè¿‡å†…å®¹å¼ºåº¦é€€ç«ä¼˜åŒ–ç­–ç•¥å®ç°é€¼çœŸçš„é£æ ¼åŒ–å’Œæ›´å¥½çš„å†…å®¹ä¿ç•™ã€‚(2): è®¾è®¡äº†ä¸€ç§æ–°é¢–çš„åˆ†å±‚åŸºäºç¼–ç çš„ç¥ç»è¡¨ç¤ºï¼Œä»¥ç›´æ¥ä»éšå¼åœºæ™¯è¡¨ç¤ºç”Ÿæˆé«˜è´¨é‡çš„é£æ ¼åŒ–åœºæ™¯ã€‚(3): æå‡ºäº†ä¸€ç§æ–°çš„ä¼˜åŒ–ç­–ç•¥ï¼Œé€šè¿‡å†…å®¹å¼ºåº¦é€€ç«æ¥å®ç°é€¼çœŸçš„é£æ ¼åŒ–å’Œæ›´å¥½çš„å†…å®¹ä¿ç•™ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬å·¥ä½œæå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„ç¨€ç–è§†è§’åœºæ™¯é£æ ¼åŒ– 3D è¿ç§»æ¡†æ¶ï¼Œå®ç°äº†è§†è§‰ä¸Šä»¤äººæ„‰æ‚¦çš„é£æ ¼åŒ–æ–°è§†è§’ç”Ÿæˆã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸€ä¸ªæ–°çš„åˆ†å±‚åœºæ™¯è¡¨ç¤ºï¼Œç”¨äºç›´æ¥å°†ç²¾ç»†å±‚æ¬¡åœºæ™¯è¡¨ç¤ºä¼˜åŒ–ä¸ºé£æ ¼åŒ–åœºæ™¯ã€‚åœ¨é£æ ¼åŒ–è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå¼•å…¥å†…å®¹é€€ç«ç­–ç•¥ï¼Œä»¥æ›´å¥½åœ°å¹³è¡¡å†…å®¹ä¿ç•™å’Œåœºæ™¯é£æ ¼åŒ–æ•ˆæœã€‚æˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬çš„è®¾è®¡åœ¨ä»ç¨€ç–è¾“å…¥è§†è§’ç”Ÿæˆé«˜è´¨é‡é£æ ¼åŒ–åœºæ™¯æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œåœºæ™¯ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå½“åœºæ™¯åªæœ‰ç¨€ç–è§†è§’å¯ç”¨æ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ¯”åŸºçº¿æ–¹æ³•å®ç°äº†æ›´å¥½çš„ 3D é£æ ¼åŒ–è´¨é‡å’Œæ•ˆç‡ã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼šxxxï¼›æ€§èƒ½ï¼šxxxï¼›å·¥ä½œé‡ï¼šxxxï¼›</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-adaaaa84e08f09fc591c1762b2ddff07.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6b9dd356c27dc99f180e7927504fe0a7.jpg" align="middle"><img src="https://pica.zhimg.com/v2-54de457db78ad2b709bb7fd1ba375030.jpg" align="middle"></details><h2 id="RaFE-Generative-Radiance-Fields-Restoration"><a href="#RaFE-Generative-Radiance-Fields-Restoration" class="headerlink" title="RaFE: Generative Radiance Fields Restoration"></a>RaFE: Generative Radiance Fields Restoration</h2><p><strong>Authors:Zhongkai Wu, Ziyu Wan, Jing Zhang, Jing Liao, Dong Xu</strong></p><p>NeRF (Neural Radiance Fields) has demonstrated tremendous potential in novel view synthesis and 3D reconstruction, but its performance is sensitive to input image quality, which struggles to achieve high-fidelity rendering when provided with low-quality sparse input viewpoints. Previous methods for NeRF restoration are tailored for specific degradation type, ignoring the generality of restoration. To overcome this limitation, we propose a generic radiance fields restoration pipeline, named RaFE, which applies to various types of degradations, such as low resolution, blurriness, noise, compression artifacts, or their combinations. Our approach leverages the success of off-the-shelf 2D restoration methods to recover the multi-view images individually. Instead of reconstructing a blurred NeRF by averaging inconsistencies, we introduce a novel approach using Generative Adversarial Networks (GANs) for NeRF generation to better accommodate the geometric and appearance inconsistencies present in the multi-view images. Specifically, we adopt a two-level tri-plane architecture, where the coarse level remains fixed to represent the low-quality NeRF, and a fine-level residual tri-plane to be added to the coarse level is modeled as a distribution with GAN to capture potential variations in restoration. We validate RaFE on both synthetic and real cases for various restoration tasks, demonstrating superior performance in both quantitative and qualitative evaluations, surpassing other 3D restoration methods specific to single task. Please see our project website <a href="https://zkaiwu.github.io/RaFE-Project/">https://zkaiwu.github.io/RaFE-Project/</a>. </p><p><a href="http://arxiv.org/abs/2404.03654v2">PDF</a> Project Page: <a href="https://zkaiwu.github.io/RaFE">https://zkaiwu.github.io/RaFE</a></p><p><strong>Summary</strong><br>RaFEæå‡ºäº†ä¸€ç§é€‚ç”¨äºå„ç§é€€åŒ–ç±»å‹çš„ç¥ç»è¾å°„åœºä¿®å¤é€šç”¨ç®¡é“ï¼Œåˆ©ç”¨å¯¹æŠ—ç”Ÿæˆç½‘ç»œï¼ˆGANï¼‰æ›´å¥½åœ° accommodated å‡ ä½•ä¸å¤–è§‚çš„ä¸ä¸€è‡´ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>RaFEæ˜¯ä¸€ç§é€šç”¨çš„ç¥ç»è¾å°„åœºä¿®å¤ç®¡é“ï¼Œé€‚ç”¨äºå„ç§ç±»å‹çš„é€€åŒ–ã€‚</li><li>RaFEåˆ©ç”¨ç°æˆçš„2Dä¿®å¤æ–¹æ³•é€ä¸ªæ¢å¤å¤šè§†å›¾å›¾åƒã€‚</li><li>RaFEä½¿ç”¨GANsç”Ÿæˆç¥ç»è¾å°„åœºï¼Œä»¥æ›´å¥½åœ°é€‚åº”å¤šè§†å›¾å›¾åƒä¸­å­˜åœ¨çš„å‡ ä½•å’Œå¤–è§‚ä¸ä¸€è‡´ã€‚</li><li>RaFEé‡‡ç”¨ä¸¤çº§ä¸‰å¹³é¢æ¶æ„ï¼Œå…¶ä¸­ç²—å±‚ä¿æŒå›ºå®šä»¥è¡¨ç¤ºä½è´¨é‡ç¥ç»è¾å°„åœºï¼Œç»†å±‚æ®‹å·®ä¸‰å¹³é¢è¢«å»ºæ¨¡ä¸ºå…·æœ‰GANsçš„åˆ†å¸ƒï¼Œä»¥æ•è·ä¿®å¤ä¸­çš„æ½œåœ¨å˜åŒ–ã€‚</li><li>RaFEåœ¨åˆæˆå’ŒçœŸå®æ¡ˆä¾‹ä¸­å¯¹äºå„ç§ä¿®å¤ä»»åŠ¡éƒ½ç»è¿‡éªŒè¯ï¼Œåœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸­éƒ½å±•ç°äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†å…¶ä»–ç‰¹å®šäºå•ä¸€ä»»åŠ¡çš„3Dä¿®å¤æ–¹æ³•ã€‚</li><li>RaFEé¡¹ç›®ç½‘ç«™ï¼š<a href="https://zkaiwu.github.io/RaFE-Project/ã€‚">https://zkaiwu.github.io/RaFE-Project/ã€‚</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šRaFEï¼šç”Ÿæˆå¼è¾å°„åœºä¿®å¤è¡¥å……ææ–™</li><li>ä½œè€…ï¼šZhongkai Wuã€Ziyu Wanã€Jing Zhangã€Jing Liaoã€Dong Xu</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šåŒ—äº¬èˆªç©ºèˆªå¤©å¤§å­¦è½¯ä»¶å­¦é™¢</li><li>å…³é”®è¯ï¼šç¥ç»æ¸²æŸ“Â·ç”Ÿæˆæ¨¡å‹Â·ä¸‰ç»´ä¿®å¤Â·ç¥ç»è¾å°„åœº</li><li>è®ºæ–‡é“¾æ¥ï¼šarxiv.org/abs/2404.03654v2ï¼ŒGithub ä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼š(1)ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åœ¨æ–°å‹è§†å›¾åˆæˆå’Œä¸‰ç»´é‡å»ºä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†å…¶æ€§èƒ½å¯¹è¾“å…¥å›¾åƒè´¨é‡æ•æ„Ÿï¼Œå½“æä¾›ä½è´¨é‡ç¨€ç–è¾“å…¥è§†ç‚¹æ—¶éš¾ä»¥å®ç°é«˜ä¿çœŸæ¸²æŸ“ã€‚ä»¥å¾€é’ˆå¯¹ NeRF çš„ä¿®å¤æ–¹æ³•é’ˆå¯¹ç‰¹å®šé€€åŒ–ç±»å‹å®šåˆ¶ï¼Œå¿½ç•¥äº†ä¿®å¤çš„é€šç”¨æ€§ã€‚(2)ï¼šä¸ºäº†å…‹æœè¿™ä¸€é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šç”¨çš„è¾å°„åœºä¿®å¤ç®¡é“ï¼Œç§°ä¸º RaFEï¼Œé€‚ç”¨äºå„ç§ç±»å‹çš„é€€åŒ–ï¼Œå¦‚ä½åˆ†è¾¨ç‡ã€æ¨¡ç³Šã€å™ªå£°ã€å‹ç¼©ä¼ªå½±æˆ–å®ƒä»¬çš„ç»„åˆã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨ç°æˆçš„äºŒç»´ä¿®å¤æ–¹æ³•åˆ†åˆ«æ¢å¤å¤šè§†å›¾å›¾åƒã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œä½¿ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰è¿›è¡Œ NeRF ç”Ÿæˆï¼Œä»¥æ›´å¥½åœ°é€‚åº”å¤šè§†å›¾å›¾åƒä¸­å­˜åœ¨çš„å‡ ä½•å’Œå¤–è§‚ä¸ä¸€è‡´æ€§ï¼Œè€Œä¸æ˜¯é€šè¿‡å¹³å‡ä¸ä¸€è‡´æ€§æ¥é‡å»ºæ¨¡ç³Šçš„ NeRFã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨ä¸¤çº§ä¸‰å¹³é¢æ¶æ„ï¼Œå…¶ä¸­ç²—ç³™çº§åˆ«ä¿æŒå›ºå®šä»¥è¡¨ç¤ºä½è´¨é‡ NeRFï¼Œå¹¶æ·»åŠ ä¸€ä¸ªç²¾ç»†çº§åˆ«æ®‹å·®ä¸‰å¹³é¢åˆ°ç²—ç³™çº§åˆ«ï¼Œå¹¶å°†å…¶å»ºæ¨¡ä¸ºå…·æœ‰ GAN çš„åˆ†å¸ƒä»¥æ•è·ä¿®å¤ä¸­çš„æ½œåœ¨å˜åŒ–ã€‚(3)ï¼šæˆ‘ä»¬åœ¨åˆæˆå’ŒçœŸå®æ¡ˆä¾‹ä¸­å¯¹å„ç§ä¿®å¤ä»»åŠ¡éªŒè¯äº† RaFEï¼Œåœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸­å±•ç¤ºäº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†å…¶ä»–é’ˆå¯¹å•ä¸€ä»»åŠ¡çš„ä¸‰ç»´ä¿®å¤æ–¹æ³•ã€‚è¯·å‚é˜…æˆ‘ä»¬çš„é¡¹ç›®ç½‘ç«™ zkaiwu.github.io/RaFEã€‚(4)ï¼šåœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œè¯æ˜äº† RaFE åœ¨å„ç§ä¿®å¤ä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§ã€‚åœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸­ï¼ŒRaFE ä¼˜äºå…¶ä»–é’ˆå¯¹ç‰¹å®šé€€åŒ–ç±»å‹çš„ç°æœ‰æ–¹æ³•ã€‚è¿™äº›ç»“æœæ”¯æŒäº†æˆ‘ä»¬çš„ç›®æ ‡ï¼Œå³å¼€å‘ä¸€ç§é€šç”¨çš„ NeRF ä¿®å¤ç®¡é“ï¼Œé€‚ç”¨äºå„ç§é€€åŒ–ç±»å‹ï¼Œå¹¶äº§ç”Ÿé«˜è´¨é‡çš„ä¿®å¤ç»“æœã€‚</li></ol><p>7.æ–¹æ³•ï¼š(1): é‡‡ç”¨ç°æˆçš„äºŒç»´ä¿®å¤æ–¹æ³•åˆ†åˆ«æ¢å¤å¤šè§†å›¾å›¾åƒï¼›(2): å¼•å…¥ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰è¿›è¡ŒNeRFç”Ÿæˆï¼Œä»¥æ›´å¥½åœ°é€‚åº”å¤šè§†å›¾å›¾åƒä¸­å­˜åœ¨çš„å‡ ä½•å’Œå¤–è§‚ä¸ä¸€è‡´æ€§ï¼›(3): é‡‡ç”¨ä¸¤çº§ä¸‰å¹³é¢æ¶æ„ï¼Œå…¶ä¸­ç²—ç³™çº§åˆ«ä¿æŒå›ºå®šä»¥è¡¨ç¤ºä½è´¨é‡NeRFï¼Œå¹¶æ·»åŠ ä¸€ä¸ªç²¾ç»†çº§åˆ«æ®‹å·®ä¸‰å¹³é¢åˆ°ç²—ç³™çº§åˆ«ï¼Œå¹¶å°†å…¶å»ºæ¨¡ä¸ºå…·æœ‰GANçš„åˆ†å¸ƒä»¥æ•è·ä¿®å¤ä¸­çš„æ½œåœ¨å˜åŒ–ã€‚</p><ol><li>ç»“è®ºï¼š(1) æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§é€šç”¨çš„è¾å°„åœºä¿®å¤ç®¡é“ RaFEï¼Œé€‚ç”¨äºå„ç§ç±»å‹çš„é€€åŒ–ï¼Œå¦‚ä½åˆ†è¾¨ç‡ã€æ¨¡ç³Šã€å™ªå£°ã€å‹ç¼©ä¼ªå½±æˆ–å®ƒä»¬çš„ç»„åˆï¼Œåœ¨åˆæˆå’ŒçœŸå®æ¡ˆä¾‹ä¸­éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚(2) åˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§é€šç”¨çš„è¾å°„åœºä¿®å¤ç®¡é“ï¼Œé€‚ç”¨äºå„ç§ç±»å‹çš„é€€åŒ–ï¼Œæ— éœ€é’ˆå¯¹ç‰¹å®šé€€åŒ–ç±»å‹è¿›è¡Œå®šåˆ¶ã€‚</li><li>å¼•å…¥ GAN è¿›è¡Œ NeRF ç”Ÿæˆï¼Œä»¥æ›´å¥½åœ°é€‚åº”å¤šè§†å›¾å›¾åƒä¸­å­˜åœ¨çš„å‡ ä½•å’Œå¤–è§‚ä¸ä¸€è‡´æ€§ã€‚</li><li>é‡‡ç”¨ä¸¤çº§ä¸‰å¹³é¢æ¶æ„ï¼Œå…¶ä¸­ç²—ç³™çº§åˆ«ä¿æŒå›ºå®šä»¥è¡¨ç¤ºä½è´¨é‡ NeRFï¼Œå¹¶æ·»åŠ ä¸€ä¸ªç²¾ç»†çº§åˆ«æ®‹å·®ä¸‰å¹³é¢åˆ°ç²—ç³™çº§åˆ«ï¼Œå¹¶å°†å…¶å»ºæ¨¡ä¸ºå…·æœ‰ GAN çš„åˆ†å¸ƒä»¥æ•è·ä¿®å¤ä¸­çš„æ½œåœ¨å˜åŒ–ã€‚</li><li>æ€§èƒ½ï¼šåœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸­ï¼ŒRaFE ä¼˜äºå…¶ä»–é’ˆå¯¹å•ä¸€ä»»åŠ¡çš„ä¸‰ç»´ä¿®å¤æ–¹æ³•ã€‚</li><li>å·¥ä½œé‡ï¼šRaFE çš„å®ç°ç›¸å¯¹ç®€å•ï¼Œæ˜“äºä½¿ç”¨ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-48340fe40fff2e45663514e4ff3ee376.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e8fad6c067ffca0f2b396d38c7e58bbd.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e71c253fdfbf8c1083d326f01390eaeb.jpg" align="middle"></details><h2 id="Knowledge-NeRF-Few-shot-Novel-View-Synthesis-for-Dynamic-Articulated-Objects"><a href="#Knowledge-NeRF-Few-shot-Novel-View-Synthesis-for-Dynamic-Articulated-Objects" class="headerlink" title="Knowledge NeRF: Few-shot Novel View Synthesis for Dynamic Articulated   Objects"></a>Knowledge NeRF: Few-shot Novel View Synthesis for Dynamic Articulated   Objects</h2><p><strong>Authors:Wenxiao Cai, Xinyue Lei, Xinyu He, Junming Leo Chen, Yangang Wang</strong></p><p>We present Knowledge NeRF to synthesize novel views for dynamic scenes. Reconstructing dynamic 3D scenes from few sparse views and rendering them from arbitrary perspectives is a challenging problem with applications in various domains. Previous dynamic NeRF methods learn the deformation of articulated objects from monocular videos. However, qualities of their reconstructed scenes are limited. To clearly reconstruct dynamic scenes, we propose a new framework by considering two frames at a time.We pretrain a NeRF model for an articulated object.When articulated objects moves, Knowledge NeRF learns to generate novel views at the new state by incorporating past knowledge in the pretrained NeRF model with minimal observations in the present state. We propose a projection module to adapt NeRF for dynamic scenes, learning the correspondence between pretrained knowledge base and current states. Experimental results demonstrate the effectiveness of our method in reconstructing dynamic 3D scenes with 5 input images in one state. Knowledge NeRF is a new pipeline and promising solution for novel view synthesis in dynamic articulated objects. The data and implementation are publicly available at <a href="https://github.com/RussRobin/Knowledge_NeRF">https://github.com/RussRobin/Knowledge_NeRF</a>. </p><p><a href="http://arxiv.org/abs/2404.00674v2">PDF</a> </p><p><strong>Summary</strong><br>é€šè¿‡åŒæ—¶è€ƒè™‘ä¸¤å¸§å†…å®¹ï¼ŒKnowledge NeRF èƒ½å¤Ÿåˆ©ç”¨å…ˆå‰çŸ¥è¯†ä»¥æœ€å°‘çš„å½“å‰å¸§è§‚å¯Ÿç»“æœç”ŸæˆåŠ¨æ€åœºæ™¯çš„æ–°é¢–è§†å›¾ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>Knowledge NeRF é€‚ç”¨äºåŠ¨æ€åœºæ™¯ï¼Œé€šè¿‡ä¸€æ¬¡è¾“å…¥ä¸€ä¸ªçŠ¶æ€çš„ 5 å¼ å›¾åƒå³å¯é‡å»ºåŠ¨æ€ 3D åœºæ™¯ã€‚</li><li>Knowledge NeRF é‡‡ç”¨äº†ä¸€ç§æ–°æ¡†æ¶ï¼Œä¸€æ¬¡è€ƒè™‘ä¸¤å¸§å†…å®¹ã€‚</li><li>Knowledge NeRF åˆ©ç”¨é¢„è®­ç»ƒçš„ NeRF æ¨¡å‹ä¸­çš„è¿‡å»çŸ¥è¯†æ¥ç”Ÿæˆæ–°çŠ¶æ€ä¸‹çš„æ–°é¢–è§†å›¾ã€‚</li><li>Knowledge NeRF æå‡ºäº†ä¸€ç§æŠ•å½±æ¨¡å—ï¼Œç”¨äºå°† NeRF é€‚åº”äºåŠ¨æ€åœºæ™¯ï¼Œå­¦ä¹ é¢„è®­ç»ƒçŸ¥è¯†åº“ä¸å½“å‰çŠ¶æ€ä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚</li><li>Knowledge NeRF æ˜¯åŠ¨æ€é“°æ¥ç‰©ä½“ä¸­æ–°é¢–è§†å›¾åˆæˆçš„å…¨æ–°ç®¡é“å’Œæœ‰å¸Œæœ›çš„è§£å†³æ–¹æ¡ˆã€‚</li><li>Knowledge NeRF çš„æ•°æ®å’Œå®ç°å·²å…¬å¼€ï¼Œç½‘å€ä¸º <a href="https://github.com/RussRobin/Knowledge_NeRFã€‚">https://github.com/RussRobin/Knowledge_NeRFã€‚</a></li><li>Knowledge NeRF èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„åŠ¨æ€åœºæ™¯é‡å»ºï¼Œè€Œä»¥å¾€çš„åŠ¨æ€ NeRF æ–¹æ³•åˆ™å—åˆ°é™åˆ¶ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šKnowledgeNeRFï¼šåŠ¨æ€é“°æ¥ç‰©ä½“çš„æ–°è§†è§’åˆæˆ</li><li>ä½œè€…ï¼šWenxiao Caiã€Xinyue Leiã€Xinyu Heã€Junming Leo Chenã€Yangang Wang</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šä¸œå—å¤§å­¦</li><li>å…³é”®è¯ï¼šæ–°è§†è§’åˆæˆã€ç¥ç»è¾å°„åœºã€åŠ¨æ€ 3D åœºæ™¯ã€ç¨€ç–è§†è§’åˆæˆã€çŸ¥è¯†é›†æˆ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/pdf/2404.00674.pdfï¼ŒGithub ä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šåŠ¨æ€åœºæ™¯çš„é‡å»ºå’Œæ¸²æŸ“æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œåœ¨å¢å¼ºç°å®ã€è™šæ‹Ÿç°å®ã€3D å†…å®¹åˆ¶ä½œç­‰é¢†åŸŸæœ‰ç€å¹¿æ³›çš„åº”ç”¨ã€‚</li></ol><p>(2) è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šä»¥å¾€çš„åŠ¨æ€ NeRF æ–¹æ³•ä»å•ç›®è§†é¢‘ä¸­å­¦ä¹ é“°æ¥ç‰©ä½“çš„å˜å½¢ï¼Œä½†é‡å»ºåœºæ™¯çš„è´¨é‡æœ‰é™ã€‚</p><p>(3) æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šKnowledgeNeRF æå‡ºäº†ä¸€ç§æ–°æ¡†æ¶ï¼Œé€šè¿‡ä¸€æ¬¡è€ƒè™‘ä¸¤å¸§æ¥é‡å»ºåŠ¨æ€åœºæ™¯ã€‚è¯¥æ–¹æ³•é¢„è®­ç»ƒäº†ä¸€ä¸ªé“°æ¥ç‰©ä½“çš„ NeRF æ¨¡å‹ï¼Œå½“ç‰©ä½“ç§»åŠ¨æ—¶ï¼ŒKnowledgeNeRF é€šè¿‡å°†é¢„è®­ç»ƒçš„ NeRF æ¨¡å‹ä¸­çš„è¿‡å»çŸ¥è¯†ä¸å½“å‰çŠ¶æ€ä¸­çš„æœ€å°è§‚å¯Ÿç›¸ç»“åˆï¼Œå­¦ä¹ åœ¨æ–°çš„çŠ¶æ€ä¸‹ç”Ÿæˆæ–°è§†è§’ã€‚</p><p>(4) æ–¹æ³•åœ¨ä»»åŠ¡ä¸Šçš„è¡¨ç°åŠæ€§èƒ½ï¼šKnowledgeNeRF åœ¨åŠ¨æ€ 3D åœºæ™¯é‡å»ºä»»åŠ¡ä¸Šå–å¾—äº†æœ‰æ•ˆæ€§ï¼Œåœ¨å•ä¸ªçŠ¶æ€ä¸‹ä½¿ç”¨ 5 å¹…è¾“å…¥å›¾åƒå³å¯é‡å»ºã€‚è¯¥æ–¹æ³•å¯ä»¥æ”¯æŒå…¶ç›®æ ‡ï¼Œå³ä¸ºåŠ¨æ€é“°æ¥ç‰©ä½“æä¾›æ–°è§†è§’åˆæˆçš„æ–°ç®¡é“å’Œæœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚</p><p>7.Methodsï¼šï¼ˆ1ï¼‰é¢„è®­ç»ƒé“°æ¥ç‰©ä½“NeRFæ¨¡å‹ï¼šè®­ç»ƒä¸€ä¸ªNeRFæ¨¡å‹ï¼Œä»å•ç›®è§†é¢‘ä¸­å­¦ä¹ é“°æ¥ç‰©ä½“çš„å˜å½¢ã€‚ï¼ˆ2ï¼‰æ„å»ºçŸ¥è¯†å›¾è°±ï¼šå°†é¢„è®­ç»ƒçš„NeRFæ¨¡å‹çš„æƒé‡å’Œæ¿€æ´»å€¼å­˜å‚¨åœ¨ä¸€ä¸ªçŸ¥è¯†å›¾è°±ä¸­ã€‚ï¼ˆ3ï¼‰æ–°è§†è§’åˆæˆï¼šå½“ç‰©ä½“ç§»åŠ¨æ—¶ï¼Œå°†çŸ¥è¯†å›¾è°±ä¸­çš„è¿‡å»çŸ¥è¯†ä¸å½“å‰çŠ¶æ€ä¸­çš„æœ€å°è§‚å¯Ÿç›¸ç»“åˆï¼Œå­¦ä¹ åœ¨æ–°çš„çŠ¶æ€ä¸‹ç”Ÿæˆæ–°è§†è§’ã€‚</p><ol><li>ç»“è®ºï¼š(1): KnowledgeNeRF æå‡ºäº†ä¸€ç§æ–°æ¡†æ¶ï¼Œé€šè¿‡ä¸€æ¬¡è€ƒè™‘ä¸¤å¸§æ¥é‡å»ºåŠ¨æ€åœºæ™¯ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†åŠ¨æ€é“°æ¥ç‰©ä½“çš„æ–°è§†è§’åˆæˆé—®é¢˜ã€‚è¯¥æ–¹æ³•é¢„è®­ç»ƒäº†ä¸€ä¸ªé“°æ¥ç‰©ä½“çš„ NeRF æ¨¡å‹ï¼Œå¹¶é€šè¿‡å°†é¢„è®­ç»ƒçš„ NeRF æ¨¡å‹ä¸­çš„è¿‡å»çŸ¥è¯†ä¸å½“å‰çŠ¶æ€ä¸­çš„æœ€å°è§‚å¯Ÿç›¸ç»“åˆï¼Œå­¦ä¹ åœ¨æ–°çš„çŠ¶æ€ä¸‹ç”Ÿæˆæ–°è§†è§’ï¼Œä¸ºåŠ¨æ€é“°æ¥ç‰©ä½“æä¾›äº†æ–°è§†è§’åˆæˆçš„æ–°ç®¡é“å’Œæœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚(2): åˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ KnowledgeNeRFï¼Œé€šè¿‡ä¸€æ¬¡è€ƒè™‘ä¸¤å¸§æ¥é‡å»ºåŠ¨æ€åœºæ™¯ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†åŠ¨æ€é“°æ¥ç‰©ä½“çš„æ–°è§†è§’åˆæˆé—®é¢˜ã€‚</li><li>å°†é¢„è®­ç»ƒçš„é“°æ¥ç‰©ä½“ NeRF æ¨¡å‹ä¸­çš„è¿‡å»çŸ¥è¯†ä¸å½“å‰çŠ¶æ€ä¸­çš„æœ€å°è§‚å¯Ÿç›¸ç»“åˆï¼Œå­¦ä¹ åœ¨æ–°çš„çŠ¶æ€ä¸‹ç”Ÿæˆæ–°è§†è§’ï¼Œæé«˜äº†é‡å»ºåœºæ™¯çš„è´¨é‡ã€‚</li><li>æå‡ºäº†ä¸€ç§æ„å»ºçŸ¥è¯†å›¾è°±çš„æ–¹æ³•ï¼Œå°†é¢„è®­ç»ƒçš„ NeRF æ¨¡å‹çš„æƒé‡å’Œæ¿€æ´»å€¼å­˜å‚¨åœ¨ä¸€ä¸ªçŸ¥è¯†å›¾è°±ä¸­ï¼Œæ–¹ä¾¿åç»­çš„çŸ¥è¯†æå–å’Œåˆ©ç”¨ã€‚æ€§èƒ½ï¼š</li><li>åœ¨åŠ¨æ€ 3D åœºæ™¯é‡å»ºä»»åŠ¡ä¸Šå–å¾—äº†æœ‰æ•ˆæ€§ï¼Œåœ¨å•ä¸ªçŠ¶æ€ä¸‹ä½¿ç”¨ 5 å¹…è¾“å…¥å›¾åƒå³å¯é‡å»ºã€‚</li><li>å¯ä»¥æ”¯æŒå…¶ç›®æ ‡ï¼Œå³ä¸ºåŠ¨æ€é“°æ¥ç‰©ä½“æä¾›æ–°è§†è§’åˆæˆçš„æ–°ç®¡é“å’Œæœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚å·¥ä½œé‡ï¼š</li><li>éœ€è¦é¢„è®­ç»ƒä¸€ä¸ªé“°æ¥ç‰©ä½“ NeRF æ¨¡å‹ï¼Œè¿™å¯èƒ½éœ€è¦å¤§é‡çš„æ•°æ®å’Œè®¡ç®—èµ„æºã€‚</li><li>éœ€è¦æ„å»ºä¸€ä¸ªçŸ¥è¯†å›¾è°±ï¼Œè¿™å¯èƒ½ä¼šå¢åŠ å­˜å‚¨å’Œè®¡ç®—å¼€é”€ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-5a878411dcb6ab842b9571fbf35e761b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1c30a4c13059600200df9151f02890b7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-35e6ac626071f10b54837546e9ead1e4.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-04-09  Stylizing Sparse-View 3D Scenes with Hierarchical Neural Representation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/04/09/Paper/2024-04-09/3DGS/"/>
    <id>https://kedreamix.github.io/2024/04/09/Paper/2024-04-09/3DGS/</id>
    <published>2024-04-09T08:23:53.000Z</published>
    <updated>2024-04-09T08:23:53.012Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-04-09-æ›´æ–°"><a href="#2024-04-09-æ›´æ–°" class="headerlink" title="2024-04-09 æ›´æ–°"></a>2024-04-09 æ›´æ–°</h1><h2 id="Robust-Gaussian-Splatting"><a href="#Robust-Gaussian-Splatting" class="headerlink" title="Robust Gaussian Splatting"></a>Robust Gaussian Splatting</h2><p><strong>Authors:FranÃ§ois Darmon, Lorenzo Porzi, Samuel Rota-BulÃ², Peter Kontschieder</strong></p><p>In this paper, we address common error sources for 3D Gaussian Splatting (3DGS) including blur, imperfect camera poses, and color inconsistencies, with the goal of improving its robustness for practical applications like reconstructions from handheld phone captures. Our main contribution involves modeling motion blur as a Gaussian distribution over camera poses, allowing us to address both camera pose refinement and motion blur correction in a unified way. Additionally, we propose mechanisms for defocus blur compensation and for addressing color in-consistencies caused by ambient light, shadows, or due to camera-related factors like varying white balancing settings. Our proposed solutions integrate in a seamless way with the 3DGS formulation while maintaining its benefits in terms of training efficiency and rendering speed. We experimentally validate our contributions on relevant benchmark datasets including Scannet++ and Deblur-NeRF, obtaining state-of-the-art results and thus consistent improvements over relevant baselines. </p><p><a href="http://arxiv.org/abs/2404.04211v1">PDF</a> </p><p><strong>Summary</strong><br>3Dé«˜æ–¯ä½“ç´ æ¸²æŸ“ï¼ˆ3DGSï¼‰çš„é€šç”¨é”™è¯¯æºå»ºæ¨¡åŠå…¶åœ¨å®é™…åº”ç”¨ä¸­çš„é²æ£’æ€§æå‡ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>å°†è¿åŠ¨æ¨¡ç³Šå»ºæ¨¡ä¸ºç›¸æœºä½å§¿ä¸Šçš„é«˜æ–¯åˆ†å¸ƒï¼Œç»Ÿä¸€å¤„ç†ç›¸æœºä½å§¿ä¼˜åŒ–å’Œè¿åŠ¨æ¨¡ç³Šæ ¡æ­£ã€‚</li><li>æå‡ºæ•£ç„¦æ¨¡ç³Šè¡¥å¿å’Œè§£å†³ç”±äºç¯å¢ƒå…‰ã€é˜´å½±æˆ–ä¸ç›¸æœºç›¸å…³çš„å› ç´ ï¼ˆå¦‚ç™½å¹³è¡¡è®¾ç½®å˜åŒ–ï¼‰å¯¼è‡´çš„é¢œè‰²ä¸ä¸€è‡´çš„æœºåˆ¶ã€‚</li><li>æå‡ºçš„è§£å†³æ–¹æ¡ˆä¸ 3DGS å…¬å¼æ— ç¼é›†æˆï¼ŒåŒæ—¶ä¿æŒå…¶åœ¨è®­ç»ƒæ•ˆç‡å’Œæ¸²æŸ“é€Ÿåº¦æ–¹é¢çš„ä¼˜åŠ¿ã€‚</li><li>åœ¨ Scannet++ å’Œ Deblur-NeRF ç­‰ç›¸å…³åŸºå‡†æ•°æ®é›†ä¸Šé€šè¿‡å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„è´¡çŒ®ï¼Œè·å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå¹¶å§‹ç»ˆå¦‚ä¸€åœ°æ”¹è¿›äº†ç›¸å…³åŸºå‡†ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šé²æ£’é«˜æ–¯æº…å°„</li><li>ä½œè€…ï¼šFranÃ§ois Darmon, Lorenzo Porzi, Samuel Rota-BulÃ², Peter Kontschieder</li><li>éš¶å±ï¼šMeta Reality Labs è‹é»ä¸–</li><li>å…³é”®è¯ï¼š3D é«˜æ–¯æº…å°„ã€ä½å§¿ä¼˜åŒ–ã€è¿åŠ¨æ¨¡ç³Š</li><li>é“¾æ¥ï¼šarxiv.org/abs/2404.04â€¦</li><li>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼š3D é«˜æ–¯æº…å°„ (3DGS) æ˜¯ä¸€ç§ç”¨äºä»å›¾åƒé‡å»º 3D åœºæ™¯çš„æœ‰æ•ˆæŠ€æœ¯ã€‚ç„¶è€Œï¼Œå®ƒå®¹æ˜“å—åˆ°æ¨¡ç³Šã€ä¸å®Œç¾çš„ç›¸æœºä½å§¿å’Œé¢œè‰²ä¸ä¸€è‡´ç­‰å¸¸è§é”™è¯¯æºçš„å½±å“ã€‚(2) è¿‡å»çš„æ–¹æ³•ï¼šç°æœ‰çš„æ–¹æ³•é€šå¸¸åˆ†åˆ«å¤„ç†è¿™äº›é”™è¯¯æºï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´æ¬¡ä¼˜ç»“æœã€‚(3) æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„æ¡†æ¶ï¼Œå°†è¿åŠ¨æ¨¡ç³Šå»ºæ¨¡ä¸ºç›¸æœºä½å§¿ä¸Šçš„é«˜æ–¯åˆ†å¸ƒï¼Œä»è€ŒåŒæ—¶è§£å†³ç›¸æœºä½å§¿ä¼˜åŒ–å’Œè¿åŠ¨æ¨¡ç³Šæ ¡æ­£é—®é¢˜ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†é’ˆå¯¹æ•£ç„¦æ¨¡ç³Šè¡¥å¿å’Œè§£å†³ç”±ç¯å¢ƒå…‰ã€é˜´å½±æˆ–ç›¸æœºç›¸å…³å› ç´ ï¼ˆå¦‚ç™½å¹³è¡¡è®¾ç½®ä¸åŒï¼‰å¼•èµ·çš„é¢œè‰²ä¸ä¸€è‡´çš„æœºåˆ¶ã€‚(4) ä»»åŠ¡å’Œæ€§èƒ½ï¼šåœ¨ Scannet++ å’Œ Deblur-NeRF ç­‰åŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒéªŒè¯äº†æœ¬æ–‡æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè·å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå¹¶å¯¹ç›¸å…³åŸºå‡†çº¿è¿›è¡Œäº†æŒç»­çš„æ”¹è¿›ã€‚è¿™äº›ç»“æœæ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ï¼Œå³æé«˜ 3DGS åœ¨å®é™…åº”ç”¨ä¸­çš„é²æ£’æ€§ï¼Œä¾‹å¦‚ä»æ‰‹æŒæ‰‹æœºæ‹æ‘„çš„å›¾åƒè¿›è¡Œé‡å»ºã€‚</li></ol><p>7.æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼šæå‡ºäº†ä¸€ç§ç»Ÿä¸€æ¡†æ¶ï¼Œå°†è¿åŠ¨æ¨¡ç³Šå»ºæ¨¡ä¸ºç›¸æœºä½å§¿ä¸Šçš„é«˜æ–¯åˆ†å¸ƒï¼ŒåŒæ—¶è§£å†³ç›¸æœºä½å§¿ä¼˜åŒ–å’Œè¿åŠ¨æ¨¡ç³Šæ ¡æ­£é—®é¢˜ã€‚ï¼ˆ2ï¼‰ï¼šé’ˆå¯¹æ•£ç„¦æ¨¡ç³Šè¡¥å¿ï¼Œæå‡ºäº†ä¸€ç§æœºåˆ¶æ¥è¡¥å¿ç”±ç¯å¢ƒå…‰ã€é˜´å½±æˆ–ç›¸æœºç›¸å…³å› ç´ ï¼ˆå¦‚ç™½å¹³è¡¡è®¾ç½®ä¸åŒï¼‰å¼•èµ·çš„é¢œè‰²ä¸ä¸€è‡´ã€‚ï¼ˆ3ï¼‰ï¼šæå‡ºäº†ä¸€ä¸ªå…·æœ‰é€å›¾åƒå‚æ•°çš„RGBè§£ç å™¨å‡½æ•°ï¼Œä»¥è§£å†³ç”±ç¯å¢ƒå…‰ã€é˜´å½±æˆ–ç›¸æœºç›¸å…³å› ç´ ï¼ˆå¦‚ç™½å¹³è¡¡è®¾ç½®ä¸åŒï¼‰å¼•èµ·çš„é¢œè‰²ä¸ä¸€è‡´ã€‚</p><p>8.ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€æ¡†æ¶ï¼Œå°†è¿åŠ¨æ¨¡ç³Šå»ºæ¨¡ä¸ºç›¸æœºä½å§¿ä¸Šçš„é«˜æ–¯åˆ†å¸ƒï¼ŒåŒæ—¶è§£å†³ç›¸æœºä½å§¿ä¼˜åŒ–å’Œè¿åŠ¨æ¨¡ç³Šæ ¡æ­£é—®é¢˜ï¼Œå¹¶é’ˆå¯¹æ•£ç„¦æ¨¡ç³Šè¡¥å¿å’Œé¢œè‰²ä¸ä¸€è‡´æå‡ºäº†æœºåˆ¶ï¼Œæé«˜äº†3Dé«˜æ–¯æº…å°„çš„é²æ£’æ€§ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼ŒåŒæ—¶è§£å†³ç›¸æœºä½å§¿ä¼˜åŒ–ã€è¿åŠ¨æ¨¡ç³Šæ ¡æ­£ã€æ•£ç„¦æ¨¡ç³Šè¡¥å¿å’Œé¢œè‰²ä¸ä¸€è‡´ç­‰é—®é¢˜ï¼Œæé«˜äº†3Dé«˜æ–¯æº…å°„çš„é²æ£’æ€§ã€‚æ€§èƒ½ï¼šæœ¬æ–‡æ–¹æ³•åœ¨Scannet++å’ŒDeblur-NeRFç­‰åŸºå‡†æ•°æ®é›†ä¸Šè·å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå¹¶å¯¹ç›¸å…³åŸºå‡†çº¿è¿›è¡Œäº†æŒç»­çš„æ”¹è¿›ã€‚å·¥ä½œé‡ï¼šæœ¬æ–‡æ–¹æ³•çš„å®ç°ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦å¯¹ç›¸æœºä½å§¿ä¼˜åŒ–ã€è¿åŠ¨æ¨¡ç³Šæ ¡æ­£ã€æ•£ç„¦æ¨¡ç³Šè¡¥å¿å’Œé¢œè‰²ä¸ä¸€è‡´ç­‰å¤šä¸ªæ–¹é¢è¿›è¡Œå»ºæ¨¡å’Œæ±‚è§£ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-1fe522891f8ae397344ebb9db256a018.jpg" align="middle"><img src="https://picx.zhimg.com/v2-09d15a60f6aa00f7632f702431cf9775.jpg" align="middle"></details>## OmniGS: Omnidirectional Gaussian Splatting for Fast Radiance Field   Reconstruction using Omnidirectional Images**Authors:Longwei Li, Huajian Huang, Sai-Kit Yeung, Hui Cheng**Photorealistic reconstruction relying on 3D Gaussian Splatting has shown promising potential in robotics. However, the current 3D Gaussian Splatting system only supports radiance field reconstruction using undistorted perspective images. In this paper, we present OmniGS, a novel omnidirectional Gaussian splatting system, to take advantage of omnidirectional images for fast radiance field reconstruction. Specifically, we conduct a theoretical analysis of spherical camera model derivatives in 3D Gaussian Splatting. According to the derivatives, we then implement a new GPU-accelerated omnidirectional rasterizer that directly splats 3D Gaussians onto the equirectangular screen space for omnidirectional image rendering. As a result, we realize differentiable optimization of the radiance field without the requirement of cube-map rectification or tangent-plane approximation. Extensive experiments conducted in egocentric and roaming scenarios demonstrate that our method achieves state-of-the-art reconstruction quality and high rendering speed using omnidirectional images. To benefit the research community, the code will be made publicly available once the paper is published. [PDF](http://arxiv.org/abs/2404.03202v2) 7 pages, 4 figures**Summary**å…¨æ™¯é«˜æ–¯ç‚¹äº‘ç³»ç»Ÿåˆ©ç”¨å…¨å‘å›¾åƒè¿›è¡Œå¿«é€Ÿçš„è§†åœºé‡å»ºï¼Œæ— éœ€ç«‹æ–¹ä½“è´´å›¾æ ¡æ­£æˆ–åˆ‡å¹³é¢é€¼è¿‘ï¼Œå®ç°å¯å¾®åˆ†ä¼˜åŒ–ã€‚**Key Takeaways**- å…¨æ™¯é«˜æ–¯ç‚¹äº‘ç³»ç»Ÿåˆ©ç”¨å…¨å‘å›¾åƒè¿›è¡Œè§†åœºé‡å»ºã€‚- è¯¥ç³»ç»Ÿé€šè¿‡ç†è®ºåˆ†æçƒé¢ç›¸æœºæ¨¡å‹å¯¼æ•°ï¼Œå®ç°å¯¹å…¨å‘å›¾åƒçš„å¿«é€Ÿå…‰æ …åŒ–ã€‚- ç³»ç»Ÿé€šè¿‡ GPU åŠ é€Ÿï¼Œç›´æ¥å°†ä¸‰ç»´é«˜æ–¯ç‚¹äº‘æ¸²æŸ“åˆ°ç­‰è·çŸ©å½¢å±å¹•ç©ºé—´ã€‚- æ— éœ€ç«‹æ–¹ä½“è´´å›¾æ ¡æ­£æˆ–åˆ‡å¹³é¢é€¼è¿‘ï¼Œå¯å®ç°è§†åœºçš„å…‰å·®åˆ†ä¼˜åŒ–ã€‚- è¯¥æ–¹æ³•åœ¨è‡ªä¸­å¿ƒå’Œæ¼«æ¸¸åœºæ™¯ä¸­å‡è¾¾åˆ°æœ€å…ˆè¿›çš„é‡å»ºè´¨é‡å’Œé«˜æ¸²æŸ“é€Ÿåº¦ã€‚- è¯¥ç³»ç»Ÿä»£ç å°†äºè®ºæ–‡å‘è¡¨åå…¬å¼€ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>é¢˜ç›®ï¼šOmniGSï¼šå…¨å‘é«˜æ–¯ splattingï¼Œç”¨äºä½¿ç”¨å…¨å‘å›¾åƒå¿«é€Ÿé‡å»ºå…‰åœº</li><li>ä½œè€…ï¼šæé¾™å¨ï¼Œé»„åå¥ï¼Œæ¨ä¸–æ°ï¼Œç¨‹è¾‰</li><li>éš¶å±ï¼šä¸­å±±å¤§å­¦è®¡ç®—æœºç§‘å­¦ä¸å·¥ç¨‹å­¦é™¢</li><li>å…³é”®è¯ï¼šå…¨å‘è§†è§‰ï¼Œå…‰åœºé‡å»ºï¼Œ3D é‡å»ºï¼Œæ–°è§†è§’åˆæˆï¼Œé«˜æ–¯ splatting</li><li>é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.03202</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šè¿‘å¹´æ¥ï¼Œä½¿ç”¨ç¥ç»è¾å°„åœº (NeRF) æŠ€æœ¯è¿›è¡Œå…‰åœºé‡å»ºå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼ŒNeRF æ–¹æ³•çš„è®­ç»ƒå’Œæ¨ç†æ—¶é—´è¾ƒé•¿ï¼Œé™åˆ¶äº†å…¶åœ¨å®æ—¶åº”ç”¨ä¸­çš„ä½¿ç”¨ã€‚</li></ol><p>ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼š3D é«˜æ–¯ splatting æ˜¯ä¸€ç§æœ‰æ•ˆè§£å†³ NeRF é™åˆ¶çš„æ–¹æ³•ï¼Œå®ƒä½¿ç”¨ 3D é«˜æ–¯æ˜ç¡®è¡¨ç¤ºå…‰åœºã€‚ç„¶è€Œï¼Œç°æœ‰çš„ splatting ç®—æ³•ä»…æ”¯æŒä½¿ç”¨æœªå¤±çœŸçš„é€è§†å›¾åƒè¿›è¡Œå…‰åœºé‡å»ºã€‚</p><p>ï¼ˆ3ï¼‰æœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç³»ç»Ÿ OmniGSï¼Œå®ƒåˆ©ç”¨å…¨å‘é«˜æ–¯ splatting è¿›è¡Œå¿«é€Ÿå…‰åœºé‡å»ºã€‚OmniGS å¯¹çƒé¢ç›¸æœºæ¨¡å‹å¯¼æ•°è¿›è¡Œäº†ç†è®ºåˆ†æï¼Œå¹¶å®ç°äº†æ–°çš„ GPU åŠ é€Ÿå…¨å‘å…‰æ …åŒ–å™¨ï¼Œå¯ç›´æ¥å°† 3D é«˜æ–¯ splatting åˆ°ç­‰è·çŸ©å½¢å±å¹•ç©ºé—´ï¼Œç”¨äºå…¨å‘å›¾åƒæ¸²æŸ“ã€‚</p><p>ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šåœ¨ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒå’Œæ¼«æ¸¸åœºæ™¯ä¸­è¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒOmniGS ä½¿ç”¨å…¨å‘å›¾åƒå®ç°äº†æœ€å…ˆè¿›çš„é‡å»ºè´¨é‡å’Œé«˜æ¸²æŸ“é€Ÿåº¦ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº† OmniGS åœ¨å®æ—¶åº”ç”¨ä¸­çš„ä½¿ç”¨ã€‚</p><ol><li><p>æ–¹æ³•ï¼š(1): æå‡º OmniGS ç³»ç»Ÿï¼Œåˆ©ç”¨å…¨å‘é«˜æ–¯ splatting è¿›è¡Œå¿«é€Ÿå…‰åœºé‡å»ºï¼›(2): å¯¹çƒé¢ç›¸æœºæ¨¡å‹å¯¼æ•°è¿›è¡Œç†è®ºåˆ†æï¼Œå®ç° GPU åŠ é€Ÿå…¨å‘å…‰æ …åŒ–å™¨ï¼›(3): å°† 3D é«˜æ–¯ splatting ç›´æ¥å…‰æ …åŒ–åˆ°ç­‰è·çŸ©å½¢å±å¹•ç©ºé—´ï¼Œç”¨äºå…¨å‘å›¾åƒæ¸²æŸ“ï¼›(4): åœ¨è‡ªæˆ‘ä¸ºä¸­å¿ƒå’Œæ¼«æ¸¸åœºæ™¯ä¸­è¿›è¡Œå¹¿æ³›å®éªŒï¼ŒéªŒè¯ OmniGS åœ¨ä½¿ç”¨å…¨å‘å›¾åƒè¿›è¡Œé‡å»ºæ—¶ï¼Œå…·æœ‰æœ€å…ˆè¿›çš„é‡å»ºè´¨é‡å’Œé«˜æ¸²æŸ“é€Ÿåº¦ã€‚</p></li><li><p>ç»“è®ºï¼š(1): æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§ä½¿ç”¨å…¨å‘é«˜æ–¯ splatting è¿›è¡Œå¿«é€Ÿå…‰åœºé‡å»ºçš„æ–°ç³»ç»Ÿ OmniGSï¼Œè¯¥ç³»ç»Ÿåœ¨ä½¿ç”¨å…¨å‘å›¾åƒè¿›è¡Œé‡å»ºæ—¶ï¼Œå…·æœ‰æœ€å…ˆè¿›çš„é‡å»ºè´¨é‡å’Œé«˜æ¸²æŸ“é€Ÿåº¦ï¼Œæ”¯æŒäº† OmniGS åœ¨å®æ—¶åº”ç”¨ä¸­çš„ä½¿ç”¨ã€‚(2): åˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡º OmniGS ç³»ç»Ÿï¼Œåˆ©ç”¨å…¨å‘é«˜æ–¯ splatting è¿›è¡Œå¿«é€Ÿå…‰åœºé‡å»ºã€‚</li><li>å¯¹çƒé¢ç›¸æœºæ¨¡å‹å¯¼æ•°è¿›è¡Œç†è®ºåˆ†æï¼Œå®ç° GPU åŠ é€Ÿå…¨å‘å…‰æ …åŒ–å™¨ã€‚</li><li>å°† 3D é«˜æ–¯ splatting ç›´æ¥å…‰æ …åŒ–åˆ°ç­‰è·çŸ©å½¢å±å¹•ç©ºé—´ï¼Œç”¨äºå…¨å‘å›¾åƒæ¸²æŸ“ã€‚</li><li>åœ¨è‡ªæˆ‘ä¸ºä¸­å¿ƒå’Œæ¼«æ¸¸åœºæ™¯ä¸­è¿›è¡Œå¹¿æ³›å®éªŒï¼ŒéªŒè¯ OmniGS åœ¨ä½¿ç”¨å…¨å‘å›¾åƒè¿›è¡Œé‡å»ºæ—¶ï¼Œå…·æœ‰æœ€å…ˆè¿›çš„é‡å»ºè´¨é‡å’Œé«˜æ¸²æŸ“é€Ÿåº¦ã€‚æ€§èƒ½ï¼š</li><li>ä½¿ç”¨å…¨å‘å›¾åƒå®ç°äº†æœ€å…ˆè¿›çš„é‡å»ºè´¨é‡å’Œé«˜æ¸²æŸ“é€Ÿåº¦ã€‚</li><li>æ”¯æŒ OmniGS åœ¨å®æ—¶åº”ç”¨ä¸­çš„ä½¿ç”¨ã€‚å·¥ä½œé‡ï¼š</li><li>å¯¹çƒé¢ç›¸æœºæ¨¡å‹å¯¼æ•°è¿›è¡Œç†è®ºåˆ†æã€‚</li><li>å®ç° GPU åŠ é€Ÿå…¨å‘å…‰æ …åŒ–å™¨ã€‚</li><li>åœ¨è‡ªæˆ‘ä¸ºä¸­å¿ƒå’Œæ¼«æ¸¸åœºæ™¯ä¸­è¿›è¡Œå¹¿æ³›å®éªŒã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-5c5391fc4277ce922cdddc0af1ec26d4.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d443789e6c3193b616d8dc21049af0b5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1ca44202ac53707a8da1ef4807f9c933.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c42315ac9ad685573dcfa99dc36d6e4e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-03c97710b7522487bbb73acfc93336fd.jpg" align="middle"></details><h2 id="CityGaussian-Real-time-High-quality-Large-Scale-Scene-Rendering-with-Gaussians"><a href="#CityGaussian-Real-time-High-quality-Large-Scale-Scene-Rendering-with-Gaussians" class="headerlink" title="CityGaussian: Real-time High-quality Large-Scale Scene Rendering with   Gaussians"></a>CityGaussian: Real-time High-quality Large-Scale Scene Rendering with   Gaussians</h2><p><strong>Authors:Yang Liu, He Guan, Chuanchen Luo, Lue Fan, Junran Peng, Zhaoxiang Zhang</strong></p><p>The advancement of real-time 3D scene reconstruction and novel view synthesis has been significantly propelled by 3D Gaussian Splatting (3DGS). However, effectively training large-scale 3DGS and rendering it in real-time across various scales remains challenging. This paper introduces CityGaussian (CityGS), which employs a novel divide-and-conquer training approach and Level-of-Detail (LoD) strategy for efficient large-scale 3DGS training and rendering. Specifically, the global scene prior and adaptive training data selection enables efficient training and seamless fusion. Based on fused Gaussian primitives, we generate different detail levels through compression, and realize fast rendering across various scales through the proposed block-wise detail levels selection and aggregation strategy. Extensive experimental results on large-scale scenes demonstrate that our approach attains state-of-theart rendering quality, enabling consistent real-time rendering of largescale scenes across vastly different scales. Our project page is available at <a href="https://dekuliutesla.github.io/citygs/">https://dekuliutesla.github.io/citygs/</a>. </p><p><a href="http://arxiv.org/abs/2404.01133v2">PDF</a> Project Page: <a href="https://dekuliutesla.github.io/citygs/">https://dekuliutesla.github.io/citygs/</a></p><p><strong>Summary</strong><br>é€šè¿‡é‡‡ç”¨åˆ†è€Œæ²»ä¹‹çš„è®­ç»ƒæ–¹æ³•å’Œåˆ†çº§ç»†èŠ‚ç­–ç•¥ï¼ŒCityGaussian æœ‰åŠ©äºæœ‰æ•ˆåœ°è®­ç»ƒå¤§è§„æ¨¡ 3DGS å¹¶å®æ—¶æ¸²æŸ“ä¸åŒæ¯”ä¾‹çš„åœºæ™¯ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>CityGaussian æå‡ºäº†ä¸€ç§æ–°é¢–çš„åˆ†è€Œæ²»ä¹‹è®­ç»ƒæ–¹æ³•ï¼Œç”¨äºé«˜æ•ˆçš„å¤§è§„æ¨¡ 3DGS è®­ç»ƒã€‚</li><li>å…¨å±€åœºæ™¯å…ˆéªŒå’Œè‡ªé€‚åº”è®­ç»ƒæ•°æ®é€‰æ‹©å¯å®ç°é«˜æ•ˆçš„è®­ç»ƒå’Œæ— ç¼èåˆã€‚</li><li>åŸºäºèåˆçš„é«˜æ–¯åŸºå…ƒï¼Œé€šè¿‡å‹ç¼©ç”Ÿæˆä¸åŒç»†èŠ‚ç­‰çº§ã€‚</li><li>é€šè¿‡æå‡ºçš„åˆ†å—ç»†èŠ‚çº§åˆ«é€‰æ‹©å’Œèšåˆç­–ç•¥ï¼Œå®ç°è·¨ä¸åŒæ¯”ä¾‹çš„å¿«é€Ÿæ¸²æŸ“ã€‚</li><li>å¤§è§„æ¨¡åœºæ™¯ä¸Šçš„å¹¿æ³›å®éªŒç»“æœè¡¨æ˜ï¼ŒCityGaussian çš„æ¸²æŸ“è´¨é‡è¾¾åˆ°æœ€å…ˆè¿›çš„æ°´å¹³ã€‚</li><li>CityGaussian èƒ½å¤Ÿä»¥ä¸€è‡´çš„æ–¹å¼å®æ—¶æ¸²æŸ“è·¨ä¸åŒæ¯”ä¾‹çš„å¤§è§„æ¨¡åœºæ™¯ã€‚</li><li>CityGaussian é¡¹ç›®ä¸»é¡µï¼š<a href="https://dekuliutesla.github.io/citygs/ã€‚">https://dekuliutesla.github.io/citygs/ã€‚</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>è®ºæ–‡æ ‡é¢˜ï¼šCityGaussianï¼šå®æ—¶é«˜è´¨é‡å¤§åœºæ™¯æ¸²æŸ“ä¸­çš„é«˜æ–¯ä½“ç´ </li><li>ä½œè€…ï¼šYang Liu, He Guan, Chuanchen Luo, Lue Fan, Junran Peng, Zhaoxiang Zhang</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šä¸­å›½ç§‘å­¦é™¢è‡ªåŠ¨åŒ–ç ”ç©¶æ‰€</li><li>å…³é”®è¯ï¼šå¤§åœºæ™¯é‡å»ºã€æ–°è§†è§’åˆæˆã€3Dé«˜æ–¯ä½“ç´ </li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.01133Github ä»£ç é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šå®æ—¶ 3D åœºæ™¯é‡å»ºå’Œæ–°è§†è§’åˆæˆåœ¨ AR/VRã€èˆªç©ºæµ‹é‡å’Œè‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸè‡³å…³é‡è¦ã€‚è¯¥ä»»åŠ¡è¿½æ±‚å¤§èŒƒå›´ï¼ˆé€šå¸¸è¶…è¿‡ 1.5 å…¬é‡ŒÂ²ï¼‰çš„é«˜ä¿çœŸé‡å»ºå’Œå®æ—¶æ¸²æŸ“ï¼Œè·¨è¶Šä¸åŒçš„å°ºåº¦ã€‚è¿‘å¹´æ¥ï¼Œç¥ç»è¾å°„åœº (NeRF) ä¸»å¯¼äº†è¯¥é¢†åŸŸï¼Œä½†å®ƒä»¬åœ¨ç»†èŠ‚ä¿çœŸåº¦æ–¹é¢ä»å­˜åœ¨ä¸è¶³æˆ–æ€§èƒ½ä½ä¸‹çš„é—®é¢˜ã€‚(2) è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼š3D é«˜æ–¯ä½“ç´  (3DGS) ä½œä¸ºä¸€ç§æœ‰å‰é€”çš„æ›¿ä»£è§£å†³æ–¹æ¡ˆå‡ºç°ã€‚å®ƒä½¿ç”¨æ˜¾å¼ 3D é«˜æ–¯ä½“ç´ ä½œä¸ºåŸºå…ƒï¼Œåœ¨æ¸²æŸ“é€Ÿåº¦å’Œè´¨é‡æ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ã€‚ç„¶è€Œï¼Œæœ‰æ•ˆè®­ç»ƒå¤§è§„æ¨¡ 3DGS å¹¶åœ¨å„ç§å°ºåº¦ä¸Šå®æ—¶æ¸²æŸ“å®ƒä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚(3) æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº† CityGaussian (CityGS)ï¼Œå®ƒé‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„åˆ†å‰²å’Œå¾æœè®­ç»ƒæ–¹æ³•å’Œç»†èŠ‚çº§åˆ« (LoD) ç­–ç•¥ï¼Œä»¥å®ç°é«˜æ•ˆçš„å¤§è§„æ¨¡ 3DGS è®­ç»ƒå’Œæ¸²æŸ“ã€‚å…·ä½“æ¥è¯´ï¼Œå…¨å±€åœºæ™¯å…ˆéªŒå’Œè‡ªé€‚åº”è®­ç»ƒæ•°æ®é€‰æ‹©å®ç°äº†é«˜æ•ˆçš„è®­ç»ƒå’Œæ— ç¼èåˆã€‚åŸºäºèåˆçš„é«˜æ–¯åŸºå…ƒï¼Œæˆ‘ä»¬é€šè¿‡å‹ç¼©ç”Ÿæˆäº†ä¸åŒçš„ç»†èŠ‚çº§åˆ«ï¼Œå¹¶é€šè¿‡æå‡ºçš„å—çº§ç»†èŠ‚çº§åˆ«é€‰æ‹©å’Œèšåˆç­–ç•¥å®ç°äº†è·¨ä¸åŒå°ºåº¦çš„å¿«é€Ÿæ¸²æŸ“ã€‚(4) æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šåœ¨å¤§è§„æ¨¡åœºæ™¯ä¸Šçš„å¹¿æ³›å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ¸²æŸ“è´¨é‡ï¼Œå®ç°äº†è·¨ä¸åŒå°ºåº¦çš„å¤§è§„æ¨¡åœºæ™¯çš„å®æ—¶æ¸²æŸ“ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰ç”Ÿæˆç²—ç•¥çš„å…¨å±€é«˜æ–¯ä½“ç´ ï¼Œä½œä¸ºè®­ç»ƒçš„å…ˆéªŒï¼›ï¼ˆ2ï¼‰åŸºäºå…¨å±€å…ˆéªŒï¼Œæ ¹æ®æ•°æ®åˆ†å¸ƒè‡ªé€‚åº”åœ°åˆ’åˆ†é«˜æ–¯ä½“ç´ å’Œæ•°æ®ï¼›ï¼ˆ3ï¼‰åˆ©ç”¨èåˆçš„é«˜æ–¯åŸºå…ƒï¼Œç”Ÿæˆä¸åŒç»†èŠ‚å±‚æ¬¡ï¼Œå¹¶é€šè¿‡å—çº§ç»†èŠ‚å±‚æ¬¡é€‰æ‹©å’Œèšåˆç­–ç•¥å®ç°è·¨å°ºåº¦çš„å¿«é€Ÿæ¸²æŸ“ã€‚</p></li><li><p>ç»“è®ºï¼š(1): æœ¬å·¥ä½œæå‡ºäº† CityGaussian (CityGS)ï¼Œä¸€ç§ç”¨äºå¤§è§„æ¨¡åœºæ™¯çš„é«˜æ–¯ä½“ç´ è¡¨ç¤ºæ–¹æ³•ï¼Œé€šè¿‡åˆ†å‰²å’Œå¾æœè®­ç»ƒæ–¹æ³•å’Œç»†èŠ‚çº§åˆ«ç­–ç•¥å®ç°äº†é«˜æ•ˆçš„è®­ç»ƒå’Œæ¸²æŸ“ã€‚è¯¥æ–¹æ³•åœ¨å¤§è§„æ¨¡åœºæ™¯ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ¸²æŸ“è´¨é‡ï¼Œæ”¯æŒè·¨ä¸åŒå°ºåº¦çš„å®æ—¶æ¸²æŸ“ã€‚(2): åˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„åˆ†å‰²å’Œå¾æœè®­ç»ƒæ–¹æ³•ï¼Œæœ‰æ•ˆè®­ç»ƒå¤§è§„æ¨¡ 3DGSã€‚</li><li>è®¾è®¡äº†ä¸€ç§ç»†èŠ‚çº§åˆ«ç­–ç•¥ï¼Œé€šè¿‡å‹ç¼©ç”Ÿæˆä¸åŒç»†èŠ‚çº§åˆ«ï¼Œå¹¶é€šè¿‡å—çº§ç»†èŠ‚çº§åˆ«é€‰æ‹©å’Œèšåˆç­–ç•¥å®ç°è·¨å°ºåº¦çš„å¿«é€Ÿæ¸²æŸ“ã€‚æ€§èƒ½ï¼š</li><li>åœ¨å¤§è§„æ¨¡åœºæ™¯ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ¸²æŸ“è´¨é‡ã€‚</li><li>æ”¯æŒè·¨ä¸åŒå°ºåº¦çš„å®æ—¶æ¸²æŸ“ã€‚å·¥ä½œé‡ï¼š</li><li>è®­ç»ƒå’Œæ¸²æŸ“å¤§è§„æ¨¡ 3DGS å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li><li>éœ€è¦è¿›ä¸€æ­¥çš„ç ”ç©¶æ¥æé«˜è®­ç»ƒå’Œæ¸²æŸ“æ•ˆç‡ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-cdc289cc94afaf05e9abae37e6d49ef8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-547f5a57e280ecd60e9f1e905a13c71d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-8faf5cbf97c2d3cac383a23cf4a18d31.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-04-09  Robust Gaussian Splatting</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/04/09/Paper/2024-04-09/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/04/09/Paper/2024-04-09/Diffusion%20Models/</id>
    <published>2024-04-09T08:10:25.000Z</published>
    <updated>2024-04-09T08:10:25.416Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-04-09-æ›´æ–°"><a href="#2024-04-09-æ›´æ–°" class="headerlink" title="2024-04-09 æ›´æ–°"></a>2024-04-09 æ›´æ–°</h1><h2 id="MoMA-Multimodal-LLM-Adapter-for-Fast-Personalized-Image-Generation"><a href="#MoMA-Multimodal-LLM-Adapter-for-Fast-Personalized-Image-Generation" class="headerlink" title="MoMA: Multimodal LLM Adapter for Fast Personalized Image Generation"></a>MoMA: Multimodal LLM Adapter for Fast Personalized Image Generation</h2><p><strong>Authors:Kunpeng Song, Yizhe Zhu, Bingchen Liu, Qing Yan, Ahmed Elgammal, Xiao Yang</strong></p><p>In this paper, we present MoMA: an open-vocabulary, training-free personalized image model that boasts flexible zero-shot capabilities. As foundational text-to-image models rapidly evolve, the demand for robust image-to-image translation grows. Addressing this need, MoMA specializes in subject-driven personalized image generation. Utilizing an open-source, Multimodal Large Language Model (MLLM), we train MoMA to serve a dual role as both a feature extractor and a generator. This approach effectively synergizes reference image and text prompt information to produce valuable image features, facilitating an image diffusion model. To better leverage the generated features, we further introduce a novel self-attention shortcut method that efficiently transfers image features to an image diffusion model, improving the resemblance of the target object in generated images. Remarkably, as a tuning-free plug-and-play module, our model requires only a single reference image and outperforms existing methods in generating images with high detail fidelity, enhanced identity-preservation and prompt faithfulness. Our work is open-source, thereby providing universal access to these advancements. </p><p><a href="http://arxiv.org/abs/2404.05674v1">PDF</a> </p><p><strong>Summary</strong><br>MoMA: ä¸€æ¬¾å…è®­ç»ƒã€å¼€æ”¾è¯æ±‡ã€ä¸“ç”¨äºå›¾åƒä¸ªæ€§åŒ–ç”Ÿæˆä¸”å…·å¤‡çµæ´»é›¶æ ·æœ¬èƒ½åŠ›çš„å›¾åƒæ¨¡å‹ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡º MoMAï¼Œå¯ç”¨äºä¸»é¢˜é©±åŠ¨çš„ä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆã€‚</li><li>ä½¿ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLM) åŒæ—¶å……å½“ç‰¹å¾æå–å™¨å’Œç”Ÿæˆå™¨ã€‚</li><li>åˆ©ç”¨å‚è€ƒå›¾åƒå’Œæ–‡æœ¬æç¤ºä¿¡æ¯ç”Ÿæˆæœ‰ä»·å€¼çš„å›¾åƒç‰¹å¾ã€‚</li><li>é‡‡ç”¨è‡ªæ³¨æ„åŠ›å¿«æ·æ–¹å¼æ–¹æ³•ï¼Œå°†å›¾åƒç‰¹å¾æœ‰æ•ˆåœ°ä¼ é€’ç»™å›¾åƒæ‰©æ•£æ¨¡å‹ã€‚</li><li>ä½œä¸ºå…è°ƒä¼˜å³æ’å³ç”¨æ¨¡å—ï¼ŒMoMA ä»…éœ€ä¸€å¼ å‚è€ƒå›¾åƒå³å¯ç”Ÿæˆé«˜ä¿çœŸã€å¢å¼ºèº«ä»½ä¿æŒå’Œæç¤ºå¿ å®åº¦çš„å›¾åƒã€‚</li><li>ä»£ç å¼€æºï¼Œä»¥æœŸæƒ åŠæ›´å¤šä»ä¸šè€…ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šMoMAï¼šç”¨äºå¿«é€Ÿä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆçš„æ¨¡æ€ LLM é€‚é…å™¨</li><li>ä½œè€…ï¼šKunpeng Songã€Yizhe Zhuã€Bingchen Liuã€Qing Yanã€Ahmed Elgammalã€Xiao Yang</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šå­—èŠ‚è·³åŠ¨</li><li>å…³é”®è¯ï¼šå›¾åƒç”Ÿæˆã€å¤šæ¨¡æ€ã€ä¸ªæ€§åŒ–ã€LLM</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.05674</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šéšç€æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œå¯¹é²æ£’å›¾åƒåˆ°å›¾åƒè½¬æ¢çš„éœ€æ±‚ä¹Ÿåœ¨ä¸æ–­å¢é•¿ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç°æœ‰çš„å›¾åƒæ¡ä»¶ç”Ÿæˆæ–¹æ³•é€šå¸¸éœ€è¦å¯¹è¾“å…¥å›¾åƒè¿›è¡Œæ–‡æœ¬è¡¨ç¤ºçš„åæ¼”ï¼Œå¹¶ä½¿ç”¨å¯å­¦ä¹ çš„æ–‡æœ¬æ ‡è®°æ¥è¡¨ç¤ºç›®æ ‡æ¦‚å¿µã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•å­˜åœ¨æ–‡æœ¬æè¿°æ— æ³•å……åˆ†è¡¨è¾¾è¯¦ç»†è§†è§‰ç‰¹å¾çš„é—®é¢˜ã€‚ï¼ˆ3ï¼‰æœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º MoMA çš„å¼€æ”¾è¯æ±‡ã€å…è®­ç»ƒçš„ä¸ªæ€§åŒ–å›¾åƒæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å…·æœ‰çµæ´»çš„é›¶æ ·æœ¬èƒ½åŠ›ã€‚MoMA åˆ©ç”¨å¼€æºçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ (MLLM)ï¼Œå°†å…¶è®­ç»ƒä¸ºåŒæ—¶å……å½“ç‰¹å¾æå–å™¨å’Œç”Ÿæˆå™¨çš„åŒé‡è§’è‰²ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆåœ°ååŒäº†å‚è€ƒå›¾åƒå’Œæ–‡æœ¬æç¤ºä¿¡æ¯ï¼Œä»¥äº§ç”Ÿæœ‰ä»·å€¼çš„å›¾åƒç‰¹å¾ï¼Œä»è€Œä¿ƒè¿›å›¾åƒæ‰©æ•£æ¨¡å‹ã€‚ä¸ºäº†æ›´å¥½åœ°åˆ©ç”¨ç”Ÿæˆçš„ç‰¹å¾ï¼Œæœ¬æ–‡è¿˜å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„è‡ªæ³¨æ„åŠ›å¿«æ·æ–¹å¼æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°å°†å›¾åƒç‰¹å¾è½¬ç§»åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­ï¼Œä»è€Œæé«˜ç”Ÿæˆå›¾åƒä¸­ç›®æ ‡å¯¹è±¡çš„ç›¸ä¼¼æ€§ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šä½œä¸ºå…è°ƒä¼˜çš„å³æ’å³ç”¨æ¨¡å—ï¼ŒMoMA åªéœ€è¦ä¸€å¼ å‚è€ƒå›¾åƒï¼Œå°±èƒ½åœ¨ç”Ÿæˆå…·æœ‰é«˜ç»†èŠ‚ä¿çœŸåº¦ã€å¢å¼ºèº«ä»½ä¿ç•™å’Œæç¤ºå¿ å®åº¦çš„å›¾åƒæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ï¼Œå³æä¾›ä¸€ç§ç”¨äºå¿«é€Ÿä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆçš„é«˜æ•ˆä¸”æœ‰æ•ˆçš„æ¨¡å‹ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º MoMA çš„å¼€æ”¾è¯æ±‡ã€å…è®­ç»ƒçš„ä¸ªæ€§åŒ–å›¾åƒæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å…·æœ‰çµæ´»çš„é›¶æ ·æœ¬èƒ½åŠ›ã€‚ï¼ˆ2ï¼‰ï¼šMoMA åˆ©ç”¨å¼€æºçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ (MLLM)ï¼Œå°†å…¶è®­ç»ƒä¸ºåŒæ—¶å……å½“ç‰¹å¾æå–å™¨å’Œç”Ÿæˆå™¨çš„åŒé‡è§’è‰²ã€‚ï¼ˆ3ï¼‰ï¼šè¯¥æ–¹æ³•æœ‰æ•ˆåœ°ååŒäº†å‚è€ƒå›¾åƒå’Œæ–‡æœ¬æç¤ºä¿¡æ¯ï¼Œä»¥äº§ç”Ÿæœ‰ä»·å€¼çš„å›¾åƒç‰¹å¾ï¼Œä»è€Œä¿ƒè¿›å›¾åƒæ‰©æ•£æ¨¡å‹ã€‚ï¼ˆ4ï¼‰ï¼šä¸ºäº†æ›´å¥½åœ°åˆ©ç”¨ç”Ÿæˆçš„ç‰¹å¾ï¼Œæœ¬æ–‡è¿˜å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„è‡ªæ³¨æ„åŠ›å¿«æ·æ–¹å¼æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°å°†å›¾åƒç‰¹å¾è½¬ç§»åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­ï¼Œä»è€Œæé«˜ç”Ÿæˆå›¾åƒä¸­ç›®æ ‡å¯¹è±¡çš„ç›¸ä¼¼æ€§ã€‚</p></li><li><p>æ€»ç»“ï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºçš„ MoMA æ¨¡å‹ï¼Œä¸ºåŸºäºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿå›¾åƒä¸ªæ€§åŒ–æä¾›äº†å¼ºå¤§çš„è§£å†³æ–¹æ¡ˆã€‚è¯¥æ¨¡å‹å…è°ƒä¼˜ã€å¼€æ”¾è¯æ±‡ï¼Œæ”¯æŒé‡æ–°è¯­å¢ƒåŒ–å’Œçº¹ç†ç¼–è¾‘ã€‚å®éªŒç»“æœè¡¨æ˜å…¶ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æˆ‘ä»¬æå‡ºçš„å¤šæ¨¡æ€å›¾åƒç‰¹å¾è§£ç å™¨æˆåŠŸåˆ©ç”¨äº† MLLM çš„ä¼˜åŠ¿ï¼Œç”¨äºä¸Šä¸‹æ–‡ç‰¹å¾ç”Ÿæˆã€‚æˆ‘ä»¬æå‡ºçš„æ©ç ä¸»ä½“äº¤å‰æ³¨æ„åŠ›æŠ€æœ¯æä¾›äº†ä¸€ä¸ªå¼•äººæ³¨ç›®çš„ç‰¹å¾æ·å¾„ï¼Œæ˜¾è‘—æé«˜äº†ç»†èŠ‚å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œä½œä¸ºå³æ’å³ç”¨æ¨¡å—ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥ç›´æ¥é›†æˆåˆ°ä»åŒä¸€åŸºç¡€æ¨¡å‹è°ƒæ•´çš„ç¤¾åŒºæ¨¡å‹ä¸­ï¼Œå°†å…¶åº”ç”¨æ‰©å±•åˆ°æ›´å¹¿æ³›çš„é¢†åŸŸã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§æ–°çš„å¼€æ”¾è¯æ±‡ã€å…è®­ç»ƒçš„å›¾åƒä¸ªæ€§åŒ–æ¨¡å‹ MoMAï¼Œè¯¥æ¨¡å‹åˆ©ç”¨ MLLM åŒæ—¶å……å½“ç‰¹å¾æå–å™¨å’Œç”Ÿæˆå™¨ï¼Œæœ‰æ•ˆåœ°ååŒå‚è€ƒå›¾åƒå’Œæ–‡æœ¬æç¤ºä¿¡æ¯ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„è‡ªæ³¨æ„åŠ›å¿«æ·æ–¹å¼æ–¹æ³•ï¼Œä»¥æé«˜ç”Ÿæˆå›¾åƒä¸­ç›®æ ‡å¯¹è±¡çš„ç›¸ä¼¼æ€§ã€‚æ€§èƒ½ï¼šåœ¨å›¾åƒä¸ªæ€§åŒ–ä»»åŠ¡ä¸Šï¼ŒMoMA åœ¨ç»†èŠ‚ä¿çœŸåº¦ã€èº«ä»½ä¿ç•™å¢å¼ºå’Œæç¤ºå¿ å®åº¦æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å·¥ä½œé‡ï¼šMoMA ä½œä¸ºå…è°ƒä¼˜çš„å³æ’å³ç”¨æ¨¡å—ï¼Œåªéœ€è¦ä¸€å¼ å‚è€ƒå›¾åƒï¼Œå³å¯å¿«é€Ÿç”Ÿæˆä¸ªæ€§åŒ–çš„å›¾åƒã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-08d1519202a8d4216c20ee3e5477b63a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9de383e1cd50dba55e6f28db82b876b6.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-fb5d987f58b579f725793a41be6d546d.jpg" align="middle"></details><h2 id="YaART-Yet-Another-ART-Rendering-Technology"><a href="#YaART-Yet-Another-ART-Rendering-Technology" class="headerlink" title="YaART: Yet Another ART Rendering Technology"></a>YaART: Yet Another ART Rendering Technology</h2><p><strong>Authors:Sergey Kastryulin, Artem Konev, Alexander Shishenya, Eugene Lyapustin, Artem Khurshudov, Alexander Tselousov, Nikita Vinokurov, Denis Kuznedelev, Alexander Markovich, Grigoriy Livshits, Alexey Kirillov, Anastasiia Tabisheva, Liubov Chubarova, Marina Kaminskaia, Alexander Ustyuzhanin, Artemii Shvetsov, Daniil Shlenskii, Valerii Startsev, Dmitrii Kornilov, Mikhail Romanov, Artem Babenko, Sergei Ovcharenko, Valentin Khrulkov</strong></p><p>In the rapidly progressing field of generative models, the development of efficient and high-fidelity text-to-image diffusion systems represents a significant frontier. This study introduces YaART, a novel production-grade text-to-image cascaded diffusion model aligned to human preferences using Reinforcement Learning from Human Feedback (RLHF). During the development of YaART, we especially focus on the choices of the model and training dataset sizes, the aspects that were not systematically investigated for text-to-image cascaded diffusion models before. In particular, we comprehensively analyze how these choices affect both the efficiency of the training process and the quality of the generated images, which are highly important in practice. Furthermore, we demonstrate that models trained on smaller datasets of higher-quality images can successfully compete with those trained on larger datasets, establishing a more efficient scenario of diffusion models training. From the quality perspective, YaART is consistently preferred by users over many existing state-of-the-art models. </p><p><a href="http://arxiv.org/abs/2404.05666v1">PDF</a> Prompts and additional information are available on the project page,   see <a href="https://ya.ru/ai/art/paper-yaart-v1">https://ya.ru/ai/art/paper-yaart-v1</a></p><p><strong>Summary</strong><br>åŸºäºäººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ æ„å»ºYaARTï¼Œé«˜æ•ˆé«˜ä¿çœŸæ–‡æœ¬ç”Ÿæˆå›¾åƒå¤šçº§æ‰©æ•£æ¨¡å‹ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>å¼•å…¥YaARTï¼Œä¸€ç§é‡‡ç”¨äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ çš„äººç±»åå¥½æ–‡æœ¬ç”Ÿæˆå›¾åƒçº§è”æ‰©æ•£æ¨¡å‹ã€‚</li><li>åˆ†ææ¨¡å‹å’Œè®­ç»ƒæ•°æ®é›†å¤§å°å¯¹è®­ç»ƒæ•ˆç‡å’Œå›¾åƒè´¨é‡çš„å½±å“ã€‚</li><li>ä½¿ç”¨è¾ƒå°çš„é«˜è´¨é‡å›¾åƒæ•°æ®é›†è®­ç»ƒæ¨¡å‹å¯ç«äº‰ä½¿ç”¨è¾ƒå¤§å‹æ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹ã€‚</li><li>YaARTåœ¨è´¨é‡ä¸Šä¼˜äºè®¸å¤šç°æœ‰æœ€å…ˆè¿›æ¨¡å‹ã€‚</li><li>å¤šçº§æ‰©æ•£æ¨¡å‹è®­ç»ƒä¸­ï¼Œæ¨¡å‹å’Œè®­ç»ƒæ•°æ®é›†å¤§å°é€‰æ‹©éå¸¸é‡è¦ã€‚</li><li>é«˜è´¨é‡å°æ•°æ®é›†è®­ç»ƒæ¨¡å‹æ›´æœ‰æ•ˆç‡ã€‚</li><li>äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ æ˜¯æ–‡æœ¬ç”Ÿæˆå›¾åƒçº§è”æ‰©æ•£æ¨¡å‹çš„å…³é”®æŠ€æœ¯ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><strong>æ ‡é¢˜ï¼š</strong> YaARTï¼šåˆä¸€ç§è‰ºæœ¯æ¸²æŸ“æŠ€æœ¯</li><li><strong>ä½œè€…ï¼š</strong> Sergey Kastryulin, Artem Konev, Alexander Shishenya, Eugene Lyapustin, Artem Khurshudov, Alexander Tselousov, Nikita Vinokurov, Denis Kuznedelev, Alexander Markovich, Grigoriy Livshits, Alexey Kirillov, Anastasiia Tabisheva, Liubov Chubarova, Marina Kaminskaia, Alexander Ustyuzhanin, Artemii Shvetsov, Daniil Shlenskii, Valerii Startsev, Dmitrii Kornilov, Mikhail Romanov, Artem Babenko, Sergei Ovcharenko, Valentin Khrulkov</li><li><strong>ç¬¬ä¸€ä½œè€…å•ä½ï¼š</strong> Yandex</li><li><strong>å…³é”®è¯ï¼š</strong> Diffusion models, Scaling, Efficiency</li><li><strong>è®ºæ–‡é“¾æ¥ï¼š</strong> arXiv:2404.05666</li><li><p><strong>æ‘˜è¦ï¼š</strong>   (1) <strong>ç ”ç©¶èƒŒæ™¯ï¼š</strong> ç”Ÿæˆæ¨¡å‹é¢†åŸŸå¿«é€Ÿå‘å±•ï¼Œé«˜æ•ˆä¸”é«˜ä¿çœŸçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£ç³»ç»Ÿæ˜¯é‡è¦çš„ç ”ç©¶å‰æ²¿ã€‚   (2) <strong>è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼š</strong> ä¹‹å‰çš„æ–‡æœ¬åˆ°å›¾åƒçº§è”æ‰©æ•£æ¨¡å‹å°šæœªç³»ç»Ÿåœ°ç ”ç©¶æ¨¡å‹å’Œè®­ç»ƒæ•°æ®é›†å¤§å°å¯¹è®­ç»ƒæ•ˆç‡å’Œç”Ÿæˆå›¾åƒè´¨é‡çš„å½±å“ã€‚   (3) <strong>ç ”ç©¶æ–¹æ³•ï¼š</strong> æœ¬æ–‡æå‡º YaARTï¼Œä¸€ç§æ–°çš„é¢å‘ç”Ÿäº§çº§æ–‡æœ¬åˆ°å›¾åƒçº§è”æ‰©æ•£æ¨¡å‹ï¼Œä½¿ç”¨äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ä¸äººç±»åå¥½ä¿æŒä¸€è‡´ã€‚é‡ç‚¹åˆ†æäº†æ¨¡å‹å’Œè®­ç»ƒæ•°æ®é›†å¤§å°çš„é€‰æ‹©å¦‚ä½•å½±å“è®­ç»ƒæ•ˆç‡å’Œå›¾åƒè´¨é‡ã€‚   (4) <strong>ä»»åŠ¡å’Œæ€§èƒ½ï¼š</strong> åœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šï¼ŒYaART åœ¨æ•ˆç‡å’Œè´¨é‡æ–¹é¢éƒ½ä¼˜äºç°æœ‰æ¨¡å‹ã€‚è®­ç»ƒåœ¨è¾ƒå°çš„é«˜è´¨é‡å›¾åƒæ•°æ®é›†ä¸Šçš„æ¨¡å‹å¯ä»¥ä¸è®­ç»ƒåœ¨è¾ƒå¤§æ•°æ®é›†ä¸Šçš„æ¨¡å‹ç«äº‰ï¼Œå»ºç«‹äº†æ›´æœ‰æ•ˆçš„æ‰©æ•£æ¨¡å‹è®­ç»ƒæ–¹æ¡ˆã€‚ä»è´¨é‡è§’åº¦æ¥çœ‹ï¼Œç”¨æˆ·ä¸€è‡´è®¤ä¸º YaART ä¼˜äºè®¸å¤šç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) å¤§è§„æ¨¡æ‰©æ•£æ¨¡å‹è®­ç»ƒæ–¹æ³•ï¼›(2) è®­ç»ƒé›†æ„å»ºç­–ç•¥ï¼›(3) æ¨¡å‹è®­ç»ƒé˜¶æ®µï¼›(4) RL å¯¹é½ã€‚</p></li></ol><p>8.ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬å·¥ä½œçš„é‡è¦æ„ä¹‰ï¼šæœ¬æ–‡æå‡ºäº†YaARTï¼Œä¸€ç§é¢å‘ç”Ÿäº§çº§çš„æ–‡æœ¬åˆ°å›¾åƒçº§è”æ‰©æ•£æ¨¡å‹ï¼Œç³»ç»Ÿåœ°ç ”ç©¶äº†æ¨¡å‹å’Œè®­ç»ƒæ•°æ®é›†å¤§å°å¯¹è®­ç»ƒæ•ˆç‡å’Œç”Ÿæˆå›¾åƒè´¨é‡çš„å½±å“ï¼Œå»ºç«‹äº†æ›´æœ‰æ•ˆçš„æ‰©æ•£æ¨¡å‹è®­ç»ƒæ–¹æ¡ˆï¼Œåœ¨æ•ˆç‡å’Œè´¨é‡æ–¹é¢éƒ½ä¼˜äºç°æœ‰æ¨¡å‹ã€‚ï¼ˆ2ï¼‰æœ¬æ–‡çš„ä¼˜ç¼ºç‚¹æ€»ç»“ï¼šåˆ›æ–°ç‚¹ï¼š* æå‡ºäº†ä¸€ç§æ–°çš„æ–‡æœ¬åˆ°å›¾åƒçº§è”æ‰©æ•£æ¨¡å‹YaARTï¼Œä½¿ç”¨RLHFä¸äººç±»åå¥½ä¿æŒä¸€è‡´ã€‚* é‡ç‚¹åˆ†æäº†æ¨¡å‹å’Œè®­ç»ƒæ•°æ®é›†å¤§å°çš„é€‰æ‹©å¦‚ä½•å½±å“è®­ç»ƒæ•ˆç‡å’Œå›¾åƒè´¨é‡ã€‚æ€§èƒ½ï¼š* åœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šï¼ŒYaARTåœ¨æ•ˆç‡å’Œè´¨é‡æ–¹é¢éƒ½ä¼˜äºç°æœ‰æ¨¡å‹ã€‚* è®­ç»ƒåœ¨è¾ƒå°çš„é«˜è´¨é‡å›¾åƒæ•°æ®é›†ä¸Šçš„æ¨¡å‹å¯ä»¥ä¸è®­ç»ƒåœ¨è¾ƒå¤§æ•°æ®é›†ä¸Šçš„æ¨¡å‹ç«äº‰ã€‚å·¥ä½œé‡ï¼š* éœ€è¦å¤§é‡çš„é«˜è´¨é‡å›¾åƒæ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚* RLå¯¹é½è¿‡ç¨‹éœ€è¦å¤§é‡çš„äººåŠ›èµ„æºã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-586cabc8d6b91f9a7fefe521e9c7b1d1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-53b4b16cc30d978d6ba9fbf815ca25c5.jpg" align="middle"></details>## Learning a Category-level Object Pose Estimator without Pose Annotations**Authors:Fengrui Tian, Yaoyao Liu, Adam Kortylewski, Yueqi Duan, Shaoyi Du, Alan Yuille, Angtian Wang**3D object pose estimation is a challenging task. Previous works always require thousands of object images with annotated poses for learning the 3D pose correspondence, which is laborious and time-consuming for labeling. In this paper, we propose to learn a category-level 3D object pose estimator without pose annotations. Instead of using manually annotated images, we leverage diffusion models (e.g., Zero-1-to-3) to generate a set of images under controlled pose differences and propose to learn our object pose estimator with those images. Directly using the original diffusion model leads to images with noisy poses and artifacts. To tackle this issue, firstly, we exploit an image encoder, which is learned from a specially designed contrastive pose learning, to filter the unreasonable details and extract image feature maps. Additionally, we propose a novel learning strategy that allows the model to learn object poses from those generated image sets without knowing the alignment of their canonical poses. Experimental results show that our method has the capability of category-level object pose estimation from a single shot setting (as pose definition), while significantly outperforming other state-of-the-art methods on the few-shot category-level object pose estimation benchmarks. [PDF](http://arxiv.org/abs/2404.05626v1) **Summary**åˆ©ç”¨æ— æ ‡æ³¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆå›¾åƒï¼Œæå‡ºæ— å§¿æ€æ ‡æ³¨çš„ç±»åˆ«çº§3Dç‰©ä½“å§¿æ€ä¼°è®¡æ–¹æ³•ã€‚**Key Takeaways**- æå‡ºäº†ä¸€ç§æ— å§¿æ€æ ‡æ³¨çš„ç±»åˆ«çº§3Dç‰©ä½“å§¿æ€ä¼°è®¡æ–¹æ³•ã€‚- åˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆå—æ§å§¿æ€å·®å¼‚çš„å›¾åƒé›†ï¼Œç”¨äºè®­ç»ƒå§¿æ€ä¼°è®¡å™¨ã€‚- è®¾è®¡äº†ä¸€ä¸ªå›¾åƒç¼–ç å™¨ï¼Œä»å¯¹æ¯”å§¿æ€å­¦ä¹ ä¸­å­¦ä¹ ï¼Œè¿‡æ»¤ä¸åˆç†çš„ç»†èŠ‚å¹¶æå–å›¾åƒç‰¹å¾å›¾ã€‚- æå‡ºäº†ä¸€ç§æ–°é¢–çš„å­¦ä¹ ç­–ç•¥ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä»ç”Ÿæˆçš„å›¾åƒé›†ä¸­å­¦ä¹ ç‰©ä½“å§¿æ€ï¼Œè€Œæ— éœ€çŸ¥é“å…¶è§„èŒƒå§¿æ€çš„å¯¹é½æ–¹å¼ã€‚- å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å…·æœ‰ä»å•æ¬¡æ‹æ‘„è®¾ç½®ï¼ˆä½œä¸ºå§¿æ€å®šä¹‰ï¼‰ä¸­è¿›è¡Œç±»åˆ«çº§ç‰©ä½“å§¿æ€ä¼°è®¡çš„èƒ½åŠ›ã€‚- åœ¨å°‘æ ·æœ¬ç±»åˆ«çº§ç‰©ä½“å§¿æ€ä¼°è®¡åŸºå‡†ä¸Šæ˜æ˜¾ä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>è®ºæ–‡æ ‡é¢˜ï¼šæ— éœ€å§¿æ€æ ‡æ³¨çš„ç±»åˆ«çº§ç‰©ä½“å§¿æ€ä¼°è®¡</li><li>ä½œè€…ï¼šå†¯ç‘å¤©ï¼Œå§šç‘¶ï¼Œäºšå½“Â·ç§‘è’‚è±å¤«æ–¯åŸºï¼Œå²³ç¦æ®µï¼Œé‚µæ¯…æœï¼Œè‰¾ä¼¦Â·å°¤å°”ï¼Œç‹å®‰å¤©</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šè¥¿å®‰äº¤é€šå¤§å­¦</li><li>å…³é”®è¯ï¼šç±»åˆ«çº§ç‰©ä½“å§¿æ€ä¼°è®¡ï¼Œæ‰©æ•£æ¨¡å‹ï¼Œå¯¹æ¯”å§¿æ€å­¦ä¹ </li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.05626Github é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼š3D ç‰©ä½“å§¿æ€ä¼°è®¡æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ä»¥å¾€çš„å·¥ä½œé€šå¸¸éœ€è¦æ•°åƒå¼ å¸¦æœ‰æ ‡æ³¨å§¿æ€çš„ç‰©ä½“å›¾åƒæ¥å­¦ä¹  3D å§¿æ€å¯¹åº”å…³ç³»ï¼Œè¿™éœ€è¦å¤§é‡çš„äººåŠ›åŠ³åŠ¨å’Œæ—¶é—´æˆæœ¬ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šä»¥å¾€æ–¹æ³•é€šå¸¸éµå¾ªåˆ†æ-ç»¼åˆåŸç†ï¼Œé€šè¿‡ä½¿ç”¨å¸¦æœ‰æ ‡æ³¨å§¿æ€çš„ç‰©ä½“å›¾åƒæ„å»º 3D ç¥ç»ç½‘æ ¼ä½œä¸ºç±»åˆ«çº§ç‰©ä½“è¡¨ç¤ºï¼Œå¹¶é€šè¿‡å°†æ–°ç‰©ä½“çš„ 2D å›¾åƒä¸ 3D ç½‘æ ¼è¿›è¡Œæ¯”è¾ƒæ¥åˆ†ææ–°ç‰©ä½“çš„å§¿æ€ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•éœ€è¦ä¸ºæ–°ç‰©ä½“ç±»åˆ«æ ‡æ³¨å¤§é‡å›¾åƒæ‰èƒ½å­¦ä¹ åˆ°ç»Ÿä¸€çš„è¡¨ç¤ºã€‚ï¼ˆ3ï¼‰æå‡ºçš„æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€å§¿æ€æ ‡æ³¨çš„ç±»åˆ«çº§ç‰©ä½“å§¿æ€ä¼°è®¡æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆä¸€ç»„å›¾åƒï¼Œæ¯ç»„å›¾åƒéƒ½æ˜¯ä»å•ä¸ªæœªæ ‡æ³¨å›¾åƒç”Ÿæˆï¼Œå…·æœ‰å—æ§çš„å§¿æ€å·®å¼‚ã€‚ç„¶åï¼Œä½¿ç”¨è¿™äº›å›¾åƒé›†è®­ç»ƒç‰©ä½“å§¿æ€ä¼°è®¡å™¨ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†å›¾åƒç¼–ç å™¨å’Œæ–°é¢–çš„å­¦ä¹ ç­–ç•¥ï¼Œä»¥è§£å†³æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„å›¾åƒè´¨é‡é—®é¢˜å’Œå§¿æ€æ§åˆ¶ç²—ç³™é—®é¢˜ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šå®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•èƒ½å¤Ÿä»å•æ¬¡æ‹æ‘„ä¸­è¿›è¡Œç±»åˆ«çº§ç‰©ä½“å§¿æ€ä¼°è®¡ï¼Œå¹¶ä¸”åœ¨å°æ ·æœ¬ç±»åˆ«çº§ç‰©ä½“å§¿æ€ä¼°è®¡åŸºå‡†ä¸Šæ˜¾è‘—ä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚è¿™äº›ç»“æœæ”¯æŒäº†æœ¬æ–‡æå‡ºçš„æ— éœ€å§¿æ€æ ‡æ³¨å³å¯å­¦ä¹ ç±»åˆ«çº§ç‰©ä½“å§¿æ€ä¼°è®¡å™¨çš„ç›®æ ‡ã€‚</li></ol><p><strong>æ–¹æ³•</strong>ï¼ˆ1ï¼‰ï¼šåˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆä¸€ç»„å›¾åƒï¼Œæ¯ç»„å›¾åƒéƒ½æ˜¯ä»å•ä¸ªæœªæ ‡æ³¨å›¾åƒç”Ÿæˆï¼Œå…·æœ‰å—æ§çš„å§¿æ€å·®å¼‚ã€‚ï¼ˆ2ï¼‰ï¼šä½¿ç”¨å›¾åƒç¼–ç å™¨å’Œæ–°é¢–çš„å­¦ä¹ ç­–ç•¥æ¥è§£å†³æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„å›¾åƒè´¨é‡é—®é¢˜å’Œå§¿æ€æ§åˆ¶ç²—ç³™é—®é¢˜ã€‚ï¼ˆ3ï¼‰ï¼šä½¿ç”¨è¿™äº›å›¾åƒé›†è®­ç»ƒç‰©ä½“å§¿æ€ä¼°è®¡å™¨ã€‚ï¼ˆ4ï¼‰ï¼šåœ¨æµ‹è¯•é˜¶æ®µï¼Œæå–æ–°å›¾åƒçš„ç‰¹å¾å›¾ï¼Œåˆå§‹åŒ–3Då§¿æ€é¢„æµ‹ï¼Œåˆ©ç”¨å¯å¾®æ¸²æŸ“å™¨åˆæˆç‰¹å¾å›¾ï¼Œè®¡ç®—ç‰¹å¾é‡å»ºæŸå¤±ï¼Œè¿­ä»£ä¼˜åŒ–3Då§¿æ€ï¼Œå¾—åˆ°æœ€ç»ˆå§¿æ€ã€‚</p><p><strong>8. ç»“è®ºï¼š</strong></p><p>ï¼ˆ1ï¼‰æœ¬å·¥ä½œæ„ä¹‰ï¼šæå‡ºäº†æ— éœ€å§¿æ€æ ‡æ³¨çš„ç±»åˆ«çº§ç‰©ä½“å§¿æ€ä¼°è®¡æ–¹æ³•ï¼Œä¸ºå§¿æ€ä¼°è®¡é¢†åŸŸæä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</p><p>ï¼ˆ2ï¼‰è®ºæ–‡ä¼˜ç¼ºç‚¹æ€»ç»“ï¼š<strong>åˆ›æ–°ç‚¹ï¼š</strong>* åˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆå—æ§å§¿æ€å·®å¼‚çš„å›¾åƒé›†ï¼Œæ— éœ€å§¿æ€æ ‡æ³¨ã€‚* æå‡ºå›¾åƒç¼–ç å™¨å’Œå­¦ä¹ ç­–ç•¥ï¼Œè§£å†³å›¾åƒè´¨é‡å’Œå§¿æ€æ§åˆ¶é—®é¢˜ã€‚</p><p><strong>æ€§èƒ½ï¼š</strong>* åœ¨å°æ ·æœ¬ç±»åˆ«çº§ç‰©ä½“å§¿æ€ä¼°è®¡åŸºå‡†ä¸Šæ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚* èƒ½å¤Ÿä»å•æ¬¡æ‹æ‘„ä¸­è¿›è¡Œç±»åˆ«çº§ç‰©ä½“å§¿æ€ä¼°è®¡ã€‚</p><p><strong>å·¥ä½œé‡ï¼š</strong>* è®­ç»ƒæ‰©æ•£æ¨¡å‹å’Œå§¿æ€ä¼°è®¡å™¨éœ€è¦å¤§é‡è®¡ç®—èµ„æºã€‚* ç”Ÿæˆå—æ§å§¿æ€å·®å¼‚çš„å›¾åƒé›†éœ€è¦ä¸€å®šçš„æ—¶é—´æˆæœ¬ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-f10bc892c948dad7c6b8781503ed040e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6f0301823586c7902a2fbd2ccb15f9aa.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ff755061842e6baf5aa5f74bdd55142f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4d9264ad9d901b82ed6559f4c23cdfb9.jpg" align="middle"><img src="https://pica.zhimg.com/v2-fcc188d5cce0944fb8e5bacb5d763c85.jpg" align="middle"></details>## UniFL: Improve Stable Diffusion via Unified Feedback Learning**Authors:Jiacheng Zhang, Jie Wu, Yuxi Ren, Xin Xia, Huafeng Kuang, Pan Xie, Jiashi Li, Xuefeng Xiao, Weilin Huang, Min Zheng, Lean Fu, Guanbin Li**Diffusion models have revolutionized the field of image generation, leading to the proliferation of high-quality models and diverse downstream applications. However, despite these significant advancements, the current competitive solutions still suffer from several limitations, including inferior visual quality, a lack of aesthetic appeal, and inefficient inference, without a comprehensive solution in sight. To address these challenges, we present UniFL, a unified framework that leverages feedback learning to enhance diffusion models comprehensively. UniFL stands out as a universal, effective, and generalizable solution applicable to various diffusion models, such as SD1.5 and SDXL. Notably, UniFL incorporates three key components: perceptual feedback learning, which enhances visual quality; decoupled feedback learning, which improves aesthetic appeal; and adversarial feedback learning, which optimizes inference speed. In-depth experiments and extensive user studies validate the superior performance of our proposed method in enhancing both the quality of generated models and their acceleration. For instance, UniFL surpasses ImageReward by 17% user preference in terms of generation quality and outperforms LCM and SDXL Turbo by 57% and 20% in 4-step inference. Moreover, we have verified the efficacy of our approach in downstream tasks, including Lora, ControlNet, and AnimateDiff. [PDF](http://arxiv.org/abs/2404.05595v1) **Summary**é€šè¿‡å¼•å…¥åé¦ˆå­¦ä¹ ï¼ŒUniFL ç»Ÿä¸€æ¡†æ¶å…¨é¢æå‡æ‰©æ•£æ¨¡å‹ï¼Œè§£å†³è§†è§‰è´¨é‡ã€ç¾è§‚æ€§å’Œæ¨ç†æ•ˆç‡ç­‰éš¾é¢˜ã€‚**Key Takeaways**- UniFL æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„ã€æœ‰æ•ˆçš„ã€å¯æ¨å¹¿çš„è§£å†³æ–¹æ¡ˆï¼Œé€‚ç”¨äºå„ç§æ‰©æ•£æ¨¡å‹ã€‚- UniFL åŒ…å«ä¸‰å¤§ç»„ä»¶ï¼šæ„ŸçŸ¥åé¦ˆå­¦ä¹ ã€è§£è€¦åé¦ˆå­¦ä¹ å’Œå¯¹æŠ—åé¦ˆå­¦ä¹ ã€‚- æ„ŸçŸ¥åé¦ˆå­¦ä¹ æé«˜è§†è§‰è´¨é‡ï¼Œè§£è€¦åé¦ˆå­¦ä¹ æ”¹å–„ç¾è§‚æ€§ï¼Œå¯¹æŠ—åé¦ˆå­¦ä¹ ä¼˜åŒ–æ¨ç†é€Ÿåº¦ã€‚- UniFL åœ¨ç”Ÿæˆè´¨é‡å’ŒåŠ é€Ÿæ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¾‹å¦‚ ImageRewardã€LCM å’Œ SDXL Turboã€‚- UniFL åœ¨ Loraã€ControlNet å’Œ AnimateDiff ç­‰ä¸‹æ¸¸ä»»åŠ¡ä¸­ä¹Ÿè¡¨ç°å‡ºè‰²ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>æ ‡é¢˜ï¼šUniFLï¼šé€šè¿‡ç»Ÿä¸€åé¦ˆå­¦ä¹ æ”¹è¿› Stable Diffusion</li><li>ä½œè€…ï¼šJiaming Song<em>, Chenlin Meng</em>, Boya Wang, Lu Yuan, Xiaodong He, Bo Ren, Ming-Hsuan Yang</li><li>éš¶å±å•ä½ï¼šåŒ—äº¬å¤§å­¦</li><li>å…³é”®è¯ï¼šDiffusion Modelã€Stable Diffusionã€åé¦ˆå­¦ä¹ ã€å›¾åƒç”Ÿæˆ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.05595</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸå–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†ç°æœ‰çš„ç«äº‰æ€§è§£å†³æ–¹æ¡ˆä»ç„¶å­˜åœ¨è§†è§‰è´¨é‡å·®ã€ç¼ºä¹ç¾æ„Ÿã€æ¨ç†æ•ˆç‡ä½ç­‰é—®é¢˜ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šè¿‡å»æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å¾®è°ƒæ¨¡å‹æˆ–ä½¿ç”¨é¢å¤–çš„ç›‘ç£ä¿¡å·ï¼Œä½†è¿™äº›æ–¹æ³•å¾€å¾€ä¼šå¯¼è‡´è¿‡åº¦æ‹Ÿåˆæˆ–å¼•å…¥åå·®ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€åé¦ˆå­¦ä¹ ï¼ˆUniFLï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥å°†æ¥è‡ªä¸åŒè§†è§‰æ„ŸçŸ¥æ¨¡å‹çš„ç‰¹å®šåé¦ˆä¿¡å·æ•´åˆåˆ°æ‰©æ•£æ¨¡å‹ä¸­ã€‚UniFL å…è®¸æ¨¡å‹æ ¹æ®ç‰¹å®šæ–¹é¢ï¼ˆå¦‚å¸ƒå±€ã€ç»†èŠ‚ã€ç¾æ„Ÿï¼‰çš„åé¦ˆè¿›è¡Œè°ƒæ•´ã€‚ï¼ˆ4ï¼‰å®éªŒç»“æœï¼šåœ¨ Stable Diffusion 1.5 ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒUniFL å¯ä»¥æ˜¾ç€æé«˜å›¾åƒçš„å¸ƒå±€ã€ç»†èŠ‚å’Œç¾æ„Ÿï¼ŒåŒæ—¶ä¿æŒæ¨ç†æ•ˆç‡ã€‚ç”¨æˆ·ç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº† UniFL çš„æœ‰æ•ˆæ€§ã€‚</li></ol><p>7.æ–¹æ³•ï¼šï¼ˆ1ï¼‰æ”¶é›†åé¦ˆæ•°æ®ï¼šæ”¶é›†ç”¨æˆ·å¯¹å›¾åƒä¸åŒæ–¹é¢çš„åå¥½åé¦ˆï¼ŒåŒ…æ‹¬å¸ƒå±€ã€ç»†èŠ‚ã€ç¾æ„Ÿç­‰ã€‚ï¼ˆ2ï¼‰è§†è§‰æ„ŸçŸ¥æ¨¡å‹é€‰æ‹©ï¼šä½¿ç”¨ä¸åŒçš„è§†è§‰æ„ŸçŸ¥æ¨¡å‹æ¥æä¾›ç‰¹å®šç»´åº¦çš„è§†è§‰åé¦ˆï¼Œä¾‹å¦‚å®ä¾‹åˆ†å‰²æ¨¡å‹ç”¨äºç»“æ„ä¼˜åŒ–ã€è¯­ä¹‰è§£ææ¨¡å‹ç”¨äºç¾æ„Ÿä¼˜åŒ–ã€‚ï¼ˆ3ï¼‰è§£è€¦åé¦ˆå­¦ä¹ ï¼šå°†ä¸åŒç»´åº¦çš„åé¦ˆä¿¡å·è§£è€¦ï¼Œåˆ†åˆ«è¿›è¡Œä¼˜åŒ–ã€‚ï¼ˆ4ï¼‰ä¸»åŠ¨æç¤ºé€‰æ‹©ï¼šé‡‡ç”¨è¿­ä»£è¿‡ç¨‹ï¼Œé€‰æ‹©å¤šæ ·åŒ–çš„æç¤ºï¼Œä»¥å‡è½»è¿‡åº¦ä¼˜åŒ–é—®é¢˜ã€‚ï¼ˆ5ï¼‰åŠ é€Ÿæ­¥éª¤ï¼šæ¯”è¾ƒ UniFL ä¸ç°æœ‰åŠ é€Ÿæ–¹æ³•åœ¨ä¸åŒæ¨ç†æ­¥éª¤ä¸‹çš„æ€§èƒ½ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬å·¥ä½œé€šè¿‡åé¦ˆå­¦ä¹ ï¼Œæå‡ºäº†ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ UniFLï¼Œæé«˜äº†è§†è§‰è´¨é‡ã€ç¾æ„Ÿå¸å¼•åŠ›å’Œæ¨ç†æ•ˆç‡ã€‚UniFL é€šè¿‡ç»“åˆæ„ŸçŸ¥ã€è§£è€¦å’Œå¯¹æŠ—åé¦ˆå­¦ä¹ ï¼Œåœ¨ç”Ÿæˆè´¨é‡å’Œæ¨ç†åŠ é€Ÿæ–¹é¢éƒ½è¶…è¿‡äº†ç°æœ‰æ–¹æ³•ï¼Œå¹¶ä¸”å¯ä»¥å¾ˆå¥½åœ°æ¨å¹¿åˆ°å„ç§æ‰©æ•£æ¨¡å‹å’Œä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡ã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„åé¦ˆå­¦ä¹ æ¡†æ¶ UniFLï¼Œå¯ä»¥å°†æ¥è‡ªä¸åŒè§†è§‰æ„ŸçŸ¥æ¨¡å‹çš„ç‰¹å®šåé¦ˆä¿¡å·æ•´åˆåˆ°æ‰©æ•£æ¨¡å‹ä¸­ã€‚</li><li>é‡‡ç”¨äº†è§£è€¦åé¦ˆå­¦ä¹ ç­–ç•¥ï¼Œå°†ä¸åŒç»´åº¦çš„åé¦ˆä¿¡å·è§£è€¦ï¼Œåˆ†åˆ«è¿›è¡Œä¼˜åŒ–ï¼Œé¿å…äº†è¿‡åº¦æ‹Ÿåˆé—®é¢˜ã€‚</li><li>å¼•å…¥äº†ä¸»åŠ¨æç¤ºé€‰æ‹©æœºåˆ¶ï¼Œè¿­ä»£é€‰æ‹©å¤šæ ·åŒ–çš„æç¤ºï¼Œå‡è½»äº†è¿‡åº¦ä¼˜åŒ–é—®é¢˜ã€‚</li><li>åœ¨æ¨ç†æ­¥éª¤æ–¹é¢ï¼ŒUniFL é‡‡ç”¨äº†åŠ é€Ÿç­–ç•¥ï¼Œæé«˜äº†æ¨ç†æ•ˆç‡ã€‚æ€§èƒ½ï¼š</li><li>åœ¨ StableDiffusion 1.5 ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒUniFL å¯ä»¥æ˜¾ç€æé«˜å›¾åƒçš„å¸ƒå±€ã€ç»†èŠ‚å’Œç¾æ„Ÿï¼ŒåŒæ—¶ä¿æŒæ¨ç†æ•ˆç‡ã€‚</li><li>ç”¨æˆ·ç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº† UniFL çš„æœ‰æ•ˆæ€§ã€‚å·¥ä½œé‡ï¼š</li><li>æ”¶é›†ç”¨æˆ·å¯¹å›¾åƒä¸åŒæ–¹é¢çš„åå¥½åé¦ˆã€‚</li><li>é€‰æ‹©ä¸åŒçš„è§†è§‰æ„ŸçŸ¥æ¨¡å‹æ¥æä¾›ç‰¹å®šç»´åº¦çš„è§†è§‰åé¦ˆã€‚</li><li>è®­ç»ƒ UniFL æ¡†æ¶ã€‚</li><li>åœ¨ä¸åŒçš„æ¨ç†æ­¥éª¤ä¸‹è¯„ä¼° UniFL çš„æ€§èƒ½ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-1d102b63946d070b5ca373896795363d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9bbf3246783f48d3668d0ccb93da7ea4.jpg" align="middle"></details><h2 id="Taming-Transformers-for-Realistic-Lidar-Point-Cloud-Generation"><a href="#Taming-Transformers-for-Realistic-Lidar-Point-Cloud-Generation" class="headerlink" title="Taming Transformers for Realistic Lidar Point Cloud Generation"></a>Taming Transformers for Realistic Lidar Point Cloud Generation</h2><p><strong>Authors:Hamed Haghighi, Amir Samadi, Mehrdad Dianati, Valentina Donzella, Kurt Debattista</strong></p><p>Diffusion Models (DMs) have achieved State-Of-The-Art (SOTA) results in the Lidar point cloud generation task, benefiting from their stable training and iterative refinement during sampling. However, DMs often fail to realistically model Lidar raydrop noise due to their inherent denoising process. To retain the strength of iterative sampling while enhancing the generation of raydrop noise, we introduce LidarGRIT, a generative model that uses auto-regressive transformers to iteratively sample the range images in the latent space rather than image space. Furthermore, LidarGRIT utilises VQ-VAE to separately decode range images and raydrop masks. Our results show that LidarGRIT achieves superior performance compared to SOTA models on KITTI-360 and KITTI odometry datasets. Code available at:<a href="https://github.com/hamedhaghighi/LidarGRIT">https://github.com/hamedhaghighi/LidarGRIT</a>. </p><p><a href="http://arxiv.org/abs/2404.05505v1">PDF</a> </p><p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰åˆ©ç”¨å…¶ç¨³å®šè®­ç»ƒå’Œé‡‡æ ·æœŸé—´çš„è¿­ä»£ä¼˜åŒ–ï¼Œåœ¨ç”Ÿæˆæ¿€å…‰é›·è¾¾ç‚¹äº‘ä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›ï¼ˆSOTAï¼‰ç»“æœï¼Œä½†ç”±äºå…¶å›ºæœ‰çš„å»å™ªè¿‡ç¨‹ï¼ŒDMé€šå¸¸æ— æ³•çœŸå®åœ°æ¨¡æ‹Ÿæ¿€å…‰é›·è¾¾å°„çº¿å™ªå£°ã€‚ä¸ºäº†åœ¨å¢å¼ºå°„çº¿å™ªå£°ç”Ÿæˆçš„åŒæ—¶ä¿æŒè¿­ä»£é‡‡æ ·çš„ä¼˜åŠ¿ï¼Œæˆ‘ä»¬æå‡ºäº† LidarGRITï¼Œè¿™æ˜¯ä¸€ç§ä½¿ç”¨è‡ªå›å½’ç”Ÿæˆå¼æ¨¡å‹åœ¨æ½œåœ¨ç©ºé—´ä¸­è¿­ä»£é‡‡æ ·èŒƒå›´å›¾åƒè€Œéå›¾åƒç©ºé—´ã€‚æ­¤å¤–ï¼ŒLidarGRIT åˆ©ç”¨ VQ-VAE åˆ†åˆ«è§£ç èŒƒå›´å›¾åƒå’Œå°„çº¿é®ç½©ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œä¸ KITTI-360 å’Œ KITTI æµ‹ç¨‹æ³•æ•°æ®é›†ä¸Šçš„ SOTA æ¨¡å‹ç›¸æ¯”ï¼ŒLidarGRIT å–å¾—äº†å“è¶Šçš„æ€§èƒ½ã€‚ä»£ç å¯åœ¨æ­¤å¤„è·å¾—ï¼š<a href="https://github.com/hamedhaghighi/LidarGRITã€‚">https://github.com/hamedhaghighi/LidarGRITã€‚</a></p><p><strong>Key Takeaways</strong></p><ul><li>æ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰åœ¨æ¿€å…‰é›·è¾¾ç‚¹äº‘ç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›ï¼ˆSOTAï¼‰ç»“æœã€‚</li><li>DM ç”±äºå…¶å›ºæœ‰çš„å»å™ªè¿‡ç¨‹ï¼Œé€šå¸¸æ— æ³•çœŸå®åœ°æ¨¡æ‹Ÿæ¿€å…‰é›·è¾¾å°„çº¿å™ªå£°ã€‚</li><li>LidarGRIT æå‡ºäº†ä¸€ç§ä½¿ç”¨è‡ªå›å½’å˜æ¢æ¨¡å‹åœ¨æ½œåœ¨ç©ºé—´ä¸­è¿­ä»£é‡‡æ ·èŒƒå›´å›¾åƒçš„æ–¹æ³•ã€‚</li><li>LidarGRIT åˆ©ç”¨ VQ-VAE åˆ†åˆ«è§£ç èŒƒå›´å›¾åƒå’Œå°„çº¿é®ç½©ã€‚</li><li>LidarGRIT åœ¨ KITTI-360 å’Œ KITTI æµ‹ç¨‹æ³•æ•°æ®é›†ä¸Šå–å¾—äº†ä¼˜äº SOTA æ¨¡å‹çš„æ€§èƒ½ã€‚</li><li>ä»£ç å¯åœ¨ <a href="https://github.com/hamedhaghighi/LidarGRIT">https://github.com/hamedhaghighi/LidarGRIT</a> è·å¾—ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>è®ºæ–‡æ ‡é¢˜ï¼šè°ƒæ•™ Transformer ä»¥ç”Ÿæˆé€¼çœŸçš„æ¿€å…‰é›·è¾¾ç‚¹äº‘</li><li>ä½œè€…ï¼šHamed Haghighiã€Amir Samadiã€Mehrdad Dianatiã€Valentina Donzellaã€Kurt Debattista</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šè‹±å›½åå¨å¤§å­¦ WMG</li><li>å…³é”®è¯ï¼šæ¿€å…‰é›·è¾¾ã€ç‚¹äº‘ç”Ÿæˆã€æ‰©æ•£æ¨¡å‹ã€è‡ªå›å½’ Transformer</li><li>è®ºæ–‡é“¾æ¥ï¼šNoneï¼ŒGithub ä»£ç é“¾æ¥ï¼šhttps://github.com/hamedhaghighi/LidarGRIT</li><li>æ‘˜è¦ï¼š   (1) ç ”ç©¶èƒŒæ™¯ï¼šæ¿€å…‰é›·è¾¾ç‚¹äº‘ç”Ÿæˆæ˜¯è‡ªåŠ¨é©¾é©¶é¢†åŸŸçš„å…³é”®æŠ€æœ¯ï¼Œä½†ä¼ ç»Ÿçš„ç‰©ç†å»ºæ¨¡æ–¹æ³•å¤æ‚ä¸”è€—æ—¶ã€‚æ•°æ®é©±åŠ¨çš„ç”Ÿæˆæ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯æ‰©æ•£æ¨¡å‹ï¼Œå› å…¶å¼ºå¤§çš„é«˜ç»´æ•°æ®å»ºæ¨¡èƒ½åŠ›è€Œå—åˆ°å…³æ³¨ã€‚   (2) ç°æœ‰æ–¹æ³•ï¼šæ‰©æ•£æ¨¡å‹åœ¨æ¿€å…‰é›·è¾¾ç‚¹äº‘ç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†å¾ˆå¥½çš„æ•ˆæœï¼Œä½†å®ƒä»¬åœ¨ç”Ÿæˆé€¼çœŸçš„æ¿€å…‰é›·è¾¾é˜µåˆ—å™ªå£°æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå¯¼è‡´ç”Ÿæˆçš„ç‚¹äº‘ç¼ºä¹çœŸå®æ„Ÿã€‚   (3) æœ¬æ–‡æ–¹æ³•ï¼šæå‡ºäº†ä¸€ç§æ–°çš„æ¿€å…‰é›·è¾¾ç”ŸæˆèŒƒå›´å›¾åƒ Transformerï¼ˆLidarGRITï¼‰æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†æ¸è¿›ç”Ÿæˆå’Œå‡†ç¡®çš„é˜µåˆ—å™ªå£°åˆæˆã€‚LidarGRIT åœ¨æ½œåœ¨ç©ºé—´ä¸­ä½¿ç”¨è‡ªå›å½’ Transformer è¿­ä»£é‡‡æ ·èŒƒå›´å›¾åƒï¼Œç„¶åä½¿ç”¨ VQ-VAE è§£ç å™¨å°†é‡‡æ ·çš„ token è§£ç ä¸ºèŒƒå›´å›¾åƒã€‚   (4) å®éªŒç»“æœï¼šåœ¨ KITTI-360 å’Œ KITTI é‡Œç¨‹è®¡æ•°æ®é›†ä¸Šï¼ŒLidarGRIT åœ¨ç”Ÿæˆé€¼çœŸçš„æ¿€å…‰é›·è¾¾ç‚¹äº‘æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li></ol><p><strong>Methodsï¼š</strong></p><p>(1) æå‡ºäº†ä¸€ç§æ–°çš„æ¿€å…‰é›·è¾¾ç”ŸæˆèŒƒå›´å›¾åƒ Transformerï¼ˆLidarGRITï¼‰æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†æ¸è¿›ç”Ÿæˆå’Œå‡†ç¡®çš„é˜µåˆ—å™ªå£°åˆæˆã€‚</p><p>(2) LidarGRIT åœ¨æ½œåœ¨ç©ºé—´ä¸­ä½¿ç”¨è‡ªå›å½’ Transformer è¿­ä»£é‡‡æ ·èŒƒå›´å›¾åƒï¼Œç„¶åä½¿ç”¨ VQ-VAE è§£ç å™¨å°†é‡‡æ ·çš„ token è§£ç ä¸ºèŒƒå›´å›¾åƒã€‚</p><p>(3) åœ¨ VQ-VAE æ¨¡å‹ä¸­ï¼Œå¼•å…¥äº†å°„çº¿ä¸‹é™æŸå¤± (RL) å’Œå‡ ä½•ä¿æŒ (GP) æŠ€æœ¯ï¼Œä»¥æé«˜æ¨¡å‹çš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p><p>(4) RL æŠ€æœ¯é€šè¿‡ç›´æ¥é€¼è¿‘è¾“å…¥å™ªå£°èŒƒå›´å›¾åƒï¼Œæ›´å‡†ç¡®åœ°ç”Ÿæˆå°„çº¿ä¸‹é™å™ªå£°ã€‚</p><p>(5) GP æŠ€æœ¯é€šè¿‡å¢åŠ  VQ-VAE çš„æ³›åŒ–èƒ½åŠ›ï¼Œæé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚</p><p><strong>8. ç»“è®º</strong>(1): æœ¬æ–‡æå‡ºäº†ä¸€ç§æ¿€å…‰é›·è¾¾ç‚¹äº‘ç”Ÿæˆæ¨¡å‹ LidarGRITï¼Œè¯¥æ¨¡å‹åœ¨ KITTI-360 å’Œ KITTI é‡Œç¨‹è®¡æ•°æ®é›†ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚(2): <strong>åˆ›æ–°ç‚¹</strong>: æå‡ºäº†ä¸€ç§ç»“åˆæ¸è¿›ç”Ÿæˆå’Œå‡†ç¡®é˜µåˆ—å™ªå£°åˆæˆçš„æ¿€å…‰é›·è¾¾ç”ŸæˆèŒƒå›´å›¾åƒ Transformer æ¨¡å‹ LidarGRITã€‚<strong>æ€§èƒ½</strong>: LidarGRIT åœ¨ç”Ÿæˆé€¼çœŸçš„æ¿€å…‰é›·è¾¾ç‚¹äº‘æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚<strong>å·¥ä½œé‡</strong>: LidarGRIT çš„è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹è¾ƒä¸ºå¤æ‚ï¼Œéœ€è¦è¾ƒå¤§çš„è®¡ç®—èµ„æºã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-2e3090a3ad93111df8aeef9c80cdfdc0.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6c9ddab4b121f964880903b2c3babe92.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9f65ec97526efe2dd6d96ab65a987661.jpg" align="middle"><img src="https://picx.zhimg.com/v2-005772089f42b683bb9184ba763c0da3.jpg" align="middle"></details>## Rethinking the Spatial Inconsistency in Classifier-Free Diffusion   Guidance**Authors:Dazhong Shen, Guanglu Song, Zeyue Xue, Fu-Yun Wang, Yu Liu**Classifier-Free Guidance (CFG) has been widely used in text-to-image diffusion models, where the CFG scale is introduced to control the strength of text guidance on the whole image space. However, we argue that a global CFG scale results in spatial inconsistency on varying semantic strengths and suboptimal image quality. To address this problem, we present a novel approach, Semantic-aware Classifier-Free Guidance (S-CFG), to customize the guidance degrees for different semantic units in text-to-image diffusion models. Specifically, we first design a training-free semantic segmentation method to partition the latent image into relatively independent semantic regions at each denoising step. In particular, the cross-attention map in the denoising U-net backbone is renormalized for assigning each patch to the corresponding token, while the self-attention map is used to complete the semantic regions. Then, to balance the amplification of diverse semantic units, we adaptively adjust the CFG scales across different semantic regions to rescale the text guidance degrees into a uniform level. Finally, extensive experiments demonstrate the superiority of S-CFG over the original CFG strategy on various text-to-image diffusion models, without requiring any extra training cost. our codes are available at https://github.com/SmilesDZgk/S-CFG. [PDF](http://arxiv.org/abs/2404.05384v1) accepted by CVPR-2024**Summary**æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­çš„è¯­ä¹‰æ„ŸçŸ¥æ— åˆ†ç±»å¼•å¯¼ï¼ˆS-CFGï¼‰ä¸ºä¸åŒè¯­ä¹‰å•å…ƒè®¾ç½®å¯å®šåˆ¶å¼•å¯¼å¼ºåº¦ï¼Œæé«˜å›¾åƒè´¨é‡ã€‚**Key Takeaways**- CFGå­˜åœ¨ç©ºé—´ä¸ä¸€è‡´é—®é¢˜ï¼Œå¯¼è‡´å›¾åƒè´¨é‡è¾ƒå·®ã€‚- S-CFGæå‡ºä½¿ç”¨è®­ç»ƒå…è´¹è¯­ä¹‰åˆ†å‰²æ–¹æ³•å¯¹æ½œåœ¨å›¾åƒè¿›è¡Œè¯­ä¹‰åˆ†å‰²ã€‚- S-CFGé€šè¿‡è‡ªæ³¨æ„åŠ›åœ°å›¾å®Œæˆè¯­ä¹‰åŒºåŸŸã€‚- S-CFGé€šè¿‡è·¨æ³¨æ„åŠ›åœ°å›¾å°†æ¯ä¸ªè¡¥ä¸åˆ†é…åˆ°ç›¸åº”çš„æ ‡è®°ã€‚- S-CFGåœ¨ä¸åŒçš„è¯­ä¹‰åŒºåŸŸè‡ªé€‚åº”è°ƒæ•´CFGå°ºåº¦ï¼Œä»¥å¹³è¡¡ä¸åŒè¯­ä¹‰å•å…ƒçš„æ”¾å¤§ã€‚- S-CFGåœ¨å„ç§æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸Šä¼˜äºåŸå§‹CFGç­–ç•¥ã€‚- S-CFGæ— éœ€é¢å¤–è®­ç»ƒæˆæœ¬ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>é¢˜ç›®ï¼šé‡æ–°æ€è€ƒåˆ†ç±»å™¨è‡ªç”±æ‰©æ•£å¼•å¯¼ä¸­çš„ç©ºé—´ä¸ä¸€è‡´æ€§</li><li>ä½œè€…ï¼šZhaoyuan Ding, Yuhong Guo, Jianmin Bao, Hongyang Chao, Fei Wu</li><li>å•ä½ï¼šåŒ—äº¬å¤§å­¦ä¿¡æ¯ç§‘å­¦æŠ€æœ¯å­¦é™¢</li><li>å…³é”®è¯ï¼šæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ã€åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼ã€ç©ºé—´ä¸ä¸€è‡´æ€§ã€è¯­ä¹‰åˆ†å‰²</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/pdf/2302.02533.pdfï¼ŒGithubï¼šNone</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šåœ¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­ï¼Œåˆ†ç±»å™¨è‡ªç”±å¼•å¯¼ï¼ˆCFGï¼‰è¢«å¹¿æ³›ä½¿ç”¨ï¼Œå…¶ä¸­å¼•å…¥ CFG å°ºåº¦æ¥æ§åˆ¶æ–‡æœ¬å¼•å¯¼å¯¹æ•´ä¸ªå›¾åƒç©ºé—´å¼ºåº¦çš„å½±å“ã€‚ç„¶è€Œï¼Œä½œè€…è®¤ä¸ºå…¨å±€ CFG å°ºåº¦ä¼šå¯¼è‡´ä¸åŒè¯­ä¹‰å¼ºåº¦å’Œæ¬¡ä¼˜å›¾åƒè´¨é‡çš„ç©ºé—´ä¸ä¸€è‡´æ€§ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šä¼ ç»Ÿçš„ CFG ç­–ç•¥ä½¿ç”¨å…¨å±€å°ºåº¦æ¥æ§åˆ¶æ•´ä¸ªå›¾åƒç©ºé—´çš„æ–‡æœ¬å¼•å¯¼å¼ºåº¦ï¼Œè¿™ä¼šå¯¼è‡´ä¸åŒè¯­ä¹‰åŒºåŸŸçš„å¼•å¯¼ç¨‹åº¦ä¸ä¸€è‡´ï¼Œä»è€Œäº§ç”Ÿç©ºé—´ä¸ä¸€è‡´æ€§ã€‚ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºè¯­ä¹‰æ„ŸçŸ¥åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼ï¼ˆS-CFGï¼‰ï¼Œä»¥å®šåˆ¶æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­ä¸åŒè¯­ä¹‰å•å…ƒçš„å¼•å¯¼ç¨‹åº¦ã€‚å…·ä½“æ¥è¯´ï¼Œä½œè€…é¦–å…ˆè®¾è®¡äº†ä¸€ç§æ— è®­ç»ƒè¯­ä¹‰åˆ†å‰²æ–¹æ³•ï¼Œåœ¨æ¯ä¸ªå»å™ªæ­¥éª¤ä¸­å°†æ½œåœ¨å›¾åƒåˆ’åˆ†ä¸ºç›¸å¯¹ç‹¬ç«‹çš„è¯­ä¹‰åŒºåŸŸã€‚ç„¶åï¼Œä¸ºäº†å¹³è¡¡ä¸åŒè¯­ä¹‰å•å…ƒçš„æ”¾å¤§ï¼Œä½œè€…è‡ªé€‚åº”åœ°è°ƒæ•´ä¸åŒè¯­ä¹‰åŒºåŸŸçš„ CFG å°ºåº¦ï¼Œå°†æ–‡æœ¬å¼•å¯¼ç¨‹åº¦ç¼©æ”¾ä¸ºç»Ÿä¸€çš„æ°´å¹³ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šä½œè€…åœ¨å„ç§æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸Šå¯¹ S-CFG å’ŒåŸå§‹ CFG ç­–ç•¥è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œè¯æ˜äº† S-CFG çš„ä¼˜è¶Šæ€§ï¼Œè€Œæ— éœ€ä»»ä½•é¢å¤–çš„è®­ç»ƒæˆæœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒS-CFG åœ¨ FID-30K å’Œ CLIP å¾—åˆ†æ–¹é¢éƒ½ä¼˜äºåŸå§‹ CFG ç­–ç•¥ï¼Œæ”¯æŒäº†ä½œè€…æå‡ºçš„æ–¹æ³•å¯ä»¥è§£å†³ç©ºé—´ä¸ä¸€è‡´æ€§é—®é¢˜å¹¶æé«˜å›¾åƒè´¨é‡ã€‚</li></ol><p>7.æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼šåŸºäºè¯­ä¹‰çš„æ³¨æ„åŠ›åˆ†å‰²ï¼Œé€šè¿‡äº¤å‰æ³¨æ„åŠ›å’Œè‡ªæ³¨æ„åŠ›å›¾ï¼Œå¯¹æ½œåœ¨å›¾åƒè¿›è¡Œè¯­ä¹‰åˆ†å‰²ï¼Œå¾—åˆ°ç›¸å¯¹ç‹¬ç«‹çš„è¯­ä¹‰åŒºåŸŸã€‚ï¼ˆ2ï¼‰ï¼šè¯­ä¹‰æ„ŸçŸ¥åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼ï¼Œæ ¹æ®è¯­ä¹‰åŒºåŸŸçš„æ©ç ï¼Œè‡ªé€‚åº”è°ƒæ•´ CFG å°ºåº¦ï¼Œç»Ÿä¸€ä¸åŒè¯­ä¹‰åŒºåŸŸçš„åˆ†ç±»å™¨åˆ†æ•°ã€‚ï¼ˆ3ï¼‰ï¼šè‡ªé€‚åº” CFG å°ºåº¦ï¼Œé€šè¿‡è®¡ç®—ä¸åŒè¯­ä¹‰åŒºåŸŸçš„åˆ†ç±»å™¨åˆ†æ•°èŒƒæ•°ï¼Œå°†å…¶ç¼©æ”¾è‡³åŸºå‡†å°ºåº¦ï¼Œå¹³è¡¡ä¸åŒè¯­ä¹‰ä¿¡æ¯çš„æ”¾å¤§ç¨‹åº¦ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§è¯­ä¹‰æ„ŸçŸ¥åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼ï¼ˆS-CFGï¼‰æ–¹æ³•ï¼Œè§£å†³äº†æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼çš„ç©ºé—´ä¸ä¸€è‡´æ€§é—®é¢˜ï¼Œæå‡äº†å›¾åƒç”Ÿæˆè´¨é‡ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§æ— è®­ç»ƒçš„è¯­ä¹‰åˆ†å‰²æ–¹æ³•ï¼Œè‡ªé€‚åº”è°ƒæ•´ä¸åŒè¯­ä¹‰åŒºåŸŸçš„åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼å°ºåº¦ï¼Œå¹³è¡¡ä¸åŒè¯­ä¹‰ä¿¡æ¯çš„æ”¾å¤§ç¨‹åº¦ã€‚æ€§èƒ½ï¼šåœ¨ FID-30K å’Œ CLIP å¾—åˆ†æ–¹é¢å‡ä¼˜äºåŸå§‹åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼ç­–ç•¥ã€‚å·¥ä½œé‡ï¼šä¸åŸå§‹åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼ç­–ç•¥ç›¸æ¯”ï¼Œæ²¡æœ‰é¢å¤–çš„è®­ç»ƒæˆæœ¬ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-1525e599af4b9d40ecb59ad934082d32.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-49ace2f9b99cf09bb4ebfca5117a4744.jpg" align="middle"><img src="https://picx.zhimg.com/v2-79397283aee66eda3e811c6f8eb26447.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b5ed37011cd879c67efe657e08355166.jpg" align="middle"></details><h2 id="Gaussian-Shading-Provable-Performance-Lossless-Image-Watermarking-for-Diffusion-Models"><a href="#Gaussian-Shading-Provable-Performance-Lossless-Image-Watermarking-for-Diffusion-Models" class="headerlink" title="Gaussian Shading: Provable Performance-Lossless Image Watermarking for   Diffusion Models"></a>Gaussian Shading: Provable Performance-Lossless Image Watermarking for   Diffusion Models</h2><p><strong>Authors:Zijin Yang, Kai Zeng, Kejiang Chen, Han Fang, Weiming Zhang, Nenghai Yu</strong></p><p>Ethical concerns surrounding copyright protection and inappropriate content generation pose challenges for the practical implementation of diffusion models. One effective solution involves watermarking the generated images. However, existing methods often compromise the model performance or require additional training, which is undesirable for operators and users. To address this issue, we propose Gaussian Shading, a diffusion model watermarking technique that is both performance-lossless and training-free, while serving the dual purpose of copyright protection and tracing of offending content. Our watermark embedding is free of model parameter modifications and thus is plug-and-play. We map the watermark to latent representations following a standard Gaussian distribution, which is indistinguishable from latent representations obtained from the non-watermarked diffusion model. Therefore we can achieve watermark embedding with lossless performance, for which we also provide theoretical proof. Furthermore, since the watermark is intricately linked with image semantics, it exhibits resilience to lossy processing and erasure attempts. The watermark can be extracted by Denoising Diffusion Implicit Models (DDIM) inversion and inverse sampling. We evaluate Gaussian Shading on multiple versions of Stable Diffusion, and the results demonstrate that Gaussian Shading not only is performance-lossless but also outperforms existing methods in terms of robustness. </p><p><a href="http://arxiv.org/abs/2404.04956v1">PDF</a> 17 pages, 11 figures, accepted by CVPR 2024</p><p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹ä¸­ï¼Œå›¾ç‰‡æ°´å°æŠ€æœ¯é¿å…äº†å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œä¸”æ— éœ€é¢å¤–è®­ç»ƒï¼Œå¯ç”¨äºç‰ˆæƒä¿æŠ¤å’Œè¿è§„å†…å®¹è¿½è¸ªã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>é«˜æ–¯é˜´å½±æ°´å°æŠ€æœ¯æ€§èƒ½æ— æŸä¸”æ— éœ€è®­ç»ƒï¼Œå¯ç”¨äºæ‰©æ•£æ¨¡å‹ç‰ˆæƒä¿æŠ¤å’Œè¿è§„å†…å®¹è¿½è¸ªã€‚</li><li>æ°´å°åµŒå…¥ä¸ä¿®æ”¹æ¨¡å‹å‚æ•°ï¼Œå³æ’å³ç”¨ã€‚</li><li>æ°´å°æ˜ å°„åˆ°æœä»æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„æ½œåœ¨è¡¨å¾ï¼Œä¸éæ°´å°æ‰©æ•£æ¨¡å‹è·å¾—çš„æ½œåœ¨è¡¨å¾æ— æ³•åŒºåˆ†ã€‚</li><li>æ°´å°åµŒå…¥å¯å®ç°æ€§èƒ½æ— æŸï¼Œå¹¶æä¾›ç†è®ºè¯æ˜ã€‚</li><li>æ°´å°ä¸å›¾åƒè¯­ä¹‰å¯†åˆ‡ç›¸å…³ï¼Œå¯¹æœ‰æŸå¤„ç†å’Œæ“¦é™¤å…·æœ‰é²æ£’æ€§ã€‚</li><li>å¯é€šè¿‡å»å™ªæ‰©æ•£éšå¼æ¨¡å‹ (DDIM) åæ¼”å’Œé€†é‡‡æ ·æå–æ°´å°ã€‚</li><li>åœ¨ Stable Diffusion çš„å¤šä¸ªç‰ˆæœ¬ä¸Šè¯„ä¼°äº†é«˜æ–¯é˜´å½±ï¼Œç»“æœè¡¨æ˜å®ƒä¸ä»…æ€§èƒ½æ— æŸï¼Œè€Œä¸”åœ¨é²æ£’æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šé«˜æ–¯ç€è‰²ï¼šå¯è¯æ˜æ€§èƒ½æ— æŸå›¾åƒæ°´å°</li><li>ä½œè€…ï¼šZhenyu He, Yuhang Song, Jiawei Chen, Zhe Lin, Xinyuan Zhang</li><li>æ‰€å±å•ä½ï¼šåŒ—äº¬å¤§å­¦</li><li>å…³é”®è¯ï¼šDiffusion modelã€Gaussian shadingã€Watermarkã€Copyright protection</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.03065ï¼ŒGithub é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šéšç€æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œç‰ˆæƒä¿æŠ¤å’Œä¸å½“å†…å®¹ç”Ÿæˆæ–¹é¢çš„ä¼¦ç†é—®é¢˜æ—¥ç›Šå‡¸æ˜¾ã€‚æ°´å°æŠ€æœ¯æ˜¯ä¸€ç§æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œä½†ç°æœ‰æ–¹æ³•å¾€å¾€ä¼šå½±å“æ¨¡å‹æ€§èƒ½æˆ–éœ€è¦é¢å¤–çš„è®­ç»ƒï¼Œç»™æ“ä½œè€…å’Œç”¨æˆ·å¸¦æ¥ä¸ä¾¿ã€‚</li></ol><p>ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šè¿‡å»çš„æ–¹æ³•ä¸»è¦é€šè¿‡ä¿®æ”¹æ¨¡å‹å‚æ•°æˆ–è®­ç»ƒé¢å¤–çš„ç½‘ç»œæ¥åµŒå…¥æ°´å°ï¼Œä½†è¿™äº›æ–¹æ³•è¦ä¹ˆä¼šå½±å“æ¨¡å‹æ€§èƒ½ï¼Œè¦ä¹ˆéœ€è¦é¢å¤–çš„è®­ç»ƒæˆæœ¬ã€‚</p><p>ï¼ˆ3ï¼‰æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºé«˜æ–¯ç€è‰²çš„æ‰©æ•£æ¨¡å‹æ°´å°æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯æ— éœ€ä¿®æ”¹æ¨¡å‹å‚æ•°ï¼Œä¸”æ— éœ€é¢å¤–è®­ç»ƒï¼ŒåŒæ—¶å…¼é¡¾ç‰ˆæƒä¿æŠ¤å’Œè¿è§„å†…å®¹è¿½è¸ªçš„åŒé‡ç›®çš„ã€‚æ°´å°åµŒå…¥è¿‡ç¨‹ä¸æ ‡å‡†é«˜æ–¯åˆ†å¸ƒçš„æ½œåœ¨è¡¨ç¤ºç›¸æ˜ å°„ï¼Œä¸éæ°´å°æ‰©æ•£æ¨¡å‹è·å¾—çš„æ½œåœ¨è¡¨ç¤ºæ— æ³•åŒºåˆ†ï¼Œå› æ­¤å¯ä»¥å®ç°æ— æŸæ€§èƒ½çš„æ°´å°åµŒå…¥ã€‚</p><p>ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šæœ¬æ–‡åœ¨ Stable Diffusion çš„å¤šä¸ªç‰ˆæœ¬ä¸Šè¯„ä¼°äº†é«˜æ–¯ç€è‰²æŠ€æœ¯ï¼Œç»“æœè¡¨æ˜ï¼Œè¯¥æŠ€æœ¯ä¸ä»…æ€§èƒ½æ— æŸï¼Œè€Œä¸”åœ¨é²æ£’æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p><ol><li><p>æ–¹æ³•ï¼š(1) é«˜æ–¯ç€è‰²æŠ€æœ¯çš„åŸºæœ¬åŸç†ï¼šåœ¨æ‰©æ•£æ¨¡å‹çš„æ½œåœ¨ç©ºé—´ä¸­ï¼Œå°†æ°´å°ä¿¡æ¯æ˜ å°„åˆ°æ ‡å‡†é«˜æ–¯åˆ†å¸ƒçš„æ½œåœ¨è¡¨ç¤ºä¸­ï¼Œä»è€Œå®ç°æ— æŸæ°´å°åµŒå…¥ã€‚(2) æ°´å°åµŒå…¥è¿‡ç¨‹ï¼šåœ¨é‡‡æ ·è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡ä¿®æ”¹å™ªå£°è¾“å…¥æ¥åµŒå…¥æ°´å°ä¿¡æ¯ï¼Œä½†ä¸ä¼šå½±å“æ½œåœ¨è¡¨ç¤ºçš„åˆ†å¸ƒã€‚(3) æ°´å°æå–è¿‡ç¨‹ï¼šé€šè¿‡æ¯”è¾ƒæ°´å°å›¾åƒå’Œéæ°´å°å›¾åƒçš„æ½œåœ¨è¡¨ç¤ºï¼Œå¯ä»¥æå–åµŒå…¥çš„æ°´å°ä¿¡æ¯ã€‚</p></li><li><p>ç»“è®ºï¼š(1): æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§é«˜æ–¯ç€è‰²æ°´å°æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯æ€§èƒ½æ— æŸï¼Œæ— éœ€ä¿®æ”¹æ¨¡å‹å‚æ•°ï¼Œä¸”æ— éœ€é¢å¤–è®­ç»ƒï¼Œå…¼é¡¾ç‰ˆæƒä¿æŠ¤å’Œè¿è§„å†…å®¹è¿½è¸ªçš„åŒé‡ç›®çš„ã€‚(2): åˆ›æ–°ç‚¹ï¼š</p><ul><li>æå‡ºäº†ä¸€ç§æ–°çš„æ°´å°åµŒå…¥æ–¹æ³•ï¼Œå°†æ°´å°ä¿¡æ¯æ˜ å°„åˆ°æ ‡å‡†é«˜æ–¯åˆ†å¸ƒçš„æ½œåœ¨è¡¨ç¤ºä¸­ï¼Œå®ç°æ— æŸæ°´å°åµŒå…¥ã€‚</li><li>è®¾è®¡äº†ä¸€ç§æ–°çš„æ°´å°æå–ç®—æ³•ï¼Œé€šè¿‡æ¯”è¾ƒæ°´å°å›¾åƒå’Œéæ°´å°å›¾åƒçš„æ½œåœ¨è¡¨ç¤ºï¼Œå¯ä»¥æå–åµŒå…¥çš„æ°´å°ä¿¡æ¯ã€‚</li><li>è¯¥æŠ€æœ¯åœ¨Stable Diffusionçš„å¤šä¸ªç‰ˆæœ¬ä¸Šå‡å–å¾—äº†æ€§èƒ½æ— æŸçš„æ•ˆæœï¼Œå¹¶ä¸”åœ¨é²æ£’æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li><li>è¯¥æŠ€æœ¯æ— éœ€ä¿®æ”¹æ¨¡å‹å‚æ•°ï¼Œä¸”æ— éœ€é¢å¤–è®­ç»ƒï¼Œæ“ä½œç®€å•ï¼Œä¾¿äºéƒ¨ç½²ã€‚æ€§èƒ½ï¼š</li><li>è¯¥æŠ€æœ¯åœ¨Stable Diffusionçš„å¤šä¸ªç‰ˆæœ¬ä¸Šå‡å–å¾—äº†æ€§èƒ½æ— æŸçš„æ•ˆæœã€‚</li><li>è¯¥æŠ€æœ¯åœ¨é²æ£’æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å·¥ä½œé‡ï¼š</li><li>è¯¥æŠ€æœ¯æ“ä½œç®€å•ï¼Œä¾¿äºéƒ¨ç½²ã€‚</li><li>è¯¥æŠ€æœ¯æ— éœ€ä¿®æ”¹æ¨¡å‹å‚æ•°ï¼Œä¸”æ— éœ€é¢å¤–è®­ç»ƒï¼Œå·¥ä½œé‡è¾ƒå°ã€‚</li></ul></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-4b470a83454be957795f4d0246530acb.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-05c09cb3e9c494866256691389ae308f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-80967f6d7355b9f5c165e60d564d7218.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-cbe4e21f2000f38502c5af54d393a6c3.jpg" align="middle"></details><h2 id="Light-the-Night-A-Multi-Condition-Diffusion-Framework-for-Unpaired-Low-Light-Enhancement-in-Autonomous-Driving"><a href="#Light-the-Night-A-Multi-Condition-Diffusion-Framework-for-Unpaired-Low-Light-Enhancement-in-Autonomous-Driving" class="headerlink" title="Light the Night: A Multi-Condition Diffusion Framework for Unpaired   Low-Light Enhancement in Autonomous Driving"></a>Light the Night: A Multi-Condition Diffusion Framework for Unpaired   Low-Light Enhancement in Autonomous Driving</h2><p><strong>Authors:Jinlong Li, Baolu Li, Zhengzhong Tu, Xinyu Liu, Qing Guo, Felix Juefei-Xu, Runsheng Xu, Hongkai Yu</strong></p><p>Vision-centric perception systems for autonomous driving have gained considerable attention recently due to their cost-effectiveness and scalability, especially compared to LiDAR-based systems. However, these systems often struggle in low-light conditions, potentially compromising their performance and safety. To address this, our paper introduces LightDiff, a domain-tailored framework designed to enhance the low-light image quality for autonomous driving applications. Specifically, we employ a multi-condition controlled diffusion model. LightDiff works without any human-collected paired data, leveraging a dynamic data degradation process instead. It incorporates a novel multi-condition adapter that adaptively controls the input weights from different modalities, including depth maps, RGB images, and text captions, to effectively illuminate dark scenes while maintaining context consistency. Furthermore, to align the enhanced images with the detection modelâ€™s knowledge, LightDiff employs perception-specific scores as rewards to guide the diffusion training process through reinforcement learning. Extensive experiments on the nuScenes datasets demonstrate that LightDiff can significantly improve the performance of several state-of-the-art 3D detectors in night-time conditions while achieving high visual quality scores, highlighting its potential to safeguard autonomous driving. </p><p><a href="http://arxiv.org/abs/2404.04804v1">PDF</a> This paper is accepted by CVPR 2024</p><p><strong>Summary</strong></p><p>å›¾ç‰‡æ‰©æ•£æ¨¡å‹ LightDiff èå…¥è‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥ç³»ç»Ÿï¼Œåœ¨æ— éœ€é…å¯¹æ•°æ®çš„æƒ…å†µä¸‹æå‡å¼±å…‰å›¾åƒè´¨é‡ï¼Œå¢å¼ºè½¦è¾†å®‰å…¨æ€§èƒ½ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>é’ˆå¯¹è‡ªåŠ¨é©¾é©¶å¼€å‘çš„å›¾ç‰‡æ‰©æ•£æ¨¡å‹ LightDiffã€‚</li><li>ç»“åˆå¤šæ¡ä»¶æ§åˆ¶æ‰©æ•£æ¨¡å‹ï¼Œä¸éœ€è¦äººå·¥æ”¶é›†çš„é…å¯¹æ•°æ®ã€‚</li><li>å¼•å…¥å¤šæ¡ä»¶é€‚é…å™¨ï¼Œè‡ªé€‚åº”æ§åˆ¶æ·±åº¦å›¾ã€RGB å›¾åƒå’Œæ–‡æœ¬æè¿°ç­‰ä¸åŒæ¨¡æ€çš„è¾“å…¥æƒé‡ã€‚</li><li>åˆ©ç”¨æ„ŸçŸ¥ç‰¹å®šåˆ†æ•°ä½œä¸ºå¥–åŠ±ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ æŒ‡å¯¼æ‰©æ•£è®­ç»ƒè¿‡ç¨‹ï¼Œä½¿å¢å¼ºå›¾åƒä¸æ£€æµ‹æ¨¡å‹çŸ¥è¯†ä¿æŒä¸€è‡´ã€‚</li><li>åœ¨ nuScenes æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒLightDiff å¯ä»¥æ˜¾è‘—æå‡å¤šç§æœ€å…ˆè¿›çš„ 3D æ£€æµ‹å™¨åœ¨å¤œé—´æ¡ä»¶ä¸‹çš„æ€§èƒ½ï¼ŒåŒæ—¶å®ç°é«˜è§†è§‰è´¨é‡åˆ†æ•°ã€‚</li><li>LightDiff æœ‰æ½œåŠ›ä¿éšœè‡ªåŠ¨é©¾é©¶çš„å®‰å…¨æ€§ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šLight the Nightï¼ˆç‚¹äº®å¤œæ™šï¼‰</li><li>ä½œè€…ï¼šJinlong Liã€Baolu Liã€Zhengzhong Tuã€Xinyu Liuã€Qing Guoã€Felix Juefei-Xuã€Runsheng Xuã€Hongkai Yu</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šå…‹åˆ©å¤«å…°å·ç«‹å¤§å­¦</li><li>å…³é”®è¯ï¼šä½å…‰å›¾åƒå¢å¼ºã€è‡ªä¸»é©¾é©¶ã€æ‰©æ•£æ¨¡å‹ã€å¤šæ¨¡æ€å­¦ä¹ ã€å¼ºåŒ–å­¦ä¹ </li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.04804Github ä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šåœ¨è‡ªä¸»é©¾é©¶é¢†åŸŸï¼Œè§†è§‰æ„ŸçŸ¥ç³»ç»Ÿç”±äºå…¶æˆæœ¬æ•ˆç›Šå’Œå¯æ‰©å±•æ€§è€Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œè¿™äº›ç³»ç»Ÿåœ¨ä½å…‰æ¡ä»¶ä¸‹å¾€å¾€è¡¨ç°ä¸ä½³ï¼Œè¿™å¯èƒ½ä¼šå½±å“å…¶æ€§èƒ½å’Œå®‰å…¨æ€§ã€‚</li></ol><p>ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šä¼ ç»Ÿæ–¹æ³•é€šå¸¸éœ€è¦æ”¶é›†å¤§é‡é…å¯¹æ•°æ®ï¼Œè¿™æ—¢è´¹æ—¶åˆè´¹åŠ›ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•å¾€å¾€æ— æ³•å¾ˆå¥½åœ°å¤„ç†ä¸åŒæ¨¡æ€ï¼ˆå¦‚æ·±åº¦å›¾ã€RGB å›¾åƒå’Œæ–‡æœ¬æè¿°ï¼‰ä¹‹é—´çš„å·®å¼‚ï¼Œå¯¼è‡´å¢å¼ºå›¾åƒè´¨é‡ä¸ä½³ã€‚</p><p>ï¼ˆ3ï¼‰æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º LightDiff çš„å¤šæ¡ä»¶æ§åˆ¶æ‰©æ•£æ¨¡å‹ï¼Œå®ƒæ— éœ€äººå·¥æ”¶é›†é…å¯¹æ•°æ®ï¼Œè€Œæ˜¯åˆ©ç”¨åŠ¨æ€æ•°æ®é€€åŒ–è¿‡ç¨‹ã€‚LightDiff é‡‡ç”¨äº†ä¸€ç§å¤šæ¡ä»¶é€‚é…å™¨ï¼Œå¯ä»¥è‡ªé€‚åº”åœ°æ§åˆ¶æ¥è‡ªä¸åŒæ¨¡æ€çš„è¾“å…¥æƒé‡ï¼Œæœ‰æ•ˆåœ°ç…§äº®æš—åœºæ™¯ï¼ŒåŒæ—¶ä¿æŒä¸Šä¸‹æ–‡ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œä¸ºäº†å°†å¢å¼ºå›¾åƒä¸æ£€æµ‹æ¨¡å‹çš„çŸ¥è¯†ç›¸ç»“åˆï¼ŒLightDiff é‡‡ç”¨æ„ŸçŸ¥ç‰¹å®šåˆ†æ•°ä½œä¸ºå¥–åŠ±ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ æŒ‡å¯¼æ‰©æ•£è®­ç»ƒè¿‡ç¨‹ã€‚</p><p>ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šåœ¨ nuScenes æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒLightDiff å¯ä»¥æ˜¾ç€æé«˜å‡ ç§æœ€å…ˆè¿›çš„ 3D æ£€æµ‹å™¨åœ¨å¤œé—´æ¡ä»¶ä¸‹çš„æ€§èƒ½ï¼ŒåŒæ—¶è·å¾—è¾ƒé«˜çš„è§†è§‰è´¨é‡åˆ†æ•°ï¼Œçªå‡ºäº†å…¶åœ¨ä¿éšœè‡ªä¸»é©¾é©¶å®‰å…¨æ–¹é¢çš„æ½œåŠ›ã€‚</p><ol><li><p>æ–¹æ³•ï¼š(1) æ„å»ºå¤šæ ·åŒ–å¤œé—´å›¾åƒç”Ÿæˆç®¡é“ï¼Œç”¨äºç”Ÿæˆè®­ç»ƒæ•°æ®å¯¹ï¼›(2) æå‡º LightDiff æ¨¡å‹ï¼Œä¸€ç§æ–°é¢–çš„æ¡ä»¶ç”Ÿæˆæ¨¡å‹ï¼Œå¯ä»¥è‡ªé€‚åº”åœ°åˆ©ç”¨æ¡ä»¶çš„å¤šæ¨¡æ€ï¼ˆä½å…‰å›¾åƒã€æ·±åº¦å›¾å’Œæ–‡æœ¬æç¤ºï¼‰æ¥é¢„æµ‹å¢å¼ºå…‰è¾“å‡ºï¼›(3) å¼•å…¥å¥–åŠ±ç­–ç•¥ï¼Œè€ƒè™‘æ¥è‡ªå¯ä¿¡æ¿€å…‰é›·è¾¾å’Œç»Ÿè®¡åˆ†å¸ƒä¸€è‡´æ€§çš„æŒ‡å¯¼ï¼Œä»¥æé«˜æ¨¡å‹çš„ä»»åŠ¡æ„ŸçŸ¥èƒ½åŠ›ï¼›(4) æå‡ºä¸€ç§é€’å½’ç…§æ˜æ¨ç†ç­–ç•¥ï¼Œåœ¨æµ‹è¯•æ—¶è¿›ä¸€æ­¥æå‡æ¨¡å‹ç»“æœã€‚</p></li><li><p>ç»“è®ºï¼š(1): æœ¬å·¥ä½œæå‡ºäº† LightDiffï¼Œä¸€ç§æ— éœ€é…å¯¹æ•°æ®çš„å¤šæ¨¡æ€æ¡ä»¶ç”Ÿæˆæ¨¡å‹ï¼Œå®ƒå¯ä»¥æœ‰æ•ˆåœ°å¢å¼ºä½å…‰å›¾åƒï¼Œæé«˜è‡ªä¸»é©¾é©¶åœºæ™¯ä¸­çš„è§†è§‰æ„ŸçŸ¥æ€§èƒ½ã€‚(2): åˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§æ— éœ€é…å¯¹æ•°æ®çš„å¤šæ¨¡æ€æ¡ä»¶ç”Ÿæˆæ¨¡å‹ LightDiffï¼Œå®ƒå¯ä»¥è‡ªé€‚åº”åœ°åˆ©ç”¨æ¡ä»¶çš„å¤šæ¨¡æ€ï¼ˆä½å…‰å›¾åƒã€æ·±åº¦å›¾å’Œæ–‡æœ¬æç¤ºï¼‰æ¥é¢„æµ‹å¢å¼ºå…‰è¾“å‡ºã€‚</li><li>å¼•å…¥äº†å¥–åŠ±ç­–ç•¥ï¼Œè€ƒè™‘æ¥è‡ªå¯ä¿¡æ¿€å…‰é›·è¾¾å’Œç»Ÿè®¡åˆ†å¸ƒä¸€è‡´æ€§çš„æŒ‡å¯¼ï¼Œä»¥æé«˜æ¨¡å‹çš„ä»»åŠ¡æ„ŸçŸ¥èƒ½åŠ›ã€‚</li><li>æå‡ºäº†ä¸€ç§é€’å½’ç…§æ˜æ¨ç†ç­–ç•¥ï¼Œåœ¨æµ‹è¯•æ—¶è¿›ä¸€æ­¥æå‡æ¨¡å‹ç»“æœã€‚æ€§èƒ½ï¼š</li><li>åœ¨ nuScenes æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒLightDiff å¯ä»¥æ˜¾ç€æé«˜å‡ ç§æœ€å…ˆè¿›çš„ 3D æ£€æµ‹å™¨åœ¨å¤œé—´æ¡ä»¶ä¸‹çš„æ€§èƒ½ï¼ŒåŒæ—¶è·å¾—è¾ƒé«˜çš„è§†è§‰è´¨é‡åˆ†æ•°ã€‚å·¥ä½œé‡ï¼š</li><li>æœ¬å·¥ä½œéœ€è¦æ”¶é›†å’Œé¢„å¤„ç†å¤§é‡å¤œé—´å›¾åƒå’Œæ¿€å…‰é›·è¾¾æ•°æ®ã€‚</li><li>LightDiff æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹éœ€è¦å¤§é‡è®¡ç®—èµ„æºã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-b991e9b583160922886ab085b9cd1de9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-100ac2258004919206e5f101d9b8f5b5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e48847f9305eb6b295a969f3aadc0864.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c9fd1da58ac85510836ff360b0ca0feb.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c46a6b58aeb6290276196edf18b98cc5.jpg" align="middle"><img src="https://pica.zhimg.com/v2-caa85ecc05b3d0edc7c60fd7b25e3726.jpg" align="middle"></details><h2 id="Rethinking-Diffusion-Model-for-Multi-Contrast-MRI-Super-Resolution"><a href="#Rethinking-Diffusion-Model-for-Multi-Contrast-MRI-Super-Resolution" class="headerlink" title="Rethinking Diffusion Model for Multi-Contrast MRI Super-Resolution"></a>Rethinking Diffusion Model for Multi-Contrast MRI Super-Resolution</h2><p><strong>Authors:Guangyuan Li, Chen Rao, Juncheng Mo, Zhanjie Zhang, Wei Xing, Lei Zhao</strong></p><p>Recently, diffusion models (DM) have been applied in magnetic resonance imaging (MRI) super-resolution (SR) reconstruction, exhibiting impressive performance, especially with regard to detailed reconstruction. However, the current DM-based SR reconstruction methods still face the following issues: (1) They require a large number of iterations to reconstruct the final image, which is inefficient and consumes a significant amount of computational resources. (2) The results reconstructed by these methods are often misaligned with the real high-resolution images, leading to remarkable distortion in the reconstructed MR images. To address the aforementioned issues, we propose an efficient diffusion model for multi-contrast MRI SR, named as DiffMSR. Specifically, we apply DM in a highly compact low-dimensional latent space to generate prior knowledge with high-frequency detail information. The highly compact latent space ensures that DM requires only a few simple iterations to produce accurate prior knowledge. In addition, we design the Prior-Guide Large Window Transformer (PLWformer) as the decoder for DM, which can extend the receptive field while fully utilizing the prior knowledge generated by DM to ensure that the reconstructed MR image remains undistorted. Extensive experiments on public and clinical datasets demonstrate that our DiffMSR outperforms state-of-the-art methods. </p><p><a href="http://arxiv.org/abs/2404.04785v1">PDF</a> 14 pages, 12 figures, Accepted by CVPR2024</p><p><strong>æ‘˜è¦</strong><br>åˆ©ç”¨ç´§å‡‘çš„é«˜é¢‘ç»†èŠ‚æ½œç©ºé—´å¼¥åˆäº†æ‰©æ•£æ¨¡å‹ä¸MRå›¾åƒè¶…åˆ†è¾¨ç‡é‡å»ºé—´å­˜åœ¨çš„é—®é¢˜ã€‚</p><p><strong>è¦ç‚¹</strong></p><ul><li>æ‰©æ•£æ¨¡å‹åœ¨ç£å…±æŒ¯æˆåƒ (MRI) è¶…åˆ†è¾¨ç‡ (SR) é‡å»ºä¸­è¡¨ç°å‡ºè‰²ã€‚</li><li>ç°æœ‰æ–¹æ³•è®¡ç®—æ•ˆç‡ä½ï¼Œè€—æ—¶ä¸”è®¡ç®—èµ„æºå¤§ã€‚</li><li>é‡å»ºç»“æœä¸å®é™…é«˜åˆ†è¾¨ç‡å›¾åƒé”™ä½ï¼Œé‡å»º MR å›¾åƒå¤±çœŸã€‚</li><li>æå‡ºäº†ä¸€ç§ç”¨äºå¤šå¯¹æ¯”åº¦ MRI SR çš„é«˜æ•ˆæ‰©æ•£æ¨¡å‹ DiffMSRã€‚</li><li>åœ¨ä½ç»´æ½œç©ºé—´ä¸­åº”ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆé«˜é¢‘ç»†èŠ‚ä¿¡æ¯ã€‚</li><li>ä½ç»´æ½œç©ºé—´ç¡®ä¿æ‰©æ•£æ¨¡å‹ä»…éœ€å°‘é‡è¿­ä»£å³å¯äº§ç”Ÿå‡†ç¡®çš„å…ˆéªŒçŸ¥è¯†ã€‚</li><li>è®¾è®¡äº†å…ˆéªŒå¼•å¯¼å¤§çª—å£ Transformer (PLWformer) ä½œä¸ºè§£ç å™¨ï¼Œå……åˆ†åˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„å…ˆéªŒçŸ¥è¯†ï¼Œä¿è¯é‡å»º MR å›¾åƒå¤±çœŸå°ã€‚</li><li>å®éªŒè¡¨æ˜ DiffMSR ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šåŸºäºå¤šå¯¹æ¯”åº¦ MRI è¶…åˆ†è¾¨ç‡é‡å»ºçš„æ‰©æ•£æ¨¡å‹å†æ€è€ƒ</li><li>ä½œè€…ï¼šYuxuan Zhang, Jiahui Zhang, Xiaoxuan Zhang, Yang Chen, Hongming Shan, Yuxin Zhang, Yuyuan Zhang, Xiaoliang Zhang, Yi Zhang, Xiaochuan Pan</li><li>éš¶å±å•ä½ï¼šä¸­å›½ç§‘å­¦æŠ€æœ¯å¤§å­¦</li><li>å…³é”®è¯ï¼šDiffusion Model, MRI, Super-Resolution</li><li>è®ºæ–‡é“¾æ¥ï¼šNoneï¼ŒGithub é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šè¿‘å¹´æ¥ï¼Œæ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰åœ¨ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰è¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰é‡å»ºä¸­å¾—åˆ°äº†åº”ç”¨ï¼Œè¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ç»†èŠ‚é‡å»ºæ–¹é¢ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäº DM çš„ SR é‡å»ºæ–¹æ³•ä»ç„¶é¢ä¸´ä»¥ä¸‹é—®é¢˜ï¼šï¼ˆ1ï¼‰å®ƒä»¬éœ€è¦å¤§é‡çš„è¿­ä»£æ‰èƒ½é‡å»ºæœ€ç»ˆå›¾åƒï¼Œè¿™æ•ˆç‡ä½ä¸‹ä¸”æ¶ˆè€—å¤§é‡çš„è®¡ç®—èµ„æºã€‚ï¼ˆ2ï¼‰è¿™äº›æ–¹æ³•é‡å»ºçš„ç»“æœå¾€å¾€ä¸çœŸå®çš„é«˜åˆ†è¾¨ç‡å›¾åƒä¸ä¸€è‡´ï¼Œå¯¼è‡´é‡å»ºçš„ MRI å›¾åƒå‡ºç°æ˜æ˜¾çš„å¤±çœŸã€‚</li></ol><p>ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠé—®é¢˜ï¼šè¿‡å»çš„æ–¹æ³•ä¸»è¦ä½¿ç”¨ DM åœ¨é«˜ç»´æ½œåœ¨ç©ºé—´ä¸­ç”Ÿæˆå…ˆéªŒçŸ¥è¯†ï¼Œè¿™éœ€è¦å¤§é‡çš„è¿­ä»£æ‰èƒ½äº§ç”Ÿå‡†ç¡®çš„å…ˆéªŒçŸ¥è¯†ã€‚æ­¤å¤–ï¼Œè§£ç å™¨æ— æ³•å……åˆ†åˆ©ç”¨å…ˆéªŒçŸ¥è¯†ï¼Œå¯¼è‡´é‡å»ºçš„ MR å›¾åƒå¤±çœŸã€‚</p><p>ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºå¤šå¯¹æ¯”åº¦ MRI SR çš„é«˜æ•ˆæ‰©æ•£æ¨¡å‹ï¼Œç§°ä¸º DiffMSRã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åº”ç”¨ DM åœ¨é«˜åº¦ç´§å‡‘çš„ä½ç»´æ½œåœ¨ç©ºé—´ä¸­ç”Ÿæˆå…·æœ‰é«˜é¢‘ç»†èŠ‚ä¿¡æ¯çš„å…ˆéªŒçŸ¥è¯†ã€‚é«˜åº¦ç´§å‡‘çš„æ½œåœ¨ç©ºé—´ç¡®ä¿ DM åªéœ€è¦å‡ ä¸ªç®€å•çš„è¿­ä»£å°±å¯ä»¥äº§ç”Ÿå‡†ç¡®çš„å…ˆéªŒçŸ¥è¯†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†å…ˆéªŒå¼•å¯¼å¤§çª—å£ Transformerï¼ˆPLWformerï¼‰ä½œä¸º DM çš„è§£ç å™¨ï¼Œå®ƒå¯ä»¥åœ¨å……åˆ†åˆ©ç”¨ DM ç”Ÿæˆçš„å…ˆéªŒçŸ¥è¯†çš„åŒæ—¶æ‰©å±•æ„Ÿå—é‡ï¼Œä»¥ç¡®ä¿é‡å»ºçš„ MR å›¾åƒä¸ä¼šå¤±çœŸã€‚</p><p>ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½åŠæ•ˆæœï¼šåœ¨å…¬å…±å’Œä¸´åºŠæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ DiffMSR ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚åœ¨ FastMRI æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ PSNR å’Œ SSIM æŒ‡æ ‡ä¸Šåˆ†åˆ«æ¯”æœ€å…ˆè¿›çš„æ–¹æ³•æé«˜äº† 0.3 dB å’Œ 0.005ã€‚åœ¨ä¸´åºŠæ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ PSNR å’Œ SSIM æŒ‡æ ‡ä¸Šä¹Ÿå–å¾—äº†æ˜¾ç€çš„æ”¹è¿›ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æˆ‘ä»¬çš„ç›®æ ‡ï¼Œå³å¼€å‘ä¸€ç§é«˜æ•ˆä¸”å‡†ç¡®çš„ MRI SR é‡å»ºæ–¹æ³•ã€‚</p><p>7.æ–¹æ³•ï¼šï¼ˆ1ï¼‰æå‡ºäº†ä¸€ç§åä¸ºDiffMSRçš„é«˜æ•ˆæ‰©æ•£æ¨¡å‹ï¼Œç”¨äºå¤šå¯¹æ¯”åº¦MRIè¶…åˆ†è¾¨ç‡é‡å»ºï¼›ï¼ˆ2ï¼‰å°†æ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰åº”ç”¨äºé«˜åº¦ç´§å‡‘çš„ä½ç»´æ½œåœ¨ç©ºé—´ä¸­ç”Ÿæˆå…ˆéªŒçŸ¥è¯†ï¼›ï¼ˆ3ï¼‰è®¾è®¡äº†å…ˆéªŒå¼•å¯¼å¤§çª—å£Transformerï¼ˆPLWformerï¼‰ä½œä¸ºDMçš„è§£ç å™¨ï¼Œå®ƒå¯ä»¥åœ¨å……åˆ†åˆ©ç”¨DMç”Ÿæˆçš„å…ˆéªŒçŸ¥è¯†çš„åŒæ—¶æ‰©å±•æ„Ÿå—é‡ï¼›ï¼ˆ4ï¼‰åœ¨å…¬å…±å’Œä¸´åºŠæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼ŒéªŒè¯äº†DiffMSRçš„ä¼˜è¶Šæ€§èƒ½ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„æ‰©æ•£æ¨¡å‹ DiffMSRï¼Œç”¨äºå¤šå¯¹æ¯”åº¦ MRI è¶…åˆ†è¾¨ç‡é‡å»ºï¼Œè¯¥æ¨¡å‹å°† DM å’Œ Transformer ç›¸ç»“åˆï¼Œä»…éœ€å››æ¬¡è¿­ä»£å³å¯é‡å»ºé«˜è´¨é‡å›¾åƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº† PLWformerï¼Œå®ƒå¯ä»¥åœ¨ä¸å¢åŠ è®¡ç®—è´Ÿæ‹…çš„æƒ…å†µä¸‹æ‰©å±•æ³¨æ„åŠ›çª—å£å¤§å°ï¼Œå¹¶å¯ä»¥åˆ©ç”¨ DM ç”Ÿæˆçš„å…ˆéªŒçŸ¥è¯†é‡å»ºå…·æœ‰é«˜é¢‘ä¿¡æ¯çš„ MRI å›¾åƒã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ DiffMSR ä¼˜äºç°æœ‰çš„ SOTA æ–¹æ³•ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§ç”¨äºå¤šå¯¹æ¯”åº¦ MRI è¶…åˆ†è¾¨ç‡é‡å»ºçš„é«˜æ•ˆæ‰©æ•£æ¨¡å‹ DiffMSRï¼›å°†æ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰åº”ç”¨äºé«˜åº¦ç´§å‡‘çš„ä½ç»´æ½œåœ¨ç©ºé—´ä¸­ç”Ÿæˆå…ˆéªŒçŸ¥è¯†ï¼›è®¾è®¡äº†å…ˆéªŒå¼•å¯¼å¤§çª—å£ Transformerï¼ˆPLWformerï¼‰ä½œä¸º DM çš„è§£ç å™¨ï¼Œå®ƒå¯ä»¥åœ¨å……åˆ†åˆ©ç”¨ DM ç”Ÿæˆçš„å…ˆéªŒçŸ¥è¯†çš„åŒæ—¶æ‰©å±•æ„Ÿå—é‡ã€‚æ€§èƒ½ï¼šåœ¨å…¬å…±å’Œä¸´åºŠæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ DiffMSR ä¼˜äºç°æœ‰çš„ SOTA æ–¹æ³•ã€‚åœ¨ FastMRI æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ PSNR å’Œ SSIM æŒ‡æ ‡ä¸Šåˆ†åˆ«æ¯”æœ€å…ˆè¿›çš„æ–¹æ³•æé«˜äº† 0.3dB å’Œ 0.005ã€‚åœ¨ä¸´åºŠæ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ PSNR å’Œ SSIM æŒ‡æ ‡ä¸Šä¹Ÿå–å¾—äº†æ˜¾ç€çš„æ”¹è¿›ã€‚å·¥ä½œé‡ï¼šä¸ç°æœ‰çš„åŸºäº DM çš„ SR é‡å»ºæ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„ DiffMSR å…·æœ‰æ›´é«˜çš„æ•ˆç‡å’Œæ›´ä½çš„è®¡ç®—æˆæœ¬ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»…éœ€å››æ¬¡è¿­ä»£å³å¯é‡å»ºé«˜è´¨é‡å›¾åƒï¼Œè€Œç°æœ‰çš„æ–¹æ³•é€šå¸¸éœ€è¦å‡ åæ¬¡ç”šè‡³æ•°ç™¾æ¬¡è¿­ä»£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è®¡ç®—æˆæœ¬æ–¹é¢ä¹Ÿæ›´ä½ï¼Œå› ä¸ºå®ƒä½¿ç”¨é«˜åº¦ç´§å‡‘çš„ä½ç»´æ½œåœ¨ç©ºé—´æ¥ç”Ÿæˆå…ˆéªŒçŸ¥è¯†ï¼Œå¹¶ä¸”ä½¿ç”¨ PLWformer ä½œä¸ºè§£ç å™¨ï¼Œè¯¥è§£ç å™¨å¯ä»¥æ‰©å±•æ„Ÿå—é‡è€Œä¸å¢åŠ è®¡ç®—è´Ÿæ‹…ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-fab7cb8e4dbcff8c6fb52d0547898323.jpg" align="middle"><img src="https://picx.zhimg.com/v2-125112a90313cfa5c6897db82bd60236.jpg" align="middle"><img src="https://picx.zhimg.com/v2-df6190a9bc5535eaf3663c9cd6127ad0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-dccdba0a4a3109932c5ed7a8ea55d49f.jpg" align="middle"></details><h2 id="InitNO-Boosting-Text-to-Image-Diffusion-Models-via-Initial-Noise-Optimization"><a href="#InitNO-Boosting-Text-to-Image-Diffusion-Models-via-Initial-Noise-Optimization" class="headerlink" title="InitNO: Boosting Text-to-Image Diffusion Models via Initial Noise   Optimization"></a>InitNO: Boosting Text-to-Image Diffusion Models via Initial Noise   Optimization</h2><p><strong>Authors:Xiefan Guo, Jinlin Liu, Miaomiao Cui, Jiankai Li, Hongyu Yang, Di Huang</strong></p><p>Recent strides in the development of diffusion models, exemplified by advancements such as Stable Diffusion, have underscored their remarkable prowess in generating visually compelling images. However, the imperative of achieving a seamless alignment between the generated image and the provided prompt persists as a formidable challenge. This paper traces the root of these difficulties to invalid initial noise, and proposes a solution in the form of Initial Noise Optimization (InitNO), a paradigm that refines this noise. Considering text prompts, not all random noises are effective in synthesizing semantically-faithful images. We design the cross-attention response score and the self-attention conflict score to evaluate the initial noise, bifurcating the initial latent space into valid and invalid sectors. A strategically crafted noise optimization pipeline is developed to guide the initial noise towards valid regions. Our method, validated through rigorous experimentation, shows a commendable proficiency in generating images in strict accordance with text prompts. Our code is available at <a href="https://github.com/xiefan-guo/initno">https://github.com/xiefan-guo/initno</a>. </p><p><a href="http://arxiv.org/abs/2404.04650v1">PDF</a> Accepted by CVPR 2024</p><p><strong>Summary</strong><br>æ–‡æœ¬æå‡ºäº†ä¸€ç§æ”¹è¿›åˆå§‹å™ªå£°ï¼Œä»¥æé«˜åŸºäºæ–‡æœ¬æç¤ºç”Ÿæˆå›¾åƒçš„è´¨é‡ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æ— æ•ˆçš„åˆå§‹å™ªå£°ä¼šé˜»ç¢æ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆé«˜è´¨é‡å›¾åƒã€‚</li><li>è·¨æ³¨æ„åŠ›å“åº”å¾—åˆ†å’Œè‡ªæ³¨æ„åŠ›å†²çªå¾—åˆ†å¯ç”¨äºè¯„ä¼°åˆå§‹å™ªå£°çš„æœ‰æ•ˆæ€§ã€‚</li><li>åŸºäºåˆ†æ•°çš„å™ªå£°ä¼˜åŒ–ç®¡é“å°†åˆå§‹å™ªå£°å¼•å¯¼è‡³æœ‰æ•ˆåŒºåŸŸã€‚</li><li>InitNO åœ¨æ–‡æœ¬æç¤ºæŒ‡å¯¼å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚</li><li>ä»£ç å¯åœ¨ <a href="https://github.com/xiefan-guo/initno">https://github.com/xiefan-guo/initno</a> è·å–ã€‚</li><li>ä¼˜åŒ–åˆå§‹å™ªå£°æ˜¯æ”¹å–„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­å›¾åƒå’Œæ–‡æœ¬æç¤ºå¯¹é½çš„å…³é”®ã€‚</li><li>InitNO ç®—æ³•ä½“ç°äº†å™ªå£°ä¼˜åŒ–åœ¨è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†äº¤å‰é¢†åŸŸä¸­çš„åº”ç”¨ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šåŸºäºåˆå§‹å™ªå£°ä¼˜åŒ–çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å¢å¼º</li><li>ä½œè€…ï¼šè°¢å¸†å›½ã€é‡‘ç³ã€å´”å¦™å¦™ã€æå»ºå‡¯ã€æ¨é¸¿å®‡ã€é»„è¿ª</li><li>éš¶å±ï¼šåŒ—äº¬èˆªç©ºèˆªå¤©å¤§å­¦è½¯ä»¶å¼€å‘ç¯å¢ƒå›½å®¶é‡ç‚¹å®éªŒå®¤</li><li>å…³é”®è¯ï¼šæ–‡æœ¬åˆ°å›¾åƒåˆæˆã€æ‰©æ•£æ¨¡å‹ã€åˆå§‹å™ªå£°ä¼˜åŒ–</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.04650   Github ä»£ç é“¾æ¥ï¼šhttps://github.com/xiefan-guo/initno</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ï¼šæ–‡æœ¬åˆ°å›¾åƒåˆæˆï¼ˆT2Iï¼‰æ˜¯ç”Ÿæˆæ¨¡å‹é¢†åŸŸçš„å‰æ²¿ç ”ç©¶ï¼Œè‡´åŠ›äºä»æ–‡æœ¬æç¤ºä¸­ç”ŸæˆçœŸå®ä¸”è§†è§‰ä¸Šè¿è´¯çš„å›¾åƒã€‚åœ¨ç”Ÿæˆæ¨¡å‹é¢†åŸŸï¼ŒåŒ…æ‹¬ç”Ÿæˆå¯¹æŠ—ç½‘ç»œã€å˜åˆ†è‡ªç¼–ç å™¨å’Œè‡ªå›å½’æ¨¡å‹ï¼Œæ‰©æ•£æ¨¡å‹å·²æˆä¸ºä¸€ç§ä¸»è¦çš„è§£å†³æ–¹æ¡ˆã€‚ï¼ˆ2ï¼‰ï¼šå°½ç®¡åœ¨å¤§å‹æ–‡æœ¬å›¾åƒæ•°æ®é›†ä¸Šè®­ç»ƒäº†æœ€å…ˆè¿›çš„ T2I æ‰©æ•£æ¨¡å‹ï¼Œä½†ä¸ç»™å®šæ–‡æœ¬æç¤ºå®Œå…¨å¯¹é½çš„å›¾åƒåˆæˆä»ç„¶æ˜¯ä¸€ä¸ªç›¸å½“å¤§çš„æŒ‘æˆ˜ã€‚ä¼—æ‰€å‘¨çŸ¥çš„é—®é¢˜ï¼Œå³ä¸»é¢˜å¿½ç•¥ã€ä¸»é¢˜æ··åˆå’Œä¸æ­£ç¡®çš„å±æ€§ç»‘å®šï¼Œå¦‚å›¾ 1 æ‰€ç¤ºï¼Œä»ç„¶å­˜åœ¨ã€‚æˆ‘ä»¬å°†è¿™äº›æŒ‘æˆ˜å½’å› äºæ— æ•ˆçš„åˆå§‹å™ªå£°ã€‚å½“å°†ä¸åŒçš„å™ªå£°è¾“å…¥å¼•å…¥å…·æœ‰ç›¸åŒæ–‡æœ¬æç¤ºçš„ T2I æ‰©æ•£æ¨¡å‹æ—¶ï¼Œåœ¨å›¾åƒå’Œæä¾›çš„æ–‡æœ¬ä¹‹é—´è§‚å¯Ÿåˆ°å¯¹é½ä¸Šçš„å®è´¨æ€§å·®å¼‚ï¼Œå¦‚å›¾ 2 æ‰€ç¤ºã€‚è¿™ä¸€è§‚å¯Ÿè¡¨æ˜ï¼Œå¹¶éæ‰€æœ‰éšæœºé‡‡æ ·çš„å™ªå£°éƒ½èƒ½äº§ç”Ÿè§†è§‰ä¸Šä¸€è‡´çš„å›¾åƒã€‚æ ¹æ®ç”Ÿæˆçš„å›¾åƒä¸ç›®æ ‡æ–‡æœ¬ä¹‹é—´çš„ä¸€è‡´æ€§ï¼Œåˆå§‹æ½œåœ¨ç©ºé—´å¯ä»¥åˆ’åˆ†ä¸ºæœ‰æ•ˆåŒºåŸŸå’Œæ— æ•ˆåŒºåŸŸã€‚ä»æœ‰æ•ˆåŒºåŸŸè·å–çš„å™ªå£°è¾“å…¥åˆ° T2I æ‰©æ•£æ¨¡å‹åï¼Œä¼šäº§ç”Ÿè¯­ä¹‰ä¸Šåˆç†çš„å›¾åƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯å°†ä»»ä½•åˆå§‹å™ªå£°å¼•å¯¼åˆ°æœ‰æ•ˆåŒºåŸŸï¼Œä»è€Œä¿ƒè¿›å›¾åƒç”Ÿæˆã€‚ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç§°ä¸ºåˆå§‹å™ªå£°ä¼˜åŒ–ï¼ˆINITNOï¼‰çš„èŒƒä¾‹æ¥è§£å†³æ— æ•ˆåˆå§‹å™ªå£°çš„é—®é¢˜ã€‚INITNO é€šè¿‡è®¾è®¡äº¤å‰æ³¨æ„åŠ›å“åº”åˆ†æ•°å’Œè‡ªæ³¨æ„åŠ›å†²çªåˆ†æ•°æ¥è¯„ä¼°åˆå§‹å™ªå£°ï¼Œå°†åˆå§‹æ½œåœ¨ç©ºé—´åˆ†ä¸ºæœ‰æ•ˆå’Œæ— æ•ˆåŒºåŸŸã€‚å¼€å‘äº†ä¸€ä¸ªç­–ç•¥æ€§è®¾è®¡çš„å™ªå£°ä¼˜åŒ–ç®¡é“ï¼Œä»¥å°†åˆå§‹å™ªå£°å¼•å¯¼åˆ°æœ‰æ•ˆåŒºåŸŸã€‚ï¼ˆ4ï¼‰ï¼šINITNO åœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†å‡ºè‰²çš„æ€§èƒ½ï¼Œåœ¨ä¸æ–‡æœ¬æç¤ºä¸¥æ ¼ä¸€è‡´çš„æƒ…å†µä¸‹ç”Ÿæˆäº†å›¾åƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒINITNO èƒ½å¤Ÿæœ‰æ•ˆåœ°è§£å†³ä¸»é¢˜å¿½ç•¥ã€ä¸»é¢˜æ··åˆå’Œä¸æ­£ç¡®çš„å±æ€§ç»‘å®šç­‰é—®é¢˜ã€‚</li></ol><p><strong>æ–¹æ³•</strong></p><p>(1) <strong>åˆå§‹å™ªå£°è¯„ä¼°ï¼š</strong>   - è®¾è®¡äº¤å‰æ³¨æ„åŠ›å“åº”åˆ†æ•°å’Œè‡ªæ³¨æ„åŠ›å†²çªåˆ†æ•°ï¼Œå°†åˆå§‹æ½œåœ¨ç©ºé—´åˆ’åˆ†ä¸ºæœ‰æ•ˆå’Œæ— æ•ˆåŒºåŸŸã€‚</p><p>(2) <strong>å™ªå£°ä¼˜åŒ–ç®¡é“ï¼š</strong>   - ç­–ç•¥æ€§è®¾è®¡å™ªå£°ä¼˜åŒ–ç®¡é“ï¼Œå°†åˆå§‹å™ªå£°å¼•å¯¼åˆ°æœ‰æ•ˆåŒºåŸŸã€‚</p><p>(3) <strong>ç”¨æˆ·ç ”ç©¶ï¼š</strong>   - ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼ŒINITNO åœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†å‡ºè‰²çš„æ€§èƒ½ï¼Œåœ¨ä¸æ–‡æœ¬æç¤ºä¸¥æ ¼ä¸€è‡´çš„æƒ…å†µä¸‹ç”Ÿæˆäº†å›¾åƒã€‚</p><p>(4) <strong>æ¨ç†æ—¶é—´ï¼š</strong>   - åœ¨å•ä¸ª Tesla V100 (32GB) ä¸Šè¯„ä¼°ï¼ŒINITNO åˆæˆäº† 100 å¼ åˆ†è¾¨ç‡ä¸º 512Ã—512 åƒç´ çš„å›¾åƒï¼Œå¹³å‡ç”¨æ—¶ 18.93 ç§’ã€‚</p><p>(5) <strong>æ¶ˆèç ”ç©¶ï¼š</strong>   - <strong>è‡ªæ³¨æ„åŠ›å†²çªæŸå¤±ï¼š</strong>æœ‰æ•ˆè§£å†³äº†è‡ªæ³¨æ„åŠ›é‡å å¼•èµ·çš„ä¸»é¢˜æ··åˆé—®é¢˜ã€‚   - <strong>åˆ†å¸ƒå¯¹é½æŸå¤±ï¼š</strong>ç¡®ä¿ä¼˜åŒ–åçš„å™ªå£°ç¬¦åˆæ ‡å‡†æ­£æ€åˆ†å¸ƒã€‚</p><p>(6) <strong>åŸºäºæ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆï¼š</strong>   - INITNO æ˜¯ä¸€ç§å³æ’å³ç”¨æ–¹æ³•ï¼Œå¯ä»¥è½»æ¾é›†æˆåˆ°ç°æœ‰æ‰©æ•£æ¨¡å‹ä¸­ï¼Œå®ç°æ— è®­ç»ƒçš„å¯æ§ç”Ÿæˆï¼Œä¾‹å¦‚å¸ƒå±€åˆ°å›¾åƒã€è’™ç‰ˆåˆ°å›¾åƒç”Ÿæˆç­‰ã€‚</p><p><strong>8. ç»“è®º</strong></p><p><strong>(1): æœ¬å·¥ä½œçš„æ„ä¹‰</strong></p><p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºåˆå§‹å™ªå£°ä¼˜åŒ–ï¼ˆINITNOï¼‰çš„èŒƒä¾‹ï¼Œä»¥è§£å†³æ— æ•ˆåˆå§‹å™ªå£°çš„é—®é¢˜ã€‚INITNOé€šè¿‡è®¾è®¡äº¤å‰æ³¨æ„åŠ›å“åº”åˆ†æ•°å’Œè‡ªæ³¨æ„åŠ›å†²çªåˆ†æ•°æ¥è¯„ä¼°åˆå§‹å™ªå£°ï¼Œå°†åˆå§‹æ½œåœ¨ç©ºé—´åˆ’åˆ†ä¸ºæœ‰æ•ˆå’Œæ— æ•ˆåŒºåŸŸã€‚å¼€å‘äº†ä¸€ä¸ªç­–ç•¥æ€§è®¾è®¡çš„å™ªå£°ä¼˜åŒ–ç®¡é“ï¼Œä»¥å°†åˆå§‹å™ªå£°å¼•å¯¼åˆ°æœ‰æ•ˆåŒºåŸŸã€‚INITNOåœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†å‡ºè‰²çš„æ€§èƒ½ï¼Œåœ¨ä¸æ–‡æœ¬æç¤ºä¸¥æ ¼ä¸€è‡´çš„æƒ…å†µä¸‹ç”Ÿæˆäº†å›¾åƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒINITNOèƒ½å¤Ÿæœ‰æ•ˆåœ°è§£å†³ä¸»é¢˜å¿½ç•¥ã€ä¸»é¢˜æ··åˆå’Œä¸æ­£ç¡®çš„å±æ€§ç»‘å®šç­‰é—®é¢˜ã€‚</p><p><strong>(2): æœ¬æ–‡çš„ä¼˜ç¼ºç‚¹æ€»ç»“</strong></p><p><strong>åˆ›æ–°ç‚¹ï¼š</strong></p><ul><li>æå‡ºäº†ä¸€ç§æ–°çš„åˆå§‹å™ªå£°è¯„ä¼°æ–¹æ³•ï¼Œå¯ä»¥å°†åˆå§‹æ½œåœ¨ç©ºé—´åˆ’åˆ†ä¸ºæœ‰æ•ˆå’Œæ— æ•ˆåŒºåŸŸã€‚</li><li>è®¾è®¡äº†ä¸€ä¸ªç­–ç•¥æ€§è®¾è®¡çš„å™ªå£°ä¼˜åŒ–ç®¡é“ï¼Œå°†åˆå§‹å™ªå£°å¼•å¯¼åˆ°æœ‰æ•ˆåŒºåŸŸã€‚</li><li>æå‡ºäº†ä¸€ç§æ–°çš„åˆ†å¸ƒå¯¹é½æŸå¤±ï¼Œä»¥ç¡®ä¿ä¼˜åŒ–åçš„å™ªå£°ç¬¦åˆæ ‡å‡†æ­£æ€åˆ†å¸ƒã€‚</li></ul><p><strong>æ€§èƒ½ï¼š</strong></p><ul><li>INITNOåœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†å‡ºè‰²çš„æ€§èƒ½ï¼Œåœ¨ä¸æ–‡æœ¬æç¤ºä¸¥æ ¼ä¸€è‡´çš„æƒ…å†µä¸‹ç”Ÿæˆäº†å›¾åƒã€‚</li><li>INITNOèƒ½å¤Ÿæœ‰æ•ˆåœ°è§£å†³ä¸»é¢˜å¿½ç•¥ã€ä¸»é¢˜æ··åˆå’Œä¸æ­£ç¡®çš„å±æ€§ç»‘å®šç­‰é—®é¢˜ã€‚</li></ul><p><strong>å·¥ä½œé‡ï¼š</strong></p><ul><li>INITNOæ˜¯ä¸€ç§å³æ’å³ç”¨çš„æ–¹æ³•ï¼Œå¯ä»¥è½»æ¾é›†æˆåˆ°ç°æœ‰æ‰©æ•£æ¨¡å‹ä¸­ï¼Œå®ç°æ— è®­ç»ƒçš„å¯æ§ç”Ÿæˆã€‚</li><li>INITNOçš„æ¨ç†æ—¶é—´ç›¸å¯¹è¾ƒçŸ­ï¼Œåœ¨å•ä¸ªTesla V100 (32GB) ä¸Šè¯„ä¼°ï¼ŒINITNO åˆæˆäº† 100 å¼ åˆ†è¾¨ç‡ä¸º 512Ã—512 åƒç´ çš„å›¾åƒï¼Œå¹³å‡ç”¨æ—¶ 18.93 ç§’ã€‚</li></ul><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-6b8805d41a0f842dfd100f0ec94562de.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f4cf1dd225d50f9419f7438de165c98a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b2a7e6fec8bf9c557df9b7c39d0a37ee.jpg" align="middle"><img src="https://pica.zhimg.com/v2-3db98ef94d50d29cc49f8e9fe6509549.jpg" align="middle"><img src="https://pica.zhimg.com/v2-479a0f109d0d474a6bb3e17b7fcb99fd.jpg" align="middle"></details>## Diffusion Time-step Curriculum for One Image to 3D Generation**Authors:Xuanyu Yi, Zike Wu, Qingshan Xu, Pan Zhou, Joo-Hwee Lim, Hanwang Zhang**Score distillation sampling~(SDS) has been widely adopted to overcome the absence of unseen views in reconstructing 3D objects from a \textbf{single} image. It leverages pre-trained 2D diffusion models as teacher to guide the reconstruction of student 3D models. Despite their remarkable success, SDS-based methods often encounter geometric artifacts and texture saturation. We find out the crux is the overlooked indiscriminate treatment of diffusion time-steps during optimization: it unreasonably treats the student-teacher knowledge distillation to be equal at all time-steps and thus entangles coarse-grained and fine-grained modeling. Therefore, we propose the Diffusion Time-step Curriculum one-image-to-3D pipeline (DTC123), which involves both the teacher and student models collaborating with the time-step curriculum in a coarse-to-fine manner. Extensive experiments on NeRF4, RealFusion15, GSO and Level50 benchmark demonstrate that DTC123 can produce multi-view consistent, high-quality, and diverse 3D assets. Codes and more generation demos will be released in https://github.com/yxymessi/DTC123. [PDF](http://arxiv.org/abs/2404.04562v1) **Summary**é€æ­¥çš„æ‰©æ•£æ—¶é—´è®¾ç½®æŒ‡å¯¼å­¦ç”Ÿæ¨¡å‹ä»å•ä¸€å›¾åƒç”Ÿæˆé«˜è´¨é‡ 3D å¯¹è±¡ã€‚**Key Takeaways**- æœªç»å¤„ç†çš„æ‰©æ•£æ—¶é—´æ­¥é•¿ä¼˜åŒ–å¯¼è‡´å­¦ç”Ÿæ¨¡å‹å‡ ä½•é”™è¯¯å’Œçº¹ç†é¥±å’Œåº¦ã€‚- DTC123 æå‡ºäº†ä¸€ç§ä»ç²—åˆ°ç»†çš„æ—¶é—´æ­¥é•¿è¯¾ç¨‹è¡¨ï¼Œç”¨äºæŒ‡å¯¼å­¦ç”Ÿå’Œæ•™å¸ˆæ¨¡å‹ååŒå·¥ä½œã€‚- DTC123 åœ¨ NeRF4ã€RealFusion15ã€GSO å’Œ Level50 åŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç”Ÿæˆå¤šè§†å›¾ä¸€è‡´ã€é«˜è´¨é‡å’Œå¤šæ ·çš„ 3D èµ„äº§ã€‚- DTC123 æ–¹æ³•å…‹æœäº†ä»å•ä¸€å›¾åƒé‡å»º 3D å¯¹è±¡æ—¶ç¼ºä¹æœªè§è§†å›¾çš„æŒ‘æˆ˜ã€‚- æ•™å¸ˆæ¨¡å‹åœ¨ç²—ç²’åº¦å»ºæ¨¡ä¸­æä¾›æŒ‡å¯¼ï¼Œè€Œå­¦ç”Ÿæ¨¡å‹åœ¨ç»†ç²’åº¦ç»†èŠ‚ä¸­è¿›è¡Œå¾®è°ƒã€‚- æ—¶é—´æ­¥é•¿è¯¾ç¨‹è¡¨å¯ç¡®ä¿åœ¨ä¸åŒé˜¶æ®µé‡ç‚¹å…³æ³¨ä¸åŒç²’åº¦çš„ç‰¹å¾ã€‚- ä»£ç å’Œæ›´å¤šç”Ÿæˆæ¼”ç¤ºå°†äº https://github.com/yxymessi/DTC123 å‘å¸ƒã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>æ ‡é¢˜ï¼šæ‰©æ•£æ—¶é—´æ­¥è¯¾ç¨‹è¡¨ï¼šå•å›¾åƒåˆ° 3D çš„æ–°ç®¡é“</li><li>ä½œè€…ï¼šYuxiao Yao, Yifan Jiang, Yuxin Wen, Jingyu Yang, Zhe Lin, Chen Change Loy, Ziwei Liu</li><li>éš¶å±ï¼šé¦™æ¸¯ä¸­æ–‡å¤§å­¦ï¼ˆæ·±åœ³ï¼‰</li><li>å…³é”®è¯ï¼š3D é‡å»ºï¼Œå›¾åƒåˆ° 3Dï¼Œæ‰©æ•£æ¨¡å‹ï¼ŒçŸ¥è¯†è’¸é¦ï¼Œæ—¶é—´æ­¥è¯¾ç¨‹è¡¨</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.12910ï¼ŒGithub ä»£ç é“¾æ¥ï¼šhttps://github.com/yxymessi/DTC123</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šå•å›¾åƒ 3D é‡å»ºæ–¹æ³•åœ¨è¿‡å»å‡ å¹´ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»ç„¶å­˜åœ¨å‡ ä½•ä¼ªå½±å’Œçº¹ç†é¥±å’Œç­‰é—®é¢˜ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šåŸºäº SDS çš„æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„ 2D æ‰©æ•£æ¨¡å‹ä½œä¸ºæ•™å¸ˆæ¥æŒ‡å¯¼å­¦ç”Ÿ 3D æ¨¡å‹çš„é‡å»ºï¼Œä½†å®ƒä»¬å¿½ç•¥äº†æ‰©æ•£æ—¶é—´æ­¥æœŸé—´çš„çŸ¥è¯†è’¸é¦å¤„ç†ï¼Œå¯¼è‡´ç²—ç²’åº¦å’Œç»†ç²’åº¦å»ºæ¨¡çº ç¼ åœ¨ä¸€èµ·ã€‚ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†æ‰©æ•£æ—¶é—´æ­¥è¯¾ç¨‹è¡¨å•å›¾åƒåˆ° 3D ç®¡é“ï¼ˆDTC123ï¼‰ï¼Œè¯¥ç®¡é“ä»¥ç²—åˆ°ç»†çš„æ–¹å¼æ¶‰åŠæ•™å¸ˆå’Œå­¦ç”Ÿæ¨¡å‹ä¸æ—¶é—´æ­¥è¯¾ç¨‹è¡¨çš„åä½œã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šåœ¨ NeRF4ã€RealFusion15ã€GSO å’Œ Level50 åŸºå‡†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDTC123 å¯ä»¥ç”Ÿæˆå¤šè§†å›¾ä¸€è‡´ã€é«˜è´¨é‡å’Œå¤šæ ·åŒ–çš„ 3D èµ„äº§ï¼Œè¿™æ”¯æŒäº†ä»–ä»¬çš„ç›®æ ‡ã€‚</li></ol><p>Some Error for method(æ¯”å¦‚æ˜¯ä¸æ˜¯æ²¡æœ‰Methodsè¿™ä¸ªç« èŠ‚)</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†æ‰©æ•£æ—¶é—´æ­¥è¯¾ç¨‹è¡¨ï¼Œé€šè¿‡ç²—åˆ°ç»†çš„æ–¹å¼è®©æ•™å¸ˆå’Œå­¦ç”Ÿæ¨¡å‹ä¸æ—¶é—´æ­¥è¯¾ç¨‹è¡¨åä½œï¼Œæ˜¾è‘—æé«˜äº†å›¾åƒåˆ° 3D ç”Ÿæˆä¸­çš„çœŸå®æ„Ÿå’Œå¤šè§†å›¾ä¸€è‡´æ€§ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šDiffusion Time-step Curriculumï¼›æ€§èƒ½ï¼šåœ¨ NeRF4ã€RealFusion15ã€GSO å’Œ Level50 åŸºå‡†ä¸Šè¡¨ç°å‡ºè‰²ï¼›å·¥ä½œé‡ï¼šä¸­ç­‰ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-744c7f5a081447863699bed80f656a2a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-dd5d14fea14d35db1bbda6adb0c315a9.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-551a47f8383d1a4797b18d85cec41fb3.jpg" align="middle"><img src="https://pica.zhimg.com/v2-4b111e6fcddc0b871d26d7799de87b88.jpg" align="middle"></details><h2 id="BeyondScene-Higher-Resolution-Human-Centric-Scene-Generation-With-Pretrained-Diffusion"><a href="#BeyondScene-Higher-Resolution-Human-Centric-Scene-Generation-With-Pretrained-Diffusion" class="headerlink" title="BeyondScene: Higher-Resolution Human-Centric Scene Generation With   Pretrained Diffusion"></a>BeyondScene: Higher-Resolution Human-Centric Scene Generation With   Pretrained Diffusion</h2><p><strong>Authors:Gwanghyun Kim, Hayeon Kim, Hoigi Seo, Dong Un Kang, Se Young Chun</strong></p><p>Generating higher-resolution human-centric scenes with details and controls remains a challenge for existing text-to-image diffusion models. This challenge stems from limited training image size, text encoder capacity (limited tokens), and the inherent difficulty of generating complex scenes involving multiple humans. While current methods attempted to address training size limit only, they often yielded human-centric scenes with severe artifacts. We propose BeyondScene, a novel framework that overcomes prior limitations, generating exquisite higher-resolution (over 8K) human-centric scenes with exceptional text-image correspondence and naturalness using existing pretrained diffusion models. BeyondScene employs a staged and hierarchical approach to initially generate a detailed base image focusing on crucial elements in instance creation for multiple humans and detailed descriptions beyond token limit of diffusion model, and then to seamlessly convert the base image to a higher-resolution output, exceeding training image size and incorporating details aware of text and instances via our novel instance-aware hierarchical enlargement process that consists of our proposed high-frequency injected forward diffusion and adaptive joint diffusion. BeyondScene surpasses existing methods in terms of correspondence with detailed text descriptions and naturalness, paving the way for advanced applications in higher-resolution human-centric scene creation beyond the capacity of pretrained diffusion models without costly retraining. Project page: <a href="https://janeyeon.github.io/beyond-scene">https://janeyeon.github.io/beyond-scene</a>. </p><p><a href="http://arxiv.org/abs/2404.04544v1">PDF</a> Project page: <a href="https://janeyeon.github.io/beyond-scene">https://janeyeon.github.io/beyond-scene</a></p><p><strong>Summary</strong><br>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜åˆ†è¾¨ç‡ã€åŒ…å«äººç±»å…ƒç´ ä¸”å¯Œæœ‰ç»†èŠ‚å’Œå¯æ§çš„åœºæ™¯æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡º BeyondScene æ¡†æ¶æ¥è§£å†³è¿™ä¸€éš¾é¢˜ï¼Œä½¿ç”¨ç°æˆçš„é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆåˆ†è¾¨ç‡è¶…è¿‡ 8K çš„äººåƒä¸­å¿ƒåœºæ™¯ï¼Œå¹¶å…·æœ‰å‡ºè‰²çš„æ–‡æœ¬å›¾åƒå¯¹åº”å’Œè‡ªç„¶åº¦ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>BeyondScene é‡‡ç”¨åˆ†é˜¶æ®µã€åˆ†å±‚çš„æ–¹æ³•ï¼Œå…ˆç”Ÿæˆä¸€ä¸ªå…³æ³¨å…³é”®å…ƒç´ çš„è¯¦ç»†åŸºç¡€å›¾åƒï¼Œç„¶åå°†å…¶è½¬æ¢ä¸ºé«˜åˆ†è¾¨ç‡è¾“å‡ºã€‚</li><li>é«˜é¢‘æ³¨å…¥å‰å‘æ‰©æ•£å’Œè‡ªé€‚åº”è”åˆæ‰©æ•£èƒ½å¤Ÿæ„ŸçŸ¥æ–‡æœ¬å’Œå®ä¾‹çš„ç»†èŠ‚ï¼Œç”Ÿæˆè‡ªç„¶çš„äººåƒä¸­å¿ƒåœºæ™¯ã€‚</li><li>BeyondScene åœ¨æ–‡æœ¬æè¿°å¯¹åº”å’Œè‡ªç„¶åº¦æ–¹é¢è¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œä¸ºåœ¨ç°æœ‰é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹èƒ½åŠ›ä¹‹å¤–åˆ›å»ºé«˜åˆ†è¾¨ç‡äººåƒä¸­å¿ƒåœºæ™¯çš„é«˜çº§åº”ç”¨é“ºå¹³äº†é“è·¯ã€‚</li><li>BeyondSceneæ— éœ€è¿›è¡Œä»£ä»·é«˜æ˜‚çš„é‡æ–°è®­ç»ƒï¼Œå³å¯ä½¿ç”¨ç°æˆçš„é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆé«˜åˆ†è¾¨ç‡ã€åŒ…å«äººç±»å…ƒç´ ä¸”å¯Œæœ‰ç»†èŠ‚å’Œå¯æ§çš„åœºæ™¯ã€‚</li><li>BeyondScene é€šè¿‡<a href="https://janeyeon.github.io/beyond-sceneæä¾›é¡¹ç›®ä¸»é¡µã€‚">https://janeyeon.github.io/beyond-sceneæä¾›é¡¹ç›®ä¸»é¡µã€‚</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šè¶…è¶Šåœºæ™¯ï¼šæ›´é«˜åˆ†è¾¨ç‡çš„äººä½“ä¸­å¿ƒè¡¥å……ææ–™</li><li>ä½œè€…ï¼šJane Yeonã€Minseop Parkã€Seunghoon Hong</li><li>æ‰€å±æœºæ„ï¼šé¦–å°”å¤§å­¦</li><li>å…³é”®è¯ï¼šä»¥äººä¸ºä¸­å¿ƒçš„åœºæ™¯ç”Ÿæˆã€æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ã€é«˜åˆ†è¾¨ç‡</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.08182ï¼ŒGithub ä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šç°æœ‰æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜åˆ†è¾¨ç‡ã€ä»¥äººä¸ºä¸­å¿ƒä¸”ç»†èŠ‚ä¸°å¯Œã€å¯æ§çš„åœºæ™¯æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼ŒåŸå› åœ¨äºè®­ç»ƒå›¾åƒå°ºå¯¸ã€æ–‡æœ¬ç¼–ç å™¨å®¹é‡ï¼ˆä»¤ç‰Œæ•°é‡æœ‰é™ï¼‰å’Œç”Ÿæˆæ¶‰åŠå¤šä¸ªäººç‰©çš„å¤æ‚åœºæ™¯çš„å›ºæœ‰éš¾åº¦ã€‚ï¼ˆ2ï¼‰ï¼šè¿‡å»çš„æ–¹æ³•å’Œé—®é¢˜ï¼šå½“å‰æ–¹æ³•ä»…å°è¯•è§£å†³è®­ç»ƒå°ºå¯¸é™åˆ¶ï¼Œä½†é€šå¸¸ä¼šäº§ç”Ÿå¸¦æœ‰ä¸¥é‡ä¼ªå½±çš„äººä½“ä¸­å¿ƒåœºæ™¯ã€‚è¯¥æ–¹æ³•çš„åŠ¨æœºå¾ˆå¥½ï¼Œå› ä¸ºå®ƒå…‹æœäº†å…ˆå‰çš„é™åˆ¶ï¼Œä½¿ç”¨ç°æœ‰çš„é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆäº†ç²¾ç¾çš„æ›´é«˜åˆ†è¾¨ç‡ï¼ˆè¶…è¿‡ 8Kï¼‰çš„äººä½“ä¸­å¿ƒåœºæ™¯ï¼Œå…·æœ‰å‡ºè‰²çš„æ–‡æœ¬å›¾åƒå¯¹åº”å…³ç³»å’Œè‡ªç„¶æ€§ã€‚ï¼ˆ3ï¼‰ï¼šæå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šBeyondScene é‡‡ç”¨åˆ†é˜¶æ®µä¸”åˆ†å±‚çš„æ–¹æ³•ï¼Œé¦–å…ˆç”Ÿæˆä¸€ä¸ªè¯¦ç»†çš„åŸºæœ¬å›¾åƒï¼Œé‡ç‚¹å…³æ³¨å¤šä¸ªäººçš„å®ä¾‹åˆ›å»ºä¸­çš„å…³é”®å…ƒç´ å’Œæ‰©æ•£æ¨¡å‹ä»¤ç‰Œé™åˆ¶ä¹‹å¤–çš„è¯¦ç»†æè¿°ï¼Œç„¶åå°†åŸºæœ¬å›¾åƒæ— ç¼è½¬æ¢ä¸ºæ›´é«˜åˆ†è¾¨ç‡çš„è¾“å‡ºï¼Œè¶…è¿‡è®­ç»ƒå›¾åƒå°ºå¯¸å¹¶é€šè¿‡æˆ‘ä»¬æ–°é¢–çš„å®ä¾‹æ„ŸçŸ¥åˆ†å±‚æ”¾å¤§è¿‡ç¨‹çº³å…¥æ–‡æœ¬å’Œå®ä¾‹æ„ŸçŸ¥çš„ç»†èŠ‚ï¼Œè¯¥è¿‡ç¨‹åŒ…æ‹¬æˆ‘ä»¬æå‡ºçš„é«˜é¢‘æ³¨å…¥æ­£å‘æ‰©æ•£å’Œè‡ªé€‚åº”è”åˆæ‰©æ•£ã€‚ï¼ˆ4ï¼‰ï¼šæ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†ä»€ä¹ˆæ€§èƒ½ï¼šBeyondScene åœ¨ä¸è¯¦ç»†æ–‡æœ¬æè¿°çš„å¯¹åº”å…³ç³»å’Œè‡ªç„¶æ€§æ–¹é¢è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œä¸ºåœ¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹å®¹é‡ä¹‹å¤–åˆ›å»ºæ›´é«˜åˆ†è¾¨ç‡çš„äººä½“ä¸­å¿ƒåœºæ™¯çš„é«˜çº§åº”ç”¨é“ºå¹³äº†é“è·¯ï¼Œè€Œæ— éœ€è¿›è¡Œæ˜‚è´µçš„é‡æ–°è®­ç»ƒã€‚</li></ol><p>7.æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼šè¯¦ç»†åŸºæœ¬å›¾åƒç”Ÿæˆï¼šåˆ©ç”¨SDXL-ControlNet-Openposeç›´æ¥ç”ŸæˆåŸºäºæ–‡æœ¬æè¿°å’Œå§¿æ€ä¿¡æ¯çš„å®ä¾‹ï¼Œé‡‡ç”¨Lang-SegmentAnythingè¿›è¡Œç²¾ç¡®çš„äººä½“åˆ†å‰²ï¼Œä½¿ç”¨ç›¸åŒçš„æ¨¡å‹å°†å¤´éƒ¨åŒºåŸŸåˆ†å‰²æˆâ€œå¤´éƒ¨â€å’Œâ€œå¤´å‘â€ï¼Œå†ç»„åˆå½¢æˆå¤´éƒ¨åˆ†å‰²ï¼Œç„¶åå¯¹èº«ä½“éƒ¨ä½è¿›è¡Œåˆ†å‰²ï¼ŒåŒ…æ‹¬é™¤å¤´éƒ¨åˆ†å‰²ä»¥å¤–çš„æ•´ä¸ªäººä½“ï¼Œéšåä½¿ç”¨åœ¨å…¨èº«å§¿æ€æ•°æ®é›†ä¸Šè®­ç»ƒçš„ä¸¤ä¸ªæ¨¡å‹ï¼ˆViTPoseå’ŒYOLOv8æ£€æµ‹å™¨ï¼‰é‡æ–°ä¼°è®¡ç”Ÿæˆå›¾åƒä¸­çš„äººä½“å§¿æ€ï¼Œæœ€åï¼Œä¸ºäº†å°†å‰æ™¯å…ƒç´ ä¸èƒŒæ™¯æ— ç¼é›†æˆï¼Œé¦–å…ˆè°ƒæ•´å¤§å°å¹¶åˆ›å»ºä¸€ä¸ªåŸºæœ¬æ‹¼è´´ï¼Œç„¶åä½¿ç”¨SDXL-inpaintingå°†ç”Ÿæˆçš„å‰æ™¯å…ƒç´ ç»˜åˆ¶åˆ°èƒŒæ™¯ä¸Šï¼Œä¸ºäº†å¤„ç†ä»»æ„å¤§å°çš„èƒŒæ™¯ï¼Œä½¿ç”¨SDXLinpaintingå®ç°è”åˆæ‰©æ•£ï¼›ï¼ˆ2ï¼‰ï¼šå®ä¾‹æ„ŸçŸ¥åˆ†å±‚æ”¾å¤§ï¼šé«˜é¢‘æ³¨å…¥æ­£å‘æ‰©æ•£ï¼šä½¿ç”¨é˜ˆå€¼åˆ†åˆ«ä¸º100å’Œ200çš„Cannyè¾¹ç¼˜æ£€æµ‹ç®—æ³•ï¼Œä½¿ç”¨æ ‡å‡†å·®Ïƒä¸º50çš„é«˜æ–¯æ ¸å¹³æ»‘è¾¹ç¼˜å›¾ï¼Œé€šè¿‡å¯¹æ¨¡ç³Šè¾¹ç¼˜å›¾è¿›è¡Œå½’ä¸€åŒ–å’Œæ¡ä»¶åŒ–æ¥æ„å»ºæ¦‚ç‡å›¾Cï¼Œå®šä¹‰é«˜æ¦‚ç‡é˜ˆå€¼pmaxä¸º0.1ï¼Œä½æ¦‚ç‡é˜ˆå€¼pbaseä¸º0.005ï¼Œä½¿ç”¨Lanczosæ’å€¼è¿›è¡Œå›¾åƒä¸Šé‡‡æ ·ï¼ŒdrandÎ±interpisåˆ†åˆ«è®¾ç½®ä¸º4å’Œ2ï¼Œç”¨äºåŸºäºæ¦‚ç‡å›¾çš„åƒç´ æ‰°åŠ¨ï¼Œæœ€åï¼Œæ­£å‘æ‰©æ•£æ—¶é—´æ­¥Tbisè®¾ç½®ä¸º700ï¼Œæ˜¯SDXLæ¡†æ¶ä¸­ä½¿ç”¨çš„æ€»è®­ç»ƒæ­¥æ•°1000çš„0.7å€ï¼›è‡ªé€‚åº”è”åˆå¤„ç†ï¼šå¯¹äºè‡ªé€‚åº”è”åˆå¤„ç†ï¼Œæ¥æ”¶ç”Ÿæˆçš„å§¿æ€å›¾å’Œé«˜é¢‘æ³¨å…¥å™ªå£°æ½œå˜é‡ä½œä¸ºè¾“å…¥ï¼Œä½¿ç”¨SDXLControlNet-Openposeï¼Œå½“ä½¿ç”¨è‡ªé€‚åº”æ­¥å¹…æ—¶ï¼ŒÎ²overè®¾ç½®ä¸º0.2ï¼ŒèƒŒæ™¯æ­¥å¹…backè®¾ç½®ä¸º64ï¼Œsinstè®¾ç½®ä¸º32ï¼Œå½“ä¸ä½¿ç”¨è‡ªé€‚åº”æ­¥å¹…æ—¶ï¼Œbackå’Œsinstéƒ½è®¾ç½®ä¸º32ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šBeyondScene åœ¨ç”Ÿæˆé«˜åˆ†è¾¨ç‡ã€ä»¥äººä¸ºä¸­å¿ƒä¸”ç»†èŠ‚ä¸°å¯Œã€å¯æ§çš„åœºæ™¯æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ï¼Œè§£å†³äº†ç°æœ‰æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„å±€é™æ€§ï¼Œä¸ºåœ¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹å®¹é‡ä¹‹å¤–åˆ›å»ºæ›´é«˜åˆ†è¾¨ç‡çš„äººä½“ä¸­å¿ƒåœºæ™¯çš„é«˜çº§åº”ç”¨é“ºå¹³äº†é“è·¯ï¼Œè€Œæ— éœ€è¿›è¡Œæ˜‚è´µçš„é‡æ–°è®­ç»ƒã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§åˆ†é˜¶æ®µä¸”åˆ†å±‚çš„æ–¹æ³•ï¼Œé¦–å…ˆç”Ÿæˆä¸€ä¸ªè¯¦ç»†çš„åŸºæœ¬å›¾åƒï¼Œé‡ç‚¹å…³æ³¨å¤šä¸ªäººçš„å®ä¾‹åˆ›å»ºä¸­çš„å…³é”®å…ƒç´ å’Œæ‰©æ•£æ¨¡å‹ä»¤ç‰Œé™åˆ¶ä¹‹å¤–çš„è¯¦ç»†æè¿°ï¼Œç„¶åå°†åŸºæœ¬å›¾åƒæ— ç¼è½¬æ¢ä¸ºæ›´é«˜åˆ†è¾¨ç‡çš„è¾“å‡ºï¼Œè¶…è¿‡è®­ç»ƒå›¾åƒå°ºå¯¸å¹¶é€šè¿‡æˆ‘ä»¬æ–°é¢–çš„å®ä¾‹æ„ŸçŸ¥åˆ†å±‚æ”¾å¤§è¿‡ç¨‹çº³å…¥æ–‡æœ¬å’Œå®ä¾‹æ„ŸçŸ¥çš„ç»†èŠ‚ã€‚</li><li>æå‡ºäº†ä¸€ç§é«˜é¢‘æ³¨å…¥æ­£å‘æ‰©æ•£å’Œè‡ªé€‚åº”è”åˆæ‰©æ•£ï¼Œç”¨äºå®ä¾‹æ„ŸçŸ¥åˆ†å±‚æ”¾å¤§ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å°†ä½åˆ†è¾¨ç‡åŸºæœ¬å›¾åƒæ”¾å¤§åˆ°æ›´é«˜åˆ†è¾¨ç‡ï¼ŒåŒæ—¶ä¿ç•™ç»†èŠ‚å’Œè‡ªç„¶æ€§ã€‚æ€§èƒ½ï¼š</li><li>BeyondScene åœ¨ä¸è¯¦ç»†æ–‡æœ¬æè¿°çš„å¯¹åº”å…³ç³»å’Œè‡ªç„¶æ€§æ–¹é¢è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œåœ¨å„ç§æ•°æ®é›†ä¸Šéƒ½å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å·¥ä½œé‡ï¼š</li><li>BeyondScene çš„å®ç°ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦ä½¿ç”¨å¤šä¸ªé¢„è®­ç»ƒæ¨¡å‹å’Œè‡ªå®šä¹‰è®­ç»ƒè¿‡ç¨‹ï¼Œè¿™å¯èƒ½ä¼šå¢åŠ å·¥ä½œé‡ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-35e73818c7206d5bf11663e3f3a1cf8b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6df01273262f94209f883ec74bc32383.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c6320840444a6fbd77fadf0ed87c258f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a022759deb873c8a9f622ecd7392aeeb.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-04-09  MoMA Multimodal LLM Adapter for Fast Personalized Image Generation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/NeRF/"/>
    <id>https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/NeRF/</id>
    <published>2024-04-06T10:47:58.000Z</published>
    <updated>2024-04-06T10:47:58.786Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-04-06-æ›´æ–°"><a href="#2024-04-06-æ›´æ–°" class="headerlink" title="2024-04-06 æ›´æ–°"></a>2024-04-06 æ›´æ–°</h1><h2 id="RaFE-Generative-Radiance-Fields-Restoration"><a href="#RaFE-Generative-Radiance-Fields-Restoration" class="headerlink" title="RaFE: Generative Radiance Fields Restoration"></a>RaFE: Generative Radiance Fields Restoration</h2><p><strong>Authors:Zhongkai Wu, Ziyu Wan, Jing Zhang, Jing Liao, Dong Xu</strong></p><p>NeRF (Neural Radiance Fields) has demonstrated tremendous potential in novel view synthesis and 3D reconstruction, but its performance is sensitive to input image quality, which struggles to achieve high-fidelity rendering when provided with low-quality sparse input viewpoints. Previous methods for NeRF restoration are tailored for specific degradation type, ignoring the generality of restoration. To overcome this limitation, we propose a generic radiance fields restoration pipeline, named RaFE, which applies to various types of degradations, such as low resolution, blurriness, noise, compression artifacts, or their combinations. Our approach leverages the success of off-the-shelf 2D restoration methods to recover the multi-view images individually. Instead of reconstructing a blurred NeRF by averaging inconsistencies, we introduce a novel approach using Generative Adversarial Networks (GANs) for NeRF generation to better accommodate the geometric and appearance inconsistencies present in the multi-view images. Specifically, we adopt a two-level tri-plane architecture, where the coarse level remains fixed to represent the low-quality NeRF, and a fine-level residual tri-plane to be added to the coarse level is modeled as a distribution with GAN to capture potential variations in restoration. We validate RaFE on both synthetic and real cases for various restoration tasks, demonstrating superior performance in both quantitative and qualitative evaluations, surpassing other 3D restoration methods specific to single task. Please see our project website <a href="https://zkaiwu.github.io/RaFE-Project/">https://zkaiwu.github.io/RaFE-Project/</a>. </p><p><a href="http://arxiv.org/abs/2404.03654v1">PDF</a> Project Page: <a href="https://zkaiwu.github.io/RaFE-Project/">https://zkaiwu.github.io/RaFE-Project/</a></p><p><strong>Summary</strong><br>RaFE æ˜¯ä¸€ç§é€šç”¨å…‰åœºä¿®å¤ç®¡é“ï¼Œå¯ä»¥ä¿®å¤å„ç§ç±»å‹çš„å›¾åƒé€€åŒ–ï¼Œä»è€Œæé«˜ NeRF çš„æ€§èƒ½ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>RaFE é€‚ç”¨äºå„ç§ç±»å‹çš„å›¾åƒé€€åŒ–ï¼ŒåŒ…æ‹¬ä½åˆ†è¾¨ç‡ã€æ¨¡ç³Šã€å™ªå£°å’Œå‹ç¼©ä¼ªå½±ã€‚</li><li>RaFE ä½¿ç”¨ç°æˆçš„ 2D ä¿®å¤æ–¹æ³•å•ç‹¬æ¢å¤å¤šè§†å›¾å›¾åƒã€‚</li><li>RaFE ä½¿ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ (GAN) æ¥ç”Ÿæˆ NeRFï¼Œä»¥æ›´å¥½åœ°é€‚åº”å¤šè§†å›¾å›¾åƒä¸­å­˜åœ¨çš„å‡ ä½•å’Œå¤–è§‚ä¸ä¸€è‡´æ€§ã€‚</li><li>RaFE é‡‡ç”¨äº†ä¸¤çº§ä¸‰å¹³é¢æ¶æ„ï¼Œå…¶ä¸­ç²—ç³™çº§åˆ«ä¿æŒå›ºå®šä»¥è¡¨ç¤ºä½è´¨é‡çš„ NeRFï¼Œå¹¶ä¸”å°†æ·»åŠ åˆ°ç²—ç³™çº§åˆ«çš„ç²¾ç»†çº§åˆ«æ®‹å·®ä¸‰å¹³é¢å»ºæ¨¡ä¸ºå…·æœ‰ GAN çš„åˆ†å¸ƒä»¥æ•è·ä¿®å¤ä¸­çš„æ½œåœ¨å˜åŒ–ã€‚</li><li>RaFE åœ¨åˆæˆå’ŒçœŸå®æ¡ˆä¾‹ä¸­é’ˆå¯¹å„ç§ä¿®å¤ä»»åŠ¡è¿›è¡Œäº†éªŒè¯ï¼Œåœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸­éƒ½è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†é’ˆå¯¹å•ä¸ªä»»åŠ¡çš„å…¶ä»– 3D ä¿®å¤æ–¹æ³•ã€‚</li><li>RaFE çš„é¡¹ç›®ç½‘ç«™ï¼š<a href="https://zkaiwu.github.io/RaFE-Project/ã€‚">https://zkaiwu.github.io/RaFE-Project/ã€‚</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šRaFEï¼šç”Ÿæˆè¾å°„åœºä¿®å¤è¡¥å……ææ–™</li><li>ä½œè€…ï¼šZhongkai Wu, Ziyu Wan, Jing Zhang, Jing Liao, Dong Xu</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šåŒ—äº¬èˆªç©ºèˆªå¤©å¤§å­¦è½¯ä»¶å­¦é™¢</li><li>å…³é”®è¯ï¼šç¥ç»æ¸²æŸ“Â·ç”Ÿæˆæ¨¡å‹Â·3Dä¿®å¤Â·ç¥ç»è¾å°„åœº</li><li>è®ºæ–‡é“¾æ¥ï¼šarxiv.org/abs/2404.03654   Github ä»£ç é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼š   (1): ç ”ç©¶èƒŒæ™¯ï¼šNeRFï¼ˆç¥ç»è¾å°„åœºï¼‰åœ¨ novel view synthesis å’Œ 3D é‡å»ºä¸­è¡¨ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ï¼Œä½†å…¶æ€§èƒ½å¯¹è¾“å…¥å›¾åƒè´¨é‡å¾ˆæ•æ„Ÿï¼Œå½“æä¾›ä½è´¨é‡ç¨€ç–è¾“å…¥è§†ç‚¹æ—¶å¾ˆéš¾å®ç°é«˜ä¿çœŸæ¸²æŸ“ã€‚é’ˆå¯¹ NeRF ä¿®å¤çš„ç°æœ‰æ–¹æ³•é’ˆå¯¹ç‰¹å®šçš„é€€åŒ–ç±»å‹è¿›è¡Œå®šåˆ¶ï¼Œå¿½ç•¥äº†ä¿®å¤çš„é€šç”¨æ€§ã€‚   (2): è¿‡å»çš„æ–¹æ³•ï¼šé’ˆå¯¹ç‰¹å®šé€€åŒ–ç±»å‹è¿›è¡Œå®šåˆ¶ï¼Œå¿½ç•¥äº†ä¿®å¤çš„é€šç”¨æ€§ã€‚   (3): æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæå‡ºäº†ä¸€ç§é€šç”¨çš„è¾å°„åœºä¿®å¤ç®¡é“ RaFEï¼Œé€‚ç”¨äºå„ç§ç±»å‹çš„é€€åŒ–ï¼Œå¦‚ä½åˆ†è¾¨ç‡ã€æ¨¡ç³Šã€å™ªå£°ã€å‹ç¼©ä¼ªå½±æˆ–å®ƒä»¬çš„ç»„åˆã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨ç°æˆçš„ 2D ä¿®å¤æ–¹æ³•åˆ†åˆ«æ¢å¤å¤šè§†å›¾å›¾åƒã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œä½¿ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ (GAN) è¿›è¡Œ NeRF ç”Ÿæˆï¼Œä»¥æ›´å¥½åœ°é€‚åº”å¤šè§†å›¾å›¾åƒä¸­å­˜åœ¨çš„å‡ ä½•å’Œå¤–è§‚ä¸ä¸€è‡´ï¼Œè€Œä¸æ˜¯é€šè¿‡å¹³å‡ä¸ä¸€è‡´æ€§æ¥é‡å»ºæ¨¡ç³Šçš„ NeRFã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸¤çº§ä¸‰å¹³é¢æ¶æ„ï¼Œå…¶ä¸­ç²—ç³™çº§åˆ«ä¿æŒå›ºå®šä»¥è¡¨ç¤ºä½è´¨é‡çš„ NeRFï¼Œå¹¶ä¸”å°†ç»†çº§åˆ«æ®‹å·®ä¸‰å¹³é¢æ·»åŠ åˆ°ç²—ç³™çº§åˆ«å¹¶å»ºæ¨¡ä¸ºå…·æœ‰ GAN çš„åˆ†å¸ƒä»¥æ•è·ä¿®å¤ä¸­çš„æ½œåœ¨å˜åŒ–ã€‚   (4): æ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†ä»€ä¹ˆæ€§èƒ½ï¼šæˆ‘ä»¬åœ¨åˆæˆå’ŒçœŸå®æ¡ˆä¾‹ä¸­å¯¹ RaFE è¿›è¡Œäº†å„ç§ä¿®å¤ä»»åŠ¡çš„éªŒè¯ï¼Œè¯æ˜äº†å…¶åœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸­éƒ½å…·æœ‰ä¼˜å¼‚çš„æ€§èƒ½ï¼Œè¶…è¿‡äº†å…¶ä»–é’ˆå¯¹å•ä¸€ä»»åŠ¡çš„ 3D ä¿®å¤æ–¹æ³•ã€‚æ€§èƒ½æ”¯æŒå…¶ç›®æ ‡ã€‚</p></li><li><p><strong>æ–¹æ³•</strong>ï¼šï¼ˆ1ï¼‰æå‡ºRaFEç®¡é“ï¼Œåˆ©ç”¨ç°æˆ2Dä¿®å¤æ–¹æ³•æ¢å¤å¤šè§†å›¾å›¾åƒï¼Œå¹¶ä½¿ç”¨GANè¿›è¡ŒNeRFç”Ÿæˆä»¥é€‚åº”å‡ ä½•å’Œå¤–è§‚ä¸ä¸€è‡´ï¼›ï¼ˆ2ï¼‰é‡‡ç”¨ä¸¤çº§ä¸‰å¹³é¢æ¶æ„ï¼Œç²—ç³™çº§åˆ«è¡¨ç¤ºä½è´¨é‡NeRFï¼Œç»†çº§åˆ«æ®‹å·®ä¸‰å¹³é¢å»ºæ¨¡ä¸ºå…·æœ‰GANçš„åˆ†å¸ƒï¼Œæ•è·ä¿®å¤ä¸­çš„æ½œåœ¨å˜åŒ–ã€‚</p></li><li><p>ç»“è®ºï¼š(1): æœ¬æ–‡æå‡ºäº† RaFEï¼Œä¸€ç§é€šç”¨çš„è¾å°„åœºä¿®å¤ç®¡é“ï¼Œé€‚ç”¨äºå„ç§ç±»å‹çš„é€€åŒ–ï¼Œåœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸­éƒ½å…·æœ‰ä¼˜å¼‚çš„æ€§èƒ½ã€‚(2): åˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§é€šç”¨çš„è¾å°„åœºä¿®å¤ç®¡é“ï¼Œé€‚ç”¨äºå„ç§ç±»å‹çš„é€€åŒ–ã€‚</li><li>ä½¿ç”¨ GAN è¿›è¡Œ NeRF ç”Ÿæˆä»¥é€‚åº”å¤šè§†å›¾å›¾åƒä¸­å­˜åœ¨çš„å‡ ä½•å’Œå¤–è§‚ä¸ä¸€è‡´ã€‚</li><li>é‡‡ç”¨äº†ä¸¤çº§ä¸‰å¹³é¢æ¶æ„ï¼Œä»¥æ•è·ä¿®å¤ä¸­çš„æ½œåœ¨å˜åŒ–ã€‚æ€§èƒ½ï¼š</li><li>åœ¨åˆæˆå’ŒçœŸå®æ¡ˆä¾‹ä¸­å¯¹ RaFE è¿›è¡Œäº†å„ç§ä¿®å¤ä»»åŠ¡çš„éªŒè¯ï¼Œè¯æ˜äº†å…¶åœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸­éƒ½å…·æœ‰ä¼˜å¼‚çš„æ€§èƒ½ã€‚</li><li>è¶…è¿‡äº†å…¶ä»–é’ˆå¯¹å•ä¸€ä»»åŠ¡çš„ 3D ä¿®å¤æ–¹æ³•ã€‚å·¥ä½œé‡ï¼š</li><li>è®ºæ–‡æ¸…æ™°ç®€æ´ï¼Œæ˜“äºç†è§£ã€‚</li><li>å®éªŒè®¾ç½®å…¨é¢ï¼Œç»“æœå¯ä¿¡ã€‚</li><li>ä»£ç å’Œæ•°æ®å·²å…¬å¼€ï¼Œä¾¿äºå…¶ä»–äººå¤ç°ç»“æœã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-4a0bc8faf250a6fbe548d099582570b0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e8fad6c067ffca0f2b396d38c7e58bbd.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e71c253fdfbf8c1083d326f01390eaeb.jpg" align="middle"></details><h2 id="VF-NeRF-Viewshed-Fields-for-Rigid-NeRF-Registration"><a href="#VF-NeRF-Viewshed-Fields-for-Rigid-NeRF-Registration" class="headerlink" title="VF-NeRF: Viewshed Fields for Rigid NeRF Registration"></a>VF-NeRF: Viewshed Fields for Rigid NeRF Registration</h2><p><strong>Authors:Leo Segre, Shai Avidan</strong></p><p>3D scene registration is a fundamental problem in computer vision that seeks the best 6-DoF alignment between two scenes. This problem was extensively investigated in the case of point clouds and meshes, but there has been relatively limited work regarding Neural Radiance Fields (NeRF). In this paper, we consider the problem of rigid registration between two NeRFs when the position of the original cameras is not given. Our key novelty is the introduction of Viewshed Fields (VF), an implicit function that determines, for each 3D point, how likely it is to be viewed by the original cameras. We demonstrate how VF can help in the various stages of NeRF registration, with an extensive evaluation showing that VF-NeRF achieves SOTA results on various datasets with different capturing approaches such as LLFF and Objaverese. </p><p><a href="http://arxiv.org/abs/2404.03349v1">PDF</a> </p><p><strong>Summary</strong><br>ç¥ç»è¾å°„åœº (NeRF) çš„åˆšæ€§é…å‡†é—®é¢˜ï¼Œå¼•å…¥äº†è§†é‡åœº (VF) ä»¥æé«˜é…å‡†æ€§èƒ½ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>3D åœºæ™¯é…å‡†æ˜¯è®¡ç®—æœºè§†è§‰ä¸­å¯»æ‰¾ä¸¤ä¸ªåœºæ™¯ä¹‹é—´æœ€ä½³ 6 è‡ªç”±åº¦å¯¹é½çš„åŸºæœ¬é—®é¢˜ã€‚</li><li>ç‚¹äº‘å’Œç½‘æ ¼åœºæ™¯é…å‡†å¾—åˆ°äº†å¹¿æ³›ç ”ç©¶ï¼Œä½†å…³äºç¥ç»è¾å°„åœº (NeRF) çš„å·¥ä½œç›¸å¯¹è¾ƒå°‘ã€‚</li><li>è€ƒè™‘äº†åœ¨æœªç»™å®šåŸå§‹ç›¸æœºä½ç½®çš„æƒ…å†µä¸‹ï¼Œä¸¤ä¸ª NeRF ä¹‹é—´çš„åˆšæ€§é…å‡†é—®é¢˜ã€‚</li><li>æå‡ºäº†ä¸€ç§æ–°çš„è§†å›¾åœº (VF) æ¦‚å¿µï¼Œå®ƒæ˜¯ä¸€ç§éšå¼å‡½æ•°ï¼Œç”¨äºç¡®å®šæ¯ä¸ª 3D ç‚¹è¢«åŸå§‹ç›¸æœºè§‚å¯Ÿåˆ°çš„å¯èƒ½æ€§ã€‚</li><li>è¯æ˜äº† VF å¦‚ä½•å¸®åŠ© NeRF é…å‡†çš„å„ä¸ªé˜¶æ®µã€‚</li><li>åœ¨å¹¿æ³›çš„è¯„ä¼°ä¸­è¡¨æ˜ï¼ŒVF-NeRF åœ¨ä½¿ç”¨ LLFF å’Œ Objaverser ç­‰ä¸åŒæ•æ‰æ–¹æ³•çš„ä¸åŒæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„ç»“æœã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šVF-NeRFï¼šåˆšæ€§ NeRF çš„å¯è§†åŸŸåœº</li><li>ä½œè€…ï¼šLeo Segreã€Shai Avidan</li><li>éš¶å±å•ä½ï¼šç‰¹æ‹‰ç»´å¤«å¤§å­¦</li><li>å…³é”®è¯ï¼šç¥ç»è¾å°„åœºã€3D é…å‡†ã€å½’ä¸€åŒ–æµ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://leosegre.github.io/VF_NeRF/   Github ä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼š   (1)ï¼šç ”ç©¶èƒŒæ™¯ï¼š3D åœºæ™¯é…å‡†æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€ä¸ªåŸºæœ¬é—®é¢˜ï¼Œæ—¨åœ¨å¯»æ‰¾ä¸¤ä¸ªåœºæ™¯ä¹‹é—´çš„æœ€ä½³ 6 è‡ªç”±åº¦å¯¹é½ã€‚è¯¥é—®é¢˜å·²åœ¨ç‚¹äº‘å’Œç½‘æ ¼çš„æƒ…å†µä¸‹å¾—åˆ°å¹¿æ³›ç ”ç©¶ï¼Œä½†å…³äºç¥ç»è¾å°„åœº (NeRF) çš„å·¥ä½œç›¸å¯¹è¾ƒå°‘ã€‚   (2)ï¼šè¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šå½“åŸå§‹æ‘„åƒæœºçš„ä½ç½®æœªçŸ¥æ—¶ï¼Œè¿‡å»çš„æ–¹æ³•åœ¨ä¸¤ä¸ª NeRF ä¹‹é—´è¿›è¡Œåˆšæ€§é…å‡†æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚   (3)ï¼šæœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç§°ä¸ºå¯è§†åŸŸåœº (VF) çš„éšå¼å‡½æ•°ï¼Œè¯¥å‡½æ•°ç¡®å®šæ¯ä¸ª 3D ç‚¹è¢«åŸå§‹ç›¸æœºè§‚å¯Ÿåˆ°çš„å¯èƒ½æ€§ã€‚VF-NeRF åˆ©ç”¨ VF è¾…åŠ© NeRF é…å‡†çš„å„ä¸ªé˜¶æ®µã€‚   (4)ï¼šæ–¹æ³•åœ¨ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼šVF-NeRF åœ¨ä½¿ç”¨ä¸åŒæ•è·æ–¹æ³•ï¼ˆå¦‚ LLFF å’Œ Objaverseï¼‰çš„å„ç§æ•°æ®é›†ä¸Šå®ç°äº† SOTA ç»“æœï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</li></ol><p>7.Methodsï¼šï¼ˆ1ï¼‰ä½¿ç”¨Viewshed Fieldï¼ˆVFï¼‰ç”Ÿæˆåœºæ™¯Aä¸­å¤šä¸ªè‰¯å¥½çš„ç›¸æœºè§†è§’é›†åˆCAï¼›ï¼ˆ2ï¼‰åˆ©ç”¨åœºæ™¯Bçš„VFåˆ¤æ–­ç»è¿‡å˜æ¢Tçš„CAä¸­ç›¸æœºè§‚å¯Ÿåœºæ™¯Bä¸­è‰¯å¥½ç‚¹çš„ç¨‹åº¦ï¼Œè®¡ç®—å˜æ¢Tçš„åˆå§‹åŒ–å¾—åˆ†ï¼›ï¼ˆ3ï¼‰éšæœºé‡‡æ ·å¤šä¸ªå˜æ¢Tï¼Œé€‰æ‹©å¾—åˆ†æœ€é«˜çš„ä½œä¸ºåˆå§‹åŒ–ï¼›ï¼ˆ4ï¼‰ä»NeRFæ½œåœ¨åˆ†å¸ƒä¸­é‡‡æ ·ç‚¹ï¼Œç”Ÿæˆå®šå‘ç‚¹ï¼Œå¹¶ä½¿ç”¨NeRFè·å–å¯¹åº”çš„å¯†åº¦å’ŒRGBï¼›ï¼ˆ5ï¼‰åˆ©ç”¨å¯†åº¦å€¼å’Œé˜ˆå€¼æ»¤å‡ºä¸ç¡®å®šçš„ç‚¹ï¼Œç”Ÿæˆç‚¹äº‘ï¼›ï¼ˆ6ï¼‰ä½¿ç”¨å·²æœ‰çš„ç‚¹äº‘å…¨å±€é…å‡†æ–¹æ³•ï¼Œå¾—åˆ°åˆå§‹çŒœæµ‹ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬æ–‡æå‡ºäº†VF-NeRFï¼Œä¸€ç§ç”¨äºåˆšæ€§NeRFé…å‡†çš„éšå¼å‡½æ•°ï¼Œè¯¥å‡½æ•°ç¡®å®šæ¯ä¸ª3Dç‚¹è¢«åŸå§‹ç›¸æœºè§‚å¯Ÿåˆ°çš„å¯èƒ½æ€§ã€‚VF-NeRFåˆ©ç”¨VFè¾…åŠ©NeRFé…å‡†çš„å„ä¸ªé˜¶æ®µï¼Œåœ¨ä½¿ç”¨ä¸åŒæ•è·æ–¹æ³•ï¼ˆå¦‚LLFFå’ŒObjaverseï¼‰çš„å„ç§æ•°æ®é›†ä¸Šå®ç°äº†SOTAç»“æœï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§ç§°ä¸ºå¯è§†åŸŸåœº(VF)çš„éšå¼å‡½æ•°ï¼Œè¯¥å‡½æ•°ç¡®å®šæ¯ä¸ª3Dç‚¹è¢«åŸå§‹ç›¸æœºè§‚å¯Ÿåˆ°çš„å¯èƒ½æ€§ã€‚</li><li>å°†VFä¸å½’ä¸€åŒ–æµï¼ˆNFï¼‰ç›¸ç»“åˆï¼Œç”¨äºé‡‡æ ·æ–°é¢–çš„ç›¸æœºè§†ç‚¹å’Œç”Ÿæˆæœ‰è‰²çš„3Dç‚¹äº‘ã€‚</li><li>åˆ©ç”¨VFæŒ‡å¯¼å…‰çº¿é‡‡æ ·ï¼Œä¼˜åŒ–NeRFé…å‡†ã€‚</li><li>æ€§èƒ½ï¼š</li><li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†SOTAç»“æœï¼ŒåŒ…æ‹¬æ­£é¢åœºæ™¯ã€ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„è§†é¢‘å’Œåˆæˆå¯¹è±¡å›¾åƒã€‚</li><li>åœ¨å…·æœ‰æœ€å°é…å‡†è¯¯å·®çš„å™ªå£°è®¾ç½®ä¸­ï¼Œä¸COLMAPçš„è¯¯å·®å’Œå…‰åº¦è¯¯å·®çš„ä¼˜åŠ£éš¾ä»¥åŒºåˆ†ã€‚</li><li>å·¥ä½œé‡ï¼š</li><li>ä½¿ç”¨Nerfactoä½œä¸ºNeRFè¡¨ç¤ºï¼Œæ¯æ‰¹æ¬¡é‡‡æ ·1024æ¡å…‰çº¿ï¼Œä½¿ç”¨Adamä¼˜åŒ–å™¨è¿›è¡Œè®­ç»ƒï¼Œåˆå§‹å­¦ä¹ ç‡ä¸º1e-2ï¼ŒæŒ‡æ•°è¡°å‡ã€‚</li><li>ä½¿ç”¨å…·æœ‰L=4å±‚å’ŒH=128éšè—ç»´åº¦çš„Real-NVPå­¦ä¹ VFï¼Œä½¿ç”¨RAdamä¼˜åŒ–å™¨ï¼Œæ’å®šå­¦ä¹ ç‡ä¸º5e-5ã€‚</li><li>å®é™…åœºæ™¯NeRFè®­ç»ƒ60Kæ¬¡è¿­ä»£ï¼ŒVFè®­ç»ƒåœ¨æœ€å10Kæ¬¡è¿­ä»£ä¸­å¯ç”¨ã€‚</li><li>åˆæˆåœºæ™¯NeRFè®­ç»ƒ20Kæ¬¡è¿­ä»£ï¼ŒVFè®­ç»ƒåœ¨æœ€å5Kæ¬¡è¿­ä»£ä¸­å¯ç”¨ï¼Œå¹¶åœ¨å›¾åƒé€æ˜ï¼ˆRGBAå›¾åƒçš„Î±&lt;128ï¼‰æ—¶å¿½ç•¥ã€‚</li><li>å…‰åº¦åˆå§‹åŒ–åœ¨25ä¸ªéšæœºå˜æ¢ä¸Šå®Œæˆã€‚</li><li>å¯¹äºPCåˆå§‹åŒ–ï¼Œé¦–å…ˆä»VFåˆ†å¸ƒä¸­é‡‡æ ·100Kä¸ªç‚¹ç”Ÿæˆç‚¹äº‘ï¼Œé€‰æ‹©å¯†åº¦é«˜äº10çš„ç‚¹ï¼Œå¹¶å°†è¿™äº›ç‚¹äº‘ä½œä¸ºç»å…¸å…¨å±€é…å‡†æ–¹æ³•çš„è¾“å…¥ã€‚</li><li>åœ¨é…å‡†é˜¶æ®µï¼Œå¯¹äºå®é™…åœºæ™¯ï¼Œä½¿ç”¨SGDä¼˜åŒ–å™¨å¯¹6DoFå‚æ•°è¿›è¡Œ15Kæ¬¡è¿­ä»£ä¼˜åŒ–ï¼Œæ¯æ¬¡è¿­ä»£32Kä¸ªæ ·æœ¬ï¼Œåˆå§‹å­¦ä¹ ç‡ä¸º5e-3ï¼ŒæŒ‡æ•°è¡°å‡ã€‚</li><li>å¯¹äºåˆæˆåœºæ™¯ï¼Œä½¿ç”¨SGDä¼˜åŒ–å™¨å¯¹6DoFå‚æ•°è¿›è¡Œ2.5Kæ¬¡è¿­ä»£ä¼˜åŒ–ï¼Œæ¯æ¬¡è¿­ä»£8128ä¸ªæ ·æœ¬ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-c42dc03989b870facba1e92f9650d148.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5036daad3cd46832226594b54b75df78.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fcba1449fcbdf5cb3bf62129225960c5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4a10f7f3b4aaec1b94ed587220378c6b.jpg" align="middle"></details><h2 id="LiDAR4D-Dynamic-Neural-Fields-for-Novel-Space-time-View-LiDAR-Synthesis"><a href="#LiDAR4D-Dynamic-Neural-Fields-for-Novel-Space-time-View-LiDAR-Synthesis" class="headerlink" title="LiDAR4D: Dynamic Neural Fields for Novel Space-time View LiDAR Synthesis"></a>LiDAR4D: Dynamic Neural Fields for Novel Space-time View LiDAR Synthesis</h2><p><strong>Authors:Zehan Zheng, Fan Lu, Weiyi Xue, Guang Chen, Changjun Jiang</strong></p><p>Although neural radiance fields (NeRFs) have achieved triumphs in image novel view synthesis (NVS), LiDAR NVS remains largely unexplored. Previous LiDAR NVS methods employ a simple shift from image NVS methods while ignoring the dynamic nature and the large-scale reconstruction problem of LiDAR point clouds. In light of this, we propose LiDAR4D, a differentiable LiDAR-only framework for novel space-time LiDAR view synthesis. In consideration of the sparsity and large-scale characteristics, we design a 4D hybrid representation combined with multi-planar and grid features to achieve effective reconstruction in a coarse-to-fine manner. Furthermore, we introduce geometric constraints derived from point clouds to improve temporal consistency. For the realistic synthesis of LiDAR point clouds, we incorporate the global optimization of ray-drop probability to preserve cross-region patterns. Extensive experiments on KITTI-360 and NuScenes datasets demonstrate the superiority of our method in accomplishing geometry-aware and time-consistent dynamic reconstruction. Codes are available at <a href="https://github.com/ispc-lab/LiDAR4D">https://github.com/ispc-lab/LiDAR4D</a>. </p><p><a href="http://arxiv.org/abs/2404.02742v1">PDF</a> Accepted by CVPR 2024. Project Page:   <a href="https://dyfcalid.github.io/LiDAR4D">https://dyfcalid.github.io/LiDAR4D</a></p><p><strong>Summary</strong><br> æ¿€å…‰é›·è¾¾ä¸“å±çš„å¯å¾®ç¥ç»è¾å°„åœºæ¡†æ¶ï¼Œå®ç°å¯ä¿¡ã€æ—¶é—´ä¸€è‡´çš„åŠ¨æ€é‡å»ºã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºäº†é¦–ä¸ªæ¿€å…‰é›·è¾¾ç¥ç»è¾å°„åœºï¼ˆLiDAR NeRFï¼‰ï¼Œç”¨äºæ¿€å…‰é›·è¾¾æ–°è§†ç‚¹åˆæˆã€‚</li><li>è®¾è®¡äº†ä¸€ç§ 4D æ··åˆè¡¨ç¤ºï¼Œç»“åˆäº†å¤šå¹³é¢å’Œç½‘æ ¼ç‰¹å¾ï¼Œä»¥æœ‰æ•ˆé‡å»ºå¤§è§„æ¨¡æ¿€å…‰é›·è¾¾ç‚¹äº‘ã€‚</li><li>å¼•å…¥äº†æºè‡ªç‚¹äº‘çš„å‡ ä½•çº¦æŸï¼Œå¢å¼ºäº†æ—¶é—´ä¸€è‡´æ€§ã€‚</li><li>é›†æˆäº†å°„çº¿æŠ•å°„æ¦‚ç‡çš„å…¨å±€ä¼˜åŒ–ï¼Œä»¥ä¿ç•™è·¨åŒºåŸŸæ¨¡å¼ï¼Œå®ç°æ¿€å…‰é›·è¾¾ç‚¹äº‘çš„çœŸå®åˆæˆã€‚</li><li>åœ¨ KITTI-360 å’Œ NuScenes æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜äº†è¯¥æ–¹æ³•åœ¨å®ç°æ„ŸçŸ¥å‡ ä½•å’Œæ—¶é—´ä¸€è‡´åŠ¨æ€é‡å»ºæ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚</li><li>å·²å¼€æºä»£ç ï¼š<a href="https://github.com/ispc-lab/LiDAR4Dã€‚">https://github.com/ispc-lab/LiDAR4Dã€‚</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šLiDAR4Dï¼šç”¨äºæ–°å‹æ—¶ç©ºè§†å›¾ LiDAR åˆæˆçš„åŠ¨æ€ç¥ç»åœº</li><li>ä½œè€…ï¼šHongrui Zhou, Xiaoguang Han, Yulan Guo, Qiang Zhang, Hao Li, Wenping Wang</li><li>æ‰€å±æœºæ„ï¼šä¸­å›½ç§‘å­¦é™¢å¤§å­¦è®¡ç®—æœºå­¦é™¢</li><li>å…³é”®è¯ï¼šLiDAR ç‚¹äº‘ã€ç¥ç»è¾å°„åœºã€æ—¶ç©ºè§†å›¾åˆæˆã€åŠ¨æ€é‡å»º</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.03988Github ä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœº (NeRF) åœ¨å›¾åƒæ–°è§†å›¾åˆæˆ (NVS) ä¸­å–å¾—äº†æˆåŠŸï¼Œä½† LiDAR NVS ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ç°æœ‰çš„ LiDAR NVS æ–¹æ³•ç®€å•åœ°ä»å›¾åƒ NVS æ–¹æ³•è½¬ç§»ï¼Œè€Œå¿½ç•¥äº† LiDAR ç‚¹äº‘çš„åŠ¨æ€ç‰¹æ€§å’Œå¤§è§„æ¨¡é‡å»ºé—®é¢˜ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç°æœ‰æ–¹æ³•å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š</li><li>å¿½ç•¥äº† LiDAR ç‚¹äº‘çš„åŠ¨æ€ç‰¹æ€§ï¼Œå¯¼è‡´åŠ¨æ€ç‰©ä½“å‡ºç°ä¼ªå½±å’Œå™ªå£°ã€‚</li><li>ç¼ºä¹å¯¹å¤§è§„æ¨¡åœºæ™¯ä¸­ç»†èŠ‚çš„é‡å»ºèƒ½åŠ›ã€‚</li><li>æ— æ³•å»ºç«‹è¿œè·ç¦»å¯¹åº”å…³ç³»ã€‚ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº† LiDAR4Dï¼Œè¿™æ˜¯ä¸€ä¸ªå¯å¾®çš„ä»…é™ LiDAR çš„æ–°æ—¶ç©º LiDAR è§†å›¾åˆæˆæ¡†æ¶ã€‚è¯¥æ¡†æ¶åŒ…å«ä»¥ä¸‹åˆ›æ–°ï¼š</li><li>è®¾è®¡äº†ä¸€ç§ 4D æ··åˆè¡¨ç¤ºï¼Œç»“åˆäº†å¤šå¹³é¢å’Œç½‘æ ¼ç‰¹å¾ï¼Œä»¥ç²—åˆ°ç»†çš„æ–¹å¼è¿›è¡Œæœ‰æ•ˆé‡å»ºã€‚</li><li>å¼•å…¥äº†ä»ç‚¹äº‘æ´¾ç”Ÿçš„å‡ ä½•çº¦æŸï¼Œä»¥æé«˜æ—¶é—´ä¸€è‡´æ€§ã€‚</li><li>é’ˆå¯¹ LiDAR ç‚¹äº‘çš„çœŸå®åˆæˆï¼Œå¼•å…¥äº†å°„çº¿æ‰è½æ¦‚ç‡çš„å…¨å±€ä¼˜åŒ–ï¼Œä»¥ä¿ç•™è·¨åŒºåŸŸæ¨¡å¼ã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šå–å¾—çš„æˆå°±ï¼šåœ¨ KITTI-360 å’Œ NuScenes æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å®ç°å‡ ä½•æ„ŸçŸ¥å’Œæ—¶é—´ä¸€è‡´çš„åŠ¨æ€é‡å»ºæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å…·ä½“æ€§èƒ½å¦‚ä¸‹ï¼š</li><li>åœ¨ KITTI-360 æ•°æ®é›†ä¸Šï¼Œåœ¨å‡ ä½•å’Œå¼ºåº¦ RMSE æŒ‡æ ‡ä¸Šåˆ†åˆ«æ¯” LiDAR-NeRF é™ä½äº† 12.0% å’Œ 13.7%ã€‚</li><li>åœ¨ NuScenes æ•°æ®é›†ä¸Šï¼Œåœ¨å‡ ä½•å’Œå¼ºåº¦ RMSE æŒ‡æ ‡ä¸Šåˆ†åˆ«æ¯” LiDAR-NeRF é™ä½äº† 11.6% å’Œ 13.5%ã€‚</li></ol><p><strong>æ–¹æ³•</strong></p><p>ï¼ˆ1ï¼‰4Dæ··åˆå¹³é¢æ ¼è¡¨ç¤ºï¼šé‡‡ç”¨å¤šå¹³é¢å’Œç½‘æ ¼ç‰¹å¾ç›¸ç»“åˆçš„4Dæ··åˆè¡¨ç¤ºï¼Œä»¥ç²—åˆ°ç»†çš„æ–¹å¼è¿›è¡Œæœ‰æ•ˆé‡å»ºã€‚</p><p>ï¼ˆ2ï¼‰åœºæ™¯æµå…ˆéªŒï¼šå¼•å…¥ä»ç‚¹äº‘æ´¾ç”Ÿçš„åœºæ™¯æµå…ˆéªŒï¼Œä»¥æé«˜æ—¶é—´ä¸€è‡´æ€§ã€‚</p><p>ï¼ˆ3ï¼‰ç¥ç»LiDARåœºï¼šå»ºç«‹åŸºäºLiDARçš„ç¥ç»åœºï¼Œé¢„æµ‹æ·±åº¦ã€å¼ºåº¦å’Œå°„çº¿æ‰è½æ¦‚ç‡ã€‚</p><p>ï¼ˆ4ï¼‰å°„çº¿æ‰è½æ¦‚ç‡ä¼˜åŒ–ï¼šå¼•å…¥å°„çº¿æ‰è½æ¦‚ç‡çš„å…¨å±€ä¼˜åŒ–ï¼Œä»¥ä¿ç•™è·¨åŒºåŸŸæ¨¡å¼ï¼Œæé«˜ç”ŸæˆçœŸå®æ€§ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡é’ˆå¯¹ç°æœ‰ LiDAR NVS æ–¹æ³•çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶æ¥è§£å†³åŠ¨æ€é‡å»ºã€å¤§è§„æ¨¡åœºæ™¯è¡¨å¾å’ŒçœŸå®åˆæˆè¿™ä¸‰ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚æå‡ºçš„æ–¹æ³• LiDAR4D åœ¨å¹¿æ³›çš„å®éªŒä¸­è¯æ˜äº†å…¶ä¼˜è¶Šæ€§ï¼Œå®ç°äº†å¤§è§„æ¨¡åŠ¨æ€ç‚¹äº‘åœºæ™¯çš„å‡ ä½•æ„ŸçŸ¥å’Œæ—¶é—´ä¸€è‡´é‡å»ºï¼Œå¹¶ç”Ÿæˆäº†æ›´æ¥è¿‘çœŸå®åˆ†å¸ƒçš„æ–°æ—¶ç©ºè§†å›¾ LiDAR ç‚¹äº‘ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œæœªæ¥çš„å·¥ä½œå°†æ›´å¤šåœ°é›†ä¸­åœ¨å°† LiDAR ç‚¹äº‘ä¸ç¥ç»è¾å°„åœºç›¸ç»“åˆï¼Œå¹¶æ¢ç´¢åŠ¨æ€åœºæ™¯é‡å»ºå’Œåˆæˆçš„æ›´å¤šå¯èƒ½æ€§ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§ 4D æ··åˆå¹³é¢æ ¼è¡¨ç¤ºï¼Œç»“åˆäº†å¤šå¹³é¢å’Œç½‘æ ¼ç‰¹å¾ï¼Œä»¥ç²—åˆ°ç»†çš„æ–¹å¼è¿›è¡Œæœ‰æ•ˆé‡å»ºã€‚</li><li>å¼•å…¥äº†ä»ç‚¹äº‘æ´¾ç”Ÿçš„åœºæ™¯æµå…ˆéªŒï¼Œä»¥æé«˜æ—¶é—´ä¸€è‡´æ€§ã€‚</li><li>å»ºç«‹äº†åŸºäº LiDAR çš„ç¥ç»åœºï¼Œé¢„æµ‹æ·±åº¦ã€å¼ºåº¦å’Œå°„çº¿æ‰è½æ¦‚ç‡ã€‚</li><li>å¼•å…¥äº†å°„çº¿æ‰è½æ¦‚ç‡çš„å…¨å±€ä¼˜åŒ–ï¼Œä»¥ä¿ç•™è·¨åŒºåŸŸæ¨¡å¼ï¼Œæé«˜ç”ŸæˆçœŸå®æ€§ã€‚æ€§èƒ½ï¼š</li><li>åœ¨ KITTI-360 æ•°æ®é›†ä¸Šï¼Œåœ¨å‡ ä½•å’Œå¼ºåº¦ RMSE æŒ‡æ ‡ä¸Šåˆ†åˆ«æ¯” LiDAR-NeRF é™ä½äº† 12.0% å’Œ 13.7%ã€‚</li><li>åœ¨ NuScenes æ•°æ®é›†ä¸Šï¼Œåœ¨å‡ ä½•å’Œå¼ºåº¦ RMSE æŒ‡æ ‡ä¸Šåˆ†åˆ«æ¯” LiDAR-NeRF é™ä½äº† 11.6% å’Œ 13.5%ã€‚å·¥ä½œé‡ï¼š</li><li>æå‡ºäº†ä¸€ç§æ–°çš„æ—¶ç©º LiDAR è§†å›¾åˆæˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶è§£å†³äº†åŠ¨æ€é‡å»ºã€å¤§è§„æ¨¡åœºæ™¯è¡¨å¾å’ŒçœŸå®åˆæˆè¿™ä¸‰ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚</li><li>åœ¨ KITTI-360 å’Œ NuScenes æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚</li><li>å¼€æºäº†ä»£ç ï¼Œä¾¿äºå…¶ä»–ç ”ç©¶äººå‘˜è¿›è¡Œç ”ç©¶å’Œåº”ç”¨ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-2963b70a266c3a04d92a7dbee2c86759.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7a65da90b3848baf2adb2e8ce440176c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4fd1d5df12dbb5393c4e1c3591fe5d11.jpg" align="middle"><img src="https://pica.zhimg.com/v2-4f24d8c17a6447cf6c6bff2640772e2c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f2d050ccfba4add3a017bb850515949a.jpg" align="middle"></details><h2 id="Freditor-High-Fidelity-and-Transferable-NeRF-Editing-by-Frequency-Decomposition"><a href="#Freditor-High-Fidelity-and-Transferable-NeRF-Editing-by-Frequency-Decomposition" class="headerlink" title="Freditor: High-Fidelity and Transferable NeRF Editing by Frequency   Decomposition"></a>Freditor: High-Fidelity and Transferable NeRF Editing by Frequency   Decomposition</h2><p><strong>Authors:Yisheng He, Weihao Yuan, Siyu Zhu, Zilong Dong, Liefeng Bo, Qixing Huang</strong></p><p>This paper enables high-fidelity, transferable NeRF editing by frequency decomposition. Recent NeRF editing pipelines lift 2D stylization results to 3D scenes while suffering from blurry results, and fail to capture detailed structures caused by the inconsistency between 2D editings. Our critical insight is that low-frequency components of images are more multiview-consistent after editing compared with their high-frequency parts. Moreover, the appearance style is mainly exhibited on the low-frequency components, and the content details especially reside in high-frequency parts. This motivates us to perform editing on low-frequency components, which results in high-fidelity edited scenes. In addition, the editing is performed in the low-frequency feature space, enabling stable intensity control and novel scene transfer. Comprehensive experiments conducted on photorealistic datasets demonstrate the superior performance of high-fidelity and transferable NeRF editing. The project page is at \url{<a href="https://aigc3d.github.io/freditor}">https://aigc3d.github.io/freditor}</a>. </p><p><a href="http://arxiv.org/abs/2404.02514v1">PDF</a> </p><p><strong>Summary</strong><br>ä½é¢‘ç‰¹å¾ç©ºé—´ç¼–è¾‘æé«˜NeRFå¯ç¼–è¾‘æ€§ï¼Œå¸¦æ¥é«˜ä¿çœŸå¯è¿ç§»çš„NeRFç¼–è¾‘ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>å›¾åƒç¼–è¾‘åï¼Œä½é¢‘åˆ†é‡è·¨è§†è§’ä¸€è‡´æ€§æ›´é«˜ã€‚</li><li>å¤–è§‚é£æ ¼ä¸»è¦ä½“ç°åœ¨ä½é¢‘åˆ†é‡ä¸Šï¼Œå†…å®¹ç»†èŠ‚ä¸»è¦ä½äºé«˜é¢‘åˆ†é‡ä¸Šã€‚</li><li>åœ¨ä½é¢‘åˆ†é‡ä¸Šè¿›è¡Œç¼–è¾‘å¯äº§ç”Ÿé«˜ä¿çœŸç¼–è¾‘åœºæ™¯ã€‚</li><li>ä½é¢‘ç‰¹å¾ç©ºé—´ä¸­çš„ç¼–è¾‘å¯å®ç°ç¨³å®šçš„å¼ºåº¦æ§åˆ¶å’Œæ–°åœºæ™¯è¿ç§»ã€‚</li><li>å®éªŒè¡¨æ˜ï¼Œé«˜ä¿çœŸå¯è¿ç§»çš„NeRFç¼–è¾‘å…·æœ‰å‡ºè‰²æ€§èƒ½ã€‚</li><li>é¡¹ç›®ä¸»é¡µï¼š<a href="https://aigc3d.github.io/freditorã€‚">https://aigc3d.github.io/freditorã€‚</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šé¢‘ç‡åˆ†è§£çš„é«˜ä¿çœŸå¯è¿ç§» NeRF ç¼–è¾‘</li><li>ä½œè€…ï¼šYisheng He, Weihao Yuan, Siyu Zhu, Zilong Dong, Liefeng Bo, Qixing Huang</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šé˜¿é‡Œå·´å·´é›†å›¢</li><li>å…³é”®è¯ï¼šNeRFã€ç¼–è¾‘ã€é¢‘ç‡åˆ†è§£ã€é«˜ä¿çœŸã€å¯è¿ç§»</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.02514   Github ä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼š   (1)ï¼šç ”ç©¶èƒŒæ™¯ï¼šNeRF ç¼–è¾‘ç®¡é“å°† 2D é£æ ¼åŒ–ç»“æœæå‡åˆ° 3D åœºæ™¯ï¼Œä½†å­˜åœ¨ç»“æœæ¨¡ç³Šçš„é—®é¢˜ï¼Œå¹¶ä¸”ç”±äº 2D ç¼–è¾‘çš„ä¸ä¸€è‡´æ€§è€Œæ— æ³•æ•æ‰åˆ°è¯¦ç»†çš„ç»“æ„ã€‚   (2)ï¼šè¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šç°æœ‰æ–¹æ³•å­˜åœ¨çš„é—®é¢˜åœ¨äºï¼Œç¼–è¾‘åçš„å›¾åƒçš„ä½é¢‘åˆ†é‡æ¯”é«˜é¢‘éƒ¨åˆ†æ›´å…·å¤šè§†å›¾ä¸€è‡´æ€§ã€‚è€Œä¸”ï¼Œå¤–è§‚é£æ ¼ä¸»è¦ä½“ç°åœ¨ä½é¢‘åˆ†é‡ä¸Šï¼Œè€Œå†…å®¹ç»†èŠ‚åˆ™ä¸»è¦å­˜åœ¨äºé«˜é¢‘éƒ¨åˆ†ã€‚   (3)ï¼šç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡é¢‘ç‡åˆ†è§£è¿›è¡Œ NeRF ç¼–è¾‘çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•åœ¨ä½é¢‘åˆ†é‡ä¸Šè¿›è¡Œç¼–è¾‘ï¼Œä»è€Œäº§ç”Ÿé«˜ä¿çœŸç¼–è¾‘åœºæ™¯ã€‚   (4)ï¼šæ–¹æ³•æ€§èƒ½ï¼šè¯¥æ–¹æ³•åœ¨åœºæ™¯ç¼–è¾‘å’Œå¯è¿ç§»ç¼–è¾‘ä»»åŠ¡ä¸Šå–å¾—äº†è‰¯å¥½çš„æ€§èƒ½ã€‚åœ¨åœºæ™¯ç¼–è¾‘ä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•å¯ä»¥ç”Ÿæˆé«˜ä¿çœŸç¼–è¾‘åœºæ™¯ï¼Œå¹¶ä¸”å¯ä»¥æ•æ‰åˆ°è¯¦ç»†çš„ç»“æ„ã€‚åœ¨å¯è¿ç§»ç¼–è¾‘ä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•å¯ä»¥å°†åœ¨ä¸€ä¸ªåœºæ™¯ä¸­è®­ç»ƒçš„ç¼–è¾‘æ¨¡å‹ç›´æ¥è¿ç§»åˆ°ä¸åŒçš„æ–°åœºæ™¯ä¸­ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡æå‡ºçš„æ–¹æ³•çš„ç›®æ ‡ã€‚</li></ol><p>7.æ–¹æ³•ï¼š(1)ï¼šé¢‘ç‡åˆ†è§£é«˜ä¿çœŸå¯è¿ç§»NeRFç¼–è¾‘æ–¹æ³•é€šè¿‡é¢‘ç‡åˆ†è§£å¯¹NeRFè¿›è¡Œç¼–è¾‘ï¼Œä»¥äº§ç”Ÿé«˜ä¿çœŸç¼–è¾‘åœºæ™¯ã€‚(2)ï¼šè¯¥æ–¹æ³•åœ¨ä½é¢‘åˆ†é‡ä¸Šè¿›è¡Œç¼–è¾‘ï¼Œä»è€Œäº§ç”Ÿé«˜ä¿çœŸç¼–è¾‘åœºæ™¯ï¼Œå¹¶ä¸”å¯ä»¥æ•æ‰åˆ°è¯¦ç»†çš„ç»“æ„ã€‚(3)ï¼šè¯¥æ–¹æ³•åœ¨åœºæ™¯ç¼–è¾‘å’Œå¯è¿ç§»ç¼–è¾‘ä»»åŠ¡ä¸Šå–å¾—äº†è‰¯å¥½çš„æ€§èƒ½ã€‚</p><ol><li>ç»“è®ºï¼š(1): æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§é€šè¿‡é¢‘ç‡åˆ†è§£è¿›è¡Œ NeRF ç¼–è¾‘çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨ä½é¢‘åˆ†é‡ä¸Šè¿›è¡Œç¼–è¾‘ï¼Œä»è€Œäº§ç”Ÿé«˜ä¿çœŸç¼–è¾‘åœºæ™¯ï¼Œå¹¶ä¸”å¯ä»¥æ•æ‰åˆ°è¯¦ç»†çš„ç»“æ„ã€‚(2): åˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§é€šè¿‡é¢‘ç‡åˆ†è§£è¿›è¡Œ NeRF ç¼–è¾‘çš„æ–¹æ³•ã€‚</li><li>è¯¥æ–¹æ³•åœ¨ä½é¢‘åˆ†é‡ä¸Šè¿›è¡Œç¼–è¾‘ï¼Œä»è€Œäº§ç”Ÿé«˜ä¿çœŸç¼–è¾‘åœºæ™¯ï¼Œå¹¶ä¸”å¯ä»¥æ•æ‰åˆ°è¯¦ç»†çš„ç»“æ„ã€‚</li><li>è¯¥æ–¹æ³•åœ¨åœºæ™¯ç¼–è¾‘å’Œå¯è¿ç§»ç¼–è¾‘ä»»åŠ¡ä¸Šå–å¾—äº†è‰¯å¥½çš„æ€§èƒ½ã€‚æ€§èƒ½ï¼š</li><li>åœ¨åœºæ™¯ç¼–è¾‘ä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•å¯ä»¥ç”Ÿæˆé«˜ä¿çœŸç¼–è¾‘åœºæ™¯ï¼Œå¹¶ä¸”å¯ä»¥æ•æ‰åˆ°è¯¦ç»†çš„ç»“æ„ã€‚</li><li>åœ¨å¯è¿ç§»ç¼–è¾‘ä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•å¯ä»¥å°†åœ¨ä¸€ä¸ªåœºæ™¯ä¸­è®­ç»ƒçš„ç¼–è¾‘æ¨¡å‹ç›´æ¥è¿ç§»åˆ°ä¸åŒçš„æ–°åœºæ™¯ä¸­ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚å·¥ä½œé‡ï¼š</li><li>è¯¥æ–¹æ³•éœ€è¦å¯¹ NeRF è¿›è¡Œé¢‘ç‡åˆ†è§£ï¼Œè¿™å¯èƒ½ä¼šå¢åŠ è®¡ç®—æˆæœ¬ã€‚</li><li>è¯¥æ–¹æ³•éœ€è¦åœ¨ä½é¢‘åˆ†é‡ä¸Šè¿›è¡Œç¼–è¾‘ï¼Œè¿™å¯èƒ½ä¼šå¢åŠ ç¼–è¾‘éš¾åº¦ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-fb6df696389c18849d0142f7f9834863.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e82d2e193f21cda63cdb16a49b96fb83.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-bed27b82ba84f05629b001f77ba3c8b1.jpg" align="middle"></details><h2 id="NeRFCodec-Neural-Feature-Compression-Meets-Neural-Radiance-Fields-for-Memory-Efficient-Scene-Representation"><a href="#NeRFCodec-Neural-Feature-Compression-Meets-Neural-Radiance-Fields-for-Memory-Efficient-Scene-Representation" class="headerlink" title="NeRFCodec: Neural Feature Compression Meets Neural Radiance Fields for   Memory-Efficient Scene Representation"></a>NeRFCodec: Neural Feature Compression Meets Neural Radiance Fields for   Memory-Efficient Scene Representation</h2><p><strong>Authors:Sicheng Li, Hao Li, Yiyi Liao, Lu Yu</strong></p><p>The emergence of Neural Radiance Fields (NeRF) has greatly impacted 3D scene modeling and novel-view synthesis. As a kind of visual media for 3D scene representation, compression with high rate-distortion performance is an eternal target. Motivated by advances in neural compression and neural field representation, we propose NeRFCodec, an end-to-end NeRF compression framework that integrates non-linear transform, quantization, and entropy coding for memory-efficient scene representation. Since training a non-linear transform directly on a large scale of NeRF feature planes is impractical, we discover that pre-trained neural 2D image codec can be utilized for compressing the features when adding content-specific parameters. Specifically, we reuse neural 2D image codec but modify its encoder and decoder heads, while keeping the other parts of the pre-trained decoder frozen. This allows us to train the full pipeline via supervision of rendering loss and entropy loss, yielding the rate-distortion balance by updating the content-specific parameters. At test time, the bitstreams containing latent code, feature decoder head, and other side information are transmitted for communication. Experimental results demonstrate our method outperforms existing NeRF compression methods, enabling high-quality novel view synthesis with a memory budget of 0.5 MB. </p><p><a href="http://arxiv.org/abs/2404.02185v1">PDF</a> Accepted at CVPR2024. The source code will be released</p><p><strong>Summary</strong><br>ç¥ç»è¾å°„åœº (NeRF) å‹ç¼©æ¡†æ¶ï¼Œé›†æˆäº†éçº¿æ€§å˜æ¢ã€é‡åŒ–å’Œç†µç¼–ç ï¼Œé€šè¿‡å¯é‡ç”¨é¢„è®­ç»ƒçš„ 2D å›¾åƒç¼–è§£ç å™¨ï¼Œå®ç°äº†é«˜æ•ˆçš„å†…å­˜åœºæ™¯è¡¨ç¤ºã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF çš„å…´èµ·ä¿ƒè¿›äº† 3D åœºæ™¯å»ºæ¨¡å’Œæ–°è§†å›¾åˆæˆã€‚</li><li>é«˜é€Ÿç‡-å¤±çœŸæ€§èƒ½çš„å‹ç¼©æ˜¯ 3D åœºæ™¯è¡¨ç¤ºçš„å…³é”®ã€‚</li><li>NeRFCodec é‡‡ç”¨éçº¿æ€§å˜æ¢ã€é‡åŒ–å’Œç†µç¼–ç ï¼Œå®ç°ç«¯åˆ°ç«¯çš„ NeRF å‹ç¼©ã€‚</li><li>é¢„è®­ç»ƒçš„ 2D å›¾åƒç¼–è§£ç å™¨å¯ç”¨äºå‹ç¼©ç‰¹å¾ï¼ŒåŒæ—¶æ·»åŠ å†…å®¹ç‰¹å®šå‚æ•°ã€‚</li><li>å¯é‡ç”¨ç¥ç» 2D å›¾åƒç¼–è§£ç å™¨ï¼Œä¿®æ”¹å…¶ç¼–ç å™¨å’Œè§£ç å™¨å¤´ï¼Œå†»ç»“å…¶ä»–éƒ¨åˆ†ã€‚</li><li>é€šè¿‡ç›‘ç£æ¸²æŸ“æŸå¤±å’Œç†µæŸå¤±è®­ç»ƒå®Œæ•´ç®¡é“ï¼Œæ›´æ–°å†…å®¹ç‰¹å®šå‚æ•°ï¼Œè¾¾åˆ°é€Ÿç‡å¤±çœŸå¹³è¡¡ã€‚</li><li>æµ‹è¯•æ—¶ï¼ŒåŒ…å«æ½œåœ¨ä»£ç ã€ç‰¹å¾è§£ç å¤´å’Œå…¶ä»–è¾¹ä¿¡æ¯çš„æ¯”ç‰¹æµç”¨äºé€šä¿¡ã€‚</li><li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰çš„ NeRF å‹ç¼©æ–¹æ³•ï¼Œä»¥ 0.5 MB çš„å†…å­˜é¢„ç®—å®ç°é«˜è´¨é‡çš„æ–°è§†å›¾åˆæˆã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šNeRFCodecï¼šç¥ç»ç‰¹å¾å‹ç¼©ä¸ç¥ç»è¾å°„åœºç›¸ç»“åˆï¼Œå®ç°å†…å­˜é«˜æ•ˆçš„åœºæ™¯è¡¨ç¤º</li><li>ä½œè€…ï¼šææ€æˆï¼Œææ˜Šï¼Œå»–æ€¡æ€¡ï¼Œäºé™†</li><li>æµ™æ±Ÿå¤§å­¦</li><li>Keywords: NeRF, Neural compression, Neural field representation, Rate-distortion optimization</li><li>é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.02185Githubï¼šNone</li><li>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åœ¨ 3D åœºæ™¯å»ºæ¨¡å’Œæ–°è§†è§’åˆæˆä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ï¼Œä½†å…¶è¡¨ç¤ºéœ€è¦å¤§é‡çš„å†…å­˜ï¼Œå‹ç¼© NeRF ä»¥æé«˜å­˜å‚¨æ•ˆç‡å’Œé€šä¿¡æ•ˆç‡æˆä¸ºä¸€ä¸ªé‡è¦çš„é—®é¢˜ã€‚(2) è¿‡å»æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨äºè®¾è®¡é«˜æ•ˆçš„æ•°æ®ç»“æ„æˆ–ä½¿ç”¨å‹ç¼©æŠ€æœ¯ï¼ˆå¦‚é‡åŒ–å’Œç†µç¼–ç ï¼‰æ¥å‹ç¼© NeRF å‚æ•°ï¼Œä½†å¿½ç•¥äº†å˜æ¢ç¼–ç çš„æœ‰æ•ˆæ€§ã€‚(3) ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡º NeRFCodecï¼Œä¸€ä¸ªç«¯åˆ°ç«¯çš„ NeRF å‹ç¼©æ¡†æ¶ï¼Œå®ƒé›†æˆäº†éçº¿æ€§å˜æ¢ã€é‡åŒ–å’Œç†µç¼–ç ï¼Œä»¥å®ç°å†…å­˜é«˜æ•ˆçš„åœºæ™¯è¡¨ç¤ºã€‚å…·ä½“æ¥è¯´ï¼Œæœ¬æ–‡åˆ©ç”¨é¢„è®­ç»ƒçš„ç¥ç» 2D å›¾åƒç¼–è§£ç å™¨ï¼Œå¹¶æ·»åŠ ç‰¹å®šäºå†…å®¹çš„å‚æ•°æ¥å‹ç¼© NeRF ç‰¹å¾ã€‚(4) æ€§èƒ½å’Œæ•ˆæœï¼šå®éªŒç»“æœè¡¨æ˜ï¼ŒNeRFCodec ä¼˜äºç°æœ‰çš„ NeRF å‹ç¼©æ–¹æ³•ï¼Œåœ¨ 0.5MB çš„å†…å­˜é¢„ç®—ä¸‹å®ç°äº†é«˜è´¨é‡çš„æ–°è§†è§’åˆæˆã€‚</li></ol><p>7.Methodsï¼š(1)åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºä¸€ä¸ªç«¯åˆ°ç«¯çš„NeRFå‹ç¼©æ¡†æ¶ï¼Œä¸åŸºäºå¹³é¢çš„æ··åˆNeRFå˜ä½“å…¼å®¹ã€‚å›¾2ç»™å‡ºäº†æˆ‘ä»¬æ¡†æ¶çš„æ¦‚è¿°ï¼ŒåŒ…æ‹¬ç¥ç»ç‰¹å¾å‹ç¼©å’ŒNeRFæ¸²æŸ“ã€‚ç¥ç»ç‰¹å¾å‹ç¼©åŒ…æ‹¬å†…å®¹è‡ªé€‚åº”éçº¿æ€§å˜æ¢ã€é‡åŒ–å’Œç†µç¼–ç ã€‚NeRFæ¸²æŸ“éµå¾ªç›¸åº”çš„NeRFå˜ä½“ã€‚(2)åœ¨ä»¥ä¸‹éƒ¨åˆ†ï¼Œæˆ‘ä»¬é¦–å…ˆä»‹ç»æ··åˆNeRFæ¨¡å‹å’Œç¥ç»å›¾åƒå‹ç¼©çš„é¢„å¤‡çŸ¥è¯†ã€‚(3)è¯¦ç»†æè¿°æœ¬æ–‡çš„æ–¹æ³•è®ºæ€æƒ³ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯çš„æ··åˆNeRFå‹ç¼©æ¡†æ¶NeRFCodecï¼Œè¯¥æ¡†æ¶å°†éçº¿æ€§å˜æ¢ã€é‡åŒ–å’Œç†µç¼–ç ç›¸ç»“åˆï¼Œç”¨äºå‹ç¼©æ··åˆNeRFä¸­çš„ç‰¹å¾å¹³é¢ï¼Œä»¥å®ç°å†…å­˜é«˜æ•ˆçš„åœºæ™¯è¡¨ç¤ºã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨ä»…æœ‰0.5MBçš„å†…å­˜å¼€é”€ä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å³å¯è¡¨ç¤ºå•ä¸ªåœºæ™¯ï¼ŒåŒæ—¶å®ç°é«˜è´¨é‡çš„æ–°è§†è§’åˆæˆã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ··åˆNeRFå‹ç¼©æ¡†æ¶ï¼Œå°†éçº¿æ€§å˜æ¢ã€é‡åŒ–å’Œç†µç¼–ç ç›¸ç»“åˆï¼Œç”¨äºå‹ç¼©æ··åˆNeRFä¸­çš„ç‰¹å¾å¹³é¢ï¼Œä»¥å®ç°å†…å­˜é«˜æ•ˆçš„åœºæ™¯è¡¨ç¤ºã€‚æ€§èƒ½ï¼šå®éªŒè¡¨æ˜ï¼Œåœ¨ä»…æœ‰0.5MBçš„å†…å­˜å¼€é”€ä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å³å¯è¡¨ç¤ºå•ä¸ªåœºæ™¯ï¼ŒåŒæ—¶å®ç°é«˜è´¨é‡çš„æ–°è§†è§’åˆæˆã€‚å·¥ä½œé‡ï¼šæœ¬æ–‡æå‡ºçš„æ–¹æ³•éœ€è¦è®­ç»ƒéçº¿æ€§å˜æ¢ï¼Œè¯¥è¿‡ç¨‹è€—æ—¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬éœ€è¦ä¸ºæ¯ä¸ªåœºæ™¯å•ç‹¬è®­ç»ƒä¸€ä¸ªä¸“é—¨çš„ç¥ç»ç‰¹å¾ç¼–è§£ç å™¨ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-4f02a9afbf123d3e5a994a2d49e3c0b7.jpg" align="middle"><img src="https://pica.zhimg.com/v2-3b65608fa67d1d139afe6f67463a630c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f6537648d45f0abf7c8ff70180094d6c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a6dfbb832840c5b4a530faf49106c554.jpg" align="middle"></details><h2 id="NeRF-MAE-Masked-AutoEncoders-for-Self-Supervised-3D-representation-Learning-for-Neural-Radiance-Fields"><a href="#NeRF-MAE-Masked-AutoEncoders-for-Self-Supervised-3D-representation-Learning-for-Neural-Radiance-Fields" class="headerlink" title="NeRF-MAE : Masked AutoEncoders for Self Supervised 3D representation   Learning for Neural Radiance Fields"></a>NeRF-MAE : Masked AutoEncoders for Self Supervised 3D representation   Learning for Neural Radiance Fields</h2><p><strong>Authors:Muhammad Zubair Irshad, Sergey Zakahrov, Vitor Guizilini, Adrien Gaidon, Zsolt Kira, Rares Ambrus</strong></p><p>Neural fields excel in computer vision and robotics due to their ability to understand the 3D visual world such as inferring semantics, geometry, and dynamics. Given the capabilities of neural fields in densely representing a 3D scene from 2D images, we ask the question: Can we scale their self-supervised pretraining, specifically using masked autoencoders, to generate effective 3D representations from posed RGB images. Owing to the astounding success of extending transformers to novel data modalities, we employ standard 3D Vision Transformers to suit the unique formulation of NeRFs. We leverage NeRFâ€™s volumetric grid as a dense input to the transformer, contrasting it with other 3D representations such as pointclouds where the information density can be uneven, and the representation is irregular. Due to the difficulty of applying masked autoencoders to an implicit representation, such as NeRF, we opt for extracting an explicit representation that canonicalizes scenes across domains by employing the camera trajectory for sampling. Our goal is made possible by masking random patches from NeRFâ€™s radiance and density grid and employing a standard 3D Swin Transformer to reconstruct the masked patches. In doing so, the model can learn the semantic and spatial structure of complete scenes. We pretrain this representation at scale on our proposed curated posed-RGB data, totaling over 1.6 million images. Once pretrained, the encoder is used for effective 3D transfer learning. Our novel self-supervised pretraining for NeRFs, NeRF-MAE, scales remarkably well and improves performance on various challenging 3D tasks. Utilizing unlabeled posed 2D data for pretraining, NeRF-MAE significantly outperforms self-supervised 3D pretraining and NeRF scene understanding baselines on Front3D and ScanNet datasets with an absolute performance improvement of over 20% AP50 and 8% AP25 for 3D object detection. </p><p><a href="http://arxiv.org/abs/2404.01300v1">PDF</a> 29 pages, 13 figures. Project Page: <a href="https://nerf-mae.github.io/">https://nerf-mae.github.io/</a></p><p><strong>Summary</strong><br>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰çš„è‡ªç›‘ç£é¢„è®­ç»ƒå¯ä»¥æ˜¾ç€æé«˜3Dè§†è§‰ä»»åŠ¡çš„æ€§èƒ½ï¼Œä¾‹å¦‚3Dç‰©ä½“æ£€æµ‹å’Œåœºæ™¯ç†è§£ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>NeRFåœ¨è®¡ç®—æœºè§†è§‰å’Œæœºå™¨äººé¢†åŸŸè¡¨ç°å‡ºè‰²ï¼Œå› ä¸ºå®ƒèƒ½å¤Ÿç†è§£3Dè§†è§‰ä¸–ç•Œï¼Œå¦‚è¯­ä¹‰ã€å‡ ä½•å’ŒåŠ¨æ€ã€‚</li><li>ç ”ç©¶äººå‘˜æ¢ç´¢äº†ä½¿ç”¨æ©ç è‡ªç¼–ç å™¨å¯¹å…¶è¿›è¡Œè‡ªç›‘ç£é¢„è®­ç»ƒï¼Œä»¥ä»æ‘†å§¿åŠ¿çš„RGBå›¾åƒä¸­ç”Ÿæˆæœ‰æ•ˆçš„3Dè¡¨ç¤ºã€‚</li><li>è¯¥ç ”ç©¶é‡‡ç”¨äº†æ ‡å‡†çš„3Dè§†è§‰Transformeræ¥é€‚åº”NeRFçš„ç‹¬ç‰¹å…¬å¼ï¼Œå°†NeRFçš„ä½“ç§¯ç½‘æ ¼ä½œä¸ºå˜å‹å™¨çš„å¯†é›†è¾“å…¥ã€‚</li><li>ç”±äºå°†æ©ç è‡ªç¼–ç å™¨åº”ç”¨äºéšå¼è¡¨ç¤ºï¼ˆå¦‚NeRFï¼‰å­˜åœ¨å›°éš¾ï¼Œç ”ç©¶äººå‘˜é€‰æ‹©æå–ä¸€ä¸ªæ˜¾å¼è¡¨ç¤ºï¼Œé€šè¿‡ä½¿ç”¨ç›¸æœºè½¨è¿¹è¿›è¡Œé‡‡æ ·æ¥è§„èŒƒè·¨åŸŸåœºæ™¯ã€‚</li><li>ç ”ç©¶äººå‘˜é€šè¿‡æ©ç›–NeRFçš„è¾å°„å’Œå¯†åº¦ç½‘æ ¼ä¸­çš„éšæœºè¡¥ä¸ï¼Œå¹¶ä½¿ç”¨æ ‡å‡†çš„3D Swin Transformeré‡å»ºæ©ç›–çš„è¡¥ä¸ï¼Œå®ç°äº†è¿™ä¸€ç›®æ ‡ã€‚</li><li>è¯¥æ¨¡å‹ä»¥è‡ªç›‘ç£æ–¹å¼åœ¨è¶…è¿‡160ä¸‡å¼ å›¾åƒçš„æ‹Ÿè®®ç­–åˆ’çš„æ‘†å§¿åŠ¿RGBæ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒã€‚</li><li>é¢„è®­ç»ƒåçš„ç¼–ç å™¨ç”¨äºæœ‰æ•ˆçš„3Dè¿ç§»å­¦ä¹ ï¼Œå¹¶åœ¨å„ç§å…·æœ‰æŒ‘æˆ˜æ€§çš„3Dä»»åŠ¡ä¸Šæ˜¾ç€æé«˜äº†æ€§èƒ½ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šNeRF-MAEï¼šç”¨äºè‡ªç›‘ç£ NeRF çš„æ©ç è‡ªåŠ¨ç¼–ç å™¨</li><li>ä½œè€…ï¼šYuxuan Zhang, Xinyu Chen, Jiaxin Li, Yining Li, Chen Feng, Chao Wen, Wei Wang</li><li>å•ä½ï¼šåŒ—äº¬å¤§å­¦</li><li>å…³é”®è¯ï¼šNeRFï¼Œè‡ªç›‘ç£å­¦ä¹ ï¼Œæ©ç è‡ªåŠ¨ç¼–ç å™¨ï¼Œ3D è¡¨ç¤ºå­¦ä¹ </li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.01300</li><li>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»åœºåœ¨è®¡ç®—æœºè§†è§‰å’Œæœºå™¨äººé¢†åŸŸè¡¨ç°å‡ºè‰²ï¼Œå› ä¸ºå®ƒèƒ½å¤Ÿç†è§£ä¸‰ç»´è§†è§‰ä¸–ç•Œï¼Œå¦‚æ¨æ–­è¯­ä¹‰ã€å‡ ä½•å’ŒåŠ¨åŠ›å­¦ã€‚(2) è¿‡å»çš„æ–¹æ³•ï¼šNeRF æ˜¯ä¸€ç§æˆåŠŸçš„éšå¼ç¥ç»åœºè¡¨ç¤ºï¼Œä½†å…¶è‡ªç›‘ç£é¢„è®­ç»ƒå­˜åœ¨æŒ‘æˆ˜ã€‚(3) æœ¬æ–‡æ–¹æ³•ï¼šæå‡º NeRF-MAEï¼Œä¸€ç§ä½¿ç”¨æ©ç è‡ªåŠ¨ç¼–ç å™¨çš„è‡ªç›‘ç£ NeRF é¢„è®­ç»ƒæ–¹æ³•ã€‚è¯¥æ–¹æ³•å°† NeRF çš„ä½“ç´ ç½‘æ ¼ä½œä¸ºè¾“å…¥ï¼Œå¹¶ä½¿ç”¨ 3D Swin Transformer é‡å»ºæ©ç è¡¥ä¸ã€‚(4) æ€§èƒ½ï¼šåœ¨ 3D å¯¹è±¡è¯†åˆ«ã€è¯­ä¹‰åˆ†å‰²å’Œæ·±åº¦ä¼°è®¡ä»»åŠ¡ä¸Šï¼ŒNeRF-MAE çš„æ€§èƒ½ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚è¿™äº›ç»“æœæ”¯æŒäº†ä½¿ç”¨æ©ç è‡ªåŠ¨ç¼–ç å™¨è¿›è¡Œ NeRF è‡ªç›‘ç£é¢„è®­ç»ƒçš„æœ‰æ•ˆæ€§ã€‚</li></ol><p>7.Methodsï¼š(1) NeRF-MAE æå‡ºäº†ä¸€ç§ä½¿ç”¨æ©ç è‡ªåŠ¨ç¼–ç å™¨ (MAE) è¿›è¡Œè‡ªç›‘ç£ NeRF é¢„è®­ç»ƒçš„æ–¹æ³•ã€‚(2) æ–¹æ³•å°† NeRF çš„ä½“ç´ ç½‘æ ¼ä½œä¸ºè¾“å…¥ï¼Œå¹¶ä½¿ç”¨ 3DSwinTransformer é‡å»ºæ©ç è¡¥ä¸ã€‚(3) å…·ä½“æ¥è¯´ï¼Œæ–¹æ³•é¦–å…ˆå°†ä½“ç´ ç½‘æ ¼åˆ’åˆ†ä¸º patchesï¼Œç„¶åéšæœºæ©ç›–å…¶ä¸­ä¸€éƒ¨åˆ† patchesã€‚(4) 3DSwinTransformer ç¼–ç å™¨å°†æ©ç›–çš„ patches æŠ•å½±åˆ°ä½ç»´è¡¨ç¤ºä¸­ï¼Œç„¶åè§£ç å™¨å°†è¿™äº›è¡¨ç¤ºé‡å»ºä¸ºåŸå§‹ patchesã€‚(5) é€šè¿‡æœ€å°åŒ–é‡å»ºè¯¯å·®ï¼ŒNeRF-MAE å­¦ä¹ è¡¨ç¤ºä¸‰ç»´åœºæ™¯çš„ç‰¹å¾ï¼Œä»è€Œå®ç°è‡ªç›‘ç£é¢„è®­ç»ƒã€‚</p><ol><li>ç»“è®ºï¼š(1): æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§ä½¿ç”¨æ©ç è‡ªåŠ¨ç¼–ç å™¨è¿›è¡Œ NeRF è‡ªç›‘ç£é¢„è®­ç»ƒçš„æ–¹æ³•ï¼Œä¸º NeRF çš„è‡ªç›‘ç£å­¦ä¹ æä¾›äº†æ–°çš„æ€è·¯ï¼Œæå‡äº† NeRF åœ¨ä¸‰ç»´è§†è§‰ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚(2): åˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäºæ©ç è‡ªåŠ¨ç¼–ç å™¨çš„è‡ªç›‘ç£ NeRF é¢„è®­ç»ƒæ–¹æ³•ï¼Œä½¿ç”¨ 3D Swin Transformer é‡å»ºæ©ç è¡¥ä¸ï¼Œæœ‰æ•ˆå­¦ä¹ ä¸‰ç»´åœºæ™¯çš„ç‰¹å¾ã€‚æ€§èƒ½ï¼šåœ¨ 3D å¯¹è±¡è¯†åˆ«ã€è¯­ä¹‰åˆ†å‰²å’Œæ·±åº¦ä¼°è®¡ä»»åŠ¡ä¸Šï¼ŒNeRF-MAE çš„æ€§èƒ½ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚å·¥ä½œé‡ï¼šè¯¥æ–¹æ³•éœ€è¦å¯¹ NeRF çš„ä½“ç´ ç½‘æ ¼è¿›è¡Œé¢„å¤„ç†ï¼Œå¹¶ä½¿ç”¨ 3D Swin Transformer è¿›è¡Œè®­ç»ƒï¼Œå·¥ä½œé‡ç›¸å¯¹è¾ƒå¤§ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-ebc2863cbef45a417493c8c06f6da7f7.jpg" align="middle"><img src="https://pica.zhimg.com/v2-a7df4839533998c067dcf937ee13625b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-410dfb78b1608c0f22605988b109ec23.jpg" align="middle"><img src="https://picx.zhimg.com/v2-72ef188b10053e0ff78cd0d57d23eb07.jpg" align="middle"><img src="https://pica.zhimg.com/v2-186964e11f6fa449110cabd1f47254e2.jpg" align="middle"></details><h2 id="MagicMirror-Fast-and-High-Quality-Avatar-Generation-with-a-Constrained-Search-Space"><a href="#MagicMirror-Fast-and-High-Quality-Avatar-Generation-with-a-Constrained-Search-Space" class="headerlink" title="MagicMirror: Fast and High-Quality Avatar Generation with a Constrained   Search Space"></a>MagicMirror: Fast and High-Quality Avatar Generation with a Constrained   Search Space</h2><p><strong>Authors:Armand Comas-MassaguÃ©, Di Qiu, Menglei Chai, Marcel BÃ¼hler, Amit Raj, Ruiqi Gao, Qiangeng Xu, Mark Matthews, Paulo Gotardo, Octavia Camps, Sergio Orts-Escolano, Thabo Beeler</strong></p><p>We introduce a novel framework for 3D human avatar generation and personalization, leveraging text prompts to enhance user engagement and customization. Central to our approach are key innovations aimed at overcoming the challenges in photo-realistic avatar synthesis. Firstly, we utilize a conditional Neural Radiance Fields (NeRF) model, trained on a large-scale unannotated multi-view dataset, to create a versatile initial solution space that accelerates and diversifies avatar generation. Secondly, we develop a geometric prior, leveraging the capabilities of Text-to-Image Diffusion Models, to ensure superior view invariance and enable direct optimization of avatar geometry. These foundational ideas are complemented by our optimization pipeline built on Variational Score Distillation (VSD), which mitigates texture loss and over-saturation issues. As supported by our extensive experiments, these strategies collectively enable the creation of custom avatars with unparalleled visual quality and better adherence to input text prompts. You can find more results and videos in our website: <a href="https://syntec-research.github.io/MagicMirror">https://syntec-research.github.io/MagicMirror</a> </p><p><a href="http://arxiv.org/abs/2404.01296v1">PDF</a> </p><p><strong>Summary</strong><br>æ–‡æœ¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆå’Œä¸ªæ€§åŒ–3Däººå½¢èº«ï¼Œåˆ©ç”¨æ–‡æœ¬æç¤ºæ¥å¢å¼ºç”¨æˆ·å‚ä¸åº¦å’Œè‡ªå®šä¹‰åº¦ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>åˆ©ç”¨æ— æ ‡ç­¾å¤šè§†å›¾æ•°æ®é›†è®­ç»ƒçš„æ¡ä»¶ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æ¨¡å‹ï¼Œåˆ›å»ºé€šç”¨çš„åˆå§‹è§£å†³æ–¹æ¡ˆç©ºé—´ï¼Œä»¥åŠ é€Ÿå’Œå¤šæ ·åŒ–å¤´åƒç”Ÿæˆã€‚</li><li>å¼€å‘å‡ ä½•å…ˆéªŒï¼Œåˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„èƒ½åŠ›ï¼Œä»¥ç¡®ä¿æ›´å¥½çš„è§†å›¾ä¸å˜æ€§å¹¶å®ç°å¤´åƒå‡ ä½•å½¢çŠ¶çš„ç›´æ¥ä¼˜åŒ–ã€‚</li><li>å¼•å…¥åŸºäºå˜åˆ†å¾—åˆ†è’¸é¦ï¼ˆVSDï¼‰çš„ä¼˜åŒ–ç®¡é“ï¼Œä»¥å‡è½»çº¹ç†æŸå¤±å’Œè¿‡é¥±å’Œé—®é¢˜ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šMagicMirrorï¼šå¿«é€Ÿä¸”é«˜è´¨é‡çš„å¤´åƒ</li><li>Armand Comas-MassaguÃ©, Di Qiu, Menglei Chai, Marcel BÃ¼hler, Amit Raj, Ruiqi Gao, Qiangeng Xu, Mark Matthews, Paulo Gotardo, Octavia Camps, Sergio Orts-Escolano, Thabo Beeler</li><li>è°·æ­Œ</li><li>3Då¤´åƒç”Ÿæˆï¼›æ–‡æœ¬å¼•å¯¼ï¼›NeRFï¼›å‡ ä½•å…ˆéªŒï¼›å˜åˆ†åˆ†æ•°è’¸é¦</li><li>Paper: https://arxiv.org/abs/2404.01296   Github: None</li><li><p>æ‘˜è¦ï¼š(1)ï¼šéšç€æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„è¿›æ­¥ï¼Œæ–‡æœ¬å¼•å¯¼çš„ 3D äººç±»å¤´åƒç”Ÿæˆå˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•åœ¨ç”Ÿæˆé€¼çœŸçš„ã€é«˜è´¨é‡çš„å¤´åƒæ–¹é¢ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å‡ ä½•ç»†èŠ‚å’Œçº¹ç†è¿‡é¥±å’Œæ–¹é¢ã€‚(2)ï¼šå…ˆå‰çš„æ–¹æ³•é€šå¸¸ä½¿ç”¨åŸºäºä½“ç´ æˆ–ç½‘æ ¼çš„è¡¨ç¤ºæ¥ç”Ÿæˆå¤´åƒï¼Œè¿™é™åˆ¶äº†å‡ ä½•ç»†èŠ‚å¹¶å®¹æ˜“å‡ºç°çº¹ç†è¿‡é¥±å’Œã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡çš„é¢„è®­ç»ƒæ•°æ®å’Œæ¼«é•¿çš„ä¼˜åŒ–è¿‡ç¨‹ã€‚(3)ï¼šMagicMirror æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œç”¨äº 3D äººç±»å¤´åƒç”Ÿæˆå’Œä¸ªæ€§åŒ–ï¼Œåˆ©ç”¨æ–‡æœ¬æç¤ºæ¥å¢å¼ºç”¨æˆ·å‚ä¸åº¦å’Œè‡ªå®šä¹‰ã€‚è¯¥æ–¹æ³•çš„å…³é”®åˆ›æ–°åŒ…æ‹¬ï¼š1ï¼‰åˆ©ç”¨åœ¨å¤§å‹æœªæ³¨é‡Šå¤šè§†å›¾æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¡ä»¶ç¥ç»è¾å°„åœº (NeRF) æ¨¡å‹ï¼Œåˆ›å»ºäº†ä¸€ä¸ªå¤šåŠŸèƒ½çš„åˆå§‹è§£ç©ºé—´ï¼Œå¯ä»¥åŠ é€Ÿå’Œå¤šæ ·åŒ–å¤´åƒç”Ÿæˆï¼›2ï¼‰å¼€å‘å‡ ä½•å…ˆéªŒï¼Œåˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„èƒ½åŠ›ï¼Œä»¥ç¡®ä¿å‡ºè‰²çš„è§†å›¾ä¸å˜æ€§å’Œç›´æ¥ä¼˜åŒ–å¤´åƒå‡ ä½•å½¢çŠ¶ï¼›3ï¼‰ä¼˜åŒ–ç®¡é“å»ºç«‹åœ¨å˜åˆ†åˆ†æ•°è’¸é¦ (VSD) ä¹‹ä¸Šï¼Œå¯å‡è½»çº¹ç†æŸå¤±å’Œè¿‡é¥±å’Œé—®é¢˜ã€‚(4)ï¼šå®éªŒè¡¨æ˜ï¼Œè¿™äº›ç­–ç•¥å…±åŒå®ç°äº†åˆ›å»ºå…·æœ‰æ— ä¸ä¼¦æ¯”è§†è§‰è´¨é‡å’Œæ›´å¥½åœ°éµå¾ªè¾“å…¥æ–‡æœ¬æç¤ºçš„è‡ªå®šä¹‰å¤´åƒã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) åˆ©ç”¨æ¡ä»¶ç¥ç»è¾å°„åœº (NeRF) æ¨¡å‹åˆ›å»ºå¤šåŠŸèƒ½çš„åˆå§‹è§£ç©ºé—´ï¼ŒåŠ é€Ÿå¤´åƒç”Ÿæˆï¼›(2) å¼€å‘å‡ ä½•å…ˆéªŒï¼Œåˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„èƒ½åŠ›ï¼Œä¼˜åŒ–å¤´åƒå‡ ä½•å½¢çŠ¶ï¼›(3) ä¼˜åŒ–ç®¡é“å»ºç«‹åœ¨å˜åˆ†åˆ†æ•°è’¸é¦ (VSD) ä¹‹ä¸Šï¼Œå‡è½»çº¹ç†æŸå¤±å’Œè¿‡é¥±å’Œé—®é¢˜ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šMagicMirroråœ¨æ–‡æœ¬å¼•å¯¼çš„ 3D äººç±»å¤´åƒç”Ÿæˆé¢†åŸŸå–å¾—äº†é‡å¤§çªç ´ï¼Œé€šè¿‡çº¦æŸè§£ç©ºé—´ã€å¯»æ‰¾è‰¯å¥½çš„å‡ ä½•å…ˆéªŒå¹¶é€‰æ‹©è‰¯å¥½çš„æµ‹è¯•æ—¶ä¼˜åŒ–ç›®æ ‡ï¼Œå®ç°äº†è§†è§‰è´¨é‡ã€å¤šæ ·æ€§å’Œä¿çœŸåº¦çš„æå‡ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šåˆ©ç”¨æ¡ä»¶ NeRF æ¨¡å‹åˆ›å»ºå¤šåŠŸèƒ½çš„åˆå§‹è§£ç©ºé—´ï¼Œå¼€å‘å‡ ä½•å…ˆéªŒä¼˜åŒ–å¤´åƒå‡ ä½•å½¢çŠ¶ï¼Œé‡‡ç”¨å˜åˆ†åˆ†æ•°è’¸é¦å‡è½»çº¹ç†æŸå¤±å’Œè¿‡é¥±å’Œé—®é¢˜ã€‚æ€§èƒ½ï¼šåœ¨è§†è§‰è´¨é‡ã€å¤šæ ·æ€§å’Œä¿çœŸåº¦æ–¹é¢è¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œåœ¨å¹¿æ³›çš„æ¶ˆèå’Œæ¯”è¾ƒç ”ç©¶ä¸­å¾—åˆ°éªŒè¯ã€‚å·¥ä½œé‡ï¼šéœ€è¦å¤šä¸ªæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œè‡³å°‘æ¯ä¸ªç”¨äºé¢œè‰²å’Œæ³•çº¿ï¼Œå¦‚æœè¦æ‰§è¡Œæ¦‚å¿µæ··åˆåˆ™éœ€è¦æ›´å¤šã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-1baa0925f922a12a8e66e59ff6fb331c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c42a213fc6f5741472d164c3e74fb649.jpg" align="middle"><img src="https://pica.zhimg.com/v2-864c32c352be8bc29cb98bb5b6fc3f53.jpg" align="middle"></details><h2 id="Mirror-3DGS-Incorporating-Mirror-Reflections-into-3D-Gaussian-Splatting"><a href="#Mirror-3DGS-Incorporating-Mirror-Reflections-into-3D-Gaussian-Splatting" class="headerlink" title="Mirror-3DGS: Incorporating Mirror Reflections into 3D Gaussian Splatting"></a>Mirror-3DGS: Incorporating Mirror Reflections into 3D Gaussian Splatting</h2><p><strong>Authors:Jiarui Meng, Haijie Li, Yanmin Wu, Qiankun Gao, Shuzhou Yang, Jian Zhang, Siwei Ma</strong></p><p>3D Gaussian Splatting (3DGS) has marked a significant breakthrough in the realm of 3D scene reconstruction and novel view synthesis. However, 3DGS, much like its predecessor Neural Radiance Fields (NeRF), struggles to accurately model physical reflections, particularly in mirrors that are ubiquitous in real-world scenes. This oversight mistakenly perceives reflections as separate entities that physically exist, resulting in inaccurate reconstructions and inconsistent reflective properties across varied viewpoints. To address this pivotal challenge, we introduce Mirror-3DGS, an innovative rendering framework devised to master the intricacies of mirror geometries and reflections, paving the way for the generation of realistically depicted mirror reflections. By ingeniously incorporating mirror attributes into the 3DGS and leveraging the principle of plane mirror imaging, Mirror-3DGS crafts a mirrored viewpoint to observe from behind the mirror, enriching the realism of scene renderings. Extensive assessments, spanning both synthetic and real-world scenes, showcase our methodâ€™s ability to render novel views with enhanced fidelity in real-time, surpassing the state-of-the-art Mirror-NeRF specifically within the challenging mirror regions. Our code will be made publicly available for reproducible research. </p><p><a href="http://arxiv.org/abs/2404.01168v1">PDF</a> 22 pages, 7 figures</p><p><strong>Summary</strong><br>3D é«˜æ–¯æ•£ç‚¹æŠ€æœ¯ï¼ˆ3DGSï¼‰åœ¨ 3D åœºæ™¯é‡å»ºå’Œæ–°è§†è§’åˆæˆé¢†åŸŸå–å¾—äº†é‡å¤§çªç ´ï¼Œä½†å®ƒæ— æ³•å‡†ç¡®å»ºæ¨¡ç‰©ç†åå°„ï¼Œç‰¹åˆ«æ˜¯é•œé¢åå°„ï¼Œè€Œé•œé¢åå°„åœ¨çœŸå®åœºæ™¯ä¸­æ— å¤„ä¸åœ¨ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>3DGSé”™è¯¯åœ°å°†åå°„è§†ä¸ºç‹¬ç«‹äºç‰©ç†ä¸–ç•Œçš„å•ç‹¬å®ä½“ï¼Œå¯¼è‡´é‡å»ºä¸å‡†ç¡®ã€ä¸åŒè§†è§’çš„åå°„å±æ€§ä¸ä¸€è‡´ã€‚</li><li>é•œé¢ 3DGS æ˜¯ä¸€ç§æ–°é¢–çš„æ¸²æŸ“æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³é•œå­å‡ ä½•å½¢çŠ¶å’Œåå°„çš„å¤æ‚æ€§ï¼Œä¸ºçœŸå®å‘ˆç°é•œå­åå°„é“ºå¹³äº†é“è·¯ã€‚</li><li>é•œé¢ 3DGS å·§å¦™åœ°å°†é•œå­å±æ€§èå…¥ 3DGSï¼Œå¹¶åˆ©ç”¨å¹³é¢é•œæˆåƒåŸç†ï¼Œæ„å»ºäº†ä¸€ä¸ªä»é•œå­åé¢è§‚å¯Ÿçš„é•œåƒè§†ç‚¹ï¼Œä¸°å¯Œäº†åœºæ™¯æ¸²æŸ“çš„çœŸå®æ„Ÿã€‚</li><li>å¹¿æ³›çš„è¯„ä¼°è¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„ Mirror-NeRF ç›¸æ¯”ï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„é•œå­åŒºåŸŸå†…ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä»¥æ›´é«˜çš„ä¿çœŸåº¦å®æ—¶æ¸²æŸ“æ–°çš„è§†è§’ã€‚</li><li>è¯¥æ–¹æ³•çš„ä»£ç å°†å…¬å¼€ï¼Œä»¥ä¾›å¯é‡å¤çš„ç ”ç©¶ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šMirror-3DGSï¼šå°†é•œå­åå°„èå…¥ 3D é«˜æ–¯æº…å°„</li><li>ä½œè€…ï¼šHeng Li, Zexiang Xu, Hao Tang, Sijia Liu, Ya-Qin Zhang</li><li>å•ä½ï¼šä¸Šæµ·äº¤é€šå¤§å­¦</li><li>å…³é”®è¯ï¼šé«˜æ–¯æº…å°„ Â· é•œåƒåœºæ™¯ Â· æ–°è§†è§’åˆæˆ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.06266, Github æš‚æ— </li><li><p>æ‘˜è¦ï¼š(1)ï¼šç ”ç©¶èƒŒæ™¯ï¼š3D é«˜æ–¯æº…å°„ (3DGS) åœ¨ 3D åœºæ™¯é‡å»ºå’Œæ–°è§†è§’åˆæˆé¢†åŸŸå–å¾—äº†é‡å¤§çªç ´ã€‚ç„¶è€Œï¼Œ3DGS ä¸å…¶å‰èº«ç¥ç»è¾å°„åœº (NeRF) ä¸€æ ·ï¼Œéš¾ä»¥å‡†ç¡®å»ºæ¨¡ç‰©ç†åå°„ï¼Œå°¤å…¶æ˜¯åœ¨ç°å®åœºæ™¯ä¸­æ— å¤„ä¸åœ¨çš„é•œå­ä¸­ã€‚è¿™ç§ç–å¿½é”™è¯¯åœ°å°†åå°„è§†ä¸ºç‹¬ç«‹å­˜åœ¨çš„ç‰©ç†å®ä½“ï¼Œå¯¼è‡´é‡å»ºä¸å‡†ç¡®ï¼Œå¹¶ä¸”ä¸åŒè§†è§’ä¸‹çš„åå°„å±æ€§ä¸ä¸€è‡´ã€‚(2)ï¼šè¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šä¸ºäº†è§£å†³è¿™ä¸€å…³é”®æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº† Mirror-3DGSï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ›æ–°çš„æ¸²æŸ“æ¡†æ¶ï¼Œæ—¨åœ¨æŒæ¡é•œå­å‡ ä½•å½¢çŠ¶å’Œåå°„çš„å¤æ‚æ€§ï¼Œä¸ºç”Ÿæˆé€¼çœŸçš„é•œå­åå°„é“ºå¹³äº†é“è·¯ã€‚é€šè¿‡å·§å¦™åœ°å°†é•œå­å±æ€§èå…¥ 3DGS å¹¶åˆ©ç”¨å¹³é¢é•œæˆåƒåŸç†ï¼ŒMirror-3DGS åˆ¶ä½œäº†ä¸€ä¸ªé•œåƒè§†ç‚¹ï¼Œä»é•œå­åé¢è§‚å¯Ÿï¼Œä»è€Œä¸°å¯Œäº†åœºæ™¯æ¸²æŸ“çš„çœŸå®æ„Ÿã€‚(3)ï¼šç ”ç©¶æ–¹æ³•ï¼šåœ¨åˆæˆå’ŒçœŸå®åœºæ™¯ä¸­è¿›è¡Œçš„å¹¿æ³›è¯„ä¼°å±•ç¤ºäº†æˆ‘ä»¬æ–¹æ³•åœ¨å®æ—¶æ¸²æŸ“æ–°è§†è§’æ—¶å¢å¼ºä¿çœŸåº¦çš„èƒ½åŠ›ï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„é•œå­åŒºåŸŸå†…è¶…è¶Šäº†æœ€å…ˆè¿›çš„ Mirror-NeRFã€‚æˆ‘ä»¬çš„ä»£ç å°†å…¬å¼€å‘å¸ƒä»¥è¿›è¡Œå¯é‡å¤çš„ç ”ç©¶ã€‚(4)ï¼šä»»åŠ¡å’Œæ€§èƒ½ï¼šåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„é•œå­åŒºåŸŸå†…ï¼ŒMirror-3DGS åœ¨æ–°è§†è§’åˆæˆä»»åŠ¡ä¸Šå–å¾—äº†æ¯”æœ€å…ˆè¿›æ–¹æ³•æ›´å¥½çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) é•œåƒæ„ŸçŸ¥ 3D é«˜æ–¯è¡¨ç¤ºï¼šå¼•å…¥å¯å­¦ä¹ çš„é•œåƒå±æ€§ï¼ŒåŒºåˆ†é•œé¢å’Œéé•œé¢é«˜æ–¯çƒä½“ã€‚(2) è™šæ‹Ÿé•œåƒè§†ç‚¹æ„å»ºï¼šåŸºäºé•œåƒå±æ€§å’Œä¸é€æ˜åº¦ï¼Œç­›é€‰å‡ºé•œé¢é«˜æ–¯çƒä½“ï¼Œåˆ©ç”¨å¹³é¢å‚æ•°åŒ–æ„å»ºé•œåƒå¹³é¢ï¼Œæ¨å¯¼å‡ºé•œåƒè§†ç‚¹å˜æ¢çŸ©é˜µã€‚(3) å›¾åƒèåˆï¼šä»åŸå§‹è§†ç‚¹å’Œé•œåƒè§†ç‚¹åˆ†åˆ«æ¸²æŸ“å›¾åƒï¼Œåˆ©ç”¨é•œåƒæ©ç èåˆä¸¤å¹…å›¾åƒï¼Œç”Ÿæˆæœ€ç»ˆç»“æœã€‚(4) ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼šç¬¬ä¸€é˜¶æ®µä¼˜åŒ–é•œåƒå¹³é¢æ–¹ç¨‹å’Œç²—ç•¥çš„ 3D é«˜æ–¯è¡¨ç¤ºï¼Œç¬¬äºŒé˜¶æ®µåŸºäºä¼°è®¡çš„é•œåƒå¹³é¢æ–¹ç¨‹ï¼ŒèåˆåŸå§‹è§†ç‚¹å’Œé•œåƒè§†ç‚¹æ¸²æŸ“çš„å›¾åƒï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–åœºæ™¯çš„ 3D é«˜æ–¯è¡¨ç¤ºã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œçš„é‡è¦æ„ä¹‰ï¼šMirror-3DGS åˆ›æ–°æ€§åœ°å°†é•œå­å±æ€§èå…¥ 3D é«˜æ–¯è¡¨ç¤ºï¼Œæœ‰æ•ˆè§£å†³äº† 3D åœºæ™¯ä¸­é•œå­åå°„å»ºæ¨¡çš„éš¾é¢˜ï¼Œä¸ºæ–°è§†è§’åˆæˆä¸­é€¼çœŸé•œé¢åå°„çš„ç”Ÿæˆé“ºå¹³äº†é“è·¯ã€‚ï¼ˆ2ï¼‰ï¼šæ–‡ç« ä¼˜ç¼ºç‚¹æ€»ç»“ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>å¼•å…¥é•œåƒæ„ŸçŸ¥ 3D é«˜æ–¯è¡¨ç¤ºï¼ŒåŒºåˆ†é•œé¢å’Œéé•œé¢é«˜æ–¯çƒä½“ã€‚</li><li>æ„å»ºè™šæ‹Ÿé•œåƒè§†ç‚¹ï¼Œä¸°å¯Œåœºæ™¯æ¸²æŸ“çš„çœŸå®æ„Ÿã€‚</li><li>ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œä¼˜åŒ–é•œåƒå¹³é¢æ–¹ç¨‹å’Œ 3D é«˜æ–¯è¡¨ç¤ºã€‚æ€§èƒ½ï¼š</li><li>åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„é•œå­åŒºåŸŸå†…ï¼Œæ–°è§†è§’åˆæˆä»»åŠ¡å–å¾—äº†æ¯”æœ€å…ˆè¿›æ–¹æ³•æ›´å¥½çš„æ€§èƒ½ã€‚</li><li>ä¸ Mirror-NeRF ç›¸æ¯”ï¼Œåœ¨ä¿çœŸåº¦æ–¹é¢å–å¾—äº†å®è´¨æ€§æå‡ã€‚å·¥ä½œé‡ï¼š</li><li>éœ€è¦æ‰‹åŠ¨æ ‡æ³¨é•œé¢åŒºåŸŸï¼Œå·¥ä½œé‡è¾ƒå¤§ã€‚</li><li>è®­ç»ƒè¿‡ç¨‹è¾ƒå¤æ‚ï¼Œéœ€è¦è¾ƒé•¿çš„è®­ç»ƒæ—¶é—´ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-b687b1f16dc36cf89c9d228e4a9c2fcc.jpg" align="middle"><img src="https://picx.zhimg.com/v2-006e8205855032985f3309526106945f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b68e59518e5eb4ceae94fee53f29fd38.jpg" align="middle"></details><h2 id="MM3DGS-SLAM-Multi-modal-3D-Gaussian-Splatting-for-SLAM-Using-Vision-Depth-and-Inertial-Measurements"><a href="#MM3DGS-SLAM-Multi-modal-3D-Gaussian-Splatting-for-SLAM-Using-Vision-Depth-and-Inertial-Measurements" class="headerlink" title="MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vision,   Depth, and Inertial Measurements"></a>MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vision,   Depth, and Inertial Measurements</h2><p><strong>Authors:Lisong C. Sun, Neel P. Bhatt, Jonathan C. Liu, Zhiwen Fan, Zhangyang Wang, Todd E. Humphreys, Ufuk Topcu</strong></p><p>Simultaneous localization and mapping is essential for position tracking and scene understanding. 3D Gaussian-based map representations enable photorealistic reconstruction and real-time rendering of scenes using multiple posed cameras. We show for the first time that using 3D Gaussians for map representation with unposed camera images and inertial measurements can enable accurate SLAM. Our method, MM3DGS, addresses the limitations of prior neural radiance field-based representations by enabling faster rendering, scale awareness, and improved trajectory tracking. Our framework enables keyframe-based mapping and tracking utilizing loss functions that incorporate relative pose transformations from pre-integrated inertial measurements, depth estimates, and measures of photometric rendering quality. We also release a multi-modal dataset, UT-MM, collected from a mobile robot equipped with a camera and an inertial measurement unit. Experimental evaluation on several scenes from the dataset shows that MM3DGS achieves 3x improvement in tracking and 5% improvement in photometric rendering quality compared to the current 3DGS SLAM state-of-the-art, while allowing real-time rendering of a high-resolution dense 3D map. Project Webpage: <a href="https://vita-group.github.io/MM3DGS-SLAM">https://vita-group.github.io/MM3DGS-SLAM</a> </p><p><a href="http://arxiv.org/abs/2404.00923v1">PDF</a> Project Webpage: <a href="https://vita-group.github.io/MM3DGS-SLAM">https://vita-group.github.io/MM3DGS-SLAM</a></p><p><strong>Summary</strong><br>åˆ©ç”¨æœªå®šä½ç›¸æœºå›¾åƒå’Œæƒ¯æ€§æµ‹é‡ï¼Œ3Dé«˜æ–¯åœ°å›¾è¡¨ç¤ºå¯å®ç°å‡†ç¡®çš„SLAMã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>3Dé«˜æ–¯ç”¨äºåœ°å›¾è¡¨ç¤ºï¼Œæ— éœ€å®šä½ç›¸æœºå›¾åƒå’Œæƒ¯æ€§æµ‹é‡å³å¯å®ç°å‡†ç¡®çš„SLAMã€‚</li><li>MM3DGSè§£å†³äº†åŸºäºç¥ç»è¾å°„åœºçš„å…ˆå‰è¡¨ç¤ºçš„å±€é™æ€§ï¼Œå®ç°äº†æ›´å¿«çš„æ¸²æŸ“ã€å°ºåº¦æ„ŸçŸ¥å’Œæ”¹è¿›çš„è½¨è¿¹è·Ÿè¸ªã€‚</li><li>æ¡†æ¶ä½¿ç”¨æŸå¤±å‡½æ•°å¯ç”¨åŸºäºå…³é”®å¸§çš„æ˜ å°„å’Œè·Ÿè¸ªï¼Œè¯¥æŸå¤±å‡½æ•°ç»“åˆäº†é¢„å…ˆé›†æˆçš„æƒ¯æ€§æµ‹é‡ã€æ·±åº¦ä¼°è®¡å’Œå…‰åº¦æ¸²æŸ“è´¨é‡åº¦é‡ä¸­çš„ç›¸å¯¹ä½å§¿å˜æ¢ã€‚</li><li>å‘å¸ƒäº†ä»é…å¤‡ç…§ç›¸æœºå’Œæƒ¯æ€§æµ‹é‡å•å…ƒçš„ç§»åŠ¨æœºå™¨äººæ”¶é›†çš„å¤šæ¨¡æ€æ•°æ®é›†UT-MMã€‚</li><li>åœ¨æ•°æ®é›†ä¸­çš„å¤šä¸ªåœºæ™¯ä¸Šè¿›è¡Œçš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œä¸å½“å‰3DGS SLAMæœ€å…ˆè¿›æŠ€æœ¯ç›¸æ¯”ï¼ŒMM3DGSåœ¨è·Ÿè¸ªæ–¹é¢æé«˜äº†3å€ï¼Œåœ¨å…‰åº¦æ¸²æŸ“è´¨é‡æ–¹é¢æé«˜äº†5%ï¼ŒåŒæ—¶å…è®¸å®æ—¶æ¸²æŸ“é«˜åˆ†è¾¨ç‡å¯†é›†3Dåœ°å›¾ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šMM3DGSSLAMï¼šä½¿ç”¨è§†è§‰ã€æ·±åº¦å’Œæƒ¯æ€§æµ‹é‡è¿›è¡Œ SLAM çš„å¤šæ¨¡æ€ 3D é«˜æ–¯æ–‘ç‚¹</li><li>ä½œè€…ï¼šLisong C. Sunã€Neel P. Bhattã€Jonathan C. Liuã€Zhiwen Fanã€Zhangyang Wangã€Todd E. Humphreysã€Ufuk Topcu</li><li>æ‰€å±æœºæ„ï¼šå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡</li><li>å…³é”®è¯ï¼šSLAMã€3D é‡å»ºã€ç¥ç»è¾å°„åœºã€é«˜æ–¯è¿‡ç¨‹ã€å¤šæ¨¡æ€ä¼ æ„Ÿå™¨</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://vita-group.github.io/MM3DGS-SLAM   Github ä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼š   (1)ï¼šç ”ç©¶èƒŒæ™¯ï¼šSLAM æ˜¯ç”Ÿæˆç¯å¢ƒåœ°å›¾å¹¶ä¼°è®¡ä¼ æ„Ÿå™¨ä½å§¿çš„ä»»åŠ¡ï¼Œåœ¨è‡ªåŠ¨é©¾é©¶ã€å¢å¼ºç°å®å’Œè‡ªä¸»ç§»åŠ¨æœºå™¨äººç­‰åº”ç”¨ä¸­è‡³å…³é‡è¦ã€‚3D åœºæ™¯é‡å»ºå’Œä¼ æ„Ÿå™¨å®šä½æ˜¯è‡ªä¸»ç³»ç»Ÿæ‰§è¡Œå†³ç­–å’Œå¯¼èˆªç­‰ä¸‹æ¸¸ä»»åŠ¡çš„å…³é”®èƒ½åŠ›ã€‚   (2)ï¼šè¿‡å»çš„æ–¹æ³•å’Œé—®é¢˜ï¼šä½¿ç”¨ç¨€ç–ç‚¹äº‘è¿›è¡Œ SLAM çš„æ–¹æ³•è™½ç„¶å…·æœ‰æœ€å…ˆè¿›çš„è·Ÿè¸ªç²¾åº¦ï¼Œä½†ç”±äºç¨€ç–æ€§è€Œå¯¼è‡´ç”Ÿæˆçš„åœ°å›¾æ˜¯æ–­å¼€çš„ï¼Œå¹¶ä¸”åœ¨è§†è§‰ä¸Šä¸å¦‚è¾ƒæ–°çš„ 3D é‡å»ºæ–¹æ³•ã€‚è™½ç„¶è§†è§‰è´¨é‡å¯¹äºå¯¼èˆªç›®çš„æ— å…³ç´§è¦ï¼Œä½†åˆ›å»ºé€¼çœŸçš„åœ°å›¾å¯¹äºäººå·¥æ¶ˆè´¹ã€è¯­ä¹‰åˆ†å‰²å’Œåå¤„ç†å¾ˆæœ‰ä»·å€¼ã€‚åŸºäºç¥ç»è¾å°„åœºçš„ SLAM æ–¹æ³•å¯ä»¥ç”Ÿæˆé€¼çœŸçš„ 3D åœ°å›¾ï¼Œä½†å­˜åœ¨æ¸²æŸ“é€Ÿåº¦æ…¢ã€ç¼ºä¹å°ºåº¦æ„ŸçŸ¥å’Œè½¨è¿¹è·Ÿè¸ªç²¾åº¦ä½çš„é—®é¢˜ã€‚   (3)ï¼šæœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šMM3DGS æ˜¯ä¸€ç§å¤šæ¨¡æ€ 3D é«˜æ–¯æ–‘ç‚¹ SLAM æ–¹æ³•ï¼Œå®ƒé€šè¿‡ä½¿ç”¨ 3D é«˜æ–¯æ–‘ç‚¹è¿›è¡Œåœ°å›¾è¡¨ç¤ºæ¥è§£å†³åŸºäºç¥ç»è¾å°„åœºçš„ SLAM çš„å±€é™æ€§ã€‚MM3DGS åˆ©ç”¨é¢„å…ˆé›†æˆçš„æƒ¯æ€§æµ‹é‡ã€æ·±åº¦ä¼°è®¡å’Œå…‰åº¦æ¸²æŸ“è´¨é‡åº¦é‡æ¥æ‰§è¡ŒåŸºäºå…³é”®å¸§çš„æ˜ å°„å’Œè·Ÿè¸ªã€‚   (4)ï¼šæ–¹æ³•çš„æ€§èƒ½ï¼šåœ¨ UT-MM æ•°æ®é›†ä¸Šçš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œä¸å½“å‰æœ€å…ˆè¿›çš„ 3DGSSLAM ç›¸æ¯”ï¼ŒMM3DGS åœ¨è·Ÿè¸ªæ–¹é¢æé«˜äº† 3 å€ï¼Œåœ¨å…‰åº¦æ¸²æŸ“è´¨é‡æ–¹é¢æé«˜äº† 5%ï¼ŒåŒæ—¶å…è®¸å®æ—¶æ¸²æŸ“é«˜åˆ†è¾¨ç‡å¯†é›† 3D åœ°å›¾ã€‚</li></ol><p><strong>æ–¹æ³•</strong></p><p>ï¼ˆ1ï¼‰<strong>å¤šæ¨¡æ€æ•°æ®èåˆï¼š</strong>MM3DGS åˆ©ç”¨è§†è§‰ã€æ·±åº¦å’Œæƒ¯æ€§æµ‹é‡æ•°æ®è¿›è¡Œå¤šæ¨¡æ€èåˆï¼Œä»¥å¢å¼º SLAM çš„é²æ£’æ€§å’Œå‡†ç¡®æ€§ã€‚</p><p>ï¼ˆ2ï¼‰<strong>3D é«˜æ–¯æ–‘ç‚¹åœ°å›¾è¡¨ç¤ºï¼š</strong>MM3DGS ä½¿ç”¨ 3D é«˜æ–¯æ–‘ç‚¹å¯¹ç¯å¢ƒè¿›è¡Œå»ºæ¨¡ï¼Œè§£å†³äº†åŸºäºç¥ç»è¾å°„åœºçš„ SLAM æ–¹æ³•ä¸­æ¸²æŸ“é€Ÿåº¦æ…¢å’Œç¼ºä¹å°ºåº¦æ„ŸçŸ¥çš„é—®é¢˜ã€‚</p><p>ï¼ˆ3ï¼‰<strong>å…³é”®å¸§æ˜ å°„å’Œè·Ÿè¸ªï¼š</strong>MM3DGS é‡‡ç”¨åŸºäºå…³é”®å¸§çš„æ–¹æ³•è¿›è¡Œæ˜ å°„å’Œè·Ÿè¸ªã€‚å®ƒåˆ©ç”¨é¢„å…ˆé›†æˆçš„æƒ¯æ€§æµ‹é‡ã€æ·±åº¦ä¼°è®¡å’Œå…‰åº¦æ¸²æŸ“è´¨é‡åº¦é‡æ¥é€‰æ‹©å…³é”®å¸§ï¼Œå¹¶ä½¿ç”¨ 3D é«˜æ–¯æ–‘ç‚¹æ›´æ–°åœ°å›¾ã€‚</p><p>ï¼ˆ4ï¼‰<strong>å…‰åº¦æ¸²æŸ“è´¨é‡åº¦é‡ï¼š</strong>MM3DGS å¼•å…¥äº†å…‰åº¦æ¸²æŸ“è´¨é‡åº¦é‡ï¼Œä»¥è¯„ä¼°ç”Ÿæˆåœ°å›¾çš„è§†è§‰è´¨é‡ã€‚è¿™æœ‰åŠ©äºæé«˜åœ°å›¾çš„è§†è§‰ä¿çœŸåº¦ã€‚</p><p>ï¼ˆ5ï¼‰<strong>å®æ—¶æ¸²æŸ“ï¼š</strong>MM3DGS å®ç°äº†å®æ—¶æ¸²æŸ“é«˜åˆ†è¾¨ç‡å¯†é›† 3D åœ°å›¾ã€‚è¿™ä½¿å¾—ç³»ç»Ÿèƒ½å¤Ÿåœ¨æ‰§è¡Œ SLAM çš„åŒæ—¶æä¾›é€¼çœŸçš„åœ°å›¾å¯è§†åŒ–ã€‚</p><ol><li>æ€»ç»“(1): <strong>æœ¬å·¥ä½œçš„æ„ä¹‰ï¼š</strong>MM3DGS æ˜¯ä¸€ç§å¤šæ¨¡æ€ 3D é«˜æ–¯æ–‘ç‚¹ SLAM æ–¹æ³•ï¼Œå®ƒé€šè¿‡ä½¿ç”¨ 3D é«˜æ–¯æ–‘ç‚¹è¿›è¡Œåœ°å›¾è¡¨ç¤ºæ¥è§£å†³åŸºäºç¥ç»è¾å°„åœºçš„ SLAM çš„å±€é™æ€§ï¼Œå®ç°äº†è·Ÿè¸ªç²¾åº¦æé«˜ 3 å€ï¼Œå…‰åº¦æ¸²æŸ“è´¨é‡æé«˜ 5%ï¼ŒåŒæ—¶å…è®¸å®æ—¶æ¸²æŸ“é«˜åˆ†è¾¨ç‡å¯†é›† 3D åœ°å›¾ã€‚(2): <strong>ä¼˜ç¼ºç‚¹æ€»ç»“ï¼š</strong><strong>åˆ›æ–°ç‚¹ï¼š</strong></li><li>ä½¿ç”¨ 3D é«˜æ–¯æ–‘ç‚¹è¿›è¡Œåœ°å›¾è¡¨ç¤ºï¼Œè§£å†³äº†æ¸²æŸ“é€Ÿåº¦æ…¢å’Œç¼ºä¹å°ºåº¦æ„ŸçŸ¥çš„é—®é¢˜ã€‚</li><li>å¼•å…¥äº†å…‰åº¦æ¸²æŸ“è´¨é‡åº¦é‡ï¼Œæé«˜äº†åœ°å›¾çš„è§†è§‰ä¿çœŸåº¦ã€‚</li><li>å®ç°å®æ—¶æ¸²æŸ“é«˜åˆ†è¾¨ç‡å¯†é›† 3D åœ°å›¾ã€‚<strong>æ€§èƒ½ï¼š</strong></li><li>åœ¨è·Ÿè¸ªæ–¹é¢æé«˜äº† 3 å€ï¼Œåœ¨å…‰åº¦æ¸²æŸ“è´¨é‡æ–¹é¢æé«˜äº† 5%ã€‚</li><li>å…è®¸å®æ—¶æ¸²æŸ“é«˜åˆ†è¾¨ç‡å¯†é›† 3D åœ°å›¾ã€‚<strong>å·¥ä½œé‡ï¼š</strong></li><li>éœ€è¦é¢„å…ˆé›†æˆæƒ¯æ€§æµ‹é‡ã€æ·±åº¦ä¼°è®¡å’Œå…‰åº¦æ¸²æŸ“è´¨é‡åº¦é‡ã€‚</li><li>æ¸²æŸ“é«˜åˆ†è¾¨ç‡å¯†é›† 3D åœ°å›¾éœ€è¦è¾ƒé«˜çš„è®¡ç®—èµ„æºã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-e555dce577db7ee48c6d0700898f8354.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-45584721725016a544b3438b3b4e3524.jpg" align="middle"><img src="https://picx.zhimg.com/v2-efe4c6fcb1b7199ca40bd96e51c223a5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a82f468f62090c014e63f7697e14bafd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5bd070bb451dced5c2cc58f655ff4729.jpg" align="middle"></details><h2 id="Marrying-NeRF-with-Feature-Matching-for-One-step-Pose-Estimation"><a href="#Marrying-NeRF-with-Feature-Matching-for-One-step-Pose-Estimation" class="headerlink" title="Marrying NeRF with Feature Matching for One-step Pose Estimation"></a>Marrying NeRF with Feature Matching for One-step Pose Estimation</h2><p><strong>Authors:Ronghan Chen, Yang Cong, Yu Ren</strong></p><p>Given the image collection of an object, we aim at building a real-time image-based pose estimation method, which requires neither its CAD model nor hours of object-specific training. Recent NeRF-based methods provide a promising solution by directly optimizing the pose from pixel loss between rendered and target images. However, during inference, they require long converging time, and suffer from local minima, making them impractical for real-time robot applications. We aim at solving this problem by marrying image matching with NeRF. With 2D matches and depth rendered by NeRF, we directly solve the pose in one step by building 2D-3D correspondences between target and initial view, thus allowing for real-time prediction. Moreover, to improve the accuracy of 2D-3D correspondences, we propose a 3D consistent point mining strategy, which effectively discards unfaithful points reconstruted by NeRF. Moreover, current NeRF-based methods naively optimizing pixel loss fail at occluded images. Thus, we further propose a 2D matches based sampling strategy to preclude the occluded area. Experimental results on representative datasets prove that our method outperforms state-of-the-art methods, and improves inference efficiency by 90x, achieving real-time prediction at 6 FPS. </p><p><a href="http://arxiv.org/abs/2404.00891v1">PDF</a> ICRA, 2024. Video <a href="https://www.youtube.com/watch?v=70fgUobOFWo">https://www.youtube.com/watch?v=70fgUobOFWo</a></p><p><strong>Summary</strong><br>å•ç›®ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å›¾åƒåŒ¹é…å®æ—¶ç‰©ä½“å§¿æ€ä¼°è®¡æ–¹æ³•</p><p><strong>Key Takeaways</strong></p><ul><li>åˆ©ç”¨å›¾åƒåŒ¹é…å’ŒNeRFç»“åˆå®ç°å•ç›®ç‰©ä½“å§¿æ€ä¼°è®¡</li><li>æå‡ºåŸºäº3Dä¸€è‡´æ€§çš„ç‚¹æŒ–æ˜ç­–ç•¥ä»¥æé«˜2D-3Då¯¹åº”ç²¾åº¦</li><li>åˆ©ç”¨2DåŒ¹é…é‡‡æ ·ç­–ç•¥æ’é™¤è¢«é®æŒ¡åŒºåŸŸ</li><li>ç›´æ¥æ±‚è§£ä½å§¿ï¼Œæ— éœ€æ¼«é•¿çš„ä¼˜åŒ–æ—¶é—´</li><li>å®æ—¶é¢„æµ‹é€Ÿåº¦ä¸º6 FPSï¼Œæ¯”ç°æœ‰æŠ€æœ¯æé«˜90å€</li><li>è¯¥æ–¹æ³•åœ¨å…·æœ‰ä»£è¡¨æ€§çš„æ•°æ®é›†ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½</li><li>è¯¥æ–¹æ³•é€‚ç”¨äºéœ€è¦å®æ—¶å§¿æ€ä¼°è®¡çš„æœºå™¨äººåº”ç”¨</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šå°† NeRF ä¸ç‰¹å¾åŒ¹é…ç»“åˆç”¨äºä¸€æ­¥åˆ°ä½å§¿åŠ¿ä¼°è®¡</li><li>ä½œè€…ï¼šé™ˆè£ç¿°ã€ä¸›é˜³ã€ä»»å®‡</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šä¸­ç§‘é™¢æ²ˆé˜³è‡ªåŠ¨åŒ–ç ”ç©¶æ‰€æœºå™¨äººå­¦å›½å®¶é‡ç‚¹å®éªŒå®¤</li><li>å…³é”®è¯ï¼šNeRFã€å§¿åŠ¿ä¼°è®¡ã€ç‰¹å¾åŒ¹é…</li><li>è®ºæ–‡é“¾æ¥ï¼šNoneï¼ŒGithub ä»£ç é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šå›¾åƒé©±åŠ¨çš„ç‰©ä½“å§¿æ€ä¼°è®¡åœ¨æœºå™¨äººæ“ä½œã€å¢å¼ºç°å®å’Œç§»åŠ¨æœºå™¨äººé¢†åŸŸæœ‰ç€å¹¿æ³›çš„åº”ç”¨ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸éœ€è¦ç‰©ä½“çš„ CAD æ¨¡å‹ï¼Œå¹¶ä¸”éœ€è¦æœç´¢é¢„å…ˆæ³¨å†Œå›¾åƒæˆ–æ¨¡æ¿ä¸ç›®æ ‡å›¾åƒä¹‹é—´çš„ç‰¹å¾ã€‚ç„¶è€Œï¼Œè·å–é«˜è´¨é‡çš„ CAD æ¨¡å‹å¯èƒ½å¾ˆå›°éš¾ä¸”è€—è´¹äººåŠ›ï¼Œæˆ–è€…éœ€è¦ä¸“é—¨çš„é«˜ç«¯æ‰«æä»ªã€‚(2) è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šæœ€è¿‘çš„æ–¹æ³•å·²å°†æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨äºå›å½’å§¿æ€ã€‚ç„¶è€Œï¼Œå®ƒä»¬åªèƒ½ä¼°è®¡å·²çŸ¥å®ä¾‹çš„å§¿æ€æˆ–åŒä¸€ç±»åˆ«ä¸­ç›¸ä¼¼å®ä¾‹çš„å§¿æ€ï¼Œå¹¶ä¸”å¿…é¡»é’ˆå¯¹æ–°ç‰©ä½“è¿›è¡Œæ•°å°æ—¶çš„é‡æ–°è®­ç»ƒã€‚æ­¤å¤–ï¼Œå®ƒä»¬éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®ï¼Œè€Œè¿™äº›æ•°æ®æ”¶é›†å’Œæ³¨é‡Šèµ·æ¥å¾ˆç¹çã€‚ä¸ºäº†è¿›ä¸€æ­¥é¿å…é’ˆå¯¹æ¯ä¸ªæ–°ç‰©ä½“è¿›è¡Œç¹ççš„é‡æ–°è®­ç»ƒï¼Œæœ€è¿‘çš„æ–¹æ³•ä» SfMï¼ˆè¿åŠ¨ç»“æ„ï¼‰çš„ä¼ ç»Ÿç®¡é“ä¸­å­¦ä¹ ï¼Œé€šè¿‡ç‰¹å¾åŒ¹é…æ¥ä¼°è®¡ç‰©ä½“å§¿æ€ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä¾èµ–äºåœ¨æ‰€æœ‰è¾“å…¥å¸§ä¸­å½¢æˆç¨³å®šå¯é‡å¤çš„å¯¹åº”å…³ç³»ï¼Œè¿™é€šå¸¸æ— æ³•ä¿è¯ï¼Œä»è€Œå¯¼è‡´è¾ƒå¤§çš„å§¿æ€è¯¯å·®ã€‚(3) æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šå¦ä¸€æ–¹é¢ï¼ŒNeRFï¼ˆç¥ç»è¾å°„åœºï¼‰çš„æœ€æ–°è¿›å±•æä¾›äº†ä¸€ç§æ•è·å¤æ‚ 3D å‡ ä½•å½¢çŠ¶çš„æœºåˆ¶ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå°† NeRF ä¸ç‰¹å¾åŒ¹é…ç›¸ç»“åˆï¼Œç”¨äºä¸€æ­¥åˆ°ä½å§¿åŠ¿ä¼°è®¡ã€‚è¯¥æ–¹æ³•é€šè¿‡æ„å»ºç›®æ ‡è§†å›¾å’Œåˆå§‹è§†å›¾ä¹‹é—´çš„ 2D-3D å¯¹åº”å…³ç³»ï¼Œç›´æ¥æ±‚è§£å§¿æ€ï¼Œä»è€Œå®ç°å®æ—¶é¢„æµ‹ã€‚æ­¤å¤–ï¼Œä¸ºäº†æé«˜ 2D-3D å¯¹åº”å…³ç³»çš„å‡†ç¡®æ€§ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ 3D ä¸€è‡´ç‚¹æŒ–æ˜ç­–ç•¥ï¼Œè¯¥ç­–ç•¥å¯ä»¥æœ‰æ•ˆåœ°ä¸¢å¼ƒ NeRF é‡å»ºçš„ä¸çœŸå®ç‚¹ã€‚(4) æ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†ä»€ä¹ˆæ€§èƒ½ï¼šå®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¹¶å°†æ¨ç†æ•ˆç‡æé«˜äº† 90 å€ï¼Œå®ç°äº† 6FPS çš„å®æ—¶é¢„æµ‹ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1): æ„å»ºç›®æ ‡è§†å›¾å’Œåˆå§‹è§†å›¾ä¹‹é—´çš„ 2D-3D å¯¹åº”å…³ç³»ï¼Œç›´æ¥æ±‚è§£å§¿æ€ï¼›(2): æå‡º 3D ä¸€è‡´ç‚¹æŒ–æ˜ç­–ç•¥ï¼Œä¸¢å¼ƒ NeRF é‡å»ºçš„ä¸çœŸå®ç‚¹ï¼Œæé«˜ 2D-3D å¯¹åº”å…³ç³»çš„å‡†ç¡®æ€§ï¼›(3): å°† NeRF ä¸ç‰¹å¾åŒ¹é…ç›¸ç»“åˆï¼Œä¸€æ­¥åˆ°ä½æ±‚è§£å§¿æ€ï¼Œå®ç°å®æ—¶é¢„æµ‹ï¼›(4): é‡‡ç”¨ 40 æ­¥åä¼˜åŒ–ï¼Œè¿›ä¸€æ­¥æå‡å§¿æ€ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäº NeRF çš„å¿«é€Ÿå›¾åƒé©±åŠ¨ã€æ—  CAD æ–°ç‰©ä½“å§¿æ€ä¼°è®¡æ¡†æ¶ã€‚é€šè¿‡å¼•å…¥å…³é”®ç‚¹åŒ¹é…ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥ç›´æ¥ä¸€æ­¥æ±‚è§£å§¿æ€ï¼Œå¹¶ä¸”ä¸å—é•¿æ—¶é—´ä¼˜åŒ–å’Œå±€éƒ¨æœ€å°å€¼çš„å½±å“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ 3D ä¸€è‡´ç‚¹æŒ–æ˜ç­–ç•¥æ¥æé«˜ 2D-3D å¯¹åº”å…³ç³»çš„è´¨é‡ï¼Œä»¥åŠä¸€ç§åŸºäºåŒ¹é…å…³é”®ç‚¹çš„é‡‡æ ·ç­–ç•¥æ¥æé«˜å¯¹é®æŒ¡å›¾åƒçš„é²æ£’æ€§ã€‚å®éªŒè¡¨æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„ä¼˜è¶Šæ€§èƒ½å’Œå¯¹é®æŒ¡çš„é²æ£’æ€§ã€‚å¯¹äºæœªæ¥çš„å·¥ä½œï¼Œæˆ‘ä»¬å¸Œæœ›è¯¥æ–¹æ³•å¯ä»¥æ‰©å±•åˆ°æœºå™¨äººæ“ä½œæˆ–æœ€è¿‘åŸºäºç¥ç»åœºçš„ SLAM ä»»åŠ¡ [36]ã€[51]â€“[54]ï¼Œä»¥æé«˜å®šä½çš„æ•ˆç‡æé™ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šå°† NeRF ä¸ç‰¹å¾åŒ¹é…ç›¸ç»“åˆï¼Œä¸€æ­¥åˆ°ä½æ±‚è§£å§¿æ€ï¼›æå‡º 3D ä¸€è‡´ç‚¹æŒ–æ˜ç­–ç•¥ï¼Œæé«˜ 2D-3D å¯¹åº”å…³ç³»çš„å‡†ç¡®æ€§ï¼›åŸºäºåŒ¹é…å…³é”®ç‚¹çš„é‡‡æ ·ç­–ç•¥ï¼Œæé«˜å¯¹é®æŒ¡å›¾åƒçš„é²æ£’æ€§ã€‚æ€§èƒ½ï¼šä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œæ¨ç†æ•ˆç‡æé«˜ 90 å€ï¼Œå®ç° 6FPS çš„å®æ—¶é¢„æµ‹ã€‚å·¥ä½œé‡ï¼šéœ€è¦æ„å»ºç›®æ ‡è§†å›¾å’Œåˆå§‹è§†å›¾ä¹‹é—´çš„ 2D-3D å¯¹åº”å…³ç³»ï¼Œå¹¶è¿›è¡Œ 40 æ­¥åä¼˜åŒ–ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-c945c9d575f76d39cd87ae54b10755b6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f9604c4b56914b94028dfc9542a10656.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-140c2b41b6b6fbcdf4d3c7b1eeb46dc2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c1ad0e80ab82bfabe091780a98abbeec.jpg" align="middle"></details><h2 id="DPA-Net-Structured-3D-Abstraction-from-Sparse-Views-via-Differentiable-Primitive-Assembly"><a href="#DPA-Net-Structured-3D-Abstraction-from-Sparse-Views-via-Differentiable-Primitive-Assembly" class="headerlink" title="DPA-Net: Structured 3D Abstraction from Sparse Views via Differentiable   Primitive Assembly"></a>DPA-Net: Structured 3D Abstraction from Sparse Views via Differentiable   Primitive Assembly</h2><p><strong>Authors:Fenggen Yu, Yiming Qian, Xu Zhang, Francisca Gil-Ureta, Brian Jackson, Eric Bennett, Hao Zhang</strong></p><p>We present a differentiable rendering framework to learn structured 3D abstractions in the form of primitive assemblies from sparse RGB images capturing a 3D object. By leveraging differentiable volume rendering, our method does not require 3D supervision. Architecturally, our network follows the general pipeline of an image-conditioned neural radiance field (NeRF) exemplified by pixelNeRF for color prediction. As our core contribution, we introduce differential primitive assembly (DPA) into NeRF to output a 3D occupancy field in place of density prediction, where the predicted occupancies serve as opacity values for volume rendering. Our network, coined DPA-Net, produces a union of convexes, each as an intersection of convex quadric primitives, to approximate the target 3D object, subject to an abstraction loss and a masking loss, both defined in the image space upon volume rendering. With test-time adaptation and additional sampling and loss designs aimed at improving the accuracy and compactness of the obtained assemblies, our method demonstrates superior performance over state-of-the-art alternatives for 3D primitive abstraction from sparse views. </p><p><a href="http://arxiv.org/abs/2404.00875v2">PDF</a> 14 pages</p><p><strong>Summary</strong><br>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰èå…¥å¯å¾®åˆ†åŸºå…ƒç»„è£…ï¼Œç›´æ¥è¾“å‡º3Då æœ‰ç‡åœºï¼Œæ— éœ€3Dç›‘ç£ï¼Œå®ç°ä»ç¨€ç–RGBå›¾åƒå­¦ä¹ æŠ½è±¡3Dç»“æ„ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>é‡‡ç”¨å¯å¾®åˆ†ä½“ç´ æ¸²æŸ“ï¼Œæ— éœ€3Dç›‘ç£ã€‚</li><li>æ¶æ„éµå¾ªåŸºäºå›¾åƒçš„NeRFç®¡é“ï¼Œé¢„æµ‹é¢œè‰²ã€‚</li><li>æ ¸å¿ƒè´¡çŒ®ï¼šå°†å¯å¾®åˆ†åŸºå…ƒç»„è£…å¼•å…¥NeRFï¼Œè¾“å‡º3Då æœ‰ç‡åœºã€‚</li><li>é¢„æµ‹çš„å æœ‰ç‡ç”¨ä½œä½“ç´ æ¸²æŸ“çš„ä¸é€æ˜åº¦å€¼ã€‚</li><li>DPAç½‘ç»œç”Ÿæˆå‡¸é›†å¹¶é›†ï¼Œé€¼è¿‘ç›®æ ‡3Dç‰©ä½“ã€‚</li><li>æŸå¤±å‡½æ•°åŒ…æ‹¬å›¾åƒç©ºé—´ä¸­çš„æŠ½è±¡æŸå¤±å’Œé®ç½©æŸå¤±ã€‚</li><li>æµ‹è¯•æ—¶è‡ªé€‚åº”ã€é¢å¤–é‡‡æ ·å’ŒæŸå¤±è®¾è®¡ï¼Œæé«˜ç»„è£…ç²¾åº¦å’Œç´§å‡‘æ€§ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šDPA-Netï¼šé€šè¿‡å¯å¾®åˆ†åŸºå…ƒè£…é…ä»ç¨€ç–è§†å›¾ä¸­è¿›è¡Œç»“æ„åŒ– 3D æŠ½è±¡</li><li>ä½œè€…ï¼šFenggen Yuã€Yiming Qianã€Xu Zhangã€Francisca Gil-Uretaã€Brian Jacksonã€Eric Bennettã€Hao Zhang</li><li>éš¶å±å•ä½ï¼šäºšé©¬é€Š</li><li>å…³é”®è¯ï¼š3D æŠ½è±¡ã€ç¨€ç–è§†å›¾ã€å¯å¾®åˆ†ä½“æ¸²æŸ“ã€ç¥ç»è¾å°„åœº</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.00875</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šä»å•è§†å›¾æˆ–å¤šè§†å›¾å›¾åƒä¸­è¿›è¡Œ 3D æ¨ç†ï¼ˆä¾‹å¦‚æŠ½è±¡æˆ–é‡å»ºï¼‰æ˜¯è®¡ç®—æœºè§†è§‰ä¸­æœ€åŸºæœ¬çš„é—®é¢˜ä¹‹ä¸€ã€‚éšç€ç¥ç»åœºï¼ˆå°¤å…¶æ˜¯ç¥ç»è¾å°„åœºå’Œ 3D é«˜æ–¯ splattingï¼‰çš„å‡ºç°ï¼Œ3D é‡å»ºçš„è´¨é‡ã€é€Ÿåº¦ä»¥åŠå¤„ç†ç¨€ç–è§†å›¾ï¼ˆè€Œä¸æ˜¯æ—©æœŸå·¥ä½œä¸­çš„å¯†é›†è¾“å…¥è§†å›¾ï¼‰çš„èƒ½åŠ›éƒ½å¾—åˆ°äº†å¿«é€Ÿå‘å±•ã€‚ä½†æ˜¯ï¼ŒNeRF åŠå…¶å¤§å¤šæ•°å˜ä½“åœ¨è®¾è®¡ä¸Šéƒ½ä»¥æ–°é¢–è§†å›¾åˆæˆä¸ºç›®æ ‡ï¼Œé‡ç‚¹åœ¨äºä¼˜åŒ–å…¶åŸºå…ƒä»¥æé«˜æ¸²æŸ“æ€§èƒ½ï¼Œè€Œä¸æ˜¯æœåŠ¡äºæ¶‰åŠå½¢çŠ¶å»ºæ¨¡æˆ–æ“ä½œçš„ä¸‹æ¸¸ä»»åŠ¡ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•ï¼šæœ€è¿‘æå‡ºäº†ä¸€äº›é€šè¿‡å­¦ä¹ åŸºå…ƒè£…é…ï¼ˆä¾‹å¦‚æ„é€ å®ä½“å‡ ä½•æ ‘ã€è‰å›¾æŒ¤å‡ºæ¨¡å‹æˆ–å½¢çŠ¶ç¨‹åºï¼‰è¿›è¡Œ CAD å»ºæ¨¡çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™äº›ç¥ç»æ¨¡å‹éƒ½é‡‡ç”¨ä½“ç´ å’Œç‚¹äº‘ç­‰ 3D è¾“å…¥ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§å¯å¾®åˆ†æ¸²æŸ“æ¡†æ¶ï¼Œç”¨äºä»æ•è· 3D ç‰©ä½“çš„ç¨€ç– RGB å›¾åƒä¸­ä»¥åŸºå…ƒè£…é…çš„å½¢å¼å­¦ä¹ ç»“æ„åŒ– 3D æŠ½è±¡ã€‚é€šè¿‡åˆ©ç”¨å¯å¾®åˆ†ä½“æ¸²æŸ“ï¼Œæœ¬æ–‡æ–¹æ³•ä¸éœ€è¦ 3D ç›‘ç£ã€‚åœ¨æ¶æ„ä¸Šï¼Œæœ¬æ–‡ç½‘ç»œéµå¾ªä»¥ pixelNeRF ä¸ºä¾‹çš„å›¾åƒæ¡ä»¶ç¥ç»è¾å°„åœºçš„ä¸€èˆ¬ç®¡é“è¿›è¡Œé¢œè‰²é¢„æµ‹ã€‚ä½œä¸ºæ ¸å¿ƒè´¡çŒ®ï¼Œæœ¬æ–‡å°†å¯å¾®åˆ†åŸºå…ƒè£…é…å¼•å…¥ NeRFï¼Œä»¥è¾“å‡º 3D å ç”¨åœºæ¥ä»£æ›¿å¯†åº¦é¢„æµ‹ï¼Œå…¶ä¸­é¢„æµ‹çš„å ç”¨ç‡ç”¨ä½œä½“ç§¯æ¸²æŸ“çš„ä¸é€æ˜åº¦å€¼ã€‚æœ¬æ–‡ç½‘ç»œç§°ä¸º DPA-Netï¼Œå®ƒç”Ÿæˆå‡¸é›†çš„å¹¶é›†ï¼Œæ¯ä¸ªå‡¸é›†éƒ½æ˜¯å‡¸äºŒæ¬¡åŸºå…ƒçš„äº¤é›†ï¼Œä»¥è¿‘ä¼¼ç›®æ ‡ 3D å¯¹è±¡ï¼Œå—æŠ½è±¡æŸå¤±å’Œæ©ç æŸå¤±çš„çº¦æŸï¼Œä¸¤è€…éƒ½åœ¨ä½“ç§¯æ¸²æŸ“æ—¶åœ¨å›¾åƒç©ºé—´ä¸­å®šä¹‰ã€‚é€šè¿‡æµ‹è¯•æ—¶é€‚åº”ä»¥åŠæ—¨åœ¨æé«˜æ‰€è·å¾—è£…é…çš„å‡†ç¡®æ€§å’Œç´§å‡‘æ€§çš„é™„åŠ é‡‡æ ·å’ŒæŸå¤±è®¾è®¡ï¼Œæœ¬æ–‡æ–¹æ³•å±•ç¤ºäº†ä»ç¨€ç–è§†å›¾ä¸­è¿›è¡Œ 3D åŸºå…ƒæŠ½è±¡çš„æœ€æ–°æ›¿ä»£æ–¹æ¡ˆçš„ä¼˜è¶Šæ€§èƒ½ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šåœ¨ ShapeNet å’Œ PartNet æ•°æ®é›†ä¸Šï¼Œæœ¬æ–‡æ–¹æ³•åœ¨å‡†ç¡®æ€§å’Œç´§å‡‘æ€§æ–¹é¢éƒ½ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚è¿™äº›æ€§èƒ½æ”¯æŒæœ¬æ–‡ç›®æ ‡ï¼Œå³ä»ç¨€ç–è§†å›¾ä¸­å­¦ä¹ ç»“æ„åŒ– 3D æŠ½è±¡ï¼Œä»¥ä¿ƒè¿›ä¸‹æ¸¸å½¢çŠ¶å»ºæ¨¡å’Œæ“ä½œä»»åŠ¡ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1): ç‰¹å¾æå–å’Œèšåˆï¼›(2): åŸå§‹è£…é…ï¼š</p><ul><li>åŸå§‹å‚æ•°åŒ–ï¼š</li><li>åŸå§‹äº¤é›†ï¼š</li><li>å‡¸é›†å¹¶é›†ï¼š(3): å¯å¾®åˆ†æ¸²æŸ“ï¼›(4): ç½‘ç»œè®­ç»ƒå’Œæµ‹è¯•æ—¶è‡ªé€‚åº”ï¼š</li><li>é¢„è®­ç»ƒï¼š</li><li>æµ‹è¯•æ—¶è‡ªé€‚åº”ï¼ˆTTAï¼‰ï¼š<ul><li>ç¬¬ä¸€é˜¶æ®µï¼š</li><li>ç¬¬äºŒé˜¶æ®µï¼š</li><li>ç¬¬ä¸‰é˜¶æ®µï¼š</li></ul></li></ul></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§å¯å¾®åˆ†æ¸²æŸ“æ¡†æ¶ DPA-Netï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿä»ä»…æœ‰çš„å‡ ä¸ªï¼ˆä¾‹å¦‚ä¸‰ä¸ªï¼‰RGB å›¾åƒä¸­ä»¥åŸºå…ƒè£…é…çš„å½¢å¼å­¦ä¹ ç»“æ„åŒ–çš„ 3D æŠ½è±¡ï¼Œè¿™äº›å›¾åƒæ˜¯åœ¨éå¸¸ä¸åŒçš„è§†è§’ä¸‹æ‹æ‘„çš„ã€‚æˆ‘ä»¬çš„å…³é”®åˆ›æ–°æ˜¯å°†å¯å¾®åˆ†åŸºå…ƒè£…é…é›†æˆåˆ° NeRF æ¶æ„ä¸­ï¼Œä»è€Œèƒ½å¤Ÿé¢„æµ‹å ç”¨ç‡ä»¥ç”¨ä½œä½“ç§¯æ¸²æŸ“çš„ä¸é€æ˜åº¦å€¼ã€‚åœ¨æ²¡æœ‰ä»»ä½• 3D æˆ–å½¢çŠ¶åˆ†è§£ç›‘ç£çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥ç”Ÿæˆä¸€ä¸ªå¯è§£é‡Šä¸”éšåå¯ç¼–è¾‘çš„å‡¸é›†å¹¶é›†ï¼Œè¯¥å¹¶é›†è¿‘ä¼¼äºç›®æ ‡ 3D å¯¹è±¡ã€‚åœ¨ ShapeNet å’Œ DTU ä¸Šçš„å®šé‡å’Œå®šæ€§è¯„ä¼°è¡¨æ˜ï¼ŒDPA-Net ä¼˜äºæœ€å…ˆè¿›çš„æ›¿ä»£æ–¹æ¡ˆã€‚å±•ç¤ºçš„åº”ç”¨ç¨‹åºè¿›ä¸€æ­¥è¡¨æ˜ï¼Œæˆ‘ä»¬å¯ç¼–è¾‘çš„ 3D æŠ½è±¡å¯ä»¥ç”¨ä½œç»“æ„æç¤ºï¼Œå¹¶æœ‰åˆ©äºå…¶ä»– 3D ç”Ÿæˆä»»åŠ¡ã€‚æˆ‘ä»¬å½“å‰çš„å®ç°åˆ©ç”¨äº† GT ç›¸æœºä½å§¿ã€‚ä¸ºäº†å‡è½»ç”±ä¼°è®¡çš„ã€å˜ˆæ‚çš„ä½å§¿å¼•èµ·çš„æ€§èƒ½ä¸‹é™ï¼Œå¯ä»¥åº”ç”¨ç°æœ‰çš„ç”¨äºè”åˆç›¸æœºåœºæ™¯ä¼˜åŒ–çš„ç°æœ‰æ–¹æ³•ï¼Œä¾‹å¦‚ [44]ã€‚ç”±äºçº¹ç†é¢„æµ‹ä¸æ˜¯æˆ‘ä»¬å·¥ä½œçš„é‡ç‚¹ï¼Œå› æ­¤éœ€è¦è¿›ä¸€æ­¥å¾®è°ƒï¼ˆä¾‹å¦‚ï¼Œåå‘è¾“å…¥è§†å›¾ï¼‰å’Œä¼˜åŒ–ä»¥æé«˜æ¸²æŸ“è´¨é‡ã€‚æœ€åï¼Œä»…ä½¿ç”¨å‡¸é›†çš„è£…é…æ˜¯æœ‰é™çš„ã€‚å¦‚è¡¥å……ææ–™æ‰€ç¤ºï¼ŒDPA-Net æ— æ³•å¾ˆå¥½åœ°å¤„ç†å‡¹å½¢ã€‚å°†å·®åˆ†è¿ç®—æ·»åŠ åˆ°å¯å¾®åˆ†è£…é…ä¸­å€¼å¾—æ¢ç´¢ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šDPA-Net å°†å¯å¾®åˆ†åŸºå…ƒè£…é…é›†æˆåˆ° NeRF æ¶æ„ä¸­ï¼Œä»è€Œèƒ½å¤Ÿé¢„æµ‹å ç”¨ç‡ä»¥ç”¨ä½œä½“ç§¯æ¸²æŸ“çš„ä¸é€æ˜åº¦å€¼ã€‚è¿™ä½¿å¾— DPA-Net èƒ½å¤Ÿä»ç¨€ç–è§†å›¾ä¸­å­¦ä¹ ç»“æ„åŒ–çš„ 3D æŠ½è±¡ï¼Œè€Œæ— éœ€ä»»ä½• 3D æˆ–å½¢çŠ¶åˆ†è§£ç›‘ç£ã€‚æ€§èƒ½ï¼šåœ¨ ShapeNet å’Œ DTU ä¸Šçš„å®šé‡å’Œå®šæ€§è¯„ä¼°è¡¨æ˜ï¼ŒDPA-Net ä¼˜äºæœ€å…ˆè¿›çš„æ›¿ä»£æ–¹æ¡ˆã€‚DPA-Net ç”Ÿæˆçš„ 3D æŠ½è±¡å‡†ç¡®ã€ç´§å‡‘ä¸”å¯ç¼–è¾‘ï¼Œå¯ä»¥ä½œä¸ºç»“æ„æç¤ºï¼Œå¹¶æœ‰åˆ©äºå…¶ä»– 3D ç”Ÿæˆä»»åŠ¡ã€‚å·¥ä½œé‡ï¼šDPA-Net çš„å®ç°åˆ©ç”¨äº† GT ç›¸æœºä½å§¿ã€‚ä¸ºäº†å‡è½»ç”±ä¼°è®¡çš„ã€å˜ˆæ‚çš„ä½å§¿å¼•èµ·çš„æ€§èƒ½ä¸‹é™ï¼Œå¯ä»¥åº”ç”¨ç°æœ‰çš„ç”¨äºè”åˆç›¸æœºåœºæ™¯ä¼˜åŒ–çš„ç°æœ‰æ–¹æ³•ï¼Œä¾‹å¦‚ [44]ã€‚æ­¤å¤–ï¼Œç”±äºçº¹ç†é¢„æµ‹ä¸æ˜¯æˆ‘ä»¬å·¥ä½œçš„é‡ç‚¹ï¼Œå› æ­¤éœ€è¦è¿›ä¸€æ­¥å¾®è°ƒï¼ˆä¾‹å¦‚ï¼Œåå‘è¾“å…¥è§†å›¾ï¼‰å’Œä¼˜åŒ–ä»¥æé«˜æ¸²æŸ“è´¨é‡ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-d1e745532008f87ea77f1571498e7a15.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-673670c0d185d530bd9f22bc5c036d4e.jpg" align="middle"></details><h2 id="Knowledge-NeRF-Few-shot-Novel-View-Synthesis-for-Dynamic-Articulated-Objects"><a href="#Knowledge-NeRF-Few-shot-Novel-View-Synthesis-for-Dynamic-Articulated-Objects" class="headerlink" title="Knowledge NeRF: Few-shot Novel View Synthesis for Dynamic Articulated   Objects"></a>Knowledge NeRF: Few-shot Novel View Synthesis for Dynamic Articulated   Objects</h2><p><strong>Authors:Wenxiao Cai, Xinyue LeiÄ±nst, Xinyu He, Junming Leo Chen, Yangang Wang</strong></p><p>We present Knowledge NeRF to synthesize novel views for dynamic scenes.Reconstructing dynamic 3D scenes from few sparse views and rendering them from arbitrary perspectives is a challenging problem with applications in various domains. Previous dynamic NeRF methods learn the deformation of articulated objects from monocular videos. However, qualities of their reconstructed scenes are limited.To clearly reconstruct dynamic scenes, we propose a new framework by considering two frames at a time.We pretrain a NeRF model for an articulated object.When articulated objects moves, Knowledge NeRF learns to generate novel views at the new state by incorporating past knowledge in the pretrained NeRF model with minimal observations in the present state. We propose a projection module to adapt NeRF for dynamic scenes, learning the correspondence between pretrained knowledge base and current states. Experimental results demonstrate the effectiveness of our method in reconstructing dynamic 3D scenes with 5 input images in one state. Knowledge NeRF is a new pipeline and promising solution for novel view synthesis in dynamic articulated objects. The data and implementation are publicly available at <a href="https://github.com/RussRobin/Knowledge_NeRF">https://github.com/RussRobin/Knowledge_NeRF</a>. </p><p><a href="http://arxiv.org/abs/2404.00674v1">PDF</a> </p><p><strong>Summary</strong><br>é€šè¿‡å°†è¿‡å»çŸ¥è¯†åº”ç”¨äºå½“å‰çŠ¶æ€çš„æœ‰é™è§‚æµ‹å€¼ï¼ŒKnowledge NeRF å¯ä¸ºåŠ¨æ€åœºæ™¯åˆæˆæ–°é¢–è§†å›¾ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>é’ˆå¯¹åŠ¨æ€åœºæ™¯ï¼ŒKnowledge NeRF æå‡ºäº†ä¸€ç§åŒæ—¶è€ƒè™‘ä¸¤å¸§çš„æ–°æ¡†æ¶ã€‚</li><li>é¢„è®­ç»ƒçš„ NeRF æ¨¡å‹ç”¨äºå­¦ä¹ é“°æ¥å¯¹è±¡çš„å˜å½¢ã€‚</li><li>æå‡ºäº†ä¸€ç§æŠ•å½±æ¨¡å—ï¼Œç”¨äºå­¦ä¹ é¢„è®­ç»ƒçŸ¥è¯†åº“å’Œå½“å‰çŠ¶æ€ä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚</li><li>Knowledge NeRF é€šè¿‡ 5 ä¸ªè¾“å…¥å›¾åƒåœ¨ä¸€å¸§ä¸­é‡å»ºåŠ¨æ€ 3D åœºæ™¯ã€‚</li><li>Knowledge NeRF ä¸ºåŠ¨æ€é“°æ¥å¯¹è±¡çš„å…¨æ–°è§†å›¾åˆæˆæä¾›äº†ä¸€ä¸ªæ–°çš„ç®¡é“å’Œæœ‰å¸Œæœ›çš„è§£å†³æ–¹æ¡ˆã€‚</li><li>è¯¥æ–¹æ³•é¿å…äº†åŠ¨æ€ NeRF æ–¹æ³•ä¸­å¸¸è§çš„é—®é¢˜ï¼Œä¾‹å¦‚æ¨¡ç³Šå’Œå˜å½¢é”™è¯¯ã€‚</li><li>æ•°æ®å’Œå®ç°å·²å…¬å¼€ï¼Œå¯ç”¨äºè¿›ä¸€æ­¥ç ”ç©¶å’Œåº”ç”¨ç¨‹åºå¼€å‘ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šçŸ¥è¯† NeRFï¼šåŠ¨æ€é“°æ¥å¯¹è±¡çš„å°æ ·æœ¬æ–°è§†è§’åˆæˆ</li><li>ä½œè€…ï¼šè”¡æ–‡æ™“ã€é›·æ¬£æ‚¦<em>ã€ä½•æ¬£å®‡</em>ã€é™ˆå›æ˜å’Œç‹æ‰¬åˆš**</li><li>å•ä½ï¼šä¸œå—å¤§å­¦</li><li>å…³é”®è¯ï¼šæ–°è§†è§’åˆæˆÂ·ç¥ç»è¾å°„åœºÂ·åŠ¨æ€ 3D åœºæ™¯Â·ç¨€ç–è§†è§’åˆæˆÂ·çŸ¥è¯†é›†æˆ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.00674</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šåŠ¨æ€åœºæ™¯é‡å»ºå’Œæ¸²æŸ“ä¸€ç›´æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„é‡è¦è¯¾é¢˜ã€‚ä¼ ç»Ÿçš„åŠ¨æ€ NeRF æ–¹æ³•é€šè¿‡å•ç›®è§†é¢‘å­¦ä¹ é“°æ¥å¯¹è±¡çš„å˜å½¢ï¼Œä½†é‡å»ºåœºæ™¯çš„è´¨é‡æœ‰é™ã€‚</li></ol><p>ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šè¿‡å»çš„æ–¹æ³•ä¸»è¦é€šè¿‡å•ç›®è§†é¢‘å­¦ä¹ é“°æ¥å¯¹è±¡çš„å˜å½¢ï¼Œä½†é‡å»ºåœºæ™¯çš„è´¨é‡æœ‰é™ã€‚</p><p>ï¼ˆ3ï¼‰æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œä¸€æ¬¡è€ƒè™‘ä¸¤å¸§å›¾åƒã€‚é¦–å…ˆï¼Œå¯¹é“°æ¥å¯¹è±¡é¢„è®­ç»ƒä¸€ä¸ª NeRF æ¨¡å‹ã€‚å½“é“°æ¥å¯¹è±¡ç§»åŠ¨æ—¶ï¼ŒçŸ¥è¯† NeRF é€šè¿‡å°†é¢„è®­ç»ƒ NeRF æ¨¡å‹ä¸­çš„è¿‡å»çŸ¥è¯†ä¸å½“å‰çŠ¶æ€ä¸­çš„æœ€å°‘è§‚å¯Ÿç›¸ç»“åˆï¼Œå­¦ä¹ åœ¨æ–°çš„çŠ¶æ€ä¸‹ç”Ÿæˆæ–°è§†è§’ã€‚æœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§æŠ•å½±æ¨¡å—ï¼Œå°† NeRF é€‚åº”äºåŠ¨æ€åœºæ™¯ï¼Œå­¦ä¹ é¢„è®­ç»ƒçŸ¥è¯†åº“å’Œå½“å‰çŠ¶æ€ä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚</p><p>ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†ä»€ä¹ˆæ€§èƒ½ï¼Œè¯¥æ€§èƒ½æ˜¯å¦èƒ½æ”¯æ’‘å…¶ç›®æ ‡ï¼šå®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä½¿ç”¨ä¸€ä¸ªçŠ¶æ€ä¸­çš„ 5 å¼ è¾“å…¥å›¾åƒé‡å»ºåŠ¨æ€ 3D åœºæ™¯ã€‚è¯¥æ–¹æ³•ä¸ºåŠ¨æ€é“°æ¥å¯¹è±¡çš„æ–°è§†è§’åˆæˆæä¾›äº†ä¸€ç§æ–°çš„ç®¡é“å’Œæœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚</p><p><methods>:(1): çŸ¥è¯†NeRFæ¡†æ¶ï¼šä¸€æ¬¡è€ƒè™‘ä¸¤å¸§å›¾åƒï¼Œå°†é¢„è®­ç»ƒNeRFæ¨¡å‹çš„çŸ¥è¯†ä¸å½“å‰çŠ¶æ€çš„ç¨€ç–è§‚å¯Ÿç›¸ç»“åˆï¼Œç”Ÿæˆæ–°è§†è§’å›¾åƒã€‚(2): æŠ•å½±æ¨¡å—ï¼šå­¦ä¹ é¢„è®­ç»ƒçŸ¥è¯†åº“å’Œå½“å‰çŠ¶æ€ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼Œå°†NeRFé€‚åº”äºåŠ¨æ€åœºæ™¯ã€‚(3): ç¨€ç–è§†è§’åˆæˆï¼šä½¿ç”¨ä¸€ä¸ªçŠ¶æ€ä¸­çš„5å¼ è¾“å…¥å›¾åƒé‡å»ºåŠ¨æ€3Dåœºæ™¯ã€‚</methods></p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„çŸ¥è¯†NeRFæ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿä¸€æ¬¡è€ƒè™‘ä¸¤å¸§å›¾åƒï¼Œå°†é¢„è®­ç»ƒNeRFæ¨¡å‹çš„çŸ¥è¯†ä¸å½“å‰çŠ¶æ€çš„ç¨€ç–è§‚å¯Ÿç›¸ç»“åˆï¼Œç”Ÿæˆæ–°è§†è§’å›¾åƒã€‚è¯¥æ¡†æ¶è¿˜æå‡ºäº†ä¸€ç§æŠ•å½±æ¨¡å—ï¼Œå­¦ä¹ é¢„è®­ç»ƒçŸ¥è¯†åº“å’Œå½“å‰çŠ¶æ€ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼Œå°†NeRFé€‚åº”äºåŠ¨æ€åœºæ™¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä½¿ç”¨ä¸€ä¸ªçŠ¶æ€ä¸­çš„5å¼ è¾“å…¥å›¾åƒé‡å»ºåŠ¨æ€3Dåœºæ™¯ï¼Œä¸ºåŠ¨æ€é“°æ¥å¯¹è±¡çš„æ–°è§†è§’åˆæˆæä¾›äº†ä¸€ç§æ–°çš„ç®¡é“å’Œæœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§æ–°çš„çŸ¥è¯†NeRFæ¡†æ¶ï¼Œå°†é¢„è®­ç»ƒNeRFæ¨¡å‹çš„çŸ¥è¯†ä¸å½“å‰çŠ¶æ€çš„ç¨€ç–è§‚å¯Ÿç›¸ç»“åˆï¼Œç”Ÿæˆæ–°è§†è§’å›¾åƒã€‚</li><li>è®¾è®¡äº†ä¸€ç§æŠ•å½±æ¨¡å—ï¼Œå­¦ä¹ é¢„è®­ç»ƒçŸ¥è¯†åº“å’Œå½“å‰çŠ¶æ€ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼Œå°†NeRFé€‚åº”äºåŠ¨æ€åœºæ™¯ã€‚æ€§èƒ½ï¼š</li><li>èƒ½å¤Ÿä½¿ç”¨ä¸€ä¸ªçŠ¶æ€ä¸­çš„5å¼ è¾“å…¥å›¾åƒé‡å»ºåŠ¨æ€3Dåœºæ™¯ã€‚å·¥ä½œé‡ï¼š</li><li>éœ€è¦é¢„è®­ç»ƒä¸€ä¸ªNeRFæ¨¡å‹ã€‚</li><li>éœ€è¦è®¾è®¡ä¸€ä¸ªæŠ•å½±æ¨¡å—ã€‚</li><li>éœ€è¦æ”¶é›†å’Œæ ‡æ³¨åŠ¨æ€3Dåœºæ™¯çš„æ•°æ®é›†ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-51d2760768289f17a022822e034438cb.jpg" align="middle"><img src="https://pica.zhimg.com/v2-1c30a4c13059600200df9151f02890b7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-35e6ac626071f10b54837546e9ead1e4.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-04-06  RaFE Generative Radiance Fields Restoration</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/3DGS/"/>
    <id>https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/3DGS/</id>
    <published>2024-04-06T10:15:08.000Z</published>
    <updated>2024-04-06T10:15:08.616Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-04-06-æ›´æ–°"><a href="#2024-04-06-æ›´æ–°" class="headerlink" title="2024-04-06 æ›´æ–°"></a>2024-04-06 æ›´æ–°</h1><h2 id="Per-Gaussian-Embedding-Based-Deformation-for-Deformable-3D-Gaussian-Splatting"><a href="#Per-Gaussian-Embedding-Based-Deformation-for-Deformable-3D-Gaussian-Splatting" class="headerlink" title="Per-Gaussian Embedding-Based Deformation for Deformable 3D Gaussian   Splatting"></a>Per-Gaussian Embedding-Based Deformation for Deformable 3D Gaussian   Splatting</h2><p><strong>Authors:Jeongmin Bae, Seoha Kim, Youngsik Yun, Hahyun Lee, Gun Bang, Youngjung Uh</strong></p><p>As 3D Gaussian Splatting (3DGS) provides fast and high-quality novel view synthesis, it is a natural extension to deform a canonical 3DGS to multiple frames. However, previous works fail to accurately reconstruct dynamic scenes, especially 1) static parts moving along nearby dynamic parts, and 2) some dynamic areas are blurry. We attribute the failure to the wrong design of the deformation field, which is built as a coordinate-based function. This approach is problematic because 3DGS is a mixture of multiple fields centered at the Gaussians, not just a single coordinate-based framework. To resolve this problem, we define the deformation as a function of per-Gaussian embeddings and temporal embeddings. Moreover, we decompose deformations as coarse and fine deformations to model slow and fast movements, respectively. Also, we introduce an efficient training strategy for faster convergence and higher quality. Project page: <a href="https://jeongminb.github.io/e-d3dgs/">https://jeongminb.github.io/e-d3dgs/</a> </p><p><a href="http://arxiv.org/abs/2404.03613v1">PDF</a> Preprint</p><p><strong>Summary</strong><br>3D é«˜æ–¯æ–‘ç‚¹é‡‡æ ·é€šè¿‡å˜å½¢ç½‘æ ¼æ¥å®ç°åŠ¨æ€åœºæ™¯çš„ç²¾ç¡®é‡å»ºï¼Œè§£å†³äº†ä»¥å¾€ä½œå“çš„å±€é™æ€§ï¼ŒåŒ…æ‹¬é™æ€éƒ¨ä»¶æ²¿ç€åŠ¨æ€éƒ¨ä»¶ç§»åŠ¨å’ŒåŠ¨æ€åŒºåŸŸæ¨¡ç³Šçš„é—®é¢˜ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>åŠ¨æ€åœºæ™¯å˜å½¢é‡å»ºå­˜åœ¨é—®é¢˜ï¼ŒåŒ…æ‹¬é™æ€éƒ¨ä»¶æ²¿åŠ¨æ€éƒ¨ä»¶ç§»åŠ¨å’ŒåŠ¨æ€åŒºåŸŸæ¨¡ç³Šã€‚</li><li>é—®é¢˜çš„æ ¹æºåœ¨äºå˜å½¢åœºçš„é”™è¯¯è®¾è®¡ï¼Œéœ€é‡‡ç”¨åŸºäºæ··åˆé«˜æ–¯æ ¸çš„å‡½æ•°ã€‚</li><li>å˜å½¢å®šä¹‰ä¸ºåŸºäºé«˜æ–¯åµŒå…¥å’Œæ—¶é—´åµŒå…¥çš„å‡½æ•°ï¼Œå¯åˆ†è§£ä¸ºç²—ç•¥å’Œç²¾ç»†å˜å½¢ã€‚</li><li>å¼•å…¥é«˜æ•ˆè®­ç»ƒç­–ç•¥ï¼ŒåŠ é€Ÿæ”¶æ•›å¹¶æå‡è´¨é‡ã€‚</li><li>è¯¥ç ”ç©¶é€šè¿‡å˜å½¢ç½‘æ ¼å®ç°äº†åŠ¨æ€åœºæ™¯çš„ç²¾ç¡®é‡å»ºã€‚</li><li>æå‡ºäº†ä¸€ç§æ–°çš„å˜å½¢åœºè®¾è®¡ï¼ŒåŸºäºæ¯ä¸ªé«˜æ–¯æ ¸çš„åµŒå…¥å’Œæ—¶é—´åµŒå…¥ã€‚</li><li>é‡‡ç”¨ç²—ç•¥å’Œç²¾ç»†å˜å½¢ç›¸ç»“åˆçš„æ–¹å¼ï¼Œåˆ†åˆ«å»ºæ¨¡ç¼“æ…¢å’Œå¿«é€Ÿè¿åŠ¨ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šåŸºäºé«˜æ–¯åµŒå…¥çš„å˜å½¢</li><li>ä½œè€…ï¼šJeongmin Baeã€Seoha Kimã€Youngsik Yunã€Hahyun Leeã€Gun Bangã€Youngjung Uh</li><li>æ‰€å±å•ä½ï¼šå»¶ä¸–å¤§å­¦</li><li>å…³é”®è¯ï¼šé«˜æ–¯æ•£å¸ƒã€åŠ¨æ€åœºæ™¯é‡å»ºã€æ–°é¢–è§†å›¾åˆæˆ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.03613   Github é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼š   (1) ç ”ç©¶èƒŒæ™¯ï¼š   3D é«˜æ–¯æ•£å¸ƒï¼ˆ3DGSï¼‰æä¾›å¿«é€Ÿä¸”é«˜è´¨é‡çš„æ–°é¢–è§†å›¾åˆæˆï¼Œå°†æ­£åˆ™ 3DGS å˜å½¢åˆ°å¤šä¸ªå¸§æ˜¯å…¶è‡ªç„¶å»¶ä¼¸ã€‚ç„¶è€Œï¼Œä»¥å¾€çš„ç ”ç©¶æ— æ³•å‡†ç¡®é‡å»ºåŠ¨æ€åœºæ™¯ï¼Œç‰¹åˆ«æ˜¯ï¼š1ï¼‰é™æ­¢éƒ¨åˆ†æ²¿ç€é™„è¿‘çš„åŠ¨æ€éƒ¨åˆ†ç§»åŠ¨ï¼›2ï¼‰ä¸€äº›åŠ¨æ€åŒºåŸŸæ¨¡ç³Šã€‚   (2) è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼š   å°†å˜å½¢åœºè®¾è®¡ä¸ºåŸºäºåæ ‡çš„å‡½æ•°ï¼Œè¿™æ˜¯å¯¼è‡´ä¸Šè¿°é—®é¢˜çš„åŸå› ã€‚è¿™ç§æ–¹æ³•å­˜åœ¨é—®é¢˜ï¼Œå› ä¸º 3DGS æ˜¯ä»¥é«˜æ–¯ä¸ºä¸­å¿ƒçš„å¤šä¸ªåœºçš„æ··åˆï¼Œè€Œä¸ä»…ä»…æ˜¯ä¸€ä¸ªåŸºäºåæ ‡çš„æ¡†æ¶ã€‚   (3) æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼š   å°†å˜å½¢å®šä¹‰ä¸ºæ¯ä¸ªé«˜æ–¯åµŒå…¥å’Œæ—¶é—´åµŒå…¥çš„å‡½æ•°ã€‚æ­¤å¤–ï¼Œå°†å˜å½¢åˆ†è§£ä¸ºç²—ç•¥å˜å½¢å’Œç²¾ç»†å˜å½¢ï¼Œåˆ†åˆ«å¯¹æ…¢é€Ÿè¿åŠ¨å’Œå¿«é€Ÿè¿åŠ¨è¿›è¡Œå»ºæ¨¡ã€‚è¿˜å¼•å…¥äº†ä¸€ç§æœ‰æ•ˆçš„è®­ç»ƒç­–ç•¥ï¼Œä»¥å®ç°æ›´å¿«çš„æ”¶æ•›å’Œæ›´é«˜çš„è´¨é‡ã€‚   (4) æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼š   è¯¥æ–¹æ³•åœ¨åŠ¨æ€åœºæ™¯é‡å»ºä»»åŠ¡ä¸Šå®ç°äº†å…ˆè¿›çš„æ€§èƒ½ã€‚å®ƒå¯ä»¥å‡†ç¡®åœ°é‡å»ºåŠ¨æ€åœºæ™¯ï¼ŒåŒæ—¶é¿å…é™æ­¢éƒ¨åˆ†æ²¿é™„è¿‘åŠ¨æ€éƒ¨åˆ†ç§»åŠ¨å’ŒåŠ¨æ€åŒºåŸŸæ¨¡ç³Šçš„é—®é¢˜ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ï¼Œå³å‡†ç¡®é‡å»ºåŠ¨æ€åœºæ™¯ã€‚</p></li><li><p>Methods:(1): å°†å˜å½¢å®šä¹‰ä¸ºæ¯ä¸ªé«˜æ–¯åµŒå…¥å’Œæ—¶é—´åµŒå…¥çš„å‡½æ•°ï¼Œä»¥è§£å†³ä»¥å¾€åŸºäºåæ ‡çš„å˜å½¢å‡½æ•°çš„å±€é™æ€§ã€‚(2): å°†å˜å½¢åˆ†è§£ä¸ºç²—ç•¥å˜å½¢å’Œç²¾ç»†å˜å½¢ï¼Œåˆ†åˆ«å»ºæ¨¡æ…¢é€Ÿè¿åŠ¨å’Œå¿«é€Ÿè¿åŠ¨ï¼Œä»è€Œæé«˜é‡å»ºç²¾åº¦ã€‚(3): æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„è®­ç»ƒç­–ç•¥ï¼ŒåŒ…æ‹¬é¢„è®­ç»ƒã€è”åˆè®­ç»ƒå’Œç»†åŒ–è®­ç»ƒï¼Œä»¥å®ç°æ›´å¿«çš„æ”¶æ•›å’Œæ›´é«˜çš„è´¨é‡ã€‚</p></li></ol><p>8.ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†åŸºäºé«˜æ–¯åµŒå…¥çš„å˜å½¢æ–¹æ³•ï¼Œè§£å†³äº†ä»¥å¾€åŸºäºåæ ‡çš„å˜å½¢å‡½æ•°çš„å±€é™æ€§ï¼Œæœ‰æ•ˆåœ°é‡å»ºåŠ¨æ€åœºæ™¯ï¼Œé¿å…äº†é™æ­¢éƒ¨åˆ†æ²¿ç€é™„è¿‘åŠ¨æ€éƒ¨åˆ†ç§»åŠ¨å’ŒåŠ¨æ€åŒºåŸŸæ¨¡ç³Šçš„é—®é¢˜ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š- å°†å˜å½¢å®šä¹‰ä¸ºæ¯ä¸ªé«˜æ–¯åµŒå…¥å’Œæ—¶é—´åµŒå…¥çš„å‡½æ•°ï¼Œæé«˜äº†é‡å»ºç²¾åº¦ã€‚- å°†å˜å½¢åˆ†è§£ä¸ºç²—ç•¥å˜å½¢å’Œç²¾ç»†å˜å½¢ï¼Œåˆ†åˆ«å»ºæ¨¡æ…¢é€Ÿè¿åŠ¨å’Œå¿«é€Ÿè¿åŠ¨ã€‚- æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„è®­ç»ƒç­–ç•¥ï¼ŒåŒ…æ‹¬é¢„è®­ç»ƒã€è”åˆè®­ç»ƒå’Œç»†åŒ–è®­ç»ƒï¼Œå®ç°æ›´å¿«çš„æ”¶æ•›å’Œæ›´é«˜çš„è´¨é‡ã€‚æ€§èƒ½ï¼š- åœ¨åŠ¨æ€åœºæ™¯é‡å»ºä»»åŠ¡ä¸Šå®ç°äº†å…ˆè¿›çš„æ€§èƒ½ã€‚- å‡†ç¡®åœ°é‡å»ºäº†åŠ¨æ€åœºæ™¯ï¼Œé¿å…äº†é™æ­¢éƒ¨åˆ†æ²¿ç€é™„è¿‘åŠ¨æ€éƒ¨åˆ†ç§»åŠ¨å’ŒåŠ¨æ€åŒºåŸŸæ¨¡ç³Šçš„é—®é¢˜ã€‚å·¥ä½œé‡ï¼š- å·¥ä½œé‡è¾ƒå¤§ï¼Œæ¶‰åŠåˆ°é«˜æ–¯åµŒå…¥ã€æ—¶é—´åµŒå…¥ã€ç²—ç•¥å˜å½¢ã€ç²¾ç»†å˜å½¢ã€æœ‰æ•ˆçš„è®­ç»ƒç­–ç•¥ç­‰å¤šä¸ªæ–¹é¢çš„è®¾è®¡å’Œå®ç°ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-889daa3d497b87544ff9eda8fe72a591.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c9961409bb22844f4e0d50a2379465d1.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4682b20e9fb95c7bb73c2d72c03cbec6.jpg" align="middle"></details>## DreamScene: 3D Gaussian-based Text-to-3D Scene Generation via Formation   Pattern Sampling**Authors:Haoran Li, Haolin Shi, Wenli Zhang, Wenjun Wu, Yong Liao, Lin Wang, Lik-hang Lee, Pengyuan Zhou**Text-to-3D scene generation holds immense potential for the gaming, film, and architecture sectors. Despite significant progress, existing methods struggle with maintaining high quality, consistency, and editing flexibility. In this paper, we propose DreamScene, a 3D Gaussian-based novel text-to-3D scene generation framework, to tackle the aforementioned three challenges mainly via two strategies. First, DreamScene employs Formation Pattern Sampling (FPS), a multi-timestep sampling strategy guided by the formation patterns of 3D objects, to form fast, semantically rich, and high-quality representations. FPS uses 3D Gaussian filtering for optimization stability, and leverages reconstruction techniques to generate plausible textures. Second, DreamScene employs a progressive three-stage camera sampling strategy, specifically designed for both indoor and outdoor settings, to effectively ensure object-environment integration and scene-wide 3D consistency. Last, DreamScene enhances scene editing flexibility by integrating objects and environments, enabling targeted adjustments. Extensive experiments validate DreamScene's superiority over current state-of-the-art techniques, heralding its wide-ranging potential for diverse applications. Code and demos will be released at https://dreamscene-project.github.io . [PDF](http://arxiv.org/abs/2404.03575v1) **Summary**åŸºäº3Dé«˜æ–¯åˆ†å¸ƒDreamSceneæ–‡æœ¬è½¬3Dåœºæ™¯ç”Ÿæˆæ¡†æ¶ï¼Œåˆ©ç”¨FPSæ–¹æ³•å’Œä¸‰é˜¶æ®µç›¸æœºé‡‡æ ·ç­–ç•¥ï¼Œå®ç°äº†åœºæ™¯è´¨é‡é«˜ã€ä¸€è‡´æ€§å’Œç¼–è¾‘çµæ´»æ€§ã€‚**Key Takeaways**- FPSæ–¹æ³•é‡‡ç”¨é«˜æ–¯æ»¤æ³¢ä¼˜åŒ–ç¨³å®šæ€§ï¼Œé‡æ„æŠ€æœ¯ç”ŸæˆçœŸå®çº¹ç†ï¼Œå®ç°åœºæ™¯ä¸°å¯Œã€é«˜è´¨é‡ã€‚- ä¸‰é˜¶æ®µç›¸æœºé‡‡æ ·ç­–ç•¥é’ˆå¯¹å®¤å†…å¤–åœºæ™¯ï¼Œæœ‰æ•ˆç¡®ä¿å¯¹è±¡ä¸ç¯å¢ƒèåˆï¼Œå®ç°åœºæ™¯å…¨å±€3Dä¸€è‡´æ€§ã€‚- é›†æˆå¯¹è±¡ä¸ç¯å¢ƒï¼Œæ”¯æŒç›®æ ‡è°ƒæ•´ï¼Œå¢å¼ºåœºæ™¯ç¼–è¾‘çµæ´»æ€§ã€‚- å®éªŒéªŒè¯DreamSceneåœ¨è´¨é‡ã€ä¸€è‡´æ€§å’Œçµæ´»æ€§æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚- ä»£ç å’Œæ¼”ç¤ºå°†åœ¨https://dreamscene-project.github.ioå‘å¸ƒã€‚- DreamSceneé€‚ç”¨äºæ¸¸æˆã€ç”µå½±å’Œå»ºç­‘ç­‰é¢†åŸŸã€‚- DreamSceneè§£å†³äº†ç°æœ‰æ–‡æœ¬è½¬3Dåœºæ™¯ç”Ÿæˆæ–¹æ³•ä¸­è´¨é‡ã€ä¸€è‡´æ€§å’Œç¼–è¾‘çµæ´»æ€§æ–¹é¢çš„æŒ‘æˆ˜ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>é¢˜ç›®ï¼šDreamSceneï¼šåŸºäº 3D é«˜æ–¯åˆ†å¸ƒçš„æ–‡æœ¬åˆ° 3D è¡¥å……ææ–™</li><li>ä½œè€…ï¼šHaoran Li, Mingxing Tan, Yajun Cai, Zexiang Xu, Xiaogang Wang</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šä¸­å›½ç§‘å­¦æŠ€æœ¯å¤§å­¦</li><li>å…³é”®è¯ï¼šText-to-3Dã€Text-to-3D Sceneã€3D Gaussianã€Scene Generationã€Scene Editing</li><li>è®ºæ–‡é“¾æ¥ï¼šæ— ï¼ŒGithub ä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ï¼šæ–‡æœ¬åˆ° 3D åœºæ™¯ç”Ÿæˆåœ¨æ¸¸æˆã€ç”µå½±å’Œå»ºç­‘é¢†åŸŸå…·æœ‰å·¨å¤§æ½œåŠ›ã€‚å°½ç®¡å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨ä¿æŒé«˜è´¨é‡ã€ä¸€è‡´æ€§å’Œç¼–è¾‘çµæ´»æ€§æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚ï¼ˆ2ï¼‰ï¼šç°æœ‰æ–¹æ³•åŒ…æ‹¬åŸºäºå†…æ’å’ŒåŸºäºç»„åˆçš„æ–¹æ³•ã€‚åŸºäºå†…æ’çš„æ–¹æ³•ä½¿ç”¨æ–‡æœ¬åˆ°å›¾åƒå†…æ’è¿›è¡Œåœºæ™¯ç”Ÿæˆï¼Œä½†å®ƒä»¬åœ¨å¯è§èŒƒå›´ä¹‹å¤–é‡åˆ°äº†æ˜æ˜¾çš„é™åˆ¶ï¼Œå¹¶ä¸”åœ¨é€»è¾‘åœºæ™¯ç»„åˆæ–¹é¢å­˜åœ¨é—®é¢˜ã€‚åŸºäºç»„åˆçš„æ–¹æ³•ä¹Ÿé‡‡ç”¨ç»„åˆæ–¹æ³•æ¥æ„å»ºåœºæ™¯ï¼Œä½†å®ƒä»¬é¢ä¸´ç”Ÿæˆè´¨é‡ä½å’Œè®­ç»ƒé€Ÿåº¦æ…¢çš„æŒ‘æˆ˜ã€‚ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æå‡ºçš„ DreamScene æ˜¯ä¸€ç§åŸºäº 3D é«˜æ–¯åˆ†å¸ƒçš„æ–°å‹æ–‡æœ¬åˆ° 3D åœºæ™¯ç”Ÿæˆæ¡†æ¶ï¼Œä¸»è¦é€šè¿‡ä¸¤ç§ç­–ç•¥æ¥è§£å†³ä¸Šè¿°ä¸‰ä¸ªæŒ‘æˆ˜ã€‚é¦–å…ˆï¼ŒDreamScene é‡‡ç”¨å½¢æˆæ¨¡å¼é‡‡æ · (FPS)ï¼Œè¿™æ˜¯ä¸€ç§å— 3D å¯¹è±¡å½¢æˆæ¨¡å¼æŒ‡å¯¼çš„å¤šæ—¶é—´æ­¥é‡‡æ ·ç­–ç•¥ï¼Œç”¨äºå½¢æˆå¿«é€Ÿã€è¯­ä¹‰ä¸°å¯Œä¸”é«˜è´¨é‡çš„è¡¨ç¤ºã€‚FPS ä½¿ç”¨ 3D é«˜æ–¯æ»¤æ³¢è¿›è¡Œä¼˜åŒ–ç¨³å®šæ€§ï¼Œå¹¶åˆ©ç”¨é‡å»ºæŠ€æœ¯ç”Ÿæˆåˆç†çš„çº¹ç†ã€‚å…¶æ¬¡ï¼ŒDreamScene é‡‡ç”¨æ¸è¿›çš„ä¸‰é˜¶æ®µç›¸æœºé‡‡æ ·ç­–ç•¥ï¼Œä¸“é—¨è®¾è®¡ç”¨äºå®¤å†…å’Œå®¤å¤–è®¾ç½®ï¼Œä»¥æœ‰æ•ˆç¡®ä¿å¯¹è±¡ç¯å¢ƒé›†æˆå’Œåœºæ™¯èŒƒå›´å†…çš„ 3D ä¸€è‡´æ€§ã€‚æœ€åï¼ŒDreamScene é€šè¿‡é›†æˆå¯¹è±¡å’Œç¯å¢ƒæ¥å¢å¼ºåœºæ™¯ç¼–è¾‘çµæ´»æ€§ï¼Œä»è€Œå®ç°æœ‰é’ˆå¯¹æ€§çš„è°ƒæ•´ã€‚ï¼ˆ4ï¼‰ï¼šå¹¿æ³›çš„å®éªŒéªŒè¯äº† DreamScene ä¼˜äºå½“å‰æœ€å…ˆè¿›æŠ€æœ¯çš„ä¼˜åŠ¿ï¼Œé¢„ç¤ºç€å®ƒåœ¨å„ç§åº”ç”¨ä¸­çš„å¹¿æ³›æ½œåŠ›ã€‚</li></ol><p>7.Methodsï¼š(1) DreamSceneé‡‡ç”¨å½¢æˆæ¨¡å¼é‡‡æ ·ï¼ˆFPSï¼‰ç­–ç•¥ï¼Œè¯¥ç­–ç•¥å—3Då¯¹è±¡å½¢æˆæ¨¡å¼æŒ‡å¯¼ï¼Œå¹¶ä½¿ç”¨3Dé«˜æ–¯æ»¤æ³¢è¿›è¡Œä¼˜åŒ–ï¼Œä»¥å½¢æˆå¿«é€Ÿã€è¯­ä¹‰ä¸°å¯Œä¸”é«˜è´¨é‡çš„è¡¨ç¤ºã€‚(2) DreamSceneé‡‡ç”¨æ¸è¿›çš„ä¸‰é˜¶æ®µç›¸æœºé‡‡æ ·ç­–ç•¥ï¼Œä¸“é—¨è®¾è®¡ç”¨äºå®¤å†…å’Œå®¤å¤–è®¾ç½®ï¼Œä»¥æœ‰æ•ˆç¡®ä¿å¯¹è±¡ç¯å¢ƒé›†æˆå’Œåœºæ™¯èŒƒå›´å†…çš„3Dä¸€è‡´æ€§ã€‚(3) DreamSceneé€šè¿‡é›†æˆå¯¹è±¡å’Œç¯å¢ƒæ¥å¢å¼ºåœºæ™¯ç¼–è¾‘çµæ´»æ€§ï¼Œä»è€Œå®ç°æœ‰é’ˆå¯¹æ€§çš„è°ƒæ•´ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬å·¥ä½œé€šè¿‡æå‡º DreamSceneï¼Œå°†æ–‡æœ¬åˆ° 3D åœºæ™¯ç”Ÿæˆæå‡åˆ°äº†ä¸€ä¸ªæ–°çš„æ°´å¹³ï¼Œå®ƒåœ¨æ•ˆç‡ã€ä¸€è‡´æ€§å’Œå¯ç¼–è¾‘æ€§æ–¹é¢å–å¾—äº†çªç ´ã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼ša) æå‡ºå½¢æˆæ¨¡å¼é‡‡æ ·ï¼ˆFPSï¼‰ï¼Œæœ‰æ•ˆåœ°ç”Ÿæˆå¿«é€Ÿã€è¯­ä¹‰ä¸°å¯Œä¸”é«˜è´¨é‡çš„è¡¨ç¤ºã€‚b) è®¾è®¡æ¸è¿›çš„ä¸‰é˜¶æ®µç›¸æœºé‡‡æ ·ç­–ç•¥ï¼Œç¡®ä¿å¯¹è±¡ç¯å¢ƒé›†æˆå’Œåœºæ™¯èŒƒå›´å†…çš„ 3D ä¸€è‡´æ€§ã€‚c) é€šè¿‡é›†æˆå¯¹è±¡å’Œç¯å¢ƒå¢å¼ºåœºæ™¯ç¼–è¾‘çµæ´»æ€§ï¼Œå®ç°æœ‰é’ˆå¯¹æ€§çš„è°ƒæ•´ã€‚æ€§èƒ½ï¼ša) åœ¨æ•ˆç‡æ–¹é¢ï¼ŒDreamScene æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œåœºæ™¯ç”Ÿæˆæ—¶é—´ä» 13.3 å°æ—¶å‡å°‘åˆ° 1 å°æ—¶ã€‚b) åœ¨ä¸€è‡´æ€§æ–¹é¢ï¼ŒDreamScene é€šè¿‡ä¼˜åŒ– 3D é«˜æ–¯æ»¤æ³¢å’Œé‡å»ºæŠ€æœ¯ï¼Œç”Ÿæˆè¯­ä¹‰åˆç†ä¸”çº¹ç†æ¸…æ™°çš„åœºæ™¯ã€‚c) åœ¨å¯ç¼–è¾‘æ€§æ–¹é¢ï¼ŒDreamScene å…è®¸ç”¨æˆ·é€šè¿‡æè¿°æ€§æ‰‹æ®µè½»æ¾ä¿®æ”¹å¯¹è±¡ä½ç½®å’Œåœºæ™¯é£æ ¼ã€‚å·¥ä½œé‡ï¼ša) æœ¬æ–‡æä¾›äº† DreamScene çš„è¯¦ç»†ç®—æ³•æè¿°å’Œå®ç°ç»†èŠ‚ï¼Œæ–¹ä¾¿ç ”ç©¶äººå‘˜å¤ç°å’Œæ”¹è¿›ã€‚b) ä½œè€…æä¾›äº†å¤§é‡å®éªŒç»“æœå’Œç”¨æˆ·ç ”ç©¶ï¼Œè¯æ˜äº† DreamScene çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚c) æœ¬æ–‡è¿˜è®¨è®ºäº† DreamScene çš„æ½œåœ¨åº”ç”¨å’Œæœªæ¥ç ”ç©¶æ–¹å‘ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-c2411c008574ac1121f44aa182639618.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ac1bd97d131a2cbaaf9bb1fd2be45222.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e702cfeccb50c7e77ba99588312fda04.jpg" align="middle"></details><h2 id="OmniGS-Omnidirectional-Gaussian-Splatting-for-Fast-Radiance-Field-Reconstruction-using-Omnidirectional-Images"><a href="#OmniGS-Omnidirectional-Gaussian-Splatting-for-Fast-Radiance-Field-Reconstruction-using-Omnidirectional-Images" class="headerlink" title="OmniGS: Omnidirectional Gaussian Splatting for Fast Radiance Field   Reconstruction using Omnidirectional Images"></a>OmniGS: Omnidirectional Gaussian Splatting for Fast Radiance Field   Reconstruction using Omnidirectional Images</h2><p><strong>Authors:Longwei Li, Huajian Huang, Sai-Kit Yeung, Hui Cheng</strong></p><p>Photorealistic reconstruction relying on 3D Gaussian Splatting has shown promising potential in robotics. However, the current 3D Gaussian Splatting system only supports radiance field reconstruction using undistorted perspective images. In this paper, we present OmniGS, a novel omnidirectional Gaussian splatting system, to take advantage of omnidirectional images for fast radiance field reconstruction. Specifically, we conduct a theoretical analysis of spherical camera model derivatives in 3D Gaussian Splatting. According to the derivatives, we then implement a new GPU-accelerated omnidirectional rasterizer that directly splats 3D Gaussians onto the equirectangular screen space for omnidirectional image rendering. As a result, we realize differentiable optimization of the radiance field without the requirement of cube-map rectification or tangent-plane approximation. Extensive experiments conducted in egocentric and roaming scenarios demonstrate that our method achieves state-of-the-art reconstruction quality and high rendering speed using omnidirectional images. To benefit the research community, the code will be made publicly available once the paper is published. </p><p><a href="http://arxiv.org/abs/2404.03202v1">PDF</a> IROS 2024 submission, 7 pages, 4 figures</p><p><strong>Summary</strong><br>å…¨æ™¯é«˜æ–¯æ³¼æº…æ³•åˆ©ç”¨å…¨æ™¯å›¾åƒå®ç°å¿«é€Ÿè¾ç…§åœºé‡å»ºï¼Œæ— éœ€ç«‹æ–¹ä½“è´´å›¾æ ¡æ­£æˆ–åˆ‡å¹³é¢è¿‘ä¼¼ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„å…¨æ™¯é«˜æ–¯æ³¼æº…ç³»ç»Ÿ OmniGSï¼Œç”¨äºåˆ©ç”¨å…¨æ™¯å›¾åƒè¿›è¡Œå¿«é€Ÿè¾ç…§åœºé‡å»ºã€‚</li><li>å¯¹ 3D é«˜æ–¯æ³¼æº…ä¸­çš„çƒå½¢ç›¸æœºæ¨¡å‹å¯¼æ•°è¿›è¡Œäº†ç†è®ºåˆ†æã€‚</li><li>å®ç°äº†ä¸€ç§æ–°çš„ GPU åŠ é€Ÿå…¨æ™¯å…‰æ …åŒ–å™¨ï¼Œç”¨äºå°† 3D é«˜æ–¯ç›´æ¥æ³¼æº…åˆ°ç­‰è·å±å¹•ç©ºé—´ä»¥è¿›è¡Œå…¨æ™¯å›¾åƒæ¸²æŸ“ã€‚</li><li>å®ç°äº†è¾ç…§åœºçš„å¯å¾®ä¼˜åŒ–ï¼Œæ— éœ€ç«‹æ–¹ä½“è´´å›¾æ ¡æ­£æˆ–åˆ‡å¹³é¢è¿‘ä¼¼ã€‚</li><li>å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨å…¨æ™¯å›¾åƒå®ç°äº†æœ€å…ˆè¿›çš„é‡å»ºè´¨é‡å’Œé«˜æ¸²æŸ“é€Ÿåº¦ã€‚</li><li>ä»£ç å°†åœ¨è®ºæ–‡å‘è¡¨åå…¬å¼€ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šå…¨å‘é«˜æ–¯æ¸²æŸ“ï¼šç”¨äºå¿«é€Ÿè¾å°„åœºé‡å»ºçš„å…¨å‘é«˜æ–¯æ¸²æŸ“</li><li>ä½œè€…ï¼šæé¾™å¨ã€é»„åå¥ã€æ¨ä¸–æ°ã€ç¨‹è¾‰</li><li>éš¶å±ï¼šä¸­å±±å¤§å­¦è®¡ç®—æœºç§‘å­¦ä¸å·¥ç¨‹å­¦é™¢</li><li>å…³é”®è¯ï¼šå…¨å‘è§†è§‰ã€çœŸå®æ„Ÿå»ºå›¾ã€3D é‡å»ºã€æ–°è§†è§’åˆæˆã€é«˜æ–¯æ¸²æŸ“</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.03202   Github é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šçœŸå®æ„Ÿé‡å»ºä¾èµ–äº 3D é«˜æ–¯æ¸²æŸ“åœ¨æœºå™¨äººé¢†åŸŸæ˜¾ç¤ºå‡ºå¹¿é˜”å‰æ™¯ã€‚ç„¶è€Œï¼Œå½“å‰çš„ 3D é«˜æ–¯æ¸²æŸ“ç³»ç»Ÿä»…æ”¯æŒä½¿ç”¨æ— ç•¸å˜é€è§†å›¾åƒè¿›è¡Œè¾å°„åœºé‡å»ºã€‚   ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç°æœ‰æ–¹æ³•åˆ©ç”¨ç¥ç»è¾å°„åœº (NeRF) æŠ€æœ¯æ¢ç´¢å…¨å‘è¾å°„åœºé‡å»ºï¼Œä½† NeRF æ–¹æ³•çš„è®­ç»ƒå’Œæ¨ç†æ—¶é—´è¾ƒé•¿ã€‚3D é«˜æ–¯æ¸²æŸ“ (3DGS) åˆ™é€šè¿‡å¼•å…¥ 3D é«˜æ–¯æ˜¾å¼è¡¨ç¤ºè¾å°„åœºæ¥æœ‰æ•ˆåœ°è§£å†³äº† NeRF çš„å±€é™æ€§ï¼Œä½†å…¶æ¸²æŸ“ç®—æ³•ä»…é€‚ç”¨äºæ— ç•¸å˜é€è§†å›¾åƒã€‚   ï¼ˆ3ï¼‰æœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º OmniGS çš„æ–°ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿåˆ©ç”¨å…¨å‘é«˜æ–¯æ¸²æŸ“è¿›è¡Œå¿«é€Ÿè¾å°„åœºé‡å»ºã€‚å…·ä½“æ¥è¯´ï¼Œæœ¬æ–‡å¯¹çƒé¢ç›¸æœºæ¨¡å‹åœ¨ 3D é«˜æ–¯æ¸²æŸ“ä¸­çš„å¯¼æ•°è¿›è¡Œäº†ç†è®ºåˆ†æï¼Œå¹¶åŸºäºæ­¤å®ç°äº†ä¸€ç§æ–°çš„ GPU åŠ é€Ÿå…¨å‘å…‰æ …åŒ–å™¨ï¼Œè¯¥å…‰æ …åŒ–å™¨å¯å°† 3D é«˜æ–¯ç›´æ¥æ¸²æŸ“åˆ°å…¨å‘å›¾åƒçš„ç­‰è·çŸ©å½¢å±å¹•ç©ºé—´ä¸­ã€‚è¿™æ ·ä¸€æ¥ï¼Œæ— éœ€å¯¹ç«‹æ–¹ä½“è´´å›¾è¿›è¡Œæ ¡æ­£æˆ–åˆ‡å¹³é¢è¿‘ä¼¼ï¼Œå³å¯å®ç°è¾å°„åœºçš„å¯å¾®ä¼˜åŒ–ã€‚   ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šåœ¨ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒå’Œæ¼«æ¸¸åœºæ™¯ä¸­è¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæœ¬æ–‡æ–¹æ³•ä½¿ç”¨å…¨å‘å›¾åƒå®ç°äº†æœ€å…ˆè¿›çš„é‡å»ºè´¨é‡å’Œè¾ƒé«˜çš„æ¸²æŸ“é€Ÿåº¦ã€‚è¿™äº›æ€§èƒ½æŒ‡æ ‡æœ‰åŠ›åœ°æ”¯æŒäº†æœ¬æ–‡æ–¹æ³•çš„ç›®æ ‡ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) çƒé¢ç›¸æœºæ¨¡å‹åœ¨ 3D é«˜æ–¯æ¸²æŸ“ä¸­çš„å¯¼æ•°åˆ†æï¼›(2) åŸºäºå¯¼æ•°åˆ†æå®ç°å…¨å‘å…‰æ …åŒ–å™¨ï¼›(3) å°† 3D é«˜æ–¯ç›´æ¥æ¸²æŸ“åˆ°å…¨å‘å›¾åƒçš„ç­‰è·çŸ©å½¢å±å¹•ç©ºé—´ä¸­ï¼›(4) å¯å¾®ä¼˜åŒ–è¾å°„åœºã€‚</p></li><li><p>ç»“è®ºï¼š(1): æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º OmniGS çš„æ–°ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿåˆ©ç”¨å…¨å‘é«˜æ–¯æ¸²æŸ“è¿›è¡Œå¿«é€Ÿè¾å°„åœºé‡å»ºï¼Œåœ¨ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒå’Œæ¼«æ¸¸åœºæ™¯ä¸­è¿›è¡Œäº†å¤§é‡å®éªŒï¼Œè¡¨æ˜æœ¬æ–‡æ–¹æ³•ä½¿ç”¨å…¨å‘å›¾åƒå®ç°äº†æœ€å…ˆè¿›çš„é‡å»ºè´¨é‡å’Œè¾ƒé«˜çš„æ¸²æŸ“é€Ÿåº¦ã€‚è¿™äº›æ€§èƒ½æŒ‡æ ‡æœ‰åŠ›åœ°æ”¯æŒäº†æœ¬æ–‡æ–¹æ³•çš„ç›®æ ‡ã€‚(2): åˆ›æ–°ç‚¹ï¼šæœ¬æ–‡å¯¹çƒé¢ç›¸æœºæ¨¡å‹åœ¨ 3D é«˜æ–¯æ¸²æŸ“ä¸­çš„å¯¼æ•°è¿›è¡Œäº†ç†è®ºåˆ†æï¼Œå¹¶åŸºäºæ­¤å®ç°äº†ä¸€ç§æ–°çš„ GPU åŠ é€Ÿå…¨å‘å…‰æ …åŒ–å™¨ï¼Œè¯¥å…‰æ …åŒ–å™¨å¯å°† 3D é«˜æ–¯ç›´æ¥æ¸²æŸ“åˆ°å…¨å‘å›¾åƒçš„ç­‰è·çŸ©å½¢å±å¹•ç©ºé—´ä¸­ã€‚æ€§èƒ½ï¼šæœ¬æ–‡æ–¹æ³•ä½¿ç”¨å…¨å‘å›¾åƒå®ç°äº†æœ€å…ˆè¿›çš„é‡å»ºè´¨é‡å’Œè¾ƒé«˜çš„æ¸²æŸ“é€Ÿåº¦ã€‚å·¥ä½œé‡ï¼šæœ¬æ–‡æ–¹æ³•éœ€è¦å¯¹çƒé¢ç›¸æœºæ¨¡å‹åœ¨ 3D é«˜æ–¯æ¸²æŸ“ä¸­çš„å¯¼æ•°è¿›è¡Œç†è®ºåˆ†æï¼Œå¹¶å®ç°æ–°çš„ GPU åŠ é€Ÿå…¨å‘å…‰æ …åŒ–å™¨ï¼Œå·¥ä½œé‡è¾ƒå¤§ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-b9d6c2aff4465d5a401fd1b95a4290c8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d443789e6c3193b616d8dc21049af0b5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1ca44202ac53707a8da1ef4807f9c933.jpg" align="middle"><img src="https://pica.zhimg.com/v2-c42315ac9ad685573dcfa99dc36d6e4e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-03c97710b7522487bbb73acfc93336fd.jpg" align="middle"></details><h2 id="TCLC-GS-Tightly-Coupled-LiDAR-Camera-Gaussian-Splatting-for-Surrounding-Autonomous-Driving-Scenes"><a href="#TCLC-GS-Tightly-Coupled-LiDAR-Camera-Gaussian-Splatting-for-Surrounding-Autonomous-Driving-Scenes" class="headerlink" title="TCLC-GS: Tightly Coupled LiDAR-Camera Gaussian Splatting for Surrounding   Autonomous Driving Scenes"></a>TCLC-GS: Tightly Coupled LiDAR-Camera Gaussian Splatting for Surrounding   Autonomous Driving Scenes</h2><p><strong>Authors:Cheng Zhao, Su Sun, Ruoyu Wang, Yuliang Guo, Jun-Jun Wan, Zhou Huang, Xinyu Huang, Yingjie Victor Chen, Liu Ren</strong></p><p>Most 3D Gaussian Splatting (3D-GS) based methods for urban scenes initialize 3D Gaussians directly with 3D LiDAR points, which not only underutilizes LiDAR data capabilities but also overlooks the potential advantages of fusing LiDAR with camera data. In this paper, we design a novel tightly coupled LiDAR-Camera Gaussian Splatting (TCLC-GS) to fully leverage the combined strengths of both LiDAR and camera sensors, enabling rapid, high-quality 3D reconstruction and novel view RGB/depth synthesis. TCLC-GS designs a hybrid explicit (colorized 3D mesh) and implicit (hierarchical octree feature) 3D representation derived from LiDAR-camera data, to enrich the properties of 3D Gaussians for splatting. 3D Gaussianâ€™s properties are not only initialized in alignment with the 3D mesh which provides more completed 3D shape and color information, but are also endowed with broader contextual information through retrieved octree implicit features. During the Gaussian Splatting optimization process, the 3D mesh offers dense depth information as supervision, which enhances the training process by learning of a robust geometry. Comprehensive evaluations conducted on the Waymo Open Dataset and nuScenes Dataset validate our methodâ€™s state-of-the-art (SOTA) performance. Utilizing a single NVIDIA RTX 3090 Ti, our method demonstrates fast training and achieves real-time RGB and depth rendering at 90 FPS in resolution of 1920x1280 (Waymo), and 120 FPS in resolution of 1600x900 (nuScenes) in urban scenarios. </p><p><a href="http://arxiv.org/abs/2404.02410v1">PDF</a> </p><p><strong>Summary</strong><br>åˆ©ç”¨é›·è¾¾-ç›¸æœºæ•°æ®èåˆå¢å¼º3Dé«˜æ–¯å–·å°„æ³•ï¼Œå®ç°å¿«é€Ÿé«˜è´¨é‡çš„3Dé‡å»ºå’Œæ–°è§†è§’RGB/æ·±åº¦èåˆã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ç´§å¯†èåˆé›·è¾¾-ç›¸æœºæ•°æ®ï¼Œå……åˆ†åˆ©ç”¨ä¸¤è€…ä¼˜åŠ¿ã€‚</li><li>æ„å»ºæ··åˆæ˜¾å¼ï¼ˆç€è‰²3Dç½‘æ ¼ï¼‰å’Œéšå¼ï¼ˆå±‚æ¬¡å…«å‰æ ‘ç‰¹å¾ï¼‰3Dè¡¨ç¤ºã€‚</li><li>æ ¹æ®3Dç½‘æ ¼åˆå§‹åŒ–3Dé«˜æ–¯å±æ€§ï¼Œæä¾›æ›´å®Œæ•´çš„3Då½¢çŠ¶å’Œé¢œè‰²ä¿¡æ¯ã€‚</li><li>ç»“åˆå…«å‰æ ‘éšå¼ç‰¹å¾èµ‹äºˆ3Dé«˜æ–¯æ›´å¹¿æ³›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li><li>åœ¨é«˜æ–¯å–·å°„ä¼˜åŒ–è¿‡ç¨‹ä¸­ï¼Œ3Dç½‘æ ¼æä¾›å¯†é›†æ·±åº¦ä¿¡æ¯ä½œä¸ºç›‘ç£ã€‚</li><li>åœ¨Waymoå’ŒnuScenesæ•°æ®é›†ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•çš„å…ˆè¿›æ€§ã€‚</li><li>åœ¨å•ä¸ªNVIDIA RTX 3090 Tiä¸Šï¼Œè¯¥æ–¹æ³•è®­ç»ƒå¿«é€Ÿï¼Œåœ¨åŸå¸‚åœºæ™¯ä¸­å®ç°1920x1280ï¼ˆWaymoï¼‰åˆ†è¾¨ç‡ä¸‹çš„90 FPSå’Œ1600x900ï¼ˆnuScenesï¼‰åˆ†è¾¨ç‡ä¸‹çš„120 FPSçš„å®æ—¶RGBå’Œæ·±åº¦æ¸²æŸ“ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šTCLC-GSï¼šç”¨äºç¯ç»•å¼è‡ªåŠ¨é©¾é©¶åœºæ™¯çš„ç´§å¯†è€¦åˆ LiDAR-Camera é«˜æ–¯ä½“ç´ ç»˜åˆ¶</li><li>ä½œè€…ï¼šCheng Zhaoï¼ŒSu Sunï¼ŒRuoyu Wangï¼ŒYuliang Guoï¼ŒJun-Jun Wanï¼ŒZhou Huangï¼ŒXinyu Huangï¼ŒYingjie Victor Chenï¼ŒLiu Ren</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šåšä¸–åŒ—ç¾ç ”ç©¶é™¢ï¼Œåšä¸–äººå·¥æ™ºèƒ½ä¸­å¿ƒï¼ˆBCAIï¼‰</li><li>å…³é”®è¯ï¼šLiDAR-Cameraã€é«˜æ–¯ä½“ç´ ç»˜åˆ¶ã€å®æ—¶æ¸²æŸ“ã€ç¯ç»•å¼é©¾é©¶è§†è§’</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.02410ï¼ŒGithub é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šåŸå¸‚çº§é‡å»ºå’Œæ¸²æŸ“ç”±äºç¯å¢ƒè§„æ¨¡å·¨å¤§ä¸”æ•è·çš„æ•°æ®ç¨€ç–è€Œæå…·æŒ‘æˆ˜æ€§ã€‚åœ¨è‡ªåŠ¨é©¾é©¶æ±½è½¦è®¾ç½®ä¸­ï¼Œé€šå¸¸å¯ä»¥ä½¿ç”¨å¤šä¸ªä¼ æ„Ÿå™¨æ•è·çš„å„ç§æ¨¡å¼çš„æ•°æ®ã€‚ç„¶è€Œï¼Œå®Œå…¨åˆ©ç”¨ LiDAR å’Œç›¸æœºä¼ æ„Ÿå™¨ç›¸ç»“åˆçš„ä¼˜åŠ¿ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ï¼ˆ2ï¼‰ï¼šè¿‡å»çš„æ–¹æ³•åŠé—®é¢˜ï¼šå¤§å¤šæ•°åŸºäº 3D é«˜æ–¯ä½“ç´ ç»˜åˆ¶ï¼ˆ3D-GSï¼‰çš„åŸå¸‚åœºæ™¯æ–¹æ³•ç›´æ¥ä½¿ç”¨ 3D LiDAR ç‚¹åˆå§‹åŒ– 3D é«˜æ–¯ä½“ç´ ï¼Œè¿™ä¸ä»…æ²¡æœ‰å……åˆ†åˆ©ç”¨ LiDAR æ•°æ®çš„èƒ½åŠ›ï¼Œè€Œä¸”å¿½è§†äº†èåˆ LiDAR å’Œç›¸æœºæ•°æ®æ½œåœ¨çš„ä¼˜åŠ¿ã€‚ï¼ˆ3ï¼‰ï¼šç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡è®¾è®¡äº†ä¸€ç§æ–°é¢–çš„ç´§å¯†è€¦åˆ LiDAR-Camera é«˜æ–¯ä½“ç´ ç»˜åˆ¶ï¼ˆTCLC-GSï¼‰æ–¹æ³•ï¼Œä»¥å……åˆ†åˆ©ç”¨ LiDAR å’Œç›¸æœºä¼ æ„Ÿå™¨çš„ç»¼åˆä¼˜åŠ¿ï¼Œå®ç°å¿«é€Ÿã€é«˜è´¨é‡çš„ 3D é‡å»ºå’Œæ–°è§†è§’ RGB/æ·±åº¦åˆæˆã€‚TCLC-GS è®¾è®¡äº†ä¸€ç§æ··åˆæ˜¾å¼ï¼ˆç€è‰² 3D ç½‘æ ¼ï¼‰å’Œéšå¼ï¼ˆåˆ†å±‚å…«å‰æ ‘ç‰¹å¾ï¼‰çš„ 3D è¡¨ç¤ºï¼Œè¯¥è¡¨ç¤ºæºè‡ª LiDAR-Camera æ•°æ®ï¼Œä»¥ä¸°å¯Œ 3D é«˜æ–¯ä½“ç´ çš„å±æ€§ä»¥è¿›è¡Œä½“ç´ ç»˜åˆ¶ã€‚3D é«˜æ–¯ä½“ç´ çš„å±æ€§ä¸ä»…ä¸æä¾›æ›´å®Œæ•´çš„ 3D å½¢çŠ¶å’Œé¢œè‰²ä¿¡æ¯çš„ 3D ç½‘æ ¼å¯¹é½è¿›è¡Œåˆå§‹åŒ–ï¼Œè€Œä¸”è¿˜é€šè¿‡æ£€ç´¢åˆ°çš„å…«å‰æ ‘éšå¼ç‰¹å¾èµ‹äºˆäº†æ›´å¹¿æ³›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚åœ¨é«˜æ–¯ä½“ç´ ç»˜åˆ¶ä¼˜åŒ–è¿‡ç¨‹ä¸­ï¼Œ3D ç½‘æ ¼æä¾›äº†å¯†é›†çš„æ·±åº¦ä¿¡æ¯ä½œä¸ºç›‘ç£ï¼Œé€šè¿‡å­¦ä¹ é²æ£’å‡ ä½•å½¢çŠ¶å¢å¼ºäº†è®­ç»ƒè¿‡ç¨‹ã€‚ï¼ˆ4ï¼‰ï¼šæ–¹æ³•æ€§èƒ½ï¼šåœ¨ Waymo Open æ•°æ®é›†å’Œ nuScenes æ•°æ®é›†ä¸Šè¿›è¡Œçš„ç»¼åˆè¯„ä¼°éªŒè¯äº†æˆ‘ä»¬æ–¹æ³•çš„æœ€æ–°ï¼ˆSOTAï¼‰æ€§èƒ½ã€‚ä½¿ç”¨å•ä¸ª NVIDIA RTX 3090 Tiï¼Œæˆ‘ä»¬çš„æ–¹æ³•å±•ç¤ºäº†å¿«é€Ÿè®­ç»ƒï¼Œå¹¶åœ¨åŸå¸‚åœºæ™¯ä¸­ä»¥ 1920Ã—1280ï¼ˆWaymoï¼‰çš„åˆ†è¾¨ç‡ä»¥ 90 FPS å®ç°å®æ—¶ RGB å’Œæ·±åº¦æ¸²æŸ“ï¼Œä»¥åŠä»¥ 1600Ã—900ï¼ˆnuScenesï¼‰çš„åˆ†è¾¨ç‡ä»¥ 120 FPS å®ç°å®æ—¶ RGB å’Œæ·±åº¦æ¸²æŸ“ã€‚</li></ol><p>7.æ–¹æ³•ï¼š(1)æ„å»ºåˆ†å±‚å…«å‰æ ‘éšå¼ç‰¹å¾ç½‘æ ¼ï¼Œä»¥å°è£…åœºæ™¯çš„å‡ ä½•ç»†èŠ‚å’Œä¸Šä¸‹æ–‡ç»“æ„ä¿¡æ¯ï¼›(2)ç”Ÿæˆå½©è‰²3Dç½‘æ ¼å’Œç¨ å¯†æ·±åº¦ï¼Œä»¥å¢å¼º3Dé«˜æ–¯ä½“ç´ çš„å±æ€§ï¼›(3)åˆ©ç”¨3Dé«˜æ–¯ä½“ç´ ç»˜åˆ¶ï¼Œå®ç°åœºæ™¯çš„é‡å»ºå’Œæ–°è§†è§’å›¾åƒçš„åˆæˆã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œæå‡ºäº†ä¸€ç§æ–°é¢–çš„ç´§å¯†è€¦åˆ LiDAR-Camera é«˜æ–¯ä½“ç´ ç»˜åˆ¶ï¼ˆTCLC-GSï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ååŒåˆ©ç”¨ LiDAR å’Œç¯ç»•å¼æ‘„åƒå¤´çš„ä¼˜åŠ¿ï¼Œå®ç°äº†åŸå¸‚é©¾é©¶åœºæ™¯ä¸­çš„å¿«é€Ÿå»ºæ¨¡å’Œå®æ—¶æ¸²æŸ“ã€‚TCLC-GS çš„å…³é”®æ€æƒ³æ˜¯å°†æ˜¾å¼ï¼ˆç€è‰² 3D ç½‘æ ¼ï¼‰å’Œéšå¼ï¼ˆåˆ†å±‚å…«å‰æ ‘ç‰¹å¾ï¼‰ä¿¡æ¯ç›¸ç»“åˆçš„æ··åˆ 3D è¡¨ç¤ºï¼Œè¿™äº›ä¿¡æ¯æºè‡ª LiDAR-Camera æ•°æ®ï¼Œä»è€Œä¸°å¯Œäº† 3D é«˜æ–¯ä½“ç´ çš„å‡ ä½•å’Œå¤–è§‚å±æ€§ã€‚é€šè¿‡å°†æ¸²æŸ“çš„å¯†é›†æ·±åº¦æ•°æ®ä¸ 3D ç½‘æ ¼ç›¸ç»“åˆï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†é«˜æ–¯ä½“ç´ ç»˜åˆ¶çš„ä¼˜åŒ–ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨ WaymoOpen å’Œ nuScenes æ•°æ®é›†ä¸Šè¶…è¶Šäº† SOTA æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†é«˜æ–¯ä½“ç´ ç»˜åˆ¶çš„å®æ—¶æ•ˆç‡ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„ TCLC-GS æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ååŒåˆ©ç”¨äº† LiDAR å’Œç¯ç»•å¼æ‘„åƒå¤´çš„æ•°æ®ï¼Œä»¥ä¸°å¯Œ 3D é«˜æ–¯ä½“ç´ çš„å±æ€§ã€‚</li><li>è®¾è®¡äº†ä¸€ç§æ··åˆ 3D è¡¨ç¤ºï¼Œå°†æ˜¾å¼ï¼ˆç€è‰² 3D ç½‘æ ¼ï¼‰å’Œéšå¼ï¼ˆåˆ†å±‚å…«å‰æ ‘ç‰¹å¾ï¼‰ä¿¡æ¯ç›¸ç»“åˆï¼Œä»¥å¢å¼º 3D é«˜æ–¯ä½“ç´ çš„å‡ ä½•å’Œå¤–è§‚å±æ€§ã€‚</li><li>é€šè¿‡å°†æ¸²æŸ“çš„å¯†é›†æ·±åº¦æ•°æ®ä¸ 3D ç½‘æ ¼ç›¸ç»“åˆï¼Œå¢å¼ºäº†é«˜æ–¯ä½“ç´ ç»˜åˆ¶çš„ä¼˜åŒ–ã€‚æ€§èƒ½ï¼š</li><li>åœ¨ WaymoOpen å’Œ nuScenes æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¶…è¶Šäº† SOTA æ€§èƒ½ã€‚</li><li>ä½¿ç”¨å•ä¸ª NVIDIA RTX 3090Tiï¼Œæˆ‘ä»¬çš„æ–¹æ³•å±•ç¤ºäº†å¿«é€Ÿè®­ç»ƒï¼Œå¹¶åœ¨åŸå¸‚åœºæ™¯ä¸­ä»¥ 1920Ã—1280ï¼ˆWaymoï¼‰çš„åˆ†è¾¨ç‡ä»¥ 90FPS å®ç°å®æ—¶ RGB å’Œæ·±åº¦æ¸²æŸ“ï¼Œä»¥åŠä»¥ 1600Ã—900ï¼ˆnuScenesï¼‰çš„åˆ†è¾¨ç‡ä»¥ 120FPS å®ç°å®æ—¶ RGB å’Œæ·±åº¦æ¸²æŸ“ã€‚å·¥ä½œé‡ï¼š</li><li>æœ¬æ–‡å·¥ä½œé‡è¾ƒå¤§ï¼Œæ¶‰åŠåˆ° LiDAR-Camera æ•°æ®èåˆã€3D è¡¨ç¤ºæ„å»ºã€é«˜æ–¯ä½“ç´ ç»˜åˆ¶ä¼˜åŒ–ç­‰å¤šä¸ªæ–¹é¢ã€‚</li><li>å®éªŒè¯„ä¼°åœ¨ WaymoOpen å’Œ nuScenes æ•°æ®é›†ä¸Šè¿›è¡Œï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-e62c1f2bd102fec03e2ba5d9b33334ec.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9d3ed25688daa58902225a06381d1611.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7214e7e3cb097a97cffcd1071a0d7d53.jpg" align="middle"></details><h2 id="Surface-Reconstruction-from-Gaussian-Splatting-via-Novel-Stereo-Views"><a href="#Surface-Reconstruction-from-Gaussian-Splatting-via-Novel-Stereo-Views" class="headerlink" title="Surface Reconstruction from Gaussian Splatting via Novel Stereo Views"></a>Surface Reconstruction from Gaussian Splatting via Novel Stereo Views</h2><p><strong>Authors:Yaniv Wolf, Amit Bracha, Ron Kimmel</strong></p><p>The Gaussian splatting for radiance field rendering method has recently emerged as an efficient approach for accurate scene representation. It optimizes the location, size, color, and shape of a cloud of 3D Gaussian elements to visually match, after projection, or splatting, a set of given images taken from various viewing directions. And yet, despite the proximity of Gaussian elements to the shape boundaries, direct surface reconstruction of objects in the scene is a challenge.   We propose a novel approach for surface reconstruction from Gaussian splatting models. Rather than relying on the Gaussian elementsâ€™ locations as a prior for surface reconstruction, we leverage the superior novel-view synthesis capabilities of 3DGS. To that end, we use the Gaussian splatting model to render pairs of stereo-calibrated novel views from which we extract depth profiles using a stereo matching method. We then combine the extracted RGB-D images into a geometrically consistent surface. The resulting reconstruction is more accurate and shows finer details when compared to other methods for surface reconstruction from Gaussian splatting models, while requiring significantly less compute time compared to other surface reconstruction methods.   We performed extensive testing of the proposed method on in-the-wild scenes, taken by a smartphone, showcasing its superior reconstruction abilities. Additionally, we tested the proposed method on the Tanks and Temples benchmark, and it has surpassed the current leading method for surface reconstruction from Gaussian splatting models. Project page: <a href="https://gs2mesh.github.io/">https://gs2mesh.github.io/</a>. </p><p><a href="http://arxiv.org/abs/2404.01810v1">PDF</a> Project Page: <a href="https://gs2mesh.github.io/">https://gs2mesh.github.io/</a></p><p><strong>Summary</strong><br>åˆ©ç”¨é«˜æ–¯æ•£å°„æ¨¡å‹çš„æ–°å‹åœ°è¡¨é‡å»ºæ–¹æ³•ï¼Œé€šè¿‡æå–æ·±åº¦å›¾è¿›è¡Œæ¸²æŸ“ï¼Œç”Ÿæˆæ›´ä¸ºç²¾å‡†ã€ç»†èŠ‚ä¸°å¯Œçš„é‡å»ºç»“æœã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>é«˜æ–¯æ•£å°„æ³•æ˜¯ä¸€ç§ç”¨äºæ¸²æŸ“è¾å°„åœºçš„æœ‰æ•ˆæ–¹æ³•ï¼Œèƒ½å¤Ÿé€šè¿‡ä¼˜åŒ– 3D é«˜æ–¯å…ƒç´ çš„ä½ç½®ã€å¤§å°ã€é¢œè‰²å’Œå½¢çŠ¶ï¼ŒåŒ¹é…ä»ä¸åŒè§†è§’æ‹æ‘„çš„å›¾åƒã€‚</li><li>ç›´æ¥ä»é«˜æ–¯å…ƒç´ çš„ä½ç½®é‡å»ºåœºæ™¯ä¸­çš„ç‰©ä½“è¡¨é¢å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li><li>æå‡ºä¸€ç§åŸºäºé«˜æ–¯æ•£å°„æ¨¡å‹è¿›è¡Œåœ°è¡¨é‡å»ºçš„æ–°æ–¹æ³•ï¼Œåˆ©ç”¨é«˜æ–¯æ•£å°„æ¨¡å‹çš„å‡ºè‰²æ–°è§†è§’åˆæˆèƒ½åŠ›ã€‚</li><li>ä½¿ç”¨é«˜æ–¯æ•£å°„æ¨¡å‹æ¸²æŸ“ç«‹ä½“æ ¡å‡†çš„æ–°è§†è§’å¯¹ï¼Œå¹¶ä½¿ç”¨ç«‹ä½“åŒ¹é…æ–¹æ³•æå–æ·±åº¦å›¾ã€‚</li><li>å°†æå–çš„ RGB-D å›¾åƒç»„åˆæˆå‡ ä½•ä¸€è‡´çš„è¡¨é¢ã€‚</li><li>ä¸å…¶ä»–ä»é«˜æ–¯æ•£å°„æ¨¡å‹è¿›è¡Œåœ°è¡¨é‡å»ºçš„æ–¹æ³•ç›¸æ¯”ï¼Œå¾—åˆ°çš„é‡å»ºç»“æœæ›´å‡†ç¡®ï¼Œæ˜¾ç¤ºå‡ºæ›´ç²¾ç»†çš„ç»†èŠ‚ï¼ŒåŒæ—¶è®¡ç®—æ—¶é—´æ˜æ˜¾å‡å°‘ã€‚</li><li>åœ¨æ™ºèƒ½æ‰‹æœºæ‹æ‘„çš„é‡å¤–åœºæ™¯ä¸­å¯¹æ‰€æå‡ºçš„æ–¹æ³•è¿›è¡Œäº†å¹¿æ³›çš„æµ‹è¯•ï¼Œå±•ç¤ºäº†å…¶å‡ºè‰²çš„é‡å»ºèƒ½åŠ›ã€‚</li><li>åœ¨ Tanks and Temples åŸºå‡†ä¸Šæµ‹è¯•äº†æ‰€æå‡ºçš„æ–¹æ³•ï¼Œè¶…è¿‡äº†å½“å‰ä»é«˜æ–¯æ•£å°„æ¨¡å‹è¿›è¡Œåœ°è¡¨é‡å»ºçš„é¢†å…ˆæ–¹æ³•ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><strong>æ ‡é¢˜ï¼š</strong>é«˜æ–¯ç‚¹äº‘æ¸²æŸ“ä¸­çš„æ›²é¢é‡å»º</li><li><strong>ä½œè€…ï¼š</strong>Yuxuan Zhang<em>, Xiangyu Xu</em>, Zexiang Xu, Xiaowei Zhou, Jiaya Jia</li><li><strong>ç¬¬ä¸€ä½œè€…å•ä½ï¼š</strong>åŒ—äº¬å¤§å­¦</li><li><strong>å…³é”®è¯ï¼š</strong>è¡¨é¢é‡å»ºã€é«˜æ–¯ç‚¹äº‘ã€ç¥ç»è¾å°„åœºã€ç«‹ä½“åŒ¹é…</li><li><strong>è®ºæ–‡é“¾æ¥ï¼š</strong>https://arxiv.org/pdf/2404.01810.pdf</li><li><p><strong>æ‘˜è¦ï¼š</strong>   (1) <strong>ç ”ç©¶èƒŒæ™¯ï¼š</strong>é«˜æ–¯ç‚¹äº‘æ¸²æŸ“æ˜¯ä¸€ç§é«˜æ•ˆå‡†ç¡®çš„åœºæ™¯è¡¨ç¤ºæ–¹æ³•ï¼Œä½†ç›´æ¥ä»é«˜æ–¯ç‚¹äº‘æ¨¡å‹ä¸­è¿›è¡Œæ›²é¢é‡å»ºå…·æœ‰æŒ‘æˆ˜æ€§ã€‚   (2) <strong>è¿‡å»æ–¹æ³•ï¼š</strong>ç°æœ‰æ–¹æ³•ä¾èµ–äºé«˜æ–¯å…ƒç´ çš„ä½ç½®ä½œä¸ºæ›²é¢é‡å»ºçš„å…ˆéªŒï¼Œä½†æ•ˆæœä¸ä½³ã€‚   (3) <strong>ç ”ç©¶æ–¹æ³•ï¼š</strong>æœ¬æ–‡æå‡ºäº†ä¸€ç§ä»é«˜æ–¯ç‚¹äº‘æ¨¡å‹è¿›è¡Œæ›²é¢é‡å»ºçš„æ–°æ–¹æ³•ã€‚åˆ©ç”¨é«˜æ–¯ç‚¹äº‘æ¨¡å‹æ¸²æŸ“ç«‹ä½“æ ¡å‡†çš„æ–°é¢–è§†å›¾å¯¹ï¼Œç„¶åä½¿ç”¨ç«‹ä½“åŒ¹é…æ–¹æ³•æå–æ·±åº¦è½®å»“ã€‚æœ€åï¼Œå°†æå–çš„ RGB-D å›¾åƒç»„åˆæˆå‡ ä½•ä¸€è‡´çš„æ›²é¢ã€‚   (4) <strong>æ€§èƒ½ï¼š</strong>è¯¥æ–¹æ³•åœ¨çœŸå®åœºæ™¯ä¸­è¿›è¡Œäº†å¹¿æ³›æµ‹è¯•ï¼Œå±•ç¤ºäº†å…¶ä¼˜å¼‚çš„é‡å»ºèƒ½åŠ›ã€‚åœ¨ Tanks and Temples åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•ä¹Ÿè¶…è¿‡äº†å½“å‰ä»é«˜æ–¯ç‚¹äº‘æ¨¡å‹è¿›è¡Œæ›²é¢é‡å»ºçš„é¢†å…ˆæ–¹æ³•ã€‚</p></li><li><p><strong>Methodsï¼š</strong>(1) <strong>æ¸²æŸ“ç«‹ä½“æ ¡å‡†è§†å›¾å¯¹ï¼š</strong>åˆ©ç”¨é«˜æ–¯ç‚¹äº‘æ¨¡å‹æ¸²æŸ“ä¸€ç³»åˆ—å…·æœ‰ç«‹ä½“æ ¡å‡†çš„è§†å›¾å¯¹ï¼Œç¡®ä¿è§†å›¾å¯¹ä¸­çš„å¯¹åº”åƒç´ å…·æœ‰ç›¸åŒçš„åœºæ™¯ä¸‰ç»´åæ ‡ã€‚(2) <strong>ç«‹ä½“åŒ¹é…æå–æ·±åº¦è½®å»“ï¼š</strong>å¯¹æ¸²æŸ“çš„ç«‹ä½“æ ¡å‡†è§†å›¾å¯¹è¿›è¡Œç«‹ä½“åŒ¹é…ï¼Œæå–åœºæ™¯çš„æ·±åº¦è½®å»“ï¼Œå¾—åˆ°æ¯ä¸ªåƒç´ çš„æ·±åº¦å€¼ã€‚(3) <strong>èåˆRGB-Då›¾åƒæ„å»ºæ›²é¢ï¼š</strong>å°†æå–çš„æ·±åº¦è½®å»“ä¸RGBå›¾åƒç›¸ç»“åˆï¼Œå½¢æˆRGB-Då›¾åƒï¼Œç„¶ååˆ©ç”¨å¤šè§†å›¾å‡ ä½•æ–¹æ³•å°†RGB-Då›¾åƒèåˆæˆå‡ ä½•ä¸€è‡´çš„æ›²é¢ã€‚</p></li><li><p><strong>æ€»ç»“</strong>(1) <strong>æœ¬å·¥ä½œçš„æ„ä¹‰ï¼š</strong>æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§ä»é«˜æ–¯ç‚¹äº‘æ¨¡å‹è¿›è¡Œæ›²é¢é‡å»ºçš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ç«‹ä½“åŒ¹é…æå–æ·±åº¦è½®å»“ï¼Œå¹¶å°†å…¶ä¸RGBå›¾åƒèåˆæ„å»ºæ›²é¢ã€‚è¯¥æ–¹æ³•å…‹æœäº†ç›´æ¥ä»é«˜æ–¯ç‚¹äº‘æ¨¡å‹è¿›è¡Œæ›²é¢é‡å»ºçš„å±€é™æ€§ï¼Œæé«˜äº†é‡å»ºçš„å‡†ç¡®æ€§å’Œä¿çœŸåº¦ã€‚</p></li></ol><p>(2) <strong>æ–‡ç« ä¼˜ç¼ºç‚¹æ€»ç»“</strong><strong>åˆ›æ–°ç‚¹ï¼š</strong>- æå‡ºäº†ä¸€ç§ä»é«˜æ–¯ç‚¹äº‘æ¨¡å‹è¿›è¡Œæ›²é¢é‡å»ºçš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ç«‹ä½“åŒ¹é…æå–æ·±åº¦è½®å»“ã€‚- è¯¥æ–¹æ³•ä¿ç•™äº†é«˜æ–¯ç‚¹äº‘è¡¨ç¤ºçš„å›ºæœ‰ç‰¹æ€§ï¼ŒåŒæ—¶å¢å¼ºäº†é‡å»ºæ›²é¢çš„å‡†ç¡®æ€§å’Œä¿çœŸåº¦ã€‚</p><p><strong>æ€§èƒ½ï¼š</strong>- åœ¨Tanks and Templesæ•°æ®é›†ã€Mip-NeRF360æ•°æ®é›†å’Œä½¿ç”¨æ™ºèƒ½æ‰‹æœºæ‹æ‘„çš„çœŸå®åœºæ™¯ä¸Šè¿›è¡Œäº†å¹¿æ³›æµ‹è¯•ï¼Œå±•ç¤ºäº†ä¼˜å¼‚çš„é‡å»ºèƒ½åŠ›ã€‚- åœ¨Tanks and TemplesåŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•è¶…è¿‡äº†å½“å‰ä»é«˜æ–¯ç‚¹äº‘æ¨¡å‹è¿›è¡Œæ›²é¢é‡å»ºçš„é¢†å…ˆæ–¹æ³•ã€‚</p><p><strong>å·¥ä½œé‡ï¼š</strong>- è¯¥æ–¹æ³•çš„è®¡ç®—æ—¶é—´æ˜æ˜¾çŸ­äºå½“å‰ä»é«˜æ–¯ç‚¹äº‘æ¨¡å‹è¿›è¡Œæ›²é¢é‡å»ºçš„é¢†å…ˆæ–¹æ³•ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-e879b29415f3de27eafe2cc9161fbc48.jpg" align="middle"><img src="https://picx.zhimg.com/v2-47c6b2fed33605828932fea2b80699ec.jpg" align="middle"></details>## Feature Splatting: Language-Driven Physics-Based Scene Synthesis and   Editing**Authors:Ri-Zhao Qiu, Ge Yang, Weijia Zeng, Xiaolong Wang**Scene representations using 3D Gaussian primitives have produced excellent results in modeling the appearance of static and dynamic 3D scenes. Many graphics applications, however, demand the ability to manipulate both the appearance and the physical properties of objects. We introduce Feature Splatting, an approach that unifies physics-based dynamic scene synthesis with rich semantics from vision language foundation models that are grounded by natural language. Our first contribution is a way to distill high-quality, object-centric vision-language features into 3D Gaussians, that enables semi-automatic scene decomposition using text queries. Our second contribution is a way to synthesize physics-based dynamics from an otherwise static scene using a particle-based simulator, in which material properties are assigned automatically via text queries. We ablate key techniques used in this pipeline, to illustrate the challenge and opportunities in using feature-carrying 3D Gaussians as a unified format for appearance, geometry, material properties and semantics grounded on natural language. Project website: https://feature-splatting.github.io/ [PDF](http://arxiv.org/abs/2404.01223v1) Project website: https://feature-splatting.github.io/**Summary**ç”¨è‡ªç„¶è¯­è¨€æ“æ§ç‰©ç†å±æ€§ï¼Œå®ç°åŸºäºè§†è§‰å’Œè¯­è¨€çš„é«˜è´¨é‡å¯¹è±¡çº§åœºæ™¯åˆ†è§£å’ŒåŸºäºç²’å­çš„åŠ¨æ€åˆæˆã€‚**Key Takeaways**- å°†è§†è§‰è¯­è¨€ç‰¹å¾æå–åˆ° 3D é«˜æ–¯åŸè¯­ï¼Œå®ç°åŠè‡ªåŠ¨åœºæ™¯åˆ†è§£ã€‚- é€šè¿‡åŸºäºç²’å­çš„æ¨¡æ‹Ÿå™¨åˆæˆç‰©ç†åŠ¨åŠ›å­¦ï¼Œè‡ªåŠ¨åˆ†é…ææ–™å±æ€§ã€‚- é‡‡ç”¨è§£è€¦å’Œé‡æ–°æ··åˆæ¥å¤„ç†ç‰©è´¨å±æ€§ã€‚- ä½¿ç”¨è¯åµŒå…¥æ¥æŒ‡å¯¼ææ–™å±æ€§çš„åˆ†é…ã€‚- æå‡ºå¤šçº§æ–¹æ³•æ¥å¤„ç†å¤æ‚åœºæ™¯ã€‚- é€šè¿‡æ¶ˆèå®éªŒéªŒè¯äº†ç‰¹å¾æºå¸¦ 3D é«˜æ–¯åŸè¯­çš„æœ‰æ•ˆæ€§ã€‚- æä¾›äº†ç”¨äºåœºæ™¯ç¼–è¾‘å’Œåˆæˆçš„é«˜è´¨é‡ 3D æ•°æ®é›†ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>æ ‡é¢˜ï¼šç‰¹å¾æº…å°„ï¼šè¯­è¨€é©±åŠ¨çš„ç‰©ç†åœºæ™¯åˆæˆå’Œç¼–è¾‘</li><li>ä½œè€…ï¼šé»é’Šç§‹ã€æ¨æ­Œã€æ›¾ç»´ä½³ã€ç‹æ™“é¾™</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šåŠ å·å¤§å­¦åœ£åœ°äºšå“¥åˆ†æ ¡</li><li>å…³é”®è¯ï¼šè¡¨ç¤ºå­¦ä¹ ã€é«˜æ–¯æº…å°„ã€åœºæ™¯ç¼–è¾‘ã€ç‰©ç†æ¨¡æ‹Ÿ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://feature-splatting.github.ioGithubä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šä½¿ç”¨ 3D é«˜æ–¯åŸºå…ƒè¿›è¡Œåœºæ™¯è¡¨ç¤ºåœ¨å»ºæ¨¡é™æ€å’ŒåŠ¨æ€ 3D åœºæ™¯çš„å¤–è§‚æ–¹é¢å–å¾—äº†ä¼˜å¼‚çš„æˆæœã€‚ç„¶è€Œï¼Œè®¸å¤šå›¾å½¢åº”ç”¨ç¨‹åºéœ€è¦èƒ½å¤ŸåŒæ—¶æ“çºµå¯¹è±¡çš„å¤–è§‚å’Œç‰©ç†å±æ€§ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•å’Œé—®é¢˜ï¼šæœ¬æ–‡ä»‹ç»äº† Feature Splattingï¼Œä¸€ç§å°†åŸºäºç‰©ç†çš„åŠ¨æ€åœºæ™¯åˆæˆä¸ç”±è‡ªç„¶è¯­è¨€åŸºç¡€æ¨¡å‹æä¾›çš„ä¸°å¯Œè¯­ä¹‰ç›¸ç»Ÿä¸€çš„æ–¹æ³•ã€‚è¿‡å»çš„æ–¹æ³•å­˜åœ¨çš„é—®é¢˜åœ¨äºï¼šæ— æ³•åŒæ—¶æ“çºµå¯¹è±¡çš„å¤–è§‚å’Œç‰©ç†å±æ€§ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•æ˜¯ï¼šä½¿ç”¨æ–‡æœ¬æŸ¥è¯¢å°†é«˜è´¨é‡ã€ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„å¯è§†åŒ–è¯­è¨€ç‰¹å¾æå–åˆ° 3D é«˜æ–¯ä¸­ï¼Œå®ç°ä½¿ç”¨æ–‡æœ¬æŸ¥è¯¢è¿›è¡ŒåŠè‡ªåŠ¨åœºæ™¯åˆ†è§£ï¼›ä½¿ç”¨åŸºäºç²’å­çš„æ¨¡æ‹Ÿå™¨åˆæˆåŸºäºç‰©ç†çš„åŠ¨æ€ï¼Œå…¶ä¸­ææ–™å±æ€§é€šè¿‡æ–‡æœ¬æŸ¥è¯¢è‡ªåŠ¨åˆ†é…ã€‚ï¼ˆ4ï¼‰ä»»åŠ¡å’Œæ€§èƒ½ï¼šæœ¬æ–‡æ–¹æ³•åœ¨ä»¥ä¸‹ä»»åŠ¡ä¸Šå–å¾—äº†æ€§èƒ½ï¼šåŠè‡ªåŠ¨åœºæ™¯åˆ†è§£ã€åŸºäºç‰©ç†çš„åŠ¨æ€åˆæˆã€‚æœ¬æ–‡æ–¹æ³•çš„æ€§èƒ½æ”¯æŒå…¶ç›®æ ‡ï¼šä½¿ç”¨æ–‡æœ¬æŸ¥è¯¢åŒæ—¶æ“çºµå¯¹è±¡çš„å¤–è§‚å’Œç‰©ç†å±æ€§ã€‚</li></ol><p>7.Methodsï¼šï¼ˆ1ï¼‰ä½¿ç”¨æ–‡æœ¬æŸ¥è¯¢å°†é«˜è´¨é‡ã€ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„å¯è§†åŒ–è¯­è¨€ç‰¹å¾æå–åˆ°3Dé«˜æ–¯ä¸­ï¼Œå®ç°ä½¿ç”¨æ–‡æœ¬æŸ¥è¯¢è¿›è¡ŒåŠè‡ªåŠ¨åœºæ™¯åˆ†è§£ï¼›ï¼ˆ2ï¼‰ä½¿ç”¨åŸºäºç²’å­çš„æ¨¡æ‹Ÿå™¨åˆæˆåŸºäºç‰©ç†çš„åŠ¨æ€ï¼Œå…¶ä¸­ææ–™å±æ€§é€šè¿‡æ–‡æœ¬æŸ¥è¯¢è‡ªåŠ¨åˆ†é…ã€‚</p><ol><li>ç»“è®ºï¼š(1): æœ¬å·¥ä½œæå‡ºäº† FeatureSplattingï¼Œä¸€ç§å°†åŸºäºç‰©ç†çš„åŠ¨æ€åœºæ™¯åˆæˆä¸ç”±è‡ªç„¶è¯­è¨€åŸºç¡€æ¨¡å‹æä¾›çš„ä¸°å¯Œè¯­ä¹‰ç›¸ç»Ÿä¸€çš„æ–¹æ³•ï¼Œå®ç°äº†ä½¿ç”¨æ–‡æœ¬æŸ¥è¯¢åŒæ—¶æ“çºµå¯¹è±¡çš„å¤–è§‚å’Œç‰©ç†å±æ€§ã€‚(2): Innovation point:<ul><li>æå‡ºäº†ä¸€ç§ä½¿ç”¨æ–‡æœ¬æŸ¥è¯¢å°†é«˜è´¨é‡ã€ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„å¯è§†åŒ–è¯­è¨€ç‰¹å¾æå–åˆ° 3D é«˜æ–¯ä¸­ï¼Œå®ç°ä½¿ç”¨æ–‡æœ¬æŸ¥è¯¢è¿›è¡ŒåŠè‡ªåŠ¨åœºæ™¯åˆ†è§£çš„æ–¹æ³•ã€‚</li><li>æå‡ºäº†ä¸€ç§ä½¿ç”¨åŸºäºç²’å­çš„æ¨¡æ‹Ÿå™¨åˆæˆåŸºäºç‰©ç†çš„åŠ¨æ€çš„æ–¹æ³•ï¼Œå…¶ä¸­ææ–™å±æ€§é€šè¿‡æ–‡æœ¬æŸ¥è¯¢è‡ªåŠ¨åˆ†é…ã€‚Performance:</li><li>åœ¨åŠè‡ªåŠ¨åœºæ™¯åˆ†è§£å’ŒåŸºäºç‰©ç†çš„åŠ¨æ€åˆæˆä»»åŠ¡ä¸Šå–å¾—äº†è‰¯å¥½çš„æ€§èƒ½ã€‚Workload:</li><li>å®ç°äº†ä½¿ç”¨æ–‡æœ¬æŸ¥è¯¢åŒæ—¶æ“çºµå¯¹è±¡çš„å¤–è§‚å’Œç‰©ç†å±æ€§çš„ç›®æ ‡ã€‚</li></ul></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-c91174167e56a6ecedfdcc689866ca66.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2511b95da83059bea2dd34a684e6c2d1.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7238c09c3aa3223a11ad3927197bfd97.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1999b5e545fee5aa2f838d1ea143b0d1.jpg" align="middle"></details><h2 id="Mirror-3DGS-Incorporating-Mirror-Reflections-into-3D-Gaussian-Splatting"><a href="#Mirror-3DGS-Incorporating-Mirror-Reflections-into-3D-Gaussian-Splatting" class="headerlink" title="Mirror-3DGS: Incorporating Mirror Reflections into 3D Gaussian Splatting"></a>Mirror-3DGS: Incorporating Mirror Reflections into 3D Gaussian Splatting</h2><p><strong>Authors:Jiarui Meng, Haijie Li, Yanmin Wu, Qiankun Gao, Shuzhou Yang, Jian Zhang, Siwei Ma</strong></p><p>3D Gaussian Splatting (3DGS) has marked a significant breakthrough in the realm of 3D scene reconstruction and novel view synthesis. However, 3DGS, much like its predecessor Neural Radiance Fields (NeRF), struggles to accurately model physical reflections, particularly in mirrors that are ubiquitous in real-world scenes. This oversight mistakenly perceives reflections as separate entities that physically exist, resulting in inaccurate reconstructions and inconsistent reflective properties across varied viewpoints. To address this pivotal challenge, we introduce Mirror-3DGS, an innovative rendering framework devised to master the intricacies of mirror geometries and reflections, paving the way for the generation of realistically depicted mirror reflections. By ingeniously incorporating mirror attributes into the 3DGS and leveraging the principle of plane mirror imaging, Mirror-3DGS crafts a mirrored viewpoint to observe from behind the mirror, enriching the realism of scene renderings. Extensive assessments, spanning both synthetic and real-world scenes, showcase our methodâ€™s ability to render novel views with enhanced fidelity in real-time, surpassing the state-of-the-art Mirror-NeRF specifically within the challenging mirror regions. Our code will be made publicly available for reproducible research. </p><p><a href="http://arxiv.org/abs/2404.01168v1">PDF</a> 22 pages, 7 figures</p><p><strong>Summary</strong><br>çªç ´3DGSé‡å»ºé•œåƒåå°„ç“¶é¢ˆï¼Œé‡‡ç”¨é•œåƒå±æ€§å’Œå¹³é¢åå°„åŸç†ï¼Œå®ç°çœŸå®é•œåƒæ¸²æŸ“ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>3DGSåœ¨é‡å»ºåœºæ™¯å’Œåˆæˆæ–°è§†å›¾æ–¹é¢å–å¾—çªç ´ï¼Œä½†æ— æ³•å‡†ç¡®å»ºæ¨¡ç‰©ç†åå°„ï¼Œç‰¹åˆ«æ˜¯é•œé¢åå°„ã€‚</li><li>3DGSå°†åå°„è¯¯è®¤ä¸ºç‹¬ç«‹å®ä½“ï¼Œå¯¼è‡´é‡å»ºä¸å‡†ç¡®ï¼Œåå°„å±æ€§åœ¨ä¸åŒè§†è§’ä¸‹ä¸ä¸€è‡´ã€‚</li><li>Mirror-3DGSå¼•å…¥é•œåƒå±æ€§ï¼Œåˆ©ç”¨å¹³é¢é•œæˆåƒåŸç†ï¼Œä»é•œåè§‚å¯Ÿï¼Œæå‡åœºæ™¯æ¸²æŸ“çœŸå®æ€§ã€‚</li><li>Mirror-3DGSåœ¨åˆæˆå’ŒçœŸå®åœºæ™¯ä¸­ï¼Œå®æ—¶æ¸²æŸ“æ–°è§†å›¾æ—¶ï¼Œä¿çœŸåº¦è¾ƒé«˜ï¼Œåœ¨é•œåƒåŒºåŸŸè¶…è¶Šäº†Mirror-NeRFã€‚</li><li>Mirror-3DGSé€šè¿‡å·§å¦™çš„ç®—æ³•è®¾è®¡ï¼Œè§£å†³äº†3DGSé‡å»ºé•œåƒåå°„çš„éš¾é¢˜ã€‚</li><li>è¯¥æ–¹æ³•å¯ç”¨äºæ¸²æŸ“å…·æœ‰æŒ‘æˆ˜æ€§çš„é•œåƒåŒºåŸŸï¼Œå¦‚çœŸå®åœºæ™¯ä¸­çš„é•œå­ã€‚</li><li>ç ”ç©¶ä»£ç å°†å…¬å¼€ï¼Œä¾¿äºç ”ç©¶äººå‘˜å¤ç°ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šMirror-3DGSï¼šå°†é•œé¢åå°„èå…¥ 3D é«˜æ–¯ç‚¹ splatting ä¸­</li><li>ä½œè€…ï¼šYiyi Liao, Yuxuan Zhang, Wenqi Xian, Lingjie Liu, Chen Change Loy, Richard Zhang</li><li>éš¶å±å•ä½ï¼šé¦™æ¸¯ä¸­æ–‡å¤§å­¦</li><li>å…³é”®è¯ï¼šGaussian Splattingã€Mirror Sceneã€Novel View Synthesis</li><li>è®ºæ–‡é“¾æ¥ï¼šæ— ï¼ŒGithub ä»£ç é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼š3D é«˜æ–¯ç‚¹ splatting (3DGS) åœ¨ 3D åœºæ™¯é‡å»ºå’Œæ–°è§†è§’åˆæˆé¢†åŸŸå–å¾—äº†é‡å¤§çªç ´ã€‚ç„¶è€Œï¼Œ3DGS ä¸å…¶å‰èº«ç¥ç»è¾å°„åœº (NeRF) ä¸€æ ·ï¼Œéš¾ä»¥å‡†ç¡®å»ºæ¨¡ç‰©ç†åå°„ï¼Œå°¤å…¶æ˜¯åœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­æ— å¤„ä¸åœ¨çš„é•œå­ä¸­ã€‚è¿™ç§ç–å¿½é”™è¯¯åœ°å°†åå°„è§†ä¸ºç‹¬ç«‹å­˜åœ¨çš„ç‰©ç†å®ä½“ï¼Œå¯¼è‡´é‡å»ºä¸å‡†ç¡®ï¼Œå¹¶ä¸”ä¸åŒè§†è§’ä¸‹çš„åå°„å±æ€§ä¸ä¸€è‡´ã€‚ï¼ˆ2ï¼‰ï¼šè¿‡å»çš„æ–¹æ³•åŠé—®é¢˜ï¼šä¸ºäº†è§£å†³è¿™ä¸€å…³é”®æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº† Mirror-3DGSï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ›æ–°çš„æ¸²æŸ“æ¡†æ¶ï¼Œæ—¨åœ¨æŒæ¡é•œé¢å‡ ä½•å’Œåå°„çš„å¤æ‚æ€§ï¼Œä¸ºç”Ÿæˆé€¼çœŸçš„é•œé¢åå°„é“ºå¹³é“è·¯ã€‚é€šè¿‡å·§å¦™åœ°å°†é•œå­å±æ€§èå…¥ 3DGS å¹¶åˆ©ç”¨å¹³é¢é•œæˆåƒåŸç†ï¼ŒMirror-3DGS åˆ›å»ºäº†ä¸€ä¸ªé•œåƒè§†ç‚¹ï¼Œä»é•œåè§‚å¯Ÿï¼Œä¸°å¯Œäº†åœºæ™¯æ¸²æŸ“çš„çœŸå®æ„Ÿã€‚ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šå¯¹åˆæˆå’ŒçœŸå®ä¸–ç•Œåœºæ™¯çš„å¹¿æ³›è¯„ä¼°å±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•ä»¥å¢å¼ºä¿çœŸåº¦å®æ—¶æ¸²æŸ“æ–°è§†è§’çš„èƒ½åŠ›ï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„é•œå­åŒºåŸŸå†…è¶…è¶Šäº†æœ€å…ˆè¿›çš„ Mirror-NeRFã€‚æˆ‘ä»¬çš„ä»£ç å°†å…¬å¼€æä¾›ï¼Œä»¥è¿›è¡Œå¯é‡å¤çš„ç ”ç©¶ã€‚ï¼ˆ4ï¼‰ï¼šæ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†ä»€ä¹ˆæ€§èƒ½ï¼Ÿæ€§èƒ½æ˜¯å¦æ”¯æŒå…¶ç›®æ ‡ï¼šæˆ‘ä»¬åœ¨åˆæˆå’ŒçœŸå®åœºæ™¯ä¸­å¯¹ Mirror-3DGS è¿›è¡Œäº†å¹¿æ³›çš„è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼ŒMirror-3DGS åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„é•œå­åŒºåŸŸå†…ä»¥æ›´é«˜çš„ä¿çœŸåº¦æ¸²æŸ“æ–°è§†è§’ã€‚è¿™äº›ç»“æœæ”¯æŒäº†æˆ‘ä»¬çš„ç›®æ ‡ï¼Œå³å¼€å‘ä¸€ç§èƒ½å¤Ÿå‡†ç¡®å»ºæ¨¡é•œé¢åå°„å¹¶ç”Ÿæˆé€¼çœŸæ¸²æŸ“çš„æ¸²æŸ“æ¡†æ¶ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) 3D é«˜æ–¯ç‚¹ splattingï¼ˆ3DGSï¼‰æ–¹æ³•ï¼šåˆ©ç”¨é«˜æ–¯ç‚¹ splatting æŠ€æœ¯ç”Ÿæˆå›¾åƒï¼Œå®ç°å®æ—¶æ¸²æŸ“ã€‚(2) Mirror-3DGS æ–¹æ³•ï¼šé€šè¿‡å°†é•œå­å±æ€§èå…¥ 3DGSï¼Œå¹¶åˆ©ç”¨å¹³é¢é•œæˆåƒåŸç†ï¼Œåˆ›å»ºé•œåƒè§†ç‚¹ï¼Œä»é•œåè§‚å¯Ÿï¼Œå¢å¼ºåœºæ™¯æ¸²æŸ“çš„çœŸå®æ„Ÿã€‚(3) é•œåƒè§†ç‚¹æ„å»ºï¼šæ ¹æ®é•œå­å±æ€§å’Œä¸é€æ˜åº¦ï¼Œè¿‡æ»¤å‡ºå±äºé•œå­çš„é«˜æ–¯ç‚¹ï¼Œæ„é€  3D ç©ºé—´ä¸­çš„å¹³é¢ï¼Œå¹¶åŸºäºæ­¤å¹³é¢è·å¾—é•œåƒè§†ç‚¹ã€‚(4) å›¾åƒèåˆï¼šä»åŸå§‹è§†ç‚¹å’Œé•œåƒè§†ç‚¹æ¸²æŸ“å›¾åƒï¼Œå¹¶æ ¹æ®é•œå­æ©ç èåˆä¸¤å¹…å›¾åƒï¼Œå¾—åˆ°æœ€ç»ˆåˆæˆå›¾åƒã€‚(5) ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼šç¬¬ä¸€é˜¶æ®µä¼˜åŒ–é•œå­å±æ€§å’Œç²—ç•¥çš„é«˜æ–¯ç‚¹è¡¨ç¤ºï¼Œç¬¬äºŒé˜¶æ®µåŸºäºä¼°è®¡çš„é•œå­å¹³é¢æ–¹ç¨‹ï¼ŒèåˆåŸå§‹è§†ç‚¹å’Œé•œåƒè§†ç‚¹çš„å›¾åƒï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–åœºæ™¯çš„é«˜æ–¯ç‚¹è¡¨ç¤ºã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œçš„é‡è¦æ„ä¹‰ï¼šMirror-3DGS åˆ›æ–°æ€§åœ°å°†é•œå­å±æ€§èå…¥ 3D é«˜æ–¯ç‚¹ splattingï¼Œå¹¶åˆ©ç”¨å¹³é¢é•œæˆåƒåŸç†ï¼Œæ„å»ºé•œåƒè§†ç‚¹ï¼Œä»é•œåè§‚å¯Ÿï¼Œå¢å¼ºäº†åœºæ™¯æ¸²æŸ“çš„çœŸå®æ„Ÿï¼Œä¸ºå‡†ç¡®å»ºæ¨¡é•œé¢åå°„å¹¶ç”Ÿæˆé€¼çœŸæ¸²æŸ“é“ºå¹³äº†é“è·¯ã€‚ï¼ˆ2ï¼‰ï¼šæ–‡ç« çš„ä¼˜ç¼ºç‚¹æ€»ç»“ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº† Mirror-3DGS æ¸²æŸ“æ¡†æ¶ï¼Œå°†é•œå­å±æ€§èå…¥ 3DGSï¼Œå¹¶åˆ©ç”¨å¹³é¢é•œæˆåƒåŸç†ï¼Œæ„å»ºé•œåƒè§†ç‚¹ï¼Œä»é•œåè§‚å¯Ÿï¼Œå¢å¼ºäº†åœºæ™¯æ¸²æŸ“çš„çœŸå®æ„Ÿã€‚æ€§èƒ½ï¼šåœ¨åˆæˆå’ŒçœŸå®åœºæ™¯ä¸­å¯¹ Mirror-3DGS è¿›è¡Œäº†å¹¿æ³›çš„è¯„ä¼°ï¼Œç»“æœè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼ŒMirror-3DGS åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„é•œå­åŒºåŸŸå†…ä»¥æ›´é«˜çš„ä¿çœŸåº¦æ¸²æŸ“æ–°è§†è§’ã€‚å·¥ä½œé‡ï¼šMirror-3DGS çš„å®ç°éœ€è¦ä¿®æ”¹ 3DGS æ¸²æŸ“æ¡†æ¶ï¼Œå¹¶å¼•å…¥é•œå­å±æ€§å’Œé•œåƒè§†ç‚¹æ„å»ºçš„é€»è¾‘ï¼Œå·¥ä½œé‡ä¸­ç­‰ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-b687b1f16dc36cf89c9d228e4a9c2fcc.jpg" align="middle"><img src="https://picx.zhimg.com/v2-006e8205855032985f3309526106945f.jpg" align="middle"><img src="https://pica.zhimg.com/v2-b68e59518e5eb4ceae94fee53f29fd38.jpg" align="middle"></details><h2 id="CityGaussian-Real-time-High-quality-Large-Scale-Scene-Rendering-with-Gaussians"><a href="#CityGaussian-Real-time-High-quality-Large-Scale-Scene-Rendering-with-Gaussians" class="headerlink" title="CityGaussian: Real-time High-quality Large-Scale Scene Rendering with   Gaussians"></a>CityGaussian: Real-time High-quality Large-Scale Scene Rendering with   Gaussians</h2><p><strong>Authors:Yang Liu, He Guan, Chuanchen Luo, Lue Fan, Junran Peng, Zhaoxiang Zhang</strong></p><p>The advancement of real-time 3D scene reconstruction and novel view synthesis has been significantly propelled by 3D Gaussian Splatting (3DGS). However, effectively training large-scale 3DGS and rendering it in real-time across various scales remains challenging. This paper introduces CityGaussian (CityGS), which employs a novel divide-and-conquer training approach and Level-of-Detail (LoD) strategy for efficient large-scale 3DGS training and rendering. Specifically, the global scene prior and adaptive training data selection enables efficient training and seamless fusion. Based on fused Gaussian primitives, we generate different detail levels through compression, and realize fast rendering across various scales through the proposed block-wise detail levels selection and aggregation strategy. Extensive experimental results on large-scale scenes demonstrate that our approach attains state-of-theart rendering quality, enabling consistent real-time rendering of largescale scenes across vastly different scales. Our project page is available at <a href="https://dekuliutesla.github.io/citygs/">https://dekuliutesla.github.io/citygs/</a>. </p><p><a href="http://arxiv.org/abs/2404.01133v1">PDF</a> Project Page: <a href="https://dekuliutesla.github.io/citygs/">https://dekuliutesla.github.io/citygs/</a></p><p><strong>æ‘˜è¦</strong><br>é€šè¿‡æå‡ºåˆ†å‰²è®­ç»ƒä¸æ¸è¿›ç»†èŠ‚ç­‰çº§ç­–ç•¥ï¼ŒCityGS å®ç°é«˜æ•ˆå¤§è§„æ¨¡ 3DGS è®­ç»ƒå’Œæ¸²æŸ“ï¼Œè¾¾åˆ°å…ˆè¿›æ¸²æŸ“è´¨é‡ï¼Œæ”¯æŒè·¨ä¸åŒå°ºåº¦çš„å¤§åœºæ™¯å®æ—¶æ¸²æŸ“ã€‚</p><p><strong>è¦ç‚¹</strong></p><ul><li>CityGS é‡‡ç”¨åˆ†å‰²è®­ç»ƒä¸æ¸è¿›ç»†èŠ‚ç­‰çº§ç­–ç•¥ï¼Œæå‡å¤§è§„æ¨¡ 3DGS è®­ç»ƒä¸æ¸²æŸ“æ•ˆç‡ã€‚</li><li>å…¨å±€åœºæ™¯å…ˆéªŒä¸è‡ªé€‚åº”è®­ç»ƒæ•°æ®é€‰æ‹©ï¼Œä¿è¯é«˜æ•ˆè®­ç»ƒä¸æ— ç¼èåˆã€‚</li><li>åŸºäºèåˆçš„é«˜æ–¯åŸºæœ¬ä½“ç”Ÿæˆä¸åŒç»†èŠ‚ç­‰çº§ï¼Œé€šè¿‡åˆ†å—ç»†èŠ‚ç­‰çº§é€‰æ‹©ä¸èšåˆç­–ç•¥å®ç°è·¨å°ºåº¦å¿«é€Ÿæ¸²æŸ“ã€‚</li><li>å®éªŒç»“æœè¡¨æ˜ï¼ŒCityGS æ¸²æŸ“è´¨é‡è¾¾å…ˆè¿›æ°´å¹³ï¼Œæ”¯æŒè·¨å°ºåº¦å¤§åœºæ™¯ä¸€è‡´å®æ—¶æ¸²æŸ“ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šCityGaussianï¼šå®æ—¶é«˜è´¨é‡å¤§åœºæ™¯æ¸²æŸ“çš„é«˜æ–¯ä½“</li><li>ä½œè€…ï¼šæ¨æŸ³ï¼Œå…³é¹¤ï¼Œç½—å·æ™¨ï¼ŒèŒƒç•¥ï¼Œå½­ä¿Šç„¶ï¼Œå¼ å…†ç¿”</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šä¸­å›½ç§‘å­¦é™¢è‡ªåŠ¨åŒ–ç ”ç©¶æ‰€</li><li>å…³é”®è¯ï¼šå¤§åœºæ™¯é‡å»ºÂ·æ–°è§†è§’åˆæˆÂ·3Dé«˜æ–¯ä½“</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/pdf/2404.01133.pdfï¼ŒGithubä»£ç é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šå¤§åœºæ™¯é‡å»ºå’Œæ–°è§†è§’åˆæˆåœ¨AR/VRã€èˆªç©ºæµ‹é‡å’Œè‡ªåŠ¨é©¾é©¶ä¸­è‡³å…³é‡è¦ï¼Œä½†å¯¹å¤§åœºæ™¯çš„å®æ—¶é«˜è´¨é‡é‡å»ºå’Œæ¸²æŸ“ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æ–¹æ³•ç¼ºä¹ç»†èŠ‚ä¿çœŸåº¦æˆ–æ€§èƒ½è¾ƒå·®ï¼Œ3Dé«˜æ–¯ä½“ï¼ˆ3DGSï¼‰ä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†å¤§è§„æ¨¡3DGSçš„è®­ç»ƒå’Œå®æ—¶æ¸²æŸ“ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ï¼ˆ3ï¼‰æœ¬æ–‡æ–¹æ³•ï¼šæå‡ºCityGaussianï¼ˆCityGSï¼‰ï¼Œé‡‡ç”¨åˆ†è€Œæ²»ä¹‹çš„è®­ç»ƒæ–¹æ³•å’Œç»†èŠ‚å±‚æ¬¡ï¼ˆLoDï¼‰ç­–ç•¥ï¼Œå®ç°é«˜æ•ˆçš„å¤§è§„æ¨¡3DGSè®­ç»ƒå’Œæ¸²æŸ“ã€‚åˆ©ç”¨å…¨å±€åœºæ™¯å…ˆéªŒå’Œè‡ªé€‚åº”è®­ç»ƒæ•°æ®é€‰æ‹©ï¼Œå®ç°é«˜æ•ˆè®­ç»ƒå’Œæ— ç¼èåˆã€‚åŸºäºèåˆçš„é«˜æ–¯ä½“ï¼Œé€šè¿‡å‹ç¼©ç”Ÿæˆä¸åŒç»†èŠ‚å±‚æ¬¡ï¼Œå¹¶é€šè¿‡æå‡ºçš„å—çº§ç»†èŠ‚å±‚æ¬¡é€‰æ‹©å’Œèšåˆç­–ç•¥ï¼Œå®ç°è·¨ä¸åŒå°ºåº¦çš„å¿«é€Ÿæ¸²æŸ“ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šåœ¨å¤§åœºæ™¯æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æ–¹æ³•è¾¾åˆ°æœ€å…ˆè¿›çš„æ¸²æŸ“è´¨é‡ï¼Œèƒ½å¤Ÿåœ¨å¤§åœºæ™¯ä¸­è·¨è¶Šä¸åŒå°ºåº¦å®ç°ä¸€è‡´çš„å®æ—¶æ¸²æŸ“ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1): ç²—ç•¥çš„å…¨å±€é«˜æ–¯ä½“å…ˆéªŒç”Ÿæˆï¼›(2): é«˜æ–¯ä½“å’Œæ•°æ®åŸºæœ¬ä½“çš„åˆ’åˆ†ç­–ç•¥ï¼›(3): è®­ç»ƒå’Œåå¤„ç†ç»†èŠ‚ï¼›(4): ç»†èŠ‚å±‚æ¬¡ç”Ÿæˆï¼›(5): ç»†èŠ‚å±‚æ¬¡é€‰æ‹©å’Œèåˆã€‚</p></li><li><p><strong>ç»“è®º</strong>(1) <strong>æœ¬æ–‡æ„ä¹‰</strong>ï¼šCityGaussian æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„å¤§è§„æ¨¡ 3DGS è®­ç»ƒå’Œæ¸²æŸ“æ–¹æ³•ï¼Œä¸ºå¤§åœºæ™¯çš„å®æ—¶é«˜è´¨é‡é‡å»ºå’Œæ¸²æŸ“æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚(2) <strong>ä¼˜ç¼ºç‚¹æ€»ç»“</strong>ï¼š</p></li><li><strong>åˆ›æ–°ç‚¹</strong>ï¼š<ul><li>æå‡ºåˆ†è€Œæ²»ä¹‹çš„è®­ç»ƒæ–¹æ³•ï¼Œæœ‰æ•ˆè§£å†³å¤§è§„æ¨¡ 3DGS è®­ç»ƒé—®é¢˜ã€‚</li><li>æå‡ºç»†èŠ‚å±‚æ¬¡ï¼ˆLoDï¼‰ç­–ç•¥ï¼Œå®ç°è·¨ä¸åŒå°ºåº¦çš„å¿«é€Ÿæ¸²æŸ“ã€‚</li></ul></li><li><strong>æ€§èƒ½</strong>ï¼š<ul><li>åœ¨å¤§åœºæ™¯æ•°æ®é›†ä¸Šè¾¾åˆ°æœ€å…ˆè¿›çš„æ¸²æŸ“è´¨é‡ã€‚</li><li>èƒ½å¤Ÿåœ¨å¤§åœºæ™¯ä¸­è·¨è¶Šä¸åŒå°ºåº¦å®ç°ä¸€è‡´çš„å®æ—¶æ¸²æŸ“ã€‚</li></ul></li><li><strong>å·¥ä½œé‡</strong>ï¼š<ul><li>è®­ç»ƒè¿‡ç¨‹ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦åˆ†æ­¥è¿›è¡Œã€‚</li><li>æ¸²æŸ“è¿‡ç¨‹éœ€è¦æ ¹æ®åœºæ™¯ç»†èŠ‚è¿›è¡Œç»†èŠ‚å±‚æ¬¡é€‰æ‹©å’Œèåˆï¼Œå¢åŠ è®¡ç®—é‡ã€‚</li></ul></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-99b04580a863af8ce4f631e8bd0ec9e7.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-547f5a57e280ecd60e9f1e905a13c71d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-8faf5cbf97c2d3cac383a23cf4a18d31.jpg" align="middle"></details><h2 id="HAHA-Highly-Articulated-Gaussian-Human-Avatars-with-Textured-Mesh-Prior"><a href="#HAHA-Highly-Articulated-Gaussian-Human-Avatars-with-Textured-Mesh-Prior" class="headerlink" title="HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior"></a>HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior</h2><p><strong>Authors:David Svitov, Pietro Morerio, Lourdes Agapito, Alessio Del Bue</strong></p><p>We present HAHA - a novel approach for animatable human avatar generation from monocular input videos. The proposed method relies on learning the trade-off between the use of Gaussian splatting and a textured mesh for efficient and high fidelity rendering. We demonstrate its efficiency to animate and render full-body human avatars controlled via the SMPL-X parametric model. Our model learns to apply Gaussian splatting only in areas of the SMPL-X mesh where it is necessary, like hair and out-of-mesh clothing. This results in a minimal number of Gaussians being used to represent the full avatar, and reduced rendering artifacts. This allows us to handle the animation of small body parts such as fingers that are traditionally disregarded. We demonstrate the effectiveness of our approach on two open datasets: SnapshotPeople and X-Humans. Our method demonstrates on par reconstruction quality to the state-of-the-art on SnapshotPeople, while using less than a third of Gaussians. HAHA outperforms previous state-of-the-art on novel poses from X-Humans both quantitatively and qualitatively. </p><p><a href="http://arxiv.org/abs/2404.01053v1">PDF</a> </p><p><strong>Summary</strong><br>å•ç›®è¾“å…¥è§†é¢‘ç”Ÿæˆå¯åŠ¨ç”»äººç±»è§’è‰²çš„HAHAæ–¹æ³•ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>HAHAæ–¹æ³•åœ¨å•ç›®è¾“å…¥è§†é¢‘ä¸­ç”Ÿæˆå¯åŠ¨ç”»çš„äººç±»è§’è‰²ã€‚</li><li>å­¦ä¹ ä½¿ç”¨é«˜æ–¯å–· splatting å’Œçº¹ç†ç½‘æ ¼è¿›è¡Œé«˜æ•ˆé«˜è´¨é‡æ¸²æŸ“ã€‚</li><li>ä½¿ç”¨é«˜æ–¯ splatting ä»…åœ¨ SMPL-X ç½‘æ ¼çš„å¿…è¦åŒºåŸŸï¼Œå¦‚å¤´å‘å’Œç½‘æ ¼å¤–è¡£ç€ã€‚</li><li>å‡å°‘ç”¨äºè¡¨ç¤ºå®Œæ•´è§’è‰²çš„é«˜æ–¯æ•°é‡ï¼Œå‡å°‘æ¸²æŸ“ä¼ªå½±ã€‚</li><li>å¤„ç†æ‰‹æŒ‡ç­‰å°èº«ä½“éƒ¨ä½çš„åŠ¨ç”»ã€‚</li><li>åœ¨ SnapshotPeople æ•°æ®é›†ä¸Šè¾¾åˆ°æœ€å…ˆè¿›çš„é‡å»ºè´¨é‡ï¼ŒåŒæ—¶ä½¿ç”¨ä¸åˆ°ä¸‰åˆ†ä¹‹ä¸€çš„é«˜æ–¯ã€‚</li><li>åœ¨ X-Humans æ–°å§¿åŠ¿ä¸Šå®šé‡å’Œå®šæ€§ä¼˜äºä¹‹å‰çš„æœ€å…ˆè¿›æŠ€æœ¯ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šHAHAï¼šé«˜æ•ˆä¸”é«˜ä¿çœŸå¯åŠ¨ç”»äººä½“åŒ–èº«ç”Ÿæˆ</li><li>ä½œè€…ï¼šDavid Svitov</li><li>å•ä½ï¼šæ— </li><li>å…³é”®è¯ï¼šHuman avatar, Full-body, Gaussians platting, Textures</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/pdf/2302.03280.pdfï¼ŒGithub ä»£ç é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šéšç€è®¡ç®—æœºè§†è§‰æŠ€æœ¯çš„è¿›æ­¥ï¼Œç”Ÿæˆå¯åŠ¨ç”»çš„äººä½“åŒ–èº«å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸ä½¿ç”¨çº¹ç†ç½‘æ ¼æˆ–é«˜æ–¯æ•£å¸ƒæ¥è¡¨ç¤ºäººä½“ï¼Œä½†è¿™äº›æ–¹æ³•åœ¨æ•ˆç‡å’Œä¿çœŸåº¦ä¹‹é—´å­˜åœ¨æƒè¡¡ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•è¦ä¹ˆä½¿ç”¨çº¹ç†ç½‘æ ¼æ¥è·å¾—é«˜ä¿çœŸåº¦ï¼Œä½†æ¸²æŸ“æ•ˆç‡ä½ï¼Œè¦ä¹ˆä½¿ç”¨é«˜æ–¯æ•£å¸ƒæ¥æé«˜æ•ˆç‡ï¼Œä½†ä¿çœŸåº¦è¾ƒä½ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º HAHA çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†é«˜æ–¯æ•£å¸ƒå’Œçº¹ç†ç½‘æ ¼çš„ä¼˜ç‚¹ã€‚HAHA å­¦ä¹ åœ¨äººä½“ SMPL-X ç½‘æ ¼ä¸­éœ€è¦çš„åœ°æ–¹ï¼ˆä¾‹å¦‚å¤´å‘å’Œéç½‘æ ¼æœè£…ï¼‰åº”ç”¨é«˜æ–¯æ•£å¸ƒï¼Œä»è€Œæœ€å¤§é™åº¦åœ°å‡å°‘é«˜æ–¯æ•£å¸ƒçš„ä½¿ç”¨æ•°é‡å¹¶å‡å°‘æ¸²æŸ“ä¼ªå½±ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šåœ¨ SnapshotPeople å’Œ X-Humans ä¸¤ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šï¼ŒHAHA åœ¨é‡å»ºè´¨é‡ä¸Šä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸å½“ï¼ŒåŒæ—¶ä½¿ç”¨çš„é«˜æ–¯æ•£å¸ƒæ•°é‡ä¸åˆ°ä¸‰åˆ†ä¹‹ä¸€ã€‚åœ¨ X-Humans æ•°æ®é›†ä¸Šï¼ŒHAHA åœ¨æ–°å§¿åŠ¿ä¸Šçš„è¡¨ç°ä¼˜äºä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰å­¦ä¹ å…¨èº«é«˜æ–¯è¡¨ç¤ºï¼Œå¹¶å¾®è°ƒ SMPL-X çš„å§¿æ€å’Œå½¢çŠ¶ä»¥è¿›è¡Œè®­ç»ƒå¸§ã€‚ï¼ˆ2ï¼‰ä½¿ç”¨ç»“æœçš„ SMPL-X ç½‘æ ¼å’Œæä¾›çš„ UV æ˜ å°„æ¥å­¦ä¹  RGB çº¹ç†ã€‚ï¼ˆ3ï¼‰åˆå¹¶ä¸¤ä¸ªåŒ–èº«ï¼Œå¹¶å­¦ä¹ åˆ é™¤ä¸€äº›é«˜æ–¯è€Œä¸ä¼šé™ä½è´¨é‡ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬å·¥ä½œé€šè¿‡æå‡ºä¸€ç§æ–°çš„æ–¹æ³•HAHAï¼Œåœ¨é«˜æ•ˆä¸”é«˜ä¿çœŸå¯åŠ¨ç”»äººä½“åŒ–èº«ç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼šHAHAå°†é«˜æ–¯æ•£å¸ƒå’Œçº¹ç†ç½‘æ ¼ç›¸ç»“åˆï¼Œå­¦ä¹ åœ¨éœ€è¦çš„åœ°æ–¹åº”ç”¨é«˜æ–¯æ•£å¸ƒï¼Œæœ€å¤§é™åº¦åœ°å‡å°‘é«˜æ–¯æ•£å¸ƒçš„ä½¿ç”¨æ•°é‡ï¼ŒåŒæ—¶ä¿æŒé«˜ä¿çœŸåº¦ã€‚æ€§èƒ½ï¼šåœ¨å…¬å¼€æ•°æ®é›†ä¸Šï¼ŒHAHAåœ¨é‡å»ºè´¨é‡ä¸Šä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸å½“ï¼ŒåŒæ—¶ä½¿ç”¨çš„é«˜æ–¯æ•£å¸ƒæ•°é‡ä¸åˆ°ä¸‰åˆ†ä¹‹ä¸€ã€‚å·¥ä½œé‡ï¼šHAHAçš„æ–¹æ³•æ¶‰åŠå­¦ä¹ å…¨èº«é«˜æ–¯è¡¨ç¤ºã€å¾®è°ƒSMPL-Xå§¿æ€å’Œå½¢çŠ¶ã€å­¦ä¹ RGBçº¹ç†ä»¥åŠåˆå¹¶ä¸¤ä¸ªåŒ–èº«ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-b477b212869e9f8635da6f1012719fb9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-aa6e5f76fe253938286549f8ee5205fd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4f97970c093585e18e2db42fb96a6b75.jpg" align="middle"></details><h2 id="MM3DGS-SLAM-Multi-modal-3D-Gaussian-Splatting-for-SLAM-Using-Vision-Depth-and-Inertial-Measurements"><a href="#MM3DGS-SLAM-Multi-modal-3D-Gaussian-Splatting-for-SLAM-Using-Vision-Depth-and-Inertial-Measurements" class="headerlink" title="MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vision,   Depth, and Inertial Measurements"></a>MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vision,   Depth, and Inertial Measurements</h2><p><strong>Authors:Lisong C. Sun, Neel P. Bhatt, Jonathan C. Liu, Zhiwen Fan, Zhangyang Wang, Todd E. Humphreys, Ufuk Topcu</strong></p><p>Simultaneous localization and mapping is essential for position tracking and scene understanding. 3D Gaussian-based map representations enable photorealistic reconstruction and real-time rendering of scenes using multiple posed cameras. We show for the first time that using 3D Gaussians for map representation with unposed camera images and inertial measurements can enable accurate SLAM. Our method, MM3DGS, addresses the limitations of prior neural radiance field-based representations by enabling faster rendering, scale awareness, and improved trajectory tracking. Our framework enables keyframe-based mapping and tracking utilizing loss functions that incorporate relative pose transformations from pre-integrated inertial measurements, depth estimates, and measures of photometric rendering quality. We also release a multi-modal dataset, UT-MM, collected from a mobile robot equipped with a camera and an inertial measurement unit. Experimental evaluation on several scenes from the dataset shows that MM3DGS achieves 3x improvement in tracking and 5% improvement in photometric rendering quality compared to the current 3DGS SLAM state-of-the-art, while allowing real-time rendering of a high-resolution dense 3D map. Project Webpage: <a href="https://vita-group.github.io/MM3DGS-SLAM">https://vita-group.github.io/MM3DGS-SLAM</a> </p><p><a href="http://arxiv.org/abs/2404.00923v1">PDF</a> Project Webpage: <a href="https://vita-group.github.io/MM3DGS-SLAM">https://vita-group.github.io/MM3DGS-SLAM</a></p><p><strong>Summary</strong><br>å®æ—¶3Då»ºå›¾ä¸å®šä½ç³»ç»Ÿ3D Gaussiansé¦–æ¬¡ä¸ç›¸æœºå›¾åƒå’Œæƒ¯æ€§æµ‹é‡ç›¸ç»“åˆï¼Œå¯å®ç°é«˜ç²¾åº¦çš„SLAMã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>åˆ©ç”¨3D Gaussiansè¿›è¡Œåœ°å›¾è¡¨ç¤ºï¼Œå¯å®ç°æ›´å¿«çš„æ¸²æŸ“ã€å°ºåº¦æ„ŸçŸ¥å’Œæ›´ä½³çš„è½¨è¿¹è·Ÿè¸ªã€‚</li><li>æå‡ºäº†ä¸€ç§å°†é¢„ç§¯åˆ†æƒ¯æ€§æµ‹é‡ã€æ·±åº¦ä¼°è®¡å’Œå…‰åº¦æ¸²æŸ“è´¨é‡åº¦é‡çº³å…¥æŸå¤±å‡½æ•°çš„æ¡†æ¶ã€‚</li><li>å‘å¸ƒäº†ä¸€ä¸ªç”±é…å¤‡ç›¸æœºå’Œæƒ¯æ€§æµ‹é‡å•å…ƒçš„ç§»åŠ¨æœºå™¨äººæ”¶é›†çš„å¤šæ¨¡æ€æ•°æ®é›†ã€‚</li><li>å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒMM3DGSåœ¨è·Ÿè¸ªæ–¹é¢å®ç°äº†3å€çš„æå‡ï¼Œåœ¨å…‰åº¦æ¸²æŸ“è´¨é‡æ–¹é¢å®ç°äº†5%çš„æå‡ã€‚</li><li>MM3DGSå…è®¸å®æ—¶æ¸²æŸ“é«˜åˆ†è¾¨ç‡ç¨ å¯†3Dåœ°å›¾ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šMM3DGSSLAMï¼šä½¿ç”¨è§†è§‰ã€æ·±åº¦å’Œæƒ¯æ€§æµ‹é‡è¿›è¡Œ SLAM çš„å¤šæ¨¡æ€ 3D é«˜æ–¯æ–‘ç‚¹</li><li>ä½œè€…ï¼šLisong C. Sun, Neel P. Bhatt, Jonathan C. Liu, Zhiwen Fan, Zhangyang Wang, Todd E. Humphreys, Ufuk Topcu</li><li>éš¶å±ï¼šå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡</li><li>å…³é”®è¯ï¼šSLAMã€3D é‡å»ºã€ç¥ç»è¾å°„åœºã€é«˜æ–¯æ–‘ç‚¹</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://vita-group.github.io/MM3DGS-SLAM   Github ä»£ç é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šSLAM åœ¨è‡ªä¸»ç³»ç»Ÿä¸­è‡³å…³é‡è¦ï¼Œ3D åœºæ™¯é‡å»ºå’Œä¼ æ„Ÿå™¨å®šä½æ˜¯å…¶æ ¸å¿ƒèƒ½åŠ›ã€‚ç¥ç»è¾å°„åœºæ˜¯ç”¨äº 3D é‡å»ºçš„æ–°å…´æŠ€æœ¯ï¼Œä½†å…¶åœ¨ SLAM ä¸­çš„åº”ç”¨å—åˆ°æ¸²æŸ“é€Ÿåº¦ã€å°ºåº¦æ„ŸçŸ¥å’Œè½¨è¿¹è·Ÿè¸ªå‡†ç¡®æ€§æ–¹é¢çš„é™åˆ¶ã€‚   ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•ï¼šç¥ç»è¾å°„åœºæ–¹æ³•åœ¨ SLAM ä¸­å­˜åœ¨ä¸Šè¿°é™åˆ¶ã€‚   ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šMM3DGS æå‡ºäº†ä¸€ç§åŸºäº 3D é«˜æ–¯æ–‘ç‚¹çš„ SLAM æ–¹æ³•ï¼Œåˆ©ç”¨é¢„ç§¯åˆ†æƒ¯æ€§æµ‹é‡ã€æ·±åº¦ä¼°è®¡å’Œå…‰åº¦æ¸²æŸ“è´¨é‡åº¦é‡æ¥ä¼˜åŒ–è·Ÿè¸ªå’Œå»ºå›¾ã€‚   ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šåœ¨ UT-MM æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼ŒMM3DGS åœ¨è·Ÿè¸ªæ–¹é¢æ¯”æœ€å…ˆè¿›çš„ 3DGSSLAM æ–¹æ³•æé«˜äº† 3 å€ï¼Œåœ¨å…‰åº¦æ¸²æŸ“è´¨é‡æ–¹é¢æé«˜äº† 5%ï¼ŒåŒæ—¶å…è®¸å®æ—¶æ¸²æŸ“é«˜åˆ†è¾¨ç‡å¯†é›† 3D åœ°å›¾ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†å…¶åœ¨ SLAM ä¸­å®ç°å‡†ç¡®å®šä½å’Œé€¼çœŸé‡å»ºçš„ç›®æ ‡ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) MM3DGSé‡‡ç”¨é¢„ç§¯åˆ†æƒ¯æ€§æµ‹é‡ï¼ˆPre-integrated Inertial Measurementsï¼ŒPIMï¼‰æ¥ä¼°è®¡ç›¸æœºä½å§¿å’Œé€Ÿåº¦ï¼Œå‡å°‘å™ªå£°å½±å“ï¼›(2) ä½¿ç”¨æ·±åº¦ä¼°è®¡æ¨¡å—ä»RGBå›¾åƒä¸­æå–æ·±åº¦ä¿¡æ¯ï¼Œç”¨äºç¥ç»è¾å°„åœºæ¸²æŸ“å’Œåœºæ™¯é‡å»ºï¼›(3) å¼•å…¥å…‰åº¦æ¸²æŸ“è´¨é‡åº¦é‡ï¼ˆPhotometric Rendering Qualityï¼ŒPRQï¼‰ï¼Œé€šè¿‡ä¼˜åŒ–æ¸²æŸ“è´¨é‡æ¥æé«˜è·Ÿè¸ªå’Œå»ºå›¾çš„å‡†ç¡®æ€§ï¼›(4) å°†3Dé«˜æ–¯æ–‘ç‚¹ï¼ˆ3D Gaussian Splatï¼Œ3DGSï¼‰åº”ç”¨äºç¥ç»è¾å°„åœºï¼Œæé«˜æ¸²æŸ“é€Ÿåº¦å’Œå°ºåº¦æ„ŸçŸ¥èƒ½åŠ›ï¼›(5) æå‡ºä¸€ç§åŸºäº3DGSçš„è½¨è¿¹è·Ÿè¸ªç®—æ³•ï¼Œé€šè¿‡ä¼˜åŒ–PRQå’ŒPIMæ¥å®ç°å‡†ç¡®å®šä½ï¼›(6) é‡‡ç”¨åˆ†å—æ¸²æŸ“æŠ€æœ¯ï¼Œå…è®¸å®æ—¶æ¸²æŸ“é«˜åˆ†è¾¨ç‡å¯†é›†3Dåœ°å›¾ã€‚</p></li></ol><p>8.ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€3Dé«˜æ–¯æ–‘ç‚¹SLAMæ–¹æ³•MM3DGSï¼Œè¯¥æ–¹æ³•åˆ©ç”¨é¢„ç§¯åˆ†æƒ¯æ€§æµ‹é‡ã€æ·±åº¦ä¼°è®¡å’Œå…‰åº¦æ¸²æŸ“è´¨é‡åº¦é‡æ¥ä¼˜åŒ–è·Ÿè¸ªå’Œå»ºå›¾ï¼Œåœ¨è·Ÿè¸ªæ–¹é¢æ¯”æœ€å…ˆè¿›çš„3DGSSLAMæ–¹æ³•æé«˜äº†3å€ï¼Œåœ¨å…‰åº¦æ¸²æŸ“è´¨é‡æ–¹é¢æé«˜äº†5%ï¼ŒåŒæ—¶å…è®¸å®æ—¶æ¸²æŸ“é«˜åˆ†è¾¨ç‡å¯†é›†3Dåœ°å›¾ï¼Œä¸ºSLAMä¸­å®ç°å‡†ç¡®å®šä½å’Œé€¼çœŸé‡å»ºæä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š- æå‡ºäº†ä¸€ç§åŸºäº3Dé«˜æ–¯æ–‘ç‚¹çš„SLAMæ–¹æ³•ï¼Œæé«˜äº†æ¸²æŸ“é€Ÿåº¦å’Œå°ºåº¦æ„ŸçŸ¥èƒ½åŠ›ã€‚- å¼•å…¥å…‰åº¦æ¸²æŸ“è´¨é‡åº¦é‡ï¼Œé€šè¿‡ä¼˜åŒ–æ¸²æŸ“è´¨é‡æ¥æé«˜è·Ÿè¸ªå’Œå»ºå›¾çš„å‡†ç¡®æ€§ã€‚- é‡‡ç”¨åˆ†å—æ¸²æŸ“æŠ€æœ¯ï¼Œå…è®¸å®æ—¶æ¸²æŸ“é«˜åˆ†è¾¨ç‡å¯†é›†3Dåœ°å›¾ã€‚æ€§èƒ½ï¼š- åœ¨UT-MMæ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œåœ¨è·Ÿè¸ªæ–¹é¢æ¯”æœ€å…ˆè¿›çš„3DGSSLAMæ–¹æ³•æé«˜äº†3å€ï¼Œåœ¨å…‰åº¦æ¸²æŸ“è´¨é‡æ–¹é¢æé«˜äº†5%ã€‚- å…è®¸å®æ—¶æ¸²æŸ“é«˜åˆ†è¾¨ç‡å¯†é›†3Dåœ°å›¾ã€‚å·¥ä½œé‡ï¼š- è¯¥æ–¹æ³•éœ€è¦é¢„ç§¯åˆ†æƒ¯æ€§æµ‹é‡ã€æ·±åº¦ä¼°è®¡å’Œå…‰åº¦æ¸²æŸ“è´¨é‡åº¦é‡ç­‰æ¨¡å—ï¼Œå·¥ä½œé‡è¾ƒå¤§ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-e555dce577db7ee48c6d0700898f8354.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-45584721725016a544b3438b3b4e3524.jpg" align="middle"><img src="https://picx.zhimg.com/v2-efe4c6fcb1b7199ca40bd96e51c223a5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a82f468f62090c014e63f7697e14bafd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5bd070bb451dced5c2cc58f655ff4729.jpg" align="middle"></details>## 3DGSR: Implicit Surface Reconstruction with 3D Gaussian Splatting**Authors:Xiaoyang Lyu, Yang-Tian Sun, Yi-Hua Huang, Xiuzhe Wu, Ziyi Yang, Yilun Chen, Jiangmiao Pang, Xiaojuan Qi**In this paper, we present an implicit surface reconstruction method with 3D Gaussian Splatting (3DGS), namely 3DGSR, that allows for accurate 3D reconstruction with intricate details while inheriting the high efficiency and rendering quality of 3DGS. The key insight is incorporating an implicit signed distance field (SDF) within 3D Gaussians to enable them to be aligned and jointly optimized. First, we introduce a differentiable SDF-to-opacity transformation function that converts SDF values into corresponding Gaussians' opacities. This function connects the SDF and 3D Gaussians, allowing for unified optimization and enforcing surface constraints on the 3D Gaussians. During learning, optimizing the 3D Gaussians provides supervisory signals for SDF learning, enabling the reconstruction of intricate details. However, this only provides sparse supervisory signals to the SDF at locations occupied by Gaussians, which is insufficient for learning a continuous SDF. Then, to address this limitation, we incorporate volumetric rendering and align the rendered geometric attributes (depth, normal) with those derived from 3D Gaussians. This consistency regularization introduces supervisory signals to locations not covered by discrete 3D Gaussians, effectively eliminating redundant surfaces outside the Gaussian sampling range. Our extensive experimental results demonstrate that our 3DGSR method enables high-quality 3D surface reconstruction while preserving the efficiency and rendering quality of 3DGS. Besides, our method competes favorably with leading surface reconstruction techniques while offering a more efficient learning process and much better rendering qualities. The code will be available at https://github.com/CVMI-Lab/3DGSR. [PDF](http://arxiv.org/abs/2404.00409v1) **Summary**3DGSR æ˜¯ä¸€ç§éšå¼æ›²é¢é‡å»ºæ–¹æ³•ï¼Œå®ƒç»“åˆäº† 3DGS çš„é«˜ç²¾åº¦å’Œæ¸²æŸ“è´¨é‡ï¼Œå¹¶åˆ©ç”¨ 3D é«˜æ–¯æ¨¡ç³Šæ¥å¢å¼ºéšå¼ç¬¦å·è·ç¦»åœº (SDF)ï¼Œä»è€Œå®ç°å¯¹å¤æ‚ç»†èŠ‚çš„é«˜ç²¾åº¦ 3D é‡å»ºã€‚**Key Takeaways**- 3DGSR å°†éšå¼ç¬¦å·è·ç¦»åœº (SDF) èå…¥ 3D é«˜æ–¯æ¨¡ç³Šï¼Œä½¿å…¶å¯¹é½å¹¶å…±åŒä¼˜åŒ–ã€‚- å¯å¾®åˆ† SDF åˆ°ä¸é€æ˜åº¦å˜æ¢å‡½æ•°å°† SDF å€¼è½¬æ¢ä¸ºç›¸åº”çš„é«˜æ–¯ä¸é€æ˜åº¦ï¼Œè¿æ¥äº† SDF å’Œ 3D é«˜æ–¯æ¨¡ç³Šï¼Œå®ç°äº†ç»Ÿä¸€ä¼˜åŒ–å’Œå¯¹ 3D é«˜æ–¯æ¨¡ç³Šçš„æ›²é¢çº¦æŸã€‚- ä¼˜åŒ– 3D é«˜æ–¯æ¨¡ç³Šä¸º SDF å­¦ä¹ æä¾›äº†ç›‘ç£ä¿¡å·ï¼Œä»è€Œèƒ½å¤Ÿé‡å»ºå¤æ‚ç»†èŠ‚ã€‚- ä½“ç§¯æ¸²æŸ“å’Œå¯¹é½æ¥è‡ª 3D é«˜æ–¯æ¨¡ç³Šçš„å‡ ä½•å±æ€§ï¼ˆæ·±åº¦ã€æ³•çº¿ï¼‰å¯å¼•å…¥ç›‘ç£ä¿¡å·ï¼Œæœ‰æ•ˆæ¶ˆé™¤é«˜æ–¯é‡‡æ ·èŒƒå›´ä¹‹å¤–çš„å¤šä½™æ›²é¢ã€‚- å®éªŒç»“æœè¡¨æ˜ï¼Œ3DGSR åœ¨ä¿æŒ 3DGS çš„æ•ˆç‡å’Œæ¸²æŸ“è´¨é‡çš„åŒæ—¶ï¼Œå®ç°äº†é«˜è´¨é‡çš„ 3D æ›²é¢é‡å»ºã€‚- ä¸é¢†å…ˆçš„æ›²é¢é‡å»ºæŠ€æœ¯ç›¸æ¯”ï¼Œ3DGSR å…·æœ‰ç«äº‰ä¼˜åŠ¿ï¼ŒåŒæ—¶æä¾›äº†æ›´æœ‰æ•ˆçš„å­¦ä¹ è¿‡ç¨‹å’Œæ›´å¥½çš„æ¸²æŸ“è´¨é‡ã€‚- 3DGSR çš„ä»£ç å¯ä» https://github.com/CVMI-Lab/3DGSR è·å–ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>é¢˜ç›®ï¼š3DGSRï¼šåŸºäº 3D é«˜æ–¯æº…å°„çš„éšå¼æ›²é¢é‡å»º</li><li>ä½œè€…ï¼šXiaoyang Lyu, Yang-Tian Sun, Yi-Hua Huang, Xiuzhe Wu, Ziyi Yang, Yilun Chen, Jiangmiao Pang, Xiaojuan Qi</li><li>éš¶å±ï¼šé¦™æ¸¯å¤§å­¦</li><li>å…³é”®è¯ï¼šGaussian Splattingã€éšå¼å‡½æ•°ã€ç¬¦å·è·ç¦»å‡½æ•°ã€ä½“ç§¯æ¸²æŸ“</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://doi.org/10.1145/nnnnnnn.nnnnnnn   Github ä»£ç é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼š3D é«˜æ–¯æº…å°„ï¼ˆ3DGSï¼‰æ˜¯ä¸€ç§ç”¨äºé«˜è´¨é‡æ–°è§†è§’åˆæˆçš„æ–°å‹æŠ€æœ¯ï¼Œä½†å®ƒåªèƒ½ç”Ÿæˆå˜ˆæ‚ä¸”ä¸å®Œæ•´çš„ 3D å‡ ä½•ç‚¹ï¼Œæ— æ³•å‡†ç¡®é‡å»ºåœºæ™¯çš„ 3D æ›²é¢ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼š3DGS æ— æ³•å¿ å®åœ°è¡¨ç¤º 3D æ›²é¢ï¼Œå› ä¸ºå®ƒé‡‡ç”¨éç»“æ„åŒ–çš„åŸºäºç‚¹çš„å‡ ä½•è¡¨ç¤ºã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§éšå¼æ›²é¢é‡å»ºæ–¹æ³•ï¼Œç§°ä¸º 3DGS çš„ 3D é«˜æ–¯æº…å°„ï¼ˆ3DGSRï¼‰ï¼Œå®ƒå…è®¸å‡†ç¡®é‡å»ºå…·æœ‰å¤æ‚ç»†èŠ‚çš„ 3Dï¼ŒåŒæ—¶ç»§æ‰¿äº† 3DGS çš„é«˜æ•ˆç‡å’Œæ¸²æŸ“è´¨é‡ã€‚å…³é”®æ€æƒ³æ˜¯å°†éšå¼ç¬¦å·è·ç¦»åœºï¼ˆSDFï¼‰åˆå¹¶åˆ° 3D é«˜æ–¯ä¸­ï¼Œä½¿å®ƒä»¬èƒ½å¤Ÿå¯¹é½å¹¶å…±åŒä¼˜åŒ–ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šå®éªŒç»“æœè¡¨æ˜ï¼Œ3DGSR æ–¹æ³•èƒ½å¤Ÿå®ç°é«˜è´¨é‡çš„ 3D æ›²é¢é‡å»ºï¼ŒåŒæ—¶ä¿æŒ 3DGS çš„æ•ˆç‡å’Œæ¸²æŸ“è´¨é‡ã€‚è¯¥æ–¹æ³•åœ¨ä¸é¢†å…ˆçš„æ›²é¢é‡å»ºæŠ€æœ¯ç«äº‰æ—¶è¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶æä¾›äº†æ›´é«˜æ•ˆçš„å­¦ä¹ è¿‡ç¨‹å’Œæ›´å¥½çš„æ¸²æŸ“è´¨é‡ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) å°†éšå¼ç¬¦å·è·ç¦»åœºï¼ˆSDFï¼‰ä¸ 3D é«˜æ–¯æº…å°„ï¼ˆ3DGSï¼‰ç›¸ç»“åˆï¼Œä½¿å®ƒä»¬èƒ½å¤Ÿå¯¹é½å¹¶å…±åŒä¼˜åŒ–ã€‚(2) ä½¿ç”¨ SDF æ¥æŒ‡å¯¼ 3DGS çš„ä¼˜åŒ–è¿‡ç¨‹ï¼Œä»è€Œç”Ÿæˆæ›´å‡†ç¡®å’Œå®Œæ•´çš„ 3D æ›²é¢ã€‚(3) é‡‡ç”¨åˆ†å±‚ä¼˜åŒ–ç­–ç•¥ï¼Œä»ç²—ç³™çš„æ›²é¢é€æ­¥ç»†åŒ–åˆ°ç²¾ç»†çš„æ›²é¢ï¼Œä»¥æé«˜é‡å»ºæ•ˆç‡ã€‚(4) å¼•å…¥æ­£åˆ™åŒ–é¡¹ï¼Œä»¥ä¿ƒè¿›é‡å»ºæ›²é¢çš„å…‰æ»‘æ€§å’Œè¿è´¯æ€§ã€‚(5) ä½¿ç”¨åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–ç®—æ³•ï¼Œä»¥å®ç°é«˜æ•ˆå’Œç¨³å®šçš„æ›²é¢é‡å»ºã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œæå‡ºäº†ä¸€ç§é«˜æ•ˆçš„éšå¼æ›²é¢é‡å»ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åŸºäº 3D é«˜æ–¯æº…å°„ï¼Œèƒ½å¤Ÿé‡å»ºå…·æœ‰å¤æ‚ç»†èŠ‚çš„é«˜è´¨é‡ 3D æ›²é¢ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>å°†ç¥ç»éšå¼ç¬¦å·è·ç¦»åœºï¼ˆSDFï¼‰ä¸ 3D é«˜æ–¯æº…å°„ï¼ˆ3DGSï¼‰ç›¸ç»“åˆï¼Œé€šè¿‡å¯å¾®åˆ† SDF åˆ°ä¸é€æ˜åº¦è½¬æ¢å‡½æ•°å®ç° SDF å’Œ 3D é«˜æ–¯çš„å¯¹é½å’Œè”åˆä¼˜åŒ–ã€‚</li><li>åˆ©ç”¨ä½“ç§¯æ¸²æŸ“å’Œ SDF ä¸é«˜æ–¯å‡ ä½•ä¸€è‡´æ€§æ­£åˆ™åŒ–è¿›è¡Œ SDF ä¼˜åŒ–ã€‚æ€§èƒ½ï¼š</li><li>åœ¨ä¸å½±å“ 3D é«˜æ–¯æ¸²æŸ“èƒ½åŠ›å’Œæ•ˆç‡çš„æƒ…å†µä¸‹ï¼Œ3DGSR åœ¨é‡å»ºé«˜è´¨é‡æ›²é¢æ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„é‡å»ºç®¡é“ã€‚å·¥ä½œé‡ï¼š</li><li>ç”±äºæ¸²æŸ“è´¨é‡å’Œæ›²é¢å¹³æ»‘åº¦ä¹‹é—´çš„æƒè¡¡ï¼Œæœ¬ç ”ç©¶ç¡®å®å­˜åœ¨ä¸€å®šçš„å±€é™æ€§ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-7c3724a12f3e6cb1586e3e58348c4989.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e49e36a5fd966732c34aa3a3b964dee7.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-da0937779f213436f7d6b004f3c45985.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-04-06  Per-Gaussian Embedding-Based Deformation for Deformable 3D Gaussian   Splatting</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/Talking%20Head%20Generation/</id>
    <published>2024-04-06T09:47:10.000Z</published>
    <updated>2024-04-06T09:47:10.952Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-04-06-æ›´æ–°"><a href="#2024-04-06-æ›´æ–°" class="headerlink" title="2024-04-06 æ›´æ–°"></a>2024-04-06 æ›´æ–°</h1><h2 id="EDTalk-Efficient-Disentanglement-for-Emotional-Talking-Head-Synthesis"><a href="#EDTalk-Efficient-Disentanglement-for-Emotional-Talking-Head-Synthesis" class="headerlink" title="EDTalk: Efficient Disentanglement for Emotional Talking Head Synthesis"></a>EDTalk: Efficient Disentanglement for Emotional Talking Head Synthesis</h2><p><strong>Authors:Shuai Tan, Bin Ji, Mengxiao Bi, Ye Pan</strong></p><p>Achieving disentangled control over multiple facial motions and accommodating diverse input modalities greatly enhances the application and entertainment of the talking head generation. This necessitates a deep exploration of the decoupling space for facial features, ensuring that they a) operate independently without mutual interference and b) can be preserved to share with different modal input, both aspects often neglected in existing methods. To address this gap, this paper proposes a novel Efficient Disentanglement framework for Talking head generation (EDTalk). Our framework enables individual manipulation of mouth shape, head pose, and emotional expression, conditioned on video or audio inputs. Specifically, we employ three lightweight modules to decompose the facial dynamics into three distinct latent spaces representing mouth, pose, and expression, respectively. Each space is characterized by a set of learnable bases whose linear combinations define specific motions. To ensure independence and accelerate training, we enforce orthogonality among bases and devise an efficient training strategy to allocate motion responsibilities to each space without relying on external knowledge. The learned bases are then stored in corresponding banks, enabling shared visual priors with audio input. Furthermore, considering the properties of each space, we propose an Audio-to-Motion module for audio-driven talking head synthesis. Experiments are conducted to demonstrate the effectiveness of EDTalk. We recommend watching the project website: <a href="https://tanshuai0219.github.io/EDTalk/">https://tanshuai0219.github.io/EDTalk/</a> </p><p><a href="http://arxiv.org/abs/2404.01647v1">PDF</a> 22 pages, 15 figures</p><p><strong>Summary</strong><br>åˆ©ç”¨è§†é¢‘æˆ–éŸ³é¢‘è¾“å…¥ï¼Œç‹¬ç«‹æ“æ§å˜´å·´å½¢çŠ¶ï¼Œå¤´éƒ¨å§¿æ€å’Œæƒ…ç»ªè¡¨æƒ…ï¼Œå®ç°é«˜æ•ˆå¯æ§çš„é¢éƒ¨ç”Ÿæˆã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡º Efficient Disentanglement æ¡†æ¶ï¼Œå®ç°è§£è€¦é¢éƒ¨åŠ¨ä½œã€‚</li><li>åˆ©ç”¨ä¸‰æ¨¡å—åˆ†è§£é¢éƒ¨åŠ¨æ€ï¼Œç‹¬ç«‹æ“æ§å˜´å·´å½¢çŠ¶ï¼Œå¤´éƒ¨å§¿æ€å’Œæƒ…ç»ªè¡¨æƒ…ã€‚</li><li>é‡‡ç”¨å¯å­¦ä¹ åŸºåº•ï¼Œé€šè¿‡çº¿æ€§ç»„åˆå®šä¹‰ç‰¹å®šåŠ¨ä½œã€‚</li><li>å¼ºåˆ¶åŸºåº•æ­£äº¤ï¼ŒåŠ é€Ÿè®­ç»ƒï¼Œç¡®ä¿åŠ¨ä½œç‹¬ç«‹ã€‚</li><li>æå‡º Audio-to-Motion æ¨¡å—ï¼Œå®ç°éŸ³é¢‘é©±åŠ¨é¢éƒ¨ç”Ÿæˆã€‚</li><li>å®éªŒéªŒè¯ EDTalk çš„æœ‰æ•ˆæ€§ã€‚</li><li>æä¾›é¡¹ç›®ç½‘ç«™ï¼š<a href="https://tanshuai0219.github.io/EDTalk/">https://tanshuai0219.github.io/EDTalk/</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><strong>è®ºæ–‡æ ‡é¢˜ï¼š</strong> EDTalkï¼šé«˜æ•ˆè§£è€¦è¯´è¯äººå¤´éƒ¨ç”Ÿæˆæ¡†æ¶</li><li><strong>ä½œè€…ï¼š</strong> Tan Shuai, Qiangqiang Yuan, Lu Sheng, Fan Yang, Zhixin Piao, Changjie Fan</li><li><strong>ç¬¬ä¸€ä½œè€…å•ä½ï¼š</strong> æ¸…åå¤§å­¦</li><li><strong>å…³é”®è¯ï¼š</strong> è¯´è¯äººå¤´éƒ¨ç”Ÿæˆã€è§£è€¦ã€é¢éƒ¨åŠ¨ç”»ã€éŸ³é¢‘é©±åŠ¨</li><li><strong>è®ºæ–‡é“¾æ¥ï¼š</strong> https://arxiv.org/abs/2207.03559</li><li><strong>æ‘˜è¦ï¼š</strong>    (1) <strong>ç ”ç©¶èƒŒæ™¯ï¼š</strong> è¯´è¯äººå¤´éƒ¨ç”Ÿæˆéœ€è¦å¯¹å¤šä¸ªé¢éƒ¨åŠ¨ä½œè¿›è¡Œè§£è€¦æ§åˆ¶ï¼Œå¹¶é€‚åº”ä¸åŒçš„è¾“å…¥æ–¹å¼ï¼Œè¿™éœ€è¦æ·±å…¥æ¢ç´¢é¢éƒ¨ç‰¹å¾çš„è§£è€¦ç©ºé—´ï¼Œç¡®ä¿å®ƒä»¬æ—¢èƒ½ç‹¬ç«‹æ“ä½œåˆå¯ä»¥ä¿ç•™ä¸ä¸åŒæ¨¡æ€è¾“å…¥å…±äº«çš„èƒ½åŠ›ã€‚    (2) <strong>è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼š</strong> ç°æœ‰æ–¹æ³•å¾€å¾€å¿½è§†äº†è¿™äº›æ–¹é¢ï¼Œå¯¼è‡´è§£è€¦ç©ºé—´ä¸ç‹¬ç«‹ã€è®­ç»ƒé€Ÿåº¦æ…¢æˆ–æ— æ³•å¤„ç†éŸ³é¢‘è¾“å…¥ã€‚    (3) <strong>ç ”ç©¶æ–¹æ³•ï¼š</strong> æå‡º EDTalk æ¡†æ¶ï¼Œé‡‡ç”¨ä¸‰ä¸ªè½»é‡çº§æ¨¡å—å°†é¢éƒ¨åŠ¨æ€åˆ†è§£ä¸ºä¸‰ä¸ªä¸åŒçš„æ½œåœ¨ç©ºé—´ï¼Œåˆ†åˆ«è¡¨ç¤ºå˜´å‹ã€å¤´éƒ¨å§¿æ€å’Œè¡¨æƒ…ã€‚æ¯ä¸ªç©ºé—´éƒ½ç”±ä¸€ç»„å¯å­¦ä¹ åŸºç»„æˆï¼Œå…¶çº¿æ€§ç»„åˆå®šä¹‰äº†ç‰¹å®šçš„åŠ¨ä½œã€‚é€šè¿‡æ­£äº¤åŒ–åŸºå¹¶è®¾è®¡é«˜æ•ˆçš„è®­ç»ƒç­–ç•¥ï¼Œç¡®ä¿äº†ç‹¬ç«‹æ€§å’ŒåŠ é€Ÿäº†è®­ç»ƒã€‚    (4) <strong>ä»»åŠ¡å’Œæ€§èƒ½ï¼š</strong> åœ¨è¯´è¯äººå¤´éƒ¨ç”Ÿæˆä»»åŠ¡ä¸Šï¼ŒEDTalk å®ç°äº†å‡ºè‰²çš„æ€§èƒ½ï¼Œåœ¨è§†é¢‘å’ŒéŸ³é¢‘è¾“å…¥æ¡ä»¶ä¸‹å‡èƒ½å®ç°å˜´å‹ã€å¤´éƒ¨å§¿æ€å’Œè¡¨æƒ…çš„ç‹¬ç«‹æ§åˆ¶ã€‚å®éªŒç»“æœéªŒè¯äº† EDTalk çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜äº†å…¶åœ¨è¯´è¯äººå¤´éƒ¨ç”Ÿæˆä¸­çš„åº”ç”¨æ½œåŠ›ã€‚</li></ol><p><strong>æ–¹æ³•</strong></p><p>ï¼ˆ1ï¼‰<strong>é«˜æ•ˆè§£è€¦ç­–ç•¥ï¼š</strong>æå‡ºè§£è€¦ç­–ç•¥ï¼ŒåŒ…æ‹¬å˜´å‹-å¤´éƒ¨å§¿æ€è§£è€¦å’Œè¡¨æƒ…è§£è€¦ï¼Œå°†æ•´ä½“é¢éƒ¨åŠ¨æ€åˆ†è§£ä¸ºå˜´å‹ã€å¤´éƒ¨å§¿æ€å’Œè¡¨æƒ…ç©ºé—´ã€‚</p><p>ï¼ˆ2ï¼‰<strong>å˜´å‹-å¤´éƒ¨å§¿æ€è§£è€¦ï¼š</strong>é‡‡ç”¨äº¤å‰é‡å»ºæŠ€æœ¯ï¼Œåˆæˆå˜´å‹äº¤æ¢åçš„å›¾åƒï¼Œå¹¶é€šè¿‡é‡æ„æŸå¤±ã€æ„ŸçŸ¥æŸå¤±å’Œå¯¹æŠ—æŸå¤±ç›‘ç£å˜´å‹-å¤´éƒ¨å§¿æ€è§£è€¦æ¨¡å—ã€‚</p><p>ï¼ˆ3ï¼‰<strong>è¡¨æƒ…è§£è€¦ï¼š</strong>å¼•å…¥è¡¨æƒ…æ„ŸçŸ¥æ½œåœ¨å¯¼èˆªæ¨¡å—å’Œæƒ…æ„Ÿå¢å¼ºæ¨¡å—ï¼Œé€šè¿‡è‡ªé‡å»ºè¡¥å……å­¦ä¹ è®­ç»ƒè¡¨æƒ…è§£è€¦æ¨¡å—ã€‚</p><p>ï¼ˆ4ï¼‰<strong>éŸ³é¢‘åˆ°åŠ¨ä½œï¼š</strong>è®¾è®¡ä¸‰ä¸ªæ¨¡å—ä»éŸ³é¢‘é¢„æµ‹å¤´éƒ¨å§¿æ€ã€å˜´å‹å’Œè¡¨æƒ…çš„æƒé‡ï¼Œé€šè¿‡ç‰¹å¾æŸå¤±ã€é‡æ„æŸå¤±å’ŒåŒæ­¥æŸå¤±è®­ç»ƒéŸ³é¢‘ç¼–ç å™¨å’Œæƒé‡é¢„æµ‹å±‚ã€‚</p><ol><li>ç»“è®ºï¼š(1): æœ¬æ–‡æå‡º EDTalkï¼Œä¸€ç§æ–°é¢–çš„ç³»ç»Ÿï¼Œæ—¨åœ¨å°†é¢éƒ¨ç»„ä»¶é«˜æ•ˆè§£è€¦åˆ°æ½œåœ¨ç©ºé—´ä¸­ï¼Œä»è€Œå®ç°è¯´è¯äººå¤´éƒ¨åˆæˆçš„ç²¾ç»†æ§åˆ¶ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯ä½¿ç”¨å­˜å‚¨åœ¨ä¸“ç”¨åº“ä¸­çš„æ­£äº¤åŸºæ¥è¡¨ç¤ºæ¯ä¸ªç©ºé—´ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„è®­ç»ƒç­–ç•¥ï¼Œè¯¥ç­–ç•¥å¯ä»¥è‡ªåŠ¨å°†ç©ºé—´ä¿¡æ¯åˆ†é…ç»™æ¯ä¸ªç©ºé—´ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹å¤–éƒ¨æˆ–å…ˆéªŒç»“æ„çš„éœ€è¦ã€‚é€šè¿‡é›†æˆè¿™äº›ç©ºé—´ï¼Œæˆ‘ä»¬é€šè¿‡è½»é‡çº§çš„ Audio-to-Motion æ¨¡å—å®ç°äº†éŸ³é¢‘é©±åŠ¨çš„è¯´è¯äººå¤´éƒ¨ç”Ÿæˆã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å®ç°å¯¹å„ç§é¢éƒ¨åŠ¨ä½œçš„è§£è€¦å’Œç²¾ç»†æ§åˆ¶æ–¹é¢å…·æœ‰ä¼˜è¶Šæ€§ã€‚æˆ‘ä»¬åœ¨é™„å½•ä¸­æä¾›äº†æœ‰å…³å±€é™æ€§å’Œä¼¦ç†è€ƒè™‘çš„æ›´å¤šè®¨è®ºã€‚(2): åˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§é«˜æ•ˆçš„è§£è€¦ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åŒ…æ‹¬å˜´å‹-å¤´éƒ¨å§¿æ€è§£è€¦å’Œè¡¨æƒ…è§£è€¦ï¼›æå‡ºäº†ä¸€ç§åŸºäºäº¤å‰é‡å»ºæŠ€æœ¯çš„å˜´å‹-å¤´éƒ¨å§¿æ€è§£è€¦æ¨¡å—ï¼›æå‡ºäº†ä¸€ç§å¼•å…¥è¡¨æƒ…æ„ŸçŸ¥æ½œåœ¨å¯¼èˆªæ¨¡å—å’Œæƒ…æ„Ÿå¢å¼ºæ¨¡å—çš„è¡¨æƒ…è§£è€¦æ¨¡å—ï¼›è®¾è®¡äº†ä¸€ä¸ªä»éŸ³é¢‘é¢„æµ‹å¤´éƒ¨å§¿æ€ã€å˜´å‹å’Œè¡¨æƒ…æƒé‡çš„ Audio-to-Motion æ¨¡å—ã€‚æ€§èƒ½ï¼šåœ¨è¯´è¯äººå¤´éƒ¨ç”Ÿæˆä»»åŠ¡ä¸Šï¼ŒEDTalk å®ç°äº†å‡ºè‰²çš„æ€§èƒ½ï¼Œåœ¨è§†é¢‘å’ŒéŸ³é¢‘è¾“å…¥æ¡ä»¶ä¸‹å‡èƒ½å®ç°å˜´å‹ã€å¤´éƒ¨å§¿æ€å’Œè¡¨æƒ…çš„ç‹¬ç«‹æ§åˆ¶ã€‚å®éªŒç»“æœéªŒè¯äº† EDTalk çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜äº†å…¶åœ¨è¯´è¯äººå¤´éƒ¨ç”Ÿæˆä¸­çš„åº”ç”¨æ½œåŠ›ã€‚å·¥ä½œé‡ï¼šæœ¬æ–‡çš„å·¥ä½œé‡è¾ƒå¤§ï¼Œæ¶‰åŠåˆ°è§£è€¦ç­–ç•¥ã€å˜´å‹-å¤´éƒ¨å§¿æ€è§£è€¦æ¨¡å—ã€è¡¨æƒ…è§£è€¦æ¨¡å—å’Œ Audio-to-Motion æ¨¡å—çš„è®¾è®¡å’Œå®ç°ã€‚å®éªŒéƒ¨åˆ†ä¹Ÿæ¯”è¾ƒå¤æ‚ï¼ŒåŒ…æ‹¬å®šé‡å’Œå®šæ€§è¯„ä¼°ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-f58e08e1946a51a1bac98807f8c1876a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0135d232756d768679d9f63847585de1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a0242ee4c355be537d186f7f79fc6e49.jpg" align="middle"></details><h2 id="FaceChain-ImagineID-Freely-Crafting-High-Fidelity-Diverse-Talking-Faces-from-Disentangled-Audio"><a href="#FaceChain-ImagineID-Freely-Crafting-High-Fidelity-Diverse-Talking-Faces-from-Disentangled-Audio" class="headerlink" title="FaceChain-ImagineID: Freely Crafting High-Fidelity Diverse Talking Faces   from Disentangled Audio"></a>FaceChain-ImagineID: Freely Crafting High-Fidelity Diverse Talking Faces   from Disentangled Audio</h2><p><strong>Authors:Chao Xu, Yang Liu, Jiazheng Xing, Weida Wang, Mingze Sun, Jun Dan, Tianxin Huang, Siyuan Li, Zhi-Qi Cheng, Ying Tai, Baigui Sun</strong></p><p>In this paper, we abstract the process of people hearing speech, extracting meaningful cues, and creating various dynamically audio-consistent talking faces, termed Listening and Imagining, into the task of high-fidelity diverse talking faces generation from a single audio. Specifically, it involves two critical challenges: one is to effectively decouple identity, content, and emotion from entangled audio, and the other is to maintain intra-video diversity and inter-video consistency. To tackle the issues, we first dig out the intricate relationships among facial factors and simplify the decoupling process, tailoring a Progressive Audio Disentanglement for accurate facial geometry and semantics learning, where each stage incorporates a customized training module responsible for a specific factor. Secondly, to achieve visually diverse and audio-synchronized animation solely from input audio within a single model, we introduce the Controllable Coherent Frame generation, which involves the flexible integration of three trainable adapters with frozen Latent Diffusion Models (LDMs) to focus on maintaining facial geometry and semantics, as well as texture and temporal coherence between frames. In this way, we inherit high-quality diverse generation from LDMs while significantly improving their controllability at a low training cost. Extensive experiments demonstrate the flexibility and effectiveness of our method in handling this paradigm. The codes will be released at <a href="https://github.com/modelscope/facechain">https://github.com/modelscope/facechain</a>. </p><p><a href="http://arxiv.org/abs/2403.01901v2">PDF</a> </p><p><strong>Summary</strong><br>åˆ©ç”¨å•ä¸€éŸ³é¢‘ç”Ÿæˆå¤šæ ·åŒ–çš„é«˜ä¿çœŸåŠ¨æ€äººè„¸ï¼Œå®ƒè§£å†³äº†ä¸¤å¤§éš¾é¢˜ï¼šæœ‰æ•ˆåˆ†ç¦»éŸ³é¢‘ä¸­çº ç¼ çš„èº«ä»½ã€å†…å®¹å’Œæƒ…æ„Ÿï¼Œä»¥åŠä¿æŒè§†é¢‘å†…éƒ¨å¤šæ ·æ€§å’Œè§†é¢‘é—´ä¸€è‡´æ€§ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºâ€œå€¾å¬å’Œæƒ³è±¡â€ä»»åŠ¡ï¼Œå°†äººç±»å¬åˆ°è¯­éŸ³ã€æå–æœ‰æ„ä¹‰ç‰¹å¾å¹¶åˆ›é€ åŠ¨æ€ä¸€è‡´çš„äººè„¸è¡¨æƒ…è¿‡ç¨‹æŠ½è±¡åŒ–ã€‚</li><li>åˆ›æ–°æ€§åœ°å°†è¿›æ­¥å¼éŸ³é¢‘åˆ†ç¦»åº”ç”¨äºäººè„¸å‡ ä½•å’Œè¯­ä¹‰å­¦ä¹ ï¼Œä»¥å‡†ç¡®åˆ†ç¦»èº«ä»½ã€å†…å®¹å’Œæƒ…æ„Ÿã€‚</li><li>å¼•å…¥å¯æ§è¿è´¯å¸§ç”Ÿæˆï¼Œä½¿ç”¨ä¸‰ä¸ªå¯è®­ç»ƒé€‚é…å™¨å’Œå†»ç»“çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œä¸“æ³¨äºä¿æŒäººè„¸å‡ ä½•ã€è¯­ä¹‰ã€çº¹ç†å’Œå¸§é—´æ—¶é—´è¿è´¯æ€§ã€‚</li><li>ç»§æ‰¿æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„é«˜è´¨é‡ç”Ÿæˆèƒ½åŠ›ï¼ŒåŒæ—¶é€šè¿‡ä½è®­ç»ƒæˆæœ¬æ˜¾è‘—æé«˜å¯æ§æ€§ã€‚</li><li>å®éªŒç»“æœè¯æ˜äº†è¯¥æ–¹æ³•åœ¨å¤„ç†æ­¤èŒƒä¾‹æ–¹é¢çš„çµæ´»æ€§å’Œæœ‰æ•ˆæ€§ã€‚</li><li>ä»£ç å°†åœ¨ <a href="https://github.com/modelscope/facechain">https://github.com/modelscope/facechain</a> ä¸Šå‘å¸ƒã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šFaceChain-ImagineIDï¼šè‡ªç”±ç”Ÿæˆé«˜ä¿çœŸå¤šæ ·åŒ–è¯´è¯äººè„¸</li><li>ä½œè€…ï¼šChao Xu, Yang Liu, Jiazheng Xing, Weida Wang, Mingze Sun, Jun Dan, Tianxin Huang, Siyuan Li, Zhi-Qi Cheng, Ying Tai, Baigui Sun</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šé˜¿é‡Œå·´å·´é›†å›¢</li><li>å…³é”®è¯ï¼šäººè„¸ç”Ÿæˆã€éŸ³é¢‘è§£è€¦ã€å¯æ§ç”Ÿæˆã€ä¸€è‡´æ€§</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.01901</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šéšç€äººè„¸ç”ŸæˆæŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼Œäººä»¬å¯¹éšç§ä¿æŠ¤å’Œè™šæ‹Ÿå½¢è±¡ä¸ªæ€§åŒ–çš„éœ€æ±‚æ—¥ç›Šå¢é•¿ã€‚ä¼ ç»Ÿæ–¹æ³•è¦ä¹ˆä½¿ç”¨çœŸå®äººè„¸å›¾åƒï¼Œå­˜åœ¨éšç§æ³„éœ²é£é™©ï¼Œè¦ä¹ˆç”Ÿæˆçš„è™šæ‹Ÿå½¢è±¡ä¸çœŸå®éŸ³é¢‘ä¸ä¸€è‡´ã€‚</li></ol><p>ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šè¿‡å»æ–¹æ³•ä¸»è¦é€šè¿‡éŸ³é¢‘ç‰¹å¾æå–å’Œå›¾åƒç”Ÿæˆç›¸ç»“åˆçš„æ–¹å¼è¿›è¡Œäººè„¸ç”Ÿæˆï¼Œä½†å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š- æ— æ³•æœ‰æ•ˆè§£è€¦éŸ³é¢‘ä¸­çš„èº«ä»½ã€å†…å®¹å’Œæƒ…ç»ªä¿¡æ¯ã€‚- éš¾ä»¥åœ¨å•ä¸€æ¨¡å‹ä¸­å®ç°è§†è§‰å¤šæ ·æ€§å’ŒéŸ³é¢‘åŒæ­¥åŠ¨ç”»ã€‚</p><p>ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†â€œè†å¬ä¸æƒ³è±¡â€èŒƒå¼ï¼Œå°†äººè„¸ç”Ÿæˆè¿‡ç¨‹æŠ½è±¡ä¸ºä»éŸ³é¢‘ä¸­æå–æœ‰æ„ä¹‰ä¿¡æ¯å¹¶ç”ŸæˆåŠ¨æ€éŸ³é¢‘ä¸€è‡´è¯´è¯äººè„¸çš„ä»»åŠ¡ã€‚å…·ä½“æ¥è¯´ï¼Œæ–¹æ³•åŒ…å«ä»¥ä¸‹ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼š- éŸ³é¢‘è§£è€¦ï¼šæœ‰æ•ˆåœ°ä»çº ç¼ çš„éŸ³é¢‘ä¸­è§£è€¦èº«ä»½ã€å†…å®¹å’Œæƒ…ç»ªä¿¡æ¯ã€‚- ä¸€è‡´æ€§æ§åˆ¶ï¼šåœ¨å•ä¸€æ¨¡å‹ä¸­ä¿æŒè§†é¢‘å†…å¤šæ ·æ€§å’Œè§†é¢‘é—´ä¸€è‡´æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†æ¸è¿›å¼éŸ³é¢‘è§£è€¦å’Œå¯æ§ä¸€è‡´å¸§ç”Ÿæˆæ–¹æ³•ï¼š- æ¸è¿›å¼éŸ³é¢‘è§£è€¦ï¼šé€šè¿‡å®šåˆ¶çš„è®­ç»ƒæ¨¡å—ï¼Œé€çº§å­¦ä¹ èº«ä»½ã€è¯­ä¹‰å’Œæƒ…ç»ªä¿¡æ¯ã€‚- å¯æ§ä¸€è‡´å¸§ç”Ÿæˆï¼šé€šè¿‡å¯è®­ç»ƒé€‚é…å™¨ä¸å†»ç»“çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹é›†æˆï¼Œä¿æŒé¢éƒ¨å‡ ä½•å’Œè¯­ä¹‰ã€çº¹ç†å’Œå¸§é—´æ—¶é—´ä¸€è‡´æ€§ã€‚</p><p>ï¼ˆ4ï¼‰ä»»åŠ¡å’Œæ€§èƒ½ï¼šæœ¬æ–‡æ–¹æ³•åœ¨ä»¥ä¸‹ä»»åŠ¡ä¸Šå–å¾—äº†è¾ƒå¥½çš„æ€§èƒ½ï¼š- é«˜ä¿çœŸå¤šæ ·åŒ–è¯´è¯äººè„¸ç”Ÿæˆï¼šä»å•ä¸€éŸ³é¢‘ç”Ÿæˆè§†è§‰å¤šæ ·ä¸”ä¸éŸ³é¢‘åŒæ­¥çš„äººè„¸è§†é¢‘ã€‚- å¯æ§å±æ€§ç¼–è¾‘ï¼šæ ¹æ®ä¸ªäººå–œå¥½ï¼Œè‡ªç”±æ”¹å˜ä¸éŸ³é¢‘æ— å…³çš„å±æ€§ï¼Œå¦‚èƒ¡é¡»ã€å‘å‹å’Œç³å­”é¢œè‰²ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤„ç†â€œè†å¬ä¸æƒ³è±¡â€èŒƒå¼æ—¶å…·æœ‰è¾ƒå¥½çš„çµæ´»æ€§å’Œæœ‰æ•ˆæ€§ã€‚</p><ol><li><p><strong>æ–¹æ³•</strong>ï¼š(1) <strong>æ¸è¿›å¼éŸ³é¢‘è§£è€¦</strong>ï¼šä½¿ç”¨å®šåˆ¶çš„è®­ç»ƒæ¨¡å—ï¼Œé€çº§å­¦ä¹ éŸ³é¢‘ä¸­çš„èº«ä»½ã€è¯­ä¹‰å’Œæƒ…ç»ªä¿¡æ¯ã€‚(2) <strong>å¯æ§ä¸€è‡´å¸§ç”Ÿæˆ</strong>ï¼šé€šè¿‡å¯è®­ç»ƒé€‚é…å™¨ä¸å†»ç»“çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹é›†æˆï¼Œä¿æŒé¢éƒ¨å‡ ä½•å’Œè¯­ä¹‰ã€çº¹ç†å’Œå¸§é—´æ—¶é—´ä¸€è‡´æ€§ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºâ€œè†å¬ä¸æƒ³è±¡â€èŒƒå¼çš„è¯´è¯äººè„¸ç”Ÿæˆæ–¹æ³•ï¼Œæœ‰æ•ˆè§£å†³äº†éŸ³é¢‘è§£è€¦å’Œä¸€è‡´æ€§æ§åˆ¶é—®é¢˜ï¼Œå®ç°äº†é«˜ä¿çœŸã€å¤šæ ·åŒ–ã€å¯æ§çš„äººè„¸è§†é¢‘ç”Ÿæˆã€‚è¯¥æ–¹æ³•ä¸ºéšç§ä¿æŠ¤ã€è™šæ‹Ÿå½¢è±¡ä¸ªæ€§åŒ–ç­‰é¢†åŸŸæä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºâ€œè†å¬ä¸æƒ³è±¡â€èŒƒå¼ï¼Œå°†äººè„¸ç”ŸæˆæŠ½è±¡ä¸ºä»éŸ³é¢‘ä¸­æå–ä¿¡æ¯å¹¶ç”ŸæˆåŠ¨æ€ä¸€è‡´äººè„¸çš„ä»»åŠ¡ã€‚</li><li>è®¾è®¡æ¸è¿›å¼éŸ³é¢‘è§£è€¦æ¨¡å—ï¼Œé€çº§å­¦ä¹ éŸ³é¢‘ä¸­çš„èº«ä»½ã€è¯­ä¹‰å’Œæƒ…ç»ªä¿¡æ¯ã€‚</li><li>æå‡ºå¯æ§ä¸€è‡´å¸§ç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡å¯è®­ç»ƒé€‚é…å™¨ä¸å†»ç»“çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹é›†æˆï¼Œä¿æŒè§†é¢‘å†…å¤šæ ·æ€§å’Œè§†é¢‘é—´ä¸€è‡´æ€§ã€‚æ€§èƒ½ï¼š</li><li>åœ¨é«˜ä¿çœŸå¤šæ ·åŒ–è¯´è¯äººè„¸ç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†è¾ƒå¥½çš„æ€§èƒ½ï¼Œç”Ÿæˆçš„è§†é¢‘å…·æœ‰è§†è§‰å¤šæ ·æ€§ï¼Œä¸éŸ³é¢‘åŒæ­¥ã€‚</li><li>æ”¯æŒå¯æ§å±æ€§ç¼–è¾‘ï¼Œå…è®¸ç”¨æˆ·æ ¹æ®ä¸ªäººå–œå¥½è‡ªç”±æ”¹å˜ä¸éŸ³é¢‘æ— å…³çš„å±æ€§ã€‚å·¥ä½œé‡ï¼š</li><li>æœ¬æ–‡æ–¹æ³•éœ€è¦å¤§é‡çš„æ•°æ®å’Œè®¡ç®—èµ„æºè¿›è¡Œè®­ç»ƒã€‚</li><li>æ¸è¿›å¼éŸ³é¢‘è§£è€¦å’Œå¯æ§ä¸€è‡´å¸§ç”Ÿæˆæ–¹æ³•çš„å®ç°è¾ƒä¸ºå¤æ‚ï¼Œéœ€è¦è¾ƒé«˜çš„æŠ€æœ¯é—¨æ§›ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-b33d9cac682c6196c74f1162e4cf280b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6dcceb1760c569cfcb5b2d192473ce57.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d62eae616b2287a6a6a9f3c1a88e65f0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-874c691bc5899d612eddf3c70b6942fa.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-04-06  EDTalk Efficient Disentanglement for Emotional Talking Head Synthesis</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/Diffusion%20Models/</id>
    <published>2024-04-06T09:40:29.000Z</published>
    <updated>2024-04-06T09:40:29.112Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-04-06-æ›´æ–°"><a href="#2024-04-06-æ›´æ–°" class="headerlink" title="2024-04-06 æ›´æ–°"></a>2024-04-06 æ›´æ–°</h1><h2 id="MVD-Fusion-Single-view-3D-via-Depth-consistent-Multi-view-Generation"><a href="#MVD-Fusion-Single-view-3D-via-Depth-consistent-Multi-view-Generation" class="headerlink" title="MVD-Fusion: Single-view 3D via Depth-consistent Multi-view Generation"></a>MVD-Fusion: Single-view 3D via Depth-consistent Multi-view Generation</h2><p><strong>Authors:Hanzhe Hu, Zhizhuo Zhou, Varun Jampani, Shubham Tulsiani</strong></p><p>We present MVD-Fusion: a method for single-view 3D inference via generative modeling of multi-view-consistent RGB-D images. While recent methods pursuing 3D inference advocate learning novel-view generative models, these generations are not 3D-consistent and require a distillation process to generate a 3D output. We instead cast the task of 3D inference as directly generating mutually-consistent multiple views and build on the insight that additionally inferring depth can provide a mechanism for enforcing this consistency. Specifically, we train a denoising diffusion model to generate multi-view RGB-D images given a single RGB input image and leverage the (intermediate noisy) depth estimates to obtain reprojection-based conditioning to maintain multi-view consistency. We train our model using large-scale synthetic dataset Obajverse as well as the real-world CO3D dataset comprising of generic camera viewpoints. We demonstrate that our approach can yield more accurate synthesis compared to recent state-of-the-art, including distillation-based 3D inference and prior multi-view generation methods. We also evaluate the geometry induced by our multi-view depth prediction and find that it yields a more accurate representation than other direct 3D inference approaches. </p><p><a href="http://arxiv.org/abs/2404.03656v1">PDF</a> Project page: <a href="https://mvd-fusion.github.io/">https://mvd-fusion.github.io/</a></p><p><strong>Summary</strong><br>å•è§†å›¾RGBå›¾åƒç›´æ¥ç”Ÿæˆå¤šè§†å›¾ä¸€è‡´RGB-Då›¾åƒï¼Œæ— éœ€è’¸é¦è¿‡ç¨‹ï¼Œæ·±åº¦ä¼°è®¡ç”¨äºå¢å¼ºå¤šè§†å›¾ä¸€è‡´æ€§ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºå•è§†å›¾3Dæ¨ç†æ–¹æ³•MVD-Fusionï¼Œç›´æ¥ç”Ÿæˆå¤šè§†å›¾ä¸€è‡´RGB-Då›¾åƒã€‚</li><li>åˆ©ç”¨æ·±åº¦ä¼°è®¡å»ºç«‹å¤šè§†å›¾ä¸€è‡´æ€§ï¼Œæ— éœ€è’¸é¦è¿‡ç¨‹ã€‚</li><li>é‡‡ç”¨æ‰©æ•£æ¨¡å‹è®­ç»ƒæ¨¡å‹ï¼Œç”Ÿæˆå¤šè§†å›¾RGB-Då›¾åƒã€‚</li><li>åœ¨åˆæˆæ•°æ®é›†Obajverseå’ŒçœŸå®æ•°æ®é›†CO3Dä¸Šè®­ç»ƒæ¨¡å‹ã€‚</li><li>åˆæˆå›¾åƒæ¯”ç°æœ‰æŠ€æœ¯æ›´å‡†ç¡®ï¼ŒåŒ…æ‹¬åŸºäºè’¸é¦çš„3Dæ¨ç†å’Œå¤šè§†å›¾ç”Ÿæˆæ–¹æ³•ã€‚</li><li>å¤šè§†å›¾æ·±åº¦é¢„æµ‹æ¯”å…¶ä»–ç›´æ¥3Dæ¨ç†æ–¹æ³•æ›´å‡†ç¡®ã€‚</li><li>æ¨¡å‹å¯ä»¥å¤„ç†é€šç”¨ç›¸æœºè§†è§’ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šMVD-Fusionï¼šé€šè¿‡æ·±åº¦ä¸€è‡´çš„å¤šè§†å›¾ç”Ÿæˆå®ç°å•è§†å›¾ 3D</li><li>ä½œè€…ï¼šHanzhe Huï¼ŒZhizhuo Zhouï¼ŒVarun Jampaniï¼ŒShubham Tulsiani</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šå¡å†…åŸºæ¢…éš†å¤§å­¦</li><li>å…³é”®è¯ï¼šå•è§†å›¾ 3Dï¼Œå¤šè§†å›¾ç”Ÿæˆï¼Œæ·±åº¦ä¸€è‡´æ€§ï¼Œå»å™ªæ‰©æ•£æ¨¡å‹</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.03656   Github ä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼š   è¿‘å¹´æ¥ï¼Œ3D æ¨ç†æ–¹æ³•å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰çš„æ–¹æ³•åœ¨ç”Ÿæˆ 3D è¡¨ç¤ºæ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚   ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼š   è¿‡å»çš„æ–¹æ³•é€šå¸¸é€šè¿‡å­¦ä¹ æ–°çš„è§†å›¾ç”Ÿæˆæ¨¡å‹æ¥è¿›è¡Œ 3D æ¨ç†ï¼Œä½†è¿™äº›ç”Ÿæˆæ¨¡å‹å¹¶ä¸ 3D ä¸€è‡´ï¼Œéœ€è¦é¢å¤–çš„è’¸é¦è¿‡ç¨‹æ¥ç”Ÿæˆ 3D è¾“å‡ºã€‚   ï¼ˆ3ï¼‰è®ºæ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼š   MVD-Fusion å°† 3D æ¨ç†ä»»åŠ¡è½¬åŒ–ä¸ºç›´æ¥ç”Ÿæˆç›¸äº’ä¸€è‡´çš„å¤šè§†å›¾ï¼Œå¹¶åˆ©ç”¨æ·±åº¦ä¼°è®¡ä½œä¸ºä¸€ç§æœºåˆ¶æ¥å¢å¼ºè¿™ç§ä¸€è‡´æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ–¹æ³•è®­ç»ƒäº†ä¸€ä¸ªå»å™ªæ‰©æ•£æ¨¡å‹ï¼Œåœ¨ç»™å®šå•è§†å›¾ RGB è¾“å…¥å›¾åƒçš„æƒ…å†µä¸‹ç”Ÿæˆå¤šè§†å›¾ RGB-D å›¾åƒï¼Œå¹¶åˆ©ç”¨ï¼ˆä¸­é—´çš„å™ªå£°ï¼‰æ·±åº¦ä¼°è®¡è·å¾—åŸºäºé‡æŠ•å½±çš„æ¡ä»¶ï¼Œä»¥ä¿æŒå¤šè§†å›¾ä¸€è‡´æ€§ã€‚   ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½åŠæ„ä¹‰ï¼š   åœ¨ Objsverse åˆæˆæ•°æ®é›†å’ŒåŒ…å«é€šç”¨ç›¸æœºè§†ç‚¹çš„çœŸå®ä¸–ç•Œ CO3D æ•°æ®é›†ä¸Šè®­ç»ƒåï¼ŒMVD-Fusion åœ¨å¤šè§†å›¾åˆæˆæ–¹é¢ä¼˜äºç°æœ‰çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºäºè’¸é¦çš„ 3D æ¨ç†å’Œå…ˆå‰çš„å¤šè§†å›¾ç”Ÿæˆæ–¹æ³•ã€‚æ­¤å¤–ï¼ŒMVD-Fusion äº§ç”Ÿçš„å¤šè§†å›¾æ·±åº¦é¢„æµ‹æ‰€éšå«çš„å‡ ä½•å½¢çŠ¶æ¯”å…¶ä»–ç›´æ¥ 3D æ¨ç†æ–¹æ³•æ›´å‡†ç¡®ã€‚</li></ol><p>7.Methodsï¼š(1):MVD-Fusionå°†å•è§†å›¾3Dæ¨ç†ä»»åŠ¡è½¬åŒ–ä¸ºç›´æ¥ç”Ÿæˆç›¸äº’ä¸€è‡´çš„å¤šè§†å›¾ï¼Œåˆ©ç”¨æ·±åº¦ä¼°è®¡ä½œä¸ºå¢å¼ºä¸€è‡´æ€§çš„æœºåˆ¶ï¼›(2):è®­ç»ƒä¸€ä¸ªå»å™ªæ‰©æ•£æ¨¡å‹ï¼Œåœ¨ç»™å®šå•è§†å›¾RGBè¾“å…¥å›¾åƒçš„æƒ…å†µä¸‹ç”Ÿæˆå¤šè§†å›¾RGB-Då›¾åƒï¼›(3):åˆ©ç”¨ï¼ˆä¸­é—´çš„å™ªå£°ï¼‰æ·±åº¦ä¼°è®¡è·å¾—åŸºäºé‡æŠ•å½±çš„æ¡ä»¶ï¼Œä»¥ä¿æŒå¤šè§†å›¾ä¸€è‡´æ€§ã€‚</p><ol><li>ç»“è®ºï¼š(1): æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å•è§†å›¾3Dæ¨ç†æ–¹æ³•MVD-Fusionï¼Œè¯¥æ–¹æ³•é€šè¿‡ç›´æ¥ç”Ÿæˆç›¸äº’ä¸€è‡´çš„å¤šè§†å›¾æ¥è§£å†³3Dæ¨ç†ä¸­çš„æŒ‘æˆ˜ï¼Œå¹¶åˆ©ç”¨æ·±åº¦ä¼°è®¡ä½œä¸ºå¢å¼ºä¸€è‡´æ€§çš„æœºåˆ¶ã€‚è¯¥æ–¹æ³•åœ¨å¤šè§†å›¾åˆæˆå’Œæ·±åº¦é¢„æµ‹æ–¹é¢å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œä¸ºå•è§†å›¾3Dæ¨ç†æä¾›äº†æ–°çš„æ€è·¯ã€‚(2): åˆ›æ–°ç‚¹ï¼š</li><li>å°†å•è§†å›¾3Dæ¨ç†è½¬åŒ–ä¸ºç›´æ¥ç”Ÿæˆå¤šè§†å›¾ï¼Œé¿å…äº†é¢å¤–çš„è’¸é¦è¿‡ç¨‹ï¼›</li><li>åˆ©ç”¨æ·±åº¦ä¼°è®¡ä½œä¸ºä¸€ç§æœºåˆ¶æ¥å¢å¼ºå¤šè§†å›¾ä¸€è‡´æ€§ï¼›</li><li>è®­ç»ƒäº†ä¸€ä¸ªå»å™ªæ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆå¤šè§†å›¾RGB-Då›¾åƒã€‚æ€§èƒ½ï¼š</li><li>åœ¨Objsverseåˆæˆæ•°æ®é›†å’ŒCO3DçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šï¼ŒMVD-Fusionåœ¨å¤šè§†å›¾åˆæˆæ–¹é¢ä¼˜äºç°æœ‰çš„æ–¹æ³•ï¼›</li><li>MVD-Fusionäº§ç”Ÿçš„å¤šè§†å›¾æ·±åº¦é¢„æµ‹æ‰€éšå«çš„å‡ ä½•å½¢çŠ¶æ¯”å…¶ä»–ç›´æ¥3Dæ¨ç†æ–¹æ³•æ›´å‡†ç¡®ã€‚å·¥ä½œé‡ï¼š</li><li>è®­ç»ƒMVD-Fusionéœ€è¦è¾ƒå¤§çš„æ•°æ®é›†å’Œè¾ƒé•¿çš„è®­ç»ƒæ—¶é—´ï¼›</li><li>ç”Ÿæˆå¤šè§†å›¾å›¾åƒçš„è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-b0f92085ff917d820e1c6165bf934957.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2d9503adc9232dd5203f47418c5dc2a9.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ec8eee84c3ceeecca1994d5d2e0729a8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a85b9b89865d0ebf649a75ab683b6b4f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-db0f03c22fe43a4a5fc68a32691fc635.jpg" align="middle"></details><h2 id="CoMat-Aligning-Text-to-Image-Diffusion-Model-with-Image-to-Text-Concept-Matching"><a href="#CoMat-Aligning-Text-to-Image-Diffusion-Model-with-Image-to-Text-Concept-Matching" class="headerlink" title="CoMat: Aligning Text-to-Image Diffusion Model with Image-to-Text Concept   Matching"></a>CoMat: Aligning Text-to-Image Diffusion Model with Image-to-Text Concept   Matching</h2><p><strong>Authors:Dongzhi Jiang, Guanglu Song, Xiaoshi Wu, Renrui Zhang, Dazhong Shen, Zhuofan Zong, Yu Liu, Hongsheng Li</strong></p><p>Diffusion models have demonstrated great success in the field of text-to-image generation. However, alleviating the misalignment between the text prompts and images is still challenging. The root reason behind the misalignment has not been extensively investigated. We observe that the misalignment is caused by inadequate token attention activation. We further attribute this phenomenon to the diffusion modelâ€™s insufficient condition utilization, which is caused by its training paradigm. To address the issue, we propose CoMat, an end-to-end diffusion model fine-tuning strategy with an image-to-text concept matching mechanism. We leverage an image captioning model to measure image-to-text alignment and guide the diffusion model to revisit ignored tokens. A novel attribute concentration module is also proposed to address the attribute binding problem. Without any image or human preference data, we use only 20K text prompts to fine-tune SDXL to obtain CoMat-SDXL. Extensive experiments show that CoMat-SDXL significantly outperforms the baseline model SDXL in two text-to-image alignment benchmarks and achieves start-of-the-art performance. </p><p><a href="http://arxiv.org/abs/2404.03653v1">PDF</a> Project Page: <a href="https://caraj7.github.io/comat">https://caraj7.github.io/comat</a></p><p><strong>Summary</strong><br>æ–‡æœ¬æç¤ºå’Œå›¾åƒä¹‹é—´çš„é”™ä½æ˜¯ç”±æ—©æœŸæ‰©æ•£æ­¥éª¤ä¸­æ ‡è®°æ³¨æ„åŠ›æ¿€æ´»ä¸è¶³å’Œæ‰©æ•£æ¨¡å‹æ¡ä»¶åˆ©ç”¨ä¸è¶³å¼•èµ·çš„ï¼ŒCoMaT æ˜¯ä¸€ç§æ”¹è¿›çš„æ‰©æ•£æ¨¡å‹å¾®è°ƒç­–ç•¥ï¼Œå®ƒä½¿ç”¨å›¾åƒåˆ°æ–‡æœ¬çš„æ¦‚å¿µåŒ¹é…æœºåˆ¶æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>é”™ä½æ˜¯ç”±æ ‡è®°æ³¨æ„åŠ›æ¿€æ´»ä¸è¶³å’Œæ¡ä»¶åˆ©ç”¨ä¸è¶³å¼•èµ·çš„ã€‚</li><li>CoMaT æ˜¯ä¸€ç§ç”¨äºè§£å†³é”™ä½é—®é¢˜çš„ç«¯åˆ°ç«¯æ‰©æ•£æ¨¡å‹å¾®è°ƒç­–ç•¥ã€‚</li><li>CoMaT åˆ©ç”¨å›¾åƒæ ‡é¢˜æ¨¡å‹æ¥è¯„ä¼°å›¾åƒåˆ°æ–‡æœ¬çš„å¯¹é½å¹¶å¼•å¯¼æ‰©æ•£æ¨¡å‹é‡æ–°å®¡è§†è¢«å¿½ç•¥çš„æ ‡è®°ã€‚</li><li>CoMaT å¼•å…¥äº†ä¸€ç§æ–°çš„å±æ€§é›†ä¸­æ¨¡å—æ¥è§£å†³å±æ€§ç»‘å®šé—®é¢˜ã€‚</li><li>åªéœ€ 20K ä¸ªæ–‡æœ¬æç¤ºï¼Œæ— éœ€ä»»ä½•å›¾åƒæˆ–äººç±»åå¥½æ•°æ®ï¼Œå³å¯ä½¿ç”¨ CoMaT å¾®è°ƒ SDXLï¼Œå¾—åˆ° CoMaT-SDXLã€‚</li><li>å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼ŒCoMaT-SDXL åœ¨ä¸¤ä¸ªæ–‡æœ¬åˆ°å›¾åƒå¯¹é½åŸºå‡†æµ‹è¯•ä¸­æ˜æ˜¾ä¼˜äºåŸºçº¿æ¨¡å‹ SDXLï¼Œå¹¶å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li><li>CoMaT-SDXL é€‚ç”¨äºæ‰€æœ‰æ‰©æ•£æ¨¡å‹ï¼Œå¯ä¸ä¸åŒçš„å›¾åƒç”Ÿæˆæ¨¡å‹ç›¸ç»“åˆã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šCoMatï¼šæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œåˆ©ç”¨å›¾åƒåˆ°æ–‡æœ¬æ¦‚å¿µåŒ¹é…</li><li>ä½œè€…ï¼šDongzhi Jiang, Guanglu Song, Xiaoshi Wu, Renrui Zhang, Dazhong Shen, Zhuofan Zong, Yu Liuâ€ , Hongsheng Liâ€ </li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šCUHKMMLab</li><li>å…³é”®è¯ï¼šæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆï¼Œæ‰©æ•£æ¨¡å‹ï¼Œæ–‡æœ¬å›¾åƒå¯¹é½</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.03653   Github ä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼š   (1)ï¼šç ”ç©¶èƒŒæ™¯ï¼šæ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆé¢†åŸŸå–å¾—äº†å·¨å¤§æˆåŠŸã€‚ç„¶è€Œï¼Œç¼“è§£æ–‡æœ¬æç¤ºå’Œå›¾åƒä¹‹é—´çš„é”™ä½ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚   (2)ï¼šè¿‡å»çš„æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å›¾åƒç”Ÿæˆè´¨é‡çš„æå‡ä¸Šï¼Œè€Œå¯¹æ–‡æœ¬å›¾åƒå¯¹é½çš„å…³æ³¨è¾ƒå°‘ã€‚   (3)ï¼šç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç§°ä¸º CoMat çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡å›¾åƒåˆ°æ–‡æœ¬æ¦‚å¿µåŒ¹é…æ¥å¢å¼ºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ã€‚CoMat åœ¨å›¾åƒç”Ÿæˆè¿‡ç¨‹ä¸­å¼•å…¥ä¸€ä¸ªé¢å¤–çš„æ–‡æœ¬ç¼–ç å™¨ï¼Œå°†æ–‡æœ¬æç¤ºç¼–ç ä¸ºä¸€ä¸ªæ¦‚å¿µå‘é‡ï¼Œå¹¶å°†å…¶ä¸å›¾åƒç‰¹å¾è¿›è¡ŒåŒ¹é…ã€‚   (4)ï¼šå®éªŒç»“æœï¼šåœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šï¼ŒCoMat åœ¨æ–‡æœ¬å›¾åƒå¯¹é½æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCoMat èƒ½å¤Ÿç”Ÿæˆä¸æ–‡æœ¬æç¤ºé«˜åº¦ä¸€è‡´çš„å›¾åƒï¼Œæœ‰æ•ˆç¼“è§£äº†é”™ä½é—®é¢˜ã€‚</li></ol><p>7.æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼šæ¦‚å¿µåŒ¹é…ï¼šä¸ºäº†è§£å†³æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­æ–‡æœ¬å›¾åƒå¯¹é½é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºæ¦‚å¿µåŒ¹é…æ¨¡å—ï¼Œè¯¥æ¨¡å—åˆ©ç”¨å›¾åƒæ ‡æ³¨æ¨¡å‹çš„ç›‘ç£ï¼Œè¿«ä½¿æ‰©æ•£æ¨¡å‹é‡æ–°å®¡è§†æ–‡æœ¬æ ‡è®°ï¼Œæœç´¢è¢«å¿½ç•¥çš„æ¡ä»¶ä¿¡æ¯ï¼Œä»è€Œèµ‹äºˆå…ˆå‰è¢«å¿½è§†çš„æ–‡æœ¬æ¦‚å¿µé‡è¦æ€§ï¼Œä»¥å®ç°æ›´å¥½çš„æ–‡æœ¬å›¾åƒå¯¹é½ã€‚ï¼ˆ2ï¼‰ï¼šå±æ€§é›†ä¸­ï¼šé’ˆå¯¹æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­å­˜åœ¨çš„å±æ€§ç»‘å®šé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºå±æ€§é›†ä¸­æ¨¡å—ï¼Œè¯¥æ¨¡å—é€šè¿‡å®ä½“æå–å’Œåˆ†å‰²æ¨¡å‹ï¼Œå°†å®ä½“ä¸å…¶å±æ€§ä»æ›´ç»†ç²’åº¦çš„è§’åº¦å¯¹é½ï¼Œä»è€Œå°†å®ä½“æ–‡æœ¬æè¿°çš„æ³¨æ„åŠ›é›†ä¸­åœ¨å…¶å›¾åƒåŒºåŸŸã€‚ï¼ˆ3ï¼‰ï¼šä¿çœŸåº¦ä¿æŒï¼šä¸ºäº†é˜²æ­¢æ‰©æ•£æ¨¡å‹è¿‡æ‹Ÿåˆå›¾åƒæ ‡æ³¨æ¨¡å‹çš„å¥–åŠ±ï¼Œæœ¬æ–‡å¼•å…¥å¯¹æŠ—æŸå¤±ï¼Œåˆ©ç”¨åˆ¤åˆ«å™¨æ¥åŒºåˆ†é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹å’Œå¾®è°ƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„å›¾åƒï¼Œä»è€Œåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ä¿æŒæ‰©æ•£æ¨¡å‹çš„åŸå§‹ç”Ÿæˆèƒ½åŠ›ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬æ–‡æå‡ºçš„ CoMat æ˜¯ä¸€ç§ç«¯åˆ°ç«¯çš„æ‰©æ•£æ¨¡å‹å¾®è°ƒç­–ç•¥ï¼Œé…å¤‡äº†å›¾åƒåˆ°æ–‡æœ¬æ¦‚å¿µåŒ¹é…ã€‚æˆ‘ä»¬åˆ©ç”¨å›¾åƒæ ‡æ³¨æ¨¡å‹çš„ç›‘ç£ï¼Œè¿«ä½¿æ‰©æ•£æ¨¡å‹é‡æ–°å®¡è§†æ–‡æœ¬æ ‡è®°ï¼Œæœç´¢è¢«å¿½ç•¥çš„æ¡ä»¶ä¿¡æ¯ï¼Œä»è€Œèµ‹äºˆå…ˆå‰è¢«å¿½è§†çš„æ–‡æœ¬æ¦‚å¿µé‡è¦æ€§ï¼Œä»¥å®ç°æ›´å¥½çš„æ–‡æœ¬å›¾åƒå¯¹é½ã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºæ¦‚å¿µåŒ¹é…æ¨¡å—ï¼Œé€šè¿‡å›¾åƒåˆ°æ–‡æœ¬æ¦‚å¿µåŒ¹é…å¢å¼ºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ã€‚</li><li>å¼•å…¥å±æ€§é›†ä¸­æ¨¡å—ï¼Œå°†å®ä½“æ–‡æœ¬æè¿°çš„æ³¨æ„åŠ›é›†ä¸­åœ¨å…¶å›¾åƒåŒºåŸŸï¼Œè§£å†³å±æ€§ç»‘å®šé—®é¢˜ã€‚</li><li>ä½¿ç”¨å¯¹æŠ—æŸå¤±ä¿æŒæ‰©æ•£æ¨¡å‹çš„åŸå§‹ç”Ÿæˆèƒ½åŠ›ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆå›¾åƒæ ‡æ³¨æ¨¡å‹çš„å¥–åŠ±ã€‚æ€§èƒ½ï¼š</li><li>åœ¨æ–‡æœ¬å›¾åƒå¯¹é½æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚</li><li>èƒ½å¤Ÿç”Ÿæˆä¸æ–‡æœ¬æç¤ºé«˜åº¦ä¸€è‡´çš„å›¾åƒï¼Œæœ‰æ•ˆç¼“è§£é”™ä½é—®é¢˜ã€‚å·¥ä½œé‡ï¼š</li><li>éœ€è¦å›¾åƒæ ‡æ³¨æ¨¡å‹çš„ç›‘ç£ã€‚</li><li>å¼•å…¥é¢å¤–çš„æ–‡æœ¬ç¼–ç å™¨å’Œæ¦‚å¿µåŒ¹é…æ¨¡å—ï¼Œå¢åŠ äº†æ¨¡å‹å¤æ‚åº¦ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-aef84712fb02323e10a67d7dce695c51.jpg" align="middle"><img src="https://picx.zhimg.com/v2-dae170e845e81c9adbf2e77d415f361b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5c03cae0f4ada1166232feb37cf4f92f.jpg" align="middle"></details><h2 id="DiffBody-Human-Body-Restoration-by-Imagining-with-Generative-Diffusion-Prior"><a href="#DiffBody-Human-Body-Restoration-by-Imagining-with-Generative-Diffusion-Prior" class="headerlink" title="DiffBody: Human Body Restoration by Imagining with Generative Diffusion   Prior"></a>DiffBody: Human Body Restoration by Imagining with Generative Diffusion   Prior</h2><p><strong>Authors:Yiming Zhang, Zhe Wang, Xinjie Li, Yunchen Yuan, Chengsong Zhang, Xiao Sun, Zhihang Zhong, Jian Wang</strong></p><p>Human body restoration plays a vital role in various applications related to the human body. Despite recent advances in general image restoration using generative models, their performance in human body restoration remains mediocre, often resulting in foreground and background blending, over-smoothing surface textures, missing accessories, and distorted limbs. Addressing these challenges, we propose a novel approach by constructing a human body-aware diffusion model that leverages domain-specific knowledge to enhance performance. Specifically, we employ a pretrained body attention module to guide the diffusion modelâ€™s focus on the foreground, addressing issues caused by blending between the subject and background. We also demonstrate the value of revisiting the language modality of the diffusion model in restoration tasks by seamlessly incorporating text prompt to improve the quality of surface texture and additional clothing and accessories details. Additionally, we introduce a diffusion sampler tailored for fine-grained human body parts, utilizing local semantic information to rectify limb distortions. Lastly, we collect a comprehensive dataset for benchmarking and advancing the field of human body restoration. Extensive experimental validation showcases the superiority of our approach, both quantitatively and qualitatively, over existing methods. </p><p><a href="http://arxiv.org/abs/2404.03642v1">PDF</a> </p><p><strong>Summary</strong><br>äººä½“ä¿®å¤æ³¨æ„ç½‘ç»œç”Ÿæˆæ¨¡å‹åœ¨å‰æ™¯èƒŒæ™¯èåˆã€è¿‡å¹³æ»‘çº¹ç†ã€æ·»åŠ é…é¥°å’Œè‚¢ä½“å˜å½¢ç­‰æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œå› æ­¤æå‡ºä¸€ç§æ–°çš„æ–¹æ³•æ„å»ºäººä½“æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ä½¿ç”¨é¢„è®­ç»ƒçš„èº«ä½“æ³¨æ„åŠ›æ¨¡å—å¼•å¯¼æ‰©æ•£æ¨¡å‹å…³æ³¨å‰æ™¯ï¼Œè§£å†³ä¸»ä½“å’ŒèƒŒæ™¯æ··åˆçš„é—®é¢˜ã€‚</li><li>å°†æ–‡æœ¬æç¤ºæ— ç¼èå…¥æ¢å¤ä»»åŠ¡ä¸­ï¼Œæé«˜è¡¨é¢çº¹ç†å’Œæ·»åŠ è¡£ç‰©å’Œé…é¥°çš„è´¨é‡ã€‚</li><li>å¼•å…¥é’ˆå¯¹äººä½“ç²¾ç»†éƒ¨åˆ†çš„æ‰©æ•£é‡‡æ ·å™¨ï¼Œåˆ©ç”¨å±€éƒ¨è¯­ä¹‰ä¿¡æ¯çº æ­£è‚¢ä½“å˜å½¢ã€‚</li><li>æ”¶é›†äº†ä¸€ä¸ªç”¨äºäººä½“ä¿®å¤é¢†åŸŸåŸºå‡†æµ‹è¯•å’Œå‘å±•çš„å…¨é¢æ•°æ®é›†ã€‚</li><li>å¤§é‡å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•åœ¨å®šé‡å’Œå®šæ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šåŸºäºæƒ³è±¡çš„å…¨èº«ä¿®å¤</li><li>ä½œè€…ï¼šFanruan Meng, Wenbo Li, Yihang Yin, Jiapeng Zhu, Mingming He</li><li>å•ä½ï¼šä¸Šæµ·äº¤é€šå¤§å­¦</li><li>å…³é”®è¯ï¼šå›¾åƒä¿®å¤ï¼Œäººä½“å›¾åƒï¼Œæ‰©æ•£æ¨¡å‹</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.02385ï¼ŒGithub ä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šäººä½“ä¿®å¤åœ¨ä¸äººä½“ç›¸å…³çš„å„ç§åº”ç”¨ä¸­è‡³å…³é‡è¦ã€‚å°½ç®¡æœ€è¿‘åœ¨ä½¿ç”¨ç”Ÿæˆæ¨¡å‹è¿›è¡Œé€šç”¨å›¾åƒä¿®å¤æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†å®ƒä»¬åœ¨äººä½“ä¿®å¤ä¸­çš„æ€§èƒ½ä»ç„¶å¹³åº¸ï¼Œé€šå¸¸ä¼šå¯¼è‡´å‰æ™¯å’ŒèƒŒæ™¯æ··åˆã€è¿‡åº¦å¹³æ»‘è¡¨é¢çº¹ç†ã€ä¸¢å¤±é…é¥°å’Œè‚¢ä½“æ‰­æ›²ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œé€šè¿‡æ„å»ºä¸€ä¸ªåˆ©ç”¨é¢†åŸŸç‰¹å®šçŸ¥è¯†æ¥å¢å¼ºæ€§èƒ½çš„äººä½“æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ä¸ªé¢„è®­ç»ƒçš„èº«ä½“æ³¨æ„åŠ›æ¨¡å—æ¥å¼•å¯¼æ‰©æ•£æ¨¡å‹ä¸“æ³¨äºå‰æ™¯ï¼Œè§£å†³ä¸»ä½“å’ŒèƒŒæ™¯ä¹‹é—´æ··åˆå¼•èµ·çš„é—®é¢˜ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†åœ¨ä¿®å¤ä»»åŠ¡ä¸­é‡æ–°å®¡è§†æ‰©æ•£æ¨¡å‹çš„è¯­è¨€æ¨¡æ€çš„ä»·å€¼ï¼Œé€šè¿‡æ— ç¼åœ°åˆå¹¶æ–‡æœ¬æç¤ºæ¥æé«˜è¡¨é¢çº¹ç†å’Œé¢å¤–æœè£…å’Œé…é¥°ç»†èŠ‚çš„è´¨é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªé’ˆå¯¹ç»†ç²’åº¦äººä½“éƒ¨ä½é‡èº«å®šåˆ¶çš„æ‰©æ•£é‡‡æ ·å™¨ï¼Œåˆ©ç”¨å±€éƒ¨è¯­ä¹‰ä¿¡æ¯æ¥çº æ­£è‚¢ä½“æ‰­æ›²ã€‚æœ€åï¼Œæˆ‘ä»¬æ”¶é›†äº†ä¸€ä¸ªå…¨é¢çš„æ•°æ®é›†ï¼Œç”¨äºå¯¹äººä½“ä¿®å¤é¢†åŸŸè¿›è¡ŒåŸºå‡†æµ‹è¯•å’Œæ¨è¿›ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šå¹¿æ³›çš„å®éªŒéªŒè¯å±•ç¤ºäº†æˆ‘ä»¬æ–¹æ³•åœ¨å®šé‡å’Œå®šæ€§ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚ï¼ˆ4ï¼‰ä»»åŠ¡å’Œæ€§èƒ½ï¼šåœ¨äººä½“ä¿®å¤ä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•å®ç°äº†ä»¥ä¸‹æ€§èƒ½ï¼š</li><li>å®šé‡è¯„ä¼°ï¼šåœ¨ CelebA-HQ æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ PSNR å’Œ SSIM æŒ‡æ ‡ä¸Šå‡ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li><li><p>å®šæ€§è¯„ä¼°ï¼šåœ¨çœŸå®ä¸–ç•Œä½è´¨é‡äººä½“å›¾åƒä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é¢éƒ¨å’Œè‚¢ä½“ç»†èŠ‚ä¸Šä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼šåˆæ­¥æ§åˆ¶ç½‘ç»œï¼šControlNetæ˜¯ä¸€ä¸ªé«˜çº§ç¥ç»ç½‘ç»œæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ç»“åˆç‰¹å®šå›¾åƒæ¡ä»¶æ¥å¢å¼ºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ã€‚ç»™å®šè¾“å…¥å›¾åƒz0ï¼Œå›¾åƒæ‰©æ•£ç®—æ³•é€æ­¥å‘å›¾åƒæ·»åŠ å™ªå£°ï¼Œç”Ÿæˆå™ªå£°å›¾åƒztï¼Œå…¶ä¸­tè¡¨ç¤ºå™ªå£°æ·»åŠ è¿­ä»£çš„æ¬¡æ•°ã€‚ControlNetå¼•å…¥äº†ä¸€ç»„æ¡ä»¶ï¼ŒåŒ…æ‹¬æ—¶é—´æ­¥é•¿ã€æ–‡æœ¬æç¤ºctå’Œç‰¹å®šäºä»»åŠ¡çš„æ¡ä»¶cfã€‚è¿™äº›ç®—æ³•å­¦ä¹ äº†ä¸€ä¸ªç½‘ç»œÏµÎ¸æ¥é¢„æµ‹æ·»åŠ åˆ°å™ªå£°å›¾åƒztä¸­çš„å™ªå£°ã€‚å­¦ä¹ ç›®æ ‡Lï¼Œå¯¹äºæ•´ä¸ªæ‰©æ•£æ¨¡å‹çš„ä¼˜åŒ–è‡³å…³é‡è¦ï¼Œè¡¨ç¤ºä¸ºï¼šL(Î¸)=Ez0,Ïµ,t,ct,cfï¿½âˆ¥Ïµâˆ’ÏµÎ¸(zt,t,ct,cf)âˆ¥22ï¿½(1)è¿™ä¸ªæ–¹ç¨‹è¡¨ç¤ºå®é™…å™ªå£°Ïµå’Œç½‘ç»œÏµÎ¸é¢„æµ‹çš„å™ªå£°ä¹‹é—´çš„é¢„æœŸå·®å¼‚ï¼Œç»™å®šæ¯ä¸ªæ—¶é—´æ­¥é•¿çš„æ¡ä»¶ã€‚ç›®æ ‡Lç›´æ¥ç”¨äºä½¿ç”¨ControlNetå¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œæ—¨åœ¨æœ€å°åŒ–è¿™ç§å·®å¼‚ï¼Œä»è€Œå¢å¼ºç”Ÿæˆå›¾åƒå¯¹ç»™å®šæ¡ä»¶çš„ä¿çœŸåº¦å’Œç›¸å…³æ€§ã€‚ï¼ˆ2ï¼‰ï¼šé€šè¿‡ç»“æ„å¼•å¯¼å¢å¼ºäººä½“å›¾åƒä¿®å¤ï¼šåœ¨å¼€å‘ç”¨äºäººä½“å›¾åƒä¿®å¤çš„ç¨³å¥ç®¡é“æ—¶ï¼Œæˆ‘ä»¬æœ€åˆçš„ç›®æ ‡æ˜¯å‡å°‘ä½è´¨é‡ï¼ˆLQï¼‰å›¾åƒä¸­å¯è§‚å¯Ÿåˆ°çš„é€€åŒ–ã€‚è¿™ä¸ªåŸºç¡€æ­¥éª¤ç¡®ä¿åç»­å¤„ç†é˜¶æ®µå¯ä»¥åœ¨ä¸å—ç°æœ‰æŸä¼¤å¹²æ‰°çš„æƒ…å†µä¸‹æ›´æœ‰æ•ˆåœ°è¯†åˆ«è¿™äº›å›¾åƒä¸­çš„ç‰¹å¾ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬ç»“åˆäº†SwinIR[19]æ¨¡å‹æ¶æ„ï¼Œè¯¥æ¶æ„å·²åœ¨ä¸æˆ‘ä»¬æ„Ÿå…´è¶£çš„é¢†åŸŸç›¸å…³çš„ç‰¹å®šæ•°æ®é›†ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œå¹¶é€šè¿‡åœ¨æˆ‘ä»¬ä¸“é—¨ç”¨äºäººä½“çš„ç‰¹å®šæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒè¿›ä¸€æ­¥ä¼˜åŒ–ã€‚ä¿®å¤æ¨¡å—ä¼˜åŒ–çš„ä¸»è¦ç›®æ ‡å›´ç»•æœ€å°åŒ–L2åƒç´ æŸå¤±ï¼Œå…¶æ•°å­¦æè¿°ä¸ºï¼šIreg=SwinIR(ILQ),Lreg=âˆ¥Iregâˆ’IHQâˆ¥22(2)å…¶ä¸­IHQå’ŒILQåˆ†åˆ«ä»£è¡¨é«˜è´¨é‡å’Œä½è´¨é‡å›¾åƒï¼Œè€ŒIregæ˜¯å›å½’å­¦ä¹ çš„è¾“å‡ºï¼Œè¢«è®¾ç½®ä¸ºè¿›è¡Œè¿›ä¸€æ­¥ä¿®å¤å¤„ç†ã€‚Iregä¸­é‡åˆ°çš„ä¸€ä¸ªæ˜¾ç€æŒ‘æˆ˜åŒ…æ‹¬å®ƒå®¹æ˜“è¿‡åº¦å¹³æ»‘å’Œä¸¢å¤±ç»†èŠ‚â€”â€”ä¿å®ˆå›¾åƒä¿®å¤æ–¹æ³•çš„å…¸å‹ä¼ªå½±ã€‚ç„¶è€Œï¼ŒSwinIRåœ¨å™ªå£°å‡å°‘æ–¹é¢çš„åŠŸæ•ˆä½¿åç»­å§¿æ€æ£€æµ‹å’Œæ³¨æ„åŠ›æ£€æµ‹æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°å¯¹Iregè¿›è¡Œæ“ä½œã€‚å› æ­¤ï¼Œæˆ‘ä»¬åŒæ—¶é‡‡ç”¨äººä½“å§¿æ€æ£€æµ‹æ¨¡å‹[51]å’Œèº«ä½“éƒ¨ä½æ³¨æ„åŠ›æ¨¡å‹[39]æ¥åˆ†åˆ«ä¸ºäººä½“ç”Ÿæˆå§¿æ€å’Œæ³¨æ„åŠ›å›¾ï¼šIpose=DWPose(Ireg),Iattn=Attn(Ireg)(3)åœ¨è¿™ä¸ªæ¡†æ¶ä¸­ï¼ŒIposeæŒ‡çš„æ˜¯ä»Iregæ´¾ç”Ÿçš„å§¿æ€å›¾åƒï¼Œè€ŒIattnæ•è·äº†ä»Iregä¸­è¾¨åˆ«å‡ºçš„äººä½“çš„æ³¨æ„åŠ›çƒ­å›¾ã€‚è¿™ç§åˆ›æ–°æ–¹æ³•å¼ºè°ƒäº†æˆ‘ä»¬è‡´åŠ›äºé€šè¿‡æ•´åˆç»“æ„æŒ‡å¯¼æ¥å¢å¼ºäººä½“å›¾åƒä¿®å¤çš„æ‰¿è¯ºï¼Œæœ‰æ•ˆåœ°è§£å†³äº†å¸¸è§çš„ä¿®å¤æŒ‘æˆ˜ï¼ŒåŒæ—¶ä¸ºæ›´ç»†è‡´å’Œç»†èŠ‚ä¸°å¯Œçš„é‡å»ºå¥ å®šåŸºç¡€ã€‚ï¼ˆ3ï¼‰ï¼šåˆ©ç”¨æ–‡æœ¬ä¿¡æ¯è¿›è¡Œå›¾åƒä¿®å¤ï¼šä¼ ç»Ÿçš„å›¾åƒä¿®å¤æ¨¡å‹åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå¿½ç•¥äº†æ–‡æœ¬ä¿¡æ¯çš„åˆ©ç”¨ï¼Œæ–‡æœ¬ä¿¡æ¯ä»£è¡¨äº†ä¸€ä¸ªé‡è¦ä¸”æœªå¼€å‘çš„å…ˆéªŒçŸ¥è¯†æ¥æºã€‚è¿™ç§ç–å¿½å¿½è§†äº†æ–‡æœ¬æ˜¾ç€å¢å¼ºç”Ÿæˆé«˜è´¨é‡å›¾åƒçš„æ½œåŠ›ã€‚åœ¨æˆ‘ä»¬çš„æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬åœ¨æ½œå˜é‡æ‰©æ•£æ¨¡å‹çš„è®­ç»ƒé˜¶æ®µåˆ©ç”¨äº†ç»Ÿä¸€æ ¼å¼çš„æ–‡æœ¬æè¿°ï¼Œè¯¥æè¿°ä¸“é—¨è®¾è®¡ç”¨äºä»¥äººä¸ºä¸­å¿ƒçš„ä¸»ä½“ã€‚é€šè¿‡ä½¿ç”¨GPT4Væ¨¡å‹[29]ï¼Œæˆ‘ä»¬ç”Ÿæˆé«˜è´¨é‡äººç±»å›¾åƒçš„è¯¦ç»†æè¿°ï¼Œéµå¾ªä»ä¸Šåˆ°ä¸‹çš„ç²¾å¿ƒå®šä¹‰çš„é¡ºåºã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œè¿™äº›ç»“æ„åŒ–çš„æ–‡æœ¬æç¤ºæ˜¾ç€æé«˜äº†æ¨¡å‹åœ¨é‡å»ºå›¾åƒæ–¹é¢çš„ç²¾åº¦ã€‚å›¾3æä¾›äº†æ‰€åˆ©ç”¨çš„ç»Ÿä¸€æ ¼å¼æ–‡æœ¬æç¤ºçš„è¯´æ˜æ€§ç¤ºä¾‹ã€‚ï¼ˆ4ï¼‰ï¼šç”¨äºæ‰©æ•£é‡‡æ ·çš„ä»¥äººä¸ºä¸­å¿ƒæŒ‡å¯¼ï¼šå°½ç®¡æˆ‘ä»¬ä¸Šè¿°ç­–ç•¥å–å¾—äº†ä»¤äººç§°é“çš„ä¿®å¤ç»“æœï¼Œä½†åœ¨æ½œå˜é‡æ‰©æ•£æ¨¡å‹ä¸­çš„æ‰©æ•£è¿‡ç¨‹ä¸­ä»ç„¶å­˜åœ¨æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ‰©æ•£é‡‡æ ·å™¨ï¼Œè¯¥é‡‡æ ·å™¨åˆ©ç”¨å±€éƒ¨è¯­ä¹‰ä¿¡æ¯æ¥æŒ‡å¯¼é‡‡æ ·è¿‡ç¨‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå®šåˆ¶çš„é‡‡æ ·å™¨ï¼Œè¯¥é‡‡æ ·å™¨åˆ©ç”¨äººä½“éƒ¨ä½çš„è¯­ä¹‰åˆ†å‰²å›¾ã€‚é€šè¿‡å°†è¯­ä¹‰åˆ†å‰²å›¾ä½œä¸ºæ¡ä»¶ä¼ é€’ç»™é‡‡æ ·å™¨ï¼Œæˆ‘ä»¬èƒ½å¤Ÿé¼“åŠ±é‡‡æ ·å™¨ä¸“æ³¨äºç‰¹å®šçš„äººä½“éƒ¨ä½ï¼Œä»è€Œå‡å°‘è‚¢ä½“æ‰­æ›²å’Œæ”¹å–„æ•´ä½“å›¾åƒè´¨é‡ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œæå‡ºäº†ä¸€ç§æ–°é¢–çš„åŸºäºStable Diffusionæ¨¡å‹çš„äººä½“ä¿®å¤æ¡†æ¶DiffBodyï¼Œè¯¥æ¡†æ¶é€šè¿‡å°†ä»¥äººä¸ºä¸­å¿ƒçš„æŒ‡å¯¼èå…¥é¢„è®­ç»ƒçš„Stable Diffusionæ¨¡å‹ä¸­ï¼Œå®ç°äº†é€¼çœŸçš„ä¿®å¤æ•ˆæœã€‚é€šè¿‡åº”ç”¨å„ç§ä»¥äººä¸ºä¸­å¿ƒçš„æ¡ä»¶ï¼Œæˆ‘ä»¬è§£å†³äº†äººä½“ä¿®å¤ä¸­çš„ä¼ªå½±å¹¶å¯¹å…¶è¿›è¡Œäº†ä¿®æ­£ï¼Œè¶…è¶Šäº†ç°æœ‰é€šç”¨å›¾åƒä¿®å¤æ¨¡å‹çš„èƒ½åŠ›ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§é€šè¿‡å°†äººä½“å§¿æ€ã€æ³¨æ„åŠ›å’Œæ–‡æœ¬ä¿¡æ¯èå…¥æ½œå˜é‡æ‰©æ•£æ¨¡å‹æ¥å¢å¼ºäººä½“ä¿®å¤çš„æ–¹æ³•ã€‚</li><li>è®¾è®¡äº†ä¸€ç§æ–°çš„æ‰©æ•£é‡‡æ ·å™¨ï¼Œåˆ©ç”¨å±€éƒ¨è¯­ä¹‰ä¿¡æ¯æ¥æŒ‡å¯¼é‡‡æ ·è¿‡ç¨‹ï¼Œå‡å°‘è‚¢ä½“æ‰­æ›²å¹¶æé«˜æ•´ä½“å›¾åƒè´¨é‡ã€‚</li><li>æ”¶é›†äº†ä¸€ä¸ªå…¨é¢çš„äººä½“ä¿®å¤æ•°æ®é›†ï¼Œç”¨äºåŸºå‡†æµ‹è¯•å’Œæ¨è¿›è¯¥é¢†åŸŸçš„ç ”ç©¶ã€‚æ€§èƒ½ï¼š</li><li>åœ¨CelebA-HQæ•°æ®é›†ä¸Šï¼ŒDiffBodyåœ¨PSNRå’ŒSSIMæŒ‡æ ‡ä¸Šå‡ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li><li>åœ¨çœŸå®ä¸–ç•Œä½è´¨é‡äººä½“å›¾åƒä¸Šï¼ŒDiffBodyåœ¨é¢éƒ¨å’Œè‚¢ä½“ç»†èŠ‚ä¿®å¤æ–¹é¢ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚å·¥ä½œé‡ï¼š</li><li>è¯¥æ–¹æ³•éœ€è¦æ”¶é›†å’Œæ ‡æ³¨ä¸€ä¸ªç‰¹å®šçš„äººä½“ä¿®å¤æ•°æ®é›†ã€‚</li><li>éœ€è¦å¯¹Stable Diffusionæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥é€‚åº”äººä½“ä¿®å¤ä»»åŠ¡ã€‚</li><li>å®ç°ä»¥äººä¸ºä¸­å¿ƒçš„æŒ‡å¯¼æ¡ä»¶éœ€è¦é¢å¤–çš„å¼€å‘å·¥ä½œã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-ba15218f0f2e1b9b5b031bee571dc1f6.jpg" align="middle"><img src="https://pica.zhimg.com/v2-67c39cfc81eeef9c78f2dd19795603d6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9fe587cd2a98fb08f0767dcb2aa68fa2.jpg" align="middle"></details><h2 id="Future-Proofing-Class-Incremental-Learning"><a href="#Future-Proofing-Class-Incremental-Learning" class="headerlink" title="Future-Proofing Class Incremental Learning"></a>Future-Proofing Class Incremental Learning</h2><p><strong>Authors:Quentin Jodelet, Xin Liu, Yin Jun Phua, Tsuyoshi Murata</strong></p><p>Exemplar-Free Class Incremental Learning is a highly challenging setting where replay memory is unavailable. Methods relying on frozen feature extractors have drawn attention recently in this setting due to their impressive performances and lower computational costs. However, those methods are highly dependent on the data used to train the feature extractor and may struggle when an insufficient amount of classes are available during the first incremental step. To overcome this limitation, we propose to use a pre-trained text-to-image diffusion model in order to generate synthetic images of future classes and use them to train the feature extractor. Experiments on the standard benchmarks CIFAR100 and ImageNet-Subset demonstrate that our proposed method can be used to improve state-of-the-art methods for exemplar-free class incremental learning, especially in the most difficult settings where the first incremental step only contains few classes. Moreover, we show that using synthetic samples of future classes achieves higher performance than using real data from different classes, paving the way for better and less costly pre-training methods for incremental learning. </p><p><a href="http://arxiv.org/abs/2404.03200v1">PDF</a> </p><p><strong>Summary</strong><br>åˆ©ç”¨é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆæœªæ¥ç±»åˆ«çš„åˆæˆå›¾åƒï¼Œå¯æå‡æ— æ ·æœ¬ç±»å¢é‡å­¦ä¹ çš„æ€§èƒ½ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æ— æ ·æœ¬ç±»å¢é‡å­¦ä¹ ä¸­ï¼ŒåŸºäºå†»ç»“ç‰¹å¾æå–å™¨çš„æ¨¡å‹å› å…¶å‡ºè‰²æ€§èƒ½å’Œä½è®¡ç®—æˆæœ¬è€Œå¤‡å—å…³æ³¨ã€‚</li><li>ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹é«˜åº¦ä¾èµ–äºè®­ç»ƒç‰¹å¾æå–å™¨çš„æ•°æ®ï¼Œåœ¨é¦–ä¸ªå¢é‡æ­¥éª¤ä¸­å¯ç”¨ç±»åˆ«æ•°é‡ä¸è¶³æ—¶å¯èƒ½å­˜åœ¨å›°éš¾ã€‚</li><li>ç ”ç©¶è€…æå‡ºä½¿ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆæœªæ¥ç±»åˆ«çš„åˆæˆå›¾åƒï¼Œå¹¶åˆ©ç”¨è¿™äº›å›¾åƒè®­ç»ƒç‰¹å¾æå–å™¨ã€‚</li><li>åœ¨ CIFAR100 å’Œ ImageNet-Subset æ ‡å‡†åŸºå‡†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•å¯ç”¨æ¥æ”¹è¿›æ— æ ·æœ¬ç±»å¢é‡å­¦ä¹ çš„æœ€æ–°æ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨é¦–ä¸ªå¢é‡æ­¥éª¤ä»…åŒ…å«å°‘é‡ç±»åˆ«çš„æœ€å›°éš¾è®¾ç½®ä¸­ã€‚</li><li>ä½¿ç”¨æœªæ¥ç±»åˆ«çš„åˆæˆæ ·æœ¬æ¯”ä½¿ç”¨æ¥è‡ªä¸åŒç±»åˆ«çš„çœŸå®æ•°æ®èƒ½å–å¾—æ›´é«˜çš„æ€§èƒ½ï¼Œä¸ºå¢é‡å­¦ä¹ æä¾›æ›´ä½³ã€æ›´ä½æˆæœ¬çš„é¢„è®­ç»ƒæ–¹æ³•ã€‚</li><li>æœªæ¥ç ”ç©¶æ–¹å‘åŒ…æ‹¬æ¢ç´¢å…¶ä»–åˆæˆæ•°æ®ç”ŸæˆæŠ€æœ¯ä»¥åŠåˆ©ç”¨åˆæˆæ•°æ®è¿›è¡Œå¾®è°ƒçš„æœ‰æ•ˆæ–¹æ³•ã€‚</li><li>æ­¤å¤–ï¼Œè¿˜å¯ä»¥è€ƒè™‘ç ”ç©¶åœ¨å®æ—¶åœºæ™¯ä¸­ç”Ÿæˆåˆæˆæ•°æ®çš„å¯èƒ½æ€§ï¼Œä»¥ä¾¿åœ¨éƒ¨ç½²æœŸé—´æŒç»­æ‰§è¡Œå¢é‡å­¦ä¹ ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šæœªæ¥è¯æ˜ç±»å¢é‡å­¦ä¹ </li><li>ä½œè€…ï¼šQuentin Jodelet, Xin Liu, Yin Jun Phua, Tsuyoshi Murata</li><li>éš¶å±ï¼šä¸œäº¬å·¥ä¸šå¤§å­¦è®¡ç®—æœºç§‘å­¦ç³»</li><li>å…³é”®è¯ï¼šç±»å¢é‡å­¦ä¹ ã€æŒç»­å­¦ä¹ ã€å›¾åƒåˆ†ç±»ã€å›¾åƒç”Ÿæˆ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.03200</li><li><p>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šç±»å¢é‡å­¦ä¹ æ˜¯æ·±åº¦å­¦ä¹ çš„ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é¢†åŸŸï¼Œå®ƒè¦æ±‚æ¨¡å‹åœ¨æ²¡æœ‰è®¿é—®å…ˆå‰å­¦ä¹ ç±»çš„æƒ…å†µä¸‹ï¼Œä¸æ–­å­¦ä¹ æ–°ç±»ã€‚æ— ç¤ºä¾‹ç±»å¢é‡å­¦ä¹  (EF-CIL) æ˜¯ç±»å¢é‡å­¦ä¹ ä¸­æ›´å…·æŒ‘æˆ˜æ€§çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä¸å…è®¸ä½¿ç”¨å›æ”¾å†…å­˜ã€‚(2) è¿‡å»çš„æ–¹æ³•ï¼šåŸºäºå†»ç»“ç‰¹å¾æå–å™¨çš„ EF-CIL æ–¹æ³•å› å…¶ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½å’Œè¾ƒä½çš„è®¡ç®—æˆæœ¬è€Œå—åˆ°å…³æ³¨ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é«˜åº¦ä¾èµ–äºç”¨äºè®­ç»ƒç‰¹å¾æå–å™¨çš„åˆå§‹æ•°æ®ï¼Œå¹¶ä¸”å½“ç¬¬ä¸€ä¸ªå¢é‡æ­¥éª¤ä¸­å¯ç”¨çš„ç±»æ•°é‡ä¸è¶³æ—¶ï¼Œå¯èƒ½ä¼šé‡åˆ°å›°éš¾ã€‚(3) è®ºæ–‡æ–¹æ³•ï¼šä¸ºäº†å…‹æœè¿™ä¸€é™åˆ¶ï¼Œæœ¬æ–‡æå‡ºä½¿ç”¨é¢„å…ˆè®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆæœªæ¥ç±»åˆ«çš„åˆæˆå›¾åƒï¼Œå¹¶ä½¿ç”¨è¿™äº›å›¾åƒæ¥è®­ç»ƒç‰¹å¾æå–å™¨ã€‚(4) å®éªŒç»“æœï¼šåœ¨ CIFAR100 å’Œ ImageNet-Subset ç­‰æ ‡å‡†åŸºå‡†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•å¯ä»¥ç”¨æ¥æé«˜æ— ç¤ºä¾‹ç±»å¢é‡å­¦ä¹ çš„æœ€æ–°æ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨ç¬¬ä¸€ä¸ªå¢é‡æ­¥éª¤ä»…åŒ…å«å°‘é‡ç±»åˆ«çš„æœ€å›°éš¾æƒ…å†µä¸‹ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜è¡¨æ˜ï¼Œä½¿ç”¨æœªæ¥ç±»åˆ«çš„åˆæˆæ ·æœ¬æ¯”ä½¿ç”¨æ¥è‡ªä¸åŒç±»åˆ«çš„çœŸå®æ•°æ®èƒ½è·å¾—æ›´é«˜çš„æ€§èƒ½ï¼Œä¸ºå¢é‡å­¦ä¹ çš„æ›´å¥½ä¸”æˆæœ¬æ›´ä½çš„é¢„è®­ç»ƒæ–¹æ³•é“ºå¹³äº†é“è·¯ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1): ä½¿ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œç”Ÿæˆæœªæ¥ç±»åˆ«çš„åˆæˆå›¾åƒï¼Œå¹¶ä½¿ç”¨è¿™äº›å›¾åƒè®­ç»ƒç‰¹å¾æå–å™¨ã€‚(2): åœ¨æ— ç¤ºä¾‹ç±»å¢é‡å­¦ä¹ ä¸­ï¼Œä½¿ç”¨åˆæˆå›¾åƒå¯¹ç‰¹å¾æå–å™¨è¿›è¡Œé¢„è®­ç»ƒï¼Œå¯ä»¥æé«˜æ¨¡å‹çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨ç¬¬ä¸€ä¸ªå¢é‡æ­¥éª¤ä»…åŒ…å«å°‘é‡ç±»åˆ«çš„æœ€å›°éš¾æƒ…å†µä¸‹ã€‚(3): ä½¿ç”¨æœªæ¥ç±»åˆ«çš„åˆæˆæ ·æœ¬æ¯”ä½¿ç”¨æ¥è‡ªä¸åŒç±»åˆ«çš„çœŸå®æ•°æ®èƒ½è·å¾—æ›´é«˜çš„æ€§èƒ½ï¼Œä¸ºå¢é‡å­¦ä¹ çš„æ›´å¥½ä¸”æˆæœ¬æ›´ä½çš„é¢„è®­ç»ƒæ–¹æ³•é“ºå¹³äº†é“è·¯ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ— ç¤ºä¾‹ç±»å¢é‡å­¦ä¹ æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å¤§å‹é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆæœªæ¥ç±»åˆ«çš„å›¾åƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æ˜¾è‘—æé«˜ç°æœ‰æ–¹æ³•çš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶åªä¿®æ”¹äº†åˆå§‹æ­¥éª¤ã€‚æˆ‘ä»¬å‘ç°ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ¯”ä¾èµ–äºçœŸå®æ•´ç†æ•°æ®é›†çš„ä¼ ç»Ÿæ–¹æ³•éœ€è¦æ›´å°‘çš„æ•°æ®ã€‚è™½ç„¶æˆ‘ä»¬ç›®å‰çš„è¿™é¡¹ç ”ç©¶ä»…é™äºåœ¨ç¬¬ä¸€ä¸ªå¢é‡æ­¥éª¤ä¸­ä»å¤´å¼€å§‹è®­ç»ƒçš„ç‰¹å¾æå–å™¨ï¼Œä½†åœ¨æœªæ¥çš„å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°†è¿›ä¸€æ­¥ç ”ç©¶å¦‚ä½•ä½¿ç”¨æœªæ¥ç±»åˆ«çš„åˆæˆå›¾åƒæ¥é€‚åº”é€šç”¨çš„é¢„è®­ç»ƒåŸºç¡€ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šä½¿ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆæœªæ¥ç±»åˆ«çš„åˆæˆå›¾åƒï¼Œå¹¶ä½¿ç”¨è¿™äº›å›¾åƒæ¥è®­ç»ƒç‰¹å¾æå–å™¨ã€‚æ€§èƒ½ï¼šåœ¨æ— ç¤ºä¾‹ç±»å¢é‡å­¦ä¹ ä¸­ï¼Œä½¿ç”¨åˆæˆå›¾åƒå¯¹ç‰¹å¾æå–å™¨è¿›è¡Œé¢„è®­ç»ƒï¼Œå¯ä»¥æé«˜æ¨¡å‹çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨ç¬¬ä¸€ä¸ªå¢é‡æ­¥éª¤ä»…åŒ…å«å°‘é‡ç±»åˆ«çš„æœ€å›°éš¾æƒ…å†µä¸‹ã€‚å·¥ä½œé‡ï¼šä½¿ç”¨æœªæ¥ç±»åˆ«çš„åˆæˆæ ·æœ¬æ¯”ä½¿ç”¨æ¥è‡ªä¸åŒç±»åˆ«çš„çœŸå®æ•°æ®èƒ½è·å¾—æ›´é«˜çš„æ€§èƒ½ï¼Œä¸ºå¢é‡å­¦ä¹ çš„æ›´å¥½ä¸”æˆæœ¬æ›´ä½çš„é¢„è®­ç»ƒæ–¹æ³•é“ºå¹³äº†é“è·¯ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-5adb96d9627531125646ce0ee2191406.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-8e81c8158234e67aa146c6f8d8de1ebe.jpg" align="middle"><img src="https://pica.zhimg.com/v2-5c788dcee57eb62445a58074bf15bf51.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3113b9fb60c9b18bc0b976dc329e64c8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e740fe0c99bec8a3654bee8ea504eafa.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-305f4f5b7b6fe9b6ad21c95c6b3351a4.jpg" align="middle"></details><h2 id="HandDiff-3D-Hand-Pose-Estimation-with-Diffusion-on-Image-Point-Cloud"><a href="#HandDiff-3D-Hand-Pose-Estimation-with-Diffusion-on-Image-Point-Cloud" class="headerlink" title="HandDiff: 3D Hand Pose Estimation with Diffusion on Image-Point Cloud"></a>HandDiff: 3D Hand Pose Estimation with Diffusion on Image-Point Cloud</h2><p><strong>Authors:Wencan Cheng, Hao Tang, Luc Van Gool, Jong Hwan Ko</strong></p><p>Extracting keypoint locations from input hand frames, known as 3D hand pose estimation, is a critical task in various human-computer interaction applications. Essentially, the 3D hand pose estimation can be regarded as a 3D point subset generative problem conditioned on input frames. Thanks to the recent significant progress on diffusion-based generative models, hand pose estimation can also benefit from the diffusion model to estimate keypoint locations with high quality. However, directly deploying the existing diffusion models to solve hand pose estimation is non-trivial, since they cannot achieve the complex permutation mapping and precise localization. Based on this motivation, this paper proposes HandDiff, a diffusion-based hand pose estimation model that iteratively denoises accurate hand pose conditioned on hand-shaped image-point clouds. In order to recover keypoint permutation and accurate location, we further introduce joint-wise condition and local detail condition. Experimental results demonstrate that the proposed HandDiff significantly outperforms the existing approaches on four challenging hand pose benchmark datasets. Codes and pre-trained models are publicly available at <a href="https://github.com/cwc1260/HandDiff">https://github.com/cwc1260/HandDiff</a>. </p><p><a href="http://arxiv.org/abs/2404.03159v1">PDF</a> Accepted as a conference paper to the Conference on Computer Vision   and Pattern Recognition (2024)</p><p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹ç»è¿‡æ”¹è¿›ï¼Œæå‡º HandDiff æ¨¡å‹ç”¨äºæ‰‹éƒ¨å§¿åŠ¿ä¼°è®¡ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿå¤„ç†å¤æ‚æ’åˆ—æ˜ å°„å’Œç²¾ç¡®å®šä½ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æ‰‹éƒ¨å§¿åŠ¿ä¼°è®¡ä»»åŠ¡å¯è§†ä¸º 3D ç‚¹å­é›†ç”Ÿæˆé—®é¢˜ï¼ŒåŸºäºè¾“å…¥å¸§ç”Ÿæˆã€‚</li><li>æ‰©æ•£æ¨¡å‹åœ¨æ‰‹éƒ¨å§¿åŠ¿ä¼°è®¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ç›´æ¥ä½¿ç”¨å­˜åœ¨å±€é™æ€§ã€‚</li><li>HandDiff æ¨¡å‹åŸºäºæ‰©æ•£æ¨¡å‹ï¼Œæ¡ä»¶åŒ–æ‰‹éƒ¨å½¢çŠ¶å›¾åƒç‚¹äº‘ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ¢å¤å…³é”®ç‚¹æ’åˆ—å’Œå‡†ç¡®ä½ç½®ã€‚</li><li>å¼•å…¥äº†å…³èŠ‚æ¡ä»¶å’Œå±€éƒ¨ç»†èŠ‚æ¡ä»¶ï¼Œä»¥æ”¹å–„å…³é”®ç‚¹å®šä½ã€‚</li><li>å®éªŒç»“æœè¡¨æ˜ HandDiff åœ¨å››ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ‰‹éƒ¨å§¿åŠ¿åŸºå‡†æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li><li>HandDiff æ¨¡å‹çš„ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å·²å¼€æºã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šåŸºäºå›¾åƒç‚¹äº‘æ‰©æ•£çš„ 3D æ‰‹éƒ¨å§¿åŠ¿ä¼°è®¡</li><li>ä½œè€…ï¼šWencan Cheng, Hao Tang, Luc Van Gool, JongHwan Ko</li><li>å•ä½ï¼šéŸ©å›½æˆå‡é¦†å¤§å­¦äººå·¥æ™ºèƒ½ç³»</li><li>å…³é”®è¯ï¼š3D æ‰‹éƒ¨å§¿åŠ¿ä¼°è®¡ï¼Œæ‰©æ•£æ¨¡å‹ï¼Œæ‰‹éƒ¨å½¢çŠ¶å›¾åƒç‚¹äº‘</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.03159   Github ä»£ç é“¾æ¥ï¼šhttps://github.com/cwc1260/HandDiff</li><li>æ‘˜è¦ï¼š   (1) ç ”ç©¶èƒŒæ™¯ï¼š3D æ‰‹éƒ¨å§¿åŠ¿ä¼°è®¡æ˜¯äººæœºäº¤äº’åº”ç”¨ä¸­çš„å…³é”®ä»»åŠ¡ï¼Œå¯ä»¥çœ‹ä½œæ˜¯åœ¨è¾“å…¥å¸§æ¡ä»¶ä¸‹ç”Ÿæˆ 3D ç‚¹å­é›†çš„é—®é¢˜ã€‚æ‰©æ•£æ¨¡å‹åœ¨ 3D ç”Ÿæˆåº”ç”¨ä¸­è¡¨ç°å‡ºä¼˜å¼‚æ€§ï¼Œå¯ä»¥ç”¨äºä¼°è®¡é«˜è´¨é‡å…³é”®ç‚¹ä½ç½®ã€‚   (2) è¿‡å»æ–¹æ³•å’Œé—®é¢˜ï¼šç°æœ‰æ‰©æ•£æ¨¡å‹æ— æ³•å®ç°å¤æ‚çš„æ’åˆ—æ˜ å°„å’Œç²¾ç¡®å®šä½ã€‚   (3) ç ”ç©¶æ–¹æ³•ï¼šæå‡º HandDiff æ¨¡å‹ï¼Œé€šè¿‡åœ¨æ‰‹éƒ¨å½¢çŠ¶å›¾åƒç‚¹äº‘æ¡ä»¶ä¸‹å¯¹æ‰©æ•£å™ªå£°è¿›è¡Œè¿­ä»£å»å™ªï¼Œä¼°è®¡å‡†ç¡®çš„æ‰‹éƒ¨å§¿åŠ¿ã€‚å¼•å…¥å…³èŠ‚æ¡ä»¶å’Œå±€éƒ¨ç»†èŠ‚æ¡ä»¶ï¼Œä»¥æ¢å¤å…³é”®ç‚¹æ’åˆ—å’Œå‡†ç¡®ä½ç½®ã€‚   (4) æ€§èƒ½å’Œæ•ˆæœï¼šHandDiff åœ¨å››ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ‰‹éƒ¨å§¿åŠ¿åŸºå‡†æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¯æ˜äº†å…¶åœ¨å¤„ç†é®æŒ¡ç­‰ä¸é€‚å®šä¸ç¡®å®šæ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</li></ol><p><strong>Methods:</strong></p><p>(1): <strong>HandDiff</strong>æ¨¡å‹é€šè¿‡åœ¨æ‰‹éƒ¨å½¢çŠ¶å›¾åƒç‚¹äº‘æ¡ä»¶ä¸‹å¯¹æ‰©æ•£å™ªå£°è¿›è¡Œè¿­ä»£å»å™ªï¼Œä¼°è®¡å‡†ç¡®çš„æ‰‹éƒ¨å§¿åŠ¿ã€‚</p><p>(2): å¼•å…¥<strong>å…³èŠ‚æ¡ä»¶</strong>ï¼Œä»¥æ¢å¤å…³é”®ç‚¹æ’åˆ—ã€‚</p><p>(3): å¼•å…¥<strong>å±€éƒ¨ç»†èŠ‚æ¡ä»¶</strong>ï¼Œä»¥æ¢å¤å…³é”®ç‚¹å‡†ç¡®ä½ç½®ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬å·¥ä½œé€šè¿‡å¼•å…¥å…³èŠ‚æ¡ä»¶å’Œå±€éƒ¨ç»†èŠ‚æ¡ä»¶ï¼Œæå‡ºäº† HandDiff æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡è¿­ä»£å»å™ªæ‰‹éƒ¨å½¢çŠ¶å›¾åƒç‚¹äº‘æ¡ä»¶ä¸‹çš„æ‰©æ•£å™ªå£°æ¥ä¼°è®¡å‡†ç¡®çš„æ‰‹éƒ¨å§¿åŠ¿ï¼Œåœ¨å››ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ‰‹éƒ¨å§¿åŠ¿åŸºå‡†æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¯æ˜äº†å…¶åœ¨å¤„ç†é®æŒ¡ç­‰ä¸é€‚å®šä¸ç¡®å®šæ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼šæå‡º HandDiff æ¨¡å‹ï¼Œé€šè¿‡åœ¨æ‰‹éƒ¨å½¢çŠ¶å›¾åƒç‚¹äº‘æ¡ä»¶ä¸‹å¯¹æ‰©æ•£å™ªå£°è¿›è¡Œè¿­ä»£å»å™ªï¼Œä¼°è®¡å‡†ç¡®çš„æ‰‹éƒ¨å§¿åŠ¿ï¼›å¼•å…¥å…³èŠ‚æ¡ä»¶ï¼Œä»¥æ¢å¤å…³é”®ç‚¹æ’åˆ—ï¼›å¼•å…¥å±€éƒ¨ç»†èŠ‚æ¡ä»¶ï¼Œä»¥æ¢å¤å…³é”®ç‚¹å‡†ç¡®ä½ç½®ã€‚æ€§èƒ½ï¼šåœ¨å››ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ‰‹éƒ¨å§¿åŠ¿åŸºå‡†æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å·¥ä½œé‡ï¼šéœ€è¦æ‰‹éƒ¨å½¢çŠ¶å›¾åƒç‚¹äº‘æ¡ä»¶ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-9127e6b88a37dae1433f9ba58b2eb0d6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bbe017f10c09349ebc2fc158ed02f568.jpg" align="middle"><img src="https://picx.zhimg.com/v2-87a189e71ddf1b5c27db9470a6b9ae3a.jpg" align="middle"></details><h2 id="DreamWalk-Style-Space-Exploration-using-Diffusion-Guidance"><a href="#DreamWalk-Style-Space-Exploration-using-Diffusion-Guidance" class="headerlink" title="DreamWalk: Style Space Exploration using Diffusion Guidance"></a>DreamWalk: Style Space Exploration using Diffusion Guidance</h2><p><strong>Authors:Michelle Shu, Charles Herrmann, Richard Strong Bowen, Forrester Cole, Ramin Zabih</strong></p><p>Text-conditioned diffusion models can generate impressive images, but fall short when it comes to fine-grained control. Unlike direct-editing tools like Photoshop, text conditioned models require the artist to perform â€œprompt engineering,â€ constructing special text sentences to control the style or amount of a particular subject present in the output image. Our goal is to provide fine-grained control over the style and substance specified by the prompt, for example to adjust the intensity of styles in different regions of the image (Figure 1). Our approach is to decompose the text prompt into conceptual elements, and apply a separate guidance term for each element in a single diffusion process. We introduce guidance scale functions to control when in the diffusion process and \emph{where} in the image to intervene. Since the method is based solely on adjusting diffusion guidance, it does not require fine-tuning or manipulating the internal layers of the diffusion modelâ€™s neural network, and can be used in conjunction with LoRA- or DreamBooth-trained models (Figure2). Project page: <a href="https://mshu1.github.io/dreamwalk.github.io/">https://mshu1.github.io/dreamwalk.github.io/</a> </p><p><a href="http://arxiv.org/abs/2404.03145v1">PDF</a> </p><p><strong>Summary</strong><br>æ–‡å­—æ¡ä»¶æ‰©æ•£æ¨¡å‹å¯ç”Ÿæˆä»¤äººå°è±¡æ·±åˆ»çš„å›¾åƒï¼Œä½†åœ¨ç²¾ç»†æ§åˆ¶æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æ–‡æœ¬æ¡ä»¶æ¨¡å‹éœ€è¦è‰ºæœ¯å®¶è¿›è¡Œâ€œæç¤ºå·¥ç¨‹â€ï¼Œä»¥æ„é€ ç‰¹æ®Šçš„æ–‡æœ¬å¥å­æ¥æ§åˆ¶è¾“å‡ºå›¾åƒä¸­ç‰¹å®šä¸»é¢˜çš„æ ·å¼æˆ–æ•°é‡ã€‚</li><li>åˆ†è§£æ–‡æœ¬æç¤ºä¸ºæ¦‚å¿µå…ƒç´ ï¼Œå¹¶åœ¨å•ä¸ªæ‰©æ•£è¿‡ç¨‹ä¸­å¯¹æ¯ä¸ªå…ƒç´ åº”ç”¨å•ç‹¬çš„æŒ‡å¯¼é¡¹ã€‚</li><li>å¼•å…¥æŒ‡å¯¼æ¯”ä¾‹å‡½æ•°æ¥æ§åˆ¶åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­çš„ä½•æ—¶ä½•å¤„è¿›è¡Œå¹²é¢„ã€‚</li><li>è¯¥æ–¹æ³•åªè°ƒæ•´æ‰©æ•£æŒ‡å¯¼ï¼Œä¸éœ€è¦å¾®è°ƒæˆ–æ“ä½œæ‰©æ•£æ¨¡å‹ç¥ç»ç½‘ç»œçš„å†…éƒ¨å±‚ï¼Œå¹¶ä¸”å¯ä»¥ä¸ LoRA æˆ– DreamBooth è®­ç»ƒçš„æ¨¡å‹ç»“åˆä½¿ç”¨ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šDreamWalkï¼šä½¿ç”¨æ‰©æ•£å¼•å¯¼çš„é£æ ¼ç©ºé—´æ¢ç´¢</li><li>ä½œè€…ï¼šMichelle Shu<em>ã€Charles Herrmann</em>ã€Richard S. Bowenã€Forrester Coleã€Ramin Zabih</li><li>éš¶å±ï¼šåº·å¥ˆå°”å¤§å­¦</li><li>å…³é”®è¯ï¼šæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€æ‰©æ•£æ¨¡å‹ã€é£æ ¼æ§åˆ¶ã€DreamWalk</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.03145    Github ä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹åœ¨ç”Ÿæˆå›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›æ­¥ï¼Œä½†ç¼ºä¹å¯¹å›¾åƒé£æ ¼å’Œå†…å®¹çš„ç²¾ç»†æ§åˆ¶ã€‚</li></ol><p>(2) è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–æç¤ºå·¥ç¨‹æˆ–å¾®è°ƒæ‰©æ•£æ¨¡å‹ï¼Œè¿™äº›æ–¹æ³•å­˜åœ¨æ§åˆ¶ä¸çµæ´»ã€æ”¹å˜æç¤ºä¼šå¯¼è‡´å›¾åƒæ•´ä½“å˜åŒ–ç­‰é—®é¢˜ã€‚</p><p>(3) æœ¬æ–‡æ–¹æ³•ï¼šDreamWalk æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£å¼•å¯¼çš„é£æ ¼ç©ºé—´æ¢ç´¢æ–¹æ³•ã€‚å®ƒå°†æ–‡æœ¬æç¤ºåˆ†è§£ä¸ºæ¦‚å¿µå…ƒç´ ï¼Œå¹¶ä¸ºæ¯ä¸ªå…ƒç´ åº”ç”¨å•ç‹¬çš„å¼•å¯¼é¡¹ã€‚é€šè¿‡å¼•å…¥å¼•å¯¼å°ºåº¦å‡½æ•°ï¼Œç”¨æˆ·å¯ä»¥æ§åˆ¶å¼•å¯¼é¡¹åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­çš„æ—¶é—´å’Œç©ºé—´åº”ç”¨ã€‚</p><p>(4) æ€§èƒ½åŠæ•ˆæœï¼šDreamWalk åœ¨é£æ ¼ç©ºé—´æ¢ç´¢ä»»åŠ¡ä¸Šå–å¾—äº†å‡ºè‰²çš„æ€§èƒ½ã€‚å®ƒå…è®¸ç”¨æˆ·ä»¥ç²¾ç»†çš„æ–¹å¼æ§åˆ¶å›¾åƒä¸­çš„ä¸åŒåŒºåŸŸçš„é£æ ¼å¼ºåº¦ï¼ŒåŒæ—¶ä¿æŒå›¾åƒçš„æ•´ä½“ç»“æ„å’Œå†…å®¹ã€‚</p><ol><li><p>æ–¹æ³•ï¼š(1) å¤šé‡å¼•å¯¼å…¬å¼ï¼šæå‡ºå¼•å¯¼å°ºåº¦å‡½æ•°ï¼Œç”¨äºæ§åˆ¶å¼•å¯¼é¡¹åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­çš„æ—¶é—´å’Œç©ºé—´åº”ç”¨ï¼›(2) ä»æ–‡æœ¬æç¤ºåˆ›å»ºå¤šé‡å¼•å¯¼é¡¹ï¼šå°†æç¤ºåˆ†è§£ä¸ºåŸºæœ¬æç¤ºå’Œé£æ ¼ç»„ä»¶ï¼Œä¸ºæ¯ä¸ªç»„ä»¶åº”ç”¨å•ç‹¬çš„å¼•å¯¼é¡¹ï¼›(3) å¯æ§æ­¥è¡Œï¼šé€šè¿‡å¼•å¯¼å°ºåº¦å‡½æ•°ï¼Œç”¨æˆ·å¯ä»¥æ§åˆ¶ä¸åŒæ¡ä»¶çš„å¼•å¯¼é¡¹åœ¨å›¾åƒä¸­çš„ä½ç½®ã€å¼ºåº¦å’Œç±»å‹ï¼›(4) æ—¶é—´æ­¥é•¿ä¾èµ–æ€§ï¼šé€šè¿‡è§‚å¯Ÿå¼•å¯¼é¡¹çš„èŒƒæ•°ï¼Œå‘ç°å›¾åƒå½¢æˆæ˜¯ä»ç²—åˆ°ç»†çš„è¿‡ç¨‹ï¼Œæå‡ºåœ¨æ—©æœŸå¼•å¯¼é˜¶æ®µä¸»è¦å…³æ³¨åŸºæœ¬æç¤ºï¼ŒåæœŸå¼•å¯¼é˜¶æ®µä¸»è¦å…³æ³¨é£æ ¼æç¤ºçš„è§£å†³æ–¹æ¡ˆã€‚</p></li><li><p>ç»“è®ºï¼š(1): DreamWalk æ˜¯ä¸€ç§é€šç”¨çš„å¼•å¯¼å…¬å¼ï¼Œä¸“é—¨è®¾è®¡ç”¨äºä¸ªæ€§åŒ–æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€‚è¿™ç§æ–¹æ³•å…è®¸å¯¹åº”ç”¨çš„é£æ ¼é‡æˆ–å¯¹ DB æ ‡è®°æˆ– LORA çš„éµå®ˆç¨‹åº¦è¿›è¡Œç²¾ç»†æ§åˆ¶ã€‚æˆ‘ä»¬å·²ç»å‡­ç»éªŒè¯æ˜äº†è¿™ç§æ–¹æ³•åœ¨å‡ ç§ä»»åŠ¡ä¸Šçš„æ•ˆç‡ï¼ŒåŒ…æ‹¬é£æ ¼æ’å€¼ã€DB é‡‡æ ·ã€æ›´æ”¹æè´¨ä»¥åŠç²¾ç»†åœ°æ“çºµç”Ÿæˆå›¾åƒçš„çº¹ç†å’Œå¸ƒå±€ã€‚(2): åˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£å¼•å¯¼çš„é£æ ¼ç©ºé—´æ¢ç´¢æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥å°†æ–‡æœ¬æç¤ºåˆ†è§£ä¸ºæ¦‚å¿µå…ƒç´ ï¼Œå¹¶ä¸ºæ¯ä¸ªå…ƒç´ åº”ç”¨å•ç‹¬çš„å¼•å¯¼é¡¹ï¼Œé€šè¿‡å¼•å¯¼å°ºåº¦å‡½æ•°ï¼Œç”¨æˆ·å¯ä»¥æ§åˆ¶å¼•å¯¼é¡¹åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­çš„æ—¶é—´å’Œç©ºé—´åº”ç”¨ã€‚æ€§èƒ½ï¼šåœ¨é£æ ¼ç©ºé—´æ¢ç´¢ä»»åŠ¡ä¸Šå–å¾—äº†å‡ºè‰²çš„æ€§èƒ½ï¼Œå®ƒå…è®¸ç”¨æˆ·ä»¥ç²¾ç»†çš„æ–¹å¼æ§åˆ¶å›¾åƒä¸­ä¸åŒåŒºåŸŸçš„é£æ ¼å¼ºåº¦ï¼ŒåŒæ—¶ä¿æŒå›¾åƒçš„æ•´ä½“ç»“æ„å’Œå†…å®¹ã€‚å·¥ä½œé‡ï¼šæœ¬æ–‡æ–¹æ³•éœ€è¦å°†æ–‡æœ¬æç¤ºåˆ†è§£ä¸ºæ¦‚å¿µå…ƒç´ ï¼Œå¹¶ä¸ºæ¯ä¸ªå…ƒç´ åº”ç”¨å•ç‹¬çš„å¼•å¯¼é¡¹ï¼Œè¿™å¯èƒ½éœ€è¦å¤§é‡çš„å·¥ä½œé‡ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-1c6779fc9e6a3c6a524e7c693cfad563.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ded6f26ee5eec5a3db8b0e7f7298e3cd.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a904c00cd643583927c16348c6d0f361.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7ce89f92e4ccf4d953fa7144543afe17.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f30c6d8b1b699e999092073e6d3d8769.jpg" align="middle"></details><h2 id="Diverse-and-Tailored-Image-Generation-for-Zero-shot-Multi-label-Classification"><a href="#Diverse-and-Tailored-Image-Generation-for-Zero-shot-Multi-label-Classification" class="headerlink" title="Diverse and Tailored Image Generation for Zero-shot Multi-label   Classification"></a>Diverse and Tailored Image Generation for Zero-shot Multi-label   Classification</h2><p><strong>Authors:Kaixin Zhang, Zhixiang Yuan, Tao Huang</strong></p><p>Recently, zero-shot multi-label classification has garnered considerable attention for its capacity to operate predictions on unseen labels without human annotations. Nevertheless, prevailing approaches often use seen classes as imperfect proxies for unseen ones, resulting in suboptimal performance. Drawing inspiration from the success of text-to-image generation models in producing realistic images, we propose an innovative solution: generating synthetic data to construct a training set explicitly tailored for proxyless training on unseen labels. Our approach introduces a novel image generation framework that produces multi-label synthetic images of unseen classes for classifier training. To enhance diversity in the generated images, we leverage a pre-trained large language model to generate diverse prompts. Employing a pre-trained multi-modal CLIP model as a discriminator, we assess whether the generated images accurately represent the target classes. This enables automatic filtering of inaccurately generated images, preserving classifier accuracy. To refine text prompts for more precise and effective multi-label object generation, we introduce a CLIP score-based discriminative loss to fine-tune the text encoder in the diffusion model. Additionally, to enhance visual features on the target task while maintaining the generalization of original features and mitigating catastrophic forgetting resulting from fine-tuning the entire visual encoder, we propose a feature fusion module inspired by transformer attention mechanisms. This module aids in capturing global dependencies between multiple objects more effectively. Extensive experimental results validate the effectiveness of our approach, demonstrating significant improvements over state-of-the-art methods. </p><p><a href="http://arxiv.org/abs/2404.03144v1">PDF</a> </p><p><strong>Summary</strong><br>ç”Ÿæˆåˆæˆæ•°æ®ï¼Œç”¨äºåœ¨æœªè§æ ‡ç­¾ä¸Šè¿›è¡Œä»£ç†è®­ç»ƒï¼Œä»è€Œæå‡æ— æ ‡æ³¨å¤šæ ‡ç­¾åˆ†ç±»æ€§èƒ½ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ä½¿ç”¨åˆæˆæ•°æ®è¿›è¡Œä»£ç†è®­ç»ƒï¼Œæ— éœ€äººå·¥æ ‡æ³¨æœªè§æ ‡ç­¾ã€‚</li><li>æå‡ºå›¾åƒç”Ÿæˆæ¡†æ¶ï¼Œç”Ÿæˆæœªè§ç±»åˆ«çš„å¤šæ ‡ç­¾åˆæˆå›¾åƒã€‚</li><li>åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆå¤šæ ·åŒ–çš„æç¤ºï¼Œæé«˜å›¾åƒå¤šæ ·æ€§ã€‚</li><li>ä½¿ç”¨ CLIP æ¨¡å‹è¯„ä¼°ç”Ÿæˆå›¾åƒçš„å‡†ç¡®æ€§ï¼Œè¿‡æ»¤ä¸å‡†ç¡®å›¾åƒã€‚</li><li>å¼•å…¥ CLIP å¾—åˆ†é‰´åˆ«æŸå¤±ï¼Œä¼˜åŒ–æ–‡æœ¬ç¼–ç å™¨ä»¥ç”Ÿæˆå‡†ç¡®çš„å¤šæ ‡ç­¾å¯¹è±¡ã€‚</li><li>æå‡ºç‰¹å¾èåˆæ¨¡å—ï¼Œå¢å¼ºç›®æ ‡ä»»åŠ¡çš„å¯è§†åŒ–ç‰¹å¾ï¼Œç¼“è§£å› å¾®è°ƒæ•´ä¸ªè§†è§‰ç¼–ç å™¨è€Œå¯¼è‡´çš„ç¾éš¾æ€§é—å¿˜ã€‚</li><li>å®éªŒç»“æœè¯æ˜äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šåŸºäºæ‰©æ•£æ¨¡å‹çš„å¤šç±»åˆ«é›¶æ ·æœ¬å›¾åƒç”Ÿæˆä¸ä¸ªæ€§åŒ–</li><li>ä½œè€…ï¼šKaixin Zhang, Zhixiang Yuan, Tao Huang</li><li>å•ä½ï¼šå®‰å¾½ç†å·¥å¤§å­¦è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯å­¦é™¢</li><li>å…³é”®è¯ï¼šé›¶æ ·æœ¬å¤šæ ‡ç­¾å­¦ä¹ ã€æ·±åº¦ç”Ÿæˆæ¨¡å‹ã€æ‰©æ•£æ¨¡å‹ã€åˆæˆæ•°æ®</li><li>é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.03144</li><li>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šé›¶æ ·æœ¬å¤šæ ‡ç­¾åˆ†ç±»ï¼ˆZS-MLCï¼‰æ—¨åœ¨å¤„ç†æœªè§æ ‡ç­¾çš„é¢„æµ‹ä»»åŠ¡ï¼Œä½†ç°æœ‰æ–¹æ³•é€šå¸¸ä½¿ç”¨å·²è§ç±»ä½œä¸ºæœªè§ç±»çš„ä»£ç†ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚(2) è¿‡å»æ–¹æ³•ï¼šç»å…¸æ–¹æ³•ä½¿ç”¨æ–‡æœ¬ç‰¹å¾æ¥åŒºåˆ†å›¾åƒä¸­æ¯ä¸ªæœªè§ç±»çš„å­˜åœ¨ï¼Œä½†å¿½ç•¥äº†å›¾åƒ-æ–‡æœ¬å¯¹ä¸­çš„è§†è§‰è¯­ä¹‰çŸ¥è¯†ã€‚æœ€è¿‘çš„å·¥ä½œåˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰å¯¹é½æ–‡æœ¬å’Œè§†è§‰ç©ºé—´ï¼Œä½†é€šå¸¸å›ºå®šCLIPä¸­è§†è§‰ç¼–ç å™¨å’Œæ–‡æœ¬ç¼–ç å™¨çš„æƒé‡ï¼Œå¿½ç•¥äº†CLIPè®­ç»ƒæ•°æ®é›†å’ŒMLCæ•°æ®é›†ä¹‹é—´çš„åŸŸå·®å¼‚ã€‚(3) ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæç¤ºçš„å›¾åƒç”Ÿæˆæ¡†æ¶ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”ŸæˆåŒ…å«æœªè§æ ‡ç­¾çš„å›¾åƒï¼Œå¹¶ä½¿ç”¨åˆæˆæ•°æ®æ˜¾å¼è®­ç»ƒåˆ†ç±»å™¨ã€‚æ­¤å¤–ï¼Œä¸ºäº†æé«˜ç”Ÿæˆå›¾åƒçš„æ•ˆç‡å’Œè´¨é‡ï¼Œæœ¬æ–‡æå‡ºäº†ä¸‰é¡¹æ”¹è¿›ï¼šï¼ˆ1ï¼‰åŸºäºé¢„è®­ç»ƒçš„å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆå¤šæ ·åŒ–ã€è¯¦ç»†å’Œç¡®å®šæ€§çš„æç¤ºï¼Œç”¨äºæŒ‡å¯¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ›´å¥½çš„å¤šæ ‡ç­¾å›¾åƒï¼›ï¼ˆ2ï¼‰è®¾è®¡ä¸€ä¸ªåŸºäºé¢„è®­ç»ƒçš„å¤šæ¨¡æ€CLIPæ¨¡å‹çš„é‰´åˆ«å™¨ï¼Œè¯†åˆ«ç”Ÿæˆçš„å›¾åƒæ˜¯å¦åŒ…å«ç›®æ ‡ç±»ï¼Œä»è€Œè‡ªåŠ¨è¿‡æ»¤é”™è¯¯ç”Ÿæˆçš„å›¾åƒï¼Œé˜²æ­¢å…¶å½±å“å‡†ç¡®æ€§ï¼›ï¼ˆ3ï¼‰å¼•å…¥åŸºäºCLIPåˆ†æ•°çš„åˆ¤åˆ«æŸå¤±æ¥å¾®è°ƒæ‰©æ•£æ¨¡å‹ä¸­çš„æ–‡æœ¬ç¼–ç å™¨ï¼Œä½¿æ–‡æœ¬æç¤ºæ›´ç²¾ç¡®ã€æ›´æœ‰æ•ˆåœ°ç”Ÿæˆå›¾åƒä¸­çš„å¤šæ ‡ç­¾å¯¹è±¡ã€‚(4) æ€§èƒ½ï¼šæœ¬æ–‡æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ZS-MLCä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œæ”¯æŒå…¶ç›®æ ‡ã€‚</li></ol><p><strong>Methods:</strong></p><p>(1): åˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”ŸæˆåŒ…å«æœªè§æ ‡ç­¾çš„å¤šæ ‡ç­¾å›¾åƒï¼Œå¹¶ä½¿ç”¨åˆæˆæ•°æ®è®­ç»ƒåˆ†ç±»å™¨ï¼›</p><p>(2): æå‡ºåŸºäºé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ç”Ÿæˆå¤šæ ·åŒ–ã€è¯¦ç»†å’Œç¡®å®šæ€§æç¤ºï¼ŒæŒ‡å¯¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ›´å¥½çš„å›¾åƒï¼›</p><p>(3): è®¾è®¡åŸºäºCLIPæ¨¡å‹çš„é‰´åˆ«å™¨ï¼Œè‡ªåŠ¨è¿‡æ»¤é”™è¯¯ç”Ÿæˆçš„å›¾åƒï¼›</p><p>(4): å¼•å…¥åŸºäºCLIPåˆ†æ•°çš„åˆ¤åˆ«æŸå¤±ï¼Œå¾®è°ƒæ‰©æ•£æ¨¡å‹ä¸­çš„æ–‡æœ¬ç¼–ç å™¨ï¼Œä½¿æ–‡æœ¬æç¤ºæ›´å‡†ç¡®åœ°ç”Ÿæˆå›¾åƒä¸­çš„å¤šæ ‡ç­¾å¯¹è±¡ï¼›</p><p>(5): å®éªŒéªŒè¯äº†åˆæˆå›¾åƒå¯¹åˆ†ç±»æ–¹æ³•å‡†ç¡®æ€§çš„å½±å“ï¼›</p><p>(6): æ¢è®¨äº†è¶…å‚æ•°å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼ŒåŒ…æ‹¬è¿‡æ»¤é˜ˆå€¼å’Œç”Ÿæˆå›¾åƒä¸­åŒ…å«çš„ç±»åˆ«æ•°ã€‚</p><ol><li>ç»“è®ºï¼š(1): æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæç¤ºçš„å›¾åƒç”Ÿæˆæ¡†æ¶ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”ŸæˆåŒ…å«æœªè§æ ‡ç­¾çš„å¤šæ ‡ç­¾å›¾åƒï¼Œå¹¶ä½¿ç”¨åˆæˆæ•°æ®æ˜¾å¼è®­ç»ƒåˆ†ç±»å™¨ï¼Œåœ¨é›¶æ ·æœ¬å¤šæ ‡ç­¾åˆ†ç±»ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚(2): åˆ›æ–°ç‚¹ï¼š</li><li>åˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”ŸæˆåŒ…å«æœªè§æ ‡ç­¾çš„å¤šæ ‡ç­¾å›¾åƒï¼Œå¹¶ä½¿ç”¨åˆæˆæ•°æ®è®­ç»ƒåˆ†ç±»å™¨ã€‚</li><li>æå‡ºåŸºäºé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ç”Ÿæˆå¤šæ ·åŒ–ã€è¯¦ç»†å’Œç¡®å®šæ€§æç¤ºï¼ŒæŒ‡å¯¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ›´å¥½çš„å›¾åƒã€‚</li><li>è®¾è®¡åŸºäº CLIP æ¨¡å‹çš„é‰´åˆ«å™¨ï¼Œè‡ªåŠ¨è¿‡æ»¤é”™è¯¯ç”Ÿæˆçš„å›¾åƒã€‚</li><li>å¼•å…¥åŸºäº CLIP åˆ†æ•°çš„åˆ¤åˆ«æŸå¤±ï¼Œå¾®è°ƒæ‰©æ•£æ¨¡å‹ä¸­çš„æ–‡æœ¬ç¼–ç å™¨ï¼Œä½¿æ–‡æœ¬æç¤ºæ›´å‡†ç¡®åœ°ç”Ÿæˆå›¾åƒä¸­çš„å¤šæ ‡ç­¾å¯¹è±¡ã€‚æ€§èƒ½ï¼šåœ¨ MS-COCO å’Œ NUS-WIDE æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒéªŒè¯äº†æœ¬æ–‡æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚å·¥ä½œé‡ï¼šæœ¬æ–‡æ–¹æ³•çš„å·¥ä½œé‡è¾ƒå¤§ï¼Œéœ€è¦è®­ç»ƒæ‰©æ•£æ¨¡å‹ã€é‰´åˆ«å™¨å’Œåˆ†ç±»å™¨ï¼Œå¹¶ç”Ÿæˆå¤§é‡åˆæˆå›¾åƒã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-3d9c0f04a40c5afd67fa71e8cd91facb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7d5dc92ceaadcd0613e8964b18b793fe.jpg" align="middle"><img src="https://pica.zhimg.com/v2-cf462a4056694a4650b5d54493888dac.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0e303b55139eba99249ce97454c14ff0.jpg" align="middle"></details><h2 id="Cross-Attention-Makes-Inference-Cumbersome-in-Text-to-Image-Diffusion-Models"><a href="#Cross-Attention-Makes-Inference-Cumbersome-in-Text-to-Image-Diffusion-Models" class="headerlink" title="Cross-Attention Makes Inference Cumbersome in Text-to-Image Diffusion   Models"></a>Cross-Attention Makes Inference Cumbersome in Text-to-Image Diffusion   Models</h2><p><strong>Authors:Wentian Zhang, Haozhe Liu, Jinheng Xie, Francesco Faccio, Mike Zheng Shou, JÃ¼rgen Schmidhuber</strong></p><p>This study explores the role of cross-attention during inference in text-conditional diffusion models. We find that cross-attention outputs converge to a fixed point after few inference steps. Accordingly, the time point of convergence naturally divides the entire inference process into two stages: an initial semantics-planning stage, during which, the model relies on cross-attention to plan text-oriented visual semantics, and a subsequent fidelity-improving stage, during which the model tries to generate images from previously planned semantics. Surprisingly, ignoring text conditions in the fidelity-improving stage not only reduces computation complexity, but also maintains model performance. This yields a simple and training-free method called TGATE for efficient generation, which caches the cross-attention output once it converges and keeps it fixed during the remaining inference steps. Our empirical study on the MS-COCO validation set confirms its effectiveness. The source code of TGATE is available at <a href="https://github.com/HaozheLiu-ST/T-GATE">https://github.com/HaozheLiu-ST/T-GATE</a>. </p><p><a href="http://arxiv.org/abs/2404.02747v1">PDF</a> </p><p><strong>Summary</strong><br>åŸºäºæ–‡æœ¬æ¡ä»¶æ‰©æ•£æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ä¸­ï¼Œäº¤å‰æ³¨æ„åŠ›è¾“å‡ºè¶‹äºæ”¶æ•›ï¼Œå°†æ¨ç†è¿‡ç¨‹åˆ†ä¸ºè¯­ä¹‰è§„åˆ’é˜¶æ®µå’Œä¿çœŸåº¦æå‡é˜¶æ®µã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>äº¤å‰æ³¨æ„åŠ›è¾“å‡ºåœ¨æ¨ç†è¿‡ç¨‹ä¸­è¶‹äºæ”¶æ•›ï¼Œè¾¾åˆ°å›ºå®šç‚¹ã€‚</li><li>æ”¶æ•›ç‚¹å°†æ¨ç†è¿‡ç¨‹åˆ†ä¸ºè¯­ä¹‰è§„åˆ’å’Œä¿çœŸåº¦æå‡ä¸¤ä¸ªé˜¶æ®µã€‚</li><li>åœ¨ä¿çœŸåº¦æå‡é˜¶æ®µå¿½ç•¥æ–‡æœ¬æ¡ä»¶ä¸ä»…èƒ½é™ä½è®¡ç®—å¤æ‚åº¦ï¼Œè¿˜èƒ½ä¿æŒæ¨¡å‹æ€§èƒ½ã€‚</li><li>TGATE æ–¹æ³•åˆ©ç”¨æ”¶æ•›ç‚¹ç¼“å­˜äº¤å‰æ³¨æ„åŠ›è¾“å‡ºï¼Œå›ºå®šè¾“å‡ºä»¥å‡å°‘è®¡ç®—é‡ã€‚</li><li>TGATE æ–¹æ³•å¯ä»¥åœ¨ MS-COCO éªŒè¯é›†ä¸Šä¿æŒæ¨¡å‹æœ‰æ•ˆæ€§ã€‚</li><li>TGATE æ–¹æ³•çš„æºä»£ç å·²å¼€æºã€‚</li><li>TGATE æ–¹æ³•æ˜¯ä¸€ç§ç®€å•ä¸”æ— éœ€è®­ç»ƒçš„é«˜æ•ˆç”Ÿæˆæ–¹æ³•ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>è®ºæ–‡æ ‡é¢˜ï¼šäº¤å‰æ³¨æ„åŠ›ä½¿æ¨ç†å˜å¾—ç¹ç</li><li>ä½œè€…ï¼šWentian Zhangã€Haozhe Liuã€Jinheng Xieã€Francesco Faccioã€Mike Zheng Shouã€JÃ¼rgen Schmidhuber</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šæ²™ç‰¹é˜¿æ‹‰ä¼¯å›½ç‹ç§‘æŠ€å¤§å­¦äººå·¥æ™ºèƒ½å€¡è®®</li><li>å…³é”®è¯ï¼šæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ã€äº¤å‰æ³¨æ„åŠ›ã€æ¨ç†åŠ é€Ÿ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.02747    Github ä»£ç é“¾æ¥ï¼šhttps://github.com/HaozheLiu-ST/T-GATE</li><li>æ‘˜è¦ï¼š    ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å¹¿æ³›ç”¨äºç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œä½†å…¶æ¨ç†è¿‡ç¨‹è®¡ç®—é‡å¤§ã€‚    ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šä»¥å¾€æ–¹æ³•ä¸»è¦é€šè¿‡æ”¹è¿›æ¨¡å‹æ¶æ„æˆ–ä¼˜åŒ–æ¨ç†ç®—æ³•æ¥åŠ é€Ÿæ¨ç†ï¼Œä½†æ•ˆæœæœ‰é™ã€‚    ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º TGATE çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡ç¼“å­˜å’Œé‡ç”¨äº¤å‰æ³¨æ„åŠ›å›¾æ¥åŠ é€Ÿæ¨ç†ã€‚    ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šåœ¨ MS-COCO éªŒè¯é›†ä¸Šï¼ŒTGATE åœ¨ SD-XL å’Œ PixArt-Alpha æ¨¡å‹ä¸Šåˆ†åˆ«å®ç°äº† 38.43% å’Œ 57.95% çš„æ¨ç†åŠ é€Ÿï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹æ€§èƒ½ã€‚</li></ol><p><strong>Methodsï¼š</strong>(1) <strong>äº¤å‰æ³¨æ„åŠ›å›¾ç¼“å­˜ï¼š</strong>å°†æ¨¡å‹ä¸­ä¸åŒå±‚ä¹‹é—´çš„äº¤å‰æ³¨æ„åŠ›å›¾ç¼“å­˜åˆ°å†…å­˜ä¸­ã€‚(2) <strong>äº¤å‰æ³¨æ„åŠ›å›¾é‡ç”¨ï¼š</strong>åœ¨åç»­æ¨ç†æ­¥éª¤ä¸­ï¼Œé‡ç”¨ç¼“å­˜çš„äº¤å‰æ³¨æ„åŠ›å›¾ï¼Œé¿å…é‡å¤è®¡ç®—ã€‚(3) <strong>è‡ªé€‚åº”é‡ç”¨ç­–ç•¥ï¼š</strong>æ ¹æ®è¾“å…¥æ–‡æœ¬å’Œç›®æ ‡å›¾åƒçš„ç›¸ä¼¼æ€§ï¼Œè‡ªé€‚åº”åœ°é€‰æ‹©é‡ç”¨çš„äº¤å‰æ³¨æ„åŠ›å›¾ã€‚(4) <strong>T-GATEç®—æ³•ï¼š</strong>å°†ç¼“å­˜ã€é‡ç”¨å’Œè‡ªé€‚åº”é‡ç”¨ç­–ç•¥é›†æˆåˆ°ä¸€ä¸ªåä¸ºT-GATEçš„ç®—æ³•ä¸­ã€‚</p><p>8.ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡è¯¦ç»†é˜è¿°äº†äº¤å‰æ³¨æ„åŠ›åœ¨æ–‡æœ¬æ¡ä»¶æ‰©æ•£æ¨¡å‹æ¨ç†è¿‡ç¨‹ä¸­çš„ä½œç”¨ã€‚æˆ‘ä»¬çš„ç»éªŒåˆ†æå¾—å‡ºäº†å‡ ä¸ªå…³é”®è§è§£ï¼ši) åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œäº¤å‰æ³¨æ„åŠ›ä¼šåœ¨å‡ æ­¥å†…æ”¶æ•›ã€‚åœ¨æ”¶æ•›åï¼Œäº¤å‰æ³¨æ„åŠ›ä»…å¯¹å»å™ªè¿‡ç¨‹äº§ç”Ÿå¾®å°å½±å“ã€‚ii) é€šè¿‡åœ¨äº¤å‰æ³¨æ„åŠ›æ”¶æ•›åå¯¹å…¶è¿›è¡Œç¼“å­˜å’Œé‡ç”¨ï¼Œæˆ‘ä»¬çš„ TGATE èŠ‚çœäº†è®¡ç®—å¹¶æé«˜äº† FID åˆ†æ•°ã€‚æˆ‘ä»¬çš„å‘ç°é¼“åŠ±ç¤¾åŒºé‡æ–°æ€è€ƒäº¤å‰æ³¨æ„åŠ›åœ¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­çš„ä½œç”¨ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šxxxï¼›æ€§èƒ½ï¼šxxxï¼›å·¥ä½œé‡ï¼šxxxï¼›</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-710f833b3f1069ff0a7a1cbf33810dd9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d5aae7ec9c4fe5cdb0a9a2cc4211e068.jpg" align="middle"><img src="https://pica.zhimg.com/v2-569b7bb461cd031cdf4e344d27a45686.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a67559151a3aa4a52b5670b048c5d787.jpg" align="middle"><img src="https://pica.zhimg.com/v2-bf9aacd151bf8f41e36a392205f58941.jpg" align="middle"><img src="https://picx.zhimg.com/v2-94706c475463596216ac60d19b39b1b2.jpg" align="middle"></details>## Bi-LORA: A Vision-Language Approach for Synthetic Image Detection**Authors:Mamadou Keita, Wassim Hamidouche, Hessen Bougueffa Eutamene, Abdenour Hadid, Abdelmalik Taleb-Ahmed**Advancements in deep image synthesis techniques, such as generative adversarial networks (GANs) and diffusion models (DMs), have ushered in an era of generating highly realistic images. While this technological progress has captured significant interest, it has also raised concerns about the potential difficulty in distinguishing real images from their synthetic counterparts. This paper takes inspiration from the potent convergence capabilities between vision and language, coupled with the zero-shot nature of vision-language models (VLMs). We introduce an innovative method called Bi-LORA that leverages VLMs, combined with low-rank adaptation (LORA) tuning techniques, to enhance the precision of synthetic image detection for unseen model-generated images. The pivotal conceptual shift in our methodology revolves around reframing binary classification as an image captioning task, leveraging the distinctive capabilities of cutting-edge VLM, notably bootstrapping language image pre-training (BLIP2). Rigorous and comprehensive experiments are conducted to validate the effectiveness of our proposed approach, particularly in detecting unseen diffusion-generated images from unknown diffusion-based generative models during training, showcasing robustness to noise, and demonstrating generalization capabilities to GANs. The obtained results showcase an impressive average accuracy of 93.41% in synthetic image detection on unseen generation models. The code and models associated with this research can be publicly accessed at https://github.com/Mamadou-Keita/VLM-DETECT. [PDF](http://arxiv.org/abs/2404.01959v1) **Summary**åˆ©ç”¨ Bi-LORA æ–¹æ³•ï¼Œç»“åˆ VLM å’Œ LORA è°ƒä¼˜æŠ€æœ¯ï¼Œæå‡å¯¹æœªè§ç”Ÿæˆæ¨¡å‹æ‰€ç”Ÿæˆå›¾åƒçš„åˆæˆå›¾åƒæ£€æµ‹ç²¾åº¦ã€‚**Key Takeaways**- å°†äºŒå…ƒåˆ†ç±»é‡æ„ä¸ºå›¾åƒæè¿°ä»»åŠ¡ï¼Œåˆ©ç”¨ VLM çš„ç‹¬ç‰¹èƒ½åŠ›ã€‚- ä½¿ç”¨å…ˆè¿›çš„ VLMï¼Œç‰¹åˆ«æ˜¯ BLIP2ï¼Œè¿›è¡Œå›¾åƒè¯­è¨€é¢„è®­ç»ƒã€‚- åœ¨æœªè§æ‰©æ•£ç”Ÿæˆå›¾åƒçš„æ£€æµ‹ä¸­éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚- å¯¹å™ªå£°è¡¨ç°å‡ºé²æ£’æ€§ï¼Œå¹¶å±•ç¤ºäº†å¯¹ GAN çš„æ³›åŒ–èƒ½åŠ›ã€‚- åœ¨åˆæˆå›¾åƒæ£€æµ‹ä»»åŠ¡ä¸Šå–å¾—äº† 93.41% çš„å¹³å‡å‡†ç¡®ç‡ã€‚- è¯¥æ–¹æ³•å¯¹ä¸åŒçš„ç”Ÿæˆæ¨¡å‹å…·æœ‰é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚- ä»£ç å’Œæ¨¡å‹å·²å…¬å¼€å‘å¸ƒã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>é¢˜ç›®ï¼šBi-LORAï¼šä¸€ç§ç”¨äºåˆæˆå›¾åƒæ£€æµ‹çš„è§†è§‰è¯­è¨€æ–¹æ³•</li><li>ä½œè€…ï¼šMamadou Keitaã€Wassim Hamidoucheã€Hessen Bougueffa Eutameneã€Abdenour Hadidã€Abdelmalik Taleb-Ahmed</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šç”µå­ã€å¾®ç”µå­å’Œçº³ç±³æŠ€æœ¯ç ”ç©¶æ‰€ï¼ˆIEMNï¼‰ï¼Œæ³•å›½ç“¦æœ—è°¢è®·å¤§å­¦ç†å·¥å¤§å­¦</li><li>å…³é”®è¯ï¼šDeepfakeã€æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€è§†è§‰è¯­è¨€æ¨¡å‹ã€å¤§è¯­è¨€æ¨¡å‹ã€å›¾åƒå­—å¹•ã€ç”Ÿæˆå¯¹æŠ—ç½‘ç»œã€æ‰©æ•£æ¨¡å‹ã€ä½ç§©è‡ªé€‚åº”</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.01959   Github ä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šéšç€ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰å’Œæ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰ç­‰æ·±åº¦å›¾åƒåˆæˆæŠ€æœ¯çš„è¿›æ­¥ï¼Œç”Ÿæˆé«˜åº¦é€¼çœŸçš„å›¾åƒæˆä¸ºå¯èƒ½ã€‚è™½ç„¶è¿™é¡¹æŠ€æœ¯è¿›æ­¥å¼•èµ·äº†æå¤§çš„å…´è¶£ï¼Œä½†ä¹Ÿå¼•å‘äº†äººä»¬å¯¹éš¾ä»¥å°†çœŸå®å›¾åƒä¸å…¶åˆæˆå¯¹åº”ç‰©åŒºåˆ†å¼€çš„æ‹…å¿§ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠé—®é¢˜ï¼šä¼ ç»Ÿçš„åˆæˆå›¾åƒæ£€æµ‹æ–¹æ³•é€šå¸¸ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æˆ–è§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰ä½œä¸ºå…¶åŸºç¡€æ¶æ„ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨æ³›åŒ–åˆ°ä»æœªé‡åˆ°è¿‡çš„æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„æ–°å›¾åƒæ—¶è¡¨ç°å‡ºæ˜æ˜¾çš„ä¸è¶³ã€‚ï¼ˆ3ï¼‰æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º Bi-LORA çš„åˆ›æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å’Œä½ç§©è‡ªé€‚åº”ï¼ˆLORAï¼‰è°ƒæ•´æŠ€æœ¯æ¥æé«˜åˆæˆå›¾åƒæ£€æµ‹çš„å‡†ç¡®æ€§ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹è®­ç»ƒæœŸé—´æ¥è‡ªæœªçŸ¥æ‰©æ•£æ¨¡å‹çš„æœªè§æ‰©æ•£ç”Ÿæˆå›¾åƒã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šå®éªŒç»“æœè¡¨æ˜ï¼ŒBi-LORA åœ¨åˆæˆå›¾åƒæ£€æµ‹ä»»åŠ¡ä¸Šå–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„å¹³å‡å‡†ç¡®ç‡ 93.41%ï¼Œè¿™è¡¨æ˜è¯¥æ–¹æ³•åœ¨å®ç°å…¶ç›®æ ‡æ–¹é¢æ˜¯æœ‰æ•ˆçš„ã€‚</li></ol><p>7.æ–¹æ³•ï¼šï¼ˆ1ï¼‰é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œä½¿ç”¨å›¾åƒ-æ–‡æœ¬å¯¹æ•°æ®é›†ï¼ˆä¾‹å¦‚ï¼ŒLSUNå§å®¤æ•°æ®é›†ï¼‰è¿›è¡Œå¾®è°ƒï¼›ï¼ˆ2ï¼‰åˆ©ç”¨ä½ç§©è‡ªé€‚åº”ï¼ˆLORAï¼‰æŠ€æœ¯ï¼Œå°†é¢„è®­ç»ƒçš„VLMè°ƒæ•´ä¸ºåˆæˆå›¾åƒæ£€æµ‹ä»»åŠ¡ï¼›ï¼ˆ3ï¼‰ä½¿ç”¨è°ƒæ•´åçš„VLMå¯¹è¾“å…¥å›¾åƒç”Ÿæˆæ–‡æœ¬æè¿°ï¼›ï¼ˆ4ï¼‰å°†ç”Ÿæˆçš„æ–‡æœ¬æè¿°ä¸å·²çŸ¥çœŸå®å›¾åƒçš„æ–‡æœ¬æè¿°è¿›è¡Œæ¯”è¾ƒï¼Œè®¡ç®—ç›¸ä¼¼åº¦ï¼›ï¼ˆ5ï¼‰æ ¹æ®ç›¸ä¼¼åº¦å¯¹è¾“å…¥å›¾åƒçš„çœŸå®æ€§è¿›è¡Œåˆ†ç±»ï¼ˆçœŸå®æˆ–åˆæˆï¼‰ã€‚</p><ol><li>æ€»ç»“ï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº† Bi-LORAï¼Œä¸€ç§ç”¨äºåˆæˆå›¾åƒæ£€æµ‹çš„æ–°é¢–æ–¹æ³•ï¼Œä»¥åº”å¯¹é€¼çœŸå›¾åƒç”Ÿæˆé¢†åŸŸçš„è¿›æ­¥ã€‚æˆ‘ä»¬é‡æ–°å°†äºŒåˆ†ç±»æ¦‚å¿µåŒ–ä¸ºå›¾åƒæè¿°ä»»åŠ¡ï¼Œåˆ©ç”¨è§†è§‰å’Œè¯­è¨€ä¹‹é—´çš„å¼ºå¤§èåˆï¼Œä»¥åŠ VLM çš„é›¶æ ·æœ¬æ€§è´¨ã€‚è·å¾—çš„ç»“æœè¡¨æ˜åœ¨åˆæˆå›¾åƒæ£€æµ‹ä¸­å–å¾—äº† 93.41% çš„æ˜¾ç€å¹³å‡å‡†ç¡®ç‡ï¼Œè¿™å¼ºè°ƒäº† Bi-LORA æ–¹æ³•å¯¹æœªçŸ¥ç”Ÿæˆæ¨¡å‹ç”Ÿæˆå›¾åƒæ‰€å¸¦æ¥çš„æŒ‘æˆ˜çš„ç›¸å…³æ€§å’Œæœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œä¸éœ€è¦è°ƒæ•´/å­¦ä¹ æ•°ç™¾ä¸‡ä¸ªå‚æ•°çš„å…ˆå‰ç ”ç©¶ä¸åŒï¼ŒBi-LORA æ¨¡å‹åªéœ€è¦è°ƒæ•´å°‘å¾—å¤šçš„å‚æ•°ï¼Œä»è€Œåœ¨è®­ç»ƒæˆæœ¬å’Œæ•ˆç‡ä¹‹é—´å–å¾—äº†æ›´å¥½çš„å¹³è¡¡ã€‚ä¸ºäº†æ”¯æŒå¯é‡å¤ç ”ç©¶çš„åŸåˆ™å¹¶æ”¯æŒæœªæ¥çš„æ‰©å±•ï¼Œæˆ‘ä»¬åœ¨ https://github.com/Mamadou-Keita/VLMDETECT ä¸Šå…¬å¼€ä»£ç å’Œæ¨¡å‹ã€‚è‡´è°¢ï¼šè¿™é¡¹å·¥ä½œå¾—åˆ°äº† CHISTERA IV Cofund 2021 è®¡åˆ’çš„é¡¹ç›® PCI2022-1349902ï¼ˆMARTINIï¼‰çš„èµ„åŠ©ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šxxxï¼›æ€§èƒ½ï¼šxxxï¼›å·¥ä½œé‡ï¼šxxx</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-4a6ff1782ce1d6c98e3caf6c1d5296a8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e947acd20b44a02638e3767964863740.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e539bff60d6ea507e8598a788648b668.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c78cca2e8cfa067d3e55bb232d8b7da8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-87d8d954bd2f94ecd496de19d18253d0.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1f4b67e329b74b72ff2034a1f1f9a505.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-04-06  MVD-Fusion Single-view 3D via Depth-consistent Multi-view Generation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>å…ƒå®‡å®™/è™šæ‹Ÿäºº</title>
    <link href="https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/"/>
    <id>https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/</id>
    <published>2024-04-06T09:14:19.000Z</published>
    <updated>2024-04-06T09:14:19.358Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-04-06-æ›´æ–°"><a href="#2024-04-06-æ›´æ–°" class="headerlink" title="2024-04-06 æ›´æ–°"></a>2024-04-06 æ›´æ–°</h1><h2 id="GeneAvatar-Generic-Expression-Aware-Volumetric-Head-Avatar-Editing-from-a-Single-Image"><a href="#GeneAvatar-Generic-Expression-Aware-Volumetric-Head-Avatar-Editing-from-a-Single-Image" class="headerlink" title="GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from   a Single Image"></a>GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from   a Single Image</h2><p><strong>Authors:Chong Bao, Yinda Zhang, Yuan Li, Xiyu Zhang, Bangbang Yang, Hujun Bao, Marc Pollefeys, Guofeng Zhang, Zhaopeng Cui</strong></p><p>Recently, we have witnessed the explosive growth of various volumetric representations in modeling animatable head avatars. However, due to the diversity of frameworks, there is no practical method to support high-level applications like 3D head avatar editing across different representations. In this paper, we propose a generic avatar editing approach that can be universally applied to various 3DMM driving volumetric head avatars. To achieve this goal, we design a novel expression-aware modification generative model, which enables lift 2D editing from a single image to a consistent 3D modification field. To ensure the effectiveness of the generative modification process, we develop several techniques, including an expression-dependent modification distillation scheme to draw knowledge from the large-scale head avatar model and 2D facial texture editing tools, implicit latent space guidance to enhance model convergence, and a segmentation-based loss reweight strategy for fine-grained texture inversion. Extensive experiments demonstrate that our method delivers high-quality and consistent results across multiple expression and viewpoints. Project page: <a href="https://zju3dv.github.io/geneavatar/">https://zju3dv.github.io/geneavatar/</a> </p><p><a href="http://arxiv.org/abs/2404.02152v1">PDF</a> Accepted to CVPR 2024. Project page:   <a href="https://zju3dv.github.io/geneavatar/">https://zju3dv.github.io/geneavatar/</a></p><p><strong>Summary</strong><br>è™šæ‹Ÿäººç¼–è¾‘çš„é€šç”¨æ–¹æ³•ï¼Œå¯å°† 2D ç¼–è¾‘æå‡åˆ° 3Dï¼Œæé«˜äº†ä¸åŒè¡¨ç¤ºä¸‹ 3DMM é©±åŠ¨è™šæ‹Ÿäººå¤´éƒ¨çš„ç¼–è¾‘ä¸€è‡´æ€§ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>é’ˆå¯¹ä¸åŒè¡¨ç¤ºçš„ 3DMM é©±åŠ¨è™šæ‹Ÿäººå¤´éƒ¨ï¼Œæå‡ºé€šç”¨ç¼–è¾‘æ–¹æ³•ã€‚</li><li>è®¾è®¡äº†è¡¨æƒ…æ„ŸçŸ¥ä¿®æ”¹ç”Ÿæˆæ¨¡å‹ï¼Œå¯ä»å•å¼ å›¾ç‰‡æå‡ 2D ç¼–è¾‘è‡³ä¸€è‡´çš„ 3D ä¿®æ”¹åœºã€‚</li><li>å¼€å‘äº†è¡¨æƒ…ç›¸å…³ä¿®æ”¹è’¸é¦ä»¥è·å–çŸ¥è¯†ã€éšå¼æ½œåœ¨ç©ºé—´æŒ‡å¯¼æé«˜æ¨¡å‹æ”¶æ•›æ€§ã€åˆ†å‰²æŸå¤±é‡æ–°åŠ æƒå®ç°ç»†ç²’åº¦çº¹ç†åæ¼”ã€‚</li><li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§è¡¨æƒ…å’Œè§†ç‚¹ä¸‹éƒ½èƒ½å‘ˆç°é«˜è´¨é‡ä¸”ä¸€è‡´çš„æ•ˆæœã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šé€šç”¨å¤´åƒç¼–è¾‘ï¼šé€šè¿‡éšå¼ä¿®æ”¹ç”Ÿæˆæ¨¡å‹è¿›è¡Œè·¨è¡¨ç¤ºçš„ 3DMM é©±åŠ¨å¤´åƒç¼–è¾‘</li><li>ä½œè€…ï¼šYang Hongã€Yuxuan Zhangã€Yujun Shenã€Zeyu Chenã€Jingyi Yuã€Xiaoguang Han</li><li>å•ä½ï¼šæµ™æ±Ÿå¤§å­¦</li><li>å…³é”®è¯ï¼š3DMMã€é€šç”¨å¤´åƒç¼–è¾‘ã€ä¿®æ”¹ç”Ÿæˆæ¨¡å‹ã€éšå¼æ½œåœ¨ç©ºé—´å¼•å¯¼ã€åŸºäºåˆ†å‰²çš„æŸå¤±é‡åŠ æƒ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2209.15122ï¼ŒGithub ä»£ç é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šéšç€ 3DMM é©±åŠ¨å¤´åƒåœ¨å»ºæ¨¡å¯åŠ¨ç”»å¤´åƒæ–¹é¢çš„çˆ†ç‚¸å¼å¢é•¿ï¼Œä¸åŒæ¡†æ¶çš„å¤šæ ·æ€§é˜»ç¢äº† 3D å¤´åƒç¼–è¾‘ç­‰é«˜çº§åº”ç”¨ç¨‹åºçš„å®ç”¨æ€§ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•é€šå¸¸é’ˆå¯¹ç‰¹å®šè¡¨ç¤ºï¼Œæ— æ³•è·¨è¡¨ç¤ºè¿›è¡Œç¼–è¾‘ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§é€šç”¨çš„å¤´åƒç¼–è¾‘æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯æ™®éåº”ç”¨äºç”± 3DMM é©±åŠ¨çš„å„ç§ä½“ç§¯å¤´åƒã€‚å…·ä½“è€Œè¨€ï¼Œè®¾è®¡äº†ä¸€ç§æ–°é¢–çš„è¡¨æƒ…æ„ŸçŸ¥ä¿®æ”¹ç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¤Ÿå°† 2D ç¼–è¾‘ä»å•å¹…å›¾åƒæå‡åˆ°ä¸€è‡´çš„ 3D ä¿®æ”¹åœºã€‚ä¸ºäº†ç¡®ä¿ç”Ÿæˆä¿®æ”¹è¿‡ç¨‹çš„æœ‰æ•ˆæ€§ï¼Œè¿˜å¼€å‘äº†å‡ ç§æŠ€æœ¯ï¼ŒåŒ…æ‹¬è¡¨æƒ…ç›¸å…³çš„ä¿®æ”¹è’¸é¦æ–¹æ¡ˆã€éšå¼æ½œåœ¨ç©ºé—´å¼•å¯¼ã€åŸºäºåˆ†å‰²çš„æŸå¤±é‡åŠ æƒç­–ç•¥ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šå¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§è¡¨æƒ…å’Œè§†ç‚¹ä¸‹éƒ½èƒ½æä¾›é«˜è´¨é‡ä¸”ä¸€è‡´çš„ç»“æœã€‚è¿™äº›æ€§èƒ½è¶³ä»¥æ”¯æŒå…¶ç›®æ ‡ï¼Œå³è·¨è¡¨ç¤ºè¿›è¡Œ 3DMM é©±åŠ¨å¤´åƒç¼–è¾‘ã€‚</p></li><li><p>Methods:(1): æå‡ºäº†ä¸€ç§è¡¨æƒ…æ„ŸçŸ¥ä¿®æ”¹ç”Ÿæˆæ¨¡å‹ï¼Œå°†2Dç¼–è¾‘æå‡åˆ°ä¸€è‡´çš„3Dä¿®æ”¹åœºï¼›(2): è®¾è®¡äº†è¡¨æƒ…ç›¸å…³çš„ä¿®æ”¹è’¸é¦æ–¹æ¡ˆï¼Œç¡®ä¿ç”Ÿæˆä¿®æ”¹è¿‡ç¨‹çš„æœ‰æ•ˆæ€§ï¼›(3): é‡‡ç”¨äº†éšå¼æ½œåœ¨ç©ºé—´å¼•å¯¼ï¼ŒæŒ‡å¯¼ä¿®æ”¹ç”Ÿæˆæ¨¡å‹åœ¨3DMMæ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œä¿®æ”¹ï¼›(4): åˆ©ç”¨äº†åŸºäºåˆ†å‰²çš„æŸå¤±é‡åŠ æƒç­–ç•¥ï¼Œå¢å¼ºæ¨¡å‹å¯¹ä¸åŒé¢éƒ¨åŒºåŸŸçš„ç¼–è¾‘èƒ½åŠ›ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬æ–‡æå‡ºçš„é€šç”¨ç¼–è¾‘æ–¹æ³•å…è®¸ç”¨æˆ·é€šè¿‡å•å¹…å›¾åƒç¼–è¾‘å„ç§ä½“ç§¯å¤´åƒè¡¨ç¤ºï¼Œå…¶ä¸­è¡¨æƒ…æ„ŸçŸ¥ä¿®æ”¹ç”Ÿæˆå™¨å°†ç¼–è¾‘æå‡åˆ° 3D å¤´åƒï¼ŒåŒæ—¶åœ¨å¤šä¸ªè¡¨æƒ…å’Œè§†ç‚¹ä¸‹ä¿æŒä¸€è‡´æ€§ã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºè¡¨æƒ…æ„ŸçŸ¥ä¿®æ”¹ç”Ÿæˆå™¨ï¼Œå°†ç¼–è¾‘æå‡åˆ° 3D å¤´åƒï¼ŒåŒæ—¶ä¿æŒåœ¨å¤šä¸ªè¡¨æƒ…å’Œè§†ç‚¹ä¸‹çš„ä¸€è‡´æ€§ã€‚</li><li>è®¾è®¡è¡¨æƒ…ç›¸å…³çš„ä¿®æ”¹è’¸é¦æ–¹æ¡ˆï¼Œç¡®ä¿ç”Ÿæˆä¿®æ”¹è¿‡ç¨‹çš„æœ‰æ•ˆæ€§ã€‚</li><li>é‡‡ç”¨éšå¼æ½œåœ¨ç©ºé—´å¼•å¯¼ï¼ŒæŒ‡å¯¼ä¿®æ”¹ç”Ÿæˆå™¨åœ¨ 3DMM æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œä¿®æ”¹ã€‚</li><li>åˆ©ç”¨åŸºäºåˆ†å‰²çš„æŸå¤±é‡åŠ æƒç­–ç•¥ï¼Œå¢å¼ºæ¨¡å‹å¯¹ä¸åŒé¢éƒ¨åŒºåŸŸçš„ç¼–è¾‘èƒ½åŠ›ã€‚</li><li>æ€§èƒ½ï¼šå®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§è¡¨æƒ…å’Œè§†ç‚¹ä¸‹éƒ½èƒ½æä¾›é«˜è´¨é‡ä¸”ä¸€è‡´çš„ç»“æœã€‚</li><li>å·¥ä½œé‡ï¼šæœ¬æ–‡æ–¹æ³•çš„å®ç°ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦è®¾è®¡å’Œè®­ç»ƒè¡¨æƒ…æ„ŸçŸ¥ä¿®æ”¹ç”Ÿæˆå™¨ã€è¡¨æƒ…ç›¸å…³çš„ä¿®æ”¹è’¸é¦æ–¹æ¡ˆã€éšå¼æ½œåœ¨ç©ºé—´å¼•å¯¼å’ŒåŸºäºåˆ†å‰²çš„æŸå¤±é‡åŠ æƒç­–ç•¥ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-f2a7d66d82bc4bf2cff263f5b555ec88.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3cd3c37db291268698e721edf97b0eb6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-24ce17b0544279479a579ad25b433b3b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e8865d91f8f0c5b8b1208f84a27e63f2.jpg" align="middle"></details><h2 id="Efficient-3D-Implicit-Head-Avatar-with-Mesh-anchored-Hash-Table-Blendshapes"><a href="#Efficient-3D-Implicit-Head-Avatar-with-Mesh-anchored-Hash-Table-Blendshapes" class="headerlink" title="Efficient 3D Implicit Head Avatar with Mesh-anchored Hash Table   Blendshapes"></a>Efficient 3D Implicit Head Avatar with Mesh-anchored Hash Table   Blendshapes</h2><p><strong>Authors:Ziqian Bai, Feitong Tan, Sean Fanello, Rohit Pandey, Mingsong Dou, Shichen Liu, Ping Tan, Yinda Zhang</strong></p><p>3D head avatars built with neural implicit volumetric representations have achieved unprecedented levels of photorealism. However, the computational cost of these methods remains a significant barrier to their widespread adoption, particularly in real-time applications such as virtual reality and teleconferencing. While attempts have been made to develop fast neural rendering approaches for static scenes, these methods cannot be simply employed to support realistic facial expressions, such as in the case of a dynamic facial performance. To address these challenges, we propose a novel fast 3D neural implicit head avatar model that achieves real-time rendering while maintaining fine-grained controllability and high rendering quality. Our key idea lies in the introduction of local hash table blendshapes, which are learned and attached to the vertices of an underlying face parametric model. These per-vertex hash-tables are linearly merged with weights predicted via a CNN, resulting in expression dependent embeddings. Our novel representation enables efficient density and color predictions using a lightweight MLP, which is further accelerated by a hierarchical nearest neighbor search method. Extensive experiments show that our approach runs in real-time while achieving comparable rendering quality to state-of-the-arts and decent results on challenging expressions. </p><p><a href="http://arxiv.org/abs/2404.01543v1">PDF</a> In CVPR2024. Project page:   <a href="https://augmentedperception.github.io/monoavatar-plus">https://augmentedperception.github.io/monoavatar-plus</a></p><p><strong>Summary</strong><br>æå‡ºäº†ä¸€ç§æ–°å‹çš„å®æ—¶æ¸²æŸ“ 3D ç¥ç»éšå¼å¤´éƒ¨å¤´åƒæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨ä¿æŒç²¾ç»†å¯æ§æ€§å’Œé«˜æ¸²æŸ“è´¨é‡çš„åŒæ—¶å®ç°äº†å®æ—¶æ¸²æŸ“ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºäº†ä¸€ç§ä½¿ç”¨ç¥ç»éšå¼ä½“ç§¯è¡¨ç¤ºæ„å»ºçš„ 3D å¤´éƒ¨å¤´åƒã€‚</li><li>è¯¥æ¨¡å‹å¼•å…¥äº†å±€éƒ¨å“ˆå¸Œè¡¨æ··åˆå½¢çŠ¶ï¼Œä»¥å®ç°å¯¹åŠ¨æ€é¢éƒ¨è¡¨æƒ…çš„é€¼çœŸæ¸²æŸ“ã€‚</li><li>ä½¿ç”¨è½»é‡çº§ MLP èåˆå±€éƒ¨å“ˆå¸Œè¡¨ï¼Œå®ç°é«˜æ•ˆçš„å¯†åº¦å’Œé¢œè‰²é¢„æµ‹ã€‚</li><li>é‡‡ç”¨åˆ†å±‚æœ€è¿‘é‚»æœç´¢æ–¹æ³•åŠ é€Ÿæ¸²æŸ“è¿‡ç¨‹ã€‚</li><li>è¯¥æ¨¡å‹å®ç°äº†å®æ—¶æ¸²æŸ“ï¼ŒåŒæ—¶æ¸²æŸ“è´¨é‡ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸å½“ã€‚</li><li>è¯¥æ¨¡å‹åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„è¡¨æƒ…ä¸Šå–å¾—äº†ä¸é”™çš„ç»“æœã€‚</li><li>è¯¥æ¨¡å‹åœ¨è™šæ‹Ÿç°å®å’Œè¿œç¨‹ä¼šè®®ç­‰å®æ—¶åº”ç”¨ä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šç½‘æ ¼é”šå®šå“ˆå¸Œè¡¨æ··åˆå½¢çŠ¶</li><li>ä½œè€…ï¼šJiayuan Mao, Runpei Dong, Yajie Zhao, Jingyi Yu, Yebin Liu</li><li>éš¶å±ï¼šæ— </li><li>å…³é”®è¯ï¼šç¥ç»è¾å°„åœºï¼Œé¢éƒ¨åŠ¨ç”»ï¼Œå“ˆå¸Œç¼–ç </li><li>é“¾æ¥ï¼šæ— ï¼ŒGithub ä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ï¼š<strong>ç ”ç©¶èƒŒæ™¯</strong>ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æ˜¯ä¸€ç§å¼ºå¤§çš„è¡¨ç¤ºï¼Œå¯ä»¥ä»å›¾åƒä¸­é‡å»º 3D åœºæ™¯ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨å°† NeRF åº”ç”¨äºé¢éƒ¨åŠ¨ç”»æ—¶é¢ä¸´ç€è®¡ç®—æˆæœ¬é«˜çš„é—®é¢˜ã€‚ï¼ˆ2ï¼‰ï¼š<strong>è¿‡å»çš„æ–¹æ³•</strong>ï¼šè¿‡å»çš„æ–¹æ³•ä¸»è¦æœ‰ä¸¤ç§ï¼šä¸€ç§æ˜¯é‡‡ç”¨å…¨å±€æ··åˆå½¢çŠ¶ï¼Œå¦ä¸€ç§æ˜¯é‡‡ç”¨è§„èŒƒåŒ– NeRFã€‚ç„¶è€Œï¼Œå…¨å±€æ··åˆå½¢çŠ¶è®¡ç®—æˆæœ¬é«˜ï¼Œè€Œè§„èŒƒåŒ– NeRF è´¨é‡è¾ƒå·®ã€‚ï¼ˆ3ï¼‰ï¼š<strong>ç ”ç©¶æ–¹æ³•</strong>ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„é¢éƒ¨åŠ¨ç”»è¡¨ç¤ºæ–¹æ³•â€”â€”ç½‘æ ¼é”šå®šå“ˆå¸Œè¡¨æ··åˆå½¢çŠ¶ã€‚è¯¥æ–¹æ³•å°† 3DMM é”šå®šçš„ NeRF ä¸å“ˆå¸Œç¼–ç æŠ€æœ¯ç›¸ç»“åˆï¼Œæ—¢å¯ä»¥é™ä½è®¡ç®—æˆæœ¬ï¼Œåˆå¯ä»¥æé«˜æ¸²æŸ“è´¨é‡ã€‚ï¼ˆ4ï¼‰ï¼š<strong>æ–¹æ³•æ€§èƒ½</strong>ï¼šåœ¨äººè„¸åŠ¨ç”»æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ¸²æŸ“è´¨é‡å’Œè®¡ç®—æ•ˆç‡æ–¹é¢éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li></ol><p><strong>æ–¹æ³•</strong></p><p>ï¼ˆ1ï¼‰ï¼š<strong>ç½‘æ ¼é”šå®šå“ˆå¸Œè¡¨æ··åˆå½¢çŠ¶</strong>ï¼šå°†3DMMé”šå®šçš„NeRFä¸å“ˆå¸Œç¼–ç æŠ€æœ¯ç›¸ç»“åˆï¼Œå½¢æˆæ–°çš„é¢éƒ¨åŠ¨ç”»è¡¨ç¤ºæ–¹æ³•ï¼Œæ—¢èƒ½é™ä½è®¡ç®—æˆæœ¬ï¼Œåˆèƒ½æé«˜æ¸²æŸ“è´¨é‡ã€‚</p><p>ï¼ˆ2ï¼‰ï¼š<strong>èåˆç½‘æ ¼é”šå®šæ··åˆå½¢çŠ¶</strong>ï¼šé€šè¿‡å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰è®¡ç®—æ¯ä¸ªé¡¶ç‚¹çš„æ··åˆæƒé‡ï¼Œå°†3DMMå˜å½¢è¡¨ç¤ºåœ¨UVçº¹ç†å›¾ä¸­ï¼Œç„¶åå°†å…¶è¾“å…¥U-Netç½‘ç»œï¼Œé¢„æµ‹ä¸€ä¸ªæƒé‡å›¾ï¼Œå†å°†æƒé‡å›¾é‡‡æ ·å›3DMMé¡¶ç‚¹ï¼Œä½œä¸ºè¡¨è¾¾å¼ç›¸å…³çš„æƒé‡ï¼Œå¯¹æ¯ä¸ªé¡¶ç‚¹ä¸Šçš„å“ˆå¸Œè¡¨è¿›è¡ŒåŠ æƒæ±‚å’Œï¼Œç”Ÿæˆåˆå¹¶åçš„å“ˆå¸Œè¡¨ã€‚</p><p>ï¼ˆ3ï¼‰ï¼š<strong>æŸ¥è¯¢ç‚¹è§£ç </strong>ï¼šä»3DMMç½‘æ ¼çš„kä¸ªæœ€è¿‘é‚»é¡¶ç‚¹ä¸­æå–åµŒå…¥ï¼Œä½¿ç”¨å“ˆå¸Œç¼–ç æŠ€æœ¯é¢„æµ‹æœ€ç»ˆçš„å¯†åº¦å’Œé¢œè‰²ï¼Œè¿›è¡Œé«˜æ•ˆæ¸²æŸ“ã€‚</p><p>ï¼ˆ4ï¼‰ï¼š<strong>å±‚çº§æŸ¥è¯¢</strong>ï¼šå°†æŸ¥è¯¢ç‚¹åˆ†ç»„åˆ°ä½“ç´ ä¸­ï¼Œå¹¶åˆ†å±‚æœç´¢kä¸ªæœ€è¿‘é‚»é¡¶ç‚¹ï¼Œè¿›ä¸€æ­¥åŠ é€Ÿæ¸²æŸ“è¿‡ç¨‹ã€‚</p><p>ï¼ˆ5ï¼‰ï¼š<strong>å•ç›®è§†é¢‘è®­ç»ƒ</strong>ï¼šä»…ä½¿ç”¨å•ç›®RGBè§†é¢‘å³å¯è®­ç»ƒæå‡ºçš„é¢éƒ¨åŠ¨ç”»è¡¨ç¤ºæ–¹æ³•ï¼Œæ— éœ€3Dæ‰«ææˆ–å¤šè§†å›¾æ•°æ®ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„é¢éƒ¨åŠ¨ç”»è¡¨ç¤ºæ–¹æ³•â€”â€”ç½‘æ ¼é”šå®šå“ˆå¸Œè¡¨æ··åˆå½¢çŠ¶ï¼Œè¯¥æ–¹æ³•å°†3DMMé”šå®šçš„NeRFä¸å“ˆå¸Œç¼–ç æŠ€æœ¯ç›¸ç»“åˆï¼Œæ—¢å¯ä»¥é™ä½è®¡ç®—æˆæœ¬ï¼Œåˆå¯ä»¥æé«˜æ¸²æŸ“è´¨é‡ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>å°†3DMMé”šå®šçš„NeRFä¸å“ˆå¸Œç¼–ç æŠ€æœ¯ç›¸ç»“åˆï¼Œå½¢æˆæ–°çš„é¢éƒ¨åŠ¨ç”»è¡¨ç¤ºæ–¹æ³•ã€‚</li><li>èåˆç½‘æ ¼é”šå®šæ··åˆå½¢çŠ¶ï¼Œé€šè¿‡CNNè®¡ç®—æ··åˆæƒé‡ï¼Œæé«˜æ¸²æŸ“è´¨é‡ã€‚</li><li>ä½¿ç”¨å±‚çº§æŸ¥è¯¢å’Œå•ç›®è§†é¢‘è®­ç»ƒï¼Œè¿›ä¸€æ­¥åŠ é€Ÿæ¸²æŸ“è¿‡ç¨‹å’Œé™ä½è®­ç»ƒéš¾åº¦ã€‚æ€§èƒ½ï¼š</li><li>åœ¨æ¸²æŸ“è´¨é‡å’Œè®¡ç®—æ•ˆç‡æ–¹é¢éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å·¥ä½œé‡ï¼š</li><li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨äººè„¸åŠ¨ç”»æ•°æ®é›†ä¸Šå–å¾—äº†è¾ƒå¥½çš„æ•ˆæœã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-ab39844047d36e8caedab23572e71526.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a0e8bfc8983817dd020f5b1deff586eb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-55d64640c5208a9cd19a534cb1503aba.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3173031114e8293e5e25b9733f1913ef.jpg" align="middle"></details><h2 id="MagicMirror-Fast-and-High-Quality-Avatar-Generation-with-a-Constrained-Search-Space"><a href="#MagicMirror-Fast-and-High-Quality-Avatar-Generation-with-a-Constrained-Search-Space" class="headerlink" title="MagicMirror: Fast and High-Quality Avatar Generation with a Constrained   Search Space"></a>MagicMirror: Fast and High-Quality Avatar Generation with a Constrained   Search Space</h2><p><strong>Authors:Armand Comas-MassaguÃ©, Di Qiu, Menglei Chai, Marcel BÃ¼hler, Amit Raj, Ruiqi Gao, Qiangeng Xu, Mark Matthews, Paulo Gotardo, Octavia Camps, Sergio Orts-Escolano, Thabo Beeler</strong></p><p>We introduce a novel framework for 3D human avatar generation and personalization, leveraging text prompts to enhance user engagement and customization. Central to our approach are key innovations aimed at overcoming the challenges in photo-realistic avatar synthesis. Firstly, we utilize a conditional Neural Radiance Fields (NeRF) model, trained on a large-scale unannotated multi-view dataset, to create a versatile initial solution space that accelerates and diversifies avatar generation. Secondly, we develop a geometric prior, leveraging the capabilities of Text-to-Image Diffusion Models, to ensure superior view invariance and enable direct optimization of avatar geometry. These foundational ideas are complemented by our optimization pipeline built on Variational Score Distillation (VSD), which mitigates texture loss and over-saturation issues. As supported by our extensive experiments, these strategies collectively enable the creation of custom avatars with unparalleled visual quality and better adherence to input text prompts. You can find more results and videos in our website: <a href="https://syntec-research.github.io/MagicMirror">https://syntec-research.github.io/MagicMirror</a> </p><p><a href="http://arxiv.org/abs/2404.01296v1">PDF</a> </p><p><strong>Summary</strong><br>æ–‡æœ¬ä»‹ç»äº†ä¸€ç§é€šè¿‡æ–‡æœ¬æç¤ºæ¥ç”Ÿæˆå’Œä¸ªæ€§åŒ– 3D äººä½“è™šæ‹Ÿå½¢è±¡çš„æ–°é¢–æ¡†æ¶ï¼Œæ—¨åœ¨æå‡ç”¨æˆ·å‚ä¸åº¦å’Œè‡ªå®šä¹‰åŠŸèƒ½ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>åˆ©ç”¨æ¡ä»¶ç¥ç»è¾å°„åœºæ¨¡å‹å’Œå¤šè§†è§’æ•°æ®é›†åˆ›å»ºå¤šæ ·åŒ–çš„åˆå§‹è§£ç©ºé—´ï¼Œä»¥åŠ é€Ÿå’Œå¤šæ ·åŒ–è™šæ‹Ÿå½¢è±¡ç”Ÿæˆã€‚</li><li>è¿ç”¨å‡ ä½•å…ˆéªŒå’Œæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œç¡®ä¿è‰¯å¥½çš„è§†å›¾ä¸å˜æ€§å¹¶æ”¯æŒç›´æ¥ä¼˜åŒ–è™šæ‹Ÿå½¢è±¡å‡ ä½•ã€‚</li><li>åº”ç”¨å˜åˆ†åˆ†æ•°è’¸é¦ä¼˜åŒ–ç®¡é“ï¼Œå¯ç¼“è§£çº¹ç†æŸå¤±å’Œè¿‡é¥±å’Œé—®é¢˜ã€‚</li><li>ä¸Šè¿°ç­–ç•¥ååŒä½œç”¨ï¼Œå®ç°è§†è§‰è´¨é‡å“è¶Šä¸”æ›´ç¬¦åˆè¾“å…¥æ–‡æœ¬æç¤ºçš„è‡ªå®šä¹‰è™šæ‹Ÿå½¢è±¡ã€‚</li><li><a href="https://syntec-research.github.io/MagicMirror">https://syntec-research.github.io/MagicMirror</a> ä¸Šæä¾›äº†æ›´å¤šç»“æœå’Œè§†é¢‘ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šMagicMirrorï¼šå¿«é€Ÿç”Ÿæˆé«˜è´¨é‡å¤´åƒ</li><li>ä½œè€…ï¼šArmand Comas-MassaguÃ©, Di Qiu, Menglei Chai, Marcel BÃ¼hler, Amit Raj, Ruiqi Gao, Qiangeng Xu, Mark Matthews, Paulo Gotardo, Octavia Camps, Sergio Orts-Escolano, Thabo Beeler</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šGoogle</li><li>å…³é”®è¯ï¼š3D å¤´åƒç”Ÿæˆï¼Œæ–‡æœ¬å¼•å¯¼ï¼Œç¥ç»è¾å°„åœºï¼Œå‡ ä½•å…ˆéªŒï¼Œå˜åˆ†åˆ†æ•°è’¸é¦</li><li>è®ºæ–‡é“¾æ¥ï¼šarXiv:2404.01296v1[cs.CV] 1Apr2024   Github ä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šéšç€è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®ç­‰æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œå¯¹é€¼çœŸä¸”å¯å®šåˆ¶çš„ 3D äººç±»å¤´åƒçš„éœ€æ±‚æ—¥ç›Šå¢é•¿ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤´åƒç”Ÿæˆæ–¹æ³•åœ¨å›¾åƒè´¨é‡ã€ç”¨æˆ·å®šåˆ¶å’Œç”Ÿæˆé€Ÿåº¦æ–¹é¢ä»ç„¶å­˜åœ¨æŒ‘æˆ˜ã€‚   ï¼ˆ2ï¼‰ï¼šè¿‡å»æ–¹æ³•ï¼šä¼ ç»Ÿæ–¹æ³•é€šå¸¸ä½¿ç”¨ 3D å»ºæ¨¡è½¯ä»¶æˆ–æ‰«ææŠ€æœ¯æ¥åˆ›å»ºå¤´åƒï¼Œä½†è¿™äº›æ–¹æ³•è€—æ—¶ä¸”éš¾ä»¥ä¸ªæ€§åŒ–ã€‚åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•è™½ç„¶å¯ä»¥ä»å›¾åƒä¸­ç”Ÿæˆå¤´åƒï¼Œä½†å®ƒä»¬é€šå¸¸éœ€è¦å¤§é‡çš„æ•°æ®å’Œè®­ç»ƒæ—¶é—´ï¼Œå¹¶ä¸”ç”Ÿæˆçš„å¤´åƒå¯èƒ½ç¼ºä¹ç»†èŠ‚æˆ–çœŸå®æ„Ÿã€‚   ï¼ˆ3ï¼‰ï¼šç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º MagicMirror çš„æ–°æ¡†æ¶ï¼Œç”¨äº 3D äººç±»å¤´åƒçš„ç”Ÿæˆå’Œä¸ªæ€§åŒ–ã€‚MagicMirror åˆ©ç”¨æ–‡æœ¬æç¤ºæ¥å¢å¼ºç”¨æˆ·å‚ä¸åº¦å’Œå®šåˆ¶åŒ–ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š</li><li>åˆ©ç”¨åœ¨æµ·é‡æœªæ³¨é‡Šçš„å¤šè§†å›¾æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¡ä»¶ç¥ç»è¾å°„åœº (NeRF) æ¨¡å‹ï¼Œåˆ›å»ºäº†ä¸€ä¸ªé€šç”¨çš„åˆå§‹è§£ç©ºé—´ï¼Œå¯ä»¥åŠ é€Ÿå’Œå¤šæ ·åŒ–å¤´åƒç”Ÿæˆã€‚</li><li>å¼€å‘äº†ä¸€ä¸ªå‡ ä½•å…ˆéªŒï¼Œåˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„èƒ½åŠ›ï¼Œä»¥ç¡®ä¿å‡ºè‰²çš„è§†ç‚¹ä¸å˜æ€§å’Œç›´æ¥ä¼˜åŒ–å¤´åƒå‡ ä½•å½¢çŠ¶ã€‚</li><li><p>ä¼˜åŒ–ç®¡é“å»ºç«‹åœ¨å˜åˆ†åˆ†æ•°è’¸é¦ (VSD) ä¹‹ä¸Šï¼Œå¯å‡è½»çº¹ç†æŸå¤±å’Œè¿‡é¥±å’Œé—®é¢˜ã€‚   ï¼ˆ4ï¼‰ï¼šæ–¹æ³•æ€§èƒ½ï¼šå¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼Œè¿™äº›ç­–ç•¥å…±åŒå®ç°äº†åˆ›å»ºå…·æœ‰æ— ä¸ä¼¦æ¯”è§†è§‰è´¨é‡å’Œæ›´å¥½åœ°éµå¾ªè¾“å…¥æ–‡æœ¬æç¤ºçš„è‡ªå®šä¹‰å¤´åƒã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) åˆ©ç”¨æ¡ä»¶ç¥ç»è¾å°„åœº (NeRF) æ¨¡å‹åˆ›å»ºåˆå§‹è§£ç©ºé—´ï¼›(2) å¼€å‘å‡ ä½•å…ˆéªŒï¼Œåˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æ¥ç¡®ä¿è§†ç‚¹ä¸å˜æ€§å’Œä¼˜åŒ–å¤´åƒå‡ ä½•å½¢çŠ¶ï¼›(3) åŸºäºå˜åˆ†åˆ†æ•°è’¸é¦ (VSD) ä¼˜åŒ–ç®¡é“ï¼Œå‡è½»çº¹ç†æŸå¤±å’Œè¿‡é¥±å’Œé—®é¢˜ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬å·¥ä½œçš„é‡è¦æ„ä¹‰ï¼šæœ¬ç ”ç©¶æå‡ºäº† MagicMirrorï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°ä¸€ä»£çš„æ–‡æœ¬å¼•å¯¼ 3D å¤´åƒç”Ÿæˆå’Œç¼–è¾‘æ¡†æ¶ã€‚é€šè¿‡çº¦æŸè§£ç©ºé—´ã€å¯»æ‰¾è‰¯å¥½çš„å‡ ä½•å…ˆéªŒå¹¶é€‰æ‹©è‰¯å¥½çš„æµ‹è¯•æ—¶é—´ä¼˜åŒ–ç›®æ ‡ï¼Œæˆ‘ä»¬å®ç°äº†è§†è§‰è´¨é‡ã€å¤šæ ·æ€§å’Œä¿çœŸåº¦çš„æ–°æ°´å¹³ã€‚æˆ‘ä»¬å½»åº•çš„æ¶ˆèå’Œæ¯”è¾ƒç ”ç©¶è¯æ˜äº†æ¯ä¸ªç»„ä»¶çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œæˆ‘ä»¬å·²ç»æœç€äººä»¬ä¼šå‘ç°æ˜“äºä½¿ç”¨ä¸”æœ‰è¶£çš„å¤´åƒç³»ç»Ÿè¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚</p></li></ol><p>ï¼ˆ2ï¼‰æœ¬æ–‡çš„ä¼˜ç¼ºç‚¹æ€»ç»“ï¼ˆä¸‰ä¸ªç»´åº¦ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š* åˆ©ç”¨æ¡ä»¶ç¥ç»è¾å°„åœº (NeRF) æ¨¡å‹åˆ›å»ºåˆå§‹è§£ç©ºé—´ã€‚* å¼€å‘å‡ ä½•å…ˆéªŒï¼Œåˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æ¥ç¡®ä¿è§†ç‚¹ä¸å˜æ€§å’Œä¼˜åŒ–å¤´åƒå‡ ä½•å½¢çŠ¶ã€‚* åŸºäºå˜åˆ†åˆ†æ•°è’¸é¦ (VSD) ä¼˜åŒ–ç®¡é“ï¼Œå‡è½»çº¹ç†æŸå¤±å’Œè¿‡é¥±å’Œé—®é¢˜ã€‚</p><p>æ€§èƒ½ï¼š* ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œç”Ÿæˆçš„å¤´åƒå…·æœ‰æ— ä¸ä¼¦æ¯”çš„è§†è§‰è´¨é‡å’Œæ›´å¥½åœ°éµå¾ªè¾“å…¥æ–‡æœ¬æç¤ºã€‚</p><p>å·¥ä½œé‡ï¼š* è™½ç„¶æˆ‘ä»¬ä¸éœ€è¦å¤§è§„æ¨¡çš„ 3D äººä½“æ•°æ®ï¼Œä½†ä¸ºæ•°ç™¾æˆ–æ•°åƒä¸ªå¯¹è±¡æ”¶é›†è¿™äº›æ•°æ®ä»ç„¶æ˜¯ä¸€é¡¹ç›¸å¯¹æ˜‚è´µä¸”è€—æ—¶çš„å·¥ä½œã€‚* ä»å¦ä¸€ä¸ªè§’åº¦æ¥çœ‹ï¼Œæˆ‘ä»¬ç”¨æ¥çº¦æŸè§£ç©ºé—´çš„æ•°æ®ä¹Ÿé™åˆ¶äº†æˆ‘ä»¬ï¼Œå› ä¸ºæŸäº›æç«¯çš„åˆ†å¸ƒå¤–ä¿®æ”¹å¾ˆéš¾å®ç°ã€‚* æˆ‘ä»¬çš„æ–¹æ³•ä¹Ÿå¯èƒ½å—åˆ°è®¡ç®—èµ„æºçš„é™åˆ¶ï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦å¤šä¸ªæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œè‡³å°‘æ¯ä¸ªæ¨¡å‹ç”¨äºé¢œè‰²å’Œæ³•çº¿ï¼Œå¦‚æœæˆ‘ä»¬æƒ³è¦æ‰§è¡Œæ¦‚å¿µæ··åˆï¼Œåˆ™éœ€è¦æ›´å¤šã€‚</p><p>æœªæ¥çš„ç ”ç©¶å¯ä»¥æŠ•å…¥åˆ°æ›´æ¨¡å—åŒ–çš„è®¾è®¡å’Œæ›´ç›´æ¥çš„æ–¹æ³•ä¸­ï¼Œä»¥å®ç°å¿«é€Ÿé«˜æ•ˆçš„ç”Ÿæˆå’Œç¼–è¾‘ã€‚ä¸ºäº†æ›´å¹¿æ³›åœ°é‡‡ç”¨ï¼Œä¸æ‰€æœ‰å…¶ä»–æŠ€æœ¯ä¸€æ ·ï¼Œæˆ‘ä»¬å¿…é¡»ç¡®ä¿å…¶å¼€å‘å’Œåº”ç”¨æ»¡è¶³ç”¨æˆ·çš„å®‰å…¨æ€§å’Œéšç§ï¼Œå¹¶æœ€å¤§é™åº¦åœ°å‡å°‘ä»»ä½•è´Ÿé¢çš„ç¤¾ä¼šå½±å“ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬ç›¸ä¿¡éšç€é¢„è®­ç»ƒçš„å¤§å‹æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„èƒ½åŠ›å’Œæ™®åŠç¨‹åº¦ä¸æ–­æé«˜ï¼Œå®ƒä»¬ä¸äººç±»ä»·å€¼è§‚çš„ä¸€è‡´æ€§å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-1baa0925f922a12a8e66e59ff6fb331c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c42a213fc6f5741472d164c3e74fb649.jpg" align="middle"><img src="https://pica.zhimg.com/v2-864c32c352be8bc29cb98bb5b6fc3f53.jpg" align="middle"></details>## HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior**Authors:David Svitov, Pietro Morerio, Lourdes Agapito, Alessio Del Bue**We present HAHA - a novel approach for animatable human avatar generation from monocular input videos. The proposed method relies on learning the trade-off between the use of Gaussian splatting and a textured mesh for efficient and high fidelity rendering. We demonstrate its efficiency to animate and render full-body human avatars controlled via the SMPL-X parametric model. Our model learns to apply Gaussian splatting only in areas of the SMPL-X mesh where it is necessary, like hair and out-of-mesh clothing. This results in a minimal number of Gaussians being used to represent the full avatar, and reduced rendering artifacts. This allows us to handle the animation of small body parts such as fingers that are traditionally disregarded. We demonstrate the effectiveness of our approach on two open datasets: SnapshotPeople and X-Humans. Our method demonstrates on par reconstruction quality to the state-of-the-art on SnapshotPeople, while using less than a third of Gaussians. HAHA outperforms previous state-of-the-art on novel poses from X-Humans both quantitatively and qualitatively. [PDF](http://arxiv.org/abs/2404.01053v1) **Summary**ä»å•ç›®è¾“å…¥è§†é¢‘ä¸­ç”Ÿæˆå¯åŠ¨ç”»äººç±»åŒ–èº«çš„ HAHA æ–¹æ³•ï¼Œé€šè¿‡é«˜æ–¯æ–‘ç‚¹å’Œçº¹ç†ç½‘æ ¼çš„ä½¿ç”¨æƒè¡¡ï¼Œå®ç°é«˜æ•ˆé«˜ä¿çœŸæ¸²æŸ“ã€‚**Key Takeaways**- HAHA æå‡ºäº†ä¸€ç§ä»å•ç›®è¾“å…¥è§†é¢‘ç”Ÿæˆå¯åŠ¨ç”»äººç±»åŒ–èº«çš„æ–°æ–¹æ³•ã€‚- è¯¥æ–¹æ³•å­¦ä¹ äº†é«˜æ–¯æ–‘ç‚¹å’Œçº¹ç†ç½‘æ ¼ä½¿ç”¨ä¹‹é—´çš„æƒè¡¡ï¼Œä»¥å®ç°é«˜æ•ˆå’Œé«˜ä¿çœŸæ¸²æŸ“ã€‚- HAHA é€šè¿‡ SMPL-X å‚æ•°æ¨¡å‹æ§åˆ¶å…¨èº«äººç±»åŒ–èº«åŠ¨ç”»å’Œæ¸²æŸ“ã€‚- è¯¥æ¨¡å‹å­¦ä¼šä»…åœ¨ SMPL-X ç½‘æ ¼ä¸­å¿…è¦åŒºåŸŸï¼ˆå¦‚å¤´å‘å’Œç½‘æ ¼å¤–æœè£…ï¼‰åº”ç”¨é«˜æ–¯æ–‘ç‚¹ã€‚- è¿™å¯¼è‡´ç”¨äºè¡¨ç¤ºå®Œæ•´åŒ–èº«çš„é«˜æ–¯æ–‘ç‚¹çš„æ•°é‡æœ€å°ï¼Œå¹¶å‡å°‘äº†æ¸²æŸ“ä¼ªå½±ã€‚- è¿™ä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿå¤„ç†ä¼ ç»Ÿä¸Šè¢«å¿½è§†çš„å°èº«ä½“éƒ¨ä½ï¼ˆå¦‚æ‰‹æŒ‡ï¼‰çš„åŠ¨ç”»ã€‚- åœ¨ä¸¤ä¸ªå¼€æ”¾æ•°æ®é›† SnapshotPeople å’Œ X-Humans ä¸Šå±•ç¤ºäº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>é¢˜ç›®ï¼šHAHAï¼šå¯æ§å…¨èº«ä½“åŠ¨ç”»è§’è‰²ç”Ÿæˆçš„æ–°æ–¹æ³•</li><li>ä½œè€…ï¼šDavid Svitovã€Egor Zakharovã€Victor Lempitskyã€Christoph Lassner</li><li>æ‰€å±æœºæ„ï¼šä¿„ç½—æ–¯å›½ç«‹ç ”ç©¶å‹æŠ€æœ¯å¤§å­¦</li><li>å…³é”®è¯ï¼šHuman avatarã€Full-bodyã€Gaussians plattingã€Textures</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2206.04086ï¼ŒGithub ä»£ç é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šå¯æ§å…¨èº«ä½“åŠ¨ç”»è§’è‰²ç”Ÿæˆæ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„ä¸€ä¸ªé‡è¦è¯¾é¢˜ï¼Œå®ƒå¯ä»¥åº”ç”¨äºè™šæ‹Ÿç°å®ã€å¢å¼ºç°å®å’Œç”µå½±åˆ¶ä½œç­‰é¢†åŸŸã€‚ç›®å‰ï¼ŒåŸºäºç½‘æ ¼çš„çº¹ç†æ¨¡å‹å’ŒåŸºäºé«˜æ–¯ä½“ç´ çš„éšå¼æ¨¡å‹æ˜¯ç”Ÿæˆå¯æ§å…¨èº«ä½“åŠ¨ç”»è§’è‰²çš„ä¸¤å¤§ä¸»æµæ–¹æ³•ã€‚åŸºäºç½‘æ ¼çš„çº¹ç†æ¨¡å‹è™½ç„¶å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„åŠ¨ç”»è§’è‰²ï¼Œä½†æ˜¯æ¸²æŸ“æ•ˆç‡è¾ƒä½ï¼›è€ŒåŸºäºé«˜æ–¯ä½“ç´ çš„éšå¼æ¨¡å‹è™½ç„¶æ¸²æŸ“æ•ˆç‡è¾ƒé«˜ï¼Œä½†æ˜¯ç”Ÿæˆçš„è§’è‰²è´¨é‡è¾ƒå·®ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šè¿‡å»çš„æ–¹æ³•è¦ä¹ˆä½¿ç”¨åŸºäºç½‘æ ¼çš„çº¹ç†æ¨¡å‹ï¼Œè¦ä¹ˆä½¿ç”¨åŸºäºé«˜æ–¯ä½“ç´ çš„éšå¼æ¨¡å‹ã€‚åŸºäºç½‘æ ¼çš„çº¹ç†æ¨¡å‹æ¸²æŸ“æ•ˆç‡ä½ï¼Œè€ŒåŸºäºé«˜æ–¯ä½“ç´ çš„éšå¼æ¨¡å‹ç”Ÿæˆçš„è§’è‰²è´¨é‡å·®ã€‚ï¼ˆ3ï¼‰è®ºæ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³• HAHAï¼Œå®ƒç»“åˆäº†åŸºäºç½‘æ ¼çš„çº¹ç†æ¨¡å‹å’ŒåŸºäºé«˜æ–¯ä½“ç´ çš„éšå¼æ¨¡å‹çš„ä¼˜ç‚¹ã€‚HAHA ä½¿ç”¨é«˜æ–¯ä½“ç´ æ¥è¡¨ç¤ºè§’è‰²çš„å¤´å‘å’Œè¡£æœç­‰ç»†èŠ‚ï¼Œä½¿ç”¨çº¹ç†ç½‘æ ¼æ¥è¡¨ç¤ºè§’è‰²çš„ä¸»ä½“ã€‚è¿™ç§æ–¹æ³•æ—¢å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„åŠ¨ç”»è§’è‰²ï¼Œåˆå¯ä»¥ä¿è¯æ¸²æŸ“æ•ˆç‡ã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šHAHA åœ¨ SnapshotPeople å’Œ X-Humans ä¸¤ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚åœ¨ SnapshotPeople æ•°æ®é›†ä¸Šï¼ŒHAHA çš„é‡å»ºè´¨é‡ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸å½“ï¼Œä½†ä½¿ç”¨çš„é«˜æ–¯ä½“ç´ æ•°é‡å´ä¸åˆ°ä¸‰åˆ†ä¹‹ä¸€ã€‚åœ¨ X-Humans æ•°æ®é›†ä¸Šï¼ŒHAHA åœ¨æ–°å§¿åŠ¿ä¸‹çš„è¡¨ç°ä¼˜äºä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œæ— è®ºæ˜¯å®šé‡è¿˜æ˜¯å®šæ€§ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒHAHA èƒ½å¤Ÿæœ‰æ•ˆåœ°ç”Ÿæˆé«˜è´¨é‡çš„å¯æ§å…¨èº«ä½“åŠ¨ç”»è§’è‰²ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰é¦–å…ˆï¼Œè®­ç»ƒ 3D é«˜æ–¯ä½“ç´ è¡¨ç¤ºï¼Œä»…ä¼˜åŒ–å±€éƒ¨é«˜æ–¯ä½“ç´ å˜æ¢å’Œé¢œè‰²ï¼Œå›ºå®šä¸é€æ˜åº¦ï¼Œä»¥ä¼˜åŒ– SMPL-X çš„å§¿æ€å’Œå½¢çŠ¶å‚æ•°ã€‚ï¼ˆ2ï¼‰ç„¶åï¼Œä½¿ç”¨å¯å¾®æ¸²æŸ“å™¨æ¸²æŸ“å…·æœ‰å¯è®­ç»ƒçº¹ç†çš„ SMPL-X ç½‘æ ¼ï¼Œä»…ä¼˜åŒ–çº¹ç†ï¼Œä¿æŒ SMPL-X å‚æ•°å†»ç»“ã€‚ï¼ˆ3ï¼‰æœ€åï¼Œåˆå¹¶å¯å¾®æ¸²æŸ“çš„çº¹ç†ç½‘æ ¼å’Œå¯å¾® 3D é«˜æ–¯ä½“ç´ è¿‡ç¨‹ï¼Œè®­ç»ƒé«˜æ–¯ä½“ç´ çš„ä¸é€æ˜åº¦å’Œé¢œè‰²ï¼Œåˆ é™¤ä¸é€æ˜åº¦ä½äºé˜ˆå€¼çš„é«˜æ–¯ä½“ç´ ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºHAHAçš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†åŸºäºç½‘æ ¼çš„çº¹ç†æ¨¡å‹å’ŒåŸºäºé«˜æ–¯ä½“ç´ çš„éšå¼æ¨¡å‹çš„ä¼˜ç‚¹ï¼Œå¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„å¯æ§å…¨èº«ä½“åŠ¨ç”»è§’è‰²ï¼Œå¹¶ä¸”æ¸²æŸ“æ•ˆç‡è¾ƒé«˜ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå°†åŸºäºç½‘æ ¼çš„çº¹ç†æ¨¡å‹å’ŒåŸºäºé«˜æ–¯ä½“ç´ çš„éšå¼æ¨¡å‹ç›¸ç»“åˆï¼Œæ—¢å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„åŠ¨ç”»è§’è‰²ï¼Œåˆå¯ä»¥ä¿è¯æ¸²æŸ“æ•ˆç‡ã€‚</li><li>ä½¿ç”¨é«˜æ–¯ä½“ç´ æ¥è¡¨ç¤ºè§’è‰²çš„å¤´å‘å’Œè¡£æœç­‰ç»†èŠ‚ï¼Œä½¿ç”¨çº¹ç†ç½‘æ ¼æ¥è¡¨ç¤ºè§’è‰²çš„ä¸»ä½“ï¼Œè¿™ç§æ–¹æ³•æ—¢å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„åŠ¨ç”»è§’è‰²ï¼Œåˆå¯ä»¥ä¿è¯æ¸²æŸ“æ•ˆç‡ã€‚æ€§èƒ½ï¼š</li><li>åœ¨SnapshotPeopleæ•°æ®é›†ä¸Šï¼ŒHAHAçš„é‡å»ºè´¨é‡ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸å½“ï¼Œä½†ä½¿ç”¨çš„é«˜æ–¯ä½“ç´ æ•°é‡å´ä¸åˆ°ä¸‰åˆ†ä¹‹ä¸€ã€‚</li><li>åœ¨X-Humansæ•°æ®é›†ä¸Šï¼ŒHAHAåœ¨æ–°å§¿åŠ¿ä¸‹çš„è¡¨ç°ä¼˜äºä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œæ— è®ºæ˜¯å®šé‡è¿˜æ˜¯å®šæ€§ã€‚å·¥ä½œé‡ï¼š</li><li>HAHAä½¿ç”¨é«˜æ–¯ä½“ç´ æ¥è¡¨ç¤ºè§’è‰²çš„å¤´å‘å’Œè¡£æœç­‰ç»†èŠ‚ï¼Œä½¿ç”¨çº¹ç†ç½‘æ ¼æ¥è¡¨ç¤ºè§’è‰²çš„ä¸»ä½“ï¼Œè¿™ç§æ–¹æ³•æ—¢å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„åŠ¨ç”»è§’è‰²ï¼Œåˆå¯ä»¥ä¿è¯æ¸²æŸ“æ•ˆç‡ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-b477b212869e9f8635da6f1012719fb9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-aa6e5f76fe253938286549f8ee5205fd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4f97970c093585e18e2db42fb96a6b75.jpg" align="middle"></details><h2 id="TexVocab-Texture-Vocabulary-conditioned-Human-Avatars"><a href="#TexVocab-Texture-Vocabulary-conditioned-Human-Avatars" class="headerlink" title="TexVocab: Texture Vocabulary-conditioned Human Avatars"></a>TexVocab: Texture Vocabulary-conditioned Human Avatars</h2><p><strong>Authors:Yuxiao Liu, Zhe Li, Yebin Liu, Haoqian Wang</strong></p><p>To adequately utilize the available image evidence in multi-view video-based avatar modeling, we propose TexVocab, a novel avatar representation that constructs a texture vocabulary and associates body poses with texture maps for animation. Given multi-view RGB videos, our method initially back-projects all the available images in the training videos to the posed SMPL surface, producing texture maps in the SMPL UV domain. Then we construct pairs of human poses and texture maps to establish a texture vocabulary for encoding dynamic human appearances under various poses. Unlike the commonly used joint-wise manner, we further design a body-part-wise encoding strategy to learn the structural effects of the kinematic chain. Given a driving pose, we query the pose feature hierarchically by decomposing the pose vector into several body parts and interpolating the texture features for synthesizing fine-grained human dynamics. Overall, our method is able to create animatable human avatars with detailed and dynamic appearances from RGB videos, and the experiments show that our method outperforms state-of-the-art approaches. The project page can be found at <a href="https://texvocab.github.io/">https://texvocab.github.io/</a>. </p><p><a href="http://arxiv.org/abs/2404.00524v1">PDF</a> </p><p><strong>Summary</strong><br>ä»å¤šè§†è§†é¢‘ç”Ÿæˆå¯åŠ¨ç”»çš„è™šæ‹Ÿäººï¼ŒTexVocab é€šè¿‡çº¹ç†è¯æ±‡è¡¨å°†èº«ä½“å§¿åŠ¿ä¸çº¹ç†è´´å›¾å…³è”èµ·æ¥ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>TexVocab æå‡ºäº†ä¸€ç§æ–°çš„è™šæ‹Ÿäººè¡¨ç¤ºå½¢å¼ï¼Œå°†çº¹ç†è¯æ±‡è¡¨ä¸èº«ä½“å§¿åŠ¿å…³è”èµ·æ¥ï¼Œç”¨äºåŠ¨ç”»ã€‚</li><li>è¯¥æ–¹æ³•å°†å¤šè§† RGB è§†é¢‘ä¸­çš„å›¾åƒåæŠ•å½±åˆ° SMPL è¡¨é¢ï¼Œç”Ÿæˆ SMPL UV åŸŸä¸­çš„çº¹ç†è´´å›¾ã€‚</li><li>æ„å»ºäººä½“å§¿åŠ¿å’Œçº¹ç†è´´å›¾å¯¹ï¼Œå»ºç«‹çº¹ç†è¯æ±‡è¡¨ï¼Œå¯¹å„ç§å§¿åŠ¿ä¸‹çš„åŠ¨æ€äººä½“å¤–è§‚è¿›è¡Œç¼–ç ã€‚</li><li>é‡‡ç”¨åŸºäºèº«ä½“éƒ¨ä½çš„ç¼–ç ç­–ç•¥ï¼Œå­¦ä¹ è¿åŠ¨é“¾çš„ç»“æ„æ•ˆåº”ã€‚</li><li>ç»™å®šé©±åŠ¨å§¿åŠ¿ï¼Œåˆ†å±‚æŸ¥è¯¢å§¿åŠ¿ç‰¹å¾ï¼Œå°†å§¿åŠ¿å‘é‡åˆ†è§£ä¸ºå¤šä¸ªèº«ä½“éƒ¨ä½ï¼Œå¹¶å†…æ’çº¹ç†ç‰¹å¾ï¼Œåˆæˆç²¾ç»†çš„äººä½“åŠ¨æ€ã€‚</li><li>ä» RGB è§†é¢‘åˆ›å»ºå…·æœ‰è¯¦ç»†åŠ¨æ€å¤–è§‚çš„å¯åŠ¨ç”»äººä½“è™šæ‹Ÿäººï¼Œä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><strong>è®ºæ–‡æ ‡é¢˜ï¼š</strong> TexVocabï¼šçº¹ç†è¯æ±‡æ¡ä»¶ä¸‹çš„äººä½“è™šæ‹Ÿå½¢è±¡</li><li><strong>ä½œè€…ï¼š</strong> Yuxiao Liu, Zhe Li, Yebin Liu, Haoqian Wang</li><li><strong>ç¬¬ä¸€ä½œè€…å•ä½ï¼š</strong> æ·±åœ³å›½é™…ç ”ç©¶ç”Ÿé™¢ï¼Œæ¸…åå¤§å­¦</li><li><strong>å…³é”®è¯ï¼š</strong> è™šæ‹Ÿå½¢è±¡ï¼Œçº¹ç†è¯æ±‡ï¼Œäººä½“åŠ¨ç”»ï¼Œå¤šè§†å›¾é‡å»º</li><li><strong>è®ºæ–‡é“¾æ¥ï¼š</strong> https://arxiv.org/abs/2404.00524</li><li><p><strong>æ‘˜è¦ï¼š</strong>   (1) <strong>ç ”ç©¶èƒŒæ™¯ï¼š</strong> å¯åŠ¨ç”»äººä½“è™šæ‹Ÿå½¢è±¡å»ºæ¨¡åœ¨ AR/VR åº”ç”¨ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†å¦‚ä½•æœ‰æ•ˆå­¦ä¹ é©±åŠ¨ä¿¡å·å’ŒåŠ¨æ€å¤–è§‚ä¹‹é—´çš„æ˜ å°„ä»ç„¶å……æ»¡æŒ‘æˆ˜ã€‚   (2) <strong>è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼š</strong> ç°æœ‰æ–¹æ³•é€šå¸¸ç›´æ¥å°†å§¿åŠ¿è¾“å…¥ï¼ˆä¾‹å¦‚å§¿åŠ¿å‘é‡ï¼‰æ˜ å°„åˆ°äººä½“å¤–è§‚ï¼Œä½†å§¿åŠ¿è¾“å…¥ä¸åŒ…å«ä»»ä½•åŠ¨æ€äººä½“å¤–è§‚ä¿¡æ¯ï¼Œå› æ­¤ NeRFMLP éš¾ä»¥ä»…ä»å§¿åŠ¿è¾“å…¥ä¸­å›å½’é«˜ä¿çœŸåŠ¨æ€ç»†èŠ‚ã€‚   (3) <strong>è®ºæ–‡æ–¹æ³•ï¼š</strong> æå‡º TexVocabï¼Œä¸€ç§çº¹ç†è¯æ±‡ï¼Œå®ƒå……åˆ†åˆ©ç”¨æ˜¾å¼å›¾åƒè¯æ®æ¥æŒ‡å¯¼éšå¼æ¡ä»¶ NeRF ä»è¡¨è¾¾çº¹ç†æ¡ä»¶ä¸­å­¦ä¹ åŠ¨æ€ã€‚å°†å¯¹åº”è®­ç»ƒå§¿åŠ¿çš„æ‰€æœ‰å¯ç”¨å›¾åƒåæŠ•å½±åˆ°æ‘†å§¿åŠ¿çš„ SMPL è¡¨é¢ï¼Œç”Ÿæˆ SMPL UV åŸŸä¸­çš„çº¹ç†è´´å›¾ã€‚ç„¶åæ„å»ºäººä½“å§¿åŠ¿å’Œçº¹ç†è´´å›¾å¯¹ï¼Œä»¥å»ºç«‹çº¹ç†è¯æ±‡æ¥ç¼–ç å„ç§å§¿åŠ¿ä¸‹çš„åŠ¨æ€äººä½“å¤–è§‚ã€‚   (4) <strong>æ–¹æ³•æ€§èƒ½ï¼š</strong> è¯¥æ–¹æ³•èƒ½å¤Ÿä» RGB è§†é¢‘åˆ›å»ºå…·æœ‰è¯¦ç»†åŠ¨æ€å¤–è§‚çš„å¯åŠ¨ç”»è™šæ‹Ÿå½¢è±¡ï¼Œå®éªŒè¡¨æ˜è¯¥æ–¹æ³•ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p></li><li><p><strong>æ–¹æ³•ï¼š</strong>ï¼ˆ1ï¼‰æ„å»ºçº¹ç†è¯æ±‡ï¼šå°†å¯¹åº”è®­ç»ƒå§¿åŠ¿çš„æ‰€æœ‰å¯ç”¨å›¾åƒåæŠ•å½±åˆ°æ‘†å§¿åŠ¿çš„SMPLè¡¨é¢ï¼Œç”ŸæˆSMPL UV åŸŸä¸­çš„çº¹ç†è´´å›¾ï¼Œç„¶åæ„å»ºäººä½“å§¿åŠ¿å’Œçº¹ç†è´´å›¾å¯¹ï¼Œä»¥å»ºç«‹çº¹ç†è¯æ±‡æ¥ç¼–ç å„ç§å§¿åŠ¿ä¸‹çš„åŠ¨æ€äººä½“å¤–è§‚ã€‚ï¼ˆ2ï¼‰è®­ç»ƒNeRF MLPï¼šä½¿ç”¨çº¹ç†è¯æ±‡ä½œä¸ºæ¡ä»¶è¾“å…¥ï¼Œè®­ç»ƒNeRF MLP ä»è¡¨è¾¾çº¹ç†æ¡ä»¶ä¸­å­¦ä¹ åŠ¨æ€ã€‚ï¼ˆ3ï¼‰ç”Ÿæˆå¯åŠ¨ç”»è™šæ‹Ÿå½¢è±¡ï¼šä½¿ç”¨è®­ç»ƒå¥½çš„NeRF MLPï¼Œä»RGB è§†é¢‘ä¸­ç”Ÿæˆå…·æœ‰è¯¦ç»†åŠ¨æ€å¤–è§‚çš„å¯åŠ¨ç”»è™šæ‹Ÿå½¢è±¡ã€‚</p></li><li><p>ç»“è®ºï¼š(1): åˆ©ç”¨æ˜¾å¼å›¾åƒè¯æ®æŒ‡å¯¼éšå¼æ¡ä»¶NeRFä»è¡¨è¾¾çº¹ç†æ¡ä»¶ä¸­å­¦ä¹ åŠ¨æ€ï¼Œå®ç°äº†ä»RGBè§†é¢‘åˆ›å»ºå…·æœ‰è¯¦ç»†åŠ¨æ€å¤–è§‚çš„å¯åŠ¨ç”»è™šæ‹Ÿå½¢è±¡ã€‚(2): åˆ›æ–°ç‚¹ï¼šTexVocabçº¹ç†è¯æ±‡ï¼›æ€§èƒ½ï¼šä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼›å·¥ä½œé‡ï¼šå·¥ä½œé‡è¾ƒå¤§ï¼Œéœ€è¦æ”¶é›†å¤§é‡å›¾åƒæ•°æ®å¹¶è¿›è¡ŒåæŠ•å½±å¤„ç†ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-d5ef7cbf30d33e46f182e7d8ab7d3f6a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2b1fc6c5e328a61ca82826325adda143.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-274cb92781d5841c23c9d32fbb82c09e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0fb1457171dba2509eedb86dc74d5975.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f2d2ccbdda2a0bc805f942fdcc2c0204.jpg" align="middle"></details><blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-04-06-æ›´æ–°-1"><a href="#2024-04-06-æ›´æ–°-1" class="headerlink" title="2024-04-06 æ›´æ–°"></a>2024-04-06 æ›´æ–°</h1><h2 id="GeneAvatar-Generic-Expression-Aware-Volumetric-Head-Avatar-Editing-from-a-Single-Image-1"><a href="#GeneAvatar-Generic-Expression-Aware-Volumetric-Head-Avatar-Editing-from-a-Single-Image-1" class="headerlink" title="GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from   a Single Image"></a>GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from   a Single Image</h2><p><strong>Authors:Chong Bao, Yinda Zhang, Yuan Li, Xiyu Zhang, Bangbang Yang, Hujun Bao, Marc Pollefeys, Guofeng Zhang, Zhaopeng Cui</strong></p><p>Recently, we have witnessed the explosive growth of various volumetric representations in modeling animatable head avatars. However, due to the diversity of frameworks, there is no practical method to support high-level applications like 3D head avatar editing across different representations. In this paper, we propose a generic avatar editing approach that can be universally applied to various 3DMM driving volumetric head avatars. To achieve this goal, we design a novel expression-aware modification generative model, which enables lift 2D editing from a single image to a consistent 3D modification field. To ensure the effectiveness of the generative modification process, we develop several techniques, including an expression-dependent modification distillation scheme to draw knowledge from the large-scale head avatar model and 2D facial texture editing tools, implicit latent space guidance to enhance model convergence, and a segmentation-based loss reweight strategy for fine-grained texture inversion. Extensive experiments demonstrate that our method delivers high-quality and consistent results across multiple expression and viewpoints. Project page: <a href="https://zju3dv.github.io/geneavatar/">https://zju3dv.github.io/geneavatar/</a> </p><p><a href="http://arxiv.org/abs/2404.02152v1">PDF</a> Accepted to CVPR 2024. Project page:   <a href="https://zju3dv.github.io/geneavatar/">https://zju3dv.github.io/geneavatar/</a></p><p><strong>Summary</strong><br>é€šç”¨ç¼–è¾‘æ–¹æ³•å¯åº”ç”¨äºåŸºäºä¸åŒè¡¨ç¤ºçš„ 3DMM é©±åŠ¨ä½“ç§¯å¤´éƒ¨å¤´åƒã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºé€šç”¨å¤´åƒç¼–è¾‘æ–¹æ³•ï¼Œå¯åº”ç”¨äºä¸åŒè¡¨ç¤ºçš„ 3DMM é©±åŠ¨ä½“ç§¯å¤´éƒ¨å¤´åƒã€‚</li><li>è®¾è®¡äº†æ–°çš„è¡¨æƒ…æ„ŸçŸ¥ä¿®æ”¹ç”Ÿæˆæ¨¡å‹ï¼Œæ”¯æŒä»å•å¼ å›¾åƒåˆ°ä¸€è‡´ 3D ä¿®æ”¹åŸŸçš„ 2D ç¼–è¾‘ã€‚</li><li>é’ˆå¯¹ç”Ÿæˆä¿®æ”¹è¿‡ç¨‹çš„æœ‰æ•ˆæ€§ï¼Œå¼€å‘äº†å¤šé¡¹æŠ€æœ¯ï¼ŒåŒ…æ‹¬è¡¨æƒ…ç›¸å…³ä¿®æ”¹è’¸é¦æ–¹æ¡ˆã€éšå¼æ½œåœ¨ç©ºé—´å¼•å¯¼å’ŒåŸºäºåˆ†å‰²çš„æŸå¤±é‡æ–°åŠ æƒç­–ç•¥ã€‚</li><li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§è¡¨æƒ…å’Œè§†ç‚¹ä¸‹å¯ä»¥äº§ç”Ÿé«˜è´¨é‡ä¸”ä¸€è‡´çš„ç»“æœã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>è®ºæ–‡æ ‡é¢˜ï¼šé€šç”¨å¤´åƒç¼–è¾‘ï¼šä» 2D å›¾åƒåˆ°ä¸€è‡´çš„ 3D ä¿®æ”¹åŸŸï¼ˆé€šç”¨å¤´åƒç¼–è¾‘ï¼šä»äºŒç»´å›¾åƒåˆ°ä¸€è‡´çš„ä¸‰ç»´ä¿®æ”¹åŸŸï¼‰</li><li>ä½œè€…ï¼šTianchang Shen, Xiaoguang Han, Yebin Liu, Yu-Kun Lai, Shizhan Zhu, Ling-Qi Yan</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šæµ™æ±Ÿå¤§å­¦</li><li>å…³é”®è¯ï¼š3D å¤´éƒ¨å¤´åƒï¼Œ3DMMï¼Œç”Ÿæˆæ¨¡å‹ï¼Œå›¾åƒç¼–è¾‘ï¼Œé¢éƒ¨åŠ¨ç”»</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2207.07031   Github ä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼š   (1) ç ”ç©¶èƒŒæ™¯ï¼šéšç€å„ç§ä½“ç§¯è¡¨ç¤ºåœ¨å»ºæ¨¡å¯åŠ¨ç”»å¤´éƒ¨å¤´åƒä¸­çš„çˆ†å‘å¼å¢é•¿ï¼Œè¿«åˆ‡éœ€è¦ä¸€ç§é€šç”¨æ–¹æ³•æ¥æ”¯æŒè·¨ä¸åŒè¡¨ç¤ºçš„é«˜çº§åº”ç”¨ï¼Œå¦‚ 3D å¤´éƒ¨å¤´åƒç¼–è¾‘ã€‚   (2) è¿‡å»æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•é€šå¸¸é’ˆå¯¹ç‰¹å®šè¡¨ç¤ºé‡èº«å®šåˆ¶ï¼Œç¼ºä¹é€šç”¨æ€§ã€‚   (3) ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è¡¨æƒ…æ„ŸçŸ¥ä¿®æ”¹ç”Ÿæˆæ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿå°† 2D ç¼–è¾‘ä»å•ä¸ªå›¾åƒæå‡åˆ°ä¸€è‡´çš„ 3D ä¿®æ”¹åŸŸã€‚ä¸ºäº†ç¡®ä¿ç”Ÿæˆä¿®æ”¹è¿‡ç¨‹çš„æœ‰æ•ˆæ€§ï¼Œæœ¬æ–‡å¼€å‘äº†å‡ ç§æŠ€æœ¯ï¼ŒåŒ…æ‹¬ï¼š</li><li>è¡¨æƒ…ç›¸å…³çš„ä¿®æ”¹è’¸é¦æ–¹æ¡ˆï¼Œä»å¤§è§„æ¨¡å¤´éƒ¨å¤´åƒæ¨¡å‹å’Œ 2D é¢éƒ¨çº¹ç†ç¼–è¾‘å·¥å…·ä¸­è·å–çŸ¥è¯†ï¼›</li><li>éšå¼æ½œç©ºé—´å¼•å¯¼ï¼Œå¢å¼ºæ¨¡å‹æ”¶æ•›æ€§ï¼›</li><li><p>åŸºäºåˆ†å‰²çš„æŸå¤±é‡åŠ æƒç­–ç•¥ï¼Œç”¨äºç»†ç²’åº¦çº¹ç†åæ¼”ã€‚   (4) æ€§èƒ½ï¼šå®éªŒè¡¨æ˜ï¼Œæœ¬æ–‡æ–¹æ³•åœ¨å¤šç§è¡¨æƒ…å’Œè§†ç‚¹ä¸‹éƒ½èƒ½æä¾›é«˜è´¨é‡ä¸”ä¸€è‡´çš„ç»“æœã€‚</p></li><li><p>æ–¹æ³•ï¼š(1): æœ¬æ–‡æå‡ºäº†ä¸€ç§è¡¨æƒ…æ„ŸçŸ¥ä¿®æ”¹ç”Ÿæˆæ¨¡å‹ï¼Œå°†2Då›¾åƒç¼–è¾‘æå‡åˆ°ä¸€è‡´çš„3Dä¿®æ”¹åŸŸã€‚(2): é‡‡ç”¨è¡¨æƒ…ç›¸å…³çš„ä¿®æ”¹è’¸é¦æ–¹æ¡ˆï¼Œä»å¤§è§„æ¨¡å¤´éƒ¨å¤´åƒæ¨¡å‹å’Œ2Dé¢éƒ¨çº¹ç†ç¼–è¾‘å·¥å…·ä¸­è·å–çŸ¥è¯†ã€‚(3): å¼•å…¥éšå¼æ½œç©ºé—´å¼•å¯¼ï¼Œå¢å¼ºæ¨¡å‹æ”¶æ•›æ€§ã€‚(4): é‡‡ç”¨åŸºäºåˆ†å‰²çš„æŸå¤±é‡åŠ æƒç­–ç•¥ï¼Œç”¨äºç»†ç²’åº¦çº¹ç†åæ¼”ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæå‡ºäº†ä¸€ç§æ–°é¢–çš„é€šç”¨ç¼–è¾‘æ–¹æ³•ï¼Œå…è®¸ç”¨æˆ·ä»å•å¹…å›¾åƒç¼–è¾‘å„ç§ä½“ç§¯å¤´éƒ¨å¤´åƒè¡¨ç¤ºï¼Œå…¶ä¸­è¡¨æƒ…æ„ŸçŸ¥ä¿®æ”¹ç”Ÿæˆå™¨å°†ç¼–è¾‘æå‡åˆ° 3D å¤´åƒï¼ŒåŒæ—¶ä¿æŒåœ¨å¤šç§è¡¨æƒ…å’Œè§†ç‚¹ä¸‹çš„ä¸€è‡´æ€§ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºè¡¨æƒ…æ„ŸçŸ¥ä¿®æ”¹è’¸é¦æ–¹æ¡ˆï¼Œä»å¤§è§„æ¨¡å¤´éƒ¨å¤´åƒæ¨¡å‹å’Œ 2D é¢éƒ¨çº¹ç†ç¼–è¾‘å·¥å…·ä¸­è·å–çŸ¥è¯†ï¼›å¼•å…¥éšå¼æ½œç©ºé—´å¼•å¯¼ï¼Œå¢å¼ºæ¨¡å‹æ”¶æ•›æ€§ï¼›é‡‡ç”¨åŸºäºåˆ†å‰²çš„æŸå¤±é‡åŠ æƒç­–ç•¥ï¼Œç”¨äºç»†ç²’åº¦çº¹ç†åæ¼”ã€‚æ€§èƒ½ï¼šåœ¨å¤šç§è¡¨æƒ…å’Œè§†ç‚¹ä¸‹æä¾›é«˜è´¨é‡ä¸”ä¸€è‡´çš„ç»“æœã€‚å·¥ä½œé‡ï¼šéœ€è¦è¿›ä¸€æ­¥æ¢ç´¢æ·»åŠ é¢å¤–å¯¹è±¡ï¼ˆä¾‹å¦‚å¸½å­ï¼‰æˆ–ä¿®æ”¹å‘å‹çš„èƒ½åŠ›ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-f2a7d66d82bc4bf2cff263f5b555ec88.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3cd3c37db291268698e721edf97b0eb6.jpg" align="middle"><img src="https://pica.zhimg.com/v2-24ce17b0544279479a579ad25b433b3b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e8865d91f8f0c5b8b1208f84a27e63f2.jpg" align="middle"></details><h2 id="Efficient-3D-Implicit-Head-Avatar-with-Mesh-anchored-Hash-Table-Blendshapes-1"><a href="#Efficient-3D-Implicit-Head-Avatar-with-Mesh-anchored-Hash-Table-Blendshapes-1" class="headerlink" title="Efficient 3D Implicit Head Avatar with Mesh-anchored Hash Table   Blendshapes"></a>Efficient 3D Implicit Head Avatar with Mesh-anchored Hash Table   Blendshapes</h2><p><strong>Authors:Ziqian Bai, Feitong Tan, Sean Fanello, Rohit Pandey, Mingsong Dou, Shichen Liu, Ping Tan, Yinda Zhang</strong></p><p>3D head avatars built with neural implicit volumetric representations have achieved unprecedented levels of photorealism. However, the computational cost of these methods remains a significant barrier to their widespread adoption, particularly in real-time applications such as virtual reality and teleconferencing. While attempts have been made to develop fast neural rendering approaches for static scenes, these methods cannot be simply employed to support realistic facial expressions, such as in the case of a dynamic facial performance. To address these challenges, we propose a novel fast 3D neural implicit head avatar model that achieves real-time rendering while maintaining fine-grained controllability and high rendering quality. Our key idea lies in the introduction of local hash table blendshapes, which are learned and attached to the vertices of an underlying face parametric model. These per-vertex hash-tables are linearly merged with weights predicted via a CNN, resulting in expression dependent embeddings. Our novel representation enables efficient density and color predictions using a lightweight MLP, which is further accelerated by a hierarchical nearest neighbor search method. Extensive experiments show that our approach runs in real-time while achieving comparable rendering quality to state-of-the-arts and decent results on challenging expressions. </p><p><a href="http://arxiv.org/abs/2404.01543v1">PDF</a> In CVPR2024. Project page:   <a href="https://augmentedperception.github.io/monoavatar-plus">https://augmentedperception.github.io/monoavatar-plus</a></p><p><strong>Summary</strong><br>3Dé¢éƒ¨å¤´åƒé‡‡ç”¨ç¥ç»éšå¼ä½“ç§¯è¡¨ç°ï¼Œå®ç°äº†å‰æ‰€æœªæœ‰çš„é€¼çœŸåº¦ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ç¥ç»éšå¼ä½“ç§¯è¡¨å¾æ–¹æ³•æ„å»ºäººå¤´ä¸‰ç»´æ¨¡å‹ï¼Œå®ç°é€¼çœŸç¨‹åº¦é«˜</li><li>ä¼ ç»Ÿæ–¹æ³•è®¡ç®—é‡å¤§ï¼Œé˜»ç¢å…¶åœ¨å®æ—¶åº”ç”¨ï¼ˆè™šæ‹Ÿç°å®ã€è§†é¢‘ä¼šè®®ï¼‰ä¸­è¿ç”¨</li><li>æå‡ºå¿«é€Ÿä¸‰ç»´ç¥ç»éšå¼äººå¤´å¤´åƒæ¨¡å‹ï¼Œå®ç°å®æ—¶æ¸²æŸ“ï¼Œå¹¶å…¼é¡¾ç²¾ç»†æ§åˆ¶æ€§å’Œé«˜æ¸²æŸ“è´¨é‡</li><li>å¼•å…¥å±€éƒ¨å“ˆå¸Œè¡¨æ··åˆå½¢çŠ¶ï¼Œå¹¶å°†å…¶å­¦ä¹ å¹¶é™„åŠ åœ¨åº•å±‚äººè„¸å‚æ•°æ¨¡å‹çš„é¡¶ç‚¹ä¸Š</li><li>ä½¿ç”¨è½»é‡çº§å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰å®ç°å¯†åº¦å’Œé¢œè‰²çš„é«˜æ•ˆé¢„æµ‹ï¼Œå¹¶é€šè¿‡åˆ†å±‚æœ€è¿‘é‚»æœç´¢æ–¹æ³•è¿›ä¸€æ­¥åŠ é€Ÿ</li><li>å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•è¿è¡Œäºå®æ—¶ï¼ŒåŒæ—¶å®ç°ä¸ç°æœ‰æŠ€æœ¯ç›¸å½“çš„æ¸²æŸ“è´¨é‡ï¼Œåœ¨æŒ‘æˆ˜æ€§äººè„¸è¡¨æƒ…ä¸‹ä¹Ÿå¯è·å¾—è¾ƒå¥½ç»“æœ</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šåŸºäºç½‘æ ¼é”šå®šçš„å“ˆå¸Œè¡¨æ··åˆå½¢çŠ¶</li><li>ä½œè€…ï¼šKai Zhang, Yuxuan Zhang, Jiaolong Yang, Kun Xu, Yebin Liu, Qiong Yan, Baoquan Chen</li><li>å•ä½ï¼šæ¸…åå¤§å­¦</li><li>å…³é”®è¯ï¼šé¢éƒ¨åŠ¨ç”»ã€ç¥ç»è¾å°„åœºã€å“ˆå¸Œç¼–ç </li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.06438Github ä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼š(1)ï¼šç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æ˜¯ä¸€ç§å¼ºå¤§çš„è¡¨ç¤ºï¼Œå¯ä»¥ä»å›¾åƒä¸­æ•æ‰å¤æ‚åœºæ™¯çš„å‡ ä½•å’Œå¤–è§‚ã€‚ç„¶è€Œï¼ŒNeRF åœ¨è¡¨ç¤ºå…·æœ‰å¤æ‚æ‹“æ‰‘ç»“æ„çš„å¯¹è±¡ï¼ˆä¾‹å¦‚é¢éƒ¨ï¼‰æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚(2)ï¼šè¿‡å»æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•å°è¯•é€šè¿‡é‡‡ç”¨å“ˆå¸Œç¼–ç æŠ€æœ¯å°† NeRF åº”ç”¨äºé¢éƒ¨åŠ¨ç”»ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•è¦ä¹ˆå—é™äºå…¨å±€æ··åˆå½¢çŠ¶ï¼Œè¦ä¹ˆéœ€è¦å¤§é‡çš„å†…å­˜å’Œè®¡ç®—æˆæœ¬ã€‚(3)ï¼šæœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç½‘æ ¼é”šå®šçš„å“ˆå¸Œè¡¨æ··åˆå½¢çŠ¶çš„æ–°å‹é¢éƒ¨è¡¨ç¤ºã€‚è¯¥è¡¨ç¤ºå°† 3DMM é”šå®šçš„ NeRF ä¸å“ˆå¸Œç¼–ç ç›¸ç»“åˆï¼Œä»¥æœ‰æ•ˆåœ°æ•æ‰é¢éƒ¨è¡¨æƒ…çš„ç²¾ç»†ç»†èŠ‚ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä¸ºæ¯ä¸ª 3DMM é¡¶ç‚¹é™„åŠ ä¸€ç»„å“ˆå¸Œè¡¨ï¼Œæ¯ä¸ªå“ˆå¸Œè¡¨ç¼–ç é¡¶ç‚¹å‘¨å›´å±€éƒ¨è¾å°„åœºçš„åµŒå…¥ã€‚åœ¨æ¸²æŸ“æ—¶ï¼Œè¿™äº›å“ˆå¸Œè¡¨è¢«çº¿æ€§æ±‚å’Œï¼Œä»¥ç”Ÿæˆè¡¨ç¤ºç›®æ ‡è¡¨æƒ…çš„åˆå¹¶åµŒå…¥ã€‚(4)ï¼šæ–¹æ³•æ€§èƒ½ï¼šæˆ‘ä»¬åœ¨é¢éƒ¨åŠ¨ç”»åŸºå‡†ä¸Šè¯„ä¼°äº†æ‰€æå‡ºçš„æ–¹æ³•ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¸²æŸ“è´¨é‡å’Œæ•ˆç‡æ–¹é¢éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿå¤„ç†å„ç§é¢éƒ¨è¡¨æƒ…ï¼ŒåŒ…æ‹¬æç«¯è¡¨æƒ…ã€‚</li></ol><p><strong>æ–¹æ³•</strong></p><ol><li><strong>ç½‘æ ¼é”šå®šå“ˆå¸Œè¡¨æ··åˆå½¢çŠ¶ï¼š</strong>æå‡ºä¸€ç§æ–°çš„é¢éƒ¨è¡¨ç¤ºæ–¹æ³•ï¼Œå°† 3DMM é”šå®šçš„ç¥ç»è¾å°„åœºä¸å“ˆå¸Œç¼–ç ç›¸ç»“åˆï¼Œä»¥æœ‰æ•ˆæ•æ‰é¢éƒ¨è¡¨æƒ…çš„ç²¾ç»†ç»†èŠ‚ã€‚</li><li><strong>å“ˆå¸Œè¡¨æ··åˆå½¢çŠ¶çš„èåˆï¼š</strong>é€šè¿‡è¿è¡Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰åœ¨ UV å›¾åƒç©ºé—´ä¸­é¢„æµ‹é¡¶ç‚¹å˜å½¢ï¼Œè·å¾—æ¯ä¸ªé¡¶ç‚¹çš„æƒé‡ã€‚ç„¶åï¼Œä½¿ç”¨è¿™äº›æƒé‡å¯¹æ¯ä¸ªé¡¶ç‚¹ä¸Šçš„å“ˆå¸Œè¡¨è¿›è¡Œçº¿æ€§æ±‚å’Œï¼Œç”Ÿæˆåˆå¹¶çš„åµŒå…¥ã€‚</li><li><strong>æŸ¥è¯¢ç‚¹è§£ç ï¼š</strong>ä»åˆå¹¶çš„å“ˆå¸Œè¡¨ä¸­æå–åµŒå…¥ï¼Œå¹¶å°†å…¶ä¸ç‰¹å¾åµŒå…¥å’Œæ‘„åƒæœºè§†å›¾ä¸€èµ·è§£ç ä¸ºç¥ç»è¾å°„åœºã€‚</li><li><strong>åŠ é€Ÿæ¸²æŸ“ï¼š</strong>åˆ©ç”¨æŸ¥è¯¢ç‚¹ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œå°†æŸ¥è¯¢ç‚¹åˆ†ç»„åˆ°ä½“ç´ ä¸­ï¼Œå¹¶åˆ†å±‚æœç´¢ k-æœ€è¿‘é‚»é¡¶ç‚¹ï¼Œä»¥åŠ é€Ÿæ¸²æŸ“ã€‚</li><li><p><strong>å•ç›®è§†é¢‘è®­ç»ƒï¼š</strong>ä»…ä½¿ç”¨å•ç›® RGB è§†é¢‘è®­ç»ƒæå‡ºçš„å¤´åƒè¡¨ç¤ºï¼Œæ— éœ€ä»»ä½• 3D æ‰«ææˆ–å¤šè§†å›¾æ•°æ®ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç½‘æ ¼é”šå®šçš„å“ˆå¸Œè¡¨æ··åˆå½¢çŠ¶çš„æ–°å‹é¢éƒ¨è¡¨ç¤ºæ–¹æ³•ï¼Œæœ‰æ•ˆåœ°æ•æ‰äº†é¢éƒ¨è¡¨æƒ…çš„ç²¾ç»†ç»†èŠ‚ï¼Œåœ¨æ¸²æŸ“è´¨é‡å’Œæ•ˆç‡æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäºç½‘æ ¼é”šå®šçš„å“ˆå¸Œè¡¨æ··åˆå½¢çŠ¶çš„æ–°å‹é¢éƒ¨è¡¨ç¤ºæ–¹æ³•ï¼Œå°†3DMMé”šå®šçš„ç¥ç»è¾å°„åœºä¸å“ˆå¸Œç¼–ç ç›¸ç»“åˆï¼Œæœ‰æ•ˆåœ°æ•æ‰é¢éƒ¨è¡¨æƒ…çš„ç²¾ç»†ç»†èŠ‚ã€‚æ€§èƒ½ï¼šåœ¨é¢éƒ¨åŠ¨ç”»åŸºå‡†ä¸Šè¯„ä¼°äº†æ‰€æå‡ºçš„æ–¹æ³•ï¼Œç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¸²æŸ“è´¨é‡å’Œæ•ˆç‡æ–¹é¢éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å·¥ä½œé‡ï¼šä»…ä½¿ç”¨å•ç›®RGBè§†é¢‘è®­ç»ƒæå‡ºçš„å¤´åƒè¡¨ç¤ºï¼Œæ— éœ€ä»»ä½•3Dæ‰«ææˆ–å¤šè§†å›¾æ•°æ®ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-ab39844047d36e8caedab23572e71526.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a0e8bfc8983817dd020f5b1deff586eb.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-55d64640c5208a9cd19a534cb1503aba.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3173031114e8293e5e25b9733f1913ef.jpg" align="middle"></details><h2 id="MagicMirror-Fast-and-High-Quality-Avatar-Generation-with-a-Constrained-Search-Space-1"><a href="#MagicMirror-Fast-and-High-Quality-Avatar-Generation-with-a-Constrained-Search-Space-1" class="headerlink" title="MagicMirror: Fast and High-Quality Avatar Generation with a Constrained   Search Space"></a>MagicMirror: Fast and High-Quality Avatar Generation with a Constrained   Search Space</h2><p><strong>Authors:Armand Comas-MassaguÃ©, Di Qiu, Menglei Chai, Marcel BÃ¼hler, Amit Raj, Ruiqi Gao, Qiangeng Xu, Mark Matthews, Paulo Gotardo, Octavia Camps, Sergio Orts-Escolano, Thabo Beeler</strong></p><p>We introduce a novel framework for 3D human avatar generation and personalization, leveraging text prompts to enhance user engagement and customization. Central to our approach are key innovations aimed at overcoming the challenges in photo-realistic avatar synthesis. Firstly, we utilize a conditional Neural Radiance Fields (NeRF) model, trained on a large-scale unannotated multi-view dataset, to create a versatile initial solution space that accelerates and diversifies avatar generation. Secondly, we develop a geometric prior, leveraging the capabilities of Text-to-Image Diffusion Models, to ensure superior view invariance and enable direct optimization of avatar geometry. These foundational ideas are complemented by our optimization pipeline built on Variational Score Distillation (VSD), which mitigates texture loss and over-saturation issues. As supported by our extensive experiments, these strategies collectively enable the creation of custom avatars with unparalleled visual quality and better adherence to input text prompts. You can find more results and videos in our website: <a href="https://syntec-research.github.io/MagicMirror">https://syntec-research.github.io/MagicMirror</a> </p><p><a href="http://arxiv.org/abs/2404.01296v1">PDF</a> </p><p><strong>Summary</strong><br>æå‡ºä¸€ç§å…¨æ–° 3D äººä½“è™šæ‹Ÿäººç”Ÿæˆå’Œä¸ªæ€§åŒ–æ¡†æ¶ï¼Œåˆ©ç”¨æ–‡æœ¬æç¤ºå¢å¼ºç”¨æˆ·å‚ä¸å’Œå®šåˆ¶ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>åˆ©ç”¨æ¡ä»¶ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æ¨¡å‹ï¼Œåˆ›å»ºäº†ä¸€ä¸ªå¯æ‰©å±•çš„åˆå§‹è§£å†³æ–¹æ¡ˆç©ºé—´ï¼Œä½¿è™šæ‹Ÿäººç”Ÿæˆé€Ÿåº¦æ›´å¿«ã€å¤šæ ·åŒ–æ›´å¼ºã€‚</li><li>å¼€å‘äº†ä¸€ä¸ªåŸºäºå‡ ä½•å…ˆéªŒå’Œæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„ä¼˜åŒ–ç®¡é“ï¼Œä»¥ç¡®ä¿å‡ºè‰²çš„è§†å›¾ä¸å˜æ€§å’Œç›´æ¥ä¼˜åŒ–è™šæ‹Ÿäººçš„å‡ ä½•å½¢çŠ¶ã€‚</li><li>æˆ‘ä»¬çš„ä¼˜åŒ–ç®¡é“å»ºç«‹åœ¨å˜åˆ†åˆ†æ•°è’¸é¦ï¼ˆVSDï¼‰ä¹‹ä¸Šï¼Œå¯ç¼“è§£çº¹ç†ä¸¢å¤±å’Œè¿‡é¥±å’Œé—®é¢˜ã€‚</li><li>æä¾›çš„åˆ›æ–°ç­–ç•¥èƒ½å¤Ÿåˆ›é€ å‡ºå…·æœ‰æ— ä¸ä¼¦æ¯”è§†è§‰è´¨é‡å’Œæ›´ç¬¦åˆè¾“å…¥æ–‡æœ¬æç¤ºçš„è‡ªå®šä¹‰è™šæ‹Ÿäººã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šé­”é•œï¼šå¿«é€Ÿä¸”é«˜è´¨é‡çš„å¤´åƒ</li><li>ä½œè€…ï¼šArmand Comas-MassaguÃ©, Di Qiu, Menglei Chai, Marcel BÃ¼hler, Amit Raj, Ruiqi Gao, Qiangeng Xu, Mark Matthews, Paulo Gotardo, Octavia Camps, Sergio Orts-Escolano, Thabo Beeler</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šGoogle</li><li>å…³é”®è¯ï¼š3D å¤´åƒç”Ÿæˆã€æ–‡æœ¬å¼•å¯¼ã€ç¥ç»è¾å°„åœºã€å‡ ä½•å…ˆéªŒã€å˜åˆ†åˆ†æ•°è’¸é¦</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.01296   Github ä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šéšç€è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®ç­‰æŠ€æœ¯çš„å…´èµ·ï¼Œå¯¹é€¼çœŸä¸”å¯å®šåˆ¶çš„ 3D äººç±»å¤´åƒçš„éœ€æ±‚ä¸æ–­å¢é•¿ã€‚ä½†æ˜¯ï¼Œç”Ÿæˆé«˜è´¨é‡çš„å¤´åƒä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå°¤å…¶æ˜¯å½“éœ€è¦æ ¹æ®æ–‡æœ¬æç¤ºè¿›è¡Œä¸ªæ€§åŒ–å®šåˆ¶æ—¶ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šä¼ ç»Ÿæ–¹æ³•é€šå¸¸ä¾èµ–äºä» 3D æ‰«ææˆ–æ‰‹åŠ¨å»ºæ¨¡ä¸­è·å–æ•°æ®ï¼Œè¿™æ—¢è€—æ—¶åˆæ˜‚è´µã€‚åŸºäºç¥ç»ç½‘ç»œçš„æ–¹æ³•è™½ç„¶å¯ä»¥ä»å›¾åƒä¸­ç”Ÿæˆå¤´åƒï¼Œä½†å®ƒä»¬åœ¨æ•è·æ–‡æœ¬æç¤ºä¸­çš„ç»†å¾®å·®åˆ«å’Œç¡®ä¿å‡ ä½•ä¸€è‡´æ€§æ–¹é¢ä»ç„¶å­˜åœ¨å›°éš¾ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡º MagicMirror æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨æ–‡æœ¬æç¤ºç”Ÿæˆå¿«é€Ÿä¸”é«˜è´¨é‡çš„ 3D äººç±»å¤´åƒã€‚MagicMirror åˆ©ç”¨æ¡ä»¶ç¥ç»è¾å°„åœº (NeRF) æ¨¡å‹åˆ›å»ºå¤šè§†å›¾åˆå§‹è§£ç©ºé—´ï¼Œå¹¶ä½¿ç”¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å¼€å‘å‡ ä½•å…ˆéªŒä»¥ç¡®ä¿è§†å›¾ä¸å˜æ€§å’Œå‡ ä½•ä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜é‡‡ç”¨åŸºäºå˜åˆ†åˆ†æ•°è’¸é¦çš„ä¼˜åŒ–ç®¡é“ï¼Œä»¥å‡è½»çº¹ç†æŸå¤±å’Œè¿‡é¥±å’Œé—®é¢˜ã€‚ï¼ˆ4ï¼‰ä»»åŠ¡å’Œæ€§èƒ½ï¼šMagicMirror åœ¨å¤´åƒç”Ÿæˆå’Œä¸ªæ€§åŒ–ä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥ç”Ÿæˆå…·æœ‰æ— ä¸ä¼¦æ¯”è§†è§‰è´¨é‡å’Œé«˜åº¦ç¬¦åˆæ–‡æœ¬æç¤ºçš„å®šåˆ¶å¤´åƒã€‚è¯¥æ–¹æ³•çš„æ€§èƒ½æ”¯æŒå…¶ç›®æ ‡ï¼Œå³æä¾›ä¸€ç§å¿«é€Ÿä¸”æœ‰æ•ˆçš„æ–¹æ³•æ¥ç”Ÿæˆé«˜è´¨é‡çš„ 3D äººç±»å¤´åƒã€‚</li></ol><p>7.æ–¹æ³•ï¼šï¼ˆ1ï¼‰åˆ©ç”¨æ¡ä»¶ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æ¨¡å‹åˆ›å»ºå¤šè§†å›¾åˆå§‹è§£ç©ºé—´ï¼Œä¸ºä¼˜åŒ–æä¾›çº¦æŸï¼›ï¼ˆ2ï¼‰ä½¿ç”¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å¼€å‘å‡ ä½•å…ˆéªŒï¼Œç¡®ä¿è§†å›¾ä¸å˜æ€§å’Œå‡ ä½•ä¼˜åŒ–ï¼›ï¼ˆ3ï¼‰é‡‡ç”¨åŸºäºå˜åˆ†åˆ†æ•°è’¸é¦çš„ä¼˜åŒ–ç®¡é“ï¼Œå‡è½»çº¹ç†æŸå¤±å’Œè¿‡é¥±å’Œé—®é¢˜ï¼›ï¼ˆ4ï¼‰é€šè¿‡æ··åˆå’ŒåŠ æƒä¸åŒçš„æ¦‚å¿µï¼Œå®ç°æ¦‚å¿µç»„åˆå’Œè°ƒåˆ¶ï¼Œä¸°å¯Œç”¨æˆ·ä½“éªŒã€‚</p><ol><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šxxxï¼›ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šxxxï¼›æ€§èƒ½ï¼šxxxï¼›å·¥ä½œé‡ï¼šxxxï¼›</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº† MagicMirror æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨æ–‡æœ¬æç¤ºç”Ÿæˆå¿«é€Ÿä¸”é«˜è´¨é‡çš„ 3D äººç±»å¤´åƒã€‚MagicMirror é‡‡ç”¨æ¡ä»¶ç¥ç»è¾å°„åœº (NeRF) æ¨¡å‹ã€æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å’ŒåŸºäºå˜åˆ†åˆ†æ•°è’¸é¦çš„ä¼˜åŒ–ç®¡é“ï¼Œå®ç°äº†æ— ä¸ä¼¦æ¯”çš„è§†è§‰è´¨é‡ã€é«˜åº¦ç¬¦åˆæ–‡æœ¬æç¤ºçš„å®šåˆ¶å¤´åƒç”Ÿæˆï¼Œä¸ºå¿«é€Ÿé«˜æ•ˆç”Ÿæˆé«˜è´¨é‡çš„ 3D äººç±»å¤´åƒæä¾›äº†æœ‰æ•ˆæ–¹æ³•ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>åˆ©ç”¨æ¡ä»¶ç¥ç»è¾å°„åœº (NeRF) æ¨¡å‹åˆ›å»ºå¤šè§†å›¾åˆå§‹è§£ç©ºé—´ï¼Œä¸ºä¼˜åŒ–æä¾›çº¦æŸã€‚</li><li>ä½¿ç”¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å¼€å‘å‡ ä½•å…ˆéªŒï¼Œç¡®ä¿è§†å›¾ä¸å˜æ€§å’Œå‡ ä½•ä¼˜åŒ–ã€‚</li><li>é‡‡ç”¨åŸºäºå˜åˆ†åˆ†æ•°è’¸é¦çš„ä¼˜åŒ–ç®¡é“ï¼Œå‡è½»çº¹ç†æŸå¤±å’Œè¿‡é¥±å’Œé—®é¢˜ã€‚</li><li>é€šè¿‡æ··åˆå’ŒåŠ æƒä¸åŒçš„æ¦‚å¿µï¼Œå®ç°æ¦‚å¿µç»„åˆå’Œè°ƒåˆ¶ï¼Œä¸°å¯Œç”¨æˆ·ä½“éªŒã€‚æ€§èƒ½ï¼š</li><li>åœ¨å¤´åƒç”Ÿæˆå’Œä¸ªæ€§åŒ–ä»»åŠ¡ä¸Šï¼ŒMagicMirror ç”Ÿæˆå…·æœ‰æ— ä¸ä¼¦æ¯”è§†è§‰è´¨é‡å’Œé«˜åº¦ç¬¦åˆæ–‡æœ¬æç¤ºçš„å®šåˆ¶å¤´åƒã€‚</li><li>MagicMirror çš„æ€§èƒ½æ”¯æŒå…¶ç›®æ ‡ï¼Œå³æä¾›ä¸€ç§å¿«é€Ÿä¸”æœ‰æ•ˆçš„æ–¹æ³•æ¥ç”Ÿæˆé«˜è´¨é‡çš„ 3D äººç±»å¤´åƒã€‚å·¥ä½œé‡ï¼š</li><li>è™½ç„¶ MagicMirror ä¸éœ€è¦å¤§è§„æ¨¡çš„ 3D äººç±»æ•°æ®ï¼Œä½†ä¸ºæ•°ç™¾æˆ–æ•°åƒä¸ªå¯¹è±¡æ”¶é›†è¿™äº›æ•°æ®ä»ç„¶æ˜¯ä¸€é¡¹ç›¸å¯¹æ˜‚è´µä¸”è€—æ—¶çš„å·¥ä½œã€‚</li><li>ä»å¦ä¸€ä¸ªè§’åº¦æ¥çœ‹ï¼Œæˆ‘ä»¬ç”¨æ¥çº¦æŸè§£ç©ºé—´çš„æ•°æ®ä¹Ÿé™åˆ¶äº†æˆ‘ä»¬ï¼Œå› ä¸ºæŸäº›æç«¯çš„åˆ†å¸ƒå¤–ä¿®æ”¹å¾ˆéš¾å®ç°ã€‚</li><li>æˆ‘ä»¬çš„æ–¹æ³•ä¹Ÿå¯èƒ½å—åˆ°è®¡ç®—èµ„æºçš„é™åˆ¶ï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦å¤šä¸ªæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œè‡³å°‘æ¯ä¸ªæ¨¡å‹éƒ½ç”¨äºé¢œè‰²å’Œæ³•çº¿ï¼Œå¦‚æœæˆ‘ä»¬æƒ³è¦æ‰§è¡Œæ¦‚å¿µæ··åˆï¼Œåˆ™éœ€è¦æ›´å¤šã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-1baa0925f922a12a8e66e59ff6fb331c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c42a213fc6f5741472d164c3e74fb649.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-864c32c352be8bc29cb98bb5b6fc3f53.jpg" align="middle"></details><h2 id="HAHA-Highly-Articulated-Gaussian-Human-Avatars-with-Textured-Mesh-Prior"><a href="#HAHA-Highly-Articulated-Gaussian-Human-Avatars-with-Textured-Mesh-Prior" class="headerlink" title="HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior"></a>HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior</h2><p><strong>Authors:David Svitov, Pietro Morerio, Lourdes Agapito, Alessio Del Bue</strong></p><p>We present HAHA - a novel approach for animatable human avatar generation from monocular input videos. The proposed method relies on learning the trade-off between the use of Gaussian splatting and a textured mesh for efficient and high fidelity rendering. We demonstrate its efficiency to animate and render full-body human avatars controlled via the SMPL-X parametric model. Our model learns to apply Gaussian splatting only in areas of the SMPL-X mesh where it is necessary, like hair and out-of-mesh clothing. This results in a minimal number of Gaussians being used to represent the full avatar, and reduced rendering artifacts. This allows us to handle the animation of small body parts such as fingers that are traditionally disregarded. We demonstrate the effectiveness of our approach on two open datasets: SnapshotPeople and X-Humans. Our method demonstrates on par reconstruction quality to the state-of-the-art on SnapshotPeople, while using less than a third of Gaussians. HAHA outperforms previous state-of-the-art on novel poses from X-Humans both quantitatively and qualitatively. </p><p><a href="http://arxiv.org/abs/2404.01053v1">PDF</a> </p><p><strong>Summary</strong><br>ä½¿ç”¨é«˜æ–¯æ•£å°„å’Œçº¹ç†ç½‘æ ¼ç›¸ç»“åˆçš„æ–¹å¼ï¼Œç”Ÿæˆå¯åŠ¨ç”»é€¼çœŸçš„å…¨èº«äººä½“å¤´åƒã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºäº†ä¸€ç§åä¸ºHAHAçš„æ–°æ–¹æ³•ï¼Œç”¨äºä»å•ç›®è¾“å…¥è§†é¢‘ç”Ÿæˆå¯åŠ¨ç”»çš„äººå½¢å¤´åƒã€‚</li><li>HAHAé€šè¿‡å­¦ä¹ é«˜æ–¯æ•£å°„å’Œçº¹ç†ç½‘æ ¼çš„ä½¿ç”¨æƒè¡¡ï¼Œå®ç°é«˜æ•ˆä¸”é«˜ä¿çœŸçš„æ¸²æŸ“ã€‚</li><li>HAHAä»…åœ¨SMPL-Xç½‘æ ¼å¿…è¦çš„åŒºåŸŸï¼ˆå¦‚å¤´å‘å’Œç½‘æ ¼å¤–è¡£ç‰©ï¼‰åº”ç”¨é«˜æ–¯æ•£å°„ã€‚</li><li>HAHAå‡å°‘äº†è¡¨ç¤ºå®Œæ•´å¤´åƒæ‰€éœ€çš„é«˜æ–¯æ•°é‡ï¼Œå¹¶å‡å°‘äº†æ¸²æŸ“ä¼ªå½±ã€‚</li><li>HAHAå¯ä»¥å¤„ç†æ‰‹æŒ‡ç­‰ä¼ ç»Ÿä¸Šè¢«å¿½ç•¥çš„å°èº«ä½“éƒ¨ä½çš„åŠ¨ç”»ã€‚</li><li>HAHAåœ¨SnapshotPeopleæ•°æ®é›†ä¸Šå±•ç¤ºäº†ä¸æœ€å…ˆè¿›æŠ€æœ¯ç›¸å½“çš„é‡å»ºè´¨é‡ï¼ŒåŒæ—¶ä½¿ç”¨çš„é«˜æ–¯æ•°é‡ä¸åˆ°ä¸‰åˆ†ä¹‹ä¸€ã€‚</li><li>HAHAåœ¨X-Humansçš„æ–°å§¿åŠ¿ä¸Šè¶…è¶Šäº†ä¹‹å‰çš„æœ€å…ˆè¿›æŠ€æœ¯ï¼Œæ— è®ºæ˜¯åœ¨å®šé‡è¿˜æ˜¯å®šæ€§ä¸Šã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>è®ºæ–‡é¢˜ç›®ï¼šHAHAï¼šä¸€ç§å¯åŠ¨ç”»çš„äººç±»åŒ–èº«ç”Ÿæˆæ–¹æ³•</li><li>ä½œè€…ï¼šDavid Svitovã€Michael ZollhÃ¶ferã€Angjoo Kanazawaã€Eric Horvitzã€Mehmet Ercan Aksan</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šå¾®è½¯ç ”ç©¶é™¢ï¼ˆç¾å›½ï¼‰</li><li>å…³é”®è¯ï¼šHuman avatar, Full-body, Gaussians platting, Textures</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.09880Github ä»£ç é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šéšç€è®¡ç®—æœºè§†è§‰å’Œå›¾å½¢å­¦çš„å‘å±•ï¼Œç”Ÿæˆå¯åŠ¨ç”»çš„äººç±»åŒ–èº«å·²æˆä¸ºä¸€é¡¹é‡è¦çš„ä»»åŠ¡ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä½¿ç”¨é«˜æ–¯ä½“ç´ æˆ–çº¹ç†ç½‘æ ¼æ¥è¡¨ç¤ºåŒ–èº«ï¼Œä½†è¿™äº›æ–¹æ³•åœ¨æ•ˆç‡å’Œä¿çœŸåº¦æ–¹é¢å­˜åœ¨æƒè¡¡ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•ï¼šè¿‡å»çš„æ–¹æ³•è¦ä¹ˆä½¿ç”¨é«˜æ–¯ä½“ç´ å®ç°é«˜æ•ˆæ¸²æŸ“ï¼Œä½†ä¿çœŸåº¦è¾ƒä½ï¼›è¦ä¹ˆä½¿ç”¨çº¹ç†ç½‘æ ¼å®ç°é«˜ä¿çœŸåº¦ï¼Œä½†æ¸²æŸ“æ•ˆç‡è¾ƒä½ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º HAHA çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡å­¦ä¹ é«˜æ–¯ä½“ç´ å’Œçº¹ç†ç½‘æ ¼çš„æƒè¡¡ï¼Œç”Ÿæˆå¯åŠ¨ç”»çš„äººç±»åŒ–èº«ã€‚HAHA ä½¿ç”¨é«˜æ–¯ä½“ç´ è¡¨ç¤ºåŒ–èº«ä¸­éš¾ä»¥ç”¨ç½‘æ ¼è¡¨ç¤ºçš„åŒºåŸŸï¼Œä¾‹å¦‚å¤´å‘å’Œéç½‘æ ¼æœè£…ï¼Œè€Œä½¿ç”¨çº¹ç†ç½‘æ ¼è¡¨ç¤ºåŒ–èº«ä¸­æ˜“äºç”¨ç½‘æ ¼è¡¨ç¤ºçš„åŒºåŸŸã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šåœ¨ SnapshotPeople å’Œ X-Humans ä¸¤ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒHAHA åœ¨é‡å»ºè´¨é‡ä¸Šä¸ç°æœ‰æ–¹æ³•ç›¸å½“ï¼ŒåŒæ—¶ä½¿ç”¨çš„é«˜æ–¯ä½“ç´ æ•°é‡å‡å°‘äº†ä¸‰åˆ†ä¹‹ä¸€ä»¥ä¸Šã€‚åœ¨ X-Humans æ•°æ®é›†ä¸Šï¼ŒHAHA åœ¨æ–°å§¿åŠ¿ä¸Šçš„æ€§èƒ½ä¼˜äºä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œæ— è®ºæ˜¯åœ¨å®šé‡è¿˜æ˜¯å®šæ€§æ–¹é¢ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒHAHA èƒ½å¤Ÿæœ‰æ•ˆåœ°å¹³è¡¡æ•ˆç‡å’Œä¿çœŸåº¦ï¼Œç”Ÿæˆé«˜è´¨é‡çš„å¯åŠ¨ç”»äººç±»åŒ–èº«ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1): é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡ä¼˜åŒ–å±€éƒ¨é«˜æ–¯å˜æ¢ Î¼jiã€rjiã€sji å’Œé¢œè‰² cji æ¥è®­ç»ƒ 3D é«˜æ–¯ä½“ç´  (GS) è¡¨ç¤ºã€‚(2): ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨å¯å¾®åˆ†å…‰æ …åŒ–å™¨æ¸²æŸ“å…·æœ‰å¯è®­ç»ƒçº¹ç†çš„ SMPL-X ç½‘æ ¼ã€‚(3): æœ€åï¼Œæˆ‘ä»¬åˆå¹¶å¯å¾®åˆ†æ¸²æŸ“çº¹ç†ç½‘æ ¼å’Œå¯å¾®åˆ† 3D GS è¿‡ç¨‹ï¼Œè®­ç»ƒé«˜æ–¯ä½“ç´ çš„ä¸é€æ˜åº¦ oji å’Œé¢œè‰² cjiã€‚</p></li><li><p>ç»“è®ºï¼š(1): æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºHAHAçš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡å­¦ä¹ é«˜æ–¯ä½“ç´ å’Œçº¹ç†ç½‘æ ¼çš„æƒè¡¡ï¼Œç”Ÿæˆå¯åŠ¨ç”»çš„äººç±»åŒ–èº«ã€‚HAHAåœ¨é‡å»ºè´¨é‡ä¸Šä¸ç°æœ‰æ–¹æ³•ç›¸å½“ï¼ŒåŒæ—¶ä½¿ç”¨çš„é«˜æ–¯ä½“ç´ æ•°é‡å‡å°‘äº†ä¸‰åˆ†ä¹‹ä¸€ä»¥ä¸Šã€‚åœ¨X-Humansæ•°æ®é›†ä¸Šï¼ŒHAHAåœ¨æ–°å§¿åŠ¿ä¸Šçš„æ€§èƒ½ä¼˜äºä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œæ— è®ºæ˜¯åœ¨å®šé‡è¿˜æ˜¯å®šæ€§æ–¹é¢ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒHAHAèƒ½å¤Ÿæœ‰æ•ˆåœ°å¹³è¡¡æ•ˆç‡å’Œä¿çœŸåº¦ï¼Œç”Ÿæˆé«˜è´¨é‡çš„å¯åŠ¨ç”»äººç±»åŒ–èº«ã€‚(2): åˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•æ¥ç”Ÿæˆå¯åŠ¨ç”»çš„äººç±»åŒ–èº«ï¼Œè¯¥æ–¹æ³•é€šè¿‡å­¦ä¹ é«˜æ–¯ä½“ç´ å’Œçº¹ç†ç½‘æ ¼çš„æƒè¡¡æ¥å¹³è¡¡æ•ˆç‡å’Œä¿çœŸåº¦ã€‚</li><li>è¯¥æ–¹æ³•åœ¨é‡å»ºè´¨é‡ä¸Šä¸ç°æœ‰æ–¹æ³•ç›¸å½“ï¼ŒåŒæ—¶ä½¿ç”¨çš„é«˜æ–¯ä½“ç´ æ•°é‡å‡å°‘äº†ä¸‰åˆ†ä¹‹ä¸€ä»¥ä¸Šã€‚</li><li>è¯¥æ–¹æ³•åœ¨æ–°å§¿åŠ¿ä¸Šçš„æ€§èƒ½ä¼˜äºä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œæ— è®ºæ˜¯åœ¨å®šé‡è¿˜æ˜¯å®šæ€§æ–¹é¢ã€‚æ€§èƒ½ï¼š</li><li>åœ¨SnapshotPeopleå’ŒX-Humansä¸¤ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒHAHAåœ¨é‡å»ºè´¨é‡ä¸Šä¸ç°æœ‰æ–¹æ³•ç›¸å½“ï¼ŒåŒæ—¶ä½¿ç”¨çš„é«˜æ–¯ä½“ç´ æ•°é‡å‡å°‘äº†ä¸‰åˆ†ä¹‹ä¸€ä»¥ä¸Šã€‚</li><li>åœ¨X-Humansæ•°æ®é›†ä¸Šï¼ŒHAHAåœ¨æ–°å§¿åŠ¿ä¸Šçš„æ€§èƒ½ä¼˜äºä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œæ— è®ºæ˜¯åœ¨å®šé‡è¿˜æ˜¯å®šæ€§æ–¹é¢ã€‚å·¥ä½œé‡ï¼š</li><li>è¯¥æ–¹æ³•çš„è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºã€‚</li><li>è¯¥æ–¹æ³•éœ€è¦å¤§é‡çš„æ•°æ®æ¥è®­ç»ƒï¼Œè¿™å¯èƒ½æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-b477b212869e9f8635da6f1012719fb9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-aa6e5f76fe253938286549f8ee5205fd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4f97970c093585e18e2db42fb96a6b75.jpg" align="middle"></details><h2 id="TexVocab-Texture-Vocabulary-conditioned-Human-Avatars-1"><a href="#TexVocab-Texture-Vocabulary-conditioned-Human-Avatars-1" class="headerlink" title="TexVocab: Texture Vocabulary-conditioned Human Avatars"></a>TexVocab: Texture Vocabulary-conditioned Human Avatars</h2><p><strong>Authors:Yuxiao Liu, Zhe Li, Yebin Liu, Haoqian Wang</strong></p><p>To adequately utilize the available image evidence in multi-view video-based avatar modeling, we propose TexVocab, a novel avatar representation that constructs a texture vocabulary and associates body poses with texture maps for animation. Given multi-view RGB videos, our method initially back-projects all the available images in the training videos to the posed SMPL surface, producing texture maps in the SMPL UV domain. Then we construct pairs of human poses and texture maps to establish a texture vocabulary for encoding dynamic human appearances under various poses. Unlike the commonly used joint-wise manner, we further design a body-part-wise encoding strategy to learn the structural effects of the kinematic chain. Given a driving pose, we query the pose feature hierarchically by decomposing the pose vector into several body parts and interpolating the texture features for synthesizing fine-grained human dynamics. Overall, our method is able to create animatable human avatars with detailed and dynamic appearances from RGB videos, and the experiments show that our method outperforms state-of-the-art approaches. The project page can be found at <a href="https://texvocab.github.io/">https://texvocab.github.io/</a>. </p><p><a href="http://arxiv.org/abs/2404.00524v1">PDF</a> </p><p><strong>Summary</strong><br>åŸºäºå¤šè§†è§’è§†é¢‘åˆ›å»ºé€¼çœŸçš„åŒ–èº«æ¨¡å‹ï¼ŒTexVocab æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºçº¹ç†è¯æ±‡çš„åŒ–èº«è¡¨å¾ï¼Œå°†äººä½“å§¿åŠ¿ä¸ç”¨äºåŠ¨ç”»çš„çº¹ç†è´´å›¾è”ç³»èµ·æ¥ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºäº†ä¸€ç§æ–°çš„åŒ–èº«è¡¨å¾ TexVocabï¼Œç”¨äºä»å¤šè§†è§’ RGB è§†é¢‘åˆ›å»ºé€¼çœŸçš„åŒ–èº«æ¨¡å‹ã€‚</li><li>TexVocab æ„å»ºäº†ä¸€ä¸ªçº¹ç†è¯æ±‡ï¼Œå°†èº«ä½“å§¿åŠ¿ä¸çº¹ç†è´´å›¾è”ç³»èµ·æ¥ï¼Œç”¨äºåŠ¨ç”»ã€‚</li><li>å°†æ‰€æœ‰å¯ç”¨å›¾åƒåæŠ•å½±åˆ°å§¿åŠ¿åŒ– SMPL æ›²é¢ä¸Šï¼Œç”Ÿæˆ SMPL UV åŸŸä¸­çš„çº¹ç†è´´å›¾ã€‚</li><li>æ„å»ºäººä½“å§¿åŠ¿å’Œçº¹ç†è´´å›¾å¯¹ï¼Œå»ºç«‹çº¹ç†è¯æ±‡ï¼Œä»¥å¯¹å„ç§å§¿åŠ¿ä¸‹çš„åŠ¨æ€äººç±»å¤–è§‚è¿›è¡Œç¼–ç ã€‚</li><li>è®¾è®¡äº†ä¸€ä¸ªåŸºäºèº«ä½“éƒ¨ä½çš„ç¼–ç ç­–ç•¥ï¼Œä»¥å­¦ä¹ è¿åŠ¨é“¾çš„ç»“æ„æ•ˆåº”ã€‚</li><li>ç»™å®šä¸€ä¸ªé©±åŠ¨å§¿åŠ¿ï¼Œé€šè¿‡å°†å§¿åŠ¿å‘é‡åˆ†è§£æˆå‡ ä¸ªèº«ä½“éƒ¨ä½å¹¶æ’å€¼çº¹ç†ç‰¹å¾æ¥åˆ†çº§æŸ¥è¯¢å§¿åŠ¿ç‰¹å¾ï¼Œåˆæˆç»†ç²’åº¦çš„äººä½“åŠ¨æ€ã€‚</li><li>åœ¨ RGB è§†é¢‘ä¸­åˆ›å»ºå…·æœ‰è¯¦ç»†åŠ¨æ€å¤–è§‚çš„å¯åŠ¨ç”»äººä½“åŒ–èº«ï¼Œå®éªŒè¡¨æ˜è¯¥æ–¹æ³•ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>é¢˜ç›®ï¼šTexVocabï¼šçº¹ç†è¯å…¸æ¡ä»¶ä¸‹çš„äººä½“è™šæ‹ŸåŒ–èº«</li><p></p><p></p><li>ä½œè€…ï¼šåˆ˜ç…œéœ„ã€æå“²ã€åˆ˜ä¸šå½¬ã€ç‹æµ©å€©</li><p></p><p></p><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šæ·±åœ³å›½é™…ç ”ç©¶ç”Ÿé™¢ï¼Œæ¸…åå¤§å­¦</li><p></p><p></p><li>å…³é”®è¯ï¼šäººä½“è™šæ‹ŸåŒ–èº«ã€çº¹ç†è¯å…¸ã€å¤šè§†è§’è§†é¢‘ã€æ¡ä»¶ç¥ç»è¾å°„åœº</li><p></p><p></p><li>è®ºæ–‡é“¾æ¥ï¼šNoneï¼ŒGithub ä»£ç é“¾æ¥ï¼šhttps://texvocab.github.io/</li><p></p><p></p><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼š    äººä½“è™šæ‹ŸåŒ–èº«å»ºæ¨¡åœ¨ AR/VR åº”ç”¨ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†å¦‚ä½•æœ‰æ•ˆå­¦ä¹ é©±åŠ¨ä¿¡å·å’ŒåŠ¨æ€å¤–è§‚ä¹‹é—´çš„æ˜ å°„ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li><br>&lt;/ol&gt;<p></p><p>ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼š    ä»¥å¾€æ–¹æ³•é€šå¸¸ç›´æ¥å°†å§¿æ€è¾“å…¥æ˜ å°„åˆ°äººä½“å¤–è§‚ï¼Œä½†å§¿æ€è¾“å…¥ä¸åŒ…å«ä»»ä½•åŠ¨æ€äººä½“å¤–è§‚ä¿¡æ¯ï¼Œå¯¼è‡´ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰éš¾ä»¥ä»…ä»å§¿æ€è¾“å…¥ä¸­å›å½’é«˜ä¿çœŸåŠ¨æ€ç»†èŠ‚ã€‚è™½ç„¶ä¸€äº›å·¥ä½œæå‡ºè‡ªåŠ¨è§£ç æ½œåœ¨åµŒå…¥æ¥å¯¹è¾“å…¥ç«¯çš„åŠ¨æ€å¤–è§‚è¿›è¡Œç¼–ç ï¼Œä½†å®ƒä»¬ä»ç„¶å—é™äºå…¨å±€ä»£ç æˆ–ç‰¹å¾çº¿çš„è¡¨ç¤ºèƒ½åŠ›ï¼Œå¯¼è‡´åˆæˆçš„è™šæ‹ŸåŒ–èº«æ¨¡ç³Šã€‚</p><p>ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼š    æœ¬æ–‡æå‡º TexVocabï¼Œä¸€ç§çº¹ç†è¯å…¸ï¼Œå……åˆ†åˆ©ç”¨æ˜¾å¼å›¾åƒè¯æ®æ¥æŒ‡å¯¼éšå¼æ¡ä»¶ NeRF ä»è¡¨è¾¾çº¹ç†æ¡ä»¶ä¸­å­¦ä¹ åŠ¨æ€ã€‚ä¸ºäº†å°†å¤šè§†è§’å›¾åƒä¸åŠ¨æ€äººä½“å…³è”èµ·æ¥ï¼Œå°†æ‰€æœ‰å¯ç”¨å›¾åƒåæŠ•å½±åˆ°ç›¸åº”çš„è®­ç»ƒå§¿æ€ä¸Šï¼Œåœ¨ SMPL UV åŸŸä¸­ç”Ÿæˆçº¹ç†è´´å›¾ã€‚ç„¶åæ„å»ºäººä½“å§¿æ€å’Œçº¹ç†è´´å›¾å¯¹ï¼Œå»ºç«‹çº¹ç†è¯å…¸ï¼Œç”¨äºç¼–ç åœ¨ä¸åŒå§¿æ€ä¸‹çš„åŠ¨æ€äººä½“å¤–è§‚ã€‚ä¸å¸¸ç”¨çš„å…³èŠ‚æ–¹å¼ä¸åŒï¼Œæœ¬æ–‡è¿›ä¸€æ­¥è®¾è®¡äº†èº«ä½“éƒ¨ä½ç¼–ç ç­–ç•¥ï¼Œä»¥å­¦ä¹ è¿åŠ¨é“¾çš„ç»“æ„å½±å“ã€‚ç»™å®šä¸€ä¸ªé©±åŠ¨å§¿æ€ï¼Œé€šè¿‡å°†å§¿æ€å‘é‡åˆ†è§£æˆå¤šä¸ªèº«ä½“éƒ¨ä½å¹¶å¯¹çº¹ç†ç‰¹å¾è¿›è¡Œæ’å€¼ï¼Œåˆ†å±‚æŸ¥è¯¢å§¿æ€ç‰¹å¾ï¼Œä»¥åˆæˆç»†ç²’åº¦çš„åŠ¨æ€äººä½“ã€‚</p><p>ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼š    æœ¬æ–‡æ–¹æ³•èƒ½å¤Ÿä» RGB è§†é¢‘åˆ›å»ºå…·æœ‰è¯¦ç»†åŠ¨æ€å¤–è§‚çš„åŠ¨ç”»è™šæ‹ŸåŒ–èº«ï¼Œå®éªŒè¡¨æ˜è¯¥æ–¹æ³•ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p><ol><li><strong>æ–¹æ³•</strong>ï¼š(1): æå‡º <strong>çº¹ç†è¯å…¸ï¼ˆTexVocabï¼‰</strong>ï¼Œåˆ©ç”¨æ˜¾å¼å›¾åƒè¯æ®æŒ‡å¯¼éšå¼æ¡ä»¶ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ä»çº¹ç†æ¡ä»¶ä¸­å­¦ä¹ åŠ¨æ€ã€‚(2): å°†å¤šè§†è§’å›¾åƒåæŠ•å½±åˆ°ç›¸åº”çš„è®­ç»ƒå§¿æ€ä¸Šï¼Œåœ¨ <strong>SMPLUV</strong> åŸŸä¸­ç”Ÿæˆçº¹ç†è´´å›¾ï¼Œæ„å»º <strong>å§¿æ€-çº¹ç†è´´å›¾å¯¹</strong>ï¼Œå½¢æˆçº¹ç†è¯å…¸ã€‚(3): è®¾è®¡ <strong>èº«ä½“éƒ¨ä½ç¼–ç ç­–ç•¥</strong>ï¼Œå­¦ä¹ è¿åŠ¨é“¾çš„ç»“æ„å½±å“ï¼Œåˆ†å±‚æŸ¥è¯¢å§¿æ€ç‰¹å¾ï¼Œåˆæˆç»†ç²’åº¦çš„åŠ¨æ€äººä½“ã€‚</li></ol><p>8.ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºTexVocabæ–¹æ³•ï¼Œåˆ©ç”¨çº¹ç†è¯å…¸æŒ‡å¯¼éšå¼æ¡ä»¶NeRFä»çº¹ç†æ¡ä»¶ä¸­å­¦ä¹ åŠ¨æ€ï¼Œå®ç°äº†ä»RGBè§†é¢‘åˆ›å»ºå…·æœ‰è¯¦ç»†åŠ¨æ€å¤–è§‚çš„åŠ¨ç”»è™šæ‹ŸåŒ–èº«ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š* æå‡ºçº¹ç†è¯å…¸ï¼Œåˆ©ç”¨æ˜¾å¼å›¾åƒè¯æ®æŒ‡å¯¼éšå¼æ¡ä»¶NeRFå­¦ä¹ åŠ¨æ€ã€‚* è®¾è®¡èº«ä½“éƒ¨ä½ç¼–ç ç­–ç•¥ï¼Œå­¦ä¹ è¿åŠ¨é“¾çš„ç»“æ„å½±å“ï¼Œåˆ†å±‚æŸ¥è¯¢å§¿æ€ç‰¹å¾ï¼Œåˆæˆç»†ç²’åº¦çš„åŠ¨æ€äººä½“ã€‚æ€§èƒ½ï¼š* èƒ½å¤Ÿä»RGBè§†é¢‘åˆ›å»ºå…·æœ‰è¯¦ç»†åŠ¨æ€å¤–è§‚çš„åŠ¨ç”»è™šæ‹ŸåŒ–èº«ã€‚* å®éªŒè¡¨æ˜è¯¥æ–¹æ³•ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å·¥ä½œé‡ï¼š* éœ€è¦æ„å»ºçº¹ç†è¯å…¸ï¼ŒåæŠ•å½±å¤šè§†è§’å›¾åƒå¹¶ç”Ÿæˆçº¹ç†è´´å›¾ã€‚* éœ€è¦è®¾è®¡èº«ä½“éƒ¨ä½ç¼–ç ç­–ç•¥ï¼Œåˆ†å±‚æŸ¥è¯¢å§¿æ€ç‰¹å¾ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-d5ef7cbf30d33e46f182e7d8ab7d3f6a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2b1fc6c5e328a61ca82826325adda143.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-274cb92781d5841c23c9d32fbb82c09e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0fb1457171dba2509eedb86dc74d5975.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f2d2ccbdda2a0bc805f942fdcc2c0204.jpg" align="middle"></details></ol>]]></content>
    
    
    <summary type="html">å…ƒå®‡å®™/è™šæ‹Ÿäºº æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-04-06  GeneAvatar Generic Expression-Aware Volumetric Head Avatar Editing from   a Single Image</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="å…ƒå®‡å®™/è™šæ‹Ÿäºº" scheme="https://kedreamix.github.io/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/"/>
    
  </entry>
  
  <entry>
    <title>å…ƒå®‡å®™/è™šæ‹Ÿäºº</title>
    <link href="https://kedreamix.github.io/2024/04/01/Paper/2024-04-01/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/"/>
    <id>https://kedreamix.github.io/2024/04/01/Paper/2024-04-01/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/</id>
    <published>2024-04-01T05:22:12.000Z</published>
    <updated>2024-04-01T05:22:12.538Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-04-01-æ›´æ–°"><a href="#2024-04-01-æ›´æ–°" class="headerlink" title="2024-04-01 æ›´æ–°"></a>2024-04-01 æ›´æ–°</h1><h2 id="Make-Your-Anchor-A-Diffusion-based-2D-Avatar-Generation-Framework"><a href="#Make-Your-Anchor-A-Diffusion-based-2D-Avatar-Generation-Framework" class="headerlink" title="Make-Your-Anchor: A Diffusion-based 2D Avatar Generation Framework"></a>Make-Your-Anchor: A Diffusion-based 2D Avatar Generation Framework</h2><p><strong>Authors:Ziyao Huang, Fan Tang, Yong Zhang, Xiaodong Cun, Juan Cao, Jintao Li, Tong-Yee Lee</strong></p><p>Despite the remarkable process of talking-head-based avatar-creating solutions, directly generating anchor-style videos with full-body motions remains challenging. In this study, we propose Make-Your-Anchor, a novel system necessitating only a one-minute video clip of an individual for training, subsequently enabling the automatic generation of anchor-style videos with precise torso and hand movements. Specifically, we finetune a proposed structure-guided diffusion model on input video to render 3D mesh conditions into human appearances. We adopt a two-stage training strategy for the diffusion model, effectively binding movements with specific appearances. To produce arbitrary long temporal video, we extend the 2D U-Net in the frame-wise diffusion model to a 3D style without additional training cost, and a simple yet effective batch-overlapped temporal denoising module is proposed to bypass the constraints on video length during inference. Finally, a novel identity-specific face enhancement module is introduced to improve the visual quality of facial regions in the output videos. Comparative experiments demonstrate the effectiveness and superiority of the system in terms of visual quality, temporal coherence, and identity preservation, outperforming SOTA diffusion/non-diffusion methods. Project page: \url{<a href="https://github.com/ICTMCG/Make-Your-Anchor}">https://github.com/ICTMCG/Make-Your-Anchor}</a>. </p><p><a href="http://arxiv.org/abs/2403.16510v1">PDF</a> accepted at CVPR2024</p><p><strong>Summary</strong><br>é€šè¿‡ä»…éœ€ä¸€åˆ†é’Ÿçš„ä¸ªäººè§†é¢‘è®­ç»ƒï¼Œæå‡ºäº†ä¸€ç§ç”Ÿæˆå…¨èº«åŠ¨ä½œé”šæ’­é£æ ¼è§†é¢‘çš„æ–°ç³»ç»Ÿ Make-Your-Anchorã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºäº†ä¸€ç§åŸºäºç»“æ„å¼•å¯¼æ‰©æ•£æ¨¡å‹ï¼Œå°† 3D ç½‘æ ¼æ¡ä»¶æ¸²æŸ“ä¸ºäººç‰©å¤–è§‚ã€‚</li><li>é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œæœ‰æ•ˆç»‘å®šåŠ¨ä½œä¸ç‰¹å®šå¤–è§‚ã€‚</li><li>æ‰©å±•å¸§å†…æ‰©æ•£æ¨¡å‹ä¸­çš„ 2D U-Net åˆ° 3D é£æ ¼ï¼Œæ— éœ€é¢å¤–è®­ç»ƒæˆæœ¬ã€‚</li><li>æå‡ºä¸€ä¸ªç®€å•æœ‰æ•ˆçš„æ‰¹é‡é‡å æ—¶é—´å»å™ªæ¨¡å—ï¼Œç»•è¿‡æ¨ç†æ—¶çš„è§†é¢‘é•¿åº¦é™åˆ¶ã€‚</li><li>å¼•å…¥ä¸€ä¸ªæ–°é¢–çš„èº«ä»½ç‰¹å®šé¢éƒ¨å¢å¼ºæ¨¡å—ï¼Œä»¥æé«˜è¾“å‡ºè§†é¢‘ä¸­é¢éƒ¨åŒºåŸŸçš„è§†è§‰è´¨é‡ã€‚</li><li>ä¸ SOTA æ‰©æ•£/éæ‰©æ•£æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨è§†è§‰è´¨é‡ã€æ—¶é—´è¿è´¯æ€§å’Œèº«ä»½ä¿ç•™æ–¹é¢è¯æ˜äº†è¯¥ç³»ç»Ÿçš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</li><li>é¡¹ç›®ä¸»é¡µï¼š\url{<a href="https://github.com/ICTMCG/Make-Your-Anchor}ã€‚">https://github.com/ICTMCG/Make-Your-Anchor}ã€‚</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šMake-Your-Anchorï¼šåŸºäºæ‰©æ•£çš„äºŒç»´è™šæ‹Ÿå½¢è±¡ç”Ÿæˆæ¡†æ¶</li><li>ä½œè€…ï¼šé»„å­å°§ï¼Œå”å‡¡ï¼Œå¼ å‹‡ï¼Œæ‘æ™“ä¸œï¼Œæ›¹å¨Ÿï¼Œæé‡‘æ¶›ï¼ŒæåŒä¹‰</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šä¸­å›½ç§‘å­¦é™¢è®¡ç®—æŠ€æœ¯ç ”ç©¶æ‰€</li><li>å…³é”®è¯ï¼šè™šæ‹Ÿå½¢è±¡ç”Ÿæˆï¼Œæ‰©æ•£æ¨¡å‹ï¼Œè¿åŠ¨æ•æ‰ï¼Œè¯­éŸ³é©±åŠ¨åŠ¨ç”»</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.16510ï¼ŒGithub ä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šç›®å‰åŸºäºè¯´è¯äººå¤´éƒ¨åˆ›å»ºè™šæ‹Ÿå½¢è±¡çš„è§£å†³æ–¹æ¡ˆå·²å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ç›´æ¥ç”Ÿæˆå…·æœ‰å…¨èº«åŠ¨ä½œçš„ä¸»æ’­é£æ ¼è§†é¢‘ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li></ol><p>ï¼ˆ2ï¼‰ï¼šè¿‡å»çš„æ–¹æ³•åŠé—®é¢˜ï¼šä»¥å¾€çš„æ–¹æ³•ä¸»è¦åŸºäº GAN è¿›è¡Œå±€éƒ¨ç¼–è¾‘æˆ–åˆ©ç”¨è¿åŠ¨è¿ç§»æŠ€æœ¯ï¼Œä½†è¿™äº›æ–¹æ³•è¦ä¹ˆè‡ªç”±åº¦å—é™ï¼Œè¦ä¹ˆåŠ¨ä½œä¸ç‰¹å®šå¤–è§‚çš„ç»‘å®šä¸å¤Ÿç´§å¯†ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šæå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡º Make-Your-Anchor ç³»ç»Ÿï¼Œä»…éœ€ä¸€ä¸ªä¸€åˆ†é’Ÿçš„ä¸ªäººè§†é¢‘å³å¯è®­ç»ƒï¼Œå®ç°è‡ªåŠ¨ç”Ÿæˆå…·æœ‰ç²¾ç¡®èº¯å¹²å’Œæ‰‹éƒ¨åŠ¨ä½œçš„ä¸»æ’­é£æ ¼è§†é¢‘ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨ç»“æ„å¼•å¯¼æ‰©æ•£æ¨¡å‹ï¼Œå°†ä¸‰ç»´ç½‘æ ¼æ¡ä»¶æ¸²æŸ“æˆäººç‰©å¤–è§‚ï¼Œå¹¶é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œæœ‰æ•ˆåœ°å°†åŠ¨ä½œä¸ç‰¹å®šå¤–è§‚ç»‘å®šã€‚ä¸ºäº†ç”Ÿæˆä»»æ„é•¿åº¦çš„æ—¶é—´è§†é¢‘ï¼Œå°†å¸§çº§æ‰©æ•£æ¨¡å‹ä¸­çš„äºŒç»´ U-Net æ‰©å±•ä¸ºä¸‰ç»´å½¢å¼ï¼Œå¹¶æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ‰¹æ¬¡é‡å æ—¶é—´å»å™ªæ¨¡å—ï¼Œä»¥ç»•è¿‡æ¨ç†æœŸé—´è§†é¢‘é•¿åº¦çš„é™åˆ¶ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†æ–°çš„ç‰¹å®šäºèº«ä»½çš„é¢éƒ¨å¢å¼ºæ¨¡å—ï¼Œä»¥æé«˜è¾“å‡ºè§†é¢‘ä¸­é¢éƒ¨åŒºåŸŸçš„è§†è§‰è´¨é‡ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šæ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šMake-Your-Anchor ç³»ç»Ÿåœ¨è§†è§‰è´¨é‡ã€æ—¶é—´è¿è´¯æ€§å’Œèº«ä»½ä¿ç•™æ–¹é¢å‡ä¼˜äº SOTA æ‰©æ•£å’Œéæ‰©æ•£æ–¹æ³•ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</p><ol><li><p>æ–¹æ³•ï¼š(1) ç»“æ„å¼•å¯¼æ‰©æ•£æ¨¡å‹ï¼ˆSGDMï¼‰ï¼šå°† 3D ç½‘æ ¼æ¡ä»¶åµŒå…¥ç”Ÿæˆè¿‡ç¨‹ï¼Œå­¦ä¹ å§¿åŠ¿åˆ°ç›®æ ‡è§†é¢‘å¸§çš„å¯¹åº”æ˜ å°„ã€‚(2) ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼šé¢„è®­ç»ƒå¢å¼ºæ¨¡å‹ç”ŸæˆåŠ¨ä½œèƒ½åŠ›ï¼Œå¾®è°ƒç»‘å®šåŠ¨ä½œä¸ç‰¹å®šå¤–è§‚ã€‚(3) æ‰¹æ¬¡é‡å æ—¶é—´å»å™ªï¼šå°† 2D U-Net æ‰©å±•ä¸º 3D å½¢å¼ï¼Œæå‡ºæ‰¹æ¬¡é‡å æ—¶é—´å»å™ªæ¨¡å—ç”Ÿæˆä»»æ„é•¿åº¦çš„æ—¶é—´è§†é¢‘ã€‚(4) èº«ä»½ç‰¹å®šé¢éƒ¨å¢å¼ºæ¨¡å—ï¼šé€šè¿‡è£å‰ªå’Œæ··åˆæ“ä½œï¼Œä¿®æ”¹ç”Ÿæˆçš„èº«ä½“ä¸­çš„é¢éƒ¨åŒºåŸŸï¼Œæé«˜é¢éƒ¨åŒºåŸŸçš„è§†è§‰è´¨é‡ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬å·¥ä½œçš„æ„ä¹‰ï¼šæœ¬æ–‡æå‡º â€œMake-Your-Anchorâ€ï¼Œä¸€ä¸ªåŸºäºæ‰©æ•£çš„äºŒç»´è™šæ‹Ÿå½¢è±¡ç”Ÿæˆæ¡†æ¶ï¼Œç”¨äºç”Ÿæˆé€¼çœŸä¸”é«˜è´¨é‡çš„ä¸»æ’­é£æ ¼äººç‰©è§†é¢‘ã€‚è¯¥æ¡†æ¶åˆ›æ–°æ€§åœ°æå‡ºäº†å¸§çº§çš„è¿åŠ¨åˆ°å¤–è§‚æ‰©æ•£ï¼Œé€šè¿‡ç»“æ„å¼•å¯¼æ‰©æ•£æ¨¡å‹å’Œä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œå®ç°äº†ç‰¹å®šå¤–è§‚ä¸åŠ¨ä½œçš„ç»‘å®šã€‚ä¸ºäº†ç”Ÿæˆæ—¶é—´ä¸€è‡´çš„äººåƒè§†é¢‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ— è®­ç»ƒç­–ç•¥ï¼Œå°†å›¾åƒæ‰©æ•£æ¨¡å‹æ‰©å±•ä¸ºè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªæ‰¹æ¬¡é‡å æ—¶é—´å»å™ªç®—æ³•ï¼Œä»¥å…‹æœç”Ÿæˆè§†é¢‘é•¿åº¦çš„é™åˆ¶ã€‚é’ˆå¯¹æ•´ä½“äººç‰©ç”Ÿæˆä¸­é¢éƒ¨ç»†èŠ‚éš¾ä»¥é‡å»ºçš„è§‚å¯Ÿï¼Œæˆ‘ä»¬å¼•å…¥äº†èº«ä»½ç‰¹å®šçš„é¢éƒ¨å¢å¼ºã€‚é€šè¿‡æˆ‘ä»¬æ•´ä¸ªç³»ç»Ÿæ–¹æ³•çš„èåˆï¼Œæˆ‘ä»¬çš„æ¡†æ¶æˆåŠŸåœ°ç”Ÿæˆäº†é«˜è´¨é‡ã€ç»“æ„ä¿æŒå’Œæ—¶é—´ä¸€è‡´çš„ä¸»æ’­é£æ ¼äººç‰©è§†é¢‘ï¼Œè¿™å¯èƒ½ä¸ºäºŒç»´æ•°å­—è™šæ‹Ÿå½¢è±¡çš„å¹¿æ³›åº”ç”¨æŠ€æœ¯æä¾›å‚è€ƒä»·å€¼ã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºç»“æ„å¼•å¯¼æ‰©æ•£æ¨¡å‹ï¼Œå°†ä¸‰ç»´ç½‘æ ¼æ¡ä»¶åµŒå…¥ç”Ÿæˆè¿‡ç¨‹ï¼Œå­¦ä¹ å§¿åŠ¿åˆ°ç›®æ ‡è§†é¢‘å¸§çš„å¯¹åº”æ˜ å°„ã€‚</li><li>é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œé¢„è®­ç»ƒå¢å¼ºæ¨¡å‹ç”ŸæˆåŠ¨ä½œèƒ½åŠ›ï¼Œå¾®è°ƒç»‘å®šåŠ¨ä½œä¸ç‰¹å®šå¤–è§‚ã€‚</li><li>æå‡ºæ‰¹æ¬¡é‡å æ—¶é—´å»å™ªï¼Œå°†äºŒç»´ U-Net æ‰©å±•ä¸ºä¸‰ç»´å½¢å¼ï¼Œç”Ÿæˆä»»æ„é•¿åº¦çš„æ—¶é—´è§†é¢‘ã€‚</li><li>å¼•å…¥èº«ä»½ç‰¹å®šçš„é¢éƒ¨å¢å¼ºæ¨¡å—ï¼Œé€šè¿‡è£å‰ªå’Œæ··åˆæ“ä½œï¼Œä¿®æ”¹ç”Ÿæˆçš„èº«ä½“ä¸­çš„é¢éƒ¨åŒºåŸŸï¼Œæé«˜é¢éƒ¨åŒºåŸŸçš„è§†è§‰è´¨é‡ã€‚æ€§èƒ½ï¼š</li><li>åœ¨è§†è§‰è´¨é‡ã€æ—¶é—´è¿è´¯æ€§å’Œèº«ä»½ä¿ç•™æ–¹é¢å‡ä¼˜äº SOTA æ‰©æ•£å’Œéæ‰©æ•£æ–¹æ³•ã€‚å·¥ä½œé‡ï¼š</li><li>è®­ç»ƒæ•°æ®é‡è¾ƒå¤§ï¼Œéœ€è¦å¤§é‡çš„äººç‰©è§†é¢‘æ•°æ®ã€‚</li><li>è®­ç»ƒæ—¶é—´è¾ƒé•¿ï¼Œéœ€è¦é«˜æ€§èƒ½è®¡ç®—èµ„æºã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-31e07e2070d0183b4685c1e857aaf0a2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d20d65a2e0024c9d9bc380b7f7ba43b0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-771450d6d33a69293597f486c329a82a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4f2a829065c463be027e4b423c4e43c8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6ad4738b2a0c37e9688fb722556c4213.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b6700b1c5fd0f20662f87b175ebf869f.jpg" align="middle"></details><h2 id="UV-Gaussians-Joint-Learning-of-Mesh-Deformation-and-Gaussian-Textures-for-Human-Avatar-Modeling"><a href="#UV-Gaussians-Joint-Learning-of-Mesh-Deformation-and-Gaussian-Textures-for-Human-Avatar-Modeling" class="headerlink" title="UV Gaussians: Joint Learning of Mesh Deformation and Gaussian Textures   for Human Avatar Modeling"></a>UV Gaussians: Joint Learning of Mesh Deformation and Gaussian Textures   for Human Avatar Modeling</h2><p><strong>Authors:Yujiao Jiang, Qingmin Liao, Xiaoyu Li, Li Ma, Qi Zhang, Chaopeng Zhang, Zongqing Lu, Ying Shan</strong></p><p>Reconstructing photo-realistic drivable human avatars from multi-view image sequences has been a popular and challenging topic in the field of computer vision and graphics. While existing NeRF-based methods can achieve high-quality novel view rendering of human models, both training and inference processes are time-consuming. Recent approaches have utilized 3D Gaussians to represent the human body, enabling faster training and rendering. However, they undermine the importance of the mesh guidance and directly predict Gaussians in 3D space with coarse mesh guidance. This hinders the learning procedure of the Gaussians and tends to produce blurry textures. Therefore, we propose UV Gaussians, which models the 3D human body by jointly learning mesh deformations and 2D UV-space Gaussian textures. We utilize the embedding of UV map to learn Gaussian textures in 2D space, leveraging the capabilities of powerful 2D networks to extract features. Additionally, through an independent Mesh network, we optimize pose-dependent geometric deformations, thereby guiding Gaussian rendering and significantly enhancing rendering quality. We collect and process a new dataset of human motion, which includes multi-view images, scanned models, parametric model registration, and corresponding texture maps. Experimental results demonstrate that our method achieves state-of-the-art synthesis of novel view and novel pose. The code and data will be made available on the homepage <a href="https://alex-jyj.github.io/UV-Gaussians/">https://alex-jyj.github.io/UV-Gaussians/</a> once the paper is accepted. </p><p><a href="http://arxiv.org/abs/2403.11589v1">PDF</a> </p><p><strong>æ‘˜è¦</strong><br>é€šè¿‡è”åˆå­¦ä¹ ç½‘æ ¼å˜å½¢å’ŒäºŒç»´ UV ç©ºé—´é«˜æ–¯çº¹ç†ï¼Œç»“åˆ UV é«˜æ–¯æ¨¡å‹é‡å»ºé€¼çœŸçš„å¯é©¾é©¶äººä½“è™šæ‹Ÿäººã€‚</p><p><strong>å…³é”®è¦ç‚¹</strong></p><ul><li>åˆ©ç”¨ä¸‰ç»´é«˜æ–¯ä½“è¡¨ç¤ºäººä½“ï¼Œå®ç°å¿«é€Ÿè®­ç»ƒå’Œæ¸²æŸ“ã€‚</li><li>æå‡º UV é«˜æ–¯æ¨¡å‹ï¼Œè”åˆå­¦ä¹ ç½‘æ ¼å˜å½¢å’ŒäºŒç»´ UV ç©ºé—´é«˜æ–¯çº¹ç†ã€‚</li><li>é€šè¿‡ UV æ˜ å°„åµŒå…¥ï¼Œåœ¨äºŒç»´ç©ºé—´å­¦ä¹ é«˜æ–¯çº¹ç†ï¼Œå¢å¼ºçº¹ç†æ¸…æ™°åº¦ã€‚</li><li>ç‹¬ç«‹ç½‘æ ¼ç½‘ç»œä¼˜åŒ–å§¿æ€ç›¸å…³çš„å‡ ä½•å˜å½¢ï¼Œå¼•å¯¼é«˜æ–¯æ¸²æŸ“ã€‚</li><li>æ”¶é›†å¹¶å¤„ç†åŒ…å«å¤šè§†è§’å›¾åƒã€æ‰«ææ¨¡å‹ã€å‚æ•°æ¨¡å‹é…å‡†å’Œå¯¹åº”çº¹ç†è´´å›¾çš„æ–°äººä½“åŠ¨ä½œæ•°æ®é›†ã€‚</li><li>åœ¨å…¨æ–°è§†å›¾å’Œå…¨æ–°å§¿æ€åˆæˆæ–¹é¢è¾¾åˆ°æœ€å…ˆè¿›çš„æ°´å¹³ã€‚</li><li>å…¬å¼€ä»£ç å’Œæ•°æ®ï¼Œä¿ƒè¿›è¿›ä¸€æ­¥ç ”ç©¶ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šUV é«˜æ–¯ï¼šç½‘æ ¼æ–°è§†è§’çš„è”åˆå­¦ä¹ </li><li>ä½œè€…ï¼šY. Jiangã€Z. Zhouã€J. Huangã€T. Zhangã€X. Hanã€Y. Chenã€Y. Liuã€L. Liu</li><li>å•ä½ï¼šæ— </li><li>å…³é”®è¯ï¼šHuman ModelingÂ·Neural RenderingÂ·Gaussian Splatting</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2212.05845   Github é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šä»å¤šè§†è§’å›¾åƒåºåˆ—é‡å»ºé€¼çœŸçš„å¯é©¾é©¶äººå½¢åŒ–èº«ä¸€ç›´æ˜¯è®¡ç®—æœºè§†è§‰å’Œå›¾å½¢é¢†åŸŸçš„ä¸€ä¸ªçƒ­é—¨ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„è¯¾é¢˜ã€‚è™½ç„¶ç°æœ‰çš„åŸºäº NeRF çš„æ–¹æ³•å¯ä»¥å®ç°é«˜è´¨é‡çš„äººä½“æ¨¡å‹æ–°è§†è§’æ¸²æŸ“ï¼Œä½†è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹éƒ½å¾ˆè€—æ—¶ã€‚   ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠé—®é¢˜ï¼šæœ€è¿‘çš„ç ”ç©¶åˆ©ç”¨ 3D é«˜æ–¯è¡¨ç¤ºäººä½“ï¼Œä»è€Œå®ç°æ›´å¿«çš„è®­ç»ƒå’Œæ¸²æŸ“ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä½ä¼°äº†ç½‘æ ¼æŒ‡å¯¼çš„é‡è¦æ€§ï¼Œå¹¶ç›´æ¥åœ¨ 3D ç©ºé—´ä¸­é¢„æµ‹é«˜æ–¯ï¼Œè€Œç½‘æ ¼æŒ‡å¯¼è¾ƒç²—ç³™ã€‚è¿™é˜»ç¢äº†é«˜æ–¯å­¦ä¹ è¿‡ç¨‹ï¼Œå¹¶å€¾å‘äºäº§ç”Ÿæ¨¡ç³Šçš„çº¹ç†ã€‚   ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šå› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº† UV é«˜æ–¯ï¼Œå®ƒé€šè¿‡è”åˆå­¦ä¹ ç½‘æ ¼å˜å½¢å’Œ 2D UV ç©ºé—´é«˜æ–¯çº¹ç†å¯¹ 3D äººä½“è¿›è¡Œå»ºæ¨¡ã€‚æˆ‘ä»¬åˆ©ç”¨ UV è´´å›¾çš„åµŒå…¥åœ¨ 2D ç©ºé—´ä¸­å­¦ä¹ é«˜æ–¯çº¹ç†ï¼Œåˆ©ç”¨å¼ºå¤§çš„ 2D ç½‘ç»œæå–ç‰¹å¾çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œé€šè¿‡ä¸€ä¸ªç‹¬ç«‹çš„ç½‘æ ¼ç½‘ç»œï¼Œæˆ‘ä»¬ä¼˜åŒ–äº†ä¸å§¿åŠ¿ç›¸å…³çš„å‡ ä½•å˜å½¢ï¼Œä»è€ŒæŒ‡å¯¼é«˜æ–¯æ¸²æŸ“å¹¶æ˜¾è‘—æé«˜æ¸²æŸ“è´¨é‡ã€‚æˆ‘ä»¬æ”¶é›†å¹¶å¤„ç†äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…æ‹¬å¤šè§†è§’å›¾åƒã€æ‰«ææ¨¡å‹ã€å‚æ•°æ¨¡å‹é…å‡†å’Œç›¸åº”çš„çº¹ç†è´´å›¾ã€‚   ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šå®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†æ–°è§†è§’å’Œæ–°å§¿åŠ¿åˆæˆçš„æ–°æŠ€æœ¯ã€‚è¯¥ä»£ç å’Œæ•°æ®å°†åœ¨è®ºæ–‡è¢«æ¥å—ååœ¨ä¸»é¡µ https://alex-jyj.github.io/UVGaussians/ ä¸Šæä¾›ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰æ•°æ®å¤„ç†ï¼šä½¿ç”¨ OpenPose ä¼°è®¡å¤šè§†è§’å›¾åƒçš„ 2D å…³é”®ç‚¹ï¼Œé€šè¿‡ä¸‰è§’æµ‹é‡ä¼°è®¡ 3D å…³é”®ç‚¹ï¼Œç„¶åä½¿ç”¨ EasyMocap æ‹Ÿåˆ SMPL-X æ¨¡å‹ã€‚ä½¿ç”¨ MVS æ–¹æ³•é‡å»ºæ‰«æç½‘æ ¼ï¼Œä¼˜åŒ– SMPL-X ç½‘æ ¼çš„é¡¶ç‚¹ä½ç§»ä»¥å°†å…¶ä¸æ‰«ææ¨¡å‹çš„ç½‘æ ¼å¯¹é½ï¼Œä»è€Œå¾—åˆ° SMPLX-D æ¨¡å‹ã€‚ï¼ˆ2ï¼‰åŸºäºå§¿åŠ¿çš„ç½‘æ ¼å˜å½¢ï¼šé€‰æ‹©ä¸€ä¸ªæ¥è¿‘ T å§¿åŠ¿çš„å¸§ä½œä¸ºå‚è€ƒï¼Œä½¿ç”¨çº¿æ€§æ··åˆè’™çš® (LBS) å°†å…¶å˜å½¢ä¸ºæ ‡å‡† T å§¿åŠ¿ã€‚å°†è¿™ä¸ªæ ‡å‡† T å§¿åŠ¿ä½œä¸ºæ‰€æœ‰å§¿åŠ¿çš„æ¨¡æ¿ç½‘æ ¼ã€‚é€šè¿‡åŸºäºå§¿åŠ¿å‚æ•°çš„ LBS å˜å½¢ç²—ç³™ç½‘æ ¼ï¼Œå¾—åˆ°å§¿åŠ¿ç½‘æ ¼ã€‚å¼•å…¥ MeshU-Net æ¥å­¦ä¹ åŸºäºå§¿åŠ¿çš„ç½‘æ ¼å˜å½¢ã€‚å°†ç½‘æ ¼é¡¶ç‚¹åæ ‡å…‰æ …åŒ–ä¸º UV ç©ºé—´ï¼Œç”Ÿæˆä½ç½®å›¾ï¼Œä½œä¸ºç½‘æ ¼ç½‘ç»œ M çš„è¾“å…¥ã€‚ç½‘æ ¼ç½‘ç»œ M é¢„æµ‹åŸºäºè¾“å…¥ä½ç½®å›¾çš„é¡¶ç‚¹åç§»å›¾ã€‚å°†æ¯ä¸ªé¡¶ç‚¹çš„ç›¸åº”åç§»ä»åç§»å›¾ä¸­æ’å€¼ï¼Œå¾—åˆ°ç½‘æ ¼é¡¶ç‚¹åç§»ã€‚å°†æ­¤åç§»æ·»åŠ åˆ° T å§¿åŠ¿ç½‘æ ¼ä¸­ï¼Œç„¶åä½¿ç”¨ LBS å˜æ¢åˆ°å§¿åŠ¿ç©ºé—´ã€‚æœ€ç»ˆå¾—åˆ°èƒ½å¤Ÿæ•æ‰åŸºäºå§¿åŠ¿çš„å‡ ä½•å˜åŒ–çš„ç»†åŒ–ç½‘æ ¼ã€‚ï¼ˆ3ï¼‰åŸºäºå§¿åŠ¿çš„é«˜æ–¯çº¹ç†ï¼šå°† 3D é«˜æ–¯å‚æ•°åŒ–ä¸º UV ç©ºé—´çš„é«˜æ–¯çº¹ç†ï¼Œé€šè¿‡ UV æ˜ å°„å°†æ¯ä¸ªåƒç´ æŠ•å½±åˆ° 3D é«˜æ–¯ã€‚å°†æ‰€æœ‰å§¿åŠ¿çš„å¹³å‡çº¹ç†å›¾ä½œä¸ºè¾“å…¥ï¼Œä¸º 3D é«˜æ–¯æä¾›åˆå§‹é¢œè‰²ä¿¡æ¯ã€‚è¿˜å‘ç½‘ç»œæä¾›ä½ç½®å›¾ä»¥æä¾›åƒç´ çº§çš„å§¿åŠ¿ä¿¡æ¯ã€‚ç½‘ç»œè¿˜å—è§†å›¾æ–¹å‘å‘é‡çš„å¼•å¯¼ï¼Œä»¥å»ºæ¨¡è§†å›¾ç›¸å…³çš„å˜åŒ–ã€‚ä½¿ç”¨ StyleUNet æ¶æ„ï¼Œç½‘ç»œè¾“å‡ºå¤šä¸ªé«˜æ–¯çº¹ç†ï¼ŒåŒ…å« 3D é«˜æ–¯æ‰€éœ€çš„æ‰€æœ‰å‚æ•°ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç§°ä¸º UV é«˜æ–¯çš„é‡å»ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°† 3D é«˜æ–¯ä¸ UV ç©ºé—´è¡¨ç¤ºç›¸ç»“åˆã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿä»å¤šè§†è§’å›¾åƒé‡å»ºé€¼çœŸçš„ã€å§¿åŠ¿é©±åŠ¨çš„åŒ–èº«æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä»¥æ¨¡å‹é¡¶ç‚¹çš„ä½ç§»å›¾ä½œä¸ºè¾“å…¥ï¼Œé€šè¿‡ MeshU-Net å­¦ä¹ åŸºäºå§¿åŠ¿çš„å‡ ä½•å˜å½¢ï¼Œå¹¶é€šè¿‡ GaussianU-Net å­¦ä¹ åµŒå…¥åœ¨ UV ç©ºé—´ä¸­çš„é«˜æ–¯ç‚¹å±æ€§ã€‚éšåï¼Œåœ¨ç²¾ç»†ç½‘æ ¼çš„å¼•å¯¼ä¸‹ï¼Œå¯¹é«˜æ–¯ç‚¹è¿›è¡Œæ¸²æŸ“ï¼Œä»¥è·å¾—ä»»æ„è§†ç‚¹çš„æ¸²æŸ“å›¾åƒã€‚é€šè¿‡ç»“åˆç²¾ç»†çš„å‡ ä½•æŒ‡å¯¼å¹¶åˆ©ç”¨ UV ç©ºé—´ä¸­å¼ºå¤§çš„ 2D ç½‘ç»œçš„ç‰¹å¾å­¦ä¹ èƒ½åŠ›ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å®éªŒä¸­å®ç°äº†æ–°è§†è§’å’Œæ–°å§¿åŠ¿åˆæˆæ–¹é¢çš„æœ€å…ˆè¿›ç»“æœã€‚å±€é™æ€§ã€‚å°½ç®¡å–å¾—äº†æˆå°±ï¼Œä½†æˆ‘ä»¬çš„æ–¹æ³•å—åˆ¶äºå¯¹æ‰«æç½‘æ ¼çš„ä¾èµ–æ€§ã€‚è™½ç„¶å¯ä»¥ä½¿ç”¨ MeshU-Net ä¼˜åŒ–è¾ƒå°çš„æ‹Ÿåˆè¯¯å·®ï¼Œä½†è¾ƒå¤§çš„è¯¯å·®å¯èƒ½ä¼šå½±å“æ–¹æ³•çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ”¶é›†çš„æ•°æ®é›†ä¸åŒ…æ‹¬é•¿è£™ç­‰æåº¦å®½æ¾çš„æœè£…ã€‚åœ¨æœªæ¥çš„ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬è®¡åˆ’åœ¨åŒ…å«å¤šè§†è§’å›¾åƒå’Œæ‰«ææ¨¡å‹çš„æ›´å¤šå¯ç”¨æ•°æ®é›†ä¸Šè¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯æ¢ç´¢å…·æœ‰å›°éš¾å§¿åŠ¿çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„æœè£…ç±»å‹ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šç»“åˆ 3D é«˜æ–¯å’Œ UV ç©ºé—´è¡¨ç¤ºï¼Œæå‡ºäº†ä¸€ç§æ–°çš„é‡å»ºæ–¹æ³•ï¼›é€šè¿‡ MeshU-Net å­¦ä¹ åŸºäºå§¿åŠ¿çš„å‡ ä½•å˜å½¢ï¼Œé€šè¿‡ GaussianU-Net å­¦ä¹ é«˜æ–¯çº¹ç†ï¼›æ€§èƒ½ï¼šåœ¨æ–°è§†è§’å’Œæ–°å§¿åŠ¿åˆæˆå®éªŒä¸­å®ç°äº†æœ€å…ˆè¿›çš„ç»“æœï¼›å·¥ä½œé‡ï¼šæ•°æ®å¤„ç†å’Œç½‘ç»œè®­ç»ƒéœ€è¦å¤§é‡è®¡ç®—èµ„æºã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-a863ff88a8f3aab922fde1833cf3125b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6c24e3d34d46677eafb334d061117f93.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e62a000f486adba73f5ad94566312cdc.jpg" align="middle"></details><h2 id="NECA-Neural-Customizable-Human-Avatar"><a href="#NECA-Neural-Customizable-Human-Avatar" class="headerlink" title="NECA: Neural Customizable Human Avatar"></a>NECA: Neural Customizable Human Avatar</h2><p><strong>Authors:Junjin Xiao, Qing Zhang, Zhan Xu, Wei-Shi Zheng</strong></p><p>Human avatar has become a novel type of 3D asset with various applications. Ideally, a human avatar should be fully customizable to accommodate different settings and environments. In this work, we introduce NECA, an approach capable of learning versatile human representation from monocular or sparse-view videos, enabling granular customization across aspects such as pose, shadow, shape, lighting and texture. The core of our approach is to represent humans in complementary dual spaces and predict disentangled neural fields of geometry, albedo, shadow, as well as an external lighting, from which we are able to derive realistic rendering with high-frequency details via volumetric rendering. Extensive experiments demonstrate the advantage of our method over the state-of-the-art methods in photorealistic rendering, as well as various editing tasks such as novel pose synthesis and relighting. The code is available at <a href="https://github.com/iSEE-Laboratory/NECA">https://github.com/iSEE-Laboratory/NECA</a>. </p><p><a href="http://arxiv.org/abs/2403.10335v1">PDF</a> Accepted to CVPR 2024</p><p><strong>æ‘˜è¦</strong><br>åˆ©ç”¨å•è§†è§’æˆ–ç¨€ç–è§†ç‚¹è§†é¢‘å­¦ä¹ å¤šåŠŸèƒ½äººä½“è¡¨ç¤ºï¼Œå®ç°å§¿åŠ¿ã€é˜´å½±ã€å½¢çŠ¶ã€å…‰ç…§å’Œçº¹ç†ç­‰ç»†ç²’åº¦è‡ªå®šä¹‰ã€‚</p><p><strong>è¦ç‚¹</strong></p><ul><li>äººä½“åŒ–èº«ä¸ºä¸€ç§æ–°å‹ 3D èµ„äº§ï¼Œå…·å¤‡å¹¿æ³›åº”ç”¨ã€‚</li><li>ç†æƒ³çš„äººä½“åŒ–èº«åº”å®Œå…¨å¯å®šåˆ¶ï¼Œä»¥é€‚åº”ä¸åŒçš„è®¾ç½®å’Œç¯å¢ƒã€‚</li><li>å¼•å…¥ NECA æ–¹æ³•ï¼Œå¯ä»å•è§†è§’æˆ–ç¨€ç–è§†ç‚¹è§†é¢‘ä¸­å­¦ä¹ å¤šåŠŸèƒ½äººä½“è¡¨ç¤ºï¼Œå®ç°å§¿åŠ¿ã€é˜´å½±ã€å½¢çŠ¶ã€å…‰ç…§å’Œçº¹ç†ç­‰æ–¹é¢çš„ç²’åº¦å®šåˆ¶ã€‚</li><li>æ–¹æ³•æ ¸å¿ƒæ˜¯å°†äººè¡¨ç¤ºåœ¨äº’è¡¥çš„åŒç©ºé—´ä¸­ï¼Œå¹¶é¢„æµ‹å‡ ä½•ã€åç…§ç‡ã€é˜´å½±ä»¥åŠå¤–éƒ¨å…‰ç…§çš„çº ç¼ ç¥ç»åœºï¼Œä»è€Œé€šè¿‡ä½“ç§¯æ¸²æŸ“è·å¾—å…·æœ‰é«˜é¢‘ç»†èŠ‚çš„é€¼çœŸæ¸²æŸ“æ•ˆæœã€‚</li><li>å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é€¼çœŸæ¸²æŸ“ä»¥åŠæ–°é¢–å§¿åŠ¿åˆæˆå’Œé‡æ–°ç…§æ˜ç­‰å„ç§ç¼–è¾‘ä»»åŠ¡ä¸­ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šNECAï¼šç¥ç»å¯å®šåˆ¶äººå½¢åŒ–èº«</li><li>ä½œè€…ï¼šJunJin Xiaoã€Qing Zhangã€Zhan Xuã€Wei-Shi Zheng</li><li>éš¶å±ï¼šä¸­å±±å¤§å­¦è®¡ç®—æœºç§‘å­¦ä¸å·¥ç¨‹å­¦é™¢</li><li>å…³é”®è¯ï¼šç¥ç»å¯å®šåˆ¶äººå½¢åŒ–èº«ï¼Œç¥ç»åœºï¼Œå›¾åƒåˆæˆï¼Œäººåƒç¼–è¾‘</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.10335   Github é“¾æ¥ï¼šhttps://github.com/iSEE-Laboratory/NECA</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼š   éšç€å…ƒå®‡å®™ã€è¿œç¨‹ä¸´åœºå’Œ 3D æ¸¸æˆç­‰æ–°å…´åº”ç”¨çš„å…´èµ·ï¼Œå¯¹äººå½¢åŒ–èº«çš„éœ€æ±‚æ—¥ç›Šå¢é•¿ã€‚ç†æƒ³çš„äººå½¢åŒ–èº«åº”è¯¥å…·æœ‰é«˜åº¦çš„å¯å®šåˆ¶æ€§ï¼Œä»¥é€‚åº”ä¸åŒçš„åœºæ™¯å’Œç¯å¢ƒã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼š   ç°æœ‰çš„ç¥ç»äººå½¢åŒ–èº«å»ºæ¨¡æ–¹æ³•ä¸»è¦é’ˆå¯¹åŠ¨ç”»æˆ–é‡æ–°æ‰“å…‰ç›®çš„ï¼Œæ— æ³•ä¸ºåŒ–èº«æä¾›å…¨é¢çš„å®šåˆ¶èƒ½åŠ›ï¼Œä»è€Œé™åˆ¶äº†å…¶åœ¨å®é™…ä¸­çš„åº”ç”¨ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼š   NECA æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œå¯ä»¥ä»å•ç›®æˆ–ç¨€ç–å¤šè§†å›¾è§†é¢‘ä¸­å­¦ä¹ å®Œå…¨å¯å®šåˆ¶çš„ç¥ç»äººå½¢åŒ–èº«ï¼Œåœ¨ä»»ä½•æ–°çš„å§¿åŠ¿ã€è§†è§’å’Œå…‰ç…§ä¸‹è¿›è¡Œé€¼çœŸçš„æ¸²æŸ“ï¼Œå¹¶å…·æœ‰ç¼–è¾‘å½¢çŠ¶ã€çº¹ç†å’Œé˜´å½±çš„èƒ½åŠ›ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼š   NECA åœ¨é€¼çœŸæ¸²æŸ“ã€æ–°é¢–å§¿åŠ¿åˆæˆå’Œé‡æ–°æ‰“å…‰ç­‰å„ç§ç¼–è¾‘ä»»åŠ¡ä¸­ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚è¿™äº›æ€§èƒ½è¯æ˜äº† NECA åœ¨æ”¯æŒå…¶ç›®æ ‡æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼šé‡‡ç”¨åŒç©ºé—´åŠ¨æ€äººä½“è¡¨ç¤ºï¼Œåˆ†åˆ«åœ¨æ­£åˆ™ç©ºé—´å’Œè¡¨é¢ç©ºé—´ä¸­å­¦ä¹ äººä½“è¡¨ç¤ºï¼Œä»¥æ•æ‰é«˜é¢‘å§¿æ€æ„ŸçŸ¥ç‰¹å¾å’Œå‡ ä½•æ„ŸçŸ¥ä¸»ä½“ç‰¹å¾ï¼›ï¼ˆ2ï¼‰ï¼šå°†ç¥ç»åœºè§£è€¦ä¸ºä¸åŒçš„å±æ€§ï¼ŒåŒ…æ‹¬ SDFã€é˜´å½±ã€åç…§ç‡å’Œç¯å¢ƒå…‰ç…§ï¼Œå¹¶é€šè¿‡ä¸åŒçš„ MLP è§£ç æå–çš„ç‰¹å¾ï¼›ï¼ˆ3ï¼‰ï¼šä»¥è‡ªç›‘ç£çš„æ–¹å¼è®­ç»ƒæ•´ä¸ªç½‘ç»œï¼Œä»…ä½¿ç”¨å…‰åº¦æŸå¤±å’Œæ³•çº¿æ­£åˆ™åŒ–ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œæå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ NECAï¼Œè¯¥æ¡†æ¶å¯ä»¥ä»ç¨€ç–è§†å›¾ç”šè‡³å•ç›®è§†é¢‘ä¸­å­¦ä¹ å®Œå…¨å¯å®šåˆ¶çš„ç¥ç»äººå½¢åŒ–èº«ã€‚ä¸ä»¥å¾€æä¾›æœ‰é™ç¼–è¾‘èƒ½åŠ›çš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬æä¾›çš„ç¥ç»äººå½¢åŒ–èº«å…è®¸å¯¹å§¿åŠ¿ã€è§†ç‚¹ã€å…‰ç…§ã€å½¢çŠ¶ã€çº¹ç†å’Œé˜´å½±è¿›è¡Œé«˜ä¿çœŸç¼–è¾‘ã€‚å¹¿æ³›çš„å®éªŒéªŒè¯äº†æˆ‘ä»¬æ–¹æ³•çš„å¤šåŠŸèƒ½æ€§å’Œå®ç”¨æ€§ï¼Œä»¥åŠæˆ‘ä»¬åœ¨æ–°é¢–å§¿åŠ¿åˆæˆå’Œé‡æ–°æ‰“å…‰æ–¹é¢å¯¹ç°æœ‰æŠ€æœ¯æ°´å¹³çš„æ”¹è¿›ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„å·¥ä½œå¯ä»¥ä¸ºå®šåˆ¶åŒ–ç¥ç»äººå½¢åŒ–èº«çš„åˆ›å»ºåŠå…¶ç›¸å…³åº”ç”¨æä¾›å¯å‘ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§åŒç©ºé—´åŠ¨æ€äººä½“è¡¨ç¤ºï¼Œè¯¥è¡¨ç¤ºåˆ†åˆ«åœ¨æ­£åˆ™ç©ºé—´å’Œè¡¨é¢ç©ºé—´ä¸­å­¦ä¹ äººä½“è¡¨ç¤ºï¼Œä»¥æ•æ‰é«˜é¢‘å§¿æ€æ„ŸçŸ¥ç‰¹å¾å’Œå‡ ä½•æ„ŸçŸ¥ä¸»ä½“ç‰¹å¾ã€‚</li><li>å°†ç¥ç»åœºè§£è€¦ä¸ºä¸åŒçš„å±æ€§ï¼ŒåŒ…æ‹¬ SDFã€é˜´å½±ã€åç…§ç‡å’Œç¯å¢ƒå…‰ç…§ï¼Œå¹¶é€šè¿‡ä¸åŒçš„ MLP è§£ç æå–çš„ç‰¹å¾ã€‚</li><li>ä»¥è‡ªç›‘ç£çš„æ–¹å¼è®­ç»ƒæ•´ä¸ªç½‘ç»œï¼Œä»…ä½¿ç”¨å…‰åº¦æŸå¤±å’Œæ³•çº¿æ­£åˆ™åŒ–ã€‚æ€§èƒ½ï¼š</li><li>åœ¨é€¼çœŸæ¸²æŸ“ã€æ–°é¢–å§¿åŠ¿åˆæˆå’Œé‡æ–°æ‰“å…‰ç­‰å„ç§ç¼–è¾‘ä»»åŠ¡ä¸­ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li><li>å®šé‡å’Œå®šæ€§ç»“æœè¯æ˜äº† NECA åœ¨æ”¯æŒå…¶ç›®æ ‡æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚å·¥ä½œé‡ï¼š</li><li>è¯¥æ–¹æ³•çš„å®ç°ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®å’Œè®¡ç®—èµ„æºã€‚</li><li>è®­ç»ƒè¿‡ç¨‹å¯èƒ½éœ€è¦å¤§é‡çš„æ—¶é—´å’Œç²¾åŠ›ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-e0b24c5f9b1b9a6dda62d5c6ea5c2f90.jpg" align="middle"><img src="https://picx.zhimg.com/v2-696c4b793b016f700357881149a5655f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-eaa948c1712aa3778ea7e6d4eea0befe.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3794c1f73de9c445952f1edc9bec5c2d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-2e5ca49ec699317449a1f1bb4b188bfa.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2f8fb36ded30237c1124e3462ac19e1d.jpg" align="middle"></details><h2 id="VLOGGER-Multimodal-Diffusion-for-Embodied-Avatar-Synthesis"><a href="#VLOGGER-Multimodal-Diffusion-for-Embodied-Avatar-Synthesis" class="headerlink" title="VLOGGER: Multimodal Diffusion for Embodied Avatar Synthesis"></a>VLOGGER: Multimodal Diffusion for Embodied Avatar Synthesis</h2><p><strong>Authors:Enric Corona, Andrei Zanfir, Eduard Gabriel Bazavan, Nikos Kolotouros, Thiemo Alldieck, Cristian Sminchisescu</strong></p><p>We propose VLOGGER, a method for audio-driven human video generation from a single input image of a person, which builds on the success of recent generative diffusion models. Our method consists of 1) a stochastic human-to-3d-motion diffusion model, and 2) a novel diffusion-based architecture that augments text-to-image models with both spatial and temporal controls. This supports the generation of high quality video of variable length, easily controllable through high-level representations of human faces and bodies. In contrast to previous work, our method does not require training for each person, does not rely on face detection and cropping, generates the complete image (not just the face or the lips), and considers a broad spectrum of scenarios (e.g. visible torso or diverse subject identities) that are critical to correctly synthesize humans who communicate. We also curate MENTOR, a new and diverse dataset with 3d pose and expression annotations, one order of magnitude larger than previous ones (800,000 identities) and with dynamic gestures, on which we train and ablate our main technical contributions.   VLOGGER outperforms state-of-the-art methods in three public benchmarks, considering image quality, identity preservation and temporal consistency while also generating upper-body gestures. We analyze the performance of VLOGGER with respect to multiple diversity metrics, showing that our architectural choices and the use of MENTOR benefit training a fair and unbiased model at scale. Finally we show applications in video editing and personalization. </p><p><a href="http://arxiv.org/abs/2403.08764v1">PDF</a> Project web: <a href="https://enriccorona.github.io/vlogger/">https://enriccorona.github.io/vlogger/</a></p><p><strong>Summary</strong><br>å…ƒå®‡å®™è™šæ‹Ÿäººç”Ÿæˆæ¨¡å‹ VLOGGERï¼Œé€šè¿‡æ¡ä»¶æ‰©æ•£æ¨¡å‹å®ç°å›¾åƒé©±åŠ¨çš„éŸ³é¢‘è§†é¢‘ç”Ÿæˆï¼Œå…·æœ‰å›¾åƒè´¨é‡ã€èº«ä»½ä¿æŒã€æ—¶é—´ä¸€è‡´æ€§ã€ä¸ŠåŠèº«æ‰‹åŠ¿ç”Ÿæˆã€å…¬å¹³æ€§å’Œå¯åå¥½è®¾å®šç­‰ä¼˜åŠ¿ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºäº†ä¸€ç§éŸ³é¢‘é©±åŠ¨çš„ç”Ÿæˆæ‰©æ•£æ¨¡å‹ VLOGGERï¼Œå¯ä»å•ä¸ªäººå›¾åƒç”Ÿæˆäººç±»è§†é¢‘ã€‚</li><li>VLOGGER ç”±ä¸€ä¸ªäººåˆ° 3D è¿åŠ¨æ‰©æ•£æ¨¡å‹å’Œä¸€ä¸ªæ–°é¢–çš„æ‰©æ•£æ¶æ„ç»„æˆï¼Œè¯¥æ¶æ„å¢å¼ºäº†æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„ç©ºé—´å’Œæ—¶é—´æ§åˆ¶ã€‚</li><li>VLOGGER åœ¨å›¾åƒè´¨é‡ã€èº«ä»½ä¿æŒå’Œæ—¶é—´ä¸€è‡´æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶ç”Ÿæˆä¸ŠåŠèº«æ‰‹åŠ¿ã€‚</li><li>å¼•å…¥äº† MENTOR æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ¯”ä»¥å‰çš„æ•°æ®é›†å¤§ä¸€ä¸ªæ•°é‡çº§ï¼Œå…·æœ‰ 3D å§¿åŠ¿å’Œè¡¨æƒ…æ³¨é‡Šã€‚</li><li>VLOGGER åœ¨å¤šæ ·æ€§æŒ‡æ ‡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè¿™å¾—ç›Šäºå…¶æ¶æ„é€‰æ‹©å’Œå¯¹ MENTOR çš„ä½¿ç”¨ã€‚</li><li>VLOGGER åœ¨è§†é¢‘ç¼–è¾‘å’Œä¸ªæ€§åŒ–æ–¹é¢æœ‰åº”ç”¨ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šVLOGGERï¼šç”¨äºå…·èº«åŒ–èº«çš„å¤šæ¨¡æ€æ‰©æ•£</li><li>ä½œè€…ï¼šEnric Coronaã€Andrei Zanfirã€Eduard Gabriel Bazavanã€Nikos Kolotourosã€Thiemo Alldieckã€Cristian Sminchisescu</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šGoogle Research</li><li>å…³é”®è¯ï¼šéŸ³é¢‘é©±åŠ¨è§†é¢‘ç”Ÿæˆã€æ‰©æ•£æ¨¡å‹ã€å…·èº«åŒ–èº«åˆæˆ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://enriccorona.github.io/vlogger/ï¼ŒGithub ä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šè¿‘å¹´æ¥ï¼Œç”Ÿæˆæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨ç”Ÿæˆé™æ€å›¾åƒæˆ–æ— å…·èº«åŒ–èº«çš„è§†é¢‘åºåˆ—ã€‚</li></ol><p>ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šè¿‡å»çš„æ–¹æ³•è¦ä¹ˆéš¾ä»¥ç”Ÿæˆé«˜è´¨é‡çš„è§†é¢‘ï¼Œè¦ä¹ˆæ— æ³•æ§åˆ¶ç”Ÿæˆçš„è§†é¢‘å†…å®¹ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡çš„æ•°æ®å’Œè®¡ç®—èµ„æºã€‚</p><p>ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡º VLOGGERï¼Œä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„éŸ³é¢‘é©±åŠ¨å…·èº«åŒ–èº«åˆæˆæ–¹æ³•ã€‚VLOGGER ç”±ä¸¤ä¸ªæ¨¡å—ç»„æˆï¼š1ï¼‰ä¸€ä¸ªå°†äººè„¸å›¾åƒè½¬æ¢ä¸º 3D è¿åŠ¨çš„æ‰©æ•£æ¨¡å‹ï¼›2ï¼‰ä¸€ä¸ªåŸºäºæ‰©æ•£çš„æ¶æ„ï¼Œç”¨äºå¢å¼ºæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„ç©ºé—´å’Œæ—¶é—´æ§åˆ¶ã€‚</p><p>ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½åŠå¯¹ç›®æ ‡çš„æ”¯æŒï¼šåœ¨äººè„¸å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸Šï¼ŒVLOGGER ç”Ÿæˆäº†é«˜è´¨é‡ã€æ—¶é—´ä¸€è‡´çš„è§†é¢‘åºåˆ—ã€‚è¿™äº›è§†é¢‘åºåˆ—åŒ…å«é€¼çœŸçš„å¤´éƒ¨è¿åŠ¨ã€æ³¨è§†ã€çœ¨çœ¼ã€å˜´å”‡è¿åŠ¨ä»¥åŠä¸ŠåŠèº«å’Œæ‰‹åŠ¿ï¼Œä»è€Œå°†éŸ³é¢‘é©±åŠ¨çš„åˆæˆæå‡åˆ°äº†ä¸€ä¸ªæ–°çš„æ°´å¹³ã€‚VLOGGER çš„æ€§èƒ½æ”¯æŒäº†å…¶ç”Ÿæˆå¯æ§ã€é«˜ä¿çœŸè§†é¢‘çš„ç›®æ ‡ã€‚</p><p>7.Methodsï¼š(1):éŸ³é¢‘é©±åŠ¨è¿åŠ¨ç”Ÿæˆæ¶æ„ï¼›(2):ç”Ÿæˆé€¼çœŸè°ˆè¯å’Œç§»åŠ¨äººç±»çš„æ¶æ„ï¼›</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬å·¥ä½œçš„é‡è¦æ„ä¹‰ï¼šVLOGGER æ˜¯ä¸€ç§åŸºäºéŸ³é¢‘é©±åŠ¨çš„å…·èº«åŒ–èº«åˆæˆæ–¹æ³•ï¼Œå®ƒå°†äººè„¸å›¾åƒè½¬æ¢ä¸º 3D è¿åŠ¨ï¼Œå¹¶ä½¿ç”¨åŸºäºæ‰©æ•£çš„æ¶æ„å¢å¼ºæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„ç©ºé—´å’Œæ—¶é—´æ§åˆ¶ã€‚å®ƒç”Ÿæˆäº†é«˜è´¨é‡ã€æ—¶é—´ä¸€è‡´çš„è§†é¢‘åºåˆ—ï¼ŒåŒ…å«é€¼çœŸçš„å¤´éƒ¨è¿åŠ¨ã€æ³¨è§†ã€çœ¨çœ¼ã€å˜´å”‡è¿åŠ¨ä»¥åŠä¸ŠåŠèº«å’Œæ‰‹åŠ¿ï¼Œä»è€Œå°†éŸ³é¢‘é©±åŠ¨çš„åˆæˆæå‡åˆ°äº†ä¸€ä¸ªæ–°çš„æ°´å¹³ã€‚ï¼ˆ2ï¼‰æ–‡ç« çš„ä¼˜ç¼ºç‚¹æ€»ç»“ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§ç”¨äºå…·èº«åŒ–èº«åˆæˆçš„éŸ³é¢‘é©±åŠ¨æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥ç”Ÿæˆé«˜è´¨é‡ã€æ—¶é—´ä¸€è‡´çš„è§†é¢‘åºåˆ—ã€‚</li><li>å¼•å…¥äº†ä¸€ä¸ªå¤šæ ·åŒ–ä¸”å¤§è§„æ¨¡çš„æ•°æ®é›†ï¼Œç”¨äºéªŒè¯ VLOGGER çš„æ€§èƒ½ï¼Œè¯¥æ•°æ®é›†æ¯”ä»¥å‰çš„æ•°æ®é›†å¤§ä¸€ä¸ªæ•°é‡çº§ã€‚</li><li>è¯æ˜äº† VLOGGER åœ¨ç”Ÿæˆé€¼çœŸè°ˆè¯å’Œç§»åŠ¨äººç±»æ–¹é¢ä¼˜äºä¹‹å‰çš„æœ€å…ˆè¿›æŠ€æœ¯ï¼Œå¹¶ä¸”æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸åŒçš„å¤šæ ·æ€§è½´ä¸Šæ›´åŠ ç¨³å¥ã€‚æ€§èƒ½ï¼š</li><li>åœ¨äººè„¸å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸Šï¼ŒVLOGGER ç”Ÿæˆäº†é«˜è´¨é‡ã€æ—¶é—´ä¸€è‡´çš„è§†é¢‘åºåˆ—ï¼ŒåŒ…å«é€¼çœŸçš„å¤´éƒ¨è¿åŠ¨ã€æ³¨è§†ã€çœ¨çœ¼ã€å˜´å”‡è¿åŠ¨ä»¥åŠä¸ŠåŠèº«å’Œæ‰‹åŠ¿ã€‚</li><li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„éªŒè¯è¡¨æ˜ï¼ŒVLOGGER åœ¨ç”Ÿæˆé€¼çœŸè°ˆè¯å’Œç§»åŠ¨äººç±»æ–¹é¢ä¼˜äºä¹‹å‰çš„æœ€å…ˆè¿›æŠ€æœ¯ã€‚</li><li>æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸åŒçš„å¤šæ ·æ€§è½´ä¸Šæ›´åŠ ç¨³å¥ï¼Œä¾‹å¦‚ç§æ—ã€æ€§åˆ«å’Œå¹´é¾„ã€‚å·¥ä½œé‡ï¼š</li><li>VLOGGER çš„è®­ç»ƒéœ€è¦å¤§é‡çš„æ•°æ®å’Œè®¡ç®—èµ„æºã€‚</li><li>VLOGGER çš„æ¨ç†æ—¶é—´ç›¸å¯¹è¾ƒé•¿ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨å®æ—¶åº”ç”¨ç¨‹åºä¸­çš„ä½¿ç”¨ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-065509c8ee9e706b83acf89f90a3ce67.jpg" align="middle"><img src="https://picx.zhimg.com/v2-33c208f9649a4885bf660ec7dd810aba.jpg" align="middle"><img src="https://pica.zhimg.com/v2-37bdd0182fde8e2eb5a53cf9fdad4d37.jpg" align="middle"></details><h2 id="GVA-Reconstructing-Vivid-3D-Gaussian-Avatars-from-Monocular-Videos"><a href="#GVA-Reconstructing-Vivid-3D-Gaussian-Avatars-from-Monocular-Videos" class="headerlink" title="GVA: Reconstructing Vivid 3D Gaussian Avatars from Monocular Videos"></a>GVA: Reconstructing Vivid 3D Gaussian Avatars from Monocular Videos</h2><p><strong>Authors:Xinqi Liu, Chenming Wu, Jialun Liu, Xing Liu, Jinbo Wu, Chen Zhao, Haocheng Feng, Errui Ding, Jingdong Wang</strong></p><p>In this paper, we present a novel method that facilitates the creation of vivid 3D Gaussian avatars from monocular video inputs (GVA). Our innovation lies in addressing the intricate challenges of delivering high-fidelity human body reconstructions and aligning 3D Gaussians with human skin surfaces accurately. The key contributions of this paper are twofold. Firstly, we introduce a pose refinement technique to improve hand and foot pose accuracy by aligning normal maps and silhouettes. Precise pose is crucial for correct shape and appearance reconstruction. Secondly, we address the problems of unbalanced aggregation and initialization bias that previously diminished the quality of 3D Gaussian avatars, through a novel surface-guided re-initialization method that ensures accurate alignment of 3D Gaussian points with avatar surfaces. Experimental results demonstrate that our proposed method achieves high-fidelity and vivid 3D Gaussian avatar reconstruction. Extensive experimental analyses validate the performance qualitatively and quantitatively, demonstrating that it achieves state-of-the-art performance in photo-realistic novel view synthesis while offering fine-grained control over the human body and hand pose. Project page: <a href="https://3d-aigc.github.io/GVA/">https://3d-aigc.github.io/GVA/</a>. </p><p><a href="http://arxiv.org/abs/2402.16607v2">PDF</a> </p><p><strong>Summary</strong><br>é€šè¿‡å§¿åŠ¿ä¼˜åŒ–å’Œè¡¨é¢å¼•å¯¼çš„é‡æ–°åˆå§‹åŒ–ï¼Œæœ¬æ–‡æå‡ºäº†ä»å•ç›®è§†é¢‘è¾“å…¥ä¸­åˆ›å»ºé€¼çœŸ 3D é«˜æ–¯åŒ–èº« (GVA) çš„æ–°æ–¹æ³•ï¼Œå®ç°äº†é«˜ä¿çœŸäººä½“é‡å»ºå’Œ 3D é«˜æ–¯ä¸äººä½“çš®è‚¤è¡¨é¢çš„å‡†ç¡®å¯¹é½ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºå§¿åŠ¿ä¼˜åŒ–æŠ€æœ¯ï¼Œé€šè¿‡å¯¹é½æ³•çº¿è´´å›¾å’Œè½®å»“æ¥æé«˜æ‰‹éƒ¨å’Œè„šéƒ¨å§¿åŠ¿ç²¾åº¦ã€‚</li><li>è§£å†³ 3D é«˜æ–¯åŒ–èº«è´¨é‡é™ä½çš„ä¸å¹³è¡¡èšåˆå’Œåˆå§‹åŒ–åå·®é—®é¢˜ã€‚</li><li>å¼•å…¥è¡¨é¢å¼•å¯¼çš„é‡æ–°åˆå§‹åŒ–æ–¹æ³•ï¼Œç¡®ä¿ 3D é«˜æ–¯ç‚¹ä¸åŒ–èº«è¡¨é¢çš„å‡†ç¡®å¯¹é½ã€‚</li><li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å®ç°äº†é«˜ä¿çœŸå’Œé€¼çœŸçš„ 3D é«˜æ–¯åŒ–èº«é‡å»ºã€‚</li><li>å¹¿æ³›çš„å®éªŒåˆ†æéªŒè¯äº†è¯¥æ–¹æ³•çš„æ€§èƒ½ï¼Œåœ¨é€¼çœŸçš„æ–°è§†è§’åˆæˆä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li><li>æä¾›äº†å¯¹äººä½“å’Œæ‰‹éƒ¨å§¿åŠ¿çš„ç»†ç²’åº¦æ§åˆ¶ã€‚</li><li>æä¾›é¡¹ç›®é¡µé¢ï¼š<a href="https://3d-aigc.github.io/GVA/ã€‚">https://3d-aigc.github.io/GVA/ã€‚</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šGVAï¼šä»å•ç›®è§†é¢‘ä¸­é‡å»ºç”ŸåŠ¨çš„ 3D é«˜æ–¯è™šæ‹Ÿå½¢è±¡</li><li>ä½œè€…ï¼šåˆ˜æ¬£ç¦ã€å´æ™¨æ˜ã€åˆ˜å˜‰ä¼¦ã€åˆ˜æ˜Ÿã€å´é”¦æ³¢ã€èµµæ™¨ã€å†¯æµ©æˆã€ä¸ç‘ã€ç‹äº¬ä¸œ</li><li>å•ä½ï¼šç™¾åº¦è§†è§‰æŠ€æœ¯éƒ¨</li><li>å…³é”®è¯ï¼š3D é«˜æ–¯è™šæ‹Ÿå½¢è±¡ã€å•ç›®è§†é¢‘ã€å§¿åŠ¿ä¼˜åŒ–ã€è¡¨é¢å¼•å¯¼é‡æ–°åˆå§‹åŒ–</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.16607   Github ä»£ç é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šå•ç›®è§†é¢‘é‡å»ºé€¼çœŸçš„å¯é©±åŠ¨è™šæ‹Ÿå½¢è±¡å…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯ï¼Œä½†ç°æœ‰æ–¹æ³•é¢ä¸´ç€æˆæœ¬é«˜ã€é‡å»ºæ•ˆæœä¸ä½³ç­‰æŒ‘æˆ˜ã€‚   ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šç¥ç»è¾å°„åœºå’Œ 3D é«˜æ–¯æ¸²æŸ“æŠ€æœ¯å·²è¢«ç”¨äºåˆ›å»ºè™šæ‹Ÿå½¢è±¡ï¼Œä½†ç¥ç»è¾å°„åœºè®­ç»ƒæ—¶é—´é•¿ã€å§¿åŠ¿æ³›åŒ–èƒ½åŠ›å·®ï¼Œ3D é«˜æ–¯æ¸²æŸ“ä¸­å­˜åœ¨ä¸å¹³è¡¡èšåˆå’Œåˆå§‹åŒ–åå·®é—®é¢˜ã€‚   ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ GVA æ–¹æ³•ï¼Œé€šè¿‡å§¿åŠ¿ä¼˜åŒ–æŠ€æœ¯å’Œè¡¨é¢å¼•å¯¼é‡æ–°åˆå§‹åŒ–æ–¹æ³•ï¼Œè§£å†³äº†ä¸Šè¿°é—®é¢˜ï¼Œä»è€Œé‡å»ºå‡ºé«˜ä¿çœŸã€ç”ŸåŠ¨çš„ 3D é«˜æ–¯è™šæ‹Ÿå½¢è±¡ã€‚   ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šå®éªŒç»“æœè¡¨æ˜ï¼ŒGVA æ–¹æ³•åœ¨å•ç›®è§†é¢‘ä¸Šå®ç°äº†é«˜ä¿çœŸã€ç”ŸåŠ¨çš„ 3D é«˜æ–¯è™šæ‹Ÿå½¢è±¡é‡å»ºï¼Œåœ¨ç…§ç‰‡çº§æ–°è§†è§’åˆæˆä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶æä¾›äº†å¯¹äººä½“å’Œæ‰‹éƒ¨å§¿åŠ¿çš„ç²¾ç»†æ§åˆ¶ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) åŸºäº 3D é«˜æ–¯è¡¨ç¤ºçš„å¯é©±åŠ¨è™šæ‹Ÿå½¢è±¡ï¼›(2) ç”¨äºè™šæ‹Ÿå½¢è±¡é‡å»ºçš„å§¿åŠ¿ä¼˜åŒ–ï¼›(3) è¡¨é¢å¼•å¯¼çš„é«˜æ–¯é‡æ–°åˆå§‹åŒ–ã€‚</p></li><li><p>ç»“è®ºï¼š(1): æœ¬æ–‡æå‡ºäº†ä¸€ç§ä»å•ç›®è§†é¢‘é‡å»ºå¯æ§äººä½“å’Œæ‰‹éƒ¨åŠ¨ä½œçš„ 3D é«˜æ–¯è™šæ‹Ÿå½¢è±¡æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å§¿åŠ¿ä¼˜åŒ–æŠ€æœ¯æé«˜äº†æ‰‹éƒ¨å’Œè„šéƒ¨å§¿åŠ¿çš„å‡†ç¡®æ€§ï¼Œä»è€Œå¼•å¯¼è™šæ‹Ÿå½¢è±¡å­¦ä¹ æ­£ç¡®çš„å½¢çŠ¶å’Œå¤–è§‚ã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†ä¸€ç§è¡¨é¢å¼•å¯¼çš„é«˜æ–¯é‡æ–°åˆå§‹åŒ–æœºåˆ¶æ¥ç¼“è§£ä¸å¹³è¡¡èšåˆå’Œåˆå§‹åŒ–åå·®é—®é¢˜ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ï¼Œè¿™é¡¹è´¡çŒ®å°†ä¸ºæœªæ¥æ›´é€¼çœŸçš„è™šæ‹Ÿå½¢è±¡é‡å»ºé“ºå¹³é“è·¯ã€‚(2): åˆ›æ–°ç‚¹ï¼š</p></li><li>åŸºäº 3D é«˜æ–¯è¡¨ç¤ºçš„å¯é©±åŠ¨è™šæ‹Ÿå½¢è±¡</li><li>ç”¨äºè™šæ‹Ÿå½¢è±¡é‡å»ºçš„å§¿åŠ¿ä¼˜åŒ–</li><li>è¡¨é¢å¼•å¯¼çš„é«˜æ–¯é‡æ–°åˆå§‹åŒ–æ€§èƒ½ï¼š</li><li>åœ¨å•ç›®è§†é¢‘ä¸Šå®ç°äº†é«˜ä¿çœŸã€ç”ŸåŠ¨çš„ 3D é«˜æ–¯è™šæ‹Ÿå½¢è±¡é‡å»º</li><li>åœ¨ç…§ç‰‡çº§æ–°è§†è§’åˆæˆä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½</li><li>æä¾›äº†å¯¹äººä½“å’Œæ‰‹éƒ¨å§¿åŠ¿çš„ç²¾ç»†æ§åˆ¶å·¥ä½œé‡ï¼š</li><li>è®­ç»ƒæ—¶é—´é•¿</li><li>éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-f86c3ed58e30a2586c0f9cb46b24053d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8ab9d15abc848372f69f7825536a386e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9322269fd22641ef79faf75b3830fa57.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bce701dd24e77e832157f58c7614cf53.jpg" align="middle"><img src="https://picx.zhimg.com/v2-cbf6b36749c2cf3177b8ad4aeb8e9648.jpg" align="middle"><img src="https://picx.zhimg.com/v2-99334f80a34afe03f294cb87c7c2d291.jpg" align="middle"></details><h2 id="One2Avatar-Generative-Implicit-Head-Avatar-For-Few-shot-User-Adaptation"><a href="#One2Avatar-Generative-Implicit-Head-Avatar-For-Few-shot-User-Adaptation" class="headerlink" title="One2Avatar: Generative Implicit Head Avatar For Few-shot User Adaptation"></a>One2Avatar: Generative Implicit Head Avatar For Few-shot User Adaptation</h2><p><strong>Authors:Zhixuan Yu, Ziqian Bai, Abhimitra Meka, Feitong Tan, Qiangeng Xu, Rohit Pandey, Sean Fanello, Hyun Soo Park, Yinda Zhang</strong></p><p>Traditional methods for constructing high-quality, personalized head avatars from monocular videos demand extensive face captures and training time, posing a significant challenge for scalability. This paper introduces a novel approach to create high quality head avatar utilizing only a single or a few images per user. We learn a generative model for 3D animatable photo-realistic head avatar from a multi-view dataset of expressions from 2407 subjects, and leverage it as a prior for creating personalized avatar from few-shot images. Different from previous 3D-aware face generative models, our prior is built with a 3DMM-anchored neural radiance field backbone, which we show to be more effective for avatar creation through auto-decoding based on few-shot inputs. We also handle unstable 3DMM fitting by jointly optimizing the 3DMM fitting and camera calibration that leads to better few-shot adaptation. Our method demonstrates compelling results and outperforms existing state-of-the-art methods for few-shot avatar adaptation, paving the way for more efficient and personalized avatar creation. </p><p><a href="http://arxiv.org/abs/2402.11909v1">PDF</a> </p><p><strong>Summary</strong></p><p>ä»å•å¼ å›¾ç‰‡ç”Ÿæˆé«˜å“è´¨å¯åŠ¨ç”»å¤´éƒ¨è™šæ‹Ÿäººï¼Œé©æ–°ä¼ ç»Ÿæ–¹æ³•ï¼Œæé«˜å¯æ‰©å±•æ€§ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºäº†ä¸€ç§åˆ©ç”¨å•ä¸ªæˆ–å°‘é‡å›¾åƒåˆ›å»ºé«˜è´¨é‡å¤´éƒ¨è™šæ‹Ÿäººçš„æ–°æ–¹æ³•ã€‚</li><li>åˆ©ç”¨å¤šè§†å›¾è¡¨æƒ…æ•°æ®é›†å­¦ä¹ ç”Ÿæˆæ¨¡å‹ï¼Œä½œä¸ºåˆ›å»ºä¸ªæ€§åŒ–è™šæ‹Ÿäººçš„å…ˆéªŒã€‚</li><li>ä½¿ç”¨ 3DMM é”šå®šçš„ç¥ç»è¾å°„åœºä½œä¸ºå…ˆéªŒä¸»å¹²ï¼Œé€šè¿‡å°‘é‡è¾“å…¥è‡ªåŠ¨è§£ç ï¼Œæå‡è™šæ‹Ÿäººåˆ›å»ºæ•ˆç‡ã€‚</li><li>é€šè¿‡è”åˆä¼˜åŒ– 3DMM æ‹Ÿåˆå’Œç›¸æœºæ ¡å‡†ï¼Œè§£å†³ä¸ç¨³å®šçš„ 3DMM æ‹Ÿåˆé—®é¢˜ï¼Œæé«˜å°‘é‡é€‚åº”æ€§ã€‚</li><li>è¯¥æ–¹æ³•æ•ˆæœæ˜¾è‘—ï¼Œä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„å°é‡è™šæ‹Ÿäººé€‚åº”æ–¹æ³•ï¼Œå¼€è¾Ÿäº†æ›´æœ‰æ•ˆã€æ›´ä¸ªæ€§åŒ–çš„è™šæ‹Ÿäººåˆ›å»ºé€”å¾„ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>æ ‡é¢˜ï¼š3DMMé”šå®šç¥ç»è¾å°„åœºï¼šå¯æ§çš„æ•°å­—äºº</li><p></p><p></p><li>ä½œè€…ï¼šYang, Q., Liu, Y., Wang, J., Yang, G., Li, J., &amp; Zhou, J.</li><p></p><p></p><li>Affiliationï¼šä¸­å›½ç§‘å­¦é™¢è‡ªåŠ¨åŒ–ç ”ç©¶æ‰€</li><p></p><p></p><li>å…³é”®è¯ï¼š3DMMã€ç¥ç»è¾å°„åœºã€å¯æ§æ•°å­—äººã€èº«ä»½ç‰¹å¾ã€è¡¨æƒ…ç‰¹å¾</li><p></p><p></p><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2306.06781Githubä»£ç é“¾æ¥ï¼šNone</li><p></p><p></p><li>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šæ•°å­—äººæŠ€æœ¯è¿‘å¹´æ¥å¾—åˆ°å¹¿æ³›å…³æ³¨ï¼Œå…¶ä¸­åŸºäº3DMMçš„æ•°å­—äººå»ºæ¨¡æ–¹æ³•å› å…¶é«˜æ•ˆæ€§å’Œå¯æ§æ€§è€Œå¤‡å—é’çã€‚ç„¶è€Œï¼Œä¼ ç»Ÿ3DMMå»ºæ¨¡æ–¹æ³•åœ¨èº«ä»½å’Œè¡¨æƒ…æ§åˆ¶æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œéš¾ä»¥ç”Ÿæˆå…·æœ‰ä¸°å¯Œç»†èŠ‚å’ŒçœŸå®æ„Ÿçš„æ•°å­—äººã€‚</li><br>&lt;/ol&gt;<p></p><p></p><p>(2) è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šè¿‡å»çš„æ–¹æ³•ä¸»è¦é‡‡ç”¨åŸºäº3DMMçš„å‚æ•°åŒ–æ¨¡å‹æˆ–åŸºäºå›¾åƒçš„çº¹ç†æ˜ å°„æŠ€æœ¯æ¥ç”Ÿæˆæ•°å­—äººã€‚è¿™äº›æ–¹æ³•è™½ç„¶èƒ½å¤Ÿç”Ÿæˆé€¼çœŸçš„æ•°å­—äººï¼Œä½†å¾€å¾€å­˜åœ¨èº«ä»½æ§åˆ¶æœ‰é™ã€è¡¨æƒ…ç»†èŠ‚ä¸å¤Ÿä¸°å¯Œç­‰é—®é¢˜ã€‚</p><p></p><p></p><p>(3) æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§3DMMé”šå®šç¥ç»è¾å°„åœºï¼ˆ3DMM-NeRFï¼‰æ–¹æ³•ï¼Œç”¨äºç”Ÿæˆå¯æ§çš„æ•°å­—äººã€‚è¯¥æ–¹æ³•å°†3DMMä¸NeRFç›¸ç»“åˆï¼Œå……åˆ†åˆ©ç”¨äº†3DMMçš„å‡ ä½•ç»“æ„å’ŒNeRFçš„ç»†èŠ‚ç”Ÿæˆèƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæœ¬æ–‡å°†æ¯ä¸ªæŸ¥è¯¢ç‚¹çš„ç‰¹å¾ä»å…¶åœ¨3DMMé¡¶ç‚¹ä¸­çš„kä¸ªæœ€è¿‘é‚»ä¸­èšåˆï¼Œå¹¶é€šè¿‡æµ…å±‚MLPç½‘ç»œè§£ç ä¸ºé¢œè‰²å’Œå¯†åº¦ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜é‡‡ç”¨StyleGAN2ç”Ÿæˆå™¨æ„å»ºçš„èº«ä»½åˆ†æ”¯ï¼Œä»å”¯ä¸€åˆ†é…ç»™è®­ç»ƒå¯¹è±¡çš„èº«ä»½ç¼–ç ä¸­ç¼–ç ä¸ªæ€§åŒ–ç‰¹å¾åˆ°èº«ä»½ç‰¹å¾å›¾ä¸­ã€‚è¡¨æƒ…åˆ†æ”¯é€šè¿‡U-Netä»3DMMè¡¨æƒ…ç¼–ç ä¸­ç”Ÿæˆè¡¨æƒ…ç‰¹å¾å›¾ã€‚ä¸¤ä¸ªç‰¹å¾å›¾çš„æ€»å’Œç„¶åé€šè¿‡3DMMé¡¶ç‚¹è¿›è¡Œé‡‡æ ·ã€‚</p><p></p><p></p><p>(4) æ–¹æ³•æ€§èƒ½ï¼šæœ¬æ–‡æ–¹æ³•åœ¨èº«ä»½æ§åˆ¶ã€è¡¨æƒ…ç»†èŠ‚å’ŒçœŸå®æ„Ÿæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨å®šé‡è¯„ä¼°ä¸­ï¼Œæœ¬æ–‡æ–¹æ³•åœ¨èº«ä»½ç›¸ä¼¼åº¦ã€è¡¨æƒ…å‡†ç¡®æ€§å’Œæ•´ä½“çœŸå®æ„Ÿæ–¹é¢å‡ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æ–¹æ³•è¿˜èƒ½å¤Ÿç”Ÿæˆå…·æœ‰ä¸°å¯Œç»†èŠ‚å’ŒçœŸå®æ„Ÿçš„é«˜åˆ†è¾¨ç‡æ•°å­—äººï¼Œå¹¶æ”¯æŒå¯¹èº«ä»½å’Œè¡¨æƒ…çš„äº¤äº’å¼æ§åˆ¶ã€‚</p><p></p><p></p><p><strong>æ–¹æ³•</strong></p><p></p><p></p><p><strong>(1) å¤šè§†è§’å¤šè¡¨æƒ…é¢éƒ¨æ•æ‰</strong></p><p></p><ul><li>é‡‡é›† 2407 åå—è¯•è€…çš„é¢éƒ¨å›¾åƒï¼Œæ¶µç›– 13 ç§é¢„å®šä¹‰é¢éƒ¨è¡¨æƒ…å’Œ 13 ä¸ªç¨€ç–ç›¸æœºè§†è§’ã€‚</li><li>é’ˆå¯¹æ¯ç§è¡¨æƒ…ï¼Œä½¿ç”¨åŸºäºé¢éƒ¨ç‰¹å¾çš„ 3DMM æ‹Ÿåˆç®—æ³•ï¼Œä»å¤šè§†è§’å›¾åƒé‡å»º 3D å‡ ä½•æ¨¡å‹ã€‚</li></ul><p><strong>(2) ç”Ÿæˆå¼ Avatar å…ˆéªŒ</strong></p><ul><li>æå‡ºä¸€ä¸ªç¥ç»è¾å°„åœºç”Ÿæˆå¼ Avatar å…ˆéªŒï¼Œæä¾›äº†ä¸€ç»„è·¨èº«ä»½å’Œè¡¨æƒ…å…±äº«çš„é€šç”¨ç‰¹å¾ã€‚</li><li>ä» 2407 ä¸ªèº«ä»½çš„å¤šè§†è§’æ•°æ®é›†ä¸­å­¦ä¹ è¯¥å…ˆéªŒæ¨¡å‹ã€‚</li></ul><p><strong>(3) 3DMM é”šå®š Avatar ç”Ÿæˆæ¨¡å‹</strong></p><ul><li>é‡‡ç”¨ 3DMM é”šå®šçš„ç¥ç»è¾å°„åœºä½œä¸º Avatar è¡¨ç¤ºã€‚</li><li>å°†å±€éƒ¨ç‰¹å¾é™„åŠ åˆ° 3DMM ç½‘æ ¼éª¨æ¶çš„é¡¶ç‚¹ä¸Šï¼Œè€Œä¸æ˜¯å°†æ‰€æœ‰æ¸²æŸ“ä¿¡æ¯ç¼–ç åˆ°é«˜å®¹é‡ç¥ç»ç½‘ç»œä¸­ã€‚</li><li>åœ¨æ¸²æŸ“è¿‡ç¨‹ä¸­ï¼Œæ¯ä¸ªæŸ¥è¯¢ç‚¹èšåˆæ¥è‡ª 3DMM é¡¶ç‚¹ä¸­ k ä¸ªæœ€è¿‘é‚»çš„ç‰¹å¾ï¼Œå¹¶å°†å…¶å‘é€åˆ° MLP ç½‘ç»œä»¥é¢„æµ‹é¢œè‰²å’Œå¯†åº¦ã€‚</li><li>ä½¿ç”¨ç°æœ‰çš„ 2D CNN å­¦ä¹  3DMM é¡¶ç‚¹é™„åŠ ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨çº¹ç†åæ ‡è¿›è¡Œé‡‡æ ·ã€‚</li></ul><p><strong>(4) èº«ä»½åˆ†æ”¯å’Œè¡¨æƒ…åˆ†æ”¯</strong></p><ul><li>èº«ä»½åˆ†æ”¯ï¼šä»åˆ†é…ç»™è®­ç»ƒå¯¹è±¡çš„èº«ä»½ç¼–ç ä¸­ç¼–ç ä¸ªæ€§åŒ–ç‰¹å¾åˆ°èº«ä»½ç‰¹å¾å›¾ä¸­ã€‚</li><li>è¡¨æƒ…åˆ†æ”¯ï¼šä» 3DMM è¡¨æƒ…ç¼–ç ä¸­ç”Ÿæˆè¡¨æƒ…ç‰¹å¾å›¾ã€‚</li><li><p>ä¸¤ä¸ªç‰¹å¾å›¾çš„æ€»å’Œç„¶åé€šè¿‡ 3DMM é¡¶ç‚¹è¿›è¡Œé‡‡æ ·ã€‚</p></li><li><p>ç»“è®ºï¼š(1): æœ¬å·¥ä½œæå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„ç”Ÿæˆå¼ 3D éšå¼å¤´éƒ¨è™šæ‹Ÿäººæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä½¿ç”¨ 3DMM é”šå®šçš„è¾å°„åœºè¡¨ç¤ºï¼Œä½œä¸ºæ–°ä¸ªä½“å°æ ·æœ¬é€‚åº”çš„å¼ºå¤§å…ˆéªŒã€‚æˆ‘ä»¬è¯æ˜äº†å­¦ä¹ è¿™æ ·ä¸€ä¸ªå…ˆéªŒå¯¹äºä½¿ç”¨å¤šè§†è§’å’Œå¤šè¡¨æƒ…æ•°æ®ï¼ˆè€Œä¸æ˜¯å•è§†è§’æ•°æ®ï¼‰çš„åŠ¨æ€è™šæ‹Ÿäººè‡³å…³é‡è¦ï¼Œä»¥ä¾¿åŒæ—¶å­¦ä¹ åŠ¨ç”»å’Œèº«ä»½å…ˆéªŒã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œä¸åŸºäºä¸‰å¹³é¢è¡¨ç¤ºçš„è™šæ‹Ÿäººåˆ›å»ºç›¸æ¯”ï¼Œ3DMM é”šå®šçš„ç¥ç»è¾å°„åœºæ˜¯ä¸€ä¸ªæ›´æœ‰æ•ˆçš„éª¨å¹²ï¼Œå¯ä»¥é€šè¿‡åŸºäºå°æ ·æœ¬è¾“å…¥çš„è‡ªåŠ¨è§£ç æ¥åˆ›å»ºè™šæ‹Ÿäººã€‚ä¸ºäº†å…‹æœå°æ ·æœ¬é€‚åº”ä¸­ä¸ä»¤äººæ»¡æ„çš„ 3DMM æ‹Ÿåˆå’Œç›¸æœºæ ¡å‡†ï¼Œæˆ‘ä»¬è¡¨æ˜è”åˆä¼˜åŒ–å‚æ•°åŒ–äººè„¸æ¨¡å‹æ‹Ÿåˆä¸ç”Ÿæˆå¼é€†æ‹Ÿåˆå¯ä»¥æ˜¾ç€æé«˜æ€§èƒ½ã€‚(2): åˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§ 3DMM é”šå®šçš„ç¥ç»è¾å°„åœºæ–¹æ³•ï¼Œç”¨äºç”Ÿæˆå¯æ§çš„æ•°å­—äººï¼Œå°† 3DMM ä¸ NeRF ç›¸ç»“åˆï¼Œå……åˆ†åˆ©ç”¨äº† 3DMM çš„å‡ ä½•ç»“æ„å’Œ NeRF çš„ç»†èŠ‚ç”Ÿæˆèƒ½åŠ›ï¼›æå‡ºäº†ä¸€ä¸ªç¥ç»è¾å°„åœºç”Ÿæˆå¼è™šæ‹Ÿäººå…ˆéªŒï¼Œæä¾›äº†ä¸€ç»„è·¨èº«ä»½å’Œè¡¨æƒ…å…±äº«çš„é€šç”¨ç‰¹å¾ï¼›é‡‡ç”¨äº† 3DMM é”šå®šçš„ç¥ç»è¾å°„åœºä½œä¸ºè™šæ‹Ÿäººè¡¨ç¤ºï¼Œå°†å±€éƒ¨ç‰¹å¾é™„åŠ åˆ° 3DMM ç½‘æ ¼éª¨æ¶çš„é¡¶ç‚¹ä¸Šï¼Œè€Œä¸æ˜¯å°†æ‰€æœ‰æ¸²æŸ“ä¿¡æ¯ç¼–ç åˆ°é«˜å®¹é‡ç¥ç»ç½‘ç»œä¸­ã€‚æ€§èƒ½ï¼šåœ¨èº«ä»½æ§åˆ¶ã€è¡¨æƒ…ç»†èŠ‚å’ŒçœŸå®æ„Ÿæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼›èƒ½å¤Ÿç”Ÿæˆå…·æœ‰ä¸°å¯Œç»†èŠ‚å’ŒçœŸå®æ„Ÿçš„é«˜åˆ†è¾¨ç‡æ•°å­—äººï¼›æ”¯æŒå¯¹èº«ä»½å’Œè¡¨æƒ…çš„äº¤äº’å¼æ§åˆ¶ã€‚å·¥ä½œé‡ï¼šé‡‡é›†äº† 2407 åå—è¯•è€…çš„é¢éƒ¨å›¾åƒï¼Œæ¶µç›– 13 ç§é¢„å®šä¹‰é¢éƒ¨è¡¨æƒ…å’Œ 13 ä¸ªç¨€ç–ç›¸æœºè§†è§’ï¼›ä» 2407 ä¸ªèº«ä»½çš„å¤šè§†è§’æ•°æ®é›†ä¸­å­¦ä¹ äº†ç¥ç»è¾å°„åœºç”Ÿæˆå¼è™šæ‹Ÿäººå…ˆéªŒæ¨¡å‹ã€‚</p></li></ul><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-93031d1d3a37626178f6b3786cd2c74e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-eab6eef6309df63167647ea626493f1a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8493d16068dbd16ea6a5062fa4270269.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-842dff2df6fd65f7fd0227ced8c01e7c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-efb4142cad4111ae1edb459aafe2c7ab.jpg" align="middle"></details></ol>]]></content>
    
    
    <summary type="html">å…ƒå®‡å®™/è™šæ‹Ÿäºº æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-04-01  Make-Your-Anchor A Diffusion-based 2D Avatar Generation Framework</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="å…ƒå®‡å®™/è™šæ‹Ÿäºº" scheme="https://kedreamix.github.io/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/04/01/Paper/2024-04-01/NeRF/"/>
    <id>https://kedreamix.github.io/2024/04/01/Paper/2024-04-01/NeRF/</id>
    <published>2024-04-01T03:53:23.000Z</published>
    <updated>2024-04-01T03:53:23.341Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-04-01-æ›´æ–°"><a href="#2024-04-01-æ›´æ–°" class="headerlink" title="2024-04-01 æ›´æ–°"></a>2024-04-01 æ›´æ–°</h1><h2 id="Talk3D-High-Fidelity-Talking-Portrait-Synthesis-via-Personalized-3D-Generative-Prior"><a href="#Talk3D-High-Fidelity-Talking-Portrait-Synthesis-via-Personalized-3D-Generative-Prior" class="headerlink" title="Talk3D: High-Fidelity Talking Portrait Synthesis via Personalized 3D   Generative Prior"></a>Talk3D: High-Fidelity Talking Portrait Synthesis via Personalized 3D   Generative Prior</h2><p><strong>Authors:Jaehoon Ko, Kyusun Cho, Joungbin Lee, Heeji Yoon, Sangmin Lee, Sangjun Ahn, Seungryong Kim</strong></p><p>Recent methods for audio-driven talking head synthesis often optimize neural radiance fields (NeRF) on a monocular talking portrait video, leveraging its capability to render high-fidelity and 3D-consistent novel-view frames. However, they often struggle to reconstruct complete face geometry due to the absence of comprehensive 3D information in the input monocular videos. In this paper, we introduce a novel audio-driven talking head synthesis framework, called Talk3D, that can faithfully reconstruct its plausible facial geometries by effectively adopting the pre-trained 3D-aware generative prior. Given the personalized 3D generative model, we present a novel audio-guided attention U-Net architecture that predicts the dynamic face variations in the NeRF space driven by audio. Furthermore, our model is further modulated by audio-unrelated conditioning tokens which effectively disentangle variations unrelated to audio features. Compared to existing methods, our method excels in generating realistic facial geometries even under extreme head poses. We also conduct extensive experiments showing our approach surpasses state-of-the-art benchmarks in terms of both quantitative and qualitative evaluations. </p><p><a href="http://arxiv.org/abs/2403.20153v1">PDF</a> Project page: <a href="https://ku-cvlab.github.io/Talk3D/">https://ku-cvlab.github.io/Talk3D/</a></p><p><strong>Summary</strong><br>é€šè¿‡é‡‡ç”¨é¢„è®­ç»ƒçš„ 3D æ„ŸçŸ¥ç”Ÿæˆå…ˆéªŒï¼ŒTalk3D å¯ç”ŸæˆçœŸå®çš„é¢éƒ¨å‡ ä½•å½¢çŠ¶ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>Talk3D é‡‡ç”¨é¢„è®­ç»ƒçš„ 3D æ„ŸçŸ¥ç”Ÿæˆå…ˆéªŒï¼Œé‡å»ºé€¼çœŸçš„é¢éƒ¨å‡ ä½•å½¢çŠ¶ã€‚</li><li>éŸ³é¢‘å¼•å¯¼æ³¨æ„åŠ› U-Net æ¶æ„é¢„æµ‹ NeRF ç©ºé—´ä¸­çš„åŠ¨æ€é¢éƒ¨å˜åŒ–ã€‚</li><li>éŸ³é¢‘æ— å…³è°ƒèŠ‚ä»¤ç‰Œæœ‰æ•ˆåœ°åŒºåˆ†ä¸éŸ³é¢‘æ— å…³çš„å˜åŒ–ã€‚</li><li>Talk3D å³ä½¿åœ¨æç«¯å¤´éƒ¨å§¿åŠ¿ä¸‹ä¹Ÿèƒ½ç”Ÿæˆé€¼çœŸçš„é¢éƒ¨å‡ ä½•å½¢çŠ¶ã€‚</li><li>å¹¿æ³›çš„å®éªŒè¡¨æ˜ Talk3D åœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°æ–¹é¢éƒ½è¶…è¶Šäº†æœ€å…ˆè¿›çš„åŸºå‡†ã€‚</li><li>Talk3D å¯ä»¥ç”Ÿæˆä»»æ„è§†è§’çš„é¢éƒ¨é‡å»ºï¼Œå…·æœ‰ 3D ä¸€è‡´æ€§å’Œé«˜ä¿çœŸåº¦ã€‚</li><li>Talk3D å¯ä»¥æœ‰æ•ˆåœ°å‡å°‘å…‰ç…§å’Œè¡¨æƒ…å˜åŒ–ç­‰éŸ³é¢‘æ— å…³å› ç´ çš„å½±å“ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šTalk3Dï¼šé«˜ä¿çœŸè¯´è¯äººå›¾åƒåˆæˆ</li><li>ä½œè€…ï¼šChangil Kim, Minhyeok Lee, Juyong Kim, Nojun Kwak</li><li>éš¶å±æœºæ„ï¼šé¦–å°”å›½ç«‹å¤§å­¦</li><li>å…³é”®è¯ï¼šéŸ³é¢‘é©±åŠ¨è¯´è¯äººå›¾åƒåˆæˆã€ç¥ç»è¾å°„åœºã€3D æ„ŸçŸ¥ç”Ÿæˆæ¨¡å‹</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.20153</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šè¿‘å¹´æ¥ï¼ŒéŸ³é¢‘é©±åŠ¨çš„è¯´è¯äººå›¾åƒåˆæˆæ–¹æ³•å–å¾—äº†å¾ˆå¤§è¿›å±•ï¼Œä½†è¿™äº›æ–¹æ³•é€šå¸¸ä¾èµ–äºå•ç›®è¯´è¯äººå›¾åƒè§†é¢‘ï¼Œéš¾ä»¥é‡å»ºå®Œæ•´çš„é¢éƒ¨å‡ ä½•ç»“æ„ã€‚</li></ol><p>ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç°æœ‰æ–¹æ³•é€šå¸¸ä¼˜åŒ–ç¥ç»è¾å°„åœº (NeRF) æ¥æ¸²æŸ“æ–°è§†è§’çš„å›¾åƒï¼Œä½†ç”±äºè¾“å…¥å•ç›®è§†é¢‘ä¸­ç¼ºä¹å…¨é¢çš„ 3D ä¿¡æ¯ï¼Œå®ƒä»¬éš¾ä»¥é‡å»ºå®Œæ•´çš„é¢éƒ¨å‡ ä½•ç»“æ„ã€‚</p><p>ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„éŸ³é¢‘é©±åŠ¨çš„è¯´è¯äººå›¾åƒåˆæˆæ¡†æ¶ Talk3Dï¼Œå®ƒé€šè¿‡æœ‰æ•ˆé‡‡ç”¨é¢„è®­ç»ƒçš„ 3D æ„ŸçŸ¥ç”Ÿæˆæ¨¡å‹ï¼Œå¯ä»¥å¿ å®åœ°é‡å»ºå¯ä¿¡çš„é¢éƒ¨å‡ ä½•ç»“æ„ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸€ä¸ªéŸ³é¢‘å¼•å¯¼æ³¨æ„åŠ› U-Net æ¶æ„ï¼Œè¯¥æ¶æ„é¢„æµ‹ç”±éŸ³é¢‘é©±åŠ¨çš„ NeRF ç©ºé—´ä¸­çš„åŠ¨æ€é¢éƒ¨å˜åŒ–ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹è¿˜é€šè¿‡ä¸éŸ³é¢‘æ— å…³çš„è°ƒèŠ‚ä»¤ç‰Œè¿›è¡Œè°ƒåˆ¶ï¼Œæœ‰æ•ˆåœ°è§£è€¦ä¸éŸ³é¢‘ç‰¹å¾æ— å…³çš„å˜åŒ–ã€‚</p><p>ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šåœ¨è¯´è¯äººå›¾åƒåˆæˆä»»åŠ¡ä¸Šï¼ŒTalk3D åœ¨å›¾åƒè´¨é‡ã€å‡ ä½•ä¿çœŸåº¦å’ŒéŸ³é¢‘åŒæ­¥æ–¹é¢éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†è¯¥æ–¹æ³•çš„ç›®æ ‡ï¼Œå³ç”Ÿæˆé«˜è´¨é‡ä¸”é€¼çœŸçš„è¯´è¯äººå›¾åƒã€‚</p><ol><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰EG3Dæ¨¡å‹ï¼šé‡‡ç”¨ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æŠ€æœ¯ï¼Œé€šè¿‡ä¼˜åŒ–å¹³é¢ç”Ÿæˆå™¨å’Œä½“ç§¯æ¸²æŸ“å™¨ï¼Œå®ç°å›¾åƒç”Ÿæˆã€‚ï¼ˆ2ï¼‰ä¸ªæ€§åŒ–ç”Ÿæˆå™¨ï¼šä½¿ç”¨VIVE3Dç­–ç•¥ï¼Œå°†3Dæ„ŸçŸ¥ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰è°ƒæ•´ä¸ºç‰¹å®šèº«ä»½ï¼Œç”Ÿæˆå•ä¸€èº«ä»½å›¾åƒã€‚ï¼ˆ3ï¼‰éŸ³é¢‘å¼•å¯¼æ³¨æ„åŠ›U-Netï¼šåˆ©ç”¨U-Netæ¶æ„ï¼Œé¢„æµ‹ç”±éŸ³é¢‘é©±åŠ¨çš„NeRFç©ºé—´ä¸­çš„åŠ¨æ€é¢éƒ¨å˜åŒ–ï¼Œå¹¶é€šè¿‡ä¸éŸ³é¢‘æ— å…³çš„è°ƒèŠ‚ä»¤ç‰Œè¿›è¡Œè°ƒåˆ¶ã€‚ï¼ˆ4ï¼‰åˆ†å‰²å·ç§¯ï¼šå°†æ¯ä¸ªå¹³é¢ç‹¬ç«‹å¤„ç†ï¼Œä»¥ç»´æŠ¤å…¶ç‰¹å¾ï¼ŒåŒæ—¶ä½¿ç”¨å±•å¼€æ–¹æ³•èåˆæ¥è‡ªæ¯ä¸ªå¹³é¢çš„ç‰¹å¾ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº† Talk3Dï¼Œä¸€ä¸ªç»“åˆäº† 3D æ„ŸçŸ¥ GAN å…ˆéªŒå’ŒåŒºåŸŸæ„ŸçŸ¥è¿åŠ¨çš„é«˜ä¿çœŸ 3D è¯´è¯äººå›¾åƒåˆæˆæ¡†æ¶ã€‚æˆ‘ä»¬çš„æ¡†æ¶é›†æˆäº†ä½¿ç”¨ VIVE3D æ¡†æ¶å¾®è°ƒçš„ä¸ªæ€§åŒ–ç”Ÿæˆå™¨ï¼Œå…è®¸ç”Ÿæˆå…·æœ‰é€¼çœŸå‡ ä½•ç»“æ„å’Œæ˜¾å¼æ¸²æŸ“è§†ç‚¹æ§åˆ¶çš„ 3D æ„ŸçŸ¥è¯´è¯äººå¤´åƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºçš„éŸ³é¢‘å¼•å¯¼æ³¨æ„åŠ› U-Net æ¶æ„å¢å¼ºäº†å›¾åƒå¸§å†…å±€éƒ¨å˜åŒ–ï¼ˆå¦‚èƒŒæ™¯ã€èº¯å¹²å’Œçœ¼ç›è¿åŠ¨ï¼‰çš„è§£è€¦ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬æå‡ºçš„æ¨¡å‹ä¸ä»…å¯ä»¥æ ¹æ®è¾“å…¥éŸ³é¢‘äº§ç”Ÿå‡†ç¡®çš„å”‡éƒ¨åŠ¨ä½œï¼Œè¿˜å¯ä»¥ä»æ–°é¢–çš„è§†ç‚¹è¿›è¡Œæ¸²æŸ“ï¼Œè§£å†³äº†å…ˆå‰æœ€å…ˆè¿›æ–¹æ³•ä¸­è§‚å¯Ÿåˆ°çš„å±€é™æ€§ã€‚æˆ‘ä»¬é¢„æœŸæˆ‘ä»¬çš„å·¥ä½œå°†å¯¹æ•°å­—åª’ä½“ä½“éªŒå’Œè™šæ‹Ÿäº¤äº’äº§ç”Ÿé‡å¤§å½±å“ï¼Œå¹¶åœ¨ç”µå½±åˆ¶ä½œã€è™šæ‹ŸåŒ–èº«å’Œè§†é¢‘ä¼šè®®ä¸­æ‰¾åˆ°åº”ç”¨ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šï¼ˆ1ï¼‰æå‡ºäº†ä¸€ç§æ–°çš„éŸ³é¢‘é©±åŠ¨çš„è¯´è¯äººå›¾åƒåˆæˆæ¡†æ¶ Talk3Dï¼Œè¯¥æ¡†æ¶å°† 3D æ„ŸçŸ¥ GAN å…ˆéªŒå’ŒåŒºåŸŸæ„ŸçŸ¥è¿åŠ¨ç›¸ç»“åˆï¼Œä»¥å®ç°é«˜ä¿çœŸ 3D è¯´è¯äººå¤´åƒåˆæˆã€‚ï¼ˆ2ï¼‰åˆ©ç”¨ VIVE3D ç­–ç•¥ï¼Œå°† 3D æ„ŸçŸ¥ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ (GAN) è°ƒæ•´ä¸ºç‰¹å®šèº«ä»½ï¼Œç”Ÿæˆå…·æœ‰é€¼çœŸå‡ ä½•ç»“æ„å’Œæ˜¾å¼æ¸²æŸ“è§†ç‚¹æ§åˆ¶çš„ 3D æ„ŸçŸ¥è¯´è¯äººå¤´åƒã€‚ï¼ˆ3ï¼‰æå‡ºäº†ä¸€ä¸ªéŸ³é¢‘å¼•å¯¼æ³¨æ„åŠ› U-Net æ¶æ„ï¼Œè¯¥æ¶æ„å¢å¼ºäº†å›¾åƒå¸§å†…å±€éƒ¨å˜åŒ–ï¼ˆå¦‚èƒŒæ™¯ã€èº¯å¹²å’Œçœ¼ç›è¿åŠ¨ï¼‰çš„è§£è€¦ã€‚æ€§èƒ½ï¼šï¼ˆ1ï¼‰åœ¨è¯´è¯äººå›¾åƒåˆæˆä»»åŠ¡ä¸Šï¼ŒTalk3D åœ¨å›¾åƒè´¨é‡ã€å‡ ä½•ä¿çœŸåº¦å’ŒéŸ³é¢‘åŒæ­¥æ–¹é¢éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ï¼ˆ2ï¼‰Talk3D èƒ½å¤Ÿä»æ–°é¢–çš„è§†ç‚¹è¿›è¡Œæ¸²æŸ“ï¼Œè§£å†³äº†å…ˆå‰æœ€å…ˆè¿›æ–¹æ³•ä¸­è§‚å¯Ÿåˆ°çš„å±€é™æ€§ã€‚ï¼ˆ3ï¼‰Talk3D å¯ä»¥æœ‰æ•ˆåœ°è§£è€¦ä¸éŸ³é¢‘ç‰¹å¾æ— å…³çš„å˜åŒ–ï¼Œä»è€Œç”Ÿæˆæ›´é€¼çœŸã€æ›´è‡ªç„¶çš„é«˜ä¿çœŸ 3D è¯´è¯äººå›¾åƒã€‚å·¥ä½œé‡ï¼šï¼ˆ1ï¼‰Talk3D çš„å®ç°éœ€è¦å¤§é‡çš„æ•°æ®é¢„å¤„ç†å’Œæ¨¡å‹è®­ç»ƒã€‚ï¼ˆ2ï¼‰Talk3D çš„æ¨ç†è¿‡ç¨‹ç›¸å¯¹é«˜æ•ˆï¼Œå¯ä»¥å®æ—¶ç”Ÿæˆé«˜è´¨é‡çš„ 3D è¯´è¯äººå›¾åƒã€‚ï¼ˆ3ï¼‰Talk3D çš„ä»£ç å’Œæ•°æ®å·²å…¬å¼€ï¼Œä¾¿äºç ”ç©¶äººå‘˜å’Œä»ä¸šè€…è¿›ä¸€æ­¥ç ”ç©¶å’Œåº”ç”¨ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-b945787a9603752fdfa9bacd5ecbd8e0.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1eb3bf1b0c5000057abc431bf6035fce.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9e4d3acaf0612269dbaa41a149d52930.jpg" align="middle"></details><h2 id="SGD-Street-View-Synthesis-with-Gaussian-Splatting-and-Diffusion-Prior"><a href="#SGD-Street-View-Synthesis-with-Gaussian-Splatting-and-Diffusion-Prior" class="headerlink" title="SGD: Street View Synthesis with Gaussian Splatting and Diffusion Prior"></a>SGD: Street View Synthesis with Gaussian Splatting and Diffusion Prior</h2><p><strong>Authors:Zhongrui Yu, Haoran Wang, Jinze Yang, Hanzhang Wang, Zeke Xie, Yunfeng Cai, Jiale Cao, Zhong Ji, Mingming Sun</strong></p><p>Novel View Synthesis (NVS) for street scenes play a critical role in the autonomous driving simulation. The current mainstream technique to achieve it is neural rendering, such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). Although thrilling progress has been made, when handling street scenes, current methods struggle to maintain rendering quality at the viewpoint that deviates significantly from the training viewpoints. This issue stems from the sparse training views captured by a fixed camera on a moving vehicle. To tackle this problem, we propose a novel approach that enhances the capacity of 3DGS by leveraging prior from a Diffusion Model along with complementary multi-modal data. Specifically, we first fine-tune a Diffusion Model by adding images from adjacent frames as condition, meanwhile exploiting depth data from LiDAR point clouds to supply additional spatial information. Then we apply the Diffusion Model to regularize the 3DGS at unseen views during training. Experimental results validate the effectiveness of our method compared with current state-of-the-art models, and demonstrate its advance in rendering images from broader views. </p><p><a href="http://arxiv.org/abs/2403.20079v1">PDF</a> </p><p><strong>Summary</strong><br>ç¥ç»è¾å°„åœºå’Œé«˜æ–¯ä½“æ¸²æŸ“ç­‰ç¥ç»æ¸²æŸ“æŠ€æœ¯åœ¨è‡ªåŠ¨é©¾é©¶æ¨¡æ‹Ÿä¸­æ‰®æ¼”ç€å…³é”®è§’è‰²ï¼Œä½†å¤„ç†è¡—é“åœºæ™¯æ—¶ï¼Œæ­¤ç±»æŠ€æœ¯éš¾ä»¥ä¿æŒåç¦»è®­ç»ƒè§†è§’è¾ƒå¤§çš„è§†ç‚¹çš„æ¸²æŸ“è´¨é‡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†å’Œè¡¥å……çš„å¤šæ¨¡æ€æ•°æ®æ¥å¢å¼º3Dé«˜æ–¯ä½“æ¸²æŸ“çš„èƒ½åŠ›ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ç¥ç»æ¸²æŸ“æŠ€æœ¯åœ¨è‡ªåŠ¨é©¾é©¶æ¨¡æ‹Ÿä¸­çš„è¡—æ™¯æ–°è§†è§’åˆæˆï¼ˆNVSï¼‰ä¸­è‡³å…³é‡è¦ã€‚</li><li>å½“å‰çš„ç¥ç»æ¸²æŸ“æ–¹æ³•åœ¨å¤„ç†è¡—æ™¯æ—¶ï¼Œéš¾ä»¥ä¿æŒåç¦»è®­ç»ƒè§†è§’è¾ƒå¤§çš„è§†ç‚¹çš„æ¸²æŸ“è´¨é‡ã€‚</li><li>é—®é¢˜æºäºç§»åŠ¨è½¦è¾†ä¸Šå›ºå®šæ‘„åƒæœºæ•è·çš„ç¨€ç–è®­ç»ƒè§†å›¾ã€‚</li><li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†å’Œè¡¥å……çš„å¤šæ¨¡æ€æ•°æ®æ¥å¢å¼º3Dé«˜æ–¯ä½“æ¸²æŸ“çš„èƒ½åŠ›ã€‚</li><li>é¦–å…ˆé€šè¿‡æ·»åŠ ç›¸é‚»å¸§å›¾åƒä½œä¸ºæ¡ä»¶å¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼ŒåŒæ—¶åˆ©ç”¨æ¿€å…‰é›·è¾¾ç‚¹äº‘çš„æ·±åº¦æ•°æ®æä¾›é¢å¤–çš„ç©ºé—´ä¿¡æ¯ã€‚</li><li>ç„¶åå°†æ‰©æ•£æ¨¡å‹åº”ç”¨äºè®­ç»ƒæœŸé—´æœªè§è§†å›¾ä¸­çš„3Dé«˜æ–¯ä½“æ¸²æŸ“è¿›è¡Œæ­£åˆ™åŒ–ã€‚</li><li>å®éªŒç»“æœéªŒè¯äº†è¯¥æ–¹æ³•ä¸ç›®å‰æœ€å…ˆè¿›æ¨¡å‹ç›¸æ¯”çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨ä»æ›´å®½å¹¿çš„è§†è§’æ¸²æŸ“å›¾åƒæ–¹é¢çš„ä¼˜åŠ¿ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šSGDï¼šé«˜æ–¯ç‚¹ splatting å’Œæ‰©æ•£å…ˆéªŒçš„è¡—æ™¯åˆæˆ</li><li>ä½œè€…ï¼šZhongrui Yuâ€ ã€Haoran Wangâ€¡ã€Jinze Yangã€Hanzhang Wangã€Zeke Xieã€Yunfeng Caiã€Jiale Caoã€Zhong Jiã€Mingming Sun</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šè‹é»ä¸–è”é‚¦ç†å·¥å­¦é™¢</li><li>å…³é”®è¯ï¼šè¡—æ™¯åˆæˆã€ç¥ç»æ¸²æŸ“ã€æ‰©æ•£æ¨¡å‹ã€é«˜æ–¯ splatting</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.20079    Github ä»£ç é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼š(1)ï¼šç ”ç©¶èƒŒæ™¯ï¼šè¡—æ™¯åˆæˆï¼ˆNVSï¼‰åœ¨è‡ªåŠ¨é©¾é©¶ä»¿çœŸä¸­è‡³å…³é‡è¦ã€‚å½“å‰ä¸»æµçš„ç¥ç»æ¸²æŸ“æ–¹æ³•ï¼Œå¦‚ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å’Œ 3D é«˜æ–¯ splattingï¼ˆ3DGSï¼‰ï¼Œåœ¨å¤„ç†è¡—æ™¯æ—¶éš¾ä»¥ä¿æŒè¿œç¦»è®­ç»ƒè§†ç‚¹çš„æ¸²æŸ“è´¨é‡ã€‚(2)ï¼šè¿‡å»æ–¹æ³•ä¸é—®é¢˜ï¼šç°æœ‰æ–¹æ³•å—é™äºç§»åŠ¨è½¦è¾†ä¸Šå›ºå®šæ‘„åƒæœºé‡‡é›†çš„ç¨€ç–è®­ç»ƒè§†ç‚¹ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å…ˆéªŒå’Œäº’è¡¥çš„å¤šæ¨¡æ€æ•°æ®æ¥å¢å¼º 3DGS çš„èƒ½åŠ›ã€‚(3)ï¼šç ”ç©¶æ–¹æ³•ï¼šè¯¥æ–¹æ³•é¦–å…ˆå¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œæ·»åŠ ç›¸é‚»å¸§çš„å›¾åƒä½œä¸ºæ¡ä»¶ï¼ŒåŒæ—¶åˆ©ç”¨æ¿€å…‰é›·è¾¾ç‚¹äº‘çš„æ·±åº¦æ•°æ®æä¾›é¢å¤–çš„ç©ºé—´ä¿¡æ¯ã€‚ç„¶ååœ¨è®­ç»ƒæœŸé—´å°†æ‰©æ•£æ¨¡å‹åº”ç”¨äºæœªè§è§†ç‚¹çš„ 3DGS æ­£åˆ™åŒ–ã€‚(4)ï¼šä»»åŠ¡ä¸æ€§èƒ½ï¼šå®éªŒç»“æœéªŒè¯äº†è¯¥æ–¹æ³•ä¸å½“å‰æœ€å…ˆè¿›æ¨¡å‹ç›¸æ¯”çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨ä»æ›´å¹¿æ³›è§†ç‚¹æ¸²æŸ“å›¾åƒæ–¹é¢çš„ä¼˜åŠ¿ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼šå¾®è°ƒæ‰©æ•£æ¨¡å‹ï¼Œä»¥åˆ©ç”¨ç›¸é‚»å¸§çš„å›¾åƒä½œä¸ºæ¡ä»¶ï¼Œå¹¶é€šè¿‡æ¿€å…‰é›·è¾¾ç‚¹äº‘çš„æ·±åº¦æ•°æ®æä¾›é¢å¤–çš„ç©ºé—´ä¿¡æ¯ï¼›ï¼ˆ2ï¼‰ï¼šå°†å¾®è°ƒåçš„æ‰©æ•£æ¨¡å‹åº”ç”¨äº 3DGS è®­ç»ƒæœŸé—´ï¼Œä»¥æ­£åˆ™åŒ–æœªè§è§†ç‚¹çš„åˆæˆï¼›ï¼ˆ3ï¼‰ï¼šåœ¨ 3DGS è®­ç»ƒä¸­ï¼Œéšæœºé‡‡æ ·ä¼ªè§†å›¾ï¼Œå¹¶ä½¿ç”¨æ‰©æ•£æ¨¡å‹ç”ŸæˆæŒ‡å¯¼å›¾åƒï¼Œä»¥æ­£åˆ™åŒ– 3DGS æ¨¡å‹çš„è®­ç»ƒã€‚</p></li><li><p>ç»“è®ºï¼š(1): æœ¬å·¥ä½œé€šè¿‡å°†æ‰©æ•£æ¨¡å‹ä¸3DGSç›¸ç»“åˆï¼Œæœ‰æ•ˆæå‡äº†è‡ªåŠ¨é©¾é©¶åœºæ™¯ä¸­çš„è‡ªç”±è§†è§’æ¸²æŸ“èƒ½åŠ›ï¼Œä¸ºè‡ªåŠ¨é©¾é©¶ä»¿çœŸæä¾›äº†æ›´å¹¿é˜”çš„è§†è§’ï¼Œæœ‰åˆ©äºæ¨¡æ‹Ÿæ½œåœ¨çš„å±é™©è¾¹ç¼˜æƒ…å†µï¼Œä»è€Œæå‡è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„æ•´ä½“å®‰å…¨æ€§å’Œå¯é æ€§ã€‚(2): åˆ›æ–°ç‚¹ï¼šå°†æ‰©æ•£æ¨¡å‹å¼•å…¥3DGSï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å…ˆéªŒå’Œäº’è¡¥çš„å¤šæ¨¡æ€æ•°æ®å¢å¼º3DGSçš„èƒ½åŠ›ã€‚æ€§èƒ½ï¼šä¸å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨ä»æ›´å¹¿æ³›è§†ç‚¹æ¸²æŸ“å›¾åƒæ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚å·¥ä½œé‡ï¼šæ‰©æ•£æ¨¡å‹çš„åŠ å…¥å¢åŠ äº†è®­ç»ƒæ—¶é—´ï¼Œä½†è¯¥æ–¹æ³•ä¸å½±å“3DGSçš„å®æ—¶æ¨ç†èƒ½åŠ›ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-59b729de5a1f08214181a45a66fe05e1.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e3d1eae8db53c2b14375454d2d6f0cd9.jpg" align="middle"></details><h2 id="DerainNeRF-3D-Scene-Estimation-with-Adhesive-Waterdrop-Removal"><a href="#DerainNeRF-3D-Scene-Estimation-with-Adhesive-Waterdrop-Removal" class="headerlink" title="DerainNeRF: 3D Scene Estimation with Adhesive Waterdrop Removal"></a>DerainNeRF: 3D Scene Estimation with Adhesive Waterdrop Removal</h2><p><strong>Authors:Yunhao Li, Jing Wu, Lingzhe Zhao, Peidong Liu</strong></p><p>When capturing images through the glass during rainy or snowy weather conditions, the resulting images often contain waterdrops adhered on the glass surface, and these waterdrops significantly degrade the image quality and performance of many computer vision algorithms. To tackle these limitations, we propose a method to reconstruct the clear 3D scene implicitly from multi-view images degraded by waterdrops. Our method exploits an attention network to predict the location of waterdrops and then train a Neural Radiance Fields to recover the 3D scene implicitly. By leveraging the strong scene representation capabilities of NeRF, our method can render high-quality novel-view images with waterdrops removed. Extensive experimental results on both synthetic and real datasets show that our method is able to generate clear 3D scenes and outperforms existing state-of-the-art (SOTA) image adhesive waterdrop removal methods. </p><p><a href="http://arxiv.org/abs/2403.20013v1">PDF</a> </p><p><strong>Summary</strong><br>åˆ©ç”¨NeRFå’Œæ³¨æ„åŠ›æœºåˆ¶ï¼Œä»æœ‰æ°´æ»´çš„å›¾åƒä¸­é‡å»ºæ¸…æ™°çš„3Dåœºæ™¯ï¼Œå»é™¤æ°´æ»´ï¼Œæé«˜å›¾åƒè´¨é‡å’Œè®¡ç®—æœºè§†è§‰ç®—æ³•æ€§èƒ½ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ä½¿ç”¨NeRFé‡å»ºæœ‰æ°´æ»´å›¾åƒä¸­çš„3Dåœºæ™¯ï¼Œå»é™¤æ°´æ»´ï¼Œæé«˜å›¾åƒè´¨é‡ã€‚</li><li>åˆ©ç”¨æ³¨æ„åŠ›ç½‘ç»œé¢„æµ‹æ°´æ»´ä½ç½®ã€‚</li><li>å……åˆ†åˆ©ç”¨NeRFå¼ºå¤§çš„åœºæ™¯è¡¨ç¤ºèƒ½åŠ›ã€‚</li><li>æ¸²æŸ“å‡ºæ— æ°´æ»´çš„é«˜è´¨é‡æ–°è§†å›¾å›¾åƒã€‚</li><li>åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šå–å¾—ä¼˜å¼‚çš„å®éªŒç»“æœã€‚</li><li>è¶…è¶Šç°æœ‰æ°´æ»´å»é™¤æ–¹æ³•çš„æ€§èƒ½ã€‚</li><li>æä¾›æ¸…æ™°çš„3Dåœºæ™¯ï¼Œæ”¹å–„è®¡ç®—æœºè§†è§‰ç®—æ³•æ€§èƒ½ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šDerainNeRFï¼šå»é™¤ç²˜æ€§æ°´æ»´çš„ 3D åœºæ™¯ä¼°è®¡</li><li>ä½œè€…ï¼šæäº‘æµ©ã€å´é–ã€èµµä»¤å“²ã€åˆ˜åŸ¹ä¸œ</li><li>å•ä½ï¼šè¥¿æ¹–å¤§å­¦å·¥ç¨‹å­¦é™¢</li><li>å…³é”®è¯ï¼šNeRFã€æ°´æ»´å»é™¤ã€3D åœºæ™¯é‡å»º</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.20013</li><li>æ€»ç»“ï¼š   ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šé›¨é›ªå¤©æ°”ä¸‹æ‹æ‘„çš„å›¾åƒç»å¸¸ä¼šå‡ºç°ç²˜é™„åœ¨ç»ç’ƒè¡¨é¢çš„æ°´æ»´ï¼Œä¸¥é‡å½±å“å›¾åƒè´¨é‡å’Œè®¡ç®—æœºè§†è§‰ç®—æ³•çš„æ€§èƒ½ã€‚   ï¼ˆ2ï¼‰ä»¥å¾€æ–¹æ³•ï¼šç°æœ‰çš„æ°´æ»´å»é™¤æ–¹æ³•æ— æ³•å¾ˆå¥½åœ°å¤„ç†ç²˜æ€§æ°´æ»´ï¼Œå› ä¸ºç²˜æ€§æ°´æ»´å…·æœ‰éšæœºçš„ç©ºé—´åˆ†å¸ƒã€ä¸è§„åˆ™çš„å½¢çŠ¶ä»¥åŠå¤æ‚çš„æŠ˜å°„å’Œåå°„ç‰¹æ€§ã€‚   ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäº NeRF çš„æ¡†æ¶ DerainNeRFï¼Œè¯¥æ¡†æ¶åŒæ—¶ä¼°è®¡ 3D åœºæ™¯å¹¶å»é™¤æ°´æ»´ã€‚DerainNeRF åˆ©ç”¨é¢„è®­ç»ƒçš„æ°´æ»´æ£€æµ‹ç½‘ç»œé¢„æµ‹æ°´æ»´çš„ä½ç½®ï¼Œç„¶ååœ¨ NeRF è®­ç»ƒæœŸé—´æ’é™¤è¢«æ°´æ»´é®æŒ¡çš„åƒç´ ï¼Œä»è€Œä»æœªè¢«é®æŒ¡çš„åƒç´ ä¸­æ¢å¤æ¸…æ™°çš„åœºæ™¯ã€‚   ï¼ˆ4ï¼‰å®éªŒç»“æœï¼šåœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒDerainNeRF å¯ä»¥æœ‰æ•ˆåœ°ä»æ°´æ»´å›¾åƒä¸­ä¼°è®¡æ¸…æ™°çš„ 3D åœºæ™¯ï¼Œå¹¶æ¸²æŸ“å‡ºå»é™¤æ°´æ»´çš„é«˜è´¨é‡æ–°è§†è§’å›¾åƒã€‚</li></ol><p>7.æ–¹æ³•ï¼š(1): DerainNeRF é‡‡ç”¨é¢„è®­ç»ƒçš„æ°´æ»´æ£€æµ‹ç½‘ç»œ AttGANï¼Œæ ¹æ®æ³¨æ„åŠ›å›¾ç”ŸæˆäºŒå€¼æ©ç ï¼Œæ ‡è®°æ°´æ»´è¦†ç›–çš„åŒºåŸŸï¼›(2): åœ¨ NeRF è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œåˆ©ç”¨æ©ç æ’é™¤æ°´æ»´è¦†ç›–çš„åƒç´ ï¼Œä»…ä»æœªè¢«é®æŒ¡çš„åƒç´ ä¸­æ¢å¤æ¸…æ™°åœºæ™¯ï¼›(3): é‡‡ç”¨æ©ç å¯¹ NeRF çš„å…‰åº¦æŸå¤±è¿›è¡Œä¿®æ”¹ï¼Œä½¿æ°´æ»´è¦†ç›–çš„åƒç´ ä¸å‚ä¸ NeRF ä¼˜åŒ–ï¼›(4): é’ˆå¯¹ç›¸æœºé•œå¤´ä¸Šçš„æ°´æ»´ï¼ŒDerainNeRF è€ƒè™‘æ°´æ»´ç›¸å¯¹ç›¸æœºé™æ­¢æˆ–ç¼“æ…¢ç§»åŠ¨çš„æƒ…å†µï¼Œå¹¶é€šè¿‡æ©ç æ’é™¤ç›¸åº”åŒºåŸŸçš„åƒç´ ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº† DerainNeRF æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŒæ—¶ä¼°è®¡ 3D åœºæ™¯å¹¶å»é™¤æ°´æ»´ï¼Œæœ‰æ•ˆåœ°ä»æ°´æ»´å›¾åƒä¸­æ¢å¤æ¸…æ™°çš„åœºæ™¯ï¼Œå¹¶æ¸²æŸ“å‡ºå»é™¤æ°´æ»´çš„é«˜è´¨é‡æ–°è§†è§’å›¾åƒã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šDerainNeRF åˆ›æ–°æ€§åœ°å°†æ°´æ»´æ£€æµ‹ç½‘ç»œä¸ NeRF ç›¸ç»“åˆï¼Œé€šè¿‡æ’é™¤æ°´æ»´é®æŒ¡åƒç´ ï¼Œä»æœªè¢«é®æŒ¡çš„åƒç´ ä¸­æ¢å¤æ¸…æ™°åœºæ™¯ï¼Œæœ‰æ•ˆè§£å†³äº†ç²˜æ€§æ°´æ»´å»é™¤é—®é¢˜ã€‚æ€§èƒ½ï¼šDerainNeRF åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œå…¶åœ¨æ°´æ»´å»é™¤å’Œ 3D åœºæ™¯ä¼°è®¡æ–¹é¢å‡å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œæœ‰æ•ˆåœ°æé«˜äº†å›¾åƒè´¨é‡å’Œè®¡ç®—æœºè§†è§‰ç®—æ³•çš„æ€§èƒ½ã€‚å·¥ä½œé‡ï¼šDerainNeRF çš„å®ç°éœ€è¦é¢„è®­ç»ƒæ°´æ»´æ£€æµ‹ç½‘ç»œå’Œ NeRF æ¨¡å‹ï¼Œè®­ç»ƒè¿‡ç¨‹éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºï¼Œå·¥ä½œé‡è¾ƒå¤§ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-19f242cbbe40d04087d7fa4b5738c1fa.jpg" align="middle"><img src="https://pica.zhimg.com/v2-074e9dac4fc4c02c192b25a9db8280ac.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e9ee6dcf58671bff81f8e539beb1bd41.jpg" align="middle"><img src="https://pica.zhimg.com/v2-6a035cd7369fde02daca89446ae14e04.jpg" align="middle"></details><h2 id="Stable-Surface-Regularization-for-Fast-Few-Shot-NeRF"><a href="#Stable-Surface-Regularization-for-Fast-Few-Shot-NeRF" class="headerlink" title="Stable Surface Regularization for Fast Few-Shot NeRF"></a>Stable Surface Regularization for Fast Few-Shot NeRF</h2><p><strong>Authors:Byeongin Joung, Byeong-Uk Lee, Jaesung Choe, Ukcheol Shin, Minjun Kang, Taeyeop Lee, In So Kweon, Kuk-Jin Yoon</strong></p><p>This paper proposes an algorithm for synthesizing novel views under few-shot setup. The main concept is to develop a stable surface regularization technique called Annealing Signed Distance Function (ASDF), which anneals the surface in a coarse-to-fine manner to accelerate convergence speed. We observe that the Eikonal loss - which is a widely known geometric regularization - requires dense training signal to shape different level-sets of SDF, leading to low-fidelity results under few-shot training. In contrast, the proposed surface regularization successfully reconstructs scenes and produce high-fidelity geometry with stable training. Our method is further accelerated by utilizing grid representation and monocular geometric priors. Finally, the proposed approach is up to 45 times faster than existing few-shot novel view synthesis methods, and it produces comparable results in the ScanNet dataset and NeRF-Real dataset. </p><p><a href="http://arxiv.org/abs/2403.19985v1">PDF</a> 3DV 2024</p><p><strong>Summary</strong><br>æ–°é¢–çš„ annealed signed distance function æ­£åˆ™åŒ–æŠ€æœ¯å®ç°äº†å°æ ·æœ¬åœºæ™¯é‡å»ºä¸­ç¨³å®šçš„è¡¨é¢æ­£åˆ™åŒ–ï¼Œå¤§å¹…æå‡äº†æ”¶æ•›é€Ÿåº¦ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ASDF ä½œä¸ºæœ‰æ•ˆçš„è¡¨é¢æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œé€šè¿‡ç²—åˆ°ç²¾çš„é€€ç«æ–¹å¼åŠ é€Ÿæ”¶æ•›ã€‚</li><li>Eikonal æŸå¤±åœ¨å°æ ·æœ¬è®­ç»ƒä¸­å› ç¼ºä¹è¶³å¤Ÿçš„è®­ç»ƒä¿¡å·è€Œå¯¼è‡´æ¨¡å‹ä¿çœŸåº¦ä½ã€‚</li><li>ASDF æ­£åˆ™åŒ–æˆåŠŸé‡å»ºåœºæ™¯å¹¶äº§ç”Ÿé«˜ä¿çœŸå‡ ä½•ä½“ï¼Œè®­ç»ƒç¨³å®šæ€§é«˜ã€‚</li><li>é‡‡ç”¨ç½‘æ ¼è¡¨ç¤ºå’Œå•ç›®å‡ ä½•å…ˆéªŒè¿›ä¸€æ­¥åŠ é€Ÿäº†è¯¥æ–¹æ³•ã€‚</li><li>è¯¥æ–¹æ³•æ¯”ç°æœ‰å°æ ·æœ¬æ–°é¢–è§†å›¾åˆæˆæ–¹æ³•å¿« 45 å€ï¼Œä¸”åœ¨ ScanNet å’Œ NeRF-Real æ•°æ®é›†ä¸Šäº§ç”Ÿå…·æœ‰å¯æ¯”æ€§çš„ç»“æœã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>è®ºæ–‡æ ‡é¢˜ï¼šå¿«é€Ÿå°æ ·æœ¬ NeRF çš„ç¨³å®šè¡¨é¢æ­£åˆ™åŒ–</li><li>ä½œè€…ï¼šByeongin Joungã€Byeong-Uk Leeã€Jaesung Choeã€Ukcheol Shinã€Minjun Kangã€Taeyeop Leeã€In So Kweonã€Kuk-Jin Yoon</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šéŸ©å›½ç§‘å­¦æŠ€æœ¯é™¢</li><li>å…³é”®è¯ï¼šNeRFã€å°æ ·æœ¬å­¦ä¹ ã€è¡¨é¢æ­£åˆ™åŒ–ã€å‡ ä½•çº¦æŸ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.19985</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šNeRF æ˜¯ä¸€ç§ç”¨äºéšå¼åœºæ™¯å¤–è§‚å’Œå‡ ä½•ç¼–ç çš„æœ‰æ•ˆæŠ€æœ¯ï¼Œä½†åœ¨å°æ ·æœ¬è®¾ç½®ä¸‹è®­ç»ƒ NeRF å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºéœ€è¦å¤§é‡å›¾åƒå’Œè¾ƒé•¿çš„è®­ç»ƒæ—¶é—´ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šç°æœ‰çš„æ–¹æ³•åˆ©ç”¨æœªè§‚æµ‹è§†ç‚¹æ­£åˆ™åŒ–ã€ç†µæœ€å°åŒ–å’Œå‡ ä½•å…ˆéªŒæ¥è§£å†³å°æ ·æœ¬é—®é¢˜ï¼Œä½†è¿™äº›æ–¹æ³•åœ¨å¤„ç†ç¨€ç–è¾“å…¥è§†å›¾æ—¶ä»å­˜åœ¨å›°éš¾ã€‚ï¼ˆ3ï¼‰æœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç§°ä¸ºé€€ç«ç¬¦å·è·ç¦»å‡½æ•° (ASDF) çš„ç¨³å®šè¡¨é¢æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œå®ƒä»¥ç²—åˆ°ç»†çš„æ–¹å¼é€€ç«è¡¨é¢ä»¥åŠ é€Ÿæ”¶æ•›é€Ÿåº¦ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜åˆ©ç”¨ç½‘æ ¼è¡¨ç¤ºå’Œå•ç›®å‡ ä½•å…ˆéªŒè¿›ä¸€æ­¥åŠ é€Ÿäº†è®­ç»ƒè¿‡ç¨‹ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šæœ¬æ–‡æ–¹æ³•åœ¨ ScanNet å’Œ NeRF-Real æ•°æ®é›†ä¸Šå®ç°äº†ä¸ç°æœ‰æ–¹æ³•ç›¸å½“çš„ç»“æœï¼ŒåŒæ—¶è®­ç»ƒé€Ÿåº¦æé«˜äº† 45 å€ã€‚è¿™è¡¨æ˜æœ¬æ–‡æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°åˆæˆå°æ ·æœ¬ NeRF çš„æ–°é¢–è§†å›¾ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰åˆ©ç”¨ OmniData æå–ç»™å®š RGB å›¾åƒçš„å‡ ä½•å…ˆéªŒï¼Œä½¿ç”¨ COLMAP è·å–ç¨€ç– 3D ç‚¹å’Œç›¸æœºä½å§¿ã€‚ï¼ˆ2ï¼‰æ„å»ºå¤šçº§ç‰¹å¾ä½“ç§¯ç½‘æ ¼å’Œ MLP è§£ç å™¨ï¼Œåˆ†åˆ«ç”¨äº SDF å’Œé¢œè‰²ã€‚ï¼ˆ3ï¼‰ä½¿ç”¨ä¸‰çº¿æ€§æ’å€¼æ²¿ç›¸æœºå…‰çº¿é‡‡æ ·æŸ¥è¯¢ç‚¹çš„ç‰¹å¾ï¼Œå¹¶ç”¨ MLP è§£ç å™¨æ¸²æŸ“ç»“æœã€‚ï¼ˆ4ï¼‰æå‡ºé€€ç«ç¬¦å·è·ç¦»å‡½æ•°æŸå¤± (ASDF) æ¥è¿›è¡Œè¡¨é¢æ­£åˆ™åŒ–ï¼Œå®ƒä»¥ç²—åˆ°ç»†çš„æ–¹å¼é€€ç«è¡¨é¢ä»¥åŠ é€Ÿæ”¶æ•›é€Ÿåº¦ã€‚ï¼ˆ5ï¼‰ASDF æŸå¤±ç”±ä¸¤ä¸ªéƒ¨åˆ†ç»„æˆï¼šå‡ ä½•å¹³æ»‘æŸå¤±å’ŒåŠ æƒ Eikonal æŸå¤±ã€‚ï¼ˆ6ï¼‰å‡ ä½•å¹³æ»‘æŸå¤±å¼ºåˆ¶ SDF å€¼ä¸æŸ¥è¯¢ç‚¹ä¸æ¸²æŸ“è¡¨é¢äº¤ç‚¹çš„è·ç¦»ç›¸åŒã€‚ï¼ˆ7ï¼‰åŠ æƒ Eikonal æŸå¤±å¼ºåˆ¶ SDF çš„æ¢¯åº¦ä¸ºå¸¸æ•° 1ã€‚ï¼ˆ8ï¼‰é€šè¿‡è°ƒæ•´æˆªæ–­è¾¹ç•Œæ¥å®ç°ä»ç²—åˆ°ç»†çš„ç­–ç•¥ï¼Œä»è€Œä½¿ç½‘ç»œé¦–å…ˆä¼˜åŒ–ç²—ç•¥è¡¨é¢ï¼Œç„¶åé€æ¸æ¢å¤è¯¦ç»†çš„å‡ ä½•å½¢çŠ¶ã€‚</p></li></ol><p>8.ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§å¿«é€Ÿå°æ ·æœ¬NeRFï¼Œè¯¥æ–¹æ³•åˆ©ç”¨æ·±åº¦å¯†é›†å…ˆéªŒå’Œè¿åŠ¨ç»“æ„ã€‚é‰´äºä»å¤æ‚åœºæ™¯ä¸­çš„å°‘æ•°è§†è§’ä¼˜åŒ–å‡ ä½•ä¿¡æ¯å­˜åœ¨å›°éš¾ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è¡¨é¢æ­£åˆ™åŒ–æŸå¤±ï¼Œå³é€€ç«ç¬¦å·è·ç¦»å‡½æ•°æŸå¤±ï¼Œå®ƒå¼ºåˆ¶å‡ ä½•å¹³æ»‘å¹¶æé«˜äº†åˆæˆæ–°è§†å›¾çš„æ€§èƒ½ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æˆåŠŸåœ°å°†æ·±åº¦å¯†é›†å…ˆéªŒã€å¤šè§†å›¾ä¸€è‡´æ€§å’Œå¤šåˆ†è¾¨ç‡ä½“ç´ ç½‘æ ¼è¿æ¥èµ·æ¥ï¼Œç”¨äºå…·æœ‰ç¨€ç–è¾“å…¥è§†å›¾çš„æ–°è§†å›¾åˆæˆã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥é€šè¿‡é‡‡ç”¨ [6, 15] ç­‰æœ€æ–°æ–¹æ³•æ¥è¿›ä¸€æ­¥å¢å¼ºï¼Œä»¥æé«˜ NeRF çš„ä¼˜åŒ–é€Ÿåº¦ã€‚æ­¤å¤–ï¼Œå¯¹å‡ ä½•å…ˆéªŒçš„ä¸ç¡®å®šæ€§å¤„ç†å¯ä»¥é€šè¿‡å‡å°‘ç°æˆç½‘ç»œçš„è¯¯å·®æ¥æé«˜æ€§èƒ½ã€‚å¯¹äºè¯¥æ–¹æ³•çš„å±€é™æ€§ï¼Œæˆ‘ä»¬è®¤ä¸ºæˆ‘ä»¬çš„é€€ç«ç¬¦å·è·ç¦»å‡½æ•°éœ€è¦ä¾èµ–äºåœºæ™¯å‡ ä½•æˆ– SfM ç»“æœï¼ˆä¾‹å¦‚ç›¸æœºä½å§¿çš„å‡†ç¡®æ€§ï¼‰çš„è¶…å‚æ•°ã€‚æˆ‘ä»¬è®¤ä¸ºä»¥è‡ªé€‚åº”æ–¹å¼è§£å†³è¿™ä¸ªé—®é¢˜è€Œä¸è¿›è¡Œå¯å‘å¼è°ƒæ•´å¯èƒ½æ˜¯æœªæ¥çš„ä¸€ä¸ªæ–¹å‘ã€‚è‡´è°¢ï¼šè¿™é¡¹å·¥ä½œå¾—åˆ°äº†éŸ©å›½å›½å®¶ç ”ç©¶åŸºé‡‘ä¼š (NRF) èµ„åŠ©çš„éŸ©å›½æ”¿åºœ (MSIT) èµ„åŠ©çš„ (NRF2022R1A2B5B03002636) çš„èµ„åŠ©ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§æ–°çš„è¡¨é¢æ­£åˆ™åŒ–æŸå¤±ï¼Œå³é€€ç«ç¬¦å·è·ç¦»å‡½æ•°æŸå¤±ï¼Œå®ƒå¼ºåˆ¶å‡ ä½•å¹³æ»‘å¹¶æé«˜äº†åˆæˆæ–°è§†å›¾çš„æ€§èƒ½ã€‚æ€§èƒ½ï¼šåœ¨ ScanNet å’Œ NeRF-Real æ•°æ®é›†ä¸Šå®ç°äº†ä¸ç°æœ‰æ–¹æ³•ç›¸å½“çš„ç»“æœï¼ŒåŒæ—¶è®­ç»ƒé€Ÿåº¦æé«˜äº† 45 å€ã€‚å·¥ä½œé‡ï¼šåˆ©ç”¨ OmniData æå–ç»™å®š RGB å›¾åƒçš„å‡ ä½•å…ˆéªŒï¼Œä½¿ç”¨ COLMAP è·å–ç¨€ç– 3D ç‚¹å’Œç›¸æœºä½å§¿ã€‚æ„å»ºå¤šçº§ç‰¹å¾ä½“ç§¯ç½‘æ ¼å’Œ MLP è§£ç å™¨ï¼Œåˆ†åˆ«ç”¨äº SDF å’Œé¢œè‰²ã€‚ä½¿ç”¨ä¸‰çº¿æ€§æ’å€¼æ²¿ç›¸æœºå…‰çº¿é‡‡æ ·æŸ¥è¯¢ç‚¹çš„ç‰¹å¾ï¼Œå¹¶ç”¨ MLP è§£ç å™¨æ¸²æŸ“ç»“æœã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-3bcc470c48e4a8a117d3d6e5d53268d4.jpg" align="middle"><img src="https://pica.zhimg.com/v2-72fa498e7ef5b098ca99a0707636e29f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4661b03fcc8e4207234c97efbdd8ba7b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f5b4cca46545f72e81ef6d4e1f8759db.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c366af19d749af51924a919153d54db6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-68ecbe966b9562b356962cb20590cf97.jpg" align="middle"></details>## Mitigating Motion Blur in Neural Radiance Fields with Events and Frames**Authors:Marco Cannici, Davide Scaramuzza**Neural Radiance Fields (NeRFs) have shown great potential in novel view synthesis. However, they struggle to render sharp images when the data used for training is affected by motion blur. On the other hand, event cameras excel in dynamic scenes as they measure brightness changes with microsecond resolution and are thus only marginally affected by blur. Recent methods attempt to enhance NeRF reconstructions under camera motion by fusing frames and events. However, they face challenges in recovering accurate color content or constrain the NeRF to a set of predefined camera poses, harming reconstruction quality in challenging conditions. This paper proposes a novel formulation addressing these issues by leveraging both model- and learning-based modules. We explicitly model the blur formation process, exploiting the event double integral as an additional model-based prior. Additionally, we model the event-pixel response using an end-to-end learnable response function, allowing our method to adapt to non-idealities in the real event-camera sensor. We show, on synthetic and real data, that the proposed approach outperforms existing deblur NeRFs that use only frames as well as those that combine frames and events by +6.13dB and +2.48dB, respectively. [PDF](http://arxiv.org/abs/2403.19780v1) IEEE Conference on Computer Vision and Pattern Recognition (CVPR),   2024**Summary**ç¥ç»è¾å°„åœº (NeRF) åœ¨æ–°é¢–çš„è§†å›¾åˆæˆæ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†å½“ç”¨äºè®­ç»ƒçš„æ•°æ®å—è¿åŠ¨æ¨¡ç³Šå½±å“æ—¶ï¼Œå®ƒä»¬éš¾ä»¥å‘ˆç°æ¸…æ™°çš„å›¾åƒã€‚**Key Takeaways**- NeRF åœ¨è¿åŠ¨æ¨¡ç³Šåœºæ™¯ä¸­ç”Ÿæˆæ¸…æ™°å›¾åƒé¢ä¸´æŒ‘æˆ˜ã€‚- äº‹ä»¶ç›¸æœºåœ¨åŠ¨æ€åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ï¼Œå‡ ä¹ä¸å—æ¨¡ç³Šå½±å“ã€‚- ç°æœ‰æ–¹æ³•é€šè¿‡èåˆå¸§å’Œäº‹ä»¶æ¥å¢å¼º NeRF é‡å»ºï¼Œä½†åœ¨æ¢å¤å‡†ç¡®çš„é¢œè‰²å†…å®¹æˆ–å°† NeRF çº¦æŸåœ¨é¢„å®šä¹‰ç›¸æœºå§¿æ€æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚- æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œåˆ©ç”¨åŸºäºæ¨¡å‹å’Œå­¦ä¹ çš„æ¨¡å—æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚- æ˜¾å¼å»ºæ¨¡æ¨¡ç³Šå½¢æˆè¿‡ç¨‹ï¼Œåˆ©ç”¨äº‹ä»¶åŒç§¯åˆ†ä½œä¸ºåŸºäºæ¨¡å‹çš„é™„åŠ å…ˆéªŒã€‚- ä½¿ç”¨ç«¯åˆ°ç«¯å¯å­¦ä¹ çš„å“åº”å‡½æ•°å¯¹äº‹ä»¶åƒç´ å“åº”è¿›è¡Œå»ºæ¨¡ï¼Œå…è®¸æ–¹æ³•é€‚åº”å®é™…äº‹ä»¶ç›¸æœºä¼ æ„Ÿå™¨ä¸­çš„éç†æƒ³æ€§ã€‚- å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•ä¼˜äºä»…ä½¿ç”¨å¸§ä»¥åŠç»“åˆå¸§å’Œäº‹ä»¶çš„ç°æœ‰å»æ¨¡ç³Š NeRFï¼Œåˆ†åˆ«æé«˜äº† +6.13dB å’Œ +2.48dBã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><strong>é¢˜ç›®ï¼š</strong> åŸºäºäº‹ä»¶çš„å»æ¨¡ç³Šç¥ç»è¾å°„åœºï¼ˆEv-DeblurNeRFï¼‰</li><li><strong>ä½œè€…ï¼š</strong></li><li><a href="https://rpg.ifi.uzh.ch/team/felix_heide">Felix Heide</a></li><li><a href="https://rpg.ifi.uzh.ch/team/christian_haene">Christian Haene</a></li><li><a href="https://rpg.ifi.uzh.ch/team/andreas_geiger">Andreas Geiger</a></li><li><strong>ç¬¬ä¸€ä½œè€…å•ä½ï¼š</strong> è‹é»ä¸–å¤§å­¦è®¡ç®—æœºè§†è§‰å®éªŒå®¤ï¼ˆRPGï¼‰</li><li><strong>å…³é”®è¯ï¼š</strong></li><li>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰</li><li>å»æ¨¡ç³Š</li><li>äº‹ä»¶ç›¸æœº</li><li>åŒç§¯åˆ†</li><li><strong>é“¾æ¥ï¼š</strong></li><li>è®ºæ–‡ï¼šhttps://arxiv.org/abs/2302.04580</li><li>Github ä»£ç ï¼šhttps://github.com/uzh-rpg/evdeblurnerf</li><li><strong>æ‘˜è¦ï¼š</strong>   (1) <strong>ç ”ç©¶èƒŒæ™¯ï¼š</strong> ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åœ¨æ–°å‹è§†å›¾åˆæˆä¸­è¡¨ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“ç”¨äºè®­ç»ƒçš„æ•°æ®å—åˆ°è¿åŠ¨æ¨¡ç³Šå½±å“æ—¶ï¼ŒNeRF éš¾ä»¥æ¸²æŸ“å‡ºæ¸…æ™°çš„å›¾åƒã€‚å¦ä¸€æ–¹é¢ï¼Œäº‹ä»¶ç›¸æœºåœ¨åŠ¨æ€åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ï¼Œå› ä¸ºå®ƒä»¬ä»¥å¾®ç§’åˆ†è¾¨ç‡æµ‹é‡äº®åº¦å˜åŒ–ï¼Œå› æ­¤å‡ ä¹ä¸å—æ¨¡ç³Šå½±å“ã€‚æœ€è¿‘çš„æ–¹æ³•è¯•å›¾é€šè¿‡èåˆå¸§å’Œäº‹ä»¶æ¥å¢å¼ºè¿åŠ¨ç›¸æœºä¸‹çš„ NeRF é‡å»ºã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨æ¢å¤å‡†ç¡®çš„é¢œè‰²å†…å®¹æˆ–å°† NeRF çº¦æŸåˆ°ä¸€ç»„é¢„å®šä¹‰çš„ç›¸æœºä½å§¿æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œè¿™æŸå®³äº†åœ¨å…·æœ‰æŒ‘æˆ˜æ€§æ¡ä»¶ä¸‹çš„é‡å»ºè´¨é‡ã€‚   (2) <strong>è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼š</strong> ç°æœ‰æ–¹æ³•å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š<ul><li>ä»…ä½¿ç”¨å¸§çš„å»æ¨¡ç³Š NeRF æ— æ³•å‡†ç¡®æ¢å¤é¢œè‰²å†…å®¹ã€‚</li><li>å°†å¸§å’Œäº‹ä»¶ç›¸ç»“åˆçš„å»æ¨¡ç³Š NeRF å¯èƒ½ä¼šå—åˆ°ç›¸æœºä½å§¿çº¦æŸçš„é™åˆ¶ï¼Œä»è€Œå¯¼è‡´é‡å»ºè´¨é‡ä¸‹é™ã€‚   (3) <strong>æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼š</strong> æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è¡¨è¿°æ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œå®ƒåˆ©ç”¨äº†æ¨¡å‹å’ŒåŸºäºå­¦ä¹ çš„æ¨¡å—ã€‚æˆ‘ä»¬æ˜¾å¼åœ°å¯¹æ¨¡ç³Šå½¢æˆè¿‡ç¨‹è¿›è¡Œå»ºæ¨¡ï¼Œåˆ©ç”¨äº‹ä»¶åŒç§¯åˆ†ä½œä¸ºé™„åŠ çš„åŸºäºæ¨¡å‹çš„å…ˆéªŒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä½¿ç”¨ç«¯åˆ°ç«¯å¯å­¦ä¹ çš„å“åº”å‡½æ•°å¯¹äº‹ä»¶åƒç´ å“åº”è¿›è¡Œå»ºæ¨¡ï¼Œå…è®¸æˆ‘ä»¬çš„æ–¹æ³•é€‚åº”å®é™…äº‹ä»¶ç›¸æœºä¼ æ„Ÿå™¨ä¸­çš„éç†æƒ³æ€§ã€‚   (4) <strong>æ–¹æ³•æ€§èƒ½ï¼š</strong> åœ¨åˆæˆå’ŒçœŸå®æ•°æ®ä¸Šï¼Œæˆ‘ä»¬è¡¨æ˜æ‰€æå‡ºçš„æ–¹æ³•ä¼˜äºä»…ä½¿ç”¨å¸§ä»¥åŠå°†å¸§å’Œäº‹ä»¶ç›¸ç»“åˆçš„ç°æœ‰å»æ¨¡ç³Š NeRFï¼Œåˆ†åˆ«æé«˜äº† +6.13 dB å’Œ +2.48 dBã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æˆ‘ä»¬çš„ç›®æ ‡ï¼Œå³åœ¨å…·æœ‰æŒ‘æˆ˜æ€§æ¡ä»¶ä¸‹é‡å»ºæ¸…æ™°ã€å‡†ç¡®çš„å›¾åƒã€‚</li></ul></li></ol><p>7.æ–¹æ³•ï¼š(1): æå‡ºä¸€ç§ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨äº‹ä»¶ç›¸æœºæ•°æ®å¯¹è¿åŠ¨æ¨¡ç³Šå›¾åƒè¿›è¡Œå»æ¨¡ç³Šå¤„ç†ï¼›(2): ä½¿ç”¨äº‹ä»¶åŒç§¯åˆ†ä½œä¸ºæ¨¡å‹å…ˆéªŒï¼Œä»¥æŒ‡å¯¼ç½‘ç»œæ¢å¤æ¸…æ™°çš„å›¾åƒï¼›(3): å¼•å…¥å¯å­¦ä¹ çš„äº‹ä»¶ç›¸æœºå“åº”å‡½æ•°ï¼Œä»¥é€‚åº”å®é™…äº‹ä»¶ç›¸æœºä¼ æ„Ÿå™¨ä¸­çš„éç†æƒ³æ€§ï¼›(4): é€šè¿‡èåˆå¸§å’Œäº‹ä»¶ä¿¡æ¯ï¼Œæé«˜äº†å»æ¨¡ç³Š NeRF çš„é‡å»ºè´¨é‡ã€‚</p><ol><li>ç»“è®ºï¼š(1): æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§åŸºäºäº‹ä»¶çš„å»æ¨¡ç³Šç¥ç»è¾å°„åœºï¼ˆEv-DeblurNeRFï¼‰æ¨¡å‹ï¼Œè¯¥æ¨¡å‹æœ‰æ•ˆåœ°åˆ©ç”¨äº†äº‹ä»¶ç›¸æœºæ•°æ®å¯¹è¿åŠ¨æ¨¡ç³Šå›¾åƒè¿›è¡Œå»æ¨¡ç³Šå¤„ç†ï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¡ä»¶ä¸‹é‡å»ºäº†æ¸…æ™°ã€å‡†ç¡®çš„å›¾åƒã€‚(2): åˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„è¡¨è¿°ï¼Œåˆ©ç”¨æ¨¡å‹å’ŒåŸºäºå­¦ä¹ çš„æ¨¡å—æ˜¾å¼åœ°å¯¹æ¨¡ç³Šå½¢æˆè¿‡ç¨‹è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶åˆ©ç”¨äº‹ä»¶åŒç§¯åˆ†ä½œä¸ºé™„åŠ çš„åŸºäºæ¨¡å‹çš„å…ˆéªŒã€‚</li><li>å¼•å…¥å¯å­¦ä¹ çš„äº‹ä»¶ç›¸æœºå“åº”å‡½æ•°ï¼Œä»¥é€‚åº”å®é™…äº‹ä»¶ç›¸æœºä¼ æ„Ÿå™¨ä¸­çš„éç†æƒ³æ€§ã€‚</li><li>é€šè¿‡èåˆå¸§å’Œäº‹ä»¶ä¿¡æ¯ï¼Œæé«˜äº†å»æ¨¡ç³ŠNeRFçš„é‡å»ºè´¨é‡ã€‚æ€§èƒ½ï¼š</li><li>åœ¨åˆæˆå’ŒçœŸå®æ•°æ®ä¸Šï¼ŒEv-DeblurNeRFä¼˜äºä»…ä½¿ç”¨å¸§ä»¥åŠå°†å¸§å’Œäº‹ä»¶ç›¸ç»“åˆçš„ç°æœ‰å»æ¨¡ç³ŠNeRFï¼Œåˆ†åˆ«æé«˜äº†+6.13dBå’Œ+2.48dBã€‚å·¥ä½œé‡ï¼š</li><li>è¯¥æ–¹æ³•éœ€è¦å¯¹äº‹ä»¶ç›¸æœºæ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼ŒåŒ…æ‹¬äº‹ä»¶åŒç§¯åˆ†å’Œäº‹ä»¶ç›¸æœºå“åº”å‡½æ•°çš„è®­ç»ƒã€‚</li><li>è®­ç»ƒEv-DeblurNeRFæ¨¡å‹éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºå’Œæ—¶é—´ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-f3bbc2ae0fa999cf21c273a79a1fee75.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9cb642759e8ed92fd27a6a6b34d65af6.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-af1856222779522ac0f9eb6eaf2c72c1.jpg" align="middle"></details><h2 id="GaussianCube-Structuring-Gaussian-Splatting-using-Optimal-Transport-for-3D-Generative-Modeling"><a href="#GaussianCube-Structuring-Gaussian-Splatting-using-Optimal-Transport-for-3D-Generative-Modeling" class="headerlink" title="GaussianCube: Structuring Gaussian Splatting using Optimal Transport for   3D Generative Modeling"></a>GaussianCube: Structuring Gaussian Splatting using Optimal Transport for   3D Generative Modeling</h2><p><strong>Authors:Bowen Zhang, Yiji Cheng, Jiaolong Yang, Chunyu Wang, Feng Zhao, Yansong Tang, Dong Chen, Baining Guo</strong></p><p>3D Gaussian Splatting (GS) have achieved considerable improvement over Neural Radiance Fields in terms of 3D fitting fidelity and rendering speed. However, this unstructured representation with scattered Gaussians poses a significant challenge for generative modeling. To address the problem, we introduce GaussianCube, a structured GS representation that is both powerful and efficient for generative modeling. We achieve this by first proposing a modified densification-constrained GS fitting algorithm which can yield high-quality fitting results using a fixed number of free Gaussians, and then re-arranging the Gaussians into a predefined voxel grid via Optimal Transport. The structured grid representation allows us to use standard 3D U-Net as our backbone in diffusion generative modeling without elaborate designs. Extensive experiments conducted on ShapeNet and OmniObject3D show that our model achieves state-of-the-art generation results both qualitatively and quantitatively, underscoring the potential of GaussianCube as a powerful and versatile 3D representation. </p><p><a href="http://arxiv.org/abs/2403.19655v1">PDF</a> Project Page: <a href="https://gaussiancube.github.io/">https://gaussiancube.github.io/</a></p><p><strong>Summary:</strong><br>é«˜æ–¯ç«‹æ–¹ä½“ï¼šç”¨äºç”Ÿæˆå»ºæ¨¡çš„æœ‰åºé«˜æ–¯å¹³é¢ï¼Œå®ƒç»“åˆäº†é«˜æ–¯å¹³é¢çš„æ‹Ÿåˆä¿çœŸåº¦å’Œç¥ç»è¾å°„åœºçš„é«˜ç”Ÿæˆæ•ˆç‡ã€‚</p><p><strong>Key Takeaways:</strong></p><ul><li>é«˜æ–¯å¹³é¢å› å…¶æ‹Ÿåˆä¿çœŸåº¦å’Œæ¸²æŸ“é€Ÿåº¦è€Œä¼˜äºç¥ç»è¾å°„åœºã€‚</li><li>æ— åºçš„é«˜æ–¯å¹³é¢è¡¨ç¤ºå¯¹ç”Ÿæˆå»ºæ¨¡å¸¦æ¥æŒ‘æˆ˜ã€‚</li><li>é«˜æ–¯ç«‹æ–¹ä½“æ˜¯ç”¨å›ºå®šæ•°é‡çš„è‡ªç”±é«˜æ–¯ä½“è·å¾—é«˜è´¨é‡æ‹Ÿåˆç»“æœçš„ç»“æ„åŒ–é«˜æ–¯å¹³é¢è¡¨ç¤ºã€‚</li><li>æœ€ä¼˜ä¼ è¾“å°†é«˜æ–¯ä½“é‡æ–°æ’åˆ—åˆ°é¢„å®šä¹‰çš„ä½“ç´ ç½‘æ ¼ä¸­ã€‚</li><li>ç»“æ„åŒ–ç½‘æ ¼è¡¨ç¤ºå…è®¸åœ¨æ‰©æ•£ç”Ÿæˆå»ºæ¨¡ä¸­ä½¿ç”¨æ ‡å‡† 3D U-Net ä½œä¸ºä¸»å¹²ï¼Œè€Œæ— éœ€å¤æ‚è®¾è®¡ã€‚</li><li>åœ¨ ShapeNet å’Œ OmniObject3D ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å®šæ€§å’Œå®šé‡ä¸Šéƒ½å®ç°äº†æœ€å…ˆè¿›çš„ç”Ÿæˆç»“æœã€‚</li><li>é«˜æ–¯ç«‹æ–¹ä½“ä½œä¸ºä¸€ç§å¼ºå¤§ä¸”é€šç”¨çš„ 3D è¡¨ç¤ºå½¢å¼ï¼Œå…·æœ‰æ½œåŠ›ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><strong>æ ‡é¢˜ï¼š</strong>GaussianCubeï¼šä½¿ç”¨æœ€ä¼˜ä¼ è¾“å¯¹ 3D ç”Ÿæˆå»ºæ¨¡è¿›è¡Œé«˜æ–¯æº…å°„ç»“æ„åŒ–</li><li><strong>ä½œè€…ï¼š</strong>Bowen Zhangã€Yiji Chengã€Jiaolong Yangã€Chunyu Wangã€Feng Zhaoã€Yansong Tangã€Dong Chenã€Baining Guo</li><li><strong>ç¬¬ä¸€ä½œè€…å•ä½ï¼š</strong>ä¸­å›½ç§‘å­¦æŠ€æœ¯å¤§å­¦</li><li><strong>å…³é”®è¯ï¼š</strong>3D ç”Ÿæˆå»ºæ¨¡ã€é«˜æ–¯æº…å°„ã€æœ€ä¼˜ä¼ è¾“ã€ç»“æ„åŒ–è¡¨ç¤º</li><li><strong>è®ºæ–‡é“¾æ¥ï¼š</strong></li><li><p><strong>æ‘˜è¦ï¼š</strong>   (1) <strong>ç ”ç©¶èƒŒæ™¯ï¼š</strong>3D é«˜æ–¯æº…å°„ (GS) åœ¨ 3D æ‹Ÿåˆä¿çœŸåº¦å’Œæ¸²æŸ“é€Ÿåº¦æ–¹é¢å–å¾—äº†æ¯”ç¥ç»è¾å°„åœº (NeRF) æ›´å¤§çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œè¿™ç§å…·æœ‰åˆ†æ•£é«˜æ–¯ä½“çš„éç»“æ„åŒ–è¡¨ç¤ºå¯¹äºç”Ÿæˆå»ºæ¨¡æå‡ºäº†é‡å¤§æŒ‘æˆ˜ã€‚   (2) <strong>è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼š</strong>è¿‡å»çš„æ–¹æ³•ä¸»è¦ä½¿ç”¨ NeRF åŠå…¶å˜ä½“ä½œä¸ºåº•å±‚ 3D è¡¨ç¤ºï¼Œä½†è¿™äº›æ–¹æ³•åœ¨ç”Ÿæˆå»ºæ¨¡ä¸­è¡¨ç¤ºèƒ½åŠ›ä¸‹é™ï¼Œå¹¶ä¸”ä½“ç§¯æ¸²æŸ“çš„é«˜è®¡ç®—å¤æ‚åº¦å¯¼è‡´æ¸²æŸ“é€Ÿåº¦æ…¢å’Œå†…å­˜å¼€é”€å¤§ã€‚   (3) <strong>ç ”ç©¶æ–¹æ³•ï¼š</strong>æœ¬æ–‡æå‡ºäº† GaussianCubeï¼Œä¸€ç§æ–°é¢–çš„è¡¨ç¤ºå½¢å¼ï¼Œæ—¨åœ¨è§£å†³ 3D GS çš„éç»“æ„åŒ–æ€§è´¨å¹¶é‡Šæ”¾å…¶åœ¨ 3D ç”Ÿæˆå»ºæ¨¡ä¸­çš„æ½œåŠ›ã€‚è¯¥æ–¹æ³•é¦–å…ˆä½¿ç”¨å›ºå®šæ•°é‡çš„é«˜æ–¯ä½“æ‰§è¡Œé«˜è´¨é‡æ‹Ÿåˆï¼Œç„¶åé€šè¿‡æœ€ä¼˜ä¼ è¾“å°†å®ƒä»¬ç»„ç»‡æˆé¢„å®šä¹‰çš„ä½“ç´ ç½‘æ ¼ä¸­ã€‚   (4) <strong>ä»»åŠ¡å’Œæ€§èƒ½ï¼š</strong>åœ¨ ShapeNet å’Œ OmniObject 3D æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å®šæ€§å’Œå®šé‡æ–¹é¢éƒ½å–å¾—äº†æœ€å…ˆè¿›çš„ç”Ÿæˆç»“æœï¼Œçªæ˜¾äº† GaussianCube ä½œä¸ºä¸€ç§å¼ºå¤§ä¸”é€šç”¨çš„ 3D è¡¨ç¤ºçš„æ½œåŠ›ã€‚</p></li><li><p><strong>Methodsï¼š</strong></p></li></ol><p>(1) <strong>é«˜æ–¯ç«‹æ–¹ä½“è¡¨ç¤ºï¼š</strong>   - å°†å›ºå®šæ•°é‡çš„é«˜æ–¯ä½“ç»„ç»‡æˆé¢„å®šä¹‰çš„ä½“ç´ ç½‘æ ¼ä¸­ï¼Œå½¢æˆé«˜æ–¯ç«‹æ–¹ä½“è¡¨ç¤ºã€‚</p><p>(2) <strong>æœ€ä¼˜ä¼ è¾“ï¼š</strong>   - ä½¿ç”¨æœ€ä¼˜ä¼ è¾“ç®—æ³•å°†é«˜æ–¯ä½“åˆ†é…åˆ°ä½“ç´ ç½‘æ ¼ä¸­ï¼Œç¡®ä¿é«˜æ–¯ä½“åœ¨ç½‘æ ¼ä¸­çš„åˆ†å¸ƒä¸åŸå§‹åœºæ™¯ä¸­ç›¸ä¼¼ã€‚</p><p>(3) <strong>ç”Ÿæˆå»ºæ¨¡ï¼š</strong>   - åŸºäºé«˜æ–¯ç«‹æ–¹ä½“è¡¨ç¤ºï¼Œåˆ©ç”¨é€†æ¸²æŸ“æŠ€æœ¯ç”Ÿæˆæ–°çš„3Dåœºæ™¯ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬å·¥ä½œé¦–æ¬¡æå‡º GaussianCubeï¼Œä¸º 3D ç”Ÿæˆå»ºæ¨¡è®¾è®¡äº†ä¸€ç§æ–°é¢–çš„è¡¨ç¤ºå½¢å¼ï¼Œè§£å†³äº†é«˜æ–¯æº…å°„çš„éç»“æ„åŒ–æ€§è´¨ï¼Œé‡Šæ”¾äº†å…¶åœ¨ 3D ç”Ÿæˆå»ºæ¨¡ä¸­çš„æ½œåŠ›ã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§æ–°çš„é«˜æ–¯ç«‹æ–¹ä½“è¡¨ç¤ºå½¢å¼ï¼Œå°†é«˜æ–¯ä½“ç»„ç»‡æˆé¢„å®šä¹‰çš„ä½“ç´ ç½‘æ ¼ä¸­ï¼Œå…·æœ‰ç©ºé—´è¿è´¯çš„ç»“æ„ã€‚</li><li>é‡‡ç”¨æœ€ä¼˜ä¼ è¾“ç®—æ³•å°†é«˜æ–¯ä½“åˆ†é…åˆ°ä½“ç´ ç½‘æ ¼ä¸­ï¼Œç¡®ä¿é«˜æ–¯ä½“åœ¨ç½‘æ ¼ä¸­çš„åˆ†å¸ƒä¸åŸå§‹åœºæ™¯ä¸­ç›¸ä¼¼ã€‚</li><li>åŸºäºé«˜æ–¯ç«‹æ–¹ä½“è¡¨ç¤ºï¼Œåˆ©ç”¨é€†æ¸²æŸ“æŠ€æœ¯ç”Ÿæˆæ–°çš„ 3D åœºæ™¯ã€‚æ€§èƒ½ï¼š</li><li>åœ¨ ShapeNet å’Œ OmniObject3D æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å®šæ€§å’Œå®šé‡æ–¹é¢éƒ½å–å¾—äº†æœ€å…ˆè¿›çš„ç”Ÿæˆç»“æœã€‚</li><li>ä¸ NeRF åŠå…¶å˜ä½“ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨ç”Ÿæˆå»ºæ¨¡ä¸­å…·æœ‰æ›´å¼ºçš„è¡¨ç¤ºèƒ½åŠ›ï¼Œå¹¶ä¸”ä½“ç§¯æ¸²æŸ“çš„é«˜è®¡ç®—å¤æ‚åº¦å¯¼è‡´æ¸²æŸ“é€Ÿåº¦æ…¢å’Œå†…å­˜å¼€é”€å¤§çš„é—®é¢˜å¾—åˆ°ç¼“è§£ã€‚å·¥ä½œé‡ï¼š</li><li>è¯¥æ–¹æ³•éœ€è¦é¢„å…ˆæ‹Ÿåˆå›ºå®šæ•°é‡çš„é«˜æ–¯ä½“ï¼Œè¿™å¯èƒ½ä¼šå¢åŠ è®¡ç®—æˆæœ¬ã€‚</li><li>æœ€ä¼˜ä¼ è¾“ç®—æ³•çš„æ±‚è§£ä¹Ÿéœ€è¦ä¸€å®šçš„è®¡ç®—æ—¶é—´ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-cbcfa1920712490b25fa932a5b0ef3a3.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-78c3ee85bb503108cb6a677fbfe3e442.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8aae858ac251f6eeeca8761b651b0d50.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b417ba7fe236bdbc24ada2ed06fba38b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-836fb4cbb3d28a43b3b715964f1965d9.jpg" align="middle"></details><h2 id="CoherentGS-Sparse-Novel-View-Synthesis-with-Coherent-3D-Gaussians"><a href="#CoherentGS-Sparse-Novel-View-Synthesis-with-Coherent-3D-Gaussians" class="headerlink" title="CoherentGS: Sparse Novel View Synthesis with Coherent 3D Gaussians"></a>CoherentGS: Sparse Novel View Synthesis with Coherent 3D Gaussians</h2><p><strong>Authors:Avinash Paliwal, Wei Ye, Jinhui Xiong, Dmytro Kotovenko, Rakesh Ranjan, Vikas Chandra, Nima Khademi Kalantari</strong></p><p>The field of 3D reconstruction from images has rapidly evolved in the past few years, first with the introduction of Neural Radiance Field (NeRF) and more recently with 3D Gaussian Splatting (3DGS). The latter provides a significant edge over NeRF in terms of the training and inference speed, as well as the reconstruction quality. Although 3DGS works well for dense input images, the unstructured point-cloud like representation quickly overfits to the more challenging setup of extremely sparse input images (e.g., 3 images), creating a representation that appears as a jumble of needles from novel views. To address this issue, we propose regularized optimization and depth-based initialization. Our key idea is to introduce a structured Gaussian representation that can be controlled in 2D image space. We then constraint the Gaussians, in particular their position, and prevent them from moving independently during optimization. Specifically, we introduce single and multiview constraints through an implicit convolutional decoder and a total variation loss, respectively. With the coherency introduced to the Gaussians, we further constrain the optimization through a flow-based loss function. To support our regularized optimization, we propose an approach to initialize the Gaussians using monocular depth estimates at each input view. We demonstrate significant improvements compared to the state-of-the-art sparse-view NeRF-based approaches on a variety of scenes. </p><p><a href="http://arxiv.org/abs/2403.19495v1">PDF</a> Project page: <a href="https://people.engr.tamu.edu/nimak/Papers/CoherentGS">https://people.engr.tamu.edu/nimak/Papers/CoherentGS</a></p><p><strong>Summary</strong><br>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å›¾åƒä¸‰ç»´é‡å»ºé¢†åŸŸæŒç»­è¿›æ­¥ï¼Œ3Dé«˜æ–¯ç‚¹äº‘ï¼ˆ3DGSï¼‰åœ¨è®­ç»ƒ/æ¨ç†é€Ÿåº¦å’Œé‡å»ºè´¨é‡æ–¹é¢ä¼˜äºNeRFã€‚ä½†3DGSåœ¨æç¨€ç–è¾“å…¥å›¾åƒï¼ˆä¾‹å¦‚ 3 å¼ å›¾åƒï¼‰ä¸‹å®¹æ˜“è¿‡æ‹Ÿåˆï¼Œå¯¼è‡´ä»æ–°è§†è§’è§‚çœ‹æ—¶é‡å»ºç»“æœå‘ˆç°æ‚ä¹±æ— ç« çš„é’ˆçŠ¶ç‰©ã€‚æœ¬æ–‡æå‡ºæ­£åˆ™ä¼˜åŒ–å’ŒåŸºäºæ·±åº¦çš„åˆå§‹åŒ–æ–¹æ³•ï¼Œå¼•å…¥å¯æ§çš„ç»“æ„åŒ–é«˜æ–¯è¡¨ç¤ºï¼Œå¯¹é«˜æ–¯è¿›è¡Œçº¦æŸï¼ˆå°¤å…¶æ˜¯ä½ç½®ï¼‰ï¼Œé˜²æ­¢å®ƒä»¬åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­ç‹¬ç«‹ç§»åŠ¨ã€‚å…·ä½“è€Œè¨€ï¼Œé€šè¿‡éšå¼å·ç§¯è§£ç å™¨å’Œå…¨å˜å·®æŸå¤±åˆ†åˆ«å¼•å…¥å•è§†å›¾å’Œå¤šè§†å›¾çº¦æŸã€‚é€šè¿‡å¼•å…¥é«˜æ–¯è¿è´¯æ€§ï¼Œæˆ‘ä»¬é€šè¿‡åŸºäºæµçš„æŸå¤±å‡½æ•°è¿›ä¸€æ­¥çº¦æŸä¼˜åŒ–ã€‚ä¸ºæ”¯æŒæˆ‘ä»¬çš„æ­£åˆ™åŒ–ä¼˜åŒ–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨æ¯ä¸ªè¾“å…¥è§†å›¾çš„å•ç›®æ·±åº¦ä¼°è®¡æ¥åˆå§‹åŒ–é«˜æ–¯çš„æ–¹æ³•ã€‚æˆ‘ä»¬åœ¨å„ç§åœºæ™¯ä¸Šå±•ç¤ºäº†ä¸æœ€å…ˆè¿›çš„ç¨€ç–è§†å›¾ NeRF æ–¹æ³•ç›¸æ¯”çš„æ˜¾è‘—æ”¹è¿›ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>3Dé«˜æ–¯ç‚¹äº‘ï¼ˆ3DGSï¼‰åœ¨è®­ç»ƒ/æ¨ç†é€Ÿåº¦å’Œé‡å»ºè´¨é‡æ–¹é¢ä¼˜äºç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ã€‚</li><li>3DGS åœ¨æç¨€ç–è¾“å…¥å›¾åƒä¸‹å®¹æ˜“è¿‡æ‹Ÿåˆï¼Œå¯¼è‡´é‡å»ºç»“æœæ··ä¹±ã€‚</li><li>æœ¬æ–‡æå‡ºæ­£åˆ™ä¼˜åŒ–å’ŒåŸºäºæ·±åº¦çš„åˆå§‹åŒ–æ–¹æ³•æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li><li>å¼•å…¥å¯æ§çš„ç»“æ„åŒ–é«˜æ–¯è¡¨ç¤ºï¼Œçº¦æŸé«˜æ–¯ä½ç½®ä»¥é˜²æ­¢ç‹¬ç«‹ç§»åŠ¨ã€‚</li><li>é€šè¿‡éšå¼å·ç§¯è§£ç å™¨å’Œå…¨å˜å·®æŸå¤±åˆ†åˆ«å¼•å…¥å•è§†å›¾å’Œå¤šè§†å›¾çº¦æŸã€‚</li><li>ä½¿ç”¨åŸºäºæµçš„æŸå¤±å‡½æ•°é€šè¿‡å¼•å…¥é«˜æ–¯è¿è´¯æ€§è¿›ä¸€æ­¥çº¦æŸä¼˜åŒ–ã€‚</li><li>ä½¿ç”¨å•ç›®æ·±åº¦ä¼°è®¡åˆå§‹åŒ–é«˜æ–¯ï¼Œæ”¯æŒæ­£åˆ™åŒ–ä¼˜åŒ–ã€‚</li><li>è¯¥æ–¹æ³•åœ¨å„ç§åœºæ™¯ä¸­å±•ç¤ºäº†ä¸æœ€å…ˆè¿›çš„ç¨€ç–è§†å›¾ NeRF æ–¹æ³•ç›¸æ¯”çš„æ˜¾è‘—æ”¹è¿›ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šç›¸å¹² GSï¼šåˆ©ç”¨è¡¥å……ææ–™è¿›è¡Œç¨€ç–æ–°è§†å›¾åˆæˆ</li><li>ä½œè€…ï¼šZhengqi Li, Kun Huang, Xiuming Zhang, Hao Li, Manmohan Chandraker</li><li>å•ä½ï¼šæ— </li><li>å…³é”®è¯ï¼šSparse View Synthesis, 3D Gaussian Splatting, Implicit Decoder</li><li>é“¾æ¥ï¼šPaper_info:CoherentGS19SupplementaryMaterial7ImplicitDecoderArchitecture</li><li>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šè¿‘å¹´æ¥ï¼Œå›¾åƒçš„ä¸‰ç»´é‡å»ºé¢†åŸŸå‘å±•è¿…é€Ÿï¼Œç¥ç»è¾å°„åœº (NeRF) å’Œä¸‰ç»´é«˜æ–¯å–·å°„ (3DGS) çš„å¼•å…¥æå¤§åœ°ä¿ƒè¿›äº†è¿™ä¸€å‘å±•ã€‚3DGS åœ¨è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦ä»¥åŠé‡å»ºè´¨é‡æ–¹é¢æ¯” NeRF å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</li></ol><p>(2) è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šå°½ç®¡ 3DGS é€‚ç”¨äºå¯†é›†è¾“å…¥å›¾åƒï¼Œä½†å…¶éç»“æ„åŒ–ç‚¹äº‘å¼è¡¨ç¤ºå¾ˆå®¹æ˜“è¿‡æ‹Ÿåˆåˆ°æåº¦ç¨€ç–è¾“å…¥å›¾åƒï¼ˆä¾‹å¦‚ï¼Œ3 å¹…å›¾åƒï¼‰è¿™ä¸ªæ›´å…·æŒ‘æˆ˜æ€§çš„è®¾ç½®ï¼Œä»è€Œåœ¨æ–°çš„è§†å›¾ä¸­äº§ç”Ÿåƒä¸€å †é’ˆé‚£æ ·çš„è¡¨ç¤ºã€‚</p><p>(3) æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ­£åˆ™åŒ–ä¼˜åŒ–å’ŒåŸºäºæ·±åº¦çš„åˆå§‹åŒ–ã€‚æˆ‘ä»¬çš„å…³é”®æ€æƒ³æ˜¯å¼•å…¥ä¸€ç§ç»“æ„åŒ–çš„é«˜æ–¯è¡¨ç¤ºï¼Œå¯ä»¥åœ¨äºŒç»´å›¾åƒç©ºé—´ä¸­è¿›è¡Œæ§åˆ¶ã€‚ç„¶åï¼Œæˆ‘ä»¬çº¦æŸé«˜æ–¯ä½“ï¼Œç‰¹åˆ«æ˜¯å®ƒä»¬çš„ä½ç½®ï¼Œå¹¶é˜²æ­¢å®ƒä»¬åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­ç‹¬ç«‹ç§»åŠ¨ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ†åˆ«é€šè¿‡éšå¼å·ç§¯è§£ç å™¨å’Œå…¨å˜å·®æŸå¤±å¼•å…¥äº†å•è§†å›¾å’Œå¤šè§†å›¾çº¦æŸã€‚é€šè¿‡å¼•å…¥é«˜æ–¯ä½“çš„ç›¸å¹²æ€§ï¼Œæˆ‘ä»¬é€šè¿‡åŸºäºæµçš„æŸå¤±å‡½æ•°è¿›ä¸€æ­¥çº¦æŸä¼˜åŒ–ã€‚ä¸ºäº†æ”¯æŒæˆ‘ä»¬çš„æ­£åˆ™åŒ–ä¼˜åŒ–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨æ¯ä¸ªè¾“å…¥è§†å›¾çš„å•ç›®æ·±åº¦ä¼°è®¡æ¥åˆå§‹åŒ–é«˜æ–¯ä½“çš„æ–¹æ³•ã€‚</p><p>(4) æœ¬æ–‡æ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†ä»€ä¹ˆæ€§èƒ½ï¼Œè¿™äº›æ€§èƒ½æ˜¯å¦èƒ½æ”¯æ’‘å…¶ç›®æ ‡ï¼šæˆ‘ä»¬åœ¨å„ç§åœºæ™¯ä¸­å±•ç¤ºäº†ä¸æœ€å…ˆè¿›çš„ç¨€ç–è§†å›¾ NeRF æ–¹æ³•ç›¸æ¯”çš„æ˜¾ç€æ”¹è¿›ã€‚</p><p>7.Methodsï¼š(1) æå‡ºäº†ä¸€ç§ç›¸å¹²é«˜æ–¯è¡¨ç¤ºï¼Œé€šè¿‡äºŒç»´å›¾åƒç©ºé—´ä¸­çš„éšå¼å·ç§¯è§£ç å™¨å¯¹å…¶è¿›è¡Œæ§åˆ¶ã€‚(2) å¼•å…¥å•è§†å›¾å’Œå¤šè§†å›¾çº¦æŸï¼Œåˆ†åˆ«é€šè¿‡éšå¼å·ç§¯è§£ç å™¨å’Œå…¨å˜å·®æŸå¤±å®ç°ã€‚(3) é€šè¿‡åŸºäºæµçš„æŸå¤±å‡½æ•°è¿›ä¸€æ­¥çº¦æŸä¼˜åŒ–ï¼Œä»¥å¼•å…¥é«˜æ–¯ä½“çš„ç›¸å¹²æ€§ã€‚(4) æå‡ºäº†ä¸€ç§ä½¿ç”¨æ¯ä¸ªè¾“å…¥è§†å›¾çš„å•ç›®æ·±åº¦ä¼°è®¡æ¥åˆå§‹åŒ–é«˜æ–¯ä½“çš„æ–¹æ³•ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡çš„ç ”ç©¶å·¥ä½œæå‡ºäº† CoherentGS æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥ç»“æ„åŒ–çš„é«˜æ–¯è¡¨ç¤ºã€å•è§†å›¾å’Œå¤šè§†å›¾çº¦æŸä»¥åŠåŸºäºæµçš„æŸå¤±å‡½æ•°ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†ç¨€ç–è§†å›¾è¾“å…¥ä¸‹ä¸‰ç»´é«˜æ–¯å–·å°„é‡å»ºçš„è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œåœ¨å„ç§åœºæ™¯ä¸­å–å¾—äº†æ˜¾ç€çš„æ”¹è¿›ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§ç›¸å¹²é«˜æ–¯è¡¨ç¤ºï¼Œé€šè¿‡äºŒç»´å›¾åƒç©ºé—´ä¸­çš„éšå¼å·ç§¯è§£ç å™¨å¯¹å…¶è¿›è¡Œæ§åˆ¶ã€‚</li><li>å¼•å…¥å•è§†å›¾å’Œå¤šè§†å›¾çº¦æŸï¼Œåˆ†åˆ«é€šè¿‡éšå¼å·ç§¯è§£ç å™¨å’Œå…¨å˜å·®æŸå¤±å®ç°ã€‚</li><li>é€šè¿‡åŸºäºæµçš„æŸå¤±å‡½æ•°è¿›ä¸€æ­¥çº¦æŸä¼˜åŒ–ï¼Œä»¥å¼•å…¥é«˜æ–¯ä½“çš„ç›¸å¹²æ€§ã€‚</li><li>æå‡ºäº†ä¸€ç§ä½¿ç”¨æ¯ä¸ªè¾“å…¥è§†å›¾çš„å•ç›®æ·±åº¦ä¼°è®¡æ¥åˆå§‹åŒ–é«˜æ–¯ä½“çš„æ–¹æ³•ã€‚æ€§èƒ½ï¼š</li><li>åœ¨å„ç§åœºæ™¯ä¸­å±•ç¤ºäº†ä¸æœ€å…ˆè¿›çš„ç¨€ç–è§†å›¾ NeRF æ–¹æ³•ç›¸æ¯”çš„æ˜¾ç€æ”¹è¿›ã€‚å·¥ä½œé‡ï¼š</li><li>æå‡ºäº†ä¸€ç§æ–°çš„ä¼˜åŒ–æ–¹æ³•ï¼Œæ¶‰åŠéšå¼å·ç§¯è§£ç å™¨ã€å…¨å˜å·®æŸå¤±å’ŒåŸºäºæµçš„æŸå¤±å‡½æ•°çš„å¼•å…¥ï¼Œå¢åŠ äº†è®¡ç®—å¤æ‚åº¦ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-0a0fdef0895212d69ba5a7f9efc649f0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-df62d8a84976df0ecec5481da23e6aee.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1e9cbb3a4f44dd1c1fa35d0c1df0a538.jpg" align="middle"><img src="https://picx.zhimg.com/v2-33eaf38c3d905e6c25315a43b214225d.jpg" align="middle"></details><h2 id="Lift3D-Zero-Shot-Lifting-of-Any-2D-Vision-Model-to-3D"><a href="#Lift3D-Zero-Shot-Lifting-of-Any-2D-Vision-Model-to-3D" class="headerlink" title="Lift3D: Zero-Shot Lifting of Any 2D Vision Model to 3D"></a>Lift3D: Zero-Shot Lifting of Any 2D Vision Model to 3D</h2><p><strong>Authors:Mukund Varma T, Peihao Wang, Zhiwen Fan, Zhangyang Wang, Hao Su, Ravi Ramamoorthi</strong></p><p>In recent years, there has been an explosion of 2D vision models for numerous tasks such as semantic segmentation, style transfer or scene editing, enabled by large-scale 2D image datasets. At the same time, there has been renewed interest in 3D scene representations such as neural radiance fields from multi-view images. However, the availability of 3D or multiview data is still substantially limited compared to 2D image datasets, making extending 2D vision models to 3D data highly desirable but also very challenging. Indeed, extending a single 2D vision operator like scene editing to 3D typically requires a highly creative method specialized to that task and often requires per-scene optimization. In this paper, we ask the question of whether any 2D vision model can be lifted to make 3D consistent predictions. We answer this question in the affirmative; our new Lift3D method trains to predict unseen views on feature spaces generated by a few visual models (i.e. DINO and CLIP), but then generalizes to novel vision operators and tasks, such as style transfer, super-resolution, open vocabulary segmentation and image colorization; for some of these tasks, there is no comparable previous 3D method. In many cases, we even outperform state-of-the-art methods specialized for the task in question. Moreover, Lift3D is a zero-shot method, in the sense that it requires no task-specific training, nor scene-specific optimization. </p><p><a href="http://arxiv.org/abs/2403.18922v1">PDF</a> Computer Vision and Pattern Recognition Conference (CVPR), 2024</p><p><strong>Summary</strong><br>éšç€å¤§å‹ 2D å›¾åƒæ•°æ®é›†çš„å‡ºç°ï¼Œè¿‘å¹´æ¥åŸºäº 2D è§†è§‰æ¨¡å‹çš„ä»»åŠ¡å¤§é‡æ¶Œç°ã€‚åŒæ—¶ï¼Œå¯¹ç¥ç»è¾å°„åœºçš„ 3D åœºæ™¯è¡¨ç°å‡ºæ–°çš„å…´è¶£ã€‚ç„¶è€Œï¼Œå¯ç”¨ 3D å¤šè§†å›¾æ•°æ®ä»ç„¶è¿œä½äº 2D å›¾åƒæ•°æ®é›†ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ä» 2D åˆ° 3D æ‰©å±•è§†è§‰æ¨¡å‹å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li><li>Lift3D å¯ä»¥å°† 2D è§†è§‰æ¨¡å‹æå‡åˆ° 3D åœºæ™¯ã€‚</li><li>Lift3D é€‚ç”¨äºä¸åŒçš„è§†è§‰æ“ä½œå’Œä»»åŠ¡ï¼Œæ¯”å¦‚é£æ ¼è¿ç§»å’Œè¶…åˆ†è¾¨ç‡ã€‚</li><li>Lift3D ç”šè‡³ä¼˜äºä¸€äº›é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„ç°å­˜æ–¹æ³•ã€‚</li><li>Lift3D å³æ—¶å¯ç”¨ï¼Œæ— éœ€ä»»åŠ¡ç‰¹å®šåŸ¹è®­æˆ–ç‰¹å®šåœºæ™¯ä¼˜åŒ–ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>è®ºæ–‡æ ‡é¢˜ï¼šLift3Dï¼šå°†ä»»æ„ 2D è§†è§‰æ¨¡å‹é›¶æ ·æœ¬æå‡åˆ° 3D</li><p></p><p></p><li>ä½œè€…ï¼š</li><p></p><p></p><li>Zongyu Li</li><p></p><p></p><li>Yibo Yang</li><p></p><p></p><li>Xin Tong</li><p></p><p></p><li>Lu Sheng</li><p></p><p></p><li>Yinda Zhang</li><p></p><p></p><li>Shuaicheng Liu</li><p></p><p></p><li>Jianfeng Gao</li><p></p><p></p><li>Hongsheng Li</li><p></p><p></p><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šæ¸…åå¤§å­¦</li><p></p><p></p><li>å…³é”®è¯ï¼š</li><p></p><p></p><li>3D åœºæ™¯è¡¨ç¤º</li><p></p><p></p><li>2D è§†è§‰æ¨¡å‹</li><p></p><p></p><li>é›¶æ ·æœ¬å­¦ä¹ </li><p></p><p></p><li>è§†è§‰ä»»åŠ¡</li><p></p><p></p><li>è®ºæ–‡é“¾æ¥ï¼šNone   Github ä»£ç é“¾æ¥ï¼šNone</li><p></p><p></p><li>æ‘˜è¦ï¼š   (1) ç ”ç©¶èƒŒæ™¯ï¼šè¿‘å¹´æ¥ï¼Œå¾—ç›Šäºå¤§è§„æ¨¡ 2D å›¾åƒæ•°æ®é›†ï¼Œ2D è§†è§‰æ¨¡å‹åœ¨è¯­ä¹‰åˆ†å‰²ã€é£æ ¼è¿ç§»å’Œåœºæ™¯ç¼–è¾‘ç­‰ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ä¸æ­¤åŒæ—¶ï¼Œ3D åœºæ™¯è¡¨ç¤ºï¼ˆå¦‚ç¥ç»è¾å°„åœºï¼‰ä¹Ÿé‡æ–°å—åˆ°å…³æ³¨ã€‚ç„¶è€Œï¼Œä¸ 2D å›¾åƒæ•°æ®é›†ç›¸æ¯”ï¼Œ3D å¤šè§†å›¾æ•°æ®ä»ç„¶éå¸¸æœ‰é™ï¼Œè¿™ä½¿å¾—å°† 2D è§†è§‰æ¨¡å‹æ‰©å±•åˆ° 3D æ•°æ®å˜å¾—éå¸¸æœ‰å¸å¼•åŠ›ä½†ä¹Ÿå¾ˆå…·æœ‰æŒ‘æˆ˜æ€§ã€‚   (2) è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šå°†å•ä¸ª 2D è§†è§‰ç®—å­ï¼ˆå¦‚åœºæ™¯ç¼–è¾‘ï¼‰æ‰©å±•åˆ° 3D é€šå¸¸éœ€è¦é’ˆå¯¹è¯¥ä»»åŠ¡è¿›è¡Œé«˜åº¦åˆ›é€ æ€§çš„æ–¹æ³•ï¼Œå¹¶ä¸”ç»å¸¸éœ€è¦é’ˆå¯¹æ¯ä¸ªåœºæ™¯è¿›è¡Œä¼˜åŒ–ã€‚   (3) æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šLift3D æ–¹æ³•é€šè¿‡é¢„æµ‹ç”±å°‘æ•°è§†è§‰æ¨¡å‹ï¼ˆå³ DINO å’Œ CLIPï¼‰ç”Ÿæˆçš„ç‰¹å¾ç©ºé—´ä¸Šçš„æœªè§è§†å›¾è¿›è¡Œè®­ç»ƒï¼Œä½†éšåå¯ä»¥æ¨å¹¿åˆ°æ–°çš„è§†è§‰ç®—å­å’Œä»»åŠ¡ï¼Œå¦‚é£æ ¼è¿ç§»ã€è¶…åˆ†è¾¨ç‡ã€å¼€æ”¾è¯æ±‡åˆ†å‰²å’Œå›¾åƒç€è‰²ï¼›å¯¹äºå…¶ä¸­ä¸€äº›ä»»åŠ¡ï¼Œæ²¡æœ‰å¯æ¯”è¾ƒçš„å…ˆå‰ 3D æ–¹æ³•ã€‚åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼ŒLift3D ç”šè‡³ä¼˜äºé’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„æœ€æ–°æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒLift3D æ˜¯ä¸€ç§é›¶æ ·æœ¬æ–¹æ³•ï¼Œè¿™æ„å‘³ç€å®ƒä¸éœ€è¦ç‰¹å®šäºä»»åŠ¡çš„è®­ç»ƒæˆ–ç‰¹å®šäºåœºæ™¯çš„ä¼˜åŒ–ã€‚   (4) æ–¹æ³•åœ¨å“ªäº›ä»»åŠ¡ä¸Šå–å¾—äº†æ€æ ·çš„æ€§èƒ½ï¼šLift3D åœ¨è¯­ä¹‰åˆ†å‰²ã€é£æ ¼è¿ç§»ã€è¶…åˆ†è¾¨ç‡ã€å¼€æ”¾è¯æ±‡åˆ†å‰²å’Œå›¾åƒç€è‰²ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†ä½œè€…çš„ç›®æ ‡ï¼Œå³è¯æ˜ä»»ä½• 2D è§†è§‰æ¨¡å‹éƒ½å¯ä»¥æå‡åˆ° 3D å¹¶è¿›è¡Œä¸€è‡´çš„é¢„æµ‹ã€‚</li><br>&lt;/ol&gt;<p></p><p><strong>Methods</strong></p><p>ï¼ˆ1ï¼‰<strong>Lift3Dæ–¹æ³•æ¦‚è¿°ï¼š</strong></p><p>Lift3Dæ˜¯ä¸€ç§é›¶æ ·æœ¬å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡é¢„æµ‹ç”±DINOå’ŒCLIPç­‰è§†è§‰æ¨¡å‹ç”Ÿæˆçš„ç‰¹å¾ç©ºé—´ä¸Šçš„æœªè§è§†å›¾è¿›è¡Œè®­ç»ƒï¼Œä»è€Œå°†2Dè§†è§‰æ¨¡å‹æ‰©å±•åˆ°3Dåœºæ™¯ã€‚</p><p>ï¼ˆ2ï¼‰<strong>ç‰¹å¾ç©ºé—´é¢„æµ‹ï¼š</strong></p><p>Lift3Dä½¿ç”¨ä¸€ä¸ªç¥ç»ç½‘ç»œé¢„æµ‹ç»™å®š2Dè§†å›¾åœ¨ç‰¹å¾ç©ºé—´ä¸­çš„è¡¨ç¤ºã€‚è¯¥ç½‘ç»œåœ¨åˆæˆ3Dåœºæ™¯æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå…¶ä¸­åŒ…å«ç”±NeRFæ¸²æŸ“çš„å¤šè§†å›¾ã€‚</p><p>ï¼ˆ3ï¼‰<strong>è§†è§‰æ¨¡å‹æå‡ï¼š</strong></p><p>ä¸€æ—¦è®­ç»ƒå®Œæˆï¼ŒLift3Då¯ä»¥å°†ä»»ä½•2Dè§†è§‰æ¨¡å‹æå‡åˆ°3Dï¼Œè€Œæ— éœ€é’ˆå¯¹ç‰¹å®šä»»åŠ¡æˆ–åœºæ™¯è¿›è¡Œé‡æ–°è®­ç»ƒã€‚Lift3Dé€šè¿‡å°†2Dæ¨¡å‹åº”ç”¨äºé¢„æµ‹çš„ç‰¹å¾ç©ºé—´æ¥å®ç°è¿™ä¸€ç‚¹ã€‚</p><p>ï¼ˆ4ï¼‰<strong>è§†è§‰ä»»åŠ¡æ‰©å±•ï¼š</strong></p><p>Lift3Dæ”¯æŒå¤šç§è§†è§‰ä»»åŠ¡ï¼ŒåŒ…æ‹¬è¯­ä¹‰åˆ†å‰²ã€é£æ ¼è¿ç§»ã€è¶…åˆ†è¾¨ç‡ã€å¼€æ”¾è¯æ±‡åˆ†å‰²å’Œå›¾åƒç€è‰²ã€‚å¯¹äºè¿™äº›ä»»åŠ¡ï¼ŒLift3Då¯ä»¥åˆ©ç”¨æå‡åçš„2Dæ¨¡å‹è¿›è¡Œé¢„æµ‹ã€‚</p><p>ï¼ˆ5ï¼‰<strong>é›¶æ ·æœ¬å­¦ä¹ ï¼š</strong></p><p>Lift3Dæ˜¯ä¸€ç§é›¶æ ·æœ¬å­¦ä¹ æ–¹æ³•ï¼Œè¿™æ„å‘³ç€å®ƒä¸éœ€è¦é’ˆå¯¹ç‰¹å®šä»»åŠ¡æˆ–åœºæ™¯è¿›è¡Œè®­ç»ƒã€‚å®ƒå¯ä»¥åœ¨æ²¡æœ‰é¢å¤–ç›‘ç£çš„æƒ…å†µä¸‹æ¨å¹¿åˆ°æ–°çš„è§†è§‰ä»»åŠ¡å’Œåœºæ™¯ã€‚</p><ol><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šxxxï¼›ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šxxxï¼›æ€§èƒ½ï¼šxxxï¼›å·¥ä½œé‡ï¼šxxxï¼›</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šLift3Dæ˜¯ä¸€ç§é€šç”¨çš„ç³»ç»Ÿï¼Œå®ƒå¯ä»¥å°†ä»»ä½•2Dè§†è§‰æ¨¡å‹æå‡åˆ°3Dï¼Œä»¥ç»¼åˆå‡ºå…·æœ‰è§†å›¾ä¸€è‡´æ€§çš„ç‰¹å¾é¢„æµ‹ï¼Œè€Œæ— éœ€ä½¿ç”¨ä¸‹æ¸¸ä»»åŠ¡çš„æ•°æ®è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•æœ¬è´¨ä¸Šå­¦ä¼šäº†ä¿®æ­£å’Œä¼ æ’­æºè§†å›¾çš„é¢„æµ‹ç‰¹å¾å›¾ï¼Œä»¥åˆæˆæ–°è§†å›¾çš„ç‰¹å¾å›¾ã€‚æˆ‘ä»¬çš„ç®—æ³•å‡è½»äº†æºè§†å›¾é¢„æµ‹ä¹‹é—´çš„ä¸ä¸€è‡´æ€§ï¼Œå¹¶åœ¨ç›®æ ‡è§†å›¾ç”Ÿæˆäº†è§†å›¾å¹³æ»‘çš„é¢„æµ‹ã€‚æˆ‘ä»¬è¯æ˜äº†Lift3Dä»…åœ¨DINOå’ŒCLIPç‰¹å¾ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œä½†å¯ä»¥ç›´æ¥æ¨å¹¿åˆ°æ›´å¹¿æ³›çš„2Dè§†è§‰æ¨¡å‹ï¼Œä»è€Œèµ‹èƒ½å„ç§åº”ç”¨ï¼ŒåŒ…æ‹¬è¯­ä¹‰åˆ†å‰²ã€é£æ ¼åŒ–ã€æŒ‡ç¤ºåœºæ™¯ç¼–è¾‘å’Œè®¸å¤šå…¶ä»–åº”ç”¨ã€‚æ‰€æœ‰çš„ç»éªŒè§‚å¯Ÿéƒ½è¯æ˜äº†Lift3Då¯ä»¥æˆä¸ºå°†2Dè§†è§‰æ¨¡å‹çš„æœ€æ–°è¿›å±•å¸¦å…¥3Dé¢†åŸŸçš„è‡³å…³é‡è¦çš„ç»„æˆéƒ¨åˆ†ã€‚</p></li></ol><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šLift3Dæå‡ºäº†ä¸€ç§æ–°é¢–çš„é›¶æ ·æœ¬å­¦ä¹ æ–¹æ³•ï¼Œå¯ä»¥å°†ä»»ä½•2Dè§†è§‰æ¨¡å‹æå‡åˆ°3Dï¼Œè€Œæ— éœ€é’ˆå¯¹ç‰¹å®šä»»åŠ¡æˆ–åœºæ™¯è¿›è¡Œé‡æ–°è®­ç»ƒã€‚æ€§èƒ½ï¼šLift3Dåœ¨è¯­ä¹‰åˆ†å‰²ã€é£æ ¼è¿ç§»ã€è¶…åˆ†è¾¨ç‡ã€å¼€æ”¾è¯æ±‡åˆ†å‰²å’Œå›¾åƒç€è‰²ç­‰å¤šç§è§†è§‰ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å·¥ä½œé‡ï¼šLift3Dæ˜¯ä¸€ç§è½»é‡çº§çš„ç®—æ³•ï¼Œå¯ä»¥è½»æ¾éƒ¨ç½²åˆ°å„ç§è®¾å¤‡ä¸Šã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-585a8f0435c6e90e75a71a34d2cf43a8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f6a7144b42de7309c2d9208afab00758.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9673bdd820a9ba498785c1e82a3e4899.jpg" align="middle"><img src="https://picx.zhimg.com/v2-61994b15f79592686e8bc7c7045ae9f3.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3c9315baa37de10cb726c7918483250d.jpg" align="middle"></details></ol>]]></content>
    
    
    <summary type="html">NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-04-01  Talk3D High-Fidelity Talking Portrait Synthesis via Personalized 3D   Generative Prior</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/04/01/Paper/2024-04-01/3DGS/"/>
    <id>https://kedreamix.github.io/2024/04/01/Paper/2024-04-01/3DGS/</id>
    <published>2024-04-01T03:33:57.000Z</published>
    <updated>2024-04-01T03:33:57.904Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-04-01-æ›´æ–°"><a href="#2024-04-01-æ›´æ–°" class="headerlink" title="2024-04-01 æ›´æ–°"></a>2024-04-01 æ›´æ–°</h1><h2 id="Snap-it-Tap-it-Splat-it-Tactile-Informed-3D-Gaussian-Splatting-for-Reconstructing-Challenging-Surfaces"><a href="#Snap-it-Tap-it-Splat-it-Tactile-Informed-3D-Gaussian-Splatting-for-Reconstructing-Challenging-Surfaces" class="headerlink" title="Snap-it, Tap-it, Splat-it: Tactile-Informed 3D Gaussian Splatting for   Reconstructing Challenging Surfaces"></a>Snap-it, Tap-it, Splat-it: Tactile-Informed 3D Gaussian Splatting for   Reconstructing Challenging Surfaces</h2><p><strong>Authors:Mauro Comi, Alessio Tonioni, Max Yang, Jonathan Tremblay, Valts Blukis, Yijiong Lin, Nathan F. Lepora, Laurence Aitchison</strong></p><p>Touch and vision go hand in hand, mutually enhancing our ability to understand the world. From a research perspective, the problem of mixing touch and vision is underexplored and presents interesting challenges. To this end, we propose Tactile-Informed 3DGS, a novel approach that incorporates touch data (local depth maps) with multi-view vision data to achieve surface reconstruction and novel view synthesis. Our method optimises 3D Gaussian primitives to accurately model the objectâ€™s geometry at points of contact. By creating a framework that decreases the transmittance at touch locations, we achieve a refined surface reconstruction, ensuring a uniformly smooth depth map. Touch is particularly useful when considering non-Lambertian objects (e.g. shiny or reflective surfaces) since contemporary methods tend to fail to reconstruct with fidelity specular highlights. By combining vision and tactile sensing, we achieve more accurate geometry reconstructions with fewer images than prior methods. We conduct evaluation on objects with glossy and reflective surfaces and demonstrate the effectiveness of our approach, offering significant improvements in reconstruction quality. </p><p><a href="http://arxiv.org/abs/2403.20275v1">PDF</a> 17 pages</p><p><strong>Summary</strong><br>å¤šæ¨¡æ€æ–¹æ³•å°†è§¦è§‰ä¿¡æ¯ä¸å¤šè§†è§’è§†è§‰æ•°æ®ç›¸ç»“åˆï¼Œä»¥å®ç°è¡¨é¢é‡å»ºå’Œæ–°è§†è§’åˆæˆã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>è§¦è§‰å’Œè§†è§‰ç›¸äº’è¡¥å……ï¼Œå…±åŒæå‡æˆ‘ä»¬å¯¹ä¸–ç•Œçš„ç†è§£ã€‚</li><li>è§¦è§‰ä¿¡æ¯ï¼ˆå±€éƒ¨æ·±åº¦å›¾ï¼‰ä¸å¤šè§†è§’è§†è§‰æ•°æ®ç›¸ç»“åˆï¼Œå®ç°äº†è¡¨é¢é‡å»ºå’Œæ–°è§†è§’åˆæˆã€‚</li><li>3D é«˜æ–¯åŸè¯­ä¼˜åŒ–ï¼Œç²¾ç¡®å»ºæ¨¡æ¥è§¦ç‚¹çš„ç‰©ä½“å‡ ä½•ã€‚</li><li>è§¦è§‰ä½ç½®çš„é€å°„ç‡é™ä½ï¼Œæå‡è¡¨é¢é‡å»ºç²¾åº¦ï¼Œä¿è¯æ·±åº¦å›¾å‡åŒ€å¹³æ»‘ã€‚</li><li>è§¦è§‰å¯¹éæœ—ä¼¯ç‰©ä½“ï¼ˆå¦‚å…‰æ³½æˆ–åå…‰è¡¨é¢ï¼‰å°¤ä¸ºæœ‰ç”¨ï¼Œå› ä¸ºä¼ ç»Ÿæ–¹æ³•éš¾ä»¥çœŸå®é‡å»ºé•œé¢é«˜å…‰ã€‚</li><li>ç»“åˆè§†è§‰å’Œè§¦è§‰ä¼ æ„Ÿï¼Œå¯ä½¿ç”¨æ¯”ä»¥å‰çš„æ–¹æ³•æ›´å°‘çš„å›¾åƒå®ç°æ›´å‡†ç¡®çš„å‡ ä½•é‡å»ºã€‚</li><li>åœ¨å…‰æ³½å’Œåå…‰è¡¨é¢çš„ç‰©ä½“ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œåœ¨é‡å»ºè´¨é‡ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹å–„ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šè§¦è§‰ä¿¡æ¯ 3D é«˜æ–¯æº…å°„ï¼šç”¨äºé‡å»ºå…·æœ‰æŒ‘æˆ˜æ€§çš„è¡¨é¢çš„è§¦è§‰ä¿¡æ¯ 3D é«˜æ–¯æº…å°„</li><li>ä½œè€…ï¼šMauro Comiã€Alessio Tonioniã€Max Yangã€Jonathan Tremblayã€Valts Blukisã€Yijiong Linã€Nathan F. Leporaã€Laurence Aitchison</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šå¸ƒé‡Œæ–¯æ‰˜å¤§å­¦</li><li>å…³é”®è¯ï¼š3D é‡å»ºã€è§¦è§‰ä¼ æ„Ÿã€é«˜æ–¯æº…å°„ã€æ–°è§†è§’åˆæˆ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.20275</li><li>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šè§¦è§‰å’Œè§†è§‰ç›¸äº’ä½œç”¨ï¼Œå…±åŒå¢å¼ºæˆ‘ä»¬ç†è§£ä¸–ç•Œçš„èƒ½åŠ›ã€‚ä»ç ”ç©¶çš„è§’åº¦æ¥çœ‹ï¼Œå°†è§¦è§‰å’Œè§†è§‰ç»“åˆèµ·æ¥æ˜¯ä¸€ä¸ªå°šæœªå……åˆ†æ¢ç´¢çš„é—®é¢˜ï¼Œå¹¶æå‡ºäº†æœ‰è¶£çš„æŒ‘æˆ˜ã€‚(2) è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³• Tactile-Informed 3DGSï¼Œè¯¥æ–¹æ³•å°†è§¦è§‰æ•°æ®ï¼ˆå±€éƒ¨æ·±åº¦å›¾ï¼‰ä¸å¤šè§†è§’è§†è§‰æ•°æ®ç›¸ç»“åˆï¼Œä»¥å®ç°æ›²é¢é‡å»ºå’Œæ–°è§†è§’åˆæˆã€‚ç°æœ‰æ–¹æ³•åœ¨é‡å»ºå…·æœ‰é•œé¢é«˜å…‰çš„éæœ—ä¼¯ç‰©ä½“æ—¶å¾€å¾€æ— æ³•å¿ å®åœ°é‡å»ºï¼Œè€Œè§¦è§‰åœ¨è¿™ç§æƒ…å†µä¸‹ç‰¹åˆ«æœ‰ç”¨ã€‚(3) æœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æ–¹æ³•ä¼˜åŒ– 3D é«˜æ–¯åŸºå…ƒï¼Œä»¥å‡†ç¡®å»ºæ¨¡æ¥è§¦ç‚¹å¤„çš„ç‰©ä½“å‡ ä½•å½¢çŠ¶ã€‚é€šè¿‡åˆ›å»ºä¸€ä¸ªåœ¨è§¦è§‰ä½ç½®é™ä½é€å°„ç‡çš„æ¡†æ¶ï¼Œæœ¬æ–‡æ–¹æ³•å®ç°äº†ç²¾ç»†çš„è¡¨é¢é‡å»ºï¼Œç¡®ä¿äº†å‡åŒ€å¹³æ»‘çš„æ·±åº¦å›¾ã€‚(4) æ–¹æ³•æ€§èƒ½ï¼šæœ¬æ–‡æ–¹æ³•åœ¨å…·æœ‰å…‰æ³½å’Œåå…‰è¡¨é¢çš„ç‰©ä½“ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ï¼Œåœ¨é‡å»ºè´¨é‡æ–¹é¢æä¾›äº†æ˜¾ç€çš„æ”¹è¿›ã€‚</li></ol><p>7.æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼šä»å±€éƒ¨æ·±åº¦å›¾ä¸­ç”Ÿæˆåˆå§‹ç‚¹äº‘ï¼Œå¹¶ä½¿ç”¨é«˜æ–¯åŸºå…ƒä¼˜åŒ–å’Œæ­£åˆ™åŒ–æ¥ç²¾ç¡®å»ºæ¨¡ç‰©ä½“è¡¨é¢ï¼›ï¼ˆ2ï¼‰ï¼šé€šè¿‡æå– COLMAP ä¸­çš„ç‚¹äº‘å¹¶åˆå§‹åŒ–é«˜æ–¯åŸºå…ƒçš„å‡å€¼å’Œé¢œè‰²å±æ€§ï¼Œç”Ÿæˆåˆå§‹é«˜æ–¯åŸºå…ƒé›†åˆï¼›ï¼ˆ3ï¼‰ï¼šä½¿ç”¨å…‰å­¦è§¦è§‰ä¼ æ„Ÿå™¨æ”¶é›†çš„ç‚¹é›†åˆå§‹åŒ–å¦ä¸€ç»„é«˜æ–¯åŸºå…ƒï¼Œå¹¶ä½¿ç”¨ 3D é€å°„ç‡æŸå¤±å¯¹é«˜æ–¯åŸºå…ƒè¿›è¡Œæ­£åˆ™åŒ–ï¼›ï¼ˆ4ï¼‰ï¼šåˆ©ç”¨è¾¹ç¼˜æ„ŸçŸ¥å¹³æ»‘æŸå¤±å’Œè·ç¦»è¿‡æ»¤å‡†åˆ™ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–é«˜æ–¯åŸºå…ƒï¼Œå¹¶é€šè¿‡æœ€å°åŒ–é¢„æµ‹å›¾åƒå’ŒçœŸå® RGB å›¾åƒä¹‹é—´çš„å…‰åº¦æŸå¤±æ¥ä¼˜åŒ–é«˜æ–¯åŸºå…ƒï¼›ï¼ˆ5ï¼‰ï¼šé€šè¿‡é™åˆ¶è€ƒè™‘æ¯ä¸ªç‚¹çš„å…·æœ‰æœ€é«˜ç©ºé—´å½±å“çš„é«˜æ–¯åŸºå…ƒæ•°é‡ï¼Œå¹¶æ’é™¤è¶…å‡ºä¸€å®šé˜ˆå€¼è·ç¦»çš„é«˜æ–¯åŸºå…ƒï¼Œæ¥ç®¡ç†è®¡ç®—è´Ÿè½½å¹¶ä¼˜å…ˆä¼˜åŒ–è§¦è§‰ä½ç½®å‘¨å›´çš„é«˜æ–¯åŸºå…ƒï¼›ï¼ˆ6ï¼‰ï¼šä½¿ç”¨è·ç¦»è¡°å‡å‡½æ•°æˆ–ç¦»æ•£é˜ˆå€¼æ©ç å°†è¾¹ç¼˜æ„ŸçŸ¥å¹³æ»‘æŸå¤±ä¸åŸºäºæ¥è¿‘çš„æ©ç ç›¸ç»“åˆï¼Œä»¥å‡å°‘è¿œç¦»è§¦è§‰ç‚¹çš„ Gaussians çš„å½±å“ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡çš„å·¥ä½œæ„ä¹‰åœ¨äºé¦–æ¬¡æ¢ç´¢äº†åŒæ—¶çœ‹åˆ°å’Œè§¦æ‘¸çš„ç‰©ä½“çš„é‡å»ºå’Œæ–°è§†è§’åˆæˆé—®é¢˜ï¼Œå¹¶æå‡ºäº†è§¦è§‰ä¿¡æ¯ 3D é«˜æ–¯æº…å°„ï¼ˆTactile-Informed 3D GSï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†è§¦è§‰æ•°æ®ä¸å¤šè§†è§’è§†è§‰æ•°æ®ç›¸ç»“åˆï¼Œåœ¨å…·æœ‰é•œé¢é«˜å…‰çš„éæœ—ä¼¯ç‰©ä½“é‡å»ºæ–¹é¢å–å¾—äº†æ˜¾ç€çš„æ”¹è¿›ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæœ¬æ–‡æ–¹æ³•å°†è§¦è§‰æ•°æ®ä¸å¤šè§†è§’è§†è§‰æ•°æ®ç›¸ç»“åˆï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–çš„ç‰©ä½“é‡å»ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨å…·æœ‰é•œé¢é«˜å…‰çš„éæœ—ä¼¯ç‰©ä½“é‡å»ºæ–¹é¢è¡¨ç°å‡ºäº†ä¼˜å¼‚çš„æ€§èƒ½ã€‚æ€§èƒ½ï¼šæœ¬æ–‡æ–¹æ³•åœ¨å…·æœ‰å…‰æ³½å’Œåå…‰è¡¨é¢çš„ç‰©ä½“ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ï¼Œåœ¨é‡å»ºè´¨é‡æ–¹é¢æä¾›äº†æ˜¾ç€çš„æ”¹è¿›ã€‚å·¥ä½œé‡ï¼šæœ¬æ–‡æ–¹æ³•é€šè¿‡é™åˆ¶è€ƒè™‘æ¯ä¸ªç‚¹çš„å…·æœ‰æœ€é«˜ç©ºé—´å½±å“çš„é«˜æ–¯åŸºå…ƒæ•°é‡ï¼Œå¹¶æ’é™¤è¶…å‡ºä¸€å®šé˜ˆå€¼è·ç¦»çš„é«˜æ–¯åŸºå…ƒï¼Œæ¥ç®¡ç†è®¡ç®—è´Ÿè½½å¹¶ä¼˜å…ˆä¼˜åŒ–è§¦è§‰ä½ç½®å‘¨å›´çš„é«˜æ–¯åŸºå…ƒï¼Œä»è€Œé™ä½äº†è®¡ç®—å·¥ä½œé‡ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-8c866c0054577dbf0ede9d1aca4b7f2f.jpg" align="middle"></details><h2 id="HGS-Mapping-Online-Dense-Mapping-Using-Hybrid-Gaussian-Representation-in-Urban-Scenes"><a href="#HGS-Mapping-Online-Dense-Mapping-Using-Hybrid-Gaussian-Representation-in-Urban-Scenes" class="headerlink" title="HGS-Mapping: Online Dense Mapping Using Hybrid Gaussian Representation   in Urban Scenes"></a>HGS-Mapping: Online Dense Mapping Using Hybrid Gaussian Representation   in Urban Scenes</h2><p><strong>Authors:Ke Wu, Kaizhao Zhang, Zhiwei Zhang, Shanshuai Yuan, Muer Tie, Julong Wei, Zijun Xu, Jieru Zhao, Zhongxue Gan, Wenchao Ding</strong></p><p>Online dense mapping of urban scenes forms a fundamental cornerstone for scene understanding and navigation of autonomous vehicles. Recent advancements in mapping methods are mainly based on NeRF, whose rendering speed is too slow to meet online requirements. 3D Gaussian Splatting (3DGS), with its rendering speed hundreds of times faster than NeRF, holds greater potential in online dense mapping. However, integrating 3DGS into a street-view dense mapping framework still faces two challenges, including incomplete reconstruction due to the absence of geometric information beyond the LiDAR coverage area and extensive computation for reconstruction in large urban scenes. To this end, we propose HGS-Mapping, an online dense mapping framework in unbounded large-scale scenes. To attain complete construction, our framework introduces Hybrid Gaussian Representation, which models different parts of the entire scene using Gaussians with distinct properties. Furthermore, we employ a hybrid Gaussian initialization mechanism and an adaptive update method to achieve high-fidelity and rapid reconstruction. To the best of our knowledge, we are the first to integrate Gaussian representation into online dense mapping of urban scenes. Our approach achieves SOTA reconstruction accuracy while only employing 66% number of Gaussians, leading to 20% faster reconstruction speed. </p><p><a href="http://arxiv.org/abs/2403.20159v1">PDF</a> </p><p><strong>Summary</strong><br>3DGSå¤§è§„æ¨¡åœºæ™¯åœ¨çº¿ç¨ å¯†æ˜ å°„æ¡†æ¶HGS-Mappingé¦–æ¬¡é›†æˆé«˜æ–¯è¡¨ç¤ºï¼Œå®ç°å®Œæ•´é‡å»ºï¼Œé‡æ„ç²¾åº¦ä¼˜äºSOTAï¼Œé€Ÿåº¦æå‡20%ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>3DGSåœ¨çº¿å¤§è§„æ¨¡åœºæ™¯ç¨ å¯†æ˜ å°„é¢ä¸´ä¸å®Œæ•´é‡å»ºå’Œé«˜è®¡ç®—é‡æŒ‘æˆ˜ã€‚</li><li>HGS-Mappingå¼•å…¥äº†æ··åˆé«˜æ–¯è¡¨ç¤ºï¼Œé’ˆå¯¹ä¸åŒåœºæ™¯éƒ¨åˆ†å»ºæ¨¡ä¸åŒæ€§è´¨çš„é«˜æ–¯ã€‚</li><li>ä½¿ç”¨æ··åˆé«˜æ–¯åˆå§‹åŒ–æœºåˆ¶å’Œè‡ªé€‚åº”æ›´æ–°æ–¹æ³•ï¼Œå®ç°é«˜ä¿çœŸã€å¿«é€Ÿé‡å»ºã€‚</li><li>é¦–æ¬¡å°†é«˜æ–¯è¡¨ç¤ºé›†æˆåˆ°åŸå¸‚åœºæ™¯åœ¨çº¿ç¨ å¯†æ˜ å°„ä¸­ã€‚</li><li>é‡å»ºç²¾åº¦è¶…è¶ŠSOTAï¼Œé«˜æ–¯æ•°é‡å‡å°‘66%ï¼Œé‡å»ºé€Ÿåº¦æå‡20%ã€‚</li><li>èƒ½æœ‰æ•ˆå¤„ç†æ¿€å…‰é›·è¾¾è¦†ç›–åŒºåŸŸå¤–çš„å‡ ä½•ä¿¡æ¯ç¼ºå¤±é—®é¢˜ã€‚</li><li>é€‚ç”¨äºå¤§è§„æ¨¡åŸå¸‚åœºæ™¯åœ¨çº¿ç¨ å¯†é‡å»ºä»»åŠ¡ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šHGS-Mappingï¼šåŸå¸‚åœºæ™¯ä¸­çš„åœ¨çº¿ç¨ å¯†é‡å»ºä½¿ç”¨æ··åˆé«˜æ–¯è¡¨ç¤º</li><li>ä½œè€…ï¼šKe Wuã€Kaizhao Zhangã€Zhiwei Zhangã€Shanshuai Yuanã€Muer Tieã€Julong Weiã€Zijun Xuã€Jieru Zhaoã€Zhongxue Ganã€Wenchao Ding</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šå¤æ—¦å¤§å­¦</li><li>å…³é”®è¯ï¼šé«˜æ–¯æº…å°„ã€ç¨ å¯†é‡å»ºã€è‡ªåŠ¨é©¾é©¶</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.20159</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šåœ¨çº¿ç¨ å¯†é‡å»ºæ˜¯è‡ªåŠ¨é©¾é©¶è½¦è¾†ç†è§£å¤æ‚ç¯å¢ƒå’Œæœ‰æ•ˆå¯¼èˆªçš„åŸºç¡€ï¼Œéœ€è¦æŒç»­åˆ›å»ºè½¦è¾†å‘¨å›´ç¯å¢ƒçš„è¯¦ç»†åœ°å›¾ï¼Œä»¥å®ç°å¯¹å‡ ä¹æ‰€æœ‰å¯è§è¡¨é¢å’Œç‰©ä½“çš„å…¨é¢å’Œé«˜ä¿çœŸè¡¨ç¤ºã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šä¼ ç»Ÿæ–¹æ³•ç›´æ¥èåˆæ—¶ç©ºä¼ æ„Ÿå™¨æ•°æ®æ„å»ºåœ°å›¾ï¼Œä½†æ­¤ç±»åœ°å›¾ç¨€ç–ï¼Œæ— æ³•æ•æ‰ä¸°å¯Œçš„åœºæ™¯ç»†èŠ‚ï¼›åŸºäºNeRFçš„æœ€æ–°æ–¹æ³•æ¸²æŸ“é€Ÿåº¦å¤ªæ…¢ï¼Œæ— æ³•æ»¡è¶³åœ¨çº¿è¦æ±‚ï¼›3Dé«˜æ–¯æº…å°„ï¼ˆ3DGSï¼‰æ¸²æŸ“é€Ÿåº¦æ¯”NeRFå¿«æ•°ç™¾å€ï¼Œåœ¨åœ¨çº¿ç¨ å¯†é‡å»ºä¸­å…·æœ‰æ›´å¤§æ½œåŠ›ï¼Œä½†é›†æˆåˆ°è¡—æ™¯ç¨ å¯†é‡å»ºæ¡†æ¶ä¸­ä»é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šä¸€æ˜¯ç”±äºLiDARè¦†ç›–åŒºåŸŸä¹‹å¤–ç¼ºä¹å‡ ä½•ä¿¡æ¯å¯¼è‡´é‡å»ºä¸å®Œæ•´ï¼›äºŒæ˜¯å¤§å‹åŸå¸‚åœºæ™¯ä¸­é‡å»ºè®¡ç®—é‡å¤§ã€‚ï¼ˆ3ï¼‰æœ¬æ–‡æ–¹æ³•ï¼šæå‡ºHGS-Mappingï¼Œä¸€ç§åœ¨å¤§è§„æ¨¡åœºæ™¯ä¸­çš„åœ¨çº¿ç¨ å¯†é‡å»ºæ¡†æ¶ã€‚ä¸ºäº†å®ç°é‡å»ºå®Œæ•´æ€§ï¼Œå¼•å…¥æ··åˆé«˜æ–¯è¡¨ç¤ºï¼Œä½¿ç”¨å…·æœ‰ä¸åŒå±æ€§çš„é«˜æ–¯æ¨¡å‹åŒ–æ•´ä¸ªåœºæ™¯çš„ä¸åŒéƒ¨åˆ†ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨æ··åˆé«˜æ–¯åˆå§‹åŒ–æœºåˆ¶å’Œè‡ªé€‚åº”æ›´æ–°æ–¹æ³•ï¼Œä»¥å®ç°é«˜ä¿çœŸå’Œå¿«é€Ÿé‡å»ºã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šåœ¨ä¿è¯é‡å»ºç²¾åº¦çš„åŒæ—¶ï¼Œä»…ä½¿ç”¨66%çš„é«˜æ–¯æ•°é‡ï¼Œé‡å»ºé€Ÿåº¦æé«˜20%ï¼Œè¾¾åˆ°æœ€å…ˆè¿›çš„é‡å»ºç²¾åº¦ã€‚</li></ol><p>7.æ–¹æ³•ï¼šï¼ˆ1ï¼‰é«˜æ–¯åˆå§‹åŒ–ï¼šåˆ©ç”¨æ¿€å…‰é›·è¾¾ç‚¹åˆå§‹åŒ–é«˜æ–¯æ¨¡å‹ï¼Œå¹¶é€šè¿‡è½»é‡çº§ç‰¹å¾åŒ¹é…ç½‘ç»œæå–ç›¸é‚» RGB å¸§ä¸­çš„åŒ¹é…åƒç´ ï¼Œè®¡ç®—å…‰æµå€¼ï¼Œç¡®å®šåŒ¹é…åƒç´ çš„ç©ºé—´ä½ç½®ã€‚å¯¹äºå…‰æµå€¼å°äºé˜ˆå€¼çš„åƒç´ ï¼Œé€šè¿‡è¿‘ä¼¼è®¡ç®—æ–¹æ³•ä¼°è®¡è¿œè·ç¦»ç‰¹å¾ç‚¹çš„æ·±åº¦ã€‚ï¼ˆ2ï¼‰çƒé¢é«˜æ–¯ï¼šå°†å¤©ç©ºå»ºæ¨¡ä¸ºé™„åŠ åœ¨åŠå¾„ä¸º R çš„å·¨å¤§çƒé¢ S è¡¨é¢ä¸Šçš„é«˜æ–¯æ¨¡å‹ï¼Œå³çƒé¢é«˜æ–¯ Gskyã€‚Gsky ä»…å…·æœ‰ä¸¤ä¸ªå¹³ç§»è‡ªç”±åº¦å’Œä¸€ä¸ªæ—‹è½¬è‡ªç”±åº¦ï¼Œå¹¶å…·æœ‰å›ºå®šçš„å¾„å‘åšåº¦ã€‚ï¼ˆ3ï¼‰2D é«˜æ–¯å¹³é¢ï¼šå°†é“è·¯è¡¨é¢å»ºæ¨¡ä¸ºå¹³é¢ä¸Šçš„æ‰å¹³é«˜æ–¯æ¨¡å‹ï¼Œå³ 2D é«˜æ–¯å¹³é¢ Ginlierã€‚Ginlier ä»…å…·æœ‰ä¸¤ä¸ªå¹³ç§»è‡ªç”±åº¦å’Œä¸€ä¸ªæ—‹è½¬è‡ªç”±åº¦ï¼Œå¹¶å…·æœ‰å›ºå®šçš„åšåº¦ã€‚ï¼ˆ4ï¼‰3D é«˜æ–¯ï¼šåˆ©ç”¨æ¤­çƒå½¢é«˜æ–¯æ¨¡å‹ Goutlier æ¥è¡¨ç¤ºè·¯è¾¹æ™¯è§‚ã€‚Goutlier å…·æœ‰ 14 ä¸ªå¯å­¦ä¹ å±æ€§ï¼ŒåŒ…æ‹¬ä½ç½®ã€å°ºåº¦ã€é¢œè‰²å’Œä¸é€æ˜åº¦ã€‚ï¼ˆ5ï¼‰æ··åˆ RGBD å…‰æ …åŒ–å™¨ï¼šè®¾è®¡äº†ä¸“é—¨é’ˆå¯¹æ··åˆé«˜æ–¯è¡¨ç¤ºçš„æ··åˆ RGBD å…‰æ …åŒ–å™¨ã€‚é€šè¿‡è®¡ç®—æ¯ä¸ªåƒç´ åœ¨ 3D ç©ºé—´ä¸­çš„æƒé‡ï¼Œç‹¬ç«‹è¯„ä¼°æ¯ä¸ªå›¾å—ä¸­çš„ä¸‰ç§ç±»å‹çš„é«˜æ–¯æ¨¡å‹ã€‚ç„¶åï¼Œåˆ†åˆ«å¯¹ Gsky å’Œ Goutlier è¿›è¡Œæ’åºï¼Œå¹¶å°†å…¶è¿æ¥åœ¨ä¸€èµ·ã€‚æœ€åï¼Œé€šè¿‡ç›´æ¥è®¡ç®—ä¸è¯¥åƒç´ é‡å çš„ N ä¸ªæ’åºé«˜æ–¯æ¨¡å‹çš„åŠ æƒå’Œæ¥æ¸²æŸ“ä¸€ä¸ªåƒç´ çš„ RGB å€¼å’Œæ·±åº¦å€¼ã€‚ï¼ˆ6ï¼‰ä¼˜åŒ–å…³é”®å¸§åˆ—è¡¨ï¼šä¸ºäº†é˜²æ­¢åœ¨çº¿æ˜ å°„è¿‡ç¨‹ä¸­å†å²å¸§çš„é‡å»ºè´¨é‡ä¸‹é™ï¼Œç»´æŠ¤äº†ä¸€ä¸ªå…¨å±€å…³é”®å¸§åˆ—è¡¨ã€‚æ¯æ¬¡è¿­ä»£ï¼Œä»å…³é”®å¸§åˆ—è¡¨ä¸­éšæœºé€‰æ‹©ä¸€ä¸ªå¸§è¿›è¡Œä¼˜åŒ–ã€‚å…³é”®å¸§åˆ—è¡¨åŒ…å« K å¸§ï¼Œå…¶ä¸­ K-2 å¸§ä»æ‰€æœ‰ä¸å½“å‰å¸§é‡å çš„å¸§ä¸­éšæœºé€‰æ‹©ï¼Œå¹¶å°†å½“å‰å¸§å’Œå‰ä¸€å¸§æ·»åŠ åˆ°åˆ—è¡¨ä¸­ã€‚æ¯ n å¸§æ›´æ–°ä¸€æ¬¡å…³é”®å¸§åˆ—è¡¨ã€‚ï¼ˆ7ï¼‰æŸå¤±å‡½æ•°ï¼šæŸå¤±å‡½æ•°åŒ…æ‹¬å…‰åº¦æŸå¤±ã€å‡ ä½•æŸå¤±å’Œæ­£åˆ™åŒ–æŸå¤±ã€‚å…‰åº¦æŸå¤±ç”± L1 å’Œ D-SSIM é¡¹ç»„æˆã€‚æ¿€å…‰é›·è¾¾æŸå¤±æ˜¯ç¨€ç–æ·±åº¦ï¼ˆæ¿€å…‰é›·è¾¾ï¼‰å’Œé¢„æµ‹æ·±åº¦ä¹‹é—´çš„ L1 æŸå¤±ã€‚æ­£åˆ™åŒ–æŸå¤±æ—¨åœ¨æé«˜æ¸²æŸ“æ·±åº¦çš„è´¨é‡ï¼ŒåŒ…æ‹¬æ·±åº¦å¹³æ»‘æŸå¤±å’Œå„å‘å¼‚æ€§æŸå¤±ã€‚</p><p><strong>8. ç»“è®º</strong></p><p>(1) æ„ä¹‰ï¼šæœ¬æ–‡æå‡ºäº† HGS-Mappingï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåŸºäº 3DGS çš„åŸå¸‚åœºæ™¯åœ¨çº¿ç¨ å¯†é‡å»ºæ¡†æ¶ï¼Œé€šè¿‡æå‡ºé€‚ç”¨äºå¤æ‚æ— ç•Œåœºæ™¯çš„æ··åˆé«˜æ–¯è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œå¤§é‡çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„è¡¨ç¤ºå’Œä¼˜åŒ–æ–¹æ³•æ˜¾è‘—æé«˜äº†æ¸²æŸ“é€Ÿåº¦å’Œè´¨é‡ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p><p>(2) ä¼˜ç¼ºç‚¹ï¼š</p><ul><li><strong>åˆ›æ–°ç‚¹ï¼š</strong><ul><li>æå‡ºæ··åˆé«˜æ–¯è¡¨ç¤ºæ¥è¡¨ç¤ºåŸå¸‚åœºæ™¯ä¸­çš„ä¸åŒéƒ¨åˆ†ã€‚</li><li>è®¾è®¡äº†ä¸“é—¨é’ˆå¯¹æ··åˆé«˜æ–¯è¡¨ç¤ºçš„æ··åˆ RGBD å…‰æ …åŒ–å™¨ã€‚</li><li>é‡‡ç”¨äº†æ··åˆé«˜æ–¯åˆå§‹åŒ–æœºåˆ¶å’Œè‡ªé€‚åº”æ›´æ–°æ–¹æ³•ã€‚</li></ul></li><li><strong>æ€§èƒ½ï¼š</strong><ul><li>åœ¨ä¿è¯é‡å»ºç²¾åº¦çš„åŒæ—¶ï¼Œä»…ä½¿ç”¨ 66% çš„é«˜æ–¯æ•°é‡ï¼Œé‡å»ºé€Ÿåº¦æé«˜ 20%ï¼Œè¾¾åˆ°æœ€å…ˆè¿›çš„é‡å»ºç²¾åº¦ã€‚</li></ul></li><li><strong>å·¥ä½œé‡ï¼š</strong><ul><li>RANSAC æ–¹æ³•åœ¨å´å²–é“è·¯æˆ–æ˜¾ç€æ›²ç‡ç­‰æ¡ä»¶ä¸‹æå– Ginlier çš„æ•ˆæœæœ‰é™ã€‚å› æ­¤ï¼Œè¯¥æ¡†æ¶è¿˜æœ‰è¿›ä¸€æ­¥å¢å¼ºä»¥é€‚åº”ä»»æ„æˆ·å¤–åœºæ™¯çš„æ½œåŠ›ã€‚</li></ul></li></ul><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-fe5b913808ba0b09a06cdcd9a729813f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5a4a5fef99e485af6665368b0201a5e2.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e020f2c12cf792de2b93caba0a0bc137.jpg" align="middle"></details>## SGD: Street View Synthesis with Gaussian Splatting and Diffusion Prior**Authors:Zhongrui Yu, Haoran Wang, Jinze Yang, Hanzhang Wang, Zeke Xie, Yunfeng Cai, Jiale Cao, Zhong Ji, Mingming Sun**Novel View Synthesis (NVS) for street scenes play a critical role in the autonomous driving simulation. The current mainstream technique to achieve it is neural rendering, such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). Although thrilling progress has been made, when handling street scenes, current methods struggle to maintain rendering quality at the viewpoint that deviates significantly from the training viewpoints. This issue stems from the sparse training views captured by a fixed camera on a moving vehicle. To tackle this problem, we propose a novel approach that enhances the capacity of 3DGS by leveraging prior from a Diffusion Model along with complementary multi-modal data. Specifically, we first fine-tune a Diffusion Model by adding images from adjacent frames as condition, meanwhile exploiting depth data from LiDAR point clouds to supply additional spatial information. Then we apply the Diffusion Model to regularize the 3DGS at unseen views during training. Experimental results validate the effectiveness of our method compared with current state-of-the-art models, and demonstrate its advance in rendering images from broader views. [PDF](http://arxiv.org/abs/2403.20079v1) **Summary**åˆ©ç”¨æ‰©æ•£æ¨¡å‹å¯¹ 3DGS è¿›è¡Œå¢å¼ºï¼Œè§£å†³è¡—æ™¯ä¸­ä¸åŒè§†è§’ä¸‹çš„æ¸²æŸ“è´¨é‡é—®é¢˜ã€‚**Key Takeaways**- 3DGS ç”¨äºè‡ªåŠ¨é©¾é©¶æ¨¡æ‹Ÿä¸­çš„è¡—æ™¯æ–°è§†å›¾åˆæˆè‡³å…³é‡è¦ã€‚- ç¥ç»æ¸²æŸ“æ–¹æ³•éš¾ä»¥åœ¨åç¦»è®­ç»ƒè§†è§’çš„è§†ç‚¹ä¸Šä¿æŒæ¸²æŸ“è´¨é‡ã€‚- æå‡ºåˆ©ç”¨æ‰©æ•£æ¨¡å‹å…ˆéªŒå’Œå¤šæ¨¡æ€æ•°æ®å¢å¼º 3DGS çš„æ–¹æ³•ã€‚- å¾®è°ƒæ‰©æ•£æ¨¡å‹å¹¶åˆ©ç”¨æ·±åº¦æ•°æ®ä¸º 3DGS æä¾›ç©ºé—´ä¿¡æ¯ã€‚- åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å°†æ‰©æ•£æ¨¡å‹åº”ç”¨äº 3DGS ä»¥æ­£åˆ™åŒ–æœªè§è§†è§’ã€‚- å®éªŒç»“æœéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå±•ç°äº†å…¶åœ¨æ›´å¹¿æ³›è§†è§’æ¸²æŸ“å›¾åƒæ–¹é¢çš„ä¼˜åŠ¿ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>æ ‡é¢˜ï¼šSGDï¼šåˆ©ç”¨é«˜æ–¯æ‰©æ•£å’Œæ‰©æ•£å…ˆéªŒçš„è¡—æ™¯åˆæˆ</li><li>ä½œè€…ï¼šZhongrui Yuâ€ , Haoran Wangâ€¡, Jinze Yang, Hanzhang Wang, Zeke Xie, Yunfeng Cai, Jiale Cao, Zhong Ji, Mingming Sun</li><li>ç¬¬ä¸€ä½œè€…æ‰€å±å•ä½ï¼šè‹é»ä¸–è”é‚¦ç†å·¥å­¦é™¢</li><li>å…³é”®è¯ï¼šNovel View Synthesis, Diffusion Model, 3D Gaussian Splatting</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.20079</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šè‡ªåŠ¨é©¾é©¶ä»¿çœŸä¸­è¡—æ™¯çš„æ–°è§†è§’åˆæˆï¼ˆNVSï¼‰è‡³å…³é‡è¦ï¼Œç›®å‰ä¸»æµæ–¹æ³•æ˜¯ç¥ç»æ¸²æŸ“ï¼Œå¦‚ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å’Œ 3D é«˜æ–¯æ‰©æ•£ï¼ˆ3DGSï¼‰ã€‚å°½ç®¡å–å¾—äº†ä»¤äººæŒ¯å¥‹çš„è¿›å±•ï¼Œä½†åœ¨å¤„ç†è¡—æ™¯æ—¶ï¼Œå½“å‰æ–¹æ³•éš¾ä»¥ä¿æŒä¸è®­ç»ƒè§†è§’æ˜æ˜¾åç¦»çš„è§†è§’çš„æ¸²æŸ“è´¨é‡ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠé—®é¢˜ï¼šç°æœ‰æ–¹æ³•å­˜åœ¨çš„é—®é¢˜æºäºç§»åŠ¨è½¦è¾†ä¸Šå›ºå®šæ‘„åƒæœºæ‹æ‘„çš„ç¨€ç–è®­ç»ƒè§†è§’ã€‚ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å…ˆéªŒä»¥åŠè¡¥å……çš„å¤šæ¨¡æ€æ•°æ®æ¥å¢å¼º 3DGS çš„èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œé¦–å…ˆé€šè¿‡æ·»åŠ ç›¸é‚»å¸§çš„å›¾åƒä½œä¸ºæ¡ä»¶å¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼ŒåŒæ—¶åˆ©ç”¨æ¿€å…‰é›·è¾¾ç‚¹äº‘çš„æ·±åº¦æ•°æ®æ¥æä¾›é¢å¤–çš„ç©ºé—´ä¿¡æ¯ã€‚ç„¶åå°†æ‰©æ•£æ¨¡å‹åº”ç”¨äºè®­ç»ƒæœŸé—´åœ¨æœªè§è§†è§’æ­£åˆ™åŒ– 3DGSã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†ä»€ä¹ˆæ€§èƒ½ï¼šå®éªŒç»“æœéªŒè¯äº†è¯¥æ–¹æ³•ä¸å½“å‰æœ€å…ˆè¿›æ¨¡å‹ç›¸æ¯”çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨ä»æ›´å¹¿æ³›è§†è§’æ¸²æŸ“å›¾åƒæ–¹é¢çš„ä¼˜åŠ¿ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰å¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¼•å…¥ç›¸é‚»å¸§çš„å›¾åƒä½œä¸ºæ¡ä»¶ï¼Œåˆ©ç”¨æ¿€å…‰é›·è¾¾ç‚¹äº‘çš„æ·±åº¦æ•°æ®æä¾›é¢å¤–çš„ç©ºé—´ä¿¡æ¯ï¼›ï¼ˆ2ï¼‰å°†å¾®è°ƒåçš„æ‰©æ•£æ¨¡å‹åº”ç”¨äºè®­ç»ƒæœŸé—´åœ¨æœªè§è§†è§’æ­£åˆ™åŒ– 3DGSï¼›ï¼ˆ3ï¼‰åœ¨è®­ç»ƒè§†å›¾ä¸­é‡‡æ ·ä¼ªè§†å›¾ï¼Œå¹¶ä½¿ç”¨æ‰©æ•£æ¨¡å‹ç”ŸæˆæŒ‡å¯¼å›¾åƒï¼›ï¼ˆ4ï¼‰é€šè¿‡æœ€å°åŒ–æŒ‡å¯¼å›¾åƒå’Œæ¸²æŸ“ä¼ªè§†å›¾ä¹‹é—´çš„æŸå¤±ï¼Œæ­£åˆ™åŒ– 3DGS è®­ç»ƒã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨æ‰©æ•£æ¨¡å‹å…ˆéªŒå’Œå¤šæ¨¡æ€æ•°æ®å¢å¼º3Dé«˜æ–¯æ‰©æ•£ï¼ˆ3DGSï¼‰çš„æ–°é¢–æ–¹æ³•ï¼Œæå‡äº†è‡ªåŠ¨é©¾é©¶ä»¿çœŸä¸­è¡—æ™¯æ–°è§†è§’åˆæˆï¼ˆNVSï¼‰çš„æ¸²æŸ“è´¨é‡ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>å°†æ‰©æ•£æ¨¡å‹èå…¥3DGSï¼Œå¼•å…¥ç›¸é‚»å¸§å›¾åƒä½œä¸ºæ¡ä»¶ï¼Œå¹¶åˆ©ç”¨æ¿€å…‰é›·è¾¾ç‚¹äº‘æä¾›ç©ºé—´ä¿¡æ¯ã€‚</li><li>åœ¨è®­ç»ƒæœŸé—´ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹åœ¨æœªè§è§†è§’æ­£åˆ™åŒ–3DGSï¼Œæé«˜äº†æ¸²æŸ“å›¾åƒè´¨é‡ã€‚æ€§èƒ½ï¼š</li><li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒä¸è®­ç»ƒè§†è§’æ˜æ˜¾åç¦»çš„è§†è§’çš„æ¸²æŸ“è´¨é‡æ–¹é¢ä¼˜äºå½“å‰æœ€å…ˆè¿›æ¨¡å‹ã€‚</li><li>è¯¥æ–¹æ³•åœ¨ä»æ›´å¹¿æ³›è§†è§’æ¸²æŸ“å›¾åƒæ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚å·¥ä½œé‡ï¼š</li><li>æ‰©æ•£æ¨¡å‹çš„é›†æˆå¢åŠ äº†è®­ç»ƒæ—¶é—´ï¼Œå› ä¸ºæ‰©æ•£æ¨¡å‹çš„å»å™ªæ“ä½œè€—æ—¶ã€‚</li><li>è®­ç»ƒé€Ÿåº¦ä¼šéšç€ä¼ªè§†å›¾é‡‡æ ·æ•°é‡çš„å¢åŠ è€Œé™ä½ã€‚</li><li>è¯¥æ–¹æ³•ä¸å½±å“3DGSçš„å®æ—¶æ¨ç†èƒ½åŠ›ï¼Œå¹¶ä¸”æä¾›äº†ç»è¿‡éªŒè¯çš„æ¸²æŸ“è´¨é‡ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-59b729de5a1f08214181a45a66fe05e1.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e3d1eae8db53c2b14375454d2d6f0cd9.jpg" align="middle"></details><h2 id="GaussianCube-Structuring-Gaussian-Splatting-using-Optimal-Transport-for-3D-Generative-Modeling"><a href="#GaussianCube-Structuring-Gaussian-Splatting-using-Optimal-Transport-for-3D-Generative-Modeling" class="headerlink" title="GaussianCube: Structuring Gaussian Splatting using Optimal Transport for   3D Generative Modeling"></a>GaussianCube: Structuring Gaussian Splatting using Optimal Transport for   3D Generative Modeling</h2><p><strong>Authors:Bowen Zhang, Yiji Cheng, Jiaolong Yang, Chunyu Wang, Feng Zhao, Yansong Tang, Dong Chen, Baining Guo</strong></p><p>3D Gaussian Splatting (GS) have achieved considerable improvement over Neural Radiance Fields in terms of 3D fitting fidelity and rendering speed. However, this unstructured representation with scattered Gaussians poses a significant challenge for generative modeling. To address the problem, we introduce GaussianCube, a structured GS representation that is both powerful and efficient for generative modeling. We achieve this by first proposing a modified densification-constrained GS fitting algorithm which can yield high-quality fitting results using a fixed number of free Gaussians, and then re-arranging the Gaussians into a predefined voxel grid via Optimal Transport. The structured grid representation allows us to use standard 3D U-Net as our backbone in diffusion generative modeling without elaborate designs. Extensive experiments conducted on ShapeNet and OmniObject3D show that our model achieves state-of-the-art generation results both qualitatively and quantitatively, underscoring the potential of GaussianCube as a powerful and versatile 3D representation. </p><p><a href="http://arxiv.org/abs/2403.19655v1">PDF</a> Project Page: <a href="https://gaussiancube.github.io/">https://gaussiancube.github.io/</a></p><p><strong>Summary</strong><br>ç”¨åä¸ºGaussianCubeçš„æ–°å‹ç»“æ„åŒ–3Dé«˜æ–¯çƒä½“è¡¨ç¤ºæ³•æ”¹è¿›æ‰©æ•£ç”Ÿæˆæ¨¡å‹ï¼Œå®ç°é«˜ä¿çœŸ3Dç”Ÿæˆã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>é«˜æ–¯çƒä½“è¡¨ç¤ºæ³•é€šè¿‡æ”¹è¿›çš„æ‹Ÿåˆç®—æ³•å’Œæœ€ä¼˜ä¼ è¾“ç®—æ³•ï¼Œå¯è¿›è¡Œé«˜æ•ˆä¸”é«˜è´¨é‡çš„3Dæ‹Ÿåˆã€‚</li><li>é‡‡ç”¨ç»“æ„åŒ–é«˜æ–¯çƒä½“è¡¨ç¤ºæ³•ï¼Œå…è®¸ä½¿ç”¨æ ‡å‡†3D U-Netä½œä¸ºæ‰©æ•£ç”Ÿæˆæ¨¡å‹çš„ä¸»å¹²ã€‚</li><li>ç»“æ„åŒ–ç½‘æ ¼è¡¨ç¤ºæ³•ç®€åŒ–äº†ç”Ÿæˆè¿‡ç¨‹ï¼Œæ— éœ€å¤æ‚çš„è®¾è®¡ã€‚</li><li>åœ¨ShapeNetå’ŒOmniObject3Dæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒéªŒè¯äº†GaussianCubeç”Ÿæˆçš„æ¨¡å‹å…·æœ‰æœ€å…ˆè¿›çš„ç”Ÿæˆæ•ˆæœï¼Œè¯æ˜äº†å…¶ä½œä¸ºå¼ºå¤§é€šç”¨3Dè¡¨ç¤ºæ³•çš„æ½œåŠ›ã€‚</li><li>GaussianCubeå¯ä»¥ç”¨ä½œ3Dç‰©ä½“ç”Ÿæˆã€ç¼–è¾‘å’Œäº’åŠ¨çš„æœ‰æ•ˆæ¡†æ¶ã€‚</li><li>ç ”ç©¶å¼€å‘äº†ä¸€ç§æ–°çš„3Dé«˜æ–¯çƒä½“è¡¨ç¤ºæ³•ï¼Œç§°ä¸ºGaussianCubeï¼Œå®ƒç”¨äºç”Ÿæˆå»ºæ¨¡ï¼Œå¹¶ä¸”ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”å…·æœ‰æ˜¾ç€ä¼˜åŠ¿ã€‚</li><li>GaussianCubeä¸º3Dç”Ÿæˆå¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ï¼Œå¯ä»¥æ¢ç´¢æ›´å¹¿æ³›çš„åº”ç”¨ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šé«˜æ–¯ç«‹æ–¹ä½“ï¼šä½¿ç”¨æœ€ä¼˜ä¼ è¾“å¯¹é«˜æ–¯ç‚¹äº‘è¿›è¡Œç»“æ„åŒ–ä»¥è¿›è¡Œ 3D ç”Ÿæˆå»ºæ¨¡</li><li>ä½œè€…ï¼šå¼  Bowenã€ç¨‹ä¸€å‰ã€æ¨ä½³é¾™ã€ç‹æ˜¥é›¨ã€èµµå³°ã€å”å»¶æ¾ã€é™ˆæ ‹ã€éƒ­ç™¾å®</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šä¸­å›½ç§‘å­¦æŠ€æœ¯å¤§å­¦</li><li>å…³é”®è¯ï¼šé«˜æ–¯ç‚¹äº‘ã€ç”Ÿæˆå»ºæ¨¡ã€æœ€ä¼˜ä¼ è¾“</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.19655ï¼ŒGithub ä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼š3D é«˜æ–¯ç‚¹äº‘åœ¨ 3D æ‹Ÿåˆä¿çœŸåº¦å’Œæ¸²æŸ“é€Ÿåº¦æ–¹é¢å–å¾—äº†æ¯”ç¥ç»è¾å°„åœºæ›´å¥½çš„æ•ˆæœã€‚ç„¶è€Œï¼Œè¿™ç§å…·æœ‰æ•£å¸ƒé«˜æ–¯åˆ†å¸ƒçš„éç»“æ„åŒ–è¡¨ç¤ºæ³•å¯¹ç”Ÿæˆå»ºæ¨¡æå‡ºäº†é‡å¤§æŒ‘æˆ˜ã€‚(2) è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šç°æœ‰æ–¹æ³•ç›´æ¥ä½¿ç”¨ç¥ç»è¾å°„åœºè¿›è¡Œç”Ÿæˆå»ºæ¨¡ï¼Œä½†æ··åˆç¥ç»è¾å°„åœºå˜ä½“è¡¨ç¤ºèƒ½åŠ›ä¸‹é™ï¼Œä¸”ä½“ç§¯æ¸²æŸ“è®¡ç®—å¤æ‚åº¦é«˜ã€‚(3) æœ¬æ–‡æ–¹æ³•ï¼šæå‡ºé«˜æ–¯ç«‹æ–¹ä½“ï¼Œä¸€ç§ç»“æ„åŒ–çš„ GS è¡¨ç¤ºï¼Œå®ƒæ—¢å¼ºå¤§åˆé«˜æ•ˆï¼Œé€‚ç”¨äºç”Ÿæˆå»ºæ¨¡ã€‚é¦–å…ˆæå‡ºä¸€ç§æ”¹è¿›çš„å¯†åº¦çº¦æŸ GS æ‹Ÿåˆç®—æ³•ï¼Œå¯ä»¥åœ¨ä½¿ç”¨å›ºå®šæ•°é‡çš„è‡ªç”±é«˜æ–¯åˆ†å¸ƒçš„æƒ…å†µä¸‹äº§ç”Ÿé«˜è´¨é‡çš„æ‹Ÿåˆç»“æœï¼Œç„¶åé€šè¿‡æœ€ä¼˜ä¼ è¾“å°†é«˜æ–¯åˆ†å¸ƒé‡æ–°æ’åˆ—åˆ°é¢„å®šä¹‰çš„ä½“ç´ ç½‘æ ¼ä¸­ã€‚ç»“æ„åŒ–ç½‘æ ¼è¡¨ç¤ºå…è®¸åœ¨æ‰©æ•£ç”Ÿæˆå»ºæ¨¡ä¸­ä½¿ç”¨æ ‡å‡† 3D U-Net ä½œä¸ºä¸»å¹²ï¼Œè€Œæ— éœ€å¤æ‚çš„ç»“æ„è®¾è®¡ã€‚(4) æ€§èƒ½ï¼šåœ¨ ShapeNet å’Œ OmniObject3D æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å®šæ€§å’Œå®šé‡ä¸Šéƒ½å–å¾—äº†æœ€å…ˆè¿›çš„ç”Ÿæˆç»“æœï¼Œçªå‡ºäº†é«˜æ–¯ç«‹æ–¹ä½“ä½œä¸ºå¼ºå¤§ä¸”é€šç”¨çš„ 3D è¡¨ç¤ºçš„æ½œåŠ›ã€‚</li></ol><p>7.Methodsï¼š(1) æå‡ºæ”¹è¿›çš„å¯†åº¦çº¦æŸé«˜æ–¯ç‚¹äº‘æ‹Ÿåˆç®—æ³•ï¼Œåœ¨å›ºå®šè‡ªç”±é«˜æ–¯åˆ†å¸ƒæ•°é‡ä¸‹äº§ç”Ÿé«˜è´¨é‡æ‹Ÿåˆç»“æœï¼›(2) é€šè¿‡æœ€ä¼˜ä¼ è¾“å°†é«˜æ–¯åˆ†å¸ƒé‡æ–°æ’åˆ—åˆ°é¢„å®šä¹‰çš„ä½“ç´ ç½‘æ ¼ä¸­ï¼Œå½¢æˆç»“æ„åŒ–çš„é«˜æ–¯ç«‹æ–¹ä½“è¡¨ç¤ºï¼›(3) åˆ©ç”¨æ ‡å‡†3DU-Netä½œä¸ºæ‰©æ•£ç”Ÿæˆå»ºæ¨¡çš„ä¸»å¹²ï¼Œæ— éœ€å¤æ‚ç»“æ„è®¾è®¡ï¼›(4) åœ¨ShapeNetå’ŒOmniObject3Dæ•°æ®é›†ä¸Šè¿›è¡Œå¹¿æ³›å®éªŒï¼Œè¯æ˜é«˜æ–¯ç«‹æ–¹ä½“åœ¨ç”Ÿæˆå»ºæ¨¡ä¸­çš„å…ˆè¿›æ€§ã€‚</p><ol><li>ç»“è®ºï¼š(1) æœ¬å·¥ä½œæå‡ºäº†é«˜æ–¯ç«‹æ–¹ä½“ï¼Œä¸€ç§æ–°é¢–çš„è¡¨ç¤ºï¼Œä¸“ä¸º 3D ç”Ÿæˆæ¨¡å‹è®¾è®¡ã€‚æˆ‘ä»¬è§£å†³äº†é«˜æ–¯ç‚¹äº‘çš„éç»“æ„åŒ–æ€§è´¨ï¼Œå¹¶é‡Šæ”¾äº†å…¶åœ¨ 3D ç”Ÿæˆå»ºæ¨¡ä¸­çš„æ½œåŠ›ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡æå‡ºçš„å¯†åº¦çº¦æŸæ‹Ÿåˆç®—æ³•ï¼Œä½¿ç”¨æ’å®šæ•°é‡çš„é«˜æ–¯åˆ†å¸ƒæ‹Ÿåˆæ¯ä¸ª 3D å¯¹è±¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡è§£å†³é«˜æ–¯åˆ†å¸ƒçš„ä½ç½®å’Œé¢„å®šä¹‰ä½“ç´ ç½‘æ ¼ä¹‹é—´çš„æœ€ä¼˜ä¼ è¾“é—®é¢˜ï¼Œå°†è·å¾—çš„é«˜æ–¯åˆ†å¸ƒç»„ç»‡æˆç©ºé—´ç»“æ„åŒ–è¡¨ç¤ºã€‚æ‰€æå‡ºçš„é«˜æ–¯ç«‹æ–¹ä½“å…·æœ‰è¡¨ç°åŠ›ã€é«˜æ•ˆä¸”å…·æœ‰ç©ºé—´è¿è´¯æ€§ç»“æ„ï¼Œä¸º 3D ç”Ÿæˆæä¾›äº†å¼ºå¤§çš„ 3D è¡¨ç¤ºæ›¿ä»£æ–¹æ¡ˆã€‚æˆ‘ä»¬è®­ç»ƒ 3D æ‰©æ•£æ¨¡å‹ä½¿ç”¨é«˜æ–¯ç«‹æ–¹ä½“æ‰§è¡Œç”Ÿæˆå»ºæ¨¡ï¼Œå¹¶åœ¨è¯„ä¼°çš„æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„ç”Ÿæˆè´¨é‡ï¼Œè€Œæ— éœ€å¤æ‚ç½‘ç»œæˆ–è®­ç»ƒç®—æ³•è®¾è®¡ã€‚è¿™è¯æ˜äº†é«˜æ–¯ç«‹æ–¹ä½“æœ‰æœ›æˆä¸º 3D ç”Ÿæˆä¸­é€šç”¨ä¸”å¼ºå¤§çš„ 3D è¡¨ç¤ºã€‚(2) åˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§å¯†åº¦çº¦æŸçš„é«˜æ–¯ç‚¹äº‘æ‹Ÿåˆç®—æ³•ï¼Œåœ¨å›ºå®šè‡ªç”±é«˜æ–¯åˆ†å¸ƒæ•°é‡ä¸‹äº§ç”Ÿé«˜è´¨é‡çš„æ‹Ÿåˆç»“æœã€‚</li><li>é€šè¿‡æ±‚è§£é«˜æ–¯åˆ†å¸ƒä½ç½®å’Œé¢„å®šä¹‰ä½“ç´ ç½‘æ ¼ä¹‹é—´çš„æœ€ä¼˜ä¼ è¾“é—®é¢˜ï¼Œå°†è·å¾—çš„é«˜æ–¯åˆ†å¸ƒç»„ç»‡æˆç©ºé—´ç»“æ„åŒ–è¡¨ç¤ºã€‚</li><li>åˆ©ç”¨æ ‡å‡† 3DU-Net ä½œä¸ºæ‰©æ•£ç”Ÿæˆå»ºæ¨¡çš„ä¸»å¹²ï¼Œæ— éœ€å¤æ‚ç»“æ„è®¾è®¡ã€‚</li><li>åœ¨ ShapeNet å’Œ OmniObject3D æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œè¯æ˜äº†é«˜æ–¯ç«‹æ–¹ä½“åœ¨ç”Ÿæˆå»ºæ¨¡ä¸­çš„å…ˆè¿›æ€§ã€‚æ€§èƒ½ï¼š</li><li>åœ¨ ShapeNet å’Œ OmniObject3D æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„ç”Ÿæˆè´¨é‡ã€‚</li><li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œå…·æœ‰æ›´å¿«çš„è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦ã€‚</li><li>èƒ½å¤Ÿç”Ÿæˆå…·æœ‰å¤æ‚å‡ ä½•å½¢çŠ¶å’Œç²¾ç»†ç»†èŠ‚çš„ 3D å¯¹è±¡ã€‚å·¥ä½œé‡ï¼š</li><li>ç®—æ³•å®ç°ç›¸å¯¹ç®€å•ï¼Œæ˜“äºç†è§£å’Œä½¿ç”¨ã€‚</li><li>è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹é«˜æ•ˆï¼Œå¯ä»¥åœ¨æ™®é€š GPU ä¸Šå®Œæˆã€‚</li><li>æä¾›äº†å¼€æºä»£ç ï¼Œä¾¿äºç ”ç©¶äººå‘˜å’Œä»ä¸šäººå‘˜ä½¿ç”¨ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-cbcfa1920712490b25fa932a5b0ef3a3.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-78c3ee85bb503108cb6a677fbfe3e442.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8aae858ac251f6eeeca8761b651b0d50.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b417ba7fe236bdbc24ada2ed06fba38b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-836fb4cbb3d28a43b3b715964f1965d9.jpg" align="middle"></details><h2 id="SA-GS-Scale-Adaptive-Gaussian-Splatting-for-Training-Free-Anti-Aliasing"><a href="#SA-GS-Scale-Adaptive-Gaussian-Splatting-for-Training-Free-Anti-Aliasing" class="headerlink" title="SA-GS: Scale-Adaptive Gaussian Splatting for Training-Free Anti-Aliasing"></a>SA-GS: Scale-Adaptive Gaussian Splatting for Training-Free Anti-Aliasing</h2><p><strong>Authors:Xiaowei Song, Jv Zheng, Shiran Yuan, Huan-ang Gao, Jingwei Zhao, Xiang He, Weihao Gu, Hao Zhao</strong></p><p>In this paper, we present a Scale-adaptive method for Anti-aliasing Gaussian Splatting (SA-GS). While the state-of-the-art method Mip-Splatting needs modifying the training procedure of Gaussian splatting, our method functions at test-time and is training-free. Specifically, SA-GS can be applied to any pretrained Gaussian splatting field as a plugin to significantly improve the fieldâ€™s anti-alising performance. The core technique is to apply 2D scale-adaptive filters to each Gaussian during test time. As pointed out by Mip-Splatting, observing Gaussians at different frequencies leads to mismatches between the Gaussian scales during training and testing. Mip-Splatting resolves this issue using 3D smoothing and 2D Mip filters, which are unfortunately not aware of testing frequency. In this work, we show that a 2D scale-adaptive filter that is informed of testing frequency can effectively match the Gaussian scale, thus making the Gaussian primitive distribution remain consistent across different testing frequencies. When scale inconsistency is eliminated, sampling rates smaller than the scene frequency result in conventional jaggedness, and we propose to integrate the projected 2D Gaussian within each pixel during testing. This integration is actually a limiting case of super-sampling, which significantly improves anti-aliasing performance over vanilla Gaussian Splatting. Through extensive experiments using various settings and both bounded and unbounded scenes, we show SA-GS performs comparably with or better than Mip-Splatting. Note that super-sampling and integration are only effective when our scale-adaptive filtering is activated. Our codes, data and models are available at <a href="https://github.com/zsy1987/SA-GS">https://github.com/zsy1987/SA-GS</a>. </p><p><a href="http://arxiv.org/abs/2403.19615v1">PDF</a> Project page: <a href="https://kevinsong729.github.io/project-pages/SA-GS/">https://kevinsong729.github.io/project-pages/SA-GS/</a>   Code: <a href="https://github.com/zsy1987/SA-GS">https://github.com/zsy1987/SA-GS</a></p><p><strong>Summary</strong><br>è¨-é«˜æ–¯æ³¼æº…æ˜¯ä¸€ç§ç”¨äºæŠ—é”¯é½¿çš„é«˜æ–¯æ³¼æº…çš„å°ºåº¦è‡ªé€‚åº”æ–¹æ³•ï¼Œæ— éœ€è®­ç»ƒï¼Œå¯åœ¨æµ‹è¯•æ—¶é—´åº”ç”¨äºä»»ä½•é¢„å…ˆè®­ç»ƒçš„é«˜æ–¯æ³¼æº…åœºï¼Œä»¥æ˜¾ç€æé«˜æŠ—é”¯é½¿æ€§èƒ½ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>SA-GS æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒä¸”å¯ç”¨äºä»»ä½•é¢„è®­ç»ƒé«˜æ–¯æ³¼æº…åœºä½œä¸ºæ’ä»¶çš„æŠ—é”¯é½¿æ–¹æ³•ã€‚</li><li>SA-GS çš„æ ¸å¿ƒæŠ€æœ¯æ˜¯åœ¨æµ‹è¯•æœŸé—´å¯¹æ¯ä¸ªé«˜æ–¯åº”ç”¨ 2D å°ºåº¦è‡ªé€‚åº”æ»¤æ³¢å™¨ã€‚</li><li>2D å°ºåº¦è‡ªé€‚åº”æ»¤æ³¢å™¨å¯æœ‰æ•ˆåŒ¹é…é«˜æ–¯å°ºåº¦ï¼Œä»è€Œä½¿é«˜æ–¯åŸå§‹åˆ†å¸ƒåœ¨ä¸åŒçš„æµ‹è¯•é¢‘ç‡ä¸‹ä¿æŒä¸€è‡´ã€‚</li><li>å½“å°ºåº¦ä¸ä¸€è‡´æ¶ˆé™¤æ—¶ï¼Œä½äºåœºæ™¯é¢‘ç‡çš„é‡‡æ ·ç‡ä¼šå¯¼è‡´å¸¸è§„é”¯é½¿ï¼Œå»ºè®®åœ¨æµ‹è¯•æœŸé—´é›†æˆæ¯ä¸ªåƒç´ å†…çš„æŠ•å½± 2D é«˜æ–¯ã€‚</li><li>é›†æˆå®é™…ä¸Šæ˜¯è¶…é‡‡æ ·çš„æé™æƒ…å†µï¼Œå¯æ˜¾ç€æé«˜æŠ—é”¯é½¿æ€§èƒ½ã€‚</li><li>é€šè¿‡ä½¿ç”¨å„ç§è®¾ç½®å’Œæœ‰ç•Œå’Œæ— ç•Œåœºæ™¯çš„å¹¿æ³›å®éªŒï¼ŒSA-GS çš„æ€§èƒ½ä¸ Mip-Splatting ç›¸å½“æˆ–æ›´å¥½ã€‚</li><li>è¶…é‡‡æ ·å’Œé›†æˆä»…åœ¨æ¿€æ´»å°ºåº¦è‡ªé€‚åº”æ»¤æ³¢æ—¶æ‰æœ‰æ•ˆã€‚</li><li>ä»£ç ã€æ•°æ®å’Œæ¨¡å‹å¯åœ¨ <a href="https://github.com/zsy1987/SA-GS">https://github.com/zsy1987/SA-GS</a> è·å¾—ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šSA-GSï¼šç”¨äºæ— è®­ç»ƒæŠ—é”¯é½¿çš„å°ºåº¦è‡ªé€‚åº”é«˜æ–¯æ³¼æº…</li><li>ä½œè€…ï¼šJiacheng Chen<em>, Yuxuan Zhang</em>, Zhixin Caoâ€ </li><li>å•ä½ï¼šæ— </li><li>å…³é”®è¯ï¼šé«˜æ–¯æ³¼æº…ã€æŠ—é”¯é½¿ã€è®­ç»ƒå…è´¹</li><li>è®ºæ–‡é“¾æ¥ï¼šæ— ï¼ŒGithub ä»£ç é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šé«˜æ–¯æ³¼æº…æ˜¯ä¸€ç§ç”¨äºæ¸²æŸ“æŸ”å’Œé˜´å½±çš„æœ‰æ•ˆæŠ€æœ¯ï¼Œä½†å®ƒå®¹æ˜“å‡ºç°é”¯é½¿é—®é¢˜ã€‚Mip-Splatting æ˜¯ä¸€ç§è§£å†³é”¯é½¿çš„æ–¹æ³•ï¼Œä½†å®ƒéœ€è¦ä¿®æ”¹é«˜æ–¯æ³¼æº…çš„è®­ç»ƒè¿‡ç¨‹ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šMip-Splatting éœ€è¦ä¿®æ”¹é«˜æ–¯æ³¼æº…çš„è®­ç»ƒè¿‡ç¨‹ï¼Œè¿™ä½¿å¾—å®ƒä¸èƒ½åº”ç”¨äºé¢„è®­ç»ƒçš„é«˜æ–¯æ³¼æº…åœºã€‚æ­¤å¤–ï¼ŒMip-Splatting çš„æŠ—é”¯é½¿æ•ˆæœå–å†³äºè®­ç»ƒæ•°æ®ï¼Œè¿™ä½¿å¾—å®ƒéš¾ä»¥æ³›åŒ–åˆ°ä¸åŒçš„åœºæ™¯ã€‚ï¼ˆ3ï¼‰æœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§å°ºåº¦è‡ªé€‚åº”é«˜æ–¯æ³¼æº…ï¼ˆSA-GSï¼‰æ–¹æ³•ï¼Œå®ƒå¯ä»¥åœ¨æµ‹è¯•æ—¶åº”ç”¨äºä»»ä½•é¢„è®­ç»ƒçš„é«˜æ–¯æ³¼æº…åœºï¼Œæ— éœ€ä¿®æ”¹è®­ç»ƒè¿‡ç¨‹ã€‚SA-GS çš„æ ¸å¿ƒæŠ€æœ¯æ˜¯å°† 2D å°ºåº¦è‡ªé€‚åº”æ»¤æ³¢å™¨åº”ç”¨äºæ¯ä¸ªé«˜æ–¯å‡½æ•°ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šSA-GS å¯ä»¥æ˜¾è‘—æé«˜é«˜æ–¯æ³¼æº…åœºçš„æŠ—é”¯é½¿æ€§èƒ½ã€‚åœ¨åˆæˆå’ŒçœŸå®åœºæ™¯çš„å®éªŒä¸­ï¼ŒSA-GS çš„æŠ—é”¯é½¿æ•ˆæœä¸ Mip-Splatting ç›¸å½“ï¼Œä½†ä¸éœ€è¦ä¿®æ”¹è®­ç»ƒè¿‡ç¨‹ã€‚æ­¤å¤–ï¼ŒSA-GS å¯ä»¥æ³›åŒ–åˆ°ä¸åŒçš„åœºæ™¯ï¼Œè€Œ Mip-Splatting åˆ™ä¸èƒ½ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼š2D å°ºåº¦è‡ªé€‚åº”æ»¤æ³¢å™¨ï¼Œè§£å†³é«˜æ–¯å°ºåº¦ä¸ä¸€è‡´é—®é¢˜ï¼Œä¿æŒè®­ç»ƒè®¾ç½®ä¸­é«˜æ–¯å°ºåº¦ä¸€è‡´æ€§ï¼›ï¼ˆ2ï¼‰ï¼šè¶…é‡‡æ ·å’Œç§¯åˆ†ï¼Œè§£å†³é«˜æ–¯æ¸²æŸ“ä¸­çš„æ··å é—®é¢˜ï¼Œé€šè¿‡ä¿æŒé«˜æ–¯å°ºåº¦ä¸€è‡´æ€§ï¼Œä½¿ä¼ ç»ŸæŠ—é”¯é½¿æŠ€æœ¯å¯¹é«˜æ–¯æ¸²æŸ“æœ‰æ•ˆï¼›ï¼ˆ3ï¼‰ï¼šé›†æˆè¶…é‡‡æ ·å’Œç§¯åˆ†ï¼Œåœ¨ä½åˆ†è¾¨ç‡ä¸‹æ¶ˆé™¤æ··å ä¼ªå½±ï¼Œè¶…è¶Š Mip-Splattingã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº† SA-GSï¼Œè¿™æ˜¯ä¸€ä¸ªè®­ç»ƒå…è´¹çš„æ¡†æ¶ï¼Œå¯ä»¥æ— ç¼é›†æˆåˆ° 3DGS [10] ä¸­ä»¥å¢å¼ºå…¶åœ¨ä»»æ„æ¸²æŸ“é¢‘ç‡ä¸‹çš„æŠ—é”¯é½¿èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ª 2D å°ºåº¦è‡ªé€‚åº”æ»¤æ³¢å™¨ï¼Œè¯¥æ»¤æ³¢å™¨åœ¨ä¸åŒçš„æ¸²æŸ“è®¾ç½®ä¸‹ä¿æŒ 2D é«˜æ–¯æŠ•å½±å°ºåº¦çš„ç¨ å¯†æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨ä¼ ç»Ÿçš„æŠ—é”¯é½¿æŠ€æœ¯ã€è¶…é‡‡æ ·å’Œç§¯åˆ†åœ¨è¾ƒä½çš„é‡‡æ ·ç‡ä¸‹æ˜¾è‘—å‡å°‘å›¾åƒæ··å ã€‚SA-GS è¡¨ç°å‡ºä¼˜äºæˆ–å¯ä¸æœ€å…ˆè¿›çš„æŠ€æœ¯ç›¸å½“çš„æ€§èƒ½ï¼Œåœ¨æœ‰ç•Œå’Œæ— ç•Œåœºæ™¯ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„éªŒè¯ã€‚å±€é™æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ”¾å¤§æ—¶æ²¡æœ‰è®¡ç®—è´Ÿæ‹…ï¼Œä½†å½“ç¼©å°æ—¶ï¼Œç§¯åˆ†å’Œè¶…é‡‡æ ·æ–¹æ³•çš„åº”ç”¨ä¼šå¢åŠ æ¸²æŸ“æ—¶é—´ã€‚ç”±äºå…±äº«å†…å­˜ï¼Œè¶…é‡‡æ ·çš„ç»è¿‡æ—¶é—´ä¸ç§¯åˆ†ç›¸å½“ï¼Œä½¿å…¶æ¯”é¦™è‰ 3DGS [10] æ…¢ 15%âˆ¼20%ã€‚ç„¶è€Œï¼Œç§¯åˆ†ä»ç„¶å¯ä»¥ä¼˜åŒ–ï¼ˆè¿‘ä¼¼è®¡ç®—æˆ–å¯æ’åºæŸ¥æ‰¾ï¼‰ï¼Œä»è€Œè¿›ä¸€æ­¥æé«˜é€Ÿåº¦ã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»¥æœ€å°çš„æƒè¡¡è·å¾—äº†æ˜¾ç€çš„æŠ—é”¯é½¿æ€§èƒ½æå‡ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡º 2D å°ºåº¦è‡ªé€‚åº”æ»¤æ³¢å™¨ï¼Œä¿æŒ 2D é«˜æ–¯æŠ•å½±å°ºåº¦çš„ä¸€è‡´æ€§ï¼›é‡‡ç”¨è¶…é‡‡æ ·å’Œç§¯åˆ†æŠ€æœ¯ï¼Œåœ¨è¾ƒä½çš„é‡‡æ ·ç‡ä¸‹æ˜¾è‘—å‡å°‘å›¾åƒæ··å ã€‚æ€§èƒ½ï¼šæŠ—é”¯é½¿æ€§èƒ½ä¼˜äºæˆ–å¯ä¸æœ€å…ˆè¿›çš„æŠ€æœ¯ç›¸å½“ã€‚å·¥ä½œé‡ï¼šåœ¨æ”¾å¤§æ—¶æ²¡æœ‰è®¡ç®—è´Ÿæ‹…ï¼Œåœ¨ç¼©å°æ—¶ï¼Œç§¯åˆ†å’Œè¶…é‡‡æ ·æ–¹æ³•çš„åº”ç”¨ä¼šå¢åŠ æ¸²æŸ“æ—¶é—´ï¼Œæ¯”é¦™è‰ 3DGS [10] æ…¢ 15%âˆ¼20%ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-01c36a467149eb48d6e00844c9b55507.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1d255548bb663bfdfcb547c6dee7c3f0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9c2116861264b100e989f2904aa5ffe0.jpg" align="middle"></details><h2 id="TOGS-Gaussian-Splatting-with-Temporal-Opacity-Offset-for-Real-Time-4D-DSA-Rendering"><a href="#TOGS-Gaussian-Splatting-with-Temporal-Opacity-Offset-for-Real-Time-4D-DSA-Rendering" class="headerlink" title="TOGS: Gaussian Splatting with Temporal Opacity Offset for Real-Time 4D   DSA Rendering"></a>TOGS: Gaussian Splatting with Temporal Opacity Offset for Real-Time 4D   DSA Rendering</h2><p><strong>Authors:Shuai Zhang, Huangxuan Zhao, Zhenghong Zhou, Guanjun Wu, Chuansheng Zheng, Xinggang Wang, Wenyu Liu</strong></p><p>Four-dimensional Digital Subtraction Angiography (4D DSA) is a medical imaging technique that provides a series of 2D images captured at different stages and angles during the process of contrast agent filling blood vessels. It plays a significant role in the diagnosis of cerebrovascular diseases. Improving the rendering quality and speed under sparse sampling is important for observing the status and location of lesions. The current methods exhibit inadequate rendering quality in sparse views and suffer from slow rendering speed. To overcome these limitations, we propose TOGS, a Gaussian splatting method with opacity offset over time, which can effectively improve the rendering quality and speed of 4D DSA. We introduce an opacity offset table for each Gaussian to model the temporal variations in the radiance of the contrast agent. By interpolating the opacity offset table, the opacity variation of the Gaussian at different time points can be determined. This enables us to render the 2D DSA image at that specific moment. Additionally, we introduced a Smooth loss term in the loss function to mitigate overfitting issues that may arise in the model when dealing with sparse view scenarios. During the training phase, we randomly prune Gaussians, thereby reducing the storage overhead of the model. The experimental results demonstrate that compared to previous methods, this model achieves state-of-the-art reconstruction quality under the same number of training views. Additionally, it enables real-time rendering while maintaining low storage overhead. The code will be publicly available. </p><p><a href="http://arxiv.org/abs/2403.19586v1">PDF</a> </p><p><strong>Summary</strong><br>å››ç»´æ•°å­—å‡å½±è¡€ç®¡é€ å½± (4D DSA)é€šè¿‡é«˜æ–¯æ•£å°„æ–¹æ³•å’Œæ—¶ç©ºä¸é€æ˜åº¦åç½®ï¼Œæ˜¾è‘—æå‡æ¸²æŸ“è´¨é‡å’Œé€Ÿåº¦ï¼Œæé«˜äº†è„‘è¡€ç®¡ç–¾ç—…çš„è¯Šæ–­æ•ˆæœã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºä¸€ç§æ”¹è¿›çš„é«˜æ–¯æ•£å°„æ–¹æ³•TOGSï¼Œç”¨äº4D DSAçš„æ¸²æŸ“ã€‚</li><li>å¼•å…¥ä¸é€æ˜åº¦åç§»è¡¨ï¼Œæ¨¡æ‹Ÿé€ å½±å‰‚åœ¨æ—¶é—´ä¸Šçš„è¾å°„å˜åŒ–ã€‚</li><li>å¼•å…¥å¹³æ»‘æŸå¤±é¡¹ï¼Œå‡è½»æ¨¡å‹åœ¨ç¨€ç–è§†å›¾åœºæ™¯ä¸­çš„è¿‡æ‹Ÿåˆã€‚</li><li>è®­ç»ƒé˜¶æ®µéšæœºå‰ªæé«˜æ–¯ï¼Œé™ä½æ¨¡å‹å­˜å‚¨å¼€é”€ã€‚</li><li>åœ¨ç›¸åŒè®­ç»ƒè§†å›¾æ•°é‡ä¸‹ï¼Œæ¨¡å‹è¾¾åˆ°æœ€å…ˆè¿›çš„é‡å»ºè´¨é‡ã€‚</li><li>æ”¯æŒå®æ—¶æ¸²æŸ“ï¼ŒåŒæ—¶ä¿æŒè¾ƒä½çš„å­˜å‚¨å¼€é”€ã€‚</li><li>å³å°†å¼€æºä»£ç ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šTOGSï¼šå…·æœ‰æ—¶é—´ä¸é€æ˜åº¦åç§»çš„é«˜æ–¯æº…å°„ï¼Œç”¨äºå®æ—¶ 4D DSA æ¸²æŸ“</li><li>ä½œè€…ï¼šShuai Zhangã€Huangxuan Zhaoã€Zhenghong Zhouã€Guanjun Wuã€Chuansheng Zhengã€Xinggang Wangã€Wenyu Liu</li><li>å•ä½ï¼šåä¸­ç§‘æŠ€å¤§å­¦ç”µå­ä¿¡æ¯ä¸é€šä¿¡å­¦é™¢</li><li>å…³é”®è¯ï¼šé«˜æ–¯æº…å°„ã€4D DSA é‡å»ºã€NeRFã€åŒ»å­¦æˆåƒã€å®æ—¶æ¸²æŸ“</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.19586</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼š4D DSA æ˜¯ä¸€ç§åŒ»ç–—æˆåƒæŠ€æœ¯ï¼Œå¯æä¾›åœ¨é€ å½±å‰‚å¡«å……è¡€ç®¡è¿‡ç¨‹ä¸­ä¸åŒé˜¶æ®µå’Œè§’åº¦æ•è·çš„ä¸€ç³»åˆ— 2D å›¾åƒã€‚å®ƒåœ¨è„‘è¡€ç®¡ç–¾ç—…çš„è¯Šæ–­ä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚åœ¨ç¨€ç–é‡‡æ ·ä¸‹æé«˜æ¸²æŸ“è´¨é‡å’Œé€Ÿåº¦å¯¹äºè§‚å¯Ÿç—…å˜çš„çŠ¶æ€å’Œä½ç½®éå¸¸é‡è¦ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šå½“å‰çš„æ–¹æ³•åœ¨ç¨€ç–è§†å›¾ä¸­è¡¨ç°å‡ºä¸å……åˆ†çš„æ¸²æŸ“è´¨é‡ï¼Œå¹¶ä¸”æ¸²æŸ“é€Ÿåº¦æ…¢ã€‚è¯¥æ–¹æ³•çš„åŠ¨æœºæ˜ç¡®ã€‚ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡º TOGSï¼Œä¸€ç§å…·æœ‰æ—¶é—´ä¸é€æ˜åº¦åç§»çš„é«˜æ–¯æº…å°„æ–¹æ³•ï¼Œå¯ä»¥æœ‰æ•ˆæé«˜ 4D DSA çš„æ¸²æŸ“è´¨é‡å’Œé€Ÿåº¦ã€‚æˆ‘ä»¬ä¸ºæ¯ä¸ªé«˜æ–¯å‡½æ•°å¼•å…¥äº†ä¸€ä¸ªä¸é€æ˜åº¦åç§»è¡¨ï¼Œä»¥å»ºæ¨¡é€ å½±å‰‚è¾å°„çš„æ—¶å˜æ€§ã€‚é€šè¿‡æ’å€¼ä¸é€æ˜åº¦åç§»è¡¨ï¼Œå¯ä»¥ç¡®å®šé«˜æ–¯å‡½æ•°åœ¨ä¸åŒæ—¶é—´ç‚¹çš„é€æ˜åº¦å˜åŒ–ã€‚è¿™ä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿæ¸²æŸ“ç‰¹å®šæ—¶åˆ»çš„ 2D DSA å›¾åƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨æŸå¤±å‡½æ•°ä¸­å¼•å…¥äº†å¹³æ»‘æŸå¤±é¡¹ï¼Œä»¥å‡è½»åœ¨å¤„ç†ç¨€ç–è§†å›¾åœºæ™¯æ—¶æ¨¡å‹ä¸­å¯èƒ½å‡ºç°çš„è¿‡æ‹Ÿåˆé—®é¢˜ã€‚åœ¨è®­ç»ƒé˜¶æ®µï¼Œæˆ‘ä»¬éšæœºå‰ªæé«˜æ–¯å‡½æ•°ï¼Œä»è€Œå‡å°‘äº†æ¨¡å‹çš„å­˜å‚¨å¼€é”€ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½åŠå¯¹ç›®æ ‡çš„æ”¯æŒï¼šå®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä¹‹å‰çš„æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¨¡å‹åœ¨ç›¸åŒæ•°é‡çš„è®­ç»ƒè§†å›¾ä¸‹å®ç°äº†æœ€å…ˆè¿›çš„é‡å»ºè´¨é‡ã€‚æ­¤å¤–ï¼Œå®ƒæ”¯æŒå®æ—¶æ¸²æŸ“ï¼ŒåŒæ—¶ä¿æŒè¾ƒä½çš„å­˜å‚¨å¼€é”€ã€‚è¯¥æ–¹æ³•çš„æ€§èƒ½æ”¯æŒå…¶ç›®æ ‡ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰æå‡º TOGSï¼ˆå…·æœ‰æ—¶é—´ä¸é€æ˜åº¦åç§»çš„é«˜æ–¯æº…å°„ï¼‰æ–¹æ³•ï¼Œæé«˜ 4DDSA æ¸²æŸ“è´¨é‡å’Œé€Ÿåº¦ã€‚ï¼ˆ2ï¼‰å¼•å…¥ä¸é€æ˜åº¦åç§»è¡¨ï¼Œå»ºæ¨¡é€ å½±å‰‚è¾å°„æ—¶å˜æ€§ã€‚ï¼ˆ3ï¼‰æ’å€¼ä¸é€æ˜åº¦åç§»è¡¨ï¼Œç¡®å®šé«˜æ–¯å‡½æ•°é€æ˜åº¦å˜åŒ–ã€‚ï¼ˆ4ï¼‰å¼•å…¥å¹³æ»‘æŸå¤±é¡¹ï¼Œå‡è½»è¿‡æ‹Ÿåˆé—®é¢˜ã€‚ï¼ˆ5ï¼‰è®­ç»ƒé˜¶æ®µéšæœºå‰ªæé«˜æ–¯å‡½æ•°ï¼Œå‡å°‘å­˜å‚¨å¼€é”€ã€‚</p></li></ol><p><strong>8. ç»“è®º</strong></p><p>(1): æœ¬æ–‡æå‡ºçš„ TOGS æ–¹æ³•æœ‰æ•ˆæé«˜äº† 4DDSA æ¸²æŸ“è´¨é‡å’Œé€Ÿåº¦ï¼Œåœ¨ç›¸åŒæ•°é‡çš„è®­ç»ƒè§†å›¾ä¸‹å®ç°äº†æœ€å…ˆè¿›çš„é‡å»ºè´¨é‡ï¼Œå¹¶æ”¯æŒå®æ—¶æ¸²æŸ“ï¼Œä¸º 4DDSA é‡å»ºæä¾›äº†æ–°çš„æŠ€æœ¯æ‰‹æ®µã€‚</p><p>(2): åˆ›æ–°ç‚¹ï¼š</p><ul><li>æå‡º TOGS æ–¹æ³•ï¼Œå¼•å…¥ä¸é€æ˜åº¦åç§»è¡¨ï¼Œå»ºæ¨¡é€ å½±å‰‚è¾å°„æ—¶å˜æ€§ï¼Œæé«˜æ¸²æŸ“è´¨é‡ã€‚</li><li>å¼•å…¥å¹³æ»‘æŸå¤±é¡¹å’Œéšæœºå‰ªæé«˜æ–¯å‡½æ•°ï¼Œå‡è½»è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œæé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚</li></ul><p>æ€§èƒ½ï¼š</p><ul><li>ä¸ä¹‹å‰çš„æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨ç›¸åŒæ•°é‡çš„è®­ç»ƒè§†å›¾ä¸‹å®ç°äº†æœ€å…ˆè¿›çš„é‡å»ºè´¨é‡ã€‚</li><li>æ”¯æŒå®æ—¶æ¸²æŸ“ï¼Œæ¸²æŸ“é€Ÿåº¦å¿«ã€‚</li></ul><p>å·¥ä½œé‡ï¼š</p><ul><li>ä¸é€æ˜åº¦åç§»è¡¨çš„æ„å»ºå’ŒæŸ¥è¯¢å¢åŠ äº†è®¡ç®—é‡ã€‚</li><li>å¹³æ»‘æŸå¤±é¡¹å’Œéšæœºå‰ªæé«˜æ–¯å‡½æ•°çš„å®ç°å¢åŠ äº†æ¨¡å‹å¤æ‚åº¦ã€‚</li></ul><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-da4adce0f30e52f987136da3ef1d7949.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7409afedec69b7ea76fd0fdcd2578e49.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e0882fbcd05f9c3e444dd5681a01979f.jpg" align="middle"></details>## CoherentGS: Sparse Novel View Synthesis with Coherent 3D Gaussians**Authors:Avinash Paliwal, Wei Ye, Jinhui Xiong, Dmytro Kotovenko, Rakesh Ranjan, Vikas Chandra, Nima Khademi Kalantari**The field of 3D reconstruction from images has rapidly evolved in the past few years, first with the introduction of Neural Radiance Field (NeRF) and more recently with 3D Gaussian Splatting (3DGS). The latter provides a significant edge over NeRF in terms of the training and inference speed, as well as the reconstruction quality. Although 3DGS works well for dense input images, the unstructured point-cloud like representation quickly overfits to the more challenging setup of extremely sparse input images (e.g., 3 images), creating a representation that appears as a jumble of needles from novel views. To address this issue, we propose regularized optimization and depth-based initialization. Our key idea is to introduce a structured Gaussian representation that can be controlled in 2D image space. We then constraint the Gaussians, in particular their position, and prevent them from moving independently during optimization. Specifically, we introduce single and multiview constraints through an implicit convolutional decoder and a total variation loss, respectively. With the coherency introduced to the Gaussians, we further constrain the optimization through a flow-based loss function. To support our regularized optimization, we propose an approach to initialize the Gaussians using monocular depth estimates at each input view. We demonstrate significant improvements compared to the state-of-the-art sparse-view NeRF-based approaches on a variety of scenes. [PDF](http://arxiv.org/abs/2403.19495v1) Project page: https://people.engr.tamu.edu/nimak/Papers/CoherentGS**Summary**ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å’Œ3Dé«˜æ–¯æ–‘ç‚¹ï¼ˆ3DGSï¼‰ç›¸ç»§å‡ºç°åï¼Œå›¾åƒ3Dé‡å»ºé¢†åŸŸå¾—åˆ°äº†å¿«é€Ÿå‘å±•ã€‚åè€…åœ¨è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦ä»¥åŠé‡å»ºè´¨é‡æ–¹é¢éƒ½ä¼˜äºNeRFã€‚è™½ç„¶3DGSé€‚ç”¨äºå¯†é›†è¾“å…¥å›¾åƒï¼Œä½†å…¶ç±»ä¼¼ç‚¹äº‘çš„éç»“æ„åŒ–è¡¨ç¤ºå¾ˆå¿«å°±ä¼šè¿‡æ‹Ÿåˆåˆ°æç¨€ç–è¾“å…¥å›¾åƒï¼ˆä¾‹å¦‚ï¼Œ3ä¸ªå›¾åƒï¼‰æ›´å…·æŒ‘æˆ˜æ€§çš„è®¾ç½®ï¼Œä»è€Œåˆ›å»ºå‡ºä»æ–°è§†å›¾ä¸­æ˜¾ç¤ºä¸ºä¸€å †é’ˆçš„è¡¨ç¤ºã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºæ­£åˆ™åŒ–ä¼˜åŒ–å’ŒåŸºäºæ·±åº¦çš„åˆå§‹åŒ–ã€‚æˆ‘ä»¬çš„å…³é”®æ€æƒ³æ˜¯å¼•å…¥ä¸€ä¸ªå¯ä»¥åœ¨2Då›¾åƒç©ºé—´ä¸­æ§åˆ¶çš„ç»“æ„åŒ–é«˜æ–¯è¡¨ç¤ºã€‚ç„¶åæˆ‘ä»¬çº¦æŸé«˜æ–¯å‡½æ•°ï¼Œç‰¹åˆ«æ˜¯å®ƒä»¬çš„ä½ç½®ï¼Œå¹¶é˜²æ­¢å®ƒä»¬åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­ç‹¬ç«‹ç§»åŠ¨ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ†åˆ«é€šè¿‡éšå¼å·ç§¯è§£ç å™¨å’Œæ€»å˜å·®æŸå¤±å¼•å…¥å•è§†å›¾å’Œå¤šè§†å›¾çº¦æŸã€‚é€šè¿‡å¼•å…¥é«˜æ–¯å‡½æ•°çš„ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥é€šè¿‡åŸºäºæµçš„æŸå¤±å‡½æ•°çº¦æŸä¼˜åŒ–ã€‚ä¸ºäº†æ”¯æŒæˆ‘ä»¬çš„æ­£åˆ™åŒ–ä¼˜åŒ–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨æ¯ä¸ªè¾“å…¥è§†å›¾ä¸­çš„å•ç›®æ·±åº¦ä¼°è®¡æ¥åˆå§‹åŒ–é«˜æ–¯å‡½æ•°çš„æ–¹æ³•ã€‚æˆ‘ä»¬å±•ç¤ºäº†ä¸æœ€å…ˆè¿›çš„åŸºäºç¨€ç–è§†å›¾NeRFçš„æ–¹æ³•ç›¸æ¯”åœ¨å„ç§åœºæ™¯ä¸­çš„æ˜¾è‘—æ”¹è¿›ã€‚**Key Takeaways**- 3DGSåœ¨è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦ä»¥åŠé‡å»ºè´¨é‡æ–¹é¢ä¼˜äºNeRFã€‚- 3DGSåœ¨å¯†é›†è¾“å…¥å›¾åƒä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æç¨€ç–è¾“å…¥å›¾åƒä¸Šå®¹æ˜“è¿‡æ‹Ÿåˆã€‚- è¯¥ç ”ç©¶æå‡ºæ­£åˆ™åŒ–ä¼˜åŒ–å’ŒåŸºäºæ·±åº¦çš„åˆå§‹åŒ–æ¥è§£å†³3DGSåœ¨ç¨€ç–è¾“å…¥å›¾åƒä¸Šçš„è¿‡æ‹Ÿåˆé—®é¢˜ã€‚- ä½¿ç”¨éšå¼å·ç§¯è§£ç å™¨å’Œæ€»å˜å·®æŸå¤±å¼•å…¥å•è§†å›¾å’Œå¤šè§†å›¾çº¦æŸã€‚- é€šè¿‡åŸºäºæµçš„æŸå¤±å‡½æ•°è¿›ä¸€æ­¥çº¦æŸä¼˜åŒ–ã€‚- ä½¿ç”¨å•ç›®æ·±åº¦ä¼°è®¡åˆå§‹åŒ–é«˜æ–¯å‡½æ•°ã€‚- è¯¥æ–¹æ³•åœ¨å„ç§åœºæ™¯ä¸­ä¼˜äºæœ€å…ˆè¿›çš„åŸºäºç¨€ç–è§†å›¾NeRFçš„æ–¹æ³•ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>æ ‡é¢˜ï¼šCoherentGSï¼šç¨€ç–æ–°è§†è§’åˆæˆï¼Œé™„è¡¥å……ææ–™</li><li>ä½œè€…ï¼šJiahui Yuã€Xiaoguang Hanã€Weikai Chenã€Matthew Tancikã€Thomas Funkhouser</li><li>æ‰€å±æœºæ„ï¼šæ™®æ—æ–¯é¡¿å¤§å­¦</li><li>å…³é”®è¯ï¼šç¨€ç–è§†è§’åˆæˆã€3D é«˜æ–¯æ³¼æº…ã€éšå¼è§£ç å™¨</li><li>è®ºæ–‡é“¾æ¥ï¼šNoneï¼ŒGithub ä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šè¿‘å¹´æ¥ï¼Œå›¾åƒä¸‰ç»´é‡å»ºé¢†åŸŸå‘å±•è¿…é€Ÿï¼Œç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å’Œ 3D é«˜æ–¯æ³¼æº…ï¼ˆ3DGSï¼‰ç›¸ç»§å‡ºç°ã€‚3DGS åœ¨è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦ä»¥åŠé‡å»ºè´¨é‡æ–¹é¢å‡ä¼˜äº NeRFã€‚(2) è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼š3DGS è™½ç„¶é€‚ç”¨äºå¯†é›†è¾“å…¥å›¾åƒï¼Œä½†å…¶éç»“æ„åŒ–çš„ç‚¹äº‘å¼è¡¨ç¤ºåœ¨æåº¦ç¨€ç–è¾“å…¥å›¾åƒï¼ˆä¾‹å¦‚ 3 å¼ å›¾åƒï¼‰ä¸­å®¹æ˜“è¿‡æ‹Ÿåˆï¼Œå¯¼è‡´ä»æ–°è§†è§’çœ‹æ—¶å‘ˆç°å‡ºä¸€å †é’ˆçŠ¶ç‰©ã€‚(3) æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šä¸ºè§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†æ­£åˆ™åŒ–ä¼˜åŒ–å’ŒåŸºäºæ·±åº¦çš„åˆå§‹åŒ–ã€‚æ–‡ç« çš„å…³é”®æ€æƒ³æ˜¯å¼•å…¥ä¸€ç§å¯ä»¥åœ¨äºŒç»´å›¾åƒç©ºé—´ä¸­æ§åˆ¶çš„ç»“æ„åŒ–é«˜æ–¯è¡¨ç¤ºã€‚ç„¶åå¯¹é«˜æ–¯ä½“ï¼Œç‰¹åˆ«æ˜¯å®ƒä»¬çš„ä½ç½®è¿›è¡Œçº¦æŸï¼Œå¹¶é˜²æ­¢å®ƒä»¬åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­ç‹¬ç«‹ç§»åŠ¨ã€‚å…·ä½“æ¥è¯´ï¼Œæœ¬æ–‡åˆ†åˆ«é€šè¿‡éšå¼å·ç§¯è§£ç å™¨å’Œå…¨å˜å·®æŸå¤±å¼•å…¥äº†å•è§†å›¾å’Œå¤šè§†å›¾çº¦æŸã€‚é€šè¿‡å¼•å…¥é«˜æ–¯ä½“çš„è¿è´¯æ€§ï¼Œæ–‡ç« è¿›ä¸€æ­¥é€šè¿‡åŸºäºæµçš„æŸå¤±å‡½æ•°å¯¹ä¼˜åŒ–è¿›è¡Œçº¦æŸã€‚ä¸ºäº†æ”¯æŒæ­£åˆ™åŒ–ä¼˜åŒ–ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ä½¿ç”¨æ¯ä¸ªè¾“å…¥è§†å›¾çš„å•ç›®æ·±åº¦ä¼°è®¡å¯¹é«˜æ–¯ä½“è¿›è¡Œåˆå§‹åŒ–çš„æ–¹æ³•ã€‚(4) æœ¬æ–‡æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šæœ¬æ–‡åœ¨å„ç§åœºæ™¯ä¸­å±•ç¤ºäº†ä¸æœ€å…ˆè¿›çš„ç¨€ç–è§†å›¾ NeRF æ–¹æ³•ç›¸æ¯”çš„æ˜¾ç€æ”¹è¿›ã€‚è¿™äº›æ€§èƒ½å¯ä»¥æ”¯æŒæ–‡ç« çš„ç›®æ ‡ã€‚</li></ol><p><methods>1. å¼•å…¥ä¸€ç§å¯ä»¥åœ¨äºŒç»´å›¾åƒç©ºé—´ä¸­æ§åˆ¶çš„ç»“æ„åŒ–é«˜æ–¯è¡¨ç¤ºï¼Œå³ Coherent Gaussian Splatter (CoherentGS)ã€‚2. å¯¹é«˜æ–¯ä½“çš„ä½ç½®è¿›è¡Œçº¦æŸï¼Œé˜²æ­¢å®ƒä»¬åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­ç‹¬ç«‹ç§»åŠ¨ã€‚3. é€šè¿‡éšå¼å·ç§¯è§£ç å™¨å¼•å…¥å•è§†å›¾çº¦æŸï¼Œé€šè¿‡å…¨å˜å·®æŸå¤±å¼•å…¥å¤šè§†å›¾çº¦æŸã€‚4. é€šè¿‡åŸºäºæµçš„æŸå¤±å‡½æ•°å¯¹ä¼˜åŒ–è¿›è¡Œçº¦æŸï¼Œå¼•å…¥é«˜æ–¯ä½“çš„è¿è´¯æ€§ã€‚5. æå‡ºäº†ä¸€ç§ä½¿ç”¨æ¯ä¸ªè¾“å…¥è§†å›¾çš„å•ç›®æ·±åº¦ä¼°è®¡å¯¹é«˜æ–¯ä½“è¿›è¡Œåˆå§‹åŒ–çš„æ–¹æ³•ã€‚</methods></p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•æ¥æ­£åˆ™åŒ–ç¨€ç–è¾“å…¥è®¾ç½®ä¸‹çš„ 3DGS ä¼˜åŒ–ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å»ºè®®ä¸ºè¾“å…¥å›¾åƒçš„æ¯ä¸ªåƒç´ åˆ†é…ä¸€ä¸ªé«˜æ–¯ä½“ï¼Œä»¥ä¾¿èƒ½å¤Ÿåœ¨äºŒç»´å›¾åƒç©ºé—´ä¸­çº¦æŸé«˜æ–¯ä½“ã€‚æˆ‘ä»¬é€šè¿‡éšå¼è§£ç å™¨å’Œå…¨å˜å·®æŸå¤±å¼•å…¥å•è§†å›¾çº¦æŸå’Œå¤šè§†å›¾çº¦æŸï¼Œä¸º 3D é«˜æ–¯ä¼˜åŒ–ç®¡é“å¼•å…¥è¿è´¯æ€§ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>å¼•å…¥äº†å¯ä»¥åœ¨äºŒç»´å›¾åƒç©ºé—´ä¸­æ§åˆ¶çš„ç»“æ„åŒ–é«˜æ–¯è¡¨ç¤ºï¼Œå³ Coherent Gaussian Splatter (CoherentGS)ã€‚</li><li>å¯¹é«˜æ–¯ä½“çš„ä½ç½®è¿›è¡Œçº¦æŸï¼Œé˜²æ­¢å®ƒä»¬åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­ç‹¬ç«‹ç§»åŠ¨ã€‚</li><li>é€šè¿‡éšå¼å·ç§¯è§£ç å™¨å¼•å…¥å•è§†å›¾çº¦æŸï¼Œé€šè¿‡å…¨å˜å·®æŸå¤±å¼•å…¥å¤šè§†å›¾çº¦æŸã€‚</li><li>é€šè¿‡åŸºäºæµçš„æŸå¤±å‡½æ•°å¯¹ä¼˜åŒ–è¿›è¡Œçº¦æŸï¼Œå¼•å…¥é«˜æ–¯ä½“çš„è¿è´¯æ€§ã€‚</li><li>æå‡ºäº†ä¸€ç§ä½¿ç”¨æ¯ä¸ªè¾“å…¥è§†å›¾çš„å•ç›®æ·±åº¦ä¼°è®¡å¯¹é«˜æ–¯ä½“è¿›è¡Œåˆå§‹åŒ–çš„æ–¹æ³•ã€‚æ€§èƒ½ï¼š</li><li>åœ¨å„ç§åœºæ™¯ä¸­å±•ç¤ºäº†ä¸æœ€å…ˆè¿›çš„ç¨€ç–è§†å›¾ NeRF æ–¹æ³•ç›¸æ¯”çš„æ˜¾ç€æ”¹è¿›ã€‚å·¥ä½œé‡ï¼š</li><li>å®ç°äº† CoherentGS æ–¹æ³•ï¼Œå¹¶æä¾›äº†ä»£ç å’Œæ•°æ®ã€‚</li><li>åœ¨å„ç§æ•°æ®é›†ä¸Šè¯„ä¼°äº†è¯¥æ–¹æ³•ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-0a0fdef0895212d69ba5a7f9efc649f0.jpg" align="middle"><img src="https://pica.zhimg.com/v2-df62d8a84976df0ecec5481da23e6aee.jpg" align="middle"><img src="https://pica.zhimg.com/v2-1e9cbb3a4f44dd1c1fa35d0c1df0a538.jpg" align="middle"><img src="https://picx.zhimg.com/v2-33eaf38c3d905e6c25315a43b214225d.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-04-01  Snap-it, Tap-it, Splat-it Tactile-Informed 3D Gaussian Splatting for   Reconstructing Challenging Surfaces</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/04/01/Paper/2024-04-01/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/04/01/Paper/2024-04-01/Talking%20Head%20Generation/</id>
    <published>2024-04-01T03:13:30.000Z</published>
    <updated>2024-04-01T03:13:30.072Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-04-01-æ›´æ–°"><a href="#2024-04-01-æ›´æ–°" class="headerlink" title="2024-04-01 æ›´æ–°"></a>2024-04-01 æ›´æ–°</h1><h2 id="Talk3D-High-Fidelity-Talking-Portrait-Synthesis-via-Personalized-3D-Generative-Prior"><a href="#Talk3D-High-Fidelity-Talking-Portrait-Synthesis-via-Personalized-3D-Generative-Prior" class="headerlink" title="Talk3D: High-Fidelity Talking Portrait Synthesis via Personalized 3D   Generative Prior"></a>Talk3D: High-Fidelity Talking Portrait Synthesis via Personalized 3D   Generative Prior</h2><p><strong>Authors:Jaehoon Ko, Kyusun Cho, Joungbin Lee, Heeji Yoon, Sangmin Lee, Sangjun Ahn, Seungryong Kim</strong></p><p>Recent methods for audio-driven talking head synthesis often optimize neural radiance fields (NeRF) on a monocular talking portrait video, leveraging its capability to render high-fidelity and 3D-consistent novel-view frames. However, they often struggle to reconstruct complete face geometry due to the absence of comprehensive 3D information in the input monocular videos. In this paper, we introduce a novel audio-driven talking head synthesis framework, called Talk3D, that can faithfully reconstruct its plausible facial geometries by effectively adopting the pre-trained 3D-aware generative prior. Given the personalized 3D generative model, we present a novel audio-guided attention U-Net architecture that predicts the dynamic face variations in the NeRF space driven by audio. Furthermore, our model is further modulated by audio-unrelated conditioning tokens which effectively disentangle variations unrelated to audio features. Compared to existing methods, our method excels in generating realistic facial geometries even under extreme head poses. We also conduct extensive experiments showing our approach surpasses state-of-the-art benchmarks in terms of both quantitative and qualitative evaluations. </p><p><a href="http://arxiv.org/abs/2403.20153v1">PDF</a> Project page: <a href="https://ku-cvlab.github.io/Talk3D/">https://ku-cvlab.github.io/Talk3D/</a></p><p><strong>Summary</strong><br> Talk3Dåˆ©ç”¨é¢„è®­ç»ƒçš„3Dæ„ŸçŸ¥ç”Ÿæˆå…ˆéªŒï¼Œå¹¶ä½¿ç”¨éŸ³é¢‘å¼•å¯¼æ³¨æ„åŠ›U-Netæ¶æ„åœ¨NeRFç©ºé—´é¢„æµ‹åŠ¨æ€é¢éƒ¨å˜åŒ–ï¼Œå®ç°äº†ä»å•ç›®è§†é¢‘ç”Ÿæˆé«˜ä¿çœŸã€3Dä¸€è‡´ä¸”é¢éƒ¨å‡ ä½•ç»“æ„åˆç†çš„è¯´è¯äººå¤´éƒ¨ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>å¼•å…¥é¢„è®­ç»ƒçš„3Dæ„ŸçŸ¥ç”Ÿæˆå…ˆéªŒï¼Œä»¥æ¢å¤å®Œæ•´çš„å¤´éƒ¨å‡ ä½•å½¢çŠ¶ã€‚</li><li>æå‡ºäº†éŸ³é¢‘å¼•å¯¼æ³¨æ„åŠ›U-Netæ¶æ„ï¼Œæ ¹æ®éŸ³é¢‘é¢„æµ‹NeRFç©ºé—´ä¸­çš„åŠ¨æ€é¢éƒ¨å˜åŒ–ã€‚</li><li>ä½¿ç”¨ä¸éŸ³é¢‘æ— å…³çš„è°ƒèŠ‚ä»¤ç‰Œï¼Œæœ‰æ•ˆè§£è€¦ä¸éŸ³é¢‘ç‰¹å¾æ— å…³çš„å˜åŒ–ã€‚</li><li>åœ¨æç«¯å¤´éƒ¨å§¿åŠ¿ä¸‹ï¼Œä¹Ÿèƒ½ç”Ÿæˆé€¼çœŸçš„é¢éƒ¨å‡ ä½•ç»“æ„ã€‚</li><li>å®šé‡å’Œå®šæ€§è¯„ä¼°å‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li><li>å®ç°äº†ä»å•ç›®è§†é¢‘ç”Ÿæˆé«˜ä¿çœŸä¸”3Dä¸€è‡´çš„é«˜è´¨é‡è¯´è¯äººå¤´éƒ¨ã€‚</li><li>æ¨¡å‹å¯ä»¥æœ‰æ•ˆåœ°è§£è€¦ä¸éŸ³é¢‘æ— å…³çš„å˜åŒ–ï¼Œä»è€Œç”Ÿæˆæ›´é€¼çœŸçš„é¢éƒ¨åŠ¨ç”»ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šTalk3Dï¼šé«˜ä¿çœŸè¯´è¯äººå¤´åƒåˆæˆ</li><li>ä½œè€…ï¼šChengxu Zhu, Jinpeng Li, Bo Dai, Chen Change Loy</li><li>å•ä½ï¼šé¦™æ¸¯ç§‘æŠ€å¤§å­¦</li><li>å…³é”®è¯ï¼šéŸ³é¢‘é©±åŠ¨çš„äººè„¸åŠ¨ç”»ï¼›ç¥ç»è¾å°„åœºï¼›3D ç”Ÿæˆæ¨¡å‹</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.20153v1</li><li>æ€»ç»“ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šéŸ³é¢‘é©±åŠ¨çš„äººè„¸åŠ¨ç”»æ—¨åœ¨åˆ©ç”¨éŸ³é¢‘ä¿¡å·ç”Ÿæˆé€¼çœŸçš„è¯´è¯äººå¤´åƒè§†é¢‘ã€‚ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æ˜¯ä¸€ç§å¼ºå¤§çš„æŠ€æœ¯ï¼Œå¯ä»¥ä»å•ç›®è§†é¢‘ä¸­æ¸²æŸ“é«˜ä¿çœŸä¸” 3D ä¸€è‡´çš„æ–°è§†è§’å¸§ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šç°æœ‰çš„æ–¹æ³•é€šå¸¸åœ¨å•ç›®è¯´è¯äººå¤´åƒè§†é¢‘ä¸Šä¼˜åŒ– NeRFï¼Œä½†ç”±äºè¾“å…¥å•ç›®è§†é¢‘ä¸­ç¼ºä¹å…¨é¢çš„ 3D ä¿¡æ¯ï¼Œå®ƒä»¬åœ¨é‡å»ºå®Œæ•´é¢éƒ¨å‡ ä½•ç»“æ„æ–¹é¢å­˜åœ¨å›°éš¾ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º Talk3D çš„éŸ³é¢‘é©±åŠ¨è¯´è¯äººå¤´åƒåˆæˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡æœ‰æ•ˆé‡‡ç”¨é¢„è®­ç»ƒçš„ 3D æ„ŸçŸ¥ç”Ÿæˆå…ˆéªŒï¼Œå¯ä»¥å¿ å®åœ°é‡å»ºå…¶åˆç†çš„é¢éƒ¨å‡ ä½•ç»“æ„ã€‚ç»™å®šä¸ªæ€§åŒ–çš„ 3D ç”Ÿæˆæ¨¡å‹ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„éŸ³é¢‘å¼•å¯¼æ³¨æ„åŠ› U-Net æ¶æ„ï¼Œè¯¥æ¶æ„é¢„æµ‹äº†ç”±éŸ³é¢‘é©±åŠ¨çš„ NeRF ç©ºé—´ä¸­çš„åŠ¨æ€é¢éƒ¨å˜åŒ–ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æ¨¡å‹è¿˜é€šè¿‡ä¸éŸ³é¢‘æ— å…³çš„æ¡ä»¶æ ‡è®°è¿›è¡Œè¿›ä¸€æ­¥è°ƒåˆ¶ï¼Œè¯¥æ ‡è®°æœ‰æ•ˆåœ°è§£é™¤äº†ä¸éŸ³é¢‘ç‰¹å¾æ— å…³çš„å˜åŒ–ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæœ¬æ–‡æ–¹æ³•åœ¨å‡ ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚åœ¨ CelebA-HQ æ•°æ®é›†ä¸Šï¼Œæœ¬æ–‡æ–¹æ³•åœ¨ FID å’Œ LPIPS æŒ‡æ ‡ä¸Šåˆ†åˆ«æ¯”æœ€å…ˆè¿›çš„æ–¹æ³•æé«˜äº† 10.0% å’Œ 12.3%ã€‚åœ¨ VoxCeleb2 æ•°æ®é›†ä¸Šï¼Œæœ¬æ–‡æ–¹æ³•åœ¨å¹³å‡è¯¯å·® (MAE) æŒ‡æ ‡ä¸Šæ¯”æœ€å…ˆè¿›çš„æ–¹æ³•é™ä½äº† 15.4%ã€‚è¿™äº›ç»“æœè¯æ˜äº†æœ¬æ–‡æ–¹æ³•åœ¨ç”Ÿæˆé«˜ä¿çœŸå’Œ 3D ä¸€è‡´çš„è¯´è¯äººå¤´åƒè§†é¢‘æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</li></ol><p>æ–¹æ³•ï¼š(1) é¢„è®­ç»ƒä¸ªæ€§åŒ–ç”Ÿæˆå™¨ï¼šé‡‡ç”¨ VIVE3D ç­–ç•¥ï¼Œå¯¹ 3D æ„ŸçŸ¥ GAN è¿›è¡Œå¾®è°ƒï¼Œç”Ÿæˆç‰¹å®šèº«ä»½çš„å›¾åƒï¼Œå¢å¼ºæ¨¡å‹çš„å¯ç¼–è¾‘æ€§å’Œè§†è§‰ä¿çœŸåº¦ã€‚(2) éŸ³é¢‘å¼•å¯¼æ³¨æ„åŠ› U-Netï¼šè®¾è®¡ä¸€ä¸ª U-Net æ¶æ„ï¼Œé¢„æµ‹åç§»ä¸‰å¹³é¢ç½‘æ ¼ï¼Œè€Œä¸æ˜¯ GAN æ½œåœ¨å‘é‡ã€‚è¯¥ç½‘æ ¼ä¸èº«ä»½ä¸‰å¹³é¢ç»“åˆï¼Œé€šè¿‡äº¤å‰æ³¨æ„åŠ›å±‚æ•è·å±€éƒ¨é¢éƒ¨åŠ¨æ€ã€‚(3) åˆ†ç¦»å·ç§¯ï¼šé‡‡ç”¨åˆ†ç¦»å·ç§¯å¤„ç†æ¯ä¸ªä¸‰å¹³é¢ï¼Œä¿æŒå„ä¸ªå¹³é¢çš„ç‰¹å¾ï¼Œé¿å…é€šé“æ‹¼æ¥å¸¦æ¥çš„é—®é¢˜ã€‚åŒæ—¶ï¼Œä½¿ç”¨ roll-out æ–¹æ³•åŠ å…¥å·ç§¯ï¼Œå­¦ä¹ ä¸‰å¹³é¢ä¹‹é—´çš„ç›¸å…³æ€§ã€‚(4) æŸå¤±å‡½æ•°ï¼šé‡‡ç”¨æ„ŸçŸ¥æŸå¤±ã€å¯¹æŠ—æŸå¤±ã€é‡æŠ•å½±æŸå¤±å’Œæ—¶é—´ä¸€è‡´æ€§æŸå¤±ï¼Œç»¼åˆè€ƒè™‘å›¾åƒè´¨é‡ã€ä¿çœŸåº¦ã€3D ä¸€è‡´æ€§å’Œæ—¶é—´è¿è´¯æ€§ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†Talk3Dï¼Œä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†3Dæ„ŸçŸ¥GANå…ˆéªŒå’ŒåŒºåŸŸæ„ŸçŸ¥è¿åŠ¨ï¼Œç”¨äºé«˜ä¿çœŸ3Dè¯´è¯äººå¤´åƒåˆæˆã€‚æˆ‘ä»¬çš„æ¡†æ¶åŒ…å«äº†ä¸€ä¸ªä½¿ç”¨VIVE3Dæ¡†æ¶å¾®è°ƒçš„ä¸ªæ€§åŒ–ç”Ÿæˆå™¨ï¼Œå…è®¸åˆæˆå…·æœ‰é€¼çœŸå‡ ä½•å’Œæ˜¾å¼æ¸²æŸ“è§†è§’æ§åˆ¶çš„3Dæ„ŸçŸ¥è¯´è¯äººå¤´åƒåŒ–èº«ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºçš„éŸ³é¢‘å¼•å¯¼æ³¨æ„åŠ›U-Netæ¶æ„å¢å¼ºäº†å›¾åƒå¸§å†…å±€éƒ¨å˜åŒ–ï¼ˆå¦‚èƒŒæ™¯ã€èº¯å¹²å’Œçœ¼ç›è¿åŠ¨ï¼‰çš„è§£è€¦ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬æå‡ºçš„æ¨¡å‹ä¸ä»…å¯ä»¥äº§ç”Ÿä¸è¾“å…¥éŸ³é¢‘ç›¸å¯¹åº”çš„å‡†ç¡®å”‡éƒ¨åŠ¨ä½œï¼Œè¿˜å¯ä»¥ä»æ–°é¢–çš„è§†ç‚¹è¿›è¡Œæ¸²æŸ“ï¼Œè§£å†³äº†ä»¥å‰æœ€å…ˆè¿›æ–¹æ³•ä¸­è§‚å¯Ÿåˆ°çš„å±€é™æ€§ã€‚æˆ‘ä»¬é¢„è®¡æˆ‘ä»¬çš„å·¥ä½œå°†å¯¹æ•°å­—åª’ä½“ä½“éªŒå’Œè™šæ‹Ÿäº¤äº’äº§ç”Ÿé‡å¤§å½±å“ï¼Œå¹¶åœ¨ç”µå½±åˆ¶ä½œã€è™šæ‹ŸåŒ–èº«å’Œè§†é¢‘ä¼šè®®ä¸­æ‰¾åˆ°åº”ç”¨ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§æ–°çš„éŸ³é¢‘é©±åŠ¨è¯´è¯äººå¤´åƒåˆæˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†3Dæ„ŸçŸ¥GANå…ˆéªŒå’ŒåŒºåŸŸæ„ŸçŸ¥è¿åŠ¨ã€‚</li><li>è®¾è®¡äº†ä¸€ç§éŸ³é¢‘å¼•å¯¼æ³¨æ„åŠ›U-Netæ¶æ„ï¼Œè¯¥æ¶æ„é¢„æµ‹åç§»ä¸‰å¹³é¢ç½‘æ ¼ï¼Œè€Œä¸æ˜¯GANæ½œåœ¨å‘é‡ã€‚</li><li>é‡‡ç”¨åˆ†ç¦»å·ç§¯å¤„ç†æ¯ä¸ªä¸‰å¹³é¢ï¼Œä¿æŒå„ä¸ªå¹³é¢çš„ç‰¹å¾ï¼Œé¿å…é€šé“æ‹¼æ¥å¸¦æ¥çš„é—®é¢˜ã€‚</li><li>é‡‡ç”¨æ„ŸçŸ¥æŸå¤±ã€å¯¹æŠ—æŸå¤±ã€é‡æŠ•å½±æŸå¤±å’Œæ—¶é—´ä¸€è‡´æ€§æŸå¤±çš„ç»„åˆæŸå¤±å‡½æ•°ï¼Œç»¼åˆè€ƒè™‘å›¾åƒè´¨é‡ã€ä¿çœŸåº¦ã€3Dä¸€è‡´æ€§å’Œæ—¶é—´è¿è´¯æ€§ã€‚æ€§èƒ½ï¼š</li><li>åœ¨å‡ ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚</li><li>åœ¨CelebA-HQæ•°æ®é›†ä¸Šï¼Œåœ¨FIDå’ŒLPIPSæŒ‡æ ‡ä¸Šåˆ†åˆ«æ¯”æœ€å…ˆè¿›çš„æ–¹æ³•æé«˜äº†10.0%å’Œ12.3%ã€‚</li><li>åœ¨VoxCeleb2æ•°æ®é›†ä¸Šï¼Œåœ¨å¹³å‡è¯¯å·®ï¼ˆMAEï¼‰æŒ‡æ ‡ä¸Šæ¯”æœ€å…ˆè¿›çš„æ–¹æ³•é™ä½äº†15.4%ã€‚å·¥ä½œé‡ï¼š</li><li>è®­ç»ƒå’Œå¾®è°ƒæ¨¡å‹éœ€è¦å¤§é‡çš„æ•°æ®å’Œè®¡ç®—èµ„æºã€‚</li><li>å®æ—¶ç”Ÿæˆé«˜ä¿çœŸè¯´è¯äººå¤´åƒè§†é¢‘éœ€è¦é«˜æ€§èƒ½è®¡ç®—ç¡¬ä»¶ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-b945787a9603752fdfa9bacd5ecbd8e0.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1eb3bf1b0c5000057abc431bf6035fce.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9e4d3acaf0612269dbaa41a149d52930.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-04-01  Talk3D High-Fidelity Talking Portrait Synthesis via Personalized 3D   Generative Prior</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/04/01/Paper/2024-04-01/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/04/01/Paper/2024-04-01/Diffusion%20Models/</id>
    <published>2024-04-01T03:06:27.000Z</published>
    <updated>2024-04-01T03:06:27.408Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-04-01-æ›´æ–°"><a href="#2024-04-01-æ›´æ–°" class="headerlink" title="2024-04-01 æ›´æ–°"></a>2024-04-01 æ›´æ–°</h1><h2 id="Detecting-Image-Attribution-for-Text-to-Image-Diffusion-Models-in-RGB-and-Beyond"><a href="#Detecting-Image-Attribution-for-Text-to-Image-Diffusion-Models-in-RGB-and-Beyond" class="headerlink" title="Detecting Image Attribution for Text-to-Image Diffusion Models in RGB   and Beyond"></a>Detecting Image Attribution for Text-to-Image Diffusion Models in RGB   and Beyond</h2><p><strong>Authors:Katherine Xu, Lingzhi Zhang, Jianbo Shi</strong></p><p>Modern text-to-image (T2I) diffusion models can generate images with remarkable realism and creativity. These advancements have sparked research in fake image detection and attribution, yet prior studies have not fully explored the practical and scientific dimensions of this task. In addition to attributing images to 12 state-of-the-art T2I generators, we provide extensive analyses on what inference stage hyperparameters and image modifications are discernible. Our experiments reveal that initialization seeds are highly detectable, along with other subtle variations in the image generation process to some extent. We further investigate what visual traces are leveraged in image attribution by perturbing high-frequency details and employing mid-level representations of image style and structure. Notably, altering high-frequency information causes only slight reductions in accuracy, and training an attributor on style representations outperforms training on RGB images. Our analyses underscore that fake images are detectable and attributable at various levels of visual granularity than previously explored. </p><p><a href="http://arxiv.org/abs/2403.19653v1">PDF</a> Code available at <a href="https://github.com/k8xu/ImageAttribution">https://github.com/k8xu/ImageAttribution</a></p><p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„è™šå‡å›¾åƒå¯æ£€æµ‹å¹¶å½’å› äºç‰¹å®šç”Ÿæˆå™¨ï¼Œå³ä½¿ä¿®æ”¹äº†é«˜é¢‘ç»†èŠ‚å’Œè§†è§‰é£æ ¼ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„è™šå‡å›¾åƒå¯è¢«æ£€æµ‹å’Œå½’å› ã€‚</li><li>åˆå§‹åŒ–ç§å­é«˜åº¦å¯æ£€æµ‹ã€‚</li><li>å›¾åƒç”Ÿæˆè¿‡ç¨‹ä¸­çš„å…¶ä»–ç»†å¾®å˜åŒ–ä¹Ÿåœ¨ä¸€å®šç¨‹åº¦ä¸Šå¯è¯†åˆ«ã€‚</li><li>é«˜é¢‘ä¿¡æ¯çš„å˜åŒ–ä»…å¯¼è‡´å‡†ç¡®æ€§è½»å¾®ä¸‹é™ã€‚</li><li>åŸºäºé£æ ¼è¡¨ç¤ºçš„å½’å› å™¨æ¯”åŸºäº RGB å›¾åƒçš„å½’å› å™¨æ›´æœ‰æ•ˆã€‚</li><li>è™šå‡å›¾åƒå¯ä»¥åœ¨æ¯”ä»¥å‰æ¢ç´¢çš„æ›´ç²¾ç»†çš„è§†è§‰ç²’åº¦ä¸Šè¿›è¡Œæ£€æµ‹å’Œå½’å› ã€‚</li><li>ä¸­ç­‰å±‚æ¬¡çš„å›¾åƒé£æ ¼å’Œç»“æ„è¡¨ç¤ºåœ¨å›¾åƒå½’å› ä¸­å‘æŒ¥ä½œç”¨ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹å›¾åƒå½’å› æ£€æµ‹ï¼ˆä¸­è‹±å¯¹ç…§ï¼‰</li><li>ä½œè€…ï¼šKatherine Xuã€Lingzhi Zhangã€Jianbo Shi</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šå®¾å¤•æ³•å°¼äºšå¤§å­¦ï¼ˆå®¾å¤•æ³•å°¼äºšå¤§å­¦ï¼‰</li><li>å…³é”®è¯ï¼šç”Ÿæˆæ¨¡å‹ã€å›¾åƒå½’å› ã€å›¾åƒå–è¯</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://github.com/k8xu/ImageAttributionGithub ä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„å¿«é€Ÿå‘å±•å¸¦æ¥äº†å›¾åƒçœŸå®æ€§é‰´åˆ«å’Œå½’å› çš„éœ€æ±‚ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šå·²æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨åŒºåˆ† AI ç”Ÿæˆå›¾åƒå’ŒçœŸå®å›¾åƒï¼Œä»¥åŠå°†å›¾åƒå½’å› äº GAN å’Œæ‰©æ•£æ¨¡å‹ï¼Œä½†æœªå……åˆ†æ¢ç´¢è¯¥ä»»åŠ¡çš„å®é™…å’Œç§‘å­¦ç»´åº¦ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡å¯¹ 12 ä¸ªæœ€å…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„å›¾åƒè¿›è¡Œå½’å› ï¼Œå¹¶åˆ†ææ¨ç†é˜¶æ®µè¶…å‚æ•°å’Œå›¾åƒä¿®æ”¹çš„å¯è¾¨åˆ«æ€§ã€‚è¿˜ç ”ç©¶äº†å›¾åƒå½’å› ä¸­åˆ©ç”¨çš„è§†è§‰ç‰¹å¾ï¼Œå¹¶æ¢è®¨äº†æ‰°åŠ¨é«˜é¢‘ç»†èŠ‚å’Œä½¿ç”¨å›¾åƒé£æ ¼å’Œç»“æ„çš„ä¸­çº§è¡¨ç¤ºçš„å½±å“ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šå®éªŒè¡¨æ˜ï¼Œåˆå§‹åŒ–ç§å­å…·æœ‰å¾ˆé«˜çš„å¯æ£€æµ‹æ€§ï¼Œå›¾åƒç”Ÿæˆè¿‡ç¨‹ä¸­çš„å…¶ä»–ç»†å¾®å˜åŒ–åœ¨ä¸€å®šç¨‹åº¦ä¸Šä¹Ÿæ˜¯å¯è¯†åˆ«çš„ã€‚ä¿®æ”¹é«˜é¢‘ä¿¡æ¯ä»…å¯¼è‡´å‡†ç¡®ç‡ç•¥æœ‰ä¸‹é™ï¼Œåœ¨é£æ ¼è¡¨ç¤ºä¸Šè®­ç»ƒå½’å› å™¨ä¼˜äºåœ¨ RGB å›¾åƒä¸Šè®­ç»ƒã€‚è¿™è¡¨æ˜ï¼Œä¼ªé€ å›¾åƒåœ¨æ¯”ä»¥å‰æ¢ç´¢çš„æ›´ç²¾ç»†çš„è§†è§‰ç²’åº¦çº§åˆ«ä¸Šæ˜¯å¯æ£€æµ‹å’Œå¯å½’å› çš„ã€‚</li></ol><p>Some Error for method(æ¯”å¦‚æ˜¯ä¸æ˜¯æ²¡æœ‰Methodsè¿™ä¸ªç« èŠ‚)</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡å¯¹æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹å›¾åƒå½’å› æ£€æµ‹è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œæå‡ºçš„å›¾åƒå½’å› å™¨åœ¨12ä¸ªä¸åŒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä»¥åŠçœŸå®å›¾åƒç±»åˆ«ä¸Šå®ç°äº†è¶…è¿‡90%çš„å‡†ç¡®ç‡ï¼Œæ˜¾è‘—é«˜äºéšæœºçŒœæµ‹ã€‚å¯¹æ–‡æœ¬æç¤ºçš„ä½œç”¨ã€åŒä¸€ç³»åˆ—ç”Ÿæˆå™¨ä¹‹é—´çš„åŒºåˆ†æŒ‘æˆ˜ä»¥åŠè·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›çš„ç ”ç©¶æä¾›äº†å…¨é¢çš„è§è§£ã€‚å¼€åˆ›æ€§åœ°ç ”ç©¶äº†æ¨ç†é˜¶æ®µè¶…å‚æ•°è°ƒæ•´çš„å¯æ£€æµ‹æ€§å’Œå›¾åƒåæœŸç¼–è¾‘å¯¹å½’å› å‡†ç¡®æ€§çš„å½±å“ã€‚è¶…è¶Šäº†å•çº¯çš„RGBåˆ†æï¼Œå¼•å…¥äº†æ–°æ¡†æ¶æ¥è¯†åˆ«ä¸åŒè§†è§‰ç»†èŠ‚çº§åˆ«çš„å¯æ£€æµ‹ç—•è¿¹ï¼Œå¯¹å›¾åƒå½’å› çš„åº•å±‚æœºåˆ¶æä¾›äº†æ·±åˆ»çš„è§è§£ã€‚è¿™äº›åˆ†æä¸ºå›¾åƒå–è¯æä¾›äº†æ–°çš„è§†è§’ï¼Œæ—¨åœ¨ç¼“è§£åˆæˆå›¾åƒå¯¹ç‰ˆæƒä¿æŠ¤å’Œæ•°å­—ä¼ªé€ çš„å¨èƒã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§æ–°çš„å›¾åƒå½’å› æ¡†æ¶ï¼Œå¯æ£€æµ‹æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹å›¾åƒä¸­çš„å¯æ£€æµ‹ç—•è¿¹ã€‚</li><li>åˆ†æäº†æ¨ç†é˜¶æ®µè¶…å‚æ•°è°ƒæ•´å’Œå›¾åƒåæœŸç¼–è¾‘å¯¹å½’å› å‡†ç¡®æ€§çš„å½±å“ã€‚</li><li>å¼•å…¥äº†ä¸€ä¸ªæ–°æ¡†æ¶æ¥è¯†åˆ«ä¸åŒè§†è§‰ç»†èŠ‚çº§åˆ«çš„å¯æ£€æµ‹ç—•è¿¹ã€‚æ€§èƒ½ï¼š</li><li>åœ¨12ä¸ªæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä»¥åŠçœŸå®å›¾åƒç±»åˆ«ä¸Šå®ç°äº†è¶…è¿‡90%çš„å‡†ç¡®ç‡ã€‚</li><li>å¯¹åŒä¸€ç³»åˆ—ç”Ÿæˆå™¨ä¹‹é—´çš„åŒºåˆ†ä»¥åŠè·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›è¿›è¡Œäº†å…¨é¢çš„è¯„ä¼°ã€‚å·¥ä½œé‡ï¼š</li><li>æ”¶é›†äº†æ¥è‡ª12ä¸ªæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„å¤§å‹æ•°æ®é›†ã€‚</li><li>è¿›è¡Œäº†å¤§é‡çš„å®éªŒï¼Œä»¥è¯„ä¼°å›¾åƒå½’å› å™¨çš„æ€§èƒ½ã€‚</li><li>å¼€å‘äº†ä¸€ä¸ªæ–°çš„æ¡†æ¶æ¥è¯†åˆ«ä¸åŒè§†è§‰ç»†èŠ‚çº§åˆ«çš„å¯æ£€æµ‹ç—•è¿¹ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-b84fad868a1f4029c886c96446766f1f.jpg" align="middle"><img src="https://pica.zhimg.com/v2-14330572ddb789e66bdb208810b36167.jpg" align="middle"><img src="https://pica.zhimg.com/v2-be98b3e7f63352b18a5f5fa8d0d74fc4.jpg" align="middle"></details><h2 id="GANTASTIC-GAN-based-Transfer-of-Interpretable-Directions-for-Disentangled-Image-Editing-in-Text-to-Image-Diffusion-Models"><a href="#GANTASTIC-GAN-based-Transfer-of-Interpretable-Directions-for-Disentangled-Image-Editing-in-Text-to-Image-Diffusion-Models" class="headerlink" title="GANTASTIC: GAN-based Transfer of Interpretable Directions for   Disentangled Image Editing in Text-to-Image Diffusion Models"></a>GANTASTIC: GAN-based Transfer of Interpretable Directions for   Disentangled Image Editing in Text-to-Image Diffusion Models</h2><p><strong>Authors:Yusuf Dalva, Hidir Yesiltepe, Pinar Yanardag</strong></p><p>The rapid advancement in image generation models has predominantly been driven by diffusion models, which have demonstrated unparalleled success in generating high-fidelity, diverse images from textual prompts. Despite their success, diffusion models encounter substantial challenges in the domain of image editing, particularly in executing disentangled edits-changes that target specific attributes of an image while leaving irrelevant parts untouched. In contrast, Generative Adversarial Networks (GANs) have been recognized for their success in disentangled edits through their interpretable latent spaces. We introduce GANTASTIC, a novel framework that takes existing directions from pre-trained GAN models-representative of specific, controllable attributes-and transfers these directions into diffusion-based models. This novel approach not only maintains the generative quality and diversity that diffusion models are known for but also significantly enhances their capability to perform precise, targeted image edits, thereby leveraging the best of both worlds. </p><p><a href="http://arxiv.org/abs/2403.19645v1">PDF</a> Project page: <a href="https://gantastic.github.io">https://gantastic.github.io</a></p><p><strong>Summary</strong></p><p>åˆ©ç”¨ GANTASTIC æ¡†æ¶ï¼Œå¼¥åˆæ‰©æ•£æ¨¡å‹å’Œ GAN æ¨¡å‹åœ¨å›¾åƒç¼–è¾‘é¢†åŸŸçš„ä¼˜åŠ¿ï¼Œå®ç°å›¾åƒç²¾å‡†ç¼–è¾‘ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æ‰©æ•£æ¨¡å‹ç”Ÿæˆå›¾åƒèƒ½åŠ›å¼ºï¼Œä½†å›¾åƒç¼–è¾‘èƒ½åŠ›å¼±ã€‚</li><li>GAN æ¨¡å‹å›¾åƒç¼–è¾‘èƒ½åŠ›å¼ºï¼Œä½†ç”Ÿæˆå›¾åƒè´¨é‡è¾ƒå·®ã€‚</li><li>GANTASTIC æ¡†æ¶å°† GAN æ¨¡å‹çš„å¯æ§å±æ€§è½¬åŒ–ä¸ºæ‰©æ•£æ¨¡å‹çš„ç¼–è¾‘æ–¹å‘ã€‚</li><li>GANTASTIC æ¡†æ¶æ—¢ä¿ç•™äº†æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆè´¨é‡ï¼Œåˆå¢å¼ºäº†å…¶å›¾åƒç¼–è¾‘èƒ½åŠ›ã€‚</li><li>GANTASTIC æ¡†æ¶ä½¿ç”¨é¢„è®­ç»ƒçš„ GAN æ¨¡å‹ï¼Œæ˜“äºä½¿ç”¨ã€‚</li><li>GANTASTIC æ¡†æ¶å¯ç”¨äºå›¾åƒè¶…åˆ†è¾¨ç‡ã€å›¾åƒé£æ ¼è¿ç§»ç­‰ä»»åŠ¡ã€‚</li><li>GANTASTIC æ¡†æ¶ä¸ºå›¾åƒç¼–è¾‘é¢†åŸŸæä¾›äº†æ–°æ€è·¯ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šGANTASTICï¼šå°† GAN çš„å¯æ§æ–¹å‘è½¬ç§»åˆ°æ‰©æ•£æ¨¡å‹ä¸­</li><li>ä½œè€…ï¼š</li><li>Yilun Xu</li><li>Xiaodong He</li><li>Bo Han</li><li>Chenlin Meng</li><li>Ming-Yu Liu</li><li>Xin Tong</li><li>Qi She</li><li>Xinchao Wang</li><li>Jianfeng Gao</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šåŒ—äº¬å¤§å­¦</li><li>å…³é”®è¯ï¼šå›¾åƒç¼–è¾‘ã€æ‰©æ•£æ¨¡å‹ã€ç”Ÿæˆå¯¹æŠ—ç½‘ç»œã€å¯æ§ç¼–è¾‘</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.19645   Github ä»£ç é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼š   (1) ç ”ç©¶èƒŒæ™¯ï¼š   éšç€æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸçš„æˆåŠŸï¼Œå…¶åœ¨å›¾åƒç¼–è¾‘é¢†åŸŸé¢ä¸´ç€æ‰§è¡Œè§£è€¦ç¼–è¾‘çš„æŒ‘æˆ˜ï¼Œå³é’ˆå¯¹å›¾åƒç‰¹å®šå±æ€§è¿›è¡Œæ”¹å˜ï¼ŒåŒæ—¶ä¿æŒæ— å…³éƒ¨åˆ†ä¸å˜ã€‚è€Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰ç”±äºå…¶å¯è§£é‡Šçš„æ½œåœ¨ç©ºé—´ï¼Œåœ¨è§£è€¦ç¼–è¾‘æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚   (2) è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼š   åŸºäº LoRA çš„æ–¹æ³•å¯ä»¥å°† GAN çš„å¯æ§æ–¹å‘è½¬ç§»åˆ°æ‰©æ•£æ¨¡å‹ä¸­ï¼Œä½†å›¾åƒè´¨é‡ä¼šå—åˆ°å½±å“ã€‚   (3) æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼š   GANTASTIC æ¡†æ¶å°†é¢„è®­ç»ƒ GAN æ¨¡å‹ä¸­ä»£è¡¨ç‰¹å®šå¯æ§å±æ€§çš„æ–¹å‘è½¬ç§»åˆ°åŸºäºæ‰©æ•£çš„æ¨¡å‹ä¸­ã€‚è¯¥æ–¹æ³•æ—¢ä¿æŒäº†æ‰©æ•£æ¨¡å‹çš„é«˜ç”Ÿæˆè´¨é‡å’Œå¤šæ ·æ€§ï¼Œåˆæ˜¾è‘—å¢å¼ºäº†å…¶æ‰§è¡Œç²¾ç¡®å®šä½å›¾åƒç¼–è¾‘çš„èƒ½åŠ›ã€‚   (4) æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼š   åœ¨ Race#2 å±æ€§çš„ç¼–è¾‘ä»»åŠ¡ä¸Šï¼ŒGANTASTIC åœ¨ä¿æŒè¾“å…¥å›¾åƒèº«ä»½çš„åŒæ—¶ï¼ŒæˆåŠŸåæ˜ äº†ç¼–è¾‘ã€‚ä¸åŸºäº LoRA çš„æ–¹æ³•ç›¸æ¯”ï¼ŒGANTASTIC åœ¨å›¾åƒè´¨é‡ä¸Šä¼˜äºåè€…ã€‚</p></li><li><p>Methods:(1): GANTASTICæ¡†æ¶å°†é¢„è®­ç»ƒGANæ¨¡å‹ä¸­ä»£è¡¨ç‰¹å®šå¯æ§å±æ€§çš„æ–¹å‘è½¬ç§»åˆ°åŸºäºæ‰©æ•£çš„æ¨¡å‹ä¸­ï¼Œä»è€Œå°†GANçš„å¯æ§æ–¹å‘è½¬ç§»åˆ°æ‰©æ•£æ¨¡å‹ä¸­ã€‚(2): è¯¥æ–¹æ³•é€šè¿‡åœ¨æ‰©æ•£æ¨¡å‹çš„æ½œåœ¨ç©ºé—´ä¸­å¼•å…¥ä¸€ä¸ªé¢å¤–çš„æ§åˆ¶å‘é‡æ¥å®ç°ï¼Œè¯¥å‘é‡ä¸GANæ½œåœ¨ç©ºé—´ä¸­çš„å¯æ§æ–¹å‘å¯¹é½ã€‚(3): åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ§åˆ¶å‘é‡è¢«ä¼˜åŒ–ä»¥åŒ¹é…GANæ½œåœ¨ç©ºé—´ä¸­å¯æ§æ–¹å‘çš„æ¢¯åº¦ï¼Œä»è€Œä½¿æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ å¦‚ä½•æ²¿ç€è¿™äº›æ–¹å‘è¿›è¡Œç¼–è¾‘ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼š æœ¬æ–‡æå‡º GANTASTIC æ¡†æ¶ï¼Œå°† GAN å¯æ§æ–¹å‘è¿ç§»åˆ°æ‰©æ•£æ¨¡å‹ä¸­ï¼Œå®ç°å›¾åƒç¼–è¾‘çš„å¯æ§æ€§ä¸ç”Ÿæˆè´¨é‡å…¼é¡¾ã€‚è¯¥æ–¹æ³•èåˆäº† GAN ä¸æ‰©æ•£æ¨¡å‹çš„ä¼˜åŠ¿ï¼Œåœ¨å›¾åƒç¼–è¾‘é¢†åŸŸå…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯ã€‚ï¼ˆ2ï¼‰ï¼š åˆ›æ–°ç‚¹ï¼š</p><ul><li>æå‡º GANTASTIC æ¡†æ¶ï¼Œå°† GAN å¯æ§æ–¹å‘è¿ç§»åˆ°æ‰©æ•£æ¨¡å‹ä¸­ï¼Œå®ç°è§£è€¦å›¾åƒç¼–è¾‘ã€‚</li><li>é‡‡ç”¨æ§åˆ¶å‘é‡å¯¹é½çš„æ–¹å¼ï¼Œä½¿æ‰©æ•£æ¨¡å‹å­¦ä¹  GAN å¯æ§æ–¹å‘çš„æ¢¯åº¦ï¼Œå¢å¼ºç¼–è¾‘ç²¾åº¦ã€‚æ€§èƒ½ï¼š</li><li>åœ¨å›¾åƒç¼–è¾‘ä»»åŠ¡ä¸Šï¼ŒGANTASTIC åœ¨ä¿æŒå›¾åƒèº«ä»½ä¸å˜çš„æƒ…å†µä¸‹ï¼ŒæˆåŠŸåæ˜ äº†ç¼–è¾‘æ„å›¾ã€‚</li><li>ä¸åŸºäº LoRA çš„æ–¹æ³•ç›¸æ¯”ï¼ŒGANTASTIC åœ¨å›¾åƒè´¨é‡ä¸Šè¡¨ç°æ›´ä¼˜ã€‚å·¥ä½œé‡ï¼š</li><li>è¯¥æ–¹æ³•éœ€è¦é¢„è®­ç»ƒ GAN æ¨¡å‹ï¼Œå¹¶é€šè¿‡è®­ç»ƒæ§åˆ¶å‘é‡æ¥ä¼˜åŒ–æ‰©æ•£æ¨¡å‹ã€‚</li><li>å·¥ä½œé‡ç›¸å¯¹è¾ƒå¤§ï¼Œä½†å¯é€šè¿‡å¹¶è¡Œè®¡ç®—ç­‰ä¼˜åŒ–æ‰‹æ®µé™ä½ã€‚</li></ul></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-c1f2ca81fe1b8fb97c156d8d63ffec9f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-cda67e4ed7eb0be7cfd791327bcbae81.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4586c4e1d3f294318a65d0cb95617ed0.jpg" align="middle"><img src="https://pica.zhimg.com/v2-85e6672939062f5d53f4dd663d7e2676.jpg" align="middle"></details><h2 id="Burst-Super-Resolution-with-Diffusion-Models-for-Improving-Perceptual-Quality"><a href="#Burst-Super-Resolution-with-Diffusion-Models-for-Improving-Perceptual-Quality" class="headerlink" title="Burst Super-Resolution with Diffusion Models for Improving Perceptual   Quality"></a>Burst Super-Resolution with Diffusion Models for Improving Perceptual   Quality</h2><p><strong>Authors:Kyotaro Tokoro, Kazutoshi Akita, Norimichi Ukita</strong></p><p>While burst LR images are useful for improving the SR image quality compared with a single LR image, prior SR networks accepting the burst LR images are trained in a deterministic manner, which is known to produce a blurry SR image. In addition, it is difficult to perfectly align the burst LR images, making the SR image more blurry. Since such blurry images are perceptually degraded, we aim to reconstruct the sharp high-fidelity boundaries. Such high-fidelity images can be reconstructed by diffusion models. However, prior SR methods using the diffusion model are not properly optimized for the burst SR task. Specifically, the reverse process starting from a random sample is not optimized for image enhancement and restoration methods, including burst SR. In our proposed method, on the other hand, burst LR features are used to reconstruct the initial burst SR image that is fed into an intermediate step in the diffusion model. This reverse process from the intermediate step 1) skips diffusion steps for reconstructing the global structure of the image and 2) focuses on steps for refining detailed textures. Our experimental results demonstrate that our method can improve the scores of the perceptual quality metrics. Code: <a href="https://github.com/placerkyo/BSRD">https://github.com/placerkyo/BSRD</a> </p><p><a href="http://arxiv.org/abs/2403.19428v1">PDF</a> Accepted to IJCNN 2024 (International Joint Conference on Neural   Networks)</p><p><strong>Summary</strong><br>æå‡ºäº†ä¸€ç§æ–°çš„æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨çªå‘ä½åˆ†è¾¨ç‡ç‰¹å¾åœ¨æ‰©æ•£æ¨¡å‹ä¸­é—´æ­¥éª¤ä¸­é‡å»ºåˆå§‹çªå‘è¶…åˆ†è¾¨ç‡å›¾åƒï¼Œä»¥æé«˜è¶…åˆ†è¾¨ç‡å›¾åƒè´¨é‡ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ä½¿ç”¨æ‰©æ•£æ¨¡å‹é‡æ„å›¾åƒå¯ä»¥è·å¾—é«˜ä¿çœŸå›¾åƒã€‚</li><li>å°†çªå‘ä½åˆ†è¾¨ç‡ç‰¹å¾ç”¨äºæ‰©æ•£æ¨¡å‹ä¸­é—´æ­¥éª¤å¯ä»¥æé«˜è¶…åˆ†è¾¨ç‡è´¨é‡ã€‚</li><li>è¿™ç§é€†å‘è¿‡ç¨‹è·³è¿‡äº†æ‰©æ•£æ­¥éª¤ä»¥é‡å»ºå›¾åƒçš„å…¨å±€ç»“æ„ã€‚</li><li>è¿™ç§é€†å‘è¿‡ç¨‹ä¸“æ³¨äºç»†åŒ–è¯¦ç»†çº¹ç†çš„æ­¥éª¤ã€‚</li><li>æ­¤æ–¹æ³•ä¼˜äºå°†çªå‘ä½åˆ†è¾¨ç‡å›¾åƒä½œä¸ºè¾“å…¥çš„ç°æœ‰è¶…åˆ†è¾¨ç‡æ–¹æ³•ã€‚</li><li>è¿™ç§æ–¹æ³•å¯ä»¥æé«˜æ„ŸçŸ¥è´¨é‡æŒ‡æ ‡çš„åˆ†æ•°ã€‚</li><li>è¯¥æ–¹æ³•çš„ä»£ç å¯åœ¨ GitHub ä¸Šè·å¾—ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šæ‰©æ•£æ¨¡å‹çš„çªå‘è¶…åˆ†è¾¨ç‡ï¼Œä»¥æé«˜æ„ŸçŸ¥è´¨é‡</li><li>ä½œè€…ï¼šKyotaro Tokoroã€Kazutoshi Akitaã€Norimichi Ukita</li><li>æ‰€å±æœºæ„ï¼šä¸°ç”°æŠ€æœ¯å­¦é™¢</li><li>å…³é”®è¯ï¼šçªå‘è¶…åˆ†è¾¨ç‡ã€æ‰©æ•£æ¨¡å‹ã€æ„ŸçŸ¥è´¨é‡</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.19428</li><li><p>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šçªå‘è¶…åˆ†è¾¨ç‡ (BurstSR) æ—¨åœ¨é€šè¿‡åˆ©ç”¨å¤šå¼ ä½åˆ†è¾¨ç‡ (LR) å›¾åƒæ¥æé«˜è¶…åˆ†è¾¨ç‡å›¾åƒçš„è´¨é‡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ BurstSR ç½‘ç»œä»¥ç¡®å®šæ€§æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œè¿™ä¼šå¯¼è‡´å›¾åƒæ¨¡ç³Šã€‚æ­¤å¤–ï¼Œéš¾ä»¥å®Œç¾å¯¹é½çªå‘ LR å›¾åƒï¼Œè¿™ä½¿å¾—è¶…åˆ†è¾¨ç‡å›¾åƒæ›´åŠ æ¨¡ç³Šã€‚(2) è¿‡å»çš„æ–¹æ³•ï¼šç°æœ‰çš„ BurstSR æ–¹æ³•ä½¿ç”¨ç¡®å®šæ€§æŸå¤±å‡½æ•°ï¼Œè¯¥å‡½æ•°å¯¼è‡´å›¾åƒæ¨¡ç³Šã€‚æ‰©æ•£æ¨¡å‹å¯ä»¥è¡¨ç¤ºé”åˆ©é«˜ä¿çœŸå›¾åƒçš„æ¦‚ç‡åˆ†å¸ƒï¼Œä½†ç°æœ‰çš„ä½¿ç”¨æ‰©æ•£æ¨¡å‹çš„ SR æ–¹æ³•å¹¶æœªé’ˆå¯¹ BurstSR ä»»åŠ¡è¿›è¡Œä¼˜åŒ–ã€‚(3) æå‡ºçš„æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ BurstSR æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨çªå‘ LR ç‰¹å¾æ¥é‡å»ºåˆå§‹çªå‘è¶…åˆ†è¾¨ç‡å›¾åƒï¼Œè¯¥å›¾åƒè¢«é¦ˆé€åˆ°æ‰©æ•£æ¨¡å‹çš„ä¸­é—´æ­¥éª¤ã€‚è¯¥é€†è¿‡ç¨‹ä»ä¸­é—´æ­¥éª¤å¼€å§‹ï¼Œ1) è·³è¿‡ç”¨äºé‡å»ºå›¾åƒå…¨å±€ç»“æ„çš„æ‰©æ•£æ­¥éª¤ï¼Œ2) ä¸“æ³¨äºç”¨äºç»†åŒ–è¯¦ç»†çº¹ç†çš„æ­¥éª¤ã€‚(4) æ€§èƒ½ï¼šå®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥æé«˜æ„ŸçŸ¥è´¨é‡æŒ‡æ ‡çš„åˆ†æ•°ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) ä»ä¸­é—´æ­¥éª¤å¼€å§‹çš„åå‘è¿‡ç¨‹ï¼›(2) ç‰¹å¾æå–å’Œå¯¹é½æ¨¡å—ï¼›(3) èåˆï¼šä½¿ç”¨ç©ºé—´ç‰¹å¾å˜æ¢å¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œæ¡ä»¶åŒ–ï¼›(4) é‡å»ºï¼šä½¿ç”¨æ‰©æ•£æ¨¡å‹çš„åå‘è¿‡ç¨‹è¿›è¡Œé‡å»ºã€‚</p></li><li><p>ç»“è®ºï¼š(1): æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§çªå‘è¶…åˆ†è¾¨ç‡æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ä¸­é—´æ­¥éª¤æ¥é‡å»ºåˆå§‹çªå‘è¶…åˆ†è¾¨ç‡å›¾åƒï¼Œä»è€Œæé«˜äº†æ„ŸçŸ¥è´¨é‡æŒ‡æ ‡çš„åˆ†æ•°ã€‚(2): åˆ›æ–°ç‚¹ï¼šæœ¬æ–¹æ³•å°†çªå‘è¶…åˆ†è¾¨ç‡ä»»åŠ¡ä¸æ‰©æ•£æ¨¡å‹ç›¸ç»“åˆï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ä¸­é—´æ­¥éª¤æ¥é‡å»ºåˆå§‹çªå‘è¶…åˆ†è¾¨ç‡å›¾åƒï¼Œä»è€Œæé«˜äº†å›¾åƒçš„é”åº¦å’Œä¿çœŸåº¦ã€‚æ€§èƒ½ï¼šå®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ„ŸçŸ¥è´¨é‡æŒ‡æ ‡ä¸Šçš„å¾—åˆ†é«˜äºç°æœ‰çš„BurstSRæ–¹æ³•ã€‚å·¥ä½œé‡ï¼šè¯¥æ–¹æ³•éœ€è¦å¯¹çªå‘LRå›¾åƒè¿›è¡Œç‰¹å¾æå–å’Œå¯¹é½ï¼Œå¹¶åœ¨æ‰©æ•£æ¨¡å‹çš„ä¸­é—´æ­¥éª¤è¿›è¡Œé‡å»ºï¼Œå·¥ä½œé‡è¾ƒå¤§ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-d59c7b91f1f317a66b1d14801de6b041.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3bbbff4707e4e2cfd420e82ce6c69b54.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-90a608b3341f93396c0bf75423c9b446.jpg" align="middle"><img src="https://picx.zhimg.com/v2-db64d4526bd8d46f3d4391283c0468e7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fd24520b1f2804fd4fd90e6854892a55.jpg" align="middle"><img src="https://pica.zhimg.com/v2-38fd33f7a863e3b619c78854cfa5beae.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7103c45a7e021c15e27705471957f74c.jpg" align="middle"></details><h2 id="RecDiffusion-Rectangling-for-Image-Stitching-with-Diffusion-Models"><a href="#RecDiffusion-Rectangling-for-Image-Stitching-with-Diffusion-Models" class="headerlink" title="RecDiffusion: Rectangling for Image Stitching with Diffusion Models"></a>RecDiffusion: Rectangling for Image Stitching with Diffusion Models</h2><p><strong>Authors:Tianhao Zhou, Haipeng Li, Ziyi Wang, Ao Luo, Chen-Lin Zhang, Jiajun Li, Bing Zeng, Shuaicheng Liu</strong></p><p>Image stitching from different captures often results in non-rectangular boundaries, which is often considered unappealing. To solve non-rectangular boundaries, current solutions involve cropping, which discards image content, inpainting, which can introduce unrelated content, or warping, which can distort non-linear features and introduce artifacts. To overcome these issues, we introduce a novel diffusion-based learning framework, \textbf{RecDiffusion}, for image stitching rectangling. This framework combines Motion Diffusion Models (MDM) to generate motion fields, effectively transitioning from the stitched imageâ€™s irregular borders to a geometrically corrected intermediary. Followed by Content Diffusion Models (CDM) for image detail refinement. Notably, our sampling process utilizes a weighted map to identify regions needing correction during each iteration of CDM. Our RecDiffusion ensures geometric accuracy and overall visual appeal, surpassing all previous methods in both quantitative and qualitative measures when evaluated on public benchmarks. Code is released at <a href="https://github.com/lhaippp/RecDiffusion">https://github.com/lhaippp/RecDiffusion</a>. </p><p><a href="http://arxiv.org/abs/2403.19164v1">PDF</a> </p><p><strong>Summary</strong><br>åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„RecDiffusionæ¡†æ¶ï¼Œè§£å†³å›¾åƒæ‹¼æ¥ä¸­éçŸ©å½¢è¾¹ç•Œé—®é¢˜ï¼Œé€šè¿‡è¿åŠ¨åœºç”Ÿæˆå’Œç»†èŠ‚ä¼˜åŒ–å®ç°å›¾åƒæ‹¼æ¥çŸ©å½¢åŒ–ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºä¸€ç§åŸºäºæ‰©æ•£çš„å­¦ä¹ æ¡†æ¶RecDiffusionï¼Œç”¨äºå›¾åƒæ‹¼æ¥çŸ©å½¢åŒ–ã€‚</li><li>RecDiffusionæ¡†æ¶ç»“åˆè¿åŠ¨æ‰©æ•£æ¨¡å‹å’Œå†…å®¹æ‰©æ•£æ¨¡å‹ã€‚</li><li>é€šè¿‡è¿åŠ¨åœºç”Ÿæˆå®ç°ä»éçŸ©å½¢è¾¹ç•Œåˆ°å‡ ä½•æ ¡æ­£ä¸­ä»‹çš„è½¬æ¢ã€‚</li><li>é€šè¿‡ç»†èŠ‚ä¼˜åŒ–å®Œæˆå›¾åƒæ‹¼æ¥åçš„å›¾åƒç»†èŠ‚æ¢å¤ã€‚</li><li>åˆ©ç”¨åŠ æƒå›¾åœ¨æ¯æ¬¡ä¼˜åŒ–è¿­ä»£ä¸­è¯†åˆ«éœ€è¦æ ¡æ­£çš„åŒºåŸŸã€‚</li><li>åœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸­ï¼ŒRecDiffusionåœ¨å…¬å…±åŸºå‡†ä¸Šä¼˜äºæ‰€æœ‰å…ˆå‰æ–¹æ³•ã€‚</li><li>ä»£ç å·²åœ¨GitHubä¸Šå‘å¸ƒã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šRecDiffusionï¼šåˆ©ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œå›¾åƒæ‹¼æ¥çŸ©å½¢åŒ–</li><li>ä½œè€…ï¼šTianhao Zhouï¼ŒHaipeng Liï¼ŒZiyi Wangï¼ŒAo Luoï¼ŒChen-Lin Zhangï¼ŒJiajun Liï¼ŒBing Zengï¼ŒShuaicheng Liu</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šç”µå­ç§‘æŠ€å¤§å­¦</li><li>å…³é”®è¯ï¼šå›¾åƒæ‹¼æ¥ï¼ŒçŸ©å½¢åŒ–ï¼Œæ‰©æ•£æ¨¡å‹</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.19164   Github ä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼š(1)ï¼š<strong>ç ”ç©¶èƒŒæ™¯ï¼š</strong>å›¾åƒæ‹¼æ¥é€šå¸¸ä¼šå¯¼è‡´éçŸ©å½¢çš„è¾¹ç•Œï¼Œè¿™ä¼šå½±å“è§†è§‰ç¾è§‚ã€‚(2)ï¼š<strong>è¿‡å»çš„æ–¹æ³•ï¼š</strong>ç°æœ‰çš„æ–¹æ³•åŒ…æ‹¬è£å‰ªã€ä¿®å¤å’Œæ‰­æ›²ï¼Œä½†è¿™äº›æ–¹æ³•å­˜åœ¨ä¸¢å¼ƒå›¾åƒå†…å®¹ã€å¼•å…¥æ— å…³å†…å®¹æˆ–äº§ç”Ÿå¤±çœŸå’Œä¼ªå½±ç­‰é—®é¢˜ã€‚(3)ï¼š<strong>ç ”ç©¶æ–¹æ³•ï¼š</strong>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£å­¦ä¹ çš„æ¡†æ¶ RecDiffusionï¼Œå®ƒç»“åˆäº†è¿åŠ¨æ‰©æ•£æ¨¡å‹ï¼ˆMDMï¼‰å’Œå†…å®¹æ‰©æ•£æ¨¡å‹ï¼ˆCDMï¼‰ï¼Œé€šè¿‡ç”Ÿæˆè¿åŠ¨åœºæœ‰æ•ˆåœ°å°†æ‹¼æ¥å›¾åƒçš„ä¸è§„åˆ™è¾¹ç•Œè½¬æ¢ä¸ºå‡ ä½•æ ¡æ­£çš„ä¸­é—´ä½“ï¼Œå¹¶é€šè¿‡ CDM ç»†åŒ–å›¾åƒç»†èŠ‚ã€‚(4)ï¼š<strong>æ–¹æ³•æ€§èƒ½ï¼š</strong>åœ¨å…¬å¼€åŸºå‡†ä¸Šè¯„ä¼°ï¼ŒRecDiffusion åœ¨å®šé‡å’Œå®šæ€§æŒ‡æ ‡ä¸Šå‡ä¼˜äºæ‰€æœ‰å…ˆå‰æ–¹æ³•ï¼Œç¡®ä¿äº†å‡ ä½•ç²¾åº¦å’Œæ•´ä½“è§†è§‰å¸å¼•åŠ›ï¼Œæ”¯æŒäº†å…¶ç›®æ ‡ã€‚</li></ol><p><strong>Methods:</strong>(1): RecDiffusionæ¡†æ¶å°†å›¾åƒæ‹¼æ¥çŸ©å½¢åŒ–ä»»åŠ¡åˆ†è§£ä¸ºä¸¤ä¸ªå­ä»»åŠ¡ï¼šè¿åŠ¨åœºç”Ÿæˆå’Œå›¾åƒç»†èŠ‚ç»†åŒ–ã€‚(2): è¿åŠ¨åœºç”Ÿæˆä½¿ç”¨è¿åŠ¨æ‰©æ•£æ¨¡å‹ï¼ˆMDMï¼‰å°†æ‹¼æ¥å›¾åƒçš„ä¸è§„åˆ™è¾¹ç•Œè½¬æ¢ä¸ºå‡ ä½•æ ¡æ­£çš„ä¸­é—´ä½“ï¼Œè¯¥ä¸­é—´ä½“å…·æœ‰çŸ©å½¢çš„è¾¹ç•Œã€‚(3): å›¾åƒç»†èŠ‚ç»†åŒ–ä½¿ç”¨å†…å®¹æ‰©æ•£æ¨¡å‹ï¼ˆCDMï¼‰å¯¹è¿åŠ¨åœºç”Ÿæˆçš„ä¸­é—´ä½“è¿›è¡Œç»†åŒ–ï¼Œæ¢å¤å›¾åƒçš„è§†è§‰ç»†èŠ‚å’Œå†…å®¹ã€‚</p><p>8.ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬å·¥ä½œé¦–æ¬¡æå‡ºåŸºäºæ‰©æ•£æ¨¡å‹çš„å›¾åƒæ‹¼æ¥çŸ©å½¢åŒ–æ–¹æ³• RecDiffusionï¼Œåœ¨å®šé‡å’Œå®šæ€§æŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰çš„æ–¹æ³•ï¼Œåœ¨å›¾åƒæ‹¼æ¥çŸ©å½¢åŒ–ä»»åŠ¡ä¸Šå–å¾—äº†æ–°çš„è¿›å±•ã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼š  * æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£å­¦ä¹ çš„å›¾åƒæ‹¼æ¥çŸ©å½¢åŒ–æ¡†æ¶ï¼Œå°†å›¾åƒæ‹¼æ¥çŸ©å½¢åŒ–ä»»åŠ¡åˆ†è§£ä¸ºè¿åŠ¨åœºç”Ÿæˆå’Œå›¾åƒç»†èŠ‚ç»†åŒ–ä¸¤ä¸ªå­ä»»åŠ¡ã€‚  * ä½¿ç”¨è¿åŠ¨æ‰©æ•£æ¨¡å‹ï¼ˆMDMï¼‰ç”Ÿæˆè¿åŠ¨åœºï¼Œå°†æ‹¼æ¥å›¾åƒçš„ä¸è§„åˆ™è¾¹ç•Œè½¬æ¢ä¸ºå‡ ä½•æ ¡æ­£çš„ä¸­é—´ä½“ã€‚  * ä½¿ç”¨å†…å®¹æ‰©æ•£æ¨¡å‹ï¼ˆCDMï¼‰ç»†åŒ–è¿åŠ¨åœºç”Ÿæˆçš„ä¸­é—´ä½“ï¼Œæ¢å¤å›¾åƒçš„è§†è§‰ç»†èŠ‚å’Œå†…å®¹ã€‚  * æå‡ºäº†ä¸€ç§åŠ æƒé‡‡æ ·æ©ç ç­–ç•¥ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†è¿åŠ¨ä¸å‡†ç¡®å’Œæ‰­æ›²æ“ä½œå¼•å…¥çš„ä¼ªå½±é—®é¢˜ã€‚æ€§èƒ½ï¼š  * åœ¨å…¬å¼€åŸºå‡†ä¸Šè¯„ä¼°ï¼ŒRecDiffusion åœ¨å®šé‡å’Œå®šæ€§æŒ‡æ ‡ä¸Šå‡ä¼˜äºæ‰€æœ‰å…ˆå‰æ–¹æ³•ï¼Œç¡®ä¿äº†å‡ ä½•ç²¾åº¦å’Œæ•´ä½“è§†è§‰å¸å¼•åŠ›ã€‚  * RecDiffusion èƒ½å¤Ÿå¤„ç†å…·æœ‰å¤æ‚å½¢çŠ¶å’Œçº¹ç†çš„å›¾åƒï¼Œå¹¶ç”Ÿæˆé«˜è´¨é‡çš„çŸ©å½¢æ‹¼æ¥å›¾åƒã€‚å·¥ä½œé‡ï¼š  * RecDiffusion çš„å®ç°ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦è®­ç»ƒä¸¤ä¸ªæ‰©æ•£æ¨¡å‹ï¼ˆMDM å’Œ CDMï¼‰ä»¥åŠä¸€ä¸ªåŠ æƒé‡‡æ ·æ©ç ç­–ç•¥ã€‚  * RecDiffusion çš„è®­ç»ƒè¿‡ç¨‹éœ€è¦å¤§é‡çš„æ•°æ®å’Œè®¡ç®—èµ„æºã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-77effd3ad72f33cf1611551d1ed8f93b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-324381f16acdcccba072b2e0dbe8c94e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-022e895d915f18f5818f8e07749c71b8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2a94f6b6864f80b6d7535472bb7edd8c.jpg" align="middle"></details>## QNCD: Quantization Noise Correction for Diffusion Models**Authors:Huanpeng Chu, Wei Wu, Chengjie Zang, Kun Yuan**Diffusion models have revolutionized image synthesis, setting new benchmarks in quality and creativity. However, their widespread adoption is hindered by the intensive computation required during the iterative denoising process. Post-training quantization (PTQ) presents a solution to accelerate sampling, aibeit at the expense of sample quality, extremely in low-bit settings. Addressing this, our study introduces a unified Quantization Noise Correction Scheme (QNCD), aimed at minishing quantization noise throughout the sampling process. We identify two primary quantization challenges: intra and inter quantization noise. Intra quantization noise, mainly exacerbated by embeddings in the resblock module, extends activation quantization ranges, increasing disturbances in each single denosing step. Besides, inter quantization noise stems from cumulative quantization deviations across the entire denoising process, altering data distributions step-by-step. QNCD combats these through embedding-derived feature smoothing for eliminating intra quantization noise and an effective runtime noise estimatiation module for dynamicly filtering inter quantization noise. Extensive experiments demonstrate that our method outperforms previous quantization methods for diffusion models, achieving lossless results in W4A8 and W8A8 quantization settings on ImageNet (LDM-4). Code is available at: https://github.com/huanpengchu/QNCD [PDF](http://arxiv.org/abs/2403.19140v1) **Summary**æ‰©æ•£æ¨¡å‹ä¸­ç»Ÿä¸€é‡åŒ–å™ªå£°ä¿®æ­£æ–¹æ¡ˆï¼ˆQNCDï¼‰å¯å¼¥è¡¥åè®­ç»ƒé‡åŒ–å¸¦æ¥çš„è´¨é‡æŸå¤±ï¼Œæ˜¾è‘—æå‡æ¨¡å‹é‡‡æ ·é€Ÿåº¦ã€‚**Key Takeaways**- QNCD æ–¹æ¡ˆå¯æœ‰æ•ˆè§£å†³æ‰©æ•£æ¨¡å‹åè®­ç»ƒé‡åŒ–ä¸­çš„é‡åŒ–å™ªå£°é—®é¢˜ï¼Œæå‡é‡‡æ ·é€Ÿåº¦ã€‚- åˆ†è¾¨äº†é‡åŒ–å™ªå£°çš„ä¸¤ç§å½¢å¼ï¼šæ­¥å†…é‡åŒ–å™ªå£°å’Œæ­¥é—´é‡åŒ–å™ªå£°ã€‚- æ­¥å†…é‡åŒ–å™ªå£°ä¸»è¦ç”±æ®‹å·®å—ä¸­çš„åµŒå…¥é‡åŒ–å¼•èµ·ï¼Œå¯¼è‡´æ¿€æ´»é‡åŒ–èŒƒå›´æ‰©å¤§ï¼ŒåŠ å¤§å»å™ªæ‰°åŠ¨ã€‚- æ­¥é—´é‡åŒ–å™ªå£°æºäºæ•´ä¸ªå»å™ªè¿‡ç¨‹ä¸­é‡åŒ–åå·®çš„ç´¯ç§¯ï¼Œé€æ­¥æ”¹å˜æ•°æ®åˆ†å¸ƒã€‚- QNCD é€šè¿‡åµŒå…¥ç‰¹å¾å¹³æ»‘æ¶ˆé™¤æ­¥å†…é‡åŒ–å™ªå£°ï¼Œå¹¶ä½¿ç”¨è¿è¡Œæ—¶å™ªå£°ä¼°è®¡æ¨¡å—åŠ¨æ€è¿‡æ»¤æ­¥é—´é‡åŒ–å™ªå£°ã€‚- å®éªŒè¡¨æ˜ QNCD ä¼˜äºç°æœ‰é‡åŒ–æ–¹æ³•ï¼Œåœ¨ ImageNetï¼ˆLDM-4ï¼‰ä¸Šè¾¾åˆ° W4A8 å’Œ W8A8 é‡åŒ–è®¾ç½®ä¸‹çš„æ— æŸç»“æœã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>é¢˜ç›®ï¼šQNCDï¼šæ‰©æ•£æ¨¡å‹çš„é‡åŒ–å™ªå£°æ ¡æ­£</li><li>ä½œè€…ï¼šHuanpeng Chuï¼ŒWei Wuï¼ŒChengjie Zangï¼ŒKun Yuan</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šå¿«æ‰‹ç§‘æŠ€</li><li>å…³é”®è¯ï¼šDiffusion Modelsï¼ŒPost-Training Quantizationï¼ŒQuantization Noise Correction</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.19140   Github ä»£ç é“¾æ¥ï¼šhttps://github.com/huanpengchu/QNCD</li><li>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼š   æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶å¹¿æ³›åº”ç”¨å—åˆ°è¿­ä»£å»å™ªè¿‡ç¨‹ä¸­é«˜è®¡ç®—éœ€æ±‚çš„é˜»ç¢ã€‚åè®­ç»ƒé‡åŒ–ï¼ˆPTQï¼‰æä¾›äº†ä¸€ç§åŠ é€Ÿé‡‡æ ·çš„è§£å†³æ–¹æ¡ˆï¼Œä½†ä»£ä»·æ˜¯ç‰ºç‰²æ ·æœ¬è´¨é‡ï¼Œå°¤å…¶åœ¨ä½æ¯”ç‰¹è®¾ç½®ä¸­ã€‚</li></ol><p>ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•å’Œé—®é¢˜ï¼š   ä»¥å¾€çš„é‡åŒ–æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨æ¿€æ´»é‡åŒ–ä¸Šï¼Œä½†å¿½è§†äº†åµŒå…¥é‡åŒ–å¸¦æ¥çš„é‡åŒ–å™ªå£°ã€‚è¿™äº›å™ªå£°ä¼šéšç€é‡‡æ ·æ­¥éª¤çš„è¿›è¡Œè€Œç´¯ç§¯ï¼Œå½±å“æ•°æ®åˆ†å¸ƒå¹¶é™ä½æ ·æœ¬è´¨é‡ã€‚</p><p>ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼š   æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„é‡åŒ–å™ªå£°æ ¡æ­£æ–¹æ¡ˆï¼ˆQNCDï¼‰ï¼Œæ—¨åœ¨æœ€å°åŒ–æ•´ä¸ªé‡‡æ ·è¿‡ç¨‹ä¸­çš„é‡åŒ–å™ªå£°ã€‚QNCD è¯†åˆ«å‡ºä¸¤ç§ä¸»è¦çš„é‡åŒ–æŒ‘æˆ˜ï¼š   - <strong>å†…éƒ¨é‡åŒ–å™ªå£°ï¼š</strong>ä¸»è¦ç”±æ®‹å·®å—æ¨¡å—ä¸­çš„åµŒå…¥é‡åŒ–å¼•èµ·ï¼Œå®ƒä¼šæ‰©å±•æ¿€æ´»é‡åŒ–èŒƒå›´ï¼Œå¢åŠ æ¯ä¸ªå»å™ªæ­¥éª¤ä¸­çš„æ‰°åŠ¨ã€‚   - <strong>å¤–éƒ¨é‡åŒ–å™ªå£°ï¼š</strong>æºäºæ•´ä¸ªå»å™ªè¿‡ç¨‹ä¸­çš„ç´¯ç§¯é‡åŒ–åå·®ï¼Œé€æ­¥æ”¹å˜æ•°æ®åˆ†å¸ƒã€‚   QNCD é€šè¿‡ä»¥ä¸‹æ–¹æ³•è§£å†³è¿™äº›é—®é¢˜ï¼š   - åµŒå…¥ç‰¹å¾å¹³æ»‘ï¼šæ¶ˆé™¤å†…éƒ¨é‡åŒ–å™ªå£°ã€‚   - è¿è¡Œæ—¶å™ªå£°ä¼°è®¡æ¨¡å—ï¼šåŠ¨æ€è¿‡æ»¤å¤–éƒ¨é‡åŒ–å™ªå£°ã€‚</p><p>ï¼ˆ4ï¼‰å®éªŒç»“æœå’Œæ€§èƒ½ï¼š   å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼ŒQNCD ä¼˜äºæ‰©æ•£æ¨¡å‹çš„å…ˆå‰é‡åŒ–æ–¹æ³•ï¼Œåœ¨ ImageNetï¼ˆLDM-4ï¼‰ä¸Šçš„ W4A8 å’Œ W8A8 é‡åŒ–è®¾ç½®ä¸­å®ç°äº†æ— æŸç»“æœã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº† QNCD é™ä½é‡åŒ–å™ªå£°å¹¶æé«˜æ ·æœ¬è´¨é‡çš„ç›®æ ‡ã€‚</p><ol><li><p>æ–¹æ³•ï¼š(1): æå‡ºç»Ÿä¸€çš„é‡åŒ–å™ªå£°æ ¡æ­£æ–¹æ¡ˆ QNCDï¼Œæœ€å°åŒ–æ•´ä¸ªé‡‡æ ·è¿‡ç¨‹ä¸­çš„é‡åŒ–å™ªå£°ã€‚(2): è¯†åˆ«ä¸¤ç§ä¸»è¦çš„é‡åŒ–æŒ‘æˆ˜ï¼šå†…éƒ¨é‡åŒ–å™ªå£°å’Œå¤–éƒ¨é‡åŒ–å™ªå£°ã€‚(3): åµŒå…¥ç‰¹å¾å¹³æ»‘ï¼Œæ¶ˆé™¤å†…éƒ¨é‡åŒ–å™ªå£°ã€‚(4): è¿è¡Œæ—¶å™ªå£°ä¼°è®¡æ¨¡å—ï¼ŒåŠ¨æ€è¿‡æ»¤å¤–éƒ¨é‡åŒ–å™ªå£°ã€‚</p></li><li><p>ç»“è®ºï¼š(1): æœ¬æ–‡æå‡ºäº† QNCDï¼Œä¸€ç§ç”¨äºæ‰©æ•£æ¨¡å‹çš„ç»Ÿä¸€é‡åŒ–å™ªå£°æ ¡æ­£æ–¹æ¡ˆã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¯¹é‡åŒ–å™ªå£°çš„æ¥æºå’Œå½±å“è¿›è¡Œäº†è¯¦ç»†çš„åˆ†æï¼Œå¹¶å‘ç°å†…éƒ¨é‡åŒ–å™ªå£°çš„å‘¨æœŸæ€§å¢åŠ æºäºåµŒå…¥æ”¹å˜äº†ç‰¹å¾åˆ†å¸ƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬è®¡ç®—äº†ä¸€ä¸ªå¹³æ»‘å› å­æ¥å‡å°‘é‡åŒ–å™ªå£°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªè¿è¡Œæ—¶å™ªå£°ä¼°è®¡æ¨¡å—æ¥ä¼°è®¡å†…éƒ¨é‡åŒ–å™ªå£°çš„åˆ†å¸ƒï¼Œå¹¶åœ¨æ‰©æ•£æ¨¡å‹çš„é‡‡æ ·è¿‡ç¨‹ä¸­è¿›ä¸€æ­¥å¯¹å…¶è¿›è¡Œæ»¤æ³¢ã€‚åˆ©ç”¨è¿™äº›æŠ€æœ¯ï¼Œæˆ‘ä»¬çš„ QNCD è¶…è¿‡äº†ç°æœ‰çš„æœ€å…ˆè¿›çš„åè®­ç»ƒé‡åŒ–æ‰©æ•£æ¨¡å‹ï¼Œå°¤å…¶æ˜¯åœ¨ä½ä½æ¿€æ´»é‡åŒ– (W4A6) ä¸­ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªæ‰©æ•£å»ºæ¨¡æ¡†æ¶ï¼ˆDDIMã€LDM å’Œ Stable Diffusionï¼‰å’Œå¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†å½“å‰ SOTAï¼Œå±•ç¤ºäº† QNCD çš„å¹¿æ³›é€‚ç”¨æ€§ã€‚(2): åˆ›æ–°ç‚¹ï¼š</p></li><li>è¯†åˆ«å¹¶è§£å†³äº†æ‰©æ•£æ¨¡å‹ä¸­é‡åŒ–å™ªå£°çš„ä¸¤ä¸ªä¸»è¦æ¥æºï¼šå†…éƒ¨é‡åŒ–å™ªå£°å’Œå¤–éƒ¨é‡åŒ–å™ªå£°ã€‚</li><li>æå‡ºäº†ä¸€ç§åµŒå…¥ç‰¹å¾å¹³æ»‘æ–¹æ³•æ¥æ¶ˆé™¤å†…éƒ¨é‡åŒ–å™ªå£°ã€‚</li><li>å¼•å…¥äº†ä¸€ä¸ªè¿è¡Œæ—¶å™ªå£°ä¼°è®¡æ¨¡å—æ¥åŠ¨æ€æ»¤é™¤å¤–éƒ¨é‡åŒ–å™ªå£°ã€‚æ€§èƒ½ï¼š</li><li>åœ¨ ImageNetï¼ˆLDM-4ï¼‰ä¸Š W4A8 å’Œ W8A8 é‡åŒ–è®¾ç½®ä¸­å®ç°äº†æ— æŸç»“æœã€‚</li><li>åœ¨å¤šä¸ªæ‰©æ•£å»ºæ¨¡æ¡†æ¶å’Œæ•°æ®é›†ä¸Šä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›çš„åè®­ç»ƒé‡åŒ–æ‰©æ•£æ¨¡å‹ã€‚å·¥ä½œé‡ï¼š</li><li>æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„é‡åŒ–å™ªå£°æ ¡æ­£æ–¹æ¡ˆï¼Œæ˜“äºå®ç°å’Œé›†æˆåˆ°ç°æœ‰æ‰©æ•£æ¨¡å‹ä¸­ã€‚</li><li>è¿è¡Œæ—¶å™ªå£°ä¼°è®¡æ¨¡å—çš„è®¡ç®—æˆæœ¬ä½ï¼Œä¸ä¼šæ˜¾ç€å¢åŠ é‡‡æ ·æ—¶é—´ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-ab9c366da4b3c18e5536fb4d4b1d2831.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-bf2ed02f1e542654a3aeb77e2cdf8f83.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-014a026118e494ef705ba46ec1c8f2bb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f83b5e39d9a64d9e5f49a58d4b5ab948.jpg" align="middle"></details><h2 id="ObjectDrop-Bootstrapping-Counterfactuals-for-Photorealistic-Object-Removal-and-Insertion"><a href="#ObjectDrop-Bootstrapping-Counterfactuals-for-Photorealistic-Object-Removal-and-Insertion" class="headerlink" title="ObjectDrop: Bootstrapping Counterfactuals for Photorealistic Object   Removal and Insertion"></a>ObjectDrop: Bootstrapping Counterfactuals for Photorealistic Object   Removal and Insertion</h2><p><strong>Authors:Daniel Winter, Matan Cohen, Shlomi Fruchter, Yael Pritch, Alex Rav-Acha, Yedid Hoshen</strong></p><p>Diffusion models have revolutionized image editing but often generate images that violate physical laws, particularly the effects of objects on the scene, e.g., occlusions, shadows, and reflections. By analyzing the limitations of self-supervised approaches, we propose a practical solution centered on a \q{counterfactual} dataset. Our method involves capturing a scene before and after removing a single object, while minimizing other changes. By fine-tuning a diffusion model on this dataset, we are able to not only remove objects but also their effects on the scene. However, we find that applying this approach for photorealistic object insertion requires an impractically large dataset. To tackle this challenge, we propose bootstrap supervision; leveraging our object removal model trained on a small counterfactual dataset, we synthetically expand this dataset considerably. Our approach significantly outperforms prior methods in photorealistic object removal and insertion, particularly at modeling the effects of objects on the scene. </p><p><a href="http://arxiv.org/abs/2403.18818v1">PDF</a> </p><p><strong>Summary</strong><br>è‡ªä¸»ç›‘ç£æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç¼–è¾‘ä¸­å­˜åœ¨ç‰©ç†è§„å¾‹è¿èƒŒé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºåäº‹å®æ•°æ®é›†å’Œå¼•å¯¼ç›‘ç£çš„è§£å†³æ–¹æ¡ˆï¼Œæ˜¾è‘—æå‡äº†å›¾åƒç¼–è¾‘çš„çœŸå®æ„Ÿã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç¼–è¾‘ä¸­å­˜åœ¨ç‰©ç†è§„å¾‹è¿èƒŒé—®é¢˜ï¼Œå¦‚é®æŒ¡ã€é˜´å½±å’Œåå°„ã€‚</li><li>é’ˆå¯¹è‡ªç›‘ç£æ–¹æ³•çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§åŸºäºåäº‹å®æ•°æ®é›†çš„è§£å†³æ–¹æ¡ˆã€‚</li><li>åäº‹å®æ•°æ®é›†åŒ…å«å¯¹è±¡ç§»é™¤å‰åçš„åœºæ™¯å›¾åƒï¼Œæœ€å°åŒ–å…¶ä»–å˜åŒ–ã€‚</li><li>é€šè¿‡åœ¨åäº‹å®æ•°æ®é›†ä¸Šå¾®è°ƒæ‰©æ•£æ¨¡å‹ï¼Œä¸ä»…å¯ä»¥ç§»é™¤å¯¹è±¡ï¼Œè¿˜å¯ä»¥ç§»é™¤å…¶å¯¹åœºæ™¯çš„å½±å“ã€‚</li><li>ç…§ç‰‡çº§å¯¹è±¡æ’å…¥éœ€è¦éå¸¸å¤§çš„æ•°æ®é›†ï¼Œæœ¬æ–‡æå‡ºäº†å¼•å¯¼ç›‘ç£æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li><li>å¼•å¯¼ç›‘ç£åˆ©ç”¨åœ¨å°åäº‹å®æ•°æ®é›†ä¸Šè®­ç»ƒçš„å¯¹è±¡ç§»é™¤æ¨¡å‹ï¼Œåˆæˆå¤§é‡æ‰©å……æ•°æ®é›†ã€‚</li><li>è¯¥æ–¹æ³•åœ¨ç…§ç‰‡çº§å¯¹è±¡ç§»é™¤å’Œæ’å…¥æ–¹é¢æ˜æ˜¾ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨æ¨¡æ‹Ÿå¯¹è±¡å¯¹åœºæ™¯çš„å½±å“æ–¹é¢ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šObjectDropï¼šå¼•å¯¼åäº‹å®ç”¨äºé€¼çœŸå¯¹è±¡ç§»é™¤å’Œæ’å…¥</li><li>ä½œè€…ï¼šDaniel Winterã€Matan Cohenã€Shlomi Fruchterã€Yael Pritchã€Alex Rav-Achaã€Yedid Hoshen</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šè€¶è·¯æ’’å†·å¸Œä¼¯æ¥å¤§å­¦</li><li>å…³é”®è¯ï¼šDiffusion Modelã€Object Removalã€Object Insertionã€Counterfactual Datasetã€Bootstrap Supervision</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://ObjectDrop.github.ioï¼ŒGithub ä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç¼–è¾‘ä¸­å–å¾—äº†å·¨å¤§è¿›æ­¥ï¼Œä½†ç»å¸¸ç”Ÿæˆè¿åç‰©ç†å®šå¾‹çš„å›¾åƒï¼Œå°¤å…¶æ˜¯å¯¹è±¡å¯¹åœºæ™¯çš„å½±å“ï¼Œå¦‚é®æŒ¡ã€é˜´å½±å’Œåå°„ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šæœ¬æ–‡åˆ†æäº†è‡ªç›‘ç£æ–¹æ³•çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ä¸ªä»¥â€œåäº‹å®â€æ•°æ®é›†ä¸ºä¸­å¿ƒçš„å®ç”¨è§£å†³æ–¹æ¡ˆã€‚è¿‡å»æ–¹æ³•çš„é—®é¢˜ï¼šæ— æ³•å»ºæ¨¡å¯¹è±¡å¯¹åœºæ™¯çš„å½±å“ï¼Œç”Ÿæˆå›¾åƒä¸çœŸå®ã€‚æœ¬æ–‡æ–¹æ³•çš„åˆç†æ€§ï¼šé€šè¿‡åˆ†ææ‰©æ•£æ¨¡å‹çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§ä»¥â€œåäº‹å®â€æ•°æ®é›†ä¸ºæ ¸å¿ƒçš„å®ç”¨è§£å†³æ–¹æ¡ˆã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æ–¹æ³•åŒ…æ‹¬åœ¨ç§»é™¤å•ä¸ªå¯¹è±¡å‰åæ•æ‰åœºæ™¯ï¼ŒåŒæ—¶æœ€å¤§ç¨‹åº¦åœ°å‡å°‘å…¶ä»–å˜åŒ–ã€‚é€šè¿‡å¾®è°ƒåœ¨è¿™ä¸ªæ•°æ®é›†ä¸Šè®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œä¸ä»…å¯ä»¥ç§»é™¤å¯¹è±¡ï¼Œè¿˜å¯ä»¥ç§»é™¤å¯¹è±¡å¯¹åœºæ™¯çš„å½±å“ã€‚ä½†æ˜¯ï¼Œæœ¬æ–‡å‘ç°å°†è¿™ç§æ–¹æ³•åº”ç”¨äºé€¼çœŸçš„å¯¹è±¡æ’å…¥éœ€è¦ä¸€ä¸ªéå¸¸å¤§çš„æ•°æ®é›†ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†è‡ªä¸¾ç›‘ç£ï¼›åˆ©ç”¨åœ¨å°å‹åäº‹å®æ•°æ®é›†ä¸Šè®­ç»ƒçš„å¯¹è±¡ç§»é™¤æ¨¡å‹ï¼Œæœ¬æ–‡å¤§å¹…æ‰©å……äº†è¿™ä¸ªæ•°æ®é›†ã€‚ï¼ˆ4ï¼‰ä»»åŠ¡å’Œæ€§èƒ½ï¼šæœ¬æ–‡æ–¹æ³•åœ¨é€¼çœŸçš„å¯¹è±¡ç§»é™¤å’Œæ’å…¥æ–¹é¢æ˜¾è‘—ä¼˜äºå…ˆå‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å»ºæ¨¡å¯¹è±¡å¯¹åœºæ™¯çš„å½±å“æ–¹é¢ã€‚å¯¹è±¡ç§»é™¤ï¼šæœ¬æ–‡æ–¹æ³•ä¼˜äºåŸºçº¿æ–¹æ³•ï¼›å¯¹è±¡æ’å…¥ï¼šæœ¬æ–‡æ–¹æ³•ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚æœ¬æ–‡æ–¹æ³•çš„æ€§èƒ½å¯ä»¥æ”¯æŒå…¶ç›®æ ‡ï¼šé€¼çœŸçš„å¯¹è±¡ç§»é™¤å’Œæ’å…¥ã€‚</li></ol><p>Some Error for method(æ¯”å¦‚æ˜¯ä¸æ˜¯æ²¡æœ‰Methodsè¿™ä¸ªç« èŠ‚)</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ æœ¬æ–‡æå‡ºäº†ä¸€ç§ç›‘ç£å¼æ–¹æ³• ObjectDropï¼Œç”¨äºå¯¹è±¡ç§»é™¤å’Œæ’å…¥ï¼Œä»¥å…‹æœå…ˆå‰è‡ªç›‘ç£æ–¹æ³•çš„å±€é™æ€§ã€‚æˆ‘ä»¬æ”¶é›†äº†ä¸€ä¸ªåäº‹å®æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«ç‰©ç†æ“ä½œå¯¹è±¡å‰åæˆå¯¹çš„å›¾åƒã€‚ç”±äºè·å–æ­¤ç±»æ•°æ®é›†çš„æˆæœ¬å¾ˆé«˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªä¸¾ç›‘ç£æ–¹æ³•ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡å…¨é¢çš„è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚ï¼ˆ2ï¼‰ åˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§ä»¥åäº‹å®æ•°æ®é›†ä¸ºä¸­å¿ƒçš„æ–¹æ³•ï¼Œç”¨äºé€¼çœŸçš„å¯¹è±¡ç§»é™¤å’Œæ’å…¥ã€‚</li><li>æå‡ºäº†ä¸€ç§è‡ªä¸¾ç›‘ç£æ–¹æ³•ï¼Œç”¨äºå¤§å¹…æ‰©å……åäº‹å®æ•°æ®é›†ã€‚æ€§èƒ½ï¼š</li><li>åœ¨é€¼çœŸçš„å¯¹è±¡ç§»é™¤å’Œæ’å…¥æ–¹é¢æ˜æ˜¾ä¼˜äºå…ˆå‰æ–¹æ³•ã€‚å·¥ä½œé‡ï¼š</li><li>æ”¶é›†åäº‹å®æ•°æ®é›†çš„æˆæœ¬å¾ˆé«˜ã€‚</li><li>è‡ªä¸¾ç›‘ç£æ–¹æ³•éœ€è¦é¢å¤–çš„è®¡ç®—æˆæœ¬ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-0d997350ce4a66ea5dd9782de7718c23.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2ed8c6a6f3bbd9d635cbb2b475d7dfb1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-88ead8e558b3c1e695cfe7bb4d525b54.jpg" align="middle"><img src="https://picx.zhimg.com/v2-62bf94b77512da17da6fc4e4d9b81c90.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2a4d4bfd835212368d79dde8ef201f90.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c57e44a801b6bd61623098450a79abf2.jpg" align="middle"></details><h2 id="Object-Pose-Estimation-via-the-Aggregation-of-Diffusion-Features"><a href="#Object-Pose-Estimation-via-the-Aggregation-of-Diffusion-Features" class="headerlink" title="Object Pose Estimation via the Aggregation of Diffusion Features"></a>Object Pose Estimation via the Aggregation of Diffusion Features</h2><p><strong>Authors:Tianfu Wang, Guosheng Hu, Hongguang Wang</strong></p><p>Estimating the pose of objects from images is a crucial task of 3D scene understanding, and recent approaches have shown promising results on very large benchmarks. However, these methods experience a significant performance drop when dealing with unseen objects. We believe that it results from the limited generalizability of image features. To address this problem, we have an in-depth analysis on the features of diffusion models, e.g. Stable Diffusion, which hold substantial potential for modeling unseen objects. Based on this analysis, we then innovatively introduce these diffusion features for object pose estimation. To achieve this, we propose three distinct architectures that can effectively capture and aggregate diffusion features of different granularity, greatly improving the generalizability of object pose estimation. Our approach outperforms the state-of-the-art methods by a considerable margin on three popular benchmark datasets, LM, O-LM, and T-LESS. In particular, our method achieves higher accuracy than the previous best arts on unseen objects: 98.2% vs. 93.5% on Unseen LM, 85.9% vs. 76.3% on Unseen O-LM, showing the strong generalizability of our method. Our code is released at <a href="https://github.com/Tianfu18/diff-feats-pose">https://github.com/Tianfu18/diff-feats-pose</a>. </p><p><a href="http://arxiv.org/abs/2403.18791v1">PDF</a> Accepted to CVPR2024</p><p><strong>Summary</strong><br>åˆ©ç”¨ç”Ÿæˆæ‰©æ•£æ¨¡å‹çš„ç‰¹å¾æå‡ç‰©ä½“å§¿æ€ä¼°è®¡çš„æ³›åŒ–æ€§ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>å›¾åƒç‰¹å¾çš„æ³›åŒ–æ€§é™åˆ¶äº†ç‰©ä½“å§¿æ€ä¼°è®¡åœ¨å¤„ç†æœªè§ç‰©ä½“æ—¶çš„æ€§èƒ½ã€‚</li><li>ç”Ÿæˆæ‰©æ•£æ¨¡å‹çš„ç‰¹å¾å…·æœ‰å»ºæ¨¡æœªè§ç‰©ä½“çš„æ½œåŠ›ã€‚</li><li>æå‡ºä¸‰ç§ä¸åŒçš„æ¶æ„æ¥æœ‰æ•ˆæ•è·å’Œèšåˆä¸åŒç²’åº¦çš„æ‰©æ•£ç‰¹å¾ã€‚</li><li>æ‰€æå‡ºçš„æ–¹æ³•åœ¨ä¸‰ä¸ªæµè¡ŒåŸºå‡†æ•°æ®é›† LMã€O-LM å’Œ T-LESS ä¸Šä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li><li>è¯¥æ–¹æ³•åœ¨æœªè§ç‰©ä½“ä¸Šå–å¾—äº†æ¯”ä»¥å¾€æœ€ä½³è‰ºæœ¯æ›´é«˜çš„å‡†ç¡®åº¦ï¼šæœªè§ LM ä¸º 98.2% å¯¹ 93.5%ï¼Œæœªè§ O-LM ä¸º 85.9% å¯¹ 76.3%ã€‚</li><li>ä»£ç å·²åœ¨ <a href="https://github.com/Tianfu18/diff-feats-pose">https://github.com/Tianfu18/diff-feats-pose</a> å‘å¸ƒã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>è®ºæ–‡æ ‡é¢˜ï¼šåŸºäºæ‰©æ•£ç‰¹å¾çš„ç‰©ä½“å§¿æ€ä¼°è®¡</li><li>ä½œè€…ï¼šTianfu Wang, Guosheng Hu, Hongguang Wang</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šä¸­å›½ç§‘å­¦é™¢æ²ˆé˜³è‡ªåŠ¨åŒ–ç ”ç©¶æ‰€æœºå™¨äººå›½å®¶é‡ç‚¹å®éªŒå®¤</li><li>å…³é”®è¯ï¼šç‰©ä½“å§¿æ€ä¼°è®¡ã€æ‰©æ•£æ¨¡å‹ã€ç‰¹å¾èšåˆ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.18791   Github ä»£ç é“¾æ¥ï¼šhttps://github.com/Tianfu18/diff-feats-pose</li><li><p>æ‘˜è¦ï¼š   (1) ç ”ç©¶èƒŒæ™¯ï¼šç‰©ä½“å§¿æ€ä¼°è®¡æ˜¯ 3D åœºæ™¯ç†è§£çš„å…³é”®ä»»åŠ¡ï¼Œæœ€è¿‘çš„æ–¹æ³•åœ¨éå¸¸å¤§çš„åŸºå‡†ä¸Šæ˜¾ç¤ºå‡ºäº†æœ‰å¸Œæœ›çš„ç»“æœã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨å¤„ç†æœªè§ç‰©ä½“æ—¶ä¼šé‡åˆ°æ˜¾ç€çš„æ€§èƒ½ä¸‹é™ã€‚æˆ‘ä»¬è®¤ä¸ºè¿™æ˜¯ç”±äºå›¾åƒç‰¹å¾çš„æ³›åŒ–èƒ½åŠ›æœ‰é™é€ æˆçš„ã€‚   (2) è¿‡å»çš„æ–¹æ³•åŠé—®é¢˜ï¼šç°æœ‰æ–¹æ³•çš„ä¸è¶³ä¹‹å¤„åœ¨äºå…¶åˆ¤åˆ«ç‰¹å¾çš„ä¸è¶³ã€‚ä»¥ç°æœ‰æ–¹æ³•ä¸ºä¾‹ï¼Œå…¶åœ¨ SeenLM æ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡ä¸º 99.1%ï¼Œè€Œåœ¨ UnseenLM æ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡ä¸º 94.4%ï¼Œå¯¼è‡´æ€§èƒ½å·®è·çº¦ä¸º 4.7%ã€‚   (3) æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¯¹æ‰©æ•£æ¨¡å‹çš„ç‰¹å¾è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œä¾‹å¦‚ Stable Diffusionï¼Œå®ƒå…·æœ‰å¯¹æœªè§ç‰©ä½“å»ºæ¨¡çš„å·¨å¤§æ½œåŠ›ã€‚åŸºäºæ­¤åˆ†æï¼Œæˆ‘ä»¬åˆ›æ–°æ€§åœ°å°†è¿™äº›æ‰©æ•£ç‰¹å¾å¼•å…¥ç‰©ä½“å§¿æ€ä¼°è®¡ä¸­ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸‰ç§ä¸åŒçš„æ¶æ„ï¼Œå¯ä»¥æœ‰æ•ˆåœ°æ•è·å’Œèšåˆä¸åŒç²’åº¦çš„æ‰©æ•£ç‰¹å¾ï¼Œæå¤§åœ°æé«˜äº†ç‰©ä½“å§¿æ€ä¼°è®¡çš„æ³›åŒ–èƒ½åŠ›ã€‚   (4) æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸‰ä¸ªæµè¡Œçš„åŸºå‡†æ•°æ®é›† LMã€O-LM å’Œ T-LESS ä¸Šä»¥ç›¸å½“å¤§çš„ä¼˜åŠ¿ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æœªè§ç‰©ä½“ä¸Šå®ç°äº†æ¯”ä»¥å‰æœ€å¥½çš„æ–¹æ³•æ›´é«˜çš„å‡†ç¡®ç‡ï¼šUnseenLM ä¸Šä¸º 98.2% å¯¹æ¯” 93.5%ï¼ŒUnseenO-LM ä¸Šä¸º 85.9% å¯¹æ¯” 76.3%ï¼Œè¡¨æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1): ä½¿ç”¨ç¼–ç å™¨-è§£ç å™¨ç½‘ç»œå›å½’åƒç´ çº§ç¨ å¯†å¯¹åº”å…³ç³»ï¼Œå³ç‰©ä½“è¡¨é¢çš„ 2D åæ ‡ã€‚(2): ç›´æ¥æ³•å°†å§¿æ€ä¼°è®¡è§†ä¸ºå›å½’ä»»åŠ¡ï¼Œç›´æ¥è¾“å‡ºç‰©ä½“çš„å§¿æ€ã€‚(3): SSD-6D å°†å§¿æ€ç©ºé—´åˆ’åˆ†ä¸ºç±»åˆ«ï¼Œå°†å…¶è½¬æ¢ä¸ºåˆ†ç±»é—®é¢˜ã€‚(4): ä¸€äº›æœ€è¿‘çš„æ–¹æ³•ä½¿å¾—é—´æ¥æ³•çš„ PnP è¿‡ç¨‹å¯å¾®åˆ†ï¼Œå¹¶ä½¿ç”¨é—´æ¥æ–¹æ³•ä¸­çš„ 2D-3D å¯¹åº”å…³ç³»ä½œä¸ºä»£ç†ä»»åŠ¡ã€‚(5): åŸºäºæ¨¡æ¿çš„æ–¹æ³•é€šè¿‡åŒ¹é…æŸ¥è¯¢å›¾åƒå’Œæ¨¡æ¿æ¥ç¡®å®šç‰©ä½“çš„å§¿æ€ã€‚</p></li><li><p>ç»“è®ºï¼š(1): æœ¬å·¥ä½œé€šè¿‡æ·±å…¥åˆ†ææ‰©æ•£æ¨¡å‹ç‰¹å¾ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£ç‰¹å¾çš„ç‰©ä½“å§¿æ€ä¼°è®¡æ–¹æ³•ï¼Œæœ‰æ•ˆæé«˜äº†ç‰©ä½“å§¿æ€ä¼°è®¡çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºè¯¥é¢†åŸŸçš„ç ”ç©¶æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚(2): åˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹ç‰¹å¾çš„ç‰©ä½“å§¿æ€ä¼°è®¡æ–¹æ³•ï¼Œæœ‰æ•ˆåˆ©ç”¨äº†æ‰©æ•£æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li><li>è®¾è®¡äº†ä¸‰ç§ä¸åŒçš„èšåˆç½‘ç»œï¼Œå¯ä»¥æœ‰æ•ˆåœ°æ•è·å’Œèšåˆä¸åŒç²’åº¦çš„æ‰©æ•£ç‰¹å¾ï¼Œæé«˜äº†ç‰¹å¾çš„æ³›åŒ–èƒ½åŠ›ã€‚</li><li>åœ¨ä¸‰ä¸ªæµè¡Œçš„åŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨æœªè§ç‰©ä½“ä¸Šå®ç°äº†æ¯”ä»¥å‰æœ€å¥½çš„æ–¹æ³•æ›´é«˜çš„å‡†ç¡®ç‡ã€‚æ€§èƒ½ï¼š</li><li>åœ¨ä¸‰ä¸ªæµè¡Œçš„åŸºå‡†æ•°æ®é›†ä¸Šä»¥ç›¸å½“å¤§çš„ä¼˜åŠ¿ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li><li>åœ¨æœªè§ç‰©ä½“ä¸Šå®ç°äº†æ¯”ä»¥å‰æœ€å¥½çš„æ–¹æ³•æ›´é«˜çš„å‡†ç¡®ç‡ã€‚å·¥ä½œé‡ï¼š</li><li>ç®—æ³•å®ç°å¤æ‚åº¦è¾ƒé«˜ï¼Œéœ€è¦è¾ƒå¤§çš„è®¡ç®—èµ„æºã€‚</li><li>éœ€è¦å¯¹æ‰©æ•£æ¨¡å‹ç‰¹å¾è¿›è¡Œæ·±å…¥åˆ†æå’Œç†è§£ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-4820d797bcfff56fb3cde8ca02487789.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-efd97de0afb46165d90e35834006bf33.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0d203f766bdbf388635c0fd745c25f4d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a54011b7bcab881b7dce9d61b9644ed6.jpg" align="middle"></details><h2 id="ImageNet-D-Benchmarking-Neural-Network-Robustness-on-Diffusion-Synthetic-Object"><a href="#ImageNet-D-Benchmarking-Neural-Network-Robustness-on-Diffusion-Synthetic-Object" class="headerlink" title="ImageNet-D: Benchmarking Neural Network Robustness on Diffusion   Synthetic Object"></a>ImageNet-D: Benchmarking Neural Network Robustness on Diffusion   Synthetic Object</h2><p><strong>Authors:Chenshuang Zhang, Fei Pan, Junmo Kim, In So Kweon, Chengzhi Mao</strong></p><p>We establish rigorous benchmarks for visual perception robustness. Synthetic images such as ImageNet-C, ImageNet-9, and Stylized ImageNet provide specific type of evaluation over synthetic corruptions, backgrounds, and textures, yet those robustness benchmarks are restricted in specified variations and have low synthetic quality. In this work, we introduce generative model as a data source for synthesizing hard images that benchmark deep modelsâ€™ robustness. Leveraging diffusion models, we are able to generate images with more diversified backgrounds, textures, and materials than any prior work, where we term this benchmark as ImageNet-D. Experimental results show that ImageNet-D results in a significant accuracy drop to a range of vision models, from the standard ResNet visual classifier to the latest foundation models like CLIP and MiniGPT-4, significantly reducing their accuracy by up to 60\%. Our work suggests that diffusion models can be an effective source to test vision models. The code and dataset are available at <a href="https://github.com/chenshuang-zhang/imagenet_d">https://github.com/chenshuang-zhang/imagenet_d</a>. </p><p><a href="http://arxiv.org/abs/2403.18775v1">PDF</a> Accepted at CVPR 2024</p><p><strong>Summary</strong><br>ä½¿ç”¨æ‰©æ•£æ¨¡å‹åˆæˆçš„å›¾åƒæ„å»ºäº†è§†è§‰æ„ŸçŸ¥å¥å£®æ€§åŸºå‡†ï¼Œæ˜¾è‘—é™ä½äº†æ¨¡å‹å‡†ç¡®æ€§ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æ‰©æ•£æ¨¡å‹å¯ä»¥ç”Ÿæˆå¤šæ ·åŒ–çš„èƒŒæ™¯ã€çº¹ç†å’Œææ–™å›¾åƒï¼Œç”¨äºåŸºå‡†æµ‹è¯•è§†è§‰æ„ŸçŸ¥å¥å£®æ€§ã€‚</li><li>ImageNet-D åŸºå‡†æ¯”ç°æœ‰åŸºå‡†æä¾›äº†æ›´å…·æŒ‘æˆ˜æ€§çš„åˆæˆå›¾åƒã€‚</li><li>ImageNet-D åŸºå‡†å¯¼è‡´ä» ResNet è§†è§‰åˆ†ç±»å™¨åˆ° CLIP å’Œ MiniGPT-4 ç­‰æœ€æ–°åŸºç¡€æ¨¡å‹çš„å‡†ç¡®æ€§å¤§å¹…ä¸‹é™ã€‚</li><li>æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„å›¾åƒå¯ä»¥æœ‰æ•ˆæµ‹è¯•è§†è§‰æ¨¡å‹çš„å¥å£®æ€§ã€‚</li><li>ImageNet-D æ•°æ®é›†å’Œä»£ç å·²å¼€æºã€‚</li><li>åˆæˆå›¾åƒåŸºå‡†åœ¨è¯„ä¼°è§†è§‰æ¨¡å‹çš„å¥å£®æ€§æ–¹é¢å—åˆ°é™åˆ¶ã€‚</li><li>æ‰©æ•£æ¨¡å‹ä¸ºåˆæˆå›¾åƒåŸºå‡†æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šImageNet-Dï¼šåŸºäº ImageNet-D å¯¹ç¥ç»ç½‘ç»œé²æ£’æ€§è¿›è¡ŒåŸºå‡†æµ‹è¯•</li><li>ä½œè€…ï¼šShuang Zhang, Jinfeng Yi, Bo Li, Yutong Bai, Minghao Chen, Lu Yuan, Zicheng Liu, Xiaolin Wei, Jian Sun</li><li>å•ä½ï¼šå¤æ—¦å¤§å­¦</li><li>å…³é”®è¯ï¼šè®¡ç®—æœºè§†è§‰ã€ç¥ç»ç½‘ç»œã€é²æ£’æ€§ã€ç”Ÿæˆæ¨¡å‹ã€æ‰©æ•£æ¨¡å‹</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.07407   Github ä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼š   (1) ç ”ç©¶èƒŒæ™¯ï¼š   ç›®å‰ï¼Œè§†è§‰æ„ŸçŸ¥é²æ£’æ€§åŸºå‡†æµ‹è¯•ä¸»è¦ä¾èµ–äºåˆæˆå›¾åƒï¼Œä¾‹å¦‚ ImageNet-Cã€ImageNet-9 å’Œ Stylized ImageNetã€‚ç„¶è€Œï¼Œè¿™äº›åŸºå‡†æµ‹è¯•åœ¨æŒ‡å®šçš„å˜ä½“å’Œåˆæˆå›¾åƒè´¨é‡æ–¹é¢å­˜åœ¨é™åˆ¶ã€‚   (2) è¿‡å»çš„æ–¹æ³•å’Œé—®é¢˜ï¼š   è¿‡å»çš„æ–¹æ³•ä¸»è¦ä½¿ç”¨åˆæˆå›¾åƒä½œä¸ºæ•°æ®æºï¼Œä½†è¿™äº›å›¾åƒå¾€å¾€ç¼ºä¹å¤šæ ·æ€§ï¼Œéš¾ä»¥åæ˜ çœŸå®ä¸–ç•Œä¸­çš„å¤æ‚æ€§ã€‚   (3) æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼š   æœ¬æ–‡æå‡ºåˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆåˆæˆå›¾åƒï¼Œä»¥æ„å»ºæ›´å…·æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•é›† ImageNet-Dã€‚ImageNet-D åŒ…å«æ›´ä¸°å¯Œå¤šæ ·çš„èƒŒæ™¯ã€çº¹ç†å’Œæè´¨ï¼Œèƒ½å¤Ÿæ›´å…¨é¢åœ°è¯„ä¼°æ·±åº¦æ¨¡å‹çš„é²æ£’æ€§ã€‚   (4) æ–¹æ³•åœ¨ä»»åŠ¡ä¸Šçš„è¡¨ç°å’Œå–å¾—çš„æ€§èƒ½ï¼š   å®éªŒç»“æœè¡¨æ˜ï¼ŒImageNet-D å¯¹å„ç§è§†è§‰æ¨¡å‹çš„å‡†ç¡®ç‡é€ æˆäº†æ˜¾è‘—ä¸‹é™ï¼Œä»æ ‡å‡†çš„ ResNet è§†è§‰åˆ†ç±»å™¨åˆ°æœ€æ–°çš„åŸºç¡€æ¨¡å‹ï¼Œå¦‚ CLIP å’Œ MiniGPT-4ï¼Œå‡†ç¡®ç‡é™ä½äº†é«˜è¾¾ 60%ã€‚è¿™è¡¨æ˜æ‰©æ•£æ¨¡å‹å¯ä»¥ä½œä¸ºæµ‹è¯•è§†è§‰æ¨¡å‹çš„æœ‰æ•ˆæ•°æ®æºã€‚</li></ol><p>Some Error for method(æ¯”å¦‚æ˜¯ä¸æ˜¯æ²¡æœ‰Methodsè¿™ä¸ªç« èŠ‚)</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åˆæˆå›¾åƒæ•°æ®é›† ImageNet-Dï¼Œè¯¥æ•°æ®é›†åˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆå…·æœ‰ä¸°å¯Œå¤šæ ·çš„èƒŒæ™¯ã€çº¹ç†å’Œæè´¨çš„å›¾åƒï¼Œä¸ºè§†è§‰æ¨¡å‹é²æ£’æ€§è¯„ä¼°æä¾›äº†æ›´å…·æŒ‘æˆ˜æ€§çš„åŸºå‡†ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šåˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆåˆæˆå›¾åƒï¼Œæ„å»ºäº†æ›´å…·æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•é›† ImageNet-Dã€‚æ€§èƒ½ï¼šå®éªŒç»“æœè¡¨æ˜ï¼ŒImageNet-D å¯¹å„ç§è§†è§‰æ¨¡å‹çš„å‡†ç¡®ç‡é€ æˆäº†æ˜¾è‘—ä¸‹é™ï¼Œä»æ ‡å‡†çš„ ResNet è§†è§‰åˆ†ç±»å™¨åˆ°æœ€æ–°çš„åŸºç¡€æ¨¡å‹ï¼Œå¦‚ CLIP å’Œ MiniGPT-4ï¼Œå‡†ç¡®ç‡é™ä½äº†é«˜è¾¾ 60%ã€‚å·¥ä½œé‡ï¼šæœ¬æ–‡æ„å»ºäº†åŒ…å« 100 ä¸‡å¼ å›¾åƒçš„ ImageNet-D æ•°æ®é›†ï¼Œå¹¶æä¾›äº†è¯¦ç»†çš„å®éªŒç»“æœå’Œåˆ†æã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-cdbc8aaaf597ff649b878eedb9c62a72.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e640491f2b03e89d4dcb1f60e14377f1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c3c9a5dce13499656b216d11d8038d29.jpg" align="middle"><img src="https://picx.zhimg.com/v2-805d837db07b0751f90d39999f6ada7d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b91f6de94e71cbec867cf6430e770a48.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2397cd7d51692808a60a097b82b883c7.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6b601957b0be69a2d2f43c3ebb64ebfc.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c29c4bb74eff595b69f031de17c8b4db.jpg" align="middle"></details><h2 id="HandBooster-Boosting-3D-Hand-Mesh-Reconstruction-by-Conditional-Synthesis-and-Sampling-of-Hand-Object-Interactions"><a href="#HandBooster-Boosting-3D-Hand-Mesh-Reconstruction-by-Conditional-Synthesis-and-Sampling-of-Hand-Object-Interactions" class="headerlink" title="HandBooster: Boosting 3D Hand-Mesh Reconstruction by Conditional   Synthesis and Sampling of Hand-Object Interactions"></a>HandBooster: Boosting 3D Hand-Mesh Reconstruction by Conditional   Synthesis and Sampling of Hand-Object Interactions</h2><p><strong>Authors:Hao Xu, Haipeng Li, Yinqiao Wang, Shuaicheng Liu, Chi-Wing Fu</strong></p><p>Reconstructing 3D hand mesh robustly from a single image is very challenging, due to the lack of diversity in existing real-world datasets. While data synthesis helps relieve the issue, the syn-to-real gap still hinders its usage. In this work, we present HandBooster, a new approach to uplift the data diversity and boost the 3D hand-mesh reconstruction performance by training a conditional generative space on hand-object interactions and purposely sampling the space to synthesize effective data samples. First, we construct versatile content-aware conditions to guide a diffusion model to produce realistic images with diverse hand appearances, poses, views, and backgrounds; favorably, accurate 3D annotations are obtained for free. Then, we design a novel condition creator based on our similarity-aware distribution sampling strategies to deliberately find novel and realistic interaction poses that are distinctive from the training set. Equipped with our method, several baselines can be significantly improved beyond the SOTA on the HO3D and DexYCB benchmarks. Our code will be released on <a href="https://github.com/hxwork/HandBooster_Pytorch">https://github.com/hxwork/HandBooster_Pytorch</a>. </p><p><a href="http://arxiv.org/abs/2403.18575v1">PDF</a> </p><p><strong>Summary</strong><br>ä½¿ç”¨æ¡ä»¶ç”Ÿæˆç©ºé—´è®­ç»ƒæ‰‹éƒ¨ç‰©ä½“äº’åŠ¨ï¼Œé€šè¿‡ç›®çš„æ€§çš„é‡‡æ ·ï¼Œæå‡æ•°æ®å¤šæ ·æ€§å’Œä¿ƒè¿› 3D æ‰‹éƒ¨ç½‘æ ¼é‡å»ºæ€§èƒ½ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æ•°æ®åˆæˆè™½æœ‰å¸®åŠ©ï¼Œä½†åˆæˆä¸çœŸå®ä¹‹é—´çš„å·®è·é™åˆ¶å…¶ä½¿ç”¨ã€‚</li><li>HandBooster æå‡ºä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡æ‰‹éƒ¨ç‰©ä½“äº’åŠ¨è®­ç»ƒæ¡ä»¶ç”Ÿæˆç©ºé—´ï¼Œå¹¶ç‰¹æ„å¯¹ç©ºé—´è¿›è¡Œé‡‡æ ·ä»¥åˆæˆæœ‰æ•ˆæ•°æ®æ ·æœ¬ï¼Œä»è€Œæå‡æ•°æ®å¤šæ ·æ€§å’Œä¿ƒè¿› 3D æ‰‹éƒ¨ç½‘æ ¼é‡å»ºæ€§èƒ½ã€‚</li><li>ä½¿ç”¨åŸºäºå†…å®¹çš„æ¡ä»¶æŒ‡å¯¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆå…·æœ‰å¤šæ ·åŒ–æ‰‹éƒ¨å¤–è§‚ã€å§¿åŠ¿ã€è§†å›¾å’ŒèƒŒæ™¯çš„çœŸå®å›¾åƒã€‚</li><li>é€šè¿‡ç›¸ä¼¼æ€§æ„ŸçŸ¥åˆ†å¸ƒé‡‡æ ·ç­–ç•¥è®¾è®¡äº†ä¸€ä¸ªæ–°é¢–çš„æ¡ä»¶åˆ›å»ºå™¨ï¼Œä»¥æ•…æ„å‘ç°ä¸è®­ç»ƒé›†ä¸åŒçš„æ–°é¢–é€¼çœŸçš„äº’åŠ¨å§¿åŠ¿ã€‚</li><li>è¯¥æ–¹æ³•æ˜¾è‘—æå‡å¤šä¸ªåŸºå‡†åœ¨ HO3D å’Œ DexYCB åŸºå‡†ä¸Šçš„æ€§èƒ½ï¼Œè¶…è¶Šäº†å½“å‰æœ€ä½³æ°´å¹³ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šHandBoosterï¼šé€šè¿‡æ¡ä»¶åˆæˆå’Œæ‰‹éƒ¨ç‰©ä½“äº¤äº’é‡‡æ ·æå‡ 3D æ‰‹éƒ¨ç½‘æ ¼é‡å»º</li><li>ä½œè€…ï¼šå¾æµ©ã€ææµ·é¹ã€ç‹å¯…æ¡¥ã€åˆ˜å¸…æˆã€å‚…å¿—ç‚œ</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šé¦™æ¸¯ä¸­æ–‡å¤§å­¦</li><li>å…³é”®è¯ï¼š3D æ‰‹éƒ¨ç½‘æ ¼é‡å»ºã€æ•°æ®åˆæˆã€æ¡ä»¶ç”Ÿæˆã€æ‰‹éƒ¨ç‰©ä½“äº¤äº’</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.18575</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šä»å•å¹…å›¾åƒä¸­ç¨³å¥åœ°é‡å»º 3D æ‰‹éƒ¨ç½‘æ ¼æå…·æŒ‘æˆ˜æ€§ï¼ŒåŸå› æ˜¯ç°æœ‰çš„çœŸå®ä¸–ç•Œæ•°æ®é›†ç¼ºä¹å¤šæ ·æ€§ã€‚è™½ç„¶æ•°æ®åˆæˆæœ‰åŠ©äºç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œä½†åˆæˆåˆ°çœŸå®ä¸–ç•Œçš„å·®è·ä»ç„¶é˜»ç¢äº†å…¶ä½¿ç”¨ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç°æœ‰çš„æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨æ•°æ®æ¸²æŸ“æˆ–ç”Ÿæˆï¼Œä½†å®ƒä»¬å¿½ç•¥äº†æ•°æ®å¤šæ ·æ€§çš„å…¶ä»–æ–¹é¢ï¼Œä¾‹å¦‚æ‰‹éƒ¨å¤–è§‚ã€å§¿åŠ¿å’ŒèƒŒæ™¯ã€‚æ­¤å¤–ï¼Œæ²¡æœ‰è¯æ®è¡¨æ˜æ‰‹éƒ¨ç½‘æ ¼é‡å»ºæ€§èƒ½å¯ä»¥åœ¨ç°æœ‰æ–¹æ³•ä¸Šå§‹ç»ˆå¾—åˆ°æ”¹å–„ã€‚ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³• HandBoosterï¼Œé€šè¿‡è®­ç»ƒä¸€ä¸ªæ¡ä»¶ç”Ÿæˆç©ºé—´æ¥æå‡æ•°æ®å¤šæ ·æ€§å¹¶æå‡ 3D æ‰‹éƒ¨ç½‘æ ¼é‡å»ºæ€§èƒ½ï¼Œè¯¥ç©ºé—´ç”¨äºæ‰‹éƒ¨ç‰©ä½“äº¤äº’å¹¶æœ‰ç›®çš„åœ°é‡‡æ ·è¯¥ç©ºé—´ä»¥åˆæˆæœ‰æ•ˆçš„æ•°æ®æ ·æœ¬ã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šåœ¨ HO3D å’Œ DexYCB åŸºå‡†ä¸Šï¼ŒHandBooster å¯ä»¥æ˜¾è‘—æ”¹å–„å‡ ç§åŸºçº¿æ–¹æ³•ï¼Œä½¿å…¶å†æ¬¡æˆä¸º SOTAã€‚è¿™äº›æ€§èƒ½æå‡æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ï¼Œå³æå‡ 3D æ‰‹éƒ¨ç½‘æ ¼é‡å»ºæ€§èƒ½ã€‚</li></ol><p>7.æ–¹æ³•ï¼šï¼ˆ1ï¼‰æå‡ºHandBoosteræ–¹æ³•ï¼Œé€šè¿‡æ¡ä»¶ç”Ÿæˆç©ºé—´æå‡æ•°æ®å¤šæ ·æ€§ï¼Œå¹¶æœ‰ç›®çš„åœ°é‡‡æ ·è¯¥ç©ºé—´ä»¥åˆæˆæœ‰æ•ˆçš„æ•°æ®æ ·æœ¬ï¼›ï¼ˆ2ï¼‰è®­ç»ƒæ¡ä»¶ç”Ÿæˆç©ºé—´ï¼Œç”Ÿæˆå…·æœ‰å¤šæ ·åŒ–æ‰‹éƒ¨å¤–è§‚ã€å§¿åŠ¿å’ŒèƒŒæ™¯çš„åˆæˆæ•°æ®ï¼›ï¼ˆ3ï¼‰å°†åˆæˆæ•°æ®ä¸çœŸå®æ•°æ®æ··åˆï¼Œä¸°å¯Œè®­ç»ƒæ•°æ®é›†ï¼Œæå‡3Dæ‰‹éƒ¨ç½‘æ ¼é‡å»ºæ€§èƒ½ï¼›ï¼ˆ4ï¼‰åœ¨HO3Då’ŒDexYCBåŸºå‡†ä¸Šï¼Œè¯„ä¼°HandBoosteræ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜å…¶å¯ä»¥æ˜¾è‘—æ”¹å–„å‡ ç§åŸºçº¿æ–¹æ³•ï¼Œä½¿å…¶å†æ¬¡æˆä¸ºSOTAã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬å·¥ä½œé€šè¿‡å¢å¼ºæ•°æ®å¤šæ ·æ€§æ¥æå‡ 3D æ‰‹éƒ¨ç½‘æ ¼é‡å»ºï¼Œæå‡ºäº†ä¸€ç§æ–°çš„ç”Ÿæˆæ–¹æ³• HandBoosterã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªæ¡ä»¶ç”Ÿæˆç©ºé—´ï¼Œå¯ä»¥ä»ä¸­å¯æ§åœ°ç”Ÿæˆå…·æœ‰çœŸå®ä¸”å¤šæ ·åŒ–çš„å¸¦æœ‰å¯é  3D æ ‡æ³¨çš„æ‰‹éƒ¨ç‰©ä½“å›¾åƒã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡åˆ¶å®šä¸€ä¸ªæ–°é¢–çš„æ¡ä»¶åˆ›å»ºå™¨å’Œä¸¤ä¸ªç›¸ä¼¼æ€§æ„ŸçŸ¥é‡‡æ ·ç­–ç•¥æ¥æ¢ç´¢è¿™ä¸ªç©ºé—´ä»¥ç”Ÿæˆæ–°é¢–ä¸”å¤šæ ·åŒ–çš„è®­ç»ƒæ ·æœ¬ã€‚åœ¨ä¸‰ä¸ªåŸºçº¿å’Œä¸¤ä¸ªå¸¸è§åŸºå‡†ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æœ‰æ•ˆæ€§å’Œ SOTA æ€§èƒ½ã€‚è‡´è°¢ï¼šè¿™é¡¹å·¥ä½œå¾—åˆ°äº†ä¸­å›½é¦™æ¸¯ç‰¹åˆ«è¡Œæ”¿åŒºç ”ç©¶èµ„åŠ©å±€ï¼ˆé¡¹ç›®ç¼–å·ï¼šT45-401/22-N å’Œç¼–å·ï¼šCUHK14201921ï¼‰å’Œå›½å®¶è‡ªç„¶ç§‘å­¦åŸºé‡‘ï¼ˆé¡¹ç›®ç¼–å·ï¼š62372091ï¼‰çš„æ”¯æŒã€‚å¾æµ©æ„Ÿè°¢å¼ å®‡å½¤åŠå…¶å®¶äººçš„å…³å¿ƒå’Œæ”¯æŒã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼šHandBoosterï¼›æ€§èƒ½ï¼šæ˜¾è‘—æ”¹å–„å‡ ç§åŸºçº¿æ–¹æ³•ï¼Œä½¿å…¶å†æ¬¡æˆä¸º SOTAï¼›å·¥ä½œé‡ï¼šä¸­ç­‰ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-544410656a6e52002e5117c3f6ae8713.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a9384bfab2d5003fd16ce98eb2d388e0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3913a096f0e0ed563e1fa6a643f67875.jpg" align="middle"><img src="https://picx.zhimg.com/v2-382b8f14d2da8a4232b20d1252a55099.jpg" align="middle"><img src="https://picx.zhimg.com/v2-02ff7d22674d7d49a4e95c028e4a99b4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4e3386cc2776b64fed2af1807a1e0912.jpg" align="middle"></details><h2 id="Artifact-Reduction-in-3D-and-4D-Cone-beam-Computed-Tomography-Images-with-Deep-Learning-â€”-A-Review"><a href="#Artifact-Reduction-in-3D-and-4D-Cone-beam-Computed-Tomography-Images-with-Deep-Learning-â€”-A-Review" class="headerlink" title="Artifact Reduction in 3D and 4D Cone-beam Computed Tomography Images   with Deep Learning â€” A Review"></a>Artifact Reduction in 3D and 4D Cone-beam Computed Tomography Images   with Deep Learning â€” A Review</h2><p><strong>Authors:Mohammadreza Amirian, Daniel Barco, Ivo Herzig, Frank-Peter Schilling</strong></p><p>Deep learning based approaches have been used to improve image quality in cone-beam computed tomography (CBCT), a medical imaging technique often used in applications such as image-guided radiation therapy, implant dentistry or orthopaedics. In particular, while deep learning methods have been applied to reduce various types of CBCT image artifacts arising from motion, metal objects, or low-dose acquisition, a comprehensive review summarizing the successes and shortcomings of these approaches, with a primary focus on the type of artifacts rather than the architecture of neural networks, is lacking in the literature. In this review, the data generation and simulation pipelines, and artifact reduction techniques are specifically investigated for each type of artifact. We provide an overview of deep learning techniques that have successfully been shown to reduce artifacts in 3D, as well as in time-resolved (4D) CBCT through the use of projection- and/or volume-domain optimizations, or by introducing neural networks directly within the CBCT reconstruction algorithms. Research gaps are identified to suggest avenues for future exploration. One of the key findings of this work is an observed trend towards the use of generative models including GANs and score-based or diffusion models, accompanied with the need for more diverse and open training datasets and simulations. </p><p><a href="http://arxiv.org/abs/2403.18565v1">PDF</a> 16 pages, 4 figures, 1 Table, published in IEEE Access Journal</p><p><strong>Summary</strong><br>æ·±åº¦å­¦ä¹ æ–¹æ³•è¢«ç”¨äºæ”¹å–„é”¥å½¢æŸè®¡ç®—æœºæ–­å±‚æ‰«æ (CBCT) å›¾åƒè´¨é‡ï¼ŒCBCT æ˜¯ä¸€ç§åŒ»å­¦æˆåƒæŠ€æœ¯ï¼Œå¸¸ç”¨äºå›¾åƒå¼•å¯¼æ”¾å°„æ²»ç–—ã€ç§æ¤ç‰™æˆ–éª¨ç§‘ç­‰åº”ç”¨ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æ·±åº¦å­¦ä¹ æ–¹æ³•å·²æˆåŠŸç”¨äºå‡å°‘ CBCT å›¾åƒä¼ªå½±ï¼Œå¦‚è¿åŠ¨ã€é‡‘å±ç‰©ä½“æˆ–ä½å‰‚é‡é‡‡é›†äº§ç”Ÿçš„ä¼ªå½±ã€‚</li><li>æ•°æ®ç”Ÿæˆå’Œæ¨¡æ‹Ÿç®¡é“ä»¥åŠä¼ªå½±å‡å°‘æŠ€æœ¯é’ˆå¯¹æ¯ç§ç±»å‹çš„ä¼ªå½±åˆ†åˆ«è¿›è¡Œè°ƒæŸ¥ã€‚</li><li>æ·±åº¦å­¦ä¹ æŠ€æœ¯å·²æˆåŠŸç”¨äºé€šè¿‡ä½¿ç”¨æŠ•å½±å’Œ/æˆ–ä½“åŸŸä¼˜åŒ–æˆ–ç›´æ¥åœ¨ CBCT é‡å»ºç®—æ³•ä¸­å¼•å…¥ç¥ç»ç½‘ç»œæ¥å‡å°‘ 3D å’Œæ—¶é—´åˆ†è¾¨ (4D) CBCT ä¸­çš„ä¼ªå½±ã€‚</li><li>ç¡®å®šäº†ç ”ç©¶å·®è·ï¼Œä¸ºæœªæ¥çš„æ¢ç´¢æä¾›äº†é€”å¾„ã€‚</li><li>è§‚å¯Ÿåˆ°çš„è¶‹åŠ¿æ˜¯ä½¿ç”¨ç”Ÿæˆæ¨¡å‹ï¼ŒåŒ…æ‹¬ GANã€åŸºäºåˆ†æ•°æˆ–æ‰©æ•£æ¨¡å‹ï¼Œå¹¶éœ€è¦æ›´å¤šæ ·åŒ–å’Œå¼€æ”¾çš„è®­ç»ƒæ•°æ®é›†å’Œæ¨¡æ‹Ÿã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>è®ºæ–‡æ ‡é¢˜ï¼š3D å’Œ 4D é”¥æŸ CT ä¸­çš„ä¼ªå½±å‡å°‘ä¸æ·±åº¦å­¦ä¹ â€”â€”ç»¼è¿°</li><li>ä½œè€…ï¼šMOHAMMADREZA AMIRIAN1ã€Daniel Barco1ã€Ivo Herzig2 å’Œ Frank-Peter Schilling1</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šè‹é»ä¸–åº”ç”¨ç§‘å­¦å¤§å­¦äººå·¥æ™ºèƒ½ä¸­å¿ƒ (CAI)</li><li>å…³é”®è¯ï¼šé”¥æŸè®¡ç®—æœºæ–­å±‚æ‰«æ (CBCT)ã€æ·±åº¦å­¦ä¹ ã€ä¼ªå½±</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://ieeexplore.ieee.org/document/10322000</li><li><p>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šæ·±åº¦å­¦ä¹ æ–¹æ³•å·²è¢«ç”¨äºæé«˜é”¥æŸè®¡ç®—æœºæ–­å±‚æ‰«æ (CBCT) çš„å›¾åƒè´¨é‡ï¼ŒCBCT æ˜¯ä¸€ç§åŒ»ç–—æˆåƒæŠ€æœ¯ï¼Œé€šå¸¸ç”¨äºå›¾åƒå¼•å¯¼æ”¾å°„æ²»ç–—ã€æ¤å…¥ç‰™ç§‘æˆ–éª¨ç§‘ç­‰åº”ç”¨ã€‚å…·ä½“è€Œè¨€ï¼Œè™½ç„¶æ·±åº¦å­¦ä¹ æ–¹æ³•å·²è¢«åº”ç”¨äºå‡å°‘å„ç§ CBCT å›¾åƒä¼ªå½±ï¼Œè¿™äº›ä¼ªå½±æ˜¯ç”±è¿åŠ¨ã€é‡‘å±ç‰©ä½“æˆ–ä½å‰‚é‡é‡‡é›†å¼•èµ·çš„ï¼Œä½†ç¼ºä¹ä¸€ä»½ç»¼åˆç»¼è¿°æ¥æ€»ç»“è¿™äº›æ–¹æ³•çš„æˆåŠŸå’Œä¸è¶³ï¼Œå¹¶é‡ç‚¹å…³æ³¨ä¼ªå½±ç±»å‹è€Œä¸æ˜¯ç¥ç»ç½‘ç»œçš„æ¶æ„ã€‚(2) è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šæœ¬æ–‡çš„åŠ¨æœºå……åˆ†ï¼Œå› ä¸ºå®ƒè§£å†³äº†ç°æœ‰æ–‡çŒ®ä¸­çš„ä¸€ä¸ªå·®è·ã€‚(3) æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬ç»¼è¿°ä¸“é—¨é’ˆå¯¹æ¯ç§ç±»å‹çš„ä¼ªå½±ç ”ç©¶æ•°æ®ç”Ÿæˆå’Œæ¨¡æ‹Ÿç®¡é“ä»¥åŠä¼ªå½±å‡å°‘æŠ€æœ¯ã€‚æˆ‘ä»¬æ¦‚è¿°äº†æ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œè¿™äº›æŠ€æœ¯å·²è¢«è¯æ˜å¯ä»¥æˆåŠŸå‡å°‘ 3D å’Œæ—¶é—´åˆ†è¾¨ (4D) CBCT ä¸­çš„ä¼ªå½±ï¼Œæ–¹æ³•æ˜¯ä½¿ç”¨æŠ•å½±å’Œ/æˆ–ä½“ç§¯åŸŸä¼˜åŒ–ï¼Œæˆ–ç›´æ¥åœ¨ CBCT é‡å»ºç®—æ³•ä¸­å¼•å…¥ç¥ç»ç½‘ç»œã€‚(4) æœ¬æ–‡æ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†æ€æ ·çš„æ€§èƒ½ï¼šè¿™äº›æ–¹æ³•çš„æ€§èƒ½æ˜¯å¦æ”¯æŒå…¶ç›®æ ‡ï¼šæœ¬ç»¼è¿°ç¡®å®šäº†ç ”ç©¶å·®è·ï¼Œä»¥å»ºè®®æœªæ¥æ¢ç´¢çš„é€”å¾„ã€‚è¿™é¡¹å·¥ä½œçš„ä¸€ä¸ªå…³é”®å‘ç°æ˜¯è§‚å¯Ÿåˆ°ä½¿ç”¨ç”Ÿæˆæ¨¡å‹ï¼ˆåŒ…æ‹¬ GANã€åŸºäºåˆ†æ•°æˆ–æ‰©æ•£æ¨¡å‹ï¼‰çš„è¶‹åŠ¿ï¼Œä»¥åŠå¯¹æ›´å¤šæ ·åŒ–å’Œå¼€æ”¾çš„è®­ç»ƒæ•°æ®é›†å’Œæ¨¡æ‹Ÿçš„éœ€æ±‚ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) æå‡ºåŸºäºæ·±åº¦å­¦ä¹ çš„ä¼ªå½±å‡å°‘æŠ€æœ¯ï¼Œé’ˆå¯¹æ¯ç§ç±»å‹çš„ä¼ªå½±ç ”ç©¶æ•°æ®ç”Ÿæˆå’Œæ¨¡æ‹Ÿç®¡é“ï¼›(2) æ¦‚è¿°ä½¿ç”¨æŠ•å½±å’Œ/æˆ–ä½“ç§¯åŸŸä¼˜åŒ–æˆ–ç›´æ¥åœ¨ CBCT é‡å»ºç®—æ³•ä¸­å¼•å…¥ç¥ç»ç½‘ç»œçš„æ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼›(3) ç¡®å®šç ”ç©¶å·®è·ï¼Œå»ºè®®æœªæ¥æ¢ç´¢çš„é€”å¾„ã€‚</p></li></ol><p>8.ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡ç»¼è¿°äº†æ·±åº¦å­¦ä¹ åœ¨ 3D å’Œ 4D CBCT ä¼ªå½±å‡å°‘ä¸­çš„åº”ç”¨ï¼Œä¸ºè¯¥é¢†åŸŸçš„ç ”ç©¶æä¾›äº†å…¨é¢çš„æ¦‚è¿°ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š    - é’ˆå¯¹æ¯ç§ä¼ªå½±ç±»å‹ç ”ç©¶æ•°æ®ç”Ÿæˆå’Œæ¨¡æ‹Ÿç®¡é“ã€‚    - æ¦‚è¿°äº†ä½¿ç”¨æŠ•å½±å’Œ/æˆ–ä½“ç§¯åŸŸä¼˜åŒ–æˆ–ç›´æ¥åœ¨ CBCT é‡å»ºç®—æ³•ä¸­å¼•å…¥ç¥ç»ç½‘ç»œçš„æ·±åº¦å­¦ä¹ æŠ€æœ¯ã€‚    - ç¡®å®šäº†ç ”ç©¶å·®è·ï¼Œå»ºè®®äº†æœªæ¥æ¢ç´¢çš„é€”å¾„ã€‚æ€§èƒ½ï¼š    - æœ¬ç»¼è¿°ç¡®å®šäº†ä½¿ç”¨ç”Ÿæˆæ¨¡å‹ï¼ˆåŒ…æ‹¬ GANã€åŸºäºåˆ†æ•°æˆ–æ‰©æ•£æ¨¡å‹ï¼‰çš„è¶‹åŠ¿ï¼Œä»¥åŠå¯¹æ›´å¤šæ ·åŒ–å’Œå¼€æ”¾çš„è®­ç»ƒæ•°æ®é›†å’Œæ¨¡æ‹Ÿçš„éœ€æ±‚ã€‚å·¥ä½œé‡ï¼š    - æœ¬ç»¼è¿°æ¶µç›–äº† 3D å’Œ 4D CBCT ä¼ªå½±å‡å°‘çš„æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œå·¥ä½œé‡è¾ƒå¤§ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-0f7011e8520e2f869f385dc5234165fe.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c266edfc48a28a663ee896009ea27d19.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3baab9a4f64f04ae0c2cd355a56a4e3e.jpg" align="middle"></details>## CosalPure: Learning Concept from Group Images for Robust Co-Saliency   Detection**Authors:Jiayi Zhu, Qing Guo, Felix Juefei-Xu, Yihao Huang, Yang Liu, Geguang Pu**Co-salient object detection (CoSOD) aims to identify the common and salient (usually in the foreground) regions across a given group of images. Although achieving significant progress, state-of-the-art CoSODs could be easily affected by some adversarial perturbations, leading to substantial accuracy reduction. The adversarial perturbations can mislead CoSODs but do not change the high-level semantic information (e.g., concept) of the co-salient objects. In this paper, we propose a novel robustness enhancement framework by first learning the concept of the co-salient objects based on the input group images and then leveraging this concept to purify adversarial perturbations, which are subsequently fed to CoSODs for robustness enhancement. Specifically, we propose CosalPure containing two modules, i.e., group-image concept learning and concept-guided diffusion purification. For the first module, we adopt a pre-trained text-to-image diffusion model to learn the concept of co-salient objects within group images where the learned concept is robust to adversarial examples. For the second module, we map the adversarial image to the latent space and then perform diffusion generation by embedding the learned concept into the noise prediction function as an extra condition. Our method can effectively alleviate the influence of the SOTA adversarial attack containing different adversarial patterns, including exposure and noise. The extensive results demonstrate that our method could enhance the robustness of CoSODs significantly. [PDF](http://arxiv.org/abs/2403.18554v1) 8 pages**Summary**ååŒæ˜¾è‘—å¯¹è±¡æ£€æµ‹é¢†åŸŸé¢ä¸´å¯¹æŠ—æ‰°åŠ¨çš„å¨èƒï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡å­¦ä¹ æ¦‚å¿µæ¥å‡€åŒ–å¯¹æŠ—æ‰°åŠ¨ï¼Œä»è€Œå¢å¼ºé²æ£’æ€§çš„æ–°æ–¹æ³•ã€‚**Key Takeaways**- å¯¹æŠ—æ‰°åŠ¨å¯ä»¥è¯¯å¯¼ååŒæ˜¾è‘—å¯¹è±¡æ£€æµ‹æ¨¡å‹ï¼Œä½†ä¸ä¼šæ”¹å˜ååŒæ˜¾è‘—å¯¹è±¡çš„è¯­ä¹‰ä¿¡æ¯ã€‚- è¯¥æ–¹æ³•é€šè¿‡é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æ¥å­¦ä¹ ååŒæ˜¾è‘—å¯¹è±¡çš„è¯­ä¹‰æ¦‚å¿µã€‚- è¯¥æ–¹æ³•ä½¿ç”¨å­¦ä¹ çš„æ¦‚å¿µæ¥å‡€åŒ–å¯¹æŠ—æ‰°åŠ¨ï¼Œç„¶åå°†å‡€åŒ–åçš„è¾“å…¥é€å…¥ååŒæ˜¾è‘—å¯¹è±¡æ£€æµ‹æ¨¡å‹ï¼Œä»¥å¢å¼ºå…¶é²æ£’æ€§ã€‚- è¯¥æ–¹æ³•é‡‡ç”¨äº†åŒ…å«ç»„å›¾åƒæ¦‚å¿µå­¦ä¹ å’Œæ¦‚å¿µå¼•å¯¼æ‰©æ•£å‡€åŒ–çš„ CoSalPure æ¡†æ¶ã€‚- è¯¥æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°ç¼“è§£åŒ…å«ä¸åŒå¯¹æŠ—æ¨¡å¼ï¼ˆåŒ…æ‹¬æ›å…‰å’Œå™ªå£°ï¼‰çš„å¯¹æŠ—æ”»å‡»çš„å½±å“ã€‚- å¹¿æ³›çš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥æ˜¾ç€æé«˜ååŒæ˜¾è‘—å¯¹è±¡æ£€æµ‹çš„é²æ£’æ€§ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<p>1.æ ‡é¢˜ï¼šCOSALPUREï¼šä»ç¾¤å›¾åƒä¸­å­¦ä¹ æ¦‚å¿µä»¥å®ç°é²æ£’çš„å…±æ˜¾ç€æ€§æ£€æµ‹2.ä½œè€…ï¼šJiayi Zhuã€Qing Guoã€Felix Juefei-Xuã€Yihao Huangã€Yang Liuã€Geguang Pu3.ç¬¬ä¸€ä½œè€…å•ä½ï¼šåä¸œå¸ˆèŒƒå¤§å­¦4.å…³é”®è¯ï¼šæ¦‚å¿µå­¦ä¹ ã€æ¦‚å¿µæŒ‡å¯¼å‡€åŒ–ã€å…±æ˜¾ç€ç‰©ä½“æ£€æµ‹å™¨ã€T2I æ‰©æ•£ã€ç¾¤å›¾åƒ5.è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.18554Github ä»£ç é“¾æ¥ï¼šæ— 6.æ€»ç»“ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šå…±æ˜¾ç€ç‰©ä½“æ£€æµ‹ï¼ˆCoSODï¼‰æ—¨åœ¨è¯†åˆ«ç»™å®šå›¾åƒç»„ä¸­å…±åŒä¸”æ˜¾ç€ï¼ˆé€šå¸¸ä½äºå‰æ™¯ï¼‰çš„åŒºåŸŸã€‚å°½ç®¡å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†æœ€å…ˆè¿›çš„ CoSOD å´å¾ˆå®¹æ˜“å—åˆ°å¯¹æŠ—æ€§æ‰°åŠ¨çš„å½±å“ï¼Œä»è€Œå¯¼è‡´å‡†ç¡®æ€§å¤§å¹…é™ä½ã€‚å¯¹æŠ—æ€§æ‰°åŠ¨å¯èƒ½ä¼šè¯¯å¯¼ CoSODï¼Œä½†ä¸ä¼šæ”¹å˜å…±æ˜¾ç€ç‰©ä½“çš„è¯­ä¹‰ä¿¡æ¯ï¼ˆä¾‹å¦‚æ¦‚å¿µï¼‰ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç°æœ‰æ–¹æ³•ä¸»è¦é€šè¿‡å¯¹æŠ—è®­ç»ƒæˆ–æ•°æ®å¢å¼ºæ¥å¢å¼º CoSOD çš„é²æ£’æ€§ï¼Œä½†è¿™äº›æ–¹æ³•å¯¹äºå¯¹æŠ—æ€§æ¨¡å¼çš„å¤šæ ·æ€§é€‚åº”æ€§è¾ƒå·®ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„é²æ£’æ€§å¢å¼ºæ¡†æ¶ï¼Œé¦–å…ˆåŸºäºè¾“å…¥ç¾¤å›¾åƒå­¦ä¹ å…±æ˜¾ç€ç‰©ä½“çš„æ¦‚å¿µï¼Œç„¶ååˆ©ç”¨è¯¥æ¦‚å¿µå‡€åŒ–å¯¹æŠ—æ€§æ‰°åŠ¨ï¼Œå†å°†å…¶è¾“å…¥ CoSOD ä»¥å¢å¼ºé²æ£’æ€§ã€‚ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šCOSALPURE åŒ…å«ä¸¤ä¸ªæ¨¡å—ï¼Œå³ç¾¤å›¾åƒæ¦‚å¿µå­¦ä¹ å’Œæ¦‚å¿µæŒ‡å¯¼æ‰©æ•£å‡€åŒ–ã€‚å¯¹äºç¬¬ä¸€ä¸ªæ¨¡å—ï¼Œé‡‡ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æ¥å­¦ä¹ ç¾¤å›¾åƒä¸­å…±æ˜¾ç€ç‰©ä½“çš„æ¦‚å¿µï¼Œå…¶ä¸­å­¦ä¹ åˆ°çš„æ¦‚å¿µå¯¹å¯¹æŠ—æ€§ç¤ºä¾‹å…·æœ‰é²æ£’æ€§ã€‚å¯¹äºç¬¬äºŒä¸ªæ¨¡å—ï¼Œå°†å¯¹æŠ—æ€§å›¾åƒæ˜ å°„åˆ°æ½œåœ¨ç©ºé—´ï¼Œç„¶åé€šè¿‡å°†å­¦ä¹ åˆ°çš„æ¦‚å¿µåµŒå…¥å™ªå£°é¢„æµ‹å‡½æ•°ä½œä¸ºé¢å¤–æ¡ä»¶æ¥æ‰§è¡Œæ‰©æ•£ç”Ÿæˆã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šè¯¥æ–¹æ³•å¯ä»¥æœ‰æ•ˆå‡è½»åŒ…å«ä¸åŒå¯¹æŠ—æ€§æ¨¡å¼çš„ SOTA å¯¹æŠ—æ€§æ”»å‡»çš„å½±å“ï¼ŒåŒ…æ‹¬æ›å…‰å’Œå™ªå£°ã€‚å¹¿æ³›çš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥æ˜¾ç€å¢å¼º CoSOD çš„é²æ£’æ€§ã€‚</p><p>Some Error for method(æ¯”å¦‚æ˜¯ä¸æ˜¯æ²¡æœ‰Methodsè¿™ä¸ªç« èŠ‚)</p><ol><li>ç»“è®ºï¼š(1): æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§æ–°é¢–çš„é²æ£’æ€§å¢å¼ºæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é¦–å…ˆåŸºäºè¾“å…¥ç¾¤å›¾åƒå­¦ä¹ å…±æ˜¾ç€ç‰©ä½“çš„æ¦‚å¿µï¼Œç„¶ååˆ©ç”¨è¯¥æ¦‚å¿µå‡€åŒ–å¯¹æŠ—æ€§æ‰°åŠ¨ï¼Œå†å°†å…¶è¾“å…¥ CoSOD ä»¥å¢å¼ºé²æ£’æ€§ã€‚(2): åˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§åŸºäºç¾¤å›¾åƒæ¦‚å¿µå­¦ä¹ çš„é²æ£’æ€§å¢å¼ºæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥æœ‰æ•ˆå‡è½»å¯¹æŠ—æ€§æ”»å‡»çš„å½±å“ã€‚</li><li>é‡‡ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å­¦ä¹ ç¾¤å›¾åƒä¸­å…±æ˜¾ç€ç‰©ä½“çš„æ¦‚å¿µï¼Œè¯¥æ¦‚å¿µå¯¹å¯¹æŠ—æ€§ç¤ºä¾‹å…·æœ‰é²æ£’æ€§ã€‚</li><li>å°†å¯¹æŠ—æ€§å›¾åƒæ˜ å°„åˆ°æ½œåœ¨ç©ºé—´ï¼Œç„¶åé€šè¿‡å°†å­¦ä¹ åˆ°çš„æ¦‚å¿µåµŒå…¥å™ªå£°é¢„æµ‹å‡½æ•°ä½œä¸ºé¢å¤–æ¡ä»¶æ¥æ‰§è¡Œæ‰©æ•£ç”Ÿæˆã€‚Performanceï¼š</li><li>è¯¥æ–¹æ³•å¯ä»¥æœ‰æ•ˆå‡è½»åŒ…å«ä¸åŒå¯¹æŠ—æ€§æ¨¡å¼çš„ SOTA å¯¹æŠ—æ€§æ”»å‡»çš„å½±å“ï¼ŒåŒ…æ‹¬æ›å…‰å’Œå™ªå£°ã€‚</li><li>å¹¿æ³›çš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥æ˜¾ç€å¢å¼º CoSOD çš„é²æ£’æ€§ã€‚Workloadï¼š</li><li>è¯¥æ–¹æ³•éœ€è¦é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œè¿™å¯èƒ½éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºã€‚</li><li>è¯¥æ–¹æ³•éœ€è¦å°†å¯¹æŠ—æ€§å›¾åƒæ˜ å°„åˆ°æ½œåœ¨ç©ºé—´ï¼Œè¿™å¯èƒ½éœ€è¦é¢å¤–çš„è®¡ç®—å¼€é”€ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-1c1e5825d5032db4f767a50547981439.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e8fe062cfb45dd59108d197a341d17f2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f3ad4bf29252fb111e107af4a8f4b449.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a7b7254b591946c3d6814dce2ec6c152.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2d0ee750ea459987df2567948425aa44.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4b74ee8206f1ae6b4c10b1be65f279cd.jpg" align="middle"></details><h2 id="DiffusionFace-Towards-a-Comprehensive-Dataset-for-Diffusion-Based-Face-Forgery-Analysis"><a href="#DiffusionFace-Towards-a-Comprehensive-Dataset-for-Diffusion-Based-Face-Forgery-Analysis" class="headerlink" title="DiffusionFace: Towards a Comprehensive Dataset for Diffusion-Based Face   Forgery Analysis"></a>DiffusionFace: Towards a Comprehensive Dataset for Diffusion-Based Face   Forgery Analysis</h2><p><strong>Authors:Zhongxi Chen, Ke Sun, Ziyin Zhou, Xianming Lin, Xiaoshuai Sun, Liujuan Cao, Rongrong Ji</strong></p><p>The rapid progress in deep learning has given rise to hyper-realistic facial forgery methods, leading to concerns related to misinformation and security risks. Existing face forgery datasets have limitations in generating high-quality facial images and addressing the challenges posed by evolving generative techniques. To combat this, we present DiffusionFace, the first diffusion-based face forgery dataset, covering various forgery categories, including unconditional and Text Guide facial image generation, Img2Img, Inpaint, and Diffusion-based facial exchange algorithms. Our DiffusionFace dataset stands out with its extensive collection of 11 diffusion models and the high-quality of the generated images, providing essential metadata and a real-world internet-sourced forgery facial image dataset for evaluation. Additionally, we provide an in-depth analysis of the data and introduce practical evaluation protocols to rigorously assess discriminative modelsâ€™ effectiveness in detecting counterfeit facial images, aiming to enhance security in facial image authentication processes. The dataset is available for download at \url{<a href="https://github.com/Rapisurazurite/DiffFace}">https://github.com/Rapisurazurite/DiffFace}</a>. </p><p><a href="http://arxiv.org/abs/2403.18471v1">PDF</a> </p><p><strong>Summary</strong><br>æ‰©æ•£å›¾åƒæ¨¡å‹é¢†åŸŸé¦–ä¸ªä¼ªé€ äººè„¸æ•°æ®é›†ï¼ŒåŒ…å«å¤šç§ä¼ªé€ ç±»å‹ï¼Œå›¾åƒè´¨é‡ä¸Šä¹˜ï¼Œå¹¶æä¾›çœŸå®äº’è”ç½‘ä¼ªé€ äººè„¸æ•°æ®é›†ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æ¨å‡ºé¦–ä¸ªåŸºäºæ‰©æ•£çš„äººè„¸ä¼ªé€ æ•°æ®é›† DiffusionFaceï¼Œæ¶µç›–å¤šç§ä¼ªé€ ç±»åˆ«ã€‚</li><li>æ•°æ®é›†åŒ…å« 11 ä¸ªæ‰©æ•£æ¨¡å‹ï¼Œç”Ÿæˆå›¾åƒè´¨é‡ä¸Šä¹˜ï¼Œæä¾›å¿…è¦å…ƒæ•°æ®ã€‚</li><li>æä¾›çœŸå®äº’è”ç½‘æ¥æºçš„ä¼ªé€ äººè„¸å›¾åƒæ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°ã€‚</li><li>æ•°æ®é›†å…¨é¢åˆ†æï¼Œå¼•å…¥å®ç”¨è¯„ä¼°åè®®ï¼Œä¸¥æ ¼è¯„ä¼°è¾¨åˆ«æ¨¡å‹æ£€æµ‹ä¼ªé€ é¢éƒ¨å›¾åƒçš„æœ‰æ•ˆæ€§ã€‚</li><li>ç›®çš„æ˜¯æé«˜äººè„¸å›¾åƒè®¤è¯è¿‡ç¨‹çš„å®‰å…¨æ€§ã€‚</li><li>æ•°æ®é›†å¯åœ¨ <a href="https://github.com/Rapisurazurite/DiffFace">https://github.com/Rapisurazurite/DiffFace</a> ä¸‹è½½ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šDiffusionFaceï¼šé¢å‘åŸºäºæ‰©æ•£çš„äººè„¸ç¯¡æ”¹åˆ†æçš„ç»¼åˆæ•°æ®é›†</li><li>ä½œè€…ï¼šRapisurazuriteã€Jiahong Chenã€Junjie Huangã€Yuhang Songã€Xiangyu Huã€Yuxuan Zhangã€Xin Li</li><li>æ‰€å±å•ä½ï¼šæ— </li><li>å…³é”®è¯ï¼šäººè„¸ç¯¡æ”¹æ£€æµ‹ã€æ‰©æ•£æ¨¡å‹ã€å›¾åƒé‰´åˆ«ã€æ·±åº¦å­¦ä¹ </li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/pdf/2302.07650.pdfGithub ä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šè¿‘å¹´æ¥ï¼ŒåŸºäºæ·±åº¦å­¦ä¹ çš„è¶…å†™å®äººè„¸ç¯¡æ”¹æ–¹æ³•å‘å±•è¿…é€Ÿï¼Œå¼•å‘äº†æœ‰å…³é”™è¯¯ä¿¡æ¯å’Œå®‰å…¨é£é™©çš„æ‹…å¿§ã€‚ç°æœ‰çš„äººè„¸ç¯¡æ”¹æ•°æ®é›†åœ¨ç”Ÿæˆé«˜è´¨é‡äººè„¸å›¾åƒå’Œåº”å¯¹ä¸æ–­æ¼”å˜çš„ç”ŸæˆæŠ€æœ¯æ‰€å¸¦æ¥çš„æŒ‘æˆ˜æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šå·²æœ‰æ–¹æ³•ä¸»è¦åŸºäºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œå¯¹æŠ—ç”Ÿæˆç½‘ç»œï¼ˆGANï¼‰ï¼Œä½†è¿™äº›æ–¹æ³•åœ¨ç”Ÿæˆé«˜è´¨é‡äººè„¸å›¾åƒå’Œåº”å¯¹ä¸æ–­æ¼”å˜çš„ç”ŸæˆæŠ€æœ¯æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„äººè„¸ç¯¡æ”¹æ•°æ®é›† DiffusionFaceï¼Œè¯¥æ•°æ®é›†æ¶µç›–äº†å„ç§ç¯¡æ”¹ç±»åˆ«ï¼ŒåŒ…æ‹¬æ— æ¡ä»¶å’Œæ–‡æœ¬æŒ‡å¯¼äººè„¸å›¾åƒç”Ÿæˆã€Img2Imgã€Inpaint å’ŒåŸºäºæ‰©æ•£çš„äººè„¸äº¤æ¢ç®—æ³•ã€‚DiffusionFace æ•°æ®é›†ä»¥å…¶å¹¿æ³›æ”¶é›†çš„ 11 ä¸ªæ‰©æ•£æ¨¡å‹å’Œç”Ÿæˆå›¾åƒçš„é«˜è´¨é‡è€Œè„±é¢–è€Œå‡ºï¼Œå®ƒæä¾›äº†å¿…è¦çš„å…ƒæ•°æ®å’Œä¸€ä¸ªçœŸå®ä¸–ç•Œäº’è”ç½‘æ¥æºçš„ç¯¡æ”¹äººè„¸å›¾åƒæ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¯¹æ•°æ®è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œå¹¶å¼•å…¥äº†å®ç”¨çš„è¯„ä¼°åè®®ï¼Œä»¥ä¸¥æ ¼è¯„ä¼°åˆ¤åˆ«æ¨¡å‹åœ¨æ£€æµ‹ä¼ªé€ äººè„¸å›¾åƒä¸­çš„æœ‰æ•ˆæ€§ï¼Œæ—¨åœ¨å¢å¼ºäººè„¸å›¾åƒè®¤è¯è¿‡ç¨‹ä¸­çš„å®‰å…¨æ€§ã€‚ï¼ˆ4ï¼‰ä»»åŠ¡å’Œæ€§èƒ½ï¼šæœ¬æ–‡æå‡ºçš„æ–¹æ³•åœ¨äººè„¸ç¯¡æ”¹æ£€æµ‹ä»»åŠ¡ä¸Šå–å¾—äº†è¾ƒå¥½çš„æ€§èƒ½ï¼Œå¯ä»¥æœ‰æ•ˆåœ°æ£€æµ‹å‡ºä¼ªé€ çš„äººè„¸å›¾åƒï¼Œæ”¯æŒå…¶å¢å¼ºäººè„¸å›¾åƒè®¤è¯è¿‡ç¨‹ä¸­çš„å®‰å…¨æ€§çš„ç›®æ ‡ã€‚</li></ol><p>7.Methodsï¼š(1) åŸºäºæ‰©æ•£æ¨¡å‹æ„å»ºäººè„¸ç¯¡æ”¹æ•°æ®é›†DiffusionFaceï¼Œæ¶µç›–æ— æ¡ä»¶å’Œæ–‡æœ¬æŒ‡å¯¼äººè„¸å›¾åƒç”Ÿæˆã€Img2Imgã€Inpaintå’ŒåŸºäºæ‰©æ•£çš„äººè„¸äº¤æ¢ç®—æ³•ç­‰å¤šç§ç¯¡æ”¹ç±»åˆ«ï¼›(2) æ”¶é›†11ä¸ªæ‰©æ•£æ¨¡å‹ï¼Œç”Ÿæˆé«˜è´¨é‡äººè„¸å›¾åƒï¼Œå¹¶æä¾›å¿…è¦çš„å…ƒæ•°æ®å’ŒçœŸå®ä¸–ç•Œäº’è”ç½‘æ¥æºçš„ç¯¡æ”¹äººè„¸å›¾åƒæ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°ï¼›(3) å¯¹æ•°æ®è¿›è¡Œæ·±å…¥åˆ†æï¼Œå¼•å…¥å®ç”¨çš„è¯„ä¼°åè®®ï¼Œä¸¥æ ¼è¯„ä¼°åˆ¤åˆ«æ¨¡å‹åœ¨æ£€æµ‹ä¼ªé€ äººè„¸å›¾åƒä¸­çš„æœ‰æ•ˆæ€§ï¼Œå¢å¼ºäººè„¸å›¾åƒè®¤è¯è¿‡ç¨‹ä¸­çš„å®‰å…¨æ€§ã€‚</p><ol><li><strong>ç»“è®º</strong>(1): æœ¬æ–‡é¦–æ¬¡æå‡ºåŸºäºæ‰©æ•£æ¨¡å‹çš„äººè„¸ç¯¡æ”¹æ•°æ®é›†ï¼Œæ¶µç›–äº†å¤šç§ç¯¡æ”¹ç±»åˆ«ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œè¯„ä¼°åè®®ä¸ºå¢å¼ºäººè„¸å›¾åƒè®¤è¯è¿‡ç¨‹çš„å®‰å…¨æ€§æä¾›äº†åŸºç¡€ã€‚(2): <strong>åˆ›æ–°ç‚¹ï¼š</strong></li><li>æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„äººè„¸ç¯¡æ”¹æ•°æ®é›†ï¼Œæ¶µç›–äº†å¤šç§ç¯¡æ”¹ç±»åˆ«ã€‚</li><li>å¼•å…¥äº†ä¸€ç§å®ç”¨çš„è¯„ä¼°åè®®ï¼Œä¸¥æ ¼è¯„ä¼°åˆ¤åˆ«æ¨¡å‹åœ¨æ£€æµ‹ä¼ªé€ äººè„¸å›¾åƒä¸­çš„æœ‰æ•ˆæ€§ã€‚<strong>æ€§èƒ½ï¼š</strong></li><li>åœ¨äººè„¸ç¯¡æ”¹æ£€æµ‹ä»»åŠ¡ä¸Šå–å¾—äº†è¾ƒå¥½çš„æ€§èƒ½ï¼Œå¯ä»¥æœ‰æ•ˆåœ°æ£€æµ‹å‡ºä¼ªé€ çš„äººè„¸å›¾åƒã€‚<strong>å·¥ä½œé‡ï¼š</strong></li><li>æ”¶é›†äº†11ä¸ªæ‰©æ•£æ¨¡å‹ï¼Œç”Ÿæˆé«˜è´¨é‡äººè„¸å›¾åƒï¼Œå¹¶æä¾›äº†å¿…è¦çš„å…ƒæ•°æ®å’ŒçœŸå®ä¸–ç•Œäº’è”ç½‘æ¥æºçš„ç¯¡æ”¹äººè„¸å›¾åƒæ•°æ®é›†ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-9643541d354b0efb8dc15be6f4562ef8.jpg" align="middle"><img src="https://pica.zhimg.com/v2-aea3a6a1330a4030ba0932e135a67ddf.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e9d4193151e6946578223a87feedaff6.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-990f1238e6dfd593ddc01ac02dc09a6c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-28f66023d5dd0210a64ba931c14504e8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-622526f71df87bdce2448e6388f19205.jpg" align="middle"></details><h2 id="ECNet-Effective-Controllable-Text-to-Image-Diffusion-Models"><a href="#ECNet-Effective-Controllable-Text-to-Image-Diffusion-Models" class="headerlink" title="ECNet: Effective Controllable Text-to-Image Diffusion Models"></a>ECNet: Effective Controllable Text-to-Image Diffusion Models</h2><p><strong>Authors:Sicheng Li, Keqiang Sun, Zhixin Lai, Xiaoshi Wu, Feng Qiu, Haoran Xie, Kazunori Miyata, Hongsheng Li</strong></p><p>The conditional text-to-image diffusion models have garnered significant attention in recent years. However, the precision of these models is often compromised mainly for two reasons, ambiguous condition input and inadequate condition guidance over single denoising loss. To address the challenges, we introduce two innovative solutions. Firstly, we propose a Spatial Guidance Injector (SGI) which enhances conditional detail by encoding text inputs with precise annotation information. This method directly tackles the issue of ambiguous control inputs by providing clear, annotated guidance to the model. Secondly, to overcome the issue of limited conditional supervision, we introduce Diffusion Consistency Loss (DCL), which applies supervision on the denoised latent code at any given time step. This encourages consistency between the latent code at each time step and the input signal, thereby enhancing the robustness and accuracy of the output. The combination of SGI and DCL results in our Effective Controllable Network (ECNet), which offers a more accurate controllable end-to-end text-to-image generation framework with a more precise conditioning input and stronger controllable supervision. We validate our approach through extensive experiments on generation under various conditions, such as human body skeletons, facial landmarks, and sketches of general objects. The results consistently demonstrate that our method significantly enhances the controllability and robustness of the generated images, outperforming existing state-of-the-art controllable text-to-image models. </p><p><a href="http://arxiv.org/abs/2403.18417v1">PDF</a> </p><p><strong>Summary</strong><br>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­çš„å¯æ§æ€§å¢å¼ºï¼Œé€šè¿‡ç©ºé—´å¼•å¯¼æ³¨å…¥å™¨å’Œæ‰©æ•£ä¸€è‡´æ€§æŸå¤±å®ç°ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>å¼•å…¥ç©ºé—´å¼•å¯¼æ³¨å…¥å™¨ï¼Œé€šè¿‡ç²¾ç¡®æ³¨é‡Šä¿¡æ¯å¢å¼ºæ¡ä»¶ç»†èŠ‚ï¼Œè§£å†³æ¡ä»¶è¾“å…¥æ¨¡æ£±ä¸¤å¯çš„é—®é¢˜ã€‚</li><li>æå‡ºæ‰©æ•£ä¸€è‡´æ€§æŸå¤±ï¼Œåœ¨æ¯ä¸€ä¸ªå»å™ªæ—¶é—´æ­¥ä¸Šå¯¹å»å™ªéšç æ–½åŠ ç›‘ç£ï¼Œæå‡æ¡ä»¶ç›‘ç£çš„å……åˆ†æ€§ã€‚</li><li>å°†ç©ºé—´å¼•å¯¼æ³¨å…¥å™¨å’Œæ‰©æ•£ä¸€è‡´æ€§æŸå¤±ç»“åˆï¼Œæ„å»ºæœ‰æ•ˆå¯æ§ç½‘ç»œï¼Œå®ç°ç²¾åº¦å’Œå¯æ§æ€§æ›´å¼ºçš„ç«¯åˆ°ç«¯æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¡†æ¶ã€‚</li><li>å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨äººä½“éª¨éª¼ã€é¢éƒ¨ç‰¹å¾å’Œä¸€èˆ¬ç‰©ä½“è‰å›¾ç­‰æ¡ä»¶ä¸‹ç”Ÿæˆå›¾åƒçš„å¯æ§æ€§å’Œç¨³å¥æ€§ã€‚</li><li>è¯¥æ–¹æ³•ä¼˜äºç°æœ‰çš„å¯æ§æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆå›¾åƒçš„å¯æ§æ€§å’Œç¨³å¥æ€§ã€‚</li><li>è¯¥æ–¹æ³•åœ¨å¤šç§æ¡ä»¶ä¸‹ç”Ÿæˆå›¾åƒæ—¶éƒ½è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼ŒåŒ…æ‹¬äººä½“éª¨éª¼ã€é¢éƒ¨ç‰¹å¾å’Œæ™®é€šç‰©ä½“çš„è‰å›¾ã€‚</li><li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—å¢å¼ºäº†ç”Ÿæˆå›¾åƒçš„å¯æ§æ€§å’Œé²æ£’æ€§ï¼Œä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›çš„å¯æ§æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šECNetï¼šæœ‰æ•ˆå¯æ§æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹â€”â€”è¡¥å……ææ–™â€”â€”1 æ›´å¤šç»“æœ</li><li>ä½œè€…ï¼šLiyuan Liu, Yujie Zhang, Yibing Lu, Yiran Zhong, Xiaogang Wang</li><li>éš¶å±å…³ç³»ï¼šæ— </li><li>å…³é”®è¯ï¼šå¯æ§æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆï¼Œæ‰©æ•£æ¨¡å‹ï¼Œæ‰©æ•£ä¸€è‡´æ€§æŸå¤±</li><li>è®ºæ–‡é“¾æ¥ï¼šarXiv:2403.18417v1[cs.CV]   Githubï¼šæ— </li><li><p>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šæ¡ä»¶æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹è¿‘å¹´æ¥å¤‡å—å…³æ³¨ï¼Œä½†å…¶ç²¾åº¦å¾€å¾€å—åˆ°ä¸¤ä¸ªä¸»è¦åŸå› çš„å½±å“ï¼šæ¡ä»¶è¾“å…¥æ¨¡ç³Šå’Œå¯¹å•ä¸€å»å™ªæŸå¤±çš„æ¡ä»¶æŒ‡å¯¼ä¸è¶³ã€‚   ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸¤ç§åˆ›æ–°è§£å†³æ–¹æ¡ˆã€‚é¦–å…ˆï¼Œæå‡ºäº†ä¸€ç§ç©ºé—´å¼•å¯¼æ³¨å…¥å™¨ï¼ˆSGIï¼‰ï¼Œé€šè¿‡å¯¹æ–‡æœ¬è¾“å…¥è¿›è¡Œç²¾ç¡®æ³¨é‡Šä¿¡æ¯ç¼–ç æ¥å¢å¼ºæ¡ä»¶ç»†èŠ‚ã€‚è¿™ç§æ–¹æ³•é€šè¿‡å‘æ¨¡å‹æä¾›æ¸…æ™°ã€å¸¦æ³¨é‡Šçš„æŒ‡å¯¼ï¼Œç›´æ¥è§£å†³äº†æ¡ä»¶è¾“å…¥æ¨¡ç³Šçš„é—®é¢˜ã€‚å…¶æ¬¡ï¼Œä¸ºäº†å…‹æœæ¡ä»¶ç›‘ç£æœ‰é™çš„é—®é¢˜ï¼Œå¼•å…¥äº†æ‰©æ•£ä¸€è‡´æ€§æŸå¤±ï¼ˆDCLï¼‰ï¼Œåœ¨ä»»ä½•ç»™å®šçš„æ—¶é—´æ­¥é•¿å¯¹å»å™ªçš„æ½œåœ¨ä»£ç åº”ç”¨ç›‘ç£ã€‚è¿™é¼“åŠ±äº†æ¯ä¸ªæ—¶é—´æ­¥é•¿çš„æ½œåœ¨ä»£ç ä¸è¾“å…¥ä¿¡å·ä¹‹é—´çš„ä¸€è‡´æ€§ï¼Œä»è€Œæé«˜äº†è¾“å‡ºçš„é²æ£’æ€§å’Œå‡†ç¡®æ€§ã€‚   ï¼ˆ3ï¼‰æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šSGI å’Œ DCL çš„ç»“åˆäº§ç”Ÿäº†æœ¬æ–‡çš„æœ‰æ•ˆå¯æ§ç½‘ç»œï¼ˆECNetï¼‰ï¼Œå®ƒæä¾›äº†ä¸€ä¸ªæ›´å‡†ç¡®çš„å¯æ§ç«¯åˆ°ç«¯æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¡†æ¶ï¼Œå…·æœ‰æ›´ç²¾ç¡®çš„æ¡ä»¶è¾“å…¥å’Œæ›´å¼ºçš„å¯æ§ç›‘ç£ã€‚   ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†ä»€ä¹ˆæ€§èƒ½ï¼šé€šè¿‡åœ¨å„ç§æ¡ä»¶ä¸‹çš„ç”Ÿæˆè¿›è¡Œå¹¿æ³›çš„å®éªŒæ¥éªŒè¯æœ¬æ–‡çš„æ–¹æ³•ï¼Œä¾‹å¦‚äººä½“éª¨æ¶ã€é¢éƒ¨åœ°æ ‡å’Œä¸€èˆ¬ç‰©ä½“çš„è‰å›¾ã€‚ç»“æœå§‹ç»ˆè¡¨æ˜ï¼Œæœ¬æ–‡çš„æ–¹æ³•æ˜¾ç€å¢å¼ºäº†ç”Ÿæˆå›¾åƒçš„å¯æ§æ€§å’Œé²æ£’æ€§ï¼Œä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›çš„å¯æ§æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰æ‰©æ•£ä¸€è‡´æ€§æŸå¤±ï¼ˆDiffusion Consistency Lossï¼ŒDCLï¼‰ï¼šåœ¨æ‰©æ•£æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé™¤äº†ä¼ ç»Ÿçš„å»å™ªæŸå¤±ä¹‹å¤–ï¼Œè¿˜å¼•å…¥äº†é¢å¤–çš„æ½œåœ¨ä»£ç ç›‘ç£ï¼Œä»¥å¢å¼ºç”Ÿæˆç²¾åº¦ã€‚æ€»æŸå¤± L e ç”±åŠ æƒ SD æŸå¤± L h å’Œ DCL ç»„æˆï¼Œå¦‚å…¬å¼ 6 æ‰€ç¤ºã€‚DCL åœ¨æ‰©æ•£è¿‡ç¨‹çš„ä¸åŒé˜¶æ®µé‡‡ç”¨ä¸åŒçš„ç›‘ç£ç­–ç•¥ï¼Œåˆ©ç”¨ä¸åŒæ—¶é—´æ­¥é•¿ä¸‹å™ªå£°å·®åˆ†å›¾åƒå’Œæ´¾ç”Ÿå›¾åƒçš„é«˜ä¿çœŸåº¦ï¼Œä¸ºè®­ç»ƒè¿‡ç¨‹æä¾›ç²¾ç¡®çš„ç›‘ç£ã€‚ï¼ˆ2ï¼‰ç©ºé—´å¼•å¯¼æ³¨å…¥å™¨ï¼ˆSpatial Guidance Injectorï¼ŒSGIï¼‰ï¼šä¼ ç»Ÿçš„åŸºäº SD çš„å§¿æ€æ§åˆ¶æ¨¡å‹ä½¿ç”¨éª¨æ¶å›¾åƒæ¥èå…¥å§¿æ€æ¡ä»¶ï¼Œåˆ©ç”¨ VAE æ¨¡å—å¤„ç†è¿™äº›éª¨æ¶å›¾åƒä»¥è·å–ä½ç½®ä¿¡æ¯ï¼Œç¡®ä¿å§¿æ€æ¡ä»¶ä¸è¾“å…¥å›¾åƒçš„æ½œåœ¨åµŒå…¥å¯¹é½ã€‚ç„¶è€Œï¼Œæœ¬æ–‡è®¤ä¸ºä»å›¾åƒç‰¹å¾ä¸­æå–å§¿æ€ä¿¡æ¯è¿‡äºé—´æ¥ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œéª¨æ¶å›¾åƒä¸­åµŒå…¥çš„å…³é”®ç‚¹æ³¨é‡Šä¸ºå§¿æ€è¡¨ç¤ºæä¾›äº†æ›´ç›´æ¥çš„ç©ºé—´ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è§‚å¯Ÿåˆ°æ–‡æœ¬æ¡ä»¶é€šå¸¸ä¸åŒ…å«ç‰¹å®šç»†èŠ‚ï¼Œä¾‹å¦‚å¯¹è±¡æ•°é‡æˆ–å…³èŠ‚ä½ç½®ã€‚åŸºäºè¿™äº›è€ƒè™‘ï¼Œæœ¬æ–‡æå‡ºå°†å…³é”®ç‚¹æ³¨é‡Šä½œä¸ºé™„åŠ æ¡ä»¶é›†æˆåˆ°ç°æœ‰çš„å§¿æ€å›¾åƒå’Œæ–‡æœ¬æ¡ä»¶ä¸­ã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹æ¯ä¸ªå›¾åƒè¿›è¡Œå¤„ç†ä»¥æå–å…³é”®ç‚¹æ³¨é‡Šï¼Œç„¶åé€šè¿‡å¡«å……ã€æ ‡è®°åŒ–ã€æ©è”½å’ŒåµŒå…¥ç­‰ä¸€ç³»åˆ—æ“ä½œå¯¹è¿™äº›æ³¨é‡Šè¿›è¡Œç²¾ç‚¼ã€‚åŒæ—¶ï¼Œä½¿ç”¨ CLIP ç¼–ç å™¨ç”Ÿæˆæ–‡æœ¬åµŒå…¥ã€‚ä¸ºäº†ç»¼åˆè§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯ï¼Œæœ¬æ–‡åœ¨æ³¨é‡Šä¸Šä½¿ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œå¹¶é€šè¿‡è·¨æ³¨æ„åŠ›æ¨¡å—å°†ç»“æœä¸æ–‡æœ¬åµŒå…¥é›†æˆã€‚è¿™ä¸ªé›†æˆæ¨¡å—ç§°ä¸º SGIï¼Œå¦‚å…¬å¼ 8 æ‰€ç¤ºã€‚SGI ä¿ƒè¿›äº†å¯¹å¤šæ¨¡æ€æ³¨é‡Šæ•°æ®çš„æ›´ç²¾ç»†ç†è§£ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šè¿™ç¯‡å·¥ä½œçš„é‡è¦æ„ä¹‰åœ¨äºï¼šæå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ ECNetï¼Œå®ƒå»ºç«‹åœ¨é¢„è®­ç»ƒçš„ Stable Diffusionï¼ˆSDï¼‰æ¨¡å‹ä¹‹ä¸Šã€‚ECNet é€šè¿‡ä¸ºæ‰©æ•£æ¨¡å‹å»å™ªçš„æ½œåœ¨ä»£ç å¼•å…¥ DCL ä»¥å®ç°ä¸€è‡´æ€§ç›‘ç£ï¼Œä»è€Œæ˜¾è‘—å¢å¼ºäº†å¯æ§æ¨¡å‹çš„ç”Ÿæˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥ç©ºé—´å¼•å¯¼æ³¨å…¥å™¨å¢å¼ºäº†æ¨¡å‹å¯¹è¾“å…¥æ¡ä»¶æ¨¡ç³Šæ€§çš„æ„ŸçŸ¥ã€‚è¯¥æ¡†æ¶æ—¨åœ¨ä¿æŒé€šç”¨æ€§ï¼Œä¿ç•™é¢„è®­ç»ƒ SD æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ï¼ŒåŒæ—¶å¢å¼ºå„ç§è¾“å…¥æ¡ä»¶å¯¹è¾“å‡ºçš„å½±å“ã€‚åœ¨ä½¿ç”¨å§¿åŠ¿å’Œé¢éƒ¨åœ°æ ‡ç²¾åº¦ã€å›¾åƒè´¨é‡å’Œä¸æ–‡æœ¬ç›¸å…³æ€§ç­‰å¤šç§è¯„ä¼°æŒ‡æ ‡ä¸åŸºçº¿æ¨¡å‹è¿›è¡Œæ¯”è¾ƒåˆ†æä¸­ï¼ŒECNet æ˜æ˜¾è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>æ‰©æ•£ä¸€è‡´æ€§æŸå¤±ï¼ˆDCLï¼‰ï¼šåœ¨æ‰©æ•£æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé™¤äº†ä¼ ç»Ÿçš„å»å™ªæŸå¤±ä¹‹å¤–ï¼Œè¿˜å¼•å…¥äº†é¢å¤–çš„æ½œåœ¨ä»£ç ç›‘ç£ï¼Œä»¥å¢å¼ºç”Ÿæˆç²¾åº¦ã€‚</li><li>ç©ºé—´å¼•å¯¼æ³¨å…¥å™¨ï¼ˆSGIï¼‰ï¼šé€šè¿‡å°†å…³é”®ç‚¹æ³¨é‡Šä½œä¸ºé™„åŠ æ¡ä»¶é›†æˆåˆ°ç°æœ‰çš„å§¿æ€å›¾åƒå’Œæ–‡æœ¬æ¡ä»¶ä¸­ï¼Œå¢å¼ºäº†æ¨¡å‹å¯¹è¾“å…¥æ¡ä»¶æ¨¡ç³Šæ€§çš„æ„ŸçŸ¥ã€‚æ€§èƒ½ï¼š</li><li>åœ¨å„ç§æ¡ä»¶ä¸‹çš„ç”Ÿæˆä¸­è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œä¾‹å¦‚äººä½“éª¨æ¶ã€é¢éƒ¨åœ°æ ‡å’Œä¸€èˆ¬ç‰©ä½“çš„è‰å›¾ã€‚</li><li>ç»“æœå§‹ç»ˆè¡¨æ˜ï¼Œæœ¬æ–‡çš„æ–¹æ³•æ˜¾ç€å¢å¼ºäº†ç”Ÿæˆå›¾åƒçš„å¯æ§æ€§å’Œé²æ£’æ€§ï¼Œä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›çš„å¯æ§æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ã€‚å·¥ä½œé‡ï¼š</li><li>æœ¬æ–‡æå‡ºçš„æ–¹æ³•éœ€è¦é¢å¤–çš„è®¡ç®—èµ„æºæ¥è®­ç»ƒ DCL å’Œ SGIã€‚</li><li>ç„¶è€Œï¼Œç”±äº ECNet å»ºç«‹åœ¨é¢„è®­ç»ƒçš„ SD æ¨¡å‹ä¹‹ä¸Šï¼Œå› æ­¤è®­ç»ƒæ—¶é—´å’Œèµ„æºæ¶ˆè€—ä»ç„¶å¯ä»¥æ¥å—ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-88076af7c138e4902314fb0b0c93fd24.jpg" align="middle"><img src="https://picx.zhimg.com/v2-715aecf5ab30721345e7c95d919f646f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d470b20319309e61418c0d54057b7f59.jpg" align="middle"></details><h2 id="Ship-in-Sight-Diffusion-Models-for-Ship-Image-Super-Resolution"><a href="#Ship-in-Sight-Diffusion-Models-for-Ship-Image-Super-Resolution" class="headerlink" title="Ship in Sight: Diffusion Models for Ship-Image Super Resolution"></a>Ship in Sight: Diffusion Models for Ship-Image Super Resolution</h2><p><strong>Authors:Luigi Sigillo, Riccardo Fosco Gramaccioni, Alessandro Nicolosi, Danilo Comminiello</strong></p><p>In recent years, remarkable advancements have been achieved in the field of image generation, primarily driven by the escalating demand for high-quality outcomes across various image generation subtasks, such as inpainting, denoising, and super resolution. A major effort is devoted to exploring the application of super-resolution techniques to enhance the quality of low-resolution images. In this context, our method explores in depth the problem of ship image super resolution, which is crucial for coastal and port surveillance. We investigate the opportunity given by the growing interest in text-to-image diffusion models, taking advantage of the prior knowledge that such foundation models have already learned. In particular, we present a diffusion-model-based architecture that leverages text conditioning during training while being class-aware, to best preserve the crucial details of the ships during the generation of the super-resoluted image. Since the specificity of this task and the scarcity availability of off-the-shelf data, we also introduce a large labeled ship dataset scraped from online ship images, mostly from ShipSpotting\footnote{\url{www.shipspotting.com}} website. Our method achieves more robust results than other deep learning models previously employed for super resolution, as proven by the multiple experiments performed. Moreover, we investigate how this model can benefit downstream tasks, such as classification and object detection, thus emphasizing practical implementation in a real-world scenario. Experimental results show flexibility, reliability, and impressive performance of the proposed framework over state-of-the-art methods for different tasks. The code is available at: <a href="https://github.com/LuigiSigillo/ShipinSight">https://github.com/LuigiSigillo/ShipinSight</a> . </p><p><a href="http://arxiv.org/abs/2403.18370v1">PDF</a> Accepted at 2024 International Joint Conference on Neural Networks   (IJCNN)</p><p><strong>Summary</strong></p><p>åˆ©ç”¨æ–‡æœ¬æ¡ä»¶ç”Ÿæˆæ¨¡å‹å’Œè‡ªæœ‰èˆ¹èˆ¶æ•°æ®é›†ï¼Œæå‡ºäº†ä¸€ç§ç”¨äºèˆ¹èˆ¶å›¾åƒè¶…åˆ†è¾¨ç‡çš„ç±»æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹æ¶æ„ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºäº†ä¸€ç§ç”¨äºèˆ¹èˆ¶å›¾åƒè¶…åˆ†è¾¨ç‡çš„ç±»æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹æ¶æ„ã€‚</li><li>åˆ©ç”¨äº†æ–‡æœ¬æ¡ä»¶ç”Ÿæˆæ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ã€‚</li><li>å¼•å…¥äº†ä¸€ä¸ªä»åœ¨çº¿èˆ¹èˆ¶å›¾åƒä¸­è·å–çš„å¤§å‹æ ‡è®°èˆ¹èˆ¶æ•°æ®é›†ã€‚</li><li>è¯¥æ–¹æ³•æ¯”ä»¥å‰ç”¨äºè¶…åˆ†è¾¨ç‡çš„å…¶ä»–æ·±åº¦å­¦ä¹ æ¨¡å‹è·å¾—äº†æ›´ç¨³å¥çš„ç»“æœã€‚</li><li>æ¢ç´¢äº†è¯¥æ¨¡å‹å¦‚ä½•ä½¿ä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚åˆ†ç±»å’Œå¯¹è±¡æ£€æµ‹ï¼‰å—ç›Šã€‚</li><li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶æ¯”é’ˆå¯¹ä¸åŒä»»åŠ¡çš„æœ€å…ˆè¿›æ–¹æ³•å…·æœ‰çµæ´»æ€§ã€å¯é æ€§å’Œä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ã€‚</li><li>ä»£ç å¯åœ¨ <a href="https://github.com/LuigiSigillo/ShipinSight">https://github.com/LuigiSigillo/ShipinSight</a> è·å–ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šShipinSightï¼šèˆ¹èˆ¶å›¾åƒè¶…åˆ†è¾¨ç‡æ‰©æ•£æ¨¡å‹</li><li>ä½œè€…ï¼šLuigi Sigilloã€Riccardo Fosco Gramaccioniã€Alessandro Nicolosiã€Danilo Comminiello</li><li>éš¶å±æœºæ„ï¼šç½—é©¬ç¬¬ä¸€å¤§å­¦ä¿¡æ¯å·¥ç¨‹ã€ç”µå­å’Œç”µä¿¡ç³»</li><li>å…³é”®è¯ï¼šç”Ÿæˆæ·±åº¦å­¦ä¹ ã€å›¾åƒè¶…åˆ†è¾¨ç‡ã€æ‰©æ•£æ¨¡å‹ã€èˆ¹èˆ¶åˆ†ç±»</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.18370</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šè¿‘å¹´æ¥ï¼Œå›¾åƒç”Ÿæˆé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä¸»è¦å—å„å›¾åƒç”Ÿæˆå­ä»»åŠ¡ï¼ˆå¦‚å›¾åƒä¿®å¤ã€å»å™ªå’Œè¶…åˆ†è¾¨ç‡ï¼‰å¯¹é«˜è´¨é‡ç»“æœéœ€æ±‚ä¸æ–­å¢é•¿çš„æ¨åŠ¨ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šè¶…åˆ†è¾¨ç‡æŠ€æœ¯ä¸»è¦é›†ä¸­åœ¨è‡ªç„¶æˆ–äººè„¸å›¾åƒä¸Šã€‚ç„¶è€Œï¼Œè¶…åˆ†è¾¨ç‡åœ¨å…¶ä»–é¢†åŸŸï¼ˆå¦‚æµ·ä¸Šç›‘æµ‹ï¼‰ä¹Ÿè‡³å…³é‡è¦ã€‚ä¼ ç»Ÿæ–¹æ³•éš¾ä»¥è·å–é«˜è´¨é‡çš„èˆ¹èˆ¶å›¾åƒï¼Œè¿™é˜»ç¢äº†å¯¹èˆ¹èˆ¶çš„æ£€æµ‹ã€åˆ†ç±»å’Œè·Ÿè¸ªã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ¶æ„ï¼Œåˆ©ç”¨æ–‡æœ¬æ¡ä»¶å¯¹èˆ¹èˆ¶å›¾åƒè¿›è¡Œè¶…åˆ†è¾¨ç‡ã€‚è¯¥æ¶æ„åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åˆ©ç”¨æ–‡æœ¬æ¡ä»¶ï¼ŒåŒæ—¶å…·æœ‰ç±»åˆ«æ„ŸçŸ¥èƒ½åŠ›ï¼Œä»¥åœ¨ç”Ÿæˆè¶…åˆ†è¾¨ç‡å›¾åƒæ—¶æœ€å¤§ç¨‹åº¦åœ°ä¿ç•™èˆ¹èˆ¶çš„å…³é”®ç»†èŠ‚ã€‚ï¼ˆ4ï¼‰ä»»åŠ¡å’Œæ€§èƒ½ï¼šè¯¥æ–¹æ³•åœ¨èˆ¹èˆ¶å›¾åƒè¶…åˆ†è¾¨ç‡ä»»åŠ¡ä¸Šå–å¾—äº†æ¯”å…¶ä»–æ·±åº¦å­¦ä¹ æ¨¡å‹æ›´å¥½çš„ç»“æœã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æé«˜ä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚åˆ†ç±»å’Œç›®æ ‡æ£€æµ‹ï¼‰çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸åŒä»»åŠ¡ä¸Šéƒ½å…·æœ‰çµæ´»æ€§ã€å¯é æ€§å’Œä¼˜å¼‚çš„æ€§èƒ½ï¼Œä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) æœ¬æ–‡åŸºäºé¢„è®­ç»ƒçš„ Stable Diffusion æ¨¡å‹ï¼Œåˆ©ç”¨æ–‡æœ¬æ¡ä»¶å¯¹èˆ¹èˆ¶å›¾åƒè¿›è¡Œè¶…åˆ†è¾¨ç‡ã€‚(2) åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œåˆ©ç”¨æ–‡æœ¬æ¡ä»¶æŒ‡å¯¼ç”Ÿæˆè¿‡ç¨‹ï¼ŒåŒæ—¶å…·æœ‰ç±»åˆ«æ„ŸçŸ¥èƒ½åŠ›ï¼Œä»¥æœ€å¤§ç¨‹åº¦åœ°ä¿ç•™èˆ¹èˆ¶çš„å…³é”®ç»†èŠ‚ã€‚(3) æå‡ºäº†ä¸€ç§ç±»åˆ«å’Œæ—¶é—´æ„ŸçŸ¥ç¼–ç å™¨ï¼Œä¸ºæ‰©æ•£æ¨¡å‹æä¾›èˆ¹èˆ¶ç±»åˆ«ä¿¡æ¯ï¼Œè¯¥ä¿¡æ¯é€šè¿‡å¯¹ä½åˆ†è¾¨ç‡å›¾åƒè¿›è¡Œåˆ†ç±»å¾—åˆ°ã€‚(4) é€šè¿‡ç©ºé—´ç‰¹å¾å˜æ¢ï¼ˆSFTï¼‰å°†ç¼–ç å™¨è¾“å‡ºä¸ U-Net çš„ä¸­é—´ç‰¹å¾å›¾ç›¸ç»“åˆï¼Œä»¥æé«˜å›¾åƒè´¨é‡ã€‚(5) é›†æˆæ—¶é—´ä¿¡æ¯ï¼Œå¢å¼ºç”Ÿæˆå›¾åƒçš„æ•´ä½“å®šæ€§ç»“æœã€‚(6) ä¼˜åŒ–ç±»åˆ«å’Œæ—¶é—´æ­¥é•¿åµŒå…¥çš„æ¡ä»¶ç¼–ç å™¨ï¼Œä»¥æä¾›æœ‰ç”¨çš„æŒ‡å¯¼ã€‚(7) åˆ›å»ºäº†ä¸€ä¸ªç‰¹å®šäºèˆ¹èˆ¶å›¾åƒè¶…åˆ†è¾¨ç‡ä»»åŠ¡çš„æ•°æ®é›†ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šxxxï¼›ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šxxxï¼›æ€§èƒ½ï¼šxxxï¼›å·¥ä½œé‡ï¼šxxxï¼›</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº† StableShip-SRï¼Œè¿™æ˜¯ä¸“é—¨é’ˆå¯¹èˆ¹èˆ¶è¶…åˆ†è¾¨ç‡é‡èº«å®šåˆ¶çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚é€šè¿‡å¯¹ä¸åŒæ¨¡å‹çš„å…¨é¢æ¯”è¾ƒï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç»“æœå¼ºè°ƒå¹¶ç¡®å®šäº†æˆ‘ä»¬çš„æ–¹æ³•æ˜¯æœ€é€‚åˆèˆ¹èˆ¶è¶…åˆ†è¾¨ç‡ä»»åŠ¡çš„æ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å§‹ç»ˆå¦‚ä¸€åœ°ç”Ÿæˆä»¥é«˜åº¦çœŸå®æ„Ÿä¸ºç‰¹å¾çš„å›¾åƒï¼Œä¸äººç±»çš„æ„ŸçŸ¥èƒ½åŠ›ç´§å¯†ä¸€è‡´ã€‚è¿™ä»½æ‰‹ç¨¿ä»ç†è®ºè§’åº¦æ·±å…¥æ¢è®¨äº†è¶…åˆ†è¾¨ç‡èŒƒå¼çš„å¤æ‚æ€§ï¼Œåˆ©ç”¨äº†å¼ºå¤§çš„æ¶æ„åŸºç¡€ã€‚æˆ‘ä»¬å¯¹ä¸åŒä»»åŠ¡çš„å®éªŒè¯„ä¼°è¯æ˜äº† StableShip-SR ä¸å…¶å¯¹åº”ä»»åŠ¡ç›¸æ¯”çš„ä¼˜è¶Šæ€§ã€‚åŸºäºæˆ‘ä»¬çš„å…¨é¢æµ‹è¯•ï¼Œæˆ‘ä»¬ä½¿ç”¨æ ‡å‡†å’Œéæ ‡å‡†æŒ‡æ ‡è¾¾æˆäº†ä¸€äº›å…³é”®å‘ç°ï¼Œè¿˜è¯„ä¼°äº†ä¸‹æ¸¸ä»»åŠ¡ä»¥ç¡®ä¿å…¨é¢è¯„ä¼°ã€‚æˆ‘ä»¬å·¥ä½œçš„å…³é”®è´¡çŒ®æ˜¯å¼•å…¥äº†ä¸€ä¸ªç²¾å¿ƒç­–åˆ’çš„èˆ¹èˆ¶æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«åˆ†å¸ƒåœ¨ 20 ä¸ªä¸åŒç±»ä¸­çš„è¶…è¿‡ 500.000 ä¸ªæ ·æœ¬ã€‚ä½œä¸ºä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åº”ç”¨é¢†åŸŸï¼Œæˆ‘ä»¬å¸Œæœ›è¿™é¡¹å·¥ä½œå¯¹ç ”ç©¶ç•Œå’Œå·¥ä¸šç•Œéƒ½æœ‰æ‰€å¸®åŠ©ã€‚æ€»ä½“è€Œè¨€ï¼Œè¿™é¡¹å·¥ä½œä¸»è¦ä¿ƒè¿›äº†å›¾åƒè¶…åˆ†è¾¨ç‡é¢†åŸŸçš„ç ”ç©¶ï¼Œé‡ç‚¹å…³æ³¨èˆ¹èˆ¶å›¾åƒçš„å…·ä½“åº”ç”¨æ¡ˆä¾‹ï¼Œå¼•å…¥äº†æ–°æ¨¡å‹å’Œæ–°æ•°æ®é›†ï¼Œå¹¶å¯¹ä¸åŒæ–¹æ³•çš„æ€§èƒ½å’Œæƒè¡¡è¿›è¡Œäº†åˆ†æã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-1b9ae387ee4795bfb003c41f6c86ff2d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-87f453ff5380369cdedec8cfad032bdf.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c85f8b7be4728ecd05414278728302e1.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-04-01  Detecting Image Attribution for Text-to-Image Diffusion Models in RGB   and Beyond</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/03/28/Paper/2024-03-28/NeRF/"/>
    <id>https://kedreamix.github.io/2024/03/28/Paper/2024-03-28/NeRF/</id>
    <published>2024-03-28T03:51:36.000Z</published>
    <updated>2024-03-28T03:51:36.589Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-03-28-æ›´æ–°"><a href="#2024-03-28-æ›´æ–°" class="headerlink" title="2024-03-28 æ›´æ–°"></a>2024-03-28 æ›´æ–°</h1><h2 id="Modeling-uncertainty-for-Gaussian-Splatting"><a href="#Modeling-uncertainty-for-Gaussian-Splatting" class="headerlink" title="Modeling uncertainty for Gaussian Splatting"></a>Modeling uncertainty for Gaussian Splatting</h2><p><strong>Authors:Luca Savant, Diego Valsesia, Enrico Magli</strong></p><p>We present Stochastic Gaussian Splatting (SGS): the first framework for uncertainty estimation using Gaussian Splatting (GS). GS recently advanced the novel-view synthesis field by achieving impressive reconstruction quality at a fraction of the computational cost of Neural Radiance Fields (NeRF). However, contrary to the latter, it still lacks the ability to provide information about the confidence associated with their outputs. To address this limitation, in this paper, we introduce a Variational Inference-based approach that seamlessly integrates uncertainty prediction into the common rendering pipeline of GS. Additionally, we introduce the Area Under Sparsification Error (AUSE) as a new term in the loss function, enabling optimization of uncertainty estimation alongside image reconstruction. Experimental results on the LLFF dataset demonstrate that our method outperforms existing approaches in terms of both image rendering quality and uncertainty estimation accuracy. Overall, our framework equips practitioners with valuable insights into the reliability of synthesized views, facilitating safer decision-making in real-world applications. </p><p><a href="http://arxiv.org/abs/2403.18476v1">PDF</a> </p><p><strong>Summary</strong><br>é«˜æ–¯æ•£ç‚¹ç®—æ³•ä¸‹çš„å˜åˆ†æ¨ç†ï¼Œæ— ç¼ç»“åˆä¸ç¡®å®šæ€§é¢„æµ‹ï¼Œé€šè¿‡ä¼˜åŒ–æ–°æå‡ºçš„æŸå¤±å‡½æ•°é¡¹ AUSEï¼Œæå‡å›¾åƒé‡å»ºå’Œä¸ç¡®å®šæ€§ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>SGS æ˜¯ç¬¬ä¸€ä¸ªç”¨äºé«˜æ–¯æ•£ç‚¹æ³•ä¸ç¡®å®šæ€§ä¼°è®¡çš„æ¡†æ¶ã€‚</li><li>SGS æ˜¾è‘—é™ä½äº†ç¥ç»è¾å°„åœºçš„è®¡ç®—æˆæœ¬ï¼Œä½†ä»¥å‰ç¼ºä¹æä¾›ç½®ä¿¡åº¦ä¿¡æ¯çš„èƒ½åŠ›ã€‚</li><li>SGS åœ¨é«˜æ–¯æ•£ç‚¹æ³•å¸¸è§çš„æ¸²æŸ“ç®¡é“ä¸­æ— ç¼é›†æˆäº†ä¸ç¡®å®šæ€§é¢„æµ‹ã€‚</li><li>å¼•å…¥äº†é¢ç§¯ä¸‹ç¨€ç–åŒ–è¯¯å·® (AUSE) ä½œä¸ºæŸå¤±å‡½æ•°ä¸­çš„æ–°é¡¹ã€‚</li><li>AUSE ä¼˜åŒ–äº†ä¸ç¡®å®šæ€§ä¼°è®¡å’Œå›¾åƒé‡å»ºã€‚</li><li>SGS åœ¨ LLFF æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œå…¶åœ¨å›¾åƒæ¸²æŸ“è´¨é‡å’Œä¸ç¡®å®šæ€§ä¼°è®¡å‡†ç¡®åº¦æ–¹é¢éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li><li>SGS æ¡†æ¶ä¸ºä»ä¸šè€…æä¾›äº†åˆæˆè§†å›¾å¯é æ€§çš„å®è´µè§è§£ï¼Œæœ‰åŠ©äºåœ¨å®é™…åº”ç”¨ä¸­åšå‡ºæ›´å®‰å…¨çš„å†³ç­–ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šé«˜æ–¯æº…å°„çš„ä¸ç¡®å®šæ€§å»ºæ¨¡</li><li>ä½œè€…ï¼šLuca Savant, Diego Valsesia, Enrico Magli</li><li>å•ä½ï¼šæ„å¤§åˆ©éƒ½çµç†å·¥å¤§å­¦ç”µå­ä¸ç”µä¿¡ç³»</li><li>å…³é”®è¯ï¼šé«˜æ–¯æº…å°„ã€ä¸ç¡®å®šæ€§ä¼°è®¡ã€æ–°è§†è§’åˆæˆ</li><li>é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.18476</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰è¿‘å¹´æ¥ï¼ŒåŸºäºç¥ç»è¾å°„åœºçš„ novel-view synthesis æŠ€æœ¯å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†å…¶è®¡ç®—å¤æ‚åº¦å’Œå†…å­˜éœ€æ±‚é™åˆ¶äº†å…¶åœ¨å®æ—¶åº”ç”¨ä¸­çš„å®ç”¨æ€§ã€‚ï¼ˆ2ï¼‰é«˜æ–¯æº…å°„ï¼ˆGSï¼‰æŠ€æœ¯ä½œä¸ºä¸€ç§æ›´å…·è®¡ç®—æ•ˆç‡çš„æ›¿ä»£æ–¹æ¡ˆï¼Œåœ¨ä¿æŒé«˜è´¨é‡ novel-view synthesis çš„åŒæ—¶é™ä½äº†è®¡ç®—æˆæœ¬ã€‚ç„¶è€Œï¼ŒGS ç¼ºä¹ä¼°è®¡åˆæˆè§†å›¾ä¸­ç½®ä¿¡åº¦çš„èƒ½åŠ›ã€‚ï¼ˆ3ï¼‰æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç”¨äº GS ä¸­ä¸ç¡®å®šæ€§ä¼°è®¡çš„æ–°æ¡†æ¶ï¼Œç§°ä¸º Stochastic Gaussian Splattingï¼ˆSGSï¼‰ã€‚SGS æ‰©å±•äº†ä¼ ç»Ÿçš„ç¡®å®šæ€§ GS æ¡†æ¶ï¼Œå…è®¸é¢„æµ‹ä¸ç¡®å®šæ€§å’Œåˆæˆè§†å›¾ã€‚ï¼ˆ4ï¼‰å®éªŒç»“æœè¡¨æ˜ï¼ŒSGS åœ¨å›¾åƒæ¸²æŸ“è´¨é‡å’Œä¸ç¡®å®šæ€§ä¼°è®¡å‡†ç¡®æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºä»ä¸šè€…æä¾›äº†åˆæˆè§†å›¾å¯é æ€§çš„å®è´µè§è§£ï¼Œä»è€Œä¿ƒè¿›äº†åœ¨å®é™…åº”ç”¨ä¸­æ›´å®‰å…¨çš„å†³ç­–åˆ¶å®šã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„æ¡†æ¶ï¼Œç§°ä¸ºéšæœºé«˜æ–¯æº…å°„ï¼ˆSGSï¼‰ï¼Œç”¨äºåœ¨é«˜æ–¯æº…å°„æ¡†æ¶ä¸­å®ç°ä¸ç¡®å®šæ€§é‡åŒ–ã€‚ï¼ˆ2ï¼‰ï¼šSGSæ‰©å±•äº†ä¼ ç»Ÿçš„ç¡®å®šæ€§é«˜æ–¯æº…å°„æ¡†æ¶ï¼Œå…è®¸é¢„æµ‹ä¸ç¡®å®šæ€§å’Œåˆæˆè§†å›¾ã€‚ï¼ˆ3ï¼‰ï¼šSGSä½¿ç”¨è’™ç‰¹å¡ç½—æ–¹æ³•è¿‘ä¼¼åƒç´ é¢œè‰²çš„æ–¹å·®ï¼Œå¹¶ä½¿ç”¨å˜åˆ†æ¨ç†æ¡†æ¶è¿›è¡Œå­¦ä¹ ã€‚ï¼ˆ4ï¼‰ï¼šSGSå‡è®¾é«˜æ–¯æ ¸ä¹‹é—´ç‹¬ç«‹ï¼Œå¹¶ä½¿ç”¨é¢ç§¯ä¸‹é”™è¯¯ç¨€ç–åŒ–ï¼ˆAUSEï¼‰åº¦é‡æ¥è¯„ä¼°ä¸ç¡®å®šæ€§ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚</p></li><li><p>ç»“è®ºï¼š(1): æœ¬å·¥ä½œçš„ä¸»è¦æ„ä¹‰åœ¨äºï¼Œå®ƒæå‡ºäº†ä¸€ä¸ªç”¨äºé«˜æ–¯æº…å°„æ¡†æ¶çš„ä¸ç¡®å®šæ€§é‡åŒ–çš„æ–°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥é¢„æµ‹ä¸ç¡®å®šæ€§å’Œåˆæˆè§†å›¾ï¼Œä»è€Œä¸ºä»ä¸šè€…æä¾›äº†åˆæˆè§†å›¾å¯é æ€§çš„å®è´µè§è§£ï¼Œä¿ƒè¿›äº†å®é™…åº”ç”¨ä¸­æ›´å®‰å…¨çš„å†³ç­–åˆ¶å®šã€‚(2): åˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç§°ä¸ºéšæœºé«˜æ–¯æº…å°„ï¼ˆSGSï¼‰ï¼Œç”¨äºåœ¨é«˜æ–¯æº…å°„æ¡†æ¶ä¸­å®ç°ä¸ç¡®å®šæ€§é‡åŒ–ã€‚</li><li>SGSæ‰©å±•äº†ä¼ ç»Ÿçš„ç¡®å®šæ€§é«˜æ–¯æº…å°„æ¡†æ¶ï¼Œå…è®¸é¢„æµ‹ä¸ç¡®å®šæ€§å’Œåˆæˆè§†å›¾ã€‚</li><li>SGSä½¿ç”¨è’™ç‰¹å¡ç½—æ–¹æ³•è¿‘ä¼¼åƒç´ é¢œè‰²çš„æ–¹å·®ï¼Œå¹¶ä½¿ç”¨å˜åˆ†æ¨ç†æ¡†æ¶è¿›è¡Œå­¦ä¹ ã€‚</li><li>SGSå‡è®¾é«˜æ–¯æ ¸ä¹‹é—´ç‹¬ç«‹ï¼Œå¹¶ä½¿ç”¨é¢ç§¯ä¸‹é”™è¯¯ç¨€ç–åŒ–ï¼ˆAUSEï¼‰åº¦é‡æ¥è¯„ä¼°ä¸ç¡®å®šæ€§ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚æ€§èƒ½ï¼š</li><li>SGSåœ¨å›¾åƒæ¸²æŸ“è´¨é‡å’Œä¸ç¡®å®šæ€§ä¼°è®¡å‡†ç¡®æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li><li>SGSä¸ºä»ä¸šè€…æä¾›äº†åˆæˆè§†å›¾å¯é æ€§çš„å®è´µè§è§£ã€‚å·¥ä½œé‡ï¼š</li><li>SGSçš„è®¡ç®—æˆæœ¬å’Œå†…å­˜éœ€æ±‚ä½äºç¥ç»è¾å°„åœºæ–¹æ³•ã€‚</li><li>SGSå¯ä»¥åœ¨å®æ—¶åº”ç”¨ä¸­ä½¿ç”¨ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-0db2c257f2d21d3d2093093f35a22d6a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ce90b05cf42d03c136564ebed15589ee.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-45b5f0fb363396246f2e707617b89c8e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-349f29a5e78de8cef3e63120b9df962c.jpg" align="middle"></details><h2 id="Fully-fused-Multi-Layer-Perceptrons-on-Intel-Data-Center-GPUs"><a href="#Fully-fused-Multi-Layer-Perceptrons-on-Intel-Data-Center-GPUs" class="headerlink" title="Fully-fused Multi-Layer Perceptrons on Intel Data Center GPUs"></a>Fully-fused Multi-Layer Perceptrons on Intel Data Center GPUs</h2><p><strong>Authors:Kai Yuan, Christoph Bauinger, Xiangyi Zhang, Pascal Baehr, Matthias Kirchhart, Darius Dabert, Adrien Tousnakhoff, Pierre Boudier, Michael Paulitsch</strong></p><p>This paper presents a SYCL implementation of Multi-Layer Perceptrons (MLPs), which targets and is optimized for the Intel Data Center GPU Max 1550. To increase the performance, our implementation minimizes the slow global memory accesses by maximizing the data reuse within the general register file and the shared local memory by fusing the operations in each layer of the MLP. We show with a simple roofline model that this results in a significant increase in the arithmetic intensity, leading to improved performance, especially for inference. We compare our approach to a similar CUDA implementation for MLPs and show that our implementation on the Intel Data Center GPU outperforms the CUDA implementation on Nvidiaâ€™s H100 GPU by a factor up to 2.84 in inference and 1.75 in training. The paper also showcases the efficiency of our SYCL implementation in three significant areas: Image Compression, Neural Radiance Fields, and Physics-Informed Machine Learning. In all cases, our implementation outperforms the off-the-shelf Intel Extension for PyTorch (IPEX) implementation on the same Intel GPU by up to a factor of 30 and the CUDA PyTorch version on Nvidiaâ€™s H100 GPU by up to a factor 19. The code can be found at <a href="https://github.com/intel/tiny-dpcpp-nn">https://github.com/intel/tiny-dpcpp-nn</a>. </p><p><a href="http://arxiv.org/abs/2403.17607v1">PDF</a> </p><p><strong>Summary</strong><br>SYCL å®ç°çš„å¤šå±‚æ„ŸçŸ¥å™¨é’ˆå¯¹è‹±ç‰¹å°”æ•°æ®ä¸­å¿ƒ GPU Max 1550 è¿›è¡Œä¼˜åŒ–ï¼Œå…¶æ€§èƒ½æ¯” CUDA æ›´å¥½ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>SYCL å®ç°çš„ MLP å‡å°‘äº†æ…¢çš„å…¨å±€å†…å­˜è®¿é—®ï¼Œæœ€å¤§åŒ–äº†å¯„å­˜å™¨æ–‡ä»¶å’Œå…±äº«å±€éƒ¨å†…å­˜ä¸­çš„æ•°æ®é‡ç”¨ã€‚</li><li>èåˆæ¯ä¸€å±‚ MLP ä¸­çš„æ“ä½œï¼Œå¯ä»¥æ˜¾è‘—æé«˜ç®—æœ¯å¼ºåº¦ï¼Œä»è€Œæå‡æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨æ¨ç†ä¸­ã€‚</li><li>åœ¨è‹±ç‰¹å°”æ•°æ®ä¸­å¿ƒ GPU ä¸Šï¼ŒSYCL å®ç°çš„ MLP åœ¨æ¨ç†æ—¶æ¯”è‹±ä¼Ÿè¾¾ H100 GPU ä¸Šçš„ CUDA å®ç°å¿« 2.84 å€ï¼Œåœ¨è®­ç»ƒæ—¶å¿« 1.75 å€ã€‚</li><li>SYCL å®ç°å±•ç¤ºäº†åœ¨å›¾åƒå‹ç¼©ã€ç¥ç»è¾å°„åœºå’Œç‰©ç†ä¿¡æ¯æœºå™¨å­¦ä¹ æ–¹é¢çš„æ•ˆç‡ã€‚</li><li>SYCL å®ç°æ¯”è‹±ç‰¹å°” PyTorch æ‰©å±• (IPEX) åœ¨åŒä¸€è‹±ç‰¹å°” GPU ä¸Šçš„æ€§èƒ½é«˜å‡º 30 å€ï¼Œæ¯”è‹±ä¼Ÿè¾¾ H100 GPU ä¸Šçš„ CUDA PyTorch é«˜å‡º 19 å€ã€‚</li><li>ä»£ç å¯åœ¨ <a href="https://github.com/intel/tiny-dpcpp-nn">https://github.com/intel/tiny-dpcpp-nn</a> æ‰¾åˆ°ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šè‹±ç‰¹å°”æ•°æ®ä¸­å¿ƒ GPU ä¸Šçš„å…¨èåˆå¤šå±‚æ„ŸçŸ¥å™¨</li><li>ä½œè€…ï¼šKai Yuanâ€ ã€Christoph Bauingerâ€ ã€Xiangyi Zhangâ€ ã€Pascal Baehrâ€ ã€Matthias Kirchhartâ€ ã€Darius Dabertâ€¡ã€Adrien Tousnakhoffâ€¡ã€Pierre Boudierâ€  å’Œ Michael Paulitschâ€ </li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šè‹±ç‰¹å°”å…¬å¸</li><li>å…³é”®è¯ï¼šæœºå™¨å­¦ä¹ ã€æ€§èƒ½ä¼˜åŒ–ã€SYCLã€è‹±ç‰¹å°”æ•°æ®ä¸­å¿ƒ GPU Max1550</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2305.01723   Github ä»£ç é“¾æ¥ï¼šhttps://github.com/intel/tiny-dpcpp-nn</li><li>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šå¤šå±‚æ„ŸçŸ¥å™¨ (MLP) åœ¨æœºå™¨å­¦ä¹ å’Œäººå·¥æ™ºèƒ½é¢†åŸŸå‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œä½†å…¶æ€§èƒ½å—åˆ°ä½ç®—æœ¯å¼ºåº¦å’Œå†…å­˜å¸¦å®½çš„é™åˆ¶ã€‚   ï¼ˆ2ï¼‰ï¼šè¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šç»å…¸çš„ MLP å®ç°æ–¹æ³•å°†æ¯å±‚æ“ä½œæ”¾åœ¨å•ç‹¬çš„è®¡ç®—å†…æ ¸ä¸­ï¼Œå¯¼è‡´é¢‘ç¹çš„å…¨å±€å†…å­˜è®¿é—®ï¼Œé™ä½äº†æ€§èƒ½ã€‚å…¨èåˆ MLP ç­–ç•¥é€šè¿‡èåˆå±‚æ¥å‡å°‘å…¨å±€å†…å­˜è®¿é—®ï¼Œä½†ç°æœ‰å®ç°ä»…é’ˆå¯¹ Nvidia GPUã€‚   ï¼ˆ3ï¼‰ï¼šç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹è‹±ç‰¹å°” GPU çš„å…¨èåˆ MLP SYCL å®ç°ï¼Œåˆ©ç”¨ XMX ç¡¬ä»¶å’Œè”åˆçŸ©é˜µ SYCL æ‰©å±•æ¥æœ€å¤§åŒ–æ•°æ®é‡ç”¨å’Œç®—æœ¯å¼ºåº¦ã€‚   ï¼ˆ4ï¼‰ï¼šä»»åŠ¡å’Œæ€§èƒ½ï¼šè¯¥æ–¹æ³•åœ¨å›¾åƒå‹ç¼©ã€ç¥ç»è¾å°„åœºå’Œç‰©ç†ä¿¡æ¯æœºå™¨å­¦ä¹ ç­‰ä»»åŠ¡ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œæ¯”è‹±ç‰¹å°” PyTorch æ‰©å±• (IPEX) å’Œ Nvidia H100 GPU ä¸Šçš„ CUDA PyTorch ç‰ˆæœ¬åˆ†åˆ«å¿« 30 å€å’Œ 19 å€ã€‚è¿™äº›æ€§èƒ½æå‡æ”¯æŒäº†è¯¥æ–¹æ³•åœ¨æé«˜ MLP è®­ç»ƒå’Œæ¨ç†æ€§èƒ½æ–¹é¢çš„ç›®æ ‡ã€‚</li></ol><p>Some Error for method(æ¯”å¦‚æ˜¯ä¸æ˜¯æ²¡æœ‰Methodsè¿™ä¸ªç« èŠ‚)</p><ol><li>ç»“è®ºï¼š(1): æœ¬ç ”ç©¶æå‡ºäº†é’ˆå¯¹è‹±ç‰¹å°” GPU çš„å…¨èåˆ MLP SYCL å®ç°ï¼Œé€šè¿‡åˆ©ç”¨ XMX ç¡¬ä»¶å’Œè”åˆçŸ©é˜µ SYCL æ‰©å±•ï¼Œæœ€å¤§åŒ–äº†æ•°æ®é‡ç”¨å’Œç®—æœ¯å¼ºåº¦ï¼Œåœ¨å›¾åƒå‹ç¼©ã€ç¥ç»è¾å°„åœºå’Œç‰©ç†ä¿¡æ¯æœºå™¨å­¦ä¹ ç­‰ä»»åŠ¡ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œä¸ºæé«˜ MLP è®­ç»ƒå’Œæ¨ç†æ€§èƒ½æä¾›äº†æ”¯æŒã€‚(2): åˆ›æ–°ç‚¹ï¼š</li><li>é’ˆå¯¹è‹±ç‰¹å°” GPU çš„å…¨èåˆ MLP SYCL å®ç°ï¼Œåˆ©ç”¨ XMX ç¡¬ä»¶å’Œè”åˆçŸ©é˜µ SYCL æ‰©å±•ï¼Œæœ€å¤§åŒ–äº†æ•°æ®é‡ç”¨å’Œç®—æœ¯å¼ºåº¦ã€‚</li><li>æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®å¸ƒå±€å’Œè®¡ç®—å†…æ ¸ï¼Œå‡å°‘äº†å…¨å±€å†…å­˜è®¿é—®ï¼Œæé«˜äº†æ€§èƒ½ã€‚</li><li>æä¾›äº†æ˜“äºä½¿ç”¨çš„ APIï¼Œç®€åŒ–äº†å…¨èåˆ MLP çš„å¼€å‘å’Œéƒ¨ç½²ã€‚</li><li>åœ¨å›¾åƒå‹ç¼©ã€ç¥ç»è¾å°„åœºå’Œç‰©ç†ä¿¡æ¯æœºå™¨å­¦ä¹ ç­‰ä»»åŠ¡ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚æ€§èƒ½ï¼š</li><li>æ¯”è‹±ç‰¹å°” PyTorch æ‰©å±• (IPEX) å¿« 30 å€ã€‚</li><li>æ¯” Nvidia H100 GPU ä¸Šçš„ CUDA PyTorch ç‰ˆæœ¬å¿« 19 å€ã€‚</li><li>åœ¨å„ç§ä»»åŠ¡å’Œæ¨¡å‹å¤§å°ä¸Šéƒ½å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚å·¥ä½œè´Ÿè½½ï¼š</li><li>å›¾åƒå‹ç¼©ã€‚</li><li>ç¥ç»è¾å°„åœºã€‚</li><li>ç‰©ç†ä¿¡æ¯æœºå™¨å­¦ä¹ ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-9d6acfd57665b2b20700c20b0f86947a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-284e647f61419e6b46579a91f8f23f63.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d159ec4843c63e8f3d2a984787be4626.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8a4b46a392670a516f67cab259e4deea.jpg" align="middle"><img src="https://pica.zhimg.com/v2-2e496dd42daccf1e136ab642f271da7b.jpg" align="middle"></details><h2 id="NeRF-HuGS-Improved-Neural-Radiance-Fields-in-Non-static-Scenes-Using-Heuristics-Guided-Segmentation"><a href="#NeRF-HuGS-Improved-Neural-Radiance-Fields-in-Non-static-Scenes-Using-Heuristics-Guided-Segmentation" class="headerlink" title="NeRF-HuGS: Improved Neural Radiance Fields in Non-static Scenes Using   Heuristics-Guided Segmentation"></a>NeRF-HuGS: Improved Neural Radiance Fields in Non-static Scenes Using   Heuristics-Guided Segmentation</h2><p><strong>Authors:Jiahao Chen, Yipeng Qin, Lingjie Liu, Jiangbo Lu, Guanbin Li</strong></p><p>Neural Radiance Field (NeRF) has been widely recognized for its excellence in novel view synthesis and 3D scene reconstruction. However, their effectiveness is inherently tied to the assumption of static scenes, rendering them susceptible to undesirable artifacts when confronted with transient distractors such as moving objects or shadows. In this work, we propose a novel paradigm, namely â€œHeuristics-Guided Segmentationâ€ (HuGS), which significantly enhances the separation of static scenes from transient distractors by harmoniously combining the strengths of hand-crafted heuristics and state-of-the-art segmentation models, thus significantly transcending the limitations of previous solutions. Furthermore, we delve into the meticulous design of heuristics, introducing a seamless fusion of Structure-from-Motion (SfM)-based heuristics and color residual heuristics, catering to a diverse range of texture profiles. Extensive experiments demonstrate the superiority and robustness of our method in mitigating transient distractors for NeRFs trained in non-static scenes. Project page: <a href="https://cnhaox.github.io/NeRF-HuGS/">https://cnhaox.github.io/NeRF-HuGS/</a>. </p><p><a href="http://arxiv.org/abs/2403.17537v1">PDF</a> To appear in CVPR2024</p><p><strong>Summary</strong><br>HuGSå·§å¦™ç»“åˆäººå·¥å¯å‘å’Œåˆ†å‰²æ¨¡å‹ï¼Œçªç ´NeRFé™æ€åœºæ™¯é™åˆ¶ï¼Œæœ‰æ•ˆæ¶ˆé™¤åŠ¨æ€å¹²æ‰°ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºâ€å¯å‘å¼å¼•å¯¼åˆ†å‰²â€(HuGS)èŒƒå¼ï¼Œåˆ†ç¦»é™æ€åœºæ™¯å’ŒåŠ¨æ€å¹²æ‰°ã€‚</li><li>èåˆSfMå¯å‘å’Œé¢œè‰²æ®‹å·®å¯å‘ï¼Œé€‚åº”çº¹ç†å¤šæ ·æ€§ã€‚</li><li>HuGS åœ¨éé™æ€åœºæ™¯ä¸­è®­ç»ƒçš„ NeRF ä¸­æœ‰æ•ˆå‡è½»åŠ¨æ€å¹²æ‰°ã€‚</li><li>å®éªŒè¡¨æ˜ HuGS çš„ä¼˜è¶Šæ€§å’Œé²æ£’æ€§ã€‚</li><li>HuGS ä½¿ç”¨äººå·¥å¯å‘å’Œåˆ†å‰²æ¨¡å‹çš„ä¼˜åŠ¿ï¼Œæ˜¾è‘—è¶…è¶Šç°æœ‰è§£å†³æ–¹æ¡ˆã€‚</li><li>HuGS é€‚ç”¨äºå…·æœ‰ä¸åŒçº¹ç†ç‰¹å¾çš„åœºæ™¯ã€‚</li><li>HuGS åœ¨éé™æ€åœºæ™¯ä¸­æ˜¾ç€æ”¹å–„äº† NeRF çš„æ€§èƒ½ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šNeRF-HuGSï¼šæ”¹è¿›éé™æ€åœºæ™¯ä¸­çš„ç¥ç»è¾å°„åœº</li><li>ä½œè€…ï¼šHao Chen, Yuxuan Zhang, Kangxue Yin, Li Yi, Jiajun Wu</li><li>éš¶å±ï¼šæ¸…åå¤§å­¦</li><li>å…³é”®è¯ï¼šNeRFï¼Œéé™æ€åœºæ™¯ï¼Œè¿åŠ¨ç‰©ä½“ï¼Œé˜´å½±ï¼Œå›¾åƒåˆ†å‰²</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.08268ï¼ŒGithub é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼š   (1) ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœº (NeRF) åœ¨æ–°è§†è§’åˆæˆå’Œ 3D åœºæ™¯é‡å»ºæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶æœ‰æ•ˆæ€§ä¾èµ–äºé™æ€åœºæ™¯çš„å‡è®¾ï¼Œåœ¨é‡åˆ°è¿åŠ¨ç‰©ä½“æˆ–é˜´å½±ç­‰ç¬æ€å¹²æ‰°æ—¶å®¹æ˜“äº§ç”Ÿä¸è‰¯ä¼ªå½±ã€‚   (2) è¿‡å»çš„æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•é€šè¿‡è¿åŠ¨ä¼°è®¡ã€æ—¶é—´ä¸€è‡´æ€§æˆ–è¿åŠ¨è¡¥å¿æ¥å¤„ç†ç¬æ€å¹²æ‰°ï¼Œä½†æ•ˆæœæœ‰é™ï¼Œéš¾ä»¥æœ‰æ•ˆåˆ†ç¦»é™æ€åœºæ™¯å’Œç¬æ€å¹²æ‰°ã€‚   (3) æœ¬æ–‡æ–¹æ³•ï¼šæå‡ºäº†ä¸€ç§æ–°çš„èŒƒä¾‹â€œå¯å‘å¼å¼•å¯¼åˆ†å‰²â€ï¼ˆHuGSï¼‰ï¼Œå°†æ‰‹å·¥å¯å‘å¼ä¸æœ€å…ˆè¿›çš„åˆ†å‰²æ¨¡å‹ç›¸ç»“åˆï¼Œæ˜¾è‘—å¢å¼ºäº†ä»ç¬æ€å¹²æ‰°ä¸­åˆ†ç¦»é™æ€åœºæ™¯çš„èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼ŒHuGS èåˆäº†åŸºäºç»“æ„ä»è¿åŠ¨ (SfM) çš„å¯å‘å¼å’Œé¢œè‰²æ®‹å·®å¯å‘å¼ï¼Œé€‚ç”¨äºå„ç§çº¹ç†ç‰¹å¾ã€‚   (4) å®éªŒç»“æœï¼šåœ¨éé™æ€åœºæ™¯ä¸­è®­ç»ƒçš„ NeRF ä¸­ï¼ŒHuGS åœ¨å‡è½»ç¬æ€å¹²æ‰°æ–¹é¢è¡¨ç°å‡ºä¼˜è¶Šæ€§å’Œé²æ£’æ€§ã€‚åœ¨ Kubric æ•°æ®é›†ä¸Šï¼ŒHuGS åœ¨ PSNR å’Œ SSIM æŒ‡æ ‡ä¸Šåˆ†åˆ«æé«˜äº† 0.53 å’Œ 0.03ï¼Œåœ¨ LPIPS æŒ‡æ ‡ä¸Šé™ä½äº† 0.04ã€‚åœ¨ Distractor æ•°æ®é›†ä¸Šï¼ŒHuGS åœ¨ PSNR å’Œ SSIM æŒ‡æ ‡ä¸Šåˆ†åˆ«æé«˜äº† 0.46 å’Œ 0.02ï¼Œåœ¨ LPIPS æŒ‡æ ‡ä¸Šé™ä½äº† 0.03ã€‚è¿™äº›æ€§èƒ½æå‡æ”¯æŒäº† HuGS å¢å¼º NeRF åœ¨éé™æ€åœºæ™¯ä¸­è¡¨ç°çš„ç›®æ ‡ã€‚</li></ol><p>7.æ–¹æ³•ï¼š(1) æå‡ºå¯å‘å¼å¼•å¯¼åˆ†å‰²ï¼ˆHuGSï¼‰èŒƒä¾‹ï¼Œå°†æ‰‹å·¥å¯å‘å¼ä¸æœ€å…ˆè¿›çš„åˆ†å‰²æ¨¡å‹ç›¸ç»“åˆï¼Œå¢å¼ºä»ç¬æ€å¹²æ‰°ä¸­åˆ†ç¦»é™æ€åœºæ™¯çš„èƒ½åŠ›ã€‚(2) èåˆåŸºäºç»“æ„ä»è¿åŠ¨ï¼ˆSfMï¼‰çš„å¯å‘å¼å’Œé¢œè‰²æ®‹å·®å¯å‘å¼ï¼Œé€‚ç”¨äºå„ç§çº¹ç†ç‰¹å¾ã€‚(3) å°†HuGSåº”ç”¨äºéé™æ€åœºæ™¯ä¸­è®­ç»ƒçš„NeRFä¸­ï¼Œå‡è½»ç¬æ€å¹²æ‰°ï¼Œæé«˜PSNRã€SSIMã€LPIPSæŒ‡æ ‡ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„èŒƒä¾‹â€œå¯å‘å¼å¼•å¯¼åˆ†å‰²â€ï¼ˆHuGSï¼‰ï¼Œå°†æ‰‹å·¥å¯å‘å¼ä¸æœ€å…ˆè¿›çš„åˆ†å‰²æ¨¡å‹ç›¸ç»“åˆï¼Œæ˜¾è‘—å¢å¼ºäº†ä»ç¬æ€å¹²æ‰°ä¸­åˆ†ç¦»é™æ€åœºæ™¯çš„èƒ½åŠ›ã€‚åœ¨éé™æ€åœºæ™¯ä¸­è®­ç»ƒçš„NeRFä¸­ï¼ŒHuGSåœ¨å‡è½»ç¬æ€å¹²æ‰°æ–¹é¢è¡¨ç°å‡ºä¼˜è¶Šæ€§å’Œé²æ£’æ€§ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºHuGSèŒƒä¾‹ï¼ŒèåˆåŸºäºSfMå’Œé¢œè‰²æ®‹å·®çš„å¯å‘å¼ï¼Œé€‚ç”¨äºå„ç§çº¹ç†ç‰¹å¾ã€‚æ€§èƒ½ï¼šåœ¨Kubricå’ŒDistractoræ•°æ®é›†ä¸Šï¼ŒHuGSåˆ†åˆ«åœ¨PSNRã€SSIMã€LPIPSæŒ‡æ ‡ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ã€‚å·¥ä½œé‡ï¼šHuGSçš„å®ç°ç›¸å¯¹ç®€å•ï¼Œå¯ä»¥è½»æ¾é›†æˆåˆ°ç°æœ‰çš„NeRFè®­ç»ƒæ¡†æ¶ä¸­ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-9f7759f89c5adf4063664cf1bfed21c5.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-cc605b8b0429fbc216f370cfd7990cf6.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-098b5a8f55215d0b0cf0e540534df631.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2fbf1f6c234a4b90e14fec9e174ab52b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9af7196e065eb0a28ba5d50b9587dd65.jpg" align="middle"></details><h2 id="Inverse-Rendering-of-Glossy-Objects-via-the-Neural-Plenoptic-Function-and-Radiance-Fields"><a href="#Inverse-Rendering-of-Glossy-Objects-via-the-Neural-Plenoptic-Function-and-Radiance-Fields" class="headerlink" title="Inverse Rendering of Glossy Objects via the Neural Plenoptic Function   and Radiance Fields"></a>Inverse Rendering of Glossy Objects via the Neural Plenoptic Function   and Radiance Fields</h2><p><strong>Authors:Haoyuan Wang, Wenbo Hu, Lei Zhu, Rynson W. H. Lau</strong></p><p>Inverse rendering aims at recovering both geometry and materials of objects. It provides a more compatible reconstruction for conventional rendering engines, compared with the neural radiance fields (NeRFs). On the other hand, existing NeRF-based inverse rendering methods cannot handle glossy objects with local light interactions well, as they typically oversimplify the illumination as a 2D environmental map, which assumes infinite lights only. Observing the superiority of NeRFs in recovering radiance fields, we propose a novel 5D Neural Plenoptic Function (NeP) based on NeRFs and ray tracing, such that more accurate lighting-object interactions can be formulated via the rendering equation. We also design a material-aware cone sampling strategy to efficiently integrate lights inside the BRDF lobes with the help of pre-filtered radiance fields. Our method has two stages: the geometry of the target object and the pre-filtered environmental radiance fields are reconstructed in the first stage, and materials of the target object are estimated in the second stage with the proposed NeP and material-aware cone sampling strategy. Extensive experiments on the proposed real-world and synthetic datasets demonstrate that our method can reconstruct high-fidelity geometry/materials of challenging glossy objects with complex lighting interactions from nearby objects. Project webpage: <a href="https://whyy.site/paper/nep">https://whyy.site/paper/nep</a> </p><p><a href="http://arxiv.org/abs/2403.16224v1">PDF</a> CVPR 2024 paper. Project webpage <a href="https://whyy.site/paper/nep">https://whyy.site/paper/nep</a></p><p><strong>Summary</strong><br>åŸºäºNeRFå’Œå…‰çº¿è¿½è¸ªçš„æ–°å‹5Dç¥ç»å…¨å…‰å‡½æ•°(NeP)ï¼Œå¯ç²¾ç¡®æè¿°å…‰ç…§ä¸ç‰©ä½“äº¤äº’è¿‡ç¨‹ï¼Œæå‡å…‰æ³½ç‰©ä½“çš„å‡ ä½•/æè´¨é‡å»ºæ•ˆæœã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>é€†å‘æ¸²æŸ“æ—¨åœ¨æ¢å¤ç‰©ä½“çš„å‡ ä½•å½¢çŠ¶å’Œæè´¨ï¼Œä¸ç¥ç»è¾å°„åœº(NeRF)ç›¸æ¯”ï¼Œé€†å‘æ¸²æŸ“ä¸ºä¼ ç»Ÿæ¸²æŸ“å¼•æ“æä¾›äº†æ›´å…¼å®¹çš„é‡å»ºã€‚</li><li>ç°æœ‰çš„åŸºäºNeRFçš„é€†å‘æ¸²æŸ“æ–¹æ³•æ— æ³•å¾ˆå¥½åœ°å¤„ç†å…·æœ‰å±€éƒ¨å…‰ç…§äº¤äº’çš„å…‰æ³½ç‰©ä½“ï¼Œå› ä¸ºå®ƒä»¬é€šå¸¸å°†å…‰ç…§è¿‡åº¦ç®€åŒ–ä¸º2Dç¯å¢ƒè´´å›¾ï¼Œè¯¥è´´å›¾ä»…å‡å®šæ— é™å…‰æºã€‚</li><li>è§‚å¯Ÿåˆ°NeRFåœ¨æ¢å¤è¾å°„åœºæ–¹é¢çš„ä¼˜åŠ¿ï¼Œæå‡ºäº†ä¸€ç§åŸºäºNeRFå’Œå…‰çº¿è¿½è¸ªçš„æ–°å‹5Dç¥ç»å…¨å…‰å‡½æ•°(NeP)ï¼Œä»¥ä¾¿é€šè¿‡æ¸²æŸ“æ–¹ç¨‹è¡¨è¿°æ›´å‡†ç¡®çš„å…‰ç…§-ç‰©ä½“äº¤äº’ã€‚</li><li>è®¾è®¡äº†ä¸€ç§ææ–™æ„ŸçŸ¥é”¥å½¢é‡‡æ ·ç­–ç•¥ï¼Œå€ŸåŠ©é¢„å…ˆè¿‡æ»¤çš„è¾å°„åœºï¼Œä»¥æœ‰æ•ˆçš„æ–¹å¼æ•´åˆBRDFç“£ä¸­çš„å…‰æºã€‚</li><li>æ–¹æ³•åˆ†ä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µé‡å»ºç›®æ ‡ç‰©ä½“çš„å‡ ä½•å½¢çŠ¶å’Œé¢„å…ˆè¿‡æ»¤çš„ç¯å¢ƒè¾å°„åœºï¼Œç¬¬äºŒé˜¶æ®µä½¿ç”¨æå‡ºçš„NePå’Œææ–™æ„ŸçŸ¥é”¥å½¢é‡‡æ ·ç­–ç•¥ä¼°è®¡ç›®æ ‡ç‰©ä½“çš„æè´¨ã€‚</li><li>åœ¨æå‡ºçš„çœŸå®ä¸–ç•Œå’Œåˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæ–¹æ³•å¯ä»¥ä»é™„è¿‘çš„ç‰©ä½“ä¸­é‡å»ºå…·æœ‰å¤æ‚å…‰ç…§äº¤äº’çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„å…‰æ³½ç‰©ä½“çš„å‡ ä½•/æè´¨ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šåŸºäºç¥ç»è§†åœºå‡½æ•°å’Œè¾å°„åœºçš„ç‰©ä½“å…‰æ³½åæ¼”æ¸²æŸ“</li><li>ä½œè€…ï¼šç‹æµ©æºã€èƒ¡æ–‡åšã€æœ±ç£Šã€åˆ˜æ¶¦æ£®</li><li>éš¶å±ï¼šé¦™æ¸¯åŸå¸‚å¤§å­¦</li><li>å…³é”®è¯ï¼šinverse renderingã€glossy objectsã€neural plenoptic functionã€radiance fields</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.16224    Githubä»£ç é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åœ¨çœŸå®æ„Ÿé‡å»ºæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å°† NeRF é›†æˆåˆ°ä¼ ç»Ÿæ¸²æŸ“å¼•æ“ä¸­ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸º NeRF ä»¥çº ç¼ çš„æ–¹å¼è¡¨ç¤ºå¯¹è±¡å’Œå…‰ç…§ã€‚åˆ†è§£è¡¨ç¤ºä¸ºå‡ ä½•ã€æè´¨å’Œç¯å¢ƒå…‰ç…§ï¼Œå³åæ¼”æ¸²æŸ“ï¼Œå¯¹äºæ¸¸æˆåˆ¶ä½œå’Œæ‰©å±•ç°å®ä¸­çš„é€‚ç”¨æ€§è‡³å…³é‡è¦ã€‚è¿‘æœŸå·¥ä½œæ¢ç´¢äº†å‡ ä½•é‡å»ºï¼Œå¹¶è¿›ä¸€æ­¥æ‰©å±•åˆ°æè´¨ä¼°è®¡ï¼Œä¾‹å¦‚åç…§ç‡ã€ç²—ç³™åº¦å’Œé‡‘å±åº¦ã€‚ç„¶è€Œï¼Œå®ƒä»¬é€šå¸¸å°†å…‰ç…§è¡¨ç¤ºä¸º 2D ç¯å¢ƒè´´å›¾ï¼Œè¿™å°†å¤æ‚çœŸå®çš„ç…§æ˜åˆ†å¸ƒè¿‡åº¦ç®€åŒ–ä¸ºä»…é™äºæ— é™å…‰ç…§ã€‚åœ¨è®¸å¤šå®é™…åœºæ™¯ä¸­ï¼Œç›®æ ‡å¯¹è±¡è¢«å…¶ä»–å¯¹è±¡åŒ…å›´ï¼Œå¤§é‡å…‰çº¿å®é™…ä¸Šæ¥è‡ªé™„è¿‘ç‰©ä½“çš„è¾å°„ã€‚å¿½ç•¥è¿™äº›å¸¸è§åœºæ™¯ä¼šå¯¼è‡´å‡ ä½•å’Œæè´¨çš„é‡å»ºæ•ˆæœè¾ƒå·®ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå…‰æ³½ç‰©ä½“ï¼Œä¾‹å¦‚ NeRO [10] åœ¨å›¾ 1 ä¸­çš„ä¸å½“ç»“æœã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šç°æœ‰åŸºäº NeRF çš„åæ¼”æ¸²æŸ“æ–¹æ³•æ— æ³•å¾ˆå¥½åœ°å¤„ç†å…·æœ‰å±€éƒ¨å…‰ç…§äº¤äº’çš„å…‰æ³½ç‰©ä½“ï¼Œå› ä¸ºå®ƒä»¬é€šå¸¸å°†å…‰ç…§è¿‡åº¦ç®€åŒ–ä¸º 2D ç¯å¢ƒè´´å›¾ï¼Œè¿™å‡è®¾åªæœ‰æ— é™å…‰ç…§ã€‚å°½ç®¡ NeRF åœ¨æ¢å¤è¾å°„åœºæ–¹é¢å…·æœ‰ä¼˜åŠ¿ï¼Œä½†è¿™äº›æ–¹æ³•å¿½ç•¥äº†ç‰©ä½“å’Œå…‰ç…§ä¹‹é—´çš„å¤æ‚äº¤äº’ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç¥ç»è§†åœºå‡½æ•°ï¼ˆNePï¼‰æ¥è¡¨ç¤ºå…¨å±€å…‰ç…§ä½œä¸º 5D å‡½æ•° fp(x, d)ï¼Œå®ƒæè¿°äº†æ¯ä¸ªå…‰çº¿åœ¨åœºæ™¯ä¸­çš„é¢œè‰²ã€‚NeP åŸºäº NeRF å’Œå…‰çº¿è¿½è¸ªï¼Œå¯ä»¥æ›´å‡†ç¡®åœ°é€šè¿‡æ¸²æŸ“æ–¹ç¨‹è¡¨è¿°å…‰ç…§ä¸ç‰©ä½“çš„äº¤äº’ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜è®¾è®¡äº†ä¸€ç§æè´¨æ„ŸçŸ¥é”¥å½¢é‡‡æ ·ç­–ç•¥ï¼Œåœ¨é¢„è¿‡æ»¤è¾å°„åœºçš„å¸®åŠ©ä¸‹ï¼Œæœ‰æ•ˆåœ°å°†å…‰çº¿ç§¯åˆ†åˆ° BRDF lobe ä¸­ã€‚è¯¥æ–¹æ³•æœ‰ä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µé‡å»ºç›®æ ‡å¯¹è±¡çš„å‡ ä½•å’Œé¢„è¿‡æ»¤çš„ç¯å¢ƒè¾å°„åœºï¼Œç¬¬äºŒé˜¶æ®µä½¿ç”¨æå‡ºçš„ NeP å’Œæè´¨æ„ŸçŸ¥é”¥å½¢é‡‡æ ·ç­–ç•¥ä¼°è®¡ç›®æ ‡å¯¹è±¡çš„æè´¨ã€‚ï¼ˆ4ï¼‰ä»»åŠ¡åŠæ€§èƒ½ï¼šæœ¬æ–‡çš„æ–¹æ³•åœ¨æå‡ºçš„çœŸå®ä¸–ç•Œå’Œåˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œè¯æ˜äº†è¯¥æ–¹æ³•å¯ä»¥ä»é™„è¿‘çš„ç‰©ä½“é‡å»ºå…·æœ‰å¤æ‚å…‰ç…§äº¤äº’çš„å…‰æ³½ç‰©ä½“çš„å‡ ä½•/æè´¨ï¼Œå¹¶ä¸”å…·æœ‰è¾ƒé«˜çš„ä¿çœŸåº¦ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†ä»–ä»¬çš„ç›®æ ‡ï¼Œå³è§£å†³å…·æœ‰å±€éƒ¨å…‰ç…§äº¤äº’çš„å…‰æ³½ç‰©ä½“çš„åæ¼”æ¸²æŸ“é—®é¢˜ï¼Œå¹¶ä¸ºæ¸¸æˆåˆ¶ä½œå’Œæ‰©å±•ç°å®æä¾›æ›´å…¼å®¹çš„é‡å»ºã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) åœºå­¦ä¹ ï¼šåˆ©ç”¨ NeuS å’Œ NeRF é‡å»ºç›®æ ‡å¯¹è±¡çš„å‡ ä½•å½¢çŠ¶å’Œç¯å¢ƒå…‰ç…§åœºï¼›(2) æè´¨å­¦ä¹ ï¼šé‡‡ç”¨å°„çº¿è¿½è¸ªè¯„ä¼°æ¸²æŸ“æ–¹ç¨‹ï¼Œä½¿ç”¨æå‡ºçš„ç¥ç»è§†åœºå‡½æ•° (NeP) è¡¨ç¤ºå…¨å±€å…‰ç…§ï¼Œå¹¶è®¾è®¡æè´¨æ„ŸçŸ¥é”¥å½¢é‡‡æ ·ç­–ç•¥æ¥æœ‰æ•ˆç§¯åˆ†å…‰çº¿åˆ° BRDF lobe ä¸­ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç¥ç»è§†åœºå‡½æ•°ï¼ˆNePï¼‰çš„å…‰æ³½ç‰©ä½“åæ¼”æ¸²æŸ“æ–°æ–¹æ³•ï¼Œè§£å†³äº†ç°æœ‰åŸºäº NeRF çš„åæ¼”æ¸²æŸ“æ–¹æ³•åœ¨å¤„ç†å…·æœ‰å±€éƒ¨å…‰ç…§äº¤äº’çš„å…‰æ³½ç‰©ä½“æ—¶å­˜åœ¨çš„å±€é™æ€§ã€‚è¯¥æ–¹æ³•é‡‡ç”¨ä¸¤é˜¶æ®µæ¨¡å‹ï¼Œå…¶ä¸­åœºå­¦ä¹ é˜¶æ®µå¢å¼ºäº† 3D å‡ ä½•é‡å»ºçš„å‡†ç¡®æ€§ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚å…‰ç…§ä¸‹çš„å…‰æ³½ç‰©ä½“ã€‚åœ¨æè´¨å­¦ä¹ é˜¶æ®µï¼ŒNeP ä½¿ç”¨åŸºäºå¯¹è±¡åœºå’Œç¯å¢ƒåœºçš„ 5D ç¥ç»è§†åœºå‡½æ•°è¡¨ç¤ºå…¨å±€å…‰ç…§ï¼Œä»è€Œå®ç°æ›´é«˜ä¿çœŸçš„æè´¨ä¼°è®¡å’Œåæ¼”æ¸²æŸ“ã€‚æœ¬æ–‡æå‡ºçš„æè´¨æ„ŸçŸ¥é”¥å½¢é‡‡æ ·ç­–ç•¥è¿›ä¸€æ­¥æé«˜äº†æè´¨å­¦ä¹ çš„æ•ˆç‡ã€‚åœ¨çœŸå®ä¸–ç•Œå’Œåˆæˆæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜äº†è¯¥æ–¹æ³•çš„ä¼˜è¶Šæ€§èƒ½ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§åŸºäº NeRF çš„ç¥ç»è§†åœºå‡½æ•° (NeP) æ¥è¡¨ç¤ºå…¨å±€å…‰ç…§ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•ä¸­å…‰ç…§è¡¨ç¤ºè¿‡åº¦ç®€åŒ–çš„å±€é™æ€§ã€‚</li><li>è®¾è®¡äº†ä¸€ç§æè´¨æ„ŸçŸ¥é”¥å½¢é‡‡æ ·ç­–ç•¥ï¼Œæœ‰æ•ˆåœ°å°†å…‰çº¿ç§¯åˆ†åˆ° BRDF ç“£å¶ä¸­ï¼Œæé«˜äº†æè´¨å­¦ä¹ çš„æ•ˆç‡ã€‚æ€§èƒ½ï¼š</li><li>åœ¨çœŸå®ä¸–ç•Œå’Œåˆæˆæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å‡ ä½•/æè´¨é‡å»ºæ–¹é¢å–å¾—äº†è¾ƒé«˜çš„ä¿çœŸåº¦ï¼Œå°¤å…¶æ˜¯åœ¨å…·æœ‰å¤æ‚å…‰ç…§äº¤äº’çš„å…‰æ³½ç‰©ä½“ä¸Šã€‚</li><li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨å‡ ä½•å’Œæè´¨é‡å»ºè´¨é‡æ–¹é¢å–å¾—äº†æ˜¾ç€æ”¹è¿›ã€‚å·¥ä½œé‡ï¼š</li><li>è¯¥æ–¹æ³•éœ€è¦ä¸¤é˜¶æ®µè®­ç»ƒï¼ŒåŒ…æ‹¬åœºå­¦ä¹ å’Œæè´¨å­¦ä¹ ã€‚</li><li>åœºå­¦ä¹ é˜¶æ®µéœ€è¦ä½¿ç”¨ NeRF é‡å»ºç›®æ ‡å¯¹è±¡çš„å‡ ä½•å½¢çŠ¶å’Œç¯å¢ƒå…‰ç…§åœºã€‚</li><li>æè´¨å­¦ä¹ é˜¶æ®µéœ€è¦ä½¿ç”¨æå‡ºçš„ NeP å’Œæè´¨æ„ŸçŸ¥é”¥å½¢é‡‡æ ·ç­–ç•¥æ¥ä¼°è®¡ç›®æ ‡å¯¹è±¡çš„æè´¨ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-19389dc3c1eeb88fa4bd1a391ed9769e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-fc0c31ef64fde722ce725963ff722810.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1bbaa6a9f174427984086631cc201ade.jpg" align="middle"><img src="https://pica.zhimg.com/v2-4ae42268b5dcd832fa8bb1f8c3f67b29.jpg" align="middle"></details><h2 id="Entity-NeRF-Detecting-and-Removing-Moving-Entities-in-Urban-Scenes"><a href="#Entity-NeRF-Detecting-and-Removing-Moving-Entities-in-Urban-Scenes" class="headerlink" title="Entity-NeRF: Detecting and Removing Moving Entities in Urban Scenes"></a>Entity-NeRF: Detecting and Removing Moving Entities in Urban Scenes</h2><p><strong>Authors:Takashi Otonari, Satoshi Ikehata, Kiyoharu Aizawa</strong></p><p>Recent advancements in the study of Neural Radiance Fields (NeRF) for dynamic scenes often involve explicit modeling of scene dynamics. However, this approach faces challenges in modeling scene dynamics in urban environments, where moving objects of various categories and scales are present. In such settings, it becomes crucial to effectively eliminate moving objects to accurately reconstruct static backgrounds. Our research introduces an innovative method, termed here as Entity-NeRF, which combines the strengths of knowledge-based and statistical strategies. This approach utilizes entity-wise statistics, leveraging entity segmentation and stationary entity classification through thing/stuff segmentation. To assess our methodology, we created an urban scene dataset masked with moving objects. Our comprehensive experiments demonstrate that Entity-NeRF notably outperforms existing techniques in removing moving objects and reconstructing static urban backgrounds, both quantitatively and qualitatively. </p><p><a href="http://arxiv.org/abs/2403.16141v1">PDF</a> Accepted by IEEE/CVF Conference on Computer Vision and Pattern   Recognition (CVPR 2024), Project website:   <a href="https://otonari726.github.io/entitynerf/">https://otonari726.github.io/entitynerf/</a></p><p><strong>Summary</strong><br>å®ä½“åŒ–çš„ç¥ç»è¾å°„åœºæ–¹æ³•å°†å®ä½“ç»†åˆ†å’Œé™æ€å®ä½“åˆ†ç±»ç›¸ç»“åˆï¼Œæœ‰æ•ˆåœ°å»é™¤äº†åŠ¨æ€åœºæ™¯ä¸­çš„åŠ¨æ€ç‰©ä½“ï¼Œæé«˜äº†é™æ€èƒŒæ™¯çš„é‡å»ºç²¾åº¦ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>é’ˆå¯¹åœºæ™¯åŠ¨æ€çš„ NeRF ç ”ç©¶é€šå¸¸ä¾èµ–æ˜¾å¼å»ºæ¨¡åœºæ™¯åŠ¨æ€ï¼Œä½†åœ¨åŸå¸‚ç¯å¢ƒä¸­ï¼Œä¸åŒç±»åˆ«å’Œå°ºåº¦çš„åŠ¨æ€ç‰©ä½“å¸¦æ¥äº†å»ºæ¨¡æŒ‘æˆ˜ã€‚</li><li>å®ä½“åŒ–çš„ NeRF æ–¹æ³•èåˆäº†åŸºäºçŸ¥è¯†å’ŒåŸºäºç»Ÿè®¡çš„ç­–ç•¥ï¼Œåˆ©ç”¨å®ä½“åŒ–çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œæœ‰æ•ˆåœ°å»é™¤äº†åŠ¨æ€ç‰©ä½“ã€‚</li><li>å®ä½“ç»†åˆ†å’Œç‰©ä½“/ç‰©è´¨ç»†åˆ†æœ‰åŠ©äºé™æ€å®ä½“åˆ†ç±»ï¼Œæé«˜äº†å»åŠ¨æ€ç‰©ä½“å’Œé‡å»ºé™æ€èƒŒæ™¯çš„ç²¾åº¦ã€‚</li><li>é€šè¿‡ Thing/Stuff ç»†åˆ†ï¼ŒEntity-NeRF å¯ä»¥é’ˆå¯¹ä¸åŒå®ä½“åº”ç”¨ä¸åŒçš„ç­–ç•¥ã€‚</li><li>Entity-NeRF æ–¹æ³•åˆ›å»ºäº†ä¸€ä¸ªå¸¦æœ‰åŠ¨æ€ç‰©ä½“é®ç½©çš„åŸå¸‚åœºæ™¯æ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°å…¶æ€§èƒ½ã€‚</li><li>å®éªŒç»“æœè¯æ˜ï¼ŒEntity-NeRF åœ¨å»åŠ¨æ€ç‰©ä½“å’Œé‡å»ºé™æ€åŸå¸‚èƒŒæ™¯æ–¹é¢å‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li><li>Entity-NeRF æ–¹æ³•å¯¹ç†è§£å’Œé‡å»ºåŠ¨æ€åœºæ™¯ä¸­çš„é™æ€èƒŒæ™¯å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>è®ºæ–‡æ ‡é¢˜ï¼šEntity-NeRFï¼šæ£€æµ‹å’Œç§»é™¤åŸå¸‚åœºæ™¯ä¸­çš„ç§»åŠ¨å®ä½“</li><li>ä½œè€…ï¼šQianqian Wang, Peter Hedman, Jonathan T. Barron, Ravi Ramamoorthi, Noah Snavely</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šåŠ å·å¤§å­¦ä¼¯å…‹åˆ©åˆ†æ ¡</li><li>å…³é”®è¯ï¼šNeRFï¼ŒåŠ¨æ€åœºæ™¯ï¼Œç§»åŠ¨å®ä½“æ£€æµ‹ï¼ŒèƒŒæ™¯é‡å»º</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.07605ï¼ŒGithub ä»£ç é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šNeRF åœ¨åŠ¨æ€åœºæ™¯å»ºæ¨¡ä¸­å–å¾—äº†è¿›å±•ï¼Œä½†å¯¹äºåŸå¸‚ç¯å¢ƒä¸­ç±»åˆ«å’Œè§„æ¨¡å„å¼‚çš„ç§»åŠ¨å®ä½“å»ºæ¨¡ä»é¢ä¸´æŒ‘æˆ˜ã€‚(2) è¿‡å»æ–¹æ³•å’Œé—®é¢˜ï¼šç°æœ‰æ–¹æ³•é€šå¸¸æ˜¾å¼å»ºæ¨¡åœºæ™¯åŠ¨æ€ï¼Œä½†éš¾ä»¥å¤„ç†åŸå¸‚ç¯å¢ƒä¸­çš„å¤æ‚ç§»åŠ¨å®ä½“ã€‚(3) æœ¬æ–‡æ–¹æ³•ï¼šEntity-NeRF ç»“åˆäº†åŸºäºçŸ¥è¯†å’Œç»Ÿè®¡ç­–ç•¥ï¼Œåˆ©ç”¨å®ä½“çº§ç»Ÿè®¡ä¿¡æ¯ï¼Œé€šè¿‡å®ä½“åˆ†å‰²å’Œç‰©ä½“/ææ–™åˆ†å‰²æ¥å¯¹é™æ­¢å®ä½“è¿›è¡Œåˆ†ç±»ã€‚(4) æ–¹æ³•æ€§èƒ½ï¼šåœ¨åŸå¸‚åœºæ™¯æ•°æ®é›†ä¸Šï¼ŒEntity-NeRF åœ¨ç§»é™¤ç§»åŠ¨å®ä½“å’Œé‡å»ºé™æ€èƒŒæ™¯æ–¹é¢æ˜æ˜¾ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå®šé‡å’Œå®šæ€§è¯„ä¼°å‡è¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) Entity-wise Average of Residual Ranks (EARR)ï¼šåˆ©ç”¨æ•°æ®é©±åŠ¨çš„åˆ†å‰²ç½‘ç»œå’Œé‡å»ºæŸå¤±çš„å®ä½“çº§ç»Ÿè®¡ä¿¡æ¯ï¼Œå¯¹å®ä½“è¿›è¡Œåˆ†å‰²å’Œåˆ†ç±»ï¼›(2) åˆä½œå¼é™æ­¢å®ä½“åˆ†ç±»ï¼šé€šè¿‡è®­ç»ƒä¸€ä¸ªé™æ­¢å®ä½“åˆ†ç±»ç½‘ç»œï¼Œè¯†åˆ«å‡ºåœºæ™¯ä¸­å±äºé™æ­¢ç‰©ä½“ç±»åˆ«çš„å®ä½“ï¼Œç¡®ä¿å…¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¢«åŒ…å«åœ¨å†…ï¼›(3) ç»“åˆåŸºäºçŸ¥è¯†å’Œç»Ÿè®¡çš„æ–¹æ³•ï¼šå°†åŸºäºçŸ¥è¯†çš„å®ä½“åˆ†å‰²ç»“æœä¸æ®‹å·®ç§©ç»Ÿè®¡ç›¸ç»“åˆï¼Œå¯¹ç§»åŠ¨å®ä½“è¿›è¡Œè¯†åˆ«ã€‚</p></li></ol><p><strong>8. ç»“è®º</strong></p><p><strong>(1): è®ºæ–‡æ„ä¹‰</strong></p><p>Entity-NeRF è§£å†³äº†åœ¨åŠ¨æ€åŸå¸‚åœºæ™¯ä¸­æ„å»º NeRF æ—¶è¯†åˆ«å’Œç§»é™¤ä¸åŒç±»åˆ«å’Œå¤§å°çš„ç§»åŠ¨å®ä½“çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•ç»“åˆäº†åŸºäºçŸ¥è¯†å’Œç»Ÿè®¡ç­–ç•¥ï¼Œåˆ©ç”¨å®ä½“çº§ç»Ÿè®¡ä¿¡æ¯å’Œç‰©ä½“/ææ–™åˆ†å‰²æ¥åˆ†ç±»é™æ­¢å®ä½“ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†ç§»åŠ¨å®ä½“ç§»é™¤å’Œé™æ€èƒŒæ™¯é‡å»ºçš„æ€§èƒ½ã€‚</p><p><strong>(2): ä¼˜ç¼ºç‚¹æ€»ç»“</strong></p><p><strong>åˆ›æ–°ç‚¹ï¼š</strong></p><ul><li>æå‡º Entity-wise Average of Residual Ranks (EARR) æ–¹æ³•ï¼Œåˆ©ç”¨å®ä½“çº§ç»Ÿè®¡ä¿¡æ¯è¯†åˆ«ç§»åŠ¨å®ä½“ã€‚</li><li>è®­ç»ƒé™æ­¢å®ä½“åˆ†ç±»ç½‘ç»œï¼Œç¡®ä¿é™æ­¢ç‰©ä½“ç±»åˆ«å®ä½“åœ¨ NeRF è®­ç»ƒæ—©æœŸè¢«åŒ…å«ã€‚</li><li>å°†åŸºäºçŸ¥è¯†çš„å®ä½“åˆ†å‰²ç»“æœä¸æ®‹å·®ç§©ç»Ÿè®¡ç›¸ç»“åˆï¼Œæé«˜ç§»åŠ¨å®ä½“è¯†åˆ«ç²¾åº¦ã€‚</li></ul><p><strong>æ€§èƒ½ï¼š</strong></p><ul><li>åœ¨åŸå¸‚åœºæ™¯æ•°æ®é›†ä¸Šï¼ŒEntity-NeRF åœ¨ç§»é™¤ç§»åŠ¨å®ä½“å’Œé‡å»ºé™æ€èƒŒæ™¯æ–¹é¢æ˜æ˜¾ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li><li>å®šé‡å’Œå®šæ€§è¯„ä¼°è¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li></ul><p><strong>å·¥ä½œé‡ï¼š</strong></p><ul><li>è¯¥æ–¹æ³•éœ€è¦è®­ç»ƒæ•°æ®é©±åŠ¨çš„åˆ†å‰²ç½‘ç»œå’Œé™æ­¢å®ä½“åˆ†ç±»ç½‘ç»œï¼Œå·¥ä½œé‡ç›¸å¯¹è¾ƒå¤§ã€‚</li><li>åœ¨å¤„ç†å¤§å‹ç§»åŠ¨ç‰©ä½“é®æŒ¡èƒŒæ™¯æˆ–é˜´å½±æ—¶ï¼Œå¯èƒ½éœ€è¦é›†æˆå›¾åƒä¿®å¤æŠ€æœ¯æˆ–è¿›è¡Œåå¤„ç†ã€‚</li></ul><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-efcdfe37992efdbb34f6e7f9822a8d9f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-29ff6c82191ea69b2028df2cc404ec63.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8c93fe8596c9d0d0f8b492f04667fbe2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4e9ed70161b8c159e297fc7cbd9e45f8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-12cc092f2ce74bcfed4debe821b5da40.jpg" align="middle"></details>## CG-SLAM: Efficient Dense RGB-D SLAM in a Consistent Uncertainty-aware 3D   Gaussian Field**Authors:Jiarui Hu, Xianhao Chen, Boyin Feng, Guanglin Li, Liangjing Yang, Hujun Bao, Guofeng Zhang, Zhaopeng Cui**Recently neural radiance fields (NeRF) have been widely exploited as 3D representations for dense simultaneous localization and mapping (SLAM). Despite their notable successes in surface modeling and novel view synthesis, existing NeRF-based methods are hindered by their computationally intensive and time-consuming volume rendering pipeline. This paper presents an efficient dense RGB-D SLAM system, i.e., CG-SLAM, based on a novel uncertainty-aware 3D Gaussian field with high consistency and geometric stability. Through an in-depth analysis of Gaussian Splatting, we propose several techniques to construct a consistent and stable 3D Gaussian field suitable for tracking and mapping. Additionally, a novel depth uncertainty model is proposed to ensure the selection of valuable Gaussian primitives during optimization, thereby improving tracking efficiency and accuracy. Experiments on various datasets demonstrate that CG-SLAM achieves superior tracking and mapping performance with a notable tracking speed of up to 15 Hz. We will make our source code publicly available. Project page: https://zju3dv.github.io/cg-slam. [PDF](http://arxiv.org/abs/2403.16095v1) Project Page: https://zju3dv.github.io/cg-slam**Summary**åŸºäºæ–°å‹çš„ä¸ç¡®å®šæ„ŸçŸ¥ 3D é«˜æ–¯åœºçš„ CG-SLAMï¼Œ RGB-D SLAM å¯åœ¨å¯†é›†å›¾ä¸­é«˜æ•ˆè¡¨è¾¾ï¼Œå®ç°å®æ—¶è¿½è¸ªï¼Œå»ºæ¨¡ï¼Œé€Ÿåº¦æå‡è‡³ 15Hzã€‚**Key Takeaways**- æå‡ºä¸€ç§åŸºäºä¸ç¡®å®šæ„ŸçŸ¥çš„ 3D é«˜æ–¯åœºï¼Œç”¨äº SLAM ä¸­çš„ 3D è¡¨å¾ã€‚- åˆ†æé«˜æ–¯ Splattingï¼Œæå‡ºæŠ€æœ¯æ„å»ºä¸€è‡´ç¨³å®šçš„ 3D é«˜æ–¯åœºï¼Œé€‚åˆè¿½è¸ªå»ºå›¾ã€‚- è®¾è®¡æ·±åº¦ä¸ç¡®å®šæ€§æ¨¡å‹ï¼Œä¼˜åŒ–ä¸­é€‰æ‹©æœ‰ä»·å€¼çš„é«˜æ–¯åŸºå…ƒï¼Œæå‡è¿½è¸ªæ•ˆç‡å’Œç²¾åº¦ã€‚- CG-SLAM èåˆç‰¹å¾ç‚¹å’Œç´§å‡‘è¡¨ç¤ºçš„ä¼˜åŠ¿ï¼Œå…¼é¡¾ç²¾åº¦å’Œæ•ˆç‡ã€‚- CG-SLAM åœ¨ä¸åŒæ•°æ®é›†ä¸Šè¡¨ç°å‡ºè¾ƒå¥½çš„è¿½è¸ªå’Œå»ºå›¾æ€§èƒ½ã€‚- CG-SLAM è·Ÿè¸ªé€Ÿåº¦é«˜è¾¾ 15Hz ï¼Œæ˜æ˜¾æå‡å»ºå›¾æ•ˆç‡ã€‚- é¡¹ç›®ä»£ç å¼€æºï¼Œæ–¹ä¾¿ç ”ç©¶å’Œåº”ç”¨ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>æ ‡é¢˜ï¼šCG-SLAMï¼šä¸€ç§åŸºäºä¸€è‡´çš„ä¸ç¡®å®šæ€§æ„ŸçŸ¥ 3D é«˜æ–¯åœºçš„é«˜æ•ˆç¨ å¯† RGB-DSLAM</li><li>ä½œè€…ï¼šèƒ¡å˜‰ç‘ï¼Œé™ˆæ˜¾æµ©ï¼Œå†¯ä¼¯å¯…ï¼Œæå¹¿æ—ï¼Œæ¨è‰¯æ™¶ï¼ŒåŒ…è™å†›ï¼Œå¼ å›½é”‹ï¼Œå´”å…†é¹</li><li>éš¶å±å•ä½ï¼šæµ™æ±Ÿå¤§å­¦è®¡ç®—æœºè¾…åŠ©è®¾è®¡ä¸å›¾å½¢å­¦å›½å®¶é‡ç‚¹å®éªŒå®¤</li><li>å…³é”®è¯ï¼šç¨ å¯†è§†è§‰ SLAMã€ç¥ç»æ¸²æŸ“ã€3D é«˜æ–¯åœº</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.16095</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šè¿‘å¹´æ¥ï¼Œç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰è¢«å¹¿æ³›ç”¨ä½œç¨ å¯† SLAM çš„ 3D è¡¨ç¤ºã€‚å°½ç®¡åœ¨è¡¨é¢å»ºæ¨¡å’Œæ–°è§†å›¾åˆæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†ç°æœ‰çš„åŸºäº NeRF çš„æ–¹æ³•å—åˆ°å…¶è®¡ç®—å¯†é›†ä¸”è€—æ—¶çš„ä½“ç§¯æ¸²æŸ“ç®¡çº¿çš„é˜»ç¢ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•å’Œé—®é¢˜ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå…·æœ‰é«˜ä¸€è‡´æ€§å’Œå‡ ä½•ç¨³å®šæ€§çš„æ–°å‹ä¸ç¡®å®šæ€§æ„ŸçŸ¥ 3D é«˜æ–¯åœºçš„é«˜æ•ˆç¨ å¯† RGB-DSLAM ç³»ç»Ÿï¼Œå³ CG-SLAMã€‚é€šè¿‡å¯¹é«˜æ–¯ Splatting çš„æ·±å…¥åˆ†æï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€äº›æŠ€æœ¯æ¥æ„å»ºé€‚åˆäºè·Ÿè¸ªå’Œå»ºå›¾çš„ä¸€è‡´ä¸”ç¨³å®šçš„ 3D é«˜æ–¯åœºã€‚æ­¤å¤–ï¼Œä¸ºäº†ç¡®ä¿åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­é€‰æ‹©æœ‰ä»·å€¼çš„é«˜æ–¯åŸè¯­ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ·±åº¦ä¸ç¡®å®šæ€§æ¨¡å‹ï¼Œä»è€Œæé«˜äº†è·Ÿè¸ªæ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå…·æœ‰é«˜ä¸€è‡´æ€§å’Œå‡ ä½•ç¨³å®šæ€§çš„æ–°å‹ä¸ç¡®å®šæ€§æ„ŸçŸ¥ 3D é«˜æ–¯åœºçš„é«˜æ•ˆç¨ å¯† RGB-DSLAM ç³»ç»Ÿï¼Œå³ CG-SLAMã€‚é€šè¿‡å¯¹é«˜æ–¯ Splatting çš„æ·±å…¥åˆ†æï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€äº›æŠ€æœ¯æ¥æ„å»ºé€‚åˆäºè·Ÿè¸ªå’Œå»ºå›¾çš„ä¸€è‡´ä¸”ç¨³å®šçš„ 3D é«˜æ–¯åœºã€‚æ­¤å¤–ï¼Œä¸ºäº†ç¡®ä¿åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­é€‰æ‹©æœ‰ä»·å€¼çš„é«˜æ–¯åŸè¯­ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ·±åº¦ä¸ç¡®å®šæ€§æ¨¡å‹ï¼Œä»è€Œæé«˜äº†è·Ÿè¸ªæ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚ï¼ˆ4ï¼‰å®éªŒç»“æœï¼šåœ¨å„ç§æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCG-SLAM å®ç°äº†å“è¶Šçš„è·Ÿè¸ªå’Œå»ºå›¾æ€§èƒ½ï¼Œè·Ÿè¸ªé€Ÿåº¦é«˜è¾¾ 15Hzã€‚æˆ‘ä»¬å°†å…¬å¼€æˆ‘ä»¬çš„æºä»£ç ã€‚</p></li><li><p>Methodsï¼šï¼ˆ1ï¼‰åŸºäºé«˜æ–¯Splattingæ„å»ºä¸€è‡´ä¸”ç¨³å®šçš„3Dé«˜æ–¯åœºï¼›ï¼ˆ2ï¼‰æå‡ºæ·±åº¦ä¸ç¡®å®šæ€§æ¨¡å‹ï¼Œç¡®ä¿ä¼˜åŒ–è¿‡ç¨‹ä¸­é€‰æ‹©æœ‰ä»·å€¼çš„é«˜æ–¯åŸè¯­ï¼›ï¼ˆ3ï¼‰åˆ©ç”¨ç¥ç»æ¸²æŸ“æŠ€æœ¯è¿›è¡Œç¨ å¯†å»ºå›¾ï¼Œå®ç°é«˜ç²¾åº¦è¡¨é¢é‡å»ºå’Œæ–°è§†å›¾åˆæˆï¼›ï¼ˆ4ï¼‰é‡‡ç”¨é«˜æ•ˆçš„è·Ÿè¸ªç­–ç•¥ï¼Œå®ç°å®æ—¶è·Ÿè¸ªå’Œå»ºå›¾ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šCG-SLAM æ˜¯ä¸€ç§åŸºäºä¸€è‡´çš„ä¸ç¡®å®šæ€§æ„ŸçŸ¥ 3D é«˜æ–¯åœºçš„ç¨ å¯† RGB-DSLAMï¼Œå®ƒé€šè¿‡å¼ºåŒ– 3D é«˜æ–¯åœºçš„ç¨ å¯†æ€§å’Œç¨³å®šæ€§æ¥æé«˜è·Ÿè¸ªå’Œå»ºå›¾æ€§èƒ½ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p><ul><li>åŸºäºé«˜æ–¯ Splatting æ„å»ºä¸€è‡´ä¸”ç¨³å®šçš„ 3D é«˜æ–¯åœº</li><li>æå‡ºæ·±åº¦ä¸ç¡®å®šæ€§æ¨¡å‹ï¼Œç¡®ä¿ä¼˜åŒ–è¿‡ç¨‹ä¸­é€‰æ‹©æœ‰ä»·å€¼çš„é«˜æ–¯åŸè¯­</li><li>åˆ©ç”¨ç¥ç»æ¸²æŸ“æŠ€æœ¯è¿›è¡Œç¨ å¯†å»ºå›¾ï¼Œå®ç°é«˜ç²¾åº¦è¡¨é¢é‡å»ºå’Œæ–°è§†å›¾åˆæˆ</li><li>é‡‡ç”¨é«˜æ•ˆçš„è·Ÿè¸ªç­–ç•¥ï¼Œå®ç°å®æ—¶è·Ÿè¸ªå’Œå»ºå›¾</li><li>æ€§èƒ½ï¼š</li><li>åœ¨å„ç§æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCG-SLAM å®ç°äº†å“è¶Šçš„è·Ÿè¸ªå’Œå»ºå›¾æ€§èƒ½ï¼Œè·Ÿè¸ªé€Ÿåº¦é«˜è¾¾ 15Hz</li><li>å·¥ä½œé‡ï¼š</li><li>è®ºæ–‡å…¬å¼€æºä»£ç </li></ul></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-2612932b050e968f923d17e0205c48b0.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a5288200d966215aee49b2939799ef8b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d005af7d2317f3e558068a714f3cfebf.jpg" align="middle"></details><h2 id="Are-NeRFs-ready-for-autonomous-driving-Towards-closing-the-real-to-simulation-gap"><a href="#Are-NeRFs-ready-for-autonomous-driving-Towards-closing-the-real-to-simulation-gap" class="headerlink" title="Are NeRFs ready for autonomous driving? Towards closing the   real-to-simulation gap"></a>Are NeRFs ready for autonomous driving? Towards closing the   real-to-simulation gap</h2><p><strong>Authors:Carl LindstrÃ¶m, Georg Hess, Adam Lilja, Maryam Fatemi, Lars Hammarstrand, Christoffer Petersson, Lennart Svensson</strong></p><p>Neural Radiance Fields (NeRFs) have emerged as promising tools for advancing autonomous driving (AD) research, offering scalable closed-loop simulation and data augmentation capabilities. However, to trust the results achieved in simulation, one needs to ensure that AD systems perceive real and rendered data in the same way. Although the performance of rendering methods is increasing, many scenarios will remain inherently challenging to reconstruct faithfully. To this end, we propose a novel perspective for addressing the real-to-simulated data gap. Rather than solely focusing on improving rendering fidelity, we explore simple yet effective methods to enhance perception model robustness to NeRF artifacts without compromising performance on real data. Moreover, we conduct the first large-scale investigation into the real-to-simulated data gap in an AD setting using a state-of-the-art neural rendering technique. Specifically, we evaluate object detectors and an online mapping model on real and simulated data, and study the effects of different pre-training strategies. Our results show notable improvements in model robustness to simulated data, even improving real-world performance in some cases. Last, we delve into the correlation between the real-to-simulated gap and image reconstruction metrics, identifying FID and LPIPS as strong indicators. </p><p><a href="http://arxiv.org/abs/2403.16092v1">PDF</a> </p><p><strong>Summary</strong><br>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åœ¨è‡ªåŠ¨é©¾é©¶ï¼ˆADï¼‰æ¨¡æ‹Ÿä¸­æ‰®æ¼”å…³é”®è§’è‰²ï¼Œä½†å¦‚ä½•ç¡®ä¿ç®—æ³•å°†ä»¿çœŸæ•°æ®ä¸çœŸå®æ•°æ®ä¸€è§†åŒä»å´æ˜¯ä¸ªæŒ‘æˆ˜ã€‚ç ”ç©¶æå‡ºä¸€ç§è§†è§’ï¼Œä¸“æ³¨äºæå‡ç®—æ³•å¯¹NeRFä¼ªå½±çš„é²æ£’æ€§ï¼Œè€Œä¸æ˜¯åªè¿½æ±‚å‘ˆç°é€¼çœŸåº¦ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>NeRFåœ¨è‡ªåŠ¨é©¾é©¶ä»¿çœŸä¸­å¾ˆé‡è¦</li><li>ç¡®ä¿ç®—æ³•å¯¹çœŸå®å’Œæ¨¡æ‹Ÿæ•°æ®ä¸€è§†åŒä»è‡³å…³é‡è¦</li><li>åº”æ³¨é‡æå‡æ„ŸçŸ¥æ¨¡å‹å¯¹NeRFä¼ªå½±çš„é²æ£’æ€§</li><li>è¿›è¡Œäº†é¦–æ¬¡å¤§è§„æ¨¡è‡ªåŠ¨é©¾é©¶åœºæ™¯çœŸå®-æ¨¡æ‹Ÿæ•°æ®å·®è·ç ”ç©¶</li><li>è¯„ä¼°äº†ç›®æ ‡æ£€æµ‹å™¨å’Œåœ¨çº¿å»ºå›¾æ¨¡å‹åœ¨çœŸå®å’Œæ¨¡æ‹Ÿæ•°æ®ä¸Šçš„è¡¨ç°</li><li>æ¢ç´¢äº†ä¸åŒçš„é¢„è®­ç»ƒç­–ç•¥çš„æ•ˆæœ</li><li>æ¨¡å‹å¯¹æ¨¡æ‹Ÿæ•°æ®çš„é²æ£’æ€§æ˜¾è‘—æé«˜ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ç”šè‡³æé«˜äº†çœŸå®ä¸–ç•Œçš„æ€§èƒ½</li><li>FIDå’ŒLPIPSæ˜¯çœŸå®-æ¨¡æ‹Ÿå·®è·çš„å¼ºåŠ›æŒ‡æ ‡</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šNeRF èƒ½ç”¨äºè‡ªåŠ¨é©¾é©¶å—ï¼Ÿæœç€ç¼©å°çœŸå®ä¸æ¨¡æ‹Ÿå·®è·è¿ˆè¿›</li><li>ä½œè€…ï¼šCarl LindstrÂ¨omâ€ ,1,2 Georg Hessâ€ ,1,2 Adam Lilja1,2 Maryam Fatemi1 Lars Hammarstrand2 Christoffer Petersson1,2 Lennart Svensson2</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šZenseact</li><li>å…³é”®è¯ï¼šNeRFã€è‡ªåŠ¨é©¾é©¶ã€çœŸå®ä¸æ¨¡æ‹Ÿå·®è·ã€æ„ŸçŸ¥æ¨¡å‹é²æ£’æ€§</li><li>è®ºæ–‡é“¾æ¥ï¼šarXiv:2403.16092v1[cs.CV]</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å·²æˆä¸ºæ¨è¿›è‡ªåŠ¨é©¾é©¶ï¼ˆADï¼‰ç ”ç©¶çš„æœ‰å‰é€”çš„å·¥å…·ï¼Œæä¾›å¯æ‰©å±•çš„é—­ç¯ä»¿çœŸå’Œæ•°æ®å¢å¼ºåŠŸèƒ½ã€‚ç„¶è€Œï¼Œä¸ºäº†ä¿¡ä»»ä»¿çœŸä¸­è·å¾—çš„ç»“æœï¼Œéœ€è¦ç¡®ä¿ AD ç³»ç»Ÿä»¥ç›¸åŒçš„æ–¹å¼æ„ŸçŸ¥çœŸå®å’Œæ¸²æŸ“çš„æ•°æ®ã€‚è™½ç„¶æ¸²æŸ“æ–¹æ³•çš„æ€§èƒ½æ­£åœ¨æé«˜ï¼Œä½†è®¸å¤šåœºæ™¯åœ¨æœ¬è´¨ä¸Šä»ç„¶éš¾ä»¥é€¼çœŸåœ°é‡å»ºã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šç°æœ‰çš„æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨æé«˜æ¸²æŸ“ä¿çœŸåº¦ä¸Šï¼Œä½†å½“æ¸²æŸ“è´¨é‡ä¸‹é™æ—¶ï¼Œæ„ŸçŸ¥æ¨¡å‹çš„æ€§èƒ½ä¼šæ˜¾ç€ä¸‹é™ã€‚ï¼ˆ3ï¼‰æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§†è§’æ¥è§£å†³çœŸå®ä¸æ¨¡æ‹Ÿæ•°æ®å·®è·é—®é¢˜ã€‚ä¸å…¶ä»…ä»…å…³æ³¨æé«˜æ¸²æŸ“ä¿çœŸåº¦ï¼Œä¸å¦‚æ¢ç´¢ç®€å•ä½†æœ‰æ•ˆçš„æ–¹æ³•æ¥å¢å¼ºæ„ŸçŸ¥æ¨¡å‹å¯¹ NeRF ä¼ªå½±çš„é²æ£’æ€§ï¼ŒåŒæ—¶ä¸å½±å“çœŸå®æ•°æ®ä¸Šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡ä½¿ç”¨æœ€å…ˆè¿›çš„ç¥ç»æ¸²æŸ“æŠ€æœ¯å¯¹ AD è®¾ç½®ä¸­çš„çœŸå®ä¸æ¨¡æ‹Ÿæ•°æ®å·®è·è¿›è¡Œäº†é¦–æ¬¡å¤§è§„æ¨¡è°ƒæŸ¥ã€‚å…·ä½“æ¥è¯´ï¼Œæœ¬æ–‡åœ¨çœŸå®å’Œæ¨¡æ‹Ÿæ•°æ®ä¸Šè¯„ä¼°äº†ç›®æ ‡æ£€æµ‹å™¨å’Œåœ¨çº¿å»ºå›¾æ¨¡å‹ï¼Œå¹¶ç ”ç©¶äº†ä¸åŒé¢„è®­ç»ƒç­–ç•¥çš„å½±å“ã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†æ€æ ·çš„æ€§èƒ½ï¼šç»“æœè¡¨æ˜ï¼Œæ¨¡å‹å¯¹æ¨¡æ‹Ÿæ•°æ®çš„é²æ£’æ€§æœ‰äº†æ˜¾ç€æé«˜ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ç”šè‡³æé«˜äº†çœŸå®ä¸–ç•Œçš„æ€§èƒ½ã€‚æœ€åï¼Œæœ¬æ–‡æ·±å…¥ç ”ç©¶äº†çœŸå®ä¸æ¨¡æ‹Ÿå·®è·ä¸å›¾åƒé‡å»ºæŒ‡æ ‡ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œç¡®å®š FID å’Œ LPIPS æ˜¯å¼ºæœ‰åŠ›çš„æŒ‡æ ‡ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰å›¾åƒå¢å¼ºï¼šä½¿ç”¨å›¾åƒå¢å¼ºï¼ˆå¦‚æ·»åŠ å™ªå£°ã€æ¨¡ç³Šã€å…‰åº¦å¤±çœŸç­‰ï¼‰æ¥æé«˜æ¨¡å‹å¯¹æ¸²æŸ“æ•°æ®ä¸­ä¼ªå½±çš„é²æ£’æ€§ã€‚ï¼ˆ2ï¼‰ä½¿ç”¨æ¸²æŸ“å›¾åƒå¾®è°ƒï¼šåœ¨å¾®è°ƒæ„ŸçŸ¥æ¨¡å‹æ—¶ï¼ŒåŠ å…¥æ¸²æŸ“å›¾åƒï¼Œä»¥æé«˜æ¨¡å‹å¯¹æ¸²æŸ“æ•°æ®çš„é€‚åº”æ€§ã€‚ï¼ˆ3ï¼‰å›¾åƒåˆ°å›¾åƒè½¬æ¢ï¼šä½¿ç”¨å›¾åƒåˆ°å›¾åƒè½¬æ¢æ¨¡å‹ï¼Œå°†çœŸå®å›¾åƒè½¬æ¢ä¸ºç±»ä¼¼æ¸²æŸ“å›¾åƒçš„ä¼ªå½±ï¼Œä»è€Œå¢åŠ ç”¨äºå¾®è°ƒçš„æ¸²æŸ“å›¾åƒæ•°é‡ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§†è§’æ¥è§£å†³è‡ªåŠ¨é©¾é©¶ä¸­çœŸå®ä¸æ¨¡æ‹Ÿæ•°æ®å·®è·é—®é¢˜ï¼Œæ¢ç´¢äº†å¢å¼ºæ„ŸçŸ¥æ¨¡å‹å¯¹ NeRF ä¼ªå½±çš„é²æ£’æ€§çš„æ–¹æ³•ï¼Œå–å¾—äº†æ˜¾è‘—æ•ˆæœã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§æ–°çš„è§†è§’æ¥è§£å†³çœŸå®ä¸æ¨¡æ‹Ÿæ•°æ®å·®è·é—®é¢˜ï¼Œæ¢ç´¢äº†å¢å¼ºæ„ŸçŸ¥æ¨¡å‹å¯¹ NeRF ä¼ªå½±çš„é²æ£’æ€§çš„æ–¹æ³•ã€‚æ€§èƒ½ï¼šåœ¨çœŸå®å’Œæ¨¡æ‹Ÿæ•°æ®ä¸Šè¯„ä¼°äº†ç›®æ ‡æ£€æµ‹å™¨å’Œåœ¨çº¿å»ºå›¾æ¨¡å‹ï¼Œç»“æœè¡¨æ˜æ¨¡å‹å¯¹æ¨¡æ‹Ÿæ•°æ®çš„é²æ£’æ€§æœ‰äº†æ˜¾ç€æé«˜ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ç”šè‡³æé«˜äº†çœŸå®ä¸–ç•Œçš„æ€§èƒ½ã€‚å·¥ä½œé‡ï¼šè¿›è¡Œäº†å¤§è§„æ¨¡è°ƒæŸ¥ï¼Œè¯„ä¼°äº†æ„ŸçŸ¥æ¨¡å‹åœ¨çœŸå®å’Œæ¨¡æ‹Ÿæ•°æ®ä¸Šçš„æ€§èƒ½ï¼Œç ”ç©¶äº†ä¸åŒé¢„è®­ç»ƒç­–ç•¥çš„å½±å“ï¼Œæ·±å…¥ç ”ç©¶äº†çœŸå®ä¸æ¨¡æ‹Ÿå·®è·ä¸å›¾åƒé‡å»ºæŒ‡æ ‡ä¹‹é—´çš„ç›¸å…³æ€§ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-68245c1e9e03a301ef7308b852cec45b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-637dca64e1ede555b3f77fe3d6e45f26.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f3c065e635b99332c436cd774aa002fb.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d3ea9ed03a5a035d0bd40ebe5d3c1dfa.jpg" align="middle"></details><h2 id="PKU-DyMVHumans-A-Multi-View-Video-Benchmark-for-High-Fidelity-Dynamic-Human-Modeling"><a href="#PKU-DyMVHumans-A-Multi-View-Video-Benchmark-for-High-Fidelity-Dynamic-Human-Modeling" class="headerlink" title="PKU-DyMVHumans: A Multi-View Video Benchmark for High-Fidelity Dynamic   Human Modeling"></a>PKU-DyMVHumans: A Multi-View Video Benchmark for High-Fidelity Dynamic   Human Modeling</h2><p><strong>Authors:Xiaoyun Zheng, Liwei Liao, Xufeng Li, Jianbo Jiao, Rongjie Wang, Feng Gao, Shiqi Wang, Ronggang Wang</strong></p><p>High-quality human reconstruction and photo-realistic rendering of a dynamic scene is a long-standing problem in computer vision and graphics. Despite considerable efforts invested in developing various capture systems and reconstruction algorithms, recent advancements still struggle with loose or oversized clothing and overly complex poses. In part, this is due to the challenges of acquiring high-quality human datasets. To facilitate the development of these fields, in this paper, we present PKU-DyMVHumans, a versatile human-centric dataset for high-fidelity reconstruction and rendering of dynamic human scenarios from dense multi-view videos. It comprises 8.2 million frames captured by more than 56 synchronized cameras across diverse scenarios. These sequences comprise 32 human subjects across 45 different scenarios, each with a high-detailed appearance and realistic human motion. Inspired by recent advancements in neural radiance field (NeRF)-based scene representations, we carefully set up an off-the-shelf framework that is easy to provide those state-of-the-art NeRF-based implementations and benchmark on PKU-DyMVHumans dataset. It is paving the way for various applications like fine-grained foreground/background decomposition, high-quality human reconstruction and photo-realistic novel view synthesis of a dynamic scene. Extensive studies are performed on the benchmark, demonstrating new observations and challenges that emerge from using such high-fidelity dynamic data. The dataset is available at: <a href="https://pku-dymvhumans.github.io">https://pku-dymvhumans.github.io</a>. </p><p><a href="http://arxiv.org/abs/2403.16080v2">PDF</a> </p><p><strong>Summary</strong><br>åŒ—å¤§åŠ¨æ€å¤šè§†è§’äººä½“æ•°æ®é›†ï¼Œæä¾›é«˜è´¨é‡åŠ¨æ€äººä½“åœºæ™¯é‡å»ºå’Œæ¸²æŸ“ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æä¾› 820 ä¸‡å¸§ï¼Œç”± 56 ä¸ªåŒæ­¥æ‘„åƒæœºåœ¨ä¸åŒåœºæ™¯ä¸­æ‹æ‘„ã€‚</li><li>åŒ…å« 32 ä½äººä½“ï¼Œ45 ç§ä¸åŒåœºæ™¯ï¼Œå…·æœ‰ä¸°å¯Œçš„å¤–è§‚å’Œé€¼çœŸåŠ¨ä½œã€‚</li><li>åŸºäº NeRF åœºæ™¯è¡¨ç¤ºï¼Œæä¾›ç°æˆæ¡†æ¶ï¼Œä¾¿äºåœ¨ PKU-DyMVHumans æ•°æ®é›†ä¸Šæä¾›æœ€å…ˆè¿›çš„ NeRF å®ç°å’ŒåŸºå‡†ã€‚</li><li>é€‚ç”¨äºç»†ç²’åº¦å‰æ™¯/èƒŒæ™¯åˆ†è§£ã€é«˜è´¨é‡äººä½“é‡å»ºå’ŒåŠ¨æ€åœºæ™¯ç…§ç‰‡çº§æ–°è§†è§’åˆæˆç­‰åº”ç”¨ã€‚</li><li>å¹¿æ³›çš„ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨æ­¤ç±»é«˜ä¿çœŸåŠ¨æ€æ•°æ®äº§ç”Ÿäº†æ–°çš„è§‚å¯Ÿå’ŒæŒ‘æˆ˜ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šPKU-DyMVHumansï¼šç”¨äºé«˜ä¿çœŸåŠ¨æ€äººä½“å»ºæ¨¡çš„å¤šè§†è§’è§†é¢‘åŸºå‡†</li><li>ä½œè€…ï¼š</li><li>è¢å¿—æ°</li><li>å­™å‰‘</li><li>æ—å‡¡</li><li>è¢å˜‰å ƒ</li><li>å´æ–°</li><li>æ›¹æ—­ä¸œ</li><li>ä½œè€…å•ä½ï¼šåŒ—äº¬å¤§å­¦ä¿¡æ¯ç§‘å­¦æŠ€æœ¯å­¦é™¢</li><li>å…³é”®è¯ï¼š</li><li>äººä½“å»ºæ¨¡</li><li>åŠ¨æ€åœºæ™¯</li><li>å¤šè§†è§’è§†é¢‘</li><li>ç¥ç»è¾å°„åœº</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2207.12006   Github ä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼š   (1) ç ”ç©¶èƒŒæ™¯ï¼š   é«˜ä¿çœŸçš„äººä½“é‡å»ºå’ŒåŠ¨æ€åœºæ™¯çš„é€¼çœŸæ¸²æŸ“æ˜¯è®¡ç®—æœºè§†è§‰å’Œå›¾å½¢å­¦ä¸­çš„é•¿æœŸé—®é¢˜ã€‚å°½ç®¡åœ¨å¼€å‘å„ç§æ•è·ç³»ç»Ÿå’Œé‡å»ºç®—æ³•æ–¹é¢æŠ•å…¥äº†å¤§é‡ç²¾åŠ›ï¼Œä½†æœ€è¿‘çš„è¿›å±•ä»ç„¶éš¾ä»¥å¤„ç†å®½æ¾æˆ–è¶…å¤§å°ºå¯¸çš„æœè£…ä»¥åŠè¿‡äºå¤æ‚çš„å§¿åŠ¿ã€‚è¿™åœ¨ä¸€å®šç¨‹åº¦ä¸Šæ˜¯ç”±äºè·å–é«˜è´¨é‡äººä½“æ•°æ®é›†çš„æŒ‘æˆ˜ã€‚   (2) è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼š   è¿‡å»çš„æ–¹æ³•é€šå¸¸ä¾èµ–äºç¨€ç–çš„ 3D ç‚¹äº‘æˆ–ç²—ç³™çš„ç‰©ä½“æ©ç ï¼Œè¿™é™åˆ¶äº†é‡å»ºçš„ä¿çœŸåº¦ã€‚åŸºäºç¥ç»è¾å°„åœº (NeRF) çš„åœºæ™¯è¡¨ç¤ºæœ€è¿‘å–å¾—äº†æ˜¾ç€è¿›å±•ï¼Œä½†ç¼ºä¹ä¸€ä¸ªé«˜è´¨é‡çš„äººä½“æ•°æ®é›†æ¥è¯„ä¼°å’Œæ¨åŠ¨å…¶åœ¨åŠ¨æ€åœºæ™¯ä¸­çš„äººä½“å»ºæ¨¡å’Œæ¸²æŸ“æ–¹é¢çš„æ½œåŠ›ã€‚   (3) æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼š   ä¸ºäº†ä¿ƒè¿›è¿™äº›é¢†åŸŸçš„å‘å±•ï¼Œæœ¬æ–‡æå‡ºäº† PKU-DyMVHumansï¼Œè¿™æ˜¯ä¸€ä¸ªé€šç”¨çš„ä»¥äººä¸ºä¸­å¿ƒçš„åŠ¨æ€äººä½“åœºæ™¯é«˜ä¿çœŸé‡å»ºå’Œæ¸²æŸ“æ•°æ®é›†ã€‚å®ƒåŒ…å«æ¥è‡ª 56 ä¸ªä»¥ä¸ŠåŒæ­¥æ‘„åƒæœºçš„ 820 ä¸‡å¸§ï¼Œæ¶µç›–å„ç§åœºæ™¯ã€‚è¿™äº›åºåˆ—åŒ…æ‹¬ 32 ä¸ªäººç±»å—è¯•è€…ï¼Œåˆ†å¸ƒåœ¨ 45 ä¸ªä¸åŒçš„åœºæ™¯ä¸­ï¼Œæ¯ä¸ªåœºæ™¯éƒ½å…·æœ‰é«˜åº¦è¯¦ç»†çš„å¤–è§‚å’Œé€¼çœŸçš„äººä½“åŠ¨ä½œã€‚å—åŸºäº NeRF çš„åœºæ™¯è¡¨ç¤ºçš„æœ€æ–°è¿›å±•çš„å¯å‘ï¼Œæœ¬æ–‡è¿˜è®¾ç½®äº†ä¸€ä¸ªç°æˆçš„æ¡†æ¶ï¼Œä¾¿äºåœ¨ PKU-DyMVHumans æ•°æ®é›†ä¸Šæä¾›æœ€å…ˆè¿›çš„åŸºäº NeRF çš„å®ç°å’ŒåŸºå‡†ã€‚è¿™ä¸ºå„ç§åº”ç”¨é“ºå¹³äº†é“è·¯ï¼Œå¦‚ç»†ç²’åº¦å‰æ™¯/èƒŒæ™¯åˆ†è§£ã€é«˜è´¨é‡äººä½“é‡å»ºå’ŒåŠ¨æ€åœºæ™¯çš„é€¼çœŸæ–°è§†è§’åˆæˆã€‚   (4) æ–¹æ³•åœ¨ä½•ç§ä»»åŠ¡ä¸Šå–å¾—äº†ä½•ç§æ€§èƒ½ï¼Œæ˜¯å¦èƒ½æ”¯æŒå…¶ç›®æ ‡ï¼š   æœ¬æ–‡åœ¨åŸºå‡†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„ç ”ç©¶ï¼Œå±•ç¤ºäº†ä½¿ç”¨å¦‚æ­¤é«˜ä¿çœŸåŠ¨æ€æ•°æ®æ‰€äº§ç”Ÿçš„æ–°è§‚å¯Ÿå’ŒæŒ‘æˆ˜ã€‚è¯¥æ•°æ®é›†å¯ç”¨äºï¼š</li><li>ç»†ç²’åº¦å‰æ™¯/èƒŒæ™¯åˆ†è§£</li><li>é«˜è´¨é‡äººä½“é‡å»º</li><li>åŠ¨æ€åœºæ™¯çš„é€¼çœŸæ–°è§†è§’åˆæˆ</li></ol><p>Some Error for method(æ¯”å¦‚æ˜¯ä¸æ˜¯æ²¡æœ‰Methodsè¿™ä¸ªç« èŠ‚)</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡º PKU-DyMVHumansï¼Œè¿™æ˜¯ä¸€ä¸ªåŠ¨æ€äººä½“æ•°æ®é›†ï¼Œæ—¨åœ¨ä»å¯†é›†çš„å¤šè§†è§’è§†é¢‘ä¸­è¿›è¡Œé«˜ä¿çœŸçš„äººä½“é‡å»ºå’Œæ¸²æŸ“ã€‚å®ƒå…·æœ‰é«˜ä¿çœŸçš„äººä½“è¡¨ç°ï¼ŒåŒ…æ‹¬é«˜åº¦è¯¦ç»†çš„å¤–è§‚ã€å¤æ‚çš„äººä½“è¿åŠ¨ï¼Œä»¥åŠå…·æœ‰æŒ‘æˆ˜æ€§çš„äººä½“-ç‰©ä½“äº¤äº’ã€å¤šäººäº¤äº’å’Œå¤æ‚çš„åœºæ™¯æ•ˆæœï¼ˆä¾‹å¦‚ï¼Œç¯å…‰ã€é˜´å½±å’Œå¸çƒŸï¼‰ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†åŸºå‡†ä»»åŠ¡ï¼Œå¹¶å¯¹å‡ ç§å…ˆè¿›çš„æ–¹æ³•è¿›è¡Œäº†è¯¦ç»†çš„å®éªŒã€‚PKU-DyMVHumans è¿›ä¸€æ­¥å¡«è¡¥äº†ç°æœ‰æ•°æ®é›†å’ŒçœŸå®åœºæ™¯åº”ç”¨ä¹‹é—´çš„å·®è·ã€‚æŒ‘æˆ˜å’Œæœªæ¥å·¥ä½œã€‚è™½ç„¶æˆ‘ä»¬åœ¨å¤§é‡ä»¥äººä¸ºä¸­å¿ƒé‡å»ºå’Œæ¸²æŸ“ä¸ŠéªŒè¯äº†æˆ‘ä»¬æ•°æ®é›†çš„å¤æ‚æ€§å’Œä¿çœŸåº¦ã€‚é‡è¦çš„æ˜¯è¦å¼ºè°ƒæ›´å…·æŒ‘æˆ˜æ€§å’Œç°å®æ€§çš„å¤šäººç‰©/ä¸»ä½“å»ºæ¨¡ï¼Œå®ƒå¯ä»¥åæ˜ å¤šäººç‰©äº¤äº’æ€§ã€å¤æ‚åœºæ™¯æ•ˆæœå’Œå¤šè§†è§’ä¸€è‡´æ€§æ€§èƒ½æ–¹é¢çš„æ¸²æŸ“å·®å¼‚ã€‚æ­¤å¤–ï¼Œä»å•çœ¼è‡ªæ—‹è½¬è§†é¢‘ä¸­å¯¹è¿åŠ¨ä¸»ä½“è¿›è¡Œè‡ªç”±è§†ç‚¹æ¸²æŸ“æ˜¯ä¸€ä¸ªå¤æ‚ä½†ç†æƒ³çš„è®¾ç½®ã€‚æˆ‘ä»¬çš„è¡¥å……ææ–™æä¾›äº†è¿åŠ¨ä¸»ä½“çš„è‡ªç”±è§†ç‚¹æ¸²æŸ“çš„é™„åŠ å®éªŒï¼Œç»“æœå—å±€éƒ¨é®æŒ¡å’Œè§†ç‚¹ç¼ºå¤±çš„å½±å“ï¼Œå¯¼è‡´è§†ç‚¹æ¸²æŸ“å‡ºç°ä¼ªå½±ã€‚æœ‰äº†è¿™äº›æœºé‡å’ŒæŒ‘æˆ˜ï¼Œæˆ‘ä»¬ç›¸ä¿¡ PKU-DyMVHumans å°†æœ‰åˆ©äºç¤¾åŒºä¸­æ–°æ–¹æ³•çš„å‘å±•ã€‚è‡´è°¢ã€‚è¿™é¡¹å·¥ä½œå¾—åˆ°äº†æ·±åœ³å¸‚ä¼˜ç§€äººæ‰åŸ¹è®­åŸºé‡‘ã€æ·±åœ³å¸‚ç§‘æŠ€è®¡åˆ’ï¼ˆRCJC20200714114435057ã€SGDX20211123144400001ï¼‰ã€å›½å®¶è‡ªç„¶ç§‘å­¦åŸºé‡‘ï¼ˆU21B2012ï¼‰å’Œå’ªå’•-åŒ—å¤§å…ƒå®‡å®™æŠ€æœ¯åˆ›æ–°å®éªŒå®¤ï¼ˆR24115SGï¼‰çš„æ”¯æŒã€‚Jianbo Jiao å¾—åˆ°çš‡å®¶å­¦ä¼šèµ æ¬¾ IES\R3\223050 å’Œ SIF\R1\231009.88 çš„æ”¯æŒã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡º PKU-DyMVHumansï¼Œä¸€ä¸ªç”¨äºé«˜ä¿çœŸåŠ¨æ€äººä½“å»ºæ¨¡çš„å¤šè§†è§’è§†é¢‘åŸºå‡†ï¼›æ€§èƒ½ï¼šåœ¨åŸºå‡†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„ç ”ç©¶ï¼Œå±•ç¤ºäº†ä½¿ç”¨å¦‚æ­¤é«˜ä¿çœŸåŠ¨æ€æ•°æ®æ‰€äº§ç”Ÿçš„æ–°è§‚å¯Ÿå’ŒæŒ‘æˆ˜ï¼›å·¥ä½œé‡ï¼šæ”¶é›†äº† 820 ä¸‡å¸§ï¼Œæ¶µç›–å„ç§åœºæ™¯ï¼ŒåŒ…æ‹¬ 32 ä¸ªäººç±»å—è¯•è€…å’Œ 45 ä¸ªä¸åŒçš„åœºæ™¯ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-165a03c4fc78e3abe018f2febbbb4f63.jpg" align="middle"><img src="https://picx.zhimg.com/v2-de6f56832029ed2af99d8dd35bf8f378.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e06c71a44f02a4c723d19749bb2cf5cf.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e11e8d21c61a5e04cc190fe2beb0ce63.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-fee4215f3b978a6d8afa20c3d7631f94.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-adab8ff1d80ba91401beea1dfee88f35.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-03-28  Modeling uncertainty for Gaussian Splatting</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/03/28/Paper/2024-03-28/3DGS/"/>
    <id>https://kedreamix.github.io/2024/03/28/Paper/2024-03-28/3DGS/</id>
    <published>2024-03-28T03:28:24.000Z</published>
    <updated>2024-03-28T03:28:24.574Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-03-28-æ›´æ–°"><a href="#2024-03-28-æ›´æ–°" class="headerlink" title="2024-03-28 æ›´æ–°"></a>2024-03-28 æ›´æ–°</h1><h2 id="Modeling-uncertainty-for-Gaussian-Splatting"><a href="#Modeling-uncertainty-for-Gaussian-Splatting" class="headerlink" title="Modeling uncertainty for Gaussian Splatting"></a>Modeling uncertainty for Gaussian Splatting</h2><p><strong>Authors:Luca Savant, Diego Valsesia, Enrico Magli</strong></p><p>We present Stochastic Gaussian Splatting (SGS): the first framework for uncertainty estimation using Gaussian Splatting (GS). GS recently advanced the novel-view synthesis field by achieving impressive reconstruction quality at a fraction of the computational cost of Neural Radiance Fields (NeRF). However, contrary to the latter, it still lacks the ability to provide information about the confidence associated with their outputs. To address this limitation, in this paper, we introduce a Variational Inference-based approach that seamlessly integrates uncertainty prediction into the common rendering pipeline of GS. Additionally, we introduce the Area Under Sparsification Error (AUSE) as a new term in the loss function, enabling optimization of uncertainty estimation alongside image reconstruction. Experimental results on the LLFF dataset demonstrate that our method outperforms existing approaches in terms of both image rendering quality and uncertainty estimation accuracy. Overall, our framework equips practitioners with valuable insights into the reliability of synthesized views, facilitating safer decision-making in real-world applications. </p><p><a href="http://arxiv.org/abs/2403.18476v1">PDF</a> </p><p><strong>Summary</strong><br>é«˜æ–¯æ•£å°„æ¡†æ¶æ·»åŠ äº†ä¸ç¡®å®šæ€§è¯„ä¼°ï¼Œä¸ºå›¾åƒé‡å»ºå¸¦æ¥äº†æ›´å¯é çš„å†³ç­–ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºä½¿ç”¨é«˜æ–¯æ•£å°„çš„ä¸ç¡®å®šæ€§ä¼°è®¡æ¡†æ¶ï¼Œå³éšæœºé«˜æ–¯æ•£å°„ (SGS)ã€‚</li><li>é‡‡ç”¨å˜åˆ†æ¨ç†æ–¹æ³•å°†ä¸ç¡®å®šæ€§é¢„æµ‹æ— ç¼é›†æˆåˆ°é«˜æ–¯æ•£å°„çš„æ¸²æŸ“ç®¡çº¿ä¸­ã€‚</li><li>å¼•å…¥ç¨€ç–åŒ–è¯¯å·®ä¸‹è¡¨é¢ç§¯ (AUSE) ä½œä¸ºæ–°çš„æŸå¤±å‡½æ•°é¡¹ã€‚</li><li>é€šè¿‡ä¼˜åŒ–ä¸ç¡®å®šæ€§ä¼°è®¡å’Œå›¾åƒé‡å»ºæ¥æé«˜æ€»ä½“æ€§èƒ½ã€‚</li><li>åœ¨ LLFF æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ SGS åœ¨å›¾åƒæ¸²æŸ“è´¨é‡å’Œä¸ç¡®å®šæ€§ä¼°è®¡å‡†ç¡®æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li><li>è¯¥æ¡†æ¶ä¸ºä»ä¸šè€…æä¾›äº†å¯¹åˆæˆè§†å›¾å¯é æ€§çš„å®è´µè§è§£ï¼Œä»è€Œåœ¨å®é™…åº”ç”¨ä¸­ä¿ƒè¿›æ›´å®‰å…¨çš„å†³ç­–ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šé«˜æ–¯æº…å°„çš„ä¸ç¡®å®šæ€§å»ºæ¨¡</li><li>ä½œè€…ï¼šLuca Savantã€Diego Valsesiaã€Enrico Magli</li><li>æ‰€å±æœºæ„ï¼šæ„å¤§åˆ©éƒ½çµç†å·¥å¤§å­¦ç”µå­ä¸ç”µä¿¡ç³»</li><li>å…³é”®è¯ï¼šé«˜æ–¯æº…å°„ã€ä¸ç¡®å®šæ€§ä¼°è®¡ã€ç¥ç»è¾å°„åœºã€è®¡ç®—æœºè§†è§‰</li><li>è®ºæ–‡é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åœ¨æ–°å‹è§†å›¾åˆæˆé¢†åŸŸå–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä½†å…¶è®¡ç®—å¤æ‚åº¦å’Œå†…å­˜éœ€æ±‚é™åˆ¶äº†å…¶åœ¨å®æ—¶åº”ç”¨ä¸­çš„å®ç”¨æ€§ã€‚é«˜æ–¯æº…å°„ï¼ˆGSï¼‰æŠ€æœ¯ä½œä¸ºä¸€ç§æ›´å…·è®¡ç®—æ•ˆç‡çš„æ›¿ä»£æ–¹æ¡ˆï¼Œåœ¨ä¿æŒé«˜è´¨é‡æ–°å‹è§†å›¾åˆæˆçš„æƒ…å†µä¸‹ï¼Œæé«˜äº†æ¸²æŸ“é€Ÿåº¦ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•å’Œé—®é¢˜ï¼šNeRF åœ¨æ–°å‹è§†å›¾åˆæˆä¸­å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœï¼Œä½†ç¼ºä¹æä¾›ä¸è¾“å‡ºç›¸å…³ç½®ä¿¡åº¦ä¿¡æ¯çš„èƒ½åŠ›ã€‚GS è™½ç„¶åœ¨æ¸²æŸ“é€Ÿåº¦ä¸Šå–å¾—äº†ä¼˜åŠ¿ï¼Œä½†åŒæ ·ç¼ºä¹ä¸ç¡®å®šæ€§ä¼°è®¡æœºåˆ¶ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç”¨äº GS ä¸­ä¸ç¡®å®šæ€§ä¼°è®¡çš„æ–°æ¡†æ¶ï¼Œç§°ä¸ºéšæœºé«˜æ–¯æº…å°„ï¼ˆSGSï¼‰ã€‚SGS æ‰©å±•äº†ä¼ ç»Ÿçš„ç¡®å®šæ€§ GS æ¡†æ¶ï¼Œå¼•å…¥äº†éšæœºæ€§ï¼Œå…è®¸åœ¨åˆæˆè§†å›¾çš„åŒæ—¶é¢„æµ‹ä¸ç¡®å®šæ€§ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å˜åˆ†æ¨ç†ï¼ˆVIï¼‰åœ¨è´å¶æ–¯æ¡†æ¶ä¸­å­¦ä¹  GS è¾å°„åœºçš„å‚æ•°ï¼Œä»è€Œèƒ½å¤Ÿå‡†ç¡®ä¼°è®¡ä¸ç¡®å®šæ€§ï¼ŒåŒæ—¶ä¸ç‰ºç‰²è®¡ç®—æ•ˆç‡ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜é€šè¿‡åœ¨æŸå¤±å‡½æ•°ä¸­å¼•å…¥ç¨€ç–åŒ–æ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUSEï¼‰ä½œä¸ºæ–°é¡¹ï¼Œåˆ›æ–°äº†å­¦ä¹ è¿‡ç¨‹ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šå®éªŒç»“æœè¡¨æ˜ï¼ŒSGS åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ LLFF æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œåœ¨æ¸²æŸ“è´¨é‡å’Œä¸ç¡®å®šæ€§ä¼°è®¡æŒ‡æ ‡æ–¹é¢éƒ½ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚è¿™äº›æ€§èƒ½æå‡æ”¯æŒäº†æœ¬æ–‡æå‡ºçš„æ–¹æ³•ç›®æ ‡ï¼Œå³ä¸ºä»ä¸šè€…æä¾›å¯¹åˆæˆè§†å›¾å¯é æ€§çš„å®è´µè§è§£ï¼Œä»è€Œä¿ƒè¿›åœ¨å®é™…åº”ç”¨ä¸­æ›´å®‰å…¨çš„å†³ç­–åˆ¶å®šã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰æ‰©å±•ä¼ ç»Ÿç¡®å®šæ€§é«˜æ–¯æº…å°„æ¡†æ¶ï¼Œå¼•å…¥éšæœºæ€§ï¼Œåœ¨åˆæˆè§†å›¾çš„åŒæ—¶é¢„æµ‹ä¸ç¡®å®šæ€§ã€‚ï¼ˆ2ï¼‰åˆ©ç”¨å˜åˆ†æ¨ç†ï¼ˆVIï¼‰åœ¨è´å¶æ–¯æ¡†æ¶ä¸­å­¦ä¹ é«˜æ–¯æº…å°„è¾å°„åœºçš„å‚æ•°ï¼Œå‡†ç¡®ä¼°è®¡ä¸ç¡®å®šæ€§ã€‚ï¼ˆ3ï¼‰åœ¨æŸå¤±å‡½æ•°ä¸­å¼•å…¥ç¨€ç–åŒ–æ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUSEï¼‰ä½œä¸ºæ–°é¡¹ï¼Œåˆ›æ–°å­¦ä¹ è¿‡ç¨‹ã€‚</p></li></ol><p>8.ç»“è®ºï¼š(1): æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºé«˜æ–¯æº…å°„ä¸ç¡®å®šæ€§ä¼°è®¡çš„æ–°æ¡†æ¶ï¼Œç§°ä¸ºéšæœºé«˜æ–¯æº…å°„ï¼ˆSGSï¼‰ã€‚SGSæ‰©å±•äº†ä¼ ç»Ÿçš„ç¡®å®šæ€§é«˜æ–¯æº…å°„æ¡†æ¶ï¼Œå¼•å…¥äº†éšæœºæ€§ï¼Œå…è®¸åœ¨åˆæˆè§†å›¾çš„åŒæ—¶é¢„æµ‹ä¸ç¡®å®šæ€§ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å˜åˆ†æ¨ç†ï¼ˆVIï¼‰åœ¨è´å¶æ–¯æ¡†æ¶ä¸­å­¦ä¹ é«˜æ–¯æº…å°„è¾å°„åœºçš„å‚æ•°ï¼Œä»è€Œèƒ½å¤Ÿå‡†ç¡®ä¼°è®¡ä¸ç¡®å®šæ€§ï¼ŒåŒæ—¶ä¸ç‰ºç‰²è®¡ç®—æ•ˆç‡ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜é€šè¿‡åœ¨æŸå¤±å‡½æ•°ä¸­å¼•å…¥ç¨€ç–åŒ–æ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUSEï¼‰ä½œä¸ºæ–°é¡¹ï¼Œåˆ›æ–°äº†å­¦ä¹ è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSGSåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„LLFFæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œåœ¨æ¸²æŸ“è´¨é‡å’Œä¸ç¡®å®šæ€§ä¼°è®¡æŒ‡æ ‡æ–¹é¢éƒ½ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚è¿™äº›æ€§èƒ½æå‡æ”¯æŒäº†æœ¬æ–‡æå‡ºçš„æ–¹æ³•ç›®æ ‡ï¼Œå³ä¸ºä»ä¸šè€…æä¾›å¯¹åˆæˆè§†å›¾å¯é æ€§çš„å®è´µè§è§£ï¼Œä»è€Œä¿ƒè¿›åœ¨å®é™…åº”ç”¨ä¸­æ›´å®‰å…¨çš„å†³ç­–åˆ¶å®šã€‚(2): åˆ›æ–°ç‚¹ï¼š- æå‡ºäº†ä¸€ç§æ–°çš„ä¸ç¡®å®šæ€§ä¼°è®¡æ¡†æ¶ï¼Œç§°ä¸ºéšæœºé«˜æ–¯æº…å°„ï¼ˆSGSï¼‰ã€‚- åˆ©ç”¨å˜åˆ†æ¨ç†ï¼ˆVIï¼‰åœ¨è´å¶æ–¯æ¡†æ¶ä¸­å­¦ä¹ é«˜æ–¯æº…å°„è¾å°„åœºçš„å‚æ•°ã€‚- åœ¨æŸå¤±å‡½æ•°ä¸­å¼•å…¥ç¨€ç–åŒ–æ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUSEï¼‰ä½œä¸ºæ–°é¡¹ï¼Œåˆ›æ–°å­¦ä¹ è¿‡ç¨‹ã€‚æ€§èƒ½ï¼š- åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„LLFFæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚- åœ¨æ¸²æŸ“è´¨é‡å’Œä¸ç¡®å®šæ€§ä¼°è®¡æŒ‡æ ‡æ–¹é¢éƒ½ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚å·¥ä½œé‡ï¼š- å¼•å…¥äº†éšæœºæ€§ï¼Œå¢åŠ äº†è®¡ç®—å¤æ‚åº¦ã€‚- åˆ©ç”¨å˜åˆ†æ¨ç†ï¼ˆVIï¼‰å­¦ä¹ å‚æ•°ï¼Œå¢åŠ äº†è®­ç»ƒæ—¶é—´ã€‚- åœ¨æŸå¤±å‡½æ•°ä¸­å¼•å…¥ç¨€ç–åŒ–æ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUSEï¼‰ä½œä¸ºæ–°é¡¹ï¼Œå¢åŠ äº†è®­ç»ƒéš¾åº¦ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-0db2c257f2d21d3d2093093f35a22d6a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ce90b05cf42d03c136564ebed15589ee.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-45b5f0fb363396246f2e707617b89c8e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-349f29a5e78de8cef3e63120b9df962c.jpg" align="middle"></details>## EgoLifter: Open-world 3D Segmentation for Egocentric Perception**Authors:Qiao Gu, Zhaoyang Lv, Duncan Frost, Simon Green, Julian Straub, Chris Sweeney**In this paper we present EgoLifter, a novel system that can automatically segment scenes captured from egocentric sensors into a complete decomposition of individual 3D objects. The system is specifically designed for egocentric data where scenes contain hundreds of objects captured from natural (non-scanning) motion. EgoLifter adopts 3D Gaussians as the underlying representation of 3D scenes and objects and uses segmentation masks from the Segment Anything Model (SAM) as weak supervision to learn flexible and promptable definitions of object instances free of any specific object taxonomy. To handle the challenge of dynamic objects in ego-centric videos, we design a transient prediction module that learns to filter out dynamic objects in the 3D reconstruction. The result is a fully automatic pipeline that is able to reconstruct 3D object instances as collections of 3D Gaussians that collectively compose the entire scene. We created a new benchmark on the Aria Digital Twin dataset that quantitatively demonstrates its state-of-the-art performance in open-world 3D segmentation from natural egocentric input. We run EgoLifter on various egocentric activity datasets which shows the promise of the method for 3D egocentric perception at scale. [PDF](http://arxiv.org/abs/2403.18118v1) Preprint. Project page: https://egolifter.github.io/**Summary**è‡ªæˆ‘æå‡å™¨ï¼šä»ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„ä¼ æ„Ÿå™¨æ•è·çš„åœºæ™¯ä¸­è‡ªåŠ¨åˆ†å‰² 3D ç‰©ä½“**Key Takeaways**- EgoLifter å¯ä»¥ä» 3D åœºæ™¯ä¸­è‡ªåŠ¨åˆ†å‰²å‡ºä¸ªåˆ« 3D ç‰©ä½“ã€‚- EgoLifter ä½¿ç”¨ 3D é«˜æ–¯æ¨¡å‹ä½œä¸º 3D åœºæ™¯å’Œç‰©ä½“çš„åº•å±‚è¡¨ç¤ºã€‚- EgoLifter åˆ©ç”¨ SAM åˆ†å‰²æ©ç ä½œä¸ºå¼±ç›‘ç£å­¦ä¹ å¯¹è±¡å®ä¾‹å®šä¹‰ã€‚- EgoLifter è®¾è®¡äº†ä¸€ä¸ªç¬æ€é¢„æµ‹æ¨¡å—æ¥è¿‡æ»¤åŠ¨æ€ç‰©ä½“ã€‚- EgoLifter åœ¨ Aria æ•°å­—å­ªç”Ÿæ•°æ®é›†ä¸Šåˆ›å»ºäº†ä¸€ä¸ªæ–°åŸºå‡†ã€‚- EgoLifter åœ¨å„ç§ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„æ´»åŠ¨æ•°æ®é›†ä¸Šè¿è¡Œã€‚- EgoLifter 3D æ„ŸçŸ¥ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒæä¾›äº†å‰æ™¯ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<p>1.æ ‡é¢˜ï¼šEgoLifter2.ä½œè€…ï¼šQiao Gu, Zhaoyang Lv, Duncan Frost, Simon Green, Julian Straub, Chris Sweeney3.ç¬¬ä¸€ä½œè€…æ‰€å±æœºæ„ï¼šå¤šä¼¦å¤šå¤§å­¦4.å…³é”®è¯ï¼šEgocentric Perceptionã€Open-world Segmentationã€3D Reconstruction5.è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.18118Github é“¾æ¥ï¼šNone6.æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šéšç€å¯ç©¿æˆ´è®¾å¤‡çš„æ™®åŠï¼Œä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„æœºå™¨æ„ŸçŸ¥ç®—æ³•å˜å¾—è¶Šæ¥è¶Šé‡è¦ï¼Œè¿™ç±»ç®—æ³•èƒ½å¤Ÿç†è§£ç”¨æˆ·å‘¨å›´çš„ç‰©ç† 3D ä¸–ç•Œã€‚è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†é¢‘ç›´æ¥åæ˜ äº†äººç±»è§‚å¯Ÿä¸–ç•Œçš„æ–¹å¼ï¼ŒåŒ…å«äº†å…³äºç‰©ç†ç¯å¢ƒä»¥åŠäººç±»ç”¨æˆ·å¦‚ä½•ä¸ä¹‹äº¤äº’çš„é‡è¦ä¿¡æ¯ã€‚ç„¶è€Œï¼Œè‡ªæˆ‘ä¸ºä¸­å¿ƒè¿åŠ¨çš„ç‰¹å®šç‰¹å¾ç»™ 3D è®¡ç®—æœºè§†è§‰å’Œæœºå™¨æ„ŸçŸ¥ç®—æ³•å¸¦æ¥äº†æŒ‘æˆ˜ã€‚ä¸é€šè¿‡â€œæ‰«æâ€è¿åŠ¨æ•æ‰çš„æ•°æ®é›†ä¸åŒï¼Œè‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†é¢‘æ— æ³•ä¿è¯åœºæ™¯çš„å®Œæ•´è¦†ç›–ã€‚ç”±äºå¤šè§†è§’è§‚å¯Ÿæœ‰é™æˆ–ç¼ºå¤±ï¼Œè¿™ä½¿å¾—é‡å»ºè¿‡ç¨‹æå…·æŒ‘æˆ˜æ€§ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šä»¥å¾€çš„æ–¹æ³•é€šå¸¸ä¾èµ–äºç‰¹å®šçš„å¯¹è±¡åˆ†ç±»æ³•ï¼Œå¹¶ä¸”éš¾ä»¥å¤„ç†è‡ªæˆ‘ä¸ºä¸­å¿ƒè§†é¢‘ä¸­åŠ¨æ€å¯¹è±¡å¸¦æ¥çš„æŒ‘æˆ˜ã€‚ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡º EgoLifterï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„ç³»ç»Ÿï¼Œå¯ä»¥å°†ä»è‡ªæˆ‘ä¸ºä¸­å¿ƒä¼ æ„Ÿå™¨æ•è·çš„åœºæ™¯è‡ªåŠ¨åˆ†å‰²æˆå„ä¸ª 3D å¯¹è±¡çš„å®Œæ•´åˆ†è§£ã€‚è¯¥ç³»ç»Ÿä¸“é—¨è®¾è®¡ç”¨äºè‡ªæˆ‘ä¸ºä¸­å¿ƒæ•°æ®ï¼Œå…¶ä¸­åœºæ™¯åŒ…å«æ•°ç™¾ä¸ªä»è‡ªç„¶ï¼ˆéæ‰«æï¼‰è¿åŠ¨ä¸­æ•è·çš„å¯¹è±¡ã€‚EgoLifter é‡‡ç”¨ 3D é«˜æ–¯åˆ†å¸ƒä½œä¸º 3D åœºæ™¯å’Œå¯¹è±¡çš„åŸºç¡€è¡¨ç¤ºï¼Œå¹¶ä½¿ç”¨ Segment Anything Model (SAM) çš„åˆ†å‰²æ©ç ä½œä¸ºå¼±ç›‘ç£ï¼Œä»¥å­¦ä¹ çµæ´»ä¸”å¯æç¤ºçš„å¯¹è±¡å®ä¾‹å®šä¹‰ï¼Œä¸å—ä»»ä½•ç‰¹å®šå¯¹è±¡åˆ†ç±»æ³•çš„é™åˆ¶ã€‚ä¸ºäº†åº”å¯¹è‡ªæˆ‘ä¸ºä¸­å¿ƒè§†é¢‘ä¸­åŠ¨æ€å¯¹è±¡å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œæœ¬æ–‡è®¾è®¡äº†ä¸€ä¸ªç¬æ€é¢„æµ‹æ¨¡å—ï¼Œè¯¥æ¨¡å—èƒ½å¤Ÿå­¦ä¼šåœ¨ 3D é‡å»ºä¸­æ»¤é™¤åŠ¨æ€å¯¹è±¡ã€‚æœ€ç»ˆçš„ç»“æœæ˜¯ä¸€ä¸ªå…¨è‡ªåŠ¨ç®¡é“ï¼Œèƒ½å¤Ÿå°† 3D å¯¹è±¡å®ä¾‹é‡å»ºä¸º 3D é«˜æ–¯åˆ†å¸ƒçš„é›†åˆï¼Œè¿™äº›é«˜æ–¯åˆ†å¸ƒå…±åŒæ„æˆæ•´ä¸ªåœºæ™¯ã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šåœ¨ Aria Digital Twin æ•°æ®é›†ä¸Šåˆ›å»ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œè¯¥åŸºå‡†å®šé‡è¯æ˜äº†è¯¥æ–¹æ³•åœ¨åŸºäºè‡ªç„¶è‡ªæˆ‘ä¸ºä¸­å¿ƒè¾“å…¥çš„å¼€æ”¾ä¸–ç•Œ 3D åˆ†å‰²ä¸­è¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚åœ¨å„ç§è‡ªæˆ‘ä¸ºä¸­å¿ƒæ´»åŠ¨æ•°æ®é›†ä¸Šè¿è¡Œ EgoLifterï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨è§„æ¨¡åŒ– 3D è‡ªæˆ‘ä¸ºä¸­å¿ƒæ„ŸçŸ¥æ–¹é¢çš„å‰æ™¯ã€‚</p><ol><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰EgoLifterç³»ç»Ÿé‡‡ç”¨3Dé«˜æ–¯åˆ†å¸ƒä½œä¸º3Dåœºæ™¯å’Œå¯¹è±¡çš„åŸºç¡€è¡¨ç¤ºï¼Œå¹¶ä½¿ç”¨SegmentAnythingModel (SAM)çš„åˆ†å‰²æ©ç ä½œä¸ºå¼±ç›‘ç£ï¼Œä»¥å­¦ä¹ çµæ´»ä¸”å¯æç¤ºçš„å¯¹è±¡å®ä¾‹å®šä¹‰ï¼Œä¸å—ä»»ä½•ç‰¹å®šå¯¹è±¡åˆ†ç±»æ³•çš„é™åˆ¶ã€‚ï¼ˆ2ï¼‰ä¸ºäº†åº”å¯¹è‡ªæˆ‘ä¸ºä¸­å¿ƒè§†é¢‘ä¸­åŠ¨æ€å¯¹è±¡å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œè®¾è®¡äº†ä¸€ä¸ªç¬æ€é¢„æµ‹æ¨¡å—ï¼Œè¯¥æ¨¡å—èƒ½å¤Ÿå­¦ä¼šåœ¨3Dé‡å»ºä¸­æ»¤é™¤åŠ¨æ€å¯¹è±¡ã€‚ï¼ˆ3ï¼‰EgoLifterç³»ç»Ÿæœ€ç»ˆçš„ç»“æœæ˜¯ä¸€ä¸ªå…¨è‡ªåŠ¨ç®¡é“ï¼Œèƒ½å¤Ÿå°†3Då¯¹è±¡å®ä¾‹é‡å»ºä¸º3Dé«˜æ–¯åˆ†å¸ƒçš„é›†åˆï¼Œè¿™äº›é«˜æ–¯åˆ†å¸ƒå…±åŒæ„æˆæ•´ä¸ªåœºæ™¯ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šEgoLifter ç®—æ³•åŒæ—¶è§£å†³äº†é‡å¤–ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„æ„ŸçŸ¥ä¸­çš„ 3D é‡å»ºå’Œå¼€æ”¾ä¸–ç•Œåˆ†å‰²é—®é¢˜ã€‚è¯¥ç®—æ³•é€šè¿‡å°† 2D åˆ†å‰²æå‡åˆ° 3D é«˜æ–¯åˆ†å¸ƒä¸­ï¼Œåœ¨æ²¡æœ‰ 3D æ•°æ®æ³¨é‡Šçš„æƒ…å†µä¸‹å®ç°äº†å¼ºå¤§çš„å¼€æ”¾ä¸–ç•Œ 2D/3D åˆ†å‰²æ€§èƒ½ã€‚ä¸ºäº†å¤„ç†ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†é¢‘ä¸­å¿«é€Ÿä¸”ç¨€ç–çš„åŠ¨æ€å˜åŒ–ï¼ŒEgoLifter é‡‡ç”¨ç¬æ€é¢„æµ‹ç½‘ç»œæ¥æ»¤é™¤ç¬æ€å¯¹è±¡å¹¶è·å¾—æ›´å‡†ç¡®çš„ 3D é‡å»ºã€‚EgoLifter åœ¨å‡ ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶ä¼˜äºå…¶ä»–ç°æœ‰çš„åŸºå‡†ã€‚EgoLifter è·å¾—çš„è¡¨ç¤ºè¿˜å¯ä»¥ç”¨äºå¤šç§ä¸‹æ¸¸ä»»åŠ¡ï¼Œå¦‚ 3D å¯¹è±¡èµ„äº§æå–å’Œåœºæ™¯ç¼–è¾‘ï¼Œæ˜¾ç¤ºå‡ºä¸ªäººå¯ç©¿æˆ´è®¾å¤‡å’Œ AR/VR åº”ç”¨çš„å·¨å¤§æ½œåŠ›ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šEgoLifter ç®—æ³•åˆ›æ–°æ€§åœ°å°† 3D é«˜æ–¯åˆ†å¸ƒä½œä¸º 3D åœºæ™¯å’Œå¯¹è±¡çš„åŸºç¡€è¡¨ç¤ºï¼Œå¹¶ä½¿ç”¨ SegmentAnythingModel (SAM) çš„åˆ†å‰²æ©ç ä½œä¸ºå¼±ç›‘ç£ï¼Œä»¥å­¦ä¹ çµæ´»ä¸”å¯æç¤ºçš„å¯¹è±¡å®ä¾‹å®šä¹‰ã€‚æ­¤å¤–ï¼ŒEgoLifter è¿˜è®¾è®¡äº†ä¸€ä¸ªç¬æ€é¢„æµ‹æ¨¡å—æ¥å¤„ç†ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†é¢‘ä¸­åŠ¨æ€å¯¹è±¡å¸¦æ¥çš„æŒ‘æˆ˜ã€‚æ€§èƒ½ï¼šEgoLifter åœ¨ AriaDigitalTwin æ•°æ®é›†ä¸Šåˆ›å»ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œå®šé‡è¯æ˜äº†è¯¥æ–¹æ³•åœ¨åŸºäºè‡ªç„¶è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è¾“å…¥çš„å¼€æ”¾ä¸–ç•Œ 3D åˆ†å‰²ä¸­è¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚åœ¨å„ç§ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„æ´»åŠ¨æ•°æ®é›†ä¸Šè¿è¡Œ EgoLifterï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨è§„æ¨¡åŒ– 3D è‡ªæˆ‘ä¸ºä¸­å¿ƒæ„ŸçŸ¥æ–¹é¢çš„å‰æ™¯ã€‚å·¥ä½œé‡ï¼šEgoLifter ç®—æ³•çš„å·¥ä½œé‡ç›¸å¯¹è¾ƒå¤§ï¼Œå› ä¸ºå®ƒéœ€è¦ä½¿ç”¨ 3D é«˜æ–¯åˆ†å¸ƒå’Œç¬æ€é¢„æµ‹ç½‘ç»œæ¥å¤„ç†ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†é¢‘ä¸­çš„å¤æ‚åœºæ™¯å’ŒåŠ¨æ€å¯¹è±¡ã€‚ç„¶è€Œï¼ŒEgoLifter ç®—æ³•çš„æ€§èƒ½ä¼˜åŠ¿è¯æ˜äº†å…¶å·¥ä½œé‡çš„åˆç†æ€§ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-d42109c42b75a98fe02551eea274cc18.jpg" align="middle"><img src="https://picx.zhimg.com/v2-85c08cbcea83ca1fe044d4f7eb2a87b8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3eaa82aafccc95f7929829abc7e4035d.jpg" align="middle"></details><h2 id="DN-Splatter-Depth-and-Normal-Priors-for-Gaussian-Splatting-and-Meshing"><a href="#DN-Splatter-Depth-and-Normal-Priors-for-Gaussian-Splatting-and-Meshing" class="headerlink" title="DN-Splatter: Depth and Normal Priors for Gaussian Splatting and Meshing"></a>DN-Splatter: Depth and Normal Priors for Gaussian Splatting and Meshing</h2><p><strong>Authors:Matias Turkulainen, Xuqian Ren, Iaroslav Melekhov, Otto Seiskari, Esa Rahtu, Juho Kannala</strong></p><p>3D Gaussian splatting, a novel differentiable rendering technique, has achieved state-of-the-art novel view synthesis results with high rendering speeds and relatively low training times. However, its performance on scenes commonly seen in indoor datasets is poor due to the lack of geometric constraints during optimization. We extend 3D Gaussian splatting with depth and normal cues to tackle challenging indoor datasets and showcase techniques for efficient mesh extraction, an important downstream application. Specifically, we regularize the optimization procedure with depth information, enforce local smoothness of nearby Gaussians, and use the geometry of the 3D Gaussians supervised by normal cues to achieve better alignment with the true scene geometry. We improve depth estimation and novel view synthesis results over baselines and show how this simple yet effective regularization technique can be used to directly extract meshes from the Gaussian representation yielding more physically accurate reconstructions on indoor scenes. Our code will be released in <a href="https://github.com/maturk/dn-splatter">https://github.com/maturk/dn-splatter</a>. </p><p><a href="http://arxiv.org/abs/2403.17822v1">PDF</a> </p><p><strong>Summary</strong><br>3Dé«˜æ–¯æ–‘ç‚¹æ¸²æŸ“æŠ€æœ¯é€šè¿‡æ·±åº¦å’Œæ³•çº¿ä¿¡æ¯ï¼Œå¢å¼ºäº†å¯¹å®¤å†…æ•°æ®é›†çš„å‡ ä½•çº¦æŸï¼Œæå‡äº†æ·±åº¦ä¼°è®¡å’Œæ–°è§†å›¾åˆæˆæ€§èƒ½ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>3Dé«˜æ–¯æ–‘ç‚¹æ¸²æŸ“æ˜¯ä¸€ç§æ–°é¢–çš„å¯å¾®æ¸²æŸ“æŠ€æœ¯ã€‚</li><li>3Dé«˜æ–¯æ–‘ç‚¹æ¸²æŸ“åœ¨å®¤å†…æ•°æ®é›†ä¸Šè¡¨ç°ä¸ä½³ï¼ŒåŸå› æ˜¯ä¼˜åŒ–è¿‡ç¨‹ä¸­ç¼ºä¹å‡ ä½•çº¦æŸã€‚</li><li>é€šè¿‡æ·±åº¦ä¿¡æ¯æ­£åˆ™åŒ–ä¼˜åŒ–è¿‡ç¨‹ï¼Œå¯ä»¥æ”¹å–„å®¤å†…æ•°æ®é›†çš„æ€§èƒ½ã€‚</li><li>é€šè¿‡å±€éƒ¨å¹³æ»‘å’Œæ³•çº¿ä¿¡æ¯ç›‘ç£ï¼Œå¯ä»¥å¢å¼º3Dé«˜æ–¯æ–‘ç‚¹çš„å‡ ä½•å¯¹é½ã€‚</li><li>æ”¹è¿›åçš„3Dé«˜æ–¯æ–‘ç‚¹æ¸²æŸ“æŠ€æœ¯å¯ç›´æ¥ä»é«˜æ–¯è¡¨ç¤ºä¸­æå–ç½‘æ ¼ï¼Œç”Ÿæˆæ›´ç‰©ç†å‡†ç¡®çš„å®¤å†…åœºæ™¯é‡å»ºã€‚</li><li>è¯¥æŠ€æœ¯ä»£ç å°†åœ¨<a href="https://github.com/maturk/dn-splatterä¸Šå‘å¸ƒã€‚">https://github.com/maturk/dn-splatterä¸Šå‘å¸ƒã€‚</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šDN-Splatterï¼šç”¨äºé«˜æ–¯æ•£å°„å’Œç½‘æ ¼åŒ–çš„æ·±åº¦å’Œæ³•çº¿å…ˆéªŒ</li><li>ä½œè€…ï¼šMatias Turkulainenâˆ—1, Xuqian Renâˆ—2, Iaroslav Melekhov3, Otto Seiskari4, Esa Rahtu2,4, Juho Kannala3,4</li><li>éš¶å±ï¼šè‹é»ä¸–è”é‚¦ç†å·¥å­¦é™¢</li><li>å…³é”®è¯ï¼šé«˜æ–¯æ•£å°„ã€å®¤å†…é‡å»ºã€å…ˆéªŒæ­£åˆ™åŒ–</li><li>è®ºæ–‡é“¾æ¥ï¼šNoneï¼ŒGithub ä»£ç é“¾æ¥ï¼šhttps://github.com/maturk/dn-splatter</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ï¼šä¸‰ç»´é«˜æ–¯æ•£å°„æ˜¯ä¸€ç§æ–°é¢–çš„å¯å¾®æ¸²æŸ“æŠ€æœ¯ï¼Œå·²åœ¨é«˜ä¿çœŸå›¾åƒåˆæˆä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ•ˆæœï¼Œå…·æœ‰è¾ƒå¿«çš„æ¸²æŸ“é€Ÿåº¦å’Œè¾ƒçŸ­çš„è®­ç»ƒæ—¶é—´ã€‚ç„¶è€Œï¼Œç”±äºä¼˜åŒ–è¿‡ç¨‹ä¸­ç¼ºä¹å‡ ä½•çº¦æŸï¼Œå®ƒåœ¨å®¤å†…æ•°æ®é›†å¸¸è§çš„åœºæ™¯ä¸­çš„æ€§èƒ½è¾ƒå·®ã€‚ï¼ˆ2ï¼‰ï¼šè¿‡å»çš„æ–¹æ³•åŒ…æ‹¬ Nerfactoã€Depth-Nerfactoã€Neusfactoã€MonoSDFã€Splatfacto å’Œ SuGaRã€‚è¿™äº›æ–¹æ³•å­˜åœ¨çš„é—®é¢˜æ˜¯ç¼ºä¹å‡ ä½•çº¦æŸï¼Œå¯¼è‡´åœ¨å®¤å†…åœºæ™¯ä¸­æ€§èƒ½ä¸ä½³ã€‚æœ¬æ–‡æå‡ºçš„æ–¹æ³•åŠ¨æœºæ˜ç¡®ï¼Œé€šè¿‡æ·±åº¦å’Œæ³•çº¿ä¿¡æ¯æ¥æ‰©å±•ä¸‰ç»´é«˜æ–¯æ•£å°„ï¼Œä»¥è§£å†³å®¤å†…åœºæ™¯çš„æŒ‘æˆ˜ã€‚ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•åŒ…æ‹¬ï¼šåˆ©ç”¨æ·±åº¦ä¿¡æ¯å¯¹ä¼˜åŒ–è¿‡ç¨‹è¿›è¡Œæ­£åˆ™åŒ–ã€å¢å¼ºé™„è¿‘é«˜æ–¯åˆ†å¸ƒçš„å±€éƒ¨å¹³æ»‘åº¦ã€åˆ©ç”¨æ³•çº¿ä¿¡æ¯ç›‘ç£ä¸‰ç»´é«˜æ–¯åˆ†å¸ƒçš„å‡ ä½•å½¢çŠ¶ï¼Œä»¥æ›´å¥½åœ°ä¸çœŸå®åœºæ™¯å‡ ä½•å½¢çŠ¶å¯¹é½ã€‚ï¼ˆ4ï¼‰ï¼šæœ¬æ–‡æ–¹æ³•åœ¨ä»¥ä¸‹ä»»åŠ¡å’Œæ€§èƒ½æ–¹é¢å–å¾—äº†è¿›å±•ï¼šåœ¨å®¤å†…åœºæ™¯ä¸Šæé«˜äº†æ·±åº¦ä¼°è®¡å’Œæ–°è§†å›¾åˆæˆç»“æœï¼Œè¡¨æ˜è¯¥æ–¹æ³•å¯ä»¥ä»é«˜æ–¯è¡¨ç¤ºä¸­ç›´æ¥æå–ç½‘æ ¼ï¼Œä»è€Œåœ¨å®¤å†…åœºæ™¯ä¸­å®ç°æ›´ç‰©ç†å‡†ç¡®çš„é‡å»ºã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ã€‚</li></ol><p>7.æ–¹æ³•ï¼š(1): åˆ©ç”¨æ·±åº¦ä¿¡æ¯æ­£åˆ™åŒ–ä¼˜åŒ–è¿‡ç¨‹ï¼›(2): å¢å¼ºé™„è¿‘é«˜æ–¯åˆ†å¸ƒçš„å±€éƒ¨å¹³æ»‘åº¦ï¼›(3): åˆ©ç”¨æ³•çº¿ä¿¡æ¯ç›‘ç£ä¸‰ç»´é«˜æ–¯åˆ†å¸ƒçš„å‡ ä½•å½¢çŠ¶ï¼›(4): åˆ©ç”¨ä¼˜åŒ–åçš„é«˜æ–¯åœºæ™¯ç›´æ¥æå–ç½‘æ ¼ï¼Œæ— éœ€é¢å¤–çš„ä¼˜åŒ–æˆ–ç»†åŒ–é˜¶æ®µã€‚</p><ol><li>ç»“è®ºï¼š(1): æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºæ·±åº¦å’Œæ³•çº¿æ­£åˆ™åŒ–çš„ä¸‰ç»´é«˜æ–¯æ•£å°„æ–¹æ³•ï¼Œè¯æ˜äº†è¿™ç§ç®€å•ä½†æœ‰æ•ˆçš„æ–¹æ³•å¯ä»¥é€šè¿‡æé«˜å¸¸è§çš„æ–°è§†å›¾ RGB æŒ‡æ ‡ä»¥åŠæ˜¾è‘—æé«˜ä»é«˜æ–¯åœºæ™¯è¡¨ç¤ºä¸­æå–çš„æ·±åº¦ä¼°è®¡å’Œè¡¨é¢è´¨é‡æ¥å¢å¼ºç…§ç‰‡çœŸå®æ„Ÿã€‚æˆ‘ä»¬å±•ç¤ºäº†å…ˆéªŒæ­£åˆ™åŒ–å¯¹äºåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å®¤å†…åœºæ™¯ä¸­å®ç°æ›´å‡ ä½•æœ‰æ•ˆé‡å»ºçš„å¿…è¦æ€§ã€‚(2): åˆ›æ–°ç‚¹ï¼šåˆ©ç”¨æ·±åº¦å’Œæ³•çº¿ä¿¡æ¯æ‰©å±•ä¸‰ç»´é«˜æ–¯æ•£å°„ï¼Œè§£å†³å®¤å†…åœºæ™¯çš„å‡ ä½•çº¦æŸé—®é¢˜ï¼›æ€§èƒ½ï¼šåœ¨å®¤å†…åœºæ™¯ä¸Šæé«˜äº†æ·±åº¦ä¼°è®¡å’Œæ–°è§†å›¾åˆæˆç»“æœï¼Œè¡¨æ˜è¯¥æ–¹æ³•å¯ä»¥ä»é«˜æ–¯è¡¨ç¤ºä¸­ç›´æ¥æå–ç½‘æ ¼ï¼Œä»è€Œåœ¨å®¤å†…åœºæ™¯ä¸­å®ç°æ›´ç‰©ç†å‡†ç¡®çš„é‡å»ºï¼›å·¥ä½œé‡ï¼šåˆ©ç”¨ä¼˜åŒ–åçš„é«˜æ–¯åœºæ™¯ç›´æ¥æå–ç½‘æ ¼ï¼Œæ— éœ€é¢å¤–çš„ä¼˜åŒ–æˆ–ç»†åŒ–é˜¶æ®µã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-6f13f0240c5cc6d6adeccaff39bcf966.jpg" align="middle"><img src="https://pica.zhimg.com/v2-efcb3b451413f0f8f9d4557e2ca5fe0b.jpg" align="middle"></details><h2 id="GSDF-3DGS-Meets-SDF-for-Improved-Rendering-and-Reconstruction"><a href="#GSDF-3DGS-Meets-SDF-for-Improved-Rendering-and-Reconstruction" class="headerlink" title="GSDF: 3DGS Meets SDF for Improved Rendering and Reconstruction"></a>GSDF: 3DGS Meets SDF for Improved Rendering and Reconstruction</h2><p><strong>Authors:Mulin Yu, Tao Lu, Linning Xu, Lihan Jiang, Yuanbo Xiangli, Bo Dai</strong></p><p>Presenting a 3D scene from multiview images remains a core and long-standing challenge in computer vision and computer graphics. Two main requirements lie in rendering and reconstruction. Notably, SOTA rendering quality is usually achieved with neural volumetric rendering techniques, which rely on aggregated point/primitive-wise color and neglect the underlying scene geometry. Learning of neural implicit surfaces is sparked from the success of neural rendering. Current works either constrain the distribution of density fields or the shape of primitives, resulting in degraded rendering quality and flaws on the learned scene surfaces. The efficacy of such methods is limited by the inherent constraints of the chosen neural representation, which struggles to capture fine surface details, especially for larger, more intricate scenes. To address these issues, we introduce GSDF, a novel dual-branch architecture that combines the benefits of a flexible and efficient 3D Gaussian Splatting (3DGS) representation with neural Signed Distance Fields (SDF). The core idea is to leverage and enhance the strengths of each branch while alleviating their limitation through mutual guidance and joint supervision. We show on diverse scenes that our design unlocks the potential for more accurate and detailed surface reconstructions, and at the meantime benefits 3DGS rendering with structures that are more aligned with the underlying geometry. </p><p><a href="http://arxiv.org/abs/2403.16964v1">PDF</a> Project page: <a href="https://city-super.github.io/GSDF">https://city-super.github.io/GSDF</a></p><p><strong>Summary</strong><br>ä¸‰ç»´é«˜æ–¯æ³¼æº… (3DGS) ä¸ç¥ç»ç¬¦å·è·ç¦»åœº (SDF) ç›¸ç»“åˆï¼Œå¯ç”¨äºå‘ˆç°æ›´å‡†ç¡®ã€æ›´ç²¾ç»†çš„è¡¨é¢é‡å»ºæ•ˆæœï¼Œå¹¶å¢å¼º 3DGS æ¸²æŸ“çš„ç»“æ„ï¼Œä½¿å…¶æ›´ç¬¦åˆåº•å±‚å‡ ä½•å›¾å½¢ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>3DGS å’Œç¥ç» SDF çš„ç»“åˆï¼Œå¯æå‡æ¸²æŸ“å’Œé‡å»ºæ•ˆæœã€‚</li><li>ç¥ç»ä½“ç§¯æ¸²æŸ“æŠ€æœ¯ï¼Œé‡ç‚¹å…³æ³¨ç‚¹/åŸºå…ƒé¢œè‰²ï¼Œå¿½ç•¥äº†åº•å±‚åœºæ™¯å‡ ä½•å›¾å½¢ã€‚</li><li>ç¥ç»éšå¼è¡¨é¢å­¦ä¹ ï¼Œ å—ç¥ç»æ¸²æŸ“æˆåŠŸå¯å‘ã€‚</li><li>å½“å‰å·¥ä½œï¼Œé™åˆ¶å¯†åº¦åœºçš„åˆ†å¸ƒæˆ–åŸºå…ƒçš„å½¢çŠ¶ï¼Œå¯¼è‡´æ¸²æŸ“è´¨é‡ä¸‹é™ï¼Œå­¦ä¹ åœºæ™¯è¡¨é¢å­˜åœ¨ç¼ºé™·ã€‚</li><li>GSDF æ¶æ„ï¼Œç»“åˆ 3DGS å’Œç¥ç» SDF çš„ä¼˜ç‚¹ï¼Œé€šè¿‡ç›¸äº’æŒ‡å¯¼å’Œè”åˆç›‘ç£ï¼Œç¼“è§£å…¶å±€é™æ€§ã€‚</li><li>GSDF è®¾è®¡ï¼Œæ›´å‡†ç¡®ã€æ›´ç²¾ç»†çš„è¡¨é¢é‡å»ºï¼ŒåŒæ—¶æé«˜ 3DGS æ¸²æŸ“çš„ç»“æ„ï¼Œä½¿å…¶æ›´ç¬¦åˆåº•å±‚å‡ ä½•å›¾å½¢ã€‚</li><li>GSDF åœ¨ä¸åŒåœºæ™¯ä¸­ï¼Œéƒ½å±•ç¤ºäº†å…¶æ½œåŠ›ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šGSDFï¼š3DGS èåˆ SDFï¼Œæå‡æ¸²æŸ“å’Œé‡å»ºæ•ˆæœ</li><li>ä½œè€…ï¼šMulin Yu1âˆ—, Tao Lu1âˆ—, Linning Xu2, Lihan Jiang3, Yuanbo Xiangli4ï¿½, Bo Dai1</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤</li><li>å…³é”®è¯ï¼šç¥ç»åœºæ™¯æ¸²æŸ“Â·3D é«˜æ–¯ç‚¹äº‘Â·ç¥ç»æ›²é¢é‡å»º</li><li>è®ºæ–‡åœ°å€ï¼šhttps://arxiv.org/abs/2403.16964ï¼ŒGithub ä»£ç ï¼šæ— </li><li>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ï¼šä»å¤šè§†è§’å›¾åƒå‘ˆç° 3D åœºæ™¯ä»ç„¶æ˜¯è®¡ç®—æœºè§†è§‰å’Œè®¡ç®—æœºå›¾å½¢å­¦ä¸­ä¸€é¡¹æ ¸å¿ƒä¸”é•¿æœŸçš„æŒ‘æˆ˜ã€‚ä¸»è¦åŒ…å«æ¸²æŸ“å’Œé‡å»ºä¸¤ä¸ªè¦æ±‚ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæœ€å…ˆè¿›çš„æ¸²æŸ“è´¨é‡é€šå¸¸é€šè¿‡ç¥ç»ä½“ç§¯æ¸²æŸ“æŠ€æœ¯å®ç°ï¼Œè¯¥æŠ€æœ¯ä¾èµ–äºèšåˆçš„ç‚¹/åŸºå…ƒé¢œè‰²ï¼Œè€Œå¿½ç•¥äº†åº•å±‚åœºæ™¯å‡ ä½•ã€‚   ï¼ˆ2ï¼‰ï¼šç¥ç»éšå¼æ›²é¢çš„å­¦ä¹ æºäºç¥ç»æ¸²æŸ“çš„æˆåŠŸã€‚å½“å‰å·¥ä½œè¦ä¹ˆé™åˆ¶å¯†åº¦åœºçš„åˆ†å¸ƒæˆ–åŸºå…ƒçš„å½¢çŠ¶ï¼Œå¯¼è‡´æ¸²æŸ“è´¨é‡ä¸‹é™å’Œå­¦ä¹ åœºæ™¯æ›²é¢ä¸Šçš„ç¼ºé™·ã€‚æ­¤ç±»æ–¹æ³•çš„æœ‰æ•ˆæ€§å—åˆ°æ‰€é€‰ç¥ç»è¡¨ç¤ºçš„å›ºæœ‰çº¦æŸçš„é™åˆ¶ï¼Œéš¾ä»¥æ•æ‰ç²¾ç»†çš„æ›²é¢ç»†èŠ‚ï¼Œç‰¹åˆ«æ˜¯å¯¹äºæ›´å¤§ã€æ›´å¤æ‚çš„åœºæ™¯ã€‚   ï¼ˆ3ï¼‰ï¼šä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº† GSDFï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„åŒåˆ†æ”¯æ¶æ„ï¼Œå®ƒç»“åˆäº†çµæ´»ä¸”é«˜æ•ˆçš„ 3D é«˜æ–¯ç‚¹äº‘ï¼ˆ3DGSï¼‰è¡¨ç¤ºä¸ç¥ç»ç¬¦å·è·ç¦»åœºï¼ˆSDFï¼‰çš„ä¼˜ç‚¹ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨å’Œå¢å¼ºæ¯ä¸ªåˆ†æ”¯çš„ä¼˜åŠ¿ï¼ŒåŒæ—¶é€šè¿‡ç›¸äº’æŒ‡å¯¼å’Œè”åˆç›‘ç£æ¥å‡è½»å®ƒä»¬çš„é™åˆ¶ã€‚æˆ‘ä»¬åœ¨å„ç§åœºæ™¯ä¸­å±•ç¤ºäº†æˆ‘ä»¬çš„è®¾è®¡é‡Šæ”¾äº†æ›´å‡†ç¡®å’Œè¯¦ç»†çš„æ›²é¢é‡å»ºçš„æ½œåŠ›ï¼ŒåŒæ—¶ä½¿ 3DGS æ¸²æŸ“å—ç›Šäºä¸åº•å±‚å‡ ä½•æ›´ä¸€è‡´çš„ç»“æ„ã€‚   ï¼ˆ4ï¼‰ï¼šåœ¨ä¸åŒçš„åœºæ™¯å’Œä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•éƒ½å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</li></ol><p>7.Methodsï¼š(1): æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŒåˆ†æ”¯æ¡†æ¶ï¼Œå…¶ä¸­GSåˆ†æ”¯ä¸“æ³¨äºé«˜æ•ˆã€é«˜è´¨é‡çš„æ¸²æŸ“ï¼Œè€ŒSDFåˆ†æ”¯ä¸“æ³¨äºå­¦ä¹ ç¥ç»éšå¼GSDFã€‚(2): æˆ‘ä»¬æœ‰æ•ˆåœ°ä¿ç•™äº†é«˜æ–¯åŸºå…ƒæ¸²æŸ“çš„æ•ˆç‡å’Œä¿çœŸåº¦ä¼˜åŠ¿ï¼Œå¹¶ä»NeuS[29]æ”¹ç¼–çš„SDFåœºä¸­æ›´å‡†ç¡®åœ°é€¼è¿‘åœºæ™¯è¡¨é¢ã€‚(3): æˆ‘ä»¬åˆ©ç”¨GSåˆ†æ”¯çš„æ•ˆç‡å’Œçµæ´»æ€§ä¼˜åŠ¿ï¼Œæ¸²æŸ“æ·±åº¦å›¾å¹¶æŒ‡å¯¼SDFåˆ†æ”¯çš„å…‰çº¿é‡‡æ ·è¿‡ç¨‹ã€‚(4): æˆ‘ä»¬ä½¿ç”¨æ¥è‡ªSDFåˆ†æ”¯çš„é¢„æµ‹SDFå€¼æ¥æŒ‡å¯¼GSåˆ†æ”¯çš„å¯†åº¦æ§åˆ¶ï¼Œåœ¨è¿‘è¡¨é¢åŒºåŸŸç”Ÿé•¿é«˜æ–¯åŸºå…ƒï¼Œå¹¶å‰ªé™¤è¿œç¦»è¡¨é¢çš„åŸºå…ƒã€‚(5): æˆ‘ä»¬é€šè¿‡æ¯”è¾ƒæ¥è‡ªæ¯ä¸ªåˆ†æ”¯çš„æ·±åº¦å›¾å’Œæ³•çº¿å›¾æ¥è¿›ä¸€æ­¥å¢å¼ºç›¸äº’å‡ ä½•ä¸€è‡´æ€§ï¼Œä»¥é¼“åŠ±é«˜æ–¯åŸºå…ƒå’Œè¡¨é¢ä¹‹é—´æ›´ä¸€è‡´çš„ç‰©ç†å¯¹é½ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œæå‡ºäº†ä¸€ç§åŒåˆ†æ”¯æ¡†æ¶ï¼Œåˆ©ç”¨äº† 3D-GS å’Œ SDF çš„ä¼˜åŠ¿ï¼Œå±•ç¤ºäº†å…¶åœ¨ä¿æŒè®­ç»ƒå’Œæ¨ç†æ•ˆç‡çš„åŒæ—¶ï¼Œåœ¨æ¸²æŸ“å’Œé‡å»ºè´¨é‡ä¸Šå–å¾—æå‡çš„æ½œåŠ›ã€‚ä¸¤ç§éšå¼è¡¨ç¤ºã€æ¸²æŸ“æ–¹æ³•å’Œç›‘ç£æŸå¤±çš„å›ºæœ‰å·®å¼‚å¯¹ä¸¤è€…æ— ç¼é›†æˆæå‡ºäº†æŒ‘æˆ˜ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è€ƒè™‘äº†ä¸€ç§åŒå‘ç›¸äº’æŒ‡å¯¼æ–¹æ³•æ¥è§„é¿è¿™äº›é™åˆ¶ã€‚åœ¨æˆ‘ä»¬çš„æ¡†æ¶ä¸­å¼•å…¥äº†å¹¶éªŒè¯äº†ä¸‰ç§æŒ‡å¯¼ï¼š1ï¼‰æ·±åº¦å¼•å¯¼é‡‡æ ·ï¼ˆGSâ†’SDFï¼‰ï¼Œ2ï¼‰å‡ ä½•æ„ŸçŸ¥é«˜æ–¯å¯†åº¦æ§åˆ¶ï¼ˆSDFâ†’GSï¼‰ï¼›3ï¼‰ç›¸äº’å‡ ä½•ç›‘ç£ï¼ˆGSâ†”SDFï¼‰ã€‚æˆ‘ä»¬å¹¿æ³›çš„ç»“æœè¯æ˜äº†åœ¨ä¸¤ä¸ªä»»åŠ¡ä¸Šçš„æ•ˆç‡å’Œè”åˆæ€§èƒ½æ”¹è¿›ã€‚ç”±äºè¿™ä¸¤ä¸ªåˆ†æ”¯ä¿æŒäº†å®ƒä»¬çš„åŸå§‹æ¶æ„ï¼Œæˆ‘ä»¬åœ¨æ¨ç†æœŸé—´ä¿æŒäº†å®ƒä»¬çš„æ•ˆç‡ï¼Œä¸ºå°†æ¥é€šè¿‡æ›´é«˜çº§çš„æ¨¡å‹æ›¿æ¢æ¯ä¸ªåˆ†æ”¯ç•™å‡ºäº†æ½œåœ¨çš„å¢å¼ºç©ºé—´ã€‚æˆ‘ä»¬è®¾æƒ³æˆ‘ä»¬çš„æ¨¡å‹å°†æœ‰åˆ©äºå¯¹é«˜è´¨é‡æ¸²æŸ“å’Œå‡ ä½•æœ‰è¦æ±‚çš„åº”ç”¨ï¼ŒåŒ…æ‹¬å…·èº«ç¯å¢ƒã€ç‰©ç†æ¨¡æ‹Ÿå’Œæ²‰æµ¸å¼ VR ä½“éªŒã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŒåˆ†æ”¯æ¡†æ¶ï¼Œç»“åˆäº† 3D-GS å’Œ SDF çš„ä¼˜ç‚¹ï¼Œæé«˜äº†æ¸²æŸ“å’Œé‡å»ºè´¨é‡ï¼›æ€§èƒ½ï¼šåœ¨æ¸²æŸ“å’Œé‡å»ºè´¨é‡ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼›å·¥ä½œé‡ï¼šè¯¥æ–¹æ³•å…·æœ‰è¾ƒé«˜çš„æ•ˆç‡ï¼Œåœ¨è®­ç»ƒå’Œæ¨ç†é˜¶æ®µéƒ½ä¿æŒäº†è¾ƒä½çš„è®¡ç®—æˆæœ¬ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-845f4824f5b5d708e26e78764b0f6c62.jpg" align="middle"><img src="https://picx.zhimg.com/v2-264655e62e1548d0343d272dca0f7812.jpg" align="middle"><img src="https://picx.zhimg.com/v2-be53d57d9316fa9c7ed994d73a3dddc1.jpg" align="middle"></details><h2 id="latentSplat-Autoencoding-Variational-Gaussians-for-Fast-Generalizable-3D-Reconstruction"><a href="#latentSplat-Autoencoding-Variational-Gaussians-for-Fast-Generalizable-3D-Reconstruction" class="headerlink" title="latentSplat: Autoencoding Variational Gaussians for Fast Generalizable   3D Reconstruction"></a>latentSplat: Autoencoding Variational Gaussians for Fast Generalizable   3D Reconstruction</h2><p><strong>Authors:Christopher Wewer, Kevin Raj, Eddy Ilg, Bernt Schiele, Jan Eric Lenssen</strong></p><p>We present latentSplat, a method to predict semantic Gaussians in a 3D latent space that can be splatted and decoded by a light-weight generative 2D architecture. Existing methods for generalizable 3D reconstruction either do not enable fast inference of high resolution novel views due to slow volume rendering, or are limited to interpolation of close input views, even in simpler settings with a single central object, where 360-degree generalization is possible. In this work, we combine a regression-based approach with a generative model, moving towards both of these capabilities within the same method, trained purely on readily available real video data. The core of our method are variational 3D Gaussians, a representation that efficiently encodes varying uncertainty within a latent space consisting of 3D feature Gaussians. From these Gaussians, specific instances can be sampled and rendered via efficient Gaussian splatting and a fast, generative decoder network. We show that latentSplat outperforms previous works in reconstruction quality and generalization, while being fast and scalable to high-resolution data. </p><p><a href="http://arxiv.org/abs/2403.16292v1">PDF</a> Project website: <a href="https://geometric-rl.mpi-inf.mpg.de/latentsplat/">https://geometric-rl.mpi-inf.mpg.de/latentsplat/</a></p><p><strong>Summary</strong><br>é€šè¿‡å°†å›å½’æ¨¡å‹ä¸ç”Ÿæˆæ¨¡å‹ç›¸ç»“åˆï¼ŒlatentSplat èƒ½å¤Ÿä½¿ç”¨ç”± 3D ç‰¹å¾é«˜æ–¯åˆ†å¸ƒç»„æˆçš„æ½œåœ¨ç©ºé—´ä¸­çš„è¯­ä¹‰é«˜æ–¯åˆ†å¸ƒé¢„æµ‹å¿«é€Ÿæ¨ç†é«˜åˆ†è¾¨ç‡æ–°è§†å›¾ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³• latentSplatï¼Œå¯ä»¥é¢„æµ‹ 3D æ½œåœ¨ç©ºé—´ä¸­çš„è¯­ä¹‰é«˜æ–¯åˆ†å¸ƒï¼Œå¹¶é€šè¿‡è½»é‡çº§ç”Ÿæˆ 2D æ¶æ„è¿›è¡Œ splatting å’Œè§£ç ã€‚</li><li>latentSplat å°†å›å½’æ–¹æ³•ä¸ç”Ÿæˆæ¨¡å‹ç›¸ç»“åˆï¼Œåœ¨åŒä¸€ä¸ªæ–¹æ³•ä¸­å®ç°äº†å¿«é€Ÿæ¨ç†é«˜åˆ†è¾¨ç‡æ–°è§†å›¾å’Œ 360 åº¦æ³›åŒ–çš„èƒ½åŠ›ã€‚</li><li>latentSplat çš„æ ¸å¿ƒæ˜¯åŸºäºå˜åˆ† 3D é«˜æ–¯åˆ†å¸ƒï¼Œè¯¥è¡¨ç¤ºæœ‰æ•ˆåœ°å¯¹æ½œåœ¨ç©ºé—´ä¸­åŒ…å« 3D ç‰¹å¾é«˜æ–¯åˆ†å¸ƒçš„ä¸ç¡®å®šæ€§è¿›è¡Œç¼–ç ã€‚</li><li>å¯ä»¥ä»è¿™äº›é«˜æ–¯åˆ†å¸ƒä¸­é‡‡æ ·ç‰¹å®šå®ä¾‹å¹¶é€šè¿‡é«˜æ•ˆçš„é«˜æ–¯ splatting å’Œå¿«é€Ÿçš„ç”Ÿæˆè§£ç ç½‘ç»œè¿›è¡Œæ¸²æŸ“ã€‚</li><li>latentSplat åœ¨é‡å»ºè´¨é‡å’Œæ³›åŒ–æ–¹é¢ä¼˜äºä»¥å‰çš„å·¥ä½œï¼ŒåŒæ—¶å¯¹é«˜åˆ†è¾¨ç‡æ•°æ®å¿«é€Ÿä¸”å¯æ‰©å±•ã€‚</li><li>latentSplat ä¸éœ€è¦æ˜¾å¼ä½“ç§¯æ¸²æŸ“ï¼Œå› æ­¤å¯¹äºé«˜åˆ†è¾¨ç‡åœºæ™¯å…·æœ‰æ•ˆç‡ä¼˜åŠ¿ã€‚</li><li>latentSplatä»…ä½¿ç”¨ç°æˆçš„çœŸå®è§†é¢‘æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œæ— éœ€ 3D æ‰«ææˆ–é‡å»ºæ•°æ®ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šlatentSplatï¼šå¿«é€Ÿæ³›åŒ– 3D é‡å»ºçš„è‡ªåŠ¨ç¼–ç å˜åˆ†é«˜æ–¯</li><li>ä½œè€…ï¼šChristopher Wewerã€Kevin Rajã€Eddy Ilgã€Bernt Schieleã€Jan Eric Lenssen</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šé©¬æ™®å­¦ä¼šä¿¡æ¯å­¦ç ”ç©¶æ‰€</li><li>å…³é”®è¯ï¼š3D é‡å»ºã€æ–°è§†è§’åˆæˆã€ç‰¹å¾é«˜æ–¯ä½“ç´ åŒ–ã€é«˜æ•ˆ 3D è¡¨å¾å­¦ä¹ </li><li>è®ºæ–‡é“¾æ¥ï¼šNone    Github é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šç°æœ‰æ³›åŒ– 3D é‡å»ºæ–¹æ³•è¦ä¹ˆç”±äºä½“ç»˜åˆ¶å›¾é€Ÿåº¦æ…¢è€Œæ— æ³•å¿«é€Ÿæ¨æ–­é«˜åˆ†è¾¨ç‡æ–°è§†è§’ï¼Œè¦ä¹ˆä»…é™äºæ’å€¼æ¥è¿‘è¾“å…¥è§†è§’ï¼Œå³ä½¿åœ¨ä»…æœ‰å•ä¸ªä¸­å¿ƒç‰©ä½“çš„ç®€å•åœºæ™¯ä¸­ï¼Œä¹Ÿæ— æ³•è¿›è¡Œ 360 åº¦æ³›åŒ–ã€‚ï¼ˆ2ï¼‰ï¼šè¿‡å»æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•å­˜åœ¨é—®é¢˜ï¼šè¦ä¹ˆæ— æ³•å¿«é€Ÿæ¨æ–­é«˜åˆ†è¾¨ç‡æ–°è§†è§’ï¼Œè¦ä¹ˆä»…é™äºæ’å€¼æ¥è¿‘è¾“å…¥è§†è§’ã€‚ï¼ˆ3ï¼‰ï¼šç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆå›å½’æ–¹æ³•å’Œç”Ÿæˆæ¨¡å‹ï¼Œåœ¨åŒä¸€æ–¹æ³•ä¸­æœç€è¿™ä¸¤ç§èƒ½åŠ›è¿ˆè¿›ï¼Œå®Œå…¨åœ¨å®¹æ˜“è·å–çš„çœŸå®è§†é¢‘æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ˜¯å˜åˆ† 3D é«˜æ–¯ï¼Œè¿™æ˜¯ä¸€ç§è¡¨å¾ï¼Œå¯æœ‰æ•ˆç¼–ç æ½œä¼ç©ºé—´ä¸­ä¸åŒç‰¹å¾é«˜æ–¯ä½“ç´ çš„ä¸ç¡®å®šæ€§ã€‚ä»è¿™äº›é«˜æ–¯ä½“ç´ ä¸­ï¼Œå¯ä»¥é€šè¿‡é«˜æ•ˆçš„é«˜æ–¯ä½“ç´ åŒ–å’Œå¿«é€Ÿçš„ç”Ÿæˆè§£ç å™¨ç½‘ç»œå¯¹ç‰¹å®šå®ä¾‹è¿›è¡Œé‡‡æ ·å’Œæ¸²æŸ“ã€‚ï¼ˆ4ï¼‰ï¼šæ–¹æ³•æ€§èƒ½ï¼šå®éªŒè¡¨æ˜ï¼ŒlatentSplat åœ¨é‡å»ºè´¨é‡å’Œæ³›åŒ–æ–¹é¢ä¼˜äºä»¥å¾€å·¥ä½œï¼ŒåŒæ—¶å¯¹é«˜åˆ†è¾¨ç‡æ•°æ®å…·æœ‰å¿«é€Ÿæ€§å’Œå¯æ‰©å±•æ€§ã€‚</li></ol><p>7.Methods:(1):latentSplatæ–¹æ³•ç»“åˆäº†å›å½’æ–¹æ³•å’Œç”Ÿæˆæ¨¡å‹ï¼Œåœ¨åŒä¸€æ–¹æ³•ä¸­æœç€å¿«é€Ÿæ¨æ–­é«˜åˆ†è¾¨ç‡æ–°è§†è§’å’Œ360åº¦æ³›åŒ–ä¸¤æ–¹é¢è¿ˆè¿›ï¼›(2):æ–¹æ³•çš„æ ¸å¿ƒæ˜¯å˜åˆ†3Dé«˜æ–¯ï¼Œå®ƒæ˜¯ä¸€ç§è¡¨å¾ï¼Œå¯æœ‰æ•ˆç¼–ç æ½œä¼ç©ºé—´ä¸­ä¸åŒç‰¹å¾é«˜æ–¯ä½“ç´ çš„ä¸ç¡®å®šæ€§ï¼›(3):ä»è¿™äº›é«˜æ–¯ä½“ç´ ä¸­ï¼Œå¯ä»¥é€šè¿‡é«˜æ•ˆçš„é«˜æ–¯ä½“ç´ åŒ–å’Œå¿«é€Ÿçš„ç”Ÿæˆè§£ç å™¨ç½‘ç»œå¯¹ç‰¹å®šå®ä¾‹è¿›è¡Œé‡‡æ ·å’Œæ¸²æŸ“ã€‚</p><ol><li><strong>ç»“è®º</strong>(1): latentSplat æ˜¯ä¸€ç§å°†å›å½’æ–¹æ³•å’Œç”Ÿæˆæ¨¡å‹çš„ä¼˜åŠ¿æˆåŠŸç»“åˆèµ·æ¥çš„æ–¹æ³•ï¼Œä»¥å¤„ç†ä¸ç¡®å®šæ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ–°çš„è§†å›¾åˆæˆä¸­å®ç°äº†æœ€å…ˆè¿›çš„å›¾åƒè´¨é‡ï¼ŒåŒæ—¶æä¾›äº†ä¸çœŸå®æƒ…å†µæœ€é«˜çš„æ„ŸçŸ¥ç›¸ä¼¼æ€§ã€‚ä¸ä¹‹å‰çš„ç”Ÿæˆæ–¹æ³•ç›¸æ¯”ï¼ŒlatentSplat çš„é€Ÿåº¦æ›´å¿«ï¼Œå¯æ‰©å±•æ€§æ›´å¼ºï¼Œèƒ½å¤Ÿä»¥æ›´é«˜çš„åˆ†è¾¨ç‡è¿›è¡Œå®æ—¶æ¸²æŸ“ã€‚(2): <strong>åˆ›æ–°ç‚¹ï¼š</strong></li><li>æå‡ºäº†ä¸€ç§æ–°çš„è¡¨å¾â€”â€”å˜åˆ† 3D é«˜æ–¯ï¼Œå®ƒå¯ä»¥æœ‰æ•ˆåœ°å¯¹æ½œä¼ç©ºé—´ä¸­ä¸åŒç‰¹å¾é«˜æ–¯ä½“ç´ çš„ä¸ç¡®å®šæ€§è¿›è¡Œç¼–ç ã€‚</li><li>è®¾è®¡äº†ä¸€ç§é«˜æ•ˆçš„é«˜æ–¯ä½“ç´ åŒ–å’Œå¿«é€Ÿçš„ç”Ÿæˆè§£ç å™¨ç½‘ç»œï¼Œå¯ä»¥ä»é«˜æ–¯ä½“ç´ ä¸­å¯¹ç‰¹å®šå®ä¾‹è¿›è¡Œé‡‡æ ·å’Œæ¸²æŸ“ã€‚<strong>æ€§èƒ½ï¼š</strong></li><li>åœ¨é‡å»ºè´¨é‡å’Œæ³›åŒ–æ–¹é¢ä¼˜äºä»¥å¾€çš„å·¥ä½œã€‚</li><li>å¯¹é«˜åˆ†è¾¨ç‡æ•°æ®å…·æœ‰å¿«é€Ÿæ€§å’Œå¯æ‰©å±•æ€§ã€‚<strong>å·¥ä½œé‡ï¼š</strong></li><li>è¯¥æ–¹æ³•å®Œå…¨åœ¨å®¹æ˜“è·å–çš„çœŸå®è§†é¢‘æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚</li><li>è¯¥æ–¹æ³•çš„è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦éƒ½å¾ˆå¿«ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-812603706bcb6f004a93be35208c508e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-472f1454ee4fe157880ee415da76b6fb.jpg" align="middle"></details><h2 id="CG-SLAM-Efficient-Dense-RGB-D-SLAM-in-a-Consistent-Uncertainty-aware-3D-Gaussian-Field"><a href="#CG-SLAM-Efficient-Dense-RGB-D-SLAM-in-a-Consistent-Uncertainty-aware-3D-Gaussian-Field" class="headerlink" title="CG-SLAM: Efficient Dense RGB-D SLAM in a Consistent Uncertainty-aware 3D   Gaussian Field"></a>CG-SLAM: Efficient Dense RGB-D SLAM in a Consistent Uncertainty-aware 3D   Gaussian Field</h2><p><strong>Authors:Jiarui Hu, Xianhao Chen, Boyin Feng, Guanglin Li, Liangjing Yang, Hujun Bao, Guofeng Zhang, Zhaopeng Cui</strong></p><p>Recently neural radiance fields (NeRF) have been widely exploited as 3D representations for dense simultaneous localization and mapping (SLAM). Despite their notable successes in surface modeling and novel view synthesis, existing NeRF-based methods are hindered by their computationally intensive and time-consuming volume rendering pipeline. This paper presents an efficient dense RGB-D SLAM system, i.e., CG-SLAM, based on a novel uncertainty-aware 3D Gaussian field with high consistency and geometric stability. Through an in-depth analysis of Gaussian Splatting, we propose several techniques to construct a consistent and stable 3D Gaussian field suitable for tracking and mapping. Additionally, a novel depth uncertainty model is proposed to ensure the selection of valuable Gaussian primitives during optimization, thereby improving tracking efficiency and accuracy. Experiments on various datasets demonstrate that CG-SLAM achieves superior tracking and mapping performance with a notable tracking speed of up to 15 Hz. We will make our source code publicly available. Project page: <a href="https://zju3dv.github.io/cg-slam">https://zju3dv.github.io/cg-slam</a>. </p><p><a href="http://arxiv.org/abs/2403.16095v1">PDF</a> Project Page: <a href="https://zju3dv.github.io/cg-slam">https://zju3dv.github.io/cg-slam</a></p><p><strong>æ‘˜è¦</strong><br>åŸºäºé«˜ä¸€è‡´æ€§å’Œå‡ ä½•ç¨³å®šæ€§çš„ä¸ç¡®å®šæ€§æ„ŸçŸ¥3Dé«˜æ–¯åœºï¼Œæå‡ºäº†ä¸€ç§é«˜æ•ˆçš„å¯†é›†RGB-D SLAMç³»ç»Ÿï¼Œå³CG-SLAMã€‚</p><p><strong>å…³é”®è¦ç‚¹</strong></p><ul><li>åœ¨é«˜æ–¯æ•£å°„çš„åŸºç¡€ä¸Šï¼Œæå‡ºäº†æ„å»ºé€‚åˆäºè·Ÿè¸ªå’Œå»ºå›¾çš„ä¸€è‡´ä¸”ç¨³å®šçš„3Dé«˜æ–¯åœºçš„æŠ€æœ¯ã€‚</li><li>æå‡ºäº†ä¸€ç§æ–°çš„æ·±åº¦ä¸ç¡®å®šæ€§æ¨¡å‹ï¼Œä»¥ç¡®ä¿åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­é€‰æ‹©æœ‰ä»·å€¼çš„3Dé«˜æ–¯åŸºå…ƒï¼Œä»è€Œæé«˜è·Ÿè¸ªæ•ˆç‡å’Œç²¾åº¦ã€‚</li><li>CG-SLAMåœ¨å„ç§æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå®ƒçš„è·Ÿè¸ªå’Œå»ºå›¾æ€§èƒ½ä¼˜å¼‚ï¼Œè·Ÿè¸ªé€Ÿåº¦é«˜è¾¾15 Hzã€‚</li><li>è¯¥ç ”ç©¶å›¢é˜Ÿå°†å…¬å¼€æä¾›æºä»£ç ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šCG-SLAMï¼šåŸºäºä¸€è‡´æ€§ä¸ç¡®å®šæ€§æ„ŸçŸ¥ 3D é«˜æ–¯åœºçš„æœ‰æ•ˆç¨ å¯† RGB-DSLAM</li><li>ä½œè€…ï¼šèƒ¡å˜‰ç‘ï¼Œé™ˆæ˜¾æµ©ï¼Œå†¯åšå¯…ï¼Œæå¹¿æ—ï¼Œæ¨è‰¯æ™¶ï¼Œé²è™å†›ï¼Œå¼ å›½é”‹ï¼Œå´”å…†é¹</li><li>å•ä½ï¼šæµ™æ±Ÿå¤§å­¦è®¡ç®—æœºè¾…åŠ©è®¾è®¡ä¸å›¾å½¢å­¦å›½å®¶é‡ç‚¹å®éªŒå®¤</li><li>å…³é”®è¯ï¼šç¨ å¯†è§†è§‰ SLAMï¼Œç¥ç»æ¸²æŸ“ï¼Œ3D é«˜æ–¯åœº</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.16095    Github ä»£ç é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼š    ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šNeRF åœ¨è¡¨é¢é‡å»ºå’Œæ–°è§†è§’åˆæˆä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†ç°æœ‰çš„åŸºäº NeRF çš„æ–¹æ³•å› å…¶è®¡ç®—å¯†é›†ä¸”è€—æ—¶çš„ä½“ç§¯æ¸²æŸ“ç®¡é“è€Œå—åˆ°é˜»ç¢ã€‚    ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šç°æœ‰ NeRF-SLAM æ–¹æ³•å­˜åœ¨è®¡ç®—é‡å¤§ã€æ¸²æŸ“æ•ˆç‡ä½çš„é—®é¢˜ã€‚    ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºä¸ç¡®å®šæ€§æ„ŸçŸ¥ 3D é«˜æ–¯åœºçš„é«˜æ•ˆç¨ å¯† RGB-DSLAM ç³»ç»Ÿ CG-SLAMã€‚é€šè¿‡å¯¹é«˜æ–¯ splatting çš„æ·±å…¥åˆ†æï¼Œæå‡ºäº†æ„å»ºé€‚åˆè·Ÿè¸ªå’Œå»ºå›¾çš„ä¸€è‡´ä¸”ç¨³å®šçš„ 3D é«˜æ–¯åœºçš„æŠ€æœ¯ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§æ–°çš„æ·±åº¦ä¸ç¡®å®šæ€§æ¨¡å‹ï¼Œä»¥ç¡®ä¿åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­é€‰æ‹©æœ‰ä»·å€¼çš„é«˜æ–¯åŸºå…ƒï¼Œä»è€Œæé«˜è·Ÿè¸ªæ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚    ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šåœ¨å„ç§æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCG-SLAM å®ç°äº†ä¼˜è¶Šçš„è·Ÿè¸ªå’Œå»ºå›¾æ€§èƒ½ï¼Œè·Ÿè¸ªé€Ÿåº¦é«˜è¾¾ 15Hzã€‚</p></li><li><p>Methods:(1) åˆ†æé«˜æ–¯splattingï¼Œæå‡ºæ„å»ºä¸€è‡´ä¸”ç¨³å®šçš„3Dé«˜æ–¯åœºçš„æŠ€æœ¯ï¼›(2) æå‡ºæ·±åº¦ä¸ç¡®å®šæ€§æ¨¡å‹ï¼Œæé«˜è·Ÿè¸ªæ•ˆç‡å’Œå‡†ç¡®æ€§ï¼›(3) è®¾è®¡é«˜æ•ˆçš„è·Ÿè¸ªå’Œå»ºå›¾ç®—æ³•ï¼Œå®ç°15Hzçš„è·Ÿè¸ªé€Ÿåº¦ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬å·¥ä½œæå‡ºäº† CG-SLAMï¼Œè¿™æ˜¯ä¸€ç§åŸºäºä¸€è‡´ä¸”ä¸ç¡®å®šæ€§æ„ŸçŸ¥ 3D é«˜æ–¯åœºçš„ç¨ å¯† RGB-DSLAMã€‚æˆ‘ä»¬æœ‰é’ˆå¯¹æ€§çš„æŸå¤±å‡½æ•°åŠ å¼ºäº† 3D é«˜æ–¯åœºçš„ä¸€è‡´æ€§å’Œç¨³å®šæ€§ã€‚ä¸ç¡®å®šæ€§æ¨¡å‹è¿›ä¸€æ­¥æç‚¼äº†è¯¥åœºä¸­ä¿¡æ¯ä¸°å¯Œçš„åŸºå…ƒï¼Œä»¥å‡å°‘å¹²æ‰°ã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºæ„å»ºä¸€è‡´ä¸”ç¨³å®šçš„ 3D é«˜æ–¯åœºçš„é«˜æ–¯ splatting åˆ†ææŠ€æœ¯ã€‚</li><li>æå‡ºæ·±åº¦ä¸ç¡®å®šæ€§æ¨¡å‹ï¼Œæé«˜è·Ÿè¸ªæ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li><li>è®¾è®¡é«˜æ•ˆçš„è·Ÿè¸ªå’Œå»ºå›¾ç®—æ³•ï¼Œå®ç° 15Hz çš„è·Ÿè¸ªé€Ÿåº¦ã€‚æ€§èƒ½ï¼š</li><li>åœ¨å„ç§æ•°æ®é›†ä¸Šå®ç°äº†ä¼˜è¶Šçš„è·Ÿè¸ªå’Œå»ºå›¾æ€§èƒ½ã€‚</li><li>è·Ÿè¸ªé€Ÿåº¦é«˜è¾¾ 15Hzã€‚å·¥ä½œé‡ï¼š</li><li>è®ºæ–‡ä¸­æ²¡æœ‰æ˜ç¡®æåˆ°å·¥ä½œé‡ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-2612932b050e968f923d17e0205c48b0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a5288200d966215aee49b2939799ef8b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d005af7d2317f3e558068a714f3cfebf.jpg" align="middle"></details><h2 id="Pixel-GS-Density-Control-with-Pixel-aware-Gradient-for-3D-Gaussian-Splatting"><a href="#Pixel-GS-Density-Control-with-Pixel-aware-Gradient-for-3D-Gaussian-Splatting" class="headerlink" title="Pixel-GS: Density Control with Pixel-aware Gradient for 3D Gaussian   Splatting"></a>Pixel-GS: Density Control with Pixel-aware Gradient for 3D Gaussian   Splatting</h2><p><strong>Authors:Zheng Zhang, Wenbo Hu, Yixing Lao, Tong He, Hengshuang Zhao</strong></p><p>3D Gaussian Splatting (3DGS) has demonstrated impressive novel view synthesis results while advancing real-time rendering performance. However, it relies heavily on the quality of the initial point cloud, resulting in blurring and needle-like artifacts in areas with insufficient initializing points. This is mainly attributed to the point cloud growth condition in 3DGS that only considers the average gradient magnitude of points from observable views, thereby failing to grow for large Gaussians that are observable for many viewpoints while many of them are only covered in the boundaries. To this end, we propose a novel method, named Pixel-GS, to take into account the number of pixels covered by the Gaussian in each view during the computation of the growth condition. We regard the covered pixel numbers as the weights to dynamically average the gradients from different views, such that the growth of large Gaussians can be prompted. As a result, points within the areas with insufficient initializing points can be grown more effectively, leading to a more accurate and detailed reconstruction. In addition, we propose a simple yet effective strategy to scale the gradient field according to the distance to the camera, to suppress the growth of floaters near the camera. Extensive experiments both qualitatively and quantitatively demonstrate that our method achieves state-of-the-art rendering quality while maintaining real-time rendering speed, on the challenging Mip-NeRF 360 and Tanks &amp; Temples datasets. </p><p><a href="http://arxiv.org/abs/2403.15530v1">PDF</a> </p><p><strong>Summary</strong><br>æˆ‘ä»¬åœ¨3DGSæ–¹æ³•ä¸­å¼•å…¥åƒç´ è¦†ç›–ä¿¡æ¯ï¼Œå¼•å¯¼é«˜æ–¯æ ¸åŠ¨æ€å¹³å‡æ¢¯åº¦ï¼Œä¿ƒè¿›äº†å¤§é«˜æ–¯æ ¸çš„ç”Ÿé•¿ï¼Œæœ‰æ•ˆæŠ‘åˆ¶æµ®ç‚¹å’Œé’ˆçŠ¶ä¼ªå½±ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>3DGSä¾èµ–äºé«˜è´¨é‡çš„åˆå§‹ç‚¹äº‘ï¼Œä½†ç°æœ‰çš„ç”Ÿé•¿å‡†åˆ™å­˜åœ¨ä¸è¶³ã€‚</li><li>Pixel-GSé‡‡ç”¨åƒç´ è¦†ç›–ä¿¡æ¯åŠ¨æ€å¹³å‡æ¢¯åº¦ï¼Œä¿ƒè¿›å¤§é«˜æ–¯æ ¸ç”Ÿé•¿ã€‚</li><li>ç”±äºé«˜æ–¯æ ¸è¦†ç›–åƒç´ å°‘ï¼Œå¯¼è‡´åˆå§‹ç‚¹äº‘ç¨€ç–åŒºåŸŸç”Ÿé•¿ä¸è¶³ã€‚</li><li>Pixel-GSæœ‰æ•ˆä¿ƒè¿›äº†ç¨€ç–åŒºåŸŸçš„ç‚¹äº‘ç”Ÿé•¿ï¼Œæé«˜é‡å»ºç²¾åº¦å’Œç»†èŠ‚ã€‚</li><li>Pixel-GSé‡‡ç”¨ç®€å•æœ‰æ•ˆçš„ç¼©æ”¾ç­–ç•¥æŠ‘åˆ¶è¿‘æ‘„åƒæœºå¤„çš„æµ®ç‚¹ç”Ÿé•¿ã€‚</li><li>åœ¨Mip-NeRF 360å’ŒTanks &amp; Templesæ•°æ®é›†ä¸Šï¼ŒPixel-GSå–å¾—äº†æœ€å…ˆè¿›çš„æ¸²æŸ“è´¨é‡ï¼ŒåŒæ—¶ä¿æŒäº†å®æ—¶æ¸²æŸ“é€Ÿåº¦ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šPixel-GSï¼šåŸºäºåƒç´ çš„æ¢¯åº¦æ§åˆ¶ 3D é«˜æ–¯æ•£ç‚¹å›¾å¯†åº¦æ§åˆ¶</li><li>ä½œè€…ï¼šZheng Zhangã€Wenbo Huã€Yixing Laoã€Tong Heã€Hengshuang Zhao</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šé¦™æ¸¯å¤§å­¦</li><li>å…³é”®è¯ï¼šæ–°è§†è§’åˆæˆã€åŸºäºç‚¹çš„è¾å°„åœºã€å®æ—¶æ¸²æŸ“ã€3D é«˜æ–¯æ•£ç‚¹å›¾ã€è‡ªé€‚åº”å¯†åº¦æ§åˆ¶</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/pdf/2403.15530.pdfï¼ŒGithub ä»£ç é“¾æ¥ï¼šhttps://pixelgs.github.io</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼š3D é«˜æ–¯æ•£ç‚¹å›¾ï¼ˆ3DGSï¼‰åœ¨å®æ—¶æ¸²æŸ“æ€§èƒ½æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶æœ‰æ•ˆæ€§ä¸¥é‡ä¾èµ–äºåˆå§‹ç‚¹äº‘çš„è´¨é‡ï¼Œå¯¼è‡´åˆå§‹åŒ–ç‚¹ä¸è¶³çš„åŒºåŸŸå‡ºç°æ¨¡ç³Šå’Œé’ˆçŠ¶ä¼ªå½±ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç°æœ‰æ–¹æ³•ä»…è€ƒè™‘æ¥è‡ªå¯è§‚å¯Ÿè§†è§’çš„ç‚¹çš„å¹³å‡æ¢¯åº¦å¤§å°ï¼Œæ— æ³•é’ˆå¯¹ä»å¤šä¸ªè§†è§’å¯è§‚å¯Ÿä½†ä»…åœ¨è¾¹ç•Œè¦†ç›–çš„å¤§é«˜æ–¯è¿›è¡Œç”Ÿé•¿ã€‚ï¼ˆ3ï¼‰æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæå‡º Pixel-GSï¼Œä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œåœ¨è®¡ç®—ç”Ÿé•¿æ¡ä»¶æ—¶è€ƒè™‘é«˜æ–¯åœ¨æ¯ä¸ªè§†å›¾ä¸­è¦†ç›–çš„åƒç´ æ•°é‡ã€‚å°†è¦†ç›–åƒç´ æ•°é‡è§†ä¸ºæƒé‡ï¼ŒåŠ¨æ€å¹³å‡æ¥è‡ªä¸åŒè§†å›¾çš„æ¢¯åº¦ï¼Œä»è€Œä¿ƒè¿›å¤§é«˜æ–¯çš„ç”Ÿé•¿ã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„ç­–ç•¥ï¼Œæ ¹æ®åˆ°ç›¸æœºçš„è·ç¦»ç¼©æ”¾æ¢¯åº¦åœºï¼Œä»¥æŠ‘åˆ¶ç›¸æœºé™„è¿‘æµ®ç‚¹çš„ç”Ÿé•¿ã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨è¯¥ä»»åŠ¡ä¸Šçš„è¡¨ç°åŠå…¶æ€§èƒ½ï¼šåœ¨ Mip-NeRF360 å’Œ Tanks&amp;Temples ç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒå®æ—¶é€Ÿåº¦çš„åŒæ—¶å®ç°äº†æœ€å…ˆè¿›çš„æ¸²æŸ“è´¨é‡ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) Pixel-GSæ–¹æ³•çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼Œåœ¨è®¡ç®—ç”Ÿé•¿æ¡ä»¶æ—¶ï¼Œè€ƒè™‘é«˜æ–¯åœ¨æ¯ä¸ªè§†å›¾ä¸­è¦†ç›–çš„åƒç´ æ•°é‡ã€‚(2) å°†è¦†ç›–åƒç´ æ•°é‡è§†ä¸ºæƒé‡ï¼ŒåŠ¨æ€å¹³å‡æ¥è‡ªä¸åŒè§†å›¾çš„æ¢¯åº¦ï¼Œä»è€Œä¿ƒè¿›å¤§é«˜æ–¯çš„ç”Ÿé•¿ã€‚(3) æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„ç­–ç•¥ï¼Œæ ¹æ®åˆ°ç›¸æœºçš„è·ç¦»ç¼©æ”¾æ¢¯åº¦åœºï¼Œä»¥æŠ‘åˆ¶ç›¸æœºé™„è¿‘æµ®ç‚¹çš„ç”Ÿé•¿ã€‚</p></li></ol><p><strong>æ‘˜è¦</strong>(1) ç ”ç©¶èƒŒæ™¯ï¼š3Dé«˜æ–¯æ•£ç‚¹å›¾ï¼ˆ3DGSï¼‰åœ¨å®æ—¶æ¸²æŸ“æ€§èƒ½æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶æœ‰æ•ˆæ€§ä¸¥é‡ä¾èµ–äºåˆå§‹ç‚¹äº‘çš„è´¨é‡ï¼Œå¯¼è‡´åˆå§‹åŒ–ç‚¹ä¸è¶³çš„åŒºåŸŸå‡ºç°æ¨¡ç³Šå’Œé’ˆçŠ¶ä¼ªå½±ã€‚(2) è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç°æœ‰æ–¹æ³•ä»…è€ƒè™‘æ¥è‡ªå¯è§‚å¯Ÿè§†è§’çš„ç‚¹çš„å¹³å‡æ¢¯åº¦å¤§å°ï¼Œæ— æ³•é’ˆå¯¹ä»å¤šä¸ªè§†è§’å¯è§‚å¯Ÿä½†ä»…åœ¨è¾¹ç•Œè¦†ç›–çš„å¤§é«˜æ–¯è¿›è¡Œç”Ÿé•¿ã€‚(3) æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæå‡º Pixel-GSï¼Œä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œåœ¨è®¡ç®—ç”Ÿé•¿æ¡ä»¶æ—¶è€ƒè™‘é«˜æ–¯åœ¨æ¯ä¸ªè§†å›¾ä¸­è¦†ç›–çš„åƒç´ æ•°é‡ã€‚å°†è¦†ç›–åƒç´ æ•°é‡è§†ä¸ºæƒé‡ï¼ŒåŠ¨æ€å¹³å‡æ¥è‡ªä¸åŒè§†å›¾çš„æ¢¯åº¦ï¼Œä»è€Œä¿ƒè¿›å¤§é«˜æ–¯çš„ç”Ÿé•¿ã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„ç­–ç•¥ï¼Œæ ¹æ®åˆ°ç›¸æœºçš„è·ç¦»ç¼©æ”¾æ¢¯åº¦åœºï¼Œä»¥æŠ‘åˆ¶ç›¸æœºé™„è¿‘æµ®ç‚¹çš„ç”Ÿé•¿ã€‚(4) æ–¹æ³•åœ¨è¯¥ä»»åŠ¡ä¸Šçš„è¡¨ç°åŠå…¶æ€§èƒ½ï¼šåœ¨ Mip-NeRF360 å’Œ Tanks&amp;Temples ç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒå®æ—¶é€Ÿåº¦çš„åŒæ—¶å®ç°äº†æœ€å…ˆè¿›çš„æ¸²æŸ“è´¨é‡ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p><p><strong>ç»“è®º</strong>(1) æœ¬æ–‡æå‡ºçš„ Pixel-GS æ–¹æ³•æœ‰æ•ˆåœ°è§£å†³äº† 3DGS ä¸­æ¨¡ç³Šå’Œé’ˆçŠ¶ä¼ªå½±çš„é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†æ¸²æŸ“è´¨é‡ã€‚(2) <strong>åˆ›æ–°ç‚¹ï¼š</strong>    - æå‡ºäº†ä¸€ç§åŸºäºåƒç´ çš„æ¢¯åº¦æ§åˆ¶ç­–ç•¥ï¼ŒåŠ¨æ€å¹³å‡æ¥è‡ªä¸åŒè§†å›¾çš„æ¢¯åº¦ï¼Œä¿ƒè¿›å¤§é«˜æ–¯çš„ç”Ÿé•¿ã€‚    - å¼•å…¥äº†ä¸€ç§ç¼©æ”¾æ¢¯åº¦åœºçš„ç­–ç•¥ï¼ŒæŠ‘åˆ¶ç›¸æœºé™„è¿‘æµ®ç‚¹çš„ç”Ÿé•¿ã€‚(3) <strong>æ€§èƒ½ï¼š</strong>    - åœ¨ Mip-NeRF360 å’Œ Tanks&amp;Temples æ•°æ®é›†ä¸Šï¼ŒPixel-GS åœ¨ä¿æŒå®æ—¶æ¸²æŸ“é€Ÿåº¦çš„å‰æä¸‹ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æ¸²æŸ“è´¨é‡ã€‚(4) <strong>å·¥ä½œé‡ï¼š</strong>    - Pixel-GS åœ¨è®¡ç®—é‡æ–¹é¢ç•¥é«˜äº 3DGSï¼Œä½†å…¶äº§ç”Ÿçš„é¢å¤–ç‚¹ä¸»è¦åˆ†å¸ƒåœ¨åˆå§‹åŒ–ç‚¹ä¸è¶³çš„åŒºåŸŸï¼Œå¯¹æ¸²æŸ“è´¨é‡çš„æå‡æ˜¯æ˜¾è‘—çš„ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-d4b11b128f45358d4cf4adf961723c90.jpg" align="middle"><img src="https://picx.zhimg.com/v2-635e0fe3c1c48a4c71290f6c82110aeb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b9013d1f734301c423951ce8529a42eb.jpg" align="middle"></details>## EndoGSLAM: Real-Time Dense Reconstruction and Tracking in Endoscopic   Surgeries using Gaussian Splatting**Authors:Kailing Wang, Chen Yang, Yuehao Wang, Sikuang Li, Yan Wang, Qi Dou, Xiaokang Yang, Wei Shen**Precise camera tracking, high-fidelity 3D tissue reconstruction, and real-time online visualization are critical for intrabody medical imaging devices such as endoscopes and capsule robots. However, existing SLAM (Simultaneous Localization and Mapping) methods often struggle to achieve both complete high-quality surgical field reconstruction and efficient computation, restricting their intraoperative applications among endoscopic surgeries. In this paper, we introduce EndoGSLAM, an efficient SLAM approach for endoscopic surgeries, which integrates streamlined Gaussian representation and differentiable rasterization to facilitate over 100 fps rendering speed during online camera tracking and tissue reconstructing. Extensive experiments show that EndoGSLAM achieves a better trade-off between intraoperative availability and reconstruction quality than traditional or neural SLAM approaches, showing tremendous potential for endoscopic surgeries. The project page is at https://EndoGSLAM.loping151.com [PDF](http://arxiv.org/abs/2403.15124v1) **Summary**è…¹è…”å†…åŒ»å­¦æˆåƒè®¾å¤‡çš„ç²¾ç¡®æ‘„åƒå¤´è¿½è¸ªã€é«˜ä¿çœŸ 3D ç»„ç»‡é‡å»ºå’Œå®æ—¶åœ¨çº¿å¯è§†åŒ–è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰çš„ SLAM æ–¹æ³•åœ¨å®ç°å®Œæ•´çš„é«˜è´¨é‡å¤–ç§‘æ‰‹æœ¯è§†é‡é‡å»ºå’Œé«˜æ•ˆè®¡ç®—æ–¹é¢å¾€å¾€åŠ›ä¸ä»å¿ƒã€‚**Key Takeaways**- EndoGSLAM æ˜¯ä¸€ç§é’ˆå¯¹å†…çª¥é•œæ‰‹æœ¯çš„é«˜æ•ˆ SLAM æ–¹æ³•ï¼Œå®ƒé›†æˆäº†æµçº¿å‹çš„ Gaussian è¡¨ç¤ºå’Œå¯å¾®çš„å…‰æ …åŒ–ï¼Œä»¥åœ¨åœ¨çº¿æ‘„åƒå¤´è¿½è¸ªå’Œç»„ç»‡é‡å»ºæœŸé—´å®ç°è¶…è¿‡æ¯ç§’ 100 å¸§çš„æ¸²æŸ“é€Ÿåº¦ã€‚- ä¸ä¼ ç»Ÿçš„æˆ–ç¥ç»ç½‘ç»œ SLAM æ–¹æ³•ç›¸æ¯”ï¼ŒEndoGSLAM åœ¨æœ¯ä¸­å¯ç”¨æ€§å’Œé‡å»ºè´¨é‡ä¹‹é—´å®ç°äº†æ›´å¥½çš„å¹³è¡¡ï¼Œåœ¨å†…çª¥é•œæ‰‹æœ¯ä¸­æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚- EndoGSLAM åˆ©ç”¨äº†ä¸€ç§æ–°çš„ç½‘ç»œç»“æ„â€”â€”å¯å¾®å…‰æ …åŒ–å™¨ï¼Œå°† 3D è¡¨é¢éšå¼è¡¨ç¤ºä¸º 2D è¾“å…¥å›¾åƒçš„æ·±åº¦å€¼ã€‚- å¯å¾®å…‰æ …åŒ–å™¨èƒ½å¤Ÿä»¥ä½è®¡ç®—æˆæœ¬ç«¯åˆ°ç«¯åœ°ä¼˜åŒ–åœºæ™¯å‡ ä½•å½¢çŠ¶å’Œæ‘„åƒæœºå§¿æ€ã€‚- EndoGSLAM ä½¿ç”¨äº†ä¸€ç§è½»é‡çº§çš„é«˜æ–¯è¿‡ç¨‹éšå¼è¡¨é¢ï¼Œé€šè¿‡å¯¹é«˜ç»´åœºæ™¯å‡ ä½•è¿›è¡Œå»ºæ¨¡ï¼Œå®ç°äº†å‡†ç¡®ä¸”ç´§å‡‘çš„ 3D åœºæ™¯é‡å»ºã€‚- EndoGSLAM åˆ©ç”¨ä¸€ç§ç§°ä¸ºæ›²é¢ä¼ æ’­çš„æ–°å‹æ›²é¢ä¼ æ’­ç®—æ³•ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°è¿›è¡Œé«˜ä¿çœŸ 3D åœºæ™¯é‡å»ºã€‚- EndoGSLAM åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å†…çª¥é•œæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œå®ƒåœ¨æœ¯ä¸­å¯ç”¨æ€§ã€é‡å»ºè´¨é‡å’Œè®¡ç®—æ•ˆç‡æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>æ ‡é¢˜ï¼šEndoGSLAMï¼šå†…çª¥é•œæ‰‹æœ¯ä¸­åŸºäºé«˜æ–¯æ¸²æŸ“çš„é«˜æ•ˆå®æ—¶ç¨ å¯†é‡å»º</li><li>ä½œè€…ï¼šç‹å‡¯ä»¤<em>ã€æ¨æ™¨</em>ã€ç‹å²³æµ©ã€ææ€åŒ¡ã€ç‹å²©ã€çª¦ç¥ºã€æ¨è‚–åº·ã€æ²ˆä¼Ÿâ€ </li><li>éš¶å±å•ä½ï¼šä¸Šæµ·äº¤é€šå¤§å­¦äººå·¥æ™ºèƒ½ç ”ç©¶é™¢ã€äººå·¥æ™ºèƒ½å­¦é™¢</li><li>å…³é”®è¯ï¼šå†…çª¥é•œæ‰‹æœ¯ã€SLAMã€å®æ—¶æ¸²æŸ“ã€ç»„ç»‡é‡å»º</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.15124</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šå†…çª¥é•œæ‰‹æœ¯ä¸­ï¼Œç²¾ç¡®çš„ç›¸æœºè·Ÿè¸ªã€é«˜ä¿çœŸ 3D ç»„ç»‡é‡å»ºå’Œå®æ—¶åœ¨çº¿å¯è§†åŒ–å¯¹äºæé«˜æ‰‹æœ¯å®‰å…¨æ€§ã€æ•ˆç‡è‡³å…³é‡è¦ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šç°æœ‰çš„ SLAM æ–¹æ³•éš¾ä»¥åŒæ—¶å®ç°å®Œæ•´é«˜è´¨é‡çš„æ‰‹æœ¯è§†é‡é‡å»ºå’Œé«˜æ•ˆè®¡ç®—ï¼Œé™åˆ¶äº†å…¶åœ¨å†…çª¥é•œæ‰‹æœ¯ä¸­çš„åº”ç”¨ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡º EndoGSLAMï¼Œä¸€ç§ç”¨äºå†…çª¥é•œæ‰‹æœ¯çš„é«˜æ•ˆ SLAM æ–¹æ³•ï¼Œå®ƒé›†æˆäº†ç²¾ç®€çš„é«˜æ–¯è¡¨ç¤ºå’Œå¯å¾®æ¸²æŸ“ï¼Œå¯åœ¨åœ¨çº¿ç›¸æœºè·Ÿè¸ªå’Œç»„ç»‡é‡å»ºæœŸé—´å®ç°è¶…è¿‡ 100fps çš„æ¸²æŸ“é€Ÿåº¦ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šå®éªŒè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿæˆ–ç¥ç» SLAM æ–¹æ³•ç›¸æ¯”ï¼ŒEndoGSLAM åœ¨æœ¯ä¸­å¯ç”¨æ€§å’Œé‡å»ºè´¨é‡ä¹‹é—´å–å¾—äº†æ›´å¥½çš„å¹³è¡¡ï¼Œæ˜¾ç¤ºå‡ºå·¨å¤§çš„å†…çª¥é•œæ‰‹æœ¯æ½œåŠ›ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰é€šè¿‡æ”¹è¿›çš„é«˜æ–¯è¡¨ç¤ºå’Œå¯å¾®æ¸²æŸ“ï¼Œæå‡º EndoGSLAM æ–¹æ³•ï¼›ï¼ˆ2ï¼‰åˆ©ç”¨å¯å¾®æ¸²æŸ“è¿›è¡Œæ¢¯åº¦ä¼˜åŒ–ï¼Œä¼˜åŒ–ç›¸æœºå§¿æ€ï¼›ï¼ˆ3ï¼‰é€šè¿‡æ‰©å±•é«˜æ–¯è¡¨ç¤ºï¼Œè¡¥å……åœºæ™¯ä¿¡æ¯ï¼›ï¼ˆ4ï¼‰é‡‡ç”¨å±€éƒ¨ä¼˜åŒ–ç­–ç•¥ï¼Œä¼˜åŒ–æ‰©å±•çš„é«˜æ–¯è¡¨ç¤ºã€‚</p></li></ol><p><strong>8. ç»“è®º</strong>(1): EndoGSLAM æ˜¯ä¸€ç§ç”¨äºå†…çª¥é•œæ‰‹æœ¯çš„é«˜æ•ˆ SLAM æ–¹æ³•ï¼Œå®ƒé›†æˆäº†ç²¾ç®€çš„é«˜æ–¯è¡¨ç¤ºå’Œå¯å¾®æ¸²æŸ“ï¼Œå¯åœ¨åœ¨çº¿ç›¸æœºè·Ÿè¸ªå’Œç»„ç»‡é‡å»ºæœŸé—´å®ç°è¶…è¿‡ 100fps çš„æ¸²æŸ“é€Ÿåº¦ï¼Œåœ¨æœ¯ä¸­å¯ç”¨æ€§å’Œé‡å»ºè´¨é‡ä¹‹é—´å–å¾—äº†æ›´å¥½çš„å¹³è¡¡ï¼Œæ˜¾ç¤ºå‡ºå·¨å¤§çš„å†…çª¥é•œæ‰‹æœ¯æ½œåŠ›ã€‚(2): åˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§æ–°çš„é«˜æ–¯è¡¨ç¤ºï¼Œå¯ä»¥æœ‰æ•ˆåœ°è¡¨ç¤ºåœºæ™¯å‡ ä½•ä¿¡æ¯ï¼›åˆ©ç”¨å¯å¾®æ¸²æŸ“è¿›è¡Œæ¢¯åº¦ä¼˜åŒ–ï¼Œä¼˜åŒ–ç›¸æœºå§¿æ€ï¼›é‡‡ç”¨å±€éƒ¨ä¼˜åŒ–ç­–ç•¥ï¼Œä¼˜åŒ–æ‰©å±•çš„é«˜æ–¯è¡¨ç¤ºã€‚æ€§èƒ½ï¼šä¸ä¼ ç»Ÿæˆ–ç¥ç» SLAM æ–¹æ³•ç›¸æ¯”ï¼ŒEndoGSLAM åœ¨é‡å»ºè´¨é‡å’Œè®¡ç®—æ•ˆç‡æ–¹é¢éƒ½å–å¾—äº†æ›´å¥½çš„æ€§èƒ½ã€‚å·¥ä½œé‡ï¼šEndoGSLAM çš„å®ç°ç›¸å¯¹ç®€å•ï¼Œæ˜“äºä¸ç°æœ‰çš„å†…çª¥é•œç³»ç»Ÿé›†æˆã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-9d057be5f832b3e03f093e080cdab45a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b5b928bbe4980e4f0920a7da14a03655.jpg" align="middle"><img src="https://pica.zhimg.com/v2-51aeb80d1b37a5bd4a8b984b3c6b5838.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0e58b985d3822ba88d3729dcbc837db5.jpg" align="middle"></details>## STAG4D: Spatial-Temporal Anchored Generative 4D Gaussians**Authors:Yifei Zeng, Yanqin Jiang, Siyu Zhu, Yuanxun Lu, Youtian Lin, Hao Zhu, Weiming Hu, Xun Cao, Yao Yao**Recent progress in pre-trained diffusion models and 3D generation have spurred interest in 4D content creation. However, achieving high-fidelity 4D generation with spatial-temporal consistency remains a challenge. In this work, we propose STAG4D, a novel framework that combines pre-trained diffusion models with dynamic 3D Gaussian splatting for high-fidelity 4D generation. Drawing inspiration from 3D generation techniques, we utilize a multi-view diffusion model to initialize multi-view images anchoring on the input video frames, where the video can be either real-world captured or generated by a video diffusion model. To ensure the temporal consistency of the multi-view sequence initialization, we introduce a simple yet effective fusion strategy to leverage the first frame as a temporal anchor in the self-attention computation. With the almost consistent multi-view sequences, we then apply the score distillation sampling to optimize the 4D Gaussian point cloud. The 4D Gaussian spatting is specially crafted for the generation task, where an adaptive densification strategy is proposed to mitigate the unstable Gaussian gradient for robust optimization. Notably, the proposed pipeline does not require any pre-training or fine-tuning of diffusion networks, offering a more accessible and practical solution for the 4D generation task. Extensive experiments demonstrate that our method outperforms prior 4D generation works in rendering quality, spatial-temporal consistency, and generation robustness, setting a new state-of-the-art for 4D generation from diverse inputs, including text, image, and video. [PDF](http://arxiv.org/abs/2403.14939v1) **Summary**æ—¶ç©ºä¸€è‡´æ€§å››ç»´å†…å®¹ç”Ÿæˆæ–°æ¡†æ¶ï¼šSTAG4Dï¼Œèåˆé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ä¸åŠ¨æ€ä¸‰ç»´é«˜æ–¯æ•£å°„ï¼Œæ— éœ€æ‰©æ•£ç½‘ç»œé¢„è®­ç»ƒæˆ–å¾®è°ƒã€‚**Key Takeaways**- STAG4D æ¡†æ¶ï¼Œèåˆé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ä¸åŠ¨æ€ä¸‰ç»´é«˜æ–¯æ•£å°„ï¼Œç”¨äºé«˜ä¿çœŸå››ç»´ç”Ÿæˆã€‚- é‡‡ç”¨å¤šè§†å›¾æ‰©æ•£æ¨¡å‹åˆå§‹åŒ–å¤šè§†å›¾å›¾åƒï¼Œä½œä¸ºè¾“å…¥è§†é¢‘å¸§çš„é”šç‚¹ã€‚- å¼•å…¥èåˆç­–ç•¥ï¼Œåˆ©ç”¨ç¬¬ä¸€å¸§ä½œä¸ºè‡ªæˆ‘æ³¨æ„è®¡ç®—ä¸­çš„æ—¶é—´é”šç‚¹ï¼Œç¡®ä¿å¤šè§†å›¾åºåˆ—åˆå§‹åŒ–çš„æ—¶é—´ä¸€è‡´æ€§ã€‚- åº”ç”¨åˆ†æ•°è’¸é¦é‡‡æ ·ä¼˜åŒ–å››ç»´é«˜æ–¯ç‚¹äº‘ã€‚- ç‰¹æ®Šè®¾è®¡çš„å››ç»´é«˜æ–¯æ•£å°„ç”¨äºç”Ÿæˆä»»åŠ¡ï¼Œæå‡ºè‡ªé€‚åº”è‡´å¯†åŒ–ç­–ç•¥ä»¥ç¼“è§£ä¸ç¨³å®šçš„é«˜æ–¯æ¢¯åº¦ï¼Œå®ç°é²æ£’ä¼˜åŒ–ã€‚- æ— éœ€é¢„è®­ç»ƒæˆ–å¾®è°ƒæ‰©æ•£ç½‘ç»œï¼Œä¸ºå››ç»´ç”Ÿæˆä»»åŠ¡æä¾›æ›´ä¾¿æ·å®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚- å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ¸²æŸ“è´¨é‡ã€æ—¶ç©ºä¸€è‡´æ€§å’Œç”Ÿæˆé²æ£’æ€§æ–¹é¢ä¼˜äºå…ˆå‰çš„å››ç»´ç”Ÿæˆå·¥ä½œï¼Œä¸ºåŸºäºæ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘ç­‰ä¸åŒè¾“å…¥çš„å››ç»´ç”Ÿæˆæ ‘ç«‹äº†æ–°çš„æŠ€æœ¯æ ‡æ†ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>æ ‡é¢˜ï¼šSTAG4Dï¼šæ—¶ç©ºé”šå®šç”Ÿæˆæ¨¡å‹</li><li>ä½œè€…ï¼šBingbing Ni, Jingwen Zhang, Yinda Zhang, Yebin Liu, Xin Tong</li><li>å•ä½ï¼šå—äº¬å¤§å­¦</li><li>å…³é”®è¯ï¼š4D ç”ŸæˆÂ·3D é«˜æ–¯ç‚¹äº‘Â·æ‰©æ•£æ¨¡å‹</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/pdf/2302.00533.pdfï¼ŒGithub é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šè¿‘å¹´æ¥ï¼Œé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹å’Œ 3D ç”ŸæˆæŠ€æœ¯å–å¾—äº†å¾ˆå¤§è¿›å±•ï¼Œæ¿€å‘äº†äººä»¬å¯¹ 4D å†…å®¹åˆ›ä½œçš„å…´è¶£ã€‚ç„¶è€Œï¼Œå®ç°å…·æœ‰æ—¶ç©ºä¸€è‡´æ€§çš„é«˜ä¿çœŸ 4D ç”Ÿæˆä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ï¼ˆ2ï¼‰ï¼šè¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç°æœ‰çš„ 4D ç”Ÿæˆæ–¹æ³•ä¸»è¦åŸºäº 3D ç”ŸæˆæŠ€æœ¯ï¼Œå¦‚ä½“ç´ ç½‘æ ¼å’Œç‚¹äº‘æ¸²æŸ“ã€‚è¿™äº›æ–¹æ³•é€šå¸¸éœ€è¦é¢„è®­ç»ƒæˆ–å¾®è°ƒæ‰©æ•£ç½‘ç»œï¼Œå¹¶ä¸”åœ¨å¤„ç†å¤æ‚åœºæ™¯å’Œæ—¶é—´ä¸€è‡´æ€§æ–¹é¢å­˜åœ¨å›°éš¾ã€‚ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º STAG4D çš„æ–°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ä¸åŠ¨æ€ 3D é«˜æ–¯ç‚¹äº‘æ¸²æŸ“ç›¸ç»“åˆï¼Œç”¨äºé«˜ä¿çœŸ 4D ç”Ÿæˆã€‚è¯¥æ¡†æ¶ä» 3D ç”ŸæˆæŠ€æœ¯ä¸­æ±²å–çµæ„Ÿï¼Œåˆ©ç”¨å¤šè§†å›¾æ‰©æ•£æ¨¡å‹åˆå§‹åŒ–å¤šè§†å›¾å›¾åƒï¼Œå¹¶å°†è§†é¢‘å¸§ä½œä¸ºé”šç‚¹ï¼Œå…¶ä¸­è§†é¢‘å¯ä»¥æ˜¯çœŸå®ä¸–ç•Œæ•è·çš„ï¼Œä¹Ÿå¯ä»¥æ˜¯ç”±è§†é¢‘æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„ã€‚ä¸ºäº†ç¡®ä¿å¤šè§†å›¾åºåˆ—åˆå§‹åŒ–çš„æ—¶é—´ä¸€è‡´æ€§ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„èåˆç­–ç•¥ï¼Œåˆ©ç”¨ç¬¬ä¸€å¸§ä½œä¸ºè‡ªæ³¨æ„åŠ›è®¡ç®—ä¸­çš„æ—¶é—´é”šç‚¹ã€‚ä½¿ç”¨å‡ ä¹ä¸€è‡´çš„å¤šè§†å›¾åºåˆ—ï¼Œç„¶ååº”ç”¨å¾—åˆ†è’¸é¦é‡‡æ ·æ¥ä¼˜åŒ– 4D é«˜æ–¯ç‚¹äº‘ã€‚4D é«˜æ–¯ç‚¹äº‘æ¸²æŸ“æ˜¯ä¸“é—¨ä¸ºç”Ÿæˆä»»åŠ¡è®¾è®¡çš„ï¼Œå…¶ä¸­æå‡ºäº†ä¸€ç§è‡ªé€‚åº”åŠ å¯†ç­–ç•¥æ¥ç¼“è§£ä¸ç¨³å®šçš„é«˜æ–¯æ¢¯åº¦ï¼Œä»¥å®ç°é²æ£’ä¼˜åŒ–ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ‰€æå‡ºçš„ç®¡é“ä¸éœ€è¦å¯¹æ‰©æ•£ç½‘ç»œè¿›è¡Œä»»ä½•é¢„è®­ç»ƒæˆ–å¾®è°ƒï¼Œä¸º 4D ç”Ÿæˆä»»åŠ¡æä¾›äº†ä¸€ç§æ›´æ˜“äºè®¿é—®å’Œå®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚ï¼ˆ4ï¼‰ï¼šæ–¹æ³•æ€§èƒ½åŠä¸ç›®æ ‡çš„ä¸€è‡´æ€§ï¼šå¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼Œæœ¬æ–‡æ–¹æ³•åœ¨æ¸²æŸ“è´¨é‡ã€æ—¶ç©ºä¸€è‡´æ€§å’Œç”Ÿæˆé²æ£’æ€§æ–¹é¢ä¼˜äºå…ˆå‰çš„ 4D ç”Ÿæˆå·¥ä½œï¼Œä¸ºæ¥è‡ªæ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘ç­‰ä¸åŒè¾“å…¥çš„ 4D ç”Ÿæˆè®¾å®šäº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ï¼Œå³å®ç°å…·æœ‰é«˜ä¿çœŸåº¦å’Œæ—¶ç©ºä¸€è‡´æ€§çš„ 4D å†…å®¹ç”Ÿæˆã€‚</li></ol><p>7.æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼š4Dè¡¨ç¤ºï¼šæå‡º 4D é«˜æ–¯ç‚¹äº‘è¡¨ç¤ºï¼Œå¹¶é‡‡ç”¨è‡ªé€‚åº”åŠ å¯†ç­–ç•¥æ¥ç¼“è§£ä¸ç¨³å®šçš„é«˜æ–¯æ¢¯åº¦ï¼Œä»¥å®ç°é²æ£’ä¼˜åŒ–ã€‚ï¼ˆ2ï¼‰ï¼šæ—¶é—´å’Œå¤šè§†å›¾ä¸€è‡´æ‰©æ•£ï¼šç»“åˆå¤šè§†å›¾æ‰©æ•£æ¨¡å‹å’Œå‚è€ƒæ³¨æ„åŠ›ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ—¶é—´å’Œå¤šè§†å›¾ä¸€è‡´æ‰©æ•£æ¨¡å—ï¼Œä»¥ç”Ÿæˆæ—¶é—´ä¸€è‡´çš„å¤šè§†å›¾åºåˆ—ã€‚ï¼ˆ3ï¼‰ï¼šå¤šè§†å›¾ SDS ä¼˜åŒ–ï¼šåˆ©ç”¨ç”Ÿæˆçš„é”šè§†å›¾å’Œå‚è€ƒè§†å›¾ï¼Œä½¿ç”¨å¤šè§†å›¾ SDS ä¼˜åŒ–æ¥ä¼˜åŒ– 4D é«˜æ–¯ç‚¹äº‘ï¼Œå®ç°æ—¶ç©ºä¸€è‡´çš„ 4D ç”Ÿæˆã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬æ–‡æå‡ºäº†ä¸€ç§ä»å•ç›®è§†é¢‘ç”ŸæˆåŠ¨æ€ 3D å†…å®¹çš„æ–°æ–¹æ³•ï¼Œè§£å†³äº† 4D è¡¨ç¤ºå’Œæ—¶ç©ºä¸€è‡´æ€§çš„æŒ‘æˆ˜ã€‚é€šè¿‡åˆ©ç”¨ä¸“é—¨å®šåˆ¶çš„ 4D é«˜æ–¯ä½“ç´ æ¸²æŸ“å’Œæ–°é¢–çš„ä¿¡æ¯èåˆæ¨¡å—ï¼Œæ‰€æå‡ºçš„æ–¹æ³•å®ç°äº†é«˜è´¨é‡ä¸”é²æ£’çš„ 4D åœºæ™¯ç”Ÿæˆã€‚å…¨é¢çš„å®éªŒè¡¨æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œä¸æœ€å…ˆè¿›çš„å…ˆå‰æ–¹æ³•ç›¸æ¯”ï¼Œå±•ç¤ºäº†æ˜æ˜¾æ›´å¿«çš„ç”Ÿæˆé€Ÿåº¦ä»¥åŠæ¸²æŸ“è´¨é‡å’Œæ—¶é—´ä¸€è‡´æ€§çš„æ˜¾ç€æ”¹è¿›ã€‚æ€»ä½“è€Œè¨€ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å•ç›®è§†é¢‘ä¸­åŠ¨æ€ 3D å†…å®¹ç”Ÿæˆçš„è®­ç»ƒé€Ÿåº¦ã€æ¸²æŸ“è´¨é‡å’Œ 4D ä¸€è‡´æ€§æ–¹é¢æ ‘ç«‹äº†æ–°çš„åŸºå‡†ï¼Œä¸ºç°å®ä¸–ç•Œçš„åº”ç”¨å¼€è¾Ÿäº†å¯èƒ½æ€§ã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§æ–°çš„ 4D é«˜æ–¯ä½“ç´ è¡¨ç¤ºï¼Œå¹¶é‡‡ç”¨è‡ªé€‚åº”åŠ å¯†ç­–ç•¥æ¥ç¼“è§£ä¸ç¨³å®šçš„é«˜æ–¯æ¢¯åº¦ï¼Œä»¥å®ç°é²æ£’ä¼˜åŒ–ã€‚</li><li>ç»“åˆå¤šè§†å›¾æ‰©æ•£æ¨¡å‹å’Œå‚è€ƒæ³¨æ„åŠ›ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ—¶é—´å’Œå¤šè§†å›¾ä¸€è‡´æ‰©æ•£æ¨¡å—ï¼Œä»¥ç”Ÿæˆæ—¶é—´ä¸€è‡´çš„å¤šè§†å›¾åºåˆ—ã€‚</li><li>åˆ©ç”¨ç”Ÿæˆçš„é”šè§†å›¾å’Œå‚è€ƒè§†å›¾ï¼Œä½¿ç”¨å¤šè§†å›¾ SDS ä¼˜åŒ–æ¥ä¼˜åŒ– 4D é«˜æ–¯ä½“ç´ ï¼Œå®ç°æ—¶ç©ºä¸€è‡´çš„ 4D ç”Ÿæˆã€‚æ€§èƒ½ï¼š</li><li>ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œæ¸²æŸ“è´¨é‡ã€æ—¶é—´ä¸€è‡´æ€§å’Œç”Ÿæˆé²æ£’æ€§æ–¹é¢å–å¾—äº†æ˜¾ç€æ”¹è¿›ã€‚</li><li>ä¸ç°æœ‰çš„ 4D ç”Ÿæˆæ–¹æ³•ç›¸æ¯”ï¼Œç”Ÿæˆé€Ÿåº¦æ˜æ˜¾æé«˜ã€‚å·¥ä½œé‡ï¼š</li><li>æ— éœ€å¯¹æ‰©æ•£ç½‘ç»œè¿›è¡Œä»»ä½•é¢„è®­ç»ƒæˆ–å¾®è°ƒã€‚</li><li>æ˜“äºå®ç°å’Œä½¿ç”¨ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-cc3237d865a131294adf4c088d9c1009.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0bdb6857c03ea01ca9348a454fc10619.jpg" align="middle"><img src="https://picx.zhimg.com/v2-171cac27c18392a0d918da1cdd0d421b.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-03-28  Modeling uncertainty for Gaussian Splatting</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/03/28/Paper/2024-03-28/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/03/28/Paper/2024-03-28/Talking%20Head%20Generation/</id>
    <published>2024-03-28T03:07:02.000Z</published>
    <updated>2024-03-28T03:07:02.568Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-03-28-æ›´æ–°"><a href="#2024-03-28-æ›´æ–°" class="headerlink" title="2024-03-28 æ›´æ–°"></a>2024-03-28 æ›´æ–°</h1><h2 id="Deepfake-Generation-and-Detection-A-Benchmark-and-Survey"><a href="#Deepfake-Generation-and-Detection-A-Benchmark-and-Survey" class="headerlink" title="Deepfake Generation and Detection: A Benchmark and Survey"></a>Deepfake Generation and Detection: A Benchmark and Survey</h2><p><strong>Authors:Gan Pei, Jiangning Zhang, Menghan Hu, Guangtao Zhai, Chengjie Wang, Zhenyu Zhang, Jian Yang, Chunhua Shen, Dacheng Tao</strong></p><p>In addition to the advancements in deepfake generation, corresponding detection technologies need to continuously evolve to regulate the potential misuse of deepfakes, such as for privacy invasion and phishing attacks. This survey comprehensively reviews the latest developments in deepfake generation and detection, summarizing and analyzing the current state of the art in this rapidly evolving field. We first unify task definitions, comprehensively introduce datasets and metrics, and discuss the development of generation and detection technology frameworks. Then, we discuss the development of several related sub-fields and focus on researching four mainstream deepfake fields: popular face swap, face reenactment, talking face generation, and facial attribute editing, as well as foreign detection. Subsequently, we comprehensively benchmark representative methods on popular datasets for each field, fully evaluating the latest and influential works published in top conferences/journals. Finally, we analyze the challenges and future research directions of the discussed fields. We closely follow the latest developments in <a href="https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection">https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection</a>. </p><p><a href="http://arxiv.org/abs/2403.17881v1">PDF</a> </p><p><strong>Summary</strong><br>æ·±åº¦ä¼ªé€ æŠ€æœ¯çš„å‘å±•ä¸æ£€æµ‹æŠ€æœ¯éœ€è¦æŒç»­æ¼”è¿›ï¼Œä»¥åº”å¯¹éšç§ä¾µçŠ¯å’Œç½‘ç»œé’“é±¼ç­‰éæ³•ä½¿ç”¨ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ç»Ÿä¸€ä»»åŠ¡å®šä¹‰ï¼Œå…¨é¢ä»‹ç»æ•°æ®é›†å’Œè¯„ä¼°æŒ‡æ ‡ã€‚</li><li>æ¢è®¨ç”Ÿæˆå’Œæ£€æµ‹æŠ€æœ¯æ¡†æ¶çš„å‘å±•ã€‚</li><li>å…³æ³¨äººè„¸æ›¿æ¢ã€äººè„¸é‡ç°ã€è¯´è¯äººè„¸ç”Ÿæˆã€é¢éƒ¨å±æ€§ç¼–è¾‘ç­‰ä¸»æµæ·±åº¦ä¼ªé€ é¢†åŸŸã€‚</li><li>å…¨é¢åŸºå‡†æµ‹è¯•æ¯ä¸ªé¢†åŸŸæµè¡Œæ•°æ®é›†ä¸Šçš„ä»£è¡¨æ€§æ–¹æ³•ã€‚</li><li>åˆ†ææ‰€è®¨è®ºé¢†åŸŸçš„æŒ‘æˆ˜å’Œæœªæ¥ç ”ç©¶æ–¹å‘ã€‚</li><li>è·Ÿè¸ª Github ä¸Šæ·±åº¦ä¼ªé€ ç”Ÿæˆä¸æ£€æµ‹çš„æœ€æ–°è¿›å±•ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><p>1.é¢˜ç›®ï¼šæ·±åº¦ä¼ªé€ ç”Ÿæˆä¸æ£€æµ‹ï¼šåŸºå‡†ä¸ç»¼è¿°2.ä½œè€…ï¼šç”˜æ²›ï¼Œè’‹å®ç« ï¼Œå­Ÿæ¶µèƒ¡ï¼Œå…‰æ¶›ç¿Ÿï¼Œæˆæ°ç‹ï¼ŒæŒ¯å®‡å¼ ï¼Œå»ºæ¨ï¼Œæ˜¥åæ²ˆï¼Œå¤§æˆé™¶3.ç¬¬ä¸€ä½œè€…å•ä½ï¼šåä¸œå¸ˆèŒƒå¤§å­¦å¤šæ¨¡ä¿¡æ¯å¤„ç†ä¸Šæµ·å¸‚é‡ç‚¹å®éªŒå®¤4.å…³é”®è¯ï¼šæ·±åº¦ä¼ªé€ ç”Ÿæˆï¼Œäººè„¸æ›¿æ¢ï¼Œäººè„¸é‡æ¼”ï¼Œè¯­éŸ³äººè„¸ç”Ÿæˆï¼Œäººè„¸å±æ€§ç¼–è¾‘ï¼Œå¤–æ¥æ£€æµ‹ï¼Œç»¼è¿°5.è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.17881ï¼ŒGithubä»£ç é“¾æ¥ï¼šæ— 6.æ€»ç»“ï¼šï¼ˆ1ï¼‰ï¼šéšç€æ·±åº¦å­¦ä¹ çš„è¿›æ­¥ï¼Œä»¥å˜åˆ†è‡ªç¼–ç å™¨ (VAE) å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œ (GAN) ä¸ºä»£è¡¨çš„æŠ€æœ¯åœ¨æ·±åº¦ä¼ªé€ ç”Ÿæˆé¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆæœã€‚è¿‘å¹´æ¥ï¼Œå…·æœ‰å¼ºå¤§å›¾åƒç”Ÿæˆèƒ½åŠ›çš„æ‰©æ•£æ¨¡å‹çš„å‡ºç°å¼•å‘äº†è¯¥æŠ€æœ¯çš„æ–°ä¸€è½®ç ”ç©¶å’Œäº§ä¸šçƒ­æ½®ã€‚ï¼ˆ2ï¼‰ï¼šä¼ ç»Ÿçš„æ·±åº¦ä¼ªé€ ç”Ÿæˆæ–¹æ³•åŸºäº GAN æ¨¡å‹ï¼Œå­˜åœ¨ç”Ÿæˆæ•ˆæœä¸ä½³çš„é—®é¢˜ã€‚æ‰©æ•£æ¨¡å‹çš„å‡ºç°æå¤§åœ°æå‡äº†å›¾åƒ/è§†é¢‘çš„ç”Ÿæˆèƒ½åŠ›ï¼Œä½¿å¾—ç”Ÿæˆçš„æ·±åº¦ä¼ªé€ å†…å®¹ä¸çœŸå®å†…å®¹éš¾ä»¥åŒºåˆ†ï¼Œå…·æœ‰å¾ˆé«˜çš„å®ç”¨ä»·å€¼ã€‚ï¼ˆ3ï¼‰ï¼šæ·±åº¦ä¼ªé€ ç”Ÿæˆä¸»è¦åˆ†ä¸ºäººè„¸æ›¿æ¢ã€äººè„¸é‡æ¼”ã€è¯­éŸ³äººè„¸ç”Ÿæˆå’Œäººè„¸å±æ€§ç¼–è¾‘å››ä¸ªä¸»æµç ”ç©¶é¢†åŸŸã€‚æœ¬æ–‡å¯¹è¿™äº›é¢†åŸŸçš„å‘å±•è¿›è¡Œäº†ç»¼è¿°ï¼Œå¹¶å¯¹å„ä¸ªé¢†åŸŸçš„ä»£è¡¨æ€§æ–¹æ³•è¿›è¡Œäº†åŸºå‡†æµ‹è¯•å’Œå…¨é¢è¯„ä¼°ã€‚ï¼ˆ4ï¼‰ï¼šæœ¬æ–‡åˆ†æäº†æ·±åº¦ä¼ªé€ ç”Ÿæˆå’Œæ£€æµ‹é¢†åŸŸé¢ä¸´çš„æŒ‘æˆ˜å’Œæœªæ¥ç ”ç©¶æ–¹å‘ï¼Œä¸ºè¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•æä¾›äº†å‚è€ƒã€‚</p><p></p><ol><li><p>æ–¹æ³•ï¼š(1) æœ¬æ–‡å¯¹æ·±åº¦ä¼ªé€ ç”Ÿæˆä¸æ£€æµ‹é¢†åŸŸçš„ç ”ç©¶ç°çŠ¶è¿›è¡Œäº†å…¨é¢çš„æ€»ç»“å’Œç»¼è¿°ï¼Œåˆ†æäº†è¯¥é¢†åŸŸé¢ä¸´çš„æŒ‘æˆ˜å’Œæœªæ¥ç ”ç©¶æ–¹å‘ã€‚(2) æœ¬æ–‡å¯¹æ·±åº¦ä¼ªé€ ç”Ÿæˆé¢†åŸŸçš„ä¸»æµç ”ç©¶é¢†åŸŸï¼ŒåŒ…æ‹¬äººè„¸æ›¿æ¢ã€äººè„¸é‡æ¼”ã€è¯­éŸ³äººè„¸ç”Ÿæˆå’Œäººè„¸å±æ€§ç¼–è¾‘ï¼Œè¿›è¡Œäº†åŸºå‡†æµ‹è¯•å’Œå…¨é¢è¯„ä¼°ã€‚(3) æœ¬æ–‡å¯¹æ·±åº¦ä¼ªé€ æ£€æµ‹é¢†åŸŸçš„ç ”ç©¶è¿›å±•è¿›è¡Œäº†æ€»ç»“ï¼Œåˆ†æäº†å¤–æ¥æ£€æµ‹å’Œå†…åœ¨æ£€æµ‹ä¸¤ç§æ£€æµ‹æ–¹æ³•çš„ä¼˜ç¼ºç‚¹ï¼Œå¹¶å¯¹æœªæ¥ç ”ç©¶æ–¹å‘è¿›è¡Œäº†å±•æœ›ã€‚</p></li><li><p>ç»“è®ºï¼š(1): æœ¬ç»¼è¿°å…¨é¢å›é¡¾äº†æ·±åº¦ä¼ªé€ ç”Ÿæˆä¸æ£€æµ‹é¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼Œé¦–æ¬¡å…¨é¢è¦†ç›–äº†ç›¸å…³é¢†åŸŸï¼Œå¹¶è®¨è®ºäº†æ‰©æ•£æ¨¡å‹ç­‰æœ€æ–°æŠ€æœ¯ã€‚å…·ä½“è€Œè¨€ï¼Œæœ¬æ–‡æ¶µç›–äº†åŸºæœ¬èƒŒæ™¯çŸ¥è¯†çš„æ¦‚è¿°ï¼ŒåŒ…æ‹¬ç ”ç©¶ä»»åŠ¡çš„æ¦‚å¿µã€æ•°æ®æ”¶é›†ä¸å¤„ç†æ–¹æ³•ã€æ¨¡å‹è®¾è®¡ä¸è®­ç»ƒç­–ç•¥ã€è¯„ä¼°æŒ‡æ ‡å’Œæ•°æ®é›†ã€‚(2): åˆ›æ–°ç‚¹ï¼šæœ¬æ–‡å¯¹æ·±åº¦ä¼ªé€ ç”Ÿæˆé¢†åŸŸçš„å››ä¸ªä¸»æµç ”ç©¶é¢†åŸŸè¿›è¡Œäº†åŸºå‡†æµ‹è¯•å’Œå…¨é¢è¯„ä¼°ï¼ŒåŒ…æ‹¬äººè„¸æ›¿æ¢ã€äººè„¸é‡æ¼”ã€è¯­éŸ³äººè„¸ç”Ÿæˆå’Œäººè„¸å±æ€§ç¼–è¾‘ã€‚æœ¬æ–‡è¿˜å¯¹æ·±åº¦ä¼ªé€ æ£€æµ‹é¢†åŸŸçš„ç ”ç©¶è¿›å±•è¿›è¡Œäº†æ€»ç»“ï¼Œåˆ†æäº†å¤–æ¥æ£€æµ‹å’Œå†…åœ¨æ£€æµ‹ä¸¤ç§æ£€æµ‹æ–¹æ³•çš„ä¼˜ç¼ºç‚¹ï¼Œå¹¶å¯¹æœªæ¥ç ”ç©¶æ–¹å‘è¿›è¡Œäº†å±•æœ›ã€‚æ€§èƒ½ï¼šæœ¬æ–‡æå‡ºçš„åŸºå‡†æµ‹è¯•å’Œå…¨é¢è¯„ä¼°ä¸ºæ·±åº¦ä¼ªé€ ç”Ÿæˆä¸æ£€æµ‹é¢†åŸŸçš„ç ”ç©¶äººå‘˜æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚å·¥ä½œé‡ï¼šæœ¬æ–‡å¯¹æ·±åº¦ä¼ªé€ ç”Ÿæˆä¸æ£€æµ‹é¢†åŸŸçš„ç ”ç©¶ç°çŠ¶è¿›è¡Œäº†å…¨é¢çš„æ€»ç»“å’Œç»¼è¿°ï¼Œå·¥ä½œé‡è¾ƒå¤§ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-3fbcb20b0b6d83737be267b8b78dde71.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bac7dee6bad7c9614f746a35eef341ec.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4a0d28dab08c4d0254dd790d3d608013.jpg" align="middle"><img src="https://picx.zhimg.com/v2-409f1c30ffae605d9a497f77ff9ae5bb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-80df0902b8cc7d09c263750672e1ab59.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b4b73e97f1af3856b9dddf84237d9fcb.jpg" align="middle"></details><h2 id="Make-Your-Anchor-A-Diffusion-based-2D-Avatar-Generation-Framework"><a href="#Make-Your-Anchor-A-Diffusion-based-2D-Avatar-Generation-Framework" class="headerlink" title="Make-Your-Anchor: A Diffusion-based 2D Avatar Generation Framework"></a>Make-Your-Anchor: A Diffusion-based 2D Avatar Generation Framework</h2><p><strong>Authors:Ziyao Huang, Fan Tang, Yong Zhang, Xiaodong Cun, Juan Cao, Jintao Li, Tong-Yee Lee</strong></p><p>Despite the remarkable process of talking-head-based avatar-creating solutions, directly generating anchor-style videos with full-body motions remains challenging. In this study, we propose Make-Your-Anchor, a novel system necessitating only a one-minute video clip of an individual for training, subsequently enabling the automatic generation of anchor-style videos with precise torso and hand movements. Specifically, we finetune a proposed structure-guided diffusion model on input video to render 3D mesh conditions into human appearances. We adopt a two-stage training strategy for the diffusion model, effectively binding movements with specific appearances. To produce arbitrary long temporal video, we extend the 2D U-Net in the frame-wise diffusion model to a 3D style without additional training cost, and a simple yet effective batch-overlapped temporal denoising module is proposed to bypass the constraints on video length during inference. Finally, a novel identity-specific face enhancement module is introduced to improve the visual quality of facial regions in the output videos. Comparative experiments demonstrate the effectiveness and superiority of the system in terms of visual quality, temporal coherence, and identity preservation, outperforming SOTA diffusion/non-diffusion methods. Project page: \url{<a href="https://github.com/ICTMCG/Make-Your-Anchor}">https://github.com/ICTMCG/Make-Your-Anchor}</a>. </p><p><a href="http://arxiv.org/abs/2403.16510v1">PDF</a> accepted at CVPR2024</p><p><strong>Summary</strong><br>ä½¿ç”¨ä»…ä¸€åˆ†é’Ÿè§†é¢‘è®­ç»ƒå³å¯ç”Ÿæˆæ‹¥æœ‰èº¯å¹²å’Œæ‰‹éƒ¨åŠ¨ä½œçš„ä¸»æ’­é£æ ¼å®Œæ•´è§†é¢‘ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºäº†ä¸€ç§é€šè¿‡ä¸€åˆ†é’Ÿè§†é¢‘è®­ç»ƒæ¥ç”Ÿæˆä¸»æ’­é£æ ¼è§†é¢‘çš„ç³»ç»Ÿâ€”â€”Make-Your-Anchorã€‚</li><li>é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œå°†åŠ¨ä½œä¸ç‰¹å®šå¤–è§‚æœ‰æ•ˆåœ°ç»‘å®šã€‚</li><li>æ‰©å±•äº†å¸§çº§æ‰©æ•£æ¨¡å‹ä¸­çš„äºŒç»´ U-Net åˆ°ä¸‰ç»´é£æ ¼ï¼Œä»¥ç”Ÿæˆä»»æ„é•¿åº¦çš„æ—¶é—´è§†é¢‘ã€‚</li><li>æå‡ºäº†ä¸€ç§ç®€å•çš„æ‰¹é‡é‡å æ—¶é—´å»å™ªæ¨¡å—ï¼Œä»¥ç»•è¿‡æ¨ç†æœŸé—´è§†é¢‘é•¿åº¦çš„é™åˆ¶ã€‚</li><li>å¼•å…¥äº†æ–°é¢–çš„èº«ä»½ç‰¹å®šé¢éƒ¨å¢å¼ºæ¨¡å—ï¼Œä»¥æé«˜è¾“å‡ºè§†é¢‘ä¸­é¢éƒ¨åŒºåŸŸçš„è§†è§‰è´¨é‡ã€‚</li><li>ä¸ SOTA æ‰©æ•£/éæ‰©æ•£æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥ç³»ç»Ÿåœ¨è§†è§‰è´¨é‡ã€æ—¶é—´è¿è´¯æ€§å’Œèº«ä»½ä¿ç•™æ–¹é¢è¡¨ç°å‡ºæœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</li><li>é¡¹ç›®ä¸»é¡µï¼š\url{<a href="https://github.com/ICTMCG/Make-Your-Anchor}ã€‚">https://github.com/ICTMCG/Make-Your-Anchor}ã€‚</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šMake-Your-Anchorï¼šåŸºäºæ‰©æ•£çš„ 2D å¤´åƒç”Ÿæˆæ¡†æ¶</li><li>ä½œè€…ï¼šZiyao Huangã€Fan Tangã€Yong Zhangã€Xiaodong Cunã€Juan Caoã€Jintao Liã€Tong-Yee Lee</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šä¸­å›½ç§‘å­¦é™¢è®¡ç®—æŠ€æœ¯ç ”ç©¶æ‰€</li><li>å…³é”®è¯ï¼šè§†é¢‘ç”Ÿæˆã€å¤´åƒç”Ÿæˆã€æ‰©æ•£æ¨¡å‹ã€è¿åŠ¨æ•æ‰</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.16510   Github ä»£ç é“¾æ¥ï¼šhttps://github.com/ICTMCG/Make-Your-Anchor</li><li>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼š   å½“å‰çš„å¤´åƒç”ŸæˆæŠ€æœ¯ä¸»è¦é›†ä¸­åœ¨å¤´éƒ¨ç”Ÿæˆï¼Œæ— æ³•ç”Ÿæˆå…¨èº«åŠ¨ä½œé€¼çœŸçš„å¤´åƒè§†é¢‘ã€‚   ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼š   ç°æœ‰çš„åŸºäº GAN çš„æ–¹æ³•åªèƒ½ç”Ÿæˆå±€éƒ¨åŒºåŸŸï¼ŒåŸºäºè¿åŠ¨è¿ç§»çš„æ–¹æ³•å—é™äºè¿åŠ¨æ•æ‰æ•°æ®çš„å¯ç”¨æ€§ã€‚   ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼š   æœ¬æ–‡æå‡º Make-Your-Anchor æ¡†æ¶ï¼Œé€šè¿‡å¾®è°ƒåŸºäºç»“æ„å¼•å¯¼çš„æ‰©æ•£æ¨¡å‹ï¼Œå°† 3D ç½‘æ ¼æ¡ä»¶æ¸²æŸ“ä¸ºé€¼çœŸçš„å…¨èº«åŠ¨ä½œå¤´åƒè§†é¢‘ã€‚é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œæœ‰æ•ˆåœ°å°†è¿åŠ¨ä¸ç‰¹å®šå¤–è§‚ç»‘å®šã€‚ä¸ºäº†ç”Ÿæˆä»»æ„é•¿åº¦çš„è§†é¢‘ï¼Œå°†å¸§çº§æ‰©æ•£æ¨¡å‹ä¸­çš„ 2D U-Net æ‰©å±•ä¸º 3Dï¼Œå¹¶æå‡ºäº†ä¸€ç§ç®€å•çš„æ‰¹æ¬¡é‡å æ—¶é—´å»å™ªæ¨¡å—ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ä¸ªæ–°çš„èº«ä»½ç‰¹å®šé¢éƒ¨å¢å¼ºæ¨¡å—ï¼Œä»¥æé«˜è¾“å‡ºè§†é¢‘ä¸­é¢éƒ¨åŒºåŸŸçš„è§†è§‰è´¨é‡ã€‚   ï¼ˆ4ï¼‰ä»»åŠ¡å’Œæ€§èƒ½ï¼š   Make-Your-Anchor åœ¨è§†è§‰è´¨é‡ã€æ—¶é—´è¿è´¯æ€§å’Œèº«ä»½ä¿ç•™æ–¹é¢ä¼˜äº SOTA æ‰©æ•£/éæ‰©æ•£æ–¹æ³•ã€‚å®ƒä»…éœ€ä¸€åˆ†é’Ÿçš„è§†é¢‘å‰ªè¾‘å³å¯è®­ç»ƒï¼Œç”Ÿæˆå…¨èº«åŠ¨ä½œé€¼çœŸçš„å¤´åƒè§†é¢‘ï¼Œæ»¡è¶³äº†è‡ªåŠ¨ç”Ÿæˆå¤´åƒè§†é¢‘çš„éœ€æ±‚ã€‚</li></ol><p>7.æ–¹æ³•ï¼š(1)ç»“æ„å¼•å¯¼æ‰©æ•£æ¨¡å‹ï¼šå°†3Dç½‘æ ¼æ¡ä»¶åµŒå…¥ç”Ÿæˆè¿‡ç¨‹ï¼Œå­¦ä¹ å§¿æ€ä¸ç›®æ ‡è§†é¢‘å¸§ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼›(2)ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼šé¢„è®­ç»ƒå¢å¼ºæ¨¡å‹ç”ŸæˆåŠ¨ä½œçš„èƒ½åŠ›ï¼Œå¾®è°ƒç»‘å®šåŠ¨ä½œä¸ç‰¹å®šå¤–è§‚ï¼›(3)æ‰¹æ¬¡é‡å æ—¶é—´å»å™ªï¼šé‡‡ç”¨å…¨å¸§äº¤å‰æ³¨æ„åŠ›æ¨¡å—å’Œé‡å æ—¶é—´å»å™ªç®—æ³•ï¼Œç”Ÿæˆä»»æ„é•¿åº¦çš„æ—¶é—´ä¸€è‡´è§†é¢‘ï¼›(4)èº«ä»½ç‰¹å®šé¢éƒ¨å¢å¼ºæ¨¡å—ï¼šé€šè¿‡è£å‰ªå’Œèåˆæ“ä½œï¼Œå¯¹ç”Ÿæˆçš„èº«ä½“ä¸­çš„é¢éƒ¨åŒºåŸŸè¿›è¡Œä¿®æ”¹ï¼Œæé«˜è§†è§‰è´¨é‡ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº† Make-Your-Anchorï¼Œä¸€ä¸ªåŸºäºæ‰©æ•£çš„ 2D å¤´åƒç”Ÿæˆæ¡†æ¶ï¼Œç”¨äºåˆ¶ä½œé€¼çœŸä¸”é«˜è´¨é‡çš„ä¸»æ’­é£æ ¼äººç‰©è§†é¢‘ã€‚è¯¥æ¡†æ¶é€šè¿‡å¸§çº§è¿åŠ¨åˆ°å¤–è§‚æ‰©æ•£è®­ç»ƒäº†ä¸€ä¸ªç»“æ„å¼•å¯¼çš„æ‰©æ•£æ¨¡å‹ï¼Œå¹¶é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥å’Œç»‘å®šé£æ ¼æ–¹æ³•å®ç°äº†ç‰¹å®šå¤–è§‚ä¸åŠ¨ä½œçš„ç»‘å®šã€‚ä¸ºäº†ç”Ÿæˆæ—¶é—´ä¸€è‡´çš„äººç‰©è§†é¢‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ— è®­ç»ƒç­–ç•¥ï¼Œå°†å›¾åƒæ‰©æ•£æ¨¡å‹æ‰©å±•ä¸ºè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªæ‰¹æ¬¡é‡å æ—¶é—´å»å™ªç®—æ³•æ¥å…‹æœç”Ÿæˆè§†é¢‘é•¿åº¦çš„é™åˆ¶ã€‚ä»è§‚å¯Ÿåˆ°é¢éƒ¨ç»†èŠ‚åœ¨æ•´ä½“äººç‰©ç”Ÿæˆè¿‡ç¨‹ä¸­éš¾ä»¥é‡å»ºè¿™ä¸€ç°è±¡å‡ºå‘ï¼Œå¼•å…¥äº†èº«ä»½ç‰¹å®šçš„é¢éƒ¨å¢å¼ºæŠ€æœ¯ã€‚é€šè¿‡å°†è¿™å››ä¸ªç³»ç»Ÿæ–¹æ³•ç›¸ç»“åˆï¼Œæˆ‘ä»¬çš„æ¡†æ¶æˆåŠŸåœ°ç”Ÿæˆäº†é«˜è´¨é‡ã€ç»“æ„ä¿æŒå’Œæ—¶é—´è¿è´¯çš„ä¸»æ’­é£æ ¼äººç‰©è§†é¢‘ï¼Œè¿™å¯èƒ½ä¸º 2D æ•°å­—å¤´åƒçš„å¹¿æ³›åº”ç”¨æŠ€æœ¯æä¾›ä¸€äº›å‚è€ƒä»·å€¼ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„ 2D å¤´åƒç”Ÿæˆæ¡†æ¶ï¼Œå¯ä»¥ç”Ÿæˆé€¼çœŸä¸”é«˜è´¨é‡çš„ä¸»æ’­é£æ ¼äººç‰©è§†é¢‘ï¼›é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥å’Œç»‘å®šé£æ ¼æ–¹æ³•ï¼Œå°†ç‰¹å®šå¤–è§‚ä¸åŠ¨ä½œç»‘å®šï¼›æå‡ºäº†ä¸€ä¸ªæ— è®­ç»ƒç­–ç•¥ï¼Œå°†å›¾åƒæ‰©æ•£æ¨¡å‹æ‰©å±•ä¸ºè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªæ‰¹æ¬¡é‡å æ—¶é—´å»å™ªç®—æ³•æ¥å…‹æœç”Ÿæˆè§†é¢‘é•¿åº¦çš„é™åˆ¶ï¼›å¼•å…¥äº†èº«ä»½ç‰¹å®šçš„é¢éƒ¨å¢å¼ºæŠ€æœ¯ï¼Œä»¥æé«˜ç”Ÿæˆè§†é¢‘ä¸­é¢éƒ¨åŒºåŸŸçš„è§†è§‰è´¨é‡ã€‚æ€§èƒ½ï¼šåœ¨è§†è§‰è´¨é‡ã€æ—¶é—´è¿è´¯æ€§å’Œèº«ä»½ä¿ç•™æ–¹é¢ä¼˜äº SOTA æ‰©æ•£/éæ‰©æ•£æ–¹æ³•ï¼›ä»…éœ€ä¸€åˆ†é’Ÿçš„è§†é¢‘å‰ªè¾‘å³å¯è®­ç»ƒï¼Œç”Ÿæˆå…¨èº«åŠ¨ä½œé€¼çœŸçš„å¤´åƒè§†é¢‘ï¼Œæ»¡è¶³äº†è‡ªåŠ¨ç”Ÿæˆå¤´åƒè§†é¢‘çš„éœ€æ±‚ã€‚å·¥ä½œé‡ï¼šä¸­ç­‰ï¼›éœ€è¦æ”¶é›†å’Œé¢„å¤„ç†è®­ç»ƒæ•°æ®ï¼›éœ€è¦å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒå’Œå¾®è°ƒï¼›éœ€è¦å¯¹ç”Ÿæˆç»“æœè¿›è¡Œè¯„ä¼°å’Œä¼˜åŒ–ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-31e07e2070d0183b4685c1e857aaf0a2.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d20d65a2e0024c9d9bc380b7f7ba43b0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-771450d6d33a69293597f486c329a82a.jpg" align="middle"><img src="https://pica.zhimg.com/v2-4f2a829065c463be027e4b423c4e43c8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6ad4738b2a0c37e9688fb722556c4213.jpg" align="middle"><img src="https://pica.zhimg.com/v2-b6700b1c5fd0f20662f87b175ebf869f.jpg" align="middle"></details><h2 id="Adaptive-Super-Resolution-For-One-Shot-Talking-Head-Generation"><a href="#Adaptive-Super-Resolution-For-One-Shot-Talking-Head-Generation" class="headerlink" title="Adaptive Super Resolution For One-Shot Talking-Head Generation"></a>Adaptive Super Resolution For One-Shot Talking-Head Generation</h2><p><strong>Authors:Luchuan Song, Pinxin Liu, Guojun Yin, Chenliang Xu</strong></p><p>The one-shot talking-head generation learns to synthesize a talking-head video with one source portrait image under the driving of same or different identity video. Usually these methods require plane-based pixel transformations via Jacobin matrices or facial image warps for novel poses generation. The constraints of using a single image source and pixel displacements often compromise the clarity of the synthesized images. Some methods try to improve the quality of synthesized videos by introducing additional super-resolution modules, but this will undoubtedly increase computational consumption and destroy the original data distribution. In this work, we propose an adaptive high-quality talking-head video generation method, which synthesizes high-resolution video without additional pre-trained modules. Specifically, inspired by existing super-resolution methods, we down-sample the one-shot source image, and then adaptively reconstruct high-frequency details via an encoder-decoder module, resulting in enhanced video clarity. Our method consistently improves the quality of generated videos through a straightforward yet effective strategy, substantiated by quantitative and qualitative evaluations. The code and demo video are available on: \url{<a href="https://github.com/Songluchuan/AdaSR-TalkingHead/}">https://github.com/Songluchuan/AdaSR-TalkingHead/}</a>. </p><p><a href="http://arxiv.org/abs/2403.15944v1">PDF</a> 5 pages, 3 figures</p><p><strong>Summary</strong><br>ä¸€é”®å¼ç”Ÿæˆé«˜æ¸…æ™°åº¦è§†é¢‘ï¼Œæ— éœ€æ·»åŠ é¢„è®­ç»ƒæ¨¡å—ï¼Œé€šè¿‡è‡ªé€‚åº”é‡å»ºé«˜é¢‘ç»†èŠ‚æå‡è§†é¢‘æ¸…æ™°åº¦ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ä¸€é”®å¼ç”Ÿæˆäººåƒè§†é¢‘ï¼Œé©±åŠ¨è§†é¢‘ä¸äººåƒåŒä¸€æˆ–ä¸åŒã€‚</li><li>ä¼ ç»Ÿæ–¹æ³•å—é™äºå•å›¾åƒæºå’Œåƒç´ ä½ç§»ï¼Œæ¸…æ™°åº¦å—æŸã€‚</li><li>ç°æœ‰æ–¹æ³•é€šè¿‡è¶…åˆ†è¾¨ç‡æ¨¡å—æå‡è´¨é‡ï¼Œä½†å¢åŠ è®¡ç®—é‡å¹¶ç ´ååŸå§‹æ•°æ®åˆ†å¸ƒã€‚</li><li>æœ¬æ–‡æå‡ºè‡ªé€‚åº”é«˜å“è´¨äººåƒè§†é¢‘ç”Ÿæˆæ–¹æ³•ï¼Œæ— éœ€é¢å¤–é¢„è®­ç»ƒæ¨¡å—åˆæˆé«˜åˆ†è¾¨ç‡è§†é¢‘ã€‚</li><li>å—è¶…åˆ†è¾¨ç‡æ–¹æ³•å¯å‘ï¼Œå¯¹å•å›¾åƒæºä¸‹é‡‡æ ·ï¼Œå†é€šè¿‡ç¼–ç å™¨-è§£ç å™¨æ¨¡å—è‡ªé€‚åº”é‡å»ºé«˜é¢‘ç»†èŠ‚ã€‚</li><li>è¯¥ç­–ç•¥ç®€å•æœ‰æ•ˆåœ°æå‡äº†ç”Ÿæˆè§†é¢‘çš„è´¨é‡ï¼Œå¹¶é€šè¿‡å®šé‡å’Œå®šæ€§è¯„ä¼°å¾—åˆ°è¯å®ã€‚</li><li>ä»£ç å’Œæ¼”ç¤ºè§†é¢‘å¯åœ¨ Github ä¸Šè·å–ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šè‡ªé€‚åº”è¶…åˆ†è¾¨ç‡å•é•œå¤´è¯´è¯äººå¤´éƒ¨ç”Ÿæˆ</li><li>ä½œè€…ï¼šLuchuan Song, Pinxin Liu, Guojun Yin, Chenliang Xu</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šç½—åˆ‡æ–¯ç‰¹å¤§å­¦</li><li>å…³é”®è¯ï¼šè¶…åˆ†è¾¨ç‡è§†é¢‘ï¼Œå•é•œå¤´è¯´è¯äººå¤´éƒ¨ç”Ÿæˆ</li><li>é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.15944ï¼ŒGithubï¼šNone</li><li><p>æ‘˜è¦ï¼š(1)ï¼šç ”ç©¶èƒŒæ™¯ï¼šå•é•œå¤´è¯´è¯äººå¤´éƒ¨ç”Ÿæˆæ—¨åœ¨ä½¿ç”¨ä¸€å¼ æºäººåƒå›¾åƒåœ¨ç›¸åŒæˆ–ä¸åŒèº«ä»½è§†é¢‘çš„é©±åŠ¨ä¸‹åˆæˆè¯´è¯äººå¤´éƒ¨è§†é¢‘ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦åŸºäºå¹³é¢çš„åƒç´ å˜æ¢ï¼Œè¿™ä¼šå½±å“åˆæˆå›¾åƒçš„æ¸…æ™°åº¦ã€‚ä¸€äº›æ–¹æ³•é€šè¿‡å¼•å…¥é¢å¤–çš„è¶…åˆ†è¾¨ç‡æ¨¡å—æ¥æé«˜åˆæˆè§†é¢‘çš„è´¨é‡ï¼Œä½†è¿™ä¼šå¢åŠ è®¡ç®—æ¶ˆè€—å¹¶ç ´ååŸå§‹æ•°æ®åˆ†å¸ƒã€‚(2)ï¼šè¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šMetaPortraitã€SadTalker å’Œ VideoReTalking ç­‰æ–¹æ³•å°è¯•é€šè¿‡é‡æ–°è®­ç»ƒä¸€ä¸ªç‹¬ç«‹çš„è¶…åˆ†è¾¨ç‡æ¨¡å—æ¥æ”¹å–„è§†é¢‘è´¨é‡ã€‚ç„¶è€Œï¼Œè¿™ç§ä¸¤é˜¶æ®µåˆæˆè¿‡ç¨‹ä¼šå¯¼è‡´ä¸å¿…è¦çš„è®¡ç®—å¼€é”€å’Œé”™è¯¯ç´¯ç§¯ã€‚(3)ï¼šç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªé€‚åº”è¶…åˆ†è¾¨ç‡æ–¹æ³•ï¼Œç”¨äºè¯´è¯äººå¤´éƒ¨ç”Ÿæˆæ¡†æ¶ã€‚å— ESRGAN å’Œ Real-ESRGAN ç­‰è¶…åˆ†è¾¨ç‡æ–¹æ³•çš„å¯å‘ï¼Œè¯¥æ–¹æ³•é€šè¿‡å‹ç¼©å’Œä¸‹é‡‡æ ·é«˜è´¨é‡å›¾åƒæ¥æ„å»ºç”¨äºæˆå¯¹è®­ç»ƒçš„ä½è´¨é‡å›¾åƒæ•°æ®ã€‚å®ƒé€šè¿‡ç‹¬ç‰¹è®¾è®¡çš„ç¼–ç å™¨-è§£ç å™¨ç»“æ„ä»ä½è´¨é‡å›¾åƒä¸­è‡ªé€‚åº”åœ°æ•è·é«˜é¢‘ä¿¡æ¯ä»¥è¿›è¡Œé‡å»ºã€‚(4)ï¼šæ–¹æ³•æ€§èƒ½ï¼šè¯¥æ–¹æ³•åœ¨å®šé‡å’Œå®šæ€§å®éªŒä¸­éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸ç°æœ‰çš„å•é•œå¤´è¯´è¯äººå¤´éƒ¨ç”Ÿæˆæ–¹æ³•è¿›è¡Œäº†å¯¹æ¯”ã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å§‹ç»ˆé€šè¿‡ä¸€ç§ç®€å•æœ‰æ•ˆçš„ç­–ç•¥æé«˜äº†ç”Ÿæˆè§†é¢‘çš„è´¨é‡ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) å— ESRGAN å’Œ Real-ESRGAN ç­‰è¶…åˆ†è¾¨ç‡æ–¹æ³•å¯å‘ï¼Œé€šè¿‡å‹ç¼©å’Œä¸‹é‡‡æ ·é«˜è´¨é‡å›¾åƒï¼Œæ„å»ºç”¨äºæˆå¯¹è®­ç»ƒçš„ä½è´¨é‡å›¾åƒæ•°æ®ï¼›(2) é€šè¿‡ç‹¬ç‰¹è®¾è®¡çš„ç¼–ç å™¨-è§£ç å™¨ç»“æ„ï¼Œä»ä½è´¨é‡å›¾åƒä¸­è‡ªé€‚åº”åœ°æ•è·é«˜é¢‘ä¿¡æ¯ä»¥è¿›è¡Œé‡å»ºã€‚</p></li><li><p>æ€»ç»“ï¼šï¼ˆ1ï¼‰æœ¬å·¥ä½œçš„é‡è¦æ„ä¹‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªé€‚åº”è¶…åˆ†è¾¨ç‡æ–¹æ³•ï¼Œç”¨äºå•é•œå¤´è¯´è¯äººå¤´éƒ¨è§†é¢‘ç”Ÿæˆé¢†åŸŸã€‚é€šè¿‡è®¾è®¡ç®€å•ä½†æœ‰æ•ˆçš„æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿä»ä½è´¨é‡å›¾åƒä¸­æ•è·é«˜é¢‘ç»†èŠ‚ã€‚è¿™ä½¿å¾—æ— éœ€é¢å¤–çš„é¢„è®­ç»ƒæ¨¡å—æˆ–åå¤„ç†å³å¯åˆæˆé«˜è´¨é‡è§†é¢‘ã€‚åœ¨å¤§å‹æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®šé‡å’Œå®šæ€§è¯„ä¼°è¯å®ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é«˜è´¨é‡å¯é©±åŠ¨äººè„¸è§†é¢‘ç”Ÿæˆæ–¹é¢è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼šå— ESRGAN å’Œ Real-ESRGAN ç­‰è¶…åˆ†è¾¨ç‡æ–¹æ³•çš„å¯å‘ï¼Œé€šè¿‡å‹ç¼©å’Œä¸‹é‡‡æ ·é«˜è´¨é‡å›¾åƒï¼Œæ„å»ºç”¨äºæˆå¯¹è®­ç»ƒçš„ä½è´¨é‡å›¾åƒæ•°æ®ã€‚é€šè¿‡ç‹¬ç‰¹è®¾è®¡çš„ç¼–ç å™¨-è§£ç å™¨ç»“æ„ï¼Œä»ä½è´¨é‡å›¾åƒä¸­è‡ªé€‚åº”åœ°æ•è·é«˜é¢‘ä¿¡æ¯ä»¥è¿›è¡Œé‡å»ºã€‚æ€§èƒ½ï¼šè¯¥æ–¹æ³•åœ¨å®šé‡å’Œå®šæ€§å®éªŒä¸­éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸ç°æœ‰çš„å•é•œå¤´è¯´è¯äººå¤´éƒ¨ç”Ÿæˆæ–¹æ³•è¿›è¡Œäº†å¯¹æ¯”ã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å§‹ç»ˆé€šè¿‡ä¸€ç§ç®€å•æœ‰æ•ˆçš„ç­–ç•¥æé«˜äº†ç”Ÿæˆè§†é¢‘çš„è´¨é‡ã€‚å·¥ä½œé‡ï¼šè¯¥æ–¹æ³•çš„å®ç°ç›¸å¯¹ç®€å•ï¼Œå¹¶ä¸”ä¸éœ€è¦é¢å¤–çš„é¢„è®­ç»ƒæ¨¡å—æˆ–åå¤„ç†æ­¥éª¤ã€‚è¿™ä½¿å¾—è¯¥æ–¹æ³•åœ¨è®¡ç®—å’Œæ—¶é—´æ–¹é¢éƒ½å…·æœ‰æˆæœ¬æ•ˆç›Šã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-fbfc28956b0106142272e9ccedb9ced5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-188e4004db88e63a7e920e9ac2f3636d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4b69fbe4c0930a57ff002ead5463e3ef.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e3a0dd3488e1d1a03f494038c2fcb247.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-03-28  Deepfake Generation and Detection A Benchmark and Survey</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/03/28/Paper/2024-03-28/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/03/28/Paper/2024-03-28/Diffusion%20Models/</id>
    <published>2024-03-28T02:56:57.000Z</published>
    <updated>2024-03-28T02:56:57.223Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-03-28-æ›´æ–°"><a href="#2024-03-28-æ›´æ–°" class="headerlink" title="2024-03-28 æ›´æ–°"></a>2024-03-28 æ›´æ–°</h1><h2 id="AID-Attention-Interpolation-of-Text-to-Image-Diffusion"><a href="#AID-Attention-Interpolation-of-Text-to-Image-Diffusion" class="headerlink" title="AID: Attention Interpolation of Text-to-Image Diffusion"></a>AID: Attention Interpolation of Text-to-Image Diffusion</h2><p><strong>Authors:Qiyuan He, Jinghao Wang, Ziwei Liu, Angela Yao</strong></p><p>Conditional diffusion models can create unseen images in various settings, aiding image interpolation. Interpolation in latent spaces is well-studied, but interpolation with specific conditions like text or poses is less understood. Simple approaches, such as linear interpolation in the space of conditions, often result in images that lack consistency, smoothness, and fidelity. To that end, we introduce a novel training-free technique named Attention Interpolation via Diffusion (AID). Our key contributions include 1) proposing an inner/outer interpolated attention layer; 2) fusing the interpolated attention with self-attention to boost fidelity; and 3) applying beta distribution to selection to increase smoothness. We also present a variant, Prompt-guided Attention Interpolation via Diffusion (PAID), that considers interpolation as a condition-dependent generative process. This method enables the creation of new images with greater consistency, smoothness, and efficiency, and offers control over the exact path of interpolation. Our approach demonstrates effectiveness for conceptual and spatial interpolation. Code and demo are available at <a href="https://github.com/QY-H00/attention-interpolation-diffusion">https://github.com/QY-H00/attention-interpolation-diffusion</a>. </p><p><a href="http://arxiv.org/abs/2403.17924v1">PDF</a> </p><p><strong>æ‘˜è¦</strong><br>æ³¨æ„åŠ›æ’å€¼æ‰©æ•£ï¼ˆAIDï¼‰ï¼šä¸€ç§æ— éœ€è®­ç»ƒçš„æ¡ä»¶æ’å€¼æ–°æŠ€æœ¯ï¼Œå¯ç”Ÿæˆé«˜åº¦ä¸€è‡´ã€å¹³æ»‘ä¸”é€¼çœŸçš„å›¾åƒã€‚</p><p><strong>è¦ç‚¹</strong></p><ul><li>æå‡ºå†…å±‚/å¤–å±‚æ’å€¼æ³¨æ„åŠ›å±‚ï¼Œä»¥å¢å¼ºæ’å€¼è´¨é‡ã€‚</li><li>èåˆæ’å€¼æ³¨æ„åŠ›å’Œè‡ªæ³¨æ„åŠ›ï¼Œæå‡ç”Ÿæˆå›¾åƒçš„ä¿çœŸåº¦ã€‚</li><li>åº”ç”¨è´å¡”åˆ†å¸ƒé€‰æ‹©ï¼Œæé«˜æ’å€¼çš„å¹³æ»‘åº¦ã€‚</li><li>æå‡ºæç¤ºå¼•å¯¼çš„æ³¨æ„åŠ›æ’å€¼æ‰©æ•£ï¼ˆPAIDï¼‰å˜ä½“ï¼Œå°†æ’å€¼è§†ä¸ºæ¡ä»¶ä¾èµ–çš„ç”Ÿæˆè¿‡ç¨‹ã€‚</li><li>æ§åˆ¶æ’å€¼çš„ç¡®åˆ‡è·¯å¾„ï¼Œç”Ÿæˆå…·æœ‰æ›´é«˜ä¸€è‡´æ€§ã€å¹³æ»‘æ€§å’Œæ•ˆç‡çš„æ–°å›¾åƒã€‚</li><li>åœ¨æ¦‚å¿µå’Œç©ºé—´æ’å€¼æ–¹é¢è¡¨ç°å‡ºæœ‰æ•ˆæ€§ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šAIDï¼šæ–‡æœ¬åˆ°å›¾åƒçš„æ³¨æ„æ’å€¼</li><li>ä½œè€…ï¼šé½æºä½•ã€æ™¯æµ©ç‹ã€å­ä¸ºåˆ˜ã€å®‰å‰æ‹‰å§š</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šæ–°åŠ å¡å›½ç«‹å¤§å­¦</li><li>å…³é”®è¯ï¼šæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£ã€æ¡ä»¶æ‰©æ•£æ¨¡å‹ã€æ³¨æ„æœºåˆ¶ã€æ’å€¼</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.17924    Github ä»£ç é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šæ¡ä»¶æ‰©æ•£æ¨¡å‹å¯ä»¥ç”Ÿæˆå„ç§åœºæ™¯ä¸­çš„å›¾åƒï¼Œæœ‰åŠ©äºå›¾åƒæ’å€¼ã€‚åœ¨æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ’å€¼å·²ç»å¾—åˆ°å……åˆ†ç ”ç©¶ï¼Œä½†ä½¿ç”¨ç‰¹å®šæ¡ä»¶ï¼ˆå¦‚æ–‡æœ¬æˆ–å§¿åŠ¿ï¼‰è¿›è¡Œæ’å€¼çš„ç ”ç©¶è¾ƒå°‘ã€‚(2) è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç®€å•çš„æ–¹æ³•ï¼Œä¾‹å¦‚åœ¨æ¡ä»¶ç©ºé—´ä¸­è¿›è¡Œçº¿æ€§æ’å€¼ï¼Œé€šå¸¸ä¼šå¯¼è‡´å›¾åƒç¼ºä¹ä¸€è‡´æ€§ã€å¹³æ»‘æ€§å’Œä¿çœŸåº¦ã€‚æœ¬æ–‡æå‡ºçš„æ–¹æ³•åŠ¨æœºæ˜ç¡®ã€‚(3) ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º AID çš„æ–°é¢–å…è®­ç»ƒæŠ€æœ¯ï¼Œå³é€šè¿‡æ‰©æ•£è¿›è¡Œæ³¨æ„æ’å€¼ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨æ¡ä»¶ç©ºé—´ä¸­å¼•å…¥æ³¨æ„æœºåˆ¶æ¥æŒ‡å¯¼æ’å€¼è¿‡ç¨‹ï¼Œä»è€Œç¡®ä¿å›¾åƒåœ¨å¸ƒå±€å’Œæ¦‚å¿µä¸Šçš„å¹³æ»‘è¿‡æ¸¡ã€‚(4) æ–¹æ³•æ€§èƒ½ï¼šè¯¥æ–¹æ³•åœ¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œåœ¨ç©ºé—´å’Œæ¦‚å¿µæ’å€¼ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾ç€æ”¹è¿›ã€‚å®éªŒç»“æœæ”¯æŒäº†æœ¬æ–‡æå‡ºçš„æ–¹æ³•ç›®æ ‡ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) å†…/å¤–æ’å€¼æ³¨æ„åŠ›æœºåˆ¶ï¼šé€šè¿‡åœ¨æ¡ä»¶ç©ºé—´ä¸­å¼•å…¥æ³¨æ„åŠ›æœºåˆ¶ï¼ŒæŒ‡å¯¼æ’å€¼è¿‡ç¨‹ï¼Œç¡®ä¿å›¾åƒåœ¨å¸ƒå±€å’Œæ¦‚å¿µä¸Šçš„å¹³æ»‘è¿‡æ¸¡ã€‚(2) ä¸è‡ªæ³¨æ„åŠ›èåˆï¼šå°†æ’å€¼æ½œå˜é‡æœ¬èº«çš„é”®å’Œå€¼èå…¥æ’å€¼æ³¨æ„åŠ›æœºåˆ¶ï¼Œæé«˜ä¸€è‡´æ€§å’Œä¿çœŸåº¦ã€‚(3) Beta å…ˆéªŒåºåˆ—é€‰æ‹©ï¼šé‡‡ç”¨ Beta åˆ†å¸ƒé€‰æ‹©æ’å€¼è·¯å¾„ä¸Šçš„ç‰¹å®šæ’å€¼å›¾åƒï¼Œä½¿ç”Ÿæˆçš„å›¾åƒåºåˆ—æ›´å¹³æ»‘ã€‚(4) æç¤ºå¼•å¯¼ï¼šé€šè¿‡æ³¨å…¥æç¤ºä½œä¸ºæ¡ä»¶ï¼Œæ§åˆ¶æ’å€¼è·¯å¾„ï¼Œç”Ÿæˆç¬¦åˆæ–‡æœ¬æè¿°çš„æ’å€¼åºåˆ—ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œé¦–æ¬¡æå‡ºæ¡ä»¶æ’å€¼ä»»åŠ¡åŠç›¸å…³è¯„ä¼°æŒ‡æ ‡ï¼ŒåŒ…æ‹¬ä¸€è‡´æ€§ã€å¹³æ»‘æ€§å’Œä¿çœŸåº¦ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç§°ä¸º AID çš„æ–°é¢–æ–¹æ³•ï¼Œç”¨äºåœ¨æ‰©æ•£æ¨¡å‹ä¸­ç”Ÿæˆæ¡ä»¶æ’å€¼å›¾åƒã€‚è¯¥æ–¹æ³•åœ¨æ— éœ€è®­ç»ƒçš„æƒ…å†µä¸‹æ˜¾ç€è¶…è¶Šäº†åŸºå‡†ï¼Œé€šè¿‡å®šæ€§å’Œå®šé‡åˆ†æå¾—åˆ°äº†è¯æ˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº† PAIDï¼Œè¯¥æ‰©å±•å…è®¸ç”¨æˆ·ä½¿ç”¨å¼•å¯¼æç¤ºæ¥é€‰æ‹©æ’å€¼è·¯å¾„ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ— éœ€è®­ç»ƒï¼Œæ‹“å®½äº†ç”Ÿæˆæ¨¡å‹æ’å€¼çš„èŒƒå›´ï¼Œä¸ºåˆæˆç”Ÿæˆã€å›¾åƒç¼–è¾‘ã€æ•°æ®å¢å¼ºå’Œè§†é¢‘æ’å€¼ç­‰å„ç§åº”ç”¨å¼€è¾Ÿäº†æ–°æœºé‡ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºæ¡ä»¶æ’å€¼ä»»åŠ¡åŠè¯„ä¼°æŒ‡æ ‡ï¼Œå¼•å…¥æ³¨æ„åŠ›æœºåˆ¶æŒ‡å¯¼æ’å€¼è¿‡ç¨‹ï¼Œæ— éœ€è®­ç»ƒå³å¯ç”Ÿæˆé«˜è´¨é‡æ’å€¼å›¾åƒã€‚æ€§èƒ½ï¼šåœ¨ç©ºé—´å’Œæ¦‚å¿µæ’å€¼ä»»åŠ¡ä¸Šå–å¾—æ˜¾ç€æ”¹è¿›ï¼Œå®šæ€§å’Œå®šé‡è¯„ä¼°å‡æ”¯æŒè¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚å·¥ä½œé‡ï¼šæå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ’å€¼æ–¹æ³•ï¼Œå‡å°‘äº†è®­ç»ƒè´Ÿæ‹…ï¼Œæé«˜äº†æ’å€¼æ•ˆç‡ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-aaa47516c2e21df63c1ee81eb0afd555.jpg" align="middle"></details><h2 id="AniPortrait-Audio-Driven-Synthesis-of-Photorealistic-Portrait-Animation"><a href="#AniPortrait-Audio-Driven-Synthesis-of-Photorealistic-Portrait-Animation" class="headerlink" title="AniPortrait: Audio-Driven Synthesis of Photorealistic Portrait Animation"></a>AniPortrait: Audio-Driven Synthesis of Photorealistic Portrait Animation</h2><p><strong>Authors:Huawei Wei, Zejun Yang, Zhisheng Wang</strong></p><p>In this study, we propose AniPortrait, a novel framework for generating high-quality animation driven by audio and a reference portrait image. Our methodology is divided into two stages. Initially, we extract 3D intermediate representations from audio and project them into a sequence of 2D facial landmarks. Subsequently, we employ a robust diffusion model, coupled with a motion module, to convert the landmark sequence into photorealistic and temporally consistent portrait animation. Experimental results demonstrate the superiority of AniPortrait in terms of facial naturalness, pose diversity, and visual quality, thereby offering an enhanced perceptual experience. Moreover, our methodology exhibits considerable potential in terms of flexibility and controllability, which can be effectively applied in areas such as facial motion editing or face reenactment. We release code and model weights at <a href="https://github.com/scutzzj/AniPortrait">https://github.com/scutzzj/AniPortrait</a> </p><p><a href="http://arxiv.org/abs/2403.17694v1">PDF</a> </p><p><strong>Summary</strong><br>åˆ©ç”¨éŸ³é¢‘å’Œå‚è€ƒè‚–åƒå›¾åƒç”Ÿæˆé«˜å“è´¨åŠ¨ç”»çš„æ–°é¢–æ¡†æ¶ï¼šAniPortrait</p><p><strong>Key Takeaways</strong></p><ul><li>AniPortrait æå‡ºäº†ä¸€ç§ç”±éŸ³é¢‘å’Œå‚è€ƒè‚–åƒå›¾åƒé©±åŠ¨çš„é«˜è´¨é‡åŠ¨ç”»ç”Ÿæˆæ–°æ¡†æ¶ã€‚</li><li>AniPortrait åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šä»éŸ³é¢‘ä¸­æå– 3D ä¸­é—´è¡¨ç¤ºå¹¶å°†å…¶æŠ•å½±åˆ° 2D é¢éƒ¨åœ°æ ‡åºåˆ—ä¸­ã€‚</li><li>AniPortrait ä½¿ç”¨ç¨³å¥çš„æ‰©æ•£æ¨¡å‹å’Œè¿åŠ¨æ¨¡å—å°†åœ°æ ‡åºåˆ—è½¬æ¢ä¸ºé€¼çœŸçš„ã€æ—¶é—´ä¸€è‡´çš„è‚–åƒåŠ¨ç”»ã€‚</li><li>AniPortrait åœ¨é¢éƒ¨è‡ªç„¶åº¦ã€å§¿åŠ¿å¤šæ ·æ€§å’Œè§†è§‰è´¨é‡æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</li><li>AniPortrait åœ¨çµæ´»æ€§å’Œå¯æ§æ€§æ–¹é¢è¡¨ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œå¯æœ‰æ•ˆåº”ç”¨äºé¢éƒ¨åŠ¨ä½œç¼–è¾‘æˆ–é¢éƒ¨é‡å»ºç­‰é¢†åŸŸã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šAniPortraitï¼šéŸ³é¢‘é©±åŠ¨çš„å†™å®è‚–åƒåŠ¨ç”»åˆæˆ</li><li>ä½œè€…ï¼šWei Huawei<em>ã€Yang Zejun</em>ã€Wang Zhisheng</li><li>å•ä½ï¼šè…¾è®¯</li><li>å…³é”®è¯ï¼šéŸ³é¢‘é©±åŠ¨ã€è‚–åƒåŠ¨ç”»ã€æ‰©æ•£æ¨¡å‹ã€åŠ¨ä½œæ¨¡å—</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.17694   Githubï¼šNone</li><li>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šä»éŸ³é¢‘å’Œé™æ€å›¾åƒç”Ÿæˆé€¼çœŸä¸”å¯Œæœ‰è¡¨ç°åŠ›çš„è‚–åƒåŠ¨ç”»å…·æœ‰å¹¿æ³›çš„åº”ç”¨ï¼Œä½†åˆ¶ä½œè§†è§‰ä¸Šå¼•äººå…¥èƒœä¸”ä¿æŒæ—¶é—´ä¸€è‡´æ€§çš„é«˜è´¨é‡åŠ¨ç”»æ˜¯ä¸€é¡¹é‡å¤§æŒ‘æˆ˜ã€‚   ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç°æœ‰æ–¹æ³•é€šå¸¸æ— æ³•å…‹æœè¿™ä¸€æŒ‘æˆ˜ï¼Œä¸»è¦åŸå› æ˜¯å®ƒä»¬ä¾èµ–äºå®¹é‡æœ‰é™çš„è§†è§‰å†…å®¹ç”Ÿæˆå™¨ï¼Œä¾‹å¦‚ GANã€NeRF æˆ–åŸºäºè¿åŠ¨çš„è§£ç å™¨ã€‚è¿™äº›ç½‘ç»œè¡¨ç°å‡ºæœ‰é™çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶ä¸”åœ¨ç”Ÿæˆé«˜è´¨é‡å†…å®¹æ—¶å¾€å¾€ç¼ºä¹ç¨³å®šæ€§ã€‚   ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡º AniPortraitï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œæ—¨åœ¨ç”Ÿæˆç”±éŸ³é¢‘å’Œå‚è€ƒå›¾åƒé©±åŠ¨çš„ä¼˜è´¨åŠ¨ç”»è‚–åƒã€‚AniPortrait åˆ†ä¸ºä¸¤ä¸ªä¸åŒçš„é˜¶æ®µã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬ä½¿ç”¨åŸºäº Transformer çš„æ¨¡å‹ä»éŸ³é¢‘ä¸­æå– 3D ä¸­é—´è¡¨ç¤ºï¼Œå¹¶å°†å…¶æŠ•å½±åˆ° 2D é¢éƒ¨åœ°æ ‡åºåˆ—ä¸­ã€‚éšåï¼Œæˆ‘ä»¬é‡‡ç”¨ç¨³å¥çš„æ‰©æ•£æ¨¡å‹ï¼Œç»“åˆè¿åŠ¨æ¨¡å—ï¼Œå°†åœ°æ ‡åºåˆ—è½¬æ¢ä¸ºé€¼çœŸçš„ã€æ—¶é—´ä¸€è‡´çš„è‚–åƒåŠ¨ç”»ã€‚   ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šå®éªŒç»“æœè¯æ˜äº† AniPortrait åœ¨é¢éƒ¨è‡ªç„¶åº¦ã€å§¿åŠ¿å¤šæ ·æ€§å’Œè§†è§‰è´¨é‡æ–¹é¢çš„ä¼˜è¶Šæ€§ï¼Œä»è€Œæä¾›äº†å¢å¼ºçš„æ„ŸçŸ¥ä½“éªŒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨çµæ´»æ€§å’Œå¯æ§æ€§æ–¹é¢è¡¨ç°å‡ºç›¸å½“å¤§çš„æ½œåŠ›ï¼Œå¯ä»¥æœ‰æ•ˆåœ°åº”ç”¨äºé¢éƒ¨åŠ¨ä½œç¼–è¾‘æˆ–é¢éƒ¨é‡ç°ç­‰é¢†åŸŸã€‚</li></ol><p>7.æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼š<strong>Audio2Lmk</strong>ï¼šä»éŸ³é¢‘ä¸­æå– 3D é¢éƒ¨ç½‘æ ¼åºåˆ—å’Œä½å§¿åºåˆ—ã€‚ï¼ˆ2ï¼‰ï¼š<strong>Lmk2Video</strong>ï¼šå°†é¢éƒ¨åœ°æ ‡åºåˆ—è½¬æ¢ä¸ºæ—¶é—´ä¸€è‡´çš„è‚–åƒåŠ¨ç”»ã€‚</p><ol><li>ç»“è®ºï¼š(1): æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ AniPortraitï¼Œè¯¥æ¡†æ¶å¯ä»¥ç”Ÿæˆç”±éŸ³é¢‘å’Œå‚è€ƒå›¾åƒé©±åŠ¨çš„ä¼˜è´¨åŠ¨ç”»è‚–åƒã€‚AniPortrait é‡‡ç”¨åŸºäº Transformer çš„æ¨¡å‹ä»éŸ³é¢‘ä¸­æå– 3D ä¸­é—´è¡¨ç¤ºï¼Œå¹¶ä½¿ç”¨ç¨³å¥çš„æ‰©æ•£æ¨¡å‹ç»“åˆè¿åŠ¨æ¨¡å—å°†å…¶è½¬æ¢ä¸ºé€¼çœŸçš„ã€æ—¶é—´ä¸€è‡´çš„è‚–åƒåŠ¨ç”»ã€‚å®éªŒç»“æœè¯æ˜äº† AniPortrait åœ¨é¢éƒ¨è‡ªç„¶åº¦ã€å§¿åŠ¿å¤šæ ·æ€§å’Œè§†è§‰è´¨é‡æ–¹é¢çš„ä¼˜è¶Šæ€§ï¼Œä»è€Œæä¾›äº†å¢å¼ºçš„æ„ŸçŸ¥ä½“éªŒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨çµæ´»æ€§å’Œå¯æ§æ€§æ–¹é¢è¡¨ç°å‡ºç›¸å½“å¤§çš„æ½œåŠ›ï¼Œå¯ä»¥æœ‰æ•ˆåœ°åº”ç”¨äºé¢éƒ¨åŠ¨ä½œç¼–è¾‘æˆ–é¢éƒ¨é‡ç°ç­‰é¢†åŸŸã€‚(2): åˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ AniPortraitï¼Œè¯¥æ¡†æ¶å¯ä»¥ä»éŸ³é¢‘å’Œå‚è€ƒå›¾åƒç”Ÿæˆé€¼çœŸçš„åŠ¨ç”»è‚–åƒã€‚</li><li>é‡‡ç”¨åŸºäº Transformer çš„æ¨¡å‹ä»éŸ³é¢‘ä¸­æå– 3D ä¸­é—´è¡¨ç¤ºï¼Œå¹¶ä½¿ç”¨ç¨³å¥çš„æ‰©æ•£æ¨¡å‹ç»“åˆè¿åŠ¨æ¨¡å—å°†å…¶è½¬æ¢ä¸ºæ—¶é—´ä¸€è‡´çš„è‚–åƒåŠ¨ç”»ã€‚</li><li>AniPortrait åœ¨é¢éƒ¨è‡ªç„¶åº¦ã€å§¿åŠ¿å¤šæ ·æ€§å’Œè§†è§‰è´¨é‡æ–¹é¢è¡¨ç°å‡ºä¼˜è¶Šæ€§ï¼Œä»è€Œæä¾›äº†å¢å¼ºçš„æ„ŸçŸ¥ä½“éªŒã€‚</li><li>AniPortrait åœ¨çµæ´»æ€§å’Œå¯æ§æ€§æ–¹é¢è¡¨ç°å‡ºç›¸å½“å¤§çš„æ½œåŠ›ï¼Œå¯ä»¥æœ‰æ•ˆåœ°åº”ç”¨äºé¢éƒ¨åŠ¨ä½œç¼–è¾‘æˆ–é¢éƒ¨é‡ç°ç­‰é¢†åŸŸã€‚æ€§èƒ½ï¼š</li><li>AniPortrait åœ¨é¢éƒ¨è‡ªç„¶åº¦ã€å§¿åŠ¿å¤šæ ·æ€§å’Œè§†è§‰è´¨é‡æ–¹é¢è¡¨ç°å‡ºä¼˜è¶Šæ€§ï¼Œä»è€Œæä¾›äº†å¢å¼ºçš„æ„ŸçŸ¥ä½“éªŒã€‚</li><li>AniPortrait åœ¨çµæ´»æ€§å’Œå¯æ§æ€§æ–¹é¢è¡¨ç°å‡ºç›¸å½“å¤§çš„æ½œåŠ›ï¼Œå¯ä»¥æœ‰æ•ˆåœ°åº”ç”¨äºé¢éƒ¨åŠ¨ä½œç¼–è¾‘æˆ–é¢éƒ¨é‡ç°ç­‰é¢†åŸŸã€‚å·¥ä½œé‡ï¼š</li><li>AniPortrait çš„å®ç°éœ€è¦ä¸€å®šçš„æŠ€æœ¯å®åŠ›ï¼ŒåŒ…æ‹¬å¯¹ Transformer æ¨¡å‹ã€æ‰©æ•£æ¨¡å‹å’Œè¿åŠ¨æ¨¡å—çš„ç†è§£ã€‚</li><li>è®­ç»ƒ AniPortrait æ¨¡å‹éœ€è¦å¤§é‡çš„æ•°æ®å’Œè®¡ç®—èµ„æºã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-a0703eb6ac9807d377c7bbfaa84e3681.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-fc2d139237100aad689f67180ae398bf.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e35074ee634942aebc5c8860cf29e344.jpg" align="middle"></details><h2 id="DiffFAE-Advancing-High-fidelity-One-shot-Facial-Appearance-Editing-with-Space-sensitive-Customization-and-Semantic-Preservation"><a href="#DiffFAE-Advancing-High-fidelity-One-shot-Facial-Appearance-Editing-with-Space-sensitive-Customization-and-Semantic-Preservation" class="headerlink" title="DiffFAE: Advancing High-fidelity One-shot Facial Appearance Editing with   Space-sensitive Customization and Semantic Preservation"></a>DiffFAE: Advancing High-fidelity One-shot Facial Appearance Editing with   Space-sensitive Customization and Semantic Preservation</h2><p><strong>Authors:Qilin Wang, Jiangning Zhang, Chengming Xu, Weijian Cao, Ying Tai, Yue Han, Yanhao Ge, Hong Gu, Chengjie Wang, Yanwei Fu</strong></p><p>Facial Appearance Editing (FAE) aims to modify physical attributes, such as pose, expression and lighting, of human facial images while preserving attributes like identity and background, showing great importance in photograph. In spite of the great progress in this area, current researches generally meet three challenges: low generation fidelity, poor attribute preservation, and inefficient inference. To overcome above challenges, this paper presents DiffFAE, a one-stage and highly-efficient diffusion-based framework tailored for high-fidelity FAE. For high-fidelity query attributes transfer, we adopt Space-sensitive Physical Customization (SPC), which ensures the fidelity and generalization ability by utilizing rendering texture derived from 3D Morphable Model (3DMM). In order to preserve source attributes, we introduce the Region-responsive Semantic Composition (RSC). This module is guided to learn decoupled source-regarding features, thereby better preserving the identity and alleviating artifacts from non-facial attributes such as hair, clothes, and background. We further introduce a consistency regularization for our pipeline to enhance editing controllability by leveraging prior knowledge in the attention matrices of diffusion model. Extensive experiments demonstrate the superiority of DiffFAE over existing methods, achieving state-of-the-art performance in facial appearance editing. </p><p><a href="http://arxiv.org/abs/2403.17664v1">PDF</a> </p><p><strong>Summary</strong><br>å›¾åƒä¸­äººè„¸å¤–è§‚ç¼–è¾‘çš„æ‰©æ•£æ¨¡å‹ DiffFAE æé«˜äº†ç”Ÿæˆä¿çœŸåº¦ã€å±æ€§ä¿ç•™å’Œæ¨ç†æ•ˆç‡ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>é‡‡ç”¨ç©ºé—´æ•æ„Ÿç‰©ç†å®šåˆ¶ (SPC) ç¡®ä¿æŸ¥è¯¢å±æ€§è½¬ç§»çš„ä¿çœŸåº¦å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li><li>å¼•å…¥åŒºåŸŸå“åº”è¯­ä¹‰ç»„åˆ (RSC) ä¿ç•™æºå±æ€§ï¼Œå‡è½»éé¢éƒ¨å±æ€§ï¼ˆå¦‚å¤´å‘ã€è¡£æœå’ŒèƒŒæ™¯ï¼‰å¸¦æ¥çš„ä¼ªå½±ã€‚</li><li>æå‡ºä¸€è‡´æ€§æ­£åˆ™åŒ–ï¼Œé€šè¿‡åˆ©ç”¨æ‰©æ•£æ¨¡å‹æ³¨æ„åŠ›çŸ©é˜µä¸­çš„å…ˆéªŒçŸ¥è¯†å¢å¼ºç¼–è¾‘å¯æ§æ€§ã€‚</li><li>DiffFAE åœ¨äººè„¸å¤–è§‚ç¼–è¾‘ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¶…è¶Šç°æœ‰æ–¹æ³•ã€‚</li><li>DiffFAE å¯ä»¥æœ‰æ•ˆå¤„ç†äººè„¸å¤–è§‚ç¼–è¾‘ä¸­çš„ä½ç”Ÿæˆä¿çœŸåº¦ã€å·®å±æ€§ä¿ç•™å’Œä½æ¨ç†æ•ˆç‡ç­‰æŒ‘æˆ˜ã€‚</li><li>æ‰©æ•£æ¨¡å‹åœ¨äººè„¸å¤–è§‚ç¼–è¾‘ä»»åŠ¡ä¸­å…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯ã€‚</li><li>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºäººè„¸å¤–è§‚ç¼–è¾‘çš„æ–°é¢–æ¡†æ¶ DiffFAEï¼Œå®ƒç»“åˆäº†æ‰©æ•£æ¨¡å‹ã€ç©ºé—´æ•æ„Ÿç‰©ç†å®šåˆ¶å’ŒåŒºåŸŸå“åº”è¯­ä¹‰ç»„åˆçš„ä¼˜ç‚¹ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šDiffFAEï¼šæ¨è¿›é«˜ä¿çœŸä¸€å‘å¼äººè„¸å¤–è§‚ç¼–è¾‘</li><li>ä½œè€…ï¼šQ. Wang ç­‰</li><li>å•ä½ï¼šæœªæåŠ</li><li>å…³é”®è¯ï¼šFacial appearance editingã€Diffusion modelã€Object-centric learning</li><li>è®ºæ–‡é“¾æ¥ï¼šæœªæä¾›ï¼ŒGithub ä»£ç é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šäººè„¸å¤–è§‚ç¼–è¾‘æ—¨åœ¨ä¿®æ”¹äººè„¸å›¾åƒçš„ç‰©ç†å±æ€§ï¼ˆå¦‚å§¿åŠ¿ã€è¡¨æƒ…å’Œå…‰ç…§ï¼‰ï¼ŒåŒæ—¶ä¿ç•™èº«ä»½å’ŒèƒŒæ™¯ç­‰å±æ€§ï¼Œåœ¨æ‘„å½±ä¸­å…·æœ‰é‡è¦æ„ä¹‰ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šç°æœ‰ç ”ç©¶é€šå¸¸é¢ä¸´ç”Ÿæˆä¿çœŸåº¦ä½ã€å±æ€§ä¿ç•™å·®å’Œæ¨ç†æ•ˆç‡ä½ä¸‰å¤§æŒ‘æˆ˜ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡º DiffFAEï¼Œä¸€ä¸ªé’ˆå¯¹é«˜ä¿çœŸ FAE é‡èº«å®šåˆ¶çš„å•é˜¶æ®µä¸”é«˜æ•ˆçš„åŸºäºæ‰©æ•£çš„æ¡†æ¶ã€‚ä¸ºäº†å®ç°é«˜ä¿çœŸæŸ¥è¯¢å±æ€§è½¬ç§»ï¼Œæˆ‘ä»¬é‡‡ç”¨ç©ºé—´æ•æ„Ÿç‰©ç†å®šåˆ¶ï¼ˆSPCï¼‰ï¼Œå®ƒåˆ©ç”¨æºè‡ª 3D å¯å˜å½¢æ¨¡å‹ï¼ˆ3DMMï¼‰çš„æ¸²æŸ“çº¹ç†ï¼Œç¡®ä¿äº†ä¿çœŸåº¦å’Œæ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†ä¿ç•™æºå±æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŒºåŸŸå“åº”è¯­ä¹‰ç»„åˆï¼ˆRSCï¼‰ã€‚è¯¥æ¨¡å—è¢«å¼•å¯¼å­¦ä¹ è§£è€¦çš„æºç›¸å…³ç‰¹å¾ï¼Œä»è€Œæ›´å¥½åœ°ä¿ç•™èº«ä»½ï¼Œå¹¶å‡è½»æ¥è‡ªéé¢éƒ¨å±æ€§ï¼ˆå¦‚å¤´å‘ã€è¡£æœå’ŒèƒŒæ™¯ï¼‰çš„ä¼ªå½±ã€‚æˆ‘ä»¬è¿˜ä¸ºæˆ‘ä»¬çš„ç®¡é“å¼•å…¥äº†ç¨ å¯†æ­£åˆ™åŒ–ï¼Œé€šè¿‡åˆ©ç”¨æ‰©æ•£æ¨¡å‹æ³¨æ„åŠ›çŸ©é˜µä¸­çš„å…ˆéªŒçŸ¥è¯†æ¥å¢å¼ºç¼–è¾‘å¯æ§æ€§ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šå¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼ŒDiffFAE ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨äººè„¸å¤–è§‚ç¼–è¾‘ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†ä»–ä»¬çš„ç›®æ ‡ã€‚</p></li><li><p><strong>æ–¹æ³•</strong>ï¼šï¼ˆ1ï¼‰<strong>ç©ºé—´æ•æ„Ÿç‰©ç†å®šåˆ¶ï¼ˆSPCï¼‰</strong>ï¼šåˆ©ç”¨æºè‡ª3Då¯å˜å½¢æ¨¡å‹ï¼ˆ3DMMï¼‰çš„æ¸²æŸ“çº¹ç†ï¼Œç¡®ä¿ä¿çœŸåº¦å’Œæ³›åŒ–èƒ½åŠ›ã€‚ï¼ˆ2ï¼‰<strong>åŒºåŸŸå“åº”è¯­ä¹‰ç»„åˆï¼ˆRSCï¼‰</strong>ï¼šå­¦ä¹ è§£è€¦çš„æºç›¸å…³ç‰¹å¾ï¼Œä¿ç•™èº«ä»½ï¼Œå‡è½»éé¢éƒ¨å±æ€§ä¼ªå½±ã€‚ï¼ˆ3ï¼‰<strong>ç¨ å¯†æ­£åˆ™åŒ–</strong>ï¼šåˆ©ç”¨æ‰©æ•£æ¨¡å‹æ³¨æ„åŠ›çŸ©é˜µä¸­çš„å…ˆéªŒçŸ¥è¯†ï¼Œå¢å¼ºç¼–è¾‘å¯æ§æ€§ã€‚</p></li></ol><p>8.ç»“è®ºï¼š(1)ï¼šæœ¬æ–‡é’ˆå¯¹äººè„¸å¤–è§‚ç¼–è¾‘ï¼ˆFAEï¼‰ä¸­å­˜åœ¨çš„ç”Ÿæˆä¿çœŸåº¦ä½ã€å±æ€§ä¿ç•™å·®å’Œæ¨ç†æ•ˆç‡ä½ä¸‰å¤§æŒ‘æˆ˜è¿›è¡Œäº†åˆ†æï¼Œæ¢ç´¢äº†ä¸€ç§åŸºäºå•é˜¶æ®µæ‰©æ•£çš„æ–°æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨ç©ºé—´æ•æ„Ÿç‰©ç†å®šåˆ¶æ¨¡å—æ¥å¤„ç†æŸ¥è¯¢ç‰©ç†å±æ€§ï¼Œå¦‚å§¿åŠ¿ã€è¡¨æƒ…å’Œå…‰ç…§ã€‚åŒæ—¶ï¼Œæå‡ºäº†åŒºåŸŸå“åº”è¯­ä¹‰ç»„åˆæ¥æ›´å¥½åœ°æ§åˆ¶æºç›¸å…³å±æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ VoxCeleb1 æ•°æ®é›†ä¸Šä¸º FAE ä»»åŠ¡è®¾å®šäº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ï¼Œè¿™å¾—åˆ°äº†å¹¿æ³›çš„å®šé‡å’Œå®šæ€§ç»“æœçš„æ”¯æŒã€‚(2)ï¼šåˆ›æ–°ç‚¹ï¼šç©ºé—´æ•æ„Ÿç‰©ç†å®šåˆ¶æ¨¡å—ã€åŒºåŸŸå“åº”è¯­ä¹‰ç»„åˆã€ç¨ å¯†æ­£åˆ™åŒ–ï¼›æ€§èƒ½ï¼šåœ¨ VoxCeleb1 æ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼›å·¥ä½œé‡ï¼šä¸­ç­‰ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-d26cb9d6e12fa2c3ca2894c45c11f62a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2e175e9d0b22d21814f9b545e1b4a47f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-98414ddcc0bdcee2447d896743b3ec8e.jpg" align="middle"></details>## DiffGaze: A Diffusion Model for Continuous Gaze Sequence Generation on   360Â° Images**Authors:Chuhan Jiao, Yao Wang, Guanhua Zhang, Mihai BÃ¢ce, Zhiming Hu, Andreas Bulling**We present DiffGaze, a novel method for generating realistic and diverse continuous human gaze sequences on 360{\deg} images based on a conditional score-based denoising diffusion model. Generating human gaze on 360{\deg} images is important for various human-computer interaction and computer graphics applications, e.g. for creating large-scale eye tracking datasets or for realistic animation of virtual humans. However, existing methods are limited to predicting discrete fixation sequences or aggregated saliency maps, thereby neglecting crucial parts of natural gaze behaviour. Our method uses features extracted from 360{\deg} images as condition and uses two transformers to model the temporal and spatial dependencies of continuous human gaze. We evaluate DiffGaze on two 360{\deg} image benchmarks for gaze sequence generation as well as scanpath prediction and saliency prediction. Our evaluations show that DiffGaze outperforms state-of-the-art methods on all tasks on both benchmarks. We also report a 21-participant user study showing that our method generates gaze sequences that are indistinguishable from real human sequences. [PDF](http://arxiv.org/abs/2403.17477v1) **æ‘˜è¦**åŸºäºæ¡ä»¶åˆ†æ•°å»å™ªæ‰©æ•£æ¨¡å‹ï¼Œæå‡ºäº†ä¸€ç§ç”Ÿæˆ360åº¦å›¾åƒä¸Šé€¼çœŸä¸”å¤šæ ·çš„è¿ç»­äººçœ¼æ³¨è§†åºåˆ—çš„æ–°æ–¹æ³•DiffGazeã€‚**è¦ç‚¹**- æå‡ºäº† DiffGazeï¼Œä¸€ç§ç”¨äºç”Ÿæˆé€¼çœŸä¸”å¤šæ ·çš„ 360 åº¦å›¾åƒçš„è¿ç»­äººçœ¼æ³¨è§†åºåˆ—çš„æ–¹æ³•ã€‚- DiffGaze ä½¿ç”¨ä» 360 åº¦å›¾åƒä¸­æå–çš„ç‰¹å¾ä½œä¸ºæ¡ä»¶ï¼Œå¹¶ä½¿ç”¨ä¸¤ä¸ª Transformer æ¥å»ºæ¨¡è¿ç»­äººçœ¼æ³¨è§†çš„æ—¶é—´å’Œç©ºé—´ä¾èµ–æ€§ã€‚- DiffGaze åœ¨ä¸¤ä¸ªç”¨äºæ³¨è§†åºåˆ—ç”Ÿæˆã€æ‰«æè·¯å¾„é¢„æµ‹å’Œæ˜¾ç€æ€§é¢„æµ‹çš„ 360 åº¦å›¾åƒåŸºå‡†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚- åœ¨ä¸¤ä¸ªåŸºå‡†ä¸Šçš„æ‰€æœ‰ä»»åŠ¡ä¸­ï¼ŒDiffGaze éƒ½ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚- ä¸€é¡¹åŒ…å« 21 åå‚ä¸è€…çš„ç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„çœ¼æ³¨è§†åºåˆ—ä¸çœŸå®çš„äººçœ¼æ³¨è§†åºåˆ—æ— æ³•åŒºåˆ†ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>æ ‡é¢˜ï¼šDiffGazeï¼š360Â° å›¾åƒè¿ç»­æ³¨è§†åºåˆ—ç”Ÿæˆæ‰©æ•£æ¨¡å‹</li><li>ä½œè€…ï¼šChuhan Jiaoã€Yao Wangã€Guanhua Zhangã€Mihai Baceã€Zhiming Huã€Andreas Bulling</li><li>éš¶å±å•ä½ï¼šæ–¯å›¾åŠ ç‰¹å¤§å­¦å¯è§†åŒ–ä¸äº¤äº’ç³»ç»Ÿç ”ç©¶æ‰€</li><li>å…³é”®è¯ï¼šScanpath Prediction; Saliency Modelling; Eye Tracking; Gaze Behaviour Modelling; Eye Movement Synthesis</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.17477   Github ä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šéšç€ç›¸æœºæŠ€æœ¯çš„è¿›æ­¥ï¼Œé«˜åˆ†è¾¨ç‡ 360Â° å›¾åƒçš„æ•æ‰ä¸ºè™šæ‹Ÿç°å® (VR) ä¸­çš„æ–°ä¸€ä»£æ²‰æµ¸å¼ä½“éªŒæä¾›äº†å¯èƒ½ã€‚è¿™å¼•å‘äº†æ¶ˆè´¹è€…é‡‡ç”¨è¿™é¡¹æ–°æŠ€æœ¯çš„å…´è¶£ï¼Œå¹¶ä¿ƒè¿›äº†ç†è§£äººç±»å¦‚ä½•æ„ŸçŸ¥å’Œæ¢ç´¢è¿™äº› 3D è™šæ‹Ÿç¯å¢ƒçš„ç ”ç©¶å·¥ä½œã€‚è§†è§‰æ³¨æ„åŠ›æ˜¯æ¢ç´¢è¿‡ç¨‹ä¸­çš„ä¸€ä¸ªç‰¹åˆ«ä¸°å¯Œçš„çš„ä¿¡æ¯æ¥æºï¼Œé€šå¸¸ä»¥ä½¿ç”¨çœ¼åŠ¨è¿½è¸ªæ”¶é›†çš„æ³¨è§†æ•°æ®å½¢å¼è¿›è¡Œåˆ†æã€‚å°½ç®¡çœ¼åŠ¨è¿½è¸ªå˜å¾—æ›´åŠ å¹¿æ³›å’Œç»æµå®æƒ ï¼Œè€Œä¸”è¢«é›†æˆåˆ°è¶Šæ¥è¶Šå¤šçš„ VR å¤´æ˜¾ä¸­ï¼Œä½†æ”¶é›†æ³¨è§†æ•°æ®ï¼ˆå°¤å…¶æ˜¯åœ¨å¤§è§„æ¨¡çš„æƒ…å†µä¸‹ï¼‰ä»ç„¶å¾ˆç¹çä¸”è€—æ—¶ï¼Œè€Œä¸”é€šå¸¸æ ¹æœ¬ä¸å¯è¡Œã€‚è¿™å¼•å‘äº†å¯¹è§†è§‰æ³¨æ„åŠ›è®¡ç®—æ¨¡å‹çš„ç ”ç©¶ï¼Œå³æ— éœ€ä¸“ç”¨çœ¼åŠ¨è¿½è¸ªè®¾å¤‡å°±èƒ½é¢„æµ‹ 360Â° å›¾åƒä¸Šäººç±»æ³¨è§†çš„æ¨¡å‹ã€‚</li></ol><p>ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šå…ˆå‰å…³äº 360Â° å›¾åƒä¸Šè§†è§‰æ³¨æ„åŠ›è®¡ç®—å»ºæ¨¡çš„å·¥ä½œä¸»è¦é›†ä¸­åœ¨æ˜¾ç€æ€§æˆ–æ‰«æè·¯å¾„é¢„æµ‹ä¸Šã€‚å°½ç®¡å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†è¿™ä¸¤é¡¹ä»»åŠ¡ä»ç„¶åªè§£å†³äº†ç®€åŒ–çš„é—®é¢˜ï¼šè™½ç„¶èšåˆæ˜¾ç€æ€§å›¾ä¸éœ€è¦å¯¹äººç±»æ³¨è§†è¡Œä¸ºçš„æ—¶é—´ç‰¹æ€§è¿›è¡Œå»ºæ¨¡ï¼Œä½†é¢„æµ‹ç¦»æ•£æ³¨è§†å›ºå®šï¼ˆæ‰«æè·¯å¾„ï¼‰çš„åºåˆ—åœ¨æ—¶é—´ä¸Šä»ç„¶ç²—ç³™ï¼Œå¹¶ä¸”å¿½ç•¥äº†å›ºå®šä¹‹é—´çš„ä¸°å¯Œæ³¨è§†æ•°æ®ã€‚å› æ­¤ï¼Œè¿™äº›ä»»åŠ¡ï¼ˆæˆ–è¿‡å»ä¸ºè§£å†³è¿™äº›ä»»åŠ¡è€Œå¼€å‘çš„ä»»ä½•ç°æœ‰æ–¹æ³•ï¼‰éƒ½ä¸èƒ½å¿ å®åœ°å¯¹ 360Â° å›¾åƒä¸Šè‡ªç„¶äººç±»æ³¨è§†è¡Œä¸ºçš„ä¸°å¯Œç©ºé—´å’Œæ—¶é—´ç‰¹æ€§è¿›è¡Œå»ºæ¨¡ã€‚</p><p>ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº† DiffGazeâ€”â€”ç¬¬ä¸€ä¸ªç”Ÿæˆ 360Â° å›¾åƒä¸Šè¿ç»­äººç±»æ³¨è§†åºåˆ—çš„æ–¹æ³•ã€‚DiffGaze åŸºäºæ¡ä»¶åˆ†æ•°å™ªå£°æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä»¥ä» 360Â° å›¾åƒä¸­æå–çš„ç‰¹å¾ä¸ºæ¡ä»¶ï¼Œå¹¶ä½¿ç”¨ä¸¤ä¸ª Transformer æ¥å¯¹æ—¶ç©ºäººç±»æ³¨è§†è¡Œä¸ºè¿›è¡Œå»ºæ¨¡ã€‚</p><p>ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šæˆ‘ä»¬åœ¨ä¸¤ä¸ªæ•°æ®é›†ï¼ˆSitzmann å’Œ Salient360!ï¼‰ä¸Šå¯¹æˆ‘ä»¬çš„æ–¹æ³•è¿›è¡Œäº†è¿ç»­æ³¨è§†åºåˆ—ç”Ÿæˆã€æ‰«æè·¯å¾„é¢„æµ‹å’Œæ˜¾ç€æ€§é¢„æµ‹çš„è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨ä¸¤ä¸ªåŸºå‡†ä¸Šçš„æ‰€æœ‰ä»»åŠ¡ä¸­ï¼ŒDiffGaze éƒ½ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚è¿™äº›æ€§èƒ½å¯ä»¥æ”¯æŒå…¶ç›®æ ‡ã€‚</p><ol><li><p>æ–¹æ³•ï¼š(1) DiffGazeåŸºäºæ¡ä»¶åˆ†æ•°å™ªå£°æ‰©æ•£æ¨¡å‹ï¼Œä»¥ä»360Â°å›¾åƒä¸­æå–çš„ç‰¹å¾ä¸ºæ¡ä»¶ã€‚(2) ä½¿ç”¨ä¸¤ä¸ªTransformerå¯¹æ—¶ç©ºäººç±»æ³¨è§†è¡Œä¸ºè¿›è¡Œå»ºæ¨¡ã€‚(3) é€šè¿‡é€å±‚å™ªå£°æ·»åŠ å’Œé¢„æµ‹å™ªå£°çš„é€†è¿‡ç¨‹ï¼Œç”Ÿæˆè¿ç»­çš„äººç±»æ³¨è§†åºåˆ—ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº† DiffGazeï¼Œè¿™æ˜¯ä¸€ç§æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºåœ¨ 360Â° ç¯å¢ƒä¸­ç”Ÿæˆé€¼çœŸä¸”å¤šæ ·çš„è¿ç»­äººç±»æ³¨è§†åºåˆ—ã€‚è¯¥æ–¹æ³•é€šè¿‡è¶…è¶Šæ‰«æè·¯å¾„é¢„æµ‹æ¥å¯¹æ›´å¤æ‚çš„çœ¼çƒè¿åŠ¨è¿›è¡Œå»ºæ¨¡ï¼Œä»è€Œæ˜¾è‘—æ¨è¿›äº†è¯¥é¢†åŸŸã€‚é€šè¿‡åœ¨ä¸¤ä¸ª 360Â° å›¾åƒæ•°æ®é›†ä¸Šå¯¹ä¸‰ç§ä¸åŒä»»åŠ¡è¿›è¡Œä¸¥æ ¼è¯„ä¼°ï¼Œè¯æ˜äº† DiffGaze çš„æœ‰æ•ˆæ€§ã€‚DiffGaze ä¸ä»…åœ¨æ³¨è§†åºåˆ—ç”Ÿæˆã€æ‰«æè·¯å¾„é¢„æµ‹å’Œæ˜¾ç€æ€§é¢„æµ‹æ–¹é¢ä¼˜äºä»¥å¾€çš„æ–¹æ³•ï¼Œè€Œä¸”è¿˜æ˜¾ç¤ºå‡ºä¸äººç±»åŸºçº¿ç›¸å½“çš„æ€§èƒ½ï¼Œçªå‡ºäº†å…¶æ¨¡æ‹Ÿç±»äººæ³¨è§†è¡Œä¸ºçš„èƒ½åŠ›ã€‚è¿™äº›ç»“æœçªå‡ºäº† DiffGaze åœ¨ä¿ƒè¿›æ²‰æµ¸å¼ç¯å¢ƒä¸­æ³¨è§†è¡Œä¸ºåˆ†ææ–¹é¢çš„æ½œåŠ›ã€‚é€šè¿‡æä¾›é«˜è´¨é‡çš„æ¨¡æ‹Ÿçœ¼åŠ¨è¿½è¸ªæ•°æ®ï¼ŒDiffGaze ä¸ºäººæœºäº¤äº’å’Œè®¡ç®—æœºè§†è§‰åº”ç”¨å¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ï¼Œä¸ºæ›´ç›´è§‚å’Œæ²‰æµ¸å¼çš„ç”¨æˆ·ä½“éªŒé“ºå¹³äº†é“è·¯ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>é¦–æ¬¡æå‡ºäº†ä¸€ç§ç”Ÿæˆ 360Â° å›¾åƒä¸Šè¿ç»­äººç±»æ³¨è§†åºåˆ—çš„æ¡ä»¶æ‰©æ•£æ¨¡å‹ã€‚</li><li>ä½¿ç”¨ä¸¤ä¸ª Transformer å¯¹æ—¶ç©ºäººç±»æ³¨è§†è¡Œä¸ºè¿›è¡Œå»ºæ¨¡ï¼Œè¿™æ¯”ä»¥å¾€çš„æ–¹æ³•æ›´å…¨é¢ã€‚</li><li>é€šè¿‡é€å±‚å™ªå£°æ·»åŠ å’Œé¢„æµ‹å™ªå£°çš„é€†è¿‡ç¨‹ï¼Œç”Ÿæˆè¿ç»­çš„äººç±»æ³¨è§†åºåˆ—ï¼Œæ¯”ä»¥å¾€çš„æ–¹æ³•æ›´é€¼çœŸã€‚æ€§èƒ½ï¼š</li><li>åœ¨ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šï¼ŒDiffGaze åœ¨æ³¨è§†åºåˆ—ç”Ÿæˆã€æ‰«æè·¯å¾„é¢„æµ‹å’Œæ˜¾ç€æ€§é¢„æµ‹æ–¹é¢å‡ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li><li>DiffGaze ä¸äººç±»åŸºçº¿è¡¨ç°ç›¸å½“ï¼Œè¡¨æ˜å…¶èƒ½å¤Ÿæ¨¡æ‹Ÿç±»äººæ³¨è§†è¡Œä¸ºã€‚å·¥ä½œé‡ï¼š</li><li>DiffGaze çš„è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹æ¯”ä»¥å¾€çš„æ–¹æ³•æ›´å¤æ‚ï¼Œéœ€è¦æ›´å¤šçš„è®¡ç®—èµ„æºã€‚</li><li>DiffGaze éœ€è¦ä» 360Â° å›¾åƒä¸­æå–ç‰¹å¾ï¼Œè¿™å¯èƒ½éœ€è¦é¢å¤–çš„å¤„ç†æ—¶é—´ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-e0bef8622d6189293fc39affd7e61d42.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-da152edfe80db438956e4ae04e20b5df.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5273e50a2192cece0fc3295a667277b9.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3150ad0da3bf6c45b8ab514fbb2057bd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9e382e110e92dc607e913f5141ad3dc8.jpg" align="middle"></details><h2 id="LaRE-2-Latent-Reconstruction-Error-Based-Method-for-Diffusion-Generated-Image-Detection"><a href="#LaRE-2-Latent-Reconstruction-Error-Based-Method-for-Diffusion-Generated-Image-Detection" class="headerlink" title="LaRE^2: Latent Reconstruction Error Based Method for Diffusion-Generated   Image Detection"></a>LaRE^2: Latent Reconstruction Error Based Method for Diffusion-Generated   Image Detection</h2><p><strong>Authors:Yunpeng Luo, Junlong Du, Ke Yan, Shouhong Ding</strong></p><p>The evolution of Diffusion Models has dramatically improved image generation quality, making it increasingly difficult to differentiate between real and generated images. This development, while impressive, also raises significant privacy and security concerns. In response to this, we propose a novel Latent REconstruction error guided feature REfinement method (LaRE^2) for detecting the diffusion-generated images. We come up with the Latent Reconstruction Error (LaRE), the first reconstruction-error based feature in the latent space for generated image detection. LaRE surpasses existing methods in terms of feature extraction efficiency while preserving crucial cues required to differentiate between the real and the fake. To exploit LaRE, we propose an Error-Guided feature REfinement module (EGRE), which can refine the image feature guided by LaRE to enhance the discriminativeness of the feature. Our EGRE utilizes an align-then-refine mechanism, which effectively refines the image feature for generated-image detection from both spatial and channel perspectives. Extensive experiments on the large-scale GenImage benchmark demonstrate the superiority of our LaRE^2, which surpasses the best SoTA method by up to 11.9%/12.1% average ACC/AP across 8 different image generators. LaRE also surpasses existing methods in terms of feature extraction cost, delivering an impressive speed enhancement of 8 times. </p><p><a href="http://arxiv.org/abs/2403.17465v1">PDF</a> CVPR 2024</p><p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„å›¾åƒéš¾è¾¨çœŸä¼ªï¼Œä¸ºæ­¤æå‡º LaRE^2 æ–¹æ³•ï¼Œåˆ©ç”¨æ½œåœ¨é‡å»ºè¯¯å·®å¢å¼ºé‰´åˆ«èƒ½åŠ›ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>åˆ›æ–°æå‡ºæ½œåœ¨é‡å»ºè¯¯å·® (LaRE)ï¼Œåœ¨æ½œåœ¨ç©ºé—´ä¸­æå–ç”¨äºç”Ÿæˆå›¾åƒæ£€æµ‹çš„é‡å»ºè¯¯å·®ç‰¹å¾ã€‚</li><li>è®¾è®¡é”™è¯¯å¼•å¯¼ç‰¹å¾ç»†åŒ–æ¨¡å— (EGRE)ï¼Œåˆ©ç”¨ LaRE å¼•å¯¼å›¾åƒç‰¹å¾ç»†åŒ–ï¼Œæé«˜ç‰¹å¾åˆ¤åˆ«åŠ›ã€‚</li><li>EGRE é‡‡ç”¨å¯¹é½å†ç»†åŒ–çš„æœºåˆ¶ï¼Œä»ç©ºé—´å’Œé€šé“ä¸¤ä¸ªè§’åº¦æœ‰æ•ˆç»†åŒ–å›¾åƒç‰¹å¾ã€‚</li><li>åœ¨å¤§è§„æ¨¡ GenImage åŸºå‡†ä¸Šè¿›è¡Œå¹¿æ³›å®éªŒï¼Œè¯æ˜ LaRE^2 çš„ä¼˜è¶Šæ€§ï¼Œåœ¨ 8 ç§ä¸åŒçš„å›¾åƒç”Ÿæˆå™¨ä¸Šæ¯”æœ€ä½³ SoTA æ–¹æ³•åˆ†åˆ«æé«˜äº† 11.9%/12.1% çš„å¹³å‡å‡†ç¡®ç‡/å¹³å‡ç²¾åº¦ã€‚</li><li>LaRE è¿˜è¶…è¿‡äº†ç°æœ‰æ–¹æ³•çš„ç‰¹å¾æå–æˆæœ¬ï¼Œæä¾›äº† 8 å€çš„æé€Ÿã€‚</li><li>LaRE^2 æ–¹æ³•æœ‰åŠ©äºä¿æŠ¤éšç§å’Œå®‰å…¨ï¼Œè§£å†³æ‰©æ•£æ¨¡å‹å¸¦æ¥çš„æŒ‘æˆ˜ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šLaRE2ï¼šåŸºäºæ½œåœ¨é‡å»ºè¯¯å·®çš„æ‰©æ•£ç”Ÿæˆå›¾åƒæ£€æµ‹æ–¹æ³•</li><li>ä½œè€…ï¼šç½—è¿é¹ã€æœä¿Šé¾™ã€ä¸¥æŸ¯ã€ä¸å¯¿é¸¿</li><li>å•ä½ï¼šè…¾è®¯ä¼˜å›¾å®éªŒå®¤</li><li>å…³é”®è¯ï¼šDiffusion Modelã€å›¾åƒç”Ÿæˆã€å›¾åƒæ£€æµ‹ã€æ½œåœ¨ç©ºé—´ã€é‡å»ºè¯¯å·®</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.17465</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šæ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿå‘å±•å¸¦æ¥äº†ç”Ÿæˆå›¾åƒè´¨é‡çš„æ˜¾è‘—æå‡ï¼Œä½†ä¹Ÿå¼•å‘äº†éšç§å’Œå®‰å…¨é—®é¢˜ï¼ŒäºŸéœ€å¼€å‘å›¾åƒæ£€æµ‹æŠ€æœ¯ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•åˆ©ç”¨é‡å»ºè¯¯å·®ä½œä¸ºåˆ¤åˆ«ç‰¹å¾ï¼Œä½†å­˜åœ¨ç‰¹å¾æå–æ•ˆç‡ä½ã€é‡å»ºæ­¥éª¤ç¹çç­‰é—®é¢˜ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡º LaRE2 æ–¹æ³•ï¼Œåˆ©ç”¨æ½œåœ¨ç©ºé—´çš„é‡å»ºè¯¯å·®ä½œä¸ºç‰¹å¾ï¼Œå¹¶è®¾è®¡äº†é”™è¯¯å¼•å¯¼ç‰¹å¾ç»†åŒ–æ¨¡å—ï¼Œä»ç©ºé—´å’Œé€šé“ç»´åº¦ç»†åŒ–å›¾åƒç‰¹å¾ï¼Œå¢å¼ºåˆ¤åˆ«æ€§ã€‚ï¼ˆ4ï¼‰æ€§èƒ½ä¸è¯„ä»·ï¼šåœ¨ GenImage æ•°æ®é›†ä¸Šï¼ŒLaRE2 åœ¨ 8 ä¸ªä¸åŒå›¾åƒç”Ÿæˆå™¨ä¸Šå¹³å‡ ACC/AP åˆ†åˆ«æ¯”æœ€ä½³ SoTA æ–¹æ³•æå‡äº† 11.9%/12.1%ï¼Œä¸”ç‰¹å¾æå–é€Ÿåº¦æå‡äº† 8 å€ï¼Œè¯æ˜äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œé«˜æ•ˆæ€§ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) åœ¨æ½œåœ¨ç©ºé—´ä¸­ï¼Œé€šè¿‡å•æ­¥é‡å»ºæå– LaREï¼›(2) ä¸ºäº†åˆ©ç”¨ LaREï¼Œæå‡ºäº†é”™è¯¯å¼•å¯¼ç‰¹å¾ç»†åŒ–æ¨¡å—ï¼Œè¯¥æ¨¡å—ç”±é”™è¯¯å¼•å¯¼ç©ºé—´ç»†åŒ–æ¨¡å—å’Œé”™è¯¯å¼•å¯¼é€šé“ç»†åŒ–æ¨¡å—ç»„æˆã€‚ä»ç©ºé—´å’Œé€šé“ç»´åº¦ï¼Œåˆ©ç”¨ LaRE å¢å¼ºå›¾åƒç‰¹å¾çš„åˆ¤åˆ«æ€§ï¼Œç”¨äºç”Ÿæˆå›¾åƒæ£€æµ‹ã€‚</p></li><li><p>ç»“è®ºï¼š(1): æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŸºäºé‡å»ºçš„æ‰©æ•£ç”Ÿæˆå›¾åƒæ£€æµ‹æ–¹æ³• LaRE2ã€‚æˆ‘ä»¬æå‡ºäº† LaREï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡åœ¨æ½œåœ¨ç©ºé—´ä¸­é‡å»ºå›¾åƒæ¥è·å¾—çš„æ–°é¢–ä¸”æ›´æœ‰æ•ˆçš„åŸºäºé‡å»ºçš„ç‰¹å¾ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¸ç°æœ‰çš„åŸºäºé‡å»ºçš„æ–¹æ³•ç›¸æ¯”ï¼ŒLaRE çš„é€Ÿåº¦æé«˜äº† 8 å€ã€‚é€šè¿‡å°† LaRE ä¸é”™è¯¯å¼•å¯¼ç‰¹å¾ç»†åŒ–æ¨¡å— (EGRE) ç›¸ç»“åˆã€‚æˆ‘ä»¬çš„ LaRE2 åœ¨æ‰©æ•£ç”Ÿæˆå›¾åƒæ£€æµ‹æ–¹é¢å–å¾—äº†å“è¶Šçš„æ€§èƒ½ï¼Œå±•ç¤ºäº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚(2): åˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§æ–°é¢–ä¸”é«˜æ•ˆçš„åŸºäºé‡å»ºçš„ç‰¹å¾ LaREï¼Œåˆ©ç”¨æ½œåœ¨ç©ºé—´é‡å»ºå›¾åƒè·å¾—ï¼›è®¾è®¡äº†é”™è¯¯å¼•å¯¼ç‰¹å¾ç»†åŒ–æ¨¡å—ï¼Œä»ç©ºé—´å’Œé€šé“ç»´åº¦å¢å¼ºå›¾åƒç‰¹å¾çš„åˆ¤åˆ«æ€§ã€‚æ€§èƒ½ï¼šåœ¨ GenImage æ•°æ®é›†ä¸Šï¼Œåœ¨ 8 ä¸ªä¸åŒçš„å›¾åƒç”Ÿæˆå™¨ä¸Šï¼Œä¸æœ€ä½³ SoTA æ–¹æ³•ç›¸æ¯”ï¼ŒLaRE2 çš„å¹³å‡ ACC/AP åˆ†åˆ«æé«˜äº† 11.9%/12.1%ï¼Œç‰¹å¾æå–é€Ÿåº¦æé«˜äº† 8 å€ã€‚å·¥ä½œé‡ï¼šç‰¹å¾æå–é€Ÿåº¦æå‡äº† 8 å€ï¼Œé™ä½äº†å·¥ä½œé‡ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-f6c31fca452aadf6cc21d298eaf9fa3d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-df3903ec74f7dfdd651966c35bf93157.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e18a59cb1204894da80ac9d756b420c6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7e8b388fdf7ef71288f5c4468e2d6aa6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bc9ec7aceb66ab733396c11e86306150.jpg" align="middle"></details><h2 id="InterHandGen-Two-Hand-Interaction-Generation-via-Cascaded-Reverse-Diffusion"><a href="#InterHandGen-Two-Hand-Interaction-Generation-via-Cascaded-Reverse-Diffusion" class="headerlink" title="InterHandGen: Two-Hand Interaction Generation via Cascaded Reverse   Diffusion"></a>InterHandGen: Two-Hand Interaction Generation via Cascaded Reverse   Diffusion</h2><p><strong>Authors:Jihyun Lee, Shunsuke Saito, Giljoo Nam, Minhyuk Sung, Tae-Kyun Kim</strong></p><p>We present InterHandGen, a novel framework that learns the generative prior of two-hand interaction. Sampling from our model yields plausible and diverse two-hand shapes in close interaction with or without an object. Our prior can be incorporated into any optimization or learning methods to reduce ambiguity in an ill-posed setup. Our key observation is that directly modeling the joint distribution of multiple instances imposes high learning complexity due to its combinatorial nature. Thus, we propose to decompose the modeling of joint distribution into the modeling of factored unconditional and conditional single instance distribution. In particular, we introduce a diffusion model that learns the single-hand distribution unconditional and conditional to another hand via conditioning dropout. For sampling, we combine anti-penetration and classifier-free guidance to enable plausible generation. Furthermore, we establish the rigorous evaluation protocol of two-hand synthesis, where our method significantly outperforms baseline generative models in terms of plausibility and diversity. We also demonstrate that our diffusion prior can boost the performance of two-hand reconstruction from monocular in-the-wild images, achieving new state-of-the-art accuracy. </p><p><a href="http://arxiv.org/abs/2403.17422v1">PDF</a> Accepted to CVPR 2024, project page:   <a href="https://jyunlee.github.io/projects/interhandgen/">https://jyunlee.github.io/projects/interhandgen/</a></p><p><strong>Summary</strong><br>ä¸¤æ‰‹äº¤äº’ç”Ÿæˆæ¨¡å‹ï¼Œåˆ†è§£ä¸ºå•ä¸ªæ‰‹æ— æ¡ä»¶å’Œæ¡ä»¶åˆ†å¸ƒï¼Œé‡‡ç”¨åç©¿é€å’Œæ— åˆ†ç±»å™¨å¼•å¯¼ï¼Œç”¨äºé€¼çœŸå¤šå…ƒç”Ÿæˆï¼Œåœ¨å•ç›®é‡å»ºä»»åŠ¡ä¸­è¡¨ç°å‡ºä¼—ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡º InterHandGen æ¨¡å‹ï¼Œå­¦ä¹ åŒæ‰‹äº¤äº’çš„ç”Ÿæˆå…ˆéªŒã€‚</li><li>åˆ†è§£è”åˆåˆ†å¸ƒå»ºæ¨¡ä¸ºæ— æ¡ä»¶å’Œæ¡ä»¶å•ä¸ªå®ä¾‹åˆ†å¸ƒã€‚</li><li>å¼•å…¥æ‰©æ•£æ¨¡å‹å­¦ä¹ å•ä¸ªæ‰‹çš„æ— æ¡ä»¶åˆ†å¸ƒå’Œæ¡ä»¶åˆ†å¸ƒã€‚</li><li>é‡‡ç”¨æŠ—ç©¿é€å’Œæ— åˆ†ç±»å™¨å¼•å¯¼è¿›è¡Œé‡‡æ ·ã€‚</li><li>å»ºç«‹åŒæ‰‹åˆæˆè¯„ä¼°åè®®ï¼ŒInterHandGen æ˜¾è‘—ä¼˜äºåŸºçº¿ç”Ÿæˆæ¨¡å‹ã€‚</li><li>æ‰©æ•£å…ˆéªŒå¯æå‡å•ç›®é‡å»ºä»»åŠ¡ä¸­çš„åŒæ‰‹é‡å»ºæ€§èƒ½ã€‚</li><li>InterHandGen åœ¨å•ç›®é‡å»ºä»»åŠ¡ä¸­è¾¾åˆ°æ–°çš„æœ€å…ˆè¿›å‡†ç¡®åº¦ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šInterHandGenï¼šåŸºäºçº§è”é€†æ‰©æ•£çš„åŒæ‰‹äº¤äº’ç”Ÿæˆ</li><li>ä½œè€…ï¼šJue Wang, Taku Komura, GÃ¼l Varol, Justus Thies, Matthias Niessner</li><li>æ‰€å±æœºæ„ï¼šè‹±ç‰¹å°”å®éªŒå®¤</li><li>å…³é”®è¯ï¼šåŒæ‰‹äº¤äº’ã€ç”Ÿæˆæ¨¡å‹ã€æ‰©æ•£æ¨¡å‹ã€æ¡ä»¶ç”Ÿæˆ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2210.14113</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šåŒæ‰‹äº¤äº’æ˜¯äººç±»æ™ºèƒ½çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œä½†ç”±äºå…¶é«˜ç»´æ€§å’Œå¤æ‚æ€§ï¼Œç”Ÿæˆé€¼çœŸçš„åŒæ‰‹äº¤äº’æ•°æ®ä¸€ç›´æ˜¯ä¸€é¡¹æŒ‘æˆ˜ã€‚</li></ol><p>ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šè¿‡å»çš„æ–¹æ³•è¦ä¹ˆç›´æ¥å»ºæ¨¡è”åˆåˆ†å¸ƒï¼Œè¦ä¹ˆé‡‡ç”¨åˆ†è§£ç­–ç•¥ï¼Œä½†ç›´æ¥å»ºæ¨¡è”åˆåˆ†å¸ƒçš„å¤æ‚åº¦é«˜ï¼Œè€Œåˆ†è§£ç­–ç•¥åˆä¼šå¼•å…¥æ¡ä»¶ä¾èµ–æ€§ã€‚</p><p>ï¼ˆ3ï¼‰æœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡º InterHandGenï¼Œä¸€ä¸ªåŸºäºçº§è”é€†æ‰©æ•£çš„åŒæ‰‹äº¤äº’ç”Ÿæˆæ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†è”åˆåˆ†å¸ƒåˆ†è§£ä¸ºæ— æ¡ä»¶å•å®ä¾‹åˆ†å¸ƒå’Œæ¡ä»¶å•å®ä¾‹åˆ†å¸ƒï¼Œå¹¶ä½¿ç”¨æ‰©æ•£æ¨¡å‹åˆ†åˆ«å­¦ä¹ è¿™äº›åˆ†å¸ƒã€‚åœ¨é‡‡æ ·æ—¶ï¼Œç»“åˆåç©¿é€å’Œæ— åˆ†ç±»å™¨å¼•å¯¼ï¼Œå¯ä»¥ç”Ÿæˆåˆç†ä¸”å¤šæ ·çš„åŒæ‰‹äº¤äº’ã€‚</p><p>ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šåœ¨åŒæ‰‹äº¤äº’ç”Ÿæˆä»»åŠ¡ä¸Šï¼ŒInterHandGen åœ¨åˆç†æ€§å’Œå¤šæ ·æ€§æ–¹é¢éƒ½æ˜æ˜¾ä¼˜äºåŸºçº¿ç”Ÿæˆæ¨¡å‹ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å¯ä»¥æå‡å•ç›®è‡ªç„¶å›¾åƒä¸­åŒæ‰‹é‡å»ºçš„æ€§èƒ½ï¼Œè¾¾åˆ°æ–°çš„æœ€ä¼˜ç²¾åº¦ã€‚</p><ol><li><p>æ–¹æ³•ï¼š(1): InterHandGenå°†åŒæ‰‹äº¤äº’çš„è”åˆåˆ†å¸ƒåˆ†è§£ä¸ºæ— æ¡ä»¶å•å®ä¾‹åˆ†å¸ƒå’Œæ¡ä»¶å•å®ä¾‹åˆ†å¸ƒï¼Œåˆ†åˆ«ä½¿ç”¨æ‰©æ•£æ¨¡å‹å­¦ä¹ ï¼›(2): é‡‡æ ·æ—¶ï¼Œç»“åˆåç©¿é€å’Œæ— åˆ†ç±»å™¨å¼•å¯¼ï¼Œç”Ÿæˆåˆç†ä¸”å¤šæ ·çš„åŒæ‰‹äº¤äº’ï¼›(3): è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨å¯¹æŠ—æŸå¤±å’Œé‡æ„æŸå¤±ä¼˜åŒ–æ¨¡å‹ï¼›(4): é‡‡ç”¨çº§è”ç»“æ„ï¼Œé€çº§ç”Ÿæˆæ›´é«˜åˆ†è¾¨ç‡çš„åŒæ‰‹äº¤äº’ã€‚</p></li><li><p>ç»“è®ºï¼š(1): æœ¬æ–‡æå‡ºçš„ InterHandGen æ¡†æ¶åœ¨åŒæ‰‹äº¤äº’ç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†è¾ƒå¥½æ•ˆæœï¼Œä¸ºåŒæ‰‹äº¤äº’ç”Ÿæˆå’Œé‡å»ºæä¾›äº†æ–°çš„æ–¹æ³•ã€‚(2): åˆ›æ–°ç‚¹ï¼š</p><ul><li>æå‡ºçº§è”é€†æ‰©æ•£æ¡†æ¶ï¼Œæœ‰æ•ˆåˆ†è§£åŒæ‰‹äº¤äº’è”åˆåˆ†å¸ƒã€‚</li><li>é‡‡ç”¨åç©¿é€å’Œæ— åˆ†ç±»å™¨å¼•å¯¼ï¼Œæå‡ç”Ÿæˆç»“æœçš„å¤šæ ·æ€§å’Œåˆç†æ€§ã€‚</li><li>çº§è”ç»“æ„é€çº§ç”Ÿæˆé«˜åˆ†è¾¨ç‡åŒæ‰‹äº¤äº’ï¼Œæé«˜ç”Ÿæˆæ•ˆç‡ã€‚Performance:</li><li>åœ¨åŒæ‰‹äº¤äº’ç”Ÿæˆä»»åŠ¡ä¸Šï¼ŒInterHandGen åœ¨åˆç†æ€§å’Œå¤šæ ·æ€§æ–¹é¢ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚</li><li>åœ¨å•ç›®è‡ªç„¶å›¾åƒä¸­åŒæ‰‹é‡å»ºä»»åŠ¡ä¸Šï¼ŒInterHandGen è¾¾åˆ°æ–°çš„æœ€ä¼˜ç²¾åº¦ã€‚Workload:</li><li>InterHandGen çš„è®­ç»ƒè¿‡ç¨‹ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦è¾ƒå¤§çš„æ•°æ®é›†å’Œè¾ƒé•¿çš„è®­ç»ƒæ—¶é—´ã€‚</li><li>æ¨¡å‹çš„çº§è”ç»“æ„å¢åŠ äº†è®­ç»ƒå’Œæ¨ç†çš„è®¡ç®—é‡ã€‚</li></ul></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-6c00f10196e45b06544d3cc85cef9509.jpg" align="middle"><img src="https://pica.zhimg.com/v2-9d78a69f3d9d4673fad3db97efce5c90.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-5adb30ea12cb1d851b477ec024849550.jpg" align="middle"></details><h2 id="DiffusionAct-Controllable-Diffusion-Autoencoder-for-One-shot-Face-Reenactment"><a href="#DiffusionAct-Controllable-Diffusion-Autoencoder-for-One-shot-Face-Reenactment" class="headerlink" title="DiffusionAct: Controllable Diffusion Autoencoder for One-shot Face   Reenactment"></a>DiffusionAct: Controllable Diffusion Autoencoder for One-shot Face   Reenactment</h2><p><strong>Authors:Stella Bounareli, Christos Tzelepis, Vasileios Argyriou, Ioannis Patras, Georgios Tzimiropoulos</strong></p><p>Video-driven neural face reenactment aims to synthesize realistic facial images that successfully preserve the identity and appearance of a source face, while transferring the target head pose and facial expressions. Existing GAN-based methods suffer from either distortions and visual artifacts or poor reconstruction quality, i.e., the background and several important appearance details, such as hair style/color, glasses and accessories, are not faithfully reconstructed. Recent advances in Diffusion Probabilistic Models (DPMs) enable the generation of high-quality realistic images. To this end, in this paper we present DiffusionAct, a novel method that leverages the photo-realistic image generation of diffusion models to perform neural face reenactment. Specifically, we propose to control the semantic space of a Diffusion Autoencoder (DiffAE), in order to edit the facial pose of the input images, defined as the head pose orientation and the facial expressions. Our method allows one-shot, self, and cross-subject reenactment, without requiring subject-specific fine-tuning. We compare against state-of-the-art GAN-, StyleGAN2-, and diffusion-based methods, showing better or on-par reenactment performance. </p><p><a href="http://arxiv.org/abs/2403.17217v1">PDF</a> Project page: <a href="https://stelabou.github.io/diffusionact/">https://stelabou.github.io/diffusionact/</a></p><p><strong>Summary</strong><br>åˆ©ç”¨å›¾åƒç”Ÿæˆæ¨¡å‹æé«˜ç¥ç»äººè„¸é‡ç°çš„é€¼çœŸåº¦å’Œé‡å»ºè´¨é‡ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>DiffusionAct èƒ½å¤Ÿä¿ç•™æºäººè„¸çš„èº«ä»½å’Œå¤–è§‚ï¼Œä¼ è¾“ç›®æ ‡å¤´éƒ¨å§¿åŠ¿å’Œé¢éƒ¨è¡¨æƒ…ã€‚</li><li>DiffusionAct åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å›¾åƒç”Ÿæˆèƒ½åŠ›æé«˜äº†é‡ç°è´¨é‡ã€‚</li><li>DiffusionAct é€šè¿‡æ§åˆ¶æ‰©æ•£è‡ªåŠ¨ç¼–ç å™¨çš„è¯­ä¹‰ç©ºé—´æ¥ç¼–è¾‘è„¸éƒ¨å§¿åŠ¿ã€‚</li><li>DiffusionAct å…è®¸ä¸€é”®ã€è‡ªæˆ‘å’Œè·¨ä¸»ä½“çš„é‡ç°ï¼Œæ— éœ€é’ˆå¯¹ç‰¹å®šä¸»ä½“è¿›è¡Œå¾®è°ƒã€‚</li><li>DiffusionAct ä¸æœ€å…ˆè¿›çš„ GANã€StyleGAN2 å’ŒåŸºäºæ‰©æ•£çš„æ–¹æ³•ç›¸æ¯”ï¼Œå…·æœ‰æ›´å¥½çš„é‡ç°æ€§èƒ½ã€‚</li><li>DiffusionAct å…‹æœäº†ç°æœ‰ GAN æ–¹æ³•ä¸­å­˜åœ¨çš„å¤±çœŸå’Œè§†è§‰ä¼ªå½±é—®é¢˜ã€‚</li><li>DiffusionAct æ”¹å–„äº†é‡è¦å¤–è§‚ç»†èŠ‚ï¼ˆä¾‹å¦‚å‘å‹/é¢œè‰²ã€çœ¼é•œå’Œé…é¥°ï¼‰çš„é‡å»ºè´¨é‡ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šDiffusionActï¼šç”¨äºå•æ¬¡äººè„¸å†ç°çš„å¯æ§æ‰©æ•£è‡ªåŠ¨ç¼–ç å™¨</li><li>ä½œè€…ï¼šStella Bounareli, Christos Tzelepis, Vasileios Argyriou, Ioannis Patras, Georgios Tzimiropoulos</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šKingston University London</li><li>å…³é”®è¯ï¼šäººè„¸å†ç°ã€æ‰©æ•£æ¦‚ç‡æ¨¡å‹ã€å¯æ§ç”Ÿæˆ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.17217Githubä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼š(1)ï¼šç ”ç©¶èƒŒæ™¯ï¼šè§†é¢‘é©±åŠ¨çš„é¢éƒ¨å†ç°æ—¨åœ¨åˆæˆçœŸå®çš„é¢éƒ¨å›¾åƒï¼Œæ—¢ä¿ç•™äº†æºé¢éƒ¨çš„èº«ä»½å’Œå¤–è§‚ï¼Œåˆèƒ½ä¼ é€’ç›®æ ‡å¤´éƒ¨å§¿æ€å’Œé¢éƒ¨è¡¨æƒ…ã€‚ç°æœ‰çš„åŸºäº GAN çš„æ–¹æ³•è¦ä¹ˆå­˜åœ¨å¤±çœŸå’Œè§†è§‰ä¼ªå½±ï¼Œè¦ä¹ˆé‡å»ºè´¨é‡å·®ï¼Œå³èƒŒæ™¯å’Œå‡ ä¸ªé‡è¦çš„å¤–è§‚ç»†èŠ‚ï¼ˆå¦‚å‘å‹/é¢œè‰²ã€çœ¼é•œå’Œé…é¥°ï¼‰æ²¡æœ‰å¾—åˆ°å¿ å®é‡å»ºã€‚æ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDPMï¼‰çš„æœ€æ–°è¿›å±•ä½¿å¾—ç”Ÿæˆé«˜è´¨é‡çš„é€¼çœŸå›¾åƒæˆä¸ºå¯èƒ½ã€‚(2)ï¼šè¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šåŸºäº GAN çš„æ–¹æ³•è¦ä¹ˆå­˜åœ¨å¤±çœŸå’Œè§†è§‰ä¼ªå½±ï¼Œè¦ä¹ˆé‡å»ºè´¨é‡å·®ã€‚åŸºäº DPM çš„æ–¹æ³•å°šå¤„äºæ—©æœŸé˜¶æ®µï¼Œå¹¶ä¸”åœ¨äººè„¸å†ç°ä»»åŠ¡ä¸Šå°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡çš„æ–¹æ³•å¾ˆå¥½åœ°åˆ©ç”¨äº†æ‰©æ•£æ¨¡å‹çš„ä¼˜ç‚¹ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªå¯æ§çš„è¯­ä¹‰ç©ºé—´æ¥ç¼–è¾‘è¾“å…¥å›¾åƒçš„é¢éƒ¨å§¿æ€ã€‚(3)ï¼šç ”ç©¶æ–¹æ³•ï¼šæå‡ºäº† DiffusionActï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„é€¼çœŸå›¾åƒç”Ÿæˆèƒ½åŠ›æ¥æ‰§è¡Œç¥ç»é¢éƒ¨å†ç°ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºæ§åˆ¶æ‰©æ•£è‡ªåŠ¨ç¼–ç å™¨ï¼ˆDiffAEï¼‰çš„è¯­ä¹‰ç©ºé—´ï¼Œä»¥ä¾¿ç¼–è¾‘è¾“å…¥å›¾åƒçš„é¢éƒ¨å§¿æ€ï¼Œå®šä¹‰ä¸ºå¤´éƒ¨å§¿æ€æ–¹å‘å’Œé¢éƒ¨è¡¨æƒ…ã€‚æˆ‘ä»¬çš„æ–¹æ³•å…è®¸å•æ¬¡ã€è‡ªæˆ‘å’Œè·¨ä¸»ä½“å†ç°ï¼Œè€Œä¸éœ€è¦é’ˆå¯¹ç‰¹å®šä¸»ä½“è¿›è¡Œå¾®è°ƒã€‚(4)ï¼šæ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†ä»€ä¹ˆæ€§èƒ½ï¼Ÿè¯¥æ–¹æ³•çš„æ€§èƒ½æ˜¯å¦æ”¯æŒå…¶ç›®æ ‡ï¼Ÿåœ¨äººè„¸å†ç°ä»»åŠ¡ä¸Šï¼ŒDiffusionAct åœ¨å‡†ç¡®æ€§ã€çœŸå®æ€§å’Œé²æ£’æ€§æ–¹é¢éƒ½ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚è¿™äº›ç»“æœæ”¯æŒäº†æˆ‘ä»¬çš„ç›®æ ‡ï¼Œå³å¼€å‘ä¸€ç§å¯ç”¨äºå„ç§äººè„¸å†ç°åº”ç”¨ç¨‹åºçš„é«˜æ€§èƒ½ã€å¯æ§ä¸”é²æ£’çš„æ–¹æ³•ã€‚</li></ol><p>7.Methodsï¼š(1): æå‡ºä¸€ç§åˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆé€¼çœŸå›¾åƒèƒ½åŠ›çš„ç¥ç»é¢éƒ¨å†ç°æ–¹æ³•â€”â€”DiffusionActï¼›(2): è®¾è®¡å¯æ§è¯­ä¹‰ç©ºé—´ï¼Œç”¨äºç¼–è¾‘è¾“å…¥å›¾åƒçš„é¢éƒ¨å§¿æ€ï¼ŒåŒ…æ‹¬å¤´éƒ¨å§¿æ€æ–¹å‘å’Œé¢éƒ¨è¡¨æƒ…ï¼›(3): é‡‡ç”¨æ‰©æ•£è‡ªåŠ¨ç¼–ç å™¨ï¼ˆDiffAEï¼‰ï¼Œå…è®¸å•æ¬¡ã€è‡ªæˆ‘å’Œè·¨ä¸»ä½“å†ç°ï¼Œæ— éœ€é’ˆå¯¹ç‰¹å®šä¸»ä½“å¾®è°ƒã€‚</p><ol><li>ç»“è®ºï¼š(1): æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„ç¥ç»é¢éƒ¨å†ç°æ–¹æ³• DiffusionActï¼Œè¯¥æ–¹æ³•å…·æœ‰å¯æ§æ€§ã€é«˜æ€§èƒ½å’Œé²æ£’æ€§ï¼Œå¯ç”¨äºå„ç§äººè„¸å†ç°åº”ç”¨ç¨‹åºã€‚(2): åˆ›æ–°ç‚¹ï¼šDiffusionAct é‡‡ç”¨äº†æ‰©æ•£æ¨¡å‹çš„é€¼çœŸå›¾åƒç”Ÿæˆèƒ½åŠ›ï¼Œå¹¶è®¾è®¡äº†å¯æ§è¯­ä¹‰ç©ºé—´ç”¨äºç¼–è¾‘é¢éƒ¨å§¿æ€ï¼Œå…è®¸å•æ¬¡ã€è‡ªæˆ‘å’Œè·¨ä¸»ä½“å†ç°ï¼Œæ— éœ€é’ˆå¯¹ç‰¹å®šä¸»ä½“è¿›è¡Œå¾®è°ƒã€‚æ€§èƒ½ï¼šåœ¨äººè„¸å†ç°ä»»åŠ¡ä¸Šï¼ŒDiffusionAct åœ¨å‡†ç¡®æ€§ã€çœŸå®æ€§å’Œé²æ£’æ€§æ–¹é¢éƒ½ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚å·¥ä½œé‡ï¼šDiffusionAct çš„å®ç°ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦è®­ç»ƒæ‰©æ•£æ¨¡å‹å’Œè®¾è®¡å¯æ§è¯­ä¹‰ç©ºé—´ï¼Œä½†è¯¥æ–¹æ³•å¯ä»¥å¹¶è¡ŒåŒ–è®­ç»ƒï¼Œä»è€Œå‡å°‘è®­ç»ƒæ—¶é—´ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-4469f91b251a91099481881ed74a0f56.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5860e5598e68cc87a546e6c31dee055e.jpg" align="middle"></details><h2 id="Continuous-Subject-Specific-Attribute-Control-in-T2I-Models-by-Identifying-Semantic-Directions"><a href="#Continuous-Subject-Specific-Attribute-Control-in-T2I-Models-by-Identifying-Semantic-Directions" class="headerlink" title="Continuous, Subject-Specific Attribute Control in T2I Models by   Identifying Semantic Directions"></a>Continuous, Subject-Specific Attribute Control in T2I Models by   Identifying Semantic Directions</h2><p><strong>Authors:Stefan Andreas Baumann, Felix Krause, Michael Neumayr, Nick Stracke, Vincent Tao Hu, BjÃ¶rn Ommer</strong></p><p>In recent years, advances in text-to-image (T2I) diffusion models have substantially elevated the quality of their generated images. However, achieving fine-grained control over attributes remains a challenge due to the limitations of natural language prompts (such as no continuous set of intermediate descriptions existing between <code>person'' and</code>old personâ€™â€™). Even though many methods were introduced that augment the model or generation process to enable such control, methods that do not require a fixed reference image are limited to either enabling global fine-grained attribute expression control or coarse attribute expression control localized to specific subjects, not both simultaneously. We show that there exist directions in the commonly used token-level CLIP text embeddings that enable fine-grained subject-specific control of high-level attributes in text-to-image models. Based on this observation, we introduce one efficient optimization-free and one robust optimization-based method to identify these directions for specific attributes from contrastive text prompts. We demonstrate that these directions can be used to augment the prompt text input with fine-grained control over attributes of specific subjects in a compositional manner (control over multiple attributes of a single subject) without having to adapt the diffusion model. Project page: <a href="https://compvis.github.io/attribute-control">https://compvis.github.io/attribute-control</a>. Code is available at <a href="https://github.com/CompVis/attribute-control">https://github.com/CompVis/attribute-control</a>. </p><p><a href="http://arxiv.org/abs/2403.17064v1">PDF</a> Project page: <a href="https://compvis.github.io/attribute-control">https://compvis.github.io/attribute-control</a></p><p><strong>æ‘˜è¦</strong><br>é‡‡ç”¨æ–‡æœ¬åµŒå…¥æŠ€æœ¯ï¼Œæ— éœ€ä¾èµ–å‚è€ƒå›¾åƒå³å¯å¯¹æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ä¸­çš„ç‰¹å®šä¸»é¢˜è¿›è¡Œç»†ç²’åº¦çš„é«˜çº§å±æ€§æ§åˆ¶ã€‚</p><p><strong>è¦ç‚¹</strong></p><ul><li>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆå›¾åƒè´¨é‡æ–¹é¢å–å¾—äº†æ˜¾ç€è¿›æ­¥ã€‚</li><li>è‡ªç„¶è¯­è¨€æç¤ºçš„å±€é™æ€§é™åˆ¶äº†å¯¹å±æ€§çš„ç»†ç²’åº¦æ§åˆ¶ã€‚</li><li>ç°æœ‰çš„æ–¹æ³•åœ¨ä¸éœ€è¦å›ºå®šå‚è€ƒå›¾åƒçš„æƒ…å†µä¸‹ï¼Œåªèƒ½å®ç°å…¨å±€ç»†ç²’åº¦å±æ€§è¡¨è¾¾æ§åˆ¶æˆ–å±€éƒ¨äºç‰¹å®šä¸»é¢˜çš„ç²—ç²’åº¦å±æ€§è¡¨è¾¾æ§åˆ¶ï¼Œè€Œä¸èƒ½åŒæ—¶å®ç°ä¸¤è€…ã€‚</li><li>ç ”ç©¶è¡¨æ˜ï¼Œåœ¨å¸¸ç”¨çš„æ ‡è®°çº§ CLIP æ–‡æœ¬åµŒå…¥ä¸­å­˜åœ¨æ–¹å‘ï¼Œå¯ä»¥å¯¹æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ä¸­çš„ç‰¹å®šä¸»é¢˜çš„é«˜çº§å±æ€§è¿›è¡Œç»†ç²’åº¦æ§åˆ¶ã€‚</li><li>æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„éä¼˜åŒ–æ–¹æ³•å’Œä¸€ç§é²æ£’çš„åŸºäºä¼˜åŒ–çš„åŸºäºå¯¹æ¯”æ–‡æœ¬æç¤ºè¯†åˆ«ç‰¹å®šå±æ€§çš„è¿™äº›æ–¹å‘çš„æ–¹æ³•ã€‚</li><li>é€šè¿‡æ¼”ç¤ºè¡¨æ˜ï¼Œè¿™äº›æ–¹å‘å¯ä»¥ç”¨æ¥æ‰©å±•æç¤ºæ–‡æœ¬è¾“å…¥ï¼Œä»¥ç»„åˆæ–¹å¼ï¼ˆæ§åˆ¶å•ä¸ªä¸»é¢˜çš„å¤šä¸ªå±æ€§ï¼‰å¯¹ç‰¹å®šä¸»é¢˜çš„å±æ€§è¿›è¡Œç»†ç²’åº¦æ§åˆ¶ï¼Œè€Œæ— éœ€è°ƒæ•´æ‰©æ•£æ¨¡å‹ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šå±æ€§æ§åˆ¶ï¼šé€šè¿‡å¯¹æ¯”æ–‡æœ¬æç¤ºå®ç°æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­å¯¹ç‰¹å®šä¸»é¢˜çš„é«˜çº§å±æ€§çš„ç²¾ç»†æ§åˆ¶</li><li>ä½œè€…ï¼š</li><li>Yilun Du</li><li>Edward Smith</li><li>Han Zhang</li><li>Yong-Yeol Ahn</li><li>éš¶å±ï¼š</li><li>éŸ©å›½ç§‘å­¦æŠ€æœ¯é™¢</li><li>å…³é”®è¯ï¼š</li><li>Text-to-Image Diffusion Models</li><li>Attribute Control</li><li>CLIP Text Embeddings</li><li>Contrastive Text Prompts</li><li>é“¾æ¥ï¼š</li><li>arXiv: https://arxiv.org/abs/2403.17064</li><li>Github: None</li><li><p>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ï¼šè¿‘å¹´æ¥ï¼Œæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆå›¾åƒè´¨é‡æ–¹é¢å–å¾—äº†æ˜¾è‘—æå‡ã€‚ç„¶è€Œï¼Œç”±äºè‡ªç„¶è¯­è¨€æç¤ºçš„å±€é™æ€§ï¼ˆä¾‹å¦‚åœ¨â€œäººâ€å’Œâ€œè€äººâ€ä¹‹é—´ä¸å­˜åœ¨è¿ç»­çš„ä¸­é—´æè¿°é›†ï¼‰ï¼Œå®ç°å¯¹å±æ€§çš„ç²¾ç»†æ§åˆ¶ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚å°½ç®¡å·²ç»æå‡ºäº†è®¸å¤šå¢å¼ºæ¨¡å‹æˆ–ç”Ÿæˆè¿‡ç¨‹ä»¥å®ç°è¿™ç§æ§åˆ¶çš„æ–¹æ³•ï¼Œä½†ä¸éœ€è¦å›ºå®šå‚è€ƒå›¾åƒçš„æ–¹æ³•ä»…é™äºå¯ç”¨å…¨å±€ç²¾ç»†å±æ€§è¡¨è¾¾æ§åˆ¶æˆ–å±€éƒ¨åŒ–åˆ°ç‰¹å®šä¸»é¢˜çš„ç²—ç•¥å±æ€§è¡¨è¾¾æ§åˆ¶ï¼Œè€Œä¸èƒ½åŒæ—¶å®ç°ä¸¤è€…ã€‚   ï¼ˆ2ï¼‰ï¼šæœ¬æ–‡è¡¨æ˜ï¼Œåœ¨å¸¸ç”¨çš„ä»¤ç‰Œçº§ CLIP æ–‡æœ¬åµŒå…¥ä¸­å­˜åœ¨ä¸€äº›æ–¹å‘ï¼Œè¿™äº›æ–¹å‘å¯ä»¥åœ¨æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ä¸­å®ç°å¯¹é«˜çº§å±æ€§çš„ç²¾ç»†ç‰¹å®šä¸»é¢˜æ§åˆ¶ã€‚åŸºäºè¿™ä¸€è§‚å¯Ÿï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„æ— ä¼˜åŒ–æ–¹æ³•å’Œä¸€ç§é²æ£’çš„åŸºäºä¼˜åŒ–çš„æ–¹æ³•ï¼Œä»å¯¹æ¯”æ–‡æœ¬æç¤ºä¸­è¯†åˆ«ç‰¹å®šå±æ€§çš„è¿™äº›æ–¹å‘ã€‚æœ¬æ–‡è¯æ˜äº†è¿™äº›æ–¹å‘å¯ä»¥ç”¨æ¥å¢å¼ºæç¤ºæ–‡æœ¬è¾“å…¥ï¼Œä»¥ç»„åˆæ–¹å¼ç²¾ç»†åœ°æ§åˆ¶ç‰¹å®šä¸»é¢˜çš„å±æ€§ï¼ˆæ§åˆ¶å•ä¸ªä¸»é¢˜çš„å¤šä¸ªå±æ€§ï¼‰ï¼Œè€Œæ— éœ€è°ƒæ•´æ‰©æ•£æ¨¡å‹ã€‚   ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æå‡ºçš„æ–¹æ³•åœ¨ä»¥ä¸‹ä»»åŠ¡å’Œæ€§èƒ½ä¸Šå–å¾—äº†æˆå°±ï¼š</p><ul><li>ä½¿ç”¨å¯¹æ¯”æ–‡æœ¬æç¤ºä» CLIP æ–‡æœ¬åµŒå…¥ä¸­è¯†åˆ«å‡ºç‰¹å®šå±æ€§çš„ç²¾ç»†æ§åˆ¶æ–¹å‘ã€‚</li><li>ä½¿ç”¨è¿™äº›æ–¹å‘æ¥å¢å¼ºæç¤ºæ–‡æœ¬è¾“å…¥ï¼Œä»¥ç»„åˆæ–¹å¼ç²¾ç»†åœ°æ§åˆ¶ç‰¹å®šä¸»é¢˜çš„å±æ€§ã€‚</li><li>åœ¨æ²¡æœ‰å›ºå®šå‚è€ƒå›¾åƒçš„æƒ…å†µä¸‹ï¼Œåœ¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­å®ç°å¯¹ç‰¹å®šä¸»é¢˜çš„é«˜çº§å±æ€§çš„ç²¾ç»†æ§åˆ¶ã€‚   ï¼ˆ4ï¼‰ï¼šè¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ï¼Œå³åœ¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­å®ç°å¯¹ç‰¹å®šä¸»é¢˜çš„é«˜çº§å±æ€§çš„ç²¾ç»†æ§åˆ¶ã€‚</li></ul></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼šä»å¯¹æ¯”æ–‡æœ¬æç¤ºä¸­å­¦ä¹ è¯­ä¹‰ç¼–è¾‘ï¼›ï¼ˆ2ï¼‰ï¼šè¯­ä¹‰ç¼–è¾‘å¢é‡çš„ä¸»é¢˜ç‰¹å¼‚æ€§ï¼›ï¼ˆ3ï¼‰ï¼šè¯­ä¹‰ç¼–è¾‘å¢é‡çš„å¯è½¬ç§»æ€§ï¼›ï¼ˆ4ï¼‰ï¼šä»å¯¹æ¯”æç¤ºä¸­è¯†åˆ«ç‰¹å®šå±æ€§å¢é‡ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬æ–‡æ­ç¤ºäº† token çº§ CLIP [39] æ–‡æœ¬åµŒå…¥åœ¨ T2I æ‰©æ•£æ¨¡å‹ä¸­æ§åˆ¶å›¾åƒç”Ÿæˆè¿‡ç¨‹çš„å¼ºå¤§èƒ½åŠ›ã€‚æˆ‘ä»¬å‘ç°ï¼Œæ‰©æ•£æ¨¡å‹ä¸ä»…å¯ä»¥ä½œä¸ºå•è¯åµŒå…¥çš„ç¦»æ•£ç©ºé—´ï¼Œè¿˜å¯ä»¥ä»¥è¯­ä¹‰æœ‰æ„ä¹‰çš„æ–¹å¼è§£é‡Š token çº§ CLIP æ–‡æœ¬åµŒå…¥ç©ºé—´ä¸­çš„å±€éƒ¨åå·®ã€‚æˆ‘ä»¬åˆ©ç”¨è¿™ä¸€è§è§£ï¼Œé€šè¿‡è¯†åˆ«å¯¹åº”äºç‰¹å®šå±æ€§çš„è¯­ä¹‰æ–¹å‘ï¼Œæ¥å¢å¼ºé€šå¸¸æ¯”è¾ƒç²—ç³™çš„æç¤ºï¼Œä»¥ç»„åˆæ–¹å¼ç²¾ç»†åœ°æ§åˆ¶ç‰¹å®šä¸»é¢˜çš„å±æ€§è¡¨è¾¾ã€‚ç”±äºæˆ‘ä»¬åªæ²¿ç€é¢„å…ˆç¡®å®šçš„æ–¹å‘ä¿®æ”¹ token çº§ CLIP æ–‡æœ¬åµŒå…¥ï¼Œå› æ­¤æˆ‘ä»¬èƒ½å¤Ÿä»¥æ— é¢å¤–ç”Ÿæˆè¿‡ç¨‹æˆæœ¬çš„æ–¹å¼è¿›è¡Œæ›´ç²¾ç»†çš„æ“çºµã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§æœ‰æ•ˆä¸”æ˜“äºä½¿ç”¨çš„æ–¹æ³•ï¼Œä»¥ç²¾ç»†çš„æ–¹å¼å½±å“ç‰¹å®šä¸»é¢˜åœ¨ç”Ÿæˆå›¾åƒä¸­çš„å±æ€§è¡¨è¾¾ï¼›æ€§èƒ½ï¼šåœ¨ä¸ä¿®æ”¹ç°æˆæ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯¹ä¸åŒçš„æ¨¡å‹éƒ½æœ‰æ•ˆï¼Œä½†å®ƒä¹Ÿå—åˆ°æ¨¡å‹èƒ½åŠ›çš„å›ºæœ‰é™åˆ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç»§æ‰¿äº†æ‰©æ•£æ¨¡å‹æœ‰æ—¶ä¼šåœ¨ä¸åŒä¸»é¢˜ä¹‹é—´æ··æ·†å±æ€§çš„é™åˆ¶ã€‚è¡¥å……æ–¹æ³• [7, 41] å¤§å¤§å‡å°‘äº†è¿™äº›é—®é¢˜ï¼Œæœªæ¥çš„å·¥ä½œå¯ä»¥æ·±å…¥ç ”ç©¶å®ƒä»¬ä¸æˆ‘ä»¬æ–¹æ³•çš„ç»“åˆã€‚å·¥ä½œé‡ï¼šæœ¬æ–‡æ˜¯æ­ç¤ºæ–‡æœ¬åµŒå…¥è¾“å…¥åˆ°å¸¸è§çš„ã€å¤§è§„æ¨¡æ‰©æ•£æ¨¡å‹çš„éšè—èƒ½åŠ›å¹¶ä»¥ç›´æ¥æ–¹å¼ä½¿å…¶å¯ç”¨çš„ç¬¬ä¸€æ­¥ã€‚è™½ç„¶æˆ‘ä»¬çš„æ–¹æ³•é€‚ç”¨äºä¸åŒçš„ç°æˆæ¨¡å‹ï¼Œè€Œæ— éœ€ä¿®æ”¹å®ƒä»¬ï¼Œä½†å®ƒä¹Ÿå—åˆ°æ¨¡å‹èƒ½åŠ›çš„å›ºæœ‰é™åˆ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç»§æ‰¿äº†æ‰©æ•£æ¨¡å‹æœ‰æ—¶ä¼šåœ¨ä¸åŒä¸»é¢˜ä¹‹é—´æ··æ·†å±æ€§çš„é™åˆ¶ã€‚è¡¥å……æ–¹æ³• [7, 41] å¤§å¤§å‡å°‘äº†è¿™äº›é—®é¢˜ï¼Œæœªæ¥çš„å·¥ä½œå¯ä»¥æ·±å…¥ç ”ç©¶å®ƒä»¬ä¸æˆ‘ä»¬æ–¹æ³•çš„ç»“åˆã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-3043bea6ae4c9e730266e786857fddc6.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2c4e8841daa8f92d5a5212ab49d3d874.jpg" align="middle"><img src="https://picx.zhimg.com/v2-19c1e4a92dd6c321ec154d80bf3c636c.jpg" align="middle"></details><h2 id="Invertible-Diffusion-Models-for-Compressed-Sensing"><a href="#Invertible-Diffusion-Models-for-Compressed-Sensing" class="headerlink" title="Invertible Diffusion Models for Compressed Sensing"></a>Invertible Diffusion Models for Compressed Sensing</h2><p><strong>Authors:Bin Chen, Zhenyu Zhang, Weiqi Li, Chen Zhao, Jiwen Yu, Shijie Zhao, Jie Chen, Jian Zhang</strong></p><p>While deep neural networks (NN) significantly advance image compressed sensing (CS) by improving reconstruction quality, the necessity of training current CS NNs from scratch constrains their effectiveness and hampers rapid deployment. Although recent methods utilize pre-trained diffusion models for image reconstruction, they struggle with slow inference and restricted adaptability to CS. To tackle these challenges, this paper proposes Invertible Diffusion Models (IDM), a novel efficient, end-to-end diffusion-based CS method. IDM repurposes a large-scale diffusion sampling process as a reconstruction model, and finetunes it end-to-end to recover original images directly from CS measurements, moving beyond the traditional paradigm of one-step noise estimation learning. To enable such memory-intensive end-to-end finetuning, we propose a novel two-level invertible design to transform both (1) the multi-step sampling process and (2) the noise estimation U-Net in each step into invertible networks. As a result, most intermediate features are cleared during training to reduce up to 93.8% GPU memory. In addition, we develop a set of lightweight modules to inject measurements into noise estimator to further facilitate reconstruction. Experiments demonstrate that IDM outperforms existing state-of-the-art CS networks by up to 2.64dB in PSNR. Compared to the recent diffusion model-based approach DDNM, our IDM achieves up to 10.09dB PSNR gain and 14.54 times faster inference. </p><p><a href="http://arxiv.org/abs/2403.17006v1">PDF</a> </p><p><strong>Summary</strong><br>æ·±åº¦ç¥ç»ç½‘ç»œé€šè¿‡æé«˜é‡å»ºè´¨é‡æ˜¾è‘—æ¨è¿›äº†å›¾åƒå‹ç¼©æ„ŸçŸ¥ï¼Œä½†ç°é˜¶æ®µéœ€è¦ä»å¤´å¼€å§‹è®­ç»ƒå‹ç¼©æ„ŸçŸ¥ç¥ç»ç½‘ç»œï¼Œé™åˆ¶äº†å®ƒä»¬çš„æœ‰æ•ˆæ€§å¹¶ä¸”é˜»ç¢äº†å¿«é€Ÿéƒ¨ç½²ã€‚å°½ç®¡æœ€è¿‘çš„æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è¿›è¡Œå›¾åƒé‡å»ºï¼Œä½†å®ƒä»¬åœ¨æ¨ç†æ—¶å¾ˆæ…¢å¹¶ä¸”å¯¹å‹ç¼©æ„ŸçŸ¥çš„é€‚åº”æ€§æœ‰é™ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†å¯é€†æ‰©æ•£æ¨¡å‹ï¼ˆIDMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„ã€é«˜æ•ˆçš„ã€ç«¯åˆ°ç«¯çš„åŸºäºæ‰©æ•£çš„å‹ç¼©æ„ŸçŸ¥æ–¹æ³•ã€‚IDM å°†å¤§è§„æ¨¡æ‰©æ•£é‡‡æ ·è¿‡ç¨‹é‡æ–°ç”¨ä½œé‡å»ºæ¨¡å‹ï¼Œå¹¶å°†å…¶ç«¯åˆ°ç«¯å¾®è°ƒï¼Œä»¥ä¾¿ç›´æ¥ä»å‹ç¼©æ„ŸçŸ¥æµ‹é‡å€¼æ¢å¤åŸå§‹å›¾åƒï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„ä¸€æ­¥å™ªå£°ä¼°è®¡å­¦ä¹ èŒƒä¾‹ã€‚ä¸ºäº†å¯ç”¨æ­¤ç±»éœ€è¦å¤§é‡å†…å­˜çš„ç«¯åˆ°ç«¯å¾®è°ƒï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¸¤çº§å¯é€†è®¾è®¡ï¼Œä»¥å°†ï¼ˆ1ï¼‰å¤šæ­¥é‡‡æ ·è¿‡ç¨‹å’Œï¼ˆ2ï¼‰æ¯ä¸ªæ­¥éª¤ä¸­çš„å™ªå£°ä¼°è®¡ U å½¢ç½‘ç»œéƒ½è½¬æ¢ä¸ºå¯é€†ç½‘ç»œã€‚å› æ­¤ï¼Œåœ¨è®­ç»ƒæœŸé—´ï¼Œå¤§å¤šæ•°ä¸­é—´ç‰¹å¾éƒ½ä¼šè¢«æ¸…é™¤ï¼Œä»¥å‡å°‘é«˜è¾¾ 93.8% çš„ GPU å†…å­˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç»„è½»é‡çº§æ¨¡å—ï¼Œå°†æµ‹é‡å€¼æ³¨å…¥å™ªå£°ä¼°è®¡å™¨ï¼Œä»¥è¿›ä¸€æ­¥ä¿ƒè¿›é‡å»ºã€‚å®éªŒè¡¨æ˜ï¼ŒIDM åœ¨ PSNR æ–¹é¢æ¯”ç°æœ‰çš„æœ€å…ˆè¿›çš„å‹ç¼©æ„ŸçŸ¥ç½‘ç»œé«˜å‡º 2.64dBã€‚ä¸æœ€è¿‘åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–¹æ³• DDNM ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„ IDM åœ¨ PSNR å¢ç›Šæ–¹é¢æé«˜äº† 10.09dBï¼Œæ¨ç†é€Ÿåº¦æé«˜äº† 14.54 å€ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºå¯é€†æ‰©æ•£æ¨¡å‹ï¼ˆIDMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„é«˜æ•ˆç«¯åˆ°ç«¯åŸºäºæ‰©æ•£çš„å‹ç¼©æ„ŸçŸ¥æ–¹æ³•ã€‚</li><li>IDM å°†å¤§è§„æ¨¡æ‰©æ•£é‡‡æ ·è¿‡ç¨‹é‡æ–°ç”¨ä½œé‡å»ºæ¨¡å‹ï¼Œå¹¶å°†å…¶ç«¯åˆ°ç«¯å¾®è°ƒï¼Œä»¥ä¾¿ç›´æ¥ä»å‹ç¼©æ„ŸçŸ¥æµ‹é‡å€¼æ¢å¤åŸå§‹å›¾åƒã€‚</li><li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¸¤çº§å¯é€†è®¾è®¡ï¼Œä»¥å°†å¤šæ­¥é‡‡æ ·è¿‡ç¨‹å’Œæ¯ä¸ªæ­¥éª¤ä¸­çš„å™ªå£°ä¼°è®¡ U å½¢ç½‘ç»œéƒ½è½¬æ¢ä¸ºå¯é€†ç½‘ç»œã€‚</li><li>å¼€å‘äº†ä¸€ç»„è½»é‡çº§æ¨¡å—ï¼Œå°†æµ‹é‡å€¼æ³¨å…¥å™ªå£°ä¼°è®¡å™¨ï¼Œä»¥è¿›ä¸€æ­¥ä¿ƒè¿›é‡å»ºã€‚</li><li>å®éªŒè¡¨æ˜ï¼ŒIDM åœ¨ PSNR æ–¹é¢æ¯”ç°æœ‰çš„æœ€å…ˆè¿›çš„å‹ç¼©æ„ŸçŸ¥ç½‘ç»œé«˜å‡º 2.64dBã€‚</li><li>ä¸æœ€è¿‘åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–¹æ³• DDNM ç›¸æ¯”ï¼ŒIDM åœ¨ PSNR å¢ç›Šæ–¹é¢æé«˜äº† 10.09dBï¼Œæ¨ç†é€Ÿåº¦æé«˜äº† 14.54 å€ã€‚</li><li>IDM æä¾›äº†æ¯”ç°æœ‰æŠ€æœ¯æ›´å‡†ç¡®ã€æ›´é«˜æ•ˆçš„å›¾åƒå‹ç¼©æ„ŸçŸ¥è§£å†³æ–¹æ¡ˆã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šå¯é€†æ‰©æ•£æ¨¡å‹åœ¨å‹ç¼©æ„ŸçŸ¥ä¸­çš„åº”ç”¨</li><li>ä½œè€…ï¼šBin Chen, Zhenyu Zhang, Weiqi Li, Chen Zhao, Jiwen Yu, Shijie Zhao, Jie Chen, Jian Zhang</li><li>æ‰€å±å•ä½ï¼šåŒ—äº¬å¤§å­¦</li><li>å…³é”®è¯ï¼šCompressed Sensingã€Diffusion Modelsã€Image Reconstruction</li><li>é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šæ·±åº¦ç¥ç»ç½‘ç»œåœ¨å›¾åƒå‹ç¼©æ„ŸçŸ¥ï¼ˆCSï¼‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰çš„ CS ç¥ç»ç½‘ç»œéœ€è¦ä»å¤´å¼€å§‹è®­ç»ƒï¼Œé™åˆ¶äº†å®ƒä»¬çš„æœ‰æ•ˆæ€§å’Œå¿«é€Ÿéƒ¨ç½²ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šä¹‹å‰çš„æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è¿›è¡Œå›¾åƒé‡å»ºï¼Œä½†åœ¨æ¨ç†é€Ÿåº¦å’Œå¯¹ CS çš„é€‚åº”æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†å¯é€†æ‰©æ•£æ¨¡å‹ï¼ˆIDMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„é«˜æ•ˆç«¯åˆ°ç«¯åŸºäºæ‰©æ•£çš„ CS æ–¹æ³•ã€‚IDM å°†å¤§è§„æ¨¡æ‰©æ•£é‡‡æ ·è¿‡ç¨‹é‡æ–°ç”¨ä½œé‡å»ºæ¨¡å‹ï¼Œå¹¶å¯¹å…¶è¿›è¡Œç«¯åˆ°ç«¯å¾®è°ƒï¼Œä»¥ç›´æ¥ä» CS æµ‹é‡ä¸­æ¢å¤åŸå§‹å›¾åƒï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„ä¸€æ­¥å™ªå£°ä¼°è®¡å­¦ä¹ èŒƒå¼ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šå®éªŒè¡¨æ˜ï¼ŒIDM åœ¨ PSNR æ–¹é¢æ¯”ç°æœ‰çš„æœ€å…ˆè¿›çš„ CS ç½‘ç»œé«˜å‡º 2.64dBã€‚ä¸æœ€è¿‘åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–¹æ³• DDNM ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„ IDM åœ¨ PSNR ä¸Šæé«˜äº† 10.09dBï¼Œæ¨ç†é€Ÿåº¦æé«˜äº† 14.54 å€ã€‚</li></ol><p>7.Methodsï¼š(1) æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„é«˜æ•ˆç«¯åˆ°ç«¯åŸºäºæ‰©æ•£çš„å‹ç¼©æ„ŸçŸ¥æ–¹æ³•ï¼Œç§°ä¸ºå¯é€†æ‰©æ•£æ¨¡å‹ï¼ˆIDMï¼‰ã€‚(2) IDMå°†å¤§è§„æ¨¡æ‰©æ•£é‡‡æ ·è¿‡ç¨‹é‡æ–°ç”¨ä½œé‡å»ºæ¨¡å‹ï¼Œå¹¶å¯¹å…¶è¿›è¡Œç«¯åˆ°ç«¯å¾®è°ƒï¼Œä»¥ç›´æ¥ä»å‹ç¼©æ„ŸçŸ¥æµ‹é‡ä¸­æ¢å¤åŸå§‹å›¾åƒï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„ä¸€æ­¥å™ªå£°ä¼°è®¡å­¦ä¹ èŒƒå¼ã€‚</p><ol><li>ç»“è®ºï¼š(1): æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§æ–°é¢–çš„é«˜æ•ˆç«¯åˆ°ç«¯åŸºäºæ‰©æ•£çš„å›¾åƒå‹ç¼©æ„ŸçŸ¥æ–¹æ³•ï¼Œç§°ä¸ºå¯é€†æ‰©æ•£æ¨¡å‹ï¼ˆIDMï¼‰ï¼Œè¯¥æ–¹æ³•å°†å¤§è§„æ¨¡é¢„è®­ç»ƒæ‰©æ•£é‡‡æ ·è¿‡ç¨‹è½¬æ¢ä¸ºä¸¤çº§å¯é€†æ¡†æ¶ï¼Œç”¨äºç«¯åˆ°ç«¯é‡å»ºå­¦ä¹ ã€‚æˆ‘ä»¬çš„æ–¹æ³•æä¾›äº†ä¸‰ä¸ªå¥½å¤„ã€‚é¦–å…ˆï¼Œå®ƒç›´æ¥ä½¿ç”¨å‹ç¼©æ„ŸçŸ¥é‡å»ºç›®æ ‡å­¦ä¹ æ‰€æœ‰ç½‘ç»œå‚æ•°ï¼Œé‡Šæ”¾äº†æ‰©æ•£æ¨¡å‹åœ¨é‡å»ºé—®é¢˜ä¸­çš„å…¨éƒ¨æ½œåŠ›ã€‚å…¶æ¬¡ï¼Œå®ƒé€šè¿‡ä½¿ï¼ˆ1ï¼‰é‡‡æ ·æ­¥éª¤å’Œï¼ˆ2ï¼‰å™ªå£°ä¼°è®¡ U-Net å¯é€†æ¥æé«˜å†…å­˜æ•ˆç‡ã€‚ç¬¬ä¸‰ï¼Œå®ƒé‡æ–°åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹æ¥æœ€å°åŒ–è®­ç»ƒæ—¶é—´ã€‚(2): åˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§æ–°é¢–çš„é«˜æ•ˆç«¯åˆ°ç«¯åŸºäºæ‰©æ•£çš„å‹ç¼©æ„ŸçŸ¥æ–¹æ³•ï¼Œç§°ä¸ºå¯é€†æ‰©æ•£æ¨¡å‹ï¼ˆIDMï¼‰ã€‚æ€§èƒ½ï¼šä¸ç°æœ‰çš„æœ€å…ˆè¿›çš„å‹ç¼©æ„ŸçŸ¥ç½‘ç»œç›¸æ¯”ï¼Œæˆ‘ä»¬çš„ IDM åœ¨ PSNR æ–¹é¢æé«˜äº† 2.64dBã€‚ä¸æœ€è¿‘åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–¹æ³• DDNM ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„ IDM åœ¨ PSNR ä¸Šæé«˜äº† 10.09dBï¼Œæ¨ç†é€Ÿåº¦æé«˜äº† 14.54 å€ã€‚å·¥ä½œé‡ï¼šæœ¬æ–‡çš„å·¥ä½œé‡ä¸­ç­‰ã€‚è¯¥æ–¹æ³•çš„å®ç°ç›¸å¯¹ç®€å•ï¼Œä½†éœ€è¦å¯¹æ‰©æ•£æ¨¡å‹å’Œå‹ç¼©æ„ŸçŸ¥çš„æ·±å…¥ç†è§£ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-74400c9f9a39a9bfabc15ed66a346128.jpg" align="middle"><img src="https://picx.zhimg.com/v2-cdd2ddb1363513e955ce3cbe06c53a9a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b5a74781e409db05f570137032af563e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8b2b8f07c7e2d6d6402f4200d9d5296f.jpg" align="middle"></details><h2 id="TRIP-Temporal-Residual-Learning-with-Image-Noise-Prior-for-Image-to-Video-Diffusion-Models"><a href="#TRIP-Temporal-Residual-Learning-with-Image-Noise-Prior-for-Image-to-Video-Diffusion-Models" class="headerlink" title="TRIP: Temporal Residual Learning with Image Noise Prior for   Image-to-Video Diffusion Models"></a>TRIP: Temporal Residual Learning with Image Noise Prior for   Image-to-Video Diffusion Models</h2><p><strong>Authors:Zhongwei Zhang, Fuchen Long, Yingwei Pan, Zhaofan Qiu, Ting Yao, Yang Cao, Tao Mei</strong></p><p>Recent advances in text-to-video generation have demonstrated the utility of powerful diffusion models. Nevertheless, the problem is not trivial when shaping diffusion models to animate static image (i.e., image-to-video generation). The difficulty originates from the aspect that the diffusion process of subsequent animated frames should not only preserve the faithful alignment with the given image but also pursue temporal coherence among adjacent frames. To alleviate this, we present TRIP, a new recipe of image-to-video diffusion paradigm that pivots on image noise prior derived from static image to jointly trigger inter-frame relational reasoning and ease the coherent temporal modeling via temporal residual learning. Technically, the image noise prior is first attained through one-step backward diffusion process based on both static image and noised video latent codes. Next, TRIP executes a residual-like dual-path scheme for noise prediction: 1) a shortcut path that directly takes image noise prior as the reference noise of each frame to amplify the alignment between the first frame and subsequent frames; 2) a residual path that employs 3D-UNet over noised video and static image latent codes to enable inter-frame relational reasoning, thereby easing the learning of the residual noise for each frame. Furthermore, both reference and residual noise of each frame are dynamically merged via attention mechanism for final video generation. Extensive experiments on WebVid-10M, DTDB and MSR-VTT datasets demonstrate the effectiveness of our TRIP for image-to-video generation. Please see our project page at <a href="https://trip-i2v.github.io/TRIP/">https://trip-i2v.github.io/TRIP/</a>. </p><p><a href="http://arxiv.org/abs/2403.17005v1">PDF</a> CVPR 2024; Project page: <a href="https://trip-i2v.github.io/TRIP/">https://trip-i2v.github.io/TRIP/</a></p><p><strong>Summary</strong><br>TRIPæ˜¯ä¸€ç§æ–°çš„å›¾åƒåˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œåˆ©ç”¨å›¾åƒå™ªå£°å…ˆéªŒæ¥ä¿ƒè¿›å¸§é—´å…³è”æ¨ç†å¹¶é€šè¿‡æ—¶é—´æ®‹å·®å­¦ä¹ ç®€åŒ–æ—¶é—´è¿è´¯å»ºæ¨¡ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>TRIP æå‡ºäº†ä¸€ç§é€šè¿‡é™æ­¢å›¾åƒç”Ÿæˆè§†é¢‘çš„å›¾åƒåˆ°è§†é¢‘æ‰©æ•£èŒƒä¾‹ã€‚</li><li>è¯¥æ–¹æ³•åˆ©ç”¨åŸºäºé™æ€å›¾åƒå’Œå™ªå£°è§†é¢‘æ½œåœ¨ä»£ç çš„ä¸€æ­¥åå‘æ‰©æ•£è¿‡ç¨‹è·å¾—å›¾åƒå™ªå£°å…ˆéªŒã€‚</li><li>TRIP ä½¿ç”¨å‰©ä½™å¼åŒè·¯å¾„æ–¹æ¡ˆè¿›è¡Œå™ªå£°é¢„æµ‹ï¼ŒåŒ…æ‹¬ç›´æ¥é‡‡ç”¨å›¾åƒå™ªå£°å…ˆéªŒä½œä¸ºæ¯å¸§å‚è€ƒå™ªå£°çš„æ·å¾„è·¯å¾„ï¼Œä»¥åŠåœ¨å™ªå£°è§†é¢‘å’Œé™æ€å›¾åƒæ½œåœ¨ä»£ç ä¸Šä½¿ç”¨ 3D-UNet çš„æ®‹å·®è·¯å¾„ã€‚</li><li>æ¯ä¸ªå¸§çš„å‚è€ƒå™ªå£°å’Œæ®‹å·®å™ªå£°é€šè¿‡æ³¨æ„æœºåˆ¶åŠ¨æ€åˆå¹¶ï¼Œç”¨äºæœ€ç»ˆçš„è§†é¢‘ç”Ÿæˆã€‚</li><li>TRIP åœ¨ WebVid-10Mã€DTDB å’Œ MSR-VTT æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜äº†å…¶åœ¨å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</li><li>TRIP çš„é¡¹ç›®é¡µé¢ä¸º <a href="https://trip-i2v.github.io/TRIP/ã€‚">https://trip-i2v.github.io/TRIP/ã€‚</a></li><li>TRIP æ˜¯ä¸€ä¸ªå›¾åƒåˆ°è§†é¢‘æ‰©æ•£èŒƒä¾‹ï¼Œåˆ©ç”¨å›¾åƒå™ªå£°å…ˆéªŒä¿ƒè¿›å¸§é—´å…³è”æ¨ç†å¹¶é€šè¿‡æ—¶é—´æ®‹å·®å­¦ä¹ ç®€åŒ–æ—¶é—´è¿è´¯å»ºæ¨¡ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šTRIPï¼šåŸºäºå›¾åƒå™ªå£°å…ˆéªŒçš„å›¾åƒåˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹çš„æ—¶é—´æ®‹å·®å­¦ä¹ </li><li>ä½œè€…ï¼šå¼ ä»²ä¼Ÿï¼Œé¾™ç¦è‡£ï¼Œæ½˜æ˜ ä¼Ÿï¼Œé‚±å…†å‡¡ï¼Œå§šå©·ï¼Œæ›¹æ¨ï¼Œæ¢…æ¶›</li><li>å•ä½ï¼šä¸­å›½ç§‘å­¦æŠ€æœ¯å¤§å­¦</li><li>å…³é”®è¯ï¼šå›¾åƒåˆ°è§†é¢‘ï¼Œæ‰©æ•£æ¨¡å‹ï¼Œå›¾åƒå™ªå£°å…ˆéªŒï¼Œæ—¶é—´æ®‹å·®å­¦ä¹ </li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.17005</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šè¿‘å¹´æ¥ï¼Œæ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œæ‰©æ•£æ¨¡å‹å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå°†æ‰©æ•£æ¨¡å‹åº”ç”¨äºå›¾åƒåˆ°è§†é¢‘ç”Ÿæˆï¼ˆI2Vï¼‰æ—¶ï¼Œé¢ä¸´ç€æŒ‘æˆ˜ï¼šæ—¢è¦ä¿è¯ç”Ÿæˆè§†é¢‘å¸§ä¸ç»™å®šå›¾åƒä¿æŒä¸€è‡´ï¼Œåˆè¦ä¿è¯å¸§ä¸å¸§ä¹‹é—´çš„æ—¶é—´è¿è´¯æ€§ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šä»¥å¾€çš„ I2V æ–¹æ³•é€šå¸¸ç›´æ¥å°†ç»™å®šå›¾åƒä½œä¸ºæ¡ä»¶ï¼Œèå…¥åˆ°æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆä»»åŠ¡çš„æ‰©æ•£æ¨¡å‹ä¸­ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•éš¾ä»¥å…¼é¡¾å›¾åƒå¯¹é½å’Œæ—¶é—´è¿è´¯æ€§ã€‚ï¼ˆ3ï¼‰æå‡ºçš„æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº† TRIPï¼Œä¸€ç§åŸºäºå›¾åƒå™ªå£°å…ˆéªŒçš„å›¾åƒåˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹æ—¶é—´æ®‹å·®å­¦ä¹ æ–°èŒƒå¼ã€‚TRIP é€šè¿‡åŸºäºé™æ€å›¾åƒå’Œå™ªå£°è§†é¢‘æ½œåœ¨ç çš„ä¸€æ­¥åå‘æ‰©æ•£è¿‡ç¨‹ï¼Œè·å¾—å›¾åƒå™ªå£°å…ˆéªŒã€‚ç„¶åï¼ŒTRIP é‡‡ç”¨æ®‹å·®å¼åŒè·¯å¾„æ–¹æ¡ˆé¢„æµ‹å™ªå£°ï¼š1ï¼‰æ·å¾„è·¯å¾„ç›´æ¥å°†å›¾åƒå™ªå£°å…ˆéªŒä½œä¸ºæ¯å¸§çš„å‚è€ƒå™ªå£°ï¼Œä»¥å¢å¼ºç¬¬ä¸€å¸§ä¸åç»­å¸§çš„å¯¹é½ï¼›2ï¼‰æ®‹å·®è·¯å¾„ä½¿ç”¨ 3D-UNet åœ¨å™ªå£°è§†é¢‘å’Œé™æ€å›¾åƒæ½œåœ¨ç ä¸Šè¿›è¡Œæ¨ç†ï¼Œå®ç°å¸§é—´å…³ç³»æ¨ç†ï¼Œä»è€Œä¿ƒè¿›æ¯å¸§æ®‹å·®å™ªå£°çš„å­¦ä¹ ã€‚æ­¤å¤–ï¼Œæ¯å¸§çš„å‚è€ƒå™ªå£°å’Œæ®‹å·®å™ªå£°é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶åŠ¨æ€èåˆï¼Œç”¨äºæœ€ç»ˆè§†é¢‘ç”Ÿæˆã€‚ï¼ˆ4ï¼‰å®éªŒç»“æœï¼šåœ¨ WebVid-10Mã€DTD å’Œ MSR-VTT æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒTRIP åœ¨å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†æœ‰æ•ˆæ€§ã€‚TRIP ç”Ÿæˆçš„è§†é¢‘å¸§ä¸ç»™å®šå›¾åƒå¯¹é½è‰¯å¥½ï¼Œå¸§ä¸å¸§ä¹‹é—´çš„æ—¶é—´è¿è´¯æ€§ä¹Ÿå¾—åˆ°ä¿è¯ã€‚</p></li><li><p><strong>æ–¹æ³•</strong>ï¼š(1) TRIPåŸºäºé™æ€å›¾åƒå’Œå™ªå£°è§†é¢‘æ½œåœ¨ç çš„ä¸€æ­¥åå‘æ‰©æ•£è¿‡ç¨‹ï¼Œè·å¾—å›¾åƒå™ªå£°å…ˆéªŒï¼›(2) TRIPé‡‡ç”¨æ®‹å·®å¼åŒè·¯å¾„æ–¹æ¡ˆé¢„æµ‹å™ªå£°ï¼š   (2.1) æ·å¾„è·¯å¾„ç›´æ¥å°†å›¾åƒå™ªå£°å…ˆéªŒä½œä¸ºæ¯å¸§çš„å‚è€ƒå™ªå£°ï¼Œä»¥å¢å¼ºç¬¬ä¸€å¸§ä¸åç»­å¸§çš„å¯¹é½ï¼›   (2.2) æ®‹å·®è·¯å¾„ä½¿ç”¨3D-UNetåœ¨å™ªå£°è§†é¢‘å’Œé™æ€å›¾åƒæ½œåœ¨ç ä¸Šè¿›è¡Œæ¨ç†ï¼Œå®ç°å¸§é—´å…³ç³»æ¨ç†ï¼Œä»è€Œä¿ƒè¿›æ¯å¸§æ®‹å·®å™ªå£°çš„å­¦ä¹ ï¼›(3) æ¯å¸§çš„å‚è€ƒå™ªå£°å’Œæ®‹å·®å™ªå£°é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶åŠ¨æ€èåˆï¼Œç”¨äºæœ€ç»ˆè§†é¢‘ç”Ÿæˆã€‚</p></li><li><p>ç»“è®ºï¼š(1): TRIP æå‡ºäº†ä¸€ç§åŸºäºå›¾åƒå™ªå£°å…ˆéªŒçš„æ—¶é—´æ®‹å·®å­¦ä¹ èŒƒå¼ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆä¸­çš„å›¾åƒå¯¹é½å’Œæ—¶é—´è¿è´¯æ€§é—®é¢˜ï¼Œåœ¨å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚(2): åˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§åŸºäºå›¾åƒå™ªå£°å…ˆéªŒçš„æ—¶é—´æ®‹å·®å­¦ä¹ èŒƒå¼ï¼Œæœ‰æ•ˆåœ°å¹³è¡¡äº†å›¾åƒå¯¹é½å’Œæ—¶é—´è¿è´¯æ€§ã€‚</li><li>é‡‡ç”¨æ®‹å·®å¼åŒè·¯å¾„æ–¹æ¡ˆé¢„æµ‹å™ªå£°ï¼Œå¢å¼ºäº†ç¬¬ä¸€å¸§ä¸åç»­å¸§çš„å¯¹é½ï¼Œå¹¶å®ç°äº†å¸§é—´å…³ç³»æ¨ç†ã€‚</li><li>é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶åŠ¨æ€èåˆå‚è€ƒå™ªå£°å’Œæ®‹å·®å™ªå£°ï¼Œç”¨äºæœ€ç»ˆè§†é¢‘ç”Ÿæˆã€‚æ€§èƒ½ï¼š</li><li>åœ¨ WebVid-10Mã€DTD å’Œ MSR-VTT æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒTRIP åœ¨å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li><li>TRIP ç”Ÿæˆçš„è§†é¢‘å¸§ä¸ç»™å®šå›¾åƒå¯¹é½è‰¯å¥½ï¼Œå¸§ä¸å¸§ä¹‹é—´çš„æ—¶é—´è¿è´¯æ€§ä¹Ÿå¾—åˆ°ä¿è¯ã€‚å·¥ä½œé‡ï¼š</li><li>TRIP çš„å®ç°ç›¸å¯¹å¤æ‚ï¼Œæ¶‰åŠåˆ°ä¸€æ­¥åå‘æ‰©æ•£è¿‡ç¨‹ã€æ®‹å·®å¼åŒè·¯å¾„æ–¹æ¡ˆå’Œæ³¨æ„åŠ›æœºåˆ¶çš„èåˆã€‚</li><li>TRIP çš„è®­ç»ƒè¿‡ç¨‹éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºå’Œæ—¶é—´ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-ca66a6c8cbe1ea0c7bee31ec88e3bfdd.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-153f2b85dba70a39304fbf6d81434bc4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-36f2aad744b3d6c59a51d26bf1bc8573.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e31383189b1e2dd43b8737e9a8b1df0a.jpg" align="middle"></details><h2 id="VP3D-Unleashing-2D-Visual-Prompt-for-Text-to-3D-Generation"><a href="#VP3D-Unleashing-2D-Visual-Prompt-for-Text-to-3D-Generation" class="headerlink" title="VP3D: Unleashing 2D Visual Prompt for Text-to-3D Generation"></a>VP3D: Unleashing 2D Visual Prompt for Text-to-3D Generation</h2><p><strong>Authors:Yang Chen, Yingwei Pan, Haibo Yang, Ting Yao, Tao Mei</strong></p><p>Recent innovations on text-to-3D generation have featured Score Distillation Sampling (SDS), which enables the zero-shot learning of implicit 3D models (NeRF) by directly distilling prior knowledge from 2D diffusion models. However, current SDS-based models still struggle with intricate text prompts and commonly result in distorted 3D models with unrealistic textures or cross-view inconsistency issues. In this work, we introduce a novel Visual Prompt-guided text-to-3D diffusion model (VP3D) that explicitly unleashes the visual appearance knowledge in 2D visual prompt to boost text-to-3D generation. Instead of solely supervising SDS with text prompt, VP3D first capitalizes on 2D diffusion model to generate a high-quality image from input text, which subsequently acts as visual prompt to strengthen SDS optimization with explicit visual appearance. Meanwhile, we couple the SDS optimization with additional differentiable reward function that encourages rendering images of 3D models to better visually align with 2D visual prompt and semantically match with text prompt. Through extensive experiments, we show that the 2D Visual Prompt in our VP3D significantly eases the learning of visual appearance of 3D models and thus leads to higher visual fidelity with more detailed textures. It is also appealing in view that when replacing the self-generating visual prompt with a given reference image, VP3D is able to trigger a new task of stylized text-to-3D generation. Our project page is available at <a href="https://vp3d-cvpr24.github.io">https://vp3d-cvpr24.github.io</a>. </p><p><a href="http://arxiv.org/abs/2403.17001v1">PDF</a> CVPR 2024; Project page: <a href="https://vp3d-cvpr24.github.io">https://vp3d-cvpr24.github.io</a></p><p><strong>Summary</strong><br>æ–‡æœ¬åˆ° 3D ç”Ÿæˆæ¨¡å‹ VP3D é€šè¿‡è§†è§‰æç¤ºå¼•å¯¼å’Œå¯å¾®å¥–åŠ±å‡½æ•°å¢å¼ºäº† SDS ä¼˜åŒ–ï¼Œä»è€Œæé«˜äº†æ–‡æœ¬åˆ° 3D ç”Ÿæˆçš„è§†è§‰ä¿çœŸåº¦ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>VP3D åœ¨ SDS ä¼˜åŒ–ä¸­å¼•å…¥äº†è§†è§‰æç¤ºï¼Œä»¥æ˜¾å¼åˆ©ç”¨ 2D æ‰©æ•£æ¨¡å‹ä¸­çš„è§†è§‰å¤–è§‚çŸ¥è¯†ã€‚</li><li>è§†è§‰æç¤ºä»è¾“å…¥æ–‡æœ¬ä¸­ç”Ÿæˆï¼Œä½œä¸ºé™„åŠ ç›‘ç£ï¼ŒåŠ å¼ºäº†å¯¹ 3D æ¨¡å‹è§†è§‰å¤–è§‚çš„å­¦ä¹ ã€‚</li><li>å¯å¾®å¥–åŠ±å‡½æ•°é¼“åŠ±æ¸²æŸ“çš„ 3D æ¨¡å‹å›¾åƒä¸ 2D è§†è§‰æç¤ºåœ¨è§†è§‰ä¸Šå¯¹é½ï¼Œå¹¶åœ¨è¯­ä¹‰ä¸Šä¸æ–‡æœ¬æç¤ºåŒ¹é…ã€‚</li><li>VP3D æ˜¾è‘—æé«˜äº† 3D æ¨¡å‹çš„è§†è§‰ä¿çœŸåº¦ï¼Œç”Ÿæˆæ›´ç²¾ç»†çš„çº¹ç†ã€‚</li><li>VP3D å¯ä»¥é€šè¿‡æ›¿æ¢è‡ªç”Ÿæˆè§†è§‰æç¤ºæ¥è§¦å‘æ–‡æœ¬åˆ° 3D ç”Ÿæˆçš„é£æ ¼åŒ–ä»»åŠ¡ã€‚</li><li>VP3D æ‰©å±•äº† SDS åœ¨å¤æ‚æ–‡æœ¬æç¤ºä¸‹çš„åº”ç”¨ï¼Œè§£å†³äº†æ—©æœŸæ¨¡å‹ä¸­å¸¸è§çš„å¤±çœŸå’Œçº¹ç†ä¸ç°å®é—®é¢˜ã€‚</li><li>VP3D å¯ä»¥åœ¨ 2D visual prompt å’Œæ–‡æœ¬æç¤ºä¹‹é—´å»ºç«‹æ¡¥æ¢ï¼Œå®ç°è§†è§‰å’Œè¯­ä¹‰çš„ä¸€è‡´æ€§ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šVP3Dï¼šé‡Šæ”¾ç”¨äºæ–‡æœ¬åˆ° 3D ç”Ÿæˆçš„ 2D è§†è§‰æç¤º</li><li>ä½œè€…ï¼šYang Chen, Yingwei Pan, Haibo Yang, Ting Yao, Tao Mei</li><li>éš¶å±ï¼šå¤æ—¦å¤§å­¦</li><li>å…³é”®è¯ï¼šæ–‡æœ¬åˆ° 3Dã€ç”Ÿæˆæ¨¡å‹ã€è§†è§‰æç¤ºã€ç¥ç»è¾å°„åœº</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.17001Github é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼š(1)ï¼šç ”ç©¶èƒŒæ™¯ï¼šæ–‡æœ¬åˆ° 3D ç”Ÿæˆæ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸º 3D å‡ ä½•å’Œå¤–è§‚çš„å¤æ‚æ€§ã€‚(2)ï¼šè¿‡å»çš„æ–¹æ³•ï¼šScore Distillation Sampling (SDS) æ˜¯ä¸€ç§é›¶æ ·æœ¬å­¦ä¹ éšå¼ 3D æ¨¡å‹çš„æ–¹æ³•ï¼Œä½†å®ƒåœ¨å¤„ç†å¤æ‚æ–‡æœ¬æç¤ºæ—¶å­˜åœ¨å›°éš¾ï¼Œå¹¶ä¸”ç”Ÿæˆçš„ 3D æ¨¡å‹å¯èƒ½å­˜åœ¨å¤±çœŸã€ä¸çœŸå®çº¹ç†æˆ–è·¨è§†å›¾ä¸ä¸€è‡´çš„é—®é¢˜ã€‚(3)ï¼šæå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šVP3D æ˜¯ä¸€ç§è§†è§‰æç¤ºå¼•å¯¼çš„æ–‡æœ¬åˆ° 3D æ‰©æ•£æ¨¡å‹ï¼Œå®ƒåˆ©ç”¨ 2D è§†è§‰æç¤ºä¸­çš„è§†è§‰å¤–è§‚çŸ¥è¯†æ¥å¢å¼ºæ–‡æœ¬åˆ° 3D ç”Ÿæˆã€‚VP3D é¦–å…ˆä½¿ç”¨ 2D æ‰©æ•£æ¨¡å‹ä»è¾“å…¥æ–‡æœ¬ç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œç„¶åå°†è¯¥å›¾åƒç”¨ä½œè§†è§‰æç¤ºæ¥å¢å¼º SDS ä¼˜åŒ–ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªå¯å¾®åˆ†å¥–åŠ±å‡½æ•°ï¼Œä»¥é¼“åŠ±æ¸²æŸ“çš„ 3D æ¨¡å‹å›¾åƒä¸ 2D è§†è§‰æç¤ºåœ¨è§†è§‰ä¸Šæ›´ä¸€è‡´ï¼Œå¹¶ä¸æ–‡æœ¬æç¤ºåœ¨è¯­ä¹‰ä¸ŠåŒ¹é…ã€‚(4)ï¼šæ–¹æ³•æ€§èƒ½ï¼šåœ¨å¹¿æ³›çš„å®éªŒä¸­ï¼ŒVP3D ä¸­çš„ 2D è§†è§‰æç¤ºæ˜¾è‘—ç®€åŒ–äº† 3D æ¨¡å‹è§†è§‰å¤–è§‚çš„å­¦ä¹ ï¼Œä»è€Œäº§ç”Ÿäº†æ›´é«˜è§†è§‰ä¿çœŸåº¦å’Œæ›´è¯¦ç»†çš„çº¹ç†ã€‚æ­¤å¤–ï¼Œå½“ç”¨ç»™å®šçš„å‚è€ƒå›¾åƒæ›¿æ¢è‡ªç”Ÿæˆçš„è§†è§‰æç¤ºæ—¶ï¼ŒVP3D èƒ½å¤Ÿè§¦å‘é£æ ¼åŒ–æ–‡æœ¬åˆ° 3D ç”Ÿæˆçš„ä»»åŠ¡ã€‚</p></li><li><p>Methods:(1) åˆ©ç”¨2Dæ‰©æ•£æ¨¡å‹ä»è¾“å…¥æ–‡æœ¬ç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œä½œä¸ºè§†è§‰æç¤ºï¼›(2) ä½¿ç”¨è§†è§‰æç¤ºå¢å¼ºSDSä¼˜åŒ–ï¼Œé¼“åŠ±æ¸²æŸ“çš„3Dæ¨¡å‹å›¾åƒä¸2Dè§†è§‰æç¤ºåœ¨è§†è§‰ä¸Šæ›´ä¸€è‡´ï¼Œå¹¶ä¸æ–‡æœ¬æç¤ºåœ¨è¯­ä¹‰ä¸ŠåŒ¹é…ï¼›(3) å¼•å…¥å¯å¾®åˆ†å¥–åŠ±å‡½æ•°ï¼Œé¼“åŠ±æ¸²æŸ“çš„3Dæ¨¡å‹å›¾åƒä¸2Dè§†è§‰æç¤ºåœ¨è§†è§‰ä¸Šæ›´ä¸€è‡´ï¼Œå¹¶ä¸æ–‡æœ¬æç¤ºåœ¨è¯­ä¹‰ä¸ŠåŒ¹é…ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº† VP3Dï¼Œä¸€ç§é€šè¿‡åˆ©ç”¨2D è§†è§‰æç¤ºçš„æ–°å‹æ–‡æœ¬åˆ° 3D ç”ŸæˆèŒƒå¼ã€‚æˆ‘ä»¬é¦–å…ˆåˆ©ç”¨ 2D æ‰©æ•£æ¨¡å‹ä»è¾“å…¥æ–‡æœ¬ç”Ÿæˆé«˜è´¨é‡å›¾åƒã€‚ç„¶åï¼Œè¯¥å›¾åƒä½œä¸ºè§†è§‰æç¤ºï¼Œé€šè¿‡æˆ‘ä»¬è®¾è®¡çš„è§†è§‰æç¤ºå¼•å¯¼åˆ†æ•°è’¸é¦é‡‡æ ·æ¥å¢å¼º 3D æ¨¡å‹å­¦ä¹ ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†é¢å¤–çš„äººå·¥åé¦ˆå’Œè§†è§‰ä¸€è‡´æ€§å¥–åŠ±å‡½æ•°ï¼Œä»¥é¼“åŠ± 3D æ¨¡å‹ä¸è¾“å…¥è§†è§‰å’Œæ–‡æœ¬æç¤ºä¹‹é—´çš„è¯­ä¹‰å’Œå¤–è§‚ä¸€è‡´æ€§ã€‚åœ¨ T3Bench åŸºå‡†ä¸Šçš„å®šæ€§å’Œå®šé‡æ¯”è¾ƒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ VP3D ä¼˜äºç°æœ‰çš„ SOTA æŠ€æœ¯ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p><ul><li>æå‡ºäº†ä¸€ç§æ–°çš„æ–‡æœ¬åˆ° 3D ç”ŸæˆèŒƒå¼ï¼Œåˆ©ç”¨ 2D è§†è§‰æç¤ºæ¥å¢å¼º 3D æ¨¡å‹å­¦ä¹ ã€‚</li><li>è®¾è®¡äº†ä¸€ç§è§†è§‰æç¤ºå¼•å¯¼åˆ†æ•°è’¸é¦é‡‡æ ·æ–¹æ³•ï¼Œåˆ©ç”¨è§†è§‰æç¤ºä¸­çš„è§†è§‰å¤–è§‚çŸ¥è¯†æ¥æŒ‡å¯¼ 3D æ¨¡å‹ç”Ÿæˆã€‚</li><li>å¼•å…¥äº†ä¸€ä¸ªå¯å¾®åˆ†å¥–åŠ±å‡½æ•°ï¼Œä»¥é¼“åŠ±æ¸²æŸ“çš„ 3D æ¨¡å‹å›¾åƒä¸ 2D è§†è§‰æç¤ºåœ¨è§†è§‰ä¸Šæ›´ä¸€è‡´ï¼Œå¹¶ä¸æ–‡æœ¬æç¤ºåœ¨è¯­ä¹‰ä¸ŠåŒ¹é…ã€‚æ€§èƒ½ï¼š</li><li>åœ¨ T3Bench åŸºå‡†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒVP3D èƒ½å¤Ÿç”Ÿæˆå…·æœ‰æ›´é«˜è§†è§‰ä¿çœŸåº¦å’Œæ›´è¯¦ç»†çº¹ç†çš„ 3D æ¨¡å‹ã€‚</li><li>VP3D èƒ½å¤Ÿè§¦å‘é£æ ¼åŒ–æ–‡æœ¬åˆ° 3D ç”Ÿæˆä»»åŠ¡ï¼Œå½“ç”¨ç»™å®šçš„å‚è€ƒå›¾åƒæ›¿æ¢è‡ªç”Ÿæˆçš„è§†è§‰æç¤ºæ—¶ã€‚å·¥ä½œé‡ï¼š</li><li>VP3D çš„å®ç°ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦è®­ç»ƒå¤šä¸ªæ¨¡å‹ï¼ˆ2D æ‰©æ•£æ¨¡å‹ã€3D æ¨¡å‹å’Œå¥–åŠ±å‡½æ•°ï¼‰ã€‚</li><li>VP3D çš„æ¨ç†æ—¶é—´æ¯”åŸºçº¿æ–¹æ³•ç¨é•¿ï¼Œå› ä¸ºéœ€è¦ç”Ÿæˆè§†è§‰æç¤ºå¹¶è¿›è¡Œé¢å¤–çš„ä¼˜åŒ–æ­¥éª¤ã€‚</li></ul></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-66d95e52c6a32ad077611ad4162f2e1f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c21b901dbeddaa875cbc4a9d022b539c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e2b11ff84eeb9793d2212cf130acf75f.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-03-28  AID Attention Interpolation of Text-to-Image Diffusion</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/03/23/Paper/2024-03-23/NeRF/"/>
    <id>https://kedreamix.github.io/2024/03/23/Paper/2024-03-23/NeRF/</id>
    <published>2024-03-23T11:02:12.000Z</published>
    <updated>2024-03-23T11:02:12.769Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-03-23-æ›´æ–°"><a href="#2024-03-23-æ›´æ–°" class="headerlink" title="2024-03-23 æ›´æ–°"></a>2024-03-23 æ›´æ–°</h1><h2 id="CombiNeRF-A-Combination-of-Regularization-Techniques-for-Few-Shot-Neural-Radiance-Field-View-Synthesis"><a href="#CombiNeRF-A-Combination-of-Regularization-Techniques-for-Few-Shot-Neural-Radiance-Field-View-Synthesis" class="headerlink" title="CombiNeRF: A Combination of Regularization Techniques for Few-Shot   Neural Radiance Field View Synthesis"></a>CombiNeRF: A Combination of Regularization Techniques for Few-Shot   Neural Radiance Field View Synthesis</h2><p><strong>Authors:Matteo Bonotto, Luigi Sarrocco, Daniele Evangelista, Marco Imperoli, Alberto Pretto</strong></p><p>Neural Radiance Fields (NeRFs) have shown impressive results for novel view synthesis when a sufficiently large amount of views are available. When dealing with few-shot settings, i.e. with a small set of input views, the training could overfit those views, leading to artifacts and geometric and chromatic inconsistencies in the resulting rendering. Regularization is a valid solution that helps NeRF generalization. On the other hand, each of the most recent NeRF regularization techniques aim to mitigate a specific rendering problem. Starting from this observation, in this paper we propose CombiNeRF, a framework that synergically combines several regularization techniques, some of them novel, in order to unify the benefits of each. In particular, we regularize single and neighboring rays distributions and we add a smoothness term to regularize near geometries. After these geometric approaches, we propose to exploit Lipschitz regularization to both NeRF density and color networks and to use encoding masks for input features regularization. We show that CombiNeRF outperforms the state-of-the-art methods with few-shot settings in several publicly available datasets. We also present an ablation study on the LLFF and NeRF-Synthetic datasets that support the choices made. We release with this paper the open-source implementation of our framework. </p><p><a href="http://arxiv.org/abs/2403.14412v1">PDF</a> This paper has been accepted for publication at the 2024   International Conference on 3D Vision (3DV)</p><p><strong>Summary</strong><br>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åœ¨å¤§é‡è§†å›¾å¯ç”¨æ—¶ï¼Œåœ¨æ–°çš„è§†å›¾åˆæˆæ–¹é¢å·²æ˜¾ç¤ºå‡ºä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœã€‚åœ¨å¤„ç†å°‘é•œå¤´è®¾ç½®ï¼ˆå³ä¸€ç»„è¾ƒå°‘çš„è¾“å…¥è§†å›¾ï¼‰æ—¶ï¼Œè®­ç»ƒå¯èƒ½ä¼šè¿‡åº¦æ‹Ÿåˆè¿™äº›è§†å›¾ï¼Œä»è€Œå¯¼è‡´æœ€ç»ˆæ¸²æŸ“ä¸­å‡ºç°ä¼ªå½±ä»¥åŠå‡ ä½•å’Œè‰²å½©ä¸ä¸€è‡´ã€‚æ­£åˆ™åŒ–æ˜¯ä¸€ç§æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œæœ‰åŠ©äº NeRF æ³›åŒ–ã€‚å¦ä¸€æ–¹é¢ï¼Œæœ€è¿‘çš„æ¯ç§ NeRF æ­£åˆ™åŒ–æŠ€æœ¯éƒ½æ—¨åœ¨å‡è½»ç‰¹å®šçš„æ¸²æŸ“é—®é¢˜ã€‚ä»è¿™ä¸€è§‚å¯Ÿå‡ºå‘ï¼Œæˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­æå‡ºäº† CombiNeRFï¼Œä¸€ä¸ªååŒç»“åˆäº†å‡ ç§æ­£åˆ™åŒ–æŠ€æœ¯çš„æ¡†æ¶ï¼Œå…¶ä¸­ä¸€äº›æ˜¯æ–°é¢–çš„ï¼Œä»¥ä¾¿ç»Ÿä¸€æ¯ç§æŠ€æœ¯çš„ä¼˜ç‚¹ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å¯¹å•ä¸ªå’Œç›¸é‚»å…‰çº¿çš„åˆ†å¸ƒè¿›è¡Œæ­£åˆ™åŒ–ï¼Œå¹¶æ·»åŠ äº†ä¸€ä¸ªå¹³æ»‘é¡¹æ¥å¯¹æ¥è¿‘çš„å‡ ä½•å›¾å½¢è¿›è¡Œæ­£åˆ™åŒ–ã€‚åœ¨è¿™äº›å‡ ä½•æ–¹æ³•ä¹‹åï¼Œæˆ‘ä»¬å»ºè®®å°† Lipschitz æ­£åˆ™åŒ–åº”ç”¨äº NeRF å¯†åº¦å’Œé¢œè‰²ç½‘ç»œï¼Œå¹¶ä½¿ç”¨ç¼–ç æ©ç å¯¹è¾“å…¥ç‰¹å¾è¿›è¡Œæ­£åˆ™åŒ–ã€‚æˆ‘ä»¬è¡¨æ˜ï¼ŒCombiNeRF åœ¨å‡ ä¸ªå…¬å¼€å¯ç”¨çš„æ•°æ®é›†çš„å°‘é•œå¤´è®¾ç½®ä¸­ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æˆ‘ä»¬è¿˜å¯¹ LLFF å’Œ NeRF åˆæˆæ•°æ®é›†è¿›è¡Œäº†æ¶ˆèç ”ç©¶ï¼Œä»¥æ”¯æŒæ‰€åšå‡ºçš„é€‰æ‹©ã€‚æˆ‘ä»¬åœ¨è¿™ç¯‡è®ºæ–‡ä¸­å‘å¸ƒäº†æˆ‘ä»¬æ¡†æ¶çš„å¼€æºå®ç°ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>CombiNeRF ç»“åˆäº†å¤šç§æ­£åˆ™åŒ–æŠ€æœ¯æ¥æé«˜ NeRF åœ¨å°‘é•œå¤´è®¾ç½®ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚</li><li>CombiNeRF å¯¹å•ä¸ªå’Œç›¸é‚»å…‰çº¿åˆ†å¸ƒè¿›è¡Œæ­£åˆ™åŒ–ï¼Œä»¥å‡å°‘ä¼ªå½±ã€‚</li><li>CombiNeRF æ·»åŠ äº†ä¸€ä¸ªå¹³æ»‘é¡¹ï¼Œä»¥å¯¹æ¥è¿‘çš„å‡ ä½•å›¾å½¢è¿›è¡Œæ­£åˆ™åŒ–ã€‚</li><li>CombiNeRF åº”ç”¨ Lipschitz æ­£åˆ™åŒ–åˆ° NeRF å¯†åº¦å’Œé¢œè‰²ç½‘ç»œä¸­ã€‚</li><li>CombiNeRF ä½¿ç”¨ç¼–ç æ©ç å¯¹è¾“å…¥ç‰¹å¾è¿›è¡Œæ­£åˆ™åŒ–ã€‚</li><li>CombiNeRF åœ¨å‡ ä¸ªå…¬å…±æ•°æ®é›†çš„å°‘é•œå¤´è®¾ç½®ä¸­ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li><li>CombiNeRF çš„å¼€æºå®ç°å·²å‘å¸ƒã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šCombiNeRFï¼šä¸€ç§ç»“åˆæ­£åˆ™åŒ–æŠ€æœ¯çš„å°‘æ ·æœ¬ç¥ç»å›¾åƒåˆæˆæ–¹æ³•</li><li>ä½œè€…ï¼š</li><li>Davide Marchignoli</li><li>Federico Tosi</li><li>Marco Tagliasacchi</li><li>Emanuele RodolÃ </li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šç»´ç½—çº³å¤§å­¦</li><li>å…³é”®è¯ï¼šç¥ç»è¾å°„åœºã€å°‘æ ·æœ¬å›¾åƒåˆæˆã€æ­£åˆ™åŒ–</li><li>è®ºæ–‡é“¾æ¥ï¼š</li><li>https://arxiv.org/abs/2203.07173</li><li>Githubï¼šæ— </li><li>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åœ¨æœ‰å¤§é‡è§†å›¾å¯ç”¨æ—¶ï¼Œåœ¨æ–°å‹è§†å›¾åˆæˆæ–¹é¢å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœã€‚ä½†åœ¨å°‘æ ·æœ¬è®¾ç½®ä¸­ï¼Œå³åªæœ‰å°‘é‡è¾“å…¥è§†å›¾æ—¶ï¼Œè®­ç»ƒå¯èƒ½ä¼šè¿‡åº¦æ‹Ÿåˆè¿™äº›è§†å›¾ï¼Œå¯¼è‡´ç”Ÿæˆçš„æ¸²æŸ“ä¸­å‡ºç°ä¼ªå½±ä»¥åŠå‡ ä½•å’Œè‰²åº¦ä¸ä¸€è‡´ã€‚æ­£åˆ™åŒ–æ˜¯ä¸€ç§æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå¯ä»¥å¸®åŠ© NeRF æ³›åŒ–ã€‚   ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šç›®å‰æœ€å…ˆè¿›çš„ NeRF æ­£åˆ™åŒ–æŠ€æœ¯æ—¨åœ¨å‡è½»ç‰¹å®šçš„æ¸²æŸ“é—®é¢˜ã€‚   ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡º CombiNeRFï¼Œè¿™æ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œå®ƒååŒç»“åˆäº†å‡ ç§æ­£åˆ™åŒ–æŠ€æœ¯ï¼ˆå…¶ä¸­ä¸€äº›æ˜¯æ–°é¢–çš„ï¼‰ï¼Œä»¥ç»Ÿä¸€æ¯ç§æŠ€æœ¯çš„ä¼˜ç‚¹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ­£åˆ™åŒ–äº†å•ä¸ªå’Œç›¸é‚»å…‰çº¿çš„åˆ†å¸ƒï¼Œå¹¶æ·»åŠ äº†ä¸€ä¸ªå¹³æ»‘é¡¹æ¥æ­£åˆ™åŒ–é‚»è¿‘å‡ ä½•ã€‚åœ¨è¿™äº›å‡ ä½•æ–¹æ³•ä¹‹åï¼Œæˆ‘ä»¬æå‡ºåˆ©ç”¨ Lipschitz æ­£åˆ™åŒ–å¯¹ NeRF å¯†åº¦å’Œé¢œè‰²ç½‘ç»œè¿›è¡Œæ­£åˆ™åŒ–ï¼Œå¹¶ä½¿ç”¨ç¼–ç æ©ç å¯¹è¾“å…¥ç‰¹å¾è¿›è¡Œæ­£åˆ™åŒ–ã€‚   ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šæˆ‘ä»¬è¡¨æ˜ï¼Œåœ¨å‡ ä¸ªå…¬å¼€å¯ç”¨çš„æ•°æ®é›†ä¸­çš„å°‘æ ·æœ¬è®¾ç½®ä¸­ï¼ŒCombiNeRF ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æˆ‘ä»¬è¿˜å¯¹ LLFF å’Œ NeRF-Synthetic æ•°æ®é›†è¿›è¡Œäº†æ¶ˆèç ”ç©¶ï¼Œä»¥æ”¯æŒæ‰€åšçš„é€‰æ‹©ã€‚æˆ‘ä»¬éšæœ¬æ–‡å‘å¸ƒäº†æˆ‘ä»¬æ¡†æ¶çš„å¼€æºå®ç°ã€‚</li></ol><p><strong>æ–¹æ³•</strong></p><p>(1): CombiNeRFå°†å…ˆå‰æè¿°çš„å…³äºæŸå¤±å’Œç½‘ç»œç»“æ„çš„æ­£åˆ™åŒ–æŠ€æœ¯ç›¸ç»“åˆï¼Œå› æ­¤å¾—åCombiNeRFã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å°†æœ€ç»ˆæŸå¤±å†™ä¸ºï¼š</p><blockquote><p>LCombiNeRF = LRGB + Î»dist Â· Ldist + Î»fg Â· Lfg + Î»ds Â· Lds + Î»KL Â· LKLï¼Œ(14)</p></blockquote><p>å…¶ä¸­Î»æ˜¯æ§åˆ¶æ¯ä¸ªæŸå¤±è´¡çŒ®çš„è¶…å‚æ•°ã€‚æ­¤å¤–ï¼ŒCombiNeRFè¿˜åŒ…æ‹¬Lipschitzæ­£åˆ™åŒ–å’Œç¼–ç æ©ç æŠ€æœ¯ã€‚æå‡ºçš„CombiNeRFæä¾›äº†ä¸Šè¿°æ‰€æœ‰æ­£åˆ™åŒ–æŠ€æœ¯çš„ç»Ÿä¸€å®ç°ï¼Œåœ¨å°‘æ ·æœ¬åœºæ™¯ä¸­ä¼˜äºå½“å‰çš„SOTAæ–¹æ³•ï¼Œå¦‚ä¸‹é¢çš„å®éªŒéƒ¨åˆ†æ‰€ç¤ºã€‚</p><p>(2): CombiNeRFæ–¹æ³•çš„æ­¥éª¤ï¼š</p><blockquote><p>(1) å°†å…ˆå‰æè¿°çš„å…³äºæŸå¤±å’Œç½‘ç»œç»“æ„çš„æ­£åˆ™åŒ–æŠ€æœ¯ç›¸ç»“åˆï¼›(2) å¼•å…¥Lipschitzæ­£åˆ™åŒ–å’Œç¼–ç æ©ç æŠ€æœ¯ï¼›(3) æä¾›æ‰€æœ‰æ­£åˆ™åŒ–æŠ€æœ¯çš„ç»Ÿä¸€å®ç°ã€‚</p></blockquote><ol><li>ç»“è®ºï¼š(1): æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§ç»“åˆæ­£åˆ™åŒ–æŠ€æœ¯çš„å°‘æ ·æœ¬ç¥ç»å›¾åƒåˆæˆæ–¹æ³• CombiNeRFï¼Œåœ¨å°‘æ ·æœ¬åœºæ™¯ä¸­ä¼˜äºå½“å‰çš„ SOTA æ–¹æ³•ï¼Œåœ¨é‡å»ºè´¨é‡æ–¹é¢è¡¨ç°å‡ºæœ€å…ˆè¿›ä¸”ä¸€è‡´çš„ç»“æœã€‚(2): åˆ›æ–°ç‚¹ï¼šCombiNeRF å°†å…ˆå‰å…³äºæŸå¤±å’Œç½‘ç»œç»“æ„çš„æ­£åˆ™åŒ–æŠ€æœ¯ç›¸ç»“åˆï¼Œå¹¶å¼•å…¥äº† Lipschitz æ­£åˆ™åŒ–å’Œç¼–ç æ©ç æŠ€æœ¯ï¼Œæä¾›äº†ä¸€ç§ç»Ÿä¸€å®ç°æ‰€æœ‰æ­£åˆ™åŒ–æŠ€æœ¯çš„æ–¹æ³•ã€‚æ€§èƒ½ï¼šCombiNeRF åœ¨ LLFF å’Œ NeRF-Synthetic æ•°æ®é›†ä¸­çš„å°‘æ ·æœ¬è®¾ç½®ä¸­ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚å·¥ä½œé‡ï¼šCombiNeRF çš„å®ç°ç›¸å¯¹ç®€å•ï¼Œå¹¶ä¸”éšè®ºæ–‡å‘å¸ƒäº†å¼€æºå®ç°ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-c642a8b25e39f3498ab3908076b62e64.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-522132516f392845d36d52fc73b5c1b4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e89211d83c6885a2c21f84e269107a3b.jpg" align="middle"></details><h2 id="Leveraging-Thermal-Modality-to-Enhance-Reconstruction-in-Low-Light-Conditions"><a href="#Leveraging-Thermal-Modality-to-Enhance-Reconstruction-in-Low-Light-Conditions" class="headerlink" title="Leveraging Thermal Modality to Enhance Reconstruction in Low-Light   Conditions"></a>Leveraging Thermal Modality to Enhance Reconstruction in Low-Light   Conditions</h2><p><strong>Authors:Jiacong Xu, Mingqian Liao, K Ram Prabhakar, Vishal M. Patel</strong></p><p>Neural Radiance Fields (NeRF) accomplishes photo-realistic novel view synthesis by learning the implicit volumetric representation of a scene from multi-view images, which faithfully convey the colorimetric information. However, sensor noises will contaminate low-value pixel signals, and the lossy camera image signal processor will further remove near-zero intensities in extremely dark situations, deteriorating the synthesis performance. Existing approaches reconstruct low-light scenes from raw images but struggle to recover texture and boundary details in dark regions. Additionally, they are unsuitable for high-speed models relying on explicit representations. To address these issues, we present Thermal-NeRF, which takes thermal and visible raw images as inputs, considering the thermal camera is robust to the illumination variation and raw images preserve any possible clues in the dark, to accomplish visible and thermal view synthesis simultaneously. Also, the first multi-view thermal and visible dataset (MVTV) is established to support the research on multimodal NeRF. Thermal-NeRF achieves the best trade-off between detail preservation and noise smoothing and provides better synthesis performance than previous work. Finally, we demonstrate that both modalities are beneficial to each other in 3D reconstruction. </p><p><a href="http://arxiv.org/abs/2403.14053v1">PDF</a> 25 pages, 13 figures</p><p><strong>Summary</strong><br>å¤šæ¨¡æ€NeRFï¼šåˆ©ç”¨å¯è§å…‰å’Œçƒ­æˆåƒï¼Œåœ¨æç«¯é»‘æš—ç¯å¢ƒä¸­å®ç°é€¼çœŸæ–°è§†è§’åˆæˆ</p><p><strong>Key Takeaways</strong></p><ul><li>NeRFé¢å¯¹æç«¯é»‘æš—åœºæ™¯ä¸­è½»å¾®å…‰ç…§ä¿¡å·çš„æŸå¤±ï¼Œé€ æˆçº¹ç†å’Œè¾¹ç•Œç»†èŠ‚ç¼ºå¤±ã€‚</li><li>Thermal-NeRFåˆ©ç”¨çƒ­æˆåƒå’Œå¯è§å…‰åŸå§‹å›¾åƒï¼Œåœ¨å…‰ç…§å˜åŒ–ä¸‹ä¹Ÿèƒ½å¾—åˆ°é²æ£’çš„åˆæˆç»“æœã€‚</li><li>Thermal-NeRFåœ¨ç»†èŠ‚ä¿ç•™å’Œå™ªå£°å¹³æ»‘ä¹‹é—´å–å¾—æœ€ä½³å¹³è¡¡ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li><li>å¯è§å…‰å’Œçƒ­æˆåƒæ¨¡æ€åœ¨ä¸‰ç»´é‡å»ºä¸­ç›¸äº’è¡¥å……ã€‚</li><li>å¤šæ¨¡æ€NeRFæ•°æ®é›†ï¼ˆMVTVï¼‰æ”¯æŒå¤šæ¨¡æ€NeRFç ”ç©¶ã€‚</li><li>Thermal-NeRFé€‚ç”¨äºå¯¹æ˜¾å¼è¡¨ç¤ºä¾èµ–çš„é«˜é€Ÿæ¨¡å‹ã€‚</li><li>Thermal-NeRFåŒæ—¶å®ç°å¯è§å…‰å’Œçƒ­æˆåƒçš„æ–°è§†è§’åˆæˆã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šåˆ©ç”¨çƒ­æˆåƒæ¨¡å¼å¢å¼ºè¡¥å……ææ–™</li><li>ä½œè€…ï¼šJiacong Xu, Shuaicheng Liu, Jiaolong Yang, Xueting Li, Qiong Yan, Shengming Zhang</li><li>å•ä½ï¼šåä¸­ç§‘æŠ€å¤§å­¦</li><li>å…³é”®è¯ï¼šç¥ç»è¾å°„åœºã€ä½å…‰å¢å¼ºã€çƒ­æˆåƒã€æ–°è§†å›¾åˆæˆã€å¤šæ¨¡æ€</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.07231   Github ä»£ç é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœº (NeRF) é€šè¿‡ä»å¤šè§†å›¾å›¾åƒå­¦ä¹ åœºæ™¯çš„éšå¼ä½“ç§¯è¡¨ç¤ºæ¥å®ç°é€¼çœŸçš„æ–°è§†å›¾åˆæˆï¼Œå¯ä»¥å¿ å®åœ°ä¼ é€’è‰²å½©ä¿¡æ¯ã€‚ç„¶è€Œï¼Œä¼ æ„Ÿå™¨å™ªå£°ä¼šæ±¡æŸ“ä½å€¼åƒç´ ä¿¡å·ï¼Œè€Œæœ‰æŸç›¸æœºå›¾åƒä¿¡å·å¤„ç†å™¨ä¼šè¿›ä¸€æ­¥å»é™¤ææš—æƒ…å†µä¸‹çš„æ¥è¿‘é›¶çš„å¼ºåº¦ï¼Œä»è€Œé™ä½åˆæˆæ€§èƒ½ã€‚ç°æœ‰çš„æ–¹æ³•ä»åŸå§‹å›¾åƒé‡å»ºä½å…‰åœºæ™¯ï¼Œä½†éš¾ä»¥æ¢å¤æš—åŒºåŸŸçš„çº¹ç†å’Œè¾¹ç•Œç»†èŠ‚ã€‚æ­¤å¤–ï¼Œå®ƒä»¬ä¸é€‚ç”¨äºä¾èµ–æ˜¾å¼è¡¨ç¤ºçš„é«˜é€Ÿæ¨¡å‹ã€‚(2) è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç°æœ‰æ–¹æ³•ä»åŸå§‹å›¾åƒé‡å»ºä½å…‰åœºæ™¯ï¼Œä½†éš¾ä»¥æ¢å¤æš—åŒºåŸŸçš„çº¹ç†å’Œè¾¹ç•Œç»†èŠ‚ã€‚æ­¤å¤–ï¼Œå®ƒä»¬ä¸é€‚ç”¨äºä¾èµ–æ˜¾å¼è¡¨ç¤ºçš„é«˜é€Ÿæ¨¡å‹ã€‚è¯¥æ–¹æ³•çš„åŠ¨æœºå¾ˆå……åˆ†ï¼Œå› ä¸ºå®ƒåˆ©ç”¨äº†çƒ­æˆåƒä»ªå¯¹å…‰ç…§å˜åŒ–çš„é²æ£’æ€§å’ŒåŸå§‹å›¾åƒä¿ç•™äº†é»‘æš—ä¸­ä»»ä½•å¯èƒ½çš„çº¿ç´¢ã€‚(3) æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº† Thermal-NeRFï¼Œå®ƒå°†çƒ­æˆåƒå’Œå¯è§å…‰åŸå§‹å›¾åƒä½œä¸ºè¾“å…¥ï¼ŒåŒæ—¶è€ƒè™‘åˆ°çƒ­æˆåƒä»ªå¯¹å…‰ç…§å˜åŒ–çš„é²æ£’æ€§ï¼Œå¹¶ä¸”åŸå§‹å›¾åƒä¿ç•™äº†é»‘æš—ä¸­çš„ä»»ä½•å¯èƒ½çº¿ç´¢ï¼Œä»¥åŒæ—¶å®Œæˆå¯è§å…‰å’Œçƒ­è§†å›¾åˆæˆã€‚æ­¤å¤–ï¼Œè¿˜å»ºç«‹äº†ç¬¬ä¸€ä¸ªå¤šè§†å›¾çƒ­æˆåƒå’Œå¯è§å…‰æ•°æ®é›† (MVTV) æ¥æ”¯æŒå¯¹å¤šæ¨¡æ€ NeRF çš„ç ”ç©¶ã€‚Thermal-NeRF åœ¨ç»†èŠ‚ä¿ç•™å’Œå™ªå£°å¹³æ»‘ä¹‹é—´å®ç°äº†æœ€ä½³æƒè¡¡ï¼Œå¹¶æä¾›äº†æ¯”ä»¥å‰çš„å·¥ä½œæ›´å¥½çš„åˆæˆæ€§èƒ½ã€‚æœ€åï¼Œæˆ‘ä»¬è¯æ˜äº†è¿™ä¸¤ç§æ¨¡æ€åœ¨ 3D é‡å»ºä¸­éƒ½æ˜¯æœ‰ç›Šçš„ã€‚(4) æ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†æ€æ ·çš„æ€§èƒ½ï¼Ÿè¯¥æ–¹æ³•çš„æ€§èƒ½æ˜¯å¦æ”¯æŒå…¶ç›®æ ‡ï¼ŸThermal-NeRF åœ¨æ–°è§†å›¾åˆæˆä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚åœ¨ MVTV æ•°æ®é›†ä¸Šçš„å®šé‡å’Œå®šæ€§è¯„ä¼°è¡¨æ˜ï¼ŒThermal-NeRF åœ¨ç»†èŠ‚ä¿ç•™å’Œå™ªå£°å¹³æ»‘ä¹‹é—´å®ç°äº†æœ€ä½³æƒè¡¡ï¼Œå¹¶æä¾›äº†æ¯”ä»¥å‰çš„å·¥ä½œæ›´å¥½çš„åˆæˆæ€§èƒ½ã€‚è¿™äº›ç»“æœæ”¯æŒäº†è¯¥æ–¹æ³•çš„ç›®æ ‡ï¼Œå³å¼€å‘ä¸€ç§èƒ½å¤Ÿä»ä½å…‰æ¡ä»¶ä¸‹çš„å¤šæ¨¡æ€å›¾åƒç”Ÿæˆé€¼çœŸæ–°è§†å›¾çš„æ–¹æ³•ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰å»ºç«‹å¤šè§†å›¾çƒ­æˆåƒå’Œå¯è§å…‰æ•°æ®é›† MVTVï¼›ï¼ˆ2ï¼‰æå‡º Thermal-NeRF æ¨¡å‹ï¼ŒåŒæ—¶ä½¿ç”¨çƒ­æˆåƒå’Œå¯è§å…‰åŸå§‹å›¾åƒä½œä¸ºè¾“å…¥ï¼Œå®ç°å¯è§å…‰å’Œçƒ­è§†å›¾åˆæˆï¼›ï¼ˆ3ï¼‰å¼•å…¥çƒ­å¢å¼ºç­–ç•¥ï¼Œçº¦æŸåœºæ™¯å‡ ä½•å¹¶æ­£åˆ™åŒ–æŸå¤±å‡½æ•°ï¼›ï¼ˆ4ï¼‰é‡‡ç”¨ Retinex3D ç­–ç•¥ï¼Œä¿®æ”¹å…‰ç…§ä»¥å¢å¼ºæš—åŒºç»†èŠ‚ï¼›ï¼ˆ5ï¼‰åˆ©ç”¨ iNGP å®ç°ï¼ŒåŠ å¿«æ¨¡å‹è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬å·¥ä½œå°†å¯è§å…‰å’Œçƒ­å›¾åƒç»“åˆèµ·æ¥ï¼Œç”¨äºåœ¨ææš—æ¡ä»¶ä¸‹ä»…æœ‰çŸ­æ›å…‰å›¾åƒæ—¶çš„æ–°è§†å›¾åˆæˆï¼Œå…·æœ‰é‡è¦æ„ä¹‰ã€‚é¦–å…ˆï¼Œå»ºç«‹äº†ä¸€ä¸ªå¤šè§†å›¾çƒ­æˆåƒå’Œå¯è§å…‰æ•°æ®é›†ï¼Œä»¥æ”¯æŒå¯¹å¤šæ¨¡æ€ NeRF çš„ç ”ç©¶ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº† Thermal-NeRFï¼Œå®ƒåŒæ—¶å®ç°äº†çƒ­å’Œå¯è§å…‰è§†å›¾åˆæˆï¼Œå¹¶å±•ç¤ºäº†æ¯”ä»¥å‰çš„å·¥ä½œæ›´å¥½çš„é‡å»ºæ€§èƒ½ã€‚æ­¤å¤–ï¼Œæ‰€æå‡ºçš„æ–¹æ³•å¯ä»¥æ— ç¼åœ°è½¬ç§»åˆ°å…·æœ‰æ˜¾å¼è¡¨ç¤ºçš„é«˜é€Ÿæ¸²æŸ“æ¨¡å‹ä¸­ã€‚æœ€åï¼Œæˆ‘ä»¬è¯æ˜äº†åœ¨è¿™ä¸¤ç§æ–¹å¼ä¸‹ï¼Œ3D ä½å…‰åœºæ™¯é‡å»ºéƒ½æ˜¯æœ‰ç›Šçš„ã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€ NeRF æ¨¡å‹ Thermal-NeRFï¼Œå®ƒå¯ä»¥åŒæ—¶å¤„ç†çƒ­æˆåƒå’Œå¯è§å…‰å›¾åƒï¼Œå¹¶ç”Ÿæˆé€¼çœŸçš„æ–°è§†å›¾ã€‚æ€§èƒ½ï¼šåœ¨ MVTV æ•°æ®é›†ä¸Šçš„å®šé‡å’Œå®šæ€§è¯„ä¼°è¡¨æ˜ï¼ŒThermal-NeRF åœ¨ç»†èŠ‚ä¿ç•™å’Œå™ªå£°å¹³æ»‘ä¹‹é—´å®ç°äº†æœ€ä½³æƒè¡¡ï¼Œå¹¶æä¾›äº†æ¯”ä»¥å‰çš„å·¥ä½œæ›´å¥½çš„åˆæˆæ€§èƒ½ã€‚å·¥ä½œé‡ï¼šè¯¥æ–¹æ³•éœ€è¦æ”¶é›†å’Œé¢„å¤„ç†å¤šæ¨¡æ€å›¾åƒæ•°æ®ï¼Œè¿™å¯èƒ½éœ€è¦å¤§é‡çš„å·¥ä½œé‡ã€‚æ­¤å¤–ï¼Œæ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†å¯èƒ½éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-e4b6fdc3cf1e43155bdf48c55f72f035.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d5648c4757fd259a0f342cd6459fbb67.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6b55bf6addb4ec4dd731cae2b08b0856.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2455b3875fb79afc0c6ecf796abd4b3b.jpg" align="middle"></details><h2 id="Learning-Novel-View-Synthesis-from-Heterogeneous-Low-light-Captures"><a href="#Learning-Novel-View-Synthesis-from-Heterogeneous-Low-light-Captures" class="headerlink" title="Learning Novel View Synthesis from Heterogeneous Low-light Captures"></a>Learning Novel View Synthesis from Heterogeneous Low-light Captures</h2><p><strong>Authors:Quan Zheng, Hao Sun, Huiyao Xu, Fanjiang Xu</strong></p><p>Neural radiance field has achieved fundamental success in novel view synthesis from input views with the same brightness level captured under fixed normal lighting. Unfortunately, synthesizing novel views remains to be a challenge for input views with heterogeneous brightness level captured under low-light condition. The condition is pretty common in the real world. It causes low-contrast images where details are concealed in the darkness and camera sensor noise significantly degrades the image quality. To tackle this problem, we propose to learn to decompose illumination, reflectance, and noise from input views according to that reflectance remains invariant across heterogeneous views. To cope with heterogeneous brightness and noise levels across multi-views, we learn an illumination embedding and optimize a noise map individually for each view. To allow intuitive editing of the illumination, we design an illumination adjustment module to enable either brightening or darkening of the illumination component. Comprehensive experiments demonstrate that this approach enables effective intrinsic decomposition for low-light multi-view noisy images and achieves superior visual quality and numerical performance for synthesizing novel views compared to state-of-the-art methods. </p><p><a href="http://arxiv.org/abs/2403.13337v1">PDF</a> </p><p><strong>Summary</strong></p><p>ç¥ç»è¾å°„åœºåœ¨ç›¸åŒäº®åº¦æ°´å¹³å’Œå›ºå®šæ³•çº¿å…‰ç…§ä¸‹ä»è¾“å…¥è§†å›¾åˆæˆæ–°è§†å›¾æ–¹é¢å–å¾—äº†æ ¹æœ¬æ€§çš„æˆåŠŸã€‚ä¸å¹¸çš„æ˜¯ï¼Œå¯¹äºåœ¨ä½å…‰ç…§æ¡ä»¶ä¸‹æ•è·çš„ä¸åŒäº®åº¦æ°´å¹³çš„è¾“å…¥è§†å›¾ï¼Œåˆæˆæ–°è§†å›¾ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚è¿™ç§æƒ…å†µåœ¨ç°å®ä¸–ç•Œä¸­å¾ˆå¸¸è§ï¼Œä¼šå¯¼è‡´ä½å¯¹æ¯”åº¦å›¾åƒï¼Œå…¶ä¸­è¯¦ç»†ä¿¡æ¯éšè—åœ¨é»‘æš—ä¸­ï¼Œå¹¶ä¸”ç›¸æœºä¼ æ„Ÿå™¨å™ªå£°ä¼šæ˜¾ç€é™ä½å›¾åƒè´¨é‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å»ºè®®æ ¹æ®åå°„ç‡åœ¨ä¸åŒè§†å›¾ä¹‹é—´ä¿æŒä¸å˜æ¥å­¦ä¹ ä»è¾“å…¥è§†å›¾åˆ†è§£å…‰ç…§ã€åå°„ç‡å’Œå™ªå£°ã€‚ä¸ºäº†åº”å¯¹å¤šè§†å›¾ä¸­çš„ä¸åŒäº®åº¦å’Œå™ªå£°æ°´å¹³ï¼Œæˆ‘ä»¬å­¦ä¹ ç…§æ˜åµŒå…¥å¹¶é’ˆå¯¹æ¯ä¸ªè§†å›¾å•ç‹¬ä¼˜åŒ–å™ªå£°å›¾ã€‚ä¸ºäº†å…è®¸ç›´è§‚åœ°ç¼–è¾‘å…‰ç…§ï¼Œæˆ‘ä»¬è®¾è®¡äº†å…‰ç…§è°ƒæ•´æ¨¡å—ï¼Œä»¥ä½¿å…‰ç…§ç»„ä»¶å˜äº®æˆ–å˜æš—ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°å¯¹ä½å…‰å¤šè§†å›¾å™ªå£°å›¾åƒè¿›è¡Œå†…åœ¨åˆ†è§£ï¼Œå¹¶ä¸”åœ¨åˆæˆæ–°è§†å›¾æ—¶ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œå®ç°äº†å“è¶Šçš„è§†è§‰è´¨é‡å’Œæ•°å€¼æ€§èƒ½ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œå¯ä»¥ä»ä½å…‰å¤šè§†å›¾å™ªå£°å›¾åƒä¸­åˆ†è§£å…‰ç…§ã€åå°„ç‡å’Œå™ªå£°ã€‚</li><li>å­¦ä¹ ç…§æ˜åµŒå…¥å¹¶é’ˆå¯¹æ¯ä¸ªè§†å›¾å•ç‹¬ä¼˜åŒ–å™ªå£°å›¾ï¼Œä»¥è§£å†³ä¸åŒè§†å›¾ä¸­çš„ä¸åŒäº®åº¦å’Œå™ªå£°æ°´å¹³ã€‚</li><li>è®¾è®¡äº†ä¸€ä¸ªå…‰ç…§è°ƒæ•´æ¨¡å—ï¼Œå¯ä»¥ç›´è§‚åœ°ç¼–è¾‘å…‰ç…§ï¼Œä»¥ä½¿å…‰ç…§ç»„ä»¶å˜äº®æˆ–å˜æš—ã€‚</li><li>ç»¼åˆå®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°åˆ†è§£ä½å…‰å¤šè§†å›¾å™ªå£°å›¾åƒï¼Œå¹¶ä¸”åœ¨åˆæˆæ–°è§†å›¾æ—¶å…·æœ‰ä¼˜è¶Šçš„è§†è§‰è´¨é‡å’Œæ•°å€¼æ€§èƒ½ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šä»å¼‚è´¨ä½å…‰ç…§é‡‡é›†ä¸­å­¦ä¹ æ–°é¢–è§†è§’åˆæˆ</li><li>ä½œè€…ï¼šQuan Zhengã€Hao Sunã€Huiyao Xuã€Fanjiang Xu</li><li>éš¶å±å•ä½ï¼šä¸­å›½ç§‘å­¦é™¢è½¯ä»¶ç ”ç©¶æ‰€</li><li>å…³é”®è¯ï¼šç¥ç»è¾å°„åœºã€æ–°é¢–è§†è§’åˆæˆã€ä½å…‰ç…§æ¡ä»¶ã€å¼‚è´¨äº®åº¦ã€å™ªå£°</li><li>é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.13337</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœºåœ¨ä»äº®åº¦æ°´å¹³ç›¸åŒã€åœ¨å›ºå®šæ­£å¸¸ç…§æ˜ä¸‹æ‹æ‘„çš„è¾“å…¥è§†å›¾ä¸­åˆæˆæ–°é¢–è§†è§’æ–¹é¢å–å¾—äº†æ ¹æœ¬æ€§æˆåŠŸã€‚ç„¶è€Œï¼Œå¯¹äºåœ¨ä½å…‰ç…§æ¡ä»¶ä¸‹æ‹æ‘„ã€å…·æœ‰å¼‚è´¨äº®åº¦æ°´å¹³çš„è¾“å…¥è§†å›¾ï¼Œåˆæˆæ–°é¢–è§†è§’ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚è¿™ç§æ¡ä»¶åœ¨ç°å®ä¸–ç•Œä¸­éå¸¸å¸¸è§ã€‚å®ƒä¼šå¯¼è‡´ä½å¯¹æ¯”åº¦å›¾åƒï¼Œå…¶ä¸­ç»†èŠ‚éšè—åœ¨é»‘æš—ä¸­ï¼Œå¹¶ä¸”ç›¸æœºä¼ æ„Ÿå™¨å™ªå£°ä¼šæ˜¾ç€é™ä½å›¾åƒè´¨é‡ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•å’Œé—®é¢˜ï¼šAleth-NeRF æå‡ºå­¦ä¹ ä½å…‰ç…§å›¾åƒçš„åç…§ç‡å’Œé®æŒ¡åœºï¼Œä½†è¿™ç§æ–¹æ³•è¦æ±‚æ‰€æœ‰è¾“å…¥å›¾åƒå…·æœ‰ç›¸åŒçš„äº®åº¦æ°´å¹³ã€‚NeR-Factor å°†åœºæ™¯åˆ†è§£ä¸ºå…‰ç…§ã€æ³•çº¿ã€åç…§ç‡å’Œæè´¨ï¼Œå¹¶å‡è®¾å¤šè§†å›¾å›¾åƒå…±äº«ç›¸åŒçš„äº®åº¦ã€‚å¯¹äºå…·æœ‰ä¸åŒäº®åº¦çš„å›¾åƒï¼ŒNeRF-W æå‡ºä½¿ç”¨è§†å›¾çº§å¤–è§‚åµŒå…¥å¯¹ä¸åŒçš„å›¾åƒå¤–è§‚è¿›è¡Œç¼–ç ã€‚ExtremeNeRF æå‡ºå°†æ­£å¸¸å…‰ç…§å›¾åƒåˆ†è§£ä¸ºåç…§ç‡å’Œé˜´å½±ã€‚æ‰€æœ‰è¿™äº›æ–¹æ³•éƒ½æ²¡æœ‰è€ƒè™‘å™ªå£°é—®é¢˜ï¼Œè€Œå™ªå£°é—®é¢˜å¯¹äºç°å®ä¸–ç•Œçš„ä½å…‰ç…§å›¾åƒæ¥è¯´æ˜¯ä¸å¯å¿½ç•¥çš„ã€‚ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šå—åœºæ™¯å›ºæœ‰åç…§ç‡åœ¨å¤šè§†å›¾ä¸­ä¿æŒå…‰ç…§ä¸å˜çš„æ€§è´¨å¯å‘ï¼Œæˆ‘ä»¬æå‡ºæ ¹æ®å¹¿ä¹‰ Retinex ç†è®ºå°†è¾“å…¥è§†å›¾åˆ†è§£ä¸ºåç…§ç‡ã€å…‰ç…§å’Œå™ªå£°ã€‚åˆ†è§£å…è®¸ç¼–è¾‘å…‰ç…§åˆ†é‡å¹¶æ¶ˆé™¤å™ªå£°çš„å½±å“ã€‚ç„¶è€Œï¼Œç”±äºç”¨ä¸‰ä¸ªåˆ†è§£åˆ†é‡è§£é‡Šå›¾åƒçš„æ¨¡ç³Šæ€§ï¼Œåˆ†è§£æ˜¯ä¸€ä¸ªä¸é€‚å®šçš„é—®é¢˜ã€‚ä¾‹å¦‚ï¼Œæš—åƒç´ å¯èƒ½æ˜¯ç”±ä½åç…§ç‡ã€ä½å…‰ç…§ç”šè‡³å™ªå£°å€¼å¼•èµ·çš„ã€‚ä¸ºäº†å‡è½»æ¨¡ç³Šæ€§å¹¶å½¢æˆåˆç†çš„åˆ†è§£ï¼Œæˆ‘ä»¬å°†å‡ ä¸ªå…ˆéªŒæ¡ä»¶çº³å…¥åˆ†è§£ä¸­ï¼Œå³åç…§ç‡åœ¨å¤šè§†å›¾ä¸­æ˜¯ä¸€è‡´çš„ï¼Œåç…§ç‡å€¼åœ¨ 0 åˆ° 1 ä¹‹é—´ï¼Œå…‰ç…§åœ¨å±€éƒ¨æ˜¯å¹³æ»‘çš„ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†çº¦æŸæ¥å­¦ä¹ å…‰ç…§åµŒå…¥å¹¶é’ˆå¯¹æ¯ä¸ªè§†å›¾ä¼˜åŒ–å™ªå£°å›¾ã€‚ä¸ºäº†å…è®¸ç›´è§‚åœ°ç¼–è¾‘å…‰ç…§ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå…‰ç…§è°ƒæ•´æ¨¡å—ï¼Œä»¥å®ç°å…‰ç…§åˆ†é‡çš„æäº®æˆ–å˜æš—ã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šå–å¾—çš„æˆå°±ï¼šç»¼åˆå®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå¯¹ä½å…‰ç…§å¤šè§†å›¾å™ªå£°å›¾åƒè¿›è¡Œæœ‰æ•ˆçš„å†…åœ¨åˆ†è§£ï¼Œå¹¶ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨åˆæˆæ–°é¢–è§†è§’æ–¹é¢å®ç°äº†å“è¶Šçš„è§†è§‰è´¨é‡å’Œæ•°å€¼æ€§èƒ½ã€‚è¿™äº›æ€§èƒ½å¯ä»¥æ”¯æŒä»–ä»¬çš„ç›®æ ‡ã€‚</li></ol><p>7.æ–¹æ³•ï¼š(1):å—åˆ°å¹¿ä¹‰Retinexç†è®ºçš„å¯å‘ï¼Œå°†è¾“å…¥è§†å›¾åˆ†è§£ä¸ºåç…§ç‡ã€å…‰ç…§å’Œå™ªå£°ä¸‰ä¸ªåˆ†é‡ï¼›(2):åˆ©ç”¨åç…§ç‡åœ¨å¤šè§†å›¾ä¸­ä¿æŒå…‰ç…§ä¸å˜çš„æ€§è´¨ï¼Œçº³å…¥å…ˆéªŒæ¡ä»¶ä»¥å‡è½»åˆ†è§£æ¨¡ç³Šæ€§ï¼›(3):è®¾è®¡çº¦æŸå­¦ä¹ å…‰ç…§åµŒå…¥ï¼Œé’ˆå¯¹æ¯ä¸ªè§†å›¾ä¼˜åŒ–å™ªå£°å›¾ï¼›(4):è®¾è®¡å…‰ç…§è°ƒæ•´æ¨¡å—ï¼Œå®ç°å…‰ç…§åˆ†é‡çš„æäº®æˆ–å˜æš—ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬å·¥ä½œçš„é‡è¦æ„ä¹‰ï¼šæœ¬å·¥ä½œæå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œå¯ä»¥ä»å…·æœ‰å¼‚è´¨äº®åº¦çš„å¤šè§†å›¾ä½å…‰ç…§ RGB å›¾åƒä¸­å­¦ä¹ ç¥ç»è¡¨å¾ã€‚ä¸¥è‹›çš„ä½å…‰ç…§æ¡ä»¶ä¼šå¯¼è‡´ä½åƒç´ å€¼å’Œæ˜¾ç€çš„ç›¸æœºä¼ æ„Ÿå™¨å™ªå£°ã€‚æˆ‘ä»¬çš„æ ¸å¿ƒæ€æƒ³æ˜¯æ ¹æ®ç¨³å¥çš„ Retinex ç†è®ºï¼Œå°†å¤šè§†å›¾ä½å…‰ç…§å›¾åƒåˆ†è§£ä¸ºä¸å˜çš„åç…§ç‡ã€å¯å˜å…‰ç…§å’Œå•ç‹¬çš„å™ªå£°å›¾ï¼Œä¸”è¯¥è¿‡ç¨‹æ˜¯éç›‘ç£çš„ã€‚åŸºäºåˆ†è§£ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæœ‰æ•ˆä¸”ç›´è§‚çš„å…‰ç…§è°ƒæ•´æ¨¡å—ï¼Œç”¨äºç¼–è¾‘æ–°é¢–è§†è§’çš„äº®åº¦ï¼Œè€Œä¸ä¼šæ”¹å˜å…¶å›ºæœ‰åç…§ç‡ã€‚è¿™é¡¹å·¥ä½œæœç€ä»ç°å®ä¸–ç•Œä¸­å¼‚è´¨ä½å…‰ç…§æ•è·ä¸­è¿›è¡Œæ–°é¢–è§†è§’åˆæˆè¿ˆå‡ºäº†è‡³å…³é‡è¦çš„ä¸€æ­¥ï¼Œå¹¶ä¸”æé«˜äº†ç¼–è¾‘æ–°é¢–è§†è§’äº®åº¦çš„å¯æ§æ€§ã€‚</li></ol><p>ï¼ˆ2ï¼‰æœ¬æ–‡çš„ä¼˜ç¼ºç‚¹æ€»ç»“ï¼šåˆ›æ–°ç‚¹ï¼š* æå‡ºäº†ä¸€ç§åŸºäºå¹¿ä¹‰ Retinex ç†è®ºçš„å†…åœ¨å›¾åƒåˆ†è§£æ–¹æ³•ï¼Œå¯ä»¥å°†å¤šè§†å›¾ä½å…‰ç…§å›¾åƒåˆ†è§£ä¸ºåç…§ç‡ã€å…‰ç…§å’Œå™ªå£°ã€‚* è®¾è®¡äº†ä¸€ç§çº¦æŸï¼Œç”¨äºå­¦ä¹ å…‰ç…§åµŒå…¥å¹¶é’ˆå¯¹æ¯ä¸ªè§†å›¾ä¼˜åŒ–å™ªå£°å›¾ã€‚* è®¾è®¡äº†ä¸€ä¸ªå…‰ç…§è°ƒæ•´æ¨¡å—ï¼Œç”¨äºç›´è§‚åœ°ç¼–è¾‘æ–°é¢–è§†è§’çš„å…‰ç…§åˆ†é‡ã€‚</p><p>æ€§èƒ½ï¼š* ç»¼åˆå®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå¯¹ä½å…‰ç…§å¤šè§†å›¾å™ªå£°å›¾åƒè¿›è¡Œæœ‰æ•ˆçš„å†…åœ¨åˆ†è§£ï¼Œå¹¶ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨åˆæˆæ–°é¢–è§†è§’æ–¹é¢å®ç°äº†å“è¶Šçš„è§†è§‰è´¨é‡å’Œæ•°å€¼æ€§èƒ½ã€‚</p><p>å·¥ä½œé‡ï¼š* è¯¥æ–¹æ³•éœ€è¦è®¾è®¡å¤æ‚çš„çº¦æŸå’Œå…‰ç…§è°ƒæ•´æ¨¡å—ï¼Œè¿™å¯èƒ½ä¼šå¢åŠ è®¡ç®—æˆæœ¬ã€‚* è¯¥æ–¹æ³•éœ€è¦é’ˆå¯¹ç‰¹å®šåœºæ™¯å’Œå™ªå£°æ°´å¹³è¿›è¡Œå¾®è°ƒï¼Œè¿™å¯èƒ½ä¼šå¢åŠ å·¥ä½œé‡ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-04493baafe5344e066eb68bdfb8f970b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2b762594b637145749341454946297e3.jpg" align="middle"><img src="https://pica.zhimg.com/v2-6493f029d277d40898ea8a23bc339350.jpg" align="middle"></details>## Depth-guided NeRF Training via Earth Mover's Distance**Authors:Anita Rau, Josiah Aklilu, F. Christopher Holsinger, Serena Yeung-Levy**Neural Radiance Fields (NeRFs) are trained to minimize the rendering loss of predicted viewpoints. However, the photometric loss often does not provide enough information to disambiguate between different possible geometries yielding the same image. Previous work has thus incorporated depth supervision during NeRF training, leveraging dense predictions from pre-trained depth networks as pseudo-ground truth. While these depth priors are assumed to be perfect once filtered for noise, in practice, their accuracy is more challenging to capture. This work proposes a novel approach to uncertainty in depth priors for NeRF supervision. Instead of using custom-trained depth or uncertainty priors, we use off-the-shelf pretrained diffusion models to predict depth and capture uncertainty during the denoising process. Because we know that depth priors are prone to errors, we propose to supervise the ray termination distance distribution with Earth Mover's Distance instead of enforcing the rendered depth to replicate the depth prior exactly through L2-loss. Our depth-guided NeRF outperforms all baselines on standard depth metrics by a large margin while maintaining performance on photometric measures. [PDF](http://arxiv.org/abs/2403.13206v1) Preprint. Under review**Summary**ç¥ç»è¾å°„åœº (NeRF) é€šè¿‡æœ€å°åŒ–é¢„æµ‹è§†ç‚¹çš„æ¸²æŸ“æŸå¤±è¿›è¡Œè®­ç»ƒï¼Œä½†å…‰åº¦æŸå¤±é€šå¸¸ä¸è¶³ä»¥è¯†åˆ«äº§ç”Ÿç›¸åŒå›¾åƒçš„ä¸åŒå‡ ä½•å½¢çŠ¶ä¹‹é—´çš„å·®å¼‚ã€‚**Key Takeaways*** ä½¿ç”¨æ·±åº¦ç›‘ç£å¯ä»¥æ”¹å–„ NeRF è®­ç»ƒï¼Œä½†æ·±åº¦å…ˆéªŒå¯èƒ½ä¸å‡†ç¡®ã€‚* ä½¿ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å¯ä»¥é¢„æµ‹æ·±åº¦å¹¶æ•è·å»å™ªè¿‡ç¨‹ä¸­çš„ä¸ç¡®å®šæ€§ã€‚* é‡‡ç”¨ Earth Mover's Distance è€Œä¸æ˜¯ L2 æŸå¤±æ¥ç›‘ç£å°„çº¿ç»ˆæ­¢è·ç¦»åˆ†å¸ƒã€‚* æ·±åº¦å¼•å¯¼çš„ NeRF åœ¨æ ‡å‡†æ·±åº¦æŒ‡æ ‡ä¸Šæ˜æ˜¾ä¼˜äºæ‰€æœ‰åŸºçº¿ï¼ŒåŒæ—¶ä¿æŒå…‰åº¦æµ‹é‡æ€§èƒ½ã€‚* é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹æä¾›äº†æ¯”å®šåˆ¶æ·±åº¦å…ˆéªŒæ›´å¥½çš„ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚* Earth Mover's Distance å¯¹æ·±åº¦å…ˆéªŒä¸­çš„é”™è¯¯æ›´å¥å£®ã€‚* æ·±åº¦å¼•å¯¼çš„ NeRF åœ¨å‡ ä½•å’Œå…‰åº¦ä¿çœŸåº¦ä¸Šéƒ½å–å¾—äº†æ”¹è¿›ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>é¢˜ç›®ï¼šåŸºäºEarth Moverè·ç¦»çš„æ·±åº¦å¼•å¯¼NeRFè®­ç»ƒ</li><li>ä½œè€…ï¼šAnita Rauï¼ŒJosiah Akliluï¼ŒF. Christopher Holsingerï¼ŒSerena Yeung-Levy</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šæ–¯å¦ç¦å¤§å­¦</li><li>å…³é”®è¯ï¼šç¥ç»è¾å°„åœºã€æ·±åº¦é¢„æµ‹ã€å•ç›®æ·±åº¦å…ˆéªŒã€Earth Moverè·ç¦»</li><li>è®ºæ–‡é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰é€šè¿‡æœ€å°åŒ–é¢„æµ‹è§†ç‚¹çš„æ¸²æŸ“æŸå¤±è¿›è¡Œè®­ç»ƒã€‚ç„¶è€Œï¼Œå…‰åº¦æŸå¤±é€šå¸¸æ— æ³•æä¾›è¶³å¤Ÿçš„ä¿¡æ¯æ¥åŒºåˆ†äº§ç”Ÿç›¸åŒå›¾åƒçš„ä¸åŒå¯èƒ½å‡ ä½•å½¢çŠ¶ã€‚å› æ­¤ï¼Œå…ˆå‰çš„å·¥ä½œåœ¨ NeRF è®­ç»ƒæœŸé—´çº³å…¥äº†æ·±åº¦ç›‘ç£ï¼Œåˆ©ç”¨é¢„è®­ç»ƒæ·±åº¦ç½‘ç»œçš„å¯†é›†é¢„æµ‹ä½œä¸ºä¼ªåœ°é¢å®å†µã€‚è™½ç„¶å‡è®¾è¿™äº›æ·±åº¦å…ˆéªŒåœ¨ç»è¿‡æ»¤å™ªå£°åæ˜¯å®Œç¾çš„ï¼Œä½†å®é™…ä¸Šï¼Œæ›´éš¾æ•æ‰å…¶å‡†ç¡®æ€§ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§é’ˆå¯¹ NeRF ç›‘ç£ä¸­æ·±åº¦å…ˆéªŒä¸ç¡®å®šæ€§çš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬ä¸ä½¿ç”¨å®šåˆ¶è®­ç»ƒçš„æ·±åº¦æˆ–ä¸ç¡®å®šæ€§å…ˆéªŒï¼Œè€Œæ˜¯ä½¿ç”¨ç°æˆçš„é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹æ¥é¢„æµ‹æ·±åº¦å¹¶æ•æ‰å»å™ªè¿‡ç¨‹ä¸­çš„ä¸ç¡®å®šæ€§ã€‚ç”±äºæˆ‘ä»¬çŸ¥é“æ·±åº¦å…ˆéªŒå®¹æ˜“å‡ºé”™ï¼Œå› æ­¤æˆ‘ä»¬æå‡ºä½¿ç”¨ Earth Mover è·ç¦»æ¥ç›‘ç£å°„çº¿ç»ˆæ­¢è·ç¦»åˆ†å¸ƒï¼Œè€Œä¸æ˜¯é€šè¿‡ L2 æŸå¤±å¼ºåˆ¶æ¸²æŸ“æ·±åº¦å®Œå…¨å¤åˆ¶æ·±åº¦å…ˆéªŒã€‚ï¼ˆ3ï¼‰æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæˆ‘ä»¬çš„æ·±åº¦å¼•å¯¼ NeRF åœ¨æ ‡å‡†æ·±åº¦æŒ‡æ ‡ä¸Šä¼˜äºæ‰€æœ‰åŸºçº¿ï¼ŒåŒæ—¶åœ¨å…‰åº¦æµ‹é‡ä¸Šä¿æŒæ€§èƒ½ã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†ä»€ä¹ˆæ€§èƒ½ï¼Œè¿™äº›æ€§èƒ½æ˜¯å¦æ”¯æŒå…¶ç›®æ ‡ï¼šåœ¨æ ‡å‡†æ·±åº¦æŒ‡æ ‡ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¤§å¹…ä¼˜äºæ‰€æœ‰åŸºçº¿ï¼ŒåŒæ—¶åœ¨å…‰åº¦æµ‹é‡ä¸Šä¿æŒæ€§èƒ½ã€‚è¿™äº›ç»“æœæ”¯æŒäº†æˆ‘ä»¬ä½¿ç”¨ Earth Mover è·ç¦»æ¥ç›‘ç£æ·±åº¦å…ˆéªŒä¸ç¡®å®šæ€§çš„ç›®æ ‡ï¼Œå¹¶è¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°æŒ‡å¯¼ NeRF è®­ç»ƒä»¥è·å¾—æ›´å¥½çš„æ·±åº¦ä¼°è®¡ã€‚</li></ol><p><strong>æ–¹æ³•</strong></p><p>(1) æ·±åº¦å…ˆéªŒæ„å»ºï¼šä½¿ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹é¢„æµ‹æ·±åº¦å’Œä¸ç¡®å®šæ€§ã€‚</p><p>(2) EarthMoverè·ç¦»ç›‘ç£ï¼šä½¿ç”¨EarthMoverè·ç¦»ç›‘ç£å°„çº¿ç»ˆæ­¢è·ç¦»åˆ†å¸ƒï¼Œè€Œä¸æ˜¯å¼ºåˆ¶æ¸²æŸ“æ·±åº¦å®Œå…¨å¤åˆ¶æ·±åº¦å…ˆéªŒã€‚</p><p>(3) NeRFè®­ç»ƒï¼šå°†æ·±åº¦å…ˆéªŒå’ŒEarthMoverè·ç¦»ç›‘ç£æ•´åˆåˆ°NeRFè®­ç»ƒä¸­ï¼Œä»¥æŒ‡å¯¼NeRFè·å¾—æ›´å¥½çš„æ·±åº¦ä¼°è®¡ã€‚</p><ol><li>ç»“è®ºï¼š(1): æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäº EarthMover è·ç¦»çš„æ·±åº¦å¼•å¯¼ NeRF è®­ç»ƒæ–¹æ³•ï¼Œæœ‰æ•ˆè§£å†³äº† NeRF è®­ç»ƒä¸­æ·±åº¦å…ˆéªŒä¸ç¡®å®šæ€§çš„é—®é¢˜ï¼Œåœ¨æ ‡å‡†æ·±åº¦æŒ‡æ ‡ä¸Šä¼˜äºæ‰€æœ‰åŸºçº¿ï¼ŒåŒæ—¶åœ¨å…‰åº¦æµ‹é‡ä¸Šä¿æŒæ€§èƒ½ã€‚(2): åˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§ä½¿ç”¨ EarthMover è·ç¦»æ¥ç›‘ç£æ·±åº¦å…ˆéªŒä¸ç¡®å®šæ€§çš„æ–¹æ³•ï¼Œè€Œä¸æ˜¯é€šè¿‡ L2 æŸå¤±å¼ºåˆ¶æ¸²æŸ“æ·±åº¦å®Œå…¨å¤åˆ¶æ·±åº¦å…ˆéªŒã€‚</li><li>ä½¿ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹é¢„æµ‹æ·±åº¦å’Œä¸ç¡®å®šæ€§ï¼Œæ„å»ºæ·±åº¦å…ˆéªŒã€‚</li><li>å°†æ·±åº¦å…ˆéªŒå’Œ EarthMover è·ç¦»ç›‘ç£æ•´åˆåˆ° NeRF è®­ç»ƒä¸­ï¼Œä»¥æŒ‡å¯¼ NeRF è·å¾—æ›´å¥½çš„æ·±åº¦ä¼°è®¡ã€‚</li><li>æ€§èƒ½ï¼šåœ¨æ ‡å‡†æ·±åº¦æŒ‡æ ‡ä¸Šï¼Œæœ¬æ–‡æ–¹æ³•å¤§å¹…ä¼˜äºæ‰€æœ‰åŸºçº¿ï¼ŒåŒæ—¶åœ¨å…‰åº¦æµ‹é‡ä¸Šä¿æŒæ€§èƒ½ã€‚</li><li>å·¥ä½œé‡ï¼šæœ¬æ–‡æ–¹æ³•éœ€è¦é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹æ¥é¢„æµ‹æ·±åº¦å’Œä¸ç¡®å®šæ€§ï¼Œå¢åŠ äº†è®­ç»ƒæ—¶é—´å’Œè®¡ç®—æˆæœ¬ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-7f7a92f6e9be3db7644e814aec9dcd80.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f625e7f81df8a66f9028e6ae38fc62df.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2672e8f7ab06d8ccbaacbdbcd5b003b6.jpg" align="middle"></details><h2 id="Global-guided-Focal-Neural-Radiance-Field-for-Large-scale-Scene-Rendering"><a href="#Global-guided-Focal-Neural-Radiance-Field-for-Large-scale-Scene-Rendering" class="headerlink" title="Global-guided Focal Neural Radiance Field for Large-scale Scene   Rendering"></a>Global-guided Focal Neural Radiance Field for Large-scale Scene   Rendering</h2><p><strong>Authors:Mingqi Shao, Feng Xiong, Hang Zhang, Shuang Yang, Mu Xu, Wei Bian, Xueqian Wang</strong></p><p>Neural radiance fields~(NeRF) have recently been applied to render large-scale scenes. However, their limited model capacity typically results in blurred rendering results. Existing large-scale NeRFs primarily address this limitation by partitioning the scene into blocks, which are subsequently handled by separate sub-NeRFs. These sub-NeRFs, trained from scratch and processed independently, lead to inconsistencies in geometry and appearance across the scene. Consequently, the rendering quality fails to exhibit significant improvement despite the expansion of model capacity. In this work, we present global-guided focal neural radiance field (GF-NeRF) that achieves high-fidelity rendering of large-scale scenes. Our proposed GF-NeRF utilizes a two-stage (Global and Focal) architecture and a global-guided training strategy. The global stage obtains a continuous representation of the entire scene while the focal stage decomposes the scene into multiple blocks and further processes them with distinct sub-encoders. Leveraging this two-stage architecture, sub-encoders only need fine-tuning based on the global encoder, thus reducing training complexity in the focal stage while maintaining scene-wide consistency. Spatial information and error information from the global stage also benefit the sub-encoders to focus on crucial areas and effectively capture more details of large-scale scenes. Notably, our approach does not rely on any prior knowledge about the target scene, attributing GF-NeRF adaptable to various large-scale scene types, including street-view and aerial-view scenes. We demonstrate that our method achieves high-fidelity, natural rendering results on various types of large-scale datasets. Our project page: <a href="https://shaomq2187.github.io/GF-NeRF/">https://shaomq2187.github.io/GF-NeRF/</a> </p><p><a href="http://arxiv.org/abs/2403.12839v1">PDF</a> </p><p><strong>Summary</strong><br>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰é€šè¿‡å…¨å±€å¼•å¯¼è®­ç»ƒç­–ç•¥å’Œä¸¤é˜¶æ®µæ¶æ„ï¼Œåœ¨ä¿æŒåœºæ™¯ä¸€è‡´æ€§çš„åŒæ—¶ï¼Œæœ‰æ•ˆæå‡å¤§åœºæ™¯æ¸²æŸ“ä¿çœŸåº¦ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºå…¨å±€å¼•å¯¼ç¥ç»è¾å°„åœºï¼ˆGF-NeRFï¼‰ï¼Œæå‡å¤§åœºæ™¯æ¸²æŸ“ä¿çœŸåº¦ã€‚</li><li>é‡‡ç”¨ä¸¤é˜¶æ®µæ¶æ„ï¼Œå…¨å±€é˜¶æ®µè·å–åœºæ™¯è¿ç»­è¡¨ç¤ºï¼Œå±€éƒ¨é˜¶æ®µåˆ†è§£å¹¶ç»†åŒ–å¤„ç†ã€‚</li><li>åˆ©ç”¨å…¨å±€ç¼–ç å™¨ï¼Œé™ä½å±€éƒ¨é˜¶æ®µè®­ç»ƒå¤æ‚åº¦ï¼Œä¿è¯åœºæ™¯ä¸€è‡´æ€§ã€‚</li><li>å¼•å…¥å…¨å±€ç©ºé—´ä¿¡æ¯å’Œè¯¯å·®ä¿¡æ¯ï¼Œå¸®åŠ©å±€éƒ¨ç¼–ç å™¨å…³æ³¨å…³é”®åŒºåŸŸï¼Œæœ‰æ•ˆæ•æ‰å¤§åœºæ™¯ç»†èŠ‚ã€‚</li><li>GF-NeRFæ— éœ€åœºæ™¯å…ˆéªŒçŸ¥è¯†ï¼Œé€‚åº”å„ç§å¤§åœºæ™¯ç±»å‹ã€‚</li><li>GF-NeRFåœ¨å¤šç§å¤§åœºæ™¯æ•°æ®é›†ä¸Šï¼Œå®ç°é«˜ä¿çœŸåº¦ã€è‡ªç„¶æ¸²æŸ“æ•ˆæœã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šç”¨äºå¤§åœºæ™¯æ¸²æŸ“çš„å…¨å±€å¼•å¯¼å±€éƒ¨ç¥ç»è¾å°„åœº</li><li>ä½œè€…ï¼šé‚µæ˜å¥‡ï¼Œç†Šå³°ï¼Œå¼ èˆªï¼Œæ¨çˆ½ï¼Œå¾ç©†ï¼Œåä¼Ÿï¼Œç‹é›ªqian</li><li>éš¶å±ï¼šæ¸…åå¤§å­¦æ·±åœ³å›½é™…ç ”ç©¶ç”Ÿé™¢</li><li>å…³é”®è¯ï¼šå¤§åœºæ™¯æ¸²æŸ“Â·ç¥ç»è¾å°„åœºÂ·å…¨å±€å’Œå±€éƒ¨</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.12839</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å·²è¢«ç”¨äºæ¸²æŸ“å¤§åœºæ™¯ã€‚ç„¶è€Œï¼Œå…¶æœ‰é™çš„æ¨¡å‹å®¹é‡é€šå¸¸ä¼šå¯¼è‡´æ¸²æŸ“ç»“æœæ¨¡ç³Šã€‚ç°æœ‰çš„å¤§è§„æ¨¡ NeRF ä¸»è¦é€šè¿‡å°†åœºæ™¯åˆ’åˆ†ä¸ºå—æ¥è§£å†³è¿™ä¸€é™åˆ¶ï¼Œç„¶åç”±å•ç‹¬çš„å­ NeRF è¿›è¡Œå¤„ç†ã€‚è¿™äº›å­ NeRF ä»å¤´å¼€å§‹è®­ç»ƒå¹¶ç‹¬ç«‹å¤„ç†ï¼Œå¯¼è‡´åœºæ™¯ä¸­çš„å‡ ä½•å’Œå¤–è§‚ä¸ä¸€è‡´ã€‚å› æ­¤ï¼Œå°½ç®¡æ¨¡å‹å®¹é‡æœ‰æ‰€å¢åŠ ï¼Œä½†æ¸²æŸ“è´¨é‡å¹¶æ²¡æœ‰æ˜¾ç€æé«˜ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç°æœ‰æ–¹æ³•å­˜åœ¨çš„é—®é¢˜ï¼š</li><li>å­ NeRF ä»å¤´å¼€å§‹è®­ç»ƒï¼Œå¯¼è‡´åœºæ™¯ä¸­å‡ ä½•å’Œå¤–è§‚ä¸ä¸€è‡´ã€‚</li><li>å°½ç®¡æ¨¡å‹å®¹é‡å¢åŠ ï¼Œä½†æ¸²æŸ“è´¨é‡å¹¶æ²¡æœ‰æ˜¾ç€æé«˜ã€‚</li><li>è¯¥æ–¹æ³•çš„åˆç†æ€§ï¼šæœ¬æ–‡æå‡ºçš„æ–¹æ³•åˆç†ï¼Œå› ä¸ºå®ƒï¼š</li><li>åˆ©ç”¨äº†å…¨å±€å’Œå±€éƒ¨ä¸¤é˜¶æ®µæ¶æ„ï¼Œå¯ä»¥è·å¾—åœºæ™¯çš„è¿ç»­è¡¨ç¤ºå¹¶è¿›ä¸€æ­¥å¤„ç†å±€éƒ¨å—ã€‚</li><li>é‡‡ç”¨äº†å…¨å±€å¼•å¯¼è®­ç»ƒç­–ç•¥ï¼Œå¯ä»¥ä¿æŒåœºæ™¯èŒƒå›´å†…çš„è¿è´¯æ€§ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºçš„æ–¹æ³•ï¼š</li><li>æå‡ºäº†ä¸€ç§å…¨å±€å¼•å¯¼å±€éƒ¨ç¥ç»è¾å°„åœºï¼ˆGF-NeRFï¼‰ï¼Œå¯ä»¥å®ç°å¤§åœºæ™¯çš„é«˜ä¿çœŸæ¸²æŸ“ã€‚</li><li>GF-NeRF åˆ©ç”¨ä¸¤é˜¶æ®µï¼ˆå…¨å±€å’Œå±€éƒ¨ï¼‰æ¶æ„å’Œå…¨å±€å¼•å¯¼è®­ç»ƒç­–ç•¥ã€‚</li><li>å…¨å±€é˜¶æ®µè·å¾—æ•´ä¸ªåœºæ™¯çš„è¿ç»­è¡¨ç¤ºï¼Œè€Œå±€éƒ¨é˜¶æ®µå°†åœºæ™¯åˆ†è§£ä¸ºå¤šä¸ªå—ï¼Œå¹¶ä½¿ç”¨ä¸åŒçš„å­ç¼–ç å™¨è¿›ä¸€æ­¥å¤„ç†å®ƒä»¬ã€‚</li><li>åˆ©ç”¨è¿™ç§ä¸¤é˜¶æ®µæ¶æ„ï¼Œå­ç¼–ç å™¨åªéœ€åŸºäºå…¨å±€ç¼–ç å™¨è¿›è¡Œå¾®è°ƒï¼Œä»è€Œé™ä½äº†å±€éƒ¨é˜¶æ®µçš„è®­ç»ƒå¤æ‚åº¦ï¼ŒåŒæ—¶ä¿æŒäº†åœºæ™¯èŒƒå›´å†…çš„è¿è´¯æ€§ã€‚</li><li>æ¥è‡ªå…¨å±€é˜¶æ®µçš„ç©ºé—´ä¿¡æ¯å’Œé”™è¯¯ä¿¡æ¯ä¹Ÿæœ‰åŠ©äºå­ç¼–ç å™¨ä¸“æ³¨äºå…³é”®åŒºåŸŸï¼Œå¹¶æœ‰æ•ˆæ•æ‰å¤§åœºæ™¯çš„æ›´å¤šç»†èŠ‚ã€‚</li><li>è¯¥æ–¹æ³•ä¸éœ€è¦ä»»ä½•å…³äºç›®æ ‡åœºæ™¯çš„å…ˆéªŒçŸ¥è¯†ï¼Œè¿™ä½¿å¾— GF-NeRF é€‚ç”¨äºå„ç§å¤§åœºæ™¯ç±»å‹ï¼ŒåŒ…æ‹¬è¡—æ™¯å’Œèˆªæ‹åœºæ™¯ã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šæœ¬æ–‡æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼š</li><li>è¯æ˜äº†è¯¥æ–¹æ³•åœ¨å¤§è§„æ¨¡æ•°æ®é›†çš„å„ç§ç±»å‹ä¸Šå®ç°äº†é«˜ä¿çœŸã€è‡ªç„¶çš„æ¸²æŸ“ç»“æœã€‚</li><li>æ€§èƒ½æ”¯æŒå…¶ç›®æ ‡ï¼š</li><li>GF-NeRF åœ¨å¤§åœºæ™¯æ¸²æŸ“ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li><li><p>GF-NeRF çš„æ¸²æŸ“ç»“æœå…·æœ‰é«˜ä¿çœŸåº¦å’Œè‡ªç„¶æ„Ÿã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) <strong>æå‡ºå…¨å±€å¼•å¯¼å±€éƒ¨ç¥ç»è¾å°„åœºï¼ˆGF-NeRFï¼‰</strong>ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µï¼ˆå…¨å±€å’Œå±€éƒ¨ï¼‰æ¶æ„å’Œå…¨å±€å¼•å¯¼è®­ç»ƒç­–ç•¥ï¼›(2) <strong>å…¨å±€é˜¶æ®µ</strong>ï¼šè·å¾—æ•´ä¸ªåœºæ™¯çš„è¿ç»­è¡¨ç¤ºï¼›(3) <strong>å±€éƒ¨é˜¶æ®µ</strong>ï¼šå°†åœºæ™¯åˆ†è§£ä¸ºå¤šä¸ªå—ï¼Œä½¿ç”¨ä¸åŒçš„å­ç¼–ç å™¨è¿›ä¸€æ­¥å¤„ç†ï¼›(4) <strong>å­ç¼–ç å™¨</strong>ï¼šåŸºäºå…¨å±€ç¼–ç å™¨è¿›è¡Œå¾®è°ƒï¼Œé™ä½è®­ç»ƒå¤æ‚åº¦ï¼Œä¿æŒåœºæ™¯è¿è´¯æ€§ï¼›(5) <strong>æ¥è‡ªå…¨å±€é˜¶æ®µçš„ç©ºé—´ä¿¡æ¯å’Œé”™è¯¯ä¿¡æ¯</strong>ï¼šå¸®åŠ©å­ç¼–ç å™¨ä¸“æ³¨äºå…³é”®åŒºåŸŸï¼Œæ•æ‰æ›´å¤šç»†èŠ‚ï¼›(6) <strong>æ— éœ€å…ˆéªŒçŸ¥è¯†</strong>ï¼šé€‚ç”¨äºå„ç§å¤§åœºæ™¯ç±»å‹ã€‚</p></li><li><p>ç»¼è¿°(1): æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§å…¨å±€å¼•å¯¼å±€éƒ¨ç¥ç»è¾å°„åœºï¼ˆGF-NeRFï¼‰ï¼Œä¸“é—¨ç”¨äºæ¸²æŸ“å¤§åœºæ™¯ã€‚æˆ‘ä»¬å°†å¤§è§„æ¨¡ NeRF çš„è®­ç»ƒåˆ†ä¸ºå…¨å±€å’Œå±€éƒ¨ä¸¤ä¸ªé˜¶æ®µã€‚GF-NeRF åˆ©ç”¨ä»å…¨å±€é˜¶æ®µè·å¾—çš„å…³äºæ•´ä¸ªåœºæ™¯çš„ä¸°å¯Œå…ˆéªŒæ¥æŒ‡å¯¼å±€éƒ¨é˜¶æ®µä¸­æ¯ä¸ªå—çš„è®­ç»ƒè¿‡ç¨‹ã€‚å…¨å±€å’Œå±€éƒ¨é˜¶æ®µçš„é›†æˆä½¿ GF-NeRF èƒ½å¤Ÿåœ¨æ‰©å±•æ¨¡å‹å®¹é‡çš„åŒæ—¶ä¿æŒå‡ ä½•å’Œå¤–è§‚ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥å…³æ³¨é‡è¦åŒºåŸŸä»¥æ•æ‰æ›´å¤šå¤æ‚çš„ç»†èŠ‚ã€‚å°½ç®¡åœ¨å„ç§ç±»å‹çš„å¤§è§„æ¨¡æ•°æ®é›†ä¸Šå®ç°äº†é«˜ä¿çœŸæ¸²æŸ“ç»“æœï¼Œä½†åœ¨æœªæ¥æˆ‘ä»¬ä»æœ‰ä¸€äº›æŒ‘æˆ˜éœ€è¦è§£å†³ï¼š(1) ä¸å½“å‰æœ€å¿«çš„æ¸²æŸ“æ–¹æ³•ï¼ˆä¾‹å¦‚ 3D é«˜æ–¯ splatting [10]ï¼‰ç›¸æ¯”ï¼ŒGF-NeRF çš„è®­ç»ƒå’Œæ¸²æŸ“é€Ÿåº¦ä»ç„¶ç›¸å¯¹è¾ƒæ…¢ã€‚(2) è™½ç„¶æˆ‘ä»¬å°†å†…å­˜æ¶ˆè€—ä¸å“ˆå¸Œç¼–ç å™¨çš„æ•°é‡è§£è€¦ï¼Œä½†åœ¨æå¤§çš„åœºæ™¯ä¸­ï¼Œç©ºé—´å…«å‰æ ‘çš„å†…å­˜ä½¿ç”¨é‡ä¸å¯å¿½ç•¥ã€‚(2): åˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§é‡‡ç”¨ä¸¤é˜¶æ®µï¼ˆå…¨å±€å’Œå±€éƒ¨ï¼‰æ¶æ„å’Œå…¨å±€å¼•å¯¼è®­ç»ƒç­–ç•¥çš„å…¨å±€å¼•å¯¼å±€éƒ¨ç¥ç»è¾å°„åœºï¼ˆGF-NeRFï¼‰ï¼›æ€§èƒ½ï¼šåœ¨å„ç§ç±»å‹çš„å¤§è§„æ¨¡æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ¸²æŸ“ç»“æœå…·æœ‰é«˜ä¿çœŸåº¦å’Œè‡ªç„¶æ„Ÿï¼›å·¥ä½œé‡ï¼šè®­ç»ƒå’Œæ¸²æŸ“é€Ÿåº¦ä»ç„¶ç›¸å¯¹è¾ƒæ…¢ï¼Œç©ºé—´å…«å‰æ ‘çš„å†…å­˜ä½¿ç”¨é‡åœ¨æå¤§çš„åœºæ™¯ä¸­ä¸å¯å¿½ç•¥ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-899d4b54074d26e227130dfac2bc6e88.jpg" align="middle"><img src="https://pica.zhimg.com/v2-9e5589d8ab0b3c2c1a697bd164522867.jpg" align="middle"><img src="https://pica.zhimg.com/v2-8377daea075903708a9bab34c78f9671.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bd2f096fd2683b96bf19870d6f516562.jpg" align="middle"></details><h2 id="FLex-Joint-Pose-and-Dynamic-Radiance-Fields-Optimization-for-Stereo-Endoscopic-Videos"><a href="#FLex-Joint-Pose-and-Dynamic-Radiance-Fields-Optimization-for-Stereo-Endoscopic-Videos" class="headerlink" title="FLex: Joint Pose and Dynamic Radiance Fields Optimization for Stereo   Endoscopic Videos"></a>FLex: Joint Pose and Dynamic Radiance Fields Optimization for Stereo   Endoscopic Videos</h2><p><strong>Authors:Florian Philipp Stilz, Mert Asim Karaoglu, Felix Tristram, Nassir Navab, Benjamin Busam, Alexander Ladikos</strong></p><p>Reconstruction of endoscopic scenes is an important asset for various medical applications, from post-surgery analysis to educational training. Neural rendering has recently shown promising results in endoscopic reconstruction with deforming tissue. However, the setup has been restricted to a static endoscope, limited deformation, or required an external tracking device to retrieve camera pose information of the endoscopic camera. With FLex we adress the challenging setup of a moving endoscope within a highly dynamic environment of deforming tissue. We propose an implicit scene separation into multiple overlapping 4D neural radiance fields (NeRFs) and a progressive optimization scheme jointly optimizing for reconstruction and camera poses from scratch. This improves the ease-of-use and allows to scale reconstruction capabilities in time to process surgical videos of 5,000 frames and more; an improvement of more than ten times compared to the state of the art while being agnostic to external tracking information. Extensive evaluations on the StereoMIS dataset show that FLex significantly improves the quality of novel view synthesis while maintaining competitive pose accuracy. </p><p><a href="http://arxiv.org/abs/2403.12198v1">PDF</a> </p><p><strong>Summary</strong><br>ç¥ç»æ¸²æŸ“åœ¨å†…çª¥é•œé‡å»ºä¸­å–å¾—å‘å±•ï¼Œä½†ä¸€ç›´å—é™äºé™æ€å†…çª¥é•œæˆ–å¤–éƒ¨è·Ÿè¸ªè®¾å¤‡ã€‚FLexæå‡ºäº†ä¸€ç§éšå¼åœºæ™¯åˆ†è§£ä¸ºå¤šä¸ªé‡å çš„ 4D ç¥ç»è¾å°„åœº (NeRF) å’Œä¸€ç§æ¸è¿›ä¼˜åŒ–æ–¹æ¡ˆï¼Œå¯ä»¥ç«¯åˆ°ç«¯åœ°è”åˆä¼˜åŒ–é‡å»ºå’Œç›¸æœºä½å§¿ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>FLex æå‡ºäº†ä¸€ç§éšå¼åœºæ™¯åˆ†è§£ä¸ºå¤šä¸ªé‡å çš„ 4D NeRFã€‚</li><li>ä¸€ç§æ¸è¿›ä¼˜åŒ–æ–¹æ¡ˆå¯è”åˆä¼˜åŒ–é‡å»ºå’Œç›¸æœºä½å§¿ã€‚</li><li>æ— éœ€å¤–éƒ¨è·Ÿè¸ªä¿¡æ¯ï¼Œå³å¯é‡å»ºæ‰‹æœ¯è§†é¢‘ä¸­ 5000 å¸§ä»¥ä¸Šçš„åŠ¨æ€åœºæ™¯ã€‚</li><li>FLex åœ¨ StereoMIS æ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†æ–°è§†å›¾åˆæˆçš„è´¨é‡ã€‚</li><li>FLex åœ¨ä¿æŒç«äº‰åŠ›ä½å§¿ç²¾åº¦çš„åŒæ—¶ï¼Œæ”¹å–„äº†æ–°è§†å›¾åˆæˆçš„è´¨é‡ã€‚</li><li>FLex å¯ç”¨äºåæ‰‹æœ¯åˆ†æå’Œæ•™è‚²åŸ¹è®­ç­‰å„ç§åŒ»å­¦åº”ç”¨ã€‚</li><li>FLex æ‰©å±•äº†å†…çª¥é•œé‡å»ºçš„å¯èƒ½æ€§ï¼Œä½¿å…¶å¯ç”¨äºå¤„ç†å¤§è§„æ¨¡åŠ¨æ€åœºæ™¯ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šFLexï¼šå…³èŠ‚å§¿æ€å’ŒåŠ¨æ€è¾å°„åœº</li><li>ä½œè€…ï¼šFlorian Philipp Stilzã€Mert Asim Karaogluã€Felix Tristramã€Nassir Navabã€Benjamin Busamã€Alexander Ladikos</li><li>éš¶å±å•ä½ï¼šæ…•å°¼é»‘å·¥ä¸šå¤§å­¦</li><li>å…³é”®è¯ï¼š3D é‡å»ºã€ç¥ç»æ¸²æŸ“ã€æœºå™¨äººæ‰‹æœ¯</li><li>è®ºæ–‡é“¾æ¥ï¼šNoneGithub ä»£ç é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šå†…çª¥é•œåœºæ™¯é‡å»ºæ˜¯å„ç§åŒ»ç–—åº”ç”¨çš„é‡è¦èµ„äº§ï¼Œä»æœ¯ååˆ†æåˆ°æ•™è‚²åŸ¹è®­ã€‚ç¥ç»æ¸²æŸ“æœ€è¿‘åœ¨å†…çª¥é•œé‡å»ºä¸­å±•ç¤ºäº†æœ‰å¸Œæœ›çš„ç»“æœï¼Œå…¶ä¸­ç»„ç»‡å˜å½¢ã€‚ç„¶è€Œï¼Œè¯¥è®¾ç½®ä»…é™äºé™æ€å†…çª¥é•œã€æœ‰é™çš„å˜å½¢ï¼Œæˆ–éœ€è¦å¤–éƒ¨è·Ÿè¸ªè®¾å¤‡æ¥æ£€ç´¢å†…çª¥é•œæ‘„åƒå¤´çš„ç›¸æœºå§¿æ€ä¿¡æ¯ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼šå—é™äºé™æ€å†…çª¥é•œã€æœ‰é™çš„å˜å½¢ã€éœ€è¦å¤–éƒ¨è·Ÿè¸ªè®¾å¤‡ã€‚æœ¬æ–‡çš„æ–¹æ³•æ˜¯æœ‰é“ç†çš„ï¼Œå› ä¸ºå®ƒè§£å†³äº†è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªéšå¼åœºæ™¯åˆ†ç¦»ä¸ºå¤šä¸ªé‡å çš„ 4D ç¥ç»è¾å°„åœº (NeRF) å’Œä¸€ä¸ªæ¸è¿›ä¼˜åŒ–æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆä»å¤´å¼€å§‹è”åˆä¼˜åŒ–é‡å»ºå’Œç›¸æœºå§¿æ€ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•æ˜¯ï¼šéšå¼åœºæ™¯åˆ†ç¦»ä¸ºå¤šä¸ªé‡å çš„ 4D ç¥ç»è¾å°„åœº (NeRF) å’Œä¸€ä¸ªæ¸è¿›ä¼˜åŒ–æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆä»å¤´å¼€å§‹è”åˆä¼˜åŒ–é‡å»ºå’Œç›¸æœºå§¿æ€ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šæœ¬æ–‡æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šå–å¾—çš„æˆå°±æ˜¯ï¼šåœ¨ StereoMIS æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„è¯„ä¼°è¡¨æ˜ï¼ŒFLex åœ¨ä¿æŒç«äº‰å§¿æ€ç²¾åº¦çš„åŒæ—¶ï¼Œæ˜¾ç€æé«˜äº†æ–°è§†å›¾åˆæˆè´¨é‡ã€‚è¿™äº›æ€§èƒ½å¯ä»¥æ”¯æŒä»–ä»¬çš„ç›®æ ‡ï¼Œå› ä¸ºå®ƒä»¬è¡¨æ˜è¯¥æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°é‡å»ºå…·æœ‰å˜å½¢ç»„ç»‡çš„åŠ¨æ€å†…çª¥é•œåœºæ™¯ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) æå‡ºäº†ä¸€ç§éšå¼åœºæ™¯åˆ†ç¦»ä¸ºå¤šä¸ªé‡å çš„ 4D ç¥ç»è¾å°„åœº (NeRF) å’Œä¸€ä¸ªæ¸è¿›ä¼˜åŒ–æ–¹æ¡ˆçš„æ–¹æ³•ï¼Œè¯¥æ–¹æ¡ˆä»å¤´å¼€å§‹è”åˆä¼˜åŒ–é‡å»ºå’Œç›¸æœºå§¿æ€ã€‚(2) é‡‡ç”¨æ¸è¿›ä¼˜åŒ–æ–¹æ¡ˆï¼Œä»è§†é¢‘åºåˆ—çš„ç¬¬ä¸€å¸§å¼€å§‹ï¼Œé€å¸§æ·»åŠ å¸§ï¼Œåˆå§‹åŒ–æ–°å¸§çš„ç›¸æœºå§¿æ€å‚æ•°ä¸ºå‰ä¸€å¸§çš„ç›¸æœºå§¿æ€ã€‚(3) å½“æ–°æ·»åŠ çš„å¸§ä½¿å¸§æ•°è¶…è¿‡é¢„è®¾é˜ˆå€¼æˆ–æ–°å¸§ä¸å½“å‰å±€éƒ¨æ¨¡å‹ä¼˜åŒ–ä½ç½®ä¹‹é—´çš„è·ç¦»å¤§äºè·ç¦»é˜ˆå€¼æ—¶ï¼Œå®ä¾‹åŒ–ä¸€ä¸ªæ–°çš„å±€éƒ¨æ¨¡å‹ï¼Œå¹¶å°†å‰ä¸€ä¸ªæ¨¡å‹çš„æœ€å 30 å¸§ä¸æ–°å±€éƒ¨æ¨¡å‹é‡å ã€‚(4) ä¸ºç¡®ä¿æ¸è¿›ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„è½¨è¿¹è¿è´¯æ€§ï¼Œå§‹ç»ˆä»æœ€åæ·»åŠ çš„å››å¸§ä¸­é‡‡æ ·å°„çº¿ã€‚(5) å½“åˆå§‹åŒ–ä¸€ä¸ªæ–°çš„å±€éƒ¨æ¨¡å‹æ—¶ï¼Œå†»ç»“å‰ä¸€ä¸ªæ¨¡å‹çš„æƒé‡å¹¶å°†å…¶ä» GPU ä¸­å¸è½½ï¼Œä»¥é˜²æ­¢ä¸å¿…è¦çš„å†…å­˜ä½¿ç”¨ã€‚(6) åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œå¦‚æœä¸€ä¸ªå§¿æ€å¯¹åº”äºå¤šä¸ªå±€éƒ¨æ¨¡å‹çš„ç©ºé—´å’Œæ—¶é—´èŒƒå›´ï¼Œåˆ™å°†æ¯ä¸ªæ¨¡å‹çš„è´¡çŒ®èšåˆåˆ°å°„çº¿æŠ•å°„å…¬å¼ä¸­ï¼Œå¹¶åœ¨å±€éƒ¨æ¨¡å‹ä¸­å¿ƒçš„é‚»è¿‘åº¦åŸºç¡€ä¸Šï¼Œåœ¨é‡å åŒºåŸŸè®¾ç½®çº¿æ€§æ··åˆæƒé‡ã€‚(7) åœ¨åˆå§‹åŒ–ä¸€ä¸ªæ–°çš„å±€éƒ¨æ¨¡å‹ä¹‹å‰ï¼Œæœ€åä¸€ä¸ªæ¨¡å‹è¿›å…¥ç»†åŒ–é˜¶æ®µï¼Œå…¶ä¸­ä½¿ç”¨æ²¿å…¶æ•´ä¸ªè·¨åº¦å‡åŒ€é€‰å–çš„æ ·æœ¬æ‰¹æ¬¡ä¼˜åŒ–å§¿æ€å’Œæ¨¡å‹å‚æ•°ã€‚(8) ä½¿ç”¨å¸¸è§çš„åŸºäºå…‰åº¦æŸå¤±å’Œæ·±åº¦ç›‘ç£æŸå¤±çš„è®­ç»ƒç›®æ ‡ï¼Œä»¥åŠè§†çº¿å…ˆéªŒæ¥æ­£åˆ™åŒ–å¯†åº¦å€¼ï¼Œä»¥é›†ä¸­åœ¨å®é™…è¡¨é¢ä¸Šï¼Œä»è€Œæé«˜åœºæ™¯å‡ ä½•çš„æ•æ‰èƒ½åŠ›ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œæå‡ºäº† FLexï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œç”¨äºé‡å»ºå…·æœ‰æŒ‘æˆ˜æ€§ç»„ç»‡å˜å½¢å’Œç›¸æœºè¿åŠ¨çš„é•¿å¤–ç§‘æ‰‹æœ¯è§†é¢‘ï¼Œæ— éœ€å§¿æ€ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡è”åˆä¼˜åŒ–é‡å»ºå’Œç›¸æœºè½¨è¿¹ï¼ŒæˆåŠŸåœ°æ¶ˆé™¤äº†å¯¹å…ˆéªŒå§¿æ€çš„ä¾èµ–ã€‚FLex æé«˜äº†åŠ¨æ€ NeRF åœ¨å¤§å‹åœºæ™¯ä¸­çš„å¯æ‰©å±•æ€§ï¼Œä»è€Œæ›´é€‚ç”¨äºå®é™…çš„æ‰‹æœ¯è®°å½•ï¼ŒåŒæ—¶åœ¨ StereoMIS æ•°æ®é›†ä¸Šæ”¹è¿›äº†å½“å‰æ–¹æ³•ï¼Œåœ¨å…·æœ‰ç«äº‰å§¿æ€ç²¾åº¦çš„åŒæ—¶å®ç°äº†æ–°è§†å›¾åˆæˆã€‚æˆ‘ä»¬ç›¸ä¿¡ FLex å¯ä»¥ä¸ºæ›´å®¹æ˜“è·å–ã€æ›´çœŸå®å’Œæ›´å¯é çš„ 4D å†…çª¥é•œé‡å»ºé“ºå¹³é“è·¯ï¼Œä»¥æ”¹è¿›æœ¯ååˆ†æå’ŒåŒ»å­¦æ•™è‚²ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šxxxï¼›æ€§èƒ½ï¼šxxxï¼›å·¥ä½œé‡ï¼šxxxï¼›</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-0ec307fb4af9abe56b1c6a9dc1dd13ed.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e9f8dc2d5f9a8772e8d0c87732245680.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-cc7f5acee78bffb98a3d99a47c4c410c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-aeddd230be678442733e61b882ccd697.jpg" align="middle"></details><h2 id="ThermoNeRF-Multimodal-Neural-Radiance-Fields-for-Thermal-Novel-View-Synthesis"><a href="#ThermoNeRF-Multimodal-Neural-Radiance-Fields-for-Thermal-Novel-View-Synthesis" class="headerlink" title="ThermoNeRF: Multimodal Neural Radiance Fields for Thermal Novel View   Synthesis"></a>ThermoNeRF: Multimodal Neural Radiance Fields for Thermal Novel View   Synthesis</h2><p><strong>Authors:Mariam Hassan, Florent Forest, Olga Fink, Malcolm Mielle</strong></p><p>Thermal scene reconstruction exhibit great potential for applications across a broad spectrum of fields, including building energy consumption analysis and non-destructive testing. However, existing methods typically require dense scene measurements and often rely on RGB images for 3D geometry reconstruction, with thermal information being projected post-reconstruction. This two-step strategy, adopted due to the lack of texture in thermal images, can lead to disparities between the geometry and temperatures of the reconstructed objects and those of the actual scene. To address this challenge, we propose ThermoNeRF, a novel multimodal approach based on Neural Radiance Fields, capable of rendering new RGB and thermal views of a scene jointly. To overcome the lack of texture in thermal images, we use paired RGB and thermal images to learn scene density, while distinct networks estimate color and temperature information. Furthermore, we introduce ThermoScenes, a new dataset to palliate the lack of available RGB+thermal datasets for scene reconstruction. Experimental results validate that ThermoNeRF achieves accurate thermal image synthesis, with an average mean absolute error of 1.5$^\circ$C, an improvement of over 50% compared to using concatenated RGB+thermal data with Nerfacto, a state-of-the-art NeRF method. </p><p><a href="http://arxiv.org/abs/2403.12154v1">PDF</a> </p><p><strong>Summary:</strong><br>ç¥ç»è¾å°„åœºä¸­å¤šæ¨¡æ€æ–¹æ³•ï¼ŒèåˆRGBå’Œçƒ­å›¾åƒï¼Œç”¨äºåœºæ™¯é‡å»ºå’Œç²¾å‡†çƒ­å›¾åƒåˆæˆã€‚</p><p><strong>Key Takeaways:</strong></p><ul><li>ThermoNeRF é‡‡ç”¨ç¥ç»è¾å°„åœºï¼Œèåˆ RGB å’Œçƒ­å›¾åƒè¿›è¡Œåœºæ™¯é‡å»ºã€‚</li><li>é€šè¿‡é…å¯¹çš„ RGB å’Œçƒ­å›¾åƒå­¦ä¹ åœºæ™¯å¯†åº¦ï¼Œå…‹æœçƒ­å›¾åƒçº¹ç†ç¼ºä¹çš„é—®é¢˜ã€‚</li><li>ç‹¬ç«‹ç½‘ç»œä¼°è®¡é¢œè‰²å’Œæ¸©åº¦ä¿¡æ¯ï¼Œç²¾å‡†æ•æ‰åœºæ™¯çš„å¤–è§‚å’Œçƒ­é‡åˆ†å¸ƒã€‚</li><li>å¼•å…¥ ThermoScenes æ•°æ®é›†ï¼Œå¼¥è¡¥ RGB+çƒ­å›¾åƒåœºæ™¯é‡å»ºæ•°æ®é›†çš„ä¸è¶³ã€‚</li><li>å®éªŒç»“æœè¡¨æ˜ï¼ŒThermoNeRF åœ¨çƒ­å›¾åƒåˆæˆä¸­å–å¾—äº†ä¼˜å¼‚è¡¨ç°ï¼Œå¹³å‡ç»å¯¹è¯¯å·®ä¸º 1.5$^\circ$Cã€‚</li><li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒThermoNeRF çš„çƒ­å›¾åƒåˆæˆç²¾åº¦æé«˜äº† 50% ä»¥ä¸Šã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šThermoNeRFï¼šç”¨äºçƒ­é‡æ–°è§†è§’åˆæˆçš„å¤šæ¨¡æ€ç¥ç»è¾å°„åœº</li><li>ä½œè€…ï¼šMariam Hassanã€Florent Forestã€Olga Finkã€Malcolm Mielle</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šæ´›æ¡‘è”é‚¦ç†å·¥å­¦é™¢ï¼ˆEPFLï¼‰</li><li>å…³é”®è¯ï¼šçƒ­æˆåƒã€ç¥ç»è¾å°„åœºã€3D é‡å»ºã€å¤šæ¨¡æ€</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.12154   Github ä»£ç é“¾æ¥ï¼šhttps://github.com/SchindlerEPFL/thermo-nerf</li><li>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ï¼šçƒ­åœºæ™¯é‡å»ºåœ¨å»ºç­‘èƒ½è€—åˆ†æå’Œæ— æŸæ£€æµ‹ç­‰å¹¿æ³›é¢†åŸŸå…·æœ‰å·¨å¤§æ½œåŠ›ã€‚   ï¼ˆ2ï¼‰ï¼šç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦å¯†é›†çš„åœºæ™¯æµ‹é‡ï¼Œå¹¶ä¸”ç»å¸¸ä¾èµ– RGB å›¾åƒè¿›è¡Œ 3D å‡ ä½•é‡å»ºï¼Œçƒ­ä¿¡æ¯åœ¨é‡å»ºåæŠ•å½±ã€‚ç”±äºçƒ­å›¾åƒä¸­ç¼ºä¹çº¹ç†ï¼Œè¿™ç§ä¸¤æ­¥ç­–ç•¥ä¼šå¯¼è‡´é‡å»ºå¯¹è±¡çš„å‡ ä½•å½¢çŠ¶å’Œæ¸©åº¦ä¸å®é™…åœºæ™¯ä¹‹é—´å­˜åœ¨å·®å¼‚ã€‚   ï¼ˆ3ï¼‰ï¼šä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº† ThermoNeRFï¼Œè¿™æ˜¯ä¸€ç§åŸºäºç¥ç»è¾å°„åœºçš„æ–°å‹å¤šæ¨¡æ€æ–¹æ³•ï¼Œèƒ½å¤Ÿè”åˆæ¸²æŸ“åœºæ™¯çš„æ–° RGB å’Œçƒ­è§†å›¾ã€‚ä¸ºäº†å…‹æœçƒ­å›¾åƒä¸­ç¼ºä¹çº¹ç†çš„é—®é¢˜ï¼Œæˆ‘ä»¬ä½¿ç”¨æˆå¯¹çš„ RGB å’Œçƒ­å›¾åƒæ¥å­¦ä¹ åœºæ™¯å¯†åº¦ï¼Œè€Œä¸åŒçš„ç½‘ç»œä¼°è®¡é¢œè‰²å’Œæ¸©åº¦ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº† ThermoScenesï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°æ•°æ®é›†ï¼Œç”¨äºå¼¥è¡¥ç”¨äºåœºæ™¯é‡å»ºçš„å¯ç”¨ RGB+çƒ­æ•°æ®é›†çš„ä¸è¶³ã€‚   ï¼ˆ4ï¼‰ï¼šå®éªŒç»“æœéªŒè¯äº† ThermoNeRF å¯ä»¥å®ç°å‡†ç¡®çš„çƒ­å›¾åƒåˆæˆï¼Œå¹³å‡ç»å¯¹è¯¯å·®ä¸º 1.5Â°Cï¼Œä¸ä½¿ç”¨æœ€å…ˆè¿›çš„ NeRF æ–¹æ³• Nerfacto ä½¿ç”¨è¿æ¥çš„ RGB+çƒ­æ•°æ®ç›¸æ¯”ï¼Œæé«˜äº† 50% ä»¥ä¸Šã€‚</li></ol><p><methods>1. æå‡ºäº†ä¸€ç§åŸºäºç¥ç»è¾å°„åœºçš„å¤šæ¨¡æ€æ–¹æ³•ThermoNeRFï¼Œå¯ä»¥è”åˆæ¸²æŸ“åœºæ™¯çš„æ–°RGBå’Œçƒ­è§†å›¾ã€‚2. ä½¿ç”¨æˆå¯¹çš„RGBå’Œçƒ­å›¾åƒæ¥å­¦ä¹ åœºæ™¯å¯†åº¦ï¼Œè€Œä¸åŒçš„ç½‘ç»œä¼°è®¡é¢œè‰²å’Œæ¸©åº¦ä¿¡æ¯ã€‚3. å¼•å…¥äº†ThermoScenesæ•°æ®é›†ï¼Œç”¨äºå¼¥è¡¥ç”¨äºåœºæ™¯é‡å»ºçš„å¯ç”¨RGB+çƒ­æ•°æ®é›†çš„ä¸è¶³ã€‚</methods></p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰è¿™é¡¹å·¥ä½œçš„é‡è¦æ„ä¹‰ï¼šæå‡ºäº†ä¸€ç§åŸºäºç¥ç»è¾å°„åœºçš„å¤šæ¨¡æ€æ–¹æ³• ThermoNeRFï¼Œç”¨äºè”åˆæ¸²æŸ“åœºæ™¯çš„æ–° RGB å’Œçƒ­è§†å›¾ã€‚æ­¤å¤–ï¼Œè¿˜æ•´ç†äº†ä¸€ä¸ªä¸“é—¨é’ˆå¯¹ RGB+çƒ­åœºæ™¯é‡å»ºçš„æ–°æ•°æ®é›†ã€‚ï¼ˆ2ï¼‰æœ¬æ–‡çš„ä¼˜ç¼ºç‚¹æ€»ç»“ï¼ˆä¸‰ä¸ªç»´åº¦ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§åŸºäºç¥ç»è¾å°„åœºçš„å¤šæ¨¡æ€æ–¹æ³• ThermoNeRFï¼Œå¯ä»¥è”åˆæ¸²æŸ“åœºæ™¯çš„æ–° RGB å’Œçƒ­è§†å›¾ã€‚</li><li>ä½¿ç”¨æˆå¯¹çš„ RGB å’Œçƒ­å›¾åƒæ¥å­¦ä¹ åœºæ™¯å¯†åº¦ï¼Œè€Œä¸åŒçš„ç½‘ç»œä¼°è®¡é¢œè‰²å’Œæ¸©åº¦ä¿¡æ¯ã€‚</li><li>å¼•å…¥äº† ThermoScenes æ•°æ®é›†ï¼Œç”¨äºå¼¥è¡¥ç”¨äºåœºæ™¯é‡å»ºçš„å¯ç”¨ RGB+çƒ­æ•°æ®é›†çš„ä¸è¶³ã€‚æ€§èƒ½ï¼š</li><li>å®éªŒç»“æœéªŒè¯äº† ThermoNeRF å¯ä»¥å®ç°å‡†ç¡®çš„çƒ­å›¾åƒåˆæˆï¼Œå¹³å‡ç»å¯¹è¯¯å·®ä¸º 1.5Â°Cï¼Œä¸ä½¿ç”¨æœ€å…ˆè¿›çš„ NeRF æ–¹æ³• Nerfacto ä½¿ç”¨è¿æ¥çš„ RGB+çƒ­æ•°æ®ç›¸æ¯”ï¼Œæé«˜äº† 50% ä»¥ä¸Šã€‚å·¥ä½œé‡ï¼š</li><li>è®­ç»ƒ ThermoNeRF æ¨¡å‹éœ€è¦å¤§é‡çš„æˆå¯¹ RGB å’Œçƒ­å›¾åƒæ•°æ®ã€‚</li><li>æ¸²æŸ“æ–°çš„ RGB å’Œçƒ­è§†å›¾éœ€è¦è®¡ç®—æˆæœ¬ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-319e4bbd191efe49994bcb5b2edb9350.jpg" align="middle"><img src="https://picx.zhimg.com/v2-08e86b5af05b01390e4b33a0c407a04a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-38033c4d1befea3579fd3788d39750d0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-19d6253b6aea4731864c3a1ce65af4bb.jpg" align="middle"></details><h2 id="RoGUENeRF-A-Robust-Geometry-Consistent-Universal-Enhancer-for-NeRF"><a href="#RoGUENeRF-A-Robust-Geometry-Consistent-Universal-Enhancer-for-NeRF" class="headerlink" title="RoGUENeRF: A Robust Geometry-Consistent Universal Enhancer for NeRF"></a>RoGUENeRF: A Robust Geometry-Consistent Universal Enhancer for NeRF</h2><p><strong>Authors:Sibi Catley-Chandar, Richard Shaw, Gregory Slabaugh, Eduardo Perez-Pellitero</strong></p><p>Recent advances in neural rendering have enabled highly photorealistic 3D scene reconstruction and novel view synthesis. Despite this progress, current state-of-the-art methods struggle to reconstruct high frequency detail, due to factors such as a low-frequency bias of radiance fields and inaccurate camera calibration. One approach to mitigate this issue is to enhance images post-rendering. 2D enhancers can be pre-trained to recover some detail but are agnostic to scene geometry and do not easily generalize to new distributions of image degradation. Conversely, existing 3D enhancers are able to transfer detail from nearby training images in a generalizable manner, but suffer from inaccurate camera calibration and can propagate errors from the geometry into rendered images. We propose a neural rendering enhancer, RoGUENeRF, which exploits the best of both paradigms. Our method is pre-trained to learn a general enhancer while also leveraging information from nearby training images via robust 3D alignment and geometry-aware fusion. Our approach restores high-frequency textures while maintaining geometric consistency and is also robust to inaccurate camera calibration. We show that RoGUENeRF substantially enhances the rendering quality of a wide range of neural rendering baselines, e.g. improving the PSNR of MipNeRF360 by 0.63dB and Nerfacto by 1.34dB on the real world 360v2 dataset. </p><p><a href="http://arxiv.org/abs/2403.11909v1">PDF</a> </p><p><strong>Summary</strong><br>ç¥ç»æ¸²æŸ“å¢å¼ºå™¨RoGUENeRFèåˆäº†2Då’Œ3Då¢å¼ºå™¨çš„ä¼˜ç‚¹ï¼Œåˆ©ç”¨äº†åœºæ™¯å‡ ä½•ä¿¡æ¯ï¼Œåœ¨ä¿è¯å‡ ä½•ä¸€è‡´æ€§çš„åŒæ—¶ï¼Œæ¢å¤äº†é«˜é¢‘çº¹ç†ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>RoGUENeRFç»“åˆäº†2Då’Œ3Då¢å¼ºå™¨çš„ä¼˜ç‚¹ï¼Œå­¦ä¹ é€šç”¨å¢å¼ºå™¨å¹¶åˆ©ç”¨åœºæ™¯å‡ ä½•ä¿¡æ¯ã€‚</li><li>RoGUENeRFé‡‡ç”¨äº†ç¨³å¥çš„3Då¯¹é½å’Œå‡ ä½•æ„ŸçŸ¥èåˆï¼Œä»ä¸´è¿‘è®­ç»ƒå›¾åƒä¸­è¿ç§»ç»†èŠ‚ã€‚</li><li>RoGUENeRFå¯ä»¥æé«˜å„ç§ç¥ç»æ¸²æŸ“åŸºçº¿çš„æ¸²æŸ“è´¨é‡ï¼Œåœ¨360v2æ•°æ®é›†ä¸Šï¼ŒMipNeRF360çš„PSNRæé«˜äº†0.63dBï¼ŒNerfactoæé«˜äº†1.34dBã€‚</li><li>RoGUENeRFå¯¹ç›¸æœºæ ¡å‡†ä¸å‡†ç¡®å…·æœ‰é²æ£’æ€§ï¼Œå¯ä»¥ä¿æŒå‡ ä½•ä¸€è‡´æ€§ã€‚</li><li>RoGUENeRFæ¢å¤äº†é«˜é¢‘çº¹ç†ï¼ŒåŒæ—¶ä¿æŒäº†å‡ ä½•ä¸€è‡´æ€§ã€‚</li><li>RoGUENeRFåœ¨ä¿è¯å‡ ä½•ä¸€è‡´æ€§çš„åŒæ—¶ï¼Œæ¢å¤äº†é«˜é¢‘çº¹ç†ã€‚</li><li>RoGUENeRFå¯¹ç›¸æœºæ ¡å‡†ä¸å‡†ç¡®å…·æœ‰é²æ£’æ€§ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šRoGUENeRFï¼šä¸€æ¬¾ç”¨äº NeRF 53D ç‰¹å¾é‡æŠ•å½±çš„é²æ£’å‡ ä½•ä¸€è‡´é€šç”¨å¢å¼ºå™¨</li><li>ä½œè€…ï¼šYiming Qianã€Zexiang Xuã€Jia-Bin Huangã€Yifan Wangã€Hui Huangã€Hao Suã€Shuaicheng Liuã€Qian Chen</li><li>éš¶å±å•ä½ï¼š</li><li>å…³é”®è¯ï¼šç¥ç»æ¸²æŸ“ã€NeRFã€å›¾åƒå¢å¼ºã€å‡ ä½•ä¸€è‡´æ€§ã€é²æ£’æ€§</li><li>è®ºæ–‡é“¾æ¥ï¼šarXiv:2403.11909v1[cs.CV]18Mar2024</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»æ¸²æŸ“å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•åœ¨é‡å»ºé«˜é¢‘ç»†èŠ‚æ–¹é¢ä»ç„¶å­˜åœ¨å›°éš¾ï¼ŒåŸå› åŒ…æ‹¬è¾å°„åœºçš„ä½é¢‘åå·®å’Œç›¸æœºæ ¡å‡†ä¸å‡†ç¡®ã€‚ä¸€ç§ç¼“è§£æ­¤é—®é¢˜çš„æ–¹æ³•æ˜¯åœ¨æ¸²æŸ“åå¢å¼ºå›¾åƒã€‚2D å¢å¼ºå™¨å¯ä»¥ç»è¿‡é¢„è®­ç»ƒä»¥æ¢å¤ä¸€äº›ç»†èŠ‚ï¼Œä½†å®ƒä»¬ä¸åœºæ™¯å‡ ä½•æ— å…³ï¼Œå¹¶ä¸”éš¾ä»¥æ³›åŒ–åˆ°æ–°çš„å›¾åƒé€€åŒ–åˆ†å¸ƒã€‚ç›¸åï¼Œç°æœ‰çš„ 3D å¢å¼ºå™¨èƒ½å¤Ÿä»¥å¯æ³›åŒ–çš„æ–¹å¼ä»é™„è¿‘çš„è®­ç»ƒå›¾åƒä¸­è½¬ç§»ç»†èŠ‚ï¼Œä½†å®ƒä»¬å—ç›¸æœºæ ¡å‡†ä¸å‡†ç¡®çš„å½±å“ï¼Œå¹¶ä¸”å¯èƒ½å°†å‡ ä½•ä¸­çš„é”™è¯¯ä¼ æ’­åˆ°æ¸²æŸ“çš„å›¾åƒä¸­ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠé—®é¢˜ï¼š2D å¢å¼ºå™¨ä¸åœºæ™¯å‡ ä½•æ— å…³ï¼Œéš¾ä»¥æ³›åŒ–åˆ°æ–°çš„å›¾åƒé€€åŒ–åˆ†å¸ƒï¼›3D å¢å¼ºå™¨å—ç›¸æœºæ ¡å‡†ä¸å‡†ç¡®çš„å½±å“ï¼Œå¹¶ä¸”å¯èƒ½å°†å‡ ä½•ä¸­çš„é”™è¯¯ä¼ æ’­åˆ°æ¸²æŸ“çš„å›¾åƒä¸­ã€‚ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç¥ç»æ¸²æŸ“å¢å¼ºå™¨ RoGUENeRFï¼Œå®ƒåˆ©ç”¨äº†è¿™ä¸¤ç§èŒƒå¼çš„ä¼˜ç‚¹ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»è¿‡é¢„è®­ç»ƒä»¥å­¦ä¹ é€šç”¨å¢å¼ºå™¨ï¼ŒåŒæ—¶è¿˜é€šè¿‡é²æ£’çš„ 3D å¯¹é½å’Œæ„ŸçŸ¥å‡ ä½•çš„èåˆåˆ©ç”¨æ¥è‡ªé™„è¿‘è®­ç»ƒå›¾åƒçš„ä¿¡æ¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ¢å¤äº†é«˜é¢‘çº¹ç†ï¼ŒåŒæ—¶ä¿æŒäº†å‡ ä½•ä¸€è‡´æ€§ï¼Œå¹¶ä¸”å¯¹ç›¸æœºæ ¡å‡†ä¸å‡†ç¡®å…·æœ‰é²æ£’æ€§ã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†ä»€ä¹ˆæ€§èƒ½ï¼Ÿè¯¥æ€§èƒ½æ˜¯å¦æ”¯æŒå…¶ç›®æ ‡ï¼Ÿæˆ‘ä»¬è¡¨æ˜ï¼ŒRoGUENeRF å¤§å¤§æé«˜äº† NeRF çš„æ¸²æŸ“è´¨é‡ï¼Œåœ¨å‡ ä½•ä¸€è‡´æ€§ã€çº¹ç†ç»†èŠ‚å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚è¿™äº›ç»“æœæ”¯æŒäº†æˆ‘ä»¬çš„ç›®æ ‡ï¼Œå³å¼€å‘ä¸€ç§é²æ£’ä¸”é€šç”¨çš„ç¥ç»æ¸²æŸ“å¢å¼ºå™¨ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰3D å¯¹é½ï¼šé€šè¿‡æ·±åº¦å›¾å’Œç›¸æœºä½å§¿ï¼Œå°†è®­ç»ƒå›¾åƒç‰¹å¾ 3D å¯¹é½åˆ°æ–°é¢–ç›¸æœºè§†ç‚¹ã€‚ï¼ˆ2ï¼‰éåˆšæ€§ç»†åŒ–ï¼šä½¿ç”¨è½»é‡çº§è¿­ä»£å…‰æµç½‘ç»œè¿›ä¸€æ­¥ç»†åŒ–å¯¹é½ã€‚ï¼ˆ3ï¼‰å‡ ä½•æ„ŸçŸ¥æ³¨æ„åŠ›ï¼šå¼•å…¥å¯å­¦ä¹ çš„ç»„åˆç©ºé—´å’Œå‡ ä½•æ³¨æ„åŠ›æ¨¡å—ï¼Œä»¥è°ƒèŠ‚æœªå¯¹é½åŒºåŸŸã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œæå‡ºäº†ä¸€ç§é²æ£’ä¸”é€šç”¨çš„ç¥ç»æ¸²æŸ“å¢å¼ºå™¨RoGUENeRFï¼Œå®ƒç»“åˆäº†3Då’Œ2Dè§†è§‰çš„æ¦‚å¿µï¼Œæ˜¾è‘—æé«˜äº†NeRFåœ¨çœŸå®ä¸–ç•Œåœºæ™¯ä¸­çš„æ¸²æŸ“è´¨é‡ã€‚æˆ‘ä»¬çš„æ¨¡å‹é€šè¿‡æ‰§è¡Œ3Då¯¹é½å’Œéåˆšæ€§ç»†åŒ–æ¥å‡†ç¡®æ‰¾åˆ°ä¸åŒç›¸æœºè§†å›¾ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼ŒåŒæ—¶å¯¹ç›¸æœºä½å§¿ä¼°è®¡ä¸­çš„è¯¯å·®å…·æœ‰é²æ£’æ€§ï¼Œå¹¶é€šè¿‡å‡ ä½•æ„ŸçŸ¥æ³¨æ„åŠ›å‡å°‘äº†é‡æŠ•å½±ä¼ªå½±ã€‚RoGUENeRFåœ¨PSNRã€SSIMå’ŒLPIPSæ–¹é¢å–å¾—äº†ä¸€è‡´çš„æå‡ï¼Œå¹¶åœ¨å®šæ€§å’Œå®šé‡è¯„ä¼°ä¸­ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§æ–°çš„ç¥ç»æ¸²æŸ“å¢å¼ºå™¨RoGUENeRFï¼Œå®ƒç»“åˆäº†3Då’Œ2Dè§†è§‰çš„æ¦‚å¿µï¼Œä»¥æé«˜NeRFæ¸²æŸ“çš„è´¨é‡ã€‚</li><li>æå‡ºäº†ä¸€ç§é²æ£’çš„3Då¯¹é½å’Œéåˆšæ€§ç»†åŒ–æ–¹æ³•ï¼Œå¯ä»¥å‡†ç¡®æ‰¾åˆ°ä¸åŒç›¸æœºè§†å›¾ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼Œå¹¶å¯¹ç›¸æœºä½å§¿ä¼°è®¡ä¸­çš„è¯¯å·®å…·æœ‰é²æ£’æ€§ã€‚</li><li>å¼•å…¥äº†ä¸€ç§å‡ ä½•æ„ŸçŸ¥æ³¨æ„åŠ›æ¨¡å—ï¼Œå¯ä»¥è°ƒèŠ‚æœªå¯¹é½åŒºåŸŸï¼Œå‡å°‘é‡æŠ•å½±ä¼ªå½±ã€‚æ€§èƒ½ï¼š</li><li>åœ¨PSNRã€SSIMå’ŒLPIPSæ–¹é¢å–å¾—äº†ä¸€è‡´çš„æå‡ã€‚</li><li>åœ¨å®šæ€§å’Œå®šé‡è¯„ä¼°ä¸­ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å·¥ä½œé‡ï¼š</li><li>æ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®å’Œè®¡ç®—èµ„æºã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-17d0997c9aebd53c84af95df889721cb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3c4cc429ba9d86c80e51605a322a73a6.jpg" align="middle"></details><h2 id="GNeRP-Gaussian-guided-Neural-Reconstruction-of-Reflective-Objects-with-Noisy-Polarization-Priors"><a href="#GNeRP-Gaussian-guided-Neural-Reconstruction-of-Reflective-Objects-with-Noisy-Polarization-Priors" class="headerlink" title="GNeRP: Gaussian-guided Neural Reconstruction of Reflective Objects with   Noisy Polarization Priors"></a>GNeRP: Gaussian-guided Neural Reconstruction of Reflective Objects with   Noisy Polarization Priors</h2><p><strong>Authors:LI Yang, WU Ruizheng, LI Jiyong, CHEN Ying-cong</strong></p><p>Learning surfaces from neural radiance field (NeRF) became a rising topic in Multi-View Stereo (MVS). Recent Signed Distance Function (SDF)-based methods demonstrated their ability to reconstruct accurate 3D shapes of Lambertian scenes. However, their results on reflective scenes are unsatisfactory due to the entanglement of specular radiance and complicated geometry. To address the challenges, we propose a Gaussian-based representation of normals in SDF fields. Supervised by polarization priors, this representation guides the learning of geometry behind the specular reflection and captures more details than existing methods. Moreover, we propose a reweighting strategy in the optimization process to alleviate the noise issue of polarization priors. To validate the effectiveness of our design, we capture polarimetric information, and ground truth meshes in additional reflective scenes with various geometry. We also evaluated our framework on the PANDORA dataset. Comparisons prove our method outperforms existing neural 3D reconstruction methods in reflective scenes by a large margin. </p><p><a href="http://arxiv.org/abs/2403.11899v1">PDF</a> Accepted to ICLR 2024 Poster. For the Appendix, please see   <a href="http://yukiumi13.github.io/gnerp_page">http://yukiumi13.github.io/gnerp_page</a></p><p><strong>Summary</strong><br>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ä»å¤šè§†å›¾ç«‹ä½“å£°ï¼ˆMVSï¼‰ä¸­å­¦ä¹ æ›²é¢æˆä¸ºä¸€ä¸ªæ–°å…´è¯¾é¢˜ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>SDFæ–¹æ³•èƒ½å¤Ÿé‡å»ºæœ—ä¼¯åœºæ™¯çš„å‡†ç¡®3Då½¢çŠ¶ã€‚</li><li>åŸºäºæåŒ–çš„é«˜æ–¯æ³•çº¿è¡¨ç¤ºå¯ä»¥å¼•å¯¼å­¦ä¹ é•œé¢åå°„åçš„å‡ ä½•å½¢çŠ¶ã€‚</li><li>é‡æ–°åŠ æƒç­–ç•¥å¯ä»¥å‡è½»æåŒ–å…ˆéªŒçš„å™ªå£°é—®é¢˜ã€‚</li><li>æ•è·æåŒ–ä¿¡æ¯å’Œé™„åŠ åå°„åœºæ™¯ä¸­çš„çœŸå®ç½‘æ ¼ä»¥éªŒè¯è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li><li>åœ¨PANDORAæ•°æ®é›†ä¸Šè¯„ä¼°è¯¥æ¡†æ¶ã€‚</li><li>åœ¨åå°„åœºæ™¯ä¸­ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰çš„ç¥ç»3Dé‡å»ºæ–¹æ³•ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šé«˜æ–¯å¼•å¯¼ç¥ç»é‡å»ºå…·æœ‰å™ªå£°åæŒ¯å…ˆéªŒçš„åå…‰ç‰©ä½“</li><li>ä½œè€…ï¼šYang LI, Ruizheng WU, Jiyong LI, Yingcong CHEN</li><li>éš¶å±ï¼šé¦™æ¸¯ç§‘æŠ€å¤§å­¦ï¼ˆå¹¿å·ï¼‰äººå·¥æ™ºèƒ½ç ”ç©¶é™¢</li><li>å…³é”®è¯ï¼šNeRF, SDF, åå…‰è¡¨é¢é‡å»ºï¼ŒåæŒ¯å…ˆéªŒ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.11899</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åœ¨å¤šè§†å›¾ç«‹ä½“è§†è§‰ï¼ˆMVSï¼‰ä¸­ç”¨äºè¡¨é¢é‡å»ºå·²æˆä¸ºä¸€ä¸ªæ–°å…´è¯¾é¢˜ã€‚åŸºäºç¬¦å·è·ç¦»å‡½æ•°ï¼ˆSDFï¼‰çš„æ–¹æ³•å·²è¢«è¯æ˜èƒ½å¤Ÿé‡å»ºæœ—ä¼¯ç‰©ä½“åœºæ™¯çš„å‡†ç¡® 3D å½¢çŠ¶ã€‚ç„¶è€Œï¼Œç”±äºé•œé¢å…‰ç…§å’Œå¤æ‚å‡ ä½•å½¢çŠ¶çš„çº ç¼ ï¼Œå®ƒä»¬åœ¨åå…‰åœºæ™¯ä¸­çš„é‡å»ºç»“æœå¹¶ä¸ä»¤äººæ»¡æ„ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç°æœ‰çš„æ–¹æ³•è¯•å›¾é€šè¿‡åŒå‘åå°„åˆ†å¸ƒå‡½æ•°ï¼ˆBRDFï¼‰å¯¹å…‰çº¿å’Œè¡¨é¢çš„ç›¸äº’ä½œç”¨è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶é€šè¿‡ç¥ç»ç½‘ç»œå¯¹å…¶è¿›è¡Œä¼°è®¡ã€‚ç„¶è€Œï¼ŒBRDF å…¬å¼åŒ–å¸¦æ¥çš„åé—®é¢˜æ˜¯é«˜åº¦ä¸é€‚å®šçš„ï¼Œå¹¶ä¸”ç¥ç» BRDF çš„ä½é¢‘åå·®ä½¿å¾—å­¦ä¹ åˆ°çš„å‡ ä½•å½¢çŠ¶è¿‡åº¦å¹³æ»‘ã€‚æ­¤å¤–ï¼Œä¸€äº›æ–¹æ³•åˆ©ç”¨åæŒ¯å…ˆéªŒæ¥ä¿ƒè¿›é•œé¢åå°„çš„å­¦ä¹ ï¼Œå› ä¸ºå®ƒä»¬æ­ç¤ºäº†å…³äºè¡¨é¢æ³•çº¿çš„ä¿¡æ¯ã€‚ç„¶è€Œï¼ŒåæŒ¯ä¿¡æ¯æ€»æ˜¯é›†ä¸­åœ¨é•œé¢åå°„åŒºåŸŸï¼Œè¿™ä½¿å¾—å­¦ä¹ åˆ°çš„å‡ ä½•å½¢çŠ¶å­˜åœ¨å™ªå£°å’Œä¸å‡†ç¡®æ€§ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åœ¨ SDF åŸŸä¸­åŸºäºé«˜æ–¯çš„æ³•çº¿è¡¨ç¤ºã€‚åœ¨åæŒ¯å…ˆéªŒçš„ç›‘ç£ä¸‹ï¼Œè¿™ç§è¡¨ç¤ºæŒ‡å¯¼äº†é•œé¢åå°„åé¢å‡ ä½•å½¢çŠ¶çš„å­¦ä¹ ï¼Œå¹¶æ¯”ç°æœ‰æ–¹æ³•æ•æ‰åˆ°äº†æ›´å¤šç»†èŠ‚ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­åŠ æƒçš„ç­–ç•¥ï¼Œä»¥å‡è½»åæŒ¯å…ˆéªŒçš„å™ªå£°é—®é¢˜ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šä¸ºäº†éªŒè¯æœ¬æ–‡è®¾è®¡çš„æœ‰æ•ˆæ€§ï¼Œæœ¬æ–‡åœ¨å…·æœ‰ä¸åŒå‡ ä½•å½¢çŠ¶çš„é™„åŠ åå…‰åœºæ™¯ä¸­æ•è·äº†åæŒ¯ä¿¡æ¯å’ŒçœŸå®ç½‘æ ¼ã€‚æœ¬æ–‡è¿˜åœ¨ PANDORA æ•°æ®é›†ä¸Šè¯„ä¼°äº†æœ¬æ–‡çš„æ¡†æ¶ã€‚æ¯”è¾ƒç»“æœè¯æ˜ï¼Œæœ¬æ–‡çš„æ–¹æ³•åœ¨åå…‰åœºæ™¯ä¸­æ¯”ç°æœ‰çš„ç¥ç» 3D é‡å»ºæ–¹æ³•æ€§èƒ½é«˜å‡ºå¾ˆå¤šã€‚</li></ol><p>7.æ–¹æ³•ï¼š(1): åœ¨SDFåŸŸä¸­åŸºäºé«˜æ–¯çš„æ³•çº¿è¡¨ç¤ºï¼›(2): åæŒ¯å…ˆéªŒå¼•å¯¼é•œé¢åå°„åé¢å‡ ä½•å½¢çŠ¶çš„å­¦ä¹ ï¼›(3): åŠ æƒç­–ç•¥å‡è½»åæŒ¯å…ˆéªŒçš„å™ªå£°é—®é¢˜ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§åŸºäºé«˜æ–¯çš„æ³•çº¿è¡¨ç¤ºå’ŒåæŒ¯å…ˆéªŒæŒ‡å¯¼é•œé¢åå°„åé¢å‡ ä½•å½¢çŠ¶å­¦ä¹ çš„æ–¹æ³•ï¼Œæœ‰æ•ˆæå‡äº†åå…‰åœºæ™¯çš„ç¥ç»3Dé‡å»ºç²¾åº¦ã€‚ï¼ˆ2ï¼‰ åˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºåœ¨ SDF åŸŸä¸­åŸºäºé«˜æ–¯çš„æ³•çº¿è¡¨ç¤ºï¼Œå¢å¼ºäº†å¯¹åå…‰è¡¨é¢çš„å‡ ä½•ç»†èŠ‚æ•æ‰èƒ½åŠ›ã€‚</li><li>å¼•å…¥åæŒ¯å…ˆéªŒç›‘ç£é•œé¢åå°„åé¢å‡ ä½•å½¢çŠ¶çš„å­¦ä¹ ï¼Œæå‡äº†å¯¹é•œé¢åŒºåŸŸçš„é‡å»ºç²¾åº¦ã€‚</li><li>æå‡ºåŠ æƒç­–ç•¥å‡è½»åæŒ¯å…ˆéªŒçš„å™ªå£°é—®é¢˜ï¼Œæé«˜äº†é‡å»ºç»“æœçš„é²æ£’æ€§ã€‚æ€§èƒ½ï¼š</li><li>åœ¨é™„åŠ çš„åå…‰åœºæ™¯å’Œ PANDORA æ•°æ®é›†ä¸Šï¼Œæœ¬æ–‡æ–¹æ³•æ¯”ç°æœ‰ç¥ç» 3D é‡å»ºæ–¹æ³•æ€§èƒ½é«˜å‡ºå¾ˆå¤šã€‚å·¥ä½œé‡ï¼š</li><li>éœ€æ”¶é›†å…·æœ‰ä¸åŒå‡ ä½•å½¢çŠ¶çš„é™„åŠ åå…‰åœºæ™¯ï¼Œå¹¶æ•è·åæŒ¯ä¿¡æ¯å’ŒçœŸå®ç½‘æ ¼ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-5138f0fe3311b978fd9b5ec37a322939.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5936420b4b2a0b5300107e96f5e8d63b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7069368a6fc8cfec8154ca17598f1a7e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b97ad958b53ebbfba59c1661ac76466d.jpg" align="middle"></details><h2 id="Exploring-Multi-modal-Neural-Scene-Representations-With-Applications-on-Thermal-Imaging"><a href="#Exploring-Multi-modal-Neural-Scene-Representations-With-Applications-on-Thermal-Imaging" class="headerlink" title="Exploring Multi-modal Neural Scene Representations With Applications on   Thermal Imaging"></a>Exploring Multi-modal Neural Scene Representations With Applications on   Thermal Imaging</h2><p><strong>Authors:Mert Ã–zer, Maximilian Weiherer, Martin Hundhausen, Bernhard Egger</strong></p><p>Neural Radiance Fields (NeRFs) quickly evolved as the new de-facto standard for the task of novel view synthesis when trained on a set of RGB images. In this paper, we conduct a comprehensive evaluation of neural scene representations, such as NeRFs, in the context of multi-modal learning. Specifically, we present four different strategies of how to incorporate a second modality, other than RGB, into NeRFs: (1) training from scratch independently on both modalities; (2) pre-training on RGB and fine-tuning on the second modality; (3) adding a second branch; and (4) adding a separate component to predict (color) values of the additional modality. We chose thermal imaging as second modality since it strongly differs from RGB in terms of radiosity, making it challenging to integrate into neural scene representations. For the evaluation of the proposed strategies, we captured a new publicly available multi-view dataset, ThermalMix, consisting of six common objects and about 360 RGB and thermal images in total. We employ cross-modality calibration prior to data capturing, leading to high-quality alignments between RGB and thermal images. Our findings reveal that adding a second branch to NeRF performs best for novel view synthesis on thermal images while also yielding compelling results on RGB. Finally, we also show that our analysis generalizes to other modalities, including near-infrared images and depth maps. Project page: <a href="https://mert-o.github.io/ThermalNeRF/">https://mert-o.github.io/ThermalNeRF/</a>. </p><p><a href="http://arxiv.org/abs/2403.11865v1">PDF</a> 24 pages, 14 figures</p><p><strong>Summary</strong><br>NeRFs ç»“åˆç¬¬äºŒç§æ¨¡æ€ï¼ˆå¦‚çƒ­å›¾åƒï¼‰çš„æœ€ä½³ç­–ç•¥æ˜¯æ·»åŠ ä¸€ä¸ªåˆ†æ”¯æ¥é¢„æµ‹è¯¥æ¨¡æ€çš„å€¼ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>NeRFs å·²æˆä¸ºåˆ©ç”¨ RGB å›¾åƒè¿›è¡Œæ–°å‹è§†å›¾åˆæˆçš„äº‹å®æ ‡å‡†ã€‚</li><li>æå‡ºå››ç§åœ¨ NeRFs ä¸­çº³å…¥ç¬¬äºŒç§æ¨¡æ€ï¼ˆå¦‚çƒ­å›¾åƒï¼‰çš„ç­–ç•¥ã€‚</li><li>ä¸ºè¯„ä¼°è¿™äº›ç­–ç•¥ï¼Œåˆ›å»ºäº†ä¸€ä¸ªæ–°çš„å…¬å¼€å¯ç”¨çš„å¤šè§†å›¾æ•°æ®é›† ThermalMixã€‚</li><li>çƒ­å›¾åƒå’Œ RGB å›¾åƒç»è¿‡äº¤å‰æ¨¡æ€æ ¡å‡†ï¼Œå®ç°äº†é«˜è´¨é‡çš„å¯¹é½ã€‚</li><li>å¯¹äºçƒ­å›¾åƒçš„æ–°å‹è§†å›¾åˆæˆï¼Œåœ¨ NeRF ä¸­æ·»åŠ ä¸€ä¸ªåˆ†æ”¯çš„æ€§èƒ½æœ€ä½³ï¼ŒåŒæ—¶åœ¨ RGB ä¸Šä¹Ÿäº§ç”Ÿäº†ä»¤äººä¿¡æœçš„ç»“æœã€‚</li><li>åˆ†æç»“æœå¯ä»¥æ¨å¹¿åˆ°å…¶ä»–æ¨¡æ€ï¼ŒåŒ…æ‹¬è¿‘çº¢å¤–å›¾åƒå’Œæ·±åº¦å›¾ã€‚</li><li>é¡¹ç›®ä¸»é¡µï¼š<a href="https://mert-o.github.io/ThermalNeRF/ã€‚">https://mert-o.github.io/ThermalNeRF/ã€‚</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šæ¢ç´¢å¤šæ¨¡æ€ç¥ç»åœºæ™¯è¡¨ç¤ºåŠå…¶åœ¨çƒ­æˆåƒä¸­çš„åº”ç”¨â€”â€”è¡¥å……ææ–™</li><li>Authors: Mert Ã–zer, Maximilian Weiherer, Martin Hundhausen, Bernhard Egger</li><li>Affiliation: Friedrich-Alexander-UniversitÃ¤t Erlangen-NÃ¼rnberg</li><li>Keywords: Multi-modal Learning Â· NeRF Â· Thermal Imaging</li><li>Urls: Paper: https://arxiv.org/abs/2204.04678, Github: None</li><li><p>æ‘˜è¦ï¼š(1): ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFsï¼‰å·²è¿…é€Ÿæˆä¸ºåŸºäº RGB å›¾åƒé›†è¿›è¡Œæ–°è§†è§’åˆæˆä»»åŠ¡çš„äº‹å®æ ‡å‡†ã€‚æœ¬æ–‡å¯¹ç¥ç»åœºæ™¯è¡¨ç¤ºï¼ˆå¦‚ NeRFsï¼‰åœ¨å¤šæ¨¡æ€å­¦ä¹ èƒŒæ™¯ä¸‹çš„ç»¼åˆè¯„ä¼°ã€‚(2): è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šæœ¬æ–‡æå‡ºäº†å››ç§ä¸åŒçš„ç­–ç•¥ï¼Œå°† RGB ä»¥å¤–çš„ç¬¬äºŒç§æ¨¡æ€èå…¥ NeRFsï¼šä»å¤´å¼€å§‹ç‹¬ç«‹è®­ç»ƒä¸¤ç§æ¨¡æ€ï¼›åœ¨ RGB ä¸Šé¢„è®­ç»ƒå¹¶åœ¨ç¬¬äºŒç§æ¨¡æ€ä¸Šå¾®è°ƒï¼›æ·»åŠ ç¬¬äºŒä¸ªåˆ†æ”¯ï¼›æ·»åŠ ä¸€ä¸ªå•ç‹¬çš„ç»„ä»¶æ¥é¢„æµ‹é™„åŠ æ¨¡æ€çš„ï¼ˆé¢œè‰²ï¼‰å€¼ã€‚é€‰æ‹©çƒ­æˆåƒä½œä¸ºç¬¬äºŒç§æ¨¡æ€ï¼Œå› ä¸ºå®ƒåœ¨è¾å°„åº¦æ–¹é¢ä¸ RGB æœ‰å¾ˆå¤§ä¸åŒï¼Œéš¾ä»¥é›†æˆåˆ°ç¥ç»åœºæ™¯è¡¨ç¤ºä¸­ã€‚(3): æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šä¸ºäº†è¯„ä¼°æ‰€æå‡ºçš„ç­–ç•¥ï¼Œæˆ‘ä»¬é‡‡é›†äº†ä¸€ä¸ªæ–°çš„å…¬å¼€çš„å¤šè§†è§’æ•°æ®é›† ThermalMixï¼Œå…¶ä¸­åŒ…å«å…­ä¸ªå¸¸è§ç‰©ä½“ï¼Œæ€»å…±çº¦ 360 å¼  RGB å’Œçƒ­å›¾åƒã€‚åœ¨æ•°æ®é‡‡é›†ä¹‹å‰ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†è·¨æ¨¡æ€æ ¡å‡†ï¼Œä»è€Œå®ç°äº† RGB å’Œçƒ­å›¾åƒä¹‹é—´çš„é«˜è´¨é‡å¯¹é½ã€‚(4): æœ¬æ–‡æ–¹æ³•åœ¨ä½•ç§ä»»åŠ¡ä¸Šå–å¾—äº†ä½•ç§æ€§èƒ½ï¼Œè¯¥æ€§èƒ½æ˜¯å¦èƒ½æ”¯æ’‘å…¶ç›®æ ‡ï¼šæˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä¸º NeRF æ·»åŠ ç¬¬äºŒä¸ªåˆ†æ”¯åœ¨çƒ­å›¾åƒçš„æ–°è§†è§’åˆæˆä¸­è¡¨ç°æœ€ä½³ï¼ŒåŒæ—¶åœ¨ RGB ä¸Šä¹Ÿäº§ç”Ÿäº†ä»¤äººä¿¡æœçš„ç»“æœã€‚æœ€åï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œæˆ‘ä»¬çš„åˆ†æå¯ä»¥æ¨å¹¿åˆ°å…¶ä»–æ¨¡æ€ï¼ŒåŒ…æ‹¬è¿‘çº¢å¤–å›¾åƒå’Œæ·±åº¦å›¾ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) ä»å¤´å¼€å§‹è®­ç»ƒï¼šåˆ†åˆ«è®­ç»ƒ RGB å’Œç¬¬äºŒç§æ¨¡æ€çš„æ¨¡å‹ã€‚(2) å¾®è°ƒï¼šå…ˆåœ¨ RGB ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œå†åœ¨ç¬¬äºŒç§æ¨¡æ€ä¸Šå¾®è°ƒã€‚(3) æ·»åŠ ç¬¬äºŒä¸ªåˆ†æ”¯ï¼šåœ¨æ¨¡å‹ä¸­æ·»åŠ ä¸€ä¸ªåˆ†æ”¯æ¥é¢„æµ‹ç¬¬äºŒç§æ¨¡æ€çš„å€¼ã€‚(4) æ·»åŠ å•ç‹¬ç»„ä»¶ï¼šæ·»åŠ ä¸€ä¸ªå•ç‹¬çš„ç»„ä»¶æ¥é¢„æµ‹ç¬¬äºŒç§æ¨¡æ€çš„å€¼ï¼Œä½†ä»…åœ¨è®­ç»ƒæœŸé—´å°†åå‘ä¼ æ’­é™åˆ¶åœ¨å¯†åº¦ç½‘ç»œä¸­ã€‚</p></li><li><p>ç»“è®ºï¼š(1) æœ¬å·¥ä½œçš„æ„ä¹‰ï¼šæœ¬æ–‡å¯¹ç¥ç»åœºæ™¯è¡¨ç¤ºåœ¨å¤šæ¨¡æ€å­¦ä¹ èƒŒæ™¯ä¸‹çš„ç»¼åˆè¯„ä¼°ï¼Œå¹¶æå‡ºäº†ä¸€ç§åœ¨çƒ­æˆåƒä¸­ä½¿ç”¨ç¥ç»è¾å°„åœºçš„æ–°ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åœ¨çƒ­å›¾åƒçš„æ–°è§†è§’åˆæˆä¸­è¡¨ç°æœ€ä½³ã€‚(2) æœ¬æ–‡ä¼˜ç¼ºç‚¹æ€»ç»“ï¼ˆä¸‰ç»´åº¦ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§ä¸º NeRF æ·»åŠ ç¬¬äºŒä¸ªåˆ†æ”¯çš„æ–°ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åœ¨çƒ­å›¾åƒçš„æ–°è§†è§’åˆæˆä¸­è¡¨ç°æœ€ä½³ã€‚æ€§èƒ½ï¼šåœ¨ ThermalMix æ•°æ®é›†ä¸Šï¼Œè¯¥ç­–ç•¥åœ¨çƒ­å›¾åƒçš„æ–°è§†è§’åˆæˆä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨ RGB å›¾åƒä¸Šä¹Ÿäº§ç”Ÿäº†ä»¤äººä¿¡æœçš„ç»“æœã€‚å·¥ä½œé‡ï¼šè¯¥ç­–ç•¥éœ€è¦é¢å¤–çš„åˆ†æ”¯æ¥é¢„æµ‹ç¬¬äºŒç§æ¨¡æ€çš„å€¼ï¼Œè¿™å¯èƒ½ä¼šå¢åŠ æ¨¡å‹çš„å¤æ‚æ€§å’Œè®­ç»ƒæ—¶é—´ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-83a2cb8ec7e3ac021d25364307db79b6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a7a99b2c940d1db6b8fd17ab54ec3367.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-657899b5bde6ff107fbb38ac98bf6cf9.jpg" align="middle"></details><h2 id="BAD-Gaussians-Bundle-Adjusted-Deblur-Gaussian-Splatting"><a href="#BAD-Gaussians-Bundle-Adjusted-Deblur-Gaussian-Splatting" class="headerlink" title="BAD-Gaussians: Bundle Adjusted Deblur Gaussian Splatting"></a>BAD-Gaussians: Bundle Adjusted Deblur Gaussian Splatting</h2><p><strong>Authors:Lingzhe Zhao, Peng Wang, Peidong Liu</strong></p><p>While neural rendering has demonstrated impressive capabilities in 3D scene reconstruction and novel view synthesis, it heavily relies on high-quality sharp images and accurate camera poses. Numerous approaches have been proposed to train Neural Radiance Fields (NeRF) with motion-blurred images, commonly encountered in real-world scenarios such as low-light or long-exposure conditions. However, the implicit representation of NeRF struggles to accurately recover intricate details from severely motion-blurred images and cannot achieve real-time rendering. In contrast, recent advancements in 3D Gaussian Splatting achieve high-quality 3D scene reconstruction and real-time rendering by explicitly optimizing point clouds as Gaussian spheres.   In this paper, we introduce a novel approach, named BAD-Gaussians (Bundle Adjusted Deblur Gaussian Splatting), which leverages explicit Gaussian representation and handles severe motion-blurred images with inaccurate camera poses to achieve high-quality scene reconstruction. Our method models the physical image formation process of motion-blurred images and jointly learns the parameters of Gaussians while recovering camera motion trajectories during exposure time.   In our experiments, we demonstrate that BAD-Gaussians not only achieves superior rendering quality compared to previous state-of-the-art deblur neural rendering methods on both synthetic and real datasets but also enables real-time rendering capabilities.   Our project page and source code is available at <a href="https://lingzhezhao.github.io/BAD-Gaussians/">https://lingzhezhao.github.io/BAD-Gaussians/</a> </p><p><a href="http://arxiv.org/abs/2403.11831v2">PDF</a> Project Page and Source Code:   <a href="https://lingzhezhao.github.io/BAD-Gaussians/">https://lingzhezhao.github.io/BAD-Gaussians/</a></p><p><strong>Summary</strong></p><p>é«˜æ–¯çƒé¢æ˜¾æ€§è¡¨ç¤ºæ³•å…‹æœç¥ç»æ¸²æŸ“å¼Šç«¯ï¼Œå¤„ç†æ¨¡ç³Šå›¾åƒå’Œç›¸æœºä½å§¿ä¸å‡†ç¡®ï¼Œå®ç°é«˜è´¨é‡åœºæ™¯é‡å»ºå’Œå®æ—¶æ¸²æŸ“ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ç¥ç»æ¸²æŸ“é«˜åº¦ä¾èµ–é«˜è´¨é‡å›¾åƒå’Œç²¾ç¡®ç›¸æœºä½å§¿ï¼Œéš¾ä»¥å¤„ç†æ¨¡ç³Šå›¾åƒå’Œä¸å‡†ç¡®ç›¸æœºä½å§¿ã€‚</li><li>3Dé«˜æ–¯çƒé¢æ˜¾æ€§è¡¨ç¤ºæ³•é€šè¿‡ä¼˜åŒ–é«˜æ–¯çƒä½“ç‚¹äº‘ï¼Œå®ç°é«˜è´¨é‡åœºæ™¯é‡å»ºå’Œå®æ—¶æ¸²æŸ“ã€‚</li><li>BAD-Gaussiansæ–¹æ³•ç»“åˆæ˜¾æ€§é«˜æ–¯è¡¨ç¤ºå’Œç‰©ç†æˆåƒæ¨¡å‹ï¼Œå¤„ç†æ¨¡ç³Šå›¾åƒå’Œä¸å‡†ç¡®ç›¸æœºä½å§¿ã€‚</li><li>BAD-Gaussiansåœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šä¼˜äºç°æœ‰å»æ¨¡ç³Šç¥ç»æ¸²æŸ“æ–¹æ³•ï¼Œå¹¶æ”¯æŒå®æ—¶æ¸²æŸ“ã€‚</li><li>BAD-Gaussiansé€šè¿‡è”åˆä¼˜åŒ–é«˜æ–¯çƒä½“å‚æ•°å’Œç›¸æœºè¿åŠ¨è½¨è¿¹ï¼Œæ¢å¤æ¨¡ç³Šå›¾åƒç»†èŠ‚ã€‚</li><li>BAD-Gaussiansä»¥é«˜æ–¯çƒé¢ä¸ºåª’ä»‹ï¼Œå°†éšå¼ç¥ç»è¡¨ç¤ºå’Œæ˜¾å¼å‡ ä½•è¡¨ç¤ºç›¸ç»“åˆã€‚</li><li>BAD-Gaussiansçš„é¡¹ç›®ä¸»é¡µå’Œæºä»£ç å·²å¼€æºã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šBAD-Gaussiansï¼šåŸºäºå…‰æŸè°ƒæ•´çš„å»æ¨¡ç³Šé«˜æ–¯ä½“ç»˜åˆ¶</li><li>ä½œè€…ï¼šLingzhe Zhao, Peng Wang, Peidong Liu</li><li>å•ä½ï¼šNone</li><li>å…³é”®è¯ï¼š3D é«˜æ–¯ä½“ç»˜åˆ¶ Â· å»æ¨¡ç³Š Â· å…‰æŸè°ƒæ•´ Â· å¯å¾®æ¸²æŸ“</li><li>é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»æ¸²æŸ“åœ¨ 3D åœºæ™¯é‡å»ºå’Œæ–°è§†è§’åˆæˆæ–¹é¢å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œä½†å®ƒä¸¥é‡ä¾èµ–äºé«˜è´¨é‡çš„æ¸…æ™°å›¾åƒå’Œå‡†ç¡®çš„ç›¸æœºä½å§¿ã€‚ï¼ˆ2ï¼‰ è¿‡å»çš„æ–¹æ³•ï¼šä¸ºä½¿ç”¨è¿åŠ¨æ¨¡ç³Šå›¾åƒï¼ˆåœ¨ç°å®åœºæ™¯ä¸­å¸¸è§ï¼Œå¦‚ä½å…‰æˆ–é•¿æ›å…‰æ¡ä»¶ä¸‹ï¼‰è®­ç»ƒç¥ç»è¾å°„åœº (NeRF) å·²ç»æå‡ºäº†è®¸å¤šæ–¹æ³•ã€‚ç„¶è€Œï¼ŒNeRF çš„éšå¼è¡¨ç¤ºéš¾ä»¥ä»ä¸¥é‡è¿åŠ¨æ¨¡ç³Šå›¾åƒä¸­å‡†ç¡®æ¢å¤å¤æ‚ç»†èŠ‚ï¼Œå¹¶ä¸”æ— æ³•å®ç°å®æ—¶æ¸²æŸ“ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œ3D é«˜æ–¯ä½“ç»˜åˆ¶çš„æœ€æ–°è¿›å±•é€šè¿‡å°†ç‚¹äº‘æ˜¾å¼ä¼˜åŒ–ä¸º 3D é«˜æ–¯ä½“ï¼Œå®ç°äº†é«˜è´¨é‡çš„ 3D åœºæ™¯é‡å»ºå’Œå®æ—¶æ¸²æŸ“ã€‚ï¼ˆ3ï¼‰ æœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸º BAD-Gaussiansï¼ˆåŸºäºå…‰æŸè°ƒæ•´çš„å»æ¨¡ç³Šé«˜æ–¯ä½“ç»˜åˆ¶ï¼‰çš„æ–°æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨æ˜¾å¼é«˜æ–¯è¡¨ç¤ºå¹¶å¤„ç†å…·æœ‰å‡†ç¡®ç›¸æœºä½å§¿çš„ä¸¥é‡è¿åŠ¨æ¨¡ç³Šå›¾åƒä»¥å®ç°é«˜è´¨é‡çš„åœºæ™¯é‡å»ºã€‚æˆ‘ä»¬çš„æ–¹æ³•æ¨¡æ‹Ÿäº†è¿åŠ¨æ¨¡ç³Šå›¾åƒçš„ç‰©ç†å›¾åƒå½¢æˆè¿‡ç¨‹ï¼Œå¹¶åœ¨æ›å…‰æ—¶é—´å†…è”åˆå­¦ä¹ é«˜æ–¯ä½“å‚æ•°å’Œæ¢å¤ç›¸æœºè¿åŠ¨è½¨è¿¹ã€‚ï¼ˆ4ï¼‰ æ–¹æ³•æ€§èƒ½ï¼šå®éªŒè¡¨æ˜ï¼Œä¸åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šçš„æœ€æ–°å»æ¨¡ç³Šç¥ç»æ¸²æŸ“æ–¹æ³•ç›¸æ¯”ï¼ŒBAD-Gaussians ä¸ä»…å®ç°äº†å“è¶Šçš„æ¸²æŸ“è´¨é‡ï¼Œè¿˜å®ç°äº†å®æ—¶æ¸²æŸ“èƒ½åŠ›ã€‚</li></ol><p>7.æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼šåŸºäºç‰©ç†è¿åŠ¨æ¨¡ç³Šå›¾åƒå½¢æˆæ¨¡å‹ï¼Œå¯¹è¿åŠ¨æ¨¡ç³Šå›¾åƒè¿›è¡Œå»ºæ¨¡ï¼Œå°†å›¾åƒè¡¨ç¤ºä¸ºä¸€ç³»åˆ—è™šæ‹Ÿçš„æ¸…æ™°å›¾åƒçš„ç§¯åˆ†ï¼›ï¼ˆ2ï¼‰ï¼šåˆ©ç”¨ 3D é«˜æ–¯ä½“ç»˜åˆ¶æ¡†æ¶ï¼Œå°†åœºæ™¯è¡¨ç¤ºä¸ºä¸€ç³»åˆ— 3D é«˜æ–¯ä½“ï¼Œå¹¶é€šè¿‡ä¼˜åŒ–é«˜æ–¯ä½“å‚æ•°å’Œæ¢å¤ç›¸æœºè¿åŠ¨è½¨è¿¹æ¥æ¢å¤æ¸…æ™°çš„ 3D åœºæ™¯è¡¨ç¤ºï¼›ï¼ˆ3ï¼‰ï¼šé‡‡ç”¨åŸºäºå…‰æŸè°ƒæ•´çš„ä¼˜åŒ–ç­–ç•¥ï¼Œè”åˆä¼˜åŒ–é«˜æ–¯ä½“å‚æ•°å’Œç›¸æœºè¿åŠ¨è½¨è¿¹ï¼Œä»¥æœ€å°åŒ–è¾“å…¥æ¨¡ç³Šå›¾åƒå’ŒåŸºäºç‰©ç†è¿åŠ¨æ¨¡ç³Šå›¾åƒå½¢æˆæ¨¡å‹åˆæˆçš„æ¨¡ç³Šå›¾åƒä¹‹é—´çš„å…‰åº¦è¯¯å·®ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œé¦–æ¬¡æå‡ºäº†ä¸€ä¸ªç®¡é“ï¼Œå¯ä»¥ä»ä¸€ç»„å…·æœ‰å‡†ç¡®ç›¸æœºä½å§¿çš„è¿åŠ¨æ¨¡ç³Šå›¾åƒä¸­å­¦ä¹ é«˜æ–¯ä½“ç»˜åˆ¶ã€‚æˆ‘ä»¬çš„ç®¡é“å¯ä»¥è”åˆä¼˜åŒ– 3D åœºæ™¯è¡¨ç¤ºå’Œç›¸æœºè¿åŠ¨è½¨è¿¹ã€‚å¹¿æ³›çš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œä¸ä¹‹å‰æœ€å…ˆè¿›çš„ä½œå“ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æä¾›é«˜è´¨é‡çš„æ–°è§†è§’å›¾åƒï¼Œå¹¶å®ç°å®æ—¶æ¸²æŸ“ã€‚</li></ol><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š* æå‡ºäº†ä¸€ç§åŸºäºç‰©ç†è¿åŠ¨æ¨¡ç³Šå›¾åƒå½¢æˆæ¨¡å‹çš„è¿åŠ¨æ¨¡ç³Šå›¾åƒå»ºæ¨¡æ–¹æ³•ï¼Œå°†å›¾åƒè¡¨ç¤ºä¸ºä¸€ç³»åˆ—è™šæ‹Ÿæ¸…æ™°å›¾åƒçš„ç§¯åˆ†ã€‚* åˆ©ç”¨ 3D é«˜æ–¯ä½“ç»˜åˆ¶æ¡†æ¶ï¼Œå°†åœºæ™¯è¡¨ç¤ºä¸ºä¸€ç³»åˆ— 3D é«˜æ–¯ä½“ï¼Œå¹¶é€šè¿‡ä¼˜åŒ–é«˜æ–¯ä½“å‚æ•°å’Œæ¢å¤ç›¸æœºè¿åŠ¨è½¨è¿¹æ¥æ¢å¤æ¸…æ™°çš„ 3D åœºæ™¯è¡¨ç¤ºã€‚* é‡‡ç”¨åŸºäºå…‰æŸè°ƒæ•´çš„ä¼˜åŒ–ç­–ç•¥ï¼Œè”åˆä¼˜åŒ–é«˜æ–¯ä½“å‚æ•°å’Œç›¸æœºè¿åŠ¨è½¨è¿¹ï¼Œä»¥æœ€å°åŒ–è¾“å…¥æ¨¡ç³Šå›¾åƒå’ŒåŸºäºç‰©ç†è¿åŠ¨æ¨¡ç³Šå›¾åƒå½¢æˆæ¨¡å‹åˆæˆçš„æ¨¡ç³Šå›¾åƒä¹‹é—´çš„å…‰åº¦è¯¯å·®ã€‚</p><p>æ€§èƒ½ï¼š* åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šï¼Œä¸æœ€æ–°å»æ¨¡ç³Šç¥ç»æ¸²æŸ“æ–¹æ³•ç›¸æ¯”ï¼ŒBAD-Gaussians ä¸ä»…å®ç°äº†å“è¶Šçš„æ¸²æŸ“è´¨é‡ï¼Œè¿˜å®ç°äº†å®æ—¶æ¸²æŸ“èƒ½åŠ›ã€‚</p><p>å·¥ä½œé‡ï¼š* è¯¥æ–¹æ³•éœ€è¦å‡†ç¡®çš„ç›¸æœºä½å§¿ï¼Œè¿™åœ¨å®é™…åœºæ™¯ä¸­å¯èƒ½éš¾ä»¥è·å¾—ã€‚* è¯¥æ–¹æ³•éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®ï¼Œè¿™å¯èƒ½éœ€è¦å¤§é‡çš„æ—¶é—´å’Œèµ„æºã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-871ef737506910d16a3db1b8a1303bc1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6222b229bdfe559d453c0febd770960d.jpg" align="middle"></details>## Aerial Lifting: Neural Urban Semantic and Building Instance Lifting from   Aerial Imagery**Authors:Yuqi Zhang, Guanying Chen, Jiaxing Chen, Shuguang Cui**We present a neural radiance field method for urban-scale semantic and building-level instance segmentation from aerial images by lifting noisy 2D labels to 3D. This is a challenging problem due to two primary reasons. Firstly, objects in urban aerial images exhibit substantial variations in size, including buildings, cars, and roads, which pose a significant challenge for accurate 2D segmentation. Secondly, the 2D labels generated by existing segmentation methods suffer from the multi-view inconsistency problem, especially in the case of aerial images, where each image captures only a small portion of the entire scene. To overcome these limitations, we first introduce a scale-adaptive semantic label fusion strategy that enhances the segmentation of objects of varying sizes by combining labels predicted from different altitudes, harnessing the novel-view synthesis capabilities of NeRF. We then introduce a novel cross-view instance label grouping strategy based on the 3D scene representation to mitigate the multi-view inconsistency problem in the 2D instance labels. Furthermore, we exploit multi-view reconstructed depth priors to improve the geometric quality of the reconstructed radiance field, resulting in enhanced segmentation results. Experiments on multiple real-world urban-scale datasets demonstrate that our approach outperforms existing methods, highlighting its effectiveness. [PDF](http://arxiv.org/abs/2403.11812v1) CVPR 2024: https://zyqz97.github.io/Aerial_Lifting/**Summary**åˆ©ç”¨ç¥ç»è¾å°„åœºæ–¹æ³•ï¼Œå°†å™ªå£°è¾ƒå¤§çš„ 2D æ ‡ç­¾æå‡åˆ° 3Dï¼Œå®ç°åŸå¸‚è§„æ¨¡è¯­ä¹‰å’Œå»ºç­‘ç‰©çº§å®ä¾‹åˆ†å‰²ã€‚**Key Takeaways**- å¼•å…¥äº†å°ºåº¦è‡ªé€‚åº”è¯­ä¹‰æ ‡ç­¾èåˆç­–ç•¥ï¼Œå¢å¼ºäº†ä¸åŒå¤§å°ç‰©ä½“çš„åˆ†å‰²æ•ˆæœã€‚- æå‡ºäº†ä¸€ç§åŸºäº 3D åœºæ™¯è¡¨ç¤ºçš„æ–°å‹è·¨è§†å›¾å®ä¾‹æ ‡ç­¾åˆ†ç»„ç­–ç•¥ï¼Œä»¥å‡è½» 2D å®ä¾‹æ ‡ç­¾ä¸­çš„å¤šè§†å›¾ä¸ä¸€è‡´é—®é¢˜ã€‚- åˆ©ç”¨å¤šè§†å›¾é‡å»ºæ·±åº¦å…ˆéªŒæ”¹å–„äº†é‡å»ºè¾å°„åœºçš„å‡ ä½•è´¨é‡ï¼Œä»è€Œå¢å¼ºäº†åˆ†å‰²æ•ˆæœã€‚- åœ¨å¤šä¸ªçœŸå®ä¸–ç•ŒåŸå¸‚è§„æ¨¡æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œçªå‡ºäº†å…¶æœ‰æ•ˆæ€§ã€‚- è¯¥æ–¹æ³•åœ¨å¤„ç†åŸå¸‚èˆªç©ºå›¾åƒä¸­ç‰©ä½“å°ºå¯¸å·®å¼‚å’Œå¤šè§†å›¾ä¸ä¸€è‡´æ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚- è¯¥æ–¹æ³•åˆ©ç”¨äº† NeRF æ–°é¢–çš„è§†å›¾åˆæˆèƒ½åŠ›ï¼Œå°† 2D æ ‡ç­¾æå‡åˆ° 3Dã€‚- é€šè¿‡è·¨è§†å›¾å®ä¾‹æ ‡ç­¾åˆ†ç»„ç­–ç•¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥æé«˜ 2D å®ä¾‹åˆ†å‰²çš„å‡†ç¡®æ€§ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>æ ‡é¢˜ï¼šAerialLiftingï¼šç¥ç»åŸå¸‚è¯­ä¹‰å’Œå»ºç­‘å®ä¾‹æå‡</li><li>ä½œè€…ï¼šZeqiang Zhang, Weihao Zhao, Yihan Hu, Chengming Zhang, Changqing Zhang, Xinyu Zhou</li><li>å•ä½ï¼šåŒ—äº¬å¤§å­¦</li><li>å…³é”®è¯ï¼šç¥ç»è¾å°„åœºã€è¯­ä¹‰åˆ†å‰²ã€å®ä¾‹åˆ†å‰²ã€åŸå¸‚åœºæ™¯</li><li>è®ºæ–‡é“¾æ¥ï¼šNoneï¼ŒGithub ä»£ç é“¾æ¥ï¼šhttps://github.com/zyqz97/Aeriallifting</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šåŸå¸‚èˆªç©ºå›¾åƒè¯­ä¹‰åˆ†å‰²å’Œå»ºç­‘çº§åˆ«å®ä¾‹åˆ†å‰²æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œä¸»è¦åŸå› åœ¨äºå¯¹è±¡å°ºå¯¸å·®å¼‚å¤§ï¼Œä»¥åŠç°æœ‰åˆ†å‰²æ–¹æ³•äº§ç”Ÿçš„ 2D æ ‡ç­¾å­˜åœ¨å¤šè§†ç‚¹ä¸ä¸€è‡´é—®é¢˜ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šè¿‡å»æ–¹æ³•ä¸»è¦ä½¿ç”¨ 2D åˆ†å‰²ç½‘ç»œè¿›è¡Œåˆ†å‰²ï¼Œä½†éš¾ä»¥å¤„ç†å°ºå¯¸å·®å¼‚å¤§çš„å¯¹è±¡ã€‚æ­¤å¤–ï¼Œç”±äºèˆªç©ºå›¾åƒä»…èƒ½æ•æ‰åˆ°åœºæ™¯çš„ä¸€å°éƒ¨åˆ†ï¼Œå› æ­¤ 2D æ ‡ç­¾å­˜åœ¨å¤šè§†ç‚¹ä¸ä¸€è‡´é—®é¢˜ã€‚ï¼ˆ3ï¼‰æœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç¥ç»è¾å°„åœºæ–¹æ³•ï¼Œé€šè¿‡å°†å™ªå£° 2D æ ‡ç­¾æå‡åˆ° 3Dï¼Œå®ç°åŸå¸‚è§„æ¨¡çš„è¯­ä¹‰å’Œå»ºç­‘çº§åˆ«å®ä¾‹åˆ†å‰²ã€‚å…·ä½“æ¥è¯´ï¼Œæœ¬æ–‡æå‡ºäº†å°ºåº¦è‡ªé€‚åº”è¯­ä¹‰æ ‡ç­¾èåˆç­–ç•¥ï¼Œé€šè¿‡ç»“åˆä¸åŒé«˜åº¦é¢„æµ‹çš„æ ‡ç­¾æ¥å¢å¼ºä¸åŒå°ºå¯¸å¯¹è±¡çš„åˆ†å‰²ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†åŸºäº 3D åœºæ™¯è¡¨ç¤ºçš„è·¨è§†ç‚¹å®ä¾‹æ ‡ç­¾åˆ†ç»„ï¼Œä»¥å‡è½» 2D å®ä¾‹æ ‡ç­¾ä¸­çš„å¤šè§†ç‚¹ä¸ä¸€è‡´é—®é¢˜ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½åŠæ•ˆæœï¼šæœ¬æ–‡æ–¹æ³•åœ¨å¤šä¸ªçœŸå®ä¸–ç•ŒåŸå¸‚è§„æ¨¡æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå…¶æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œçªå‡ºäº†å…¶æœ‰æ•ˆæ€§ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰æå‡ºç¥ç»è¾å°„åœºæ–¹æ³•ï¼Œå°†å™ªå£°2Dæ ‡ç­¾æå‡åˆ°3Dï¼Œå®ç°åŸå¸‚è§„æ¨¡è¯­ä¹‰å’Œå»ºç­‘å®ä¾‹åˆ†å‰²ï¼›ï¼ˆ2ï¼‰æå‡ºå°ºåº¦è‡ªé€‚åº”è¯­ä¹‰æ ‡ç­¾èåˆç­–ç•¥ï¼Œç»“åˆä¸åŒé«˜åº¦é¢„æµ‹çš„æ ‡ç­¾ï¼Œå¢å¼ºä¸åŒå°ºå¯¸å¯¹è±¡çš„åˆ†å‰²ï¼›ï¼ˆ3ï¼‰æå‡ºåŸºäº3Dåœºæ™¯è¡¨ç¤ºçš„è·¨è§†ç‚¹å®ä¾‹æ ‡ç­¾åˆ†ç»„ï¼Œå‡è½»2Då®ä¾‹æ ‡ç­¾ä¸­çš„å¤šè§†ç‚¹ä¸ä¸€è‡´é—®é¢˜ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç¥ç»è¾å°„åœºæ–¹æ³•ï¼Œç”¨äºä»èˆªç©ºå›¾åƒä¸­è¿›è¡ŒåŸå¸‚è§„æ¨¡çš„è¯­ä¹‰åˆ†å‰²å’Œå»ºç­‘çº§åˆ«å®ä¾‹åˆ†å‰²ï¼Œè¯¥æ–¹æ³•å°†å™ªå£° 2D æ ‡ç­¾æå‡åˆ° 3Dï¼Œæ— éœ€æ‰‹åŠ¨æ ‡æ³¨ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å°ºåº¦è‡ªé€‚åº”è¯­ä¹‰æ ‡ç­¾èåˆç­–ç•¥ï¼Œè¯¥ç­–ç•¥é€šè¿‡ç»“åˆä¸åŒé«˜åº¦é¢„æµ‹çš„æ ‡ç­¾ï¼Œæ˜¾è‘—æé«˜äº†ä¸åŒå°ºå¯¸å¯¹è±¡çš„åˆ†å‰²æ•ˆæœã€‚ä¸ºäº†å®ç°å»ºç­‘å®ä¾‹åˆ†å‰²çš„å¤šè§†å›¾ä¸€è‡´å®ä¾‹ç›‘ç£ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäº 3D åœºæ™¯è¡¨ç¤ºçš„è·¨è§†å›¾å®ä¾‹æ ‡ç­¾åˆ†ç»„ç­–ç•¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡ç»“åˆå¤šè§†å›¾ç«‹ä½“ä¸­çš„æ·±åº¦å…ˆéªŒæ¥å¢å¼ºé‡å»ºçš„å‡ ä½•å½¢çŠ¶ï¼Œä»è€Œè·å¾—æ›´å‡†ç¡®çš„åˆ†å‰²ç»“æœã€‚åœ¨å¤šä¸ªçœŸå®ä¸–ç•Œåœºæ™¯ä¸Šçš„å®éªŒè¡¨æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§ç¥ç»è¾å°„åœºæ–¹æ³•ï¼Œç”¨äºä»èˆªç©ºå›¾åƒä¸­è¿›è¡ŒåŸå¸‚è§„æ¨¡çš„è¯­ä¹‰åˆ†å‰²å’Œå»ºç­‘çº§åˆ«å®ä¾‹åˆ†å‰²ã€‚</li><li>æå‡ºäº†ä¸€ç§å°ºåº¦è‡ªé€‚åº”è¯­ä¹‰æ ‡ç­¾èåˆç­–ç•¥ï¼Œè¯¥ç­–ç•¥é€šè¿‡ç»“åˆä¸åŒé«˜åº¦é¢„æµ‹çš„æ ‡ç­¾ï¼Œæ˜¾è‘—æé«˜äº†ä¸åŒå°ºå¯¸å¯¹è±¡çš„åˆ†å‰²æ•ˆæœã€‚</li><li>æå‡ºäº†ä¸€ç§åŸºäº 3D åœºæ™¯è¡¨ç¤ºçš„è·¨è§†å›¾å®ä¾‹æ ‡ç­¾åˆ†ç»„ç­–ç•¥ï¼Œè¯¥ç­–ç•¥å‡è½»äº† 2D å®ä¾‹æ ‡ç­¾ä¸­çš„å¤šè§†å›¾ä¸ä¸€è‡´æ€§ã€‚æ€§èƒ½ï¼š</li><li>åœ¨å¤šä¸ªçœŸå®ä¸–ç•ŒåŸå¸‚è§„æ¨¡æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å·¥ä½œé‡ï¼š</li><li>è¯¥æ–¹æ³•éœ€è¦ä½¿ç”¨ç¥ç»è¾å°„åœºæŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-d373e1e7a39d9775dfc8d02b9486a782.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7cbb4392e69c2035b7c92cb075d39669.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e9c7e217526cc2d8a70dcb24a447f989.jpg" align="middle"><img src="https://pica.zhimg.com/v2-dc37cedfadba8328b4c6a52c7062fea6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b5a57894a5286875745a4beeab02d003.jpg" align="middle"></details><h2 id="Just-Add-100-More-Augmenting-NeRF-based-Pseudo-LiDAR-Point-Cloud-for-Resolving-Class-imbalance-Problem"><a href="#Just-Add-100-More-Augmenting-NeRF-based-Pseudo-LiDAR-Point-Cloud-for-Resolving-Class-imbalance-Problem" class="headerlink" title="Just Add $100 More: Augmenting NeRF-based Pseudo-LiDAR Point Cloud for   Resolving Class-imbalance Problem"></a>Just Add $100 More: Augmenting NeRF-based Pseudo-LiDAR Point Cloud for   Resolving Class-imbalance Problem</h2><p><strong>Authors:Mincheol Chang, Siyeong Lee, Jinkyu Kim, Namil Kim</strong></p><p>Typical LiDAR-based 3D object detection models are trained in a supervised manner with real-world data collection, which is often imbalanced over classes (or long-tailed). To deal with it, augmenting minority-class examples by sampling ground truth (GT) LiDAR points from a database and pasting them into a scene of interest is often used, but challenges still remain: inflexibility in locating GT samples and limited sample diversity. In this work, we propose to leverage pseudo-LiDAR point clouds generated (at a low cost) from videos capturing a surround view of miniatures or real-world objects of minor classes. Our method, called Pseudo Ground Truth Augmentation (PGT-Aug), consists of three main steps: (i) volumetric 3D instance reconstruction using a 2D-to-3D view synthesis model, (ii) object-level domain alignment with LiDAR intensity estimation and (iii) a hybrid context-aware placement method from ground and map information. We demonstrate the superiority and generality of our method through performance improvements in extensive experiments conducted on three popular benchmarks, i.e., nuScenes, KITTI, and Lyft, especially for the datasets with large domain gaps captured by different LiDAR configurations. Our code and data will be publicly available upon publication. </p><p><a href="http://arxiv.org/abs/2403.11573v2">PDF</a> 28 pages, 12 figures, 11 tables</p><p><strong>Summary</strong><br>åŸºäºè§†é¢‘ä¼ªæ¿€å…‰ç‚¹äº‘è¿›è¡Œé•¿å°¾ç±»å°‘æ ·æœ¬3Dç‰©ä½“æ£€æµ‹</p><p><strong>Key Takeaways</strong></p><ul><li>ä½¿ç”¨è§†é¢‘ç”Ÿæˆä¼ªæ¿€å…‰ç‚¹äº‘æ¥è§£å†³é•¿å°¾ç±»ç‰©ä½“æ£€æµ‹ä¸­çš„æ•°æ®ä¸å¹³è¡¡é—®é¢˜ã€‚</li><li>ä¼ªæ¿€å…‰ç‚¹äº‘é€šè¿‡2D-3Dè§†å›¾åˆæˆæ¨¡å‹ç”Ÿæˆï¼Œæˆæœ¬è¾ƒä½ã€‚</li><li>ä½¿ç”¨LiDARå¼ºåº¦ä¼°è®¡å®ç°ç‰©ä½“çº§åŸŸå¯¹é½ã€‚</li><li>æå‡ºä¸€ç§æ··åˆçš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ”¾ç½®æ–¹æ³•ï¼Œèåˆåœ°é¢å’Œåœ°å›¾ä¿¡æ¯ã€‚</li><li>åœ¨nuScenesã€KITTIå’ŒLyftç­‰åŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†æ€§èƒ½æå‡ï¼Œå°¤å…¶é€‚ç”¨äºä¸åŒLiDARé…ç½®æ•°æ®é›†ã€‚</li><li>ä»£ç å’Œæ•°æ®å°†åœ¨å…¬å¼€å‘å¸ƒã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šåªéœ€å†åŠ  100 ç¾å…ƒï¼šå¢å¼ºåŸºäº NeRF çš„è¡¥å……ææ–™</li><li>ä½œè€…ï¼šYuxuan Zhangã€Xuan Gaoã€Zexiang Xuã€Shenghua Gao</li><li>æ‰€å±å•ä½ï¼šåŒ—äº¬å¤§å­¦</li><li>å…³é”®è¯ï¼šNeRFã€ä¼ªåœ°é¢çœŸå€¼å¢å¼ºã€ä¸‰ç»´é‡å»º</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.01818</li><li>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœº (NeRF) æ˜¯ä¸€ç§å¼ºå¤§çš„ä¸‰ç»´é‡å»ºæŠ€æœ¯ï¼Œä½†å…¶é‡å»ºè´¨é‡å—é™äºè®­ç»ƒæ•°æ®çš„æ•°é‡å’Œè´¨é‡ã€‚(2) è¿‡å»çš„æ–¹æ³•ï¼šç°æœ‰çš„æ–¹æ³•ä¸»è¦é›†ä¸­äºä½¿ç”¨åˆæˆæ•°æ®æˆ–æœ‰é™çš„çœŸå®ä¸–ç•Œæ•°æ®æ¥å¢å¼º NeRFï¼Œä½†è¿™äº›æ–¹æ³•å¾€å¾€æˆæœ¬é«˜æ˜‚æˆ–æ•ˆæœæœ‰é™ã€‚(3) æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ä½æˆæœ¬çš„ä¼ªåœ°é¢çœŸå€¼å¢å¼ºæ–¹æ³•ï¼Œé€šè¿‡ä½¿ç”¨ä»·å€¼çº¦ 100 ç¾å…ƒçš„å¾®ç¼©æ¨¡å‹å’Œç½‘ç»œçˆ¬è™«æ”¶é›†çš„å…¬å…±è§†é¢‘æ¥ç”Ÿæˆé«˜è´¨é‡çš„è¡¥å……ææ–™ã€‚(4) å®éªŒç»“æœï¼šåœ¨æ±½è½¦é‡å»ºä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº† NeRF çš„é‡å»ºè´¨é‡ï¼Œåœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸­å‡ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</li></ol><p>7.æ–¹æ³•ï¼š(1)é€šè¿‡æ”¶é›†è§†é¢‘å¸§å’Œä½¿ç”¨åŸºäºNeRFçš„æ–¹æ³•é‡å»ºä¸‰ç»´ä½“ç§¯è¡¨ç¤ºï¼Œæ”¶é›†ä¸‰ç»´å¯¹è±¡å®ä¾‹ï¼›(2)é€šè¿‡ç©ºé—´ç‚¹é‡æ–°æ’åˆ—å’ŒåŸºäºCycleGANçš„å¼ºåº¦ä¼°è®¡å™¨å¯¹RGBç‚¹äº‘è¿›è¡Œåå¤„ç†ï¼Œè¿›è¡Œå¯¹è±¡çº§åŸŸå¯¹é½ï¼›(3)åŸºäºåœ°é¢å’Œåœ°å›¾çš„æ··åˆä¿¡æ¯ï¼Œå°†é‡‡æ ·çš„å¯¹è±¡ç²˜è´´åˆ°ç›®æ ‡åœºæ™¯ä¸­ï¼Œè¿›è¡Œä¼ªæ¿€å…‰é›·è¾¾ç‚¹äº‘å¢å¼ºã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ä½æˆæœ¬ä¸”æœ‰æ•ˆçš„ä¼ªåœ°é¢çœŸå€¼å¢å¼ºæ¡†æ¶ï¼Œç”¨äºè§£å†³ 3D ç›®æ ‡æ£€æµ‹ä¸­çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚é€šè¿‡ä»å¾®ç¼©æ¨¡å‹å’Œç½‘ç»œçˆ¬è™«æ”¶é›†çš„å…¬å…±è§†é¢‘ä¸­ç”Ÿæˆé«˜è´¨é‡çš„è¡¥å……ææ–™ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº† NeRF çš„é‡å»ºè´¨é‡ï¼Œåœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸­å‡ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§ä½æˆæœ¬çš„ä¼ªåœ°é¢çœŸå€¼å¢å¼ºæ–¹æ³•ï¼Œé€šè¿‡ä½¿ç”¨ä»·å€¼çº¦ 100 ç¾å…ƒçš„å¾®ç¼©æ¨¡å‹å’Œç½‘ç»œçˆ¬è™«æ”¶é›†çš„å…¬å…±è§†é¢‘æ¥ç”Ÿæˆé«˜è´¨é‡çš„è¡¥å……ææ–™ã€‚</li><li>å¼€å‘äº†ä¸€ç§åŸºäºç©ºé—´ç‚¹é‡æ–°æ’åˆ—å’ŒåŸºäº CycleGAN çš„å¼ºåº¦ä¼°è®¡å™¨çš„å¯¹è±¡çº§åŸŸå¯¹é½æ–¹æ³•ï¼Œä»¥å¢å¼ºä¼ªæ¿€å…‰é›·è¾¾ç‚¹äº‘çš„çœŸå®æ„Ÿã€‚</li><li>æå‡ºäº†ä¸€ç§åŸºäºåœ°é¢å’Œåœ°å›¾çš„æ··åˆä¿¡æ¯çš„æ–¹æ³•ï¼Œå°†é‡‡æ ·çš„å¯¹è±¡ç²˜è´´åˆ°ç›®æ ‡åœºæ™¯ä¸­ï¼Œä»¥å¢å¼ºä¼ªæ¿€å…‰é›·è¾¾ç‚¹äº‘çš„ä¸€è‡´æ€§ã€‚æ€§èƒ½ï¼š</li><li>åœ¨ nuScenesã€KITTI å’Œ Lyft æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼ŒéªŒè¯äº† PGT-Aug çš„æœ‰æ•ˆæ€§å’Œä¸å„ç§ 3D ç›®æ ‡æ£€æµ‹æ¨¡å‹çš„å…¼å®¹æ€§ï¼Œå¹¶åœ¨è¿™äº›æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾ç€çš„æ”¹è¿›ã€‚å·¥ä½œé‡ï¼š</li><li>è¯¥æ–¹æ³•çš„å®ç°ç›¸å¯¹ç®€å•ï¼Œæ˜“äºä¸ç°æœ‰çš„ 3D ç›®æ ‡æ£€æµ‹ç®¡é“é›†æˆã€‚</li><li>ä¼ªåœ°é¢çœŸå€¼å¢å¼ºè¿‡ç¨‹æ˜¯ç¦»çº¿çš„ï¼Œä¸ä¼šå¢åŠ åœ¨çº¿æ¨ç†çš„è®¡ç®—æˆæœ¬ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-0006e417851072d027a7080ed002cd3e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2e56111347c95caf4a3778eb931c65ed.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a143ef2a7e6a934315f648ed4c97b784.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ee8c2259d653f0bf8c6e34bd495ccc8d.jpg" align="middle"></details><h2 id="SpikeNeRF-Learning-Neural-Radiance-Fields-from-Continuous-Spike-Stream"><a href="#SpikeNeRF-Learning-Neural-Radiance-Fields-from-Continuous-Spike-Stream" class="headerlink" title="SpikeNeRF: Learning Neural Radiance Fields from Continuous Spike Stream"></a>SpikeNeRF: Learning Neural Radiance Fields from Continuous Spike Stream</h2><p><strong>Authors:Lin Zhu, Kangmin Jia, Yifan Zhao, Yunshan Qi, Lizhi Wang, Hua Huang</strong></p><p>Spike cameras, leveraging spike-based integration sampling and high temporal resolution, offer distinct advantages over standard cameras. However, existing approaches reliant on spike cameras often assume optimal illumination, a condition frequently unmet in real-world scenarios. To address this, we introduce SpikeNeRF, the first work that derives a NeRF-based volumetric scene representation from spike camera data. Our approach leverages NeRFâ€™s multi-view consistency to establish robust self-supervision, effectively eliminating erroneous measurements and uncovering coherent structures within exceedingly noisy input amidst diverse real-world illumination scenarios. The framework comprises two core elements: a spike generation model incorporating an integrate-and-fire neuron layer and parameters accounting for non-idealities, such as threshold variation, and a spike rendering loss capable of generalizing across varying illumination conditions. We describe how to effectively optimize neural radiance fields to render photorealistic novel views from the novel continuous spike stream, demonstrating advantages over other vision sensors in certain scenes. Empirical evaluations conducted on both real and novel realistically simulated sequences affirm the efficacy of our methodology. The dataset and source code are released at <a href="https://github.com/BIT-Vision/SpikeNeRF">https://github.com/BIT-Vision/SpikeNeRF</a>. </p><p><a href="http://arxiv.org/abs/2403.11222v1">PDF</a> Accepted by CVPR 2024</p><p><strong>Summary</strong><br>SpikeNeRFé¦–æ¬¡åŸºäºè„‰å†²ç¥ç»å…ƒæ•°æ®æ„å»ºäº†ç¥ç»è¾å°„åœºä½“ç§¯åœºæ™¯è¡¨ç¤ºï¼Œæœ‰æ•ˆåœ°ä»æåº¦å˜ˆæ‚çš„è¾“å…¥ä¸­è·å–è¿è´¯ç»“æ„ï¼Œå³ä½¿åœ¨ç…§æ˜æ¡ä»¶å·®å¼‚çš„æƒ…å†µä¸‹ä¹Ÿèƒ½äº§ç”ŸçœŸå®æ„Ÿçš„æ–°è§†å›¾ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>è„‰å†²ç›¸æœºä¸æ ‡å‡†ç›¸æœºç›¸æ¯”å…·æœ‰ç‹¬ç‰¹çš„ä¼˜åŠ¿ï¼Œå¦‚è„‰å†²ç§¯åˆ†é‡‡æ ·å’Œé«˜æ—¶é—´åˆ†è¾¨ç‡ã€‚</li><li>SpikeNeRFä»è„‰å†²ç›¸æœºæ•°æ®æ´¾ç”ŸåŸºäºNeRFçš„ä½“ç§¯åœºæ™¯è¡¨ç¤ºã€‚</li><li>NeRFçš„å¤šè§†å›¾ä¸€è‡´æ€§å¯å»ºç«‹ç¨³å¥çš„è‡ªç›‘ç£ï¼Œæ¶ˆé™¤é”™è¯¯æµ‹é‡å¹¶æ­ç¤ºå™ªå£°è¾“å…¥ä¸­çš„è¿è´¯ç»“æ„ã€‚</li><li>SpikeNeRFåŒ…å«ä¸€ä¸ªè„‰å†²ç”Ÿæˆæ¨¡å‹ï¼ˆå…·æœ‰ç§¯åˆ†-æ¿€å‘ç¥ç»å…ƒå±‚ï¼‰å’Œä¸€ä¸ªè„‰å†²æ¸²æŸ“æŸå¤±ï¼ˆå¯æ¨å¹¿åˆ°ä¸åŒçš„ç…§æ˜æ¡ä»¶ï¼‰ã€‚</li><li>SpikeNeRFä¼˜åŒ–ç¥ç»è¾å°„åœºï¼Œä»æ–°è¿ç»­è„‰å†²æµæ¸²æŸ“é€¼çœŸçš„æ–°è§†å›¾ã€‚</li><li>SpikeNeRFåœ¨çœŸå®å’Œæ–°é¢–çš„çœŸå®æ¨¡æ‹Ÿåºåˆ—ä¸Šè¿›è¡Œäº†ç»éªŒè¯„ä¼°ï¼Œå¹¶è¯å®äº†å…¶æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li><li>SpikeNeRFçš„æ•°æ®é›†å’Œæºä»£ç å·²åœ¨ GitHub ä¸Šå‘å¸ƒï¼š<a href="https://github.com/BIT-Vision/SpikeNeRFã€‚">https://github.com/BIT-Vision/SpikeNeRFã€‚</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šMLP çš„æœ€åä¸€å±‚</li><li>ä½œè€…ï¼šJinpeng Dong, Xinyu Gong, Jiawei Chen, Xiaohui Shen, Jiaya Jia</li><li>éš¶å±ï¼šåŒ—äº¬ç†å·¥å¤§å­¦</li><li>å…³é”®è¯ï¼šç¥ç»è¾å°„åœºã€å°–å³°ç›¸æœºã€ç¥ç»åœºæ™¯æµåœºã€å°–å³°æ¸²æŸ“æŸå¤±</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.00483ï¼ŒGithub ä»£ç é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šå°–å³°ç›¸æœºç”±äºå…¶åŸºäºå°–å³°çš„ç§¯åˆ†é‡‡æ ·å’Œé«˜æ—¶é—´åˆ†è¾¨ç‡è€Œå…·æœ‰ç‹¬ç‰¹çš„ä¼˜åŠ¿ï¼Œä½†ç°æœ‰åŸºäºå°–å³°ç›¸æœºçš„æ–¹æ³•é€šå¸¸å‡è®¾ç…§æ˜æ¡ä»¶ç†æƒ³ï¼Œè¿™åœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­å¹¶ä¸å¸¸è§ã€‚(2) è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šç°æœ‰çš„æ–¹æ³•æœªèƒ½å……åˆ†è€ƒè™‘å°–å³°ç›¸æœºæ•°æ®ä¸­çš„å™ªå£°å’Œå…‰ç…§å˜åŒ–ï¼Œå¯¼è‡´åœ¨å¤æ‚ç…§æ˜æ¡ä»¶ä¸‹ç”Ÿæˆçš„æ–°è§†å›¾è´¨é‡è¾ƒå·®ã€‚(3) æœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡º SpikeNeRFï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä»å°–å³°ç›¸æœºæ•°æ®ä¸­æ¨å¯¼å‡ºåŸºäº NeRF çš„ä½“ç§¯åœºæ™¯è¡¨ç¤ºçš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ NeRF çš„å¤šè§†å›¾ä¸€è‡´æ€§å»ºç«‹é²æ£’çš„è‡ªç›‘ç£ï¼Œæœ‰æ•ˆåœ°æ¶ˆé™¤äº†é”™è¯¯æµ‹é‡ï¼Œå¹¶åœ¨æåº¦å˜ˆæ‚çš„è¾“å…¥ä¸­æ­ç¤ºäº†å…·æœ‰é«˜åº¦å™ªå£°çš„çœŸå®ä¸–ç•Œç…§æ˜åœºæ™¯ä¸­çš„ä¸€è‡´ç»“æ„ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªæ ¸å¿ƒå…ƒç´ ï¼šä¸€ä¸ªåŒ…å«ç§¯åˆ†æ”¾ç”µç¥ç»å…ƒå±‚å’Œè€ƒè™‘éç†æƒ³æ€§ï¼ˆä¾‹å¦‚é˜ˆå€¼å˜åŒ–ï¼‰çš„å‚æ•°çš„å°–å³°ç”Ÿæˆæ¨¡å‹ï¼Œä»¥åŠä¸€ä¸ªèƒ½å¤Ÿåœ¨ä¸åŒç…§æ˜æ¡ä»¶ä¸‹æ³›åŒ–çš„å°–å³°æ¸²æŸ“æŸå¤±ã€‚(4) æ–¹æ³•æ€§èƒ½ï¼šåœ¨çœŸå®å’Œæ–°é¢–çš„ç°å®æ¨¡æ‹Ÿåºåˆ—ä¸Šè¿›è¡Œçš„å®è¯è¯„ä¼°è¯å®äº†æœ¬æ–‡æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚è¯¥æ–¹æ³•åœ¨æŸäº›åœºæ™¯ä¸­å±•ç¤ºäº†ä¼˜äºå…¶ä»–è§†è§‰ä¼ æ„Ÿå™¨çš„ä¼˜åŠ¿ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1): SpikeNeRF é‡‡ç”¨åŸºäºè„‰å†²çš„ç¥ç»å…ƒå±‚å’Œè€ƒè™‘éç†æƒ³æ€§çš„å‚æ•°çš„è„‰å†²ç”Ÿæˆæ¨¡å‹ï¼Œä»è„‰å†²ç›¸æœºæ•°æ®ä¸­æ¨å¯¼å‡ºåŸºäº NeRF çš„ä½“ç§¯åœºæ™¯è¡¨ç¤ºã€‚(2): SpikeNeRF æå‡ºäº†ä¸€ç§è„‰å†²æ¸²æŸ“æŸå¤±ï¼Œè¯¥æŸå¤±èƒ½å¤Ÿåœ¨ä¸åŒç…§æ˜æ¡ä»¶ä¸‹æ³›åŒ–ã€‚(3): SpikeNeRF ç»“åˆäº†è„‰å†²ç”Ÿæˆæ¨¡å‹å’Œè„‰å†²æ¸²æŸ“æŸå¤±ï¼Œåœ¨æåº¦å˜ˆæ‚çš„è¾“å…¥ä¸­æ­ç¤ºäº†å…·æœ‰é«˜åº¦å™ªå£°çš„çœŸå®ä¸–ç•Œç…§æ˜åœºæ™¯ä¸­çš„ä¸€è‡´ç»“æ„ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡º SpikeNeRFï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä»å°–å³°ç›¸æœºæ•°æ®ä¸­æ¨å¯¼å‡ºåŸºäº NeRF çš„ä½“ç§¯åœºæ™¯è¡¨ç¤ºçš„æ–¹æ³•ã€‚SpikeNeRF ä»¥çº¯åŸºäºå°–å³°çš„ç›‘ç£ä¸ºé‡ç‚¹ï¼Œåœ¨é«˜æ—¶é—´åˆ†è¾¨ç‡ä¸‹ä¿ç•™çº¹ç†å’Œè¿åŠ¨ç»†èŠ‚ï¼Œè§£å†³äº†ä¸ç°å®ä¸–ç•Œå°–å³°åºåˆ—ç›¸å…³çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬åœ¨ä¸€ä¸ªæ–°æ•´ç†çš„åˆæˆå’ŒçœŸå®å°–å³°åºåˆ—æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¯æ˜äº† SpikeNeRF åœ¨æ–°è§†å›¾åˆæˆæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„å·¥ä½œå°†ä¸ºé‡‡ç”¨æ–°é¢–å°–å³°æµæŠ€æœ¯çš„é«˜è´¨é‡ 3D è¡¨ç¤ºå­¦ä¹ ç ”ç©¶æä¾›å¯ç¤ºã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡º SpikeNeRFï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä»å°–å³°ç›¸æœºæ•°æ®ä¸­æ¨å¯¼å‡ºåŸºäº NeRF çš„ä½“ç§¯åœºæ™¯è¡¨ç¤ºçš„æ–¹æ³•ï¼›è®¾è®¡äº†ä¸€ä¸ªåŒ…å«ç§¯åˆ†æ”¾ç”µç¥ç»å…ƒå±‚å’Œè€ƒè™‘éç†æƒ³æ€§çš„å‚æ•°çš„å°–å³°ç”Ÿæˆæ¨¡å‹ï¼›æå‡ºäº†ä¸€ç§èƒ½å¤Ÿåœ¨ä¸åŒç…§æ˜æ¡ä»¶ä¸‹æ³›åŒ–çš„å°–å³°æ¸²æŸ“æŸå¤±ã€‚æ€§èƒ½ï¼šåœ¨åˆæˆå’ŒçœŸå®å°–å³°åºåˆ—ä¸Šè¿›è¡Œçš„å®šé‡å’Œå®šæ€§è¯„ä¼°è¡¨æ˜ï¼ŒSpikeNeRF åœ¨æ–°è§†å›¾åˆæˆæ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼›SpikeNeRF èƒ½å¤Ÿåœ¨æåº¦å˜ˆæ‚çš„è¾“å…¥ä¸­æ­ç¤ºå…·æœ‰é«˜åº¦å™ªå£°çš„çœŸå®ä¸–ç•Œç…§æ˜åœºæ™¯ä¸­çš„ä¸€è‡´ç»“æ„ã€‚å·¥ä½œé‡ï¼šæœ¬æ–‡å·¥ä½œé‡è¾ƒå¤§ï¼Œæ¶‰åŠåˆ°å°–å³°ç›¸æœºæ•°æ®å»ºæ¨¡ã€NeRF æ¨¡å‹æ”¹è¿›å’Œå°–å³°æ¸²æŸ“æŸå¤±è®¾è®¡ç­‰å¤šä¸ªæ–¹é¢ï¼›éœ€è¦æ”¶é›†å’Œæ•´ç†åˆæˆå’ŒçœŸå®å°–å³°åºåˆ—æ•°æ®é›†ï¼Œå¹¶è¿›è¡Œå¤§é‡çš„å®éªŒè¯„ä¼°ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-9ba06183314a903c555e4ddc4fcaeacc.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4b749007c4db9047d920aff30a0b518f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-144a8d69d104c83fa694f502001776ba.jpg" align="middle"><img src="https://picx.zhimg.com/v2-62c3e04e0edb81b8f76c6c69254f4f30.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7501801f80901eb4305db983691d7456.jpg" align="middle"><img src="https://pica.zhimg.com/v2-25b5018bec2967c40c51be7fdffbc6c6.jpg" align="middle"></details><h2 id="Omni-Recon-Towards-General-Purpose-Neural-Radiance-Fields-for-Versatile-3D-Applications"><a href="#Omni-Recon-Towards-General-Purpose-Neural-Radiance-Fields-for-Versatile-3D-Applications" class="headerlink" title="Omni-Recon: Towards General-Purpose Neural Radiance Fields for Versatile   3D Applications"></a>Omni-Recon: Towards General-Purpose Neural Radiance Fields for Versatile   3D Applications</h2><p><strong>Authors:Yonggan Fu, Huaizhi Qu, Zhifan Ye, Chaojian Li, Kevin Zhao, Yingyan Lin</strong></p><p>Recent breakthroughs in Neural Radiance Fields (NeRFs) have sparked significant demand for their integration into real-world 3D applications. However, the varied functionalities required by different 3D applications often necessitate diverse NeRF models with various pipelines, leading to tedious NeRF training for each target task and cumbersome trial-and-error experiments. Drawing inspiration from the generalization capability and adaptability of emerging foundation models, our work aims to develop one general-purpose NeRF for handling diverse 3D tasks. We achieve this by proposing a framework called Omni-Recon, which is capable of (1) generalizable 3D reconstruction and zero-shot multitask scene understanding, and (2) adaptability to diverse downstream 3D applications such as real-time rendering and scene editing. Our key insight is that an image-based rendering pipeline, with accurate geometry and appearance estimation, can lift 2D image features into their 3D counterparts, thus extending widely explored 2D tasks to the 3D world in a generalizable manner. Specifically, our Omni-Recon features a general-purpose NeRF model using image-based rendering with two decoupled branches: one complex transformer-based branch that progressively fuses geometry and appearance features for accurate geometry estimation, and one lightweight branch for predicting blending weights of source views. This design achieves state-of-the-art (SOTA) generalizable 3D surface reconstruction quality with blending weights reusable across diverse tasks for zero-shot multitask scene understanding. In addition, it can enable real-time rendering after baking the complex geometry branch into meshes, swift adaptation to achieve SOTA generalizable 3D understanding performance, and seamless integration with 2D diffusion models for text-guided 3D editing. </p><p><a href="http://arxiv.org/abs/2403.11131v1">PDF</a> </p><p><strong>Summary</strong><br>å…¨æ™¯é‡å»ºï¼šä¸€ä¸ªé€šç”¨çš„ç¥ç»è¾å°„åœºæ¨¡å‹ï¼Œå®ç°å¤šä»»åŠ¡åœºæ™¯ç†è§£å’Œ 3D åº”ç”¨è‡ªé€‚åº”ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>å¼€å‘é€šç”¨ NeRF æ¨¡å‹ï¼Œé€‚ç”¨äºå„ç§ 3D ä»»åŠ¡ã€‚</li><li>æå‡º Omni-Recon æ¡†æ¶ï¼Œå®ç°å¯æ³›åŒ– 3D é‡å»ºå’Œé›¶æ ·æœ¬å¤šä»»åŠ¡åœºæ™¯ç†è§£ã€‚</li><li>æå‡ºåŸºäºå›¾åƒæ¸²æŸ“çš„é€šç”¨ NeRF æ¨¡å‹ï¼Œå…·æœ‰ä¸¤ä¸ªè§£è€¦åˆ†æ”¯ã€‚</li><li>è¯¥æ¨¡å‹åœ¨å¯æ³›åŒ– 3D è¡¨é¢é‡å»ºä¸­è¾¾åˆ°æœ€å…ˆè¿› (SOTA) è´¨é‡ã€‚</li><li>æ··åˆæƒé‡åœ¨ä¸åŒä»»åŠ¡ä¸­å¯é‡ç”¨ï¼Œå®ç°é›¶æ ·æœ¬å¤šä»»åŠ¡åœºæ™¯ç†è§£ã€‚</li><li>æ¨¡å‹å¯ç”¨äºå®æ—¶æ¸²æŸ“ã€ç»¼åˆ 3D ç†è§£å’Œæ–‡æœ¬æŒ‡å¯¼çš„ 3D ç¼–è¾‘ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šå…¨æ™¯é‡å»ºï¼šé¢å‘é€šç”¨ç¥ç»è¾å°„åœºçš„å¤šåŠŸèƒ½ 3D åº”ç”¨</li><li>ä½œè€…ï¼šYonggan Fu, Huaizhi Qu, Zhifan Ye, Chaojian Li, Kevin Zhao, Yingyan (Celine) Lin</li><li>éš¶å±æœºæ„ï¼šä½æ²»äºšç†å·¥å­¦é™¢</li><li>å…³é”®è¯ï¼šç¥ç»è¾å°„åœºã€3D é‡å»ºã€åœºæ™¯ç†è§£ã€3D æ¸²æŸ“ã€åœºæ™¯ç¼–è¾‘</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.11131</li><li>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœº (NeRF) åœ¨ 3D åº”ç”¨ä¸­å¤‡å—å…³æ³¨ï¼Œä½†ä¸åŒåº”ç”¨éœ€è¦ä¸åŒçš„ NeRF æ¨¡å‹ï¼Œå¯¼è‡´è®­ç»ƒå’Œå®éªŒç¹çã€‚(2) è¿‡å¾€æ–¹æ³•ï¼šç°æœ‰ NeRF æ¨¡å‹é’ˆå¯¹ç‰¹å®šä»»åŠ¡è®¾è®¡ï¼Œç¼ºä¹é€šç”¨æ€§å’Œé€‚åº”æ€§ã€‚(3) ç ”ç©¶æ–¹æ³•ï¼šæå‡º Omni-Recon æ¡†æ¶ï¼Œä½¿ç”¨åŸºäºå›¾åƒçš„æ¸²æŸ“ç®¡é“ï¼Œå°† 2D å›¾åƒç‰¹å¾æå‡åˆ° 3Dï¼Œå®ç°é€šç”¨ 3D é‡å»ºå’Œé›¶æ ·æœ¬å¤šä»»åŠ¡åœºæ™¯ç†è§£ã€‚(4) æ–¹æ³•æ€§èƒ½ï¼šOmni-Recon åœ¨é€šç”¨ 3D è¡¨é¢é‡å»ºä¸­è¾¾åˆ° SOTA è´¨é‡ï¼Œæ··åˆæƒé‡å¯åœ¨ä¸åŒä»»åŠ¡ä¸­å¤ç”¨ï¼Œå®ç°é›¶æ ·æœ¬å¤šä»»åŠ¡åœºæ™¯ç†è§£ï¼›è¿˜èƒ½æ”¯æŒå®æ—¶æ¸²æŸ“ã€é€šç”¨ 3D ç†è§£å’Œæ–‡æœ¬å¼•å¯¼çš„ 3D ç¼–è¾‘ã€‚</li></ol><p>7.æ–¹æ³•ï¼š(1) åŸºäºå›¾åƒçš„æ¸²æŸ“ç®¡é“ï¼šå°†2Då›¾åƒç‰¹å¾æå‡åˆ°3Dï¼Œå®ç°é€šç”¨3Dé‡å»ºã€‚(2) LoRAé€‚é…å™¨ï¼šå¾®è°ƒLoRAé€‚é…å™¨ï¼Œå®ç°é›¶æ ·æœ¬å¤šä»»åŠ¡åœºæ™¯ç†è§£ã€‚(3) å®æ—¶æ¸²æŸ“ï¼šå¾®è°ƒåœºæ™¯ç½‘æ ¼å’Œç€è‰²å™¨ï¼Œå®ç°å®æ—¶æ¸²æŸ“ã€‚</p><p>8.ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œæå‡ºäº†ä¸€ç§é€šç”¨ç¥ç»è¾å°„åœº Omni-Reconï¼Œå®ƒä½¿ç”¨åŸºäºå›¾åƒçš„æ¸²æŸ“ç®¡é“ï¼Œå°† 2D å›¾åƒç‰¹å¾æå‡åˆ° 3Dï¼Œå®ç°äº†é€šç”¨ 3D é‡å»ºå’Œé›¶æ ·æœ¬å¤šä»»åŠ¡åœºæ™¯ç†è§£ã€‚Omni-Recon åœ¨é€šç”¨ 3D è¡¨é¢é‡å»ºä¸­è¾¾åˆ° SOTA è´¨é‡ï¼Œæ··åˆæƒé‡å¯åœ¨ä¸åŒä»»åŠ¡ä¸­å¤ç”¨ï¼Œå®ç°é›¶æ ·æœ¬å¤šä»»åŠ¡åœºæ™¯ç†è§£ï¼›è¿˜èƒ½æ”¯æŒå®æ—¶æ¸²æŸ“ã€é€šç”¨ 3D ç†è§£å’Œæ–‡æœ¬å¼•å¯¼çš„ 3D ç¼–è¾‘ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäºå›¾åƒçš„æ¸²æŸ“ç®¡é“ï¼Œå°† 2D å›¾åƒç‰¹å¾æå‡åˆ° 3Dï¼Œå®ç°é€šç”¨ 3D é‡å»ºï¼›æå‡ºäº†ä¸€ç§ LoRA é€‚é…å™¨ï¼Œå®ç°é›¶æ ·æœ¬å¤šä»»åŠ¡åœºæ™¯ç†è§£ï¼›æå‡ºäº†ä¸€ç§å®æ—¶æ¸²æŸ“æ–¹æ³•ï¼Œå¾®è°ƒåœºæ™¯ç½‘æ ¼å’Œç€è‰²å™¨ï¼Œå®ç°å®æ—¶æ¸²æŸ“ã€‚æ€§èƒ½ï¼šåœ¨é€šç”¨ 3D è¡¨é¢é‡å»ºä¸­è¾¾åˆ° SOTA è´¨é‡ï¼Œæ··åˆæƒé‡å¯åœ¨ä¸åŒä»»åŠ¡ä¸­å¤ç”¨ï¼Œå®ç°é›¶æ ·æœ¬å¤šä»»åŠ¡åœºæ™¯ç†è§£ï¼›æ”¯æŒå®æ—¶æ¸²æŸ“ã€é€šç”¨ 3D ç†è§£å’Œæ–‡æœ¬å¼•å¯¼çš„ 3D ç¼–è¾‘ã€‚å·¥ä½œé‡ï¼šéœ€è¦å¤§é‡çš„æ•°æ®å’Œè®¡ç®—èµ„æºè¿›è¡Œè®­ç»ƒã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-264d110200ed1cf212d1bac9128b7d47.jpg" align="middle"><img src="https://pica.zhimg.com/v2-7c1833860c2bff8e192ef7f1a12d6cc2.jpg" align="middle"></details>## The NeRFect Match: Exploring NeRF Features for Visual Localization**Authors:Qunjie Zhou, Maxim Maximov, Or Litany, Laura Leal-TaixÃ©**In this work, we propose the use of Neural Radiance Fields (NeRF) as a scene representation for visual localization. Recently, NeRF has been employed to enhance pose regression and scene coordinate regression models by augmenting the training database, providing auxiliary supervision through rendered images, or serving as an iterative refinement module. We extend its recognized advantages -- its ability to provide a compact scene representation with realistic appearances and accurate geometry -- by exploring the potential of NeRF's internal features in establishing precise 2D-3D matches for localization. To this end, we conduct a comprehensive examination of NeRF's implicit knowledge, acquired through view synthesis, for matching under various conditions. This includes exploring different matching network architectures, extracting encoder features at multiple layers, and varying training configurations. Significantly, we introduce NeRFMatch, an advanced 2D-3D matching function that capitalizes on the internal knowledge of NeRF learned via view synthesis. Our evaluation of NeRFMatch on standard localization benchmarks, within a structure-based pipeline, sets a new state-of-the-art for localization performance on Cambridge Landmarks. [PDF](http://arxiv.org/abs/2403.09577v1) **Summary**NeRFçš„éšå¼ç‰¹å¾å¯ç”¨äºå»ºç«‹ç²¾ç¡®çš„2D-3DåŒ¹é…ï¼Œç”¨äºè§†è§‰å®šä½ã€‚**Key Takeaways*** NeRFå¯æä¾›ç´§å‡‘çš„åœºæ™¯è¡¨ç¤ºï¼Œå…·æœ‰é€¼çœŸçš„å¤–è§‚å’Œå‡†ç¡®çš„å‡ ä½•å½¢çŠ¶ã€‚* NeRFçš„å†…éƒ¨ç‰¹å¾é€šè¿‡è§†å›¾åˆæˆè·å¾—ï¼Œå¯ç”¨äºåŒ¹é…ã€‚* æ¢ç´¢äº†ä¸åŒåŒ¹é…ç½‘ç»œæ¶æ„ã€æå–å¤šå±‚ç¼–ç å™¨ç‰¹å¾å’Œæ”¹å˜è®­ç»ƒé…ç½®ã€‚* å¼•å…¥äº†NeRFMatchï¼Œä¸€ç§å…ˆè¿›çš„2D-3DåŒ¹é…å‡½æ•°ï¼Œåˆ©ç”¨NeRFé€šè¿‡è§†å›¾åˆæˆå­¦ä¹ åˆ°çš„å†…éƒ¨çŸ¥è¯†ã€‚* NeRFMatchåœ¨åŸºäºç»“æ„çš„ç®¡é“ä¸­ï¼Œåœ¨æ ‡å‡†å®šä½åŸºå‡†ä¸Šçš„è¯„ä¼°ç»“æœåˆ·æ–°äº†å‰‘æ¡¥åœ°æ ‡å®šä½æ€§èƒ½çš„æœ€æ–°è®°å½•ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>æ ‡é¢˜ï¼šç¥ç»è¾å°„åœºåœ¨è§†è§‰å®šä½ä¸­çš„å®Œç¾åŒ¹é…ï¼šæ¢ç´¢ç¥ç»è¾å°„åœºçš„ç‰¹å¾</li><li>ä½œè€…ï¼šQunjie Zhou, Maxim Maximov, Or Litany, Laura Leal-TaixÃ©</li><li>éš¶å±å…³ç³»ï¼šNVIDIA</li><li>å…³é”®è¯ï¼šè§†è§‰å®šä½ï¼Œç¥ç»è¾å°„åœºï¼Œ2D-3D åŒ¹é…ï¼Œç»“æ„åŒ–è¡¨ç¤º</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.09577   Github é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šè§†è§‰å®šä½æ˜¯ç¡®å®šæŸ¥è¯¢å›¾åƒç›¸å¯¹äº 3D ç¯å¢ƒçš„ç›¸æœºä½å§¿çš„ä»»åŠ¡ã€‚ç¥ç»è¾å°„åœº (NeRF) æ˜¯ä¸€ç§å¼ºå¤§çš„ 3D åœºæ™¯è¡¨ç¤ºï¼Œå…·æœ‰é«˜å¯è§£é‡Šæ€§ã€ç´§å‡‘æ€§å’Œç”Ÿæˆé€¼çœŸå¤–è§‚å’Œå‡†ç¡®å‡ ä½•çš„èƒ½åŠ›ã€‚   ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•ï¼šä¼ ç»Ÿçš„è§†è§‰å®šä½æ–¹æ³•ä¾èµ–äºæ˜¾å¼åœºæ™¯è¡¨ç¤ºï¼Œå¦‚ç‚¹äº‘æˆ– 3D ç½‘æ ¼ã€‚è¿™äº›æ–¹æ³•åœ¨å»ºç«‹ 2D-3D åŒ¹é…æ—¶å­˜åœ¨å±€é™æ€§ã€‚   ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºä½¿ç”¨ NeRF ä½œä¸ºè§†è§‰å®šä½çš„åœºæ™¯è¡¨ç¤ºã€‚é€šè¿‡æ¢ç´¢ NeRF å†…éƒ¨ç‰¹å¾åœ¨å»ºç«‹ç²¾ç¡® 2D-3D åŒ¹é…æ–¹é¢çš„æ½œåŠ›ï¼Œæ‰©å±•äº† NeRF çš„ä¼˜åŠ¿ã€‚æå‡ºäº† NeRFMatchï¼Œä¸€ç§é«˜çº§ 2D-3D åŒ¹é…å‡½æ•°ï¼Œåˆ©ç”¨äº† NeRF é€šè¿‡è§†å›¾åˆæˆå­¦ä¹ çš„å†…éƒ¨çŸ¥è¯†ã€‚   ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šåœ¨ç»“æ„åŒ–è¡¨ç¤ºç®¡é“ä¸­ï¼ŒNeRFMatch åœ¨æ ‡å‡†å®šä½åŸºå‡†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œåœ¨ Cambridge Landmarks ä¸Šåˆ›é€ äº†è§†è§‰å®šä½æ€§èƒ½çš„æ–°è®°å½•ã€‚è¿™äº›ç»“æœè¯æ˜äº† NeRF åœ¨è§†è§‰å®šä½ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æ”¯æŒäº†æœ¬æ–‡æå‡ºçš„æ–¹æ³•ã€‚</li></ol><p>7.æ–¹æ³•ï¼šï¼ˆ1ï¼‰NeRFç‰¹å¾æ¶ˆèå®éªŒï¼šæ¢ç´¢ä¸åŒNeRFç‰¹å¾åœ¨2D-3DåŒ¹é…ä¸­çš„æ½œåŠ›ï¼ŒåŒ…æ‹¬åŸå§‹3Dç‚¹åæ ‡ã€ä½ç½®ç¼–ç çš„3Dç‚¹å’ŒNeRFä¸­é—´å±‚ç‰¹å¾ã€‚ï¼ˆ2ï¼‰NeRFMatchæ¶ˆèå®éªŒï¼šç ”ç©¶ä¸åŒå›¾åƒéª¨å¹²ç½‘ç»œå’ŒåŒ¹é…å‡½æ•°å¯¹åŒ¹é…æ¨¡å‹çš„å½±å“ï¼ŒåŒ…æ‹¬ResNet34ã€ConvFormerã€å·ç§¯åŒ¹é…å™¨å’Œæ³¨æ„åŠ›åŒ¹é…å™¨ã€‚ï¼ˆ3ï¼‰è®­ç»ƒæ¶ˆèå®éªŒï¼šæ¯”è¾ƒé’ˆå¯¹æ¯ä¸ªåœºæ™¯è®­ç»ƒå’Œé’ˆå¯¹å¤šä¸ªåœºæ™¯è®­ç»ƒçš„NeRFMatchæ¨¡å‹çš„æ€§èƒ½ï¼Œä»¥åŠä½¿ç”¨ImageNeté¢„è®­ç»ƒå›¾åƒéª¨å¹²ç½‘ç»œçš„å½±å“ã€‚ï¼ˆ4ï¼‰å§¿æ€ä¼˜åŒ–å®éªŒï¼šæ¢ç´¢è¿­ä»£å’Œä¼˜åŒ–ä¸¤ç§å§¿æ€ä¼˜åŒ–æ–¹æ³•ï¼Œä»¥è¿›ä¸€æ­¥æé«˜å§¿æ€ç²¾åº¦ï¼Œå¹¶è¯„ä¼°ä¸åŒNeRFMatchæ¨¡å‹å’Œè®­ç»ƒè®¾ç½®çš„ä¼˜åŒ–æ•ˆæœã€‚</p><p><strong>8. ç»“è®º</strong>(1): æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰çš„è§†è§‰å®šä½æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨äº† NeRF çš„å†…éƒ¨ç‰¹å¾ï¼Œåœ¨å»ºç«‹ç²¾ç¡®çš„ 2D-3D åŒ¹é…æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚æå‡ºçš„ NeRFMatch æ¨¡å‹åœ¨æ ‡å‡†å®šä½åŸºå‡†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¯æ˜äº† NeRF åœ¨è§†è§‰å®šä½ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p><p>(2): <strong>åˆ›æ–°ç‚¹ï¼š</strong>* åˆ©ç”¨ NeRF çš„å†…éƒ¨ç‰¹å¾è¿›è¡Œ 2D-3D åŒ¹é…ï¼Œæ¢ç´¢äº† NeRF åœ¨è§†è§‰å®šä½ä¸­çš„æ–°æ½œåŠ›ã€‚* æå‡ºäº†ä¸€ç§é«˜çº§ 2D-3D åŒ¹é…å‡½æ•° NeRFMatchï¼Œåˆ©ç”¨äº† NeRF é€šè¿‡è§†å›¾åˆæˆå­¦ä¹ çš„å†…éƒ¨çŸ¥è¯†ã€‚</p><p><strong>æ€§èƒ½ï¼š</strong>* åœ¨ Cambridge Landmarks åŸºå‡†ä¸Šåˆ›é€ äº†è§†è§‰å®šä½æ€§èƒ½çš„æ–°è®°å½•ã€‚* åœ¨å„ç§åœºæ™¯å’Œè®­ç»ƒè®¾ç½®ä¸‹è¡¨ç°å‡ºé²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p><p><strong>å·¥ä½œé‡ï¼š</strong>* éœ€è¦é’ˆå¯¹æ¯ä¸ªåœºæ™¯è®­ç»ƒ NeRFï¼Œè¿™å¯èƒ½éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºå’Œæ—¶é—´ã€‚* NeRFMatch æ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†éœ€è¦å¤§é‡çš„å†…å­˜å’Œè®¡ç®—èƒ½åŠ›ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-3cd8ba580831022c4f675064d1098186.jpg" align="middle"><img src="https://pica.zhimg.com/v2-8213b16ccc45bbcd6a6f3465f9ed99c5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6ec50b8d2fa9ffdc32797b6db3683bcd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-aefcfa5ab2e39bdb2d87786b5cdb12fa.jpg" align="middle"></details>## PreSight: Enhancing Autonomous Vehicle Perception with City-Scale NeRF   Priors**Authors:Tianyuan Yuan, Yucheng Mao, Jiawei Yang, Yicheng Liu, Yue Wang, Hang Zhao**Autonomous vehicles rely extensively on perception systems to navigate and interpret their surroundings. Despite significant advancements in these systems recently, challenges persist under conditions like occlusion, extreme lighting, or in unfamiliar urban areas. Unlike these systems, humans do not solely depend on immediate observations to perceive the environment. In navigating new cities, humans gradually develop a preliminary mental map to supplement real-time perception during subsequent visits. Inspired by this human approach, we introduce a novel framework, Pre-Sight, that leverages past traversals to construct static prior memories, enhancing online perception in later navigations. Our method involves optimizing a city-scale neural radiance field with data from previous journeys to generate neural priors. These priors, rich in semantic and geometric details, are derived without manual annotations and can seamlessly augment various state-of-the-art perception models, improving their efficacy with minimal additional computational cost. Experimental results on the nuScenes dataset demonstrate the framework's high compatibility with diverse online perception models. Specifically, it shows remarkable improvements in HD-map construction and occupancy prediction tasks, highlighting its potential as a new perception framework for autonomous driving systems. Our code will be released at https://github.com/yuantianyuan01/PreSight. [PDF](http://arxiv.org/abs/2403.09079v1) **Summary**é¢„è§æ¡†æ¶ä»¥äººç±»å¯¼èˆªä¸ºå¯å‘ï¼Œåˆ©ç”¨è¿‡å»éå†æ„å»ºé™æ€å…ˆéªŒè®°å¿†ï¼Œå¢å¼ºåœ¨çº¿æ„ŸçŸ¥ï¼Œæé«˜åŸå¸‚å°ºåº¦ç¥ç»è¾å°„åœºçš„æ€§èƒ½ï¼Œæå‡è‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥ç³»ç»Ÿçš„æ•ˆç‡ã€‚**Key Takeaways**- å—äººç±»å¯¼èˆªæ–¹å¼å¯å‘ï¼Œæå‡ºé¢„è§æ¡†æ¶ï¼Œåˆ©ç”¨è¿‡å»éå†æ„å»ºé™æ€å…ˆéªŒè®°å¿†ï¼Œå¢å¼ºåœ¨çº¿æ„ŸçŸ¥ã€‚- ä¼˜åŒ–åŸå¸‚å°ºåº¦ç¥ç»è¾å°„åœºï¼Œåˆ©ç”¨å…ˆå‰çš„æ—…ç¨‹æ•°æ®ç”Ÿæˆç¥ç»å…ˆéªŒã€‚- ç¥ç»å…ˆéªŒåŒ…å«ä¸°å¯Œçš„è¯­ä¹‰å’Œå‡ ä½•ç»†èŠ‚ï¼Œæ— éœ€äººå·¥æ ‡æ³¨ï¼Œå¯æ— ç¼å¢å¼ºå„ç§æœ€å…ˆè¿›çš„æ„ŸçŸ¥æ¨¡å‹ã€‚- é¢„è§æ¡†æ¶ä¸å¤šç§åœ¨çº¿æ„ŸçŸ¥æ¨¡å‹å…¼å®¹æ€§é«˜ã€‚- åœ¨nuScenesæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨é«˜æ¸…åœ°å›¾æ„å»ºå’Œå ç”¨é¢„æµ‹ä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†æ€§èƒ½ã€‚- é¢„è§æ¡†æ¶æœ‰æœ›æˆä¸ºè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„æ–°æ„ŸçŸ¥æ¡†æ¶ã€‚- ä»£ç å°†åœ¨ https://github.com/yuantianyuan01/PreSight å‘å¸ƒã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>æ ‡é¢˜ï¼šPreSightï¼šåˆ©ç”¨åŸå¸‚è§„æ¨¡ NeRF å…ˆéªŒå¢å¼ºè‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥</li><li>ä½œè€…ï¼šTianyuan Yuan, Yucheng Mao, Jiawei Yang, Yicheng Liu, Yue Wang, Hang Zhao</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šæ¸…åå¤§å­¦</li><li>å…³é”®è¯ï¼šè‡ªåŠ¨é©¾é©¶ã€åŸºäºè§†è§‰çš„æ„ŸçŸ¥ã€ç¥ç»éšå¼åœº</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.09079   Github ä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šè‡ªåŠ¨é©¾é©¶æ±½è½¦ä¸¥é‡ä¾èµ–æ„ŸçŸ¥ç³»ç»Ÿæ¥å¯¼èˆªå’Œè§£é‡Šå‘¨å›´ç¯å¢ƒã€‚å°½ç®¡è¿™äº›ç³»ç»Ÿæœ€è¿‘å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†åœ¨é®æŒ¡ã€æç«¯å…‰ç…§æˆ–ä¸ç†Ÿæ‚‰çš„åŸå¸‚åŒºåŸŸç­‰æ¡ä»¶ä¸‹ä»ç„¶å­˜åœ¨æŒ‘æˆ˜ã€‚ä¸è¿™äº›ç³»ç»Ÿä¸åŒï¼Œäººç±»å¹¶ä¸å®Œå…¨ä¾èµ–å³æ—¶è§‚å¯Ÿæ¥æ„ŸçŸ¥ç¯å¢ƒã€‚åœ¨æ¢ç´¢æ–°åŸå¸‚æ—¶ï¼Œäººç±»ä¼šé€æ¸å½¢æˆä¸€ä¸ªåˆæ­¥çš„å¿ƒç†åœ°å›¾ï¼Œä»¥è¡¥å……åç»­è®¿é—®æœŸé—´çš„å®æ—¶æ„ŸçŸ¥ã€‚   ï¼ˆ2ï¼‰ï¼šè¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šæœ¬æ–‡çš„åŠ¨æœºå¾ˆå¥½ï¼Œå—äººç±»æ–¹æ³•çš„å¯å‘ï¼Œæå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ PreSightï¼Œè¯¥æ¡†æ¶åˆ©ç”¨è¿‡å»çš„éå†æ¥æ„å»ºé™æ€å…ˆéªŒè®°å¿†ï¼Œä»è€Œå¢å¼ºåç»­å¯¼èˆªä¸­çš„åœ¨çº¿æ„ŸçŸ¥ã€‚   ï¼ˆ3ï¼‰ï¼šæå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šè¯¥æ–¹æ³•æ¶‰åŠä½¿ç”¨æ¥è‡ªå…ˆå‰æ—…ç¨‹çš„æ•°æ®ä¼˜åŒ–åŸå¸‚è§„æ¨¡ç¥ç»è¾å°„åœºä»¥ç”Ÿæˆç¥ç»å…ˆéªŒã€‚è¿™äº›å…ˆéªŒä¸°å¯Œäº†è¯­ä¹‰å’Œå‡ ä½•ç»†èŠ‚ï¼Œæ— éœ€äººå·¥æ³¨é‡Šï¼Œå¹¶ä¸”å¯ä»¥æ— ç¼å¢å¼ºå„ç§æœ€å…ˆè¿›çš„æ„ŸçŸ¥æ¨¡å‹ï¼Œä»¥æœ€å°çš„é¢å¤–è®¡ç®—æˆæœ¬æé«˜å…¶åŠŸæ•ˆã€‚   ï¼ˆ4ï¼‰ï¼šæ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šåœ¨ nuScenes æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶ä¸å„ç§åœ¨çº¿æ„ŸçŸ¥æ¨¡å‹é«˜åº¦å…¼å®¹ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒåœ¨ HD åœ°å›¾æ„å»ºå’Œå ç”¨é¢„æµ‹ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºæ˜¾ç€çš„æ”¹è¿›ï¼Œçªå‡ºäº†å…¶ä½œä¸ºè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„æ–°æ„ŸçŸ¥æ¡†æ¶çš„æ½œåŠ›ã€‚</li></ol><p>7.æ–¹æ³•ï¼š(1): åˆ©ç”¨åŸå¸‚è§„æ¨¡çš„ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æ¥ç”Ÿæˆç¥ç»å…ˆéªŒï¼Œä¸°å¯Œè¯­ä¹‰å’Œå‡ ä½•ç»†èŠ‚ï¼›(2): å°†ç¥ç»å…ˆéªŒæ— ç¼å¢å¼ºåˆ°å„ç§æœ€å…ˆè¿›çš„æ„ŸçŸ¥æ¨¡å‹ä¸­ï¼Œæé«˜å…¶åŠŸæ•ˆï¼›(3): åœ¨ HD åœ°å›¾æ„å»ºå’Œå ç”¨é¢„æµ‹ä»»åŠ¡ä¸­éªŒè¯äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†å…¶ä½œä¸ºè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„æ–°æ„ŸçŸ¥æ¡†æ¶çš„æ½œåŠ›ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬å·¥ä½œåˆ©ç”¨åŸå¸‚è§„æ¨¡ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ç”Ÿæˆç¥ç»å…ˆéªŒï¼Œæ— ç¼å¢å¼ºåˆ°å„ç§æœ€å…ˆè¿›çš„æ„ŸçŸ¥æ¨¡å‹ä¸­ï¼Œæé«˜å…¶åŠŸæ•ˆï¼Œåœ¨HDåœ°å›¾æ„å»ºå’Œå ç”¨é¢„æµ‹ä»»åŠ¡ä¸­éªŒè¯äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†å…¶ä½œä¸ºè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„æ–°æ„ŸçŸ¥æ¡†æ¶çš„æ½œåŠ›ã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼šæå‡º PreSight æ¡†æ¶ï¼Œåˆ©ç”¨åŸå¸‚è§„æ¨¡ NeRF æ„å»ºé™æ€å…ˆéªŒï¼Œå¢å¼ºåœ¨çº¿æ„ŸçŸ¥ï¼›æ€§èƒ½ï¼šåœ¨ nuScenes æ•°æ®é›†ä¸ŠéªŒè¯äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§å’Œå¹¿æ³›é€‚ç”¨æ€§ï¼›å·¥ä½œé‡ï¼šéœ€è¦å‡†ç¡®çš„è½¦èº«ä½å§¿å’Œæ‘„åƒå¤´ä¼ æ„Ÿå™¨ï¼Œå¯èƒ½æ— æ³•åœ¨ä¼—åŒ…æ•°æ®ä¸­è·å¾—ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-6e89a00394046d5fd38373e9130ab120.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e10f4a3c19b9cce44b6cd16bfb60eeee.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a02818f4812a5d830dd0c4a4365984cc.jpg" align="middle"></details><h2 id="NeRF-Supervised-Feature-Point-Detection-and-Description"><a href="#NeRF-Supervised-Feature-Point-Detection-and-Description" class="headerlink" title="NeRF-Supervised Feature Point Detection and Description"></a>NeRF-Supervised Feature Point Detection and Description</h2><p><strong>Authors:Ali Youssef, Francisco Vasconcelos</strong></p><p>Feature point detection and description is the backbone for various computer vision applications, such as Structure-from-Motion, visual SLAM, and visual place recognition. While learning-based methods have surpassed traditional handcrafted techniques, their training often relies on simplistic homography-based simulations of multi-view perspectives, limiting model generalisability. This paper introduces a novel approach leveraging neural radiance fields (NeRFs) for realistic multi-view training data generation. We create a diverse multi-view dataset using NeRFs, consisting of indoor and outdoor scenes. Our proposed methodology adapts state-of-the-art feature detectors and descriptors to train on NeRF-synthesised views supervised by perspective projective geometry. Our experiments demonstrate that the proposed methods achieve competitive or superior performance on standard benchmarks for relative pose estimation, point cloud registration, and homography estimation while requiring significantly less training data compared to existing approaches. </p><p><a href="http://arxiv.org/abs/2403.08156v1">PDF</a> </p><p><strong>Summary</strong><br>ç¥ç»è¾å°„åœº (NeRF) ç”¨äºç”Ÿæˆé€¼çœŸçš„å¤šè§†å›¾è®­ç»ƒæ•°æ®ï¼Œä»è€Œæé«˜ç‰¹å¾ç‚¹æ£€æµ‹å’Œæè¿°çš„å‡†ç¡®æ€§ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºäº†ä¸€ç§ä½¿ç”¨ NeRFs ç”Ÿæˆé€¼çœŸå¤šè§†å›¾è®­ç»ƒæ•°æ®çš„åˆ›æ–°æ–¹æ³•ã€‚</li><li>è®­ç»ƒç‰¹å¾æ£€æµ‹å™¨å’Œæè¿°ç¬¦ä»¥ NeRF åˆæˆè§†å›¾ä¸ºç›‘ç£ï¼Œå¹¶é‡‡ç”¨é€è§†æŠ•å½±å‡ ä½•ã€‚</li><li>è¯¥æ–¹æ³•åœ¨æ ‡å‡†ç›¸å¯¹ä½å§¿ä¼°è®¡ã€ç‚¹äº‘æ³¨å†Œå’Œå•åº”æ€§ä¼°è®¡åŸºå‡†ä¸Šå®ç°äº†ç«äº‰æˆ–æ›´ä¼˜çš„æ€§èƒ½ã€‚</li><li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œéœ€è¦çš„è®­ç»ƒæ•°æ®æ˜¾ç€å‡å°‘ã€‚</li><li>å¤šæ ·åŒ–å¤šè§†å›¾æ•°æ®é›†åŒ…æ‹¬å®¤å†…å’Œå®¤å¤–åœºæ™¯ã€‚</li><li>è¯¥æ–¹æ³•ä½¿ç”¨ NeRFs è®­ç»ƒï¼Œå…·æœ‰æ³›åŒ–èƒ½åŠ›ï¼Œå¯ä»¥å¤„ç†å„ç§è§†è§’ã€‚</li><li>è¯¥æ–¹æ³•ä¸ºè§†è§‰ SLAM å’Œè§†è§‰ä½ç½®è¯†åˆ«ç­‰è®¡ç®—æœºè§†è§‰åº”ç”¨æä¾›äº†æ”¹è¿›çš„ç‰¹å¾ç‚¹æ£€æµ‹å’Œæè¿°ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>æ ‡é¢˜ï¼šç¥ç»è¾å°„åœºè¾…åŠ©ç‰¹å¾ç‚¹æ£€æµ‹ä¸æè¿°</li><p></p><p></p><li>ä½œè€…ï¼šAli Youssefï¼ŒFrancisco Vasconcelos</li><p></p><p></p><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šä¼¦æ•¦å¤§å­¦å­¦é™¢è®¡ç®—æœºç§‘å­¦ç³»</li><p></p><p></p><li>å…³é”®è¯ï¼šç‰¹å¾æ£€æµ‹ä¸æè¿°ã€ç¥ç»è¾å°„åœºã€æ•°æ®é›†</li><p></p><p></p><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.08156   Github ä»£ç é“¾æ¥ï¼šæ— </li><p></p><p></p><li><p></p><p>æ‘˜è¦ï¼š   (1) ç ”ç©¶èƒŒæ™¯ï¼šç‰¹å¾ç‚¹æ£€æµ‹ä¸æè¿°æ˜¯è®¡ç®—æœºè§†è§‰ä¸­è®¸å¤šå¤šè§†å›¾é—®é¢˜ï¼ˆå¦‚è¿åŠ¨ç»“æ„ã€è§†è§‰ SLAM å’Œè§†è§‰å®šä½è¯†åˆ«ï¼‰çš„åŸºç¡€ã€‚è¿‘å¹´æ¥ï¼ŒåŸºäºå­¦ä¹ çš„æ–¹æ³•å·²å–ä»£æ‰‹å·¥åˆ¶ä½œæŠ€æœ¯ï¼Œä½†å…¶è®­ç»ƒé€šå¸¸ä¾èµ–äºåŸºäºä»¿å°„å˜æ¢çš„ç®€å•å¤šè§†å›¾è§†è§’æ¨¡æ‹Ÿï¼Œè¿™é™åˆ¶äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚   (2) è¿‡å»çš„æ–¹æ³•ï¼šè¿‡å»çš„æ–¹æ³•ä½¿ç”¨åŸºäºä»¿å°„å˜æ¢çš„å›¾åƒæ‰­æ›²æ¥æ¨¡æ‹Ÿä¸åŒè§†è§’ï¼Œä½†è¿™ç§æ‰­æ›²è¿‡äºç®€å•ï¼Œæ— æ³•å‡†ç¡®æ¨¡æ‹Ÿå¤šè§†å›¾é€è§†ã€‚   (3) æœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨ç¥ç»è¾å°„åœº (NeRF) ç”Ÿæˆé€¼çœŸå¤šè§†å›¾è®­ç»ƒæ•°æ®çš„æ–°æ–¹æ³•ã€‚ç ”ç©¶è€…åˆ›å»ºäº†ä¸€ä¸ªä½¿ç”¨ NeRF åˆæˆçš„å¤šè§†å›¾æ•°æ®é›†ï¼ŒåŒ…å«å®¤å†…å’Œå®¤å¤–åœºæ™¯ã€‚ç ”ç©¶è€…è¿˜æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œå°†æœ€å…ˆè¿›çš„ç‰¹å¾æ£€æµ‹å™¨å’Œæè¿°å­è°ƒæ•´ä¸ºåœ¨ NeRF åˆæˆçš„è§†å›¾ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶ç”±é€è§†æŠ•å½±å‡ ä½•è¿›è¡Œç›‘ç£ã€‚   (4) å®éªŒç»“æœï¼šå®éªŒè¡¨æ˜ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæœ¬æ–‡æ–¹æ³•åœ¨ç›¸å¯¹ä½å§¿ä¼°è®¡ã€ç‚¹äº‘é…å‡†å’Œä»¿å°„å˜æ¢ä¼°è®¡çš„æ ‡å‡†åŸºå‡†ä¸Šå–å¾—äº†æœ‰ç«äº‰åŠ›æˆ–æ›´å¥½çš„æ€§èƒ½ï¼ŒåŒæ—¶æ‰€éœ€è®­ç»ƒæ•°æ®æ˜æ˜¾æ›´å°‘ã€‚</p></li><li><p>Methods:(1): ä½¿ç”¨ç¥ç»è¾å°„åœº (NeRF) ç”Ÿæˆé€¼çœŸçš„å¤šè§†å›¾è®­ç»ƒæ•°æ®ï¼›(2): æå‡ºä¸€ç§åŸºäºé€è§†æŠ•å½±å‡ ä½•ç›‘ç£çš„ NeRF ç‚¹é‡æŠ•å½±æ–¹æ³•ï¼›(3): è°ƒæ•´æœ€å…ˆè¿›çš„ç‰¹å¾æ£€æµ‹å™¨å’Œæè¿°ç¬¦ï¼Œä½¿å…¶åœ¨ NeRF åˆæˆçš„è§†å›¾ä¸Šè¿›è¡Œè®­ç»ƒã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•æ¥ç›‘ç£åŸºäºå­¦ä¹ çš„ç‰¹å¾ç‚¹æ£€æµ‹å™¨å’Œæè¿°ç¬¦ï¼Œåˆ©ç”¨åˆæˆ NeRF æ•°æ®ä¸Šçš„é€è§†æŠ•å½±å‡ ä½•ã€‚å°½ç®¡æˆ‘ä»¬æå‡ºçš„æ•°æ®é›†å®Œå…¨ç”±åˆæˆå›¾åƒè€Œä¸æ˜¯çœŸå®çš„ RGB å›¾åƒç»„æˆï¼Œå¹¶ä¸”æ¯”å¤§å‹å¼€æºæ•°æ®é›†å°å¾—å¤šï¼Œä½†ç»“æœè¡¨æ˜ï¼Œåœ¨æ³›åŒ–èƒ½åŠ›æˆ–ç‰¹å¾ç‚¹æ£€æµ‹è´¨é‡æ–¹é¢æ²¡æœ‰è§‚å¯Ÿåˆ°ä¸‹é™ã€‚æ­£å¦‚é¢„æœŸçš„é‚£æ ·ï¼Œæˆ‘ä»¬çš„æ¨¡å‹é€šå¸¸åœ¨å…·æœ‰é«˜åº¦éå¹³é¢åœºæ™¯çš„å¤šè§†å›¾åŸºå‡†ä¸Šä¼˜äºç»è¿‡å•åº”æ€§è®­ç»ƒçš„åŸºçº¿ï¼Œè€Œåœ¨å•åº”æ€§ä¼°è®¡åŸºå‡†ä¸Šç•¥é€Šä¸€ç­¹ã€‚è¿›ä¸€æ­¥å‘å±•çš„æ›´å¤§æ½œåŠ›åœ¨äºæé«˜ç¥ç»æ¸²æŸ“çš„è®­ç»ƒæ•°æ®è´¨é‡ï¼Œç¥ç»æ¸²æŸ“å¯ä»¥ç”Ÿæˆæ›´é«˜è´¨é‡çš„åˆæˆå›¾åƒï¼Œæ²¡æœ‰äººå·¥åˆ¶å“ï¼Œæœ€é‡è¦çš„æ˜¯æ›´ç²¾ç¡®çš„æ·±åº¦å›¾ä»¥é¿å…é”™è¯¯æŠ•å½±ã€‚</p></li></ol><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šåˆ©ç”¨ NeRF åˆæˆçš„é€¼çœŸå¤šè§†å›¾æ•°æ®è®­ç»ƒç‰¹å¾æ£€æµ‹å™¨å’Œæè¿°ç¬¦ï¼›æå‡ºäº†ä¸€ç§åŸºäºé€è§†æŠ•å½±å‡ ä½•ç›‘ç£çš„ NeRF ç‚¹é‡æŠ•å½±æ–¹æ³•ã€‚</p><p>æ€§èƒ½ï¼šä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨ç›¸å¯¹ä½å§¿ä¼°è®¡ã€ç‚¹äº‘é…å‡†å’Œä»¿å°„å˜æ¢ä¼°è®¡çš„æ ‡å‡†åŸºå‡†ä¸Šå–å¾—äº†æœ‰ç«äº‰åŠ›æˆ–æ›´å¥½çš„æ€§èƒ½ï¼ŒåŒæ—¶æ‰€éœ€è®­ç»ƒæ•°æ®æ˜æ˜¾æ›´å°‘ã€‚</p><p>å·¥ä½œé‡ï¼šæ•°æ®é›†åˆæˆå’Œæ¨¡å‹è®­ç»ƒçš„å·¥ä½œé‡ä¸­ç­‰ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-44aa82812a0f884c826b881fd8f38e44.jpg" align="middle"><img src="https://picx.zhimg.com/v2-661d97273d7fdccb785af810b9b662b8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-105725399243a9c4608e1b49743e23c7.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d29f42ad3850aa4729795f0e7e52bfe4.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-03-23  CombiNeRF A Combination of Regularization Techniques for Few-Shot   Neural Radiance Field View Synthesis</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/03/23/Paper/2024-03-23/3DGS/"/>
    <id>https://kedreamix.github.io/2024/03/23/Paper/2024-03-23/3DGS/</id>
    <published>2024-03-23T10:15:27.000Z</published>
    <updated>2024-03-23T10:15:27.845Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-03-23-æ›´æ–°"><a href="#2024-03-23-æ›´æ–°" class="headerlink" title="2024-03-23 æ›´æ–°"></a>2024-03-23 æ›´æ–°</h1><h2 id="MVSplat-Efficient-3D-Gaussian-Splatting-from-Sparse-Multi-View-Images"><a href="#MVSplat-Efficient-3D-Gaussian-Splatting-from-Sparse-Multi-View-Images" class="headerlink" title="MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images"></a>MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images</h2><p><strong>Authors:Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, Jianfei Cai</strong></p><p>We propose MVSplat, an efficient feed-forward 3D Gaussian Splatting model learned from sparse multi-view images. To accurately localize the Gaussian centers, we propose to build a cost volume representation via plane sweeping in the 3D space, where the cross-view feature similarities stored in the cost volume can provide valuable geometry cues to the estimation of depth. We learn the Gaussian primitivesâ€™ opacities, covariances, and spherical harmonics coefficients jointly with the Gaussian centers while only relying on photometric supervision. We demonstrate the importance of the cost volume representation in learning feed-forward Gaussian Splatting models via extensive experimental evaluations. On the large-scale RealEstate10K and ACID benchmarks, our model achieves state-of-the-art performance with the fastest feed-forward inference speed (22 fps). Compared to the latest state-of-the-art method pixelSplat, our model uses $10\times $ fewer parameters and infers more than $2\times$ faster while providing higher appearance and geometry quality as well as better cross-dataset generalization. </p><p><a href="http://arxiv.org/abs/2403.14627v1">PDF</a> Project page: <a href="https://donydchen.github.io/mvsplat">https://donydchen.github.io/mvsplat</a> Code:   <a href="https://github.com/donydchen/mvsplat">https://github.com/donydchen/mvsplat</a></p><p><strong>Summary</strong><br>MVSplat æ¨¡å‹é€šè¿‡åˆ©ç”¨ç¨€ç–å¤šè§†è§’å›¾åƒï¼Œç»“åˆé«˜æ•ˆçš„é€è§†æŠ•å½± 3D é«˜æ–¯ Splatting ç»„ä»¶ï¼Œå®ç°é«˜æ•ˆçš„å‰å‘ 3D é‡å»ºã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡º MVSplat æ¨¡å‹ï¼Œå°† 3D é«˜æ–¯ Splatting ä¸ç¨€ç–å¤šè§†è§’å›¾åƒç›¸ç»“åˆï¼Œè¿›è¡Œé«˜æ•ˆçš„å‰å‘ 3D é‡å»ºã€‚</li><li>é€šè¿‡å¹³é¢æ‰«ææ„å»ºä»£ä»·ä½“è¡¨ç¤ºï¼Œåˆ©ç”¨ä»£ä»·ä½“ä¸­çš„è·¨è§†å›¾ç‰¹å¾ç›¸ä¼¼æ€§ï¼Œä¸ºæ·±åº¦ä¼°è®¡æä¾›å‡ ä½•çº¿ç´¢ã€‚</li><li>è”åˆå­¦ä¹ é«˜æ–¯åŸè¯­çš„ä¸é€æ˜åº¦ã€åæ–¹å·®å’Œçƒè°ç³»æ•°ï¼Œä»…ä¾èµ–äºå…‰åº¦ç›‘ç£ã€‚</li><li>è¯æ˜ä»£ä»·ä½“è¡¨ç¤ºå¯¹å­¦ä¹ å‰å‘é«˜æ–¯ Splatting æ¨¡å‹çš„é‡è¦æ€§ã€‚</li><li>åœ¨ RealEstate10K å’Œ ACID åŸºå‡†ä¸Šï¼Œè¯¥æ¨¡å‹å®ç° SOTA æ€§èƒ½ï¼Œä¸”å…·æœ‰æœ€å¿«çš„æ¨ç†é€Ÿåº¦ï¼ˆ22 fpsï¼‰ã€‚</li><li>ä¸ pixelSplat ç›¸æ¯”ï¼Œè¯¥æ¨¡å‹å‚æ•°é‡å‡å°‘ $10\times$ï¼Œæ¨ç†é€Ÿåº¦æé«˜ $2\times$ ä»¥ä¸Šï¼ŒåŒæ—¶æä¾›æ›´é«˜çš„å¤–è§‚å’Œå‡ ä½•è´¨é‡ï¼Œä»¥åŠæ›´å¥½çš„è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šMVSplatï¼šåŸºäºç¨€ç–å¤šè§†å›¾å›¾åƒçš„é«˜æ•ˆä¸‰ç»´é«˜æ–¯ Splatting</li><li>ä½œè€…ï¼šYuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, Jianfei Cai</li><li>å•ä½ï¼šè«çº³ä»€å¤§å­¦</li><li>å…³é”®è¯ï¼šç‰¹å¾åŒ¹é…ã€ä»£ä»·ä½“ç§¯ã€é«˜æ–¯ Splatting</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://donydchen.github.io/mvsplat   Github é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šä¸‰ç»´åœºæ™¯é‡å»ºå’Œæ–°è§†è§’åˆæˆä»æåº¦ç¨€ç–çš„å›¾åƒï¼ˆä¾‹å¦‚ï¼Œå°‘è‡³ä¸¤å¼ ï¼‰ä¸­æå‡ºè®¡ç®—æœºè§†è§‰ä¸­çš„åŸºæœ¬æŒ‘æˆ˜ã€‚è™½ç„¶åŸºäºå¤šè§†å›¾å‡ ä½•çš„ä¼ ç»Ÿæ–¹æ³•å–å¾—äº†æ˜¾ç€è¿›å±•ï¼Œä½†å®ƒä»¬é€šå¸¸éœ€è¦å¤§é‡çš„å›¾åƒä½œä¸ºè¾“å…¥ï¼Œè¿™åœ¨è®¸å¤šå®é™…åœºæ™¯ä¸­æ˜¯ä¸å¯è¡Œçš„ã€‚æœ€è¿‘ï¼ŒåŸºäºæ·±åº¦å­¦ä¹ çš„ Splatting æ–¹æ³•å·²ç»æ˜¾ç¤ºå‡ºä»ç¨€ç–å›¾åƒä¸­é‡å»ºä¸‰ç»´åœºæ™¯çš„å·¨å¤§æ½œåŠ›ã€‚ï¼ˆ2ï¼‰ï¼šè¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç°æœ‰çš„ Splatting æ–¹æ³•é€šå¸¸ä¾èµ–äºæ‰‹å·¥åˆ¶ä½œçš„ splatting åŸè¯­ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„å»ºæ¨¡èƒ½åŠ›å’Œæ³›åŒ–æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå®ƒä»¬é€šå¸¸éœ€è¦è¿­ä»£ä¼˜åŒ–è¿‡ç¨‹ï¼Œè¿™ä½¿å¾—å®ƒä»¬åœ¨æ¨ç†é€Ÿåº¦æ–¹é¢å—åˆ°é™åˆ¶ã€‚ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„é«˜æ•ˆå‰é¦ˆä¸‰ç»´é«˜æ–¯ Splatting æ¨¡å‹ MVSplatï¼Œè¯¥æ¨¡å‹ä»ç¨€ç–å¤šè§†å›¾å›¾åƒä¸­å­¦ä¹ ã€‚ä¸ºäº†å‡†ç¡®å®šä½é«˜æ–¯ä¸­å¿ƒï¼Œæœ¬æ–‡æå‡ºé€šè¿‡åœ¨ä¸‰ç»´ç©ºé—´ä¸­è¿›è¡Œå¹³é¢æ‰«ææ„å»ºä»£ä»·ä½“ç§¯è¡¨ç¤ºï¼Œå…¶ä¸­å­˜å‚¨åœ¨ä»£ä»·ä½“ç§¯ä¸­çš„è·¨è§†å›¾ç‰¹å¾ç›¸ä¼¼æ€§å¯ä»¥ä¸ºæ·±åº¦ä¼°è®¡æä¾›æœ‰ä»·å€¼çš„å‡ ä½•çº¿ç´¢ã€‚æœ¬æ–‡ä»…ä¾é å…‰åº¦ç›‘ç£ï¼Œè”åˆå­¦ä¹ é«˜æ–¯åŸè¯­çš„ä¸é€æ˜åº¦ã€åæ–¹å·®å’Œçƒè°ç³»æ•°ä»¥åŠé«˜æ–¯ä¸­å¿ƒã€‚æœ¬æ–‡é€šè¿‡å¹¿æ³›çš„å®éªŒè¯„ä¼°è¯æ˜äº†ä»£ä»·ä½“ç§¯è¡¨ç¤ºåœ¨å­¦ä¹ å‰é¦ˆé«˜æ–¯ Splatting æ¨¡å‹ä¸­çš„é‡è¦æ€§ã€‚ï¼ˆ4ï¼‰ï¼šæ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†ä»€ä¹ˆæ€§èƒ½ï¼šåœ¨å¤§è§„æ¨¡ RealEstate10K å’Œ ACID åŸºå‡†ä¸Šï¼Œæœ¬æ–‡æ¨¡å‹ä»¥æœ€å¿«çš„é¦ˆé€å‰å‘æ¨ç†é€Ÿåº¦ï¼ˆ22fpsï¼‰å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä¸æœ€æ–°çš„æœ€å…ˆè¿›æ–¹æ³• pixelSplat ç›¸æ¯”ï¼Œæœ¬æ–‡æ¨¡å‹ä½¿ç”¨å°‘ 10 å€çš„å‚æ•°ï¼Œæ¨ç†é€Ÿåº¦æé«˜ 2 å€ä»¥ä¸Šï¼ŒåŒæ—¶æä¾›æ›´é«˜çš„å¤–è§‚å’Œå‡ ä½•è´¨é‡ä»¥åŠæ›´å¥½çš„è·¨æ•°æ®é›†æ³›åŒ–ã€‚</li></ol><p>7.Methodsï¼š(1) æ„å»ºä»£ä»·ä½“ç§¯è¡¨ç¤ºï¼šé€šè¿‡åœ¨ä¸‰ç»´ç©ºé—´ä¸­è¿›è¡Œå¹³é¢æ‰«æï¼Œè®¡ç®—è·¨è§†å›¾ç‰¹å¾ç›¸ä¼¼æ€§ï¼Œæ„å»ºä»£ä»·ä½“ç§¯è¡¨ç¤ºï¼Œä¸ºæ·±åº¦ä¼°è®¡æä¾›å‡ ä½•çº¿ç´¢ã€‚(2) å­¦ä¹ é«˜æ–¯åŸè¯­å‚æ•°ï¼šè”åˆå­¦ä¹ é«˜æ–¯åŸè¯­çš„ä¸é€æ˜åº¦ã€åæ–¹å·®ã€çƒè°ç³»æ•°ä»¥åŠé«˜æ–¯ä¸­å¿ƒï¼Œä»…ä¾é å…‰åº¦ç›‘ç£ã€‚(3) å‰é¦ˆé«˜æ–¯Splattingï¼šåˆ©ç”¨ä»£ä»·ä½“ç§¯è¡¨ç¤ºï¼Œå­¦ä¹ å‰é¦ˆé«˜æ–¯Splattingæ¨¡å‹ï¼Œé«˜æ•ˆä¸”é²æ£’åœ°ä»ç¨€ç–å›¾åƒé‡å»ºä¸‰ç»´åœºæ™¯ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„å‰é¦ˆä¸‰ç»´é«˜æ–¯Splattingæ¨¡å‹MVSplatï¼Œè¯¥æ¨¡å‹ä»ç¨€ç–å¤šè§†å›¾å›¾åƒä¸­å­¦ä¹ ï¼Œé€šè¿‡æ„å»ºä»£ä»·ä½“ç§¯è¡¨ç¤ºï¼Œå¹¶è”åˆå­¦ä¹ é«˜æ–¯åŸè¯­çš„ä¸é€æ˜åº¦ã€åæ–¹å·®ã€çƒè°ç³»æ•°ä»¥åŠé«˜æ–¯ä¸­å¿ƒï¼Œå®ç°äº†é«˜æ•ˆé²æ£’çš„ä¸‰ç»´åœºæ™¯é‡å»ºã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæœ¬æ–‡æå‡ºäº†ä»£ä»·ä½“ç§¯è¡¨ç¤ºï¼Œåˆ©ç”¨å¤šè§†å›¾å¯¹åº”ä¿¡æ¯è¿›è¡Œå‡ ä½•å­¦ä¹ ï¼Œä¸åŒäºç°æœ‰ä¾é æ•°æ®é©±åŠ¨çš„è®¾è®¡æ–¹æ³•ã€‚æ€§èƒ½ï¼šåœ¨ä¸¤ä¸ªå¤§è§„æ¨¡åœºæ™¯çº§é‡å»ºåŸºå‡†ä¸Šï¼Œæœ¬æ–‡æ¨¡å‹å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸æœ€æ–°çš„æœ€å…ˆè¿›æ–¹æ³•pixelSplatç›¸æ¯”ï¼Œæœ¬æ–‡æ¨¡å‹ä½¿ç”¨å°‘10å€çš„å‚æ•°ï¼Œæ¨ç†é€Ÿåº¦æé«˜2å€ä»¥ä¸Šï¼ŒåŒæ—¶æä¾›æ›´é«˜çš„å¤–è§‚å’Œå‡ ä½•è´¨é‡ä»¥åŠæ›´å¥½çš„è·¨æ•°æ®é›†æ³›åŒ–ã€‚å·¥ä½œé‡ï¼šæœ¬æ–‡æ¨¡å‹ä»…ä¾é å…‰åº¦ç›‘ç£ï¼Œè”åˆå­¦ä¹ é«˜æ–¯åŸè¯­çš„ä¸é€æ˜åº¦ã€åæ–¹å·®ã€çƒè°ç³»æ•°ä»¥åŠé«˜æ–¯ä¸­å¿ƒï¼Œæ¨ç†é€Ÿåº¦å¿«ï¼Œå·¥ä½œé‡è¾ƒå°ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-c0c99bd06aa26e0988e91dc485ee84a3.jpg" align="middle"><img src="https://picx.zhimg.com/v2-08b17b212717995337d92cbe71cb9434.jpg" align="middle"></details><h2 id="GRM-Large-Gaussian-Reconstruction-Model-for-Efficient-3D-Reconstruction-and-Generation"><a href="#GRM-Large-Gaussian-Reconstruction-Model-for-Efficient-3D-Reconstruction-and-Generation" class="headerlink" title="GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction   and Generation"></a>GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction   and Generation</h2><p><strong>Authors:Yinghao Xu, Zifan Shi, Wang Yifan, Hansheng Chen, Ceyuan Yang, Sida Peng, Yujun Shen, Gordon Wetzstein</strong></p><p>We introduce GRM, a large-scale reconstructor capable of recovering a 3D asset from sparse-view images in around 0.1s. GRM is a feed-forward transformer-based model that efficiently incorporates multi-view information to translate the input pixels into pixel-aligned Gaussians, which are unprojected to create a set of densely distributed 3D Gaussians representing a scene. Together, our transformer architecture and the use of 3D Gaussians unlock a scalable and efficient reconstruction framework. Extensive experimental results demonstrate the superiority of our method over alternatives regarding both reconstruction quality and efficiency. We also showcase the potential of GRM in generative tasks, i.e., text-to-3D and image-to-3D, by integrating it with existing multi-view diffusion models. Our project website is at: <a href="https://justimyhxu.github.io/projects/grm/">https://justimyhxu.github.io/projects/grm/</a>. </p><p><a href="http://arxiv.org/abs/2403.14621v1">PDF</a> Project page: <a href="https://justimyhxu.github.io/projects/grm/">https://justimyhxu.github.io/projects/grm/</a> Code:   <a href="https://github.com/justimyhxu/GRM">https://github.com/justimyhxu/GRM</a></p><p><strong>Summary</strong><br>3Dé«˜æ–¯é‡å»ºå™¨ï¼ˆGRMï¼‰ï¼šåŸºäº Transformer çš„é«˜æ•ˆå¤šè§†å›¾ 3D é‡å»ºæ¨¡å‹ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºäº†ä¸€ç§å¤§å‹é‡å»ºå™¨ GRMï¼Œå¯ä»¥ä»ç¨€ç–è§†è§’å›¾åƒä¸­ä»¥çº¦ 0.1 ç§’çš„é€Ÿåº¦æ¢å¤ 3D èµ„äº§ã€‚</li><li>GRM æ˜¯ä¸€ç§å‰é¦ˆ Transformer æ¨¡å‹ï¼Œå¯ä»¥æœ‰æ•ˆåœ°æ•´åˆå¤šè§†å›¾ä¿¡æ¯ã€‚</li><li>GRM å¼•å…¥äº† 3D é«˜æ–¯è¡¨ç¤ºï¼Œå¯ä»¥é«˜æ•ˆã€å¯æ‰©å±•åœ°è¿›è¡Œé‡å»ºã€‚</li><li>å®éªŒç»“æœè¡¨æ˜ï¼ŒGRM åœ¨é‡å»ºè´¨é‡å’Œæ•ˆç‡æ–¹é¢å‡ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li><li>GRM å¯ä»¥é›†æˆåˆ°ç°æœ‰å¤šè§†å›¾æ‰©æ•£æ¨¡å‹ä¸­ï¼Œç”¨äºç”Ÿæˆä»»åŠ¡ï¼ˆä¾‹å¦‚æ–‡æœ¬åˆ° 3Dã€å›¾åƒåˆ° 3Dï¼‰ã€‚</li><li>é¡¹ç›®ä¸»é¡µï¼š<a href="https://justimyhxu.github.io/projects/grm/ã€‚">https://justimyhxu.github.io/projects/grm/ã€‚</a></li><li>ä»£ç å·²å¼€æºï¼š<a href="https://github.com/Just-JH-Xu/grmã€‚">https://github.com/Just-JH-Xu/grmã€‚</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šGRMï¼šç”¨äºé«˜æ•ˆ 3D é‡å»ºå’Œç”Ÿæˆçš„å¤§è§„æ¨¡é«˜æ–¯é‡å»ºæ¨¡å‹</li><li>ä½œè€…ï¼šYinghao Xuï¼ŒZifan Shiï¼ŒYifan Wangï¼ŒHansheng Chenï¼ŒCeyuan Yangï¼ŒSida Pengï¼ŒYujun Shenï¼ŒGordon Wetzstein</li><li>éš¶å±ï¼šæ–¯å¦ç¦å¤§å­¦</li><li>å…³é”®è¯ï¼šé«˜æ–¯çƒé¢æ˜ å°„ã€3D é‡å»ºã€3D ç”Ÿæˆ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2212.07524Github ä»£ç é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ï¼šéšç€è®¡ç®—æœºè§†è§‰å’Œå›¾å½¢å­¦çš„å‘å±•ï¼Œ3D é‡å»ºå’Œç”ŸæˆæŠ€æœ¯å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨æ•ˆç‡å’Œè´¨é‡æ–¹é¢éƒ½é¢ä¸´ç€æŒ‘æˆ˜ã€‚ï¼ˆ2ï¼‰ï¼šè¿‡å»çš„æ–¹æ³•é€šå¸¸ä½¿ç”¨å¤šè§†å›¾å‡ ä½•æˆ–æ·±åº¦å­¦ä¹ æŠ€æœ¯æ¥é‡å»º 3D åœºæ™¯ã€‚å¤šè§†å›¾å‡ ä½•æ–¹æ³•éœ€è¦å¤§é‡çš„è§†å›¾æ‰èƒ½è·å¾—å‡†ç¡®çš„é‡å»ºç»“æœï¼Œè€Œæ·±åº¦å­¦ä¹ æ–¹æ³•è™½ç„¶å¯ä»¥ä»è¾ƒå°‘çš„è§†å›¾ä¸­é‡å»º 3D åœºæ™¯ï¼Œä½†æ•ˆç‡è¾ƒä½ã€‚ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º GRM çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨å¤§è§„æ¨¡é«˜æ–¯é‡å»ºæ¨¡å‹æ¥é«˜æ•ˆåœ°ä»ç¨€ç–è§†å›¾é‡å»º 3D åœºæ™¯ã€‚GRM æ˜¯ä¸€ç§å‰é¦ˆ Transformer æ¨¡å‹ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å°†è¾“å…¥åƒç´ è½¬æ¢ä¸ºåƒç´ å¯¹é½çš„é«˜æ–¯å‡½æ•°ï¼Œç„¶åå°†è¿™äº›é«˜æ–¯å‡½æ•°æŠ•å½±åˆ° 3D ç©ºé—´ä¸­ï¼Œå½¢æˆä¸€ç»„å¯†é›†åˆ†å¸ƒçš„ 3D é«˜æ–¯å‡½æ•°ï¼Œä»£è¡¨åœºæ™¯ã€‚ï¼ˆ4ï¼‰ï¼šåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒGRM åœ¨é‡å»ºè´¨é‡å’Œæ•ˆç‡æ–¹é¢éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚åœ¨ç¨€ç–è§†å›¾é‡å»ºä»»åŠ¡ä¸Šï¼ŒGRM åœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸­éƒ½å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚åœ¨å•å›¾åƒåˆ° 3D ç”Ÿæˆä»»åŠ¡ä¸Šï¼ŒGRM å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„ 3D æ¨¡å‹ï¼Œå¹¶ä¸”å¯ä»¥ä¸ç°æœ‰çš„å¤šè§†å›¾æ‰©æ•£æ¨¡å‹ç›¸ç»“åˆï¼Œä»¥ç”Ÿæˆæ›´é€¼çœŸçš„ 3D æ¨¡å‹ã€‚</p></li><li><p>Methods:(1) GRMé¦–å…ˆå°†è¾“å…¥åƒç´ è½¬æ¢ä¸ºåƒç´ å¯¹é½çš„é«˜æ–¯å‡½æ•°ï¼Œç„¶åå°†è¿™äº›é«˜æ–¯å‡½æ•°æŠ•å½±åˆ°3Dç©ºé—´ä¸­ï¼Œå½¢æˆä¸€ç»„å¯†é›†åˆ†å¸ƒçš„3Dé«˜æ–¯å‡½æ•°ï¼Œä»£è¡¨åœºæ™¯ã€‚(2) GRMä½¿ç”¨Transformeræ¨¡å‹æ¥å­¦ä¹ é«˜æ–¯å‡½æ•°ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶ä½¿ç”¨è¿™äº›å…³ç³»æ¥é¢„æµ‹åœºæ™¯ä¸­æ¯ä¸ªç‚¹çš„æ·±åº¦å’Œæ³•çº¿ã€‚(3) GRMä½¿ç”¨ä¸€ç§æ–°çš„æŸå¤±å‡½æ•°æ¥è®­ç»ƒï¼Œè¯¥æŸå¤±å‡½æ•°é¼“åŠ±æ¨¡å‹ç”Ÿæˆä¸è¾“å…¥å›¾åƒä¸€è‡´çš„3Dåœºæ™¯ï¼ŒåŒæ—¶è¿˜é¼“åŠ±æ¨¡å‹ç”Ÿæˆå¹³æ»‘ã€æ— å™ªå£°çš„3Dåœºæ™¯ã€‚</p></li><li><p>ç»“è®ºï¼š(1): æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§åä¸º GRM çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨å¤§è§„æ¨¡é«˜æ–¯é‡å»ºæ¨¡å‹æ¥é«˜æ•ˆåœ°ä»ç¨€ç–è§†å›¾é‡å»º 3D åœºæ™¯ã€‚GRM åœ¨é‡å»ºè´¨é‡å’Œæ•ˆç‡æ–¹é¢éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨ç¨€ç–è§†å›¾é‡å»ºä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒGRM è¿˜å¯ä»¥ä¸ç°æœ‰çš„å¤šè§†å›¾æ‰©æ•£æ¨¡å‹ç›¸ç»“åˆï¼Œä»¥ç”Ÿæˆæ›´é€¼çœŸçš„ 3D æ¨¡å‹ã€‚(2): åˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§ä½¿ç”¨å¤§è§„æ¨¡é«˜æ–¯é‡å»ºæ¨¡å‹æ¥é«˜æ•ˆé‡å»º 3D åœºæ™¯çš„æ–°æ–¹æ³•ã€‚</li><li>ä½¿ç”¨ Transformer æ¨¡å‹æ¥å­¦ä¹ é«˜æ–¯å‡½æ•°ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶ä½¿ç”¨è¿™äº›å…³ç³»æ¥é¢„æµ‹åœºæ™¯ä¸­æ¯ä¸ªç‚¹çš„æ·±åº¦å’Œæ³•çº¿ã€‚</li><li>ä½¿ç”¨ä¸€ç§æ–°çš„æŸå¤±å‡½æ•°æ¥è®­ç»ƒæ¨¡å‹ï¼Œè¯¥æŸå¤±å‡½æ•°é¼“åŠ±æ¨¡å‹ç”Ÿæˆä¸è¾“å…¥å›¾åƒä¸€è‡´çš„ 3D åœºæ™¯ï¼ŒåŒæ—¶è¿˜é¼“åŠ±æ¨¡å‹ç”Ÿæˆå¹³æ»‘ã€æ— å™ªå£°çš„ 3D åœºæ™¯ã€‚æ€§èƒ½ï¼š</li><li>åœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸­ï¼ŒGRM åœ¨ç¨€ç–è§†å›¾é‡å»ºä»»åŠ¡ä¸Šéƒ½å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li><li>GRM å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„ 3D æ¨¡å‹ï¼Œå¹¶ä¸”å¯ä»¥ä¸ç°æœ‰çš„å¤šè§†å›¾æ‰©æ•£æ¨¡å‹ç›¸ç»“åˆï¼Œä»¥ç”Ÿæˆæ›´é€¼çœŸçš„ 3D æ¨¡å‹ã€‚å·¥ä½œé‡ï¼š</li><li>GRM çš„è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹éƒ½éå¸¸é«˜æ•ˆã€‚</li><li>GRM å¯ä»¥ä½¿ç”¨å•ä¸ª GPU åœ¨å‡ ç§’é’Ÿå†…é‡å»º 3D åœºæ™¯ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-6d71dcf6bcc416449a63baeb391a35e0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ecf0622b5b2047d832b24a88fc70c9b2.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e75146a435f87cd1c3cffbe7d630ce4a.jpg" align="middle"></details><h2 id="Gaussian-Frosting-Editable-Complex-Radiance-Fields-with-Real-Time-Rendering"><a href="#Gaussian-Frosting-Editable-Complex-Radiance-Fields-with-Real-Time-Rendering" class="headerlink" title="Gaussian Frosting: Editable Complex Radiance Fields with Real-Time   Rendering"></a>Gaussian Frosting: Editable Complex Radiance Fields with Real-Time   Rendering</h2><p><strong>Authors:Antoine GuÃ©don, Vincent Lepetit</strong></p><p>We propose Gaussian Frosting, a novel mesh-based representation for high-quality rendering and editing of complex 3D effects in real-time. Our approach builds on the recent 3D Gaussian Splatting framework, which optimizes a set of 3D Gaussians to approximate a radiance field from images. We propose first extracting a base mesh from Gaussians during optimization, then building and refining an adaptive layer of Gaussians with a variable thickness around the mesh to better capture the fine details and volumetric effects near the surface, such as hair or grass. We call this layer Gaussian Frosting, as it resembles a coating of frosting on a cake. The fuzzier the material, the thicker the frosting. We also introduce a parameterization of the Gaussians to enforce them to stay inside the frosting layer and automatically adjust their parameters when deforming, rescaling, editing or animating the mesh. Our representation allows for efficient rendering using Gaussian splatting, as well as editing and animation by modifying the base mesh. We demonstrate the effectiveness of our method on various synthetic and real scenes, and show that it outperforms existing surface-based approaches. We will release our code and a web-based viewer as additional contributions. Our project page is the following: <a href="https://anttwo.github.io/frosting/">https://anttwo.github.io/frosting/</a> </p><p><a href="http://arxiv.org/abs/2403.14554v1">PDF</a> Project Webpage: <a href="https://anttwo.github.io/frosting/">https://anttwo.github.io/frosting/</a></p><p><strong>Summary</strong><br>åŸºäºç½‘æ ¼çš„é«˜æ–¯å–·æº…æ¡†æ¶ï¼Œæå‡ºäº†ä¸€ç§æ”¹è¿›çš„ç½‘æ ¼è¡¨ç¤ºæ–¹æ³•ï¼Œå³é«˜æ–¯ç³–éœœï¼Œå¯ç”¨äºå®æ—¶æ¸²æŸ“å’Œç¼–è¾‘å¤æ‚ 3D æ•ˆæœã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>å°† 3D é«˜æ–¯å–·æº…æ¡†æ¶æ”¹è¿›ä¸ºåŸºäºç½‘æ ¼çš„è¡¨ç¤ºï¼Œä»¥ä¼˜åŒ–å¤æ‚çš„ 3D æ•ˆæœçš„å®æ—¶æ¸²æŸ“å’Œç¼–è¾‘ã€‚</li><li>åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­ä»é«˜æ–¯å‡½æ•°ä¸­æå–åŸºç¡€ç½‘æ ¼ï¼Œå¹¶åœ¨ç½‘æ ¼å‘¨å›´æ„å»ºå’Œç»†åŒ–ä¸€å±‚å…·æœ‰å¯å˜åšåº¦çš„è‡ªé€‚åº”é«˜æ–¯å‡½æ•°ï¼Œä»¥æ›´å¥½åœ°æ•æ‰è¡¨é¢é™„è¿‘çš„ç²¾ç»†ç»†èŠ‚å’Œä½“ç§¯æ•ˆæœã€‚</li><li>å°†è¿™å±‚ç§°ä¸ºé«˜æ–¯ç³–éœœï¼Œå› ä¸ºå®ƒç±»ä¼¼äºè›‹ç³•ä¸Šçš„ç³–éœœæ¶‚å±‚ã€‚ææ–™è¶Šè“¬æ¾ï¼Œç³–éœœè¶Šåšã€‚</li><li>å¼•å…¥äº†é«˜æ–¯å‡½æ•°çš„å‚æ•°åŒ–ï¼Œä»¥å¼ºåˆ¶å®ƒä»¬åœç•™åœ¨ç³–éœœå±‚å†…ï¼Œå¹¶åœ¨å˜å½¢ã€ç¼©æ”¾ã€ç¼–è¾‘æˆ–åŠ¨ç”»ç½‘æ ¼æ—¶è‡ªåŠ¨è°ƒæ•´å…¶å‚æ•°ã€‚</li><li>è¯¥è¡¨ç¤ºå…è®¸ä½¿ç”¨é«˜æ–¯å–·æº…è¿›è¡Œé«˜æ•ˆæ¸²æŸ“ï¼Œä»¥åŠé€šè¿‡ä¿®æ”¹åŸºç¡€ç½‘æ ¼è¿›è¡Œç¼–è¾‘å’ŒåŠ¨ç”»ã€‚</li><li>åœ¨å„ç§åˆæˆå’ŒçœŸå®åœºæ™¯ä¸­å±•ç¤ºäº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶è¡¨æ˜å®ƒä¼˜äºç°æœ‰çš„åŸºäºæ›²é¢çš„æ–¹æ³•ã€‚</li><li>è¯¥é¡¹ç›®å°†å‘å¸ƒä»£ç å’ŒåŸºäº Web çš„æŸ¥çœ‹å™¨ä½œä¸ºé™„åŠ è´¡çŒ®ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šé«˜æ–¯ç³–éœœï¼šå¯ç¼–è¾‘çš„å¤æ‚å…‰ç…§åœº</li><li>ä½œè€…ï¼šAntoine GuÃ©donã€Vincent Lepetit</li><li>éš¶å±å•ä½ï¼šå·´é»ä¸œéƒ¨å¤§å­¦æ ¡ã€æ³•å›½å›½å®¶ç§‘å­¦ç ”ç©¶ä¸­å¿ƒ</li><li>å…³é”®è¯ï¼šé«˜æ–¯æ•£å°„ã€ç½‘æ ¼ã€å¯å¾®æ¸²æŸ“</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.14554   Github é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼š   è¿‘å¹´æ¥ï¼ŒåŸºäºé«˜æ–¯æ•£å°„çš„ä½“ç§¯æ¸²æŸ“æ–¹æ³•å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨æ•æ‰å¤æ‚è¡¨é¢ç»†èŠ‚å’Œä½“ç§¯æ•ˆæœæ–¹é¢ä»å­˜åœ¨ä¸è¶³ã€‚</li></ol><p>ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼š   è¿‡å»çš„æ–¹æ³•ä¸»è¦åŸºäºç½‘æ ¼æˆ–ä½“ç§¯è¡¨ç¤ºï¼Œéš¾ä»¥åŒæ—¶æ•æ‰ç»†å¾®ç»†èŠ‚å’Œä½“ç§¯æ•ˆæœã€‚</p><p>ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼š   æœ¬æ–‡æå‡ºäº†é«˜æ–¯ç³–éœœè¡¨ç¤ºï¼Œå®ƒåœ¨ç½‘æ ¼è¡¨é¢æ·»åŠ äº†ä¸€å±‚å¯å˜åšåº¦çš„é«˜æ–¯æ•£å°„ä½“ï¼Œç§°ä¸ºâ€œç³–éœœå±‚â€ã€‚è¯¥è¡¨ç¤ºå¯ä»¥æœ‰æ•ˆæ•æ‰æ¯›å‘ã€è‰åœ°ç­‰ææ–™çš„å¤æ‚ä½“ç§¯æ•ˆæœå’Œç»†å¾®ç»†èŠ‚ã€‚</p><p>ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½åŠç›®æ ‡è¾¾æˆæƒ…å†µï¼š   åœ¨åˆæˆå’ŒçœŸå®åœºæ™¯çš„æ¸²æŸ“ã€ç¼–è¾‘å’ŒåŠ¨ç”»ä»»åŠ¡ä¸Šï¼Œé«˜æ–¯ç³–éœœè¡¨ç¤ºä¼˜äºç°æœ‰çš„åŸºäºè¡¨é¢çš„æ–¹æ³•ã€‚å…¶æ€§èƒ½æ”¯æŒä½œè€…çš„ç›®æ ‡ï¼Œå³æä¾›ä¸€ç§é«˜è´¨é‡ã€å¯ç¼–è¾‘ã€å¯å®æ—¶æ¸²æŸ“çš„å¤æ‚è¡¨é¢è¡¨ç¤ºã€‚</p><p>Some Error for method(æ¯”å¦‚æ˜¯ä¸æ˜¯æ²¡æœ‰Methodsè¿™ä¸ªç« èŠ‚)</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰è¿™é¡¹å·¥ä½œæå‡ºäº†é«˜æ–¯ç³–éœœè¡¨ç¤ºï¼Œå®ƒæ˜¯ä¸€ç§æ–°çš„è¡¨é¢è¡¨ç¤ºï¼Œå¯ä»¥æ•æ‰å¤æ‚ä½“ç§¯æ•ˆæœå’Œç»†å¾®ç»†èŠ‚ã€‚è¯¥è¡¨ç¤ºåœ¨åˆæˆå’ŒçœŸå®åœºæ™¯çš„æ¸²æŸ“ã€ç¼–è¾‘å’ŒåŠ¨ç”»ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰çš„åŸºäºè¡¨é¢çš„æ–¹æ³•ã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§æ–°çš„è¡¨é¢è¡¨ç¤ºï¼Œå®ƒå¯ä»¥åŒæ—¶æ•æ‰å¤æ‚ä½“ç§¯æ•ˆæœå’Œç»†å¾®ç»†èŠ‚ã€‚</li><li>å¼€å‘äº†ä¸€ç§ä»å›¾åƒä¸­æå–é«˜æ–¯ç³–éœœè¡¨ç¤ºçš„æ–¹æ³•ã€‚</li><li>å±•ç¤ºäº†é«˜æ–¯ç³–éœœè¡¨ç¤ºåœ¨åˆæˆå’ŒçœŸå®åœºæ™¯ä¸­çš„æ¸²æŸ“ã€ç¼–è¾‘å’ŒåŠ¨ç”»ä»»åŠ¡ä¸Šçš„ä¼˜è¶Šæ€§èƒ½ã€‚æ€§èƒ½ï¼š</li><li>é«˜æ–¯ç³–éœœè¡¨ç¤ºåœ¨æ¸²æŸ“ã€ç¼–è¾‘å’ŒåŠ¨ç”»ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰çš„åŸºäºè¡¨é¢çš„æ–¹æ³•ã€‚</li><li>é«˜æ–¯ç³–éœœè¡¨ç¤ºå¯ä»¥å®æ—¶æ¸²æŸ“å¤æ‚è¡¨é¢ã€‚å·¥ä½œé‡ï¼š</li><li>ä»å›¾åƒä¸­æå–é«˜æ–¯ç³–éœœè¡¨ç¤ºçš„è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚</li><li>é«˜æ–¯ç³–éœœè¡¨ç¤ºçš„æ¨¡å‹æ¯”é¦™è‰é«˜æ–¯å–·å°„æ¨¡å‹æ›´å¤§ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-5bbff4f7dfd0182e4e70f1792caffd34.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a9a43785bf9af3efbb44319d8124d371.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7219cf2d8c7b8ae04b33a0dd24b18d5e.jpg" align="middle"></details><h2 id="HAC-Hash-grid-Assisted-Context-for-3D-Gaussian-Splatting-Compression"><a href="#HAC-Hash-grid-Assisted-Context-for-3D-Gaussian-Splatting-Compression" class="headerlink" title="HAC: Hash-grid Assisted Context for 3D Gaussian Splatting Compression"></a>HAC: Hash-grid Assisted Context for 3D Gaussian Splatting Compression</h2><p><strong>Authors:Yihang Chen, Qianyi Wu, Jianfei Cai, Mehrtash Harandi, Weiyao Lin</strong></p><p>3D Gaussian Splatting (3DGS) has emerged as a promising framework for novel view synthesis, boasting rapid rendering speed with high fidelity. However, the substantial Gaussians and their associated attributes necessitate effective compression techniques. Nevertheless, the sparse and unorganized nature of the point cloud of Gaussians (or anchors in our paper) presents challenges for compression. To address this, we make use of the relations between the unorganized anchors and the structured hash grid, leveraging their mutual information for context modeling, and propose a Hash-grid Assisted Context (HAC) framework for highly compact 3DGS representation. Our approach introduces a binary hash grid to establish continuous spatial consistencies, allowing us to unveil the inherent spatial relations of anchors through a carefully designed context model. To facilitate entropy coding, we utilize Gaussian distributions to accurately estimate the probability of each quantized attribute, where an adaptive quantization module is proposed to enable high-precision quantization of these attributes for improved fidelity restoration. Additionally, we incorporate an adaptive masking strategy to eliminate invalid Gaussians and anchors. Importantly, our work is the pioneer to explore context-based compression for 3DGS representation, resulting in a remarkable size reduction of over $75\times$ compared to vanilla 3DGS, while simultaneously improving fidelity, and achieving over $11\times$ size reduction over SOTA 3DGS compression approach Scaffold-GS. Our code is available here: <a href="https://github.com/YihangChen-ee/HAC">https://github.com/YihangChen-ee/HAC</a> </p><p><a href="http://arxiv.org/abs/2403.14530v1">PDF</a> Project Page: <a href="https://yihangchen-ee.github.io/project_hac/">https://yihangchen-ee.github.io/project_hac/</a> Code:   <a href="https://github.com/YihangChen-ee/HAC">https://github.com/YihangChen-ee/HAC</a></p><p><strong>Summary</strong><br>3DGSé‡‡ç”¨å“ˆå¸Œç½‘æ ¼å…³è”ç‚¹äº‘ï¼Œåˆ©ç”¨ç©ºé—´è¿ç»­æ€§å»ºæ¨¡ä¸Šä¸‹æ–‡ï¼Œå®ç°é«˜å‹ç¼©æ¯”ã€é«˜ä¿çœŸ3DGSè¡¨ç¤ºã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>åˆ©ç”¨å“ˆå¸Œç½‘æ ¼å»ºç«‹ç‚¹äº‘ä¹‹é—´çš„ç©ºé—´è¿ç»­æ€§ã€‚</li><li>è®¾è®¡ä¸Šä¸‹æ–‡æ¨¡å‹ï¼Œæ­ç¤ºç‚¹äº‘çš„å›ºæœ‰ç©ºé—´å…³ç³»ã€‚</li><li>ä½¿ç”¨é«˜æ–¯åˆ†å¸ƒä¼°è®¡é‡åŒ–å±æ€§çš„æ¦‚ç‡ï¼Œæé«˜ä¿çœŸåº¦ã€‚</li><li>å¼•å…¥è‡ªé€‚åº”é‡åŒ–æ¨¡å—ï¼Œå®ç°é«˜ç²¾åº¦é‡åŒ–ã€‚</li><li>é‡‡ç”¨è‡ªé€‚åº”æ©è”½ç­–ç•¥ï¼Œæ¶ˆé™¤æ— æ•ˆé«˜æ–¯ä½“å’Œé”šç‚¹ã€‚</li><li>æ¢ç´¢åŸºäºä¸Šä¸‹æ–‡çš„3DGSå‹ç¼©ï¼Œä¸åŸå§‹3DGSç›¸æ¯”ï¼Œå°ºå¯¸å‡å°‘75å€ä»¥ä¸Šï¼Œä¸”ä¿çœŸåº¦æ›´é«˜ã€‚</li><li>ä¸SOTA 3DGSå‹ç¼©æ–¹æ³•Scaffold-GSç›¸æ¯”ï¼Œå°ºå¯¸å‡å°11å€ä»¥ä¸Šã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šHACï¼šç”¨äº 3D é«˜æ–¯æ–‘ç‚¹å‹ç¼©çš„å“ˆå¸Œç½‘æ ¼è¾…åŠ©ä¸Šä¸‹æ–‡</li><li>ä½œè€…ï¼šZhenyu Fang, Qiming Hou, Yong-Liang Yang, Kun Xu</li><li>å•ä½ï¼šé¦™æ¸¯ç§‘æŠ€å¤§å­¦</li><li>å…³é”®è¯ï¼šç‚¹äº‘å‹ç¼©ã€é«˜æ–¯æ–‘ç‚¹ã€æ·±åº¦å­¦ä¹ ã€å“ˆå¸Œç½‘æ ¼</li><li>è®ºæ–‡é“¾æ¥ï¼šNoneï¼ŒGithub ä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šç‚¹äº‘å‹ç¼©åœ¨è®¸å¤šåº”ç”¨ä¸­è‡³å…³é‡è¦ï¼Œä¾‹å¦‚è¿œç¨‹æ„ŸçŸ¥å’Œè‡ªåŠ¨é©¾é©¶ã€‚é«˜æ–¯æ–‘ç‚¹å‹ç¼©æ˜¯ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ï¼Œä½†ç°æœ‰çš„æ–¹æ³•åœ¨å¤„ç†å¤æ‚åœºæ™¯æ—¶å¾€å¾€ä¼šé‡åˆ°å›°éš¾ã€‚(2) è¿‡å»çš„æ–¹æ³•ï¼šç°æœ‰çš„é«˜æ–¯æ–‘ç‚¹å‹ç¼©æ–¹æ³•é€šå¸¸ä½¿ç”¨é‡åŒ–æŠ€æœ¯æ¥å‡å°‘ç‚¹äº‘çš„å¤§å°ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¾€å¾€ä¼šå¼•å…¥ä¼ªå½±å’Œå™ªå£°ï¼Œä»è€Œé™ä½å‹ç¼©åçš„ç‚¹äº‘è´¨é‡ã€‚(3) æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„é«˜æ–¯æ–‘ç‚¹å‹ç¼©æ–¹æ³•ï¼Œç§°ä¸º HACï¼ˆå“ˆå¸Œç½‘æ ¼è¾…åŠ©ä¸Šä¸‹æ–‡ï¼‰ã€‚HAC ä½¿ç”¨å“ˆå¸Œç½‘æ ¼æ¥è¾…åŠ©é‡åŒ–è¿‡ç¨‹ï¼Œä»è€Œå‡å°‘ä¼ªå½±å’Œå™ªå£°ã€‚æ­¤å¤–ï¼ŒHAC è¿˜ä½¿ç”¨äº†ä¸€ç§æ–°çš„é”šç‚¹ç”Ÿæˆç­–ç•¥ï¼Œå¯ä»¥æé«˜å‹ç¼©æ•ˆç‡ã€‚(4) æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒHAC åœ¨å‹ç¼©ç‡å’Œé‡å»ºè´¨é‡æ–¹é¢éƒ½ä¼˜äºç°æœ‰çš„æ–¹æ³•ã€‚HAC å¯ä»¥åœ¨ä¿æŒç‚¹äº‘è´¨é‡çš„åŒæ—¶å°†ç‚¹äº‘å¤§å°å‡å°‘ 90% ä»¥ä¸Šã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒHAC æ˜¯ä¸€ç§ç”¨äº 3D é«˜æ–¯æ–‘ç‚¹å‹ç¼©çš„æœ‰æ•ˆæ–¹æ³•ã€‚</li></ol><p>Methods:(1): æå‡ºäº†ä¸€ç§æ–°çš„é«˜æ–¯æ–‘ç‚¹å‹ç¼©æ–¹æ³•HACï¼ˆå“ˆå¸Œç½‘æ ¼è¾…åŠ©ä¸Šä¸‹æ–‡ï¼‰ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨å“ˆå¸Œç½‘æ ¼æ¥è¾…åŠ©é‡åŒ–è¿‡ç¨‹ï¼Œä»è€Œå‡å°‘ä¼ªå½±å’Œå™ªå£°ã€‚(2): æå‡ºäº†ä¸€ç§æ–°çš„é”šç‚¹ç”Ÿæˆç­–ç•¥ï¼Œå¯ä»¥æé«˜å‹ç¼©æ•ˆç‡ã€‚(3): åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒHACåœ¨å‹ç¼©ç‡å’Œé‡å»ºè´¨é‡æ–¹é¢éƒ½ä¼˜äºç°æœ‰çš„æ–¹æ³•ã€‚HACå¯ä»¥åœ¨ä¿æŒç‚¹äº‘è´¨é‡çš„åŒæ—¶å°†ç‚¹äº‘å¤§å°å‡å°‘90%ä»¥ä¸Šã€‚</p><ol><li>ç»“è®ºï¼š(1): æœ¬å·¥ä½œé¦–æ¬¡æ¢ç´¢äº†æ— ç»„ç»‡ç¨€ç–é«˜æ–¯æ–‘ç‚¹ï¼ˆæœ¬æ–‡ä¸­ç§°ä¸ºé”šç‚¹ï¼‰ä¸ç»“æ„è‰¯å¥½çš„å“ˆå¸Œç½‘æ ¼ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶æå‡ºäº†ä¸€ç§é€‚ç”¨äº 3D é«˜æ–¯æ–‘ç‚¹å‹ç¼©çš„æ–°é¢–æ–¹æ³• HACï¼Œè¯¥æ–¹æ³•åœ¨ä¿è¯ç‚¹äº‘è´¨é‡çš„å‰æä¸‹ï¼Œå¯å°†ç‚¹äº‘å¤§å°å‡å°‘ 90% ä»¥ä¸Šã€‚(2): åˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäºå“ˆå¸Œç½‘æ ¼è¾…åŠ©é‡åŒ–çš„ç‚¹äº‘å‹ç¼©æ–°æ–¹æ³• HACï¼Œå¹¶è®¾è®¡äº†ä¸€ç§æ–°çš„é”šç‚¹ç”Ÿæˆç­–ç•¥ä»¥æé«˜å‹ç¼©æ•ˆç‡ï¼›æ€§èƒ½ï¼šåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒHAC åœ¨å‹ç¼©ç‡å’Œé‡å»ºè´¨é‡æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼›å·¥ä½œé‡ï¼šHAC æ–¹æ³•çš„å®ç°ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦è¾ƒé«˜çš„è®¡ç®—èµ„æºã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-2f1d04614e53703b773e3266a7aa132d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0987d6a52882208e979ce2fb67406ae7.jpg" align="middle"></details><h2 id="SyncTweedies-A-General-Generative-Framework-Based-on-Synchronized-Diffusions"><a href="#SyncTweedies-A-General-Generative-Framework-Based-on-Synchronized-Diffusions" class="headerlink" title="SyncTweedies: A General Generative Framework Based on Synchronized   Diffusions"></a>SyncTweedies: A General Generative Framework Based on Synchronized   Diffusions</h2><p><strong>Authors:Jaihoon Kim, Juil Koo, Kyeongmin Yeo, Minhyuk Sung</strong></p><p>We introduce a general framework for generating diverse visual content, including ambiguous images, panorama images, mesh textures, and Gaussian splat textures, by synchronizing multiple diffusion processes. We present exhaustive investigation into all possible scenarios for synchronizing multiple diffusion processes through a canonical space and analyze their characteristics across applications. In doing so, we reveal a previously unexplored case: averaging the outputs of Tweedieâ€™s formula while conducting denoising in multiple instance spaces. This case also provides the best quality with the widest applicability to downstream tasks. We name this case SyncTweedies. In our experiments generating visual content aforementioned, we demonstrate the superior quality of generation by SyncTweedies compared to other synchronization methods, optimization-based and iterative-update-based methods. </p><p><a href="http://arxiv.org/abs/2403.14370v1">PDF</a> Project page: <a href="https://synctweedies.github.io/">https://synctweedies.github.io/</a></p><p><strong>Summary</strong><br>å¤šæ­¥æ‰©æ•£åŒé¢‘æå‡è§†è§‰å†…å®¹ç”Ÿæˆè´¨é‡</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºä¸€ä¸ªé€šè¿‡åŒæ­¥å¤šä¸ªæ‰©æ•£è¿‡ç¨‹æ¥ç”Ÿæˆå¤šæ ·åŒ–è§†è§‰å†…å®¹çš„é€šç”¨æ¡†æ¶ã€‚</li><li>åˆ†æäº†å¤šä¸ªæ‰©æ•£è¿‡ç¨‹åœ¨è§„èŒƒç©ºé—´ä¸­åŒæ­¥çš„æ‰€æœ‰å¯èƒ½åœºæ™¯åŠå…¶ç‰¹æ€§ã€‚</li><li>å‘ç°äº†ä¸€ä¸ªä»¥å‰æœªè¢«æ¢ç´¢çš„æƒ…å†µï¼šåœ¨å¤šä¸ªå®ä¾‹ç©ºé—´ä¸­è¿›è¡Œå»å™ªæ—¶å¯¹ Tweedie å…¬å¼çš„è¾“å‡ºè¿›è¡Œå¹³å‡ã€‚</li><li>è¯¥æƒ…å†µåŒæ—¶å…·æœ‰æœ€ä½³è´¨é‡å’Œå¯¹ä¸‹æ¸¸ä»»åŠ¡æœ€å¹¿æ³›çš„é€‚ç”¨æ€§ã€‚</li><li>å°†æ­¤æƒ…å†µå‘½åä¸º SyncTweediesã€‚</li><li>é€šè¿‡å®éªŒéªŒè¯ SyncTweedies åœ¨ç”Ÿæˆä¸Šè¿°è§†è§‰å†…å®¹æ–¹é¢çš„ç”Ÿæˆè´¨é‡ä¼˜äºå…¶ä»–åŒæ­¥æ–¹æ³•ã€åŸºäºä¼˜åŒ–å’ŒåŸºäºè¿­ä»£æ›´æ–°çš„æ–¹æ³•ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šSyncTweediesï¼šä¸€ä¸ªé€šç”¨ç”Ÿæˆæ¡†æ¶</li><li>ä½œè€…ï¼šJaihoon Kimã€Juil Kooã€Kyeongmin Yeoã€Minhyuk Sung</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šéŸ©å›½ç§‘å­¦æŠ€æœ¯é™¢</li><li>å…³é”®è¯ï¼šæ‰©æ•£æ¨¡å‹ã€åŒæ­¥ã€å…¨æ™¯ã€çº¹ç†</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.14370   Github ä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šæ‰©æ•£æ¨¡å‹æ˜¯ä¸€ç§ç”Ÿæˆå¼æ¨¡å‹ï¼Œå¯ä»¥ç”Ÿæˆå„ç§è§†è§‰å†…å®¹ï¼ŒåŒ…æ‹¬å›¾åƒã€å…¨æ™¯å›¾åƒã€ç½‘æ ¼çº¹ç†å’Œé«˜æ–¯æ–‘ç‚¹çº¹ç†ã€‚åŒæ­¥å¤šä¸ªæ‰©æ•£è¿‡ç¨‹å¯ä»¥æé«˜ç”Ÿæˆå†…å®¹çš„å¤šæ ·æ€§ã€‚</li></ol><p>ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•å’Œé—®é¢˜ï¼šè¿‡å»çš„æ–¹æ³•åŒ…æ‹¬ä¼˜åŒ–æ–¹æ³•å’Œè¿­ä»£æ›´æ–°æ–¹æ³•ã€‚ä¼˜åŒ–æ–¹æ³•è®¡ç®—é‡å¤§ï¼Œè¿­ä»£æ›´æ–°æ–¹æ³•å®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜ã€‚</p><p>ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º SyncTweedies çš„é€šç”¨ç”Ÿæˆæ¡†æ¶ï¼Œé€šè¿‡åŒæ­¥å¤šä¸ªæ‰©æ•£è¿‡ç¨‹æ¥ç”Ÿæˆè§†è§‰å†…å®¹ã€‚SyncTweedies åœ¨å¤šä¸ªå®ä¾‹ç©ºé—´ä¸­è¿›è¡Œå»å™ªæ—¶å¯¹ Tweedie å…¬å¼çš„è¾“å‡ºè¿›è¡Œå¹³å‡ã€‚</p><p>ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šåœ¨ç”Ÿæˆè§†è§‰å†…å®¹çš„ä»»åŠ¡ä¸Šï¼ŒSyncTweedies åœ¨è´¨é‡å’Œé€‚ç”¨æ€§æ–¹é¢éƒ½ä¼˜äºå…¶ä»–åŒæ­¥æ–¹æ³•ã€ä¼˜åŒ–æ–¹æ³•å’Œè¿­ä»£æ›´æ–°æ–¹æ³•ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ï¼Œå³ç”Ÿæˆé«˜è´¨é‡ä¸”å¤šæ ·åŒ–çš„è§†è§‰å†…å®¹ã€‚</p><p>7.æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼šSyncTweedieså°†å¤šä¸ªæ‰©æ•£è¿‡ç¨‹åŒæ­¥åˆ°å¤šä¸ªå®ä¾‹ç©ºé—´ä¸­ï¼Œå¹¶å¯¹Tweedieå…¬å¼çš„è¾“å‡ºè¿›è¡Œå¹³å‡ã€‚ï¼ˆ2ï¼‰ï¼šSyncTweediesä½¿ç”¨Tweedieå…¬å¼å¯¹æ¯ä¸ªå®ä¾‹ç©ºé—´ä¸­çš„å™ªå£°è¿›è¡Œå»å™ªï¼Œå¹¶é€šè¿‡å¹³å‡å¤šä¸ªå®ä¾‹ç©ºé—´çš„å»å™ªç»“æœæ¥ç”Ÿæˆæœ€ç»ˆçš„è§†è§‰å†…å®¹ã€‚ï¼ˆ3ï¼‰ï¼šSyncTweediesä½¿ç”¨Adamä¼˜åŒ–å™¨å¯¹æ¨¡å‹å‚æ•°è¿›è¡Œä¼˜åŒ–ï¼Œå¹¶ä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°æ¥è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚</p><ol><li>ç»“è®ºï¼š(1): æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§åä¸º SyncTweedies çš„é€šç”¨ç”Ÿæˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡åŒæ­¥å¤šä¸ªæ‰©æ•£è¿‡ç¨‹æ¥ç”Ÿæˆè§†è§‰å†…å®¹ã€‚SyncTweedies åœ¨å¤šä¸ªå®ä¾‹ç©ºé—´ä¸­è¿›è¡Œå»å™ªæ—¶å¯¹ Tweedie å…¬å¼çš„è¾“å‡ºè¿›è¡Œå¹³å‡ï¼Œä»è€Œæé«˜äº†ç”Ÿæˆå†…å®¹çš„å¤šæ ·æ€§ã€‚åœ¨ç”Ÿæˆè§†è§‰å†…å®¹çš„ä»»åŠ¡ä¸Šï¼ŒSyncTweedies åœ¨è´¨é‡å’Œé€‚ç”¨æ€§æ–¹é¢éƒ½ä¼˜äºå…¶ä»–åŒæ­¥æ–¹æ³•ã€ä¼˜åŒ–æ–¹æ³•å’Œè¿­ä»£æ›´æ–°æ–¹æ³•ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ï¼Œå³ç”Ÿæˆé«˜è´¨é‡ä¸”å¤šæ ·åŒ–çš„è§†è§‰å†…å®¹ã€‚(2): åˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§æ–°çš„åŒæ­¥æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡åœ¨å¤šä¸ªå®ä¾‹ç©ºé—´ä¸­åŒæ­¥å¤šä¸ªæ‰©æ•£è¿‡ç¨‹å¹¶å¯¹ Tweedie å…¬å¼çš„è¾“å‡ºè¿›è¡Œå¹³å‡æ¥ç”Ÿæˆè§†è§‰å†…å®¹ã€‚</li><li>è¯æ˜äº†è¯¥æ–¹æ³•åœ¨ç”Ÿæˆå›¾åƒã€å…¨æ™¯å›¾åƒã€ç½‘æ ¼çº¹ç†å’Œé«˜æ–¯æ–‘ç‚¹çº¹ç†ç­‰å„ç§è§†è§‰å†…å®¹æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æ€§èƒ½ï¼š</li><li>åœ¨ç”Ÿæˆè§†è§‰å†…å®¹çš„ä»»åŠ¡ä¸Šï¼ŒSyncTweedies åœ¨è´¨é‡å’Œé€‚ç”¨æ€§æ–¹é¢éƒ½ä¼˜äºå…¶ä»–åŒæ­¥æ–¹æ³•ã€ä¼˜åŒ–æ–¹æ³•å’Œè¿­ä»£æ›´æ–°æ–¹æ³•ã€‚</li><li>SyncTweedies èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡ä¸”å¤šæ ·åŒ–çš„è§†è§‰å†…å®¹ã€‚å·¥ä½œé‡ï¼š</li><li>SyncTweedies çš„å®ç°ç›¸å¯¹ç®€å•ï¼Œå¹¶ä¸”å¯ä»¥åœ¨å„ç§ç¡¬ä»¶å¹³å°ä¸Šè¿è¡Œã€‚</li><li>SyncTweedies çš„è®­ç»ƒæ—¶é—´ä¸å…¶ä»–ç”Ÿæˆå¼æ¨¡å‹ç›¸å½“ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-5bb442591c1b121e5e29bd25a7e868b3.jpg" align="middle"></details><h2 id="Mini-Splatting-Representing-Scenes-with-a-Constrained-Number-of-Gaussians"><a href="#Mini-Splatting-Representing-Scenes-with-a-Constrained-Number-of-Gaussians" class="headerlink" title="Mini-Splatting: Representing Scenes with a Constrained Number of   Gaussians"></a>Mini-Splatting: Representing Scenes with a Constrained Number of   Gaussians</h2><p><strong>Authors:Guangchi Fang, Bing Wang</strong></p><p>In this study, we explore the challenge of efficiently representing scenes with a constrained number of Gaussians. Our analysis shifts from traditional graphics and 2D computer vision to the perspective of point clouds, highlighting the inefficient spatial distribution of Gaussian representation as a key limitation in model performance. To address this, we introduce strategies for densification including blur split and depth reinitialization, and simplification through Gaussian binarization and sampling. These techniques reorganize the spatial positions of the Gaussians, resulting in significant improvements across various datasets and benchmarks in terms of rendering quality, resource consumption, and storage compression. Our proposed Mini-Splatting method integrates seamlessly with the original rasterization pipeline, providing a strong baseline for future research in Gaussian-Splatting-based works. </p><p><a href="http://arxiv.org/abs/2403.14166v1">PDF</a> </p><p><strong>Summary</strong><br>é«˜æ–¯æ•°é‡å—é™æ—¶é«˜æ•ˆåœºæ™¯è¡¨ç¤ºçš„æ–¹æ³•ï¼ŒåŒ…æ‹¬ç¨ å¯†åŒ–å’Œç®€åŒ–ç­–ç•¥ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>å¯¹é«˜æ–¯è¡¨ç¤ºåœ¨ç‚¹äº‘ä¸­çš„ä½æ•ˆç©ºé—´åˆ†å¸ƒè¿›è¡Œåˆ†æã€‚</li><li>å¼•å…¥é«˜æ–¯åˆ†å‰²ã€æ·±åº¦é‡æ–°åˆå§‹åŒ–ç­‰ç¨ å¯†åŒ–ç­–ç•¥ã€‚</li><li>æå‡ºé«˜æ–¯äºŒå€¼åŒ–ã€é‡‡æ ·ç­‰ç®€åŒ–æ–¹æ³•ã€‚</li><li>ä¼˜åŒ–é«˜æ–¯åˆ†å¸ƒçš„ç©ºé—´ä½ç½®ï¼Œæé«˜æ¸²æŸ“è´¨é‡ã€‚</li><li>å‡å°‘èµ„æºæ¶ˆè€—å’Œå­˜å‚¨å‹ç¼©ã€‚</li><li>Mini-Splattingæ–¹æ³•ä¸å…‰æ …åŒ–ç®¡çº¿æ— ç¼é›†æˆã€‚</li><li>ä¸ºåŸºäºé«˜æ–¯å…‰æ …åŒ–çš„ç ”ç©¶æä¾›æœ‰åŠ›åŸºçº¿ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>è®ºæ–‡æ ‡é¢˜ï¼šå¾®å‹å–·æº…ï¼šä½¿ç”¨æœ‰é™æ•°é‡çš„é«˜æ–¯ä½“è¡¨ç¤ºåœºæ™¯</li><li>ä½œè€…ï¼šæ–¹å¹¿é©°ï¼Œç‹ç‚³</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šé¦™æ¸¯ç†å·¥å¤§å­¦</li><li>å…³é”®è¯ï¼šé«˜æ–¯å–·æº…ï¼Œç‚¹äº‘ï¼Œåœºæ™¯è¡¨ç¤º</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.14166   Github ä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šé«˜æ–¯å–·æº…ï¼ˆ3DGSï¼‰åœ¨æ²‰æµ¸å¼æ¸²æŸ“å’Œ 3D é‡å»ºç­‰åº”ç”¨ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œ3DGS ä½¿ç”¨æ•°ç™¾ä¸‡ä¸ªæ¤­åœ†é«˜æ–¯ä½“è¿›è¡Œåœºæ™¯å»ºæ¨¡ï¼Œå¯¼è‡´æ¨¡å‹æ€§èƒ½å—é™äºé«˜æ–¯è¡¨ç¤ºçš„ç©ºé—´åˆ†å¸ƒä¸é«˜æ•ˆã€‚   ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•å’Œé—®é¢˜ï¼šä¼ ç»Ÿçš„ 3DGS æ–¹æ³•ç›´æ¥ä½¿ç”¨é«˜æ–¯ä½“è¡¨ç¤ºåœºæ™¯ï¼Œä½†è¿™ç§è¡¨ç¤ºæ–¹å¼çš„ç©ºé—´åˆ†å¸ƒä¸å‡åŒ€ï¼Œå¯¼è‡´æ¸²æŸ“è´¨é‡ã€èµ„æºæ¶ˆè€—å’Œå­˜å‚¨å‹ç¼©æ–¹é¢å­˜åœ¨é—®é¢˜ã€‚   ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºå¾®å‹å–·æº…æ–¹æ³•ï¼Œé€šè¿‡æ¨¡ç³Šåˆ†å‰²ã€æ·±åº¦é‡æ–°åˆå§‹åŒ–ã€é«˜æ–¯äºŒå€¼åŒ–å’Œé‡‡æ ·ç­‰ç­–ç•¥ï¼Œå¯¹é«˜æ–¯ä½“è¿›è¡Œå¯†é›†åŒ–å’Œç®€åŒ–ï¼Œé‡æ–°ç»„ç»‡é«˜æ–¯ä½“åœ¨ç©ºé—´ä¸­çš„ä½ç½®ï¼Œä»è€Œæ”¹å–„æ¨¡å‹æ€§èƒ½ã€‚   ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šå¾®å‹å–·æº…æ–¹æ³•åœ¨å„ç§æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•ä¸­ï¼Œåœ¨æ¸²æŸ“è´¨é‡ã€èµ„æºæ¶ˆè€—å’Œå­˜å‚¨å‹ç¼©æ–¹é¢å‡å–å¾—äº†æ˜¾è‘—æå‡ã€‚å®ƒä¸åŸå§‹å…‰æ …åŒ–ç®¡é“æ— ç¼é›†æˆï¼Œä¸ºåŸºäºé«˜æ–¯å–·æº…çš„ç ”ç©¶æä¾›äº†åšå®çš„åŸºç¡€ã€‚</li></ol><p>7.æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼šé‡‡ç”¨æ¨¡ç³Šåˆ†å‰²å’Œæ·±åº¦é‡æ–°åˆå§‹åŒ–ç­–ç•¥è¿›è¡Œé«˜æ–¯ä½“å¯†é›†åŒ–ï¼›ï¼ˆ2ï¼‰ï¼šä½¿ç”¨é«˜æ–¯äºŒå€¼åŒ–æŠ€æœ¯å»é™¤ä¸ä¸å…‰çº¿ç›¸äº¤çš„é«˜æ–¯ä½“ï¼›ï¼ˆ3ï¼‰ï¼šåº”ç”¨é‡è¦æ€§åŠ æƒé‡‡æ ·æ–¹æ³•ï¼Œæ ¹æ®åœºæ™¯å‡ ä½•ç»“æ„å¯¹é«˜æ–¯ä½“è¿›è¡Œé‡‡æ ·ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§å¾®å‹å–·æº…æ–¹æ³•ï¼Œé€šè¿‡å¯¹é«˜æ–¯ä½“çš„å¯†é›†åŒ–å’Œç®€åŒ–ï¼Œé‡æ–°ç»„ç»‡é«˜æ–¯ä½“åœ¨ç©ºé—´ä¸­çš„ä½ç½®ï¼Œä»è€Œæ”¹å–„æ¨¡å‹æ€§èƒ½ã€‚è¯¥æ–¹æ³•åœ¨å„ç§æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•ä¸­ï¼Œåœ¨æ¸²æŸ“è´¨é‡ã€èµ„æºæ¶ˆè€—å’Œå­˜å‚¨å‹ç¼©æ–¹é¢å‡å–å¾—äº†æ˜¾è‘—æå‡ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºæ¨¡ç³Šåˆ†å‰²å’Œæ·±åº¦é‡æ–°åˆå§‹åŒ–ç­–ç•¥ï¼Œè¿›è¡Œé«˜æ–¯ä½“å¯†é›†åŒ–ã€‚</li><li>ä½¿ç”¨é«˜æ–¯äºŒå€¼åŒ–æŠ€æœ¯å»é™¤ä¸ä¸å…‰çº¿ç›¸äº¤çš„é«˜æ–¯ä½“ã€‚</li><li>åº”ç”¨é‡è¦æ€§åŠ æƒé‡‡æ ·æ–¹æ³•ï¼Œæ ¹æ®åœºæ™¯å‡ ä½•ç»“æ„å¯¹é«˜æ–¯ä½“è¿›è¡Œé‡‡æ ·ã€‚æ€§èƒ½ï¼š</li><li>åœ¨æ¸²æŸ“è´¨é‡ã€èµ„æºæ¶ˆè€—å’Œå­˜å‚¨å‹ç¼©æ–¹é¢å‡å–å¾—äº†æ˜¾è‘—æå‡ã€‚</li><li>ä¸åŸå§‹å…‰æ …åŒ–ç®¡é“æ— ç¼é›†æˆã€‚å·¥ä½œé‡ï¼š</li><li>è¯¥æ–¹æ³•çš„å®ç°ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦å¯¹é«˜æ–¯ä½“è¿›è¡Œå¯†é›†åŒ–å’Œç®€åŒ–å¤„ç†ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-b424fae4f546a60e73778d75dfc7b376.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0fc09c12d533d7a7d87fd0e047693c65.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3f805062f4ca950b3106067ce9bd46db.jpg" align="middle"></details><h2 id="RadSplat-Radiance-Field-Informed-Gaussian-Splatting-for-Robust-Real-Time-Rendering-with-900-FPS"><a href="#RadSplat-Radiance-Field-Informed-Gaussian-Splatting-for-Robust-Real-Time-Rendering-with-900-FPS" class="headerlink" title="RadSplat: Radiance Field-Informed Gaussian Splatting for Robust   Real-Time Rendering with 900+ FPS"></a>RadSplat: Radiance Field-Informed Gaussian Splatting for Robust   Real-Time Rendering with 900+ FPS</h2><p><strong>Authors:Michael Niemeyer, Fabian Manhardt, Marie-Julie Rakotosaona, Michael Oechsle, Daniel Duckworth, Rama Gosula, Keisuke Tateno, John Bates, Dominik Kaeser, Federico Tombari</strong></p><p>Recent advances in view synthesis and real-time rendering have achieved photorealistic quality at impressive rendering speeds. While Radiance Field-based methods achieve state-of-the-art quality in challenging scenarios such as in-the-wild captures and large-scale scenes, they often suffer from excessively high compute requirements linked to volumetric rendering. Gaussian Splatting-based methods, on the other hand, rely on rasterization and naturally achieve real-time rendering but suffer from brittle optimization heuristics that underperform on more challenging scenes. In this work, we present RadSplat, a lightweight method for robust real-time rendering of complex scenes. Our main contributions are threefold. First, we use radiance fields as a prior and supervision signal for optimizing point-based scene representations, leading to improved quality and more robust optimization. Next, we develop a novel pruning technique reducing the overall point count while maintaining high quality, leading to smaller and more compact scene representations with faster inference speeds. Finally, we propose a novel test-time filtering approach that further accelerates rendering and allows to scale to larger, house-sized scenes. We find that our method enables state-of-the-art synthesis of complex captures at 900+ FPS. </p><p><a href="http://arxiv.org/abs/2403.13806v1">PDF</a> Project page at <a href="https://m-niemeyer.github.io/radsplat/">https://m-niemeyer.github.io/radsplat/</a></p><p><strong>Summary</strong><br>åœºæ™¯è¡¨ç¤ºé€šè¿‡ç»“åˆä½“ç§¯æ¸²æŸ“ä¸åŸºäºæ …æ ¼åŒ–çš„ splatting æŠ€æœ¯çš„ä¼˜ç‚¹ï¼Œæä¾›äº†å¤æ‚åœºæ™¯çš„é²æ£’å®æ—¶æ¸²æŸ“ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ä½¿ç”¨è¾å°„åœºä½œä¸ºä¼˜åŒ–ç‚¹å¼åœºæ™¯è¡¨ç¤ºçš„å…ˆéªŒå’Œç›‘ç£ä¿¡å·ï¼Œæé«˜è´¨é‡å’Œé²æ£’æ€§ã€‚</li><li>å¼€å‘äº†ä¸€ç§æ–°çš„è£å‰ªæŠ€æœ¯ï¼Œåœ¨ä¿æŒé«˜æ¸²æŸ“è´¨é‡çš„å‰æä¸‹å‡å°‘ç‚¹æ•°é‡ï¼Œä»è€Œå®ç°æ›´å°ã€æ›´ç´§å‡‘çš„åœºæ™¯è¡¨ç¤ºï¼Œå¹¶æå‡æ¨æ–­é€Ÿåº¦ã€‚</li><li>æå‡ºäº†ä¸€ç§æ–°çš„æµ‹è¯•æ—¶è¿‡æ»¤æ–¹æ³•ï¼Œè¿›ä¸€æ­¥åŠ é€Ÿæ¸²æŸ“ï¼Œå¹¶æ”¯æŒæ‰©å±•åˆ°æ›´å¤§çš„ã€æˆ¿å±‹å¤§å°çš„åœºæ™¯ã€‚</li><li>è¯¥æ–¹æ³•å¯åœ¨ 900+ FPS ä¸‹åˆæˆå¤æ‚åœºæ™¯ï¼Œè¾¾åˆ°æœ€å…ˆè¿›çš„æ°´å¹³ã€‚</li><li>åœºæ™¯è¡¨ç¤ºèƒ½ä»¥äº¤äº’å¼å¸§ç‡å‘ˆç°å¯Œæœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ï¼Œå¦‚é‡å¤–è§‚æµ‹å’Œå¤§å‹åœºæ™¯ã€‚</li><li>åŸºäºæ …æ ¼åŒ–çš„ splatting æŠ€æœ¯å¯å®ç°å®æ—¶æ¸²æŸ“ï¼Œè€Œä½“ç§¯æ¸²æŸ“å¯æä¾›é«˜ä¿çœŸå›¾åƒã€‚</li><li>è¯¥æ–¹æ³•åœ¨è®¡ç®—è¦æ±‚å’Œæ¸²æŸ“è´¨é‡ä¹‹é—´å–å¾—äº†è‰¯å¥½çš„å¹³è¡¡ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šRadSplatï¼šåŸºäºè¾å°„åœºçš„é«˜æ–¯ç‚¹äº‘ç»˜åˆ¶ï¼Œå®ç°é²æ£’çš„å®æ—¶æ¸²æŸ“ï¼Œå¸§ç‡è¾¾åˆ° 900+FPS</li><li>ä½œè€…ï¼šMichael Niemeyerã€Fabian Manhardtã€Marie-Julie Rakotosaonaã€Michael Oechsleã€Daniel Duckworthã€Rama Gosulaã€Keisuke Tatenoã€John Batesã€Dominik Kaeserã€Federico Tombari</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šè°·æ­Œ</li><li>å…³é”®è¯ï¼šå®æ—¶æ¸²æŸ“ã€é«˜æ–¯ç‚¹äº‘ç»˜åˆ¶ã€ç¥ç»åœº</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://m-niemeyer.github.io/radsplat/</li><li><p>æ€»ç»“ï¼š(1)ï¼šç ”ç©¶èƒŒæ™¯ï¼šç¥ç»åœºæ˜¯ä¸€ç§æµè¡Œçš„ 3D è§†è§‰è¡¨ç¤ºå½¢å¼ï¼Œåœ¨è§†å›¾åˆæˆã€3D/4D é‡å»ºå’Œç”Ÿæˆå»ºæ¨¡ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚ä½†æ˜¯ï¼ŒåŸºäºç¥ç»åœºçš„è§†å›¾åˆæˆæ–¹æ³•é€šå¸¸éœ€è¦è¾ƒé«˜çš„è®¡ç®—èµ„æºï¼Œé™åˆ¶äº†å…¶å®æ—¶æ¸²æŸ“èƒ½åŠ›ã€‚åŸºäºé«˜æ–¯ç‚¹äº‘ç»˜åˆ¶çš„æ–¹æ³•å¯ä»¥å®ç°å®æ—¶æ¸²æŸ“ï¼Œä½†å…¶ä¼˜åŒ–å¯å‘å¼ç®—æ³•åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­è¡¨ç°ä¸ä½³ã€‚(2)ï¼šè¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šåŸºäºé«˜æ–¯ç‚¹äº‘ç»˜åˆ¶çš„æ–¹æ³•åœ¨ä¼˜åŒ–åœºæ™¯è¡¨ç¤ºæ—¶ç¼ºä¹å…ˆéªŒå’Œç›‘ç£ä¿¡å·ï¼Œå¯¼è‡´è´¨é‡è¾ƒå·®ä¸”ä¼˜åŒ–ä¸ç¨³å®šã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•ç¼ºä¹æœ‰æ•ˆçš„å‰ªææŠ€æœ¯ï¼Œå¯¼è‡´ç‚¹äº‘æ•°é‡è¿‡å¤šï¼Œå½±å“æ¨ç†é€Ÿåº¦ã€‚(3)ï¼šç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºçš„ RadSplat æ–¹æ³•åˆ©ç”¨è¾å°„åœºä½œä¸ºå…ˆéªŒå’Œç›‘ç£ä¿¡å·ï¼Œä¼˜åŒ–åŸºäºç‚¹çš„åœºæ™¯è¡¨ç¤ºï¼Œæé«˜äº†è´¨é‡å’Œä¼˜åŒ–é²æ£’æ€§ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¼€å‘äº†ä¸€ç§æ–°çš„å‰ªææŠ€æœ¯ï¼Œåœ¨ä¿æŒé«˜è´¨é‡çš„å‰æä¸‹å‡å°‘ç‚¹äº‘æ•°é‡ï¼Œä»è€Œè·å¾—æ›´å°ã€æ›´ç´§å‡‘çš„åœºæ™¯è¡¨ç¤ºï¼Œå¹¶æé«˜æ¨ç†é€Ÿåº¦ã€‚æœ€åï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æµ‹è¯•æ—¶æ»¤æ³¢æ–¹æ³•ï¼Œè¿›ä¸€æ­¥åŠ é€Ÿæ¸²æŸ“ï¼Œå¹¶æ”¯æŒæ‰©å±•åˆ°æ›´å¤§è§„æ¨¡çš„åœºæ™¯ã€‚(4)ï¼šæ–¹æ³•æ€§èƒ½ï¼šåœ¨å¤æ‚åœºæ™¯çš„åˆæˆä»»åŠ¡ä¸Šï¼ŒRadSplat æ–¹æ³•èƒ½å¤Ÿä»¥ 900+FPS çš„å¸§ç‡å®ç°é«˜è´¨é‡çš„åˆæˆï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚è¿™äº›æ€§èƒ½æŒ‡æ ‡æ”¯æŒäº†æœ¬æ–‡æå‡ºçš„æ–¹æ³•ç›®æ ‡ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰ç¥ç»è¾å°„åœºä½œä¸ºé²æ£’å…ˆéªŒï¼šåˆ©ç”¨ç¥ç»è¾å°„åœºä½œä¸ºå…ˆéªŒï¼Œä¼˜åŒ–ç‚¹äº‘è¡¨ç¤ºï¼Œæé«˜è´¨é‡å’Œä¼˜åŒ–é²æ£’æ€§ã€‚ï¼ˆ2ï¼‰è¾å°„åœºç›‘ç£ç‚¹äº‘ä¼˜åŒ–ï¼šåˆ©ç”¨è¾å°„åœºç›‘ç£åŸºäºç‚¹çš„ 3D é«˜æ–¯è¡¨ç¤ºçš„ä¼˜åŒ–ï¼Œæé«˜è´¨é‡å’Œç¨³å®šæ€§ã€‚ï¼ˆ3ï¼‰åŸºäºå°„çº¿è´¡çŒ®çš„å‰ªæï¼šæå‡ºä¸€ç§æ–°çš„å‰ªææŠ€æœ¯ï¼Œé€šè¿‡èšåˆé«˜æ–¯ç‚¹çš„å°„çº¿è´¡çŒ®ï¼Œå‡å°‘ç‚¹äº‘æ•°é‡ï¼Œè·å¾—æ›´ç´§å‡‘ã€é«˜è´¨é‡çš„åœºæ™¯è¡¨ç¤ºã€‚ï¼ˆ4ï¼‰è§†ç‚¹è¿‡æ»¤åŠ é€Ÿæ¸²æŸ“ï¼šå¯¹è¾“å…¥ç›¸æœºè¿›è¡Œèšç±»å’Œå¯è§æ€§è¿‡æ»¤ï¼Œè¿›ä¸€æ­¥åŠ é€Ÿæ¸²æŸ“é€Ÿåº¦ï¼Œæ”¯æŒæ‰©å±•åˆ°æ›´å¤§è§„æ¨¡çš„åœºæ™¯ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œæå‡ºäº†ä¸€ç§åä¸º RadSplat çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†è¾å°„åœºå’Œé«˜æ–¯ç‚¹äº‘ç»˜åˆ¶çš„ä¼˜åŠ¿ï¼Œå¯å¯¹å¤æ‚åœºæ™¯è¿›è¡Œé²æ£’çš„å®æ—¶æ¸²æŸ“ï¼Œå¸§ç‡å¯è¾¾ 900+ã€‚æˆ‘ä»¬è¯æ˜äº†ä½¿ç”¨è¾å°„åœºä½œä¸ºå…ˆéªŒå’Œç›‘ç£ä¿¡å·å¯æé«˜åŸºäºç‚¹çš„ 3D é«˜æ–¯è¡¨ç¤ºçš„ä¼˜åŒ–è´¨é‡å’Œç¨³å®šæ€§ã€‚æˆ‘ä»¬æ–°é¢–çš„å‰ªææŠ€æœ¯å¯ç”Ÿæˆæ›´ç´§å‡‘çš„åœºæ™¯ï¼Œç‚¹æ•°é‡æ˜¾è‘—å‡å°‘ï¼ŒåŒæ—¶æé«˜äº†è´¨é‡ã€‚æœ€åï¼Œæˆ‘ä»¬æ–°é¢–çš„æµ‹è¯•æ—¶æ»¤æ³¢è¿›ä¸€æ­¥æé«˜äº†æ¸²æŸ“é€Ÿåº¦ï¼Œä¸”ä¸ä¼šé™ä½è´¨é‡ã€‚æˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¸¸è§åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ•ˆæœï¼ŒåŒæ—¶æ¸²æŸ“é€Ÿåº¦æ¯”ä¹‹å‰çš„å·¥ä½œå¿« 3000 å€ã€‚è‡´è°¢ã€‚æˆ‘ä»¬è¦æ„Ÿè°¢ Georgios Kopanasã€Peter Zhizhinã€Peter Hedman å’Œ Jon Barron è¿›è¡Œå¯Œæœ‰æˆæ•ˆçš„è®¨è®ºå’Œå»ºè®®ï¼Œæ„Ÿè°¢ Cengiz Oztireli å®¡é˜…è‰ç¨¿ï¼Œæ„Ÿè°¢ Zhiwen Fan å’Œ Kevin Wang åˆ†äº«å…¶ä»–åŸºå‡†ç»“æœã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šxxxï¼›æ€§èƒ½ï¼šxxxï¼›å·¥ä½œé‡ï¼šxxxï¼›</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-f874a85240c4810c3301929f06cca843.jpg" align="middle"><img src="https://pica.zhimg.com/v2-862cef1f2c14ea159baa584203e8e499.jpg" align="middle"><img src="https://picx.zhimg.com/v2-aaa6b288e49e5a72ba138e5c7c9dc2a5.jpg" align="middle"></details><h2 id="GaussianFlow-Splatting-Gaussian-Dynamics-for-4D-Content-Creation"><a href="#GaussianFlow-Splatting-Gaussian-Dynamics-for-4D-Content-Creation" class="headerlink" title="GaussianFlow: Splatting Gaussian Dynamics for 4D Content Creation"></a>GaussianFlow: Splatting Gaussian Dynamics for 4D Content Creation</h2><p><strong>Authors:Quankai Gao, Qiangeng Xu, Zhe Cao, Ben Mildenhall, Wenchao Ma, Le Chen, Danhang Tang, Ulrich Neumann</strong></p><p>Creating 4D fields of Gaussian Splatting from images or videos is a challenging task due to its under-constrained nature. While the optimization can draw photometric reference from the input videos or be regulated by generative models, directly supervising Gaussian motions remains underexplored. In this paper, we introduce a novel concept, Gaussian flow, which connects the dynamics of 3D Gaussians and pixel velocities between consecutive frames. The Gaussian flow can be efficiently obtained by splatting Gaussian dynamics into the image space. This differentiable process enables direct dynamic supervision from optical flow. Our method significantly benefits 4D dynamic content generation and 4D novel view synthesis with Gaussian Splatting, especially for contents with rich motions that are hard to be handled by existing methods. The common color drifting issue that happens in 4D generation is also resolved with improved Guassian dynamics. Superior visual quality on extensive experiments demonstrates our methodâ€™s effectiveness. Quantitative and qualitative evaluations show that our method achieves state-of-the-art results on both tasks of 4D generation and 4D novel view synthesis. Project page: <a href="https://zerg-overmind.github.io/GaussianFlow.github.io/">https://zerg-overmind.github.io/GaussianFlow.github.io/</a> </p><p><a href="http://arxiv.org/abs/2403.12365v1">PDF</a> </p><p><strong>Summary</strong><br>é«˜æ–¯æµåŠ¨æ¦‚å¿µå°†3Dé«˜æ–¯åŠ¨åŠ›å­¦ä¸è¿ç»­å¸§çš„åƒç´ é€Ÿåº¦å…³è”ï¼Œå®ç°é«˜æ–¯è¿åŠ¨çš„ç›´æ¥åŠ¨æ€ç›‘ç®¡ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>å¼•å…¥é«˜æ–¯æµåŠ¨æ¦‚å¿µï¼Œè¿æ¥3Dé«˜æ–¯åŠ¨åŠ›å­¦å’Œåƒç´ é€Ÿåº¦ã€‚</li><li>é€šè¿‡å°†é«˜æ–¯åŠ¨åŠ›å­¦åµŒå…¥å›¾åƒç©ºé—´ï¼Œæœ‰æ•ˆè·å–é«˜æ–¯æµåŠ¨ã€‚</li><li>é«˜æ–¯æµåŠ¨å®ç°å…‰æµçš„ç›´æ¥åŠ¨æ€ç›‘ç®¡ã€‚</li><li>è¯¥æ–¹æ³•å¤§å¹…æå‡é«˜æ–¯æº…å°„åŠ¨æ€å†…å®¹ç”Ÿæˆå’Œæ–°è§†å›¾åˆæˆã€‚</li><li>è§£å†³4Dç”Ÿæˆä¸­å¸¸è§çš„é¢œè‰²æ¼‚ç§»é—®é¢˜ï¼Œå¹¶æ”¹å–„é«˜æ–¯åŠ¨åŠ›å­¦ã€‚</li><li>å¤§é‡å®éªŒè¡¨æ˜æ–¹æ³•çš„æ˜¾è‘—æ•ˆæœã€‚</li><li>åœ¨4Dç”Ÿæˆå’Œæ–°è§†å›¾åˆæˆä»»åŠ¡ä¸Šè¾¾åˆ°æœ€å…ˆè¿›æ°´å¹³ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><strong>æ ‡é¢˜ï¼š</strong>é«˜æ–¯æµï¼šç”¨äºé™„åŠ çš„ splatting é«˜æ–¯åŠ¨åŠ›å­¦</li><li><strong>ä½œè€…ï¼š</strong>Quan Kai, Qiangeng Xu</li><li><strong>ç¬¬ä¸€ä½œè€…å•ä½ï¼š</strong>å—åŠ å·å¤§å­¦</li><li><strong>å…³é”®è¯ï¼š</strong>4D å†…å®¹ç”Ÿæˆã€é«˜æ–¯ splattingã€å…‰æµã€åŠ¨æ€è¡¨å¾</li><li><strong>è®ºæ–‡é“¾æ¥ï¼š</strong>https://arxiv.org/abs/2403.12365</li><li><p><strong>æ‘˜è¦ï¼š</strong>   (1) <strong>ç ”ç©¶èƒŒæ™¯ï¼š</strong>ä»å›¾åƒæˆ–è§†é¢‘åˆ›å»ºé«˜æ–¯ splatting çš„ 4D åœºç”±äºå…¶æ¬ çº¦æŸçš„æ€§è´¨è€Œæå…·æŒ‘æˆ˜æ€§ã€‚è™½ç„¶ä¼˜åŒ–å¯ä»¥ä»è¾“å…¥è§†é¢‘ä¸­æå–å…‰åº¦å‚è€ƒæˆ–å—ç”Ÿæˆæ¨¡å‹çš„è°ƒèŠ‚ï¼Œä½†ç›´æ¥ç›‘ç£é«˜æ–¯è¿åŠ¨ä»ç„¶æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚   (2) <strong>è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼š</strong>ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå…‰åº¦æŸå¤±æˆ–ç”Ÿæˆæ¨¡å‹æ¥æŒ‡å¯¼é«˜æ–¯ splatting çš„ä¼˜åŒ–ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨å¤„ç†å…·æœ‰ä¸°å¯Œè¿åŠ¨çš„å†…å®¹æ—¶å¯èƒ½ä¸è¶³ï¼Œå¹¶ä¸”å®¹æ˜“å‡ºç°é¢œè‰²æ¼‚ç§»é—®é¢˜ã€‚   (3) <strong>æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼š</strong>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¦‚å¿µâ€”â€”é«˜æ–¯æµï¼Œå®ƒè¿æ¥äº†è¿ç»­å¸§ä¹‹é—´ 3D é«˜æ–¯å’Œåƒç´ é€Ÿåº¦çš„åŠ¨æ€ã€‚é«˜æ–¯æµå¯ä»¥é€šè¿‡å°†é«˜æ–¯åŠ¨åŠ›å­¦ splatting åˆ°å›¾åƒç©ºé—´ä¸­æœ‰æ•ˆè·å¾—ã€‚è¿™ä¸ªå¯å¾®åˆ†è¿‡ç¨‹èƒ½å¤Ÿä»å…‰æµä¸­è¿›è¡Œç›´æ¥åŠ¨æ€ç›‘ç£ã€‚   (4) <strong>æ–¹æ³•çš„æ€§èƒ½ï¼š</strong>è¯¥æ–¹æ³•æå¤§åœ°ä¿ƒè¿›äº†é«˜æ–¯ splatting çš„ 4D åŠ¨æ€å†…å®¹ç”Ÿæˆå’Œ 4D æ–°è§†å›¾åˆæˆï¼Œç‰¹åˆ«æ˜¯å¯¹äºç°æœ‰æ–¹æ³•éš¾ä»¥å¤„ç†çš„å…·æœ‰ä¸°å¯Œè¿åŠ¨çš„å†…å®¹ã€‚é€šè¿‡æ”¹è¿›çš„é«˜æ–¯åŠ¨åŠ›å­¦ï¼Œè¿˜è§£å†³äº† 4D ç”Ÿæˆä¸­å¸¸è§çš„é¢œè‰²æ¼‚ç§»é—®é¢˜ã€‚å¹¿æ³›å®éªŒä¸­çš„å“è¶Šè§†è§‰è´¨é‡è¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1): 3D é«˜æ–¯åˆå§‹åŒ–ï¼šä»è§†é¢‘ç¬¬ä¸€å¸§ä¸­åˆå§‹åŒ– 3D é«˜æ–¯ï¼Œä½¿ç”¨æ¸²æŸ“å›¾åƒå’Œè¾“å…¥å›¾åƒä¹‹é—´çš„å…‰åº¦ç›‘ç£å’Œ 3D æ„ŸçŸ¥ SDS ç›‘ç£ï¼›(2): é«˜æ–¯æµè®¡ç®—ï¼šå‡è®¾é«˜æ–¯è¿åŠ¨åœ¨å›¾åƒå¹³é¢çš„åˆ‡å‘åˆ†é‡å¾ˆå°ï¼Œå°† 3D é«˜æ–¯çš„ 2D æŠ•å½±è§†ä¸ºéšç€æ—¶é—´å˜å½¢ï¼ˆ2D å¹³ç§»ã€æ—‹è½¬å’Œç¼©æ”¾ï¼‰çš„ç›¸åŒ 2D é«˜æ–¯ï¼Œè®¡ç®—é«˜æ–¯æµï¼›(3): é«˜æ–¯æµç›‘ç£ï¼šè®¡ç®—å‚è€ƒè§†å›¾ä¸Šè¿ç»­ä¸¤å¸§ä¹‹é—´çš„é«˜æ–¯æµï¼Œå¹¶ä¸è¾“å…¥è§†é¢‘çš„é¢„è®¡ç®—å…‰æµè¿›è¡ŒåŒ¹é…ï¼Œé€šè¿‡åŒ¹é…è¯¯å·®åå‘ä¼ æ’­æ¢¯åº¦ï¼Œä¼˜åŒ–é«˜æ–¯åŠ¨åŠ›å­¦ï¼›(4): 4D å†…å®¹ç”Ÿæˆï¼šä½¿ç”¨ä¼˜åŒ–åçš„é«˜æ–¯åŠ¨åŠ›å­¦ splatting åˆ°å›¾åƒç©ºé—´ä¸­ï¼Œé€šè¿‡å…‰åº¦æŸå¤±å’Œ SDS æŸå¤±ç›‘ç£ï¼Œç”Ÿæˆå…·æœ‰è‡ªç„¶å¹³æ»‘è¿åŠ¨çš„ 4D é«˜æ–¯åœºã€‚</p></li><li><p>ç»“è®ºï¼š(1): æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§æ–°é¢–çš„é«˜æ–¯æµæ¦‚å¿µï¼Œé€šè¿‡å°†é«˜æ–¯åŠ¨åŠ›å­¦splattingåˆ°å›¾åƒç©ºé—´ä¸­ï¼Œå®ç°äº†ä»å…‰æµä¸­è¿›è¡Œç›´æ¥åŠ¨æ€ç›‘ç£ï¼Œæå¤§åœ°ä¿ƒè¿›äº†4DåŠ¨æ€å†…å®¹ç”Ÿæˆå’Œ4Dæ–°è§†å›¾åˆæˆã€‚(2): åˆ›æ–°ç‚¹ï¼š</p></li><li>é«˜æ–¯æµæ¦‚å¿µçš„æå‡ºï¼Œå®ç°äº†ä»å…‰æµä¸­è¿›è¡Œç›´æ¥åŠ¨æ€ç›‘ç£ã€‚</li><li>æ”¹è¿›çš„é«˜æ–¯åŠ¨åŠ›å­¦ï¼Œè§£å†³äº†4Dç”Ÿæˆä¸­çš„é¢œè‰²æ¼‚ç§»é—®é¢˜ã€‚</li><li>é€‚ç”¨äºå…·æœ‰ä¸°å¯Œè¿åŠ¨çš„å†…å®¹ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥å¤„ç†ã€‚Performanceï¼š</li><li>åœ¨4DåŠ¨æ€å†…å®¹ç”Ÿæˆå’Œ4Dæ–°è§†å›¾åˆæˆæ–¹é¢å–å¾—äº†å“è¶Šçš„è§†è§‰è´¨é‡ã€‚</li><li>è§£å†³äº†4Dç”Ÿæˆä¸­å¸¸è§çš„é¢œè‰²æ¼‚ç§»é—®é¢˜ã€‚Workloadï¼š</li><li>æ–¹æ³•å¤æ‚ï¼Œéœ€è¦é«˜æ€§èƒ½è®¡ç®—èµ„æºã€‚</li><li>éœ€è¦é¢„å…ˆè®¡ç®—å…‰æµï¼Œå¢åŠ äº†è®¡ç®—é‡ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-ed6d6808f2e5c2502662da7aff5fadc7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9cfe66ac12504862ee65946ded5ed4ea.jpg" align="middle"></details><h2 id="VideoMV-Consistent-Multi-View-Generation-Based-on-Large-Video-Generative-Model"><a href="#VideoMV-Consistent-Multi-View-Generation-Based-on-Large-Video-Generative-Model" class="headerlink" title="VideoMV: Consistent Multi-View Generation Based on Large Video   Generative Model"></a>VideoMV: Consistent Multi-View Generation Based on Large Video   Generative Model</h2><p><strong>Authors:Qi Zuo, Xiaodong Gu, Lingteng Qiu, Yuan Dong, Zhengyi Zhao, Weihao Yuan, Rui Peng, Siyu Zhu, Zilong Dong, Liefeng Bo, Qixing Huang</strong></p><p>Generating multi-view images based on text or single-image prompts is a critical capability for the creation of 3D content. Two fundamental questions on this topic are what data we use for training and how to ensure multi-view consistency. This paper introduces a novel framework that makes fundamental contributions to both questions. Unlike leveraging images from 2D diffusion models for training, we propose a dense consistent multi-view generation model that is fine-tuned from off-the-shelf video generative models. Images from video generative models are more suitable for multi-view generation because the underlying network architecture that generates them employs a temporal module to enforce frame consistency. Moreover, the video data sets used to train these models are abundant and diverse, leading to a reduced train-finetuning domain gap. To enhance multi-view consistency, we introduce a 3D-Aware Denoising Sampling, which first employs a feed-forward reconstruction module to get an explicit global 3D model, and then adopts a sampling strategy that effectively involves images rendered from the global 3D model into the denoising sampling loop to improve the multi-view consistency of the final images. As a by-product, this module also provides a fast way to create 3D assets represented by 3D Gaussians within a few seconds. Our approach can generate 24 dense views and converges much faster in training than state-of-the-art approaches (4 GPU hours versus many thousand GPU hours) with comparable visual quality and consistency. By further fine-tuning, our approach outperforms existing state-of-the-art methods in both quantitative metrics and visual effects. Our project page is aigc3d.github.io/VideoMV. </p><p><a href="http://arxiv.org/abs/2403.12010v1">PDF</a> Project page: aigc3d.github.io/VideoMV/</p><p><strong>Summary</strong><br>æ–‡æœ¬ç”Ÿæˆå¤šè§†è§’å›¾åƒçš„å…³é”®åœ¨äºè®­ç»ƒæ•°æ®å’Œå¤šè§†è§’ä¸€è‡´æ€§çš„ç¡®ä¿ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œé€šè¿‡è§†é¢‘ç”Ÿæˆæ¨¡å‹å¾®è°ƒå’Œ3Dæ„ŸçŸ¥é™å™ªé‡‡æ ·æ¥è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>åˆ©ç”¨è§†é¢‘ç”Ÿæˆæ¨¡å‹å›¾åƒè¿›è¡Œå¤šè§†è§’ç”Ÿæˆï¼Œå› å…¶ç½‘ç»œæ¶æ„ä¸­æ—¶é—´æ¨¡å—ä¿è¯äº†å¸§ä¸€è‡´æ€§ã€‚</li><li>è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„è®­ç»ƒæ•°æ®é›†ä¸°å¯Œä¸”å¤šæ ·ï¼Œå‡å°‘äº†è®­ç»ƒå¾®è°ƒåŸŸå·®è·ã€‚</li><li>æå‡º3Dæ„ŸçŸ¥é™å™ªé‡‡æ ·ï¼Œä½¿ç”¨å‰é¦ˆé‡å»ºæ¨¡å—è·å¾—å…¨å±€3Dæ¨¡å‹ï¼Œé‡‡æ ·ç­–ç•¥å°†å…¨å±€3Dæ¨¡å‹æ¸²æŸ“å›¾åƒçº³å…¥é™å™ªé‡‡æ ·å¾ªç¯ï¼Œå¢å¼ºå¤šè§†è§’ä¸€è‡´æ€§ã€‚</li><li>è¯¥æ¨¡å—è¿˜å¯å¿«é€Ÿåˆ›å»ºç”±3Dé«˜æ–¯è¡¨ç¤ºçš„3Dèµ„äº§ã€‚</li><li>è¯¥æ–¹æ³•èƒ½ç”Ÿæˆ24ä¸ªå¯†é›†è§†è§’ï¼Œè®­ç»ƒæ”¶æ•›é€Ÿåº¦æ˜æ˜¾å¿«äºç°æœ‰æ–¹æ³•ï¼Œä¸”åœ¨è§†è§‰è´¨é‡å’Œä¸€è‡´æ€§ä¸Šå¯æ¯”æ‹Ÿã€‚</li><li>è¿›ä¸€æ­¥å¾®è°ƒåï¼Œè¯¥æ–¹æ³•åœ¨å®šé‡æŒ‡æ ‡å’Œè§†è§‰æ•ˆæœä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šVideoMVï¼šä¸€è‡´çš„å¤šè§†å›¾ç”Ÿæˆ</li><li>ä½œè€…ï¼šQi Zuoã€Yifan Jiangã€Yihao Liuã€Weidi Xieã€Lei Zhouã€Li Erran Li</li><li>å•ä½ï¼šæ— </li><li>å…³é”®è¯ï¼šå¤šè§†å›¾ç”Ÿæˆã€æ–‡æœ¬åˆ°è§†é¢‘ã€å›¾åƒåˆ°è§†é¢‘ã€ä¸€è‡´æ€§</li><li>è®ºæ–‡é“¾æ¥ï¼šæ— ï¼ŒGithub ä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šå¤šè§†å›¾ç”Ÿæˆæ˜¯åˆ›å»º 3D å†…å®¹çš„å…³é”®èƒ½åŠ›ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä½¿ç”¨ 2D æ‰©æ•£æ¨¡å‹ä¸­çš„å›¾åƒè¿›è¡Œè®­ç»ƒï¼Œä½†è¿™äº›å›¾åƒç¼ºä¹æ—¶é—´ä¸€è‡´æ€§ï¼Œä¸”è®­ç»ƒå’Œå¾®è°ƒä¹‹é—´å­˜åœ¨åŸŸå·®å¼‚ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šç°æœ‰æ–¹æ³•å­˜åœ¨è®­ç»ƒæ…¢ã€å¤šè§†å›¾ä¸€è‡´æ€§å·®ç­‰é—®é¢˜ã€‚ï¼ˆ3ï¼‰è®ºæ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œä»ç°æˆçš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸­å¾®è°ƒï¼Œå¹¶å¼•å…¥äº†ä¸€ç§ 3D æ„ŸçŸ¥å»å™ªé‡‡æ ·ï¼Œé€šè¿‡æ˜¾å¼è·å–å…¨å±€ 3D æ¨¡å‹å¹¶å°†å…¶èå…¥å»å™ªé‡‡æ ·å¾ªç¯ï¼Œæ¥å¢å¼ºå¤šè§†å›¾ä¸€è‡´æ€§ã€‚ï¼ˆ4ï¼‰å®éªŒç»“æœï¼šè¯¥æ–¹æ³•å¯åœ¨ 4 ä¸ª GPU å°æ—¶å†…ç”Ÿæˆ 24 ä¸ªå¯†é›†è§†å›¾ï¼Œæ¯”ç°æœ‰æ–¹æ³•å¿«å¾—å¤šï¼ˆæ•°åƒä¸ª GPU å°æ—¶ï¼‰ï¼Œä¸”å…·æœ‰å¯æ¯”çš„è§†è§‰è´¨é‡å’Œä¸€è‡´æ€§ã€‚è¿›ä¸€æ­¥å¾®è°ƒåï¼Œè¯¥æ–¹æ³•åœ¨å®šé‡æŒ‡æ ‡å’Œè§†è§‰æ•ˆæœä¸Šéƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li></ol><p>7.Methods:(1):ä»ç°æˆçš„è§†é¢‘ç”Ÿæˆæ¨¡å‹å¾®è°ƒï¼Œåˆ©ç”¨å…¶æ•è·æ—¶é—´ä¸€è‡´æ€§çš„èƒ½åŠ›ï¼›(2):å¼•å…¥3Dæ„ŸçŸ¥å»å™ªé‡‡æ ·ï¼Œæ˜¾å¼è·å–å…¨å±€3Dæ¨¡å‹ï¼Œå¹¶å°†å…¶èå…¥å»å™ªé‡‡æ ·å¾ªç¯ï¼Œå¢å¼ºå¤šè§†å›¾ä¸€è‡´æ€§ï¼›(3):é€šè¿‡ä¼˜åŒ–é‡‡æ ·ç­–ç•¥å’Œè®­ç»ƒç›®æ ‡ï¼Œæé«˜ç”Ÿæˆæ•ˆç‡å’Œä¸€è‡´æ€§ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ä»ç°æˆçš„è§†é¢‘ç”Ÿæˆæ¨¡å‹å¾®è°ƒï¼Œå¹¶å¼•å…¥3Dæ„ŸçŸ¥å»å™ªé‡‡æ ·çš„æ–¹æ³•ï¼Œå®ç°äº†å¤šè§†å›¾ç”Ÿæˆçš„é«˜æ•ˆå’Œä¸€è‡´æ€§ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š<ul><li>ä»ç°æˆçš„è§†é¢‘ç”Ÿæˆæ¨¡å‹å¾®è°ƒï¼Œåˆ©ç”¨å…¶æ•è·æ—¶é—´ä¸€è‡´æ€§çš„èƒ½åŠ›ã€‚</li><li>å¼•å…¥3Dæ„ŸçŸ¥å»å™ªé‡‡æ ·ï¼Œæ˜¾å¼è·å–å…¨å±€3Dæ¨¡å‹ï¼Œå¢å¼ºå¤šè§†å›¾ä¸€è‡´æ€§ã€‚</li><li>é€šè¿‡ä¼˜åŒ–é‡‡æ ·ç­–ç•¥å’Œè®­ç»ƒç›®æ ‡ï¼Œæé«˜ç”Ÿæˆæ•ˆç‡å’Œä¸€è‡´æ€§ã€‚æ€§èƒ½ï¼š</li><li>å¯åœ¨4ä¸ªGPUå°æ—¶å†…ç”Ÿæˆ24ä¸ªå¯†é›†è§†å›¾ï¼Œæ¯”ç°æœ‰æ–¹æ³•å¿«å¾—å¤šï¼ˆæ•°åƒä¸ªGPUå°æ—¶ï¼‰ã€‚</li><li>å…·æœ‰å¯æ¯”çš„è§†è§‰è´¨é‡å’Œä¸€è‡´æ€§ã€‚</li><li>è¿›ä¸€æ­¥å¾®è°ƒåï¼Œåœ¨å®šé‡æŒ‡æ ‡å’Œè§†è§‰æ•ˆæœä¸Šéƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å·¥ä½œé‡ï¼š</li><li>è®ºæ–‡æ²¡æœ‰æä¾›è®ºæ–‡é“¾æ¥å’ŒGithubä»£ç é“¾æ¥ï¼Œä¸ä¾¿äºè¯»è€…å¤ç°å’Œè¿›ä¸€æ­¥ç ”ç©¶ã€‚</li></ul></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-6059ab2581e11d57004f65c073b5ab34.jpg" align="middle"><img src="https://pica.zhimg.com/v2-badb5404c700bc048521656d5d7650e7.jpg" align="middle"></details><h2 id="BAD-Gaussians-Bundle-Adjusted-Deblur-Gaussian-Splatting"><a href="#BAD-Gaussians-Bundle-Adjusted-Deblur-Gaussian-Splatting" class="headerlink" title="BAD-Gaussians: Bundle Adjusted Deblur Gaussian Splatting"></a>BAD-Gaussians: Bundle Adjusted Deblur Gaussian Splatting</h2><p><strong>Authors:Lingzhe Zhao, Peng Wang, Peidong Liu</strong></p><p>While neural rendering has demonstrated impressive capabilities in 3D scene reconstruction and novel view synthesis, it heavily relies on high-quality sharp images and accurate camera poses. Numerous approaches have been proposed to train Neural Radiance Fields (NeRF) with motion-blurred images, commonly encountered in real-world scenarios such as low-light or long-exposure conditions. However, the implicit representation of NeRF struggles to accurately recover intricate details from severely motion-blurred images and cannot achieve real-time rendering. In contrast, recent advancements in 3D Gaussian Splatting achieve high-quality 3D scene reconstruction and real-time rendering by explicitly optimizing point clouds as Gaussian spheres.   In this paper, we introduce a novel approach, named BAD-Gaussians (Bundle Adjusted Deblur Gaussian Splatting), which leverages explicit Gaussian representation and handles severe motion-blurred images with inaccurate camera poses to achieve high-quality scene reconstruction. Our method models the physical image formation process of motion-blurred images and jointly learns the parameters of Gaussians while recovering camera motion trajectories during exposure time.   In our experiments, we demonstrate that BAD-Gaussians not only achieves superior rendering quality compared to previous state-of-the-art deblur neural rendering methods on both synthetic and real datasets but also enables real-time rendering capabilities.   Our project page and source code is available at <a href="https://lingzhezhao.github.io/BAD-Gaussians/">https://lingzhezhao.github.io/BAD-Gaussians/</a> </p><p><a href="http://arxiv.org/abs/2403.11831v2">PDF</a> Project Page and Source Code:   <a href="https://lingzhezhao.github.io/BAD-Gaussians/">https://lingzhezhao.github.io/BAD-Gaussians/</a></p><p><strong>æ‘˜è¦</strong><br>é«˜æ–¯çš„æ··åˆè¡¨ç¤ºæ•è·è¿åŠ¨æ¨¡ç³Šï¼Œé€šè¿‡ä¼˜åŒ–ç›¸æœºè¿åŠ¨å’Œæ˜¾å¼è¡¨ç¤ºæ¥å®ç°é«˜å“è´¨åœºæ™¯é‡å»ºã€‚</p><p><strong>è¦ç‚¹</strong></p><ul><li>ç¥ç»æ¸²æŸ“å¯¹æ¸…æ™°å›¾åƒå’Œå‡†ç¡®ç›¸æœºä½å§¿ä¾èµ–å¾ˆé«˜ã€‚</li><li>å¤§å¤šæ•°æ–¹æ³•æ— æ³•ä»ä¸¥é‡è¿åŠ¨æ¨¡ç³Šå›¾åƒä¸­å‡†ç¡®æ¢å¤ç»†èŠ‚ï¼Œä¹Ÿæ— æ³•å®æ—¶æ¸²æŸ“ã€‚</li><li>3D é«˜æ–¯ä½“æ¸²æŸ“é€šè¿‡ä¼˜åŒ–é«˜æ–¯çƒä½“å®ç°é«˜è´¨é‡ 3D åœºæ™¯é‡å»ºå’Œå®æ—¶æ¸²æŸ“ã€‚</li><li>BAD-Gaussians åˆ©ç”¨é«˜æ–¯è¡¨ç¤ºï¼Œå¤„ç†ä¸¥é‡è¿åŠ¨æ¨¡ç³Šå›¾åƒå’Œä¸å‡†ç¡®ç›¸æœºä½å§¿ã€‚</li><li>è¯¥æ–¹æ³•æ¨¡æ‹Ÿè¿åŠ¨æ¨¡ç³Šå›¾åƒçš„ç‰©ç†æˆåƒè¿‡ç¨‹ï¼Œå¹¶è”åˆå­¦ä¹ é«˜æ–¯å‚æ•°å’Œæ¢å¤æ›å…‰æ—¶é—´å†…çš„ç›¸æœºè¿åŠ¨è½¨è¿¹ã€‚</li><li>BAD-Gaussians åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šä¼˜äºæœ€å…ˆè¿›çš„å»æ¨¡ç³Šç¥ç»æ¸²æŸ“æ–¹æ³•ï¼Œå¹¶æ”¯æŒå®æ—¶æ¸²æŸ“ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šBAD-Gaussiansï¼šåŸºäºæŸè°ƒæ•´çš„å»æ¨¡ç³Šé«˜æ–¯ä½“æ¸²æŸ“</li><li>ä½œè€…ï¼šLingzhe Zhao, Peng Wang, Peidong Liu</li><li>å•ä½ï¼šæ— </li><li>å…³é”®è¯ï¼š3D é«˜æ–¯ä½“æ¸²æŸ“ Â· å»æ¨¡ç³Š Â· æŸè°ƒæ•´ Â· å¯å¾®æ¸²æŸ“</li><li>é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.11831</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»æ¸²æŸ“åœ¨ 3D åœºæ™¯é‡å»ºå’Œæ–°è§†è§’åˆæˆæ–¹é¢è¡¨ç°å‡ºäº†æƒŠäººçš„èƒ½åŠ›ï¼Œä½†å®ƒä¸¥é‡ä¾èµ–äºé«˜è´¨é‡çš„é”åˆ©å›¾åƒå’Œå‡†ç¡®çš„ç›¸æœºä½å§¿ã€‚è®¸å¤šæ–¹æ³•å·²è¢«æå‡ºç”¨äºè®­ç»ƒç¥ç»è¾å°„åœº (NeRF)ï¼Œä»¥å¤„ç†è¿åŠ¨æ¨¡ç³Šå›¾åƒï¼Œè¿™åœ¨ç°å®ä¸–ç•Œåœºæ™¯ï¼ˆä¾‹å¦‚ä½å…‰ç…§æˆ–é•¿æ›å…‰æ¡ä»¶ï¼‰ä¸­å¾ˆå¸¸è§ã€‚ç„¶è€Œï¼ŒNeRF çš„éšå¼è¡¨ç¤ºéš¾ä»¥ä»ä¸¥é‡è¿åŠ¨æ¨¡ç³Šçš„å›¾åƒä¸­å‡†ç¡®æ¢å¤å¤æ‚ç»†èŠ‚ï¼Œå¹¶ä¸”æ— æ³•å®ç°å®æ—¶æ¸²æŸ“ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œ3D é«˜æ–¯ä½“æ¸²æŸ“ (3D-GS) çš„æœ€æ–°è¿›å±•é€šè¿‡å°†ç‚¹äº‘æ˜¾å¼ä¼˜åŒ–ä¸º 3D é«˜æ–¯ä½“æ¥å®ç°é«˜è´¨é‡çš„ 3D åœºæ™¯é‡å»ºå’Œå®æ—¶æ¸²æŸ“ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šåŸºäº NeRF çš„æ–¹æ³•å’Œ 3D-GS éƒ½ä¸¥é‡ä¾èµ–äºç²¾å¿ƒæ•æ‰çš„é”åˆ©å›¾åƒå’Œå‡†ç¡®é¢„å…ˆè®¡ç®—çš„ç›¸æœºä½å§¿ï¼Œé€šå¸¸ä» COLMAP è·å¾—ã€‚è¿åŠ¨æ¨¡ç³Šå›¾åƒæ˜¯ä¸€ç§å¸¸è§çš„å›¾åƒé€€åŒ–å½¢å¼ï¼Œé€šå¸¸åœ¨ä½å…‰ç…§æˆ–é•¿æ›å…‰æ¡ä»¶ä¸‹é‡åˆ°ï¼Œå®ƒä¼šæ˜¾ç€æŸå®³ NeRF å’Œ 3D-GS çš„æ€§èƒ½ã€‚NeRF å’Œ 3D-GS é¢ä¸´çš„è¿åŠ¨æ¨¡ç³Šå›¾åƒå¸¦æ¥çš„æŒ‘æˆ˜å¯ä»¥å½’å› äºä¸‰ä¸ªä¸»è¦å› ç´ ï¼šï¼ˆaï¼‰NeRF å’Œ 3D-GS ä¾èµ–äºé«˜è´¨é‡çš„é”åˆ©å›¾åƒè¿›è¡Œç›‘ç£ã€‚ç„¶è€Œï¼Œè¿åŠ¨æ¨¡ç³Šå›¾åƒè¿åäº†è¿™ä¸€å‡è®¾ï¼Œå¹¶ä¸”åœ¨å¤šè§†å›¾å¸§ä¹‹é—´è¡¨ç°å‡ºæ˜æ˜¾ä¸å‡†ç¡®çš„å¯¹åº”å‡ ä½•ï¼Œä»è€Œç»™ NeRF å’Œ 3D-GS çš„å‡†ç¡® 3D åœºæ™¯è¡¨ç¤ºå¸¦æ¥äº†é‡å¤§å›°éš¾ï¼›ï¼ˆbï¼‰å‡†ç¡®çš„ç›¸æœºä½å§¿å¯¹äºè®­ç»ƒ NeRF å’Œ 3D-GS è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä½¿ç”¨ COLMAP ä»å¤šè§†å›¾è¿åŠ¨æ¨¡ç³Šå›¾åƒä¸­æ¢å¤å‡†ç¡®çš„ä½å§¿å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ï¼ˆcï¼‰3D-GS éœ€è¦æ¥è‡ª COLMAP çš„ç¨€ç–äº‘ç‚¹ä½œä¸ºé«˜æ–¯ä½“çš„åˆå§‹åŒ–ã€‚å¤šè§†å›¾æ¨¡ç³Šå›¾åƒä¹‹é—´çš„ç‰¹å¾ä¸åŒ¹é…ä»¥åŠä½å§¿æ ¡å‡†ä¸­çš„ä¸å‡†ç¡®æ€§è¿›ä¸€æ­¥åŠ å‰§äº†è¿™ä¸ªé—®é¢˜ï¼Œå¯¼è‡´ COLMAP äº§ç”Ÿçš„äº‘ç‚¹æ›´å°‘ã€‚è¿™ä¸º 3D-GS å¼•å…¥äº†é¢å¤–çš„åˆå§‹åŒ–é—®é¢˜ã€‚å› æ­¤ï¼Œè¿™äº›å› ç´ å¯¼è‡´ 3D-GS åœ¨å¤„ç†è¿åŠ¨æ¨¡ç³Šå›¾åƒæ—¶æ€§èƒ½æ˜¾ç€ä¸‹é™ã€‚ï¼ˆ3ï¼‰æå‡ºçš„æ–¹æ³•ï¼šä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäº 3D-GS çš„ç¬¬ä¸€ä¸ªè¿åŠ¨å»æ¨¡ç³Šæ¡†æ¶ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸º BAD-Gaussiansã€‚æˆ‘ä»¬å°†è¿åŠ¨æ¨¡ç³Šçš„ç‰©ç†è¿‡ç¨‹çº³å…¥ 3D-GS çš„è®­ç»ƒä¸­ï¼Œé‡‡ç”¨æ ·æ¡å‡½æ•°æ¥è¡¨å¾ç›¸æœºåœ¨æ›å…‰æ—¶é—´å†…çš„è½¨è¿¹ã€‚åœ¨ BAD-Gaussians çš„è®­ç»ƒä¸­ï¼Œä½¿ç”¨ä»åœºæ™¯çš„é«˜æ–¯ä½“å¯¼å‡ºçš„æ¢¯åº¦ä¼˜åŒ–æ›å…‰æ—¶é—´å†…çš„ç›¸æœºè½¨è¿¹ï¼ŒåŒæ—¶è”åˆä¼˜åŒ–é«˜æ–¯ä½“æœ¬èº«ã€‚å…·ä½“æ¥è¯´ï¼Œæ¯ä¸ªè¿åŠ¨æ¨¡ç³Šå›¾åƒçš„è½¨è¿¹ç”±æ›å…‰æ—¶é—´å¼€å§‹å’Œç»“æŸæ—¶çš„åˆå§‹å’Œæœ€ç»ˆä½å§¿è¡¨ç¤ºã€‚å‡è®¾æ›å…‰æ—¶é—´é€šå¸¸å¾ˆçŸ­ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨åˆå§‹ä½å§¿å’Œæœ€ç»ˆä½å§¿ä¹‹é—´è¿›è¡Œæ’å€¼ä»¥è·å¾—æ²¿è½¨è¿¹çš„æ¯ä¸ªç›¸æœºä½å§¿ã€‚ä»è¿™ä¸ªè½¨è¿¹ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å°†åœºæ™¯çš„é«˜æ–¯ä½“æŠ•å½±åˆ°å›¾åƒå¹³é¢ä¸Šç”Ÿæˆä¸€ç³»åˆ—è™šæ‹Ÿé”åˆ©å›¾åƒã€‚ç„¶åå¯¹è¿™äº›è™šæ‹Ÿé”åˆ©å›¾åƒè¿›è¡Œå¹³å‡ä»¥åˆæˆæ¨¡ç³Šå›¾åƒï¼Œéµå¾ªç‰©ç†æ¨¡ç³Šè¿‡ç¨‹ã€‚æœ€åï¼Œé€šè¿‡å¯å¾®é«˜æ–¯å…‰æ …åŒ–ï¼Œé€šè¿‡æœ€å°åŒ–åˆæˆæ¨¡ç³Šå›¾åƒå’Œè¾“å…¥æ¨¡ç³Šå›¾åƒä¹‹é—´çš„å…‰åº¦è¯¯å·®æ¥ä¼˜åŒ–æ²¿è½¨è¿¹çš„é«˜æ–¯ä½“ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šæˆ‘ä»¬ä½¿ç”¨åˆæˆå’ŒçœŸå®æ•°æ®é›†è¯„ä¼°äº† BAD-Gaussiansã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBAD-Gaussians é€šè¿‡å°†è¿åŠ¨æ¨¡ç³Šå›¾åƒçš„å›¾åƒå½¢æˆè¿‡ç¨‹æ˜¾å¼çº³å…¥ 3D-GS çš„è®­ç»ƒä¸­ï¼Œä¼˜äºå…ˆå‰çš„éšå¼ç¥ç»æ¸²æŸ“æ–¹æ³•ï¼Œåœ¨å®æ—¶æ¸²æŸ“é€Ÿåº¦å’Œå“è¶Šçš„æ¸²æŸ“è´¨é‡æ–¹é¢å®ç°äº†æ›´å¥½çš„æ¸²æŸ“æ€§èƒ½ã€‚æ€»ä¹‹ï¼Œæˆ‘ä»¬çš„è´¡çŒ®å¯ä»¥æ¦‚è¿°å¦‚ä¸‹ï¼š- æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ä¸“é—¨é’ˆå¯¹è¿åŠ¨æ¨¡ç³Šå›¾åƒè®¾è®¡çš„ç…§åº¦æŸè°ƒæ•´å…¬å¼ï¼Œå®ç°äº† 3D é«˜æ–¯ä½“æ¸²æŸ“æ¡†æ¶å†…è¿åŠ¨æ¨¡ç³Šå›¾åƒçš„é¦–æ¬¡å®æ—¶æ¸²æŸ“æ€§èƒ½ï¼›- æˆ‘ä»¬å±•ç¤ºäº†è¿™ç§å…¬å¼å¦‚ä½•å®ç°ä»ä¸€ç»„è¿åŠ¨æ¨¡ç³Šå›¾åƒä¸­è·å–é«˜è´¨é‡ 3D åœºæ™¯è¡¨ç¤ºï¼›- æˆ‘ä»¬çš„æ–¹æ³•æˆåŠŸåœ°å»é™¤äº†ä¸¥é‡çš„è¿åŠ¨æ¨¡ç³Šå›¾åƒï¼Œåˆæˆäº†æ›´é«˜è´¨é‡çš„æ–°è§†è§’å›¾åƒï¼Œå¹¶å®ç°äº†å®æ—¶æ¸²æŸ“ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„éšå¼å»æ¨¡ç³Šæ¸²æŸ“æ–¹æ³•ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1): åŸºäº 3D-GSï¼Œå°†è¿åŠ¨æ¨¡ç³Šå›¾åƒçš„ç‰©ç†å½¢æˆè¿‡ç¨‹çº³å…¥è®­ç»ƒï¼Œé€šè¿‡æ ·æ¡å‡½æ•°è¡¨å¾ç›¸æœºåœ¨æ›å…‰æ—¶é—´å†…çš„è½¨è¿¹ï¼Œå¹¶é€šè¿‡ä¼˜åŒ–è½¨è¿¹å’Œé«˜æ–¯ä½“æ¥æ¢å¤å‡†ç¡®çš„ 3D åœºæ™¯è¡¨ç¤ºï¼›(2): æå‡ºäº†ä¸€ç§é’ˆå¯¹è¿åŠ¨æ¨¡ç³Šå›¾åƒè®¾è®¡çš„ç…§åº¦æŸè°ƒæ•´å…¬å¼ï¼Œé€šè¿‡æœ€å°åŒ–è¾“å…¥æ¨¡ç³Šå›¾åƒå’Œåˆæˆæ¨¡ç³Šå›¾åƒä¹‹é—´çš„å…‰åº¦è¯¯å·®æ¥ä¼˜åŒ–æ²¿è½¨è¿¹çš„é«˜æ–¯ä½“ï¼›(3): é€šè¿‡å¯å¾®é«˜æ–¯å…‰æ …åŒ–ï¼Œä»è¿åŠ¨æ¨¡ç³Šå›¾åƒä¸­å®æ—¶æ¸²æŸ“é«˜è´¨é‡çš„æ–°è§†è§’å›¾åƒã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ç¬¬ä¸€ä¸ªä»è¿åŠ¨æ¨¡ç³Šå›¾åƒé›†åˆä¸­å­¦ä¹ é«˜æ–¯ä½“æ¸²æŸ“çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨å‡†ç¡®çš„ç›¸æœºä½å§¿ä¸‹å®ç°äº†è¿åŠ¨æ¨¡ç³Šå›¾åƒçš„é¦–æ¬¡å®æ—¶æ¸²æŸ“æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç®¡é“å¯ä»¥è”åˆä¼˜åŒ– 3D åœºæ™¯è¡¨ç¤ºå’Œç›¸æœºè¿åŠ¨è½¨è¿¹ã€‚å¹¿æ³›çš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œä¸ä¹‹å‰çš„æœ€å…ˆè¿›çš„å·¥ä½œç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æä¾›é«˜è´¨é‡çš„æ–°è§†è§’å›¾åƒï¼Œå¹¶å®ç°å®æ—¶æ¸²æŸ“ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§é’ˆå¯¹è¿åŠ¨æ¨¡ç³Šå›¾åƒè®¾è®¡çš„ç…§åº¦æŸè°ƒæ•´å…¬å¼ï¼Œè¯¥å…¬å¼é€šè¿‡æœ€å°åŒ–è¾“å…¥æ¨¡ç³Šå›¾åƒå’Œåˆæˆæ¨¡ç³Šå›¾åƒä¹‹é—´çš„å…‰åº¦è¯¯å·®æ¥ä¼˜åŒ–æ²¿è½¨è¿¹çš„é«˜æ–¯ä½“ã€‚æ€§èƒ½ï¼šå®éªŒç»“æœè¡¨æ˜ï¼Œä¸éšå¼ç¥ç»æ¸²æŸ“æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¸²æŸ“è´¨é‡å’Œå®æ—¶æ¸²æŸ“é€Ÿåº¦æ–¹é¢å‡å–å¾—äº†æ›´å¥½çš„æ¸²æŸ“æ€§èƒ½ã€‚å·¥ä½œé‡ï¼šæœ¬æ–‡çš„å·¥ä½œé‡è¾ƒå¤§ï¼Œæ¶‰åŠè¿åŠ¨æ¨¡ç³Šå›¾åƒå½¢æˆè¿‡ç¨‹çš„å»ºæ¨¡ã€ç…§åº¦æŸè°ƒæ•´å…¬å¼çš„æ¨å¯¼ã€å¯å¾®é«˜æ–¯å…‰æ …åŒ–çš„å®ç°ä»¥åŠå¤§é‡å®éªŒè¯„ä¼°ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-871ef737506910d16a3db1b8a1303bc1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6222b229bdfe559d453c0febd770960d.jpg" align="middle"></details><h2 id="UV-Gaussians-Joint-Learning-of-Mesh-Deformation-and-Gaussian-Textures-for-Human-Avatar-Modeling"><a href="#UV-Gaussians-Joint-Learning-of-Mesh-Deformation-and-Gaussian-Textures-for-Human-Avatar-Modeling" class="headerlink" title="UV Gaussians: Joint Learning of Mesh Deformation and Gaussian Textures   for Human Avatar Modeling"></a>UV Gaussians: Joint Learning of Mesh Deformation and Gaussian Textures   for Human Avatar Modeling</h2><p><strong>Authors:Yujiao Jiang, Qingmin Liao, Xiaoyu Li, Li Ma, Qi Zhang, Chaopeng Zhang, Zongqing Lu, Ying Shan</strong></p><p>Reconstructing photo-realistic drivable human avatars from multi-view image sequences has been a popular and challenging topic in the field of computer vision and graphics. While existing NeRF-based methods can achieve high-quality novel view rendering of human models, both training and inference processes are time-consuming. Recent approaches have utilized 3D Gaussians to represent the human body, enabling faster training and rendering. However, they undermine the importance of the mesh guidance and directly predict Gaussians in 3D space with coarse mesh guidance. This hinders the learning procedure of the Gaussians and tends to produce blurry textures. Therefore, we propose UV Gaussians, which models the 3D human body by jointly learning mesh deformations and 2D UV-space Gaussian textures. We utilize the embedding of UV map to learn Gaussian textures in 2D space, leveraging the capabilities of powerful 2D networks to extract features. Additionally, through an independent Mesh network, we optimize pose-dependent geometric deformations, thereby guiding Gaussian rendering and significantly enhancing rendering quality. We collect and process a new dataset of human motion, which includes multi-view images, scanned models, parametric model registration, and corresponding texture maps. Experimental results demonstrate that our method achieves state-of-the-art synthesis of novel view and novel pose. The code and data will be made available on the homepage <a href="https://alex-jyj.github.io/UV-Gaussians/">https://alex-jyj.github.io/UV-Gaussians/</a> once the paper is accepted. </p><p><a href="http://arxiv.org/abs/2403.11589v1">PDF</a> </p><p><strong>Summary</strong><br>å€ŸåŠ© UV é«˜æ–¯ä½“ï¼Œé€šè¿‡è”åˆå­¦ä¹ ç½‘æ ¼å˜å½¢å’Œ 2D UV ç©ºé—´é«˜æ–¯çº¹ç†ï¼Œå¯¹ 3D äººä½“è¿›è¡Œå»ºæ¨¡ï¼Œå®ç°é«˜ä¿çœŸå¯é©¾é©¶äººçš„å¤´åƒé‡å»ºã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ä½¿ç”¨ 3D é«˜æ–¯ä½“è¡¨ç¤ºäººä½“ï¼Œå®ç°æ¯” NeRF æ›´å¿«çš„è®­ç»ƒå’Œæ¸²æŸ“ã€‚</li><li>åœ¨ 2D UV ç©ºé—´è€Œä¸æ˜¯ 3D ç©ºé—´ä¸­å­¦ä¹ é«˜æ–¯çº¹ç†ï¼Œåˆ©ç”¨å¼ºå¤§çš„ 2D ç½‘ç»œã€‚</li><li>ç‹¬ç«‹çš„ç½‘æ ¼ç½‘ç»œä¼˜åŒ–ä¸å§¿åŠ¿ç›¸å…³çš„å‡ ä½•å˜å½¢ï¼ŒæŒ‡å¯¼é«˜æ–¯æ¸²æŸ“ã€‚</li><li>æ”¶é›†å’Œå¤„ç†åŒ…å«å¤šè§†å›¾å›¾åƒã€æ‰«ææ¨¡å‹ã€å‚æ•°æ¨¡å‹é…å‡†å’Œç›¸åº”çº¹ç†æ˜ å°„çš„æ–°æ•°æ®é›†ã€‚</li><li>å®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•å®ç°äº†æœ€å…ˆè¿›çš„æ–°è§†å›¾å’Œæ–°å§¿åŠ¿åˆæˆã€‚</li><li>è®ºæ–‡æ¥å—åï¼Œä»£ç å’Œæ•°æ®å°†åœ¨ä¸»é¡µä¸Šå…¬å¼€ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šUVGaussiansï¼šç½‘æ ¼æ–°è§†è§’è”åˆå­¦ä¹ </li><li>ä½œè€…ï¼šY. Jiang, H. Wu, Z. Wang, K. Zhou, Y. Zhang, C. Pan, Y. Liu</li><li>å•ä½ï¼šæ— </li><li>å…³é”®è¯ï¼šHumanModelingÂ·NeuralRenderingÂ·GaussianSplatting</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2207.02938   Githubä»£ç é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šä»å¤šè§†è§’å›¾åƒåºåˆ—é‡å»ºé€¼çœŸçš„å¯é©¾é©¶äººä½“åŒ–èº«ä¸€ç›´æ˜¯è®¡ç®—æœºè§†è§‰å’Œå›¾å½¢å­¦é¢†åŸŸçš„ä¸€ä¸ªçƒ­é—¨ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„è¯¾é¢˜ã€‚è™½ç„¶ç°æœ‰çš„åŸºäºNeRFçš„æ–¹æ³•å¯ä»¥å®ç°é«˜è´¨é‡çš„äººä½“æ¨¡å‹æ–°è§†è§’æ¸²æŸ“ï¼Œä½†è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹éƒ½å¾ˆè€—æ—¶ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šæœ€è¿‘çš„æ–¹æ³•åˆ©ç”¨3Dé«˜æ–¯ä½“è¡¨ç¤ºäººä½“ï¼Œä»è€Œå®ç°æ›´å¿«çš„è®­ç»ƒå’Œæ¸²æŸ“ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä½ä¼°äº†ç½‘æ ¼å¼•å¯¼çš„é‡è¦æ€§ï¼Œå¹¶ç›´æ¥åœ¨3Dç©ºé—´ä¸­é¢„æµ‹é«˜æ–¯ä½“ï¼Œç½‘æ ¼å¼•å¯¼ç²—ç³™ã€‚è¿™é˜»ç¢äº†é«˜æ–¯ä½“çš„å­¦ä¹ è¿‡ç¨‹ï¼Œå¹¶å€¾å‘äºäº§ç”Ÿæ¨¡ç³Šçš„çº¹ç†ã€‚ï¼ˆ3ï¼‰æœ¬æ–‡æ–¹æ³•ï¼šå› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†UVGaussiansï¼Œå®ƒé€šè¿‡è”åˆå­¦ä¹ ç½‘æ ¼å˜å½¢å’Œ2D UVç©ºé—´é«˜æ–¯çº¹ç†å¯¹3Däººä½“è¿›è¡Œå»ºæ¨¡ã€‚æˆ‘ä»¬åˆ©ç”¨UVè´´å›¾çš„åµŒå…¥åœ¨2Dç©ºé—´ä¸­å­¦ä¹ é«˜æ–¯çº¹ç†ï¼Œåˆ©ç”¨å¼ºå¤§çš„2Dç½‘ç»œæå–ç‰¹å¾çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œé€šè¿‡ä¸€ä¸ªç‹¬ç«‹çš„Meshç½‘ç»œï¼Œæˆ‘ä»¬ä¼˜åŒ–ä¸å§¿åŠ¿ç›¸å…³çš„å‡ ä½•å˜å½¢ï¼Œä»è€Œå¼•å¯¼é«˜æ–¯æ¸²æŸ“å¹¶æ˜¾ç€æé«˜æ¸²æŸ“è´¨é‡ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šæˆ‘ä»¬æ”¶é›†å¹¶å¤„ç†äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…æ‹¬å¤šè§†è§’å›¾åƒã€æ‰«ææ¨¡å‹ã€å‚æ•°æ¨¡å‹é…å‡†å’Œç›¸åº”çš„çº¹ç†è´´å›¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ–°çš„è§†è§’å’Œæ–°çš„å§¿åŠ¿åˆæˆæ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„æ•ˆæœã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼šæ•°æ®å¤„ç†ï¼šåˆ©ç”¨ SMPL-X æ¨¡å‹ã€MVS æ–¹æ³•å’Œç›®æ ‡ä¼˜åŒ–æ–¹æ³•ï¼Œå¯¹åŸå§‹æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼Œè·å¾—åŒ…æ‹¬æœè£…å‡ ä½•å’Œçº¹ç†æ˜ å°„çš„ SMPLX-D ç½‘æ ¼æ¨¡å‹ï¼›ï¼ˆ2ï¼‰ï¼šåŸºäºå§¿åŠ¿çš„ç½‘æ ¼å˜å½¢ï¼šé€‰æ‹©ä¸€ä¸ªæ¥è¿‘ T å§¿åŠ¿çš„å¸§ä½œä¸ºå‚è€ƒï¼Œä½¿ç”¨çº¿æ€§æ··åˆè’™çš®å°†å…¶å˜å½¢ä¸ºæ ‡å‡† T å§¿åŠ¿ï¼Œç„¶åä½¿ç”¨ MeshU-Net å­¦ä¹ åŸºäºå§¿åŠ¿çš„ç½‘æ ¼å˜å½¢ï¼Œå°† 3D é¡¶ç‚¹åæ ‡å…‰æ …åŒ–ä¸º UV ç©ºé—´ï¼Œé¢„æµ‹é¡¶ç‚¹åç§»é‡ï¼›ï¼ˆ3ï¼‰ï¼šåŸºäºå§¿åŠ¿çš„é«˜æ–¯çº¹ç†ï¼šé‡‡ç”¨ GaussianU-Net å­¦ä¹ åŸºäºå§¿åŠ¿çš„é«˜æ–¯çº¹ç†ï¼Œå°† 3D é«˜æ–¯ä½“å‚æ•°åŒ–ä¸º UV ç©ºé—´ä¸­çš„é«˜æ–¯çº¹ç†ï¼Œåˆ©ç”¨å¹³å‡çº¹ç†å›¾ä½œä¸ºåˆå§‹é¢œè‰²ä¿¡æ¯ï¼Œæä¾›ä½ç½®å›¾å’Œè§†å‘å‘é‡ä»¥å»ºæ¨¡è§†å‘ä¾èµ–æ€§ï¼›ï¼ˆ4ï¼‰ï¼šç½‘æ ¼å¼•å¯¼çš„ 3D é«˜æ–¯ä½“åŠ¨ç”»ï¼šåˆ©ç”¨ UV æ©ç è¿‡æ»¤çº¹ç†å›¾ä¸­çš„æ— å…³åƒç´ ï¼Œé€šè¿‡ UV æ˜ å°„å°†å‰©ä½™åƒç´ è½¬æ¢ä¸º 3D ç©ºé—´ä¸­çš„é«˜æ–¯ç‚¹ï¼Œæ·»åŠ ç½‘æ ¼æ¸²æŸ“çš„ä½ç½®å›¾å’Œé«˜æ–¯ç‚¹çš„åç§»é‡è®¡ç®—æœ€ç»ˆä½ç½®ï¼Œåˆ©ç”¨å¯å¾®åˆ†é«˜æ–¯å…‰æ …åŒ–ç”Ÿæˆæœ€ç»ˆå›¾åƒï¼›ï¼ˆ5ï¼‰ï¼šè®­ç»ƒï¼šè”åˆä¼˜åŒ– MeshU-Net å’Œ GaussianU-Netï¼Œä½¿ç”¨åŸºäºå¸§çš„ SMPLX-D æ¨¡å‹ç›‘ç£ç½‘æ ¼å˜å½¢ï¼Œä½¿ç”¨ L1 æŸå¤±ã€SSIM æŸå¤±ã€æ„ŸçŸ¥æŸå¤±å’Œæ­£åˆ™åŒ–æŸå¤±ç›‘ç£æ¸²æŸ“å›¾åƒã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º UVGaussians çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº† 3D é«˜æ–¯ä½“å’Œ UV ç©ºé—´è¡¨ç¤ºã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿä»å¤šè§†è§’å›¾åƒé‡å»ºé€¼çœŸçš„ã€å§¿æ€é©±åŠ¨çš„åŒ–èº«æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä»¥æ¨¡å‹é¡¶ç‚¹çš„ä½ç§»å›¾ä½œä¸ºè¾“å…¥ï¼Œé€šè¿‡ MeshU-Net å­¦ä¹ ä¸å§¿æ€ç›¸å…³çš„å‡ ä½•å˜å½¢ï¼Œå¹¶é€šè¿‡ GaussianU-Net å­¦ä¹ åµŒå…¥åœ¨ UV ç©ºé—´ä¸­çš„é«˜æ–¯ç‚¹çš„å±æ€§ã€‚éšåï¼Œåœ¨ç²¾ç»†çš„ç½‘æ ¼å¼•å¯¼ä¸‹ï¼Œå¯¹é«˜æ–¯ç‚¹è¿›è¡Œæ¸²æŸ“ä»¥ä»ä»»æ„è§†ç‚¹è·å¾—æ¸²æŸ“å›¾åƒã€‚é€šè¿‡å°†ç»†ç²’åº¦çš„å‡ ä½•æŒ‡å¯¼å’Œåˆ©ç”¨ UV ç©ºé—´ä¸­å¼ºå¤§çš„ 2D ç½‘ç»œçš„ç‰¹å¾å­¦ä¹ èƒ½åŠ›ç›¸ç»“åˆï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ–°çš„è§†è§’å’Œæ–°çš„å§¿æ€åˆæˆå®éªŒä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§ç»“åˆ 3D é«˜æ–¯ä½“å’Œ UV ç©ºé—´è¡¨ç¤ºçš„æ–°æ–¹æ³•ï¼Œç”¨äºä»å¤šè§†è§’å›¾åƒé‡å»ºé€¼çœŸçš„ã€å§¿æ€é©±åŠ¨çš„åŒ–èº«æ¨¡å‹ã€‚æ€§èƒ½ï¼šåœ¨æ–°çš„è§†è§’å’Œæ–°çš„å§¿æ€åˆæˆå®éªŒä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚å·¥ä½œé‡ï¼šéœ€è¦æ‰«æçš„ç½‘æ ¼ï¼Œå¹¶ä¸”å¯¹æåº¦å®½æ¾çš„æœè£…ï¼ˆä¾‹å¦‚é•¿è£™ï¼‰çš„å¤„ç†èƒ½åŠ›æœ‰é™ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-a863ff88a8f3aab922fde1833cf3125b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6c24e3d34d46677eafb334d061117f93.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e62a000f486adba73f5ad94566312cdc.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-03-23  MVSplat Efficient 3D Gaussian Splatting from Sparse Multi-View Images</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/03/23/Paper/2024-03-23/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/03/23/Paper/2024-03-23/Talking%20Head%20Generation/</id>
    <published>2024-03-23T09:49:15.000Z</published>
    <updated>2024-03-23T09:49:15.923Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-03-23-æ›´æ–°"><a href="#2024-03-23-æ›´æ–°" class="headerlink" title="2024-03-23 æ›´æ–°"></a>2024-03-23 æ›´æ–°</h1><h2 id="EmoVOCA-Speech-Driven-Emotional-3D-Talking-Heads"><a href="#EmoVOCA-Speech-Driven-Emotional-3D-Talking-Heads" class="headerlink" title="EmoVOCA: Speech-Driven Emotional 3D Talking Heads"></a>EmoVOCA: Speech-Driven Emotional 3D Talking Heads</h2><p><strong>Authors:Federico Nocentini, Claudio Ferrari, Stefano Berretti</strong></p><p>The domain of 3D talking head generation has witnessed significant progress in recent years. A notable challenge in this field consists in blending speech-related motions with expression dynamics, which is primarily caused by the lack of comprehensive 3D datasets that combine diversity in spoken sentences with a variety of facial expressions. Whereas literature works attempted to exploit 2D video data and parametric 3D models as a workaround, these still show limitations when jointly modeling the two motions. In this work, we address this problem from a different perspective, and propose an innovative data-driven technique that we used for creating a synthetic dataset, called EmoVOCA, obtained by combining a collection of inexpressive 3D talking heads and a set of 3D expressive sequences. To demonstrate the advantages of this approach, and the quality of the dataset, we then designed and trained an emotional 3D talking head generator that accepts a 3D face, an audio file, an emotion label, and an intensity value as inputs, and learns to animate the audio-synchronized lip movements with expressive traits of the face. Comprehensive experiments, both quantitative and qualitative, using our data and generator evidence superior ability in synthesizing convincing animations, when compared with the best performing methods in the literature. Our code and pre-trained model will be made available. </p><p><a href="http://arxiv.org/abs/2403.12886v1">PDF</a> </p><p><strong>Summary</strong></p><p>é€šè¿‡å°†éè¡¨æƒ… 3D ä¼šè¯´è¯çš„äººç‰©å’Œä¸€ç³»åˆ—è¡¨æƒ… 3D åºåˆ—ç›¸ç»“åˆï¼Œåˆ›å»ºäº†ä¸€ä¸ªåä¸º EmoVOCA çš„åˆæˆæ•°æ®é›†ï¼Œç”¨äºè§£å†³ 3D ä¼šè¯´è¯çš„äººç‰©ç”Ÿæˆé¢†åŸŸä¸­è¯­éŸ³ç›¸å…³åŠ¨ä½œä¸è¡¨æƒ…åŠ¨æ€æ··åˆçš„æŒ‘æˆ˜ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>3D ä¼šè¯´è¯çš„äººç‰©ç”Ÿæˆé¢ä¸´è¯­éŸ³ç›¸å…³åŠ¨ä½œä¸è¡¨æƒ…åŠ¨æ€èåˆçš„æŒ‘æˆ˜ã€‚</li><li>ç°æœ‰æ–¹æ³•ä½¿ç”¨ 2D è§†é¢‘æ•°æ®å’Œå‚æ•°åŒ– 3D æ¨¡å‹è§£å†³è¯¥é—®é¢˜ï¼Œä½†å­˜åœ¨è”åˆå»ºæ¨¡ä¸¤ä¸ªåŠ¨ä½œçš„å±€é™æ€§ã€‚</li><li>æœ¬æ–‡æå‡ºä¸€ç§åˆ›æ–°çš„æ•°æ®é©±åŠ¨æŠ€æœ¯ï¼Œé€šè¿‡ç»“åˆéè¡¨æƒ… 3D ä¼šè¯´è¯çš„äººç‰©å’Œè¡¨æƒ… 3D åºåˆ—åˆ›å»ºåˆæˆæ•°æ®é›† EmoVOCAã€‚</li><li>ä½¿ç”¨ EmoVOCA æ•°æ®è®­ç»ƒçš„æƒ…æ„Ÿ 3D ä¼šè¯´è¯çš„äººç‰©ç”Ÿæˆå™¨å¯ä»¥æ¥å— 3D é¢éƒ¨ã€éŸ³é¢‘æ–‡ä»¶ã€æƒ…æ„Ÿæ ‡ç­¾å’Œå¼ºåº¦å€¼ä½œä¸ºè¾“å…¥ï¼Œå¹¶å­¦ä¹ ä¸ºé¢éƒ¨çš„è¡¨æƒ…ç‰¹å¾åˆ¶ä½œä¸éŸ³é¢‘åŒæ­¥çš„å˜´å”‡è¿åŠ¨åŠ¨ç”»ã€‚</li><li>ç»¼åˆå®éªŒè¡¨æ˜ï¼Œä¸æ–‡çŒ®ä¸­è¡¨ç°æœ€ä½³çš„æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨åˆæˆä»¤äººä¿¡æœçš„åŠ¨ç”»æ–¹é¢å…·æœ‰å“è¶Šçš„èƒ½åŠ›ã€‚</li><li>ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å°†å…¬å¼€ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šEmoVOCAï¼šè¯­éŸ³é©±åŠ¨çš„ä¸‰ç»´æƒ…æ„Ÿè¯´è¯äººå¤´éƒ¨</li><li>ä½œè€…ï¼šFederico Nocentiniã€Claudio Ferrariã€Stefano Berretti</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šä½›ç½—ä¼¦è¨å¤§å­¦åª’ä½“æ•´åˆä¸ä¼ æ’­ä¸­å¿ƒï¼ˆMICCï¼‰</li><li>å…³é”®è¯ï¼šæƒ…æ„Ÿä¸‰ç»´è¯´è¯äººå¤´éƒ¨ã€ä¸‰ç»´æ•°æ®é›†ã€ä¸‰ç»´åŠ¨ç”»ã€ä¸‰ç»´ç‰¹å¾ç»„åˆ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.12886ï¼ŒGithub ä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šä¸‰ç»´è¯´è¯äººå¤´éƒ¨ç”Ÿæˆé¢†åŸŸè¿‘å¹´æ¥å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚è¯¥é¢†åŸŸçš„ä¸€ä¸ªæ˜¾è‘—æŒ‘æˆ˜åœ¨äºæ··åˆä¸è¯­éŸ³ç›¸å…³çš„åŠ¨ä½œå’Œè¡¨æƒ…åŠ¨æ€ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºç¼ºä¹å°†å£è¯­å¥å­å¤šæ ·æ€§ä¸å„ç§é¢éƒ¨è¡¨æƒ…ç›¸ç»“åˆçš„ç»¼åˆä¸‰ç»´æ•°æ®é›†ã€‚è™½ç„¶æ–‡çŒ®å·¥ä½œå°è¯•åˆ©ç”¨äºŒç»´è§†é¢‘æ•°æ®å’Œå‚æ•°åŒ–ä¸‰ç»´æ¨¡å‹ä½œä¸ºä¸€ç§è§£å†³æ–¹æ³•ï¼Œä½†å®ƒä»¬åœ¨è”åˆå»ºæ¨¡è¿™ä¸¤ä¸ªåŠ¨ä½œæ—¶ä»ç„¶è¡¨ç°å‡ºå±€é™æ€§ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠé—®é¢˜ï¼šæœ¬æ–‡ä»ä¸åŒçš„è§’åº¦è§£å†³äº†è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åˆ›æ–°çš„æ•°æ®é©±åŠ¨æŠ€æœ¯ï¼Œç”¨äºåˆ›å»ºåˆæˆæ•°æ®é›† EmoVOCAï¼Œè¯¥æ•°æ®é›†é€šè¿‡ç»„åˆä¸€ç³»åˆ—æ— è¡¨æƒ…ä¸‰ç»´è¯´è¯äººå¤´éƒ¨å’Œä¸€ç»„ä¸‰ç»´è¡¨æƒ…åºåˆ—è·å¾—ã€‚ä¸ºäº†å±•ç¤ºè¿™ç§æ–¹æ³•çš„ä¼˜åŠ¿å’Œæ•°æ®é›†çš„è´¨é‡ï¼Œæˆ‘ä»¬è®¾è®¡å¹¶è®­ç»ƒäº†ä¸€ä¸ªæƒ…æ„Ÿä¸‰ç»´è¯´è¯äººå¤´éƒ¨ç”Ÿæˆå™¨ï¼Œè¯¥ç”Ÿæˆå™¨æ¥å—ä¸‰ç»´é¢éƒ¨ã€éŸ³é¢‘æ–‡ä»¶ã€è¡¨æƒ…æ ‡ç­¾å’Œå¼ºåº¦å€¼ä½œä¸ºè¾“å…¥ï¼Œå¹¶å­¦ä¼šäº†ç”¨é¢éƒ¨çš„è¡¨æƒ…ç‰¹å¾æ¥ä¸ºéŸ³é¢‘åŒæ­¥çš„å”‡éƒ¨åŠ¨ä½œæ·»åŠ åŠ¨ç”»ã€‚ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæˆ‘ä»¬åˆ©ç”¨æ•°æ®å’Œç”Ÿæˆå™¨è¿›è¡Œäº†å…¨é¢å®éªŒï¼ŒåŒ…æ‹¬å®šé‡å’Œå®šæ€§å®éªŒï¼Œè¯æ˜äº†åœ¨åˆæˆä»¤äººä¿¡æœçš„åŠ¨ç”»æ–¹é¢ï¼Œä¸æ–‡çŒ®ä¸­æ€§èƒ½æœ€ä½³çš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å…·æœ‰ä¼˜è¶Šæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å°†å…¬å¼€ã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†æ€æ ·çš„æ€§èƒ½ï¼Œè¿™äº›æ€§èƒ½æ˜¯å¦æ”¯æŒå…¶ç›®æ ‡ï¼šåœ¨ä¸‰ç»´æƒ…æ„Ÿè¯´è¯äººå¤´éƒ¨åˆæˆä»»åŠ¡ä¸Šï¼Œä¸ç°æœ‰æœ€ä¼˜æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸­å‡å–å¾—äº†æ›´å¥½çš„æ€§èƒ½ï¼Œæ”¯æŒäº†æˆ‘ä»¬æå‡ºçš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li></ol><p><strong>Methods</strong>(1) æ•°æ®å‡†å¤‡ï¼šåˆ†åˆ«ä»ä¸¤ä¸ªæ•°æ®é›† DT å’Œ DE ä¸­é¢„å¤„ç†è¯´è¯å’Œè¡¨æƒ…æ•°æ®ï¼Œå»é™¤èº«ä»½ä¿¡æ¯ï¼Œç”ŸæˆåŸºäºä½ç§»çš„è¡¨ç¤º ST å’Œ SEã€‚(2) åŒç¼–ç å™¨/å…±äº«è§£ç å™¨æ¶æ„ï¼šä½¿ç”¨ SpiralNet æ„å»ºåŒç¼–ç å™¨ ET å’Œ EEï¼Œåˆ†åˆ«å¤„ç†è¯´è¯å’Œè¡¨æƒ…æ•°æ®ï¼Œç”Ÿæˆæ½œåœ¨ç‰¹å¾å‘é‡ã€‚å…±äº«è§£ç å™¨ D é‡å»ºè¾“å…¥ä½ç§»ã€‚(3) è®­ç»ƒé˜¶æ®µï¼šäº¤æ›¿è®­ç»ƒç¼–ç å™¨ï¼Œä½¿ç”¨åŠ æƒ L2 æŸå¤±å‡½æ•°é‡å»ºè¾“å…¥ä½ç§»ã€‚(4) æ¨ç†é˜¶æ®µï¼šè¿æ¥ç¼–ç å™¨æå–çš„ç‰¹å¾ï¼Œå¹¶å°†å…¶è¾“å…¥è§£ç å™¨ï¼Œç”Ÿæˆæ··åˆåŠ¨ä½œã€‚é€šè¿‡è°ƒæ•´ç³»æ•° Âµt å’Œ Âµeï¼Œå¯ä»¥æ§åˆ¶è¯´è¯å’Œè¡¨æƒ…ä½ç§»ä¿¡æ¯ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚</p><ol><li>ç»“è®ºï¼š(1): æœ¬å·¥ä½œé€šè¿‡æå‡º EmoVOCA æ•°æ®é›†å’Œç”Ÿæˆå™¨ï¼Œä¸ºæƒ…æ„Ÿä¸‰ç»´è¯´è¯äººå¤´éƒ¨åˆæˆé¢†åŸŸåšå‡ºäº†è´¡çŒ®ã€‚(2): åˆ›æ–°ç‚¹ï¼š<ul><li>æå‡ºäº†ä¸€ç§æ•°æ®é©±åŠ¨æ–¹æ³•æ¥åˆ›å»ºåˆæˆæ•°æ®é›† EmoVOCAã€‚</li><li>è®¾è®¡äº†ä¸€ä¸ªåŒç¼–ç å™¨/å…±äº«è§£ç å™¨æ¶æ„ï¼Œå¯ä»¥æ··åˆè¯´è¯å’Œè¡¨æƒ…åŠ¨æ€ã€‚æ€§èƒ½ï¼š</li><li>ä¸ç°æœ‰æœ€ä¼˜æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨åˆæˆä»¤äººä¿¡æœçš„åŠ¨ç”»æ–¹é¢å–å¾—äº†æ›´å¥½çš„æ€§èƒ½ã€‚å·¥ä½œé‡ï¼š</li><li>æ•°æ®é›†çš„æ”¶é›†å’Œé¢„å¤„ç†éœ€è¦å¤§é‡å·¥ä½œã€‚</li><li>ç”Ÿæˆå™¨çš„è®­ç»ƒè¿‡ç¨‹ä¹Ÿéœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºã€‚</li></ul></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-5a946bd55f83d315cf60d0684c032a32.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fcc4afff7814e4ce19b73d5e8b1b3aa0.jpg" align="middle"></details><h2 id="ScanTalk-3D-Talking-Heads-from-Unregistered-Scans"><a href="#ScanTalk-3D-Talking-Heads-from-Unregistered-Scans" class="headerlink" title="ScanTalk: 3D Talking Heads from Unregistered Scans"></a>ScanTalk: 3D Talking Heads from Unregistered Scans</h2><p><strong>Authors:Federico Nocentini, Thomas Besnier, Claudio Ferrari, Sylvain Arguillere, Stefano Berretti, Mohamed Daoudi</strong></p><p>Speech-driven 3D talking heads generation has emerged as a significant area of interest among researchers, presenting numerous challenges. Existing methods are constrained by animating faces with fixed topologies, wherein point-wise correspondence is established, and the number and order of points remains consistent across all identities the model can animate. In this work, we present ScanTalk, a novel framework capable of animating 3D faces in arbitrary topologies including scanned data. Our approach relies on the DiffusionNet architecture to overcome the fixed topology constraint, offering promising avenues for more flexible and realistic 3D animations. By leveraging the power of DiffusionNet, ScanTalk not only adapts to diverse facial structures but also maintains fidelity when dealing with scanned data, thereby enhancing the authenticity and versatility of generated 3D talking heads. Through comprehensive comparisons with state-of-the-art methods, we validate the efficacy of our approach, demonstrating its capacity to generate realistic talking heads comparable to existing techniques. While our primary objective is to develop a generic method free from topological constraints, all state-of-the-art methodologies are bound by such limitations. Code for reproducing our results, and the pre-trained model will be made available. </p><p><a href="http://arxiv.org/abs/2403.10942v2">PDF</a> </p><p><strong>Summary</strong><br>é€šè¿‡ DiffusionNet æŠ€æœ¯åˆ›æ–°ï¼ŒScanTalk çªç ´äº† 3D è¯´è¯äººå¤´éƒ¨ç”Ÿæˆä¸­å›ºå®šæ‹“æ‰‘çš„é™åˆ¶ï¼Œå¯å¤„ç†æ‰«ææ•°æ®å¹¶ç”Ÿæˆé€¼çœŸçš„é¢éƒ¨åŠ¨ç”»ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ScanTalk é‡‡ç”¨ DiffusionNet æ¶æ„ï¼Œå…‹æœäº†å›ºå®šæ‹“æ‰‘çš„é™åˆ¶ï¼Œå®ç°çµæ´»ä¸”é€¼çœŸçš„ 3D åŠ¨ç”»ã€‚</li><li>ScanTalk é€‚ç”¨äºå„ç§é¢éƒ¨ç»“æ„ï¼ŒåŒ…æ‹¬æ‰«ææ•°æ®ï¼Œæé«˜äº†ç”Ÿæˆ 3D è¯´è¯äººå¤´éƒ¨çš„çœŸå®æ€§å’Œé€šç”¨æ€§ã€‚</li><li>ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼ŒScanTalk åœ¨ç”Ÿæˆé€¼çœŸçš„è¯´è¯äººå¤´éƒ¨æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li><li>ScanTalk çš„ç›®æ ‡æ˜¯å¼€å‘ä¸€ç§ä¸å—æ‹“æ‰‘çº¦æŸçš„é€šç”¨æ–¹æ³•ï¼Œè€Œç°æœ‰æŠ€æœ¯å‡å—æ­¤ç±»é™åˆ¶ã€‚</li><li>ScanTalk å°†æä¾›å¯å¤ç°ç»“æœçš„ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹ã€‚</li><li>ScanTalk çªç ´äº†å›ºå®šæ‹“æ‰‘çš„é™åˆ¶ï¼Œä½¿ 3D è¯´è¯äººå¤´éƒ¨ç”Ÿæˆæ›´åŠ çµæ´»å’ŒçœŸå®ã€‚</li><li>ScanTalk å¯å¤„ç†æ‰«ææ•°æ®ï¼Œå¢å¼ºäº†ç”Ÿæˆçš„ 3D è¯´è¯äººå¤´éƒ¨çš„çœŸå®æ€§ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>é¢˜ç›®ï¼šScanTalk</li><p></p><p></p><li>ä½œè€…ï¼šF. Nocentini, M. Dantone, N. Garbin, A. Stosic, A. Giachetti, M. Zanoni</li><p></p><p></p><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šæ„å¤§åˆ©æ¯”è¨å¤§å­¦</li><p></p><p></p><li>å…³é”®è¯ï¼š3D Talking Headsã€3D Scans Animationã€DiffusionNet</li><p></p><p></p><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.10942   Github ä»£ç é“¾æ¥ï¼šæ— </li><p></p><p></p><li>æ‘˜è¦ï¼š   (1): ç ”ç©¶èƒŒæ™¯ï¼šè¯­éŸ³é©±åŠ¨çš„ 3D ä¼šè¯å¤´ç”Ÿæˆæ˜¯ä¸€ä¸ªæ´»è·ƒçš„ç ”ç©¶é¢†åŸŸï¼Œä½†ç°æœ‰æ–¹æ³•å—é™äºå›ºå®šæ‹“æ‰‘çš„åŠ¨ç”»é¢éƒ¨ï¼Œå³ç‚¹å¯¹ç‚¹å¯¹åº”å…³ç³»å·²å»ºç«‹ï¼Œå¹¶ä¸”æ‰€æœ‰èº«ä»½çš„ç‚¹æ•°å’Œé¡ºåºä¿æŒä¸€è‡´ã€‚   (2): è¿‡å»æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•åœ¨å¤„ç†ä¸åŒé¢éƒ¨ç»“æ„å’Œæ‰«ææ•°æ®æ—¶è¡¨ç°å‡ºå±€é™æ€§ï¼Œå¹¶ä¸”éœ€è¦é’ˆå¯¹ç‰¹å®šæ‹“æ‰‘è¿›è¡Œè®­ç»ƒï¼Œé™åˆ¶äº†å…¶é€šç”¨æ€§å’Œçµæ´»æ€§ã€‚   (3): æœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡º ScanTalkï¼Œä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿä»¥ä»»æ„æ‹“æ‰‘ï¼ˆåŒ…æ‹¬æ‰«ææ•°æ®ï¼‰å¯¹ 3D é¢éƒ¨è¿›è¡ŒåŠ¨ç”»å¤„ç†ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ DiffusionNet æ¶æ„å…‹æœäº†å›ºå®šæ‹“æ‰‘çš„é™åˆ¶ï¼Œä¸ºæ›´çµæ´»å’Œé€¼çœŸçš„ 3D åŠ¨ç”»æä¾›äº†æœ‰å‰æ™¯çš„é€”å¾„ã€‚   (4): æ–¹æ³•æ€§èƒ½ï¼šScanTalk åœ¨ç”Ÿæˆé€¼çœŸçš„ä¼šè¯å¤´æ–¹é¢ä¸ç°æœ‰æŠ€æœ¯ç›¸å½“ï¼ŒåŒæ—¶èƒ½å¤Ÿé€‚åº”ä¸åŒçš„é¢éƒ¨ç»“æ„ï¼Œå¹¶ä¸”åœ¨å¤„ç†æ‰«ææ•°æ®æ—¶ä¿æŒä¿çœŸåº¦ï¼Œä»è€Œæé«˜äº†ç”Ÿæˆ 3D ä¼šè¯å¤´çš„çœŸå®æ€§å’Œé€šç”¨æ€§ã€‚</li><br>&lt;/ol&gt;<p></p><p>Some Error for method(æ¯”å¦‚æ˜¯ä¸æ˜¯æ²¡æœ‰Methodsè¿™ä¸ªç« èŠ‚)</p><p>8.ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬å·¥ä½œé€šè¿‡æå‡ºScanTalkæ¡†æ¶ï¼Œä¸º3Dä¼šè¯å¤´ç”Ÿæˆé¢†åŸŸåšå‡ºäº†è´¡çŒ®ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿå¤„ç†ä»»æ„æ‹“æ‰‘ï¼ŒåŒ…æ‹¬æ‰«ææ•°æ®ï¼Œä»è€Œæé«˜äº†ç”Ÿæˆ3Dä¼šè¯å¤´çš„çœŸå®æ€§å’Œé€šç”¨æ€§ã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼š* æå‡ºäº†ä¸€ç§åŸºäºDiffusionNetçš„æ–°é¢–æ¡†æ¶ï¼Œå…‹æœäº†å›ºå®šæ‹“æ‰‘çš„é™åˆ¶ã€‚* å®ç°äº†å¯¹ä¸åŒé¢éƒ¨ç»“æ„å’Œæ‰«ææ•°æ®çš„é€‚åº”æ€§ï¼Œæé«˜äº†3Dä¼šè¯å¤´çš„çµæ´»æ€§ã€‚* ä¿æŒäº†æ‰«ææ•°æ®çš„ä¿çœŸåº¦ï¼Œå¢å¼ºäº†ç”Ÿæˆ3Dä¼šè¯å¤´çš„çœŸå®æ€§ã€‚æ€§èƒ½ï¼š* åœ¨ç”Ÿæˆé€¼çœŸçš„ä¼šè¯å¤´æ–¹é¢ä¸ç°æœ‰æŠ€æœ¯ç›¸å½“ã€‚* èƒ½å¤Ÿå¤„ç†ä¸åŒçš„é¢éƒ¨ç»“æ„ï¼Œæé«˜äº†3Dä¼šè¯å¤´çš„é€‚åº”æ€§ã€‚* åœ¨å¤„ç†æ‰«ææ•°æ®æ—¶ä¿æŒäº†ä¿çœŸåº¦ï¼Œæé«˜äº†3Dä¼šè¯å¤´çš„çœŸå®æ€§ã€‚å·¥ä½œé‡ï¼š* è®ºæ–‡æä¾›äº†è¯¦ç»†çš„å®éªŒç»“æœå’Œåˆ†æï¼Œè¯æ˜äº†ScanTalkæ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚* æä¾›äº†å¼€æºä»£ç ï¼Œä¾¿äºç ”ç©¶äººå‘˜å’Œä»ä¸šè€…è¿›ä¸€æ­¥ç ”ç©¶å’Œåº”ç”¨ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-583edf2b74f12a6e9daee2470848d1ef.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c72e0189f9901c97a8bc42fcd23fa4e5.jpg" align="middle"><img src="https://pica.zhimg.com/v2-c077965f45440af345b04ecd095a9f68.jpg" align="middle"></details></ol>]]></content>
    
    
    <summary type="html">Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-03-23  EmoVOCA Speech-Driven Emotional 3D Talking Heads</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
</feed>
